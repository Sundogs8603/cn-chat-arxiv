# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Cross-Entropy Loss Functions: Theoretical Analysis and Applications.](http://arxiv.org/abs/2304.07288) | 本文对交叉熵、广义交叉熵、均方误差等一大类损失函数进行了理论分析，并提出了具有优势的双交叉熵损失函数，特别适用于存在标签噪声或类别不平衡的情况。 |
| [^2] | [Synthetically Generating Human-like Data for Sequential Decision Making Tasks via Reward-Shaped Imitation Learning.](http://arxiv.org/abs/2304.07280) | 本文提出了一种从少量人类数据开始，通过奖励塑形和模仿学习算法合成类似人类决策的数据的方法，并通过应用到电脑游戏的连续决策任务中证明其有效性。 |
| [^3] | [Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning.](http://arxiv.org/abs/2304.07278) | 本文研究了强化学习中的无关奖励探索并设计了一种算法，在保证样本收集数量满足多项式级别的情况下，能够发现所有给定奖励函数的最小值，实现了可证明的极小极大最优探索方案。 |
| [^4] | [CAD-RADS scoring of coronary CT angiography with Multi-Axis Vision Transformer: a clinically-inspired deep learning pipeline.](http://arxiv.org/abs/2304.07277) | 本文提出了一个完全自动化且可视化解释的深度学习流程，用作冠状动脉疾病筛查的决策支持系统。该流程使用经过微调的多轴视觉转换器架构对冠状动脉的多平面投影进行预处理和分类，可根据常用CAD-RADS阈值分类患者到不同程度的狭窄亚组中。 |
| [^5] | [Directly Optimizing IoU for Bounding Box Localization.](http://arxiv.org/abs/2304.07256) | 本文提出了一种新的损失函数——平滑IoU损失，可以直接最大化ground-truth bounding box和预测bounding box之间的重叠部分，从而提高目标检测的准确率。 |
| [^6] | [The R-mAtrIx Net.](http://arxiv.org/abs/2304.07247) | 本文提出了一种可以生成量子可积自旋链 R-矩阵的神经网络，通过限制神经网络学习的方式，可以搜索可积哈密顿量和相应的 R-矩阵，并探索可积自旋链的家族。该网络已在二维差分型自旋链上得到应用，并被证明是有用的。 |
| [^7] | [Machine Learning-Based Multi-Objective Design Exploration Of Flexible Disc Elements.](http://arxiv.org/abs/2304.07245) | 本文采用基于机器学习的方法，结合人工神经网络和遗传算法，对盘式联轴器中的柔性盘元件进行改进设计，降低其质量和应力，而不降低扭矩传递和不对齐能力。 |
| [^8] | [Sparsity in neural networks can increase their privacy.](http://arxiv.org/abs/2304.07234) | 神经网络的稀疏性能提高隐私性并保持性能表现 |
| [^9] | [BS-GAT Behavior Similarity Based Graph Attention Network for Network Intrusion Detection.](http://arxiv.org/abs/2304.07226) | 本文提出了一种基于行为相似性的图神经网络算法（BS-GAT），利用图注意力网络来学习构建图的表示，实现网络入侵检测，该方法优于现有的最先进方法。 |
| [^10] | [Model Predictive Control with Self-supervised Representation Learning.](http://arxiv.org/abs/2304.07219) | 该论文提出了在TD-MPC框架内使用重构函数进行自监督表示学习的方法，在机器人控制任务中实现更好的性能表现。 |
| [^11] | [Sub-meter resolution canopy height maps using self-supervised learning and a vision transformer trained on Aerial and GEDI Lidar.](http://arxiv.org/abs/2304.07213) | 本文使用自监督学习和视觉转换器方法，制作了亚米级冠层高度图，可用于细粒度的植被结构监测，为碳通量评估和土地利用管理提供宝贵信息。 |
| [^12] | [Measuring Re-identification Risk.](http://arxiv.org/abs/2304.07210) | 该论文提出了一个用于测量用户表示中再识别风险的理论框架，该框架基于假设检验，可以限制攻击者从用户的表示中获取其身份的概率。作者展示了该框架应对实际应用的普适性，并补充了攻击算法来衡量现实应用的风险。 |
| [^13] | [On the convergence of nonlinear averaging dynamics with three-body interactions on hypergraphs.](http://arxiv.org/abs/2304.07203) | 本文研究了超图上具有三体相互作用的离散时间非线性平均动力学，在初态和超图拓扑以及更新非线性相互作用下，产生了高阶动力效应。 |
| [^14] | [Just Tell Me: Prompt Engineering in Business Process Management.](http://arxiv.org/abs/2304.07183) | 该论文探讨了提示工程方法在业务流程管理中的应用。这种方法可以利用预训练的语言模型解决微调需要大量数据的问题，并为BPM研究带来诸多潜力。 |
| [^15] | [A Comparative Study on Generative Models for High Resolution Solar Observation Imaging.](http://arxiv.org/abs/2304.07169) | 本文研究了当前最先进的生成模型在高分辨率太阳图像生成中的应用，并发现扩散生成模型在生成细节方面表现比StyleGAN更好。 |
| [^16] | [Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning.](http://arxiv.org/abs/2304.07163) | 本文研究了如何基于Bandit方法将外部建议融入到强化学习中，并提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。实验结果表明这些算法在样本复杂度、学习速度和形状质量方面都取得了良好的效果。 |
| [^17] | [Combining Stochastic Explainers and Subgraph Neural Networks can Increase Expressivity and Interpretability.](http://arxiv.org/abs/2304.07152) | 本论文提出了一种结合随机解释器和子图神经网络的新框架，通过挑选有意义的子图来提高图神经网络的表达能力，并提供可解释性结果。与标准子图提取策略相比，该框架产生的子图在准确性方面可达到可比较的性能，同时提供解释的附加好处。 |
| [^18] | [End-to-End Learning with Multiple Modalities for System-Optimised Renewables Nowcasting.](http://arxiv.org/abs/2304.07151) | 本文介绍了一种利用多模态和全端到端学习来实现系统优化可再生能源即时预报的方法。该方法将全天候图像和气象传感器数据的特征结合起来，有效组合预测值，并用于实现优化电力流公式来模拟能源管理。该方法首次结合多模态和全端到端学习进行培训，实现了最小化预期总体系统成本的目的。使用荷兰真实的天空和气象数据进行的案例研究表明，所提出的MM-E2E模型将系统成本与单模式基线相比降低了30％。 |
| [^19] | [Cross Attention Transformers for Multi-modal Unsupervised Whole-Body PET Anomaly Detection.](http://arxiv.org/abs/2304.07147) | 本论文提出了一种基于交叉注意力变换器的多模态无监督全身PET异常检测方法，该方法利用变换器强大的建模能力实现了高效准确地学习器官间和其成像模式的长程交互，并取得了最先进的结果。 |
| [^20] | [On Data Sampling Strategies for Training Neural Network Speech Separation Models.](http://arxiv.org/abs/2304.07142) | 本文研究了训练神经网络语音分离模型的数据采样策略对模型性能的影响。研究表明，对于特定的信号长度分布，采用特定的训练信号长度限制可以获得更好的性能。 |
| [^21] | [TUM-FA\c{C}ADE: Reviewing and enriching point cloud benchmarks for fa\c{c}ade segmentation.](http://arxiv.org/abs/2304.07140) | 本文介绍了一种用于增强现有点云数据集的方法，以便进行表面分割测试，并利用该方法创建了TUM-FA\c{C}ADE数据集。这个数据集可以促进基于点云的表面分割任务的开发，并且该方法也可以用于其他基准类型，创建更多样化的评估数据集。 |
| [^22] | [One Explanation Does Not Fit XIL.](http://arxiv.org/abs/2304.07136) | 提出了一种解释性交互式机器学习框架（XIL）来修正模型，但同时发现"一种解释不能适用于XIL"，建议考虑多种解释。 |
| [^23] | [Towards Controllable Diffusion Models via Reward-Guided Exploration.](http://arxiv.org/abs/2304.07132) | 本论文提出了一种新的框架(RGDM)，可以通过强化学习(RL)引导扩散模型的训练阶段，在不需要可微分的引导信号的情况下，实现对生成的样本的有效控制，并生成高质量、细粒度控制的样本。 |
| [^24] | [Grouping Shapley Value Feature Importances of Random Forests for explainable Yield Prediction.](http://arxiv.org/abs/2304.07111) | 本文提出了一种新的方法来解释机器学习模型中的分组特征对产量预测的影响，具有较高的效率。 |
| [^25] | [Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10.](http://arxiv.org/abs/2304.07101) | 本论文总结了HLTPR@RWTH团队在DSTC9和DSTC10中为任务导向型文档对话系统所做的贡献，包括提出了不同的方法来使选择任务更有效率，在DSTC10中提出了数据增强技术来提高模型的鲁棒性并适应生成回答的风格，以及提出了一个嘈杂的通道模型来直接建模语音识别错误。实验结果表明，该团队的方法显著优于基线模型。 |
| [^26] | [Weighted Siamese Network to Predict the Time to Onset of Alzheimer's Disease from MRI Images.](http://arxiv.org/abs/2304.07097) | 本文提出使用加权连体网络对进展性MCI患者进行序数分类，以预测他们距离严重AD阶段的距离，从而实现更准确的早期检测。 |
| [^27] | [Delta Denoising Score.](http://arxiv.org/abs/2304.07090) | 本文提出了一种新颖的Delta去噪分数（DDS）方法，可以用作基于文本的图像编辑、优化问题中的损失项，通过使用分数蒸馏采样（SDS）机制进行图像编辑，并通过匹配提示和图像来去除不希望的SDS错误方向，提高了图像编辑的精度和清晰度。 |
| [^28] | [Memory Efficient Diffusion Probabilistic Models via Patch-based Generation.](http://arxiv.org/abs/2304.07087) | 本文提出了一种基于补丁生成的扩散概率模型，通过两种条件方法来保证生成的图像在适当的位置，并具有一致的内容。这种方法可以节省内存，更适用于边缘设备。 |
| [^29] | [Uncertainty-Aware Vehicle Energy Efficiency Prediction using an Ensemble of Neural Networks.](http://arxiv.org/abs/2304.07073) | 本文基于深度神经网络集成学习方法，提出了一种减少预测不确定性并输出衡量值的车辆能效预测方法，可应用于降低碳足迹，并在测试中表现出高度的预测性能。 |
| [^30] | [Who breaks early, looses: goal oriented training of deep neural networks based on port Hamiltonian dynamics.](http://arxiv.org/abs/2304.07070) | 本文提出了一种基于哈密顿动力学的深度神经网络目标导向训练方法，通过达到预定的损失函数减少来实现从勘探到利用的转换，与标准随机梯度下降相比，该方法在多个标准数据集上取得了更好的性能。 |
| [^31] | [On Existential First Order Queries Inference on Knowledge Graphs.](http://arxiv.org/abs/2304.07063) | 本文阐述了关于知识图谱中存在性一阶查询推理的新方法，提出了一个新数据集，并开发了一种来自模糊逻辑理论的新搜索算法，该算法能够解决新公式，并在现有公式中超过以前的方法。 |
| [^32] | [Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method.](http://arxiv.org/abs/2304.07056) | 本文介绍了大规模压缩面部视频质量评估（CFVQA）数据库，用于系统地了解面部视频感知质量和多样化压缩失真。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。 |
| [^33] | [Lossy Compression of Large-Scale Radio Interferometric Data.](http://arxiv.org/abs/2304.07050) | 本文提出了一种通过基线相关的有损压缩技术来降低射电干涉可见度数据体积的方法，将整个可见度数据表示为基线的数据矩阵集合，从而实现数据的压缩，同时保留视场边缘的模糊效应。 |
| [^34] | [Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation.](http://arxiv.org/abs/2304.07048) | 本文介绍了扩展的 Wasserstein PAC-Bayes 框架，利用损失函数上的几何假设提供了新的泛化界限。通过该框架，证明了 \cite{lambert2022variational} 中算法的输出具有强大的泛化能力。同时，建立了 PAC-Bayes 和优化算法之间的桥梁。 |
| [^35] | [Hierarchical Agent-based Reinforcement Learning Framework for Automated Quality Assessment of Fetal Ultrasound Video.](http://arxiv.org/abs/2304.07036) | 本研究提出了一个基于强化学习的自动评估胎儿超声视频质量的层次代理框架，具有优化标注量、整体质量评估和人类专家结果一致性等显著优势。 |
| [^36] | [Spectral Transfer Guided Active Domain Adaptation For Thermal Imagery.](http://arxiv.org/abs/2304.07031) | 本论文提出了一种面向热成像的主动适应方法，将可见光谱和热成像结合起来并通过选择少量目标样本进行注释来缓解源域和目标域之间的域适应差距。 |
| [^37] | [Continuous time recurrent neural networks: overview and application to forecasting blood glucose in the intensive care unit.](http://arxiv.org/abs/2304.07025) | 本文介绍了连续时间自回归递归神经网络(CTRNNs)的应用, 通过连续演化来解决非规则采样的时间序列问题, 以概率预测临床监护设置中的血糖水平。 |
| [^38] | [DIPNet: Efficiency Distillation and Iterative Pruning for Image Super-Resolution.](http://arxiv.org/abs/2304.07018) | 该论文提出了一种名为DIPNet的新型多阶段轻量级网络增强方法，通过增强的高分辨率输出作为额外监督，提高轻量级学生网络的学习能力，并进一步采用重新参数化技术和迭代网络修剪等简化网络结构的方法，以实现高质量超分辨率的有效网络训练。 |
| [^39] | [Unsupervised ANN-Based Equalizer and Its Trainable FPGA Implementation.](http://arxiv.org/abs/2304.06987) | 本文提出了一种无监督基于ANN的均衡器及其可训练的FPGA实现，通过自定义的损失函数使其能够适应不同的信道条件，并实现了高效率的通信系统。 |
| [^40] | [Multi-fidelity prediction of fluid flow and temperature field based on transfer learning using Fourier Neural Operator.](http://arxiv.org/abs/2304.06972) | 本文提出了基于傅里叶神经算子和传递学习的多保真度学习方法，可以用较少的高保真度数据预测流体流动和温度场，且性能优于其他方法。 |
| [^41] | [Preserving Locality in Vision Transformers for Class Incremental Learning.](http://arxiv.org/abs/2304.06971) | 本文提出了一种Locality-Preserved Attention（LPA）层，以保留ViT中的局部特征以增强其在类增量学习中的性能。 |
| [^42] | [Self-Supervised Learning based Depth Estimation from Monocular Images.](http://arxiv.org/abs/2304.06966) | 本论文研究了基于自监督学习的单目图像深度估计，探索了多种扩展深度估计模型的方法，并计划实现姿态估计和语义分割等技术来提供更准确的深度图预测。 |
| [^43] | [Convex Dual Theory Analysis of Two-Layer Convolutional Neural Networks with Soft-Thresholding.](http://arxiv.org/abs/2304.06959) | 本文通过设计一个凸对偶网络，从理论上分析了双层卷积神经网络中使用软阈值的凸性，并在实验中进行了验证。 |
| [^44] | [Cultural-aware Machine Learning based Analysis of COVID-19 Vaccine Hesitancy.](http://arxiv.org/abs/2304.06953) | 本研究提出一种文化意识机器学习（ML）模型，可以预测COVID-19疫苗接种意愿，并揭示了西班牙裔和非洲裔美国人最有可能受到文化差异的影响，这有助于未来制定更成功的疫苗接种运动。 |
| [^45] | [PPG Signals for Hypertension Diagnosis: A Novel Method using Deep Learning Models.](http://arxiv.org/abs/2304.06952) | 本研究提出了基于PPG信号和深度学习模型的高血压分级新方法，经实验验证其在高血压诊断方面具有潜在应用价值。 |
| [^46] | [TimelyFL: Heterogeneity-aware Asynchronous Federated Learning with Adaptive Partial Training.](http://arxiv.org/abs/2304.06947) | TimelyFL提出了一种面向异构异步联邦学习的自适应部分训练框架，在训练过程中根据设备的异构性和通信模式调整汇总计划，并自适应地选择要更新模型的设备子集，从而克服了现有方法在面对异构网络时可能导致的训练精度下降和收敛速度变慢的问题。 |
| [^47] | [AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks.](http://arxiv.org/abs/2304.06941) | 本文提出了一种叫做AutoSparse的自动稀疏训练算法，其中包含梯度退火法来权衡稀疏和准确性，在ResNet50和MobileNetV1上表现出更好的准确性和较少的计算资源需求。 |
| [^48] | [Classification of social media Toxic comments using Machine learning models.](http://arxiv.org/abs/2304.06934) | 该论文提出使用Lstm-cnn模型分类器，从而更好地区分社交媒体平台上有害和非有害评论，以解决网络欺凌问题。 |
| [^49] | [Scale Federated Learning for Label Set Mismatch in Medical Image Classification.](http://arxiv.org/abs/2304.06931) | 本文提出了FedLSM框架以解决医学图像分类中标签集不匹配问题，该框架采用不同的训练策略以有效利用未标记或部分标记的数据，并在分类层采用逐类别自适应聚合，适用于多个标签集的场景。 |
| [^50] | [Interpretability is a Kind of Safety: An Interpreter-based Ensemble for Adversary Defense.](http://arxiv.org/abs/2304.06919) | 本论文揭示了解释器与对抗样本生成过程之间的相关性，提出了一种基于解释器的集成框架X-Ensemble，该框架采用了新颖的检测-矫正过程，能够进行强大的防御。 |
| [^51] | [Generating Adversarial Examples with Better Transferability via Masking Unimportant Parameters of Surrogate Model.](http://arxiv.org/abs/2304.06908) | 本文提出的MUP方法通过掩盖无关参数的方式，从而生成更具可转移性的对抗性样本，提高了攻击成功率和样本的转移能力。 |
| [^52] | [Toward Real-Time Image Annotation Using Marginalized Coupled Dictionary Learning.](http://arxiv.org/abs/2304.06907) | 本文提出了一种基于耦合字典学习和边缘化损失函数的实时图像注释方法，该方法能够学习有限数量的视觉原型和相应的语义，并保持标签的稀疏不平衡性，取得了良好的实验效果。 |
| [^53] | [Systemic Fairness.](http://arxiv.org/abs/2304.06901) | 本文解决了算法公平性文献中更广泛的系统公平性问题，并为其开发了形式主义。 |
| [^54] | [Performative Prediction with Neural Networks.](http://arxiv.org/abs/2304.06879) | 本文提出了执行预测的框架，通过找到具有执行稳定性的分类器来适用于数据分布。通过假设数据分布相对于模型的预测值可Lipschitz连续，使得我们能够放宽对损失函数的假设要求。 |
| [^55] | [Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales.](http://arxiv.org/abs/2304.06875) | 提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。 |
| [^56] | [Tempo vs. Pitch: understanding self-supervised tempo estimation.](http://arxiv.org/abs/2304.06868) | 在音乐信息检索中，本文通过严格实验剖析了一种应用于节奏估计的自监督音高估计模型，着重研究了自监督节奏估计中输入表述和数据分布的关系。 |
| [^57] | [Evaluation of Social Biases in Recent Large Pre-Trained Models.](http://arxiv.org/abs/2304.06861) | 本文研究了最近发布的三个预训练模型的偏见问题，并评估了它们在两个偏见基准上的表现，探讨了是否随着技术进步，最新的、更快、更轻的模型在开发时负责任地降低了与旧模型相比的社会偏见。 |
| [^58] | [Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter.](http://arxiv.org/abs/2304.06858) | 本文介绍了一个推特疫苗数据集Vax-Culture，它旨在找出推广疫苗错误信息的文化和政治信念的重叠部分，帮助开发机器学习模型以自动检测疫苗错误信息帖子并应对其负面影响。 |
| [^59] | [CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments.](http://arxiv.org/abs/2304.06848) | 本文提出了一种新的因果关系在线POMDP规划方法CAR-DESPOT，使用因果建模和推理来消除未测量混淆变量引起的错误，并在混杂环境中表现优异。 |
| [^60] | [PIE: Personalized Interest Exploration for Large-Scale Recommender Systems.](http://arxiv.org/abs/2304.06844) | 本文提出了一个用于大规模推荐系统的探索框架，包括用户-创建者探索、在线探索框架和反馈组成机制，以解决推荐系统中流行内容的限制和无法系统探索用户兴趣的挑战，从而提高推荐的整体质量和训练数据的有效性。 |
| [^61] | [Video alignment using unsupervised learning of local and global features.](http://arxiv.org/abs/2304.06841) | 本文提出了一种无需训练的视频对齐方法，利用全局和局部特征将帧转化为时间序列并使用对角化动态时间规整算法进行对齐。 |
| [^62] | [Structured Pruning for Multi-Task Deep Neural Networks.](http://arxiv.org/abs/2304.06840) | 本研究探索了在多任务模型上应用结构化剪枝的有效性，通过实验发现，在参数数量相似的情况下，来自不同剪枝方法的架构在任务性能上没有显着差异，迭代结构剪枝可能不是实现最优结构的最佳方法。 |
| [^63] | [Neural Network Architectures for Optical Channel Nonlinear Compensation in Digital Subcarrier Multiplexing Systems.](http://arxiv.org/abs/2304.06836) | 本研究提出了使用各种人工神经网络（ANN）结构在数字子载波复用（DSCM）光传输系统中模拟和补偿光纤非线性干扰，通过应用全连接网络和模块化结构来完成非线性信道均衡，为未来光学通信解决方案提供了现实的解决方案。 |
| [^64] | [Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective.](http://arxiv.org/abs/2304.06833) | 本文提出，当模型类足够丰富以涵盖真实情况时，非线性问题的“先估计再优化”方法优于集成方法，包括优化间隙的渐进优势的均值，所有其他时刻和整个渐进分布。 |
| [^65] | [Task Adaptive Feature Transformation for One-Shot Learning.](http://arxiv.org/abs/2304.06832) | 该论文提出了一种针对一次性学习的适应任务的特征转换方法，可在低样本情况下改善推理效果，并在多个一次性测试中得到验证。 |
| [^66] | [Ranking from Pairwise Comparisons in General Graphs and Graphs with Locality.](http://arxiv.org/abs/2304.06821) | 本文研究了通用图和具有局部性质的图中的成对比较排序问题。研究表明，最大似然估计（MLE）可以实现符合Cram\'er-Rao下界的逐元估计误差。同时，文章还确定了局部性不会影响的条件，并提出了分治算法以实现类似保障。 |
| [^67] | [Improving Few-Shot Prompts with Relevant Static Analysis Products.](http://arxiv.org/abs/2304.06815) | 本文研究了用相关静态分析产品改善大型语言模型在少样本提示中的表现，探讨如何通过添加显示信息来提取代码中的语义事实。 |
| [^68] | [Unified Out-Of-Distribution Detection: A Model-Specific Perspective.](http://arxiv.org/abs/2304.06813) | 本文提出一种新颖的统一框架，用于将机器学习模型中的外部分布检测扩展到更广泛的范围，该框架旨在检测模型无法正确预测的测试示例，而不是特定的外部分布原因。 |
| [^69] | [Designing Nonlinear Photonic Crystals for High-Dimensional Quantum State Engineering.](http://arxiv.org/abs/2304.06810) | 该论文提出了一种通过设计非线性光子晶体和泵浦光束生成高维量子态的方法，同时基于所提出的物理约束和可微分的方法，理论和实验上演示了如何生成最大纠缠态。这为控制任意量子态提供了新途径，并在全光学相干控制成为可能。 |
| [^70] | [Active Cost-aware Labeling of Streaming Data.](http://arxiv.org/abs/2304.06808) | 本文研究了流式数据中的主动计费标注问题，提出了一种算法，通过选择标记点并维护时间和成本相关阈值，在$T$轮之后实现了$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。 |
| [^71] | [Graph-informed simulation-based inference for models of active matter.](http://arxiv.org/abs/2304.06806) | 本文探索了基于图形信息的方法，证明了它能够优于典型指标，改善活性物质模型的参数推断精度，只需使用少量的系统快照。 |
| [^72] | [Sample Average Approximation for Black-Box VI.](http://arxiv.org/abs/2304.06803) | 该论文提出了一种用于黑盒变分推断的样本平均估计方法，有效地解决了随机梯度上升等问题，实验结果表明其比现有方法更快且性能更佳。 |
| [^73] | [Efficient Sequence Transduction by Jointly Predicting Tokens and Durations.](http://arxiv.org/abs/2304.06795) | 本文提出了一种新型的序列转导架构TDT，它可以联合预测标记和持续时间，从而实现比传统Transducers更高的准确性和显着更快的推理速度。 |
| [^74] | [Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline.](http://arxiv.org/abs/2304.06793) | Speck是一款智能视觉传感器SoC，该芯片集成了基于事件的相机和低功耗的sCNN计算架构，可显著降低单元生产成本。通过优化高度稀疏的计算和最小化延迟，Speck可以处理高速的稀疏数据流。 |
| [^75] | [A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions.](http://arxiv.org/abs/2304.06787) | 本论文提出了第一个多项式时间、纯差分隐私估计器，可以在$\{0,1\}^d$上准确估计二元积分布的均值，达到了最优的样本复杂度。 |
| [^76] | [A Distributionally Robust Approach to Regret Optimal Control using the Wasserstein Distance.](http://arxiv.org/abs/2304.06783) | 提出了一种基于Wasserstein距离的分布鲁棒方法实现后悔最优控制的控制器设计策略。 |
| [^77] | [Semi-Equivariant Conditional Normalizing Flows.](http://arxiv.org/abs/2304.06779) | 本文介绍了一种能够保持刚性运动条件不变性的连续正则化流，并在分子环境中展示了其有效性。 |
| [^78] | [Online Recognition of Incomplete Gesture Data to Interface Collaborative Robots.](http://arxiv.org/abs/2304.06777) | 本文介绍了一个人机交互框架，通过可穿戴传感器捕捉交织的静态手势和动态手势，并使用数据降维技术以获得DG特征。实验结果表明，通过随机森林和人工神经网络，分别可以对24个SG和10个DG进行准确的分类，从而实现在非结构化环境中的手势识别。 |
| [^79] | [Improving Gradient Methods via Coordinate Transformations: Applications to Quantum Machine Learning.](http://arxiv.org/abs/2304.06768) | 本文介绍了一种通过坐标变换来加速梯度优化算法、改善荒原高原和局部最小值影响的通用策略，有效提高了多种量子机器学习算法的性能。 |
| [^80] | [RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment.](http://arxiv.org/abs/2304.06767) | RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。 |
| [^81] | [End-to-end codesign of Hessian-aware quantized neural networks for FPGAs and ASICs.](http://arxiv.org/abs/2304.06745) | 该论文提出了一个端到端的工作流程，用于训练和实现协同设计的神经网络（NNs），以高效地在FPGAs和ASICs上实现NNs，并在粒子物理应用程序中得到了应用。 |
| [^82] | [A Study of Biologically Plausible Neural Network: The Role and Interactions of Brain-Inspired Mechanisms in Continual Learning.](http://arxiv.org/abs/2304.06738) | 本论文提出了一种基于脑启发机制的生物可行的神经网络框架，并研究了其中包括稀疏不重叠表示、赫布学习、突触巩固和重播等机制在持续学习中的相互作用，实现了基于序列的持续学习基准测试的最先进性能。 |
| [^83] | [Near-Optimal Degree Testing for Bayes Nets.](http://arxiv.org/abs/2304.06733) | 本文提出了一个近似最优入度检测算法，并给出了样本复杂度的上界，同时开发了新的算法进行“近似正确性”的贝叶斯网络学习和在 $\chi^2$ 散度下的高概率学习。 |
| [^84] | [PCD2Vec: A Poisson Correction Distance-Based Approach for Viral Host Classification.](http://arxiv.org/abs/2304.06731) | 本文提出了一种新的病毒宿主分类方法，通过分析刺突蛋白序列预测冠状病毒的宿主特异性。该方法使用泊松校正距离生成距离矩阵，然后对刺突蛋白序列进行聚类。 |
| [^85] | [Meta-Learned Models of Cognition.](http://arxiv.org/abs/2304.06729) | 元学习模型是构建人类认知模型的有前途的工具，可以用于构建贝叶斯最优学习算法，并且比传统的贝叶斯方法有几个优势。 |
| [^86] | [GradMDM: Adversarial Attack on Dynamic Networks.](http://arxiv.org/abs/2304.06724) | 本文研究了针对动态神经网络的一种新型能量导向攻击算法GradMDM，其可以有效地增加计算复杂度同时减少扰动的感知度。 |
| [^87] | [Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation.](http://arxiv.org/abs/2304.06671) | 本文提出了布局引导下图像生成的诊断基准LayoutBench，对数量、位置、大小和形状四种空间控制技能进行了研究，发现好的ID布局控制在任意布局的野外环境下可能不具有良好的推广性。接着，我们提出了一种新的基准方法IterInpaint通过修复逐步生成前景和背景区域，显现出在OOD布局方面更强的通用性。 |
| [^88] | [G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection.](http://arxiv.org/abs/2304.06653) | G2T是一种基于预训练语言模型和社区检测的主题建模框架，自动评估表明，G2T在多个数据集上均与当前最先进的方法相比表现更好。 |
| [^89] | [MProtoNet: A Case-Based Interpretable Model for Brain Tumor Classification with 3D Multi-parametric Magnetic Resonance Imaging.](http://arxiv.org/abs/2304.06258) | 本论文提出了一种基于案例的可解释性模型MProtoNet，用于带有3D多参数磁共振成像的脑肿瘤分类，通过引入新型的注意模块，比四个状态-of-the-art深度学习模型在解释性和分类性能上有所提高。 |
| [^90] | [Maximum-likelihood Estimators in Physics-Informed Neural Networks for High-dimensional Inverse Problems.](http://arxiv.org/abs/2304.05991) | 本论文提出了在PINNs中使用MLE的方法，消除了超参数调整。通过ODE耦合矩阵的SVD分解降维，增加了PINNs预测的稳定性和泛化能力。 |
| [^91] | [A Tale of Sampling and Estimation in Discounted Reinforcement Learning.](http://arxiv.org/abs/2304.05073) | 本文通过最小值上界提出了折扣均值估计问题的估计误差与马尔可夫过程混合特性和折扣因子之间的明确联系，并对一组显著估计器及其对应的采样程序进行了统计分析。 |
| [^92] | [Model sparsification can simplify machine unlearning.](http://arxiv.org/abs/2304.04934) | 本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。 |
| [^93] | [Deploying Machine Learning Models to Ahead-of-Time Runtime on Edge Using MicroTVM.](http://arxiv.org/abs/2304.04842) | 本文介绍了使用MicroTVM在边缘设备上部署机器学习模型的方法，可以将预训练模型解析为后端的C源代码库，并使用自动生成的Ahead-of-Time C运行时在ARM Cortex M4F核心上进行手势识别实验。 |
| [^94] | [REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network.](http://arxiv.org/abs/2304.03997) | 本文提出了一种基于长短期记忆网络的智能电网可再生能源需求预测模型REDf，可以提供准确的能量需求预测，改善可再生能源的集成，实验结果表明其准确度优于其他模型。 |
| [^95] | [InstructBio: A Large-scale Semi-supervised Learning Paradigm for Biochemical Problems.](http://arxiv.org/abs/2304.03906) | InstructBio是一种针对生物化学问题的大规模半监督学习算法，引入教练模型提供有效的置信度比率来指导目标模型对不同数据点给予明显关注，避免依赖有限的标记数据和不正确的伪注释，提高了分子模型的泛化能力。 |
| [^96] | [Personalized Federated Learning with Local Attention.](http://arxiv.org/abs/2304.01783) | 本文提出了一个名为pFedLA的算法，通过将注意力机制并入个性化模型来解决联邦学习中客户端数据异质性的问题，并在实验中表现出了优异的表现，尤其是在缓解特征漂移问题方面。 |
| [^97] | [Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization.](http://arxiv.org/abs/2303.17354) | 该论文提出一种基于Transformer骨干网络的渐进式自监督学习方法，可用于图像异常检测和定位，其中第一阶段使用MAE模型进行正常图像的训练，第二阶段使用像素级数据增强技术来生成损坏的正常图像，最终通过像素重建误差矩阵和像素异常概率矩阵综合得到一个异常得分矩阵。 |
| [^98] | [Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task.](http://arxiv.org/abs/2303.16210) | 本研究探究了在多输出回归任务中应用深度集合量化校准不确定性的方法，提出了该方法的改进框架，其在回归准确性、不确定性估计可靠性和训练效率方面具有优越表现。 |
| [^99] | [Federated Learning on Heterogenous Data using Chest CT.](http://arxiv.org/abs/2303.13567) | 本研究使用联邦学习技术在全球21家医院的10,000多位COVID患者的胸部CT扫描图像数据集上进行了研究，提出了三种联邦学习策略，并提出了结合合成生成数据的联邦学习策略，为医学AI在异构数据上的应用提供了新的解决方案。 |
| [^100] | [CoLT5: Faster Long-Range Transformers with Conditional Computation.](http://arxiv.org/abs/2303.09752) | CoLT5是一种基于条件计算的Transformer模型，通过优先处理重要标记来加速长距离输入的处理。CoLT5在SCROLLS基准测试上表现最好，并能够有效地处理长达64k输入长度。 |
| [^101] | [n-Step Temporal Difference Learning with Optimal n.](http://arxiv.org/abs/2303.07068) | 本文提出了使用SPSA算法求解n步时序差分学习中的最优n值的算法SDPSA，并证明了其收敛性和有效性。 |
| [^102] | [MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder.](http://arxiv.org/abs/2303.04651) | MCTS-GEB是一个通用的重写系统，使用强化学习和蒙特卡洛树搜索来构建最优的E图，有效消除了E图构建中的顺序问题，并在评估中表现出很好的性能。 |
| [^103] | [Learning to Backdoor Federated Learning.](http://arxiv.org/abs/2303.03320) | 本文提出了一种基于强化学习的后门攻击框架，能够在联邦学习中成功设置后门，在现有防御措施下也具有强大的攻击性能和耐用性。 |
| [^104] | [WISK: A Workload-aware Learned Index for Spatial Keyword Queries.](http://arxiv.org/abs/2302.14287) | WISK是一种用于空间关键字查询的学习索引，它可以利用已知的查询分布来自适应地优化查询成本，并在考虑了空间属性和文本信息后进行学习。在实验中，WISK表现出比现有的索引更好的查询延迟和竞争性的索引大小。 |
| [^105] | [Inseq: An Interpretability Toolkit for Sequence Generation Models.](http://arxiv.org/abs/2302.13942) | 本文介绍了Inseq，这是一个Python工具包，旨在推广可解释性序列生成模型的分析。它为常见的解码器和编码器-解码器Transformers架构提供了提取模型内部信息和特征重要性得分的直观优化方法。作者还在机器翻译模型和GPT-2中展示了Inseq的潜力，证明其有助于推动可解释性自然语言生成的未来发展。 |
| [^106] | [TwERC: High Performance Ensembled Candidate Generation for Ads Recommendation at Twitter.](http://arxiv.org/abs/2302.13915) | 本文介绍了TwERC方法，用于解决Twitter广告推荐的候选生成问题。该方法结合了实时轻型排名器和数据来源策略，成功提高了推荐系统的性能和收入。其中，基于相互作用图和排名分数缓存的两种策略互补应用，分别实现了4.08%和1.38%的增益。 |
| [^107] | [Alloprof: a new French question-answer education dataset and its use in an information retrieval case study.](http://arxiv.org/abs/2302.07738) | 这个论文介绍了一个新的阿洛普夫法语问答数据集，收集了来自10,368名学生的29,349个问题和解释，并展示了在信息检索任务中使用该数据集的案例研究。 |
| [^108] | [Is Distance Matrix Enough for Geometric Deep Learning?.](http://arxiv.org/abs/2302.05743) | 本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。 |
| [^109] | [Learning Near-Optimal Intrusion Responses Against Dynamic Attackers.](http://arxiv.org/abs/2301.06085) | 本文通过博弈论建模，开发一种防御策略，以应对自主适应攻击者，在阈值特性证明的基础上，使用 Threshold Fictitious Self-Play (T-FP) 算法学习纳什均衡，实现近似最优解。 |
| [^110] | [ViTs for SITS: Vision Transformers for Satellite Image Time Series.](http://arxiv.org/abs/2301.04944) | 本文介绍了一种全自注意力模型TSViT，用于处理通用的卫星图像时序(SITS)，通过分解的时空编码器处理非重叠的块，提出了两种新的机制以增强模型的判别能力，最终在SITS语义分割和分类中取得了显著的优势，达到了最先进水平。 |
| [^111] | [Bayesian Weapon System Reliability Modeling with Cox-Weibull Neural Network.](http://arxiv.org/abs/2301.01850) | 该论文提出了一种带有Cox-Weibull神经网络的贝叶斯武器系统可靠性建模方法，可以通过整合武器系统特征来改善预测性维修，相较于传统模型的表现更优。 |
| [^112] | [Mixed moving average field guided learning for spatio-temporal data.](http://arxiv.org/abs/2301.00736) | 本论文提出了一种理论引导机器学习方法，采用广义贝叶斯算法进行混合移动平均场引导的时空数据建模，可以进行因果未来预测。 |
| [^113] | [Berlin V2X: A Machine Learning Dataset from Multiple Vehicles and Radio Access Technologies.](http://arxiv.org/abs/2212.10343) | 这篇论文介绍了一个详细的测量活动，提供了一个多车辆和无线电访问技术的机器学习数据集，该数据集通过各种V2X研究为车辆和工业通信领域开辟了新的可能性。 |
| [^114] | [AUC Maximization for Low-Resource Named Entity Recognition.](http://arxiv.org/abs/2212.04800) | 本文提出了在低资源命名实体识别中使用AUC最大化的方法，通过结合两个最大化AUC分数的二进制分类器，在低资源NER设置下实现了显着的性能提高，优于传统的损失函数。 |
| [^115] | [Low Variance Off-policy Evaluation with State-based Importance Sampling.](http://arxiv.org/abs/2212.03932) | 本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。 |
| [^116] | [FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee.](http://arxiv.org/abs/2211.15072) | 本研究提出了FaiREE算法，它是一种可满足群体公平性约束的公平分类算法，并且具有有限样本和无分布理论保证。在实验中表现优异。 |
| [^117] | [Monitoring machine learning (ML)-based risk prediction algorithms in the presence of confounding medical interventions.](http://arxiv.org/abs/2211.09781) | 在医疗保健中，通过监测条件性表现，可以有效地监测考虑混淆医疗干预时基于机器学习的风险预测算法。 |
| [^118] | [A study of uncertainty quantification in overparametrized high-dimensional models.](http://arxiv.org/abs/2210.12760) | 本论文研究了过度参数化高维模型中的不确定性问题，探讨了几种方法，比较了校准和分类准确性之间的权衡。结果发现最佳正则化估计量的校准曲线具有双重下降行为，与经验贝叶斯方法形成对比。 |
| [^119] | [Compressing multidimensional weather and climate data into neural networks.](http://arxiv.org/abs/2210.12538) | 这篇论文提出了一种新方法，将多维天气和气候数据通过训练神经网络进行压缩。该方法在保留重要气象结构和不引入伪像的情况下，可将数据压缩三个数量级，为高分辨率气候数据民主化访问和多个新的研究方向提供了可能。 |
| [^120] | [Bottleneck Analysis of Dynamic Graph Neural Network Inference on CPU and GPU.](http://arxiv.org/abs/2210.03900) | 本文分析了8种不同特征的DGNN在CPU和GPU上的性能瓶颈，并提出了未来优化的机会。 |
| [^121] | [TPGNN: Learning High-order Information in Dynamic Graphs via Temporal Propagation.](http://arxiv.org/abs/2210.01171) | TPGNN 是一种基于时间传播的图卷积神经网络，用于学习动态图中的高阶信息。对节点分类和链接预测任务上的实验表明，TPGNN明显优于现有方法。 |
| [^122] | [Sparsity-Constrained Optimal Transport.](http://arxiv.org/abs/2209.15466) | 这篇论文提出了一种新的带有输运计划显式基数约束的 OT 方法，以确保每个输入令牌都与少量专家匹配，从而提高模型的可解释性。 |
| [^123] | [Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows.](http://arxiv.org/abs/2209.10873) | 本文提出了一种将正规化流转换为 OT 效率更高的蒙热映射的方法，通过学习源分布的重新排列实现，同时通过欧拉方程约束导致估计的蒙热映射路径位于保持体积的微分同胚空间中的测地线上。 |
| [^124] | [Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms.](http://arxiv.org/abs/2207.09572) | 本文研究多元概率预测模型的对抗攻击威胁和有效的防御机制，发现稀疏修改其他时间序列的观测对目标时间序列的预测有负面影响，并开发了两种防御策略，实验验证了其有效性。 |
| [^125] | [Towards a More Rigorous Science of Blindspot Discovery in Image Models.](http://arxiv.org/abs/2207.04104) | 本文介绍了一种新的BDM（盲点发现方法）评估框架SpotCheck和一个使用2D图像表示的BDMPlaneSpot。实验结果给出影响BDM性能的因素，证明PlaneSpot与现有的BDM相竞争，在许多情况下表现更好。 |
| [^126] | [Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse.](http://arxiv.org/abs/2206.13714) | 研究提出了一种广义策略提升算法，结合了在线方法的策略提升保证和离线策略算法通过样本重用有效利用数据的效率。 |
| [^127] | [Stochastic Gradient Methods with Compressed Communication for Decentralized Saddle Point Problems.](http://arxiv.org/abs/2205.14452) | 这篇论文提出了两种基于压缩的随机梯度算法，用于解决非光滑强凸-强凹鞍点问题的分散求解，并提供了理论保证，包括梯度计算和通信复杂度的限制。 |
| [^128] | [A Blessing of Dimensionality in Membership Inference through Regularization.](http://arxiv.org/abs/2205.14055) | 研究探讨了过度参数化对成员推理攻击易受攻击性的影响，发现适当的正则化可以在增加模型参数数量的同时提高隐私和性能，消除了隐私与效用之间的权衡问题。 |
| [^129] | [Making SGD Parameter-Free.](http://arxiv.org/abs/2205.02160) | 该论文提出了一种无参数化的随机梯度下降法，能够在一定程度上适应未知梯度范数、平滑性和强凸性，并在收敛速度方面具有高概率保证。 |
| [^130] | [Sources of Irreproducibility in Machine Learning: A Review.](http://arxiv.org/abs/2204.07610) | 本文提出了一个理解机器学习中不可重复性来源的框架，并指出实验设置问题和未能正确考虑数据变异是机器学习研究中最常见的不可重复性来源。 |
| [^131] | [Real-time Neural-MPC: Deep Learning Model Predictive Control for Quadrotors and Agile Robotic Platforms.](http://arxiv.org/abs/2203.07747) | 本论文介绍了一种实时神经网络模型预测控制系统，在模型预测控制流程中高效集成复杂神经网络架构作为动力学模型，实现对四旋翼等灵活机器人平台高性能的控制。 |
| [^132] | [Reproducibility in Learning.](http://arxiv.org/abs/2201.08430) | 该论文介绍了可重复性学习算法的概念，这种算法能够抵御样本变异，在保证高准确度的同时，能够以高概率返回相同的输出。同时，该论文证明了可重复性并不与学习效果相悖，设计可重复性算法可以推动我们开发更高效、更稳健的数据分析和建模方法。 |
| [^133] | [Concise Logarithmic Loss Function for Robust Training of Anomaly Detection Model.](http://arxiv.org/abs/2201.05748) | 本研究提出了一种新的对数均方误差（LMSE）损失函数，相比于现有的均方误差（MSE）函数，在神经网络训练中更稳定、具有更强的收敛性和更好的异常检测性能。 |
| [^134] | [Problem-dependent attention and effort in neural networks with applications to image resolution and model selection.](http://arxiv.org/abs/2201.01415) | 本文介绍了两种新的基于集成的方法以减少数据和计算成本。第一种方法通过降低数据使用量来减少成本，第二种方法通过使用较简单的模型来降低计算成本。本文考虑的最佳分类器在所有数据集上的分类精度不降低超过5%。 |
| [^135] | [Physics-Informed Neural Operator for Learning Partial Differential Equations.](http://arxiv.org/abs/2111.03794) | 本文提出了一种名为PINO的方法，它能够通过同时利用数据和物理约束条件来学习偏微分方程的解算子，克服了纯数据驱动和基于物理的方法的局限性，并且可以在不同分辨率上合并数据和约束条件。 |
| [^136] | [PAC-Bayesian Learning of Aggregated Binary Activated Neural Networks with Probabilities over Representations.](http://arxiv.org/abs/2110.15137) | 该研究使用PAC-Bayesian框架为具有正态分布权重的二值激活神经网络的聚合提供了紧凑的泛化界限和学习过程，这导致了一种奇特的界限最小化学习算法。 |
| [^137] | [Don't Knock! Rowhammer at the Backdoor of DNN Models.](http://arxiv.org/abs/2110.07683) | 本研究首次提出了一种基于Rowhammer方法的端到端后门注入攻击方法，可在实际硬件上攻击分类器模型。通过新颖的优化角度解决了硬件实现中的实际问题。 |
| [^138] | [Graph-Based Machine Learning Improves Just-in-Time Defect Prediction.](http://arxiv.org/abs/2110.05371) | 该论文使用基于图的机器学习技术，构建了一个由开发人员和源文件组成的贡献图，利用这个图提取的特征，改进了 JIT 缺陷预测，比传统的机器学习方法更好地预测了易出现缺陷的更改。 |
| [^139] | [Variational Diffusion Models.](http://arxiv.org/abs/2107.00630) | 该论文提出了一族基于扩散的生成模型，通过对噪声时间表的有效优化，这些模型在图像密度估计基准测试中获得了最先进的可能性和视觉质量。 |
| [^140] | [Hyperplane Arrangements of Trained ConvNets Are Biased.](http://arxiv.org/abs/2003.07797) | 本文通过研究训练好的 ConvNets 的超平面配置，发现其存在统计性偏差，而且这种偏差与验证性能息息相关。 |

# 详细

[^1]: 交叉熵损失函数：理论分析与应用

    Cross-Entropy Loss Functions: Theoretical Analysis and Applications. (arXiv:2304.07288v1 [cs.LG])

    [http://arxiv.org/abs/2304.07288](http://arxiv.org/abs/2304.07288)

    本文对交叉熵、广义交叉熵、均方误差等一大类损失函数进行了理论分析，并提出了具有优势的双交叉熵损失函数，特别适用于存在标签噪声或类别不平衡的情况。

    

    交叉熵是广泛应用的损失函数。当使用softmax函数时，它与神经网络输出应用于逻辑回归损失函数相符。但是，使用交叉熵作为代理损失函数时，我们能依靠什么保证呢？我们提出了对广泛的损失函数家族进行理论分析，包括交叉熵（或逻辑损失）、广义交叉熵、均方误差和其他交叉熵类函数。我们给出了这些损失函数的第一个$H$-连续性界限。这些都是非渐进保证，以估计代理损失的估计误差为上限，用于特定的假设集$H$。我们进一步展示了这些边界的紧密程度。这些边界取决于称为可最小化间隙的量，这些间隙只取决于损失函数和假设集。为了使它们更具体化，我们对复杂和损失函数的这些间隙进行了具体分析。我们还引入了一种新的损失函数，称为双交叉熵损失，它基于两个交叉熵损失的组合。我们表明，它可以优于标准交叉熵损失，特别是在存在标签噪声或类别不平衡的情况下。

    Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of losses, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other loss cross-entropy-like functions. We give the first $H$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set $H$ used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps, which only depend on the loss function and the hypothesis set. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduc
    
[^2]: 使用基于奖励塑形的模仿学习合成类似人类数据，来解决连续决策问题

    Synthetically Generating Human-like Data for Sequential Decision Making Tasks via Reward-Shaped Imitation Learning. (arXiv:2304.07280v1 [cs.LG])

    [http://arxiv.org/abs/2304.07280](http://arxiv.org/abs/2304.07280)

    本文提出了一种从少量人类数据开始，通过奖励塑形和模仿学习算法合成类似人类决策的数据的方法，并通过应用到电脑游戏的连续决策任务中证明其有效性。

    

    本文针对在与AI系统进行交互的情况下，例如与电脑游戏交互，如何合成类似于人类决策的数据进行研究。本文提出一种新的算法，能够从少量的人类决策数据入手，将奖励塑形的概念与模仿学习算法相结合，生成合成的、类似于人类决策的数据。作者使用这种技术在一个小型的电脑游戏中完成了三个不同难度的连续决策任务，并对实验结果进行了多方面的经验性和统计性分析，证明了合成的数据可以代替人类数据，实现与人类类似的决策和任务表现。

    We consider the problem of synthetically generating data that can closely resemble human decisions made in the context of an interactive human-AI system like a computer game. We propose a novel algorithm that can generate synthetic, human-like, decision making data while starting from a very small set of decision making data collected from humans. Our proposed algorithm integrates the concept of reward shaping with an imitation learning algorithm to generate the synthetic data. We have validated our synthetic data generation technique by using the synthetically generated data as a surrogate for human interaction data to solve three sequential decision making tasks of increasing complexity within a small computer game-like setup. Different empirical and statistical analyses of our results show that the synthetically generated data can substitute the human data and perform the game-playing tasks almost indistinguishably, with very low divergence, from a human performing the same tasks.
    
[^3]: 强化学习中的极小极大最优无关奖励探索

    Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning. (arXiv:2304.07278v1 [cs.LG])

    [http://arxiv.org/abs/2304.07278](http://arxiv.org/abs/2304.07278)

    本文研究了强化学习中的无关奖励探索并设计了一种算法，在保证样本收集数量满足多项式级别的情况下，能够发现所有给定奖励函数的最小值，实现了可证明的极小极大最优探索方案。

    

    本文研究了强化学习中的无关奖励探索，设计了一种算法来改进现有技术。研究了具有S个状态，A个动作和有限时间水平H的非平稳马尔科夫决策过程，并收集了一定数量的无引导奖励信息的样本集，在保证收集的数量满足多项式级别时，算法能够发现所有这些奖励函数的最小值，实现了可证明的极小极大最优探索方案。

    This paper studies reward-agnostic exploration in reinforcement learning (RL) -- a scenario where the learner is unware of the reward functions during the exploration stage -- and designs an algorithm that improves over the state of the art. More precisely, consider a finite-horizon non-stationary Markov decision process with $S$ states, $A$ actions, and horizon length $H$, and suppose that there are no more than a polynomial number of given reward functions of interest. By collecting an order of \begin{align*}  \frac{SAH^3}{\varepsilon^2} \text{ sample episodes (up to log factor)} \end{align*} without guidance of the reward information, our algorithm is able to find $\varepsilon$-optimal policies for all these reward functions, provided that $\varepsilon$ is sufficiently small. This forms the first reward-agnostic exploration scheme in this context that achieves provable minimax optimality. Furthermore, once the sample size exceeds $\frac{S^2AH^3}{\varepsilon^2}$ episodes (up to log f
    
[^4]: 冠状动脉CT血管造影中的CAD-RADS评分与多轴视觉转换器的深度学习流程

    CAD-RADS scoring of coronary CT angiography with Multi-Axis Vision Transformer: a clinically-inspired deep learning pipeline. (arXiv:2304.07277v1 [eess.IV])

    [http://arxiv.org/abs/2304.07277](http://arxiv.org/abs/2304.07277)

    本文提出了一个完全自动化且可视化解释的深度学习流程，用作冠状动脉疾病筛查的决策支持系统。该流程使用经过微调的多轴视觉转换器架构对冠状动脉的多平面投影进行预处理和分类，可根据常用CAD-RADS阈值分类患者到不同程度的狭窄亚组中。

    

    冠状动脉计算机断层扫描血管造影（CCTA）是评估冠状动脉疾病（CAD）严重程度和范围的标准非创伤性成像技术。然而，根据CAD-Reporting和数据系统（CAD-RADS）评分对每个患者的CCTA进行手动分级非常耗时且操作者依赖性强，特别是对于边缘病例。本文提出了一个完全自动化且可视化解释的深度学习流程，用作CAD筛查过程的决策支持系统。该流程执行两个分类任务：首先，识别需要进一步临床检查的患者；其次，根据常用的CAD-RADS阈值将患者分类到不同程度的狭窄亚组中。该流程对原始CCTA中的冠状动脉的多平面投影进行预处理和分类，使用经过微调的多轴视觉转换器架构进行分类。旨在模拟当前的医学实践流程，缩短临床评估时间，降低操作者依赖性。

    The standard non-invasive imaging technique used to assess the severity and extent of Coronary Artery Disease (CAD) is Coronary Computed Tomography Angiography (CCTA). However, manual grading of each patient's CCTA according to the CAD-Reporting and Data System (CAD-RADS) scoring is time-consuming and operator-dependent, especially in borderline cases. This work proposes a fully automated, and visually explainable, deep learning pipeline to be used as a decision support system for the CAD screening procedure. The pipeline performs two classification tasks: firstly, identifying patients who require further clinical investigations and secondly, classifying patients into subgroups based on the degree of stenosis, according to commonly used CAD-RADS thresholds. The pipeline pre-processes multiplanar projections of the coronary arteries, extracted from the original CCTAs, and classifies them using a fine-tuned Multi-Axis Vision Transformer architecture. With the aim of emulating the current
    
[^5]: 直接优化IoU用于边界框定位

    Directly Optimizing IoU for Bounding Box Localization. (arXiv:2304.07256v1 [cs.CV])

    [http://arxiv.org/abs/2304.07256](http://arxiv.org/abs/2304.07256)

    本文提出了一种新的损失函数——平滑IoU损失，可以直接最大化ground-truth bounding box和预测bounding box之间的重叠部分，从而提高目标检测的准确率。

    

    最近几年，采用卷积神经网络（CNN）的引进使得目标检测得到了显著的进步。目标检测是一个多任务学习问题，需要正确地标识图像中的对象位置和类别。本文提出了一种新的损失函数——平滑IoU损失，可以直接最大化ground-truth bounding box和预测bounding box之间的重叠部分，而不需要使用近似方法。

    Object detection has seen remarkable progress in recent years with the introduction of Convolutional Neural Networks (CNN). Object detection is a multi-task learning problem where both the position of the objects in the images as well as their classes needs to be correctly identified. The idea here is to maximize the overlap between the ground-truth bounding boxes and the predictions i.e. the Intersection over Union (IoU). In the scope of work seen currently in this domain, IoU is approximated by using the Huber loss as a proxy but this indirect method does not leverage the IoU information and treats the bounding box as four independent, unrelated terms of regression. This is not true for a bounding box where the four coordinates are highly correlated and hold a semantic meaning when taken together. The direct optimization of the IoU is not possible due to its non-convex and non-differentiable nature. In this paper, we have formulated a novel loss namely, the Smooth IoU, which directly
    
[^6]: R-mAtrIx Net：一种生成量子可积自旋链 R-矩阵的神经网络

    The R-mAtrIx Net. (arXiv:2304.07247v1 [hep-th])

    [http://arxiv.org/abs/2304.07247](http://arxiv.org/abs/2304.07247)

    本文提出了一种可以生成量子可积自旋链 R-矩阵的神经网络，通过限制神经网络学习的方式，可以搜索可积哈密顿量和相应的 R-矩阵，并探索可积自旋链的家族。该网络已在二维差分型自旋链上得到应用，并被证明是有用的。

    

    本文提出了一种新颖的神经网络架构，可以：i）输出给定量子可积自旋链的R矩阵，ii）在满足一定的对称性或其他限制的假设下搜索可积哈密顿量和相应的R矩阵，iii）探索围绕已学会的模型的哈密顿量空间，并重构它们所属的可积自旋链家族。神经网络训练通过最小化编码杨-巴克斯特方程、正则性和其他特定于模型的限制例如厄米性的损失函数完成。全纯性是通过激活函数的选择来实现的。我们展示了我们的神经网络在二维差分型自旋链上的工作。特别地，我们重构了所有14个类别的R矩阵。我们还展示了它作为“探索器”的有用性，扫描特定的哈密顿量子空间并在聚类后识别可积类。最后的策略可以在未来用于建立更复杂的模型。

    We provide a novel Neural Network architecture that can: i) output R-matrix for a given quantum integrable spin chain, ii) search for an integrable Hamiltonian and the corresponding R-matrix under assumptions of certain symmetries or other restrictions, iii) explore the space of Hamiltonians around already learned models and reconstruct the family of integrable spin chains which they belong to. The neural network training is done by minimizing loss functions encoding Yang-Baxter equation, regularity and other model-specific restrictions such as hermiticity. Holomorphy is implemented via the choice of activation functions. We demonstrate the work of our Neural Network on the two-dimensional spin chains of difference form. In particular, we reconstruct the R-matrices for all 14 classes. We also demonstrate its utility as an \textit{Explorer}, scanning a certain subspace of Hamiltonians and identifying integrable classes after clusterisation. The last strategy can be used in future to car
    
[^7]: 基于机器学习的柔性盘元件的多目标设计探索

    Machine Learning-Based Multi-Objective Design Exploration Of Flexible Disc Elements. (arXiv:2304.07245v1 [cs.NE])

    [http://arxiv.org/abs/2304.07245](http://arxiv.org/abs/2304.07245)

    本文采用基于机器学习的方法，结合人工神经网络和遗传算法，对盘式联轴器中的柔性盘元件进行改进设计，降低其质量和应力，而不降低扭矩传递和不对齐能力。

    

    设计探索是工程设计过程中的一个重要步骤。它涉及寻找满足指定设计标准和完成预定义目标的设计方案。近年来，机器学习方法在工程设计问题中得到了广泛应用。本文展示了将人工神经网络（ANN）架构应用于工程设计问题中，以探索和识别改进的设计方案。本研究的案例问题是柔性盘元件的设计，该元件用于盘式联轴器中。我们需要通过降低质量和应力而不降低扭矩传递和不对齐能力来改进盘元件的设计。为了实现这个目标，我们在设计探索步骤中采用了ANN和遗传算法，以识别符合指定标准（扭矩和不对齐）且具有最小质量和应力的设计方案。结果与优化结果相当。

    Design exploration is an important step in the engineering design process. This involves the search for design/s that meet the specified design criteria and accomplishes the predefined objective/s. In recent years, machine learning-based approaches have been widely used in engineering design problems. This paper showcases Artificial Neural Network (ANN) architecture applied to an engineering design problem to explore and identify improved design solutions. The case problem of this study is the design of flexible disc elements used in disc couplings. We are required to improve the design of the disc elements by lowering the mass and stress without lowering the torque transmission and misalignment capability. To accomplish this objective, we employ ANN coupled with genetic algorithm in the design exploration step to identify designs that meet the specified criteria (torque and misalignment) while having minimum mass and stress. The results are comparable to the optimized results obtained
    
[^8]: 稀疏性可提高神经网络的隐私性

    Sparsity in neural networks can increase their privacy. (arXiv:2304.07234v1 [cs.CR])

    [http://arxiv.org/abs/2304.07234](http://arxiv.org/abs/2304.07234)

    神经网络的稀疏性能提高隐私性并保持性能表现

    

    本文研究了稀疏性如何使神经网络对成员推断攻击更加鲁棒。实证结果表明，稀疏性能够提高网络的隐私性，同时保持相应任务的性能表现。这项实证研究完善了并扩展了现有文献的内容。

    This article measures how sparsity can make neural networks more robust to membership inference attacks. The obtained empirical results show that sparsity improves the privacy of the network, while preserving comparable performances on the task at hand. This empirical study completes and extends existing literature.
    
[^9]: 基于行为相似性的图注意力网络的网络入侵检测方法（arXiv：2304.07226v1 [cs.CR]）

    BS-GAT Behavior Similarity Based Graph Attention Network for Network Intrusion Detection. (arXiv:2304.07226v1 [cs.CR])

    [http://arxiv.org/abs/2304.07226](http://arxiv.org/abs/2304.07226)

    本文提出了一种基于行为相似性的图神经网络算法（BS-GAT），利用图注意力网络来学习构建图的表示，实现网络入侵检测，该方法优于现有的最先进方法。

    

    随着物联网（IoT）的发展，网络入侵检测变得越来越复杂和广泛。研究智能、自动化和稳健的网络入侵检测方法至关重要。基于图神经网络的网络入侵检测方法已被提出。然而，由于现有方法的图构建方法并不完全适应实际网络入侵数据集的特征，因此仍需要进一步研究。为了解决上述问题，本文提出了一种基于行为相似性的图神经网络算法（BS-GAT），利用图注意力网络。首先，通过分析实际数据集的特征，开发了一种新的基于行为相似性的图构建方法。以数据流为节点，在图中使用节点的行为规则作为边缘，构建一个邻居节点数量相对平均的图。然后，所提出的BS-GAT算法利用图注意力网络来学习构建图的表示，利用行为相似性捕捉网络流量的判别特征。在三个基准数据集上的实验结果表明，该方法优于现有的最先进方法。

    With the development of the Internet of Things (IoT), network intrusion detection is becoming more complex and extensive. It is essential to investigate an intelligent, automated, and robust network intrusion detection method. Graph neural networks based network intrusion detection methods have been proposed. However, it still needs further studies because the graph construction method of the existing methods does not fully adapt to the characteristics of the practical network intrusion datasets. To address the above issue, this paper proposes a graph neural network algorithm based on behavior similarity (BS-GAT) using graph attention network. First, a novel graph construction method is developed using the behavior similarity by analyzing the characteristics of the practical datasets. The data flows are treated as nodes in the graph, and the behavior rules of nodes are used as edges in the graph, constructing a graph with a relatively uniform number of neighbors for each node. Then, th
    
[^10]: 具有自监督表示学习的模型预测控制

    Model Predictive Control with Self-supervised Representation Learning. (arXiv:2304.07219v1 [cs.LG])

    [http://arxiv.org/abs/2304.07219](http://arxiv.org/abs/2304.07219)

    该论文提出了在TD-MPC框架内使用重构函数进行自监督表示学习的方法，在机器人控制任务中实现更好的性能表现。

    

    在过去的几年中，我们没有看到模型无关或模型基础学习方法有任何重大进展，使得其中一个相对于另一个过时。在大多数情况下，所使用的技术严重依赖于用例场景或其他属性，例如环境。两种方法都有自己的优点，例如样本效率或计算效率。然而，当将两种方法结合起来时，可以结合各自的优点，从而实现更好的性能。TD-MPC框架就是这种方法的一个例子。一方面，结合模型预测控制的世界模型用于获得良好的值函数初始估计。另一方面，Q函数用于提供良好的长期估计。与MuZero等算法类似，使用潜在状态表示，其中仅对任务相关信息进行编码以减少复杂性。本文提出了在TD-MPC框架内使用重构函数进行自监督表示学习。这可以创建更具信息性和鲁棒性的潜在状态表示，从而提高了一系列机器人控制任务的性能。

    Over the last few years, we have not seen any major developments in model-free or model-based learning methods that would make one obsolete relative to the other. In most cases, the used technique is heavily dependent on the use case scenario or other attributes, e.g. the environment. Both approaches have their own advantages, for example, sample efficiency or computational efficiency. However, when combining the two, the advantages of each can be combined and hence achieve better performance. The TD-MPC framework is an example of this approach. On the one hand, a world model in combination with model predictive control is used to get a good initial estimate of the value function. On the other hand, a Q function is used to provide a good long-term estimate. Similar to algorithms like MuZero a latent state representation is used, where only task-relevant information is encoded to reduce the complexity. In this paper, we propose the use of a reconstruction function within the TD-MPC fram
    
[^11]: 利用自监督学习和视觉转换器制作亚米级冠层高度图

    Sub-meter resolution canopy height maps using self-supervised learning and a vision transformer trained on Aerial and GEDI Lidar. (arXiv:2304.07213v1 [cs.CV])

    [http://arxiv.org/abs/2304.07213](http://arxiv.org/abs/2304.07213)

    本文使用自监督学习和视觉转换器方法，制作了亚米级冠层高度图，可用于细粒度的植被结构监测，为碳通量评估和土地利用管理提供宝贵信息。

    

    植被结构的映射对于理解全球碳循环和监测基于自然的气候适应和减缓方法至关重要。本文利用自监督学习和视觉转换器方法，使用航空和GEDI激光遥感数据制作了亚米级冠层高度图。该方法在两个基准数据集上取得了最先进的结果，并具有更高的精度和空间分辨率。制作的冠层高度图可以实现细粒度的植被结构监测，为碳通量评估和土地利用管理提供宝贵信息。

    Vegetation structure mapping is critical for understanding the global carbon cycle and monitoring nature-based approaches to climate adaptation and mitigation. Repeat measurements of these data allow for the observation of deforestation or degradation of existing forests, natural forest regeneration, and the implementation of sustainable agricultural practices like agroforestry. Assessments of tree canopy height and crown projected area at a high spatial resolution are also important for monitoring carbon fluxes and assessing tree-based land uses, since forest structures can be highly spatially heterogeneous, especially in agroforestry systems. Very high resolution satellite imagery (less than one meter (1m) ground sample distance) makes it possible to extract information at the tree level while allowing monitoring at a very large scale. This paper presents the first high-resolution canopy height map concurrently produced for multiple sub-national jurisdictions. Specifically, we produc
    
[^12]: 计量再识别风险

    Measuring Re-identification Risk. (arXiv:2304.07210v1 [cs.CR])

    [http://arxiv.org/abs/2304.07210](http://arxiv.org/abs/2304.07210)

    该论文提出了一个用于测量用户表示中再识别风险的理论框架，该框架基于假设检验，可以限制攻击者从用户的表示中获取其身份的概率。作者展示了该框架应对实际应用的普适性，并补充了攻击算法来衡量现实应用的风险。

    

    紧凑的用户表示（例如嵌入）是个性化服务的支柱。在这项工作中，我们提出了一个新的理论框架，用于测量这种用户表示中的再识别风险。我们的框架基于假设检验，正式限制了攻击者能够从用户的表示中获取其身份的概率。作为应用，我们展示了我们的框架足够普遍以模拟重要的现实世界应用，例如 Chrome 的基于兴趣的广告的 Topics API。我们通过展示再识别攻击的合理好的算法来补充我们的理论界限，我们用这些算法来估计 Topics API 中的再识别风险。我们相信这项工作提供了一个严谨且可解释的再识别风险概念和测量它的框架，可以用来指导现实世界的应用。

    Compact user representations (such as embeddings) form the backbone of personalization services. In this work, we present a new theoretical framework to measure re-identification risk in such user representations. Our framework, based on hypothesis testing, formally bounds the probability that an attacker may be able to obtain the identity of a user from their representation. As an application, we show how our framework is general enough to model important real-world applications such as the Chrome's Topics API for interest-based advertising. We complement our theoretical bounds by showing provably good attack algorithms for re-identification that we use to estimate the re-identification risk in the Topics API. We believe this work provides a rigorous and interpretable notion of re-identification risk and a framework to measure it that can be used to inform real-world applications.
    
[^13]: 关于超图上三体相互作用的非线性平均动力学收敛性的研究

    On the convergence of nonlinear averaging dynamics with three-body interactions on hypergraphs. (arXiv:2304.07203v1 [math.DS])

    [http://arxiv.org/abs/2304.07203](http://arxiv.org/abs/2304.07203)

    本文研究了超图上具有三体相互作用的离散时间非线性平均动力学，在初态和超图拓扑以及更新非线性相互作用下，产生了高阶动力效应。

    

    物理学、生物学和社会科学等领域的复杂网络系统通常涉及超出简单的成对相互作用的交互。超图作为描述和分析具有多体相互作用的系统复杂行为的强大建模工具。本文研究了具有三体相互作用的离散时间非线性平均动力学：底层超图由三元组作为超边界定相互作用的结构，而顶点通过加权的状态依赖的邻域对的状态平均更新其状态。相较于带有二体相互作用的图上的线性平均动力学，这个动力学不会收敛到初始状态的平均值，而是减少了初态和超图拓扑的复杂相互作用和更新的非线性产生高阶动力效应。

    Complex networked systems in fields such as physics, biology, and social sciences often involve interactions that extend beyond simple pairwise ones. Hypergraphs serve as powerful modeling tools for describing and analyzing the intricate behaviors of systems with multi-body interactions. Herein, we investigate a discrete-time nonlinear averaging dynamics with three-body interactions: an underlying hypergraph, comprising triples as hyperedges, delineates the structure of these interactions, while the vertices update their states through a weighted, state-dependent average of neighboring pairs' states. This dynamics captures reinforcing group effects, such as peer pressure, and exhibits higher-order dynamical effects resulting from a complex interplay between initial states, hypergraph topology, and nonlinearity of the update. Differently from linear averaging dynamics on graphs with two-body interactions, this model does not converge to the average of the initial states but rather induc
    
[^14]: Just Tell Me: 业务流程管理中的提示工程

    Just Tell Me: Prompt Engineering in Business Process Management. (arXiv:2304.07183v1 [cs.AI])

    [http://arxiv.org/abs/2304.07183](http://arxiv.org/abs/2304.07183)

    该论文探讨了提示工程方法在业务流程管理中的应用。这种方法可以利用预训练的语言模型解决微调需要大量数据的问题，并为BPM研究带来诸多潜力。

    

    GPT-3和其他几个语言模型可以有效地处理各种自然语言处理任务，包括机器翻译和文本摘要。最近，它们也在业务流程管理（BPM）领域成功应用，例如用于预测过程监控和从文本中提取过程。然而，这通常需要对所用语言模型进行微调，其中包括大量合适的训练数据。解决这个问题的一种可能解决方案是使用提示工程，它利用预训练的语言模型，而无需进行微调。认识到这一点，我们认为提示工程可以帮助将语言模型的能力引入BPM研究。本篇文章利用这一观点，通过确定相关的潜力和挑战，为BPM研究的提示工程使用制定研究议程。

    GPT-3 and several other language models (LMs) can effectively address various natural language processing (NLP) tasks, including machine translation and text summarization. Recently, they have also been successfully employed in the business process management (BPM) domain, e.g., for predictive process monitoring and process extraction from text. This, however, typically requires fine-tuning the employed LM, which, among others, necessitates large amounts of suitable training data. A possible solution to this problem is the use of prompt engineering, which leverages pre-trained LMs without fine-tuning them. Recognizing this, we argue that prompt engineering can help bring the capabilities of LMs to BPM research. We use this position paper to develop a research agenda for the use of prompt engineering for BPM research by identifying the associated potentials and challenges.
    
[^15]: 用于高分辨率太阳观测成像的生成模型的比较研究

    A Comparative Study on Generative Models for High Resolution Solar Observation Imaging. (arXiv:2304.07169v1 [cs.CV])

    [http://arxiv.org/abs/2304.07169](http://arxiv.org/abs/2304.07169)

    本文研究了当前最先进的生成模型在高分辨率太阳图像生成中的应用，并发现扩散生成模型在生成细节方面表现比StyleGAN更好。

    

    太阳活动是我们太阳系变化的主要驱动因素，也是影响地球和近地空间的空间天气现象的关键来源。来自太阳动力学台站（SDO）的高分辨率极紫外线观测（EUV）记录提供了一个前所未有的庞大太阳图像数据集。在这项工作中，我们利用这个全面数据集来研究当前最先进的生成模型的能力，以准确捕捉观察到的太阳活动状态背后的数据分布。我们从基于StyleGAN的方法开始，发现该模型在处理高分辨率样本时处理太阳图像的细节存在严重缺陷，而处理自然面部图像时则相反。当转换为基于扩散的生成模型族时，我们观察到对细节生成的明显改进。对于GAN族，我们能够在牺牲一些较大比例的细节情况下实现类似的细节生成改进。此外，我们还研究了生成图像的视觉质量、多样性，以及旨在评估其作为太阳观测图像模型的准确性和真实性的一些度量标准。

    Solar activity is one of the main drivers of variability in our solar system and the key source of space weather phenomena that affect Earth and near Earth space. The extensive record of high resolution extreme ultraviolet (EUV) observations from the Solar Dynamics Observatory (SDO) offers an unprecedented, very large dataset of solar images. In this work, we make use of this comprehensive dataset to investigate capabilities of current state-of-the-art generative models to accurately capture the data distribution behind the observed solar activity states. Starting from StyleGAN-based methods, we uncover severe deficits of this model family in handling fine-scale details of solar images when training on high resolution samples, contrary to training on natural face images. When switching to the diffusion based generative model family, we observe strong improvements of fine-scale detail generation. For the GAN family, we are able to achieve similar improvements in fine-scale generation wh
    
[^16]: 强化学习中基于Bandit方法的显式塑形外部建议算法的研究

    Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning. (arXiv:2304.07163v1 [cs.AI])

    [http://arxiv.org/abs/2304.07163](http://arxiv.org/abs/2304.07163)

    本文研究了如何基于Bandit方法将外部建议融入到强化学习中，并提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。实验结果表明这些算法在样本复杂度、学习速度和形状质量方面都取得了良好的效果。

    

    强化学习（RL）算法中的一个关键问题是如何将外部或专家的建议融入到学习当中。本文将将将此问题表述为一种多臂赌博机称为塑形赌博机（shaping-bandits）。我们提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。通过在模拟环境和LQR和Atari环境中的实验，我们证明了这三种算法在样本复杂度、学习速度和形状质量方面的有效性。

    A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason a
    
[^17]: 结合随机解释器和子图神经网络可以增强表达能力和可解释性

    Combining Stochastic Explainers and Subgraph Neural Networks can Increase Expressivity and Interpretability. (arXiv:2304.07152v1 [cs.LG])

    [http://arxiv.org/abs/2304.07152](http://arxiv.org/abs/2304.07152)

    本论文提出了一种结合随机解释器和子图神经网络的新框架，通过挑选有意义的子图来提高图神经网络的表达能力，并提供可解释性结果。与标准子图提取策略相比，该框架产生的子图在准确性方面可达到可比较的性能，同时提供解释的附加好处。

    

    子图增强图神经网络（SGNN）可以增强标准的消息传递框架的表达能力。该模型家族将每个图表示为一组子图，通常通过随机抽样或手工启发式方法提取。我们的关键观察是，通过选择“有意义”的子图，除了提高GNN的表达能力外，还可以获得可解释的结果。为此，我们引入了一种新的框架，同时预测图的类别和一组解释性稀疏子图，可以分析这些子图来理解分类器的决策过程。我们将我们的框架与标准子图提取策略进行了比较，如随机节点/边缘删除策略。我们的框架产生的子图允许在准确性方面实现可比较的性能，同时提供解释的附加好处。

    Subgraph-enhanced graph neural networks (SGNN) can increase the expressive power of the standard message-passing framework. This model family represents each graph as a collection of subgraphs, generally extracted by random sampling or with hand-crafted heuristics. Our key observation is that by selecting "meaningful" subgraphs, besides improving the expressivity of a GNN, it is also possible to obtain interpretable results. For this purpose, we introduce a novel framework that jointly predicts the class of the graph and a set of explanatory sparse subgraphs, which can be analyzed to understand the decision process of the classifier. We compare the performance of our framework against standard subgraph extraction policies, like random node/edge deletion strategies. The subgraphs produced by our framework allow to achieve comparable performance in terms of accuracy, with the additional benefit of providing explanations.
    
[^18]: 多模态全端到端系统优化可再生能源即时预报学习

    End-to-End Learning with Multiple Modalities for System-Optimised Renewables Nowcasting. (arXiv:2304.07151v1 [eess.SY])

    [http://arxiv.org/abs/2304.07151](http://arxiv.org/abs/2304.07151)

    本文介绍了一种利用多模态和全端到端学习来实现系统优化可再生能源即时预报的方法。该方法将全天候图像和气象传感器数据的特征结合起来，有效组合预测值，并用于实现优化电力流公式来模拟能源管理。该方法首次结合多模态和全端到端学习进行培训，实现了最小化预期总体系统成本的目的。使用荷兰真实的天空和气象数据进行的案例研究表明，所提出的MM-E2E模型将系统成本与单模式基线相比降低了30％。

    

    随着风能和太阳能等可再生能源的普及，准确的短期可再生能源预测越来越重要。本文研究多模式（MM）学习和全端到端（E2E）学习方法，在实现能源管理系统时使用中间预测可再生能源的方法。MM将全天候图像和气象传感器数据的特征结合起来，作为两种模态来预测可再生能源的产生，以此实现预测值的有效组合。然后，将组合后的预测值作为不同iable优化电力流（OPF）公式的输入，模拟能源管理。首次使用MM结合E2E模型进行培训，以达到最小化预期总体系统成本的目的。案例研究采用荷兰真实的天空和气象数据测试了所提出的方法。在我们的研究中，所提出的MM-E2E模型将系统成本与单模式基线相比降低了30％。

    With the increasing penetration of renewable power sources such as wind and solar, accurate short-term, nowcasting renewable power prediction is becoming increasingly important. This paper investigates the multi-modal (MM) learning and end-to-end (E2E) learning for nowcasting renewable power as an intermediate to energy management systems. MM combines features from all-sky imagery and meteorological sensor data as two modalities to predict renewable power generation that otherwise could not be combined effectively. The combined, predicted values are then input to a differentiable optimal power flow (OPF) formulation simulating the energy management. For the first time, MM is combined with E2E training of the model that minimises the expected total system cost. The case study tests the proposed methodology on the real sky and meteorological data from the Netherlands. In our study, the proposed MM-E2E model reduced system cost by 30% compared to uni-modal baselines.
    
[^19]: 多模态无监督全身PET异常检测的交叉注意力变换器

    Cross Attention Transformers for Multi-modal Unsupervised Whole-Body PET Anomaly Detection. (arXiv:2304.07147v1 [eess.IV])

    [http://arxiv.org/abs/2304.07147](http://arxiv.org/abs/2304.07147)

    本论文提出了一种基于交叉注意力变换器的多模态无监督全身PET异常检测方法，该方法利用变换器强大的建模能力实现了高效准确地学习器官间和其成像模式的长程交互，并取得了最先进的结果。

    

    癌症是一种高度异质性的疾病，可以在人体的几乎任何部位发生。18F-氟脱氧葡萄糖是一种常用于检测癌症的成像模式，由于其高灵敏度和代谢活性模式的明显可视化。尽管如此，由于癌症高度异质性，训练通用的区分性癌症检测模型是具有挑战性的，数据可用性和疾病复杂性常常被认为是一项限制性因素。无监督的异常检测模型被提出作为一种解决方案。这些模型学习健康组织的表示，并通过预测与健康正常的偏差来检测癌症，这需要具有高度表现力的器官之间和它们的成像模式之间准确学习长程交互的模型。这些特征通过变换器得到了满足，变换器已经被证明在无监督异常检测方面产生了最先进的结果，通过训练正常的PET成像数据来实现。

    Cancer is a highly heterogeneous condition that can occur almost anywhere in the human body. 18F-fluorodeoxyglucose is an imaging modality commonly used to detect cancer due to its high sensitivity and clear visualisation of the pattern of metabolic activity. Nonetheless, as cancer is highly heterogeneous, it is challenging to train general-purpose discriminative cancer detection models, with data availability and disease complexity often cited as a limiting factor. Unsupervised anomaly detection models have been suggested as a putative solution. These models learn a healthy representation of tissue and detect cancer by predicting deviations from the healthy norm, which requires models capable of accurately learning long-range interactions between organs and their imaging patterns with high levels of expressivity. Such characteristics are suitably satisfied by transformers, which have been shown to generate state-of-the-art results in unsupervised anomaly detection by training on norma
    
[^20]: 训练神经网络语音分离模型的数据采样策略研究

    On Data Sampling Strategies for Training Neural Network Speech Separation Models. (arXiv:2304.07142v1 [cs.SD])

    [http://arxiv.org/abs/2304.07142](http://arxiv.org/abs/2304.07142)

    本文研究了训练神经网络语音分离模型的数据采样策略对模型性能的影响。研究表明，对于特定的信号长度分布，采用特定的训练信号长度限制可以获得更好的性能。

    

    语音分离仍然是多说话信号处理的重要领域。深度神经网络（DNN）模型在许多语音分离基准上取得了最佳性能。一些模型需要较长的训练时间和较高的内存需求。以前的研究提出了缩短训练示例以解决这些问题，但这对模型性能的影响尚不清楚。本文分析了应用这些训练信号长度（TSL）限制对两个语音分离模型（SepFormer，一个变换器模型，和Conv-TasNet，一个卷积模型）的影响。使用WJS0-2Mix，WHAMR和Libri2Mix数据集来分析信号长度分布及其对训练效率的影响。研究表明，对于特定的分布，应用特定的TSL限制可以获得更好的性能。这主要是由于对波形起始索引进行随机采样导致更多独特的示例用于训练。

    Speech separation remains an important area of multi-speaker signal processing. Deep neural network (DNN) models have attained the best performance on many speech separation benchmarks. Some of these models can take significant time to train and have high memory requirements. Previous work has proposed shortening training examples to address these issues but the impact of this on model performance is not yet well understood. In this work, the impact of applying these training signal length (TSL) limits is analysed for two speech separation models: SepFormer, a transformer model, and Conv-TasNet, a convolutional model. The WJS0-2Mix, WHAMR and Libri2Mix datasets are analysed in terms of signal length distribution and its impact on training efficiency. It is demonstrated that, for specific distributions, applying specific TSL limits results in better performance. This is shown to be mainly due to randomly sampling the start index of the waveforms resulting in more unique examples for tra
    
[^21]: TUM-FA\c{C}ADE：用于表面分割的点云基准的评估和增强

    TUM-FA\c{C}ADE: Reviewing and enriching point cloud benchmarks for fa\c{c}ade segmentation. (arXiv:2304.07140v1 [cs.CV])

    [http://arxiv.org/abs/2304.07140](http://arxiv.org/abs/2304.07140)

    本文介绍了一种用于增强现有点云数据集的方法，以便进行表面分割测试，并利用该方法创建了TUM-FA\c{C}ADE数据集。这个数据集可以促进基于点云的表面分割任务的开发，并且该方法也可以用于其他基准类型，创建更多样化的评估数据集。

    

    点云数据被广泛认为是城市制图最佳数据类型之一。因此，点云数据集通常被用于各种城市解释方法的基准类型。然而，很少有研究者探究使用点云基准来进行表面分割。稳健的表面分割已经成为各种应用的关键因素，范围从模拟自动驾驶功能到文化遗产保护。在本文中，我们提出了一种丰富现有点云数据集的方法，其中新增了面相关类别，以便于进行表面分割测试。我们提出了如何高效地扩展现有数据集，并全面评估它们在表面分割方面的潜力。我们使用该方法创建了TUM-FA\c{C}ADE数据集，用于扩展TUM-MLS-2016的功能。TUM-FA\c{C}ADE不仅可以促进基于点云的表面分割任务的开发，而且我们丰富点云数据集的方法也可以应用于其他基准类型，为城市解释方法创建更多样化的评估数据集。

    Point clouds are widely regarded as one of the best dataset types for urban mapping purposes. Hence, point cloud datasets are commonly investigated as benchmark types for various urban interpretation methods. Yet, few researchers have addressed the use of point cloud benchmarks for fa\c{c}ade segmentation. Robust fa\c{c}ade segmentation is becoming a key factor in various applications ranging from simulating autonomous driving functions to preserving cultural heritage. In this work, we present a method of enriching existing point cloud datasets with fa\c{c}ade-related classes that have been designed to facilitate fa\c{c}ade segmentation testing. We propose how to efficiently extend existing datasets and comprehensively assess their potential for fa\c{c}ade segmentation. We use the method to create the TUM-FA\c{C}ADE dataset, which extends the capabilities of TUM-MLS-2016. Not only can TUM-FA\c{C}ADE facilitate the development of point-cloud-based fa\c{c}ade segmentation tasks, but our 
    
[^22]: 一种解释不能适用于XIL

    One Explanation Does Not Fit XIL. (arXiv:2304.07136v1 [cs.LG])

    [http://arxiv.org/abs/2304.07136](http://arxiv.org/abs/2304.07136)

    提出了一种解释性交互式机器学习框架（XIL）来修正模型，但同时发现"一种解释不能适用于XIL"，建议考虑多种解释。

    

    当前的机器学习模型在许多领域产生出色的结果，但同时也存在着快捷学习和错误相关的缺陷。为了解决这些缺陷，提出了解释性交互式机器学习（XIL）框架，通过使用用户对模型解释的反馈来修正模型。本文重点探讨了该框架中使用的解释。特别地，我们研究了通过多个解释方法进行同时模型修正，从而发现"一种解释不能适用于XIL" ，并建议在通过XIL进行模型修正时考虑多种解释。

    Current machine learning models produce outstanding results in many areas but, at the same time, suffer from shortcut learning and spurious correlations. To address such flaws, the explanatory interactive machine learning (XIL) framework has been proposed to revise a model by employing user feedback on a model's explanation. This work sheds light on the explanations used within this framework. In particular, we investigate simultaneous model revision through multiple explanation methods. To this end, we identified that \textit{one explanation does not fit XIL} and propose considering multiple ones when revising models via XIL.
    
[^23]: 通过奖励引导探索实现可控扩散模型

    Towards Controllable Diffusion Models via Reward-Guided Exploration. (arXiv:2304.07132v1 [cs.LG])

    [http://arxiv.org/abs/2304.07132](http://arxiv.org/abs/2304.07132)

    本论文提出了一种新的框架(RGDM)，可以通过强化学习(RL)引导扩散模型的训练阶段，在不需要可微分的引导信号的情况下，实现对生成的样本的有效控制，并生成高质量、细粒度控制的样本。

    

    通过将数据样本的生成形式化为马尔可夫去噪过程，扩散模型在多个任务中实现了最先进的性能。最近，许多扩散模型的变体已被提出，以使生成的样本能够被有效控制。大多数现有方法要么将控制信息作为噪声逼近器的输入（即条件表示），要么在测试阶段引入预先训练的分类器来指导Langevin动力学朝向条件目标。然而，前一种方法仅适用于控制信息可以被表示为条件表示的情况，而后一种方法则需要可微分的预训练辅导分类器。本文提出了一种名为RGDM（奖励引导扩散模型）的新框架，通过强化学习（RL）引导扩散模型的训练阶段。所提出的训练框架将加权对数-似然的目标和最大熵RL目标相结合，从而仅使用非可微分的引导信号即可训练模型。我们的实验表明，RGDM可以生成高质量、细粒度控制的样本，同时实现了与最先进的扩散模型相当的性能。

    By formulating data samples' formation as a Markov denoising process, diffusion models achieve state-of-the-art performances in a collection of tasks. Recently, many variants of diffusion models have been proposed to enable controlled sample generation. Most of these existing methods either formulate the controlling information as an input (i.e.,: conditional representation) for the noise approximator, or introduce a pre-trained classifier in the test-phase to guide the Langevin dynamic towards the conditional goal. However, the former line of methods only work when the controlling information can be formulated as conditional representations, while the latter requires the pre-trained guidance classifier to be differentiable. In this paper, we propose a novel framework named RGDM (Reward-Guided Diffusion Model) that guides the training-phase of diffusion models via reinforcement learning (RL). The proposed training framework bridges the objective of weighted log-likelihood and maximum e
    
[^24]: 随机森林的分组Shapley值特征重要性解释可用于产量预测

    Grouping Shapley Value Feature Importances of Random Forests for explainable Yield Prediction. (arXiv:2304.07111v1 [cs.LG])

    [http://arxiv.org/abs/2304.07111](http://arxiv.org/abs/2304.07111)

    本文提出了一种新的方法来解释机器学习模型中的分组特征对产量预测的影响，具有较高的效率。

    

    产量预测中的可解释性有助于充分探索机器学习模型的潜力，这些模型已经能够在各种产量预测方案中实现高精度。用于产量预测的数据是复杂的，模型通常难以理解。然而，通过使用自然分组的输入特征可以简化理解模型。例如，可以通过采集特征的时间或传感器来实现分组。解释机器学习模型的最新方法是通过Shapley values的博弈论方法实现的。为了处理特征组，通常将计算出的Shapley values相加，忽略了这种方法的理论限制。我们解释了Shapley values的概念，并介绍了一种在树结构上高效计算它们的算法，以直接计算预定特征组的Shapley values。

    Explainability in yield prediction helps us fully explore the potential of machine learning models that are already able to achieve high accuracy for a variety of yield prediction scenarios. The data included for the prediction of yields are intricate and the models are often difficult to understand. However, understanding the models can be simplified by using natural groupings of the input features. Grouping can be achieved, for example, by the time the features are captured or by the sensor used to do so. The state-of-the-art for interpreting machine learning models is currently defined by the game-theoretic approach of Shapley values. To handle groups of features, the calculated Shapley values are typically added together, ignoring the theoretical limitations of this approach. We explain the concept of Shapley values directly computed for predefined groups of features and introduce an algorithm to compute them efficiently on tree structures. We provide a blueprint for designing swar
    
[^25]: HLTPR@RWTH在DSTC9和DSTC10中的任务导向型文档对话系统

    Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10. (arXiv:2304.07101v1 [cs.CL])

    [http://arxiv.org/abs/2304.07101](http://arxiv.org/abs/2304.07101)

    本论文总结了HLTPR@RWTH团队在DSTC9和DSTC10中为任务导向型文档对话系统所做的贡献，包括提出了不同的方法来使选择任务更有效率，在DSTC10中提出了数据增强技术来提高模型的鲁棒性并适应生成回答的风格，以及提出了一个嘈杂的通道模型来直接建模语音识别错误。实验结果表明，该团队的方法显著优于基线模型。

    

    本文总结了我们在第9和第10次Dialog System Technology Challenges（DSTC9和DSTC10）中为对话基于文档的任务作出的贡献。在两次迭代中，任务由三个子任务组成：首先检测当前回合是否需要知识，其次选择相关的知识文档，第三生成基于所选文档的回答。对于DSTC9，我们提出了不同的方法来使选择任务更有效率。其中最好的方法——分层选择，实际上比原始基线改进了结果，并提高了24倍速度。在DSTC10迭代中，挑战是要使经过书面对话训练的系统能够在嘈杂的自动语音识别转录中表现良好。因此，我们提出了数据增强技术来提高模型的鲁棒性，以及适应生成回答的风格，使其与前期对话相匹配的方法。此外，我们提出了一个嘈杂的通道模型来直接建模语音识别错误。实验结果表明，我们的方法在所有子任务上显著优于基线模型。

    This paper summarizes our contributions to the document-grounded dialog tasks at the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). In both iterations the task consists of three subtasks: first detect whether the current turn is knowledge seeking, second select a relevant knowledge document, and third generate a response grounded on the selected document. For DSTC9 we proposed different approaches to make the selection task more efficient. The best method, Hierarchical Selection, actually improves the results compared to the original baseline and gives a speedup of 24x. In the DSTC10 iteration of the task, the challenge was to adapt systems trained on written dialogs to perform well on noisy automatic speech recognition transcripts. Therefore, we proposed data augmentation techniques to increase the robustness of the models as well as methods to adapt the style of generated responses to fit well into the proceeding dialog. Additionally, we proposed a noisy channel
    
[^26]: 权重连体网络用于从MRI图像预测阿尔茨海默病发病时间

    Weighted Siamese Network to Predict the Time to Onset of Alzheimer's Disease from MRI Images. (arXiv:2304.07097v1 [eess.IV])

    [http://arxiv.org/abs/2304.07097](http://arxiv.org/abs/2304.07097)

    本文提出使用加权连体网络对进展性MCI患者进行序数分类，以预测他们距离严重AD阶段的距离，从而实现更准确的早期检测。

    

    阿尔茨海默病（AD）是痴呆症最常见的原因，是一种进行性疾病，会先出现轻度认知障碍（MCI）。早期检测对于做出治疗决策至关重要。然而，大多数关于计算机辅助检测AD的文献聚焦于将脑图像分类为三个主要类别之一：健康、MCI和AD；或将MCI患者分类为(1)进展：在给定研究期内的未来检查时间进展到AD的患者，以及(2)稳定：持续作为MCI患者而从未进展为AD。这种方法错过了准确识别进展性MCI患者轨迹的机会。本文重新审视了AD识别的脑图像分类任务，将其重新构建为一个序数分类任务，以预测患者距离严重AD阶段有多近。为此，我们从阿尔茨海默病神经影像计划（ADNI）数据集中选择进展性MCI患者，并使用加权连体网络进行实验。

    Alzheimer's Disease (AD), which is the most common cause of dementia, is a progressive disease preceded by Mild Cognitive Impairment (MCI). Early detection of the disease is crucial for making treatment decisions. However, most of the literature on computer-assisted detection of AD focuses on classifying brain images into one of three major categories: healthy, MCI, and AD; or categorising MCI patients into one of (1) progressive: those who progress from MCI to AD at a future examination time during a given study period, and (2) stable: those who stay as MCI and never progress to AD. This misses the opportunity to accurately identify the trajectory of progressive MCI patients. In this paper, we revisit the brain image classification task for AD identification and re-frame it as an ordinal classification task to predict how close a patient is to the severe AD stage. To this end, we select progressive MCI patients from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and co
    
[^27]: Delta去噪分数

    Delta Denoising Score. (arXiv:2304.07090v1 [cs.CV])

    [http://arxiv.org/abs/2304.07090](http://arxiv.org/abs/2304.07090)

    本文提出了一种新颖的Delta去噪分数（DDS）方法，可以用作基于文本的图像编辑、优化问题中的损失项，通过使用分数蒸馏采样（SDS）机制进行图像编辑，并通过匹配提示和图像来去除不希望的SDS错误方向，提高了图像编辑的精度和清晰度。

    

    本论文提出了Delta去噪分数（DDS），这是一种新颖的评分函数，用于基于文本的图像编辑，可以引导最小修改输入图像，使其朝向目标提示所描述的内容。DDS利用了文本到图像扩散模型的丰富生成先验知识，并可用作优化问题中的损失项，以便将图像朝着由文本指定的所需方向引导。DDS利用了分数蒸馏采样（SDS）机制来进行图像编辑。我们发现，仅使用SDS通常会产生非详细和模糊的输出，因为存在噪声梯度的问题。为了解决这个问题，DDS使用一个与输入图像匹配的提示来识别和去除不希望的SDS错误方向。我们的主要前提是，当计算匹配提示和图像的对时，SDS应为零，这意味着如果分数非零，则其梯度可以归因于SDS的错误组成部分。我们的分析证明了DDS在基于文本的图像到图像编辑上的竞争力。

    We introduce Delta Denoising Score (DDS), a novel scoring function for text-based image editing that guides minimal modifications of an input image towards the content described in a target prompt. DDS leverages the rich generative prior of text-to-image diffusion models and can be used as a loss term in an optimization problem to steer an image towards a desired direction dictated by a text. DDS utilizes the Score Distillation Sampling (SDS) mechanism for the purpose of image editing. We show that using only SDS often produces non-detailed and blurry outputs due to noisy gradients. To address this issue, DDS uses a prompt that matches the input image to identify and remove undesired erroneous directions of SDS. Our key premise is that SDS should be zero when calculated on pairs of matched prompts and images, meaning that if the score is non-zero, its gradients can be attributed to the erroneous component of SDS. Our analysis demonstrates the competence of DDS for text based image-to-i
    
[^28]: 基于补丁生成的内存有效扩散概率模型

    Memory Efficient Diffusion Probabilistic Models via Patch-based Generation. (arXiv:2304.07087v1 [cs.CV])

    [http://arxiv.org/abs/2304.07087](http://arxiv.org/abs/2304.07087)

    本文提出了一种基于补丁生成的扩散概率模型，通过两种条件方法来保证生成的图像在适当的位置，并具有一致的内容。这种方法可以节省内存，更适用于边缘设备。

    

    扩散概率模型已经成功地生成了高质量和多样化的图像。然而，传统模型的输入和输出都是高分辨率图像，需要过多的内存，因此在边缘设备上不太实用。为了解决这个问题，我们提出了一个基于补丁的方法来生成扩散概率模型。我们提出了两种基于补丁生成的条件方法：第一种是使用单热表示的位置条件，以确保补丁在适当的位置；第二种是全局内容条件，以确保拼接在一起的补丁具有一致的内容。我们在CelebA和LSUN床上进行了定性和定量评估。

    Diffusion probabilistic models have been successful in generating high-quality and diverse images. However, traditional models, whose input and output are high-resolution images, suffer from excessive memory requirements, making them less practical for edge devices. Previous approaches for generative adversarial networks proposed a patch-based method that uses positional encoding and global content information. Nevertheless, designing a patch-based approach for diffusion probabilistic models is non-trivial. In this paper, we resent a diffusion probabilistic model that generates images on a patch-by-patch basis. We propose two conditioning methods for a patch-based generation. First, we propose position-wise conditioning using one-hot representation to ensure patches are in proper positions. Second, we propose Global Content Conditioning (GCC) to ensure patches have coherent content when concatenated together. We evaluate our model qualitatively and quantitatively on CelebA and LSUN bed
    
[^29]: 基于神经网络集成的不确定性感知车辆能效预测

    Uncertainty-Aware Vehicle Energy Efficiency Prediction using an Ensemble of Neural Networks. (arXiv:2304.07073v1 [cs.LG])

    [http://arxiv.org/abs/2304.07073](http://arxiv.org/abs/2304.07073)

    本文基于深度神经网络集成学习方法，提出了一种减少预测不确定性并输出衡量值的车辆能效预测方法，可应用于降低碳足迹，并在测试中表现出高度的预测性能。

    

    运输部门约占全球温室气体排放的25％。因此，在交通部门提高能效是减少碳足迹的关键。能效通常以每行驶距离的能源消耗来衡量，例如每公里的燃油升数。影响能效的主要因素包括车辆类型，环境，驾驶员行为和天气条件。这些不同的因素引入了估计车辆能效的不确定性。本文提出了一种基于深度神经网络集成学习方法，旨在减少预测不确定性并输出这种不确定性的衡量值。本研究使用公开可得的车辆能源数据集（VED）进行了评估，并将其与每辆车和能源类型的几个基线进行了比较。结果表明，该方法具有高度的预测性能，并且能够输出预测不确定性的衡量值。

    The transportation sector accounts for about 25% of global greenhouse gas emissions. Therefore, an improvement of energy efficiency in the traffic sector is crucial to reducing the carbon footprint. Efficiency is typically measured in terms of energy use per traveled distance, e.g. liters of fuel per kilometer. Leading factors that impact the energy efficiency are the type of vehicle, environment, driver behavior, and weather conditions. These varying factors introduce uncertainty in estimating the vehicles' energy efficiency. We propose in this paper an ensemble learning approach based on deep neural networks (ENN) that is designed to reduce the predictive uncertainty and to output measures of such uncertainty. We evaluated it using the publicly available Vehicle Energy Dataset (VED) and compared it with several baselines per vehicle and energy type. The results showed a high predictive performance and they allowed to output a measure of predictive uncertainty.
    
[^30]: 基于哈密顿动力学的深度神经网络目标导向训练: “谁早退，谁输”

    Who breaks early, looses: goal oriented training of deep neural networks based on port Hamiltonian dynamics. (arXiv:2304.07070v1 [cs.LG])

    [http://arxiv.org/abs/2304.07070](http://arxiv.org/abs/2304.07070)

    本文提出了一种基于哈密顿动力学的深度神经网络目标导向训练方法，通过达到预定的损失函数减少来实现从勘探到利用的转换，与标准随机梯度下降相比，该方法在多个标准数据集上取得了更好的性能。

    

    由于深度神经网络损失作为参数函数的高度结构化能源景观，因此需要使用精密的优化策略来发现保证合理性能的（局部）最小值。最小化次优解是一个重要的前提，通常使用动量方法来实现。然而，像其他非局部优化过程一样，这创建了在勘探和利用之间平衡的必要性。在本文中，我们提出了一种基于达到预定损失函数减少的事件控制机制，用于从勘探转向利用。由于我们给出了动量法的哈密顿解释，我们应用了“带有摩擦的重球”解释，并在实现某些目标时触发“摩擦”或“断裂”。我们将我们的方法与标准随机梯度下降进行基准测试，并提供了有关在多个标准数据集上改善深度神经网络训练性能的实验证据。

    The highly structured energy landscape of the loss as a function of parameters for deep neural networks makes it necessary to use sophisticated optimization strategies in order to discover (local) minima that guarantee reasonable performance. Overcoming less suitable local minima is an important prerequisite and often momentum methods are employed to achieve this. As in other non local optimization procedures, this however creates the necessity to balance between exploration and exploitation. In this work, we suggest an event based control mechanism for switching from exploration to exploitation based on reaching a predefined reduction of the loss function. As we give the momentum method a port Hamiltonian interpretation, we apply the 'heavy ball with friction' interpretation and trigger breaking (or friction) when achieving certain goals. We benchmark our method against standard stochastic gradient descent and provide experimental evidence for improved performance of deep neural netwo
    
[^31]: 关于知识图谱中存在性一阶查询推理的研究

    On Existential First Order Queries Inference on Knowledge Graphs. (arXiv:2304.07063v1 [cs.AI])

    [http://arxiv.org/abs/2304.07063](http://arxiv.org/abs/2304.07063)

    本文阐述了关于知识图谱中存在性一阶查询推理的新方法，提出了一个新数据集，并开发了一种来自模糊逻辑理论的新搜索算法，该算法能够解决新公式，并在现有公式中超过以前的方法。

    

    知识图谱推理是一项具有挑战性的任务，因为它利用观察到的信息来预测缺失的信息。特别地，回答一阶逻辑公式是特别感兴趣的，因为它具有清晰的语法和语义。最近，提出了查询嵌入方法，该方法学习了一组实体的嵌入，并将逻辑运算视为集合运算。尽管有很多研究遵循相同的方法，但它缺乏从逻辑角度进行系统检查的方法。在本文中，我们描述了先前研究调查的查询范围，并准确地确定了它与整个存在性公式家族之间的差距。此外，我们还开发了一个包含十个新公式的新数据集，并讨论了同时出现的新挑战。最后，我们提出了一种来自模糊逻辑理论的新搜索算法，该算法能够解决新公式，并在现有公式中超过以前的方法。

    Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Specifically, answering first-order logic formulas is of particular interest because of its clear syntax and semantics. Recently, the query embedding method has been proposed which learns the embedding of a set of entities and treats logic operations as set operations. Though there has been much research following the same methodology, it lacks a systematic inspection from the standpoint of logic. In this paper, we characterize the scope of queries investigated previously and precisely identify the gap between it and the whole family of existential formulas. Moreover, we develop a new dataset containing ten new formulas and discuss the new challenges coming simultaneously. Finally, we propose a new search algorithm from fuzzy logic theory which is capable of solving new formulas and outperforming the previous methods in existing formulas.
    
[^32]: 面部视频压缩的感知质量评估：基准和有效方法

    Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method. (arXiv:2304.07056v1 [eess.IV])

    [http://arxiv.org/abs/2304.07056](http://arxiv.org/abs/2304.07056)

    本文介绍了大规模压缩面部视频质量评估（CFVQA）数据库，用于系统地了解面部视频感知质量和多样化压缩失真。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。

    

    近年来，对面部视频压缩的需求呈指数级增长，人工智能的成功使得超出了传统的混合视频编码范围。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。然而，空间和时间域中扭曲类型的极大多样性，从传统的混合编码框架到生成模型，给压缩面部视频质量评估（VQA）带来了巨大挑战。在本文中，我们介绍了大规模压缩面部视频质量评估（CFVQA）数据库，这是系统地了解面部视频感知质量和多样化压缩失真的第一次尝试。该数据库包含 3,240 个压缩的面部视频片段，涵盖多个压缩级别，这些片段来自 135 个源视频，具有多样性。

    Recent years have witnessed an exponential increase in the demand for face video compression, and the success of artificial intelligence has expanded the boundaries beyond traditional hybrid video coding. Generative coding approaches have been identified as promising alternatives with reasonable perceptual rate-distortion trade-offs, leveraging the statistical priors of face videos. However, the great diversity of distortion types in spatial and temporal domains, ranging from the traditional hybrid coding frameworks to generative models, present grand challenges in compressed face video quality assessment (VQA). In this paper, we introduce the large-scale Compressed Face Video Quality Assessment (CFVQA) database, which is the first attempt to systematically understand the perceptual quality and diversified compression distortions in face videos. The database contains 3,240 compressed face video clips in multiple compression levels, which are derived from 135 source videos with diversif
    
[^33]: 大规模射电干涉数据的有损压缩

    Lossy Compression of Large-Scale Radio Interferometric Data. (arXiv:2304.07050v1 [astro-ph.IM])

    [http://arxiv.org/abs/2304.07050](http://arxiv.org/abs/2304.07050)

    本文提出了一种通过基线相关的有损压缩技术来降低射电干涉可见度数据体积的方法，将整个可见度数据表示为基线的数据矩阵集合，从而实现数据的压缩，同时保留视场边缘的模糊效应。

    

    本文提出使用一种基线相关的有损压缩技术来减少可见度数据的体积，同时保留视场边缘的模糊效果。我们利用矩阵秩和低秩近似描述原始可见度数据的基本分量和具体的天空分布矩阵。因此，整个可见度数据被表示为基线的数据矩阵集合，而不是单个张量。我们提出了两种压缩算法，分别是简单的$SVD$和命名为$BDSVD$的算法。

    This work proposes to reduce visibility data volume using a baseline-dependent lossy compression technique that preserves smearing at the edges of the field-of-view. We exploit the relation of the rank of a matrix and the fact that a low-rank approximation can describe the raw visibility data as a sum of basic components where each basic component corresponds to a specific Fourier component of the sky distribution. As such, the entire visibility data is represented as a collection of data matrices from baselines, instead of a single tensor. The proposed methods are formulated as follows: provided a large dataset of the entire visibility data; the first algorithm, named $simple~SVD$ projects the data into a regular sampling space of rank$-r$ data matrices. In this space, the data for all the baselines has the same rank, which makes the compression factor equal across all baselines. The second algorithm, named $BDSVD$ projects the data into an irregular sampling space of rank$-r_{pq}$ da
    
[^34]: Wasserstein PAC-Bayes 学习：泛化与优化之间的桥梁。

    Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation. (arXiv:2304.07048v1 [stat.ML])

    [http://arxiv.org/abs/2304.07048](http://arxiv.org/abs/2304.07048)

    本文介绍了扩展的 Wasserstein PAC-Bayes 框架，利用损失函数上的几何假设提供了新的泛化界限。通过该框架，证明了 \cite{lambert2022variational} 中算法的输出具有强大的泛化能力。同时，建立了 PAC-Bayes 和优化算法之间的桥梁。

    

    PAC-Bayes 学习是一种已建立的框架，用于在训练阶段评估学习算法的泛化能力。然而，在训练之前，弄清楚为什么知名算法的输出具有良好的泛化特性而 PAC-Bayes 是否有用仍然具有挑战性。我们通过扩展简要介绍在文献 \cite{amit2022ipm} 中提出的 \emph{Wasserstein PAC-Bayes} 框架来积极回答这个问题。我们提供了新的泛化界限，利用损失函数上的几何假设。使用我们的框架，我们在任何训练之前就证明了 \cite{lambert2022variational} 中算法的输出具有强大的渐近泛化能力。更具体地说，我们展示了如何在泛化框架中将优化结果结合起来，构建了 PAC-Bayes 和优化算法之间的桥梁。

    PAC-Bayes learning is an established framework to assess the generalisation ability of learning algorithm during the training phase. However, it remains challenging to know whether PAC-Bayes is useful to understand, before training, why the output of well-known algorithms generalise well. We positively answer this question by expanding the \emph{Wasserstein PAC-Bayes} framework, briefly introduced in \cite{amit2022ipm}. We provide new generalisation bounds exploiting geometric assumptions on the loss function. Using our framework, we prove, before any training, that the output of an algorithm from \citet{lambert2022variational} has a strong asymptotic generalisation ability. More precisely, we show that it is possible to incorporate optimisation results within a generalisation framework, building a bridge between PAC-Bayes and optimisation algorithms.
    
[^35]: 基于层次代理的强化学习框架用于自动评估胎儿超声视频质量的研究

    Hierarchical Agent-based Reinforcement Learning Framework for Automated Quality Assessment of Fetal Ultrasound Video. (arXiv:2304.07036v1 [eess.IV])

    [http://arxiv.org/abs/2304.07036](http://arxiv.org/abs/2304.07036)

    本研究提出了一个基于强化学习的自动评估胎儿超声视频质量的层次代理框架，具有优化标注量、整体质量评估和人类专家结果一致性等显著优势。

    

    超声是孕期检查胎儿生长的主要方法，但图像质量可能会受到各种因素的影响。质量评估对于控制超声图像的质量以保证其感知和诊断价值至关重要。现有的自动方法通常需要大量结构性标注，而且其预测结果可能不一定与人类专家的评估结果保持一致。此外，不应忽视扫描的整体质量和帧间质量的相关性。本文提出了一种强化学习框架，由两个层次代理协同学习执行帧级和视频级质量评估。它配备了一种特殊设计的奖励机制，考虑帧质量之间的时间依赖性，只需要稀疏的二进制注释进行训练。在挑战性的胎儿脑数据集上的实验结果验证了所提出的框架在超过现有最先进方法的同时，效果优于人类专家在一致性和效率方面。

    Ultrasound is the primary modality to examine fetal growth during pregnancy, while the image quality could be affected by various factors. Quality assessment is essential for controlling the quality of ultrasound images to guarantee both the perceptual and diagnostic values. Existing automated approaches often require heavy structural annotations and the predictions may not necessarily be consistent with the assessment results by human experts. Furthermore, the overall quality of a scan and the correlation between the quality of frames should not be overlooked. In this work, we propose a reinforcement learning framework powered by two hierarchical agents that collaboratively learn to perform both frame-level and video-level quality assessments. It is equipped with a specially-designed reward mechanism that considers temporal dependency among frame quality and only requires sparse binary annotations to train. Experimental results on a challenging fetal brain dataset verify that the prop
    
[^36]: 面向热成像的光谱转移引导的主动领域适应

    Spectral Transfer Guided Active Domain Adaptation For Thermal Imagery. (arXiv:2304.07031v1 [cs.CV])

    [http://arxiv.org/abs/2304.07031](http://arxiv.org/abs/2304.07031)

    本论文提出了一种面向热成像的主动适应方法，将可见光谱和热成像结合起来并通过选择少量目标样本进行注释来缓解源域和目标域之间的域适应差距。

    

    可见光谱数据集的利用已经使得深度网络取得了显着的成功。然而，实际应用任务包括低光照条件，这使得在大规模RGB图像数据集上训练的模型表现出性能瓶颈。热红外相机对这种情况更为稳健。因此，在实际应用中使用热成像可以是有用的。无监督域适应（UDA）允许将信息从源域转移至完全未标记的目标域。尽管UDA取得了实质性的改进，但其与其监督学习对应方法之间的性能差距仍然显著。通过选择少量目标样本进行注释并将其用于训练，主动域适应试图以最小的注释费用来缓解这种差距。我们提出了一种主动域适应方法，以检验将可见光谱与热成像模态结合的效率。当域差距明显较大时，

    The exploitation of visible spectrum datasets has led deep networks to show remarkable success. However, real-world tasks include low-lighting conditions which arise performance bottlenecks for models trained on large-scale RGB image datasets. Thermal IR cameras are more robust against such conditions. Therefore, the usage of thermal imagery in real-world applications can be useful. Unsupervised domain adaptation (UDA) allows transferring information from a source domain to a fully unlabeled target domain. Despite substantial improvements in UDA, the performance gap between UDA and its supervised learning counterpart remains significant. By picking a small number of target samples to annotate and using them in training, active domain adaptation tries to mitigate this gap with minimum annotation expense. We propose an active domain adaptation method in order to examine the efficiency of combining the visible spectrum and thermal imagery modalities. When the domain gap is considerably la
    
[^37]: 连续时间递归神经网络：概述及在监护病房血糖预测中的应用

    Continuous time recurrent neural networks: overview and application to forecasting blood glucose in the intensive care unit. (arXiv:2304.07025v1 [stat.ML])

    [http://arxiv.org/abs/2304.07025](http://arxiv.org/abs/2304.07025)

    本文介绍了连续时间自回归递归神经网络(CTRNNs)的应用, 通过连续演化来解决非规则采样的时间序列问题, 以概率预测临床监护设置中的血糖水平。

    

    在很多应用中，非规则采样的时间序列是常见的，包括医学领域。这提供了模型选择的挑战，通常需要插补或类似策略。连续时间自回归递归神经网络(CTRNNs) 是一种深度学习模型，通过在观测值之间融合隐藏状态的连续演化来解决这些问题。这是通过神经常微分方程(ODE)或神经流层来实现的。在本文中，我们概述了这些模型，包括为应对 ongoing medical interventions 等问题而提出的各种结构。此外，我们证明了这些模型在使用电子病历和模拟数据进行血糖概率预测的临床监护设置中的应用。实验证实了添加神经ODE或神经流层的一般性。

    Irregularly measured time series are common in many of the applied settings in which time series modelling is a key statistical tool, including medicine. This provides challenges in model choice, often necessitating imputation or similar strategies. Continuous time autoregressive recurrent neural networks (CTRNNs) are a deep learning model that account for irregular observations through incorporating continuous evolution of the hidden states between observations. This is achieved using a neural ordinary differential equation (ODE) or neural flow layer. In this manuscript, we give an overview of these models, including the varying architectures that have been proposed to account for issues such as ongoing medical interventions. Further, we demonstrate the application of these models to probabilistic forecasting of blood glucose in a critical care setting using electronic medical record and simulated data. The experiments confirm that addition of a neural ODE or neural flow layer general
    
[^38]: DIPNet：图像超分辨率的效率蒸馏与迭代修剪

    DIPNet: Efficiency Distillation and Iterative Pruning for Image Super-Resolution. (arXiv:2304.07018v1 [cs.CV])

    [http://arxiv.org/abs/2304.07018](http://arxiv.org/abs/2304.07018)

    该论文提出了一种名为DIPNet的新型多阶段轻量级网络增强方法，通过增强的高分辨率输出作为额外监督，提高轻量级学生网络的学习能力，并进一步采用重新参数化技术和迭代网络修剪等简化网络结构的方法，以实现高质量超分辨率的有效网络训练。

    

    现有基于深度学习的高效单幅图像超分辨率方法在性能上取得了显著的成果。然而，近期关于高效超分辨率的研究主要集中在通过各种网络设计减少参数数量和浮点运算。虽然这些方法可以降低参数数量和浮点运算，但并不一定缩短实际运行时间。为了解决这个问题，我们提出了一种新的多阶段轻量级网络增强方法，可以使轻量级网络实现出色的性能。具体地，我们利用增强的高分辨率输出作为额外的监督，提高轻量级学生网络的学习能力。在学生网络收敛后，我们进一步利用重新参数化技术和迭代网络修剪简化网络结构到更轻量级的水平。同时，我们采用有效的轻量级网络训练策略，使我们的方法不仅可以保证网络效率，同时也能在实现高质量超分辨率方面取得优异的性能。

    Efficient deep learning-based approaches have achieved remarkable performance in single image super-resolution. However, recent studies on efficient super-resolution have mainly focused on reducing the number of parameters and floating-point operations through various network designs. Although these methods can decrease the number of parameters and floating-point operations, they may not necessarily reduce actual running time. To address this issue, we propose a novel multi-stage lightweight network boosting method, which can enable lightweight networks to achieve outstanding performance. Specifically, we leverage enhanced high-resolution output as additional supervision to improve the learning ability of lightweight student networks. Upon convergence of the student network, we further simplify our network structure to a more lightweight level using reparameterization techniques and iterative network pruning. Meanwhile, we adopt an effective lightweight network training strategy that c
    
[^39]: 无监督的基于ANN的均衡器及其训练可编程门阵列实现

    Unsupervised ANN-Based Equalizer and Its Trainable FPGA Implementation. (arXiv:2304.06987v1 [eess.SP])

    [http://arxiv.org/abs/2304.06987](http://arxiv.org/abs/2304.06987)

    本文提出了一种无监督基于ANN的均衡器及其可训练的FPGA实现，通过自定义的损失函数使其能够适应不同的信道条件，并实现了高效率的通信系统。

    

    近年来，通信工程师强调基于人工神经网络（ANN）的算法，以提高系统及其组件的灵活性和自主性。在这个背景下，无监督训练具有特殊的意义，因为它可以在不传输导频符号的情况下进行自适应。本文介绍一种新颖的基于ANN的无监督均衡器及其可训练的现场可编程门阵列（FPGA）实现。我们证明了自定义的损失函数可以让ANN适应不同的信道条件，接近监督基线的性能。此外，为了实现实际的通信系统，我们设计了一个高效的FPGA实现，其吞吐量达到Gbit/s级别，远远超出高性能GPU的效率。

    In recent years, communication engineers put strong emphasis on artificial neural network (ANN)-based algorithms with the aim of increasing the flexibility and autonomy of the system and its components. In this context, unsupervised training is of special interest as it enables adaptation without the overhead of transmitting pilot symbols. In this work, we present a novel ANN-based, unsupervised equalizer and its trainable field programmable gate array (FPGA) implementation. We demonstrate that our custom loss function allows the ANN to adapt for varying channel conditions, approaching the performance of a supervised baseline. Furthermore, as a first step towards a practical communication system, we design an efficient FPGA implementation of our proposed algorithm, which achieves a throughput in the order of Gbit/s, outperforming a high-performance GPU by a large margin.
    
[^40]: 基于傅里叶神经算子的传递学习多保真度流场和温度场预测

    Multi-fidelity prediction of fluid flow and temperature field based on transfer learning using Fourier Neural Operator. (arXiv:2304.06972v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2304.06972](http://arxiv.org/abs/2304.06972)

    本文提出了基于傅里叶神经算子和传递学习的多保真度学习方法，可以用较少的高保真度数据预测流体流动和温度场，且性能优于其他方法。

    

    数据驱动的流体流动和温度分布预测在海洋和航空航天工程中得到了广泛的研究，并最近展示了其实时预测的潜力。然而，通常需要大量高保真度数据来描述和准确预测复杂的物理信息，而在现实中，由于高成本实验/计算，只有有限的高保真度数据可用。因此，本文提出了一种基于傅里叶神经算子的多保真度学习方法，在传递学习范式下，联合丰富的低保真度数据和有限的高保真度数据。首先，作为一个分辨率不变的算子，傅里叶神经算子首先被成功应用于直接集成多保真度数据，可以同时利用稀缺的高保真度数据和丰富的低保真度数据。然后，通过提取丰富的低保真度数据，为当前任务开发传递学习框架，可以帮助增强有限的高保真度数据的学习能力。实验结果表明，该方法可以通过更少的高保真度数据实现更准确的流体流动和温度场预测，并且优于许多最先进的方法。

    Data-driven prediction of fluid flow and temperature distribution in marine and aerospace engineering has received extensive research and demonstrated its potential in real-time prediction recently. However, usually large amounts of high-fidelity data are required to describe and accurately predict the complex physical information, while in reality, only limited high-fidelity data is available due to the high experiment/computational cost. Therefore, this work proposes a novel multi-fidelity learning method based on the Fourier Neural Operator by jointing abundant low-fidelity data and limited high-fidelity data under transfer learning paradigm. First, as a resolution-invariant operator, the Fourier Neural Operator is first and gainfully applied to integrate multi-fidelity data directly, which can utilize the scarce high-fidelity data and abundant low-fidelity data simultaneously. Then, the transfer learning framework is developed for the current task by extracting the rich low-fidelit
    
[^41]: 保留视觉Transformer中的局部特征以应用于类增量学习

    Preserving Locality in Vision Transformers for Class Incremental Learning. (arXiv:2304.06971v1 [cs.LG])

    [http://arxiv.org/abs/2304.06971](http://arxiv.org/abs/2304.06971)

    本文提出了一种Locality-Preserved Attention（LPA）层，以保留ViT中的局部特征以增强其在类增量学习中的性能。

    

    对于分类模型而言，能够在不忘记以前学到的知识的情况下学习新的类别对于其在实际应用中具有重要意义。在类增量学习方面，Vision Transformer（ViT）近来表现出了显著的性能。以往的研究主要集中于ViT的块设计和模型扩展。然而，本文发现在ViT进行增量训练时，注意力层逐渐失去对局部特征的关注能力。我们将这种现象称为ViT在类增量学习中的\emph{局部信息降解}。由于低级别的局部信息对于特征传递的效果至关重要，保留局部特征对于模型的性能提升具有积极的作用。因此，本文鼓励模型在训练过程中保留更多的局部信息，设计了一种Locality-Preserved Attention（LPA）层来强调局部特征的重要性，并将局部信息直接整合到原始注意力中以控制初始梯度。

    Learning new classes without forgetting is crucial for real-world applications for a classification model. Vision Transformers (ViT) recently achieve remarkable performance in Class Incremental Learning (CIL). Previous works mainly focus on block design and model expansion for ViTs. However, in this paper, we find that when the ViT is incrementally trained, the attention layers gradually lose concentration on local features. We call this interesting phenomenon as \emph{Locality Degradation} in ViTs for CIL. Since the low-level local information is crucial to the transferability of the representation, it is beneficial to preserve the locality in attention layers. In this paper, we encourage the model to preserve more local information as the training procedure goes on and devise a Locality-Preserved Attention (LPA) layer to emphasize the importance of local features. Specifically, we incorporate the local information directly into the vanilla attention and control the initial gradients 
    
[^42]: 基于自监督学习的单目图像深度估计

    Self-Supervised Learning based Depth Estimation from Monocular Images. (arXiv:2304.06966v1 [cs.CV])

    [http://arxiv.org/abs/2304.06966](http://arxiv.org/abs/2304.06966)

    本论文研究了基于自监督学习的单目图像深度估计，探索了多种扩展深度估计模型的方法，并计划实现姿态估计和语义分割等技术来提供更准确的深度图预测。

    

    深度估计在计算机视觉中有广泛的应用，如目标跟踪、增强现实和自动驾驶汽车。单目深度估计的目标是预测深度图，给定一个二维单目 RGB 图像作为输入。传统的深度估计方法基于深度线索，使用了诸如极线几何等概念。随着卷积神经网络的发展，深度估计已经取得了巨大的进展。在这个项目中，我们的目标是探索可能的深度估计模型的扩展，以及是否可以进一步提高性能度量。在更广泛的意义上，我们正在考虑实现姿态估计、高效亚像素卷积插值、语义分割估计技术，以进一步增强我们的提出的架构，并提供细粒度和更全局一致的深度图预测。我们还计划放弃相机内参数，并调查基于自监督学习的深度估计，其中训练数据是从单目 RGB 图像自动生成的，无需地面真实深度图。

    Depth Estimation has wide reaching applications in the field of Computer vision such as target tracking, augmented reality, and self-driving cars. The goal of Monocular Depth Estimation is to predict the depth map, given a 2D monocular RGB image as input. The traditional depth estimation methods are based on depth cues and used concepts like epipolar geometry. With the evolution of Convolutional Neural Networks, depth estimation has undergone tremendous strides. In this project, our aim is to explore possible extensions to existing SoTA Deep Learning based Depth Estimation Models and to see whether performance metrics could be further improved. In a broader sense, we are looking at the possibility of implementing Pose Estimation, Efficient Sub-Pixel Convolution Interpolation, Semantic Segmentation Estimation techniques to further enhance our proposed architecture and to provide fine-grained and more globally coherent depth map predictions. We also plan to do away with camera intrinsic 
    
[^43]: 使用软阈值的双层卷积神经网络的凸对偶理论分析

    Convex Dual Theory Analysis of Two-Layer Convolutional Neural Networks with Soft-Thresholding. (arXiv:2304.06959v1 [cs.LG])

    [http://arxiv.org/abs/2304.06959](http://arxiv.org/abs/2304.06959)

    本文通过设计一个凸对偶网络，从理论上分析了双层卷积神经网络中使用软阈值的凸性，并在实验中进行了验证。

    

    软阈值被广泛用于神经网络中，其基本网络结构为具有软阈值的双层卷积神经网络。由于网络的非线性和非凸性，训练过程严重依赖于网络参数的适当初始化，导致难以获得全局最优解。为了解决这个问题，本文设计了一个凸对偶网络。我们从理论上分析了网络的凸性，并在数值上证明了强对偶性。这个结论在线性拟合和去噪实验中得到了进一步验证。本文提供了一种凸化软阈值神经网络的新方法。

    Soft-thresholding has been widely used in neural networks. Its basic network structure is a two-layer convolution neural network with soft-thresholding. Due to the network's nature of nonlinearity and nonconvexity, the training process heavily depends on an appropriate initialization of network parameters, resulting in the difficulty of obtaining a globally optimal solution. To address this issue, a convex dual network is designed here. We theoretically analyze the network convexity and numerically confirm that the strong duality holds. This conclusion is further verified in the linear fitting and denoising experiments. This work provides a new way to convexify soft-thresholding neural networks.
    
[^44]: 基于文化意识的 COVID-19 疫苗犹豫现象的机器学习分析

    Cultural-aware Machine Learning based Analysis of COVID-19 Vaccine Hesitancy. (arXiv:2304.06953v1 [cs.SI])

    [http://arxiv.org/abs/2304.06953](http://arxiv.org/abs/2304.06953)

    本研究提出一种文化意识机器学习（ML）模型，可以预测COVID-19疫苗接种意愿，并揭示了西班牙裔和非洲裔美国人最有可能受到文化差异的影响，这有助于未来制定更成功的疫苗接种运动。

    

    理解 COVID-19 疫苗犹豫现象（如何以及为什么犹豫）对于控制流行病最有效的方法之一，即大规模接种疫苗非常重要。这种理解还为未来的流行病设计成功的接种运动提供了见解。不幸的是，从文化的角度来看，有很多因素涉及决定是否接种疫苗。为了实现这些目标，我们设计了一种新的文化意识机器学习（ML）模型，基于我们的新数据收集，用于预测接种意愿。我们进一步使用先进的 AI 解释器分析了对 ML 模型预测有贡献的最重要特征，如概率图模型（PGM）和 Shapley Additive Explanations（SHAP）。这些分析揭示了最有可能影响疫苗接种决策的关键因素。我们的研究发现，西班牙裔和非洲裔美国人最有可能受到文化差异的影响。

    Understanding the COVID-19 vaccine hesitancy, such as who and why, is very crucial since a large-scale vaccine adoption remains as one of the most efficient methods of controlling the pandemic. Such an understanding also provides insights into designing successful vaccination campaigns for future pandemics. Unfortunately, there are many factors involving in deciding whether to take the vaccine, especially from the cultural point of view. To obtain these goals, we design a novel culture-aware machine learning (ML) model, based on our new data collection, for predicting vaccination willingness. We further analyze the most important features which contribute to the ML model's predictions using advanced AI explainers such as the Probabilistic Graphical Model (PGM) and Shapley Additive Explanations (SHAP). These analyses reveal the key factors that most likely impact the vaccine adoption decisions. Our findings show that Hispanic and African American are most likely impacted by cultural cha
    
[^45]: 基于深度学习模型的PPG信号用于高血压诊断：一种新方法

    PPG Signals for Hypertension Diagnosis: A Novel Method using Deep Learning Models. (arXiv:2304.06952v1 [cs.LG])

    [http://arxiv.org/abs/2304.06952](http://arxiv.org/abs/2304.06952)

    本研究提出了基于PPG信号和深度学习模型的高血压分级新方法，经实验验证其在高血压诊断方面具有潜在应用价值。

    

    高血压是由于血压高引起的一种常见病，将其分为不同的阶段对于管理该疾病非常重要。本研究提出了一种基于光容积脉搏图（PPG）信号和深度学习模型的高血压分级新方法，名为AvgPool_VGG-16。 PPG信号是通过使用测量组织微血管中血容量变化的光传感器来测量血压的非侵入性方法。使用公开可用的血压分类数据集中的PPG图像来训练模型。进行了各种PPG阶段的多类分类。结果显示，所提出的方法在高血压分级中达到了较高的准确性，证明了PPG信号和深度学习模型在高血压诊断和管理中的潜力。

    Hypertension is a medical condition characterized by high blood pressure, and classifying it into its various stages is crucial to managing the disease. In this project, a novel method is proposed for classifying stages of hypertension using Photoplethysmography (PPG) signals and deep learning models, namely AvgPool_VGG-16. The PPG signal is a non-invasive method of measuring blood pressure through the use of light sensors that measure the changes in blood volume in the microvasculature of tissues. PPG images from the publicly available blood pressure classification dataset were used to train the model. Multiclass classification for various PPG stages were done. The results show the proposed method achieves high accuracy in classifying hypertension stages, demonstrating the potential of PPG signals and deep learning models in hypertension diagnosis and management.
    
[^46]: TimelyFL：面向异构异步联邦学习的自适应部分训练

    TimelyFL: Heterogeneity-aware Asynchronous Federated Learning with Adaptive Partial Training. (arXiv:2304.06947v1 [cs.LG])

    [http://arxiv.org/abs/2304.06947](http://arxiv.org/abs/2304.06947)

    TimelyFL提出了一种面向异构异步联邦学习的自适应部分训练框架，在训练过程中根据设备的异构性和通信模式调整汇总计划，并自适应地选择要更新模型的设备子集，从而克服了现有方法在面对异构网络时可能导致的训练精度下降和收敛速度变慢的问题。

    

    在跨设备联邦学习环境中，同步联邦学习方法的规模化存在挑战，因为滞后者会阻碍训练过程。此外，由于系统异构性和间歇连接，每个客户端加入训练的可用性随时间高度变化。最近提出了异步联邦学习方法（例如FedBuff）以克服这些问题，允许较慢的用户基于旧模型继续进行本地训练，并在准备好时进行贡献汇总。然而，我们经验证明，这种方法可能会导致训练精度大幅下降，收敛速度变慢。主要原因是快速设备对许多汇总轮次做出贡献，而其他设备不定期加入，模型更新也过时了。为克服这一障碍，我们提出了TimelyFL，一种面向异构异步联邦学习的自适应部分训练框架。在训练过程中，TimelyFL根据每个设备的系统异构性和通信模式来调整汇总计划，并自适应地选择每轮要更新模型的设备子集。因此，在实际异构网络条件下，我们的方法可以实现更快的收敛和更高的精度。

    In cross-device Federated Learning (FL) environments, scaling synchronous FL methods is challenging as stragglers hinder the training process. Moreover, the availability of each client to join the training is highly variable over time due to system heterogeneities and intermittent connectivity. Recent asynchronous FL methods (e.g., FedBuff) have been proposed to overcome these issues by allowing slower users to continue their work on local training based on stale models and to contribute to aggregation when ready. However, we show empirically that this method can lead to a substantial drop in training accuracy as well as a slower convergence rate. The primary reason is that fast-speed devices contribute to many more rounds of aggregation while others join more intermittently or not at all, and with stale model updates. To overcome this barrier, we propose TimelyFL, a heterogeneity-aware asynchronous FL framework with adaptive partial training. During the training, TimelyFL adjusts the 
    
[^47]: AUTOSPARSE:实现神经网络自动稀疏训练的方法研究

    AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks. (arXiv:2304.06941v1 [cs.LG])

    [http://arxiv.org/abs/2304.06941](http://arxiv.org/abs/2304.06941)

    本文提出了一种叫做AutoSparse的自动稀疏训练算法，其中包含梯度退火法来权衡稀疏和准确性，在ResNet50和MobileNetV1上表现出更好的准确性和较少的计算资源需求。

    

    稀疏训练是减少训练神经网络计算成本的一个有前途的途径。最近的研究提出了使用可学习阈值的修剪方法，以有效地探索模型中内在稀疏性的不均匀分布。本文提出了梯度退火法（GA），其中掩码权重的梯度按非线性方式缩小。 GA在不需要额外稀疏诱导正则化的情况下提供了一种优美的权衡稀疏性和准确性的方法。我们将GA与最新的可学习修剪方法相结合，创建了一种称为AutoSparse的自动稀疏训练算法，它在ImageNet-1K上的稀疏ResNet50和MobileNetV1上实现了更好的准确性和/或训练/推理FLOPS减少，例如AutoSparse在80％的稀疏下，ResNet50在ImageNet上实现了（2倍，7倍）的（训练，推理）FLOPS减少。最后，AutoSparse在创新性稀疏领域表现优异。

    Sparse training is emerging as a promising avenue for reducing the computational cost of training neural networks. Several recent studies have proposed pruning methods using learnable thresholds to efficiently explore the non-uniform distribution of sparsity inherent within the models. In this paper, we propose Gradient Annealing (GA), where gradients of masked weights are scaled down in a non-linear manner. GA provides an elegant trade-off between sparsity and accuracy without the need for additional sparsity-inducing regularization. We integrated GA with the latest learnable pruning methods to create an automated sparse training algorithm called AutoSparse, which achieves better accuracy and/or training/inference FLOPS reduction than existing learnable pruning methods for sparse ResNet50 and MobileNetV1 on ImageNet-1K: AutoSparse achieves (2x, 7x) reduction in (training,inference) FLOPS for ResNet50 on ImageNet at 80% sparsity. Finally, AutoSparse outperforms sparse-to-sparse SotA me
    
[^48]: 使用机器学习模型分类社交媒体有害评论

    Classification of social media Toxic comments using Machine learning models. (arXiv:2304.06934v1 [cs.LG])

    [http://arxiv.org/abs/2304.06934](http://arxiv.org/abs/2304.06934)

    该论文提出使用Lstm-cnn模型分类器，从而更好地区分社交媒体平台上有害和非有害评论，以解决网络欺凌问题。

    

    摘要概述了社交媒体平台上有害评论的问题。个别人使用不尊重、滥用和不合理的语言，可能会驱使用户离开讨论。这种行为被称为反社会行为，经常出现在在线辩论、评论和争斗中。含有明确不良语言的评论可以被归类为多种类型，如有害、严重有害、淫秽、威胁、侮辱和身份仇恨。这种行为导致网络骚扰和网络欺凌，迫使个人停止表达自己的观点和想法。为了保护用户免受冒犯性言论，公司已经开始标记评论和阻止用户。摘要提出了使用Lstm-cnn模型创建分类器，以高准确率区分有害和非有害评论。分类器可以帮助组织更好地检查评论区的有害程度。

    The abstract outlines the problem of toxic comments on social media platforms, where individuals use disrespectful, abusive, and unreasonable language that can drive users away from discussions. This behavior is referred to as anti-social behavior, which occurs during online debates, comments, and fights. The comments containing explicit language can be classified into various categories, such as toxic, severe toxic, obscene, threat, insult, and identity hate. This behavior leads to online harassment and cyberbullying, which forces individuals to stop expressing their opinions and ideas. To protect users from offensive language, companies have started flagging comments and blocking users. The abstract proposes to create a classifier using an Lstm-cnn model that can differentiate between toxic and non-toxic comments with high accuracy. The classifier can help organizations examine the toxicity of the comment section better.
    
[^49]: 面向医学图像分类中标签集不匹配的规模化联邦学习

    Scale Federated Learning for Label Set Mismatch in Medical Image Classification. (arXiv:2304.06931v1 [cs.CV])

    [http://arxiv.org/abs/2304.06931](http://arxiv.org/abs/2304.06931)

    本文提出了FedLSM框架以解决医学图像分类中标签集不匹配问题，该框架采用不同的训练策略以有效利用未标记或部分标记的数据，并在分类层采用逐类别自适应聚合，适用于多个标签集的场景。

    

    联邦学习（FL）作为一种去中心化的学习范式，在医疗保健领域得到了广泛应用，允许多个参与方在不泄露隐私的前提下协同训练模型。然而，以往的研究大多假定每个客户端拥有相同的标签集。事实上，医学专家往往只注释其知识领域或兴趣范围内的疾病。这意味着每个客户端的标签集可能是不同甚至是不相交的。本文提出了FedLSM框架以解决标签集不匹配问题。FedLSM采用不同的训练策略处理不同不确定性程度的数据，以有效利用未标记或部分标记的数据，并在分类层采用逐类别自适应聚合以避免客户端缺失标签时的不准确聚合。我们在两个公共实际医学图像数据集上评估了FedLSM，包括拥有112,120张胸透图像的胸部X射线诊断和皮肤病变诊断。

    Federated learning (FL) has been introduced to the healthcare domain as a decentralized learning paradigm that allows multiple parties to train a model collaboratively without privacy leakage. However, most previous studies have assumed that every client holds an identical label set. In reality, medical specialists tend to annotate only diseases within their knowledge domain or interest. This implies that label sets in each client can be different and even disjoint. In this paper, we propose the framework FedLSM to solve the problem Label Set Mismatch. FedLSM adopts different training strategies on data with different uncertainty levels to efficiently utilize unlabeled or partially labeled data as well as class-wise adaptive aggregation in the classification layer to avoid inaccurate aggregation when clients have missing labels. We evaluate FedLSM on two public real-world medical image datasets, including chest x-ray (CXR) diagnosis with 112,120 CXR images and skin lesion diagnosis wit
    
[^50]: 解释性是一种安全：一种基于解释器的集成防御对抗攻击方法

    Interpretability is a Kind of Safety: An Interpreter-based Ensemble for Adversary Defense. (arXiv:2304.06919v1 [cs.LG])

    [http://arxiv.org/abs/2304.06919](http://arxiv.org/abs/2304.06919)

    本论文揭示了解释器与对抗样本生成过程之间的相关性，提出了一种基于解释器的集成框架X-Ensemble，该框架采用了新颖的检测-矫正过程，能够进行强大的防御。

    

    深度神经网络在现实应用中取得了巨大成功，然而它们对抗攻击的脆弱性一直受到批评。为了减轻对抗攻击威胁，研究人员已经做出了巨大的努力，但对抗样本的本质特征尚不清楚，大多数现有方法仍然容易受到混合攻击和反制攻击。在这篇论文中，我们首先揭示了敏感性分析型DNN解释器与对抗样本生成过程之间的梯度相关性，这表明了对抗攻击的弱点，并为将DNN的两个长期挑战——脆弱性和不可解释性联系在一起提供了思路。我们提出了一个名为X-Ensemble的基于解释器的集成框架来进行强大的防御。X-Ensemble采用了一种新颖的检测-矫正过程，并在构建多个子检测器和一个检测器集合上具有特色。

    While having achieved great success in rich real-life applications, deep neural network (DNN) models have long been criticized for their vulnerability to adversarial attacks. Tremendous research efforts have been dedicated to mitigating the threats of adversarial attacks, but the essential trait of adversarial examples is not yet clear, and most existing methods are yet vulnerable to hybrid attacks and suffer from counterattacks. In light of this, in this paper, we first reveal a gradient-based correlation between sensitivity analysis-based DNN interpreters and the generation process of adversarial examples, which indicates the Achilles's heel of adversarial attacks and sheds light on linking together the two long-standing challenges of DNN: fragility and unexplainability. We then propose an interpreter-based ensemble framework called X-Ensemble for robust adversary defense. X-Ensemble adopts a novel detection-rectification process and features in building multiple sub-detectors and a 
    
[^51]: 通过掩盖无关参数，生成更具可转移性的对抗样本

    Generating Adversarial Examples with Better Transferability via Masking Unimportant Parameters of Surrogate Model. (arXiv:2304.06908v1 [cs.LG])

    [http://arxiv.org/abs/2304.06908](http://arxiv.org/abs/2304.06908)

    本文提出的MUP方法通过掩盖无关参数的方式，从而生成更具可转移性的对抗性样本，提高了攻击成功率和样本的转移能力。

    

    深度神经网络（DNN）已被证明容易受到对抗性样本的攻击。在近年来，对抗性样本的可转移性也受到广泛关注，这意味着由代理模型生成的对抗性样本也可以攻击未知模型。这一现象产生了基于转移的对抗攻击，旨在提高生成的对抗性样本的转移能力。本文提出了一种掩盖无关参数（MUP）的方法，以提高基于转移攻击的对抗性样本的转移性。MUP的关键思想是通过改进预训练的代理模型来提高基于转移攻击的攻击。基于该思想，使用基于泰勒展开的度量方法评估参数重要性得分，并在生成对抗样本时掩盖无关重要的参数。这个过程简单易行，可以自然地与各种现有的基于梯度的优化器结合使用，以生成对抗性的样本。实验结果表明，我们的MUP方法在各种目标模型上获得了更高的攻击成功率，并增加了对抗性样本的转移能力。

    Deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples. Moreover, the transferability of the adversarial examples has received broad attention in recent years, which means that adversarial examples crafted by a surrogate model can also attack unknown models. This phenomenon gave birth to the transfer-based adversarial attacks, which aim to improve the transferability of the generated adversarial examples. In this paper, we propose to improve the transferability of adversarial examples in the transfer-based attack via masking unimportant parameters (MUP). The key idea in MUP is to refine the pretrained surrogate models to boost the transfer-based attack. Based on this idea, a Taylor expansion-based metric is used to evaluate the parameter importance score and the unimportant parameters are masked during the generation of adversarial examples. This process is simple, yet can be naturally combined with various existing gradient-based optimizers for generating
    
[^52]: 基于边缘化耦合字典学习的实时图像注释方法

    Toward Real-Time Image Annotation Using Marginalized Coupled Dictionary Learning. (arXiv:2304.06907v1 [cs.CV])

    [http://arxiv.org/abs/2304.06907](http://arxiv.org/abs/2304.06907)

    本文提出了一种基于耦合字典学习和边缘化损失函数的实时图像注释方法，该方法能够学习有限数量的视觉原型和相应的语义，并保持标签的稀疏不平衡性，取得了良好的实验效果。

    

    在大多数图像检索系统中，图像包含各种高层语义，被称为标签或注释。几乎所有处理非均衡标记的最先进的图像注释方法都是基于搜索的技术，这些技术非常耗时。本文提出了一种新的耦合字典学习方法，同时学习有限数量的视觉原型和相应的语义，从而实现了实时图像注释过程。本文的另一个贡献是使用边缘损失函数，而不是对于不平衡标签的图像注释不合适的平方损失函数。在我们的方法中，我们使用了边缘化损失函数来利用一种简单有效的原型更新方法。同时，我们在语义原型上引入了${\ell}_1$正则化，以保持学习语义原型中标签的稀疏不平衡性。最后，进行了全面的实验验证。

    In most image retrieval systems, images include various high-level semantics, called tags or annotations. Virtually all the state-of-the-art image annotation methods that handle imbalanced labeling are search-based techniques which are time-consuming. In this paper, a novel coupled dictionary learning approach is proposed to learn a limited number of visual prototypes and their corresponding semantics simultaneously. This approach leads to a real-time image annotation procedure. Another contribution of this paper is that utilizes a marginalized loss function instead of the squared loss function that is inappropriate for image annotation with imbalanced labels. We have employed a marginalized loss function in our method to leverage a simple and effective method of prototype updating. Meanwhile, we have introduced ${\ell}_1$ regularization on semantic prototypes to preserve the sparse and imbalanced nature of labels in learned semantic prototypes. Finally, comprehensive experimental resu
    
[^53]: 系统性公平性

    Systemic Fairness. (arXiv:2304.06901v1 [cs.LG])

    [http://arxiv.org/abs/2304.06901](http://arxiv.org/abs/2304.06901)

    本文解决了算法公平性文献中更广泛的系统公平性问题，并为其开发了形式主义。

    

    机器学习算法在越来越多的场景中用于做出或支持决策。随着广泛使用，对这种方法的公平性也日益关注。以前关于算法公平性的文献广泛探讨了风险，并且在许多情况下提出了管理某些风险的方法。然而，大多数研究都集中在由单个决策制定者或代理人引起的公平性问题上。相反，大多数现实系统都有许多代理人，他们共同作为更大生态系统的一部分工作。例如，在贷款场景中，有多个放贷人评估申请者的贷款，同时还有决策制定者和其他机构其决策也会影响结果。因此，单个决策制定者做出的任何放贷决策的更广泛影响可能取决于生态系统中多个不同代理人的行动。本文为企业和系统公平性开发了形式主义，并呼吁算法公平性文献更加关注后者。

    Machine learning algorithms are increasingly used to make or support decisions in a wide range of settings. With such expansive use there is also growing concern about the fairness of such methods. Prior literature on algorithmic fairness has extensively addressed risks and in many cases presented approaches to manage some of them. However, most studies have focused on fairness issues that arise from actions taken by a (single) focal decision-maker or agent. In contrast, most real-world systems have many agents that work collectively as part of a larger ecosystem. For example, in a lending scenario, there are multiple lenders who evaluate loans for applicants, along with policymakers and other institutions whose decisions also affect outcomes. Thus, the broader impact of any lending decision of a single decision maker will likely depend on the actions of multiple different agents in the ecosystem. This paper develops formalisms for firm versus systemic fairness, and calls for a greater
    
[^54]: 神经网络下的执行预测

    Performative Prediction with Neural Networks. (arXiv:2304.06879v1 [cs.LG])

    [http://arxiv.org/abs/2304.06879](http://arxiv.org/abs/2304.06879)

    本文提出了执行预测的框架，通过找到具有执行稳定性的分类器来适用于数据分布。通过假设数据分布相对于模型的预测值可Lipschitz连续，使得我们能够放宽对损失函数的假设要求。

    

    执行预测是一种学习模型并影响其预测数据的框架。本文旨在找到分类器，使其具有执行稳定性，即适用于其产生的数据分布的最佳分类器。在使用重复风险最小化方法找到具有执行稳定性的分类器的标准收敛结果中，假设数据分布对于模型参数是可Lipschitz连续的。在这种情况下，损失必须对这些参数强凸和平滑；否则，该方法将在某些问题上发散。然而本文则假设数据分布是相对于模型的预测值可Lipschitz连续的，这是执行系统的更加自然的假设。结果，我们能够显著放宽对损失函数的假设要求。作为一个说明，我们介绍了一种建模真实数据分布的重采样过程，并使用其来实证执行稳定性相对于其他目标的效益。

    Performative prediction is a framework for learning models that influence the data they intend to predict. We focus on finding classifiers that are performatively stable, i.e. optimal for the data distribution they induce. Standard convergence results for finding a performatively stable classifier with the method of repeated risk minimization assume that the data distribution is Lipschitz continuous to the model's parameters. Under this assumption, the loss must be strongly convex and smooth in these parameters; otherwise, the method will diverge for some problems. In this work, we instead assume that the data distribution is Lipschitz continuous with respect to the model's predictions, a more natural assumption for performative systems. As a result, we are able to significantly relax the assumptions on the loss function. In particular, we do not need to assume convexity with respect to the model's parameters. As an illustration, we introduce a resampling procedure that models realisti
    
[^55]: 不需重新搜索的研究：最大更新参数化可精确预测跨尺度的损失

    Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])

    [http://arxiv.org/abs/2304.06875](http://arxiv.org/abs/2304.06875)

    提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。

    

    随着语言模型的扩大，验证研究想法变得越来越昂贵，因为小模型的结论不能简单地转移到大模型。解决方案是建立一个通用系统，仅基于小模型的结果和超参数直接预测大模型的一些指标。现有的基于缩放律的方法需要在最大的模型上进行超参数搜索，但由于资源有限，这是不切实际的。我们通过展示我们的发现表明，最大更新参数化（muP）使得可以在靠近常见损失流域的超参数的情况下准确拟合超参数的缩放律，而不需要任何搜索。因此，不同的模型可以在大尺度上进行损失预测，在训练开始之前就可以进行直接比较。我们提出了一种新的范式，作为可靠的学术研究的第一步，适用于任何模型规模，而不需大量的计算。代码将很快公开可用。

    As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
    
[^56]: 速度与音调：理解自监督节奏估计

    Tempo vs. Pitch: understanding self-supervised tempo estimation. (arXiv:2304.06868v1 [cs.SD])

    [http://arxiv.org/abs/2304.06868](http://arxiv.org/abs/2304.06868)

    在音乐信息检索中，本文通过严格实验剖析了一种应用于节奏估计的自监督音高估计模型，着重研究了自监督节奏估计中输入表述和数据分布的关系。

    

    自监督方法通过解决不需要人工标注的先决任务来学习表示方法，减轻了需要耗费时间的标注工作。这些方法已被应用于计算机视觉、自然语言处理、环境声音分析，最近还用于音乐信息检索，例如音高估计。尤其是在音乐领域，这些模型在不同数据分布下的脆弱性以及如何缓解这些问题方面的见解还很少。在本文中，我们通过对合成数据进行严格的实验，剖析了一种自监督音高估计模型并将其应用于节奏估计，探讨了输入表示和数据分布与自监督节奏估计之间的关系。

    Self-supervision methods learn representations by solving pretext tasks that do not require human-generated labels, alleviating the need for time-consuming annotations. These methods have been applied in computer vision, natural language processing, environmental sound analysis, and recently in music information retrieval, e.g. for pitch estimation. Particularly in the context of music, there are few insights about the fragility of these models regarding different distributions of data, and how they could be mitigated. In this paper, we explore these questions by dissecting a self-supervised model for pitch estimation adapted for tempo estimation via rigorous experimentation with synthetic data. Specifically, we study the relationship between the input representation and data distribution for self-supervised tempo estimation.
    
[^57]: 对最新大规模预训练模型中的社会偏见进行评估

    Evaluation of Social Biases in Recent Large Pre-Trained Models. (arXiv:2304.06861v1 [cs.CL])

    [http://arxiv.org/abs/2304.06861](http://arxiv.org/abs/2304.06861)

    本文研究了最近发布的三个预训练模型的偏见问题，并评估了它们在两个偏见基准上的表现，探讨了是否随着技术进步，最新的、更快、更轻的模型在开发时负责任地降低了与旧模型相比的社会偏见。

    

    大规模预训练语言模型广泛应用在社区中，通常使用来自于互联网等开放来源的未审核或未筛选的数据进行训练。由于这一点，我们在在线平台上看到的偏见反映了社会上的偏见，并被这些模型所捕捉和学习。这些模型被应用于影响数百万人的应用程序中，它们内在的偏见对于定向的社会群体是有害的。在本文中，我们研究新预训练模型发布后的偏见缩减趋势。选择了三个最新模型(ELECTRA、DeBERTa和DistilBERT)，并对两个偏见基准（StereoSet和CrowS-Pairs）进行评估。它们使用相关度量标准与BERT进行比较。我们探索是否随着技术的进步和新的、更快、更轻的模型发布，它们是否负责任地发展，使其内在的社会偏见与旧模型相比有所降低？

    Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. In this work, we study the general trend in bias reduction as newer pre-trained models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT) are chosen and evaluated against two bias benchmarks, StereoSet and CrowS-Pairs. They are compared to the baseline of BERT using the associated metrics. We explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? The re
    
[^58]: Vax-Culture: 用于研究推特上疫苗讨论的数据集

    Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v1 [cs.SI])

    [http://arxiv.org/abs/2304.06858](http://arxiv.org/abs/2304.06858)

    本文介绍了一个推特疫苗数据集Vax-Culture，它旨在找出推广疫苗错误信息的文化和政治信念的重叠部分，帮助开发机器学习模型以自动检测疫苗错误信息帖子并应对其负面影响。

    

    COVID-19疫情期间，疫苗犹豫继续是公共卫生官员面临的主要挑战。由于该犹豫破坏了疫苗运动，许多研究人员试图确定其根本原因，并发现社交媒体平台上反疫苗错误信息的不断增长是该问题的关键因素。我们将推特作为误导内容的来源，并旨在提取推广疫苗错误信息的文化和政治信念的重叠部分。为此，我们收集了一个与疫苗有关的推文数据集，并借助专业沟通和新闻背景的注释人员进行注释。我们最终希望这可以带来有效和有针对性的公共卫生通信策略，以接触那些持反疫苗信仰者。此外，这些信息有助于开发机器学习模型以自动检测疫苗错误信息帖子并应对其负面影响。

    Vaccine hesitancy continues to be a main challenge for public health officials during the COVID-19 pandemic. As this hesitancy undermines vaccine campaigns, many researchers have sought to identify its root causes, finding that the increasing volume of anti-vaccine misinformation on social media platforms is a key element of this problem. We explored Twitter as a source of misleading content with the goal of extracting overlapping cultural and political beliefs that motivate the spread of vaccine misinformation. To do this, we have collected a data set of vaccine-related Tweets and annotated them with the help of a team of annotators with a background in communications and journalism. Ultimately we hope this can lead to effective and targeted public health communication strategies for reaching individuals with anti-vaccine beliefs. Moreover, this information helps with developing Machine Learning models to automatically detect vaccine misinformation posts and combat their negative impa
    
[^59]: CAR-DESPOT: 针对混杂环境下的机器人的因果关系在线POMDP规划

    CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v1 [cs.RO])

    [http://arxiv.org/abs/2304.06848](http://arxiv.org/abs/2304.06848)

    本文提出了一种新的因果关系在线POMDP规划方法CAR-DESPOT，使用因果建模和推理来消除未测量混淆变量引起的错误，并在混杂环境中表现优异。

    

    在现实环境中工作的机器人必须考虑随机行为的可能结果，并根据真实的世界状态的部分观察进行决策。因果混淆的问题是进行准确和强健的行为预测的主要挑战。部分可观察的马尔可夫决策过程(POMDP)是一种广泛使用的框架，用于模拟这些随机和部分可观测的决策问题。然而，由于缺乏明确的因果语义，POMDP规划方法容易受到混淆偏差的影响，在未观察到混杂变量的情况下，可能会产生表现不佳的策略。本文提出了一种新的因果关系在线POMDP规划方法，使用因果建模和推理来消除未测量混淆变量引起的错误。我们进一步提出了一种从观测数据中学习因果模型的方法，以在我们的方法中使用。实验结果表明，我们的方法CAR-DESPOT在混杂环境中比现有的最先进的POMDP规划程序表现显著更好。

    Robots operating in real-world environments must reason about possible outcomes of stochastic actions and make decisions based on partial observations of the true world state. A major challenge for making accurate and robust action predictions is the problem of confounding, which if left untreated can lead to prediction errors. The partially observable Markov decision process (POMDP) is a widely-used framework to model these stochastic and partially-observable decision-making problems. However, due to a lack of explicit causal semantics, POMDP planning methods are prone to confounding bias and thus in the presence of unobserved confounders may produce underperforming policies. This paper presents a novel causally-informed extension of "anytime regularized determinized sparse partially observable tree" (AR-DESPOT), a modern anytime online POMDP planner, using causal modelling and inference to eliminate errors caused by unmeasured confounder variables. We further propose a method to lear
    
[^60]: PIE: 针对大规模推荐系统的个性化兴趣探索

    PIE: Personalized Interest Exploration for Large-Scale Recommender Systems. (arXiv:2304.06844v1 [cs.IR])

    [http://arxiv.org/abs/2304.06844](http://arxiv.org/abs/2304.06844)

    本文提出了一个用于大规模推荐系统的探索框架，包括用户-创建者探索、在线探索框架和反馈组成机制，以解决推荐系统中流行内容的限制和无法系统探索用户兴趣的挑战，从而提高推荐的整体质量和训练数据的有效性。

    

    推荐系统越来越成功地向用户推荐个性化内容。然而，这些系统常常利用热门内容，且用户兴趣的持续演进需要被捕捉，但没有直接的方式来系统性地探索用户的兴趣。这也经常影响到推荐系统的整体质量，因为训练数据是从推荐给用户的候选项中生成的。本文提出了一种用于大规模推荐系统中的探索框架来解决这些挑战。它由三个部分组成：第一部分是用户创建者探索，专注于识别用户感兴趣的最佳创建者，第二部分是在线探索框架，第三部分是一个平衡探索与利用的反馈组成机制，以确保探索性视频的最佳普及率。我们的方法可以很容易地被集成到现有的大规模推荐系统中，只需进行最小限度的修改。

    Recommender systems are increasingly successful in recommending personalized content to users. However, these systems often capitalize on popular content. There is also a continuous evolution of user interests that need to be captured, but there is no direct way to systematically explore users' interests. This also tends to affect the overall quality of the recommendation pipeline as training data is generated from the candidates presented to the user. In this paper, we present a framework for exploration in large-scale recommender systems to address these challenges. It consists of three parts, first the user-creator exploration which focuses on identifying the best creators that users are interested in, second the online exploration framework and third a feed composition mechanism that balances explore and exploit to ensure optimal prevalence of exploratory videos. Our methodology can be easily integrated into an existing large-scale recommender system with minimal modifications. We 
    
[^61]: 无监督学习局部和全局特征用于视频对齐

    Video alignment using unsupervised learning of local and global features. (arXiv:2304.06841v1 [cs.CV])

    [http://arxiv.org/abs/2304.06841](http://arxiv.org/abs/2304.06841)

    本文提出了一种无需训练的视频对齐方法，利用全局和局部特征将帧转化为时间序列并使用对角化动态时间规整算法进行对齐。

    

    本文致力于解决视频对齐的问题，即匹配包含相似活动的一对视频的帧。视频对齐的主要挑战在于，尽管两个视频之间的执行过程和外观有所不同，但仍需要建立精确的对应关系。我们提出了一种使用帧的全局和局部特征进行对齐的无监督方法。特别地，我们利用人物检测、姿态估计和VGG网络三种机器视觉工具为每个视频帧引入有效的特征。然后对这些特征进行处理和组合以构建代表视频的多维时间序列。使用一种名为对角化动态时间规整的新版本（Diagonalized Dynamic Time Warping, DDTW）对生成的时间序列进行对齐。我们的方法的主要优点在于不需要任何训练，因此适用于任何新类型的活动而无需处理。

    In this paper, we tackle the problem of video alignment, the process of matching the frames of a pair of videos containing similar actions. The main challenge in video alignment is that accurate correspondence should be established despite the differences in the execution processes and appearances between the two videos. We introduce an unsupervised method for alignment that uses global and local features of the frames. In particular, we introduce effective features for each video frame by means of three machine vision tools: person detection, pose estimation, and VGG network. Then the features are processed and combined to construct a multidimensional time series that represent the video. The resulting time series are used to align videos of the same actions using a novel version of dynamic time warping named Diagonalized Dynamic Time Warping(DDTW). The main advantage of our approach is that no training is required, which makes it applicable for any new type of action without any need
    
[^62]: 多任务深度神经网络的结构化剪枝

    Structured Pruning for Multi-Task Deep Neural Networks. (arXiv:2304.06840v1 [cs.LG])

    [http://arxiv.org/abs/2304.06840](http://arxiv.org/abs/2304.06840)

    本研究探索了在多任务模型上应用结构化剪枝的有效性，通过实验发现，在参数数量相似的情况下，来自不同剪枝方法的架构在任务性能上没有显着差异，迭代结构剪枝可能不是实现最优结构的最佳方法。

    

    虽然相对于单个单任务DNN模型，多任务深度神经网络模型具有计算和存储优势，但是它们可以通过模型压缩进一步优化。许多结构化剪枝方法已经被开发出来，可以轻松地实现单任务模型的加速，但是对于多任务网络的剪枝尚未得到广泛研究。在这项工作中，我们研究了结构化剪枝在多任务模型上的有效性。我们使用现有的单任务滤波器剪枝准则，并引入了一个基于MTL的滤波器剪枝准则来估计滤波器重要性分数。我们使用迭代剪枝策略使用两种剪枝方法来剪枝模型。我们展示了，在仔细的超参数调整下，当参数数量相似时，来自不同剪枝方法的架构在任务之间的性能上没有显着差异。我们还展示了迭代结构剪枝可能不是实现多任务网络最优结构的最佳方法。

    Although multi-task deep neural network (DNN) models have computation and storage benefits over individual single-task DNN models, they can be further optimized via model compression. Numerous structured pruning methods are already developed that can readily achieve speedups in single-task models, but the pruning of multi-task networks has not yet been extensively studied. In this work, we investigate the effectiveness of structured pruning on multi-task models. We use an existing single-task filter pruning criterion and also introduce an MTL-based filter pruning criterion for estimating the filter importance scores. We prune the model using an iterative pruning strategy with both pruning methods. We show that, with careful hyper-parameter tuning, architectures obtained from different pruning methods do not have significant differences in their performances across tasks when the number of parameters is similar. We also show that iterative structure pruning may not be the best way to ac
    
[^63]: 数字子载波复用系统中光信道非线性补偿的神经网络结构

    Neural Network Architectures for Optical Channel Nonlinear Compensation in Digital Subcarrier Multiplexing Systems. (arXiv:2304.06836v1 [cs.IT])

    [http://arxiv.org/abs/2304.06836](http://arxiv.org/abs/2304.06836)

    本研究提出了使用各种人工神经网络（ANN）结构在数字子载波复用（DSCM）光传输系统中模拟和补偿光纤非线性干扰，通过应用全连接网络和模块化结构来完成非线性信道均衡，为未来光学通信解决方案提供了现实的解决方案。

    

    本研究提出了使用不同的人工神经网络（ANN）结构来对数字子载波复用（DSCM）光传输系统中的子载波内部和互间光纤非线性干扰建模和补偿。我们采用卷积神经网络（CNN）和长短期记忆（LSTM）层等不同的ANN核心进行非线性信道均衡。通过在所有子载波上采用全连接网络来开始对DSCM系统中光纤非线性失真进行补偿。在随后的步骤中，借鉴光纤非线性分析，我们逐步将设计升级为具有更好性能-复杂性优势的模块化结构。我们的研究表明，在DSCM系统的ANN非线性均衡器设计中放置适当的宏观结构对未来世代相干光收发机的实际解决方案至关重要。

    In this work, we propose to use various artificial neural network (ANN) structures for modeling and compensation of intra- and inter-subcarrier fiber nonlinear interference in digital subcarrier multiplexing (DSCM) optical transmission systems. We perform nonlinear channel equalization by employing different ANN cores including convolutional neural networks (CNN) and long short-term memory (LSTM) layers. We start to compensate the fiber nonlinearity distortion in DSCM systems by a fully connected network across all subcarriers. In subsequent steps, and borrowing from fiber nonlinearity analysis, we gradually upgrade the designs towards modular structures with better performance-complexity advantages. Our study shows that putting proper macro structures in design of ANN nonlinear equalizers in DSCM systems can be crucial for practical solutions in future generations of coherent optical transceivers.
    
[^64]: 评估-优化方法与集成评估优化法：基于随机优势的观点

    Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective. (arXiv:2304.06833v1 [stat.ML])

    [http://arxiv.org/abs/2304.06833](http://arxiv.org/abs/2304.06833)

    本文提出，当模型类足够丰富以涵盖真实情况时，非线性问题的“先估计再优化”方法优于集成方法，包括优化间隙的渐进优势的均值，所有其他时刻和整个渐进分布。

    

    在数据驱动的随机优化中，除了需要优化任务，还需要从数据中估计潜在分布的模型参数。最近的文献表明，通过选择导致最佳经验目标性能的模型参数，可以集成估计和优化过程。当模型被错误地指定时，这种集成方法可以很容易地显示出优于简单的“先估计再优化”的方法。本文认为，在模型类足够丰富以涵盖真实情况的情况下，对于非线性问题，两种方法之间的性能排序在强烈的意义下被颠倒。在受限条件和当上下文特征可用时，类似的结果也成立。

    In data-driven stochastic optimization, model parameters of the underlying distribution need to be estimated from data in addition to the optimization task. Recent literature suggests the integration of the estimation and optimization processes, by selecting model parameters that lead to the best empirical objective performance. Such an integrated approach can be readily shown to outperform simple ``estimate then optimize" when the model is misspecified. In this paper, we argue that when the model class is rich enough to cover the ground truth, the performance ordering between the two approaches is reversed for nonlinear problems in a strong sense. Simple ``estimate then optimize" outperforms the integrated approach in terms of stochastic dominance of the asymptotic optimality gap, i,e, the mean, all other moments, and the entire asymptotic distribution of the optimality gap is always better. Analogous results also hold under constrained settings and when contextual features are availa
    
[^65]: 一种适应任务的特征转换方法用于一次性学习

    Task Adaptive Feature Transformation for One-Shot Learning. (arXiv:2304.06832v1 [cs.LG])

    [http://arxiv.org/abs/2304.06832](http://arxiv.org/abs/2304.06832)

    该论文提出了一种针对一次性学习的适应任务的特征转换方法，可在低样本情况下改善推理效果，并在多个一次性测试中得到验证。

    

    我们引入了一种简单的非线性嵌入调整层，它在预训练的固定特征之上进行微调，以改善低样本情况下转导熵推理的效果。我们的范数引导转换可以理解为在特定任务中重新参数化特征空间，以解开不同类别的表示方法。它专注于相关的特征维度，同时阻止可能会在一次性学习中导致过拟合的非相关维度的影响。我们还在K均值聚类的基本情况下提供了对我们提出的特征转换的解释。此外，我们在K均值和熵最小化之间提供了一个有趣的界限优化链接。这强调了我们的特征转换在熵最小化的情况下为什么有用。我们报告了全面的实验，展示了在各种一次性测试中的持续改进。

    We introduce a simple non-linear embedding adaptation layer, which is fine-tuned on top of fixed pre-trained features for one-shot tasks, improving significantly transductive entropy-based inference for low-shot regimes. Our norm-induced transformation could be understood as a re-parametrization of the feature space to disentangle the representations of different classes in a task specific manner. It focuses on the relevant feature dimensions while hindering the effects of non-relevant dimensions that may cause overfitting in a one-shot setting. We also provide an interpretation of our proposed feature transformation in the basic case of few-shot inference with K-means clustering. Furthermore, we give an interesting bound-optimization link between K-means and entropy minimization. This emphasizes why our feature transformation is useful in the context of entropy minimization. We report comprehensive experiments, which show consistent improvements over a variety of one-shot benchmarks, 
    
[^66]: 通用图和具有局部性质的图中的成对比较排序

    Ranking from Pairwise Comparisons in General Graphs and Graphs with Locality. (arXiv:2304.06821v1 [stat.ML])

    [http://arxiv.org/abs/2304.06821](http://arxiv.org/abs/2304.06821)

    本文研究了通用图和具有局部性质的图中的成对比较排序问题。研究表明，最大似然估计（MLE）可以实现符合Cram\'er-Rao下界的逐元估计误差。同时，文章还确定了局部性不会影响的条件，并提出了分治算法以实现类似保障。

    

    本技术报告研究了经典的Bradley-Terry-Luce（BTL）模型中的成对比较排序问题，重点关注得分估计。对于通用图，我们表明，通过足够多的样本，最大似然估计（MLE）可以实现一个符合Cram\'er-Rao下界的逐元估计误差，这可以用有效电阻来说明；我们分析的关键是统计估计和通过预处理梯度下降进行迭代优化之间的联系。我们还特别关注具有局部性质的图，其中仅相邻项之间可以连接边缘；我们的分析确定了局部性不会影响的条件，即在图中距离较远的一对项目之间进行得分比较几乎与比较相邻项目对一对项目相似。我们进一步探讨了分治算法，即使在最稀疏的样本区域内，也可以证明能够实现类似的保证，并享受本地方法的计算效率。

    This technical report studies the problem of ranking from pairwise comparisons in the classical Bradley-Terry-Luce (BTL) model, with a focus on score estimation. For general graphs, we show that, with sufficiently many samples, maximum likelihood estimation (MLE) achieves an entrywise estimation error matching the Cram\'er-Rao lower bound, which can be stated in terms of effective resistances; the key to our analysis is a connection between statistical estimation and iterative optimization by preconditioned gradient descent. We are also particularly interested in graphs with locality, where only nearby items can be connected by edges; our analysis identifies conditions under which locality does not hurt, i.e. comparing the scores between a pair of items that are far apart in the graph is nearly as easy as comparing a pair of nearby items. We further explore divide-and-conquer algorithms that can provably achieve similar guarantees even in the regime with the sparsest samples, while enj
    
[^67]: 用相关静态分析产品改善少样本提示

    Improving Few-Shot Prompts with Relevant Static Analysis Products. (arXiv:2304.06815v1 [cs.SE])

    [http://arxiv.org/abs/2304.06815](http://arxiv.org/abs/2304.06815)

    本文研究了用相关静态分析产品改善大型语言模型在少样本提示中的表现，探讨如何通过添加显示信息来提取代码中的语义事实。

    

    大型语言模型是一类新型计算引擎，通过提示工程实现"编程"。我们仍在学习如何最好地"编程"这些大型语言模型以帮助开发人员。我们的研究从这样一种直觉出发，即开发人员在处理编码任务时会有一系列意识和无意识的语义事实。对于一个函数而言，这些语义事实可能包括参数和局部变量名称、返回表达式、简单的前置和后置条件以及基本的控制和数据流，等等。我们的目标是调查这个问题，使用代码摘要任务并评估是否使用显式添加信息能够帮助大型语言模型提取这些语义事实。

    Large Language Models (LLM) are a new class of computation engines, "programmed" via prompt engineering. We are still learning how to best "program" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.  One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of "code analysis" and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether auto
    
[^68]: 模型特定视角下的统一外部分布检测

    Unified Out-Of-Distribution Detection: A Model-Specific Perspective. (arXiv:2304.06813v1 [cs.LG])

    [http://arxiv.org/abs/2304.06813](http://arxiv.org/abs/2304.06813)

    本文提出一种新颖的统一框架，用于将机器学习模型中的外部分布检测扩展到更广泛的范围，该框架旨在检测模型无法正确预测的测试示例，而不是特定的外部分布原因。

    

    外部分布检测旨在识别不属于训练分布并不可靠预测的测试样例。虽然已有大量相关工作，但其中大多数只关注来自语义转换（如未见过的类别）的OOD例子，而忽略了其他可能的原因（如协变量转换）。本文提出了一种新颖的统一框架，以更广泛的范围研究OOD检测。我们建议不是检测特定原因导致的OOD例子，而是检测已部署机器学习模型（例如图像分类器）无法正确预测的例子。也就是说，是否应该检测和拒绝测试例子是“模型特定”的。我们展示了该框架统一了由语义变化和协变量变化引起的OOD例子的检测，并密切关注将机器学习模型应用于不受控制的环境的问题。我们对该框架进行了广泛的分析和实验。

    Out-of-distribution (OOD) detection aims to identify test examples that do not belong to the training distribution and are thus unlikely to be predicted reliably. Despite a plethora of existing works, most of them focused only on the scenario where OOD examples come from semantic shift (e.g., unseen categories), ignoring other possible causes (e.g., covariate shift). In this paper, we present a novel, unifying framework to study OOD detection in a broader scope. Instead of detecting OOD examples from a particular cause, we propose to detect examples that a deployed machine learning model (e.g., an image classifier) is unable to predict correctly. That is, whether a test example should be detected and rejected or not is ``model-specific''. We show that this framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and closely addresses the concern of applying a machine learning model to uncontrolled environments. We provide an extensive analysis that 
    
[^69]: 高维量子态工程的非线性光子晶体设计

    Designing Nonlinear Photonic Crystals for High-Dimensional Quantum State Engineering. (arXiv:2304.06810v1 [quant-ph])

    [http://arxiv.org/abs/2304.06810](http://arxiv.org/abs/2304.06810)

    该论文提出了一种通过设计非线性光子晶体和泵浦光束生成高维量子态的方法，同时基于所提出的物理约束和可微分的方法，理论和实验上演示了如何生成最大纠缠态。这为控制任意量子态提供了新途径，并在全光学相干控制成为可能。

    

    我们提出了一种新颖、受物理约束和可微分的方法，通过量子光学中的自发参量下转换 (SPDC)，生成 D 维 qudit 状态。我们规避了物理过程固有的随机性所带来的任何限制，并并入了一组随机动力学方程，控制其在 SPDC 哈密顿下的演化。我们通过设计结构化的非线性光子晶体 (NLPCs) 和形状化的泵浦光束，展示了我们模型的有效性；理论上和实验上展示了如何在空间自由度中生成最大纠缠态。学习 NLPC 结构为塑造和控制任意量子态提供了有前途的新途径，并且使得生成态的全光学相干控制成为可能。我们相信这种方法可以很容易地从庞大的晶体扩展到薄型元表面，并且可能应用于其他共享类似哈密顿量的量子系统。

    We propose a novel, physically-constrained and differentiable approach for the generation of D-dimensional qudit states via spontaneous parametric down-conversion (SPDC) in quantum optics. We circumvent any limitations imposed by the inherently stochastic nature of the physical process and incorporate a set of stochastic dynamical equations governing its evolution under the SPDC Hamiltonian. We demonstrate the effectiveness of our model through the design of structured nonlinear photonic crystals (NLPCs) and shaped pump beams; and show, theoretically and experimentally, how to generate maximally entangled states in the spatial degree of freedom. The learning of NLPC structures offers a promising new avenue for shaping and controlling arbitrary quantum states and enables all-optical coherent control of the generated states. We believe that this approach can readily be extended from bulky crystals to thin Metasurfaces and potentially applied to other quantum systems sharing a similar Ham
    
[^70]: 流式数据主动计费标注

    Active Cost-aware Labeling of Streaming Data. (arXiv:2304.06808v1 [cs.LG])

    [http://arxiv.org/abs/2304.06808](http://arxiv.org/abs/2304.06808)

    本文研究了流式数据中的主动计费标注问题，提出了一种算法，通过选择标记点并维护时间和成本相关阈值，在$T$轮之后实现了$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。

    

    我们研究了主动标记流数据问题，其中主动学习者面临一系列数据点，并必须通过昂贵的实验精心选择哪些点进行标记，此类问题常常出现在医疗和天文学等领域。我们首先研究的是数据输入属于$K$个离散分布之一的情况，并通过损失函数形式化描述此问题，该损失函数捕捉了标记成本和预测误差。当标记成本为$B$时，我们的算法通过选择标记点，仅在不确定性大于时间和成本相关阈值时进行，可以在$T$轮之后实现$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$的最坏情况上界。我们还提供了更细致的上界，证明了在到达模式更有利时，算法可以适应到达模式，并实现更好的性能。我们还补充了两个上界的匹配下界。接下来，我们研究了在流数据具有不确定性分布的情况下标记流数据的问题，并提供与前面情况类似的结果。

    We study actively labeling streaming data, where an active learner is faced with a stream of data points and must carefully choose which of these points to label via an expensive experiment. Such problems frequently arise in applications such as healthcare and astronomy. We first study a setting when the data's inputs belong to one of $K$ discrete distributions and formalize this problem via a loss that captures the labeling cost and the prediction error. When the labeling cost is $B$, our algorithm, which chooses to label a point if the uncertainty is larger than a time and cost dependent threshold, achieves a worst-case upper bound of $O(B^{\frac{1}{3}} K^{\frac{1}{3}} T^{\frac{2}{3}})$ on the loss after $T$ rounds. We also provide a more nuanced upper bound which demonstrates that the algorithm can adapt to the arrival pattern, and achieves better performance when the arrival pattern is more favorable. We complement both upper bounds with matching lower bounds. We next study this pr
    
[^71]: 活性物质模型的基于图形信息的模拟推断方法

    Graph-informed simulation-based inference for models of active matter. (arXiv:2304.06806v1 [cond-mat.soft])

    [http://arxiv.org/abs/2304.06806](http://arxiv.org/abs/2304.06806)

    本文探索了基于图形信息的方法，证明了它能够优于典型指标，改善活性物质模型的参数推断精度，只需使用少量的系统快照。

    

    自细胞薄片到鸟群等许多集体系统都存在于远离平衡状态的自然环境中。这些系统反映了一种活性物质形式，其中单个物质组件具有内部能量。在特定的参数范围内，这些活性系统会经历相变，表现为单个组件的小波动可以导致系统流变性质的全局变化。通常使用统计物理学的模拟和方法来理解和预测这些相变的真实观测。在这项工作中，我们证明了基于模拟的推断可用于从系统观察中强健地推断出活性物质参数。此外，我们证明，可以使用少量（从一个到三个）系统快照来进行参数推断，并且这种基于图形信息的方法优于系统的平均速度或平均平方位移等典型指标。我们的工作强调，可以使用有限数量的快照来准确预测高级系统行为，并且基于图形信息的方法可以提高活性物质模型的参数推断精度。

    Many collective systems exist in nature far from equilibrium, ranging from cellular sheets up to flocks of birds. These systems reflect a form of active matter, whereby individual material components have internal energy. Under specific parameter regimes, these active systems undergo phase transitions whereby small fluctuations of single components can lead to global changes to the rheology of the system. Simulations and methods from statistical physics are typically used to understand and predict these phase transitions for real-world observations. In this work, we demonstrate that simulation-based inference can be used to robustly infer active matter parameters from system observations. Moreover, we demonstrate that a small number (from one to three) snapshots of the system can be used for parameter inference and that this graph-informed approach outperforms typical metrics such as the average velocity or mean square displacement of the system. Our work highlights that high-level sys
    
[^72]: 用于黑盒变分推断的样本平均估计方法

    Sample Average Approximation for Black-Box VI. (arXiv:2304.06803v1 [cs.LG])

    [http://arxiv.org/abs/2304.06803](http://arxiv.org/abs/2304.06803)

    该论文提出了一种用于黑盒变分推断的样本平均估计方法，有效地解决了随机梯度上升等问题，实验结果表明其比现有方法更快且性能更佳。

    

    我们提出了一种新的方法，用于解决随机梯度上升的困难，包括选择步长的任务。我们的方法涉及使用一系列样本平均估计问题（SAA）。通过将随机优化问题转化为确定性问题，SAA逼近了随机优化问题的解。我们使用拟牛顿方法和线性搜索来解决每个确定性优化问题，并提出了一种启发式策略来自动选择超参数。我们的实验表明，我们的方法简化了变分推断问题，并实现了比现有方法更快的性能。

    We present a novel approach for black-box VI that bypasses the difficulties of stochastic gradient ascent, including the task of selecting step-sizes. Our approach involves using a sequence of sample average approximation (SAA) problems. SAA approximates the solution of stochastic optimization problems by transforming them into deterministic ones. We use quasi-Newton methods and line search to solve each deterministic optimization problem and present a heuristic policy to automate hyperparameter selection. Our experiments show that our method simplifies the VI problem and achieves faster performance than existing methods.
    
[^73]: Token-and-Duration Transducer架构：联合预测标记与时长的高效序列转导

    Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. (arXiv:2304.06795v1 [eess.AS])

    [http://arxiv.org/abs/2304.06795](http://arxiv.org/abs/2304.06795)

    本文提出了一种新型的序列转导架构TDT，它可以联合预测标记和持续时间，从而实现比传统Transducers更高的准确性和显着更快的推理速度。

    

    本文提出了一种用于序列到序列任务的新型Token-and-Duration Transducer(TDT)架构。TDT通过联合预测标记和持续时间，即发射的标记覆盖的输入帧的数量，来扩展传统的RNN-Transducer架构。它使用具有两个独立标准化输出的联合网络来生成标记和持续时间的分布。在推理期间，TDT模型可以通过预测的持续时间输出跳过输入帧，使其比逐帧处理编码器输出的传统Transducers显着更快。在不同的序列转导任务上，TDT模型均实现了更高的准确性和显着更快的推理速度。语音识别的TDT模型比RNN-Transducers获得更好的准确性，并且推理速度高达2.82倍。语音翻译的TDT模型与MUST-C测试相比提高了1个BLEU分数。

    This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than RNN-Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared wi
    
[^74]: Speck: 一款拥有低延迟327K神经元卷积神经网络处理管道的智能事件型视觉传感器

    Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline. (arXiv:2304.06793v1 [cs.NE])

    [http://arxiv.org/abs/2304.06793](http://arxiv.org/abs/2304.06793)

    Speck是一款智能视觉传感器SoC，该芯片集成了基于事件的相机和低功耗的sCNN计算架构，可显著降低单元生产成本。通过优化高度稀疏的计算和最小化延迟，Speck可以处理高速的稀疏数据流。

    

    需要从各种传感器中提取高级信息的边缘计算方案正在日益增长。为了解决这个问题，我们提出了一种智能视觉传感器SoC，具有基于事件的相机和低功耗的sCNN计算架构。该SoC的端到端设计简单，可作为独立应用或作为较大系统中的边缘节点。综合传感器和处理于单个芯片中可以显著降低单元生产成本。在处理管道中，对高度稀疏的计算进行优化，并将延迟最小化为$3.36\mu s$，以应对视觉传感器的事件驱动信号等高速信号的稀疏数据流。

    Edge computing solutions that enable the extraction of high level information from a variety of sensors is in increasingly high demand. This is due to the increasing number of smart devices that require sensory processing for their application on the edge. To tackle this problem, we present a smart vision sensor System on Chip (Soc), featuring an event-based camera and a low power asynchronous spiking Convolutional Neuronal Network (sCNN) computing architecture embedded on a single chip. By combining both sensor and processing on a single die, we can lower unit production costs significantly. Moreover, the simple end-to-end nature of the SoC facilitates small stand-alone applications as well as functioning as an edge node in a larger systems. The event-driven nature of the vision sensor delivers high-speed signals in a sparse data stream. This is reflected in the processing pipeline, focuses on optimising highly sparse computation and minimising latency for 9 sCNN layers to $3.36\mu s$
    
[^75]: 二元积分布的多项式时间和纯差分隐私估计器

    A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions. (arXiv:2304.06787v1 [cs.DS])

    [http://arxiv.org/abs/2304.06787](http://arxiv.org/abs/2304.06787)

    本论文提出了第一个多项式时间、纯差分隐私估计器，可以在$\{0,1\}^d$上准确估计二元积分布的均值，达到了最优的样本复杂度。

    

    我们提出了第一个ε-差分隐私、计算有效的算法，可以在总变化距离下准确地估计$\{0,1\}^d$上的乘积分布的均值，同时在多项式对数因子内获得了最优的样本复杂度。之前的工作要么在更弱的隐私概念下有效地解决了这个问题，要么在指数级运行时间内最优地解决了这个问题。

    We present the first $\varepsilon$-differentially private, computationally efficient algorithm that estimates the means of product distributions over $\{0,1\}^d$ accurately in total-variation distance, whilst attaining the optimal sample complexity to within polylogarithmic factors. The prior work had either solved this problem efficiently and optimally under weaker notions of privacy, or had solved it optimally while having exponential running times.
    
[^76]: 基于Wasserstein距离的分布鲁棒方法实现后悔最优控制

    A Distributionally Robust Approach to Regret Optimal Control using the Wasserstein Distance. (arXiv:2304.06783v1 [math.OC])

    [http://arxiv.org/abs/2304.06783](http://arxiv.org/abs/2304.06783)

    提出了一种基于Wasserstein距离的分布鲁棒方法实现后悔最优控制的控制器设计策略。

    

    本文提出了一种基于分布鲁棒方法的离散时间线性动态系统的后悔最优控制策略，该系统受到随机加性扰动影响，成本为二次型。扰动过程的概率分布未知，但假定位于给定的分布球内，定义为二阶Wasserstein距离。在该框架下，设计具有严格因果线性扰动反馈控制器，以最小化最坏情况下的预期后悔。控制器所产生的后悔是指它对抗扰动时产生的成本与最优非因果控制器在一开始就有完全了解扰动过程后产生的成本之差。建立在最优输运问题的一个很好的对偶理论基础上，我们展示了如何等价地改写这个极小极大的后悔最优控制问题为一个可行的半定规划问题。

    This paper proposes a distributionally robust approach to regret optimal control of discrete-time linear dynamical systems with quadratic costs subject to stochastic additive disturbance on the state process. The underlying probability distribution of the disturbance process is unknown, but assumed to lie in a given ball of distributions defined in terms of the type-2 Wasserstein distance. In this framework, strictly causal linear disturbance feedback controllers are designed to minimize the worst-case expected regret. The regret incurred by a controller is defined as the difference between the cost it incurs in response to a realization of the disturbance process and the cost incurred by the optimal noncausal controller which has perfect knowledge of the disturbance process realization at the outset. Building on a well-established duality theory for optimal transport problems, we show how to equivalently reformulate this minimax regret optimal control problem as a tractable semidefini
    
[^77]: 半等变条件正则化流

    Semi-Equivariant Conditional Normalizing Flows. (arXiv:2304.06779v1 [cs.LG])

    [http://arxiv.org/abs/2304.06779](http://arxiv.org/abs/2304.06779)

    本文介绍了一种能够保持刚性运动条件不变性的连续正则化流，并在分子环境中展示了其有效性。

    

    本文研究了使用连续正则化流学习形如 $p(G | \hat G)$ 的条件分布的问题，其中 $G$ 和 $\hat G$ 是两个三维图形。我们导出了流的半等变条件，确保对刚性运动的条件不变性保持。我们在感受受体感知配体生成的分子环境中展示了这种技术的有效性。

    We study the problem of learning conditional distributions of the form $p(G | \hat G)$, where $G$ and $\hat G$ are two 3D graphs, using continuous normalizing flows. We derive a semi-equivariance condition on the flow which ensures that conditional invariance to rigid motions holds. We demonstrate the effectiveness of the technique in the molecular setting of receptor-aware ligand generation.
    
[^78]: 在线识别不完整的手势数据以接口协作机器人

    Online Recognition of Incomplete Gesture Data to Interface Collaborative Robots. (arXiv:2304.06777v1 [cs.RO])

    [http://arxiv.org/abs/2304.06777](http://arxiv.org/abs/2304.06777)

    本文介绍了一个人机交互框架，通过可穿戴传感器捕捉交织的静态手势和动态手势，并使用数据降维技术以获得DG特征。实验结果表明，通过随机森林和人工神经网络，分别可以对24个SG和10个DG进行准确的分类，从而实现在非结构化环境中的手势识别。

    

    在实现人机交互和进一步推动协作机器人进入市场，使机器人能够更广泛地被人们接受时，手势的在线识别对于实现直观的人机交互至关重要。然而，在真实的非结构化环境中，使用失真和不完整的多传感器数据实现准确的手势识别却很困难。本文介绍了一个人机交互框架，使用可穿戴的传感器捕捉交织的静态手势（SG）和动态手势（DG），通过对原始传感器数据进行数据降维（使用三次插值和主成分分析重采样）来获取DG特征。实验测试使用了UC2017手势数据集从8个不同的受试者中采集的样本。分类模型对于包含24个SG的库使用随机森林显示出95.6％的准确率，并使用人工神经网络对10个DG显示出99.3％的准确率。这些结果与不同的先前研究相比表现一致或更有优势。

    Online recognition of gestures is critical for intuitive human-robot interaction (HRI) and further push collaborative robotics into the market, making robots accessible to more people. The problem is that it is difficult to achieve accurate gesture recognition in real unstructured environments, often using distorted and incomplete multisensory data. This paper introduces an HRI framework to classify large vocabularies of interwoven static gestures (SGs) and dynamic gestures (DGs) captured with wearable sensors. DG features are obtained by applying data dimensionality reduction to raw data from sensors (resampling with cubic interpolation and principal component analysis). Experimental tests were conducted using the UC2017 hand gesture dataset with samples from eight different subjects. The classification models show an accuracy of 95.6% for a library of 24 SGs with a random forest and 99.3% for 10 DGs using artificial neural networks. These results compare equally or favorably with dif
    
[^79]: 通过坐标变换改进梯度方法：应用于量子机器学习

    Improving Gradient Methods via Coordinate Transformations: Applications to Quantum Machine Learning. (arXiv:2304.06768v1 [quant-ph])

    [http://arxiv.org/abs/2304.06768](http://arxiv.org/abs/2304.06768)

    本文介绍了一种通过坐标变换来加速梯度优化算法、改善荒原高原和局部最小值影响的通用策略，有效提高了多种量子机器学习算法的性能。

    

    无论是经典的还是量子的机器学习算法，都大量依赖基于梯度的优化算法，如梯度下降等。总体性能取决于局部最小值和荒原高原的出现，这会减缓计算速度并导致非最优解。实际应用中，这会导致人工智能应用的计算和能源成本激增。本文介绍了一种通用策略，以加速和改善这些方法的总体性能，从而减轻了荒原高原和局部最小值的影响。我们的方法基于坐标变换，有点类似于变分旋转，在参数空间中添加了额外的方向，这些方向取决于成本函数本身，并且允许更有效地探索配置景观。我们已通过增强多种量子机器学习算法来测试我们的方法，得到了非常显着的改进。

    Machine learning algorithms, both in their classical and quantum versions, heavily rely on optimization algorithms based on gradients, such as gradient descent and alike. The overall performance is dependent on the appearance of local minima and barren plateaus, which slow-down calculations and lead to non-optimal solutions. In practice, this results in dramatic computational and energy costs for AI applications. In this paper we introduce a generic strategy to accelerate and improve the overall performance of such methods, allowing to alleviate the effect of barren plateaus and local minima. Our method is based on coordinate transformations, somehow similar to variational rotations, adding extra directions in parameter space that depend on the cost function itself, and which allow to explore the configuration landscape more efficiently. The validity of our method is benchmarked by boosting a number of quantum machine learning algorithms, getting a very significant improvement in their
    
[^80]: RAFT: 奖励排名微调用于生成型基础模型对齐

    RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])

    [http://arxiv.org/abs/2304.06767](http://arxiv.org/abs/2304.06767)

    RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。

    

    生成型基础模型容易受到广泛的无监督训练数据带来的隐式偏见的影响。这些偏见可能导致子优样本、扭曲的结果和不公平，可能产生重大影响。因此，将这些模型与人的伦理和偏好对齐是确保它们在真实应用中负责任和有效的部署的关键步骤。以往的研究主要采用人类反馈的强化学习（ RLHF）作为解决这个问题的手段。在 RL 算法的指导下，用人类反馈指导的奖励模型对生成模型进行微调。然而， RL 算法的低效性和不稳定性常常会对生成模型的成功对齐产生重大障碍，因此需要开发一种更为强大和简化的方法。为此，我们引入了一个新的框架，即奖励排名微调（ RAFT ），旨在对齐生成基础模型。

    Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
    
[^81]: 面向 FPGAs 和 ASICs 的 Hessian-aware 量化神经网络的端到端协同设计

    End-to-end codesign of Hessian-aware quantized neural networks for FPGAs and ASICs. (arXiv:2304.06745v1 [cs.LG])

    [http://arxiv.org/abs/2304.06745](http://arxiv.org/abs/2304.06745)

    该论文提出了一个端到端的工作流程，用于训练和实现协同设计的神经网络（NNs），以高效地在FPGAs和ASICs上实现NNs，并在粒子物理应用程序中得到了应用。

    

    我们开发了一个端到端的工作流程，用于训练和实现协同设计的神经网络（NNs），以实现高效的现场可编程门阵列（FPGA）和专用集成电路（ASIC）硬件。我们的方法利用基于 Hessian 的量化（HAWQ）NNs，量化开放神经网络交换（QONNX）中间表示和hls4ml工具流，将NNs转换为FPGA和ASIC固件。这使得非专家可以在单个开源工作流中实现高效的NN硬件实现，并且可以在广泛的科学和工业应用中部署用于实时机器学习应用。我们在一个粒子物理应用程序中演示了工作流程，涉及必须在CERN大型强子对撞机（LHC）的40 MHz碰撞率下操作的触发决策。由于高碰撞率，所有数据处理必须在严格的面积和延迟内在定制的ASIC和FPGA硬件上实现。基于这些要求我们展示了本文工作流程。

    We develop an end-to-end workflow for the training and implementation of co-designed neural networks (NNs) for efficient field-programmable gate array (FPGA) and application-specific integrated circuit (ASIC) hardware. Our approach leverages Hessian-aware quantization (HAWQ) of NNs, the Quantized Open Neural Network Exchange (QONNX) intermediate representation, and the hls4ml tool flow for transpiling NNs into FPGA and ASIC firmware. This makes efficient NN implementations in hardware accessible to nonexperts, in a single open-sourced workflow that can be deployed for real-time machine learning applications in a wide range of scientific and industrial settings. We demonstrate the workflow in a particle physics application involving trigger decisions that must operate at the 40 MHz collision rate of the CERN Large Hadron Collider (LHC). Given the high collision rate, all data processing must be implemented on custom ASIC and FPGA hardware within a strict area and latency. Based on these
    
[^82]: 一项生物可行的神经网络研究：脑启发机制在持续学习中的作用和相互作用

    A Study of Biologically Plausible Neural Network: The Role and Interactions of Brain-Inspired Mechanisms in Continual Learning. (arXiv:2304.06738v1 [cs.NE])

    [http://arxiv.org/abs/2304.06738](http://arxiv.org/abs/2304.06738)

    本论文提出了一种基于脑启发机制的生物可行的神经网络框架，并研究了其中包括稀疏不重叠表示、赫布学习、突触巩固和重播等机制在持续学习中的相互作用，实现了基于序列的持续学习基准测试的最先进性能。

    

    人类擅长不断从不断变化的环境中获取、巩固和保留信息，而人工神经网络则表现出灾难性遗忘。生物神经网络和它们的人工对应物在突触复杂性、信息处理和学习机制方面存在显著差异，这可能解释了性能上的不匹配。我们考虑一个生物可行的框架，其中包括完全兴奋和抑制神经元的单独种群，遵循戴尔原则，并为兴奋的锥体神经元增加了类似树突的结构，用于上下文相关的刺激处理。然后我们对受脑启发的不同机制进行了全面研究，包括稀疏不重叠表示、赫布学习、突触巩固和重播伴随学习事件的过去激活。我们的解决方案在基于序列的持续学习基准测试中实现了最先进的性能，并展示了考虑这些机制在生物可行的框架中相互作用的重要性。

    Humans excel at continually acquiring, consolidating, and retaining information from an ever-changing environment, whereas artificial neural networks (ANNs) exhibit catastrophic forgetting. There are considerable differences in the complexity of synapses, the processing of information, and the learning mechanisms in biological neural networks and their artificial counterparts, which may explain the mismatch in performance. We consider a biologically plausible framework that constitutes separate populations of exclusively excitatory and inhibitory neurons that adhere to Dale's principle, and the excitatory pyramidal neurons are augmented with dendritic-like structures for context-dependent processing of stimuli. We then conduct a comprehensive study on the role and interactions of different mechanisms inspired by the brain, including sparse non-overlapping representations, Hebbian learning, synaptic consolidation, and replay of past activations that accompanied the learning event. Our s
    
[^83]: 贝叶斯网络近似最优入度检测

    Near-Optimal Degree Testing for Bayes Nets. (arXiv:2304.06733v1 [cs.LG])

    [http://arxiv.org/abs/2304.06733](http://arxiv.org/abs/2304.06733)

    本文提出了一个近似最优入度检测算法，并给出了样本复杂度的上界，同时开发了新的算法进行“近似正确性”的贝叶斯网络学习和在 $\chi^2$ 散度下的高概率学习。

    

    本文考虑在对 $P$ 的样本访问的情况下，检测潜在的贝叶斯网络的最大入度。我们证明问题的样本复杂度为 $\tilde{\Theta}(2^{n/2}/\varepsilon^2)$。我们的算法依赖于测试和学习框架，该框架先前用于获得样本最优测试器。为了应用此框架，我们开发了新的算法，用于具有“近似正确性”的贝叶斯网络学习，并在 $\chi^2$ 散度下的高概率学习，这些算法都具有独立的意义。

    This paper considers the problem of testing the maximum in-degree of the Bayes net underlying an unknown probability distribution $P$ over $\{0,1\}^n$, given sample access to $P$. We show that the sample complexity of the problem is $\tilde{\Theta}(2^{n/2}/\varepsilon^2)$. Our algorithm relies on a testing-by-learning framework, previously used to obtain sample-optimal testers; in order to apply this framework, we develop new algorithms for ``near-proper'' learning of Bayes nets, and high-probability learning under $\chi^2$ divergence, which are of independent interest.
    
[^84]: PCD2Vec：一种基于泊松校正距离的病毒宿主分类方法

    PCD2Vec: A Poisson Correction Distance-Based Approach for Viral Host Classification. (arXiv:2304.06731v1 [q-bio.QM])

    [http://arxiv.org/abs/2304.06731](http://arxiv.org/abs/2304.06731)

    本文提出了一种新的病毒宿主分类方法，通过分析刺突蛋白序列预测冠状病毒的宿主特异性。该方法使用泊松校正距离生成距离矩阵，然后对刺突蛋白序列进行聚类。

    

    冠状病毒是一种带膜的、非分段的正链RNA病毒，属于冠状病毒科。它严重感染各种动物，主要是哺乳动物和禽鸟，引起严重担忧，如最近的大流行病（COVID-19）。因此，深入理解这些病毒对制定预防和缓解机制至关重要。 在冠状病毒基因组中，一个重要的结构区域是刺突区域，它负责将病毒附着在宿主细胞膜上。因此，使用仅刺突蛋白而不是完整基因组，提供了执行诸如宿主分类之类的分析所需的大部分基本信息。在本文中，我们提出了一种新的方法，通过分析来自不同病毒亚属和物种的刺突蛋白序列，预测冠状病毒的宿主特异性。我们的方法涉及使用泊松校正距离生成距离矩阵，然后基于该矩阵对刺突蛋白序列进行聚类。我们在各种数据集上评估了我们的方法，并取得了有希望的结果，优于几种最先进的方法。

    Coronaviruses are membrane-enveloped, non-segmented positive-strand RNA viruses belonging to the Coronaviridae family. Various animal species, mainly mammalian and avian, are severely infected by various coronaviruses, causing serious concerns like the recent pandemic (COVID-19). Therefore, building a deeper understanding of these viruses is essential to devise prevention and mitigation mechanisms. In the Coronavirus genome, an essential structural region is the spike region, and it's responsible for attaching the virus to the host cell membrane. Therefore, the usage of only the spike protein, instead of the full genome, provides most of the essential information for performing analyses such as host classification. In this paper, we propose a novel method for predicting the host specificity of coronaviruses by analyzing spike protein sequences from different viral subgenera and species. Our method involves using the Poisson correction distance to generate a distance matrix, followed by
    
[^85]: 认知的元学习模型

    Meta-Learned Models of Cognition. (arXiv:2304.06729v1 [cs.AI])

    [http://arxiv.org/abs/2304.06729](http://arxiv.org/abs/2304.06729)

    元学习模型是构建人类认知模型的有前途的工具，可以用于构建贝叶斯最优学习算法，并且比传统的贝叶斯方法有几个优势。

    

    元学习是通过与环境的反复交互而学习学习算法的框架，而不是手动设计它们。近年来，这个框架已经成为构建人类认知模型的有前途的工具。然而，还缺乏一个关于元学习认知模型的连贯的研究计划。本文的目的是综合先前的工作，在这个领域建立这样的研究计划。我们依靠三个关键支柱来实现这个目标。首先，我们指出元学习可以用于构建贝叶斯最优学习算法。这个结果不仅意味着任何可以通过贝叶斯模型解释的行为现象也可以通过元学习模型解释，而且还允许我们与认知的理性分析建立强连接。然后，我们讨论了元学习框架相对于传统的贝叶斯方法的几个优势。特别是，我们认为元学习可以

    Meta-learning is a framework for learning learning algorithms through repeated interactions with an environment as opposed to designing them by hand. In recent years, this framework has established itself as a promising tool for building models of human cognition. Yet, a coherent research program around meta-learned models of cognition is still missing. The purpose of this article is to synthesize previous work in this field and establish such a research program. We rely on three key pillars to accomplish this goal. We first point out that meta-learning can be used to construct Bayes-optimal learning algorithms. This result not only implies that any behavioral phenomenon that can be explained by a Bayesian model can also be explained by a meta-learned model but also allows us to draw strong connections to the rational analysis of cognition. We then discuss several advantages of the meta-learning framework over traditional Bayesian methods. In particular, we argue that meta-learning can
    
[^86]: GradMDM：动态网络的对抗攻击

    GradMDM: Adversarial Attack on Dynamic Networks. (arXiv:2304.06724v1 [cs.CR])

    [http://arxiv.org/abs/2304.06724](http://arxiv.org/abs/2304.06724)

    本文研究了针对动态神经网络的一种新型能量导向攻击算法GradMDM，其可以有效地增加计算复杂度同时减少扰动的感知度。

    

    动态神经网络可以通过根据输入调整其结构来极大地减少计算冗余，而不会影响精度。 在本文中，我们探讨了针对旨在降低其效率的能量导向攻击的动态神经网络的鲁棒性。 具体而言，我们使用我们的新算法GradMDM攻击动态模型。 GradMDM是一种技术，它调整梯度的方向和大小，有效地为每个输入找到一个小扰动，在推理过程中激活动态模型的更多计算单元。 我们评估了GradMDM在多个数据集和动态模型上的表现，它优于先前的能量导向攻击技术，显着增加了计算复杂性，同时减少了扰动的感知度。

    Dynamic neural networks can greatly reduce computation redundancy without compromising accuracy by adapting their structures based on the input. In this paper, we explore the robustness of dynamic neural networks against energy-oriented attacks targeted at reducing their efficiency. Specifically, we attack dynamic models with our novel algorithm GradMDM. GradMDM is a technique that adjusts the direction and the magnitude of the gradients to effectively find a small perturbation for each input, that will activate more computational units of dynamic models during inference. We evaluate GradMDM on multiple datasets and dynamic models, where it outperforms previous energy-oriented attack techniques, significantly increasing computation complexity while reducing the perceptibility of the perturbations.
    
[^87]: 布局引导下的图像生成的诊断基准和迭代修复

    Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])

    [http://arxiv.org/abs/2304.06671](http://arxiv.org/abs/2304.06671)

    本文提出了布局引导下图像生成的诊断基准LayoutBench，对数量、位置、大小和形状四种空间控制技能进行了研究，发现好的ID布局控制在任意布局的野外环境下可能不具有良好的推广性。接着，我们提出了一种新的基准方法IterInpaint通过修复逐步生成前景和背景区域，显现出在OOD布局方面更强的通用性。

    

    空间控制是可控图像生成的核心能力。在布局引导下的图像生成方面的进展已经显示出在具有类似空间配置的内分布（ID）数据集上有良好的结果。然而，当面对任意不确定的布局的离线分布样本时，这些模型的表现还不清楚。在本文中，我们提出了LayoutBench，这是一种对布局引导下的图像生成进行诊断的基准，它检查了四种空间控制技能：数量，位置，大小和形状。我们对两种最近代表性的布局引导下的图像生成方法进行了基准测试，并观察到良好的ID布局控制可能无法很好地推广到任意布局的野外环境（例如，边界上的对象）。接下来，我们提出了一个新的基准方法IterInpaint，它通过修复逐步生成前景和背景区域，展示出在LayoutBench的OOD布局上更强的通用性。我们进行了数量和定性评估，表明IterInpaint相对于现有方法具有更好的生成多样和视觉上令人愉悦的图像和可控的空间布局。

    Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
    
[^88]: G2T: 基于预训练语言模型和社区检测的主题建模框架

    G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])

    [http://arxiv.org/abs/2304.06653](http://arxiv.org/abs/2304.06653)

    G2T是一种基于预训练语言模型和社区检测的主题建模框架，自动评估表明，G2T在多个数据集上均与当前最先进的方法相比表现更好。

    

    先前的研究表明，基于聚类的主题模型能够通过适当的词语筛选方法聚类高质量的句子嵌入，生成比生成式概率主题模型更好的主题。然而，这些方法存在选择合适参数的困难以及不完整的模型忽略单词与主题及主题与文本之间的定量关系的问题。为了解决这些问题，我们提出了一种简洁但有效的主题建模框架，即图主题（G2T）。

    It has been reported that clustering-based topic models, which cluster high-quality sentence embeddings with an appropriate word selection method, can generate better topics than generative probabilistic topic models. However, these approaches suffer from the inability to select appropriate parameters and incomplete models that overlook the quantitative relation between words with topics and topics with text. To solve these issues, we propose graph to topic (G2T), a simple but effective framework for topic modelling. The framework is composed of four modules. First, document representation is acquired using pretrained language models. Second, a semantic graph is constructed according to the similarity between document representations. Third, communities in document semantic graphs are identified, and the relationship between topics and documents is quantified accordingly. Fourth, the word--topic distribution is computed based on a variant of TFIDF. Automatic evaluation suggests that G2
    
[^89]: MProtoNet：一种基于案例的可解释性模型，用于带有3D多参数磁共振成像的脑肿瘤分类

    MProtoNet: A Case-Based Interpretable Model for Brain Tumor Classification with 3D Multi-parametric Magnetic Resonance Imaging. (arXiv:2304.06258v1 [cs.CV])

    [http://arxiv.org/abs/2304.06258](http://arxiv.org/abs/2304.06258)

    本论文提出了一种基于案例的可解释性模型MProtoNet，用于带有3D多参数磁共振成像的脑肿瘤分类，通过引入新型的注意模块，比四个状态-of-the-art深度学习模型在解释性和分类性能上有所提高。

    

    近年来，深度卷积神经网络在医学影像中的应用引发了人们对其可解释性的担忧。我们提出了第一个医疗原型网络（MProtoNet），用于将ProtoPNet扩展到使用3D多参数磁共振成像数据进行脑肿瘤分类。为了解决2D自然图像和3D mpMRI之间在本地化注意区域方面的不同要求，引入了一种使用软掩膜和在线CAM损失的新型注意模块。MProtoNet通过注意力映射和原型可视化维持解释性，并在一个包含脑胶质瘤和脑膜瘤的mpMRI数据集上取得了显著的分类性能提高。

    Recent applications of deep convolutional neural networks in medical imaging raise concerns about their interpretability. While most explainable deep learning applications use post hoc methods (such as GradCAM) to generate feature attribution maps, there is a new type of case-based reasoning models, namely ProtoPNet and its variants, which identify prototypes during training and compare input image patches with those prototypes. We propose the first medical prototype network (MProtoNet) to extend ProtoPNet to brain tumor classification with 3D multi-parametric magnetic resonance imaging (mpMRI) data. To address different requirements between 2D natural images and 3D mpMRIs especially in terms of localizing attention regions, a new attention module with soft masking and online-CAM loss is introduced. Soft masking helps sharpen attention maps, while online-CAM loss directly utilizes image-level labels when training the attention module. MProtoNet achieves statistically significant improv
    
[^90]: 物理信息神经网络中的最大似然估计器用于高维反问题求解

    Maximum-likelihood Estimators in Physics-Informed Neural Networks for High-dimensional Inverse Problems. (arXiv:2304.05991v1 [cs.LG])

    [http://arxiv.org/abs/2304.05991](http://arxiv.org/abs/2304.05991)

    本论文提出了在PINNs中使用MLE的方法，消除了超参数调整。通过ODE耦合矩阵的SVD分解降维，增加了PINNs预测的稳定性和泛化能力。

    

    物理信息神经网络(PINNs)已被证明是解决反常(ODE)和偏微分方程(PDE)的合适数学框架。典型的反向PINNs被制定为带有几个超参数的软约束多目标优化问题。在本文中，我们证明反向PINNs可以用极大似然估计器(MLE)的形式来表达，通过Taylor展开，将插值误差明确地传播到物理模型空间中，而无需进行超参数调整。我们探讨了其应用于高维耦合ODEs的情况，这些ODEs受到在瞬态化学和生物动力学中常见的微分代数方程的限制。此外，我们还展示了ODE耦合矩阵(反应化学计量矩阵)的奇异值分解(SVD)提供了减少的不相关子空间，在其中可以表示PINNs解，并可以对残差进行投影。最后，SVD基函数作为先验约束增强了预测的稳定性和泛化能力。

    Physics-informed neural networks (PINNs) have proven a suitable mathematical scaffold for solving inverse ordinary (ODE) and partial differential equations (PDE). Typical inverse PINNs are formulated as soft-constrained multi-objective optimization problems with several hyperparameters. In this work, we demonstrate that inverse PINNs can be framed in terms of maximum-likelihood estimators (MLE) to allow explicit error propagation from interpolation to the physical model space through Taylor expansion, without the need of hyperparameter tuning. We explore its application to high-dimensional coupled ODEs constrained by differential algebraic equations that are common in transient chemical and biological kinetics. Furthermore, we show that singular-value decomposition (SVD) of the ODE coupling matrices (reaction stoichiometry matrix) provides reduced uncorrelated subspaces in which PINNs solutions can be represented and over which residuals can be projected. Finally, SVD bases serve as pr
    
[^91]: 折扣强化学习中的采样和估计故事

    A Tale of Sampling and Estimation in Discounted Reinforcement Learning. (arXiv:2304.05073v1 [cs.LG])

    [http://arxiv.org/abs/2304.05073](http://arxiv.org/abs/2304.05073)

    本文通过最小值上界提出了折扣均值估计问题的估计误差与马尔可夫过程混合特性和折扣因子之间的明确联系，并对一组显著估计器及其对应的采样程序进行了统计分析。

    

    折扣强化学习中最相关的问题包括在马尔可夫奖励过程的稳态分布下对函数均值进行估计，例如策略评估中的预期回报或策略优化中的策略梯度。在实践中，这些估计通过有限地进行周期采样来产生，这种采样方式忽略了马尔可夫过程的混合特性。目前还不清楚这种实际和理论设置之间的不匹配如何影响估计，文献中也缺乏对周期采样的缺陷以及如何最优地进行周期采样的正式研究。在本文中，我们提出了折扣均值估计问题的最小值上界，明确地将估计误差与马尔可夫过程的混合特性和折扣因子联系起来。然后，我们对一组显著估计器及其对应的采样程序进行了统计分析，包括通常使用的有限时间估计器。

    The most relevant problems in discounted reinforcement learning involve estimating the mean of a function under the stationary distribution of a Markov reward process, such as the expected return in policy evaluation, or the policy gradient in policy optimization. In practice, these estimates are produced through a finite-horizon episodic sampling, which neglects the mixing properties of the Markov process. It is mostly unclear how this mismatch between the practical and the ideal setting affects the estimation, and the literature lacks a formal study on the pitfalls of episodic sampling, and how to do it optimally. In this paper, we present a minimax lower bound on the discounted mean estimation problem that explicitly connects the estimation error with the mixing properties of the Markov process and the discount factor. Then, we provide a statistical analysis on a set of notable estimators and the corresponding sampling procedures, which includes the finite-horizon estimators often u
    
[^92]: 模型稀疏化可以简化机器反学习

    Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])

    [http://arxiv.org/abs/2304.04934](http://arxiv.org/abs/2304.04934)

    本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。

    

    最近的数据管制要求机器反学习（MU）：从模型中移除指定样例的影响。虽然可以通过使用剩余数据从头开始进行模型重新训练来进行精确反学习，但是其计算成本导致了近似但高效的反学习方案的开发。除了数据中心的MU解决方案，我们通过一种新颖的基于模型的视角推进MU：通过权值修剪进行稀疏化。我们的理论和实践结果表明，模型稀疏性可以提高近似反学习器的多标准反学习性能，缩小近似间隙，同时保持高效。有了这个认识，我们制定了两个新的稀疏感知反学习元方案，称为“先修剪，然后反学习”和“稀疏感知反学习”。广泛的实验表明，我们的发现和提议在各种场景下始终有益于MU，包括按类数据擦除、随机数据擦除和后门数据伪造等。

    Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
    
[^93]: 使用MicroTVM将机器学习模型部署到边缘Ahead-of-Time运行

    Deploying Machine Learning Models to Ahead-of-Time Runtime on Edge Using MicroTVM. (arXiv:2304.04842v1 [cs.LG])

    [http://arxiv.org/abs/2304.04842](http://arxiv.org/abs/2304.04842)

    本文介绍了使用MicroTVM在边缘设备上部署机器学习模型的方法，可以将预训练模型解析为后端的C源代码库，并使用自动生成的Ahead-of-Time C运行时在ARM Cortex M4F核心上进行手势识别实验。

    

    近年来，越来越多的AI应用程序已经应用到边缘设备上。然而，由数据科学家使用机器学习框架（如PyTorch或TensorFlow）训练的模型无法无缝地在边缘上执行。在本文中，我们开发了一个端到端的代码生成器，使用MicroTVM将预训练模型解析为后端的C源代码库，MicroTVM是一种机器学习编译器框架扩展，用于处理裸机设备上的推断。分析表明，特定的计算密集型运算符可以轻松地通过通用模块加速器（UMA）接口卸载到专用加速器上，而其他运算符则在CPU核心中处理。通过使用自动生成的Ahead-of-Time C运行时，在ARM Cortex M4F核心上进行手势识别实验。

    In the past few years, more and more AI applications have been applied to edge devices. However, models trained by data scientists with machine learning frameworks, such as PyTorch or TensorFlow, can not be seamlessly executed on edge. In this paper, we develop an end-to-end code generator parsing a pre-trained model to C source libraries for the backend using MicroTVM, a machine learning compiler framework extension addressing inference on bare metal devices. An analysis shows that specific compute-intensive operators can be easily offloaded to the dedicated accelerator with a Universal Modular Accelerator (UMA) interface, while others are processed in the CPU cores. By using the automatically generated ahead-of-time C runtime, we conduct a hand gesture recognition experiment on an ARM Cortex M4F core.
    
[^94]: REDf：基于长短期记忆网络的智能电网可再生能源需求预测模型

    REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network. (arXiv:2304.03997v1 [cs.LG])

    [http://arxiv.org/abs/2304.03997](http://arxiv.org/abs/2304.03997)

    本文提出了一种基于长短期记忆网络的智能电网可再生能源需求预测模型REDf，可以提供准确的能量需求预测，改善可再生能源的集成，实验结果表明其准确度优于其他模型。

    

    随着世界向更可持续的能源未来发展，将可再生能源源纳入电网的集成变得越来越重要。然而，可再生能源的间歇性使电网管理和确保稳定的电力供应变得具有挑战性。本文提出了一种基于深度学习的方法来预测智能电网中的能量需求，可以通过提供准确的能量需求预测来改善可再生能源的集成。我们使用长短期记忆网络来捕捉能럟需求数据中的复杂模式和依赖关系，这些网络特别适用于时间序列数据。所提出的方法使用了四个历史能量需求数据集，这些数据集来自不同的能源分配公司，包括美国电力、Commonwealth Edison、Dayton Power and Light以及宾夕法尼亚-新泽西-马里兰互联网。该方法还将REDf模型与其他两个深度学习模型和基准模型进行比较。实验结果表明，我们提出的REDf模型在平均绝对误差、均方根误差和决定系数等准确度指标方面优于其他模型。因此，REDf可以作为可再生能源需求预测的可靠工具，并提高可再生能源纳入智能电网的能力。

    The integration of renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future. However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity. In this paper, we propose a deep learning-based approach for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. We use long short-term memory networks, which are well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four datasets of historical energy demand data from different energy distribution companies including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is also compared with two 
    
[^95]: InstructBio：一种针对生物化学问题的大规模半监督学习范式。

    InstructBio: A Large-scale Semi-supervised Learning Paradigm for Biochemical Problems. (arXiv:2304.03906v1 [cs.LG])

    [http://arxiv.org/abs/2304.03906](http://arxiv.org/abs/2304.03906)

    InstructBio是一种针对生物化学问题的大规模半监督学习算法，引入教练模型提供有效的置信度比率来指导目标模型对不同数据点给予明显关注，避免依赖有限的标记数据和不正确的伪注释，提高了分子模型的泛化能力。

    

    在科学人工智能领域，面对真实世界问题中的有限标记数据始终是一个重要的挑战。目前的方法是在大型未标记语料库上预训练强力的任务无关模型，但在向下游任务转移知识方面可能存在困难。在本研究中，我们提出了InstructBio，一种半监督学习算法，更好地利用未标记的样例。它引入教练模型来提供伪标签可靠性的置信度比率。这些置信度分数然后指导目标模型对不同的数据点给予明显的关注，避免对标记数据的过度依赖以及不正确的伪注释的负面影响。全面的实验表明，InstructBio显著提高了分子模型的泛化能力，不仅在分子属性预测方面，在活性悬崖估计方面也表现出优越性。

    In the field of artificial intelligence for science, it is consistently an essential challenge to face a limited amount of labeled data for real-world problems. The prevailing approach is to pretrain a powerful task-agnostic model on a large unlabeled corpus but may struggle to transfer knowledge to downstream tasks. In this study, we propose InstructMol, a semi-supervised learning algorithm, to take better advantage of unlabeled examples. It introduces an instructor model to provide the confidence ratios as the measurement of pseudo-labels' reliability. These confidence scores then guide the target model to pay distinct attention to different data points, avoiding the over-reliance on labeled data and the negative influence of incorrect pseudo-annotations. Comprehensive experiments show that InstructBio substantially improves the generalization ability of molecular models, in not only molecular property predictions but also activity cliff estimations, demonstrating the superiority of 
    
[^96]: 个性化联邦学习与本地注意力

    Personalized Federated Learning with Local Attention. (arXiv:2304.01783v1 [cs.LG])

    [http://arxiv.org/abs/2304.01783](http://arxiv.org/abs/2304.01783)

    本文提出了一个名为pFedLA的算法，通过将注意力机制并入个性化模型来解决联邦学习中客户端数据异质性的问题，并在实验中表现出了优异的表现，尤其是在缓解特征漂移问题方面。

    

    联邦学习旨在学习一个单一的全局模型，使得中央服务器可以帮助在本地客户端进行模型训练，而不必访问其本地数据。联邦学习的关键挑战是不同客户端中本地数据的异质性，例如异质标签分布和特征偏移，这可能导致学习模型的显着性能降低。为解决这个问题，我们提出了一种简单而有效的算法，即具有本地注意力的个性化联邦学习（pFedLA），通过将注意机制并入客户端的个性化模型，同时保持注意块特定于客户端。具体而言，pFedLA提出了两个模块，即个性化单注意模块和个性化混合注意模块。此外，我们还介绍了一个新的FL数据集SplitMNIST-C，通过引入训练和测试数据之间的偏移。在SplitMNIST-C和EMNIST上的实验结果表明，pFedLA在准确性和收敛速度方面优于最先进的FL算法，并且在缓解特征漂移问题方面特别有效。

    Federated Learning (FL) aims to learn a single global model that enables the central server to help the model training in local clients without accessing their local data. The key challenge of FL is the heterogeneity of local data in different clients, such as heterogeneous label distribution and feature shift, which could lead to significant performance degradation of the learned models. Although many studies have been proposed to address the heterogeneous label distribution problem, few studies attempt to explore the feature shift issue. To address this issue, we propose a simple yet effective algorithm, namely \textbf{p}ersonalized \textbf{Fed}erated learning with \textbf{L}ocal \textbf{A}ttention (pFedLA), by incorporating the attention mechanism into personalized models of clients while keeping the attention blocks client-specific. Specifically, two modules are proposed in pFedLA, i.e., the personalized single attention module and the personalized hybrid attention module. In addit
    
[^97]: 基于Transformer的渐进式自监督学习在异常检测和定位中的应用

    Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization. (arXiv:2303.17354v1 [cs.CV])

    [http://arxiv.org/abs/2303.17354](http://arxiv.org/abs/2303.17354)

    该论文提出一种基于Transformer骨干网络的渐进式自监督学习方法，可用于图像异常检测和定位，其中第一阶段使用MAE模型进行正常图像的训练，第二阶段使用像素级数据增强技术来生成损坏的正常图像，最终通过像素重建误差矩阵和像素异常概率矩阵综合得到一个异常得分矩阵。

    

    在机器学习领域，对于图像数据中的异常检测和定位的研究，尤其是在工业缺陷检测等实际应用中引起了重视。虽然现有的方法主要依赖于卷积神经网络（CNN）作为骨干网络，但我们提出了一种基于Transformer骨干网络的创新方法。我们的方法采用了两阶段的渐进式学习策略。在第一阶段，我们仅使用正常图像对Masked Autoencoder （MAE）模型进行训练，在第二阶段，我们实现了像素级数据增强技术以生成已损坏的正常图像及其相应的像素标签。这个过程使得模型可以学习如何修复损坏的区域和分类每个像素的状态。最终，该模型产生一个像素重建误差矩阵和一个像素异常概率矩阵，这两个矩阵综合成一个异常得分矩阵，有效地用于图像异常检测。

    In the machine learning domain, research on anomaly detection and localization within image data has garnered significant attention, particularly in practical applications such as industrial defect detection. While existing approaches predominantly rely on Convolutional Neural Networks (CNN) as their backbone network, we propose an innovative method based on the Transformer backbone network. Our approach employs a two-stage incremental learning strategy. In the first stage, we train a Masked Autoencoder (MAE) model exclusively on normal images. Subsequently, in the second stage, we implement pixel-level data augmentation techniques to generate corrupted normal images and their corresponding pixel labels. This process enables the model to learn how to repair corrupted regions and classify the state of each pixel. Ultimately, the model produces a pixel reconstruction error matrix and a pixel anomaly probability matrix, which are combined to create an anomaly scoring matrix that effective
    
[^98]: 深度集合在多输出回归任务中量化校准不确定性的探究

    Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task. (arXiv:2303.16210v1 [cs.LG])

    [http://arxiv.org/abs/2303.16210](http://arxiv.org/abs/2303.16210)

    本研究探究了在多输出回归任务中应用深度集合量化校准不确定性的方法，提出了该方法的改进框架，其在回归准确性、不确定性估计可靠性和训练效率方面具有优越表现。

    

    深度集合是逼近贝叶斯推断的一种简单直接的方法，已被成功应用于许多分类任务。本研究旨在全面探究该方法在多输出回归任务中的应用，以预测导弹结构的空气动力性能。通过仔细研究集合中神经网络数量的影响，观察到估计的不确定性普遍存在低估的趋势。在此背景下，提出了一种应用事后校准的深度集合框架，并证明其改进的不确定性量化性能。直观地将其与高斯过程回归进行比较，这是工程中最常用的不确定性量化模型，结果表明在回归准确性、估计不确定性的可靠性和训练效率方面具有卓越的表现。最后，本文也研究了所提出框架对贝叶斯优化结果的影响。

    Deep ensemble is a simple and straightforward approach for approximating Bayesian inference and has been successfully applied to many classification tasks. This study aims to comprehensively investigate this approach in the multi-output regression task to predict the aerodynamic performance of a missile configuration. By scrutinizing the effect of the number of neural networks used in the ensemble, an obvious trend toward underconfidence in estimated uncertainty is observed. In this context, we propose the deep ensemble framework that applies the post-hoc calibration method, and its improved uncertainty quantification performance is demonstrated. It is compared with Gaussian process regression, the most prevalent model for uncertainty quantification in engineering, and is proven to have superior performance in terms of regression accuracy, reliability of estimated uncertainty, and training efficiency. Finally, the impact of the suggested framework on the results of Bayesian optimizatio
    
[^99]: 使用胸部CT进行异构数据联邦学习

    Federated Learning on Heterogenous Data using Chest CT. (arXiv:2303.13567v1 [cs.LG])

    [http://arxiv.org/abs/2303.13567](http://arxiv.org/abs/2303.13567)

    本研究使用联邦学习技术在全球21家医院的10,000多位COVID患者的胸部CT扫描图像数据集上进行了研究，提出了三种联邦学习策略，并提出了结合合成生成数据的联邦学习策略，为医学AI在异构数据上的应用提供了新的解决方案。

    

    大规模数据推动了人工智能的快速发展。虽然众所周知，来自遗传、性别、种族、饮食和各种环境因素的人群差异对疾病贡献显著，但是，在医学中进行的AI研究主要集中在具有较少数据来源和较少样本差异的区域患者队列中。这种限制源于在医学中实现大规模数据共享的障碍以及对数据隐私的道德担忧。联邦学习（FL）是一种潜在的AI发展途径，可以在医院之间进行学习，而无需进行数据共享。在本研究中，我们展示了使用三种技术（联邦平均（FedAvg）、增量制度学习（IIL）和循环增量制度学习（CIIL））在涵盖5个大陆的21家参与医院中进行的联邦学习策略在COVID-19胸部CT数据集上的结果，该数据集是最大和最多样化的，包括>10,000位患者和>1,000,000张图像。我们还提出了一种利用合成生成数据和FL策略相结合的方法。

    Large data have accelerated advances in AI. While it is well known that population differences from genetics, sex, race, diet, and various environmental factors contribute significantly to disease, AI studies in medicine have largely focused on locoregional patient cohorts with less diverse data sources. Such limitation stems from barriers to large-scale data share in medicine and ethical concerns over data privacy. Federated learning (FL) is one potential pathway for AI development that enables learning across hospitals without data share. In this study, we show the results of various FL strategies on one of the largest and most diverse COVID-19 chest CT datasets: 21 participating hospitals across five continents that comprise >10,000 patients with >1 million images. We present three techniques: Fed Averaging (FedAvg), Incremental Institutional Learning (IIL), and Cyclical Incremental Institutional Learning (CIIL). We also propose an FL strategy that leverages synthetically generated 
    
[^100]: CoLT5: 基于条件计算的快速长距离Transformer模型

    CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])

    [http://arxiv.org/abs/2303.09752](http://arxiv.org/abs/2303.09752)

    CoLT5是一种基于条件计算的Transformer模型，通过优先处理重要标记来加速长距离输入的处理。CoLT5在SCROLLS基准测试上表现最好，并能够有效地处理长达64k输入长度。

    

    许多自然语言处理任务需要处理长输入，但使用Transformer处理长文档很昂贵——这不仅是因为二次注意复杂性，还因为对每个标记应用前馈和投影层。然而，不是所有标记都同样重要，特别是对于较长的文档。我们提出了CoLT5，一种长输入Transformer模型，通过使用条件计算来利用此直觉，在前馈和注意层中为重要标记提供更多资源。我们展示了CoLT5比LongT5表现更强，训练和推理速度更快，在长输入SCROLLS基准测试上达到了SOTA。此外，CoLT5能够有效且可控地利用极长的输入，展示了高达64k输入长度的强大增益。

    Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
    
[^101]: 《具有最优n的n步时序差分学习》

    n-Step Temporal Difference Learning with Optimal n. (arXiv:2303.07068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.07068](http://arxiv.org/abs/2303.07068)

    本文提出了使用SPSA算法求解n步时序差分学习中的最优n值的算法SDPSA，并证明了其收敛性和有效性。

    

    本文考虑了在n步时序差分算法中找到最优n值的问题。我们采用了模型自由优化技术，即同时扰动随机逼近（SPSA）方法来寻找最优n。我们采用了一个模拟SPSA程序，将其原始连续优化框架引入离散优化框架，但并结合了循环扰动序列。我们证明了我们提出的算法SDPSA的收敛性，并表明它可以在任意初始值的情况下找到n步TD中的最优n值。通过实验，我们展示了SDPSA能够实现最优n值的求解。

    We consider the problem of finding the optimal value of n in the n-step temporal difference (TD) algorithm. We find the optimal n by resorting to the model-free optimization technique of simultaneous perturbation stochastic approximation (SPSA). We adopt a one-simulation SPSA procedure that is originally for continuous optimization to the discrete optimization framework but incorporates a cyclic perturbation sequence. We prove the convergence of our proposed algorithm, SDPSA, and show that it finds the optimal value of n in n-step TD. Through experiments, we show that the optimal value of n is achieved with SDPSA for any arbitrary initial value of the same.
    
[^102]: MCTS-GEB：蒙特卡洛树搜索是一个好的E图构建器

    MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder. (arXiv:2303.04651v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.04651](http://arxiv.org/abs/2303.04651)

    MCTS-GEB是一个通用的重写系统，使用强化学习和蒙特卡洛树搜索来构建最优的E图，有效消除了E图构建中的顺序问题，并在评估中表现出很好的性能。

    

    重写系统广泛使用等式饱和技术来优化重写顺序，但是当E图没有饱和时，无法代表所有可能的重写机会，会重新引入问题。为了解决这个问题，我们提出了MCTS-GEB，一个应用强化学习于E图构建的通用重写系统。MCTS-GEB使用蒙特卡洛树搜索（MCTS）高效规划最优的E图构建，有效地消除了E图构建阶段的顺序问题，并且在合理时间内取得了更好的性能。在两个不同领域的评估中，MCTS-GEB都表现出很好的性能。

    Rewrite systems [6, 10, 12] have been widely employing equality saturation [9], which is an optimisation methodology that uses a saturated e-graph to represent all possible sequences of rewrite simultaneously, and then extracts the optimal one. As such, optimal results can be achieved by avoiding the phase-ordering problem. However, we observe that when the e-graph is not saturated, it cannot represent all possible rewrite opportunities and therefore the phase-ordering problem is re-introduced during the construction phase of the e-graph. To address this problem, we propose MCTS-GEB, a domain-general rewrite system that applies reinforcement learning (RL) to e-graph construction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3] to efficiently plan for the optimal e-graph construction, and therefore it can effectively eliminate the phase-ordering problem at the construction phase and achieve better performance within a reasonable time. Evaluation in two different domains 
    
[^103]: 学习如何在联邦学习中设置后门

    Learning to Backdoor Federated Learning. (arXiv:2303.03320v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03320](http://arxiv.org/abs/2303.03320)

    本文提出了一种基于强化学习的后门攻击框架，能够在联邦学习中成功设置后门，在现有防御措施下也具有强大的攻击性能和耐用性。

    

    在联邦学习（FL）系统中，恶意参与者可以轻易地将后门嵌入到聚合模型中，同时保持模型在主任务上的性能。为此，近期提出了各种防御措施，包括训练阶段的基于聚合的防御和后期防御措施。虽然这些防御措施在现有的基于启发式的后门攻击中可以获得合理的性能，但我们发现它们在面对更高级的攻击时不足以应对。特别地，我们提出了一种基于强化学习（RL）的后门攻击框架，攻击者首先使用基于其本地数据和FL系统的共同知识建立的仿真器训练一个（非近视）攻击策略，然后在实际的FL训练中应用它。我们的攻击框架既是适应性又是灵活的，并且即使在最先进的防御措施下也能实现强大的攻击性能和耐用性。

    In a federated learning (FL) system, malicious participants can easily embed backdoors into the aggregated model while maintaining the model's performance on the main task. To this end, various defenses, including training stage aggregation-based defenses and post-training mitigation defenses, have been proposed recently. While these defenses obtain reasonable performance against existing backdoor attacks, which are mainly heuristics based, we show that they are insufficient in the face of more advanced attacks. In particular, we propose a general reinforcement learning-based backdoor attack framework where the attacker first trains a (non-myopic) attack policy using a simulator built upon its local data and common knowledge on the FL system, which is then applied during actual FL training. Our attack framework is both adaptive and flexible and achieves strong attack performance and durability even under state-of-the-art defenses.
    
[^104]: WISK：一种用于空间关键字查询的工作负载感知学习索引

    WISK: A Workload-aware Learned Index for Spatial Keyword Queries. (arXiv:2302.14287v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2302.14287](http://arxiv.org/abs/2302.14287)

    WISK是一种用于空间关键字查询的学习索引，它可以利用已知的查询分布来自适应地优化查询成本，并在考虑了空间属性和文本信息后进行学习。在实验中，WISK表现出比现有的索引更好的查询延迟和竞争性的索引大小。

    

    空间对象常常伴随着文本信息，例如包含它们描述的兴趣点 (POIs)，这被称为地理-文本数据。为了检索这样的数据，已经广泛研究了同时考虑空间接近性和文本关联性的空间关键字查询。为空间关键字查询设计的现有索引大多基于地理-文本数据构建，而没有考虑已经收到的查询的分布。然而，先前的研究表明，利用已知的查询分布可以改进未来查询处理的索引结构。在本文中，我们提出了 WISK，一种用于空间关键字查询的学习索引，它自适应于优化查询成本，给定一个查询工作负载。其中一个关键挑战是如何在学习索引时利用结构化的空间属性和非结构化的文本信息。我们首先将数据对象划分为分区，旨在最小化给定查询工作负载的处理成本。然后，我们提出了一种基于模型的方法，通过考虑已经接收到的查询的分布来学习空间关键字查询的最优划分方案和索引结构。我们在真实的地理-文本数据集上进行的实验表明，WISK在查询延迟方面优于现有的索引，同时保持竞争性的索引大小。

    Spatial objects often come with textual information, such as Points of Interest (POIs) with their descriptions, which are referred to as geo-textual data. To retrieve such data, spatial keyword queries that take into account both spatial proximity and textual relevance have been extensively studied. Existing indexes designed for spatial keyword queries are mostly built based on the geo-textual data without considering the distribution of queries already received. However, previous studies have shown that utilizing the known query distribution can improve the index structure for future query processing. In this paper, we propose WISK, a learned index for spatial keyword queries, which self-adapts for optimizing querying costs given a query workload. One key challenge is how to utilize both structured spatial attributes and unstructured textual information during learning the index. We first divide the data objects into partitions, aiming to minimize the processing costs of the given que
    
[^105]: Inseq：一个用于序列生成模型的可解释性工具包

    Inseq: An Interpretability Toolkit for Sequence Generation Models. (arXiv:2302.13942v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13942](http://arxiv.org/abs/2302.13942)

    本文介绍了Inseq，这是一个Python工具包，旨在推广可解释性序列生成模型的分析。它为常见的解码器和编码器-解码器Transformers架构提供了提取模型内部信息和特征重要性得分的直观优化方法。作者还在机器翻译模型和GPT-2中展示了Inseq的潜力，证明其有助于推动可解释性自然语言生成的未来发展。

    

    自然语言处理领域的过去的可解释性研究主要集中在流行的分类任务上，而在生成任务中往往被忽视，部分原因是缺乏专门的工具。在本文中，我们介绍了Inseq，一个Python库，用于使序列生成模型的可解释性分析普及化。Inseq能够直观且优化地提取流行的仅解码器和编码器解码器Transformers架构的模型内部信息和特征重要性分数。我们还展示了它的潜力，通过使用它来突出机器翻译模型中的性别偏见并在GPT-2中定位事实知识。由于其支持对比特征归因等前沿技术的可扩展接口，因此Inseq可以推动可解释性自然语言生成的未来发展，集中优良实践，并实现公正和可重复的模型评估。

    Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.
    
[^106]: TwERC: Twitter广告推荐高性能集成式候选生成

    TwERC: High Performance Ensembled Candidate Generation for Ads Recommendation at Twitter. (arXiv:2302.13915v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2302.13915](http://arxiv.org/abs/2302.13915)

    本文介绍了TwERC方法，用于解决Twitter广告推荐的候选生成问题。该方法结合了实时轻型排名器和数据来源策略，成功提高了推荐系统的性能和收入。其中，基于相互作用图和排名分数缓存的两种策略互补应用，分别实现了4.08%和1.38%的增益。

    

    推荐系统是社交媒体公司的核心功能，包括有机和推广内容推荐。现代推荐系统通常分为多个阶段，即候选生成和重排序，以平衡计算成本和推荐质量。本文聚焦于大规模广告推荐问题中的候选生成阶段，并提出了一种机器学习为先的异构重构方法，称为TwERC。我们展示了一个将实时轻型排名器与能够捕获额外信息的数据来源策略相结合的系统，可以提供验证增益。我们提出了两种策略。第一个策略使用相互作用图中的相似性概念，而第二个策略缓存排名阶段的先前得分。基于图形的策略实现了4.08%的收入增益，而基于排名分数的策略实现了1.38%的增益。这两种策略具有互补偏差。

    Recommendation systems are a core feature of social media companies with their uses including recommending organic and promoted contents. Many modern recommendation systems are split into multiple stages - candidate generation and heavy ranking - to balance computational cost against recommendation quality. We focus on the candidate generation phase of a large-scale ads recommendation problem in this paper, and present a machine learning first heterogeneous re-architecture of this stage which we term TwERC. We show that a system that combines a real-time light ranker with sourcing strategies capable of capturing additional information provides validated gains. We present two strategies. The first strategy uses a notion of similarity in the interaction graph, while the second strategy caches previous scores from the ranking stage. The graph based strategy achieves a 4.08% revenue gain and the rankscore based strategy achieves a 1.38% gain. These two strategies have biases that complemen
    
[^107]: Alloprof：一个新的法语问答教育数据集及其在信息检索案例研究中的应用

    Alloprof: a new French question-answer education dataset and its use in an information retrieval case study. (arXiv:2302.07738v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07738](http://arxiv.org/abs/2302.07738)

    这个论文介绍了一个新的阿洛普夫法语问答数据集，收集了来自10,368名学生的29,349个问题和解释，并展示了在信息检索任务中使用该数据集的案例研究。

    

    教师和学生越来越依赖在线学习资源来补充学校提供的资源。可用资源的广度和深度的增加对学生来说是一件好事，但前提是他们能够找到答案。问答和信息检索系统已受益于公共数据集，以训练和评估其算法，但大多数这些数据集都是英文文本，由成年人编写和阅读。我们介绍了一个新的公共法语问答数据集，从总部位于魁北克的小学和中学帮助网站Alloprof收集，包含29,349个问题及其解释，涵盖各种学科的10,368名学生，超过一半的解释包含链接到其他问题或网站上的2,596个参考页面之一。我们还向您展示了在信息检索任务中使用本数据集的案例研究。

    Teachers and students are increasingly relying on online learning resources to supplement the ones provided in school. This increase in the breadth and depth of available resources is a great thing for students, but only provided they are able to find answers to their queries. Question-answering and information retrieval systems have benefited from public datasets to train and evaluate their algorithms, but most of these datasets have been in English text written by and for adults. We introduce a new public French question-answering dataset collected from Alloprof, a Quebec-based primary and high-school help website, containing 29 349 questions and their explanations in a variety of school subjects from 10 368 students, with more than half of the explanations containing links to other questions or some of the 2 596 reference pages on the website. We also present a case study of this dataset in an information retrieval task. This dataset was collected on the Alloprof public forum, with 
    
[^108]: 几何深度学习仅依靠距离矩阵足够吗？

    Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05743](http://arxiv.org/abs/2302.05743)

    本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。

    

    图神经网络（GNN）常用于涉及图形几何的任务，例如分子动力学模拟。虽然几何图的距离矩阵包含完整的几何信息，但已经证明消息传递神经网络（MPNNs）无法学习这种几何信息。本文通过构造新颖的对称几何图的家族，扩展了MPNN无法区分其距离矩阵的反例家族，并提出$k$-DisGNNs，可以有效地利用距离矩阵中丰富的几何结构。我们证明了模型的高表达能力，并证明了一些现有的精心设计的几何模型可以作为$k$-DisGNNs的特殊情况统一起来。最重要的是，我们建立了几何深度学习和传统图表示学习之间的联系，展示了那些最初为低度表达能力的GNN模型设计的高度表达力的GNN模型。

    Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
    
[^109]: 学习动态攻击下的近似最优入侵响应

    Learning Near-Optimal Intrusion Responses Against Dynamic Attackers. (arXiv:2301.06085v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2301.06085](http://arxiv.org/abs/2301.06085)

    本文通过博弈论建模，开发一种防御策略，以应对自主适应攻击者，在阈值特性证明的基础上，使用 Threshold Fictitious Self-Play (T-FP) 算法学习纳什均衡，实现近似最优解。

    

    本文研究自动化入侵响应，并将攻击者与防御者之间的交互形式建模为一种最优停止博弈，攻击和防御策略通过强化学习和自我博弈发展。博弈论建模使我们能够找到针对动态攻击者有效的防御者策略，即攻击者根据防御者策略进行反应来自适应的攻击者。此外，最优停止公式使我们能够证明最优策略具有阈值特性。为了获得近似最优的防御者策略，我们开发了 Threshold Fictitious Self-Play (T-FP)，这是一种虚构自我博弈算法，通过随机逼近学习纳什均衡 。我们证明了 T-FP 在我们的用例中优于现有算法。该研究的实验部分包括两个系统：一个模拟系统，其中防御策略是逐步学习的，以及一个仿真系统，收集了驱动模拟的统计数据。

    We study automated intrusion response and formulate the interaction between an attacker and a defender as an optimal stopping game where attack and defense strategies evolve through reinforcement learning and self-play. The game-theoretic modeling enables us to find defender strategies that are effective against a dynamic attacker, i.e. an attacker that adapts its strategy in response to the defender strategy. Further, the optimal stopping formulation allows us to prove that optimal strategies have threshold properties. To obtain near-optimal defender strategies, we develop Threshold Fictitious Self-Play (T-FP), a fictitious self-play algorithm that learns Nash equilibria through stochastic approximation. We show that T-FP outperforms a state-of-the-art algorithm for our use case. The experimental part of this investigation includes two systems: a simulation system where defender strategies are incrementally learned and an emulation system where statistics are collected that drive simu
    
[^110]: ViTs for SITS：基于Vision Transformer的卫星图像时序处理模型

    ViTs for SITS: Vision Transformers for Satellite Image Time Series. (arXiv:2301.04944v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.04944](http://arxiv.org/abs/2301.04944)

    本文介绍了一种全自注意力模型TSViT，用于处理通用的卫星图像时序(SITS)，通过分解的时空编码器处理非重叠的块，提出了两种新的机制以增强模型的判别能力，最终在SITS语义分割和分类中取得了显著的优势，达到了最先进水平。

    

    本文介绍了一种全自注意力模型——Temporo-Spatial Vision Transformer（TSViT），用于处理通用的卫星图像时序(SITS)，并将其基于Vision Transformer（ViT）进行了改进。TSViT将SITS记录在时空上划分成非重叠的块，这些块被记号化后由分解的时空编码器处理。我们认为，相比自然图像，时域优先空间次之的分解对于SITS处理更加直观，并提供了实验证据。此外，我们通过引入两种新的机制，即获取时间特定的编码和多个可学习类标记，增强了模型的判别能力。所有新设计的效果通过广泛的消融研究进行了评估。我们提出的架构在三个公开的SITS语义分割和分类中取得了显著的优势，达到了最先进水平。

    In this paper we introduce the Temporo-Spatial Vision Transformer (TSViT), a fully-attentional model for general Satellite Image Time Series (SITS) processing based on the Vision Transformer (ViT). TSViT splits a SITS record into non-overlapping patches in space and time which are tokenized and subsequently processed by a factorized temporo-spatial encoder. We argue, that in contrast to natural images, a temporal-then-spatial factorization is more intuitive for SITS processing and present experimental evidence for this claim. Additionally, we enhance the model's discriminative power by introducing two novel mechanisms for acquisition-time-specific temporal positional encodings and multiple learnable class tokens. The effect of all novel design choices is evaluated through an extensive ablation study. Our proposed architecture achieves state-of-the-art performance, surpassing previous approaches by a significant margin in three publicly available SITS semantic segmentation and classific
    
[^111]: 带有Cox-Weibull神经网络的贝叶斯武器系统可靠性建模

    Bayesian Weapon System Reliability Modeling with Cox-Weibull Neural Network. (arXiv:2301.01850v5 [stat.AP] UPDATED)

    [http://arxiv.org/abs/2301.01850](http://arxiv.org/abs/2301.01850)

    该论文提出了一种带有Cox-Weibull神经网络的贝叶斯武器系统可靠性建模方法，可以通过整合武器系统特征来改善预测性维修，相较于传统模型的表现更优。

    

    我们提出通过神经网络（例如DeepSurv）将武器系统特征（如武器系统制造商、部署时间和地点、存储时间和地点等）整合到参数化的Cox-Weibull可靠性模型中，以改善预测性维修。同时，我们通过使用MC-dropout等退火方法来将Weibull参数参数化并开发了另一种贝叶斯模型，以进行比较目的。因为武器系统测试中的数据收集程序，我们采用了一个新颖的区间被审查对数似然函数，它在梯度下降优化期间合并了Weibull参数的MCMC取样。我们比较了分类指标，如接收器工作特性（ROC）曲线下面积（AUC）、精度-召回（PR）AUC和F分数，以展示我们的模型通常优于传统的强大模型，如XGBoost和当前标准的条件Weibull概率模型。

    We propose to integrate weapon system features (such as weapon system manufacturer, deployment time and location, storage time and location, etc.) into a parameterized Cox-Weibull [1] reliability model via a neural network, like DeepSurv [2], to improve predictive maintenance. In parallel, we develop an alternative Bayesian model by parameterizing the Weibull parameters with a neural network and employing dropout methods such as Monte-Carlo (MC)-dropout for comparative purposes. Due to data collection procedures in weapon system testing we employ a novel interval-censored log-likelihood which incorporates Monte-Carlo Markov Chain (MCMC) [3] sampling of the Weibull parameters during gradient descent optimization. We compare classification metrics such as receiver operator curve (ROC) area under the curve (AUC), precision-recall (PR) AUC, and F scores to show our model generally outperforms traditional powerful models such as XGBoost and the current standard conditional Weibull probabili
    
[^112]: 混合移动平均场引导的时空数据学习

    Mixed moving average field guided learning for spatio-temporal data. (arXiv:2301.00736v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.00736](http://arxiv.org/abs/2301.00736)

    本论文提出了一种理论引导机器学习方法，采用广义贝叶斯算法进行混合移动平均场引导的时空数据建模，可以进行因果未来预测。

    

    受到混合移动平均场的影响，时空数据的建模是一个多功能的技巧。但是，它们的预测分布通常不可访问。在这个建模假设下，我们定义了一种新的理论引导机器学习方法，采用广义贝叶斯算法进行预测。我们采用Lipschitz预测器（例如线性模型或前馈神经网络），并通过最小化沿空间和时间维度串行相关的数据的新型PAC贝叶斯界限来确定一个随机估计值。进行因果未来预测是我们方法的一个亮点，因为它适用于具有短期和长期相关性的数据。最后，我们通过展示线性预测器和模拟STOU过程的时空数据的示例来展示学习方法的性能。

    Influenced mixed moving average fields are a versatile modeling class for spatio-temporal data. However, their predictive distribution is not generally accessible. Under this modeling assumption, we define a novel theory-guided machine learning approach that employs a generalized Bayesian algorithm to make predictions. We employ a Lipschitz predictor, for example, a linear model or a feed-forward neural network, and determine a randomized estimator by minimizing a novel PAC Bayesian bound for data serially correlated along a spatial and temporal dimension. Performing causal future predictions is a highlight of our methodology as its potential application to data with short and long-range dependence. We conclude by showing the performance of the learning methodology in an example with linear predictors and simulated spatio-temporal data from an STOU process.
    
[^113]: Berlin V2X：多车辆和无线电访问技术的机器学习数据集

    Berlin V2X: A Machine Learning Dataset from Multiple Vehicles and Radio Access Technologies. (arXiv:2212.10343v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10343](http://arxiv.org/abs/2212.10343)

    这篇论文介绍了一个详细的测量活动，提供了一个多车辆和无线电访问技术的机器学习数据集，该数据集通过各种V2X研究为车辆和工业通信领域开辟了新的可能性。

    

    无线通信的发展已被预期依靠新的基于机器学习（ML）的能力。这可以使无线网络组件采取主动决策和行动，以维持服务质量（QoS）和用户体验。此外，将出现在车辆和工业通信领域的新用例。具体在车辆通信领域，车辆对一切（V2X）方案将会从这些进展中受益匪浅。考虑到此，我们进行了详细的测量活动，为各种多样的基于ML的研究铺平了道路。所得到的数据集提供了在不同城市环境下基站（使用两个不同运营商）和邻近链路无线电访问技术的GPS定位无线测量，因此可以进行各种不同的V2X研究。数据集标有标签，并以高时间分辨率采样。此外，我们将数据公开发布，并附有所需的信息。

    The evolution of wireless communications into 6G and beyond is expected to rely on new machine learning (ML)-based capabilities. These can enable proactive decisions and actions from wireless-network components to sustain quality-of-service (QoS) and user experience. Moreover, new use cases in the area of vehicular and industrial communications will emerge. Specifically in the area of vehicle communication, vehicle-to-everything (V2X) schemes will benefit strongly from such advances. With this in mind, we have conducted a detailed measurement campaign that paves the way to a plethora of diverse ML-based studies. The resulting datasets offer GPS-located wireless measurements across diverse urban environments for both cellular (with two different operators) and sidelink radio access technologies, thus enabling a variety of different studies towards V2X. The datasets are labeled and sampled with a high time resolution. Furthermore, we make the data publicly available with all the necessar
    
[^114]: 低资源命名实体识别中的AUC最大化

    AUC Maximization for Low-Resource Named Entity Recognition. (arXiv:2212.04800v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.04800](http://arxiv.org/abs/2212.04800)

    本文提出了在低资源命名实体识别中使用AUC最大化的方法，通过结合两个最大化AUC分数的二进制分类器，在低资源NER设置下实现了显着的性能提高，优于传统的损失函数。

    

    目前命名实体识别领域的工作使用交叉熵（CE）或条件随机场（CRF）作为优化NER模型的目标/损失函数。然而，这两种传统的NER问题的目标函数通常在数据分布平衡，并且有足够的注释训练样例时可以产生足够的性能。但由于NER本质上是一个不平衡的标记问题，在低资源情况下使用这些标准目标函数时，模型性能可能会受到影响。基于最大化ROC曲线下面积（AUC）的最新进展，我们提出通过最大化AUC分数来优化NER模型。我们提供证据表明，通过简单地结合两个最大化AUC分数的二进制分类器，在低资源NER设置下实现了显着的性能提高，优于传统的损失函数。我们还进行了广泛的实验，以展示我们的方法在低资源情况下的优势。

    Current work in named entity recognition (NER) uses either cross entropy (CE) or conditional random fields (CRF) as the objective/loss functions to optimize the underlying NER model. Both of these traditional objective functions for the NER problem generally produce adequate performance when the data distribution is balanced and there are sufficient annotated training examples. But since NER is inherently an imbalanced tagging problem, the model performance under the low-resource settings could suffer using these standard objective functions. Based on recent advances in area under the ROC curve (AUC) maximization, we propose to optimize the NER model by maximizing the AUC score. We give evidence that by simply combining two binary-classifiers that maximize the AUC score, significant performance improvement over traditional loss functions is achieved under low-resource NER settings. We also conduct extensive experiments to demonstrate the advantages of our method under the low-resource 
    
[^115]: 基于状态的重要性抽样方法实现低方差的行为策略离线评估

    Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03932](http://arxiv.org/abs/2212.03932)

    本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。

    

    在强化学习的离线评估中，需要评估目标策略的性能，而这需要使用由行为策略采集的样本数据。传统的重要性抽样方法由于计算动作概率比值的乘积而导致方差增加，从而在涉及长期规划的任务中出现估计不准确的问题。本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。

    In off-policy reinforcement learning, a behaviour policy performs exploratory interactions with the environment to obtain state-action-reward samples which are then used to learn a target policy that optimises the expected return. This leads to a problem of off-policy evaluation, where one needs to evaluate the target policy from samples collected by the often unrelated behaviour policy. Importance sampling is a traditional statistical technique that is often applied to off-policy evaluation. While importance sampling estimators are unbiased, their variance increases exponentially with the horizon of the decision process due to computing the importance weight as a product of action probability ratios, yielding estimates with low accuracy for domains involving long-term planning. This paper proposes state-based importance sampling (SIS), which drops the action probability ratios of sub-trajectories with "negligible states" -- roughly speaking, those for which the chosen actions have no 
    
[^116]: FaiREE：具有有限样本和无分布保证的公平分类算法

    FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee. (arXiv:2211.15072v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.15072](http://arxiv.org/abs/2211.15072)

    本研究提出了FaiREE算法，它是一种可满足群体公平性约束的公平分类算法，并且具有有限样本和无分布理论保证。在实验中表现优异。

    

    算法公平性在机器学习研究中发挥着越来越重要的作用。已经提出了几种群体公平性概念和算法。然而，现有公平分类方法的公平保证主要依赖于特定的数据分布假设，通常需要大样本量，并且在样本量较小的情况下可能会违反公平性，而这在实践中经常发生。本文提出了FaiREE算法，它是一种公平分类算法，可以在有限样本和无分布理论保证下满足群体公平性约束。FaiREE可以适应各种群体公平性概念（例如，机会平等，平衡几率，人口统计学平衡等）并实现最佳准确性。这些理论保证进一步得到了对合成和实际数据的实验支持。FaiREE表现出比最先进的算法更好的性能。

    Algorithmic fairness plays an increasingly critical role in machine learning research. Several group fairness notions and algorithms have been proposed. However, the fairness guarantee of existing fair classification methods mainly depends on specific data distributional assumptions, often requiring large sample sizes, and fairness could be violated when there is a modest number of samples, which is often the case in practice. In this paper, we propose FaiREE, a fair classification algorithm that can satisfy group fairness constraints with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfy various group fairness notions (e.g., Equality of Opportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal accuracy. These theoretical guarantees are further supported by experiments on both synthetic and real data. FaiREE is shown to have favorable performance over state-of-the-art algorithms.
    
[^117]: 监测考虑混淆医疗干预时基于机器学习的风险预测算法

    Monitoring machine learning (ML)-based risk prediction algorithms in the presence of confounding medical interventions. (arXiv:2211.09781v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.09781](http://arxiv.org/abs/2211.09781)

    在医疗保健中，通过监测条件性表现，可以有效地监测考虑混淆医疗干预时基于机器学习的风险预测算法。

    

    在医疗保健中，基于机器学习的风险预测模型的性能监测受混淆医疗干预（CMI）的影响，因为临床医生更可能为高风险患者提供预防性治疗并改变算法所预测的目标。忽略CMI并只监测未接受治疗的患者可能会导致类型I错误的膨胀，不过我们证明如果监控条件性表现，并且符合条件交换性或时间不变选择偏差，则仍然可以进行有效推断。

    Performance monitoring of machine learning (ML)-based risk prediction models in healthcare is complicated by the issue of confounding medical interventions (CMI): when an algorithm predicts a patient to be at high risk for an adverse event, clinicians are more likely to administer prophylactic treatment and alter the very target that the algorithm aims to predict. A simple approach is to ignore CMI and monitor only the untreated patients, whose outcomes remain unaltered. In general, ignoring CMI may inflate Type I error because (i) untreated patients disproportionally represent those with low predicted risk and (ii) evolution in both the model and clinician trust in the model can induce complex dependencies that violate standard assumptions. Nevertheless, we show that valid inference is still possible if one monitors conditional performance and if either conditional exchangeability or time-constant selection bias hold. Specifically, we develop a new score-based cumulative sum (CUSUM) m
    
[^118]: 过度参数化高维模型的不确定性量化研究

    A study of uncertainty quantification in overparametrized high-dimensional models. (arXiv:2210.12760v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12760](http://arxiv.org/abs/2210.12760)

    本论文研究了过度参数化高维模型中的不确定性问题，探讨了几种方法，比较了校准和分类准确性之间的权衡。结果发现最佳正则化估计量的校准曲线具有双重下降行为，与经验贝叶斯方法形成对比。

    

    不确定性量化是可靠和可信机器学习的中心挑战。在过度参数化的神经网络背景下，朴素的度量方法(如最后一层分数)已经被广为人知地产生过度自信的估计。提出了几种方法，从温度缩放到神经网络的不同贝叶斯处理，以缓解过度自信，通常通过数值观察支持它们产生更好的校准不确定性度量。在这项工作中，我们在一个数学可处理的过度参数化神经网络模型中，对于二元分类，提供了常见不确定度量之间的尖锐比较：随机特征模型。我们讨论了分类准确性和校准之间的折衷，披露最佳正则化估计量的校准曲线与过参数化的函数的双重下降行为。这与经验贝叶斯方法形成对比，我们展示它的校准是良好的。

    Uncertainty quantification is a central challenge in reliable and trustworthy machine learning. Naive measures such as last-layer scores are well-known to yield overconfident estimates in the context of overparametrized neural networks. Several methods, ranging from temperature scaling to different Bayesian treatments of neural networks, have been proposed to mitigate overconfidence, most often supported by the numerical observation that they yield better calibrated uncertainty measures. In this work, we provide a sharp comparison between popular uncertainty measures for binary classification in a mathematically tractable model for overparametrized neural networks: the random features model. We discuss a trade-off between classification accuracy and calibration, unveiling a double descent like behavior in the calibration curve of optimally regularized estimators as a function of overparametrization. This is in contrast with the empirical Bayes method, which we show to be well calibrate
    
[^119]: 压缩多维天气和气候数据到神经网络中

    Compressing multidimensional weather and climate data into neural networks. (arXiv:2210.12538v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12538](http://arxiv.org/abs/2210.12538)

    这篇论文提出了一种新方法，将多维天气和气候数据通过训练神经网络进行压缩。该方法在保留重要气象结构和不引入伪像的情况下，可将数据压缩三个数量级，为高分辨率气候数据民主化访问和多个新的研究方向提供了可能。

    

    天气和气候模拟产生的高分辨率数据达到了Petabyte级别，研究者需要对其进行分析以了解气候变化或恶劣天气。我们提出了一种新方法来压缩这种多维天气和气候数据：训练一个基于坐标的神经网络来过度拟合数据，然后取得到的参数作为原始基于网格数据的紧凑表示。虽然压缩比范围从300倍到超过3,000倍，但我们的方法在加权RMSE，MAE方面优于最先进的SZ3压缩器。它可以忠实地保留重要的大气结构并不会引入伪像。使用得到的神经网络作为790倍压缩数据加载器来训练WeatherBench预测模型，其RMSE只增加不到2%。三个数量级的压缩使得高分辨率气候数据得以民主化地访问，并使多个新的研究方向成为可能。

    Weather and climate simulations produce petabytes of high-resolution data that are later analyzed by researchers in order to understand climate change or severe weather. We propose a new method of compressing this multidimensional weather and climate data: a coordinate-based neural network is trained to overfit the data, and the resulting parameters are taken as a compact representation of the original grid-based data. While compression ratios range from 300x to more than 3,000x, our method outperforms the state-of-the-art compressor SZ3 in terms of weighted RMSE, MAE. It can faithfully preserve important large scale atmosphere structures and does not introduce artifacts. When using the resulting neural network as a 790x compressed dataloader to train the WeatherBench forecasting model, its RMSE increases by less than 2%. The three orders of magnitude compression democratizes access to high-resolution climate data and enables numerous new research directions.
    
[^120]: CPU和GPU上动态图神经网络推理的瓶颈分析

    Bottleneck Analysis of Dynamic Graph Neural Network Inference on CPU and GPU. (arXiv:2210.03900v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2210.03900](http://arxiv.org/abs/2210.03900)

    本文分析了8种不同特征的DGNN在CPU和GPU上的性能瓶颈，并提出了未来优化的机会。

    

    动态图神经网络(DGNN)因其在捕捉现实世界动态特征方面的广泛应用而变得越来越受欢迎。从算法的角度设计了各种动态图神经网络，成功地将时间信息纳入了图形处理中。尽管算法性能表现有前途，但将DGNN部署到硬件上仍面临着额外的挑战，包括模型复杂性、多样性以及时间依赖性的特性。同时，DGNN与静态图神经网络之间的差异使得针对静态图神经网络的硬件相关优化不适用于DGNN。本文选取了八种具有不同特征的流行DGNN，并在CPU和GPU上进行了分析。总结并分析了分析结果，深入探讨了DGNN在硬件上的瓶颈，并确定了未来DGNN加速的潜在优化机会。

    Dynamic graph neural network (DGNN) is becoming increasingly popular because of its widespread use in capturing dynamic features in the real world. A variety of dynamic graph neural networks designed from algorithmic perspectives have succeeded in incorporating temporal information into graph processing. Despite the promising algorithmic performance, deploying DGNNs on hardware presents additional challenges due to the model complexity, diversity, and the nature of the time dependency. Meanwhile, the differences between DGNNs and static graph neural networks make hardware-related optimizations for static graph neural networks unsuitable for DGNNs. In this paper, we select eight prevailing DGNNs with different characteristics and profile them on both CPU and GPU. The profiling results are summarized and analyzed, providing in-depth insights into the bottlenecks of DGNNs on hardware and identifying potential optimization opportunities for future DGNN acceleration. Followed by a comprehen
    
[^121]: TPGNN: 通过时间传播学习动态图中的高阶信息

    TPGNN: Learning High-order Information in Dynamic Graphs via Temporal Propagation. (arXiv:2210.01171v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01171](http://arxiv.org/abs/2210.01171)

    TPGNN 是一种基于时间传播的图卷积神经网络，用于学习动态图中的高阶信息。对节点分类和链接预测任务上的实验表明，TPGNN明显优于现有方法。

    

    时间图是用于建模由演化互动要素组成的动态系统的抽象概念。本文旨在解决一个被忽视的重要问题——如何从时间图中学习高阶邻居的信息，以增强所学节点表示的信息量和区分度？我们认为，在学习动态图中的高阶信息时，我们会遇到两个挑战：计算效率低和过度平滑，这些问题无法通过应用静态图上的传统技术来解决。为了弥补这些缺陷，我们提出了一种基于时间传播的图卷积神经网络，即TPGNN。具体而言，该模型由两个不同的部分组成，即传播器和节点编码器。传播器是用于将消息从锚节点传播到其$k$步时间邻居的工具，然后同时更新邻域状态，从而实现高效的计算，特别是当$k$很大时。节点编码器旨在编码锚节点及其$k$步邻居的信息，并生成最终的节点嵌入。我们将TPGNN应用于一组真实世界的动态网络上的节点分类和链接预测任务中，并且实验结果表明，TPGNN明显优于现有的一些方法。

    Temporal graph is an abstraction for modeling dynamic systems that consist of evolving interaction elements. In this paper, we aim to solve an important yet neglected problem -- how to learn information from high-order neighbors in temporal graphs? -- to enhance the informativeness and discriminativeness for the learned node representations. We argue that when learning high-order information from temporal graphs, we encounter two challenges, i.e., computational inefficiency and over-smoothing, that cannot be solved by conventional techniques applied on static graphs. To remedy these deficiencies, we propose a temporal propagation-based graph neural network, namely TPGNN. To be specific, the model consists of two distinct components, i.e., propagator and node-wise encoder. The propagator is leveraged to propagate messages from the anchor node to its temporal neighbors within $k$-hop, and then simultaneously update the state of neighborhoods, which enables efficient computation, especial
    
[^122]: 稀疏受限最优输运

    Sparsity-Constrained Optimal Transport. (arXiv:2209.15466v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.15466](http://arxiv.org/abs/2209.15466)

    这篇论文提出了一种新的带有输运计划显式基数约束的 OT 方法，以确保每个输入令牌都与少量专家匹配，从而提高模型的可解释性。

    

    正则化的最优输运 (OT) 现在越来越多地被用作神经网络中的损失或匹配层。使用熵正则化 OT 可以使用 Sinkhorn 算法计算，但它会产生完全密集的运输计划，这意味着所有源都与所有目标（分数）匹配。为了解决这个问题，几篇论文研究了二次正则化。这种正则化保留了稀疏性，并导致了无约束和平滑（半）对偶目标，可以使用现有的梯度方法进行求解。不幸的是，二次正则化不能直接控制运输计划的基数（非零数的数量）。本文提出了一种新的带有输运计划显式基数约束的 OT 方法。我们的工作是由对稀疏专家混合物的应用所驱动的，其中 OT 可以用于将输入令牌（例如图像补丁）与专家模型（例如神经网络）进行匹配。基数限制可以确保每个输入令牌与少量专家匹配，这有助于解释混合权重并构建更具可解释性的模型。我们推导了一种基于 Proximal 梯度方法的稀疏受限 OT 问题，并在合成和真实数据实验中展示了其有效性。

    Regularized optimal transport (OT) is now increasingly used as a loss or as a matching layer in neural networks. Entropy-regularized OT can be computed using the Sinkhorn algorithm but it leads to fully-dense transportation plans, meaning that all sources are (fractionally) matched with all targets. To address this issue, several works have investigated quadratic regularization instead. This regularization preserves sparsity and leads to unconstrained and smooth (semi) dual objectives, that can be solved with off-the-shelf gradient methods. Unfortunately, quadratic regularization does not give direct control over the cardinality (number of nonzeros) of the transportation plan. We propose in this paper a new approach for OT with explicit cardinality constraints on the transportation plan. Our work is motivated by an application to sparse mixture of experts, where OT can be used to match input tokens such as image patches with expert models such as neural networks. Cardinality constraint
    
[^123]: 将正规化流转换为保持测地线高斯流的蒙热映射

    Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows. (arXiv:2209.10873v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10873](http://arxiv.org/abs/2209.10873)

    本文提出了一种将正规化流转换为 OT 效率更高的蒙热映射的方法，通过学习源分布的重新排列实现，同时通过欧拉方程约束导致估计的蒙热映射路径位于保持体积的微分同胚空间中的测地线上。

    

    正规化流（NF）是一种强大的基于似然的生成模型，能够在表现力和可建模复杂密度的可处理性之间进行权衡。现在已经成为一个成熟的研究方向是利用最优传输（OT）寻找蒙热映射，即源分布与目标分布之间具有最小成本的模型。本文介绍了一种基于Brenier极分解定理的方法，将任何训练好的NF转换为更具OT效率版本，而不改变最终密度。我们通过学习源（高斯）分布的重新排列，最小化源与最终密度之间的OT成本。我们进一步通过欧拉方程约束导致估计的蒙热映射路径位于保持体积的微分同胚空间中的测地线上。所提出的方法导致多个现有模型的平滑流，并降低了OT成本，而不影响模型性能。

    Normalizing Flows (NF) are powerful likelihood-based generative models that are able to trade off between expressivity and tractability to model complex densities. A now well established research avenue leverages optimal transport (OT) and looks for Monge maps, i.e. models with minimal effort between the source and target distributions. This paper introduces a method based on Brenier's polar factorization theorem to transform any trained NF into a more OT-efficient version without changing the final density. We do so by learning a rearrangement of the source (Gaussian) distribution that minimizes the OT cost between the source and the final density. We further constrain the path leading to the estimated Monge map to lie on a geodesic in the space of volume-preserving diffeomorphisms thanks to Euler's equations. The proposed method leads to smooth flows with reduced OT cost for several existing models without affecting the model performance.
    
[^124]: 鲁棒性多变量时间序列预测：对抗攻击与防御机制

    Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms. (arXiv:2207.09572v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09572](http://arxiv.org/abs/2207.09572)

    本文研究多元概率预测模型的对抗攻击威胁和有效的防御机制，发现稀疏修改其他时间序列的观测对目标时间序列的预测有负面影响，并开发了两种防御策略，实验验证了其有效性。

    

    本文研究了对多元概率预测模型的对抗攻击威胁和有效的防御机制。我们的研究发现一种新的攻击模式，通过对少数其他时间序列的过去观测进行战略性的稀疏（不可察觉的）修改，对目标时间序列的预测产生负面影响。为了减轻这种攻击的影响，我们开发了两种防御策略。首先，我们将先前开发的分类随机平滑技术扩展到多元预测场景中。其次，我们开发了一种对抗训练算法，学习创建对抗性示例，同时优化预测模型以提高其对此类对抗模拟的鲁棒性。在真实世界数据集上进行的广泛实验证实了我们的攻击方案强大，我们的防御算法与基线防御机制相比更加有效。

    This work studies the threats of adversarial attack on multivariate probabilistic forecasting models and viable defense mechanisms. Our studies discover a new attack pattern that negatively impact the forecasting of a target time series via making strategic, sparse (imperceptible) modifications to the past observations of a small number of other time series. To mitigate the impact of such attack, we have developed two defense strategies. First, we extend a previously developed randomized smoothing technique in classification to multivariate forecasting scenarios. Second, we develop an adversarial training algorithm that learns to create adversarial examples and at the same time optimizes the forecasting model to improve its robustness against such adversarial simulation. Extensive experiments on real-world datasets confirm that our attack schemes are powerful and our defense algorithms are more effective compared with baseline defense mechanisms.
    
[^125]: 在图像模型的盲点发现中朝着更加严谨的科学方法

    Towards a More Rigorous Science of Blindspot Discovery in Image Models. (arXiv:2207.04104v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04104](http://arxiv.org/abs/2207.04104)

    本文介绍了一种新的BDM（盲点发现方法）评估框架SpotCheck和一个使用2D图像表示的BDMPlaneSpot。实验结果给出影响BDM性能的因素，证明PlaneSpot与现有的BDM相竞争，在许多情况下表现更好。

    

    越来越多的工作研究了盲点发现方法（“BDM”），这些方法使用图像嵌入来查找数据的语义有意义的子集，在这些子集中图像分类器表现显著更差（即存在盲点）。受到之前工作中观察到的差距的启发，我们介绍了一种新的BDM评估框架（SpotCheck），该框架使用合成图像数据集来训练具有已知盲点的模型，并且使用新的BDM（PlaneSpot）来使用2D图像表示。我们使用SpotCheck进行受控实验，以确定影响BDM性能的因素（例如模型中的盲点数量或定义盲点的特征），并表明PlaneSpot与现有的BDM相竞争，在许多情况下表现更好。重要的是，我们通过设计额外的实验来验证这些发现，使用MS-COCO的真实图像数据集进行实验。我们的发现为未来在BDM设计方面提出了几个有前途的方向。

    A growing body of work studies Blindspot Discovery Methods ("BDM"s): methods that use an image embedding to find semantically meaningful (i.e., united by a human-understandable concept) subsets of the data where an image classifier performs significantly worse. Motivated by observed gaps in prior work, we introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic image datasets to train models with known blindspots and a new BDM, PlaneSpot, that uses a 2D image representation. We use SpotCheck to run controlled experiments that identify factors that influence BDM performance (e.g., the number of blindspots in a model, or features used to define the blindspot) and show that PlaneSpot is competitive with and in many cases outperforms existing BDMs. Importantly, we validate these findings by designing additional experiments that use real image data from MS-COCO, a large image benchmark dataset. Our findings suggest several promising directions for future work on BDM des
    
[^126]: 带理论支持的样本重用的广义策略提升算法

    Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse. (arXiv:2206.13714v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.13714](http://arxiv.org/abs/2206.13714)

    研究提出了一种广义策略提升算法，结合了在线方法的策略提升保证和离线策略算法通过样本重用有效利用数据的效率。

    

    数据驱动的学习控制方法具有改善复杂系统运行的潜力，而基于模型的深度强化学习代表了一种流行的数据驱动控制方法。然而，现有的算法类别在实际控制部署的两个重要要求之间存在权衡：（i）实际性能保证和（ii）数据效率。离线策略算法通过样本重用有效利用数据，但缺乏理论保证，而在线策略算法保证了训练期间的近似策略改进，但受到高样本复杂度的影响。为了平衡这些竞争目标，我们开发了一类广义策略提升算法，它结合了在线方法的策略提升保证和样本重用的效率。通过对来自DeepMind C的多种连续控制任务进行 extensive 的实验分析，我们证明了这种新类算法的益处。

    Data-driven, learning-based control methods offer the potential to improve operations in complex systems, and model-free deep reinforcement learning represents a popular approach to data-driven control. However, existing classes of algorithms present a trade-off between two important deployment requirements for real-world control: (i) practical performance guarantees and (ii) data efficiency. Off-policy algorithms make efficient use of data through sample reuse but lack theoretical guarantees, while on-policy algorithms guarantee approximate policy improvement throughout training but suffer from high sample complexity. In order to balance these competing goals, we develop a class of Generalized Policy Improvement algorithms that combines the policy improvement guarantees of on-policy methods with the efficiency of sample reuse. We demonstrate the benefits of this new class of algorithms through extensive experimental analysis on a variety of continuous control tasks from the DeepMind C
    
[^127]: 压缩通信的随机梯度方法用于分散鞍点问题解决

    Stochastic Gradient Methods with Compressed Communication for Decentralized Saddle Point Problems. (arXiv:2205.14452v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14452](http://arxiv.org/abs/2205.14452)

    这篇论文提出了两种基于压缩的随机梯度算法，用于解决非光滑强凸-强凹鞍点问题的分散求解，并提供了理论保证，包括梯度计算和通信复杂度的限制。

    

    本文提出了两种基于压缩的随机梯度算法来解决一类非光滑强凸-强凹鞍点问题的分散式（无中央服务器）求解。首先，我们提出了一种面向一般随机情形的基于重启的压缩分散式近端随机梯度方法（C-RDPSG）。我们提供了严格的理论保证，其中C-RDPSG的梯度计算复杂度和通信复杂度分别为$\mathcal{O}( (1+\delta)^4 \frac{1}{L^2}{\kappa_f^2}\kappa_g^2 \frac{1}{\epsilon} )$，以实现$\epsilon$精度的鞍点解。接下来，我们提出了一种基于压缩的分散式近端随机方差缩减梯度算法(C-DPSVRG)。

    We develop two compression based stochastic gradient algorithms to solve a class of non-smooth strongly convex-strongly concave saddle-point problems in a decentralized setting (without a central server). Our first algorithm is a Restart-based Decentralized Proximal Stochastic Gradient method with Compression (C-RDPSG) for general stochastic settings. We provide rigorous theoretical guarantees of C-RDPSG with gradient computation complexity and communication complexity of order $\mathcal{O}( (1+\delta)^4 \frac{1}{L^2}{\kappa_f^2}\kappa_g^2 \frac{1}{\epsilon} )$, to achieve an $\epsilon$-accurate saddle-point solution, where $\delta$ denotes the compression factor, $\kappa_f$ and $\kappa_g$ denote respectively the condition numbers of objective function and communication graph, and $L$ denotes the smoothness parameter of the smooth part of the objective function. Next, we present a Decentralized Proximal Stochastic Variance Reduced Gradient algorithm with Compression (C-DPSVRG) for fini
    
[^128]: 正则化对成员推理中维度的祝福

    A Blessing of Dimensionality in Membership Inference through Regularization. (arXiv:2205.14055v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14055](http://arxiv.org/abs/2205.14055)

    研究探讨了过度参数化对成员推理攻击易受攻击性的影响，发现适当的正则化可以在增加模型参数数量的同时提高隐私和性能，消除了隐私与效用之间的权衡问题。

    

    这项研究探讨了过度参数化对分类器在成员推理攻击中的易受攻击性的影响。我们首先展示了模型参数数量如何引发隐私和效用的权衡问题：增加参数数量通常会提高泛化性能，但会降低隐私保障。然而，令人惊讶的是，我们随后证明，如果与适当的正则化相结合，增加模型参数数量实际上可以同时增加其隐私和性能，从而消除隐私与效用之间的权衡问题。在理论上，我们通过对双重特征集合设置的岭回归逻辑回归进行的实验来证明了这一神奇现象。在我们的理论探索之后，我们开发了一种新的leave-one-out分析工具，以精确刻画线性分类器对最佳成员推理攻击的易受攻击性。最后我们还在实验中进行了验证。

    Is overparameterization a privacy liability? In this work, we study the effect that the number of parameters has on a classifier's vulnerability to membership inference attacks. We first demonstrate how the number of parameters of a model can induce a privacy--utility trade-off: increasing the number of parameters generally improves generalization performance at the expense of lower privacy. However, remarkably, we then show that if coupled with proper regularization, increasing the number of parameters of a model can actually simultaneously increase both its privacy and performance, thereby eliminating the privacy--utility trade-off. Theoretically, we demonstrate this curious phenomenon for logistic regression with ridge regularization in a bi-level feature ensemble setting. Pursuant to our theoretical exploration, we develop a novel leave-one-out analysis tool to precisely characterize the vulnerability of a linear classifier to the optimal membership inference attack. We empirically
    
[^129]: 使随机梯度下降法无参数化

    Making SGD Parameter-Free. (arXiv:2205.02160v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2205.02160](http://arxiv.org/abs/2205.02160)

    该论文提出了一种无参数化的随机梯度下降法，能够在一定程度上适应未知梯度范数、平滑性和强凸性，并在收敛速度方面具有高概率保证。

    

    我们开发了一种无参数随机凸优化（SCO）算法，其收敛速度仅比对应的已知参数设置的最优速度多一个双对数因子。相比之下，先前已知的无参数SCO的最佳速度是基于在线无参数后悔界的，与已知参数的对应方法相比包含不可避免的额外对数项。我们的算法具有概念上的简单性，具有高概率保证，并且部分适应未知梯度范数、平滑性和强凸性。我们的成果的核心是SGD步长选择的新型无参数证书，以及假设在SGD迭代上没有先验界限的时间一致集中结果。

    We develop an algorithm for parameter-free stochastic convex optimization (SCO) whose rate of convergence is only a double-logarithmic factor larger than the optimal rate for the corresponding known-parameter setting. In contrast, the best previously known rates for parameter-free SCO are based on online parameter-free regret bounds, which contain unavoidable excess logarithmic terms compared to their known-parameter counterparts. Our algorithm is conceptually simple, has high-probability guarantees, and is also partially adaptive to unknown gradient norms, smoothness, and strong convexity. At the heart of our results is a novel parameter-free certificate for SGD step size choice, and a time-uniform concentration result that assumes no a-priori bounds on SGD iterates.
    
[^130]: 机器学习中的不可重复性来源：一篇综述

    Sources of Irreproducibility in Machine Learning: A Review. (arXiv:2204.07610v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.07610](http://arxiv.org/abs/2204.07610)

    本文提出了一个理解机器学习中不可重复性来源的框架，并指出实验设置问题和未能正确考虑数据变异是机器学习研究中最常见的不可重复性来源。

    

    背景：许多发布的机器学习研究是不可重复的。方法论问题以及未能正确考虑算法本身或其实现引入的变异被认为是不可重复性的主要贡献因素。问题：不存在将实验设计选择与其对结论的潜在影响联系起来的理论框架。缺乏这样的框架，从业者和研究人员评估实验结果和描述实验的限制会更加困难。缺乏这样的框架也使得独立研究人员难以系统地归因于失败的可重复性实验的原因。目标：本文的目的是开发一个框架，使应用数据科学从业者和研究人员能够理解哪些实验设计选择可能导致误导性的发现，并通过此理解如何分析可重复性实验的结论，从而帮助分析结论。方法：我们提出了一个理解机器学习中不可重复性来源的框架，包括算法选择、数据变异、实验设置和实现细节。我们回顾了关于可重复性的最近文献，确定了常见问题，并提供了如何解决这些问题对机器学习研究可重复性的影响的例子。结果：我们发现实验设置问题和未能正确考虑数据变异是机器学习研究中最常见的不可重复性来源。结论：更好地理解这些不可重复性来源可以提高机器学习研究的可重复性，并增加结果的信心。

    Background: Many published machine learning studies are irreproducible. Issues with methodology and not properly accounting for variation introduced by the algorithm themselves or their implementations are attributed as the main contributors to the irreproducibility.Problem: There exist no theoretical framework that relates experiment design choices to potential effects on the conclusions. Without such a framework, it is much harder for practitioners and researchers to evaluate experiment results and describe the limitations of experiments. The lack of such a framework also makes it harder for independent researchers to systematically attribute the causes of failed reproducibility experiments. Objective: The objective of this paper is to develop a framework that enable applied data science practitioners and researchers to understand which experiment design choices can lead to false findings and how and by this help in analyzing the conclusions of reproducibility experiments. Method: We
    
[^131]: 实时神经网络模型预测控制：面向四旋翼和灵活机器人平台的深度学习模型预测控制系统

    Real-time Neural-MPC: Deep Learning Model Predictive Control for Quadrotors and Agile Robotic Platforms. (arXiv:2203.07747v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.07747](http://arxiv.org/abs/2203.07747)

    本论文介绍了一种实时神经网络模型预测控制系统，在模型预测控制流程中高效集成复杂神经网络架构作为动力学模型，实现对四旋翼等灵活机器人平台高性能的控制。

    

    模型预测控制(MPC)已成为高性能自主系统嵌入式控制的流行框架。然而，要使用MPC实现良好的控制性能，精确的动力学模型是关键。为了保持实时操作，嵌入式系统上使用的动力学模型一般限于简单的基于第一原理的模型，这显著限制了它们的表示能力。与这种简单模型相反的是，机器学习方法，特别是神经网络，已经被证明可以准确地建模复杂的动态效应，但它们的巨大计算复杂性阻碍了它们与快速实时迭代循环的结合。本文提出了实时神经MPC框架，用于在模型预测控制流程中高效地集成大型复杂神经网络架构作为动力学模型。我们在模拟和高度灵活的四旋翼平台上进行了实验，证明了所描述系统的能力，展示了其能够实现基于深度神经网络的学习动力学模型在实时控制中高性能的控制能力。

    Model Predictive Control (MPC) has become a popular framework in embedded control for high-performance autonomous systems. However, to achieve good control performance using MPC, an accurate dynamics model is key. To maintain real-time operation, the dynamics models used on embedded systems have been limited to simple first-principle models, which substantially limits their representative power. In contrast to such simple models, machine learning approaches, specifically neural networks, have been shown to accurately model even complex dynamic effects, but their large computational complexity hindered combination with fast real-time iteration loops. With this work, we present Real-time Neural MPC, a framework to efficiently integrate large, complex neural network architectures as dynamics models within a model-predictive control pipeline. Our experiments, performed in simulation and the real world onboard a highly agile quadrotor platform, demonstrate the capabilities of the described 
    
[^132]: 学习中的可重复性

    Reproducibility in Learning. (arXiv:2201.08430v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.08430](http://arxiv.org/abs/2201.08430)

    该论文介绍了可重复性学习算法的概念，这种算法能够抵御样本变异，在保证高准确度的同时，能够以高概率返回相同的输出。同时，该论文证明了可重复性并不与学习效果相悖，设计可重复性算法可以推动我们开发更高效、更稳健的数据分析和建模方法。

    

    我们在学习的背景下介绍了一个可重复性算法的概念。可重复性的学习算法能够抵御样本变异——在相同的基础分布下，运行在两个样本上时能够以高概率返回相同的输出。我们开始解开该定义，澄清随机性如何在平衡准确性和可重复性方面起到了作用。我们启动了一个可重复算法的理论，展示了可重复性如何暗示着具有良好性质，例如数据重用和高效性可测试性，尽管要求重复性非常强。我们展示了对于统计学和学习中的若干基础问题，有一些高效率的可重复性算法。首先，我们证明了任何统计查询算法都可以通过适度增加样本复杂度而变得可重复，并且我们使用这一点构建了寻找近似关键值和中位数的可重复性算法。利用这些想法，我们为寻找对数凹密度估计提供了第一个可重复性算法。

    We introduce the notion of a reproducible algorithm in the context of learning. A reproducible learning algorithm is resilient to variations in its samples -- with high probability, it returns the exact same output when run on two samples from the same underlying distribution. We begin by unpacking the definition, clarifying how randomness is instrumental in balancing accuracy and reproducibility. We initiate a theory of reproducible algorithms, showing how reproducibility implies desirable properties such as data reuse and efficient testability. Despite the exceedingly strong demand of reproducibility, there are efficient reproducible algorithms for several fundamental problems in statistics and learning. First, we show that any statistical query algorithm can be made reproducible with a modest increase in sample complexity, and we use this to construct reproducible algorithms for finding approximate heavy-hitters and medians. Using these ideas, we give the first reproducible algorith
    
[^133]: 稳健训练异常检测模型的简明对数损失函数

    Concise Logarithmic Loss Function for Robust Training of Anomaly Detection Model. (arXiv:2201.05748v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.05748](http://arxiv.org/abs/2201.05748)

    本研究提出了一种新的对数均方误差（LMSE）损失函数，相比于现有的均方误差（MSE）函数，在神经网络训练中更稳定、具有更强的收敛性和更好的异常检测性能。

    

    近年来，由于能够在没有或最少领域知识情况下建立异常检测模型的优势，基于深度学习的算法被广泛采用。然而，为了更稳定地训练人工神经网络，应该定义适当的神经网络结构或损失函数。对于异常检测模型的训练，均方误差（MSE）函数被广泛采用。另一方面，本文提出了一种新的损失函数，对数均方误差（LMSE），以更稳定地训练神经网络。本研究涵盖了数学比较，反向传播的差分域可视化，训练过程中的损失收敛和异常检测性能等各个方面的比较。在总体上，LMSE在损失收敛强度、异常检测性能方面优于现有的MSE函数。LMSE函数预计可适用于训练各种类型的异常检测模型。

    Recently, deep learning-based algorithms are widely adopted due to the advantage of being able to establish anomaly detection models without or with minimal domain knowledge of the task. Instead, to train the artificial neural network more stable, it should be better to define the appropriate neural network structure or the loss function. For the training anomaly detection model, the mean squared error (MSE) function is adopted widely. On the other hand, the novel loss function, logarithmic mean squared error (LMSE), is proposed in this paper to train the neural network more stable. This study covers a variety of comparisons from mathematical comparisons, visualization in the differential domain for backpropagation, loss convergence in the training process, and anomaly detection performance. In an overall view, LMSE is superior to the existing MSE function in terms of strongness of loss convergence, anomaly detection performance. The LMSE function is expected to be applicable for train
    
[^134]: 问题相关的神经网络关注力和努力在图像分辨率和模型选择中的应用

    Problem-dependent attention and effort in neural networks with applications to image resolution and model selection. (arXiv:2201.01415v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2201.01415](http://arxiv.org/abs/2201.01415)

    本文介绍了两种新的基于集成的方法以减少数据和计算成本。第一种方法通过降低数据使用量来减少成本，第二种方法通过使用较简单的模型来降低计算成本。本文考虑的最佳分类器在所有数据集上的分类精度不降低超过5%。

    

    本文介绍了两种新的基于集成的方法，以减少图像分类的数据和计算成本。它们可以与任何分类器集合一起使用，而且不需要额外的训练。在第一种方法中，通过只在模型对低分辨率像素化版本的分类表现出低信心时才分析完整尺寸的图像，从而减少数据使用量。当应用于本文考虑的最佳分类器时，MNIST数据集的数据使用量减少了61.2%，KMNIST减少了69.6%，FashionMNIST减少了56.3%，SVHN减少了84.6%，ImageNet减少了40.6%，ImageNet-V2减少了27.6%，但所有数据集的分类精度均不降低超过5%。然而，对于CIFAR-10，像素化的数据并不特别有信息量，而集成方法会增加数据使用量同时降低精度。在第二种方法中，只有在一个简单模型的分类置信度较低时，才使用一个复杂模型以减少计算成本。在MNIST上的计算成本降低了82.1%，KMNIST降低了47.6%，FashionMNIST降低了72.3%，同时分类精度不降低超过0.5%。

    This paper introduces two new ensemble-based methods to reduce the data and computation costs of image classification. They can be used with any set of classifiers and do not require additional training. In the first approach, data usage is reduced by only analyzing a full-sized image if the model has low confidence in classifying a low-resolution pixelated version. When applied on the best performing classifiers considered here, data usage is reduced by 61.2% on MNIST, 69.6% on KMNIST, 56.3% on FashionMNIST, 84.6% on SVHN, 40.6% on ImageNet, and 27.6% on ImageNet-V2, all with a less than 5% reduction in accuracy. However, for CIFAR-10, the pixelated data are not particularly informative, and the ensemble approach increases data usage while reducing accuracy. In the second approach, compute costs are reduced by only using a complex model if a simpler model has low confidence in its classification. Computation cost is reduced by 82.1% on MNIST, 47.6% on KMNIST, 72.3% on FashionMNIST, 86
    
[^135]: 物理启发式神经算子用于学习偏微分方程

    Physics-Informed Neural Operator for Learning Partial Differential Equations. (arXiv:2111.03794v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.03794](http://arxiv.org/abs/2111.03794)

    本文提出了一种名为PINO的方法，它能够通过同时利用数据和物理约束条件来学习偏微分方程的解算子，克服了纯数据驱动和基于物理的方法的局限性，并且可以在不同分辨率上合并数据和约束条件。

    

    本文提出了一种名为“物理启发式神经算子”（PINO）的方法，它使用现有数据和/或物理约束条件来学习一类参数化的偏微分方程的解算子。该混合方法允许PINO克服纯数据驱动和基于物理的方法所面临的局限性。将数据和PDE约束相结合，PINO能够同时适应数据量有限和/或质量较差的情况以及困难PDE约束的优化。此外，与其他混合学习方法相比，PINO具有一个独特的特性，即能够在不同分辨率上合并数据和PDE约束。这允许我们将来自数值求解器的低分辨率数据与高分辨率PDE约束相结合，而所得到的PINO即使在高分辨率的测试实例上也没有精度下降。

    In this paper, we propose physics-informed neural operators (PINO) that uses available data and/or physics constraints to learn the solution operator of a family of parametric Partial Differential Equation (PDE). This hybrid approach allows PINO to overcome the limitations of purely data-driven and physics-based methods. For instance, data-driven methods fail to learn when data is of limited quantity and/or quality, and physics-based approaches fail to optimize on challenging PDE constraints. By combining both data and PDE constraints, PINO overcomes all these challenges. Additionally, a unique property that PINO enjoys over other hybrid learning methods is its ability to incorporate data and PDE constraints at different resolutions. This allows us to combine coarse-resolution data, which is inexpensive to obtain from numerical solvers, with higher resolution PDE constraints, and the resulting PINO has no degradation in accuracy even on high-resolution test instances. This discretizati
    
[^136]: 以表示概率为基础的聚合二值激活神经网络的PAC-Bayesian学习

    PAC-Bayesian Learning of Aggregated Binary Activated Neural Networks with Probabilities over Representations. (arXiv:2110.15137v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.15137](http://arxiv.org/abs/2110.15137)

    该研究使用PAC-Bayesian框架为具有正态分布权重的二值激活神经网络的聚合提供了紧凑的泛化界限和学习过程，这导致了一种奇特的界限最小化学习算法。

    

    考虑到对参数的概率分布是一种有效的策略来学习带有不可微激活函数的神经网络。我们研究了期望为自身的概率神经网络作为预测器，重点关注具有正态分布权重的二值激活神经网络的聚合。我们利用了最近从PAC-Bayesian框架中派生出的分析，为这种聚合的预期输出值提供了紧凑的泛化界限和学习过程，这由解析表达式给出。虽然后者的组合性质在先前的工作中被近似替代，但我们表明对于深而窄的神经网络，确切的计算仍然是可行的，这得益于一种动态编程方法。这导致我们提出了一种奇特的用于二元激活神经网络的界限最小化学习算法，其中前向传递传播表示概率的表示。

    Considering a probability distribution over parameters is known as an efficient strategy to learn a neural network with non-differentiable activation functions. We study the expectation of a probabilistic neural network as a predictor by itself, focusing on the aggregation of binary activated neural networks with normal distributions over real-valued weights. Our work leverages a recent analysis derived from the PAC-Bayesian framework that derives tight generalization bounds and learning procedures for the expected output value of such an aggregation, which is given by an analytical expression. While the combinatorial nature of the latter has been circumvented by approximations in previous works, we show that the exact computation remains tractable for deep but narrow neural networks, thanks to a dynamic programming approach. This leads us to a peculiar bound minimization learning algorithm for binary activated neural networks, where the forward pass propagates probabilities over repre
    
[^137]: 别敲门！Rowhammer攻击进入DNN模型的后门

    Don't Knock! Rowhammer at the Backdoor of DNN Models. (arXiv:2110.07683v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.07683](http://arxiv.org/abs/2110.07683)

    本研究首次提出了一种基于Rowhammer方法的端到端后门注入攻击方法，可在实际硬件上攻击分类器模型。通过新颖的优化角度解决了硬件实现中的实际问题。

    

    现代深度神经网络（DNN）已被证明容易受到对抗性操纵和后门攻击的影响。后门模型在预先定义的触发器输入的情况下会偏离预期行为，而在干净数据上保持性能。最近的研究集中于在推断阶段通过修改网络权重对后门注入进行软件模拟，然而由于硬件限制，这种方式在实际应用中往往不太现实。与此相反，在本研究中，我们首次提出了一种基于Rowhammer作为故障注入方法的实际硬件上的端到端后门注入攻击方法，用于对分类器模型进行攻击。为此，我们首先研究了在硬件上实际部署DNNs中后门注入攻击的可行性，并从新颖的优化角度解决了这些实际问题。我们的动机在于易受攻击的内存位置非常罕见、特定于设备并且稀疏分布。

    State-of-the-art deep neural networks (DNNs) have been proven to be vulnerable to adversarial manipulation and backdoor attacks. Backdoored models deviate from expected behavior on inputs with predefined triggers while retaining performance on clean data. Recent works focus on software simulation of backdoor injection during the inference phase by modifying network weights, which we find often unrealistic in practice due to restrictions in hardware.  In contrast, in this work for the first time, we present an end-to-end backdoor injection attack realized on actual hardware on a classifier model using Rowhammer as the fault injection method. To this end, we first investigate the viability of backdoor injection attacks in real-life deployments of DNNs on hardware and address such practical issues in hardware implementation from a novel optimization perspective. We are motivated by the fact that vulnerable memory locations are very rare, device-specific, and sparsely distributed. Conseque
    
[^138]: 基于图的机器学习改进了 JIT 缺陷预测

    Graph-Based Machine Learning Improves Just-in-Time Defect Prediction. (arXiv:2110.05371v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2110.05371](http://arxiv.org/abs/2110.05371)

    该论文使用基于图的机器学习技术，构建了一个由开发人员和源文件组成的贡献图，利用这个图提取的特征，改进了 JIT 缺陷预测，比传统的机器学习方法更好地预测了易出现缺陷的更改。

    

    当今软件不断增加的复杂性需要数千名开发人员的贡献。由于复杂的合作结构，开发人员更可能引入易出现缺陷的更改，导致软件故障。确定这些易出现缺陷的更改被引入的时间已经变得具有挑战性，而使用传统的机器学习方法进行这些决策似乎已经达到了瓶颈。在本研究中，我们构建了由开发人员和源文件组成的贡献图来捕捉构建软件所需的微妙复杂性。通过利用这些贡献图，我们的研究展示了利用基于图的机器学习改进 JIT 缺陷预测的潜力。我们假设从贡献图中提取的特征可能比从软件特征派生的本质特征更好地预测易出现缺陷的更改。我们使用基于图的机器学习来分类表示开发人员之间交互的边，以证实我们的假设。

    The increasing complexity of today's software requires the contribution of thousands of developers. This complex collaboration structure makes developers more likely to introduce defect-prone changes that lead to software faults. Determining when these defect-prone changes are introduced has proven challenging, and using traditional machine learning (ML) methods to make these determinations seems to have reached a plateau. In this work, we build contribution graphs consisting of developers and source files to capture the nuanced complexity of changes required to build software. By leveraging these contribution graphs, our research shows the potential of using graph-based ML to improve Just-In-Time (JIT) defect prediction. We hypothesize that features extracted from the contribution graphs may be better predictors of defect-prone changes than intrinsic features derived from software characteristics. We corroborate our hypothesis using graph-based ML for classifying edges that represent 
    
[^139]: 变分扩散模型

    Variational Diffusion Models. (arXiv:2107.00630v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.00630](http://arxiv.org/abs/2107.00630)

    该论文提出了一族基于扩散的生成模型，通过对噪声时间表的有效优化，这些模型在图像密度估计基准测试中获得了最先进的可能性和视觉质量。

    

    基于扩散的生成模型已经展示了令人印象深刻的合成能力，但它们也能成为很好的基于可能性的模型吗？我们回答了这个问题，并引入了一族基于扩散的生成模型，在标准图像密度估计基准上获得了最先进的可能性。与其他基于扩散的模型不同，我们的方法允许与模型的其余部分共同有效地优化噪声时间表。我们表明，在扩散数据的信噪比方面，变分下限（VLB）简化为一个非常简短的表达式，从而改善了我们对该模型类的理论理解。利用这一见解，我们证明了文献中提出的几个模型之间的等价性。此外，我们表明，连续时间VLB对于噪声时间表是不变的，除了其端点处的信噪比。这使我们能够学习一种噪声时间表，该表针对信噪比最小化变分下限。我们的实验证明，我们的方法在包括CIFAR-10， CelebA-HQ和FFHQ在内的几个基准测试上实现了良好的性能，无论是可能性还是视觉质量。

    Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the vari
    
[^140]: 训练好的 ConvNets 的超平面安排存在偏差

    Hyperplane Arrangements of Trained ConvNets Are Biased. (arXiv:2003.07797v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2003.07797](http://arxiv.org/abs/2003.07797)

    本文通过研究训练好的 ConvNets 的超平面配置，发现其存在统计性偏差，而且这种偏差与验证性能息息相关。

    

    本文通过对卷积层预激活空间中的函数进行几何属性研究，探究了经过训练的 ConvNets 所识别的超平面配置。我们介绍了一个统计学方法来研究局部配置，并将其与训练过程中的动态关联起来。研究表明，训练好的 ConvNets 面向规则超平面配置出现了显著的统计偏差。此外，我们发现对于在 CIFAR10、CIFAR100 和 ImageNet 上进行训练的结构，偏置层的配置对于验证性能至关重要。

    We investigate the geometric properties of the functions learned by trained ConvNets in the preactivation space of their convolutional layers, by performing an empirical study of hyperplane arrangements induced by a convolutional layer. We introduce statistics over the weights of a trained network to study local arrangements and relate them to the training dynamics. We observe that trained ConvNets show a significant statistical bias towards regular hyperplane configurations. Furthermore, we find that layers showing biased configurations are critical to validation performance for the architectures considered, trained on CIFAR10, CIFAR100 and ImageNet.
    

