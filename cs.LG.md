# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations.](http://arxiv.org/abs/2306.00481) | 该论文介绍了一种专门针对声学领域不匹配情况下的自动数据增强方法来解决语音自监督学习的表现问题，并在低资源领域中比基线展现出更好的性能。 |
| [^2] | [Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning.](http://arxiv.org/abs/2306.00477) | 本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。 |
| [^3] | [MindBigData 2023 MNIST-8B The 8 billion datapoints Multimodal Dataset of Brain Signals.](http://arxiv.org/abs/2306.00455) | MindBigData 2023 MNIST-8B是史上最大的开放脑信号数据集，采用自定义设备复制了MNIST数据集中的全部70,000个数字，捕捉了受试者观看像素并听取数字标签时的大脑信号。 |
| [^4] | [Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?.](http://arxiv.org/abs/2306.00452) | 研究发现语音自监督表示的基准测试结果对解码器架构变化很敏感，而使用有限解码器进行基准测试可能会导致开发的模型过大。 |
| [^5] | [Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift.](http://arxiv.org/abs/2306.00427) | 连续学习中存在一种特殊形式的灾难性遗忘——越界遗忘，当给定类别引入类内分布转移时，它会显着削弱该类别的连续学习方法的识别准确率。 |
| [^6] | [Faster Robust Tensor Power Method for Arbitrary Order.](http://arxiv.org/abs/2306.00406) | 本文提出了一种新的张量幂方法，用于分解任意阶张量，与现有方法相比，它可以更高效地处理高阶张量。 |
| [^7] | [Coneheads: Hierarchy Aware Attention.](http://arxiv.org/abs/2306.00392) | “锥注意力”是点积注意力的替代方案，它通过基于双曲锥的层次结构联系数据点，明确建模了真实世界数据集的复杂结构属性，提高了任务级性能并实现了优于点积注意力的效果，而且参数量更少。 |
| [^8] | [Learning Gaussian Mixture Representations for Tensor Time Series Forecasting.](http://arxiv.org/abs/2306.00390) | 本文提出了一种新的张量时间序列预测框架GMRL，它可以单独模拟时间、位置和源变量中所暗示的每个异构性组件，相比于最先进的基准方法，本文的方法显示出了优越性。 |
| [^9] | [Calibrated Propensity Scores for Causal Effect Estimation.](http://arxiv.org/abs/2306.00382) | 本研究提出基于校准倾向分数技术的因果效应估计方法，可通过简单的重新校准技术实现倾向分数模型的概率输出的校准，具有较为广泛的应用前景。 |
| [^10] | [Better Context Makes Better Code Language Models: A Case Study on Function Call Argument Completion.](http://arxiv.org/abs/2306.00381) | 本文针对代码补全任务展开研究，提出了考虑局部和非局部上下文信息的方法，设计了一个新的Python包数据集，通过程序分析器提取了非局部信息，并证明了这种方法可以提高代码补全模型的性能。 |
| [^11] | [Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes.](http://arxiv.org/abs/2306.00379) | 本文提出了一个多模态生成式模型来解决电商商品属性识别中的标注问题，通过在文本与视觉特征中生成属性值，系统在三个数据集上优于现有最先进方法，提升了7.3％。 |
| [^12] | [Graph Switching Dynamical Systems.](http://arxiv.org/abs/2306.00370) | 本论文提出一种新颖的基于图形的切换动力学系统方法，可以在涉及交互对象设置中自动发现模式并从时序数据中学习切换行为，这一方法在实验中表现出优越性能。 |
| [^13] | [On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diffusion Models, and Fokker-Planck Regularization.](http://arxiv.org/abs/2306.00367) | 本研究建立了一致性模型，一致性扩散模型和Fokker-Planck正则化之间的理论联系，为不同目标增强扩散模型提供了更全面和包容性的框架的可能性。 |
| [^14] | [Sharded Bayesian Additive Regression Trees.](http://arxiv.org/abs/2306.00361) | 本文提出了一种基于分片的贝叶斯加性回归树模型，通过引入随机化辅助变量和分片树结构，采用贝叶斯加性回归树适配每个分区组件到一个子模型中进行数据分区，引入交集树结构来完全使用树结构指定分片和建模。研究中还推导了理论最优权重和证明了模型复杂度。 |
| [^15] | [How Do ConvNets Understand Image Intensity?.](http://arxiv.org/abs/2306.00360) | 该论文研究了ConvNets需要依赖图像亮度信息的情况，并通过可视化进行了展示。 |
| [^16] | [Efficient and Robust Bayesian Selection of Hyperparameters in Dimension Reduction for Visualization.](http://arxiv.org/abs/2306.00357) | 本文提出了一种利用有代理模型的贝叶斯优化进行数据驱动敏感性分析和权衡多目标的维数约减参数选择方法，并在多个合成和现实数据集上进行评估，提供了一种稳健且高效的解决方案。 |
| [^17] | [Addressing Negative Transfer in Diffusion Models.](http://arxiv.org/abs/2306.00354) | 本文从多任务学习角度出发，研究了扩散训练中的负迁移现象，提出了利用正则化技术增强扩散训练的方法，以减轻负迁移并提高去噪任务的性能。 |
| [^18] | [Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective.](http://arxiv.org/abs/2306.00353) | 本研究提出了一个基于概率视角的对抗样本构建方法，可以生成语义感知的对抗性样本，并可以有效规避传统对抗性攻击的强化对抗训练方法。 |
| [^19] | [Improving Energy Conserving Descent for Machine Learning: Theory and Practice.](http://arxiv.org/abs/2306.00352) | 这篇论文介绍了能量守恒下降（ECD）的理论，提出了基于梯度的优化算法ECDSep，能够处理凸优化问题和非凸优化问题，通过改进动态和混沌诱导元素，提高性能；在各种机器学习问题上与流行的优化方法进行了实证比较，发现在每个任务中具有竞争力或改进的性能。 |
| [^20] | [CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception.](http://arxiv.org/abs/2306.00349) | 论文提出了一种新的多模式BEV感知自我监督表示学习框架CALICO，它将对比目标应用于LiDAR和相机骨干，证明其比现有模型更加高效有效，并在BEV感知任务中提供了显着的收益。 |
| [^21] | [CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for Mitigating Class Imbalance in Causal Claim Identification.](http://arxiv.org/abs/2306.00346) | 该论文提出了一种针对类别不平衡问题的新颖反事实数据增广方法，该方法通过动词替换进行，用于医学因果关系提取，并且与其他3种数据增广技术相比较，在少数类方面有显著提高。 |
| [^22] | [BOtied: Multi-objective Bayesian optimization with tied multivariate ranks.](http://arxiv.org/abs/2306.00344) | BOtied 是一种带有相关多元等级的多目标贝叶斯优化算法，相较于现有的方法具有更高的样本效率和较好的近似质量。 |
| [^23] | [Combining Explicit and Implicit Regularization for Efficient Learning in Deep Networks.](http://arxiv.org/abs/2306.00342) | 本文提出了一种显式正则化方法，与隐式正则化结合，可以使单层网络实现与深度线性网络相当的低秩近似和泛化误差，使深度不再是学习的必要条件。 |
| [^24] | [Last Switch Dependent Bandits with Monotone Payoff Functions.](http://arxiv.org/abs/2306.00338) | 本文研究了最后一次切换依赖赌博机的规划问题，设计了第一个有效的常数近似算法，并在自然单调递增的情况下实现了近乎最优的性能保证。 |
| [^25] | [Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning.](http://arxiv.org/abs/2306.00324) | 本文利用强化学习方法在多智能体马尔可夫决策过程中实现公平性，引入公平函数来确保代理之间的奖励公平，并提出一种基于在线凸优化的方法获得策略。 |
| [^26] | [Thought Cloning: Learning to Think while Acting by Imitating Human Thinking.](http://arxiv.org/abs/2306.00323) | 本论文提出了一种新的模仿学习框架“思维克隆”，通过学习人类的思维来训练AI代理，以在泛化、探索、规划等能力方面实现更好的表现。 |
| [^27] | [Improving Offline RL by Blending Heuristics.](http://arxiv.org/abs/2306.00321) | HUBL是一种用于基于值函数回溯的广泛类离线强化学习算法的简单性能提升技术。我们证明了HUBL可通过调整奖励和折扣因子来简单实现，并且实验结果表明HUBL能够在提高性能的同时降低复杂度。 |
| [^28] | [FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization.](http://arxiv.org/abs/2306.00317) | 该论文提出了一种新的基于元素除法的可学习舍入机制FlexRound，使得后训练量化时更好地重构每个层或块的输出，并且能够学习公共量化网格大小以及每个预训练权重的不同比例尺。 |
| [^29] | [(Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy.](http://arxiv.org/abs/2306.00312) | 本论文提出了一种基于不一致性偏差的方法，通过一个简单、直观的条件推导出深度神经网络在分布转移下的(几乎)可证明误差界限，相比于$\mathcal{H}\Delta\mathcal{H}$-divergence更紧密、更易评价。 |
| [^30] | [CapText: Large Language Model-based Caption Generation From Image Context and Description.](http://arxiv.org/abs/2306.00301) | 研究提出了一种基于大型语言模型的图像字幕生成方法，从文本描述和上下文中生成字幕，而不直接处理图像。在CIDEr指标上，优于当前最先进的图像文本对齐模型。 |
| [^31] | [Transformers learn to implement preconditioned gradient descent for in-context learning.](http://arxiv.org/abs/2306.00297) | 本论文通过研究线性transformers在训练过程中的全局最小值，证明了对于一个注意力层，transformers能够实现一次预处理梯度下降，而对于一个$k$个注意力层的transformer，它可以实现多次预处理梯度下降。 |
| [^32] | [EMOTE: An Explainable architecture for Modelling the Other Through Empathy.](http://arxiv.org/abs/2306.00295) | EMOTE是一个可解释的建模架构，用于模拟另一个智能体的动作价值函数，通过共情的想象网络将其他智能体的观察状态转换成可解释的“共情状态”。该方法在多智能体游戏中取得了比现有方法更好的性能。 |
| [^33] | [Training-free Neural Architecture Search for RNNs and Transformers.](http://arxiv.org/abs/2306.00288) | 本文研究了适用于语言模型任务的循环神经网络和Transformer的无训练神经网络结构搜索算法，并开发了一个新的性能预测指标hidden covariance，可显著优化现有指标。 |
| [^34] | [Case Study-Based Approach of Quantum Machine Learning in Cybersecurity: Quantum Support Vector Machine for Malware Classification and Protection.](http://arxiv.org/abs/2306.00284) | 本文介绍了基于案例研究的方法，将量子机器学习应用于网络安全领域，通过10个学习模块覆盖各种网络安全主题。该研究旨在提供实践机会以解决实际安全问题，激发和鼓励学生学习。 |
| [^35] | [Transfer Learning for Underrepresented Music Generation.](http://arxiv.org/abs/2306.00281) | 本文介绍了一种组合创造性方法进行迁移学习，可以使深度神经网络在分布外流派上的性能得到提高，以伊朗民间音乐为例进行了实验，结果表明未来可能通过这种方法实现代表性不足音乐流派的生成。 |
| [^36] | [Towards Bias Correction of FedAvg over Nonuniform and Time-Varying Communications.](http://arxiv.org/abs/2306.00280) | 本文面向非均匀和时变通信故障问题，提出了一种名为FedPBC的联邦学习算法来修正FedAvg的偏差，该算法构建二叉树来减轻非均匀通信故障的影响，并在非凸任务上实验结果表现优于FedAvg。 |
| [^37] | [Provable Benefit of Mixup for Finding Optimal Decision Boundaries.](http://arxiv.org/abs/2306.00267) | 本研究证明了使用Mixup训练具有可证实的益处，可以显著降低在更可分离数据分布中寻找最佳决策边界的样本复杂度。 |
| [^38] | [Doubly Robust Self-Training.](http://arxiv.org/abs/2306.00265) | 本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。 |
| [^39] | [Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior.](http://arxiv.org/abs/2306.00258) | 本研究研究了预训练模型在科学机器学习中的迁移性能，发现适当微调迁移学习可以代替从头开始训练，达到更高的准确性水平，而且减少了许多数据集的使用。 |
| [^40] | [DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm.](http://arxiv.org/abs/2306.00256) | DSGD-CECA 提出了一种通信优化的精确共识算法，使分散式 SGD 能够在没有2的幂次方限制的情况下实现收敛，在大规模时更为实用。 |
| [^41] | [From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces.](http://arxiv.org/abs/2306.00245) | 本文提出了一种基于像素级别的预训练方法，建立了一种模拟人类概念界面和混合动作空间的代理，实现了在GUI指令遵循任务的MiniWob++基准测试中超越人类工作者的目标。 |
| [^42] | [Combinatorial Neural Bandits.](http://arxiv.org/abs/2306.00242) | 该论文提出了两个组合神经臂算法，通过使用深度神经网络来近似未知得分函数，这是处理类似问题的第一个算法框架。 |
| [^43] | [Predictive Limitations of Physics-Informed Neural Networks in Vortex Shedding.](http://arxiv.org/abs/2306.00230) | 本文研究发现，物理信息神经网络（PINN）在涡 shedding 上预测的局限性较大，数据驱动型的PINN只有在训练数据可用时才能预测涡 shedding，且可能是由于谱偏差的原因，PINN会恢复到稳态解。 |
| [^44] | [Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images.](http://arxiv.org/abs/2306.00219) | 本论文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地根据目标区域修改AI合成图像并保留原始上下文。与其他最先进的图像修复技术进行比较，该方法在用户研究中表现出更好的可用性和有效性。 |
| [^45] | [Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2306.00212) | 本文研究了在有约束的马尔可夫博弈中，多个智能体通过最大化总体奖励并在对总体效用的期望值上设置约束条件进行多智能体强化学习。通过泛化Lagrangian策略优化，采用基于占用测量的方法，并利用置信上限强化学习算法，实现对约束的处理。 |
| [^46] | [PERFOGRAPH: A Numerical Aware Program Graph Representation for Performance Optimization and Program Analysis.](http://arxiv.org/abs/2306.00210) | 论文提出了一种新颖的基于图形的程序表示，称为PERFOGRAPH，它可以通过捕获数值信息和复合数据结构来克服现有程序表示的限制和挑战，是一种高度灵活和可扩展的表示形式。 |
| [^47] | [Toward Understanding Why Adam Converges Faster Than SGD for Transformers.](http://arxiv.org/abs/2306.00204) | 本文研究了Adam在Transformer的训练中为什么比SGD更快收敛，提出了方向锐度的概念并通过比较证明了SGD相对于自适应算法具有更差的方向锐度，进而在理论上证明只需要裁剪一小部分坐标即可提高SGD的表现。 |
| [^48] | [Building Manufacturing Deep Learning Models with Minimal and Imbalanced Training Data Using Domain Adaptation and Data Augmentation.](http://arxiv.org/abs/2306.00202) | 本文提出了一种新颖的域自适应方法和数据增强方法相结合的深度学习模型构建方法，适用于标记训练数据稀缺或不平衡的情况。在晶圆缺陷预测任务中，该方法表现出优异的性能。 |
| [^49] | [Generalized Implicit Follow-The-Regularized-Leader.](http://arxiv.org/abs/2306.00201) | 提出了一种新的在线学习算法——广义隐式Follow-The-Regularized-Leader (FTRL)，它可以恢复已知的算法，如线性损失的FTRL和隐式FTRL，并允许设计新的更新规则。该算法的关键思想是用Fenchel-Young不等式代替损失的线性化，可以直接改进最坏情况下的后悔上限。 |
| [^50] | [An Invariant Learning Characterization of Controlled Text Generation.](http://arxiv.org/abs/2306.00198) | 本文提出了一种控制文本生成的不变学习方法，解决了生成的文本分布转移的问题。 |
| [^51] | [SSL-CPCD: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis.](http://arxiv.org/abs/2306.00197) | 本论文提出了一种名为SSL-CPCD的新方法，通过针对补丁级别的实例组区分和跨类别变化的惩罚，在使用自我监督学习提高内窥镜图像分析泛化性能方面取得了重要的进展。 |
| [^52] | [Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption.](http://arxiv.org/abs/2306.00196) | 本文提出了一个通用的框架，将任何单臂策略转化为原始的$N$臂问题的策略，解决了依赖于复杂UGAP假设的问题，并实现了具有$O(1/\sqrt{N})$最优性差距的策略。 |
| [^53] | [Multi-environment lifelong deep reinforcement learning for medical imaging.](http://arxiv.org/abs/2306.00188) | 本文提出了一种生命周期DRL框架SERIL，在医学影像领域中实现了在不断变化的成像环境中持续学习的目的，展现出了出色的性能。 |
| [^54] | [Diffused Redundancy in Pre-trained Representations.](http://arxiv.org/abs/2306.00183) | 本文发现预训练表示中存在一定程度的扩散冗余，即随机选择一部分神经元与全部神经元相似度高且性能相似，可用于降低实际部署中的计算和内存成本，同时保持相当的性能水平。 |
| [^55] | [FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow.](http://arxiv.org/abs/2306.00180) | 本文提出了一种利用可微分渲染将帧间光流映射到三维场景流以同步重建相机位姿和三维神经场表示的方法。这种方法不需要精确相机位姿，可在真实世界的视频数据上进行全自监督训练。 |
| [^56] | [Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees.](http://arxiv.org/abs/2306.00172) | 本论文提出了一个新的基于RL的边缘加权在线二分图匹配方法（LOMAR），实现了好的平均情况和最坏情况的性能，其关键新颖之处在于决策是基于一个新的在线切换操作，该操作可以对抗未来的不确定性。 |
| [^57] | [Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training.](http://arxiv.org/abs/2306.00169) | 本文的理论分析与实证研究表明，深度神经网络训练中模型输出的不一致性和不稳定性可以作为估计泛化间隙的重要指标，消除不一致性的算法能够提高模型性能。 |
| [^58] | [Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model.](http://arxiv.org/abs/2306.00160) | 本文提出名称为AVLIT的轻量级迭代模型，采用渐进式学习在嘈杂环境中实现音视频语音的分离。实验证明该模型在两种情境下的音频分离和音视频基线方法相比具有优越性，同时其减小的模型大小使得其适用于低资源应用场景。 |
| [^59] | [Information Fusion via Symbolic Regression: A Tutorial in the Context of Human Health.](http://arxiv.org/abs/2306.00153) | 本教程论文介绍了符号回归技术，以人类健康和营养领域的实际应用为例，将各种人体测量指标融合成一个简单的数学表达式以估算体脂百分比。 |
| [^60] | [Learning the Right Layers: a Data-Driven Layer-Aggregation Strategy for Semi-Supervised Learning on Multilayer Graphs.](http://arxiv.org/abs/2306.00152) | 本文提出了一种参数-free的拉普拉斯正则化模型，能够无需预先评估层的贡献，通过学习一个最佳的非线性层级组合来实现基于多层图的半监督学习。 |
| [^61] | [SafeDiffuser: Safe Planning with Diffusion Probabilistic Models.](http://arxiv.org/abs/2306.00148) | SafeDiffuser提出了一种新方法解决扩散概率模型在安全性上的挑战。通过使用一类控制障碍函数，将有限时间扩散不变性嵌入到去噪扩散过程中，实现了可靠的扩散数据生成，并在多个安全规划任务上展示了鲁棒性和保证。 |
| [^62] | [On the Expressive Power of Neural Networks.](http://arxiv.org/abs/2306.00145) | 本文研究了神经网络的表达能力，证明了某些问题在之前的研究中未得到解决，并提出了新问题的回答，包括广而浅的ReLU网络不能被深而窄的ReLU网络很好地逼近等。 |
| [^63] | [Mechanic: A Learning Rate Tuner.](http://arxiv.org/abs/2306.00144) | 机械师是一种学习率调节器，能自动调整任何基本优化算法和调度的学习率比例因子，可以实现在大规模深度学习任务中接近、匹配或甚至优于手动调整学习率的效果。 |
| [^64] | [A Note On Interpreting Canary Exposure.](http://arxiv.org/abs/2306.00133) | 本文提供了关于如何解释金丝雀曝光的直觉，包括其与成员推理攻击和差分隐私的关系。 |
| [^65] | [Surrogate Model Extension (SME): A Fast and Accurate Weight Update Attack on Federated Learning.](http://arxiv.org/abs/2306.00127) | 本文提出了一种称为 Surrogate Model Extension (SME) 的方法，使得在联邦学习中，攻击者可以使用比之前更加高效和准确的方法对权重更新进行隐私攻击，进一步暴露了联邦学习中的隐私保护弱点。 |
| [^66] | [Optimal Sets and Solution Paths of ReLU Networks.](http://arxiv.org/abs/2306.00119) | 本研究开发了一个分析框架，通过凸规划表征所有ReLU网络的最优集合和解路径，并提供了最小网络的最优剪枝算法，建立了ReLU网络正则化路径连续的条件，并为最小ReLU网络提供了灵敏度结果。 |
| [^67] | [The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture.](http://arxiv.org/abs/2306.00114) | 该论文提出了一个时间补丁数据集，包含了加拿大农田的多时相遥感影像。该数据集是手动经过确认和筛选的高分辨率地理参考图像，覆盖四个农作物生产年度和五个月份。这个数据集可以用于提高土地覆盖分类的准确性。 |
| [^68] | [MuseCoco: Generating Symbolic Music from Text.](http://arxiv.org/abs/2306.00110) | MuseCoco是一种从文本描述中生成符号音乐的系统，具备高效和灵活等特点，为音乐家提供了更好的音乐生成方式。 |
| [^69] | [Majority Rule: better patching via Self-Consistency.](http://arxiv.org/abs/2306.00108) | 本文提出了一种无需解释的算法，基于自洽性和大型语言模型（LLMs）来进行软件补丁选择，从而解决当前缺乏解释的软件数据集的问题。 |
| [^70] | [MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training.](http://arxiv.org/abs/2306.00107) | 提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。 |
| [^71] | [ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning.](http://arxiv.org/abs/2306.00103) | 提出了ManagerTower，一种新型的VL模型体系结构，可以集合并组合不同级别的预先训练的单模态专家的见解，并可以自适应地聚合单模态语义知识以促进更全面的跨模态对齐和融合。仅使用4M VLP数据，ManagerTower在各种下游VL任务中都取得了更好的性能，尤其是在VQAv2测试标准下达到了79.15%的准确率，在Flickr30K上的IR@1为86.56% TR@1为95.64%。 |
| [^72] | [Pareto Front Identification with Regret Minimization.](http://arxiv.org/abs/2306.00096) | 本文提出了适用于线性Bandit的Pareto前沿识别算法，该算法使用所有动作的上下文信息，具有较低的样本复杂度，并同时保证了Pareto前沿识别和Pareto遗憾最小化。 |
| [^73] | [A General Framework for Equivariant Neural Networks on Reductive Lie Groups.](http://arxiv.org/abs/2306.00091) | 本文提出了一个通用的等变神经网络架构，能够尊重任何一个约化李群G的有限维表示的对称性，并通过在多个领域的任务上进行实验来证明其性能。 |
| [^74] | [Auto-Differentiation of Relational Computations for Very Large Scale Machine Learning.](http://arxiv.org/abs/2306.00088) | 本文提出了一种面向关系计算的自动微分方法，实验表明该方法可达到非常大的数据集规模，并在大规模分布式机器学习中表现出与专用系统相媲美的竞争力。 |
| [^75] | [Adaptive Coordination in Social Embodied Rearrangement.](http://arxiv.org/abs/2306.00087) | 本文介绍了一项名为“社交重组”的任务，使用适应性的协调方法，在没有特权信息的情况下，两个机器人使用本地感知和自我参照观察来完成一个长期的任务。作者提出了一种行为多样性游戏的方法，可以帮助机器人在协调中生成更多样的行为，具有更强的适应性。 |
| [^76] | [Human-Aligned Calibration for AI-Assisted Decision Making.](http://arxiv.org/abs/2306.00074) | 本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。 |
| [^77] | [Shadows of quantum machine learning.](http://arxiv.org/abs/2306.00061) | 量子机器学习模型需要使用量子计算机进行评估，但我们提出在训练完后，使用量子计算机生成一个经典阴影模型来计算函数的经典计算近似，避免了对量子计算机的需求。 |
| [^78] | [Lottery Tickets in Evolutionary Optimization: On Sparse Backpropagation-Free Trainability.](http://arxiv.org/abs/2306.00045) | 本文证明了进化策略算法具有高度稀疏可训练的初始参数，并比较了与梯度下降算法稀疏训练的差异。进化策略算法可以探索各种不同的、平坦的局部最优解，并且不会保留线性模式的连接性。 |
| [^79] | [How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning.](http://arxiv.org/abs/2306.00044) | 本研究提出了一种通用方法来识别深度学习为基础的反欺骗对策中的快捷方式，并在实验中证明了快捷方式的存在，分析了如何影响类条件得分统计信息。 |
| [^80] | [Graph-based methods coupled with specific distributional distances for adversarial attack detection.](http://arxiv.org/abs/2306.00042) | 本论文提出了利用图形的方法结合特定分布距离来检测对抗性攻击，通过研究神经网络的图结构，介绍了用于预测和解释对抗性攻击的特定测量方法，这有助于研究对抗性攻击的内在机理。 |
| [^81] | [Research And Implementation Of Drug Target Interaction Confidence Measurement Method Based On Causal Intervention.](http://arxiv.org/abs/2306.00041) | 本研究提出了一种基于因果干预的置信度测量方法来提高药物靶点相互作用预测模型的准确性，着重解决了知识图谱嵌入模型的不足问题。 |
| [^82] | [Assessing the Generalizability of a Performance Predictive Model.](http://arxiv.org/abs/2306.00040) | 本研究提出一种工作流程来评估性能预测模型的泛化能力，可以在不同的基准测试套件上进行验证。 |
| [^83] | [FedCSD: A Federated Learning Based Approach for Code-Smell Detection.](http://arxiv.org/abs/2306.00038) | 本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。该方法在三个实验中分别使用不同的数据集，实现了高精度的检测效果，并且比集中式和交叉验证方法具有更好的性能表现。 |
| [^84] | [ROSARL: Reward-Only Safe Reinforcement Learning.](http://arxiv.org/abs/2306.00035) | ROSARL提出了一种基于奖励的安全强化学习方法，通过定义“Minmax惩罚”确定智能体在达到不安全状态时所允许的奖励上限，并考虑环境的可控性和直径来获得这个上限。 |
| [^85] | [Self-Verification Improves Few-Shot Clinical Information Extraction.](http://arxiv.org/abs/2306.00024) | 本文探索了一种利用自我验证的通用缓解框架，该框架利用语言模型为其自我提取提供权威性并检查其自己的输出。在临床信息提取任务中，该方法能够显著提高各种LLMs的准确性，并展示了极好的可解释性和认知增强效果。 |
| [^86] | [Predicting Heart Disease and Reducing Survey Time Using Machine Learning Algorithms.](http://arxiv.org/abs/2306.00023) | 本研究使用机器学习算法对美国CDC的心脏病调查进行了准确性调查，确定了最相关的问题子集，并证明使用有限的问题集仍然可以保持高的预测准确率，并显著减少调查时间。 |
| [^87] | [GPT4GEO: How a Language Model Sees the World's Geography.](http://arxiv.org/abs/2306.00020) | 研究调查了GPT-4的地理知识水平，并探讨其在地理数据分析等领域的应用潜力。 |
| [^88] | [Incorporating Domain Knowledge in Deep Neural Networks for Discrete Choice Models.](http://arxiv.org/abs/2306.00016) | 本文提出了一种框架，将领域知识和先验信念纳入离散选择模型（DCM）的深度神经网络（DNN）中，以扩展数据驱动方法在DCM中的潜力。所提出的方法既具有可解释性，又保留了DCM的优点和DNNs的灵活性，并优于现有的最先进方法。 |
| [^89] | [GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks.](http://arxiv.org/abs/2306.00015) | 该论文提出了一种后期方法GraphCleaner，用于在流行的图学习基准数据集中检测和纠正错误标注节点。GraphCleaner 组合了合成错误标注数据集的生成和邻域感知错误标注检测两种新颖思想，实证评估结果表明其平均F1得分提高了0.14。 |
| [^90] | [PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models.](http://arxiv.org/abs/2306.00014) | 本文提出了一种新颖的“量化前微调”框架 PreQuant，可用于预训练语言模型的通用量化，有效降低了模型的复杂度和使用成本。 |
| [^91] | [Machine Learning Approach for Cancer Entities Association and Classification.](http://arxiv.org/abs/2306.00013) | 本研究利用命名实体识别和文本分类的机器学习方法自动从大量生物医学文献中提取癌症相关实体和实体间关系，以帮助推进癌症研究进展。 |
| [^92] | [Graph Neural Network for spatiotemporal data: methods and applications.](http://arxiv.org/abs/2306.00012) | 本文综述了基于图神经网络的时空数据方法及应用，并提供了一个GNN的分类及其应用领域。这个综述突出了使用GNN分析时空数据的强大工具及其局限性，并指出未来研究方向和目前面临的挑战。 |
| [^93] | [A Self-Supervised Approach for Cluster Assessment of High-Dimensional Data.](http://arxiv.org/abs/2306.00011) | 本文提出了一种利用自监督深度神经网络生成代表性嵌入的方法，用于评估复杂图像数据中的聚类结构。 |
| [^94] | [Explainability in Simplicial Map Neural Networks.](http://arxiv.org/abs/2306.00010) | 本文提出了简单形式映射神经网络（SMNN）的训练过程和替代凸多面体的方法，并且首次引入了 SMNN 的可解释性能力。 |
| [^95] | [Graph Exploration Matters: Improving both individual-level and system-level diversity in WeChat Feed Recommender.](http://arxiv.org/abs/2306.00009) | 该论文提出了一种基于图形探索的方法，以捕捉用户对不同项目和项目自身属性的固有偏好，从而提高微信推荐系统中个人层面和系统层面的多样性。 |
| [^96] | [Brainformers: Trading Simplicity for Efficiency.](http://arxiv.org/abs/2306.00008) | Brainformers 是一个新的深度神经网络模型，它通过使用多样层级的结构完善了 Transformer 的设计缺陷，具有更高效的训练收敛和更快的步骤时间，表现出更优秀的性能。 |
| [^97] | [Datasets for Portuguese Legal Semantic Textual Similarity: Comparing weak supervision and an annotation process approaches.](http://arxiv.org/abs/2306.00007) | 本文提供了四个法律领域的数据集，其中两个未标记，另外两个使用启发式标签进行了标记，旨在用于文本语义相似性任务。研究比较了弱监督和传统注释过程方法，表明弱监督可以达到与传统方法相当的性能。 |
| [^98] | [Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection.](http://arxiv.org/abs/2306.00006) | 本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。 |
| [^99] | [Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning.](http://arxiv.org/abs/2306.00003) | 本研究提出了一种基于监督式注意力多实例学习的方法，可以自动分析超声图像，实现AS的精确筛查且精度优于传统机器学习和深度学习方法。 |
| [^100] | [Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust.](http://arxiv.org/abs/2305.20030) | 本文提出了一种名为树轮数字水印的技术，可以稳定地指纹扩散模型的输出，与现有的在采样后对图像进行修改的方法不同，数字水印微妙地影响整个采样过程，从而产生模型指纹，对人类不可见。 |
| [^101] | [Beam Tree Recursive Cells.](http://arxiv.org/abs/2305.19999) | 本论文提出了一种支持反向传播的递归神经网络框架——束搜索递归单元（BT-Cell），用于扩展递归神经网络，实现对潜在结构的感知；此外，我们提出了一种放松束搜索中硬前k算子的方法，以实现更好的梯度信号传递。在评估中发现，BT-Cell在合成和实际数据的多个具有结构敏感性的任务中表现优异。 |
| [^102] | [InGram: Inductive Knowledge Graph Embedding via Relation Graphs.](http://arxiv.org/abs/2305.19987) | InGram是一种新的归纳式知识图谱补全方法，可以在推理时生成新关系和实体的嵌入，并使用注意力机制汇总邻居嵌入生成关系和实体嵌入。该方法在多个基准数据集上的性能优于现有的基准方法。 |
| [^103] | [Multi-Dataset Co-Training with Sharpness-Aware Optimization for Audio Anti-spoofing.](http://arxiv.org/abs/2305.19953) | 本文提出了一种锐度感知优化的多数据集协同训练方法，用于解决音频反欺骗问题。实验结果表明，该方法在各种数据集上表现出了良好的结果，同时仅需要大型预训练模型的4000倍参数。 |
| [^104] | [Spontaneous symmetry breaking in generative diffusion models.](http://arxiv.org/abs/2305.19693) | 本文揭示生成式扩散模型存在自发对称破缺现象，这将其生成动力学分为两种不同的“相”，提出了高斯后期初始化方案，能够显著提高模型性能。 |
| [^105] | [Point-GCC: Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast.](http://arxiv.org/abs/2305.19623) | Point-GCC提出了一种通过几何-颜色对比进行通用自监督三维场景预训练的方法，并设计了分层监督和架构无关的骨干网络，以缩小预训练和下游任务之间的差距。 |
| [^106] | [Offline Meta Reinforcement Learning with In-Distribution Online Adaptation.](http://arxiv.org/abs/2305.19529) | 本文提出了一种带有不确定性量化的内部分布在线适应(IDAQ)的框架，利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务，在离线元强化学习上具有竞争性表现。 |
| [^107] | [Label Embedding by Johnson-Lindenstrauss Matrices.](http://arxiv.org/abs/2305.19470) | 这篇论文提出了基于JLMs的标签嵌入方法，将多元分类问题转化为有限回归问题，具有较高的计算效率和预测准确性。 |
| [^108] | [Hierarchical Graph Generation with $K^2$-trees.](http://arxiv.org/abs/2305.19125) | 本文介绍了一种基于$K^2$-树的图生成方法，该方法可以实现紧凑生成，并同时捕获图的内在分层结构。通过提出顺序$K^2$-树表示和引入基于Transformer的架构，本文进一步改进了这种方法。实验表明，该方法在图生成方面具有卓越的表现。 |
| [^109] | [Policy Gradient Algorithms for Robust MDPs with Non-Rectangular Uncertainty Sets.](http://arxiv.org/abs/2305.19004) | 本文提出了针对具有非矩形不确定性集的强健MDP的策略梯度算法，并开发了投射Langevin动力学算法和确定性策略梯度方法。数值实验展示了这些算法的有效性。 |
| [^110] | [Policy Optimization for Continuous Reinforcement Learning.](http://arxiv.org/abs/2305.18901) | 本研究提出了连续强化学习领域的占用时间概念，并在此基础上扩展了离散强化学习中的PG、TRPO和PPO方法，为连续强化学习领域的研究提供了新的思路和方法。 |
| [^111] | [Contrastive Shapelet Learning for Unsupervised Multivariate Time Series Representation Learning.](http://arxiv.org/abs/2305.18888) | 该论文提出了一种新的无监督多元时间序列表示学习框架，通过对比形态片段学习以及多粒度对比和多尺度对齐的学习目标，学习时间序列特定表示。 |
| [^112] | [Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation.](http://arxiv.org/abs/2305.18885) | 本文提出了一种面向多准则推荐的标准偏好感知轻量图卷积网络，该方法结合了MC扩展图，可以准确地捕捉用户的标准偏好，并进一步将用户对各个标准的偏好合并到最终的推荐列表中。 |
| [^113] | [Task-Equivariant Graph Few-shot Learning.](http://arxiv.org/abs/2305.18758) | 本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。 |
| [^114] | [Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting.](http://arxiv.org/abs/2305.18687) | 本文提出了一种称为GRAM-ODENNs的新型神经网络结构，用于解决当前图ODE模型的局限性，包括局部模式的忽略和缺乏动态语义边缘，用于更准确的长程交通预测。 |
| [^115] | [Controllable Path of Destruction.](http://arxiv.org/abs/2305.18553) | 本文介绍了可控毁灭路径方法，该方法是一种自我监督的迭代生成器学习方法，通过向修复轨迹的状态-动作对添加条件输入来实现可控性。 |
| [^116] | [Bandwidth Optimal Pipeline Schedule for Collective Communication.](http://arxiv.org/abs/2305.18461) | 本文提出了一种强多项式时间算法，用于在任何拓扑结构上实现带宽最优的全聚合/归约散开，为解决方案提供了通用性，可以轻松扩展到其他形式的集合通信方法。 |
| [^117] | [Conformal Prediction with Large Language Models for Multi-Choice Question Answering.](http://arxiv.org/abs/2305.18404) | 本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。 |
| [^118] | [Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning.](http://arxiv.org/abs/2305.18403) | 本论文提出了一种名为LoRAPrune的框架，可以高效微调和部署大型预训练模型，通过利用低秩自适应的值和梯度来设计PEFT感知的剪枝准则，并提出了一个迭代剪枝过程来去除冗余参数，实验结果表明与最先进的方法相比，可以显著降低模型大小和推理时间，同时保持竞争性的准确性。 |
| [^119] | [A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning.](http://arxiv.org/abs/2305.18400) | 提出了一个元学习框架，用于调整可信联邦学习保护机制的参数，以在隐私泄露、效用损失和效率降低之间进行权衡。 |
| [^120] | [Neural Task Synthesis for Visual Programming.](http://arxiv.org/abs/2305.18342) | 该论文提出了一种基于神经符号技术的可视化编程任务合成方法NeurTaskSyn。该方法能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，自动生成编程任务。 |
| [^121] | [Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers.](http://arxiv.org/abs/2305.18256) | 本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。 |
| [^122] | [Counterfactual Formulation of Patient-Specific Root Causes of Disease.](http://arxiv.org/abs/2305.17574) | 本文提出了一种针对疾病患者个体的根本原因的新公式，可以用于自动从数据中检测根本原因，并考虑了噪声标签和疾病流行率等因素，同时具有快速计算的优势。 |
| [^123] | [Federated Conformal Predictors for Distributed Uncertainty Quantification.](http://arxiv.org/abs/2305.17564) | 本文将符合预测推广到联邦学习设置，提出联邦符合预测（FCP）框架，处理了数据集异质性的问题。实验结果表明，FCP具有严格的理论保证，在分布式和异质环境中有效。 |
| [^124] | [Translatotron 3: Speech to Speech Translation with Monolingual Data.](http://arxiv.org/abs/2305.17547) | Translatotron 3提出了一种新方法，使用单语数据进行语音到语音翻译，无需配对的数据或专业建模，展示了保留语言/非语言信息的能力。 |
| [^125] | [The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent.](http://arxiv.org/abs/2305.17490) | 本文研究了随机梯度下降的动态稳定性隐式正则化，证明了其具有良好的泛化性。 |
| [^126] | [A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU.](http://arxiv.org/abs/2305.17473) | 本文全面概括了深度学习模型的类型和应用，比较分析了各个模型的结构、优点和局限性，有助于选择和设计深度学习模型。 |
| [^127] | [Attention Schema in Neural Agents.](http://arxiv.org/abs/2305.17375) | 本文研究了神经智能中的注意力模式，并提出了注意力模式理论（AST）。作者发现将AS实现为一种循环内部控制的智能体效果最佳，这一理论为应用与改进神经智能提供了新思路。 |
| [^128] | [Distilling BlackBox to Interpretable models for Efficient Transfer Learning.](http://arxiv.org/abs/2305.17303) | 本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。 |
| [^129] | [Heterogeneous Value Evaluation for Large Language Models.](http://arxiv.org/abs/2305.17147) | 本文提出了一种自动对齐评估方法A2EHV，采用异质价值系统，并基于价值合理性和社会价值定向框架评估代理人行为的社会偏好，结果表明比传统对齐方法更合理。 |
| [^130] | [Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory.](http://arxiv.org/abs/2305.17144) | 本文提出了Ghost in the Minecraft (GITM)框架，利用大型语言模型与基于文本的知识和记忆，创造了一种在Minecraft中具备通用能力的智能体，可在以文本为基础的复杂编程环境中熟练导航。 |
| [^131] | [Towards Certification of Machine Learning-Based Distributed Systems.](http://arxiv.org/abs/2305.16822) | 认证技术对于机器学习驱动的分布式系统验证不适用，本文提出了第一个ML-based分布式系统认证方案，以验证其非功能属性。 |
| [^132] | [Counterfactual Explainer Framework for Deep Reinforcement Learning Models Using Policy Distillation.](http://arxiv.org/abs/2305.16532) | 本文提出了一种新的基于反事实解释方法的框架来解释黑盒DRL所作的决策，并且在实验中展示了该解释框架的可行性和有效性。 |
| [^133] | [Empirical Optimal Transport between Conditional Distributions.](http://arxiv.org/abs/2305.15901) | 本文考虑在一个公共变量的条件下，相应分布之间的最优输运问题。通过采用基于 MMD 的核正则化器，克服了条件变量是连续的和两个分布中该变量的边缘是不同的挑战。 |
| [^134] | [LFTK: Handcrafted Features in Computational Linguistics.](http://arxiv.org/abs/2305.15878) | 该论文收集和分类了超过220个受欢迎的手工语言特征，设计了一个多语言的手工语言特征提取系统，以系统性的可扩展方式实现，并在几个任务特定的数据集上进行了相关性分析研究。 |
| [^135] | [Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective.](http://arxiv.org/abs/2305.15408) | 本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。 |
| [^136] | [LMs with a Voice: Spoken Language Modeling beyond Speech Tokens.](http://arxiv.org/abs/2305.15255) | SPECTRON是一个新颖的语音延续模型，通过利用预训练的语言模型和语音编码器进行端到端的训练来生成文本和语音输出，在语义内容和讲话者保护方面超越了现有的口语语言模型。 |
| [^137] | [Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings.](http://arxiv.org/abs/2305.14117) | 该论文提出使用语音处理技术结合自然语言样本(NLS)分析孤独症儿童的口语发展水平，能够分类出儿童和成人语音，以及语音和非语言发声，并取得了较高的准确率。 |
| [^138] | [Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis.](http://arxiv.org/abs/2305.10638) | 本文提出了CORAL，一种用于在线无监督根本原因分析的新框架，可以自动触发该过程并增量更新模型，包括三个主要部分：触发点检测，增量因果图学习和基于网络传播的根本原因定位。 |
| [^139] | [Evaluation Metrics for CNNs Compression.](http://arxiv.org/abs/2305.10616) | 本文提供了神经网络压缩的评估度量综述，从而为标准化神经网络压缩贡献力量。 |
| [^140] | [Automatic Photo Orientation Detection with Convolutional Neural Networks.](http://arxiv.org/abs/2305.10319) | 本论文使用CNN解决了照片方向检测的问题，并在数据集上显著提高性能。使用Guided Backpropagation获得了CNN检测方向的见解。 |
| [^141] | [Continual Vision-Language Representaion Learning with Off-Diagonal Information.](http://arxiv.org/abs/2305.07437) | 本文探讨了通过流数据持续训练CLIP模型的可行性，提出了一种有效的连续学习框架Mod-X，并证明内部旋转和跨模态偏差导致了CLIP在跨模态检索任务中性能下降。 |
| [^142] | [Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization.](http://arxiv.org/abs/2305.05760) | 本研究提出了一种新方法，可基于周期时间设置超参数，使得PPO和SAC在广泛的循环时间范围内进行学习，同时实现了接近耗时的在线超参数调整获得的性能。 |
| [^143] | [Scalable Optimal Margin Distribution Machine.](http://arxiv.org/abs/2305.04837) | 本文提出了一种可扩展的最优边缘分布机（ODM）训练方法，与原始ODM训练方法相比，可实现近十倍的加速。对于非线性核，我们提出了一种新颖的分布感知分区方法，使得每个分区上训练的本地ODM接近全局的ODM，并快速收敛。在应用线性核时，我们扩展了一种通信有效的SVRG方法以进一步加速训练。 |
| [^144] | [Building Neural Networks on Matrix Manifolds: A Gyrovector Space Approach.](http://arxiv.org/abs/2305.04560) | 本文通过将陀螺矢量空间中的一些概念推广到SPD和Grassmann流形，提出了在这些流形上构建神经网络的新模型和新层，并在人类动作识别和知识图谱完成两个应用中展示了其有效性。 |
| [^145] | [Disentangled Multi-Fidelity Deep Bayesian Active Learning.](http://arxiv.org/abs/2305.04392) | 本论文提出了一种新的框架D-MFDAL，它可以学习具有多个保真度的函数分布，相比于现有基于高斯过程的方法，该方法具有更好的性能和高斯过程的灵活性和可解释性。 |
| [^146] | [Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs.](http://arxiv.org/abs/2305.03935) | 本文提出了基于扩散ODEs最大似然估计的改进技术，包括速度参数化和方差减少技术等用于训练的技术，以及误差有界的高阶流匹配目标用于微调和截断正态去量化方法用于评估。 |
| [^147] | [How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?.](http://arxiv.org/abs/2305.01555) | 本文通过使用GPT-3.5模型在少样本关系抽取中，实现在四个不同数据集上的新的最优性能，并提出了与任务相关的指导说明和约束模式下的数据生成方法。 |
| [^148] | [Generalization for slowly mixing processes.](http://arxiv.org/abs/2305.00977) | 该论文研究了慢混合过程的泛化能力，给出了对由平稳且phi混合过程生成的数据的不同种类损失类别的一种上界。 |
| [^149] | [Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation.](http://arxiv.org/abs/2305.00955) | 本文调查研究了利用人类反馈改进自然语言生成方面的最近研究，包括对反馈的全面形式化、反馈的格式和目的的描述，和提出使用反馈的两种方法。我们还讨论了用于人类反馈数据收集的现有数据集，和相关的担忧。 |
| [^150] | [Power Grid Behavioral Patterns and Risks of Generalization in Applied Machine Learning.](http://arxiv.org/abs/2304.10702) | 本论文基于实际运行数据，分析了电力系统行为模式及模型泛化风险。研究结果表明，忽略电网特定模式可能导致对新输入输出不可行、不可实现或完全无意义的预测。 |
| [^151] | [Graph Neural Network-Based Anomaly Detection for River Network Systems.](http://arxiv.org/abs/2304.09367) | 本研究采用图神经网络模型Graph Deviation Network (GDN)来捕捉河流传感器数据的复杂时空关系，并提出了备用异常阈值标准GDN+，以实现对水质的准确持续监测。 |
| [^152] | [EfficientNet Algorithm for Classification of Different Types of Cancer.](http://arxiv.org/abs/2304.08715) | 本文用EfficientNet算法分类不同类型的癌症，实验结果表明该算法在公共数据集上表现优异，具有在临床实践中提高癌症诊断准确性和效率的潜力。 |
| [^153] | [Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value.](http://arxiv.org/abs/2304.07718) | Data-OOB是一种新的数据价值估计方法，它利用out-of-bag估计，并可以在计算上高效处理大型数据集。 |
| [^154] | [A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models.](http://arxiv.org/abs/2304.04916) | 本文提出了一种基于数据驱动的状态聚合方法来降低动态离散选择模型的估计计算和样本复杂度，首先利用反向强化学习估计代理Q函数，然后用聚类算法选择重要的状态聚合，最终利用嵌套固定点算法进行最大似然估计。 |
| [^155] | [Brain-Inspired Spiking Neural Network for Online Unsupervised Time Series Prediction.](http://arxiv.org/abs/2304.04697) | 本论文提出了一种新的连续学习无监督循环尖峰神经网络模型，可以高效、节能地进行在线时间序列预测，并且通过随机延迟嵌入和使用底层动态系统中介数中心性最高的神经元的膜电位进行重构。 |
| [^156] | [Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge.](http://arxiv.org/abs/2304.03392) | 该论文提出了一种采用机器学习和领域知识进行个性化数字健康行为变革干预的系统，其利用反事实例子进行特征控制以预测干预效果并优化干预效果。 |
| [^157] | [The G-invariant graph Laplacian.](http://arxiv.org/abs/2303.17001) | 本文提出了 G不变图拉普拉斯算子 用于处理数据集不仅在流形上，而且在一个连续群的作用下也是封闭的情形，相较于标准图拉普拉斯算子收敛速度更快。 |
| [^158] | [End-to-End Diffusion Latent Optimization Improves Classifier Guidance.](http://arxiv.org/abs/2303.13703) | 本文提出了一种新的分类器引导方法DOODL，它可以通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，并在计算和人类评估度量上优于一步分类器指导。 |
| [^159] | [Explaining Recommendation System Using Counterfactual Textual Explanations.](http://arxiv.org/abs/2303.11160) | 本文提供了一种利用反事实推理来生成可理解解释的方法，其在推荐系统上取得了成功应用。 |
| [^160] | [Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts.](http://arxiv.org/abs/2303.05155) | 本文介绍了一种名为Aux-Drop的策略，适用于在线学习，能够处理混乱的输入特征。通过调整常规的丢失正则化方案，确保最终输出受到这些特征混乱出现的最小影响，有助于构建更健壮有效的在线学习系统。 |
| [^161] | [Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss.](http://arxiv.org/abs/2303.03027) | 本文使用布雷-瓦瑟斯坦距离训练协方差矩阵的深度矩阵分解模型，并在有限秩矩阵空间内表征关键点和最小化问题，最终确定了梯度下降算法的收敛性。 |
| [^162] | [Neural Operator Learning for Long-Time Integration in Dynamical Systems with Recurrent Neural Networks.](http://arxiv.org/abs/2303.02243) | 本文提出了一种将神经算子和递归神经网络相结合的新型框架，解决了深度神经网络无法准确外推和误差积累的问题，在 Korteweg-de Vries 方程中表现出更好的精度和稳定性。 |
| [^163] | [When does Privileged Information Explain Away Label Noise?.](http://arxiv.org/abs/2303.01806) | 本文研究了在什么情况下，使用特权信息可以解决标签噪声问题，发现当PI帮助神经网络轻松分辨出嘈杂和干净的数据，与提供记忆嘈杂数据的学习快捷方式时，它是最有用的，在PI过于预测目标标签时，PI方法表现会更差。在此基础上，提出了一些增强方法，用于处理标签噪音。 |
| [^164] | [Bootstrapping Parallel Anchors for Relative Representations.](http://arxiv.org/abs/2303.00721) | 该论文提出了一种引导方法，通过已知的集合发现新的并行锚点，以克服相对表示式中获取并行锚点困难的问题，能够用于不同域之间的语义对应，取得竞争性结果。 |
| [^165] | [Hiding Data Helps: On the Benefits of Masking for Sparse Coding.](http://arxiv.org/abs/2302.12715) | 本文研究了稀疏编码中学习得到的大于实际字典的情况下，噪声会导致标准的字典学习目标函数无法恢复出实际字典的问题，提出了通过遮盖数据的方法进行可靠的字典恢复适用于多种信号模态。 |
| [^166] | [K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs.](http://arxiv.org/abs/2302.11996) | 本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。 |
| [^167] | [Distributional Offline Policy Evaluation with Predictive Error Guarantees.](http://arxiv.org/abs/2302.09456) | 本论文提出了一种名为Fitted Likelihood Estimation (FLE)的算法来解决分布式离线策略评估的问题，该算法能够学习到密切接近真实分布的策略回报分布。 |
| [^168] | [Measuring Equality in Machine Learning Security Defenses.](http://arxiv.org/abs/2302.08973) | 本文研究了机器学习安全防御方法的平等性能问题，提出了一种简单的平等度量和分析框架，鼓励进一步探索公平性在该领域的应用。 |
| [^169] | [Best Arm Identification for Stochastic Rising Bandits.](http://arxiv.org/abs/2302.07510) | 本文提出了两种算法解决了固定预算下，针对随机递增赌博机最佳臂识别的问题，并且在足够大的预算下，这两个算法都能正确识别最优选项。 |
| [^170] | [Near-optimal learning with average H\"older smoothness.](http://arxiv.org/abs/2302.06005) | 通过推广平均Lipschitz平滑性到Hölder平滑性，得到了关于平均Hölder平滑性的上下风险界，最优的下界对数因子最多差一个，提供了独立的学习算法。 |
| [^171] | [ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction.](http://arxiv.org/abs/2302.05783) | 本文提出了一种基于对比学习的框架ConCerNet，用于提高DNN动力学建模的可靠性，实现对系统不变量的自动捕捉和保留，经实验证明其性能优于传统神经网络方法。 |
| [^172] | [A Survey on Causal Reinforcement Learning.](http://arxiv.org/abs/2302.05209) | 本文综述了因果关系强化学习的最新进展和未来研究方向，将现有的CRL方法分为两类，并讨论了因果关系和RL之间的关系。 |
| [^173] | [Better Diffusion Models Further Improve Adversarial Training.](http://arxiv.org/abs/2302.04638) | 本文证明了更好的扩散模型可以进一步提高对抗训练的性能，通过采用最新的扩散模型，我们训练的模型仅使用生成的数据就在RobustBench上实现了最佳性能，并在CIFAR-10和CIFAR-100数据集上分别提高了$+4.58\%$和$+8.03\%$的性能。 |
| [^174] | [Bag of Tricks for Training Data Extraction from Language Models.](http://arxiv.org/abs/2302.04460) | 本文总结了一些技巧用于改进语言模型训练数据提取，提出了在文本生成和文本排名中可以使用的技巧，实验证明这些技巧对于提高训练数据提取的效果非常重要。 |
| [^175] | [Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery.](http://arxiv.org/abs/2302.03668) | 该论文提出了一种基于梯度的离散优化方法，用于自动生成文本提示，进而控制现代生成模型的输出。该方法可以优化文本到图像和文本到文本的应用，为API用户提供了轻松生成、发现、混合和匹配图像概念的能力，同时自动发现硬提示可以有效地提高模型性能。 |
| [^176] | [The SSL Interplay: Augmentations, Inductive Bias, and Generalization.](http://arxiv.org/abs/2302.02774) | 本论文通过分析数据增强、网络架构和训练算法的相互作用，研究了预训练和下游任务的泛化性能，并为SSL从业人员提供了一些见解。 |
| [^177] | [Oscillation-free Quantization for Low-bit Vision Transformers.](http://arxiv.org/abs/2302.02210) | 研究发现，可学习比例因子加剧了权重振荡，本文提出三种技术以解决这个问题，并在多个基准测试上显著提高了模型的性能。 |
| [^178] | [LazyGNN: Large-Scale Graph Neural Networks via Lazy Propagation.](http://arxiv.org/abs/2302.01503) | 本文提出了一种基于浅层模型和惰性传播的图神经网络模型 LazyGNN，用于大规模图的表示学习，具有很高的效率和可扩展性。 |
| [^179] | [Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with Record Level Differential Privacy.](http://arxiv.org/abs/2302.01068) | 本文通过利用合成样本实现全局优化，加入记录级差分隐私以保护隐私，验证了该方法的数据集有效性。 |
| [^180] | [Grounding Language Models to Images for Multimodal Inputs and Outputs.](http://arxiv.org/abs/2301.13823) | 该论文提出一种有效的方法，将仅处理文本的语言模型与图像进行联系，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的自由形式文本。该方法在环境相关的图像检索和多模态对话等任务中表现十分优异，是利用预训练语言模型解决视觉场景下交互问题的有效解决方案。 |
| [^181] | [Improved Algorithms for Multi-period Multi-class Packing Problems with Bandit Feedback.](http://arxiv.org/abs/2301.13791) | 本文提出了一个改进的算法来解决多时期多分类装载问题，其中使用bandit反馈，提出的算法具有更快的收敛速率和更低的遗憾，同时解决了一个相关问题。 |
| [^182] | [Retrosynthetic Planning with Dual Value Networks.](http://arxiv.org/abs/2301.13755) | PDVN是一种新的在线训练算法，它在逆向合成规划中利用双价值网络优化完整的路线，成功率和效率上均优于现有方法。 |
| [^183] | [Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity.](http://arxiv.org/abs/2301.13443) | 本文提出两种新公平度量方法：概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），实现人口统计特征的公平机器学习 |
| [^184] | [Equivariant Architectures for Learning in Deep Weight Spaces.](http://arxiv.org/abs/2301.12780) | 本论文提出了一种新的网络架构，用于在深层权重空间中学习，它对MLP权重的自然置换对称等变，可以处理广泛有趣的任务。 |
| [^185] | [A Closer Look at Few-shot Classification Again.](http://arxiv.org/abs/2301.12246) | 本文研究了Few-shot分类问题中的训练算法和适应算法，并实证证明这两个算法是可以完全分离的。此外，本文的元分析揭示出了关于Few-shot分类的关键方面和与其他领域（如视觉表示学习和迁移学习）的联系的有趣见解。 |
| [^186] | [Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates.](http://arxiv.org/abs/2301.11294) | 本文提出了一种基于硬币投注的贝叶斯推断方法，完全不需要学习速率，可以在高维模型和数据集上表现出与其他方法相当的性能。 |
| [^187] | [Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons.](http://arxiv.org/abs/2301.11270) | 该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。 |
| [^188] | [Graph Neural Tangent Kernel: Convergence on Large Graphs.](http://arxiv.org/abs/2301.10808) | 本文利用GNTK和图核函数探究大规模图神经网络的训练动态，证明了GNTK在收敛于图核函数时的确切性质。这意味着在大规模图的情况下，可以在中等大小的图上进行拟合并在整个图上使用。 |
| [^189] | [The Backpropagation algorithm for a math student.](http://arxiv.org/abs/2301.09977) | 本文介绍了针对深度神经网络的反向传播算法。通过使用Jacobian算子将损失函数的梯度表示为矩阵乘积，可以高效地计算梯度，而且该方法适用于不同数量的层数。 |
| [^190] | [Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation.](http://arxiv.org/abs/2301.09318) | 该论文提出的基于深度学习的自然灾害分割模型在透明云，烟雾柱和洪水分割任务中实现了最先进的性能，并通过在适当的预训练任务上进行预训练，显着提高了模型的通用性。 |
| [^191] | [Learning Decorrelated Representations Efficiently Using Fast Fourier Transform.](http://arxiv.org/abs/2301.01569) | 本文提出了一种使用快速傅里叶变换计算去相关正则化器的方法，相较于传统方法训练更快且内存需求更小，在下游任务中显示出同等的准确性。 |
| [^192] | [Transferable Energy Storage Bidder.](http://arxiv.org/abs/2301.01233) | 本文提出了一种可转移、多功能的储能竞标者模型，在整个电力市场中表现出了最先进的结果，并通过预训练和应用于另一个地区的套利得到了证明。 |
| [^193] | [CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection.](http://arxiv.org/abs/2301.00785) | 本文提出了基于CLIP的通用模型，通过文本嵌入学习解剖学关系，能够分割25种器官和6种肿瘤，具有强大的泛化能力。 |
| [^194] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^195] | [Teaching Small Language Models to Reason.](http://arxiv.org/abs/2212.08410) | 本文研究了如何通过知识蒸馏，将大型语言模型的推理能力传递到小型语言模型中，并证明这样的方法可以提高小型模型在算术、常识和符号推理方面的性能。 |
| [^196] | [AirfRANS: High Fidelity Computational Fluid Dynamics Dataset for Approximating Reynolds-Averaged Navier-Stokes Solutions.](http://arxiv.org/abs/2212.07564) | 本论文开发了一个数据集AirfRANS，用于研究不同攻击角的翼型在亚音速区域的二维不可压可稳定雷诺平均纳维-斯托克斯方程，并通过度量方法和可视化评估模型的预测能力，提出了深度学习基线研究，以研究AirfRANS的泛化考虑。 |
| [^197] | [Pattern Attention Transformer with Doughnut Kernel.](http://arxiv.org/abs/2211.16961) | 本论文提出了一种新的模式注意力变换器(PAT)，它采用了新的圆环核设计，以解决图像分类中像素高分辨率的问题。 |
| [^198] | [Offline Reinforcement Learning with Closed-Form Policy Improvement Operators.](http://arxiv.org/abs/2211.15956) | 本文提出了基于行为约束的离线强化学习中的闭合形式策略改进算子，该算子将行为策略建模为高斯混合，利用LogSumExp的下界和Jensen不等式克服了优化困难，能有效处理实际数据集中的异构策略。 |
| [^199] | [Prosody-controllable spontaneous TTS with neural HMMs.](http://arxiv.org/abs/2211.13533) | 本文提出一种神经HMM TTS系统，在保持和自然语音对齐状态的前提下，实现了话语级语调控制，使得从少量不规则数据中快速学习变得可能，能够重现自然语音中的多样性表现。 |
| [^200] | [Structural Optimization of Factor Graphs for Symbol Detection via Continuous Clustering and Machine Learning.](http://arxiv.org/abs/2211.11406) | 本文提出了一种基于连续聚类和机器学习的因子图结构优化方法，用于线性码间干扰信道上的符号检测。该方法能够将因子图框架的潜力发挥到极致，实现低复杂度符号检测器，同时结合神经置信传播算法，在特定信道下实现近似最大后验符号检测性能。 |
| [^201] | [An Advantage Using Feature Selection with a Quantum Annealer.](http://arxiv.org/abs/2211.09756) | 利用量子退火器进行特征选择的这种技术，可以尽可能提高特征的预测能力，同时最小化冗余性，使数据处理更准确且有效。 |
| [^202] | [Reward Gaming in Conditional Text Generation.](http://arxiv.org/abs/2211.08714) | 在条件文本生成中，使用强化学习（RL）进行训练时，噪声、自然发生的虚假相关性和协变量漂移可能会导致不良模式被错误地赋予高奖励值，这可能会导致奖励博弈，需要解决。 |
| [^203] | [Mechanistic Mode Connectivity.](http://arxiv.org/abs/2211.08422) | 本文从模式连通性的视角研究神经网络损失景观，提出了机制相似性的定义，并演示了两个模型之间缺乏线性连通性意味着它们使用不同的机制来做出预测。此外，本文还提出了一种名为基于连通性的微调（CBFT）的方法，用于目标修改模型机制，有助于消除模型对虚假特征的依赖。 |
| [^204] | [Emergent Linguistic Structures in Neural Networks are Fragile.](http://arxiv.org/abs/2210.17406) | 本文提出了一个框架来评估语言模型对于语法的表述的一致性和稳健性，通过多项实验证据表明，神经网络中 emergent 语言结构是脆弱的。 |
| [^205] | [Real-time high-resolution CO$_2$ geological storage prediction using nested Fourier neural operators.](http://arxiv.org/abs/2210.17051) | 介绍了一种机器学习框架，嵌套傅里叶神经算子（Nested Fourier Neural Operator，FNO），用于在盆地尺度上进行高分辨率动态三维CO$_2$封存建模。相对于现有方法，加快了将近700,000倍的流动预测速度。 |
| [^206] | [Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models.](http://arxiv.org/abs/2210.15458) | 本文章提出了一种算术采样框架，该方法可兼容常见的采样变化，具有可证明的束多样性和令人尴尬的并行性，从原始模型提供无偏和一致的期望。在WMT机器翻译中表现出良好的效果。 |
| [^207] | [Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup.](http://arxiv.org/abs/2210.13512) | 本文提出了一种基于中点 Mixup 的多视角数据学习方法，相比于传统经验风险最小化方法能够更好地学习每个类别的所有特征，具有更好的泛化和鲁棒性。 |
| [^208] | [Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries.](http://arxiv.org/abs/2210.10750) | 本文提出了一种利用对抗工具直接优化具有鉴别性和多样性的查询来实现更好的成员推断的方法，相比于现有方法，在离线场景和低误报重要的情况下实现了更准确的成员推断。 |
| [^209] | [Efficient Bi-Level Optimization for Recommendation Denoising.](http://arxiv.org/abs/2210.10321) | 本文提出了一种推荐去噪的高效双层优化方法，该方法可以迭代调整推荐模型，以考虑前几次迭代中为每个反馈分配的权重。 |
| [^210] | [On the Identifiability and Estimation of Causal Location-Scale Noise Models.](http://arxiv.org/abs/2210.09054) | 本文研究了一类异方差噪声模型，发现除特殊情况外因果方向是可识别的。提出了两个估计器，能够准确识别因果效应。 |
| [^211] | [SQuId: Measuring Speech Naturalness in Many Languages.](http://arxiv.org/abs/2210.06324) | SQuId是一个使用一百万个评分进行训练并在65个语言环境下进行测试的多语言自然度预测模型，该模型的训练在许多环境中始终优于单一环境的基线，并展现了比竞争基线更出色的表现;研究还突出了跨语言细调的有效性，以及非语言效果如声音畸变在跨语言细调中的作用。 |
| [^212] | [Temporally Consistent Transformers for Video Generation.](http://arxiv.org/abs/2210.02396) | 本文介绍了一种名为TECO的生成模型，其可以大大提高视频生成的长期时间一致性。作者提出了3个难度不同的视频数据集以评估该模型和现有模型在时间一致性方面的局限性。 |
| [^213] | [Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations.](http://arxiv.org/abs/2209.13446) | 本文提出了一种基于特征学习的多样性隐私保护反事实解释方法，可以有效地处理反事实约束条件，并为私有解释模型做出贡献。 |
| [^214] | [Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics.](http://arxiv.org/abs/2208.10533) | 本文提出一种名为评判置信度引导探索的方法，用于将现有的神谕策略纳入标准的演员-评论家强化学习算法中，以提高探索效率。在不确定性高时，该方法会将神谕策略的行动作为建议纳入学习方案中，而在不确定性低时忽略它。 |
| [^215] | [Sample Complexity of Forecast Aggregation.](http://arxiv.org/abs/2207.13126) | 本文研究了一种预测聚合模型，考虑了贝叶斯方法应用于专家命中后的信号汇聚。论文中提供了对于该问题的样本复杂度，表明该复杂度至少为 $\tilde \Omega(m^{n-2} / \varepsilon)$，其中 m 是每个专家信号空间的大小。 |
| [^216] | [An Ultra-low Power TinyML System for Real-time Visual Processing at Edge.](http://arxiv.org/abs/2207.04663) | 本文提出了一种超低功耗TinyML系统，采用微型骨干网络构建高效CNN模型，再由特别设计的神经协处理器与MCU相连，将所有特征和权重储存在芯片上，完全消除芯片外存储器访问的延迟和功耗。此系统具有相当的准确性并实现了创纪录的160mW超低功耗。 |
| [^217] | [A Theoretical Analysis of the Learning Dynamics under Class Imbalance.](http://arxiv.org/abs/2207.00391) | 本文分析证明了数据不平衡对学习的负面影响，说明在使用梯度下降训练时，少数和多数类的学习曲线会遵循次优轨迹，同时提出对每种类别梯度做出贡献的归一化变体，以解决优化不同类别之间的竞争问题。 |
| [^218] | [Stability Verification of Neural Network Controllers using Mixed-Integer Programming.](http://arxiv.org/abs/2206.13374) | 本文提出了一种用混合整数规划方法验证神经网络控制器稳定性的方法，通过解决混合整数二次规划来检查策略的稳定性。此方法通用，适用于广泛的控制策略。 |
| [^219] | [Resolution Limits of Non-Adaptive 20 Questions Search for a Moving Target.](http://arxiv.org/abs/2206.08884) | 本文研究了在未知初始位置和速度下的移动目标的非自适应搜索策略，通过最少次数的查询预言机，准确估计目标在任何指定时间的位置的分辨率极限。 |
| [^220] | [On the Identifiability of Nonlinear ICA: Sparsity and Beyond.](http://arxiv.org/abs/2206.07751) | 本文提出一个新的方法，考虑混合过程的假设，即结构稀疏性，来实现非线性ICA的可识别性，无需辅助变量。 |
| [^221] | [StructCoder: Structure-Aware Transformer for Code Generation.](http://arxiv.org/abs/2206.05239) | 本文提出了一个结构感知的Transformer模型，通过引入AST和DFG辅助任务，旨在解决现有代码生成模型在面对代码语法和语义时的训练不足问题。 |
| [^222] | [Improving adversarial robustness by putting more regularizations on less robust samples.](http://arxiv.org/abs/2206.03353) | 本文提出了一种新的对抗训练算法，通过在容易受到对抗攻击的数据上施加更多正则化以提高对抗性鲁棒性，得到了在准确性和鲁棒性方面均为最优的表现。 |
| [^223] | [Static Scheduling with Predictions Learned through Efficient Exploration.](http://arxiv.org/abs/2205.15695) | 本文研究了单机作业调度的问题，提出了一种基于学习预测的静态调度算法，在类型未知的情况下实现了次线性的过剩成本，尤其在抢占式问题中表现出色，可以在不同作业类型持续时间相差很大时优于非抢占匹配。 |
| [^224] | [Constrained Monotonic Neural Networks.](http://arxiv.org/abs/2205.11775) | 本文针对实际应用场景需要的单调性，提出了一种通过在层中的一部分神经元中采用原始激活函数，同时在另一部分采用其点对称反射来解决构建单调深度神经网络的方法。实验证明，该方法的精度符合要求。 |
| [^225] | [MixFlows: principled variational inference via mixed flows.](http://arxiv.org/abs/2205.07475) | 本文提出了混合变分流（MixFlows），是由对初始参考分布的映射重复应用的混合组成的一种新的变分家族。MixFlows具有类似于MCMC的收敛保证，并可以提供比几种黑盒归一化流更可靠的后验逼近，与最先进的MCMC方法所获得的样本质量相当。 |
| [^226] | [A Law of Robustness beyond Isoperimetry.](http://arxiv.org/abs/2202.11592) | 本文提出了一种两层次的稳健性律法，研究了在任意数据分布下的稳健插值问题，并证明其Lipschitz下界分别为$\Omega(\sqrt{n/p})$和$\Omega(n^{1/d})$。 |
| [^227] | [Supervising the Multi-Fidelity Race of Hyperparameter Configurations.](http://arxiv.org/abs/2202.09774) | 本文介绍了一种贝叶斯优化方法DyHPO，它能够学习动态决定哪个超参数配置在所有可行的配置中进行下一步训练。实验表明DyHPO相比于最先进的超参数优化方法有显著的优越性。 |
| [^228] | [Neural Architecture Search for Energy Efficient Always-on Audio Models.](http://arxiv.org/abs/2202.05397) | 本研究提出对神经架构搜索的改进，能够优化网络在能效、内存使用和准确性上的表现。评估结果表明相比基线MobileNetV1/V2，搜索出的网络每个推理能量少一个数量级且内存占用小得多，同时略微提高了准确率。 |
| [^229] | [Adaptive Client Sampling in Federated Learning via Online Learning with Bandit Feedback.](http://arxiv.org/abs/2112.14332) | 本文提出一种基于赌博反馈的在线学习算法，用于自适应选择哪些客户端用于联邦学习的训练，并通过理论证明该算法可提高优化算法的收敛速度。 |
| [^230] | [NN2Poly: A polynomial representation for deep feed-forward artificial neural networks.](http://arxiv.org/abs/2112.11397) | 本文提出了一种名为NN2Poly的方法，用于生成已经训练好的全连接前馈人工神经网络的精确多项式表示。该方法适用于任意深度的多层感知器（MLP）且计算成本相对较低，能够在回归和分类任务中提供非常准确的逼近结果。 |
| [^231] | [TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs.](http://arxiv.org/abs/2112.02052) | 本文提出了TC-GNN，这是第一个基于GPU张量核心单元（TCUs）的GNN加速框架。采用稀疏图翻译技术来协调“稀疏”GNN计算与高性能的“密集”TCUs，实现了GNN计算效率的提升。 |
| [^232] | [Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities.](http://arxiv.org/abs/2111.08851) | 本文提出新的一种排除限制的排序一致性序回归方法，基于深度神经网络和条件概率的建模，可以在保持输出概率排序一致性的同时，提高了模型的表现。 |
| [^233] | [Extracting Dynamical Models from Data.](http://arxiv.org/abs/2110.06917) | 本文介绍了一种使用机器学习从数据中获得确切动力学信息的方法，并命名为“FJet”。这种方法在谐振子、摆和达芬奇振荡器等示例中被证明可以准确地复制动力学并恢复潜在的微分方程。 |
| [^234] | [Finding Materialized Models for Model Reuse.](http://arxiv.org/abs/2110.06532) | 本文提出了无需源数据，通用，高效且有效的物化模型查询框架\textsf{MMQ}，使用基于高斯混合模型的分离度对物化模型进行排序以测量相关知识。 |
| [^235] | [On Tilted Losses in Machine Learning: Theory and Applications.](http://arxiv.org/abs/2109.06141) | 本文研究了一种在机器学习中不常见但在统计、概率与优化等领域常用的技术——指数倾斜，并将其应用到风险最小化中。所提出的方法可以修正单个损失的影响，增加或减少异常值的作用，可以提高泛化性能，并可以被视为损失的尾部概率的平滑近似。 |
| [^236] | [Disentangled Generative Models for Robust Prediction of System Dynamics.](http://arxiv.org/abs/2108.11684) | 本文研究了解缠生成模型用于预测系统动力学，将领域参数与生成模型的潜在空间中的动力学分离，取得了良好的泛化和长期稳定性。 |
| [^237] | [Model Transferability With Responsive Decision Subjects.](http://arxiv.org/abs/2107.05911) | 本论文研究在响应式和交互式数据分布下，算法预测器的可迁移性问题，提供了性能差距的上界和分类器必须满足的权衡的下界，并刻画了预测器、策略空间选择和偏移性质对可迁移性程度的影响。 |
| [^238] | [Learning Runtime Decisions for Adaptive Real-Time Perception.](http://arxiv.org/abs/2106.05665) | Chanakya是一种学习近似执行框架，通过在实时感知中平衡精度和延迟的训练，自动学习权衡的决策，并在多个感知任务中取得了最先进的性能。 |
| [^239] | [Near Optimal Adversarial Attack on UCB Bandits.](http://arxiv.org/abs/2008.09312) | 本文提出了一种在对抗攻击下的UCB最优拉臂策略，成本为$\sqrt{\log T}$，并且证明了此攻击策略近乎是最优的。 |
| [^240] | [Graph Clustering with Graph Neural Networks.](http://arxiv.org/abs/2006.16904) | 本文探讨了图神经网络在图聚类这一无监督问题上的缺陷，并提出了一种名为DMoN的无监督汇聚方法，可以有效地解决聚类恢复问题。 |
| [^241] | [Code Prediction by Feeding Trees to Transformers.](http://arxiv.org/abs/2003.13848) | 本文使用Transformer架构进行代码预测，超越了先前的模型，通过多种方式将代码结构传达给Transformer，进一步提高了准确度。 |
| [^242] | [Learning representations of irregular particle-detector geometry with distance-weighted graph networks.](http://arxiv.org/abs/1902.07987) | 该论文介绍了使用距离加权图网络处理不规则几何形状探测器的方法，利用其表示学习能力并可以处理事件的稀疏性和任意复杂的探测器几何形状。该算法为处理不规则探测器几何形状的粒子重建提供了一个有趣的替代方案。 |

# 详细

[^1]: 自监督语音表示域适应微调的自动数据增强

    Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations. (arXiv:2306.00481v1 [eess.AS])

    [http://arxiv.org/abs/2306.00481](http://arxiv.org/abs/2306.00481)

    该论文介绍了一种专门针对声学领域不匹配情况下的自动数据增强方法来解决语音自监督学习的表现问题，并在低资源领域中比基线展现出更好的性能。

    

    自监督学习（SSL）使得利用大量未标记的语音数据来提高语音识别模型的性能成为可能，即使是只有少量有注释的数据集也能做到。然而，语音SSL表示可能会在预训练和目标数据集之间遇到声学不匹配。为了解决这个问题，我们提出了一种新的监督域适应方法，专门针对声学领域存在不匹配的情况。该方法包括对大型干净数据集应用经过适当校准的数据增强，使其更接近目标域，并将其用作初始微调阶段的一部分。增强通过针对目标数据集基于条件依赖性估计器的最小化自动选择。该方法通过控制失真进行了神谕实验并在两个业余收集的低资源领域进行了验证，在两种情况下均比基线展现出更好的性能。

    Self-Supervised Learning (SSL) has allowed leveraging large amounts of unlabeled speech data to improve the performance of speech recognition models even with small annotated datasets. Despite this, speech SSL representations may fail while facing an acoustic mismatch between the pretraining and target datasets. To address this issue, we propose a novel supervised domain adaptation method, designed for cases exhibiting such a mismatch in acoustic domains. It consists in applying properly calibrated data augmentations on a large clean dataset, bringing it closer to the target domain, and using it as part of an initial fine-tuning stage. Augmentations are automatically selected through the minimization of a conditional-dependence estimator, based on the target dataset. The approach is validated during an oracle experiment with controlled distortions and on two amateur-collected low-resource domains, reaching better performances compared to the baselines in both cases.
    
[^2]: 使预训练模型具有可逆性：从参数到内存高效的微调

    Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])

    [http://arxiv.org/abs/2306.00477](http://arxiv.org/abs/2306.00477)

    本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。

    

    预训练语言模型（PLM）的参数高效微调已经成为一种非常成功的方法，只需训练少量参数而不会降低性能，并随着PLM越来越大而成为事实上的学习范式。然而，现有的PEFT方法不具备内存效率，因为它们仍需要存储大部分中间激活值以便计算梯度，类似于微调。一个减少激活内存的有效方法是应用可逆模型，这样中间激活值就无需缓存，可以重新计算。然而，将PLM修改为它的可逆变体并进行PEFT并不是一件容易的事，因为可逆模型具有与当前发布的PLM不同的体系结构。本文首先调查现有PEFT方法成功的关键因素，认识到在初始化PEFT时保留PLM的起点是至关重要的。

    Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
    
[^3]: MindBigData 2023 MNIST-8B：史上最大的脑信号开放数据集

    MindBigData 2023 MNIST-8B The 8 billion datapoints Multimodal Dataset of Brain Signals. (arXiv:2306.00455v1 [cs.LG])

    [http://arxiv.org/abs/2306.00455](http://arxiv.org/abs/2306.00455)

    MindBigData 2023 MNIST-8B是史上最大的开放脑信号数据集，采用自定义设备复制了MNIST数据集中的全部70,000个数字，捕捉了受试者观看像素并听取数字标签时的大脑信号。

    

    MindBigData 2023 MNIST-8B是迄今为止（2023年6月1日），基于单个受试者的EEG信号创建的最大的大脑信号开放数据集，采用自定义的128通道设备，复制了Yaan LeCun等人的MNIST数据集中的全部70,000个数字。在受试者观看屏幕上的原始数字像素并同时听取真实标签的0至9数字时，捕捉了大脑信号。详细描述了数据、收集过程、硬件和软件，背景额外信息和其他相关数据集可以在我们先前的论文MindBigData 2022：大型脑信号数据集中找到。

    MindBigData 2023 MNIST-8B is the largest, to date (June 1st 2023), brain signals open dataset created for Machine Learning, based on EEG signals from a single subject captured using a custom 128 channels device, replicating the full 70,000 digits from Yaan LeCun et all MNIST dataset. The brain signals were captured while the subject was watching the pixels of the original digits one by one on a screen and listening at the same time to the spoken number 0 to 9 from the real label. The data, collection procedures, hardware and software created are described in detail, background extra information and other related datasets can be found at our previous paper MindBigData 2022: A Large Dataset of Brain Signals.
    
[^4]: 语音自监督表示基准测试：我们做得对吗？

    Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?. (arXiv:2306.00452v1 [eess.AS])

    [http://arxiv.org/abs/2306.00452](http://arxiv.org/abs/2306.00452)

    研究发现语音自监督表示的基准测试结果对解码器架构变化很敏感，而使用有限解码器进行基准测试可能会导致开发的模型过大。

    

    最近，自监督学习（SSL）使得利用大量未标记的语音信号数据集，在只有少量注释数据的情况下，在语音任务上达到了令人印象深刻的性能。大量提出的方法促进了基准测试的需求和崛起，该测试评估它们在一组探索语音信号的各个方面的下游任务上的性能。然而，虽然涉及的任务数量正在增长，但大多数任务都依赖于单个解码器，将冻结的SSL表示映射到下游标签。本文研究了这种基准测试结果对解码器架构变化的鲁棒性。有趣的是，改变下游解码器的结构会导致大多数任务排行榜的显着变化。令人担忧的是，我们的研究表明，使用有限的解码器进行基准测试可能会导致开发的SSL模型的大小不必要地增加。

    Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance on speech tasks using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models.
    
[^5]: 针对类内分布转移的过度遗忘：连续学习的脆弱性

    Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift. (arXiv:2306.00427v1 [cs.LG])

    [http://arxiv.org/abs/2306.00427](http://arxiv.org/abs/2306.00427)

    连续学习中存在一种特殊形式的灾难性遗忘——越界遗忘，当给定类别引入类内分布转移时，它会显着削弱该类别的连续学习方法的识别准确率。

    

    连续学习是让人工神经网络在开放环境中工作的重要技术。在联合学习中，人们已经知道由意图攻击或环境扰动引起的越界问题严重影响网络的泛化能力。在这项工作中，我们报告了连续学习设置中由越界问题引起的一种特殊形式的灾难性遗忘，我们将其称为越界遗忘（OODF）。在连续图像分类任务中，我们发现，针对给定类别，引入类内分布转移显着削弱了后续学习过程中该类别的连续学习方法的识别准确率。有趣的是，这种现象对于连续学习而言是特殊的，因为同样级别的分布转移只有微不足道的影响。

    Continual learning (CL) is an important technique to allow artificial neural networks to work in open environments. CL enables a system to learn new tasks without severe interference to its performance on old tasks, i.e., overcome the problems of catastrophic forgetting. In joint learning, it is well known that the out-of-distribution (OOD) problem caused by intentional attacks or environmental perturbations will severely impair the ability of networks to generalize. In this work, we reported a special form of catastrophic forgetting raised by the OOD problem in continual learning settings, and we named it out-of-distribution forgetting (OODF). In continual image classification tasks, we found that for a given category, introducing an intra-class distribution shift significantly impaired the recognition accuracy of CL methods for that category during subsequent learning. Interestingly, this phenomenon is special for CL as the same level of distribution shift had only negligible effects
    
[^6]: 高阶鲁棒张量幂方法的快速实现

    Faster Robust Tensor Power Method for Arbitrary Order. (arXiv:2306.00406v1 [cs.LG])

    [http://arxiv.org/abs/2306.00406](http://arxiv.org/abs/2306.00406)

    本文提出了一种新的张量幂方法，用于分解任意阶张量，与现有方法相比，它可以更高效地处理高阶张量。

    

    张量分解是处理高维数据的一种基本方法，其中张量幂方法 (TPM) 是张量分解中广泛使用的技术之一。本文提出了一种新的张量幂方法，用于分解任意阶张量，它克服了现有方法的局限性，这些方法通常局限于低阶张量（小于3）或需要对底层数据结构做出强烈的假设。我们应用了草图方法，并能够在幂p和维度n的张量上实现运行时间为$\widetilde{O}(n^{p-1})$。我们提供了详细的分析，适用于任何p阶张量，这在以前的工作中从未给出。

    Tensor decomposition is a fundamental method used in various areas to deal with high-dimensional data. \emph{Tensor power method} (TPM) is one of the widely-used techniques in the decomposition of tensors. This paper presents a novel tensor power method for decomposing arbitrary order tensors, which overcomes limitations of existing approaches that are often restricted to lower-order (less than $3$) tensors or require strong assumptions about the underlying data structure. We apply sketching method, and we are able to achieve the running time of $\widetilde{O}(n^{p-1})$, on the power $p$ and dimension $n$ tensor. We provide a detailed analysis for any $p$-th order tensor, which is never given in previous works.
    
[^7]: 锥机制: 层次感知注意力

    Coneheads: Hierarchy Aware Attention. (arXiv:2306.00392v1 [cs.LG])

    [http://arxiv.org/abs/2306.00392](http://arxiv.org/abs/2306.00392)

    “锥注意力”是点积注意力的替代方案，它通过基于双曲锥的层次结构联系数据点，明确建模了真实世界数据集的复杂结构属性，提高了任务级性能并实现了优于点积注意力的效果，而且参数量更少。

    

    注意力网络，如transformers，在许多领域已经实现了最先进的性能。这些网络严重依赖于点积注意力运算符，它通过取两个点的内积来计算它们之间的相似性。然而，内积不能明确地对真实世界数据集的复杂结构属性（如数据点之间的层次结构）进行建模。为了解决这个问题，我们引入了锥注意力，一种基于双曲锥的点积注意力的替代方案。锥注意力通过双曲锥定义的层次结构将两个点联系起来，直观地衡量了两个点的分歧，并给出了一个层次感知的相似度分数。我们在各种模型和任务上测试了锥注意力，并表明它在任务级性能上优于点积注意力和其他基线算法，并且能够以显著较少的参数匹配点积注意力。

    Attention networks such as transformers have achieved state-of-the-art performance in many domains. These networks rely heavily on the dot product attention operator, which computes the similarity between two points by taking their inner product. However, the inner product does not explicitly model the complex structural properties of real world datasets, such as hierarchies between data points. To remedy this, we introduce cone attention, a drop-in replacement for dot product attention based on hyperbolic entailment cones. Cone attention associates two points by the depth of their lowest common ancestor in a hierarchy defined by hyperbolic cones, which intuitively measures the divergence of two points and gives a hierarchy aware similarity score. We test cone attention on a wide variety of models and tasks and show that it improves task-level performance over dot product attention and other baselines, and is able to match dot-product attention with significantly fewer parameters. Our 
    
[^8]: 学习高斯混合表示用于张量时间序列预测

    Learning Gaussian Mixture Representations for Tensor Time Series Forecasting. (arXiv:2306.00390v1 [cs.LG])

    [http://arxiv.org/abs/2306.00390](http://arxiv.org/abs/2306.00390)

    本文提出了一种新的张量时间序列预测框架GMRL，它可以单独模拟时间、位置和源变量中所暗示的每个异构性组件，相比于最先进的基准方法，本文的方法显示出了优越性。

    

    张量时间序列（TTS）数据是高维空间中一维时间序列的一般化，是现实场景中万能的存在，特别是在涉及多源时空数据的监测系统中（例如交通需求和空气污染物）。与建模时间序列或多元时间序列相比，在最近几年已经受到广泛关注并取得了巨大进展的情况下，张量时间序列付出的努力较少。由于其高维和复杂的内部结构，正确处理张量时间序列是一个更具挑战性的任务。在本文中，我们开发了一种新的TTS预测框架，该框架旨在单独模拟时间、位置和源变量中所暗示的每个异构性组件。我们将此框架命名为GMRL，即高斯混合表示学习。在两个实际TTS数据集上的实验结果验证了我们的方法相对于最先进的基准方法的优越性。

    Tensor time series (TTS) data, a generalization of one-dimensional time series on a high-dimensional space, is ubiquitous in real-world scenarios, especially in monitoring systems involving multi-source spatio-temporal data (e.g., transportation demands and air pollutants). Compared to modeling time series or multivariate time series, which has received much attention and achieved tremendous progress in recent years, tensor time series has been paid less effort. Properly coping with the tensor time series is a much more challenging task, due to its high-dimensional and complex inner structure. In this paper, we develop a novel TTS forecasting framework, which seeks to individually model each heterogeneity component implied in the time, the location, and the source variables. We name this framework as GMRL, short for Gaussian Mixture Representation Learning. Experiment results on two real-world TTS datasets verify the superiority of our approach compared with the state-of-the-art baseli
    
[^9]: 基于校准倾向分数进行因果效应估计

    Calibrated Propensity Scores for Causal Effect Estimation. (arXiv:2306.00382v1 [stat.ME])

    [http://arxiv.org/abs/2306.00382](http://arxiv.org/abs/2306.00382)

    本研究提出基于校准倾向分数技术的因果效应估计方法，可通过简单的重新校准技术实现倾向分数模型的概率输出的校准，具有较为广泛的应用前景。

    

    倾向分数通常用于在估计治疗效果时平衡观测到的协变量。当倾向分数模型无法学习真实的治疗分配机制时，通过倾向分数加权获得的估计结果可能存在偏差。我们认为，学习到的倾向分数模型的概率输出应进行校准，即90%的预测治疗概率应对应90%的个体被分配到治疗组。我们提出了简单的重新校准技术来确保这一属性。我们研究了校准倾向分数模型的理论特性及其在无偏治疗效应估计中的作用。我们展示了在多项任务中使用校准倾向分数进行改进的因果效应估计，其中包括高维基因组关联研究，在应用于更简单的倾向分数模型时还可以降低计算要求。

    Propensity scores are commonly used to balance observed covariates while estimating treatment effects. Estimates obtained through propensity score weighing can be biased when the propensity score model cannot learn the true treatment assignment mechanism. We argue that the probabilistic output of a learned propensity score model should be calibrated, i.e. a predictive treatment probability of 90% should correspond to 90% of individuals being assigned the treatment group. We propose simple recalibration techniques to ensure this property. We investigate the theoretical properties of a calibrated propensity score model and its role in unbiased treatment effect estimation. We demonstrate improved causal effect estimation with calibrated propensity scores in several tasks including high-dimensional genome-wide association studies, where we also show reduced computational requirements when calibration is applied to simpler propensity score models.
    
[^10]: 更好的上下文使得代码语言模型更出色：以函数调用实参自动补全为例的案例研究

    Better Context Makes Better Code Language Models: A Case Study on Function Call Argument Completion. (arXiv:2306.00381v1 [cs.SE])

    [http://arxiv.org/abs/2306.00381](http://arxiv.org/abs/2306.00381)

    本文针对代码补全任务展开研究，提出了考虑局部和非局部上下文信息的方法，设计了一个新的Python包数据集，通过程序分析器提取了非局部信息，并证明了这种方法可以提高代码补全模型的性能。

    

    预训练的代码语言模型在程序合成方面取得了很大进展。然而，常见方法只考虑文件内的局部上下文，因此忽视了代码库其他部分及其外部依赖所施加的信息和限制。现有的代码补全基准测试也缺乏这种上下文。为解决这些限制，我们策划了一个包含完整项目及其依赖的、许可宽松的Python包的新数据集，并提供工具来辅助程序分析器提取非本地信息。我们接着关注函数调用实参自动补全任务，该任务需要预测函数调用的实参。我们展示了现有的代码补全模型不能很好地解决我们的自动补全任务。为更好地解决此任务，我们向程序分析器查询与给定函数调用相关的信息，并考虑在推理和训练期间将分析器结果提供给不同的代码补全模型的方法。我们的研究表明非局部上下文可以帮助提高代码补全模型的性能。

    Pretrained code language models have enabled great progress towards program synthesis. However, common approaches only consider in-file local context and thus miss information and constraints imposed by other parts of the codebase and its external dependencies. Existing code completion benchmarks also lack such context. To resolve these restrictions we curate a new dataset of permissively licensed Python packages that includes full projects and their dependencies and provide tools to extract non-local information with the help of program analyzers. We then focus on the task of function call argument completion which requires predicting the arguments to function calls. We show that existing code completion models do not yield good results on our completion task. To better solve this task, we query a program analyzer for information relevant to a given function call, and consider ways to provide the analyzer results to different code completion models during inference and training. Our e
    
[^11]: 针对电子商务属性的大规模生成式多模态属性提取

    Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes. (arXiv:2306.00379v1 [cs.CV])

    [http://arxiv.org/abs/2306.00379](http://arxiv.org/abs/2306.00379)

    本文提出了一个多模态生成式模型来解决电商商品属性识别中的标注问题，通过在文本与视觉特征中生成属性值，系统在三个数据集上优于现有最先进方法，提升了7.3％。

    

    电子商务网站（如亚马逊）在其产品页面上有大量的结构化和非结构化信息（文本和图片）。销售者往往不对其产品的属性（如颜色，尺码等）进行标注或标注有误。从既包含文本又包含图片的电子商务产品页面中自动识别这些属性值是一项具有挑战性的任务，尤其是当目录中未明确提及属性值时。本文提出了一个可扩展的解决方案，将属性提取问题作为问答任务，使用MXT（Multimodal Adaptation Gate，Xception网络和T5编码器-解码器的三个关键组件）解决。我们的系统包括一个生成模型，通过利用产品的文本和视觉特征（例如图像）来“生成”给定产品的属性值。我们展示了我们的系统在三个公开数据集上优于现有最先进方法，平均提高7.3％，并且可以在单个GPU上每秒提取12K个以上的属性。

    E-commerce websites (e.g. Amazon) have a plethora of structured and unstructured information (text and images) present on the product pages. Sellers often either don't label or mislabel values of the attributes (e.g. color, size etc.) for their products. Automatically identifying these attribute values from an eCommerce product page that contains both text and images is a challenging task, especially when the attribute value is not explicitly mentioned in the catalog. In this paper, we present a scalable solution for this problem where we pose attribute extraction problem as a question-answering task, which we solve using \textbf{MXT}, consisting of three key components: (i) \textbf{M}AG (Multimodal Adaptation Gate), (ii) \textbf{X}ception network, and (iii) \textbf{T}5 encoder-decoder. Our system consists of a generative model that \emph{generates} attribute-values for a given product by using both textual and visual characteristics (e.g. images) of the product. We show that our syste
    
[^12]: 图切换动力学系统

    Graph Switching Dynamical Systems. (arXiv:2306.00370v1 [cs.CV])

    [http://arxiv.org/abs/2306.00370](http://arxiv.org/abs/2306.00370)

    本论文提出一种新颖的基于图形的切换动力学系统方法，可以在涉及交互对象设置中自动发现模式并从时序数据中学习切换行为，这一方法在实验中表现出优越性能。

    

    具有复杂行为的动力学系统（例如免疫系统细胞与病原体的相互作用）通常通过将行为分为不同的模式或状态，并从中学习模式之间的切换行为来建模。切换动力学系统（SDS）是一种强大的工具，可以自动发现这些模式并从时序数据中学习切换行为。然而，这些方法主要关注独立对象，其中一个对象的模式与其他对象的模式无关。本文则侧重于更一般的相互作用对象设置，涉及切换动力学系统的交互对象，其中每个对象的动态也取决于另一个未知且动态更改的对象及其模式的子集。为此，我们提出了一种新颖的基于图形的切换动力学系统方法，名为“图切换动力学系统”（GRASS），其中我们使用动态图形来描述对象之间的交互和模式切换。我们提供了关于GRASS模型可识别性的理论结果，并开发了高效的推理算法。我们在模拟和实际数据集上展示了我们方法的优越性能。

    Dynamical systems with complex behaviours, e.g. immune system cells interacting with a pathogen, are commonly modelled by splitting the behaviour into different regimes, or modes, each with simpler dynamics, and then learning the switching behaviour from one mode to another. Switching Dynamical Systems (SDS) are a powerful tool that automatically discovers these modes and mode-switching behaviour from time series data. While effective, these methods focus on independent objects, where the modes of one object are independent of the modes of the other objects. In this paper, we focus on the more general interacting object setting for switching dynamical systems, where the per-object dynamics also depends on an unknown and dynamically changing subset of other objects and their modes. To this end, we propose a novel graph-based approach for switching dynamical systems, GRAph Switching dynamical Systems (GRASS), in which we use a dynamic graph to characterize interactions between objects an
    
[^13]: 关于一致性模型的等价性: 一致性模型，一致性扩散模型和Fokker-Planck正则化

    On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diffusion Models, and Fokker-Planck Regularization. (arXiv:2306.00367v1 [math.ST])

    [http://arxiv.org/abs/2306.00367](http://arxiv.org/abs/2306.00367)

    本研究建立了一致性模型，一致性扩散模型和Fokker-Planck正则化之间的理论联系，为不同目标增强扩散模型提供了更全面和包容性的框架的可能性。

    

    扩散模型中各种“一致性”概念的出现引起了很大关注，帮助改善了样本质量、似然估计和加速抽样。虽然在文献中类似的概念已经被提出，但它们之间的确切关系仍不清楚。在本研究中，我们建立了三个最近的“一致性”概念之间的理论联系，旨在为不同目标增强扩散模型。我们的见解为一致性类型模型提供了更全面和包容性的框架的可能性。

    The emergence of various notions of ``consistency'' in diffusion models has garnered considerable attention and helped achieve improved sample quality, likelihood estimation, and accelerated sampling. Although similar concepts have been proposed in the literature, the precise relationships among them remain unclear. In this study, we establish theoretical connections between three recent ``consistency'' notions designed to enhance diffusion models for distinct objectives. Our insights offer the potential for a more comprehensive and encompassing framework for consistency-type models.
    
[^14]: 基于分片的贝叶斯加性回归树

    Sharded Bayesian Additive Regression Trees. (arXiv:2306.00361v1 [stat.ML])

    [http://arxiv.org/abs/2306.00361](http://arxiv.org/abs/2306.00361)

    本文提出了一种基于分片的贝叶斯加性回归树模型，通过引入随机化辅助变量和分片树结构，采用贝叶斯加性回归树适配每个分区组件到一个子模型中进行数据分区，引入交集树结构来完全使用树结构指定分片和建模。研究中还推导了理论最优权重和证明了模型复杂度。

    

    本文提出了一种随机分片的贝叶斯加性回归树（SBT）模型。我们引入了一个随机化辅助变量和一个分片树来决定数据的分区，并使用贝叶斯加性回归树（BART）适配每个分区组件到一个子模型中。我们观察到，分片树的最优设计可以确定产品空间上子模型的最优分片，我们引入了一个交集树结构来完全使用树结构指定分片和建模。除了实验外，我们还推导了最小化后验收缩的理论最优权重，并证明了SBT的最坏情况复杂度。

    In this paper we develop the randomized Sharded Bayesian Additive Regression Trees (SBT) model. We introduce a randomization auxiliary variable and a sharding tree to decide partitioning of data, and fit each partition component to a sub-model using Bayesian Additive Regression Tree (BART). By observing that the optimal design of a sharding tree can determine optimal sharding for sub-models on a product space, we introduce an intersection tree structure to completely specify both the sharding and modeling using only tree structures. In addition to experiments, we also derive the theoretical optimal weights for minimizing posterior contractions and prove the worst-case complexity of SBT.
    
[^15]: ConvNets是如何理解图像亮度的？

    How Do ConvNets Understand Image Intensity?. (arXiv:2306.00360v1 [cs.CV])

    [http://arxiv.org/abs/2306.00360](http://arxiv.org/abs/2306.00360)

    该论文研究了ConvNets需要依赖图像亮度信息的情况，并通过可视化进行了展示。

    

    卷积神经网络(ConvNets)通常依赖边缘/形状信息来分类图像。过去十年间开发的可视化方法确认ConvNets依靠边缘信息。我们研究了ConvNets需要除形状信息外还需依赖图像亮度的情况。我们通过可视化展示了ConvNets依赖图像亮度信息。

    Convolutional Neural Networks (ConvNets) usually rely on edge/shape information to classify images. Visualization methods developed over the last decade confirm that ConvNets rely on edge information. We investigate situations where the ConvNet needs to rely on image intensity in addition to shape. We show that the ConvNet relies on image intensity information using visualization.
    
[^16]: 高效稳健的贝叶斯维数约减参数选择方法

    Efficient and Robust Bayesian Selection of Hyperparameters in Dimension Reduction for Visualization. (arXiv:2306.00357v1 [stat.ML])

    [http://arxiv.org/abs/2306.00357](http://arxiv.org/abs/2306.00357)

    本文提出了一种利用有代理模型的贝叶斯优化进行数据驱动敏感性分析和权衡多目标的维数约减参数选择方法，并在多个合成和现实数据集上进行评估，提供了一种稳健且高效的解决方案。

    

    我们提出了一种高效稳健的自动调参框架，在维数约减算法中选择超参数，注重大规模数据集和任意性能指标。通过利用有代理模型的贝叶斯优化，我们的方法使得超参数选择具有多目标权衡，并且允许我们进行数据驱动的敏感性分析。通过结合归一化和子采样，所提出的框架在可视化技术如t-SNE和UMAP中展现出了通用性和高效性。我们使用多个质量度量在各种合成和现实数据集上评估了我们的结果，为维数约减算法的超参数选择提供了一种稳健且高效的解决方案。

    We introduce an efficient and robust auto-tuning framework for hyperparameter selection in dimension reduction (DR) algorithms, focusing on large-scale datasets and arbitrary performance metrics. By leveraging Bayesian optimization (BO) with a surrogate model, our approach enables efficient hyperparameter selection with multi-objective trade-offs and allows us to perform data-driven sensitivity analysis. By incorporating normalization and subsampling, the proposed framework demonstrates versatility and efficiency, as shown in applications to visualization techniques such as t-SNE and UMAP. We evaluate our results on various synthetic and real-world datasets using multiple quality metrics, providing a robust and efficient solution for hyperparameter selection in DR algorithms.
    
[^17]: 解决扩散模型中的负迁移问题

    Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v1 [cs.CV])

    [http://arxiv.org/abs/2306.00354](http://arxiv.org/abs/2306.00354)

    本文从多任务学习角度出发，研究了扩散训练中的负迁移现象，提出了利用正则化技术增强扩散训练的方法，以减轻负迁移并提高去噪任务的性能。

    

    基于扩散的生成模型在各个领域都取得了显著的成功。它在同时涵盖不同噪声水平的去噪任务上训练模型，代表了一种多任务学习（MTL）的形式。然而，从MTL的角度分析和改善扩散模型仍然未被充分探索。特别地，MTL有时会导致众所周知的$\textit{负迁移}$现象，这种现象是由于任务之间存在冲突而导致某些任务的性能降低。本文旨在从MTL的角度分析扩散训练，提出了两个关键观察：$\textbf{(O1)}$ 随着噪声水平之间的差距加大，去噪任务之间的任务亲和力减弱， $\textbf{(O2)}$ 在扩散训练的背景下，负迁移也可能会出现。基于这些观察结果，我们的目标是通过减轻负迁移来增强扩散训练。为了实现这一目标，我们提出了利用现有的MTL方法、具体是正则化技术，来鼓励任务特定的特征提取并减少任务干扰。实验结果表明，我们提出的方法有效地减轻了负迁移，提高了扩散模型在一系列去噪任务上的性能。

    Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL metho
    
[^18]: 从概率角度构建语义感知的对抗样本

    Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective. (arXiv:2306.00353v1 [stat.ML])

    [http://arxiv.org/abs/2306.00353](http://arxiv.org/abs/2306.00353)

    本研究提出了一个基于概率视角的对抗样本构建方法，可以生成语义感知的对抗性样本，并可以有效规避传统对抗性攻击的强化对抗训练方法。

    

    本研究提出了一种新颖的概率视角对抗样本构建方法——箱约束 Langevin Monte Carlo（LMC）。从这个角度出发，我们开发了一种创新性的方法，以原则性的方式生成语义感知的对抗性样本。这种方法超越了几何距离所施加的限制，选择了语义约束。我们的方法赋予了个体将其对语义的理解融入到模型中的能力。通过人类评估，我们验证了我们的语义感知的对抗样本保持其固有的含义。在 MNIST 和 SVHN 数据集上的实验结果表明，我们的语义感知的对抗样本可以有效地规避针对传统对抗性攻击的强健性对抗训练方法。

    In this study, we introduce a novel, probabilistic viewpoint on adversarial examples, achieved through box-constrained Langevin Monte Carlo (LMC). Proceeding from this perspective, we develop an innovative approach for generating semantics-aware adversarial examples in a principled manner. This methodology transcends the restriction imposed by geometric distance, instead opting for semantic constraints. Our approach empowers individuals to incorporate their personal comprehension of semantics into the model. Through human evaluation, we validate that our semantics-aware adversarial examples maintain their inherent meaning. Experimental findings on the MNIST and SVHN datasets demonstrate that our semantics-aware adversarial examples can effectively circumvent robust adversarial training methods tailored for traditional adversarial attacks.
    
[^19]: 改进机器学习的能量守恒下降法：理论与实践

    Improving Energy Conserving Descent for Machine Learning: Theory and Practice. (arXiv:2306.00352v1 [cs.LG])

    [http://arxiv.org/abs/2306.00352](http://arxiv.org/abs/2306.00352)

    这篇论文介绍了能量守恒下降（ECD）的理论，提出了基于梯度的优化算法ECDSep，能够处理凸优化问题和非凸优化问题，通过改进动态和混沌诱导元素，提高性能；在各种机器学习问题上与流行的优化方法进行了实证比较，发现在每个任务中具有竞争力或改进的性能。

    

    我们发展了能量守恒下降（ECD）的理论，并介绍了ECDSep，这是一种基于梯度的优化算法，能够处理凸优化问题和非凸优化问题。该方法基于新颖的ECD优化框架，通过适当混沌的能量守恒动力系统的物理演化来实现，使得即使对于无对称性的通用高维问题的结果分布也能进行分析控制，从而主导低损失。与以往的实现相比，我们利用理论控制来改进动态和诱导混沌的元素，提高性能，同时简化面向不同类别问题的优化算法的超参数调整。我们在各种机器学习问题上与流行的优化方法（如SGD，Adam和AdamW等）进行了实证比较，发现在每个任务中与它们中的最佳方法相比具有竞争力或改进的性能。我们确定了我们方法的局限性，并提出了未来研究的方向。

    We develop the theory of Energy Conserving Descent (ECD) and introduce ECDSep, a gradient-based optimization algorithm able to tackle convex and non-convex optimization problems. The method is based on the novel ECD framework of optimization as physical evolution of a suitable chaotic energy-conserving dynamical system, enabling analytic control of the distribution of results - dominated at low loss - even for generic high-dimensional problems with no symmetries. Compared to previous realizations of this idea, we exploit the theoretical control to improve both the dynamics and chaos-inducing elements, enhancing performance while simplifying the hyper-parameter tuning of the optimization algorithm targeted to different classes of problems. We empirically compare with popular optimization methods such as SGD, Adam and AdamW on a wide range of machine learning problems, finding competitive or improved performance compared to the best among them on each task. We identify limitations in our
    
[^20]: CALICO：用于BEV感知的自我监督相机-LiDAR对比预训练

    CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception. (arXiv:2306.00349v1 [cs.CV])

    [http://arxiv.org/abs/2306.00349](http://arxiv.org/abs/2306.00349)

    论文提出了一种新的多模式BEV感知自我监督表示学习框架CALICO，它将对比目标应用于LiDAR和相机骨干，证明其比现有模型更加高效有效，并在BEV感知任务中提供了显着的收益。

    

    在自动驾驶系统领域，感知是至关重要的，其中基于鸟瞰图（BEV）的架构最近已达到最先进的性能。自我监督表示学习的可取性源于注释2D和3D数据的昂贵和费力过程。尽管以前的研究已经研究了LiDAR和基于相机的3D物体检测的预训练方法，但缺少多模式BEV感知的统一预训练框架。在本研究中，我们介绍了CALICO，这是一个新颖的框架，它将对比目标应用于LiDAR和相机骨干。具体而言，CALICO包括两个阶段：点区域对比（PRC）和区域感知蒸馏（RAD）。PRC更好地平衡了对LiDAR模态的区域和场景级表示学习，并与现有方法相比提供了显着的性能改进。RAD有效地在我们的自我训练教师模型上实现了对比蒸馏。CALICO的高效和有效的预训练技术与以前的方法相比，在BEV感知任务中提供了显着的收益。

    Perception is crucial in the realm of autonomous driving systems, where bird's eye view (BEV)-based architectures have recently reached state-of-the-art performance. The desirability of self-supervised representation learning stems from the expensive and laborious process of annotating 2D and 3D data. Although previous research has investigated pretraining methods for both LiDAR and camera-based 3D object detection, a unified pretraining framework for multimodal BEV perception is missing. In this study, we introduce CALICO, a novel framework that applies contrastive objectives to both LiDAR and camera backbones. Specifically, CALICO incorporates two stages: point-region contrast (PRC) and region-aware distillation (RAD). PRC better balances the region- and scene-level representation learning on the LiDAR modality and offers significant performance improvement compared to existing methods. RAD effectively achieves contrastive distillation on our self-trained teacher model. CALICO's effi
    
[^21]: SemEval2023任务8中的CAISA：反事实数据增广用于减轻因果关系提取中的类别不平衡问题

    CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for Mitigating Class Imbalance in Causal Claim Identification. (arXiv:2306.00346v1 [cs.CL])

    [http://arxiv.org/abs/2306.00346](http://arxiv.org/abs/2306.00346)

    该论文提出了一种针对类别不平衡问题的新颖反事实数据增广方法，该方法通过动词替换进行，用于医学因果关系提取，并且与其他3种数据增广技术相比较，在少数类方面有显著提高。

    

    类别不平衡问题会导致机器学习模型在少数类和整个数据集上产生不理想的表现。使用数据增广技术来增加样本数量是解决这个问题的一种方式。我们提出了一种新颖的通过动词替换进行反事实数据增广的方法，用于医学因果关系提取。此外，我们还研究了该方法的影响，并将其与其他3种数据增广技术进行了比较，结果显示该方法可以显著（相对）提高少数类的性能。

    The class imbalance problem can cause machine learning models to produce an undesirable performance on the minority class as well as the whole dataset. Using data augmentation techniques to increase the number of samples is one way to tackle this problem. We introduce a novel counterfactual data augmentation by verb replacement for the identification of medical claims. In addition, we investigate the impact of this method and compare it with 3 other data augmentation techniques, showing that the proposed method can result in a significant (relative) improvement in the minority class.
    
[^22]: BOtied: 带有相关多元等级的多目标贝叶斯优化

    BOtied: Multi-objective Bayesian optimization with tied multivariate ranks. (arXiv:2306.00344v1 [cs.LG])

    [http://arxiv.org/abs/2306.00344](http://arxiv.org/abs/2306.00344)

    BOtied 是一种带有相关多元等级的多目标贝叶斯优化算法，相较于现有的方法具有更高的样本效率和较好的近似质量。

    

    许多科学和工业应用需要同时优化多个潜在的相互冲突的目标。多目标贝叶斯优化 (MOBO) 是一种高效地识别 Pareto 最优解的框架。我们展示了无支配解和最高多元等级之间的自然联系，它与联合累积分布函数的最外层等高线重合。我们提出了 CDF indicator，这是一种 Pareto 合规的度量，用于评估近似 Pareto 集合的质量，它补充了流行的 hypervolume indicator。MOBO 的核心是采集函数，它通过导航目标之间的最佳折中来确定下一个要评估的候选项。 基于盒子分解目标空间的多目标采集函数（例如期望的 hypervolume 改进（EHVI）和熵搜索）在存在大量目标时的性能缩放很差。我们提出一种采集函数，称为 BOtied，它利用相关多元等级来高效搜索 Pareto frontier。我们的实验表明，BOtied 在样本效率和近似质量方面优于现有方法。

    Many scientific and industrial applications require joint optimization of multiple, potentially competing objectives. Multi-objective Bayesian optimization (MOBO) is a sample-efficient framework for identifying Pareto-optimal solutions. We show a natural connection between non-dominated solutions and the highest multivariate rank, which coincides with the outermost level line of the joint cumulative distribution function (CDF). We propose the CDF indicator, a Pareto-compliant metric for evaluating the quality of approximate Pareto sets that complements the popular hypervolume indicator. At the heart of MOBO is the acquisition function, which determines the next candidate to evaluate by navigating the best compromises among the objectives. Multi-objective acquisition functions that rely on box decomposition of the objective space, such as the expected hypervolume improvement (EHVI) and entropy search, scale poorly to a large number of objectives. We propose an acquisition function, call
    
[^23]: 结合显式和隐式正则化的深度网络高效学习

    Combining Explicit and Implicit Regularization for Efficient Learning in Deep Networks. (arXiv:2306.00342v1 [cs.LG])

    [http://arxiv.org/abs/2306.00342](http://arxiv.org/abs/2306.00342)

    本文提出了一种显式正则化方法，与隐式正则化结合，可以使单层网络实现与深度线性网络相当的低秩近似和泛化误差，使深度不再是学习的必要条件。

    

    隐式正则化研究优化过程中的梯度轨迹，以解释为什么深度网络更倾向于某些解决方案。在深度线性网络中，已经证明梯度下降隐式地朝向矩阵补全/因式分解任务上的低秩解决方案进行正则化。添加层数不仅可以提高这些任务的性能，而且作为一种加速的预处理方法进一步增强了这种低秩偏向。受此启发，我们提出一种显式惩罚来反映这种隐式偏差，只在某些自适应梯度优化器（例如Adam）起作用。这种组合可以使退化的单层网络实现与深度线性网络相当的低秩近似和泛化误差，使深度不再是学习的必要条件。单层网络还在一系列参数和数据集上能够表现优异，甚至超过了各种矩阵补全方法的表现。

    Works on implicit regularization have studied gradient trajectories during the optimization process to explain why deep networks favor certain kinds of solutions over others. In deep linear networks, it has been shown that gradient descent implicitly regularizes toward low-rank solutions on matrix completion/factorization tasks. Adding depth not only improves performance on these tasks but also acts as an accelerative pre-conditioning that further enhances this bias towards low-rankedness. Inspired by this, we propose an explicit penalty to mirror this implicit bias which only takes effect with certain adaptive gradient optimizers (e.g. Adam). This combination can enable a degenerate single-layer network to achieve low-rank approximations with generalization error comparable to deep linear networks, making depth no longer necessary for learning. The single-layer network also performs competitively or out-performs various approaches for matrix completion over a range of parameter and da
    
[^24]: 具有单调收益函数的最后一次切换依赖赌博机的规划问题的近似算法

    Last Switch Dependent Bandits with Monotone Payoff Functions. (arXiv:2306.00338v1 [cs.LG])

    [http://arxiv.org/abs/2306.00338](http://arxiv.org/abs/2306.00338)

    本文研究了最后一次切换依赖赌博机的规划问题，设计了第一个有效的常数近似算法，并在自然单调递增的情况下实现了近乎最优的性能保证。

    

    最近，Laforgue等人引入了最后一次切换依赖赌博机模型，以捕捉由玩家和环境之间的交互引起的非静态现象。本文针对计算最优拉臂策略（该问题是NP-hard）这一规划问题，研究了LSD（Last Switch Dependent）赌博机的近似性质。特别地，我们设计了第一个有效的常数近似算法，并证明，在收益自然单调递增的情况下，与特殊的充能赌博机（也称为延迟依赖赌博机）的最优性保证（几乎）相匹配。在此尝试中，我们为这类赌博机开发了新的工具和见解，这可能是独立感兴趣的。

    In a recent work, Laforgue et al. introduce the model of last switch dependent (LSD) bandits, in an attempt to capture nonstationary phenomena induced by the interaction between the player and the environment. Examples include satiation, where consecutive plays of the same action lead to decreased performance, or deprivation, where the payoff of an action increases after an interval of inactivity. In this work, we take a step towards understanding the approximability of planning LSD bandits, namely, the (NP-hard) problem of computing an optimal arm-pulling strategy under complete knowledge of the model. In particular, we design the first efficient constant approximation algorithm for the problem and show that, under a natural monotonicity assumption on the payoffs, its approximation guarantee (almost) matches the state-of-the-art for the special and well-studied class of recharging bandits (also known as delay-dependent). In this attempt, we develop new tools and insights for this clas
    
[^25]: 利用强化学习在多智能体马尔可夫决策过程中实现公平性

    Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning. (arXiv:2306.00324v1 [cs.LG])

    [http://arxiv.org/abs/2306.00324](http://arxiv.org/abs/2306.00324)

    本文利用强化学习方法在多智能体马尔可夫决策过程中实现公平性，引入公平函数来确保代理之间的奖励公平，并提出一种基于在线凸优化的方法获得策略。

    

    公平性在各种多智能体系统（例如通信网络、金融市场等）中都扮演着至关重要的角色。许多多智能体动态交互可以被视为马尔可夫决策过程（MDPs）。尽管现有研究专注于研究已知环境中的公平性，但是在未知环境中探索这种系统中的公平性仍然是开放的问题。在本文中，我们提出了一种强化学习（RL）方法，以实现多智能体有限时间段情节MDPs中的公平性。我们引入了一个公平函数，它确保代理之间的奖励公平，而不是最大化各个代理的值函数之和。由于当不最大化单个值函数之和时，经典的Bellman方程不再成立，因此我们不能使用传统的方法。相反，为了进行探索，我们维护未知环境的置信度界限，然后提出了一种在线凸优化方法来获得受限于此置信范围的策略。

    Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence 
    
[^26]: “思维克隆：通过模仿人类思维学习思考并行动”。（arXiv:2306.00323v1 [cs.AI]）

    Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v1 [cs.AI])

    [http://arxiv.org/abs/2306.00323](http://arxiv.org/abs/2306.00323)

    本论文提出了一种新的模仿学习框架“思维克隆”，通过学习人类的思维来训练AI代理，以在泛化、探索、规划等能力方面实现更好的表现。

    

    语言通常被认为是人类思维的一个关键方面，它为我们提供了非凡的泛化、探索、规划、重新规划和适应新情况的能力。然而，强化学习（RL）代理在这些能力中远未达到人类水平的表现。我们假设其中一个认知缺陷的原因是他们缺乏使用语言思考所带来的好处。我们认为通过训练AI代理人像人类一样思考，可以改善其性能。我们引入了一种新的模仿学习框架“思维克隆”，其想法不仅是克隆人类示范者的行为，而且还包括人类在执行这些行为时所产生的想法。虽然我们希望“思维克隆”在处理网络规模的人类思维和行为数据时能够发挥出色（例如，带有剧本的在线视频），但在这里，我们进行了在思考和行动数据为合成生成的领域的实验。结果显示，“思维克隆”学习速度比传统的强化学习方法快得多。

    Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than
    
[^27]: 通过启发式策略混合改进离线强化学习

    Improving Offline RL by Blending Heuristics. (arXiv:2306.00321v1 [cs.LG])

    [http://arxiv.org/abs/2306.00321](http://arxiv.org/abs/2306.00321)

    HUBL是一种用于基于值函数回溯的广泛类离线强化学习算法的简单性能提升技术。我们证明了HUBL可通过调整奖励和折扣因子来简单实现，并且实验结果表明HUBL能够在提高性能的同时降低复杂度。

    

    我们提出了启发式策略混合（HUBL），一种用于基于值函数回溯的广泛类离线强化学习算法的简单性能提升技术。HUBL修改了这些算法中使用的Bellman操作符，部分用启发式的蒙特卡罗回报替换了值函数回溯。对于回报更高的轨迹，HUBL更多地依赖于启发式，较少依赖于值函数回溯。否则，它会更加倚重于值函数回溯。我们证明了这个想法可以通过调整奖励和折扣因子来简单实现，使HUBL可适用于现有的许多离线强化学习算法。我们从理论上证明HUBL降低了离线强化学习的复杂度，从而提高了其有限样本的性能。此外，我们还从实证方面表明了HUBL对四种最先进的基于回溯的离线强化学习算法的策略质量进行了改进，平均提高了9%的效果，在D4RL和Meta-Wo的27个数据集上表现稳定。

    We propose Heuristic Blending (HUBL), a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping. HUBL modifies Bellman operators used in these algorithms, partially replacing the bootstrapped values with Monte-Carlo returns as heuristics. For trajectories with higher returns, HUBL relies more on heuristics and less on bootstrapping; otherwise, it leans more heavily on bootstrapping. We show that this idea can be easily implemented by relabeling the offline datasets with adjusted rewards and discount factors, making HUBL readily usable by many existing offline RL implementations. We theoretically prove that HUBL reduces offline RL's complexity and thus improves its finite-sample performance. Furthermore, we empirically demonstrate that HUBL consistently improves the policy quality of four state-of-the-art bootstrapping-based offline RL algorithms (ATAC, CQL, TD3+BC, and IQL), by 9% on average over 27 datasets of the D4RL and Meta-Wo
    
[^28]: 基于元素除法的可学习舍入用于后训练量化

    FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization. (arXiv:2306.00317v1 [cs.LG])

    [http://arxiv.org/abs/2306.00317](http://arxiv.org/abs/2306.00317)

    该论文提出了一种新的基于元素除法的可学习舍入机制FlexRound，使得后训练量化时更好地重构每个层或块的输出，并且能够学习公共量化网格大小以及每个预训练权重的不同比例尺。

    

    后训练量化（PTQ）已经在资源有限设备上部署深度神经网络方面越来越受欢迎，因为与量化感知培训不同，完全不需要全面的训练数据集或端到端培训。因为基于重构每个层或块输出的PTQ方案效果显着以增强量化模型性能，所以最近的研究已经开发了算法来设计和学习一种新的权重舍入方案，以更好地重构每个层或块的输出。在这项工作中，我们提出了一种简单而有效的新的PTQ权重舍入机制，名为FlexRound，其基于元素除法而不是典型的元素加法，从而使FlexRound能够同时学习公共量化网格大小以及每个预训练权重的不同比例尺。由于元素除法产生的导数的互补规则，FlexRound在更新其相关预训练权重时天生能够利用它们。

    Post-training quantization (PTQ) has been gaining popularity for the deployment of deep neural networks on resource-limited devices since unlike quantization-aware training, neither a full training dataset nor end-to-end training is required at all. As PTQ schemes based on reconstructing each layer or block output turn out to be effective to enhance quantized model performance, recent works have developed algorithms to devise and learn a new weight-rounding scheme so as to better reconstruct each layer or block output. In this work, we propose a simple yet effective new weight-rounding mechanism for PTQ, coined FlexRound, based on element-wise division instead of typical element-wise addition such that FlexRound enables jointly learning a common quantization grid size as well as a different scale for each pre-trained weight. Thanks to the reciprocal rule of derivatives induced by element-wise division, FlexRound is inherently able to exploit pre-trained weights when updating their corr
    
[^29]: 通过不一致性偏差推导出分布转移下的(几乎)可证明误差界限

    (Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy. (arXiv:2306.00312v1 [stat.ML])

    [http://arxiv.org/abs/2306.00312](http://arxiv.org/abs/2306.00312)

    本论文提出了一种基于不一致性偏差的方法，通过一个简单、直观的条件推导出深度神经网络在分布转移下的(几乎)可证明误差界限，相比于$\mathcal{H}\Delta\mathcal{H}$-divergence更紧密、更易评价。

    

    本文使用未标记的测试数据，推导出深度神经网络在分布转换下误差的(几乎)保证上限。我们的方法需要一个简单、直观的条件，由先前的经验研究很好地证明且在实际操作中有效率地满足，而且相比于$\mathcal{H}\Delta\mathcal{H}$-divergence更容易评价且保证更紧密、更不虚伪。

    We derive an (almost) guaranteed upper bound on the error of deep neural networks under distribution shift using unlabeled test data. Prior methods either give bounds that are vacuous in practice or give estimates that are accurate on average but heavily underestimate error for a sizeable fraction of shifts. In particular, the latter only give guarantees based on complex continuous measures such as test calibration -- which cannot be identified without labels -- and are therefore unreliable. Instead, our bound requires a simple, intuitive condition which is well justified by prior empirical works and holds in practice effectively 100% of the time. The bound is inspired by $\mathcal{H}\Delta\mathcal{H}$-divergence but is easier to evaluate and substantially tighter, consistently providing non-vacuous guarantees. Estimating the bound requires optimizing one multiclass classifier to disagree with another, for which some prior works have used sub-optimal proxy losses; we devise a "disagree
    
[^30]: CapText: 基于大型语言模型的图像内容和描述生成字幕

    CapText: Large Language Model-based Caption Generation From Image Context and Description. (arXiv:2306.00301v1 [cs.LG])

    [http://arxiv.org/abs/2306.00301](http://arxiv.org/abs/2306.00301)

    研究提出了一种基于大型语言模型的图像字幕生成方法，从文本描述和上下文中生成字幕，而不直接处理图像。在CIDEr指标上，优于当前最先进的图像文本对齐模型。

    

    尽管深度学习模型在图像到文本数据集上表现良好，但在实践中难以用于图像字幕生成，因为传统的图片字幕往往是与图像相关的，并且提供有关图像的补充信息，而模型往往生成描述图像视觉特征的“描述”。在字幕生成方面的研究已探索了使用模型在提供对应的描述或上下文信息的情况下生成字幕的方法。我们提出并评估了一种新的方法，该方法利用现有的大型语言模型从文本描述和上下文中生成字幕，而不直接处理图像。我们证明，在微调后，我们的方法在 CIDEr 指标上胜过了当前最先进的图像文本对齐模型，如 OSCAR-VinVL。

    While deep-learning models have been shown to perform well on image-to-text datasets, it is difficult to use them in practice for captioning images. This is because \textit{captions} traditionally tend to be context-dependent and offer complementary information about an image, while models tend to produce \textit{descriptions} that describe the visual features of the image. Prior research in caption generation has explored the use of models that generate captions when provided with the images alongside their respective descriptions or contexts. We propose and evaluate a new approach, which leverages existing large language models to generate captions from textual descriptions and context alone, without ever processing the image directly. We demonstrate that after fine-tuning, our approach outperforms current state-of-the-art image-text alignment models like OSCAR-VinVL on this task on the CIDEr metric.
    
[^31]: Transformers学习实现预处理梯度下降用于上下文学习

    Transformers learn to implement preconditioned gradient descent for in-context learning. (arXiv:2306.00297v1 [cs.LG])

    [http://arxiv.org/abs/2306.00297](http://arxiv.org/abs/2306.00297)

    本论文通过研究线性transformers在训练过程中的全局最小值，证明了对于一个注意力层，transformers能够实现一次预处理梯度下降，而对于一个$k$个注意力层的transformer，它可以实现多次预处理梯度下降。

    

    受到transformers在上下文学习方面的显著能力的驱动，一些研究表明transformers可以实现像梯度下降这样的算法。通过精心的权重构造，这些研究表明多层transformers具有足够的表达能力来模拟梯度下降迭代。超越表达能力的问题，我们问：transformers能否通过在随机问题实例上训练来学习实现这样的算法？据我们所知，通过对线性回归的随机实例进行训练，我们第一次在这个问题上取得了理论进展，通过对线性transformers的损失函数进行分析。对于一个注意力层，我们证明了训练目标的全局最小值实现了一次预处理梯度下降。值得注意的是，预处理矩阵不仅适应输入分布，而且还适应于数据不充分引起的方差。对于一个具有$k$个注意力层的transformer，我们证明了一定条件下它可以实现多次预处理梯度下降。

    Motivated by the striking ability of transformers for in-context learning, several works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate gradient descent iterations. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress toward this question via analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $k$ attention layers, we prove certain 
    
[^32]: 模拟共情过程的可解释建模架构EMOTE

    EMOTE: An Explainable architecture for Modelling the Other Through Empathy. (arXiv:2306.00295v1 [cs.AI])

    [http://arxiv.org/abs/2306.00295](http://arxiv.org/abs/2306.00295)

    EMOTE是一个可解释的建模架构，用于模拟另一个智能体的动作价值函数，通过共情的想象网络将其他智能体的观察状态转换成可解释的“共情状态”。该方法在多智能体游戏中取得了比现有方法更好的性能。

    

    我们通常可以假设他人与我们有类似的目标。这种假设有时也适用于多智能体游戏，例如，Agent 1对绿色颗粒的吸引类比于Agent 2对红色颗粒的吸引。这种“类比”假设与共情认知过程密切相关。受到共情的启发，我们设计了一个简单且可解释的建模架构，用于模拟另一个智能体的动作价值函数。这涉及学习一个“想象网络”，以转换其他智能体的观察状态，从而产生可解释的“共情状态”，当呈现给学习智能体时，会产生模仿其他智能体行为的行为。我们的方法适用于由单个学习智能体和根据固定策略行动的其他（独立）智能体组成的多智能体场景。该架构特别适用于使用复合值或奖励函数的算法。我们展示了我们的方法在具有挑战性的多智能体游戏环境中产生比现有方法更好的性能。

    We can usually assume others have goals analogous to our own. This assumption can also, at times, be applied to multi-agent games - e.g. Agent 1's attraction to green pellets is analogous to Agent 2's attraction to red pellets. This "analogy" assumption is tied closely to the cognitive process known as empathy. Inspired by empathy, we design a simple and explainable architecture to model another agent's action-value function. This involves learning an "Imagination Network" to transform the other agent's observed state in order to produce a human-interpretable "empathetic state" which, when presented to the learning agent, produces behaviours that mimic the other agent. Our approach is applicable to multi-agent scenarios consisting of a single learning agent and other (independent) agents acting according to fixed policies. This architecture is particularly beneficial for (but not limited to) algorithms using a composite value or reward function. We show our method produces better perfo
    
[^33]: 无训练神经网络结构搜索在RNN和Transformer上的应用

    Training-free Neural Architecture Search for RNNs and Transformers. (arXiv:2306.00288v1 [cs.LG])

    [http://arxiv.org/abs/2306.00288](http://arxiv.org/abs/2306.00288)

    本文研究了适用于语言模型任务的循环神经网络和Transformer的无训练神经网络结构搜索算法，并开发了一个新的性能预测指标hidden covariance，可显著优化现有指标。

    

    神经网络结构搜索(NAS)可以自动创建新的有效的神经网络架构，为手动设计复杂架构提供了一种选择。然而，传统的NAS算法速度较慢，需要大量计算力。最近的研究探讨了用于图像分类架构的无训练NAS指标，极大地加速了搜索算法。本文研究针对语言建模任务的循环神经网络(RNN)和BERT-based transformer架构的无训练NAS指标。首先，我们开发了一个名为hidden covariance的新的无训练指标，可以预测训练后RNN架构的性能，并显著优于现有的无训练指标。其次，我们发现目前的转换器搜索空间范例

    Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS metrics for recurrent neural network (RNN) and BERT-based transformer architectures, targeted towards language modeling tasks. First, we develop a new training-free metric, named hidden covariance, that predicts the trained performance of an RNN architecture and significantly outperforms existing training-free metrics. We experimentally evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP benchmark. Second, we find that the current search space paradigm for transformer
    
[^34]: 基于案例研究的量子机器学习在网络安全中的应用：利用量子支持向量机进行恶意软件分类和保护

    Case Study-Based Approach of Quantum Machine Learning in Cybersecurity: Quantum Support Vector Machine for Malware Classification and Protection. (arXiv:2306.00284v1 [cs.CR])

    [http://arxiv.org/abs/2306.00284](http://arxiv.org/abs/2306.00284)

    本文介绍了基于案例研究的方法，将量子机器学习应用于网络安全领域，通过10个学习模块覆盖各种网络安全主题。该研究旨在提供实践机会以解决实际安全问题，激发和鼓励学生学习。

    

    量子机器学习（QML）是利用量子计算改进经典机器学习方法以解决复杂实际问题的新兴研究领域。QML有潜力应对网络安全相关的挑战。由于QML的新颖性和复杂性架构，目前尚未明确提供资源以使网络安全学习者具备关于这一新兴技术的有效知识。本研究采用以学生为中心的案例研究学习方法，设计和开发了基于QML的包含10个学习模块，涵盖各种网络安全主题的课程。我们将QML的一个子主题应用于网络安全主题，并通过实验室前、实验室和实验室后的活动，为学习者提供在解决现实安全问题方面实践QML的机会。为了在鼓励所有学生学习的学习环境中吸引和激励学生，实验室前还提供了关于QML子主题和网络安全主题的简短介绍。

    Quantum machine learning (QML) is an emerging field of research that leverages quantum computing to improve the classical machine learning approach to solve complex real world problems. QML has the potential to address cybersecurity related challenges. Considering the novelty and complex architecture of QML, resources are not yet explicitly available that can pave cybersecurity learners to instill efficient knowledge of this emerging technology. In this research, we design and develop QML-based ten learning modules covering various cybersecurity topics by adopting student centering case-study based learning approach. We apply one subtopic of QML on a cybersecurity topic comprised of pre-lab, lab, and post-lab activities towards providing learners with hands-on QML experiences in solving real-world security problems. In order to engage and motivate students in a learning environment that encourages all students to learn, pre-lab offers a brief introduction to both the QML subtopic and c
    
[^35]: 面向代表性不足音乐生成的迁移学习

    Transfer Learning for Underrepresented Music Generation. (arXiv:2306.00281v1 [cs.LG])

    [http://arxiv.org/abs/2306.00281](http://arxiv.org/abs/2306.00281)

    本文介绍了一种组合创造性方法进行迁移学习，可以使深度神经网络在分布外流派上的性能得到提高，以伊朗民间音乐为例进行了实验，结果表明未来可能通过这种方法实现代表性不足音乐流派的生成。

    

    本文研究了一种组合创造性方法来进行迁移学习，以提高基于深度神经网络的音乐生成模型在分布外（OOD）流派上的性能。我们以伊朗民间音乐为例，这是 MusicVAE，一个大型生成音乐模型，无法处理的OOD流派之一。我们发现，组合创造性的迁移学习方法可以有效地将MusicVAE调整为伊朗民间音乐数据集，这表明未来可以生成代表性不足的音乐流派。

    This paper investigates a combinational creativity approach to transfer learning to improve the performance of deep neural network-based models for music generation on out-of-distribution (OOD) genres. We identify Iranian folk music as an example of such an OOD genre for MusicVAE, a large generative music model. We find that a combinational creativity transfer learning approach can efficiently adapt MusicVAE to an Iranian folk music dataset, indicating potential for generating underrepresented music genres in the future.
    
[^36]: 面向非均匀和时变通信的FedAvg偏差修正研究

    Towards Bias Correction of FedAvg over Nonuniform and Time-Varying Communications. (arXiv:2306.00280v1 [cs.LG])

    [http://arxiv.org/abs/2306.00280](http://arxiv.org/abs/2306.00280)

    本文面向非均匀和时变通信故障问题，提出了一种名为FedPBC的联邦学习算法来修正FedAvg的偏差，该算法构建二叉树来减轻非均匀通信故障的影响，并在非凸任务上实验结果表现优于FedAvg。

    

    联邦学习是一种分散的学习框架，在这个框架下，参数服务器（PS）和多个客户端合作通过最小化全局目标来训练模型。通信带宽是一种稀缺资源；在每轮迭代中，PS仅从客户端的子集聚合更新。本文关注的是非凸优化问题，这种问题容易受到PS和客户端之间不均匀、时变的通信故障的影响。具体而言，在每轮$t$中，PS和客户端$i$之间的连接只有概率$p_i^t$是活跃的，而这个概率是不知道的。当信道条件在客户端之间异构并且随时间变化时，这种情况会发生。我们发现当$p_i^t$非均匀时，被广泛采用的FL算法FedAvg无法最小化全局目标。鉴于此，我们提出了一种名为FedPBC的简单有效的方法来减轻非均匀通信故障的影响。FedPBC在客户端之间构建二叉树，在建立拓扑结构之前推迟更新的传输，并捕捉本地更新的重要性。我们在温和假设下理论证明了FedPBC收敛于非凸问题的全局最优解。在非凸任务（如图像分类和语言建模）上的实验结果表明，FedPBC在非均匀和时变的通信设置下优于FedAvg。

    Federated learning (FL) is a decentralized learning framework wherein a parameter server (PS) and a collection of clients collaboratively train a model via minimizing a global objective. Communication bandwidth is a scarce resource; in each round, the PS aggregates the updates from a subset of clients only. In this paper, we focus on non-convex minimization that is vulnerable to non-uniform and time-varying communication failures between the PS and the clients. Specifically, in each round $t$, the link between the PS and client $i$ is active with probability $p_i^t$, which is $\textit{unknown}$ to both the PS and the clients. This arises when the channel conditions are heterogeneous across clients and are changing over time.  We show that when the $p_i^t$'s are not uniform, $\textit{Federated Average}$ (FedAvg) -- the most widely adopted FL algorithm -- fails to minimize the global objective. Observing this, we propose $\textit{Federated Postponed Broadcast}$ (FedPBC) which is a simple
    
[^37]: Mixup在寻找最佳决策边界中的可证实益处

    Provable Benefit of Mixup for Finding Optimal Decision Boundaries. (arXiv:2306.00267v1 [cs.LG])

    [http://arxiv.org/abs/2306.00267](http://arxiv.org/abs/2306.00267)

    本研究证明了使用Mixup训练具有可证实的益处，可以显著降低在更可分离数据分布中寻找最佳决策边界的样本复杂度。

    

    本文研究了像Mixup这样的成对数据增强技术如何影响在二元线性分类问题中寻找最佳决策边界的样本复杂度。针对一类具有可分离常数$\kappa$的数据分布，我们分析了训练损失最优分类器与测试准确率最优分类器（即贝叶斯最优分类器）之间的对齐程度。对于没有增强的普通训练，我们发现了一种有趣的现象，称为可分离性的诅咒。随着我们增加$\kappa$使数据分布更加可分离，普通训练的样本复杂度会在$\kappa$中呈指数增长。也许更令人惊讶的是，对于更可分离的数据分布而言，寻找最佳决策边界的任务变得更加困难。针对Mixup训练，我们展示了Mixup减轻了这个问题，通过显著降低样本复杂度。为此，我们开发了适用于Mixup考虑的$n^2$成对增强数据点的新的集中结果。我们的结果提供了关于Mixup的泛化益处的可证保证，并提供了理解Mixup为什么在实践中表现良好的见解。

    We investigate how pair-wise data augmentation techniques like Mixup affect the sample complexity of finding optimal decision boundaries in a binary linear classification problem. For a family of data distributions with a separability constant $\kappa$, we analyze how well the optimal classifier in terms of training loss aligns with the optimal one in test accuracy (i.e., Bayes optimal classifier). For vanilla training without augmentation, we uncover an interesting phenomenon named the curse of separability. As we increase $\kappa$ to make the data distribution more separable, the sample complexity of vanilla training increases exponentially in $\kappa$; perhaps surprisingly, the task of finding optimal decision boundaries becomes harder for more separable distributions. For Mixup training, we show that Mixup mitigates this problem by significantly reducing the sample complexity. To this end, we develop new concentration results applicable to $n^2$ pair-wise augmented data points cons
    
[^38]: 双重稳健自我训练

    Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])

    [http://arxiv.org/abs/2306.00265](http://arxiv.org/abs/2306.00265)

    本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。

    

    自我训练是解决半监督学习问题的一种重要技术。它通过生成伪标签并将其与有限的标记数据集结合使用进行训练，从而利用无标签数据。自我训练的有效性在很大程度上依赖于这些伪标签的准确性。本文引入了双重稳健自我训练，这是一种新颖的半监督算法，可以保证在两个极端之间平衡。当伪标签完全不正确时，我们的方法将被减少到仅使用标记数据进行训练。相反，当伪标签完全准确时，我们的方法将变成利用所有伪标签数据和标记数据进行训练的过程，从而增加有效的样本量。通过在ImageNet图像分类和nuScenes自主驾驶数据集上的实证评估，我们证明了双重稳健损失优于标准自我训练基线的优越性。

    Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
    
[^39]: 面向科学机器学习的基础模型：特征比较与迁移性能

    Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior. (arXiv:2306.00258v1 [cs.LG])

    [http://arxiv.org/abs/2306.00258](http://arxiv.org/abs/2306.00258)

    本研究研究了预训练模型在科学机器学习中的迁移性能，发现适当微调迁移学习可以代替从头开始训练，达到更高的准确性水平，而且减少了许多数据集的使用。

    

    预训练的机器学习（ML）模型在自然语言处理（NLP）和计算机视觉（CV）等各种应用中表现出巨大的性能。在这里，我们研究了预训练模型如何在科学机器学习（SciML）应用中应用，特别是在迁移学习的上下文中。我们研究了这些模型的迁移特性，包括（i）预训练模型大小的缩放，（ii）下游训练数据集大小的缩放，（iii）系统地将物理参数推出分布，以及（iv）如何将单个在不同物理问题的混合物上预训练的模型适应于各种下游应用。我们发现，适当地微调迁移学习可以帮助在比从头开始训练少几个数量级的下游示例（甚至可以是分布之外的不同任务）达到所需的准确性水平，并且在各种下游示例中具有一致的行为。

    Pre-trained machine learning (ML) models have shown great performance for a wide range of applications, in particular in natural language processing (NLP) and computer vision (CV). Here, we study how pre-training could be used for scientific machine learning (SciML) applications, specifically in the context of transfer learning. We study the transfer behavior of these models as (i) the pre-trained model size is scaled, (ii) the downstream training dataset size is scaled, (iii) the physics parameters are systematically pushed out of distribution, and (iv) how a single model pre-trained on a mixture of different physics problems can be adapted to various downstream applications. We find that-when fine-tuned appropriately-transfer learning can help reach desired accuracy levels with orders of magnitude fewer downstream examples (across different tasks that can even be out-of-distribution) than training from scratch, with consistent behavior across a wide range of downstream examples. We a
    
[^40]: DSGD-CECA: 具有通信优化精确共识算法的分散式 SGD

    DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm. (arXiv:2306.00256v1 [cs.LG])

    [http://arxiv.org/abs/2306.00256](http://arxiv.org/abs/2306.00256)

    DSGD-CECA 提出了一种通信优化的精确共识算法，使分散式 SGD 能够在没有2的幂次方限制的情况下实现收敛，在大规模时更为实用。

    

    分散式 SGD 是一种新兴的神经网络训练方法，它使多个代理能够协作地并行地训练模型。与使用中央参数服务器从所有代理收集梯度不同，每个代理保留模型参数的副本并与少量其他代理通信以交换模型更新。他们的通信由通信拓扑和八卦权重矩阵控制，促进模型更新的交换。最先进的方法使用动态的指数-2拓扑结构，实现比环形、网格、环面和超立方拓扑结构更快的训练时间和更好的可扩展性。然而，这种方法需要一个2的幂次方数量的代理，这在大规模时不切实际。在本文中，我们消除了这个限制，并提出了具有通信优化精确共识算法的分散式 SGD-CECA，利用这种算法在分散式环境下确保收敛性而不需要2的幂次方限制。我们证明，我们的方法在实现更实际的网络规模的同时，实现了与先进技术相当的训练时间和模型质量。

    Decentralized Stochastic Gradient Descent (SGD) is an emerging neural network training approach that enables multiple agents to train a model collaboratively and simultaneously. Rather than using a central parameter server to collect gradients from all the agents, each agent keeps a copy of the model parameters and communicates with a small number of other agents to exchange model updates. Their communication, governed by the communication topology and gossip weight matrices, facilitates the exchange of model updates. The state-of-the-art approach uses the dynamic one-peer exponential-2 topology, achieving faster training times and improved scalability than the ring, grid, torus, and hypercube topologies. However, this approach requires a power-of-2 number of agents, which is impractical at scale. In this paper, we remove this restriction and propose \underline{D}ecentralized \underline{SGD} with \underline{C}ommunication-optimal \underline{E}xact \underline{C}onsensus \underline{A}lgo
    
[^41]: 从像素到用户界面操作：通过图形用户界面学习遵循指令

    From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces. (arXiv:2306.00245v1 [cs.LG])

    [http://arxiv.org/abs/2306.00245](http://arxiv.org/abs/2306.00245)

    本文提出了一种基于像素级别的预训练方法，建立了一种模拟人类概念界面和混合动作空间的代理，实现了在GUI指令遵循任务的MiniWob++基准测试中超越人类工作者的目标。

    

    先前为了构建操作图形用户界面（GUI）的数字化代理，大多数工作都依赖基于文本的表示（从HTML或其他结构化数据源派生），这些表示并不总是容易获取。这些输入表示通常与自定义的任务特定动作空间相关联。本文旨在创建使用与人类通常使用的相同概念界面-通过基于像素的屏幕截图和对应于键盘和鼠标操作的通用动作空间与数字世界交互的代理。在近期关于像素级预训练方法的基础上，我们首次展示了这样的代理在GUI指令遵循任务的MiniWob ++基准测试中能够超越人类工作者。

    Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on text-based representations (derived from HTML or other structured data sources), which are not always readily available. These input representations have been often coupled with custom, task-specific action spaces. This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use -via pixel-based screenshots and a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following tasks.
    
[^42]: 组合神经臂的研究

    Combinatorial Neural Bandits. (arXiv:2306.00242v1 [stat.ML])

    [http://arxiv.org/abs/2306.00242](http://arxiv.org/abs/2306.00242)

    该论文提出了两个组合神经臂算法，通过使用深度神经网络来近似未知得分函数，这是处理类似问题的第一个算法框架。

    

    我们研究了一种上下文组合臂问题，其中学习代理在每一轮选择一组臂并根据其分数接收反馈。臂的得分是臂特征的未知函数。通过使用深度神经网络来近似这个未知的得分函数，我们提出了算法：组合神经UCB（CN-UCB）和组合神经汤普森抽样（CN-TS）。我们证明，CN-UCB可以达到$\tilde{\mathcal{O}}(\tilde{d} \sqrt{T})$或 $\tilde{\mathcal{O}}(\sqrt{\tilde{d} T K})$遗憾值，其中$\tilde{d}$是神经切向核矩阵的有效维度，$K$是一组臂的大小，$T$是时间跨度。对于CN-TS，我们采用一种乐观抽样技术来确保组合动作的乐观性，达到最差情况（频率学派）遗憾为$\tilde{\mathcal{O}}(\tilde{d} \sqrt{TK})$。据我们所知，这是第一个解决此类组合臂问题的算法框架。

    We consider a contextual combinatorial bandit problem where in each round a learning agent selects a subset of arms and receives feedback on the selected arms according to their scores. The score of an arm is an unknown function of the arm's feature. Approximating this unknown score function with deep neural networks, we propose algorithms: Combinatorial Neural UCB ($\texttt{CN-UCB}$) and Combinatorial Neural Thompson Sampling ($\texttt{CN-TS}$). We prove that $\texttt{CN-UCB}$ achieves $\tilde{\mathcal{O}}(\tilde{d} \sqrt{T})$ or $\tilde{\mathcal{O}}(\sqrt{\tilde{d} T K})$ regret, where $\tilde{d}$ is the effective dimension of a neural tangent kernel matrix, $K$ is the size of a subset of arms, and $T$ is the time horizon. For $\texttt{CN-TS}$, we adapt an optimistic sampling technique to ensure the optimism of the sampled combinatorial action, achieving a worst-case (frequentist) regret of $\tilde{\mathcal{O}}(\tilde{d} \sqrt{TK})$. To the best of our knowledge, these are the first 
    
[^43]: 物理信息神经网络在涡 shedding 中的预测局限性

    Predictive Limitations of Physics-Informed Neural Networks in Vortex Shedding. (arXiv:2306.00230v1 [cs.CE])

    [http://arxiv.org/abs/2306.00230](http://arxiv.org/abs/2306.00230)

    本文研究发现，物理信息神经网络（PINN）在涡 shedding 上预测的局限性较大，数据驱动型的PINN只有在训练数据可用时才能预测涡 shedding，且可能是由于谱偏差的原因，PINN会恢复到稳态解。

    

    最近对物理信息神经网络(PINN)方法的兴趣激增，导致了一系列研究证明了它们解决偏微分方程(PDE)和预测物理系统动力学的潜力。然而，PINN的预测局限性还没有得到深入的研究。本研究研究了2D圆柱周围的流动，发现无数据的PINN无法预测涡 shedding。数据驱动的PINN只有在训练数据(来自传统CFD求解器)可用时才显示涡 shedding，但当数据流停止时，它会恢复到稳态解。我们进行了动态模态分解，并分析了通过PINN和传统流体求解器(PetIBM)获得的解中的Koopman模式。Koopman特征值在复平面上的分布表明，PINN在数值上是展散和扩散的。 PINN方法可能是由于谱偏差，才恢复到稳态解。

    The recent surge of interest in physics-informed neural network (PINN) methods has led to a wave of studies that attest to their potential for solving partial differential equations (PDEs) and predicting the dynamics of physical systems. However, the predictive limitations of PINNs have not been thoroughly investigated. We look at the flow around a 2D cylinder and find that data-free PINNs are unable to predict vortex shedding. Data-driven PINN exhibits vortex shedding only while the training data (from a traditional CFD solver) is available, but reverts to the steady state solution when the data flow stops. We conducted dynamic mode decomposition and analyze the Koopman modes in the solutions obtained with PINNs versus a traditional fluid solver (PetIBM). The distribution of the Koopman eigenvalues on the complex plane suggests that PINN is numerically dispersive and diffusive. The PINN method reverts to the steady solution possibly as a consequence of spectral bias. This case study r
    
[^44]: 扩散画笔：基于潜在扩散模型的AI生成图像编辑工具

    Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])

    [http://arxiv.org/abs/2306.00219](http://arxiv.org/abs/2306.00219)

    本论文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地根据目标区域修改AI合成图像并保留原始上下文。与其他最先进的图像修复技术进行比较，该方法在用户研究中表现出更好的可用性和有效性。

    

    文本到图像的生成模型在生成高质量图像方面取得了显著的进展。然而，由于模型限制，生成的图像经常包含不良的伪影或其他错误。现有的微调生成图像的技术要么耗时（手动编辑），要么产生不够完美的结果（修补），要么会导致整体图像产生意想不到的变化（变体选择和提示微调）。本文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地微调AI合成图像中所需的区域。我们的方法在反向扩散过程中在目标区域引入了新的随机噪声模式，使模型能够在保留其他区域原始上下文的同时，高效地对指定区域进行更改。我们通过艺术家进行的用户研究评估了我们方法的可用性和有效性，将我们的技术与其他最先进的图像修复技术进行了比较。

    Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpaintin
    
[^45]: 多智能体强化学习中的安全性优化技术——泛化Lagrangian策略优化的可证明高效性

    Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning. (arXiv:2306.00212v1 [cs.LG])

    [http://arxiv.org/abs/2306.00212](http://arxiv.org/abs/2306.00212)

    本文研究了在有约束的马尔可夫博弈中，多个智能体通过最大化总体奖励并在对总体效用的期望值上设置约束条件进行多智能体强化学习。通过泛化Lagrangian策略优化，采用基于占用测量的方法，并利用置信上限强化学习算法，实现对约束的处理。

    

    本文研究在有约束的马尔科夫博弈中，互相竞争的多个智能体通过最大化总体奖励并在对总体效用的期望值上设置约束条件进行在线安全性多智能体强化学习。我们专注于一个拥有两个独立转移函数，对智能体未知的且存在对抗性奖励函数和随机效用函数的双人零和约束马尔可夫博弈。为了解决该马尔可夫博弈问题，我们采用基于占用测量的方法将其描述为带有显式约束的在线约束鞍点问题。我们将在约束优化中推广Lagrange乘数方法并创建一个带有Minimax决策原始变量和双变量的广义Lagrangian处理约束。接下来，我们开发了一个基于置信上限的强化学习算法来解决这个问题，实现探索和利用的平衡。我们的算法通过在线镜像更新Minimax决策原始变量。

    We examine online safe multi-agent reinforcement learning using constrained Markov games in which agents compete by maximizing their expected total rewards under a constraint on expected total utilities. Our focus is confined to an episodic two-player zero-sum constrained Markov game with independent transition functions that are unknown to agents, adversarial reward functions, and stochastic utility functions. For such a Markov game, we employ an approach based on the occupancy measure to formulate it as an online constrained saddle-point problem with an explicit constraint. We extend the Lagrange multiplier method in constrained optimization to handle the constraint by creating a generalized Lagrangian with minimax decision primal variables and a dual variable. Next, we develop an upper confidence reinforcement learning algorithm to solve this Lagrangian problem while balancing exploration and exploitation. Our algorithm updates the minimax decision primal variables via online mirror
    
[^46]: PERFOGRAPH：一种数值感知的程序图表示，用于性能优化和程序分析

    PERFOGRAPH: A Numerical Aware Program Graph Representation for Performance Optimization and Program Analysis. (arXiv:2306.00210v1 [cs.PL])

    [http://arxiv.org/abs/2306.00210](http://arxiv.org/abs/2306.00210)

    论文提出了一种新颖的基于图形的程序表示，称为PERFOGRAPH，它可以通过捕获数值信息和复合数据结构来克服现有程序表示的限制和挑战，是一种高度灵活和可扩展的表示形式。

    

    机器学习的显著增长和重要成功将其应用扩展到了编程语言和程序分析中。然而，采用最新的机器学习方法的一个关键挑战是编程语言的表示，这直接影响了机器学习方法关于程序的推理能力。以往的表示作品中缺乏数值意识、复合数据结构信息和变量表示不当等问题限制了它们的性能。为了克服当前程序表示的限制和挑战，我们提出了一种新颖的基于图形的程序表示，称为PERFOGRAPH。 PERFOGRAPH可以通过引入新节点和边来捕获数值信息和复合数据结构。此外，我们提出了一种适应性嵌入方法来合并数值意识。这些增强使PERFOGRAPH成为高度灵活和可扩展的表示形式，可以有效地捕获程序。

    The remarkable growth and significant success of machine learning have expanded its applications into programming languages and program analysis. However, a key challenge in adopting the latest machine learning methods is the representation of programming languages, which directly impacts the ability of machine learning methods to reason about programs. The absence of numerical awareness, composite data structure information, and improper way of presenting variables in previous representation works have limited their performances. To overcome the limitations and challenges of current program representations, we propose a novel graph-based program representation called PERFOGRAPH. PERFOGRAPH can capture numerical information and the composite data structure by introducing new nodes and edges. Furthermore, we propose an adapted embedding method to incorporate numerical awareness. These enhancements make PERFOGRAPH a highly flexible and scalable representation that can effectively capture
    
[^47]: 探究Adam在Transformers上比SGD更快收敛的原因

    Toward Understanding Why Adam Converges Faster Than SGD for Transformers. (arXiv:2306.00204v1 [cs.LG])

    [http://arxiv.org/abs/2306.00204](http://arxiv.org/abs/2306.00204)

    本文研究了Adam在Transformer的训练中为什么比SGD更快收敛，提出了方向锐度的概念并通过比较证明了SGD相对于自适应算法具有更差的方向锐度，进而在理论上证明只需要裁剪一小部分坐标即可提高SGD的表现。

    

    虽然随机梯度下降（SGD）仍是深度学习中最受欢迎的优化算法，但是像Adam这样的自适应算法在一些深度学习应用中已经证明了优于SGD的经验优势，特别是在Transformer的训练上。然而，为什么Adam在这些情况下会显著比SGD更快地收敛仍然是一个问题。本文提出了一个新的概念——方向锐度，解释了为什么Adam比SGD更快地收敛。我们认为优化算法的性能与更新步骤的方向锐度密切相关，证明了SGD相对于自适应算法具有更差的方向锐度。我们进一步观察到只有一小部分坐标导致了SGD的锐度不足和收敛速度缓慢，提出了使用坐标级别的裁剪作为解决方案。我们展示了坐标级别裁剪对于锐度降低和加速收敛的效果。

    While stochastic gradient descent (SGD) is still the most popular optimization algorithm in deep learning, adaptive algorithms such as Adam have established empirical advantages over SGD in some deep learning applications such as training transformers. However, it remains a question that why Adam converges significantly faster than SGD in these scenarios. In this paper, we propose one explanation of why Adam converges faster than SGD using a new concept directional sharpness. We argue that the performance of optimization algorithms is closely related to the directional sharpness of the update steps, and show SGD has much worse directional sharpness compared to adaptive algorithms. We further observe that only a small fraction of the coordinates causes the bad sharpness and slow convergence of SGD, and propose to use coordinate-wise clipping as a solution to SGD and other optimization algorithms. We demonstrate the effect of coordinate-wise clipping on sharpness reduction and speeding u
    
[^48]: 使用域自适应和数据增强构建具有极少和不平衡训练数据的制造深度学习模型

    Building Manufacturing Deep Learning Models with Minimal and Imbalanced Training Data Using Domain Adaptation and Data Augmentation. (arXiv:2306.00202v1 [cs.CV])

    [http://arxiv.org/abs/2306.00202](http://arxiv.org/abs/2306.00202)

    本文提出了一种新颖的域自适应方法和数据增强方法相结合的深度学习模型构建方法，适用于标记训练数据稀缺或不平衡的情况。在晶圆缺陷预测任务中，该方法表现出优异的性能。

    

    深度学习技术在图像缺陷检测方面非常有效。然而，训练深度学习分类模型需要大量的标记数据，这通常很昂贵。在许多情况下，可用的训练数据不仅有限，而且可能是不平衡的。本文提出了一种新颖的域自适应（DA）方法，通过从用于类似学习任务的现有源数据集中转移所获得的知识，来解决目标学习任务的标记训练数据稀缺问题。我们的方法适用于源数据集和可用于目标学习任务的数据集具有相同或不同的特征空间的情况。我们将我们的DA方法与基于自编码器的数据增强方法相结合，以解决目标数据集不平衡的问题。我们使用晶圆缺陷预测的图像数据来评估我们的组合方法。实验结果表明，我们的组合方法在标记数据有限或不平衡时具有比其他算法更优秀的性能。

    Deep learning (DL) techniques are highly effective for defect detection from images. Training DL classification models, however, requires vast amounts of labeled data which is often expensive to collect. In many cases, not only the available training data is limited but may also imbalanced. In this paper, we propose a novel domain adaptation (DA) approach to address the problem of labeled training data scarcity for a target learning task by transferring knowledge gained from an existing source dataset used for a similar learning task. Our approach works for scenarios where the source dataset and the dataset available for the target learning task have same or different feature spaces. We combine our DA approach with an autoencoder-based data augmentation approach to address the problem of imbalanced target datasets. We evaluate our combined approach using image data for wafer defect prediction. The experiments show its superior performance against other algorithms when the number of lab
    
[^49]: 广义隐式Follow-The-Regularized-Leader算法

    Generalized Implicit Follow-The-Regularized-Leader. (arXiv:2306.00201v1 [cs.LG])

    [http://arxiv.org/abs/2306.00201](http://arxiv.org/abs/2306.00201)

    提出了一种新的在线学习算法——广义隐式Follow-The-Regularized-Leader (FTRL)，它可以恢复已知的算法，如线性损失的FTRL和隐式FTRL，并允许设计新的更新规则。该算法的关键思想是用Fenchel-Young不等式代替损失的线性化，可以直接改进最坏情况下的后悔上限。

    

    我们提出了一类新的在线学习算法，称为广义隐式Follow-The-Regularized-Leader (FTRL)，它扩展了FTRL框架的范围。广义隐式FTRL可以恢复已知的算法，如线性损失的FTRL和隐式FTRL，并允许设计新的更新规则，例如aProx和Mirror-Prox的扩展到FTRL。我们的理论是建设性的，因为它提供了一个简单的统一框架，以设计直接改进最坏情况后悔的上限的更新。关键思想是用Fenchel-Young不等式代替损失的线性化。我们通过证明一些已知的算法，如Mirror-Prox更新，是广义隐式FTRL的实例，展示了框架的灵活性。最后，新框架使我们能够恢复隐式OMD的时间变化界，同时具有相同的计算复杂度。

    We propose a new class of online learning algorithms, generalized implicit Follow-The-Regularized-Leader (FTRL), that expands the scope of FTRL framework. Generalized implicit FTRL can recover known algorithms, as FTRL with linearized losses and implicit FTRL, and it allows the design of new update rules, as extensions of aProx and Mirror-Prox to FTRL. Our theory is constructive in the sense that it provides a simple unifying framework to design updates that directly improve the worst-case upper bound on the regret. The key idea is substituting the linearization of the losses with a Fenchel-Young inequality. We show the flexibility of the framework by proving that some known algorithms, like the Mirror-Prox updates, are instantiations of the generalized implicit FTRL. Finally, the new framework allows us to recover the temporal variation bound of implicit OMD, with the same computational complexity.
    
[^50]: 控制文本生成的不变学习特征描述

    An Invariant Learning Characterization of Controlled Text Generation. (arXiv:2306.00198v1 [cs.CL])

    [http://arxiv.org/abs/2306.00198](http://arxiv.org/abs/2306.00198)

    本文提出了一种控制文本生成的不变学习方法，解决了生成的文本分布转移的问题。

    

    控制生成是指创建包含所需样式或语义属性的文本的问题。许多方法将这个问题归结为训练所需属性的预测器。在实际应用中，生成的文本可能来自各种分布。本文通过不变学习的方式解决了控制生成中分布转移的问题。

    Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions. In this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural en
    
[^51]: SSL-CPCD：自我监督的组合预文本类别区分学习，提高内窥镜图像分析的泛化性能

    SSL-CPCD: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis. (arXiv:2306.00197v1 [cs.CV])

    [http://arxiv.org/abs/2306.00197](http://arxiv.org/abs/2306.00197)

    本论文提出了一种名为SSL-CPCD的新方法，通过针对补丁级别的实例组区分和跨类别变化的惩罚，在使用自我监督学习提高内窥镜图像分析泛化性能方面取得了重要的进展。

    

    数据驱动方法在医学图像分析方面取得了巨大进步。在这个背景下，基于深度学习的监督方法广受欢迎。然而，它们需要大量的训练数据，并面临泛化到未见过数据集的问题，这影响了临床转化。内窥镜成像数据具有大的患者内外变异性，这使得这些模型更具有挑战性，无法学习代表下游任务的特征。因此，尽管公开可用的数据集和医院内部可生成的数据集，大多数监督模型仍表现不佳。而自我监督学习在自然场景数据中对该问题做了一定程度的解决，但在医学图像领域存在显著的性能差距。在本文中，我们提出了探索基于补丁的实例组区分和使用余弦相似度度量中的加性角度边距对跨类别变化的惩罚的方法。我们的新方法使模型能够更好地学习代表下游任务的特征，从而提高了内窥镜图像分析的泛化性能。

    Data-driven methods have shown tremendous progress in medical image analysis. In this context, deep learning-based supervised methods are widely popular. However, they require a large amount of training data and face issues in generalisability to unseen datasets that hinder clinical translation. Endoscopic imaging data incorporates large inter- and intra-patient variability that makes these models more challenging to learn representative features for downstream tasks. Thus, despite the publicly available datasets and datasets that can be generated within hospitals, most supervised models still underperform. While self-supervised learning has addressed this problem to some extent in natural scene data, there is a considerable performance gap in the medical image domain. In this paper, we propose to explore patch-level instance-group discrimination and penalisation of inter-class variation using additive angular margin within the cosine similarity metrics. Our novel approach enables mode
    
[^52]: 具有平均奖励的不安定赌徒问题：打破统一全局引子假设

    Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption. (arXiv:2306.00196v1 [cs.LG])

    [http://arxiv.org/abs/2306.00196](http://arxiv.org/abs/2306.00196)

    本文提出了一个通用的框架，将任何单臂策略转化为原始的$N$臂问题的策略，解决了依赖于复杂UGAP假设的问题，并实现了具有$O(1/\sqrt{N})$最优性差距的策略。

    

    我们研究了具有平均奖励标准下的无限时不安定赌徒问题，包括离散时间和连续时间设置。一个基本问题是如何设计计算有效的策略，使得优化差距随着臂的数量$N$的增加而减小。现有的渐近最优性结果都依赖于统一全局引子性质(UGAP)，这是一个复杂且难以验证的假设。在本文中，我们提出了一个通用的、基于模拟的框架，将任何单臂策略转化为原始的$N$臂问题的策略。这是通过在每个臂上模拟单臂策略，并仔细地将真实状态引导向模拟状态来实现的。我们的框架可以实例化，产生一个具有$O(1/\sqrt{N})$的最优解差距的策略。在离散时间设置中，我们的结果在更简单的同步假设下成立，涵盖了一些不满足UGAP的问题实例。更值得注意的是，我们的框架可以处理比现有方法更大的问题类，而不需对问题实例做任何特定的结构假设。

    We study the infinite-horizon restless bandit problem with the average reward criterion, under both discrete-time and continuous-time settings. A fundamental question is how to design computationally efficient policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. Existing results on asymptotical optimality all rely on the uniform global attractor property (UGAP), a complex and challenging-to-verify assumption. In this paper, we propose a general, simulation-based framework that converts any single-armed policy into a policy for the original $N$-armed problem. This is accomplished by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. Our framework can be instantiated to produce a policy with an $O(1/\sqrt{N})$ optimality gap. In the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that do not satisfy UGAP. More notabl
    
[^53]: 多环境生命周期深度强化学习在医学影像中的应用

    Multi-environment lifelong deep reinforcement learning for medical imaging. (arXiv:2306.00188v1 [cs.LG])

    [http://arxiv.org/abs/2306.00188](http://arxiv.org/abs/2306.00188)

    本文提出了一种生命周期DRL框架SERIL，在医学影像领域中实现了在不断变化的成像环境中持续学习的目的，展现出了出色的性能。

    

    深度强化学习（DRL）在医学影像领域的应用越来越多。然而，由于成像方向、成像序列和病理学的变化，医学影像任务的环境不断演变。因此，我们开发了一种生命周期DRL框架SERIL，采用基于选择性体验回放的生命周期学习技术，在24个不同的成像环境中进行人脑MRI上五个解剖标志的定位。与两个基准设置MERT（多环境最佳情况）和SERT（单环境最劣情况）相比，SERIL表现出了出色的性能，在所有120个任务中，与期望的标志距离的平均距离为$9.90\pm7.35$像素，而MERT和SERT分别为$10.29\pm9.07$和$36.37\pm22.41$($p<0.05$)，展示了在不断变化的成像环境中持续学习的出色潜力。

    Deep reinforcement learning(DRL) is increasingly being explored in medical imaging. However, the environments for medical imaging tasks are constantly evolving in terms of imaging orientations, imaging sequences, and pathologies. To that end, we developed a Lifelong DRL framework, SERIL to continually learn new tasks in changing imaging environments without catastrophic forgetting. SERIL was developed using selective experience replay based lifelong learning technique for the localization of five anatomical landmarks in brain MRI on a sequence of twenty-four different imaging environments. The performance of SERIL, when compared to two baseline setups: MERT(multi-environment-best-case) and SERT(single-environment-worst-case) demonstrated excellent performance with an average distance of $9.90\pm7.35$ pixels from the desired landmark across all 120 tasks, compared to $10.29\pm9.07$ for MERT and $36.37\pm22.41$ for SERT($p<0.05$), demonstrating the excellent potential for continuously le
    
[^54]: 预训练表示中的扩散冗余

    Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v1 [cs.LG])

    [http://arxiv.org/abs/2306.00183](http://arxiv.org/abs/2306.00183)

    本文发现预训练表示中存在一定程度的扩散冗余，即随机选择一部分神经元与全部神经元相似度高且性能相似，可用于降低实际部署中的计算和内存成本，同时保持相当的性能水平。

    

    在大型数据集上预训练神经网络获得的表示已被越来越多地成功应用于各种下游任务中。在本文中，我们更加深入地研究了这种预训练表示中的特征是如何被编码的。我们发现，在给定层中学到的表示展现出一定程度的扩散冗余，即对于超过一个阈值大小的任何随机子集神经元，都与完整层具有很高的相似度，并且在各种下游任务中能够表现出与整个层相似的性能。我们在各种不同的神经架构（包括CNN和Transformer）上进行了实验，使用了ImageNet1k和ImageNet21k进行预训练，并评估了各种下游任务，如图像分类、目标检测和自然语言处理。我们的实验结果表明，可以利用预训练表示中的冗余来降低在实际部署中使用这些模型的计算和内存成本，同时仍然保持相当的性能水平。

    Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstrea
    
[^55]: FlowCam: 通过像素对齐场景流，无需相机位姿训练具有通用性的三维辐射场

    FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow. (arXiv:2306.00180v1 [cs.CV])

    [http://arxiv.org/abs/2306.00180](http://arxiv.org/abs/2306.00180)

    本文提出了一种利用可微分渲染将帧间光流映射到三维场景流以同步重建相机位姿和三维神经场表示的方法。这种方法不需要精确相机位姿，可在真实世界的视频数据上进行全自监督训练。

    

    从定位图像中重建三维神经场已成为自监督表示学习的一种有前途的方法。但依赖于大规模视频数据上结构光学精确相机位姿，这也是其无法推广的关键挑战。我们提出了一种联合在线重建相机位姿和三维神经场表示的方法，并通过单次前向传递来完成。该方法通过可微分渲染将帧间光流映射到三维场景流，以估计位姿。然后再通过对场景流场进行加权最小二乘拟合来执行SE（3）相机位姿估计。该方法能够通过重新渲染输入视频来联合监督位姿估计和通用的神经场表示，从而在真实世界的视频数据上进行全自监督训练。

    Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datas
    
[^56]: 具有鲁棒性保证的带权在线二分图匹配学习

    Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees. (arXiv:2306.00172v1 [cs.LG])

    [http://arxiv.org/abs/2306.00172](http://arxiv.org/abs/2306.00172)

    本论文提出了一个新的基于RL的边缘加权在线二分图匹配方法（LOMAR），实现了好的平均情况和最坏情况的性能，其关键新颖之处在于决策是基于一个新的在线切换操作，该操作可以对抗未来的不确定性。

    

    许多问题，如在线广告显示可以被公式化为在线二分图匹配。关键的挑战在于基于序列化公布的在线条目信息，我们在每个步骤上做出不可逆转的匹配决策。虽然已经提出了许多具有有界最坏情况竞争比的专家在线算法，但它们在平均情况下可能无法提供令人满意的性能。另一方面，强化学习（RL）已被应用于提高平均性能，但缺乏鲁棒性，可能表现得任意差。本文提出了一种新颖的基于RL的具有鲁棒性保证的边缘加权在线二分图匹配方法（LOMAR），实现了好的平均情况和最坏情况的性能。 LOMAR的关键新颖之处在于一种新的在线切换操作，该操作基于谨慎的条件来对抗未来不确定性，决定是否根据每个在线项目遵循专家决策还是RL决策。

    Many problems, such as online ad display, can be formulated as online bipartite matching. The crucial challenge lies in the nature of sequentially-revealed online item information, based on which we make irreversible matching decisions at each step. While numerous expert online algorithms have been proposed with bounded worst-case competitive ratios, they may not offer satisfactory performance in average cases. On the other hand, reinforcement learning (RL) has been applied to improve the average performance, but it lacks robustness and can perform arbitrarily poorly. In this paper, we propose a novel RL-based approach to edge-weighted online bipartite matching with robustness guarantees (LOMAR), achieving both good average-case and worst-case performance. The key novelty of LOMAR is a new online switching operation which, based on a judicious condition to hedge against future uncertainties, decides whether to follow the expert's decision or the RL decision for each online item. We pro
    
[^57]: 深度神经网络训练的不一致性、不稳定性和泛化差距

    Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training. (arXiv:2306.00169v1 [cs.LG])

    [http://arxiv.org/abs/2306.00169](http://arxiv.org/abs/2306.00169)

    本文的理论分析与实证研究表明，深度神经网络训练中模型输出的不一致性和不稳定性可以作为估计泛化间隙的重要指标，消除不一致性的算法能够提高模型性能。

    

    随着深度神经网络具有很高的表现力，寻找具有小泛化差距的解决方案变得很重要（即在训练数据和未见数据之间的性能差异）。本文通过关注训练的随机性，首先提出了一个理论分析，其中泛化间隙的界限取决于我们称之为模型输出的不一致性和不稳定性，这可以在未标记的数据上进行估计。我们的实证研究基于这个分析，表明在各种设置中，不稳定性和不一致性强烈预示着泛化间隙。特别地，我们的发现表明，不一致性比损失变化的锐度更可靠地指示着泛化间隙。此外，我们还展示了消除不一致性的算法可以带来更优异的表现。这些结果还为现有的方法提供了理论基础，如共同蒸馏和集成。

    As deep neural networks are highly expressive, it is important to find solutions with small generalization gap (the difference between the performance on the training data and unseen data). Focusing on the stochastic nature of training, we first present a theoretical analysis in which the bound of generalization gap depends on what we call inconsistency and instability of model outputs, which can be estimated on unlabeled data. Our empirical study based on this analysis shows that instability and inconsistency are strongly predictive of generalization gap in various settings. In particular, our finding indicates that inconsistency is a more reliable indicator of generalization gap than the sharpness of the loss landscape. Furthermore, we show that algorithmic reduction of inconsistency leads to superior performance. The results also provide a theoretical basis for existing methods such as co-distillation and ensemble.
    
[^58]: 在嘈杂环境下利用轻量级迭代模型进行音视频语音分离

    Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model. (arXiv:2306.00160v1 [eess.AS])

    [http://arxiv.org/abs/2306.00160](http://arxiv.org/abs/2306.00160)

    本文提出名称为AVLIT的轻量级迭代模型，采用渐进式学习在嘈杂环境中实现音视频语音的分离。实验证明该模型在两种情境下的音频分离和音视频基线方法相比具有优越性，同时其减小的模型大小使得其适用于低资源应用场景。

    

    我们提出了一种名为 AVLIT 的音视频轻量级迭代模型，它使用渐进式学习来在嘈杂环境中进行音视频语音分离。为此，我们采用了异步全循环卷积神经网络（A-FRCNN），它在仅利用音频进行语音分离方面已经取得了成功的结果。我们的架构由音频分支和视频分支组成，每个模态的迭代 A-FRCNN 块共享权重。我们使用 NTCD-TIMIT 数据集在受控环境中进行了模型评估，并使用合成数据集 LRS3 和 WHAM! 进行了野外测试。实验表明，与各种音频分离和音视频基线方法相比，我们的模型在两种情况下均具有优越性。此外，我们模型的减少的占用空间使其适用于低资源应用场景。

    We propose Audio-Visual Lightweight ITerative model (AVLIT), an effective and lightweight neural network that uses Progressive Learning (PL) to perform audio-visual speech separation in noisy environments. To this end, we adopt the Asynchronous Fully Recurrent Convolutional Neural Network (A-FRCNN), which has shown successful results in audio-only speech separation. Our architecture consists of an audio branch and a video branch, with iterative A-FRCNN blocks sharing weights for each modality. We evaluated our model in a controlled environment using the NTCD-TIMIT dataset and in-the-wild using a synthetic dataset that combines LRS3 and WHAM!. The experiments demonstrate the superiority of our model in both settings with respect to various audio-only and audio-visual baselines. Furthermore, the reduced footprint of our model makes it suitable for low resource applications.
    
[^59]: 基于符号回归的信息融合：以人类健康为背景的教程

    Information Fusion via Symbolic Regression: A Tutorial in the Context of Human Health. (arXiv:2306.00153v1 [cs.LG])

    [http://arxiv.org/abs/2306.00153](http://arxiv.org/abs/2306.00153)

    本教程论文介绍了符号回归技术，以人类健康和营养领域的实际应用为例，将各种人体测量指标融合成一个简单的数学表达式以估算体脂百分比。

    

    本教程论文提供了符号回归（SR）的一般概述，重点关注可解释性标准。我们认为，尽管文献中对可解释性建模的定义仍有争议，但可解释性模型是支持评估成功信息融合的实用方式。为了传达SR作为建模技术的优点，我们利用来自疾病控制和预防中心（CDC）的公开可用的国家健康和营养调查（NHANES）数据，在健康和营养领域展示了一个应用，将各种人体测量指标融合成一个简单的数学表达式以估算体脂百分比。我们讨论了SR建模的优点和挑战，并提供了所学模型的定性和定量分析。

    This tutorial paper provides a general overview of symbolic regression (SR) with specific focus on standards of interpretability. We posit that interpretable modeling, although its definition is still disputed in the literature, is a practical way to support the evaluation of successful information fusion. In order to convey the benefits of SR as a modeling technique, we demonstrate an application within the field of health and nutrition using publicly available National Health and Nutrition Examination Survey (NHANES) data from the Centers for Disease Control and Prevention (CDC), fusing together anthropometric markers into a simple mathematical expression to estimate body fat percentage. We discuss the advantages and challenges associated with SR modeling and provide qualitative and quantitative analyses of the learned models.
    
[^60]: 学习正确的层级：一种基于数据的多层图半监督学习的层级聚合策略

    Learning the Right Layers: a Data-Driven Layer-Aggregation Strategy for Semi-Supervised Learning on Multilayer Graphs. (arXiv:2306.00152v1 [cs.LG])

    [http://arxiv.org/abs/2306.00152](http://arxiv.org/abs/2306.00152)

    本文提出了一种参数-free的拉普拉斯正则化模型，能够无需预先评估层的贡献，通过学习一个最佳的非线性层级组合来实现基于多层图的半监督学习。

    

    在多层图上进行聚类（或社区检测）会面临一些额外的复杂性问题，因为不同的层可以通过不同的结构和信息类型进行特征化。其中一个主要的挑战是确定每个层对于群集分配的贡献程度，从而有效地利用多层结构，并改进使用单个层或它们的联合获得的分类。然而，对于层的聚类信息内容做出知情的预先评估可能非常复杂。在这项工作中，我们假设半监督学习环境，在最初提供少量节点的类别的情况下，提出一个无参数的拉普拉斯正则化模型，通过可用输入标签学习不同层之间的最佳非线性组合。学习算法基于带有不精确梯度的Frank-Wolfe优化方案。

    Clustering (or community detection) on multilayer graphs poses several additional complications with respect to standard graphs as different layers may be characterized by different structures and types of information. One of the major challenges is to establish the extent to which each layer contributes to the cluster assignment in order to effectively take advantage of the multilayer structure and improve upon the classification obtained using the individual layers or their union. However, making an informed a-priori assessment about the clustering information content of the layers can be very complicated. In this work, we assume a semi-supervised learning setting, where the class of a small percentage of nodes is initially provided, and we propose a parameter-free Laplacian-regularized model that learns an optimal nonlinear combination of the different layers from the available input labels. The learning algorithm is based on a Frank-Wolfe optimization scheme with inexact gradient, 
    
[^61]: SafeDiffuser：基于扩散概率模型的安全规划

    SafeDiffuser: Safe Planning with Diffusion Probabilistic Models. (arXiv:2306.00148v1 [cs.LG])

    [http://arxiv.org/abs/2306.00148](http://arxiv.org/abs/2306.00148)

    SafeDiffuser提出了一种新方法解决扩散概率模型在安全性上的挑战。通过使用一类控制障碍函数，将有限时间扩散不变性嵌入到去噪扩散过程中，实现了可靠的扩散数据生成，并在多个安全规划任务上展示了鲁棒性和保证。

    

    基于扩散模型的方法已经在数据驱动的规划中显示出了很好的效果，但是由于缺乏安全保障，因此很难应用于安全关键应用。为了解决这些挑战，我们提出了一种新方法，称为SafeDiffuser，通过使用一类控制障碍函数来确保扩散概率模型满足规格说明。我们方法的关键思想是将所提出的有限时间扩散不变性嵌入到去噪扩散过程中，从而实现可信扩散数据生成。此外，我们证明了我们的有限时间扩散不变性方法通过生成模型不仅维持了通用性能，而且在安全数据生成中也产生出了鲁棒性。我们将我们的方法测试在一系列的安全规划任务中，包括迷宫路径生成、多足机器人动力学和三维空间操作，结果显示了相对于基本的扩散模型，我们方法的优势和保证。

    Diffusion model-based approaches have shown promise in data-driven planning, but there are no safety guarantees, thus making it hard to be applied for safety-critical applications. To address these challenges, we propose a new method, called SafeDiffuser, to ensure diffusion probabilistic models satisfy specifications by using a class of control barrier functions. The key idea of our approach is to embed the proposed finite-time diffusion invariance into the denoising diffusion procedure, which enables trustworthy diffusion data generation. Moreover, we demonstrate that our finite-time diffusion invariance method through generative models not only maintains generalization performance but also creates robustness in safe data generation. We test our method on a series of safe planning tasks, including maze path generation, legged robot locomotion, and 3D space manipulation, with results showing the advantages of robustness and guarantees over vanilla diffusion models.
    
[^62]: 关于神经网络表达能力的研究

    On the Expressive Power of Neural Networks. (arXiv:2306.00145v1 [math.CA])

    [http://arxiv.org/abs/2306.00145](http://arxiv.org/abs/2306.00145)

    本文研究了神经网络的表达能力，证明了某些问题在之前的研究中未得到解决，并提出了新问题的回答，包括广而浅的ReLU网络不能被深而窄的ReLU网络很好地逼近等。

    

    1989年，George Cybenko在一篇里程碑式的论文中证明了宽而浅的神经网络可以在紧致集上逼近任意连续函数，这个通用逼近定理引发了很多后续研究。本文将通过一个框架回答“有没有一些广而浅的ReLU网络无法被深而窄的ReLU网络很好地逼近？”“普遍逼近定理是否仍适用于Sobolev空间范数W 1,1？”“这些结果是否适用于除ReLU之外的激活函数？”等问题。

    In 1989 George Cybenko proved in a landmark paper that wide shallow neural networks can approximate arbitrary continuous functions on a compact set. This universal approximation theorem sparked a lot of follow-up research.  Shen, Yang and Zhang determined optimal approximation rates for ReLU-networks in $L^p$-norms with $p \in [1,\infty)$. Kidger and Lyons proved a universal approximation theorem for deep narrow ReLU-networks. Telgarsky gave an example of a deep narrow ReLU-network that cannot be approximated by a wide shallow ReLU-network unless it has exponentially many neurons.  However, there are even more questions that still remain unresolved. Are there any wide shallow ReLU-networks that cannot be approximated well by deep narrow ReLU-networks? Is the universal approximation theorem still true for other norms like the Sobolev norm $W^{1,1}$? Do these results hold for activation functions other than ReLU?  We will answer all of those questions and more with a framework of two exp
    
[^63]: Mechanic: 一种学习率调节器

    Mechanic: A Learning Rate Tuner. (arXiv:2306.00144v1 [cs.LG])

    [http://arxiv.org/abs/2306.00144](http://arxiv.org/abs/2306.00144)

    机械师是一种学习率调节器，能自动调整任何基本优化算法和调度的学习率比例因子，可以实现在大规模深度学习任务中接近、匹配或甚至优于手动调整学习率的效果。

    

    我们介绍了一种名为“机械师”的技术，用于自动调整任何基本优化算法和调度的学习率比例因子。我们的方法提供了实现类似目标的最近理论减少的实际应用，用于在线凸优化。我们在具有不同批量大小、调度和基本优化算法的一系列大规模深度学习任务中进行了严格评估。这些实验表明，根据问题，机械师要么非常接近，要么匹配或甚至优于手动调整学习率。

    We introduce a technique for tuning the learning rate scale factor of any base optimization algorithm and schedule automatically, which we call \textsc{mechanic}. Our method provides a practical realization of recent theoretical reductions for accomplishing a similar goal in online convex optimization. We rigorously evaluate \textsc{mechanic} on a range of large scale deep learning tasks with varying batch sizes, schedules, and base optimization algorithms. These experiments demonstrate that depending on the problem, \textsc{mechanic} either comes very close to, matches or even improves upon manual tuning of learning rates.
    
[^64]: 关于金丝雀曝光解释的一些注释

    A Note On Interpreting Canary Exposure. (arXiv:2306.00133v1 [cs.CR])

    [http://arxiv.org/abs/2306.00133](http://arxiv.org/abs/2306.00133)

    本文提供了关于如何解释金丝雀曝光的直觉，包括其与成员推理攻击和差分隐私的关系。

    

    Carlini等人介绍的金丝雀暴露经常被用来实证评估或审核机器学习模型培训的隐私。这篇笔记的目的是提供一些关于如何解释金丝雀曝光的直觉，包括与成员推理攻击和差分隐私的关系。

    Canary exposure, introduced in Carlini et al. is frequently used to empirically evaluate, or audit, the privacy of machine learning model training. The goal of this note is to provide some intuition on how to interpret canary exposure, including by relating it to membership inference attacks and differential privacy.
    
[^65]: Surrogate Model Extension (SME): 一种快速准确的联邦学习权重更新攻击方法

    Surrogate Model Extension (SME): A Fast and Accurate Weight Update Attack on Federated Learning. (arXiv:2306.00127v1 [cs.LG])

    [http://arxiv.org/abs/2306.00127](http://arxiv.org/abs/2306.00127)

    本文提出了一种称为 Surrogate Model Extension (SME) 的方法，使得在联邦学习中，攻击者可以使用比之前更加高效和准确的方法对权重更新进行隐私攻击，进一步暴露了联邦学习中的隐私保护弱点。

    

    在联邦学习（FL）和分布式训练框架中，合作者可以在本地保存其私有数据，并在多次迭代后仅分享使用本地数据训练的网络权重。梯度反演是一类从生成的梯度中恢复数据的隐私攻击方法。在FL中，单个步骤的梯度被多次本地迭代的梯度积累所遮盖，似乎可以提供一定的保护，防止梯度反演攻击权重更新。本文提出了一种合理的方法，将梯度反演攻击扩展到FL权重更新中，从而更好地暴露FL本身所假定的隐私保护弱点。特别地，我们提出了一种基于二维梯度流特征和本地更新低秩性质的替代模型方法。我们的方法极大地提升了包含多次迭代的权重更新受梯度反演攻击的能力，并实现了非常高的攻击成功率。

    In Federated Learning (FL) and many other distributed training frameworks, collaborators can hold their private data locally and only share the network weights trained with the local data after multiple iterations. Gradient inversion is a family of privacy attacks that recovers data from its generated gradients. Seemingly, FL can provide a degree of protection against gradient inversion attacks on weight updates, since the gradient of a single step is concealed by the accumulation of gradients over multiple local iterations. In this work, we propose a principled way to extend gradient inversion attacks to weight updates in FL, thereby better exposing weaknesses in the presumed privacy protection inherent in FL. In particular, we propose a surrogate model method based on the characteristic of two-dimensional gradient flow and low-rank property of local updates. Our method largely boosts the ability of gradient inversion attacks on weight updates containing many iterations and achieves s
    
[^66]: ReLU网络的最优集合和解路径

    Optimal Sets and Solution Paths of ReLU Networks. (arXiv:2306.00119v1 [cs.LG])

    [http://arxiv.org/abs/2306.00119](http://arxiv.org/abs/2306.00119)

    本研究开发了一个分析框架，通过凸规划表征所有ReLU网络的最优集合和解路径，并提供了最小网络的最优剪枝算法，建立了ReLU网络正则化路径连续的条件，并为最小ReLU网络提供了灵敏度结果。

    

    我们开发了一个分析框架，通过将非凸训练问题重新定义为凸规划问题，来刻画最优ReLU神经网络集合。我们证明了凸参数化的全局最优解由一个多面体集合给出，并将这个表征扩展到了非凸训练目标的最优集。由于ReLU训练问题的所有稳定点都可以表示为子采样凸规划的最优解，因此我们的工作为所有非凸目标的临界点提供了通用表达式。然后，我们利用我们的结果提供了计算最小网络的最优剪枝算法，建立了ReLU网络正则化路径连续的条件，并为最小ReLU网络提供了灵敏度结果。

    We develop an analytical framework to characterize the set of optimal ReLU neural networks by reformulating the non-convex training problem as a convex program. We show that the global optima of the convex parameterization are given by a polyhedral set and then extend this characterization to the optimal set of the non-convex training objective. Since all stationary points of the ReLU training problem can be represented as optima of sub-sampled convex programs, our work provides a general expression for all critical points of the non-convex objective. We then leverage our results to provide an optimal pruning algorithm for computing minimal networks, establish conditions for the regularization path of ReLU networks to be continuous, and develop sensitivity results for minimal ReLU networks.
    
[^67]: 加拿大农田数据集：用于农业多时相深度学习分类的新地表覆盖数据集

    The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture. (arXiv:2306.00114v1 [cs.CV])

    [http://arxiv.org/abs/2306.00114](http://arxiv.org/abs/2306.00114)

    该论文提出了一个时间补丁数据集，包含了加拿大农田的多时相遥感影像。该数据集是手动经过确认和筛选的高分辨率地理参考图像，覆盖四个农作物生产年度和五个月份。这个数据集可以用于提高土地覆盖分类的准确性。

    

    利用遥感监测土地覆盖是研究环境变化和通过粮食产量预测确保全球粮食安全的关键。尤其是，多时相遥感影像提供了关于场景动态的相关信息，已经被证明可以带来更好的土地覆盖分类结果。然而，由于难以获取可靠、细粒度和高质量的注释样本支持他们的假设，很少有研究受益于高空间和时间分辨率数据。因此，我们介绍了一个加拿大农田的时间补丁数据集，其中包含了来自10个农作物类别的78,536个手动经过确认和筛选的高分辨率(10米/像素，640 x 640米)地理参考图像，覆盖了四个农作物生产年度(2017-2020)和五个月份(六月-十月)。每个实例都包含12个光谱波段、一张RGB图像和额外的植被指数计算。

    Monitoring land cover using remote sensing is vital for studying environmental changes and ensuring global food security through crop yield forecasting. Specifically, multitemporal remote sensing imagery provides relevant information about the dynamics of a scene, which has proven to lead to better land cover classification results. Nevertheless, few studies have benefited from high spatial and temporal resolution data due to the difficulty of accessing reliable, fine-grained and high-quality annotated samples to support their hypotheses. Therefore, we introduce a temporal patch-based dataset of Canadian croplands, enriched with labels retrieved from the Canadian Annual Crop Inventory. The dataset contains 78,536 manually verified and curated high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10 crop classes collected over four crop production years (2017-2020) and five months (June-October). Each instance contains 12 spectral bands, an RGB image, and additional veget
    
[^68]: MuseCoco: 从文本生成符号音乐

    MuseCoco: Generating Symbolic Music from Text. (arXiv:2306.00110v1 [cs.SD])

    [http://arxiv.org/abs/2306.00110](http://arxiv.org/abs/2306.00110)

    MuseCoco是一种从文本描述中生成符号音乐的系统，具备高效和灵活等特点，为音乐家提供了更好的音乐生成方式。

    

    从文本描述中生成音乐是一种用户友好的方式，因为文本是相对易于用户参与的界面。而有些方法利用文本来控制音乐音频的生成，但是编辑生成音频的音乐元素对于用户来说是具有挑战性的。相比之下，符号音乐具有易于编辑的优点，使用户更容易操作特定的音乐元素。本文介绍了MuseCoco，它利用音乐属性作为桥梁，将任务分解为文本到属性理解和属性到音乐生成的两个阶段，从而生成符号音乐。MuseCoCo代表音乐作曲副驾驶，使音乐家可以直接从给定的文本描述中生成音乐，与从头开始创作相比，大大提高了效率。该系统具有两个主要优点：数据高效。在属性到音乐生成阶段，属性可以直接进行编码，而不需要大量的音乐数据。其次，此系统具有高级别的灵活性，因为它可以通过变更文本输入来生成多样化且有个性的符号音乐。

    Generating music from text descriptions is a user-friendly mode since the text is a relatively easy interface for user engagement. While some approaches utilize texts to control music audio generation, editing musical elements in generated audio is challenging for users. In contrast, symbolic music offers ease of editing, making it more accessible for users to manipulate specific musical elements. In this paper, we propose MuseCoco, which generates symbolic music from text descriptions with musical attributes as the bridge to break down the task into text-to-attribute understanding and attribute-to-music generation stages. MuseCoCo stands for Music Composition Copilot that empowers musicians to generate music directly from given text descriptions, offering a significant improvement in efficiency compared to creating music entirely from scratch. The system has two main advantages: Firstly, it is data efficient. In the attribute-to-music generation stage, the attributes can be directly e
    
[^69]: 多数规则：通过自洽性实现更好的修补

    Majority Rule: better patching via Self-Consistency. (arXiv:2306.00108v1 [cs.SE])

    [http://arxiv.org/abs/2306.00108](http://arxiv.org/abs/2306.00108)

    本文提出了一种无需解释的算法，基于自洽性和大型语言模型（LLMs）来进行软件补丁选择，从而解决当前缺乏解释的软件数据集的问题。

    

    大型语言模型（LLMs）可以被引导解决一些需要“少量提示”的问题，包括示例问题-解决方案对。现在，如果少量提示也包括“思维链”（CoT）解释，形式为问题-解释-解决方案，LLMs将会产生一个“解释”的解决方案，并表现得更好。最近，一项更出色的技术“自洽性”（S-C）出现了，基于这样一种直觉：正确的解决方案有许多可能的解释。当LLM被反复采样以生成问题的解释-解决方案对时，对于给定的问题，池中最常发生的解决方案（忽略解释）更有可能是正确的！但是，这种高性能的S-C（甚至CoT）方法在软件工程环境中的使用受到解释的缺乏的限制；大多数软件数据集都缺乏解释。在本文中，我们描述了一个S-C算法的应用，该算法不需要解释，因此可以用于软件补丁选择。

    Large Language models (LLMs) can be induced to solve non-trivial problems with "few-shot" prompts including illustrative problem-solution examples. Now if the few-shots also include "chain of thought" (CoT) explanations, which are of the form problem-explanation-solution, LLMs will generate a "explained" solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] (S-C) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant S-C (or even CoT) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the S-C a
    
[^70]: MERT:带有大规模自监督训练的声学音乐理解模型

    MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])

    [http://arxiv.org/abs/2306.00107](http://arxiv.org/abs/2306.00107)

    提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。

    

    自监督学习（SSL）最近在视觉、文本和语音领域中已被证明是训练通用模型的一种很有前景的范例，对于跨越音乐领域的应用，尤其是对于调性和音高这样的特殊音乐知识的建模颇具挑战性。为了解决这一问题，我们提出了一个基于大规模自监督训练的声学音乐理解模型，即MERT。在我们的探索中，我们确定了更优秀的教师模型组合，这种组合方法在性能方面优于传统的语音和音频方法。

    Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
    
[^71]: ManagerTower：聚合单模态专家见解用于视觉语言表示学习

    ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning. (arXiv:2306.00103v1 [cs.CV])

    [http://arxiv.org/abs/2306.00103](http://arxiv.org/abs/2306.00103)

    提出了ManagerTower，一种新型的VL模型体系结构，可以集合并组合不同级别的预先训练的单模态专家的见解，并可以自适应地聚合单模态语义知识以促进更全面的跨模态对齐和融合。仅使用4M VLP数据，ManagerTower在各种下游VL任务中都取得了更好的性能，尤其是在VQAv2测试标准下达到了79.15%的准确率，在Flickr30K上的IR@1为86.56% TR@1为95.64%。

    

    两塔视觉语言模型已经在各种下游视觉语言任务上显示出有希望的改进。尽管最先进的工作通过构建编码器之间的桥梁来提高性能，但它遭受了单模态表示的逐层利用效果不佳的困境，并且不能灵活地利用不同级别的单模态语义知识。在这项工作中，我们提出了ManagerTower，这是一种新型的VL模型体系结构，它集合并组合了预先训练的不同级别的单模态专家的见解。在每个跨模态层中引入的管理器可以自适应地聚合单模态语义知识，以促进更全面的跨模态对齐和融合。ManagerTower在有和没有视觉语言预训练(VLP)的情况下都优于以前的强基线。仅使用4M VLP数据，ManagerTower在各种下游VL任务中都取得了更好的性能，尤其是在VQAv2测试标准下达到了79.15%的准确率，在Flickr30K上的IR@1为86.56% TR@1为95.64%。Code and check

    Two-Tower Vision-Language (VL) models have shown promising improvements on various downstream VL tasks. Although the most advanced work improves performance by building bridges between encoders, it suffers from ineffective layer-by-layer utilization of uni-modal representations and cannot flexibly exploit different levels of uni-modal semantic knowledge. In this work, we propose ManagerTower, a novel VL model architecture that gathers and combines the insights of pre-trained uni-modal experts at different levels. The managers introduced in each cross-modal layer can adaptively aggregate uni-modal semantic knowledge to facilitate more comprehensive cross-modal alignment and fusion. ManagerTower outperforms previous strong baselines both with and without Vision-Language Pre-training (VLP). With only 4M VLP data, ManagerTower achieves superior performances on various downstream VL tasks, especially 79.15% accuracy on VQAv2 Test-Std, 86.56% IR@1 and 95.64% TR@1 on Flickr30K. Code and check
    
[^72]: 通过遗憾最小化方法进行Pareto前沿识别

    Pareto Front Identification with Regret Minimization. (arXiv:2306.00096v1 [stat.ML])

    [http://arxiv.org/abs/2306.00096](http://arxiv.org/abs/2306.00096)

    本文提出了适用于线性Bandit的Pareto前沿识别算法，该算法使用所有动作的上下文信息，具有较低的样本复杂度，并同时保证了Pareto前沿识别和Pareto遗憾最小化。

    

    本文考虑线性Bandit情况下的Pareto前沿识别（PFILin），其目标是在平均奖励向量作为环境的线性函数的情况下，识别一组奖励向量不被其他任何向量所占优。PFILin包括最佳动作识别和多目标主动学习等特殊情况。我们提出的算法的样本复杂度为$\tilde{O}(d/\Delta^2)$，其中$d$是上下文的维数，$\Delta$是问题复杂性的一种度量。我们的样本复杂度在对数因子上是最优的。本算法的一个新特点是使用所有动作的上下文信息。除了有效地识别Pareto前沿之外，我们的算法还保证，在样本数大于$\Omega(d\log dL)$时，对于$L$维矢量奖励，瞬时Pareto遗憾的$\tilde{O}(\sqrt{d/t})$界限。通过使用所有动作的上下文信息，我们提出的算法同时为线性Bandit提供了有效的Pareto前沿识别和Pareto遗憾最小化。

    We consider Pareto front identification for linear bandits (PFILin) where the goal is to identify a set of arms whose reward vectors are not dominated by any of the others when the mean reward vector is a linear function of the context. PFILin includes the best arm identification problem and multi-objective active learning as special cases. The sample complexity of our proposed algorithm is $\tilde{O}(d/\Delta^2)$, where $d$ is the dimension of contexts and $\Delta$ is a measure of problem complexity. Our sample complexity is optimal up to a logarithmic factor. A novel feature of our algorithm is that it uses the contexts of all actions. In addition to efficiently identifying the Pareto front, our algorithm also guarantees $\tilde{O}(\sqrt{d/t})$ bound for instantaneous Pareto regret when the number of samples is larger than $\Omega(d\log dL)$ for $L$ dimensional vector rewards. By using the contexts of all arms, our proposed algorithm simultaneously provides efficient Pareto front ide
    
[^73]: 一个在约化李群上等变的神经网络通用框架

    A General Framework for Equivariant Neural Networks on Reductive Lie Groups. (arXiv:2306.00091v1 [stat.ML])

    [http://arxiv.org/abs/2306.00091](http://arxiv.org/abs/2306.00091)

    本文提出了一个通用的等变神经网络架构，能够尊重任何一个约化李群G的有限维表示的对称性，并通过在多个领域的任务上进行实验来证明其性能。

    

    约化李群，如正交群、洛伦兹群或幺正群，在高能物理、量子力学、量子色动力学、分子动力学、计算机视觉和成像等各个科学领域中扮演着重要角色。本文提出了一个通用的等变神经网络架构，能够尊重任何一个约化李群G的有限维表示的对称性。我们的方法推广了成功的ACE和MACE架构对于点云的原子级数据等变到任何一个对约化李群作用等变的数据。我们还引入了lie-nn软件库，提供了开发和实现这种通用G-等变神经网络所需的所有工具。它实现了将表示的通用张量积减少到不可约表示的例行程序，使得我们的架构可以应用到各种问题和群体。通过在各种任务上的实验，包括分子动力学模拟、扩散MRI和计算机视觉，展示了我们框架的普适性和性能。

    Reductive Lie Groups, such as the orthogonal groups, the Lorentz group, or the unitary groups, play essential roles across scientific fields as diverse as high energy physics, quantum mechanics, quantum chromodynamics, molecular dynamics, computer vision, and imaging. In this paper, we present a general Equivariant Neural Network architecture capable of respecting the symmetries of the finite-dimensional representations of any reductive Lie Group G. Our approach generalizes the successful ACE and MACE architectures for atomistic point clouds to any data equivariant to a reductive Lie group action. We also introduce the lie-nn software library, which provides all the necessary tools to develop and implement such general G-equivariant neural networks. It implements routines for the reduction of generic tensor products of representations into irreducible representations, making it easy to apply our architecture to a wide range of problems and groups. The generality and performance of our 
    
[^74]: 面向大规模机器学习的关系计算自动微分

    Auto-Differentiation of Relational Computations for Very Large Scale Machine Learning. (arXiv:2306.00088v1 [cs.LG])

    [http://arxiv.org/abs/2306.00088](http://arxiv.org/abs/2306.00088)

    本文提出了一种面向关系计算的自动微分方法，实验表明该方法可达到非常大的数据集规模，并在大规模分布式机器学习中表现出与专用系统相媲美的竞争力。

    

    关系数据模型被设计用于大规模数据管理和分析。本文探讨了如何求解关系计算中的自动微分问题。我们在实验中展示了一个运行自动微分关系算法的关系引擎可以轻松扩展到非常大的数据集，并且在大规模分布式机器学习方面与最先进的专用系统相竞争。

    The relational data model was designed to facilitate large-scale data management and analytics. We consider the problem of how to differentiate computations expressed relationally. We show experimentally that a relational engine running an auto-differentiated relational algorithm can easily scale to very large datasets, and is competitive with state-of-the-art, special-purpose systems for large-scale distributed machine learning.
    
[^75]: 社交身体重组中的自适应协调

    Adaptive Coordination in Social Embodied Rearrangement. (arXiv:2306.00087v1 [cs.LG])

    [http://arxiv.org/abs/2306.00087](http://arxiv.org/abs/2306.00087)

    本文介绍了一项名为“社交重组”的任务，使用适应性的协调方法，在没有特权信息的情况下，两个机器人使用本地感知和自我参照观察来完成一个长期的任务。作者提出了一种行为多样性游戏的方法，可以帮助机器人在协调中生成更多样的行为，具有更强的适应性。

    

    我们介绍了“社交重组”任务，包括在多智能体环境下完成合作日常任务，如摆餐桌、整理房屋或卸下杂货。在社交重组中，两个机器人使用本地感知和自我参照观察来协调完成一个长期任务，没有关于环境的特权信息。我们研究了零射协调在这个任务中的应用，其中一个代理人与一个新的合作伙伴协调，模拟一个机器人与一个新的人类合作伙伴协调的场景。以前的零射协调方法在我们复杂和视觉丰富的环境中很难推广，进一步分析表明，它们在训练时无法生成多样化的协调行为。为了克服这个问题，我们提出了一种新的零射协调方法——行为多样性游戏(BDP)，通过一个可辨别性目标鼓励多样性。我们的结果表明，BDP学习到可以处理视觉协调和与各种代理人（包括人类）进行零射协调的自适应代理。

    We present the task of "Social Rearrangement", consisting of cooperative everyday tasks like setting up the dinner table, tidying a house or unpacking groceries in a simulated multi-agent environment. In Social Rearrangement, two robots coordinate to complete a long-horizon task, using onboard sensing and egocentric observations, and no privileged information about the environment. We study zero-shot coordination (ZSC) in this task, where an agent collaborates with a new partner, emulating a scenario where a robot collaborates with a new human partner. Prior ZSC approaches struggle to generalize in our complex and visually rich setting, and on further analysis, we find that they fail to generate diverse coordination behaviors at training time. To counter this, we propose Behavior Diversity Play (BDP), a novel ZSC approach that encourages diversity through a discriminability objective. Our results demonstrate that BDP learns adaptive agents that can tackle visual coordination, and zero-
    
[^76]: 人类对齐校准用于AI辅助决策制定

    Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])

    [http://arxiv.org/abs/2306.00074](http://arxiv.org/abs/2306.00074)

    本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。

    

    当使用二元分类器提供决策支持时，它通常提供标签预测和置信度值。然后，决策者应使用置信度值来校准对预测的信任程度。在这种情况下，人们经常认为置信度值应对预测标签与实际标签匹配的概率进行良好校准的估计。然而，多条实证证据表明，决策者难以使用这些置信度值很好地确定何时信任预测。本文的目标首先是理解为什么，然后研究如何构建更有用的置信度值。我们首先认为，在广泛类的效用函数中，存在数据分布，对于这些分布，理性决策者通常难以使用以上置信度值发现最佳决策政策——最佳的决策者需要人类对齐。然后，我们引入了一种基于主动询问决策者他们在所面临的二元分类任务的决策上的个人偏好的新方法来构造置信度值。我们表明，该方法产生的置信度值比使用标准置信度度量导致更好的决策。

    Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
    
[^77]: 量子机器学习的阴影

    Shadows of quantum machine learning. (arXiv:2306.00061v1 [quant-ph])

    [http://arxiv.org/abs/2306.00061](http://arxiv.org/abs/2306.00061)

    量子机器学习模型需要使用量子计算机进行评估，但我们提出在训练完后，使用量子计算机生成一个经典阴影模型来计算函数的经典计算近似，避免了对量子计算机的需求。

    

    量子机器学习经常被认为是利用量子计算机解决实际问题的最有前途的应用之一。然而，阻碍其在实践中广泛使用的主要障碍是这些模型即使在训练过程后，仍需要访问量子计算机才能对新数据进行评估。为解决这个问题，我们建议在量子模型的训练阶段之后，量子计算机可以用来生成我们所谓的该模型的“经典阴影”，即已学习函数的经典计算近似。虽然最近的研究已经探讨了这个想法并提出了构建这种影子模型的方法，但它们也提出了一个完全经典模型可能代替的可能性，从而首先回避了量子计算机的需要。本文采用新的方法，基于量子线性模型和经典阴影重构的框架来定义阴影模型。

    Quantum machine learning is often highlighted as one of the most promising uses for a quantum computer to solve practical problems. However, a major obstacle to the widespread use of quantum machine learning models in practice is that these models, even once trained, still require access to a quantum computer in order to be evaluated on new data. To solve this issue, we suggest that following the training phase of a quantum model, a quantum computer could be used to generate what we call a classical shadow of this model, i.e., a classically computable approximation of the learned function. While recent works already explore this idea and suggest approaches to construct such shadow models, they also raise the possibility that a completely classical model could be trained instead, thus circumventing the need for a quantum computer in the first place. In this work, we take a novel approach to define shadow models based on the frameworks of quantum linear models and classical shadow tomogr
    
[^78]: 进化优化中的彩票票现象：稀疏且无需反向传播训练

    Lottery Tickets in Evolutionary Optimization: On Sparse Backpropagation-Free Trainability. (arXiv:2306.00045v1 [cs.NE])

    [http://arxiv.org/abs/2306.00045](http://arxiv.org/abs/2306.00045)

    本文证明了进化策略算法具有高度稀疏可训练的初始参数，并比较了与梯度下降算法稀疏训练的差异。进化策略算法可以探索各种不同的、平坦的局部最优解，并且不会保留线性模式的连接性。

    

    彩票票现象是否只存在于梯度下降算法中，还是可以推广到进化优化算法中？本文证明了进化策略算法具有高度稀疏可训练的初始参数，并比较了与梯度下降算法稀疏训练的差异。我们提出了一种新的信噪比迭代剪枝过程，将损失曲率信息融入到网络剪枝步骤中，可以发现与梯度下降相比，黑盒进化算法方法有可能发现更稀疏且可训练的网络初始参数。此外，我们发现这些初始参数包含了归纳偏见，可以在不同的进化策略任务及梯度下降训练中进行传递。最后，我们比较了不同优化算法和稀疏水平产生的局部最优解，与梯度下降算法不同，进化策略算法可以探索各种不同的、平坦的局部最优解，并且不会保留线性模式的连接性。

    Is the lottery ticket phenomenon an idiosyncrasy of gradient-based training or does it generalize to evolutionary optimization? In this paper we establish the existence of highly sparse trainable initializations for evolution strategies (ES) and characterize qualitative differences compared to gradient descent (GD)-based sparse training. We introduce a novel signal-to-noise iterative pruning procedure, which incorporates loss curvature information into the network pruning step. This can enable the discovery of even sparser trainable network initializations when using black-box evolution as compared to GD-based optimization. Furthermore, we find that these initializations encode an inductive bias, which transfers across different ES, related tasks and even to GD-based training. Finally, we compare the local optima resulting from the different optimization paradigms and sparsity levels. In contrast to GD, ES explore diverse and flat local optima and do not preserve linear mode connectivi
    
[^79]: 如何构建完美的反欺骗对策及比瞎猜还差的对策：关于快捷学习的警告（arXiv:2306.00044v1 [cs.LG]）

    How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning. (arXiv:2306.00044v1 [cs.LG])

    [http://arxiv.org/abs/2306.00044](http://arxiv.org/abs/2306.00044)

    本研究提出了一种通用方法来识别深度学习为基础的反欺骗对策中的快捷方式，并在实验中证明了快捷方式的存在，分析了如何影响类条件得分统计信息。

    

    快捷学习，或称为“聪明汉斯效应”，指的是学习代理（例如深度神经网络）学习数据中存在的伪相关性，导致有偏差的模型。我们在寻找深度学习为基础的反欺骗对策（CMs）中发现了快捷方式，这些对策用于预测给定话语是否被欺骗。虽然先前的工作已经涉及特定的数据伪装技巧，例如静默，但尚未探索分析CMs的快捷学习的通用规范框架。在本研究中，我们提出了一种通用方法来识别快捷方式，包括对训练和测试方面的系统干预，包括“近乎完美”和“比瞎猜还差”（标签翻转）的边界情况。通过使用从经典到最先进的三种不同模型，我们展示了在五个模拟条件下存在快捷学习的情况。我们使用回归模型分析结果，以了解偏差如何影响类条件得分统计信息。

    Shortcut learning, or `Clever Hans effect` refers to situations where a learning agent (e.g., deep neural networks) learns spurious correlations present in data, resulting in biased models. We focus on finding shortcuts in deep learning based spoofing countermeasures (CMs) that predict whether a given utterance is spoofed or not. While prior work has addressed specific data artifacts, such as silence, no general normative framework has been explored for analyzing shortcut learning in CMs. In this study, we propose a generic approach to identifying shortcuts by introducing systematic interventions on the training and test sides, including the boundary cases of `near-perfect` and `worse than coin flip` (label flip). By using three different models, ranging from classic to state-of-the-art, we demonstrate the presence of shortcut learning in five simulated conditions. We analyze the results using a regression model to understand how biases affect the class-conditional score statistics.
    
[^80]: 基于图形的方法与特定分布距离相结合的对抗攻击检测

    Graph-based methods coupled with specific distributional distances for adversarial attack detection. (arXiv:2306.00042v1 [cs.LG])

    [http://arxiv.org/abs/2306.00042](http://arxiv.org/abs/2306.00042)

    本论文提出了利用图形的方法结合特定分布距离来检测对抗性攻击，通过研究神经网络的图结构，介绍了用于预测和解释对抗性攻击的特定测量方法，这有助于研究对抗性攻击的内在机理。

    

    人工神经网络容易被精心扰动的输入所欺骗，导致严重的误分类。这些“对抗性”攻击已成为广泛研究的焦点。同样，对抗攻击的检测和防御也有大量研究。我们从图的角度介绍了一种新的方法来检测和解释对抗性攻击。对于图像，无论是良性的还是对抗性的，我们研究了神经网络的架构如何引入一个相关的图形。我们研究了这个图形，并引入了用于预测和解释对抗性攻击的特定测量方法。我们展示了基于图形的方法有助于研究对抗性攻击的内在机理。

    Artificial neural networks are prone to being fooled by carefully perturbed inputs which cause an egregious misclassification. These \textit{adversarial} attacks have been the focus of extensive research. Likewise, there has been an abundance of research in ways to detect and defend against them. We introduce a novel approach of detection and interpretation of adversarial attacks from a graph perspective. For an image, benign or adversarial, we study how a neural network's architecture can induce an associated graph. We study this graph and introduce specific measures used to predict and interpret adversarial attacks. We show that graphs-based approaches help to investigate the inner workings of adversarial attacks.
    
[^81]: 基于因果干预的药物靶点相互作用可信度测量方法的研究与实现

    Research And Implementation Of Drug Target Interaction Confidence Measurement Method Based On Causal Intervention. (arXiv:2306.00041v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.00041](http://arxiv.org/abs/2306.00041)

    本研究提出了一种基于因果干预的置信度测量方法来提高药物靶点相互作用预测模型的准确性，着重解决了知识图谱嵌入模型的不足问题。

    

    药物与靶点相互作用（DTI）的识别和发现是药物研究和开发中的重要步骤，可以帮助科学家发现新药并加速开发过程。近年来，知识图谱和相关的知识图谱嵌入（KGE）模型在药物发现领域迅速发展并表现出良好性能。在药物靶点识别任务中，模型的真实性和准确性不足会导致误判率增加和药物开发效率低下。为解决以上问题，本研究聚焦于以知识映射为核心技术的药物靶点链接预测问题，并采用基于因果干预的置信度测量方法来测量三元组得分，从而提高药物靶点相互作用预测模型的准确性。通过在不同的KGE模型上与传统的Softmax和Sigmod置信度测量方法进行比较，结果表明...

    The identification and discovery of drug-target Interaction (DTI) is an important step in the field of Drug research and development, which can help scientists discover new drugs and accelerate the development process. KnowledgeGraph and the related knowledge graph Embedding (KGE) model develop rapidly and show good performance in the field of drug discovery in recent years. In the task of drug target identification, the lack of authenticity and accuracy of the model will lead to the increase of misjudgment rate and the low efficiency of drug development. To solve the above problems, this study focused on the problem of drug target link prediction with knowledge mapping as the core technology, and adopted the confidence measurement method based on causal intervention to measure the triplet score, so as to improve the accuracy of drug target interaction prediction model. By comparing with the traditional Softmax and Sigmod confidence measurement methods on different KGE models, the resu
    
[^82]: 评估性能预测模型的普适性

    Assessing the Generalizability of a Performance Predictive Model. (arXiv:2306.00040v1 [cs.LG])

    [http://arxiv.org/abs/2306.00040](http://arxiv.org/abs/2306.00040)

    本研究提出一种工作流程来评估性能预测模型的泛化能力，可以在不同的基准测试套件上进行验证。

    

    自动算法选择和配置的关键组成部分是优秀的预测性能模型，而在大多数情况下，这是通过监督式机器学习方法实现的。该模型使用问题实例的特征表示作为输入数据，并预测其表现所使用的算法性能。然而，常见的机器学习模型很难对未被训练数据覆盖的特征表示进行准确的预测，导致对未见问题的泛化能力较差。在本研究中，我们提出了一种工作流程，用于评估在一个基准测试套件上训练的预测性能模型对另一个基准测试套件的普适性。我们通过在基准测试套件之间训练预测性能模型来测试工作流程，并发现景观特征空间中的普适性模式在性能空间中得到了体现。

    A key component of automated algorithm selection and configuration, which in most cases are performed using supervised machine learning (ML) methods is a good-performing predictive model. The predictive model uses the feature representation of a set of problem instances as input data and predicts the algorithm performance achieved on them. Common machine learning models struggle to make predictions for instances with feature representations not covered by the training data, resulting in poor generalization to unseen problems. In this study, we propose a workflow to estimate the generalizability of a predictive model for algorithm performance, trained on one benchmark suite to another. The workflow has been tested by training predictive models across benchmark suites and the results show that generalizability patterns in the landscape feature space are reflected in the performance space.
    
[^83]: 基于联邦学习的代码异味检测方法(FedCSD)

    FedCSD: A Federated Learning Based Approach for Code-Smell Detection. (arXiv:2306.00038v1 [cs.SE])

    [http://arxiv.org/abs/2306.00038](http://arxiv.org/abs/2306.00038)

    本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。该方法在三个实验中分别使用不同的数据集，实现了高精度的检测效果，并且比集中式和交叉验证方法具有更好的性能表现。

    

    本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。通过三个实验来支持这些断言，这些实验利用了三个手动验证的数据集，来检测和研究不同的代码异味场景。

    This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\%) where fewer smells exist in the training dataset, which has a noticeab
    
[^84]: ROSARL: 基于奖励的安全强化学习

    ROSARL: Reward-Only Safe Reinforcement Learning. (arXiv:2306.00035v1 [cs.LG])

    [http://arxiv.org/abs/2306.00035](http://arxiv.org/abs/2306.00035)

    ROSARL提出了一种基于奖励的安全强化学习方法，通过定义“Minmax惩罚”确定智能体在达到不安全状态时所允许的奖励上限，并考虑环境的可控性和直径来获得这个上限。

    

    强化学习中一个重要的问题是设计能够在环境中安全学习完成任务的智能体。常见的解决方法是由人类专家定义一个奖励函数中的惩罚或要最小化的成本，以达到不走入危险状态的目的。然而，太小的惩罚可能会导致智能体进入不安全的状态，而太大的惩罚则会增加收敛时间。此外，设计奖励或成本函数的难度随着问题的复杂度而增加。因此，对于给定的环境和一组不安全状态，我们希望找到在不考虑任务奖励的情况下，最小化到达不安全状态的概率的最高奖励上限。我们称这个确切的上限为“Minmax惩罚”，并展示了可以通过考虑环境的可控性和直径来获得它。我们提供了一个简单的实践模型。

    An important problem in reinforcement learning is designing agents that learn to solve tasks safely in an environment. A common solution is for a human expert to define either a penalty in the reward function or a cost to be minimised when reaching unsafe states. However, this is non-trivial, since too small a penalty may lead to agents that reach unsafe states, while too large a penalty increases the time to convergence. Additionally, the difficulty in designing reward or cost functions can increase with the complexity of the problem. Hence, for a given environment with a given set of unsafe states, we are interested in finding the upper bound of rewards at unsafe states whose optimal policies minimise the probability of reaching those unsafe states, irrespective of task rewards. We refer to this exact upper bound as the "Minmax penalty", and show that it can be obtained by taking into account both the controllability and diameter of an environment. We provide a simple practical model
    
[^85]: 自我验证改善少样本临床信息提取

    Self-Verification Improves Few-Shot Clinical Information Extraction. (arXiv:2306.00024v1 [cs.CL])

    [http://arxiv.org/abs/2306.00024](http://arxiv.org/abs/2306.00024)

    本文探索了一种利用自我验证的通用缓解框架，该框架利用语言模型为其自我提取提供权威性并检查其自己的输出。在临床信息提取任务中，该方法能够显著提高各种LLMs的准确性，并展示了极好的可解释性和认知增强效果。

    

    从非结构化文本中提取患者信息是卫生决策支持和临床研究的关键任务。大型语言模型（LLM）展示了通过少量上下文学习加速临床管理的潜力，相较于需要更昂贵的人工注释的监督学习。然而，尽管现代LLM（如GPT-4）取得了巨大进步，它们仍然存在精度和可解释性等问题，特别是在关键任务领域（如健康）。在这里，我们通过自我验证，探索了一种通用的缓解框架，该框架利用LLM为其自我提取提供权威性并检查自己的输出。这得益于验证和生成之间的不对称性，后者通常比前者容易得多。实验结果表明，我们的方法在标准临床信息提取任务中，对于各种LLM都能持续提高准确性。此外，自我验证产生了极好的可解释性和认知增强效果。

    Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning which requires much more costly human annotations. However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health. Here, we explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This is made possible by the asymmetry between verification and generation, where the latter is often much easier than the former. Experimental results show that our method consistently improves accuracy for various LLMs in standard clinical information extraction tasks. Additionally, self-verification yields
    
[^86]: 使用机器学习算法预测心脏病并减少调查时间

    Predicting Heart Disease and Reducing Survey Time Using Machine Learning Algorithms. (arXiv:2306.00023v1 [cs.LG])

    [http://arxiv.org/abs/2306.00023](http://arxiv.org/abs/2306.00023)

    本研究使用机器学习算法对美国CDC的心脏病调查进行了准确性调查，确定了最相关的问题子集，并证明使用有限的问题集仍然可以保持高的预测准确率，并显著减少调查时间。

    

    目前，许多研究人员和分析师正致力于改善各种疾病的医学诊断。心脏病是一种常见疾病，可视为全球重要死因。早期发现心脏病有助于显著降低心力衰竭的风险。因此，疾病控制和预防中心每年从400,000多名参与者中进行与健康相关的电话调查。然而，人们对于预测心脏病的数据可靠性以及是否所有调查问题都具有强相关性提出了一些担忧。本研究旨在利用支持向量机和逻辑回归等多种机器学习技术，以调查美国疾病控制和预防中心的心脏病调查准确性。此外，我们使用各种特征选择方法来确定可以用于预测心脏病的最相关的问题子集。为了得出有力的结论，我们在全国健康和营养调查数据集上测试了开发的模型。结果表明，使用有限的问题集可以显著缩短调查时间，同时仍然保持高的心脏病预测准确率。

    Currently, many researchers and analysts are working toward medical diagnosis enhancement for various diseases. Heart disease is one of the common diseases that can be considered a significant cause of mortality worldwide. Early detection of heart disease significantly helps in reducing the risk of heart failure. Consequently, the Centers for Disease Control and Prevention (CDC) conducts a health-related telephone survey yearly from over 400,000 participants. However, several concerns arise regarding the reliability of the data in predicting heart disease and whether all of the survey questions are strongly related. This study aims to utilize several machine learning techniques, such as support vector machines and logistic regression, to investigate the accuracy of the CDC's heart disease survey in the United States. Furthermore, we use various feature selection methods to identify the most relevant subset of questions that can be utilized to forecast heart conditions. To reach a robus
    
[^87]: GPT4GEO：一个语言模型如何看待世界地理

    GPT4GEO: How a Language Model Sees the World's Geography. (arXiv:2306.00020v1 [cs.CL])

    [http://arxiv.org/abs/2306.00020](http://arxiv.org/abs/2306.00020)

    研究调查了GPT-4的地理知识水平，并探讨其在地理数据分析等领域的应用潜力。

    

    大型语言模型（LLM）在涉及问题回答、生成连贯文本和代码的广泛任务中展现出了卓越的能力。全面理解LLM的优点和缺点对于安全、下游应用和性能改进都有益处。在本研究中，我们调查了GPT-4获得事实地理知识的程度，并能否将这些知识用于解释性推理，这对于涉及地理数据的应用（如地理空间分析、供应链管理和灾难响应）尤其重要。为此，我们设计并进行了一系列多样化的实验，从定位、距离和高度估计等事实任务开始，到生成国家轮廓和旅游网络、在约束条件下寻找路线和供应链分析等更复杂的问题。我们提供了GPT-4（没有插件或Internet访问）了解和不了解世界地理的广泛描述，并讨论了在地理空间领域应用的影响。

    Large language models (LLMs) have shown remarkable capabilities across a broad range of tasks involving question answering and the generation of coherent text and code. Comprehensively understanding the strengths and weaknesses of LLMs is beneficial for safety, downstream applications and improving performance. In this work, we investigate the degree to which GPT-4 has acquired factual geographic knowledge and is capable of using this knowledge for interpretative reasoning, which is especially important for applications that involve geographic data, such as geospatial analysis, supply chain management, and disaster response. To this end, we design and conduct a series of diverse experiments, starting from factual tasks such as location, distance and elevation estimation to more complex questions such as generating country outlines and travel networks, route finding under constraints and supply chain analysis. We provide a broad characterisation of what GPT-4 (without plugins or Interne
    
[^88]: 将领域知识纳入深度神经网络的离散选择模型中

    Incorporating Domain Knowledge in Deep Neural Networks for Discrete Choice Models. (arXiv:2306.00016v1 [cs.LG])

    [http://arxiv.org/abs/2306.00016](http://arxiv.org/abs/2306.00016)

    本文提出了一种框架，将领域知识和先验信念纳入离散选择模型（DCM）的深度神经网络（DNN）中，以扩展数据驱动方法在DCM中的潜力。所提出的方法既具有可解释性，又保留了DCM的优点和DNNs的灵活性，并优于现有的最先进方法。

    

    离散选择模型（DCM）被广泛应用于旅行需求分析中，作为一种强大的理论计量框架来理解和预测选择行为。DCM是随机效用模型（RUM），其主要优点是可解释性。然而，估计这些模型的核心要求是先验指定相关的效用函数，使其对模型人员的主观信念敏感。最近，机器学习（ML）方法已经成为学习DCM中未观测到的非线性关系的一种有前途的方法。然而，ML模型被认为是“黑盒子”，可能与预期的关系不符。本文提出了一个框架，通过约束支持开发并将领域知识和先验信念纳入可解释模型中，以扩展数据驱动方法在DCM中的潜力。所提出的框架包括代表所需关系的伪数据样本和将DCM转换为具有软约束的深度神经网络（DNN）的对数似然函数。由此得到的模型既具有可解释性，又保留了RUM的优点和DNNs的灵活性。实验结果表明，所提出的方法优于现有的最先进方法。

    Discrete choice models (DCM) are widely employed in travel demand analysis as a powerful theoretical econometric framework for understanding and predicting choice behaviors. DCMs are formed as random utility models (RUM), with their key advantage of interpretability. However, a core requirement for the estimation of these models is a priori specification of the associated utility functions, making them sensitive to modelers' subjective beliefs. Recently, machine learning (ML) approaches have emerged as a promising avenue for learning unobserved non-linear relationships in DCMs. However, ML models are considered "black box" and may not correspond with expected relationships. This paper proposes a framework that expands the potential of data-driven approaches for DCM by supporting the development of interpretable models that incorporate domain knowledge and prior beliefs through constraints. The proposed framework includes pseudo data samples that represent required relationships and a l
    
[^89]: GraphCleaner: 在流行的图学习基准数据集中检测错误标注的样本

    GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks. (arXiv:2306.00015v1 [cs.LG])

    [http://arxiv.org/abs/2306.00015](http://arxiv.org/abs/2306.00015)

    该论文提出了一种后期方法GraphCleaner，用于在流行的图学习基准数据集中检测和纠正错误标注节点。GraphCleaner 组合了合成错误标注数据集的生成和邻域感知错误标注检测两种新颖思想，实证评估结果表明其平均F1得分提高了0.14。

    

    研究发现，流行的文本，图像和音频数据集中存在标签错误，这严重影响了机器学习算法的开发和评估的可靠性。尽管针对通用数据类型（如图像和文本）的质量改进工作有所增加，但在图数据中检测错误标注的问题仍然未被充分探讨。该论文旨在探索流行的实际图数据集中的错误标注问题，并提出了GraphCleaner，一种后期方法，用于检测和纠正这些图数据集中的错误标注节点。GraphCleaner结合了两种新颖的思想：1）合成错误标注数据集的生成，旨在生成逼真的错误标注；2）邻域感知错误标注检测，其中利用了标签和基分类器预测中的邻域依赖性。在6个基准数据集和6个实验设置上的实证评估表明，GraphCleaner优于最接近的基准线，并具有平均F1得分改进0.14。

    Label errors have been found to be prevalent in popular text, vision, and audio datasets, which heavily influence the safe development and evaluation of machine learning algorithms. Despite increasing efforts towards improving the quality of generic data types, such as images and texts, the problem of mislabel detection in graph data remains underexplored. To bridge the gap, we explore mislabelling issues in popular real-world graph datasets and propose GraphCleaner, a post-hoc method to detect and correct these mislabelled nodes in graph datasets. GraphCleaner combines the novel ideas of 1) Synthetic Mislabel Dataset Generation, which seeks to generate realistic mislabels; and 2) Neighborhood-Aware Mislabel Detection, where neighborhood dependency is exploited in both labels and base classifier predictions. Empirical evaluations on 6 datasets and 6 experimental settings demonstrate that GraphCleaner outperforms the closest baseline, with an average improvement of 0.14 in F1 score, and
    
[^90]: 面向预训练语言模型的通用量化方法PreQuant

    PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models. (arXiv:2306.00014v1 [cs.CL])

    [http://arxiv.org/abs/2306.00014](http://arxiv.org/abs/2306.00014)

    本文提出了一种新颖的“量化前微调”框架 PreQuant，可用于预训练语言模型的通用量化，有效降低了模型的复杂度和使用成本。

    

    近年来，基于transformer的预训练语言模型（PLMs）已经在许多NLP应用中占据主导地位，但这些模型部署复杂、使用昂贵，因此有效压缩大规模PLMs变得越来越重要。量化是一种可行的解决方案，它用低比特定点格式表示高精度张量。本文提出了一种新颖的“量化前微调”框架PreQuant，它与各种量化策略兼容，并结合“异常值感知参数高效微调”进行校正。

    While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision tensors with low-bit fix-point format, is a viable solution. However, most existing quantization methods are task-specific, requiring customized training and quantization with a large number of trainable parameters on each individual task. Inspired by the observation that the over-parameterization nature of PLMs makes it possible to freeze most of the parameters during the fine-tuning stage, in this work, we propose a novel ``quantize before fine-tuning'' framework, PreQuant, that differs from both quantization-aware training and post-training quantization. PreQuant is compatible with various quantization strategies, with outlier-aware parameter-efficient fine-tuning incorporated to correct 
    
[^91]: 癌症实体的关联和分类的机器学习方法

    Machine Learning Approach for Cancer Entities Association and Classification. (arXiv:2306.00013v1 [cs.CL])

    [http://arxiv.org/abs/2306.00013](http://arxiv.org/abs/2306.00013)

    本研究利用命名实体识别和文本分类的机器学习方法自动从大量生物医学文献中提取癌症相关实体和实体间关系，以帮助推进癌症研究进展。

    

    根据世界卫生组织（WHO）的数据，癌症是全球第二大死因。不同类型癌症的科学研究以每年发布大量的研究文章的速度不断增长。与基因相关的药物、诊断、风险、症状、治疗等的信息和知识是帮助探索和推进癌症研究进展的重要因素。手动筛选这么大量的文章非常费时费力，很难制定任何假设。本研究使用两种最为重要的自然语言处理（NLP）功能，实体识别和文本分类，从生物医学文献中发现知识。命名实体识别（NER）借助用户友好的界面和内置字典识别并提取与癌症相关的预定义实体。文本分类采用机器学习方法，帮助探究癌症实体之间的关系。

    According to the World Health Organization (WHO), cancer is the second leading cause of death globally. Scientific research on different types of cancers grows at an ever-increasing rate, publishing large volumes of research articles every year. The insight information and the knowledge of the drug, diagnostics, risk, symptoms, treatments, etc., related to genes are significant factors that help explore and advance the cancer research progression. Manual screening of such a large volume of articles is very laborious and time-consuming to formulate any hypothesis. The study uses the two most non-trivial NLP, Natural Language Processing functions, Entity Recognition, and text classification to discover knowledge from biomedical literature. Named Entity Recognition (NER) recognizes and extracts the predefined entities related to cancer from unstructured text with the support of a user-friendly interface and built-in dictionaries. Text classification helps to explore the insights into the 
    
[^92]: 基于图神经网络的时空数据方法及应用：综述

    Graph Neural Network for spatiotemporal data: methods and applications. (arXiv:2306.00012v1 [cs.LG])

    [http://arxiv.org/abs/2306.00012](http://arxiv.org/abs/2306.00012)

    本文综述了基于图神经网络的时空数据方法及应用，并提供了一个GNN的分类及其应用领域。这个综述突出了使用GNN分析时空数据的强大工具及其局限性，并指出未来研究方向和目前面临的挑战。

    

    在大数据时代，蕴含着空间和时间信息的数据的可用性急剧增加，为天气预报、自然灾害管理、智能交通系统和精准农业等应用提供了宝贵的洞见。图神经网络是一种用于对具有依赖关系的数据（如空间和时间依赖关系）进行建模和理解的强大工具。已有大量工作致力于利用GNN解决时空数据中的复杂空间和时间依赖关系。然而，时空数据的强跨学科性质导致众多特定于不同应用领域的GNN变体的出现。尽管这些技术通常适用于各个领域，但由于缺乏关于时空数据GNN的全面文献综述，因此跨领域参考这些方法仍然是必要的，但也带来了挑战。本文综述了现有关于基于图神经网络的时空数据研究，并对其优点、局限性和潜在应用进行了讨论。我们也提供了未来研究的路线图和强调了一些未解决的挑战。

    In the era of big data, there has been a surge in the availability of data containing rich spatial and temporal information, offering valuable insights into dynamic systems and processes for applications such as weather forecasting, natural disaster management, intelligent transport systems, and precision agriculture. Graph neural networks (GNNs) have emerged as a powerful tool for modeling and understanding data with dependencies to each other such as spatial and temporal dependencies. There is a large amount of existing work that focuses on addressing the complex spatial and temporal dependencies in spatiotemporal data using GNNs. However, the strong interdisciplinary nature of spatiotemporal data has created numerous GNNs variants specifically designed for distinct application domains. Although the techniques are generally applicable across various domains, cross-referencing these methods remains essential yet challenging due to the absence of a comprehensive literature review on GN
    
[^93]: 自监督方法用于高维数据聚类评估

    A Self-Supervised Approach for Cluster Assessment of High-Dimensional Data. (arXiv:2306.00011v1 [cs.LG])

    [http://arxiv.org/abs/2306.00011](http://arxiv.org/abs/2306.00011)

    本文提出了一种利用自监督深度神经网络生成代表性嵌入的方法，用于评估复杂图像数据中的聚类结构。

    

    估计数据集中聚类数量和基础聚类结构是一个至关重要的任务。现实世界中的数据通常是无标签、复杂且高维的，这使得传统聚类算法难以表现良好。为解决这个问题，我们提出了一个基于深度学习的框架，用于评估复杂图像数据中的聚类结构。首先，我们的框架使用自监督深度神经网络生成复杂数据的代表性嵌入，然后将这些低维嵌入馈送到VAT/iVAT中。

    Estimating the number of clusters and underlying cluster structure in a dataset is a crucial task. Real-world data are often unlabeled, complex and high-dimensional, which makes it difficult for traditional clustering algorithms to perform well. In recent years, a matrix reordering based algorithm, called "visual assessment of tendency" (VAT), and its variants have attracted many researchers from various domains to estimate the number of clusters and inherent cluster structure present in the data. However, these algorithms fail when applied to high-dimensional data due to the curse of dimensionality, as they rely heavily on the notions of closeness and farness between data points. To address this issue, we propose a deep-learning based framework for cluster structure assessment in complex, image datasets. First, our framework generates representative embeddings for complex data using a self-supervised deep neural network, and then, these low-dimensional embeddings are fed to VAT/iVAT a
    
[^94]: 简单形式映射神经网络中的可解释性

    Explainability in Simplicial Map Neural Networks. (arXiv:2306.00010v1 [cs.LG])

    [http://arxiv.org/abs/2306.00010](http://arxiv.org/abs/2306.00010)

    本文提出了简单形式映射神经网络（SMNN）的训练过程和替代凸多面体的方法，并且首次引入了 SMNN 的可解释性能力。

    

    简单形式映射神经网络（SMNN）是基于拓扑学的神经网络，具有普适逼近能力和在适当条件下对抗性示例的鲁棒性。然而，在高维中应用 SMNN 存在一些瓶颈，首先没有定义 SMNN 的训练过程，其次对于输入数据集需要构建一个包围凸多面体。本文提出了基于给定数据集的支持子集和投影到超球面的方法作为替代凸多面体的 SMNN 训练过程，并首次引入了 SMNN 的可解释性能力。

    Simplicial map neural networks (SMNNs) are topology-based neural networks with interesting properties such as universal approximation capability and robustness to adversarial examples under appropriate conditions. However, SMNNs present some bottlenecks for their possible application in high dimensions. First, no SMNN training process has been defined so far. Second, SMNNs require the construction of a convex polytope surrounding the input dataset. In this paper, we propose a SMNN training procedure based on a support subset of the given dataset and a method based on projection to a hypersphere as a replacement for the convex polytope construction. In addition, the explainability capacity of SMNNs is also introduced for the first time in this paper.
    
[^95]: 图形探索的重要性：在微信推荐系统中提高个人层面和系统层面的多样性

    Graph Exploration Matters: Improving both individual-level and system-level diversity in WeChat Feed Recommender. (arXiv:2306.00009v1 [cs.LG])

    [http://arxiv.org/abs/2306.00009](http://arxiv.org/abs/2306.00009)

    该论文提出了一种基于图形探索的方法，以捕捉用户对不同项目和项目自身属性的固有偏好，从而提高微信推荐系统中个人层面和系统层面的多样性。

    

    实际工业推荐系统的候选项生成（检索）、排序和再排序大致分为三个阶段。个人层面的多样性和系统层面的多样性对于工业推荐系统同样重要。前者关注每个用户的体验，而后者关注用户之间的差异。基于图形的检索策略不可避免地被重度用户和热门项目所劫持，导致用户之间的候选项收敛和系统层面的多样性缺乏。同时，在再排序阶段，采用析因点过程（DPP）来增加个人层面的多样性。DPP高度依赖于项目的语义信息，容易受到标题党和不准确属性的影响。此外，大多数研究仅关注其中一种多样性层面，并忽略了实际推荐系统中不同阶段之间的相互影响。我们认为应该将个人层面的多样性和系统层面的多样性视为一个整体，并提出一种基于图形探索的方法，通过探索包含用户和项目的社交图形来提高两种多样性，从而捕捉用户对不同项目和项目自身属性的固有偏好。我们在微信资讯推荐数据集上进行了实验，结果表明我们的方法在个人层面和系统层面的多样性以及用户参与度方面均优于强基线。

    There are roughly three stages in real industrial recommendation systems, candidates generation (retrieval), ranking and reranking. Individual-level diversity and system-level diversity are both important for industrial recommender systems. The former focus on each single user's experience, while the latter focus on the difference among users. Graph-based retrieval strategies are inevitably hijacked by heavy users and popular items, leading to the convergence of candidates for users and the lack of system-level diversity. Meanwhile, in the reranking phase, Determinantal Point Process (DPP) is deployed to increase individual-level diverisity. Heavily relying on the semantic information of items, DPP suffers from clickbait and inaccurate attributes. Besides, most studies only focus on one of the two levels of diversity, and ignore the mutual influence among different stages in real recommender systems. We argue that individual-level diversity and system-level diversity should be viewed a
    
[^96]: Brainformers：以效率换取简洁性

    Brainformers: Trading Simplicity for Efficiency. (arXiv:2306.00008v1 [cs.LG])

    [http://arxiv.org/abs/2306.00008](http://arxiv.org/abs/2306.00008)

    Brainformers 是一个新的深度神经网络模型，它通过使用多样层级的结构完善了 Transformer 的设计缺陷，具有更高效的训练收敛和更快的步骤时间，表现出更优秀的性能。

    

    Transformer 是自然语言处理和计算机视觉的最近成功的核心技术。Transformer 具有一个几乎统一的骨架，其中层次在前馈和自注意力之间交替以建立深度网络。在本文中，我们研究了这种设计选择，并发现更复杂的块可以更高效地完成任务。根据这个发现，我们提出了一个复杂的块，称为 Brainformer，它由各种形式的层归一化和激活函数、稀疏门控前馈层、密集前馈层、注意力层等多样层级组成。在质量和效率方面，Brainformer 总是优于现有的稠密和稀疏 Transformer。一个具有 80 亿个每个标记激活参数的 Brainformer 模型，相比于其 GLaM 对应物，表现出 2 倍更快的训练收敛和 5 倍更快的步骤时间。在下游任务评估中，Brainformer 也表现得更优秀。

    Transformers are central to recent successes in natural language processing and computer vision. Transformers have a mostly uniform backbone where layers alternate between feed-forward and self-attention in order to build a deep network. Here we investigate this design choice and find that more complex blocks that have different permutations of layer primitives can be more efficient. Using this insight, we develop a complex block, named Brainformer, that consists of a diverse sets of layers such as sparsely gated feed-forward layers, dense feed-forward layers, attention layers, and various forms of layer normalization and activation functions. Brainformer consistently outperforms the state-of-the-art dense and sparse Transformers, in terms of both quality and efficiency. A Brainformer model with 8 billion activated parameters per token demonstrates 2x faster training convergence and 5x faster step time compared to its GLaM counterpart. In downstream task evaluation, Brainformer also de
    
[^97]: 葡语法律语义文本相似性数据集：比较弱监督和注释过程方法

    Datasets for Portuguese Legal Semantic Textual Similarity: Comparing weak supervision and an annotation process approaches. (arXiv:2306.00007v1 [cs.CL])

    [http://arxiv.org/abs/2306.00007](http://arxiv.org/abs/2306.00007)

    本文提供了四个法律领域的数据集，其中两个未标记，另外两个使用启发式标签进行了标记，旨在用于文本语义相似性任务。研究比较了弱监督和传统注释过程方法，表明弱监督可以达到与传统方法相当的性能。

    

    巴西司法机关工作量大，导致司法程序时间长。巴西国家司法委员会在469/2022号决议中制定了数字化文件和流程的正式指导方针，开拓了在法律领域中使用自动技术以帮助日常任务的可能性，尤其是在大量文本的情况下。人工智能技术允许从文本数据中处理和提取有用信息，潜在地加快该过程。然而，多个人工智能技术所需的法律领域数据集很少且难以获得，因为它们需要专家的标签。为了解决这一挑战，本文为法律领域贡献了四个数据集，其中两个具有文档和元数据但未标记，另外两个则标记了启发式标签，旨在用于文本语义相似性任务。此外，为了评估使用弱监督创建语义文本相似性标记数据集的有效性，我们将其与传统的注释过程进行了比较。结果表明，弱监督可以是一个可行的选择，达到了与传统方法相当的性能。

    The Brazilian judiciary has a large workload, resulting in a long time to finish legal proceedings. Brazilian National Council of Justice has established in Resolution 469/2022 formal guidance for document and process digitalization opening up the possibility of using automatic techniques to help with everyday tasks in the legal field, particularly in a large number of texts yielded on the routine of law procedures. Notably, Artificial Intelligence (AI) techniques allow for processing and extracting useful information from textual data, potentially speeding up the process. However, datasets from the legal domain required by several AI techniques are scarce and difficult to obtain as they need labels from experts. To address this challenge, this article contributes with four datasets from the legal domain, two with documents and metadata but unlabeled, and another two labeled with a heuristic aiming at its use in textual semantic similarity tasks. Also, to evaluate the effectiveness of 
    
[^98]: 截断亲和力最大化：用于图形异常监测的单类同型建模

    Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])

    [http://arxiv.org/abs/2306.00006](http://arxiv.org/abs/2306.00006)

    本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。

    

    我们在现实世界的图形异常监测（GAD）数据集中经常发现一种普遍的属性......本文提出了一种新的无监督异常评分度量 - 当前节点亲和力......我们进一步提出了截断亲和力最大化 (TAM)，该方法通过最大化与_neighbors的本地亲和力来学习量身定制的节点表示。本文所提方法在原始图形结构上进行优化，可以进行双重One-Class的GAD。

    One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
    
[^99]: 通过监督式注意力多实例学习从多视角超声图像检测心脏病

    Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning. (arXiv:2306.00003v1 [eess.IV])

    [http://arxiv.org/abs/2306.00003](http://arxiv.org/abs/2306.00003)

    本研究提出了一种基于监督式注意力多实例学习的方法，可以自动分析超声图像，实现AS的精确筛查且精度优于传统机器学习和深度学习方法。

    

    主动脉瓣狭窄(AS)是一种导致严重发病率和死亡率的退行性瓣膜疾病。这种情况经常被低估和低治疗。在临床实践中，AS是通过超声心动图的专家审查来诊断的，这会产生数十个下肺采样的超声图像。只有一些视图显示主动脉瓣。为了自动化筛查AS，深度网络必须学习模仿人类专家识别主动脉瓣视图的能力，然后汇总这些相关图像以产生研究级诊断。我们发现先前的AS检测方法由于依赖于跨图像的不灵活平均值而导致精度不足。我们进一步发现，现成的基于注意力的多实例(MIL)学习表现不佳。我们提出了一种新的端到端MIL方法，包含两个关键方法创新。首先，通过监督式注意技术，引导学习的注意机制偏爱相关视图。其次，一种新颖的自我监督学习技术提高了每个单独图像的表现。我们的方法在一个真实的临床数据集（4569名患者）上实现了最先进的性能，在传统的机器学习和深度学习方法之上。

    Aortic stenosis (AS) is a degenerative valve condition that causes substantial morbidity and mortality. This condition is under-diagnosed and under-treated. In clinical practice, AS is diagnosed with expert review of transthoracic echocardiography, which produces dozens of ultrasound images of the heart. Only some of these views show the aortic valve. To automate screening for AS, deep networks must learn to mimic a human expert's ability to identify views of the aortic valve then aggregate across these relevant images to produce a study-level diagnosis. We find previous approaches to AS detection yield insufficient accuracy due to relying on inflexible averages across images. We further find that off-the-shelf attention-based multiple instance learning (MIL) performs poorly. We contribute a new end-to-end MIL approach with two key methodological innovations. First, a supervised attention technique guides the learned attention mechanism to favor relevant views. Second, a novel self-sup
    
[^100]: 树轮数字水印：一种不可见且健壮的扩散图像指纹技术

    Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust. (arXiv:2305.20030v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.20030](http://arxiv.org/abs/2305.20030)

    本文提出了一种名为树轮数字水印的技术，可以稳定地指纹扩散模型的输出，与现有的在采样后对图像进行修改的方法不同，数字水印微妙地影响整个采样过程，从而产生模型指纹，对人类不可见。

    

    生成模型输出的数字水印是追踪版权和防止人工智能生成内容潜在威胁的关键技术。本文引入一种新颖的技术，称为树轮数字水印，可以稳定地指纹扩散模型的输出。与现有的在采样后对图像进行修改的方法不同，树轮数字水印微妙地影响整个采样过程，从而产生模型指纹，对人类不可见。数字水印将图像生成所使用的初始噪声向量中嵌入一个模式。这些模式在傅里叶空间中结构化，因此它们对卷积、裁剪、膨胀、翻转和旋转具有不变性。图像生成后，通过反演扩散过程来检测水印信号，以检索嵌入信号的噪声向量。我们展示了这种技术可以轻松应用于任意扩散模型，包括文本条件稳定扩散。

    Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diff
    
[^101]: 束搜索递归单元：一种支持反向传播的递归神经网络框架

    Beam Tree Recursive Cells. (arXiv:2305.19999v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19999](http://arxiv.org/abs/2305.19999)

    本论文提出了一种支持反向传播的递归神经网络框架——束搜索递归单元（BT-Cell），用于扩展递归神经网络，实现对潜在结构的感知；此外，我们提出了一种放松束搜索中硬前k算子的方法，以实现更好的梯度信号传递。在评估中发现，BT-Cell在合成和实际数据的多个具有结构敏感性的任务中表现优异。

    

    本文提出了一种叫做束搜索递归单元（BT-Cell）的框架，用于扩展支持使用束搜索进行潜在结构感知的递归神经网络（RvNN）。我们进一步通过提出在束搜索中对硬性前k算子的放松来扩展此框架，以更好地传递梯度信号。我们在合成和实际数据的不同代表性分布上评估了我们的模型。实验结果表明，BT-Cell在多个具有挑战性的体现结构敏感性的任务（如ListOps和逻辑推理）上达到了几乎完美的性能，同时在实际数据上与其他基于RvNN的模型具有可比性的性能。此外，我们在ListOps中确定了神经模型在推广到未见过的参数数量上的未知失效案例。代码可在https://github.com/JRC1995/BeamTreeRecursiveCells上获得。

    We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly framework to extend Recursive Neural Networks (RvNNs) with beam search for latent structure induction. We further extend this framework by proposing a relaxation of the hard top-k operators in beam search for better propagation of gradient signals. We evaluate our proposed models in different out-of-distribution splits in both synthetic and realistic data. Our experiments show that BTCell achieves near-perfect performance on several challenging structure-sensitive synthetic tasks like ListOps and logical inference while maintaining comparable performance in realistic data against other RvNN-based models. Additionally, we identify a previously unknown failure case for neural models in generalization to unseen number of arguments in ListOps. The code is available at: https://github.com/JRC1995/BeamTreeRecursiveCells.
    
[^102]: InGram：通过关系图进行归纳知识图谱嵌入

    InGram: Inductive Knowledge Graph Embedding via Relation Graphs. (arXiv:2305.19987v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19987](http://arxiv.org/abs/2305.19987)

    InGram是一种新的归纳式知识图谱补全方法，可以在推理时生成新关系和实体的嵌入，并使用注意力机制汇总邻居嵌入生成关系和实体嵌入。该方法在多个基准数据集上的性能优于现有的基准方法。

    

    归纳知识图谱补全被视为预测训练期间未观察到的新实体之间的缺失三元组的任务。该论文提出了一个新方法InGram，它可以在推理时生成新关系和实体的嵌入，并基于关系图和原始知识图谱使用注意力机制来汇总邻居嵌入以生成关系和实体嵌入。实验结果表明，在几个基准数据集上，InGram的性能优于现有的基准方法。

    Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outper
    
[^103]: 用锐度感知优化的多数据集协同训练用于音频反欺骗

    Multi-Dataset Co-Training with Sharpness-Aware Optimization for Audio Anti-spoofing. (arXiv:2305.19953v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2305.19953](http://arxiv.org/abs/2305.19953)

    本文提出了一种锐度感知优化的多数据集协同训练方法，用于解决音频反欺骗问题。实验结果表明，该方法在各种数据集上表现出了良好的结果，同时仅需要大型预训练模型的4000倍参数。

    

    自动说话人验证中的音频反欺骗旨在保护用户的身份免受欺骗攻击。尽管最新的欺骗对策（CM）模型在特定数据集上表现良好，但在使用不同数据集进行评估时缺乏泛化性。为了解决这个问题，之前的研究已经探索了大型预训练模型，这需要大量的资源和时间。我们旨在开发一种紧凑而又具有良好泛化性的CM模型，可以与大型预训练模型竞争。我们的方法涉及多数据集协同训练和锐度感知最小化，在这个领域中尚未得到研究。广泛的实验证明，所提出的方法在利用比大型预训练模型少4000倍的参数的同时，在各种数据集上产生了竞争性的结果。

    Audio anti-spoofing for automatic speaker verification aims to safeguard users' identities from spoofing attacks. Although state-of-the-art spoofing countermeasure(CM) models perform well on specific datasets, they lack generalization when evaluated with different datasets. To address this limitation, previous studies have explored large pre-trained models, which require significant resources and time. We aim to develop a compact but well-generalizing CM model that can compete with large pre-trained models. Our approach involves multi-dataset co-training and sharpness-aware minimization, which has not been investigated in this domain. Extensive experiments reveal that proposed method yield competitive results across various datasets while utilizing 4,000 times less parameters than the large pre-trained models.
    
[^104]: 生成式扩散模型中的自发对称破缺

    Spontaneous symmetry breaking in generative diffusion models. (arXiv:2305.19693v1 [cs.LG])

    [http://arxiv.org/abs/2305.19693](http://arxiv.org/abs/2305.19693)

    本文揭示生成式扩散模型存在自发对称破缺现象，这将其生成动力学分为两种不同的“相”，提出了高斯后期初始化方案，能够显著提高模型性能。

    

    生成式扩散模型近期成为了生成高维数据的主要方法。在本文中，我们展示了这些模型的动力学存在自发对称破缺，将生成式动力学分为两个不同的“相”：1）中心固定点周围的线性稳态动力学，2）朝向数据流形的吸引子动力学。这两种“相”由中心固定点稳定性变化所分隔，而不稳定窗口负责生成样本的多样性。我们使用理论和实证证据表明，准确地模拟早期动力学并不会对最终生成产生重要影响，因为早期涨落会回到中心固定点。为了利用这一见解，我们提出了一个高斯后期初始化方案，这显著提高了模型的性能，在快速取样器上实现了长达3倍的FID改进。

    Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking that divides the generative dynamics into two distinct phases: 1) A linear steady-state dynamics around a central fixed-point and 2) an attractor dynamics directed towards the data manifold. These two "phases" are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3x FID improvements on fast samplers, whi
    
[^105]: Point-GCC: 基于几何-颜色对比的通用自监督三维场景预训练

    Point-GCC: Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast. (arXiv:2305.19623v1 [cs.CV])

    [http://arxiv.org/abs/2305.19623](http://arxiv.org/abs/2305.19623)

    Point-GCC提出了一种通过几何-颜色对比进行通用自监督三维场景预训练的方法，并设计了分层监督和架构无关的骨干网络，以缩小预训练和下游任务之间的差距。

    

    点云提供的几何和颜色信息对于三维场景理解都非常重要，然而现有方法在区分和相关性方面缺乏精细设计，因此提出了一种可以更好地利用点云信息关系的三维自监督范式。具体来说，提出了一种通过 Siamese 网络对齐几何和颜色信息的通用三维场景预训练框架 Point-GCC。为了照顾实际应用任务，设计了（i）基于新颖的深度聚类模块的点级对比和重建和物体级对比的分层监督，来缩小预训练和下游任务之间的差距；（ii）架构无关的骨干网络，以适应各种下游模型。由于与下游任务相关联的物体级表示，Point-GCC 可以直接评估。

    Geometry and color information provided by the point clouds are both crucial for 3D scene understanding. Two pieces of information characterize the different aspects of point clouds, but existing methods lack an elaborate design for the discrimination and relevance. Hence we explore a 3D self-supervised paradigm that can better utilize the relations of point cloud information. Specifically, we propose a universal 3D scene pre-training framework via Geometry-Color Contrast (Point-GCC), which aligns geometry and color information using a Siamese network. To take care of actual application tasks, we design (i) hierarchical supervision with point-level contrast and reconstruct and object-level contrast based on the novel deep clustering module to close the gap between pre-training and downstream tasks; (ii) architecture-agnostic backbone to adapt for various downstream models. Benefiting from the object-level representation associated with downstream tasks, Point-GCC can directly evaluate 
    
[^106]: 带有内部分布在线适应的离线元强化学习

    Offline Meta Reinforcement Learning with In-Distribution Online Adaptation. (arXiv:2305.19529v1 [cs.LG])

    [http://arxiv.org/abs/2305.19529](http://arxiv.org/abs/2305.19529)

    本文提出了一种带有不确定性量化的内部分布在线适应(IDAQ)的框架，利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务，在离线元强化学习上具有竞争性表现。

    

    近期的离线元强化学习方法通常利用任务相关的行为策略(例如，对每个个体任务进行RL智能体的训练)来收集多任务数据集。然而，这些方法总是需要额外的信息进行快速调整，例如测试任务的离线上下文。为了解决这个问题，我们首先正式地表征了离线元强化学习中的一个独特挑战：离线数据集和在线适应之间的转换-奖励分布偏移。我们的理论发现，来自分布之外的适应情况可能会导致不可靠的策略评估，并且使用分布内的情况进行在线适应可以确保适应性能保证。基于这些理论洞察，我们提出了一种新的适应框架，称为带有不确定性量化的内部分布在线适应(IDAQ)，它利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务。 实验结果表明，IDAQ的效果优于现有的离线元强化学习方法，并且达到了最先进的在线元强化学习方法的竞争性表现。

    Recent offline meta-reinforcement learning (meta-RL) methods typically utilize task-dependent behavior policies (e.g., training RL agents on each individual task) to collect a multi-task dataset. However, these methods always require extra information for fast adaptation, such as offline context for testing tasks. To address this problem, we first formally characterize a unique challenge in offline meta-RL: transition-reward distribution shift between offline datasets and online adaptation. Our theory finds that out-of-distribution adaptation episodes may lead to unreliable policy evaluation and that online adaptation with in-distribution episodes can ensure adaptation performance guarantee. Based on these theoretical insights, we propose a novel adaptation framework, called In-Distribution online Adaptation with uncertainty Quantification (IDAQ), which generates in-distribution context using a given uncertainty quantification and performs effective task belief inference to address new
    
[^107]: 用Johnson-Lindenstrauss矩阵进行标签嵌入

    Label Embedding by Johnson-Lindenstrauss Matrices. (arXiv:2305.19470v1 [cs.LG])

    [http://arxiv.org/abs/2305.19470](http://arxiv.org/abs/2305.19470)

    这篇论文提出了基于JLMs的标签嵌入方法，将多元分类问题转化为有限回归问题，具有较高的计算效率和预测准确性。

    

    我们提出了一个基于Johnson-Lindenstrauss矩阵（JLMs）的简单且可扩展的极端多元分类框架。利用JLM的列来嵌入标签，将一个C类分类问题转化为具有$\cO(\log C)$输出维度的回归问题。我们得出了一个超量风险限制，阐明了计算效率和预测准确性之间的权衡，并进一步表明，在Massart噪声条件下，降维的惩罚会消失。我们的方法易于并行化，并且实验结果展示了在大规模应用中其有效性和可扩展性。

    We present a simple and scalable framework for extreme multiclass classification based on Johnson-Lindenstrauss matrices (JLMs). Using the columns of a JLM to embed the labels, a $C$-class classification problem is transformed into a regression problem with $\cO(\log C)$ output dimension. We derive an excess risk bound, revealing a tradeoff between computational efficiency and prediction accuracy, and further show that under the Massart noise condition, the penalty for dimension reduction vanishes. Our approach is easily parallelizable, and experimental results demonstrate its effectiveness and scalability in large-scale applications.
    
[^108]: 基于$K^2$-树的分级图生成

    Hierarchical Graph Generation with $K^2$-trees. (arXiv:2305.19125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19125](http://arxiv.org/abs/2305.19125)

    本文介绍了一种基于$K^2$-树的图生成方法，该方法可以实现紧凑生成，并同时捕获图的内在分层结构。通过提出顺序$K^2$-树表示和引入基于Transformer的架构，本文进一步改进了这种方法。实验表明，该方法在图生成方面具有卓越的表现。

    

    从目标分布生成图是许多领域的一个重大挑战，包括药物发现和社交网络分析。在本文中，我们介绍了一种利用原本设计用于无损图压缩的$K^2$-树表示的新颖图生成方法。我们的动机源于$K^2$-树能够在进行紧凑生成的同时，捕获图的内在分层结构的能力。此外，我们还通过(1)提出了一种包含剪枝、扁平化和记号化过程的顺序K2树表示和(2)引入了一种基于Transformer的架构，旨在通过结合专业树形位置编码方案来生成序列。最后，我们对四个常规和两个分子图数据集进行了广泛的评估，以证实我们的算法在图生成方面的优越性。

    Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation which was originally designed for lossless graph compression. Our motivation stems from the ability of the $K^2$-trees to enable compact generation while concurrently capturing the inherent hierarchical structure of a graph. In addition, we make further contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.
    
[^109]: 非矩形不确定性集的强健MDP的策略梯度算法

    Policy Gradient Algorithms for Robust MDPs with Non-Rectangular Uncertainty Sets. (arXiv:2305.19004v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2305.19004](http://arxiv.org/abs/2305.19004)

    本文提出了针对具有非矩形不确定性集的强健MDP的策略梯度算法，并开发了投射Langevin动力学算法和确定性策略梯度方法。数值实验展示了这些算法的有效性。

    

    我们为具有非矩形不确定性集的强健无限时域马尔可夫决策过程（MDP）提出了一个策略梯度算法，从而解决了强健MDP文献中的一个开放性挑战。确实，显示统计最优性质并充分利用有限数据的不确定性集往往不是矩形的。不幸的是，对应的强健MDPs不能用动态规划技术解决，并且实际上是可证明的不可解决的。这促使我们开发一个针对强健策略评估问题量身定制的投射Langevin动力学算法，该算法提供全局最优性保证。我们还提出了一种确定性策略梯度方法，该方法近似解决了强健策略评估问题，并证明了近似误差与不确定性集的非矩形度量成比例。数值实验展示了我们的投影Langevin动力学算法可以避免局部最优，而算法是量身定制的。

    We propose a policy gradient algorithm for robust infinite-horizon Markov Decision Processes (MDPs) with non-rectangular uncertainty sets, thereby addressing an open challenge in the robust MDP literature. Indeed, uncertainty sets that display statistical optimality properties and make optimal use of limited data often fail to be rectangular. Unfortunately, the corresponding robust MDPs cannot be solved with dynamic programming techniques and are in fact provably intractable. This prompts us to develop a projected Langevin dynamics algorithm tailored to the robust policy evaluation problem, which offers global optimality guarantees. We also propose a deterministic policy gradient method that solves the robust policy evaluation problem approximately, and we prove that the approximation error scales with a new measure of non-rectangularity of the uncertainty set. Numerical experiments showcase that our projected Langevin dynamics algorithm can escape local optima, while algorithms tailor
    
[^110]: 连续强化学习的策略优化

    Policy Optimization for Continuous Reinforcement Learning. (arXiv:2305.18901v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18901](http://arxiv.org/abs/2305.18901)

    本研究提出了连续强化学习领域的占用时间概念，并在此基础上扩展了离散强化学习中的PG、TRPO和PPO方法，为连续强化学习领域的研究提供了新的思路和方法。

    

    我们研究了连续时间和空间下的强化学习，采用折扣奖励和随机微分方程的基本动态。在连续强化学习的最新进展的基础上，我们提出了占用时间的概念（特别是针对折扣奖励）并展示了如何有效地使用它来导出性能差异和局部逼近公式。我们还将这些结果扩展到了 PG（策略梯度）、TRPO（信任区域策略优化）和 PPO（近端策略优化）方法，这些方法在离散强化学习中是熟知和强大的工具，但在连续强化学习中尚未得到充分发展。通过数字实验，我们展示了我们的方法的有效性和优势。

    We study reinforcement learning (RL) in the setting of continuous time and space, for an infinite horizon with a discounted objective and the underlying dynamics driven by a stochastic differential equation. Built upon recent advances in the continuous approach to RL, we develop a notion of occupation time (specifically for a discounted objective), and show how it can be effectively used to derive performance-difference and local-approximation formulas. We further extend these results to illustrate their applications in the PG (policy gradient) and TRPO/PPO (trust region policy optimization/ proximal policy optimization) methods, which have been familiar and powerful tools in the discrete RL setting but under-developed in continuous RL. Through numerical experiments, we demonstrate the effectiveness and advantages of our approach.
    
[^111]: 无监督多元时间序列表示学习的对比形态片段学习

    Contrastive Shapelet Learning for Unsupervised Multivariate Time Series Representation Learning. (arXiv:2305.18888v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18888](http://arxiv.org/abs/2305.18888)

    该论文提出了一种新的无监督多元时间序列表示学习框架，通过对比形态片段学习以及多粒度对比和多尺度对齐的学习目标，学习时间序列特定表示。

    

    近期的研究表明，对于多元时间序列数据，无监督表示学习（URL）具备学习泛化表示以及无需使用不可访问标签就能适用于多数下游任务的能力。然而，现有方法通常采用原本为其他领域（如计算机视觉）设计的模型进行编码，且依赖于强假设设计学习目标，这限制了它们的运行表现。为解决这些问题，我们提出了一种新颖的无监督表示学习框架，通过流行的对比学习范式，学习基于形态片段的时间序列特定表示。据我们所知，这是第一篇探索无监督通用表示学习中形态片段嵌入的研究。特别地，我们提出了一个统一的基于形态片段的编码器和一种新的学习目标，具有多粒度对比和多尺度对齐。

    Recent studies have shown great promise in unsupervised representation learning (URL) for multivariate time series, because URL has the capability in learning generalizable representation for many downstream tasks without using inaccessible labels. However, existing approaches usually adopt the models originally designed for other domains (e.g., computer vision) to encode the time series data and rely on strong assumptions to design learning objectives, which limits their ability to perform well. To deal with these problems, we propose a novel URL framework for multivariate time series by learning time-series-specific shapelet-based representation through a popular contrasting learning paradigm. To the best of our knowledge, this is the first work that explores the shapelet-based embedding in the unsupervised general-purpose representation learning. A unified shapelet-based encoder and a novel learning objective with multi-grained contrasting and multi-scale alignment are particularly 
    
[^112]: 标准比评分更重要：面向多准则推荐的标准偏好感知轻量图卷积网络

    Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2305.18885](http://arxiv.org/abs/2305.18885)

    本文提出了一种面向多准则推荐的标准偏好感知轻量图卷积网络，该方法结合了MC扩展图，可以准确地捕捉用户的标准偏好，并进一步将用户对各个标准的偏好合并到最终的推荐列表中。

    

    多准则推荐系统现在在广泛的电子商务领域中利用多准则 (MC) 评分信息，而深度学习中的图神经网络 (GNN) 已经被广泛应用于各种推荐系统的开发中。在这种情况下，本文首次尝试使用GNN辅助设计MC推荐系统。具体而言，我们提出了一种新颖的标准偏好感知轻量图卷积方法(CPA-LGC),可以准确捕捉用户的标准偏好以及复杂高阶连接中的协作信号。本文在MC扩展图上构建了一个能够将用户-物品MC评分转换为扩展二分图的MC扩展图，再进一步将标准重要性编码到图卷积过程中，并引入了一种新的标准偏好感知聚合方法来将用户对不同标准的偏好合并到最终的推荐列表中。

    The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
    
[^113]: 任务等变图Few-shot学习

    Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18758](http://arxiv.org/abs/2305.18758)

    本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。

    

    虽然图神经网络（GNN）在节点分类任务中取得了成功，但其性能严重依赖于每类具有足够标记节点的可用性。在现实情况下，不是所有类都有许多标记节点，模型可能需要分类新类别，使得手动标记变得困难。为了解决这个问题，GNN需要能够用有限数量的标记节点对节点进行分类，称为Few-shot节点分类。先前的基于剧集元学习的方法在Few-shot节点分类中取得了成功，但我们的发现表明仅有多样的训练元任务才能实现最佳性能。为了应对基于元学习的Few-shot学习的挑战，我们提出了一种新的方法，即任务等变图Few-shot学习（TEG）框架。我们的TEG框架通过利用图神经网络的等变性质来使模型学习可转移的任务适应策略。我们在各种Few-shot分类基准上展示了我们提出的方法的有效性，实现了最先进的性能。

    Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
    
[^114]: 基于图的多ODE神经网络用于时空交通预测

    Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting. (arXiv:2305.18687v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18687](http://arxiv.org/abs/2305.18687)

    本文提出了一种称为GRAM-ODENNs的新型神经网络结构，用于解决当前图ODE模型的局限性，包括局部模式的忽略和缺乏动态语义边缘，用于更准确的长程交通预测。

    

    近年来交通领域中出现了许多时空预测模型的发展。然而，由于交通网络中观察到的复杂而广泛的时空相关性，长程交通预测仍然是一个具有挑战性的任务。目前的作品主要依赖于具有图形结构的道路网络，并使用图神经网络（GNN）学习表示，但这种方法在深度架构中存在过度平滑的问题。为了解决这个问题，最近的方法介绍了将GNN与残差连接或神经普通微分方程（ODE）相结合的方法。然而，当前的图形ODE模型在特征提取方面存在两个关键限制：（1）它们倾向于全局时间模式，忽略了对于意外事件很重要的局部模式；（2）它们在其架构设计中缺乏动态语义边缘。在本文中，我们提出了一种称为基于图的多ODE神经网络（GRAM-ODENNs）的新型架构，用于解决当前图形ODE模型的局限性。GRAM-ODENNs使用多个并行ODE编码器和动态图结构，适应交通数据的时间演变，从而实现更准确的长程交通预测。

    There is a recent surge in the development of spatio-temporal forecasting models in the transportation domain. Long-range traffic forecasting, however, remains a challenging task due to the intricate and extensive spatio-temporal correlations observed in traffic networks. Current works primarily rely on road networks with graph structures and learn representations using graph neural networks (GNNs), but this approach suffers from over-smoothing problem in deep architectures. To tackle this problem, recent methods introduced the combination of GNNs with residual connections or neural ordinary differential equations (ODE). However, current graph ODE models face two key limitations in feature extraction: (1) they lean towards global temporal patterns, overlooking local patterns that are important for unexpected events; and (2) they lack dynamic semantic edges in their architectural design. In this paper, we propose a novel architecture called Graph-based Multi-ODE Neural Networks (GRAM-OD
    
[^115]: 可控毁灭路径

    Controllable Path of Destruction. (arXiv:2305.18553v1 [cs.AI])

    [http://arxiv.org/abs/2305.18553](http://arxiv.org/abs/2305.18553)

    本文介绍了可控毁灭路径方法，该方法是一种自我监督的迭代生成器学习方法，通过向修复轨迹的状态-动作对添加条件输入来实现可控性。

    

    毁灭路径（PoD）是一种自我监督的迭代生成器学习方法。其核心思想是通过破坏一组物品来产生一个训练集，为每个破坏步骤创建一个与相应修复动作相关的训练实例。在此数据集上训练的生成器可以通过从任意状态“修复”来生成新的物品。PoD方法在原始训练示例方面非常节省，并且非常适合由分类数据组成的功能部件，例如游戏关卡和离散的3D结构。在本文中，我们将毁灭路径方法扩展到允许设计师控制生成的物品的各个方面。通过向构成修复轨迹的状态-动作对添加条件输入来引入可控性。我们在2D地牢设置以及小型3D乐高汽车领域测试了可控PoD方法。

    Path of Destruction (PoD) is a self-supervised method for learning iterative generators. The core idea is to produce a training set by destroying a set of artifacts, and for each destructive step create a training instance based on the corresponding repair action. A generator trained on this dataset can then generate new artifacts by ``repairing'' from arbitrary states. The PoD method is very data-efficient in terms of original training examples and well-suited to functional artifacts composed of categorical data, such as game levels and discrete 3D structures. In this paper, we extend the Path of Destruction method to allow designer control over aspects of the generated artifacts. Controllability is introduced by adding conditional inputs to the state-action pairs that make up the repair trajectories. We test the controllable PoD method in a 2D dungeon setting, as well as in the domain of small 3D Lego cars.
    
[^116]: 集合通信带宽最优管道调度算法

    Bandwidth Optimal Pipeline Schedule for Collective Communication. (arXiv:2305.18461v1 [cs.NI])

    [http://arxiv.org/abs/2305.18461](http://arxiv.org/abs/2305.18461)

    本文提出了一种强多项式时间算法，用于在任何拓扑结构上实现带宽最优的全聚合/归约散开，为解决方案提供了通用性，可以轻松扩展到其他形式的集合通信方法。

    

    我们提出了一种强多项式时间算法，用于在任何网络拓扑上（无论是否有交换机）生成带宽最优的全聚合/归约散开。我们的算法构建了一种管道调度，可在给定拓扑中实现保证最佳可行带宽性能。为了提供一个通用解决方案，我们将网络拓扑建模为具有异构链路容量和交换机的有向图，并将交换机直接建模为图表示中的顶点。该算法相对于拓扑大小具有强多项式时间。此工作严重依赖于先前的图论研究中的边不相交生成树和边分裂。虽然我们的重点是全聚合，但本文中的方法可以轻松扩展为归约、广播、归约散开和全归约的调度生成。

    We present a strongly polynomial-time algorithm to generate bandwidth optimal allgather/reduce-scatter on any network topology, with or without switches. Our algorithm constructs pipeline schedules achieving provably the best possible bandwidth performance on a given topology. To provide a universal solution, we model the network topology as a directed graph with heterogeneous link capacities and switches directly as vertices in the graph representation. The algorithm is strongly polynomial-time with respect to the topology size. This work heavily relies on previous graph theory work on edge-disjoint spanning trees and edge splitting. While we focus on allgather, the methods in this paper can be easily extended to generate schedules for reduce, broadcast, reduce-scatter, and allreduce.
    
[^117]: 基于大语言模型的多项选择题答案确认预测研究

    Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])

    [http://arxiv.org/abs/2305.18404](http://arxiv.org/abs/2305.18404)

    本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。

    

    随着大型语言模型的广泛开发，对它们进行健壮的不确定性量化技术将成为它们在高风险场景下安全部署的关键。本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。这种观察对于下游应用，如选择性分类和过滤低质量预测，可能会有用。我们还研究了符合性预测对于超出主题的问题的交换性假设，这可能是许多实际应用的更为现实的场景。本研究为在需要可靠保证错误率的安全关键情况下更加值得信赖和可靠地使用大型语言模型做出了贡献。

    As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
    
[^118]: 剪枝与低秩参数高效微调相遇

    Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. (arXiv:2305.18403v1 [cs.LG])

    [http://arxiv.org/abs/2305.18403](http://arxiv.org/abs/2305.18403)

    本论文提出了一种名为LoRAPrune的框架，可以高效微调和部署大型预训练模型，通过利用低秩自适应的值和梯度来设计PEFT感知的剪枝准则，并提出了一个迭代剪枝过程来去除冗余参数，实验结果表明与最先进的方法相比，可以显著降低模型大小和推理时间，同时保持竞争性的准确性。

    

    大型预训练模型（LPM）在各种任务中表现出卓越的性能。虽然出现了参数高效微调（PEFT）来便宜地微调这些大型模型用于下游任务，但它们的部署仍然受到巨大的模型规模和计算成本的制约。神经网络剪枝通过删除冗余参数来提供模型压缩的解决方案，但大多数现有方法依赖于计算参数梯度。然而，对于LPM而言，获得梯度是计算上禁止的，这需要探索替代方法。为此，我们提出了一种用于LPM高效微调和部署的统一框架，称为LoRAPrune。我们首先设计了一个PEFT感知的剪枝准则，该准则利用了低秩自适应（LoRA）的值和梯度，而不是预训练参数的梯度进行重要性评估。然后，我们提出了一个迭代剪枝过程来基于剪枝准则去除冗余参数。各种基准数据集上的实验结果表明，与最先进的方法相比，我们的框架可以显著降低模型大小和推理时间，同时保持竞争性的准确性。

    Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shown exceptional performance across various tasks. Although parameter-efficient fine-tuning (PEFT) has emerged to cheaply fine-tune these large models on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a solution for model compression by removing redundant parameters, but most existing methods rely on computing parameter gradients. However, obtaining the gradients is computationally prohibitive for LPMs, which necessitates the exploration of alternative approaches. To this end, we propose a unified framework for efficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first design a PEFT-aware pruning criterion, which utilizes the values and gradients of Low-Rank Adaption (LoRA), rather than the gradients of pre-trained parameters for importance estimation. We then propose an iterative pruning procedure to remove redundant paramet
    
[^119]: 一种元学习框架用于调整可信联邦学习保护机制的参数

    A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning. (arXiv:2305.18400v1 [cs.LG])

    [http://arxiv.org/abs/2305.18400](http://arxiv.org/abs/2305.18400)

    提出了一个元学习框架，用于调整可信联邦学习保护机制的参数，以在隐私泄露、效用损失和效率降低之间进行权衡。

    

    可信联邦学习（TFL）通常利用保护机制来保证隐私安全。然而，保护机制不可避免地会引入效用损失或效率降低，同时保护数据隐私。因此，保护机制及其参数应该仔细选择，以在保护隐私泄露、效用损失和效率降低之间取得最佳平衡。为此，联邦学习从业者需要工具来衡量这三个因素，并优化它们之间的权衡，选择最适合手头应用的保护机制。基于这个要求，我们提出了一个框架，它(1)将TFL定义为找到保护机制来优化隐私泄露、效用损失和效率降低三者之间的权衡的问题；(2)正式定义了这三个因素的有界测量。然后，我们提出了一个元学习算法来近似解决此优化问题。

    Trustworthy Federated Learning (TFL) typically leverages protection mechanisms to guarantee privacy. However, protection mechanisms inevitably introduce utility loss or efficiency reduction while protecting data privacy. Therefore, protection mechanisms and their parameters should be carefully chosen to strike an optimal tradeoff between \textit{privacy leakage}, \textit{utility loss}, and \textit{efficiency reduction}. To this end, federated learning practitioners need tools to measure the three factors and optimize the tradeoff between them to choose the protection mechanism that is most appropriate to the application at hand. Motivated by this requirement, we propose a framework that (1) formulates TFL as a problem of finding a protection mechanism to optimize the tradeoff between privacy leakage, utility loss, and efficiency reduction and (2) formally defines bounded measurements of the three factors. We then propose a meta-learning algorithm to approximate this optimization proble
    
[^120]: 可视化编程中神经任务合成

    Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])

    [http://arxiv.org/abs/2305.18342](http://arxiv.org/abs/2305.18342)

    该论文提出了一种基于神经符号技术的可视化编程任务合成方法NeurTaskSyn。该方法能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，自动生成编程任务。

    

    通过合成新的内容，生成式神经模型在增强编程教育方面具有巨大的潜力。我们旨在设计神经模型，能够根据可视化编程环境下给定的规范自动生成编程任务。尽管近年来像 GPT-4 这样的大型生成模型获得了成功，但我们的初步结果显示，这些模型在合成可视化编程任务方面效果不佳，并且在逻辑和空间推理方面存在困难。我们提出了一种新颖的神经符号技术 NeurTaskSyn，该技术能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，合成编程任务。NeurTaskSyn 由两个部分构成：第一个部分通过模仿学习程序进行训练，生成可能的解决方案代码，第二个部分通过强化学习程序进行训练，指导底层符号执行引擎生成可视化任务。

    Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
    
[^121]: 用Transformer学习超关系型和数值知识图中的表征学习

    Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18256](http://arxiv.org/abs/2305.18256)

    本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。

    

    近期研究了一个超关系型知识图谱，其中三元组与限定词集合相关联; 一个限定词由关系和实体组成，为三元组提供辅助信息。现有的超关系型知识图嵌入方法假定实体是离散对象，但有些信息应使用数值表示，例如(J.R.R.，出生于，1892)。同时，三元组(J.R.R.，就读于，牛津大学)可以与限定词(开始时间，1911)相关联。在本文中，我们提出了一个名为HyNT的统一框架，用于学习包含三元组或限定词中数值文字的超关系型知识图的表示。我们定义了一个上下文Transformer和一个预测Transformer，来学习表示，不仅基于三元组和其限定词之间的相关性，还基于数值信息。通过学习三元组和限定词的紧凑表示，并将它们馈送给Transformer来获得模型

    A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
    
[^122]: 疾病患者个体根本原因的反事实公式化

    Counterfactual Formulation of Patient-Specific Root Causes of Disease. (arXiv:2305.17574v1 [cs.AI])

    [http://arxiv.org/abs/2305.17574](http://arxiv.org/abs/2305.17574)

    本文提出了一种针对疾病患者个体的根本原因的新公式，可以用于自动从数据中检测根本原因，并考虑了噪声标签和疾病流行率等因素，同时具有快速计算的优势。

    

    疾病的根本原因直观地对应于增加诊断可能性的根本顶点。然而，这种根本原因的描述缺乏计算机算法发展所需的严格数学公式。在以前的工作中，使用干预主义者帐户定义了疾病的病人特定根本原因，该帐户仅攀升到珍珠的因果Ladder的第二层。在这个理论性的文章中，我们通过提出反事实的定义来攀升到第三层，以匹配基于固定事实数据的临床直觉。然后，我们展示了如何使用可解释的人工智能的Shapley值为每个变量分配根因贡献得分。提出的疾病患者个体根本原因的反事实公式化考虑了噪声标签，适应了疾病的流行率，并允许快速计算，无需反事实模拟。

    Root causes of disease intuitively correspond to root vertices that increase the likelihood of a diagnosis. This description of a root cause nevertheless lacks the rigorous mathematical formulation needed for the development of computer algorithms designed to automatically detect root causes from data. Prior work defined patient-specific root causes of disease using an interventionalist account that only climbs to the second rung of Pearl's Ladder of Causation. In this theoretical piece, we climb to the third rung by proposing a counterfactual definition matching clinical intuition based on fixed factual data alone. We then show how to assign a root causal contribution score to each variable using Shapley values from explainable artificial intelligence. The proposed counterfactual formulation of patient-specific root causes of disease accounts for noisy labels, adapts to disease prevalence and admits fast computation without the need for counterfactual simulation.
    
[^123]: 分布式不确定性量化的联邦符合预测器

    Federated Conformal Predictors for Distributed Uncertainty Quantification. (arXiv:2305.17564v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17564](http://arxiv.org/abs/2305.17564)

    本文将符合预测推广到联邦学习设置，提出联邦符合预测（FCP）框架，处理了数据集异质性的问题。实验结果表明，FCP具有严格的理论保证，在分布式和异质环境中有效。

    

    由于可以轻松应用为后处理步骤以已经训练过的模型中，符合预测逐渐成为提供机器学习中严格不确定性量化的流行范式。本文将符合预测推广到联邦学习设置。我们面临的主要挑战是客户端之间的数据异质性，这违反了符合预测所需的交换性的基本原则。我们提出了一个较弱的部分交换性概念，更适合于此种情况，并使用它来开发联邦符合预测（FCP）框架。我们展示了FCP具有严格的理论保证，在几个计算机视觉和医学成像数据集上具有优秀的经验性能。我们的结果展示了在分布式和异质环境中融入有意义的不确定性量化的实用方法。我们提供了我们实验中使用的代码 https://github.com/clu5/federated-conformal。

    Conformal prediction is emerging as a popular paradigm for providing rigorous uncertainty quantification in machine learning since it can be easily applied as a post-processing step to already trained models. In this paper, we extend conformal prediction to the federated learning setting. The main challenge we face is data heterogeneity across the clients - this violates the fundamental tenet of exchangeability required for conformal prediction. We propose a weaker notion of partial exchangeability, better suited to the FL setting, and use it to develop the Federated Conformal Prediction (FCP) framework. We show FCP enjoys rigorous theoretical guarantees and excellent empirical performance on several computer vision and medical imaging datasets. Our results demonstrate a practical approach to incorporating meaningful uncertainty quantification in distributed and heterogeneous environments. We provide code used in our experiments https://github.com/clu5/federated-conformal.
    
[^124]: Translatotron 3: 使用单语数据进行语音到语音翻译

    Translatotron 3: Speech to Speech Translation with Monolingual Data. (arXiv:2305.17547v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17547](http://arxiv.org/abs/2305.17547)

    Translatotron 3提出了一种新方法，使用单语数据进行语音到语音翻译，无需配对的数据或专业建模，展示了保留语言/非语言信息的能力。

    

    本文提出了Translatotron 3的新方法，通过遮蔽自编码器、无监督的嵌入映射和回译将直接语音到语音翻译模型从单声道语音-文本数据集中完全无监督地进行训练。在西班牙语和英语之间的语音到语音翻译任务中，实验结果表明，Translatotron 3优于基准级联系统，在 synthesized Unpaired-Conversational 数据集上报告了18.14 BLEU分数的提高。与需要真实配对数据或专业建模来复制语言/非语言信息的监督方法不同，Translatotron 3展示了它保留了像暂停、说话速度和说话人身份等语言/非语言信息的能力。

    This paper presents Translatotron 3, a novel approach to train a direct speech-to-speech translation model from monolingual speech-text datasets only in a fully unsupervised manner. Translatotron 3 combines masked autoencoder, unsupervised embedding mapping, and back-translation to achieve this goal. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, which is unavailable, or specialized modeling to replicate para-/non-linguistic information, Translatotron 3 showcases its capability to retain para-/non-linguistic such as pauses, speaking rates, and speaker identity. Audio samples can be found in our website this http URL
    
[^125]: 随机梯度下降中动态稳定性的隐式正则化

    The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent. (arXiv:2305.17490v1 [stat.ML])

    [http://arxiv.org/abs/2305.17490](http://arxiv.org/abs/2305.17490)

    本文研究了随机梯度下降的动态稳定性隐式正则化，证明了其具有良好的泛化性。

    

    本文从“动态稳定性”的角度研究了随机梯度下降（SGD）的隐式正则化。我们首先修正了现有SGD稳定性分析的问题，展示了Hessian矩阵的Frobenius范数和迹与不同稳定性概念的关系。特别地，如果全局最小值在SGD中是线性稳定的，则Hessian矩阵的迹必须小于或等于$2/\eta$，其中$\eta$表示学习率。然而，对于梯度下降（GD），稳定性只对Hessian矩阵的最大特征值施加类似的约束。我们接着分析了这些稳定极值的泛化性质，着重关注了两层ReLU网络和对角线性网络。特别地，我们建立了这两个模型的尖锐度度量和某些参数规范之间的“等价性”，从而证明了SGD的稳定极值具有良好的泛化性。然而，GD的稳定性正则化只在特定情况下产生泛化效益。最后，我们将我们的理论应用于深度线性网络问题，结果表明它对某些模型的表现优于Lasso或岭正则化。

    In this paper, we study the implicit regularization of stochastic gradient descent (SGD) through the lens of {\em dynamical stability} (Wu et al., 2018). We start by revising existing stability analyses of SGD, showing how the Frobenius norm and trace of Hessian relate to different notions of stability. Notably, if a global minimum is linearly stable for SGD, then the trace of Hessian must be less than or equal to $2/\eta$, where $\eta$ denotes the learning rate. By contrast, for gradient descent (GD), the stability imposes a similar constraint but only on the largest eigenvalue of Hessian. We then turn to analyze the generalization properties of these stable minima, focusing specifically on two-layer ReLU networks and diagonal linear networks. Notably, we establish the {\em equivalence} between these metrics of sharpness and certain parameter norms for the two models, which allows us to show that the stable minima of SGD provably generalize well. By contrast, the stability-induced reg
    
[^126]: 深度学习模型概述与比较分析：CNN、RNN、LSTM、GRU。

    A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU. (arXiv:2305.17473v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17473](http://arxiv.org/abs/2305.17473)

    本文全面概括了深度学习模型的类型和应用，比较分析了各个模型的结构、优点和局限性，有助于选择和设计深度学习模型。

    

    深度学习（DL）是机器学习（ML）和人工智能（AI）的强大子集，特别在处理非结构化和大型数据集方面优于传统的ML方法。其影响跨越各个领域，包括语音识别、医疗保健、自动驾驶汽车、网络安全、预测分析等。然而，实际问题的复杂性和动态性给设计有效的深度学习模型带来了挑战。因此，人们开发出了几种不同的深度学习模型来解决不同的问题和应用。在本文中，我们对各种深度学习模型进行了全面调查，包括卷积神经网络（CNN）、循环神经网络（RNN）、生成模型、深度强化学习（DRL）和深度迁移学习。我们考察了每个模型的结构、应用、好处和局限性。此外，我们使用了三个公开可用的数据集进行了分析。

    Deep learning (DL) has emerged as a powerful subset of machine learning (ML) and artificial intelligence (AI), outperforming traditional ML methods, especially in handling unstructured and large datasets. Its impact spans across various domains, including speech recognition, healthcare, autonomous vehicles, cybersecurity, predictive analytics, and more. However, the complexity and dynamic nature of real-world problems present challenges in designing effective deep learning models. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep Transfer Learning. We examine the structure, applications, benefits, and limitations of each model. Furthermore, we perform an analysis using three publicly available dataset
    
[^127]: 神经智能中的注意力模式

    Attention Schema in Neural Agents. (arXiv:2305.17375v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.17375](http://arxiv.org/abs/2305.17375)

    本文研究了神经智能中的注意力模式，并提出了注意力模式理论（AST）。作者发现将AS实现为一种循环内部控制的智能体效果最佳，这一理论为应用与改进神经智能提供了新思路。

    

    注意力已经成为深度学习架构中的常见要素。它通过加入动态信息选择，支持静态的权重选择。同样地，我们可以想象在注意力之上构建一个更高阶的信息过滤器：注意力模式（AS）。也就是说，一个描述性和预测性的注意力模型。在认知神经科学中，注意力模式理论（AST）支持这种区分注意力和AS的想法。该理论的一个重要预测是，一个智能体可以使用自己的AS来推断其他智能体的注意状态，从而增强与其他智能体的协调。因此，多智能体强化学习是一个实验测试AST有效性的理想场景。我们探讨了注意力和AS相互作用的不同方式，初步结果表明，将AS实现为一种循环内部控制的智能体效果最佳。总体而言，这些实验为理解以及改进神经智能中的注意力模式提供了新的思路。

    Attention has become a common ingredient in deep learning architectures. It adds a dynamical selection of information on top of the static selection of information supported by weights. In the same way, we can imagine a higher-order informational filter built on top of attention: an Attention Schema (AS), namely, a descriptive and predictive model of attention. In cognitive neuroscience, Attention Schema Theory (AST) supports this idea of distinguishing attention from AS. A strong prediction of this theory is that an agent can use its own AS to also infer the states of other agents' attention and consequently enhance coordination with other agents. As such, multi-agent reinforcement learning would be an ideal setting to experimentally test the validity of AST. We explore different ways in which attention and AS interact with each other. Our preliminary results indicate that agents that implement the AS as a recurrent internal control achieve the best performance. In general, these expl
    
[^128]: 从黑盒模型到可解释模型的转化，用于高效的迁移学习

    Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])

    [http://arxiv.org/abs/2305.17303](http://arxiv.org/abs/2305.17303)

    本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。

    

    建立具有普适性的AI模型是医疗领域面临的主要挑战之一。神经网络（NN）模型即使输入分布轻微移位（例如扫描仪类型），也会受到影响，而放射科医生则依赖于异常性的通用描述性规则。微调模型以将知识从一个领域转移到另一个领域需要大量标记数据。在本文中，我们开发了一种可解释的模型，它可以在计算成本最小的情况下，高效地针对未知的目标域进行微调。我们认为NN的可解释组件大致是域不变的。然而，可解释模型通常表现不及它们的BB变体。在源域中我们先使用人类理解的概念从BB开始，将其提炼成一组浅显易懂的interpretable模型。由于每个interpretable模型都覆盖了数据的一个子集，具有一组interpretable模型的混合可以实现与BB相当的性能。

    Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
    
[^129]: 大型语言模型的异质价值评估

    Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])

    [http://arxiv.org/abs/2305.17147](http://arxiv.org/abs/2305.17147)

    本文提出了一种自动对齐评估方法A2EHV，采用异质价值系统，并基于价值合理性和社会价值定向框架评估代理人行为的社会偏好，结果表明比传统对齐方法更合理。

    

    大型语言模型（LLM）的出现使得将它们的价值与人类价值对齐变得至关重要。当前的方法通常尝试将其与一种同质的人类价值对齐，并需要人类验证，但缺乏对对齐所需方面和深度的共识以及造成的人类偏见。在本文中，我们提出了一种自动对齐评估方法A2EHV，该方法采用异质价值系统，（1）是自动化的，以最小化单个人类偏见，并且（2）允许评估针对各种目标值的异质代理人。我们的方法基于价值合理性的概念，它代表了代理人执行最能满足目标价值行为的能力。价值合理性的量化是通过社会心理学中的社会价值定向框架进行的，该框架将价值空间分为四个类别，以评估代理人行为的社会偏好。我们评估了三个模型的价值合理性，结果表明A2EHV方法比传统对齐方法更合理。

    The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
    
[^130]: Minecraft中的幽灵：利用基于文本知识和记忆的大型语言模型实现开放世界环境中的通用能力智能体。

    Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory. (arXiv:2305.17144v1 [cs.AI])

    [http://arxiv.org/abs/2305.17144](http://arxiv.org/abs/2305.17144)

    本文提出了Ghost in the Minecraft (GITM)框架，利用大型语言模型与基于文本的知识和记忆，创造了一种在Minecraft中具备通用能力的智能体，可在以文本为基础的复杂编程环境中熟练导航。

    

    近年来，Minecraft玩法吸引了大量的研究关注，成为开发能够在开放世界环境中运行的智能体的丰富平台。然而，当前的研究主要集中在特定的目标上，例如流行的“ObtainDiamond”任务，并且还没有显示出有效地推广到更广泛任务的能力。此外，“ObtainDiamond”任务的目前最高成功率只有约20％，凸显了现有方法中使用强化学习（RL）控制器的局限性。为了解决这些挑战，我们引入了Ghost in the Minecraft (GITM)，一个新颖的框架，将大型语言模型与基于文本的知识和记忆相结合，旨在创建Minecraft中的通用能力智能体。这些具备LLM中的逻辑和常识能力的智能体可以熟练地在以文本为基础的复杂编程环境中导航。

    The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "ObtainDiamond" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "ObtainDiamond" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based 
    
[^131]: 机器学习驱动的分布式系统认证之路

    Towards Certification of Machine Learning-Based Distributed Systems. (arXiv:2305.16822v1 [cs.LG])

    [http://arxiv.org/abs/2305.16822](http://arxiv.org/abs/2305.16822)

    认证技术对于机器学习驱动的分布式系统验证不适用，本文提出了第一个ML-based分布式系统认证方案，以验证其非功能属性。

    

    机器学习（ML）日益被用于驱动部署在5G云边缘连续体上的复杂分布式系统的运行。相应地，分布式系统的行为变得更具非确定性。这种分布式系统的演化需要定义新的保证方法来验证非功能属性。认证作为系统和软件验证的最流行的保证技术，不能立即适用于其行为由基于机器学习的推理决定的系统。然而，政策制定者、监管机构和产业利益相关者越来越推崇定义ML的非功能属性（如公平性、鲁棒性、隐私）的认证技术。本文分析了当前认证方案的挑战和不足之处，讨论了开放的研究问题，并提出了第一个ML-based分布式系统认证方案。

    Machine Learning (ML) is increasingly used to drive the operation of complex distributed systems deployed on the cloud-edge continuum enabled by 5G. Correspondingly, distributed systems' behavior is becoming more non-deterministic in nature. This evolution of distributed systems requires the definition of new assurance approaches for the verification of non-functional properties. Certification, the most popular assurance technique for system and software verification, is not immediately applicable to systems whose behavior is determined by Machine Learning-based inference. However, there is an increasing push from policy makers, regulators, and industrial stakeholders towards the definition of techniques for the certification of non-functional properties (e.g., fairness, robustness, privacy) of ML. This article analyzes the challenges and deficiencies of current certification schemes, discusses open research issues and proposes a first certification scheme for ML-based distributed syst
    
[^132]: 使用策略蒸馏的反事实解释器框架解释深度强化学习模型

    Counterfactual Explainer Framework for Deep Reinforcement Learning Models Using Policy Distillation. (arXiv:2305.16532v1 [cs.LG])

    [http://arxiv.org/abs/2305.16532](http://arxiv.org/abs/2305.16532)

    本文提出了一种新的基于反事实解释方法的框架来解释黑盒DRL所作的决策，并且在实验中展示了该解释框架的可行性和有效性。

    

    深度强化学习（DRL）已经展示出解决复杂控制问题的有希望能力。然而，在安全关键系统中，DRL应用受到缺乏强大的验证技术来保证在这些应用中的性能。验证过程的关键要求之一是开发有效的技术来解释系统的功能，即为什么系统在特定情况下产生特定结果。最近，提出了基于反事实（Counterfactual，CF）解释方法的解释方法来解决DRL中的解释问题。本文提出了一种新的CF解释框架，来解释黑盒DRL所作的决策。为了评估所提出的解释框架的有效性，我们在自动驾驶系统和Atari Pong游戏领域进行了几项实验。我们的分析表明，所提出的框架生成了合理和有意义的解释结果，同时保持了与原始DRL模型相比的高度真实性。

    Deep Reinforcement Learning (DRL) has demonstrated promising capability in solving complex control problems. However, DRL applications in safety-critical systems are hindered by the inherent lack of robust verification techniques to assure their performance in such applications. One of the key requirements of the verification process is the development of effective techniques to explain the system functionality, i.e., why the system produces specific results in given circumstances. Recently, interpretation methods based on the Counterfactual (CF) explanation approach have been proposed to address the problem of explanation in DRLs. This paper proposes a novel CF explanation framework to explain the decisions made by a black-box DRL. To evaluate the efficacy of the proposed explanation framework, we carried out several experiments in the domains of automated driving systems and Atari Pong game. Our analysis demonstrates that the proposed framework generates plausible and meaningful expl
    
[^133]: 条件分布之间的经验最优输运

    Empirical Optimal Transport between Conditional Distributions. (arXiv:2305.15901v1 [cs.LG])

    [http://arxiv.org/abs/2305.15901](http://arxiv.org/abs/2305.15901)

    本文考虑在一个公共变量的条件下，相应分布之间的最优输运问题。通过采用基于 MMD 的核正则化器，克服了条件变量是连续的和两个分布中该变量的边缘是不同的挑战。

    

    给定两个联合分布的样本，考虑在一个公共变量的条件下，相应分布之间的最优输运问题。本文的目标是估计伴随条件值的输运成本（Wasserstein 距离），以及条件分布间的输运计划。由于匹配条件分布是监督训练判别模型和（隐式）条件生成模型的核心，条件分布之间的最优输运具有在不同的机器学习应用中被应用的潜力。然而，由于涉及到隐式特定于联合（样本）的条件分布，因此制定这个问题是具有挑战性的，特别是在（i）条件变量是连续的和（ii）两个分布中该变量的边缘是不同的情况下。我们通过采用特定的基于 MMD（最大均值差异）的核正则化器来克服这些挑战。

    Given samples from two joint distributions, we consider the problem of Optimal Transportation (OT) between the corresponding distributions conditioned on a common variable. The objective of this work is to estimate the associated transport cost (Wasserstein distance) as well as the transport plan between the conditionals as a function of the conditioned value. Since matching conditional distributions is at the core of supervised training of discriminative models and (implicit) conditional-generative models, OT between conditionals has the potential to be employed in diverse machine learning applications. However, since the conditionals involved in OT are implicitly specified via the joint samples, it is challenging to formulate this problem, especially when (i) the variable conditioned on is continuous and (ii) the marginal of this variable in the two distributions is different. We overcome these challenges by employing a specific kernel MMD (Maximum Mean Discrepancy) based regularizer
    
[^134]: LFTK: 计算语言学中的手工特征

    LFTK: Handcrafted Features in Computational Linguistics. (arXiv:2305.15878v1 [cs.CL])

    [http://arxiv.org/abs/2305.15878](http://arxiv.org/abs/2305.15878)

    该论文收集和分类了超过220个受欢迎的手工语言特征，设计了一个多语言的手工语言特征提取系统，以系统性的可扩展方式实现，并在几个任务特定的数据集上进行了相关性分析研究。

    

    过去的研究已经鉴定出了一组丰富的手工语言特征，可以潜在地帮助各种任务。但是，由于这些特征数量庞大，因此难以有效地选择和利用现有的手工特征。加上在研究工作中实现不一致的问题，目前还不存在分类方案或者统一接受的特征名称，这造成了不必要的混乱。此外，大多数现有的手工特征提取库都不是开源的，或者没有得到积极的维护。因此，研究人员经常需要从零开始构建这样的提取系统。我们通过过去的文献收集和分类了超过220个受欢迎的手工特征。然后，我们在几个任务特定的数据集上进行了相关性分析研究，并报告了每个特征的潜在用途。最后，我们设计了一个多语言的手工语言特征提取系统，以系统性的可扩展方式实现。我们开源了我们的系统。

    Past research has identified a rich set of handcrafted linguistic features that can potentially assist various tasks. However, their extensive number makes it difficult to effectively select and utilize existing handcrafted features. Coupled with the problem of inconsistent implementation across research works, there has been no categorization scheme or generally-accepted feature names. This creates unwanted confusion. Also, most existing handcrafted feature extraction libraries are not open-source or not actively maintained. As a result, a researcher often has to build such an extraction system from the ground up.  We collect and categorize more than 220 popular handcrafted features grounded on past literature. Then, we conduct a correlation analysis study on several task-specific datasets and report the potential use cases of each feature. Lastly, we devise a multilingual handcrafted linguistic feature extraction system in a systematically expandable manner. We open-source our system
    
[^135]: 从理论角度揭示“思维链”背后的奥秘

    Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])

    [http://arxiv.org/abs/2305.15408](http://arxiv.org/abs/2305.15408)

    本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。

    

    最近的研究发现，"思维链"提示能够显著提高大型语言模型（LLMs）的性能，特别是在涉及数学或推理的复杂任务中。尽管获得了巨大的实证成功，但“思维链”背后的机制以及它如何释放LLMs的潜力仍然是神秘的。本文首次从理论上回答了这些问题。具体而言，我们研究了LLMs带有“思维链”在解决基本数学和决策问题中的能力。我们首先给出一个不可能的结果，表明任何有限深度的Transformer都不能直接输出正确的基本算术/方程任务的答案，除非模型大小随着输入长度的增加呈超多项式增长。相反，我们通过构造证明，大小恒定的自回归Transformer足以通过使用常用的数学语言形式生成“思维链”推导来解决这两个任务。

    Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
    
[^136]: 带有语音的LM：超越语音令牌的口语语言建模

    LMs with a Voice: Spoken Language Modeling beyond Speech Tokens. (arXiv:2305.15255v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15255](http://arxiv.org/abs/2305.15255)

    SPECTRON是一个新颖的语音延续模型，通过利用预训练的语言模型和语音编码器进行端到端的训练来生成文本和语音输出，在语义内容和讲话者保护方面超越了现有的口语语言模型。

    

    我们提出了SPECTRON，一种新颖的方法来适应预训练的语言模型（LM）以执行语音延续。通过利用预训练的语音编码器，我们的模型可以生成文本和语音输出，整个系统都在频谱图上进行端到端的训练。在频谱图领域训练整个模型相对于使用离散语音表示的现有级联方法简化了我们的语音延续系统。我们进一步展示了我们的方法在语义内容和讲话者保护方面超过了现有的口语语言模型，同时也从预先存在的模型中获得了知识传递的好处。我们的网站https://michelleramanovich.github.io/spectron/spectron上可以找到音频样本。

    We present SPECTRON, a novel approach to adapting pre-trained language models (LMs) to perform speech continuation. By leveraging pre-trained speech encoders, our model generates both text and speech outputs with the entire system being trained end-to-end operating directly on spectrograms. Training the entire model in the spectrogram domain simplifies our speech continuation system versus existing cascade methods which use discrete speech representations. We further show our method surpasses existing spoken language models both in semantic content and speaker preservation while also benefiting from the knowledge transferred from pre-existing models. Audio samples can be found in our website https://michelleramanovich.github.io/spectron/spectron
    
[^137]: 使用预训练的语音嵌入理解孤独症儿童口语语言发展

    Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings. (arXiv:2305.14117v1 [eess.AS] CROSS LISTED)

    [http://arxiv.org/abs/2305.14117](http://arxiv.org/abs/2305.14117)

    该论文提出使用语音处理技术结合自然语言样本(NLS)分析孤独症儿童的口语发展水平，能够分类出儿童和成人语音，以及语音和非语言发声，并取得了较高的准确率。

    

    语音处理技术对于分析孤独症谱系障碍(ASD)儿童的语音和语言发展很有帮助，这些孩子在获得这些技能方面常常是多样性和延迟的。尽早识别和干预是至关重要的，但传统的评估方法如护理人员报告对于所需行为表型描述是不足够的。通过自然语言样本(NLS)分析已获得关注作为一种有前途的补充手段。研究人员在分析NLS中孤独症儿童的口语能力水平时已经制定了基准。本文提出了语音处理技术的应用，通过分类来支持儿童口语语言发展的自动化评估，包括分辨儿童和成人语音，以及语音和非语言发声，其F1宏平均分别为82.6％和67.8％，强调了ASD研究和临床应用的潜力。

    Speech processing techniques are useful for analyzing speech and language development in children with Autism Spectrum Disorder (ASD), who are often varied and delayed in acquiring these skills. Early identification and intervention are crucial, but traditional assessment methodologies such as caregiver reports are not adequate for the requisite behavioral phenotyping. Natural Language Sample (NLS) analysis has gained attention as a promising complement. Researchers have developed benchmarks for spoken language capabilities in children with ASD, obtainable through the analysis of NLS. This paper proposes applications of speech processing technologies in support of automated assessment of children's spoken language development by classification between child and adult speech and between speech and nonverbal vocalization in NLS, with respective F1 macro scores of 82.6% and 67.8%, underscoring the potential for accurate and scalable tools for ASD research and clinical use.
    
[^138]: 增量因果图学习进行在线无监督根本原因分析

    Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis. (arXiv:2305.10638v1 [cs.LG])

    [http://arxiv.org/abs/2305.10638](http://arxiv.org/abs/2305.10638)

    本文提出了CORAL，一种用于在线无监督根本原因分析的新框架，可以自动触发该过程并增量更新模型，包括三个主要部分：触发点检测，增量因果图学习和基于网络传播的根本原因定位。

    

    根本原因分析（RCA）的任务是分析系统监控数据，以识别系统故障/失效的根本原因。有效的RCA可以大大加速系统故障恢复，并减轻系统损失或财务损失。然而，以前的研究大多集中在开发离线RCA算法上，这通常需要手动启动RCA过程，需要大量时间和数据来训练稳健的模型，然后需要从头开始重新训练新的系统故障。在本文中，我们提出了CORAL，一种新颖的在线RCA框架，可以自动触发RCA过程并增量更新RCA模型。CORAL包括触发点检测、增量解缠因果图学习和基于网络传播的根本原因定位。触发点检测组件旨在自动检测系统状态转换并进行准实时检测。为此，我们开发了一种基于m的在线触发点检测方法。

    The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.  In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on m
    
[^139]: CNN 压缩的评估度量

    Evaluation Metrics for CNNs Compression. (arXiv:2305.10616v1 [cs.LG])

    [http://arxiv.org/abs/2305.10616](http://arxiv.org/abs/2305.10616)

    本文提供了神经网络压缩的评估度量综述，从而为标准化神经网络压缩贡献力量。

    

    研究人员致力于开发不同的神经网络压缩技术，但社区似乎缺乏标准化的评估和比较不同压缩技术的方法，这是识别不同应用程序的最合适的压缩技术的关键。本文通过提出评估度量的综述来为神经网络压缩的标准化贡献。这些度量已被实现到NetZIP，一个标准化的神经网络压缩基准之中。我们通过三个案例研究展示一些被审查的度量，分别聚焦于目标分类、目标检测和边缘设备。

    There is a lot of research effort devoted by researcher into developing different techniques for neural networks compression, yet the community seems to lack standardised ways of evaluating and comparing between different compression techniques, which is key to identifying the most suitable compression technique for different applications. In this paper we contribute towards standardisation of neural network compression by providing a review of evaluation metrics. These metrics have been implemented into NetZIP, a standardised neural network compression bench. We showcase some of the metrics reviewed using three case studies focusing on object classification, object detection, and edge devices.
    
[^140]: 基于卷积神经网络的自动照片方向检测

    Automatic Photo Orientation Detection with Convolutional Neural Networks. (arXiv:2305.10319v1 [cs.CV])

    [http://arxiv.org/abs/2305.10319](http://arxiv.org/abs/2305.10319)

    本论文使用CNN解决了照片方向检测的问题，并在数据集上显著提高性能。使用Guided Backpropagation获得了CNN检测方向的见解。

    

    本文探讨了利用卷积神经网络(CNN)来解决确定消费者照片正确方向(0°, 90°, 180°和270°)的问题，特别对于模拟照片的数字化非常重要。我们在标准数据集上显著提高了性能，并在更困难的消费者照片大型数据集上进行了测试。我们使用引导反向传播(Guided Backpropagation)来获得关于CNN如何检测照片方向的见解，并解释其错误。

    We apply convolutional neural networks (CNN) to the problem of image orientation detection in the context of determining the correct orientation (from 0, 90, 180, and 270 degrees) of a consumer photo. The problem is especially important for digitazing analog photographs. We substantially improve on the published state of the art in terms of the performance on one of the standard datasets, and test our system on a more difficult large dataset of consumer photos. We use Guided Backpropagation to obtain insights into how our CNN detects photo orientation, and to explain its mistakes.
    
[^141]: 带有非对角信息的视觉-语言连续表示学习

    Continual Vision-Language Representaion Learning with Off-Diagonal Information. (arXiv:2305.07437v1 [cs.LG])

    [http://arxiv.org/abs/2305.07437](http://arxiv.org/abs/2305.07437)

    本文探讨了通过流数据持续训练CLIP模型的可行性，提出了一种有效的连续学习框架Mod-X，并证明内部旋转和跨模态偏差导致了CLIP在跨模态检索任务中性能下降。

    

    本文讨论了通过流数据持续训练CLIP模型的可行性。通过追踪连续更新的CLIP模型中表示向量的方向变化，我们探索和总结了这些空间变化，称为空间混乱（SD），可以分为内部旋转和跨模态偏差。此外，我们从经验和理论上证明了内部旋转和跨模态偏差如何导致CLIP在跨模态检索任务中性能下降。为了缓解空间混乱，我们提出了一种简单而有效的连续学习框架Mod-X: 维护非对角信息矩阵。在各种不同规模和范围的常用数据集上的实验表明了我们方法的有效性。

    This paper discusses the feasibility of continuously training the CLIP model through streaming data. Then, by tracking the directional changes of the representation vectors in the continuously updated CLIP model, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we demonstrate how intra-modal rotation and inter-modal deviation lead to a performance decline for CLIP on cross-modal retrieval tasks in both empirically and theoretically. To alleviate the spatial disorder, we propose a simple yet effective continual learning framework Mod-X: Maintain off-diagonal information-matriX. The experiments (in Section \ref{method}, \ref{experiments} and Appendix \ref{Appendix_to_experiments}) on commonly used datasets with different scales and scopes have illustrated the effectiveness of our method.
    
[^142]: 降低现实策略优化中循环时间调整的代价

    Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization. (arXiv:2305.05760v1 [cs.LG])

    [http://arxiv.org/abs/2305.05760](http://arxiv.org/abs/2305.05760)

    本研究提出了一种新方法，可基于周期时间设置超参数，使得PPO和SAC在广泛的循环时间范围内进行学习，同时实现了接近耗时的在线超参数调整获得的性能。

    

    连续时间强化学习任务通常使用固定周期时间的离散步骤进行操作。实践中需要为给定任务选择操作周期时间，一个重要问题是学习算法的超参数是否需要为每个周期时间重新调整，这对于现实世界的机器人来说是不可行的。在本文中，我们研究了两种策略梯度算法--PPO和SAC--在不同的周期时间下使用基线超参数值的情况。通过使用基线超参数在一个基准任务中展示这两种算法表现良好的情况，我们发现当选择不同于任务默认值的周期时间时，使用基线超参数的PPO无法学习。此外，当超参数用于每个周期时间时，基于基线的PPO和SAC表现均明显劣于它们的调整值。我们提出了一种基于周期时间设置这些超参数的新方法。在我们的仿真机器人运动任务实验中，我们的方法使得PPO和SAC在极其广泛的周期时间范围内进行学习，同时实现了接近于耗时的在线超参数调整获得的性能。

    Continuous-time reinforcement learning tasks commonly use discrete steps of fixed cycle times for actions. As practitioners need to choose the action-cycle time for a given task, a significant concern is whether the hyper-parameters of the learning algorithm need to be re-tuned for each choice of the cycle time, which is prohibitive for real-world robotics. In this work, we investigate the widely-used baseline hyper-parameter values of two policy gradient algorithms -- PPO and SAC -- across different cycle times. Using a benchmark task where the baseline hyper-parameters of both algorithms were shown to work well, we reveal that when a cycle time different than the task default is chosen, PPO with baseline hyper-parameters fails to learn. Moreover, both PPO and SAC with their baseline hyper-parameters perform substantially worse than their tuned values for each cycle time. We propose novel approaches for setting these hyper-parameters based on the cycle time. In our experiments on simu
    
[^143]: 可扩展的最优边缘分布机（Scalable Optimal Margin Distribution Machine）

    Scalable Optimal Margin Distribution Machine. (arXiv:2305.04837v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04837](http://arxiv.org/abs/2305.04837)

    本文提出了一种可扩展的最优边缘分布机（ODM）训练方法，与原始ODM训练方法相比，可实现近十倍的加速。对于非线性核，我们提出了一种新颖的分布感知分区方法，使得每个分区上训练的本地ODM接近全局的ODM，并快速收敛。在应用线性核时，我们扩展了一种通信有效的SVRG方法以进一步加速训练。

    

    最优边缘分布机（ODM）是一种新型的统计学习框架，根据新的边缘理论建立，表现出比传统的基于大间隔的对应方法更好的泛化性能。然而，像其他核方法一样，它在计算时间和内存方面普遍存在可扩展性问题。本文提出了一种可扩展的ODM，与原始ODM训练方法相比，可实现近十倍的加速。对于非线性核，我们提出了一种新颖的分布感知分区方法，使得每个分区上训练的本地ODM接近全局的ODM，并快速收敛。当应用线性核时，我们扩展了一种通信有效的SVRG方法以进一步加速训练。大量经验证据表明，我们提出的方法在计算效率方面极高，并且几乎不会恶化泛化性能。

    Optimal margin Distribution Machine (ODM) is a newly proposed statistical learning framework rooting in the novel margin theory, which demonstrates better generalization performance than the traditional large margin based counterparts. Nonetheless, it suffers from the ubiquitous scalability problem regarding both computation time and memory as other kernel methods. This paper proposes a scalable ODM, which can achieve nearly ten times speedup compared to the original ODM training method. For nonlinear kernels, we propose a novel distribution-aware partition method to make the local ODM trained on each partition be close and converge fast to the global one. When linear kernel is applied, we extend a communication efficient SVRG method to accelerate the training further. Extensive empirical studies validate that our proposed method is highly computational efficient and almost never worsen the generalization.
    
[^144]: 基于矩阵流形的神经网络构建：陀螺矢量空间方法

    Building Neural Networks on Matrix Manifolds: A Gyrovector Space Approach. (arXiv:2305.04560v1 [stat.ML])

    [http://arxiv.org/abs/2305.04560](http://arxiv.org/abs/2305.04560)

    本文通过将陀螺矢量空间中的一些概念推广到SPD和Grassmann流形，提出了在这些流形上构建神经网络的新模型和新层，并在人类动作识别和知识图谱完成两个应用中展示了其有效性。

    

    矩阵流形，如对称正定（SPD）矩阵和Grassmann流形，出现在许多应用中。最近，通过应用陀螺群和陀螺矢量空间的理论——这是一个研究双曲几何的强大框架——一些工作尝试在矩阵流形上构建欧几里德神经网络的原则性推广。然而，由于缺乏考虑流形的内积和陀螺角等概念的陀螺矢量空间，相比于用于研究双曲几何的那些概念，这些工作提供的技术和数学工具仍然有限。在本文中，我们将陀螺矢量空间中的一些概念推广到SPD和Grassmann流形，并提出了在这些流形上构建神经网络的新模型和新层。我们展示了我们的方法在人类动作识别和知识图谱完成两个应用中的有效性。

    Matrix manifolds, such as manifolds of Symmetric Positive Definite (SPD) matrices and Grassmann manifolds, appear in many applications. Recently, by applying the theory of gyrogroups and gyrovector spaces that is a powerful framework for studying hyperbolic geometry, some works have attempted to build principled generalizations of Euclidean neural networks on matrix manifolds. However, due to the lack of many concepts in gyrovector spaces for the considered manifolds, e.g., the inner product and gyroangles, techniques and mathematical tools provided by these works are still limited compared to those developed for studying hyperbolic geometry. In this paper, we generalize some notions in gyrovector spaces for SPD and Grassmann manifolds, and propose new models and layers for building neural networks on these manifolds. We show the effectiveness of our approach in two applications, i.e., human action recognition and knowledge graph completion.
    
[^145]: 多保真度深度贝叶斯主动学习的解缠框架

    Disentangled Multi-Fidelity Deep Bayesian Active Learning. (arXiv:2305.04392v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04392](http://arxiv.org/abs/2305.04392)

    本论文提出了一种新的框架D-MFDAL，它可以学习具有多个保真度的函数分布，相比于现有基于高斯过程的方法，该方法具有更好的性能和高斯过程的灵活性和可解释性。

    

    为了平衡质量和成本，科学和工程的多个领域会在多个复杂程度上运行模拟。多保真度主动学习旨在通过从多个保真度水平积极地获取数据来学习从输入参数到模拟输出的直接映射。然而，现有的基于高斯过程的方法很难扩展到高维数据。基于深度学习的方法通常在隐藏表示中强制实施分层结构，仅支持从低保真度到高保真度传递信息。这些方法可能导致从低保真度表示到高保真度表示中不良误差的传播。我们提出了一种称为解缠多保真度深度贝叶斯主动学习（D-MFDAL）的新框架，该框架学习了在多个保真度上函数分布的条件下的代理模型。在学习偏微分方程的深度代理的基准任务中，我们的方法展示了优越的性能，同时保留了高斯过程的灵活性和可解释性。

    To balance quality and cost, various domain areas of science and engineering run simulations at multiple levels of sophistication. Multi-fidelity active learning aims to learn a direct mapping from input parameters to simulation outputs at the highest fidelity by actively acquiring data from multiple fidelity levels. However, existing approaches based on Gaussian processes are hardly scalable to high-dimensional data. Deep learning-based methods often impose a hierarchical structure in hidden representations, which only supports passing information from low-fidelity to high-fidelity. These approaches can lead to the undesirable propagation of errors from low-fidelity representations to high-fidelity ones. We propose a novel framework called Disentangled Multi-fidelity Deep Bayesian Active Learning (D-MFDAL), that learns the surrogate models conditioned on the distribution of functions at multiple fidelities. On benchmark tasks of learning deep surrogates of partial differential equatio
    
[^146]: 基于扩散ODEs最大似然估计的改进技术

    Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs. (arXiv:2305.03935v1 [cs.LG])

    [http://arxiv.org/abs/2305.03935](http://arxiv.org/abs/2305.03935)

    本文提出了基于扩散ODEs最大似然估计的改进技术，包括速度参数化和方差减少技术等用于训练的技术，以及误差有界的高阶流匹配目标用于微调和截断正态去量化方法用于评估。

    

    扩散模型在各领域中表现出良好的性能。扩散模型的概率流常微分方程（ODE）（即扩散ODE）是连续归一化流（CNFs）的一个特例，它使得确定性推断和精确似然评估成为可能。然而，与最先进的基于似然的生成模型相比，扩散ODE的似然估计结果仍有很大差距。在本文中，我们提出了一些改进的技术，包括训练和评估两个方面，用于扩散ODE的最大似然估计。对于训练，我们提出了速度参数化，并探索方差减少技术以加快收敛速度。我们还设计了一个误差有界的高阶流匹配目标用于微调，从而提高ODE的似然估计并平滑其轨迹。对于评估，我们提出了一种新颖的无须训练的截断正态去量化方法来填补训练-评估间的差距。

    Diffusion models have exhibited excellent performance in various domains. The probability flow ordinary differential equation (ODE) of diffusion models (i.e., diffusion ODEs) is a particular case of continuous normalizing flows (CNFs), which enables deterministic inference and exact likelihood evaluation. However, the likelihood estimation results by diffusion ODEs are still far from those of the state-of-the-art likelihood-based generative models. In this work, we propose several improved techniques for maximum likelihood estimation for diffusion ODEs, including both training and evaluation perspectives. For training, we propose velocity parameterization and explore variance reduction techniques for faster convergence. We also derive an error-bounded high-order flow matching objective for finetuning, which improves the ODE likelihood and smooths its trajectory. For evaluation, we propose a novel training-free truncated-normal dequantization to fill the training-evaluation gap commonly
    
[^147]: 如何发挥大语言模型在少样本关系抽取中的能力？

    How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])

    [http://arxiv.org/abs/2305.01555](http://arxiv.org/abs/2305.01555)

    本文通过使用GPT-3.5模型在少样本关系抽取中，实现在四个不同数据集上的新的最优性能，并提出了与任务相关的指导说明和约束模式下的数据生成方法。

    

    语言模型的扩展已经彻底改变了广泛的自然语言处理任务，但是使用大型语言模型进行少样本关系抽取还没有得到全面探索。本文通过详细实验，研究了使用GPT-3.5进行少样本关系抽取的基本方法——上下文学习和数据生成。为了增强少样本性能，我们进一步提出了与任务相关的指导说明和约束模式下的数据生成。我们观察到，在上下文学习的情况下，可以实现与以前的提示学习方法相当的性能，而使用大型语言模型的数据生成可以推动以前的解决方案以在四个广泛研究的关系抽取数据集上获得新的最先进的少样本结果。我们希望我们的工作可以激发未来对大型语言模型在少样本关系抽取中的能力的研究。代码可以在 \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} 中找到。

    Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
    
[^148]: 慢混合过程的泛化能力研究

    Generalization for slowly mixing processes. (arXiv:2305.00977v1 [cs.LG])

    [http://arxiv.org/abs/2305.00977](http://arxiv.org/abs/2305.00977)

    该论文研究了慢混合过程的泛化能力，给出了对由平稳且phi混合过程生成的数据的不同种类损失类别的一种上界。

    

    该论文给出了针对由平稳且phi混合过程生成的数据的各种损失类别的一种上界，其中混合时间（获得近似独立所需的时间）仅以加法方式进入样本复杂度。对于慢速混合过程而言，这可以是其优势，因为其与混合时间的乘法依赖性相比要好。允许的损失类别包括具有指定Lipschitz归一化或平滑度参数的函数。该上界还可以应用于对无限制损失类别的统一性研究，其取决于样本路径上函数的局部Lipschitz性质。

    A bound uniform over various loss-classes is given for data generated by stationary and phi-mixing processes, where the mixing time (the time needed to obtain approximate independence) enters the sample complexity only in an additive way. For slowly mixing processes this can be a considerable advantage over results with multiplicative dependence on the mixing time. The admissible loss-classes include functions with prescribed Lipschitz norms or smoothness parameters. The bound can also be applied to be uniform over unconstrained loss-classes, where it depends on local Lipschitz properties of the function on the sample path.
    
[^149]: 架起桥梁：自然语言生成中整合（人类）反馈的调查研究。

    Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation. (arXiv:2305.00955v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.00955](http://arxiv.org/abs/2305.00955)

    本文调查研究了利用人类反馈改进自然语言生成方面的最近研究，包括对反馈的全面形式化、反馈的格式和目的的描述，和提出使用反馈的两种方法。我们还讨论了用于人类反馈数据收集的现有数据集，和相关的担忧。

    

    最近许多自然语言生成方面的进展都是基于利用互联网规模的数据对大型语言模型进行训练。然而，这种范式可能会导致生成有害、不准确和无用内容的模型，而自动评估指标通常无法识别这些行为。随着模型变得更加强大，人类反馈成为评价和改进模型的宝贵信号。本调查旨在概述最近利用人类反馈改进自然语言生成方面的研究。首先，我们介绍了对反馈的全面形式化，并根据这种形式化将现有研究进行分类和组织。接下来，我们讨论了反馈可以通过其格式和目的来描述，并涵盖了提出使用反馈的两种方法（用于训练或解码）：直接使用反馈或训练反馈模型。我们还讨论了用于人类反馈数据收集的现有数据集，以及相关的担忧。

    Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns 
    
[^150]: 电力系统行为模式及应用机器学习中的泛化风险

    Power Grid Behavioral Patterns and Risks of Generalization in Applied Machine Learning. (arXiv:2304.10702v1 [eess.SY])

    [http://arxiv.org/abs/2304.10702](http://arxiv.org/abs/2304.10702)

    本论文基于实际运行数据，分析了电力系统行为模式及模型泛化风险。研究结果表明，忽略电网特定模式可能导致对新输入输出不可行、不可实现或完全无意义的预测。

    

    近年来，出现了大量针对电力系统应用的数据驱动方法的文献。然而，忽略领域知识会对这些方法的实用性造成高风险。具体来说，忽略电网特定的时空模式（负荷、发电和拓扑等）可能导致对新输入输出不可行、不可实现或完全无意义的预测。为了解决这个问题，本文调查实际运行数据，提供电力系统行为模式的见解，包括时变的拓扑、负荷和发电以及单个负荷和发电之间的空间差异（在峰值时间、多种风格下）。然后根据这些观察结果，评估了一些现有机器学习方法在模型设计和训练中忽略这些电网特定模式所造成的泛化风险。

    Recent years have seen a rich literature of data-driven approaches designed for power grid applications. However, insufficient consideration of domain knowledge can impose a high risk to the practicality of the methods. Specifically, ignoring the grid-specific spatiotemporal patterns (in load, generation, and topology, etc.) can lead to outputting infeasible, unrealizable, or completely meaningless predictions on new inputs. To address this concern, this paper investigates real-world operational data to provide insights into power grid behavioral patterns, including the time-varying topology, load, and generation, as well as the spatial differences (in peak hours, diverse styles) between individual loads and generations. Then based on these observations, we evaluate the generalization risks in some existing ML works causedby ignoring these grid-specific patterns in model design and training.
    
[^151]: 基于图神经网络的河流系统异常检测

    Graph Neural Network-Based Anomaly Detection for River Network Systems. (arXiv:2304.09367v1 [cs.LG])

    [http://arxiv.org/abs/2304.09367](http://arxiv.org/abs/2304.09367)

    本研究采用图神经网络模型Graph Deviation Network (GDN)来捕捉河流传感器数据的复杂时空关系，并提出了备用异常阈值标准GDN+，以实现对水质的准确持续监测。

    

    水是河流网络的生命线，其质量对维护水生态系统和人类社会都有着至关重要的作用。现场传感器技术越来越依赖实时监测水质。异常检测是识别传感器数据中错误模式的关键步骤，但由于数据复杂性和变异性，即使在正常情况下也是一项具有挑战性的任务。本文提出了一种针对河流网络传感器数据异常检测的解决方案，这对于精确持续监测水质非常重要。我们使用图神经网络模型——最近提出的Graph Deviation Network (GDN)，它利用基于图注意力的预测来捕捉传感器之间复杂的时空关系。我们根据所学图形提出了模型GDN+的备用异常阈值标准。为了评估模型的有效性，我们引入了新的基准仿真实验。

    Water is the lifeblood of river networks, and its quality plays a crucial role in sustaining both aquatic ecosystems and human societies. Real-time monitoring of water quality is increasingly reliant on in-situ sensor technology. Anomaly detection is crucial for identifying erroneous patterns in sensor data, but can be a challenging task due to the complexity and variability of the data, even under normal conditions. This paper presents a solution to the challenging task of anomaly detection for river network sensor data, which is essential for the accurate and continuous monitoring of water quality. We use a graph neural network model, the recently proposed Graph Deviation Network (GDN), which employs graph attention-based forecasting to capture the complex spatio-temporal relationships between sensors. We propose an alternate anomaly threshold criteria for the model, GDN+, based on the learned graph. To evaluate the model's efficacy, we introduce new benchmarking simulation experimen
    
[^152]: 用于不同类型癌症分类的EfficientNet算法

    EfficientNet Algorithm for Classification of Different Types of Cancer. (arXiv:2304.08715v1 [eess.IV])

    [http://arxiv.org/abs/2304.08715](http://arxiv.org/abs/2304.08715)

    本文用EfficientNet算法分类不同类型的癌症，实验结果表明该算法在公共数据集上表现优异，具有在临床实践中提高癌症诊断准确性和效率的潜力。

    

    准确高效地识别不同类型癌症对早期发现和有效治疗至关重要。本文介绍了使用EfficientNet算法分类脑瘤、乳腺癌乳房X线摄影、胸部癌和皮肤癌的实验结果。我们使用公共数据集并对图像进行预处理以确保一致性和可比性。实验表明EfficientNet算法在每个癌症数据集上都取得了高精度、高召回率和高F1分数，优于文献中其他最先进的算法。我们还讨论了EfficientNet算法的优缺点及其在临床实践中的潜在应用。结果表明，EfficientNet算法非常适用于不同类型癌症的分类，并可用于提高癌症诊断的准确性和效率。

    Accurate and efficient classification of different types of cancer is critical for early detection and effective treatment. In this paper, we present the results of our experiments using the EfficientNet algorithm for classification of brain tumor, breast cancer mammography, chest cancer, and skin cancer. We used publicly available datasets and preprocessed the images to ensure consistency and comparability. Our experiments show that the EfficientNet algorithm achieved high accuracy, precision, recall, and F1 scores on each of the cancer datasets, outperforming other state-of-the-art algorithms in the literature. We also discuss the strengths and weaknesses of the EfficientNet algorithm and its potential applications in clinical practice. Our results suggest that the EfficientNet algorithm is well-suited for classification of different types of cancer and can be used to improve the accuracy and efficiency of cancer diagnosis.
    
[^153]: Data-OOB:以无需额外计算的Out-of-bag估计为准的数据价值估计方法

    Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value. (arXiv:2304.07718v1 [cs.LG])

    [http://arxiv.org/abs/2304.07718](http://arxiv.org/abs/2304.07718)

    Data-OOB是一种新的数据价值估计方法，它利用out-of-bag估计，并可以在计算上高效处理大型数据集。

    

    数据评估是一个强大的框架，可以为模型训练提供统计洞察力，以区分哪些数据对于模型训练是有益的，哪些是有害的。纵观各种下游任务，许多以Shapley为基础的数据价值评估方法均显示出了很有前途的结果。然而，由于这需要训练大量的模型，因此众所周知，这是具有挑战性的。因此，将此应用于大型数据集是不可行的。为了解决这个问题，我们提出了Data-OOB，这是一种新的数据价值估计方法，针对bagging模型，它利用了out-of-bag估计。所提出的方法在计算上是高效的，可以通过重复使用训练好的弱学习器来扩展到数百万个数据。具体而言，当评估100个输入维度且存在$10^6$个样本时，Data-OOB仅需要在单个CPU处理器上执行不到2.25个小时。此外，Data-OOB在理论上有坚实的解释，当两个离差值函数相同时，其识别具有相同重要性的数据点。

    Data valuation is a powerful framework for providing statistical insights into which data are beneficial or detrimental to model training. Many Shapley-based data valuation methods have shown promising results in various downstream tasks, however, they are well known to be computationally challenging as it requires training a large number of models. As a result, it has been recognized as infeasible to apply to large datasets. To address this issue, we propose Data-OOB, a new data valuation method for a bagging model that utilizes the out-of-bag estimate. The proposed method is computationally efficient and can scale to millions of data by reusing trained weak learners. Specifically, Data-OOB takes less than 2.25 hours on a single CPU processor when there are $10^6$ samples to evaluate and the input dimension is 100. Furthermore, Data-OOB has solid theoretical interpretations in that it identifies the same important data point as the infinitesimal jackknife influence function when two d
    
[^154]: 一种基于数据驱动的状态聚合方法用于动态离散选择模型

    A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models. (arXiv:2304.04916v1 [cs.LG])

    [http://arxiv.org/abs/2304.04916](http://arxiv.org/abs/2304.04916)

    本文提出了一种基于数据驱动的状态聚合方法来降低动态离散选择模型的估计计算和样本复杂度，首先利用反向强化学习估计代理Q函数，然后用聚类算法选择重要的状态聚合，最终利用嵌套固定点算法进行最大似然估计。

    

    我们研究了动态离散选择模型，其中一个常见的问题是使用代理行为数据估计代理奖励函数（也称为“结构参数”）的参数。这种模型的最大似然估计需要动态规划，这受到维度灾难的限制。在本文中，我们提出了一种新颖的算法，提供了一种数据驱动的方法来选择和聚合状态，降低了估计的计算和样本复杂度。我们的方法分两个阶段。在第一阶段中，我们使用灵活的反向强化学习方法来估计代理Q函数。我们使用这些估计的Q函数，以及一个聚类算法，选择了一些最为重要的状态，这些状态对于驱动Q函数的变化最为关键。在第二阶段，利用这些被选择的“聚合”状态，我们使用常用的嵌套固定点算法进行最大似然估计。所提出的二阶段方法实现了...

    We study dynamic discrete choice models, where a commonly studied problem involves estimating parameters of agent reward functions (also known as "structural" parameters), using agent behavioral data. Maximum likelihood estimation for such models requires dynamic programming, which is limited by the curse of dimensionality. In this work, we present a novel algorithm that provides a data-driven method for selecting and aggregating states, which lowers the computational and sample complexity of estimation. Our method works in two stages. In the first stage, we use a flexible inverse reinforcement learning approach to estimate agent Q-functions. We use these estimated Q-functions, along with a clustering algorithm, to select a subset of states that are the most pivotal for driving changes in Q-functions. In the second stage, with these selected "aggregated" states, we conduct maximum likelihood estimation using a commonly used nested fixed-point algorithm. The proposed two-stage approach 
    
[^155]: 基于大脑启发的尖峰神经网络，用于在线无监督时间序列预测

    Brain-Inspired Spiking Neural Network for Online Unsupervised Time Series Prediction. (arXiv:2304.04697v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2304.04697](http://arxiv.org/abs/2304.04697)

    本论文提出了一种新的连续学习无监督循环尖峰神经网络模型，可以高效、节能地进行在线时间序列预测，并且通过随机延迟嵌入和使用底层动态系统中介数中心性最高的神经元的膜电位进行重构。

    

    在多个领域中，特别是需要基于流数据不断更新的边缘AI应用中，能够高效、节能地进行在线时间序列预测是至关重要的。然而，当前基于深度神经网络的有监督在线学习模型需要大量的训练数据，且无法在底层系统发生变化时快速适应。此外，这些模型需要使用传入数据进行持续的重新训练，使其高度低效。为了解决这些问题，我们提出了一种新的连续学习无监督循环尖峰神经网络模型（CLURSNN），其通过尖峰时序相关可塑性（STDP）进行训练。CLURSNN通过使用在可重构RSNN的具有最高介数中心性的神经元的膜电位来进行随机延迟嵌入，以重构底层动态系统进行在线预测。我们还利用拓扑数据分析提出了一种新的方法，使用Wasserstein距离。

    Energy and data-efficient online time series prediction for predicting evolving dynamical systems are critical in several fields, especially edge AI applications that need to update continuously based on streaming data. However, current DNN-based supervised online learning models require a large amount of training data and cannot quickly adapt when the underlying system changes. Moreover, these models require continuous retraining with incoming data making them highly inefficient. To solve these issues, we present a novel Continuous Learning-based Unsupervised Recurrent Spiking Neural Network Model (CLURSNN), trained with spike timing dependent plasticity (STDP). CLURSNN makes online predictions by reconstructing the underlying dynamical system using Random Delay Embedding by measuring the membrane potential of neurons in the recurrent layer of the RSNN with the highest betweenness centrality. We also use topological data analysis to propose a novel methodology using the Wasserstein Di
    
[^156]: 应用机器学习和领域知识个性化数字健康行为变革干预

    Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])

    [http://arxiv.org/abs/2304.03392](http://arxiv.org/abs/2304.03392)

    该论文提出了一种采用机器学习和领域知识进行个性化数字健康行为变革干预的系统，其利用反事实例子进行特征控制以预测干预效果并优化干预效果。

    

    我们正在开发一种虚拟教练系统，帮助患者坚持行为变革干预（BCI）。我们的系统预测患者是否会执行目标行为，并使用反事实例子进行特征控制，以指导个性化BCI。我们使用具有不同响应水平的模拟患者数据评估了我们的预测模型。

    We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
    
[^157]: G不变图拉普拉斯算子

    The G-invariant graph Laplacian. (arXiv:2303.17001v1 [cs.LG])

    [http://arxiv.org/abs/2303.17001](http://arxiv.org/abs/2303.17001)

    本文提出了 G不变图拉普拉斯算子 用于处理数据集不仅在流形上，而且在一个连续群的作用下也是封闭的情形，相较于标准图拉普拉斯算子收敛速度更快。

    

    基于图拉普拉斯算子的算法已经被证明在降维、聚类和去噪等领域对流形数据非常有效。本文考虑的数据集不仅在流形上，而且在一个连续群的作用下也是封闭的。这类数据集的一个例子是沿着低维流形传播的体积，其中每个体积可以在三维空间中旋转。我们介绍了G不变图拉普拉斯算子，通过考虑数据集上的群的作用来广义化图拉普拉斯算子。我们显示了与标准图拉普拉斯算子类似，G不变图拉普拉斯算子收敛于数据流形上的Laplace-Beltrami算子，但收敛速度显著提高。此外，我们还表明G不变图拉普拉斯算子的特征函数具有群元素和某些矩阵的特征向量的张量积形式，可以使用F高效地计算。

    Graph Laplacian based algorithms for data lying on a manifold have been proven effective for tasks such as dimensionality reduction, clustering, and denoising. In this work, we consider data sets whose data point not only lie on a manifold, but are also closed under the action of a continuous group. An example of such data set is volumes that line on a low dimensional manifold, where each volume may be rotated in three-dimensional space. We introduce the G-invariant graph Laplacian that generalizes the graph Laplacian by accounting for the action of the group on the data set. We show that like the standard graph Laplacian, the G-invariant graph Laplacian converges to the Laplace-Beltrami operator on the data manifold, but with a significantly improved convergence rate. Furthermore, we show that the eigenfunctions of the G-invariant graph Laplacian admit the form of tensor products between the group elements and eigenvectors of certain matrices, which can be computed efficiently using F
    
[^158]: 端到端扩散潜在优化提高分类器引导能力。

    End-to-End Diffusion Latent Optimization Improves Classifier Guidance. (arXiv:2303.13703v1 [cs.CV])

    [http://arxiv.org/abs/2303.13703](http://arxiv.org/abs/2303.13703)

    本文提出了一种新的分类器引导方法DOODL，它可以通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，并在计算和人类评估度量上优于一步分类器指导。

    

    分类器指导——利用图像分类器的梯度来引导扩散模型的生成——有潜力大幅扩展对图像生成和编辑的创造性控制。然而，目前分类器指导要么需要训练新的噪声感知模型以获得精确的梯度，要么使用一步去噪的近似最终生成物，并导致梯度不对齐和次优控制。我们强调了这种近似的缺点，并提出了一种新的指导方法：直接优化扩散潜变（DOODL），它通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，使用可逆扩散过程实现内存有效的反向传递。展示了更精确指导潜力的 DOODL 在不同形式的指导的计算和人类评估度量上优于一步分类器指导。

    Classifier guidance -- using the gradients of an image classifier to steer the generations of a diffusion model -- has the potential to dramatically expand the creative control over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control. We highlight this approximation's shortcomings and propose a novel guidance method: Direct Optimization of Diffusion Latents (DOODL), which enables plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more precise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidanc
    
[^159]: 利用反事实文本解释来解释推荐系统

    Explaining Recommendation System Using Counterfactual Textual Explanations. (arXiv:2303.11160v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2303.11160](http://arxiv.org/abs/2303.11160)

    本文提供了一种利用反事实推理来生成可理解解释的方法，其在推荐系统上取得了成功应用。

    

    目前，在人工智能领域，有大量的研究致力于改进深度学习模型的可解释性和可解读性。研究发现，如果最终用户理解某些输出的原因，就更容易信任系统。推荐系统是需要进行改进以使其输出更加可解释的系统之一。产生更可解释的输出的一种方法是使用反事实推理，这涉及对最小要素进行修改，以生成导致系统输出变化的反事实项目。这一过程允许识别对期望输出有重大影响的输入要素，从而提供有效的解释。在本文中，我们提出了一种方法来生成针对表格和文本要素的反事实解释。我们在三个真实数据集上评估了我们提出的方法的性能，并证明它在为最终用户提供可理解的解释方面是有效的。

    Currently, there is a significant amount of research being conducted in the field of artificial intelligence to improve the explainability and interpretability of deep learning models. It is found that if end-users understand the reason for the production of some output, it is easier to trust the system. Recommender systems are one example of systems that great efforts have been conducted to make their output more explainable. One method for producing a more explainable output is using counterfactual reasoning, which involves altering minimal features to generate a counterfactual item that results in changing the output of the system. This process allows the identification of input features that have a significant impact on the desired output, leading to effective explanations. In this paper, we present a method for generating counterfactual explanations for both tabular and textual features. We evaluated the performance of our proposed method on three real-world datasets and demonstra
    
[^160]: Aux-Drop: 使用辅助丢弃处理在线学习中的混乱输入

    Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts. (arXiv:2303.05155v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05155](http://arxiv.org/abs/2303.05155)

    本文介绍了一种名为Aux-Drop的策略，适用于在线学习，能够处理混乱的输入特征。通过调整常规的丢失正则化方案，确保最终输出受到这些特征混乱出现的最小影响，有助于构建更健壮有效的在线学习系统。

    

    许多基于在线学习的真实世界应用程序生成的流式数据具有混乱的特性，即包含缺失的特征，在时间上变得过时，以后出现新特征并且缺乏对总输入特征数量的清晰认识。这些挑战使得为这些应用构建可学习系统变得困难，并且几乎没有关于这个问题的深度学习研究工作。本文提出了Aux-Drop，一种用于在线学习的辅助丢失正则化策略，以有效处理混乱的输入特征。Aux-Drop为混乱的输入特征空间调整常规的丢失正则化方案，确保最终输出受到这些特征混乱出现的最小影响。它有助于防止特别是辅助和基础特征的共适应，并减少模型中任何辅助输入对输出的强烈依赖。这有助于为真实世界应用程序构建更健壮有效的在线学习系统。

    Many real-world applications based on online learning produce streaming data that is haphazard in nature, i.e., contains missing features, features becoming obsolete in time, the appearance of new features at later points in time and a lack of clarity on the total number of input features. These challenges make it hard to build a learnable system for such applications, and almost no work exists in deep learning that addresses this issue. In this paper, we present Aux-Drop, an auxiliary dropout regularization strategy for online learning that handles the haphazard input features in an effective manner. Aux-Drop adapts the conventional dropout regularization scheme for the haphazard input feature space ensuring that the final output is minimally impacted by the chaotic appearance of such features. It helps to prevent the co-adaptation of especially the auxiliary and base features, as well as reduces the strong dependence of the output on any of the auxiliary inputs of the model. This hel
    
[^161]: 布雷-瓦瑟斯坦距离训练下的生成式深度线性网络的关键点和收敛性分析

    Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss. (arXiv:2303.03027v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.03027](http://arxiv.org/abs/2303.03027)

    本文使用布雷-瓦瑟斯坦距离训练协方差矩阵的深度矩阵分解模型，并在有限秩矩阵空间内表征关键点和最小化问题，最终确定了梯度下降算法的收敛性。

    

    本文探讨了一种使用布雷-瓦瑟斯坦距离训练协方差矩阵的深度矩阵分解模型。相较于以往的研究，我们所提出的模型在损失函数和生成式设置上有所不同。我们在有限秩矩阵空间内表征了该方法的关键点和最小化问题。针对低秩矩阵而言，该方法的海森矩阵理论上可能会爆炸，这为优化方法的收敛性分析带来了挑战。我们确定了梯度下降算法中使用损失的平滑微扰版本时的收敛性，并在初始权重的一定假设条件下证明了有限步长梯度下降的收敛性。

    We consider a deep matrix factorization model of covariance matrices trained with the Bures-Wasserstein distance. While recent works have made important advances in the study of the optimization problem for overparametrized low-rank matrix approximation, much emphasis has been placed on discriminative settings and the square loss. In contrast, our model considers another interesting type of loss and connects with the generative setting. We characterize the critical points and minimizers of the Bures-Wasserstein distance over the space of rank-bounded matrices. For low-rank matrices the Hessian of this loss can theoretically blow up, which creates challenges to analyze convergence of optimizaton methods. We establish convergence results for gradient flow using a smooth perturbative version of the loss and convergence results for finite step size gradient descent under certain assumptions on the initial weights.
    
[^162]: 基于递归神经网络的神经算子学习在动力系统中的应用及长时间积分问题解决

    Neural Operator Learning for Long-Time Integration in Dynamical Systems with Recurrent Neural Networks. (arXiv:2303.02243v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02243](http://arxiv.org/abs/2303.02243)

    本文提出了一种将神经算子和递归神经网络相结合的新型框架，解决了深度神经网络无法准确外推和误差积累的问题，在 Korteweg-de Vries 方程中表现出更好的精度和稳定性。

    

    相比传统科学计算方法，深度神经网络在模拟复杂动力系统方面具有计算成本降低、可以直接从观测数据进行训练等优势。然而，现有的方法在长时间积分中存在无法准确外推和误差积累的问题。本文通过将神经算子与递归神经网络相结合，构建出一种新颖有效的框架，提升了比现有技术更准确的积分。这个新型的混合模型基于算子学习，同时提供递归结构来捕捉时间依赖关系。实验结果表明，这个综合框架对于 Korteweg-de Vries 方程插值和外推均稳定解决了误差积累问题。

    Deep neural networks are an attractive alternative for simulating complex dynamical systems, as in comparison to traditional scientific computing methods, they offer reduced computational costs during inference and can be trained directly from observational data. Existing methods, however, cannot extrapolate accurately and are prone to error accumulation in long-time integration. Herein, we address this issue by combining neural operators with recurrent neural networks to construct a novel and effective architecture, resulting in superior accuracy compared to the state-of-the-art. The new hybrid model is based on operator learning while offering a recurrent structure to capture temporal dependencies. The integrated framework is shown to stabilize the solution and reduce error accumulation for both interpolation and extrapolation of the Korteweg-de Vries equation.
    
[^163]: 当特权信息能够解释标签噪音的时候？

    When does Privileged Information Explain Away Label Noise?. (arXiv:2303.01806v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01806](http://arxiv.org/abs/2303.01806)

    本文研究了在什么情况下，使用特权信息可以解决标签噪声问题，发现当PI帮助神经网络轻松分辨出嘈杂和干净的数据，与提供记忆嘈杂数据的学习快捷方式时，它是最有用的，在PI过于预测目标标签时，PI方法表现会更差。在此基础上，提出了一些增强方法，用于处理标签噪音。

    

    近期研究表明，利用训练过程中可用但测试时无法获取的特权信息（PI）来处理标签噪音是一种有效的方法，然而其有效性的原因尚不清楚。本研究通过在CIFAR-N/H多个数据集和新的大规模基准ImageNet-PI上的实验发现，当特权信息能够帮助神经网络轻松分辨出干净和嘈杂的点时，并能够为记忆嘈杂的例子提供学习的快捷方式时，它是最有帮助的。有趣的是，当PI变得过于预测目标标签时，PI方法往往比无PI基线表现更差。基于这些发现，我们提出了几种改进最先进的PI方法的方式，并展示了PI作为解决标签噪音的手段的潜力。最后，我们展示了如何轻松地将结果PI方法与常规方法相结合。

    Leveraging privileged information (PI), or features available during training but not at test time, has recently been shown to be an effective method for addressing label noise. However, the reasons for its effectiveness are not well understood. In this study, we investigate the role played by different properties of the PI in explaining away label noise. Through experiments on multiple datasets with real PI (CIFAR-N/H) and a new large-scale benchmark ImageNet-PI, we find that PI is most helpful when it allows networks to easily distinguish clean from noisy data, while enabling a learning shortcut to memorize the noisy examples. Interestingly, when PI becomes too predictive of the target label, PI methods often perform worse than their no-PI baselines. Based on these findings, we propose several enhancements to the state-of-the-art PI methods and demonstrate the potential of PI as a means of tackling label noise. Finally, we show how we can easily combine the resulting PI approaches wi
    
[^164]: 为相对表示式引入并行锚点的引导方法

    Bootstrapping Parallel Anchors for Relative Representations. (arXiv:2303.00721v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00721](http://arxiv.org/abs/2303.00721)

    该论文提出了一种引导方法，通过已知的集合发现新的并行锚点，以克服相对表示式中获取并行锚点困难的问题，能够用于不同域之间的语义对应，取得竞争性结果。

    

    相对表示法在各种应用中都显示出了作为潜在嵌入法的潜力，例如启用潜在空间通信和模型拼接。然而，相对表示依赖于一定量的并行锚点作为输入，但在某些情况下难以获取这些锚点。为了克服这一限制，我们提出了一种优化方法，从有限的已知集合（种子）中发现新的并行锚点。我们的方法可以用于在不同域之间寻找语义对应，对齐它们的相对空间，并在多项任务中取得竞争性结果。

    The use of relative representations for latent embeddings has shown potential in enabling latent space communication and zero-shot model stitching across a wide range of applications. Nevertheless, relative representations rely on a certain amount of parallel anchors to be given as input, which can be impractical to obtain in certain scenarios. To overcome this limitation, we propose an optimization-based method to discover new parallel anchors from a limited known set (seed). Our approach can be used to find semantic correspondence between different domains, align their relative spaces, and achieve competitive results in several tasks.
    
[^165]: 遮盖数据有帮助：关于稀疏编码中遮盖的好处

    Hiding Data Helps: On the Benefits of Masking for Sparse Coding. (arXiv:2302.12715v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12715](http://arxiv.org/abs/2302.12715)

    本文研究了稀疏编码中学习得到的大于实际字典的情况下，噪声会导致标准的字典学习目标函数无法恢复出实际字典的问题，提出了通过遮盖数据的方法进行可靠的字典恢复适用于多种信号模态。

    

    稀疏编码被用在信号处理、计算机视觉以及医学成像等领域，在这些应用中，信号被建模为用学习到的字典中的元素的稀疏线性组合。然而，目前大多数研究关注学习到的字典与真实字典大小相同时的情况，并且仅研究了没有噪声的情景。本文中，我们提出了一种简单而有效的数据遮盖方法，能够保证在遮盖的情况下获得可靠的字典恢复结果。我们在多个数据集和信号模态下对我们的遮盖方法进行了实验验证，得到了优于现有方法的结果。

    Sparse coding, which refers to modeling a signal as sparse linear combinations of the elements of a learned dictionary, has proven to be a successful (and interpretable) approach in applications such as signal processing, computer vision, and medical imaging. While this success has spurred much work on provable guarantees for dictionary recovery when the learned dictionary is the same size as the ground-truth dictionary, work on the setting where the learned dictionary is larger (or over-realized) with respect to the ground truth is comparatively nascent. Existing theoretical results in this setting have been constrained to the case of noise-less data. We show in this work that, in the presence of noise, minimizing the standard dictionary learning objective can fail to recover the elements of the ground-truth dictionary in the over-realized regime, regardless of the magnitude of the signal in the data-generating process. Furthermore, drawing from the growing body of work on self-superv
    
[^166]: K-SHAP: 一种用于匿名状态-动作对的策略聚类算法

    K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11996](http://arxiv.org/abs/2302.11996)

    本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。

    

    从观测数据中学习智能体行为已经被证明可以提高我们对它们决策过程的理解，从而增强我们解释它们与环境和其他智能体之间交互的能力。尽管文献中已经提出了多种学习技术，但还有一种特定的情况尚未被探索，那就是智能体身份保持匿名的多智能体系统。例如，在金融市场中，标记数据通常是专有的，仅公开多个市场参与者交互而产生的匿名状态-动作对。因此，智能体行动序列不可观测，限制了现有工作的适用性。本文提出了一种策略聚类算法K-SHAP，它学习根据智能体策略对匿名状态-动作对进行分组。我们将该问题作为模仿学习(IL)任务，学习一个w...

    Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
    
[^167]: 具有预测误差保证的分布式离线策略评估算法

    Distributional Offline Policy Evaluation with Predictive Error Guarantees. (arXiv:2302.09456v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09456](http://arxiv.org/abs/2302.09456)

    本论文提出了一种名为Fitted Likelihood Estimation (FLE)的算法来解决分布式离线策略评估的问题，该算法能够学习到密切接近真实分布的策略回报分布。

    

    本研究探讨使用非策略生成的离线数据集来估算策略回报分布的问题，即分布式离线策略评估（OPE）。提出了一种名为Fitted Likelihood Estimation（FLE）的算法，它执行了一系列的最大似然估计，具有将任何最先进的概率生成模型集成的灵活性，只要它可以通过最大似然估计进行训练。FLE能够用于有限或无限时间折扣设置，其中奖励可以是多维向量。我们的理论结果表明，无论是在有限时间折扣设置还是无限时间折扣设置下，FLE都可以学习到密切接近真实分布的分布，分别在总变差距离和Wasserstein距离下。在训练MLE过程成功时，我们的理论结果适用于离线数据覆盖测试策略痕迹的条件。在实验上，我们证明了FLE在各种环境中都能取得良好的效果。

    We study the problem of estimating the distribution of the return of a policy using an offline dataset that is not generated from the policy, i.e., distributional offline policy evaluation (OPE). We propose an algorithm called Fitted Likelihood Estimation (FLE), which conducts a sequence of Maximum Likelihood Estimation (MLE) and has the flexibility of integrating any state-of-the-art probabilistic generative models as long as it can be trained via MLE. FLE can be used for both finite-horizon and infinite-horizon discounted settings where rewards can be multi-dimensional vectors. Our theoretical results show that for both finite-horizon and infinite-horizon discounted settings, FLE can learn distributions that are close to the ground truth under total variation distance and Wasserstein distance, respectively. Our theoretical results hold under the conditions that the offline data covers the test policy's traces and that the supervised learning MLE procedures succeed. Experimentally, we
    
[^168]: 机器学习安全防御中的平等度量

    Measuring Equality in Machine Learning Security Defenses. (arXiv:2302.08973v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08973](http://arxiv.org/abs/2302.08973)

    本文研究了机器学习安全防御方法的平等性能问题，提出了一种简单的平等度量和分析框架，鼓励进一步探索公平性在该领域的应用。

    

    在过去的十年中，机器学习安全社区已经发展了许多对抗攻击的防御方法。但这个社区中鲜有研究一个问题：这些防御方法为谁提供保护呢？本文考虑了一些常见的防御方法，并研究了当这些防御方法被不同的子群体使用时，它们是否会产生意想不到的平等性能问题。我们提出了一种简单的平等度量和分析框架，通过机器学习安全方法的公平性实证结果来回答这个问题。许多方法可能会直接造成伤害，我们称之为有偏漏洞和有偏排斥。我们的框架和度量方法可以应用于强化训练模型、基于预处理的方法和拒绝方法，以捕捉在安全预算上的行为。我们确定了一个实际的数据集，具有合理的计算成本，适合于测量防御的平等性。通过一个案例研究，我们展示了现代防御方法的准确性和平等性性能的衡量价值。我们希望我们提出的指标和方法能够鼓励和促进机器学习安全和防御领域的公平性探索。

    The machine learning security community has developed myriad defenses for evasion attacks over the past decade. An understudied question in that community is: for whom do these defenses defend? In this work, we consider some common approaches to defending learned systems and whether those approaches may offer unexpected performance inequities when used by different sub-populations. We outline simple parity metrics and a framework for analysis that can begin to answer this question through empirical results of the fairness implications of machine learning security methods. Many methods have been proposed that can cause direct harm, which we describe as biased vulnerability and biased rejection. Our framework and metric can be applied to robustly trained models, preprocessing-based methods, and rejection methods to capture behavior over security budgets. We identify a realistic dataset with a reasonable computational cost suitable for measuring the equality of defenses. Through a case st
    
[^169]: 针对随机递增赌博机的最佳臂识别问题

    Best Arm Identification for Stochastic Rising Bandits. (arXiv:2302.07510v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07510](http://arxiv.org/abs/2302.07510)

    本文提出了两种算法解决了固定预算下，针对随机递增赌博机最佳臂识别的问题，并且在足够大的预算下，这两个算法都能正确识别最优选项。

    

    随机递增赌博机 (SRB) 模型描述了顺序决策问题，其中可选选项的预期奖励每次选择都会增加。这个设置涵盖了许多场景，其中可用选项是学习实体，其表现（期望）随时间改善。虽然以前的工作解决了遗憾最小化问题，但本文专注于 SRB 的固定预算最佳臂识别 (BAI) 问题。在这种情况下，给定一定轮数的固定预算，我们要在识别过程结束时提供关于最佳选项的推荐。我们提出了两个算法来解决上述问题，即 R-UCBE，它采用类似于 UCB 的方法，和 R-SR，它采用逐步拒绝程序。然后，我们证明了在足够大的预算下，它们可以保证能够正确识别最优选项的概率。此外，我们对 SRB 的 BAI 问题的随机复杂度进行了分析，并们证明了 R-UCBE 的上界和 R-SR 的下界。

    Stochastic Rising Bandits (SRBs) model sequential decision-making problems in which the expected rewards of the available options increase every time they are selected. This setting captures a wide range of scenarios in which the available options are learning entities whose performance improves (in expectation) over time. While previous works addressed the regret minimization problem, this paper, focuses on the fixed-budget Best Arm Identification (BAI) problem for SRBs. In this scenario, given a fixed budget of rounds, we are asked to provide a recommendation about the best option at the end of the identification process. We propose two algorithms to tackle the above-mentioned setting, namely R-UCBE, which resorts to a UCB-like approach, and R-SR, which employs a successive reject procedure. Then, we prove that, with a sufficiently large budget, they provide guarantees on the probability of properly identifying the optimal option at the end of the learning process. Furthermore, we de
    
[^170]: 平均Hölder平滑度下的近似最优学习

    Near-optimal learning with average H\"older smoothness. (arXiv:2302.06005v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06005](http://arxiv.org/abs/2302.06005)

    通过推广平均Lipschitz平滑性到Hölder平滑性，得到了关于平均Hölder平滑性的上下风险界，最优的下界对数因子最多差一个，提供了独立的学习算法。

    

    我们将Ashlagi等人（COLT 2021）提出的平均Lipschitz平滑性概念推广到Hölder平滑性，并证明了关于平均Hölder平滑性的上下风险界，这些界的速率甚至在平均Lipschitz平滑性的特殊情况下也优于之前已知界。此外，我们的下界在可实现情况下是最优的，最多差一个对数因子，从而建立了极小值率。从算法的角度来看，由于我们对平均平滑度的定义是针对未知的基础分布的，因此学习者没有函数类的显式表示，无法执行ERM。尽管如此，我们提供了独立的学习算法。

    We generalize the notion of average Lipschitz smoothness proposed by Ashlagi et al. (COLT 2021) by extending it to H\"older smoothness. This measure of the "effective smoothness" of a function is sensitive to the underlying distribution and can be dramatically smaller than its classic "worst-case H\"older constant. We consider both the realizable and the agnostic (noisy) regression settings, proving upper and lower risk bounds in terms of the average H\"older smoothness; these rates improve upon both previously known rates even in the special case of average Lipschitz smoothness. Moreover, our lower bound is tight in the realizable setting up to log factors, thus we establish the minimax rate. From an algorithmic perspective, since our notion of average smoothness is defined with respect to the unknown underlying distribution, the learner does not have an explicit representation of the function class, hence is unable to execute ERM. Nevertheless, we provide distinct learning algorithms
    
[^171]: ConCerNet：一种基于对比学习的自动发现守恒律和可靠动力学系统预测框架

    ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction. (arXiv:2302.05783v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05783](http://arxiv.org/abs/2302.05783)

    本文提出了一种基于对比学习的框架ConCerNet，用于提高DNN动力学建模的可靠性，实现对系统不变量的自动捕捉和保留，经实验证明其性能优于传统神经网络方法。

    

    深度神经网络(DNN)在动力学系统建模方面表现出极大的能力；然而，它们通常不遵守物理约束，如守恒定律。本文提出了一种名为ConCerNet的新的学习框架，以提高基于DNN的动力学建模的可靠性，赋予不变的属性。ConCerNet由两个步骤组成:(i)对比学习方法，自动捕捉轨迹观测中的系统不变量(即守恒性质)；(ii)神经投影层，保证学习到的动力学模型保留学习到的不变量。我们从理论上证明了学习到的潜在表示和未知系统不变量函数之间的功能关系。实验表明，我们的方法在坐标误差和守恒指标方面始终比基线神经网络表现出更好的效果。使用基于神经网络的参数化且不依赖于先前知识，我们的方法在动力学方面具有广阔的应用前景。

    Deep neural networks (DNN) have shown great capacity of modeling a dynamical system; nevertheless, they usually do not obey physics constraints such as conservation laws. This paper proposes a new learning framework named ConCerNet to improve the trustworthiness of the DNN based dynamics modeling to endow the invariant properties. ConCerNet consists of two steps: (i) a contrastive learning method to automatically capture the system invariants (i.e. conservation properties) along the trajectory observations; (ii) a neural projection layer to guarantee that the learned dynamics models preserve the learned invariants. We theoretically prove the functional relationship between the learned latent representation and the unknown system invariant function. Experiments show that our method consistently outperforms the baseline neural networks in both coordinate error and conservation metrics by a large margin. With neural network based parameterization and no dependence on prior knowledge, our 
    
[^172]: 因果关系强化学习综述

    A Survey on Causal Reinforcement Learning. (arXiv:2302.05209v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.05209](http://arxiv.org/abs/2302.05209)

    本文综述了因果关系强化学习的最新进展和未来研究方向，将现有的CRL方法分为两类，并讨论了因果关系和RL之间的关系。

    

    在许多领域的顺序决策问题中，强化学习取得了巨大的成功，但仍面临数据效率和解释性的关键挑战。有趣的是，许多研究者最近已经利用因果关系文献的见解，带来了繁荣的工作，以统一因果关系的优点，并解决RL所面临的挑战。因此，总结这些因果关系强化学习（CRL）工作，提供CRL方法的综述，以及研究因果关系对RL的潜在功能，具有重要的必要性和意义。特别地，我们根据它们是否提前给出基于因果关系的信息，将现有的CRL方法分为两类。我们进一步分析每个类别在不同模型的规范化方面的关系，包括马尔可夫决策过程（MDP），部分观察到的马尔可夫决策过程（POMDP），多臂老虎机（MAB）和动态治疗效果（DTE）。通过各种角度讨论因果关系和RL之间的关系，包括探索-开发困境，反事实评估和基于模型的强化学习。总之，这篇综述论文全面概述了因果关系强化学习的最新进展，并指出了未来研究方向的潜在机会。

    While Reinforcement Learning (RL) achieves tremendous success in sequential decision-making problems of many domains, it still faces key challenges of data inefficiency and the lack of interpretability. Interestingly, many researchers have leveraged insights from the causality literature recently, bringing forth flourishing works to unify the merits of causality and address well the challenges from RL. As such, it is of great necessity and significance to collate these Causal Reinforcement Learning (CRL) works, offer a review of CRL methods, and investigate the potential functionality from causality toward RL. In particular, we divide existing CRL approaches into two categories according to whether their causality-based information is given in advance or not. We further analyze each category in terms of the formalization of different models, ranging from the Markov Decision Process (MDP), Partially Observed Markov Decision Process (POMDP), Multi-Arm Bandits (MAB), and Dynamic Treatment
    
[^173]: 更好的扩散模型进一步改进对抗训练

    Better Diffusion Models Further Improve Adversarial Training. (arXiv:2302.04638v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.04638](http://arxiv.org/abs/2302.04638)

    本文证明了更好的扩散模型可以进一步提高对抗训练的性能，通过采用最新的扩散模型，我们训练的模型仅使用生成的数据就在RobustBench上实现了最佳性能，并在CIFAR-10和CIFAR-100数据集上分别提高了$+4.58\%$和$+8.03\%$的性能。

    

    众所周知，由去噪扩散概率模型（DDPM）产生的数据可以提高对抗性训练。随着扩散模型的迅速发展，一个自然的问题出现了：更好的扩散模型是否能够进一步改进对抗训练？本文采用了最新的扩散模型，与DDPM相比，该模型具有更高的效率（约$\sim20$个采样步骤）和更低的图像质量（更低的FID分数），证明了更好的扩散模型可以进一步提高对抗训练。我们训练的模型仅使用生成的数据（没有外部数据集）在RobustBench上实现了最佳性能。在$\ell_\infty$-norm威胁模型下，当$\epsilon=8/255$时，我们的模型在CIFAR-10和CIFAR-100上分别达到$70.69\%$和$42.67\%$的鲁棒准确度，即分别比以前的最先进模型提高了$+4.58\%$和$+8.03\%$。在$\ell_2$-norm威胁模型下，当$\epsilon=128/255$时，我们的模型在CIFAR-10上可以达到$84.86\%$的准确率，提高了$+4.44\%$。

    It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency ($\sim 20$ sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the $\ell_\infty$-norm threat model with $\epsilon=8/255$, our models achieve $70.69\%$ and $42.67\%$ robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by $+4.58\%$ and $+8.03\%$. Under the $\ell_2$-norm threat model with $\epsilon=128/255$, our models achieve $84.86\%$ on CIFAR-10 ($+4.44\%$). These results also beat
    
[^174]: 语言模型训练数据提取的技巧总结

    Bag of Tricks for Training Data Extraction from Language Models. (arXiv:2302.04460v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.04460](http://arxiv.org/abs/2302.04460)

    本文总结了一些技巧用于改进语言模型训练数据提取，提出了在文本生成和文本排名中可以使用的技巧，实验证明这些技巧对于提高训练数据提取的效果非常重要。

    

    随着语言模型的不断研究，隐私保护变得越来越重要。因此，训练数据提取作为潜在的评估隐私泄露的工具变得非常关键。然而，由于这项任务的困难程度，目前现有的大多数方法仍然不够有效。本文提出了一些技巧用于改进训练数据提取，我们对公开可用的数据集进行了实验。由于大多数现有的提取方法使用生成然后排序的流程（例如，生成潜在的训练数据文本，然后根据特定的标准对它们进行排序），因此我们的研究重点在于文本生成和文本排名的技巧。（例如，采样策略和令牌级标准）。实验结果表明，一些之前被忽视的技巧对于训练数据提取的成功非常关键。基于GPT-Neo 1.3B的评估结果，我们提出的技巧优于现有的方法。

    With the advance of language models, privacy protection is receiving more attention. Training data extraction is therefore of great importance, as it can serve as a potential tool to assess privacy leakage. However, due to the difficulty of this task, most of the existing methods are proof-of-concept and still not effective enough. In this paper, we investigate and benchmark tricks for improving training data extraction using a publicly available dataset. Because most existing extraction methods use a pipeline of generating-then-ranking, i.e., generating text candidates as potential training data and then ranking them based on specific criteria, our research focuses on the tricks for both text generation (e.g., sampling strategy) and text ranking (e.g., token-level criteria). The experimental results show that several previously overlooked tricks can be crucial to the success of training data extraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks outperform the b
    
[^175]: 硬提示变简单：用基于梯度的离散优化方法进行提示调节和发现

    Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery. (arXiv:2302.03668v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03668](http://arxiv.org/abs/2302.03668)

    该论文提出了一种基于梯度的离散优化方法，用于自动生成文本提示，进而控制现代生成模型的输出。该方法可以优化文本到图像和文本到文本的应用，为API用户提供了轻松生成、发现、混合和匹配图像概念的能力，同时自动发现硬提示可以有效地提高模型性能。

    

    现代生成模型的优点在于可以通过基于文本的提示进行控制。传统的“硬”提示是由可解释的词汇和标记构成，必须由人手工制作。此外还有“软”提示，它们由连续的特征向量组成，可以通过强大的优化方法发现，但它们不能很容易地解释，不能在不同模型之间重复使用，也不能用于基于文本的接口。我们提出了一种基于高效梯度优化的方法，来稳健地优化硬文本提示。该方法自动为文本到图像和文本到文本应用生成硬文本提示。在文本到图像的设置中，该方法为扩散模型创建硬提示，使API用户可以轻松生成、发现、混合和匹配图像概念，而不需要事先了解如何提示模型。在文本到文本的设置中，我们展示了自动发现硬提示可以有效地优化模型性能。

    The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical "hard" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also "soft" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface.  We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effectiv
    
[^176]: SSL的相互作用：增强、归纳偏差和泛化

    The SSL Interplay: Augmentations, Inductive Bias, and Generalization. (arXiv:2302.02774v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02774](http://arxiv.org/abs/2302.02774)

    本论文通过分析数据增强、网络架构和训练算法的相互作用，研究了预训练和下游任务的泛化性能，并为SSL从业人员提供了一些见解。

    

    自监督学习（SSL）已成为一种无需监督即可从原始数据中学习表示的强大框架。然而，在实践中，工程师面临着调整优化器和训练过程中表示塌陷等问题。这些挑战促使我们需要一种理论来阐明数据增强、网络架构和训练算法的选择之间的复杂相互作用。我们在一个理论友好的设置下，通过对预训练和下游任务的泛化性能进行精确分析，研究了这种相互作用，并强调了我们理论得出的SSL从业者的一些见解。

    Self-supervised learning (SSL) has emerged as a powerful framework to learn representations from raw data without supervision. Yet in practice, engineers face issues such as instability in tuning optimizers and collapse of representations during training. Such challenges motivate the need for a theory to shed light on the complex interplay between the choice of data augmentation, network architecture, and training algorithm. We study such an interplay with a precise analysis of generalization performance on both pretraining and downstream tasks in a theory friendly setup, and highlight several insights for SSL practitioners that arise from our theory.
    
[^177]: 低位视觉变压器的无振荡量化

    Oscillation-free Quantization for Low-bit Vision Transformers. (arXiv:2302.02210v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.02210](http://arxiv.org/abs/2302.02210)

    研究发现，可学习比例因子加剧了权重振荡，本文提出三种技术以解决这个问题，并在多个基准测试上显著提高了模型的性能。

    

    量化意识训练的一个不良副作用是权重振荡，其中量化权重经常在两个量化级别之间跳动，导致训练不稳定和子优化的最终模型。本研究发现，可学习的比例因子——在量化中广泛使用的$\textit{de facto}$设置——加剧了权重振荡。本研究研究了可学习比例因子与量化权重振荡之间的关系，并以ViT为案例来说明发现和解决方法。此外，我们还发现自注意力层中量化权重的$\textit{query}$和$\textit{key}$之间的相互依存使ViT容易受到振荡的影响。因此，我们相应地提出了三种技术：统计权重量化（$\rm StatsQ$）以改善量化鲁棒性，与普遍使用的可学习比例因子方法相比；置信度引导的退火（$\rm CGA$）在训练期间冻结具有$\textit{高置信度}$的权重，以减少权重振荡；以及相互依赖权重的均衡（$\rm IWEqual$），以有效处理相互依赖问题。我们的实验表明，我们提出的方法相比于最先进的方法在多个基准测试上显著提高了模型的性能。

    Weight oscillation is an undesirable side effect of quantization-aware training, in which quantized weights frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. We discover that the learnable scaling factor, a widely-used $\textit{de facto}$ setting in quantization aggravates weight oscillation. In this study, we investigate the connection between the learnable scaling factor and quantized weight oscillation and use ViT as a case driver to illustrate the findings and remedies. In addition, we also found that the interdependence between quantized weights in $\textit{query}$ and $\textit{key}$ of a self-attention layer makes ViT vulnerable to oscillation. We, therefore, propose three techniques accordingly: statistical weight quantization ($\rm StatsQ$) to improve quantization robustness compared to the prevalent learnable-scale-based method; confidence-guided annealing ($\rm CGA$) that freezes the weights with $\textit{high confi
    
[^178]: 通过惰性传播实现大规模图神经网络的 LazyGNN

    LazyGNN: Large-Scale Graph Neural Networks via Lazy Propagation. (arXiv:2302.01503v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01503](http://arxiv.org/abs/2302.01503)

    本文提出了一种基于浅层模型和惰性传播的图神经网络模型 LazyGNN，用于大规模图的表示学习，具有很高的效率和可扩展性。

    

    近期的研究表明，通过更深的图神经网络（GNN）捕捉图中的远程依赖性带来了好处。但是，在大规模图中，由于邻域爆炸问题，更深的GNN会受到长期的扩展挑战。在本文中，我们提出通过更浅的模型来捕获图中的远程依赖性，从而得到一种更高效的模型——LazyGNN，用于图表示学习。此外，我们证明，LazyGNN 与现有的可扩展方法（如采样方法）兼容，可以通过开发混合批量的LazyGNN来进一步加速。全面的实验显示了它在大规模基准测试上优异的预测性能和可扩展性。LazyGNN 的实现可在 https://github.com/RXPHD/Lazy_GNN 进行查看。

    Recent works have demonstrated the benefits of capturing long-distance dependency in graphs by deeper graph neural networks (GNNs). But deeper GNNs suffer from the long-lasting scalability challenge due to the neighborhood explosion problem in large-scale graphs. In this work, we propose to capture long-distance dependency in graphs by shallower models instead of deeper models, which leads to a much more efficient model, LazyGNN, for graph representation learning. Moreover, we demonstrate that LazyGNN is compatible with existing scalable approaches (such as sampling methods) for further accelerations through the development of mini-batch LazyGNN. Comprehensive experiments demonstrate its superior prediction performance and scalability on large-scale benchmarks. The implementation of LazyGNN is available at https://github.com/RXPHD/Lazy_GNN.
    
[^179]: Fed-GLOSS-DP: 利用具有记录级差分隐私的合成集进行联邦全局学习

    Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with Record Level Differential Privacy. (arXiv:2302.01068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01068](http://arxiv.org/abs/2302.01068)

    本文通过利用合成样本实现全局优化，加入记录级差分隐私以保护隐私，验证了该方法的数据集有效性。

    

    本文提出了Fed-GLOSS-DP，一种新颖的保护隐私的联邦学习方法。与以前的线性逐点梯度分享方案（如FedAvg）不同，我们的公式利用从客户端接收到的合成样本实现了一种全局优化。这些合成样本作为损失替代物，通过模拟本地区域内真实图像的实用性来近似本地损失地形。我们还引入了一种衡量有效逼近区域的方法，反映了近似的质量。因此，服务器可以恢复全局损失地形并全面优化模型。此外，受日益严重的隐私问题的启发，我们演示了我们的方法与记录级差分隐私（DP）无缝配合，为客户端上的每个数据记录提供理论上的隐私保证。广泛的结果验证了我们的公式在具有高度倾斜分布的各种数据集上的有效性。

    This work proposes Fed-GLOSS-DP, a novel privacy-preserving approach for federated learning. Unlike previous linear point-wise gradient-sharing schemes, such as FedAvg, our formulation enables a type of global optimization by leveraging synthetic samples received from clients. These synthetic samples, serving as loss surrogates, approximate local loss landscapes by simulating the utility of real images within a local region. We additionally introduce an approach to measure effective approximation regions reflecting the quality of the approximation. Therefore, the server can recover the global loss landscape and comprehensively optimize the model. Moreover, motivated by the emerging privacy concerns, we demonstrate that our approach seamlessly works with record-level differential privacy (DP), granting theoretical privacy guarantees for every data record on the clients. Extensive results validate the efficacy of our formulation on various datasets with highly skewed distributions. Our m
    
[^180]: 将语言模型与图像进行联系以处理多模态信息

    Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13823](http://arxiv.org/abs/2301.13823)

    该论文提出一种有效的方法，将仅处理文本的语言模型与图像进行联系，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的自由形式文本。该方法在环境相关的图像检索和多模态对话等任务中表现十分优异，是利用预训练语言模型解决视觉场景下交互问题的有效解决方案。

    

    我们提出了一种有效的方法，将预训练的仅文本语言模型与视觉领域联系起来，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的文本。我们利用从大规模文本预训练中学到的语言模型的能力，例如上下文学习和自由形式文本生成。我们保持语言模型冻结，并微调输入和输出线性层以实现跨模态交互。这使得我们的模型能够处理任意交错的图像和文本输入，并生成与检索图像交错的自由形式文本。我们在环境相关的图像检索和多模态对话等任务中取得了强大的零-shot表现，并展示了引人入胜的交互能力。我们的方法适用于任何现成的语言模型，为在视觉场景下利用预训练语言模型提供了一个有效且通用的解决方案。

    We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
    
[^181]: 改进的多时期多分类装载问题算法，同时带有bandit反馈

    Improved Algorithms for Multi-period Multi-class Packing Problems with Bandit Feedback. (arXiv:2301.13791v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13791](http://arxiv.org/abs/2301.13791)

    本文提出了一个改进的算法来解决多时期多分类装载问题，其中使用bandit反馈，提出的算法具有更快的收敛速率和更低的遗憾，同时解决了一个相关问题。

    

    本文考虑了线性背景下的多类多期装载问题（LMMP），它的目标是将物品装载到预算向量下，并使总价值尽可能大。我们考虑的情况是，与每个操作相关联的奖励和消耗向量是上下文相关的线性函数，并且决策者得到了bandit反馈。当预算至少增长为$ \sqrt{T}$时，我们提出了一种闭合形式的bandit策略，这样可以在上下文维度，分类数和时间范围$T$下保持次线性的遗憾。我们还解决了Agrawal＆Goyal（2018）在LCBK问题中提出的一个开放问题，并提出了一个可以获得更快收敛速率的新的估计器。

    We consider the linear contextual multi-class multi-period packing problem (LMMP) where the goal is to pack items such that the total vector of consumption is below a given budget vector and the total value is as large as possible. We consider the setting where the reward and the consumption vector associated with each action is a class-dependent linear function of the context, and the decision-maker receives bandit feedback. LMMP includes linear contextual bandits with knapsacks and online revenue management as special cases. We establish a new estimator which guarantees a faster convergence rate, and consequently, a lower regret in such problems. We propose a bandit policy that is a closed-form function of said estimated parameters. When the contexts are non-degenerate, the regret of the proposed policy is sublinear in the context dimension, the number of classes, and the time horizon $T$ when the budget grows at least as $\sqrt{T}$. We also resolve an open problem posed by Agrawal &
    
[^182]: 双价值网络在逆向合成规划中的应用

    Retrosynthetic Planning with Dual Value Networks. (arXiv:2301.13755v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.13755](http://arxiv.org/abs/2301.13755)

    PDVN是一种新的在线训练算法，它在逆向合成规划中利用双价值网络优化完整的路线，成功率和效率上均优于现有方法。

    

    逆向合成旨在从商业上可得的起始材料中找到合成目标分子的路线，是药物发现和材料设计中的关键任务。最近，基于机器学习的单步反应预测器与多步规划器的组合已经取得了令人鼓舞的结果。然而，单步预测器大多数情况下是离线训练的，只优化单步的准确性，而不考虑完整的路线。本文提出了一种新的在线训练算法PDVN，通过使用树形MDP来优化完整的路线，利用强化学习（RL）改善单步预测器。在PDVN中，我们构建了两个单独的价值网络，分别预测分子的可合成性和成本。为了保持单步预测器的准确性，我们设计了一个双分支网络结构。

    Retrosynthesis, which aims to find a route to synthesize a target molecule from commercially available starting materials, is a critical task in drug discovery and materials design. Recently, the combination of ML-based single-step reaction predictors with multi-step planners has led to promising results. However, the single-step predictors are mostly trained offline to optimize the single-step accuracy, without considering complete routes. Here, we leverage reinforcement learning (RL) to improve the single-step predictor, by using a tree-shaped MDP to optimize complete routes. Specifically, we propose a novel online training algorithm, called Planning with Dual Value Networks (PDVN), which alternates between the planning phase and updating phase. In PDVN, we construct two separate value networks to predict the synthesizability and cost of molecules, respectively. To maintain the single-step accuracy, we design a two-branch network structure for the single-step predictor. On the widely
    
[^183]: 新的分布水平度量方法：弃用$\Delta$DP，实现人口统计特征的公平机器学习

    Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity. (arXiv:2301.13443v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13443](http://arxiv.org/abs/2301.13443)

    本文提出两种新公平度量方法：概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），实现人口统计特征的公平机器学习

    

    人口统计特征的平等对待是机器学习中最广泛认可的公平度量标准。为实现人口统计平等，许多研究致力于追求常用度量方法$\Delta DP$。然而，本文揭示了公平度量方法$\Delta DP$存在固有缺陷：i) 零值$\Delta DP$不保证民族统计平等的零违规，ii) $\Delta DP$值随不同分类阈值变化。为此，提出了两种新公平度量方法——概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），以精确测量不同民族统计群体预测概率分布之间的差异。

    Demographic parity is the most widely recognized measure of group fairness in machine learning, which ensures equal treatment of different demographic groups. Numerous works aim to achieve demographic parity by pursuing the commonly used metric $\Delta DP$. Unfortunately, in this paper, we reveal that the fairness metric $\Delta DP$ can not precisely measure the violation of demographic parity, because it inherently has the following drawbacks: i) zero-value $\Delta DP$ does not guarantee zero violation of demographic parity, ii) $\Delta DP$ values can vary with different classification thresholds. To this end, we propose two new fairness metrics, Area Between Probability density function Curves (ABPC) and Area Between Cumulative density function Curves (ABCC), to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic group
    
[^184]: 深层权重空间中学习的等变架构

    Equivariant Architectures for Learning in Deep Weight Spaces. (arXiv:2301.12780v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12780](http://arxiv.org/abs/2301.12780)

    本论文提出了一种新的网络架构，用于在深层权重空间中学习，它对MLP权重的自然置换对称等变，可以处理广泛有趣的任务。

    

    设计用于以原始权重矩阵形式处理神经网络的机器学习架构是一条新引入的研究方向。不幸的是，深层权重空间的独特对称结构使得这种设计非常具有挑战性。如果成功，这样的架构将能够执行广泛的有趣任务，从将预训练的网络适应到新的领域，到编辑作为函数表示的对象（INRs或NeRFs）。作为实现这一目标的第一步，我们在这里提出了一种新的深层权重空间中学习的网络架构。它以预训练MLP的权重和偏置的串联作为输入，并使用一组对MLP权重的自然置换对称等变的层组成来处理它：改变MLP中间层中神经元的顺序不会影响它所表示的函数。我们为这些对称结构提供了所有仿射等变和不变层的完整特征，并展示...

    Designing machine learning architectures for processing neural networks in their raw weight matrix form is a newly introduced research direction. Unfortunately, the unique symmetry structure of deep weight spaces makes this design very challenging. If successful, such architectures would be capable of performing a wide range of intriguing tasks, from adapting a pre-trained network to a new domain to editing objects represented as functions (INRs or NeRFs). As a first step towards this goal, we present here a novel network architecture for learning in deep weight spaces. It takes as input a concatenation of weights and biases of a pre-trained MLP and processes it using a composition of layers that are equivariant to the natural permutation symmetry of the MLP's weights: Changing the order of neurons in intermediate layers of the MLP does not affect the function it represents. We provide a full characterization of all affine equivariant and invariant layers for these symmetries and show 
    
[^185]: 再次深入探究Few-shot分类问题

    A Closer Look at Few-shot Classification Again. (arXiv:2301.12246v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12246](http://arxiv.org/abs/2301.12246)

    本文研究了Few-shot分类问题中的训练算法和适应算法，并实证证明这两个算法是可以完全分离的。此外，本文的元分析揭示出了关于Few-shot分类的关键方面和与其他领域（如视觉表示学习和迁移学习）的联系的有趣见解。

    

    Few-shot分类算法由训练阶段和适应阶段组成，在训练阶段，模型在一个相对大的数据集上进行学习，在适应阶段，已学习的模型被调整以适应之前从未见过的仅有有限标注样本的任务。本文通过实证证明，训练算法和适应算法是完全独立的，这使得可以分别为每个阶段进行算法分析和设计。针对每个阶段的元分析揭示了一些有趣的见解，有助于更好地理解Few-shot分类的关键方面和与其他领域（如视觉表示学习和迁移学习）的联系。我们希望本文揭示的见解和研究挑战能激发相关方向的未来工作。论文代码和预训练模型（使用PyTorch）可在 https://github.com/Frankluox/CloserLookAgainFewShot 找到。

    Few-shot classification consists of a training phase where a model is learned on a relatively large dataset and an adaptation phase where the learned model is adapted to previously-unseen tasks with limited labeled samples. In this paper, we empirically prove that the training algorithm and the adaptation algorithm can be completely disentangled, which allows algorithm analysis and design to be done individually for each phase. Our meta-analysis for each phase reveals several interesting insights that may help better understand key aspects of few-shot classification and connections with other fields such as visual representation learning and transfer learning. We hope the insights and research challenges revealed in this paper can inspire future work in related directions. Code and pre-trained models (in PyTorch) are available at https://github.com/Frankluox/CloserLookAgainFewShot.
    
[^186]: 基于硬币采样的无需学习速率的基于梯度的贝叶斯推断方法

    Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates. (arXiv:2301.11294v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11294](http://arxiv.org/abs/2301.11294)

    本文提出了一种基于硬币投注的贝叶斯推断方法，完全不需要学习速率，可以在高维模型和数据集上表现出与其他方法相当的性能。

    

    近年来，基于粒子的变分推断（ParVI）方法如Stein变分梯度下降（SVGD）因可扩展性在贝叶斯推理中越来越受欢迎。然而，这些方法的性质不可避免地取决于超参数（如学习速率），必须由从业者仔细调整，以确保以合适的速率收敛到目标测度。在本文中，我们引入了一组新的基于硬币投注的可扩展贝叶斯推断方法，这些方法完全不需要学习速率。我们在一系列数值例子中演示了我们方法的性能，包括几个高维模型和数据集，证明了与其他ParVI算法相当的性能，而无需调整学习速率。

    In recent years, particle-based variational inference (ParVI) methods such as Stein variational gradient descent (SVGD) have grown in popularity as scalable methods for Bayesian inference. Unfortunately, the properties of such methods invariably depend on hyperparameters such as the learning rate, which must be carefully tuned by the practitioner in order to ensure convergence to the target measure at a suitable rate. In this paper, we introduce a suite of new particle-based methods for scalable Bayesian inference based on coin betting, which are entirely learning-rate free. We illustrate the performance of our approach on a range of numerical examples, including several high-dimensional models and datasets, demonstrating comparable performance to other ParVI algorithms with no need to tune a learning rate.
    
[^187]: 使用来自成对或$K$元比较的人类反馈的规范强化学习

    Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11270](http://arxiv.org/abs/2301.11270)

    该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。

    

    我们为带有人类反馈的强化学习问题提供了一个理论框架。我们的分析表明，当真实奖励函数为线性函数时，最大似然估计（MLE）在Bradley-Terry-Luce（BTL）模型和Plackett-Luce（PL）模型下均收敛。然而，我们发现当基于学得的奖励模型训练策略时，MLE会失败，而基于悲观估计的MLE在一定的覆盖假设下提供性能更好的策略。此外，我们证明在PL模型下，真实MLE和将$k$元比较拆分为成对比较的备选MLE都收敛。而真实MLE是渐近更为高效的。我们的结果验证了现有RLHF算法（如InstructGPT）的实验成功，并为算法设计提供了新的见解。此外，我们的结果统一了RLHF问题和最大熵反向强化学习(IRL)问题，并为其提供了第一个样本复杂度界。

    We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
    
[^188]: 图神经切比雪夫核：大规模图上的收敛分析

    Graph Neural Tangent Kernel: Convergence on Large Graphs. (arXiv:2301.10808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10808](http://arxiv.org/abs/2301.10808)

    本文利用GNTK和图核函数探究大规模图神经网络的训练动态，证明了GNTK在收敛于图核函数时的确切性质。这意味着在大规模图的情况下，可以在中等大小的图上进行拟合并在整个图上使用。

    

    图神经网络在图机器学习任务中表现出色，但在大规模图数据上训练时往往比较困难，因为它们的学习动态难以理解。本文利用图神经切比雪夫核（GNTK）和图核函数探究大规模图神经网络的训练动态。在宽度趋于无穷大时，过参数化神经网络的优化等价于在NTK上进行核回归。我们研究了GNTK在另一个独立的维度（图大小）变化时的演化情况，并使用图核函数来定义极限对象——GNN的图核函数和GNTK的图核函数，并证明，在一系列图上，GNTK收敛于图核函数。进一步证明了GNTK的谱依赖于最快学习的方向，这在早期停止训练时变得特别重要，谱也收敛于图核函数的谱。这意味着在大规模图的限制下，对中等大小的图进行拟合的GNTK可以用于在图上进行拟合。

    Graph neural networks (GNNs) achieve remarkable performance in graph machine learning tasks but can be hard to train on large-graph data, where their learning dynamics are not well understood. We investigate the training dynamics of large-graph GNNs using graph neural tangent kernels (GNTKs) and graphons. In the limit of large width, optimization of an overparametrized NN is equivalent to kernel regression on the NTK. Here, we investigate how the GNTK evolves as another independent dimension is varied: the graph size. We use graphons to define limit objects -- graphon NNs for GNNs, and graphon NTKs for GNTKs -- , and prove that, on a sequence of graphs, the GNTKs converge to the graphon NTK. We further prove that the spectrum of the GNTK, which is related to the directions of fastest learning which becomes relevant during early stopping, converges to the spectrum of the graphon NTK. This implies that in the large-graph limit, the GNTK fitted on a graph of moderate size can be used to s
    
[^189]: 面向数学生的反向传播算法

    The Backpropagation algorithm for a math student. (arXiv:2301.09977v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09977](http://arxiv.org/abs/2301.09977)

    本文介绍了针对深度神经网络的反向传播算法。通过使用Jacobian算子将损失函数的梯度表示为矩阵乘积，可以高效地计算梯度，而且该方法适用于不同数量的层数。

    

    深度神经网络是向量值函数的复合函数，为了训练深度神经网络，需要计算相对于所有参数的损失函数梯度。这个计算是一个非常棘手的任务，因为深度神经网络的损失函数是由许多非线性函数组成的，并且每个函数都有许多参数。反向传播算法利用了深度神经网络的组合结构来高效地计算梯度。因此，网络中层数的数量不会显著影响计算的复杂性。本文的目的是使用Jacobian算子将损失函数的梯度表示为矩阵乘积，并通过考虑每层对其参数的全导数并将其表示为Jacobian矩阵来实现这一目标。可以将梯度表示为这些Jacobian矩阵的矩阵乘积。这种方法是有效的，因为计算链规则所需的乘积规则等于Jacobian矩阵的乘积。

    A Deep Neural Network (DNN) is a composite function of vector-valued functions, and in order to train a DNN, it is necessary to calculate the gradient of the loss function with respect to all parameters. This calculation can be a non-trivial task because the loss function of a DNN is a composition of several nonlinear functions, each with numerous parameters. The Backpropagation (BP) algorithm leverages the composite structure of the DNN to efficiently compute the gradient. As a result, the number of layers in the network does not significantly impact the complexity of the calculation. The objective of this paper is to express the gradient of the loss function in terms of a matrix multiplication using the Jacobian operator. This can be achieved by considering the total derivative of each layer with respect to its parameters and expressing it as a Jacobian matrix. The gradient can then be represented as the matrix product of these Jacobian matrices. This approach is valid because the ch
    
[^190]: 基于深度学习的自然灾害分割模型的通用性

    Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation. (arXiv:2301.09318v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.09318](http://arxiv.org/abs/2301.09318)

    该论文提出的基于深度学习的自然灾害分割模型在透明云，烟雾柱和洪水分割任务中实现了最先进的性能，并通过在适当的预训练任务上进行预训练，显着提高了模型的通用性。

    

    气候变化导致极端天气事件的概率增加，这对全球范围内的社会和企业构成风险。因此，近实时地图制作已成为支持自然灾害救援、风险管理和政府政策决策的新优先事项。最近，实现近实时制图的方法越来越多地利用深度学习（DL）。但是，基于DL的方法仅针对单个地理区域中特定频段的卫星数据的一个特定任务。因此，用于制图特定自然灾害的DL模型难以将其推广到未见过的其他类型的自然灾害上。 在这项工作中，我们提出了一种方法，通过在适当的预训练任务上进行预训练，显着提高DL自然灾害映射器的通用性。在没有任何目标领域的数据的情况下，我们展示了该改进的通用性跨越了四个透明云，烟雾柱和洪水分段任务的多个地理区域。通过合并预训练步骤，我们的模型在所有三个自然灾害分割任务中实现了最先进的性能，同时与其他DL体系结构相比，保持更快的计算速度。我们的方法代表着迈向能够在多个领域通用的近实时自然灾害制图基本模型的有希望的一步。

    Climate change results in an increased probability of extreme weather events that put societies and businesses at risk on a global scale. Therefore, near real-time mapping of natural hazards is an emerging priority for the support of natural disaster relief, risk management, and informing governmental policy decisions. Recent methods to achieve near real-time mapping increasingly leverage deep learning (DL). However, DL-based approaches are designed for one specific task in a single geographic region based on specific frequency bands of satellite data. Therefore, DL models used to map specific natural hazards struggle with their generalization to other types of natural hazards in unseen regions. In this work, we propose a methodology to significantly improve the generalizability of DL natural hazards mappers based on pre-training on a suitable pre-task. Without access to any data from the target domain, we demonstrate this improved generalizability across four U-Net architectures for t
    
[^191]: 使用快速傅里叶变换高效地学习不相关表示

    Learning Decorrelated Representations Efficiently Using Fast Fourier Transform. (arXiv:2301.01569v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01569](http://arxiv.org/abs/2301.01569)

    本文提出了一种使用快速傅里叶变换计算去相关正则化器的方法，相较于传统方法训练更快且内存需求更小，在下游任务中显示出同等的准确性。

    

    Barlow Twins和VICReg是自监督表示学习模型，它们使用正则化器来去除特征之间的相关性。虽然这些模型与传统的表示学习模型一样有效，但如果投影嵌入的维度d很高，则它们的训练可能需要大量计算资源。由于正则化器是基于交叉-correlation或covariance矩阵的单个元素来定义的，计算n个样本的损失需要O(n d^2)的时间。在本文中，我们提出了一种放松的去相关正则化器，可以通过快速傅里叶变换在O(n d log d)的时间内计算。我们还提出了一种廉价的技术来缓解放松时出现的不良局部最小值。所提出的正则化器在下游任务中展现出与现有正则化器相当的准确性，而且对于大的d，其训练所需的内存更少，速度更快。源代码可用。

    Barlow Twins and VICReg are self-supervised representation learning models that use regularizers to decorrelate features. Although these models are as effective as conventional representation learning models, their training can be computationally demanding if the dimension d of the projected embeddings is high. As the regularizers are defined in terms of individual elements of a cross-correlation or covariance matrix, computing the loss for n samples takes O(n d^2) time. In this paper, we propose a relaxed decorrelating regularizer that can be computed in O(n d log d) time by Fast Fourier Transform. We also propose an inexpensive technique to mitigate undesirable local minima that develop with the relaxation. The proposed regularizer exhibits accuracy comparable to that of existing regularizers in downstream tasks, whereas their training requires less memory and is faster for large d. The source code is available.
    
[^192]: 可转移的储能竞标者

    Transferable Energy Storage Bidder. (arXiv:2301.01233v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01233](http://arxiv.org/abs/2301.01233)

    本文提出了一种可转移、多功能的储能竞标者模型，在整个电力市场中表现出了最先进的结果，并通过预训练和应用于另一个地区的套利得到了证明。

    

    储能资源参与整个电力市场时，必须考虑价格波动和物理运行特性。由于电力价格高度波动，储能效率损失，功率和能量限制，这是一个具有挑战性的问题。本文提出了一种新颖、多功能和可转移的方法，将基于模型的优化与卷积长短期记忆网络结合起来，用于储能响应或投标整个电力市场。我们使用纽约州的历史价格进行测试，并表明我们的方法在响应价格和整个市场竞标设置中均取得了最先进的结果，在各种储能持续时间下，与完美预见情况相比，实现了70%至近90%的利润比。我们还通过预先使用纽约数据预训练出价模型，并将其应用于澳大利亚昆士兰州的套利中来测试转移学习方法。结果表明，我们的模型在新的环境中实现了强大的性能，可将出价模型从一个位置转移到另一个位置，同时保持可比的利润。

    Energy storage resources must consider both price uncertainties and their physical operating characteristics when participating in wholesale electricity markets. This is a challenging problem as electricity prices are highly volatile, and energy storage has efficiency losses, power, and energy constraints. This paper presents a novel, versatile, and transferable approach combining model-based optimization with a convolutional long short-term memory network for energy storage to respond to or bid into wholesale electricity markets. We test our proposed approach using historical prices from New York State, showing it achieves state-of-the-art results, achieving between 70% to near 90% profit ratio compared to perfect foresight cases, in both price response and wholesale market bidding setting with various energy storage durations. We also test a transfer learning approach by pre-training the bidding model using New York data and applying it to arbitrage in Queensland, Australia. The resu
    
[^193]: 基于CLIP的通用模型用于器官分割和肿瘤检测

    CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection. (arXiv:2301.00785v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.00785](http://arxiv.org/abs/2301.00785)

    本文提出了基于CLIP的通用模型，通过文本嵌入学习解剖学关系，能够分割25种器官和6种肿瘤，具有强大的泛化能力。

    

    越来越多的公开数据集在自动化器官分割和肿瘤检测方面产生了显著的影响。然而，由于每个数据集的规模较小且部分标注问题，以及对不同类型肿瘤的有限探究，导致得到的模型通常限于分割特定的器官/肿瘤，并忽略解剖结构的语义，也无法推广到新领域。为了解决这些问题，我们提出了基于CLIP驱动的通用模型，将从对比语言-图像预训练 （CLIP）中学习到的文本嵌入结合到分割模型中。这种基于CLIP的标签编码捕捉了解剖学关系，使模型学习到结构化特征嵌入，并分割25个器官和6种类型的肿瘤。该模型由14个数据集组成，使用3410个CT扫描进行训练，然后在来自3个额外数据集的6162个外部CT扫描上进行评估。我们在医学影像分析的国际准确性基准测试（MIoU）中排名第一。

    An increasing number of public datasets have shown a marked impact on automated organ segmentation and tumor detection. However, due to the small size and partially labeled problem of each dataset, as well as a limited investigation of diverse types of tumors, the resulting models are often limited to segmenting specific organs/tumors and ignore the semantics of anatomical structures, nor can they be extended to novel domains. To address these issues, we propose the CLIP-Driven Universal Model, which incorporates text embedding learned from Contrastive Language-Image Pre-training (CLIP) to segmentation models. This CLIP-based label encoding captures anatomical relationships, enabling the model to learn a structured feature embedding and segment 25 organs and 6 types of tumors. The proposed model is developed from an assembly of 14 datasets, using a total of 3,410 CT scans for training and then evaluated on 6,162 external CT scans from 3 additional datasets. We rank first on the Medical
    
[^194]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^195]: 小型语言模型的推理能力训练

    Teaching Small Language Models to Reason. (arXiv:2212.08410v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08410](http://arxiv.org/abs/2212.08410)

    本文研究了如何通过知识蒸馏，将大型语言模型的推理能力传递到小型语言模型中，并证明这样的方法可以提高小型模型在算术、常识和符号推理方面的性能。

    

    通过思维链的启发，成功地提高了大型语言模型的推理能力，在一系列数据集上实现了最先进的结果。然而，这些推理能力似乎仅在拥有超过1000亿个参数的模型中出现。本文探讨了通过知识蒸馏将这种推理能力传递到小于1000亿个参数的模型中的方法。具体来说，我们通过对较大的教师模型生成的思维链输出进行微调，对学生模型进行了训练。我们的实验表明，所提出的方法可以在算术、常识和符号推理数据集上提高任务性能。例如，T5 XXL在GSM8K数据集上的准确率从8.11%提高到21.99%，当它被PaLM-540B生成的思维链进行微调时。

    Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on PaLM-540B generated chains of thought.
    
[^196]: AirfRANS：近似雷诺平均纳维-斯托克斯解的高保真计算流体力学数据集

    AirfRANS: High Fidelity Computational Fluid Dynamics Dataset for Approximating Reynolds-Averaged Navier-Stokes Solutions. (arXiv:2212.07564v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07564](http://arxiv.org/abs/2212.07564)

    本论文开发了一个数据集AirfRANS，用于研究不同攻击角的翼型在亚音速区域的二维不可压可稳定雷诺平均纳维-斯托克斯方程，并通过度量方法和可视化评估模型的预测能力，提出了深度学习基线研究，以研究AirfRANS的泛化考虑。

    

    由于递归数值解的高昂代价，替代模型对于优化物理动力学中的有意义数量是必要的。特别是对于液体动力学和纳维-斯托克斯方程的计算。然而，尽管数据驱动模型的快速增长，代表真实世界现象的参考数据集仍然缺乏。在这项工作中，我们开发了AirfRANS，在亚音速区域的二维不可压可稳定雷诺平均纳维-斯托克斯方程上，对不同攻击角的翼型进行研究的数据集。我们还介绍了应力力量在几何表面和边界层上的度量方法和可视化来评估模型准确预测问题的关键信息的能力。最后，我们提出了在四个机器学习任务上对AirfRANS进行深度学习基线研究，以研究在不同约束下AirfRANS的泛化考虑。

    Surrogate models are necessary to optimize meaningful quantities in physical dynamics as their recursive numerical resolutions are often prohibitively expensive. It is mainly the case for fluid dynamics and the resolution of Navier-Stokes equations. However, despite the fast-growing field of data-driven models for physical systems, reference datasets representing real-world phenomena are lacking. In this work, we develop AirfRANS, a dataset for studying the two-dimensional incompressible steady-state Reynolds-Averaged Navier-Stokes equations over airfoils at a subsonic regime and for different angles of attacks. We also introduce metrics on the stress forces at the surface of geometries and visualization of boundary layers to assess the capabilities of models to accurately predict the meaningful information of the problem. Finally, we propose deep learning baselines on four machine learning tasks to study AirfRANS under different constraints for generalization considerations: big and s
    
[^197]: 带有圆环核的模式注意力变换器

    Pattern Attention Transformer with Doughnut Kernel. (arXiv:2211.16961v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.16961](http://arxiv.org/abs/2211.16961)

    本论文提出了一种新的模式注意力变换器(PAT)，它采用了新的圆环核设计，以解决图像分类中像素高分辨率的问题。

    

    本文介绍了一种新的体系结构，即Pattern Attention Transformer（PAT），该体系结构由新的圆环核组成。与NLP领域的标记不同，计算机视觉中的Transformer解决了处理图像中像素高分辨率的问题。在ViT中，图像被切成方形的补丁。作为ViT的后续，Swin Transformer提出了一个额外的移位步骤以减少固定边界的存在，这也导致“两个连接的Swin Transformer块”成为模型的最小单位。继承了补丁/窗口的想法，我们的圆环核进一步增强了补丁的设计。它用传感器和更新两种区域代替了线型边界，这是基于自我关注的理解（称为QKVA网格）。圆环核还带来了一个关于核形状的新话题，超越了方形。为了验证其在图像分类上的性能，PAT被设计为由定期八边形形状的Transformer块组成。

    We present in this paper a new architecture, the Pattern Attention Transformer (PAT), that is composed of the new doughnut kernel. Compared with tokens in the NLP field, Transformer in computer vision has the problem of handling the high resolution of pixels in images. In ViT, an image is cut into square-shaped patches. As the follow-up of ViT, Swin Transformer proposes an additional step of shifting to decrease the existence of fixed boundaries, which also incurs 'two connected Swin Transformer blocks' as the minimum unit of the model. Inheriting the patch/window idea, our doughnut kernel enhances the design of patches further. It replaces the line-cut boundaries with two types of areas: sensor and updating, which is based on the comprehension of self-attention (named QKVA grid). The doughnut kernel also brings a new topic about the shape of kernels beyond square. To verify its performance on image classification, PAT is designed with Transformer blocks of regular octagon shape doughn
    
[^198]: 基于闭合形式策略改进算子的离线强化学习

    Offline Reinforcement Learning with Closed-Form Policy Improvement Operators. (arXiv:2211.15956v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15956](http://arxiv.org/abs/2211.15956)

    本文提出了基于行为约束的离线强化学习中的闭合形式策略改进算子，该算子将行为策略建模为高斯混合，利用LogSumExp的下界和Jensen不等式克服了优化困难，能有效处理实际数据集中的异构策略。

    

    行为约束策略优化已被证明是解决离线强化学习问题的一种成功的范式。本文提出了我们的闭合形式策略改进算子。我们发现，行为约束自然地激励了使用一阶泰勒近似，从而导致了策略目标的线性近似。此外，由于实际数据集通常由异构策略收集而来，我们将行为策略建模为高斯混合，并利用LogSumExp的下界和Jensen不等式克服了引起优化困难的问题，从而得到了闭式策略改进算子。我们使用我们的新颖策略改进算子来实例化离线RL算法，并在实验中展示了它们的效果。

    Behavior constrained policy optimization has been demonstrated to be a successful paradigm for tackling Offline Reinforcement Learning. By exploiting historical transitions, a policy is trained to maximize a learned value function while constrained by the behavior policy to avoid a significant distributional shift. In this paper, we propose our closed-form policy improvement operators. We make a novel observation that the behavior constraint naturally motivates the use of first-order Taylor approximation, leading to a linear approximation of the policy objective. Additionally, as practical datasets are usually collected by heterogeneous policies, we model the behavior policies as a Gaussian Mixture and overcome the induced optimization difficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality, giving rise to a closed-form policy improvement operator. We instantiate offline RL algorithms with our novel policy improvement operators and empirically demonstrate their e
    
[^199]: 带有神经HMM的语调可控的自然语音合成技术

    Prosody-controllable spontaneous TTS with neural HMMs. (arXiv:2211.13533v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.13533](http://arxiv.org/abs/2211.13533)

    本文提出一种神经HMM TTS系统，在保持和自然语音对齐状态的前提下，实现了话语级语调控制，使得从少量不规则数据中快速学习变得可能，能够重现自然语音中的多样性表现。

    

    自然语音合成技术中，控制表现情感和语用功能的流畅语音是很有挑战性的。本文提出了一种能够从少量不规则数据中快速学习并同时重现自然语音中表现多样性的TTS架构。具体来说，我们在一个已有的基于神经HMM的TTS系统中添加了话语级的语调控制，这个系统能够稳定、单调地保持和自然语音的对齐状态。我们对控制准确性进行了客观的评估，并进行了感知测试来证明语调控制不会降低合成质量。

    Spontaneous speech has many affective and pragmatic functions that are interesting and challenging to model in TTS. However, the presence of reduced articulation, fillers, repetitions, and other disfluencies in spontaneous speech make the text and acoustics less aligned than in read speech, which is problematic for attention-based TTS. We propose a TTS architecture that can rapidly learn to speak from small and irregular datasets, while also reproducing the diversity of expressive phenomena present in spontaneous speech. Specifically, we add utterance-level prosody control to an existing neural HMM-based TTS system which is capable of stable, monotonic alignments for spontaneous speech. We objectively evaluate control accuracy and perform perceptual tests that demonstrate that prosody control does not degrade synthesis quality. To exemplify the power of combining prosody control and ecologically valid data for reproducing intricate spontaneous speech phenomena, we evaluate the system's
    
[^200]: 基于连续聚类和机器学习的因子图结构优化在符号检测中的应用

    Structural Optimization of Factor Graphs for Symbol Detection via Continuous Clustering and Machine Learning. (arXiv:2211.11406v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2211.11406](http://arxiv.org/abs/2211.11406)

    本文提出了一种基于连续聚类和机器学习的因子图结构优化方法，用于线性码间干扰信道上的符号检测。该方法能够将因子图框架的潜力发挥到极致，实现低复杂度符号检测器，同时结合神经置信传播算法，在特定信道下实现近似最大后验符号检测性能。

    

    我们提出了一种优化因子图结构用于基于图的推断的新方法。作为一个例子推断任务，我们考虑线性码间干扰信道上的符号检测。因子图框架具有产生低复杂度符号检测器的潜力。然而，循环因子图上的求和-乘积算法是次优的，并且其性能对底层图高度敏感。因此，我们使用机器学习以端到端的方式优化底层因子图的结构。为此，我们将结构优化转化为包含已知信道模型的低度因子节点的聚类问题。此外，我们研究了这种方法与神经置信传播的结合，针对特定信道实现了接近最大后验符号检测性能。

    We propose a novel method to optimize the structure of factor graphs for graph-based inference. As an example inference task, we consider symbol detection on linear inter-symbol interference channels. The factor graph framework has the potential to yield low-complexity symbol detectors. However, the sum-product algorithm on cyclic factor graphs is suboptimal and its performance is highly sensitive to the underlying graph. Therefore, we optimize the structure of the underlying factor graphs in an end-to-end manner using machine learning. For that purpose, we transform the structural optimization into a clustering problem of low-degree factor nodes that incorporates the known channel model into the optimization. Furthermore, we study the combination of this approach with neural belief propagation, yielding near-maximum a posteriori symbol detection performance for specific channels.
    
[^201]: 一种利用量子退火器进行特征选择的优势

    An Advantage Using Feature Selection with a Quantum Annealer. (arXiv:2211.09756v4 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.09756](http://arxiv.org/abs/2211.09756)

    利用量子退火器进行特征选择的这种技术，可以尽可能提高特征的预测能力，同时最小化冗余性，使数据处理更准确且有效。

    

    特征选择是一种在统计预测建模中使用的技术，它确定一条记录中与目标变量有强统计关联的特征。在训练中排除与目标变量没有强关联的特征不仅降低了数据的维度，从而减少了算法的时空复杂度，还减少了数据中的噪声，有助于避免过拟合。总之，特征选择有助于训练出性能优异且稳定的统计模型。鉴于经典计算的缺乏可扩展性，当前的技术仅考虑特征的预测能力，而不考虑特征自身之间的冗余性。利用量子退火(QA) 的特征选择近期有所进展，可以提供一种可扩展的技术，旨在尽可能地提高特征的预测能力，同时最小化冗余性。因此，预计该算法将有助于进行bias/variance的平衡，使预测更准确。

    Feature selection is a technique in statistical prediction modeling that identifies features in a record with a strong statistical connection to the target variable. Excluding features with a weak statistical connection to the target variable in training not only drops the dimension of the data, which decreases the time complexity of the algorithm, it also decreases noise within the data which assists in avoiding overfitting. In all, feature selection assists in training a robust statistical model that performs well and is stable. Given the lack of scalability in classical computation, current techniques only consider the predictive power of the feature and not redundancy between the features themselves. Recent advancements in feature selection that leverages quantum annealing (QA) gives a scalable technique that aims to maximize the predictive power of the features while minimizing redundancy. As a consequence, it is expected that this algorithm would assist in the bias/variance trade
    
[^202]: 条件文本生成中的奖励博弈

    Reward Gaming in Conditional Text Generation. (arXiv:2211.08714v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08714](http://arxiv.org/abs/2211.08714)

    在条件文本生成中，使用强化学习（RL）进行训练时，噪声、自然发生的虚假相关性和协变量漂移可能会导致不良模式被错误地赋予高奖励值，这可能会导致奖励博弈，需要解决。

    

    为了使条件文本生成模型的输出与所需行为相一致，越来越多的关注点在于使用从人类注释中学习的奖励函数以强化学习（RL）训练模型。在这个框架下，我们确定了三种常见情况，即由噪声引起的虚假相关性、自然发生的虚假相关性和协变量漂移，其中高奖励被错误地分配给不良模式。我们表明，即使学习到的度量在训练奖励函数所使用的数据分布上表现良好，不良模式在文本生成模型的RL训练过程中仍有可能被放大。尽管RL或安全社区已经开始讨论奖励博弈，但在这篇讨论中，我们希望使用具体的条件文本生成示例，重点介绍自然语言生成（NLG）社区中的奖励博弈，并讨论可能的修复措施和未来的研究方向。

    To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during RL training of the text generation model. While there has been discussion about reward gaming in the RL or safety community, in this discussion piece, we would like to highlight reward gaming in the natural language generation (NLG) community using concrete conditional text generation examples and discuss potential fixes and areas for future work.
    
[^203]: 机制模式连通性

    Mechanistic Mode Connectivity. (arXiv:2211.08422v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08422](http://arxiv.org/abs/2211.08422)

    本文从模式连通性的视角研究神经网络损失景观，提出了机制相似性的定义，并演示了两个模型之间缺乏线性连通性意味着它们使用不同的机制来做出预测。此外，本文还提出了一种名为基于连通性的微调（CBFT）的方法，用于目标修改模型机制，有助于消除模型对虚假特征的依赖。

    

    我们从模式连通性的视角研究神经网络损失景观，即通过在数据集上训练而检索到的神经网络的最小化器通过低损失的简单路径相互连接。具体而言，我们提出了机制相似性的定义，即与输入转换的共享不变性，并演示了两个模型之间缺乏线性连通性意味着它们使用不同的机制来做出预测。与实践相关的是，这个结果帮助我们证明了在下游数据集上的简单微调可能无法改变模型的机制，例如，微调可能无法消除模型对虚假特征的依赖。我们的分析还推动了一种名为基于连通性的微调（CBFT）的目标变化模型机制的方法，我们使用几个合成数据集对其进行了分析。

    We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model's mechanisms, e.g., fine-tuning can fail to eliminate a model's reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model's mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthe
    
[^204]: 神经网络中的 emergent 语言结构是脆弱的

    Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17406](http://arxiv.org/abs/2210.17406)

    本文提出了一个框架来评估语言模型对于语法的表述的一致性和稳健性，通过多项实验证据表明，神经网络中 emergent 语言结构是脆弱的。

    

    大型语言模型（LLM）在自然语言处理任务中表现强劲。然而，准确度等性能指标并不能衡量模型在代表复杂语言结构方面的质量。本文针对语言模型代表语法的能力，提出了一个评估语言表述的一致性和稳健性的框架。为此，我们介绍了一些稳健性的神经网络模型度量方式，这些度量方式利用最近在通过探测任务从LLM中提取语言结构的先进技术，即用于从语言模型中提取有意义信息的简单任务，如语法重构和根识别。实证上，我们通过分析四种LLM在六个不同的语料库上对语法保持扰动的性能和稳健性来研究所提出的稳健度量方式的表现。我们提供了证据

    Large Language Models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structure. In this paper, focusing on the ability of language models to represent syntax, we propose a framework to assess the consistency and robustness of linguistic representations. To this end, we introduce measures of robustness of neural network models that leverage recent advances in extracting linguistic constructs from LLMs via probing tasks, i.e., simple tasks used to extract meaningful information about a single facet of a language model, such as syntax reconstruction and root identification. Empirically, we study the performance of four LLMs across six different corpora on the proposed robustness measures by analysing their performance and robustness with respect to syntax-preserving perturbations. We provide evide
    
[^205]: 使用嵌套傅里叶神经算子进行实时高分辨率CO$_2$地质封存预测

    Real-time high-resolution CO$_2$ geological storage prediction using nested Fourier neural operators. (arXiv:2210.17051v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17051](http://arxiv.org/abs/2210.17051)

    介绍了一种机器学习框架，嵌套傅里叶神经算子（Nested Fourier Neural Operator，FNO），用于在盆地尺度上进行高分辨率动态三维CO$_2$封存建模。相对于现有方法，加快了将近700,000倍的流动预测速度。

    

    碳捕集和封存在全球减碳过程中扮演着至关重要的角色。扩大CO$_2$封存的规模需要准确高分辨率地建模储存库的压力积累和气态羽流动。然而，由于现有数值方法的高计算成本，在规模上进行这样的建模非常具有挑战性。这样的挑战导致评估封存机会存在显着不确定性，这可能会延迟大规模碳捕集和封存的步伐。我们介绍了嵌套傅里叶神经算子（Nested Fourier Neural Operator，FNO），这是一个机器学习框架，用于在盆地尺度上进行高分辨率动态三维CO$_2$封存建模。嵌套FNO使用一系列FNO的分层预测产生不同细化级别的预测结果，相对于现有方法，加快了将近700,000倍的流动预测速度。通过学习控制偏微分方程组家族的解算符，嵌套FNO为CO$_2$封存创建了一个通用的数值模拟器备选方案。

    Carbon capture and storage (CCS) plays an essential role in global decarbonization. Scaling up CCS deployment requires accurate and high-resolution modeling of the storage reservoir pressure buildup and the gaseous plume migration. However, such modeling is very challenging at scale due to the high computational costs of existing numerical methods. This challenge leads to significant uncertainties in evaluating storage opportunities, which can delay the pace of large-scale CCS deployment. We introduce Nested Fourier Neural Operator (FNO), a machine-learning framework for high-resolution dynamic 3D CO2 storage modeling at a basin scale. Nested FNO produces forecasts at different refinement levels using a hierarchy of FNOs and speeds up flow prediction nearly 700,000 times compared to existing methods. By learning the solution operator for the family of governing partial differential equations, Nested FNO creates a general-purpose numerical simulator alternative for CO2 storage with dive
    
[^206]: 算术采样：用于大型语言模型的并行多样化解码

    Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models. (arXiv:2210.15458v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.15458](http://arxiv.org/abs/2210.15458)

    本文章提出了一种算术采样框架，该方法可兼容常见的采样变化，具有可证明的束多样性和令人尴尬的并行性，从原始模型提供无偏和一致的期望。在WMT机器翻译中表现出良好的效果。

    

    大型语言模型的解码方法通常在输出多样性和计算并行性之间进行权衡。本文提出了一种框架，根据由大型语言模型隐式定义的算术代码书进行采样，兼容常见的采样变化，满足一定条件下的可证明的束多样性，同时具有令人尴尬的并行性，并从原始模型提供无偏和一致的期望。我们在WMT机器翻译上展示了我们方法的有效性，将预期的BLEU分数奖励的标准差减少了一半以上，同时与先前的最新方法有了相当的表现。

    Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a different output for each element of the beam, but are not easy to parallelize. Alternatively, methods such as temperature sampling and its modifications (top-k sampling, nucleus sampling, typical decoding, and others), are embarrassingly parallel, but have no guarantees about duplicate samples. We present a framework for sampling according to an arithmetic code book implicitly defined by a large language model, compatible with common sampling variations, with provable beam diversity under certain conditions, as well as being embarrassingly parallel and providing unbiased and consistent expectations from the original model. We demonstrate the effectiveness of our approach on WMT machine translation, more than halving the standard deviation when estimating expected BLEU score reward, and closing the 
    
[^207]: 用中点 Mixup 在多视角数据中可证明学习多元特征

    Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup. (arXiv:2210.13512v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13512](http://arxiv.org/abs/2210.13512)

    本文提出了一种基于中点 Mixup 的多视角数据学习方法，相比于传统经验风险最小化方法能够更好地学习每个类别的所有特征，具有更好的泛化和鲁棒性。

    

    Mixup 是一种数据增强技术，依赖于使用数据点和标签的随机凸组合进行训练。近年来，Mixup 已成为训练最先进的图像分类模型的标准基元，因为它在泛化和鲁棒性方面比经验风险最小化有明显的优势。在这项工作中，我们试图从特征学习的角度解释一些这种成功。我们关注的分类问题是，每个类别可能具有多个相关特征（或视图），可用于正确预测类别。我们的主要理论结果表明，在具有每类两个特征的一类非平凡数据分布中，使用经验风险最小化训练 2 层卷积网络可能会导致几乎所有类别只学习一个特征，而使用 Mixup 的特定实例进行训练可以成功地学习每个类别的两个特征。

    Mixup is a data augmentation technique that relies on training using random convex combinations of data points and their labels. In recent years, Mixup has become a standard primitive used in the training of state-of-the-art image classification models due to its demonstrated benefits over empirical risk minimization with regards to generalization and robustness. In this work, we try to explain some of this success from a feature learning perspective. We focus our attention on classification problems in which each class may have multiple associated features (or views) that can be used to predict the class correctly. Our main theoretical results demonstrate that, for a non-trivial class of data distributions with two features per class, training a 2-layer convolutional network using empirical risk minimization can lead to learning only one feature for almost all classes while training with a specific instantiation of Mixup succeeds in learning both features for every class. We also show
    
[^208]: 煤矿中的金丝雀：使用集成对抗查询实现更好的成员推断

    Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries. (arXiv:2210.10750v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10750](http://arxiv.org/abs/2210.10750)

    本文提出了一种利用对抗工具直接优化具有鉴别性和多样性的查询来实现更好的成员推断的方法，相比于现有方法，在离线场景和低误报重要的情况下实现了更准确的成员推断。

    

    随着机器学习模型的日益自动化，保护个人数据所有权和知识产权需要追溯训练数据的原始所有者。成员推断算法通过使用统计技术来确定目标样本是否包含在模型的训练集中来解决这个问题。然而，现有方法只利用原始目标样本或目标样本的简单扩展来计算统计量。这种对模型行为的稀疏采样带来了很少的信息，导致成员推断的能力较差。在本文中，我们使用对抗工具直接优化具有鉴别性和多样性的查询。与现有方法相比，我们的改进在离线场景和低误报重要的情况下实现了更准确的成员推断。代码可在 https://github.com/YuxinWenRick/canary-in-a-coalmine 中获取。

    As industrial applications are increasingly automated by machine learning models, enforcing personal data ownership and intellectual property rights requires tracing training data back to their rightful owners. Membership inference algorithms approach this problem by using statistical techniques to discern whether a target sample was included in a model's training set. However, existing methods only utilize the unaltered target sample or simple augmentations of the target to compute statistics. Such a sparse sampling of the model's behavior carries little information, leading to poor inference capabilities. In this work, we use adversarial tools to directly optimize for queries that are discriminative and diverse. Our improvements achieve significantly more accurate membership inference than existing methods, especially in offline scenarios and in the low false-positive regime which is critical in legal settings. Code is available at https://github.com/YuxinWenRick/canary-in-a-coalmine
    
[^209]: 推荐去噪的高效双层优化方法

    Efficient Bi-Level Optimization for Recommendation Denoising. (arXiv:2210.10321v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2210.10321](http://arxiv.org/abs/2210.10321)

    本文提出了一种推荐去噪的高效双层优化方法，该方法可以迭代调整推荐模型，以考虑前几次迭代中为每个反馈分配的权重。

    

    在实际推荐系统中，获得明确的用户反馈（例如评分）通常会受到需要用户积极参与的限制。为了缓解这个问题，利用用户浏览期间生成的隐式反馈（例如点击）作为可行的替代方法。然而，隐式反馈具有很高的噪声，这会显着损害推荐质量。本文中，我们将推荐去噪建模为一个双层优化问题，通过考虑前几次迭代中为每个反馈分配的权重来迭代地调整推荐模型。实验结果表明，该方法在两个大规模推荐数据集上的表现优于现有的基准方法。

    The acquisition of explicit user feedback (e.g., ratings) in real-world recommender systems is often hindered by the need for active user involvement. To mitigate this issue, implicit feedback (e.g., clicks) generated during user browsing is exploited as a viable substitute. However, implicit feedback possesses a high degree of noise, which significantly undermines recommendation quality. While many methods have been proposed to address this issue by assigning varying weights to implicit feedback, two shortcomings persist: (1) the weight calculation in these methods is iteration-independent, without considering the influence of weights in previous iterations, and (2) the weight calculation often relies on prior knowledge, which may not always be readily available or universally applicable.  To overcome these two limitations, we model recommendation denoising as a bi-level optimization problem. The inner optimization aims to derive an effective model for the recommendation, as well as g
    
[^210]: 关于因果位置-尺度噪声模型的可识别性和估计

    On the Identifiability and Estimation of Causal Location-Scale Noise Models. (arXiv:2210.09054v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.09054](http://arxiv.org/abs/2210.09054)

    本文研究了一类异方差噪声模型，发现除特殊情况外因果方向是可识别的。提出了两个估计器，能够准确识别因果效应。

    

    本文研究了一类位置-尺度或异方差噪声模型（LSNMs），在其中，效应$ Y $可以被写成是因果$ X $和与$ X $无关的噪声源$ N $的函数，但可能被因果$ X $缩放为一个正函数$ g（X）$，即$ Y = f（X）+ g（X）N $。尽管模型类别非常广泛，但我们发现除一些特殊情况外，因果方向是可识别的。为了在实证上验证这些理论发现，我们提出了两个LSNMs的估计器：一个基于（非线性）特征映射的估计器和一个基于神经网络的估计器。两者将$ Y $给定$ X $的条件分布建模为由其自然参数参数化的高斯分布。当特征映射被正确规定时，我们证明我们的估计器是联合凸的，并且是因果效应识别任务的一致估计器。尽管神经网络没有继承这些保证，但它可以拟合任意复杂性的函数，并达到最先进的性能。

    We study the class of location-scale or heteroscedastic noise models (LSNMs), in which the effect $Y$ can be written as a function of the cause $X$ and a noise source $N$ independent of $X$, which may be scaled by a positive function $g$ over the cause, i.e., $Y = f(X) + g(X)N$. Despite the generality of the model class, we show the causal direction is identifiable up to some pathological cases. To empirically validate these theoretical findings, we propose two estimators for LSNMs: an estimator based on (non-linear) feature maps, and one based on neural networks. Both model the conditional distribution of $Y$ given $X$ as a Gaussian parameterized by its natural parameters. When the feature maps are correctly specified, we prove that our estimator is jointly concave, and a consistent estimator for the cause-effect identification task. Although the the neural network does not inherit those guarantees, it can fit functions of arbitrary complexity, and reaches state-of-the-art performance
    
[^211]: SQuId: 在多语言中测量语音自然度

    SQuId: Measuring Speech Naturalness in Many Languages. (arXiv:2210.06324v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06324](http://arxiv.org/abs/2210.06324)

    SQuId是一个使用一百万个评分进行训练并在65个语言环境下进行测试的多语言自然度预测模型，该模型的训练在许多环境中始终优于单一环境的基线，并展现了比竞争基线更出色的表现;研究还突出了跨语言细调的有效性，以及非语言效果如声音畸变在跨语言细调中的作用。

    

    文本到语音的许多研究都依赖于人类评估，这会产生重大成本并减缓开发进程。在重度多语言应用中，招募和调查评审员可能需要数周时间。我们介绍了SQuId（语音质量识别），这是一个使用一百万个评分进行训练并在65个语言环境下进行测试的多语言自然度预测模型，是迄今为止这种类型中最大的努力。主要的见解是，在许多环境中训练一个模型始终优于单一环境的基线。我们介绍了我们的任务、模型，并展示了它的表现优于基于w2v-BERT和VoiceMOS的竞争基线50.0%。然后，我们演示了跨语言细调的有效性，并强调了它对零样本环境的影响，即没有细调数据的环境。通过一系列分析，我们突出了非语言效果（如声音畸变）在跨语言细调中的作用。最后，我们展示了可能是第一个多语言自然度数据集，其中包含65个环境中超过一百万评分。

    Much of text-to-speech research relies on human evaluation, which incurs heavy costs and slows down the development process. The problem is particularly acute in heavily multilingual applications, where recruiting and polling judges can take weeks. We introduce SQuId (Speech Quality Identification), a multilingual naturalness prediction model trained on over a million ratings and tested in 65 locales-the largest effort of this type to date. The main insight is that training one model on many locales consistently outperforms mono-locale baselines. We present our task, the model, and show that it outperforms a competitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then demonstrate the effectiveness of cross-locale transfer during fine-tuning and highlight its effect on zero-shot locales, i.e., locales for which there is no fine-tuning data. Through a series of analyses, we highlight the role of non-linguistic effects such as sound artifacts in cross-locale transfer. Finally, we
    
[^212]: 时态一致的变换器用于视频生成

    Temporally Consistent Transformers for Video Generation. (arXiv:2210.02396v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.02396](http://arxiv.org/abs/2210.02396)

    本文介绍了一种名为TECO的生成模型，其可以大大提高视频生成的长期时间一致性。作者提出了3个难度不同的视频数据集以评估该模型和现有模型在时间一致性方面的局限性。

    

    为了生成准确的视频，算法必须理解世界中的时空依赖关系。目前的算法虽然能够准确预测短期内的内容，但往往存在时间上的不一致性。当生成的内容消失后再次出现时，模型会发明不同的内容。尽管存在这种严重的限制，但目前尚不存在复杂数据的已建立基准来对具有长期时间依赖性的视频生成进行严格评估。在本文中，我们通过渲染通过 3D 场景的流程迷宫，Minecraft 世界和室内扫描，策划了三个具有长程依赖的视频数据集。我们对当前模型进行全面评估，并观察到它们在时间一致性方面的局限性。此外，我们引入了时态一致的变换器（TECO），一种生成模型，它显著提高了长期一致性，同时也减少了采样时间。通过将其输入序列压缩为更少的项，

    To generate accurate videos, algorithms have to understand the spatial and temporal dependencies in the world. Current algorithms enable accurate predictions over short horizons but tend to suffer from temporal inconsistencies. When generated content goes out of view and is later revisited, the model invents different content instead. Despite this severe limitation, no established benchmarks on complex data exist for rigorously evaluating video generation with long temporal dependencies. In this paper, we curate 3 challenging video datasets with long-range dependencies by rendering walks through 3D scenes of procedural mazes, Minecraft worlds, and indoor scans. We perform a comprehensive evaluation of current models and observe their limitations in temporal consistency. Moreover, we introduce the Temporally Consistent Transformer (TECO), a generative model that substantially improves long-term consistency while also reducing sampling time. By compressing its input sequence into fewer e
    
[^213]: 基于特征学习的多样性隐私保护反事实解释方法

    Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations. (arXiv:2209.13446v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.13446](http://arxiv.org/abs/2209.13446)

    本文提出了一种基于特征学习的多样性隐私保护反事实解释方法，可以有效地处理反事实约束条件，并为私有解释模型做出贡献。

    

    可解释的机器学习致力于理解长期以来因缺乏可解释性而声名狼藉的复杂黑盒子系统的推理过程。其中一种蓬勃发展的方法是通过反事实解释，为用户提供如何改变结果的建议。反事实样本不仅必须反驳黑盒分类器的原始预测，还必须满足各种实际应用的约束条件，其中多样性是关键约束之一但仍较少讨论。虽然多样化的反事实很理想，但同时解决其他约束条件在计算上具有挑战性。此外，共享反事实数据存在越来越多的隐私问题。因此，本文提出了一种基于特征学习的框架来有效地处理反事实约束条件，并为私有解释模型的有限资料库做出贡献。我们在各种数据集和黑盒模型上展示了我们的框架的灵活性和有效性。

    Interpretable machine learning seeks to understand the reasoning process of complex black-box systems that are long notorious for lack of explainability. One flourishing approach is through counterfactual explanations, which provide suggestions on what a user can do to alter an outcome. Not only must a counterfactual example counter the original prediction from the black-box classifier but it should also satisfy various constraints for practical applications. Diversity is one of the critical constraints that however remains less discussed. While diverse counterfactuals are ideal, it is computationally challenging to simultaneously address some other constraints. Furthermore, there is a growing privacy concern over the released counterfactual data. To this end, we propose a feature-based learning framework that effectively handles the counterfactual constraints and contributes itself to the limited pool of private explanation models. We demonstrate the flexibility and effectiveness of o
    
[^214]: 一些监督是必须的：通过认知不确定性度量在强化学习中引入神谕策略

    Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics. (arXiv:2208.10533v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10533](http://arxiv.org/abs/2208.10533)

    本文提出一种名为评判置信度引导探索的方法，用于将现有的神谕策略纳入标准的演员-评论家强化学习算法中，以提高探索效率。在不确定性高时，该方法会将神谕策略的行动作为建议纳入学习方案中，而在不确定性低时忽略它。

    

    强化学习的固有问题是通过随机行动探索环境，其中很大一部分可能是无效的。相反，可以通过使用现有的（先前学习的或硬编码的）神谕策略、离线数据或演示来改善探索。但在使用神谕策略的情况下，如何最大化学习样本效率地将神谕经验融入到学习策略中可能不清楚。本文提出了一种名为评判置信度引导探索（Critic Confidence Guided Exploration，CCGE）的方法，用于将这样的神谕策略纳入标准的演员-评论家强化学习算法中。具体而言，当不确定性高时，CCGE以神谕策略的行动为建议，并将此信息纳入学习方案中，而当不确定性低时忽略它。CCGE对不确定性估计方法不加区分，并且我们证明它与现有算法相当。

    An inherent problem of reinforcement learning is performing exploration of an environment through random actions, of which a large portion can be unproductive. Instead, exploration can be improved by initializing the learning policy with an existing (previously learned or hard-coded) oracle policy, offline data, or demonstrations. In the case of using an oracle policy, it can be unclear how best to incorporate the oracle policy's experience into the learning policy in a way that maximizes learning sample efficiency. In this paper, we propose a method termed Critic Confidence Guided Exploration (CCGE) for incorporating such an oracle policy into standard actor-critic reinforcement learning algorithms. More specifically, CCGE takes in the oracle policy's actions as suggestions and incorporates this information into the learning scheme when uncertainty is high, while ignoring it when the uncertainty is low. CCGE is agnostic to methods of estimating uncertainty, and we show that it is equa
    
[^215]: 预测聚合的样本复杂度

    Sample Complexity of Forecast Aggregation. (arXiv:2207.13126v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.13126](http://arxiv.org/abs/2207.13126)

    本文研究了一种预测聚合模型，考虑了贝叶斯方法应用于专家命中后的信号汇聚。论文中提供了对于该问题的样本复杂度，表明该复杂度至少为 $\tilde \Omega(m^{n-2} / \varepsilon)$，其中 m 是每个专家信号空间的大小。

    

    我们考虑了一种贝叶斯预测聚合模型，有 n 个专家根据未知二元事件的私有信号报告事件的后验信念给负责人，随后该负责人将报告聚合为对该事件的单个预测。专家的信号和事件的结果服从一个联合分布，该分布对于负责人是未知的，但负责人可以从分布中得到 i.i.d.“样本”，其中每个样本都是由专家的报告（不是信号）和事件的实现组成的元组。使用这些样本，负责人旨在找到一个 $\varepsilon$-最优聚合器，其中最优性是以聚合预测与事件实现之间的预期平方距离来衡量的。我们证明，对于任意离散分布，这个问题的样本复杂度至少是 $\tilde \Omega(m^{n-2} / \varepsilon)$，其中 m 是每个专家信号空间的大小。

    We consider a Bayesian forecast aggregation model where $n$ experts, after observing private signals about an unknown binary event, report their posterior beliefs about the event to a principal, who then aggregates the reports into a single prediction for the event. The signals of the experts and the outcome of the event follow a joint distribution that is unknown to the principal, but the principal has access to i.i.d. "samples" from the distribution, where each sample is a tuple of the experts' reports (not signals) and the realization of the event. Using these samples, the principal aims to find an $\varepsilon$-approximately optimal aggregator, where optimality is measured in terms of the expected squared distance between the aggregated prediction and the realization of the event. We show that the sample complexity of this problem is at least $\tilde \Omega(m^{n-2} / \varepsilon)$ for arbitrary discrete distributions, where $m$ is the size of each expert's signal space. This sample
    
[^216]: 一种用于实时视觉处理的超低功耗TinyML系统

    An Ultra-low Power TinyML System for Real-time Visual Processing at Edge. (arXiv:2207.04663v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2207.04663](http://arxiv.org/abs/2207.04663)

    本文提出了一种超低功耗TinyML系统，采用微型骨干网络构建高效CNN模型，再由特别设计的神经协处理器与MCU相连，将所有特征和权重储存在芯片上，完全消除芯片外存储器访问的延迟和功耗。此系统具有相当的准确性并实现了创纪录的160mW超低功耗。

    

    TinyML是在资源和功耗严格限制的系统上执行AI工作负载的重要且具有挑战性的领域。本文首先提出了一种极其微小的骨干网络，用于构建用于各种视觉任务的高效CNN模型。然后，一种特别设计的神经协处理器（NCP）与MCU相连，构建了一种超低功耗的TinyML系统，该系统将所有特征和权重存储在芯片上，并完全消除了芯片外存储器访问的延迟和功耗。此外，还提出了一种应用特定的指令集，以实现敏捷开发和快速部署。广泛的实验表明，基于我们的模型、NCP和指令集的TinyML系统在实现30FPS的目标检测和识别时具有相当的准确性，并实现了创纪录的160mW超低功耗。演示视频在\url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}上可用。

    Tiny machine learning (TinyML), executing AI workloads on resource and power strictly restricted systems, is an important and challenging topic. This brief firstly presents an extremely tiny backbone to construct high efficiency CNN models for various visual tasks. Then, a specially designed neural co-processor (NCP) is interconnected with MCU to build an ultra-low power TinyML system, which stores all features and weights on chip and completely removes both of latency and power consumption in off-chip memory access. Furthermore, an application specific instruction-set is further presented for realizing agile development and rapid deployment. Extensive experiments demonstrate that the proposed TinyML system based on our model, NCP and instruction set yields considerable accuracy and achieves a record ultra-low power of 160mW while implementing object detection and recognition at 30FPS. The demo video is available on \url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}.
    
[^217]: 类别不平衡下的学习动态理论分析

    A Theoretical Analysis of the Learning Dynamics under Class Imbalance. (arXiv:2207.00391v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.00391](http://arxiv.org/abs/2207.00391)

    本文分析证明了数据不平衡对学习的负面影响，说明在使用梯度下降训练时，少数和多数类的学习曲线会遵循次优轨迹，同时提出对每种类别梯度做出贡献的归一化变体，以解决优化不同类别之间的竞争问题。

    

    数据不平衡是机器学习中常见的问题，会严重影响模型性能。虽然有各种解决方案，但它们对学习动态的收敛影响尚未被理解。本文阐明了数据不平衡对学习的显著负面影响，当使用梯度优化器进行训练时，少数类和多数类的学习曲线会遵循次优轨迹。这种放缓与不平衡比相关，可以追溯到优化不同类别之间的竞争。我们的主要贡献在于分析了全批次（GD）和随机梯度下降（SGD）的收敛和各种对每种类别梯度做出贡献的归一化变体。我们发现GD不能保证降低每个类别的损失，但可以通过执行各自归一化梯度来解决这个问题。使用SGD时, 类别不平衡会对算法产生额外的影响。

    Data imbalance is a common problem in machine learning that can have a critical effect on the performance of a model. Various solutions exist but their impact on the convergence of the learning dynamics is not understood. Here, we elucidate the significant negative impact of data imbalance on learning, showing that the learning curves for minority and majority classes follow sub-optimal trajectories when training with a gradient-based optimizer. This slowdown is related to the imbalance ratio and can be traced back to a competition between the optimization of different classes. Our main contribution is the analysis of the convergence of full-batch (GD) and stochastic gradient descent (SGD), and of variants that renormalize the contribution of each per-class gradient. We find that GD is not guaranteed to decrease the loss for each class but that this problem can be addressed by performing a per-class normalization of the gradient. With SGD, class imbalance has an additional effect on th
    
[^218]: 混合整数规划验证神经网络控制器的稳定性

    Stability Verification of Neural Network Controllers using Mixed-Integer Programming. (arXiv:2206.13374v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2206.13374](http://arxiv.org/abs/2206.13374)

    本文提出了一种用混合整数规划方法验证神经网络控制器稳定性的方法，通过解决混合整数二次规划来检查策略的稳定性。此方法通用，适用于广泛的控制策略。

    

    我们提出了一个框架，用于混合整数线性规划（MILP）可表示控制策略的稳定性验证。该框架将一个固定的候选策略与一个已知稳定但计算成本高的固定基准策略进行比较，并提供了有关候选策略的闭环稳定性的充分条件，这些条件是基于与基准策略的最坏近似误差关系的，并且这些条件可以通过解决混合整数二次规划（MIQP）来检查。此外，我们展示了一个MIPL可以计算候选策略稳定区域的外部和内部逼近。所提出的框架足够通用，可适应广泛的候选策略，包括ReLU神经网络（NN）、参数二次规划的最优解映射和带有混合整数约束的模型预测控制（MPC）策略。

    We propose a framework for the stability verification of Mixed-Integer Linear Programming (MILP) representable control policies. This framework compares a fixed candidate policy, which admits an efficient parameterization and can be evaluated at a low computational cost, against a fixed baseline policy, which is known to be stable but expensive to evaluate. We provide sufficient conditions for the closed-loop stability of the candidate policy in terms of the worst-case approximation error with respect to the baseline policy, and we show that these conditions can be checked by solving a Mixed-Integer Quadratic Program (MIQP). Additionally, we demonstrate that an outer and inner approximation of the stability region of the candidate policy can be computed by solving an MILP. The proposed framework is sufficiently general to accommodate a broad range of candidate policies including ReLU Neural Networks (NNs), optimal solution maps of parametric quadratic programs, and Model Predictive Con
    
[^219]: 非自适应20问题搜索移动目标的分辨率极限

    Resolution Limits of Non-Adaptive 20 Questions Search for a Moving Target. (arXiv:2206.08884v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2206.08884](http://arxiv.org/abs/2206.08884)

    本文研究了在未知初始位置和速度下的移动目标的非自适应搜索策略，通过最少次数的查询预言机，准确估计目标在任何指定时间的位置的分辨率极限。

    

    使用带有查询依赖噪声的20个问题估计框架，我们研究了移动目标在未知初始位置和速度下的非自适应搜索策略，并采用分段常速度模型在单位立方体上进行。在这个搜索问题中，有一个知道任何时候目标瞬时位置的预言机。我们的任务是尽可能少地查询预言机，以准确地估计目标在任何指定时间的位置。我们首先研究了预言机对每个查询的回答都受到离散噪声污染的情况，然后将我们的结果推广到加性高斯噪声的情况。在我们的公式中，性能准则是分辨率，它定义为真实位置和估计位置之间的最大$L_\infty$距离。我们通过导出非渐近和渐近界来描述具有有限查询次数的最佳非自适应查询过程的最小分辨率。我们的界限是严格的。

    Using the 20 questions estimation framework with query-dependent noise, we study non-adaptive search strategies for a moving target over the unit cube with unknown initial location and velocities under a piecewise constant velocity model. In this search problem, there is an oracle who knows the instantaneous location of the target at any time. Our task is to query the oracle as few times as possible to accurately estimate the location of the target at any specified time. We first study the case where the oracle's answer to each query is corrupted by discrete noise and then generalize our results to the case of additive white Gaussian noise. In our formulation, the performance criterion is the resolution, which is defined as the maximal $L_\infty$ distance between the true locations and estimated locations. We characterize the minimal resolution of an optimal non-adaptive query procedure with a finite number of queries by deriving non-asymptotic and asymptotic bounds. Our bounds are tig
    
[^220]: 非线性独立分量分析的可辨识性：稀疏性及其它

    On the Identifiability of Nonlinear ICA: Sparsity and Beyond. (arXiv:2206.07751v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07751](http://arxiv.org/abs/2206.07751)

    本文提出一个新的方法，考虑混合过程的假设，即结构稀疏性，来实现非线性ICA的可识别性，无需辅助变量。

    

    非线性独立分量分析旨在从其可观测的非线性混合中恢复出潜在独立分量。如何使非线性ICA模型可辨识直到某些平凡不确定性是无监督学习中的一个长期问题。最近的突破是将源的标准独立性假设重新定义为在某些辅助变量（例如类标签和/或域/时间索引）给定的条件独立性，作为弱监督或归纳偏置。然而，具有无条件先验的非线性ICA无法从这些发展中受益。我们探索了一条替代路径，并仅考虑混合过程的假设，例如结构稀疏性。我们展示了在这些约束的具体实例下，独立的潜在分量可以从其非线性混合中辨识出来，达到非平凡的非线性ICA可识别性，而无需辅助变量。

    Nonlinear independent component analysis (ICA) aims to recover the underlying independent latent sources from their observable nonlinear mixtures. How to make the nonlinear ICA model identifiable up to certain trivial indeterminacies is a long-standing problem in unsupervised learning. Recent breakthroughs reformulate the standard independence assumption of sources as conditional independence given some auxiliary variables (e.g., class labels and/or domain/time indexes) as weak supervision or inductive bias. However, nonlinear ICA with unconditional priors cannot benefit from such developments. We explore an alternative path and consider only assumptions on the mixing process, such as Structural Sparsity. We show that under specific instantiations of such constraints, the independent latent sources can be identified from their nonlinear mixtures up to a permutation and a component-wise transformation, thus achieving nontrivial identifiability of nonlinear ICA without auxiliary variable
    
[^221]: StructCoder: 面向代码生成的结构感知Transformer模型

    StructCoder: Structure-Aware Transformer for Code Generation. (arXiv:2206.05239v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05239](http://arxiv.org/abs/2206.05239)

    本文提出了一个结构感知的Transformer模型，通过引入AST和DFG辅助任务，旨在解决现有代码生成模型在面对代码语法和语义时的训练不足问题。

    

    近年来，使用深度学习来自动化软件工程任务的兴趣日益增长。本文解决了代码生成问题，目标在于在给定不同语言或自然语言描述的源代码的情况下生成目标代码。针对代码语法和语义的严格理解和生成需要一种更为严谨的训练策略。出于这个动机，我们开发了一个编码器-解码器Transformer模型，在此模型中，编码器和解码器都明确地受过训练，以识别源代码和目标代码的语法和数据流。我们不仅通过利用源代码的语法树和数据流图使编码器结构感知，还通过引入两个新的辅助任务——AST（抽象语法）和DFG（数据流图）帮助解码器保留目标代码的语法和数据流。

    There has been a recent surge of interest in automating software engineering tasks using deep learning. This paper addresses the problem of code generation where the goal is to generate target code given source code in a different language or a natural language description. Most of the state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model where both the encoder and decoder are explicitly trained to recognize the syntax and data flow in the source and target codes, respectively. We not only make the encoder structure-aware by leveraging the source code's syntax tree and data flow graph, but we also support the decoder in preserving the syntax and data flow of the target code by introducing two novel auxiliary tasks: AST (Abstract Syntax
    
[^222]: 在不稳健样本上施加更多正则化以提高对抗性鲁棒性

    Improving adversarial robustness by putting more regularizations on less robust samples. (arXiv:2206.03353v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.03353](http://arxiv.org/abs/2206.03353)

    本文提出了一种新的对抗训练算法，通过在容易受到对抗攻击的数据上施加更多正则化以提高对抗性鲁棒性，得到了在准确性和鲁棒性方面均为最优的表现。

    

    对抗性训练是提高对抗攻击鲁棒性的一种方法，在人类视觉无法察觉的数据扰动下，使给定的深度神经网络产生误判。本文提出了一种新的对抗训练算法，它在理论上得到很好的证明，并且在实践中表现优于其他现有的算法。该算法的一个新的特点是：对于容易受到对抗攻击的数据，比其他现有的正则化算法更多地应用正则化。理论上，我们证明了我们的算法可以被理解为一个最小化经验风险的正则化算法，它来自一个新的鲁棒风险上界的动机。数值实验表明，我们提出的算法同时提高了泛化性能(在例子上的准确性)和鲁棒性(在对抗攻击上的准确性)，达到了最先进的性能水平。

    Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing the regularized empirical risk motivated from a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.
    
[^223]: 通过高效探索学习预测的静态调度

    Static Scheduling with Predictions Learned through Efficient Exploration. (arXiv:2205.15695v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15695](http://arxiv.org/abs/2205.15695)

    本文研究了单机作业调度的问题，提出了一种基于学习预测的静态调度算法，在类型未知的情况下实现了次线性的过剩成本，尤其在抢占式问题中表现出色，可以在不同作业类型持续时间相差很大时优于非抢占匹配。

    

    我们研究了单机作业调度，每个作业都属于决定其持续时间分布的作业类型。我们首先分析了类型特征已知的情况，然后转向两种学习情景，其中类型未知：非抢占式问题，它要求完成已启动的作业，然后才能移动到另一个作业；和抢占式问题，这里作业执行可以暂停以优先转移到另一个作业。在两种情况下，我们设计的算法相对于已知类型的性能实现了次线性的过剩成本，并证明了非抢占式情况的下限。值得注意的是，我们展示了抢占算法在不同作业类型持续时间相差很大时，理论上和通过模拟的方式可以优于非抢占匹配，在类型持续时间已知时并不存在这种现象。

    We study single-machine scheduling of jobs, each belonging to a job type that determines its duration distribution. We start by analyzing the scenario where the type characteristics are known and then move to two learning scenarios where the types are unknown: non-preemptive problems, where each started job must be completed before moving to another job; and preemptive problems, where job execution can be paused in the favor of moving to a different job. In both cases, we design algorithms that achieve sublinear excess cost, compared to the performance with known types, and prove lower bounds for the non-preemptive case. Notably, we demonstrate, both theoretically and through simulations, how preemptive algorithms can greatly outperform non-preemptive ones when the durations of different job types are far from one another, a phenomenon that does not occur when the type durations are known.
    
[^224]: 受限单调神经网络

    Constrained Monotonic Neural Networks. (arXiv:2205.11775v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11775](http://arxiv.org/abs/2205.11775)

    本文针对实际应用场景需要的单调性，提出了一种通过在层中的一部分神经元中采用原始激活函数，同时在另一部分采用其点对称反射来解决构建单调深度神经网络的方法。实验证明，该方法的精度符合要求。

    

    深度神经网络越来越流行，可以逼近从嘈杂数据中得出的任意函数，但在推广过程中需要解释这些模型并对它们施加额外的限制，其中单调性是最受实际应用场景需要的属性之一，并且是该论文的重点。最早构建单调全连接神经网络的方法是将其权重约束为非负，同时采用单调激活函数。不幸的是，该方法无法与常用的非饱和激活函数（如ReLU，ELU，SELU等）一起使用，因为它只能逼近凸函数。我们通过在层中的一部分神经元中采用原始激活函数，同时在另一部分采用其点对称反射来解决这个问题。我们的实验证明，采用这种方法建立单调深度神经网络的精度与其他方法相当甚至更好，同时满足单调性约束。

    Deep neural networks are becoming increasingly popular in approximating arbitrary functions from noisy data. But wider adoption is being hindered by the need to explain such models and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain its weights to be non-negative while employing a monotonic activation function. Unfortunately, this construction does not work with popular non-saturated activation functions such as ReLU, ELU, SELU etc, as it can only approximate convex functions. We show this shortcoming can be fixed by employing the original activation function for a part of the neurons in the layer, and employing its point reflection for the other part. Our experiments show this approach of building monotonic deep neural networks have matching or better accuracy when compared to ot
    
[^225]: 混合流：基于混合流的原则变分推理

    MixFlows: principled variational inference via mixed flows. (arXiv:2205.07475v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.07475](http://arxiv.org/abs/2205.07475)

    本文提出了混合变分流（MixFlows），是由对初始参考分布的映射重复应用的混合组成的一种新的变分家族。MixFlows具有类似于MCMC的收敛保证，并可以提供比几种黑盒归一化流更可靠的后验逼近，与最先进的MCMC方法所获得的样本质量相当。

    

    本文提出了混合变分流（MixFlows），这是一种新的变分家族，由对初始参考分布的映射重复应用的混合组成。首先，我们提供了有效的算法，用于i.i.d.采样、密度评估和无偏ELBO估计。然后，我们证明了当流映射是遍历和保度量的时，MixFlows具有类似于MCMC的收敛保证，并为流映射近似实现提供了误差积累的界限。最后，我们基于未纠正的离散哈密顿动力学和确定性动量恢复开发了 MixFlows 的实现。模拟和实际数据实验表明，MixFlows可以提供比几种黑盒归一化流更可靠的后验逼近，以及与最先进的MCMC方法所获得的样本质量相当的样本。

    This work presents mixed variational flows (MixFlows), a new variational family that consists of a mixture of repeated applications of a map to an initial reference distribution. First, we provide efficient algorithms for i.i.d. sampling, density evaluation, and unbiased ELBO estimation. We then show that MixFlows have MCMC-like convergence guarantees when the flow map is ergodic and measure-preserving, and provide bounds on the accumulation of error for practical implementations where the flow map is approximated. Finally, we develop an implementation of MixFlows based on uncorrected discretized Hamiltonian dynamics combined with deterministic momentum refreshment. Simulated and real data experiments show that MixFlows can provide more reliable posterior approximations than several black-box normalizing flows, as well as samples of comparable quality to those obtained from state-of-the-art MCMC methods.
    
[^226]: 超出等周性的稳健性律法

    A Law of Robustness beyond Isoperimetry. (arXiv:2202.11592v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.11592](http://arxiv.org/abs/2202.11592)

    本文提出了一种两层次的稳健性律法，研究了在任意数据分布下的稳健插值问题，并证明其Lipschitz下界分别为$\Omega(\sqrt{n/p})$和$\Omega(n^{1/d})$。

    

    本文研究在一个有界空间上支持任意数据分布的稳健插值问题，并提出了一个两层次的稳健性律法。稳健插值是指通过Lipschitz函数插值$\mathbb{R}^d$中的$n$个嘈杂训练数据点。尽管当样本来自等周性分布时，已经很好地理解了这个问题，但在一般或最坏情况分布下，其性能仍然不清楚。我们证明了对于任意数据分布，插值神经网络的Lipschitz下界为$\Omega(\sqrt{n/p})$，其中$p$表示参数数量。通过这个结果，我们验证了Bubeck，Li和Nagaraj在二层神经网络中使用多项式权重提出的稳健性律法猜想。然后，我们将结果扩展到任意插值逼近器，并证明了稳健插值的Lipschitz下界为$\Omega(n^{1/d})$。我们的结果证明了一个两层次的稳健性律法：

    We study the robust interpolation problem of arbitrary data distributions supported on a bounded space and propose a two-fold law of robustness. Robust interpolation refers to the problem of interpolating $n$ noisy training data points in $\mathbb{R}^d$ by a Lipschitz function. Although this problem has been well understood when the samples are drawn from an isoperimetry distribution, much remains unknown concerning its performance under generic or even the worst-case distributions. We prove a Lipschitzness lower bound $\Omega(\sqrt{n/p})$ of the interpolating neural network with $p$ parameters on arbitrary data distributions. With this result, we validate the law of robustness conjecture in prior work by Bubeck, Li, and Nagaraj on two-layer neural networks with polynomial weights. We then extend our result to arbitrary interpolating approximators and prove a Lipschitzness lower bound $\Omega(n^{1/d})$ for robust interpolation. Our results demonstrate a two-fold law of robustness: i) w
    
[^227]: 监督多保真度超参数配置竞赛

    Supervising the Multi-Fidelity Race of Hyperparameter Configurations. (arXiv:2202.09774v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.09774](http://arxiv.org/abs/2202.09774)

    本文介绍了一种贝叶斯优化方法DyHPO，它能够学习动态决定哪个超参数配置在所有可行的配置中进行下一步训练。实验表明DyHPO相比于最先进的超参数优化方法有显著的优越性。

    

    近期，多保真度超参数优化技术（HPO）作为调整深度学习方法的一种有希望的方向而出现，然而，现有方法存在超参数配置的HPO预算分配不足的问题。本文提出了一种贝叶斯优化方法DyHPO，该方法可以学习决定在所有可行的配置中动态赛跑中进一步训练哪个超参数配置。我们提出了一种新的深度高斯过程内核，它嵌入了学习曲线动力学，以及一个包含多预算信息的获得函数。我们通过包含50个数据集（表格、图像和自然语言处理）和不同架构（MLP、CNN/NAS、RNN）的大规模实验证明了DyHPO相比于最先进的超参数优化方法的显著优越性。

    Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have recently emerged as a promising direction for tuning Deep Learning methods. However, existing methods suffer from a sub-optimal allocation of the HPO budget to the hyperparameter configurations. In this work, we introduce DyHPO, a Bayesian Optimization method that learns to decide which hyperparameter configuration to train further in a dynamic race among all feasible configurations. We propose a new deep kernel for Gaussian Processes that embeds the learning curve dynamics, and an acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of DyHPO against state-of-the-art hyperparameter optimization methods through large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and diverse architectures (MLP, CNN/NAS, RNN).
    
[^228]: 面向能效的始终开启音频模型的神经架构搜索

    Neural Architecture Search for Energy Efficient Always-on Audio Models. (arXiv:2202.05397v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2202.05397](http://arxiv.org/abs/2202.05397)

    本研究提出对神经架构搜索的改进，能够优化网络在能效、内存使用和准确性上的表现。评估结果表明相比基线MobileNetV1/V2，搜索出的网络每个推理能量少一个数量级且内存占用小得多，同时略微提高了准确率。

    

    始终开启的分类任务对移动和边缘计算设备需要能效高的神经网络架构。本文提出了几种神经架构搜索的改进方法，以提高在实际情况下成功的机会。我们的搜索同时优化网络准确性、能效和内存使用量。我们在真实硬件上对我们的搜索性能进行基准测试，但由于使用真实硬件运行成千上万次测试很困难，因此我们使用随机森林模型粗略预测候选网络的能源使用情况。我们提出了一种搜索策略，使用贝叶斯和正则化进化搜索粒子群，并采用提前停止减少计算负担。我们在基于AudioSet的声事件分类数据集上评估我们的搜索，结果每个推理的能量比我们的基线MobileNetV1/V2实现少一个数量级，内存占用量也小得多，同时略微提高了准确率。

    Mobile and edge computing devices for always-on classification tasks require energy-efficient neural network architectures. In this paper we present several changes to neural architecture searches (NAS) that improve the chance of success in practical situations. Our search simultaneously optimizes for network accuracy, energy efficiency and memory usage. We benchmark the performance of our search on real hardware, but since running thousands of tests with real hardware is difficult we use a random forest model to roughly predict the energy usage of a candidate network. We present a search strategy that uses both Bayesian and regularized evolutionary search with particle swarms, and employs early-stopping to reduce the computational burden. Our search, evaluated on a sound-event classification dataset based upon AudioSet, results in an order of magnitude less energy per inference and a much smaller memory footprint than our baseline MobileNetV1/V2 implementations while slightly improvin
    
[^229]: 基于赌博反馈的在线学习自适应客户端采样在联邦学习中的应用

    Adaptive Client Sampling in Federated Learning via Online Learning with Bandit Feedback. (arXiv:2112.14332v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.14332](http://arxiv.org/abs/2112.14332)

    本文提出一种基于赌博反馈的在线学习算法，用于自适应选择哪些客户端用于联邦学习的训练，并通过理论证明该算法可提高优化算法的收敛速度。

    

    由于通信成本高，联邦学习（FL）系统需要采样一部分客户端参与每一轮训练。因此，客户端采样在FL系统中具有重要作用，它影响用于训练机器学习模型的优化算法的收敛速度。尽管具有重要性，但有效采样客户端方面的研究很有限。在本文中，我们将客户端采样建模为在带有赌博反馈的在线学习任务，使用在线随机镜像下降（OSMD）算法来最小化采样方差。然后，我们在理论上展示了我们的采样方法如何提高优化算法的收敛速度。为了处理OSMD中依赖于未知问题参数的调整参数，我们使用在线集成方法和翻倍技巧。我们证明了相对于任何采样序列的动态遗憾界。遗憾界取决于比较序列的总变化。

    Due to the high cost of communication, federated learning (FL) systems need to sample a subset of clients that are involved in each round of training. As a result, client sampling plays an important role in FL systems as it affects the convergence rate of optimization algorithms used to train machine learning models. Despite its importance, there is limited work on how to sample clients effectively. In this paper, we cast client sampling as an online learning task with bandit feedback, which we solve with an online stochastic mirror descent (OSMD) algorithm designed to minimize the sampling variance. We then theoretically show how our sampling method can improve the convergence speed of optimization algorithms. To handle the tuning parameters in OSMD that depend on the unknown problem parameters, we use the online ensemble method and doubling trick. We prove a dynamic regret bound relative to any sampling sequence. The regret bound depends on the total variation of the comparator seque
    
[^230]: NN2Poly：用于深度前馈人工神经网络的多项式表示方法

    NN2Poly: A polynomial representation for deep feed-forward artificial neural networks. (arXiv:2112.11397v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.11397](http://arxiv.org/abs/2112.11397)

    本文提出了一种名为NN2Poly的方法，用于生成已经训练好的全连接前馈人工神经网络的精确多项式表示。该方法适用于任意深度的多层感知器（MLP）且计算成本相对较低，能够在回归和分类任务中提供非常准确的逼近结果。

    

    尽管深度学习应用非常成功，但神经网络的可解释性和理论行为仍然是一个开放的研究领域。本文提出NN2Poly：一种理论方法，用于获取一个显式多项式模型，以提供已经训练好的全连接前馈人工神经网络（多层感知器或MLP）的精确表示。这种方法扩展了文献中提出的先前想法，该想法仅限于单隐藏层的网络，并且适用于回归和分类任务的任意深度MLP。本文的目标是通过在每层上使用激活函数的泰勒展开式，然后使用几个组合性质来计算所需多项式的系数，从而实现此目标。作者讨论了此方法的主要计算挑战以及通过引入一些逼近来克服这些挑战的方法，而不会影响其准确性。通过实验验证表明，尽管NN2Poly方法简单且计算成本低，但对于合成和真实数据集，提供非常准确的多项式逼近。

    Interpretability of neural networks and their underlying theoretical behavior remain an open field of study even after the great success of their practical applications, particularly with the emergence of deep learning. In this work, NN2Poly is proposed: a theoretical approach to obtain an explicit polynomial model that provides an accurate representation of an already trained fully-connected feed-forward artificial neural network (a multilayer perceptron or MLP). This approach extends a previous idea proposed in the literature, which was limited to single hidden layer networks, to work with arbitrarily deep MLPs in both regression and classification tasks. The objective of this paper is to achieve this by using a Taylor expansion on the activation function, at each layer, and then using several combinatorial properties to calculate the coefficients of the desired polynomials. Discussion is presented on the main computational challenges of this method, and the way to overcome them by i
    
[^231]: TC-GNN：在GPU上连接稀疏GNN计算与密集Tensor Cores的桥梁

    TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs. (arXiv:2112.02052v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.02052](http://arxiv.org/abs/2112.02052)

    本文提出了TC-GNN，这是第一个基于GPU张量核心单元（TCUs）的GNN加速框架。采用稀疏图翻译技术来协调“稀疏”GNN计算与高性能的“密集”TCUs，实现了GNN计算效率的提升。

    

    最近，作为基于图的机器学习的骨干，图神经网络（GNN）展示了在各个领域（如电子商务）的巨大成功。然而，由于高度稀疏和不规则的基于图的操作，GNN的性能通常不尽如人意。为此，我们提出了TC-GNN，基于GPU张量核心单元（TCUs）的第一个GNN加速框架。其核心思想是通过将“稀疏”GNN计算与高性能的“密集”TCUs协调一致，实现GNN计算效率的提升。具体来说，我们对主流GNN计算框架中的稀疏操作进行了深入分析。我们引入了一种新颖的稀疏图翻译技术，以便TCU处理稀疏的GNN工作负载。我们实现了一种有效的CUDA核心和TCU协作设计，充分利用GPU资源。我们将TC-GNN与PyTorch框架集成，以实现高可编程性。严格的实验表明，在各种模型和数据集上，相比于最先进的DGL框架，平均加速了1.70倍。

    Recently, graph neural networks (GNNs), as the backbone of graph-based machine learning, demonstrate great success in various domains (e.g., e-commerce). However, the performance of GNNs is usually unsatisfactory due to the highly sparse and irregular graph-based operations. To this end, we propose TC-GNN, the first GNN acceleration framework based on GPU Tensor Core Units (TCUs). The core idea is to reconcile the "Sparse" GNN computation with the high-performance "Dense" TCUs. Specifically, we conduct an in-depth analysis of the sparse operations in mainstream GNN computing frameworks. We introduce a novel sparse graph translation technique to facilitate TCU processing of the sparse GNN workload. We implement an effective CUDA core and TCU collaboration design to fully utilize GPU resources. We integrate TC-GNN with the PyTorch framework for high programmability. Rigorous experiments show an average of 1.70X speedup over the state-of-the-art DGL framework across various models and dat
    
[^232]: 基于条件概率的深度神经网络在排序一致序回归中的应用

    Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. (arXiv:2111.08851v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08851](http://arxiv.org/abs/2111.08851)

    本文提出新的一种排除限制的排序一致性序回归方法，基于深度神经网络和条件概率的建模，可以在保持输出概率排序一致性的同时，提高了模型的表现。

    

    近年来，深度神经网络在各种分类和模式识别任务中取得了出色的预测性能。然而，许多实际的预测问题具有序响应变量，并且传统的分类损失函数忽略了这种排序信息。基于深度神经网络的序回归方法解决了这个问题。本文提出了一种新的方法，该方法基于建立每个序分类的条件概率的深度神经结构，从而保证模型的输出概率在预测标签类别中保持其排序一致性。

    In recent times, deep neural networks achieved outstanding predictive performance on various classification and pattern recognition tasks. However, many real-world prediction problems have ordinal response variables, and this ordering information is ignored by conventional classification losses such as the multi-category cross-entropy. Ordinal regression methods for deep neural networks address this. One such method is the CORAL method, which is based on an earlier binary label extension framework and achieves rank consistency among its output layer tasks by imposing a weight-sharing constraint. However, while earlier experiments showed that CORAL's rank consistency is beneficial for performance, it is limited by a weight-sharing constraint in a neural network's fully connected output layer, which may restrict the expressiveness and capacity of a network trained using CORAL. We propose a new method for rank-consistent ordinal regression without this limitation. Our rank-consistent ordi
    
[^233]: 从数据中提取动力学模型

    Extracting Dynamical Models from Data. (arXiv:2110.06917v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.06917](http://arxiv.org/abs/2110.06917)

    本文介绍了一种使用机器学习从数据中获得确切动力学信息的方法，并命名为“FJet”。这种方法在谐振子、摆和达芬奇振荡器等示例中被证明可以准确地复制动力学并恢复潜在的微分方程。

    

    通过使用机器学习来对相空间变量的更新建模，本文介绍了确定系统基础动力学的方法。可以准确地复制谐振子、摆和达芬奇振荡器的动力学，并在每个示例中准确地恢复了潜在的微分方程。此方法命名为“FJet”，类似于得到的模型于Runge-Kutta数值积分方案的Taylor级数展开。这种类比具有显式揭示适当函数形式的优势。

    The problem of determining the underlying dynamics of a system when only given data of its state over time has challenged scientists for decades. In this paper, the approach of using machine learning to model the {\em updates} of the phase space variables is introduced; this is done as a function of the phase space variables. (More generally, the modeling is done over the jet space of the variables.) This approach is shown to accurately replicate the dynamics for the examples of the harmonic oscillator, the pendulum, and the Duffing oscillator; the underlying differential equation is also accurately recovered in each example. In addition, the results in no way depend on how the data is sampled over time (i.e., regularly or irregularly). It is demonstrated that this approach (named "FJet") is similar to the model resulting from a Taylor series expansion of the Runge-Kutta (RK) numerical integration scheme. This analogy confers the advantage of explicitly revealing the appropriate functi
    
[^234]: 寻找用于模型重用的物化模型

    Finding Materialized Models for Model Reuse. (arXiv:2110.06532v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.06532](http://arxiv.org/abs/2110.06532)

    本文提出了无需源数据，通用，高效且有效的物化模型查询框架\textsf{MMQ}，使用基于高斯混合模型的分离度对物化模型进行排序以测量相关知识。

    

    物化模型查询旨在寻找最合适的物化模型作为模型重用的初始模型。这是模型重用的前提条件，并且最近受到了广泛关注。然而，现有方法存在提供源数据的需求，应用范围有限和效率低等问题，因为它们没有构建适合度量物化模型目标相关知识的度量标准。为了解决这个问题，我们提出了\textsf{MMQ}，这是一个无需源数据，通用，高效且有效的物化模型查询框架。它使用基于高斯混合模型的度量标准称为分离度来对物化模型进行排序。对于每个物化模型，\textsf{MMQ}首先通过直接应用该模型将目标数据集中的样本向量化为概率向量，然后利用高斯分布为每个概率向量类进行拟合，最后在高斯分布上使用分离度来衡量相关知识。

    Materialized model query aims to find the most appropriate materialized model as the initial model for model reuse. It is the precondition of model reuse, and has recently attracted much attention. {Nonetheless, the existing methods suffer from the need to provide source data, limited range of applications, and inefficiency since they do not construct a suitable metric to measure the target-related knowledge of materialized models. To address this, we present \textsf{MMQ}, a source-data free, general, efficient, and effective materialized model query framework.} It uses a Gaussian mixture-based metric called separation degree to rank materialized models. For each materialized model, \textsf{MMQ} first vectorizes the samples in the target dataset into probability vectors by directly applying this model, then utilizes Gaussian distribution to fit for each class of probability vectors, and finally uses separation degree on the Gaussian distributions to measure the target-related knowledge
    
[^235]: 关于倾斜损失的机器学习理论与应用研究

    On Tilted Losses in Machine Learning: Theory and Applications. (arXiv:2109.06141v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.06141](http://arxiv.org/abs/2109.06141)

    本文研究了一种在机器学习中不常见但在统计、概率与优化等领域常用的技术——指数倾斜，并将其应用到风险最小化中。所提出的方法可以修正单个损失的影响，增加或减少异常值的作用，可以提高泛化性能，并可以被视为损失的尾部概率的平滑近似。

    

    指数倾斜是统计学、概率论、信息论和优化等领域常用的一种技术，用于创建参数化分布转移。尽管在相关领域中非常常见，但倾斜在机器学习中的应用尚不普及。本文旨在通过研究倾斜在风险最小化中的应用来填补这一空白。我们研究了验算风险最小化（TERM）的简单扩展，它使用指数倾斜来灵活调整个别损失的影响。所得到的框架具有多种有用的性质：我们展示了TERM可以分别增加或减少异常值的影响，从而实现公平或鲁棒性。TERM也具有降低方差从而提高泛化性能的优势，并且可以被看作是对损失的尾部概率的平滑近似。我们的工作在TERM和其他相关目标之间建立了严格的联系，例如风险价值、条件风险价值和分布鲁棒优化等。

    Exponential tilting is a technique commonly used in fields such as statistics, probability, information theory, and optimization to create parametric distribution shifts. Despite its prevalence in related fields, tilting has not seen widespread use in machine learning. In this work, we aim to bridge this gap by exploring the use of tilting in risk minimization. We study a simple extension to ERM -- tilted empirical risk minimization (TERM) -which uses exponential tilting to flexibly tune the impact of individual losses. The resulting framework has several useful properties: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to the tail probability of losses. Our work makes rigorous connections between TERM and related objectives, such as Value-at-Risk, Conditional Value-at-Risk, and distributionally robust op
    
[^236]: 鲁棒预测系统动力学的解缠生成模型

    Disentangled Generative Models for Robust Prediction of System Dynamics. (arXiv:2108.11684v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.11684](http://arxiv.org/abs/2108.11684)

    本文研究了解缠生成模型用于预测系统动力学，将领域参数与生成模型的潜在空间中的动力学分离，取得了良好的泛化和长期稳定性。

    

    深度神经网络在动力系统预测中越来越受到关注，但是超出分布范围的泛化和长期稳定性仍然具有挑战性。本文将动力系统的领域参数视为数据生成过程的变异因素，借鉴监督解缠和因果分解的思想，旨在将领域参数与生成模型的潜在空间中的动力学分离。在我们的实验中，我们模拟了相空间和视频序列的动态，并进行了严格的超出分布价值评估。结果表明，解缠的 VAEs 更适应于在训练数据中不存在的领域参数空间。同时，解缠可以改善视频序列中现有最先进模型的长期和超出分布预测能力。

    Deep neural networks have become increasingly of interest in dynamical system prediction, but out-of-distribution generalization and long-term stability still remains challenging. In this work, we treat the domain parameters of dynamical systems as factors of variation of the data generating process. By leveraging ideas from supervised disentanglement and causal factorization, we aim to separate the domain parameters from the dynamics in the latent space of generative models. In our experiments we model dynamics both in phase space and in video sequences and conduct rigorous OOD evaluations. Results indicate that disentangled VAEs adapt better to domain parameters spaces that were not present in the training data. At the same time, disentanglement can improve the long-term and out-of-distribution predictions of state-of-the-art models in video sequences.
    
[^237]: 响应式决策主体下的模型可迁移性研究

    Model Transferability With Responsive Decision Subjects. (arXiv:2107.05911v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.05911](http://arxiv.org/abs/2107.05911)

    本论文研究在响应式和交互式数据分布下，算法预测器的可迁移性问题，提供了性能差距的上界和分类器必须满足的权衡的下界，并刻画了预测器、策略空间选择和偏移性质对可迁移性程度的影响。

    

    在人类智能决策参与的一些数据集上，如果存在一个准确的算法预测器，那么它在这些数据集上的准确率是否会被保持呢？在我们的背景设置中，一个代理人或用户对应于从分布$D$中抽样得到的一个样本$(X,Y)$，并面临着模型$h$及其分类结果$h(X)$。代理人可以修改$X$以适应$h$，这将对$(X,Y)$产生分布偏移。我们的设置是受到了机器学习模型被人类代理人使用并最终面对响应式和交互式数据分布的应用案例的启发。我们通过研究模型在可用源分布（数据）上训练的性能如何转化为其诱导域中性能来系统化地讨论模型的可迁移性。我们提供了由于诱导域偏移而导致的性能差距的上界，以及分类器必须满足的权衡的下界。我们的理论刻画了预测器、策略空间选择和偏移性质对可迁移性程度的影响。

    Given an algorithmic predictor that is accurate on some source population consisting of strategic human decision subjects, will it remain accurate if the population respond to it? In our setting, an agent or a user corresponds to a sample $(X,Y)$ drawn from a distribution $\cal{D}$ and will face a model $h$ and its classification result $h(X)$. Agents can modify $X$ to adapt to $h$, which will incur a distribution shift on $(X,Y)$. Our formulation is motivated by applications where the deployed machine learning models are subjected to human agents, and will ultimately face responsive and interactive data distributions. We formalize the discussions of the transferability of a model by studying how the performance of the model trained on the available source distribution (data) would translate to the performance on its induced domain. We provide both upper bounds for the performance gap due to the induced domain shift, as well as lower bounds for the trade-offs that a classifier has to s
    
[^238]: 自适应实时感知的运行时决策学习

    Learning Runtime Decisions for Adaptive Real-Time Perception. (arXiv:2106.05665v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2106.05665](http://arxiv.org/abs/2106.05665)

    Chanakya是一种学习近似执行框架，通过在实时感知中平衡精度和延迟的训练，自动学习权衡的决策，并在多个感知任务中取得了最先进的性能。

    

    实时感知需要计划的资源利用。计算规划在实时感知中受精度和延迟两个因素控制。存在可运行时决策（例如选择输入分辨率）会引起权衡，影响给定硬件上的性能，来自固有（内容，例如场景混乱）和外在（系统，例如资料争用）特征。我们提出了 Chanakya，一个学习近似执行框架，它自然地从流式感知范例中衍生出来，以自动学习这些权衡所引起的决策。Chanakya 是通过新颖的奖励训练平衡精度和延迟来隐式地进行训练，而不是近似这两个目标。Chanakya 同时考虑内在和外在背景，以及来自多个视角的预测深度，以学习从图像序列到动态可调整的感知算法流水线的映射。所提出的方法学习了实时感知中的高效决策，并在多个感知任务上取得了最先进的性能。

    Real-time perception requires planned resource utilization. Computational planning in real-time perception is governed by two considerations -- accuracy and latency. There exist run-time decisions (e.g. choice of input resolution) that induce tradeoffs affecting performance on a given hardware, arising from intrinsic (content, e.g. scene clutter) and extrinsic (system, e.g. resource contention) characteristics.  Earlier runtime execution frameworks employed rule-based decision algorithms and operated with a fixed algorithm latency budget to balance these concerns, which is sub-optimal and inflexible. We propose Chanakya, a learned approximate execution framework that naturally derives from the streaming perception paradigm, to automatically learn decisions induced by these tradeoffs instead. Chanakya is trained via novel rewards balancing accuracy and latency implicitly, without approximating either objectives. Chanakya simultaneously considers intrinsic and extrinsic context, and pred
    
[^239]: UCB Bandits在对抗攻击中的近乎最优攻击策略

    Near Optimal Adversarial Attack on UCB Bandits. (arXiv:2008.09312v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.09312](http://arxiv.org/abs/2008.09312)

    本文提出了一种在对抗攻击下的UCB最优拉臂策略，成本为$\sqrt{\log T}$，并且证明了此攻击策略近乎是最优的。

    

    本文考虑了一种随机多臂赌博问题，其中奖励受到对抗性破坏。我们提出了一种新颖的攻击策略，通过操作UCB原则来拉动一些非最优目标臂$T-o(T)$次，累积成本的标度为$\sqrt{\log T}$，其中$T$为回合数。我们还证明了累积攻击成本的第一个下界。我们的下界与我们的上界匹配，除了$\log\log T$因子，表明我们的攻击策略近乎是最优的。

    We consider a stochastic multi-arm bandit problem where rewards are subject to adversarial corruption. We propose a novel attack strategy that manipulates a UCB principle into pulling some non-optimal target arm $T - o(T)$ times with a cumulative cost that scales as $\sqrt{\log T}$, where $T$ is the number of rounds. We also prove the first lower bound on the cumulative attack cost. Our lower bound matches our upper bound up to $\log \log T$ factors, showing our attack to be near optimal.
    
[^240]: 用图神经网络进行图聚类

    Graph Clustering with Graph Neural Networks. (arXiv:2006.16904v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.16904](http://arxiv.org/abs/2006.16904)

    本文探讨了图神经网络在图聚类这一无监督问题上的缺陷，并提出了一种名为DMoN的无监督汇聚方法，可以有效地解决聚类恢复问题。

    

    图神经网络已经在诸如节点分类和链路预测之类的许多图分析任务中取得了最先进的结果。然而，在图上的一些重要无监督问题，比如图聚类，却对GNN的进展更加有抵抗力。我们通过精心设计一组实验，研究了不同信噪比情况下的表征数据对于图结构的影响。为了解决这些方法在聚类方面的性能问题，我们引入了一种无监督汇聚方法：深度模块网络(DMoN)。这种方法受聚类质量的模块度度量的启发，能够处理聚类恢复问题。

    Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. Graph clustering has the same overall goal as node pooling in GNNs - does this mean that GNN pooling methods do a good job at clustering graphs?  Surprisingly, the answer is no - current GNN pooling methods often fail to recover the cluster structure in cases where simple baselines, such as k-means applied on learned representations, work well. We investigate further by carefully designing a set of experiments to study different signal-to-noise scenarios both in graph structure and attribute data. To address these methods' poor performance in clustering, we introduce Deep Modularity Networks (DMoN), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery
    
[^241]: 将树结构输入Transformer进行代码预测

    Code Prediction by Feeding Trees to Transformers. (arXiv:2003.13848v4 [cs.SE] CROSS LISTED)

    [http://arxiv.org/abs/2003.13848](http://arxiv.org/abs/2003.13848)

    本文使用Transformer架构进行代码预测，超越了先前的模型，通过多种方式将代码结构传达给Transformer，进一步提高了准确度。

    

    我们通过将语法树输入Transformer提高了代码预测（下一个标记预测）的准确度，进一步超越了先前的神经和非神经系统。本文提出了多种将代码结构传达给Transformer的方法，并在标准Python数据集上进行了全面的实验评估。

    We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. First, we report that using the recently proposed Transformer architecture even out-of-the-box outperforms previous neural and non-neural systems for code prediction. We then show that by making the Transformer architecture aware of the syntactic structure of code, we further increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of an RNN-based system (similar to Hellendoorn et al. 2018) by 18.3%, the Deep3 system (Raychev et al 2016) by 14.1%, and an adaptation of Code2Seq (Alon et al., 2018) for code prediction by 14.4%.  We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Pytho
    
[^242]: 学习使用距离加权图网络处理不规则探测器几何形状

    Learning representations of irregular particle-detector geometry with distance-weighted graph networks. (arXiv:1902.07987v2 [physics.data-an] CROSS LISTED)

    [http://arxiv.org/abs/1902.07987](http://arxiv.org/abs/1902.07987)

    该论文介绍了使用距离加权图网络处理不规则几何形状探测器的方法，利用其表示学习能力并可以处理事件的稀疏性和任意复杂的探测器几何形状。该算法为处理不规则探测器几何形状的粒子重建提供了一个有趣的替代方案。

    

    我们研究了在粒子重建的背景下使用图网络处理不规则几何形状探测器的方法。由于图网络的表示学习能力，它可以利用全部探测器的细节，同时本地管理事件稀疏性和任意复杂的探测器几何形状。我们介绍了两种距离加权图网络结构，被称为GarNet和GravNet层，并将它们应用于典型的粒子重建任务。新架构的表现在模拟粒子相互作用数据集上进行评估，使用了高度细致的卡仑计数器玩具模型，该模型受到了为高亮度LHC阶段安装在CMS探测器中的端盖卡仑计数器的启发。我们研究了能量沉积的聚类，这是 calorimetric 粒子重建的基础，并提供了与其他方法的定量比较。所提出的算法为现有方法提供了一种有趣的替代方案，为处理不规则探测器几何形状的粒子重建任务提供了有希望的解决方案。

    We explore the use of graph networks to deal with irregular-geometry detectors in the context of particle reconstruction. Thanks to their representation-learning capabilities, graph networks can exploit the full detector granularity, while natively managing the event sparsity and arbitrarily complex detector geometries. We introduce two distance-weighted graph network architectures, dubbed GarNet and GravNet layers, and apply them to a typical particle reconstruction task. The performance of the new architectures is evaluated on a data set of simulated particle interactions on a toy model of a highly granular calorimeter, loosely inspired by the endcap calorimeter to be installed in the CMS detector for the High-Luminosity LHC phase. We study the clustering of energy depositions, which is the basis for calorimetric particle reconstruction, and provide a quantitative comparison to alternative approaches. The proposed algorithms provide an interesting alternative to existing methods, off
    

