# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning.](http://arxiv.org/abs/2310.19958) | 本文针对剪枝联邦学习中的隐私问题进行了调查，推导出了泄露信息的上限，并进行了理论验证和实验验证。 |
| [^2] | [Absolute Policy Optimization.](http://arxiv.org/abs/2310.13230) | 这篇论文提出了绝对策略优化（APO）的方法，通过优化一个新颖的目标函数，在保证性能下界的同时，实现了连续控制任务和Atari游戏中的令人瞩目的结果。 |
| [^3] | [Effects of cavity nonlinearities and linear losses on silicon microring-based reservoir computing.](http://arxiv.org/abs/2310.09433) | 本文研究了微环谐振器中腔体非线性和线性损耗对库仑计算性能的影响，并发现了三个区域，其中一个区域在低输入功率和节点数下具有非常低的时间序列预测误差。这项研究对于改进时间延迟库仑计算的预测性能有着重要的设计和优化指导作用。 |
| [^4] | [Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets.](http://arxiv.org/abs/2310.01438) | 本文提出了一种灵活、可扩展且机器学习准备的多模态肿瘤学数据集(MINDS)框架，用于融合来自不同来源的数据，并提供了探索关系和构建大规模多模态机器学习模型的界面。 |
| [^5] | [Enhancing Sharpness-Aware Optimization Through Variance Suppression.](http://arxiv.org/abs/2309.15639) | 本文通过方差抑制的方法（VaSSO）增强了锐度感知最小化（SAM）的优化算法，提高了深度神经网络的泛化能力，特别适用于模型无关任务和对高水平标签噪声具有鲁棒性的情况。 |
| [^6] | [PrNet: A Neural Network for Correcting Pseudoranges to Improve Positioning with Android Raw GNSS Measurements.](http://arxiv.org/abs/2309.12204) | PrNet是一个用于改善Android原始GNSS测量定位的神经网络，通过校正伪距偏差来提高定位性能，实现了同时处理伪距偏差和噪声的混合管道。 |
| [^7] | [Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?.](http://arxiv.org/abs/2309.01108) | 本研究探讨了使用预先训练的自监督表示对语音障碍者的声学到发音反演任务的影响。实验结果表明，在低资源条件下，经过微调的DeCoAR模型在精细训练方案中相对于健康对照组和患者，分别取得了约1.81\%和约4.56\%的皮尔逊相关系数(CC)的改进。 |
| [^8] | [Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation.](http://arxiv.org/abs/2308.11197) | 该研究提供了使用嵌套交叉验证方法的定量证据，并提出了基于机器学习分析进行功效分析的方法。通过对交叉验证方法、特征和模型维度之间的相互作用进行蒙特卡罗模拟，比较了不同交叉验证方法的统计功效和置信度。同时，确定了获得统计显著结果所需的最小样本容量。 |
| [^9] | [Constructing Custom Thermodynamics Using Deep Learning.](http://arxiv.org/abs/2308.04119) | 本文使用深度学习构建了一个基于广义Onsager原理的平台，可以从微观轨迹的观察中学习任意随机耗散系统的宏观动力学描述。 |
| [^10] | [Unified Model for Image, Video, Audio and Language Tasks.](http://arxiv.org/abs/2307.16184) | UnIVAL是一个统一的模型，可以同时支持图像、视频、音频和语言任务。 |
| [^11] | [Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning.](http://arxiv.org/abs/2307.09619) | Dataset Grouper是一个库，用于创建大规模群组结构化数据集，并克服了内存限制、提供了灵活性，并且与不同的软件框架兼容。实验证明它可以实现比以前更大规模的联邦语言建模模拟。 |
| [^12] | [Next Steps for Human-Centered Generative AI: A Technical Perspective.](http://arxiv.org/abs/2306.15774) | 这项研究从技术角度定义和提出了人类中心生成式人工智能(HGAI)的下一步工作，包括与人类价值观对齐、适应人类的意图表达和增强人类在协作工作流中的能力。这个工作的目标是吸引跨学科研究团队对HGAI的新兴想法进行讨论，并保持未来工作景观的整体连贯性。 |
| [^13] | [MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing.](http://arxiv.org/abs/2306.11758) | MRFI是一个高度可配置的神经网络故障注入工具，用户可以修改独立的故障配置文件进行注入和漏洞分析。 |
| [^14] | [Backdoor Attack with Sparse and Invisible Trigger.](http://arxiv.org/abs/2306.06209) | 本论文提出了一种名为SIBA的稀疏隐形后门攻击方法，解决了现有后门攻击存在的可见或稀疏性不足等问题。 |
| [^15] | [Two Independent Teachers are Better Role Model.](http://arxiv.org/abs/2306.05745) | 提出了一种名为3D-DenseUNet的新深度学习模型，该模型采用自适应全局聚合块和自注意模块，以解决半监督技术中的效率和效果问题，同时通过训练两个独立的训练器处理不同的组织特性，在婴儿脑部分析中取得更好的结果。 |
| [^16] | [Provable convergence guarantees for black-box variational inference.](http://arxiv.org/abs/2306.03638) | 本文提出了一种基于密集高斯变分族的梯度估计器，在此基础上使用近端和投影随机梯度下降，提供了黑盒变分推断收敛于逼真推断问题的第一个严格保证。 |
| [^17] | [End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes.](http://arxiv.org/abs/2305.15930) | 本文提出了第一个可泛化到学习获取函数的神经过程端到端框架，使用强化学习解决了缺乏标签获取数据以及利用代理模型或获取函数的传统Meta-BO方法训练过程中的挑战。 |
| [^18] | [Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions.](http://arxiv.org/abs/2305.05400) | 本研究探讨了使用随机Lp范数失真对图像分类器的训练和测试数据进行增强，并评估模型对不可感知随机失真的稳健性，发现稳健性可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。 |
| [^19] | [FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework.](http://arxiv.org/abs/2305.01658) | FlightBERT++提出了一种非自回归的多时域飞行轨迹预测框架，通过引入时域感知上下文生成器解决了误差累积和低效率的问题。 |
| [^20] | [AutoNeRF: Training Implicit Scene Representations with Autonomous Agents.](http://arxiv.org/abs/2304.11241) | 本文提出了AutoNeRF方法，使用自主体代理收集训练NeRF所需数据，训练NeRF成功。 |
| [^21] | [Scalable Online Learning of Approximate Stackelberg Solutions in Energy Trading Games with Demand Response Aggregators.](http://arxiv.org/abs/2304.02086) | 本文提出了一个Stackelberg博弈理论框架，用于在需求响应（DR）聚合器和中间商之间双向交易能源，解决方案可扩展，且保证满足中间商每日能源的需求。 |
| [^22] | [Diffusion Bridge Mixture Transports, Schr\"odinger Bridge Problems and Generative Modeling.](http://arxiv.org/abs/2304.00917) | 本文提出了一种新的迭代算法IDBM，用于解决动态Schr\"odinger桥问题，该算法能够在每一步有效地耦合目标度量，并在各种应用中表现出竞争力。此外，还讨论了使用扩散过程的时间反演来定义一个近似传输简单分布到目标分布的生成过程的最新进展。 |
| [^23] | [Strategic Trading in Quantitative Markets through Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2303.11959) | 本文将CPPI和TIPP策略集成到多智能体深度确定性策略梯度中，提出了两种特定设计的MARL方法CPPI-MADDPG和TIPP-MADDPG，用于量化市场的战略交易，结果显示这些方法通常优于传统方法。 |
| [^24] | [CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training.](http://arxiv.org/abs/2302.00845) | 该论文提出了一种名为CD-GraB的算法，可以协调分布式示例顺序以加速机器学习训练。CD-GraB展现出线性加速收敛速率并且在基准任务上优于其他基线方法。 |
| [^25] | [FI-ODE: Certifiably Robust Forward Invariance in Neural ODEs.](http://arxiv.org/abs/2210.16940) | 本论文提出了一个通用框架，用于在神经ODE中训练和证明鲁棒前不变性，应用于鲁棒连续控制和图像分类，具有非虚假保证。 |
| [^26] | [Auto-Encoding Adversarial Imitation Learning.](http://arxiv.org/abs/2206.11004) | 自动编码对抗性模仿学习（AEAIL）是一种稳健且可扩展的方法，利用自动编码器的重构误差作为奖励信号来优化策略。在基于状态和基于图像的环境中，AEAIL比现有方法表现更好，并且对于噪声示范专家具有更好的稳健性。 |

# 详细

[^1]: PriPrune: 在剪枝联邦学习中量化和保护隐私

    PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning. (arXiv:2310.19958v1 [cs.LG])

    [http://arxiv.org/abs/2310.19958](http://arxiv.org/abs/2310.19958)

    本文针对剪枝联邦学习中的隐私问题进行了调查，推导出了泄露信息的上限，并进行了理论验证和实验验证。

    

    联邦学习（FL）是一种允许多个客户设备和服务器通过仅交换模型更新而共同训练全局模型的范例，而不需要设备共享他们的局部训练数据的方法。这些设备在通信和计算资源方面往往受到限制，并且可以进一步从模型剪枝中受益 - 这是一种广泛用于减小模型大小和复杂度的范例。直观地说，通过使局部模型更粗糙，剪枝预计在FL环境中也会提供一定的隐私攻击保护。然而，目前尚未对此保护进行正式或实验性的特征化，并且不清楚它是否足以抵御最先进的攻击。在这篇论文中，我们对剪枝FL模型的隐私保障进行了首次调查。我们推导出剪枝FL模型泄露的信息论上界。我们通过理论发现进行了补充和验证

    Federated learning (FL) is a paradigm that allows several client devices and a server to collaboratively train a global model, by exchanging only model updates, without the devices sharing their local training data. These devices are often constrained in terms of communication and computation resources, and can further benefit from model pruning -- a paradigm that is widely used to reduce the size and complexity of models. Intuitively, by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL. However this protection has not been previously characterized, formally or experimentally, and it is unclear if it is sufficient against state-of-the-art attacks.  In this paper, we perform the first investigation of privacy guarantees for model pruning in FL. We derive information-theoretic upper bounds on the amount of information leaked by pruned FL models. We complement and validate these theoretical findings, with compreh
    
[^2]: 绝对策略优化

    Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])

    [http://arxiv.org/abs/2310.13230](http://arxiv.org/abs/2310.13230)

    这篇论文提出了绝对策略优化（APO）的方法，通过优化一个新颖的目标函数，在保证性能下界的同时，实现了连续控制任务和Atari游戏中的令人瞩目的结果。

    

    近年来，基于信任域的在线策略强化学习在解决复杂控制任务和游戏场景方面取得了令人瞩目的结果。然而，这一类别中现有的最先进算法主要强调对预期性能的改进，缺乏对最坏情况下性能结果的控制能力。为了解决这个限制，我们引入了一个新颖的目标函数；通过优化该函数，可以确保近乎总体性能样本的下界（绝对性能）呈现单调改进。考虑到这一具有突破性的理论进展，我们通过一系列的近似对这个理论基础算法进行了改进，得到了一种实用的解决方案称为绝对策略优化（APO）。我们的实验证明了我们的方法在具有挑战性的连续控制基准任务上的有效性，并将其适用性扩展到掌握Atari游戏。我们的发现表明，APO在提高性能的同时也显著改善了最坏情况下的性能结果。

    In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
    
[^3]: 基于硅微环的库仑计算中腔体非线性和线性损耗的影响

    Effects of cavity nonlinearities and linear losses on silicon microring-based reservoir computing. (arXiv:2310.09433v1 [physics.optics])

    [http://arxiv.org/abs/2310.09433](http://arxiv.org/abs/2310.09433)

    本文研究了微环谐振器中腔体非线性和线性损耗对库仑计算性能的影响，并发现了三个区域，其中一个区域在低输入功率和节点数下具有非常低的时间序列预测误差。这项研究对于改进时间延迟库仑计算的预测性能有着重要的设计和优化指导作用。

    

    微环谐振器是时间延迟光子库仑计算的有希望的器件，但是微环谐振器中不同物理效应对库仑计算性能的影响尚未完全理解。我们通过数值分析线性损耗、热光和自由载流子效应松弛时间对时间序列任务 NARMA-10 的预测误差的影响。我们证明了存在三个区域，由输入功率和光源与微环谐振之间的频率失谐定义，这些区域揭示了腔体从线性到非线性状态的转变。其中一个区域在相对较低的输入功率和节点数下提供了非常低的时间序列预测误差，而其他区域要么缺乏非线性，要么变得不稳定。这项研究为微环谐振器的设计和优化其物理特性以提高时间延迟库仑计算的预测性能提供了洞察。

    Microring resonators (MRRs) are promising devices for time-delay photonic reservoir computing, but the impact of the different physical effects taking place in the MRRs on the reservoir computing performance is yet to be fully understood. We numerically analyze the impact of linear losses as well as thermo-optic and free-carrier effects relaxation times on the prediction error of the time-series task NARMA-10. We demonstrate the existence of three regions, defined by the input power and the frequency detuning between the optical source and the microring resonance, that reveal the cavity transition from linear to nonlinear regimes. One of these regions offers very low error in time-series prediction under relatively low input power and number of nodes while the other regions either lack nonlinearity or become unstable. This study provides insight into the design of the MRR and the optimization of its physical properties for improving the prediction performance of time-delay reservoir co
    
[^4]: 构建灵活、可扩展且机器学习准备的多模态肿瘤学数据集

    Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets. (arXiv:2310.01438v1 [cs.LG])

    [http://arxiv.org/abs/2310.01438](http://arxiv.org/abs/2310.01438)

    本文提出了一种灵活、可扩展且机器学习准备的多模态肿瘤学数据集(MINDS)框架，用于融合来自不同来源的数据，并提供了探索关系和构建大规模多模态机器学习模型的界面。

    

    数据采集、存储和处理技术的进步导致了异质医学数据的快速增长。将放射学扫描、组织病理学图像和分子信息与临床数据整合是开发对疾病有全面理解和优化治疗至关重要的。在复杂疾病（如癌症）中，将来自多个来源的数据进行整合的需求更加突出，以实现精准医学和个性化治疗。本研究提出了多模态肿瘤数据系统（MINDS）-一种灵活、可扩展且经济高效的元数据框架，用于将来自公共来源（如癌症研究数据共享库）的异构数据有效地融合到一个相互连接且以患者为中心的框架中。MINDS提供了一个可以探索不同数据类型之间关系并构建大规模多模态机器学习模型的界面。通过协调多模态数据，MINDS旨在实现促进研究创新、精准医学和个性化治疗的目标。

    The advancements in data acquisition, storage, and processing techniques have resulted in the rapid growth of heterogeneous medical data. Integrating radiological scans, histopathology images, and molecular information with clinical data is essential for developing a holistic understanding of the disease and optimizing treatment. The need for integrating data from multiple sources is further pronounced in complex diseases such as cancer for enabling precision medicine and personalized treatments. This work proposes Multimodal Integration of Oncology Data System (MINDS) - a flexible, scalable, and cost-effective metadata framework for efficiently fusing disparate data from public sources such as the Cancer Research Data Commons (CRDC) into an interconnected, patient-centric framework. MINDS offers an interface for exploring relationships across data types and building cohorts for developing large-scale multimodal machine learning models. By harmonizing multimodal data, MINDS aims to pot
    
[^5]: 通过方差抑制增强锐度感知优化

    Enhancing Sharpness-Aware Optimization Through Variance Suppression. (arXiv:2309.15639v1 [cs.LG])

    [http://arxiv.org/abs/2309.15639](http://arxiv.org/abs/2309.15639)

    本文通过方差抑制的方法（VaSSO）增强了锐度感知最小化（SAM）的优化算法，提高了深度神经网络的泛化能力，特别适用于模型无关任务和对高水平标签噪声具有鲁棒性的情况。

    

    锐度感知最小化（SAM）在增强深度神经网络的泛化能力方面有着良好的记录，即使没有大规模的数据增强。SAM借助损失函数的几何特性，通过最小化在邻域内参数对敌对扰动引起的最大损失，寻找“平坦最小值”所在的“平坦山谷”，提高泛化能力。尽管考虑了损失函数的锐度是至关重要的，但这种“过于友好的敌对者”可能会限制泛化的最高水平。本文的新方法通过方差抑制（VaSSO）来稳定敌对者，避免这种友好性。 VaSSO的稳定性可证明，并在模型无关任务中（包括图像分类和机器翻译）相对于SAM有着数值上的改进。此外，实验证实VaSSO赋予SAM对高水平标签噪声的鲁棒性。

    Sharpness-aware minimization (SAM) has well documented merits in enhancing generalization of deep neural networks, even without sizable data augmentation. Embracing the geometry of the loss function, where neighborhoods of 'flat minima' heighten generalization ability, SAM seeks 'flat valleys' by minimizing the maximum loss caused by an adversary perturbing parameters within the neighborhood. Although critical to account for sharpness of the loss function, such an 'over-friendly adversary' can curtail the outmost level of generalization. The novel approach of this contribution fosters stabilization of adversaries through variance suppression (VaSSO) to avoid such friendliness. VaSSO's provable stability safeguards its numerical improvement over SAM in model-agnostic tasks, including image classification and machine translation. In addition, experiments confirm that VaSSO endows SAM with robustness against high levels of label noise.
    
[^6]: PrNet：一个用于改善Android原始GNSS测量定位的神经网络，用于校正伪距偏差

    PrNet: A Neural Network for Correcting Pseudoranges to Improve Positioning with Android Raw GNSS Measurements. (arXiv:2309.12204v1 [cs.LG])

    [http://arxiv.org/abs/2309.12204](http://arxiv.org/abs/2309.12204)

    PrNet是一个用于改善Android原始GNSS测量定位的神经网络，通过校正伪距偏差来提高定位性能，实现了同时处理伪距偏差和噪声的混合管道。

    

    我们提出了一个神经网络来减轻伪距偏差，以提高使用从安卓智能手机收集的数据的定位性能。我们使用一个实用的基于卫星的多层感知器（MLP）表示伪距偏差，其输入是从Android原始全球导航卫星系统（GNSS）测量中得出的与卫星-接收器-环境相关的六个特征。为了监督训练过程，我们仔细计算伪距偏差的目标值，并利用地理真实位置和平滑技术优化了一个包含智能手机时钟偏差估计残差的损失函数。在推理过程中，我们使用基于模型的定位引擎校正神经网络的伪距计算位置。因此，这个混合管道可以处理伪距偏差和噪声。我们在一个开放数据集上评估了该框架，并考虑了四个应用场景来研究指纹定位和交叉跟踪定位。

    We present a neural network for mitigating pseudorange bias to improve localization performance with data collected from Android smartphones. We represent pseudorange bias using a pragmatic satellite-wise Multiple Layer Perceptron (MLP), the inputs of which are six satellite-receiver-context-related features derived from Android raw Global Navigation Satellite System (GNSS) measurements. To supervise the training process, we carefully calculate the target values of pseudorange bias using location ground truth and smoothing techniques and optimize a loss function containing the estimation residuals of smartphone clock bias. During the inference process, we employ model-based localization engines to compute locations with pseudoranges corrected by the neural network. Consequently, this hybrid pipeline can attend to both pseudorange bias and noise. We evaluate the framework on an open dataset and consider four application scenarios for investigating fingerprinting and cross-trace localiza
    
[^7]: 语音障碍者的声学到发音反演: 预先训练的自监督表示是否有利？

    Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?. (arXiv:2309.01108v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.01108](http://arxiv.org/abs/2309.01108)

    本研究探讨了使用预先训练的自监督表示对语音障碍者的声学到发音反演任务的影响。实验结果表明，在低资源条件下，经过微调的DeCoAR模型在精细训练方案中相对于健康对照组和患者，分别取得了约1.81\%和约4.56\%的皮尔逊相关系数(CC)的改进。

    

    声学到发音反演(ACI)涉及从声学空间映射到发音空间。信号处理特征如MFCCs已被广泛应用于ACI任务。对于有语音障碍的患者，由于不准确和不清晰的发音，ACI是具有挑战性的。本研究使用预先训练的自监督学习(SSL)模型中的表示对语音障碍的ACI进行了实验。我们评估了不同预训练特征对这个具有挑战性的低资源ACI任务的影响。此外，我们还将x-vectors与提取的SSL特征相结合，训练了一个BLSTM网络。在已知情况下，我们尝试了三种ACI训练方案（主题特定，聚合和微调）。结果一致表明，DeCoAR在微调方案中，相对于健康对照组和患者，皮尔逊相关系数(CC)的改进幅度分别为约1.81\%和约4.56\%。

    $ $Acoustic-to-articulatory inversion (AAI) involves mapping from the acoustic space to the articulatory space. Signal-processing features like the MFCCs, have been widely used for the AAI task. For subjects with dysarthric speech, AAI is challenging because of an imprecise and indistinct pronunciation. In this work, we perform AAI for dysarthric speech using representations from pre-trained self-supervised learning (SSL) models. We demonstrate the impact of different pre-trained features on this challenging AAI task, at low-resource conditions. In addition, we also condition x-vectors to the extracted SSL features to train a BLSTM network. In the seen case, we experiment with three AAI training schemes (subject-specific, pooled, and fine-tuned). The results, consistent across training schemes, reveal that DeCoAR, in the fine-tuned scheme, achieves a relative improvement of the Pearson Correlation Coefficient (CC) by ${\sim}$1.81\% and ${\sim}$4.56\% for healthy controls and patients, 
    
[^8]: 在语音、语言和听力科学中建立通用的机器学习模型：功效分析和样本容量估计

    Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation. (arXiv:2308.11197v1 [cs.LG])

    [http://arxiv.org/abs/2308.11197](http://arxiv.org/abs/2308.11197)

    该研究提供了使用嵌套交叉验证方法的定量证据，并提出了基于机器学习分析进行功效分析的方法。通过对交叉验证方法、特征和模型维度之间的相互作用进行蒙特卡罗模拟，比较了不同交叉验证方法的统计功效和置信度。同时，确定了获得统计显著结果所需的最小样本容量。

    

    该研究的第一个目的是提供定量证据，以激励研究人员改用更健壮的嵌套交叉验证方法。第二个目的是在研究设计过程中提出基于机器学习分析的功效分析方法和MATLAB代码。通过蒙特卡罗模拟，量化了所采用的交叉验证方法、特征的判别力、特征空间的维度和模型的维度之间的相互作用。基于机器学习模型的统计功效和统计置信度，比较了四种不同的交叉验证方法（单一留出法、10折交叉验证、训练-验证-测试法和嵌套10折交叉验证）。利用零假设和备择假设的分布确定了获得统计显著结果所需的最小样本容量（α=0.05，1-β=0.8）。模型的统计置信度被定义为正确特征的概率。

    This study's first purpose is to provide quantitative evidence that would incentivize researchers to instead use the more robust method of nested cross-validation. The second purpose is to present methods and MATLAB codes for doing power analysis for ML-based analysis during the design of a study. Monte Carlo simulations were used to quantify the interactions between the employed cross-validation method, the discriminative power of features, the dimensionality of the feature space, and the dimensionality of the model. Four different cross-validations (single holdout, 10-fold, train-validation-test, and nested 10-fold) were compared based on the statistical power and statistical confidence of the ML models. Distributions of the null and alternative hypotheses were used to determine the minimum required sample size for obtaining a statistically significant outcome ({\alpha}=0.05, 1-\b{eta}=0.8). Statistical confidence of the model was defined as the probability of correct features being 
    
[^9]: 使用深度学习构建定制热力学

    Constructing Custom Thermodynamics Using Deep Learning. (arXiv:2308.04119v1 [cond-mat.soft])

    [http://arxiv.org/abs/2308.04119](http://arxiv.org/abs/2308.04119)

    本文使用深度学习构建了一个基于广义Onsager原理的平台，可以从微观轨迹的观察中学习任意随机耗散系统的宏观动力学描述。

    

    人工智能的一个令人兴奋的应用是基于先前积累的数据以及已知的物理原理（包括对称性和守恒定律）提供的限制，进行自动科学发现。这样的自动假设创建和验证可以帮助科学家研究复杂的现象，传统的物理直觉可能无法应对。尤其重要的是复杂的动态系统，其时间演变受到变化的外部参数的强烈影响。在本文中，我们开发了一个基于广义Onsager原理的平台，从微观轨迹的观察中直接学习任意随机耗散系统的宏观动力学描述。我们专注于那些微观描述完整不切实际、构建理论宏观模型需要广泛领域知识或试错的系统。我们的机器学习方法通过模拟来解决这个问题。

    One of the most exciting applications of AI is automated scientific discovery based on previously amassed data, coupled with restrictions provided by the known physical principles, including symmetries and conservation laws. Such automated hypothesis creation and verification can assist scientists in studying complex phenomena, where traditional physical intuition may fail. Of particular importance are complex dynamic systems where their time evolution is strongly influenced by varying external parameters. In this paper we develop a platform based on a generalised Onsager principle to learn macroscopic dynamical descriptions of arbitrary stochastic dissipative systems directly from observations of their microscopic trajectories. We focus on systems whose complexity and sheer sizes render complete microscopic description impractical, and constructing theoretical macroscopic models requires extensive domain knowledge or trial-and-error. Our machine learning approach addresses this by sim
    
[^10]: 统一的图像、视频、音频和语言任务模型

    Unified Model for Image, Video, Audio and Language Tasks. (arXiv:2307.16184v1 [cs.CV])

    [http://arxiv.org/abs/2307.16184](http://arxiv.org/abs/2307.16184)

    UnIVAL是一个统一的模型，可以同时支持图像、视频、音频和语言任务。

    

    大型语言模型（LLMs）使得建立通用代理变得不再是幻想。构建这种通用模型的一个关键难题是任务和模态的多样性和异质性。统一解决方案是一个有希望的解决方案，可以在一个统一的框架内支持多样的任务和模态。虽然一些大型模型（例如Flameigno）经过大规模数据集训练，可以支持超过两个模态，但目前小到中型的统一模型仍然局限于两个模态，通常是图像-文本或视频-文本。我们要提出的问题是：是否可能构建一个高效支持所有模态的统一模型？为了回答这个问题，我们提出了UnIVAL，这是对这个雄心勃勃目标迈出的一步。UnIVAL模型拥有约0.25亿个参数，不依赖于复杂的数据集大小或数十亿参数的模型，将文本、图像、视频和音频统一到一个模型中。

    Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model 
    
[^11]: 面向联邦基础模型的可扩展数据集流水线：用于群组结构化学习

    Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning. (arXiv:2307.09619v1 [cs.LG])

    [http://arxiv.org/abs/2307.09619](http://arxiv.org/abs/2307.09619)

    Dataset Grouper是一个库，用于创建大规模群组结构化数据集，并克服了内存限制、提供了灵活性，并且与不同的软件框架兼容。实验证明它可以实现比以前更大规模的联邦语言建模模拟。

    

    我们引入了一个名为Dataset Grouper的库，用于创建大规模的群组结构化（例如联邦）数据集，实现基础模型规模的联邦学习模拟。该库允许根据用户指定的分区创建现有数据集的群组结构化版本，并直接导致各种有用的异构数据集，可以插入现有的软件框架中。Dataset Grouper具有三个关键优势。首先，它能够适应即使单个群组的数据集也太大无法放入内存的情况。其次，它提供了灵活性，既可以选择基础（非分区）数据集，也可以定义分区。最后，它与框架无关。我们通过实验证明，Dataset Grouper允许在比先前工作中大几个数量级的数据集上进行大规模联邦语言建模模拟。我们的实验结果表明，像FedAvg这样的算法更像元学习方法而不是经验学习方法。

    We introduce a library, Dataset Grouper, to create large-scale group-structured (e.g., federated) datasets, enabling federated learning simulation at the scale of foundation models. This library allows the creation of group-structured versions of existing datasets based on user-specified partitions, and directly leads to a variety of useful heterogeneous datasets that can be plugged into existing software frameworks. Dataset Grouper offers three key advantages. First, it scales to settings where even a single group's dataset is too large to fit in memory. Second, it provides flexibility, both in choosing the base (non-partitioned) dataset and in defining partitions. Finally, it is framework-agnostic. We empirically demonstrate that Dataset Grouper allows for large-scale federated language modeling simulations on datasets that are orders of magnitude larger than in previous work. Our experimental results show that algorithms like FedAvg operate more as meta-learning methods than as empi
    
[^12]: 人类中心生成式人工智能的下一步：技术视角

    Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v1 [cs.HC])

    [http://arxiv.org/abs/2306.15774](http://arxiv.org/abs/2306.15774)

    这项研究从技术角度定义和提出了人类中心生成式人工智能(HGAI)的下一步工作，包括与人类价值观对齐、适应人类的意图表达和增强人类在协作工作流中的能力。这个工作的目标是吸引跨学科研究团队对HGAI的新兴想法进行讨论，并保持未来工作景观的整体连贯性。

    

    通过反复跨学科讨论，我们从技术角度为人类中心生成式人工智能(HGAI)定义和提出了下一步的工作。我们贡献了一个路线图，概述了生成式人工智能在三个层面上的未来方向：与人类价值观对齐；适应人类的意图表达；增强人类在协作工作流中的能力。该路线图旨在吸引跨学科研究团队对HGAI的新兴想法进行全面的讨论，同时保持未来工作景观的整体连贯性。

    Through iterative, cross-disciplinary discussions, we define and propose next-steps for Human-centered Generative AI (HGAI) from a technical perspective. We contribute a roadmap that lays out future directions of Generative AI spanning three levels: Aligning with human values; Accommodating humans' expression of intents; and Augmenting humans' abilities in a collaborative workflow. This roadmap intends to draw interdisciplinary research teams to a comprehensive list of emergent ideas in HGAI, identifying their interested topics while maintaining a coherent big picture of the future work landscape.
    
[^13]: MRFI：神经网络处理的开源多分辨率故障注入框架

    MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing. (arXiv:2306.11758v1 [cs.LG])

    [http://arxiv.org/abs/2306.11758](http://arxiv.org/abs/2306.11758)

    MRFI是一个高度可配置的神经网络故障注入工具，用户可以修改独立的故障配置文件进行注入和漏洞分析。

    

    为了确保即使在不可靠的硬件上也能进行有弹性的神经网络处理，通常需要在深度神经网络模型部署之前进行各种硬件故障的全面可靠性分析，并且需要高效的错误注入工具。然而，大多数现有的故障注入工具仍然局限于对神经元的基本故障注入，并未提供细粒度漏洞分析能力。此外，许多故障注入工具仍需要更改神经网络模型并使故障注入与正常神经网络处理紧密耦合，这进一步增加了故障注入工具的使用难度并减慢了故障模拟。在这项工作中，我们提出了一个高度可配置的深度神经网络多分辨率故障注入工具MRFI。它使用户能够修改独立的故障配置文件，而不是修改神经网络模型进行故障注入和漏洞分析。

    To ensure resilient neural network processing on even unreliable hardware, comprehensive reliability analysis against various hardware faults is generally required before the deep neural network models are deployed, and efficient error injection tools are highly demanded. However, most existing fault injection tools remain rather limited to basic fault injection to neurons and fail to provide fine-grained vulnerability analysis capability. In addition, many of the fault injection tools still need to change the neural network models and make the fault injection closely coupled with normal neural network processing, which further complicates the use of the fault injection tools and slows down the fault simulation. In this work, we propose MRFI, a highly configurable multi-resolution fault injection tool for deep neural networks. It enables users to modify an independent fault configuration file rather than neural network models for the fault injection and vulnerability analysis. Particul
    
[^14]: 稀疏隐形触发器的后门攻击

    Backdoor Attack with Sparse and Invisible Trigger. (arXiv:2306.06209v1 [cs.CV])

    [http://arxiv.org/abs/2306.06209](http://arxiv.org/abs/2306.06209)

    本论文提出了一种名为SIBA的稀疏隐形后门攻击方法，解决了现有后门攻击存在的可见或稀疏性不足等问题。

    

    深度神经网络（DNN）容易受到后门攻击，攻击者在小部分训练数据中进行操作，使得受害的模型对正常样本有正确的预测，但是将带有触发器的样本归类为目标分类。后门攻击是一种新兴而又危险的训练阶段威胁，对DNN应用带来严重风险。本文研究了现有后门攻击的触发器模式，揭示了它们是否可见或稀疏性不足，因此不够隐秘。最重要的是，在设计有效的稀疏隐形后门攻击时，不能简单地将现有方法组合起来。为了解决这个问题，我们将触发器生成形式化为一个具有稀疏性和隐秘性约束的双层优化问题，并提出了一种有效的方法来解决它，称为稀疏隐形后门攻击（SIBA）。我们在基准数据集上进行了广泛的实验。

    Deep neural networks (DNNs) are vulnerable to backdoor attacks, where the adversary manipulates a small portion of training data such that the victim model predicts normally on the benign samples but classifies the triggered samples as the target class. The backdoor attack is an emerging yet threatening training-phase threat, leading to serious risks in DNN-based applications. In this paper, we revisit the trigger patterns of existing backdoor attacks. We reveal that they are either visible or not sparse and therefore are not stealthy enough. More importantly, it is not feasible to simply combine existing methods to design an effective sparse and invisible backdoor attack. To address this problem, we formulate the trigger generation as a bi-level optimization problem with sparsity and invisibility constraints and propose an effective method to solve it. The proposed method is dubbed sparse and invisible backdoor attack (SIBA). We conduct extensive experiments on benchmark datasets unde
    
[^15]: 两个独立训练器是更好的模范

    Two Independent Teachers are Better Role Model. (arXiv:2306.05745v1 [eess.IV])

    [http://arxiv.org/abs/2306.05745](http://arxiv.org/abs/2306.05745)

    提出了一种名为3D-DenseUNet的新深度学习模型，该模型采用自适应全局聚合块和自注意模块，以解决半监督技术中的效率和效果问题，同时通过训练两个独立的训练器处理不同的组织特性，在婴儿脑部分析中取得更好的结果。

    

    最近深度学习模型在婴儿脑部分析中引起了巨大的关注。这些模型表现出最先进的性能，例如半监督技术（例如，时间集成，平均教师）。然而，这些模型依赖于编码器-解码器结构，用堆叠的局部运算符来收集远程信息，而局部运算符限制了效率和效果。此外，MRI数据包含不同的组织特性（TPs），例如T1和T2。这些模型的一个主要限制是，它们将两种数据都作为输入用于分割过程，即模型一次训练于数据集，推断过程中需要大量的计算和存储空间。在本文中，我们通过设计一个名为3D-DenseUNet的新深度学习模型来解决上述限制，其作为自适应全局聚合块在下采样中工作以解决空间信息丢失的问题。自注意模块将下采样层与上采样层连接在一起，以收集远程信息。此外，我们提出的模型通过训练两个独立的训练器处理不同的组织特性，并取得比当前最先进模型更好的结果。

    Recent deep learning models have attracted substantial attention in infant brain analysis. These models have performed state-of-the-art performance, such as semi-supervised techniques (e.g., Temporal Ensembling, mean teacher). However, these models depend on an encoder-decoder structure with stacked local operators to gather long-range information, and the local operators limit the efficiency and effectiveness. Besides, the $MRI$ data contain different tissue properties ($TPs$) such as $T1$ and $T2$. One major limitation of these models is that they use both data as inputs to the segment process, i.e., the models are trained on the dataset once, and it requires much computational and memory requirements during inference. In this work, we address the above limitations by designing a new deep-learning model, called 3D-DenseUNet, which works as adaptable global aggregation blocks in down-sampling to solve the issue of spatial information loss. The self-attention module connects the down-s
    
[^16]: 黑盒变分推断的收敛性保证

    Provable convergence guarantees for black-box variational inference. (arXiv:2306.03638v1 [cs.LG])

    [http://arxiv.org/abs/2306.03638](http://arxiv.org/abs/2306.03638)

    本文提出了一种基于密集高斯变分族的梯度估计器，在此基础上使用近端和投影随机梯度下降，提供了黑盒变分推断收敛于逼真推断问题的第一个严格保证。

    

    尽管黑盒变分推断被广泛应用，但没有证明其随机优化成功的证明。我们提出这是现有随机优化证明中的理论差距，即具有异常噪声边界和复合非平滑目标的梯度估计器的挑战。对于密集的高斯变分族，我们观察到现有的基于再参数化的梯度估计器满足二次噪声界，并为使用该界限的近端和投影随机梯度下降提供新的收敛保证。这提供了第一个黑盒变分推断收敛于逼真推断问题的严格保证。

    While black-box variational inference is widely used, there is no proof that its stochastic optimization succeeds. We suggest this is due to a theoretical gap in existing stochastic optimization proofs-namely the challenge of gradient estimators with unusual noise bounds, and a composite non-smooth objective. For dense Gaussian variational families, we observe that existing gradient estimators based on reparameterization satisfy a quadratic noise bound and give novel convergence guarantees for proximal and projected stochastic gradient descent using this bound. This provides the first rigorous guarantee that black-box variational inference converges for realistic inference problems.
    
[^17]: 基于Transformer神经过程的端到端Meta-Bayesian优化

    End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes. (arXiv:2305.15930v1 [cs.LG])

    [http://arxiv.org/abs/2305.15930](http://arxiv.org/abs/2305.15930)

    本文提出了第一个可泛化到学习获取函数的神经过程端到端框架，使用强化学习解决了缺乏标签获取数据以及利用代理模型或获取函数的传统Meta-BO方法训练过程中的挑战。

    

    元贝叶斯优化（Meta-Bayesian optimization，Meta-BO）通过利用相关任务的数据来提高贝叶斯优化的样本效率。尽管之前的方法已经成功地独立元学习过代理模型或获取函数，但是同时训练这两个组件仍然是一个挑战。本文提出了第一个端到端可微分的Meta-BO框架，通过Transformer体系结构将神经过程泛化到学习获取函数。我们使用强化学习（RL）使这种端到端框架具有处理缺乏标签获取数据的能力。

    Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of Bayesian optimisation by leveraging data from related tasks. While previous methods successfully meta-learn either a surrogate model or an acquisition function independently, joint training of both components remains an open challenge. This paper proposes the first end-to-end differentiable meta-BO framework that generalises neural processes to learn acquisition functions via transformer architectures. We enable this end-to-end framework with reinforcement learning (RL) to tackle the lack of labelled acquisition data. Early on, we notice that training transformer-based neural processes from scratch with RL is challenging due to insufficient supervision, especially when rewards are sparse. We formalise this claim with a combinatorial analysis showing that the widely used notion of regret as a reward signal exhibits a logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we augment the RL 
    
[^18]: 使用随机Lp范数失真探究图像分类器的腐败稳健性

    Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v1 [cs.LG])

    [http://arxiv.org/abs/2305.05400](http://arxiv.org/abs/2305.05400)

    本研究探讨了使用随机Lp范数失真对图像分类器的训练和测试数据进行增强，并评估模型对不可感知随机失真的稳健性，发现稳健性可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。

    

    稳健性是机器学习分类器实现安全和可靠的基本属性。在对图像分类模型的对抗稳健性和形式稳健性验证领域中，稳健性通常被定义为在Lp范数距离内对所有输入变化的稳定性。然而，对随机失真的稳健性通常通过在现实世界中观察到的变化来改进和评估，而很少考虑数学定义的Lp范数失真。本研究探讨了使用随机Lp范数失真来增强图像分类器的训练和测试数据。我们借鉴了对抗稳健性领域的方法来评估模型对不可感知随机失真的稳健性。我们实证和理论上研究了在不同Lp范数之间稳健性是否可转移，并得出结论，哪些Lp范数的失真应该用来训练和评估模型。我们发现训练数据增强可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。

    Robustness is a fundamental property of machine learning classifiers to achieve safety and reliability. In the fields of adversarial robustness and formal robustness verification of image classification models, robustness is commonly defined as the stability to all input variations within an Lp-norm distance. However, robustness to random corruptions is usually improved and evaluated using variations observed in the real-world, while mathematically defined Lp-norm corruptions are rarely considered. This study investigates the use of random Lp-norm corruptions to augment the training and test data of image classifiers. We adapt an approach from the field of adversarial robustness to assess the model robustness to imperceptible random corruptions. We empirically and theoretically investigate whether robustness is transferable across different Lp-norms and derive conclusions on which Lp-norm corruptions a model should be trained and evaluated on. We find that training data augmentation wi
    
[^19]: FlightBERT++：一种非自回归多时域飞行轨迹预测框架

    FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework. (arXiv:2305.01658v1 [cs.LG])

    [http://arxiv.org/abs/2305.01658](http://arxiv.org/abs/2305.01658)

    FlightBERT++提出了一种非自回归的多时域飞行轨迹预测框架，通过引入时域感知上下文生成器解决了误差累积和低效率的问题。

    

    飞行轨迹预测是空中交通管制中的重要任务，可以帮助空管员更安全高效地管理空域。现有方法通常采用自回归方式执行多时域飞行轨迹预测任务，容易出现误差累积和低效率问题。本文提出了一种新的框架，称为FlightBERT++，以i）直接以非自回归方式预测多时域飞行轨迹，和ii）改善FlightBERT框架中二进制编码（BE）表示的限制。具体而言，所提出的框架通过通用的编码器-解码器架构实现，其中编码器从历史观测中学习时空模式，而解码器预测未来时间步的飞行状态。与传统架构相比，额外的时域感知上下文生成器（HACG）专门设计考虑先前的时域。

    Flight Trajectory Prediction (FTP) is an essential task in Air Traffic Control (ATC), which can assist air traffic controllers to manage airspace more safely and efficiently. Existing approaches generally perform multi-horizon FTP tasks in an autoregressive manner, which is prone to suffer from error accumulation and low-efficiency problems. In this paper, a novel framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight trajectories directly in a non-autoregressive way, and ii) improved the limitation of the binary encoding (BE) representation in the FlightBERT framework. Specifically, the proposed framework is implemented by a generalized Encoder-Decoder architecture, in which the encoder learns the temporal-spatial patterns from historical observations and the decoder predicts the flight status for the future time steps. Compared to conventional architecture, an extra horizon-aware contexts generator (HACG) is dedicatedly designed to consider the prior horizon 
    
[^20]: AutoNeRF: 自主代理训练隐式场景表示

    AutoNeRF: Training Implicit Scene Representations with Autonomous Agents. (arXiv:2304.11241v1 [cs.CV])

    [http://arxiv.org/abs/2304.11241](http://arxiv.org/abs/2304.11241)

    本文提出了AutoNeRF方法，使用自主体代理收集训练NeRF所需数据，训练NeRF成功。

    

    隐式表示，如神经辐射场（NeRF），已被证明在新视角综合方面非常有效。然而，这些模型通常需要人工收集数据并进行细致的处理。在本文中，我们提出了AutoNeRF，这是一种使用自主体代理收集训练NeRF所需数据的方法。我们的方法允许代理有效地探索未知环境并使用经验自主地构建相应的隐式地图表示。我们比较了不同的探索策略，包括手工设计的基于前沿的探索和由经过训练的高级规划器和经典的低级路径追踪器组成的模块化方法的影响。我们使用针对这个问题量身定制的不同奖励函数来训练这些模型，并在四个不同的下游任务上评估学习表示的质量：经典视角渲染、地图重建、规划和姿态微调。实证结果表明，使用自主代理AutoNeRF可以成功地训练NeRF。

    Implicit representations such as Neural Radiance Fields (NeRF) have been shown to be very effective at novel view synthesis. However, these models typically require manual and careful human data collection for training. In this paper, we present AutoNeRF, a method to collect data required to train NeRFs using autonomous embodied agents. Our method allows an agent to explore an unseen environment efficiently and use the experience to build an implicit map representation autonomously. We compare the impact of different exploration strategies including handcrafted frontier-based exploration and modular approaches composed of trained high-level planners and classical low-level path followers. We train these models with different reward functions tailored to this problem and evaluate the quality of the learned representations on four different downstream tasks: classical viewpoint rendering, map reconstruction, planning, and pose refinement. Empirical results show that NeRFs can be trained 
    
[^21]: 可扩展的在线学习近似Stackelberg解决方案在带有需求响应聚合器的能源交易中的应用

    Scalable Online Learning of Approximate Stackelberg Solutions in Energy Trading Games with Demand Response Aggregators. (arXiv:2304.02086v1 [cs.LG])

    [http://arxiv.org/abs/2304.02086](http://arxiv.org/abs/2304.02086)

    本文提出了一个Stackelberg博弈理论框架，用于在需求响应（DR）聚合器和中间商之间双向交易能源，解决方案可扩展，且保证满足中间商每日能源的需求。

    

    本文提出了一个Stackelberg博弈理论框架，用于在需求响应（DR）聚合器和中间商之间双向交易能源。该框架允许灵活的能源套利和额外的货币奖励，同时确保满足中间商所需的每日能源需求。然后，提出了一种可扩展（随着中间商数量增加）的方法，基于在线采样和学习中间商的累积最佳响应，寻找近似的均衡解。此外，还提供了有关近似均衡解质量的界限。最后，利用来自加利福尼亚日前能源市场和加州大学戴维斯分校建筑能源需求的真实数据，展示了所提出的框架和在线可扩展解决方案的功效。

    In this work, a Stackelberg game theoretic framework is proposed for trading energy bidirectionally between the demand-response (DR) aggregator and the prosumers. This formulation allows for flexible energy arbitrage and additional monetary rewards while ensuring that the prosumers' desired daily energy demand is met. Then, a scalable (with the number of prosumers) approach is proposed to find approximate equilibria based on online sampling and learning of the prosumers' cumulative best response. Moreover, bounds are provided on the quality of the approximate equilibrium solution. Last, real-world data from the California day-ahead energy market and the University of California at Davis building energy demands are utilized to demonstrate the efficacy of the proposed framework and the online scalable solution.
    
[^22]: 扩散桥混合传输、薛定谔桥问题和生成建模

    Diffusion Bridge Mixture Transports, Schr\"odinger Bridge Problems and Generative Modeling. (arXiv:2304.00917v1 [stat.ML])

    [http://arxiv.org/abs/2304.00917](http://arxiv.org/abs/2304.00917)

    本文提出了一种新的迭代算法IDBM，用于解决动态Schr\"odinger桥问题，该算法能够在每一步有效地耦合目标度量，并在各种应用中表现出竞争力。此外，还讨论了使用扩散过程的时间反演来定义一个近似传输简单分布到目标分布的生成过程的最新进展。

    

    动态薛定谔桥问题寻求定义在两个目标概率分布之间的传输的随机过程，同时最优地满足最接近参考过程的Kullback-Leibler散度的准则。我们提出了一种新的基于采样的迭代算法，即迭代扩散桥混合传输（IDBM），旨在解决动态薛定谔桥问题。IDBM过程表现出在每一步实现目标度量之间的有效耦合的有吸引力的属性。我们进行了IDBM过程的初始理论研究，建立了其收敛性质。理论发现通过许多数值实验证明了IDBM过程在各种应用中出色的性能。生成建模方面的最新进展使用扩散过程的时间反演来定义一个近似传输简单分布到目标分布的生成过程。本文提出了一种新的算法，称为迭代扩散桥混合传输（IDBM），用于解决动态薛定谔桥问题。IDBM在每一步实现目标度量之间的有效耦合，并且在各种应用中表现出良好的性能。我们的理论研究证明了IDBM算法的收敛性质。通过许多数值实验进一步说明了所提出的算法的有效性。此外，还讨论了生成建模方面的最新进展，它使用扩散过程的时间反演来近似目标分布。

    The dynamic Schr\"odinger bridge problem seeks a stochastic process that defines a transport between two target probability measures, while optimally satisfying the criteria of being closest, in terms of Kullback-Leibler divergence, to a reference process.  We propose a novel sampling-based iterative algorithm, the iterated diffusion bridge mixture transport (IDBM), aimed at solving the dynamic Schr\"odinger bridge problem. The IDBM procedure exhibits the attractive property of realizing a valid coupling between the target measures at each step. We perform an initial theoretical investigation of the IDBM procedure, establishing its convergence properties. The theoretical findings are complemented by numerous numerical experiments illustrating the competitive performance of the IDBM procedure across various applications.  Recent advancements in generative modeling employ the time-reversal of a diffusion process to define a generative process that approximately transports a simple distri
    
[^23]: 多智能体强化学习在量化市场中的战略交易研究

    Strategic Trading in Quantitative Markets through Multi-Agent Reinforcement Learning. (arXiv:2303.11959v1 [q-fin.TR])

    [http://arxiv.org/abs/2303.11959](http://arxiv.org/abs/2303.11959)

    本文将CPPI和TIPP策略集成到多智能体深度确定性策略梯度中，提出了两种特定设计的MARL方法CPPI-MADDPG和TIPP-MADDPG，用于量化市场的战略交易，结果显示这些方法通常优于传统方法。

    

    在量化市场中，由于市场动态快速变化和大量的不确定性，如何采取适当的行动利润仍然是一个具有挑战性的问题。强化学习作为一种面向奖励的最优控制方法，在这种复杂的金融场景中已成为解决策略决策问题的有希望的方法。本文将两种先前的金融交易策略（恒定比例组合保险（CPPI）和时间不变组合保护（TIPP））集成到多智能体深度确定性策略梯度（MADDPG）中，并提出了两种特别设计的多智能体强化学习（MARL）方法：CPPI-MADDPG和TIPP-MADDPG研究量化市场中的战略交易。之后，我们选择了实际金融市场上的100种不同股票来测试这些特别提出的方法。实验结果表明，CPPI-MADDPG和TIPP-MADDPG方法通常优于传统方法。

    Due to the rapid dynamics and a mass of uncertainties in the quantitative markets, the issue of how to take appropriate actions to make profits in stock trading remains a challenging one. Reinforcement learning (RL), as a reward-oriented approach for optimal control, has emerged as a promising method to tackle this strategic decision-making problem in such a complex financial scenario. In this paper, we integrated two prior financial trading strategies named constant proportion portfolio insurance (CPPI) and time-invariant portfolio protection (TIPP) into multi-agent deep deterministic policy gradient (MADDPG) and proposed two specifically designed multi-agent RL (MARL) methods: CPPI-MADDPG and TIPP-MADDPG for investigating strategic trading in quantitative markets. Afterward, we selected 100 different shares in the real financial market to test these specifically proposed approaches. The experiment results show that CPPI-MADDPG and TIPP-MADDPG approaches generally outperform the conve
    
[^24]: CD-GraB：协调分布式示例顺序以证明加速训练

    CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training. (arXiv:2302.00845v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00845](http://arxiv.org/abs/2302.00845)

    该论文提出了一种名为CD-GraB的算法，可以协调分布式示例顺序以加速机器学习训练。CD-GraB展现出线性加速收敛速率并且在基准任务上优于其他基线方法。

    

    最近有关在线梯度平衡（GraB）的研究表明，存在基于置换的示例排序可以保证优于随机重排（RR）。而RR会任意排列训练示例，GraB利用先前时期的陈旧梯度对示例进行排序--实现比RR更快的收敛速率。但是，GraB在设计上存在限制：虽然它展示了在集中数据上扩展训练的出色能力，但并不自然地扩展到现代分布式ML工作负载。因此，我们提出了协调分布式GraB（CD-GraB），它利用先前关于内核稀疏化工作的洞察力，将置换排序的可证明更快的优势转化为分布式设置。CD-GraB具有可忽略的开销，在中央集权GraB上具有线性加速收敛速率的性能，并在各种基准任务上经验性地优于分布式RR等基线方法。

    Recent research on online Gradient Balancing (GraB) has revealed that there exist permutation-based example orderings that are guaranteed to outperform random reshuffling (RR). Whereas RR arbitrarily permutes training examples, GraB leverages stale gradients from prior epochs to order examples -- achieving a provably faster convergence rate than RR. However, GraB is limited by design: While it demonstrates an impressive ability to scale-up training on centralized data, it does not naturally extend to modern distributed ML workloads. We therefore propose Coordinated Distributed GraB (CD-GraB), which uses insights from prior work on kernel thinning to translate the benefits of provably faster permutation-based example ordering to distributed settings. With negligible overhead, CD-GraB exhibits a linear speedup in convergence rate over centralized GraB and outperforms baselines empirically, including distributed RR, on a variety of benchmark tasks.
    
[^25]: FI-ODE: 神经ODE中的可证明鲁棒前不变性

    FI-ODE: Certifiably Robust Forward Invariance in Neural ODEs. (arXiv:2210.16940v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16940](http://arxiv.org/abs/2210.16940)

    本论文提出了一个通用框架，用于在神经ODE中训练和证明鲁棒前不变性，应用于鲁棒连续控制和图像分类，具有非虚假保证。

    

    前不变性是控制理论中长期研究的性质，用于证明动态系统在所有时间内保持在一些预定状态集合内，并且具有鲁棒性保证（例如，在扰动下保持证书有效）。我们提出了一个通用框架，用于训练和可证明证实神经ODE中的鲁棒前不变性。我们在两个场景中应用了这个框架：鲁棒连续控制中的可证明安全性，以及图像分类中的可证明的对抗鲁棒性。据我们所知，这是第一个具有非虚假保证的训练NODE策略的实例。

    Forward invariance is a long-studied property in control theory that is used to certify that a dynamical system stays within some pre-specified set of states for all time, and also admits robustness guarantees (e.g., the certificate holds under perturbations). We propose a general framework for training and provably certifying robust forward invariance in Neural ODEs. We apply this framework in two settings: certified safety in robust continuous control, and certified adversarial robustness for image classification. To our knowledge, this is the first instance of training NODE policies with such non-vacuous certified guarantees.
    
[^26]: 自动编码对抗性模仿学习

    Auto-Encoding Adversarial Imitation Learning. (arXiv:2206.11004v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.11004](http://arxiv.org/abs/2206.11004)

    自动编码对抗性模仿学习（AEAIL）是一种稳健且可扩展的方法，利用自动编码器的重构误差作为奖励信号来优化策略。在基于状态和基于图像的环境中，AEAIL比现有方法表现更好，并且对于噪声示范专家具有更好的稳健性。

    

    强化学习（RL）提供了一个强大的决策框架，但在实践中，其应用通常需要精心设计的奖励函数。对抗性模仿学习（AIL）揭示了在没有来自环境的奖励信号的情况下自动获取策略的方法。在这项工作中，我们提出了一种稳健且可扩展的自动编码对抗性模仿学习（AEAIL）框架。为了从示范中推导出专家策略，AEAIL利用自动编码器的重构误差作为奖励信号，这比之前基于鉴别器的方法提供了更多用于优化策略的信息。随后，我们使用所得到的目标函数训练自动编码器和智能体策略。实验证明，我们的AEAIL在基于状态和基于图像的环境中表现优于现有方法。更重要的是，当示范专家具有噪声时，AEAIL表现出更好的稳健性。

    Reinforcement learning (RL) provides a powerful framework for decision-making, but its application in practice often requires a carefully designed reward function. Adversarial Imitation Learning (AIL) sheds light on automatic policy acquisition without access to the reward signal from the environment. In this work, we propose Auto-Encoding Adversarial Imitation Learning (AEAIL), a robust and scalable AIL framework. To induce expert policies from demonstrations, AEAIL utilizes the reconstruction error of an auto-encoder as a reward signal, which provides more information for optimizing policies than the prior discriminator-based ones. Subsequently, we use the derived objective functions to train the auto-encoder and the agent policy. Experiments show that our AEAIL performs superior compared to state-of-the-art methods on both state and image based environments. More importantly, AEAIL shows much better robustness when the expert demonstrations are noisy.
    

