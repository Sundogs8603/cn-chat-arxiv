# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Personalized human mobility prediction for HuMob challenge.](http://arxiv.org/abs/2310.12900) | 该论文介绍了一种个性化的人类移动预测方法，基于个体数据而不是整体数据进行预测，并成功运用于HuMob Challenge竞赛中。采用了特征设计和机器学习模型SVR，通过离线评估和特征选择和参数调整，使预测准确性得到验证。 |
| [^2] | [Blind quantum machine learning with quantum bipartite correlator.](http://arxiv.org/abs/2310.12893) | 该论文介绍了基于量子二分关联器算法的盲量子机器学习协议，提出了具有降低通信开销和保护数据隐私的特点，并引入了低计算开销、不需要复杂加密技术的隐私保护机制。论文通过复杂性和隐私分析验证了协议的有效性，为发展分布式量子计算和隐私感知机器学习应用提供了新的可能性。 |
| [^3] | [Fine-Tuning Generative Models as an Inference Method for Robotic Tasks.](http://arxiv.org/abs/2310.12862) | 本论文提出了一种将生成模型微调应用于机器人任务推理的方法。通过利用现代GPU加速，将神经网络模型的样本生成快速适应到机器人任务观测中。该方法适用于各种深度生成模型和机器人环境，通过将模型迅速微调以适应与观测证据相符的生成样本来实现，使用交叉熵方法进行优化。并且在物体形状推理、逆向运动学计算和点云补全等任务中展示了可行性。 |
| [^4] | [Audio Editing with Non-Rigid Text Prompts.](http://arxiv.org/abs/2310.12858) | 本文研究了使用非刚性文本编辑进行音频编辑的方法，并展示了其在保持输入音频一致性方面的优势。 |
| [^5] | [Model-agnostic variable importance for predictive uncertainty: an entropy-based approach.](http://arxiv.org/abs/2310.12842) | 本文提出了一种基于熵的方法，通过扩展现有的解释性方法，可以理解不确定性感知模型中的预测来源和置信度，并利用改编后的特征重要性、部分依赖图和个体条件期望图等方法来测量特征对预测分布的熵和基于真实标签的对数似然的影响。 |
| [^6] | [Knowledge-Augmented Language Model Verification.](http://arxiv.org/abs/2310.12836) | 提出了一种知识增强的语言模型验证方法，通过一个独立的验证器来检测模型输出和知识的错误，并通过检索新知识或修正摘要来纠正错误。 |
| [^7] | [AgentTuning: Enabling Generalized Agent Abilities for LLMs.](http://arxiv.org/abs/2310.12823) | 本论文提出了AgentTuning，一种简单而通用的方法，可提升LLMs的代理能力，同时保持其通用能力。通过构建AgentInstruct数据集，并采用一种混合训练方法，作者成功地实现了提高LLMs代理能力的目标。 |
| [^8] | [Generating collective counterfactual explanations in score-based classification via mathematical optimization.](http://arxiv.org/abs/2310.12822) | 通过数学优化，本文引入了集体反事实解释，为高风险决策场景中的机器学习模型提供了解释工具。与传统的单实例解释不同，该方法针对一组实例提供解释，并通过最小化扰动的总成本来提供最优解释。 |
| [^9] | [Hybrid Search for Efficient Planning with Completeness Guarantees.](http://arxiv.org/abs/2310.12819) | 本论文提出了一种混合搜索方法，用于在离散动作空间中进行高效规划，并保证了完整性。通过在高级搜索中添加低级动作，该方法既具有高级搜索的实际效率，又具有低级搜索的完整性。 |
| [^10] | [Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models.](http://arxiv.org/abs/2310.12818) | 本论文提出了一种提升参数共享预训练语言模型推理效率的简单技术，并介绍了一种简单的预训练方法来实现完全或部分共享的模型，实验结果证明了这些方法在各种模型上的有效性，为更有效地利用参数提供了新的见解。 |
| [^11] | [2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision.](http://arxiv.org/abs/2310.12817) | 本文提出了一种基于场景级监督的2D-3D交错Transformer模型，用于弱监督点云分割。该模型通过两个编码器计算2D和3D数据的自注意特征，并通过交替切换查询和键值对的角色，实现了2D和3D特征的融合。 |
| [^12] | [Prompt Injection Attacks and Defenses in LLM-Integrated Applications.](http://arxiv.org/abs/2310.12815) | 本文提出了一个通用框架来形式化提示注入攻击，并系统化防御这种类型的攻击。 |
| [^13] | [Hierarchical Forecasting at Scale.](http://arxiv.org/abs/2310.12809) | 提出了一种大规模层次预测的方法，使用稀疏损失函数直接优化层次产品和/或时间结构，从而为数百万个时间序列提供一致的预测。在实验中，该方法在M5数据集上表现出10%的性能提升。 |
| [^14] | [Model Merging by Uncertainty-Based Gradient Matching.](http://arxiv.org/abs/2310.12808) | 本论文通过不确定性梯度匹配的方法，提出了一种新的模型合并方案，该方案能够减少梯度不匹配，从而提高了模型合并的性能并对超参数更具鲁棒性。 |
| [^15] | [DCSI -- An improved measure of cluster separability based on separation and connectedness.](http://arxiv.org/abs/2310.12806) | 这篇论文提出了一种改进的聚类可分离性度量方法，旨在量化类间分离和类内连通性，对于密度聚类具有较好的性能表现。 |
| [^16] | [Detection and Evaluation of bias-inducing Features in Machine learning.](http://arxiv.org/abs/2310.12805) | 本论文介绍了一种在机器学习中检测和评估偏倚特征的方法，通过因果分析来理解系统偏倚行为的原因，并且可以准确识别潜在的偏倚特征，即使它们最初是未知的。 |
| [^17] | [Differentiable Vertex Fitting for Jet Flavour Tagging.](http://arxiv.org/abs/2310.12804) | 本论文提出了一种可微分的顶点拟合算法，可应用于次级顶点拟合，并与神经网络无缝集成，用于喷注标签。这一方法将物理知识与神经网络模型相结合，可以改善重味道喷注分类。 |
| [^18] | [Causal-structure Driven Augmentations for Text OOD Generalization.](http://arxiv.org/abs/2310.12803) | 本文提出了一种基于因果结构的反事实数据增强方法，用于改善文本分类器在应用中的泛化效果，特别适用于存在虚假相关性的标签与属性预测问题。 |
| [^19] | [An effective theory of collective deep learning.](http://arxiv.org/abs/2310.12802) | 通过竞争局部学习和单元之间的扩散耦合，我们引入了一个简化模型，预测了集体学习中的无序-有序-无序相变，并验证了这个理论。 |
| [^20] | [Exploring Graph Neural Networks for Indian Legal Judgment Prediction.](http://arxiv.org/abs/2310.12800) | 本研究探索了使用图神经网络来解决印度法律判决预测问题，通过识别司法案件的图结构并进行节点分类，以实现自动化推测案件结果。研究考虑了模型特征、公平性和链接预测任务。 |
| [^21] | [OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift.](http://arxiv.org/abs/2310.12793) | OODRobustBench是一个基准，用于评估和分析在分布迁移下的对抗性鲁棒性。大规模分析结果表明，对抗性鲁棒性在离群分布测试下存在严重的泛化问题，而内分布鲁棒性与离群分布鲁棒性呈强正线性相关。 |
| [^22] | [A Theoretical Approach to Characterize the Accuracy-Fairness Trade-off Pareto Frontier.](http://arxiv.org/abs/2310.12785) | 本研究通过建立一个理论框架，对公平性和准确性之间的权衡帕累托前沿进行了建模和分析，并发现了一些有见地的发现。 |
| [^23] | [Conditional Density Estimations from Privacy-Protected Data.](http://arxiv.org/abs/2310.12781) | 本文提出了一种从隐私保护数据中进行条件密度估计的方法，使用神经条件密度估计器来近似模型参数的后验分布，从而解决了在统计分析过程中只能访问私有化数据导致的计算复杂度增加的问题。 |
| [^24] | [Label-Aware Automatic Verbalizer for Few-Shot Text Classification.](http://arxiv.org/abs/2310.12778) | 这篇论文提出了一种标签感知的自动语言表达方式（LAAV），通过增加手动标签和连接词“和”来提升语言模型的生成，从而在少样本分类任务中取得更好的结果。 |
| [^25] | [Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning.](http://arxiv.org/abs/2310.12774) | 本文提出了一种名为ClaPS的简单黑盒搜索方法，通过聚类和修剪搜索空间中最有影响力的提示令牌，解决了现代黑盒方法中的效率问题。 |
| [^26] | [Safe RLHF: Safe Reinforcement Learning from Human Feedback.](http://arxiv.org/abs/2310.12773) | 我们提出了一种名为安全RLHF的算法，用于在大型语言模型的训练过程中平衡性能和安全性。它通过解耦人类偏好，并训练分别的奖励和成本模型，成功解决了有益和无害目标之间的固有张力，并通过动态调整平衡来优化算法性能。 |
| [^27] | [Stochastic Average Gradient : A Simple Empirical Investigation.](http://arxiv.org/abs/2310.12771) | 本研究通过对比SAG与机器学习中一些标准优化器，旨在将随机方法的成本与确定性方法的收敛速度结合起来。 |
| [^28] | [SemantIC: Semantic Interference Cancellation Towards 6G Wireless Communications.](http://arxiv.org/abs/2310.12768) | 本文提出了一种新颖的技术，语义干扰消除（SemantIC），用于提高6G无线通信网络的信息质量。通过在接收器上使用语义自动编码器，SemantIC能够迭代消除信号中的噪声和语义干扰。仿真结果表明，SemantIC能够在不增加额外信道资源成本的情况下改进性能。 |
| [^29] | [Transformer-based Entity Legal Form Classification.](http://arxiv.org/abs/2310.12766) | 提出了一种使用Transformer-based语言模型进行实体法律形式分类的方法，该方法在比较中表现出较高的性能并得到了第三方评审的支持。 |
| [^30] | [Energy-Based Models For Speech Synthesis.](http://arxiv.org/abs/2310.12765) | 本文引入了一种新的非自回归模型，即基于能量的模型 (EBMs)，通过噪声对比估计进行训练。该模型可以使用 Langevin MCMC 进行采样，并在语音合成任务中取得了改进。 |
| [^31] | [Discretize Relaxed Solution of Spectral Clustering via a Non-Heuristic Algorithm.](http://arxiv.org/abs/2310.12752) | 通过非启发式算法将谱聚类的松弛解离散化，以寻找最小化原始目标函数的离散解，并理论上证明了连续最优解的优势。 |
| [^32] | [TabuLa: Harnessing Language Models for Tabular Data Synthesis.](http://arxiv.org/abs/2310.12746) | 本文提出了一种基于语言模型结构的Tabula表格数据合成工具，通过研究我们发现，为自然语言处理设计的预训练语言模型在表格数据合成方面存在固有限制。 |
| [^33] | [Canonical normalizing flows for manifold learning.](http://arxiv.org/abs/2310.12743) | 该论文介绍了一种规范化正态流方法，用于流形学习。通过可学习的可逆变换将数据嵌入到高维空间中，从而实现了在流形上计算概率密度并优化网络参数的目标。然而，当前的方法在学习到的流形表示中存在着与流形关联且退化的内在基函数的问题。 |
| [^34] | [Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness.](http://arxiv.org/abs/2310.12713) | 本文提出了一个基于代理的对抗性防御框架，通过引入目标模型的历史状态作为代理来增强模型对各种对抗攻击的鲁棒性。 |
| [^35] | [Neurosymbolic Grounding for Compositional World Models.](http://arxiv.org/abs/2310.12690) | 本论文介绍了一种名为Cosmos的框架，用于对象为中心的世界建模，通过使用神经符号化基础和视觉-语言基础模型，实现了在未见过的输入场景上的高性能组合泛化能力。 |
| [^36] | [Compression of Recurrent Neural Networks using Matrix Factorization.](http://arxiv.org/abs/2310.12688) | 本论文提出了一种称为Rank-Tuning的训练后秩选择方法，可以在循环神经网络中高效压缩模型，并在几乎没有性能降低的情况下实现高压缩率。 |
| [^37] | [On the Optimization and Generalization of Multi-head Attention.](http://arxiv.org/abs/2310.12680) | 本论文研究了使用多头注意力在优化和泛化方面的优势，推导了单层多头自注意力模型的梯度下降训练的收敛性和泛化保证，并证明了对于一个简单的分词混合模型，初始化条件满足可实现性条件。 |
| [^38] | [Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff.](http://arxiv.org/abs/2310.12671) | 本研究通过深度学习结构的神经网络对频率-严重性保险定价进行了基准研究，比较了不同模型的性能，并提出了一种联合精算神经网络(CANN)的方法。 |
| [^39] | [STANLEY: Stochastic Gradient Anisotropic Langevin Dynamics for Learning Energy-Based Models.](http://arxiv.org/abs/2310.12667) | 本文提出了一种名为STANLEY的算法用于采样高维数据，改善了能量模型学习算法的质量。 |
| [^40] | [Knowledge from Uncertainty in Evidential Deep Learning.](http://arxiv.org/abs/2310.12663) | 本论文研究了证据深度学习（EDL）中产生的来自不确定性的证据信号，并发现这些信号可以在某些情况下区分类别。研究还探讨了EDL与其他基于Dirichlet的方法之间的关联，并表明EDL的“证据信号”是由于误分类偏差而产生的。 |
| [^41] | [Gradient Descent Fails to Learn High-frequency Functions and Modular Arithmetic.](http://arxiv.org/abs/2310.12660) | 梯度下降无法学习高频函数和模运算，该研究为使用基于梯度的学习技术训练高频周期函数和模乘法提供了数学分析。 |
| [^42] | [Towards a Deep Learning-based Online Quality Prediction System for Welding Processes.](http://arxiv.org/abs/2310.12632) | 该论文提出了一个基于深度学习的气体金属电弧焊接预测质量系统的概念，通过收集和管理多传感器数据以及实时处理和特征工程的方式，可以识别关系并预测焊接质量。 |
| [^43] | [Inverse Renormalization Group of Disordered Systems.](http://arxiv.org/abs/2310.12631) | 本文介绍了一种逆重整群的方法，通过机器学习算法构造了缩放的晶格配置，用于研究自旋玻璃。该方法可以实现在不使用超级计算机的情况下，探索不断增大的晶格体积下的精确配置。 |
| [^44] | [An Improved Metarounding Algorithm via Frank-Wolfe.](http://arxiv.org/abs/2310.12629) | 该论文通过引入Frank-Wolfe算法，改进了Metarounding算法，从而提高了线性优化算法在组合类问题上的效率。 |
| [^45] | [How a student becomes a teacher: learning and forgetting through Spectral methods.](http://arxiv.org/abs/2310.12612) | 本论文提出了基于谱方法的优化方案，用于解决在非凸性问题下学生网络与教师网络之间存在的不变子网络的识别问题。 |
| [^46] | [Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning.](http://arxiv.org/abs/2310.12609) | 本文提出了一种使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法。该方法通过单一的视觉输入，在推理时能够同时生成可达目标并规划避开障碍物的运动路径，具有稳健性和多模态适应性。 |
| [^47] | [Causal Similarity-Based Hierarchical Bayesian Models.](http://arxiv.org/abs/2310.12595) | 本文提出了一种基于因果相似性的分层贝叶斯模型，通过学习如何从具有相似因果机制的训练任务中汇集数据来提高机器学习算法对新任务的泛化能力。 |
| [^48] | [Time-Aware Representation Learning for Time-Sensitive Question Answering.](http://arxiv.org/abs/2310.12585) | 该论文提出了一种时态感知的问题回答框架，通过引入时间上下文的区间抽取任务和相应的数据生成框架来训练模型，提高了QA模型的时间感知能力，在TimeQA数据集中的F1分数上超过了基准模型达8.5。 |
| [^49] | [DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation.](http://arxiv.org/abs/2310.12570) | DA-TransUNet 提出了一种新的深度医学图像分割框架，集成了Transformer和双重注意力块。通过注意机制和多方面特征提取，提升医学图像分割结果。 |
| [^50] | [Julearn: an easy-to-use library for leakage-free evaluation and inspection of ML models.](http://arxiv.org/abs/2310.12568) | Julearn是一个易于使用的Python库，可以帮助研究人员设计和评估复杂的机器学习流水线，避免泄漏问题。 |
| [^51] | [Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark.](http://arxiv.org/abs/2310.12567) | 本文介绍了一个名为Safety-Gymnasium的环境套件，其中包括单个和多个Agent场景中的安全关键任务，并提供了一个包含16种最先进的SafeRL算法的算法库。这个基准旨在促进对安全性能的评估和比较，推动强化学习在安全性能上的发展。 |
| [^52] | [Open-World Lifelong Graph Learning.](http://arxiv.org/abs/2310.12565) | 本论文研究了开放世界场景下的终身图学习问题，提出了一种将超出分布（OOD）检测方法与图形邻域信息聚合相结合的新类别检测方法，并提出了一种弱监督相关反馈（Open-WRF）方法来降低OOD检测中对阈值的敏感性。实验证明所提出的方法在OOD得分方面优于现有方法，并且更加鲁棒。 |
| [^53] | [Approximate information maximization for bandit games.](http://arxiv.org/abs/2310.12563) | 本论文提出了一种基于近似信息最大化的强盗游戏算法，通过最大化关键变量的信息近似值来进行优化，在传统强盗设置中表现出很强的性能，并证明了其对于两臂强盗问题的渐近最优性。 |
| [^54] | [Fast Model Debias with Machine Unlearning.](http://arxiv.org/abs/2310.12560) | 这篇论文提出了一种快速模型去偏置的框架（FMD），可以有效识别、评估和消除深度神经网络中的偏见，解决了现有方法在成本和解释性方面的不足。 |
| [^55] | [Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers.](http://arxiv.org/abs/2310.12553) | 提出一种插入/删除指标感知的基于解释的优化(ID-ExpO)方法，通过优化可区分的预测器来提高解释的插入和删除得分，并保持预测准确性。实验结果表明，ID-ExpO能够使流行的事后解释器产生更忠实的解释。 |
| [^56] | [PGA: Personalizing Grasping Agents with Single Human-Robot Interaction.](http://arxiv.org/abs/2310.12547) | 这项研究介绍了个性化抓取代理（PGA），它通过单一人机交互学习并定位和抓取个人物体。PGA通过用户提供的信息和用户环境中的原始图像，实现个性化物体抓取。 |
| [^57] | [Neural Likelihood Approximation for Integer Valued Time Series Data.](http://arxiv.org/abs/2310.12544) | 本文构建了整数值时间序列数据的神经似然近似方法，使用因果卷积并行评估整个时间序列的似然，实现了对生态学和流行病学模型进行准确推断并显著加快计算速度。 |
| [^58] | [Constructing Impactful Machine Learning Research for Astronomy: Best Practices for Researchers and Reviewers.](http://arxiv.org/abs/2310.12528) | 这篇论文旨在为天文学界的作者、评审人员和编辑提供机器学习模型的实施和结果报告的最佳实践，以确保结果准确性、研究结果的可重复性和方法的实用性。 |
| [^59] | [Testing the Consistency of Performance Scores Reported for Binary Classification Problems.](http://arxiv.org/abs/2310.12527) | 这篇论文介绍了一种测试报告的二分类问题性能分数和实验设置一致性的数值方法，该方法不依赖于统计推断。 |
| [^60] | [Parallel Bayesian Optimization Using Satisficing Thompson Sampling for Time-Sensitive Black-Box Optimization.](http://arxiv.org/abs/2310.12526) | 该论文提出了一种基于满意度汤普森抽样的并行贝叶斯优化（STS-PBO）方法，用于解决时敏黑盒优化问题。该方法通过引入率失真理论构建了一个平衡信息量和次优性的损失函数，并采用Blahut-Arimoto算法计算目标解决方案。 |
| [^61] | [Named Entity Recognition for Monitoring Plant Health Threats in Tweets: a ChouBERT Approach.](http://arxiv.org/abs/2310.12522) | 本文研究了一种基于ChouBERT的命名实体识别方法，应用于监测植物健康威胁。通过利用社交媒体中的非结构化文本数据，可以检测并提取关键信息，从而解决现有解决方案中缺乏标记数据和细粒度语义资源的问题。 |
| [^62] | [Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks.](http://arxiv.org/abs/2310.12516) | 本文提出了一种通过可迁移的对抗攻击在大型语言模型中自动生成评估数据的方法，并使用ChatGPT和Natural Questions（NQ）数据集进行了验证。 |
| [^63] | [WeaveNet for Approximating Two-sided Matching Problems.](http://arxiv.org/abs/2310.12515) | 本文提出了一种用于近似双边匹配问题的图神经网络WeaveNet，通过保留边信息并密集传递消息，避免了节点信息丢失的问题。在公平稳定匹配问题中，我们的模型达到了与最新算法相当的性能。 |
| [^64] | [SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation.](http://arxiv.org/abs/2310.12508) | 这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。 |
| [^65] | [Attack Prompt Generation for Red Teaming and Defending Large Language Models.](http://arxiv.org/abs/2310.12505) | 我们提出了一种综合方法来经济地生成高质量的攻击提示，通过上下文学习指导大型语言模型（LLMs）模仿人类生成的提示，并通过迭代交互来加强受攻击的LLMs的安全性。这些方法在不同LLMs上的实验验证了其有效性。 |
| [^66] | [American Option Pricing using Self-Attention GRU and Shapley Value Interpretation.](http://arxiv.org/abs/2310.12500) | 本文提出了一种使用自注意力GRU和Shapley值解释的美式期权定价的机器学习方法，通过分割数据集和建立不同的模型，可以更准确地预测期权的价格。 |
| [^67] | [Quasi Manhattan Wasserstein Distance.](http://arxiv.org/abs/2310.12498) | 类曼哈顿瓦瑟斯坦距离（QMWD）是一种用于度量两个矩阵之间相异性的度量标准，它具有更好的时间和空间复杂度，并且适用于大型数据集或资源有限的情况。 |
| [^68] | [SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models.](http://arxiv.org/abs/2310.12494) | 这项研究提出了SDGym，一个基于系统动力学模型的低代码强化学习环境库，通过生成定制的RL环境来解决现实环境设计的困难，从而构建在实际设置中表现良好的强大智能体。 |
| [^69] | [Improved Operator Learning by Orthogonal Attention.](http://arxiv.org/abs/2310.12487) | 本研究提出了一种基于正交注意力的神经运算符，通过核积分算子的特征分解和神经近似的特征函数，来解决现有方法在有限训练数据上过拟合的问题。实验证明，该方法在六个标准神经运算符数据集上的表现优于其他基线模型。 |
| [^70] | [Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights.](http://arxiv.org/abs/2310.12462) | 本文提出了一种基于注意力权重和输出的理论框架，用于恢复Transformer模型中的输入数据。研究结果暗示模型设计存在潜在的漏洞。 |
| [^71] | [Balanced Group Convolution: An Improved Group Convolution Based on Approximability Estimates.](http://arxiv.org/abs/2310.12461) | 本文通过理论分析，发现组卷积相对于标准卷积的近似程度，提出了一种名为平衡组卷积的新变体，它在增加较小计算成本的情况下，展现出更高的近似性。实验结果验证了该方法的卓越性能。 |
| [^72] | [MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale.](http://arxiv.org/abs/2310.12457) | MuseGNN提出了一种可解释和可收敛的大规模图神经网络层，通过迭代地减少基于采样的能量函数，同时作为预测特征和能量函数最小化者，具有竞争力的准确性和可扩展性。 |
| [^73] | [MTS-LOF: Medical Time-Series Representation Learning via Occlusion-Invariant Features.](http://arxiv.org/abs/2310.12451) | MTS-LOF是一种利用对比学习和遮挡自编码器方法的医学时间序列表示学习框架，能够为医疗应用提供更复杂、更丰富的上下文表示，并通过多遮挡策略实现了遮挡不变特征的学习。 |
| [^74] | [Constrained Reweighting of Distributions: an Optimal Transport Approach.](http://arxiv.org/abs/2310.12447) | 本文提出了一种最优传输方法，通过引入非参数化的分布约束权重，并利用最大熵原理和最优传输工具设计了一个通用框架，以实现对观测数据的最优权重调整。这种方法在不同的应用场景中展现了灵活性和多功能性。 |
| [^75] | [Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer.](http://arxiv.org/abs/2310.12442) | 提出了一种高效的长程Transformer模型MASFormer，通过在少数层使用全局注意力和在其他层使用稀疏注意力，实现了在具有长序列的任务中高效的计算和建模能力。 |
| [^76] | [CAT: Closed-loop Adversarial Training for Safe End-to-End Driving.](http://arxiv.org/abs/2310.12432) | 本文提出了闭环对抗训练（CAT）框架用于实现安全的端到端驾驶。CAT通过训练代理在动态生成的安全关键情景上来不断提高驾驶代理的安全性。与现有的安全关键场景生成方法相比，CAT可以发起更高效的物理攻击，并显著降低计算成本。 |
| [^77] | [Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach.](http://arxiv.org/abs/2310.12428) | 这项研究提出了一种利用随机森林模型的特征空间中的邻近性来解释模型预测的方法，为模型预测提供了局部的解释性，与现有方法相辅相成。通过实验证明了这种方法在债券定价模型中的有效性。 |
| [^78] | [Automated Repair of Declarative Software Specifications in the Era of Large Language Models.](http://arxiv.org/abs/2310.12425) | 大语言模型时代下，自动修复声明式软件规范的有效技术需求愈发突出。该研究评估了利用ChatGPT修复Alloy声明式语言规范的效果，并探索了大型语言模型在此自动修复过程中的机会。 |
| [^79] | [Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling.](http://arxiv.org/abs/2310.12421) | 本文提出了使用因果建模来检测和减轻算法偏见的新方法，并在性别偏见和二元分类问题上进行了实证研究。通过交叉验证实验证明了因果模型在减轻性别偏见方面的有效性，并轻微提高了整体分类准确率。 |
| [^80] | [Provable Guarantees for Neural Networks via Gradient Feature Learning.](http://arxiv.org/abs/2310.12408) | 本研究提出了一个针对梯度特征学习的统一分析框架，证明了双层神经网络在训练过程中的可靠性，并在多个典型问题上展示了其有效性和有趣的学习现象。 |
| [^81] | [Classification-Aided Robust Multiple Target Tracking Using Neural Enhanced Message Passing.](http://arxiv.org/abs/2310.12407) | 本文提出了一种利用神经增强的方法进行分类辅助的鲁棒多目标跟踪，在强杂波环境中利用雷达传感器的测量信息来增强杂波剔除和数据关联，从而提高目标跟踪的鲁棒性。 |
| [^82] | [Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing.](http://arxiv.org/abs/2310.12404) | Loop Copilot是一种新型的AI音乐合奏系统，能够通过交互式多轮对话界面生成和迭代改进音乐，通过选择适当的AI模型执行任务，并在一个集中的表中保持关键属性以确保音乐的连贯性。 |
| [^83] | [Cooperative Minibatching in Graph Neural Networks.](http://arxiv.org/abs/2310.12403) | 本文提出了一种协作小批处理的方法来解决图神经网络中的邻域爆炸现象（NEP），该方法通过利用采样子图的大小与批处理大小的关系来减少每个种子顶点的工作量。 |
| [^84] | [Closed-Form Diffusion Models.](http://arxiv.org/abs/2310.12395) | 本研究提出了一种闭式扩散模型，通过显式平滑的闭式得分函数来生成新样本，无需训练，且在消费级CPU上能够实现与神经SGMs相竞争的采样速度。 |
| [^85] | [Learning to Solve Climate Sensor Placement Problems with a Transformer.](http://arxiv.org/abs/2310.12387) | 本文介绍了一种使用深度强化学习方法学习改进传感器布放策略的新方法，通过与其他方法的对比实验证明了该方法在产生高质量解决方案方面的有效性和优越性。 |
| [^86] | [No-Regret Learning in Bilateral Trade via Global Budget Balance.](http://arxiv.org/abs/2310.12370) | 本文引入全局预算平衡的概念，提出了首个针对具有对抗输入的双边贸易的无悔算法，并在不同的反馈模型下进行了验证。 |
| [^87] | [MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits.](http://arxiv.org/abs/2310.12359) | MARVEL是一个多智能体强化学习框架，可以实现利用常见数据在高速公路走廊上进行大规模可变速限控制。它通过奖励结构和智能体之间的协调，提高交通安全性和流动性。与无控制情况相比，MARVEL可以提高交通安全性63.4%，提高交通流动性14.6%。 |
| [^88] | [Networkwide Traffic State Forecasting Using Exogenous Information: A Multi-Dimensional Graph Attention-Based Approach.](http://arxiv.org/abs/2310.12353) | 本文提出了一种基于多维图神经网络注意力的交通预测方法，利用过去观测到的速度、车道封闭事件、温度和可视性来预测交通情况。该方法基于交通网络结构进行学习，展示了较好的实验结果。 |
| [^89] | [Equipping Federated Graph Neural Networks with Structure-aware Group Fairness.](http://arxiv.org/abs/2310.12350) | 本论文提出了一种名为F2GNN的方法，它旨在增强联邦图神经网络的群体公平性，解决了在联邦学习中减轻偏见的新挑战。 |
| [^90] | [Tracking electricity losses and their perceived causes using nighttime light and social media.](http://arxiv.org/abs/2310.12346) | 本研究利用夜间灯光数据和社交媒体监测停电及其感知原因，在委内瑞拉的案例中显示出夜间灯光强度与停电量之间的反向关系，并发现提到委内瑞拉总统的推文显示出更高的负面情绪和绿色词汇趋势。 |
| [^91] | [ClusT3: Information Invariant Test-Time Training.](http://arxiv.org/abs/2310.12345) | ClusT3是一种新颖的无监督测试时间训练方法，通过最大化多尺度特征图和离散潜在表示之间的互信息来增强深度学习模型的鲁棒性。实验结果表明，在不同的测试时间自适应基准上具有竞争性的分类性能。 |
| [^92] | [Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives.](http://arxiv.org/abs/2310.12324) | 适应性实验为持续课程改进提供了机会，通过动态部署最有效的条件以满足学生需求。 |
| [^93] | [A Unifying Framework for Learning Argumentation Semantics.](http://arxiv.org/abs/2310.12309) | 本文提出了一种统一的框架来学习论证语义，通过使用可解释的方法，优于现有的论证求解器，并在形式论证和人机对话领域开辟了新的研究方向。 |
| [^94] | [Preference Optimization for Molecular Language Models.](http://arxiv.org/abs/2310.12304) | 本研究调查了使用直接偏好优化进行微调的方法，以更好地与化学家的偏好对齐生成的分子。这种方法简单、高效，且非常有效。 |
| [^95] | [Document-Level Language Models for Machine Translation.](http://arxiv.org/abs/2310.12303) | 这项工作提出了一种利用文档级别语言模型构建上下文感知的翻译系统的方法。通过结合任何现有的句级别翻译模型与文档级别语言模型，并借鉴模型组合的最新进展，尤其是权重技术的提出，可以显著提高文档级别指标并降低计算开销。 |
| [^96] | [Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization.](http://arxiv.org/abs/2310.12298) | 本文介绍了Jorge，一种GPU高效的二阶优化算法，通过近似预处理方法替代矩阵求逆计算来提高计算效率，同时兼具二阶方法的收敛性能。实验证明了Jorge的有效性。 |
| [^97] | [Open-Set Multivariate Time-Series Anomaly Detection.](http://arxiv.org/abs/2310.12294) | 本论文提出了一种开放式时间序列异常检测方法，能够在训练阶段识别有限异常类别的少量标记异常，并在测试阶段检测到见过和未见过的异常类别。 |
| [^98] | [Enhancing the Performance of Automated Grade Prediction in MOOC using Graph Representation Learning.](http://arxiv.org/abs/2310.12281) | 本研究通过图表示学习构建了一个独特的知识图谱，以提高MOOC中自动化成绩预测的性能。 |
| [^99] | [An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning.](http://arxiv.org/abs/2310.12274) | 提出了一种多概念提示学习（MCPL）框架，通过同时学习多个新的“词”来解决在单个场景中识别和整合多个对象级概念的挑战。针对词概念相关性准确性问题，提出了注意力掩码、提示对比损失和绑定形容词等三种正则化技术。通过图像生成进行了评估，结果表明该框架能够生成更多样化和合成的图像。 |
| [^100] | [Improving SCGAN's Similarity Constraint and Learning a Better Disentangled Representation.](http://arxiv.org/abs/2310.12262) | 本论文改进了SCGAN模型中的相似性约束，使用SSIM度量图像相似性并应用对比损失原则，提高了模型的性能和泛化能力。 |
| [^101] | [A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs.](http://arxiv.org/abs/2310.12248) | 这个论文介绍了一种基于模型的PAC学习算法，用于在MDP中学习ω-regular目标，不需要系统拓扑的先前知识。 |
| [^102] | [A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm.](http://arxiv.org/abs/2310.12244) | 这篇论文提出了一种统一的带有记忆的领域增量学习方法（UDIL），通过统一不同的现有方法并使用自适应系数，实现了更紧的泛化误差界限，并在实验证明在合成数据和真实数据集上优于最先进的方法。 |
| [^103] | [REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary Objects in Realistic Scenes.](http://arxiv.org/abs/2310.12243) | REVAMP是一个易于使用的Python库，可用于在现实场景中进行对任意对象的对抗攻击的自动化模拟。通过使用可微渲染和模拟真实环境因素，REVAMP使研究人员和实践者能够快速探索各种攻击场景。 |
| [^104] | [Few-Shot In-Context Imitation Learning via Implicit Graph Alignment.](http://arxiv.org/abs/2310.12238) | 本文提出了一种少样本背景下的模仿学习方法，通过将模仿学习视为物体的图表示之间的条件对齐问题，实现了在没有先验知识或进一步训练的情况下，机器人可以在新的物体集上执行任务。 |
| [^105] | [Fast Parameter Inference on Pulsar Timing Arrays with Normalizing Flows.](http://arxiv.org/abs/2310.12209) | 本研究使用归一化流加速且准确地估计了脉冲星时间阵列中随机引力波背景的后验分布。相比传统MCMC方法，将采样时间从几周缩短到几秒钟。 |
| [^106] | [Architectural Implications of GNN Aggregation Programming Abstractions.](http://arxiv.org/abs/2310.12184) | 本文通过对现有GNN聚合编程抽象进行分类，并在最先进的GNN库上进行特征研究和性能比较，提供了未来GNN加速的见解。 |
| [^107] | [Enhanced Graph Neural Networks with Ego-Centric Spectral Subgraph Embeddings Augmentation.](http://arxiv.org/abs/2310.12169) | 本论文提出了一种名为ESGEA的新方法，在缺乏信息的情况下增强并设计节点特征，通过利用局部子图的拓扑结构来创建拓扑感知的节点特征。我们的方法使用高效的谱图嵌入技术生成子图特征，并将其作为节点特征捕捉网络的局部拓扑组织。 |
| [^108] | [RK-core: An Established Methodology for Exploring the Hierarchical Structure within Datasets.](http://arxiv.org/abs/2310.12168) | RK-core方法探索数据集中的层次结构，通过分析样本的核心值，发现核心值高的样本在性能上具有更大的贡献。 |
| [^109] | [AI Potentiality and Awareness: A Position Paper from the Perspective of Human-AI Teaming in Cybersecurity.](http://arxiv.org/abs/2310.12162) | 这篇立场文件通过探讨人工智能在网络安全中的潜力，强调了人工智能与人类专家的协作的重要性。人工智能系统可以通过模式识别和预测建模主动发现漏洞并检测异常情况，而人类专家可以补充和辅助AI的输出结果，提高整体的网络安全防护能力。 |
| [^110] | [Operator-Based Detecting, Learning, and Stabilizing Unstable Periodic Orbits of Chaotic Attractors.](http://arxiv.org/abs/2310.12156) | 本文提出了基于算子的方法用于分析混沌系统中的非稳定周期轨道。我们使用核积分算子检测非稳定周期轨道，利用Koopman算子识别动态行为，并扩展了这种方法用于稳定奇怪吸引子。以Lorenz吸引子为例，验证了我们方法的有效性。 |
| [^111] | [Transformers for scientific data: a pedagogical review for astronomers.](http://arxiv.org/abs/2310.12069) | 本综述旨在向科学家介绍Transformer的应用，包括自然语言处理和自注意机制。此外，还介绍了在天文学中应用于时间序列和成像数据的具体情况，并提供了常见问题解答部分。 |
| [^112] | [Improving Generalization of Alignment with Human Preferences through Group Invariant Learning.](http://arxiv.org/abs/2310.11971) | 该论文提出了一种通过强化学习实现在不同数据组或领域中学习一致策略的方法，该方法可以提高AI助手对不同领域的泛化能力，并更好地与人类偏好对齐。 |
| [^113] | [A Quasi-Wasserstein Loss for Learning Graph Neural Networks.](http://arxiv.org/abs/2310.11762) | 这篇论文提出了一种新的准瓦狄斯坦损失函数，通过利用图上的最优传输来学习图神经网络，消除了现有损失函数在节点级别预测中可能存在的不一致性。 |
| [^114] | [When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting.](http://arxiv.org/abs/2310.11569) | 提出了一个新的概率分层时间序列预测模型，该模型能够有效建模和预测具有层次化关系的多变量时间序列。相较于现有方法，该模型不仅考虑点预测，还能提供经过良好校准的概率预测分布，并且在建模过程中考虑了预测分布的相关性。 |
| [^115] | [Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction.](http://arxiv.org/abs/2310.11466) | 本文研究了蛋白质基于结构的性质预测中使用预测结构时性能下降的原因，并将其归因为结构嵌入偏差。 |
| [^116] | [HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning.](http://arxiv.org/abs/2310.11102) | HGCVAE是一种将生成式学习和对比学习整合为一体的异构图学习方法，通过利用生成式的自监督学习能力来解决异构图学习的挑战。 |
| [^117] | [Adaptive Pairwise Encodings for Link Prediction.](http://arxiv.org/abs/2310.11009) | 提出了一种自适应的对向编码方法，用于解决链路预测中现有方法的归纳偏差问题。该方法将消息传递神经网络和启发式方法结合起来，能够更好地分类各种不同因素形成的链路。 |
| [^118] | [ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors.](http://arxiv.org/abs/2310.10692) | ACES是一种使用自我目标语言模型和语义描述符生成多样化的编程难题的方法，能够优化有趣的多样性和少样本生成。 |
| [^119] | [In-Context Pretraining: Language Modeling Beyond Document Boundaries.](http://arxiv.org/abs/2310.10638) | 本论文提出了一种超越文档边界的上下文预训练方法，通过在相关文档序列上训练语言模型，鼓励模型进行跨文档的阅读和推理。该方法通过改变文档顺序并应用现有的预训练管道来实现。 |
| [^120] | [Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories.](http://arxiv.org/abs/2310.10541) | 本论文提出了一种高效的数据集精炼方法，通过与平滑高质量的专家轨迹对齐，实现对大规模数据集的替代，并提出了剪辑损失和梯度惩罚的集成来调节学生和专家之间的互动。 |
| [^121] | [Microscaling Data Formats for Deep Learning.](http://arxiv.org/abs/2310.10537) | 本文评估了Microscaling（MX）数据格式在降低深度学习应用的计算和存储成本方面的可行性。实证结果显示MX数据格式可以作为基线FP32的替代，同时保持低用户摩擦，并且成功在超过两打基准测试中以小于8位的数据格式进行了训练。 |
| [^122] | [Data Augmentation for Time-Series Classification: An Extensive Empirical Study and Comprehensive Survey.](http://arxiv.org/abs/2310.10060) | 本研究对时间序列分类中的数据增强方法进行了广泛研究和综述，总结了60多种独特的方法，并提出了一个针对TSC的新的分类体系。 |
| [^123] | [Rank-DETR for High Quality Object Detection.](http://arxiv.org/abs/2310.08854) | Rank-DETR是一种高质量目标检测方法，通过引入排名导向的设计，包括架构设计和损失函数设计，实现了性能卓越的基于DETR的目标检测器。 |
| [^124] | [Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift.](http://arxiv.org/abs/2310.08237) | 该论文提出了一个在协变量漂移下对一般非参数方法进行统一分析的方法，通过建立收敛速度为一般损失函数提供了统一的理论分析。 |
| [^125] | [Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines.](http://arxiv.org/abs/2310.07940) | 该论文探索了在物联网设备中使用深度神经网络时的硬件软件协同优化，重点考虑了成本、延迟和用户体验，并研究了量化、模型缩放和多模态等方法与系统组件的相互作用。 |
| [^126] | [Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer.](http://arxiv.org/abs/2310.07587) | 本文提出了一种名为Fed-GraB的方法，该方法通过自适应梯度平衡器来解决联邦式长尾学习的问题。该方法能够在隐私约束下刻画全局长尾分布，并通过调整本地学习策略来解决头部-尾部不平衡的问题。 |
| [^127] | [ROMO: Retrieval-enhanced Offline Model-based Optimization.](http://arxiv.org/abs/2310.07560) | ROMO是一种检索增强的离线模型优化方法，通过在离线数据集中优化平庸设计，并保持给定的约束来解决约束MBO问题。 |
| [^128] | [KwaiYiiMath: Technical Report.](http://arxiv.org/abs/2310.07488) | KwaiYiiMath是一个用于增强数学推理能力的大型语言模型，通过应用监督微调和人类反馈强化学习，在英语和中文数学任务上取得了最先进的性能，并且能够正确解决生成的问题过程。 |
| [^129] | [Recurrent Neural Language Models as Probabilistic Finite-state Automata.](http://arxiv.org/abs/2310.05161) | 本文研究了循环神经网络语言模型（RNN LMs）作为概率有限状态自动机的能力，并发现它们只能表示有限状态模型所能表达的概率分布的一个严格子集。 |
| [^130] | [Digital Ethics in Federated Learning.](http://arxiv.org/abs/2310.03178) | 本文探讨了在联邦学习中作为客户端的以人为中心的设备引发的数字伦理问题。瞭解了其中涉及的游戏动态、公平性、奖励机制和连贯性等挑战，并探讨了解决方案和以人为中心的物联网在联邦学习中的机遇。 |
| [^131] | [zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning.](http://arxiv.org/abs/2310.02554) | zkFL是一种基于零知识证明的联邦学习梯度聚合方法，通过提供每轮的证明来解决协调者恶意行为的问题。 |
| [^132] | [SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training.](http://arxiv.org/abs/2310.02227) | SNIP引入了一种统一的预训练框架，通过联合对比学习加强了符号和数值领域之间的相似性，并提供了跨领域的表示洞察力。 |
| [^133] | [OceanGPT: A Large Language Model for Ocean Science Tasks.](http://arxiv.org/abs/2310.02031) | OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。 |
| [^134] | [A path-norm toolkit for modern networks: consequences, promises and challenges.](http://arxiv.org/abs/2310.01225) | 本文介绍了适用于现代神经网络的路径范数工具包，可以包括具有偏差、跳跃连接和最大池化的通用DAG ReLU网络。这个工具包恢复或超越了已知的路径范数界限，并挑战了基于路径范数的一些具体承诺。 |
| [^135] | [AnglE-Optimized Text Embeddings.](http://arxiv.org/abs/2309.12871) | 本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。 |
| [^136] | [The Kernel Density Integral Transformation.](http://arxiv.org/abs/2309.10194) | 本文提出了一种使用核密度积分转换作为特征预处理步骤的方法，可以替代线性最小最大缩放和分位数转换，并通过调整超参数优化性能。这种方法在统计数据分析中具有应用价值。 |
| [^137] | [Generative modeling, design and analysis of spider silk protein sequences for enhanced mechanical properties.](http://arxiv.org/abs/2309.10170) | 提出了一种生成式语言模型，用于设计具有复杂目标机械性能组合的新型蜘蛛丝蛋白序列。通过BLAST搜索、性能评估、分子结构比较和序列基序分析等方式对模型性能进行了评估。 |
| [^138] | [CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling.](http://arxiv.org/abs/2309.05270) | 本论文提出了CONFLATOR：一种针对代码混合语言的神经语言建模方法，通过引入旋转位置编码和切换点信息，在混合语言建模中取得最佳结果。 |
| [^139] | [An ML-assisted OTFS vs. OFDM adaptable modem.](http://arxiv.org/abs/2309.01319) | 本文提出了一种基于机器学习辅助的OTFS与OFDM适应性调制解调器，通过观察信道状态、接收信噪比和调制格式，在发送端和接收端之间切换OTFS或OFDM信号处理链以获得最优均方误差（MSE）性能。 |
| [^140] | [Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair.](http://arxiv.org/abs/2309.00608) | 这篇论文提出了一种框架，利用完成引擎来进一步支持大型语言模型在自动化程序修复中合成更多有效的修补程序。 |
| [^141] | [Topic-Level Bayesian Surprise and Serendipity for Recommender Systems.](http://arxiv.org/abs/2308.06368) | 本文通过引入基于主题的贝叶斯惊喜概念，提出了一种用于推荐系统的意外性模型，以解决过滤泡问题，通过识别相似用户和测量用户对物品的意外性来推荐具有高潜力的意外性物品。 |
| [^142] | [Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation.](http://arxiv.org/abs/2307.09688) | Amazon-M2是一个多语言多区域购物会话数据集，可以增强个性化推荐和理解用户偏好能力。 |
| [^143] | [Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation.](http://arxiv.org/abs/2307.06125) | 这项工作提出了一个层次化的强化学习方法，用于解决在未知环境中需要同时进行操控和导航的交互式多目标搜索任务。实验证明，该方法可以在新环境中进行零样本迁移，并对未见过的子任务具有鲁棒性。 |
| [^144] | [Deep Probabilistic Movement Primitives with a Bayesian Aggregator.](http://arxiv.org/abs/2307.05141) | 该论文提出了一个统一的深度运动原理模型，具备多种操作，并展示了高样本效率和泛化能力。 |
| [^145] | [On the power of graph neural networks and the role of the activation function.](http://arxiv.org/abs/2307.04661) | 本文通过对称多项式代数的工具证明了对于具有分段多项式激活函数且体系结构大小不变的GNNs，存在一对非同构根树在任意迭代次数内无法被区分，与此同时，具有不同大小的GNNs只需两次迭代即可区分。此外，我们还证明了如果允许非分段多项式激活函数，则在两次迭代内，单个神经元感知器可以区分任意一对非同构树的根节点。 |
| [^146] | [Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks.](http://arxiv.org/abs/2307.04228) | 本论文提出了一种高效的贝叶斯行程时间层析成像方法，利用敏感性信息的多项式混沌展开和深度生成网络，以处理地质复杂性先验的挑战。 |
| [^147] | [URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates.](http://arxiv.org/abs/2307.03810) | URL基准是一个评估预训练模型可转移性和不确定性估计的方式，研究发现专注于表示本身不确定性或直接估计预测风险的方法效果优于基于概率的方法。 |
| [^148] | [Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale.](http://arxiv.org/abs/2306.15687) | Voicebox是一种大规模的多语言通用语音生成模型，通过使用非自回归的流匹配模型，在文本和音频上下文条件下进行训练，可以实现零样本跨语言文本到语音合成，噪声去除，内容编辑，风格转换和多样化的样本生成等多种任务。 |
| [^149] | [Machine Learning Based Compensation for Inconsistencies in Knitted Force Sensors.](http://arxiv.org/abs/2306.12129) | 本文展示了一种基于机器学习的针织力传感器不一致性补偿方法，通过使用指数平滑滤波器进行预处理，并使用最小的人工神经网络来提高传感器读数和执行力之间的映射。 |
| [^150] | [Provably Powerful Graph Neural Networks for Directed Multigraphs.](http://arxiv.org/abs/2306.11586) | 本文分析了一组简单的改进方法，将标准的消息传递图神经网络（GNN）转化为可证明强大的有向多图神经网络，能够检测任何有向子图模式。实验结果展示了这些改进方法在合成子图检测任务和金融犯罪分析任务上的出色性能。 |
| [^151] | [Evaluating Superhuman Models with Consistency Checks.](http://arxiv.org/abs/2306.09983) | 本文提出了一个用一致性检查评估超人模型的框架，可以发现决策制定中的逻辑不一致性，即使对于超人模型的决策正确性可能是不可能评估的情况。 |
| [^152] | [The Power of Populations in Decentralized Learning Dynamics.](http://arxiv.org/abs/2306.08670) | 本文研究了分散式学习动力学中个体群体的力量。我们介绍了一种分散式的多臂赌博机设置，并分析了几个针对此任务的分散式动力学家族。我们展示了这些动力学与一类“零和”乘法权重更新算法的联系，并开发了一个通用框架来分析这些协议的群体级遗憾。在广泛的参数范围下，我们得到了次线性的遗憾界限。 |
| [^153] | [ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations.](http://arxiv.org/abs/2306.08141) | 为研究人工智能与人类交互，研究者创建了ArtWhisperer数据集，这是一个在线游戏，人们通过反复尝试不同的提示词，来生成和目标图像类似的图像，并记录了50,000多个交互记录。在初步分析中，研究者发现人们提交了各种各样的提示词，并能够发现生成各种文本描述的图像。 |
| [^154] | [Kepler: Robust Learning for Faster Parametric Query Optimization.](http://arxiv.org/abs/2306.06798) | Kepler是一种快速参数查询优化的鲁棒学习方法，通过使用基于实际执行数据的评估和机器学习模型预测最快计划，Kepler能够在查询延迟上显著加速，避免查询性能回归。 |
| [^155] | [Variational Imbalanced Regression.](http://arxiv.org/abs/2306.06599) | 本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。 |
| [^156] | [Language-Guided Traffic Simulation via Scene-Level Diffusion.](http://arxiv.org/abs/2306.06344) | 该论文提出了一种可以受到语言指导的场景级条件扩散模型，该模型能够生成真实且可控的交通，并通过大型语言模型将用户的查询转化为损失函数，指导模型生成符合查询条件的仿真交通。 |
| [^157] | [On the Design Fundamentals of Diffusion Models: A Survey.](http://arxiv.org/abs/2306.04542) | 本文综述了扩散模型的设计基础，即其三个关键组件：正向过程、逆向过程和采样过程，为未来的研究提供了有益的细粒度透视。 |
| [^158] | [Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE.](http://arxiv.org/abs/2306.03659) | 本论文提出了一种通过 MASCHInE 捕捉语义学习知识图嵌入的方法，通过设计生成原型图并利用其语义，进而训练出更好地捕捉语义的 KGEs。 |
| [^159] | [Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning.](http://arxiv.org/abs/2306.00477) | 本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。 |
| [^160] | [AdANNS: A Framework for Adaptive Semantic Search.](http://arxiv.org/abs/2305.19435) | AdANNS是一种自适应语义搜索框架，利用不同容量的自适应表示形式可以获得更好的精度-计算折衷权衡，相似度计算越接近的数据点将使用更低容量的表示形式进行计算，演示了最先进的精度-计算折衷权衡。 |
| [^161] | [Comparing Long Short-Term Memory (LSTM) and Bidirectional LSTM Deep Neural Networks for power consumption prediction.](http://arxiv.org/abs/2305.16546) | 本文比较了LSTM和BLSTM两种深度学习模型在电力消耗短期预测中的性能，通过四个数据集的结果表明BLSTM的表现更好。 |
| [^162] | [Voyager: An Open-Ended Embodied Agent with Large Language Models.](http://arxiv.org/abs/2305.16291) | Voyager是一个在Minecraft中使用大型语言模型进行开放式探索和学习的代理，它通过自动课程设置、可执行代码技能库和迭代提示机制不断提升自己的能力，并展现出强大的终身学习能力和在玩Minecraft方面的出色表现。 |
| [^163] | [Post-processing Private Synthetic Data for Improving Utility on Selected Measures.](http://arxiv.org/abs/2305.15538) | 本论文提出一种后处理技术，能够通过重新抽样，过滤掉不符合最终用户选择的度量指标的合成数据样本，从而提高其效用，同时保持隐私和数据集质量 |
| [^164] | [Fairness in Streaming Submodular Maximization over a Matroid Constraint.](http://arxiv.org/abs/2305.15118) | 这篇论文研究了在一个Matroid约束下流式子模最大化中的公平性问题，提供了流式算法和不可能的结果来权衡效率、质量和公平性，并在现实世界应用中进行了实证验证。 |
| [^165] | [Connecting Multi-modal Contrastive Representations.](http://arxiv.org/abs/2305.14381) | 本文提出了一种无需配对数据学习MCR的方法，叫做C-MCR，并且在新空间中使用重叠模态B的数据来对齐两个MCR。通过这个方法，非重叠模态对（A，C）也可以使用连接。 |
| [^166] | [Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks.](http://arxiv.org/abs/2305.12467) | 本文对于ReLU神经网络通过Gradient Flow训练的两层模型在线性可分数据上进行了完整的理论分析，揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。 |
| [^167] | [Online Resource Allocation in Episodic Markov Decision Processes.](http://arxiv.org/abs/2305.10744) | 本文将长期资源分配问题形式化为剧集式MDP模型，提出在面临转换和奖励特性不确定的情况下进行在线资源分配问题的解决方案。该方案基于占有度量提供在线镜像下降算法，其期望遗憾受到界限约束 $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ . |
| [^168] | [Seeing double with a multifunctional reservoir computer.](http://arxiv.org/abs/2305.05799) | 本文研究了储备计算机的多功能性能力，探讨了不同吸引子之间的关系如何影响其实现多任务。结果表明，要实现多功能性，需要RC内部网络c的谱半径合适选择的临界依赖性。 |
| [^169] | [Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins.](http://arxiv.org/abs/2305.04934) | 本研究使用基于语言模型的深度学习策略，在蛋白质建模中应用transformer和图卷积的结构预训练生成模型，进一步训练后能够设计具有特定性质的蛋白质，案例验证表明该方法可生成理想目标性质的蛋白质。 |
| [^170] | [Automatic Prompt Optimization with "Gradient Descent" and Beam Search.](http://arxiv.org/abs/2305.03495) | 在基于大型语言模型的自然语言处理中，使用梯度下降和 beam search 的自动提示优化方法可以自动改进提示，提高性能。 |
| [^171] | [Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models.](http://arxiv.org/abs/2304.12526) | Patch Diffusion 提出了一种通用的基于块的训练框架，可将扩散模型训练时间缩短至少一倍，并在更小的数据集上实现更好的性能表现。 |
| [^172] | [PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques.](http://arxiv.org/abs/2304.12410) | 本文提出了PEFT-Ref参考架构，标准化了不同PEFT技术共享的方面，隔离了差异到特定位置和交互中，模块化的视图有助于比较不同技术及其效率和任务性能，并有助于更好地理解PEFT的基本原理。 |
| [^173] | [An Introduction to Transformers.](http://arxiv.org/abs/2304.10557) | Transformer是一种神经网络组件，可以学习序列或数据集表示，在自然语言处理、计算机视觉和时空建模方面取得了重大进展。本论文提供了一个数学精确、直观、简洁的Transformer架构描述。 |
| [^174] | [Multi-label Node Classification On Graph-Structured Data.](http://arxiv.org/abs/2304.10398) | 该论文提出了一种新的图神经网络架构 MLGCN，用于处理多标签节点分类任务，并研究了多标签场景中同类偏好的语义。在各种基准测试中，MLGCN优于现有的最先进的多标签分类方法。 |
| [^175] | [The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties.](http://arxiv.org/abs/2304.09310) | 本文提出了一种新型鲁棒的自适应 $\tau$-Lasso 估计器，同时采用自适应 $\ell_1$-范数惩罚项以降低真实回归系数的偏差。它具有变量选择一致性和真实支持下回归向量的渐近正态性的最优性质，假定已知真实回归向量的支持。 |
| [^176] | [A Scalable Test Problem Generator for Sequential Transfer Optimization.](http://arxiv.org/abs/2304.08503) | STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。 |
| [^177] | [On Existential First Order Queries Inference on Knowledge Graphs.](http://arxiv.org/abs/2304.07063) | 本文阐述了关于知识图谱中存在性一阶查询推理的新方法，提出了一个新数据集，并开发了一种来自模糊逻辑理论的新搜索算法，该算法能够解决新公式，并在现有公式中超过以前的方法。 |
| [^178] | [Physics-informed neural networks in the recreation of hydrodynamic simulations from dark matter.](http://arxiv.org/abs/2303.14090) | 本文介绍了一种新的方法，将物理启发的神经网络应用于重建流体力学模拟中，通过将理论知识注入模型损失函数并结合新的性能评估指标，成功实现了对重子散射的重建。 |
| [^179] | [Towards Better Dynamic Graph Learning: New Architecture and Unified Library.](http://arxiv.org/abs/2303.13047) | 我们提出了一种基于Transformer的新型动态图学习架构DyGFormer，并引入了统一的库DyGLib，以促进可重复、可扩展和可信的动态图学习研究。DyGFormer通过邻居共现编码方案和分块技术实现更长期历史的高效推理，在13个不同领域的数据集上实现了最先进的性能。 |
| [^180] | [EDGI: Equivariant Diffusion for Planning with Embodied Agents.](http://arxiv.org/abs/2303.12410) | EDGI是一种基于模型的强化学习和规划算法，通过等变扩散处理内在对称性，具有更高效的采样和更好的泛化能力，适用于具有内在对称性的机器人操作任务。 |
| [^181] | [Test-Time Distribution Normalization for Contrastively Learned Vision-language Models.](http://arxiv.org/abs/2302.11084) | 这篇论文介绍了一个针对对比学习的视觉-语言模型的测试时分布归一化的方法，解决了常见的点乘操作导致测试时信息丢失的问题，提高了模型在测试阶段的准确性和效率。 |
| [^182] | [Piecewise Deterministic Markov Processes for Bayesian Neural Networks.](http://arxiv.org/abs/2302.08724) | 本文介绍了基于分段确定性马尔可夫过程的贝叶斯神经网络推理方法，通过引入新的自适应稀疏方案，实现了对困难采样问题的加速处理。实验证明，这种方法在计算上可行，并能提高预测准确性、MCMC混合性能，并提供更有信息量的不确定性测量。 |
| [^183] | [One-shot Empirical Privacy Estimation for Federated Learning.](http://arxiv.org/abs/2302.03098) | 本论文提出了一种用于联邦学习的单次经验隐私估计方法，可有效进行隐私损失审计，且无需事先了解模型体系结构或训练数据分布，适用于在实践中大规模部署。 |
| [^184] | [IC3: Image Captioning by Committee Consensus.](http://arxiv.org/abs/2302.01328) | "IC3: Image Captioning by Committee Consensus"引入了一种通过委员会共识生成图像字幕的方法，能够从多个注释者的视角捕捉高层细节，优于单个人生成的参考字幕，并在视觉描述方面取得了显著改进。 |
| [^185] | [Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data.](http://arxiv.org/abs/2301.12321) | 本研究提出了一种利用数据的关系结构识别标签错误和异常数据的统一方法，并提供了可视化工具，在大规模图像、语音和语言领域任务中表现良好。 |
| [^186] | [Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units.](http://arxiv.org/abs/2212.09730) | DISSC是一种新颖、轻量级的语音风格转换方法，它可以以无需文本的方式将录音的节奏、音高轮廓和音色转换为目标说话者的风格。该方法使用自监督模型编码语音为离散单元，具有简单、有效且快速的训练过程，适用于无配对数据的多对多语音转换。 |
| [^187] | [Learning threshold neurons via the "edge of stability".](http://arxiv.org/abs/2212.07469) | 该论文通过对简化的两层神经网络模型的梯度下降进行详细分析，揭示了大学习率下非凸训练动态的稳定边缘现象，并发现了神经网络无法学习阈值样式神经元的临界步长。 |
| [^188] | [An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws.](http://arxiv.org/abs/2212.01365) | 我们进行了一项关于大型神经网络的计算最优权衡的研究，并提出了一种信息论上可行的期望误差最小化模型和数据集大小的计算分配方法。 |
| [^189] | [Convergence of policy gradient methods for finite-horizon stochastic linear-quadratic control problems.](http://arxiv.org/abs/2211.00617) | 本文研究了有限时间随机线性二次控制问题中策略梯度方法的全局线性收敛性，并提出了基于Fisher几何和Bures-Wasserstein几何的几何感知梯度下降算法，该算法以线性速率全局收敛到最优策略。 |
| [^190] | [Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook.](http://arxiv.org/abs/2210.13623) | 这篇综述调查了强化学习和赌博机在语音和自然语言处理中的最新进展，讨论了如何有效利用它们解决相关问题，构建适应性、交互性和可扩展性的模型。 |
| [^191] | [Can Brain Signals Reveal Inner Alignment with Human Languages?.](http://arxiv.org/abs/2208.06348) | 本研究探索了脑信号和人类语言之间的关系，并介绍了一种名为MTAM的方法，该方法在情感分析和关系检测等下游应用中取得了新的最先进结果。 |
| [^192] | [Seeking the Truth Beyond the Data. An Unsupervised Machine Learning Approach.](http://arxiv.org/abs/2207.06949) | 本文介绍了一种无监督的机器学习方法-聚类，并深入描述了最常用的聚类方法，提供了有关参数选择和初始化的指导。同时，通过比较三个数据集的聚类效率，揭示了不同算法在准确性和复杂性方面的优劣。 |
| [^193] | [PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series.](http://arxiv.org/abs/2206.07940) | PROFHIT是一个概率鲁棒的分层时间序列预测模型，能够提供整个层次结构的预测分布，并引入一种新颖的分布一致性正则化方法。 |
| [^194] | [Category-Agnostic 6D Pose Estimation with Conditional Neural Processes.](http://arxiv.org/abs/2206.07162) | 该论文提出了一种无关类别的6D姿态估计方法，通过神经过程元学习来捕捉对象的纹理和几何形状，并使用几何感知解码器考虑对象的几何约束进行关键点预测。 |
| [^195] | [INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold.](http://arxiv.org/abs/2204.07439) | 本研究提出了一种名为具有实例感知阈值的二值神经网络（INSTA-BNN）的新颖设计，它根据输入相关信息动态调整量化阈值，并通过精心优化，实现了最小的设备开销。实验结果表明，在ImageNet分类任务上，INSTA-BNN比基线方法性能提升了3.0%和2.8%。 |
| [^196] | [Example-based Hypernetworks for Out-of-Distribution Generalization.](http://arxiv.org/abs/2203.14276) | 本文提出了一个基于示例的超网络框架，利用多个源领域的标记数据来进行领域外泛化。该框架通过生成输入示例的唯一签名，并将其嵌入源领域的语义空间中，并利用超网络生成任务分类器的权重。实验结果表明，该方法在29个适应场景中表现优于现有算法，且在输入示例的表示上具有丰富性。同时，与少样本GPT-3进行了比较，证明了其有效性。 |
| [^197] | [Relational Self-Supervised Learning.](http://arxiv.org/abs/2203.08717) | 本文介绍了一种关系式自监督学习（ReSSL）框架，通过建模不同实例之间的关系来学习视觉表示，以弥补当前自监督学习方法中对不同实例关系的缺乏关注。 |
| [^198] | [Deep Discriminative to Kernel Generative Networks for Calibrated Inference.](http://arxiv.org/abs/2201.13001) | 该论文提出了将判别网络转换为生成网络的方法，用高斯核替换多面体中的仿射函数来生成模型，解决了内部和外部数据校准问题，并在 CIFAR-10，CIFAR-100 和 SVHN 等基准数据集上测试了方法的有效性。 |
| [^199] | [Neural networks with linear threshold activations: structure and algorithms.](http://arxiv.org/abs/2111.08117) | 本文研究了具有线性阈值激活函数的神经网络。我们确定了这类神经网络可以表示的函数的特点，并发现使用两个隐藏层可以表示该类中的任何函数。我们还给出了表示这类函数所需神经网络大小的界限，并设计了一个能够解决具有固定架构的这类神经网络的经验风险最小化问题的算法。该算法的运行时间与数据样本大小呈多项式关系，而输入维度和网络架构的大小被认为是固定常数。 |
| [^200] | [Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data.](http://arxiv.org/abs/1811.11479) | 本论文提出了一种在设备上高效通信的机器学习方法，通过联邦蒸馏和增强解决了模型大小和非IID数据的问题。结果表明，与传统的联邦学习相比，该方法在减少通信开销的同时，仍能达到较高的测试准确性。 |

# 详细

[^1]: 个性化的人类移动预测方法及其贡献

    Personalized human mobility prediction for HuMob challenge. (arXiv:2310.12900v1 [cs.LG])

    [http://arxiv.org/abs/2310.12900](http://arxiv.org/abs/2310.12900)

    该论文介绍了一种个性化的人类移动预测方法，基于个体数据而不是整体数据进行预测，并成功运用于HuMob Challenge竞赛中。采用了特征设计和机器学习模型SVR，通过离线评估和特征选择和参数调整，使预测准确性得到验证。

    

    我们介绍了用于创建提交给HuMob Challenge的数据的方法，这是一个用于人类移动预测的数据分析竞赛。我们采用了一个个性化模型基于个体的数据来预测其运动轨迹，而不是基于整体运动的预测，基于人类运动对于每个人而言是独特的假设。我们设计了特征，如日期和时间，活动时间，周几，一天中的时间和POI（兴趣点）访问频率等。作为额外的特征，我们通过聚类来融合具有相似行为模式的其他个体的运动。我们采用的机器学习模型是支持向量回归（SVR）。我们通过离线评估进行准确性检验，并进行特征选择和参数调整。尽管总体数据集包含10万名用户的轨迹，但我们的方法只使用了2万名目标用户的数据，并不需要使用其他8万名用户的数据。

    We explain the methodology used to create the data submitted to HuMob Challenge, a data analysis competition for human mobility prediction. We adopted a personalized model to predict the individual's movement trajectory from their data, instead of predicting from the overall movement, based on the hypothesis that human movement is unique to each person. We devised the features such as the date and time, activity time, days of the week, time of day, and frequency of visits to POI (Point of Interest). As additional features, we incorporated the movement of other individuals with similar behavior patterns through the employment of clustering. The machine learning model we adopted was the Support Vector Regression (SVR). We performed accuracy through offline assessment and carried out feature selection and parameter tuning. Although overall dataset provided consists of 100,000 users trajectory, our method use only 20,000 target users data, and do not need to use other 80,000 data. Despite 
    
[^2]: 盲量子机器学习与量子二分关联器

    Blind quantum machine learning with quantum bipartite correlator. (arXiv:2310.12893v1 [quant-ph])

    [http://arxiv.org/abs/2310.12893](http://arxiv.org/abs/2310.12893)

    该论文介绍了基于量子二分关联器算法的盲量子机器学习协议，提出了具有降低通信开销和保护数据隐私的特点，并引入了低计算开销、不需要复杂加密技术的隐私保护机制。论文通过复杂性和隐私分析验证了协议的有效性，为发展分布式量子计算和隐私感知机器学习应用提供了新的可能性。

    

    分布式量子计算是一种有前途的计算范式，可以执行单个量子设备无法达到的计算。在分布式量子计算中，隐私对于在不可信计算节点存在的情况下维护机密性和保护数据至关重要。在这项工作中，我们引入了基于量子二分关联器算法的新型盲量子机器学习协议。我们的协议在保护数据隐私的同时降低了通信开销。我们引入了具有低计算开销且不需要复杂加密技术的强韧算法特定的隐私保护机制。然后，我们通过复杂性和隐私分析验证了所提议协议的有效性。我们的发现为发展分布式量子计算铺平了道路，为量子技术时代下的隐私感知机器学习应用开辟了新的可能性。

    Distributed quantum computing is a promising computational paradigm for performing computations that are beyond the reach of individual quantum devices. Privacy in distributed quantum computing is critical for maintaining confidentiality and protecting the data in the presence of untrusted computing nodes. In this work, we introduce novel blind quantum machine learning protocols based on the quantum bipartite correlator algorithm. Our protocols have reduced communication overhead while preserving the privacy of data from untrusted parties. We introduce robust algorithm-specific privacy-preserving mechanisms with low computational overhead that do not require complex cryptographic techniques. We then validate the effectiveness of the proposed protocols through complexity and privacy analysis. Our findings pave the way for advancements in distributed quantum computing, opening up new possibilities for privacy-aware machine learning applications in the era of quantum technologies.
    
[^3]: 将生成模型微调作为机器人任务推理方法

    Fine-Tuning Generative Models as an Inference Method for Robotic Tasks. (arXiv:2310.12862v1 [cs.LG])

    [http://arxiv.org/abs/2310.12862](http://arxiv.org/abs/2310.12862)

    本论文提出了一种将生成模型微调应用于机器人任务推理的方法。通过利用现代GPU加速，将神经网络模型的样本生成快速适应到机器人任务观测中。该方法适用于各种深度生成模型和机器人环境，通过将模型迅速微调以适应与观测证据相符的生成样本来实现，使用交叉熵方法进行优化。并且在物体形状推理、逆向运动学计算和点云补全等任务中展示了可行性。

    

    可适应的模型可以极大地改善在真实世界中操作的机器人代理，使其能够处理新颖和多变的情况。虽然诸如贝叶斯推理等方法已成为将模型适应于证据的研究框架，但我们在深度生成模型的最新进展的基础上进行了研究，这些进展已经极大地影响了机器人领域的许多方面。通过利用现代GPU加速，我们研究了如何将神经网络模型的样本生成快速适应到机器人任务观测中。我们提出了一种简单而通用的方法，适用于各种深度生成模型和机器人环境。关键思想是通过使用交叉熵方法，将模型迅速微调以适应与观测证据相符的生成样本。我们展示了我们的方法可应用于自回归模型和变分自动编码器，并展示了它在物体形状推理、逆向运动学计算和点云补全中的可用性。

    Adaptable models could greatly benefit robotic agents operating in the real world, allowing them to deal with novel and varying conditions. While approaches such as Bayesian inference are well-studied frameworks for adapting models to evidence, we build on recent advances in deep generative models which have greatly affected many areas of robotics. Harnessing modern GPU acceleration, we investigate how to quickly adapt the sample generation of neural network models to observations in robotic tasks. We propose a simple and general method that is applicable to various deep generative models and robotic environments. The key idea is to quickly fine-tune the model by fitting it to generated samples matching the observed evidence, using the cross-entropy method. We show that our method can be applied to both autoregressive models and variational autoencoders, and demonstrate its usability in object shape inference from grasping, inverse kinematics calculation, and point cloud completion.
    
[^4]: 非刚性文本提示的音频编辑

    Audio Editing with Non-Rigid Text Prompts. (arXiv:2310.12858v1 [cs.SD])

    [http://arxiv.org/abs/2310.12858](http://arxiv.org/abs/2310.12858)

    本文研究了使用非刚性文本编辑进行音频编辑的方法，并展示了其在保持输入音频一致性方面的优势。

    

    本文探讨了使用非刚性文本编辑进行音频编辑。我们展示了所提出的编辑流程能够创建与输入音频保持一致的音频编辑结果。我们探索了能够进行添加、风格转换和修复的文本提示。我们定量和定性地证明了这些编辑能够优于最近发布的文本提示音频生成模型Audio-LDM的结果。对结果的定性检查表明，我们的方法给出了更加保持输入音频原始起始和结束的编辑结果。

    In this paper, we explore audio-editing with non-rigid text edits. We show that the proposed editing pipeline is able to create audio edits that remain faithful to the input audio. We explore text prompts that perform addition, style transfer, and in-painting. We quantitatively and qualitatively show that the edits are able to obtain results which outperform Audio-LDM, a recently released text-prompted audio generation model. Qualitative inspection of the results points out that the edits given by our approach remain more faithful to the input audio in terms of keeping the original onsets and offsets of the audio events.
    
[^5]: 针对预测不确定性的模型无关变量重要性：一种基于熵的方法

    Model-agnostic variable importance for predictive uncertainty: an entropy-based approach. (arXiv:2310.12842v1 [stat.ML])

    [http://arxiv.org/abs/2310.12842](http://arxiv.org/abs/2310.12842)

    本文提出了一种基于熵的方法，通过扩展现有的解释性方法，可以理解不确定性感知模型中的预测来源和置信度，并利用改编后的特征重要性、部分依赖图和个体条件期望图等方法来测量特征对预测分布的熵和基于真实标签的对数似然的影响。

    

    为了相信机器学习算法的预测结果，必须理解导致这些预测的因素。对于概率和不确定性感知的模型来说，不仅需要理解预测本身的原因，还要理解模型对这些预测的置信度。本文展示了如何将现有的解释性方法扩展到不确定性感知的模型，并如何利用这些扩展来理解模型预测分布中的不确定性来源。特别是通过改编排列特征重要性、部分依赖图和个体条件期望图，我们证明可以获得对模型行为的新见解，并且可以使用这些方法来衡量特征对预测分布的熵和基于该分布的真实标签的对数似然的影响。通过使用两个数据集的实验，我们验证了所提方法的有效性。

    In order to trust the predictions of a machine learning algorithm, it is necessary to understand the factors that contribute to those predictions. In the case of probabilistic and uncertainty-aware models, it is necessary to understand not only the reasons for the predictions themselves, but also the model's level of confidence in those predictions. In this paper, we show how existing methods in explainability can be extended to uncertainty-aware models and how such extensions can be used to understand the sources of uncertainty in a model's predictive distribution. In particular, by adapting permutation feature importance, partial dependence plots, and individual conditional expectation plots, we demonstrate that novel insights into model behaviour may be obtained and that these methods can be used to measure the impact of features on both the entropy of the predictive distribution and the log-likelihood of the ground truth labels under that distribution. With experiments using both s
    
[^6]: 知识增强的语言模型验证

    Knowledge-Augmented Language Model Verification. (arXiv:2310.12836v1 [cs.CL])

    [http://arxiv.org/abs/2310.12836](http://arxiv.org/abs/2310.12836)

    提出了一种知识增强的语言模型验证方法，通过一个独立的验证器来检测模型输出和知识的错误，并通过检索新知识或修正摘要来纠正错误。

    

    最近的语言模型在生成文本方面展现出了令人瞩目的能力，因为它们具备了参数中内部化的知识。然而，由于它们的知识可能是不准确、不完整和过时的，因此语言模型往往会对给定查询生成出事实上不正确的回应。为了解决这个问题，之前的研究提出了利用从外部知识源检索到的知识来增强语言模型的方法。然而，这样的方法经常因为两个原因而显示出次优的文本生成性能：1）模型可能无法检索到与给定查询相关的知识；2）模型可能无法在生成文本中忠实地反映检索到的知识。为了克服这些问题，我们提出了利用一个单独的验证器来验证知识增强的语言模型的输出和知识，这个验证器是一个小型语言模型，通过指导微调的方式训练来检测这两种类型的错误。然后，当验证器检测到错误时，我们可以通过检索新的知识或获取更改摘要来进行修复。

    Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or ge
    
[^7]: AgentTuning: 为LLMs实现通用代理能力

    AgentTuning: Enabling Generalized Agent Abilities for LLMs. (arXiv:2310.12823v1 [cs.CL])

    [http://arxiv.org/abs/2310.12823](http://arxiv.org/abs/2310.12823)

    本论文提出了AgentTuning，一种简单而通用的方法，可提升LLMs的代理能力，同时保持其通用能力。通过构建AgentInstruct数据集，并采用一种混合训练方法，作者成功地实现了提高LLMs代理能力的目标。

    

    开放的大型语言模型（LLMs）在各种任务中具有出色的性能，极大地推动了LLMs的发展。然而，当它们作为代理在现实世界中应对复杂任务时，它们远不及ChatGPT和GPT-4等商业模型。这些代理任务将LLMs作为负责规划、记忆和工具利用的中央控制器，需要细粒度的提示方法和强大的LLMs才能达到令人满意的性能。虽然已经提出了许多提示方法来完成特定的代理任务，但缺乏研究专注于提高LLMs自身的代理能力而不损害其通用能力。在这项工作中，我们提出了AgentTuning，一种简单而通用的方法，可以提升LLMs的代理能力，同时保持其通用的LLM能力。我们构建了AgentInstruct，一个轻量级的指令调整数据集，其中包含高质量的交互轨迹。

    Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hy
    
[^8]: 通过数学优化在基于评分的分类中生成集体反事实解释

    Generating collective counterfactual explanations in score-based classification via mathematical optimization. (arXiv:2310.12822v1 [stat.ML])

    [http://arxiv.org/abs/2310.12822](http://arxiv.org/abs/2310.12822)

    通过数学优化，本文引入了集体反事实解释，为高风险决策场景中的机器学习模型提供了解释工具。与传统的单实例解释不同，该方法针对一组实例提供解释，并通过最小化扰动的总成本来提供最优解释。

    

    随着机器学习模型在高风险决策场景中的增加使用，了解模型如何做出决策变得越来越重要。在经过训练的监督分类模型中，可以通过反事实分析获得解释：一个实例的反事实解释指示应该如何最小程度地修改这个实例，使得被扰动的实例在机器学习模型中被分类到所期望的类别中。大部分反事实分析文献集中在单实例单反事实的情况下，即针对一个单一实例来提供一个单一解释。从利益相关者的角度出发，在本文中我们介绍了所谓的集体反事实解释。通过新颖的数学优化模型，我们为感兴趣的一组实例提供反事实解释，以使扰动的总成本最小。

    Due to the increasing use of Machine Learning models in high stakes decision making settings, it has become increasingly important to have tools to understand how models arrive at decisions. Assuming a trained Supervised Classification model, explanations can be obtained via counterfactual analysis: a counterfactual explanation of an instance indicates how this instance should be minimally modified so that the perturbed instance is classified in the desired class by the Machine Learning classification model. Most of the Counterfactual Analysis literature focuses on the single-instance single-counterfactual setting, in which the analysis is done for one single instance to provide one single explanation. Taking a stakeholder's perspective, in this paper we introduce the so-called collective counterfactual explanations. By means of novel Mathematical Optimization models, we provide a counterfactual explanation for each instance in a group of interest, so that the total cost of the perturb
    
[^9]: 带有完整性保证的高效规划的混合搜索

    Hybrid Search for Efficient Planning with Completeness Guarantees. (arXiv:2310.12819v1 [cs.AI])

    [http://arxiv.org/abs/2310.12819](http://arxiv.org/abs/2310.12819)

    本论文提出了一种混合搜索方法，用于在离散动作空间中进行高效规划，并保证了完整性。通过在高级搜索中添加低级动作，该方法既具有高级搜索的实际效率，又具有低级搜索的完整性。

    

    在计算机科学中，解决复杂的规划问题一直是一个长期存在的挑战。基于学习的子目标搜索方法在处理这些问题上显示出了很大的潜力，但它们经常缺乏完整性保证，这意味着它们可能无法找到解决方案，即使存在一个解决方案。在本文中，我们提出了一种有效的方法来增强子目标搜索方法，以实现在离散动作空间中的完整性。具体地，我们通过在高级搜索中添加低级动作来执行多层次（混合）搜索，我们称之为完整的子目标搜索。这个解决方案实现了高级搜索的实际效率和低级搜索的完整性的最佳结合。我们将所提出的搜索方法应用于最近提出的子目标搜索算法，并评估使用离线数据训练的算法在复杂的规划问题上的表现。我们证明我们的完整子目标搜索不仅保证了完整性，还可以改进性能。

    Solving complex planning problems has been a long-standing challenge in computer science. Learning-based subgoal search methods have shown promise in tackling these problems, but they often suffer from a lack of completeness guarantees, meaning that they may fail to find a solution even if one exists. In this paper, we propose an efficient approach to augment a subgoal search method to achieve completeness in discrete action spaces. Specifically, we augment the high-level search with low-level actions to execute a multi-level (hybrid) search, which we call complete subgoal search. This solution achieves the best of both worlds: the practical efficiency of high-level search and the completeness of low-level search. We apply the proposed search method to a recently proposed subgoal search algorithm and evaluate the algorithm trained on offline data on complex planning problems. We demonstrate that our complete subgoal search not only guarantees completeness but can even improve performan
    
[^10]: 提升推理效率：释放参数共享的预训练语言模型的能力

    Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models. (arXiv:2310.12818v1 [cs.CL])

    [http://arxiv.org/abs/2310.12818](http://arxiv.org/abs/2310.12818)

    本论文提出了一种提升参数共享预训练语言模型推理效率的简单技术，并介绍了一种简单的预训练方法来实现完全或部分共享的模型，实验结果证明了这些方法在各种模型上的有效性，为更有效地利用参数提供了新的见解。

    

    参数共享的预训练语言模型（PLMs）已经成为在资源有限环境中的成功方法，能够在不显著降低性能的情况下实现模型存储和内存成本的大幅降低。然而，需要注意的是，参数共享不能减轻推理过程中的计算负担，这使得在具有严格时延要求或计算资源受限的情况下，其实用性受到限制。基于神经常微分方程（ODEs），我们引入了一种简单的技术来提高参数共享的PLMs的推理效率。此外，我们提出了一种简单的预训练技术，可以实现完全或部分共享的模型，从而实现更大的推理加速。实验结果表明，我们的方法对于自回归和自编码PLMs都具有很好的效果，为更有效地利用参数提供了新的见解。

    Parameter-shared pre-trained language models (PLMs) have emerged as a successful approach in resource-constrained environments, enabling substantial reductions in model storage and memory costs without significant performance compromise. However, it is important to note that parameter sharing does not alleviate computational burdens associated with inference, thus impeding its practicality in situations characterized by limited stringent latency requirements or computational resources. Building upon neural ordinary differential equations (ODEs), we introduce a straightforward technique to enhance the inference efficiency of parameter-shared PLMs. Additionally, we propose a simple pre-training technique that leads to fully or partially shared models capable of achieving even greater inference acceleration. The experimental results demonstrate the effectiveness of our methods on both autoregressive and autoencoding PLMs, providing novel insights into more efficient utilization of paramet
    
[^11]: 基于场景级监督的点云分割的2D-3D交错Transformer模型

    2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision. (arXiv:2310.12817v1 [cs.CV])

    [http://arxiv.org/abs/2310.12817](http://arxiv.org/abs/2310.12817)

    本文提出了一种基于场景级监督的2D-3D交错Transformer模型，用于弱监督点云分割。该模型通过两个编码器计算2D和3D数据的自注意特征，并通过交替切换查询和键值对的角色，实现了2D和3D特征的融合。

    

    本文提出了一种多模态交错Transformer模型（MIT），用于考虑2D和3D数据进行弱监督点云分割。研究表明，2D和3D特征在点云分割中互补。然而，现有方法需要额外的2D注释来实现2D-3D信息融合。鉴于点云的高注释成本，基于弱监督学习的有效2D和3D特征融合需求非常迫切。为此，我们提出了一个具有两个编码器和一个解码器的Transformer模型，仅使用场景级类标签进行弱监督点云分割。具体而言，两个编码器分别计算3D点云和2D多视图图像的自注意特征。解码器实现交错的2D-3D交叉注意力，并进行隐式2D和3D特征融合。我们在解码器层中交替切换查询和键值对的角色。实验证明，2D和3D特征是互补的。

    We present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud segmentation. However, existing methods require extra 2D annotations to achieve 2D-3D information fusion. Considering the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly supervised point cloud segmentation using only scene-level class tags. Specifically, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queries and key-value pairs in the decoder layers. It turns out that the 2D and 3D features are 
    
[^12]: LLM-集成应用中的提示注入攻击和防御

    Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])

    [http://arxiv.org/abs/2310.12815](http://arxiv.org/abs/2310.12815)

    本文提出了一个通用框架来形式化提示注入攻击，并系统化防御这种类型的攻击。

    

    大型语言模型（LLMs）越来越多地用作各种称为LLM-集成应用的实际应用程序的后端。最近的多项研究表明，LLM-集成应用容易受到提示注入攻击的威胁，攻击者可以将恶意指令/数据注入这些应用程序的输入中，以达到攻击者的预期结果。然而，现有的研究仅限于案例研究，缺乏对提示注入攻击及其防御的系统理解。本论文旨在填补这一空白。我们提出了一个通用框架来形式化提示注入攻击，并将研究论文和博客文章中讨论的现有攻击视为我们框架的特例。我们的框架使我们能够通过组合现有攻击设计新的攻击方式。此外，我们还提出了一个系统化提示注入攻击防御的框架。利用我们的框架，我们可以预防和缓解这种类型的攻击。

    Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
    
[^13]: 大规模层次预测

    Hierarchical Forecasting at Scale. (arXiv:2310.12809v1 [cs.LG])

    [http://arxiv.org/abs/2310.12809](http://arxiv.org/abs/2310.12809)

    提出了一种大规模层次预测的方法，使用稀疏损失函数直接优化层次产品和/或时间结构，从而为数百万个时间序列提供一致的预测。在实验中，该方法在M5数据集上表现出10%的性能提升。

    

    当时间序列的数量增加时，现有的层次预测技术的扩展性较差。我们提出使用稀疏损失函数来学习千万个时间序列的一致预测，该损失函数直接优化层次产品和/或时间结构。我们稀疏层次损失函数的优点是提供了一种方法，使实践者能够产生与任何选择的横向或时间层次一致的底层预测。此外，消除传统层次预测技术中需要的后处理步骤，减少了预测流程中的计算成本。在公开的M5数据集上，与基准损失函数相比，我们的稀疏层次损失函数性能提高了10%（RMSE）。我们将稀疏层次损失函数实现在bol这个大型欧洲电子商务平台上现存的预测模型中。

    Existing hierarchical forecasting techniques scale poorly when the number of time series increases. We propose to learn a coherent forecast for millions of time series with a single bottom-level forecast model by using a sparse loss function that directly optimizes the hierarchical product and/or temporal structure. The benefit of our sparse hierarchical loss function is that it provides practitioners a method of producing bottom-level forecasts that are coherent to any chosen cross-sectional or temporal hierarchy. In addition, removing the need for a post-processing step as required in traditional hierarchical forecasting techniques reduces the computational cost of the prediction phase in the forecasting pipeline. On the public M5 dataset, our sparse hierarchical loss function performs up to 10% (RMSE) better compared to the baseline loss function. We implement our sparse hierarchical loss function within an existing forecasting model at bol, a large European e-commerce platform, res
    
[^14]: 基于不确定性梯度匹配的模型合并

    Model Merging by Uncertainty-Based Gradient Matching. (arXiv:2310.12808v1 [cs.LG])

    [http://arxiv.org/abs/2310.12808](http://arxiv.org/abs/2310.12808)

    本论文通过不确定性梯度匹配的方法，提出了一种新的模型合并方案，该方案能够减少梯度不匹配，从而提高了模型合并的性能并对超参数更具鲁棒性。

    

    在不同数据集上训练的模型可以通过参数的加权平均来合并，但为什么会起作用，什么情况下会失败？在这里，我们将加权平均的不准确性与梯度不匹配联系起来，并提出了一种新的基于不确定性的方案，通过减少不匹配来提高性能。这种联系还揭示了其他方案（如平均值、任务算术和Fisher加权平均）中的隐含假设。我们的新方法在大型语言模型和视觉转换器方面都在性能和超参数鲁棒性方面得到了一致的改进。

    Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch. The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and Fisher-weighted averaging. Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.
    
[^15]: DCSI -- 基于分离和连通性的改进的聚类可分离性度量

    DCSI -- An improved measure of cluster separability based on separation and connectedness. (arXiv:2310.12806v1 [stat.ML])

    [http://arxiv.org/abs/2310.12806](http://arxiv.org/abs/2310.12806)

    这篇论文提出了一种改进的聚类可分离性度量方法，旨在量化类间分离和类内连通性，对于密度聚类具有较好的性能表现。

    

    确定给定数据集中的类别标签是否对应于有意义的聚类对于使用真实数据集评估聚类算法至关重要。这个特性可以通过可分离性度量来量化。现有文献的综述显示，既有的基于分类的复杂性度量方法和聚类有效性指标 (CVIs) 都没有充分融入基于密度的聚类的核心特征：类间分离和类内连通性。一种新开发的度量方法 (密度聚类可分离性指数, DCSI) 旨在量化这两个特征，并且也可用作 CVI。对合成数据的广泛实验表明，DCSI 与通过调整兰德指数 (ARI) 测量的DBSCAN的性能之间有很强的相关性，但在对多类数据集进行密度聚类不适当的重叠类别时缺乏鲁棒性。对经常使用的真实数据集进行详细评估显示，DCSI 能够更好地区分密度聚类的可分离性。

    Whether class labels in a given data set correspond to meaningful clusters is crucial for the evaluation of clustering algorithms using real-world data sets. This property can be quantified by separability measures. A review of the existing literature shows that neither classification-based complexity measures nor cluster validity indices (CVIs) adequately incorporate the central aspects of separability for density-based clustering: between-class separation and within-class connectedness. A newly developed measure (density cluster separability index, DCSI) aims to quantify these two characteristics and can also be used as a CVI. Extensive experiments on synthetic data indicate that DCSI correlates strongly with the performance of DBSCAN measured via the adjusted rand index (ARI) but lacks robustness when it comes to multi-class data sets with overlapping classes that are ill-suited for density-based hard clustering. Detailed evaluation on frequently used real-world data sets shows that
    
[^16]: 机器学习中偏倚特征的检测和评估

    Detection and Evaluation of bias-inducing Features in Machine learning. (arXiv:2310.12805v1 [cs.LG])

    [http://arxiv.org/abs/2310.12805](http://arxiv.org/abs/2310.12805)

    本论文介绍了一种在机器学习中检测和评估偏倚特征的方法，通过因果分析来理解系统偏倚行为的原因，并且可以准确识别潜在的偏倚特征，即使它们最初是未知的。

    

    因果分析可以帮助我们解析问题的所有可能原因，如不良的商业情况或对个人的意外伤害。这意味着我们可以确定问题的继承方式，对原因进行排序以帮助确定修复优先级，简化复杂问题并进行可视化。在机器学习（ML）的背景下，可以使用因果分析来理解系统偏倚行为的原因。例如，我们可以通过检查每个特征是否对模型中的偏见有潜在影响来检查偏见的根本原因。为了实现这一点，可以对给定的特征或一对特征进行微小修改，并遵循一些准则观察它对模型决策（即模型预测）的影响。因此，即使这些特征最初是未知的，我们也可以使用因果分析来确定潜在的引起偏倚特征。这很重要，因为大多数当前的方法要求事先知道这些特征。

    The cause-to-effect analysis can help us decompose all the likely causes of a problem, such as an undesirable business situation or unintended harm to the individual(s). This implies that we can identify how the problems are inherited, rank the causes to help prioritize fixes, simplify a complex problem and visualize them. In the context of machine learning (ML), one can use cause-to-effect analysis to understand the reason for the biased behavior of the system. For example, we can examine the root causes of biases by checking each feature for a potential cause of bias in the model. To approach this, one can apply small changes to a given feature or a pair of features in the data, following some guidelines and observing how it impacts the decision made by the model (i.e., model prediction). Therefore, we can use cause-to-effect analysis to identify the potential bias-inducing features, even when these features are originally are unknown. This is important since most current methods req
    
[^17]: 可微分的顶点拟合用于喷注标签

    Differentiable Vertex Fitting for Jet Flavour Tagging. (arXiv:2310.12804v1 [hep-ex])

    [http://arxiv.org/abs/2310.12804](http://arxiv.org/abs/2310.12804)

    本论文提出了一种可微分的顶点拟合算法，可应用于次级顶点拟合，并与神经网络无缝集成，用于喷注标签。这一方法将物理知识与神经网络模型相结合，可以改善重味道喷注分类。

    

    我们提出了一种可微分的顶点拟合算法，可用于次级顶点拟合，并可无缝集成到神经网络中，用于喷注标签。顶点拟合被制定为一个优化问题，其中优化的解顶点的梯度通过隐式微分定义，并可以传递给上游或下游神经网络组件进行网络训练。更广泛地说，这是将物理知识集成到高能物理中神经网络模型中的一个可微分编程应用。我们展示了如何将可微分次级顶点拟合集成到基于Transformer的大模型中，以改善重味道喷注分类。

    We propose a differentiable vertex fitting algorithm that can be used for secondary vertex fitting, and that can be seamlessly integrated into neural networks for jet flavour tagging. Vertex fitting is formulated as an optimization problem where gradients of the optimized solution vertex are defined through implicit differentiation and can be passed to upstream or downstream neural network components for network training. More broadly, this is an application of differentiable programming to integrate physics knowledge into neural network models in high energy physics. We demonstrate how differentiable secondary vertex fitting can be integrated into larger transformer-based models for flavour tagging and improve heavy flavour jet classification.
    
[^18]: 基于因果结构的文本离群值泛化增强方法

    Causal-structure Driven Augmentations for Text OOD Generalization. (arXiv:2310.12803v1 [cs.LG])

    [http://arxiv.org/abs/2310.12803](http://arxiv.org/abs/2310.12803)

    本文提出了一种基于因果结构的反事实数据增强方法，用于改善文本分类器在应用中的泛化效果，特别适用于存在虚假相关性的标签与属性预测问题。

    

    文本分类器对虚假相关性的依赖可能导致在实际应用中的泛化效果不佳，这引发了对其在如医疗领域等安全关键行业中使用的担忧。在本研究中，我们提出使用因果结构知识指导的反事实数据增强方法，模拟对虚假特征进行干预，以学习更加鲁棒的文本分类器。我们证明了在标签与属性之间存在虚假相关性的预测问题中，这种策略是合适的。在这种问题的假设下，我们讨论了反事实数据增强相对于重要性重加权的有利样本复杂性。实际上，我们使用辅助数据通过差分在差分的方法来匹配样本，并使用大型语言模型（LLM）来表示文本的条件概率。通过对从医学叙述中学习与看护者无关的临床诊断预测器以及半合成数据上进行了广泛的实验。

    The reliance of text classifiers on spurious correlations can lead to poor generalization at deployment, raising concerns about their use in safety-critical domains such as healthcare. In this work, we propose to use counterfactual data augmentation, guided by knowledge of the causal structure of the data, to simulate interventions on spurious features and to learn more robust text classifiers. We show that this strategy is appropriate in prediction problems where the label is spuriously correlated with an attribute. Under the assumptions of such problems, we discuss the favorable sample complexity of counterfactual data augmentation, compared to importance re-weighting. Pragmatically, we match examples using auxiliary data, based on diff-in-diff methodology, and use a large language model (LLM) to represent a conditional probability of text. Through extensive experimentation on learning caregiver-invariant predictors of clinical diagnoses from medical narratives and on semi-synthetic 
    
[^19]: 一种集体深度学习的有效理论

    An effective theory of collective deep learning. (arXiv:2310.12802v1 [physics.soc-ph])

    [http://arxiv.org/abs/2310.12802](http://arxiv.org/abs/2310.12802)

    通过竞争局部学习和单元之间的扩散耦合，我们引入了一个简化模型，预测了集体学习中的无序-有序-无序相变，并验证了这个理论。

    

    揭示耦合的人工神经网络系统中集体学习的出现是对物理学、机器学习、神经科学和社会学的广泛影响的一项努力。我们引入了一个简化模型，通过考虑各个神经网络单元参数的局部学习动态和单元之间的扩散耦合之间的竞争，将几个最近的分散算法进行了压缩。我们通过一个与具有淬灭随机性的Ginzburg-Landau模型类似的线性网络的有效理论，推导出了我们模型的粗粒化行为。这个框架预测了参数解的（深度依赖的）无序-有序-无序相变，揭示了集体学习相的开始，以及深度引起的临界点延迟和微观学习路径的鲁棒形状。我们在现实中验证了我们的理论。

    Unraveling the emergence of collective learning in systems of coupled artificial neural networks is an endeavor with broader implications for physics, machine learning, neuroscience and society. Here we introduce a minimal model that condenses several recent decentralized algorithms by considering a competition between two terms: the local learning dynamics in the parameters of each neural network unit, and a diffusive coupling among units that tends to homogenize the parameters of the ensemble. We derive the coarse-grained behavior of our model via an effective theory for linear networks that we show is analogous to a deformed Ginzburg-Landau model with quenched disorder. This framework predicts (depth-dependent) disorder-order-disorder phase transitions in the parameters' solutions that reveal the onset of a collective learning phase, along with a depth-induced delay of the critical point and a robust shape of the microscopic learning path. We validate our theory in realistic ensembl
    
[^20]: 探索图神经网络在印度法律判决预测中的应用

    Exploring Graph Neural Networks for Indian Legal Judgment Prediction. (arXiv:2310.12800v1 [cs.LG])

    [http://arxiv.org/abs/2310.12800](http://arxiv.org/abs/2310.12800)

    本研究探索了使用图神经网络来解决印度法律判决预测问题，通过识别司法案件的图结构并进行节点分类，以实现自动化推测案件结果。研究考虑了模型特征、公平性和链接预测任务。

    

    不平衡的法官与案件比例对司法系统产生了繁重的影响，表现为大量积压的未决案件以及持续涌入的新案件。为了解决这个问题并加快司法程序，提出了利用事实证据和过去案件的先例来推测案件结果的自动化系统的建议变得重要。本研究论文侧重于开发基于图神经网络的模型来解决法律判决预测（LJP）问题，识别司法案件的内在图结构，并将其转化为二进制节点分类问题。我们探索了各种嵌入作为模型特征，同时添加和修剪了时间节点和司法行为节点以评估模型的性能。本研究还考虑了这些预测的公平性的伦理维度，考虑了性别和姓名的偏见。还进行了链接预测任务以评估模型对未来案件的熟练程度。

    The burdensome impact of a skewed judges-to-cases ratio on the judicial system manifests in an overwhelming backlog of pending cases alongside an ongoing influx of new ones. To tackle this issue and expedite the judicial process, the proposition of an automated system capable of suggesting case outcomes based on factual evidence and precedent from past cases gains significance. This research paper centres on developing a graph neural network-based model to address the Legal Judgment Prediction (LJP) problem, recognizing the intrinsic graph structure of judicial cases and making it a binary node classification problem. We explored various embeddings as model features, while nodes such as time nodes and judicial acts were added and pruned to evaluate the model's performance. The study is done while considering the ethical dimension of fairness in these predictions, considering gender and name biases. A link prediction task is also conducted to assess the model's proficiency in anticipati
    
[^21]: OODRobustBench: 在分布迁移下评估和分析对抗性鲁棒性的基准

    OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift. (arXiv:2310.12793v1 [cs.LG])

    [http://arxiv.org/abs/2310.12793](http://arxiv.org/abs/2310.12793)

    OODRobustBench是一个基准，用于评估和分析在分布迁移下的对抗性鲁棒性。大规模分析结果表明，对抗性鲁棒性在离群分布测试下存在严重的泛化问题，而内分布鲁棒性与离群分布鲁棒性呈强正线性相关。

    

    现有的研究在提高对抗性鲁棒性方面取得了很大进展，但通常只在与训练数据相同分布的数据上进行测试，即内分布（ID）测试。因此，目前尚不清楚这种鲁棒性在输入分布迁移，即离群分布（OOD）测试下的泛化性能。在实际部署时，由于这种分布迁移是不可避免的，这一问题十分令人担忧。为了解决这个问题，我们提出了一个名为OODRobustBench的基准，通过使用23个基于数据集的迁移（即输入分布的自然迁移）和6个基于威胁的迁移（即未知的对抗性威胁模型）来全面评估OOD的对抗性鲁棒性。OODRobustBench用于评估了706个鲁棒模型，并进行了60.7K次对抗性评估。这个大规模分析表明：1）对抗性鲁棒性存在严重的OOD泛化问题；2）ID鲁棒性与OOD鲁棒性呈强正线性相关。

    Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear wa
    
[^22]: 一种理论方法来表征准确性和公平性之间的权衡帕累托前沿

    A Theoretical Approach to Characterize the Accuracy-Fairness Trade-off Pareto Frontier. (arXiv:2310.12785v1 [cs.LG])

    [http://arxiv.org/abs/2310.12785](http://arxiv.org/abs/2310.12785)

    本研究通过建立一个理论框架，对公平性和准确性之间的权衡帕累托前沿进行了建模和分析，并发现了一些有见地的发现。

    

    在公平机器学习的文献中，我们经常观察到准确性和公平性之间的权衡。然而，对于这一长期存在的挑战，缺乏严格的理论分析。为了揭示这个挑战，本研究通过对准确性和公平性之间的权衡帕累托前沿进行建模，发展了一个理论框架。准确性和公平性之间的权衡帕累托前沿是由一组所有最优帕累托分类器组成的，没有其他分类器能够支配。具体地，我们首先在现实场景中证明了该权衡的存在，然后提出了四个潜在的分类来表征准确性和公平性帕累托前沿的重要特性。对于每个分类，我们确定了导致相应权衡的必要条件。在合成数据的实验结果中，我们对所提出的框架得出了有见地的发现：（1）当敏感属性可以完全通过非敏感属性进行解释时，权衡帕累托前沿大多是连续的。（2）准确性可能会受到...

    While the accuracy-fairness trade-off has been frequently observed in the literature of fair machine learning, rigorous theoretical analyses have been scarce. To demystify this long-standing challenge, this work seeks to develop a theoretical framework by characterizing the shape of the accuracy-fairness trade-off Pareto frontier (FairFrontier), determined by a set of all optimal Pareto classifiers that no other classifiers can dominate. Specifically, we first demonstrate the existence of the trade-off in real-world scenarios and then propose four potential categories to characterize the important properties of the accuracy-fairness Pareto frontier. For each category, we identify the necessary conditions that lead to corresponding trade-offs. Experimental results on synthetic data suggest insightful findings of the proposed framework: (1) When sensitive attributes can be fully interpreted by non-sensitive attributes, FairFrontier is mostly continuous. (2) Accuracy can suffer a \textit{
    
[^23]: 从隐私保护数据中进行条件密度估计

    Conditional Density Estimations from Privacy-Protected Data. (arXiv:2310.12781v1 [stat.ML])

    [http://arxiv.org/abs/2310.12781](http://arxiv.org/abs/2310.12781)

    本文提出了一种从隐私保护数据中进行条件密度估计的方法，使用神经条件密度估计器来近似模型参数的后验分布，从而解决了在统计分析过程中只能访问私有化数据导致的计算复杂度增加的问题。

    

    许多现代统计分析和机器学习应用需要在敏感用户数据上进行模型训练。差分隐私提供了一种正式的保证，即个体用户信息不会泄露。在这个框架下，随机算法向保密数据注入校准的噪声，从而产生隐私保护的数据集或查询。然而，在统计分析过程中只能访问私有化数据会导致计算复杂度增加，难以对基础机密数据的参数进行有效的推理。在本工作中，我们提出了基于隐私保护数据集的基于模拟的推理方法。具体而言，我们使用神经条件密度估计器作为一组灵活的分布来近似给定观测到的私有查询结果的模型参数的后验分布。我们在传染病模型下的离散时间序列数据以及普通线性回归模型上说明了我们的方法。

    Many modern statistical analysis and machine learning applications require training models on sensitive user data. Differential privacy provides a formal guarantee that individual-level information about users does not leak. In this framework, randomized algorithms inject calibrated noise into the confidential data, resulting in privacy-protected datasets or queries. However, restricting access to only the privatized data during statistical analysis makes it computationally challenging to perform valid inferences on parameters underlying the confidential data. In this work, we propose simulation-based inference methods from privacy-protected datasets. Specifically, we use neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. We illustrate our methods on discrete time-series data under an infectious disease model and on ordinary linear regression models. Illustra
    
[^24]: 使用标签感知的自动语言表达方式提供少样本文本分类

    Label-Aware Automatic Verbalizer for Few-Shot Text Classification. (arXiv:2310.12778v1 [cs.CL])

    [http://arxiv.org/abs/2310.12778](http://arxiv.org/abs/2310.12778)

    这篇论文提出了一种标签感知的自动语言表达方式（LAAV），通过增加手动标签和连接词“和”来提升语言模型的生成，从而在少样本分类任务中取得更好的结果。

    

    基于提示的学习在少样本文本分类中显示出有效性。其成功的一个重要因素是一种语言表达方式，将语言模型的输出转化为预测类别。值得注意的是，最简单和广泛认可的语言表达方式使用手动标签来表示类别。然而，手动选择并不能保证在选择的语言模型的条件下所选择的单词的最优性。因此，我们提出了一种标签感知的自动语言表达方式（LAAV），通过有效增加手动标签来实现更好的少样本分类结果。具体而言，我们使用手动标签以及连接词“和”来诱导模型生成更有效的语言表达方式中的单词。在五个数据集跨五种语言上的实验结果表明，LAAV明显优于现有的语言表达方式。此外，我们的分析还发现，与类似方法相比，LAAV提供更相关的单词，特别是在中到

    Prompt-based learning has shown its effectiveness in few-shot text classification. One important factor in its success is a verbalizer, which translates output from a language model into a predicted class. Notably, the simplest and widely acknowledged verbalizer employs manual labels to represent the classes. However, manual selection does not guarantee the optimality of the selected words when conditioned on the chosen language model. Therefore, we propose Label-Aware Automatic Verbalizer (LAAV), effectively augmenting the manual labels to achieve better few-shot classification results. Specifically, we use the manual labels along with the conjunction "and" to induce the model to generate more effective words for the verbalizer. The experimental results on five datasets across five languages demonstrate that LAAV significantly outperforms existing verbalizers. Furthermore, our analysis reveals that LAAV suggests more relevant words compared to similar approaches, especially in mid-to-
    
[^25]: 存活最有影响力的提示：通过聚类和修剪实现高效的黑盒提示搜索

    Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning. (arXiv:2310.12774v1 [cs.CL])

    [http://arxiv.org/abs/2310.12774](http://arxiv.org/abs/2310.12774)

    本文提出了一种名为ClaPS的简单黑盒搜索方法，通过聚类和修剪搜索空间中最有影响力的提示令牌，解决了现代黑盒方法中的效率问题。

    

    基于提示的学习已经成为大型预训练语言模型（LLM）的有效范例，使得少样本甚至零样本学习成为可能。最近，黑盒提示搜索因其梯度-free优化的独特特性而受到越来越多的关注，被证明在模型即服务的使用中特别有用和强大。然而，组合优化的离散本质和复杂性阻碍了现代黑盒方法的效率。尽管在搜索算法上进行了广泛研究，但搜索空间设计和优化的关键方面却被大部分忽视了。在本文中，我们首先通过提示LLM进行敏感性分析，揭示只有少量的令牌对LLM预测产生了不成比例的影响。利用这一洞见，我们提出了一种名为Clustering and Pruning for Efficient Black-box Prompt Search（ClaPS）的简单黑盒搜索方法，该方法首先对搜索空间进行聚类和修剪，只关注最具影响力的提示令牌。

    Prompt-based learning has been an effective paradigm for large pretrained language models (LLM), enabling few-shot or even zero-shot learning. Black-box prompt search has received growing interest recently for its distinctive properties of gradient-free optimization, proven particularly useful and powerful for model-as-a-service usage. However, the discrete nature and the complexity of combinatorial optimization hinder the efficiency of modern black-box approaches. Despite extensive research on search algorithms, the crucial aspect of search space design and optimization has been largely overlooked. In this paper, we first conduct a sensitivity analysis by prompting LLM, revealing that only a small number of tokens exert a disproportionate amount of influence on LLM predictions. Leveraging this insight, we propose the Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple black-box search method that first clusters and prunes the search space to focus exclusivel
    
[^26]: 安全RLHF：从人类反馈中进行安全强化学习

    Safe RLHF: Safe Reinforcement Learning from Human Feedback. (arXiv:2310.12773v1 [cs.AI])

    [http://arxiv.org/abs/2310.12773](http://arxiv.org/abs/2310.12773)

    我们提出了一种名为安全RLHF的算法，用于在大型语言模型的训练过程中平衡性能和安全性。它通过解耦人类偏好，并训练分别的奖励和成本模型，成功解决了有益和无害目标之间的固有张力，并通过动态调整平衡来优化算法性能。

    

    随着大型语言模型（LLM）的发展，平衡AI系统的性能和安全性变得更加关键。然而，LLM训练过程中的有益和无害目标之间的固有张力在很大程度上增加了挑战。为了解决这个问题，我们提出了安全RLHF：一种用于人类价值对齐的新颖算法。安全RLHF明确解耦了关于有益性和无害性的人类偏好，有效避免了众包工作者对张力的困惑，并允许我们训练分别的奖励和成本模型。我们将LLM的安全问题形式化为一个优化任务，即在满足指定成本约束的同时最大化奖励函数。通过利用Lagrangian方法解决这个约束问题，安全RLHF在精调过程中动态调整两个目标之间的平衡。通过三轮使用安全RLHF进行精调，我们得到了一个安全且具有优良性能的AI系统。

    With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we d
    
[^27]: 随机平均梯度：一项简单的实证研究

    Stochastic Average Gradient : A Simple Empirical Investigation. (arXiv:2310.12771v1 [cs.LG])

    [http://arxiv.org/abs/2310.12771](http://arxiv.org/abs/2310.12771)

    本研究通过对比SAG与机器学习中一些标准优化器，旨在将随机方法的成本与确定性方法的收敛速度结合起来。

    

    尽管神经网络的理论研究和实证成功近年来取得了显著进展，但梯度反向传播仍然是训练这些网络的最常用算法。一方面，我们有确定性或全梯度（FG）方法，其成本与所使用的训练数据量成正比，但收敛速度为线性；另一方面，我们有随机梯度（SG）方法，其成本与数据集大小无关，但收敛速度不如确定性方法理想。为了将随机方法的成本与确定性方法的收敛速度结合起来，提出了随机平均梯度（SAG）方法。SAG是一种用于优化有限个平滑凸函数之和的方法。与SG方法一样，SAG方法的迭代成本与求和中的项数无关。在这项工作中，我们提出将SAG与机器学习中一些标准优化器进行比较。

    Despite the recent growth of theoretical studies and empirical successes of neural networks, gradient backpropagation is still the most widely used algorithm for training such networks. On the one hand, we have deterministic or full gradient (FG) approaches that have a cost proportional to the amount of training data used but have a linear convergence rate, and on the other hand, stochastic gradient (SG) methods that have a cost independent of the size of the dataset, but have a less optimal convergence rate than the determinist approaches. To combine the cost of the stochastic approach with the convergence rate of the deterministic approach, a stochastic average gradient (SAG) has been proposed. SAG is a method for optimizing the sum of a finite number of smooth convex functions. Like SG methods, the SAG method's iteration cost is independent of the number of terms in the sum. In this work, we propose to compare SAG to some standard optimizers used in machine learning. SAG converges f
    
[^28]: SemantIC: 语义干扰消除在6G无线通信中的应用

    SemantIC: Semantic Interference Cancellation Towards 6G Wireless Communications. (arXiv:2310.12768v1 [eess.SP])

    [http://arxiv.org/abs/2310.12768](http://arxiv.org/abs/2310.12768)

    本文提出了一种新颖的技术，语义干扰消除（SemantIC），用于提高6G无线通信网络的信息质量。通过在接收器上使用语义自动编码器，SemantIC能够迭代消除信号中的噪声和语义干扰。仿真结果表明，SemantIC能够在不增加额外信道资源成本的情况下改进性能。

    

    本文提出了一种新颖的抗干扰技术，即语义干扰消除（SemantIC），用于提高第六代（6G）无线网络中的信息质量。SemantIC只需要接收器将信道解码器与语义自动编码器连接起来。这构建了一个迭代循环，交替消除信号域和语义域中的噪声。从网络信息论的角度来看，语义自动编码器的神经网络通过训练存储了辅助信息，并在迭代解码中提供辅助信息，作为Wyner-Ziv定理的一种实现。仿真结果验证了SemantIC在不增加额外信道资源成本的情况下改进了性能。

    This letter proposes a novel anti-interference technique, semantic interference cancellation (SemantIC), for enhancing information quality towards the sixth-generation (6G) wireless networks. SemantIC only requires the receiver to concatenate the channel decoder with a semantic auto-encoder. This constructs a turbo loop which iteratively and alternately eliminates noise in the signal domain and the semantic domain. From the viewpoint of network information theory, the neural network of the semantic auto-encoder stores side information by training, and provides side information in iterative decoding, as an implementation of the Wyner-Ziv theorem. Simulation results verify the performance improvement by SemantIC without extra channel resource cost.
    
[^29]: 基于Transformer的实体法律形式分类

    Transformer-based Entity Legal Form Classification. (arXiv:2310.12766v1 [cs.CL])

    [http://arxiv.org/abs/2310.12766](http://arxiv.org/abs/2310.12766)

    提出了一种使用Transformer-based语言模型进行实体法律形式分类的方法，该方法在比较中表现出较高的性能并得到了第三方评审的支持。

    

    我们提出了使用基于Transformer的语言模型来对原始法律实体名称进行实体法律形式分类的方法。具体而言，我们采用了各种BERT变种，并将它们的性能与多个传统基准进行比较。我们的评估涵盖了一个庞大的自由可用的法律实体标识符（LEI）数据子集，包括来自30个不同法律司法辖区的超过110万个法律实体。每个司法辖区的分类的真实标签来自实体法律形式（ELF）代码标准（ISO 20275）。我们的研究结果表明，预训练的BERT变种在F1分数方面优于传统的文本分类方法，在Macro F1分数方面表现相当好。此外，我们的提案得到了在十个选择的司法辖区进行的第三方专家评审的支持。本研究凸显了基于Transformer模型在推进数据标准化方面的重要潜力。

    We propose the application of Transformer-based language models for classifying entity legal forms from raw legal entity names. Specifically, we employ various BERT variants and compare their performance against multiple traditional baselines. Our evaluation encompasses a substantial subset of freely available Legal Entity Identifier (LEI) data, comprising over 1.1 million legal entities from 30 different legal jurisdictions. The ground truth labels for classification per jurisdiction are taken from the Entity Legal Form (ELF) code standard (ISO 20275). Our findings demonstrate that pre-trained BERT variants outperform traditional text classification approaches in terms of F1 score, while also performing comparably well in the Macro F1 Score. Moreover, the validity of our proposal is supported by the outcome of third-party expert reviews conducted in ten selected jurisdictions. This study highlights the significant potential of Transformer-based models in advancing data standardization
    
[^30]: 基于能量的模型用于语音合成

    Energy-Based Models For Speech Synthesis. (arXiv:2310.12765v1 [cs.SD])

    [http://arxiv.org/abs/2310.12765](http://arxiv.org/abs/2310.12765)

    本文引入了一种新的非自回归模型，即基于能量的模型 (EBMs)，通过噪声对比估计进行训练。该模型可以使用 Langevin MCMC 进行采样，并在语音合成任务中取得了改进。

    

    最近对于非自回归 (non-AR) 模型在语音合成领域引起了很多关注，比如 FastSpeech 2 和扩散模型。与自回归模型不同的是，这些模型的输出之间没有自回归的依赖关系，这使得推断过程更加高效。本文通过引入基于能量的模型 (EBMs) 来拓展非自回归模型的范围。论文描述了如何使用噪声对比估计来训练基于能量的模型，噪声对比估计依赖于正样本和负样本之间的比较。论文提出了一些生成有效负样本的策略，包括使用表现优秀的自回归模型。论文还描述了如何使用 Langevin Markov Chain Monte-Carlo (MCMC) 从基于能量的模型中进行采样。使用 Langevin MCMC 可以建立基于能量的模型与当前流行的扩散模型之间的联系。在 LJSpeech 数据集上的实验证明，所提出的方法相对于 Tacotron 2 有改进。

    Recently there has been a lot of interest in non-autoregressive (non-AR) models for speech synthesis, such as FastSpeech 2 and diffusion models. Unlike AR models, these models do not have autoregressive dependencies among outputs which makes inference efficient. This paper expands the range of available non-AR models with another member called energy-based models (EBMs). The paper describes how noise contrastive estimation, which relies on the comparison between positive and negative samples, can be used to train EBMs. It proposes a number of strategies for generating effective negative samples, including using high-performing AR models. It also describes how sampling from EBMs can be performed using Langevin Markov Chain Monte-Carlo (MCMC). The use of Langevin MCMC enables to draw connections between EBMs and currently popular diffusion models. Experiments on LJSpeech dataset show that the proposed approach offers improvements over Tacotron 2.
    
[^31]: 通过非启发式算法将谱聚类的松弛解离散化

    Discretize Relaxed Solution of Spectral Clustering via a Non-Heuristic Algorithm. (arXiv:2310.12752v1 [cs.LG])

    [http://arxiv.org/abs/2310.12752](http://arxiv.org/abs/2310.12752)

    通过非启发式算法将谱聚类的松弛解离散化，以寻找最小化原始目标函数的离散解，并理论上证明了连续最优解的优势。

    

    谱聚类及其扩展通常包括两个步骤：（1）构建图并计算松弛解；（2）离散化松弛解。尽管前者已经得到广泛研究，但离散化技术主要是启发式方法，如k-means，谱旋转。不幸的是，现有方法的目标并非寻找使原始目标函数最小化的离散解。换句话说，主要缺点是在计算离散解时忽略了原始目标。受一阶优化算法的启发，我们提出了开发第一阶项来连接原始问题和离散化算法的方法，据我们所知这是非启发式方法中的首个。由于非启发式方法意识到了原始图割问题，最终离散解的可靠性更高，可以达到较好的损失值。我们还从理论上证明了连续最优解的优势。

    Spectral clustering and its extensions usually consist of two steps: (1) constructing a graph and computing the relaxed solution; (2) discretizing relaxed solutions. Although the former has been extensively investigated, the discretization techniques are mainly heuristic methods, e.g., k-means, spectral rotation. Unfortunately, the goal of the existing methods is not to find a discrete solution that minimizes the original objective. In other words, the primary drawback is the neglect of the original objective when computing the discrete solution. Inspired by the first-order optimization algorithms, we propose to develop a first-order term to bridge the original problem and discretization algorithm, which is the first non-heuristic to the best of our knowledge. Since the non-heuristic method is aware of the original graph cut problem, the final discrete solution is more reliable and achieves the preferable loss value. We also theoretically show that the continuous optimum is beneficial 
    
[^32]: TabuLa: 利用语言模型进行表格数据合成

    TabuLa: Harnessing Language Models for Tabular Data Synthesis. (arXiv:2310.12746v1 [cs.LG])

    [http://arxiv.org/abs/2310.12746](http://arxiv.org/abs/2310.12746)

    本文提出了一种基于语言模型结构的Tabula表格数据合成工具，通过研究我们发现，为自然语言处理设计的预训练语言模型在表格数据合成方面存在固有限制。

    

    鉴于表格数据在各行业中的广泛应用以及对数据隐私和安全性的日益关注，表格数据合成成为一个重要的研究领域。最近的最先进方法表明，可以采用大型语言模型（LLMs）来生成逼真的表格数据。由于LLMs将表格数据预处理为全文，它们具有避免高维度数据的独热编码所带来的维度灾难的优势。然而，它们训练时间长、在新任务上的可重用性有限，使得它们无法取代现有的表格生成模型。在本文中，我们提出了基于语言模型结构的Tabula，一种表格数据合成器。通过Tabula，我们展示了在表格数据合成的背景下，采用为自然语言处理（NLP）设计的预训练语言模型的固有限制。我们的研究深入探讨了专门针对表格数据定制的基础模型的开发。

    Given the ubiquitous use of tabular data in industries and the growing concerns in data privacy and security, tabular data synthesis emerges as a critical research area. The recent state-of-the-art methods show that large language models (LLMs) can be adopted to generate realistic tabular data. As LLMs pre-process tabular data as full text, they have the advantage of avoiding the curse of dimensionality associated with one-hot encoding high-dimensional data. However, their long training time and limited re-usability on new tasks prevent them from replacing exiting tabular generative models. In this paper, we propose Tabula, a tabular data synthesizer based on the language model structure. Through Tabula, we demonstrate the inherent limitation of employing pre-trained language models designed for natural language processing (NLP) in the context of tabular data synthesis. Our investigation delves into the development of a dedicated foundational model tailored specifically for tabular dat
    
[^33]: 流形学习的规范化正态流

    Canonical normalizing flows for manifold learning. (arXiv:2310.12743v1 [stat.ML])

    [http://arxiv.org/abs/2310.12743](http://arxiv.org/abs/2310.12743)

    该论文介绍了一种规范化正态流方法，用于流形学习。通过可学习的可逆变换将数据嵌入到高维空间中，从而实现了在流形上计算概率密度并优化网络参数的目标。然而，当前的方法在学习到的流形表示中存在着与流形关联且退化的内在基函数的问题。

    

    流形学习流是一类生成建模技术，假设数据具有低维流形描述。通过可学习的可逆变换将这种流形嵌入到数据的高维空间中。因此，一旦通过重构损失正确对齐流形，流形上的概率密度就是可计算的，并且可以使用最大似然来优化网络参数。自然地，数据的低维表示需要是单射映射。最近的方法能够在建模的流形上对密度进行对准，并在嵌入到高维空间时高效计算密度体积变化项。然而，除非单射映射在解析上预定义，否则学习到的流形不一定是数据的有效表示。也就是说，这种模型的潜在维度经常会学习到与流形相关并且退化的内在基函数。

    Manifold learning flows are a class of generative modelling techniques that assume a low-dimensional manifold description of the data. The embedding of such manifold into the high-dimensional space of the data is achieved via learnable invertible transformations. Therefore, once the manifold is properly aligned via a reconstruction loss, the probability density is tractable on the manifold and maximum likelihood can be used optimize the network parameters. Naturally, the lower-dimensional representation of the data requires an injective-mapping. Recent approaches were able to enforce that density aligns with the modelled manifold, while efficiently calculating the density volume-change term when embedding to the higher-dimensional space. However, unless the injective-mapping is analytically predefined, the learned manifold is not necessarily an efficient representation of the data. Namely, the latent dimensions of such models frequently learn an entangled intrinsic basis with degenerat
    
[^34]: 从过去学习：基于代理的对抗性防御框架来增强鲁棒性

    Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness. (arXiv:2310.12713v1 [cs.LG])

    [http://arxiv.org/abs/2310.12713](http://arxiv.org/abs/2310.12713)

    本文提出了一个基于代理的对抗性防御框架，通过引入目标模型的历史状态作为代理来增强模型对各种对抗攻击的鲁棒性。

    

    鉴于深度学习模型对对抗样本的脆弱性及其带来的安全问题，一系列方法，包括突出代表性的对抗训练（AT），旨在增强模型对各种对抗攻击的鲁棒性，得到了快速发展。然而，现有方法主要是通过明确或隐性的计算负担帮助目标模型的当前状态来防御面向参数的对抗攻击，同时由于优化轨迹不一致而导致不稳定的收敛行为。与以往的工作不同，本文重新考虑了目标模型的更新规则及其当前状态下的防御不足。通过引入目标模型的历史状态作为代理，并赋予其用于防御的先验信息，我们制定了一个两阶段的更新规则，从而得到一个通用的对抗性防御框架，我们称之为"LAST"。

    In light of the vulnerability of deep learning models to adversarial samples and the ensuing security issues, a range of methods, including Adversarial Training (AT) as a prominent representative, aimed at enhancing model robustness against various adversarial attacks, have seen rapid development. However, existing methods essentially assist the current state of target model to defend against parameter-oriented adversarial attacks with explicit or implicit computation burdens, which also suffers from unstable convergence behavior due to inconsistency of optimization trajectories. Diverging from previous work, this paper reconsiders the update rule of target model and corresponding deficiency to defend based on its current state. By introducing the historical state of the target model as a proxy, which is endowed with much prior information for defense, we formulate a two-stage update rule, resulting in a general adversarial defense framework, which we refer to as `LAST' ({\bf L}earn fr
    
[^35]: 神经符号化基础上的组合式世界建模

    Neurosymbolic Grounding for Compositional World Models. (arXiv:2310.12690v1 [cs.LG])

    [http://arxiv.org/abs/2310.12690](http://arxiv.org/abs/2310.12690)

    本论文介绍了一种名为Cosmos的框架，用于对象为中心的世界建模，通过使用神经符号化基础和视觉-语言基础模型，实现了在未见过的输入场景上的高性能组合泛化能力。

    

    我们引入了Cosmos，一个针对组合泛化（CG）设计的以对象为中心的世界建模框架，即在通过已知的视觉“原子”组合获得的未见过的输入场景上具有高性能。Cosmos的核心洞察力是使用一种新颖的神经符号化基础。具体来说，该框架引入了两个新工具：（i）神经符号化场景编码，使用神经编码器计算每个场景中的实体的实向量表示，并使用描述实体属性的可组合符号向量，以及（ii）神经符号化注意机制，将这些实体与学习到的交互规则绑定起来。Cosmos是端到端可微分的；此外，与传统的神经符号化方法需要手动将表示映射为符号不同，它使用视觉-语言基础模型计算实体的符号属性。通过对已建立的blocks场景进行两种不同形式的CG评估，我们验证了Cosmos的有效性。

    We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CG), i.e., high performance on unseen input scenes obtained through the composition of known visual "atoms." The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-
    
[^36]: 使用矩阵因式分解压缩循环神经网络

    Compression of Recurrent Neural Networks using Matrix Factorization. (arXiv:2310.12688v1 [cs.LG])

    [http://arxiv.org/abs/2310.12688](http://arxiv.org/abs/2310.12688)

    本论文提出了一种称为Rank-Tuning的训练后秩选择方法，可以在循环神经网络中高效压缩模型，并在几乎没有性能降低的情况下实现高压缩率。

    

    在实时或嵌入式应用中部署模型时，压缩神经网络是一个关键步骤。使用低秩近似对模型的矩阵进行分解是一种有前途的压缩方法。虽然在训练之前可以设置秩，但这种方法既不灵活也不最优。在这项工作中，我们提出了一种名为Rank-Tuning的训练后秩选择方法，可以为每个矩阵选择不同的秩。结合训练适应性的使用，我们的方法在几乎没有性能降低或者有很少性能降低的情况下实现了高压缩率。我们在信号处理任务上的数值实验结果显示，我们可以将循环神经网络压缩至最多14倍，且相对性能降低最多为1.4%。

    Compressing neural networks is a key step when deploying models for real-time or embedded applications. Factorizing the model's matrices using low-rank approximations is a promising method for achieving compression. While it is possible to set the rank before training, this approach is neither flexible nor optimal. In this work, we propose a post-training rank-selection method called Rank-Tuning that selects a different rank for each matrix. Used in combination with training adaptations, our method achieves high compression rates with no or little performance degradation. Our numerical experiments on signal processing tasks show that we can compress recurrent neural networks up to 14x with at most 1.4% relative performance reduction.
    
[^37]: 关于多头注意力的优化与泛化

    On the Optimization and Generalization of Multi-head Attention. (arXiv:2310.12680v1 [cs.LG])

    [http://arxiv.org/abs/2310.12680](http://arxiv.org/abs/2310.12680)

    本论文研究了使用多头注意力在优化和泛化方面的优势，推导了单层多头自注意力模型的梯度下降训练的收敛性和泛化保证，并证明了对于一个简单的分词混合模型，初始化条件满足可实现性条件。

    

    Transformer核心机制——Attention机制的训练和泛化动态仍未深入研究。此外，现有分析主要集中在单头注意力上。受到全连接网络训练时过参数化的益处启发，我们研究了使用多头注意力的潜在优化和泛化优势。为此，我们在数据的适当可实现性条件下，推导出单层多头自注意力模型的梯度下降训练的收敛性和泛化保证。然后，我们建立起初始化时确保可实现性得到满足的基本条件。最后，我们证明了这些条件适用于一个简单的分词混合模型。我们期望这个分析可以扩展到各种数据模型和架构变体。

    The training and generalization dynamics of the Transformer's core mechanism, namely the Attention mechanism, remain under-explored. Besides, existing analyses primarily focus on single-head attention. Inspired by the demonstrated benefits of overparameterization when training fully-connected networks, we investigate the potential optimization and generalization advantages of using multiple attention heads. Towards this goal, we derive convergence and generalization guarantees for gradient-descent training of a single-layer multi-head self-attention model, under a suitable realizability condition on the data. We then establish primitive conditions on the initialization that ensure realizability holds. Finally, we demonstrate that these conditions are satisfied for a simple tokenized-mixture model. We expect the analysis can be extended to various data-model and architecture variations.
    
[^38]: 利用频率和严重性数据进行保险定价的神经网络：从数据预处理到技术定价的基准研究

    Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff. (arXiv:2310.12671v1 [cs.LG])

    [http://arxiv.org/abs/2310.12671](http://arxiv.org/abs/2310.12671)

    本研究通过深度学习结构的神经网络对频率-严重性保险定价进行了基准研究，比较了不同模型的性能，并提出了一种联合精算神经网络(CANN)的方法。

    

    保险公司通常使用广义线性模型来建模索赔的频率和严重性数据。由于其在其他领域的成功，机器学习技术在精算工具箱中越来越受欢迎。本文通过深度学习结构为频率-严重性保险定价与机器学习相关的文献做出了贡献。我们在四个保险数据集上进行了基准研究，这些数据集包含有多种类型的输入特征和频率-严重性目标。我们详细比较了广义线性模型在分箱输入数据、梯度提升树模型、前馈神经网络（FFNN）和联合精算神经网络（CANN）上的性能。我们的CANN将通过GLM和GBM分别建立的基线预测与神经网络校正相结合。我们解释了数据预处理步骤，特别关注通常存在于表格保险数据集中的多种类型的输入特征，比如邮编和数字编码。

    Insurers usually turn to generalized linear models for modelling claim frequency and severity data. Due to their success in other fields, machine learning techniques are gaining popularity within the actuarial toolbox. Our paper contributes to the literature on frequency-severity insurance pricing with machine learning via deep learning structures. We present a benchmark study on four insurance data sets with frequency and severity targets in the presence of multiple types of input features. We compare in detail the performance of: a generalized linear model on binned input data, a gradient-boosted tree model, a feed-forward neural network (FFNN), and the combined actuarial neural network (CANN). Our CANNs combine a baseline prediction established with a GLM and GBM, respectively, with a neural network correction. We explain the data preprocessing steps with specific focus on the multiple types of input features typically present in tabular insurance data sets, such as postal codes, nu
    
[^39]: STANLEY：用于学习能量模型的随机梯度异向拉格朗日动力学

    STANLEY: Stochastic Gradient Anisotropic Langevin Dynamics for Learning Energy-Based Models. (arXiv:2310.12667v1 [stat.ML])

    [http://arxiv.org/abs/2310.12667](http://arxiv.org/abs/2310.12667)

    本文提出了一种名为STANLEY的算法用于采样高维数据，改善了能量模型学习算法的质量。

    

    本文提出了一种名为STANLEY的随机梯度异向拉格朗日动力学算法，用于采样高维数据。通过增强能量模型（EBM）的学习算法来改善采样数据点的质量，我们展示了EBM的端到端学习算法，该算法也被称为非归一化概率建模。由于EBMs的未知归一化常数导致训练过程难以处理，采用马尔科夫链蒙特卡罗（MCMC）通常是可行的选择。在本文中，我们提出了一种新颖的高维采样方法，该方法基于细分随机过程的异向步长和梯度信息的协方差矩阵。我们通过论证马尔科夫链中负样本的异向更新的必要性来解释了MCMC在EBM训练中的作用。

    We propose in this paper, STANLEY, a STochastic gradient ANisotropic LangEvin dYnamics, for sampling high dimensional data. With the growing efficacy and potential of Energy-Based modeling, also known as non-normalized probabilistic modeling, for modeling a generative process of different natures of high dimensional data observations, we present an end-to-end learning algorithm for Energy-Based models (EBM) with the purpose of improving the quality of the resulting sampled data points. While the unknown normalizing constant of EBMs makes the training procedure intractable, resorting to Markov Chain Monte Carlo (MCMC) is in general a viable option. Realizing what MCMC entails for the EBM training, we propose in this paper, a novel high dimensional sampling method, based on an anisotropic stepsize and a gradient-informed covariance matrix, embedded into a discretized Langevin diffusion. We motivate the necessity for an anisotropic update of the negative samples in the Markov Chain by the
    
[^40]: 从不确定性中获取知识的证据深度学习

    Knowledge from Uncertainty in Evidential Deep Learning. (arXiv:2310.12663v1 [cs.LG])

    [http://arxiv.org/abs/2310.12663](http://arxiv.org/abs/2310.12663)

    本论文研究了证据深度学习（EDL）中产生的来自不确定性的证据信号，并发现这些信号可以在某些情况下区分类别。研究还探讨了EDL与其他基于Dirichlet的方法之间的关联，并表明EDL的“证据信号”是由于误分类偏差而产生的。

    

    这项工作揭示了在证据深度学习（EDL）中从不确定性值中产生的证据信号。 EDL是一类考虑不确定性的深度学习方法的示例，旨在提供关于当前测试样本的置信度（或认知不确定性）。特别是对于计算机视觉和双向编码器大型语言模型，EDL中的Dirichlet强度引发的“证据信号”在某些情况下可以区分类别，尤其是在使用大型语言模型时表现得非常强。我们假设KL正则化项导致EDL将aleatoric不确定性和认知不确定性耦合在一起。在本文中，我们通过实证研究了误分类和评估不确定性之间的相关性，并展示了EDL的“证据信号”是由于误分类偏差而产生的。我们批判性地评估了EDL与其他基于Dirichlet的方法，即生成式证据神经网络（EDL-GEN）和先验网络，并理论上和实验上证明了。

    This work reveals an evidential signal that emerges from the uncertainty value in Evidential Deep Learning (EDL). EDL is one example of a class of uncertainty-aware deep learning approaches designed to provide confidence (or epistemic uncertainty) about the current test sample. In particular for computer vision and bidirectional encoder large language models, the `evidential signal' arising from the Dirichlet strength in EDL can, in some cases, discriminate between classes, which is particularly strong when using large language models. We hypothesise that the KL regularisation term causes EDL to couple aleatoric and epistemic uncertainty. In this paper, we empirically investigate the correlations between misclassification and evaluated uncertainty, and show that EDL's `evidential signal' is due to misclassification bias. We critically evaluate EDL with other Dirichlet-based approaches, namely Generative Evidential Neural Networks (EDL-GEN) and Prior Networks, and show theoretically and
    
[^41]: 梯度下降无法学习高频函数和模运算

    Gradient Descent Fails to Learn High-frequency Functions and Modular Arithmetic. (arXiv:2310.12660v1 [cs.LG])

    [http://arxiv.org/abs/2310.12660](http://arxiv.org/abs/2310.12660)

    梯度下降无法学习高频函数和模运算，该研究为使用基于梯度的学习技术训练高频周期函数和模乘法提供了数学分析。

    

    已知一些包含大量近似正交元素的目标函数类别难以被统计查询算法学习到。最近，这一经典事实再次出现在神经网络梯度优化的理论中。在这个新的框架中，一个类的难度通常由梯度对随机选择的目标函数的方差来衡量。最近，一个形式为$x \to ax \bmod p$的函数集合，其中$a$取自${\mathbb Z}_p$，引起了深度学习理论家和密码学家的关注。这个类可以被理解为${\mathbb Z}$上的$p$-周期函数的子集，并且与实数线上的高频周期函数类紧密相关。我们对使用基于梯度的学习技术从示例中训练高频周期函数或模乘法进行了数学分析，并强调了相关的限制和挑战。

    Classes of target functions containing a large number of approximately orthogonal elements are known to be hard to learn by the Statistical Query algorithms. Recently this classical fact re-emerged in a theory of gradient-based optimization of neural networks. In the novel framework, the hardness of a class is usually quantified by the variance of the gradient with respect to a random choice of a target function.  A set of functions of the form $x\to ax \bmod p$, where $a$ is taken from ${\mathbb Z}_p$, has attracted some attention from deep learning theorists and cryptographers recently. This class can be understood as a subset of $p$-periodic functions on ${\mathbb Z}$ and is tightly connected with a class of high-frequency periodic functions on the real line.  We present a mathematical analysis of limitations and challenges associated with using gradient-based learning techniques to train a high-frequency periodic function or modular multiplication from examples. We highlight that t
    
[^42]: 面向焊接过程的基于深度学习的在线质量预测系统

    Towards a Deep Learning-based Online Quality Prediction System for Welding Processes. (arXiv:2310.12632v1 [cs.LG])

    [http://arxiv.org/abs/2310.12632](http://arxiv.org/abs/2310.12632)

    该论文提出了一个基于深度学习的气体金属电弧焊接预测质量系统的概念，通过收集和管理多传感器数据以及实时处理和特征工程的方式，可以识别关系并预测焊接质量。

    

    制造过程的数字化为机器学习辅助的质量保证提供了有前景的应用。一个广泛应用的制造过程，可以从数据驱动的解决方案中受益匪浅，是气体金属电弧焊接（GMAW）。焊接过程以材料性质、工艺条件和焊接质量之间复杂的因果关系为特征。在频繁更改工艺参数的非实验室环境中，通过破坏性测试准确确定焊缝质量是经济上不可行的。深度学习提供了从工艺观察中识别关系并预测焊接质量的潜力。本文提出了一个基于深度学习的气体金属电弧焊接预测质量系统的概念。核心概念包括由四个主要阶段组成的管线：多传感器数据（如电流和电压）的收集和管理、时间序列的实时处理和特征工程

    The digitization of manufacturing processes enables promising applications for machine learning-assisted quality assurance. A widely used manufacturing process that can strongly benefit from data-driven solutions is \ac{GMAW}. The welding process is characterized by complex cause-effect relationships between material properties, process conditions and weld quality. In non-laboratory environments with frequently changing process parameters, accurate determination of weld quality by destructive testing is economically unfeasible. Deep learning offers the potential to identify the relationships in available process data and predict the weld quality from process observations. In this paper, we present a concept for a deep learning based predictive quality system in \ac{GMAW}. At its core, the concept involves a pipeline consisting of four major phases: collection and management of multi-sensor data (e.g. current and voltage), real-time processing and feature engineering of the time series 
    
[^43]: 不规则系统的逆重整群

    Inverse Renormalization Group of Disordered Systems. (arXiv:2310.12631v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2310.12631](http://arxiv.org/abs/2310.12631)

    本文介绍了一种逆重整群的方法，通过机器学习算法构造了缩放的晶格配置，用于研究自旋玻璃。该方法可以实现在不使用超级计算机的情况下，探索不断增大的晶格体积下的精确配置。

    

    我们提出了逆重整群变换，用于构建尚未被超级计算机或大规模模拟方法所访问的晶格体积的近似配置，以研究自旋玻璃。具体而言，在三维爱德华-安德森模型的情况下，从体积为$V=8^{3}$的晶格开始，我们采用机器学习算法构造了经过缩放的晶格，最大到$V'=128^{3}$，并提取了两个临界指数。最后，我们讨论了如何在逆重整群方法中融入数值精确性，从而为不断增大的晶格体积提供了探索可持续、节能的精确配置，而无需使用专用超级计算机。

    We propose inverse renormalization group transformations to construct approximate configurations for lattice volumes that have not yet been accessed by supercomputers or large-scale simulations in the study of spin glasses. Specifically, starting from lattices of volume $V=8^{3}$ in the case of the three-dimensional Edwards-Anderson model we employ machine learning algorithms to construct rescaled lattices up to $V'=128^{3}$, which we utilize to extract two critical exponents. We conclude by discussing how to incorporate numerical exactness within inverse renormalization group approaches of disordered systems, thus opening up the opportunity to explore a sustainable and energy-efficient generation of exact configurations for increasing lattice volumes without the use of dedicated supercomputers.
    
[^44]: 通过Frank-Wolfe算法改进的Metarounding算法

    An Improved Metarounding Algorithm via Frank-Wolfe. (arXiv:2310.12629v1 [cs.DS])

    [http://arxiv.org/abs/2310.12629](http://arxiv.org/abs/2310.12629)

    该论文通过引入Frank-Wolfe算法，改进了Metarounding算法，从而提高了线性优化算法在组合类问题上的效率。

    

    Metarounding是一种将线性优化的近似算法转化为同一类的在线线性优化算法的方法。我们提出了一种新的Metarounding算法，基于一个对于组合类存在基于松弛的近似算法的自然假设。我们的算法在理论和实践方面都更高效。

    Metarounding is an approach to convert an approximation algorithm for linear optimization over some combinatorial classes to an online linear optimization algorithm for the same class. We propose a new metarounding algorithm under a natural assumption that a relax-based approximation algorithm exists for the combinatorial class. Our algorithm is much more efficient in both theoretical and practical aspects.
    
[^45]: 学生如何成为教师：通过谱方法学习和遗忘

    How a student becomes a teacher: learning and forgetting through Spectral methods. (arXiv:2310.12612v1 [cs.LG])

    [http://arxiv.org/abs/2310.12612](http://arxiv.org/abs/2310.12612)

    本论文提出了基于谱方法的优化方案，用于解决在非凸性问题下学生网络与教师网络之间存在的不变子网络的识别问题。

    

    在理论机器学习中，学生-教师模型常被用作现实生活中教学的有效隐喻。当学生网络相对于教师网络过度参数化时，上述模型尤为相关。在这种操作条件下，很容易推测学生处理给定任务的能力最终可能储存在整个网络的一个子部分中。根据适当的指标，这个子部分应该在一定程度上类似于冻结的教师结构，并且在学生候选网络的不同架构下近似不变。然而，由于所研究问题的固有非凸性程度，最新的传统学习技术无法识别这样一个不变子网络的存在。在这项工作中，我们采取了一个根本不同的优化方案，该方案建立在谱表示的基础上。

    In theoretical ML, the teacher-student paradigm is often employed as an effective metaphor for real-life tuition. The above scheme proves particularly relevant when the student network is overparameterized as compared to the teacher network. Under these operating conditions, it is tempting to speculate that the student ability to handle the given task could be eventually stored in a sub-portion of the whole network. This latter should be to some extent reminiscent of the frozen teacher structure, according to suitable metrics, while being approximately invariant across different architectures of the student candidate network. Unfortunately, state-of-the-art conventional learning techniques could not help in identifying the existence of such an invariant subnetwork, due to the inherent degree of non-convexity that characterizes the examined problem. In this work, we take a leap forward by proposing a radically different optimization scheme which builds on a spectral representation of th
    
[^46]: 使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法

    Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])

    [http://arxiv.org/abs/2310.12609](http://arxiv.org/abs/2310.12609)

    本文提出了一种使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法。该方法通过单一的视觉输入，在推理时能够同时生成可达目标并规划避开障碍物的运动路径，具有稳健性和多模态适应性。

    

    由于其灵活性和多模态性，扩散模型在机器人领域中已经成为一种强大的工具。尽管其中一些方法有效地解决了复杂问题，但它们往往严重依赖于推理时的障碍物检测并需要额外的设备。为了应对这些挑战，我们提出了一种方法，该方法在推理时能够从单一的视觉输入中同时生成可达目标并规划避开障碍物的运动路径。我们的方法的核心是对训练过程中新颖的碰撞避免扩散核进行使用。通过与行为克隆和经典扩散模型进行评估，我们的框架证明了其稳健性。特别是在多模态环境中，它能够导航到目标并避开被障碍物阻挡的不可达目标，同时确保避免碰撞。

    Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
    
[^47]: 基于因果相似性的分层贝叶斯模型

    Causal Similarity-Based Hierarchical Bayesian Models. (arXiv:2310.12595v1 [cs.LG])

    [http://arxiv.org/abs/2310.12595](http://arxiv.org/abs/2310.12595)

    本文提出了一种基于因果相似性的分层贝叶斯模型，通过学习如何从具有相似因果机制的训练任务中汇集数据来提高机器学习算法对新任务的泛化能力。

    

    机器学习的关键挑战是对新数据的泛化能力。本研究探讨了对由相关任务组成的数据集进行泛化的问题，这些任务可能在因果机制上存在差异。例如，复杂疾病的观察性医学数据在不同患者间具有疾病因果机制的异质性，这给需要对训练数据集之外的新患者进行泛化的机器学习算法带来了挑战。常用的处理异质性数据集的方法包括为整个数据集学习一个全局模型，为每个任务的数据学习本地模型，或者利用分层、元学习和多任务学习方法从汇集的多个任务的数据中学习泛化。本文提出了基于因果相似性的分层贝叶斯模型，通过学习如何从具有相似因果机制的训练任务中汇集数据来提高对新任务的泛化能力。我们应用这种通用建模方法

    The key challenge underlying machine learning is generalisation to new data. This work studies generalisation for datasets consisting of related tasks that may differ in causal mechanisms. For example, observational medical data for complex diseases suffers from heterogeneity in causal mechanisms of disease across patients, creating challenges for machine learning algorithms that need to generalise to new patients outside of the training dataset. Common approaches for learning supervised models with heterogeneous datasets include learning a global model for the entire dataset, learning local models for each tasks' data, or utilising hierarchical, meta-learning and multi-task learning approaches to learn how to generalise from data pooled across multiple tasks. In this paper we propose causal similarity-based hierarchical Bayesian models to improve generalisation to new tasks by learning how to pool data from training tasks with similar causal mechanisms. We apply this general modelling
    
[^48]: 时态敏感问题回答的时态感知表示学习

    Time-Aware Representation Learning for Time-Sensitive Question Answering. (arXiv:2310.12585v1 [cs.CL])

    [http://arxiv.org/abs/2310.12585](http://arxiv.org/abs/2310.12585)

    该论文提出了一种时态感知的问题回答框架，通过引入时间上下文的区间抽取任务和相应的数据生成框架来训练模型，提高了QA模型的时间感知能力，在TimeQA数据集中的F1分数上超过了基准模型达8.5。

    

    时间是现实世界中问题回答的关键因素之一，然而语言模型很难理解时间限定词如“之后”和“之前”与数字之间的关系，因为现有的问题回答数据集中没有足够的时间表达。为了解决这个问题，我们提出了一种时间上下文感知的问题回答框架。我们提出了一种基于时间上下文的区间抽取任务，并构建了一个依赖于时间上下文的数据生成框架用于模型训练。此外，我们提出了一个评估QA模型的时间感知度的度量方法。区间抽取任务包括一个问题和四个句子候选项，根据时间和上下文分类为正确或错误。模型被训练为从在时间和上下文上都正确的句子中提取答案区间。通过使用TCQA训练的模型在TimeQA数据集上的F1分数上优于基准模型达8.5。我们的数据集和代码可在以下链接中获得。

    Time is one of the crucial factors in real-world question answering (QA) problems. However, language models have difficulty understanding the relationships between time specifiers, such as 'after' and 'before', and numbers, since existing QA datasets do not include sufficient time expressions. To address this issue, we propose a Time-Context aware Question Answering (TCQA) framework. We suggest a Time-Context dependent Span Extraction (TCSE) task, and build a time-context dependent data generation framework for model training. Moreover, we present a metric to evaluate the time awareness of the QA model using TCSE. The TCSE task consists of a question and four sentence candidates classified as correct or incorrect based on time and context. The model is trained to extract the answer span from the sentence that is both correct in time and context. The model trained with TCQA outperforms baseline models up to 8.5 of the F1-score in the TimeQA dataset. Our dataset and code are available at
    
[^49]: DA-TransUNet: 将Spatial和Channel Dual Attention与Transformer U-Net集成，用于医学图像分割

    DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation. (arXiv:2310.12570v1 [eess.IV])

    [http://arxiv.org/abs/2310.12570](http://arxiv.org/abs/2310.12570)

    DA-TransUNet 提出了一种新的深度医学图像分割框架，集成了Transformer和双重注意力块。通过注意机制和多方面特征提取，提升医学图像分割结果。

    

    由于强大的深度表示学习，自动化医学图像分割取得了很大的进展。Transformer的影响导致了对其变体的研究，并大规模替代传统的CNN模块。然而，这种趋势经常忽视了Transformer的固有特征提取能力以及通过微小调整对模型和Transformer模块的潜在改进。本研究提出了一种新颖的深度医学图像分割框架，称为DA-TransUNet，旨在将Transformer和双重注意块引入传统U形架构的编码器和解码器中。与之前基于Transformer的解决方案不同，我们的DA-TransUNet利用了Transformer的注意机制和DA-Block的多方面特征提取，可以有效地结合全局、局部和多尺度特征以增强医学图像分割。同时，实验结果表明在之前的Transformer U-Net的基础上添加了一个双重注意块。

    Great progress has been made in automatic medical image segmentation due to powerful deep representation learning. The influence of transformer has led to research into its variants, and large-scale replacement of traditional CNN modules. However, such trend often overlooks the intrinsic feature extraction capabilities of the transformer and potential refinements to both the model and the transformer module through minor adjustments. This study proposes a novel deep medical image segmentation framework, called DA-TransUNet, aiming to introduce the Transformer and dual attention block into the encoder and decoder of the traditional U-shaped architecture. Unlike prior transformer-based solutions, our DA-TransUNet utilizes attention mechanism of transformer and multifaceted feature extraction of DA-Block, which can efficiently combine global, local, and multi-scale features to enhance medical image segmentation. Meanwhile, experimental results show that a dual attention block is added bef
    
[^50]: Julearn: 一个易于使用的库，用于无泄漏评估和检查机器学习模型

    Julearn: an easy-to-use library for leakage-free evaluation and inspection of ML models. (arXiv:2310.12568v1 [cs.LG])

    [http://arxiv.org/abs/2310.12568](http://arxiv.org/abs/2310.12568)

    Julearn是一个易于使用的Python库，可以帮助研究人员设计和评估复杂的机器学习流水线，避免泄漏问题。

    

    快速发展的机器学习方法以及它在研究中越来越广泛的应用为没有深入培训的研究人员带来了挑战。例如，在神经科学中，机器学习可以帮助理解脑与行为之间的关系，诊断疾病，并利用磁共振成像和脑电图等各种数据源开发生物标志物。机器学习的主要目标是构建能够在未见数据上进行准确预测的模型。研究人员通过使用交叉验证等技术来评估性能，从而证明这样的可推广模型的存在，交叉验证使用系统子抽样来估计泛化性能。选择交叉验证方案并评估机器学习流水线可能是具有挑战性的，并且如果使用不当可能导致过高的结果和错误的解释。我们创建了一个名为julearn的开源Python库，允许研究人员设计和评估复杂的机器学习流水线而没有遇到漏洞。

    The fast-paced development of machine learning (ML) methods coupled with its increasing adoption in research poses challenges for researchers without extensive training in ML. In neuroscience, for example, ML can help understand brain-behavior relationships, diagnose diseases, and develop biomarkers using various data sources like magnetic resonance imaging and electroencephalography. The primary objective of ML is to build models that can make accurate predictions on unseen data. Researchers aim to prove the existence of such generalizable models by evaluating performance using techniques such as cross-validation (CV), which uses systematic subsampling to estimate the generalization performance. Choosing a CV scheme and evaluating an ML pipeline can be challenging and, if used improperly, can lead to overestimated results and incorrect interpretations.  We created julearn, an open-source Python library, that allow researchers to design and evaluate complex ML pipelines without encount
    
[^51]: Safety-Gymnasion：一个统一的安全强化学习基准

    Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v1 [cs.AI])

    [http://arxiv.org/abs/2310.12567](http://arxiv.org/abs/2310.12567)

    本文介绍了一个名为Safety-Gymnasium的环境套件，其中包括单个和多个Agent场景中的安全关键任务，并提供了一个包含16种最先进的SafeRL算法的算法库。这个基准旨在促进对安全性能的评估和比较，推动强化学习在安全性能上的发展。

    

    人工智能系统拥有推动社会进步的巨大潜力。然而，它们的部署经常面临严重的安全问题。安全强化学习(SafeRL)作为一种解决方案出现，可以在同时遵守多个约束的情况下优化策略，从而解决集成强化学习在安全关键场景中的挑战。本文介绍了一个名为Safety-Gymnasium的环境套件，包括单个和多个Agent场景中的安全关键任务，并接受向量和仅视觉输入。此外，我们提供了一个名为Safe Policy Optimization（SafePO）的算法库，包含16种最先进的SafeRL算法。这个综合性库可以作为研究社区的验证工具。通过引入这个基准，我们旨在促进对安全性能的评估和比较，从而推动强化学习在安全性能上的发展。

    Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for s
    
[^52]: 开放世界下的终身图学习

    Open-World Lifelong Graph Learning. (arXiv:2310.12565v1 [cs.LG])

    [http://arxiv.org/abs/2310.12565](http://arxiv.org/abs/2310.12565)

    本论文研究了开放世界场景下的终身图学习问题，提出了一种将超出分布（OOD）检测方法与图形邻域信息聚合相结合的新类别检测方法，并提出了一种弱监督相关反馈（Open-WRF）方法来降低OOD检测中对阈值的敏感性。实验证明所提出的方法在OOD得分方面优于现有方法，并且更加鲁棒。

    

    我们研究了在开放世界场景中的终身图学习问题，其中模型需要处理新的任务和潜在的未知类别。我们利用超出分布（OOD）检测方法识别新的类别，并将现有的非图形OOD检测方法适应于图形数据。关键是，我们建议通过将OOD检测方法与从图形邻域聚合的信息相结合，来进行新类别的检测。大多数OOD检测方法都避免确定一个确定的阈值，来决定一个顶点是否为OOD。为了解决这个问题，我们提出了一种弱监督相关反馈（Open-WRF）方法，降低了OOD检测中对阈值的敏感性。我们在六个基准数据集上评估了我们的方法。我们的结果表明，所提出的邻域聚合方法对OOD得分的表现优于现有方法，而不受基础图神经网络的影响。此外，我们证明了我们的Open-WRF方法对于阈值选择更加鲁棒。

    We study the problem of lifelong graph learning in an open-world scenario, where a model needs to deal with new tasks and potentially unknown classes. We utilize Out-of-Distribution (OOD) detection methods to recognize new classes and adapt existing non-graph OOD detection methods to graph data. Crucially, we suggest performing new class detection by combining OOD detection methods with information aggregated from the graph neighborhood. Most OOD detection methods avoid determining a crisp threshold for deciding whether a vertex is OOD. To tackle this problem, we propose a Weakly-supervised Relevance Feedback (Open-WRF) method, which decreases the sensitivity to thresholds in OOD detection. We evaluate our approach on six benchmark datasets. Our results show that the proposed neighborhood aggregation method for OOD scores outperforms existing methods independent of the underlying graph neural network. Furthermore, we demonstrate that our Open-WRF method is more robust to threshold sele
    
[^53]: 适用于强盗游戏的近似信息最大化方法

    Approximate information maximization for bandit games. (arXiv:2310.12563v1 [stat.ML])

    [http://arxiv.org/abs/2310.12563](http://arxiv.org/abs/2310.12563)

    本论文提出了一种基于近似信息最大化的强盗游戏算法，通过最大化关键变量的信息近似值来进行优化，在传统强盗设置中表现出很强的性能，并证明了其对于两臂强盗问题的渐近最优性。

    

    熵最大化和自由能最小化是用于模拟各种物理系统动态的一般物理原理。其中包括使用自由能原理对大脑内的决策进行建模，使用信息瓶颈原理对访问隐藏变量时优化准确性和复杂性的权衡，以及使用信息最大化进行随机环境导航。基于这一原理，我们提出了一种新的强盗算法类别，通过最大化系统中一个关键变量的信息近似来进行优化。为此，我们开发了一个基于物理的近似分析熵的表示方法，以预测每个动作的信息增益，并贪婪地选择信息增益最大的动作。这种方法在传统强盗设置中表现出很强的性能。受到其经验性成功的启发，我们证明了其对于两臂强盗问题的渐近最优性。

    Entropy maximization and free energy minimization are general physical principles for modeling the dynamics of various physical systems. Notable examples include modeling decision-making within the brain using the free-energy principle, optimizing the accuracy-complexity trade-off when accessing hidden variables with the information bottleneck principle (Tishby et al., 2000), and navigation in random environments using information maximization (Vergassola et al., 2007). Built on this principle, we propose a new class of bandit algorithms that maximize an approximation to the information of a key variable within the system. To this end, we develop an approximated analytical physics-based representation of an entropy to forecast the information gain of each action and greedily choose the one with the largest information gain. This method yields strong performances in classical bandit settings. Motivated by its empirical success, we prove its asymptotic optimality for the two-armed bandit
    
[^54]: 快速模型去偏置与机器取消学习

    Fast Model Debias with Machine Unlearning. (arXiv:2310.12560v1 [cs.LG])

    [http://arxiv.org/abs/2310.12560](http://arxiv.org/abs/2310.12560)

    这篇论文提出了一种快速模型去偏置的框架（FMD），可以有效识别、评估和消除深度神经网络中的偏见，解决了现有方法在成本和解释性方面的不足。

    

    最近的研究发现，深度神经网络在许多现实场景中可能表现出偏差的行为。例如，在一个大规模的人脸识别数据集CelebA上训练的深度网络倾向于预测女性的金色头发和男性的黑色头发。这些偏差不仅危害了模型的稳健性，而且会持续和放大社会偏见，这对于医疗、招聘等自动决策过程尤其令人担忧，因为它们可能加剧不同群体之间的不公平经济和社会不平等。现有的去偏置方法在偏见标记或模型重新训练方面成本高昂，同时也在阐明模型内部偏见的起源方面存在不足。为此，我们提出了一个快速模型去偏置框架(FMD)，它提供了一种有效的方法来识别、评估和消除训练模型中固有的偏见。FMD通过显式的反事实机制来识别偏置属性。

    Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing framework (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual 
    
[^55]: 利用可区分插入/删除指标感知正则化进行解释性训练

    Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers. (arXiv:2310.12553v1 [cs.LG])

    [http://arxiv.org/abs/2310.12553](http://arxiv.org/abs/2310.12553)

    提出一种插入/删除指标感知的基于解释的优化(ID-ExpO)方法，通过优化可区分的预测器来提高解释的插入和删除得分，并保持预测准确性。实验结果表明，ID-ExpO能够使流行的事后解释器产生更忠实的解释。

    

    复杂机器学习预测器的解释质量通常使用插入和删除指标进行衡量，这些指标评估解释的忠实度，即解释正确地反映了预测器的行为程度。为了提高忠实度，我们提出了插入/删除指标感知的基于解释的优化（ID-ExpO），该优化能够改善解释的插入和删除得分，同时保持其预测准确性。由于原始的插入和删除指标对于解释来说是不可区分的，并且无法直接进行基于梯度的优化，我们扩展了这些指标以使其可区分，并将其用于形式化插入和删除指标的正则化。在图像和表格数据集上的实验结果表明，使用ID-ExpO进行微调的基于深度神经网络的预测器能够使流行的事后解释器产生更忠实的解释。

    The quality of explanations for the predictions of complex machine learning predictors is often measured using insertion and deletion metrics, which assess the faithfulness of the explanations, i.e., how correctly the explanations reflect the predictor's behavior. To improve the faithfulness, we propose insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which optimizes differentiable predictors to improve both insertion and deletion scores of the explanations while keeping their predictive accuracy. Since the original insertion and deletion metrics are indifferentiable with respect to the explanations and directly unavailable for gradient-based optimization, we extend the metrics to be differentiable and use them to formalize insertion and deletion metric-based regularizers. The experimental results on image and tabular datasets show that the deep neural networks-based predictors fine-tuned using ID-ExpO enable popular post-hoc explainers to produce more faithful
    
[^56]: PGA: 个性化抓取代理与单一人机交互

    PGA: Personalizing Grasping Agents with Single Human-Robot Interaction. (arXiv:2310.12547v1 [cs.RO])

    [http://arxiv.org/abs/2310.12547](http://arxiv.org/abs/2310.12547)

    这项研究介绍了个性化抓取代理（PGA），它通过单一人机交互学习并定位和抓取个人物体。PGA通过用户提供的信息和用户环境中的原始图像，实现个性化物体抓取。

    

    语言条件化机器人抓取（LCRG）旨在开发基于自然语言指令的机器人来进行物体的接地和抓取。虽然能够识别个人物品如“我的钱包”的机器人可以更自然地与非专家用户交互，但当前的LCRG系统主要限制机器人只能理解一般表达。为此，我们引入了一个名为GraspMine的任务场景，并提供了一个新颖的数据集，旨在通过从单一人机交互中学习定位和抓取个人物体。为了解决GraspMine，我们提出了个性化抓取代理（PGA），通过将用户提供的信息传播到个人物体上，通过Reminiscence-用户环境中的一系列原始图像，获取个人物体信息。具体而言，PGA通过用户展示带有相关指示器的个人物体，并以旋转的方式检查物体来获取个人物体信息。根据所获得的信息，PGA为物体进行伪标签化。

    Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that ground and grasp objects based on natural language instructions. While robots capable of recognizing personal objects like "my wallet" can interact more naturally with non-expert users, current LCRG systems primarily limit robots to understanding only generic expressions. To this end, we introduce a task scenario GraspMine with a novel dataset that aims to locate and grasp personal objects given personal indicators via learning from a single human-robot interaction. To address GraspMine, we propose Personalized Grasping Agent (PGA), that learns personal objects by propagating user-given information through a Reminiscence-a collection of raw images from the user's environment. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in
    
[^57]: 整数值时间序列数据的神经似然近似

    Neural Likelihood Approximation for Integer Valued Time Series Data. (arXiv:2310.12544v1 [stat.ML])

    [http://arxiv.org/abs/2310.12544](http://arxiv.org/abs/2310.12544)

    本文构建了整数值时间序列数据的神经似然近似方法，使用因果卷积并行评估整个时间序列的似然，实现了对生态学和流行病学模型进行准确推断并显著加快计算速度。

    

    在物理和生物科学中，定义在整数值状态空间上的随机过程很常见。这些模型用于捕捉小系统的动力学，其中个体群体的个体属性不能被忽视，随机效应很重要。由于似然的复杂性，从时间序列数据中推断这些模型的参数是困难的；目前的方法基于基础模型的模拟，计算成本非常高昂，以至于难以实现。在本文中，我们使用因果卷积构建了用于整数值时间序列数据的神经似然近似方法，这使我们能够并行评估整个时间序列的似然。我们通过对一些生态学和流行病学模型进行推断来演示我们的方法，结果显示我们能够准确地近似真实的后验概率，同时在当前方法受限的情况下实现显著的计算加速。

    Stochastic processes defined on integer valued state spaces are popular within the physical and biological sciences. These models are necessary for capturing the dynamics of small systems where the individual nature of the populations cannot be ignored and stochastic effects are important. The inference of the parameters of such models, from time series data, is difficult due to intractability of the likelihood; current methods, based on simulations of the underlying model, can be so computationally expensive as to be prohibitive. In this paper we construct a neural likelihood approximation for integer valued time series data using causal convolutions, which allows us to evaluate the likelihood of the whole time series in parallel. We demonstrate our method by performing inference on a number of ecological and epidemiological models, showing that we can accurately approximate the true posterior while achieving significant computational speed ups in situations where current methods stru
    
[^58]: 为天文学构建有影响力的机器学习研究：研究人员和评审人员的最佳实践

    Constructing Impactful Machine Learning Research for Astronomy: Best Practices for Researchers and Reviewers. (arXiv:2310.12528v1 [astro-ph.IM])

    [http://arxiv.org/abs/2310.12528](http://arxiv.org/abs/2310.12528)

    这篇论文旨在为天文学界的作者、评审人员和编辑提供机器学习模型的实施和结果报告的最佳实践，以确保结果准确性、研究结果的可重复性和方法的实用性。

    

    机器学习已迅速成为天文学界的首选工具。它被应用于各种波长和问题，从瞬变物的分类到宏观模拟的神经网络模拟器，并改变了我们生成和报告科学结果的范式。与此同时，这类方法也带来了一系列最佳实践、挑战和缺点，然而目前在天体物理领域的文献中往往没有完整地报道。通过本文，我们旨在向天文学界的作者、评审人员和编辑提供一份入门指南，教授如何实施机器学习模型，并以确保结果准确性、研究结果的可重复性和方法的实用性的方式报告研究结果。

    Machine learning has rapidly become a tool of choice for the astronomical community. It is being applied across a wide range of wavelengths and problems, from the classification of transients to neural network emulators of cosmological simulations, and is shifting paradigms about how we generate and report scientific results. At the same time, this class of method comes with its own set of best practices, challenges, and drawbacks, which, at present, are often reported on incompletely in the astrophysical literature. With this paper, we aim to provide a primer to the astronomical community, including authors, reviewers, and editors, on how to implement machine learning models and report their results in a way that ensures the accuracy of the results, reproducibility of the findings, and usefulness of the method.
    
[^59]: 测试报告的二分类问题性能分数的一致性

    Testing the Consistency of Performance Scores Reported for Binary Classification Problems. (arXiv:2310.12527v1 [cs.LG])

    [http://arxiv.org/abs/2310.12527](http://arxiv.org/abs/2310.12527)

    这篇论文介绍了一种测试报告的二分类问题性能分数和实验设置一致性的数值方法，该方法不依赖于统计推断。

    

    二分类是机器学习中的一个基本任务，应用范围涵盖各个科学领域。科学家在进行基础研究或优化实际应用时，通常会根据准确率、敏感度和特异度等性能指标评估和排名分类技术。然而，报告的性能得分并不总是可靠的研究排名依据。这可能归因于未公开或非常规的交叉验证实践、排版错误和其他因素。在给定的实验设置中，具有特定数量的阳性和阴性测试项，大多数性能得分可以假设的特定、相互相关的值。在本文中，我们引入了数值技术来评估报告的性能得分的一致性和假设的实验设置。重要的是，所提出的方法不依赖于统计推断，而是使用数值方法来识别不一致的情况。

    Binary classification is a fundamental task in machine learning, with applications spanning various scientific domains. Whether scientists are conducting fundamental research or refining practical applications, they typically assess and rank classification techniques based on performance metrics such as accuracy, sensitivity, and specificity. However, reported performance scores may not always serve as a reliable basis for research ranking. This can be attributed to undisclosed or unconventional practices related to cross-validation, typographical errors, and other factors. In a given experimental setup, with a specific number of positive and negative test items, most performance scores can assume specific, interrelated values. In this paper, we introduce numerical techniques to assess the consistency of reported performance scores and the assumed experimental setup. Importantly, the proposed approach does not rely on statistical inference but uses numerical methods to identify inconsi
    
[^60]: 并行贝叶斯优化在时敏黑盒优化中的应用：基于满意度汤普森抽样的方法

    Parallel Bayesian Optimization Using Satisficing Thompson Sampling for Time-Sensitive Black-Box Optimization. (arXiv:2310.12526v1 [cs.LG])

    [http://arxiv.org/abs/2310.12526](http://arxiv.org/abs/2310.12526)

    该论文提出了一种基于满意度汤普森抽样的并行贝叶斯优化（STS-PBO）方法，用于解决时敏黑盒优化问题。该方法通过引入率失真理论构建了一个平衡信息量和次优性的损失函数，并采用Blahut-Arimoto算法计算目标解决方案。

    

    贝叶斯优化（BO）被广泛应用于黑盒优化问题，并在各种实际任务中表现良好。然而，大部分现有的BO方法旨在学习最优解，但当参数空间极大或问题时敏时，这可能变得不可行。在这些背景下，转向需要更少信息的满意解可以提供更好的性能。本文针对时敏黑盒优化问题，提出了基于满意度汤普森抽样的并行贝叶斯优化（STS-PBO）方法，包括同步和异步版本。我们将目标从最优解转移至更易学习的满意解。引入了率失真理论构建一个平衡需要学习的信息量和次优性的损失函数，并采用Blahut-Arimoto算法计算目标解决方案。

    Bayesian optimization (BO) is widely used for black-box optimization problems, and have been shown to perform well in various real-world tasks. However, most of the existing BO methods aim to learn the optimal solution, which may become infeasible when the parameter space is extremely large or the problem is time-sensitive. In these contexts, switching to a satisficing solution that requires less information can result in better performance. In this work, we focus on time-sensitive black-box optimization problems and propose satisficing Thompson sampling-based parallel Bayesian optimization (STS-PBO) approaches, including synchronous and asynchronous versions. We shift the target from an optimal solution to a satisficing solution that is easier to learn. The rate-distortion theory is introduced to construct a loss function that balances the amount of information that needs to be learned with sub-optimality, and the Blahut-Arimoto algorithm is adopted to compute the target solution that
    
[^61]: 用于监测植物健康威胁的命名实体识别：一种基于ChouBERT的方法

    Named Entity Recognition for Monitoring Plant Health Threats in Tweets: a ChouBERT Approach. (arXiv:2310.12522v1 [cs.CL])

    [http://arxiv.org/abs/2310.12522](http://arxiv.org/abs/2310.12522)

    本文研究了一种基于ChouBERT的命名实体识别方法，应用于监测植物健康威胁。通过利用社交媒体中的非结构化文本数据，可以检测并提取关键信息，从而解决现有解决方案中缺乏标记数据和细粒度语义资源的问题。

    

    精密农业的一个重要应用场景是利用传感器和数据分析技术检测和测量作物健康威胁。然而，由于缺乏标记数据和细粒度语义资源，现有解决方案对文本数据仍未充分探索。最近的研究表明，农民之间日益增长的互联性和在线农业社区的出现使得Twitter等社交媒体成为检测陌生的植物健康事件的广泛参与平台，前提是我们能够从非结构化的文本数据中提取关键信息。ChouBERT是一个法语预训练语言模型，能够识别涉及植物健康问题观察的推文，并对未见过的自然灾害具有一般化能力。本文通过进一步研究ChouBERT在小型标记数据集上的令牌级注释任务，解决了标记数据不足的问题。

    An important application scenario of precision agriculture is detecting and measuring crop health threats using sensors and data analysis techniques. However, the textual data are still under-explored among the existing solutions due to the lack of labelled data and fine-grained semantic resources. Recent research suggests that the increasing connectivity of farmers and the emergence of online farming communities make social media like Twitter a participatory platform for detecting unfamiliar plant health events if we can extract essential information from unstructured textual data. ChouBERT is a French pre-trained language model that can identify Tweets concerning observations of plant health issues with generalizability on unseen natural hazards. This paper tackles the lack of labelled data by further studying ChouBERT's know-how on token-level annotation tasks over small labeled sets.
    
[^62]: 通过可迁移的对抗攻击实现对齐大型语言模型的自动幻觉评估

    Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks. (arXiv:2310.12516v1 [cs.CL])

    [http://arxiv.org/abs/2310.12516](http://arxiv.org/abs/2310.12516)

    本文提出了一种通过可迁移的对抗攻击在大型语言模型中自动生成评估数据的方法，并使用ChatGPT和Natural Questions（NQ）数据集进行了验证。

    

    尽管在使用指令调整和检索增强技术防止大型语言模型（LLM）的幻觉方面取得了显著进展，但衡量LLM的可靠性仍然具有挑战性，因为人工评估数据对于许多任务和领域来说并不可用且可能存在数据泄漏。受到对抗机器学习的启发，本文旨在开发一种通过适当修改LLM在其中表现忠实的现有数据来自动生成评估数据的方法。具体而言，本文提出了一种基于LLM的框架AutoDebug，使用提示链接来生成以问答示例形式的可迁移对抗攻击。我们希望了解这些示例在多大程度上触发了LLM的幻觉行为。我们使用ChatGPT实现了AutoDebug，并对一个热门的开放领域问答数据集Natural Questions（NQ）的两个变体进行了评估。

    Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs.  We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-sour
    
[^63]: 用于近似双边匹配问题的WeaveNet

    WeaveNet for Approximating Two-sided Matching Problems. (arXiv:2310.12515v1 [cs.LG])

    [http://arxiv.org/abs/2310.12515](http://arxiv.org/abs/2310.12515)

    本文提出了一种用于近似双边匹配问题的图神经网络WeaveNet，通过保留边信息并密集传递消息，避免了节点信息丢失的问题。在公平稳定匹配问题中，我们的模型达到了与最新算法相当的性能。

    

    匹配是在约束条件下为有限资源进行最优分配的一项基本技术，该任务可能具有不同的目标、条件和约束，然而，用于匹配的高效神经网络架构尚未充分研究。本文提出了一种新颖的图神经网络（GNN）WeaveNet，专门用于处理二分图。由于二分图通常是密集的，常规的GNN架构在深度堆叠时会过度平滑从而丢失节点信息，这对于解决匹配问题是不理想的。WeaveNet通过保留边信息并密集传递消息以达到更好的解决方案，避免了这种现象。为了评估模型，我们近似解决了一个强NP困难问题——公平稳定匹配。尽管这个问题固有困难，且网络是通用设计的，但我们的模型在性能上与专门设计用于稳定匹配的最新算法相当。

    Matching, a task to optimally assign limited resources under constraints, is a fundamental technology for society. The task potentially has various objectives, conditions, and constraints; however, the efficient neural network architecture for matching is underexplored. This paper proposes a novel graph neural network (GNN), \textit{WeaveNet}, designed for bipartite graphs. Since a bipartite graph is generally dense, general GNN architectures lose node-wise information by over-smoothing when deeply stacked. Such a phenomenon is undesirable for solving matching problems. WeaveNet avoids it by preserving edge-wise information while passing messages densely to reach a better solution. To evaluate the model, we approximated one of the \textit{strongly NP-hard} problems, \textit{fair stable matching}. Despite its inherent difficulties and the network's general purpose design, our model reached a comparative performance with state-of-the-art algorithms specially designed for stable matching 
    
[^64]: SalUn：通过基于梯度的权重显著性增强机器遗忘在图像分类和生成中的效果

    SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])

    [http://arxiv.org/abs/2310.12508](http://arxiv.org/abs/2310.12508)

    这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。

    

    随着数据法规的不断发展，机器遗忘（MU）已成为增强当前AI模型的信任和安全性的重要工具。然而，现有的MU方法通常在遗忘精度、稳定性和跨领域适用性方面存在局限。为了解决这些挑战，我们引入了MU中的“权重显著性”概念，借鉴了模型解释中的输入显著性。这一创新将MU的关注点从整个模型引导到了具体的模型权重上，提高了其效果和效率。我们称之为显著性遗忘（SalUn）的方法将其与“精确”遗忘（在删除遗忘数据集后从头开始重新训练模型）的性能差距缩小。据我们所知，SalUn是第一个能够在图像分类和生成中有效消除遗忘数据、类别或概念影响的有原则的MU方法。例如，SalUn可在图片分类和生成任务中擦除遗忘数据、类别或概念。

    With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
    
[^65]: 大型语言模型的红队攻击提示生成和防御

    Attack Prompt Generation for Red Teaming and Defending Large Language Models. (arXiv:2310.12505v1 [cs.CL])

    [http://arxiv.org/abs/2310.12505](http://arxiv.org/abs/2310.12505)

    我们提出了一种综合方法来经济地生成高质量的攻击提示，通过上下文学习指导大型语言模型（LLMs）模仿人类生成的提示，并通过迭代交互来加强受攻击的LLMs的安全性。这些方法在不同LLMs上的实验验证了其有效性。

    

    大型语言模型（LLMs）容易受到红队攻击的影响，这可能导致LLMs生成有害内容。以前的研究通过手动或自动方法构建攻击提示，但这些方法在构建成本和质量上都存在限制。为了解决这些问题，我们提出了一种综合方法，结合了手动和自动方法，经济地生成高质量的攻击提示。具体而言，考虑到新兴LLMs的卓越能力，我们提出了一个攻击框架，通过上下文学习指导LLMs模仿人类生成的提示。此外，我们提出了一个防御框架，通过与攻击框架的迭代交互来对受攻击的LLMs进行微调，增强它们对红队攻击的安全性。对不同LLMs进行的大量实验验证了我们提出的攻击和防御框架的有效性。此外，我们发布了一系列攻击提示数据集，名为SAP，大小不同，便于研究者进行进一步研究。

    Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facili
    
[^66]: 使用自注意力GRU和Shapley值解释的美式期权定价

    American Option Pricing using Self-Attention GRU and Shapley Value Interpretation. (arXiv:2310.12500v1 [q-fin.PR])

    [http://arxiv.org/abs/2310.12500](http://arxiv.org/abs/2310.12500)

    本文提出了一种使用自注意力GRU和Shapley值解释的美式期权定价的机器学习方法，通过分割数据集和建立不同的模型，可以更准确地预测期权的价格。

    

    期权作为一种关键的金融工具，在证券市场中被投资者用来管理和减轻投资风险。精确预测期权的现价使投资者能够做出明智和高效的决策。在本文中，我们提出了一种基于门控循环单元（GRU）和自注意机制的机器学习方法，用于预测SPY（ETF）期权的价格。我们首先根据货币价值和到期日的标准将原始数据集分成15个子集。对于每个子集，我们匹配相应的美国政府债券利率和隐含波动率指数。这种分割方式可以更深入地探索无风险利率和基础波动率对期权定价的影响。接下来，我们构建了四个不同的机器学习模型，包括多层感知机（MLP）、长短期记忆（LSTM）、自注意LSTM和自注意GRU，与传统的二项模型进行比较。

    Options, serving as a crucial financial instrument, are used by investors to manage and mitigate their investment risks within the securities market. Precisely predicting the present price of an option enables investors to make informed and efficient decisions. In this paper, we propose a machine learning method for forecasting the prices of SPY (ETF) option based on gated recurrent unit (GRU) and self-attention mechanism. We first partitioned the raw dataset into 15 subsets according to moneyness and days to maturity criteria. For each subset, we matched the corresponding U.S. government bond rates and Implied Volatility Indices. This segmentation allows for a more insightful exploration of the impacts of risk-free rates and underlying volatility on option pricing. Next, we built four different machine learning models, including multilayer perceptron (MLP), long short-term memory (LSTM), self-attention LSTM, and self-attention GRU in comparison to the traditional binomial model. The e
    
[^67]: 类曼哈顿瓦瑟斯坦距离

    Quasi Manhattan Wasserstein Distance. (arXiv:2310.12498v1 [cs.LG])

    [http://arxiv.org/abs/2310.12498](http://arxiv.org/abs/2310.12498)

    类曼哈顿瓦瑟斯坦距离（QMWD）是一种用于度量两个矩阵之间相异性的度量标准，它具有更好的时间和空间复杂度，并且适用于大型数据集或资源有限的情况。

    

    类曼哈顿瓦瑟斯坦距离（QMWD）是一种用于度量两个矩阵之间相异性的度量标准，它将瓦瑟斯坦距离的元素与特定的变换相结合。与曼哈顿瓦瑟斯坦距离（MWD）相比，QMWD具有更好的时间和空间复杂度，并保持准确性。QMWD特别适用于大型数据集或计算资源有限的情况。本文详细阐述了QMWD的计算、复杂性分析以及与WD和MWD的比较。

    The Quasi Manhattan Wasserstein Distance (QMWD) is a metric designed to quantify the dissimilarity between two matrices by combining elements of the Wasserstein Distance with specific transformations. It offers improved time and space complexity compared to the Manhattan Wasserstein Distance (MWD) while maintaining accuracy. QMWD is particularly advantageous for large datasets or situations with limited computational resources. This article provides a detailed explanation of QMWD, its computation, complexity analysis, and comparisons with WD and MWD.
    
[^68]: SDGym: 使用系统动力学模型的低代码强化学习环境

    SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models. (arXiv:2310.12494v1 [cs.LG])

    [http://arxiv.org/abs/2310.12494](http://arxiv.org/abs/2310.12494)

    这项研究提出了SDGym，一个基于系统动力学模型的低代码强化学习环境库，通过生成定制的RL环境来解决现实环境设计的困难，从而构建在实际设置中表现良好的强大智能体。

    

    理解算法干预对社会的长期影响对于实现负责任的人工智能至关重要。传统的评估策略通常难以应对社会的复杂、适应性和动态性。虽然强化学习（RL）可以是优化动态环境下决策的强大方法，但现实环境设计的困难仍然是构建在实际环境中表现良好的强大智能体的障碍。为了解决这个问题，我们利用系统动力学（SD）领域作为一种补充方法，纳入协作仿真模型规范实践。我们介绍了SDGym，这是一个基于OpenAI Gym框架构建的低代码库，它可以根据SD模拟模型生成定制的RL环境。通过一项可行性研究，我们验证了可以从现有SD模型和少量配置代码中生成具有规范良好、丰富的RL环境。我们展示了SDGym的功能。

    Understanding the long-term impact of algorithmic interventions on society is vital to achieving responsible AI. Traditional evaluation strategies often fall short due to the complex, adaptive and dynamic nature of society. While reinforcement learning (RL) can be a powerful approach for optimizing decisions in dynamic settings, the difficulty of realistic environment design remains a barrier to building robust agents that perform well in practical settings. To address this issue we tap into the field of system dynamics (SD) as a complementary method that incorporates collaborative simulation model specification practices. We introduce SDGym, a low-code library built on the OpenAI Gym framework which enables the generation of custom RL environments based on SD simulation models. Through a feasibility study we validate that well specified, rich RL environments can be generated from preexisting SD models and a few lines of configuration code. We demonstrate the capabilities of the SDGym 
    
[^69]: 通过正交注意力提升运算符学习

    Improved Operator Learning by Orthogonal Attention. (arXiv:2310.12487v1 [cs.LG])

    [http://arxiv.org/abs/2310.12487](http://arxiv.org/abs/2310.12487)

    本研究提出了一种基于正交注意力的神经运算符，通过核积分算子的特征分解和神经近似的特征函数，来解决现有方法在有限训练数据上过拟合的问题。实验证明，该方法在六个标准神经运算符数据集上的表现优于其他基线模型。

    

    神经运算符是一种有效的代理模型，用于学习偏微分方程的解，受到科学机器学习领域的广泛关注。其中，基于注意力的神经运算符已成为相关研究的主流之一。然而，由于注意机制中参数数量巨大，现有方法在有限的训练数据上过拟合。为了解决这个问题，我们基于核积分算子的特征分解和神经近似的特征函数，开发了一种正交注意力。正交化自然地对结果神经运算符施加适当的正则化效果，有助于抵抗过拟合和提升泛化能力。在包括正常和非正常几何形状的六个标准神经运算符基准数据集上的实验证明，我们的方法可以胜过竞争对手，并取得了相当大的优势。

    Neural operators, as an efficient surrogate model for learning the solutions of PDEs, have received extensive attention in the field of scientific machine learning. Among them, attention-based neural operators have become one of the mainstreams in related research. However, existing approaches overfit the limited training data due to the considerable number of parameters in the attention mechanism. To address this, we develop an orthogonal attention based on the eigendecomposition of the kernel integral operator and the neural approximation of eigenfunctions. The orthogonalization naturally poses a proper regularization effect on the resulting neural operator, which aids in resisting overfitting and boosting generalization. Experiments on six standard neural operator benchmark datasets comprising both regular and irregular geometries show that our method can outperform competing baselines with decent margins.
    
[^70]: 揭示Transformer机器学习模型：基于注意力权重的数据恢复的理论方法

    Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights. (arXiv:2310.12462v1 [cs.LG])

    [http://arxiv.org/abs/2310.12462](http://arxiv.org/abs/2310.12462)

    本文提出了一种基于注意力权重和输出的理论框架，用于恢复Transformer模型中的输入数据。研究结果暗示模型设计存在潜在的漏洞。

    

    在深度学习领域中，Transformer已经成为了一种主导的架构，特别是在自然语言处理任务中。然而，随着它们的广泛应用，有关这些模型处理数据的安全性和隐私性的问题已经引起了关注。本文针对一个关键问题进行了研究：是否可以使用Transformer的注意力权重和输出来恢复输入数据？我们提出了一个理论框架来解决这个问题。具体地，我们介绍了一种算法，旨在通过最小化损失函数$L(X)$从给定的注意力权重$W = QK^\top$和输出$B$中恢复输入数据$X$，其中$X \in \mathbb{R}^{d \times n}$，$W \in \mathbb{R}^{d \times d}$，$B \in \mathbb{R}^{n \times n}$。这个损失函数捕捉了预期输出与实际输出之间的差异。我们的研究结果对于局部化分层机制（Localized Layer-wise Mechanism，LLM）具有重要的影响，表明模型设计存在潜在的漏洞。

    In the realm of deep learning, transformers have emerged as a dominant architecture, particularly in natural language processing tasks. However, with their widespread adoption, concerns regarding the security and privacy of the data processed by these models have arisen. In this paper, we address a pivotal question: Can the data fed into transformers be recovered using their attention weights and outputs? We introduce a theoretical framework to tackle this problem. Specifically, we present an algorithm that aims to recover the input data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top \in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by minimizing the loss function $L(X)$. This loss function captures the discrepancy between the expected output and the actual output of the transformer. Our findings have significant implications for the Localized Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's design from a s
    
[^71]: 平衡组卷积：一种基于近似性评估的改进组卷积方法

    Balanced Group Convolution: An Improved Group Convolution Based on Approximability Estimates. (arXiv:2310.12461v1 [cs.LG])

    [http://arxiv.org/abs/2310.12461](http://arxiv.org/abs/2310.12461)

    本文通过理论分析，发现组卷积相对于标准卷积的近似程度，提出了一种名为平衡组卷积的新变体，它在增加较小计算成本的情况下，展现出更高的近似性。实验结果验证了该方法的卓越性能。

    

    通过增加卷积层中的通道数量，神经网络的性能得到了显著提升。然而，这种性能提升伴随着更高的计算成本，导致许多研究致力于减少它。解决这个问题的一种有希望的方法是组卷积，通过分组通道有效地减少计算成本。然而，据我们所知，关于组卷积如何近似标准卷积还没有进行理论分析。本文在理论上分析了组卷积相对于组数的标准卷积的近似程度。此外，我们提出了一种名为平衡组卷积的新变体，它在仅增加小的计算成本的情况下展现出更高的近似性。我们提供的实验结果验证了我们的理论发现，并展示了平衡组卷积的卓越性能。

    The performance of neural networks has been significantly improved by increasing the number of channels in convolutional layers. However, this increase in performance comes with a higher computational cost, resulting in numerous studies focused on reducing it. One promising approach to address this issue is group convolution, which effectively reduces the computational cost by grouping channels. However, to the best of our knowledge, there has been no theoretical analysis on how well the group convolution approximates the standard convolution. In this paper, we mathematically analyze the approximation of the group convolution to the standard convolution with respect to the number of groups. Furthermore, we propose a novel variant of the group convolution called balanced group convolution, which shows a higher approximation with a small additional computational cost. We provide experimental results that validate our theoretical findings and demonstrate the superior performance of the ba
    
[^72]: MuseGNN: 可解释和可收敛的大规模图神经网络层

    MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale. (arXiv:2310.12457v1 [cs.LG])

    [http://arxiv.org/abs/2310.12457](http://arxiv.org/abs/2310.12457)

    MuseGNN提出了一种可解释和可收敛的大规模图神经网络层，通过迭代地减少基于采样的能量函数，同时作为预测特征和能量函数最小化者，具有竞争力的准确性和可扩展性。

    

    在能够建模具有跨实例关系的数据的许多图神经网络（GNN）架构中，一类重要的子类涉及设计层，其正向传递迭代地减少感兴趣的图正则化能量函数。通过这种方式，输出层产生的节点嵌入既可作为解决下游任务（如节点分类）的预测特征，又可作为能量函数最小化者，继承了可靠的归纳偏置和可解释性。然而，构建以这种方式构建的GNN架构的扩展仍然具有挑战性，部分原因是正向传递的收敛可能涉及具有相当深度的模型。为了解决这个限制，我们提出了一种基于采样的能量函数和可扩展的GNN层，通过在某些设置中具有收敛保证的指导，迭代地减少它。我们还基于这些设计实例化了一个完整的GNN架构，该模型在准确性和可扩展性方面均具有竞争力。

    Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability whe
    
[^73]: MTS-LOF: 利用遮挡不变特征进行医学时间序列的表示学习

    MTS-LOF: Medical Time-Series Representation Learning via Occlusion-Invariant Features. (arXiv:2310.12451v1 [cs.LG])

    [http://arxiv.org/abs/2310.12451](http://arxiv.org/abs/2310.12451)

    MTS-LOF是一种利用对比学习和遮挡自编码器方法的医学时间序列表示学习框架，能够为医疗应用提供更复杂、更丰富的上下文表示，并通过多遮挡策略实现了遮挡不变特征的学习。

    

    医学时间序列数据在医疗保健中不可或缺，为疾病诊断、治疗计划和患者管理提供了重要的洞察力。先进的传感器技术带来的数据复杂性的指数增长，使数据标注面临挑战。自监督学习（SSL）已经成为解决这些挑战的一种变革性方法，消除了对广泛人工标注的需求。在本研究中，我们介绍了一种新的医学时间序列表示学习框架，称为MTS-LOF。MTS-LOF利用对比学习和遮挡自编码器（MAE）方法的优势，提供了一种针对医学时间序列数据的独特的表示学习方法。通过结合这些技术，MTS-LOF通过提供更复杂、更丰富的上下文表示，增强了医疗应用的潜力。此外，MTS-LOF采用多遮挡策略，促进了遮挡不变特征的学习。

    Medical time series data are indispensable in healthcare, providing critical insights for disease diagnosis, treatment planning, and patient management. The exponential growth in data complexity, driven by advanced sensor technologies, has presented challenges related to data labeling. Self-supervised learning (SSL) has emerged as a transformative approach to address these challenges, eliminating the need for extensive human annotation. In this study, we introduce a novel framework for Medical Time Series Representation Learning, known as MTS-LOF. MTS-LOF leverages the strengths of contrastive learning and Masked Autoencoder (MAE) methods, offering a unique approach to representation learning for medical time series data. By combining these techniques, MTS-LOF enhances the potential of healthcare applications by providing more sophisticated, context-rich representations. Additionally, MTS-LOF employs a multi-masking strategy to facilitate occlusion-invariant feature learning. This appr
    
[^74]: 约束重加权分布：一种最优传输方法的研究

    Constrained Reweighting of Distributions: an Optimal Transport Approach. (arXiv:2310.12447v1 [stat.ML])

    [http://arxiv.org/abs/2310.12447](http://arxiv.org/abs/2310.12447)

    本文提出了一种最优传输方法，通过引入非参数化的分布约束权重，并利用最大熵原理和最优传输工具设计了一个通用框架，以实现对观测数据的最优权重调整。这种方法在不同的应用场景中展现了灵活性和多功能性。

    

    我们经常遇到的问题是要识别出符合预定义的权重约束条件的观测数据的经验分布的最优调整版本。这些约束通常表现为对权重的矩、尾部行为、形状、模式数量等的限制。在本文中，我们通过引入一种非参数化的分布约束权重并利用最大熵原理和最优传输工具开发了一个通用框架，从而大大提高了这种方法的灵活性。关键思想是确保观测数据的最大熵权重调整经验分布与预定的概率分布在最优传输度量下接近，并允许细微的偏差。该框架的多功能性在三个不同的应用场景中得到了证明，其中数据重加权是合理的。

    We commonly encounter the problem of identifying an optimally weight adjusted version of the empirical distribution of observed data, adhering to predefined constraints on the weights. Such constraints often manifest as restrictions on the moments, tail behaviour, shapes, number of modes, etc., of the resulting weight adjusted empirical distribution. In this article, we substantially enhance the flexibility of such methodology by introducing a nonparametrically imbued distributional constraints on the weights, and developing a general framework leveraging the maximum entropy principle and tools from optimal transport. The key idea is to ensure that the maximum entropy weight adjusted empirical distribution of the observed data is close to a pre-specified probability distribution in terms of the optimal transport metric while allowing for subtle departures. The versatility of the framework is demonstrated in the context of three disparate applications where data re-weighting is warrante
    
[^75]: 高效长程Transformer：需要更多关注，但不一定在每一层都需要

    Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer. (arXiv:2310.12442v1 [cs.CL])

    [http://arxiv.org/abs/2310.12442](http://arxiv.org/abs/2310.12442)

    提出了一种高效的长程Transformer模型MASFormer，通过在少数层使用全局注意力和在其他层使用稀疏注意力，实现了在具有长序列的任务中高效的计算和建模能力。

    

    预训练的Transformer模型在各种自然语言处理任务中展示了卓越的性能。这些模型利用注意机制来捕捉序列中的长程和短程依赖关系。然而，全局注意机制的计算成本与序列长度呈二次关系，在具有长序列的任务中（例如8k个标记的输入）是不可承受的。尽管现有工作中建议使用稀疏注意力来提高计算效率，但它的建模能力有限，往往无法捕捉长序列中的复杂依赖关系。为了解决这个挑战，我们提出了MASFormer，一种易于实现的变种Transformer，具有混合注意范围。具体而言，MASFormer配备了全局注意力来捕捉长程依赖关系，但只在少数几层使用。对于剩余层，MASFormer只采用稀疏注意力来捕捉短程依赖关系。我们在n上的实验结果表明，MASFormer在长序列任务上具有较高的计算效率和建模能力。

    Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g., inputs with 8k tokens. Although sparse attention can be used to improve computational efficiency, as suggested in existing work, it has limited modeling capacity and often fails to capture complicated dependencies in long sequences. To tackle this challenge, we propose MASFormer, an easy-to-implement transformer variant with Mixed Attention Spans. Specifically, MASFormer is equipped with full attention to capture long-range dependencies, but only at a small number of layers. For the remaining layers, MASformer only employs sparse attention to capture short-range dependencies. Our experiments on n
    
[^76]: CAT: 用于安全端到端驾驶的闭环对抗训练

    CAT: Closed-loop Adversarial Training for Safe End-to-End Driving. (arXiv:2310.12432v1 [cs.LG])

    [http://arxiv.org/abs/2310.12432](http://arxiv.org/abs/2310.12432)

    本文提出了闭环对抗训练（CAT）框架用于实现安全的端到端驾驶。CAT通过训练代理在动态生成的安全关键情景上来不断提高驾驶代理的安全性。与现有的安全关键场景生成方法相比，CAT可以发起更高效的物理攻击，并显著降低计算成本。

    

    对于自动驾驶车辆来说，驾驶安全是最重要的。在本文中，我们通过环境增强的视角研究了闭环对抗训练（CAT）框架，用于实现安全的端到端驾驶。CAT旨在通过训练代理在动态生成的安全关键情景上，持续提高驾驶代理的安全性。我们开发了一种新颖的重采样技术，通过概率分解将实际驾驶场景转化为安全关键场景，其中对抗性交通生成被建模为标准运动预测子问题的乘法。因此，CAT相比现有的安全关键场景生成方法，可以发起更高效的物理攻击，并显著减少迭代学习流程中的计算成本。我们将CAT应用于MetaDrive模拟器中。

    Driving safety is a top priority for autonomous vehicles. Orthogonal to prior work handling accident-prone traffic events by algorithm designs at the policy level, we investigate a Closed-loop Adversarial Training (CAT) framework for safe end-to-end driving in this paper through the lens of environment augmentation. CAT aims to continuously improve the safety of driving agents by training the agent on safety-critical scenarios that are dynamically generated over time. A novel resampling technique is developed to turn log-replay real-world driving scenarios into safety-critical ones via probabilistic factorization, where the adversarial traffic generation is modeled as the multiplication of standard motion prediction sub-problems. Consequently, CAT can launch more efficient physical attacks compared to existing safety-critical scenario generation methods and yields a significantly less computational cost in the iterative learning pipeline. We incorporate CAT into the MetaDrive simulator
    
[^77]: 实现随机森林的局部可解释性增强：基于邻近性的方法

    Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach. (arXiv:2310.12428v1 [stat.ML])

    [http://arxiv.org/abs/2310.12428](http://arxiv.org/abs/2310.12428)

    这项研究提出了一种利用随机森林模型的特征空间中的邻近性来解释模型预测的方法，为模型预测提供了局部的解释性，与现有方法相辅相成。通过实验证明了这种方法在债券定价模型中的有效性。

    

    我们提出一种新的方法来解释随机森林（RF）模型的样本外性能，利用了任何RF都可以被表述为自适应加权K最近邻（KNN）模型的事实。具体而言，我们利用RF在特征空间中学到的点之间的邻近性，将随机森林的预测重写为训练数据点目标标签的加权平均值。这种线性性质有助于在训练集观测中为任何模型预测生成属性，从而为RF预测提供了局部的解释性，补充了SHAP等已有方法，这些方法则为特征空间维度上的模型预测生成属性。我们在训练于美国公司债券交易数据的债券定价模型中演示了这种方法，并将其与各种现有的模型解释方法进行了比较。

    We initiate a novel approach to explain the out of sample performance of random forest (RF) models by exploiting the fact that any RF can be formulated as an adaptive weighted K nearest-neighbors model. Specifically, we use the proximity between points in the feature space learned by the RF to re-write random forest predictions exactly as a weighted average of the target labels of training data points. This linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established methods like SHAP, which instead generates attributions for a model prediction across dimensions of the feature space. We demonstrate this approach in the context of a bond pricing model trained on US corporate bond trades, and compare our approach to various existing approaches to model explainability.
    
[^78]: 大语言模型时代下的声明式软件规范自动修复

    Automated Repair of Declarative Software Specifications in the Era of Large Language Models. (arXiv:2310.12425v1 [cs.SE])

    [http://arxiv.org/abs/2310.12425](http://arxiv.org/abs/2310.12425)

    大语言模型时代下，自动修复声明式软件规范的有效技术需求愈发突出。该研究评估了利用ChatGPT修复Alloy声明式语言规范的效果，并探索了大型语言模型在此自动修复过程中的机会。

    

    声明式软件规范语言的广泛采用以及其在调试方面的困难性，凸显了对适用于此类语言的有效自动化修复技术的需求。研究人员最近探索了各种方法来自动修复声明式软件规范，如基于模板的修复、反馈驱动的迭代修复和有界穷举方法。大型语言模型的最新发展为自动修复声明式规范提供了新机会。在这项研究中，我们评估了利用OpenAI的ChatGPT修复用Alloy声明式语言编写的软件规范的效果。与命令式语言不同，Alloy中的规范不会被执行，而是被转换为逻辑公式，并使用后端约束求解器进行评估，以识别规范实例和断言的反例。我们的评估重点是ChatGPT在改进声明式规范修复能力方面的能力。

    The growing adoption of declarative software specification languages, coupled with their inherent difficulty in debugging, has underscored the need for effective and automated repair techniques applicable to such languages. Researchers have recently explored various methods to automatically repair declarative software specifications, such as template-based repair, feedback-driven iterative repair, and bounded exhaustive approaches. The latest developments in large language models provide new opportunities for the automatic repair of declarative specifications. In this study, we assess the effectiveness of utilizing OpenAI's ChatGPT to repair software specifications written in the Alloy declarative language. Unlike imperative languages, specifications in Alloy are not executed but rather translated into logical formulas and evaluated using backend constraint solvers to identify specification instances and counterexamples to assertions. Our evaluation focuses on ChatGPT's ability to impr
    
[^79]: 使用因果建模检测和减轻二元分类中的算法偏见

    Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling. (arXiv:2310.12421v1 [cs.LG])

    [http://arxiv.org/abs/2310.12421](http://arxiv.org/abs/2310.12421)

    本文提出了使用因果建模来检测和减轻算法偏见的新方法，并在性别偏见和二元分类问题上进行了实证研究。通过交叉验证实验证明了因果模型在减轻性别偏见方面的有效性，并轻微提高了整体分类准确率。

    

    本文提出使用因果建模来检测和减轻算法偏见。我们简要介绍了因果建模的概念并概述了我们的方法。我们使用了UC Irvine机器学习库中可下载的成年人数据集，分别开发了（1）一个被视为黑箱的预测模型和（2）一个用于减轻偏见的因果模型。本文重点关注性别偏见和二元分类问题。我们展示了预测模型中性别偏见的统计显著性（p<0.05），并通过交叉验证展示了因果模型在减轻性别偏见方面的有效性。此外，我们还展示了整体分类准确率的轻微提升。我们的创新方法直观易懂，并且可以使用现有统计软件工具（如R中的“lavaan”）实现。因此，它提高了可解释性并促进了信任。

    This paper proposes the use of causal modeling to detect and mitigate algorithmic bias. We provide a brief description of causal modeling and a general overview of our approach. We then use the Adult dataset, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on gender bias and the problem of binary classification. We show that gender bias in the prediction model is statistically significant at the 0.05 level. We demonstrate the effectiveness of the causal model in mitigating gender bias by cross-validation. Furthermore, we show that the overall classification accuracy is improved slightly. Our novel approach is intuitive, easy-to-use, and can be implemented using existing statistical software tools such as "lavaan" in R. Hence, it enhances explainability and promotes trust.
    
[^80]: 通过梯度特征学习证明神经网络的可靠性

    Provable Guarantees for Neural Networks via Gradient Feature Learning. (arXiv:2310.12408v1 [cs.LG])

    [http://arxiv.org/abs/2310.12408](http://arxiv.org/abs/2310.12408)

    本研究提出了一个针对梯度特征学习的统一分析框架，证明了双层神经网络在训练过程中的可靠性，并在多个典型问题上展示了其有效性和有趣的学习现象。

    

    神经网络在实践中取得了显著的表现，但目前的理论分析不足以理解其成功，例如神经切线核方法无法捕捉到其关键的特征学习能力，而最近对特征学习的分析通常是问题特定的。本研究提出了一个统一的分析框架，针对由梯度下降训练的双层网络。该框架以梯度特征学习原理为核心，并通过在几个典型问题中的应用来证明其有效性，例如高斯混合和奇偶函数。该框架还揭示了有趣的网络学习现象，如超越核的特征学习和彩票票据假设。

    Neural networks have achieved remarkable empirical performance, while the current theoretical analysis is not adequate for understanding their success, e.g., the Neural Tangent Kernel approach fails to capture their key feature learning ability, while recent analyses on feature learning are typically problem-specific. This work proposes a unified analysis framework for two-layer networks trained by gradient descent. The framework is centered around the principle of feature learning from gradients, and its effectiveness is demonstrated by applications in several prototypical problems, such as mixtures of Gaussians and parity functions. The framework also sheds light on interesting network learning phenomena such as feature learning beyond kernels and the lottery ticket hypothesis.
    
[^81]: 利用神经增强的消息传递实现分类辅助的鲁棒多目标跟踪

    Classification-Aided Robust Multiple Target Tracking Using Neural Enhanced Message Passing. (arXiv:2310.12407v1 [cs.LG])

    [http://arxiv.org/abs/2310.12407](http://arxiv.org/abs/2310.12407)

    本文提出了一种利用神经增强的方法进行分类辅助的鲁棒多目标跟踪，在强杂波环境中利用雷达传感器的测量信息来增强杂波剔除和数据关联，从而提高目标跟踪的鲁棒性。

    

    本文解决了在强杂波环境中利用雷达传感器测量对未知数量目标的跟踪挑战。通过利用距离-多普勒谱信息，我们识别出测量类别，这可以作为额外信息增强杂波剔除和数据关联，从而增强目标跟踪的鲁棒性。我们首先引入了一种新颖的神经增强的消息传递方法，其中通过统一的消息传递获得的信念被输入到神经网络中作为额外信息。然后利用输出的信念来优化原始信念。接着，我们提出了一种分类辅助的鲁棒多目标跟踪算法，利用神经增强的消息传递技术。该算法由三个模块组成：消息传递模块，神经网络模块和Dempster-Shafer模块。消息传递模块用于通过因子图表示统计模型，并推断目标运动状态。

    We address the challenge of tracking an unknown number of targets in strong clutter environments using measurements from a radar sensor. Leveraging the range-Doppler spectra information, we identify the measurement classes, which serve as additional information to enhance clutter rejection and data association, thus bolstering the robustness of target tracking. We first introduce a novel neural enhanced message passing approach, where the beliefs obtained by the unified message passing are fed into the neural network as additional information. The output beliefs are then utilized to refine the original beliefs. Then, we propose a classification-aided robust multiple target tracking algorithm, employing the neural enhanced message passing technique. This algorithm is comprised of three modules: a message-passing module, a neural network module, and a Dempster-Shafer module. The message-passing module is used to represent the statistical model by the factor graph and infers target kinema
    
[^82]: Loop Copilot: 用于音乐生成和迭代编辑的AI合奏系统

    Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing. (arXiv:2310.12404v1 [cs.SD])

    [http://arxiv.org/abs/2310.12404](http://arxiv.org/abs/2310.12404)

    Loop Copilot是一种新型的AI音乐合奏系统，能够通过交互式多轮对话界面生成和迭代改进音乐，通过选择适当的AI模型执行任务，并在一个集中的表中保持关键属性以确保音乐的连贯性。

    

    创建音乐是一个迭代过程，每个阶段都需要不同的方法。然而，现有的AI音乐系统在组织多个子系统以满足不同需求方面存在不足。为了解决这个问题，我们引入了Loop Copilot，这是一个能够通过交互式、多轮对话界面生成和迭代改进音乐的新型系统。该系统使用一种大型语言模型来解释用户意图，并选择适当的AI模型进行任务执行。每个后端模型都专门针对特定任务，并将它们的输出聚合起来以满足用户的要求。为了确保音乐的连贯性，关键属性被保留在一个集中的表中。我们通过半结构化的访谈和问卷调查评估了所提出的系统的有效性，突出了它在促进音乐创作方面的实用性，以及它在更广泛应用中的潜力。

    Creating music is iterative, requiring varied methods at each stage. However, existing AI music systems fall short in orchestrating multiple subsystems for diverse needs. To address this gap, we introduce Loop Copilot, a novel system that enables users to generate and iteratively refine music through an interactive, multi-round dialogue interface. The system uses a large language model to interpret user intentions and select appropriate AI models for task execution. Each backend model is specialized for a specific task, and their outputs are aggregated to meet the user's requirements. To ensure musical coherence, essential attributes are maintained in a centralized table. We evaluate the effectiveness of the proposed system through semi-structured interviews and questionnaires, highlighting its utility not only in facilitating music creation but also its potential for broader applications.
    
[^83]: 图神经网络中的协作小批次

    Cooperative Minibatching in Graph Neural Networks. (arXiv:2310.12403v1 [cs.LG])

    [http://arxiv.org/abs/2310.12403](http://arxiv.org/abs/2310.12403)

    本文提出了一种协作小批处理的方法来解决图神经网络中的邻域爆炸现象（NEP），该方法通过利用采样子图的大小与批处理大小的关系来减少每个种子顶点的工作量。

    

    在大规模训练图神经网络（GNN）时需要大量的计算资源，这个过程非常密集。减少资源需求的最有效方法之一是将小批量训练与图采样相结合。GNN具有一个独特的特性，即小批量中的项具有重叠的数据。然而，常用的独立小批量方法将每个处理单元（PE）分配给自己的小批量进行处理，导致重复计算和跨PE的输入数据访问。这放大了邻域爆炸现象（NEP），这是限制扩展性的主要瓶颈。为了减少多PE环境中NEP的影响，我们提出了一种新的方法，称为协作小批处理。我们的方法利用采样子图的大小是批处理大小的凹函数这一特性，可以明显减少每个种子顶点的工作量，同时增加批处理大小。因此，这是一种有利的方法。

    Significant computational resources are required to train Graph Neural Networks (GNNs) at a large scale, and the process is highly data-intensive. One of the most effective ways to reduce resource requirements is minibatch training coupled with graph sampling. GNNs have the unique property that items in a minibatch have overlapping data. However, the commonly implemented Independent Minibatching approach assigns each Processing Element (PE) its own minibatch to process, leading to duplicated computations and input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch size, leading to significant reductions in the amount of work per seed vertex as batch sizes increase. Hence, it is favorable 
    
[^84]: 闭式扩散模型

    Closed-Form Diffusion Models. (arXiv:2310.12395v1 [cs.LG])

    [http://arxiv.org/abs/2310.12395](http://arxiv.org/abs/2310.12395)

    本研究提出了一种闭式扩散模型，通过显式平滑的闭式得分函数来生成新样本，无需训练，且在消费级CPU上能够实现与神经SGMs相竞争的采样速度。

    

    基于得分的生成模型(SGMs)通过迭代地使用扰动目标函数的得分函数来从目标分布中采样。对于任何有限的训练集，可以闭式地评估这个得分函数，但由此得到的SGMs会记忆其训练数据，不能生成新样本。在实践中，可以通过训练神经网络来近似得分函数，但这种近似的误差有助于推广，然而神经SGMs的训练和采样代价高，而且对于这种误差提供的有效正则化方法在理论上尚不清楚。因此，在这项工作中，我们采用显式平滑的闭式得分来获得一个生成新样本的SGMs，而无需训练。我们分析了我们的模型，并提出了一个基于最近邻的高效得分函数估计器。利用这个估计器，我们的方法在消费级CPU上运行时能够达到与神经SGMs相竞争的采样速度。

    Score-based generative models (SGMs) sample from a target distribution by iteratively transforming noise using the score function of the perturbed target. For any finite training set, this score function can be evaluated in closed form, but the resulting SGM memorizes its training data and does not generate novel samples. In practice, one approximates the score by training a neural network via score-matching. The error in this approximation promotes generalization, but neural SGMs are costly to train and sample, and the effective regularization this error provides is not well-understood theoretically. In this work, we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. We analyze our model and propose an efficient nearest-neighbor-based estimator of its score function. Using this estimator, our method achieves sampling times competitive with neural SGMs while running on consumer-grade CPUs.
    
[^85]: 使用Transformer学习解决气候传感器布放问题

    Learning to Solve Climate Sensor Placement Problems with a Transformer. (arXiv:2310.12387v1 [cs.LG])

    [http://arxiv.org/abs/2310.12387](http://arxiv.org/abs/2310.12387)

    本文介绍了一种使用深度强化学习方法学习改进传感器布放策略的新方法，通过与其他方法的对比实验证明了该方法在产生高质量解决方案方面的有效性和优越性。

    

    由于其NP难性质，环境监测和灾害管理中的传感器布放优化是一个具有挑战性的问题。传统的传感器布放方法包括精确、近似或启发式方法，其中启发式方法是最常用的。然而，启发式方法受到专家直觉和经验的限制。深度学习（DL）已经成为自动生成启发式算法的一种有前景的方法。本文介绍了一种新颖的传感器布放方法，重点是使用深度强化学习（RL）方法学习改进的启发式算法。我们的方法利用了一个强化学习公式来学习改进的启发式算法，通过演员-评论家算法训练策略网络。我们通过进行全面的实验将我们的方法与几种最先进的方法进行比较，证明了我们提出的方法在产生高质量解决方案方面的有效性和优越性。我们的工作提出了一种有前景的方法，用于解决气候传感器布放问题。

    The optimal placement of sensors for environmental monitoring and disaster management is a challenging problem due to its NP-hard nature. Traditional methods for sensor placement involve exact, approximation, or heuristic approaches, with the latter being the most widely used. However, heuristic methods are limited by expert intuition and experience. Deep learning (DL) has emerged as a promising approach for generating heuristic algorithms automatically. In this paper, we introduce a novel sensor placement approach focused on learning improvement heuristics using deep reinforcement learning (RL) methods. Our approach leverages an RL formulation for learning improvement heuristics, driven by an actor-critic algorithm for training the policy network. We compare our method with several state-of-the-art approaches by conducting comprehensive experiments, demonstrating the effectiveness and superiority of our proposed approach in producing high-quality solutions. Our work presents a promisi
    
[^86]: 双边贸易中基于全局预算平衡的无悔学习

    No-Regret Learning in Bilateral Trade via Global Budget Balance. (arXiv:2310.12370v1 [cs.GT])

    [http://arxiv.org/abs/2310.12370](http://arxiv.org/abs/2310.12370)

    本文引入全局预算平衡的概念，提出了首个针对具有对抗输入的双边贸易的无悔算法，并在不同的反馈模型下进行了验证。

    

    双边贸易涉及在两个战略代理人之间促进交易的挑战，一个是卖家，一个是买家，两者都对物品有私人估值。我们研究了该问题的在线版本，在每个时间步长，都会出现一个新的卖家和买家。学习者的任务是在不了解他们估值的情况下为每个代理人设置价格。卖家和买家的序列由一个遗忘性对手选择。在这个设置下，已知的负面结果排除了在每次迭代中学习者必须保证预算平衡的可能性。在本文中，我们引入了全局预算平衡的概念，该概念仅要求代理人在整个时间范围内保持预算平衡。通过要求全局预算平衡，我们提供了首个针对具有对抗输入的双边贸易的无悔算法，并在不同的反馈模型下进行了验证。首先，我们证明在全反馈模型中，学习者可以实现最小遗憾。

    Bilateral trade revolves around the challenge of facilitating transactions between two strategic agents -- a seller and a buyer -- both of whom have a private valuations for the item. We study the online version of the problem, in which at each time step a new seller and buyer arrive. The learner's task is to set a price for each agent, without any knowledge about their valuations. The sequence of sellers and buyers is chosen by an oblivious adversary. In this setting, known negative results rule out the possibility of designing algorithms with sublinear regret when the learner has to guarantee budget balance for each iteration. In this paper, we introduce the notion of global budget balance, which requires the agent to be budget balance only over the entire time horizon. By requiring global budget balance, we provide the first no-regret algorithms for bilateral trade with adversarial inputs under various feedback models. First, we show that in the full-feedback model the learner can g
    
[^87]: MARVEL: 用于大规模可变速限的多智能体强化学习

    MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits. (arXiv:2310.12359v1 [cs.MA])

    [http://arxiv.org/abs/2310.12359](http://arxiv.org/abs/2310.12359)

    MARVEL是一个多智能体强化学习框架，可以实现利用常见数据在高速公路走廊上进行大规模可变速限控制。它通过奖励结构和智能体之间的协调，提高交通安全性和流动性。与无控制情况相比，MARVEL可以提高交通安全性63.4%，提高交通流动性14.6%。

    

    可变速限（VSL）控制是一种提高安全性和流动性的有前途的交通管理策略。本工作介绍了MARVEL，这是一个使用仅有常见可用数据实现高速公路走廊大规模VSL控制的多智能体强化学习（MARL）框架。智能体通过包括对交通状况的适应性、安全性和流动性在内的奖励结构进行学习，实现了智能体之间的协调。该框架通过在所有VSL智能体之间共享参数，可以扩展到包括许多立柱的走廊。智能体在一个基于一个短的高速公路路段的微仿真环境中进行训练，该路段有8个立柱，跨越7英里，并在纳什维尔附近的I-24上有34个立柱，跨越17英里进行测试。与无控制情况相比，MARVEL将交通安全性提高了63.4%，并将交通流动性提高了14.6%，与已在I-24上部署的最新算法相比。进行了一项可解释性分析。

    Variable speed limit (VSL) control is a promising traffic management strategy for enhancing safety and mobility. This work introduces MARVEL, a multi-agent reinforcement learning (MARL) framework for implementing large-scale VSL control on freeway corridors using only commonly available data. The agents learn through a reward structure that incorporates adaptability to traffic conditions, safety, and mobility; enabling coordination among the agents. The proposed framework scales to cover corridors with many gantries thanks to a parameter sharing among all VSL agents. The agents are trained in a microsimulation environment based on a short freeway stretch with 8 gantries spanning 7 miles and tested with 34 gantries spanning 17 miles of I-24 near Nashville, TN. MARVEL improves traffic safety by 63.4% compared to the no control scenario and enhances traffic mobility by 14.6% compared to a state-of-the-practice algorithm that has been deployed on I-24. An explainability analysis is underta
    
[^88]: 使用外部信息的网络流量状态预测：一种基于多维图神经网络注意力的方法

    Networkwide Traffic State Forecasting Using Exogenous Information: A Multi-Dimensional Graph Attention-Based Approach. (arXiv:2310.12353v1 [cs.LG])

    [http://arxiv.org/abs/2310.12353](http://arxiv.org/abs/2310.12353)

    本文提出了一种基于多维图神经网络注意力的交通预测方法，利用过去观测到的速度、车道封闭事件、温度和可视性来预测交通情况。该方法基于交通网络结构进行学习，展示了较好的实验结果。

    

    交通状态预测对交通管理和控制策略，以及交通网络的用户和系统级决策至关重要。尽管在过去的几十年里，交通预测已经用多种技术进行了研究，但大多数方法仅依赖内生交通变量进行状态预测，尽管有证据表明外生因素可以显著影响交通状况。本文提出了一种基于多维时空图神经网络注意力的交通预测方法（M-STGAT），它根据过去观测到的速度、车道封闭事件、温度和可视性来预测交通情况。该方法基于一个注意力图神经网络结构，还根据观察到这些变量的交通网络的结构进行学习。通过使用来自加利福尼亚州交通部的交通速度和车道封闭数据进行数值实验。

    Traffic state forecasting is crucial for traffic management and control strategies, as well as user- and system-level decision making in the transportation network. While traffic forecasting has been approached with a variety of techniques over the last couple of decades, most approaches simply rely on endogenous traffic variables for state prediction, despite the evidence that exogenous factors can significantly impact traffic conditions. This paper proposes a multi-dimensional spatio-temporal graph attention-based traffic prediction approach (M-STGAT), which predicts traffic based on past observations of speed, along with lane closure events, temperature, and visibility across the transportation network. The approach is based on a graph attention network architecture, which also learns based on the structure of the transportation network on which these variables are observed. Numerical experiments are performed using traffic speed and lane closure data from the California Department 
    
[^89]: 为联邦图神经网络提供结构感知群体公平性

    Equipping Federated Graph Neural Networks with Structure-aware Group Fairness. (arXiv:2310.12350v1 [cs.LG])

    [http://arxiv.org/abs/2310.12350](http://arxiv.org/abs/2310.12350)

    本论文提出了一种名为F2GNN的方法，它旨在增强联邦图神经网络的群体公平性，解决了在联邦学习中减轻偏见的新挑战。

    

    图神经网络（GNN）广泛应用于不同领域的各种图数据处理和分析任务。由于隐私和监管限制，对集中式图数据进行训练可能不可行。因此，联邦学习（FL）成为解决这一挑战的一种趋势性解决方案。然而，由于GNN可能从训练数据中继承历史偏见并导致歧视性预测，在分布式环境中，局部模型的偏见很容易传播到全局模型，这给在联邦GNN中减轻偏见带来了新的挑战。为了解决这一问题，我们提出了F2GNN，一种增强联邦GNN群体公平性的方法。由于偏见可能来自数据和学习算法，F2GNN旨在在联邦环境下减少这两种类型的偏见。

    Graph Neural Networks (GNNs) have been widely used for various types of graph data processing and analytical tasks in different domains. Training GNNs over centralized graph data can be infeasible due to privacy concerns and regulatory restrictions. Thus, federated learning (FL) becomes a trending solution to address this challenge in a distributed learning paradigm. However, as GNNs may inherit historical bias from training data and lead to discriminatory predictions, the bias of local models can be easily propagated to the global model in distributed settings. This poses a new challenge in mitigating bias in federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair Federated Graph Neural Network, that enhances group fairness of federated GNNs. As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN aims to mitigate both types of bias under federated settings. First, we provide theoretical insights on the connection between data bias in a tra
    
[^90]: 使用夜间灯光和社交媒体追踪电力损失及其感知原因

    Tracking electricity losses and their perceived causes using nighttime light and social media. (arXiv:2310.12346v1 [physics.soc-ph])

    [http://arxiv.org/abs/2310.12346](http://arxiv.org/abs/2310.12346)

    本研究利用夜间灯光数据和社交媒体监测停电及其感知原因，在委内瑞拉的案例中显示出夜间灯光强度与停电量之间的反向关系，并发现提到委内瑞拉总统的推文显示出更高的负面情绪和绿色词汇趋势。

    

    城市环境是复杂的系统，关键基础设施的故障可能对社区的经济和社会福祉产生影响。电力系统具有特殊意义，因为它们对其他基础设施至关重要，中断可能引发广泛后果。通常，评估电力供应需要地面数据，而在冲突区域和资源有限的地区可能具有挑战性。本研究展示了如何利用卫星图像、社交媒体和信息提取来监测停电及其感知原因。利用2019年3月的夜间灯光数据（委内瑞拉卡拉卡斯）来指示停电区域。利用Twitter数据来确定情感和主题趋势，同时进行统计分析和主题建模以深入了解公众对停电原因的看法。研究结果显示夜间灯光强度与停电量之间存在反向关系。提到委内瑞拉总统的推文显示出更高的负面情绪和绿色词汇趋势。

    Urban environments are intricate systems where the breakdown of critical infrastructure can impact both the economic and social well-being of communities. Electricity systems hold particular significance, as they are essential for other infrastructure, and disruptions can trigger widespread consequences. Typically, assessing electricity availability requires ground-level data, a challenge in conflict zones and regions with limited access. This study shows how satellite imagery, social media, and information extraction can monitor blackouts and their perceived causes. Night-time light data (in March 2019 for Caracas, Venezuela) is used to indicate blackout regions. Twitter data is used to determine sentiment and topic trends, while statistical analysis and topic modeling delved into public perceptions regarding blackout causes. The findings show an inverse relationship between nighttime light intensity. Tweets mentioning the Venezuelan President displayed heightened negativity and a gre
    
[^91]: ClusT3:信息不变的测试时间训练

    ClusT3: Information Invariant Test-Time Training. (arXiv:2310.12345v1 [cs.CV])

    [http://arxiv.org/abs/2310.12345](http://arxiv.org/abs/2310.12345)

    ClusT3是一种新颖的无监督测试时间训练方法，通过最大化多尺度特征图和离散潜在表示之间的互信息来增强深度学习模型的鲁棒性。实验结果表明，在不同的测试时间自适应基准上具有竞争性的分类性能。

    

    深度学习模型在各种视觉任务中展现出了显著的性能。然而，它们在测试时间经常受到域偏移的影响。为了减轻这些漏洞，已经开发了测试时间训练（TTT）方法，其中在训练时同时解决了一个次要任务和主任务，并在测试时作为自监督的代理任务使用。在本研究中，我们提出了一种基于多尺度特征图和离散潜在表示之间的互信息最大化的新颖无监督TTT技术，该技术可以作为辅助聚类任务集成到标准训练中。实验结果表明，在不同的常见测试时间自适应基准上具有竞争性的分类性能。

    Deep Learning models have shown remarkable performance in a broad range of vision tasks. However, they are often vulnerable against domain shifts at test-time. Test-time training (TTT) methods have been developed in an attempt to mitigate these vulnerabilities, where a secondary task is solved at training time simultaneously with the main task, to be later used as an self-supervised proxy task at test-time. In this work, we propose a novel unsupervised TTT technique based on the maximization of Mutual Information between multi-scale feature maps and a discrete latent representation, which can be integrated to the standard training as an auxiliary clustering task. Experimental results demonstrate competitive classification performance on different popular test-time adaptation benchmarks.
    
[^92]: 通过平衡教师和研究者的激励，探索适应性实验促进持续改进的机会

    Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives. (arXiv:2310.12324v1 [cs.HC])

    [http://arxiv.org/abs/2310.12324](http://arxiv.org/abs/2310.12324)

    适应性实验为持续课程改进提供了机会，通过动态部署最有效的条件以满足学生需求。

    

    随机实验比较不同教学策略的机会可以为教师的决策提供有用的经验证据。然而，传统实验缺乏清晰简明的使用数据快速增加实验学生获得最佳条件机会的途径。受领先科技公司在产品开发中使用机器学习和实验的启示，我们探讨了如何利用适应性实验来持续改进课程。在适应性实验中，不同的条件将被应用于学生身上，数据将被分析并用于改变未来学生的学习体验。可以使用机器学习算法来识别哪些行动可以更有希望改善学生的体验或结果。然后，该算法可以动态地将最有效的条件应用于未来的学生，从而更好地满足学生的需求。我们通过实例说明了这种方法的应用。

    Randomized experimental comparisons of alternative pedagogical strategies could provide useful empirical evidence in instructors' decision-making. However, traditional experiments do not have a clear and simple pathway to using data rapidly to try to increase the chances that students in an experiment get the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might help in continuous course improvement. In adaptive experiments, as different arms/conditions are deployed to students, data is analyzed and used to change the experience for future students. This can be done using machine learning algorithms to identify which actions are more promising for improving student experience or outcomes. This algorithm can then dynamically deploy the most effective conditions to future students, resulting in better support for students' needs. We illustrate the appr
    
[^93]: 一种统一的学习论证语义的框架

    A Unifying Framework for Learning Argumentation Semantics. (arXiv:2310.12309v1 [cs.AI])

    [http://arxiv.org/abs/2310.12309](http://arxiv.org/abs/2310.12309)

    本文提出了一种统一的框架来学习论证语义，通过使用可解释的方法，优于现有的论证求解器，并在形式论证和人机对话领域开辟了新的研究方向。

    

    论证是人工智能领域的一个非常活跃的研究领域，涉及到在人与人或人与人工智能代理之间的对话中所使用的论证的表示和评估。正式论证系统的可接受性语义定义了论证的接受或拒绝的标准。已经开发了一些称为论证求解器的软件系统，用于使用这些标准计算被接受/被拒绝的论证。其中一些系统通过使用不可解释的方法来学习识别接受的论证。在本文中，我们提出了一种新颖的框架，该框架采用归纳逻辑编程方法以可解释的方式来学习多个抽象和结构化论证框架的可接受性语义。通过实证评估，我们展示了我们的框架优于现有的论证求解器，从而开辟了形式论证和人机对话领域的新的未来研究方向。

    Argumentation is a very active research field of Artificial Intelligence concerned with the representation and evaluation of arguments used in dialogues between humans and/or artificial agents. Acceptability semantics of formal argumentation systems define the criteria for the acceptance or rejection of arguments. Several software systems, known as argumentation solvers, have been developed to compute the accepted/rejected arguments using such criteria. These include systems that learn to identify the accepted arguments using non-interpretable methods. In this paper we present a novel framework, which uses an Inductive Logic Programming approach to learn the acceptability semantics for several abstract and structured argumentation frameworks in an interpretable way. Through an empirical evaluation we show that our framework outperforms existing argumentation solvers, thus opening up new future research directions in the area of formal argumentation and human-machine dialogues.
    
[^94]: 分子语言模型的偏好优化

    Preference Optimization for Molecular Language Models. (arXiv:2310.12304v1 [stat.ML])

    [http://arxiv.org/abs/2310.12304](http://arxiv.org/abs/2310.12304)

    本研究调查了使用直接偏好优化进行微调的方法，以更好地与化学家的偏好对齐生成的分子。这种方法简单、高效，且非常有效。

    

    分子语言建模是一种生成新颖化学结构的有效方法。然而，这些模型不会\emph{先验地}编码化学家可能期望的某些偏好。本研究调查了使用直接偏好优化进行微调的方法，以更好地与化学家的偏好对齐生成的分子。我们的发现表明，这种方法简单、高效，且非常有效。

    Molecular language modeling is an effective approach to generating novel chemical structures. However, these models do not \emph{a priori} encode certain preferences a chemist may desire. We investigate the use of fine-tuning using Direct Preference Optimization to better align generated molecules with chemist preferences. Our findings suggest that this approach is simple, efficient, and highly effective.
    
[^95]: 文档级语言模型用于机器翻译

    Document-Level Language Models for Machine Translation. (arXiv:2310.12303v1 [cs.CL])

    [http://arxiv.org/abs/2310.12303](http://arxiv.org/abs/2310.12303)

    这项工作提出了一种利用文档级别语言模型构建上下文感知的翻译系统的方法。通过结合任何现有的句级别翻译模型与文档级别语言模型，并借鉴模型组合的最新进展，尤其是权重技术的提出，可以显著提高文档级别指标并降低计算开销。

    

    尽管已知存在局限性，但大多数机器翻译系统仍然在句级别上运行。其中一个原因是，大多数平行训练数据只有句级别的对齐，没有文档级别的元信息。在这项工作中，我们利用文档级别的单语数据构建上下文感知的翻译系统。我们通过将任何现有的句级别翻译模型与文档级别语言模型相结合来实现这一目标。我们通过利用模型组合的最新进展来改进现有方法。此外，我们提出了能够使系统组合更灵活、显著降低计算开销的权重技术。通过对四个不同的翻译任务进行全面评估，我们证明了我们的扩展显著提高了文档级别指标，并且在计算效率上也更优。然而，我们还发现在大多数情况下，反向翻译的结果更好，

    Despite the known limitations, most machine translation systems today still operate on the sentence-level. One reason for this is, that most parallel training data is only sentence-level aligned, without document-level meta information available. In this work, we set out to build context-aware translation systems utilizing document-level monolingual data instead. This can be achieved by combining any existing sentence-level translation model with a document-level language model. We improve existing approaches by leveraging recent advancements in model combination. Additionally, we propose novel weighting techniques that make the system combination more flexible and significantly reduce computational overhead. In a comprehensive evaluation on four diverse translation tasks, we show that our extensions improve document-targeted scores substantially and are also computationally more efficient. However, we also find that in most scenarios, back-translation gives even better results, at the
    
[^96]: Jorge: GPU高效的二阶优化的近似预处理方法

    Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization. (arXiv:2310.12298v1 [cs.LG])

    [http://arxiv.org/abs/2310.12298](http://arxiv.org/abs/2310.12298)

    本文介绍了Jorge，一种GPU高效的二阶优化算法，通过近似预处理方法替代矩阵求逆计算来提高计算效率，同时兼具二阶方法的收敛性能。实验证明了Jorge的有效性。

    

    尽管与一阶优化器相比，二阶优化器具有更好的收敛性能，但由于计算成本较大，深度学习中的二阶优化器一直不太受欢迎。这种优化器中的主要效率瓶颈是预处理步骤中的矩阵求逆计算，在GPU上计算昂贵。在本文中，我们引入了Jorge，一种二阶优化器，它兼具二阶方法的快速收敛特性和一阶方法的高计算效率。我们通过完全消除矩阵求逆计算的方法来解决计算瓶颈，用近似的预处理器计算替代。这使得Jorge在墙钟时间上在GPU上非常高效。此外，我们描述了一种直接从调整良好的SGD基准中确定Jorge超参数的方法，从而显著减少了调参工作。我们的实证评估证明了Jorge的效果。

    Despite their better convergence properties compared to first-order optimizers, second-order optimizers for deep learning have been less popular due to their significant computational costs. The primary efficiency bottleneck in such optimizers is matrix inverse calculations in the preconditioning step, which are expensive to compute on GPUs. In this paper, we introduce Jorge, a second-order optimizer that promises the best of both worlds -- rapid convergence benefits of second-order methods, and high computational efficiency typical of first-order methods. We address the primary computational bottleneck of computing matrix inverses by completely eliminating them using an approximation of the preconditioner computation. This makes Jorge extremely efficient on GPUs in terms of wall-clock time. Further, we describe an approach to determine Jorge's hyperparameters directly from a well-tuned SGD baseline, thereby significantly minimizing tuning efforts. Our empirical evaluations demonstrate
    
[^97]: 开放式多元时间序列异常检测

    Open-Set Multivariate Time-Series Anomaly Detection. (arXiv:2310.12294v1 [cs.LG])

    [http://arxiv.org/abs/2310.12294](http://arxiv.org/abs/2310.12294)

    本论文提出了一种开放式时间序列异常检测方法，能够在训练阶段识别有限异常类别的少量标记异常，并在测试阶段检测到见过和未见过的异常类别。

    

    近年来出现了许多时间序列异常检测方法。大多数现有方法是无监督的，仅假设有正常的训练样本，而一些监督方法通过在训练阶段加入标记的异常样本来提高性能。然而，某些异常类型对无监督方法来说在区分正常数据时具有挑战性，而监督方法仅能检测类似于训练期间存在的异常，无法推广到未见异常类别。本文首次提出了一种针对开放式时间序列异常检测问题的新方法，在训练阶段可以看到来自有限异常类别的少量标记异常，并旨在在测试阶段检测到见过和未见过的异常类别。所提出的方法被称为多变量开放式时间序列异常检测（MOSAD），包括三个主要步骤。

    Numerous methods for time series anomaly detection (TSAD) methods have emerged in recent years. Most existing methods are unsupervised and assume the availability of normal training samples only, while few supervised methods have shown superior performance by incorporating labeled anomalous samples in the training phase. However, certain anomaly types are inherently challenging for unsupervised methods to differentiate from normal data, while supervised methods are constrained to detecting anomalies resembling those present during training, failing to generalize to unseen anomaly classes. This paper is the first attempt in providing a novel approach for the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are visible in the training phase, with the objective of detecting both seen and unseen anomaly classes in the test phase. The proposed method, called Multivariate Open-Set timeseries Anomaly Detection (MOSAD) consists of three prim
    
[^98]: 使用图表示学习提高 MOOC 自动化成绩预测的性能

    Enhancing the Performance of Automated Grade Prediction in MOOC using Graph Representation Learning. (arXiv:2310.12281v1 [cs.LG])

    [http://arxiv.org/abs/2310.12281](http://arxiv.org/abs/2310.12281)

    本研究通过图表示学习构建了一个独特的知识图谱，以提高MOOC中自动化成绩预测的性能。

    

    近年来，大规模在线开放课程（MOOCs）作为一种快速增长的在线学习现象获得了显著的关注。与传统的教室不同，MOOCs提供了一个独特的机会，以满足来自不同背景和地理位置的各种受众。著名大学和专门提供MOOCs的供应商，如Coursera，在各种主题上提供MOOC课程。由于高入学率和教师与学习者之间的有限直接互动，自动评估任务如成绩和早期退学预测变得必要。然而，当前自动评估方法忽视了下游任务中涉及不同实体之间的结构链接，例如学生和课程。我们的假设是，通过交互图表现的这些结构关系包含可以提高所需任务性能的宝贵信息。为了验证这一点，我们为一个大规模MOOC构建了一个独特的知识图谱。

    In recent years, Massive Open Online Courses (MOOCs) have gained significant traction as a rapidly growing phenomenon in online learning. Unlike traditional classrooms, MOOCs offer a unique opportunity to cater to a diverse audience from different backgrounds and geographical locations. Renowned universities and MOOC-specific providers, such as Coursera, offer MOOC courses on various subjects. Automated assessment tasks like grade and early dropout predictions are necessary due to the high enrollment and limited direct interaction between teachers and learners. However, current automated assessment approaches overlook the structural links between different entities involved in the downstream tasks, such as the students and courses. Our hypothesis suggests that these structural relationships, manifested through an interaction graph, contain valuable information that can enhance the performance of the task at hand. To validate this, we construct a unique knowledge graph for a large MOOC 
    
[^99]: 一图抵千言：使用多概念提示学习来学习对象级概念

    An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])

    [http://arxiv.org/abs/2310.12274](http://arxiv.org/abs/2310.12274)

    提出了一种多概念提示学习（MCPL）框架，通过同时学习多个新的“词”来解决在单个场景中识别和整合多个对象级概念的挑战。针对词概念相关性准确性问题，提出了注意力掩码、提示对比损失和绑定形容词等三种正则化技术。通过图像生成进行了评估，结果表明该框架能够生成更多样化和合成的图像。

    

    文字反转是一种提示学习方法，它学习一种新的“单词”的嵌入表示图像风格和外观，使其能够整合到自然语言句子中生成新的合成图像。然而，即使对于可获得个别概念的嵌入，识别和整合一个场景中的多个对象级概念仍然面临着显著的挑战，这也得到了我们的实证测试的进一步证实。为了解决这个挑战，我们引入了一个多概念提示学习（MCPL）的框架，可以从一个句子-图像对中同时学习多个新的“词”。为了增强词概念相关性的准确性，我们提出了三种正则化技术：注意力掩码（AttnMask）将学习集中在相关区域；提示对比损失（PromptCL）将不同概念的嵌入分离开来；以及绑定形容词（Bind adj.）将新的“词”与已知词相关联。我们通过图像生成进行评估

    Textural Inversion, a prompt learning method, learns a singular embedding for a new "word" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation
    
[^100]: 改进SCGAN的相似性约束并学习更好的解耦表示

    Improving SCGAN's Similarity Constraint and Learning a Better Disentangled Representation. (arXiv:2310.12262v1 [cs.CV])

    [http://arxiv.org/abs/2310.12262](http://arxiv.org/abs/2310.12262)

    本论文改进了SCGAN模型中的相似性约束，使用SSIM度量图像相似性并应用对比损失原则，提高了模型的性能和泛化能力。

    

    SCGAN在生成对抗网络中添加了一个相似性约束，将生成的图像与条件之间的相似性作为正则化项。相似性约束作为导师，指导生成器网络理解基于条件的表示差异。我们深入理解了SCGAN的工作原理。这种理解使我们意识到相似性约束的功能类似于对比损失函数。我们相信，具有高度理解和智能的模型可以根据图像的结构和高级特征来度量它们之间的相似性，就像人类一样。我们对SCGAN进行了两个主要改变，以创建一个改进的模型：使用SSIM来度量图像之间的相似性，并将对比损失原则应用于相似性约束。改进的模型在FID和FactorVAE指标下表现更好。与其他模型相比，改进的模型还具有更好的泛化能力。

    SCGAN adds a similarity constraint between generated images and conditions as a regularization term on generative adversarial networks. Similarity constraint works as a tutor to instruct the generator network to comprehend the difference of representations based on conditions. We understand how SCGAN works on a deeper level. This understanding makes us realize that the similarity constraint functions like the contrastive loss function. We believe that a model with high understanding and intelligence measures the similarity between images based on their structure and high level features, just like humans do. Two major changes we applied to SCGAN in order to make a modified model are using SSIM to measure similarity between images and applying contrastive loss principles to the similarity constraint. The modified model performs better using FID and FactorVAE metrics. The modified model also has better generalisability compared to other models. Keywords Generative Adversarial Nets, Unsupe
    
[^101]: MDP中LTL和ω-regular目标的PAC学习算法

    A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs. (arXiv:2310.12248v1 [cs.LG])

    [http://arxiv.org/abs/2310.12248](http://arxiv.org/abs/2310.12248)

    这个论文介绍了一种基于模型的PAC学习算法，用于在MDP中学习ω-regular目标，不需要系统拓扑的先前知识。

    

    线性时序逻辑（LTL）和ω-regular目标是近期用于在强化学习中表达非马尔可夫目标的一种方式。我们提出了一种基于模型的可能近似正确（PAC）学习算法，用于MDP中的ω-regular目标。与之前的方法不同，我们的算法从系统的采样轨迹中进行学习，并且不需要系统拓扑的先前知识。

    Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL -- have seen recent use as a way to express non-Markovian objectives in reinforcement learning. We introduce a model-based probably approximately correct (PAC) learning algorithm for omega-regular objectives in Markov decision processes. Unlike prior approaches, our algorithm learns from sampled trajectories of the system and does not require prior knowledge of the system's topology.
    
[^102]: 一种统一的带有记忆的领域增量学习方法: 理论和算法

    A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm. (arXiv:2310.12244v1 [cs.LG])

    [http://arxiv.org/abs/2310.12244](http://arxiv.org/abs/2310.12244)

    这篇论文提出了一种统一的带有记忆的领域增量学习方法（UDIL），通过统一不同的现有方法并使用自适应系数，实现了更紧的泛化误差界限，并在实验证明在合成数据和真实数据集上优于最先进的方法。

    

    领域增量学习旨在适应一系列领域，仅能访问先前领域的一小部分数据（即记忆）。针对这个问题已经提出了各种方法，但它们之间的关系以及从实践者角度何时选择其中一种方法仍然不清楚。为此，我们提出了一种统一的框架，称为统一领域增量学习（UDIL），用于带有记忆的领域增量学习。我们的UDIL将各种现有方法统一起来，我们的理论分析表明，与这些方法相比，UDIL始终实现更紧的泛化误差界限。关键观点是不同的现有方法对应于我们的边界具有不同的固定系数；基于这种统一的洞察力，我们的UDIL允许在训练过程中使用自适应系数，从而始终实现最紧的界限。实证结果表明，我们的UDIL在合成数据和真实数据集上均优于最先进的领域增量学习方法。

    Domain incremental learning aims to adapt to a sequence of domains with access to only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, dubbed Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL **unifies** various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different **fixed** coefficients; based on insights from this unification, our UDIL allows **adaptive** coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthe
    
[^103]: REVAMP：自动化模拟现实场景中对任意对象的对抗攻击

    REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary Objects in Realistic Scenes. (arXiv:2310.12243v1 [cs.LG])

    [http://arxiv.org/abs/2310.12243](http://arxiv.org/abs/2310.12243)

    REVAMP是一个易于使用的Python库，可用于在现实场景中进行对任意对象的对抗攻击的自动化模拟。通过使用可微渲染和模拟真实环境因素，REVAMP使研究人员和实践者能够快速探索各种攻击场景。

    

    深度学习模型，如自动驾驶车辆中使用的模型，对于对抗攻击是脆弱的，攻击者可以在环境中放置对抗性物体，从而导致误分类。虽然在数字空间中生成这些对抗性物体已被广泛研究，但是成功地将这些攻击从数字领域转移到现实领域在控制现实世界环境因素方面仍然具有挑战性。鉴于这些限制，我们介绍了REVAMP，一个易于使用的Python库，它是首个为创建具有任意对象的攻击场景并模拟真实环境因素、光照、反射和折射的工具。REVAMP使研究人员和实践者能够通过提供广泛可配置的选项来设计实验并使用可微渲染来复现物理上可信的对抗性物体，从而迅速探索数字领域中的各种场景。

    Deep Learning models, such as those used in an autonomous vehicle are vulnerable to adversarial attacks where an attacker could place an adversarial object in the environment, leading to mis-classification. Generating these adversarial objects in the digital space has been extensively studied, however successfully transferring these attacks from the digital realm to the physical realm has proven challenging when controlling for real-world environmental factors. In response to these limitations, we introduce REVAMP, an easy-to-use Python library that is the first-of-its-kind tool for creating attack scenarios with arbitrary objects and simulating realistic environmental factors, lighting, reflection, and refraction. REVAMP enables researchers and practitioners to swiftly explore various scenarios within the digital realm by offering a wide range of configurable options for designing experiments and using differentiable rendering to reproduce physically plausible adversarial objects. We 
    
[^104]: 通过隐式图对齐进行少样本背景下的模仿学习

    Few-Shot In-Context Imitation Learning via Implicit Graph Alignment. (arXiv:2310.12238v1 [cs.RO])

    [http://arxiv.org/abs/2310.12238](http://arxiv.org/abs/2310.12238)

    本文提出了一种少样本背景下的模仿学习方法，通过将模仿学习视为物体的图表示之间的条件对齐问题，实现了在没有先验知识或进一步训练的情况下，机器人可以在新的物体集上执行任务。

    

    考虑以下问题：给定在几个不同物体上进行的任务的少量演示，机器人如何学会在新的、之前未见过的物体上执行相同的任务？这是具有挑战性的，因为类别内多样的物体使得推断新物体与演示中的物体之间的任务相关关系变得困难。我们通过将模仿学习视为物体的图表示之间的条件对齐问题来解决这个问题。因此，我们展示了这种条件允许背景下的学习，在演示之后，机器人可以立即在一组新物体上执行任务，而无需关于物体类别的任何先验知识或任何进一步的训练。在实验中，我们探索和验证了我们的设计选择，并证明了我们的方法在几个真实世界的日常任务的少样本学习中非常有效，同时优于基线方法。可以在我们的项目网页上观看视频。

    Consider the following problem: given a few demonstrations of a task across a few different objects, how can a robot learn to perform that same task on new, previously unseen objects? This is challenging because the large variety of objects within a class makes it difficult to infer the task-relevant relationship between the new objects and the objects in the demonstrations. We address this by formulating imitation learning as a conditional alignment problem between graph representations of objects. Consequently, we show that this conditioning allows for in-context learning, where a robot can perform a task on a set of new objects immediately after the demonstrations, without any prior knowledge about the object class or any further training. In our experiments, we explore and validate our design choices, and we show that our method is highly effective for few-shot learning of several real-world, everyday tasks, whilst outperforming baselines. Videos are available on our project webpag
    
[^105]: 使用归一化流在脉冲星时间阵列上进行快速参数推断

    Fast Parameter Inference on Pulsar Timing Arrays with Normalizing Flows. (arXiv:2310.12209v1 [astro-ph.IM])

    [http://arxiv.org/abs/2310.12209](http://arxiv.org/abs/2310.12209)

    本研究使用归一化流加速且准确地估计了脉冲星时间阵列中随机引力波背景的后验分布。相比传统MCMC方法，将采样时间从几周缩短到几秒钟。

    

    脉冲星时间阵列（PTAs）使用昂贵的MCMC方法进行贝叶斯后验推断。给定一个包含约10-100颗脉冲星和每颗星的O（10^3）个时间残差的数据集，为随机引力波背景（SGWB）生成后验分布可能需要几天到一周的时间。计算瓶颈产生的原因是在考虑搜索空间的维度时，MCMC所需的似然函数评估非常昂贵。幸运的是，生成模拟数据是快速的，所以可以使用现代基于模拟的推断技术来解决这个问题。在本文中，我们演示了如何使用在模拟数据上训练的条件归一化流来对SGWB后验进行极快且准确的估计，将采样时间从几周减少到几秒钟。

    Pulsar timing arrays (PTAs) perform Bayesian posterior inference with expensive MCMC methods. Given a dataset of ~10-100 pulsars and O(10^3) timing residuals each, producing a posterior distribution for the stochastic gravitational wave background (SGWB) can take days to a week. The computational bottleneck arises because the likelihood evaluation required for MCMC is extremely costly when considering the dimensionality of the search space. Fortunately, generating simulated data is fast, so modern simulation-based inference techniques can be brought to bear on the problem. In this paper, we demonstrate how conditional normalizing flows trained on simulated data can be used for extremely fast and accurate estimation of the SGWB posteriors, reducing the sampling time from weeks to a matter of seconds.
    
[^106]: GNN聚合编程抽象的架构影响

    Architectural Implications of GNN Aggregation Programming Abstractions. (arXiv:2310.12184v1 [cs.LG])

    [http://arxiv.org/abs/2310.12184](http://arxiv.org/abs/2310.12184)

    本文通过对现有GNN聚合编程抽象进行分类，并在最先进的GNN库上进行特征研究和性能比较，提供了未来GNN加速的见解。

    

    图神经网络（GNN）由于从图数据中提取有用表示的强大能力而受到广泛关注。随着对高效GNN计算的需求增加，为优化GNN聚合而设计的各种编程抽象应运而生，以促进加速。然而，对现有抽象没有全面的评估和分析，因此对哪种方法更好没有明确的共识。在这封信中，我们通过数据组织和传播方法的维度对现有的GNN聚合编程抽象进行分类。通过在最先进的GNN库上构建这些抽象，我们进行了彻底和详细的特征研究，以比较它们的性能和效率，并根据我们的分析提供了一些关于未来GNN加速的见解。

    Graph neural networks (GNNs) have gained significant popularity due to the powerful capability to extract useful representations from graph data. As the need for efficient GNN computation intensifies, a variety of programming abstractions designed for optimizing GNN Aggregation have emerged to facilitate acceleration. However, there is no comprehensive evaluation and analysis upon existing abstractions, thus no clear consensus on which approach is better. In this letter, we classify existing programming abstractions for GNN Aggregation by the dimension of data organization and propagation method. By constructing these abstractions on a state-of-the-art GNN library, we perform a thorough and detailed characterization study to compare their performance and efficiency, and provide several insights on future GNN acceleration based on our analysis.
    
[^107]: 增强的图神经网络与以“自我为中心”的谱子图嵌入增强

    Enhanced Graph Neural Networks with Ego-Centric Spectral Subgraph Embeddings Augmentation. (arXiv:2310.12169v1 [cs.SI])

    [http://arxiv.org/abs/2310.12169](http://arxiv.org/abs/2310.12169)

    本论文提出了一种名为ESGEA的新方法，在缺乏信息的情况下增强并设计节点特征，通过利用局部子图的拓扑结构来创建拓扑感知的节点特征。我们的方法使用高效的谱图嵌入技术生成子图特征，并将其作为节点特征捕捉网络的局部拓扑组织。

    

    图神经网络（GNNs）在复杂网络中执行各种基于学习的任务时显示出显著的优点。GNN的优越性能往往与输入网络中节点级特征的可用性和质量相关。然而，对于许多网络应用来说，这种节点级信息可能是缺失或不可靠的，从而限制了GNN的适用性和功效。为了解决这个问题，我们提出了一种名为“以自我为中心的谱子图嵌入增强（ESGEA）”的新方法，旨在增强和设计节点特征，特别是在信息缺失的情况下。我们的方法利用局部子图的拓扑结构来创建拓扑感知的节点特征。子图特征是使用高效的谱图嵌入技术生成的，它们作为节点特征捕捉了网络的局部拓扑组织。然后，如果存在明确的节点特征，则对其进行增强。

    Graph Neural Networks (GNNs) have shown remarkable merit in performing various learning-based tasks in complex networks. The superior performance of GNNs often correlates with the availability and quality of node-level features in the input networks. However, for many network applications, such node-level information may be missing or unreliable, thereby limiting the applicability and efficacy of GNNs. To address this limitation, we present a novel approach denoted as Ego-centric Spectral subGraph Embedding Augmentation (ESGEA), which aims to enhance and design node features, particularly in scenarios where information is lacking. Our method leverages the topological structure of the local subgraph to create topology-aware node features. The subgraph features are generated using an efficient spectral graph embedding technique, and they serve as node features that capture the local topological organization of the network. The explicit node features, if present, are then enhanced with th
    
[^108]: RK-core: 一种可用于探索数据集中层次结构的已建立方法

    RK-core: An Established Methodology for Exploring the Hierarchical Structure within Datasets. (arXiv:2310.12168v1 [cs.LG])

    [http://arxiv.org/abs/2310.12168](http://arxiv.org/abs/2310.12168)

    RK-core方法探索数据集中的层次结构，通过分析样本的核心值，发现核心值高的样本在性能上具有更大的贡献。

    

    最近，机器学习领域已经从以模型为中心转向以数据为中心。对各种学习任务的进展是由于更广泛的数据集的积累所推动的，随之而来的是在这些数据集上训练更大模型的可能性。然而，这些数据集仍然相对未被充分探索。为此，我们引入了一种开创性的方法，称为RK-core，以帮助更深入地了解数据集内复杂的层次结构。在多个基准数据集上，我们发现具有较低核心值的样本在其相应类别中的代表性较低，相反，具有较高核心值的样本表现出更大的代表性。相应地，具有较高核心值的样本相对于具有较低核心值的样本对性能贡献更大。基于此，我们进一步利用RK-core分析具有不同coreness值的样本的层次结构。

    Recently, the field of machine learning has undergone a transition from model-centric to data-centric. The advancements in diverse learning tasks have been propelled by the accumulation of more extensive datasets, subsequently facilitating the training of larger models on these datasets. However, these datasets remain relatively under-explored. To this end, we introduce a pioneering approach known as RK-core, to empower gaining a deeper understanding of the intricate hierarchical structure within datasets. Across several benchmark datasets, we find that samples with low coreness values appear less representative of their respective categories, and conversely, those with high coreness values exhibit greater representativeness. Correspondingly, samples with high coreness values make a more substantial contribution to the performance in comparison to those with low coreness values. Building upon this, we further employ RK-core to analyze the hierarchical structure of samples with differen
    
[^109]: AI潜力与认知：人工智能与网络安全中的人机协作的立场文件

    AI Potentiality and Awareness: A Position Paper from the Perspective of Human-AI Teaming in Cybersecurity. (arXiv:2310.12162v1 [cs.CR])

    [http://arxiv.org/abs/2310.12162](http://arxiv.org/abs/2310.12162)

    这篇立场文件通过探讨人工智能在网络安全中的潜力，强调了人工智能与人类专家的协作的重要性。人工智能系统可以通过模式识别和预测建模主动发现漏洞并检测异常情况，而人类专家可以补充和辅助AI的输出结果，提高整体的网络安全防护能力。

    

    本立场文件在网络安全环境下探讨了人工智能的潜力，并特别强调了与意识相关的可能风险因素，这可以通过将人类专家纳入“人工智能-人类”协作中进行管理。随着人工智能技术的发展，它们将为攻击识别、事件响应和恢复提供无与伦比的机会。然而，成功将人工智能部署到网络安全措施中需要深入了解其能力、挑战以及与之相关的风险因素的道德和法律影响，以应对实际应用领域中的问题。为此，我们强调了一种平衡的方法，将人工智能的计算能力与人类专业知识结合起来。人工智能系统可以通过模式识别和预测建模主动发现漏洞并检测异常情况，极大提高识别速度和准确性。人类专家可以解释AI的输出结果，并提供领域专业知识的补充和辅助，从而增强整体的网络安全防护能力。

    This position paper explores the broad landscape of AI potentiality in the context of cybersecurity, with a particular emphasis on its possible risk factors with awareness, which can be managed by incorporating human experts in the loop, i.e., "Human-AI" teaming. As artificial intelligence (AI) technologies advance, they will provide unparalleled opportunities for attack identification, incident response, and recovery. However, the successful deployment of AI into cybersecurity measures necessitates an in-depth understanding of its capabilities, challenges, and ethical and legal implications to handle associated risk factors in real-world application areas. Towards this, we emphasize the importance of a balanced approach that incorporates AI's computational power with human expertise. AI systems may proactively discover vulnerabilities and detect anomalies through pattern recognition, and predictive modeling, significantly enhancing speed and accuracy. Human experts can explain AI-gene
    
[^110]: 基于算子的混沌系统非稳定周期轨道的检测、学习和稳定

    Operator-Based Detecting, Learning, and Stabilizing Unstable Periodic Orbits of Chaotic Attractors. (arXiv:2310.12156v1 [nlin.AO])

    [http://arxiv.org/abs/2310.12156](http://arxiv.org/abs/2310.12156)

    本文提出了基于算子的方法用于分析混沌系统中的非稳定周期轨道。我们使用核积分算子检测非稳定周期轨道，利用Koopman算子识别动态行为，并扩展了这种方法用于稳定奇怪吸引子。以Lorenz吸引子为例，验证了我们方法的有效性。

    

    本文通过运算符理论方法研究了混沌系统中非稳定周期轨道的分析。我们提出了三种数据驱动方法来检测、识别和稳定非稳定周期轨道。我们展示了在延迟坐标中使用核积分算子作为非稳定周期轨道检测的创新方法。为了识别与每个非稳定周期轨道相关的动态行为，我们利用Koopman算子将动态表示为Koopman特征函数空间中的线性方程。这样可以通过研究在不同非稳定周期轨道上的主要动态模式来表征混沌吸引子。我们将这种方法扩展为一个可解释的机器学习框架，旨在稳定奇怪吸引子上的非稳定周期轨道。为了证明我们方法的有效性，我们以Lorenz吸引子作为案例研究对象。

    This paper examines the use of operator-theoretic approaches to the analysis of chaotic systems through the lens of their unstable periodic orbits (UPOs). Our approach involves three data-driven steps for detecting, identifying, and stabilizing UPOs. We demonstrate the use of kernel integral operators within delay coordinates as an innovative method for UPO detection. For identifying the dynamic behavior associated with each individual UPO, we utilize the Koopman operator to present the dynamics as linear equations in the space of Koopman eigenfunctions. This allows for characterizing the chaotic attractor by investigating its principal dynamical modes across varying UPOs. We extend this methodology into an interpretable machine learning framework aimed at stabilizing strange attractors on their UPOs. To illustrate the efficacy of our approach, we apply it to the Lorenz attractor as a case study.
    
[^111]: 用于科学数据的Transformer：天文学家的教学综述

    Transformers for scientific data: a pedagogical review for astronomers. (arXiv:2310.12069v1 [astro-ph.IM])

    [http://arxiv.org/abs/2310.12069](http://arxiv.org/abs/2310.12069)

    本综述旨在向科学家介绍Transformer的应用，包括自然语言处理和自注意机制。此外，还介绍了在天文学中应用于时间序列和成像数据的具体情况，并提供了常见问题解答部分。

    

    与ChatGPT和相关生成型人工智能产品相关的深度学习架构被称为Transformer。最初应用于自然语言处理，Transformer和它们利用的自注意机制在自然科学领域引起了广泛关注。本教学和非正式综述的目标是向科学家介绍Transformer。我们的教学和非正式综述包括自注意机制的数学基础，对原始Transformer架构的描述，以及在天文学中应用于时间序列和成像数据的一节。我们还包括了一个常见问题解答部分，供那些对生成型人工智能感兴趣并希望开始使用Transformer进行研究的读者参考。

    The deep learning architecture associated with ChatGPT and related generative AI products is known as transformers. Initially applied to Natural Language Processing, transformers and the self-attention mechanism they exploit have gained widespread interest across the natural sciences. The goal of this pedagogical and informal review is to introduce transformers to scientists. Our pedagogical and informal review includes the mathematics underlying the attention mechanism, a description of the original transformer architecture, and a section on applications to time series and imaging data in astronomy. We include with a Frequently Asked Questions section for readers who are curious about generative AI and interested in getting started with transformers for their research problem.
    
[^112]: 通过群体不变性学习提高与人类偏好的对齐的泛化能力

    Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v1 [cs.LG])

    [http://arxiv.org/abs/2310.11971](http://arxiv.org/abs/2310.11971)

    该论文提出了一种通过强化学习实现在不同数据组或领域中学习一致策略的方法，该方法可以提高AI助手对不同领域的泛化能力，并更好地与人类偏好对齐。

    

    基于语言模型(LLMs)的AI助手的成功在于强化学习从人类反馈中, 使生成的回答更加与人类偏好一致. 作为通用AI助手, 人们越来越期望它们在不同领域中表现一致. 然而, 先前的工作表明,强化学习(RL)经常利用捷径以获得较高的奖励, 忽略了具有挑战性的样本. 这种对快速奖励收益的关注不仅削弱了训练的稳定性, 也削弱了模型对新的未见数据的泛化能力. 在这项工作中, 我们提出了一种新颖的方法, 可以通过RL在不同数据组或领域中学习一致的策略. 鉴于获得群体标注的挑战, 我们的方法会自动将数据分类到不同的组中, 有意地最大化性能差异. 然后, 我们优化策略以在具有挑战性的组中表现良好. 最后, 利用已建立的

    The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the estab
    
[^113]: 用于学习图神经网络的准瓦狄斯坦损失

    A Quasi-Wasserstein Loss for Learning Graph Neural Networks. (arXiv:2310.11762v1 [cs.LG])

    [http://arxiv.org/abs/2310.11762](http://arxiv.org/abs/2310.11762)

    这篇论文提出了一种新的准瓦狄斯坦损失函数，通过利用图上的最优传输来学习图神经网络，消除了现有损失函数在节点级别预测中可能存在的不一致性。

    

    当在节点级别预测任务中学习图神经网络（GNNs）时，大多数现有的损失函数是独立地应用于每个节点的，即使节点嵌入和它们的标签由于图结构的存在而不是独立同分布的。为了消除这种不一致性，本研究提出了一种新的准瓦狄斯坦（QW）损失函数，借助于在图上定义的最优传输，从而引导GNN的新学习和预测范式。特别地，我们设计了一种“准瓦狄斯坦”距离，用于观测到的多维节点标签和它们的估计之间，通过优化在图边上定义的标签传输。这些估计是由一个GNN参数化的，其中最优标签传输可以选择性地确定图边的权重。通过将标签传输的严格约束重新表达为基于Bregman散度的正则化项，我们得到了所提出的准瓦狄斯坦损失，关联两个高效求解器来学习GNN以及最优标签传输。

    When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transp
    
[^114]: 当刚性成为问题：软一致性正则化用于概率分层时间序列预测

    When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting. (arXiv:2310.11569v1 [cs.LG])

    [http://arxiv.org/abs/2310.11569](http://arxiv.org/abs/2310.11569)

    提出了一个新的概率分层时间序列预测模型，该模型能够有效建模和预测具有层次化关系的多变量时间序列。相较于现有方法，该模型不仅考虑点预测，还能提供经过良好校准的概率预测分布，并且在建模过程中考虑了预测分布的相关性。

    

    概率分层时间序列预测是时间序列预测的重要变体，其目标是对具有层次化关系的多变量时间序列进行建模和预测。大多数方法关注点预测，并未提供经过良好校准的概率预测分布。最近的概率预测方法也在点预测和分布样本中施加层次关系，但未考虑预测分布的相关性。以往的研究也默认数据集总是与给定的层次关系保持一致，并未适应显示出偏离此假设的真实世界数据集。我们填补了这两个空白，并提出了PROFHiT模型，它是一个完全概率的分层预测模型，同时对整个层次的预测分布进行建模。PROFHiT使用灵活的概率贝叶斯方法，并引入了一种新颖的分布一致性正则化方法。

    Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gap and propose PROFHiT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHiT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulariz
    
[^115]: 蛋白质三维图结构学习用于稳健的基于结构的蛋白质性质预测

    Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction. (arXiv:2310.11466v1 [cs.LG])

    [http://arxiv.org/abs/2310.11466](http://arxiv.org/abs/2310.11466)

    本文研究了蛋白质基于结构的性质预测中使用预测结构时性能下降的原因，并将其归因为结构嵌入偏差。

    

    蛋白质基于结构的性质预测已经成为各种生物学任务（如蛋白质功能预测和亚细胞定位估计）的一种有希望的方法。现有方法高度依赖实验蛋白质结构数据，在这些数据不可用的情况下失败。利用人工智能工具（如AlphaFold2）预测的蛋白质结构作为替代方案。然而，我们观察到目前的做法，即在推理过程中仅使用准确预测的结构，会导致预测准确性明显下降。虽然类似现象已经在一般领域（如计算机视觉）中进行了广泛研究作为模型的稳健性，但它们对蛋白质性质预测的影响尚未被探索。在本文中，我们首先从结构表示学习的角度研究了在利用预测的结构时性能下降的原因，将其归因为结构嵌入偏差。为了研究这个问题

    Protein structure-based property prediction has emerged as a promising approach for various biological tasks, such as protein function prediction and sub-cellular location estimation. The existing methods highly rely on experimental protein structure data and fail in scenarios where these data are unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were utilized as alternatives. However, we observed that current practices, which simply employ accurately predicted structures during inference, suffer from notable degradation in prediction accuracy. While similar phenomena have been extensively studied in general fields (e.g., Computer Vision) as model robustness, their impact on protein property prediction remains unexplored. In this paper, we first investigate the reason behind the performance decrease when utilizing predicted structures, attributing it to the structure embedding bias from the perspective of structure representation learning. To study this problem
    
[^116]: HGCVAE: 将生成式学习和对比学习整合为一体的异构图学习方法

    HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])

    [http://arxiv.org/abs/2310.11102](http://arxiv.org/abs/2310.11102)

    HGCVAE是一种将生成式学习和对比学习整合为一体的异构图学习方法，通过利用生成式的自监督学习能力来解决异构图学习的挑战。

    

    生成式自监督学习（SSL）在图学习中展示了巨大的潜力和越来越多的关注。本研究旨在探索生成式SSL在异构图学习（HGL）中的问题。以往关于异构图的SSL方法主要依赖对比学习，需要设计复杂的视图来捕捉异质性。然而，现有的生成式SSL方法并未充分利用生成模型的能力来解决HGL的挑战。在本文中，我们提出了HGCVAE，一种新颖的对比变分图自编码器，使HGL摆脱了复杂异质性的负担。HGCVAE不再专注于复杂的异质性，而是充分利用了生成式SSL的潜力。HGCVAE创新地将对比学习与生成式SSL相结合，引入了几个关键创新。首先，我们采用渐进机制生成高质量的hard样本，

    Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
    
[^117]: 自适应的对向编码用于链路预测

    Adaptive Pairwise Encodings for Link Prediction. (arXiv:2310.11009v1 [cs.LG])

    [http://arxiv.org/abs/2310.11009](http://arxiv.org/abs/2310.11009)

    提出了一种自适应的对向编码方法，用于解决链路预测中现有方法的归纳偏差问题。该方法将消息传递神经网络和启发式方法结合起来，能够更好地分类各种不同因素形成的链路。

    

    链路预测是一种常见的基于图结构数据的任务，在各个领域都有应用。经典方法通常使用手工设计的启发式策略来进行预测。启发式度量被选择为在与链路形成相关的基本因素上与之相关良好。近年来，出现了一类新的方法，将消息传递神经网络（MPNN）的优势与启发式方法结合起来。这些方法通过使用MPNN的输出以及捕捉候选链路中节点之间关系的“对向编码”来进行预测。它们已经在许多数据集上表现出强大的性能。然而，目前的对向编码往往具有强烈的归纳偏差，使用相同的基本因素来分类所有链路。这限制了现有方法学习如何正确分类可能由不同因素形成的各种不同链路的能力。为了解决这个问题，我们提出了一个自适应的对向编码方法。

    Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a 
    
[^118]: ACES: 使用自我目标语言模型和语义描述符生成多样的编程难题

    ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])

    [http://arxiv.org/abs/2310.10692](http://arxiv.org/abs/2310.10692)

    ACES是一种使用自我目标语言模型和语义描述符生成多样化的编程难题的方法，能够优化有趣的多样性和少样本生成。

    

    寻找和选择新颖有趣的问题是好奇心、科学和创新的核心。在Python编程难题的无限空间中，我们研究了自动问题生成。现有的生成模型通常旨在建模参考分布，没有明确的多样性优化。其他方法在有限的手工编码表示空间或不可解释的学习嵌入空间中明确优化多样性，这些嵌入空间可能与人类对有趣变化的感知不符。通过ACES（自我目标代码探索与语义描述符），我们引入了一种新的自我目标生成方法，利用大型语言模型（LLM）生成语义描述符，直接优化有趣的多样性，以及少样本生成。每个难题都标记有10个维度，每个维度捕捉了解决它所需的编程技能。ACES生成并追求新颖可行的目标。

    Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
    
[^119]: 超越文档边界的上下文预训练：语言模型

    In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10638](http://arxiv.org/abs/2310.10638)

    本论文提出了一种超越文档边界的上下文预训练方法，通过在相关文档序列上训练语言模型，鼓励模型进行跨文档的阅读和推理。该方法通过改变文档顺序并应用现有的预训练管道来实现。

    

    目前，大型语言模型（LMs）通过预测给定文档前缀的标记来进行训练，从而能够直接进行长篇生成和提示式任务，这可以简化为文档完成。现有的预训练管道通过连接随机组合的短文档来训练LMs，以创建输入上下文，但前一个文档对于预测下一个文档没有提供任何信号。我们提出了一种新方法——上下文预训练，即在相关文档序列上预先训练语言模型，从而明确鼓励它们跨越文档边界进行阅读和推理。我们可以通过改变文档顺序，使每个上下文包含相关的文档，并直接应用现有的预训练管道来进行上下文预训练。然而，这个文档排序问题很具有挑战性。有数十亿个文档，我们希望在每个文档中最大化上下文相似性而不重复任何数据。

    Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
    
[^120]: 通过与平滑高质量专家轨迹对齐实现高效数据集精炼

    Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories. (arXiv:2310.10541v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2310.10541](http://arxiv.org/abs/2310.10541)

    本论文提出了一种高效的数据集精炼方法，通过与平滑高质量的专家轨迹对齐，实现对大规模数据集的替代，并提出了剪辑损失和梯度惩罚的集成来调节学生和专家之间的互动。

    

    训练一大型的先进机器学习模型通常需要使用大规模数据集，这使得训练和参数调整过程变得昂贵且耗时。一些研究人员选择将真实世界数据集中的信息精炼为小型合成数据集，同时保持其训练性能，从而提出了一种称为数据集精炼（DD）的数据高效方法。尽管该领域近年来取得了进展，但现有方法仍然表现不佳，不能有效替代大规模数据集。在本文中，与仅关注改进学生成绩的先前方法不同，我们首次认识到专家和学生之间的重要相互作用。我们认为在后续数据集精炼中，采用更强大的专家轨迹时，专家的平滑性具有重要影响。基于此，我们引入了剪辑损失和梯度惩罚的集成，以调节学生和专家之间的互动。

    Training a large and state-of-the-art machine learning model typically necessitates the use of large-scale datasets, which, in turn, makes the training and parameter-tuning process expensive and time-consuming. Some researchers opt to distil information from real-world datasets into tiny and compact synthetic datasets while maintaining their ability to train a well-performing model, hence proposing a data-efficient method known as Dataset Distillation (DD). Despite recent progress in this field, existing methods still underperform and cannot effectively replace large datasets. In this paper, unlike previous methods that focus solely on improving the efficacy of student distillation, we are the first to recognize the important interplay between expert and student. We argue the significant impact of expert smoothness when employing more potent expert trajectories in subsequent dataset distillation. Based on this, we introduce the integration of clipping loss and gradient penalty to regul
    
[^121]: 深度学习的微扩展数据格式

    Microscaling Data Formats for Deep Learning. (arXiv:2310.10537v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10537](http://arxiv.org/abs/2310.10537)

    本文评估了Microscaling（MX）数据格式在降低深度学习应用的计算和存储成本方面的可行性。实证结果显示MX数据格式可以作为基线FP32的替代，同时保持低用户摩擦，并且成功在超过两打基准测试中以小于8位的数据格式进行了训练。

    

    窄位宽数据格式对于降低现代深度学习应用的计算和存储成本至关重要。本文评估了将每个块的缩放因子与窄浮点和整数类型相结合的微扩展（MX）数据格式，以满足硬件效率、模型准确性和用户摩擦之间的竞争需求。对于AI推理和训练，MX数据格式在超过两打基准测试中的实证结果证明了其作为基线FP32的可行性，并且使用时用户摩擦小。我们还展示了在最小的准确性损失和无需修改训练配方的情况下，首次训练生成式语言模型在小于8位的权重、激活和渐变上。

    Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements. MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.
    
[^122]: 时间序列分类的数据增强：一项广泛的实证研究和综述

    Data Augmentation for Time-Series Classification: An Extensive Empirical Study and Comprehensive Survey. (arXiv:2310.10060v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10060](http://arxiv.org/abs/2310.10060)

    本研究对时间序列分类中的数据增强方法进行了广泛研究和综述，总结了60多种独特的方法，并提出了一个针对TSC的新的分类体系。

    

    数据增强（DA）已成为时间序列分类（TSC）中不可或缺的策略，主要因为它可以增加训练样本的数量，从而提高模型的健壮性，使数据集多样化，并减少过拟合。然而，目前TSC中的DA研究存在着文献评审的片段化，方法学分类不清晰，评估指标不足以及缺乏用户友好的工具等问题。鉴于这些挑战，本研究对TSC领域中的DA方法进行了详尽的研究。我们首先进行了持续十年的广泛文献回顾，发现当代综述文章很少能够涵盖DA在TSC上的全部进展，因此我们仔细分析了100多篇学术文章，总结出了60多种独特的DA技术。这项严格的分析形成了一种新颖的分类体系，专门针对TSC中的DA细节进行分类。

    Data Augmentation (DA) has emerged as an indispensable strategy in Time Series Classification (TSC), primarily due to its capacity to amplify training samples, thereby bolstering model robustness, diversifying datasets, and curtailing overfitting. However, the current landscape of DA in TSC is plagued with fragmented literature reviews, nebulous methodological taxonomies, inadequate evaluative measures, and a dearth of accessible, user-oriented tools. In light of these challenges, this study embarks on an exhaustive dissection of DA methodologies within the TSC realm. Our initial approach involved an extensive literature review spanning a decade, revealing that contemporary surveys scarcely capture the breadth of advancements in DA for TSC, prompting us to meticulously analyze over 100 scholarly articles to distill more than 60 unique DA techniques. This rigorous analysis precipitated the formulation of a novel taxonomy, purpose-built for the intricacies of DA in TSC, categorizing tech
    
[^123]: 高质量目标检测的Rank-DETR方法

    Rank-DETR for High Quality Object Detection. (arXiv:2310.08854v1 [cs.CV])

    [http://arxiv.org/abs/2310.08854](http://arxiv.org/abs/2310.08854)

    Rank-DETR是一种高质量目标检测方法，通过引入排名导向的设计，包括架构设计和损失函数设计，实现了性能卓越的基于DETR的目标检测器。

    

    现代检测变换器（DETR）使用一组对象查询来预测边界框列表，通过将其分类置信度得分进行排序，并选择排名靠前的预测结果作为给定输入图像的最终检测结果。性能卓越的目标检测器需要对边界框预测进行准确的排序。对于基于DETR的检测器，排名靠前的边界框由于分类得分与定位准确性之间的不对齐而导致定位质量较差，从而阻碍了高质量检测器的构建。在这项工作中，我们通过提出一系列面向排名的设计，共同称为Rank-DETR，引入了一个简单且性能卓越的基于DETR的目标检测器。我们的主要贡献包括：（i）一个面向排名的架构设计，可以促进正面预测并抑制负面预测，以确保更低的假阳性率，以及（ii）一个面向排名的损失函数和匹配成本设计。

    Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design 
    
[^124]: 在协变量漂移下基于核方法的统一分析

    Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift. (arXiv:2310.08237v1 [stat.ML])

    [http://arxiv.org/abs/2310.08237](http://arxiv.org/abs/2310.08237)

    该论文提出了一个在协变量漂移下对一般非参数方法进行统一分析的方法，通过建立收敛速度为一般损失函数提供了统一的理论分析。

    

    在实际应用中，协变量漂移是普遍存在的，即源数据和目标数据的输入分布存在显著差异。尽管在各种学习问题中具有实际重要性，但现有的大多数方法只关注于一些特定的学习任务，并没有在理论上和数值上得到很好的验证。为了解决这个问题，我们提出了一个在协变量漂移下对一般非参数方法进行统一分析的方法。我们的理论结果适用于属于一个丰富的损失函数家族的一般损失，其中包括许多常用的方法，如均值回归、分位数回归、基于似然的分类和基于边缘的分类。本文重点研究了两类协变量漂移问题，并为一般损失函数建立了尖锐的收敛速度以提供一个统一的理论分析，该结果与文献中的最优结果一致。

    Covariate shift occurs prevalently in practice, where the input distributions of the source and target data are substantially different. Despite its practical importance in various learning problems, most of the existing methods only focus on some specific learning tasks and are not well validated theoretically and numerically. To tackle this problem, we propose a unified analysis of general nonparametric methods in a reproducing kernel Hilbert space (RKHS) under covariate shift. Our theoretical results are established for a general loss belonging to a rich loss function family, which includes many commonly used methods as special cases, such as mean regression, quantile regression, likelihood-based classification, and margin-based classification. Two types of covariate shift problems are the focus of this paper and the sharp convergence rates are established for a general loss function to provide a unified theoretical analysis, which concurs with the optimal results in literature wher
    
[^125]: 成本驱动的机器学习流水线硬件软件协同优化

    Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines. (arXiv:2310.07940v1 [cs.LG])

    [http://arxiv.org/abs/2310.07940](http://arxiv.org/abs/2310.07940)

    该论文探索了在物联网设备中使用深度神经网络时的硬件软件协同优化，重点考虑了成本、延迟和用户体验，并研究了量化、模型缩放和多模态等方法与系统组件的相互作用。

    

    研究人员长期以来一直宣扬着由物联网设备（包括智能传感器，家居和城市）推动的未来愿景。越来越多地，将智能嵌入这些设备中涉及到深度神经网络的使用。然而，它们的存储和处理需求使它们对于廉价的现成平台来说是禁止的。克服这些要求对于实现广泛适用的智能设备至关重要。虽然已经开发出了许多使模型变得更小更高效的方法，但对于特定场景最适合的方法缺乏理解。更重要的是对于边缘平台，这些选择不能与成本和用户体验相割离地进行分析。在这项工作中，我们从成本、延迟和用户体验的角度全面探索了量化、模型缩放和多模态与存储、传感器和处理器等系统组件的相互作用。我们从硬件/软件协同设计的角度进行，考虑成本、延迟和用户体验的因素。

    Researchers have long touted a vision of the future enabled by a proliferation of internet-of-things devices, including smart sensors, homes, and cities. Increasingly, embedding intelligence in such devices involves the use of deep neural networks. However, their storage and processing requirements make them prohibitive for cheap, off-the-shelf platforms. Overcoming those requirements is necessary for enabling widely-applicable smart devices. While many ways of making models smaller and more efficient have been developed, there is a lack of understanding of which ones are best suited for particular scenarios. More importantly for edge platforms, those choices cannot be analyzed in isolation from cost and user experience. In this work, we holistically explore how quantization, model scaling, and multi-modality interact with system components such as memory, sensors, and processors. We perform this hardware/software co-design from the cost, latency, and user-experience perspective, and d
    
[^126]: Fed-GraB：具有自适应梯度平衡器的联邦式长尾学习

    Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer. (arXiv:2310.07587v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.07587](http://arxiv.org/abs/2310.07587)

    本文提出了一种名为Fed-GraB的方法，该方法通过自适应梯度平衡器来解决联邦式长尾学习的问题。该方法能够在隐私约束下刻画全局长尾分布，并通过调整本地学习策略来解决头部-尾部不平衡的问题。

    

    数据隐私和长尾分布在许多现实任务中是常态而非例外。本文研究了一种联邦式长尾学习（Fed-LT）任务，在该任务中，每个客户端持有一个本地异构数据集；如果可以全局聚合数据集，则它们共同展现出长尾分布。在这样的设置下，现有的联邦优化和/或集中式长尾学习方法很难应用，因为存在以下挑战：（a）在隐私约束下刻画全局长尾分布，以及（b）调整本地学习策略以应对头部-尾部不平衡。为此，我们提出了一种方法称为$\texttt{Fed-GraB}$，它包括一个自适应梯度平衡器（SGB）模块，该模块以闭环方式根据全局长尾分布的反馈对客户端的梯度进行重新加权，评估方法为直接先验分析器（DPA）模块。使用$\texttt{Fed-GraB}$，客户端可以有效缓解数据分布的不均衡问题。

    Data privacy and long-tailed distribution are the norms rather than the exception in many real-world tasks. This paper investigates a federated long-tailed learning (Fed-LT) task in which each client holds a locally heterogeneous dataset; if the datasets can be globally aggregated, they jointly exhibit a long-tailed distribution. Under such a setting, existing federated optimization and/or centralized long-tailed learning methods hardly apply due to challenges in (a) characterizing the global long-tailed distribution under privacy constraints and (b) adjusting the local learning strategy to cope with the head-tail imbalance. In response, we propose a method termed $\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB) module that re-weights clients' gradients in a closed-loop manner, based on the feedback of global long-tailed distribution evaluated by a Direct Prior Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively alleviate the distribution
    
[^127]: ROMO: 检索增强的离线模型优化

    ROMO: Retrieval-enhanced Offline Model-based Optimization. (arXiv:2310.07560v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.07560](http://arxiv.org/abs/2310.07560)

    ROMO是一种检索增强的离线模型优化方法，通过在离线数据集中优化平庸设计，并保持给定的约束来解决约束MBO问题。

    

    数据驱动的黑盒模型优化（MBO）问题在许多实际应用场景中出现，其目标是基于静态离线数据集，寻找使黑盒目标函数最大化的整个空间上的设计。在这项工作中，我们考虑了一种更一般但具有挑战性的MBO设置，称为约束MBO（CoMBO），其中只有部分设计空间可以优化，而其余部分受环境约束。CoMBO带来的新挑战是大多数满足约束条件的观察设计在评估中是平庸的。因此，我们的重点是在离线数据集中优化这些平庸的设计，同时保持给定的约束，而不是在传统的MBO设置中进一步增强最佳观察设计。我们提出了检索增强的离线模型优化（ROMO），一种新的可导性前向方法，它检索离线数据集并聚合相关样本以提供可靠的预测，然后将其应用于CoMBO。

    Data-driven black-box model-based optimization (MBO) problems arise in a great number of practical application scenarios, where the goal is to find a design over the whole space maximizing a black-box target function based on a static offline dataset. In this work, we consider a more general but challenging MBO setting, named constrained MBO (CoMBO), where only part of the design space can be optimized while the rest is constrained by the environment. A new challenge arising from CoMBO is that most observed designs that satisfy the constraints are mediocre in evaluation. Therefore, we focus on optimizing these mediocre designs in the offline dataset while maintaining the given constraints rather than further boosting the best observed design in the traditional MBO setting. We propose retrieval-enhanced offline model-based optimization (ROMO), a new derivable forward approach that retrieves the offline dataset and aggregates relevant samples to provide a trusted prediction, and use it f
    
[^128]: KwaiYiiMath: 技术报告

    KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])

    [http://arxiv.org/abs/2310.07488](http://arxiv.org/abs/2310.07488)

    KwaiYiiMath是一个用于增强数学推理能力的大型语言模型，通过应用监督微调和人类反馈强化学习，在英语和中文数学任务上取得了最先进的性能，并且能够正确解决生成的问题过程。

    

    近年来，大型语言模型（LLMs）在处理各种自然语言处理（NLP）下游任务方面展示出了显著的能力，甚至可以处理需要多步推理的数学任务。在本报告中，我们介绍了KwaiYiiMath，通过应用监督微调（SFT）和人类反馈强化学习（RLHF），增强了KwaiYiiBase1的数学推理能力，包括英语和中文的数学任务。同时，我们还构建了一个小规模的中小学数学测试集（命名为KMath），包含188个例子，用来评估模型生成的问题解决过程的正确性。实证研究表明，与类似规模的模型相比，KwaiYiiMath在GSM8k、CMath和KMath上均能取得最先进的性能（SOTA）。

    Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
    
[^129]: 循环神经语言模型作为概率有限状态自动机

    Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05161](http://arxiv.org/abs/2310.05161)

    本文研究了循环神经网络语言模型（RNN LMs）作为概率有限状态自动机的能力，并发现它们只能表示有限状态模型所能表达的概率分布的一个严格子集。

    

    通过以容易理解的形式来研究语言模型（LMs）可以使我们精确地描述它们的能力和局限性。先前的研究已经考察了循环神经网络（RNN）语言模型在识别无权重形式语言的能力。然而，LMs并不描述无权重形式语言，而是定义了对字符串的概率分布。在本研究中，我们研究了RNN LMs可以表示哪些类的概率分布，这使得我们可以更直接地陈述它们的能力。我们证明了简单的RNN等价于概率有限状态自动机的一个子类，因此只能模拟有限状态模型所能表达的概率分布的一个严格子集。此外，我们研究了用RNNs表示有限状态LMs的空间复杂度。我们证明了，为了表示一个任意确定的有限状态LMs，其中有$N$个状态且字符集为$\Sigma$的RNN requir

    Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the representational capacity of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages -- rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requir
    
[^130]: 联邦学习中的数字伦理问题

    Digital Ethics in Federated Learning. (arXiv:2310.03178v1 [cs.LG])

    [http://arxiv.org/abs/2310.03178](http://arxiv.org/abs/2310.03178)

    本文探讨了在联邦学习中作为客户端的以人为中心的设备引发的数字伦理问题。瞭解了其中涉及的游戏动态、公平性、奖励机制和连贯性等挑战，并探讨了解决方案和以人为中心的物联网在联邦学习中的机遇。

    

    物联网（IoT）不断产生大量数据，引发了对数据隐私保护和数据滥用限制的越来越多的关注。联邦学习（FL）通过共享机器学习（ML）模型参数而不是原始用户数据，促进了多方之间的协作能力，并因其在隐私保护和学习效率提升方面的潜力而引起了广泛关注。本文重点讨论了在FL中作为客户端的以人为中心的设备引发的数字伦理问题。具体而言，在客户端和服务器之间存在不同的观点和目标的情况下，FL面临着游戏动态、公平性、奖励机制和连贯性等挑战。我们从客户端和服务器的角度，以及集中式和分散式FL的视角分析了这些挑战及其解决方案。最后，我们探讨了以人为中心的物联网在FL中的机遇。

    The Internet of Things (IoT) consistently generates vast amounts of data, sparking increasing concern over the protection of data privacy and the limitation of data misuse. Federated learning (FL) facilitates collaborative capabilities among multiple parties by sharing machine learning (ML) model parameters instead of raw user data, and it has recently gained significant attention for its potential in privacy preservation and learning efficiency enhancement. In this paper, we highlight the digital ethics concerns that arise when human-centric devices serve as clients in FL. More specifically, challenges of game dynamics, fairness, incentive, and continuity arise in FL due to differences in perspectives and objectives between clients and the server. We analyze these challenges and their solutions from the perspectives of both the client and the server, and through the viewpoints of centralized and decentralized FL. Finally, we explore the opportunities in FL for human-centric IoT as dir
    
[^131]: zkFL: 基于零知识证明的联邦学习梯度聚合

    zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])

    [http://arxiv.org/abs/2310.02554](http://arxiv.org/abs/2310.02554)

    zkFL是一种基于零知识证明的联邦学习梯度聚合方法，通过提供每轮的证明来解决协调者恶意行为的问题。

    

    联邦学习是一种机器学习范式，使多个分散的客户端在中央协调者的组织下共同训练一个模型。传统的联邦学习解决方案依赖于对中央协调者的信任，它以公平诚实的方式形成客户端的群体。然而，在现实中，恶意的协调者可能会放弃并替换客户端的训练模型，或者发动虚假客户端的肆意攻击。这种恶意行为让协调者在联邦学习环境中拥有更多控制客户端和决定最终训练结果的权力。本文介绍了zkFL，它利用零知识证明(ZKPs)来解决训练模型聚合过程中的恶意协调者问题。为了保证正确的聚合结果，协调者需要每轮提供一个证明。这个证明可以向客户端证明协调者忠实执行预期行为。为了进一步保护客户端隐私和数据安全，我们还引入了差分隐私机制，并对zkFL进行了实验评估。

    Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
    
[^132]: SNIP: 用统一的预训练框架连接数学符号和数值领域

    SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training. (arXiv:2310.02227v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.02227](http://arxiv.org/abs/2310.02227)

    SNIP引入了一种统一的预训练框架，通过联合对比学习加强了符号和数值领域之间的相似性，并提供了跨领域的表示洞察力。

    

    在一个无法缺少符号数学方程来建模复杂自然现象的时代，科学探究往往涉及到收集观察数据并将其转化为数学表达式。最近，深度学习已经成为从数据中提取洞察力的强大工具。然而，现有模型通常特化于数值领域或符号领域，并且通常在为特定任务量身定制的监督式训练中进行训练。这种方法忽视了符号方程和其数值对应物之间可能产生的重大好处。为了弥合这种差距，我们引入了SNIP，一种符号-数值集成预训练的方法，它通过在符号和数值领域之间进行联合对比学习，增强了它们在预训练嵌入中的相互相似性。通过进行潜空间分析，我们观察到SNIP提供了跨领域的表示洞察力，揭示了符号和数值之间的关联关系。

    In an era where symbolic mathematical equations are indispensable for modeling complex natural phenomena, scientific inquiry often involves collecting observations and translating them into mathematical expressions. Recently, deep learning has emerged as a powerful tool for extracting insights from data. However, existing models typically specialize in either numeric or symbolic domains, and are usually trained in a supervised manner tailored to specific tasks. This approach neglects the substantial benefits that could arise from a task-agnostic unified understanding between symbolic equations and their numeric counterparts. To bridge the gap, we introduce SNIP, a Symbolic-Numeric Integrated Pre-training, which employs joint contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the pre-trained embeddings. By performing latent space analysis, we observe that SNIP provides cross-domain insights into the representations, revealing that symbolic 
    
[^133]: OceanGPT：用于海洋科学任务的大型语言模型

    OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])

    [http://arxiv.org/abs/2310.02031](http://arxiv.org/abs/2310.02031)

    OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。

    

    海洋科学是探索充满生命和生物多样性的海洋的科学，考虑到海洋覆盖了地球表面的70％以上，这一领域具有重要意义。最近，大型语言模型（LLM）的进展改变了科学的范式。尽管在其他领域取得了成功，但现有的LLM通常无法满足海洋学家等领域专家的需求，同时对LLM在海洋科学中的潜力尚未得到充分探索。这其中的根本原因可能是海洋数据的庞大而复杂的性质，以及对更高的粒度和丰富的知识的需求。为了解决这些问题，我们推出了首个海洋领域的LLM——OceanGPT，该模型擅长各种海洋科学任务。我们提出了一个新颖的框架DoInstruct，用于自动获取大量的海洋领域指导数据，它基于多智能体的协作生成指导。

    Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
    
[^134]: 一种适用于现代网络的路径范数工具包：影响、前景和挑战

    A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v1 [stat.ML])

    [http://arxiv.org/abs/2310.01225](http://arxiv.org/abs/2310.01225)

    本文介绍了适用于现代神经网络的路径范数工具包，可以包括具有偏差、跳跃连接和最大池化的通用DAG ReLU网络。这个工具包恢复或超越了已知的路径范数界限，并挑战了基于路径范数的一些具体承诺。

    

    本文介绍了第一个完全能够包括具有偏差、跳跃连接和最大池化的通用DAG ReLU网络的路径范数工具包。这个工具包不仅适用于最广泛的基于路径范数的现代神经网络，还可以恢复或超越已知的此类范数的最尖锐界限。这些扩展的路径范数还享有路径范数的常规优点：计算简便、对网络的对称性具有不变性，在前馈网络上比操作符范数的乘积（另一种常用的复杂度度量）具有更好的锐度。工具包的多功能性和易于实施使我们能够通过数值评估在ImageNet上对ResNet的最尖锐界限来挑战基于路径范数的具体承诺。

    This work introduces the first toolkit around path-norms that is fully able to encompass general DAG ReLU networks with biases, skip connections and max pooling. This toolkit notably allows us to establish generalization bounds for real modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used.  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.
    
[^135]: 角度优化的文本嵌入

    AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])

    [http://arxiv.org/abs/2309.12871](http://arxiv.org/abs/2309.12871)

    本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。

    

    高质量的文本嵌入对于提升语义文本相似度（STS）任务至关重要，而这些任务又是大型语言模型（LLM）应用中的关键组成部分。然而，现有的文本嵌入模型面临的一个普遍挑战是渐变消失问题，主要是由于它们在优化目标中依赖余弦函数，而余弦函数具有饱和区域。为了解决这个问题，本文提出了一种称为AnglE的新型角度优化文本嵌入模型。AnglE的核心思想是在一个复杂空间中引入角度优化。这种新颖的方法有效地缓解了余弦函数饱和区域产生的不利影响，从而可以阻碍梯度并阻碍优化过程。为了建立全面的STS评估，我们在现有的短文本STS数据集和从GitHub Issues中新收集的长文本STS数据集上进行了实验。此外，我们还研究了具有有限标签数据的特定领域STS场景，并探讨了AnglE的工作原理。

    High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
    
[^136]: 核密度积分转换

    The Kernel Density Integral Transformation. (arXiv:2309.10194v1 [stat.ML])

    [http://arxiv.org/abs/2309.10194](http://arxiv.org/abs/2309.10194)

    本文提出了一种使用核密度积分转换作为特征预处理步骤的方法，可以替代线性最小最大缩放和分位数转换，并通过调整超参数优化性能。这种方法在统计数据分析中具有应用价值。

    

    在应用机器学习和统计方法于表格数据时，特征预处理继续发挥关键作用。在本文中，我们提出了使用核密度积分转换作为特征预处理步骤的方法。我们的方法综合了两种主要的特征预处理方法作为极限情况：线性最小最大缩放和分位数转换。我们证明了，在不调整超参数的情况下，核密度积分转换可以作为这两种方法的简单替代方法，对每种方法的弱点具有鲁棒性。另外，通过调整一个连续超参数，我们经常优于这两种方法。最后，我们表明核密度转换可以有益地应用于统计数据分析，特别是在相关性分析和单变量聚类上。

    Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.
    
[^137]: 生成建模，设计和分析蜘蛛丝蛋白序列以提高机械性能

    Generative modeling, design and analysis of spider silk protein sequences for enhanced mechanical properties. (arXiv:2309.10170v1 [cond-mat.mtrl-sci] CROSS LISTED)

    [http://arxiv.org/abs/2309.10170](http://arxiv.org/abs/2309.10170)

    提出了一种生成式语言模型，用于设计具有复杂目标机械性能组合的新型蜘蛛丝蛋白序列。通过BLAST搜索、性能评估、分子结构比较和序列基序分析等方式对模型性能进行了评估。

    

    蜘蛛丝是一种拥有出色的机械性能（如强度，延展性和轻量化）的材料。然而，到目前为止，仅有有限的模型可用于完全探索序列-性能关系以进行分析和设计。在这里，我们提出了一种定制的生成式语言模型，用于设计新的蜘蛛丝蛋白序列以满足复杂的目标机械性能组合。该模型在大量蛋白序列的预训练基础上进行微调，针对约1,000个主要泡腺丝蛋白（MaSp）序列进行了正向和逆向生成策略的端到端设计。通过以下评估性能：（1）通过BLAST搜索对生成的蜘蛛丝蛋白序列进行新颖性分析和蛋白类型分类，（2）对比类似序列的性能评估和比较，（3）分子结构比较，以及（4）详细的序列基序分析。

    Spider silks are remarkable materials characterized by superb mechanical properties such as strength, extensibility and lightweightedness. Yet, to date, limited models are available to fully explore sequence-property relationships for analysis and design. Here we propose a custom generative large-language model to enable design of novel spider silk protein sequences to meet complex combinations of target mechanical properties. The model, pretrained on a large set of protein sequences, is fine-tuned on ~1,000 major ampullate spidroin (MaSp) sequences for which associated fiber-level mechanical properties exist, to yield an end-to-end forward and inverse generative strategy. Performance is assessed through: (1), a novelty analysis and protein type classification for generated spidroin sequences through BLAST searches, (2) property evaluation and comparison with similar sequences, (3) comparison of molecular structures, as well as, and (4) a detailed sequence motif analyses. We generate s
    
[^138]: CONFLATOR:将基于切换点的旋转位置编码纳入混合语言建模中

    CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling. (arXiv:2309.05270v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05270](http://arxiv.org/abs/2309.05270)

    本论文提出了CONFLATOR：一种针对代码混合语言的神经语言建模方法，通过引入旋转位置编码和切换点信息，在混合语言建模中取得最佳结果。

    

    两种或多种语言的混合称为代码混合（CM）。 CM是多语言社会的社会规范。神经语言模型（NLMs）（如变压器）在许多自然语言处理任务上非常有效。然而，对于CM的NLM是一个未被充分探索的领域。尽管变压器具有能力，但由于它们是非递归的，它们不能始终编码位置信息。因此，为了丰富词的信息并纳入位置信息，定义了位置编码。我们假设转换点（SPs），即语言切换的文本中的交汇点（L1-> L2或L2-> L1），对CM语言模型（LMs）构成挑战，并对建模过程中SPs给予特别重视。我们尝试了几种位置编码机制，并表明旋转位置编码以及切换点信息可以获得最佳结果。我们引入CONFLATOR：一种针对混合语言的神经语言建模方法。

    The mixing of two or more languages is called Code-Mixing (CM). CM is a social norm in multilingual societies. Neural Language Models (NLMs) like transformers have been effective on many NLP tasks. However, NLM for CM is an under-explored area. Though transformers are capable and powerful, they cannot always encode positional information since they are non-recurrent. Therefore, to enrich word information and incorporate positional information, positional encoding is defined. We hypothesize that Switching Points (SPs), i.e., junctions in the text where the language switches (L1 -> L2 or L2 -> L1), pose a challenge for CM Language Models (LMs), and hence give special emphasis to SPs in the modeling process. We experiment with several positional encoding mechanisms and show that rotatory positional encodings along with switching point information yield the best results.  We introduce CONFLATOR: a neural language modeling approach for code-mixed languages. CONFLATOR tries to learn to empha
    
[^139]: 一种基于机器学习辅助的OTFS与OFDM适应性调制解调器

    An ML-assisted OTFS vs. OFDM adaptable modem. (arXiv:2309.01319v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2309.01319](http://arxiv.org/abs/2309.01319)

    本文提出了一种基于机器学习辅助的OTFS与OFDM适应性调制解调器，通过观察信道状态、接收信噪比和调制格式，在发送端和接收端之间切换OTFS或OFDM信号处理链以获得最优均方误差（MSE）性能。

    

    正交时频空（OTFS）信号具有强大的抗双重展宽信道的能力，适用于高移动情景。同时，正交频分复用（OFDM）波形可重复使用传统结构，接收机设计简单，并具有低复杂度检测的优势。许多比较OTFS和OFDM性能的研究因高移动条件下涉及大量系统参数而产生了混合结果。本文通过模拟验证了这一观察结果，并提出了一种基于深度神经网络（DNN）的自适应方案，以在发送端和接收端之间切换OTFS或OFDM信号处理链以获得最优均方误差（MSE）性能。DNN分类器通过观察信道状态、接收信噪比和调制格式来进行方案切换的训练。我们比较了OTFS、OFDM和所提出的方案的性能。

    The Orthogonal-Time-Frequency-Space (OTFS) signaling is known to be resilient to doubly-dispersive channels, which impacts high mobility scenarios. On the other hand, the Orthogonal-Frequency-Division-Multiplexing (OFDM) waveforms enjoy the benefits of the reuse of legacy architectures, simplicity of receiver design, and low-complexity detection. Several studies that compare the performance of OFDM and OTFS have indicated mixed outcomes due to the plethora of system parameters at play beyond high-mobility conditions. In this work, we exemplify this observation using simulations and propose a deep neural network (DNN)-based adaptation scheme to switch between using either an OTFS or OFDM signal processing chain at the transmitter and receiver for optimal mean-squared-error (MSE) performance. The DNN classifier is trained to switch between the two schemes by observing the channel condition, received SNR, and modulation format. We compare the performance of the OTFS, OFDM, and the propose
    
[^140]: Copiloting the Copilots: 将大型语言模型与完成引擎融合用于自动化程序修复

    Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair. (arXiv:2309.00608v1 [cs.SE])

    [http://arxiv.org/abs/2309.00608](http://arxiv.org/abs/2309.00608)

    这篇论文提出了一种框架，利用完成引擎来进一步支持大型语言模型在自动化程序修复中合成更多有效的修补程序。

    

    在自动化程序修复中，对于通用编程语言中的实际系统合成正确的修补程序可能具有挑战性。最近的大型语言模型（LLMs）已被证明对开发人员在各种编码任务中具有帮助，并且已直接应用于修补程序的合成。然而，大多数LLMs将程序视为令牌序列，这意味着它们对目标编程语言的底层语义约束一无所知。这导致生成了大量静态无效的修补程序，阻碍了该技术的实用性。因此，我们提出了Repilot，一种在修复过程中通过合成更多有效修补程序从而进一步支持AI“副驾驶员”（即LLMs）的框架。我们的关键见解是，许多LLMs以自回归方式生成输出（即逐个令牌生成），类似于人类编写程序，这可以通过完成引擎显著提升和引导。Repilot协同合成了修补程序。

    During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful "copilots" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthe
    
[^141]: 基于主题的贝叶斯惊喜和意外性用于推荐系统

    Topic-Level Bayesian Surprise and Serendipity for Recommender Systems. (arXiv:2308.06368v1 [cs.IR])

    [http://arxiv.org/abs/2308.06368](http://arxiv.org/abs/2308.06368)

    本文通过引入基于主题的贝叶斯惊喜概念，提出了一种用于推荐系统的意外性模型，以解决过滤泡问题，通过识别相似用户和测量用户对物品的意外性来推荐具有高潜力的意外性物品。

    

    推荐系统优化其推荐仅适合用户对已消费物品的评级历史，这可能导致过滤泡，用户无法从新颖、未见过的类别中体验物品。我们提出了一种基于内容的意外性形式，以贝叶斯惊喜为基础，用于测量用户消费并评级后物品的意外性。结合识别相似用户的协同过滤组件，可以推荐具有高潜力意外性的物品。为了便于评估主题级别的惊喜和意外性模型，我们介绍了一个从Goodreads中提取的图书阅读历史数据集，包含超过26千个用户和近130万本书，并对其中的449篇书进行了手动注释。

    A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 
    
[^142]: Amazon-M2: 一个用于推荐和文本生成的多语言多区域购物会话数据集

    Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation. (arXiv:2307.09688v1 [cs.IR])

    [http://arxiv.org/abs/2307.09688](http://arxiv.org/abs/2307.09688)

    Amazon-M2是一个多语言多区域购物会话数据集，可以增强个性化推荐和理解用户偏好能力。

    

    对于电子商务来说，建模客户购物意图是一个重要的任务，因为它直接影响用户体验和参与度。因此，准确理解客户的偏好对于提供个性化推荐至关重要。基于会话的推荐技术利用客户会话数据来预测他们的下一次互动，已经越来越受到欢迎。然而，现有的会话数据集在项目属性、用户多样性和数据集规模方面存在局限性。因此，它们不能全面地捕捉用户行为和偏好的谱系。为了弥补这一差距，我们提出了Amazon Multilingual Multi-locale Shopping Session Dataset，即Amazon-M2。它是第一个由来自六个不同区域的数百万用户会话组成的多语言数据集，其中产品的主要语言是英语、德语、日语、法语、意大利语和西班牙语。值得注意的是，这个数据集可以帮助我们增强个性化和理解用户偏好能力。

    Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, w
    
[^143]: 学习层次交互式多目标搜索以进行移动操作

    Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])

    [http://arxiv.org/abs/2307.06125](http://arxiv.org/abs/2307.06125)

    这项工作提出了一个层次化的强化学习方法，用于解决在未知环境中需要同时进行操控和导航的交互式多目标搜索任务。实验证明，该方法可以在新环境中进行零样本迁移，并对未见过的子任务具有鲁棒性。

    

    现有的目标搜索方法使得机器人可以在自由路径上进行搜索，然而，在非结构化的以人为中心的环境中操作的机器人经常需要操控环境以满足他们的需求。在这项工作中，我们引入了一种新的交互式多目标搜索任务，机器人需要打开门以浏览房间，并在橱柜和抽屉内搜索目标物品。这些新挑战需要在未知环境中结合操控和导航技能。我们提出了HIMOS，一种层次强化学习方法，学习组合探索、导航和操控技能。为了实现这一点，我们设计了一个围绕语义地图记忆的抽象高级动作空间，并利用探索过的环境作为实例导航点。我们在仿真和真实世界中进行了大量实验，证明HIMOS可以零样本方式有效地迁移到新的环境中，并且对于未见过的子任务具有鲁棒性。

    Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
    
[^144]: 深度概率运动原理与贝叶斯聚合器

    Deep Probabilistic Movement Primitives with a Bayesian Aggregator. (arXiv:2307.05141v1 [cs.RO])

    [http://arxiv.org/abs/2307.05141](http://arxiv.org/abs/2307.05141)

    该论文提出了一个统一的深度运动原理模型，具备多种操作，并展示了高样本效率和泛化能力。

    

    运动原理是可训练的参数模型，可以从有限的演示集合中复制机器人的运动。先前的工作提出了简单的线性模型，通过允许时间调制运动（加速或减速复制运动）、混合（将两个运动合并为一个）、通过点调节（将运动约束到特定的通过点）和上下文调节（基于观察到的变量生成运动，例如物体的位置）展示出高样本效率和泛化能力。以前的工作已经提出了基于神经网络的运动原理模型，并展示了它们在一些形式的输入调节或时间调制表达中执行任务的能力。然而，迄今为止还没有提出一个单一统一的深度运动原理模型，它能够具备所有先前的操作，这限制了神经运动原理的潜在应用。本文提出了一个深度运动原理的架构。

    Movement primitives are trainable parametric models that reproduce robotic movements starting from a limited set of demonstrations. Previous works proposed simple linear models that exhibited high sample efficiency and generalization power by allowing temporal modulation of movements (reproducing movements faster or slower), blending (merging two movements into one), via-point conditioning (constraining a movement to meet some particular via-points) and context conditioning (generation of movements based on an observed variable, e.g., position of an object). Previous works have proposed neural network-based motor primitive models, having demonstrated their capacity to perform tasks with some forms of input conditioning or time-modulation representations. However, there has not been a single unified deep motor primitive's model proposed that is capable of all previous operations, limiting neural motor primitive's potential applications. This paper proposes a deep movement primitive arch
    
[^145]: 关于图神经网络的能力和激活函数的作用

    On the power of graph neural networks and the role of the activation function. (arXiv:2307.04661v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04661](http://arxiv.org/abs/2307.04661)

    本文通过对称多项式代数的工具证明了对于具有分段多项式激活函数且体系结构大小不变的GNNs，存在一对非同构根树在任意迭代次数内无法被区分，与此同时，具有不同大小的GNNs只需两次迭代即可区分。此外，我们还证明了如果允许非分段多项式激活函数，则在两次迭代内，单个神经元感知器可以区分任意一对非同构树的根节点。

    

    在这篇文章中，我们提出了关于图神经网络（GNNs）表达能力的新结果。我们证明了对于任何具有分段多项式激活函数、其体系结构大小不随图输入大小增长的GNNs，存在一对深度为二的非同构根树，使得GNNs在任意迭代次数内无法区分它们的根节点。证明依赖于对称多项式代数的工具。相比之下，已经知道具有分段多项式激活函数的无界GNNs（其大小允许随图大小改变）只需两次迭代即可区分这些顶点。我们的结果对于有界大小和无界大小的GNNs之间存在严格的分离，回答了 [Grohe, 2021] 提出的一个开放性问题。接下来，我们证明如果允许非分段多项式激活函数，则在两次迭代中，单个神经元感知器可以区分任意一对非同构树的根节点。

    In this article we present new results about the expressivity of Graph Neural Networks (GNNs). We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations. The proof relies on tools from the algebra of symmetric polynomials. In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations. Our results imply a strict separation between bounded and unbounded size GNNs, answering an open question formulated by [Grohe, 2021]. We next prove that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of 
    
[^146]: 高效的贝叶斯行程时间层析成像方法，利用基于敏感性的多项式混沌展开和深度生成网络的地质复杂先验

    Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v1 [physics.geo-ph])

    [http://arxiv.org/abs/2307.04228](http://arxiv.org/abs/2307.04228)

    本论文提出了一种高效的贝叶斯行程时间层析成像方法，利用敏感性信息的多项式混沌展开和深度生成网络，以处理地质复杂性先验的挑战。

    

    蒙特卡洛马尔可夫链（MCMC）方法常常面临两个根本性挑战：先验分布的准确刻画和似然函数的高效评估。在层析成像的贝叶斯研究中，主成分分析（PCA）在某些情况下可以方便地定义先验分布，并同时借助基于多项式混沌展开（PCE）的准确代理模型来替代计算密集的全物理正向求解器。当PCA无法直接提供定义先验分布的方式时，可以采用深度生成模型（例如变分自编码器（VAEs））等替代方法。然而，准确产生一个能够捕捉VAE的潜在参数与正向建模输出之间复杂非线性关系的代理模型是一个显著的挑战。

    Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental challenges: the accurate characterization of the prior distribution and the efficient evaluation of the likelihood. In the context of Bayesian studies on tomography, principal component analysis (PCA) can in some cases facilitate the straightforward definition of the prior distribution, while simultaneously enabling the implementation of accurate surrogate models based on polynomial chaos expansion (PCE) to replace computationally intensive full-physics forward solvers. When faced with scenarios where PCA does not offer a direct means of easily defining the prior distribution alternative methods like deep generative models (e.g., variational autoencoders (VAEs)), can be employed as viable options. However, accurately producing a surrogate capable of capturing the intricate non-linear relationship between the latent parameters of a VAE and the outputs of forward modeling presents a notable challenge. Indeed, while
    
[^147]: URL：一种可转移不确定性估计的表示学习基准

    URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates. (arXiv:2307.03810v1 [cs.LG])

    [http://arxiv.org/abs/2307.03810](http://arxiv.org/abs/2307.03810)

    URL基准是一个评估预训练模型可转移性和不确定性估计的方式，研究发现专注于表示本身不确定性或直接估计预测风险的方法效果优于基于概率的方法。

    

    表示学习显著推动了该领域发展出能够作为从零开始迁移到新数据集时的有价值起点的预训练模型。随着对可靠机器学习和不确定性量化的需求不断增加，需要的预训练模型不仅能提供嵌入向量，还能提供可转移的不确定性估计。为了引导这样的模型的开发，我们提出了URL（Uncertainty-aware Representation Learning）基准。除了表示的可转移性之外，它还使用一种新颖的度量标准来测量不确定性估计的零样本可转移性。我们应用URL来评估11种在ImageNet上进行预训练并转移到8个下游数据集的不确定性量化器。我们发现，着重于表示本身的不确定性或直接估计预测风险的方法优于基于上游类别的概率的方法。然而，实现可转移的不确定性仍然是一个挑战。

    Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction risk directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertaint
    
[^148]: Voicebox：大规模的多语言通用语音生成模型

    Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v1 [eess.AS])

    [http://arxiv.org/abs/2306.15687](http://arxiv.org/abs/2306.15687)

    Voicebox是一种大规模的多语言通用语音生成模型，通过使用非自回归的流匹配模型，在文本和音频上下文条件下进行训练，可以实现零样本跨语言文本到语音合成，噪声去除，内容编辑，风格转换和多样化的样本生成等多种任务。

    

    大规模生成模型，如GPT和DALL-E已经改变了自然语言处理和计算机视觉研究的方式。这些模型不仅可以生成高质量的文本或图像输出，而且还是通用的，可以解决未被明确教授的任务。相比之下，语音生成模型在规模和任务通用化方面仍然比较原始。在本文中，我们介绍了Voicebox，这是最多功能的面向规模的文本引导生成模型。Voicebox是一个非自回归的流匹配模型，通过在音频上下文和文本条件下进行训练，用50,000小时的未经过滤或增强的语音进行填充。与GPT类似，Voicebox可以通过上下文学习执行多种不同的任务，但更加灵活，因为它还可以对未来的上下文进行条件约束。Voicebox可以用于单语或跨语言零样本的文本到语音合成，噪声去除，内容编辑，风格转换和多样化的样本生成。特别是，Voicebox

    Large-scale generative models such as GPT and DALL-E have revolutionized natural language processing and computer vision research. These models not only generate high fidelity text or image outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are neither filtered nor enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox 
    
[^149]: 基于机器学习的针织力传感器不一致性补偿

    Machine Learning Based Compensation for Inconsistencies in Knitted Force Sensors. (arXiv:2306.12129v1 [eess.SY])

    [http://arxiv.org/abs/2306.12129](http://arxiv.org/abs/2306.12129)

    本文展示了一种基于机器学习的针织力传感器不一致性补偿方法，通过使用指数平滑滤波器进行预处理，并使用最小的人工神经网络来提高传感器读数和执行力之间的映射。

    

    针织传感器经常受到固有效应（如偏移、松弛和漂移）的影响而产生不一致性。这些属性的结合使得从传感器数据到物理执行具有挑战性。在本文中，我们展示了一种利用最小人工神经网络（ANN）结合简单预处理方法来进行处理的方法来对抗这种不一致性。我们在重新采样过的传感器信号上应用了多个指数平滑滤波器，以产生保留不同历史传感器数据水平的特征，并结合表示以前传感器执行充分状态的特征。通过训练一个三层ANN，总共有8个神经元，我们成功地提高了传感器读数和执行力之间的映射。我们的研究发现，我们的技术对于材料和结构相当不同的传感器也是适用的，而且还可以应用于相关的物理特征。

    Knitted sensors frequently suffer from inconsistencies due to innate effects such as offset, relaxation, and drift. These properties, in combination, make it challenging to reliably map from sensor data to physical actuation. In this paper, we demonstrate a method for counteracting this by applying processing using a minimal artificial neural network (ANN) in combination with straightforward pre-processing. We apply a number of exponential smoothing filters on a re-sampled sensor signal, to produce features that preserve different levels of historical sensor data and, in combination, represent an adequate state of previous sensor actuation. By training a three-layer ANN with a total of 8 neurons, we manage to significantly improve the mapping between sensor reading and actuation force. Our findings also show that our technique translates to sensors of reasonably different composition in terms of material and structure, and it can furthermore be applied to related physical features such
    
[^150]: 可证明强大的有向多图神经网络

    Provably Powerful Graph Neural Networks for Directed Multigraphs. (arXiv:2306.11586v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11586](http://arxiv.org/abs/2306.11586)

    本文分析了一组简单的改进方法，将标准的消息传递图神经网络（GNN）转化为可证明强大的有向多图神经网络，能够检测任何有向子图模式。实验结果展示了这些改进方法在合成子图检测任务和金融犯罪分析任务上的出色性能。

    

    本文分析了一组简单的改进方法，将标准的消息传递图神经网络（GNN）转化为可证明强大的有向多图神经网络。改进方法包括多图端口编号、个体ID和反向消息传递。我们证明这些方法的组合在理论上能够检测任何有向子图模式。为了验证我们提出的改进方法在实践中的有效性，我们在合成子图检测任务上进行了实验，结果表明其具有出色的性能，几乎可以得到完美的结果。此外，我们将提出的改进方法应用于两个金融犯罪分析任务。我们观察到在检测洗钱交易方面有显著的改善，将标准的消息传递GNN的少数类F1分数提高了高达30%，并且与基于树和GNN的基准相媲美或超越。在一个实际的网络钓鱼检测数据集上也观察到了类似令人印象深刻的结果，提升了三个标准方法的性能。

    This paper analyses a set of simple adaptations that transform standard message-passing Graph Neural Networks (GNN) into provably powerful directed multigraph neural networks. The adaptations include multigraph port numbering, ego IDs, and reverse message passing. We prove that the combination of these theoretically enables the detection of any directed subgraph pattern. To validate the effectiveness of our proposed adaptations in practice, we conduct experiments on synthetic subgraph detection tasks, which demonstrate outstanding performance with almost perfect results. Moreover, we apply our proposed adaptations to two financial crime analysis tasks. We observe dramatic improvements in detecting money laundering transactions, improving the minority-class F1 score of a standard message-passing GNN by up to 30%, and closely matching or outperforming tree-based and GNN baselines. Similarly impressive results are observed on a real-world phishing detection dataset, boosting three standar
    
[^151]: 用一致性检查评估超人模型

    Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])

    [http://arxiv.org/abs/2306.09983](http://arxiv.org/abs/2306.09983)

    本文提出了一个用一致性检查评估超人模型的框架，可以发现决策制定中的逻辑不一致性，即使对于超人模型的决策正确性可能是不可能评估的情况。

    

    如果机器学习模型在各种推理或决策任务上实现了超人能力，那么我们该如何评估这些模型，考虑到人类代理会产生偏差? 在本文中，我们提出了一个用一致性检查评估超人模型的框架。我们的前提是，虽然评估超人决策的正确性可能是不可能的，但是如果模型的决策未能满足某些逻辑上、可解释的规则，我们仍然可以发现错误。我们将我们的框架实现在三个任务上，这些任务的决策正确性由于超人模型能力或其他缺乏基本事实而难以评估：评估国际象棋局面、预测未来事件和作出法律判断。我们表明，无论模型在这些任务上的表现如何(可能是超人的)，我们都能发现决策制定中的逻辑不一致性。例如：国际象棋引擎给出对局中棋子相对估值的不同排列。

    If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
    
[^152]: 分散式学习动力学中个体群体的力量

    The Power of Populations in Decentralized Learning Dynamics. (arXiv:2306.08670v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08670](http://arxiv.org/abs/2306.08670)

    本文研究了分散式学习动力学中个体群体的力量。我们介绍了一种分散式的多臂赌博机设置，并分析了几个针对此任务的分散式动力学家族。我们展示了这些动力学与一类“零和”乘法权重更新算法的联系，并开发了一个通用框架来分析这些协议的群体级遗憾。在广泛的参数范围下，我们得到了次线性的遗憾界限。

    

    我们研究了一种分散式多臂赌博机设置，在一个由$n$个受内存限制的节点组成的种群中，采用了谣言模型：每轮，每个节点本地采用$m$个臂之一，观察从臂的（对抗选择的）分布中抽取的奖励，然后与随机抽取的邻居进行通信，交换信息，以确定下一轮的策略。我们介绍并分析了几个针对此任务的分散式动力学家族：每个节点的决策完全是局部的，只依赖于其最新获得的奖励以及它抽样的邻居的奖励。我们展示了这些分散式动力学的全局演化与特定类型的“零和”乘法权重更新算法之间的联系，并且开发了一个分析这些自然协议的群体级遗憾的通用框架。利用这个框架，我们在广泛的参数范围（即，种群的大小和nu的大小）下推导了次线性遗憾界限。

    We study a distributed multi-armed bandit setting among a population of $n$ memory-constrained nodes in the gossip model: at each round, every node locally adopts one of $m$ arms, observes a reward drawn from the arm's (adversarially chosen) distribution, and then communicates with a randomly sampled neighbor, exchanging information to determine its policy in the next round. We introduce and analyze several families of dynamics for this task that are decentralized: each node's decision is entirely local and depends only on its most recently obtained reward and that of the neighbor it sampled. We show a connection between the global evolution of these decentralized dynamics with a certain class of "zero-sum" multiplicative weights update algorithms, and we develop a general framework for analyzing the population-level regret of these natural protocols. Using this framework, we derive sublinear regret bounds under a wide range of parameter regimes (i.e., the size of the population and nu
    
[^153]: ArtWhisperer：一个用于描述艺术创作中人工智能与人类交互的数据集

    ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])

    [http://arxiv.org/abs/2306.08141](http://arxiv.org/abs/2306.08141)

    为研究人工智能与人类交互，研究者创建了ArtWhisperer数据集，这是一个在线游戏，人们通过反复尝试不同的提示词，来生成和目标图像类似的图像，并记录了50,000多个交互记录。在初步分析中，研究者发现人们提交了各种各样的提示词，并能够发现生成各种文本描述的图像。

    

    随着生成型人工智能越来越普及，研究人类用户如何与这些模型交互变得越来越重要。在这项工作中，我们研究了人们如何使用文本到图像的模型生成所需的目标图像。为了研究这种交互，我们创建了ArtWhisperer，这是一个在线游戏，用户会得到一个目标图像，并需要反复尝试不同的提示词，以便生成类似目标图像的图像。通过这个游戏，我们记录了50,000多个人工智能-人类交互的记录；每个交互都对应着用户创建的一个提示词和相应生成的图像。大多数记录都是重复的交互，用户通过反复尝试找到最佳的提示词以生成目标图像，这使得这个数据集成为研究人工智能与人类协作的独特连续数据集。在对这个数据集的初步分析中，我们发现了一些提示词交互和用户策略的特征。人们提交了各种各样的提示词，并能够发现生成各种文本描述的图像。

    As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
    
[^154]: Kepler: 快速参数查询优化的鲁棒学习方法

    Kepler: Robust Learning for Faster Parametric Query Optimization. (arXiv:2306.06798v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2306.06798](http://arxiv.org/abs/2306.06798)

    Kepler是一种快速参数查询优化的鲁棒学习方法，通过使用基于实际执行数据的评估和机器学习模型预测最快计划，Kepler能够在查询延迟上显著加速，避免查询性能回归。

    

    大多数现有的参数查询优化（PQO）技术依赖于传统的查询优化器成本模型，这些模型通常不准确，导致查询性能不佳。我们提出了Kepler，一种端到端的基于学习的PQO方法，在查询延迟方面显示出显著的加速。我们方法的核心是行计数演进（RCE），一种基于子计划基数空间扰动的新型计划生成算法。以前的方法需要准确的成本模型，而我们通过使用实际执行数据对候选计划进行评估，并训练一个机器学习模型来预测给定参数绑定值的最快计划，绕过了这一要求。我们的模型利用了神经网络不确定性的最新进展，以鲁棒地预测更快的计划，避免查询性能回归。在实验证明，Kepler 在多个 PostgreSQL 数据集上实现了查询运行时的显著改进。

    Most existing parametric query optimization (PQO) techniques rely on traditional query optimizer cost models, which are often inaccurate and result in suboptimal query performance. We propose Kepler, an end-to-end learning-based approach to PQO that demonstrates significant speedups in query latency over a traditional query optimizer. Central to our method is Row Count Evolution (RCE), a novel plan generation algorithm based on perturbations in the sub-plan cardinality space. While previous approaches require accurate cost models, we bypass this requirement by evaluating candidate plans via actual execution data and training an ML model to predict the fastest plan given parameter binding values. Our models leverage recent advances in neural network uncertainty in order to robustly predict faster plans while avoiding regressions in query performance. Experimentally, we show that Kepler achieves significant improvements in query runtime on multiple datasets on PostgreSQL.
    
[^155]: 变分不平衡回归(Variational Imbalanced Regression)

    Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])

    [http://arxiv.org/abs/2306.06599](http://arxiv.org/abs/2306.06599)

    本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。

    

    当标签分布不平衡时，现有的回归模型往往在准确性和不确定性估计方面表现不佳。本文提出了一种概率深度学习模型——变分不平衡回归（VIR），它不仅在不平衡回归方面表现出色，而且自然地产生合理的不确定性估计。与典型的变分自编码器假设I.I.D.表示（数据点的表示不直接受其他数据点的影响）不同，我们的VIR借用具有类似回归标签的数据来计算潜在表示的变分分布；此外，不同于产生点估计的确定性回归模型， VIR预测整个正态反-伽玛分布并调节相关联的共轭分布，对不平衡数据施加概率重新加权，从而提供更好的不确定性估计。在几个真实世界的数据集上进行了实验。

    Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
    
[^156]: 语言指导下的场景级交通仿真模拟

    Language-Guided Traffic Simulation via Scene-Level Diffusion. (arXiv:2306.06344v1 [cs.RO])

    [http://arxiv.org/abs/2306.06344](http://arxiv.org/abs/2306.06344)

    该论文提出了一种可以受到语言指导的场景级条件扩散模型，该模型能够生成真实且可控的交通，并通过大型语言模型将用户的查询转化为损失函数，指导模型生成符合查询条件的仿真交通。

    

    实现真实和可控的交通仿真是加速自主驾驶汽车（AV）发展的核心能力。然而，目前用于控制基于学习的交通模型的方法需要大量领域专业知识，对于从业者来说很难使用。为了解决这个问题，我们提出了CTG++，一种可以受到语言指导的场景级条件扩散模型。为了达到这个目的，我们需要解决两个问题：需要一个真实和可控的交通模型骨干结构，并且要有一种有效的方法来使用语言与交通模型进行交互。为了解决这些问题，我们首先提出了一个带有时空转换器骨干结构的场景级扩散模型，它生成了真实和可控的交通。然后，我们利用大型语言模型（LLM）将用户的查询转换为损失函数，指导扩散模型朝着查询合规的生成方向前进。通过全面的评估，我们展示了该模型的有效性。

    Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) development. However, current approaches for controlling learning-based traffic models require significant domain expertise and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic and controllable traffic model backbone, and an effective method to interface with a traffic model using language. To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert a user's query into a loss function, guiding the diffusion model towards query-compliant generation. Through comprehensive evaluation, we demonstrate the effect
    
[^157]: 关于扩散模型的设计基础：综述

    On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v1 [cs.LG])

    [http://arxiv.org/abs/2306.04542](http://arxiv.org/abs/2306.04542)

    本文综述了扩散模型的设计基础，即其三个关键组件：正向过程、逆向过程和采样过程，为未来的研究提供了有益的细粒度透视。

    

    扩散模型是一种生成模型，通过逐渐添加和删除噪声来学习训练数据的潜在分布以生成数据。扩散模型的组成部分已经受到了广泛的关注，许多设计选择被提出。现有的评论主要关注高层次的解决方案，对组件的设计基础覆盖较少。本研究旨在通过提供一个全面而连贯的综述，针对扩散模型的组件设计选择进行分析。具体来说，我们将这个综述按照三个关键组件进行组织，即正向过程、逆向过程和采样过程。这使得我们可以提供扩散模型的细粒度透视，有助于未来研究分析个体组件、设计选择的适用性以及扩散模型的实现。

    Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.
    
[^158]: Schema First！通过MASCHInE捕捉语义学习通用知识图嵌入

    Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE. (arXiv:2306.03659v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.03659](http://arxiv.org/abs/2306.03659)

    本论文提出了一种通过 MASCHInE 捕捉语义学习知识图嵌入的方法，通过设计生成原型图并利用其语义，进而训练出更好地捕捉语义的 KGEs。

    

    近年来，知识图嵌入模型（KGEMs）受到了广泛关注，这些模型学习了知识图中实体和关系的向量表示，即知识图嵌入（KGEs）。学习多功能的KGEs非常有意义，因为这使得它们在广泛的任务上有用。然而，KGEMs通常是针对特定任务进行训练的，这使得它们的嵌入是任务相关的。与此同时，关于KGEMs实际上是否创建了底层实体和关系的语义表示（例如，将相似的实体放在一起，将不相似的实体放在一起）的普遍假设受到了质疑。在这项工作中，我们设计了启发式方法来生成原型图-一个小型、修改过的KG版本，利用了RDF/S信息。所学习的基于原型图的嵌入旨在封装KG的语义，并可以在学习KGEs时加以利用，从而更好地捕捉语义。对各种评估基准进行了大量实验证明了这一点。

    Knowledge graph embedding models (KGEMs) have gained considerable traction in recent years. These models learn a vector representation of knowledge graph entities and relations, a.k.a. knowledge graph embeddings (KGEs). Learning versatile KGEs is desirable as it makes them useful for a broad range of tasks. However, KGEMs are usually trained for a specific task, which makes their embeddings task-dependent. In parallel, the widespread assumption that KGEMs actually create a semantic representation of the underlying entities and relations (e.g., project similar entities closer than dissimilar ones) has been challenged. In this work, we design heuristics for generating protographs -small, modified versions of a KG that leverage RDF/S information. The learnt protograph-based embeddings are meant to encapsulate the semantics of a KG, and can be leveraged in learning KGEs that, in turn, also better capture semantics. Extensive experiments on various evaluation benchmarks demonstrate the so
    
[^159]: 使预训练模型具有可逆性：从参数到内存高效的微调

    Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])

    [http://arxiv.org/abs/2306.00477](http://arxiv.org/abs/2306.00477)

    本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。

    

    预训练语言模型（PLM）的参数高效微调已经成为一种非常成功的方法，只需训练少量参数而不会降低性能，并随着PLM越来越大而成为事实上的学习范式。然而，现有的PEFT方法不具备内存效率，因为它们仍需要存储大部分中间激活值以便计算梯度，类似于微调。一个减少激活内存的有效方法是应用可逆模型，这样中间激活值就无需缓存，可以重新计算。然而，将PLM修改为它的可逆变体并进行PEFT并不是一件容易的事，因为可逆模型具有与当前发布的PLM不同的体系结构。本文首先调查现有PEFT方法成功的关键因素，认识到在初始化PEFT时保留PLM的起点是至关重要的。

    Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
    
[^160]: AdANNS: 一种自适应语义搜索框架

    AdANNS: A Framework for Adaptive Semantic Search. (arXiv:2305.19435v1 [cs.LG])

    [http://arxiv.org/abs/2305.19435](http://arxiv.org/abs/2305.19435)

    AdANNS是一种自适应语义搜索框架，利用不同容量的自适应表示形式可以获得更好的精度-计算折衷权衡，相似度计算越接近的数据点将使用更低容量的表示形式进行计算，演示了最先进的精度-计算折衷权衡。

    

    网络规模的搜索系统学习一个编码器来嵌入一个给定的查询，然后将其连接到近似最近邻搜索(ANNS)管道中来检索相似的数据点。为了准确地捕捉尾部查询和数据点，学习到的表示通常是刚性的、高维的向量，通常在整个ANNS管道中一成不变，并且可能导致计算上昂贵的检索。本文认为，与其使用刚性的表示形式，ANNS的不同阶段可以利用不同容量的自适应表示形式以获得显著的精度-计算折衷权衡，即可以进行更加近似计算的ANNS阶段应该使用相同数据点的低容量表示。为此，我们引入了AdANNS，一种新颖的ANNS设计框架，明确利用Matryoshka表示的灵活性。我们使用基于AdANNS的新型关键ANNS构建演示了最先进的精度-计算折衷权衡。

    Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building
    
[^161]: 比较长短时记忆（LSTM）和双向LSTM深度神经网络在电力消耗预测中的应用

    Comparing Long Short-Term Memory (LSTM) and Bidirectional LSTM Deep Neural Networks for power consumption prediction. (arXiv:2305.16546v1 [cs.LG])

    [http://arxiv.org/abs/2305.16546](http://arxiv.org/abs/2305.16546)

    本文比较了LSTM和BLSTM两种深度学习模型在电力消耗短期预测中的性能，通过四个数据集的结果表明BLSTM的表现更好。

    

    电力消耗预测方法是为了决策节能以及在能源市场中预测需求等多种原因而进行研究的。本研究旨在比较两种深度学习模型，即长短时记忆（LSTM）和双向LSTM（BLSTM），用于单变量电度量时间序列的短期预测。为了评估模型的鲁棒性，选择了四个不同上下文和规模的数据集。这些四个数据集分别是：（a）法国家庭的用电量；（b）巴西Santaém的一座大学建筑的用电量；（c）摩洛哥Tétouan市的用电需求；（d）新加坡聚合电力需求。采用时间序列交叉验证方案计算了RMSE、MAE、MAPE和R2等指标。对归一化RMSE（NRMSE）的结果应用了Friedman检验，表明BLSTM比LSTM表现更好并且具有统计显著性。

    Electric consumption prediction methods are investigated for many reasons such as decision-making related to energy efficiency as well as for anticipating demand in the energy market dynamics. The objective of the present work is the comparison between two Deep Learning models, namely the Long Short-Term Memory (LSTM) and Bi-directional LSTM (BLSTM) for univariate electric consumption Time Series (TS) short-term forecast. The Data Sets (DSs) were selected for their different contexts and scales, aiming the assessment of the models' robustness. Four DSs were used, related to the power consumption of: (a) a household in France; (b) a university building in Santar\'em, Brazil; (c) the T\'etouan city zones, in Morocco; and (c) the Singapore aggregated electric demand. The metrics RMSE, MAE, MAPE and R2 were calculated in a TS cross-validation scheme. The Friedman's test was applied to normalized RMSE (NRMSE) results, showing that BLSTM outperforms LSTM with statistically significant differ
    
[^162]: Voyager:一个具有大型语言模型的开放式机器体代理

    Voyager: An Open-Ended Embodied Agent with Large Language Models. (arXiv:2305.16291v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.16291](http://arxiv.org/abs/2305.16291)

    Voyager是一个在Minecraft中使用大型语言模型进行开放式探索和学习的代理，它通过自动课程设置、可执行代码技能库和迭代提示机制不断提升自己的能力，并展现出强大的终身学习能力和在玩Minecraft方面的出色表现。

    

    我们介绍了Voyager，这是一个在Minecraft中持续探索世界、获得多种技能和进行新的发现而无需人类干预的第一个LLM驱动的全能代理。Voyager包括三个关键组件：1)最大化探索的自动课程设置，2)用于存储和检索复杂行为的不断增长的可执行代码技能库，和3)一种通过环境反馈、执行错误和自我验证进行程序改进的新的迭代提示机制。Voyager通过黑盒查询与GPT-4互动，避免了模型参数微调的需要。Voyager开发的技能具有时间延长性、可解释性和组合性，这加快了代理的能力增长并减轻了灾难性遗忘。从经验上看，Voyager表现出强大的上下文型终身学习能力，并在玩Minecraft方面展现了异常的熟练程度。

    We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique it
    
[^163]: 改善选择性度量的私有合成数据的后处理方法

    Post-processing Private Synthetic Data for Improving Utility on Selected Measures. (arXiv:2305.15538v1 [cs.LG])

    [http://arxiv.org/abs/2305.15538](http://arxiv.org/abs/2305.15538)

    本论文提出一种后处理技术，能够通过重新抽样，过滤掉不符合最终用户选择的度量指标的合成数据样本，从而提高其效用，同时保持隐私和数据集质量

    

    现有的私有合成数据生成算法忽略下游任务，但是最终用户可能有特定的需求，合成数据必须满足这些需求，否则可能会显著降低数据的下游用途效用。我们提出了一种后处理技术，针对最终用户选择的度量指标，提高了合成数据的效用，同时保持了强大的隐私保证和数据集质量。我们的技术涉及从合成数据中重新抽样，过滤掉不满足所选效用度量的样本，使用有效的随机一阶算法寻找最优的重新抽样权重。通过全面的数值实验，我们证明了我们的方法能够始终在多个基准数据集和最先进的合成数据生成算法中提高合成数据的效用。

    Existing private synthetic data generation algorithms are agnostic to downstream tasks. However, end users may have specific requirements that the synthetic data must satisfy. Failure to meet these requirements could significantly reduce the utility of the data for downstream use. We introduce a post-processing technique that improves the utility of the synthetic data with respect to measures selected by the end user, while preserving strong privacy guarantees and dataset quality. Our technique involves resampling from the synthetic data to filter out samples that do not meet the selected utility measures, using an efficient stochastic first-order algorithm to find optimal resampling weights. Through comprehensive numerical experiments, we demonstrate that our approach consistently improves the utility of synthetic data across multiple benchmark datasets and state-of-the-art synthetic data generation algorithms.
    
[^164]: 在一个Matroid约束下流式子模最大化中的公平性

    Fairness in Streaming Submodular Maximization over a Matroid Constraint. (arXiv:2305.15118v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15118](http://arxiv.org/abs/2305.15118)

    这篇论文研究了在一个Matroid约束下流式子模最大化中的公平性问题，提供了流式算法和不可能的结果来权衡效率、质量和公平性，并在现实世界应用中进行了实证验证。

    

    流式子模最大化是从一个大规模数据集中选择一个代表性子集的自然模型。如果数据点具有敏感属性，如性别或种族，强制公平性以避免偏见和歧视变得重要。这引起了对开发公平机器学习算法的极大兴趣。最近，这样的算法已经被开发用于基于基数约束的单调子模最大化。在本文中，我们研究了这个问题的自然推广到一个Matroid约束。我们提供了流式算法以及不可能的结果，这些结果在效率、质量和公平性之间提供了权衡。我们在一系列知名的现实世界应用中对我们的发现进行了经验证实：基于示例的聚类、电影推荐和社交网络中的最大覆盖。

    Streaming submodular maximization is a natural model for the task of selecting a representative subset from a large-scale dataset. If datapoints have sensitive attributes such as gender or race, it becomes important to enforce fairness to avoid bias and discrimination. This has spurred significant interest in developing fair machine learning algorithms. Recently, such algorithms have been developed for monotone submodular maximization under a cardinality constraint.  In this paper, we study the natural generalization of this problem to a matroid constraint. We give streaming algorithms as well as impossibility results that provide trade-offs between efficiency, quality and fairness. We validate our findings empirically on a range of well-known real-world applications: exemplar-based clustering, movie recommendation, and maximum coverage in social networks.
    
[^165]: 连接多模态对比表示

    Connecting Multi-modal Contrastive Representations. (arXiv:2305.14381v1 [cs.LG])

    [http://arxiv.org/abs/2305.14381](http://arxiv.org/abs/2305.14381)

    本文提出了一种无需配对数据学习MCR的方法，叫做C-MCR，并且在新空间中使用重叠模态B的数据来对齐两个MCR。通过这个方法，非重叠模态对（A，C）也可以使用连接。

    

    多模态对比表示（MCR）学习旨在将不同的模态编码到一个语义对齐的共享空间中。该范例在各种模式下的大量下游任务中表现出了显著的泛化能力。然而，对大规模高质量数据对的依赖限制了其在更多模态上的进一步发展。本文提出了一种新的无需配对数据学习MCR的训练高效方法，称为连接多模态对比表示（C-MCR）。具体而言，在（A，B）和（B，C）模态对上预训练两个现有的MCR之后，我们将它们投影到一个新的空间，并使用重叠模态B的数据来在新空间中对齐两个MCR。同时，由于模态对（A，B）和（B，C）在每个MCR内已经对齐，因此通过重叠模态学习到的连接也可以转移到非重叠模态对（A，C）。为了发挥C-MCR的潜力，我们进一步引入了一个语义增强的int

    Multi-modal Contrastive Representation (MCR) learning aims to encode different modalities into a semantically aligned shared space. This paradigm shows remarkable generalization ability on numerous downstream tasks across various modalities. However, the reliance on massive high-quality data pairs limits its further development on more modalities. This paper proposes a novel training-efficient method for learning MCR without paired data called Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project them to a new space and use the data from the overlapping modality B to aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A, B) and (B, C) are already aligned within each MCR, the connection learned by overlapping modality can also be transferred to non-overlapping modality pair (A, C). To unleash the potential of C-MCR, we further introduce a semantic-enhanced int
    
[^166]: 理解ReLU网络的多阶段优化动态和丰富的非线性行为

    Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks. (arXiv:2305.12467v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12467](http://arxiv.org/abs/2305.12467)

    本文对于ReLU神经网络通过Gradient Flow训练的两层模型在线性可分数据上进行了完整的理论分析，揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。

    

    ReLU神经网络的训练过程经常表现出复杂的非线性现象。模型的非线性和损失的非凸性为理论分析带来了重大挑战。本文对通过Gradient Flow训练的二层ReLU网络在线性可分数据上进行了完整的理论描述。在这种特定的设置下，我们的分析捕获了从随机初始化到最终收敛的整个优化过程。尽管我们研究的模型和数据相对简单，但我们揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。特定的非线性行为也可以被精确地识别和理论上捕获，例如...

    The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such 
    
[^167]: 面向剧集式马尔可夫决策过程的在线资源分配问题

    Online Resource Allocation in Episodic Markov Decision Processes. (arXiv:2305.10744v1 [cs.DS])

    [http://arxiv.org/abs/2305.10744](http://arxiv.org/abs/2305.10744)

    本文将长期资源分配问题形式化为剧集式MDP模型，提出在面临转换和奖励特性不确定的情况下进行在线资源分配问题的解决方案。该方案基于占有度量提供在线镜像下降算法，其期望遗憾受到界限约束 $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ .

    

    本文研究了一个长期的资源分配问题，它需要在多个时间段内进行多阶段的决策过程。我们将这个问题形式化为一个剧集式有限时间段的马尔可夫决策过程中的在线资源分配问题，其中转换和奖励以及每一次的资源消耗函数都是非定态的。我们提供了一种等效的在线线性规划重构方法，基于占有度量，为此我们开发了一种在线镜像下降算法。我们的资源分配在线镜像下降算法处理了在估算真实可行集时的不确定性和误差，这是相对独立的。我们证明，对于随机奖励和资源消耗函数，在线镜像下降算法的期望遗憾受到界限约束，其界限受到 $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ 的约束，其中 $\rho\in(0,1)$ 是预算参数，$H$ 是地平线长度，$S$ 和 $A$ 是. . .

    This paper studies a long-term resource allocation problem over multiple periods where each period requires a multi-stage decision-making process. We formulate the problem as an online resource allocation problem in an episodic finite-horizon Markov decision process with unknown non-stationary transitions and stochastic non-stationary reward and resource consumption functions for each episode. We provide an equivalent online linear programming reformulation based on occupancy measures, for which we develop an online mirror descent algorithm. Our online dual mirror descent algorithm for resource allocation deals with uncertainties and errors in estimating the true feasible set, which is of independent interest. We prove that under stochastic reward and resource consumption functions, the expected regret of the online mirror descent algorithm is bounded by $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ where $\rho\in(0,1)$ is the budget parameter, $H$ is the length of the horizon, $S$ and $A$ are the
    
[^168]: 多功能储备计算机的双重视觉效应

    Seeing double with a multifunctional reservoir computer. (arXiv:2305.05799v1 [math.DS])

    [http://arxiv.org/abs/2305.05799](http://arxiv.org/abs/2305.05799)

    本文研究了储备计算机的多功能性能力，探讨了不同吸引子之间的关系如何影响其实现多任务。结果表明，要实现多功能性，需要RC内部网络c的谱半径合适选择的临界依赖性。

    

    多功能生物神经网络利用多重稳定性以执行多种任务而不改变任何网络属性。使人工神经网络（ANN）获得某些多稳定性以执行多个任务，其中每个任务与网络状态空间中的特定吸引子相关联，从机器学习的角度自然具有许多优点。因为与多稳定性有关，本文探讨了不同吸引子之间的关系如何影响储备计算机（RC）的多功能性能力，储备计算机是一种以ANN形式呈现的动态系统。我们构建了“双重视觉效应”问题来系统地研究当两个吸引子之间存在重叠时RC如何重构吸引子的共存。随着重叠量的增加，我们发现要实现多功能性，需要RC内部网络c的谱半径合适选择的临界依赖性。

    Multifunctional biological neural networks exploit multistability in order to perform multiple tasks without changing any network properties. Enabling artificial neural networks (ANNs) to obtain certain multistabilities in order to perform several tasks, where each task is related to a particular attractor in the network's state space, naturally has many benefits from a machine learning perspective. Given the association to multistability, in this paper we explore how the relationship between different attractors influences the ability of a reservoir computer (RC), which is a dynamical system in the form of an ANN, to achieve multifunctionality. We construct the `seeing double' problem to systematically study how a RC reconstructs a coexistence of attractors when there is an overlap between them. As the amount of overlap increases, we discover that for multifunctionality to occur, there is a critical dependence on a suitable choice of the spectral radius for the RC's internal network c
    
[^169]: 应用基于生成式预训练自回归Transformer图神经网络的方法分析和发现新型蛋白质

    Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.04934](http://arxiv.org/abs/2305.04934)

    本研究使用基于语言模型的深度学习策略，在蛋白质建模中应用transformer和图卷积的结构预训练生成模型，进一步训练后能够设计具有特定性质的蛋白质，案例验证表明该方法可生成理想目标性质的蛋白质。

    

    本文报道了一种灵活的基于语言模型的深度学习策略，应用于解决蛋白质建模中的正向和反向问题，使用一个整合了transformer和图卷积的注意力神经网络结构，在因果多头图机制中实现预训练生成模型。该模型被用于预测二级结构内容（每个残基的水平和总体内容）、蛋白质可溶性和测序任务。进一步在反向任务上训练，该模型能够设计具有这些性质作为目标特征的蛋白质。该模型被制定为一个通用的框架，完全基于提示，可以为各种下游任务进行适应。我们发现添加额外任务会产生相互协同作用，使模型在整体性能上得到提高，超过仅在每个数据集上训练模型的可能性。案例研究用于验证该方法，生成具有理想目标性质，包括稳定性和可溶性的蛋白质，并进行实验性研究。

    We report a flexible language-model based deep learning strategy, applied here to solve complex forward and inverse problems in protein modeling, based on an attention neural network that integrates transformer and graph convolutional architectures in a causal multi-headed graph mechanism, to realize a generative pretrained model. The model is applied to predict secondary structure content (per-residue level and overall content), protein solubility, and sequencing tasks. Further trained on inverse tasks, the model is rendered capable of designing proteins with these properties as target features. The model is formulated as a general framework, completely prompt-based, and can be adapted for a variety of downstream tasks. We find that adding additional tasks yields emergent synergies that the model exploits in improving overall performance, beyond what would be possible by training a model on each dataset alone. Case studies are presented to validate the method, yielding protein designs
    
[^170]: 基于“梯度下降”与 beam search 的自动提示优化

    Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])

    [http://arxiv.org/abs/2305.03495](http://arxiv.org/abs/2305.03495)

    在基于大型语言模型的自然语言处理中，使用梯度下降和 beam search 的自动提示优化方法可以自动改进提示，提高性能。

    

    大型语言模型（LLM）在通用智能方面展现了出色性能，但其能力仍高度依赖于手写的提示，需要大量的试错尝试。我们提出了一个简单而非参数化的解决方案——自动提示优化（APO），其灵感来自于使用数值梯度下降自动改进提示。

    Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt. The gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing 
    
[^171]: Patch Diffusion: 更快更高效的扩散模型训练方法

    Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models. (arXiv:2304.12526v1 [cs.CV])

    [http://arxiv.org/abs/2304.12526](http://arxiv.org/abs/2304.12526)

    Patch Diffusion 提出了一种通用的基于块的训练框架，可将扩散模型训练时间缩短至少一倍，并在更小的数据集上实现更好的性能表现。

    

    扩散模型是强大的，但需要大量时间和数据来训练。我们提出了一种通用的基于块的训练框架 Patch Diffusion，显著降低了训练时间成本，同时提高了数据效率，从而有助于将扩散模型的训练推广到更广泛的用户。我们的创新核心是新的条件评分函数，它在块级别上进行操作，并将原始图像中的块位置作为附加坐标通道，同时通过在训练过程中随机化和多样化块大小来编码多尺度的跨区域依赖关系。在采样方面，我们的方法与原始扩散模型一样简单易用。通过 Patch Diffusion，我们能够实现 $\mathbf{\ge 2\times}$ 更快的训练速度，同时保持可比较或更好的生成质量。同时，Patch Diffusion 提高了在相对较小的数据集上训练的扩散模型的性能，例如，使用仅 5,000 张图像进行从头开始训练。

    Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve 
    
[^172]: PEFT-Ref: 一种模块化的参考架构和类型，用于参数效率微调技术

    PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])

    [http://arxiv.org/abs/2304.12410](http://arxiv.org/abs/2304.12410)

    本文提出了PEFT-Ref参考架构，标准化了不同PEFT技术共享的方面，隔离了差异到特定位置和交互中，模块化的视图有助于比较不同技术及其效率和任务性能，并有助于更好地理解PEFT的基本原理。

    

    最近的参数效率微调(PEFT)技术旨在改善完全微调大型预训练语言模型(PLM)的高昂成本。随着不同的PEFT技术不断出现，对它们进行比较变得越来越困难，特别是在以下方面：(i)它们添加到PLM的结构和功能，(ii)不同类型和程度的效率改进，(iii)在不同的下游任务中的性能，以及(iv)结构和功能差异如何与效率和任务性能相关联。为了促进这样的比较，本文提出了一个参考框架，标准化了不同PEFT技术共享的方面，同时将差异隔离到与标准组件的特定位置和交互中。通过这个标准化和隔离差异的过程，出现了PEFT技术的模块化视图，不仅支持直接比较不同技术及其效率和任务性能，而且还有助于更好地理解PEFT的基本原理。所提出的参考架构称为PEFT-Ref，包括七个核心模块，每个模块都处理PEFT的特定方面，并可用作开发新PEFT技术和比较现有技术的指南。

    Recent parameter-efficient finetuning (PEFT) techniques aim to improve over the considerable cost of fully finetuning large pretrained language models (PLM). As different PEFT techniques proliferate, it is becoming difficult to compare them, in particular in terms of (i) the structure and functionality they add to the PLM, (ii) the different types and degrees of efficiency improvements achieved, (iii) performance at different downstream tasks, and (iv) how differences in structure and functionality relate to efficiency and task performance. To facilitate such comparisons, this paper presents a reference framework which standardises aspects shared by different PEFT techniques, while isolating differences to specific locations and interactions with the standard components. Through this process of standardising and isolating differences, a modular view of PEFT techniques emerges, supporting not only direct comparison of different techniques and their efficiency and task performance, but a
    
[^173]: Transformer介绍

    An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])

    [http://arxiv.org/abs/2304.10557](http://arxiv.org/abs/2304.10557)

    Transformer是一种神经网络组件，可以学习序列或数据集表示，在自然语言处理、计算机视觉和时空建模方面取得了重大进展。本论文提供了一个数学精确、直观、简洁的Transformer架构描述。

    

    Transformer是一种可以学习序列或数据集表示的神经网络组件。Transformer在自然语言处理、计算机视觉和时空建模方面取得了重大进展。虽然有很多Transformer的介绍，但大多数都缺少对其架构的精确数学描述，其设计选择的直觉也常常缺失。此外，随着研究路径的曲折，Transformer部件的解释可能是异质的。在这篇论文中，我们旨在提供一个数学精确、直观、简洁的Transformer架构描述。

    The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
    
[^174]: 基于图结构的多标签节点分类

    Multi-label Node Classification On Graph-Structured Data. (arXiv:2304.10398v1 [cs.LG])

    [http://arxiv.org/abs/2304.10398](http://arxiv.org/abs/2304.10398)

    该论文提出了一种新的图神经网络架构 MLGCN，用于处理多标签节点分类任务，并研究了多标签场景中同类偏好的语义。在各种基准测试中，MLGCN优于现有的最先进的多标签分类方法。

    

    图神经网络（GNN）在图中节点分类任务方面展示了最先进的改进。虽然这些进展在多类分类场景中得到了广泛的展示，但一个更普遍和现实的情况，在这种情况下，每个节点可能有多个标签，一直以来受到了很少关注。在进行关于多标签节点分类的重点研究的首要挑战是公开可用的多标签图数据集数量有限。因此，作为我们的第一个贡献，我们收集并发布了三个真实的生物数据集，并开发了一个多标签图生成器，以生成具有可调属性的数据集。虽然高标签相似性（高同类偏好）通常被归因于GNN的成功，但我们认为多标签场景并不遵循目前为多类场景定义的同类偏好和异类偏好的常规语义。作为我们的第二个贡献，我们除了为多标签场景定义同类偏好外，还开发了一种新颖的GNN体系结构，即MLGCN（多标签图卷积网络），来处理多标签节点分类任务。我们的实验表明，在各种基准测试中，MLGCN优于现有的最先进的多标签分类方法。

    Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, besides defining homophily for the multi-label scenario, we dev
    
[^175]: 自适应 $\tau$-Lasso：其健壮性和最优性质。

    The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties. (arXiv:2304.09310v1 [stat.ML])

    [http://arxiv.org/abs/2304.09310](http://arxiv.org/abs/2304.09310)

    本文提出了一种新型鲁棒的自适应 $\tau$-Lasso 估计器，同时采用自适应 $\ell_1$-范数惩罚项以降低真实回归系数的偏差。它具有变量选择一致性和真实支持下回归向量的渐近正态性的最优性质，假定已知真实回归向量的支持。

    

    本文介绍了一种用于分析高维数据集的新型正则化鲁棒 $\tau$-回归估计器，以应对响应变量和协变量的严重污染。我们称这种估计器为自适应 $\tau$-Lasso，它对异常值和高杠杆点具有鲁棒性，同时采用自适应 $\ell_1$-范数惩罚项来减少真实回归系数的偏差。具体而言，该自适应 $\ell_1$-范数惩罚项为每个回归系数分配一个权重。对于固定数量的预测变量 $p$，我们显示出自适应 $\tau$-Lasso 具有变量选择一致性和真实支持下回归向量的渐近正态性的最优性质，假定已知真实回归向量的支持。然后我们通过有限样本断点和影响函数来表征其健壮性。我们进行了广泛的模拟来比较不同的估计器的性能。

    This paper introduces a new regularized version of the robust $\tau$-regression estimator for analyzing high-dimensional data sets subject to gross contamination in the response variables and covariates. We call the resulting estimator adaptive $\tau$-Lasso that is robust to outliers and high-leverage points and simultaneously employs adaptive $\ell_1$-norm penalty term to reduce the bias associated with large true regression coefficients. More specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\tau$-Lasso has the oracle property with respect to variable-selection consistency and asymptotic normality for the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We then characterize its robustness via the finite-sample breakdown point and the influence function. We carry-out extensive simulations to compare the per
    
[^176]: 一种可扩展的序列转移优化问题生成器

    A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])

    [http://arxiv.org/abs/2304.08503](http://arxiv.org/abs/2304.08503)

    STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。

    

    近年来，序列转移优化(STO)受到越来越多的研究关注，旨在利用储存在数据库中以前求解的优化任务的知识来提高优化性能。然而，尽管算法设计已有重大进展，但STO中的测试问题设计并不完善。它们往往是由其他基准函数随机组合而成，这些基准函数具有相同的最佳值，或者生成自表现出有限变化的实际问题。这些问题中源任务和目标任务的最优解之间的关系是手动配置的，因此单调，限制了它们表征真实问题多样化关系的能力。因此，许多算法在这些问题上取得的有前途的结果具有高度的偏见，并且难以推广到其他问题。鉴于此，我们首先引入了一些表征STO问题的基本概念。

    Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
    
[^177]: 关于知识图谱中存在性一阶查询推理的研究

    On Existential First Order Queries Inference on Knowledge Graphs. (arXiv:2304.07063v1 [cs.AI])

    [http://arxiv.org/abs/2304.07063](http://arxiv.org/abs/2304.07063)

    本文阐述了关于知识图谱中存在性一阶查询推理的新方法，提出了一个新数据集，并开发了一种来自模糊逻辑理论的新搜索算法，该算法能够解决新公式，并在现有公式中超过以前的方法。

    

    知识图谱推理是一项具有挑战性的任务，因为它利用观察到的信息来预测缺失的信息。特别地，回答一阶逻辑公式是特别感兴趣的，因为它具有清晰的语法和语义。最近，提出了查询嵌入方法，该方法学习了一组实体的嵌入，并将逻辑运算视为集合运算。尽管有很多研究遵循相同的方法，但它缺乏从逻辑角度进行系统检查的方法。在本文中，我们描述了先前研究调查的查询范围，并准确地确定了它与整个存在性公式家族之间的差距。此外，我们还开发了一个包含十个新公式的新数据集，并讨论了同时出现的新挑战。最后，我们提出了一种来自模糊逻辑理论的新搜索算法，该算法能够解决新公式，并在现有公式中超过以前的方法。

    Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Specifically, answering first-order logic formulas is of particular interest because of its clear syntax and semantics. Recently, the query embedding method has been proposed which learns the embedding of a set of entities and treats logic operations as set operations. Though there has been much research following the same methodology, it lacks a systematic inspection from the standpoint of logic. In this paper, we characterize the scope of queries investigated previously and precisely identify the gap between it and the whole family of existential formulas. Moreover, we develop a new dataset containing ten new formulas and discuss the new challenges coming simultaneously. Finally, we propose a new search algorithm from fuzzy logic theory which is capable of solving new formulas and outperforming the previous methods in existing formulas.
    
[^178]: 物理启发的神经网络在利用暗物质重建流体力学模拟中的应用

    Physics-informed neural networks in the recreation of hydrodynamic simulations from dark matter. (arXiv:2303.14090v1 [astro-ph.CO])

    [http://arxiv.org/abs/2303.14090](http://arxiv.org/abs/2303.14090)

    本文介绍了一种新的方法，将物理启发的神经网络应用于重建流体力学模拟中，通过将理论知识注入模型损失函数并结合新的性能评估指标，成功实现了对重子散射的重建。

    

    物理启发的神经网络已经成为一个合理的框架，用于构建将统计模式与领域知识相结合的预测模型。其基本理念是通过已知关系来丰富优化损失函数以限制可能解决方案的空间。水动力学模拟是现代宇宙学的核心组成部分，而所需的计算既昂贵又耗时。与此同时，快速模拟暗物质需要更少的资源，这导致了机器学习算法成为研究的一个活跃领域;在这里，重建流体力学模拟中发现的散射是一个持续的挑战。本文提出了将物理启发的神经网络应用于重建流体力学模拟中的新方法，它结合了神经网络架构的进步和物理约束，将关于重子转化效率的理论注入模型损失函数。我们还介绍了一种新的性能评估指标，基于结果图像中动力学功率谱中的误差，这使得可以量化网络对宇宙学参数推断的适用性。

    Physics-informed neural networks have emerged as a coherent framework for building predictive models that combine statistical patterns with domain knowledge. The underlying notion is to enrich the optimization loss function with known relationships to constrain the space of possible solutions. Hydrodynamic simulations are a core constituent of modern cosmology, while the required computations are both expensive and time-consuming. At the same time, the comparatively fast simulation of dark matter requires fewer resources, which has led to the emergence of machine learning algorithms for baryon inpainting as an active area of research; here, recreating the scatter found in hydrodynamic simulations is an ongoing challenge. This paper presents the first application of physics-informed neural networks to baryon inpainting by combining advances in neural network architectures with physical constraints, injecting theory on baryon conversion efficiency into the model loss function. We also in
    
[^179]: 向更好的动态图学习迈进：新的架构和统一库

    Towards Better Dynamic Graph Learning: New Architecture and Unified Library. (arXiv:2303.13047v1 [cs.LG])

    [http://arxiv.org/abs/2303.13047](http://arxiv.org/abs/2303.13047)

    我们提出了一种基于Transformer的新型动态图学习架构DyGFormer，并引入了统一的库DyGLib，以促进可重复、可扩展和可信的动态图学习研究。DyGFormer通过邻居共现编码方案和分块技术实现更长期历史的高效推理，在13个不同领域的数据集上实现了最先进的性能。

    

    我们提出了DyGFormer，这是一种基于Transformer的新型动态图学习架构，仅从节点历史的第一跳交互序列中学习。DyGFormer结合了两种不同的设计：一种邻居共现编码方案，探索源节点和目标节点基于它们的序列的相关性；一种分块技术，将每个序列分成多个块并将其馈送给Transformer，使模型能够有效而高效地受益于更长期的历史。我们还引入了DyGLib，这是一个统一的库，具有标准的训练管道、可扩展的编码接口和综合的评估协议，以促进可重复、可伸缩和可信的动态图学习研究。通过在来自各个领域的13个数据集上执行广泛的实验，进行推导/归纳动态链接预测和动态节点分类任务，我们观察到：DyGFormer在mo上实现了最先进的性能

    We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning that solely learns from the sequences of nodes' historical first-hop interactions. DyGFormer incorporates two distinct designs: a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their sequences; a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing extensive experiments on thirteen datasets from various domains for transductive/inductive dynamic link prediction and dynamic node classification tasks, we observe that: DyGFormer achieves state-of-the-art performance on mo
    
[^180]: EDGI: 内在对称性规划的等变扩散

    EDGI: Equivariant Diffusion for Planning with Embodied Agents. (arXiv:2303.12410v1 [cs.LG])

    [http://arxiv.org/abs/2303.12410](http://arxiv.org/abs/2303.12410)

    EDGI是一种基于模型的强化学习和规划算法，通过等变扩散处理内在对称性，具有更高效的采样和更好的泛化能力，适用于具有内在对称性的机器人操作任务。

    

    内在对称性是时空和排列上的，大多数计划和基于模型的强化学习算法没有考虑这种丰富的几何结构，导致采样效率低和泛化能力弱。本文提出了一种内在对称性规划的等变扩散算法(EDGI), 可用于基于模型的强化学习和规划，并引入一种新的支持多种表示形式的扩散模型。

    Embodied agents operate in a structured world, often solving tasks with spatial, temporal, and permutation symmetries. Most algorithms for planning and model-based reinforcement learning (MBRL) do not take this rich geometric structure into account, leading to sample inefficiency and poor generalization. We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an algorithm for MBRL and planning that is equivariant with respect to the product of the spatial symmetry group $\mathrm{SE(3)}$, the discrete-time translation group $\mathbb{Z}$, and the object permutation group $\mathrm{S}_n$. EDGI follows the Diffuser framework (Janner et al. 2022) in treating both learning a world model and planning in it as a conditional generative modeling problem, training a diffusion model on an offline trajectory dataset. We introduce a new $\mathrm{SE(3)} \times \mathbb{Z} \times \mathrm{S}_n$-equivariant diffusion model that supports multiple representations. We integrate this model i
    
[^181]: 对比学习的视觉-语言模型的测试时分布归一化

    Test-Time Distribution Normalization for Contrastively Learned Vision-language Models. (arXiv:2302.11084v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11084](http://arxiv.org/abs/2302.11084)

    这篇论文介绍了一个针对对比学习的视觉-语言模型的测试时分布归一化的方法，解决了常见的点乘操作导致测试时信息丢失的问题，提高了模型在测试阶段的准确性和效率。

    

    视觉-语言对比学习的进展使得许多下游应用可以通过简单地对图像和文本表示进行点乘来高效准确地进行。最近提出的代表性方法之一是CLIP，由于其有效性已经得到了广泛的采用。CLIP使用InfoNCE损失进行训练，该损失同时考虑了正样本和负样本，以帮助学习更加稳健的表示空间。本文揭示了常见的下游实践——进行点乘仅仅是对优化目标的零阶近似，导致了测试时信息的丢失。直观上，由于模型是基于InfoNCE损失进行优化的，测试时的过程也应该保持一致。问题在于如何以一种计算高效的方式检索到任何负样本信息。为此，我们提出了一种测试时分布归一化的方法

    Advances in the field of vision-language contrastive learning have made it possible for many downstream applications to be carried out efficiently and accurately by simply taking the dot product between image and text representations. One of the most representative approaches proposed recently known as CLIP has garnered widespread adoption due to its effectiveness. CLIP is trained with an InfoNCE loss that takes into account both positive and negative samples to help learn a much more robust representation space. This paper reveals that the common downstream practice of taking a dot product is only a zeroth-order approximation of the optimization goal, resulting in a loss of information during test-time. Intuitively, since the model has been optimized based on the InfoNCE loss, test-time procedures should also be in alignment. The question lies in how one can retrieve any semblance of negative samples information during inference in a computationally efficient way. To this end, we prop
    
[^182]: 基于分段确定性马尔可夫过程的贝叶斯神经网络研究

    Piecewise Deterministic Markov Processes for Bayesian Neural Networks. (arXiv:2302.08724v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08724](http://arxiv.org/abs/2302.08724)

    本文介绍了基于分段确定性马尔可夫过程的贝叶斯神经网络推理方法，通过引入新的自适应稀疏方案，实现了对困难采样问题的加速处理。实验证明，这种方法在计算上可行，并能提高预测准确性、MCMC混合性能，并提供更有信息量的不确定性测量。

    

    现代贝叶斯神经网络（BNNs）的推理通常依赖于变分推断处理，这要求违反了独立性和后验形式的假设。传统的MCMC方法避免了这些假设，但由于无法适应似然的子采样，导致计算量增加。新的分段确定性马尔可夫过程（PDMP）采样器允许子采样，但引入了模型特定的不均匀泊松过程（IPPs），从中采样困难。本研究引入了一种新的通用自适应稀疏方案，用于从这些IPPs中进行采样，并展示了如何加速将PDMPs应用于BNNs推理。实验表明，使用这些方法进行推理在计算上是可行的，可以提高预测准确性、MCMC混合性能，并与其他近似推理方案相比，提供更有信息量的不确定性测量。

    Inference on modern Bayesian Neural Networks (BNNs) often relies on a variational inference treatment, imposing violated assumptions of independence and the form of the posterior. Traditional MCMC approaches avoid these assumptions at the cost of increased computation due to its incompatibility to subsampling of the likelihood. New Piecewise Deterministic Markov Process (PDMP) samplers permit subsampling, though introduce a model specific inhomogenous Poisson Process (IPPs) which is difficult to sample from. This work introduces a new generic and adaptive thinning scheme for sampling from these IPPs, and demonstrates how this approach can accelerate the application of PDMPs for inference in BNNs. Experimentation illustrates how inference with these methods is computationally feasible, can improve predictive accuracy, MCMC mixing performance, and provide informative uncertainty measurements when compared against other approximate inference schemes.
    
[^183]: 一种用于联邦学习的单次经验隐私估计方法

    One-shot Empirical Privacy Estimation for Federated Learning. (arXiv:2302.03098v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03098](http://arxiv.org/abs/2302.03098)

    本论文提出了一种用于联邦学习的单次经验隐私估计方法，可有效进行隐私损失审计，且无需事先了解模型体系结构或训练数据分布，适用于在实践中大规模部署。

    

    不同ially private（DP）算法的隐私估计技术可用于与分析上界进行比较，或在已知分析上界不紧的情况下实验测量隐私损失。但是，现有的隐私审计技术通常对对手做出强烈假设（例如，了解中间模型迭代或训练数据分布），针对特定任务和模型架构进行调整，并需要重新训练模型多次（通常数量级为数千）。这些缺点使得在实践中难以大规模部署此类技术，尤其是在联邦设置中，模型训练可能需要数天或数周。在本研究中，我们提出了一种新的“单次”方法，可以系统地解决这些挑战，在单个训练运行期间高效地审计或估计模型的隐私损失，而不需要事先了解模型体系结构或训练数据分布。我们的方法适用于联邦学习等设置中使用的一般DP算法，并由实验在实际数据集上验证其提供的准确隐私损失估计。

    Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss in settings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions on the adversary (e.g., knowledge of intermediate model iterates or the training data distribution), are tailored to specific tasks and model architectures, and require retraining the model many times (typically on the order of thousands). These shortcomings make deploying such techniques at scale difficult in practice, especially in federated settings where model training can take days or weeks. In this work, we present a novel "one-shot" approach that can systematically address these challenges, allowing efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, and without requiring any a priori knowledge about the mod
    
[^184]: IC3：通过委员会共识进行图像字幕生成

    IC3: Image Captioning by Committee Consensus. (arXiv:2302.01328v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.01328](http://arxiv.org/abs/2302.01328)

    "IC3: Image Captioning by Committee Consensus"引入了一种通过委员会共识生成图像字幕的方法，能够从多个注释者的视角捕捉高层细节，优于单个人生成的参考字幕，并在视觉描述方面取得了显著改进。

    

    如果你请一个人描述一幅图像，他们可能会用一千种不同的方式来描述。传统上，图像字幕生成模型被训练成生成一个“最佳”（与参考最相似）的图像字幕。然而，这样做会鼓励生成“信息贫乏”的字幕，并且只关注可能细节的一个子集，而忽略了场景中其他可能有用的信息。在这项工作中，我们引入了一种简单而新颖的方法："通过委员会共识进行图像字幕生成"（IC3），旨在生成一个能够从多个注释者的视角捕捉到高层细节的单个字幕。人类评价IC3生成的字幕至少与基准SOTA模型一样有帮助的情况占了三分之二以上，并且IC3可以将SOTA自动召回系统的性能提升高达84%，胜过单个人生成的参考字幕，并显示出在视觉描述方面相比于SOTA方法的显著改进。代码可通过https://davidmchan获取。

    If you ask a human to describe an image, they might do so in a thousand different ways. Traditionally, image captioning models are trained to generate a single "best" (most like a reference) image caption. Unfortunately, doing so encourages captions that are "informationally impoverished," and focus on only a subset of the possible details, while ignoring other potentially useful information in the scene. In this work, we introduce a simple, yet novel, method: "Image Captioning by Committee Consensus" (IC3), designed to generate a single caption that captures high-level details from several annotator viewpoints. Humans rate captions produced by IC3 at least as helpful as baseline SOTA models more than two thirds of the time, and IC3 can improve the performance of SOTA automated recall systems by up to 84%, outperforming single human-generated reference captions, and indicating significant improvements over SOTA approaches for visual description. Code is available at https://davidmchan.
    
[^185]: 神经关系图：识别标签噪音和异常数据的统一框架

    Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data. (arXiv:2301.12321v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12321](http://arxiv.org/abs/2301.12321)

    本研究提出了一种利用数据的关系结构识别标签错误和异常数据的统一方法，并提供了可视化工具，在大规模图像、语音和语言领域任务中表现良好。

    

    诊断和清理数据是构建健壮的机器学习系统的关键步骤。但是，由于存在复杂问题，如标签错误、欠表示和异常值，因此在具有真实世界分布的大规模数据集中识别问题具有挑战性。在本文中，我们提出了一种利用特征嵌入空间中数据的关系结构这一被忽视的信息来源，来识别有问题的数据的统一方法。为此，我们提出了基于数据的关系图结构来检测标签错误和异常数据的可扩展和有效的算法。我们进一步引入了一种可视化工具，提供特征嵌入空间中数据点的上下文信息，作为交互式诊断数据的有效工具。我们在大规模图像、语音和语言领域任务中评估了我们方法的标签错误和离群值/分布外（OOD）检测性能。

    Diagnosing and cleaning data is a crucial step for building robust machine learning systems. However, identifying problems within large-scale datasets with real-world distributions is challenging due to the presence of complex issues such as label errors, under-representation, and outliers. In this paper, we propose a unified approach for identifying the problematic data by utilizing a largely ignored source of information: a relational structure of data in the feature-embedded space. To this end, we present scalable and effective algorithms for detecting label errors and outlier data based on the relational graph structure of data. We further introduce a visualization tool that provides contextual information of a data point in the feature-embedded space, serving as an effective tool for interactively diagnosing data. We evaluate the label error and outlier/out-of-distribution (OOD) detection performances of our approach on the large-scale image, speech, and language domain tasks, inc
    
[^186]: 在波形域中使用离散自监督单元进行语音风格转换

    Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units. (arXiv:2212.09730v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2212.09730](http://arxiv.org/abs/2212.09730)

    DISSC是一种新颖、轻量级的语音风格转换方法，它可以以无需文本的方式将录音的节奏、音高轮廓和音色转换为目标说话者的风格。该方法使用自监督模型编码语音为离散单元，具有简单、有效且快速的训练过程，适用于无配对数据的多对多语音转换。

    

    我们介绍了一种名为DISSC的新颖、轻量级的方法，它可以以无需文本的方式将录音的节奏、音高轮廓和音色转换为目标说话者的风格。与DISSC不同，大多数语音转换（VC）方法主要关注音色，并忽略人们独特的说话风格（韵律）。所提出的方法使用预训练的自监督模型将语音编码为离散单元，使得训练简单、有效且快速。所有的转换模块仅在重建任务上进行训练，因此适用于无配对数据的多对多语音转换。我们介绍了一套定量和定性评估指标，并通过实验证明DISSC在这个设置下明显优于评估基线。代码和样例可在https://pages.cs.huji.ac.il/adiyoss-lab/dissc/ 上找到。

    We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people's unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.
    
[^187]: 通过"稳定边缘"学习阈值神经元

    Learning threshold neurons via the "edge of stability". (arXiv:2212.07469v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07469](http://arxiv.org/abs/2212.07469)

    该论文通过对简化的两层神经网络模型的梯度下降进行详细分析，揭示了大学习率下非凸训练动态的稳定边缘现象，并发现了神经网络无法学习阈值样式神经元的临界步长。

    

    现有的神经网络训练分析通常基于极小学习率的不现实假设。与实际智慧和经验研究相反，例如J. Cohen等人的工作（ICLR 2021），展示了惊人的新现象（"稳定边缘"或"不稳定收敛"），以及大学习率体制下的潜在泛化效果。然而，尽管最近有大量关于这个主题的研究，但后一种效应仍然理解有限。本文通过对简化的两层神经网络模型的梯度下降进行详细分析，迈出了理解大学习率下真正非凸训练动态的一步。对于这些模型，我们证明了稳定边缘现象，并发现了一个尖锐的阶跃转变，当步长小于此值时，神经网络无法学习到"阈值样式"神经元（即具有非零第一层偏置的神经元）。

    Existing analyses of neural network training often operate under the unrealistic assumption of an extremely small learning rate. This lies in stark contrast to practical wisdom and empirical studies, such as the work of J. Cohen et al. (ICLR 2021), which exhibit startling new phenomena (the "edge of stability" or "unstable convergence") and potential benefits for generalization in the large learning rate regime. Despite a flurry of recent works on this topic, however, the latter effect is still poorly understood. In this paper, we take a step towards understanding genuinely non-convex training dynamics with large learning rates by performing a detailed analysis of gradient descent for simplified models of two-layer neural networks. For these models, we provably establish the edge of stability phenomenon and discover a sharp phase transition for the step size below which the neural network fails to learn "threshold-like" neurons (i.e., neurons with a non-zero first-layer bias). This elu
    
[^188]: 计算最优神经网络缩放定律的信息论分析

    An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws. (arXiv:2212.01365v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01365](http://arxiv.org/abs/2212.01365)

    我们进行了一项关于大型神经网络的计算最优权衡的研究，并提出了一种信息论上可行的期望误差最小化模型和数据集大小的计算分配方法。

    

    我们研究了大型神经网络模型和训练数据集大小之间的计算最优权衡。我们的结果表明，类似于白鼠实验所支持的线性关系。尽管白鼠实验研究的是基于MassiveText语料库的基于变压器的大型语言模型，我们的研究重点是简化的学习模型和数据生成过程，每个都基于一个具有S型输出单元和单隐藏层ReLU激活单元的神经网络。我们引入了一类算法的一般误差上界，这些算法逐步更新一个统计量（例如梯度下降）。针对受到barron 1993启发的特定学习模型，我们建立了一个上界，该上界是作为模型和数据集大小的函数的信息论上可实现的期望误差的最小值。然后，我们推导出最小化这一上界的计算分配方式。我们展示了一些实证结果，这些结果表明...（待续）

    We study the compute-optimal trade-off between model and training data set sizes for large neural networks. Our result suggests a linear relation similar to that supported by the empirical analysis of chinchilla. While that work studies transformer-based large language models trained on the MassiveText corpus gopher, as a starting point for development of a mathematical theory, we focus on a simpler learning model and data generating process, each based on a neural network with a sigmoidal output unit and single hidden layer of ReLU activation units. We introduce general error upper bounds for a class of algorithms which incrementally update a statistic (for example gradient descent). For a particular learning model inspired by barron 1993, we establish an upper bound on the minimal information-theoretically achievable expected error as a function of model and data set sizes. We then derive allocations of computation that minimize this bound. We present empirical results which suggest 
    
[^189]: 这是一个有限时间随机线性二次控制问题的策略梯度方法的收敛性研究

    Convergence of policy gradient methods for finite-horizon stochastic linear-quadratic control problems. (arXiv:2211.00617v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2211.00617](http://arxiv.org/abs/2211.00617)

    本文研究了有限时间随机线性二次控制问题中策略梯度方法的全局线性收敛性，并提出了基于Fisher几何和Bures-Wasserstein几何的几何感知梯度下降算法，该算法以线性速率全局收敛到最优策略。

    

    本文研究了有限时间连续时间探索性线性二次控制问题中策略梯度方法的全局线性收敛性。该设置包括具有非确定性成本和允许目标中添加额外的熵正则化项的随机线性二次控制问题。我们考虑了一个连续时间的高斯策略，其均值是状态变量的线性函数，方差与状态无关。与离散时间问题相反，策略中的成本函数不是严格凸函数，并且并非所有的下降方向都导致有界的迭代。我们提出了基于Fisher几何和Bures-Wasserstein几何的策略均值和协方差的几何感知梯度下降算法。策略迭代被证明满足先验界，并以线性速率全局收敛到最优策略。我们进一步提出了一种新的具有离散时间策略的PG方法。该算法利用了连续时间的分析结果，并实现了鲁棒的线性收敛。

    We study the global linear convergence of policy gradient (PG) methods for finite-horizon continuous-time exploratory linear-quadratic control (LQC) problems. The setting includes stochastic LQC problems with indefinite costs and allows additional entropy regularisers in the objective. We consider a continuous-time Gaussian policy whose mean is linear in the state variable and whose covariance is state-independent. Contrary to discrete-time problems, the cost is noncoercive in the policy and not all descent directions lead to bounded iterates. We propose geometry-aware gradient descents for the mean and covariance of the policy using the Fisher geometry and the Bures-Wasserstein geometry, respectively. The policy iterates are shown to satisfy an a-priori bound, and converge globally to the optimal policy with a linear rate. We further propose a novel PG method with discrete-time policies. The algorithm leverages the continuous-time analysis, and achieves a robust linear convergence acr
    
[^190]: 强化学习和赌博机在语音和自然语言处理中的应用: 教程，回顾和展望

    Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.13623](http://arxiv.org/abs/2210.13623)

    这篇综述调查了强化学习和赌博机在语音和自然语言处理中的最新进展，讨论了如何有效利用它们解决相关问题，构建适应性、交互性和可扩展性的模型。

    

    最近几年，强化学习和赌博机已经在包括医疗保健，金融，推荐系统，机器人以及语音和自然语言处理等广泛的实际应用中发生了转变。虽然大部分强化学习算法在语音和语言处理领域的应用主要集中于利用其灵活的优化属性提高深度神经网络的训练，但仍有许多研究空间可以利用强化学习的好处，比如基于奖励驱动的适应性、状态表示、时间结构和通用性。在本文中，我们提出了强化学习和赌博机的最新进展综述，并讨论了如何有效地利用它们来解决语音和自然语言处理问题，构建适应性、交互性和可扩展性的模型。

    In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.
    
[^191]: 能否通过脑信号揭示人类语言的内部一致性？

    Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2208.06348](http://arxiv.org/abs/2208.06348)

    本研究探索了脑信号和人类语言之间的关系，并介绍了一种名为MTAM的方法，该方法在情感分析和关系检测等下游应用中取得了新的最先进结果。

    

    脑信号（如脑电图）和人类语言在许多下游任务中被广泛研究，但二者之间的联系尚未得到很好的探索。本研究探讨了脑电图和语言之间的关系和依赖性。在表示层面上，我们引入了一种名为MTAM（Multimodal Transformer Alignment Model）的方法，用于观察这两种模态之间的协调表示。我们使用了多种关系对齐技术，如典型相关分析和Wasserstein距离，作为损失函数来转换特征。在情感分析和关系检测等下游应用中，我们在ZuCo和K-EmoCon两个数据集上实现了新的最先进结果。我们的方法在情感分析方面使K-EmoCon数据集的F1分数提高了1.7％，ZuCo数据集提高了9.3％，在关系检测方面ZuCo数据集提高了7.4％。此外，我们提供了国际上最大的人类类比推理数据集的编码方案。

    Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal \textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide inter
    
[^192]: 寻求数据背后的真相：一种无监督的机器学习方法

    Seeking the Truth Beyond the Data. An Unsupervised Machine Learning Approach. (arXiv:2207.06949v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.06949](http://arxiv.org/abs/2207.06949)

    本文介绍了一种无监督的机器学习方法-聚类，并深入描述了最常用的聚类方法，提供了有关参数选择和初始化的指导。同时，通过比较三个数据集的聚类效率，揭示了不同算法在准确性和复杂性方面的优劣。

    

    聚类是一种无监督的机器学习方法，通过将未标记的元素/对象分组，旨在构建良好建立的聚类，其中元素根据它们的相似性进行分类。该过程的目标是为研究人员提供有用的帮助，帮助他们识别数据中的模式。处理大型数据库时，这些模式可能不容易在没有聚类算法的贡献下检测到。本文深入描述了最常用的聚类方法，并提供了有关适当参数选择和初始化的有用介绍。同时，本文不仅是一篇评论，突出了所检查的聚类技术的主要要素，还通过对3个数据集的基于准确性和复杂性的聚类效率比较，揭示了它们的现有弱点和能力。

    Clustering is an unsupervised machine learning methodology where unlabeled elements/objects are grouped together aiming to the construction of well-established clusters that their elements are classified according to their similarity. The goal of this process is to provide a useful aid to the researcher that will help her/him to identify patterns among the data. Dealing with large databases, such patterns may not be easily detectable without the contribution of a clustering algorithm. This article provides a deep description of the most widely used clustering methodologies accompanied by useful presentations concerning suitable parameter selection and initializations. Simultaneously, this article not only represents a review highlighting the major elements of examined clustering techniques but emphasizes the comparison of these algorithms' clustering efficiency based on 3 datasets, revealing their existing weaknesses and capabilities through accuracy and complexity, during the confront
    
[^193]: PROFHIT: 面向分层时间序列的概率鲁棒预测

    PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series. (arXiv:2206.07940v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07940](http://arxiv.org/abs/2206.07940)

    PROFHIT是一个概率鲁棒的分层时间序列预测模型，能够提供整个层次结构的预测分布，并引入一种新颖的分布一致性正则化方法。

    

    概率性分层时间序列预测是时间序列预测的重要变种，其目标是建模和预测具有分层关系的多变量时间序列。大多数方法关注点预测，并没有提供良好校准的概率预测分布。最近的概率预测方法在点预测和分布样本上也引入了分层关系，但没有考虑预测分布的一致性。之前的工作也默默地假设数据集总是与给定的分层关系一致，并且不适应显示与此假设偏离的真实世界数据集。我们填补了这两个差距，并提出了PROFHIT，这是一个完全概率性的分层预测模型，能够同时建模整个层次结构的预测分布。PROFHIT采用灵活的概率贝叶斯方法，并引入一种新颖的分布一致性正则化方法。

    Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gaps and propose PROFHIT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHIT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulari
    
[^194]: 无关类别的有条件神经过程的6D姿态估计

    Category-Agnostic 6D Pose Estimation with Conditional Neural Processes. (arXiv:2206.07162v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.07162](http://arxiv.org/abs/2206.07162)

    该论文提出了一种无关类别的6D姿态估计方法，通过神经过程元学习来捕捉对象的纹理和几何形状，并使用几何感知解码器考虑对象的几何约束进行关键点预测。

    

    我们提出了一种新颖的元学习方法，用于未知物体的6D姿态估计。与“实例级”和“类别级”姿态估计方法相比，我们的算法以无关类别的方式学习对象表示，使其具有强大的跨对象类别的泛化能力。具体地，我们采用基于神经过程的元学习方法，通过很少的RGB-D图像和真实关键点来训练编码器捕捉对象的纹理和几何形状，并得到一个潜在表示。然后，潜在表示被同时元训练的解码器用于预测新图像中物体的6D姿态。此外，我们提出了一种针对关键点预测的新颖几何感知解码器，使用图神经网络（GNN）明确考虑到每个对象的几何约束。为了评估我们的算法，在linemod数据集和我们的新的完全注释数据集上进行了大量实验。

    We present a novel meta-learning approach for 6D pose estimation on unknown objects. In contrast to ``instance-level" and ``category-level" pose estimation methods, our algorithm learns object representation in a category-agnostic way, which endows it with strong generalization capabilities across object categories. Specifically, we employ a neural process-based meta-learning approach to train an encoder to capture texture and geometry of an object in a latent representation, based on very few RGB-D images and ground-truth keypoints. The latent representation is then used by a simultaneously meta-trained decoder to predict the 6D pose of the object in new images. Furthermore, we propose a novel geometry-aware decoder for the keypoint prediction using a Graph Neural Network (GNN), which explicitly takes geometric constraints specific to each object into consideration. To evaluate our algorithm, extensive experiments are conducted on the \linemod dataset, and on our new fully-annotated s
    
[^195]: INSTA-BNN: 具有实例感知阈值的二值神经网络

    INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold. (arXiv:2204.07439v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.07439](http://arxiv.org/abs/2204.07439)

    本研究提出了一种名为具有实例感知阈值的二值神经网络（INSTA-BNN）的新颖设计，它根据输入相关信息动态调整量化阈值，并通过精心优化，实现了最小的设备开销。实验结果表明，在ImageNet分类任务上，INSTA-BNN比基线方法性能提升了3.0%和2.8%。

    

    二值神经网络（BNN）已成为降低深度神经网络内存占用和计算成本的有前途的解决方案，但由于激活和权重受限于二进制值，导致质量下降。为了弥补准确性下降，我们提出了一种新颖的BNN设计，称为具有实例感知阈值的二值神经网络（INSTA-BNN），它以输入相关或实例感知的方式动态地控制量化阈值。根据我们的观察，高阶统计量可以作为估计输入分布特征的代表性指标。INSTA-BNN被设计为根据各种信息（包括高阶统计量）动态调整阈值，但也经过精心优化，以实现在实际设备上的最小开销。我们进行了广泛的研究表明，INSTA-BNN在ImageNet分类任务上的性能优于基线3.0%和2.8%。

    Binary Neural Networks (BNNs) have emerged as a promising solution for reducing the memory footprint and compute costs of deep neural networks, but they suffer from quality degradation due to the lack of freedom as activations and weights are constrained to the binary values. To compensate for the accuracy drop, we propose a novel BNN design called Binary Neural Network with INSTAnce-aware threshold (INSTA-BNN), which controls the quantization threshold dynamically in an input-dependent or instance-aware manner. According to our observation, higher-order statistics can be a representative metric to estimate the characteristics of the input distribution. INSTA-BNN is designed to adjust the threshold dynamically considering various information, including higher-order statistics, but it is also optimized judiciously to realize minimal overhead on a real device. Our extensive study shows that INSTA-BNN outperforms the baseline by 3.0% and 2.8% on the ImageNet classification task with compa
    
[^196]: 基于示例的超网络用于领域外泛化

    Example-based Hypernetworks for Out-of-Distribution Generalization. (arXiv:2203.14276v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.14276](http://arxiv.org/abs/2203.14276)

    本文提出了一个基于示例的超网络框架，利用多个源领域的标记数据来进行领域外泛化。该框架通过生成输入示例的唯一签名，并将其嵌入源领域的语义空间中，并利用超网络生成任务分类器的权重。实验结果表明，该方法在29个适应场景中表现优于现有算法，且在输入示例的表示上具有丰富性。同时，与少样本GPT-3进行了比较，证明了其有效性。

    

    随着自然语言处理(NLP)算法不断突破新的里程碑，领域外泛化仍然是一个重大挑战。本文解决了对于陌生领域的多源适应问题：我们利用来自多个源领域的标记数据，在训练中泛化到未知目标领域。我们的创新性框架采用基于示例的超网络适应：一个T5编码-解码器首先从输入示例中生成一个唯一的签名，并将其嵌入到源领域的语义空间中。然后，这个签名被一个超网络利用来生成任务分类器的权重。我们在29种适应场景中评估了我们的方法，涉及情感分类和自然语言推理两个任务，在这些场景中，我们的方法超过了已有算法。在高级版本中，签名还丰富了输入示例的表示。我们还将我们的微调架构与少样本GPT-3进行了比较，证明了其有效性。

    As Natural Language Processing (NLP) algorithms continually achieve new milestones, out-of-distribution generalization remains a significant challenge. This paper addresses the issue of multi-source adaptation for unfamiliar domains: We leverage labeled data from multiple source domains to generalize to unknown target domains at training. Our innovative framework employs example-based Hypernetwork adaptation: a T5 encoder-decoder initially generates a unique signature from an input example, embedding it within the source domains' semantic space. This signature is subsequently utilized by a Hypernetwork to generate the task classifier's weights. We evaluated our method across two tasks - sentiment classification and natural language inference - in 29 adaptation scenarios, where it outpaced established algorithms. In an advanced version, the signature also enriches the input example's representation. We also compare our finetuned architecture to few-shot GPT-3, demonstrating its effectiv
    
[^197]: 关系式自监督学习

    Relational Self-Supervised Learning. (arXiv:2203.08717v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.08717](http://arxiv.org/abs/2203.08717)

    本文介绍了一种关系式自监督学习（ReSSL）框架，通过建模不同实例之间的关系来学习视觉表示，以弥补当前自监督学习方法中对不同实例关系的缺乏关注。

    

    自监督学习（SSL），包括主流的对比学习，在没有数据注释的情况下取得了很大的成功，可以学习视觉表示。然而，大多数方法主要关注实例级信息（即，相同实例的不同增强图像应具有相同的特征或聚集到相同的类别），但对不同实例之间的关系缺乏关注。在本文中，我们介绍了一种新的自监督学习范式，即关系式自监督学习（ReSSL）框架，通过建模不同实例之间的关系来学习表示。具体而言，我们提出的方法利用不同实例之间的成对相似性的锐化分布作为“关系”度量，然后利用该度量匹配不同增强的特征嵌入。为了提高性能，我们认为弱增强对于表示更可靠的关系很重要，并利用动量技术来实现。

    Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most methods mainly focus on the instance level information (\ie, the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduce a novel SSL paradigm, which we term as relational self-supervised learning (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as \textit{relation} metric, which is thus utilized to match the feature embeddings of different augmentations. To boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum s
    
[^198]: 深度判别到核生成网络的定标推断方法

    Deep Discriminative to Kernel Generative Networks for Calibrated Inference. (arXiv:2201.13001v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.13001](http://arxiv.org/abs/2201.13001)

    该论文提出了将判别网络转换为生成网络的方法，用高斯核替换多面体中的仿射函数来生成模型，解决了内部和外部数据校准问题，并在 CIFAR-10，CIFAR-100 和 SVHN 等基准数据集上测试了方法的有效性。

    

    判别与生成网络在人工智能和自然智能的研究中都有其重要性，我们提出了一种将二者相结合的方法，将深度判别网络转换为核生成网络。我们将深度模型视为广义的划分规则，并使用高斯核替换由训练数据构成的多面体中的仿射函数，来获得生成模型。实验证明了我们方法的有效性。

    The fight between discriminative versus generative goes deep, in both the study of artificial and natural intelligence. In our view, both camps have complementary values. So, we sought to synergistically combine them. Here, we propose a methodology to convert deep discriminative networks to kernel generative networks. We leveraged the fact that deep models, including both random forests and deep networks, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as generalized partitioning rules. We replace the affine function in each polytope populated by the training data with Gaussian kernel that results in a generative model. Theoretically, we derive the conditions under which our generative models are a consistent estimator of the corresponding class conditional density. Moreover, our proposed models obtain well calibrated posteriors for in-distribution, and extrapolate beyond the training data to handle out-of-distrib
    
[^199]: 具有线性阈值激活的神经网络：结构与算法

    Neural networks with linear threshold activations: structure and algorithms. (arXiv:2111.08117v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08117](http://arxiv.org/abs/2111.08117)

    本文研究了具有线性阈值激活函数的神经网络。我们确定了这类神经网络可以表示的函数的特点，并发现使用两个隐藏层可以表示该类中的任何函数。我们还给出了表示这类函数所需神经网络大小的界限，并设计了一个能够解决具有固定架构的这类神经网络的经验风险最小化问题的算法。该算法的运行时间与数据样本大小呈多项式关系，而输入维度和网络架构的大小被认为是固定常数。

    

    本文中我们介绍了关于具有线性阈值激活函数的神经网络的新结果。我们精确地描述了可以由这样的神经网络表示的函数类，并且证明了用2个隐藏层表示该类中的任何可表示函数既是必要的也是充分的。这是一个令人惊讶的结果，考虑到最近使用其他流行激活函数如修正线性单元（ReLU）进行神经网络精确可表示性研究。我们还给出了表示该类中任何函数所需的神经网络大小的精确界限。最后，我们设计了一种算法来解决具有固定架构的这些神经网络的经验风险最小化（ERM）问题，以全局最优性。如果将输入维度和网络架构的大小视为固定常数，则该算法的运行时间是多项式时间。该算法在任何情况下都可使用。

    In this article we present new results on neural networks with linear threshold activation functions. We precisely characterize the class of functions that are representable by such neural networks and show that 2 hidden layers are necessary and sufficient to represent any function representable in the class. This is a surprising result in the light of recent exact representability investigations for neural networks using other popular activation functions like rectified linear units (ReLU). We also give precise bounds on the sizes of the neural networks required to represent any function in the class. Finally, we design an algorithm to solve the empirical risk minimization (ERM) problem to global optimality for these neural networks with a fixed architecture. The algorithm's running time is polynomial in the size of the data sample, if the input dimension and the size of the network architecture are considered fixed constants. The algorithm is unique in the sense that it works for any
    
[^200]: 在设备上高效通信的机器学习：非IID私有数据下的联邦蒸馏与增强

    Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data. (arXiv:1811.11479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1811.11479](http://arxiv.org/abs/1811.11479)

    本论文提出了一种在设备上高效通信的机器学习方法，通过联邦蒸馏和增强解决了模型大小和非IID数据的问题。结果表明，与传统的联邦学习相比，该方法在减少通信开销的同时，仍能达到较高的测试准确性。

    

    设备上的机器学习（ML）使训练过程能够利用大量用户生成的私有数据样本。为了享受这一优势，应该尽量减少设备间的通信开销。为此，我们提出了联邦蒸馏（FD），这是一种分布式模型训练算法，其通信负载大小远小于基准方案联邦学习（FL），特别是当模型大小很大时。此外，用户生成的数据样本在设备之间往往不是独立同分布（IID），这常常降低性能与IID数据集相比的性能。为了应对这个问题，我们提出了联邦增强（FAug），在这种方法中，每个设备都共同训练一个生成模型，从而增强其本地数据以产生IID数据集。实证研究表明，FD与FAug相比FL，通信开销减少了约26倍，同时实现了95-98%的测试准确性。

    On-device machine learning (ML) enables the training process to exploit a massive amount of user-generated private data samples. To enjoy this benefit, inter-device communication overhead should be minimized. With this end, we propose federated distillation (FD), a distributed model training algorithm whose communication payload size is much smaller than a benchmark scheme, federated learning (FL), particularly when the model size is large. Moreover, user-generated data samples are likely to become non-IID across devices, which commonly degrades the performance compared to the case with an IID dataset. To cope with this, we propose federated augmentation (FAug), where each device collectively trains a generative model, and thereby augments its local data towards yielding an IID dataset. Empirical studies demonstrate that FD with FAug yields around 26x less communication overhead while achieving 95-98% test accuracy compared to FL.
    

