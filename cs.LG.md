# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation](https://arxiv.org/abs/2403.07869) | TeleMoMa 是一种面向移动操作的模块化多功能远程操作系统，通过整合多种人机接口、降低门槛且具有通用性，为移动操作器提供了全身远程操作的解决方案。 |
| [^2] | [Exploring Safety Generalization Challenges of Large Language Models via Code](https://arxiv.org/abs/2403.07865) | 本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。 |
| [^3] | [Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias](https://arxiv.org/abs/2403.07857) | 模型诱导的分布转移可能导致性能、公平性和边缘群体表现的损失，提出了算法修复(AR)框架以通过积极干预实现对历史歧视的补救 |
| [^4] | [Distilling the Knowledge in Data Pruning](https://arxiv.org/abs/2403.07854) | 在数据剪枝中引入知识蒸馏方法，通过与预先训练的教师网络软预测相结合，实现了在各种数据集、剪枝方法和所有剪枝分数上的显著提升。 |
| [^5] | [12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2403.07851) | 该论文提出了基于轻量级模型的在线少样本类增量学习（O-FSCIL），使用特征正交正则化和多边界损失进行预训练和元学习，在设备上实现每类12毫焦的学习能力。 |
| [^6] | [Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations](https://arxiv.org/abs/2403.07849) | 通过解释增强图神经网络学习，提出了一个迭代自我改进的算法EEGL，通过频繁子图挖掘和过滤来获取节点邻域中特定子图的应用相关特征。 |
| [^7] | [A Machine learning and Empirical Bayesian Approach for Predictive Buying in B2B E-commerce](https://arxiv.org/abs/2403.07843) | 该研究采用了XGBoost和修改版的方法，以解决B2B电子商务中准确预测买家订单下达行为的挑战 |
| [^8] | [Quantifying and Mitigating Privacy Risks for Tabular Generative Models](https://arxiv.org/abs/2403.07842) | 该论文研究了量化和减轻表格生成模型的隐私风险，通过对五种最先进的表格合成器进行实证分析，提出了差分隐私表格潜在扩散模型。 |
| [^9] | [Fusing Climate Data Products using a Spatially Varying Autoencoder](https://arxiv.org/abs/2403.07822) | 该研究提出了一种利用贝叶斯统计框架的可识别和可解释的自编码器，用于融合气候数据产品，其空间变化捕捉了有用的空间模式。 |
| [^10] | [Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling](https://arxiv.org/abs/2403.07818) | 本文研究利用多个数据集进行深度学习超声心动图分割，在处理部分标记数据时采用改进的交叉熵损失函数。 |
| [^11] | [Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815) | Chronos框架通过在固定词汇上训练预训练的概率时间序列模型，在大量数据集上进行了全面基准测试，表现出在训练语料库中的数据集上明显优于其他方法，并且在新数据集上的零样本性能表现可比甚至优于其他方法。 |
| [^12] | [pyvene: A Library for Understanding and Improving PyTorch Models via Interventions](https://arxiv.org/abs/2403.07809) | pyvene是一个开源Python库，支持在PyTorch模型上进行可定制的干预，提供统一和可扩展的框架，用于解释神经模型并与他人分享经过干预的模型。 |
| [^13] | [Boosting keyword spotting through on-device learnable user speech characteristics](https://arxiv.org/abs/2403.07802) | 提出一种新颖的设备本地学习架构，通过学习用户语音特征来增强关键词识别系统，在35类问题的Google Speech Commands数据集中实现了高达19%的错误率降低。 |
| [^14] | [Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data](https://arxiv.org/abs/2403.07797) | jam-pgm机制在合成数据生成中能够联合选择公共数据和私密数据，并且能够在公共数据分布存在偏差的情况下优于其他机制。 |
| [^15] | [DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation](https://arxiv.org/abs/2403.07788) | DexCap是一个可移植的手部动作捕捉系统，结合DexIL算法从人类手部运动数据中训练机器人技能，具有精确追踪和复制人类动作的能力。 |
| [^16] | [FairRR: Pre-Processing for Group Fairness through Randomized Response](https://arxiv.org/abs/2403.07780) | 本文提出了一种名为FairRR的预处理算法，通过在随机响应框架中修改响应变量的最优设计矩阵，直接控制群体公平性的度量，从而产生出色的下游模型效用和公平性。 |
| [^17] | [Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets](https://arxiv.org/abs/2403.07767) | 本研究批判性评估了语音识别数据集中的文本依赖性，揭示了一些机器学习模型可能会过于关注词汇特征而非预期的语音交际特征。 |
| [^18] | [Probabilistic Easy Variational Causal Effect](https://arxiv.org/abs/2403.07745) | 论文提出了一种称为Probabilistic Easy Variational Causal Effect (PEACE)的函数，可以测量X对Y的直接因果效应，适用于连续和离散情况，通过管理概率密度值强度$d\ge 0$来实现干预。 |
| [^19] | [Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs](https://arxiv.org/abs/2403.07743) | 提出了一种专家混合方案，用于在计算病理学系统中检测和排除五种显著的工件，并应用概率阈值处理。 |
| [^20] | [The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels](https://arxiv.org/abs/2403.07735) | HSIC估计的极小化率对平移不变核的独立性度量具有重要意义 |
| [^21] | [CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control](https://arxiv.org/abs/2403.07728) | CAS框架允许在在线选择性预测中控制FCR，通过自适应选择和校准集构造输出符合预测区间 |
| [^22] | [Balancing Fairness and Accuracy in Data-Restricted Binary Classification](https://arxiv.org/abs/2403.07724) | 研究提出了一个框架，直接分析最优贝叶斯分类器在数据限制的情况下的行为，以平衡准确性和公平性。 |
| [^23] | [On the Last-Iterate Convergence of Shuffling Gradient Methods](https://arxiv.org/abs/2403.07723) | 该论文证明了针对目标函数的洗牌梯度方法最后迭代的收敛速率，弥合了在不同设置中最后迭代的良好性能与现有理论之间的差距。 |
| [^24] | [WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?](https://arxiv.org/abs/2403.07718) | 该研究探究了基于大型语言模型的代理在通过web浏览器与软件交互时的能力，提出了WorkArena和BrowserGym两个工具，在29个任务的基准测试中显示出潜力，但也揭示了实现完全任务自动化仍存在挑战。 |
| [^25] | [Fast and Simple Explainability for Point Cloud Networks](https://arxiv.org/abs/2403.07706) | 该方法提出了一种基于特征的解释（FBI）方法，通过计算每个点在瓶颈层之前的特征范数，实现了与当前XAI方法至少三个数量级的速度提升，适用于大型点云或大规模架构。 |
| [^26] | [Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning](https://arxiv.org/abs/2403.07704) | 对称 Q-learning方法通过添加合成噪声来减少贝尔曼误差的偏斜，在在线强化学习中提高了样本效率。 |
| [^27] | [Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons](https://arxiv.org/abs/2403.07688) | 重新评估深度神经网络中的死亡神经元现象，提出了Demon Pruning（DemP）方法，通过控制死亡神经元的产生，动态实现网络稀疏化。 |
| [^28] | [Machine Learning for Soccer Match Result Prediction](https://arxiv.org/abs/2403.07669) | 机器学习在足球比赛结果预测中取得了显著进展，目前最佳表现的模型是梯度提升树模型（如CatBoost）应用于足球特定评分，但仍需进一步比较深度学习模型和随机森林模型在不同数据集上的表现。 |
| [^29] | [Scalable Spatiotemporal Prediction with Bayesian Neural Fields](https://arxiv.org/abs/2403.07657) | 该论文提出了贝叶斯神经场（BayesNF），结合了深度神经网络和分层贝叶斯推断，用于处理大规模时空预测问题。 |
| [^30] | [Harder Tasks Need More Experts: Dynamic Routing in MoE Models](https://arxiv.org/abs/2403.07652) | 通过动态选择专家来提高计算效率和模型性能，针对不同难度的任务激活不同数量的专家，相比传统的Top-K路由方法，我们的动态路由方法在各种基准测试中取得了明显的改进。 |
| [^31] | [Characterization of Large Language Model Development in the Datacenter](https://arxiv.org/abs/2403.07648) | 本研究对大型语言模型的开发工作负载进行了深入特征化研究，发现了与先前任务特定深度学习工作负载的差异，探索了资源利用模式，并提出了优化系统以适应LLMs的潜在机会。 |
| [^32] | [CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability](https://arxiv.org/abs/2403.07632) | 提出了CardioGenAI，一个基于机器学习的框架，用于减少药物的hERG活性并保留药理活性。 |
| [^33] | [generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation](https://arxiv.org/abs/2403.07627) | 提出了一种树-环路方法，通过将束搜索树与各种小部件相结合，提供了可视化和交互可能性，从而分析、解释和调整生成的输出。 |
| [^34] | [Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning](https://arxiv.org/abs/2403.07611) | 该论文介绍了一种新颖的机器遗忘算法，分别采用部分失忆式遗忘和逐层部分更新的方法，以更高效地在训练模型中删除知识。 |
| [^35] | [Couler: Unified Machine Learning Workflow Optimization in Cloud](https://arxiv.org/abs/2403.07608) | 设计并实现了Couler系统，用于云中统一机器学习工作流优化，主要见解在于能够使用自然生成ML工作流 |
| [^36] | [Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation](https://arxiv.org/abs/2403.07605) | 提出NegOpt方法，通过监督微调和强化学习优化负面提示的生成，显著提高图像生成质量，超越其他方法并构建了负面提示数据集。 |
| [^37] | [ProPML: Probability Partial Multi-label Learning](https://arxiv.org/abs/2403.07603) | ProPML是一种新颖的概率方法，用于扩展二元交叉熵到Partial Multi-label Learning（PML）设置中，可以应用于任何深度架构，并且在人工和真实数据集上的实验证明其优于现有方法，尤其适用于候选集中高噪声的情况。 |
| [^38] | [Robustifying and Boosting Training-Free Neural Architecture Search](https://arxiv.org/abs/2403.07591) | 提出了RoBoT算法，旨在解决无训练神经架构搜索中指标估计不准确和性能限制的问题 |
| [^39] | [Visual Privacy Auditing with Diffusion Models](https://arxiv.org/abs/2403.07588) | 在这项研究中，通过使用扩散模型进行重建攻击，作者发现在DP-SGD下，真实世界的数据先验对于重建成功具有显著影响。 |
| [^40] | [Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments](https://arxiv.org/abs/2403.07586) | 本研究提出了在模拟家庭环境中进行联邦学习的新方法，旨在评估机器人行为的社会适宜性，并结合持续学习方法，使机器人可以从彼此的经验中学习社会规范。 |
| [^41] | [Communication Optimization for Distributed Training: Architecture, Advances, and Opportunities](https://arxiv.org/abs/2403.07585) | 本文介绍了分布式深度神经网络训练的通信优化架构，并对并行化策略、集体通信库和网络关系进行了分析，总结了当前的研究进展。 |
| [^42] | [Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)](https://arxiv.org/abs/2403.07573) | 本文提出了可适应性CNC（ACNC）的概念，作为一种自主的机器学习（ML）辅助机制，旨在联合编排计算和网络资源，满足对动态和大量用户请求的严格要求。 |
| [^43] | [Exploring Challenges in Deep Learning of Single-Station Ground Motion Records](https://arxiv.org/abs/2403.07569) | 本研究旨在评估深度学习模型从场地运动记录中学习的效果，并探讨辅助信息对此过程的影响。 |
| [^44] | [Learning Generalizable Feature Fields for Mobile Manipulation](https://arxiv.org/abs/2403.07563) | 提出了GeFF（通用特征场），作为导航和操作的统一表示，可以实时执行，通过将生成的丰富场景先验与自然语言对齐来提高效果。 |
| [^45] | [A Flexible Cell Classification for ML Projects in Jupyter Notebooks](https://arxiv.org/abs/2403.07562) | 本文提出了一种基于混合分类方法的更灵活的单元分类方法，结合了基于规则和决策树分类器。我们讨论了设计原理，并详细描述了开发的分类器。 |
| [^46] | [Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding](https://arxiv.org/abs/2403.07559) | 提出了Ensembling Prioritized Hybrid Policies (EPH)方法，通过选择性通信模块和三种高级推理策略，提高了基于通信的多智能体路径规划解决方案的性能。 |
| [^47] | [SIFiD: Reassess Summary Factual Inconsistency Detection with LLM](https://arxiv.org/abs/2403.07557) | 本研究重新评估了使用LLM进行摘要不一致性检测的方法，提出了SIFiD（带有过滤文档的摘要不一致性检测），旨在通过自然语言推理或测量语义相似性来识别文档中的关键句子。 |
| [^48] | [Online Continual Learning For Interactive Instruction Following Agents](https://arxiv.org/abs/2403.07548) | 我们提出了针对具身代理的两种持续学习设置：学习新行为和新环境。同时，我们通过自信度得分来更新存储的信息，从而避免需要任务边界信息的问题。 |
| [^49] | [A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions](https://arxiv.org/abs/2403.07542) | 视觉Transformer模型在自动驾驶中的成功应用表明其在全局上下文捕捉方面的优势，对于实时、动态视觉场景处理具有关键意义。 |
| [^50] | [LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes](https://arxiv.org/abs/2403.07536) | LaB-GATr 是一种几何代数变换器神经网络，通过序列压缩和插值有效地学习大规模生物医学表面和体积网格，扩展了传统的 GATr 方法并尊重了欧几里得对称性，达到了最先进的结果。 |
| [^51] | [Physics-Transfer Learning for Material Strength Screening](https://arxiv.org/abs/2403.07526) | 提出了使用物理迁移框架来学习晶体塑性物理并从原子模拟中预测Peierls应力的方法。 |
| [^52] | [Reconstructions of Jupiter's magnetic field using physics informed neural networks](https://arxiv.org/abs/2403.07507) | 本研究提出了一种利用物理信息神经网络重建木星内部磁场的新方法，相比其他方法，能够更清晰地解析局部结构并避免深度噪声干扰。 |
| [^53] | [Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach](https://arxiv.org/abs/2403.07503) | 首次提出了从受约束强化学习的视角全球首次提供混合动力车辆的受约束最优燃料消耗的数学表达，并首次利用两种主流的受约束强化学习方法来获得车辆在电池电气平衡条件下的最小燃料消耗。 |
| [^54] | [Detecting Security-Relevant Methods using Multi-label Machine Learning](https://arxiv.org/abs/2403.07501) | 该论文介绍了一种使用多标签机器学习方法检测安全相关方法的插件，可以自动生成静态分析工具配置，运行分析并显示结果。 |
| [^55] | [Signed graphs in data sciences via communicability geometry](https://arxiv.org/abs/2403.07493) | 提出了符号图的可通信性几何概念，证明了其度量是欧几里德的和球形的，然后应用于解决符号图数据分析中的多个问题。 |
| [^56] | [XpertAI: uncovering model strategies for sub-manifolds](https://arxiv.org/abs/2403.07486) | XpertAI是一个框架，可以将预测策略解开为多个特定范围的子策略，并允许将模型的查询制定为这些子策略的线性组合。 |
| [^57] | [PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates](https://arxiv.org/abs/2403.07485) | PMBO通过多项式模型逼近和贝叶斯优化结合，相比于传统贝叶斯优化表现更好，对相关函数和超参数设置更加鲁棒，与进化算法性能相当。 |
| [^58] | [A Deep Learning Approach to Diabetes Diagnosis](https://arxiv.org/abs/2403.07483) | 采用深度学习方法，提出一种无创糖尿病诊断方法，通过反向传播神经网络和数据平衡技术，在准确性、敏感性和特异性方面取得显著改进 |
| [^59] | [Towards Graph Foundation Models for Personalization](https://arxiv.org/abs/2403.07478) | 本文提出了一种面向个性化的基于图的基础建模方法，其中的Heterogeneous GNN旨在捕捉跨多种可推荐项目类型的多跳内容和消费关系。 |
| [^60] | [Imbalance-aware Presence-only Loss Function for Species Distribution Modeling](https://arxiv.org/abs/2403.07472) | 本研究通过使用平衡存在-仅损失函数在公民科学数据集上训练深度学习模型来解决物种分布模型中存在的类别不平衡问题，取得了比传统方法更好的性能表现。 |
| [^61] | [On the nonconvexity of some push-forward constraints and its consequences in machine learning](https://arxiv.org/abs/2403.07471) | 本文提供了关于推进约束的非凸性的理论见解，并展示了这对相关学习问题的影响。 |
| [^62] | [One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices](https://arxiv.org/abs/2403.07465) | 介绍了一种用于嵌入式设备的轻量级 CFA 方法 RAGE，可以检测代码重用攻击（CRA），包括控制和非控制数据攻击，有效提取一个执行跟踪的特征，并利用无监督图神经网络（GNN）识别攻击。 |
| [^63] | [Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index](https://arxiv.org/abs/2403.07460) | 本研究通过对集成方法和事件时间分析模型进行比较，证明了集成方法可以提高预测准确性并增强预测性能的稳健性 |
| [^64] | [A tutorial on multi-view autoencoders using the multi-view-AE library](https://arxiv.org/abs/2403.07456) | 提出了一个统一的多视图自编码器数学框架，整合了各种公式，并拓展了 \texttt{multi-view-AE} 库的文档和功能。 |
| [^65] | [Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings](https://arxiv.org/abs/2403.07454) | 使用结构混合概率分布提供了准确的后验推断，同时具有更小的计算占用量，相较于现有的基于神经网络的SBI方法。 |
| [^66] | [Ab-initio variational wave functions for the time-dependent many-electron Schr\"odinger equation](https://arxiv.org/abs/2403.07447) | 提出一种针对费米子时间相关波函数的变分方法，通过捕捉多体相关性超越平均场近似，可以解决实时演化非平衡量子电子系统的挑战。 |
| [^67] | [Proxy Methods for Domain Adaptation](https://arxiv.org/abs/2403.07442) | 该论文研究了针对域自适应问题的代理方法，利用近端因果学习技术估计因果效应，在不恢复或建模潜在变量的情况下通过代理变量实现了对分布转移的适应。 |
| [^68] | [Knowledge Transfer across Multiple Principal Component Analysis Studies](https://arxiv.org/abs/2403.07431) | 提出了一种跨多个主成分分析研究的知识转移算法，通过整合多个研究中共享的子空间信息来增强目标PCA任务的估计准确性。 |
| [^69] | [Learning-Augmented Algorithms with Explicit Predictors](https://arxiv.org/abs/2403.07413) | 该论文主要探讨了在在线问题中采用具有显式预测器的学习增强算法，通过允许预测器随着输入的增加进行学习，旨在设计出专门针对特定算法任务的在线学习算法。 |
| [^70] | [Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning](https://arxiv.org/abs/2403.07404) | 早期退出网络在持续学习中展现出降低遗忘和在资源利用上表现优异的特点 |
| [^71] | [SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models](https://arxiv.org/abs/2403.07384) | S2L提出了一种通过总结小模型的训练轨迹，来指导大型语言模型数据选择的方法，显著提高了数学问题解决中监督微调的数据效率，并在数据集性能上表现优异。 |
| [^72] | [Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends](https://arxiv.org/abs/2403.07379) | 分析神经网络和LLMs中优化轨迹的复杂性，揭示了优化过程中的关键特征，包括方向探索和方向正则化。 |
| [^73] | [SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression](https://arxiv.org/abs/2403.07378) | SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。 |
| [^74] | [Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors](https://arxiv.org/abs/2403.07366) | 研究显示熵作为测试时适应性的置信度度量在偏倚场景下不可靠，本文提出了一种新的测试时适应性方法DeYO，利用Pseudo-Label Probability Difference（PLPD）作为置信度度量 |
| [^75] | [Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning](https://arxiv.org/abs/2403.07362) | 该论文从对抗的角度提出了一种新的机器遗忘评估方法，通过确定最具挑战性的数据子集，即最坏情况遗忘集，来增强对影响擦除的挑战。 |
| [^76] | [Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning](https://arxiv.org/abs/2403.07356) | 结合大型语言模型和图像生成模型，在持续学习中利用预感来预测数据变化，为监督预训练提供了新的途径。 |
| [^77] | [Graph Unlearning with Efficient Partial Retraining](https://arxiv.org/abs/2403.07353) | 提出了一种新颖的图去除框架GraphRevoker，通过图属性感知划分和图对比子模型聚合，更好地保持了不可训练GNNs的模型效用。 |
| [^78] | [IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers](https://arxiv.org/abs/2403.07339) | 本研究旨在验证在移除低位宽限制时，对于各种Transformer-based模型，整数是否足以满足所有GEMM需求（训练和推断阶段），并且可以与浮点数相媲美，而无需复杂技巧。 |
| [^79] | [Unknown Domain Inconsistency Minimization for Domain Generalization](https://arxiv.org/abs/2403.07329) | 该论文提出了一种名为未知领域不一致性最小化（UDIM）的方法，通过降低源领域和未知领域之间的损失景观不一致性来改善领域泛化。 |
| [^80] | [A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models](https://arxiv.org/abs/2403.07322) | 通过一个问题中心的多专家对比学习框架，提高深度序列知识追踪模型的准确性和可解释性，解决了知识追踪中个体问题信息建模和模型预测结果解释的重要挑战 |
| [^81] | [Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding](https://arxiv.org/abs/2403.07320) | 格点变换编码（LTC）通过在潜空间中采用格点量化，实现了神经压缩中接近速率失真极限的优化。 |
| [^82] | [Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) for Improved User Engagement](https://arxiv.org/abs/2403.07314) | 该研究提出了一种定制化头像系统CADyFACE，通过FACS标记的动态面部表情，以及一种新颖的神经网络BeCoME-Net来量化用户对刺激的面部反应。 |
| [^83] | [Knowledge Graph Large Language Model (KG-LLM) for Link Prediction](https://arxiv.org/abs/2403.07311) | 该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。 |
| [^84] | [How does promoting the minority fraction affect generalization? A theoretical study of the one-hidden-layer neural network on group imbalance](https://arxiv.org/abs/2403.07310) | 本文通过高斯混合模型量化了群体不平衡对样本复杂性、收敛速率和平均以及群体级测试性能的影响，首次提供了ERM在群体级泛化的理论分析。 |
| [^85] | [Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer](https://arxiv.org/abs/2403.07309) | 该研究提出了POSNEGDM框架，利用变压器模型和反馈强化器，在败血症治疗中取得显著改进，将患者生存率提高至97.39％，明显优于传统算法。 |
| [^86] | [Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees](https://arxiv.org/abs/2403.07308) | 提出了一种使用验证辅助学习神经网络屏障函数的终止保证方法。 |
| [^87] | [Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation](https://arxiv.org/abs/2403.07300) | 通过跨模态知识蒸馏和LLMs对齐框架，该方法利用静态和动态知识，充分释放LLMs在时间序列预测中的潜力 |
| [^88] | [Graph Data Condensation via Self-expressive Graph Structure Reconstruction](https://arxiv.org/abs/2403.07294) | 通过自表达图结构重建的方法解决了图数据压缩中的问题 |
| [^89] | [A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism](https://arxiv.org/abs/2403.07283) | 介绍了一种名为CypherTalk的成本效益和自适应的LLM摇晃调整和恢复机制，通过优化摇晃操作符设置，实现了在成本、模型效用和隐私之间权衡的结果。 |
| [^90] | [Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling](https://arxiv.org/abs/2403.07282) | 这项研究引入了灵活的非参数后验抽样方法，命名为非参数迁移学习（NPTL），用以解决在非参数学习背景下的分布转移问题，并提高了迁移学习的性能。 |
| [^91] | [Anderson acceleration for iteratively reweighted $\ell_1$ algorithm](https://arxiv.org/abs/2403.07271) | 提出了一种Anderson加速的IRL1算法，将其收敛结果扩展到非光滑场景，不依赖于Kurdyka-Lojasiewicz条件 |
| [^92] | [Near-Interpolators: Rapid Norm Growth and the Trade-Off between Interpolation and Generalization](https://arxiv.org/abs/2403.07264) | 研究了几乎插值线性回归器的泛化能力，证明了范数增长迅速且插值与泛化之间存在明确的权衡关系。 |
| [^93] | [Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction](https://arxiv.org/abs/2403.07263) | 通过两步形式预测方法，本文实现了自适应边界框不确定性的量化，保证了对象边界框不确定性区间的覆盖率，包括了错误分类的对象，同时确保边界框区间能够适应物体大小，实现更平衡的覆盖率。 |
| [^94] | [Advantage-Aware Policy Optimization for Offline Reinforcement Learning](https://arxiv.org/abs/2403.07262) | 介绍了一种新的适应优势的策略优化（A2PO）方法，用于离线学习，能够解决多行为策略收集的约束冲突问题，有效避免过拟合问题。 |
| [^95] | [Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation](https://arxiv.org/abs/2403.07261) | 通过对抗性数据增强方法，该研究解决了从有限数量策略中学习任务表示的问题，从而将策略与离线任务表示学习分离。 |
| [^96] | [Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication](https://arxiv.org/abs/2403.07255) | 论文提出了一种基于深度学习辅助的并行干扰消除方法，用于在无授权NOMA系统中联合处理活动检测、信道估计和数据检测问题。 |
| [^97] | [GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation](https://arxiv.org/abs/2403.07247) | 该论文提出了一种名为GuideGen的框架，可以根据文本提示联合生成CT图像和腹部器官以及结直肠癌组织掩膜，为医学图像分析领域提供了一种生成数据集的新途径。 |
| [^98] | [Dataset Condensation for Time Series Classification via Dual Domain Matching](https://arxiv.org/abs/2403.07245) | 本文提出了一种名为“Dataset Condensation”的新框架，通过双域匹配能够更有效地利用时间序列数据中的丰富信息。 |
| [^99] | [Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations](https://arxiv.org/abs/2403.07241) | 本文旨在探索如何减少CLIP对伪特征的依赖，从而提高群体鲁棒性，而无需使用注释数据。 |
| [^100] | [Tractable Joint Prediction and Planning over Discrete Behavior Modes for Urban Driving](https://arxiv.org/abs/2403.07232) | 在城市驾驶中，提出了一种可处理的联合预测和规划方法，利用学习的锚点嵌入来参数化高级驾驶行为的离散模式，并实现针对这些离散模式的闭环规划。 |
| [^101] | [Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences](https://arxiv.org/abs/2403.07230) | 提出了一种名为Curry-DPO的方法，在直接偏好优化(DPO)中利用课程学习方法，通过构建多个偏好对来训练模型，相比于标准单一对DPO设置有着更好的性能表现。 |
| [^102] | [LookupFFN: Making Transformers Compute-lite for CPU inference](https://arxiv.org/abs/2403.07221) | 提出了一种名为LookupFFN的替代模块，通过将关键操作重新构建为内存查找，使得基于变压器的前馈网络（FFNs）在CPU推断中变得更轻巧 |
| [^103] | [SoK: Can Trajectory Generation Combine Privacy and Utility?](https://arxiv.org/abs/2403.07218) | 本文提出了一个旨在设计保护隐私的轨迹发布方法的框架，特别强调了选择适当隐私单位的重要性。 |
| [^104] | [Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control](https://arxiv.org/abs/2403.07216) | 使用强化学习训练策略逐步调整四轴飞行器控制器的增益, 显著减小跟踪误差。 |
| [^105] | [Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits](https://arxiv.org/abs/2403.07213) | 该论文提出了一种具有收敛意识的增量时间臂在线模型选择方法，用于在选择最佳模型时平衡任务奖励和探索成本。 |
| [^106] | [Tracking Dynamic Gaussian Density with a Theoretically Optimal Sliding Window Approach](https://arxiv.org/abs/2403.07207) | 通过理论上对滑动窗口高斯核密度估计器进行研究，提供了选择最优权重序列的原则指南，通过实证证据表明该加权方案相比启发式方法具有更好的跟踪性能 |
| [^107] | [A multi-cohort study on prediction of acute brain dysfunction states using selective state space models](https://arxiv.org/abs/2403.07201) | 该研究利用电子健康记录数据开发了用于ICU病人急性脑功能障碍预测的自动化方法，动态预测谵妄、昏迷和死亡，填补了现有文献中的研究空白。 |
| [^108] | [Improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources](https://arxiv.org/abs/2403.07194) | 使用属性选择和集成方法，结合不同多模态数据源，可以改进智能辅导系统中对学生表现的预测能力。 |
| [^109] | [$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model](https://arxiv.org/abs/2403.07191) | 提出了一种$(N,K)$-Puzzle测试平台，用于评估和比较生成语言模型中的强化学习算法。 |
| [^110] | [UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation](https://arxiv.org/abs/2403.07187) | UPS通过跨模态适应训练神经网络，将不同PDE统一到一致的表示空间，并在少样本下达到了强有力的实验结果，优于现有基线，实现了1D和2D数据集上的最先进结果。 |
| [^111] | [Uncertainty in Graph Neural Networks: A Survey](https://arxiv.org/abs/2403.07185) | 本调查旨在全面概述图神经网络中的不确定性，并提出了关于如何识别、量化和利用不确定性来增强模型性能和 GNN 预测可靠性的观点。 |
| [^112] | [Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews](https://arxiv.org/abs/2403.07183) | 该研究提出了一种估计大语料库中被大语言模型大幅修改的文本比例的方法，并在AI会议的同行评审中进行了实证分析，发现6.5%至16.9%的文本可能被LLMs大幅修改，揭示了用户行为的一些见解。 |
| [^113] | [3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs](https://arxiv.org/abs/2403.07179) | 提出了3M-Diffusion，一种新颖的多模态分子图生成方法，可以生成具有所需属性的多样化、理想情况下是新颖的分子。 |
| [^114] | [Don't Forget What I did?: Assessing Client Contributions in Federated Learning](https://arxiv.org/abs/2403.07151) | 提出了一个历史感知的博弈理论框架FLContrib，用来评估联邦学习中的客户贡献。 |
| [^115] | [Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities](https://arxiv.org/abs/2403.07148) | 该论文针对三类变分不等式问题提出了具有随机重排的随机外推法（SEG-RR），并证明其在单调情况下实现了比均匀替换采样SEG更快的收敛速度。 |
| [^116] | [New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production](https://arxiv.org/abs/2403.07143) | 本研究从在线学习的视角研究了重复的委托-代理问题，针对不同情形提出了设计学习算法的不同方法和技术，包括异质代理、同质代理和非单纯视角代理。 |
| [^117] | [One Category One Prompt: Dataset Distillation using Diffusion Models](https://arxiv.org/abs/2403.07142) | 提出了使用扩散模型进行数据集精炼的新方法，有效地解决了数据集精炼在处理高分辨率图像和复杂架构时的可扩展性问题 |
| [^118] | [Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution](https://arxiv.org/abs/2403.07137) | 该论文研究了尼罗尔牛视觉评分与测量值之间的相关性，并提出使用k均值算法对牛的一批进行聚类的新方法。 |
| [^119] | [On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency](https://arxiv.org/abs/2403.07136) | 研究发现基于值函数的表征能力有限，导致在某些情况下基于值函数的方法在统计上低效率，这揭示了价值函数和统计效率之间的关联。 |
| [^120] | [COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization](https://arxiv.org/abs/2403.07134) | 提出了一种名为COMQ的创新后训练量化算法，通过逐层减小重构误差来有效降低大型神经网络的存储要求，同时保持原始准确性。 |
| [^121] | [A New Machine Learning Dataset of Bulldog Nostril Images for Stenosis Degree Classification](https://arxiv.org/abs/2403.07132) | 提出了一种新的狮子狗鼻孔图像数据集，用于狭窄程度分类，同时探索使用深度学习自动推断狭窄程度，在多种神经网络模型下进行了评估 |
| [^122] | [FAX: Scalable and Differentiable Federated Primitives in JAX](https://arxiv.org/abs/2403.07128) | FAX是一个在JAX中嵌入联邦计算原语的库，支持大规模分布式计算，提供了联邦自动微分的实现，并可解释至现有的生产跨设备联邦计算系统。 |
| [^123] | [Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies](https://arxiv.org/abs/2403.07113) | 本研究针对对象检测中前景-前景类别不平衡问题，引入了基于YOLOv5检测器的基准框架，并通过构建COCO-ZIPF数据集和比较分析采样、损失加权等方法，发现它们在单阶段检测器中的效果并不如在双阶段检测器中显著。 |
| [^124] | [A slice classification neural network for automated classification of axial PET/CT slices from a multi-centric lymphoma dataset](https://arxiv.org/abs/2403.07105) | 通过训练ResNet-18网络对淋巴瘤PET/CT图像的切片进行分类，实现对肿瘤切片的自动化识别。 |
| [^125] | [Overcoming the Paradox of Certified Training with Gaussian Smoothing](https://arxiv.org/abs/2403.07095) | 通过使用高斯损失平滑方法，本研究提出了一种结合PGPE算法和不同凸放宽的认证训练方法，可以在训练神经网络时缓解紧凑凸松弛带来的问题，并获得更好性能的网络。 |
| [^126] | [FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning](https://arxiv.org/abs/2403.07094) | FALCON提出了一种面向神经网络剪枝的新型优化框架，同时考虑了模型准确性、FLOPs和稀疏性约束。提出了一种新颖的一阶方法来优化网络剪枝。 |
| [^127] | [A cascaded deep network for automated tumor detection and segmentation in clinical PET imaging of diffuse large B-cell lymphoma](https://arxiv.org/abs/2403.07092) | 该研究提出了一种快速高效的三步级联深度学习模型，用于自动检测和分割DLBCL PET图像中的肿瘤，相比于单一端到端网络，其能够显著提高分割准确性（将3D Dice得分从58.9%提高到78.1%）。 |
| [^128] | [Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning](https://arxiv.org/abs/2403.07078) | 本文调查了如何借助先验知识和认知模型来改善深度学习，以提升对抗防御、可解释性人工智能（XAI）和零样本学习，弥补现有深度学习模型在领域知识利用、对抗性攻击防御、解释性以及在开放环境推理中的性能限制。 |
| [^129] | [Explainable Learning with Gaussian Processes](https://arxiv.org/abs/2403.07072) | 本工作探讨了在高斯过程回归中的特征归因问题，提出了一种基于模型不确定性的定义方法，得到了可解释的特征归因的闭式表达式，同时展示了GPR模型归因也遵循高斯过程分布。 |
| [^130] | [Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models](https://arxiv.org/abs/2403.07066) | 提出了一种新颖的基于重新模拟的自监督学习策略RS3L，通过介入模拟过程并重新模拟事件实现，生成一组涵盖所有物理驱动变化的数据增强，从而促进基础模型的发展，并展示了预训练R3SL在下游任务中表现出强大性能。 |
| [^131] | [Better than classical? The subtle art of benchmarking quantum machine learning models](https://arxiv.org/abs/2403.07059) | 通过开发开源软件包进行大规模研究，研究发现总体而言，开箱即用的经典机器学习模型胜过量子分类器，还发现将量子模型中的纠缠去除通常会导致同样或更好的性能。 |
| [^132] | [Ant Colony Sampling with GFlowNets for Combinatorial Optimization](https://arxiv.org/abs/2403.07041) | 本文提出了生成流蚁群采样器（GFACS），一种结合生成流网络与蚁群优化方法的神经引导元启发式算法，在组合优化任务中表现优于基线ACO算法并与特定问题启发式方法具有竞争力。 |
| [^133] | [All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)](https://arxiv.org/abs/2403.07040) | 本文介绍了一种新颖的多任务提示方法，用于解决预训练图模型与不同任务之间的差距，启发自NLP中提示学习的成功。 |
| [^134] | [Leveraging graph neural networks for supporting Automatic Triage of Patients](https://arxiv.org/abs/2403.07038) | 该研究利用图神经网络为急诊科的患者分诊过程加入了人工智能模块，用于管理患者的急症编码分配，可最大化信息收集并最小化错误。 |
| [^135] | [A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge](https://arxiv.org/abs/2403.07036) | 提出了一种面向边缘设备的低延迟和高能效DNN推断框架，利用“转换”自编码器和轻量级DNN，改进了现有的方法。 |
| [^136] | [Multiple Population Alternate Evolution Neural Architecture Search](https://arxiv.org/abs/2403.07035) | 提出了多种群交替进化神经架构搜索（MPAE）范式，能够在更小的搜索成本下实现神经网络结构的模块多样性。 |
| [^137] | [Interpreting What Typical Fault Signals Look Like via Prototype-matching](https://arxiv.org/abs/2403.07033) | 提出了原型匹配网络（PMN），结合人类固有原型匹配和自编码器（AE），通过匹配特征与原型并选择最相似的原型来解释典型故障信号的特征。 |
| [^138] | [The Cram Method for Efficient Simultaneous Learning and Evaluation](https://arxiv.org/abs/2403.07031) | Cram方法是一种同时学习和评估的高效方法，利用整个样本进行训练和测试，比传统的样本分割策略更高效。 |
| [^139] | [AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation](https://arxiv.org/abs/2403.07030) | 提出了一种名为AuG-KD的方法，通过利用基于锚点的混合生成，解决了无数据知识蒸馏中的知识转移挑战。 |
| [^140] | [An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem](https://arxiv.org/abs/2403.07028) | 该论文提出了一种基于神经网络的解决器，通过引入方向感知技术和监督强化学习，显著缩小了与先进元启发式算法的差距，同时表现出优越的效率。 |
| [^141] | [FWin transformer for dengue prediction under climate and ocean influence](https://arxiv.org/abs/2403.07027) | 本研究利用深度神经网络和FWin转换器对新加坡2000年至2019年气候数据进行长期登革病例预测，发现FWin表现最佳。 |
| [^142] | [Whiteness-based bilevel learning of regularization parameters in imaging](https://arxiv.org/abs/2403.07026) | 提出了一种基于白度的成像反问题中学习正则化参数的双层优化策略，无需地面实况数据。 |
| [^143] | [Enhancing Quantum Variational Algorithms with Zero Noise Extrapolation via Neural Networks](https://arxiv.org/abs/2403.07025) | 本研究利用神经网络进行零噪声外推，以改善量子变分算法在噪声环境中的准确性和可靠性 |
| [^144] | [A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary Modifiable Areal Units](https://arxiv.org/abs/2403.07022) | 本文提出了一种One4All-ST框架，可以仅使用一个模型为任意可修改区域单位进行ST预测，有效解决了多尺度预测的成本问题和预测不一致性。 |
| [^145] | [Adaptive Hyperparameter Optimization for Continual Learning Scenarios](https://arxiv.org/abs/2403.07015) | 本文旨在探讨在持续学习中的超参数选择作用和根据任务复杂性持续自动调整它们的必要性，通过利用序列任务学习特性来提高超参数优化效率，实验证明这种方法可以使超参数优化在不同任务中持续加速。 |
| [^146] | [AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional Mutual Information](https://arxiv.org/abs/2403.07013) | AdaNovo 提出了一个新的框架，通过计算光谱和每个氨基酸/肽段之间的条件互信息，实现了自适应模型训练。 |
| [^147] | [Non-Intrusive Load Monitoring with Missing Data Imputation Based on Tensor Decomposition](https://arxiv.org/abs/2403.07012) | 这项研究提出了基于张量分解的非侵入式负载监测的缺失数据插补方法，通过引入PID控制器和非负更新规则，解决了NILM数据丢失的问题。 |
| [^148] | [AutoEval Done Right: Using Synthetic Data for Model Evaluation](https://arxiv.org/abs/2403.07008) | 提出了用合成数据进行模型评估的方法，通过高效和统计上合理的算法，在GPT-4实验中有效的人工标记样本大小增加了50%。 |
| [^149] | [Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines](https://arxiv.org/abs/2403.07005) | 提出了具有层次结构奖励机器的多智能体强化学习（MAHRM），可以处理更复杂的情况，将任务分解为简单子任务的层次结构，以减少计算复杂性。 |
| [^150] | [Convergence of Some Convex Message Passing Algorithms to a Fixed Point](https://arxiv.org/abs/2403.07004) | 这项研究证明了一些凸消息传递算法会收敛到固定点，并在一定迭代次数内达到特定精度。 |
| [^151] | [Evacuation Management Framework towards Smart City-wide Intelligent Emergency Interactive Response System](https://arxiv.org/abs/2403.07003) | 提出了一种面向智能城市全覆盖智能应急互动响应系统的疏散管理框架，结合人工智能和机器学习技术，旨在改进现有的应急响应系统，以提高居民的公共服务和生活质量。 |
| [^152] | [Survival modeling using deep learning, machine learning and statistical methods: A comparative analysis for predicting mortality after hospital admission](https://arxiv.org/abs/2403.06999) | 该研究进行了使用多种生存分析方法的比较研究，包括传统统计模型和先进的机器学习算法，旨在预测入院后的死亡率。 |
| [^153] | [Physics Sensor Based Deep Learning Fall Detection System](https://arxiv.org/abs/2403.06994) | 本文提出了一个基于物理传感器的深度学习跌倒检测系统TSFallDetect，利用顺序深度学习方法解决跌倒动作预测问题。 |
| [^154] | [Automatic driving lane change safety prediction model based on LSTM](https://arxiv.org/abs/2403.06993) | 通过使用基于深度学习方法的安全敏感深度学习模型，提出了一种基于LSTM的自动驾驶车道变更安全预测模型，旨在提高自动驾驶车辆驾驶安全性。 |
| [^155] | [Phase autoencoder for limit-cycle oscillators](https://arxiv.org/abs/2403.06992) | 该论文介绍了一种用于极限环振荡器的相位自编码器，可以通过训练估计振荡器的渐近相位和相位敏感度函数，以及在原始空间中重建振荡器状态，同时提出了一个简单方法来实现两个振荡器的全局同步。 |
| [^156] | [Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation](https://arxiv.org/abs/2403.06988) | 提出了一种新颖的解码算法DOMINO，在生成文本过程中以完全基于子词对齐的方式强制执行约束，几乎没有性能开销并有时甚至实现近2倍速度提升，远远优于现有方法。 |
| [^157] | [Enhancing Joint Motion Prediction for Individuals with Limb Loss Through Model Reprogramming](https://arxiv.org/abs/2403.06569) | 通过深度学习的重编程特性，研究者成功将原本设计用于健全人的模型改编为适用于截肢者预测关节运动。 |
| [^158] | [DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2403.06397) | DeepSafeMPC是一种基于深度学习的模型预测控制方法，旨在有效预测多智体环境的复杂动态，并应用MARL原则寻找最优解。 |
| [^159] | [Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond](https://arxiv.org/abs/2403.06279) | 本文致力于研究连续时间扩散模型中的熵正则化微调问题，并展示了分析如何扩展到涉及一般$f$-散度正则化的微调中。 |
| [^160] | [Decoupled Data Consistency with Diffusion Purification for Image Restoration](https://arxiv.org/abs/2403.06054) | 通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。 |
| [^161] | [General surgery vision transformer: A video pre-trained foundation model for general surgery](https://arxiv.org/abs/2403.05949) | 该论文开源了迄今为止最大的通用外科视频数据集，提出了用于外科应用的视频预训练通用外科视觉变换器（GSViT）技术，并展示了其在Cholec80阶段注释任务上的优越性能。 |
| [^162] | [SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data](https://arxiv.org/abs/2403.05918) | 基于残差网络的扩散建模方法能够有效处理不平衡数据，克服了经典过采样方法和基于生成网络的模式塌陷与训练不稳定问题。 |
| [^163] | [PR-NET: Leveraging Pathway Refined Network Structures for Prostate Cancer Patient Condition Prediction](https://arxiv.org/abs/2403.05818) | PR-NET模型通过压缩和优化P-NET的网络结构，降低了模型复杂性，同时保持高准确性和可解释性，在前列腺癌患者状况预测中表现出优越性能。 |
| [^164] | [GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM](https://arxiv.org/abs/2403.05527) | GEAR提出了一种高效的KV缓存压缩框架，实现几乎无损的高比率压缩，用于解决大型语言模型推断中因缓存需求增长而导致的记忆绑定问题和性能下降。 |
| [^165] | [Switching the Loss Reduces the Cost in Batch Reinforcement Learning](https://arxiv.org/abs/2403.05385) | 使用对数损失函数来训练适合的Q迭代的批强化学习方法，在实现目标时不产生成本的问题中，其样本数量需求与最优策略的累积成本成比例，能够提供与最优可达成本成比例的“小成本”界限，并在实验中验证在那些最优策略可靠实现目标的问题中，FQI-LOG比使用平方损失训练的FQI使用更少的样本。 |
| [^166] | [Jet Discrimination with Quantum Complete Graph Neural Network](https://arxiv.org/abs/2403.04990) | QCGNN通过量子并行性实现了对喷注判别的多项式加速，为喷注判别问题带来新的解决方案 |
| [^167] | [An Efficient Difference-of-Convex Solver for Privacy Funnel](https://arxiv.org/abs/2403.04778) | 提出了一种针对隐私漏斗方法的高效求解器，能在已知和未知分布条件下均有效地进行求解，并在实验中展示出优于现有方法的性能。 |
| [^168] | [Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology](https://arxiv.org/abs/2403.04558) | 本研究探讨了在计算病理学中减少对比自监督学习复杂性对分类性能的改善，通过利用消费级硬件。 |
| [^169] | [Decentralized and Equitable Optimal Transport](https://arxiv.org/abs/2403.04259) | 本文提出了分散的最优输运问题和分散公平最优输运问题，引入了具有迭代复杂度为O(1/{\epsilon})的单循环分散算法，以匹配现有的集中式一阶方法，并改进了现有集中式算法的迭代复杂度。 |
| [^170] | [Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right Ventricular Volume](https://arxiv.org/abs/2403.03229) | 提出了一种利用监督树核强化集成模型，预测右心室容积的二维超声心动图的方法，并通过不确定性得分增强了预测表现，该方法在小规模数据集上表现出较好的概率和点预测性能。 |
| [^171] | [Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects](https://arxiv.org/abs/2403.02624) | 该论文提出了帕累托最优估计和策略学习的方法，用于确定如何在短期和长期治疗效果之间进行权衡从而实现最佳治疗。 |
| [^172] | [Making Pre-trained Language Models Great on Tabular Prediction](https://arxiv.org/abs/2403.01841) | 提出了一种专门为表格数据预测而预训练的语言模型TP-BERTa，通过新颖的相对大小标记化方法和内部特征关注方法解决了预训练语言模型在数值特征值上的不兼容性问题 |
| [^173] | [In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation](https://arxiv.org/abs/2403.01548) | 本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。 |
| [^174] | [Privacy-Preserving Collaborative Split Learning Framework for Smart Grid Load Forecasting](https://arxiv.org/abs/2403.01438) | 提出了面向智能电网负荷预测的隐私保护协同分裂学习框架，通过将深度神经网络模型分裂为Grid Station（GS）和服务提供商（SP）部分，实现智能电表数据的隐私保护和负载预测的个性化模型训练。 |
| [^175] | [PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase Distribution Systems](https://arxiv.org/abs/2403.00892) | PowerFlowMultiNet是一种专门为不平衡三相功率网格设计的新颖多图GNN框架，能够有效捕捉不平衡网格中的不对称性，并引入了图嵌入机制来捕获电力系统网络内部的空间依赖关系。 |
| [^176] | [Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency](https://arxiv.org/abs/2403.00673) | 提出了快照强化学习（SnapshotRL）框架，通过简单改变环境来增强样本效率，而无需对算法和模型进行任何修改 |
| [^177] | [PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling](https://arxiv.org/abs/2403.00012) | 提出了基于预排序GNN的定时预测方法，包括全局电路预训练、局部时延学习和注意力单元建模，以解决大规模工业电路中的信号衰减和误差累积问题。 |
| [^178] | [Data Interpreter: An LLM Agent For Data Science](https://arxiv.org/abs/2402.18679) | 本研究引入了数据解释器，采用动态规划、工具集成和逻辑错误识别等关键技术，旨在增强数据科学中的问题解决能力。 |
| [^179] | [MMSR: Symbolic Regression is a Multimodal Task](https://arxiv.org/abs/2402.18603) | 符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。 |
| [^180] | [RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences](https://arxiv.org/abs/2402.17257) | RIME是一种针对嘈杂偏好的健壮PbRL算法，通过动态过滤去噪偏好和热启动奖励模型，极大增强了现有最先进PbRL方法的鲁棒性。 |
| [^181] | [Information-based Transductive Active Learning](https://arxiv.org/abs/2402.15898) | ITL是一种基于信息的转导式学习方法，可以在现实世界设置中自适应采样，以最大化关于指定预测目标的信息获取，并在少样本微调和安全贝叶斯优化应用中显著优于最先进技术。 |
| [^182] | [Active Few-Shot Fine-Tuning](https://arxiv.org/abs/2402.15441) | 该论文提出了ITL方法来实现主动少样本微调，通过最大化对下游任务的信息获取，从而在大型神经网络的微调中取得了显著的改进。 |
| [^183] | [Novelty Detection on Radio Astronomy Data using Signatures](https://arxiv.org/abs/2402.14892) | 提出了一个新的半监督框架SigNova，用于检测射电天文数据中的异常值，采用特征转换提取摘要统计信息并计算新颖性评分，以识别偏离预期行为的观察范围。 |
| [^184] | [Transformer tricks: Precomputing the first layer](https://arxiv.org/abs/2402.13388) | 该论文描述了一种加速具有RoPE的transformer推断的技巧，通过预计算第一层来降低延迟和成本，最大节省取决于总层数。 |
| [^185] | [CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples](https://arxiv.org/abs/2402.13254) | 本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。 |
| [^186] | [Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning](https://arxiv.org/abs/2402.12177) | Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。 |
| [^187] | [3D Diffuser Actor: Policy Diffusion with 3D Scene Representations](https://arxiv.org/abs/2402.10885) | 通过策略扩散和3D场景表示相结合，提出了3D Diffuser Actor，一个神经策略架构，可以根据语言指令构建3D视觉场景表示，并对机器人末端执行器的3D旋转和平移进行迭代去噪。 |
| [^188] | [Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations](https://arxiv.org/abs/2402.10433) | 本研究探索了预训练生成采样器在少样本情况下结合MD模拟的方法，以提高蛋白质构象采样的准确性和效率。 |
| [^189] | [Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model](https://arxiv.org/abs/2402.09786) | 这项研究发现了StyleGAN3模型中判别器的病态偏见，它在图像和面部质量上的得分分层影响了不同性别、种族和其他类别的图像。 |
| [^190] | [NetInfoF Framework: Measuring and Exploiting Network Usable Information](https://arxiv.org/abs/2402.07999) | NetInfoF框架提供了一种快速度量和利用节点属性图中的可利用信息的方法，能同时处理链路预测和节点分类任务，并具备理论保证和闭式解的方法。 |
| [^191] | [A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals](https://arxiv.org/abs/2402.05482) | 本研究提出了一种新的非侵入性神经质量评估模型QASE-net，可以有效预测表面肌电信号的信噪比，实验证明其相比之前的模型具有更低的预测误差和更高的线性相关性。 |
| [^192] | [Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving](https://arxiv.org/abs/2402.05359) | 该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。 |
| [^193] | [IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images](https://arxiv.org/abs/2402.03227) | IGUANe是一种三维通用CycleGAN模型，通过集成多个域的训练实现了脑MR图像的多中心协调，使其成为通用生成器。 |
| [^194] | [One-Dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications](https://arxiv.org/abs/2312.16145) | 基于一维结构，该研究提出了一种新的擦除框架，旨在解决现有概念消除方法存在的问题，实现非侵入性、精确性、可定制性和可转移性。 |
| [^195] | [ACPO: AI-Enabled Compiler-Driven Program Optimization](https://arxiv.org/abs/2312.09982) | 该论文提出了ACPO框架，通过机器学习模型提供给LLVM简单全面的工具，以实现编译器驱动的程序优化。 |
| [^196] | [Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods](https://arxiv.org/abs/2312.08531) | 研究了随机梯度方法的最终迭代收敛性，并提出了不需要限制性假设的最优收敛速率问题。 |
| [^197] | [SparQ Attention: Bandwidth-Efficient LLM Inference](https://arxiv.org/abs/2312.04985) | SparQ Attention通过减少注意力块内存带宽需求的技术，从而增加LLMs推理的吞吐量，同时保持模型准确性。 |
| [^198] | [Quantum Inception Score](https://arxiv.org/abs/2311.12163) | 通过量子启蒙分数，我们提出了一个用于评估量子生成模型质量的新指标，证明量子生成模型在质量上优于经典生成模型，并利用量子波动定理揭示了其物理限制。 |
| [^199] | [FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients](https://arxiv.org/abs/2311.11227) | 提出了一种名为FedRA的联邦调优算法，可以随机生成分配矩阵来应对拥有不同计算和通信资源的异构客户端，在不需要修改原模型的情况下进行微调。 |
| [^200] | [Nondestructive, quantitative viability analysis of 3D tissue cultures using machine learning image segmentation](https://arxiv.org/abs/2311.09354) | 本研究提出了一种图像处理算法，用于在三维培养中量化细胞的活力，无需基于试剂的指示物，并且展示了其与人类专家的表现类似。 |
| [^201] | [Gradual Optimization Learning for Conformational Energy Minimization](https://arxiv.org/abs/2311.06295) | 通过提供优化轨迹作为额外训练数据，可以改善使用神经网络的分子构象能量最小化质量，但需要大量额外的构象数据才能达到物理模拟器的优化质量。 |
| [^202] | [Efficient Compression of Overparameterized Deep Models through Low-Dimensional Learning Dynamics](https://arxiv.org/abs/2311.05061) | 通过研究深度模型的学习动态，提出了一种压缩超参数化模型的新方法，通过在低维不变子空间内更新权重矩阵来压缩深度线性网络，并在矩阵恢复问题上进行了有效性评估 |
| [^203] | [Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs](https://arxiv.org/abs/2311.04417) | 本研究提供了对这些商用AI/ML加速器的初步评估和比较，深入探讨它们的硬件和软件设计特点，以辨别它们的创新数据流架构和其他设计优化，承诺为AI/ML任务提供卓越性能和能量效率。 |
| [^204] | [Bayesian Regression Markets](https://arxiv.org/abs/2310.14992) | 本论文提出了一种贝叶斯回归市场机制，为数据共享提供了经济激励，并展示了如何缓解市场代理商面临的财务风险。 |
| [^205] | [Enhancing Group Fairness in Online Settings Using Oblique Decision Forests](https://arxiv.org/abs/2310.11401) | 提出了Aranyani，一种斜裁集成的方法，用于解决在在线环境中优化群体公平性目标所面临的挑战 |
| [^206] | [Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World](https://arxiv.org/abs/2310.10207) | Bongard-OpenWorld基准旨在评估机器视觉中对真实世界中的自由形式视觉概念进行少样本推理，并且提出了开放世界自由形式概念和真实世界图像两项新挑战。 |
| [^207] | [Unveiling the Pitfalls of Knowledge Editing for Large Language Models](https://arxiv.org/abs/2310.02129) | 这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。 |
| [^208] | [Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models](https://arxiv.org/abs/2309.05436) | 提出了一种量化傅立叶和多项式特征的方法，并基于此特征量化提出将相关模型权重也进行量化，得到了更具表达力的张量网络模型。 |
| [^209] | [Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge](https://arxiv.org/abs/2309.02001) | 使用直方图匹配来转换额外数据在处理域偏移时比简单归一化取得更好的结果 |
| [^210] | [Bias and Fairness in Large Language Models: A Survey](https://arxiv.org/abs/2309.00770) | 该论文在大型语言模型领域提出了偏见评估和缓解技术的综合调查，定义了公平性的不同方面，并提出了三个分类体系，以协助研究人员对LLMs进行公平性分析和改进。 |
| [^211] | [Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold](https://arxiv.org/abs/2308.10547) | 该论文提出了一种在Stiefel流形上进行分布式优化的黎曼共轭梯度下降方法，克服了全局函数非凸的限制。 |
| [^212] | [Defending Against Malicious Behaviors in Federated Learning with Blockchain](https://arxiv.org/abs/2307.00543) | 该研究提出了一个基于区块链和分布式分类账技术的安全和可靠的联邦学习系统，包括点对点投票机制和奖励和惩罚机制，以检测和阻止恶意行为，证明了该框架对抗恶意客户的有效性。 |
| [^213] | [ProMIL: Probabilistic Multiple Instance Learning for Medical Imaging](https://arxiv.org/abs/2306.10535) | ProMIL是一种基于实例的方法，在医学应用中能够自动检测最佳决策百分比水平，并在真实场景中表现优异. |
| [^214] | [TAP: The Attention Patch for Cross-Modal Knowledge Transfer from Unlabeled Modality](https://arxiv.org/abs/2302.02224) | 通过引入The Attention Patch（TAP）神经网络附加组件，本文提出了一种简单且有效的方法，允许从未标记的次要模态实现跨模态的数据级知识传递。 |
| [^215] | [QLABGrad: a Hyperparameter-Free and Convergence-Guaranteed Scheme for Deep Learning](https://arxiv.org/abs/2302.00252) | QLABGrad是一种无需超参数的学习率自适应方案，通过优化QLAB函数自动确定学习率，并在平滑的Lipschitz条件下证明了其收敛性，实验证明其在多种架构和数据集上表现优越。 |
| [^216] | [StoRM: A Diffusion-based Stochastic Regeneration Model for Speech Enhancement and Dereverberation](https://arxiv.org/abs/2212.11851) | 该论文提出了一种基于扩散的随机再生模型，用于语音增强和去混响，填补了预测性和生成性方法在性能上的差距。 |
| [^217] | [APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning](https://arxiv.org/abs/2212.07249) | APOLLO提出了一种优化的训练方法，通过数字感知的负采样策略和基于一致性的强化学习，提高了长篇数字推理框架的准确性和多样性。 |
| [^218] | [The Principles of Data-Centric AI (DCAI)](https://arxiv.org/abs/2211.14611) | 数据中心人工智能（DCAI）强调数据的质量和动态性，提出六项指导原则，并为人工智能系统的未来发展方向指明了道路。 |
| [^219] | [SATformer: Transformer-Based UNSAT Core Learning](https://arxiv.org/abs/2209.00953) | SATformer通过引入基于Transformer的方法，采用对不可满足性进行建模的方式，以识别不可满足的子问题，得到了优于NeuroSAT的性能。 |
| [^220] | [An alternative approach to train neural networks using monotone variational inequality](https://arxiv.org/abs/2202.08876) | 通过单调变分不等式，提出了一种高效的神经网络训练方法，可以快速收敛并在特定情况下提供保证。 |
| [^221] | [Convolutional Neural Networks on Graphs with Chebyshev Approximation, Revisited](https://arxiv.org/abs/2202.03580) | 重新审视了使用切比雪夫多项式逼近谱图卷积的问题，发现ChebNet性能较差主要是由于其学习到的非法系数近似解析滤波器函数 |
| [^222] | [Disjoint Contrastive Regression Learning for Multi-Sourced Annotations](https://arxiv.org/abs/2112.15411) | 提出了用于不相交注解的对比回归框架，解决了多个标注者间的不一致性和偏见问题。 |
| [^223] | [Domain-Aware Continual Zero-Shot Learning](https://arxiv.org/abs/2112.12989) | DACZSL提出了面向域感知的持续零样本学习任务，引入了域不变网络(DIN)，不断学习全局共享网络用于领域不变和任务不变特征，以及为每个任务提供专门的私有网络用于特定任务的特征 |
| [^224] | [A Two-Stage Feature Selection Approach for Robust Evaluation of Treatment Effects in High-Dimensional Observational Data](https://arxiv.org/abs/2111.13800) | 提出了一种名为Outcome Adaptive Elastic Net（OAENet）的两阶段特征选择技术，用于在高维观察数据中进行稳健的因果推断决策。 |
| [^225] | [Efficient GPU implementation of randomized SVD and its applications](https://arxiv.org/abs/2110.03423) | 本研究利用现代GPU上的高效处理操作，将随机化分解问题重新制定，结合快速矩阵乘法操作和随机数发生器，充分发挥GPU并行处理的潜力，减少计算矩阵分解的计算负担。 |
| [^226] | [Learning and Decision-Making with Data: Optimal Formulations and Phase Transitions](https://arxiv.org/abs/2109.06911) | 本论文提出了一种新的方法，通过首先定义一个衡量基于数据公式质量的标尺，然后寻找最优公式，使其在保证样本外性能的同时，更加接近真实成本。 |
| [^227] | [CSC-Unet: A Novel Convolutional Sparse Coding Strategy Based Neural Network for Semantic Segmentation](https://arxiv.org/abs/2108.00408) | 通过将常用的卷积操作重新构造为多层卷积稀疏编码块，提出了一种可能用于显著提高语义分割模型性能的新策略。 |
| [^228] | [Customizing Graph Neural Networks using Path Reweighting](https://arxiv.org/abs/2106.10866) | 使用路径重加权的定制图神经网络CustomGNN可以自动学习特定下游任务的高层语义，突出与语义相关的路径，并过滤掉任务无关的噪音，避免传统GNN中的过度平滑、鲁棒性差和过拟合问题。 |
| [^229] | [MgSvF: Multi-Grained Slow vs. Fast Framework for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2006.15524) | 提出了一种多粒度SvF学习策略，通过设计频率感知正则化和特征空间组合操作来平衡保留旧知识与适应新知识，以解决少样本类别增量学习中的"慢vs.快"困境 |
| [^230] | [Epoch-evolving Gaussian Process Guided Learning](https://arxiv.org/abs/2006.14347) | 提出了一种名为GPGL的epoch-evolving Gaussian Process Guided Learning学习方案，通过上下文标签和地面真实标签指导模型参数更新，进一步推广并应用于当前深度模型，在主流数据集上表现显著优于现有基于批次的最先进模型 |
| [^231] | [lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold Gap.](http://arxiv.org/abs/2401.15879) | 本文提出了一种名为lil'HDoC的算法，用于解决小阈值间隙下的好臂识别问题。实验证明该算法在样本效率上优于现有算法。 |
| [^232] | [Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View.](http://arxiv.org/abs/2401.11237) | 这篇论文研究了强化学习方法在训练过程中将不同经验片段组合起来解决未见过的任务的属性，并通过分析发现这种组合属性与组合泛化有关。 |
| [^233] | [Cascading Reinforcement Learning.](http://arxiv.org/abs/2401.08961) | 本文提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响，在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。 |
| [^234] | [Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks.](http://arxiv.org/abs/2401.06187) | Scissorhands 是一种新的机器取消学习方法，通过连接敏感性识别与遗忘数据相关的最相关参数，并通过重新训练修剪的模型来擦除数据影响。 |
| [^235] | [RudolfV: A Foundation Model by Pathologists for Pathologists.](http://arxiv.org/abs/2401.04079) | 本文提出了一种由病理学家为病理学家构建的基础模型，通过数据整理和融入病理学领域知识，扩展了数字病理学全玻片图像基础模型，解决了人工智能在处理罕见疾病方面的挑战。 |
| [^236] | [Beyond Regrets: Geometric Metrics for Bayesian Optimization.](http://arxiv.org/abs/2401.01981) | 本论文提出了四个新的几何度量，可以比较贝叶斯优化算法在考虑查询点和全局最优解的几何特性时的性能。 |
| [^237] | [EyePreserve: Identity-Preserving Iris Synthesis.](http://arxiv.org/abs/2312.12028) | 本论文提出了一种完全数据驱动的、保持身份的、瞳孔尺寸变化的虹膜图像合成方法，能够合成不同瞳孔尺寸的虹膜图像，代表不存在的身份，并能够在保持身份的同时进行非线性纹理变形。 |
| [^238] | [LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery.](http://arxiv.org/abs/2311.02058) | LOTUS是一种持续模仿学习算法，通过无监督技能发现，使得机器人能够在其整个寿命中持续学习解决新的操作任务。该算法通过构建技能库，并使用元控制器灵活组合技能来提高成功率，在实验中表现出优越的知识传递能力。 |
| [^239] | [Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers.](http://arxiv.org/abs/2310.12553) | 提出一种插入/删除指标感知的基于解释的优化(ID-ExpO)方法，通过优化可区分的预测器来提高解释的插入和删除得分，并保持预测准确性。实验结果表明，ID-ExpO能够使流行的事后解释器产生更忠实的解释。 |
| [^240] | [Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics.](http://arxiv.org/abs/2310.07990) | 本文提出了一种新的方法，利用多视图变分自动编码器来填充非目标代谢组学中的缺失值，该方法利用了全基因组测序数据和参考代谢物的信息，可以有效地根据基因组信息填充缺失的代谢组学值。 |
| [^241] | [Secure Decentralized Learning with Blockchain.](http://arxiv.org/abs/2310.07079) | 本文提出了基于区块链的分散式联邦学习（BDFL），通过利用区块链进行分散式模型验证和审计，解决了分散式联邦学习中虚假模型和数据攻击的问题。 |
| [^242] | [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models.](http://arxiv.org/abs/2310.06117) | 本文提出了一种简单的提示技术，使得大型语言模型能够通过抽象获得高层概念和基本原理，并将其应用于推理路径中，从而显著提升模型在各种推理密集型任务上的表现。 |
| [^243] | [Learning to Grasp: from Somewhere to Anywhere.](http://arxiv.org/abs/2310.04349) | 本研究介绍了一个采用质量-多样性方法的流水线，用于将生成的抓取轨迹适应到新物体姿态。使用RGB-D数据流，该方法能够自动检测目标物体、预测其姿态，并生成可达到的抓取轨迹。 |
| [^244] | [TRAM: Bridging Trust Regions and Sharpness Aware Minimization.](http://arxiv.org/abs/2310.03646) | TRAM是一种桥接信任区域和锐度感知最小化的算法，通过减少损失曲面的曲率来提供鲁棒性改进。它通过在微调过程中优化可转移的表示来实现领域外泛化，并且通过结合信任区域方法和SAM风格的正则化器来统一参数和表示空间平滑方法。TRAM在保持预训练结构的同时实现了平坦的极小值和平滑、有信息量的表示。 |
| [^245] | [Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness.](http://arxiv.org/abs/2310.02832) | 本文提出了一种通过利用神经网络中间层变换的平滑性来检测带外数据的方法(BLOOD),该方法适用于没有训练数据访问权限的预训练模型，并在Transformer网络上的文本分类任务中取得了良好的效果。 |
| [^246] | [GRANDE: Gradient-Based Decision Tree Ensembles.](http://arxiv.org/abs/2309.17130) | 这篇论文提出了一种名为GRANDE的基于梯度的决策树集成模型，通过端到端梯度下降学习坚硬、轴对齐的决策树集成，并结合了轴对齐分割和梯度优化的灵活性，引入了一种先进的逐个实例加权方法，可以在一个模型中便于学习简单和复杂关系的表示。 |
| [^247] | [MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback.](http://arxiv.org/abs/2309.10691) | MINT是一个评估LLMs在多轮交互中解决任务能力的基准，通过使用工具和利用用户的自然语言反馈。它解决了当前评估协议忽略细致互动和低估自然语言反馈的问题，促进了研究基准评估和实际应用之间的一致性。 |
| [^248] | [Exploiting Problem Geometry in Safe Linear Bandits.](http://arxiv.org/abs/2308.15006) | 通过利用安全线性赌博机问题的几何特征，我们提出了改进的遗憾保证算法，并将其推广到具有凸约束的情况。模拟结果显示，在各种随机采样的设置中，我们的算法表现出优越的性能。 |
| [^249] | [Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation.](http://arxiv.org/abs/2308.13212) | 提出一种受物理启发的神经图ODE算法（PINGO），用于模拟和建模多对象物理系统的长期动态，并解决了当前模型泛化能力差的问题。 |
| [^250] | [Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation.](http://arxiv.org/abs/2308.08079) | 该论文介绍了一种用于地下数据集的稳定降维方法，通过刚性变换实现了欧几里德不变表示，能够量化地下数据的不确定性并且适应外样本点的扩展。 |
| [^251] | [Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI.](http://arxiv.org/abs/2308.05525) | 本文研究了三维点云的临界点与非分布样本之间的相互作用，并将临界点的概念推广为重要性度量方法。通过仅基于非重要点进行分类网络训练，可以提高鲁棒性，同时在干净数据集上会有些性能损失。建议使用标准化熵选择非临界点集合的自适应阈值。这种重要性度量方法计算速度极快，可以应用于可解释AI、离群值去除、不确定性估计、鲁棒分类和对抗性防御等多种应用。 |
| [^252] | [Deep Learning for Diverse Data Types Steganalysis: A Review.](http://arxiv.org/abs/2308.04522) | 本综述论文详细综述了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的最新研究进展。 |
| [^253] | [Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models.](http://arxiv.org/abs/2308.02409) | 该论文通过融合多个空间维度的方法，使用脑电信号对心理负荷进行分类和估计连续级别。在时间域中使用了时态卷积网络，而在频率域中引入了新的架构——多维残差块。 |
| [^254] | [Symmetric Equilibrium Learning of VAEs.](http://arxiv.org/abs/2307.09883) | 本文提出了一种对称均衡学习方法，允许在只能通过采样获得数据和潜在分布的情况下学习VAEs。实验证明该方法与传统ELBO学习方法获得的模型相当，并具有广泛的应用性。 |
| [^255] | [Overthinking the Truth: Understanding how Language Models Process False Demonstrations.](http://arxiv.org/abs/2307.09476) | 该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。 |
| [^256] | [Dynamic Prediction using Time-Dependent Cox Survival Neural Network.](http://arxiv.org/abs/2307.05881) | 该论文通过使用time-dependent Cox模型和神经网络，提出了一种动态预测模型来预测进行性眼部疾病年龄相关性黄斑变性（AMD）的进展。通过使用纵向眼底图像作为输入，该模型可以建立一个个体化的风险预测模型，并且在实证研究中表现出良好的效果。 |
| [^257] | [TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences.](http://arxiv.org/abs/2306.14114) | 该论文提出了一种基于拓扑神经泊松自回归模型的方法，同时考虑先验拓扑网络和潜在的Granger因果结构来学习事件序列中的Granger因果关系，该方法通过推断因果关系来解决序列数据中的非 i.i.d. 问题 |
| [^258] | [Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information.](http://arxiv.org/abs/2306.08762) | 本论文研究了具有部分在线状态信息的POMDP问题的理论困难性和可计算性。作者通过建立下界得出一个惊人的难度结果：除非具有完整的在线状态信息，否则需要指数级的样本复杂度才能得到POMDP的最优策略解。然而，作者还发现了具有部分在线状态信息下的可计算POMDP类别，并提出了新的算法来证明其接近最优性。 |
| [^259] | [SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation.](http://arxiv.org/abs/2306.03403) | 本论文提出了SGAT4PASS，一种面向球面几何意识的全景语义分割Transformer，通过加入球面几何感知的约束，能更好地捕捉全景图像的3D属性，从而提高分割性能。 |
| [^260] | [Sharpened Lazy Incremental Quasi-Newton Method.](http://arxiv.org/abs/2305.17283) | 本文提出了一种新算法—— Sharpened Lazy Incremental Quasi-Newton (SLIQN) 方法，其具有显式的超线性收敛速率和$O(d^2)$的迭代复杂度。 |
| [^261] | [Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models.](http://arxiv.org/abs/2305.14585) | 本研究通过建立一个规范化的伪神经切线核，证明了它能够更好地与神经网络决策函数相关，比基于嵌入和影响的替代品更有效，并且从它创建的归因会更准确地选择被扰动的训练数据，从而证明了核线性模型是跨多个数据领域并有效的替代模型。 |
| [^262] | [Sequence Modeling is a Robust Contender for Offline Reinforcement Learning.](http://arxiv.org/abs/2305.14550) | 序列建模是离线强化学习中比Q-Learning和Imitation Learning更适合在稀疏奖励和低质量数据设置下的选择，在任务范围增加时，序列建模和模仿学习更可取。 |
| [^263] | [P-NOC: Adversarial CAM Generation for Weakly Supervised Semantic Segmentation.](http://arxiv.org/abs/2305.12522) | 本文提出了一种新的弱监督语义分割策略，通过对抗性CAM生成网络生成稳健的语义分割提议，结果表明该方法显著提高了分割效果。 |
| [^264] | [Utility Theory of Synthetic Data Generation.](http://arxiv.org/abs/2305.10015) | 本文从统计学角度建立效用理论，旨在基于一般性指标定量评估合成算法的效用，效用指标的分析界限揭示了指标收敛的关键条件，令人惊讶的是，只要下游学习任务中的模型规范是正确的，合成特征分布不一定与原始特征分布相同，效用指标会收敛。 |
| [^265] | [OpenBox: A Python Toolkit for Generalized Black-box Optimization.](http://arxiv.org/abs/2304.13339) | OpenBox是一个通用黑盒优化的Python工具包，提供了用户友好的接口和可视化功能，模块化设计能够在现有系统中灵活部署，并且实验证明其比现有系统更有效和高效。 |
| [^266] | [An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response.](http://arxiv.org/abs/2303.17823) | 本文提出了一种基于可解释神经网络的非比例赔率模型 (N$^3$POM) 用于有序回归，可以对连续变量进行预测，同时保留了可解释性，具有灵活性。 |
| [^267] | [Neural DAEs: Constrained neural networks.](http://arxiv.org/abs/2211.14302) | 本文研究了将辅助代数轨迹信息明确添加到神经网络中的影响，并通过稳定化和投影方法合并信息，对多体摆和分子动力学情景进行了模拟实验。该方法易于实现，对训练性能影响有限，在推理方面给出了显著提升。 |
| [^268] | [Federated Adaptive Prompt Tuning for Multi-domain Collaborative Learning.](http://arxiv.org/abs/2211.07864) | 本文提出了一种面向多领域协作学习的联邦自适应提示调优算法 FedAPT，利用强大的预训练模型实现更高的性能。其核心思想是为每个测试样本提供个性化提示，通过自适应地释放特定领域的知识来实现。通过设计一个自适应提示调优模块，服务器生成关键信息并分配给客户端，从而实现协同训练全局的自适应网络和元提示。 |
| [^269] | [Majorization-minimization for Sparse Nonnegative Matrix Factorization with the $\beta$-divergence.](http://arxiv.org/abs/2207.06316) | 本文提出了一种带 $\beta$-差异的稀疏非负矩阵分解的主导最小化算法，其能够适用于任何 $\beta$-差异和其他稀疏约束。 |

# 详细

[^1]: TeleMoMa：一种用于移动操作的模块化多功能远程操作系统

    TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation

    [https://arxiv.org/abs/2403.07869](https://arxiv.org/abs/2403.07869)

    TeleMoMa 是一种面向移动操作的模块化多功能远程操作系统，通过整合多种人机接口、降低门槛且具有通用性，为移动操作器提供了全身远程操作的解决方案。

    

    机器人学中限制模仿学习的关键瓶颈是数据的匮乏。这个问题在移动操作中更为严重，因为与静止操作相比，由于缺乏可用且易于使用的远程操作界面，收集演示更加困难。在这项工作中，我们展示了TeleMoMa，这是一种用于全身远程操作移动操作器的通用和模块化界面。TeleMoMa将包括RGB和深度摄像头、虚拟现实控制器、键盘、操纵杆等多个人机接口整合在一起，以及这些接口的任何组合。在其更易访问的版本中， TeleMoMa可以仅使用视觉（如RGB-D相机）即可工作，降低了人类提供移动操作演示的门槛。我们通过在模拟环境和现实世界中远程操作几个现有的移动操作器——PAL Tiago++, Toyota HSR和Fetch来展现TeleMoMa的多功能性。

    arXiv:2403.07869v1 Announce Type: cross  Abstract: A critical bottleneck limiting imitation learning in robotics is the lack of data. This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces. In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators. TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof. In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations. We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world. We demonstrate the quality of the demonst
    
[^2]: 通过代码探索大型语言模型的安全泛化挑战

    Exploring Safety Generalization Challenges of Large Language Models via Code

    [https://arxiv.org/abs/2403.07865](https://arxiv.org/abs/2403.07865)

    本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。

    

    大型语言模型（LLMs）的快速发展带来了自然语言处理方面的显著能力，但也引发了人们对它们潜在误用的担忧。本文引入了CodeAttack，一个将自然语言输入转换为代码输入的框架，为测试LLMs的安全泛化提供了一个新颖的环境。我们对包括GPT-4、Claude-2和Llama-2系列在内的最新LLMs进行了全面研究，发现这些模型对于代码输入存在共同的安全漏洞：CodeAttack在超过80%的时间内始终绕过所有模型的安全保护。

    arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
    
[^3]: 公平反馈循环：在合成数据上训练会放大偏见

    Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias

    [https://arxiv.org/abs/2403.07857](https://arxiv.org/abs/2403.07857)

    模型诱导的分布转移可能导致性能、公平性和边缘群体表现的损失，提出了算法修复(AR)框架以通过积极干预实现对历史歧视的补救

    

    模型诱导的分布转移(MIDS)会导致先前模型输出污染新模型训练集，随着模型的演变。对于生成模型来说，这被称为模型崩溃，对于监督模型来说则是表现预测或不公平反馈循环。当一个模型诱导了分布的转移，它也将其错误、偏见和不公平性编码到数据生态系统的基本事实中。我们引入了一个框架，使我们能够跟踪多个MIDS长时间的演变，发现它们可能导致性能、公平性和边缘群体表现的损失，即使在最初是无偏的数据集中也是如此。尽管存在这些负面后果，我们确定了模型如何可以用于积极、有意的干预其数据生态系统，通过一种名为算法修复(AR)的框架为历史上的歧视提供补救。我们通过策划代表性培训来模拟AR干预。

    arXiv:2403.07857v1 Announce Type: new  Abstract: Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative traini
    
[^4]: 在数据剪枝中蒸馏知识

    Distilling the Knowledge in Data Pruning

    [https://arxiv.org/abs/2403.07854](https://arxiv.org/abs/2403.07854)

    在数据剪枝中引入知识蒸馏方法，通过与预先训练的教师网络软预测相结合，实现了在各种数据集、剪枝方法和所有剪枝分数上的显著提升。

    

    随着训练神经网络使用的数据集规模不断增加，数据剪枝成为了一个有吸引力的研究领域。然而，大多数当前的数据剪枝算法在保持准确性方面受到限制，特别是在高度剪枝的情况下与使用完整数据训练的模型相比。本文探讨了在训练基于剪枝子集的模型时，结合知识蒸馏（KD）的应用。也就是说，我们不仅依赖于地面真实标签，还使用了已在完整数据上预先训练的老师网络的软预测。通过将知识蒸馏整合到训练中，我们在各种数据集、剪枝方法和所有剪枝分数上都展示了显著的改进。我们首先建立了采用自蒸馏来改善在剪枝数据上的训练的理论动机。然后，我们在实证上进行了引人注目且高度实用的观察：使用知识蒸馏，简单的随机剪枝也会取得显着改进。

    arXiv:2403.07854v1 Announce Type: cross  Abstract: With the increasing size of datasets used for training neural networks, data pruning becomes an attractive field of research. However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes. In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset. That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data. By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions. We first establish a theoretical motivation for employing self-distillation to improve training on pruned data. Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is c
    
[^5]: 设备上每类12毫焦的在线少样本类增量学习

    12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning

    [https://arxiv.org/abs/2403.07851](https://arxiv.org/abs/2403.07851)

    该论文提出了基于轻量级模型的在线少样本类增量学习（O-FSCIL），使用特征正交正则化和多边界损失进行预训练和元学习，在设备上实现每类12毫焦的学习能力。

    

    《少样本类增量学习（FSCIL）》使机器学习系统仅使用少量已标记的示例即可扩展其对新类的推理能力，而不会忘记先前学习过的类。本文介绍了一种基于轻量级模型的《在线少样本类增量学习（O-FSCIL）》，该模型由预训练和元学习的特征提取器以及存储类原型的可扩展显式内存组成。该架构使用一种新颖的特征正交正则化进行预训练，并通过多边界损失进行元学习。对于学习新类，我们的方法通过新的类原型扩展显式内存，同时保持其余架构冻结。这样可以基于仅仅少量示例学习以前未见过的类。

    arXiv:2403.07851v1 Announce Type: new  Abstract: Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems to expand their inference capabilities to new classes using only a few labeled examples, without forgetting the previously learned classes. Classical backpropagation-based learning and its variants are often unsuitable for battery-powered, memory-constrained systems at the extreme edge. In this work, we introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a lightweight model consisting of a pretrained and metalearned feature extractor and an expandable explicit memory storing the class prototypes. The architecture is pretrained with a novel feature orthogonality regularization and metalearned with a multi-margin loss. For learning a new class, our approach extends the explicit memory with novel class prototypes, while the remaining architecture is kept frozen. This allows learning previously unseen classes based on only a few examples with on
    
[^6]: 通过频繁子图挖掘增强的迭代图神经网络优化

    Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations

    [https://arxiv.org/abs/2403.07849](https://arxiv.org/abs/2403.07849)

    通过解释增强图神经网络学习，提出了一个迭代自我改进的算法EEGL，通过频繁子图挖掘和过滤来获取节点邻域中特定子图的应用相关特征。

    

    我们提出了一种基于可解释性人工智能（XAI）的模型改进方法，用于节点分类的图神经网络（GNN），称为Explanation Enhanced Graph Learning (EEGL)。该方法旨在通过解释来提高GNN的预测性能。EEGL是一个迭代自我改进的算法，从学习到的“原始”GNN开始，通过频繁子图挖掘反复发现解释子图中的相关模式。这些模式进一步被过滤以获取与节点邻域中特定子图的存在对应的应用相关特征。以应用相关算法为例，对于Weisfeiler-Leman （1-WL）算法的子图扩展此前被提出为一个开放性问题。我们提供了实验证据，使用合成和真实世界数据，显示EEGL在预测性能上优于相关方法，并具有超越简单节点区分能力。

    arXiv:2403.07849v1 Announce Type: new  Abstract: We formulate an XAI-based model improvement approach for Graph Neural Networks (GNNs) for node classification, called Explanation Enhanced Graph Learning (EEGL). The goal is to improve predictive performance of GNN using explanations. EEGL is an iterative self-improving algorithm, which starts with a learned "vanilla" GNN, and repeatedly uses frequent subgraph mining to find relevant patterns in explanation subgraphs. These patterns are then filtered further to obtain application-dependent features corresponding to the presence of certain subgraphs in the node neighborhoods. Giving an application-dependent algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL) algorithm has previously been posed as an open problem. We present experimental evidence, with synthetic and real-world data, which show that EEGL outperforms related approaches in predictive performance and that it has a node-distinguishing power beyond that 
    
[^7]: 机器学习和经验贝叶斯方法用于B2B电子商务中的预测购买

    A Machine learning and Empirical Bayesian Approach for Predictive Buying in B2B E-commerce

    [https://arxiv.org/abs/2403.07843](https://arxiv.org/abs/2403.07843)

    该研究采用了XGBoost和修改版的方法，以解决B2B电子商务中准确预测买家订单下达行为的挑战

    

    在像印度这样的发展中国家的背景下，传统的企业间（B2B）商业在买家和卖家之间建立强大的关系、信任和信用安排的基础上进行，因此，电子商务企业经常。创立于2016年，Udaan的愿景是通过技术改变印度的贸易，是印度最大的企业间电子商务平台。Udaan在包括生活方式、电子产品、家居在内的各种产品类别中运营，并雇用电话销售员来培养买家关系、简化订单下达程序并促进特别促销。准确预测买家订单下达行为成为实现可持续增长、增强竞争力和优化这些电话销售员效率的关键因素。为了解决这一挑战，我们采用了XGBoost和修改版的集成方法

    arXiv:2403.07843v1 Announce Type: new  Abstract: In the context of developing nations like India, traditional business to business (B2B) commerce heavily relies on the establishment of robust relationships, trust, and credit arrangements between buyers and sellers. Consequently, ecommerce enterprises frequently. Established in 2016 with a vision to revolutionize trade in India through technology, Udaan is the countrys largest business to business ecommerce platform. Udaan operates across diverse product categories, including lifestyle, electronics, home and employ telecallers to cultivate buyer relationships, streamline order placement procedures, and promote special promotions. The accurate anticipation of buyer order placement behavior emerges as a pivotal factor for attaining sustainable growth, heightening competitiveness, and optimizing the efficiency of these telecallers. To address this challenge, we have employed an ensemble approach comprising XGBoost and a modified version of
    
[^8]: 量化和减轻表格生成模型的隐私风险

    Quantifying and Mitigating Privacy Risks for Tabular Generative Models

    [https://arxiv.org/abs/2403.07842](https://arxiv.org/abs/2403.07842)

    该论文研究了量化和减轻表格生成模型的隐私风险，通过对五种最先进的表格合成器进行实证分析，提出了差分隐私表格潜在扩散模型。

    

    针对合成数据生成模型出现作为保护隐私的数据共享解决方案的情况，该合成数据集应该类似于原始数据，而不会透露可识别的私人信息。表格合成器的核心技术根植于图像生成模型，范围从生成对抗网络（GAN）到最近的扩散模型。最近的先前工作揭示和量化了表格数据上的效用-隐私权衡，揭示了合成数据的隐私风险。我们首先进行了详尽的实证分析，突出了五种最先进的表格合成器针对八种隐私攻击的效用-隐私权衡，特别关注成员推断攻击。在观察到表格扩散中高数据质量但也高隐私风险的情况下，我们提出了DP-TLDM，差分隐私表格潜在扩散模型，由自动编码器网络组成。

    arXiv:2403.07842v1 Announce Type: new  Abstract: Synthetic data from generative models emerges as the privacy-preserving data-sharing solution. Such a synthetic data set shall resemble the original data without revealing identifiable private information. The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models. Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data. We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks. Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder networ
    
[^9]: 使用空间变化的自编码器融合气候数据产品

    Fusing Climate Data Products using a Spatially Varying Autoencoder

    [https://arxiv.org/abs/2403.07822](https://arxiv.org/abs/2403.07822)

    该研究提出了一种利用贝叶斯统计框架的可识别和可解释的自编码器，用于融合气候数据产品，其空间变化捕捉了有用的空间模式。

    

    自编码器是强大的机器学习模型，用于压缩来自多个数据源的信息。然而，像所有人工神经网络一样，自编码器通常是不可识别且不可解释的。本研究旨在创建一个可识别和可解释的自编码器，用于混合和合并气候数据产品。所提出的自编码器利用贝叶斯统计框架，允许概率性解释，同时在空间上变化以捕捉各种数据产品之间的有用空间模式。在自编码器学习数据模式时对其施加约束，形成一个可解释的共识，包括每个输入中的重要特征。我们通过将来自多个降水产品的信息结合在高山亚洲展示了自编码器的实用性。

    arXiv:2403.07822v1 Announce Type: cross  Abstract: Autoencoders are powerful machine learning models used to compress information from multiple data sources. However, autoencoders, like all artificial neural networks, are often unidentifiable and uninterpretable. This research focuses on creating an identifiable and interpretable autoencoder that can be used to meld and combine climate data products. The proposed autoencoder utilizes a Bayesian statistical framework, allowing for probabilistic interpretations while also varying spatially to capture useful spatial patterns across the various data products. Constraints are placed on the autoencoder as it learns patterns in the data, creating an interpretable consensus that includes the important features from each input. We demonstrate the utility of the autoencoder by combining information from multiple precipitation products in High Mountain Asia.
    
[^10]: 标签丢失率：利用具有域转移和部分标记的多个数据集改进深度学习超声心动图分割

    Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling

    [https://arxiv.org/abs/2403.07818](https://arxiv.org/abs/2403.07818)

    本文研究利用多个数据集进行深度学习超声心动图分割，在处理部分标记数据时采用改进的交叉熵损失函数。

    

    超声心动图（超声）是评估心脏功能时使用的第一种成像方式。从超声中测量功能生物标志物依赖于对心脏结构进行分割，深度学习模型被提出来自动化这一过程。然而，为了将这些工具转化为广泛的临床应用，重要的是分割模型对各种图像具有鲁棒性（例如，由不同扫描仪获得，由不同级别的专家操作员获得等）。为了实现这种鲁棒性水平，有必要使用多个不同的数据集来训练模型。在使用多个不同的数据集进行训练时面临的一个重要挑战是标签存在的变化，即合并数据通常是部分标记的。已经提出了交叉熵损失函数的改进来处理部分标记数据。在本文中，我们展示了训练的naively

    arXiv:2403.07818v1 Announce Type: cross  Abstract: Echocardiography (echo) is the first imaging modality used when assessing cardiac function. The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process. However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.). To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets. A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled. Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data. In this paper we show that training naively 
    
[^11]: Chronos: 学习时间序列的语言

    Chronos: Learning the Language of Time Series

    [https://arxiv.org/abs/2403.07815](https://arxiv.org/abs/2403.07815)

    Chronos框架通过在固定词汇上训练预训练的概率时间序列模型，在大量数据集上进行了全面基准测试，表现出在训练语料库中的数据集上明显优于其他方法，并且在新数据集上的零样本性能表现可比甚至优于其他方法。

    

    我们介绍了Chronos，一个简单但有效的预训练概率时间序列模型框架。Chronos使用缩放和量化将时间序列值标记化为固定词汇，并通过交叉熵损失在这些标记化的时间序列上训练现有的基于Transformer的语言模型架构。我们在大量公开可用数据集上基于T5系列（参数范围从20M到710M）对Chronos模型进行了预训练，同时通过高斯过程生成了一个合成数据集以提高泛化能力。在包含42个数据集的全面基准测试中，涵盖了传统的本地模型和深度学习方法，我们展示了Chronos模型：（a）在训练语料库中的数据集上明显优于其他方法；（b）相对于专门训练的方法，在新数据集上的零样本性能可比甚至优于其他方法。

    arXiv:2403.07815v1 Announce Type: cross  Abstract: We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained spe
    
[^12]: pyvene: 通过干预来理解和改进PyTorch模型的库

    pyvene: A Library for Understanding and Improving PyTorch Models via Interventions

    [https://arxiv.org/abs/2403.07809](https://arxiv.org/abs/2403.07809)

    pyvene是一个开源Python库，支持在PyTorch模型上进行可定制的干预，提供统一和可扩展的框架，用于解释神经模型并与他人分享经过干预的模型。

    

    模型内部状态的干预是人工智能许多领域的基本操作，包括模型编辑、控制、稳健性和可解释性。为了促进这方面的研究，我们介绍了pyvene，一个开源的Python库，支持在各种不同PyTorch模块上进行可定制的干预。Pyvene支持复杂的干预方案，具有直观的配置格式，并且其干预可以是静态的或包含可训练参数。我们展示了pyvene如何提供一个统一和可扩展的框架，用于对神经模型进行干预并与他人共享经过干预的模型。我们通过因果抽象和知识定位的可解释性分析展示了该库的能力。我们通过Python Package Index（PyPI）发布了我们的库，并在https://github.com/stanfordnlp/pyvene 提供了代码、文档和教程。

    arXiv:2403.07809v1 Announce Type: cross  Abstract: Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, we introduce $\textbf{pyvene}$, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. $\textbf{pyvene}$ supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. We show how $\textbf{pyvene}$ provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene.
    
[^13]: 通过设备本地可学习的用户语音特征增强关键词识别

    Boosting keyword spotting through on-device learnable user speech characteristics

    [https://arxiv.org/abs/2403.07802](https://arxiv.org/abs/2403.07802)

    提出一种新颖的设备本地学习架构，通过学习用户语音特征来增强关键词识别系统，在35类问题的Google Speech Commands数据集中实现了高达19%的错误率降低。

    

    TinyML受限应用的关键词识别系统需要现场调整，以提高离线训练分类器在未知推理条件下部署时的准确性。适应目标用户的语音特点需要大量领域内样本，通常在现实场景中不可用。此外，当前的设备本地学习技术依赖于计算密集型和内存消耗极大的骨干更新方案，不适用于始终开启、使用电池供电的设备。在这项工作中，我们提出了一种新颖的设备本地学习架构，由预训练骨干和学习用户语音特征的用户感知嵌入组成。生成的特征被融合并用于对输入的话语进行分类。针对由未知讲话者引起的领域转移，我们在Google Speech Commands数据集的35类问题上测得错误率降低高达19%，从30.1%降至24.3%。

    arXiv:2403.07802v1 Announce Type: cross  Abstract: Keyword spotting systems for always-on TinyML-constrained applications require on-site tuning to boost the accuracy of offline trained classifiers when deployed in unseen inference conditions. Adapting to the speech peculiarities of target users requires many in-domain samples, often unavailable in real-world scenarios. Furthermore, current on-device learning techniques rely on computationally intensive and memory-hungry backbone update schemes, unfit for always-on, battery-powered devices. In this work, we propose a novel on-device learning architecture, composed of a pretrained backbone and a user-aware embedding learning the user's speech characteristics. The so-generated features are fused and used to classify the input utterance. For domain shifts generated by unseen speakers, we measure error rate reductions of up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google Speech Commands dataset, through the inexpensi
    
[^14]: 联合选择：适应性地将公共信息纳入私密合成数据

    Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data

    [https://arxiv.org/abs/2403.07797](https://arxiv.org/abs/2403.07797)

    jam-pgm机制在合成数据生成中能够联合选择公共数据和私密数据，并且能够在公共数据分布存在偏差的情况下优于其他机制。

    

    基于边际和图模型的不同ially私密合成数据生成机制已在各种设置中取得成功。然而，这些方法的一个局限性是它们无法整合公共数据。通过在公共数据上进行预训练来初始化数据生成模型已被证明可以提高合成数据的质量，但当模型结构未事先确定时，该技术无法应用。我们开发了机制jam-pgm，将自适应测量框架扩展到联合选择测量公共数据和私密数据之间。这一技术允许将公共数据纳入基于图模型的机制中。我们展示了，即使在公共数据分布存在偏差的情况下，jam-pgm能够胜过公共辅助和非公共辅助的合成数据生成机制。

    arXiv:2403.07797v1 Announce Type: cross  Abstract: Mechanisms for generating differentially private synthetic data based on marginals and graphical models have been successful in a wide range of settings. However, one limitation of these methods is their inability to incorporate public data. Initializing a data generating model by pre-training on public data has shown to improve the quality of synthetic data, but this technique is not applicable when model structure is not determined a priori. We develop the mechanism jam-pgm, which expands the adaptive measurements framework to jointly select between measuring public data and private data. This technique allows for public data to be included in a graphical-model-based mechanism. We show that jam-pgm is able to outperform both publicly assisted and non publicly assisted synthetic data generation mechanisms even when the public data distribution is biased.
    
[^15]: DexCap：用于灵巧操作的可扩展和可移植动作捕捉数据收集系统

    DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation

    [https://arxiv.org/abs/2403.07788](https://arxiv.org/abs/2403.07788)

    DexCap是一个可移植的手部动作捕捉系统，结合DexIL算法从人类手部运动数据中训练机器人技能，具有精确追踪和复制人类动作的能力。

    

    从人类手部运动数据中学习是为机器人赋予类人灵巧在现实操纵任务中的潜在途径，然而，现存手部动作捕捉系统的可移植性以及将动作捕捉数据转化为有效控制策略的困难仍然存在挑战。为了应对这些问题，我们引入了DexCap，一个便携式手部动作捕捉系统，以及DexIL，一种新颖的模仿算法，可直接从人类手部动作捕捉数据训练灵巧机器人技能。DexCap基于SLAM和电磁场以及环境的3D观察，提供了对手腕和手指运动的精确、抗遮挡的跟踪。利用这一丰富的数据集，DexIL采用逆运动学和基于点云的模仿学习来复制人类动作与机器人手。除了从人类运动中学习外，DexCap还提供了一种op

    arXiv:2403.07788v1 Announce Type: cross  Abstract: Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an op
    
[^16]: 公平RR：通过随机响应实现群体公平的预处理

    FairRR: Pre-Processing for Group Fairness through Randomized Response

    [https://arxiv.org/abs/2403.07780](https://arxiv.org/abs/2403.07780)

    本文提出了一种名为FairRR的预处理算法，通过在随机响应框架中修改响应变量的最优设计矩阵，直接控制群体公平性的度量，从而产生出色的下游模型效用和公平性。

    

    机器学习模型在重要决策过程中的使用日益增多，这促使人们开始研究这些系统的公平性。虽然在处理过程和后处理设置中已经进行了大量工作来研究群体公平性，但很少有研究能够从理论上将这些结果与预处理领域连接起来。本文提出，实现下游模型中的群体公平性可以被形式化为在随机响应框架中修改响应变量的最优设计矩阵。我们展示了群体公平性的度量可以通过最优模型效用直接控制，提出了一种称为FairRR的预处理算法，该算法产生出色的下游模型效用和公平性。

    arXiv:2403.07780v1 Announce Type: cross  Abstract: The increasing usage of machine learning models in consequential decision-making processes has spurred research into the fairness of these systems. While significant work has been done to study group fairness in the in-processing and post-processing setting, there has been little that theoretically connects these results to the pre-processing domain. This paper proposes that achieving group fairness in downstream models can be formulated as finding the optimal design matrix in which to modify a response variable in a Randomized Response framework. We show that measures of group fairness can be directly controlled for with optimal model utility, proposing a pre-processing algorithm called FairRR that yields excellent downstream model utility and fairness.
    
[^17]: 超越标签：揭示语音识别数据集中的文本依赖性

    Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets

    [https://arxiv.org/abs/2403.07767](https://arxiv.org/abs/2403.07767)

    本研究批判性评估了语音识别数据集中的文本依赖性，揭示了一些机器学习模型可能会过于关注词汇特征而非预期的语音交际特征。

    

    类似认知负荷和情绪等语音交际特征越来越被认可为语音识别研究中的关键领域，通常通过专门的数据集（如CLSE和IEMOCAP）进行研究。然而，很少有人审查这些数据集是否存在文本依赖性。本文批判性地评估了机器学习模型在这些数据集上训练时真正学会识别语音交际特征，而不仅仅是捕捉词汇特征的普遍假设。通过检查这些数据集中的词汇重叠并测试机器学习模型的性能，我们揭示了特征标签中的显著文本依赖性。我们的结果表明，一些机器学习模型，特别是像HuBERT这样的大型预训练模型，可能无意中专注于词汇特征，而不是预期的语音交际特征。本研究号召研究界重新评估数据集的可靠性。

    arXiv:2403.07767v1 Announce Type: cross  Abstract: Paralinguistic traits like cognitive load and emotion are increasingly recognized as pivotal areas in speech recognition research, often examined through specialized datasets like CLSE and IEMOCAP. However, the integrity of these datasets is seldom scrutinized for text-dependency. This paper critically evaluates the prevalent assumption that machine learning models trained on such datasets genuinely learn to identify paralinguistic traits, rather than merely capturing lexical features. By examining the lexical overlap in these datasets and testing the performance of machine learning models, we expose significant text-dependency in trait-labeling. Our results suggest that some machine learning models, especially large pre-trained models like HuBERT, might inadvertently focus on lexical characteristics rather than the intended paralinguistic features. The study serves as a call to action for the research community to reevaluate the relia
    
[^18]: 概率易变量因果效应研究

    Probabilistic Easy Variational Causal Effect

    [https://arxiv.org/abs/2403.07745](https://arxiv.org/abs/2403.07745)

    论文提出了一种称为Probabilistic Easy Variational Causal Effect (PEACE)的函数，可以测量X对Y的直接因果效应，适用于连续和离散情况，通过管理概率密度值强度$d\ge 0$来实现干预。

    

    论文研究随机向量$X$和$Z$，以及$Y=g(X,Z)$的情况。一方面，对于连续的$X$和$Z$，通过使用总变差和$g$的通量的思想，我们发展了一个在因果推断中能够处理广泛因果问题领域的视角。我们关注一个称为Probabilistic Easy Variational Causal Effect (PEACE)的函数，它可以测量$X$对$Y$的直接因果效应，而同时改变$X$的值，但保持$Z$的值不变。PEACE是关于$d\ge 0$的一个函数，管理着概率密度值$f(x|z)$的强度。另一方面，我们将上述思想推广到离散情况，并展示其与连续情况的兼容性。此外，我们使用测度论概念研究了PEACE的一些性质。此外，我们提供了一些可辨识性标准。

    arXiv:2403.07745v1 Announce Type: cross  Abstract: Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the total variation and the flux of $g$, we develop a point of view in causal inference capable of dealing with a broad domain of causal problems. Indeed, we focus on a function, called Probabilistic Easy Variational Causal Effect (PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect to continuously and interventionally changing the values of $X$ while keeping the value of $Z$ constant. PEACE is a function of $d\ge 0$, which is a degree managing the strengths of probability density values $f(x|z)$. On the other hand, we generalize the above idea for the discrete case and show its compatibility with the continuous case. Further, we investigate some properties of PEACE using measure theoretical concepts. Furthermore, we provide some identifiability criteria and s
    
[^19]: 为计算病理学系统配备工件处理流水线：计算与性能权衡的展示

    Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs

    [https://arxiv.org/abs/2403.07743](https://arxiv.org/abs/2403.07743)

    提出了一种专家混合方案，用于在计算病理学系统中检测和排除五种显著的工件，并应用概率阈值处理。

    

    组织病理学是癌症诊断的黄金标准，在显微镜下进行检查。然而，组织病理学处理过程会产生一些工件，最终会转移到玻璃载玻片的数字化版本，即全玻幻灯片。工件是诊断无关的区域，可能导致错误的深度学习算法预测。因此，在计算病理学（CPATH）系统中检测和排除工件对于可靠的自动诊断至关重要。在本文中，我们提出了一种专家混合（MoE）方案，用于检测包括损坏组织、模糊、褶皱组织、气泡和在WSIs中的组织学无关血液等五种显著工件。首先，我们训练独立的二元DL模型作为专家来捕捉特定的工件形态。然后，我们使用融合机制来集成它们的预测。我们对最终的概率进行概率阈值处理

    arXiv:2403.07743v1 Announce Type: cross  Abstract: Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probabilit
    
[^20]: HSIC估计的极小化率对平移不变核

    The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels

    [https://arxiv.org/abs/2403.07735](https://arxiv.org/abs/2403.07735)

    HSIC估计的极小化率对平移不变核的独立性度量具有重要意义

    

    Kernel技术是数据科学和统计学中最有影响力的方法之一。在温和条件下，与核相关的再生核希尔伯特空间能够编码$M\ge 2$个随机变量的独立性。在核上依赖的最普遍的独立性度量可能是所谓的Hilbert-Schmidt独立性准则(HSIC; 在统计文献中也称为距离协方差)。尽管自近二十年前引入以来已经有各种现有的设计的HSIC估计量，HSIC可以被估计的速度的基本问题仍然是开放的。在这项工作中，我们证明了对于包含具有连续有界平移不变特征核的高斯Borel测度在$\mathbb R^d$上的HSIC估计的极小化最优速率是$\mathcal O\!\left(n^{-1/2}\right)$。具体地，我们的结果意味着许多方面在极小化意义上的最优性

    arXiv:2403.07735v1 Announce Type: cross  Abstract: Kernel techniques are among the most influential approaches in data science and statistics. Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of $M\ge 2$ random variables. Probably the most widespread independence measure relying on kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature). Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open. In this work, we prove that the minimax optimal rate of HSIC estimation on $\mathbb R^d$ for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is $\mathcal O\!\left(n^{-1/2}\right)$. Specifically, our result implies the optimality in the minimax sense of many 
    
[^21]: CAS: 一种具有FCR控制的在线选择性符合预测的通用算法

    CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control

    [https://arxiv.org/abs/2403.07728](https://arxiv.org/abs/2403.07728)

    CAS框架允许在在线选择性预测中控制FCR，通过自适应选择和校准集构造输出符合预测区间

    

    我们研究了在线方式下后选择预测推断的问题。为了避免将资源耗费在不重要的单位上，在报告其预测区间之前对当前个体进行初步选择在在线预测任务中是常见且有意义的。由于在线选择导致所选预测区间中存在时间多重性，因此控制实时误覆盖陈述率（FCR）来测量平均误覆盖误差是重要的。我们开发了一个名为CAS（适应性选择后校准）的通用框架，可以包裹任何预测模型和在线选择规则，以输出后选择的预测区间。如果选择了当前个体，我们首先对历史数据进行自适应选择来构建校准集，然后为未观察到的标签输出符合预测区间。我们为校准集提供了可行的构造方式

    arXiv:2403.07728v1 Announce Type: cross  Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for 
    
[^22]: 在数据限制的二分类中平衡公平性和准确性

    Balancing Fairness and Accuracy in Data-Restricted Binary Classification

    [https://arxiv.org/abs/2403.07724](https://arxiv.org/abs/2403.07724)

    研究提出了一个框架，直接分析最优贝叶斯分类器在数据限制的情况下的行为，以平衡准确性和公平性。

    

    处理敏感信息的应用可能对机器学习（ML）分类器可用数据设置限制。本文提出一个框架，模拟准确性和公平性之间的权衡，在四种实际情景下探讨可用于分析的数据类型。与先前研究通过分析经训练以隐式学习数据集的特征向量、类别标签和敏感属性的潜在分布的评分函数的输出来考虑这种权衡不同，我们的框架直接通过从数据集本身构建的离散近似来分析最优贝叶斯分类器在这个潜在分布上的行为。这种方法使我们能够制定多个凸优化问题，以更好地平衡准确性和公平性。

    arXiv:2403.07724v1 Announce Type: cross  Abstract: Applications that deal with sensitive information may have restrictions placed on the data available to a machine learning (ML) classifier. For example, in some applications, a classifier may not have direct access to sensitive attributes, affecting its ability to produce accurate and fair decisions. This paper proposes a framework that models the trade-off between accuracy and fairness under four practical scenarios that dictate the type of data available for analysis. Prior works examine this trade-off by analyzing the outputs of a scoring function that has been trained to implicitly learn the underlying distribution of the feature vector, class label, and sensitive attribute of a dataset. In contrast, our framework directly analyzes the behavior of the optimal Bayesian classifier on this underlying distribution by constructing a discrete approximation it from the dataset itself. This approach enables us to formulate multiple convex 
    
[^23]: 关于洗牌梯度方法的最后迭代收敛性

    On the Last-Iterate Convergence of Shuffling Gradient Methods

    [https://arxiv.org/abs/2403.07723](https://arxiv.org/abs/2403.07723)

    该论文证明了针对目标函数的洗牌梯度方法最后迭代的收敛速率，弥合了在不同设置中最后迭代的良好性能与现有理论之间的差距。

    

    洗牌梯度方法，也被称为无替换的随机梯度下降（SGD），在实践中被广泛应用，特别包括三种流行算法：Random Reshuffle（RR）、Shuffle Once（SO）和Incremental Gradient（IG）。与经验成功相比，长期以来对于洗牌梯度方法的理论保证并不充分了解。最近，只为凸函数的平均迭代和强凸问题的最后迭代（以平方距离为度量）建立了收敛速率。然而，当将函数值差作为收敛准则时，现有理论无法解释在不同设置中（例如受约束的优化）最后迭代的良好性能。为了弥合这种实践与理论之间的差距，我们针对目标函数证明了洗牌梯度方法最后迭代的收敛速率。

    arXiv:2403.07723v1 Announce Type: new  Abstract: Shuffling gradient methods, which are also known as stochastic gradient descent (SGD) without replacement, are widely implemented in practice, particularly including three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understanding for a long time. Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric). However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization). To bridge this gap between practice and theory, we prove last-iterate convergence rates for shuffling gradient methods with respect to the objectiv
    
[^24]: WorkArena：Web代理在解决常见知识工作任务中的能力如何？

    WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?

    [https://arxiv.org/abs/2403.07718](https://arxiv.org/abs/2403.07718)

    该研究探究了基于大型语言模型的代理在通过web浏览器与软件交互时的能力，提出了WorkArena和BrowserGym两个工具，在29个任务的基准测试中显示出潜力，但也揭示了实现完全任务自动化仍存在挑战。

    

    我们研究了基于大型语言模型的代理与软件通过web浏览器交互的应用。与先前的研究不同，我们关注衡量这些代理执行任务的能力，这些任务涵盖了利用企业软件系统的知识工作者的典型日常工作。为此，我们提出了WorkArena，一个基于广泛使用的ServiceNow平台的29个任务的远程主机基准。我们还介绍了BrowserGym，这是一个用于设计和评估这些代理的环境，提供了丰富的行为和多模态观察。我们的实证评估显示，尽管当前的代理在WorkArena上表现出了潜力，但要实现完全任务自动化仍存在相当大的差距。值得注意的是，我们的分析揭示了开源和闭源LLMs之间显著的性能差距，突出了未来探索和发展领域的一个重要领域。

    arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
    
[^25]: 点云网络的快速简单可解释性

    Fast and Simple Explainability for Point Cloud Networks

    [https://arxiv.org/abs/2403.07706](https://arxiv.org/abs/2403.07706)

    该方法提出了一种基于特征的解释（FBI）方法，通过计算每个点在瓶颈层之前的特征范数，实现了与当前XAI方法至少三个数量级的速度提升，适用于大型点云或大规模架构。

    

    我们提出了一种针对点云数据的快速简单可解释的人工智能（XAI）方法。它计算了相对于已经训练好的网络下游任务的每个点的重要性。这有助于更好地理解网络的特性，对于安全关键应用至关重要。除了调试和可视化之外，我们的低计算复杂度有助于在线反馈到网络进行推断。这可以用于减少不确定性并提高鲁棒性。在这项工作中，我们引入了“基于特征的解释”（FBI），在瓶颈层之前计算每个点的特征范数。我们分析了梯度的使用以及后瓶颈和前瓶颈策略，结果显示前瓶颈更受青睐，从平滑度和排名角度来看。与当前的XAI方法相比，我们实现了至少三个数量级的速度提升，因此适用于大型点云或大规模架构。我们的方法实现了SOTA水平。

    arXiv:2403.07706v1 Announce Type: cross  Abstract: We propose a fast and simple explainable AI (XAI) method for point cloud data. It computes pointwise importance with respect to a trained network downstream task. This allows better understanding of the network properties, which is imperative for safety-critical applications. In addition to debugging and visualization, our low computational complexity facilitates online feedback to the network at inference. This can be used to reduce uncertainty and to increase robustness. In this work, we introduce \emph{Feature Based Interpretability} (FBI), where we compute the features' norm, per point, before the bottleneck. We analyze the use of gradients and post- and pre-bottleneck strategies, showing pre-bottleneck is preferred, in terms of smoothness and ranking. We obtain at least three orders of magnitude speedup, compared to current XAI methods, thus, scalable for big point clouds or large-scale architectures. Our approach achieves SOTA re
    
[^26]: 对称 Q-learning：减少在线强化学习中贝尔曼误差的偏斜

    Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning

    [https://arxiv.org/abs/2403.07704](https://arxiv.org/abs/2403.07704)

    对称 Q-learning方法通过添加合成噪声来减少贝尔曼误差的偏斜，在在线强化学习中提高了样本效率。

    

    在深度强化学习中，估计值函数以评估状态和动作的质量是至关重要的。该值函数通常使用最小二乘法进行训练，隐含地假定一个高斯误差分布。然而，最近的研究表明，由于贝尔曼算子的特性，用于训练值函数的误差分布通常是倾斜的，违反了最小二乘法中对正态误差分布的隐含假设。为了解决这个问题，我们提出了一种称为对称 Q-learning 的方法，其中从零均值分布生成的合成噪声被添加到目标值中，以生成高斯误差分布。我们在MuJoCo的连续控制基准任务上评估了所提出的方法。通过减少错误分布的偏斜，它提高了一种最先进的强化学习方法的样本效率。

    arXiv:2403.07704v1 Announce Type: cross  Abstract: In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control benchmark tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution.
    
[^27]: Maxwell的恶魔之工作：通过利用神经元饱和实现有效修剪

    Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons

    [https://arxiv.org/abs/2403.07688](https://arxiv.org/abs/2403.07688)

    重新评估深度神经网络中的死亡神经元现象，提出了Demon Pruning（DemP）方法，通过控制死亡神经元的产生，动态实现网络稀疏化。

    

    在训练深度神经网络时，$\textit{死亡神经元}$现象——在训练期间变得不活跃或饱和，输出为零的单元—传统上被视为不可取的，与优化挑战有关，并导致在不断学习的情况下丧失可塑性。本文重新评估了这一现象，专注于稀疏性和修剪。通过系统地探索各种超参数配置对死亡神经元的影响，我们揭示了它们有助于促进简单而有效的结构化修剪算法的潜力。我们提出了$\textit{Demon Pruning}$（DemP），一种控制死亡神经元扩张，动态导致网络稀疏性的方法。通过在活跃单元上注入噪声和采用单周期调度正则化策略的组合，DemP因其简单性和广泛适用性而脱颖而出。在CIFAR10上的实验中...

    arXiv:2403.07688v1 Announce Type: cross  Abstract: When training deep neural networks, the phenomenon of $\textit{dying neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero during training$\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 an
    
[^28]: 用于足球比赛结果预测的机器学习

    Machine Learning for Soccer Match Result Prediction

    [https://arxiv.org/abs/2403.07669](https://arxiv.org/abs/2403.07669)

    机器学习在足球比赛结果预测中取得了显著进展，目前最佳表现的模型是梯度提升树模型（如CatBoost）应用于足球特定评分，但仍需进一步比较深度学习模型和随机森林模型在不同数据集上的表现。

    

    机器学习已经成为预测足球比赛结果的常用方法，这一领域的文献数量在过去十五年中大幅增长。本章讨论了可用数据集、模型类型和特征，以及评估该应用领域中模型性能的方法。本章旨在广泛概述当前机器学习用于足球比赛结果预测的现状和潜在未来发展，供有兴趣在该领域开展未来研究的人参考。我们的主要发现是，虽然梯度提升树模型（如CatBoost）应用于诸如pi-评分之类的足球特定评分，目前是在只包含进球作为比赛特征的数据集上表现最佳的模型，但需要对不同数据集上深度学习模型和随机森林模型的性能进行更彻底的比较。

    arXiv:2403.07669v1 Announce Type: new  Abstract: Machine learning has become a common approach to predicting the outcomes of soccer matches, and the body of literature in this domain has grown substantially in the past decade and a half. This chapter discusses available datasets, the types of models and features, and ways of evaluating model performance in this application domain. The aim of this chapter is to give a broad overview of the current state and potential future developments in machine learning for soccer match results prediction, as a resource for those interested in conducting future studies in the area. Our main findings are that while gradient-boosted tree models such as CatBoost, applied to soccer-specific ratings such as pi-ratings, are currently the best-performing models on datasets containing only goals as the match features, there needs to be a more thorough comparison of the performance of deep learning models and Random Forest on a range of datasets with differen
    
[^29]: 使用贝叶斯神经场进行可扩展的时空预测

    Scalable Spatiotemporal Prediction with Bayesian Neural Fields

    [https://arxiv.org/abs/2403.07657](https://arxiv.org/abs/2403.07657)

    该论文提出了贝叶斯神经场（BayesNF），结合了深度神经网络和分层贝叶斯推断，用于处理大规模时空预测问题。

    

    时空数据集由空间参考的时间序列表示，广泛应用于许多科学和商业智能领域，例如空气污染监测，疾病跟踪和云需求预测。随着现代数据集规模和复杂性的不断增加，需要新的统计方法来捕捉复杂的时空动态并处理大规模预测问题。本研究介绍了Bayesian Neural Field (BayesNF)，这是一个用于推断时空域上丰富概率分布的通用领域统计模型，可用于包括预测、插值和变异分析在内的数据分析任务。BayesNF将用于高容量函数估计的新型深度神经网络架构与用于鲁棒不确定性量化的分层贝叶斯推断相结合。通过在定义先验分布方面进行序列化

    arXiv:2403.07657v1 Announce Type: cross  Abstract: Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequenc
    
[^30]: 较困难的任务需要更多专家：MoE模型中的动态路由

    Harder Tasks Need More Experts: Dynamic Routing in MoE Models

    [https://arxiv.org/abs/2403.07652](https://arxiv.org/abs/2403.07652)

    通过动态选择专家来提高计算效率和模型性能，针对不同难度的任务激活不同数量的专家，相比传统的Top-K路由方法，我们的动态路由方法在各种基准测试中取得了明显的改进。

    

    在本文中，我们引入了一种新颖的动态专家选择框架，用于Mixture of Experts（MoE）模型，旨在通过根据输入难度调整激活的专家数量，增强计算效率和模型性能。与依赖于固定Top-K路由的传统MoE方法不同，该方法根据对每个输入的专家选择的置信水平动态选择专家。这允许更有效地利用计算资源，对需要高级推理的复杂任务激活更多的专家，对较简单的任务激活更少的专家。通过广泛的评估，我们的动态路由方法在各种基准测试中表现出明显的改进，与常规Top-2路由相比，实现了平均改进0.7%的效果，且激活参数少于90%。进一步的分析显示我们的模型

    arXiv:2403.07652v1 Announce Type: cross  Abstract: In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model 
    
[^31]: 大型语言模型在数据中心开发的特征化研究

    Characterization of Large Language Model Development in the Datacenter

    [https://arxiv.org/abs/2403.07648](https://arxiv.org/abs/2403.07648)

    本研究对大型语言模型的开发工作负载进行了深入特征化研究，发现了与先前任务特定深度学习工作负载的差异，探索了资源利用模式，并提出了优化系统以适应LLMs的潜在机会。

    

    大型语言模型（LLMs）在多个革命性任务上展现出了令人印象深刻的性能。然而，要有效利用大规模集群资源来开发LLMs并非易事，经常面临诸多挑战，如频繁的硬件故障、复杂的并行化策略和资源利用不平衡。本文对我们的GPU数据中心Acme中收集的为期六个月的LLM开发工作负载跟踪进行了深入的特征化研究。具体地，我们调查了LLMs与先前任务特定的深度学习（DL）工作负载之间的差异，探索了资源利用模式，并确定了各种作业故障的影响。我们的分析总结了我们遇到的障碍，并发现了优化专为LLMs定制的系统的潜在机会。此外，我们介绍了我们的系统努力：（1）容错预训练，通过LLM参与来增强容错能力。

    arXiv:2403.07648v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have presented impressive performance across several transformative tasks. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. In this paper, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures. Our analysis summarizes hurdles we encountered and uncovers potential opportunities to optimize systems tailored for LLMs. Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining, which enhances fault tolerance through LLM-involved 
    
[^32]: CardioGenAI：基于机器学习的框架用于减少hERG毒性的药物再设计

    CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability

    [https://arxiv.org/abs/2403.07632](https://arxiv.org/abs/2403.07632)

    提出了CardioGenAI，一个基于机器学习的框架，用于减少药物的hERG活性并保留药理活性。

    

    药物诱导的心脏毒性是一个重要的健康问题，可能导致严重不良反应，包括通过阻滞电压门控的hERG钾离子通道导致生命威胁的心律失常。因此，开发早期阶段鉴定hERG活性化合物的先进方法，以及优化商业化药物以减少hERG活性非常重要。在这项工作中，我们提出了CardioGenAI，这是一个基于机器学习的框架，用于再设计开发中和已上市药物，以减少hERG活性同时保留其药理活性。该框架结合了用于预测hERG通道活性的最新判别模型，以及钠离子通道NaV1.5和钙离子通道CaV1.2活性，因为它们在调节由hERG通道阻滞引起的心律失常潜在影响中具有潜在意义。这些模型还可以se

    arXiv:2403.07632v1 Announce Type: new  Abstract: Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel. It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity. In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity. The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade. These models can also se
    
[^33]: generAItor: 树-环路文本生成用于语言模型的可解释性和适应性

    generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation

    [https://arxiv.org/abs/2403.07627](https://arxiv.org/abs/2403.07627)

    提出了一种树-环路方法，通过将束搜索树与各种小部件相结合，提供了可视化和交互可能性，从而分析、解释和调整生成的输出。

    

    大语言模型（LLMs）广泛部署在各种下游任务中，例如自动完成、辅助写作或基于对话的文本生成。然而，搜索算法的输出候选结果被较少探索和解释。我们通过提出一种树-环路方法来解决这一不足，其中束搜索树的可视化表示是分析、解释和调整生成的输出的核心组件。为支持这些任务，我们提出了generAItor，一种视觉分析技术，将中心束搜索树与各种特定于任务的小部件相结合，提供有针对性的可视化和交互可能性。我们的方法允许在多个层面进行交互，并提供一个迭代流程，包括生成、探索和比较输出的候选结果，以及根据适应数据微调模型。我们的案例研究表明我们的工具 generat

    arXiv:2403.07627v1 Announce Type: cross  Abstract: Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation. However, the considered output candidates of the underlying search algorithm are under-explored and under-explained. We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs. To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities. Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data. Our case study shows that our tool generat
    
[^34]: 通过逐层部分机器遗忘实现训练模型中的有效知识删除

    Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning

    [https://arxiv.org/abs/2403.07611](https://arxiv.org/abs/2403.07611)

    该论文介绍了一种新颖的机器遗忘算法，分别采用部分失忆式遗忘和逐层部分更新的方法，以更高效地在训练模型中删除知识。

    

    arXiv:2403.07611v1 发表类型：cross  摘要：机器遗忘因其能够有选择地擦除已经训练的机器学习模型中从特定训练数据样本获得的知识而受到广泛关注。这种能力使数据持有者能够严格遵守数据保护法规。然而，现有的遗忘技术面临实际约束，通常导致性能下降，需要遗忘后进行简短的微调，并需要大量存储空间。作为回应，本文引入了一种新颖的机器遗忘算法。第一种方法是部分失忆式遗忘，将逐层修剪与失忆式遗忘相结合。在这种方法中，训练过程中对模型进行的更新被修剪并存储，随后用于从训练模型中遗忘特定数据。第二种方法将逐层部分更新集成到标签翻转和基于优化的遗忘中，以减轻由于遗忘而产生的不利影响。

    arXiv:2403.07611v1 Announce Type: cross  Abstract: Machine unlearning has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained machine learning model. This capability enables data holders to adhere strictly to data protection regulations. However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief fine-tuning post unlearning, and requiring significant storage. In response, this paper introduces a novel class of machine unlearning algorithms. First method is partial amnesiac unlearning, integration of layer-wise pruning with amnesiac unlearning. In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model. The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of da
    
[^35]: Couler: 云中统一机器学习工作流优化

    Couler: Unified Machine Learning Workflow Optimization in Cloud

    [https://arxiv.org/abs/2403.07608](https://arxiv.org/abs/2403.07608)

    设计并实现了Couler系统，用于云中统一机器学习工作流优化，主要见解在于能够使用自然生成ML工作流

    

    机器学习（ML）已经变得无处不在，推动着各种组织中的数据驱动应用。与传统观念中研究中的ML相反，ML工作流可能是复杂的，资源密集的，并且耗时的。扩展ML工作流以包含更广泛的数据基础设施和数据类型可能导致更大的工作量和增加的部署成本。目前，有许多工作流引擎可用（其中超过十个得到广泛认可）。这种多样性对于最终用户来说构成了掌握不同引擎API的挑战。虽然目前的努力主要集中在针对特定工作流引擎优化ML操作（MLOps），但当前方法在跨不同引擎进行工作流优化方面很大程度上被忽视。

    arXiv:2403.07608v1 Announce Type: cross  Abstract: Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations. Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming. Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. Currently, numerous workflow engines are available (with over ten being widely recognized). This variety poses a challenge for end-users in terms of mastering different engine APIs. While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.   In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud. Our main insight lies in the ability to generate an ML workflow using natural 
    
[^36]: 优化负面提示以增强文本到图像生成中的美学和保真度

    Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation

    [https://arxiv.org/abs/2403.07605](https://arxiv.org/abs/2403.07605)

    提出NegOpt方法，通过监督微调和强化学习优化负面提示的生成，显著提高图像生成质量，超越其他方法并构建了负面提示数据集。

    

    在文本到图像生成中，使用描述不良图像特征的负面提示可以显著提高图像质量。然而，生成良好的负面提示是一项手工而繁琐的工作。为了解决这个问题，我们提出了NegOpt，一种新颖的方法，通过监督微调和强化学习来优化负面提示生成，从而增强图像生成。我们的综合方法相对于其他方法大幅提高了25%的Inception Score，并超越了来自测试集的标准负面提示。此外，使用NegOpt，我们可以有选择地优化对我们最重要的指标。最后，我们构建了负面提示数据集Negative Prompts DB。

    arXiv:2403.07605v1 Announce Type: cross  Abstract: In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.
    
[^37]: ProPML: 概率部分多标签学习

    ProPML: Probability Partial Multi-label Learning

    [https://arxiv.org/abs/2403.07603](https://arxiv.org/abs/2403.07603)

    ProPML是一种新颖的概率方法，用于扩展二元交叉熵到Partial Multi-label Learning（PML）设置中，可以应用于任何深度架构，并且在人工和真实数据集上的实验证明其优于现有方法，尤其适用于候选集中高噪声的情况。

    

    Partial Multi-label Learning (PML)是一种弱监督学习类型，其中每个训练实例对应于一组候选标签，其中只有一些是真实的。在本文中，我们引入了ProPML，这是一种新颖的概率方法，用于扩展二元交叉熵到PML设置中。与现有方法相比，它不需要次优的消歧，因此可以应用于任何深度架构。此外，在人工和真实数据集上进行的实验表明，ProPML在候选集中存在高噪声时优于现有方法。

    arXiv:2403.07603v1 Announce Type: new  Abstract: Partial Multi-label Learning (PML) is a type of weakly supervised learning where each training instance corresponds to a set of candidate labels, among which only some are true. In this paper, we introduce \our{}, a novel probabilistic approach to this problem that extends the binary cross entropy to the PML setup. In contrast to existing methods, it does not require suboptimal disambiguation and, as such, can be applied to any deep architecture. Furthermore, experiments conducted on artificial and real-world datasets indicate that \our{} outperforms existing approaches, especially for high noise in a candidate set.
    
[^38]: 强化和增强无训练神经架构搜索

    Robustifying and Boosting Training-Free Neural Architecture Search

    [https://arxiv.org/abs/2403.07591](https://arxiv.org/abs/2403.07591)

    提出了RoBoT算法，旨在解决无训练神经架构搜索中指标估计不准确和性能限制的问题

    

    神经架构搜索（NAS）已成为AutoML的关键组件和自动设计深度神经网络的标准工具。最近，作为新兴范式的无训练NAS通过仅使用无训练指标估计真实架构性能，成功降低了标准基于训练NAS的搜索成本。然而，这些指标的估计能力通常在不同任务间变化，使得仅使用单一无训练指标在多样任务上实现稳健和一致良好的搜索性能具有挑战性。与此同时，无训练指标与真实架构性能之间的估计差距限制了无训练NAS实现卓越性能。为解决这些挑战，我们提出了强化和增强无训练NAS（RoBoT）算法，该算法（a）利用从贝叶斯优化中探索出的现有无训练指标的优化组合

    arXiv:2403.07591v1 Announce Type: new  Abstract: Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks. Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics. Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric. Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance. To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimizat
    
[^39]: 基于扩散模型的视觉隐私审计

    Visual Privacy Auditing with Diffusion Models

    [https://arxiv.org/abs/2403.07588](https://arxiv.org/abs/2403.07588)

    在这项研究中，通过使用扩散模型进行重建攻击，作者发现在DP-SGD下，真实世界的数据先验对于重建成功具有显著影响。

    

    arXiv:2403.07588v1 声明类型: 新的 摘要: 对机器学习模型的图像重建攻击可能会导致泄露敏感信息，从而对隐私构成重大风险。虽然使用差分隐私(DP)来抵御此类攻击已被证明是有效的，但确定适当的DP参数仍然具有挑战性。当前对数据重建成功的形式化保证受到了关于对手对目标数据的了解的过于理论化的假设的影响，特别是在图像领域。在这项工作中，我们通过实证调查这一差异，并发现这些假设的实际性在很大程度上取决于数据先验和重建目标之间的域转移。我们提出了一种基于扩散模型(DMs)的重建攻击，假设对手可以访问真实世界的图像先验，并评估其对在DP-SGD下的隐私泄露的影响。我们展示了(1)真实世界的数据先验显著影响重建成功，

    arXiv:2403.07588v1 Announce Type: new  Abstract: Image reconstruction attacks on machine learning models pose a significant risk to privacy by potentially leaking sensitive information. Although defending against such attacks using differential privacy (DP) has proven effective, determining appropriate DP parameters remains challenging. Current formal guarantees on data reconstruction success suffer from overly theoretical assumptions regarding adversary knowledge about the target data, particularly in the image domain. In this work, we empirically investigate this discrepancy and find that the practicality of these assumptions strongly depends on the domain shift between the data prior and the reconstruction target. We propose a reconstruction attack based on diffusion models (DMs) that assumes adversary access to real-world image priors and assess its implications on privacy leakage under DP-SGD. We show that (1) real-world data priors significantly influence reconstruction success, 
    
[^40]: 在模拟家庭环境中联邦学习社会适宜的机器人行为

    Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments

    [https://arxiv.org/abs/2403.07586](https://arxiv.org/abs/2403.07586)

    本研究提出了在模拟家庭环境中进行联邦学习的新方法，旨在评估机器人行为的社会适宜性，并结合持续学习方法，使机器人可以从彼此的经验中学习社会规范。

    

    随着社交机器人越来越多地融入日常生活，确保它们的行为符合社会规范变得至关重要。为了广泛应用于开放世界，探索个体机器人可以在学习其独特环境的同时也从彼此的经验中学习的联邦学习（FL）设置非常重要。本文提出了一个新颖的FL基准，评估不同策略，使用多标签回归目标，其中每个客户端分别学习预测不同机器人行为的社会适宜性，同时与他人共享其学习。此外，通过将训练数据按不同上下文拆分，使每个客户端逐渐跨上下文学习，我们提出了一个新颖的联邦持续学习（FCL）基准，将FL方法调整为使用最先进的持续学习（CL）方法，以持续学习社会适宜的代理行为。

    arXiv:2403.07586v1 Announce Type: cross  Abstract: As social robots become increasingly integrated into daily life, ensuring their behaviours align with social norms is crucial. For their widespread open-world application, it is important to explore Federated Learning (FL) settings where individual robots can learn about their unique environments while also learning from each others' experiences. In this paper, we present a novel FL benchmark that evaluates different strategies, using multi-label regression objectives, where each client individually learns to predict the social appropriateness of different robot actions while also sharing their learning with others. Furthermore, splitting the training data by different contexts such that each client incrementally learns across contexts, we present a novel Federated Continual Learning (FCL) benchmark that adapts FL-based methods to use state-of-the-art Continual Learning (CL) methods to continually learn socially appropriate agent behav
    
[^41]: 分布式训练的通信优化：架构、进展和机遇

    Communication Optimization for Distributed Training: Architecture, Advances, and Opportunities

    [https://arxiv.org/abs/2403.07585](https://arxiv.org/abs/2403.07585)

    本文介绍了分布式深度神经网络训练的通信优化架构，并对并行化策略、集体通信库和网络关系进行了分析，总结了当前的研究进展。

    

    近年来，大规模深度神经网络模型的蓬勃发展，参数量不断增长。训练这些大规模模型通常需要庞大的内存和计算资源，超出了单个GPU的范围，需要进行分布式训练。由于近年来GPU性能迅速发展，计算时间缩短，因此通信在整体训练时间中的比例增加。因此，优化分布式训练的通信已经成为一个紧迫问题。本文简要介绍了分布式深度神经网络训练的总体架构，并从通信优化的角度分析了并行化策略、集体通信库和网络之间的关系，形成了一个三层范式。然后，我们回顾了当前具有代表性的研究进展与这个三层范式。我们发现lay

    arXiv:2403.07585v1 Announce Type: cross  Abstract: The past few years have witnessed the flourishing of large-scale deep neural network models with ever-growing parameter numbers. Training such large-scale models typically requires massive memory and computing resources that exceed those of a single GPU, necessitating distributed training. As GPU performance has rapidly evolved in recent years, computation time has shrunk, thereby increasing the proportion of communication in the overall training time. Therefore, optimizing communication for distributed training has become an urgent issue. In this article, we briefly introduce the general architecture of distributed deep neural network training and analyze relationships among Parallelization Strategy, Collective Communication Library, and Network from the perspective of communication optimization, which forms a three-layer paradigm. We then review current representative research advances with this three-layer paradigm. We find that lay
    
[^42]: 迈向具有可适应性计算和网络融合的动态未来（ACNC）

    Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)

    [https://arxiv.org/abs/2403.07573](https://arxiv.org/abs/2403.07573)

    本文提出了可适应性CNC（ACNC）的概念，作为一种自主的机器学习（ML）辅助机制，旨在联合编排计算和网络资源，满足对动态和大量用户请求的严格要求。

    

    在推进6G的背景下，预计会出现实质性的范式转变，突出了由大量连接和严格遵守服务质量/体验（QoS/E）先决条件所特征化的全面的一切对一切交互。即将面临的挑战源于资源稀缺，促使有意识地向计算-网络融合（CNC）过渡，作为联合资源编排的有前途的方法。虽然基于CNC的机制引起了人们的关注，但它们在实现未来服务方面的有效性，特别是在类似Metaverse的使用情景中，可能会由于用户、服务和资源不断变化的特性而受到限制。因此，本文提出了可适应性CNC（ACNC）的概念，作为一种自主的机器学习（ML）辅助机制，旨在联合编排计算和网络资源，满足对动态和大量用户请求的严格要求。

    arXiv:2403.07573v1 Announce Type: cross  Abstract: In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent r
    
[^43]: 探究单台场地运动记录的深度学习挑战

    Exploring Challenges in Deep Learning of Single-Station Ground Motion Records

    [https://arxiv.org/abs/2403.07569](https://arxiv.org/abs/2403.07569)

    本研究旨在评估深度学习模型从场地运动记录中学习的效果，并探讨辅助信息对此过程的影响。

    

    当代的深度学习模型在地震学和地震工程的各种应用中展现出了令人期待的结果。这些模型主要依赖利用场地运动记录进行地震事件分类、定位、地震早期预警系统和结构健康监测等任务。然而，这些模型从这些复杂的时间序列信号中有效学习的程度尚未得到彻底分析。本研究的目标是评估辅助信息（如地震相到达时间或网络内地震台站分布）在从场地运动记录中进行深度学习过程中的主导程度，可能会影响其有效性。我们对两种深度学习模型进行超参数搜索，评估它们在从场地运动记录中进行深度学习的有效性，同时检查辅助信息的影响。

    arXiv:2403.07569v1 Announce Type: cross  Abstract: Contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering. These models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring. However, the extent to which these models effectively learn from these complex time-series signals has not been thoroughly analyzed. In this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness. We perform a hyperparameter search on two deep learning models to assess their effectiveness in deep learning from ground motion records while also examining the impact of auxiliary information o
    
[^44]: 学习移动操作的通用特征场

    Learning Generalizable Feature Fields for Mobile Manipulation

    [https://arxiv.org/abs/2403.07563](https://arxiv.org/abs/2403.07563)

    提出了GeFF（通用特征场），作为导航和操作的统一表示，可以实时执行，通过将生成的丰富场景先验与自然语言对齐来提高效果。

    

    移动操作中的一个悬而未决的问题是如何以统一的方式表示物体和场景，使得机器人可以同时用于在环境中导航和操作物体。本工作提出了GeFF（通用特征场），这是一个场景级的通用神经特征场，作为导航和操作的统一表示，可以实时执行。为此，我们将生成新视图合成视为一个预训练任务，然后通过CLIP特征提炼将生成的丰富场景先验与自然语言对齐。

    arXiv:2403.07563v1 Announce Type: cross  Abstract: An open problem in mobile manipulation is how to represent objects and scenes in a unified manner, so that robots can use it both for navigating in the environment and manipulating objects. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherit to an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's ability to generalize to open-set objects as 
    
[^45]: Jupyter笔记本中的机器学习项目灵活单元分类

    A Flexible Cell Classification for ML Projects in Jupyter Notebooks

    [https://arxiv.org/abs/2403.07562](https://arxiv.org/abs/2403.07562)

    本文提出了一种基于混合分类方法的更灵活的单元分类方法，结合了基于规则和决策树分类器。我们讨论了设计原理，并详细描述了开发的分类器。

    

    Jupyter Notebook是一个常用的交互式开发环境，用于快速实验机器学习（ML）解决方案。描述代码单元中执行的ML活动可提高笔记本的可读性和理解性。手动注释代码单元耗时且容易出错。因此，已开发出工具，根据在其中执行的ML活动对笔记本的单元进行分类。然而，目前的工具缺乏灵活性，因为它们基于已创建的查找表工作，该查找表将常用ML库的函数调用映射到ML活动。这些表必须手动调整以考虑新的或更改的库。

    arXiv:2403.07562v1 Announce Type: cross  Abstract: Jupyter Notebook is an interactive development environment commonly used for rapid experimentation of machine learning (ML) solutions. Describing the ML activities performed along code cells improves the readability and understanding of Notebooks. Manual annotation of code cells is time-consuming and error-prone. Therefore, tools have been developed that classify the cells of a notebook concerning the ML activity performed in them. However, the current tools are not flexible, as they work based on look-up tables that have been created, which map function calls of commonly used ML libraries to ML activities. These tables must be manually adjusted to account for new or changed libraries.   This paper presents a more flexible approach to cell classification based on a hybrid classification approach that combines a rule-based and a decision tree classifier. We discuss the design rationales and describe the developed classifiers in detail. 
    
[^46]: 为多智能体路径规划集成优先级混合策略

    Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding

    [https://arxiv.org/abs/2403.07559](https://arxiv.org/abs/2403.07559)

    提出了Ensembling Prioritized Hybrid Policies (EPH)方法，通过选择性通信模块和三种高级推理策略，提高了基于通信的多智能体路径规划解决方案的性能。

    

    基于多智能体强化学习（MARL）的多智能体路径规划（MAPF）近来因其高效性和可扩展性而受到关注。我们提出了一种新方法，Ensembling Prioritized Hybrid Policies (EPH)，以进一步提高基于通信的MARL-MAPF求解器的性能。我们首先提出了一个选择性通信模块，以在多智能体环境中收集更丰富的信息，从而实现更好的智能体协调，并使用基于Q-learning的算法对模型进行训练。

    arXiv:2403.07559v1 Announce Type: cross  Abstract: Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Se
    
[^47]: SIFiD：使用LLM重新评估摘要的事实不一致性检测

    SIFiD: Reassess Summary Factual Inconsistency Detection with LLM

    [https://arxiv.org/abs/2403.07557](https://arxiv.org/abs/2403.07557)

    本研究重新评估了使用LLM进行摘要不一致性检测的方法，提出了SIFiD（带有过滤文档的摘要不一致性检测），旨在通过自然语言推理或测量语义相似性来识别文档中的关键句子。

    

    确保摘要与原始文档之间的事实一致性在摘要任务中至关重要。因此，人们致力于检测不一致性。随着大型语言模型（LLMs）的出现，最近的研究开始利用它们先进的语言理解能力进行不一致性检测。然而，早期尝试表明，由于LLMs有限的遵循指令能力和缺乏有效的检测方法论，它们的性能不及传统模型。在这项研究中，我们使用GPT-3.5和GPT-4比较LLMs的摘要不一致性检测表现，以推动基于LLM的不一致性检测研究。我们提出了SIFiD（带有过滤文档的摘要不一致性检测），它通过使用自然语言推理或测量摘要和文档之间的语义相似性来识别文档中的关键句子。

    arXiv:2403.07557v1 Announce Type: new  Abstract: Ensuring factual consistency between the summary and the original document is paramount in summarization tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documen
    
[^48]: 互动指令跟随代理的在线持续学习

    Online Continual Learning For Interactive Instruction Following Agents

    [https://arxiv.org/abs/2403.07548](https://arxiv.org/abs/2403.07548)

    我们提出了针对具身代理的两种持续学习设置：学习新行为和新环境。同时，我们通过自信度得分来更新存储的信息，从而避免需要任务边界信息的问题。

    

    在通过语言指令执行日常任务的具身代理学习过程中，文献大都假定代理在开始时就学习所有训练数据。我们认为这样的学习场景较不现实，因为机器人代理应该在探索和感知世界的过程中不断地学习。为了朝着更真实的具身代理学习场景迈进一步，我们提出了两种持续学习设置供具身代理使用；学习新行为（行为增量学习，Behavior-IL）和新环境（环境增量学习，Environment-IL）。在任务中，先前基于“数据先验”的持续学习方法维护过去任务的logits。然而，存储的信息往往是不充分学习的信息，需要任务边界信息，而这种信息并不总是可用。在这里，我们提议基于自信度得分而无需任务边界信息来更新它们。

    arXiv:2403.07548v1 Announce Type: new  Abstract: In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information d
    
[^49]: 自动驾驶中视觉Transformer的调研：当前趋势与未来发展方向

    A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions

    [https://arxiv.org/abs/2403.07542](https://arxiv.org/abs/2403.07542)

    视觉Transformer模型在自动驾驶中的成功应用表明其在全局上下文捕捉方面的优势，对于实时、动态视觉场景处理具有关键意义。

    

    这项调研探讨了在自动驾驶中采用视觉Transformer模型的适应性，这是受到它们在自然语言处理中取得成功的启发。Transformer模型在连续图像处理等任务中超越了传统的循环神经网络，并在全局上下文捕捉方面表现出色，如在复杂场景识别中表现优异，因此在计算机视觉领域中得到了认可。这些能力在自动驾驶中的实时、动态视觉场景处理中至关重要。我们的调研全面概述了自动驾驶中视觉Transformer的应用，重点关注自注意力、多头注意力和编码器-解码器架构等基础概念。我们涵盖了目标检测、分割、行人检测、车道检测等各种应用，比较它们的架构优点和局限性。调研最后探讨了未来的研究方向。

    arXiv:2403.07542v1 Announce Type: cross  Abstract: This survey explores the adaptation of visual transformer models in Autonomous Driving, a transition inspired by their success in Natural Language Processing. Surpassing traditional Recurrent Neural Networks in tasks like sequential image processing and outperforming Convolutional Neural Networks in global context capture, as evidenced in complex scene recognition, Transformers are gaining traction in computer vision. These capabilities are crucial in Autonomous Driving for real-time, dynamic visual scene processing. Our survey provides a comprehensive overview of Vision Transformer applications in Autonomous Driving, focusing on foundational concepts such as self-attention, multi-head attention, and encoder-decoder architecture. We cover applications in object detection, segmentation, pedestrian detection, lane detection, and more, comparing their architectural merits and limitations. The survey concludes with future research directio
    
[^50]: LaB-GATr：大规模生物医学表面和体积网格的几何代数变换器

    LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes

    [https://arxiv.org/abs/2403.07536](https://arxiv.org/abs/2403.07536)

    LaB-GATr 是一种几何代数变换器神经网络，通过序列压缩和插值有效地学习大规模生物医学表面和体积网格，扩展了传统的 GATr 方法并尊重了欧几里得对称性，达到了最先进的结果。

    

    许多解剖结构可以用表面或体积网格来描述。机器学习是从这些3D模型中提取信息的一种有前途的工具。然而，高保真度的网格通常包含成千上万个顶点，这在构建深度神经网络架构时带来了独特的挑战。此外，患者特异性网格可能没有经典对齐，这限制了机器学习算法的泛化。我们提出了LaB-GATr，一种具有几何标记化的转换器神经网络，通过序列压缩和插值有效地学习大规模（生物）医学表面和体积网格。我们的方法扩展了最近提出的几何代数变换器（GATr），因此尊重所有欧几里得对称性，即旋转、平移和反射，有效地缓解了患者之间经典对齐的问题。

    arXiv:2403.07536v1 Announce Type: cross  Abstract: Many anatomical structures can be described by surface or volume meshes. Machine learning is a promising tool to extract information from these 3D models. However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures. Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms. We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation. Our method extends the recently proposed geometric algebra transformer (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients. LaB-GATr achieves state-of-the-art results on three ta
    
[^51]: 物理学迁移学习用于材料强度筛选

    Physics-Transfer Learning for Material Strength Screening

    [https://arxiv.org/abs/2403.07526](https://arxiv.org/abs/2403.07526)

    提出了使用物理迁移框架来学习晶体塑性物理并从原子模拟中预测Peierls应力的方法。

    

    材料强度，像许多自然科学中的问题一样，涵盖多个长度和时间尺度，并且解决方案必须在准确性和性能之间取得平衡。Peierls应力是晶体塑性中的一个中心概念，通过位错对塑性流动的阻力来衡量强度。确定Peierls应力涉及到弹性晶格响应和晶体滑移能量景观的多尺度性质。通过第一性原理计算通过Peierls应力的材料强度筛选对于位错的非局部特性而言在计算上很难，并且没有包含在最先进的计算材料数据库中。在这项工作中，我们提出了一个物理迁移框架，从经验性原子模拟中学习晶体塑性的物理规律，然后预测Peierls应力从化学准确的基于密度泛函理论的计算中。

    arXiv:2403.07526v1 Announce Type: cross  Abstract: The strength of materials, like many problems in the natural sciences, spans multiple length and time scales, and the solution has to balance accuracy and performance. Peierls stress is one of the central concepts in crystal plasticity that measures the strength through the resistance of a dislocation to plastic flow. The determination of Peierls stress involves a multiscale nature depending on both elastic lattice responses and the energy landscape of crystal slips. Material screening by strength via the Peierls stress from first-principles calculations is computationally intractable for the nonlocal characteristics of dislocations, and not included in the state-of-the-art computational material databases. In this work, we propose a physics-transfer framework to learn the physics of crystal plasticity from empirical atomistic simulations and then predict the Peierls stress from chemically accurate density functional theory-based calcu
    
[^52]: 利用物理信息神经网络重建木星的磁场

    Reconstructions of Jupiter's magnetic field using physics informed neural networks

    [https://arxiv.org/abs/2403.07507](https://arxiv.org/abs/2403.07507)

    本研究提出了一种利用物理信息神经网络重建木星内部磁场的新方法，相比其他方法，能够更清晰地解析局部结构并避免深度噪声干扰。

    

    利用由Juno任务收集的数据进行磁测可以用来约束木星的内部。然而，假设电导率为零并以球谐函数表示的重建向内继续是受到小尺度噪声增强的限制。本文描述了基于物理信息神经网络的木星内部磁场的新重建方法，使用Juno轨道的前33个（PINN33）或前50个（PINN50）。该方法可以解析出局部结构，并允许存在弱环境电流。与其他方法相比，我们对木星磁场的表面和上方的重建都相似，并且与Juno数据拟合相似。然而，我们的模型不受深度噪声干扰，因此能够提供更清晰的内部结构图像。

    arXiv:2403.07507v1 Announce Type: cross  Abstract: Magnetic sounding using data collected from the Juno mission can be used to provide constraints on Jupiter's interior. However, inwards continuation of reconstructions assuming zero electrical conductivity and a representation in spherical harmonics are limited by the enhancement of noise at small scales. In this paper we describe new reconstructions of Jupiter's internal magnetic field based on physics-informed neural networks and either the first 33 (PINN33) or the first 50 (PINN50) of Juno's orbits. The method can resolve local structures, and allows for weak ambient electrical currents. Compared with other methods, our reconstructions of Jupiter's magnetic field both on and above the surface are similar, and we achieve a similar fit to the Juno data. However, our models are not hampered by noise at depth, and so offer a much clearer picture of the interior structure. We estimate that the dynamo boundary is at a fractional radius of
    
[^53]: 受约束的混合动力车辆的最优燃料消耗：一种受约束的强化学习方法

    Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach

    [https://arxiv.org/abs/2403.07503](https://arxiv.org/abs/2403.07503)

    首次提出了从受约束强化学习的视角全球首次提供混合动力车辆的受约束最优燃料消耗的数学表达，并首次利用两种主流的受约束强化学习方法来获得车辆在电池电气平衡条件下的最小燃料消耗。

    

    混合动力车辆（HEVs）因能更好地结合内燃机和电动机的工作特性而日益受欢迎。然而，在特定装配条件和特定速度曲线下，HEV的最小燃料消耗对于电池电气平衡情况仍需在学术界和工业界进一步阐明。针对这一问题，本工作首次从受约束强化学习（CRL）的视角提供了受约束的最优燃料消耗（COFC）的数学表达。同时，首次利用了两种主流的CRL方法，即受约束变分策略优化（CVPO）和基于拉格朗日的方法，以在电池电气平衡条件下获得车辆的最小燃料消耗。我们在知名的普锐斯丰田混合系统（THS）下NEDC条件下进行案例研究；我们g

    arXiv:2403.07503v1 Announce Type: new  Abstract: Hybrid electric vehicles (HEVs) are becoming increasingly popular because they can better combine the working characteristics of internal combustion engines and electric motors. However, the minimum fuel consumption of an HEV for a battery electrical balance case under a specific assembly condition and a specific speed curve still needs to be clarified in academia and industry. Regarding this problem, this work provides the mathematical expression of constrained optimal fuel consumption (COFC) from the perspective of constrained reinforcement learning (CRL) for the first time globally. Also, two mainstream approaches of CRL, constrained variational policy optimization (CVPO) and Lagrangian-based approaches, are utilized for the first time to obtain the vehicle's minimum fuel consumption under the battery electrical balance condition. We conduct case studies on the well-known Prius TOYOTA hybrid system (THS) under the NEDC condition; we g
    
[^54]: 使用多标签机器学习检测安全相关方法

    Detecting Security-Relevant Methods using Multi-label Machine Learning

    [https://arxiv.org/abs/2403.07501](https://arxiv.org/abs/2403.07501)

    该论文介绍了一种使用多标签机器学习方法检测安全相关方法的插件，可以自动生成静态分析工具配置，运行分析并显示结果。

    

    为了检测安全漏洞，静态分析工具需要配置安全相关方法。目前的方法可以使用二元关联的机器学习方法自动识别这些方法。然而，它们忽略了安全相关方法之间的依赖关系，在实践中表现不佳。此外，用户仍然必须手动配置静态分析工具使用检测到的方法。基于用户的反馈和我们的观察，这些繁琐的手动步骤往往令人厌烦、容易出错且不直观。本文介绍了Dev-Assist，一个IntelliJ IDEA插件，它使用考虑标签之间依赖关系的多标签机器学习方法来检测安全相关方法。该插件可以自动生成静态分析工具的配置，运行静态分析，并在IntelliJ IDEA中显示结果。我们的实验表明，Dev-A

    arXiv:2403.07501v1 Announce Type: new  Abstract: To detect security vulnerabilities, static analysis tools need to be configured with security-relevant methods. Current approaches can automatically identify such methods using binary relevance machine learning approaches. However, they ignore dependencies among security-relevant methods, over-generalize and perform poorly in practice. Additionally, users have to nevertheless manually configure static analysis tools using the detected methods. Based on feedback from users and our observations, the excessive manual steps can often be tedious, error-prone and counter-intuitive.   In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects security-relevant methods using a multi-label machine learning approach that considers dependencies among labels. The plugin can automatically generate configurations for static analysis tools, run the static analysis, and show the results in IntelliJ IDEA. Our experiments reveal that Dev-A
    
[^55]: 通过可通信性几何学在数据科学中的符号图

    Signed graphs in data sciences via communicability geometry

    [https://arxiv.org/abs/2403.07493](https://arxiv.org/abs/2403.07493)

    提出了符号图的可通信性几何概念，证明了其度量是欧几里德的和球形的，然后应用于解决符号图数据分析中的多个问题。

    

    符号图是表示多种存在冲突交互的数据的新兴方式，包括来自生物学、生态学和社会系统的数据。我们在这里提出了符号图的可通信性几何概念，证明了在这个空间中的度量，比如可通信性距离和角度，是欧几里德的和球形的。然后我们将这些度量应用于以统一方式解决符号图数据分析中的几个问题，包括符号图的分区、维度约简、找到符号网络中的联盟等级以及量化系统中现有派系之间极化程度的问题。

    arXiv:2403.07493v1 Announce Type: cross  Abstract: Signed graphs are an emergent way of representing data in a variety of contexts were conflicting interactions exist. These include data from biological, ecological, and social systems. Here we propose the concept of communicability geometry for signed graphs, proving that metrics in this space, such as the communicability distance and angles, are Euclidean and spherical. We then apply these metrics to solve several problems in data analysis of signed graphs in a unified way. They include the partitioning of signed graphs, dimensionality reduction, finding hierarchies of alliances in signed networks as well as the quantification of the degree of polarization between the existing factions in systems represented by this type of graphs.
    
[^56]: XpertAI：揭示子流形的模型策略

    XpertAI: uncovering model strategies for sub-manifolds

    [https://arxiv.org/abs/2403.07486](https://arxiv.org/abs/2403.07486)

    XpertAI是一个框架，可以将预测策略解开为多个特定范围的子策略，并允许将模型的查询制定为这些子策略的线性组合。

    

    近年来，可解释人工智能（XAI）方法已经促进了深入验证和知识提取机器学习模型。尽管针对分类进行了广泛研究，但很少有XAI解决方案解决了特定于回归模型的挑战。在回归中，解释需要精确制定以应对特定用户查询（例如区分“为什么输出大于0？”和“为什么输出大于50？”）。此外，它们应反映模型在相关数据子流形上的行为。在本文中，我们介绍了XpertAI，这是一个将预测策略解开为多个范围特定的子策略，并允许将对模型的精准查询（“被解释物”）的制定为这些子策略的线性组合的框架。XpertAI通常制定可以与基于遮挡、梯度集成或反向传播的流行XAI归因技术一起使用。

    arXiv:2403.07486v1 Announce Type: new  Abstract: In recent years, Explainable AI (XAI) methods have facilitated profound validation and knowledge extraction from ML models. While extensively studied for classification, few XAI solutions have addressed the challenges specific to regression models. In regression, explanations need to be precisely formulated to address specific user queries (e.g.\ distinguishing between `Why is the output above 0?' and `Why is the output above 50?'). They should furthermore reflect the model's behavior on the relevant data sub-manifold. In this paper, we introduce XpertAI, a framework that disentangles the prediction strategy into multiple range-specific sub-strategies and allows the formulation of precise queries about the model (the `explanandum') as a linear combination of those sub-strategies. XpertAI is formulated generally to work alongside popular XAI attribution techniques, based on occlusion, gradient integration, or reverse propagation. Qualitat
    
[^57]: 通过多项式模型代理提升黑盒优化性能的 PMBO

    PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates

    [https://arxiv.org/abs/2403.07485](https://arxiv.org/abs/2403.07485)

    PMBO通过多项式模型逼近和贝叶斯优化结合，相比于传统贝叶斯优化表现更好，对相关函数和超参数设置更加鲁棒，与进化算法性能相当。

    

    我们引入了一种基于代理的黑盒优化方法，称为多项式模型优化（PMBO）。该算法通过多项式逼近和贝叶斯优化步骤交替进行，使用高斯过程来模拟目标与其多项式拟合之间的误差。我们描述了PMBO的算法设计，并将PMBO的性能结果与几种优化方法在一组解析测试函数上的表现进行了比较。结果表明，PMBO优于经典的贝叶斯优化，并且对于相关函数族和超参数设置的选择具有鲁棒性，而这在经典的贝叶斯优化中需要仔细调整。值得注意的是，PMBO的性能与协方差矩阵适应 - 进化策略（CMA-ES）等最先进的进化算法相当。这一发现表明，PMBO成为代理方法中的重要选择。

    arXiv:2403.07485v1 Announce Type: cross  Abstract: We introduce a surrogate-based black-box optimization method, termed Polynomial-model-based optimization (PMBO). The algorithm alternates polynomial approximation with Bayesian optimization steps, using Gaussian processes to model the error between the objective and its polynomial fit. We describe the algorithmic design of PMBO and compare the results of the performance of PMBO with several optimization methods for a set of analytic test functions.   The results show that PMBO outperforms the classic Bayesian optimization and is robust with respect to the choice of its correlation function family and its hyper-parameter setting, which, on the contrary, need to be carefully tuned in classic Bayesian optimization. Remarkably, PMBO performs comparably with state-of-the-art evolutionary algorithms such as the Covariance Matrix Adaptation -- Evolution Strategy (CMA-ES). This finding suggests that PMBO emerges as the pivotal choice among sur
    
[^58]: 一种深度学习方法用于糖尿病诊断

    A Deep Learning Approach to Diabetes Diagnosis

    [https://arxiv.org/abs/2403.07483](https://arxiv.org/abs/2403.07483)

    采用深度学习方法，提出一种无创糖尿病诊断方法，通过反向传播神经网络和数据平衡技术，在准确性、敏感性和特异性方面取得显著改进

    

    糖尿病是由胰岛素产生或利用不足导致的，对身体造成了广泛的危害。现有的诊断方法通常是侵入性的，并伴有诸多缺点，比如成本限制。尽管存在像类间k最近邻(CkNN)和通用回归神经网络(GRNN)这样的机器学习模型，但它们在处理不平衡数据时往往表现不佳。利用传感技术和机器学习的进展，我们提出了一种使用带有批量标准化的反向传播神经网络(BPNN)进行无创糖尿病诊断的方法，结合数据重采样和归一化以实现类平衡。我们的方法解决了传统机器学习中存在的诸多挑战，比如与传统方法相关的性能受限。在三个数据集上的实验结果显示，与传统方法相比，我们在整体准确性、敏感性和特异性方面取得了显著的改进。值得注意的是，我们实现了高准确率

    arXiv:2403.07483v1 Announce Type: cross  Abstract: Diabetes, resulting from inadequate insulin production or utilization, causes extensive harm to the body. Existing diagnostic methods are often invasive and come with drawbacks, such as cost constraints. Although there are machine learning models like Classwise k Nearest Neighbor (CkNN) and General Regression Neural Network (GRNN), they struggle with imbalanced data and result in under-performance. Leveraging advancements in sensor technology and machine learning, we propose a non-invasive diabetes diagnosis using a Back Propagation Neural Network (BPNN) with batch normalization, incorporating data re-sampling and normalization for class balancing. Our method addresses existing challenges such as limited performance associated with traditional machine learning. Experimental results on three datasets show significant improvements in overall accuracy, sensitivity, and specificity compared to traditional methods. Notably, we achieve accur
    
[^59]: 面向个性化的图基础模型

    Towards Graph Foundation Models for Personalization

    [https://arxiv.org/abs/2403.07478](https://arxiv.org/abs/2403.07478)

    本文提出了一种面向个性化的基于图的基础建模方法，其中的Heterogeneous GNN旨在捕捉跨多种可推荐项目类型的多跳内容和消费关系。

    

    在个性化领域，整合消费信号和基于内容的表示等多样信息源变得日益关键，以构建最先进的解决方案。在这方面，围绕Graph Neural Networks（GNNs）和Foundation Models（FMs）的研究存在两大趋势。虽然GNNs成为工业界在规模上实现个性化的热门解决方案，但FMs最近才因其在排名和检索等个性化任务中表现出色而受到关注。本文提出了一种针对个性化的基于图的基础建模方法。该方法的核心是一种设计用于捕捉跨各种可推荐项目类型的多跳内容和消费关系的异质GNN（HGNN）。为确保基础模型所需的一般性，我们采用了基于大语言模型（LLM）的文本特征化方法。

    arXiv:2403.07478v1 Announce Type: cross  Abstract: In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions. In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval. In this paper, we present a graph-based foundation modeling approach tailored to personalization. Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types. To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of
    
[^60]: 考虑不平衡的存在-仅损失函数用于物种分布建模

    Imbalance-aware Presence-only Loss Function for Species Distribution Modeling

    [https://arxiv.org/abs/2403.07472](https://arxiv.org/abs/2403.07472)

    本研究通过使用平衡存在-仅损失函数在公民科学数据集上训练深度学习模型来解决物种分布模型中存在的类别不平衡问题，取得了比传统方法更好的性能表现。

    

    由于生物多样性显著下降，物种分布模型（SDMs）对于通过将环境条件与物种出现地联系起来，理解气候变化对物种栖息地的影响至关重要。一直以来，由于物种观察的稀缺性受到限制，这些模型通过整合公民科学倡议提供的更大数据集而得到显著改进。然而，它们仍然受到数据集中物种之间强烈的类别不平衡问题的困扰，这往往导致对稀有物种（对保护工作至关重要的物种）的惩罚。为解决这一问题，本研究评估了使用平衡的存在-仅损失函数在大型基于公民科学的数据集上训练深度学习模型的有效性。我们展示了这种考虑不平衡的损失函数在各种数据集和任务中均优于传统损失函数，特别是在...

    arXiv:2403.07472v1 Announce Type: new  Abstract: In the face of significant biodiversity decline, species distribution models (SDMs) are essential for understanding the impact of climate change on species habitats by connecting environmental conditions to species occurrences. Traditionally limited by a scarcity of species observations, these models have significantly improved in performance through the integration of larger datasets provided by citizen science initiatives. However, they still suffer from the strong class imbalance between species within these datasets, often resulting in the penalization of rare species--those most critical for conservation efforts. To tackle this issue, this study assesses the effectiveness of training deep learning models using a balanced presence-only loss function on large citizen science-based datasets. We demonstrate that this imbalance-aware loss function outperforms traditional loss functions across various datasets and tasks, particularly in a
    
[^61]: 有关某些推进约束的非凸性及其在机器学习中的影响

    On the nonconvexity of some push-forward constraints and its consequences in machine learning

    [https://arxiv.org/abs/2403.07471](https://arxiv.org/abs/2403.07471)

    本文提供了关于推进约束的非凸性的理论见解，并展示了这对相关学习问题的影响。

    

    push-forward操作使人能够通过确定性映射重新分配概率测度。它在统计和优化中起着关键作用：许多学习问题（特别是来自最优输运、生成建模和算法公平性的问题）包括作为模型上的推进条件或处罚的约束。然而，文献缺乏关于这些约束的（非）凸性及其对相关学习问题的影响的一般理论见解。本文旨在填补这一空白。在第一部分中，我们提供了两组函数（将一个概率测度传输到另一个的映射；诱导不同概率测度之间相等输出分布的映射）的（非）凸性的一系列充分必要条件。这突出了对于大多数概率测度而言，这些推进约束是非凸的。在接下来，我们展示了这一结果如何暗示

    arXiv:2403.07471v1 Announce Type: cross  Abstract: The push-forward operation enables one to redistribute a probability measure through a deterministic map. It plays a key role in statistics and optimization: many learning problems (notably from optimal transport, generative modeling, and algorithmic fairness) include constraints or penalties framed as push-forward conditions on the model. However, the literature lacks general theoretical insights on the (non)convexity of such constraints and its consequences on the associated learning problems. This paper aims at filling this gap. In a first part, we provide a range of sufficient and necessary conditions for the (non)convexity of two sets of functions: the maps transporting one probability measure to another; the maps inducing equal output distributions across distinct probability measures. This highlights that for most probability measures, these push-forward constraints are not convex. In a second time, we show how this result impli
    
[^62]: 一个都不能少：基于GNN的嵌入式设备控制流认证

    One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices

    [https://arxiv.org/abs/2403.07465](https://arxiv.org/abs/2403.07465)

    介绍了一种用于嵌入式设备的轻量级 CFA 方法 RAGE，可以检测代码重用攻击（CRA），包括控制和非控制数据攻击，有效提取一个执行跟踪的特征，并利用无监督图神经网络（GNN）识别攻击。

    

    控制流 attestation (CFA) 是一种安全服务，允许一个实体（验证者）验证远程计算机系统（证明者）上代码执行的完整性。现有的 CFA 方案存在不切实际的假设，例如需要访问证明者的内部状态（如内存或代码）、证明者软件的完整控制流图（CFG）、大量测量数据或定制硬件。此外，当前的 CFA 方案因计算开销高和资源使用率高而不适合用于对嵌入式系统的认证。

    arXiv:2403.07465v1 Announce Type: cross  Abstract: Control-Flow Attestation (CFA) is a security service that allows an entity (verifier) to verify the integrity of code execution on a remote computer system (prover). Existing CFA schemes suffer from impractical assumptions, such as requiring access to the prover's internal state (e.g., memory or code), the complete Control-Flow Graph (CFG) of the prover's software, large sets of measurements, or tailor-made hardware. Moreover, current CFA schemes are inadequate for attesting embedded systems due to their high computational overhead and resource usage.   In this paper, we overcome the limitations of existing CFA schemes for embedded devices by introducing RAGE, a novel, lightweight CFA approach with minimal requirements. RAGE can detect Code Reuse Attacks (CRA), including control- and non-control-data attacks. It efficiently extracts features from one execution trace and leverages Unsupervised Graph Neural Networks (GNNs) to identify de
    
[^63]: 通过综合Brier得分和协调指数对集成方法和事件时间分析模型进行实验比较

    Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index

    [https://arxiv.org/abs/2403.07460](https://arxiv.org/abs/2403.07460)

    本研究通过对集成方法和事件时间分析模型进行比较，证明了集成方法可以提高预测准确性并增强预测性能的稳健性

    

    时间到事件分析是统计学的一个分支，在过去几十年中因其在预测性维护、客户流失预测和人口寿命估计等许多应用领域中的增加而变得越来越受欢迎。本文回顾和比较了几种用于时间到事件分析的预测模型的性能。这些模型包括半参数和参数统计模型，以及机器学习方法。我们在三个数据集上进行了研究，并用两种不同的分数（综合Brier得分和协调指数）进行评估。此外，我们展示了集成方法如何提高预测准确性，并增强预测性能的稳健性，这在时间到事件分析中令人惊讶地尚未得到广泛研究。我们用一个模拟实验总结了分析，评估了影响方法性能排名的因素。

    arXiv:2403.07460v1 Announce Type: new  Abstract: Time-to-event analysis is a branch of statistics that has increased in popularity during the last decades due to its many application fields, such as predictive maintenance, customer churn prediction and population lifetime estimation. In this paper, we review and compare the performance of several prediction models for time-to-event analysis. These consist of semi-parametric and parametric statistical models, in addition to machine learning approaches. Our study is carried out on three datasets and evaluated in two different scores (the integrated Brier score and concordance index). Moreover, we show how ensemble methods, which surprisingly have not yet been much studied in time-to-event analysis, can improve the prediction accuracy and enhance the robustness of the prediction performance. We conclude the analysis with a simulation experiment in which we evaluate the factors influencing the performance ranking of the methods using both 
    
[^64]: 使用 multi-view-AE 库的多视图自编码器教程

    A tutorial on multi-view autoencoders using the multi-view-AE library

    [https://arxiv.org/abs/2403.07456](https://arxiv.org/abs/2403.07456)

    提出了一个统一的多视图自编码器数学框架，整合了各种公式，并拓展了 \texttt{multi-view-AE} 库的文档和功能。

    

    近年来，对建模数据的多个模态（或视图）以便理解模态之间的关系或生成缺失数据引起了越来越多的关注。多视图自编码器因其能够适应和灵活建模多模态数据的能力而备受关注，表明其具有根据手头数据特征调整方法的能力。然而，大多数多视图自编码器存在一致性符号标注不一的问题，并且通常使用不同的编码框架实现。为解决这个问题，我们提出了一个统一的多视图自编码器数学框架，整合了它们的公式。此外，我们提供了对每个模型动机和理论优势的见解。为了方便访问和实际使用，我们扩展了先前介绍的 multi-view-AE 库的文档和功能。该库提供了 Python 实现。

    arXiv:2403.07456v1 Announce Type: new  Abstract: There has been a growing interest in recent years in modelling multiple modalities (or views) of data to for example, understand the relationship between modalities or to generate missing data. Multi-view autoencoders have gained significant traction for their adaptability and versatility in modelling multi-modal data, demonstrating an ability to tailor their approach to suit the characteristics of the data at hand. However, most multi-view autoencoders have inconsistent notation and are often implemented using different coding frameworks. To address this, we present a unified mathematical framework for multi-view autoencoders, consolidating their formulations. Moreover, we offer insights into the motivation and theoretical advantages of each model. To facilitate accessibility and practical use, we extend the documentation and functionality of the previously introduced \texttt{multi-view-AE} library. This library offers Python implementa
    
[^65]: 使用高斯局部线性映射进行快速、准确和轻量级的顺序仿真推断

    Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings

    [https://arxiv.org/abs/2403.07454](https://arxiv.org/abs/2403.07454)

    使用结构混合概率分布提供了准确的后验推断，同时具有更小的计算占用量，相较于现有的基于神经网络的SBI方法。

    

    arXiv:2403.07454v1 公告类型: 跨领域 摘要: 针对具有难以处理的似然函数的复杂模型的贝叶斯推断可以使用多次调用计算模拟器的算法来解决。 这些方法被统称为“基于仿真的推断”（SBI）。 最近的SBI方法利用神经网络（NN）提供近似但表达丰富的构造，用于不可用的似然函数和后验分布。 然而，它们通常无法实现准确性和计算需求之间的最佳折衷。 在这项工作中，我们提出了一种提供似然函数和后验分布近似的替代方法，使用结构化的概率分布混合物。 相对于最先进的基于NN的SBI方法，我们的方法在产生准确的后验推断的同时，具有更小的计算占用量。 我们在SBI文献中的几个基准模型上展示了我们的结果。

    arXiv:2403.07454v1 Announce Type: cross  Abstract: Bayesian inference for complex models with an intractable likelihood can be tackled using algorithms performing many calls to computer simulators. These approaches are collectively known as "simulation-based inference" (SBI). Recent SBI methods have made use of neural networks (NN) to provide approximate, yet expressive constructs for the unavailable likelihood function and the posterior distribution. However, they do not generally achieve an optimal trade-off between accuracy and computational demand. In this work, we propose an alternative that provides both approximations to the likelihood and the posterior distribution, using structured mixtures of probability distributions. Our approach produces accurate posterior inference when compared to state-of-the-art NN-based SBI methods, while exhibiting a much smaller computational footprint. We illustrate our results on several benchmark models from the SBI literature.
    
[^66]: 针对时间相关多电子Schr\"odinger方程的从头变分波函数

    Ab-initio variational wave functions for the time-dependent many-electron Schr\"odinger equation

    [https://arxiv.org/abs/2403.07447](https://arxiv.org/abs/2403.07447)

    提出一种针对费米子时间相关波函数的变分方法，通过捕捉多体相关性超越平均场近似，可以解决实时演化非平衡量子电子系统的挑战。

    

    arXiv:2403.07447v1 公告类型：交叉 摘要：描述多电子量子系统动力学对于预测量子化学中的电子结构、凝聚态系统的性质和复杂材料的行为等应用至关重要。然而，非平衡量子电子系统的实时演化对于理论和计算方法来说是一个巨大挑战，因为系统探索了广阔的构型空间。本文引入了一种针对费米子时间相关波函数的变分方法，通过捕捉多体相关性超越平均场近似。所提出的方法涉及参数化时间演化的量子态，从而实现对态演化的近似。为了考虑电子相关性，我们采用了时间相关的Jastrow因子和回流变换。我们还展示了可以利用神经网络来参数化这些函数。

    arXiv:2403.07447v1 Announce Type: cross  Abstract: Describing the dynamics of many-electron quantum systems is crucial for applications such as predicting electronic structures in quantum chemistry, the properties of condensed matter systems, and the behaviors of complex materials. However, the real-time evolution of non-equilibrium quantum electronic systems poses a significant challenge for theoretical and computational approaches, due to the system's exploration of a vast configuration space. This work introduces a variational approach for fermionic time-dependent wave functions, surpassing mean-field approximations by capturing many-body correlations. The proposed methodology involves parameterizing the time-evolving quantum state, enabling the approximation of the state's evolution. To account for electron correlations, we employ time-dependent Jastrow factors and backflow transformations. We also show that we can incorporate neural networks to parameterize these functions. The ti
    
[^67]: 针对域自适应的代理方法

    Proxy Methods for Domain Adaptation

    [https://arxiv.org/abs/2403.07442](https://arxiv.org/abs/2403.07442)

    该论文研究了针对域自适应问题的代理方法，利用近端因果学习技术估计因果效应，在不恢复或建模潜在变量的情况下通过代理变量实现了对分布转移的适应。

    

    我们研究了在分布转移下的域自适应问题，这种转移是由于未观察到的潜在变量分布发生改变，导致协变量和标签都被混淆。在这种情况下，既不适用协变量转移也不适用标签转移的假设。我们的自适应方法采用了近端因果学习，一种在可获得未观察混淆变量代理的设置中估计因果效应的技术。我们展示了代理变量允许适应分布转移，而无需显式恢复或建模潜在变量。我们考虑了两种情况，(i) 概念瓶颈：观察到一个额外的“概念”变量，它在协变量和标签之间起到中介作用；(ii) 多领域：可用来自多个源领域的训练数据，其中每个源领域展示了对潜在混杂变量的不同分布。我们开发了一个两阶段的 k

    arXiv:2403.07442v1 Announce Type: new  Abstract: We study the problem of domain adaptation under distribution shift, where the shift is due to a change in the distribution of an unobserved, latent variable that confounds both the covariates and the labels. In this setting, neither the covariate shift nor the label shift assumptions apply. Our approach to adaptation employs proximal causal learning, a technique for estimating causal effects in settings where proxies of unobserved confounders are available. We demonstrate that proxy variables allow for adaptation to distribution shift without explicitly recovering or modeling latent variables. We consider two settings, (i) Concept Bottleneck: an additional ''concept'' variable is observed that mediates the relationship between the covariates and labels; (ii) Multi-domain: training data from multiple source domains is available, where each source domain exhibits a different distribution over the latent confounder. We develop a two-stage k
    
[^68]: 跨多个主成分分析研究的知识转移

    Knowledge Transfer across Multiple Principal Component Analysis Studies

    [https://arxiv.org/abs/2403.07431](https://arxiv.org/abs/2403.07431)

    提出了一种跨多个主成分分析研究的知识转移算法，通过整合多个研究中共享的子空间信息来增强目标PCA任务的估计准确性。

    

    Transfer learning在统计领域引起了极大兴趣。本文关注于无监督学习任务的知识转移，与文献中的监督学习任务相对比。在给定可转移的源人口的情况下，我们提出了一个两步的转移学习算法，从多个源主成分分析（PCA）研究中提取有用信息，从而增强目标PCA任务的估计准确性。第一步中，我们通过一种名为Grassmannian barycenter的方法，整合跨多个研究共享的子空间信息，而不是直接在汇总数据集上执行PCA。提出的Grassmannian barycenter方法在更一般的情况下具有鲁棒性和计算优势。然后，第一步得到的共享子空间的估计器进一步用于估计第二步中的目标私有子空间。

    arXiv:2403.07431v1 Announce Type: cross  Abstract: Transfer learning has aroused great interest in the statistical community. In this article, we focus on knowledge transfer for unsupervised learning tasks in contrast to the supervised learning tasks in the literature. Given the transferable source populations, we propose a two-step transfer learning algorithm to extract useful information from multiple source principal component analysis (PCA) studies, thereby enhancing estimation accuracy for the target PCA task. In the first step, we integrate the shared subspace information across multiple studies by a proposed method named as Grassmannian barycenter, instead of directly performing PCA on the pooled dataset. The proposed Grassmannian barycenter method enjoys robustness and computational advantages in more general cases. Then the resulting estimator for the shared subspace from the first step is further utilized to estimate the target private subspace in the second step. Our theoret
    
[^69]: 具有显式预测器的学习增强算法

    Learning-Augmented Algorithms with Explicit Predictors

    [https://arxiv.org/abs/2403.07413](https://arxiv.org/abs/2403.07413)

    该论文主要探讨了在在线问题中采用具有显式预测器的学习增强算法，通过允许预测器随着输入的增加进行学习，旨在设计出专门针对特定算法任务的在线学习算法。

    

    最近算法设计方面的进展展示了如何利用由过去和当前数据的机器学习模型得到的预测。这些方法在预测准确时表现出性能增强，同时在预测失败时提供最坏情况保证以确保鲁棒性。本文关注在线问题；先前研究集中在一个范式上，其中预测器在过去数据上进行预训练，然后作为黑匣子使用（获得它被训练用于的预测）。相反，本文中，我们拆解预测器并将其引发的学习问题整合到算法挑战中。具体来说，我们允许预测器在接收更大输入部分时进行学习，最终设计出专门针对手头算法任务的在线学习算法。采用这种观点，我们专注于几个基本

    arXiv:2403.07413v1 Announce Type: new  Abstract: Recent advances in algorithmic design show how to utilize predictions obtained by machine learning models from past and present data. These approaches have demonstrated an enhancement in performance when the predictions are accurate, while also ensuring robustness by providing worst-case guarantees when predictions fail. In this paper we focus on online problems; prior research in this context was focused on a paradigm where the predictor is pre-trained on past data and then used as a black box (to get the predictions it was trained for). In contrast, in this work, we unpack the predictor and integrate the learning problem it gives rise for within the algorithmic challenge. In particular we allow the predictor to learn as it receives larger parts of the input, with the ultimate goal of designing online learning algorithms specifically tailored for the algorithmic task at hand. Adopting this perspective, we focus on a number of fundamenta
    
[^70]: 提高推理速度和减少遗忘：早期退出网络在持续学习中的双重好处

    Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning

    [https://arxiv.org/abs/2403.07404](https://arxiv.org/abs/2403.07404)

    早期退出网络在持续学习中展现出降低遗忘和在资源利用上表现优异的特点

    

    arXiv:2403.07404v1 公告类型: 跨界 摘要: 受深度神经网络能源高效利用需求驱动，早期退出方法备受关注。这些策略通过在网络早期做出决定，实现快速预测，从而节省计算时间和资源。然而，迄今为止，早期退出网络仅针对静态数据分布进行了开发，限制了它们在具有持续非静态数据的实际场景中的应用。本研究旨在探讨早期退出网络的持续学习。我们改编现有的持续学习方法以适应早期退出架构，并研究它们在持续设置中的行为。我们注意到，早期网络层表现出减少遗忘，即使使用的资源显著更少，也能胜过标准网络。此外，我们分析任务最近性偏差对早期退出推理的影响，并提出任务...

    arXiv:2403.07404v1 Announce Type: cross  Abstract: Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task
    
[^71]: SmallToLarge (S2L): 通过总结小模型的训练轨迹，为大型语言模型的微调提供可伸缩的数据选择

    SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models

    [https://arxiv.org/abs/2403.07384](https://arxiv.org/abs/2403.07384)

    S2L提出了一种通过总结小模型的训练轨迹，来指导大型语言模型数据选择的方法，显著提高了数学问题解决中监督微调的数据效率，并在数据集性能上表现优异。

    

    尽管数据选择在大型语言模型（LLMs）的预训练和指导微调阶段非常有效，但在专业领域的监督微调（SFT）中改善数据效率面临着重大挑战，原因是微调数据的复杂性。为弥合这一差距，我们引入了一种有效且可伸缩的数据选择方法S2L（SmallToLarge），它利用小模型的训练轨迹来指导更大模型的数据选择。通过大量实验，我们证明了S2L显著提高了数学问题解决的SFT数据效率，将训练数据缩减到原始MathInstruct数据集（Yue等人，2023）的仅11%，以达到全数据集的性能，并在6个领域内外评估数据集中平均优于最先进的数据选择算法4.7%。值得注意的是，仅选择50K数据进行SFT，S2L实现...

    arXiv:2403.07384v1 Announce Type: cross  Abstract: Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achi
    
[^72]: 神经网络和LLMs中优化轨迹的特征：长度、拐点和死胡同

    Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends

    [https://arxiv.org/abs/2403.07379](https://arxiv.org/abs/2403.07379)

    分析神经网络和LLMs中优化轨迹的复杂性，揭示了优化过程中的关键特征，包括方向探索和方向正则化。

    

    我们提出了一种全新的方法来理解神经网络的机制，通过分析其优化轨迹中包含的丰富参数结构。为此，我们引入了一些关于优化轨迹复杂性的自然概念，既定性又定量地揭示了各种优化选择（如动量、权重衰减和批大小）之间所涉及的内在微妙和相互作用。我们利用这些概念来提供关于深度神经网络优化本质的关键特征：何时顺利进行，何时陷入死胡同。此外，基于我们的轨迹视角，我们揭示了动量和权重衰减之间促进方向探索的交织行为，以及其他一些行为的方向正则化行为。我们在大规模视觉和语言设置中进行实验，包括具有最多120亿个参数的大型语言模型（LLMs）。

    arXiv:2403.07379v1 Announce Type: cross  Abstract: We propose a fresh take on understanding the mechanisms of neural networks by analyzing the rich structure of parameters contained within their optimization trajectories. Towards this end, we introduce some natural notions of the complexity of optimization trajectories, both qualitative and quantitative, which reveal the inherent nuance and interplay involved between various optimization choices, such as momentum, weight decay, and batch size. We use them to provide key hallmarks about the nature of optimization in deep neural networks: when it goes right, and when it finds itself in a dead end. Further, thanks to our trajectory perspective, we uncover an intertwined behaviour of momentum and weight decay that promotes directional exploration, as well as a directional regularization behaviour of some others. We perform experiments over large-scale vision and language settings, including large language models (LLMs) with up to 12 billio
    
[^73]: SVD-LLM: 针对大型语言模型压缩的截断感知奇异值分解

    SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression

    [https://arxiv.org/abs/2403.07378](https://arxiv.org/abs/2403.07378)

    SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。

    

    大型语言模型（LLMs）的进展受到其庞大尺寸的限制，这需要LLM压缩方法以实现实际部署。奇异值分解（SVD）为LLM压缩提供了一个有希望的解决方案。然而，现有的基于SVD的LLM压缩方法存在两个关键限制：截断较小的奇异值可能导致更高的压缩损失，并且在SVD截断后剩余模型参数的更新缺失。在这项工作中，我们提出了SVD-LLM，一种新的基于SVD的LLM压缩方法，解决了现有方法的限制。SVD-LLM采用了一种截断感知的数据白化策略，以确保奇异值和压缩损失之间的直接映射。此外，SVD-LLM采用一种逐层闭式模型参数更新策略，以弥补SVD截断引起的准确性降低。我们在总共11个数据集和七个m上评估了SVD-LLM。

    arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
    
[^74]: 熵对于测试时适应性来说还不够：从解缠分因素的角度看

    Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors

    [https://arxiv.org/abs/2403.07366](https://arxiv.org/abs/2403.07366)

    研究显示熵作为测试时适应性的置信度度量在偏倚场景下不可靠，本文提出了一种新的测试时适应性方法DeYO，利用Pseudo-Label Probability Difference（PLPD）作为置信度度量

    

    测试时适应性（TTA）微调预训练的深度神经网络以适应未知的测试数据。TTA的主要挑战是在线更新过程中对整个测试数据集的有限访问，导致误差累积。为了减轻这种情况，TTA方法利用模型输出的熵作为置信度度量，旨在确定哪些样本更不可能导致错误。然而，通过实验研究，我们观察到在偏倚场景下熵作为TTA的置信度度量是不可靠的，理论上揭示了这种不可靠性源于忽视数据的潜在解缠分因素对预测的影响。基于这些发现，我们提出了一种名为Destroy Your Object（DeYO）的新型TTA方法，它利用了一种新提出的置信度度量，名为伪标签概率差（PLPD）。PLPD通过测量对象形状对预测的影响程度来量化。

    arXiv:2403.07366v1 Announce Type: cross  Abstract: Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for unseen test data. The primary challenge of TTA is limited access to the entire test dataset during online updates, causing error accumulation. To mitigate it, TTA methods have utilized the model output's entropy as a confidence metric that aims to determine which samples have a lower likelihood of causing error. Through experimental studies, however, we observed the unreliability of entropy as a confidence metric for TTA under biased scenarios and theoretically revealed that it stems from the neglect of the influence of latent disentangled factors of data on predictions. Building upon these findings, we introduce a novel TTA method named Destroy Your Object (DeYO), which leverages a newly proposed confidence metric named Pseudo-Label Probability Difference (PLPD). PLPD quantifies the influence of the shape of an object on prediction by measuring the difference b
    
[^75]: 挑战遗忘：揭示机器遗忘中最坏情况遗忘集

    Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning

    [https://arxiv.org/abs/2403.07362](https://arxiv.org/abs/2403.07362)

    该论文从对抗的角度提出了一种新的机器遗忘评估方法，通过确定最具挑战性的数据子集，即最坏情况遗忘集，来增强对影响擦除的挑战。

    

    靠谱的机器学习(Machine Learning, ML)社区越来越认识到模型在训练后有选择性地“遗忘”数据点的重要性。这引出了机器遗忘(Machine Unlearning, MU)问题，旨在消除选定数据点对模型性能的影响，同时仍保持模型在遗忘后的实用性。尽管有各种MU方法来擦除数据影响，评估主要集中在随机数据遗忘上，忽视了对于真实衡量遗忘性能的数据子集选择的重要探究。为解决这一问题，我们从对抗的角度引入了一种新的MU评估视角。我们提出确定那些对影响擦除构成最大挑战的数据子集，即找出最坏情况遗忘集。利用双层优化原则，我们增强了在上层优化中的遗忘挑战。

    arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
    
[^76]: 预感：利用生成模型预测持续学习中未来数据变化

    Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning

    [https://arxiv.org/abs/2403.07356](https://arxiv.org/abs/2403.07356)

    结合大型语言模型和图像生成模型，在持续学习中利用预感来预测数据变化，为监督预训练提供了新的途径。

    

    持续学习要求模型能够适应数据分布的持续变化，通常也要适应要执行的任务集。然而，数据和任务变化很少是完全不可预测的。鉴于一个概括性目标或数据主题的描述，我们称之为领域，人类通常可以猜测与之相关的概念。我们在这里展示，大型语言模型和图像生成模型的组合可以类似地提供有用的预感，以预测持续学习挑战随时间如何发展。我们使用大型语言模型生成未来可能出现在数据流中的语义相关类别的文本描述。然后利用稳定扩散来生成新的带标签图像样本。生成的合成数据集用于监督预训练，但在开始持续学习之前会被丢弃。

    arXiv:2403.07356v1 Announce Type: cross  Abstract: Continual learning requires a model to adapt to ongoing changes in the data distribution, and often to the set of tasks to be performed. It is rare, however, that the data and task changes are completely unpredictable. Given a description of an overarching goal or data theme, which we call a realm, humans can often guess what concepts are associated with it. We show here that the combination of a large language model and an image generation model can similarly provide useful premonitions as to how a continual learning challenge might develop over time. We use the large language model to generate text descriptions of semantically related classes that might potentially appear in the data stream in future. These descriptions are then rendered using Stable Diffusion to generate new labelled image samples. The resulting synthetic dataset is employed for supervised pre-training, but is discarded prior to commencing continual learning, along 
    
[^77]: 具有高效部分重新训练的图去除方法

    Graph Unlearning with Efficient Partial Retraining

    [https://arxiv.org/abs/2403.07353](https://arxiv.org/abs/2403.07353)

    提出了一种新颖的图去除框架GraphRevoker，通过图属性感知划分和图对比子模型聚合，更好地保持了不可训练GNNs的模型效用。

    

    图神经网络（GNNs）在各种现实世界应用中取得了显著成功。然而，GNNs 可能会在不良的图数据上进行训练，这可能会降低它们的性能和可靠性。为了让已经训练过的GNNs能够有效地去除不需要的数据，一种理想的解决方案是基于重新训练的图去除方法，该方法将训练图分成子图，并在其上训练子模型，从而通过部分重新训练实现快速去除。然而，图分区过程会导致训练图中的信息丢失，从而导致子GNN模型的模型效用较低。

    arXiv:2403.07353v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various real-world applications. However, GNNs may be trained on undesirable graph data, which can degrade their performance and reliability. To enable trained GNNs to efficiently unlearn unwanted data, a desirable solution is retraining-based graph unlearning, which partitions the training graph into subgraphs and trains sub-models on them, allowing fast unlearning through partial retraining. However, the graph partition process causes information loss in the training graph, resulting in the low model utility of sub-GNN models. In this paper, we propose GraphRevoker, a novel graph unlearning framework that better maintains the model utility of unlearnable GNNs. Specifically, we preserve the graph property with graph property-aware sharding and effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation. We conduct extensive experime
    
[^78]: IM-Unpack: 使用任意低精度整数进行训练和推断

    IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers

    [https://arxiv.org/abs/2403.07339](https://arxiv.org/abs/2403.07339)

    本研究旨在验证在移除低位宽限制时，对于各种Transformer-based模型，整数是否足以满足所有GEMM需求（训练和推断阶段），并且可以与浮点数相媲美，而无需复杂技巧。

    

    GEneral Matrix Multiply (GEMM)是深度学习中的一个核心操作，对应于计算占比最大的部分。因此，提高其效率是一个正在进行研究的热门主题。一种流行的策略是使用低位宽整数来近似矩阵中的原始条目。这样可以提高效率，但常常需要复杂的技术来控制产生的舍入误差。在这项工作中，我们首次验证当移除低位宽限制时，对于各种基于Transformer的模型，整数是否足够满足所有GEMMs的需求 - 无论是训练阶段还是推断阶段，并且可以与浮点数对应项达到一致。无需复杂技术。我们发现，虽然在这些模型中遇到的大多数矩阵条目可以很容易地用低位宽整数表示，但存在一些重要条目

    arXiv:2403.07339v1 Announce Type: cross  Abstract: GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of Transformer-based models, whether integers are sufficient for all GEMMs need -- for {\em both} training and inference stages, and can achieve parity with floating point counterparts. No sophisticated techniques are needed. We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by {\em low} bit-width integers, the existence of a few heavy hitter entries m
    
[^79]: 未知领域不一致性最小化的域泛化方法

    Unknown Domain Inconsistency Minimization for Domain Generalization

    [https://arxiv.org/abs/2403.07329](https://arxiv.org/abs/2403.07329)

    该论文提出了一种名为未知领域不一致性最小化（UDIM）的方法，通过降低源领域和未知领域之间的损失景观不一致性来改善领域泛化。

    

    领域泛化（DG）的目标是增强从源领域学得的模型对未观测领域的可迁移性。为了防止对特定领域的过拟合，Sharpness-Aware Minimization（SAM）减少源领域的损失尖锐度。尽管SAM的变种在DG中取得了显著的改进，但我们指出，在数据空间中仍有潜力通过探索来提高对未知领域的泛化能力。本文介绍了一种根植于参数和数据扰动区域的领域泛化目标，命名为未知领域不一致性最小化（UDIM）。UDIM减少了源领域和未知领域之间的损失景观不一致性。由于无法访问未知领域，这些域是通过扰动源领域数据集中的实例来经验性地构建的。具体来说，通过将在源领域中获得的损失景观与损失景观对齐

    arXiv:2403.07329v1 Announce Type: new  Abstract: The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain's loss sharpness. Although SAM variants have delivered significant improvements in DG, we highlight that there's still potential for improvement in generalizing to unknown domains through the exploration on data space. This paper introduces an objective rooted in both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM). UDIM reduces the loss landscape inconsistency between source domain and unknown domains. As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset. In particular, by aligning the loss landscape acquired in the source domain to the loss landscap
    
[^80]: 一个问题中心的多专家对比学习框架，用于提高深度序列知识追踪模型的准确性和可解释性

    A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models

    [https://arxiv.org/abs/2403.07322](https://arxiv.org/abs/2403.07322)

    通过一个问题中心的多专家对比学习框架，提高深度序列知识追踪模型的准确性和可解释性，解决了知识追踪中个体问题信息建模和模型预测结果解释的重要挑战

    

    知识追踪在通过分析学生历史学习过程来预测其未来表现中发挥着至关重要的作用。深度神经网络在解决知识追踪问题方面展现出巨大潜力。然而，将深度学习技术应用于模拟知识追踪过程仍然存在一些重要挑战。第一个挑战在于将问题的个体信息融入建模中。这很关键，因为尽管问题共享相同的知识组件（KC），但学生对同质问题的知识习得可以有显著差异。第二个挑战在于解释现有基于深度学习的知识追踪模型的预测结果。在真实应用中，虽然可能并不需要完全透明和可解释的模型参数，但关键是以老师能理解的方式呈现模型的预测结果。

    arXiv:2403.07322v1 Announce Type: cross  Abstract: Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This ma
    
[^81]: 用格点变换编码接近神经压缩中的速率失真极限

    Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding

    [https://arxiv.org/abs/2403.07320](https://arxiv.org/abs/2403.07320)

    格点变换编码（LTC）通过在潜空间中采用格点量化，实现了神经压缩中接近速率失真极限的优化。

    

    神经压缩在设计具有良好速率失真（RD）性能但复杂度低的有损压缩器方面取得了巨大进展。迄今为止，神经压缩设计涉及将源转换为潜变量，然后舍入为整数并进行熵编码。尽管这种方法已被证明在某些源上的一次性情况下是最佳的，但我们表明在i.i.d.序列上它是高度次优的，事实上总是恢复原始源序列的标量量化。我们展示亚优越性是由于潜空间中量化方案的选择，而非变换设计所致。通过在潜空间中采用格点量化而非标量量化，我们展示了格点变换编码（Lattice Transform Coding，LTC）能够在各个维度上恢复最佳矢量量化，并在合理的复杂度下接近渐近可实现的速率失真函数。

    arXiv:2403.07320v1 Announce Type: cross  Abstract: Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal in a one-shot sense on certain sources, we show that it is highly sub-optimal on i.i.d. sequences, and in fact always recovers scalar quantization of the original source sequence. We demonstrate that the sub-optimality is due to the choice of quantization scheme in the latent space, and not the transform design. By employing lattice quantization instead of scalar quantization in the latent space, we demonstrate that Lattice Transform Coding (LTC) is able to recover optimal vector quantization at various dimensions and approach the asymptotically-achievable rate-distortion function at reasonable complexity. 
    
[^82]: 可定制化头像的动态面部表情编码表达（CADyFACE）以提升用户参与度

    Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) for Improved User Engagement

    [https://arxiv.org/abs/2403.07314](https://arxiv.org/abs/2403.07314)

    该研究提出了一种定制化头像系统CADyFACE，通过FACS标记的动态面部表情，以及一种新颖的神经网络BeCoME-Net来量化用户对刺激的面部反应。

    

    定制化的3D头像为基础的面部表情刺激可能提高用户在行为生物标志物发现和自闭症、阿尔茨海默病、面瘫等疾病治疗干预中的参与度。然而，缺乏具有面部动作编码系统（FACS）动作单元（AU）标签的可定制化头像刺激。因此，本研究侧重于（1）具有FACS标记的可定制化头像表达刺激，以维持受试者的参与度，（2）基于学习的测量，量化受试者对此类刺激的面部反应，以及（3）验证由刺激-测量对表示的构造。我们提出了由获得FACS专家认证的AU标记的Customizable Avatars with Dynamic Facial Action Coded Expressions（CADyFACE）。为了测量受试者对CADyFACE的AU的反应，我们提出了一种新颖的Beta引导相关和多任务表达学习神经网络（BeCoME-Net）用于多标签AU检测。

    arXiv:2403.07314v1 Announce Type: cross  Abstract: Customizable 3D avatar-based facial expression stimuli may improve user engagement in behavioral biomarker discovery and therapeutic intervention for autism, Alzheimer's disease, facial palsy, and more. However, there is a lack of customizable avatar-based stimuli with Facial Action Coding System (FACS) action unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled, customizable avatar-based expression stimuli for maintaining subjects' engagement, (2) learning-based measurements that quantify subjects' facial responses to such stimuli, and (3) validation of constructs represented by stimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) labeled with AUs by a certified FACS expert. To measure subjects' AUs in response to CADyFACE, we propose a novel Beta-guided Correlation and Multi-task Expression learning neural network (BeCoME-Net) for multi-label AU detection. 
    
[^83]: 知识图谱大型语言模型（KG-LLM）用于链接预测

    Knowledge Graph Large Language Model (KG-LLM) for Link Prediction

    [https://arxiv.org/abs/2403.07311](https://arxiv.org/abs/2403.07311)

    该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。

    

    在知识图谱分析领域，预测知识图谱（KGs）内多个链接的任务是一个挑战，由于自然语言处理（NLP）和知识图嵌入技术的进步，这一挑战变得越来越可解决。本文介绍了一种新的方法，即知识图谱大型语言模型框架（KG-LLM），该框架利用关键的NLP范例，包括思维链提示（CoT）和上下文学习（ICL），以增强知识图谱中的多跳链接预测。通过将KG转换为CoT提示，我们的框架旨在识别并学习实体及其相互关系的潜在表示。为了展示KG-LLM框架的有效性，我们在该框架内微调了三种主要的大型语言模型（LLMs），同时采用了非ICL和ICL任务进行全面评估。此外，我们探讨了该框架为LLMs提供零次尝试能力的潜力。

    arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
    
[^84]: 推动少数群体份额如何影响泛化？关于一层隐藏层神经网络在群体不平衡上的理论研究

    How does promoting the minority fraction affect generalization? A theoretical study of the one-hidden-layer neural network on group imbalance

    [https://arxiv.org/abs/2403.07310](https://arxiv.org/abs/2403.07310)

    本文通过高斯混合模型量化了群体不平衡对样本复杂性、收敛速率和平均以及群体级测试性能的影响，首次提供了ERM在群体级泛化的理论分析。

    

    群体不平衡一直是经验风险最小化（ERM）中已知的问题，其中取得的高平均准确率伴随着少数群体的低准确率。尽管有算法努力改善少数群体的准确性，但关于ERM在各个群体上的理论泛化分析仍然难以实现。通过用高斯混合模型表达群体不平衡问题，本文量化了各个群体对样本复杂性、收敛速率以及平均和群体级测试性能的影响。虽然我们的理论框架集中在使用一层隐藏层神经网络进行二分类，但据我们所知，我们首次提供了ERM在群体级泛化的理论分析，除了通常研究的平均泛化性能。我们的理论结果的一些见解包括当所有群体级协方差都在...

    arXiv:2403.07310v1 Announce Type: cross  Abstract: Group imbalance has been a known problem in empirical risk minimization (ERM), where the achieved high average accuracy is accompanied by low accuracy in a minority group. Despite algorithmic efforts to improve the minority group accuracy, a theoretical generalization analysis of ERM on individual groups remains elusive. By formulating the group imbalance problem with the Gaussian Mixture Model, this paper quantifies the impact of individual groups on the sample complexity, the convergence rate, and the average and group-level testing performance. Although our theoretical framework is centered on binary classification using a one-hidden-layer neural network, to the best of our knowledge, we provide the first theoretical analysis of the group-level generalization of ERM in addition to the commonly studied average generalization performance. Sample insights of our theoretical results include that when all group-level co-variance is in th
    
[^85]: 针对败血症治疗的强化序贯决策：具有死亡分类器和变压器的POSNEGDM框架

    Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer

    [https://arxiv.org/abs/2403.07309](https://arxiv.org/abs/2403.07309)

    该研究提出了POSNEGDM框架，利用变压器模型和反馈强化器，在败血症治疗中取得显著改进，将患者生存率提高至97.39％，明显优于传统算法。

    

    败血症是由机体对感染产生夸张反应引发的一种危及生命的情况，要求紧急干预以防止严重并发症。现有的用于处理败血症的机器学习方法在离线场景中效果不佳，存活率低于50％。本文介绍了POSNEGDM框架，即“用于序贯决策的具有正负示范的强化学习”，利用创新的基于变压器的模型和反馈强化器复制专家行为，同时考虑个体患者特征。具有96.7％准确度的死亡分类器指导治疗决策取得积极结果。POSNEGDM框架显著提高了患者的生存率，挽救了97.39％的患者，优于现有的机器学习算法（决策变压器和行为克隆）的存活率分别为33.4％和43.5％。

    arXiv:2403.07309v1 Announce Type: cross  Abstract: Sepsis, a life-threatening condition triggered by the body's exaggerated response to infection, demands urgent intervention to prevent severe complications. Existing machine learning methods for managing sepsis struggle in offline scenarios, exhibiting suboptimal performance with survival rates below 50%. This paper introduces the POSNEGDM -- ``Reinforcement Learning with Positive and Negative Demonstrations for Sequential Decision-Making" framework utilizing an innovative transformer-based model and a feedback reinforcer to replicate expert actions while considering individual patient characteristics. A mortality classifier with 96.7\% accuracy guides treatment decisions towards positive outcomes. The POSNEGDM framework significantly improves patient survival, saving 97.39% of patients, outperforming established machine learning algorithms (Decision Transformer and Behavioral Cloning) with survival rates of 33.4% and 43.5%, respective
    
[^86]: 使用验证辅助学习神经网络屏障函数并具有终止保证

    Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees

    [https://arxiv.org/abs/2403.07308](https://arxiv.org/abs/2403.07308)

    提出了一种使用验证辅助学习神经网络屏障函数的终止保证方法。

    

    屏障函数是为系统建立安全保证的一般框架。然而，目前尚无通用方法找到这些函数。为了解决这一缺点，最近的方法使用自监督学习技术，通过由验证程序周期生成的训练数据来学习这些函数，从而导致一个具有验证辅助的学习框架。尽管验证辅助学习框架在自动合成屏障函数方面有巨大潜力，但该框架缺乏终止保证，并且在实践中可能在找到有效屏障函数的成功率方面存在问题。在本文中，我们提出了一种全面解决这些缺点的方法。通过对屏障函数合成的凸形式化，我们首先提出学习一个经验良好的NN基函数，然后应用一种利用凸性和验证中的反例的微调算法。

    arXiv:2403.07308v1 Announce Type: cross  Abstract: Barrier functions are a general framework for establishing a safety guarantee for a system. However, there is no general method for finding these functions. To address this shortcoming, recent approaches use self-supervised learning techniques to learn these functions using training data that are periodically generated by a verification procedure, leading to a verification-aided learning framework. Despite its immense potential in automating barrier function synthesis, the verification-aided learning framework does not have termination guarantees and may suffer from a low success rate of finding a valid barrier function in practice. In this paper, we propose a holistic approach to address these drawbacks. With a convex formulation of the barrier function synthesis, we propose to first learn an empirically well-behaved NN basis function and then apply a fine-tuning algorithm that exploits the convexity and counterexamples from the verif
    
[^87]: 通过跨模态知识蒸馏控制预训练LLMs进行广义时间序列预测

    Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation

    [https://arxiv.org/abs/2403.07300](https://arxiv.org/abs/2403.07300)

    通过跨模态知识蒸馏和LLMs对齐框架，该方法利用静态和动态知识，充分释放LLMs在时间序列预测中的潜力

    

    多变量时间序列预测最近随着深度学习模型的快速增长取得了巨大成功。然而，现有方法通常使用有限的时间数据从头开始训练模型，阻碍了它们的泛化。最近，随着大语言模型（LLMs）的激增，一些工作尝试将LLMs引入时间序列预测中。尽管取得了有希望的结果，但这些方法直接将时间序列作为LLMs的输入，忽略了时间和文本数据之间固有的模态差距。在这项工作中，我们提出了一个新颖的大语言模型和时间序列对齐框架，称为LLaTA，以充分发挥LLMs在时间序列预测挑战中的潜力。基于跨模态知识蒸馏，所提出的方法利用了预训练LLMs中的输入无关静态知识和输入相关动态知识。通过这种方式，该方法为预测模型赋能

    arXiv:2403.07300v1 Announce Type: cross  Abstract: Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting. Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge. Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers the forecasting model with f
    
[^88]: 通过自表达图结构重建对图数据进行压缩

    Graph Data Condensation via Self-expressive Graph Structure Reconstruction

    [https://arxiv.org/abs/2403.07294](https://arxiv.org/abs/2403.07294)

    通过自表达图结构重建的方法解决了图数据压缩中的问题

    

    随着训练大规模图神经网络（GNNs）需求的增加，图数据压缩已经成为在训练阶段减轻存储和时间成本的关键技术。它旨在将原始大规模图压缩为一个更小的合成图，同时保留训练下游GNN所需的基本信息。然而，现有方法要么集中于仅优化节点特征，要么努力独立学习节点特征和图结构生成器。它们无法明确利用原始图结构的信息，并未能为合成数据集构建可解释的图结构。为了解决这些问题，我们引入了一种名为\textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR})的新型框架。我们的方法突出之处在于

    arXiv:2403.07294v1 Announce Type: cross  Abstract: With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named \textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR}). Our method stands out by 
    
[^89]: 一种成本效益和自适应的LLM摇晃和恢复机制框架

    A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism

    [https://arxiv.org/abs/2403.07283](https://arxiv.org/abs/2403.07283)

    介绍了一种名为CypherTalk的成本效益和自适应的LLM摇晃调整和恢复机制，通过优化摇晃操作符设置，实现了在成本、模型效用和隐私之间权衡的结果。

    

    随着大型语言模型（LLMs）在真实应用中取得巨大成功，越来越多的用户希望通过云服务开发和部署他们定制的LLMs。然而，在一些特定领域，人们仍然关注成本、隐私问题和准确性之间的权衡。本研究引入了一种名为CypherTalk的成本效益和自适应LLM摇晃调整和恢复机制，通过精心设计的水平和垂直摇晃操作符，我们能够实现与基于密码学或差分隐私方法的LLM隐私保护方案相当的准确性结果。实验证明，使用CypherTalk框架，用户可以在使用优化摇晃操作符设置时实现可靠的准确性。据我们所知，这是首个考虑在LLM场景中成本、模型效用和隐私之间权衡的工作。

    arXiv:2403.07283v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) gain great success in real-world applications, an increasing number of users are seeking to develop and deploy their customized LLMs through cloud services. Nonetheless, in some specific domains, there are still concerns regarding cost and trade-offs between privacy issues and accuracy. In this study, we introduce a cost-effective and self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With carefully designed horizontal and vertical shaking operators, we can achieve comparable accuracy results with SOTA privacy-preserving LLM schemes using Cryptography-based or Differential Privacy-based methods. Experiments also show that with the CypherTalk framework, users can achieve reliable accuracy when using optimized shaking operator settings. To our best knowledge, this is the first work that considers cost, and trade-off between model utility and privacy in LLM scenarios.
    
[^90]: 通过灵活的非参数后验抽样增强迁移学习

    Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling

    [https://arxiv.org/abs/2403.07282](https://arxiv.org/abs/2403.07282)

    这项研究引入了灵活的非参数后验抽样方法，命名为非参数迁移学习（NPTL），用以解决在非参数学习背景下的分布转移问题，并提高了迁移学习的性能。

    

    近期工作表明，涉及深度神经网络的各种任务中，迁移学习显示出了显著的性能。在这些迁移学习场景中，下游数据的先验分布在贝叶斯模型平均（BMA）中变得至关重要。虽然先前的工作提出了以围绕预训练解决方案为中心的神经网络参数的先验，但在处理上游数据和下游数据之间的分布转移时，这样的策略存在局限性。本文介绍了非参数迁移学习（NPTL），一种灵活的后验抽样方法，以解决在非参数学习背景下的分布转移问题。非参数学习（NPL）方法是一种最近引入的方法，它采用了非参数先验进行后验抽样，有效地解决了模型错误规范情况，适用于涉及上游和下游数据之间分布转移的迁移学习场景。

    arXiv:2403.07282v1 Announce Type: new  Abstract: Transfer learning has recently shown significant performance across various tasks involving deep neural networks. In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning. The nonparametric learning (NPL) method is a recent approach that employs a nonparametric prior for posterior sampling, efficiently accounting for model misspecification scenarios, which is suitable for transfer learning scenarios that may involve the distribution shift between upstream and do
    
[^91]: Anderson加速用于迭代重新加权的$\ell_1$算法

    Anderson acceleration for iteratively reweighted $\ell_1$ algorithm

    [https://arxiv.org/abs/2403.07271](https://arxiv.org/abs/2403.07271)

    提出了一种Anderson加速的IRL1算法，将其收敛结果扩展到非光滑场景，不依赖于Kurdyka-Lojasiewicz条件

    

    迭代重新加权L1（IRL1）算法是一种常见算法，用于解决具有非凸和非光滑正则化的稀疏优化问题。其加速算法的发展，通常采用Nesterov加速，引起了极大兴趣。然而，这些加速算法的收敛性和复杂性分析一直存在重大挑战。最近，Anderson加速因其在加速固定点迭代方面的出色性能而备受瞩目，许多最近的研究将其应用于基于梯度的算法。受到Anderson加速强大影响的启发，我们提出了一种Anderson加速的IRL1算法，并建立了其局部线性收敛速度。我们将这一通常在平滑设置中观察到的收敛结果扩展到非光滑场景。重要的是，我们的理论结果不依赖于Kurdyka-Lojasiewicz条件。

    arXiv:2403.07271v1 Announce Type: cross  Abstract: Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving sparse optimization problems with nonconvex and nonsmooth regularization. The development of its acceleration algorithm, often employing Nesterov acceleration, has sparked significant interest. Nevertheless, the convergence and complexity analysis of these acceleration algorithms consistently poses substantial challenges. Recently, Anderson acceleration has gained prominence owing to its exceptional performance for speeding up fixed-point iteration, with numerous recent studies applying it to gradient-based algorithms. Motivated by the powerful impact of Anderson acceleration, we propose an Anderson-accelerated IRL1 algorithm and establish its local linear convergence rate. We extend this convergence result, typically observed in smooth settings, to a nonsmooth scenario. Importantly, our theoretical results do not depend on the Kurdyka-Lojasiewicz condition, a
    
[^92]: 近插值器：快速范数增长与插值与泛化之间的权衡

    Near-Interpolators: Rapid Norm Growth and the Trade-Off between Interpolation and Generalization

    [https://arxiv.org/abs/2403.07264](https://arxiv.org/abs/2403.07264)

    研究了几乎插值线性回归器的泛化能力，证明了范数增长迅速且插值与泛化之间存在明确的权衡关系。

    

    我们研究了几乎插值线性回归器的泛化能力：其训练误差τ为正但很小，即低于噪声水平。在对数据分布进行随机矩阵理论假设和对数据协方差矩阵Σ进行特征衰减假设的情况下，我们证明了任何近插值器都表现出快速的范数增长：对于固定的τ，β的平方∥β∥2 的增长率为Ω(n^α)，其中n为样本数量，α>1是特征衰减的指数，即λ_i(Σ)∼i^(-α)。这意味着现有的独立于数据的范数界限必定松弛。另一方面，在相同的区间内，我们精确地刻画了插值和泛化之间的渐近权衡。我们的表征揭示出了大多数现有范数界限是宽松的。

    arXiv:2403.07264v1 Announce Type: cross  Abstract: We study the generalization capability of nearly-interpolating linear regressors: $\boldsymbol{\beta}$'s whose training error $\tau$ is positive but small, i.e., below the noise floor. Under a random matrix theoretic assumption on the data distribution and an eigendecay assumption on the data covariance matrix $\boldsymbol{\Sigma}$, we demonstrate that any near-interpolator exhibits rapid norm growth: for $\tau$ fixed, $\boldsymbol{\beta}$ has squared $\ell_2$-norm $\mathbb{E}[\|{\boldsymbol{\beta}}\|_{2}^{2}] = \Omega(n^{\alpha})$ where $n$ is the number of samples and $\alpha >1$ is the exponent of the eigendecay, i.e., $\lambda_i(\boldsymbol{\Sigma}) \sim i^{-\alpha}$. This implies that existing data-independent norm-based bounds are necessarily loose. On the other hand, in the same regime we precisely characterize the asymptotic trade-off between interpolation and generalization. Our characterization reveals that larger norm scalin
    
[^93]: 通过两步形式预测实现自适应边界框不确定性

    Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction

    [https://arxiv.org/abs/2403.07263](https://arxiv.org/abs/2403.07263)

    通过两步形式预测方法，本文实现了自适应边界框不确定性的量化，保证了对象边界框不确定性区间的覆盖率，包括了错误分类的对象，同时确保边界框区间能够适应物体大小，实现更平衡的覆盖率。

    

    量化模型的预测不确定性对于像自动驾驶这样的安全关键应用至关重要。我们考虑为多物体检测量化这种不确定性。具体来说，我们利用形式预测来获得具有保证覆盖率的物体边界框不确定性区间。这样做的一个挑战是边界框的预测取决于物体的类别标签。因此，我们开发了一种新颖的两步形式方法，将对预测类别标签的不确定性传播到边界框的不确定性区间中。这样，我们的形式覆盖保证的有效性更广泛，包括了被错误分类的物体，确保它们在需要最大安全保证时的实用性。此外，我们研究了新颖的集成和分位数回归形式，以确保边界框区间能够适应物体大小，从而实现更平衡的覆盖率。

    arXiv:2403.07263v1 Announce Type: cross  Abstract: Quantifying a model's predictive uncertainty is essential for safety-critical applications such as autonomous driving. We consider quantifying such uncertainty for multi-object detection. In particular, we leverage conformal prediction to obtain uncertainty intervals with guaranteed coverage for object bounding boxes. One challenge in doing so is that bounding box predictions are conditioned on the object's class label. Thus, we develop a novel two-step conformal approach that propagates uncertainty in predicted class labels into the uncertainty intervals for the bounding boxes. This broadens the validity of our conformal coverage guarantees to include incorrectly classified objects, ensuring their usefulness when maximal safety assurances are required. Moreover, we investigate novel ensemble and quantile regression formulations to ensure the bounding box intervals are adaptive to object size, leading to a more balanced coverage across
    
[^94]: 适应优势的策略优化用于离线强化学习

    Advantage-Aware Policy Optimization for Offline Reinforcement Learning

    [https://arxiv.org/abs/2403.07262](https://arxiv.org/abs/2403.07262)

    介绍了一种新的适应优势的策略优化（A2PO）方法，用于离线学习，能够解决多行为策略收集的约束冲突问题，有效避免过拟合问题。

    

    离线强化学习致力于利用离线数据集来制定有效的智能体策略，而无需在线交互，通过在行为策略的支持下施加适当的保守约束来解决分布外问题。本文引入了一种新的适应优势的策略优化（A2PO）方法，以明确构建针对混合质量数据集的离线学习优势感知策略约束。

    arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
    
[^95]: 通过对抗性数据增强来将策略从离线任务表示学习中解藕

    Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation

    [https://arxiv.org/abs/2403.07261](https://arxiv.org/abs/2403.07261)

    通过对抗性数据增强方法，该研究解决了从有限数量策略中学习任务表示的问题，从而将策略与离线任务表示学习分离。

    

    离线元强化学习（OMRL）高效地允许一个代理在仅依赖于静态数据集的情况下处理新颖任务。为了精准高效地识别任务，现有的OMRL研究建议学习单独的任务表示，并将其与策略输入结合，从而形成基于上下文的元策略。训练任务表示的主要方法是采用使用多任务离线数据的对比学习。数据集通常涵盖来自各种策略（即行为策略）的相互作用，从而提供了关于不同任务的大量上下文信息。然而，在现实设置中，收集来自大量策略的数据不仅不切实际，而且通常难以实现。相反，我们转而采取更受限制但更实际的情形，即使用有限数量的策略进行多任务数据收集。我们观察到，学习到的任务表示

    arXiv:2403.07261v1 Announce Type: cross  Abstract: Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset. For precise and efficient task identification, existing OMRL research suggests learning separate task representations that be incorporated with policy input, thus forming a context-based meta-policy. A major approach to train task representations is to adopt contrastive learning using multi-task offline data. The dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks. Nonetheless, amassing data from a substantial number of policies is not only impractical but also often unattainable in realistic settings. Instead, we resort to a more constrained yet practical scenario, where multi-task data collection occurs with a limited number of policies. We observed that learned task representatio
    
[^96]: 基于深度学习辅助的机器类型通信中无授权NOMA的并行干扰消除

    Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication

    [https://arxiv.org/abs/2403.07255](https://arxiv.org/abs/2403.07255)

    论文提出了一种基于深度学习辅助的并行干扰消除方法，用于在无授权NOMA系统中联合处理活动检测、信道估计和数据检测问题。

    

    在这篇论文中，我们提出了一种新颖的方法，用于在上行无授权非正交多址接入（NOMA）系统中联合进行活动检测（AD）、信道估计（CE）和数据检测（DD）。我们的方法采用了一个受并行干扰消除（PIC）启发的迭代和并行干扰移除策略，并结合了深度学习来共同解决AD、CE和DD问题。基于这种方法，我们开发了三种PIC框架，每种框架都设计用于相干或非一致方案。第一个框架在相干方案中使用接收到的导频信号进行联合AD和CE。在此框架基础上，第二个框架利用接收到的导频和数据信号进行CE，进一步增强了相干方案中AD、CE和DD的性能。第三个框架设计用于适应包含少量数据位的非相干方案，同时实现...

    arXiv:2403.07255v1 Announce Type: cross  Abstract: In this paper, we present a novel approach for joint activity detection (AD), channel estimation (CE), and data detection (DD) in uplink grant-free non-orthogonal multiple access (NOMA) systems. Our approach employs an iterative and parallel interference removal strategy inspired by parallel interference cancellation (PIC), enhanced with deep learning to jointly tackle the AD, CE, and DD problems. Based on this approach, we develop three PIC frameworks, each of which is designed for either coherent or non-coherence schemes. The first framework performs joint AD and CE using received pilot signals in the coherent scheme. Building upon this framework, the second framework utilizes both the received pilot and data signals for CE, further enhancing the performances of AD, CE, and DD in the coherent scheme. The third framework is designed to accommodate the non-coherent scheme involving a small number of data bits, which simultaneously perf
    
[^97]: GuideGen：一种用于联合CT体积和解剖结构生成的文本引导框架

    GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation

    [https://arxiv.org/abs/2403.07247](https://arxiv.org/abs/2403.07247)

    该论文提出了一种名为GuideGen的框架，可以根据文本提示联合生成CT图像和腹部器官以及结直肠癌组织掩膜，为医学图像分析领域提供了一种生成数据集的新途径。

    

    arXiv:2403.07247v1 公告类型：交叉 摘要：为了收集带有图像和相应标签的大型医学数据集而进行的注释负担和大量工作很少是划算且令人望而生畏的。这导致了缺乏丰富的训练数据，削弱了下游任务，并在一定程度上加剧了医学领域面临的图像分析挑战。作为一种权宜之计，鉴于生成性神经模型的最近成功，现在可以在外部约束的引导下以高保真度合成图像数据集。本文探讨了这种可能性，并提出了GuideGen：一种联合生成腹部器官和结直肠癌CT图像和组织掩膜的管线，其受文本提示条件约束。首先，我们介绍了体积掩膜采样器，以适应掩膜标签的离散分布并生成低分辨率3D组织掩膜。其次，我们的条件图像生成器会在收到相应文本提示的情况下自回归生成CT切片。

    arXiv:2403.07247v1 Announce Type: cross  Abstract: The annotation burden and extensive labor for gathering a large medical dataset with images and corresponding labels are rarely cost-effective and highly intimidating. This results in a lack of abundant training data that undermines downstream tasks and partially contributes to the challenge image analysis faces in the medical field. As a workaround, given the recent success of generative neural models, it is now possible to synthesize image datasets at a high fidelity guided by external constraints. This paper explores this possibility and presents \textbf{GuideGen}: a pipeline that jointly generates CT images and tissue masks for abdominal organs and colorectal cancer conditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to fit the discrete distribution of mask labels and generate low-resolution 3D tissue masks. Secondly, our Conditional Image Generator autoregressively generates CT slices conditioned on a corre
    
[^98]: 通过双域匹配进行时间序列分类的数据集压缩

    Dataset Condensation for Time Series Classification via Dual Domain Matching

    [https://arxiv.org/abs/2403.07245](https://arxiv.org/abs/2403.07245)

    本文提出了一种名为“Dataset Condensation”的新框架，通过双域匹配能够更有效地利用时间序列数据中的丰富信息。

    

    时间序列数据在各种研究领域中被证明是至关重要的。管理大量的时间序列数据在深度学习任务方面存在挑战，特别是训练深度神经网络。最近，一种名为“数据集压缩”技术已经成为解决这一问题的方法。该技术生成一个较小的合成数据集，其在诸如分类等下游任务中具有与完整真实数据集相近的性能。然而，先前的方法主要设计用于图像和图数据集，直接将它们适应于时间序列数据集会导致性能不佳，因为它们无法有效利用时间序列数据中固有的丰富信息，尤其是在频域中。

    arXiv:2403.07245v1 Announce Type: new  Abstract: Time series data has been demonstrated to be crucial in various research fields. The management of large quantities of time series data presents challenges in terms of deep learning tasks, particularly for training a deep neural network. Recently, a technique named \textit{Dataset Condensation} has emerged as a solution to this problem. This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification. However, previous methods are primarily designed for image and graph datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series data, particularly in the frequency domain. In this paper, we propose a novel framework named Dataset \textit{\textbf{Cond}}ensation for \textit{\textbf{T}}ime \textit{\textbf{S}}eries \textit{\textbf{
    
[^99]: 校准多模态表示：在不使用注释的情况下追求群体鲁棒性

    Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations

    [https://arxiv.org/abs/2403.07241](https://arxiv.org/abs/2403.07241)

    本文旨在探索如何减少CLIP对伪特征的依赖，从而提高群体鲁棒性，而无需使用注释数据。

    

    arXiv:2403.07241v1 公告类型：交叉 摘要：微调预训练的视觉-语言模型，如CLIP，在多样的下游任务上取得成功。然而，这种范式存在一些痛点：(i) 直接微调整个预训练模型既时间密集又计算成本高。此外，这些调整后的模型往往变得高度专业化，限制了它们在实际部署中的实用性；(ii) 最近的研究表明，预训练的视觉-语言分类器可能过度依赖于伪特征-在训练数据中与目标相关的模式，但与真实标签函数无关；(iii) 现有关于减少对伪特征依赖的研究，主要基于我们能够识别这些特征的假设，对于实际应用并没有提供确切的保证。作为一项试点研究，本工作侧重于探索在不使用任何注释的情况下减少CLIP对伪特征依赖的方法。

    arXiv:2403.07241v1 Announce Type: cross  Abstract: Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any 
    
[^100]: 可处理的城市驾驶中离散行为模式的联合预测和规划

    Tractable Joint Prediction and Planning over Discrete Behavior Modes for Urban Driving

    [https://arxiv.org/abs/2403.07232](https://arxiv.org/abs/2403.07232)

    在城市驾驶中，提出了一种可处理的联合预测和规划方法，利用学习的锚点嵌入来参数化高级驾驶行为的离散模式，并实现针对这些离散模式的闭环规划。

    

    在自动驾驶方面，训练多模态轨迹预测模型取得了显著进展。然而，有效地将这些模型与下游规划器和基于模型的控制方法整合仍然是一个悬而未决的问题。我们展示，尽管这些模型传统上被用于开环预测的评估，但它们可以被用来参数化不需要重新训练的自回归闭环模型。我们考虑了利用学习的锚点嵌入来预测多条轨迹的最近轨迹预测方法，发现这些锚点嵌入可以参数化代表高级驾驶行为的离散和独特模式。我们建议在这些离散潜在模式上执行充分反应性的闭环规划，从而使我们能够可处理地模拟每一步骤中代理之间的因果交互。

    arXiv:2403.07232v1 Announce Type: cross  Abstract: Significant progress has been made in training multimodal trajectory forecasting models for autonomous driving. However, effectively integrating these models with downstream planners and model-based control approaches is still an open problem. Although these models have conventionally been evaluated for open-loop prediction, we show that they can be used to parameterize autoregressive closed-loop models without retraining. We consider recent trajectory prediction approaches which leverage learned anchor embeddings to predict multiple trajectories, finding that these anchor embeddings can parameterize discrete and distinct modes representing high-level driving behaviors. We propose to perform fully reactive closed-loop planning over these discrete latent modes, allowing us to tractably model the causal interactions between agents at each step. We validate our approach on a suite of more dynamic merging scenarios, finding that our approa
    
[^101]: Curry-DPO：利用课程学习和排名偏好增强对齐

    Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences

    [https://arxiv.org/abs/2403.07230](https://arxiv.org/abs/2403.07230)

    提出了一种名为Curry-DPO的方法，在直接偏好优化(DPO)中利用课程学习方法，通过构建多个偏好对来训练模型，相比于标准单一对DPO设置有着更好的性能表现。

    

    直接偏好优化(DPO)是一种有效的技术，利用成对偏好数据(通常是每个用户提示选择和拒绝的响应对)将LLMs与人类偏好对齐。在实践中，对于给定提示可能会存在多个响应，这些响应的质量相对于彼此而言有所不同。有了这些多个响应的质量评级，我们提出利用这些响应为给定提示创建多个偏好对。我们的工作侧重于通过课程学习方法系统地利用构建的多个偏好对来进行DPO训练。特别是，我们根据不同的标准将这些多个偏好数据对从易到难(模拟课程训练)排序。我们详细比较了我们提出的方法与标准单一对DPO设置。我们的方法，我们称之为Curry-DPO，在MTbench、Vicuna、Wiz上始终表现出增强的性能收益。

    arXiv:2403.07230v1 Announce Type: cross  Abstract: Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, Wiz
    
[^102]: LookupFFN: 让变压器在CPU推断中更轻巧

    LookupFFN: Making Transformers Compute-lite for CPU inference

    [https://arxiv.org/abs/2403.07221](https://arxiv.org/abs/2403.07221)

    提出了一种名为LookupFFN的替代模块，通过将关键操作重新构建为内存查找，使得基于变压器的前馈网络（FFNs）在CPU推断中变得更轻巧

    

    虽然GPU集群如今是训练大型深度神经网络（DNN）模型的首选，但出于诸多原因，包括工作流程的便利性、安全性和成本，一些努力正在探讨CPU在行业的许多领域中是否可以成为推断的可行选择。受这些考虑的启发，我们研究了现代DNN架构中的一个工作模块，基于GEMM的前馈网络（FFNs），并评估它可以被制作为计算轻量（或FLOP轻量）的程度。具体来说，我们提出了一种替代公式（我们称之为LookupFFN），用于取代受最近关于使用局部敏感哈希（LSH）近似FFNs的研究启发的GEMM基础的FFNs。我们的公式将大部分基本操作重新构建为内存查找，利用了任何平台上两种资源之间的权衡：计算和内存（因为CPU提供了这种权衡）。

    arXiv:2403.07221v1 Announce Type: new  Abstract: While GPU clusters are the de facto choice for training large deep neural network (DNN) models today, several reasons including ease of workflow, security and cost have led to efforts investigating whether CPUs may be viable for inference in routine use in many sectors of the industry. But the imbalance between the compute capabilities of GPUs and CPUs is huge. Motivated by these considerations, we study a module which is a workhorse within modern DNN architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent to which it can be made compute- (or FLOP-) lite. Specifically, we propose an alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by the recent studies of using Locality Sensitive Hashing (LSH) to approximate FFNs. Our formulation recasts most essential operations as a memory look-up, leveraging the trade-off between the two resources on any platform: compute and memory (since CPUs offer it 
    
[^103]: SoK：轨迹生成是否能够兼顾隐私和实用性？

    SoK: Can Trajectory Generation Combine Privacy and Utility?

    [https://arxiv.org/abs/2403.07218](https://arxiv.org/abs/2403.07218)

    本文提出了一个旨在设计保护隐私的轨迹发布方法的框架，特别强调了选择适当隐私单位的重要性。

    

    虽然位置轨迹代表着供各种分析和基于位置的服务的宝贵数据来源，但它们可能泄漏敏感信息，如政治和宗教偏好。已经提出了不同ially private发布机制，允许在严格的隐私保证下进行分析。然而，传统的保护方案存在隐私和实用性的权衡限制，并容易受到相关性和重构攻击的威胁。合成轨迹数据生成和发布代表了保护算法的一个具有前景的替代方案。虽然最初的提议取得了显著的实用性，但未能提供严格的隐私保证。本文提出了一个框架，通过定义五个设计目标，特别强调选择适当的隐私单位的重要性，来设计一个保护隐私的轨迹发布方法。基于这一框架，我们简要讨论了现有的轨迹发布方法。

    arXiv:2403.07218v1 Announce Type: cross  Abstract: While location trajectories represent a valuable data source for analyses and location-based services, they can reveal sensitive information, such as political and religious preferences. Differentially private publication mechanisms have been proposed to allow for analyses under rigorous privacy guarantees. However, the traditional protection schemes suffer from a limiting privacy-utility trade-off and are vulnerable to correlation and reconstruction attacks. Synthetic trajectory data generation and release represent a promising alternative to protection algorithms. While initial proposals achieve remarkable utility, they fail to provide rigorous privacy guarantees. This paper proposes a framework for designing a privacy-preserving trajectory publication approach by defining five design goals, particularly stressing the importance of choosing an appropriate Unit of Privacy. Based on this framework, we briefly discuss the existing traje
    
[^104]: 使用强化学习进行无人机控制的自适应增益调度

    Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control

    [https://arxiv.org/abs/2403.07216](https://arxiv.org/abs/2403.07216)

    使用强化学习训练策略逐步调整四轴飞行器控制器的增益, 显著减小跟踪误差。

    

    该论文提出了一种使用强化学习（RL）来调整四轴飞行器控制器增益的技术。具体来说，我们使用近端政策优化（PPO）来训练一个在飞行中调整级联反馈控制器增益的策略。该控制器的主要目标是在沿着指定轨迹飞行时最小化跟踪误差。论文的主要目标是分析自适应增益策略的有效性，并将其与静态增益控制算法的性能进行比较，其中积分平方误差和积分时间平方误差被用作度量标准。结果显示，与静态增益控制器相比，自适应增益方案的跟踪误差减少了超过40％。

    arXiv:2403.07216v1 Announce Type: cross  Abstract: The paper presents a technique using reinforcement learning (RL) to adapt the control gains of a quadcopter controller. Specifically, we employed Proximal Policy Optimization (PPO) to train a policy which adapts the gains of a cascaded feedback controller in-flight. The primary goal of this controller is to minimize tracking error while following a specified trajectory. The paper's key objective is to analyze the effectiveness of the adaptive gain policy and compare it to the performance of a static gain control algorithm, where the Integral Squared Error and Integral Time Squared Error are used as metrics. The results show that the adaptive gain scheme achieves over 40$\%$ decrease in tracking error as compared to the static gain controller.
    
[^105]: 选择哪个LLM？具有收敛意识的增量时间臂的在线模型选择

    Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits

    [https://arxiv.org/abs/2403.07213](https://arxiv.org/abs/2403.07213)

    该论文提出了一种具有收敛意识的增量时间臂在线模型选择方法，用于在选择最佳模型时平衡任务奖励和探索成本。

    

    Web-based应用，如聊天机器人、搜索引擎和新闻推荐，随着LLMs的日益普及，在规模和复杂性上继续增长。在线模型选择因需要在平衡任务奖励和探索成本的同时选择最佳模型而引起了越来越多的关注。传统的选择方法通常在选择一个模型之前评估每个候选模型，随着训练和微调LLMs成本的上升，这些方法变得不切实际。此外，分配过多资源去探索表现不佳的模型是不可取的。尽管一些最新的工作利用在线臂算法来管理模型选择中的这种探索-开发权衡，但它们往往忽视了增长然后收敛趋势。

    arXiv:2403.07213v1 Announce Type: new  Abstract: Web-based applications such as chatbots, search engines and news recommendations continue to grow in scale and complexity with the recent surge in the adoption of LLMs. Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost. Organizations faces decisions like whether to employ a costly API-based LLM or a locally finetuned small LLM, weighing cost against performance. Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs. Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models. While some recent works leverage online bandit algorithm to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend i
    
[^106]: 使用理论上最优的滑动窗口方法跟踪动态高斯密度

    Tracking Dynamic Gaussian Density with a Theoretically Optimal Sliding Window Approach

    [https://arxiv.org/abs/2403.07207](https://arxiv.org/abs/2403.07207)

    通过理论上对滑动窗口高斯核密度估计器进行研究，提供了选择最优权重序列的原则指南，通过实证证据表明该加权方案相比启发式方法具有更好的跟踪性能

    

    动态密度估计在许多应用中是普遍存在的，包括计算机视觉和信号处理。解决这一问题的一种流行方法是“滑动窗口”核密度估计器。该方法存在多种不同的实现，这些实现使用启发式定义的加权序列用于观测数据。但是，权重序列是影响估计器跟踪性能的关键因素。在这项工作中，我们研究了针对不断演变的高斯密度的“滑动窗口”高斯核密度估计器的精确均方集成误差（MISE）。我们通过理论上表征准确MISE的方式提供了选择最优权重序列的原则指南，它可以被构建为受限二次规划。我们通过合成数据集提供实证证据，表明我们的加权方案确实提高了跟踪性能，与启发式方法相比有显著改进。

    arXiv:2403.07207v1 Announce Type: cross  Abstract: Dynamic density estimation is ubiquitous in many applications, including computer vision and signal processing. One popular method to tackle this problem is the "sliding window" kernel density estimator. There exist various implementations of this method that use heuristically defined weight sequences for the observed data. The weight sequence, however, is a key aspect of the estimator affecting the tracking performance significantly. In this work, we study the exact mean integrated squared error (MISE) of "sliding window" Gaussian Kernel Density Estimators for evolving Gaussian densities. We provide a principled guide for choosing the optimal weight sequence by theoretically characterizing the exact MISE, which can be formulated as constrained quadratic programming. We present empirical evidence with synthetic datasets to show that our weighting scheme indeed improves the tracking performance compared to heuristic approaches.
    
[^107]: 使用选择性状态空间模型预测急性脑功能障碍状态的多队列研究

    A multi-cohort study on prediction of acute brain dysfunction states using selective state space models

    [https://arxiv.org/abs/2403.07201](https://arxiv.org/abs/2403.07201)

    该研究利用电子健康记录数据开发了用于ICU病人急性脑功能障碍预测的自动化方法，动态预测谵妄、昏迷和死亡，填补了现有文献中的研究空白。

    

    评估急性脑功能障碍（包括重症监护室（ICU）中的谵妄和昏迷）是一项重要挑战，目前的诊断方法依赖于不经常的临床观察，我们的研究旨在利用电子健康记

    arXiv:2403.07201v1 Announce Type: cross  Abstract: Assessing acute brain dysfunction (ABD), including delirium and coma in the intensive care unit (ICU), is a critical challenge due to its prevalence and severe implications for patient outcomes. Current diagnostic methods rely on infrequent clinical observations, which can only determine a patient's ABD status after onset. Our research attempts to solve these problems by harnessing Electronic Health Records (EHR) data to develop automated methods for ABD prediction for patients in the ICU. Existing models solely predict a single state (e.g., either delirium or coma), require at least 24 hours of observation data to make predictions, do not dynamically predict fluctuating ABD conditions during ICU stay (typically a one-time prediction), and use small sample size, proprietary single-hospital datasets. Our research fills these gaps in the existing literature by dynamically predicting delirium, coma, and mortality for 12-hour intervals thr
    
[^108]: 使用属性选择和不同多模态数据源的集成改进智能辅导系统中预测学生表现

    Improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources

    [https://arxiv.org/abs/2403.07194](https://arxiv.org/abs/2403.07194)

    使用属性选择和集成方法，结合不同多模态数据源，可以改进智能辅导系统中对学生表现的预测能力。

    

    本研究旨在利用智能辅导系统来预测大学生的学习表现，使用了不同来源的数据。我们从40名学生采集和预处理了来自不同多模态来源的数据：系统日志中的学习策略，面部录像中的情绪，眼动追踪中的交互区域，以及最终知识评估的测试表现。我们的目标是通过使用属性选择和分类集成来测试是否可以改进预测。我们通过将六种分类算法应用于数值化和离散化预处理的多模态数据进行了三个实验。结果表明，使用集成和选择最佳属性的方法结合数值数据时可以得到最佳的预测结果。

    arXiv:2403.07194v1 Announce Type: cross  Abstract: The aim of this study was to predict university students' learning performance using different sources of data from an Intelligent Tutoring System. We collected and preprocessed data from 40 students from different multimodal sources: learning strategies from system logs, emotions from face recording videos, interaction zones from eye tracking, and test performance from final knowledge evaluation. Our objective was to test whether the prediction could be improved by using attribute selection and classification ensembles. We carried out three experiments by applying six classification algorithms to numerical and discretized preprocessed multimodal data. The results show that the best predictions were produced using ensembles and selecting the best attributes approach with numerical data.
    
[^109]: $\mathbf{(N,K)}$-Puzzle：一种用于基准测试生成语言模型中强化学习算法的成本效益测试平台

    $\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model

    [https://arxiv.org/abs/2403.07191](https://arxiv.org/abs/2403.07191)

    提出了一种$(N,K)$-Puzzle测试平台，用于评估和比较生成语言模型中的强化学习算法。

    

    强化学习（RL）算法的最新进展旨在提高规模化语言模型的性能。 然而，缺乏一种成本效益且标准化的测试平台，专门用于评估和比较这些算法。 为填补这一空白，我们提出了24-Puzzle的一般化版本：$(N, K)$-Puzzle，挑战语言模型以使用$N$个整数达到目标值$K$。 我们评估了已建立的RL算法（如Proximal Policy Optimization（PPO）），以及新颖方法（如Identity Policy Optimization（IPO）和Direct Policy Optimization（DPO））的有效性。

    arXiv:2403.07191v1 Announce Type: cross  Abstract: Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these algorithms. To bridge this gap, we present a generalized version of the 24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a target value $K$ with $N$ integers. We evaluate the effectiveness of established RL algorithms such as Proximal Policy Optimization (PPO), alongside novel approaches like Identity Policy Optimization (IPO) and Direct Policy Optimization (DPO).
    
[^110]: UPS: 通过跨模态适应实现偏微分方程求解的基础模型

    UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation

    [https://arxiv.org/abs/2403.07187](https://arxiv.org/abs/2403.07187)

    UPS通过跨模态适应训练神经网络，将不同PDE统一到一致的表示空间，并在少样本下达到了强有力的实验结果，优于现有基线，实现了1D和2D数据集上的最先进结果。

    

    我们介绍了UPS（统一PDE求解器），这是一种有效的数据高效方法，用于解决不同域、维度和分辨率上定义的各种时空PDE。UPS将不同的PDE统一到一致的表示空间中，并使用将LLMs与特定域神经算子相结合的统一网络架构处理各种PDE数据集合。我们通过两阶段的跨模态适应过程训练网络，利用模态对齐和多任务学习的思想。通过从预训练的LLMs进行调整并利用文本形式的元信息，我们能够使用比以前的方法少得多的训练样本，并获得强有力的实证结果。UPS在PDEBench的广泛1D和2D数据集上明显优于现有基线，对考虑的10个任务中的8个任务达到了最先进的结果。与此同时，它能够少样本快速转移至不同的PDE。

    arXiv:2403.07187v1 Announce Type: new  Abstract: We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators. We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning. By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results. UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable of few-shot transfer to different PDE f
    
[^111]: 图神经网络中的不确定性：一项调查

    Uncertainty in Graph Neural Networks: A Survey

    [https://arxiv.org/abs/2403.07185](https://arxiv.org/abs/2403.07185)

    本调查旨在全面概述图神经网络中的不确定性，并提出了关于如何识别、量化和利用不确定性来增强模型性能和 GNN 预测可靠性的观点。

    

    图神经网络（GNNs）已被广泛应用于各种现实世界的应用中。然而，GNNs的预测不确定性源自数据中的固有随机性和模型训练误差等多种因素，可能导致不稳定和错误的预测。因此，识别、量化和利用不确定性对于增强模型性能和GNN预测的可靠性是至关重要的。本调查旨在从不确定性的角度全面概述GNNs，并强调其在图学习中的整合。我们比较和总结了现有的图不确定性理论和方法，以及相应的下游任务。通过这种方式，我们弥合了理论与实践之间的差距，同时连接不同的GNN社区。此外，我们的工作为这一领域的未来方向提供了宝贵的见解。

    arXiv:2403.07185v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have been extensively used in various real-world applications. However, the predictive uncertainty of GNNs stemming from diverse sources such as inherent randomness in data and model training errors can lead to unstable and erroneous predictions. Therefore, identifying, quantifying, and utilizing uncertainty are essential to enhance the performance of the model for the downstream tasks as well as the reliability of the GNN predictions. This survey aims to provide a comprehensive overview of the GNNs from the perspective of uncertainty with an emphasis on its integration in graph learning. We compare and summarize existing graph uncertainty theory and methods, alongside the corresponding downstream tasks. Thereby, we bridge the gap between theory and practice, meanwhile connecting different GNN communities. Moreover, our work provides valuable insights into promising directions in this field.
    
[^112]: 在规模上监测AI修改的内容：AI会议同行评审中ChatGPT影响的案例研究

    Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews

    [https://arxiv.org/abs/2403.07183](https://arxiv.org/abs/2403.07183)

    该研究提出了一种估计大语料库中被大语言模型大幅修改的文本比例的方法，并在AI会议的同行评审中进行了实证分析，发现6.5%至16.9%的文本可能被LLMs大幅修改，揭示了用户行为的一些见解。

    

    我们提出了一种估计大语料库中文本可能被大语言模型（LLM）大幅修改或生成的部分比例的方法。我们的最大似然模型利用专家撰写和AI生成的参考文本，准确高效地检查语料库级别上真实世界LLM使用。我们将这种方法应用于AI会议上科学同行评审的案例研究，该研究发生在ChatGPT发布之后，包括ICLR 2024、NeurIPS 2023、CoRL 2023和EMNLP 2023。我们的研究结果表明，在这些会议提交的同行评审中，6.5%至16.9%的文本可能是由LLMs大幅修改的，即超出拼写检查或小幅更新的范围。生成文本出现的情况为用户行为提供了见解：在报告信心较低、在截止日期前提交的评论以及从评论公司

    arXiv:2403.07183v1 Announce Type: cross  Abstract: We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from review
    
[^113]: 3M-Diffusion：用于文本引导生成分子图的潜在多模态扩散

    3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs

    [https://arxiv.org/abs/2403.07179](https://arxiv.org/abs/2403.07179)

    提出了3M-Diffusion，一种新颖的多模态分子图生成方法，可以生成具有所需属性的多样化、理想情况下是新颖的分子。

    

    生成具有所需属性的分子是一项关键任务，在药物发现和材料设计中具有广泛应用。受到大型语言模型的最新进展的启发，越来越多的人对使用分子的自然语言描述来生成具有所需属性的分子产生了兴趣。大多数现有方法侧重于生成与文本描述精确匹配的分子。然而，实际应用需要能够生成具有所需属性的多样化，理想情况下是新颖的分子的方法。我们提出了一种新颖的多模态分子图生成方法3M-Diffusion，以解决这一挑战。

    arXiv:2403.07179v1 Announce Type: cross  Abstract: Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in large language models, there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to address this challenge. 3M-Diffusion first encodes molecular graphs into a graph latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the late
    
[^114]: 不要忘记我做的事：评估联邦学习中的客户贡献

    Don't Forget What I did?: Assessing Client Contributions in Federated Learning

    [https://arxiv.org/abs/2403.07151](https://arxiv.org/abs/2403.07151)

    提出了一个历史感知的博弈理论框架FLContrib，用来评估联邦学习中的客户贡献。

    

    联邦学习（FL）是一种协作机器学习（ML）方法，多个客户参与训练ML模型，而不暴露私人数据。公平准确评估客户贡献在FL中是一个重要问题，以促进激励分配并鼓励多样化客户参与统一模型训练。本文提出了一个历史感知的博弈理论框架FLContrib，用于评估在每个FL训练时期中的（潜在非独立同分布）客户参与。

    arXiv:2403.07151v1 Announce Type: cross  Abstract: Federated Learning (FL) is a collaborative machine learning (ML) approach, where multiple clients participate in training an ML model without exposing the private data. Fair and accurate assessment of client contributions is an important problem in FL to facilitate incentive allocation and encouraging diverse clients to participate in a unified model training. Existing methods for assessing client contribution adopts co-operative game-theoretic concepts, such as Shapley values, but under simplified assumptions. In this paper, we propose a history-aware game-theoretic framework, called FLContrib, to assess client contributions when a subset of (potentially non-i.i.d.) clients participate in each epoch of FL training. By exploiting the FL training process and linearity of Shapley value, we develop FLContrib that yields a historical timeline of client contributions as FL training progresses over epochs. Additionally, to assess client cont
    
[^115]: 具有随机重排的随机外推法：改进变分不等式的收敛性

    Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities

    [https://arxiv.org/abs/2403.07148](https://arxiv.org/abs/2403.07148)

    该论文针对三类变分不等式问题提出了具有随机重排的随机外推法（SEG-RR），并证明其在单调情况下实现了比均匀替换采样SEG更快的收敛速度。

    

    随机外推法（SEG）方法是解决出现在各种机器学习任务中的有限求和极小-极大优化和变分不等式问题（VIPs）的最流行算法之一。然而，现有的SEG收敛分析专注于其带替换变体，而方法的实际实现会随机重新排列分量并按顺序使用它们。与广为研究的带替换变体不同，具有随机重排的SEG（SEG-RR）缺乏已建立的理论保证。在本工作中，我们针对三类VIPs（i）强单调，（ii）仿射和（iii）单调提供了SEG-RR的收敛性分析。我们推导了SEG-RR实现比均匀带替换采样SEG具有更快收敛速度的条件。在单调设置中，我们的SEG-RR分析保证了收敛到任意精度而无需大批量大小，这是对大批量大小而言的强要求。

    arXiv:2403.07148v1 Announce Type: cross  Abstract: The Stochastic Extragradient (SEG) method is one of the most popular algorithms for solving finite-sum min-max optimization and variational inequality problems (VIPs) appearing in various machine learning tasks. However, existing convergence analyses of SEG focus on its with-replacement variants, while practical implementations of the method randomly reshuffle components and sequentially use them. Unlike the well-studied with-replacement variants, SEG with Random Reshuffling (SEG-RR) lacks established theoretical guarantees. In this work, we provide a convergence analysis of SEG-RR for three classes of VIPs: (i) strongly monotone, (ii) affine, and (iii) monotone. We derive conditions under which SEG-RR achieves a faster convergence rate than the uniform with-replacement sampling SEG. In the monotone setting, our analysis of SEG-RR guarantees convergence to an arbitrary accuracy without large batch sizes, a strong requirement needed in 
    
[^116]: 在线合同设计的新视角：异质、同质、非单纯视角代理和团队生产

    New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production

    [https://arxiv.org/abs/2403.07143](https://arxiv.org/abs/2403.07143)

    本研究从在线学习的视角研究了重复的委托-代理问题，针对不同情形提出了设计学习算法的不同方法和技术，包括异质代理、同质代理和非单纯视角代理。

    

    这项工作从在线学习的视角研究了重复的委托-代理问题。 委托方的目标是通过重复互动学习最大化其效用的最佳合同，而没有关于代理方类型（即代理方的成本和生产函数）的先验知识。 我研究了三种不同的情境，委托方在每一轮与$\textit{单个}$代理方签订合同时：1. 代理方是异质的；2. 代理方是同质的；3. 委托方与相同的代理方互动且该代理方是非单纯的。 我提出不同的方法和技术来设计每种情况下的学习算法。 对于异质代理类型，我确定了一个条件，允许将问题直接简化为Lipschitz老虎机问题。 对于相同代理方，我提出了一个基于逆博弈论的多项式样本复杂度方案来学习最佳合同。 对于战略性非单纯代理，我设计了一个低战略性

    arXiv:2403.07143v1 Announce Type: cross  Abstract: This work studies the repeated principal-agent problem from an online learning perspective. The principal's goal is to learn the optimal contract that maximizes her utility through repeated interactions, without prior knowledge of the agent's type (i.e., the agent's cost and production functions).   I study three different settings when the principal contracts with a $\textit{single}$ agent each round: 1. The agents are heterogeneous; 2. the agents are homogenous; 3. the principal interacts with the same agent and the agent is non-myopic. I present different approaches and techniques for designing learning algorithms in each setting. For heterogeneous agent types, I identify a condition that allows the problem to be reduced to Lipschitz bandits directly. For identical agents, I give a polynomial sample complexity scheme to learn the optimal contract based on inverse game theory. For strategic non-myopic agents, I design a low strategic
    
[^117]: 一个类别一个提示：使用扩散模型进行数据集精炼

    One Category One Prompt: Dataset Distillation using Diffusion Models

    [https://arxiv.org/abs/2403.07142](https://arxiv.org/abs/2403.07142)

    提出了使用扩散模型进行数据集精炼的新方法，有效地解决了数据集精炼在处理高分辨率图像和复杂架构时的可扩展性问题

    

    深度神经网络训练所需的大量数据对存储和传输方面提出了重大挑战。数据集精炼已经成为将大规模数据集的信息压缩成一组代表性合成样本的有前途的技术。然而，传统的数据集精炼方法通常在处理高分辨率图像和更复杂架构时很难有效扩展，这是由于双层优化的限制。最近，一些工作提出利用分离的优化方案将知识精炼和数据集精炼相结合，以扩大数据集精炼规模。尽管这些方法有效地解决了可扩展性问题，但它们依赖于广泛的图像增强，需要存储增强图像的软标签。在本文中，我们引入了使用扩散模型的数据集精炼（D3M）作为数据集精炼的新范式，利用

    arXiv:2403.07142v1 Announce Type: cross  Abstract: The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging
    
[^118]: 探讨尼罗尔牛视觉评分归因中的聚类分析

    Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution

    [https://arxiv.org/abs/2403.07137](https://arxiv.org/abs/2403.07137)

    该论文研究了尼罗尔牛视觉评分与测量值之间的相关性，并提出使用k均值算法对牛的一批进行聚类的新方法。

    

    通过人类视觉检查评估牛的生物类型是精准育种中非常常见和重要的实践。本文介绍了对尼罗尔牛的评分与从图像或其他仪器中可以得出的各种测量之间的相关性分析结果。它还展示了使用k均值算法进行研究，以生成利用与动物体重和视觉评分最相关的测量对一批牛进行聚类的新方法。

    arXiv:2403.07137v1 Announce Type: cross  Abstract: Assessing the biotype of cattle through human visual inspection is a very common and important practice in precision cattle breeding. This paper presents the results of a correlation analysis between scores produced by humans for Nelore cattle and a variety of measurements that can be derived from images or other instruments. It also presents a study using the k-means algorithm to generate new ways of clustering a batch of cattle using the measurements that most correlate with the animal's body weight and visual scores.
    
[^119]: 价值函数的有限表征能力及其与统计(不)效率的关联

    On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency

    [https://arxiv.org/abs/2403.07136](https://arxiv.org/abs/2403.07136)

    研究发现基于值函数的表征能力有限，导致在某些情况下基于值函数的方法在统计上低效率，这揭示了价值函数和统计效率之间的关联。

    

    在强化学习中，识别基于模型和无模型方法之间的权衡是一个核心问题。 基于值的方法提供了重要的计算优势，并且有时在统计上和基于模型的方法一样有效。 然而，当关注策略评估的核心问题时，我们发现关于转移动态的信息可能无法在价值函数空间中表示。 我们通过一系列着重于许多重要问题中出现的结构的案例研究来探究这一点。 在其中几种情况中，没有信息丢失，基于值的方法与基于模型的方法在统计效率上相当。 在其他相关示例中，信息丢失严重，基于值的方法性能严重不及基于模型的方法。 更深入的研究指出了表征能力的限制作为低效性的驱动因素，而非算法设计上的失误。

    arXiv:2403.07136v1 Announce Type: cross  Abstract: Identifying the trade-offs between model-based and model-free methods is a central question in reinforcement learning. Value-based methods offer substantial computational advantages and are sometimes just as statistically efficient as model-based methods. However, focusing on the core problem of policy evaluation, we show information about the transition dynamics may be impossible to represent in the space of value functions. We explore this through a series of case studies focused on structures that arises in many important problems. In several, there is no information loss and value-based methods are as statistically efficient as model based ones. In other closely-related examples, information loss is severe and value-based methods are severely outperformed. A deeper investigation points to the limitations of the representational power as the driver of the inefficiency, as opposed to failure in algorithm design.
    
[^120]: COMQ: 一种无需反向传播的后训练量化算法

    COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization

    [https://arxiv.org/abs/2403.07134](https://arxiv.org/abs/2403.07134)

    提出了一种名为COMQ的创新后训练量化算法，通过逐层减小重构误差来有效降低大型神经网络的存储要求，同时保持原始准确性。

    

    后训练量化（PTQ）已经成为一种将大型神经网络压缩的实用方法，使其在部署时高度高效。然而，有效地将这些模型降至低比特表示而不损害原始准确性仍然是一个关键挑战。在本文中，我们提出了一种创新的PTQ算法称为COMQ，它通过依次减小逐层重构误差来进行坐标方向上的最小化。我们考虑了广泛使用的整数量化，其中每个量化权重可以分解为一个共享的浮点标量和一个整数位编码。在固定层内，COMQ将所有缩放因子和位编码视为重构误差的变量。每次迭代都会沿着一个坐标轴改进这个错误，同时保持所有其他变量恒定。COMQ易于使用，无需调整超参数。它只涉及点乘和四舍五入。

    arXiv:2403.07134v1 Announce Type: new  Abstract: Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding o
    
[^121]: 一种用于狮子狗鼻孔图像的狭窄程度分类的新机器学习数据集

    A New Machine Learning Dataset of Bulldog Nostril Images for Stenosis Degree Classification

    [https://arxiv.org/abs/2403.07132](https://arxiv.org/abs/2403.07132)

    提出了一种新的狮子狗鼻孔图像数据集，用于狭窄程度分类，同时探索使用深度学习自动推断狭窄程度，在多种神经网络模型下进行了评估

    

    短头犬品种中的一种外形特征——短颅，引起了BOAS，一种呼吸障碍疾病，影响了狗的健康和福祉，表现为各种症状。本文提出了一个由190张狮子狗鼻孔图像组成的新的注释数据集。数据集中大约均匀代表了三种狭窄程度：轻度、中度和严重狭窄。数据集还包括少量非狭窄鼻孔图像。据我们所知，这是首个解决这一问题的图像数据集。此外，还探讨了使用鼻孔图像自动推断狭窄程度的深度学习作为替代方法。在本研究中，测试了几种神经网络：ResNet50、MobileNetV3、DenseNet201、SwinV2和MaxViT。在这个评估中，问题被建模为两种不同的方式：首先，作为一个三类分类问题（轻度或开放，中度，和严重）；其次，作为一个二分类问题

    arXiv:2403.07132v1 Announce Type: cross  Abstract: Brachycephaly, a conformation trait in some dog breeds, causes BOAS, a respiratory disorder that affects the health and welfare of the dogs with various symptoms. In this paper, a new annotated dataset composed of 190 images of bulldogs' nostrils is presented. Three degrees of stenosis are approximately equally represented in the dataset: mild, moderate and severe stenosis. The dataset also comprises a small quantity of non stenotic nostril images. To the best of our knowledge, this is the first image dataset addressing this problem. Furthermore, deep learning is investigated as an alternative to automatically infer stenosis degree using nostril images. In this work, several neural networks were tested: ResNet50, MobileNetV3, DenseNet201, SwinV2 and MaxViT. For this evaluation, the problem was modeled in two different ways: first, as a three-class classification problem (mild or open, moderate, and severe); second, as a binary classifi
    
[^122]: FAX: JAX中可扩展且可微分的联邦原语

    FAX: Scalable and Differentiable Federated Primitives in JAX

    [https://arxiv.org/abs/2403.07128](https://arxiv.org/abs/2403.07128)

    FAX是一个在JAX中嵌入联邦计算原语的库，支持大规模分布式计算，提供了联邦自动微分的实现，并可解释至现有的生产跨设备联邦计算系统。

    

    我们介绍了FAX，这是一个基于JAX设计的库，旨在支持数据中心和跨设备应用中的大规模分布式和联邦计算。FAX利用JAX的分片机制，实现了原生针对TPU和最先进的JAX运行时（包括Pathways）的定位。FAX将联邦计算的基本构件嵌入JAX中，带来了三个关键好处。首先，FAX的计算可以转换为XLA HLO。其次，FAX提供了联邦自动微分的完整实现，极大地简化了联邦计算的表达。最后，FAX的计算可以解释成现有的生产跨设备联邦计算系统。我们展示了FAX为数据中心中的联邦计算提供了易编程、高性能和可扩展的框架。FAX可在https://github.com/google-research/google-research/tree/master/fax 获取。

    arXiv:2403.07128v1 Announce Type: cross  Abstract: We present FAX, a JAX-based library designed to support large-scale distributed and federated computations in both data center and cross-device applications. FAX leverages JAX's sharding mechanisms to enable native targeting of TPUs and state-of-the-art JAX runtimes, including Pathways. FAX embeds building blocks for federated computations as primitives in JAX. This enables three key benefits. First, FAX computations can be translated to XLA HLO. Second, FAX provides a full implementation of federated automatic differentiation, greatly simplifying the expression of federated computations. Last, FAX computations can be interpreted out to existing production cross-device federated compute systems. We show that FAX provides an easily programmable, performant, and scalable framework for federated computations in the data center. FAX is available at https://github.com/google-research/google-research/tree/master/fax .
    
[^123]: 对象检测中的类别不平衡：实验诊断和缓解策略研究

    Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies

    [https://arxiv.org/abs/2403.07113](https://arxiv.org/abs/2403.07113)

    本研究针对对象检测中前景-前景类别不平衡问题，引入了基于YOLOv5检测器的基准框架，并通过构建COCO-ZIPF数据集和比较分析采样、损失加权等方法，发现它们在单阶段检测器中的效果并不如在双阶段检测器中显著。

    

    对象检测是计算机视觉中的一个关键任务，经常受到数据集不平衡的影响，尤其是前景-前景类不平衡这一未被充分探讨的问题。本研究引入了一个基于YOLOv5单阶段检测器的基准框架，以应对前景-前景类别不平衡问题。我们从COCO数据集中构建了一个新颖的10类长尾数据集，名为COCO-ZIPF，旨在反映具有有限物体类别数量的常见实际检测场景。在此背景下，我们对三种已建立的技术进行了详细研究：采样、损失加权和数据增强。我们的比较分析表明，采样和损失加权方法，在双阶段检测器设置中被证明有益的，但在单阶段检测器中并没有同样有效转化。

    arXiv:2403.07113v1 Announce Type: cross  Abstract: Object detection, a pivotal task in computer vision, is frequently hindered by dataset imbalances, particularly the under-explored issue of foreground-foreground class imbalance. This lack of attention to foreground-foreground class imbalance becomes even more pronounced in the context of single-stage detectors. This study introduces a benchmarking framework utilizing the YOLOv5 single-stage detector to address the problem of foreground-foreground class imbalance. We crafted a novel 10-class long-tailed dataset from the COCO dataset, termed COCO-ZIPF, tailored to reflect common real-world detection scenarios with a limited number of object classes. Against this backdrop, we scrutinized three established techniques: sampling, loss weighing, and data augmentation. Our comparative analysis reveals that sampling and loss reweighing methods, while shown to be beneficial in two-stage detector settings, do not translate as effectively in impr
    
[^124]: 用于来自多中心淋巴瘤数据集的自动分类的切片分类神经网络

    A slice classification neural network for automated classification of axial PET/CT slices from a multi-centric lymphoma dataset

    [https://arxiv.org/abs/2403.07105](https://arxiv.org/abs/2403.07105)

    通过训练ResNet-18网络对淋巴瘤PET/CT图像的切片进行分类，实现对肿瘤切片的自动化识别。

    

    自动切片分类在临床上具有相关性，因为它可以被纳入医学图像分割工作流程中作为一个预处理步骤，可以标记出可能包含肿瘤较高概率的切片，从而引导医生关注重要的切片。在这项工作中，我们训练了一个ResNet-18网络，根据轴性淋巴瘤PET/CT图像的切片是否与3D图像中的肿瘤相交（阳性切片）或不相交（阴性切片）来分类。

    arXiv:2403.07105v1 Announce Type: cross  Abstract: Automated slice classification is clinically relevant since it can be incorporated into medical image segmentation workflows as a preprocessing step that would flag slices with a higher probability of containing tumors, thereby directing physicians attention to the important slices. In this work, we train a ResNet-18 network to classify axial slices of lymphoma PET/CT images (collected from two institutions) depending on whether the slice intercepted a tumor (positive slice) in the 3D image or if the slice did not (negative slice). Various instances of the network were trained on 2D axial datasets created in different ways: (i) slice-level split and (ii) patient-level split; inputs of different types were used: (i) only PET slices and (ii) concatenated PET and CT slices; and different training strategies were employed: (i) center-aware (CAW) and (ii) center-agnostic (CAG). Model performances were compared using the area under the recei
    
[^125]: 用高斯平滑克服认证培训的悖论

    Overcoming the Paradox of Certified Training with Gaussian Smoothing

    [https://arxiv.org/abs/2403.07095](https://arxiv.org/abs/2403.07095)

    通过使用高斯损失平滑方法，本研究提出了一种结合PGPE算法和不同凸放宽的认证训练方法，可以在训练神经网络时缓解紧凑凸松弛带来的问题，并获得更好性能的网络。

    

    尽管付出了大量努力，但训练神经网络以高认证准确度对抗对抗性示例仍然是一个悬而未决的问题。在训练中，尽管认证方法可以有效地利用紧凑的凸松弛进行界计算，但这些方法表现不如较松的松弛。先前的工作假设这是由这些更紧的松弛导致的损失表面的不连续性和扰动敏感性。在这项研究中，我们理论上展示了高斯损失平滑可以缓解这两个问题。我们通过提出一种结合PGPE的认证训练方法，该算法计算平滑损失的梯度，并使用不同的凸放宽来确认这一点。在使用这种训练方法时，我们观察到更紧密的界限确实导致更好的网络，可以在相同网络上胜过同类技术。尽管扩展基于PGPE的训练仍然具有挑战性。

    arXiv:2403.07095v1 Announce Type: new  Abstract: Training neural networks with high certified accuracy against adversarial examples remains an open problem despite significant efforts. While certification methods can effectively leverage tight convex relaxations for bound computation, in training, these methods perform worse than looser relaxations. Prior work hypothesized that this is caused by the discontinuity and perturbation sensitivity of the loss surface induced by these tighter relaxations. In this work, we show theoretically that Gaussian Loss Smoothing can alleviate both of these issues. We confirm this empirically by proposing a certified training method combining PGPE, an algorithm computing gradients of a smoothed loss, with different convex relaxations. When using this training method, we observe that tighter bounds indeed lead to strictly better networks that can outperform state-of-the-art methods on the same network. While scaling PGPE-based training remains challengin
    
[^126]: FALCON：面向神经网络剪枝的FLOP感知组合优化

    FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning

    [https://arxiv.org/abs/2403.07094](https://arxiv.org/abs/2403.07094)

    FALCON提出了一种面向神经网络剪枝的新型优化框架，同时考虑了模型准确性、FLOPs和稀疏性约束。提出了一种新颖的一阶方法来优化网络剪枝。

    

    现代神经网络增加的计算需求给资源受限设备上的部署带来挑战。网络剪枝提供了一种减小模型大小和计算成本的解决方案，同时保持性能。然而，大多数当前的剪枝方法主要专注于通过减少非零参数的数量来提高稀疏性，通常忽略了与浮点运算数量（FLOPs）密切相关的其他部署成本，如推断时间。在本文中，我们提出了FALCON，一个新颖的基于组合优化的网络剪枝框架，它同时考虑了模型准确性（忠实度）、FLOPs和稀疏性约束。我们方法的一个主要组成部分是一个整数线性规划（ILP），它同时处理FLOP和稀疏性约束。我们提出了一个新颖的算法来近似解决ILP。我们为我们的优化框架提出了一种新颖的一阶方法。

    arXiv:2403.07094v1 Announce Type: new  Abstract: The increasing computational demands of modern neural networks present deployment challenges on resource-constrained devices. Network pruning offers a solution to reduce model size and computational cost while maintaining performance. However, most current pruning methods focus primarily on improving sparsity by reducing the number of nonzero parameters, often neglecting other deployment costs such as inference time, which are closely related to the number of floating-point operations (FLOPs). In this paper, we propose FALCON, a novel combinatorial-optimization-based framework for network pruning that jointly takes into account model accuracy (fidelity), FLOPs, and sparsity constraints. A main building block of our approach is an integer linear program (ILP) that simultaneously handles FLOP and sparsity constraints. We present a novel algorithm to approximately solve the ILP. We propose a novel first-order method for our optimization fra
    
[^127]: 用于临床PET成像中弥漫大B细胞淋巴瘤自动肿瘤检测和分割的级联深度网络

    A cascaded deep network for automated tumor detection and segmentation in clinical PET imaging of diffuse large B-cell lymphoma

    [https://arxiv.org/abs/2403.07092](https://arxiv.org/abs/2403.07092)

    该研究提出了一种快速高效的三步级联深度学习模型，用于自动检测和分割DLBCL PET图像中的肿瘤，相比于单一端到端网络，其能够显著提高分割准确性（将3D Dice得分从58.9%提高到78.1%）。

    

    精确检测和分割弥漫性大B细胞淋巴瘤(DLBCL) PET图像在总代谢性肿瘤体积估计，放射组织学分析，手术干预和放射治疗中具有重要意义。手动分割全身PET图像中的肿瘤是耗时、劳动密集和依赖操作员的。在这项工作中，我们开发并验证了一个快速有效的三步级联深度学习模型，用于从PET图像自动检测和分割DLBCL肿瘤。与用于全身PET图像中肿瘤分割的单一端到端网络相比，我们的三步模型更为有效（将3D Dice得分从58.9%提高到78.1%），因为它的各个专门模块，即切片分类器，肿瘤检测器和肿瘤分段器，可以独立训练到高技能水平以执行特定任务，而不是一个具有次优性能的单一网络。

    arXiv:2403.07092v1 Announce Type: cross  Abstract: Accurate detection and segmentation of diffuse large B-cell lymphoma (DLBCL) from PET images has important implications for estimation of total metabolic tumor volume, radiomics analysis, surgical intervention and radiotherapy. Manual segmentation of tumors in whole-body PET images is time-consuming, labor-intensive and operator-dependent. In this work, we develop and validate a fast and efficient three-step cascaded deep learning model for automated detection and segmentation of DLBCL tumors from PET images. As compared to a single end-to-end network for segmentation of tumors in whole-body PET images, our three-step model is more effective (improves 3D Dice score from 58.9% to 78.1%) since each of its specialized modules, namely the slice classifier, the tumor detector and the tumor segmentor, can be trained independently to a high degree of skill to carry out a specific task, rather than a single network with suboptimal performance 
    
[^128]: 借助先验知识和认知模型改善深度学习：增强可解释性、对抗鲁棒性和零样本学习的调查

    Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning

    [https://arxiv.org/abs/2403.07078](https://arxiv.org/abs/2403.07078)

    本文调查了如何借助先验知识和认知模型来改善深度学习，以提升对抗防御、可解释性人工智能（XAI）和零样本学习，弥补现有深度学习模型在领域知识利用、对抗性攻击防御、解释性以及在开放环境推理中的性能限制。

    

    我们审查了当前和新兴的知识驱动和脑启发的认知系统，用于实现对抗性防御、可解释的人工智能（XAI）以及零样本或少样本学习。数据驱动的深度学习模型在许多应用中取得了显著的性能，并展示了超越人类专家的能力。然而，它们由于无法利用领域知识而在实际应用中存在严重性能限制。特别是，深度学习系统容易受到对抗性攻击，这可能导致它们做出明显错误的决定。此外，复杂的数据驱动模型通常缺乏解释性，即它们的决策无法被人类主体理解。此外，模型通常是在标准数据集上训练的，具有封闭世界的假设。因此，在实际的开放环境中进行推理时，它们很难推广到未见案例。

    arXiv:2403.07078v1 Announce Type: cross  Abstract: We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environmen
    
[^129]: 使用高斯过程进行可解释学习

    Explainable Learning with Gaussian Processes

    [https://arxiv.org/abs/2403.07072](https://arxiv.org/abs/2403.07072)

    本工作探讨了在高斯过程回归中的特征归因问题，提出了一种基于模型不确定性的定义方法，得到了可解释的特征归因的闭式表达式，同时展示了GPR模型归因也遵循高斯过程分布。

    

    人工智能领域中的可解释性研究旨在开发能够揭示复杂机器学习模型进行预测的方法。很多解释方法都集中在特征归因的概念上，即将模型的预测分解为对应于每个输入特征的个体贡献。在这项工作中，我们探讨了在高斯过程回归（GPR）背景下的特征归因问题。我们采取了一种基于模型不确定性的原则性方法来定义特征归因，从而扩展了现有的文献。我们展示了，尽管GPR是一种高度灵活和非参数化的方法，我们可以推导出对特征归因的可解释的闭式表达式。当使用集成梯度作为归因方法时，我们展示了GPR模型的归因也遵循一个高斯过程分布，用以量化归因中出现的不确定性。

    arXiv:2403.07072v1 Announce Type: new  Abstract: The field of explainable artificial intelligence (XAI) attempts to develop methods that provide insight into how complicated machine learning methods make predictions. Many methods of explanation have focused on the concept of feature attribution, a decomposition of the model's prediction into individual contributions corresponding to each input feature. In this work, we explore the problem of feature attribution in the context of Gaussian process regression (GPR). We take a principled approach to defining attributions under model uncertainty, extending the existing literature. We show that although GPR is a highly flexible and non-parametric approach, we can derive interpretable, closed-form expressions for the feature attributions. When using integrated gradients as an attribution method, we show that the attributions of a GPR model also follow a Gaussian process distribution, which quantifies the uncertainty in attribution arising fro
    
[^130]: 基于重新模拟的自监督学习用于预训练基础模型

    Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models

    [https://arxiv.org/abs/2403.07066](https://arxiv.org/abs/2403.07066)

    提出了一种新颖的基于重新模拟的自监督学习策略RS3L，通过介入模拟过程并重新模拟事件实现，生成一组涵盖所有物理驱动变化的数据增强，从而促进基础模型的发展，并展示了预训练R3SL在下游任务中表现出强大性能。

    

    自监督学习（SSL）是训练现代大型机器学习模型的核心，提供了一种学习强大表示的方案，可用于各种下游任务。然而，SSL策略必须适应所需的训练数据类型和下游任务。我们提出了RS3L，一种新颖的基于模拟的SSL策略，采用重新模拟的方法来驱动对比学习的数据增强。通过介入模拟过程的中间并重新运行介入之后的模拟组件，我们生成一个事件的多个实现，从而产生一组涵盖模拟器中所有物理驱动变化的增强。通过使用高能物理实验，我们探讨了这种策略如何促进基础模型的发展；我们展示了R3SL预训练如何在下游任务中实现强大的性能，例如区分任务。

    arXiv:2403.07066v1 Announce Type: cross  Abstract: Self-Supervised Learning (SSL) is at the core of training modern large machine learning models, providing a scheme for learning powerful representations that can be used in a variety of downstream tasks. However, SSL strategies must be adapted to the type of training data and downstream tasks required. We propose RS3L, a novel simulation-based SSL strategy that employs a method of re-simulation to drive data augmentation for contrastive learning. By intervening in the middle of the simulation process and re-running simulation components downstream of the intervention, we generate multiple realizations of an event, thus producing a set of augmentations covering all physics-driven variations available in the simulator. Using experiments from high-energy physics, we explore how this strategy may enable the development of a foundation model; we show how R3SL pre-training enables powerful performance in downstream tasks such as discriminati
    
[^131]: 优于经典？量子机器学习模型基准测试的微妙艺术

    Better than classical? The subtle art of benchmarking quantum machine learning models

    [https://arxiv.org/abs/2403.07059](https://arxiv.org/abs/2403.07059)

    通过开发开源软件包进行大规模研究，研究发现总体而言，开箱即用的经典机器学习模型胜过量子分类器，还发现将量子模型中的纠缠去除通常会导致同样或更好的性能。

    

    通过经典模拟来进行基准测试是在没有无噪声硬件可用之前评估量子机器学习思想的主要方式之一。然而，实验设计对结果的巨大影响，当前可达到的小规模，以及受量子技术商业化影响的叙述使得难以获得稳健的见解。为了促进更好的决策，我们开发了一个基于PennyLane软件框架的开源软件包，并使用它进行了大规模研究，系统地测试了12种流行的量子机器学习模型在用于创建160个单独数据集的6个二元分类任务上。我们发现，总体而言，开箱即用的经典机器学习模型胜过量子分类器。此外，从量子模型中移除纠缠往往会导致同样或更好的性能，这表明“量子特性”可能并非关键成分。

    arXiv:2403.07059v1 Announce Type: cross  Abstract: Benchmarking models via classical simulations is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available. However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights. To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets. We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers. Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that "quantumness" may not be the crucial ingredi
    
[^132]: 使用GFlowNets的蚁群采样用于组合优化

    Ant Colony Sampling with GFlowNets for Combinatorial Optimization

    [https://arxiv.org/abs/2403.07041](https://arxiv.org/abs/2403.07041)

    本文提出了生成流蚁群采样器（GFACS），一种结合生成流网络与蚁群优化方法的神经引导元启发式算法，在组合优化任务中表现优于基线ACO算法并与特定问题启发式方法具有竞争力。

    

    本文介绍了生成流蚁群采样器（GFACS），这是一种新颖的用于组合优化的神经引导元启发式算法。GFACS 将生成流网络（GFlowNets）与蚁群优化（ACO）方法相结合。GFlowNets 是一种生成模型，它在组合空间中学习构造性策略，通过在输入图实例上提供决策变量的知情先验分布来增强 ACO。此外，我们引入了一种新颖的训练技巧组合，包括搜索引导的局部探索、能量归一化和能量塑形，以提高 GFACS 的性能。我们的实验结果表明，GFACS 在七个组合优化任务中优于基线 ACO 算法，并且在车辆路径问题的问题特定启发式方法中具有竞争力。源代码可在 \url{https://github.com/ai4co/gfacs} 获取。

    arXiv:2403.07041v1 Announce Type: new  Abstract: This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS integrates generative flow networks (GFlowNets) with the ant colony optimization (ACO) methodology. GFlowNets, a generative model that learns a constructive policy in combinatorial spaces, enhance ACO by providing an informed prior distribution of decision variables conditioned on input graph instances. Furthermore, we introduce a novel combination of training tricks, including search-guided local exploration, energy normalization, and energy shaping to improve GFACS. Our experimental results demonstrate that GFACS outperforms baseline ACO algorithms in seven CO tasks and is competitive with problem-specific heuristics for vehicle routing problems. The source code is available at \url{https://github.com/ai4co/gfacs}.
    
[^133]: 一站式：图神经网络的多任务提示（扩展摘要）

    All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)

    [https://arxiv.org/abs/2403.07040](https://arxiv.org/abs/2403.07040)

    本文介绍了一种新颖的多任务提示方法，用于解决预训练图模型与不同任务之间的差距，启发自NLP中提示学习的成功。

    

    本文是我们在KDD23中获得最佳研究论文奖的原始工作的扩展摘要，其中我们介绍了一个新颖的方法，用于弥合预训练图模型和它们应用于的不同任务之间的差距，灵感来源于NLP中提示学习的成功。我们意识到了将预训练模型与各种图任务（节点级、边级和图级）对齐的挑战，这可能导致负迁移和性能下降，因此我们提出了一种用于图的多任务提示方法。该方法涉及统一图和语言提示格式，使NLP的提示策略能够适用于图任务。通过分析图应用的任务空间，我们重新制定问题以适应图级任务，并应用元学习来改进提示初始化。

    arXiv:2403.07040v1 Announce Type: cross  Abstract: This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. KDD 23) The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they're applied to, inspired by the success of prompt learning in NLP. Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs. This method involves unifying graph and language prompt formats, enabling NLP's prompting strategies to be adapted for graph tasks. By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initializ
    
[^134]: 利用图神经网络支持患者自动分诊

    Leveraging graph neural networks for supporting Automatic Triage of Patients

    [https://arxiv.org/abs/2403.07038](https://arxiv.org/abs/2403.07038)

    该研究利用图神经网络为急诊科的患者分诊过程加入了人工智能模块，用于管理患者的急症编码分配，可最大化信息收集并最小化错误。

    

    患者分诊在急诊科起着关键作用，确保根据正确评估患者病情的急症等级及时和适当地提供护理。传统的分诊方法主要由人员基于自己的经验和从患者管理过程中收集的信息执行。因此，这是一个可能会产生急症级别关联错误的过程。最近，传统的分诊方法严重依赖于人类决策，这可能是主观的并且容易出现错误。最近，人们开始关注利用人工智能（AI）开发能够最大化信息收集并最小化患者分诊处理中错误的算法。我们定义并实施了一个基于AI的模块来管理急诊科中患者急症编码分配。它使用急诊科历史数据来训练医疗决策过程。数据包含有关相关性的信息。

    arXiv:2403.07038v1 Announce Type: new  Abstract: Patient triage plays a crucial role in emergency departments, ensuring timely and appropriate care based on correctly evaluating the emergency grade of patient conditions.   Triage methods are generally performed by human operator based on her own experience and information that are gathered from the patient management process.   Thus, it is a process that can generate errors in emergency level associations. Recently, Traditional triage methods heavily rely on human decisions, which can be subjective and prone to errors.   Recently, a growing interest has been focused on leveraging artificial intelligence (AI) to develop algorithms able to maximize information gathering and minimize errors in patient triage processing.   We define and implement an AI based module to manage patients emergency code assignments in emergency departments. It uses emergency department historical data to train the medical decision process. Data containing relev
    
[^135]: 一种面向边缘低延迟和高能效DNN推断的转换自编码器

    A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge

    [https://arxiv.org/abs/2403.07036](https://arxiv.org/abs/2403.07036)

    提出了一种面向边缘设备的低延迟和高能效DNN推断框架，利用“转换”自编码器和轻量级DNN，改进了现有的方法。

    

    降低推断时间和能量使用，同时保持预测准确性，已成为资源受限边缘设备上深度神经网络（DNN）推断的一个重要关注点。为了解决这个问题，我们提出了一种基于“转换”自编码器和轻量级DNN的新方法。这一方法改进了最近的工作，如提前退出框架和DNN分区。

    arXiv:2403.07036v1 Announce Type: new  Abstract: Reducing inference time and energy usage while maintaining prediction accuracy has become a significant concern for deep neural networks (DNN) inference on resource-constrained edge devices. To address this problem, we propose a novel approach based on "converting" autoencoder and lightweight DNNs. This improves upon recent work such as early-exiting framework and DNN partitioning. Early-exiting frameworks spend different amounts of computation power for different input data depending upon their complexity. However, they can be inefficient in real-world scenarios that deal with many hard image samples. On the other hand, DNN partitioning algorithms that utilize the computation power of both the cloud and edge devices can be affected by network delays and intermittent connections between the cloud and the edge. We present CBNet, a low-latency and energy-efficient DNN inference framework tailored for edge devices. It utilizes a "converting
    
[^136]: 多种群交替进化神经架构搜索

    Multiple Population Alternate Evolution Neural Architecture Search

    [https://arxiv.org/abs/2403.07035](https://arxiv.org/abs/2403.07035)

    提出了多种群交替进化神经架构搜索（MPAE）范式，能够在更小的搜索成本下实现神经网络结构的模块多样性。

    

    arXiv:2403.07035v1 公告类型: 跨领域 摘要: 进化神经架构搜索(ENAS)的有效性受到搜索空间设计的影响。然而，包括全局搜索空间、可扩展搜索空间和分层搜索空间在内的常见方法都存在一定局限性。特别是，全局搜索空间需要大量计算资源和时间，可扩展搜索空间牺牲了网络结构的多样性，而分层搜索空间则增加了搜索成本以换取网络多样性。为解决上述限制，我们提出了一种新颖的神经网络架构搜索范式，设计了多种群交替进化神经架构搜索（MPAE），可以在更小的搜索成本下实现模块多样性。MPAE将搜索空间转换为L个互连单元，并依次搜索这些单元，然后重复几次搜索整个网络。

    arXiv:2403.07035v1 Announce Type: cross  Abstract: The effectiveness of Evolutionary Neural Architecture Search (ENAS) is influenced by the design of the search space. Nevertheless, common methods including the global search space, scalable search space and hierarchical search space have certain limitations. Specifically, the global search space requires a significant amount of computational resources and time, the scalable search space sacrifices the diversity of network structures and the hierarchical search space increases the search cost in exchange for network diversity. To address above limitation, we propose a novel paradigm of searching neural network architectures and design the Multiple Population Alternate Evolution Neural Architecture Search (MPAE), which can achieve module diversity with a smaller search cost. MPAE converts the search space into L interconnected units and sequentially searches the units, then the above search of the entire network be cycled several times t
    
[^137]: 通过原型匹配解释典型故障信号的特征

    Interpreting What Typical Fault Signals Look Like via Prototype-matching

    [https://arxiv.org/abs/2403.07033](https://arxiv.org/abs/2403.07033)

    提出了原型匹配网络（PMN），结合人类固有原型匹配和自编码器（AE），通过匹配特征与原型并选择最相似的原型来解释典型故障信号的特征。

    

    神经网络具有强大的非线性映射和分类能力，在机械故障诊断中得到广泛应用以确保安全。然而，作为典型的黑盒模型，它们在高可靠性要求的场景中的应用受到限制。为了理解分类逻辑并解释典型故障信号的特征，提出了结合人类固有原型匹配和自编码器（AE）的原型匹配网络（PMN）。PMN将AE提取的特征与每个原型进行匹配，并选择最相似的原型作为预测结果。它在分类逻辑、故障原型和匹配贡献方面有三条解释路径。传统诊断和领域泛化实验证明了其具有竞争性的诊断性能以及在表示学习中的显着优势。此外，学习到的典型故障信号（即样本级原型）展示了

    arXiv:2403.07033v1 Announce Type: cross  Abstract: Neural networks, with powerful nonlinear mapping and classification capabilities, are widely applied in mechanical fault diagnosis to ensure safety. However, being typical black-box models, their application is limited in high-reliability-required scenarios. To understand the classification logic and explain what typical fault signals look like, the prototype matching network (PMN) is proposed by combining the human-inherent prototype-matching with autoencoder (AE). The PMN matches AE-extracted feature with each prototype and selects the most similar prototype as the prediction result. It has three interpreting paths on classification logic, fault prototypes, and matching contributions. Conventional diagnosis and domain generalization experiments demonstrate its competitive diagnostic performance and distinguished advantages in representation learning. Besides, the learned typical fault signals (i.e., sample-level prototypes) showcase 
    
[^138]: 用于高效同时学习和评估的Cram方法

    The Cram Method for Efficient Simultaneous Learning and Evaluation

    [https://arxiv.org/abs/2403.07031](https://arxiv.org/abs/2403.07031)

    Cram方法是一种同时学习和评估的高效方法，利用整个样本进行训练和测试，比传统的样本分割策略更高效。

    

    我们介绍了“Cram”方法，这是一种通用且高效的方法，使用通用的机器学习（ML）算法进行同时学习和评估。在批处理数据的单次传递中，该方法反复训练ML算法并测试其经验性能。由于它同时利用了整个样本进行学习和评估，所以Cram方法比样本分割要高效得多。Cram方法还自然地适用于在线学习算法，使其实施具有计算效率。为了展示Cram方法的强大之处，我们考虑了标准策略学习设置，其中将Cram应用于相同数据以开发个性化治疗规则（ITR）并估计如果学习的ITR被部署将会产生的平均结果。我们展示了在最小一组假设下，由此产生的Cram评估估计器是一致且渐近的。

    arXiv:2403.07031v1 Announce Type: new  Abstract: We introduce the "cram" method, a general and efficient approach to simultaneous learning and evaluation using a generic machine learning (ML) algorithm. In a single pass of batched data, the proposed method repeatedly trains an ML algorithm and tests its empirical performance. Because it utilizes the entire sample for both learning and evaluation, cramming is significantly more data-efficient than sample-splitting. The cram method also naturally accommodates online learning algorithms, making its implementation computationally efficient. To demonstrate the power of the cram method, we consider the standard policy learning setting where cramming is applied to the same data to both develop an individualized treatment rule (ITR) and estimate the average outcome that would result if the learned ITR were to be deployed. We show that under a minimal set of assumptions, the resulting crammed evaluation estimator is consistent and asymptoticall
    
[^139]: AuG-KD: 基于锚点的混合生成用于领域之外知识蒸馏

    AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation

    [https://arxiv.org/abs/2403.07030](https://arxiv.org/abs/2403.07030)

    提出了一种名为AuG-KD的方法，通过利用基于锚点的混合生成，解决了无数据知识蒸馏中的知识转移挑战。

    

    由于隐私或专利问题，越来越多的大型模型发布时不提供其训练数据的访问权限，这使得将它们的知识转移变得低效且问题复杂。针对这一问题，出现了无数据知识蒸馏（DFKD）方法作为直接解决方案。然而，简单地采用从DFKD派生的模型用于实际应用会导致显著的性能下降，这是因为教师训练数据与实际场景（学生领域）之间存在巨大差异。这种性能下降源于教师知识中不适用于学生领域的部分，这些知识是特定于教师领域的，会削弱学生的性能。因此，在DFKD中，有选择地转移适用于学生领域的教师知识成为主要挑战。在本研究中，我们提出了一种简单而有效的方法AuG-KD。它利用一种基于不确定性和样本特定的锚点来对齐学生领域。

    arXiv:2403.07030v1 Announce Type: new  Abstract: Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers' training data and real-world scenarios (student domain). The degradation stems from the portions of teachers' knowledge that are not applicable to the student domain. They are specific to the teacher domain and would undermine students' performance. Hence, selectively transferring teachers' appropriate knowledge becomes the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-doma
    
[^140]: 一种与元启发式算法可比的高效学习型解决器，用于容量弧路由问题

    An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem

    [https://arxiv.org/abs/2403.07028](https://arxiv.org/abs/2403.07028)

    该论文提出了一种基于神经网络的解决器，通过引入方向感知技术和监督强化学习，显著缩小了与先进元启发式算法的差距，同时表现出优越的效率。

    

    最近，神经网络（NN）在组合优化方面取得了长足进展。然而，它们在解决容量弧路由问题（CARP）时面临挑战，CARP是指在图上找到覆盖所有必需边的最小成本路径，同时在容量约束内。在解决CARP方面，基于NN的方法往往落后于先进的元启发式算法，因为它们缺乏针对复杂CARP定制的定向弧建模和高效学习方法。在本文中，我们引入了一种基于NN的解决器，以大大缩小与先进元启发式算法之间的差距，同时表现出更高的效率。首先，我们提出了方向感知注意模型（DaAM），将方向性引入嵌入过程，促进更有效的一阶段决策。其次，我们设计了一个监督强化学习方案，涉及监督预训练，为随后的强化学习建立一个强大的初始策略。

    arXiv:2403.07028v1 Announce Type: cross  Abstract: Recently, neural networks (NN) have made great strides in combinatorial optimization. However, they face challenges when solving the capacitated arc routing problem (CARP) which is to find the minimum-cost tour covering all required edges on a graph, while within capacity constraints. In tackling CARP, NN-based approaches tend to lag behind advanced metaheuristics, since they lack directed arc modeling and efficient learning methods tailored for complex CARP. In this paper, we introduce an NN-based solver to significantly narrow the gap with advanced metaheuristics while exhibiting superior efficiency. First, we propose the direction-aware attention model (DaAM) to incorporate directionality into the embedding process, facilitating more effective one-stage decision-making. Second, we design a supervised reinforcement learning scheme that involves supervised pre-training to establish a robust initial policy for subsequent reinforcement 
    
[^141]: 基于FWin转换器的登革热预测在气候和海洋影响下

    FWin transformer for dengue prediction under climate and ocean influence

    [https://arxiv.org/abs/2403.07027](https://arxiv.org/abs/2403.07027)

    本研究利用深度神经网络和FWin转换器对新加坡2000年至2019年气候数据进行长期登革病例预测，发现FWin表现最佳。

    

    骨病热是最致命的蚊媒传染病之一。详细的长期预测模型对于控制疾病传播和进行缓解工作至关重要。本研究考察了用于长期预测骨病病例的方法。数据集包括2000年至2019年新加坡的本地气候/天气以及全球气候指标。我们利用新开发的深度神经网络来学习特征之间的复杂关系。本研究中的基准模型属于最近用于长序列预测任务的变压器类别。我们发现，基于傅里叶混合窗口注意力（FWin）的变压器在长达60周的登革热预测中，在均方误差和最大绝对误差上表现最佳。

    arXiv:2403.07027v1 Announce Type: new  Abstract: Dengue fever is one of the most deadly mosquito-born tropical infectious diseases. Detailed long range forecast model is vital in controlling the spread of disease and making mitigation efforts. In this study, we examine methods used to forecast dengue cases for long range predictions. The dataset consists of local climate/weather in addition to global climate indicators of Singapore from 2000 to 2019. We utilize newly developed deep neural networks to learn the intricate relationship between the features. The baseline models in this study are in the class of recent transformers for long sequence forecasting tasks. We found that a Fourier mixed window attention (FWin) based transformer performed the best in terms of both the mean square error and the maximum absolute error on the long range dengue forecast up to 60 weeks.
    
[^142]: 基于白度的成像正则参数双层学习

    Whiteness-based bilevel learning of regularization parameters in imaging

    [https://arxiv.org/abs/2403.07026](https://arxiv.org/abs/2403.07026)

    提出了一种基于白度的成像反问题中学习正则化参数的双层优化策略，无需地面实况数据。

    

    我们考虑一种无监督的双层优化策略，用于在存在加性白高斯噪声的成像反问题背景下学习正则化参数。与依赖于参考数据的先验知识和/或噪声统计的监督和半监督度量相比，所提出的方法优化了观测数据和观测模型之间残差的白度，无需地面实况数据。我们在标准总变差正则化图像反卷积问题上验证了该方法，结果表明所提出的质量度量提供了接近均方误差巴拉得原则的估计。

    arXiv:2403.07026v1 Announce Type: cross  Abstract: We consider an unsupervised bilevel optimization strategy for learning regularization parameters in the context of imaging inverse problems in the presence of additive white Gaussian noise. Compared to supervised and semi-supervised metrics relying either on the prior knowledge of reference data and/or on some (partial) knowledge on the noise statistics, the proposed approach optimizes the whiteness of the residual between the observed data and the observation model with no need of ground-truth data.We validate the approach on standard Total Variation-regularized image deconvolution problems which show that the proposed quality metric provides estimates close to the mean-square error oracle and to discrepancy-based principles.
    
[^143]: 利用神经网络通过零噪声外推增强量子变分算法

    Enhancing Quantum Variational Algorithms with Zero Noise Extrapolation via Neural Networks

    [https://arxiv.org/abs/2403.07025](https://arxiv.org/abs/2403.07025)

    本研究利用神经网络进行零噪声外推，以改善量子变分算法在噪声环境中的准确性和可靠性

    

    在新兴的量子计算领域中，变分量子本征求解器（VQE）作为一种有希望解决复杂量子问题的算法在嘈杂的中等规模量子（NISQ）时代尤为突出。然而，量子设备中普遍存在的噪声经常限制了VQE结果的准确性和可靠性。本研究通过利用神经网络在VQE计算中进行零噪声外推（ZNE）来改善这一挑战。通过使用Qiskit框架，我们使用RY-RZ参数化量子电路，并检查它们在不同级别的去极化噪声下的行为。我们的研究涵盖了在不同噪声强度下确定Hamiltonian（定义为Z算子的张量积）的期望值，以提取基态能量。为了将观察到的噪声下的结果与理想的无噪声情况连接起来

    arXiv:2403.07025v1 Announce Type: cross  Abstract: In the emergent realm of quantum computing, the Variational Quantum Eigensolver (VQE) stands out as a promising algorithm for solving complex quantum problems, especially in the noisy intermediate-scale quantum (NISQ) era. However, the ubiquitous presence of noise in quantum devices often limits the accuracy and reliability of VQE outcomes. This research introduces a novel approach to ameliorate this challenge by utilizing neural networks for zero noise extrapolation (ZNE) in VQE computations. By employing the Qiskit framework, we crafted parameterized quantum circuits using the RY-RZ ansatz and examined their behavior under varying levels of depolarizing noise. Our investigations spanned from determining the expectation values of a Hamiltonian, defined as a tensor product of Z operators, under different noise intensities to extracting the ground state energy. To bridge the observed outcomes under noise with the ideal noise-free scenar
    
[^144]: 一种统一的模型用于具有任意可修改区域单位的时空预测查询

    A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary Modifiable Areal Units

    [https://arxiv.org/abs/2403.07022](https://arxiv.org/abs/2403.07022)

    本文提出了一种One4All-ST框架，可以仅使用一个模型为任意可修改区域单位进行ST预测，有效解决了多尺度预测的成本问题和预测不一致性。

    

    时空（ST）预测对于在城市基于位置的应用中做出明智决策（如顺风车）至关重要。然而，现有的ST模型通常需要区域划分作为先决条件，导致两个主要缺点。首先，基于位置的服务需要为不同目的而定义临时区域，需要支持成本高昂的多个具有不同规模和区域的ST模型。其次，不同的ST模型可能产生冲突的输出，导致混乱的预测。 本文提出了One4All-ST框架，可以仅使用一个模型来为任意可修改的区域单元进行ST预测。为了减少获取多尺度预测的成本，我们设计了具有分层空间建模和规模归一化模块的ST网络，以有效且平等地学习多尺度表示。为了解决跨尺度的预测不一致性，我们提出了一种动态规划sch

    arXiv:2403.07022v1 Announce Type: cross  Abstract: Spatio-Temporal (ST) prediction is crucial for making informed decisions in urban location-based applications like ride-sharing. However, existing ST models often require region partition as a prerequisite, resulting in two main pitfalls. Firstly, location-based services necessitate ad-hoc regions for various purposes, requiring multiple ST models with varying scales and zones, which can be costly to support. Secondly, different ST models may produce conflicting outputs, resulting in confusing predictions. In this paper, we propose One4All-ST, a framework that can conduct ST prediction for arbitrary modifiable areal units using only one model. To reduce the cost of getting multi-scale predictions, we design an ST network with hierarchical spatial modeling and scale normalization modules to efficiently and equally learn multi-scale representations. To address prediction inconsistencies across scales, we propose a dynamic programming sch
    
[^145]: 针对持续学习场景的自适应超参数优化

    Adaptive Hyperparameter Optimization for Continual Learning Scenarios

    [https://arxiv.org/abs/2403.07015](https://arxiv.org/abs/2403.07015)

    本文旨在探讨在持续学习中的超参数选择作用和根据任务复杂性持续自动调整它们的必要性，通过利用序列任务学习特性来提高超参数优化效率，实验证明这种方法可以使超参数优化在不同任务中持续加速。

    

    在持续学习场景中，超参数选择是一个具有挑战性和尚未充分探索的方面，特别是在实际非平稳环境中。本文旨在探讨超参数选择在持续学习中的作用，以及根据手头任务的复杂性持续自动调整它们的必要性。因此，我们提出利用序列任务学习的特性来提高超参数优化效率。通过使用基于方差的功能分析技术，我们识别出对性能产生影响的最关键的超参数。我们通过经验性地证明，这种方法无视持续场景和策略，使我们能够持续加快超参数在不同任务间的优化，并展示

    arXiv:2403.07015v1 Announce Type: new  Abstract: Hyperparameter selection in continual learning scenarios is a challenging and underexplored aspect, especially in practical non-stationary environments. Traditional approaches, such as grid searches with held-out validation data from all tasks, are unrealistic for building accurate lifelong learning systems. This paper aims to explore the role of hyperparameter selection in continual learning and the necessity of continually and automatically tuning them according to the complexity of the task at hand. Hence, we propose leveraging the nature of sequence task learning to improve Hyperparameter Optimization efficiency. By using the functional analysis of variance-based techniques, we identify the most crucial hyperparameters that have an impact on performance. We demonstrate empirically that this approach, agnostic to continual scenarios and strategies, allows us to speed up hyperparameters optimization continually across tasks and exhibit
    
[^146]: AdaNovo：具有条件互信息的自适应\emph{De Novo}肽片段测序

    AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional Mutual Information

    [https://arxiv.org/abs/2403.07013](https://arxiv.org/abs/2403.07013)

    AdaNovo 提出了一个新的框架，通过计算光谱和每个氨基酸/肽段之间的条件互信息，实现了自适应模型训练。

    

    质谱联用已在促进蛋白质组学方面发挥关键作用，使得可以分析生物样本中的蛋白质组成。尽管已开发了各种深度学习方法用于识别导致观察光谱的氨基酸序列（肽段），但\emph{de novo}肽段测序仍然存在挑战。为了解决这些挑战，我们提出了AdaNovo，这是一个新颖的框架，它计算了光谱和每个氨基酸/肽段之间的条件互信息（CMI），并利用CMI进行自适应模型训练。

    arXiv:2403.07013v1 Announce Type: cross  Abstract: Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the analysis of protein composition in biological samples. Despite the development of various deep learning methods for identifying amino acid sequences (peptides) responsible for observed spectra, challenges persist in \emph{de novo} peptide sequencing. Firstly, prior methods struggle to identify amino acids with post-translational modifications (PTMs) due to their lower frequency in training data compared to canonical amino acids, further resulting in decreased peptide-level identification precision. Secondly, diverse types of noise and missing peaks in mass spectra reduce the reliability of training data (peptide-spectrum matches, PSMs). To address these challenges, we propose AdaNovo, a novel framework that calculates conditional mutual information (CMI) between the spectrum and each amino acid/peptide, using CMI for adaptive model training. Extens
    
[^147]: 基于张量分解的缺失数据插补非侵入式负载监测

    Non-Intrusive Load Monitoring with Missing Data Imputation Based on Tensor Decomposition

    [https://arxiv.org/abs/2403.07012](https://arxiv.org/abs/2403.07012)

    这项研究提出了基于张量分解的非侵入式负载监测的缺失数据插补方法，通过引入PID控制器和非负更新规则，解决了NILM数据丢失的问题。

    

    随着非侵入式负载监测（NILM）在建筑能源管理中的广泛应用，确保NILM数据的高质量变得至关重要。然而，NILM的实际应用面临数据丢失的挑战，严重影响能源管理的准确性和可靠性。本文通过引入创新的张量完成（TC）模型-基于积分比-导数（PID）的张量的非负潜因子分解（PNLFT）来解决NILM数据丢失问题，其中包含两个思想：1）为解决随机梯度下降（SGD）中潜在张量分解（LFT）的收敛缓慢问题，学习过程中引入比例-积分-导数控制器。PID控制器利用历史信息和当前信息控制学习残差。2）考虑到NILM数据的特性，提出了非负更新规则。

    arXiv:2403.07012v1 Announce Type: new  Abstract: With the widespread adoption of Non-Intrusive Load Monitoring (NILM) in building energy management, ensuring the high quality of NILM data has become imperative. However, practical applications of NILM face challenges associated with data loss, significantly impacting accuracy and reliability in energy management. This paper addresses the issue of NILM data loss by introducing an innovative tensor completion(TC) model- Proportional-Integral-Derivative (PID)-incorporated Non-negative Latent Factorization of Tensors (PNLFT) with twofold ideas: 1) To tackle the issue of slow convergence in Latent Factorization of Tensors (LFT) using Stochastic Gradient Descent (SGD), a Proportional-Integral-Derivative controller is introduced during the learning process. The PID controller utilizes historical and current information to control learning residuals. 2) Considering the characteristics of NILM data, non-negative update rules are proposed in the 
    
[^148]: 自动评价正确: 使用合成数据进行模型评估

    AutoEval Done Right: Using Synthetic Data for Model Evaluation

    [https://arxiv.org/abs/2403.07008](https://arxiv.org/abs/2403.07008)

    提出了用合成数据进行模型评估的方法，通过高效和统计上合理的算法，在GPT-4实验中有效的人工标记样本大小增加了50%。

    

    机器学习模型的评估使用人工标记的验证数据可能既昂贵又耗时。可以使用AI标记的合成数据来减少此类目的人工注释数量，这一过程称为自动评估。我们提出了用于此目的的高效和统计上合理的算法，可以提高样本效率，同时保持不偏。这些算法在与GPT-4进行的实验中将有效的人工标记样本大小增加了高达50%。

    arXiv:2403.07008v1 Announce Type: cross  Abstract: The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
    
[^149]: 具有奖励机器层次结构的多智能体强化学习

    Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines

    [https://arxiv.org/abs/2403.07005](https://arxiv.org/abs/2403.07005)

    提出了具有层次结构奖励机器的多智能体强化学习（MAHRM），可以处理更复杂的情况，将任务分解为简单子任务的层次结构，以减少计算复杂性。

    

    在本文中，我们研究了利用奖励机器（RMs）来指定奖励函数，并通过这种方式利用任务中高级事件的先验知识来促进学习效率的合作式多智能体强化学习（MARL）问题。我们提出了具有层次结构奖励机器的多智能体强化学习（MAHRM），能够处理更复杂的场景，其中智能体之间的事件可以同时发生且互相依赖。

    arXiv:2403.07005v1 Announce Type: new  Abstract: In this paper, we study the cooperative Multi-Agent Reinforcement Learning (MARL) problems using Reward Machines (RMs) to specify the reward functions such that the prior knowledge of high-level events in a task can be leveraged to facilitate the learning efficiency. Unlike the existing work that RMs have been incorporated into MARL for task decomposition and policy learning in relatively simple domains or with an assumption of independencies among the agents, we present Multi-Agent Reinforcement Learning with a Hierarchy of RMs (MAHRM) that is capable of dealing with more complex scenarios when the events among agents can occur concurrently and the agents are highly interdependent.   MAHRM exploits the relationship of high-level events to decompose a task into a hierarchy of simpler subtasks that are assigned to a small group of agents, so as to reduce the overall computational complexity.   Experimental results in three cooperative MAR
    
[^150]: 一些凸消息传递算法收敛到固定点

    Convergence of Some Convex Message Passing Algorithms to a Fixed Point

    [https://arxiv.org/abs/2403.07004](https://arxiv.org/abs/2403.07004)

    这项研究证明了一些凸消息传递算法会收敛到固定点，并在一定迭代次数内达到特定精度。

    

    在图模型中解决MAP推断问题的一种流行方法是通过（块状）坐标下降最小化从对偶线性规划或Lagrange松弛中获得的一个上界。这样的算法包括最大和扩散以及顺序树重新加权消息传递。这些方法的收敛性质目前尚未完全理解。它们已被证明会收敛到由活跃约束的局部一致性所表征的集合，但收敛速度未知；然而，尚不清楚迭代是否会收敛（到任何一个单一点）。我们证明了一个更强的结果（之前有猜想但从未证明过）：迭代会收敛到算法的一个固定点。此外，我们还展示它们在$\mathcal{O}(1/\varepsilon)$次迭代中达到了精度$\varepsilon>0$。

    arXiv:2403.07004v1 Announce Type: new  Abstract: A popular approach to the MAP inference problem in graphical models is to minimize an upper bound obtained from a dual linear programming or Lagrangian relaxation by (block-)coordinate descent. Examples of such algorithms are max-sum diffusion and sequential tree-reweighted message passing. Convergence properties of these methods are currently not fully understood. They have been proved to converge to the set characterized by local consistency of active constraints, with unknown convergence rate; however, it was not clear if the iterates converge at all (to any single point). We prove a stronger result (which was conjectured before but never proved): the iterates converge to a fixed point of the algorithm. Moreover, we show that they achieve precision $\varepsilon>0$ in $\mathcal{O}(1/\varepsilon)$ iterations.   We first prove this for a version of coordinate descent applied to a general piecewise-affine convex objective, using a novel p
    
[^151]: 面向智能城市全覆盖智能应急互动响应系统的疏散管理框架

    Evacuation Management Framework towards Smart City-wide Intelligent Emergency Interactive Response System

    [https://arxiv.org/abs/2403.07003](https://arxiv.org/abs/2403.07003)

    提出了一种面向智能城市全覆盖智能应急互动响应系统的疏散管理框架，结合人工智能和机器学习技术，旨在改进现有的应急响应系统，以提高居民的公共服务和生活质量。

    

    一种面向未来6G网络部署的智能城市解决方案，允许中小企业、行业和政府机构与基础设施连接，并在提高应急准备能力方面发挥关键作用。本文旨在提出一套协调的技术解决方案，将现有的应急响应系统转变为智能互动系统，从而改善居民在家中、在道路上、在医院、交通枢纽等地的公共服务和生活质量。在这一背景下，我们从与人们日常生活密切相关的三个不同应用场景着手考虑城市全景视角，以优化相关部门采取的行动。因此，采用人工智能（AI）和机器学习（ML）技术来实现下一代互联车辆体验，我们专注于发生在工厂的事故。

    arXiv:2403.07003v1 Announce Type: new  Abstract: A smart city solution toward future 6G network deployment allows small and medium sized enterprises (SMEs), industry, and government entities to connect with the infrastructures and play a crucial role in enhancing emergency preparedness with advanced sensors. The objective of this work is to propose a set of coordinated technological solutions to transform an existing emergency response system into an intelligent interactive system, thereby improving the public services and the quality of life for residents at home, on road, in hospitals, transport hubs, etc. In this context, we consider a city wide view from three different application scenes that are closely related to peoples daily life, to optimize the actions taken at relevant departments. Therefore, using artificial intelligence (AI) and machine learning (ML) techniques to enable the next generation connected vehicle experiences, we specifically focus on accidents happening in ind
    
[^152]: 使用深度学习、机器学习和统计方法的生存建模：预测入院后死亡率的比较分析

    Survival modeling using deep learning, machine learning and statistical methods: A comparative analysis for predicting mortality after hospital admission

    [https://arxiv.org/abs/2403.06999](https://arxiv.org/abs/2403.06999)

    该研究进行了使用多种生存分析方法的比较研究，包括传统统计模型和先进的机器学习算法，旨在预测入院后的死亡率。

    

    生存分析对于研究事件发生的时间至关重要，并且能够动态地理解事件随时间发生的概率。各种生存分析技术，从传统的统计模型到最先进的机器学习算法，支持医疗干预和政策决策。然而，它们的比较性能仍在持续讨论中。我们进行了几种生存分析方法的比较研究，包括Cox比例风险模型（CoxPH）、逐步CoxPH、弹性网惩罚Cox模型、随机生存森林（RSF）、梯度提升机器（GBM）学习、AutoScore-Survival、DeepSurv、基于神经网络的时间相关Cox模型（CoxTime）以及DeepHit生存神经网络。我们应用了一致性指数（C指数）进行模型拟合度评估，用积分Brier分数（IBS）进行校准，并考虑了模型的可解释性。

    arXiv:2403.06999v1 Announce Type: cross  Abstract: Survival analysis is essential for studying time-to-event outcomes and providing a dynamic understanding of the probability of an event occurring over time. Various survival analysis techniques, from traditional statistical models to state-of-the-art machine learning algorithms, support healthcare intervention and policy decisions. However, there remains ongoing discussion about their comparative performance. We conducted a comparative study of several survival analysis methods, including Cox proportional hazards (CoxPH), stepwise CoxPH, elastic net penalized Cox model, Random Survival Forests (RSF), Gradient Boosting machine (GBM) learning, AutoScore-Survival, DeepSurv, time-dependent Cox model based on neural network (CoxTime), and DeepHit survival neural network. We applied the concordance index (C-index) for model goodness-of-fit, and integral Brier scores (IBS) for calibration, and considered the model interpretability. As a case 
    
[^153]: 基于物理传感器的深度学习跌倒检测系统

    Physics Sensor Based Deep Learning Fall Detection System

    [https://arxiv.org/abs/2403.06994](https://arxiv.org/abs/2403.06994)

    本文提出了一个基于物理传感器的深度学习跌倒检测系统TSFallDetect，利用顺序深度学习方法解决跌倒动作预测问题。

    

    基于嵌入式传感器的跌倒检测是近年来的一个实用且流行的研究方向。本文提出了一个名为TSFallDetect的完整系统，包括基于嵌入式传感器的数据接收设备、移动深度学习模型部署平台以及一个简单的服务器，用于收集模型和数据以进行未来扩展。此外，我们利用顺序深度学习方法来解决基于惯性和薄膜压力传感器收集的数据的跌倒动作预测问题。

    arXiv:2403.06994v1 Announce Type: cross  Abstract: Fall detection based on embedded sensor is a practical and popular research direction in recent years. In terms of a specific application: fall detection methods based upon physics sensors such as [gyroscope and accelerator] have been exploited using traditional hand crafted features and feed them in machine learning models like Markov chain or just threshold based classification methods. In this paper, we build a complete system named TSFallDetect including data receiving device based on embedded sensor, mobile deep-learning model deploying platform, and a simple server, which will be used to gather models and data for future expansion. On the other hand, we exploit the sequential deep-learning methods to address this falling motion prediction problem based on data collected by inertial and film pressure sensors. We make a empirical study based on existing datasets and our datasets collected from our system separately, which shows tha
    
[^154]: 基于LSTM的自动驾驶车道变更安全预测模型

    Automatic driving lane change safety prediction model based on LSTM

    [https://arxiv.org/abs/2403.06993](https://arxiv.org/abs/2403.06993)

    通过使用基于深度学习方法的安全敏感深度学习模型，提出了一种基于LSTM的自动驾驶车道变更安全预测模型，旨在提高自动驾驶车辆驾驶安全性。

    

    自动驾驶技术可以提高交通安全，减少交通事故。此外，它提高了交通流量，减少拥堵，节约能源并提高出行效率。在相对成熟的自动驾驶技术中，自动驾驶功能分为几个模块：感知、决策、规划和控制，合理的分工可以提高系统的稳定性。因此，自动驾驶车辆需要具备预测周围车辆轨迹的能力，以做出合理的决策规划和安全措施，提高驾驶安全性。本文提出了一种基于深度学习方法的、以短期记忆（LSTM）网络为基础的安全敏感深度学习模型。该模型可以缓解当前自动驾驶轨迹规划的缺点，输出轨迹不仅保证了高准确性，还提升了安全性。

    arXiv:2403.06993v1 Announce Type: cross  Abstract: Autonomous driving technology can improve traffic safety and reduce traffic accidents. In addition, it improves traffic flow, reduces congestion, saves energy and increases travel efficiency. In the relatively mature automatic driving technology, the automatic driving function is divided into several modules: perception, decision-making, planning and control, and a reasonable division of labor can improve the stability of the system. Therefore, autonomous vehicles need to have the ability to predict the trajectory of surrounding vehicles in order to make reasonable decision planning and safety measures to improve driving safety. By using deep learning method, a safety-sensitive deep learning model based on short term memory (LSTM) network is proposed. This model can alleviate the shortcomings of current automatic driving trajectory planning, and the output trajectory not only ensures high accuracy but also improves safety. The cell sta
    
[^155]: 针对极限环振荡器的相位自编码器

    Phase autoencoder for limit-cycle oscillators

    [https://arxiv.org/abs/2403.06992](https://arxiv.org/abs/2403.06992)

    该论文介绍了一种用于极限环振荡器的相位自编码器，可以通过训练估计振荡器的渐近相位和相位敏感度函数，以及在原始空间中重建振荡器状态，同时提出了一个简单方法来实现两个振荡器的全局同步。

    

    我们介绍了一种相位自编码器，用于编码极限环振荡器的渐近相位，这是表征其同步动力学的基本量。该自编码器经过训练，使其潜在变量直接表示振荡器的渐近相位。经过训练的自编码器可以执行两个功能，无需依赖振荡器的数学模型：首先，它可以评估振荡器的渐近相位和相位敏感度函数；其次，它可以从相位值作为输入，在原始空间中重建极限环上的振荡器状态。通过几个极限环振荡器的示例，我们展示了通过训练的自编码器只能从时间序列数据中估计出渐近相位和相位敏感度函数。我们还提出一种简单的方法，作为训练自编码器的应用，用于全局同步两个振荡器。

    arXiv:2403.06992v1 Announce Type: cross  Abstract: We present a phase autoencoder that encodes the asymptotic phase of a limit-cycle oscillator, a fundamental quantity characterizing its synchronization dynamics. This autoencoder is trained in such a way that its latent variables directly represent the asymptotic phase of the oscillator. The trained autoencoder can perform two functions without relying on the mathematical model of the oscillator: first, it can evaluate the asymptotic phase and phase sensitivity function of the oscillator; second, it can reconstruct the oscillator state on the limit cycle in the original space from the phase value as an input. Using several examples of limit-cycle oscillators, we demonstrate that the asymptotic phase and phase sensitivity function can be estimated only from time-series data by the trained autoencoder. We also present a simple method for globally synchronizing two oscillators as an application of the trained autoencoder.
    
[^156]: 引导LLM走向正确之路：快速、非侵入式受限生成

    Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation

    [https://arxiv.org/abs/2403.06988](https://arxiv.org/abs/2403.06988)

    提出了一种新颖的解码算法DOMINO，在生成文本过程中以完全基于子词对齐的方式强制执行约束，几乎没有性能开销并有时甚至实现近2倍速度提升，远远优于现有方法。

    

    为了确保大型语言模型（LLMs）生成的文本符合预期格式，受限解码提出在生成过程中强制执行严格的形式语言约束。然而，正如我们在这项工作中所展示的，这类方法不仅在生成过程中产生性能开销，而且许多方法如果没有正确地将LLM子词词汇与外部约束对齐，则还会显著损害任务准确性。为了解决这个问题，我们提出了一种新颖的解码算法DOMINO，可以以完全基于子词对齐的方式强制执行约束，同时利用预计算和推测解码来实现几乎零开销，有时甚至比不受限制的解码快近2倍，从而远远超过现有方法的表现。

    arXiv:2403.06988v1 Announce Type: cross  Abstract: To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding proposes to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2$\times$ speedup over unconstrained decoding -- thereby outperforming existing approaches by a wide margin.
    
[^157]: 通过模型重编程增强假肢使用者的关节运动预测

    Enhancing Joint Motion Prediction for Individuals with Limb Loss Through Model Reprogramming

    [https://arxiv.org/abs/2403.06569](https://arxiv.org/abs/2403.06569)

    通过深度学习的重编程特性，研究者成功将原本设计用于健全人的模型改编为适用于截肢者预测关节运动。

    

    肢体丧失导致的行动障碍是全球数百万人面临的重大挑战。先进的辅助技术（如假肢设备）的开发有可能大大提高截肢患者的生活质量。设计这类技术的关键组成部分是准确预测缺失肢体的参考关节运动。然而，与大量来自健全受试者的数据形成对比，截肢患者关节运动数据的稀缺性阻碍了这一任务的完成。为了克服这一障碍，我们利用深度学习的重编程特性，无需改变模型参数即可重新利用训练有素的模型实现新目标。通过仅在数据级别进行操作，我们将原本设计用于健全人的模型改编为适用于截肢者预测关节运动。本研究结果对于推进辅助技术具有重要意义。

    arXiv:2403.06569v1 Announce Type: new  Abstract: Mobility impairment caused by limb loss is a significant challenge faced by millions of individuals worldwide. The development of advanced assistive technologies, such as prosthetic devices, has the potential to greatly improve the quality of life for amputee patients. A critical component in the design of such technologies is the accurate prediction of reference joint motion for the missing limb. However, this task is hindered by the scarcity of joint motion data available for amputee patients, in contrast to the substantial quantity of data from able-bodied subjects. To overcome this, we leverage deep learning's reprogramming property to repurpose well-trained models for a new goal without altering the model parameters. With only data-level manipulation, we adapt models originally designed for able-bodied people to forecast joint motion in amputees. The findings in this study have significant implications for advancing assistive tech a
    
[^158]: DeepSafeMPC: 基于深度学习的安全多智体强化学习模型预测控制

    DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning

    [https://arxiv.org/abs/2403.06397](https://arxiv.org/abs/2403.06397)

    DeepSafeMPC是一种基于深度学习的模型预测控制方法，旨在有效预测多智体环境的复杂动态，并应用MARL原则寻找最优解。

    

    安全多智体强化学习（safe MARL）在最近几年逐渐受到关注，强调了智体不仅需要优化全局回报，还需要通过行为约束遵守安全要求的必要性。近期一些工作将控制理论与多智体强化学习相结合，以解决确保安全性的挑战。然而，在这一领域中应用模型预测控制（MPC）方法的应用非常有限，主要是由于多智体环境中复杂且隐式动态的特性。为弥合这一差距，我们提出了一种称为基于深度学习的安全多智体强化学习模型预测控制（DeepSafeMPC）的新方法。DeepSafeMPC 的关键见解是利用集中式深度学习模型很好地预测环境动态。我们的方法应用MARL原则来寻找最优解。

    arXiv:2403.06397v1 Announce Type: cross  Abstract: Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained attention in recent years, emphasizing the need for agents to not only optimize the global return but also adhere to safety requirements through behavioral constraints. Some recent work has integrated control theory with multi-agent reinforcement learning to address the challenge of ensuring safety. However, there have been only very limited applications of Model Predictive Control (MPC) methods in this domain, primarily due to the complex and implicit dynamics characteristic of multi-agent environments. To bridge this gap, we propose a novel method called Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of DeepSafeMPC is leveraging a entralized deep learning model to well predict environmental dynamics. Our method applies MARL principles to search for optimal solutions. Through the employme
    
[^159]: 通过随机控制进行扩散模型的微调：熵正则化及更多

    Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond

    [https://arxiv.org/abs/2403.06279](https://arxiv.org/abs/2403.06279)

    本文致力于研究连续时间扩散模型中的熵正则化微调问题，并展示了分析如何扩展到涉及一般$f$-散度正则化的微调中。

    

    本文旨在发展并对连续时间扩散模型中熵正则化微调问题进行严格处理，该问题最近由上原等人提出。我们还展示了如何将分析扩展到涉及一般$f$-散度正则化的微调中。

    arXiv:2403.06279v1 Announce Type: cross  Abstract: This paper aims to develop and provide a rigorous treatment to the problem of entropy regularized fine-tuning in the context of continuous-time diffusion models, which was recently proposed by Uehara et al. ( arXiv:2402.15194, 2024). We also show how the analysis can be extended to fine-tuning involving a general $f$-divergence regularizer.
    
[^160]: 具有扩散净化的分离数据一致性的图像恢复

    Decoupled Data Consistency with Diffusion Purification for Image Restoration

    [https://arxiv.org/abs/2403.06054](https://arxiv.org/abs/2403.06054)

    通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。

    

    最近，扩散模型作为一种强大的深度生成先验类别已经引起了人们的关注，由于其出色地建模数据分布的能力，在各种图像恢复任务中表现出色。为了解决图像恢复问题，许多现有技术通过将额外的似然梯度步骤纳入到扩散模型的反向采样过程中来实现数据一致性。然而，这些额外的梯度步骤对于实际应用中存在挑战，因为它们造成了巨大的计算开销，从而增加了推理时间。当使用加速的扩散模型采样器时，这些额外的步骤还会导致额外的困难，因为数据一致性步骤的数量受限于反向采样步骤的数量。在这项工作中，我们提出了一种新颖的基于扩散的图像恢复求解器，通过将反向过程与数据一致性步骤分离来解决这些问题。我们的方法涉及

    arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
    
[^161]: 通用外科视觉变换器：用于通用外科的视频预训练基础模型

    General surgery vision transformer: A video pre-trained foundation model for general surgery

    [https://arxiv.org/abs/2403.05949](https://arxiv.org/abs/2403.05949)

    该论文开源了迄今为止最大的通用外科视频数据集，提出了用于外科应用的视频预训练通用外科视觉变换器（GSViT）技术，并展示了其在Cholec80阶段注释任务上的优越性能。

    

    缺乏开放获取的数据和专门的基础模型是外科计算研究的主要障碍。为此，我们开源迄今为止最大的通用外科视频数据集，包括来自28种手术技术的680小时手术视频数据；我们提出了一种基于前向视频预测的通用外科视觉变换器（GSViT）视频预训练技术，可实时运行用于外科应用，我们还开源了GSViT的代码和权重；我们还发布了针对10种手术程序的特定程序微调版本的GSViT的代码和权重；我们展示了GSViT在Cholec80阶段注释任务上的性能，显示出优于最先进的单帧预测器的性能。

    arXiv:2403.05949v1 Announce Type: cross  Abstract: The absence of openly accessible data and specialized foundation models is a major barrier for computational research in surgery. Toward this, (i) we open-source the largest dataset of general surgery videos to-date, consisting of 680 hours of surgical videos, including data from robotic and laparoscopic techniques across 28 procedures; (ii) we propose a technique for video pre-training a general surgery vision transformer (GSViT) on surgical videos based on forward video prediction that can run in real-time for surgical applications, toward which we open-source the code and weights of GSViT; (iii) we also release code and weights for procedure-specific fine-tuned versions of GSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the Cholec80 phase annotation task, displaying improved performance over state-of-the-art single frame predictors.
    
[^162]: 基于残差网络的扩散建模在不平衡数据上的应用

    SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data

    [https://arxiv.org/abs/2403.05918](https://arxiv.org/abs/2403.05918)

    基于残差网络的扩散建模方法能够有效处理不平衡数据，克服了经典过采样方法和基于生成网络的模式塌陷与训练不稳定问题。

    

    在数据挖掘和机器学习领域，通常使用的分类模型在不平衡数据中无法有效学习。为了平衡模型训练前的数据分布，通常使用过采样方法为少数类生成数据，以解决分类不平衡数据的问题。大多数经典的过采样方法基于SMOTE技术，该技术仅关注数据的局部信息，因此生成的数据可能存在不够逼真的问题。在基于生成网络的当前过采样方法中，基于GAN的方法可以捕获数据的真实分布，但训练中存在模式崩溃和不稳定性的问题；基于去噪扩散概率模型的过采样方法中，使用U-Net的逆扩散过程神经网络不适用于表格数据。

    arXiv:2403.05918v1 Announce Type: cross  Abstract: In the field of data mining and machine learning, commonly used classification models cannot effectively learn in unbalanced data. In order to balance the data distribution before model training,oversamplingmethods are often used to generate data for a small number of classes to solve the problem of classifying unbalanced data. Most of the classical oversampling methods are based on theSMOTE technique, which only focuses on the local information of the data, and therefore the generated data may have the problem of not being realistic enough. In the current oversampling methods based on generative networks, the methods based on GANs can capture the true distribution of data, but there is the problem of pattern collapse and training instability in training; in the oversampling methods based on denoising diffusion probability models, the neural network of the inverse diffusion process using the U-Net is not applicable to tabular data, and
    
[^163]: PR-NET：利用精细化通路网络结构进行前列腺癌患者状况预测

    PR-NET: Leveraging Pathway Refined Network Structures for Prostate Cancer Patient Condition Prediction

    [https://arxiv.org/abs/2403.05818](https://arxiv.org/abs/2403.05818)

    PR-NET模型通过压缩和优化P-NET的网络结构，降低了模型复杂性，同时保持高准确性和可解释性，在前列腺癌患者状况预测中表现出优越性能。

    

    动机：诊断和监测去势抵抗性前列腺癌（CRPC）对癌症患者至关重要，但目前的模型（如P-NET）在参数数量、泛化能力和成本方面存在局限性。结果：为解决上述问题，我们开发了一种更准确高效的前列腺癌患者状况预测模型，名为PR-NET。通过压缩和优化P-NET的网络结构，降低了模型复杂性，同时保持了高准确性和可解释性。PR-NET在预测前列腺癌患者结果方面表现出色，超过了P-NET和其他六种传统模型显著。在我们的严格评估中，PR-NET不仅在已知数据上取得了令人印象深刻的平均AUC和召回率分数（分别为0.94和0.83），而且在五个未知数据集上保持了强大的泛化能力，平均AUC为0.73，召回率为0

    arXiv:2403.05818v1 Announce Type: new  Abstract: Motivation: The diagnosis and monitoring of Castrate Resistant Prostate Cancer (CRPC) are crucial for cancer patients, but the current models (such as P-NET) have limitations in terms of parameter count, generalization, and cost. Results: To address the above issues, we develop a more accurate and efficient Prostate Cancer patient condition prediction model, named PR-NET. By compressing and optimizing the network structure of P-NET, the model complexity is reduced while maintaining high accuracy and interpretability. The PR-NET demonstrated superior performance in predicting prostate cancer patient outcomes, outshining P-NET and six other traditional models with a significant margin. In our rigorous evaluation, PR-NET not only achieved impressive average AUC and Recall scores of 0.94 and 0.83, respectively, on known data but also maintained robust generalizability on five unknown datasets with a higher average AUC of 0.73 and Recall of 0
    
[^164]: GEAR: 一种用于几乎无损生成推断大型语言模型的高效KV缓存压缩方案

    GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM

    [https://arxiv.org/abs/2403.05527](https://arxiv.org/abs/2403.05527)

    GEAR提出了一种高效的KV缓存压缩框架，实现几乎无损的高比率压缩，用于解决大型语言模型推断中因缓存需求增长而导致的记忆绑定问题和性能下降。

    

    关键-值（KV）缓存已成为加快大型语言模型（LLMs）推断生成速度的事实标准。然而，随着序列长度增加而增长的缓存需求已将LLM推断转变为一个记忆绑定问题，显著地限制了系统吞吐量。现有方法依赖于丢弃不重要的标记或均匀量化所有条目。然而，这种方法往往会产生较高的近似误差来表示压缩后的矩阵。自回归解码过程进一步增加了每个步骤的误差，导致模型生成中的重大偏差和性能恶化。为了解决这一挑战，我们提出了GEAR，一种高效的KV缓存压缩框架，实现几乎无损的高压缩比。

    arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
    
[^165]: 在批强化学习中，通过切换损失函数来降低成本

    Switching the Loss Reduces the Cost in Batch Reinforcement Learning

    [https://arxiv.org/abs/2403.05385](https://arxiv.org/abs/2403.05385)

    使用对数损失函数来训练适合的Q迭代的批强化学习方法，在实现目标时不产生成本的问题中，其样本数量需求与最优策略的累积成本成比例，能够提供与最优可达成本成比例的“小成本”界限，并在实验中验证在那些最优策略可靠实现目标的问题中，FQI-LOG比使用平方损失训练的FQI使用更少的样本。

    

    我们提出了一种使用对数损失（FQI-LOG）来训练适合的Q迭代的批强化学习（RL）方法。我们展示了使用FQI-LOG学习接近最优策略所需的样本数量与最优策略的累积成本成比例，对于那些通过最优行为实现目标且不产生成本的问题，最优策略的累积成本为零。通过这种方式，我们提供了一种在批RL中证明具有与最优可达成本成比例的“小成本”界限的一般框架。此外，我们从经验上验证，FQI-LOG在那些最优策略可靠地实现目标的问题上使用的样本比使用平方损失训练的FQI要少。

    arXiv:2403.05385v1 Announce Type: new  Abstract: We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.
    
[^166]: 使用量子完全图神经网络进行喷注判别

    Jet Discrimination with Quantum Complete Graph Neural Network

    [https://arxiv.org/abs/2403.04990](https://arxiv.org/abs/2403.04990)

    QCGNN通过量子并行性实现了对喷注判别的多项式加速，为喷注判别问题带来新的解决方案

    

    机器学习，特别是深度神经网络，在高能物理中被广泛应用，并在各种应用中展现出显著的效果。此外，机器学习的概念已扩展到量子计算机，形成了一个被称为量子机器学习的新研究领域。在本文中，我们提出了一个新颖的变分量子电路模型，即量子完全图神经网络（QCGNN），旨在学习完全图。我们认为QCGNN由于量子并行性的特性，在速度上对其经典对应物具有多项式加速。本文研究了QCGNN在具有挑战性的喷注判别中的应用，其中喷注用完全图表示。随后，我们与经典图神经网络进行了比较分析，以建立基准。

    arXiv:2403.04990v1 Announce Type: cross  Abstract: Machine learning, particularly deep neural networks, has been widely utilized in high energy physics and has shown remarkable results in various applications. Moreover, the concept of machine learning has been extended to quantum computers, giving rise to a new research area known as quantum machine learning. In this paper, we propose a novel variational quantum circuit model, Quantum Complete Graph Neural Network (QCGNN), designed for learning complete graphs. We argue that QCGNN has a polynomial speedup against its classical counterpart, due to the property of quantum parallelism. In this paper, we study the application of QCGNN through the challenging jet discrimination, where the jets are represented with complete graphs. Subsequently, we conduct a comparative analysis with classical graph neural networks to establish a benchmark.
    
[^167]: 一种用于隐私漏斗的高效凸差分求解器

    An Efficient Difference-of-Convex Solver for Privacy Funnel

    [https://arxiv.org/abs/2403.04778](https://arxiv.org/abs/2403.04778)

    提出了一种针对隐私漏斗方法的高效求解器，能在已知和未知分布条件下均有效地进行求解，并在实验中展示出优于现有方法的性能。

    

    我们提出了一种针对隐私漏斗（PF）方法的高效求解器，利用其凸差分（DC）结构。所提出的DC分离导致了闭式更新方程，可以直接应用于已知和未知分布设置。对于已知分布情况，我们证明了所提出的非贪婪求解器的收敛性（局部稳定点），并在经验上展示它在表征隐私-效用权衡方面优于现有技术方法。我们的DC方法洞察力适用于具有标记经验样本的未知分布设置。利用这些洞察力，我们的交替最小化求解器满足了PF的基本Markov关系，与以往基于变分推理的求解器相比。在经验上，我们使用MNIST和Fashion-MNIST数据集评估了所提出的求解器。

    arXiv:2403.04778v1 Announce Type: new  Abstract: We propose an efficient solver for the privacy funnel (PF) method, leveraging its difference-of-convex (DC) structure. The proposed DC separation results in a closed-form update equation, which allows straightforward application to both known and unknown distribution settings. For known distribution case, we prove the convergence (local stationary points) of the proposed non-greedy solver, and empirically show that it outperforms the state-of-the-art approaches in characterizing the privacy-utility trade-off. The insights of our DC approach apply to unknown distribution settings where labeled empirical samples are available instead. Leveraging the insights, our alternating minimization solver satisfies the fundamental Markov relation of PF in contrast to previous variational inference-based solvers. Empirically, we evaluate the proposed solver with MNIST and Fashion-MNIST datasets. Our results show that under a comparable reconstruction 
    
[^168]: 减少自监督学习复杂性改善计算病理学中的弱监督分类性能

    Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology

    [https://arxiv.org/abs/2403.04558](https://arxiv.org/abs/2403.04558)

    本研究探讨了在计算病理学中减少对比自监督学习复杂性对分类性能的改善，通过利用消费级硬件。

    

    深度学习模型已成功应用于从常规可用的组织学数据中提取临床可操作见解。通常，这些模型需要临床医生进行的标注，这种标注稀缺且昂贵。自监督学习（SSL）方法的出现消除了这一障碍，允许对非标注数据进行大规模分析。然而，最近的SSL方法采用日益庞大的模型架构和更大的数据集，导致数据量迅速增加，硬件要求和整体成本增加，使得很少机构能够获得这些资源。因此，我们研究了对比自监督学习在计算病理学中的复杂性与分类性能之间的关系，利用消费级硬件。具体而言，我们分析了数据量、架构和算法的调整对下游分类任务的影响。

    arXiv:2403.04558v1 Announce Type: cross  Abstract: Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream clas- sification tasks, empha
    
[^169]: 分散且公平的最优输运

    Decentralized and Equitable Optimal Transport

    [https://arxiv.org/abs/2403.04259](https://arxiv.org/abs/2403.04259)

    本文提出了分散的最优输运问题和分散公平最优输运问题，引入了具有迭代复杂度为O(1/{\epsilon})的单循环分散算法，以匹配现有的集中式一阶方法，并改进了现有集中式算法的迭代复杂度。

    

    本文考虑了分散（离散）最优输运（D-OT）问题。在这种情况下，一组代理人共同设计运输方案，其中成本函数是每个代理人持有的成本之和。我们将D-OT问题重新表述为约束耦合优化问题，并提出了一种具有迭代复杂度为O(1/{\epsilon})的单循环分散算法，与现有的集中式一阶方法相匹配。此外，我们提出了分散公平最优输运（DE-OT）问题。在DE-OT中，代理不仅协作设计最小化运输成本的运输计划，还努力确保各自成本的公平性。解决DE-OT的方法的迭代复杂度也是O(1/{\epsilon})，这一速率提高了现有的集中式算法，其中获得的最佳迭代复杂度为O(1/{\epsilon}^2)。

    arXiv:2403.04259v1 Announce Type: cross  Abstract: This paper considers the decentralized (discrete) optimal transport (D-OT) problem. In this setting, a network of agents seeks to design a transportation plan jointly, where the cost function is the sum of privately held costs for each agent. We reformulate the D-OT problem as a constraint-coupled optimization problem and propose a single-loop decentralized algorithm with an iteration complexity of O(1/{\epsilon}) that matches existing centralized first-order approaches. Moreover, we propose the decentralized equitable optimal transport (DE-OT) problem. In DE-OT, in addition to cooperatively designing a transportation plan that minimizes transportation costs, agents seek to ensure equity in their individual costs. The iteration complexity of the proposed method to solve DE-OT is also O(1/{\epsilon}). This rate improves existing centralized algorithms, where the best iteration complexity obtained is O(1/{\epsilon}^2).
    
[^170]: 拥抱不确定性灵活性：利用监督树核强化集成模型，预测右心室容积的二维超声心动图

    Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right Ventricular Volume

    [https://arxiv.org/abs/2403.03229](https://arxiv.org/abs/2403.03229)

    提出了一种利用监督树核强化集成模型，预测右心室容积的二维超声心动图的方法，并通过不确定性得分增强了预测表现，该方法在小规模数据集上表现出较好的概率和点预测性能。

    

    右心室（RV）功能恶化在许多情况下都能强力预测临床结果。为了增强使用广泛可用的二维超声心动图（2DE）的表格数据量化RV容积的集成回归方法的临床部署，我们建议将体积预测与不确定性得分相结合。为此，我们采用一种基于实例的方法，该方法使用学习的树结构来识别目标实例周围的最近训练样本，然后使用多种分布类型来更灵活地建模输出。所提出的框架的概率和点预测性能在一个相对小规模的数据集上进行评估，包括100个舒张末和收缩末RV容积。点性能的参考值来自MRI。结果表明，我们的灵活方法在概率和点预测性能上表现出更好的表现。

    arXiv:2403.03229v1 Announce Type: cross  Abstract: The right ventricular (RV) function deterioration strongly predicts clinical outcomes in numerous circumstances. To boost the clinical deployment of ensemble regression methods that quantify RV volumes using tabular data from the widely available two-dimensional echocardiography (2DE), we propose to complement the volume predictions with uncertainty scores. To this end, we employ an instance-based method which uses the learned tree structure to identify the nearest training samples to a target instance and then uses a number of distribution types to more flexibly model the output. The probabilistic and point-prediction performances of the proposed framework are evaluated on a relatively small-scale dataset, comprising 100 end-diastolic and end-systolic RV volumes. The reference values for point performance were obtained from MRI. The results demonstrate that our flexible approach yields improved probabilistic and point performances ove
    
[^171]: Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects

    Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects

    [https://arxiv.org/abs/2403.02624](https://arxiv.org/abs/2403.02624)

    该论文提出了帕累托最优估计和策略学习的方法，用于确定如何在短期和长期治疗效果之间进行权衡从而实现最佳治疗。

    

    这篇论文专注于发展帕累托最优估计和策略学习，以确定最有效的治疗方法，从而最大化来自短期和长期效果的总奖励，这可能会相互冲突。 例如，药物剂量的增加可能会提高患者康复速度（短期），但也可能导致严重的长期副作用。虽然最近的研究已经探讨了有关短期或长期效应或两者的问题，但如何在它们之间取得平衡以实现最佳治疗仍然是一个悬而未决的挑战。此外，当使用传统因果表示学习直接估计多个目标时，各种任务之间的优化方向也可能发生冲突。在这篇论文中，我们系统地研究了这些问题，并引入了一个帕累托有效算法，包括帕累托最优估计（POE）和帕累托最优策略学习（POPL）。

    arXiv:2403.02624v1 Announce Type: cross  Abstract: This paper focuses on developing Pareto-optimal estimation and policy learning to identify the most effective treatment that maximizes the total reward from both short-term and long-term effects, which might conflict with each other. For example, a higher dosage of medication might increase the speed of a patient's recovery (short-term) but could also result in severe long-term side effects. Although recent works have investigated the problems about short-term or long-term effects or the both, how to trade-off between them to achieve optimal treatment remains an open challenge. Moreover, when multiple objectives are directly estimated using conventional causal representation learning, the optimization directions among various tasks can conflict as well. In this paper, we systematically investigate these issues and introduce a Pareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and Pareto-Optimal Policy Learning (POPL
    
[^172]: 在表格预测上优化预训练语言模型的方法

    Making Pre-trained Language Models Great on Tabular Prediction

    [https://arxiv.org/abs/2403.01841](https://arxiv.org/abs/2403.01841)

    提出了一种专门为表格数据预测而预训练的语言模型TP-BERTa，通过新颖的相对大小标记化方法和内部特征关注方法解决了预训练语言模型在数值特征值上的不兼容性问题

    

    深度神经网络（DNN）的可迁移性在图像和语言处理领域取得了显著进展。然而，由于表格之间的异质性，这种DNN的优势在表格数据预测（例如回归或分类任务）上仍未得到充分利用。本文提出了TP-BERTa，这是一种专门为表格数据预测而预训练的语言模型。具体而言，一种新颖的相对大小标记化方法将标量数值特征值转换为离散度高、高维度的标记，并且一种内部特征关注方法整合了特征名称和数值特征值。

    arXiv:2403.01841v1 Announce Type: new  Abstract: The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM model for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates fe
    
[^173]: 基于内部表征的上下文锐度作为警报：减少幻觉的一个视角

    In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation

    [https://arxiv.org/abs/2403.01548](https://arxiv.org/abs/2403.01548)

    本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。

    

    大型语言模型（LLMs）经常会产生幻觉并产生事实错误，然而我们对它们为什么会犯这些错误的理解仍然有限。在本研究中，我们从内部表征的角度深入探讨LLM幻觉的潜在机制，并发现与幻觉相关的一个突出模式：正确的生成在上下文标记的隐藏状态中具有更清晰的上下文激活，而不正确的生成则没有。利用这一见解，我们提出了一种基于熵的度量来量化上下文隐藏状态之间的“锐度”，并将其纳入解码过程中以制定一种受限解码方法。在各种知识寻求和幻觉基准测试上的实验证明了我们方法的一致有效性，例如，在TruthfulQA上实现了高达8.6点的改进。我们相信这项研究可以提高我们对幻觉的理解。

    arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
    
[^174]: 面向智能电网负荷预测的隐私保护协同分裂学习框架

    Privacy-Preserving Collaborative Split Learning Framework for Smart Grid Load Forecasting

    [https://arxiv.org/abs/2403.01438](https://arxiv.org/abs/2403.01438)

    提出了面向智能电网负荷预测的隐私保护协同分裂学习框架，通过将深度神经网络模型分裂为Grid Station（GS）和服务提供商（SP）部分，实现智能电表数据的隐私保护和负载预测的个性化模型训练。

    

    准确的负荷预测对能源管理、基础设施规划和供需平衡至关重要。智能电表数据的可用性导致了传感器数据驱动的负荷预测需求。传统机器学习允许使用来自多个智能电表的数据训练单个全局模型，这需要将数据传输到中央服务器，引发了对网络要求、隐私和安全性的担忧。我们提出了一种基于分裂学习的负荷预测框架，以缓解这一问题。我们将深度神经网络模型分为两个部分，一个用于每个Grid Station（GS），负责一个整个社区的智能电表；另一个用于服务提供商（SP）。客户智能电表不共享其数据，而是使用各自的GS模型拆分进行前向传递，只将其激活与GS共享。在这一框架下，每个GS负责为其各自的社区训练个性化模型分裂。

    arXiv:2403.01438v1 Announce Type: new  Abstract: Accurate load forecasting is crucial for energy management, infrastructure planning, and demand-supply balancing. Smart meter data availability has led to the demand for sensor-based load forecasting. Conventional ML allows training a single global model using data from multiple smart meters requiring data transfer to a central server, raising concerns for network requirements, privacy, and security. We propose a split learning-based framework for load forecasting to alleviate this issue. We split a deep neural network model into two parts, one for each Grid Station (GS) responsible for an entire neighbourhood's smart meters and the other for the Service Provider (SP). Instead of sharing their data, client smart meters use their respective GSs' model split for forward pass and only share their activations with the GS. Under this framework, each GS is responsible for training a personalized model split for their respective neighbourhoods,
    
[^175]: PowerFlowMultiNet：用于不平衡三相配电系统的多图神经网络

    PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase Distribution Systems

    [https://arxiv.org/abs/2403.00892](https://arxiv.org/abs/2403.00892)

    PowerFlowMultiNet是一种专门为不平衡三相功率网格设计的新颖多图GNN框架，能够有效捕捉不平衡网格中的不对称性，并引入了图嵌入机制来捕获电力系统网络内部的空间依赖关系。

    

    高效解决配电网中不平衡的三相功率流问题对于网格分析和仿真至关重要。目前急需可处理大规模不平衡功率网格并能提供准确快速解决方案的可扩展算法。为解决这一问题，深度学习技术尤其是图神经网络（GNNs）应运而生。然而，现有文献主要集中在平衡网络上，缺乏支持不平衡三相功率网络的关键内容。本文介绍了PowerFlowMultiNet，这是一个专门为不平衡三相功率网格设计的新颖多图GNN框架。提出的方法在多图表示中分别对每个相进行建模，有效捕捉不平衡网格中固有的不对称性。引入了利用消息传递捕获电力系统网络内部空间依赖关系的图嵌入机制。

    arXiv:2403.00892v1 Announce Type: cross  Abstract: Efficiently solving unbalanced three-phase power flow in distribution grids is pivotal for grid analysis and simulation. There is a pressing need for scalable algorithms capable of handling large-scale unbalanced power grids that can provide accurate and fast solutions. To address this, deep learning techniques, especially Graph Neural Networks (GNNs), have emerged. However, existing literature primarily focuses on balanced networks, leaving a critical gap in supporting unbalanced three-phase power grids. This letter introduces PowerFlowMultiNet, a novel multigraph GNN framework explicitly designed for unbalanced three-phase power grids. The proposed approach models each phase separately in a multigraph representation, effectively capturing the inherent asymmetry in unbalanced grids. A graph embedding mechanism utilizing message passing is introduced to capture spatial dependencies within the power system network. PowerFlowMultiNet out
    
[^176]: 快照强化学习：利用先前轨迹提高效率

    Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency

    [https://arxiv.org/abs/2403.00673](https://arxiv.org/abs/2403.00673)

    提出了快照强化学习（SnapshotRL）框架，通过简单改变环境来增强样本效率，而无需对算法和模型进行任何修改

    

    深度强化学习（DRL）算法需要大量样本和计算资源才能实现更高的性能，这限制了它们的实际应用并对进一步发展构成挑战。鉴于资源有限的约束，利用现有的计算工作（例如学习策略、样本）来增强样本效率和减少DRL算法的计算资源消耗至关重要。以前利用现有计算工作的研究需要对现有算法和模型进行干扰性修改，专门为特定算法设计，缺乏灵活性和通用性。本文提出了快照强化学习（SnapshotRL）框架，通过简单改变环境来增强样本效率，而无需对算法和模型进行任何修改。

    arXiv:2403.00673v1 Announce Type: new  Abstract: Deep reinforcement learning (DRL) algorithms require substantial samples and computational resources to achieve higher performance, which restricts their practical application and poses challenges for further development. Given the constraint of limited resources, it is essential to leverage existing computational work (e.g., learned policies, samples) to enhance sample efficiency and reduce the computational resource consumption of DRL algorithms. Previous works to leverage existing computational work require intrusive modifications to existing algorithms and models, designed specifically for specific algorithms, lacking flexibility and universality. In this paper, we present the Snapshot Reinforcement Learning (SnapshotRL) framework, which enhances sample efficiency by simply altering environments, without making any modifications to algorithms and models. By allowing student agents to choose states in teacher trajectories as the initi
    
[^177]: 基于预排序GNN的定时预测：全局电路预训练，局部时延学习和注意力单元建模

    PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling

    [https://arxiv.org/abs/2403.00012](https://arxiv.org/abs/2403.00012)

    提出了基于预排序GNN的定时预测方法，包括全局电路预训练、局部时延学习和注意力单元建模，以解决大规模工业电路中的信号衰减和误差累积问题。

    

    预路由定时预测最近被研究用于评估芯片设计中候选单元布局的质量。它直接估计引脚级（余量、斜率）和边级（网延迟、单元延迟）的定时指标，而无需耗时路由。然而，在大规模工业电路中，由于长时延路径，它经常遭受信号衰减和误差累积的困扰。为了解决这些挑战，我们提出了一个两阶段方法。首先，我们提出了全局电路训练来预训练一个图自动编码器，从电路网表中学习全局图嵌入。其次，我们使用一种新颖的节点更新方案进行GCN上的消息传递，遵循学习到的图嵌入和电路图的拓扑排序序列。这个方案在更新序列中残留地建模了相邻引脚之间的局部时间延迟，并通过一个注意力单元提取了每个单元内部的查找表信息。

    arXiv:2403.00012v1 Announce Type: new  Abstract: Pre-routing timing prediction has been recently studied for evaluating the quality of a candidate cell placement in chip design. It involves directly estimating the timing metrics for both pin-level (slack, slew) and edge-level (net delay, cell delay), without time-consuming routing. However, it often suffers from signal decay and error accumulation due to the long timing paths in large-scale industrial circuits. To address these challenges, we propose a two-stage approach. First, we propose global circuit training to pre-train a graph auto-encoder that learns the global graph embedding from circuit netlist. Second, we use a novel node updating scheme for message passing on GCN, following the topological sorting sequence of the learned graph embedding and circuit graph. This scheme residually models the local time delay between two adjacent pins in the updating sequence, and extracts the lookup table information inside each cell via a ne
    
[^178]: 数据解释器：用于数据科学的LLM代理

    Data Interpreter: An LLM Agent For Data Science

    [https://arxiv.org/abs/2402.18679](https://arxiv.org/abs/2402.18679)

    本研究引入了数据解释器，采用动态规划、工具集成和逻辑错误识别等关键技术，旨在增强数据科学中的问题解决能力。

    

    大型语言模型（LLM）代理已表现出显著的有效性。然而，在需要实时数据调整、优化专业知识以应对各种任务间复杂依赖性以及精确推理的逻辑错误识别的数据科学场景中，它们的性能可能会受到影响。本研究介绍了数据解释器，这是一个设计用于解决强调三种关键技术以增强数据科学中问题解决的方案的代码：1）具有分层图结构的动态规划，用于实时数据适应性；2）工具集成动态化，以增强代码执行过程中的熟练度，丰富必要的专业知识；3）在反馈中识别逻辑不一致性，并通过经验记录来提高效率。我们评估了数据解释器在各种数据科学和现实任务上的表现。与开源基线相比，它展现了s

    arXiv:2402.18679v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated s
    
[^179]: MMSR：符号回归是一个多模态任务

    MMSR: Symbolic Regression is a Multimodal Task

    [https://arxiv.org/abs/2402.18603](https://arxiv.org/abs/2402.18603)

    符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。

    

    数学公式是探索自然规律几千年来人类智慧的结晶。用简洁的数学公式描述复杂的自然规律是科学家不断追求的目标，也是人工智能面临的重大挑战。这一领域被称为符号回归。在本文中，研究人员将从数据到表达式的映射视为翻译问题，并引入了相应的大规模预训练模型。

    arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
    
[^180]: RIME: 具有嘈杂偏好的健壮偏好强化学习

    RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences

    [https://arxiv.org/abs/2402.17257](https://arxiv.org/abs/2402.17257)

    RIME是一种针对嘈杂偏好的健壮PbRL算法，通过动态过滤去噪偏好和热启动奖励模型，极大增强了现有最先进PbRL方法的鲁棒性。

    

    偏好强化学习（PbRL）通过利用人类偏好作为奖励信号，避免了对奖励设计的需求。然而，当前PbRL算法过度依赖来自领域专家的高质量反馈，导致缺乏鲁棒性。在本文中，我们提出了RIME，一种针对嘈杂偏好的健壮PbRL算法，用于有效地从嘈杂偏好中学习奖励。我们的方法结合了基于样本选择的鉴别器，动态过滤去噪偏好以进行健壮训练。为了减轻选择不正确造成的累积误差，我们提出热启动奖励模型，此外还能填补PbRL中从预训练到在线训练过渡时的性能差距。我们在机器人操纵和运动任务上的实验表明，RIME显著提升了当前最先进的PbRL方法的鲁棒性。消融研究进一步表明，热启动

    arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
    
[^181]: 基于信息的转导式主动学习

    Information-based Transductive Active Learning

    [https://arxiv.org/abs/2402.15898](https://arxiv.org/abs/2402.15898)

    ITL是一种基于信息的转导式学习方法，可以在现实世界设置中自适应采样，以最大化关于指定预测目标的信息获取，并在少样本微调和安全贝叶斯优化应用中显著优于最先进技术。

    

    我们将主动学习推广到解决现实世界中采样受限于可访问域的情况，而预测目标可能位于这个域之外。为此，我们提出了ITL，即基于信息的转导式学习，一种自适应采样的方法，旨在最大化关于指定预测目标的信息获取。在一般正则性假设下，我们展示了ITL收敛到可从可访问数据中获得的最小可能不确定性。我们在两个关键应用中展示了ITL：大型神经网络的少样本微调和安全贝叶斯优化，在两种情况下，ITL明显优于最先进技术。

    arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
    
[^182]: 主动少样本微调

    Active Few-Shot Fine-Tuning

    [https://arxiv.org/abs/2402.15441](https://arxiv.org/abs/2402.15441)

    该论文提出了ITL方法来实现主动少样本微调，通过最大化对下游任务的信息获取，从而在大型神经网络的微调中取得了显著的改进。

    

    我们研究了大型神经网络对下游任务进行主动少样本微调。我们表明少样本微调是传统主动学习和转导主动学习的泛化实例，我们提出了信息基于转导学习（ITL）的方法，该方法自适应地进行采样以最大化获得对指定下游任务的信息。在一般正则性假设下，我们证明ITL均匀收敛到可从可访问数据获取的最小可能的不确定性。据我们所知，我们是首批推导出这种泛化界限的人，这对于主动学习可能是具有独立意义的。我们将ITL应用于大型神经网络的少样本微调中，结果显示ITL明显改进了现有技术。

    arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
    
[^183]: 利用特征对射电天文数据进行新颖性检测

    Novelty Detection on Radio Astronomy Data using Signatures

    [https://arxiv.org/abs/2402.14892](https://arxiv.org/abs/2402.14892)

    提出了一个新的半监督框架SigNova，用于检测射电天文数据中的异常值，采用特征转换提取摘要统计信息并计算新颖性评分，以识别偏离预期行为的观察范围。

    

    我们引入了SigNova，一个新的半监督框架，用于检测流数据中的异常值。尽管我们最初的例子侧重于在射电天文学领域内检测数字信号中的射频干扰（RFI），但重要的是要注意，SigNova的适用范围扩展到任何类型的流数据。该框架由三个主要组件组成。首先，我们使用特征转换从观测序列中提取一组规范的摘要统计信息。这使我们能够将可变长度的可见性样本表示为有限维特征向量。其次，每个特征向量被分配一个新颖性评分，计算为到无RFI训练集中最近邻的马氏距离。通过设定这些分数的阈值，我们识别出偏离无RFI可见性样本预期行为的观察范围，而不依赖于严格的分布假设。

    arXiv:2402.14892v1 Announce Type: cross  Abstract: We introduce SigNova, a new semi-supervised framework for detecting anomalies in streamed data. While our initial examples focus on detecting radio-frequency interference (RFI) in digitized signals within the field of radio astronomy, it is important to note that SigNova's applicability extends to any type of streamed data. The framework comprises three primary components. Firstly, we use the signature transform to extract a canonical collection of summary statistics from observational sequences. This allows us to represent variable-length visibility samples as finite-dimensional feature vectors. Secondly, each feature vector is assigned a novelty score, calculated as the Mahalanobis distance to its nearest neighbor in an RFI-free training set. By thresholding these scores we identify observation ranges that deviate from the expected behavior of RFI-free visibility samples without relying on stringent distributional assumptions. Thirdl
    
[^184]: Transformer 技巧：预计算第一层

    Transformer tricks: Precomputing the first layer

    [https://arxiv.org/abs/2402.13388](https://arxiv.org/abs/2402.13388)

    该论文描述了一种加速具有RoPE的transformer推断的技巧，通过预计算第一层来降低延迟和成本，最大节省取决于总层数。

    

    这篇简短的论文描述了一种加速具有 RoPE（如 LLaMA、Mistral 和 PaLM）的 transformer 推断的技巧。对于这些模型，第一个 transformer 层的大部分内容可以预先计算，从而导致稍低的延迟和更低的每令牌成本。因为这种技巧仅优化了一层，相对节省取决于总层数。例如，对于只有 4 层的模型（如 Whisper tiny），最大节省仅限于 25%，而对于 32 层模型（如 Mistral-7B），节省则是 3%。

    arXiv:2402.13388v1 Announce Type: new  Abstract: This short paper describes a trick to speed up inference of transformers with RoPE (such as LLaMA, Mistral, and PaLM). For these models, a large portion of the first transformer layer can be precomputed, which results in slightly lower latency and lower cost-per-token. Because this trick optimizes only one layer, the relative savings depend on the total number of layers. For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model (such as Mistral-7B) is limited to 3% savings.
    
[^185]: CounterCurate: 通过对照例子增强物理和语义视觉-语言组合推理能力

    CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples

    [https://arxiv.org/abs/2402.13254](https://arxiv.org/abs/2402.13254)

    本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。

    

    我们提出CounterCurate，一个框架，全面提升对比和生成式多模态模型的视觉-语言组合推理能力。特别地，我们确定了两个尚未充分探讨的关键问题：忽视了基于物理的推理（计数和位置理解），以及利用高性能文本和图像生成模型进行语义反事实微调的潜力。我们的工作开创了一个解决这些空白的方法。我们首先突出了多模态模型（如CLIP和LLaVA）在基于物理的组合推理中几乎无法胜任的表现。然后，我们应用简单的数据增强，使用基于图像的生成模型GLIGEN生成微调数据，使得性能显著提高：在我们新的策划的Flickr30k-Positions基准测试中，CLIP和LLaVA的性能分别提高了+33%和+37%。此外，我们利用了高性能文本和图像生成模型的能力。

    arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
    
[^186]: Mafin: 用模型增强微调来增强黑盒嵌入

    Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning

    [https://arxiv.org/abs/2402.12177](https://arxiv.org/abs/2402.12177)

    Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。

    

    检索增强生成（RAG）已经成为缓解大型语言模型（LLMs）中幻觉的有效解决方案。RAG中的检索阶段通常涉及预训练的嵌入模型，将查询和段落转换为向量以捕获它们的语义。然而，当应用于特定领域知识时，标准的预训练嵌入模型可能表现出次优性能，需要进行微调。本文解决了仅能从黑盒模型获取嵌入的情况。我们引入了模型增强微调（Mafin）--一种通过用可训练的嵌入模型增强黑盒嵌入模型来进行微调的新方法。我们的结果表明，Mafin仅需要训练一个小的增强模型就可以显著提高黑盒嵌入的性能。我们验证了我们的方法在有标签和无标签数据集上的有效性。

    arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
    
[^187]: 基于3D场景表示的3D扩散器Actor：通过策略扩散进行机器人操作

    3D Diffuser Actor: Policy Diffusion with 3D Scene Representations

    [https://arxiv.org/abs/2402.10885](https://arxiv.org/abs/2402.10885)

    通过策略扩散和3D场景表示相结合，提出了3D Diffuser Actor，一个神经策略架构，可以根据语言指令构建3D视觉场景表示，并对机器人末端执行器的3D旋转和平移进行迭代去噪。

    

    我们将扩散策略和3D场景表示相结合，用于机器人操作。扩散策略通过条件扩散模型学习基于机器人和环境状态的动作分布。最近，它们已经表现出优于确定性和其他基于状态的动作分布学习方法。3D机器人策略使用从单个或多个摄像头视角获取的感应深度聚合的3D场景特征表示。它们已经证明比其2D对应物在摄像机视角上具有更好的泛化能力。我们统一了这两条线路的工作，并提出了3D扩散器Actor，这是一个神经策略架构，它在给定语言指令的情况下，构建视觉场景的3D表示，并在其上进行条件迭代去噪机器人末端执行器的3D旋转和平移。在每个去噪迭代中，我们的模型将末端执行器姿态估计表示为3D场景令牌，并预测t

    arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
    
[^188]: 将神经与物理融合：用可处理的模拟增强蛋白质构象采样

    Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations

    [https://arxiv.org/abs/2402.10433](https://arxiv.org/abs/2402.10433)

    本研究探索了预训练生成采样器在少样本情况下结合MD模拟的方法，以提高蛋白质构象采样的准确性和效率。

    

    蛋白质动力学对于它们的生物功能和性质非常普遍且重要，研究通常涉及耗时的分子动力学(MD)模拟。最近，生成模型被利用作为一个替代采样器，以比传统方法快几个数量级地获得构象集合，而且不需要任何模拟数据（“零次推断”）。然而，由于不考虑底层能量景观，这种生成模型的准确性仍然可能受到限制。在这项工作中，我们探索了这种预训练生成采样器的少样本设置，它以一种可处理的方式结合了MD模拟。具体而言，对于一个目标蛋白质，我们首先从预训练采样器中获取一些种子构象，然后从这些种子样本开始进行一系列物理模拟。然后我们通过模拟轨迹对生成模型进行微调。

    arXiv:2402.10433v1 Announce Type: cross  Abstract: The protein dynamics are common and important for their biological functions and properties, the study of which usually involves time-consuming molecular dynamics (MD) simulations in silico. Recently, generative models has been leveraged as a surrogate sampler to obtain conformation ensembles with orders of magnitude faster and without requiring any simulation data (a "zero-shot" inference). However, being agnostic of the underlying energy landscape, the accuracy of such generative model may still be limited. In this work, we explore the few-shot setting of such pre-trained generative sampler which incorporates MD simulations in a tractable manner. Specifically, given a target protein of interest, we first acquire some seeding conformations from the pre-trained sampler followed by a number of physical simulations in parallel starting from these seeding samples. Then we fine-tuned the generative model using the simulation trajectories a
    
[^189]: 检查生成对抗网络判别器中的病态偏见：以StyleGAN3模型为例的案例研究

    Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model

    [https://arxiv.org/abs/2402.09786](https://arxiv.org/abs/2402.09786)

    这项研究发现了StyleGAN3模型中判别器的病态偏见，它在图像和面部质量上的得分分层影响了不同性别、种族和其他类别的图像。

    

    生成对抗网络可以生成逼真的人脸，往往难以被人类区分出来。我们发现预训练的StyleGAN3模型中的判别器在图像和面部质量上系统地对得分进行分层，并且这不成比例地影响了不同性别、种族和其他类别的图像。我们检查了判别器在色彩和亮度方面对感知的种族和性别的偏见，然后检查了社会心理学中关于刻板印象研究中常见的偏见。

    arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
    
[^190]: NetInfoF 框架：度量和利用网络可利用信息

    NetInfoF Framework: Measuring and Exploiting Network Usable Information

    [https://arxiv.org/abs/2402.07999](https://arxiv.org/abs/2402.07999)

    NetInfoF框架提供了一种快速度量和利用节点属性图中的可利用信息的方法，能同时处理链路预测和节点分类任务，并具备理论保证和闭式解的方法。

    

    对于一个节点属性图和一个图任务（链路预测或节点分类），我们能否判断图神经网络（GNN）是否能很好地完成任务？具体而言，图结构和节点特征是否包含足够可利用的信息来完成任务？我们的目标是（1）开发一个快速工具来度量图结构和节点特征中包含的信息量，以及（2）根据信息量来解决任务。我们提出了NetInfoF框架，包括NetInfoF_Probe和NetInfoF_Act两个部分，分别用于度量和利用网络可利用信息（NUI）。给定一个图数据，NetInfoF_Probe在不进行任何模型训练的情况下度量NUI，而NetInfoF_Act则用于解决链路预测和节点分类任务，两个模块共享相同的骨干网络。总之，NetInfoF具有以下显著优点：（a）通用性，能处理链路预测和节点分类两种任务；（b）原则性，具备理论保证和闭式解的方法。

    Given a node-attributed graph, and a graph task (link prediction or node classification), can we tell if a graph neural network (GNN) will perform well? More specifically, do the graph structure and the node features carry enough usable information for the task? Our goals are (1) to develop a fast tool to measure how much information is in the graph structure and in the node features, and (2) to exploit the information to solve the task, if there is enough. We propose NetInfoF, a framework including NetInfoF_Probe and NetInfoF_Act, for the measurement and the exploitation of network usable information (NUI), respectively. Given a graph data, NetInfoF_Probe measures NUI without any model training, and NetInfoF_Act solves link prediction and node classification, while two modules share the same backbone. In summary, NetInfoF has following notable advantages: (a) General, handling both link prediction and node classification; (b) Principled, with theoretical guarantee and closed-form solu
    
[^191]: 一种非侵入性神经质量评估模型应用于表面肌电信号

    A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals

    [https://arxiv.org/abs/2402.05482](https://arxiv.org/abs/2402.05482)

    本研究提出了一种新的非侵入性神经质量评估模型QASE-net，可以有效预测表面肌电信号的信噪比，实验证明其相比之前的模型具有更低的预测误差和更高的线性相关性。

    

    在涉及测量肌肉表面肌电（sEMG）的实际场景中，尤其是靠近心脏的区域，主要的污染源之一是心电图（ECG）信号的存在。为了更有效地评估实际世界中的sEMG数据质量，本研究提出了QASE-net，一种新的非侵入性模型，可以预测sEMG信号的信噪比。QASE-net将CNN-BLSTM与注意力机制相结合，并采用端到端的训练策略。我们的实验框架利用了两个开放访问数据库的实际世界sEMG和ECG数据，分别是非侵入性适应性假肢数据库和MIT-BIH正常窦性心律数据库。实验结果表明，QASE-net优于先前的评估模型，具有显著降低的预测误差和明显更高的线性相关性。这些发现显示了QASE-net在提高可靠性和的潜力

    In practical scenarios involving the measurement of surface electromyography (sEMG) in muscles, particularly those areas near the heart, one of the primary sources of contamination is the presence of electrocardiogram (ECG) signals. To assess the quality of real-world sEMG data more effectively, this study proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG signals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an end-to-end training strategy. Our experimental framework utilizes real-world sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database, respectively. The experimental results demonstrate the superiority of QASE-net over the previous assessment model, exhibiting significantly reduced prediction errors and notably higher linear correlations with the ground truth. These findings show the potential of QASE-net to substantially enhance the reliability and 
    
[^192]: 利用分治程序指导大型语言模型对问题求解进行引导

    Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving

    [https://arxiv.org/abs/2402.05359](https://arxiv.org/abs/2402.05359)

    该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。

    

    基础模型，如大型语言模型（LLMs），因其广泛的应用而引起了广泛的关注。现有的研究表明，适当的提示设计，如思维链，可以释放LLM在不同领域的强大能力。然而，对于处理涉及重复子任务和/或具有欺骗性内容的任务（如算术计算和文章级虚假新闻检测），现有的提示策略要么表现出表达能力不足，要么由幻觉引发中间错误。为了使LLM对这些中间错误更具辨别力，我们提出了一种以分治程序引导LLM的方法，同时确保优越的表达能力和任务分解、子任务解决和解决组装过程的分离。理论分析表明，我们的策略可以引导LLM扩展固定深度Transformer的表达能力。实验表明，我们提出的方法可以实现

    Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
    
[^193]: IGUANe: 一种适用于脑MR图像多中心协调的三维通用CycleGAN模型

    IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images

    [https://arxiv.org/abs/2402.03227](https://arxiv.org/abs/2402.03227)

    IGUANe是一种三维通用CycleGAN模型，通过集成多个域的训练实现了脑MR图像的多中心协调，使其成为通用生成器。

    

    在MRI研究中，来自多个采集点的图像数据的聚合可以增加样本大小，但可能引入阻碍后续分析一致性的与采集点相关的变异。图像翻译的深度学习方法已经成为协调MR图像跨站点的解决方案。在本研究中，我们引入了IGUANe（具有统一对抗网络的图像生成），这是一种原始的三维模型，它结合了域转换的优势和直接应用样式转移方法来实现多中心脑MR图像协调。IGUANe通过多对一策略，集成了任意数量的域进行训练，扩展了CycleGAN架构。在推断过程中，该模型可以应用于任何图像，甚至来自未知采集点，使其成为协调的通用生成器。在由11台不同扫描仪的T1加权图像组成的数据集上进行训练，IGUANe在未见站点的数据上进行了评估。

    In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
    
[^194]: 一维适配器来统治它们所有：概念、扩散模型和消除应用

    One-Dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications

    [https://arxiv.org/abs/2312.16145](https://arxiv.org/abs/2312.16145)

    基于一维结构，该研究提出了一种新的擦除框架，旨在解决现有概念消除方法存在的问题，实现非侵入性、精确性、可定制性和可转移性。

    

    由于商业和开源扩散模型（DMs）在文本到图像生成中的广泛应用，为防止不良行为，现有的概念擦除方法基于完全参数或基于规范的精细调整，观察到以下问题：1）不断侵蚀朝向的生成：目标消除过程中的参数漂移导致所有生成的变化和潜在变形，甚至在多概念消除时侵蚀其他概念，在不同程度上更加明显；2）转移能力和部署效率：之前基于模型的擦除阻碍了概念的灵活组合和训练免费转移到其他模型，导致部署场景增加时成本线性增长。为了实现无侵入、精确、可定制和可转移的消除，我们将擦除框架建立在一维基础上。

    arXiv:2312.16145v2 Announce Type: replace-cross  Abstract: The prevalent use of commercial and open-source diffusion models (DMs) for text-to-image generation prompts risk mitigation to prevent undesired behaviors. Existing concept erasing methods in academia are all based on full parameter or specification-based fine-tuning, from which we observe the following issues: 1) Generation alternation towards erosion: Parameter drift during target elimination causes alternations and potential deformations across all generations, even eroding other concepts at varying degrees, which is more evident with multi-concept erased; 2) Transfer inability & deployment inefficiency: Previous model-specific erasure impedes the flexible combination of concepts and the training-free transfer towards other models, resulting in linear cost growth as the deployment scenarios increase. To achieve non-invasive, precise, customizable, and transferable elimination, we ground our erasing framework on one-dimension
    
[^195]: ACPO: AI-Enabled Compiler-Driven Program Optimization

    ACPO: AI-Enabled Compiler-Driven Program Optimization

    [https://arxiv.org/abs/2312.09982](https://arxiv.org/abs/2312.09982)

    该论文提出了ACPO框架，通过机器学习模型提供给LLVM简单全面的工具，以实现编译器驱动的程序优化。

    

    该论文提出了ACPO：AI-Enabled Compiler-driven Program Optimization，这是一个新颖的框架，为LLVM提供简单全面的工具，以从应用机器学习模型来进行不同的优化通路中获益。首先展示了ACPO的高层视图、类层次结构和功能，然后通过将循环展开和函数内联传递的ML使能化，展示了ACPO的一些用例，描述了ACPO如何发挥作用。

    arXiv:2312.09982v2 Announce Type: replace-cross  Abstract: The key to performance optimization of a program is to decide correctly when a certain transformation should be applied by a compiler. This is an ideal opportunity to apply machine-learning models to speed up the tuning process; while this realization has been around since the late 90s, only recent advancements in ML enabled a practical application of ML to compilers as an end-to-end framework.   This paper presents ACPO: \textbf{\underline{A}}I-Enabled \textbf{\underline{C}}ompiler-driven \textbf{\underline{P}}rogram \textbf{\underline{O}}ptimization; a novel framework to provide LLVM with simple and comprehensive tools to benefit from employing ML models for different optimization passes. We first showcase the high-level view, class hierarchy, and functionalities of ACPO and subsequently, demonstrate a couple of use cases of ACPO by ML-enabling the Loop Unroll and Function Inlining passes and describe how ACPO can be leverage
    
[^196]: 重新审视随机梯度方法的最终迭代收敛性

    Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods

    [https://arxiv.org/abs/2312.08531](https://arxiv.org/abs/2312.08531)

    研究了随机梯度方法的最终迭代收敛性，并提出了不需要限制性假设的最优收敛速率问题。

    

    在过去几年里，随机梯度下降（SGD）算法的最终迭代收敛引起了人们的兴趣，因为它在实践中表现良好但缺乏理论理解。对于Lipschitz凸函数，不同的研究建立了最佳的$O(\log(1/\delta)\log T/\sqrt{T})$或$O(\sqrt{\log(1/\delta)/T})$最终迭代的高概率收敛速率，其中$T$是时间跨度，$\delta$是失败概率。然而，为了证明这些界限，所有现有的工作要么局限于紧致域，要么需要几乎肯定有界的噪声。很自然地会问，不需要这两个限制性假设的情况下，SGD的最终迭代是否仍然可以保证最佳的收敛速率。除了这个重要问题外，还有很多理论问题仍然没有答案。

    arXiv:2312.08531v2 Announce Type: replace  Abstract: In the past several years, the last-iterate convergence of the Stochastic Gradient Descent (SGD) algorithm has triggered people's interest due to its good performance in practice but lack of theoretical understanding. For Lipschitz convex functions, different works have established the optimal $O(\log(1/\delta)\log T/\sqrt{T})$ or $O(\sqrt{\log(1/\delta)/T})$ high-probability convergence rates for the final iterate, where $T$ is the time horizon and $\delta$ is the failure probability. However, to prove these bounds, all the existing works are either limited to compact domains or require almost surely bounded noises. It is natural to ask whether the last iterate of SGD can still guarantee the optimal convergence rate but without these two restrictive assumptions. Besides this important question, there are still lots of theoretical problems lacking an answer. For example, compared with the last-iterate convergence of SGD for non-smoot
    
[^197]: SparQ注意力：高效带宽的LLM推理

    SparQ Attention: Bandwidth-Efficient LLM Inference

    [https://arxiv.org/abs/2312.04985](https://arxiv.org/abs/2312.04985)

    SparQ Attention通过减少注意力块内存带宽需求的技术，从而增加LLMs推理的吞吐量，同时保持模型准确性。

    

    生成式大语言模型（LLMs）开创了许多新可能性，但由于其巨大的计算需求，它们的普遍使用仍然具有挑战性。我们引入了SparQ注意力，一种通过选择性获取缓存历史来减少注意力块内存带宽需求的技术，从而增加了LLMs的推理吞吐量。

    arXiv:2312.04985v2 Announce Type: replace  Abstract: Generative large language models (LLMs) have opened up numerous novel possibilities, but due to their significant computational requirements their ubiquitous use remains challenging. Some of the most useful applications require processing large numbers of samples at a time and using long contexts, both significantly increasing the memory communication load of the models. We introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by reducing the memory bandwidth requirements within the attention blocks through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show how SparQ Attention can decrease the attention memory bandwidth requirements up to eight times without any loss in accuracy by evaluating Llama 2 and Pythia models on a wide ra
    
[^198]: 量子启蒙分数

    Quantum Inception Score

    [https://arxiv.org/abs/2311.12163](https://arxiv.org/abs/2311.12163)

    通过量子启蒙分数，我们提出了一个用于评估量子生成模型质量的新指标，证明量子生成模型在质量上优于经典生成模型，并利用量子波动定理揭示了其物理限制。

    

    受到经典生成模型在机器学习中取得巨大成功的启发，近期开始了对它们量子版本的热切探索。为了开始这一探索之旅，开发一个相关的度量标准来评估量子生成模型的质量是很重要的；在经典情况下，一个这样的例子便是启蒙分数。在本文中，我们提出了量子启蒙分数，它将质量与用于对给定数据集进行分类的量子通道的Holevo信息联系起来。我们证明，在这个提出的度量标准下，量子生成模型提供比它们的经典对应物更好的质量，因为存在着由不对称性的资源理论和纠缠所表征的量子相干性。此外，我们利用量子波动定理来表征限制量子生成模型质量的物理限制。最后，我们应用量子启蒙分数来

    arXiv:2311.12163v2 Announce Type: replace-cross  Abstract: Motivated by the great success of classical generative models in machine learning, enthusiastic exploration of their quantum version has recently started. To depart on this journey, it is important to develop a relevant metric to evaluate the quality of quantum generative models; in the classical case, one such example is the inception score. In this paper, we propose the quantum inception score, which relates the quality to the Holevo information of the quantum channel that classifies a given dataset. We prove that, under this proposed measure, the quantum generative models provide better quality than their classical counterparts because of the presence of quantum coherence, characterized by the resource theory of asymmetry, and entanglement. Furthermore, we harness the quantum fluctuation theorem to characterize the physical limitation of the quality of quantum generative models. Finally, we apply the quantum inception score 
    
[^199]: FedRA:一种用于释放异构客户端强大潜力的随机分配策略

    FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients

    [https://arxiv.org/abs/2311.11227](https://arxiv.org/abs/2311.11227)

    提出了一种名为FedRA的联邦调优算法，可以随机生成分配矩阵来应对拥有不同计算和通信资源的异构客户端，在不需要修改原模型的情况下进行微调。

    

    随着基础模型的日益可用，联邦调优在联邦学习领域引起了关注，利用多个客户端的数据和计算资源共同对基础模型进行微调。然而，在现实世界的联邦场景中，通常存在大量具有不同计算和通信资源的异构客户端，导致它们无法支持整个模型的微调过程。针对这一挑战，我们提出了一种新颖的联邦调优算法FedRA。FedRA的实施简单，可以无缝集成到任何基于Transformer的模型中，无需对原模型进行进一步修改。具体而言，在每一轮通信中，FedRA会随机生成一个分配矩阵。对于资源受限的客户端，它会根据分配情况重新组织原模型中的少量层。

    arXiv:2311.11227v2 Announce Type: replace-cross  Abstract: With the increasing availability of Foundation Models, federated tuning has garnered attention in the field of federated learning, utilizing data and computation resources from multiple clients to collaboratively fine-tune foundation models. However, in real-world federated scenarios, there often exist a multitude of heterogeneous clients with varying computation and communication resources, rendering them incapable of supporting the entire model fine-tuning process. In response to this challenge, we propose a novel federated tuning algorithm, FedRA. The implementation of FedRA is straightforward and can be seamlessly integrated into any transformer-based model without the need for further modification to the original model. Specifically, in each communication round, FedRA randomly generates an allocation matrix. For resource-constrained clients, it reorganizes a small number of layers from the original model based on the alloc
    
[^200]: 使用机器学习图像分割进行三维组织培养的非破坏性定量活力分析

    Nondestructive, quantitative viability analysis of 3D tissue cultures using machine learning image segmentation

    [https://arxiv.org/abs/2311.09354](https://arxiv.org/abs/2311.09354)

    本研究提出了一种图像处理算法，用于在三维培养中量化细胞的活力，无需基于试剂的指示物，并且展示了其与人类专家的表现类似。

    

    确定不同细胞培养条件下细胞的集体活力通常依赖于平均色度指标，并且通常以简单的二进制读数报告。最近的研究将活力评估技术与基于图像的深度学习模型相结合，以自动化特征化细胞属性。然而，需要进一步发展活力测量技术，以评估可能的细胞状态连续性和对培养条件下的干扰响应。在这项工作中，我们展示了一种图像处理算法，用于在三维培养中量化细胞的活力，无需基于试剂的指示物。我们展示了我们的算法在一系列天数和培养基组成下在全孔图像中的表现与一对人类专家类似。为了展示潜在的实用性，我们进行了一项纵向研究，调查已知疗法的影响

    arXiv:2311.09354v2 Announce Type: replace-cross  Abstract: Ascertaining the collective viability of cells in different cell culture conditions has typically relied on averaging colorimetric indicators and is often reported out in simple binary readouts. Recent research has combined viability assessment techniques with image-based deep-learning models to automate the characterization of cellular properties. However, further development of viability measurements to assess the continuity of possible cellular states and responses to perturbation across cell culture conditions is needed. In this work, we demonstrate an image processing algorithm for quantifying cellular viability in 3D cultures without the need for assay-based indicators. We show that our algorithm performs similarly to a pair of human experts in whole-well images over a range of days and culture matrix compositions. To demonstrate potential utility, we perform a longitudinal study investigating the impact of a known therap
    
[^201]: 逐步优化学习用于构象能量最小化

    Gradual Optimization Learning for Conformational Energy Minimization

    [https://arxiv.org/abs/2311.06295](https://arxiv.org/abs/2311.06295)

    通过提供优化轨迹作为额外训练数据，可以改善使用神经网络的分子构象能量最小化质量，但需要大量额外的构象数据才能达到物理模拟器的优化质量。

    

    分子构象优化对于计算辅助药物发现和材料设计至关重要。传统的能量最小化技术依赖于使用物理模拟器（oracle）计算的分子力作为反梯度的迭代优化方法。然而，这是一种计算昂贵的方法，需要与物理模拟器进行许多交互。加速这个过程的一种方法是用神经网络替换物理模拟器。尽管近年来神经网络在分子构象能量预测方面取得了进展，但这种模型容易发生分布转移，导致能量最小化不准确。我们发现，通过提供优化轨迹作为额外训练数据，可以改善使用神经网络的能量最小化质量。然而，需要额外约$5 \times 10^5$个构象才能匹配物理模拟器的优化质量。

    arXiv:2311.06295v2 Announce Type: replace-cross  Abstract: Molecular conformation optimization is crucial to computer-aided drug discovery and materials design. Traditional energy minimization techniques rely on iterative optimization methods that use molecular forces calculated by a physical simulator (oracle) as anti-gradients. However, this is a computationally expensive approach that requires many interactions with a physical simulator. One way to accelerate this procedure is to replace the physical simulator with a neural network. Despite recent progress in neural networks for molecular conformation energy prediction, such models are prone to distribution shift, leading to inaccurate energy minimization. We find that the quality of energy minimization with neural networks can be improved by providing optimization trajectories as additional training data. Still, it takes around $5 \times 10^5$ additional conformations to match the physical simulator's optimization quality. In this 
    
[^202]: 通过低维学习动态实现超参数化深度模型的高效压缩

    Efficient Compression of Overparameterized Deep Models through Low-Dimensional Learning Dynamics

    [https://arxiv.org/abs/2311.05061](https://arxiv.org/abs/2311.05061)

    通过研究深度模型的学习动态，提出了一种压缩超参数化模型的新方法，通过在低维不变子空间内更新权重矩阵来压缩深度线性网络，并在矩阵恢复问题上进行了有效性评估

    

    超参数化模型已被证明是解决各种机器学习任务的强大工具。然而，过度参数化往往导致计算和内存成本大幅增加，进而需要大量资源来训练。在这项工作中，我们提出了一种新颖的方法来压缩超参数化模型，通过研究它们的学习动态来实现。我们观察到对于许多深度模型，权重矩阵的更新发生在低维不变子空间内。对于深度线性模型，我们展示了它们的主要成分在一个小子空间内逐渐适配，并利用这些见解提出了一种针对深度线性网络的压缩算法，其中包括减小其中间层的宽度。我们从实验角度评估了我们的压缩技术在矩阵恢复问题上的有效性。

    arXiv:2311.05061v2 Announce Type: replace  Abstract: Overparameterized models have proven to be powerful tools for solving various machine learning tasks. However, overparameterization often leads to a substantial increase in computational and memory costs, which in turn requires extensive resources to train. In this work, we present a novel approach for compressing overparameterized models, developed through studying their learning dynamics. We observe that for many deep models, updates to the weight matrices occur within a low-dimensional invariant subspace. For deep linear models, we demonstrate that their principal components are fitted incrementally within a small subspace, and use these insights to propose a compression algorithm for deep linear networks that involve decreasing the width of their intermediate layers. We empirically evaluate the effectiveness of our compression technique on matrix recovery problems. Remarkably, by using an initialization that exploits the structur
    
[^203]: 评估新兴的AI/ML加速器：IPU、RDU和NVIDIA/AMD GPU

    Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs

    [https://arxiv.org/abs/2311.04417](https://arxiv.org/abs/2311.04417)

    本研究提供了对这些商用AI/ML加速器的初步评估和比较，深入探讨它们的硬件和软件设计特点，以辨别它们的创新数据流架构和其他设计优化，承诺为AI/ML任务提供卓越性能和能量效率。

    

    人工智能（AI）和机器学习（ML）应用的不断发展需要开发能够处理日益复杂和计算需求的专用硬件加速器。传统计算架构基于冯·诺伊曼模型，已经被当代AI/ML算法的要求超越，导致像Graphcore Intelligence Processing Unit (IPU)、Sambanova Reconfigurable Datafl····

    arXiv:2311.04417v2 Announce Type: replace-cross  Abstract: The relentless advancement of artificial intelligence (AI) and machine learning (ML) applications necessitates the development of specialized hardware accelerators capable of handling the increasing complexity and computational demands. Traditional computing architectures, based on the von Neumann model, are being outstripped by the requirements of contemporary AI/ML algorithms, leading to a surge in the creation of accelerators like the Graphcore Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit (RDU), and enhanced GPU platforms. These hardware accelerators are characterized by their innovative data-flow architectures and other design optimizations that promise to deliver superior performance and energy efficiency for AI/ML tasks.   This research provides a preliminary evaluation and comparison of these commercial AI/ML accelerators, delving into their hardware and software design features to discern t
    
[^204]: 贝叶斯回归市场

    Bayesian Regression Markets

    [https://arxiv.org/abs/2310.14992](https://arxiv.org/abs/2310.14992)

    本论文提出了一种贝叶斯回归市场机制，为数据共享提供了经济激励，并展示了如何缓解市场代理商面临的财务风险。

    

    机器学习任务对所使用的数据质量很敏感。然而，公司往往很难获得足够的数据集，因为这些数据自然分布在各个所有者之间，而这些所有者在实践中可能是竞争对手，不愿意共享信息。我们针对回归任务的监督学习，开发了一个回归市场，以提供数据共享的经济激励。我们提出的机制采用贝叶斯框架，使我们能够考虑更一般的回归任务类别。我们对市场属性进行了彻底探讨，并展示了目前文献中类似提议暴露市场代理商面临可观的财务风险，而这些风险在我们的设置中可以得到缓解。

    arXiv:2310.14992v2 Announce Type: replace  Abstract: Machine learning tasks are vulnerable to the quality of data used as input. Yet, it is often challenging for firms to obtain adequate datasets, with them being naturally distributed amongst owners, that in practice, may be competitors in a downstream market and reluctant to share information. Focusing on supervised learning for regression tasks, we develop a regression market to provide a monetary incentive for data sharing. Our proposed mechanism adopts a Bayesian framework, allowing us to consider a more general class of regression tasks. We present a thorough exploration of the market properties, and show that similar proposals in current literature expose the market agents to sizeable financial risks, which can be mitigated in our setup.
    
[^205]: 利用斜裁决策森林增强在线环境中的群体公平性

    Enhancing Group Fairness in Online Settings Using Oblique Decision Forests

    [https://arxiv.org/abs/2310.11401](https://arxiv.org/abs/2310.11401)

    提出了Aranyani，一种斜裁集成的方法，用于解决在在线环境中优化群体公平性目标所面临的挑战

    

    公平性，特别是群体公平性，在机器学习系统中是一个重要的考虑因素。目前最常见的群体公平性增强技术是通过在训练过程中依赖公平目标（例如人口统计平等）和任务特定目标（例如交叉熵）的混合方法。然而，在数据以在线方式一次一个实例到达时，优化这样的公平性目标面临着几个挑战。特别是，群体公平性目标是通过不同人口统计群体的预测期望来定义的。在在线环境中，算法每次只能访问一个实例，估计群体公平性目标需要额外的存储和比任务特定目标更多的计算（例如前向/后向传递）在每个时间步上。

    arXiv:2310.11401v2 Announce Type: replace  Abstract: Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of obliq
    
[^206]: Bongard-OpenWorld: 在真实世界中进行自由形式视觉概念的少样本推理

    Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World

    [https://arxiv.org/abs/2310.10207](https://arxiv.org/abs/2310.10207)

    Bongard-OpenWorld基准旨在评估机器视觉中对真实世界中的自由形式视觉概念进行少样本推理，并且提出了开放世界自由形式概念和真实世界图像两项新挑战。

    

    我们介绍了Bongard-OpenWorld，这是一个用于评估机器视觉中真实世界少样本推理的新基准。 它源自经典的Bongard问题（BPs）：给定两组图像（正和负），模型需要通过诱导视觉概念来确定查询图像所属的图像集，这些概念仅由正集中的图像所描述。 我们的基准继承了原始BPs的少样本概念归纳，同时增加了两层新挑战：1）开放世界的自由形式概念，因为Bongard-OpenWorld中的视觉概念是从开放词汇表中独特组合的术语，范围从对象类别到抽象视觉属性和常识事实知识； 2）真实世界的图像，而不是许多类似物使用的合成图表。在我们的探索中，Bongard-OpenWorld已经对当前的少样本推理算法提出了重大挑战。我们还远

    arXiv:2310.10207v2 Announce Type: replace  Abstract: We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We furt
    
[^207]: 揭示大语言模型知识编辑的陷阱

    Unveiling the Pitfalls of Knowledge Editing for Large Language Models

    [https://arxiv.org/abs/2310.02129](https://arxiv.org/abs/2310.02129)

    这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。

    

    随着调整大型语言模型（LLMs）成本不断上升，最近的研究工作已经转向开发编辑LLMs内在知识的方法。然而，仍有一个阴云悬在头顶上 - 知识编辑是否会触发蝴蝶效应？因为目前尚不清楚知识编辑是否会引入可能带来潜在风险的副作用。本文首次探讨了与LLMs知识编辑相关的潜在陷阱。为实现此目的，我们引入了新的基准数据集并提出了创新性的评估指标。我们的结果强调了两个关键问题：（1）知识冲突：编辑逻辑冲突的事实组可能会放大LLMs固有的不一致性 - 这是以前方法忽略的一个方面。（2）知识扭曲：为了编辑事实知识而更改参数可能会不可逆地扭曲

    arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
    
[^208]: 用于更具表达力的张量网络模型的量化傅立叶和多项式特征

    Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models

    [https://arxiv.org/abs/2309.05436](https://arxiv.org/abs/2309.05436)

    提出了一种量化傅立叶和多项式特征的方法，并基于此特征量化提出将相关模型权重也进行量化，得到了更具表达力的张量网络模型。

    

    在核机器的背景下，多项式和傅立叶特征通常用于通过将数据映射到更高维空间来为线性模型提供非线性扩展。本文中，我们量化了多项式和傅立叶特征，提出了将相关模型权重量化的方法，得到了量化模型。

    arXiv:2309.05436v2 Announce Type: replace  Abstract: In the context of kernel machines, polynomial and Fourier features are commonly used to provide a nonlinear extension to linear models by mapping the data to a higher-dimensional space. Unless one considers the dual formulation of the learning problem, which renders exact large-scale learning unfeasible, the exponential increase of model parameters in the dimensionality of the data caused by their tensor-product structure prohibits to tackle high-dimensional problems. One of the possible approaches to circumvent this exponential scaling is to exploit the tensor structure present in the features by constraining the model weights to be an underparametrized tensor network. In this paper we quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization we propose to quantize the associated model weights, yielding quantized models. We show that, for the same number of model parameters, the resulting 
    
[^209]: 分析在MICCAI KiTS23挑战中使用额外数据时的域偏移

    Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge

    [https://arxiv.org/abs/2309.02001](https://arxiv.org/abs/2309.02001)

    使用直方图匹配来转换额外数据在处理域偏移时比简单归一化取得更好的结果

    

    使用额外的训练数据已知可以改善结果，特别是对于医学图像3D分割，在那里缺乏训练资料，模型需要在少量数据上有很好的泛化能力。然而，新数据可能是使用其他仪器获取并经过预处理，使得其分布与原始训练数据显著不同。因此，我们研究了在训练过程中改善域偏移的技术，使得额外数据可以更好地用于预处理和与原始数据一起训练。我们的结果表明，使用直方图匹配转换额外数据的效果优于简单的归一化。

    arXiv:2309.02001v2 Announce Type: replace-cross  Abstract: Using additional training data is known to improve the results, especially for medical image 3D segmentation where there is a lack of training material and the model needs to generalize well from few available data. However, the new data could have been acquired using other instruments and preprocessed such its distribution is significantly different from the original training data. Therefore, we study techniques which ameliorate domain shift during training so that the additional data becomes better usable for preprocessing and training together with the original data. Our results show that transforming the additional data using histogram matching has better results than using simple normalization.
    
[^210]: 大型语言模型中的偏见与公平性：一项调查

    Bias and Fairness in Large Language Models: A Survey

    [https://arxiv.org/abs/2309.00770](https://arxiv.org/abs/2309.00770)

    该论文在大型语言模型领域提出了偏见评估和缓解技术的综合调查，定义了公平性的不同方面，并提出了三个分类体系，以协助研究人员对LLMs进行公平性分析和改进。

    

    大型语言模型（LLMs）的快速发展使得人们能够处理、理解和生成类似人类文本，逐渐融入触及我们社交领域的系统。然而，尽管取得成功，这些模型可能学习、延续和放大有害的社会偏见。本文对LLMs的偏见评估和缓解技术进行了全面调查。我们首先整合、形式化和扩展自然语言处理中社会偏见和公平性的概念，定义了伤害的不同方面，并引入了几个实现LLMs公平性的必要条件。然后，我们通过提出三个直观的分类体系统一了文献，其中包括两个用于偏见评估的分类体系，即指标和数据集，以及一个用于缓解的分类体系。

    arXiv:2309.00770v2 Announce Type: replace-cross  Abstract: Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which t
    
[^211]: 基于Stiefel流形的分布式黎曼共轭梯度方法

    Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold

    [https://arxiv.org/abs/2308.10547](https://arxiv.org/abs/2308.10547)

    该论文提出了一种在Stiefel流形上进行分布式优化的黎曼共轭梯度下降方法，克服了全局函数非凸的限制。

    

    共轭梯度法是一种至关重要的一阶优化方法，通常比最速下降法收敛更快，计算成本也远低于二阶方法。然而，尽管在欧几里德空间和黎曼流形上已研究了各种类型的共轭梯度方法，但在分布式场景下的研究却很少。本文提出了一种旨在在Stiefel流形上最小化全局函数的分布式黎曼共轭梯度下降（DRCGD）方法。优化问题在一组代理网络中分布，每个代理与一个局部函数相关联，并且代理之间的通信在一个无向连通图上进行。由于Stiefel流形是一个非凸集，全局函数被表示为可能非凸（但平滑）局部函数的有限和。该方法不受最优非凸常数的限制。

    arXiv:2308.10547v2 Announce Type: replace-cross  Abstract: The conjugate gradient method is a crucial first-order optimization method that generally converges faster than the steepest descent method, and its computational cost is much lower than the second-order methods. However, while various types of conjugate gradient methods have been studied in Euclidean spaces and on Riemannian manifolds, there is little study for those in distributed scenarios. This paper proposes a decentralized Riemannian conjugate gradient descent (DRCGD) method that aims at minimizing a global function over the Stiefel manifold. The optimization problem is distributed among a network of agents, where each agent is associated with a local function, and the communication between agents occurs over an undirected connected graph. Since the Stiefel manifold is a non-convex set, a global function is represented as a finite sum of possibly non-convex (but smooth) local functions. The proposed method is free from ex
    
[^212]: 使用区块链防御联邦学习中的恶意行为

    Defending Against Malicious Behaviors in Federated Learning with Blockchain

    [https://arxiv.org/abs/2307.00543](https://arxiv.org/abs/2307.00543)

    该研究提出了一个基于区块链和分布式分类账技术的安全和可靠的联邦学习系统，包括点对点投票机制和奖励和惩罚机制，以检测和阻止恶意行为，证明了该框架对抗恶意客户的有效性。

    

    在深度学习时代，联邦学习(FL)提供了一种有前途的方法，允许多家机构数据所有者或客户共同训练机器学习模型，而不会损害数据隐私。然而，大多数现有的FL方法依赖于用于全局模型聚合的集中式服务器，导致单点故障。这使系统在处理不诚实的客户时容易受到恶意攻击。在这项工作中，我们通过提出基于区块链和分布式分类账技术的安全可靠FL系统来解决这个问题。我们的系统结合了点对点投票机制和奖励和惩罚机制，由链上智能合约提供动力，以检测和阻止恶意行为。我们提出了理论和实证分析，以展示所提出方法的有效性，表明我们的框架对恶意客户是强大的。

    arXiv:2307.00543v2 Announce Type: replace-cross  Abstract: In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-s
    
[^213]: ProMIL：面向医学成像的概率多实例学习

    ProMIL: Probabilistic Multiple Instance Learning for Medical Imaging

    [https://arxiv.org/abs/2306.10535](https://arxiv.org/abs/2306.10535)

    ProMIL是一种基于实例的方法，在医学应用中能够自动检测最佳决策百分比水平，并在真实场景中表现优异.

    

    多实例学习（MIL）是一个弱监督问题，其中一个标签被分配给整个实例包。其中一种重要的MIL模型是基于实例的，我们首先对实例进行分类，然后聚合这些预测结果以获得包标签。最常见的MIL模型是将包视为正类，如果其实例中至少有一个实例具有正标签。然而，在许多现实场景中，这种推理并不成立，其中正包标签通常是部分正实例的结果。为了解决这个问题，我们引入了一种专门的基于实例的方法，称为ProMIL，基于深度神经网络和Bernstein多项式估计。ProMIL的一个重要优势是它可以自动检测用于决策的最佳百分比水平。我们展示了ProMIL在真实世界医学应用中优于标准基于实例的MIL。我们提供代码。

    arXiv:2306.10535v2 Announce Type: replace-cross  Abstract: Multiple Instance Learning (MIL) is a weakly-supervised problem in which one label is assigned to the whole bag of instances. An important class of MIL models is instance-based, where we first classify instances and then aggregate those predictions to obtain a bag label. The most common MIL model is when we consider a bag as positive if at least one of its instances has a positive label. However, this reasoning does not hold in many real-life scenarios, where the positive bag label is often a consequence of a certain percentage of positive instances. To address this issue, we introduce a dedicated instance-based method called ProMIL, based on deep neural networks and Bernstein polynomial estimation. An important advantage of ProMIL is that it can automatically detect the optimal percentage level for decision-making. We show that ProMIL outperforms standard instance-based MIL in real-world medical applications. We make the code 
    
[^214]: TAP: 跨模态知识传递中的注意力补丁

    TAP: The Attention Patch for Cross-Modal Knowledge Transfer from Unlabeled Modality

    [https://arxiv.org/abs/2302.02224](https://arxiv.org/abs/2302.02224)

    通过引入The Attention Patch（TAP）神经网络附加组件，本文提出了一种简单且有效的方法，允许从未标记的次要模态实现跨模态的数据级知识传递。

    

    本文解决了跨模态学习框架，其目标是通过未标记、不配对的次要模态，增强主要模态中监督学习的性能。采用概率方法进行缺失信息估计，我们表明次要模态中包含的额外信息可以通过Nadaraya-Watson（NW）核回归进行估计，其可以进一步表示为经过线性变换的核交叉注意力模块。我们的结果为引入The Attention Patch（TAP）奠定了基础，这是一个简单的神经网络附加组件，允许从未标记的模态进行数据级知识传递。我们使用四个真实世界数据集进行了大量数值模拟，结果表明TAP能够显著提高跨不同领域和不同神经网络架构的泛化能力，利用看似无用的未标记信息。

    arXiv:2302.02224v2 Announce Type: replace  Abstract: This paper addresses a cross-modal learning framework, where the objective is to enhance the performance of supervised learning in the primary modality using an unlabeled, unpaired secondary modality. Taking a probabilistic approach for missing information estimation, we show that the extra information contained in the secondary modality can be estimated via Nadaraya-Watson (NW) kernel regression, which can further be expressed as a kernelized cross-attention module (under linear transformation). Our results lay the foundations for introducing The Attention Patch (TAP), a simple neural network add-on that allows data-level knowledge transfer from the unlabeled modality. We provide extensive numerical simulations using four real-world datasets to show that TAP can provide statistically significant improvement in generalization across different domains and different neural network architectures, making use of seemingly unusable unlabel
    
[^215]: QLABGrad: 一种无需超参数且收敛保证的深度学习方案

    QLABGrad: a Hyperparameter-Free and Convergence-Guaranteed Scheme for Deep Learning

    [https://arxiv.org/abs/2302.00252](https://arxiv.org/abs/2302.00252)

    QLABGrad是一种无需超参数的学习率自适应方案，通过优化QLAB函数自动确定学习率，并在平滑的Lipschitz条件下证明了其收敛性，实验证明其在多种架构和数据集上表现优越。

    

    学习率是深度学习任务中关键的超参数，因为它决定了模型参数在学习过程中的更新程度。然而，学习率的选择通常依赖于经验判断，在没有进行大量尝试和错误实验的情况下可能无法得到令人满意的结果。在本研究中，我们提出了一种名为QLABGrad的新型学习率自适应方案。QLABGrad无需任何用户指定的超参数，通过优化基于二次损失近似(QLAB)函数来自动确定学习率，仅需要进行一次额外的前向传播。我们在损失函数上理论上证明了QLABGrad的收敛性具有平滑的Lipschitz条件。在多种架构（包括MLP、CNN和ResNet）以及MNIST、CIFAR10和ImageNet数据集上进行的实验结果表明，QLABGrad表现优越。

    arXiv:2302.00252v2 Announce Type: replace  Abstract: The learning rate is a critical hyperparameter for deep learning tasks since it determines the extent to which the model parameters are updated during the learning course. However, the choice of learning rates typically depends on empirical judgment, which may not result in satisfactory outcomes without intensive try-and-error experiments. In this study, we propose a novel learning rate adaptation scheme called QLABGrad. Without any user-specified hyperparameter, QLABGrad automatically determines the learning rate by optimizing the Quadratic Loss Approximation-Based (QLAB) function for a given gradient descent direction, where only one extra forward propagation is required. We theoretically prove the convergence of QLABGrad with a smooth Lipschitz condition on the loss function. Experiment results on multiple architectures, including MLP, CNN, and ResNet, on MNIST, CIFAR10, and ImageNet datasets, demonstrate that QLABGrad outperforms
    
[^216]: StoRM：一种基于扩散的随机再生模型用于语音增强和去混响

    StoRM: A Diffusion-based Stochastic Regeneration Model for Speech Enhancement and Dereverberation

    [https://arxiv.org/abs/2212.11851](https://arxiv.org/abs/2212.11851)

    该论文提出了一种基于扩散的随机再生模型，用于语音增强和去混响，填补了预测性和生成性方法在性能上的差距。

    

    扩散模型在填补语音增强中预测性和生成性方法之间的性能差距方面显示出了巨大能力。我们已经证明，它们甚至在非加性污染类型或在评估不匹配的条件下可能优于它们的预测性对应物。然而，扩散模型存在着较高的计算负担，主要是因为它们需要为每个反向传播步骤运行一个神经网络，而预测性方法只需要一次遍历。由于扩散模型是生成方法，它们在不利条件下可能还会产生发声和呼吸的人工痕迹。相比之下，在这种困难情况下，预测模型通常不会产生这样的人工痕迹，但倾向于扭曲目标语音，从而降低语音质量。在这项工作中，我们提出了一种随机再生方法，其中由预测模型给出的估计被提供为

    arXiv:2212.11851v2 Announce Type: replace-cross  Abstract: Diffusion models have shown a great ability at bridging the performance gap between predictive and generative approaches for speech enhancement. We have shown that they may even outperform their predictive counterparts for non-additive corruption types or when they are evaluated on mismatched conditions. However, diffusion models suffer from a high computational burden, mainly as they require to run a neural network for each reverse diffusion step, whereas predictive approaches only require one pass. As diffusion models are generative approaches they may also produce vocalizing and breathing artifacts in adverse conditions. In comparison, in such difficult scenarios, predictive models typically do not produce such artifacts but tend to distort the target speech instead, thereby degrading the speech quality. In this work, we present a stochastic regeneration approach where an estimate given by a predictive model is provided as a
    
[^217]: APOLLO: 一种用于长篇数字推理的优化训练方法

    APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning

    [https://arxiv.org/abs/2212.07249](https://arxiv.org/abs/2212.07249)

    APOLLO提出了一种优化的训练方法，通过数字感知的负采样策略和基于一致性的强化学习，提高了长篇数字推理框架的准确性和多样性。

    

    本文针对财务分析中长篇数字推理提出了一种优化训练方法，旨在生成一个推理程序以计算给定问题的正确答案。我们提出了APOLLO来改善长篇数字推理框架，针对相关性选择器，我们采用了数字感知的负采样策略，使其能够更加辨别关键的数字事实。而对于生成器，我们设计了基于一致性的强化学习和目标程序增强策略。

    arXiv:2212.07249v3 Announce Type: replace  Abstract: Long-form numerical reasoning in financial analysis aims to generate a reasoning program to calculate the correct answer for a given question. Previous work followed a retriever-generator framework, where the retriever selects key facts from a long-form document, and the generator generates a reasoning program based on retrieved facts. However, they treated all facts equally without considering the different contributions of facts with and without numbers. Meanwhile, the program consistency were ignored under supervised training, resulting in lower training accuracy and diversity. To solve these problems, we proposed APOLLO to improve the long-form numerical reasoning framework. For the retriever, we adopt a number-aware negative sampling strategy to enable the retriever to be more discriminative on key numerical facts. For the generator, we design consistency-based reinforcement learning and target program augmentation strategy base
    
[^218]: 数据中心人工智能（DCAI）的原则

    The Principles of Data-Centric AI (DCAI)

    [https://arxiv.org/abs/2211.14611](https://arxiv.org/abs/2211.14611)

    数据中心人工智能（DCAI）强调数据的质量和动态性，提出六项指导原则，并为人工智能系统的未来发展方向指明了道路。

    

    数据对于人工智能（AI）系统的学习至关重要。然而，迄今为止，这些系统主要以模型为中心，以数据质量为代价。数据质量问题影响了AI系统的性能，尤其是在下游部署和实际应用中。作为一个新兴概念，数据中心人工智能（DCAI）通过迭代和系统化的方法将数据、其质量和动态性置于AI系统考虑的前沿。作为首次概述之一，本文汇集了数据中心的视角和概念，勾勒了DCAI的基础。它特别为研究人员和从业者制定了六项指导原则，并为DCAI的未来发展提供了方向。

    arXiv:2211.14611v2 Announce Type: replace-cross  Abstract: Data is a crucial infrastructure to how artificial intelligence (AI) systems learn. However, these systems to date have been largely model-centric, putting a premium on the model at the expense of the data quality. Data quality issues beset the performance of AI systems, particularly in downstream deployments and in real-world applications. Data-centric AI (DCAI) as an emerging concept brings data, its quality and its dynamism to the forefront in considerations of AI systems through an iterative and systematic approach. As one of the first overviews, this article brings together data-centric perspectives and concepts to outline the foundations of DCAI. It specifically formulates six guiding principles for researchers and practitioners and gives direction for future advancement of DCAI.
    
[^219]: SATformer: 基于Transformer的UNSAT核心学习

    SATformer: Transformer-Based UNSAT Core Learning

    [https://arxiv.org/abs/2209.00953](https://arxiv.org/abs/2209.00953)

    SATformer通过引入基于Transformer的方法，采用对不可满足性进行建模的方式，以识别不可满足的子问题，得到了优于NeuroSAT的性能。

    

    本文介绍了SATformer，这是一种用于布尔可满足性（SAT）问题的新型基于Transformer的方法。 SATformer并非直接解决问题，而是从相反的方向入手，着重于不可满足性。具体来说，它通过模拟子句之间的相互作用来识别任何不可满足的子问题。我们利用图神经网络将子句转换为子句嵌入，并采用分层Transformer模型来理解子句之间的相关性。 SATformer通过多任务学习方法进行训练，使用单比特可满足性结果以及最小不可满足核心（MUC）作为子句监督来处理UNSAT问题。作为端到端学习的可满足性分类器，SATformer的性能显著超越了NeuroSAT。此外，我们将SATformer做出的子句预测集成到现代启发式SAT求解器中，并验证了我们的方法。

    arXiv:2209.00953v2 Announce Type: replace  Abstract: This paper introduces SATformer, a novel Transformer-based approach for the Boolean Satisfiability (SAT) problem. Rather than solving the problem directly, SATformer approaches the problem from the opposite direction by focusing on unsatisfiability. Specifically, it models clause interactions to identify any unsatisfiable sub-problems. Using a graph neural network, we convert clauses into clause embeddings and employ a hierarchical Transformer-based model to understand clause correlation. SATformer is trained through a multi-task learning approach, using the single-bit satisfiability result and the minimal unsatisfiable core (MUC) for UNSAT problems as clause supervision. As an end-to-end learning-based satisfiability classifier, the performance of SATformer surpasses that of NeuroSAT significantly. Furthermore, we integrate the clause predictions made by SATformer into modern heuristic-based SAT solvers and validate our approach wit
    
[^220]: 使用单调变分不等式训练神经网络的另一种方法

    An alternative approach to train neural networks using monotone variational inequality

    [https://arxiv.org/abs/2202.08876](https://arxiv.org/abs/2202.08876)

    通过单调变分不等式，提出了一种高效的神经网络训练方法，可以快速收敛并在特定情况下提供保证。

    

    我们提出了一种使用单调矢量场的替代方法来训练神经网络，这个想法受到Juditsky和Nemirovski的开创性工作的启发，最初是为了解决广义线性模型（GLM）的参数估计问题，通过将原始非凸问题简化为解决单调变分不等式（VI）的凸问题。我们的方法导致了计算效率高且在某些特殊情况下收敛快速并提供了保证，例如训练单层神经网络或微调预训练模型的最后一层。我们的方法可以用于更高效地微调预训练模型的同时冻结底层，这是部署许多机器学习模型（如大型语言模型LLM）的重要步骤。我们证明了它在训练全连接（FC）神经网络、图神经网络方面的适用性。

    arXiv:2202.08876v4 Announce Type: replace-cross  Abstract: We propose an alternative approach to neural network training using the monotone vector field, an idea inspired by the seminal work of Juditsky and Nemirovski [Juditsky & Nemirovsky, 2019] developed originally to solve parameter estimation problems for generalized linear models (GLM) by reducing the original non-convex problem to a convex problem of solving a monotone variational inequality (VI). Our approach leads to computationally efficient procedures that converge fast and offer guarantee in some special cases, such as training a single-layer neural network or fine-tuning the last layer of the pre-trained model. Our approach can be used for more efficient fine-tuning of a pre-trained model while freezing the bottom layers, an essential step for deploying many machine learning models such as large language models (LLM). We demonstrate its applicability in training fully-connected (FC) neural networks, graph neural networks (
    
[^221]: 使用切比雪夫逼近的图卷积神经网络，重新审视

    Convolutional Neural Networks on Graphs with Chebyshev Approximation, Revisited

    [https://arxiv.org/abs/2202.03580](https://arxiv.org/abs/2202.03580)

    重新审视了使用切比雪夫多项式逼近谱图卷积的问题，发现ChebNet性能较差主要是由于其学习到的非法系数近似解析滤波器函数

    

    在图学习中，设计谱卷积网络是一个具有挑战性的问题。ChebNet是早期尝试之一，它使用切比雪夫多项式近似谱图卷积。GCN简化了ChebNet，仅利用前两个切比雪夫多项式，同时在真实世界数据集上性能优于其。GPR-GNN和BernNet表明，单项式和伯恩斯坦基也在学习谱图卷积方面优于切比雪夫基。这样的结论在逼近理论领域是反直觉的，逼近函数时切比雪夫多项式实现了最佳收敛速率。

    arXiv:2202.03580v5 Announce Type: replace-cross  Abstract: Designing spectral convolutional networks is a challenging problem in graph learning. ChebNet, one of the early attempts, approximates the spectral graph convolutions using Chebyshev polynomials. GCN simplifies ChebNet by utilizing only the first two Chebyshev polynomials while still outperforming it on real-world datasets. GPR-GNN and BernNet demonstrate that the Monomial and Bernstein bases also outperform the Chebyshev basis in terms of learning the spectral graph convolutions. Such conclusions are counter-intuitive in the field of approximation theory, where it is established that the Chebyshev polynomial achieves the optimum convergent rate for approximating a function.   In this paper, we revisit the problem of approximating the spectral graph convolutions with Chebyshev polynomials. We show that ChebNet's inferior performance is primarily due to illegal coefficients learnt by ChebNet approximating analytic filter functio
    
[^222]: 多源注解的不相交对比回归学习

    Disjoint Contrastive Regression Learning for Multi-Sourced Annotations

    [https://arxiv.org/abs/2112.15411](https://arxiv.org/abs/2112.15411)

    提出了用于不相交注解的对比回归框架，解决了多个标注者间的不一致性和偏见问题。

    

    大规模数据集对深度学习模型的发展至关重要。这些数据集通常需要大量注解工作，这是非常耗时且昂贵的。为了加速注解过程，可以雇用多个标注者为数据的不同子集进行标记。然而，不同标注者之间的不一致性和偏见对模型训练有害，特别是对于定性和主观任务。为了解决这一挑战，在本文中，我们提出了一种新颖的对比回归框架来解决不相交注解问题，其中每个样本仅由一个标注者标记，多个标注者在数据的不相交子集上工作。为了考虑标注者内一致性和标注者间不一致性，我们采用了两种策略。首先，应用基于对比的损失来学习同一标注者的不同样本之间的相对排序。

    arXiv:2112.15411v2 Announce Type: replace  Abstract: Large-scale datasets are important for the development of deep learning models. Such datasets usually require a heavy workload of annotations, which are extremely time-consuming and expensive. To accelerate the annotation procedure, multiple annotators may be employed to label different subsets of the data. However, the inconsistency and bias among different annotators are harmful to the model training, especially for qualitative and subjective tasks.To address this challenge, in this paper, we propose a novel contrastive regression framework to address the disjoint annotations problem, where each sample is labeled by only one annotator and multiple annotators work on disjoint subsets of the data. To take account of both the intra-annotator consistency and inter-annotator inconsistency, two strategies are employed.Firstly, a contrastive-based loss is applied to learn the relative ranking among different samples of the same annotator,
    
[^223]: 面向域感知的持续零样本学习

    Domain-Aware Continual Zero-Shot Learning

    [https://arxiv.org/abs/2112.12989](https://arxiv.org/abs/2112.12989)

    DACZSL提出了面向域感知的持续零样本学习任务，引入了域不变网络(DIN)，不断学习全局共享网络用于领域不变和任务不变特征，以及为每个任务提供专门的私有网络用于特定任务的特征

    

    现代视觉系统在自然科学研究中具有广泛的潜在应用，如帮助发现物种，监测野生动物等。然而，真实世界中的视觉任务可能会经历环境条件的变化，导致捕捉图像呈现方式的变化。为了解决这个问题，我们引入了面向域感知的持续零样本学习（DACZSL），这是一个持续识别不断变化域中未见类别图像的任务。因此，我们提出了一个域不变网络（DIN），用于学习适应变化域的分解特征和改进的文本表示未见类别。DIN不断学习全局共享网络以获取领域不变和任务不变特征，并为每个任务提供专门的私有网络用于特定任务的特征。

    arXiv:2112.12989v3 Announce Type: replace-cross  Abstract: Modern visual systems have a wide range of potential applications in vision tasks for natural science research, such as aiding in species discovery, monitoring animals in the wild, and so on. However, real-world vision tasks may experience changes in environmental conditions, leading to shifts in how captured images are presented. To address this issue, we introduce Domain-Aware Continual Zero-Shot Learning (DACZSL), a task to recognize images of unseen categories in continuously changing domains. Accordingly, we propose a Domain-Invariant Network (DIN) to learn factorized features for shifting domains and improved textual representation for unseen classes. DIN continually learns a global shared network for domain-invariant and task-invariant features, and per-task private networks for task-specific features. Furthermore, we enhance the dual network with class-wise learnable prompts to improve class-level text representation, t
    
[^224]: 一种用于在高维观察数据中稳健评估治疗效果的两阶段特征选择方法

    A Two-Stage Feature Selection Approach for Robust Evaluation of Treatment Effects in High-Dimensional Observational Data

    [https://arxiv.org/abs/2111.13800](https://arxiv.org/abs/2111.13800)

    提出了一种名为Outcome Adaptive Elastic Net（OAENet）的两阶段特征选择技术，用于在高维观察数据中进行稳健的因果推断决策。

    

    随机对照试验（RCT）被认为是评估任何干预或治疗效果的黄金标准。然而，由于伦理、经济和法律考虑，其可行性经常受阻，使得观察数据成为绘制因果结论的宝贵替代方法。然而，由于医疗观察数据具有高维性，这带来了困难挑战，需要仔细考虑以确保无偏、可靠和稳健的因果推断。为了克服这一挑战，本研究提出了一种名为Outcome Adaptive Elastic Net（OAENet）的新颖两阶段特征选择技术，专门设计用于使用匹配技术做出稳健的因果推断决策。OAENet相对于现有方法具有几个关键优势：在相关和高维数据上表现优越于现有方法，并具有选择特定变量集的能力。

    arXiv:2111.13800v2 Announce Type: replace-cross  Abstract: A Randomized Control Trial (RCT) is considered as the gold standard for evaluating the effect of any intervention or treatment. However, its feasibility is often hindered by ethical, economical, and legal considerations, making observational data a valuable alternative for drawing causal conclusions. Nevertheless, healthcare observational data presents a difficult challenge due to its high dimensionality, requiring careful consideration to ensure unbiased, reliable, and robust causal inferences. To overcome this challenge, in this study, we propose a novel two-stage feature selection technique called, Outcome Adaptive Elastic Net (OAENet), explicitly designed for making robust causal inference decisions using matching techniques. OAENet offers several key advantages over existing methods: superior performance on correlated and high-dimensional data compared to the existing methods and the ability to select specific sets of vari
    
[^225]: 高效GPU实现随机化SVD及其应用

    Efficient GPU implementation of randomized SVD and its applications

    [https://arxiv.org/abs/2110.03423](https://arxiv.org/abs/2110.03423)

    本研究利用现代GPU上的高效处理操作，将随机化分解问题重新制定，结合快速矩阵乘法操作和随机数发生器，充分发挥GPU并行处理的潜力，减少计算矩阵分解的计算负担。

    

    矩阵分解在机器学习中应用广泛，包括在降维、数据压缩和深度学习算法中的应用。传统的矩阵分解方法具有多项式复杂度，这显著增加了它们的计算成本和时间。在这项工作中，我们利用现代图形处理单元（GPU）上可并行运行的高效处理操作，这是深度学习等领域中广泛使用的计算架构，以减少计算矩阵分解的计算负担。具体而言，我们重新制定了随机化分解问题，以包含快速矩阵乘法操作（BLAS-3）作为基本构建模块。我们展示该表述与快速随机数发生器相结合，可以充分利用GPU中实现的并行处理潜力。我们的广泛评估确认了这种方法优于co

    arXiv:2110.03423v2 Announce Type: replace  Abstract: Matrix decompositions are ubiquitous in machine learning, including applications in dimensionality reduction, data compression and deep learning algorithms. Typical solutions for matrix decompositions have polynomial complexity which significantly increases their computational cost and time. In this work, we leverage efficient processing operations that can be run in parallel on modern Graphical Processing Units (GPUs), predominant computing architecture used e.g. in deep learning, to reduce the computational burden of computing matrix decompositions. More specifically, we reformulate the randomized decomposition problem to incorporate fast matrix multiplication operations (BLAS-3) as building blocks. We show that this formulation, combined with fast random number generators, allows to fully exploit the potential of parallel processing implemented in GPUs. Our extensive evaluation confirms the superiority of this approach over the co
    
[^226]: 使用数据进行学习和决策：最优公式与相变

    Learning and Decision-Making with Data: Optimal Formulations and Phase Transitions

    [https://arxiv.org/abs/2109.06911](https://arxiv.org/abs/2109.06911)

    本论文提出了一种新的方法，通过首先定义一个衡量基于数据公式质量的标尺，然后寻找最优公式，使其在保证样本外性能的同时，更加接近真实成本。

    

    我们研究了当只有历史数据可用时设计最优学习和决策公式的问题。之前的工作通常会致力于某一类基于数据的公式，然后试图建立样本外性能保证。我们这里采取了相反的方法。我们首先定义一个合理的标尺来衡量任何基于数据的公式的质量，然后寻求找到一个最优的这样的公式。不正式地说，任何基于数据的公式可以被视为在保证样本外性能水平的同时平衡估计成本与实际成本的接近度的度量。在给定一个可接受的样本外性能水平后，我们显式构造了一个基于数据的公式，该公式在与同样具有样本外性能的其他公式相比是统一更接近真实成本的。我们展示了三种不同的样本外执行

    arXiv:2109.06911v3 Announce Type: replace-cross  Abstract: We study the problem of designing optimal learning and decision-making formulations when only historical data is available. Prior work typically commits to a particular class of data-driven formulation and subsequently tries to establish out-of-sample performance guarantees. We take here the opposite approach. We define first a sensible yard stick with which to measure the quality of any data-driven formulation and subsequently seek to find an optimal such formulation. Informally, any data-driven formulation can be seen to balance a measure of proximity of the estimated cost to the actual cost while guaranteeing a level of out-of-sample performance. Given an acceptable level of out-of-sample performance, we construct explicitly a data-driven formulation that is uniformly closer to the true cost than any other formulation enjoying the same out-of-sample performance. We show the existence of three distinct out-of-sample performan
    
[^227]: CSC-Unet：一种基于卷积稀疏编码策略的神经网络用于语义分割

    CSC-Unet: A Novel Convolutional Sparse Coding Strategy Based Neural Network for Semantic Segmentation

    [https://arxiv.org/abs/2108.00408](https://arxiv.org/abs/2108.00408)

    通过将常用的卷积操作重新构造为多层卷积稀疏编码块，提出了一种可能用于显著提高语义分割模型性能的新策略。

    

    准确执行语义分割是一项具有挑战性的任务，这是由于真实图像场景的复杂性。许多基于传统深度学习的语义分割方法未能充分捕捉图像的语义和外观信息，这限制了它们对各种应用场景的普适性和鲁棒性。在本文中，我们提出了一种新颖的策略，将常用的卷积操作重新构造为多层卷积稀疏编码块，以缓解前述缺陷。该策略可能用于显著提高任何涉及卷积操作的语义分割模型的分割性能。为了证明我们想法的有效性，我们选择了广泛使用的U-Net模型进行演示，并基于U-Net设计了CSC-Unet模型系列。通过广泛的分析和实验证据，我们提供了可信的证据显示

    arXiv:2108.00408v2 Announce Type: replace-cross  Abstract: It is a challenging task to accurately perform semantic segmentation due to the complexity of real picture scenes. Many semantic segmentation methods based on traditional deep learning insufficiently captured the semantic and appearance information of images, which put limit on their generality and robustness for various application scenes. In this paper, we proposed a novel strategy that reformulated the popularly-used convolution operation to multi-layer convolutional sparse coding block to ease the aforementioned deficiency. This strategy can be possibly used to significantly improve the segmentation performance of any semantic segmentation model that involves convolutional operations. To prove the effectiveness of our idea, we chose the widely-used U-Net model for the demonstration purpose, and we designed CSC-Unet model series based on U-Net. Through extensive analysis and experiments, we provided credible evidence showing
    
[^228]: 使用路径重加权定制图神经网络

    Customizing Graph Neural Networks using Path Reweighting

    [https://arxiv.org/abs/2106.10866](https://arxiv.org/abs/2106.10866)

    使用路径重加权的定制图神经网络CustomGNN可以自动学习特定下游任务的高层语义，突出与语义相关的路径，并过滤掉任务无关的噪音，避免传统GNN中的过度平滑、鲁棒性差和过拟合问题。

    

    图神经网络（GNNs）已广泛用于挖掘图结构数据，表现出卓越的性能。然而，传统GNNs并未区分不同下游任务，因此它们嵌入的嵌入向量并非总是有效。本文以图中的路径为灵感，设计了一种新颖的GNN解决方案，即带有路径重加权的定制图神经网络（简称CustomGNN）。具体而言，所提出的CustomGNN可以自动学习特定下游任务的高层语义，突出与语义相关的路径，并过滤掉图中与任务无关的噪音。此外，我们通过实验证明了CustomGNN学习的语义以及其避免传统GNN中存在的过度平滑、鲁棒性差和过拟合等三个固有问题的能力。

    arXiv:2106.10866v3 Announce Type: replace  Abstract: Graph Neural Networks (GNNs) have been extensively used for mining graph-structured data with impressive performance. However, because these traditional GNNs do not distinguish among various downstream tasks, embeddings embedded by them are not always effective. Intuitively, paths in a graph imply different semantics for different downstream tasks. Inspired by this, we design a novel GNN solution, namely Customized Graph Neural Network with Path Reweighting (CustomGNN for short). Specifically, the proposed CustomGNN can automatically learn the high-level semantics for specific downstream tasks to highlight semantically relevant paths as well to filter out task-irrelevant noises in a graph. Furthermore, we empirically analyze the semantics learned by CustomGNN and demonstrate its ability to avoid the three inherent problems in traditional GNNs, i.e., over-smoothing, poor robustness, and overfitting. In experiments with the node classi
    
[^229]: MgSvF：多粒度慢vs.快框架用于少样本类别增量学习

    MgSvF: Multi-Grained Slow vs. Fast Framework for Few-Shot Class-Incremental Learning

    [https://arxiv.org/abs/2006.15524](https://arxiv.org/abs/2006.15524)

    提出了一种多粒度SvF学习策略，通过设计频率感知正则化和特征空间组合操作来平衡保留旧知识与适应新知识，以解决少样本类别增量学习中的"慢vs.快"困境

    

    作为一个具有挑战性的问题，少样本类别增量学习(FSCIL)不断学习一系列任务，面临着在忘记旧知识和快速适应新知识之间的困境。本文关注这种“慢vs.快”(SvF)困境，确定哪些知识组件应以慢速或快速方式更新，从而平衡保留旧知识和适应新知识。我们提出了一种多粒度SvF学习策略，以应对来自两个不同粒度的SvF困境：内部空间(在相同特征空间内)和外部空间(在两个不同特征空间之间)。所提出的策略设计了一种新颖的频率感知正则化，以提升内部空间SvF能力，并同时开发了一种新的特征空间组合操作，以增强外部空间SvF学习性能。通过多粒度SvF学习策略，我们的方法获得了哪

    arXiv:2006.15524v4 Announce Type: replace-cross  Abstract: As a challenging problem, few-shot class-incremental learning (FSCIL) continually learns a sequence of tasks, confronting the dilemma between slow forgetting of old knowledge and fast adaptation to new knowledge. In this paper, we concentrate on this "slow vs. fast" (SvF) dilemma to determine which knowledge components to be updated in a slow fashion or a fast fashion, and thereby balance old-knowledge preservation and new-knowledge adaptation. We propose a multi-grained SvF learning strategy to cope with the SvF dilemma from two different grains: intra-space (within the same feature space) and inter-space (between two different feature spaces). The proposed strategy designs a novel frequency-aware regularization to boost the intra-space SvF capability, and meanwhile develops a new feature space composition operation to enhance the inter-space SvF learning performance. With the multi-grained SvF learning strategy, our method ou
    
[^230]: 不断演变的高斯过程引导学习

    Epoch-evolving Gaussian Process Guided Learning

    [https://arxiv.org/abs/2006.14347](https://arxiv.org/abs/2006.14347)

    提出了一种名为GPGL的epoch-evolving Gaussian Process Guided Learning学习方案，通过上下文标签和地面真实标签指导模型参数更新，进一步推广并应用于当前深度模型，在主流数据集上表现显著优于现有基于批次的最先进模型

    

    在这篇论文中，我们提出了一种名为不断演变的高斯过程引导学习（GPGL）的新颖学习方案，旨在描述批级分布与全局数据分布之间的相关信息。这种相关信息被编码为上下文标签，需要每个时期进行更新。在上下文标签和地面真实标签的指导下，GPGL方案通过使用三角一致性损失来更新模型参数，从而提供更高效的优化。此外，我们的GPGL方案可以进一步推广并自然应用于当前的深度模型，在主流数据集（CIFAR-10、CIFAR-100和Tiny-ImageNet）上显著优于现有基于批次的最先进模型。

    arXiv:2006.14347v2 Announce Type: replace  Abstract: In this paper, we propose a novel learning scheme called epoch-evolving Gaussian Process Guided Learning (GPGL), which aims at characterizing the correlation information between the batch-level distribution and the global data distribution. Such correlation information is encoded as context labels and needs renewal every epoch. With the guidance of the context label and ground truth label, GPGL scheme provides a more efficient optimization through updating the model parameters with a triangle consistency loss. Furthermore, our GPGL scheme can be further generalized and naturally applied to the current deep models, outperforming the existing batch-based state-of-the-art models on mainstream datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) remarkably.
    
[^231]: 小阈值间隙下的好臂识别算法: lil'HDoC

    lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold Gap. (arXiv:2401.15879v1 [cs.LG])

    [http://arxiv.org/abs/2401.15879](http://arxiv.org/abs/2401.15879)

    本文提出了一种名为lil'HDoC的算法，用于解决小阈值间隙下的好臂识别问题。实验证明该算法在样本效率上优于现有算法。

    

    好臂识别（GAI）是一个纯探索性的赌博机问题，在这个问题中，一个单独的学习器会在确定一个臂是好臂时立即输出该臂。好臂被定义为期望回报大于等于给定阈值的臂。本文聚焦于小阈值间隙下的GAI问题，该间隙指的是臂的期望回报与给定阈值之间的距离。我们提出了一种名为lil'HDoC的新算法，显著改善了HDoC算法的总样本复杂度。我们证明了在小阈值间隙下，lil'HDoC算法输出的第一个λ臂的样本复杂度与原始HDoC算法相比仅有微小的差异。大量实验证明我们的算法在合成数据集和真实世界数据集上表现优于最先进的算法。

    Good arm identification (GAI) is a pure-exploration bandit problem in which a single learner outputs an arm as soon as it is identified as a good arm. A good arm is defined as an arm with an expected reward greater than or equal to a given threshold. This paper focuses on the GAI problem under a small threshold gap, which refers to the distance between the expected rewards of arms and the given threshold. We propose a new algorithm called lil'HDoC to significantly improve the total sample complexity of the HDoC algorithm. We demonstrate that the sample complexity of the first $\lambda$ output arm in lil'HDoC is bounded by the original HDoC algorithm, except for one negligible term, when the distance between the expected reward and threshold is small. Extensive experiments confirm that our algorithm outperforms the state-of-the-art algorithms in both synthetic and real-world datasets.
    
[^232]: 缩小TD学习和监督学习之间的差距--从一般化的角度来看

    Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View. (arXiv:2401.11237v1 [cs.LG])

    [http://arxiv.org/abs/2401.11237](http://arxiv.org/abs/2401.11237)

    这篇论文研究了强化学习方法在训练过程中将不同经验片段组合起来解决未见过的任务的属性，并通过分析发现这种组合属性与组合泛化有关。

    

    一些强化学习算法可以将经验片段组合起来解决训练过程中从未见过的任务。这种经常被追求的特性是基于动态规划的强化学习方法与基于监督学习的强化学习方法之间的几种区别之一。然而，某些基于现成监督学习算法的强化学习方法在没有明确的组合机制的情况下，也能取得出色的结果；目前尚不清楚这些方法是否放弃了这种重要的组合特性。本文研究了即将达到目标状态和达到目标回报值的问题，我们的主要结果是展示了组合特性对应了一种组合泛化：在(state, goal)对的分布上进行训练后，希望在训练数据中没有同时出现的(state, goal)对上进行评估。我们的分析表明，这种组合泛化与i.i.d.泛化是不同的。这种连接将组合特性与泛化联系在一起。

    Some reinforcement learning (RL) algorithms can stitch pieces of experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which RL methods based on dynamic-programming differ from RL methods based on supervised-learning (SL). Yet, certain RL methods based on off-the-shelf SL algorithms achieve excellent results without an explicit mechanism for stitching; it remains unclear whether those methods forgo this important stitching property. This paper studies this question for the problems of achieving a target goal state and achieving a target return value. Our main result is to show that the stitching property corresponds to a form of combinatorial generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data. Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching 
    
[^233]: 级联强化学习

    Cascading Reinforcement Learning. (arXiv:2401.08961v1 [cs.LG])

    [http://arxiv.org/abs/2401.08961](http://arxiv.org/abs/2401.08961)

    本文提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响，在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。

    

    最近几年，级联赌博机在推荐系统和在线广告中应用广泛。在级联赌博机模型中，每个时刻，一个代理人从一组具有未知吸引概率的项目中推荐一个有序的项目子集（称为项目列表）。然后，用户检查列表，并点击第一个有吸引力的项目（如果有的话），之后，代理收到一个奖励。代理的目标是最大化预期的累积奖励。然而，以往的级联赌博机文献忽略了用户状态（例如历史行为）对推荐的影响以及会话进行过程中状态的变化。受此事实的启发，我们提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响。在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。

    Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with large attraction probabilities but also leading to good successor states. This im
    
[^234]: Scissorhands: 通过网络连接敏感性在数据影响中进行数据擦除

    Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks. (arXiv:2401.06187v1 [cs.LG])

    [http://arxiv.org/abs/2401.06187](http://arxiv.org/abs/2401.06187)

    Scissorhands 是一种新的机器取消学习方法，通过连接敏感性识别与遗忘数据相关的最相关参数，并通过重新训练修剪的模型来擦除数据影响。

    

    机器取消学习已成为一项重要任务，旨在擦除训练模型中的数据影响。它符合最新的数据监管标准，增强了机器学习应用的隐私和安全性。大多数现有的机器取消学习方法表现良好，但通常需要访问其余数据的全部内容，在某些情况下可能不可行。在这项工作中，我们提出了一种新的机器取消学习方法“Scissorhands”，它只使用训练数据的子集来有效运行。初始阶段，Scissorhands通过连接敏感性在给定模型中识别与遗忘数据相关的最相关参数。该过程通过重新初始化这些参数中具有最大影响力的前k%的最相关参数，从而产生一个用于擦除遗忘数据影响的修剪模型。随后，Scissorhands通过最小-最大优化过程对修剪的模型进行再训练，寻找保留信息的参数。

    Machine unlearning has become a pivotal task to erase the influence of data from a trained model. It adheres to recent data regulation standards and enhances the privacy and security of machine learning applications. Most existing machine unlearning methods perform well, however, they typically necessitate access to the entirety of the remaining data, which might not be feasible in certain scenarios. In this work, we present a new machine unlearning approach Scissorhands, which operates effectively with only a subset of the training data. Initially, Scissorhands identifies the most pertinent parameters in the given model relative to the forgetting data via connection sensitivity. This process involves reinitializing the most influential top-$k$ percent of these parameters, resulting in a trimmed model for erasing the influence of the forgetting data. Subsequently, Scissorhands retrains the trimmed model through a min-max optimization process, seeking parameters that preserve informatio
    
[^235]: RudolfV：一种由病理学家为病理学家构建的基础模型

    RudolfV: A Foundation Model by Pathologists for Pathologists. (arXiv:2401.04079v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2401.04079](http://arxiv.org/abs/2401.04079)

    本文提出了一种由病理学家为病理学家构建的基础模型，通过数据整理和融入病理学领域知识，扩展了数字病理学全玻片图像基础模型，解决了人工智能在处理罕见疾病方面的挑战。

    

    组织病理学在临床医学和生物医学研究中起着核心作用。虽然人工智能在许多病理学任务上显示出有希望的结果，但在泛化和处理训练数据稀缺的罕见疾病方面仍然是一个挑战。在学习来自有限标记数据之前，从无标记数据中提取知识到基础模型可以解决这些挑战。在这项工作中，我们通过半自动数据整理和融入病理学领域知识，扩展了数字病理学全玻片图像基础模型的最新技术。具体而言，我们结合计算和病理学领域知识(1)整理了一个多样化的数据集，包括10.3万个玻片图像对应的7.5亿个图像块，涵盖了来自欧美不同修复、染色和扫描协议以及不同指示和实验室的数据，(2)用于对语义上相似的玻片和组织块进行分组。

    Histopathology plays a central role in clinical medicine and biomedical research. While artificial intelligence shows promising results on many pathological tasks, generalization and dealing with rare diseases, where training data is scarce, remains a challenge. Distilling knowledge from unlabeled data into a foundation model before learning from, potentially limited, labeled data provides a viable path to address these challenges. In this work, we extend the state of the art of foundation models for digital pathology whole slide images by semi-automated data curation and incorporating pathologist domain knowledge. Specifically, we combine computational and pathologist domain knowledge (1) to curate a diverse dataset of 103k slides corresponding to 750 million image patches covering data from different fixation, staining, and scanning protocols as well as data from different indications and labs across the EU and US, (2) for grouping semantically similar slides and tissue patches, and 
    
[^236]: 超越遗憾：贝叶斯优化的几何度量

    Beyond Regrets: Geometric Metrics for Bayesian Optimization. (arXiv:2401.01981v1 [cs.LG])

    [http://arxiv.org/abs/2401.01981](http://arxiv.org/abs/2401.01981)

    本论文提出了四个新的几何度量，可以比较贝叶斯优化算法在考虑查询点和全局最优解的几何特性时的性能。

    

    贝叶斯优化是一种针对黑盒子目标函数的原则性优化策略。它在科学发现和实验设计等各种实际应用中的效果得到了证明。通常，贝叶斯优化的性能是通过基于遗憾的度量来评估的，如瞬时遗憾、简单遗憾和累积遗憾。这些度量仅依赖于函数评估，因此它们不考虑查询点和全局解之间的几何关系，也不考虑查询点本身。值得注意的是，它们不能区分是否成功找到了多个全局解。此外，它们也不能评估贝叶斯优化在给定搜索空间中利用和探索的能力。为了解决这些问题，我们提出了四个新的几何度量，即精确度、召回率、平均度和平均距离。这些度量使我们能够比较考虑查询点和全局最优解的几何特性的贝叶斯优化算法。

    Bayesian optimization is a principled optimization strategy for a black-box objective function. It shows its effectiveness in a wide variety of real-world applications such as scientific discovery and experimental design. In general, the performance of Bayesian optimization is assessed by regret-based metrics such as instantaneous, simple, and cumulative regrets. These metrics only rely on function evaluations, so that they do not consider geometric relationships between query points and global solutions, or query points themselves. Notably, they cannot discriminate if multiple global solutions are successfully found. Moreover, they do not evaluate Bayesian optimization's abilities to exploit and explore a search space given. To tackle these issues, we propose four new geometric metrics, i.e., precision, recall, average degree, and average distance. These metrics allow us to compare Bayesian optimization algorithms considering the geometry of both query points and global optima, or que
    
[^237]: EyePreserve: 保持身份的虹膜合成

    EyePreserve: Identity-Preserving Iris Synthesis. (arXiv:2312.12028v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.12028](http://arxiv.org/abs/2312.12028)

    本论文提出了一种完全数据驱动的、保持身份的、瞳孔尺寸变化的虹膜图像合成方法，能够合成不同瞳孔尺寸的虹膜图像，代表不存在的身份，并能够在保持身份的同时进行非线性纹理变形。

    

    在广泛的瞳孔尺寸范围内保持身份的同身份生物特征虹膜图像的合成是复杂的，因为它涉及到虹膜肌肉收缩机制，需要将虹膜非线性纹理变形模型嵌入到合成流程中。本论文提出了一种完全数据驱动的、保持身份的、瞳孔尺寸变化的虹膜图像合成方法。这种方法能够合成具有不同瞳孔尺寸的虹膜图像，代表不存在的身份，并能够在给定目标虹膜图像的分割掩膜下非线性地变形现有主体的虹膜图像纹理。虹膜识别实验表明，所提出的变形模型不仅在改变瞳孔尺寸时保持身份，而且在瞳孔尺寸有显著差异的同身份虹膜样本之间提供更好的相似度，与最先进的线性方法相比。

    Synthesis of same-identity biometric iris images, both for existing and non-existing identities while preserving the identity across a wide range of pupil sizes, is complex due to intricate iris muscle constriction mechanism, requiring a precise model of iris non-linear texture deformations to be embedded into the synthesis pipeline. This paper presents the first method of fully data-driven, identity-preserving, pupil size-varying s ynthesis of iris images. This approach is capable of synthesizing images of irises with different pupil sizes representing non-existing identities as well as non-linearly deforming the texture of iris images of existing subjects given the segmentation mask of the target iris image. Iris recognition experiments suggest that the proposed deformation model not only preserves the identity when changing the pupil size but offers better similarity between same-identity iris samples with significant differences in pupil size, compared to state-of-the-art linear an
    
[^238]: LOTUS：通过无监督技能发现的持续模仿学习，用于机器人操作

    LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery. (arXiv:2311.02058v1 [cs.RO])

    [http://arxiv.org/abs/2311.02058](http://arxiv.org/abs/2311.02058)

    LOTUS是一种持续模仿学习算法，通过无监督技能发现，使得机器人能够在其整个寿命中持续学习解决新的操作任务。该算法通过构建技能库，并使用元控制器灵活组合技能来提高成功率，在实验中表现出优越的知识传递能力。

    

    我们介绍了一种名为LOTUS的持续模仿学习算法，它使得物理机器人能够在其整个寿命中持续而高效地学习解决新的操作任务。LOTUS的核心思想是通过一系列新任务的少量人类演示构建一个不断增长的技能库。LOTUS首先使用开放词汇视觉模型进行持续技能发现过程，该模型从未分段的演示中提取重复出现的技能模式。持续技能发现更新现有技能以避免对以前任务的灾难性遗忘，并添加新技能以解决新任务。LOTUS训练一个元控制器，在终身学习过程中灵活地组合各种技能来解决基于视觉的操作任务。我们的综合实验证明，与先前方法相比，LOTUS在成功率上超过了现有技术基线方法11％以上，显示了其优越的知识传递能力。

    We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of human demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unsegmented demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to solve novel tasks. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in success rate, showing its superior knowledge transfer ability compared to prior methods. More result
    
[^239]: 利用可区分插入/删除指标感知正则化进行解释性训练

    Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers. (arXiv:2310.12553v1 [cs.LG])

    [http://arxiv.org/abs/2310.12553](http://arxiv.org/abs/2310.12553)

    提出一种插入/删除指标感知的基于解释的优化(ID-ExpO)方法，通过优化可区分的预测器来提高解释的插入和删除得分，并保持预测准确性。实验结果表明，ID-ExpO能够使流行的事后解释器产生更忠实的解释。

    

    复杂机器学习预测器的解释质量通常使用插入和删除指标进行衡量，这些指标评估解释的忠实度，即解释正确地反映了预测器的行为程度。为了提高忠实度，我们提出了插入/删除指标感知的基于解释的优化（ID-ExpO），该优化能够改善解释的插入和删除得分，同时保持其预测准确性。由于原始的插入和删除指标对于解释来说是不可区分的，并且无法直接进行基于梯度的优化，我们扩展了这些指标以使其可区分，并将其用于形式化插入和删除指标的正则化。在图像和表格数据集上的实验结果表明，使用ID-ExpO进行微调的基于深度神经网络的预测器能够使流行的事后解释器产生更忠实的解释。

    The quality of explanations for the predictions of complex machine learning predictors is often measured using insertion and deletion metrics, which assess the faithfulness of the explanations, i.e., how correctly the explanations reflect the predictor's behavior. To improve the faithfulness, we propose insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which optimizes differentiable predictors to improve both insertion and deletion scores of the explanations while keeping their predictive accuracy. Since the original insertion and deletion metrics are indifferentiable with respect to the explanations and directly unavailable for gradient-based optimization, we extend the metrics to be differentiable and use them to formalize insertion and deletion metric-based regularizers. The experimental results on image and tabular datasets show that the deep neural networks-based predictors fine-tuned using ID-ExpO enable popular post-hoc explainers to produce more faithful
    
[^240]: 多视图变分自动编码器在非目标代谢组学中缺失值填充中的应用

    Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics. (arXiv:2310.07990v1 [q-bio.GN])

    [http://arxiv.org/abs/2310.07990](http://arxiv.org/abs/2310.07990)

    本文提出了一种新的方法，利用多视图变分自动编码器来填充非目标代谢组学中的缺失值，该方法利用了全基因组测序数据和参考代谢物的信息，可以有效地根据基因组信息填充缺失的代谢组学值。

    

    背景：在基于质谱的代谢组学中，缺失数据是一个常见的挑战，可能导致偏倚和不完整的分析。将全基因组测序（WGS）数据与代谢组学数据整合起来，已经成为增强代谢组学研究中数据填充准确性的一种有前景的方法。方法：在本研究中，我们提出了一种新的方法，利用来自WGS数据和参考代谢物的信息来填充未知代谢物。我们的方法利用多视图变分自动编码器共同对负担评分、多基因风险评分（PGS）和连锁不平衡（LD）删减的单核苷酸多态性（SNPs）进行特征提取和缺失代谢组学数据的填充。通过学习两种组学数据的潜在表示，我们的方法可以根据基因组信息有效地填充缺失的代谢组学值。结果：我们在具有缺失值和不完整数据的实验代谢组学数据集上评估了我们方法的性能。

    Background: Missing data is a common challenge in mass spectrometry-based metabolomics, which can lead to biased and incomplete analyses. The integration of whole-genome sequencing (WGS) data with metabolomics data has emerged as a promising approach to enhance the accuracy of data imputation in metabolomics studies. Method: In this study, we propose a novel method that leverages the information from WGS data and reference metabolites to impute unknown metabolites. Our approach utilizes a multi-view variational autoencoder to jointly model the burden score, polygenetic risk score (PGS), and linkage disequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature extraction and missing metabolomics data imputation. By learning the latent representations of both omics data, our method can effectively impute missing metabolomics values based on genomic information. Results: We evaluate the performance of our method on empirical metabolomics datasets with missing values and de
    
[^241]: 使用区块链实现安全的分散式学习

    Secure Decentralized Learning with Blockchain. (arXiv:2310.07079v1 [cs.CR])

    [http://arxiv.org/abs/2310.07079](http://arxiv.org/abs/2310.07079)

    本文提出了基于区块链的分散式联邦学习（BDFL），通过利用区块链进行分散式模型验证和审计，解决了分散式联邦学习中虚假模型和数据攻击的问题。

    

    联邦学习是一种在移动设备和物联网设备上的分布式机器学习范式，它保护数据隐私并优化通信效率。为了避免联邦学习中存在的单点故障问题，提出了分散式联邦学习（DFL），利用点对点通信进行模型聚合，这被认为是分布式个人设备上机器学习任务的有吸引力的解决方案。然而，这个过程容易受到共享虚假模型和数据的攻击者的攻击。如果存在一组恶意客户端，他们可能通过进行投毒攻击来损害模型的性能。此外，在DFL中，客户端通常缺乏激励来贡献他们的计算能力来进行模型训练。在本文中，我们提出了基于区块链的分散式联邦学习（BDFL），它利用区块链进行分散式模型验证和审计。BDFL包括一个审核委员会用于模型验证，一个...

    Federated Learning (FL) is a well-known paradigm of distributed machine learning on mobile and IoT devices, which preserves data privacy and optimizes communication efficiency. To avoid the single point of failure problem in FL, decentralized federated learning (DFL) has been proposed to use peer-to-peer communication for model aggregation, which has been considered an attractive solution for machine learning tasks on distributed personal devices. However, this process is vulnerable to attackers who share false models and data. If there exists a group of malicious clients, they might harm the performance of the model by carrying out a poisoning attack. In addition, in DFL, clients often lack the incentives to contribute their computing powers to do model training. In this paper, we proposed Blockchain-based Decentralized Federated Learning (BDFL), which leverages a blockchain for decentralized model verification and auditing. BDFL includes an auditor committee for model verification, a
    
[^242]: 退后一步：通过抽象唤起大型语言模型的推理能力

    Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])

    [http://arxiv.org/abs/2310.06117](http://arxiv.org/abs/2310.06117)

    本文提出了一种简单的提示技术，使得大型语言模型能够通过抽象获得高层概念和基本原理，并将其应用于推理路径中，从而显著提升模型在各种推理密集型任务上的表现。

    

    我们提出了一种称为“退后提示”的简单提示技术，使得大型语言模型能够通过从包含具体细节的实例中进行抽象，得出高层概念和基本原理。利用这些概念和原理来指导推理步骤，语言模型在正确推理路径上显著提升了能力。我们使用PaLM-2L模型进行了退后提示实验，在包括STEM、知识问答和多跳推理在内的各种具有挑战性的推理密集型任务上观察到了明显的性能提升。例如，在MMLU物理和化学任务上，退后提示可以将PaLM-2L的性能提升7%和11%，在TimeQA任务上提升27%，在MuSiQue任务上提升7%。

    We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
    
[^243]: 从某处到任何地方的学习：抓取技术

    Learning to Grasp: from Somewhere to Anywhere. (arXiv:2310.04349v1 [cs.RO])

    [http://arxiv.org/abs/2310.04349](http://arxiv.org/abs/2310.04349)

    本研究介绍了一个采用质量-多样性方法的流水线，用于将生成的抓取轨迹适应到新物体姿态。使用RGB-D数据流，该方法能够自动检测目标物体、预测其姿态，并生成可达到的抓取轨迹。

    

    机器人抓取仍然是一个部分解决的、多学科的问题，在其中数据驱动的技术发挥着越来越重要的作用。奖励的稀疏性使得自动生成抓取数据集变得具有挑战性，尤其是对于非传统形态或高度驱动的末端执行器。获得大规模数据集的大多数方法依赖于众多人工提供的演示或严重工程化的解决方案，这些方法很难扩展。最新的质量-多样性（QD）方法的进展研究了如何使用不同机器人形态学习特定姿势下的物体抓取。本研究介绍了一个将QD生成的轨迹适应到新物体姿态的流水线。使用RGB-D数据流，视觉流水线首先检测目标物体，预测其6自由度姿态，最后跟踪它。然后通过将轨迹相对于物体框架进行投影来自动生成可达到的抓取轨迹。数百个轨迹已经在实验中部署。

    Robotic grasping is still a partially solved, multidisciplinary problem where data-driven techniques play an increasing role. The sparse nature of rewards make the automatic generation of grasping datasets challenging, especially for unconventional morphologies or highly actuated end-effectors. Most approaches for obtaining large-scale datasets rely on numerous human-provided demonstrations or heavily engineered solutions that do not scale well. Recent advances in Quality-Diversity (QD) methods have investigated how to learn object grasping at a specific pose with different robot morphologies. The present work introduces a pipeline for adapting QD-generated trajectories to new object poses. Using an RGB-D data stream, the vision pipeline first detects the targeted object, predicts its 6-DOF pose, and finally tracks it. An automatically generated reach-and-grasp trajectory can then be adapted by projecting it relatively to the object frame. Hundreds of trajectories have been deployed in
    
[^244]: TRAM: 桥接信任区域和锐度感知最小化

    TRAM: Bridging Trust Regions and Sharpness Aware Minimization. (arXiv:2310.03646v1 [cs.LG])

    [http://arxiv.org/abs/2310.03646](http://arxiv.org/abs/2310.03646)

    TRAM是一种桥接信任区域和锐度感知最小化的算法，通过减少损失曲面的曲率来提供鲁棒性改进。它通过在微调过程中优化可转移的表示来实现领域外泛化，并且通过结合信任区域方法和SAM风格的正则化器来统一参数和表示空间平滑方法。TRAM在保持预训练结构的同时实现了平坦的极小值和平滑、有信息量的表示。

    

    通过减少参数空间中损失曲面的曲率，锐度感知最小化（SAM）在领域转移下提供了广泛的鲁棒性改进。然而，本文不是关注参数，而是考虑到表示的可转移性作为优化目标，在微调设置中实现领域外泛化。为了鼓励保留可转移的表示，我们考虑到基于信任区域的微调方法，这些方法利用任务特定的技能，而不会忘记预训练的任务无关表示。我们通过使用信任区域边界在这两种优化表面上通知SAM风格的正则化器，统一了参数和表示空间平滑方法。我们提出了Trust Region Aware Minimization (TRAM)，一种优化平坦的极小值和平滑、有信息量的表示的微调算法，而不会忘记预先训练的结构。我们发现，TRAM优于锐度感知和基于信任区域的方法。

    By reducing the curvature of the loss surface in the parameter space, Sharpness-aware minimization (SAM) yields widespread robustness improvement under domain transfer. Instead of focusing on parameters, however, this work considers the transferability of representations as the optimization target for out-of-domain generalization in a fine-tuning setup. To encourage the retention of transferable representations, we consider trust region-based fine-tuning methods, which exploit task-specific skills without forgetting task-agnostic representations from pre-training. We unify parameter- and representation-space smoothing approaches by using trust region bounds to inform SAM-style regularizers on both of these optimization surfaces. We propose Trust Region Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat minima and smooth, informative representations without forgetting pre-trained structure. We find that TRAM outperforms both sharpness-aware and trust region-based
    
[^245]: 通过利用层间变换的平滑性进行带外分布检测

    Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness. (arXiv:2310.02832v1 [cs.LG])

    [http://arxiv.org/abs/2310.02832](http://arxiv.org/abs/2310.02832)

    本文提出了一种通过利用神经网络中间层变换的平滑性来检测带外数据的方法(BLOOD),该方法适用于没有训练数据访问权限的预训练模型，并在Transformer网络上的文本分类任务中取得了良好的效果。

    

    有效的带外分布检测对于可靠的机器学习模型至关重要，然而大多数当前方法由于需要访问训练数据或者干预训练而在实际应用中受到限制。我们提出了一种新的方法，通过网络中间层的变换平滑性来检测深度神经网络中的带外数据（BLOOD），该方法适用于没有训练数据访问权限的预训练模型。BLOOD利用内分布（ID）数据的层间表示变换相较于带外数据的变换更平滑的倾向，这也是我们在Transformer网络中经验证明的一个特性。我们在几个文本分类任务上评估了BLOOD与Transformer网络，并证明其在资源需求相当的方法上性能更好。我们的分析还表明，当学习更简单的任务时，带外数据的变换会保持其原始的锐度，而锐度会随着任务的增加而增加。

    Effective OOD detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting OOD data in deep neural networks based on transformation smoothness between intermediate layers of a network (BLOOD), which is applicable to pre-trained models without access to training data. BLOOD utilizes the tendency of between-layer representation transformations of in-distribution (ID) data to be smoother than the corresponding transformations of OOD data, a property that we also demonstrate empirically for Transformer networks. We evaluate BLOOD on several text classification tasks with Transformer networks and demonstrate that it outperforms methods with comparable resource requirements. Our analysis also suggests that when learning simpler tasks, OOD data transformations maintain their original sharpness, whereas sharpness increases wit
    
[^246]: GRANDE: 基于梯度的决策树集成模型

    GRANDE: Gradient-Based Decision Tree Ensembles. (arXiv:2309.17130v1 [cs.LG])

    [http://arxiv.org/abs/2309.17130](http://arxiv.org/abs/2309.17130)

    这篇论文提出了一种名为GRANDE的基于梯度的决策树集成模型，通过端到端梯度下降学习坚硬、轴对齐的决策树集成，并结合了轴对齐分割和梯度优化的灵活性，引入了一种先进的逐个实例加权方法，可以在一个模型中便于学习简单和复杂关系的表示。

    

    尽管深度学习在文本和图像数据方面取得了成功，但基于树的集成模型仍然是处理异构表格数据的机器学习的最先进方法。然而，由于其高灵活性，对于表格数据来说，存在对特定于表格的梯度方法的显著需求。在本文中，我们提出了一种名为GRANDE的新方法，即基于梯度的决策树集成模型，通过端到端梯度下降学习坚硬、轴对齐的决策树集成。GRANDE基于决策树集成的稠密表示，可以使用直通操作符和反向传播一起优化所有模型参数。我们的方法结合了轴对齐分割（这是表格数据的一个有用的归纳偏置）和梯度优化的灵活性。此外，我们引入了一种先进的逐个实例加权方法，可以在一个模型中便于学习简单和复杂关系的表示。我们在广泛的实验数据集上评估了GRANDE的性能，并与其他方法进行了比较。

    Despite the success of deep learning for text and image data, tree-based ensemble models are still state-of-the-art for machine learning with heterogeneous tabular data. However, there is a significant need for tabular-specific gradient-based methods due to their high flexibility. In this paper, we propose $\text{GRANDE}$, $\text{GRA}$die$\text{N}$t-Based $\text{D}$ecision Tree $\text{E}$nsembles, a novel approach for learning hard, axis-aligned decision tree ensembles using end-to-end gradient descent. GRANDE is based on a dense representation of tree ensembles, which affords to use backpropagation with a straight-through operator to jointly optimize all model parameters. Our method combines axis-aligned splits, which is a useful inductive bias for tabular data, with the flexibility of gradient-based optimization. Furthermore, we introduce an advanced instance-wise weighting that facilitates learning representations for both, simple and complex relations, within a single model. We con
    
[^247]: MINT: 评估在与工具和语言反馈进行多轮交互中的LLMs的能力

    MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.10691](http://arxiv.org/abs/2309.10691)

    MINT是一个评估LLMs在多轮交互中解决任务能力的基准，通过使用工具和利用用户的自然语言反馈。它解决了当前评估协议忽略细致互动和低估自然语言反馈的问题，促进了研究基准评估和实际应用之间的一致性。

    

    为了解决复杂任务，大语言模型（LLMs）通常需要与用户进行多轮交互，有时候辅以外部工具的帮助。然而，当前的评估协议常常强调用单轮交流的基准性能，忽略了用户、LLMs和外部工具之间的细致互动，并低估了用户的自然语言反馈的重要性。这些疏忽导致了研究基准评估结果与实际应用情况之间的差异。我们引入了MINT，这是一个通过使用工具和利用用户的自然语言反馈来评估LLMs解决多轮交互任务能力的基准。为了保证可重复性，我们提供了一个评估框架，在这个框架中，LLMs可以通过执行Python代码来访问工具，并接收由GPT-4模拟的用户的自然语言反馈。我们重新利用了一系列多样的已建立评估数据集，重点关注推理、编码和决策方面。

    To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and careful
    
[^248]: 利用问题几何特征的安全线性赌博机问题

    Exploiting Problem Geometry in Safe Linear Bandits. (arXiv:2308.15006v1 [cs.LG])

    [http://arxiv.org/abs/2308.15006](http://arxiv.org/abs/2308.15006)

    通过利用安全线性赌博机问题的几何特征，我们提出了改进的遗憾保证算法，并将其推广到具有凸约束的情况。模拟结果显示，在各种随机采样的设置中，我们的算法表现出优越的性能。

    

    安全线性赌博机问题是经典线性赌博机问题的一个版本，其中学习器的行动必须在所有回合满足一个不确定的线性约束。由于其在许多实际场景中的适用性，近年来这个问题受到了相当大的关注。我们发现通过利用特定问题设置的几何特征，可以为相互分离的问题实例和有限星凸集的行动集提供改进的遗憾保证。此外，我们提出了一种新的算法，能够自适应地选择问题参数，并具有至少与现有算法相当的遗憾保证。最后，我们引入了安全线性赌博机设置的推广，其中约束是凸的，并利用了一种基于凸分析的新方法来调整我们的算法和分析。通过模拟结果显示，相对于现有算法，我们的算法在各种随机采样的设置中表现出改进的性能。

    The safe linear bandit problem is a version of the classic linear bandit problem where the learner's actions must satisfy an uncertain linear constraint at all rounds. Due its applicability to many real-world settings, this problem has received considerable attention in recent years. We find that by exploiting the geometry of the specific problem setting, we can achieve improved regret guarantees for both well-separated problem instances and action sets that are finite star convex sets. Additionally, we propose a novel algorithm for this setting that chooses problem parameters adaptively and enjoys at least as good regret guarantees as existing algorithms. Lastly, we introduce a generalization of the safe linear bandit setting where the constraints are convex and adapt our algorithms and analyses to this setting by leveraging a novel convex-analysis based approach. Simulation results show improved performance over existing algorithms for a variety of randomly sampled settings.
    
[^249]: 受物理启发的神经图ODE用于长期动力学模拟

    Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation. (arXiv:2308.13212v1 [cs.LG])

    [http://arxiv.org/abs/2308.13212](http://arxiv.org/abs/2308.13212)

    提出一种受物理启发的神经图ODE算法（PINGO），用于模拟和建模多对象物理系统的长期动态，并解决了当前模型泛化能力差的问题。

    

    模拟和建模多对象物理系统的长期动态是一项重要且具有挑战性的任务。目前的研究利用具有等变性质的图神经网络(GNNs)对物理系统进行建模。具体而言，他们将动力学建模为一系列具有固定时间间隔的离散状态，并学习所有相邻状态之间的直接映射。然而，这种直接映射忽略了两个状态之间的连续性。换句话说，我们已经验证了在当前基于GNN的直接映射模型中，两个离散动态状态之间存在无数可能的轨迹。这个问题极大地阻碍了模型的泛化能力，导致长期模拟的性能较差。在本文中，为了更好地通过离散监督信号建模潜在轨迹，我们提出了一个受物理启发的神经图ODE(PINGO)算法。

    Simulating and modeling the long-term dynamics of multi-object physical systems is an essential and challenging task. Current studies model the physical systems utilizing Graph Neural Networks (GNNs) with equivariant properties. Specifically, they model the dynamics as a sequence of discrete states with a fixed time interval and learn a direct mapping for all the two adjacent states. However, this direct mapping overlooks the continuous nature between the two states. Namely, we have verified that there are countless possible trajectories between two discrete dynamic states in current GNN-based direct mapping models. This issue greatly hinders the model generalization ability, leading to poor performance of the long-term simulation. In this paper, to better model the latent trajectory through discrete supervision signals, we propose a Physics-Inspired Neural Graph ODE (PINGO) algorithm. In PINGO, to ensure the uniqueness of the trajectory, we construct a Physics-Inspired Neural ODE fram
    
[^250]: 用于支持地下不确定性量化和解释的稳定低维空间刚性变换

    Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation. (arXiv:2308.08079v1 [cs.LG])

    [http://arxiv.org/abs/2308.08079](http://arxiv.org/abs/2308.08079)

    该论文介绍了一种用于地下数据集的稳定降维方法，通过刚性变换实现了欧几里德不变表示，能够量化地下数据的不确定性并且适应外样本点的扩展。

    

    地下数据集天然地具有大数据特征，如庞大的体积、多样的特征和高速采样速度，受到各种物理、工程和地质输入引起的维数诅咒的影响。在现有的降维方法中，非线性降维方法，特别是度量多维缩放（MDS），是地下数据集中首选的方法，因为它们具有固有的复杂性。虽然MDS保留了内在的数据结构和不确定性的量化，但其局限性包括不稳定的唯一解，不变于欧几里德变换，并且没有外样本点（OOSP）扩展。为了增强地下推理和机器学习工作流程，必须将数据集转化为稳定的、降维的表示，以容纳OOSP。我们的解决方案利用刚性变换实现了LDS的稳定欧几里德不变表示。通过计算MDS输入的不相似度，

    Subsurface datasets inherently possess big data characteristics such as vast volume, diverse features, and high sampling speeds, further compounded by the curse of dimensionality from various physical, engineering, and geological inputs. Among the existing dimensionality reduction (DR) methods, nonlinear dimensionality reduction (NDR) methods, especially Metric-multidimensional scaling (MDS), are preferred for subsurface datasets due to their inherent complexity. While MDS retains intrinsic data structure and quantifies uncertainty, its limitations include unstabilized unique solutions invariant to Euclidean transformations and an absence of out-of-sample points (OOSP) extension. To enhance subsurface inferential and machine learning workflows, datasets must be transformed into stable, reduced-dimension representations that accommodate OOSP.  Our solution employs rigid transformations for a stabilized Euclidean invariant representation for LDS. By computing an MDS input dissimilarity m
    
[^251]: 临界点++：一种用于鲁棒分类、对抗性防御和可解释AI的敏捷点云重要性度量方法

    Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI. (arXiv:2308.05525v1 [cs.CV])

    [http://arxiv.org/abs/2308.05525](http://arxiv.org/abs/2308.05525)

    本文研究了三维点云的临界点与非分布样本之间的相互作用，并将临界点的概念推广为重要性度量方法。通过仅基于非重要点进行分类网络训练，可以提高鲁棒性，同时在干净数据集上会有些性能损失。建议使用标准化熵选择非临界点集合的自适应阈值。这种重要性度量方法计算速度极快，可以应用于可解释AI、离群值去除、不确定性估计、鲁棒分类和对抗性防御等多种应用。

    

    在真实世界的安全需求应用中，准确且快速地处理非分布样本是至关重要的。本文首先研究了三维点云的临界点与非分布样本之间的相互作用。我们的研究发现，常见的数据损坏和离群点往往会被解释为临界点。我们将临界点的概念推广为重要性度量。我们证明，仅基于非重要点的分类网络训练，可以大大提高鲁棒性，而在干净数据集上会稍微损失一些性能。我们观察到，标准化熵对于数据损坏分析非常有信息量。建议基于标准化熵选择非临界点集合的自适应阈值。我们提出的重要性度量计算极其快速。我们展示了它可以用于多种应用，例如可解释AI(XAI)，离群值去除，不确定性估计，鲁棒分类和对抗性防御。

    The ability to cope accurately and fast with Out-Of-Distribution (OOD) samples is crucial in real-world safety demanding applications. In this work we first study the interplay between critical points of 3D point clouds and OOD samples. Our findings are that common corruptions and outliers are often interpreted as critical points. We generalize the notion of critical points into importance measures. We show that training a classification network based only on less important points dramatically improves robustness, at a cost of minor performance loss on the clean set. We observe that normalized entropy is highly informative for corruption analysis. An adaptive threshold based on normalized entropy is suggested for selecting the set of uncritical points. Our proposed importance measure is extremely fast to compute. We show it can be used for a variety of applications, such as Explainable AI (XAI), Outlier Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense. We 
    
[^252]: 深度学习在不同数据类型隐写分析中的应用：综述

    Deep Learning for Diverse Data Types Steganalysis: A Review. (arXiv:2308.04522v1 [cs.CR])

    [http://arxiv.org/abs/2308.04522](http://arxiv.org/abs/2308.04522)

    本综述论文详细综述了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的最新研究进展。

    

    隐写术和隐写分析是信息安全领域的两个相关方面。隐写术旨在隐藏通信，而隐写分析则旨在找到这些隐藏信息，甚至尝试恢复其所包含的数据。隐写术和隐写分析引起了广泛的关注，特别受到执法部门的关注。隐写术常被网络犯罪分子甚至恐怖分子用来避免在拥有证据时被捕，即使加密也一样，因为在许多国家禁止或限制使用密码学。因此，了解揭示隐藏信息的尖端技术对揭露非法行为至关重要。在过去几年中，文献中引入了许多强大可靠的隐写术和隐写分析技术。本综述论文提供了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的全面概述。

    Steganography and steganalysis are two interrelated aspects of the field of information security. Steganography seeks to conceal communications, whereas steganalysis is aimed to either find them or even, if possible, recover the data they contain. Steganography and steganalysis have attracted a great deal of interest, particularly from law enforcement. Steganography is often used by cybercriminals and even terrorists to avoid being captured while in possession of incriminating evidence, even encrypted, since cryptography is prohibited or restricted in many countries. Therefore, knowledge of cutting-edge techniques to uncover concealed information is crucial in exposing illegal acts. Over the last few years, a number of strong and reliable steganography and steganalysis techniques have been introduced in the literature. This review paper provides a comprehensive overview of deep learning-based steganalysis techniques used to detect hidden information within digital media. The paper cove
    
[^253]: 通过融合多空间深度模型的脑电信号来估计心理负荷

    Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models. (arXiv:2308.02409v1 [eess.SP])

    [http://arxiv.org/abs/2308.02409](http://arxiv.org/abs/2308.02409)

    该论文通过融合多个空间维度的方法，使用脑电信号对心理负荷进行分类和估计连续级别。在时间域中使用了时态卷积网络，而在频率域中引入了新的架构——多维残差块。

    

    人脑在工作和休息时都处于持续活动的状态。心理活动是日常过程中的一部分，当大脑过度劳累时，会对人体健康产生负面影响。近年来，人们对于早期检测心理健康问题的重视逐渐增加，因为这可以帮助预防严重的健康问题，并改善生活质量。多种信号被用于评估心理状态，但由于大量提供关于大脑信息的特点，脑电图（EEG）被研究人员广泛使用。本文旨在将心理负荷分为三种状态并估计连续级别。我们的方法通过融合多个空间维度来实现最佳的心理估计结果。在时间域方法中，我们使用了时态卷积网络，而在频率域上，我们提出了一种名为多维残差块的新架构，它结合了残差块。

    The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
    
[^254]: VAE的对称均衡学习

    Symmetric Equilibrium Learning of VAEs. (arXiv:2307.09883v1 [cs.LG])

    [http://arxiv.org/abs/2307.09883](http://arxiv.org/abs/2307.09883)

    本文提出了一种对称均衡学习方法，允许在只能通过采样获得数据和潜在分布的情况下学习VAEs。实验证明该方法与传统ELBO学习方法获得的模型相当，并具有广泛的应用性。

    

    我们将变分自动编码器（VAE）视为解码器-编码器对，将数据空间中的分布映射到潜在空间中的分布，反之亦然。VAEs的标准学习方法，即最大化证据下界（ELBO），存在明显的不对称性。此外，它需要一个闭合形式的先验潜在分布。这限制了VAEs在更复杂的情况下的适用性，如一般的半监督学习和使用复杂的生成模型作为先验。我们提出了一种纳什均衡学习方法，放宽了这些限制，在只能通过采样获得数据和潜在分布的情况下学习VAEs。这种方法的灵活性和简单性使其适用于广泛的学习场景和下游任务。实验证明，通过这种方法学习的模型与ELBO学习获得的模型相当，并展示了其在实践中的适用性。

    We view variational autoencoders (VAE) as decoder-encoder pairs, which map distributions in the data space to distributions in the latent space and vice versa. The standard learning approach for VAEs, i.e. maximisation of the evidence lower bound (ELBO), has an obvious asymmetry in that respect. Moreover, it requires a closed form a-priori latent distribution. This limits the applicability of VAEs in more complex scenarios, such as general semi-supervised learning and employing complex generative models as priors. We propose a Nash equilibrium learning approach that relaxes these restrictions and allows learning VAEs in situations where both the data and the latent distributions are accessible only by sampling. The flexibility and simplicity of this approach allows its application to a wide range of learning scenarios and downstream tasks. We show experimentally that the models learned by this method are comparable to those obtained by ELBO learning and demonstrate its applicability fo
    
[^255]: 过度思考真相：理解语言模型如何处理虚假演示

    Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])

    [http://arxiv.org/abs/2307.09476](http://arxiv.org/abs/2307.09476)

    该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。

    

    现代语言模型可以通过少量示范进行复杂模式的模仿学习，使其能够在没有微调的情况下完成具有挑战性的任务。然而，模仿也可能导致模型在上下文中重现不准确或有害的内容。我们通过模型的内部表示来研究有害的模仿，并确定了两个相关现象：过度思考和错误归纳头。第一个现象，过度思考，在给出正确与错误的少量示范时，我们从中间层解码预测。在早期层中，两种示范引起了相似的模型行为，但在某个“关键层”之后，给出错误示范的准确性逐渐降低。第二个现象，错误归纳头，可能是过度思考的一种机制性原因：这些是位于较晚层的头部，它们关注并复制先前示范中的错误信息，其削弱会减少过度思考现象。

    Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
    
[^256]: 动态预测使用时变Cox生存神经网络

    Dynamic Prediction using Time-Dependent Cox Survival Neural Network. (arXiv:2307.05881v1 [stat.ML])

    [http://arxiv.org/abs/2307.05881](http://arxiv.org/abs/2307.05881)

    该论文通过使用time-dependent Cox模型和神经网络，提出了一种动态预测模型来预测进行性眼部疾病年龄相关性黄斑变性（AMD）的进展。通过使用纵向眼底图像作为输入，该模型可以建立一个个体化的风险预测模型，并且在实证研究中表现出良好的效果。

    

    动态预测的目标是在不断更新的新数据可用时提供个体化的风险预测。受到建立一个针对进行性眼部疾病，年龄相关性黄斑变性（AMD），我们提出了一种基于时变Cox模型的生存神经网络（tdCoxSNN）来预测其在持续时间尺度上的进展，使用纵向眼底图像。tdCoxSNN通过利用神经网络来模拟时变协变量对生存结果的非线性影响扩展了时变Cox模型。此外，通过结合卷积神经网络（CNN），tdCoxSNN可以以纵向原始图像作为输入。我们通过全面的模拟，使用两个时变精度度量标准，Brier分数和动态AUC比较和评估我们提出的方法与联合建模和里程碑方法。我们将所提出的方法应用于两个真实数据集。一个是一个大型AMD数据集。

    The target of dynamic prediction is to provide individualized risk predictions over time which can be updated as new data become available. Motivated by establishing a dynamic prediction model for the progressive eye disease, age-related macular degeneration (AMD), we proposed a time-dependent Cox model-based survival neural network (tdCoxSNN) to predict its progression on a continuous time scale using longitudinal fundus images. tdCoxSNN extends the time-dependent Cox model by utilizing a neural network to model the non-linear effect of the time-dependent covariates on the survival outcome. Additionally, by incorporating the convolutional neural network (CNN), tdCoxSNN can take the longitudinal raw images as input. We evaluate and compare our proposed method with joint modeling and landmarking approaches through comprehensive simulations using two time-dependent accuracy metrics, the Brier Score and dynamic AUC. We applied the proposed approach to two real datasets. One is a large AMD
    
[^257]: TNPAR: 基于拓扑神经泊松自回归模型的事件序列Granger因果结构学习

    TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences. (arXiv:2306.14114v1 [cs.LG])

    [http://arxiv.org/abs/2306.14114](http://arxiv.org/abs/2306.14114)

    该论文提出了一种基于拓扑神经泊松自回归模型的方法，同时考虑先验拓扑网络和潜在的Granger因果结构来学习事件序列中的Granger因果关系，该方法通过推断因果关系来解决序列数据中的非 i.i.d. 问题

    

    从事件序列中学习Granger因果关系是各种应用中具有挑战性但又至关重要的任务。大多数现有方法都依赖于事件序列独立同分布 (i.i.d.) 的假设。然而，由于事件序列之间的固有依赖关系，这一 i.i.d. 假设经常被违反。幸运的是，在实践中，我们发现这些依赖关系可以被建模成一个拓扑网络，因此可以通过将先验拓扑网络引入Granger因果发现来解决非 i.i.d. 问题。这一发现促使我们解决两个问题：1) 如何在模型事件序列时同时考虑先验拓扑网络和潜在的Granger因果结构；2) 如何学习Granger因果结构。为此，我们设计了一个两阶段的统一拓扑神经泊松自回归模型。在生成阶段，我们采用神经泊松过程的一种变体来建模事件发生的时刻，并通过拓扑关系和现有事件序列推断因果关系。

    Learning Granger causality from event sequences is a challenging but essential task across various applications. Most existing methods rely on the assumption that event sequences are independent and identically distributed (i.i.d.). However, this i.i.d. assumption is often violated due to the inherent dependencies among the event sequences. Fortunately, in practice, we find these dependencies can be modeled by a topological network, suggesting a potential solution to the non-i.i.d. problem by introducing the prior topological network into Granger causal discovery. This observation prompts us to tackle two ensuing challenges: 1) how to model the event sequences while incorporating both the prior topological network and the latent Granger causal structure, and 2) how to learn the Granger causal structure. To this end, we devise a two-stage unified topological neural Poisson auto-regressive model. During the generation stage, we employ a variant of the neural Poisson process to model the 
    
[^258]: 在具有部分在线状态信息的强化学习中，POMDP的理论难度和可计算性

    Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information. (arXiv:2306.08762v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08762](http://arxiv.org/abs/2306.08762)

    本论文研究了具有部分在线状态信息的POMDP问题的理论困难性和可计算性。作者通过建立下界得出一个惊人的难度结果：除非具有完整的在线状态信息，否则需要指数级的样本复杂度才能得到POMDP的最优策略解。然而，作者还发现了具有部分在线状态信息下的可计算POMDP类别，并提出了新的算法来证明其接近最优性。

    

    部分可观察的马尔可夫决策过程（POMDP）被广泛应用于捕捉许多现实世界的应用。然而，现有的理论结果已经表明，在一般的POMDP中学习可能是不可计算的，主要挑战在于缺乏潜在的状态信息。一个关键的基本问题是有多少在线状态信息（OSI）足以实现可计算性。在本文中，我们建立了一个下界，揭示了一个惊人的难度结果：除非我们具有完整的OSI，否则我们需要指数级的采样复杂度才能获得POMDP的$\epsilon$-最优策略解。尽管如此，受到我们下界设计的关键见解的启发，我们发现即使只有部分OSI，也存在重要的可计算的POMDP类别。特别地，对于具有部分OSI的两个新颖的POMDP类别，我们通过建立新的遗憾上下界证明了新的算法是接近最优的。

    Partially observable Markov decision processes (POMDPs) have been widely applied to capture many real-world applications. However, existing theoretical results have shown that learning in general POMDPs could be intractable, where the main challenge lies in the lack of latent state information. A key fundamental question here is how much online state information (OSI) is sufficient to achieve tractability. In this paper, we establish a lower bound that reveals a surprising hardness result: unless we have full OSI, we need an exponentially scaling sample complexity to obtain an $\epsilon$-optimal policy solution for POMDPs. Nonetheless, inspired by the key insights in our lower bound design, we find that there exist important tractable classes of POMDPs even with only partial OSI. In particular, for two novel classes of POMDPs with partial OSI, we provide new algorithms that are proved to be near-optimal by establishing new regret upper and lower bounds.
    
[^259]: SGAT4PASS：面向球面几何意识的全景语义分割Transformer

    SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation. (arXiv:2306.03403v1 [cs.CV])

    [http://arxiv.org/abs/2306.03403](http://arxiv.org/abs/2306.03403)

    本论文提出了SGAT4PASS，一种面向球面几何意识的全景语义分割Transformer，通过加入球面几何感知的约束，能更好地捕捉全景图像的3D属性，从而提高分割性能。

    

    作为计算机视觉中一个重要且具有挑战性的问题，全景语义分割可以根据超广角观察到的完整场景来进行感知。传统的针对2D全景图像的PASS方法侧重于解决图像畸变问题，但缺乏对原始360°数据的3D属性的考虑。因此，当输入具有3D扰动的全景图像时，它们的性能会大幅下降。为了更好地应对3D扰动，我们提出了一种面向球面几何意识的全景语义分割Transformer，即SGAT4PASS。具体来说，我们提出了一个球面几何意识的分割框架，它包括三个模块，即球面几何感知图像投影，球面可形变补丁嵌入和全景感知损失，它对具有3D扰动的输入图像进行处理，并对已有的可形变补丁嵌入加入了球面几何感知的约束。

    As an important and challenging problem in computer vision, PAnoramic Semantic Segmentation (PASS) gives complete scene perception based on an ultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic image input focus on solving image distortions but lack consideration of the 3D properties of original $360^{\circ}$ data. Therefore, their performance will drop a lot when inputting panoramic images with the 3D disturbance. To be more robust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical geometry knowledge. Specifically, a spherical geometry-aware framework is proposed for PASS. It includes three modules, i.e., spherical geometry-aware image projection, spherical deformable patch embedding, and a panorama-aware loss, which takes input images with 3D disturbance into account, adds a spherical geometry-aware constraint on the existing deformable patch embedding, and indicates
    
[^260]: 优化退火算法的误差界和局部搜索策略

    Sharpened Lazy Incremental Quasi-Newton Method. (arXiv:2305.17283v1 [math.OC])

    [http://arxiv.org/abs/2305.17283](http://arxiv.org/abs/2305.17283)

    本文提出了一种新算法—— Sharpened Lazy Incremental Quasi-Newton (SLIQN) 方法，其具有显式的超线性收敛速率和$O(d^2)$的迭代复杂度。

    

    本文考虑了具有$Lipschitz$连续Hessian矩阵在$d$维空间中，$n$个强凸光滑函数的有限和最小化问题。在许多应用中，$n$的数量很大，因此必须使用每迭代一次与$n$无关的增量式或随机算法。本文提出了一种新算法—— Sharpened Lazy Incremental Quasi-Newton (SLIQN) 方法，其具有显式的超线性收敛速率和$O(d^2)$的迭代复杂度。

    We consider the finite sum minimization of $n$ strongly convex and smooth functions with Lipschitz continuous Hessians in $d$ dimensions. In many applications where such problems arise, including maximum likelihood estimation, empirical risk minimization, and unsupervised learning, the number of observations $n$ is large, and it becomes necessary to use incremental or stochastic algorithms whose per-iteration complexity is independent of $n$. Of these, the incremental/stochastic variants of the Newton method exhibit superlinear convergence, but incur a per-iteration complexity of $O(d^3)$, which may be prohibitive in large-scale settings. On the other hand, the incremental Quasi-Newton method incurs a per-iteration complexity of $O(d^2)$ but its superlinear convergence rate has only been characterized asymptotically. This work puts forth the Sharpened Lazy Incremental Quasi-Newton (SLIQN) method that achieves the best of both worlds: an explicit superlinear convergence rate with a per-
    
[^261]: 通过伪神经切线核代理模型提供深度神经网络的鲁棒性解释

    Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models. (arXiv:2305.14585v1 [cs.LG])

    [http://arxiv.org/abs/2305.14585](http://arxiv.org/abs/2305.14585)

    本研究通过建立一个规范化的伪神经切线核，证明了它能够更好地与神经网络决策函数相关，比基于嵌入和影响的替代品更有效，并且从它创建的归因会更准确地选择被扰动的训练数据，从而证明了核线性模型是跨多个数据领域并有效的替代模型。

    

    最近，通过数据归属任务，解释型AI的进步之一是通过解释示例策略实现的。然而，用于将决策归因于训练数据的特征空间，尚未相互比较，以确定它们是否形成神经网络(NN)的真正代理模型。在这里，我们通过两种方式证明了线性特征空间对神经网络的有效性：(1)我们建立了一个规范化的伪神经切线核(pNTK)，它在计算机视觉和大语言模型架构中与神经网络决策函数更相关，比基于嵌入和影响的替代品更为有效；(2)我们展示了从规范化pNTK创建的归因比这些替代品更准确地选择被扰动的训练数据。基于这些观察结果，我们得出结论，核线性模型是跨多个数据领域并有效的替代模型。

    One of the ways recent progress has been made on explainable AI has been via explain-by-example strategies, specifically, through data attribution tasks. The feature spaces used to attribute decisions to training data, however, have not been compared against one another as to whether they form a truly representative surrogate model of the neural network (NN). Here, we demonstrate the efficacy of surrogate linear feature spaces to neural networks through two means: (1) we establish that a normalized psuedo neural tangent kernel (pNTK) is more correlated to the neural network decision functions than embedding based and influence based alternatives in both computer vision and large language model architectures; (2) we show that the attributions created from the normalized pNTK more accurately select perturbed training data in a data poisoning attribution task than these alternatives. Based on these observations, we conclude that kernel linear models are effective surrogate models across m
    
[^262]: 序列建模是离线强化学习的一个强有力的竞争者。

    Sequence Modeling is a Robust Contender for Offline Reinforcement Learning. (arXiv:2305.14550v1 [cs.LG])

    [http://arxiv.org/abs/2305.14550](http://arxiv.org/abs/2305.14550)

    序列建模是离线强化学习中比Q-Learning和Imitation Learning更适合在稀疏奖励和低质量数据设置下的选择，在任务范围增加时，序列建模和模仿学习更可取。

    

    离线强化学习使代理能够从静态数据集中学习有效的最大化收益策略。离线RL的三大范式是Q-Learning、Imitation Learning和Sequence Modeling。一个关键的问题是：在什么条件下，哪种范式被优先选择？我们通过探索代表性算法——保守Q-Learning(CQL)、行为克隆 (BC)和决策Transformer (DT)——在常用的D4RL和Robomimic基准测试中的表现来对这个问题进行了实证研究。我们设计了有针对性的实验来理解它们在数据子优性和任务复杂性方面的行为。我们的主要发现是：(1)序列建模需要比Q-Learning更多的数据来学习竞争性策略，但更加稳健；(2)序列建模在稀疏奖励和低质量数据设置中比Q-Learning和Imitation Learning都要好得多；(3)随着任务范围的增加，序列建模和模仿学习更可取。

    Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three major paradigms for offline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key open question is: which paradigm is preferred under what conditions? We study this question empirically by exploring the performance of representative algorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT) -- across the commonly used D4RL and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality and task complexity. Our key findings are: (1) Sequence Modeling requires more data than Q-Learning to learn competitive policies but is more robust; (2) Sequence Modeling is a substantially better choice than both Q-Learning and Imitation Learning in sparse-reward and low-quality data settings; and (3) Sequence Modeling and Imitation Learning are preferable as task horizon inc
    
[^263]: P-NOC:对弱监督语义分割进行对抗性CAM生成的研究

    P-NOC: Adversarial CAM Generation for Weakly Supervised Semantic Segmentation. (arXiv:2305.12522v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.12522](http://arxiv.org/abs/2305.12522)

    本文提出了一种新的弱监督语义分割策略，通过对抗性CAM生成网络生成稳健的语义分割提议，结果表明该方法显著提高了分割效果。

    

    为了减少对大量有监督分割注释集的依赖，已经提出了多种弱监督语义分割（WSSS）策略。这些策略通常依赖于先进的数据和模型正则化策略，以激发分割先验中有用属性（例如，预测完整性和对语义边界的忠实度）的发展，而不考虑注释信息的缺乏。在这项工作中，我们首先通过分析互补的WSSS技术和正则化策略，考虑它们的优点和局限性，建立了一个强大的基准。然后，我们提出了一种新的类别特定的对抗性擦除策略，包括逐渐改进的两个对抗性CAM生成网络，以产生稳健的语义分割提议。实证结果表明，我们的方法显著提高了基准的效果，在Pascal VOC 2012和MS COCO 2014数据集上都取得了显著的改进。

    To mitigate the necessity for large amounts of supervised segmentation annotation sets, multiple Weakly Supervised Semantic Segmentation (WSSS) strategies have been devised. These will often rely on advanced data and model regularization strategies to instigate the development of useful properties (e.g., prediction completeness and fidelity to semantic boundaries) in segmentation priors, notwithstanding the lack of annotated information. In this work, we first create a strong baseline by analyzing complementary WSSS techniques and regularizing strategies, considering their strengths and limitations. We then propose a new Class-specific Adversarial Erasing strategy, comprising two adversarial CAM generating networks being gradually refined to produce robust semantic segmentation proposals. Empirical results suggest that our approach induces substantial improvement in the effectiveness of the baseline, resulting in a noticeable improvement over both Pascal VOC 2012 and MS COCO 2014 datas
    
[^264]: 合成数据生成的效用理论

    Utility Theory of Synthetic Data Generation. (arXiv:2305.10015v1 [stat.ML])

    [http://arxiv.org/abs/2305.10015](http://arxiv.org/abs/2305.10015)

    本文从统计学角度建立效用理论，旨在基于一般性指标定量评估合成算法的效用，效用指标的分析界限揭示了指标收敛的关键条件，令人惊讶的是，只要下游学习任务中的模型规范是正确的，合成特征分布不一定与原始特征分布相同，效用指标会收敛。

    

    评估合成数据的效用对于衡量合成算法的有效性和效率至关重要。现有的结果侧重于对合成数据效用的经验评估，而针对合成数据算法如何影响效用的理论理解仍然未被充分探索。本文从统计学角度建立效用理论，旨在基于一般性指标定量评估合成算法的效用。该指标定义为在合成和原始数据集上训练的模型之间泛化的绝对差异。我们建立了该效用指标的分析界限来研究指标收敛的关键条件。一个有趣的结果是，只要下游学习任务中的模型规范是正确的，合成特征分布不一定与原始特征分布相同，则该效用指标会收敛。另一个重要的效用指标基于合成和原始数据之间潜在的因果机制一致性。该理论使用几种合成算法进行说明，并分析了它们的效用属性。

    Evaluating the utility of synthetic data is critical for measuring the effectiveness and efficiency of synthetic algorithms. Existing results focus on empirical evaluations of the utility of synthetic data, whereas the theoretical understanding of how utility is affected by synthetic data algorithms remains largely unexplored. This paper establishes utility theory from a statistical perspective, aiming to quantitatively assess the utility of synthetic algorithms based on a general metric. The metric is defined as the absolute difference in generalization between models trained on synthetic and original datasets. We establish analytical bounds for this utility metric to investigate critical conditions for the metric to converge. An intriguing result is that the synthetic feature distribution is not necessarily identical to the original one for the convergence of the utility metric as long as the model specification in downstream learning tasks is correct. Another important utility metri
    
[^265]: OpenBox：通用黑盒优化的 Python 工具包

    OpenBox: A Python Toolkit for Generalized Black-box Optimization. (arXiv:2304.13339v1 [cs.LG])

    [http://arxiv.org/abs/2304.13339](http://arxiv.org/abs/2304.13339)

    OpenBox是一个通用黑盒优化的Python工具包，提供了用户友好的接口和可视化功能，模块化设计能够在现有系统中灵活部署，并且实验证明其比现有系统更有效和高效。

    

    黑盒优化具有广泛的应用，包括自动机器学习、实验设计和数据库参数调整。然而，使用现有软件包时，用户在适用性、性能和效率方面仍面临挑战。本文介绍了 OpenBox，这是一个开源的黑盒优化工具包，提高了其可用性。它实现了用户友好的接口和可视化功能，让用户能够定义和管理任务。OpenBox 的模块化设计有助于在现有系统中灵活部署。实验结果表明，OpenBox比现有系统更有效和高效。OpenBox 的源代码可在 https://github.com/PKU-DAIR/open-box 中找到。

    Black-box optimization (BBO) has a broad range of applications, including automatic machine learning, experimental design, and database knob tuning. However, users still face challenges when applying BBO methods to their problems at hand with existing software packages in terms of applicability, performance, and efficiency. This paper presents OpenBox, an open-source BBO toolkit with improved usability. It implements user-friendly inferfaces and visualization for users to define and manage their tasks. The modular design behind OpenBox facilitates its flexible deployment in existing systems. Experimental results demonstrate the effectiveness and efficiency of OpenBox over existing systems. The source code of OpenBox is available at https://github.com/PKU-DAIR/open-box.
    
[^266]: 一种基于可解释神经网络的连续回应有序回归非比例赔率模型

    An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response. (arXiv:2303.17823v1 [stat.ME])

    [http://arxiv.org/abs/2303.17823](http://arxiv.org/abs/2303.17823)

    本文提出了一种基于可解释神经网络的非比例赔率模型 (N$^3$POM) 用于有序回归，可以对连续变量进行预测，同时保留了可解释性，具有灵活性。

    

    本文提出了一种基于可解释神经网络的非比例赔率模型（N$^3$POM) 用于有序回归，其中反应变量不仅可以取离散值，也可以取连续值，而回归系数根据预测顺序反应也不同。与传统方法直接从离散反应估计线性系数不同，我们训练了一个非线性的神经网络，通过以反应为输入产生线性系数。由于神经网络的优势，N$^3$POM可以在保留传统有序回归的可解释性的同时具有灵活性。我们给出了充分的条件，使得在指定的用户区域内，预测的条件累积概率（CCP）满足局部单调性约束。我们还提供了一种保持单调性的随机（MPS）算法来充分训练神经网络。

    This paper proposes an interpretable neural network-based non-proportional odds model (N$^3$POM) for ordinal regression, where the response variable can take not only discrete but also continuous values, and the regression coefficients vary depending on the predicting ordinal response. In contrast to conventional approaches estimating the linear coefficients of regression directly from the discrete response, we train a non-linear neural network that outputs the linear coefficients by taking the response as its input. By virtue of the neural network, N$^3$POM may have flexibility while preserving the interpretability of the conventional ordinal regression. We show a sufficient condition so that the predicted conditional cumulative probability~(CCP) satisfies the monotonicity constraint locally over a user-specified region in the covariate space; we also provide a monotonicity-preserving stochastic (MPS) algorithm for training the neural network adequately.
    
[^267]: 约束神经网络：神经DAEs

    Neural DAEs: Constrained neural networks. (arXiv:2211.14302v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14302](http://arxiv.org/abs/2211.14302)

    本文研究了将辅助代数轨迹信息明确添加到神经网络中的影响，并通过稳定化和投影方法合并信息，对多体摆和分子动力学情景进行了模拟实验。该方法易于实现，对训练性能影响有限，在推理方面给出了显著提升。

    

    本文研究了将辅助代数轨迹信息明确添加到动态系统的神经网络中的影响。我们从微分代数方程和流形上的微分方程领域汲取灵感，并在残差神经网络中实现相关方法，尽管存在一些基本情境上的差异。通过稳定化和投影方法，将约束或辅助信息效果合并，并通过对多体摆和分子动力学情景的模拟实验展示了何时使用哪种方法。我们的一些方法易于在现有代码中实现，并对训练性能影响有限，同时在推理方面给出了显著的提升。

    This article investigates the effect of explicitly adding auxiliary algebraic trajectory information to neural networks for dynamical systems. We draw inspiration from the field of differential-algebraic equations and differential equations on manifolds and implement related methods in residual neural networks, despite some fundamental scenario differences. Constraint or auxiliary information effects are incorporated through stabilization as well as projection methods, and we show when to use which method based on experiments involving simulations of multi-body pendulums and molecular dynamics scenarios. Several of our methods are easy to implement in existing code and have limited impact on training performance while giving significant boosts in terms of inference.
    
[^268]: 面向多领域协作学习的联邦自适应提示调优

    Federated Adaptive Prompt Tuning for Multi-domain Collaborative Learning. (arXiv:2211.07864v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07864](http://arxiv.org/abs/2211.07864)

    本文提出了一种面向多领域协作学习的联邦自适应提示调优算法 FedAPT，利用强大的预训练模型实现更高的性能。其核心思想是为每个测试样本提供个性化提示，通过自适应地释放特定领域的知识来实现。通过设计一个自适应提示调优模块，服务器生成关键信息并分配给客户端，从而实现协同训练全局的自适应网络和元提示。

    

    联邦学习使得多个客户端能够在不泄露数据的情况下协同训练全局模型。以往的研究通常需要训练完整的模型参数。然而，强大的预训练模型的出现使得在联邦学习中使用更少的可训练参数能够实现更高的性能。本文提出了一种面向多领域协作图像分类的联邦自适应提示调优算法 FedAPT，利用类似 CLIP 的强大基础模型。与直接联邦提示调优相比，我们的核心思想是针对每个测试样本自适应地释放特定领域的知识，为其提供个性化提示。为了实现这个想法，我们设计了一个自适应提示调优模块，它包括元提示，自适应网络和一些关键信息。服务器随机生成一组关键信息，并将每个客户端分配一个唯一的关键信息。然后，所有客户端协同训练全局自适应网络和元提示。

    Federated learning (FL) enables multiple clients to collaboratively train a global model without disclosing their data. Previous researches often require training the complete model parameters. However, the emergence of powerful pre-trained models makes it possible to achieve higher performance with fewer learnable parameters in FL. In this paper, we propose a federated adaptive prompt tuning algorithm, FedAPT, for multi-domain collaborative image classification with powerful foundation models, like CLIP. Compared with direct federated prompt tuning, our core idea is to adaptively unlock specific domain knowledge for each test sample in order to provide them with personalized prompts. To implement this idea, we design an adaptive prompt tuning module, which consists of a meta prompt, an adaptive network, and some keys. The server randomly generates a set of keys and assigns a unique key to each client. Then all clients cooperatively train the global adaptive network and meta prompt wit
    
[^269]: 带 $\beta$-差异的稀疏非负矩阵分解的主导最小化算法

    Majorization-minimization for Sparse Nonnegative Matrix Factorization with the $\beta$-divergence. (arXiv:2207.06316v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.06316](http://arxiv.org/abs/2207.06316)

    本文提出了一种带 $\beta$-差异的稀疏非负矩阵分解的主导最小化算法，其能够适用于任何 $\beta$-差异和其他稀疏约束。

    

    本文提出了一种新的多元乘法更新方法，用于具有 $\beta$-差异和两个因子中的一个（比如说，激活矩阵）稀疏正则化的非负矩阵分解。标准的做法是限制字典的列具有单位范数，从而控制另一个因子（字典矩阵）的范数，以避免病态问题。我们的方法将原问题重新参数化为等价的标度不变的目标函数的优化问题。然后，我们导出块下降主导最小化算法，这些算法对于 $\ell_{1}$-正则化或更 "激进" 的对数正则化都可以产生简单的多元乘法更新。与其他最先进的方法相比，我们的算法在任何 $\beta$-差异（即任何 $\beta$ 的值）和其他稀疏约束上也具有通用性。

    This article introduces new multiplicative updates for nonnegative matrix factorization with the $\beta$-divergence and sparse regularization of one of the two factors (say, the activation matrix). It is well known that the norm of the other factor (the dictionary matrix) needs to be controlled in order to avoid an ill-posed formulation. Standard practice consists in constraining the columns of the dictionary to have unit norm, which leads to a nontrivial optimization problem. Our approach leverages a reparametrization of the original problem into the optimization of an equivalent scale-invariant objective function. From there, we derive block-descent majorization-minimization algorithms that result in simple multiplicative updates for either $\ell_{1}$-regularization or the more "aggressive" log-regularization. In contrast with other state-of-the-art methods, our algorithms are universal in the sense that they can be applied to any $\beta$-divergence (i.e., any value of $\beta$) and t
    

