# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images.](http://arxiv.org/abs/2305.07644) | 扩散模型在医学图像合成中可能会导致记忆训练图像的问题，研究人员在选择合适的模型时需要谨慎。 |
| [^2] | [The ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2023: Intracranial Meningioma.](http://arxiv.org/abs/2305.07642) | ASNR-MICCAI脑肿瘤分割挑战2023将提供一个适用于自动诊断颅内脑膜瘤的最先进自动化颅内脑膜瘤分割模型的基准。 |
| [^3] | [Efficient Neural Network based Classification and Outlier Detection for Image Moderation using Compressed Sensing and Group Testing.](http://arxiv.org/abs/2305.07639) | 该论文提出了一种利用压缩感知技术和神经群组测试的方法，可以有效减少图像审核引擎的计算成本，提高图像审核效率。 |
| [^4] | [Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery.](http://arxiv.org/abs/2305.07637) | Text2Cohort是一个基于大语言模型的工具箱，可以将用户输入转化为IDC数据库查询，促进自然语言队列发现，减少研究人员查询IDC数据库的学习曲线，实现了癌症成像数据的民主化。 |
| [^5] | [Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training.](http://arxiv.org/abs/2305.07633) | 本论文提出了一种使用产品知识图谱预训练模型从预训练的语言模型提取项目特征，以解决零样本项目推荐任务。该方法通过提出四个预训练任务和任务导向的适应层来解决预训练过程中的挑战，并将模型微调到新的推荐任务中。 |
| [^6] | [Agile gesture recognition for capacitive sensing devices: adapting on-the-job.](http://arxiv.org/abs/2305.07624) | 该论文介绍了一种使用电容传感器信号识别手势的技术，通过机器学习技术能够在低功耗设备上实现，并在500毫秒内能够确定5个手指的三个特征，将手势的识别效率提高了。 |
| [^7] | [Uncertainty Estimation for Deep Learning Image Reconstruction using a Local Lipschitz Metric.](http://arxiv.org/abs/2305.07618) | 本文提出了一种基于局部Lipschitz度量的方法，可以用于估计深度学习图像重建模型的不确定性。该方法可用于确定特定深度学习重建方法的适用性，识别分布外的测试样本并指导适当的数据增强。 |
| [^8] | [Scalable Coupling of Deep Learning with Logical Reasoning.](http://arxiv.org/abs/2305.07617) | 本文介绍了一种可扩展的神经网络模型和损失函数，能够有效学习如何解决NP-hard推理问题，并在离散图模型上进行了实验验证。同时可以提高数据效率和可解释性，并具有对预测的控制能力。 |
| [^9] | [Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training.](http://arxiv.org/abs/2305.07613) | 本文提出了一种名为Spider GAN的新方法，通过寻找数据集之间的友好邻居来提高GAN的训练效率，加速收敛，即使是不相关的数据集之间也可以发现对应关系。 |
| [^10] | [Lower Bounds and Accelerated Algorithms in Distributed Stochastic Optimization with Communication Compression.](http://arxiv.org/abs/2305.07612) | 本文研究采用通信压缩的分布式随机优化算法的性能下限并提出了一种名为NEOLITHIC的新型通信压缩算法，通过加速收敛速率缩小下限和现有算法的差距。 |
| [^11] | [RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection.](http://arxiv.org/abs/2305.07598) | 本文提出了一种面向定向目标检测的DINO基线模型RHINO。并通过匈牙利匹配和查询对齐的方式实现动态降噪，解决了重复预测的问题，从而在公共基准测试中达到最先进的性能水平。 |
| [^12] | [MoMo: Momentum Models for Adaptive Learning Rates.](http://arxiv.org/abs/2305.07583) | 本文提出了新的自适应学习率，可与任何动量方法一起使用，通过构建损失函数模型并使用下限截断，以及即时估计未知下限，来近似最小化该模型以计算下一步，实验表明，相较于SGDM和Adam，该方法在精度和超参数调优的鲁棒性方面有所提高。 |
| [^13] | [Fisher Information Embedding for Node and Graph Learning.](http://arxiv.org/abs/2305.07580) | 本文提出了一种新的基于注意力机制的图节点嵌入框架，可以更好地理解基于注意力机制的GNN。 |
| [^14] | [Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts.](http://arxiv.org/abs/2305.07572) | 本文提出新颖的Voronoi Loss函数来解决高斯门控混合专家模型参数估计的收敛速率问题，并在两种不同的门控网络下提供理论收敛速率的证明。 |
| [^15] | [A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information.](http://arxiv.org/abs/2305.07565) | 该论文提出了一种记忆模型，在处理流式数据时，通过排练和预期来记忆有关问题回答任务的重要信息。该模型应用自监督机制，通过核指代信息的屏蔽建模任务训练，成功通过短序列数据集和大型基准测试。 |
| [^16] | [Saturated Non-Monotonic Activation Functions.](http://arxiv.org/abs/2305.07537) | 本文提出了三种新的饱和非单调激活函数（SGELU、SSiLU和SMish），它们由GELU、SiLU、Mish及ReLU的正部分组成，能够在CIFAR-100图像分类实验中展现很高的有效性。 |
| [^17] | [Learn to Unlearn: A Survey on Machine Unlearning.](http://arxiv.org/abs/2305.07512) | 本综述总结了机器去学习技术，用于从训练模型中删除敏感数据，但重新训练ML模型往往不可行。针对这个挑战，需要开发强大的模型以缓解公平性问题。 |
| [^18] | [eXplainable Artificial Intelligence on Medical Images: A Survey.](http://arxiv.org/abs/2305.07511) | 这项调查分析了XAI领域中应用于医学诊断的研究，以便解释黑匣子模型的结果，针对的疾病包括癌症和COVID-19。 |
| [^19] | [MolDiff: Addressing the Atom-Bond Inconsistency Problem in 3D Molecule Diffusion Generation.](http://arxiv.org/abs/2305.07508) | MolDiff 提出了一种新的扩散模型，可以同时生成原子和键，并通过显式建模它们之间的关系来保持一致性，从而解决了3D分子生成中的原子键不一致性问题。实验证明，该模型在生成高质量的3D分子方面表现出色。 |
| [^20] | [Calibration-Aware Bayesian Learning.](http://arxiv.org/abs/2305.07504) | 本文提出了一个综合框架，称为校准感知的贝叶斯神经网络 (CAB)，用于共同解决深度神经网络中的校准和贝叶斯学习，通过在训练过程中正则化模型的后验预测分布来提高模型的校准性。 |
| [^21] | [Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation.](http://arxiv.org/abs/2305.07500) | 本文提出了一种新的基于最优输运（OT）的领域自适应（DA）方法，通过学习一个嵌入空间，使得OT问题的解是最优且计算量较少的，适用于同质和异质的DA设置。 |
| [^22] | [Device-Robust Acoustic Scene Classification via Impulse Response Augmentation.](http://arxiv.org/abs/2305.07499) | 本篇论文提出了一种基于冲击响应增强的方法，用于解决音频分类模型泛化到未被训练设备上时性能下降的问题。 |
| [^23] | [Gallery Sampling for Robust and Fast Face Identification.](http://arxiv.org/abs/2305.07495) | 该论文提出了一种鲁棒且快速的人脸识别方法，通过对图库数据进行采样处理，在减少搜索时间的同时，对异常图像如错误标记、低质量和信息较少的图像具有较强的鲁棒性。在5.4M网络图像数据集上，我们的方法在FNIR方面达到了0.0975，而传统方法为0.3891。 |
| [^24] | [Benchmarks and leaderboards for sound demixing tasks.](http://arxiv.org/abs/2305.07489) | 本文介绍了两个新的声源分离任务基准，并将流行的模型及其集成在这些基准上的表现进行了比较。他们还开发了一种新的音频分离方法，基于适合特定音轨的不同模型的集成，该方法在2023年音乐分离挑战赛中取得了高水平成绩，并开源了代码和方法。 |
| [^25] | [Identify, Estimate and Bound the Uncertainty of Reinforcement Learning for Autonomous Driving.](http://arxiv.org/abs/2305.07487) | 本文提出了一种方法来限制自动驾驶中深度强化学习模型的决策不可靠性，以保护决策的可靠性，该方法通过估计和限制策略的性能不确定性来实现。 |
| [^26] | [Reduced Label Complexity For Tight $\ell_2$ Regression.](http://arxiv.org/abs/2305.07486) | 提出了一种算法，可以降低标签复杂度并实现紧密的最优近似值。 |
| [^27] | [Online Learning Under A Separable Stochastic Approximation Framework.](http://arxiv.org/abs/2305.07484) | 本篇论文提出了一种新的在线学习算法，通过分离随机逼近框架，使用递归最小二乘算法和随机梯度下降算法分别更新模型的线性和非线性参数。此算法在多个数据集上表现出高效和有效性。 |
| [^28] | [BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text.](http://arxiv.org/abs/2305.07468) | BactInt是一种面向领域的自动化方法，使用迁移学习从生物医学文本中提取细菌间相互作用并挖掘特定细菌群之间的关系。公开可用的BactInt语料库标注了1200篇PubMed摘要。 |
| [^29] | [Systematic Review on Reinforcement Learning in the Field of Fintech.](http://arxiv.org/abs/2305.07466) | 本文综述了近年来强化学习在金融科技领域的应用，包括组合优化、降低信用风险、投资资本管理、利润最大化、有效的推荐系统和更好的价格策略确定。本文通过PRISMA技术筛选文献，突出了强化学习在Fintech中的预测精度、复杂性、可扩展性、风险、盈利能力和业绩，旨在探讨其在Fintech领域中的实际贡献。 |
| [^30] | [Deep Deterministic Policy Gradient for End-to-End Communication Systems without Prior Channel Knowledge.](http://arxiv.org/abs/2305.07448) | 本研究提出了一种基于深度确定性策略梯度（DDPG）的框架来解决无先验信道知识的端到端通信系统中发射器和接收器联合训练的问题。与现有方案相比，该方法可以获得更好的检测性能。 |
| [^31] | [A Lightweight Domain Adversarial Neural Network Based on Knowledge Distillation for EEG-based Cross-subject Emotion Recognition.](http://arxiv.org/abs/2305.07446) | 本研究提出了一种基于知识蒸馏的轻量级领域对抗神经网络来提升基于EEG的跨主体情绪识别。教师模型学习复杂的EEG特征，指导学生模型学习更挑战的领域不变特征。 |
| [^32] | [Optimizing Memory Mapping Using Deep Reinforcement Learning.](http://arxiv.org/abs/2305.07440) | 本文提出了一种使用强化学习解决机器学习程序中内存映射问题的方法。 |
| [^33] | [Continual Vision-Language Representaion Learning with Off-Diagonal Information.](http://arxiv.org/abs/2305.07437) | 本文探讨了通过流数据持续训练CLIP模型的可行性，提出了一种有效的连续学习框架Mod-X，并证明内部旋转和跨模态偏差导致了CLIP在跨模态检索任务中性能下降。 |
| [^34] | [Expertise-based Weighting for Regression Models with Noisy Labels.](http://arxiv.org/abs/2305.07430) | 本文提出了基于专家评估的加权回归模型，可用于处理具有不同观点的嘈杂标签。该方法包括两个步骤：估计每个专家的专业程度和结合他们的意见，然后将加权平均用于回归建模。本方法在模拟和真实数据上优于现有技术，具有简单、快速和有效的特点。 |
| [^35] | [Unlocking the Potential of Medical Imaging with ChatGPT's Intelligent Diagnostics.](http://arxiv.org/abs/2305.07429) | 本文提出了一种使用深度学习模型和ChatGPT生成自动诊断的决策支持系统，可以帮助医疗保健提供者和患者在诊断、治疗和管理健康状况方面做出更好的决策。 |
| [^36] | [Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding.](http://arxiv.org/abs/2305.07424) | 本文提出了IS-CSE方法，通过实例平滑对比学习来学习无监督句子嵌入，以平滑嵌入在特征空间中的边界，从而提高模型的泛化性能。在标准的STS任务中取得了良好的得分。 |
| [^37] | [Selective imitation on the basis of reward function similarity.](http://arxiv.org/abs/2305.07421) | 人们在模仿别人时更倾向于选择那些被认为与自己奖励函数相似的人。 |
| [^38] | [A Multidimensional Graph Fourier Transformation Neural Network for Vehicle Trajectory Prediction.](http://arxiv.org/abs/2305.07416) | 介绍了一种名为多维图傅立叶转换神经网络(GFTNN)的网络结构，用于在高速公路上进行长期轨迹预测。通过强大的操作多维图傅立叶变换(GFT)，可以汇总场景属性。该模型在高速公路轨迹预测任务中胜过现有的最先进模型，即使没有包含任何循环元件。 |
| [^39] | [Comparison of machine learning models applied on anonymized data with different techniques.](http://arxiv.org/abs/2305.07415) | 本研究比较了四种经典机器学习方法在不同匿名化技术下对成人数据集分类任务的表现，分析了匿名化技术和参数对模型性能的影响。 |
| [^40] | [Distributed Gradient Descent for Functional Learning.](http://arxiv.org/abs/2305.07408) | 该论文提出了一种针对函数数据的分布式梯度下降函数学习算法，在再生核希尔伯特空间框架下通过积分算子方法得到了该算法的理论理解，并取得了不饱和边界的置信度最优学习率。 |
| [^41] | [Two-in-One: A Model Hijacking Attack Against Text Generation Models.](http://arxiv.org/abs/2305.07406) | 本文扩展了模型劫持攻击的范围，提出了一种名为Ditto的攻击方法，能够将不同的文本分类任务劫持为多个生成任务，并使用多个基准数据集验证了攻击的成功性。 |
| [^42] | [Color Deconvolution applied to Domain Adaptation in HER2 histopathological images.](http://arxiv.org/abs/2305.07404) | 本文提出了一种结合色彩去卷积技术和Pix2Pix GAN网络的新方法，解决不同HER2品牌之间颜色变化的问题，并保持细胞的HER2分数，从而改善了肿瘤诊断效果。 |
| [^43] | [One-step Bipartite Graph Cut: A Normalized Formulation and Its Application to Scalable Subspace Clustering.](http://arxiv.org/abs/2305.07386) | 本研究提出了一种新的一步二分图切割准则（OBCut），并证明了其与一个迹最大化问题的等价性。该方法避免了使用k-means进行后处理，并可以应用于可扩展子空间聚类问题中。 |
| [^44] | [Surfacing Biases in Large Language Models using Contrastive Input Decoding.](http://arxiv.org/abs/2305.07378) | 本文介绍了一种叫做对比输入解码（CID）的解码算法，用于生成给定两个输入的文本，能够揭示大型语言模型中可能存在的微妙偏见和输出差异。 |
| [^45] | [DAISM: Digital Approximate In-SRAM Multiplier-based Accelerator for DNN Training and Inference.](http://arxiv.org/abs/2305.07376) | 本论文提出了一种基于多项式近似的SRAM内数字乘法器，在不依赖于新型存储技术和避免了比特串行计算的情况下，通过内存执行GEMM计算，从而为DNN训练和推理提供了高效的加速器。 |
| [^46] | [Decentralized Learning over Wireless Networks: The Effect of Broadcast with Random Access.](http://arxiv.org/abs/2305.07368) | 本文研究了分散式学习在无线网络中的通信问题，发现优化接入概率以最大化成功链路数的期望值是提升系统收敛速度的有效策略。 |
| [^47] | [S-REINFORCE: A Neuro-Symbolic Policy Gradient Approach for Interpretable Reinforcement Learning.](http://arxiv.org/abs/2305.07367) | S-REINFORCE是一种神经符号策略梯度方法，利用神经网络和符号回归器生成数字和符号策略，从而为强化学习任务提供可解释解决方案。 |
| [^48] | [Model-based Programming: Redefining the Atomic Unit of Programming for the Deep Learning Era.](http://arxiv.org/abs/2305.07341) | 本文提出一种新的编程范式——基于模型的编程，旨在解决深度学习模型部署过程中的问题。推出的M语言将模型作为基本的计算单位，加强了开发人员进行关键任务的效率，这种创新的编程范式将彻底改变我们使用深度学习模型的方式。 |
| [^49] | [Locking and Quacking: Stacking Bayesian model predictions by log-pooling and superposition.](http://arxiv.org/abs/2305.07334) | 本文提出了两种新的贝叶斯模型组合工具，可以通过对数池化和超叠加来组合后验密度，避免了标准化常数的负担，并在预测准确性方面优于传统方法。 |
| [^50] | [ActUp: Analyzing and Consolidating tSNE and UMAP.](http://arxiv.org/abs/2305.07320) | 本文分析了tSNE和UMAP的参数空间，并发现归一化参数可以在两者之间切换，并提出了一种方法(\ourmethod)可以结合之前不兼容的tSNE和UMAP技术，即可以复制任意算法的结果，同时还可以加速获得UMAP的输出结果。 |
| [^51] | [$\partial\mathbb{B}$ nets: learning discrete functions by gradient descent.](http://arxiv.org/abs/2305.07315) | 研究人员提出了一种通过梯度下降学习离散函数的可微分神经网络 $\partial\mathbb{B}$ 网络，它通过软网络和硬网络相结合的方式实现学习到的离散函数具有可解释性和较高的精度。 |
| [^52] | [Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions.](http://arxiv.org/abs/2305.07303) | 本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。 |
| [^53] | [A Central Asian Food Dataset for Personalized Dietary Interventions, Extended Abstract.](http://arxiv.org/abs/2305.07257) | 这项工作创建了一个可靠的中亚地区食品数据集，包含42个类别和超过16,000张属于该地区独特的国家美食的图像，并达到了88.70％（42个类别）的分类准确率。 |
| [^54] | [Machine-learning-accelerated simulations enable heuristic-free surface reconstruction.](http://arxiv.org/abs/2305.07251) | 本文提出了一种双重计算环路的方法来预测多组分材料的表面相图，通过机器学习交互作用势加速了能量评分和统计采样方法，在NiAl (110)上预测了新颖的表面结构。 |
| [^55] | [Quantile-Based Deep Reinforcement Learning using Two-Timescale Policy Gradient Algorithms.](http://arxiv.org/abs/2305.07248) | 本文探讨了优化累积奖励分位数的强化学习设置，提出了QPO和其变体QPPO算法，并使用神经网络对控制动作的策略进行参数化。实验结果表明该算法优于现有基线算法。 |
| [^56] | [Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation.](http://arxiv.org/abs/2305.07247) | 本论文提出了一种基于近似投影的Schr\"odinger bridge算法，它能够应用于概率时间序列填充，并在医疗保健和环境数据方面实现最先进的结果。 |
| [^57] | [On the Optimality of Misspecified Kernel Ridge Regression.](http://arxiv.org/abs/2305.07241) | 本文证明当使用Sobolev RKHS时，对于误匹配核岭回归问题，KRR对于任何$s\in (0,1)$都是最优的 |
| [^58] | [Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks.](http://arxiv.org/abs/2305.07216) | 本文提出了一个通用视听学习（VAVL）框架，可用于处理情感回归和情感分类任务中的单模态和多模态系统，即使数据缺失或不匹配也能进行有效训练和切换。 |
| [^59] | [Rethinking k-means from manifold learning perspective.](http://arxiv.org/abs/2305.07213) | 该论文从流形学习的角度出发，重新思考k-means算法，提出了一种新的聚类算法，可以直接检测数据的聚类而不需要均值估计，并且能充分利用不同视图中的信息。 |
| [^60] | [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers.](http://arxiv.org/abs/2305.07185) | MEGABYTE是一种基于多尺度Transformer的解码器架构，能够对超过一百万字节的序列进行端到端的可微建模，在训练和生成过程中提高了性能并降低了成本，同时证明了在大规模上下文无需标记的自回归序列建模的可行性。 |
| [^61] | [Boosting Value Decomposition via Unit-Wise Attentive State Representation for Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2305.07182) | 本文提出了一种基于UNSR的简单而强大的价值分解方法，缓解了多智能体协作强化学习中的局部可观察性问题，采用多头注意力机制提供有效的信用分配，实现了最先进的性能。 |
| [^62] | [Automatic Radiology Report Generation by Learning with Increasingly Hard Negatives.](http://arxiv.org/abs/2305.07176) | 本文提出了一种通过学习不断困难负样本的方法实现自动放射学报告生成的框架，以获得更具判别能力的特征，从而避免产生不期望或不匹配的报告。 |
| [^63] | [Towards Understanding and Improving GFlowNet Training.](http://arxiv.org/abs/2305.07170) | 该论文研究了生成流网络的训练问题，提出了三种改进方法，包括优先回放训练、相对边缘流、引导轨迹平衡目标，并在大幅度提高样本效率的同时改善了代理训练的效果。 |
| [^64] | [OneCAD: One Classifier for All image Datasets using multimodal learning.](http://arxiv.org/abs/2305.07167) | 本文提出了一种训练和推断框架OneCAD，通过Mask-Image-Modeling(MIM)和多模态学习解决了当前架构(如ViTs和CNNs)存在的问题，并创建了一种可以适用于所有图像数据集且与类别数无关的DNN模型架构。 |
| [^65] | [Learning-Augmented Online Packet Scheduling with Deadlines.](http://arxiv.org/abs/2305.07164) | 本研究提出了一种学习增强的在线数据包调度算法，能有效解决网络缓冲区管理问题。 |
| [^66] | [A Deep Learning-based Compression and Classification Technique for Whole Slide Histopathology Images.](http://arxiv.org/abs/2305.07161) | 本文提出了一种能够压缩组织学图像并保留更有意义表征的自编码器压缩神经网络，用于全组织切片图像。测试结果证明其有效性。 |
| [^67] | [Enhancing Petrophysical Studies with Machine Learning: A Field Case Study on Permeability Prediction in Heterogeneous Reservoirs.](http://arxiv.org/abs/2305.07145) | 本实例研究利用机器学习算法中的ANN、RFC和SVM预测渗透率曲线，并将其与岩心数据相匹配，结果表明FZI方法和机器学习算法在改善储层预测方面是有效的。 |
| [^68] | [The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain.](http://arxiv.org/abs/2305.07141) | 本研究提出了一个新的基准数据集ConceptARC，针对ARC领域的抽象和推理问题进行了深入评估，以提高人工智能系统的抽象和泛化能力。 |
| [^69] | [Promise and Limitations of Supervised Optimal Transport-Based Graph Summarization via Information Theoretic Measures.](http://arxiv.org/abs/2305.07138) | 本文介绍了一个最优输运框架的图形概括方法，可以将节点、边缘和属性重要性纳入概括过程中。为解决受监督图形概括问题，本文将其制定为最大化所概括图形与类标签之间的香农互信息的问题，并找到了其近似的NP难问题。 |
| [^70] | [Tackling Interpretability in Audio Classification Networks with Non-negative Matrix Factorization.](http://arxiv.org/abs/2305.07132) | 本文通过引入非负矩阵分解(NMF)的解释器设计，提出了一种具有高性能的本质上可解释模型，解决了音频分类网络的可解释性问题。 |
| [^71] | [Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques.](http://arxiv.org/abs/2305.07116) | 本文研究了隐私增强技术对机器学习模型准确性和能量消耗的影响，发现k-匿名化训练的模型准确性较低，但能源消耗较少，合成数据是一种有潜力的选择，因为它能够在能源消耗更少的情况下取得可比的准确性。 |
| [^72] | [$\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks.](http://arxiv.org/abs/2305.07100) | 本文提出了$\mathrm{E}(n)$等变消息传递单纯网络(EMPSNs)，一种同时将消息传递单纯网络和$\mathrm{E}(n)$等变图神经网络的优势结合，在处理高维数据时利用几何信息防止过度平滑的方法。 |
| [^73] | [Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales.](http://arxiv.org/abs/2305.07095) | 该论文研究了机器产生的自然语言理由对人类是否有用，发现现有理由的人类效用远低于理想状态，并提出通过估计理由在回答给定问题中的有用性来提高机器生成理由的人类效用。 |
| [^74] | [Stability and Convergence of Distributed Stochastic Approximations with large Unbounded Stochastic Information Delays.](http://arxiv.org/abs/2305.07091) | 本文通过引入信息年龄过程，将Borkar-Meyn稳定性定理推广到具有任意矩界的信息延迟的分布式随机逼近中，并讨论了分布式基于梯度的优化和分析SA的新方法。 |
| [^75] | [HINT: Hierarchical Mixture Networks For Coherent Probabilistic Forecasting.](http://arxiv.org/abs/2305.07089) | HINT是一种用于概率预测的新型模型族，能够有效、准确地进行一致性预测，通过引入Bootstrap方法并为网络加入规范化特征提取和输出规范化来保证其性能，在多个数据集上的预测精度比现有技术更高。 |
| [^76] | [Sequential Experimental Design for Spectral Measurement: Active Learning Using a Parametric Model.](http://arxiv.org/abs/2305.07040) | 本文提出了一种利用参数模型的主动学习方法，可用于光谱测量的顺序实验设计，提高了实验效率，缩短了测量时间。 |
| [^77] | [Value Iteration Networks with Gated Summarization Module.](http://arxiv.org/abs/2305.07039) | 本文提出了一种具有门控汇总模块的值迭代网络（GS-VIN）来解决值迭代网络在处理更大的输入地图和减轻累积误差方面的挑战。通过自适应迭代策略和门控汇总模块，这种模型可以在保持准确性的同时，减少网络深度并提高训练稳定性。 |
| [^78] | [Revealing Patterns of Symptomatology in Parkinson's Disease: A Latent Space Analysis with 3D Convolutional Autoencoders.](http://arxiv.org/abs/2305.07038) | 本文提出一种使用卷积自编码器(CVAEs)检测和量化DaT的浓度及其空间模式的新方法，结合回归算法将其与不同症状类别相关联，成功地将UPDRS与低维表示相关联，有望在PD的早期诊断及理解神经退行性和症状学方面发挥重要作用。 |
| [^79] | [Rethink Depth Separation with Intra-layer Links.](http://arxiv.org/abs/2305.07037) | 添加内部层连接可以显著提高网络的表示能力，并修改深度分离理论，使得带有内部层连接的浅层网络可以表示深层网络的一些困难函数。 |
| [^80] | [GFlowNets with Human Feedback.](http://arxiv.org/abs/2305.07036) | GFlowNets框架通过人类反馈来改善AI模型的探索能力，通过适应不同轨迹上的人类评估，可以学习到严格与人类评级成比例的策略，实验结果表明比RLHF更出色。 |
| [^81] | [Quran Recitation Recognition using End-to-End Deep Learning.](http://arxiv.org/abs/2305.07034) | 本文提出了一种基于端到端深度学习模型，使用CTC作为目标函数，来识别古兰经的朗诵。采用公共数据集进行实验。 |
| [^82] | [Hawkes Process based on Controlled Differential Equations.](http://arxiv.org/abs/2305.07031) | 本文提出了一种基于控制微分方程的Hawkes过程模型，可精确计算对数似然，并能够正确处理不规则时间序列，适用于社会扩散和地震预测。 |
| [^83] | [An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation.](http://arxiv.org/abs/2305.06924) | 本论文提出了一种基于模仿学习的算法，利用先验知识和与变化环境的交互实现了GENCO的投标策略优化和贝叶斯纳什均衡估计，针对现代电力市场中先验知识未被充分利用导致现有方法不准确和低效的问题进行了改进。 |
| [^84] | [How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications.](http://arxiv.org/abs/2305.06921) | 本文开发了一种基于强化学习的模拟方法来联合设计电力市场，详细阐述了设计电力现货市场、辅助服务市场中的保留能力产品和金融市场中的虚拟竞标产品的方法，并通过案例研究演示了如何选择最佳市场设计选项。 |
| [^85] | [ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps.](http://arxiv.org/abs/2305.06472) | 该论文综述了基于大规模基础模型（LSF-Models）如ChatGPT和DALLE-E的人工智能（AI）技术在预测与健康管理（PHM）中的广泛应用。这种技术可以实现多模态、多任务、大量数据和超大模型范式，成为AI-2.0的新时代的标志之一。 |
| [^86] | [Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation.](http://arxiv.org/abs/2305.06446) | 该论文探讨了多智能体强化学习在情节式马尔可夫决策过程中的协作问题，提出了一种基于值迭代的算法，可以实现异步通信，在保证合作优势的同时降低通信开销。通过提供和证明的算法和复杂度界限，为多智能体强化学习在实际应用中提供理论依据。 |
| [^87] | [Phase transitions in the mini-batch size for sparse and dense neural networks.](http://arxiv.org/abs/2305.06435) | 本文系统地研究了小批量大小对稀疏和密集神经网络训练的影响，发现在临界值处会出现尖锐的相变，阐明了神经网络优化的基本机制。 |
| [^88] | [Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments.](http://arxiv.org/abs/2305.06026) | 本文提出了一种GNN学习环境下的社区检测算法比较框架，包括数据集和评估指标，以解决目前文献中对于基于GNN的社区检测缺乏公平且严谨评估的问题。 |
| [^89] | [Random Smoothing Regularization in Kernel Gradient Descent Learning.](http://arxiv.org/abs/2305.03531) | 本文提出了一种随机平滑正则化的框架，能够自适应地、有效地学习属于经典Sobolev空间范围内的各种真实函数，通过引入噪声避免过拟合，该方法可以在较快的速度下实现最优收敛率。 |
| [^90] | [How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 1: A Paradigmatic Theory.](http://arxiv.org/abs/2305.02485) | 本文提出基于强化学习的方法，设计联合市场以应对电力行业脱碳，实现电力系统的安全和经济效益，并为环境做出贡献。该范型理论的框架将在两部分中详细介绍。 |
| [^91] | [GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content.](http://arxiv.org/abs/2305.02422) | GAMIVAL是一种新型的游戏专用无参考视频质量评估模型，结合了多种优点。在移动云游戏内容的主观质量评估数据库上进行测试，表现出更好的NR VQA性能。 |
| [^92] | [Representation Learning via Manifold Flattening and Reconstruction.](http://arxiv.org/abs/2305.01777) | 本文提出了一种通过神经网络构建展平流形的算法FlatNet，具有可解释性和可扩展性，同时在测试数据上具有良好的泛化性能。 |
| [^93] | [Stratified Adversarial Robustness with Rejection.](http://arxiv.org/abs/2305.01139) | 本文提出了一种新的防御方法——基于一致预测的拒绝对抗训练（CPR），用于构建鲁棒的选择性分类器。该方法可以在分层拒绝设置下进行对抗鲁棒分类，并且在实验中表现出很好的性能。 |
| [^94] | [A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization.](http://arxiv.org/abs/2304.08914) | 本文提出了广义神经崩溃假设，发现了Grassmannian Frame结构和对称泛化现象，这对特征选择和神经网络设计都具有重要作用。 |
| [^95] | [Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance.](http://arxiv.org/abs/2304.06715) | 本文提出了解释不变性和等变性的概念，通过对对称群下具有不变性的神经网络，建立两种度量方法来提高解释方法对于不变性的健壮性并证明为一些流行的解释方法提供了理论健壮性保证。 |
| [^96] | [Understanding plasticity in neural networks.](http://arxiv.org/abs/2303.01486) | 本文通过对失去可塑性问题进行系统实证分析，发现其深度与损失梯度曲率变化密切相关，饱和单元或发散梯度范数并非原因。基于这一发现，识别了一系列参数化和优化方法，有效提高神经网络保持可塑性的能力，在深度强化学习问题中具有显著的适应性和鲁棒性。 |
| [^97] | [Local Causal Discovery for Estimating Causal Effects.](http://arxiv.org/abs/2302.08070) | 本文介绍了一种新的本地因果关系发现算法 LDECC，可以提高算法的效率，实现因果效应估计。 |
| [^98] | [Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks.](http://arxiv.org/abs/2302.07260) | 本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。 |
| [^99] | [Robust and Scalable Bayesian Online Changepoint Detection.](http://arxiv.org/abs/2302.04759) | 该论文提出了一种在线变点检测方法，具有坚固和可扩展性，通过利用广义贝叶斯视角提供了可证明的坚固性，并通过扩散得分匹配解决了可扩展性问题。所得算法是精确的，更新简单，速度较之前的算法快10倍以上。 |
| [^100] | [GPS++: Reviving the Art of Message Passing for Molecular Property Prediction.](http://arxiv.org/abs/2302.02947) | GPS++是一种混合消息传递神经网络/图形转换器模型，通过集成本地消息传递组件和全局关注，以及过去文献中的其他关键思想，在分子属性预测方面实现了最先进的结果。 |
| [^101] | [Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning.](http://arxiv.org/abs/2302.02662) | 本文研究了一种名为GLAM的方法，通过功能基础设施建设，利用在线强化学习提高LLM代理程序的性能来实现LLMs与环境之间的对齐，解决决策问题。 |
| [^102] | [AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners.](http://arxiv.org/abs/2302.01877) | 本文介绍了AdaptDiffuser方法,通过使用奖励梯度的指导生成富有丰富的目标条件任务的合成专家数据，并通过鉴别器选择高质量数据来微调扩散模型，从而改善对未知任务的泛化能力。 |
| [^103] | [Inapplicable Actions Learning for Knowledge Transfer in Reinforcement Learning.](http://arxiv.org/abs/2211.15589) | 本文提出了一种新的强化学习方法，在学习过程中通过奖励设计让智能体学习屏蔽无关的动作，从而降低了样本数量，并获得了比传统RL方法更好的最优策略。 |
| [^104] | [Learning Coherent Clusters in Weakly-Connected Network Systems.](http://arxiv.org/abs/2211.15301) | 本文提出一种学习协同聚类方法，通过基于图拉普拉斯矩阵的谱聚类算法识别协同群组，构建减少网络，并给出逼近误差上界。 |
| [^105] | [GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective.](http://arxiv.org/abs/2211.08073) | 本文提出了第一个创建名为方法的统一基准的尝试，用于评估NLP模型中的OOD鲁棒性，该基准包括13个公开可用的OOD测试数据集，并在21个常用的PLMs上对8个经典NLP任务进行评估。 |
| [^106] | [Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning.](http://arxiv.org/abs/2211.03044) | 该论文通过调整预训练语言模型生成大量新的训练样本，从而增强原始训练集，提高了少样本学习的性能。 |
| [^107] | [Graph Neural Modeling of Network Flows.](http://arxiv.org/abs/2209.05208) | 本文提出了一种新颖的网络流问题图学习架构 PEW，相较于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。 |
| [^108] | [Applications of Reinforcement Learning in Deregulated Power Market: A Comprehensive Review.](http://arxiv.org/abs/2205.08369) | 本文综述了RL在去监管电力市场中应用的最新研究成果和未来的研究方向，RL相较于传统优化工具具有优势，能够解决特征不确定、计算效率低等难题，是一种有效的方法。 |
| [^109] | [A Comprehensive Survey on Model Quantization for Deep Neural Networks.](http://arxiv.org/abs/2205.07877) | 本文综述了深度神经网络中的模型量化，这是一种用低位宽存储全精度值以实现节约内存和操作成本的压缩方法。文章分类介绍了各种量化方法，并讨论了使用比例因子匹配数据范围和适当的训练方法的重要性。本文还回顾了模型量化的最新研究，并强调了其优缺点和当前的挑战和未来研究方向。 |
| [^110] | [Hierarchical Bayesian Modelling for Knowledge Transfer Across Engineering Fleets via Multitask Learning.](http://arxiv.org/abs/2204.12404) | 本文提出了一种分层贝叶斯建模方法，利用操作机群中的领域专业知识，在不同的子群之间自动地共享信息。该方法成功地解决了卡车机群的生存分析和风电场的功率预测问题。 |
| [^111] | [BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification.](http://arxiv.org/abs/2203.01937) | 本文提出了一种适用于多标签、嘈杂CXR学习的方法，使用基于袋的多标签描述符平滑地重新标记数据集中的样本，并进行训练以提高模型性能。 |
| [^112] | [Transformers in Time Series: A Survey.](http://arxiv.org/abs/2202.07125) | 本文针对Transformer在时间序列建模中的应用进行了系统回顾，总结了其优点及局限性，从网络结构和应用两个角度审视了其适应和改进。 |
| [^113] | [Models for information propagation on graphs.](http://arxiv.org/abs/2201.07577) | 本文提出了统一的图上信息传播模型，其中包括三种不同的类别，利用波、路径行程时间和eikonal方程来描述信息的传播，并给出了它们之间的等价性。此外，本文还提出了一种新的混合模型，用于描述波和eikonal模型的结合。作者在随机图形、小世界图和实际网络上进行了数值模拟。 |
| [^114] | [Connection Sensitivity Matters for Training-free DARTS: From Architecture-Level Scoring to Operation-Level Sensitivity Analysis.](http://arxiv.org/abs/2106.11542) | 本论文提出一种叫做ZEROS的连接概念来评估DARTS中的操作重要性，使得整个架构的搜索过程更高效，提出了一种基于NTK理论的全新框架FreeDARTS。 |
| [^115] | [Linear Classifiers Under Infinite Imbalance.](http://arxiv.org/abs/2106.05797) | 研究了在无限不平衡情况下的线性分类器，通过权重函数指定的经验损失最小化系数。截距发散但其余系数向量有一个有限的几乎肯定的极限，极限依赖于权重函数的左尾增长速率。极限系数向量反映稳健性或保守性属性，而在亚指数情况下，极限等价于少数类的上采样分布的隐式选择。 |
| [^116] | [Aleatoric uncertainty for Errors-in-Variables models in deep regression.](http://arxiv.org/abs/2105.09095) | 本文提出了一种基于贝叶斯深度回归的方法，利用变量误差模型考虑所使用神经网络的输入所关联的不确定性，并将预测不确定性分解为随机和认识部分。相比于不使用该模型，使用错误变量模型能够提高对已知回归函数的覆盖率，且保持预测性能。 |
| [^117] | [The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information.](http://arxiv.org/abs/2102.10019) | 本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。 |
| [^118] | [The Power of Linear Recurrent Neural Networks.](http://arxiv.org/abs/1802.03308) | 本研究展示了线性递归神经网络(LRNNs)可以逼近任何时变函数f(t)。通过检查网络转移矩阵的主要特征值，可以显著降低LRNN的规模。LRNNs具有以椭圆轨迹结束的有趣特性，并允许预测进一步的值和函数的紧凑表示。 |

# 详细

[^1]: 警惕扩散模型合成医学图像 -- 与 GAN 在记忆脑肿瘤图像方面的比较。

    Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images. (arXiv:2305.07644v1 [eess.IV])

    [http://arxiv.org/abs/2305.07644](http://arxiv.org/abs/2305.07644)

    扩散模型在医学图像合成中可能会导致记忆训练图像的问题，研究人员在选择合适的模型时需要谨慎。

    

    扩散模型最初是为文本到图像生成而开发的，现在也被用于生成高质量的合成图像。在 GAN 之前，扩散模型已经展示了令人印象深刻的结果，使用了各种评估指标。然而，常用的指标如 FID 和 IS 并不适合确定扩散模型是否只是复制了训练图像。这里我们使用 BRATS20 和 BRATS21 数据集训练 StyleGAN 和扩散模型，生成脑肿瘤图像，并测量合成图像与所有训练图像之间的相关性。我们的结果表明，扩散模型更有可能记忆训练图像，特别是对于小数据集。如果最终目标是共享合成的图像，研究人员在使用扩散模型进行医学成像时应该小心。

    Diffusion models were initially developed for text-to-image generation and are now being utilized to generate high quality synthetic images. Preceded by GANs, diffusion models have shown impressive results using various evaluation metrics. However, commonly used metrics such as FID and IS are not suitable for determining whether diffusion models are simply reproducing the training images. Here we train StyleGAN and diffusion models, using BRATS20 and BRATS21 datasets, to synthesize brain tumor images, and measure the correlation between the synthetic images and all training images. Our results show that diffusion models are much more likely to memorize the training images, especially for small datasets. Researchers should be careful when using diffusion models for medical imaging, if the final goal is to share the synthetic images.
    
[^2]: ASNR-MICCAI脑肿瘤分割挑战2023：颅内脑膜瘤

    The ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2023: Intracranial Meningioma. (arXiv:2305.07642v1 [cs.CV])

    [http://arxiv.org/abs/2305.07642](http://arxiv.org/abs/2305.07642)

    ASNR-MICCAI脑肿瘤分割挑战2023将提供一个适用于自动诊断颅内脑膜瘤的最先进自动化颅内脑膜瘤分割模型的基准。

    

    脑膜瘤是成人颅内最常见的原发性肿瘤，可能与重大的发病率和死亡率有关。放射科医生、神经外科医生、神经肿瘤学家和放射肿瘤科医生依靠多参数MRI（mpMRI）进行诊断、治疗规划和长期治疗监测；然而，缺乏自动化、客观化和定量化的工具来对mpMRI中的脑膜瘤进行非侵入性评估。BraTS脑膜瘤2023挑战将提供一个社区标准和基于迄今为止最大的专家注释的多标签脑膜瘤mpMRI数据集的最先进自动化颅内脑膜瘤分割模型的基准。挑战参赛者将开发自动化分割模型，预测MRI上的三个不同的脑膜瘤亚区域，包括增强肿瘤、非增强肿瘤核心和周围无增强T2/FLAIR高信号区。模型将使用标准化指标在单独的验证和保留测试数据集上进行评估。

    Meningiomas are the most common primary intracranial tumor in adults and can be associated with significant morbidity and mortality. Radiologists, neurosurgeons, neuro-oncologists, and radiation oncologists rely on multiparametric MRI (mpMRI) for diagnosis, treatment planning, and longitudinal treatment monitoring; yet automated, objective, and quantitative tools for non-invasive assessment of meningiomas on mpMRI are lacking. The BraTS meningioma 2023 challenge will provide a community standard and benchmark for state-of-the-art automated intracranial meningioma segmentation models based on the largest expert annotated multilabel meningioma mpMRI dataset to date. Challenge competitors will develop automated segmentation models to predict three distinct meningioma sub-regions on MRI including enhancing tumor, non-enhancing tumor core, and surrounding nonenhancing T2/FLAIR hyperintensity. Models will be evaluated on separate validation and held-out test datasets using standardized metri
    
[^3]: 使用压缩感知和群组检测的高效神经网络图像分类和离群检测

    Efficient Neural Network based Classification and Outlier Detection for Image Moderation using Compressed Sensing and Group Testing. (arXiv:2305.07639v1 [cs.CV])

    [http://arxiv.org/abs/2305.07639](http://arxiv.org/abs/2305.07639)

    该论文提出了一种利用压缩感知技术和神经群组测试的方法，可以有效减少图像审核引擎的计算成本，提高图像审核效率。

    

    流行的社交媒体平台利用基于神经网络的图像审核引擎，对上传的图片进行可能存在问题内容的分类。这样的审核引擎必须回答大量查询并具有重计算成本，尽管具有问题内容的实际图像数量通常只是微不足道的一部分。受神经群组测试的最新工作启发，我们提出了一种利用压缩感知技术来减少此类引擎的整体计算成本的方法。我们提出了定量矩阵池化神经网络（QMPNN），其输入为n个图像和一个m×n的二元汇总矩阵，其中m<n，行表示m个图像池，即从n个图像中选择r个图像的选择。 QMPNN有效地输出该矩阵与表示每个图像是否存在问题的未知稀疏二进制向量的乘积，即输出每个池中存在问题的图像数量。

    Popular social media platforms employ neural network based image moderation engines to classify images uploaded on them as having potentially objectionable content. Such moderation engines must answer a large number of queries with heavy computational cost, even though the actual number of images with objectionable content is usually a tiny fraction. Inspired by recent work on Neural Group Testing, we propose an approach which exploits this fact to reduce the overall computational cost of such engines using the technique of Compressed Sensing (CS). We present the quantitative matrix-pooled neural network (QMPNN), which takes as input $n$ images, and a $m \times n$ binary pooling matrix with $m < n$, whose rows indicate $m$ pools of images i.e. selections of $r$ images out of $n$. The QMPNN efficiently outputs the product of this matrix with the unknown sparse binary vector indicating whether each image is objectionable or not, i.e. it outputs the number of objectionable images in each 
    
[^4]: Text2Cohort: 自然语言队列发现对癌症影像数据共享平台的民主化

    Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])

    [http://arxiv.org/abs/2305.07637](http://arxiv.org/abs/2305.07637)

    Text2Cohort是一个基于大语言模型的工具箱，可以将用户输入转化为IDC数据库查询，促进自然语言队列发现，减少研究人员查询IDC数据库的学习曲线，实现了癌症成像数据的民主化。

    

    影像数据共享平台(IDC)是一个基于云的数据库，为研究人员提供开放获取的癌症成像数据和分析工具，旨在促进医学成像研究中的协作。然而，由于其复杂和技术性质，查询IDC数据库以进行队列发现和访问成像数据对研究人员来说具有显著的学习曲线。我们开发了基于大语言模型（LLM）的Text2Cohort工具箱，通过提示工程将用户输入转化为IDC数据库查询，并将查询的响应返回给用户，以促进自然语言队列发现。此外，实现了自动校正以解决查询中的语法和语义错误，通过将错误传回模型进行解释和校正。我们对50个自然语言用户输入进行了Text2Cohort评估，范围从信息提取到队列发现。结果查询和输出由两位计算机科学家进行了确认。

    The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data and tools for analysis, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex and technical nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate natural language cohort discovery by translating user input into IDC database queries through prompt engineering and returning the query's response to the user. Furthermore, autocorrection is implemented to resolve syntax and semantic errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to me
    
[^5]: 基于多任务产品知识图谱预训练的零样本基于项目推荐

    Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training. (arXiv:2305.07633v1 [cs.IR])

    [http://arxiv.org/abs/2305.07633](http://arxiv.org/abs/2305.07633)

    本论文提出了一种使用产品知识图谱预训练模型从预训练的语言模型提取项目特征，以解决零样本项目推荐任务。该方法通过提出四个预训练任务和任务导向的适应层来解决预训练过程中的挑战，并将模型微调到新的推荐任务中。

    

    现有的推荐系统在处理零样本项目（即在训练阶段没有与用户进行过历史互动的项目）时面临困难。虽然最近的工作通过预训练语言模型（PLM）提取通用项目表示，但它们忽略了关键的项目关系。本文提出了一种新的方法，使用产品知识图谱（PKG）对模型进行预训练，以从PLMs中提炼出项目特征来解决零样本项目推荐（ZSIR）任务。我们确定了预训练PKG的三个挑战，即PKG中的多类型关系，项目通用信息和关系之间的语义差异以及从PKG到下游ZSIR任务的域差异。我们通过提出四个预训练任务和新颖的面向任务的适应（ToA）层来解决这些挑战。此外，本文还讨论了如何对新的推荐任务进行微调，使得ToA层适应于ZSIR任务。在18个市场数据集上进行了全面实验。

    Existing recommender systems face difficulties with zero-shot items, i.e. items that have no historical interactions with users during the training stage. Though recent works extract universal item representation via pre-trained language models (PLMs), they ignore the crucial item relationships. This paper presents a novel paradigm for the Zero-Shot Item-based Recommendation (ZSIR) task, which pre-trains a model on product knowledge graph (PKG) to refine the item features from PLMs. We identify three challenges for pre-training PKG, which are multi-type relations in PKG, semantic divergence between item generic information and relations and domain discrepancy from PKG to downstream ZSIR task. We address the challenges by proposing four pre-training tasks and novel task-oriented adaptation (ToA) layers. Moreover, this paper discusses how to fine-tune the model on new recommendation task such that the ToA layers are adapted to ZSIR task. Comprehensive experiments on 18 markets dataset ar
    
[^6]: 电容传感设备的敏捷手势识别：在工作中的适应

    Agile gesture recognition for capacitive sensing devices: adapting on-the-job. (arXiv:2305.07624v1 [cs.LG])

    [http://arxiv.org/abs/2305.07624](http://arxiv.org/abs/2305.07624)

    该论文介绍了一种使用电容传感器信号识别手势的技术，通过机器学习技术能够在低功耗设备上实现，并在500毫秒内能够确定5个手指的三个特征，将手势的识别效率提高了。

    

    自动手势识别是人工智能社区几十年来的关注焦点。传统上，该领域的研究主要围绕假设用户手部图像流的可用性展开。这部分原因在于基于相机的设备的普及和图像数据的广泛可用性。然而，越来越多的需求需要手势识别技术，该技术可以使用有限的传感器数据而不是高维度的输入(如手部图像)来实现在低功耗设备上。在这项工作中，我们展示了一种手势识别系统和方法，它利用了嵌入到etee手柄控制器中的电容传感器的信号。该控制器从佩戴者的五个手指中产生实时信号。我们使用机器学习技术来分析时序信号，并确定可以在500毫秒内表示5个手指的三个特征。分析由两个阶段的培训策略组成，包括通过主成分分析进行的降维

    Automated hand gesture recognition has been a focus of the AI community for decades. Traditionally, work in this domain revolved largely around scenarios assuming the availability of the flow of images of the user hands. This has partly been due to the prevalence of camera-based devices and the wide availability of image data. However, there is growing demand for gesture recognition technology that can be implemented on low-power devices using limited sensor data instead of high-dimensional inputs like hand images. In this work, we demonstrate a hand gesture recognition system and method that uses signals from capacitive sensors embedded into the etee hand controller. The controller generates real-time signals from each of the wearer five fingers. We use a machine learning technique to analyse the time series signals and identify three features that can represent 5 fingers within 500 ms. The analysis is composed of a two stage training strategy, including dimension reduction through pr
    
[^7]: 基于局部Lipschitz度量的深度学习图像重建的不确定性估计

    Uncertainty Estimation for Deep Learning Image Reconstruction using a Local Lipschitz Metric. (arXiv:2305.07618v1 [cs.CV])

    [http://arxiv.org/abs/2305.07618](http://arxiv.org/abs/2305.07618)

    本文提出了一种基于局部Lipschitz度量的方法，可以用于估计深度学习图像重建模型的不确定性。该方法可用于确定特定深度学习重建方法的适用性，识别分布外的测试样本并指导适当的数据增强。

    

    深度学习在医学成像中广泛应用于解决与成像相关的反问题，尤其是图像重建。在模型部署时，可能会遇到与训练数据差异较大的输入分布，部分原因是数据偏差或漂移。本文提出一种基于从单个训练模型中确定的局部Lipschitz度量的方法，可以用于估计图像重建的模型不确定性。我们展示了局部Lipschitz值与平均绝对误差之间的单调关系，并表明可以使用此方法提供确定是否适合特定深度学习重建方法的阈值。我们的不确定性估计方法可用于识别分布外的测试样本，关联关于认识不确定性的信息，并指导适当的数据增强。在医学成像应用中特别重要的是，量化学习重构方法的不确定性，因为诊断和治疗可能会受到重建图像的准确性和可靠性的影响。

    The use of deep learning approaches for image reconstruction is of contemporary interest in radiology, especially for approaches that solve inverse problems associated with imaging. In deployment, these models may be exposed to input distributions that are widely shifted from training data, due in part to data biases or drifts. We propose a metric based on local Lipschitz determined from a single trained model that can be used to estimate the model uncertainty for image reconstructions. We demonstrate a monotonic relationship between the local Lipschitz value and Mean Absolute Error and show that this method can be used to provide a threshold that determines whether a given DL reconstruction approach was well suited to the task. Our uncertainty estimation method can be used to identify out-of-distribution test samples, relate information regarding epistemic uncertainties, and guide proper data augmentation. Quantifying uncertainty of learned reconstruction approaches is especially pert
    
[^8]: 深度学习与逻辑推理的可扩展耦合

    Scalable Coupling of Deep Learning with Logical Reasoning. (arXiv:2305.07617v1 [cs.AI])

    [http://arxiv.org/abs/2305.07617](http://arxiv.org/abs/2305.07617)

    本文介绍了一种可扩展的神经网络模型和损失函数，能够有效学习如何解决NP-hard推理问题，并在离散图模型上进行了实验验证。同时可以提高数据效率和可解释性，并具有对预测的控制能力。

    

    在将离散推理与神经网络混合的不断探索中，出现了越来越多的对神经结构具备从自然输入中学习如何解决离散推理或优化问题的兴趣。本文提出了一种可扩展的神经结构以及专门用于学习被表示为离散图模型的 NP-hard 推理问题的约束和标准的损失函数。我们的损失函数解决了 Besag 的伪对数似然的主要限制之一，能够学习高能量函数。我们通过实验证明，它能够有效地从自然输入中学习如何解决 NP-hard 推理问题，如符号、视觉或多解数数独问题，以及蛋白质设计问题的能量优化形式，提高了数据效率、可解释性以及对预测的 \textit{a posteriori} 控制。

    In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs. In this paper, we introduce a scalable neural architecture and loss function dedicated to learning the constraints and criteria of NP-hard reasoning problems expressed as discrete Graphical Models. Our loss function solves one of the main limitations of Besag's pseudo-loglikelihood, enabling learning of high energies. We empirically show it is able to efficiently learn how to solve NP-hard reasoning problems from natural inputs as the symbolic, visual or many-solutions Sudoku problems as well as the energy optimization formulation of the protein design problem, providing data efficiency, interpretability, and \textit{a posteriori} control over predictions.
    
[^9]: Spider GAN:利用友好邻居加速GAN训练

    Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training. (arXiv:2305.07613v1 [cs.CV])

    [http://arxiv.org/abs/2305.07613](http://arxiv.org/abs/2305.07613)

    本文提出了一种名为Spider GAN的新方法，通过寻找数据集之间的友好邻居来提高GAN的训练效率，加速收敛，即使是不相关的数据集之间也可以发现对应关系。

    

    GAN的训练是个有挑战性的任务，本文提出了Spider GAN方法，该方法利用图像结构的特点优化生成器的转换，通过定义一种新的度量方式，即有符号启动距离（SID），使其更高效地寻找友好邻居，结果导致更快的收敛，即使在看似不相关的数据集之间也可以找到对应关系。

    Training Generative adversarial networks (GANs) stably is a challenging task. The generator in GANs transform noise vectors, typically Gaussian distributed, into realistic data such as images. In this paper, we propose a novel approach for training GANs with images as inputs, but without enforcing any pairwise constraints. The intuition is that images are more structured than noise, which the generator can leverage to learn a more robust transformation. The process can be made efficient by identifying closely related datasets, or a ``friendly neighborhood'' of the target distribution, inspiring the moniker, Spider GAN. To define friendly neighborhoods leveraging proximity between datasets, we propose a new measure called the signed inception distance (SID), inspired by the polyharmonic kernel. We show that the Spider GAN formulation results in faster convergence, as the generator can discover correspondence even between seemingly unrelated datasets, for instance, between Tiny-ImageNet 
    
[^10]: 通信压缩下的分布式随机优化中的下限和加速算法

    Lower Bounds and Accelerated Algorithms in Distributed Stochastic Optimization with Communication Compression. (arXiv:2305.07612v1 [cs.LG])

    [http://arxiv.org/abs/2305.07612](http://arxiv.org/abs/2305.07612)

    本文研究采用通信压缩的分布式随机优化算法的性能下限并提出了一种名为NEOLITHIC的新型通信压缩算法，通过加速收敛速率缩小下限和现有算法的差距。

    

    通信压缩是减轻分布式随机优化中计算节点间信息交换量的重要策略。本文研究了采用通信压缩的分布式随机优化算法的性能下限，并关注两种主要类型的压缩器：无偏和压缩型，并解决了可以通过这些压缩器获得的最佳收敛速率问题。本文针对六种不同设置，结合强凸、一般凸或非凸函数，并用无偏或压缩型压缩器建立了分布式随机优化的收敛速率下限。我们提出了一种名为NEOLITHIC的新型通信压缩算法，通过加速收敛速率相比经典方法，缩小了下限和现有算法的差距。理论分析和数值实验提供了关于分布式随机优化中通信压缩算法的最优性能的见解。

    Communication compression is an essential strategy for alleviating communication overhead by reducing the volume of information exchanged between computing nodes in large-scale distributed stochastic optimization. Although numerous algorithms with convergence guarantees have been obtained, the optimal performance limit under communication compression remains unclear.  In this paper, we investigate the performance limit of distributed stochastic optimization algorithms employing communication compression. We focus on two main types of compressors, unbiased and contractive, and address the best-possible convergence rates one can obtain with these compressors. We establish the lower bounds for the convergence rates of distributed stochastic optimization in six different settings, combining strongly-convex, generally-convex, or non-convex functions with unbiased or contractive compressor types. To bridge the gap between lower bounds and existing algorithms' rates, we propose NEOLITHIC, a n
    
[^11]: RHINO：通过匈牙利匹配实现动态降噪的旋转目标检测的旋转DETR

    RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection. (arXiv:2305.07598v1 [cs.CV])

    [http://arxiv.org/abs/2305.07598](http://arxiv.org/abs/2305.07598)

    本文提出了一种面向定向目标检测的DINO基线模型RHINO。并通过匈牙利匹配和查询对齐的方式实现动态降噪，解决了重复预测的问题，从而在公共基准测试中达到最先进的性能水平。

    

    随着DINO的发布，一种DETR的变体，检测变压器正在通过其端到端设计和可扩展性在目标检测基准中刷新记录。然而，虽然预计从其端到端架构中获得更多的好处，如消除NMS和与锚相关的成本，但尚未彻底研究DETR在定向目标检测方面的扩展。本文提出了首个面向定向目标检测的DINO基线。我们发现，直接使用DETR进行定向目标检测并不能保证不重复预测，并提出了一种简单的成本来减轻这种情况。此外，我们介绍了一种新的去噪策略，该策略使用匈牙利匹配来过滤冗余的带噪声的查询，并使用查询对齐来保持Transformer解码器层之间的匹配一致性。我们提出的模型在公共基准测试中优于以前的旋转DETR和其他对手，实现了最先进的性能。

    With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a novel denoising strategy that uses Hungarian matching to filter redundant noised queries and query alignment to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art pe
    
[^12]: MoMo: 动量模型的自适应学习率

    MoMo: Momentum Models for Adaptive Learning Rates. (arXiv:2305.07583v1 [cs.LG])

    [http://arxiv.org/abs/2305.07583](http://arxiv.org/abs/2305.07583)

    本文提出了新的自适应学习率，可与任何动量方法一起使用，通过构建损失函数模型并使用下限截断，以及即时估计未知下限，来近似最小化该模型以计算下一步，实验表明，相较于SGDM和Adam，该方法在精度和超参数调优的鲁棒性方面有所提高。

    

    本文提出了新的自适应学习率，可与任何动量方法一起使用。为了展示我们的新学习率，我们开发了MoMo和MoMo-Adam，它们是具有动量（SGDM）的SGD和Adam方法与我们的新自适应学习率一起使用。我们的MoMo方法是通过基于模型的随机优化来激发的，其中我们使用每次迭代采样的批量损失和梯度的动量估计来构建损失函数模型。我们的模型还利用了已知损失函数下限的截断方法。实际上，大多数损失都被下限为零。然后，在每次迭代中，我们近似最小化此模型以计算下一步。对于具有未知下限的损失，我们开发了新的即时下限估计，这些估计用于我们的模型中。数值实验表明，我们的MoMo方法在MNIST、CIFAR10、CIFAR100和Imagenet32等数据集的图像分类训练中，相较于SGDM和Adam，在精度和超参数调优的鲁棒性方面都有所提高。

    We present new adaptive learning rates that can be used with any momentum method. To showcase our new learning rates we develop MoMo and MoMo-Adam, which are SGD with momentum (SGDM) and Adam together with our new adaptive learning rates. Our MoMo methods are motivated through model-based stochastic optimization, wherein we use momentum estimates of the batch losses and gradients sampled at each iteration to build a model of the loss function. Our model also makes use of any known lower bound of the loss function by using truncation. Indeed most losses are bounded below by zero. We then approximately minimize this model at each iteration to compute the next step. For losses with unknown lower bounds, we develop new on-the-fly estimates of the lower bound that we use in our model. Numerical experiments show that our MoMo methods improve over SGDM and Adam in terms of accuracy and robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR10, CIFAR100, Imagenet32, 
    
[^13]: 基于Fisher信息嵌入的节点和图学习

    Fisher Information Embedding for Node and Graph Learning. (arXiv:2305.07580v1 [stat.ML])

    [http://arxiv.org/abs/2305.07580](http://arxiv.org/abs/2305.07580)

    本文提出了一种新的基于注意力机制的图节点嵌入框架，可以更好地理解基于注意力机制的GNN。

    

    基于注意力机制的图神经网络（GNN），例如图注意力网络（GAT），已成为处理图结构数据和学习节点嵌入的流行神经网络结构。尽管这些模型在经验上取得了成功，但它们依赖于标注数据，且这些模型的理论属性尚未完全理解。本文提出了一种新颖的基于注意力机制的图节点嵌入框架。我们的框架建立在一种多重集合内节点周围子图的分层核之上（例如，邻域），并且每个核利用平滑统计流形的几何来比较多重集合的成对差异，通过将多重集合“映射”到流形上。通过显式计算高斯混合物流形中的节点嵌入，我们的方法引导出了一种新的关注机制进行邻域聚合。我们提供了有关嵌入的泛化和表达能力的理论见解，为更深入理解基于注意力机制的GNN做出了贡献。

    Attention-based graph neural networks (GNNs), such as graph attention networks (GATs), have become popular neural architectures for processing graph-structured data and learning node embeddings. Despite their empirical success, these models rely on labeled data and the theoretical properties of these models have yet to be fully understood. In this work, we propose a novel attention-based node embedding framework for graphs. Our framework builds upon a hierarchical kernel for multisets of subgraphs around nodes (e.g. neighborhoods) and each kernel leverages the geometry of a smooth statistical manifold to compare pairs of multisets, by "projecting" the multisets onto the manifold. By explicitly computing node embeddings with a manifold of Gaussian mixtures, our method leads to a new attention mechanism for neighborhood aggregation. We provide theoretical insights into genralizability and expressivity of our embeddings, contributing to a deeper understanding of attention-based GNNs. We p
    
[^14]: 高斯门控混合专家模型参数估计的收敛速率研究

    Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts. (arXiv:2305.07572v1 [stat.ML])

    [http://arxiv.org/abs/2305.07572](http://arxiv.org/abs/2305.07572)

    本文提出新颖的Voronoi Loss函数来解决高斯门控混合专家模型参数估计的收敛速率问题，并在两种不同的门控网络下提供理论收敛速率的证明。

    

    混合专家模型因其在集成学习中的应用而被引入神经网络中，近年来成为现代深度神经网络中处理异构数据分析的基本构件。然而，对于高斯门控混合专家模型参数估计的收敛行为的理解还不充分。我们通过设计新颖的Voronoi Loss函数来解决这些问题，并提供了理论收敛速率的证明，揭示了在两种分离的门控网络下最大似然估计器的不同行为。

    Originally introduced as a neural network for ensemble learning, mixture of experts (MoE) has recently become a fundamental building block of highly successful modern deep neural networks for heterogeneous data analysis in several applications, including those in machine learning, statistics, bioinformatics, economics, and medicine. Despite its popularity in practice, a satisfactory level of understanding of the convergence behavior of Gaussian-gated MoE parameter estimation is far from complete. The underlying reason for this challenge is the inclusion of covariates in the Gaussian gating and expert networks, which leads to their intrinsically complex interactions via partial differential equations with respect to their parameters. We address these issues by designing novel Voronoi loss functions to accurately capture heterogeneity in the maximum likelihood estimator (MLE) for resolving parameter estimation in these models. Our results reveal distinct behaviors of the MLE under two se
    
[^15]: 一种支持核指代信息的问答流式数据记忆模型

    A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information. (arXiv:2305.07565v1 [cs.CL])

    [http://arxiv.org/abs/2305.07565](http://arxiv.org/abs/2305.07565)

    该论文提出了一种记忆模型，在处理流式数据时，通过排练和预期来记忆有关问题回答任务的重要信息。该模型应用自监督机制，通过核指代信息的屏蔽建模任务训练，成功通过短序列数据集和大型基准测试。

    

    现有的问答方法往往假设输入内容（如文件或视频）总是可访问的，以解决任务。相反，记忆网络被引入来模仿人类逐步理解和压缩信息的过程。然而，这些模型只学习如何通过整个网络反向传播错误来维护内存。相反，人类具有提高记忆容量的有效机制，例如排练和预期。受此启发，我们提出了一种记忆模型，通过排练和预期来处理输入以记忆有关问题回答任务的重要信息。所提出的机制在训练期间通过针对核指代信息的屏蔽建模任务进行自监督应用。我们在短序列（bAbI）数据集以及大型基准测试中验证了我们的模型。

    Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-s
    
[^16]: 饱和非单调激活函数

    Saturated Non-Monotonic Activation Functions. (arXiv:2305.07537v1 [cs.NE])

    [http://arxiv.org/abs/2305.07537](http://arxiv.org/abs/2305.07537)

    本文提出了三种新的饱和非单调激活函数（SGELU、SSiLU和SMish），它们由GELU、SiLU、Mish及ReLU的正部分组成，能够在CIFAR-100图像分类实验中展现很高的有效性。

    

    激活函数对于深度学习网络至关重要。大多数流行的、灵活性强的激活函数都是单调函数，但一些非单调激活函数正在被探索并展现出很有前景的表现。本文提出了三种新的激活函数: SGELU、SSiLU和SMish。这些激活函数是由GELU、SiLU、Mish以及ReLU的正部分组成，并在CIFAR-100图像分类实验中展示了很高的有效性。

    Activation functions are essential to deep learning networks. Popular and versatile activation functions are mostly monotonic functions, some non-monotonic activation functions are being explored and show promising performance. But by introducing non-monotonicity, they also alter the positive input, which is proved to be unnecessary by the success of ReLU and its variants. In this paper, we double down on the non-monotonic activation functions' development and propose the Saturated Gaussian Error Linear Units by combining the characteristics of ReLU and non-monotonic activation functions. We present three new activation functions built with our proposed method: SGELU, SSiLU, and SMish, which are composed of the negative portion of GELU, SiLU, and Mish, respectively, and ReLU's positive portion. The results of image classification experiments on CIFAR-100 indicate that our proposed activation functions are highly effective and outperform state-of-the-art baselines across multiple deep l
    
[^17]: 学习去学习：机器去学习的综述

    Learn to Unlearn: A Survey on Machine Unlearning. (arXiv:2305.07512v1 [cs.LG])

    [http://arxiv.org/abs/2305.07512](http://arxiv.org/abs/2305.07512)

    本综述总结了机器去学习技术，用于从训练模型中删除敏感数据，但重新训练ML模型往往不可行。针对这个挑战，需要开发强大的模型以缓解公平性问题。

    

    机器学习模型包含私密信息，实现被遗忘权是许多数据应用的难题。机器去学习已成为从训练模型中删除敏感数据的替代方法，但重新训练机器学习模型往往是不可行的。本综述提供了机器去学习技术的简要评估，涵盖了精确和近似方法、可能的攻击以及验证方法。本综述比较了每种方法的优点和局限性，并使用Deltagrad精确机器去学习方法评估了它们的性能。本综述还强调了挑战，如非IID删除的强大模型，以缓解公平性问题。总的来说，本综述提供了机器去学习技术和应用的全面概述，并指出了这个不断发展的领域的未来研究方向。本综述旨在成为寻求机器去学习资料的研究人员和从业者的有价值资源。

    Machine Learning (ML) models contain private information, and implementing the right to be forgotten is a challenging privacy issue in many data applications. Machine unlearning has emerged as an alternative to remove sensitive data from a trained model, but completely retraining ML models is often not feasible. This survey provides a concise appraisal of Machine Unlearning techniques, encompassing both exact and approximate methods, probable attacks, and verification approaches. The survey compares the merits and limitations each method and evaluates their performance using the Deltagrad exact machine unlearning method. The survey also highlights challenges like the pressing need for a robust model for non-IID deletion to mitigate fairness issues. Overall, the survey provides a thorough synopsis of machine unlearning techniques and applications, noting future research directions in this evolving field. The survey aims to be a valuable resource for researchers and practitioners seeking
    
[^18]: 医学图像中可解释的人工智能：一项调查

    eXplainable Artificial Intelligence on Medical Images: A Survey. (arXiv:2305.07511v1 [cs.LG])

    [http://arxiv.org/abs/2305.07511](http://arxiv.org/abs/2305.07511)

    这项调查分析了XAI领域中应用于医学诊断的研究，以便解释黑匣子模型的结果，针对的疾病包括癌症和COVID-19。

    

    在过去的几年中，应用深度学习于医疗领域的研究数量迅速增加。为了向参与医学检查的所有人解释这些结果，需要对这些模型进行严格的评估。最近，在机器学习领域中出现了可解释的人工智能，也称为XAI，旨在解释这些黑匣子模型的结果，以便进行所需评估。这项调查分析了XAI领域中针对医学诊断研究的几项最新研究，允许有关多种不同疾病（如癌症和COVID-19）的机器学习结果的可解释性。

    Over the last few years, the number of works about deep learning applied to the medical field has increased enormously. The necessity of a rigorous assessment of these models is required to explain these results to all people involved in medical exams. A recent field in the machine learning area is explainable artificial intelligence, also known as XAI, which targets to explain the results of such black box models to permit the desired assessment. This survey analyses several recent studies in the XAI field applied to medical diagnosis research, allowing some explainability of the machine learning results in several different diseases, such as cancers and COVID-19.
    
[^19]: MolDiff：在3D分子扩散生成中解决原子键不一致性问题

    MolDiff: Addressing the Atom-Bond Inconsistency Problem in 3D Molecule Diffusion Generation. (arXiv:2305.07508v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.07508](http://arxiv.org/abs/2305.07508)

    MolDiff 提出了一种新的扩散模型，可以同时生成原子和键，并通过显式建模它们之间的关系来保持一致性，从而解决了3D分子生成中的原子键不一致性问题。实验证明，该模型在生成高质量的3D分子方面表现出色。

    

    近年来，深度生成模型在3D分子生成方面取得了优异性能。其中大多数模型首先生成原子，然后基于生成的原子以一种后处理的方式添加化学键。但是，由于生成的原子位置是在不考虑可能的键的情况下生成的，所以这些原子可能没有相应的键解决方案。我们将这个问题定义为原子键不一致性问题，并声称这是目前生成不逼真3D分子方法的主要原因。为了解决这个问题，我们提出了一种新的扩散模型，称为MolDiff，它可以同时生成原子和键，并通过显式地建模它们之间关系的依赖性来保持它们的一致性。我们使用与几何和化学属性相关的标准评估了我们提出的模型的生成能力和生成分子的质量。经验证实，我们的模型在生成高质量3D分子方面优于先前的方法。

    Deep generative models have recently achieved superior performance in 3D molecule generation. Most of them first generate atoms and then add chemical bonds based on the generated atoms in a post-processing manner. However, there might be no corresponding bond solution for the temporally generated atoms as their locations are generated without considering potential bonds. We define this problem as the atom-bond inconsistency problem and claim it is the main reason for current approaches to generating unrealistic 3D molecules. To overcome this problem, we propose a new diffusion model called MolDiff which can generate atoms and bonds simultaneously while still maintaining their consistency by explicitly modeling the dependence between their relationships. We evaluated the generation ability of our proposed model and the quality of the generated molecules using criteria related to both geometry and chemical properties. The empirical studies showed that our model outperforms previous appro
    
[^20]: 校准感知的贝叶斯学习

    Calibration-Aware Bayesian Learning. (arXiv:2305.07504v1 [cs.LG])

    [http://arxiv.org/abs/2305.07504](http://arxiv.org/abs/2305.07504)

    本文提出了一个综合框架，称为校准感知的贝叶斯神经网络 (CAB)，用于共同解决深度神经网络中的校准和贝叶斯学习，通过在训练过程中正则化模型的后验预测分布来提高模型的校准性。

    

    深度学习模型，包括现代系统如大型语言模型，在提供其决策的不确定性方面往往无法提供可靠的估计。为了提高模型置信水平（也称为校准）的质量，常见的方法包括向训练损失添加基于数据的或基于数据无关的正则化项。在传统的频率派学习上最近引入了基于数据的正则化器，以惩罚置信度和准确度之间的偏差。相反，数据无关的正则化器在贝叶斯学习的核心，强制使模型参数空间中的变分分布服从先验密度。前一种方法无法量化认识不确定性，而后者则严重受到模型错误规范的影响。鉴于两种方法的局限性，本文提出了一个综合框架，称为校准感知的贝叶斯神经网络 (CAB)，用于共同解决深度神经网络中的校准和贝叶斯学习。所提出的框架涉及一种新颖的数据相关惩罚项，在训练过程中正则化模型的后验预测分布，从而提高模型的校准性。

    Deep learning models, including modern systems like large language models, are well known to offer unreliable estimates of the uncertainty of their decisions. In order to improve the quality of the confidence levels, also known as calibration, of a model, common approaches entail the addition of either data-dependent or data-independent regularization terms to the training loss. Data-dependent regularizers have been recently introduced in the context of conventional frequentist learning to penalize deviations between confidence and accuracy. In contrast, data-independent regularizers are at the core of Bayesian learning, enforcing adherence of the variational distribution in the model parameter space to a prior density. The former approach is unable to quantify epistemic uncertainty, while the latter is severely affected by model misspecification. In light of the limitations of both methods, this paper proposes an integrated framework, referred to as calibration-aware Bayesian neural n
    
[^21]: 超越不变表示学习：线性可对齐的潜在空间用于高效闭合形式领域自适应

    Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation. (arXiv:2305.07500v1 [cs.LG])

    [http://arxiv.org/abs/2305.07500](http://arxiv.org/abs/2305.07500)

    本文提出了一种新的基于最优输运（OT）的领域自适应（DA）方法，通过学习一个嵌入空间，使得OT问题的解是最优且计算量较少的，适用于同质和异质的DA设置。

    

    最优输运（OT）是一种强大的几何工具，用于比较和对齐概率测度，遵循最小努力原则。在机器学习（ML）中，OT的许多成功应用之一是领域自适应（DA），这是一种研究领域，其目标是将分类器从一个带标签的领域转移到另一个类似但不同的未标记或稀疏标记的领域。我们提出了一种全新的基于OT的DA方法，该方法使用由仿射映射给出的OT问题的闭式解，并学习了一个嵌入空间，使得该解是最优且计算量较少。我们展示了我们的方法适用于同质和异质的DA设置。

    Optimal transport (OT) is a powerful geometric tool used to compare and align probability measures following the least effort principle. Among many successful applications of OT in machine learning (ML), domain adaptation (DA) -- a field of study where the goal is to transfer a classifier from one labelled domain to another similar, yet different unlabelled or scarcely labelled domain -- has been historically among the most investigated ones. This success is due to the ability of OT to provide both a meaningful discrepancy measure to assess the similarity of two domains' distributions and a mapping that can project source domain data onto the target one. In this paper, we propose a principally new OT-based approach applied to DA that uses the closed-form solution of the OT problem given by an affine mapping and learns an embedding space for which this solution is optimal and computationally less complex. We show that our approach works in both homogeneous and heterogeneous DA settings 
    
[^22]: 基于冲击响应增强的设备鲁棒声学场景分类

    Device-Robust Acoustic Scene Classification via Impulse Response Augmentation. (arXiv:2305.07499v1 [cs.SD])

    [http://arxiv.org/abs/2305.07499](http://arxiv.org/abs/2305.07499)

    本篇论文提出了一种基于冲击响应增强的方法，用于解决音频分类模型泛化到未被训练设备上时性能下降的问题。

    

    对于音频分类模型而言，广泛适用于各种录音设备是关键性能因素。不同类型的麦克风特性由于其不同的频率响应，会引入数字化音频信号的分布差异。如果在训练期间不考虑此领域偏移，那么当它用于未见过的设备记录音频时，模型的性能可能会严重下降。特别是，在少数不同麦克风上录制音频信号的模型训练可能会使泛化到未被训练的设备困难。为解决这个问题，我们使用预录制设备脉冲响应(DIR)卷积训练集中的音频信号，从而人工增加录音设备的多样性。我们系统地研究了使用CNN和音频光谱变换进行Acoustic Scene Classification任务的DIR增强效果。结果表明，仅使用DIR增强就能提升模型在未见过设备上的性能。

    The ability to generalize to a wide range of recording devices is a crucial performance factor for audio classification models. The characteristics of different types of microphones introduce distributional shifts in the digitized audio signals due to their varying frequency responses. If this domain shift is not taken into account during training, the model's performance could degrade severely when it is applied to signals recorded by unseen devices. In particular, training a model on audio signals recorded with a small number of different microphones can make generalization to unseen devices difficult. To tackle this problem, we convolve audio signals in the training set with pre-recorded device impulse responses (DIRs) to artificially increase the diversity of recording devices. We systematically study the effect of DIR augmentation on the task of Acoustic Scene Classification using CNNs and Audio Spectrogram Transformers. The results show that DIR augmentation in isolation performs
    
[^23]: 基于图库采样的人脸识别快速准确方法

    Gallery Sampling for Robust and Fast Face Identification. (arXiv:2305.07495v1 [cs.CV])

    [http://arxiv.org/abs/2305.07495](http://arxiv.org/abs/2305.07495)

    该论文提出了一种鲁棒且快速的人脸识别方法，通过对图库数据进行采样处理，在减少搜索时间的同时，对异常图像如错误标记、低质量和信息较少的图像具有较强的鲁棒性。在5.4M网络图像数据集上，我们的方法在FNIR方面达到了0.0975，而传统方法为0.3891。

    

    深度学习方法在人脸识别中取得了极大的成功。但是，为了提高性能，收集和标记尽可能多的图像是一项重要的任务。然而，标识数据和检查大量图像数据的质量是困难的任务，并且在处理大数据时不能避免错误。之前的工作一直试图解决训练数据中的问题，然而，如果错误出现在人脸识别的图库数据中，会带来更为严重的问题。我们提出了一种对异常值具有鲁棒性的图库数据采样方法，包括错误标记、低质量和信息较少的图像，并减少了搜索时间。我们提出的采样-修剪和采样-生成方法在5.4M个名人网络图像数据集上显著提高了人脸识别性能。在FPIR=0.01的情况下，我们的方法在FNIR方面达到了0.0975，而传统方法则显示为0.3891。平均特征向量数量减少了

    Deep learning methods have been achieved brilliant results in face recognition. One of the important tasks to improve the performance is to collect and label images as many as possible. However, labeling identities and checking qualities of large image data are difficult task and mistakes cannot be avoided in processing large data. Previous works have been trying to deal with the problem only in training domain, however it can cause much serious problem if the mistakes are in gallery data of face identification. We proposed gallery data sampling methods which are robust to outliers including wrong labeled, low quality, and less-informative images and reduce searching time. The proposed sampling-by-pruning and sampling-by-generating methods significantly improved face identification performance on our 5.4M web image dataset of celebrities. The proposed method achieved 0.0975 in terms of FNIR at FPIR=0.01, while conventional method showed 0.3891. The average number of feature vectors for
    
[^24]: 声音拆分任务的基准和排行榜

    Benchmarks and leaderboards for sound demixing tasks. (arXiv:2305.07489v1 [cs.SD])

    [http://arxiv.org/abs/2305.07489](http://arxiv.org/abs/2305.07489)

    本文介绍了两个新的声源分离任务基准，并将流行的模型及其集成在这些基准上的表现进行了比较。他们还开发了一种新的音频分离方法，基于适合特定音轨的不同模型的集成，该方法在2023年音乐分离挑战赛中取得了高水平成绩，并开源了代码和方法。

    

    音乐拆分是将给定的单音频信号分离成组成部分（例如鼓、低音和人声等）与其他伴奏音乐分离的任务。源分离在许多领域中都十分有用，包括娱乐和助听器。本文提出了两个新的声源分离任务基准，并比较了流行的声音拆分模型及其集成在这些基准上的表现。我们提供了模型排行榜 https://mvsep.com/quality_checker/，以对各种模型进行比较。新的基准数据集可供下载。我们还开发了一种新的音频分离方法，基于适合特定音轨的不同模型的集成。所提出的解决方案在2023音乐分离挑战赛中取得了高水平成绩。我们的代码和方法在GitHub上公开发布。

    Music demixing is the task of separating different tracks from the given single audio signal into components, such as drums, bass, and vocals from the rest of the accompaniment. Separation of sources is useful for a range of areas, including entertainment and hearing aids. In this paper, we introduce two new benchmarks for the sound source separation tasks and compare popular models for sound demixing, as well as their ensembles, on these benchmarks. For the models' assessments, we provide the leaderboard at https://mvsep.com/quality_checker/, giving a comparison for a range of models. The new benchmark datasets are available for download. We also develop a novel approach for audio separation, based on the ensembling of different models that are suited best for the particular stem. The proposed solution was evaluated in the context of the Music Demixing Challenge 2023 and achieved top results in different tracks of the challenge. The code and the approach are open-sourced on GitHub.
    
[^25]: 针对自动驾驶的深度强化学习不确定性的识别、评估和边界确定 (arXiv:2305.07487v1 [cs.AI])

    Identify, Estimate and Bound the Uncertainty of Reinforcement Learning for Autonomous Driving. (arXiv:2305.07487v1 [cs.AI])

    [http://arxiv.org/abs/2305.07487](http://arxiv.org/abs/2305.07487)

    本文提出了一种方法来限制自动驾驶中深度强化学习模型的决策不可靠性，以保护决策的可靠性，该方法通过估计和限制策略的性能不确定性来实现。

    

    深度强化学习(DRL)已成为开发更智能化自动驾驶汽车(AVs)的一种有前途的方法。AVs上的典型DRL应用是训练基于神经网络的驾驶策略。然而，神经网络的黑盒特性可能导致不可预测的决策失误，使这些AVs不可靠。为此，本文提出了一种方法来识别和保护DRL驾驶策略的不可靠决策。基本思想是估计和限制策略的性能不确定性，该不确定性量化由于训练数据不足或网络拟合误差导致的潜在性能下降。通过限制不确定性，DRL模型的性能始终优于基线策略。由不足的数据引起的不确定性采用自助法估计。然后，使用集成网络估计由网络拟合误差引起的不确定性。最后，将基线策略添加为性能下限。

    Deep reinforcement learning (DRL) has emerged as a promising approach for developing more intelligent autonomous vehicles (AVs). A typical DRL application on AVs is to train a neural network-based driving policy. However, the black-box nature of neural networks can result in unpredictable decision failures, making such AVs unreliable. To this end, this work proposes a method to identify and protect unreliable decisions of a DRL driving policy. The basic idea is to estimate and constrain the policy's performance uncertainty, which quantifies potential performance drop due to insufficient training data or network fitting errors. By constraining the uncertainty, the DRL model's performance is always greater than that of a baseline policy. The uncertainty caused by insufficient data is estimated by the bootstrapped method. Then, the uncertainty caused by the network fitting error is estimated using an ensemble network. Finally, a baseline policy is added as the performance lower bound to a
    
[^26]: 减少紧$\ell_2$回归的标签复杂度

    Reduced Label Complexity For Tight $\ell_2$ Regression. (arXiv:2305.07486v1 [cs.LG])

    [http://arxiv.org/abs/2305.07486](http://arxiv.org/abs/2305.07486)

    提出了一种算法，可以降低标签复杂度并实现紧密的最优近似值。

    

    给定数据${\rm X}\in\mathbb{R}^{n\times d}$和标签$\mathbf{y}\in\mathbb{R}^{n}$，目标是找到$\mathbf{w}\in\mathbb{R}^d$，使$\Vert{\rm X}\mathbf{w}-\mathbf{y}\Vert^2$最小化。我们提供了一个多项式算法，它无视$\mathbf{y}$，舍弃$n/(d+\sqrt{n})$个数据点，并在期望下是$(1+d/n)$最优的近似值。其动机是在减少标签复杂程度（所需揭示的标签数）的同时实现紧密的近似。我们通过$\Omega(\sqrt{n})$减少标签复杂度。未解决的问题：是否可以通过紧密的$(1+d/n)$-近似来降低标签复杂度$\Omega(n)$？

    Given data ${\rm X}\in\mathbb{R}^{n\times d}$ and labels $\mathbf{y}\in\mathbb{R}^{n}$ the goal is find $\mathbf{w}\in\mathbb{R}^d$ to minimize $\Vert{\rm X}\mathbf{w}-\mathbf{y}\Vert^2$. We give a polynomial algorithm that, \emph{oblivious to $\mathbf{y}$}, throws out $n/(d+\sqrt{n})$ data points and is a $(1+d/n)$-approximation to optimal in expectation. The motivation is tight approximation with reduced label complexity (number of labels revealed). We reduce label complexity by $\Omega(\sqrt{n})$. Open question: Can label complexity be reduced by $\Omega(n)$ with tight $(1+d/n)$-approximation?
    
[^27]: 分离随机逼近框架下的在线学习算法

    Online Learning Under A Separable Stochastic Approximation Framework. (arXiv:2305.07484v1 [cs.LG])

    [http://arxiv.org/abs/2305.07484](http://arxiv.org/abs/2305.07484)

    本篇论文提出了一种新的在线学习算法，通过分离随机逼近框架，使用递归最小二乘算法和随机梯度下降算法分别更新模型的线性和非线性参数。此算法在多个数据集上表现出高效和有效性。

    

    本文提出了一个基于分离随机逼近框架的在线学习算法，适用于一类机器学习模型。我们的想法的重点在于观察模型中某些参数比其他参数更容易优化。本文重点关注一类线性参数较多的机器学习模型。我们的算法使用递归最小二乘（RLS）算法来更新线性参数，然后基于更新后的线性参数，使用随机梯度下降（SGD）算法来更新非线性参数。这个算法可以理解为块坐标梯度下降方法的随机逼近版本，在这个版本中，其中一部分参数使用二阶随机梯度下降方法更新，而另一部分参数使用一阶随机梯度下降更新。虽然该算法对于非凸问题的全局收敛性没有讨论，但在几个数据集上的实证结果证明了其高效和有效性。

    We propose an online learning algorithm for a class of machine learning models under a separable stochastic approximation framework. The essence of our idea lies in the observation that certain parameters in the models are easier to optimize than others. In this paper, we focus on models where some parameters have a linear nature, which is common in machine learning. In one routine of the proposed algorithm, the linear parameters are updated by the recursive least squares (RLS) algorithm, which is equivalent to a stochastic Newton method; then, based on the updated linear parameters, the nonlinear parameters are updated by the stochastic gradient method (SGD). The proposed algorithm can be understood as a stochastic approximation version of block coordinate gradient descent approach in which one part of the parameters is updated by a second-order SGD method while the other part is updated by a first-order SGD. Global convergence of the proposed online algorithm for non-convex cases is 
    
[^28]: BactInt:一种面向领域的迁移学习方法和一个语料库，用于从生物医学文本中提取细菌间相互作用

    BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text. (arXiv:2305.07468v1 [cs.IR])

    [http://arxiv.org/abs/2305.07468](http://arxiv.org/abs/2305.07468)

    BactInt是一种面向领域的自动化方法，使用迁移学习从生物医学文本中提取细菌间相互作用并挖掘特定细菌群之间的关系。公开可用的BactInt语料库标注了1200篇PubMed摘要。

    

    生物学领域中不同类型微生物在生物学空间中发挥着重要作用，这些微生物之间的相互作用是微生物群落结构的基本构建单元。生物医学文本中的证据可作为预测这种相互作用的可靠来源。然而，阅读海量且不断增长的生物医学文献是一项耗时并令人望而生畏的工作。这就必然需要开发自动化方法来准确提取生物医学文献中所报道的细菌关系。本文介绍了一种从生物医学文献中自动提取微生物相互作用（特别是细菌之间）的方法以及使用迁移学习来提高其准确性的方法。我们还描述了一个管道，用于挖掘特定细菌群之间的关系。此外，我们还介绍了第一个公开可用的Bacterial Interaction (BactInt)语料库，其中包括1200篇PubMed摘要，注释有细菌间关系。

    The community of different types of microbes present in a biological niche plays a very important role in functioning of the system. The crosstalk or interactions among the different microbes contributes to the building blocks of such microbial community structures. Evidence reported in biomedical text serves as a reliable source for predicting such interactions. However, going through the vast and ever-increasing volume of biomedical literature is an intimidating and time consuming process. This necessitates development of automated methods capable of accurately extracting bacterial relations reported in biomedical literature. In this paper, we introduce a method for automated extraction of microbial interactions (specifically between bacteria) from biomedical literature along with ways of using transfer learning to improve its accuracy. We also describe a pipeline using which relations among specific bacteria groups can be mined. Additionally, we introduce the first publicly availabl
    
[^29]: 金融科技领域中强化学习的系统综述

    Systematic Review on Reinforcement Learning in the Field of Fintech. (arXiv:2305.07466v1 [q-fin.CP])

    [http://arxiv.org/abs/2305.07466](http://arxiv.org/abs/2305.07466)

    本文综述了近年来强化学习在金融科技领域的应用，包括组合优化、降低信用风险、投资资本管理、利润最大化、有效的推荐系统和更好的价格策略确定。本文通过PRISMA技术筛选文献，突出了强化学习在Fintech中的预测精度、复杂性、可扩展性、风险、盈利能力和业绩，旨在探讨其在Fintech领域中的实际贡献。

    

    强化学习在金融科技（Fintech）领域的应用近年来备受关注。毫无疑问，通过其丰富的能力和高效性，强化学习在Fintech领域取得了卓越成果。本系统性综述的目的是开展一项探索性研究，研究强化学习与Fintech之间的相关性，以突出其预测精度、复杂性、可扩展性、风险、盈利能力和业绩。强化学习在金融或Fintech领域的主要用途包括组合优化、降低信用风险、投资资本管理、利润最大化、有效的推荐系统和更好的价格策略确定。一些研究已经探讨了强化学习对金融机构业绩的实际贡献。本综述包含了2018年以来发表的最新研究。使用PRISMA技术开展了本次综述研究，重点关注发现和筛选受纳入的研究。

    Applications of Reinforcement Learning in the Finance Technology (Fintech) have acquired a lot of admiration lately. Undoubtedly Reinforcement Learning, through its vast competence and proficiency, has aided remarkable results in the field of Fintech. The objective of this systematic survey is to perform an exploratory study on a correlation between reinforcement learning and Fintech to highlight the prediction accuracy, complexity, scalability, risks, profitability and performance. Major uses of reinforcement learning in finance or Fintech include portfolio optimization, credit risk reduction, investment capital management, profit maximization, effective recommendation systems, and better price setting strategies. Several studies have addressed the actual contribution of reinforcement learning to the performance of financial institutions. The latest studies included in this survey are publications from 2018 onward. The survey is conducted using PRISMA technique which focuses on the re
    
[^30]: 无先验信道知识的端到端通信系统的深度确定性策略梯度

    Deep Deterministic Policy Gradient for End-to-End Communication Systems without Prior Channel Knowledge. (arXiv:2305.07448v1 [cs.NI])

    [http://arxiv.org/abs/2305.07448](http://arxiv.org/abs/2305.07448)

    本研究提出了一种基于深度确定性策略梯度（DDPG）的框架来解决无先验信道知识的端到端通信系统中发射器和接收器联合训练的问题。与现有方案相比，该方法可以获得更好的检测性能。

    

    最近引入了端到端（E2E）基于学习的概念，以共同优化无线通信系统中的发送器和接收器。不幸的是，这种E2E学习架构需要先前的可微分信道模型来共同训练发射接收器的深度神经网络（DNNs），这在实践中几乎不可能。本文旨在通过开发基于深度确定性策略梯度（DDPG）的框架来解决这个问题。特别地，所提出的解决方案使用接收器DNN的损失值作为奖励来训练发射器DNN。模拟结果显示，我们的提议可以共同训练发射器和接收器而不需要先前的信道模型。此外，我们证明所提出的基于DDPG的解决方案可以实现比现有解决方案更好的检测性能。

    End-to-End (E2E) learning-based concept has been recently introduced to jointly optimize both the transmitter and the receiver in wireless communication systems. Unfortunately, this E2E learning architecture requires a prior differentiable channel model to jointly train the deep neural networks (DNNs) at the transceivers, which is hardly obtained in practice. This paper aims to solve this issue by developing a deep deterministic policy gradient (DDPG)-based framework. In particular, the proposed solution uses the loss value of the receiver DNN as the reward to train the transmitter DNN. The simulation results then show that our proposed solution can jointly train the transmitter and the receiver without requiring the prior channel model. In addition, we demonstrate that the proposed DDPG-based solution can achieve better detection performance compared to the state-of-the-art solutions.
    
[^31]: 基于知识蒸馏的轻量级领域对抗神经网络用于基于EEG的跨主体情绪识别

    A Lightweight Domain Adversarial Neural Network Based on Knowledge Distillation for EEG-based Cross-subject Emotion Recognition. (arXiv:2305.07446v1 [eess.SP])

    [http://arxiv.org/abs/2305.07446](http://arxiv.org/abs/2305.07446)

    本研究提出了一种基于知识蒸馏的轻量级领域对抗神经网络来提升基于EEG的跨主体情绪识别。教师模型学习复杂的EEG特征，指导学生模型学习更挑战的领域不变特征。

    

    个体差异会导致跨主体情绪识别中的领域偏移，影响性能，本研究基于知识蒸馏提出轻量级领域对抗神经网络，利用教师模型学习复杂的EEG时空动态和空间关联，指导学生模型学习更具挑战性的领域不变特征，在特征学习中采用基于转换器的分层时空学习模型作为教师。

    Individual differences of Electroencephalogram (EEG) could cause the domain shift which would significantly degrade the performance of cross-subject strategy. The domain adversarial neural networks (DANN), where the classification loss and domain loss jointly update the parameters of feature extractor, are adopted to deal with the domain shift. However, limited EEG data quantity and strong individual difference are challenges for the DANN with cumbersome feature extractor. In this work, we propose knowledge distillation (KD) based lightweight DANN to enhance cross-subject EEG-based emotion recognition. Specifically, the teacher model with strong context learning ability is utilized to learn complex temporal dynamics and spatial correlations of EEG, and robust lightweight student model is guided by the teacher model to learn more difficult domain-invariant features. In the feature-based KD framework, a transformer-based hierarchical temporalspatial learning model is served as the teache
    
[^32]: 使用深度强化学习优化内存映射

    Optimizing Memory Mapping Using Deep Reinforcement Learning. (arXiv:2305.07440v1 [cs.PF])

    [http://arxiv.org/abs/2305.07440](http://arxiv.org/abs/2305.07440)

    本文提出了一种使用强化学习解决机器学习程序中内存映射问题的方法。

    

    资源调度和分配是许多高影响系统的关键组成部分，涵盖拥塞控制到云计算。在这篇论文中，我们专注于调度问题的一个特定实例，即编译机器学习程序期间出现的内存映射问题：即将张量映射到不同的内存层以优化执行时间。我们介绍了一种使用强化学习解决内存映射问题的方法。使用强化学习是解决顺序决策问题和高维数据输入组合搜索空间的解决方案。

    Resource scheduling and allocation is a critical component of many high impact systems ranging from congestion control to cloud computing. Finding more optimal solutions to these problems often has significant impact on resource and time savings, reducing device wear-and-tear, and even potentially improving carbon emissions. In this paper, we focus on a specific instance of a scheduling problem, namely the memory mapping problem that occurs during compilation of machine learning programs: That is, mapping tensors to different memory layers to optimize execution time.  We introduce an approach for solving the memory mapping problem using Reinforcement Learning. RL is a solution paradigm well-suited for sequential decision making problems that are amenable to planning, and combinatorial search spaces with high-dimensional data inputs. We formulate the problem as a single-player game, which we call the mallocGame, such that high-reward trajectories of the game correspond to efficient memo
    
[^33]: 带有非对角信息的视觉-语言连续表示学习

    Continual Vision-Language Representaion Learning with Off-Diagonal Information. (arXiv:2305.07437v1 [cs.LG])

    [http://arxiv.org/abs/2305.07437](http://arxiv.org/abs/2305.07437)

    本文探讨了通过流数据持续训练CLIP模型的可行性，提出了一种有效的连续学习框架Mod-X，并证明内部旋转和跨模态偏差导致了CLIP在跨模态检索任务中性能下降。

    

    本文讨论了通过流数据持续训练CLIP模型的可行性。通过追踪连续更新的CLIP模型中表示向量的方向变化，我们探索和总结了这些空间变化，称为空间混乱（SD），可以分为内部旋转和跨模态偏差。此外，我们从经验和理论上证明了内部旋转和跨模态偏差如何导致CLIP在跨模态检索任务中性能下降。为了缓解空间混乱，我们提出了一种简单而有效的连续学习框架Mod-X: 维护非对角信息矩阵。在各种不同规模和范围的常用数据集上的实验表明了我们方法的有效性。

    This paper discusses the feasibility of continuously training the CLIP model through streaming data. Then, by tracking the directional changes of the representation vectors in the continuously updated CLIP model, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we demonstrate how intra-modal rotation and inter-modal deviation lead to a performance decline for CLIP on cross-modal retrieval tasks in both empirically and theoretically. To alleviate the spatial disorder, we propose a simple yet effective continual learning framework Mod-X: Maintain off-diagonal information-matriX. The experiments (in Section \ref{method}, \ref{experiments} and Appendix \ref{Appendix_to_experiments}) on commonly used datasets with different scales and scopes have illustrated the effectiveness of our method.
    
[^34]: 基于专家评估的加权回归模型处理嘈杂标签

    Expertise-based Weighting for Regression Models with Noisy Labels. (arXiv:2305.07430v1 [stat.ML])

    [http://arxiv.org/abs/2305.07430](http://arxiv.org/abs/2305.07430)

    本文提出了基于专家评估的加权回归模型，可用于处理具有不同观点的嘈杂标签。该方法包括两个步骤：估计每个专家的专业程度和结合他们的意见，然后将加权平均用于回归建模。本方法在模拟和真实数据上优于现有技术，具有简单、快速和有效的特点。

    

    回归方法假设训练数据的标签是准确的，然而在某些情况下，获取准确的标签并不可行，因此需要依赖多个具有不同观点的专家。本文提出了一种新的、更灵活的方法来处理嘈杂标签。该方法包括两个步骤：先估计每个专家的专业程度，然后使用学习到的权重结合他们的意见。接着，将加权平均用于回归建模。本文方法经过正式验证，证明在模拟和真实数据上优于现有技术。此外，其灵活性使得可以在两个步骤中利用任何机器学习技术。总之，该方法提供了一种简单、快速和有效的解决方案，用于训练回归模型并处理获取自不同专业来源的嘈杂标签。

    Regression methods assume that accurate labels are available for training. However, in certain scenarios, obtaining accurate labels may not be feasible, and relying on multiple specialists with differing opinions becomes necessary. Existing approaches addressing noisy labels often impose restrictive assumptions on the regression function. In contrast, this paper presents a novel, more flexible approach. Our method consists of two steps: estimating each labeler's expertise and combining their opinions using learned weights. We then regress the weighted average against the input features to build the prediction model. The proposed method is formally justified and empirically demonstrated to outperform existing techniques on simulated and real data. Furthermore, its flexibility enables the utilization of any machine learning technique in both steps. In summary, this method offers a simple, fast, and effective solution for training regression models with noisy labels derived from diverse e
    
[^35]: 使用ChatGPT的智能诊断，发掘医学影像的潜力

    Unlocking the Potential of Medical Imaging with ChatGPT's Intelligent Diagnostics. (arXiv:2305.07429v1 [eess.IV])

    [http://arxiv.org/abs/2305.07429](http://arxiv.org/abs/2305.07429)

    本文提出了一种使用深度学习模型和ChatGPT生成自动诊断的决策支持系统，可以帮助医疗保健提供者和患者在诊断、治疗和管理健康状况方面做出更好的决策。

    

    医学影像是诊断各种健康疾病和状况的重要工具。然而，分析医学图像是一项复杂和耗时的任务，需要专业知识和经验。本文旨在设计一个决策支持系统，以帮助医疗保健提供者和患者做出关于诊断、治疗和管理健康状况的决策。所提出的架构包含三个阶段：1）数据收集和标记、2）模型训练和3）诊断报告生成。关键思想是在医学图像数据集上训练深度学习模型，提取四种类型的信息：图像扫描类型、身体部位、测试图像和结果。这些信息随后被输入到ChatGPT中，以生成自动诊断。所提出的系统有可能增强决策制定、降低成本和改善医疗保健提供者的能力。通过进行广泛的实验来分析所提出系统的有效性。

    Medical imaging is an essential tool for diagnosing various healthcare diseases and conditions. However, analyzing medical images is a complex and time-consuming task that requires expertise and experience. This article aims to design a decision support system to assist healthcare providers and patients in making decisions about diagnosing, treating, and managing health conditions. The proposed architecture contains three stages: 1) data collection and labeling, 2) model training, and 3) diagnosis report generation. The key idea is to train a deep learning model on a medical image dataset to extract four types of information: the type of image scan, the body part, the test image, and the results. This information is then fed into ChatGPT to generate automatic diagnostics. The proposed system has the potential to enhance decision-making, reduce costs, and improve the capabilities of healthcare providers. The efficacy of the proposed system is analyzed by conducting extensive experiments
    
[^36]: 无监督句子嵌入的实例平滑对比学习

    Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v1 [cs.CL])

    [http://arxiv.org/abs/2305.07424](http://arxiv.org/abs/2305.07424)

    本文提出了IS-CSE方法，通过实例平滑对比学习来学习无监督句子嵌入，以平滑嵌入在特征空间中的边界，从而提高模型的泛化性能。在标准的STS任务中取得了良好的得分。

    

    基于对比学习的方法，如unsup-SimCSE，在学习无监督句子嵌入方面取得了最先进（SOTA）的性能。然而，在以前的研究中，用于对比学习的每个嵌入仅来自于一个句子实例，我们称这些嵌入为实例级嵌入。换句话说，在这种情况下，每个嵌入被视为是一类独特的类，这可能会损害泛化性能。在本研究中，我们提出了IS-CSE（实例平滑对比句子嵌入）来平滑特征空间中嵌入的边界。具体而言，我们根据语义相似性从动态内存缓冲区中检索嵌入以获得正嵌入组。然后我们通过自注意力操作对组中的嵌入进行聚合，以生成平滑实例嵌入以进行进一步分析。我们在标准的语义文本相似性（STS）任务中评估了我们的方法，并实现了平均78.30％，79.47％，77.73％和79.42％的得分。

    Contrastive learning-based methods, such as unsup-SimCSE, have achieved state-of-the-art (SOTA) performances in learning unsupervised sentence embeddings. However, in previous studies, each embedding used for contrastive learning only derived from one sentence instance, and we call these embeddings instance-level embeddings. In other words, each embedding is regarded as a unique class of its own, whichmay hurt the generalization performance. In this study, we propose IS-CSE (instance smoothing contrastive sentence embedding) to smooth the boundaries of embeddings in the feature space. Specifically, we retrieve embeddings from a dynamic memory buffer according to the semantic similarity to get a positive embedding group. Then embeddings in the group are aggregated by a self-attention operation to produce a smoothed instance embedding for further analysis. We evaluate our method on standard semantic text similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%, and 79.42% 
    
[^37]: 基于奖励函数相似性的选择性模仿

    Selective imitation on the basis of reward function similarity. (arXiv:2305.07421v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.07421](http://arxiv.org/abs/2305.07421)

    人们在模仿别人时更倾向于选择那些被认为与自己奖励函数相似的人。

    

    模仿是人类社会行为的关键组成部分，被儿童和成人广泛用作在不确定或陌生情况下导航的方式。但在由多个追求不同目标或目的的异质因体所占的环境中，盲目模仿不太可能是一种有效的策略——模仿者必须确定谁是最有用的复制对象。在这些决策中可能有许多因素，取决于上下文和信息的可用性。在这里，我们研究了这个假设：这些决策涉及对其他智体奖励函数的推断。

    Imitation is a key component of human social behavior, and is widely used by both children and adults as a way to navigate uncertain or unfamiliar situations. But in an environment populated by multiple heterogeneous agents pursuing different goals or objectives, indiscriminate imitation is unlikely to be an effective strategy -- the imitator must instead determine who is most useful to copy. There are likely many factors that play into these judgements, depending on context and availability of information. Here we investigate the hypothesis that these decisions involve inferences about other agents' reward functions. We suggest that people preferentially imitate the behavior of others they deem to have similar reward functions to their own. We further argue that these inferences can be made on the basis of very sparse or indirect data, by leveraging an inductive bias toward positing the existence of different \textit{groups} or \textit{types} of people with similar reward functions, a
    
[^38]: 一种多维图傅立叶转换神经网络在车辆轨迹预测上的应用

    A Multidimensional Graph Fourier Transformation Neural Network for Vehicle Trajectory Prediction. (arXiv:2305.07416v1 [cs.LG])

    [http://arxiv.org/abs/2305.07416](http://arxiv.org/abs/2305.07416)

    介绍了一种名为多维图傅立叶转换神经网络(GFTNN)的网络结构，用于在高速公路上进行长期轨迹预测。通过强大的操作多维图傅立叶变换(GFT)，可以汇总场景属性。该模型在高速公路轨迹预测任务中胜过现有的最先进模型，即使没有包含任何循环元件。

    

    本文介绍了一种名为多维图傅立叶转换神经网络(GFTNN)的网络结构，用于在高速公路上进行长期轨迹预测。类似于图神经网络(GNNs)，GFTNN是一种新颖的网络结构，可在图结构上操作。通过强大的操作多维图傅立叶变换(GFT)，模型将场景属性汇总到一起。通过使用GFT，车辆在空间和时间上的交互图被转换成为谱域场景表示。该有益的表示被输入到由神经网络和描述编码器组成的预测框架中。尽管所提出的GFTNN没有包含任何循环元件，但在高速公路轨迹预测任务中胜过了现有的最先进模型。公开数据集highD和NGSIM被用于实验和评估。

    This work introduces the multidimensional Graph Fourier Transformation Neural Network (GFTNN) for long-term trajectory predictions on highways. Similar to Graph Neural Networks (GNNs), the GFTNN is a novel network architecture that operates on graph structures. While several GNNs lack discriminative power due to suboptimal aggregation schemes, the proposed model aggregates scenario properties through a powerful operation: the multidimensional Graph Fourier Transformation (GFT). The spatio-temporal vehicle interaction graph of a scenario is converted into a spectral scenario representation using the GFT. This beneficial representation is input to the prediction framework composed of a neural network and a descriptive decoder. Even though the proposed GFTNN does not include any recurrent element, it outperforms state-of-the-art models in the task of highway trajectory prediction. For experiments and evaluation, the publicly available datasets highD and NGSIM are used
    
[^39]: 应用不同技术对匿名数据的机器学习模型比较

    Comparison of machine learning models applied on anonymized data with different techniques. (arXiv:2305.07415v1 [cs.LG])

    [http://arxiv.org/abs/2305.07415](http://arxiv.org/abs/2305.07415)

    本研究比较了四种经典机器学习方法在不同匿名化技术下对成人数据集分类任务的表现，分析了匿名化技术和参数对模型性能的影响。

    

    基于对值概括层次结构的偏识标识符（quasi-identifiers）进行混淆的匿名化技术被广泛用于实现预设的隐私级别，为了防止对数据库隐私的不同类型攻击，需要应用多种匿名化技术，但这些方法的应用与预测和决策制定任务的效用降低直接相关。本研究使用四种用于分类目的的经典机器学习方法，以分析与应用的匿名化技术和每种方法选定的参数相关的结果。我们研究了这些模型的性能，当改变k值用于k-匿名性，并在众所周知的成人数据集上部署了$\ell$-多样性、t-密切度和$\delta$-泄露隐私等附加工具。

    Anonymization techniques based on obfuscating the quasi-identifiers by means of value generalization hierarchies are widely used to achieve preset levels of privacy. To prevent different types of attacks against database privacy it is necessary to apply several anonymization techniques beyond the classical k-anonymity or $\ell$-diversity. However, the application of these methods is directly connected to a reduction of their utility in prediction and decision making tasks. In this work we study four classical machine learning methods currently used for classification purposes in order to analyze the results as a function of the anonymization techniques applied and the parameters selected for each of them. The performance of these models is studied when varying the value of k for k-anonymity and additional tools such as $\ell$-diversity, t-closeness and $\delta$-disclosure privacy are also deployed on the well-known adult dataset.
    
[^40]: 面向函数学习的分布式梯度下降算法

    Distributed Gradient Descent for Functional Learning. (arXiv:2305.07408v1 [stat.ML])

    [http://arxiv.org/abs/2305.07408](http://arxiv.org/abs/2305.07408)

    该论文提出了一种针对函数数据的分布式梯度下降函数学习算法，在再生核希尔伯特空间框架下通过积分算子方法得到了该算法的理论理解，并取得了不饱和边界的置信度最优学习率。

    

    近年来，不同类型的分布式学习方案因其在处理大规模数据信息方面的巨大优势而受到越来越多的关注。针对最近从函数数据分析中产生的大数据挑战，我们在再生核希尔伯特空间框架下提出了一种新颖的分布式梯度下降函数学习（DGDFL）算法，用于处理来自众多本地机器（处理器）的函数数据。基于积分算子方法，我们提供了DGDFL算法在文献中的许多方面的第一个理论理解。在理解DGDFL的过程中，首先，我们提出并全面研究了基于数据的渐进式下降函数学习（GDFL）算法与单机模型相关联。在温和的条件下，得到了DGDFL的置信度最优学习率，避免了先前在正则性索引上遭受的饱和边界。

    In recent years, different types of distributed learning schemes have received increasing attention for their strong advantages in handling large-scale data information. In the information era, to face the big data challenges which stem from functional data analysis very recently, we propose a novel distributed gradient descent functional learning (DGDFL) algorithm to tackle functional data across numerous local machines (processors) in the framework of reproducing kernel Hilbert space. Based on integral operator approaches, we provide the first theoretical understanding of the DGDFL algorithm in many different aspects in the literature. On the way of understanding DGDFL, firstly, a data-based gradient descent functional learning (GDFL) algorithm associated with a single-machine model is proposed and comprehensively studied. Under mild conditions, confidence-based optimal learning rates of DGDFL are obtained without the saturation boundary on the regularity index suffered in previous w
    
[^41]: 两合一：一种针对文本生成模型的模型劫持攻击

    Two-in-One: A Model Hijacking Attack Against Text Generation Models. (arXiv:2305.07406v1 [cs.CR])

    [http://arxiv.org/abs/2305.07406](http://arxiv.org/abs/2305.07406)

    本文扩展了模型劫持攻击的范围，提出了一种名为Ditto的攻击方法，能够将不同的文本分类任务劫持为多个生成任务，并使用多个基准数据集验证了攻击的成功性。

    

    机器学习在各种应用中取得了显著进展，从人脸识别到文本生成。然而，它的成功也伴随着各种攻击。最近提出了一种新的攻击，即模型劫持攻击，该攻击提高了问责和寄生计算的风险。但是，该攻击仅集中于图像分类任务。在本文中，我们将此攻击的范围扩大到包括文本生成和分类模型，从而展示其更广泛的适用性。具体而言，我们提出了一种新的模型劫持攻击——Ditto，它可以将不同的文本分类任务劫持为多个生成任务，例如语言翻译、文本摘要和语言建模。我们使用一系列文本基准数据集（如SST-2、TweetEval、AGnews、QNLI和IMDB）来评估我们攻击的性能。我们的结果表明，使用Ditto，攻击者可以成功地劫持文本生成模型。

    Machine learning has progressed significantly in various applications ranging from face recognition to text generation. However, its success has been accompanied by different attacks. Recently a new attack has been proposed which raises both accountability and parasitic computing risks, namely the model hijacking attack. Nevertheless, this attack has only focused on image classification tasks. In this work, we broaden the scope of this attack to include text generation and classification models, hence showing its broader applicability. More concretely, we propose a new model hijacking attack, Ditto, that can hijack different text classification tasks into multiple generation ones, e.g., language translation, text summarization, and language modeling. We use a range of text benchmark datasets such as SST-2, TweetEval, AGnews, QNLI, and IMDB to evaluate the performance of our attacks. Our results show that by using Ditto, an adversary can successfully hijack text generation models withou
    
[^42]: 基于色彩去卷积的HER2组织病理图像领域自适应性

    Color Deconvolution applied to Domain Adaptation in HER2 histopathological images. (arXiv:2305.07404v1 [eess.IV])

    [http://arxiv.org/abs/2305.07404](http://arxiv.org/abs/2305.07404)

    本文提出了一种结合色彩去卷积技术和Pix2Pix GAN网络的新方法，解决不同HER2品牌之间颜色变化的问题，并保持细胞的HER2分数，从而改善了肿瘤诊断效果。

    

    早期发现乳腺癌对改善患者预后至关重要。 Institut Català de la Salut（ICS）启动了DigiPatICS项目，以开发和实施人工智能算法来协助癌症的诊断。在本文中，我们提出了一种新的方法来解决乳腺癌组织中HER2染色病理学图像中的颜色规范化问题，该问题被表示为样式转移问题。我们将色彩去卷积技术与Pix2Pix GAN网络相结合，提出了一种纠正不同HER2品牌之间颜色变化的新方法。我们的方法侧重于保持变换后图像中细胞的HER2分数，这对于HER2分析至关重要。结果表明，我们的最终模型在保持变换后图像的细胞类别方面优于最先进的图像样式转移方法，并且在生成逼真图像方面与它们一样有效。

    Breast cancer early detection is crucial for improving patient outcomes. The Institut Catal\`a de la Salut (ICS) has launched the DigiPatICS project to develop and implement artificial intelligence algorithms to assist with the diagnosis of cancer. In this paper, we propose a new approach for facing the color normalization problem in HER2-stained histopathological images of breast cancer tissue, posed as an style transfer problem. We combine the Color Deconvolution technique with the Pix2Pix GAN network to present a novel approach to correct the color variations between different HER2 stain brands. Our approach focuses on maintaining the HER2 score of the cells in the transformed images, which is crucial for the HER2 analysis. Results demonstrate that our final model outperforms the state-of-the-art image style transfer methods in maintaining the cell classes in the transformed images and is as effective as them in generating realistic images.
    
[^43]: 一步二分图切割：规范化公式及其在可扩展子空间聚类中的应用

    One-step Bipartite Graph Cut: A Normalized Formulation and Its Application to Scalable Subspace Clustering. (arXiv:2305.07386v1 [cs.LG])

    [http://arxiv.org/abs/2305.07386](http://arxiv.org/abs/2305.07386)

    本研究提出了一种新的一步二分图切割准则（OBCut），并证明了其与一个迹最大化问题的等价性。该方法避免了使用k-means进行后处理，并可以应用于可扩展子空间聚类问题中。

    

    二分图结构在促进大规模数据集的子空间聚类和谱聚类算法方面显示出了很大的潜力。为了避免二分图划分过程中需要使用k-means进行后处理，我们通常采用约束拉普拉斯秩（CLR）来限制二分图中连接成分（即簇）的数量，但CLR忽略了这些连接成分的分布（或规范化），可能导致不平衡甚至不良簇。尽管规范化切割（Ncut）在一般图中取得了显著的成功，但如何强制执行二分图的一步规范化切割，特别是具有线性时间复杂度，仍然是一个有待解决的问题。本文首先利用规范化约束表征了一种新的一步二分图切割(OBCut)准则，并在理论上证明了该准则与一个迹最大化问题的等价性。然后，我们将这种切割准则推广到可扩展子空间聚类中。

    The bipartite graph structure has shown its promising ability in facilitating the subspace clustering and spectral clustering algorithms for large-scale datasets. To avoid the post-processing via k-means during the bipartite graph partitioning, the constrained Laplacian rank (CLR) is often utilized for constraining the number of connected components (i.e., clusters) in the bipartite graph, which, however, neglects the distribution (or normalization) of these connected components and may lead to imbalanced or even ill clusters. Despite the significant success of normalized cut (Ncut) in general graphs, it remains surprisingly an open problem how to enforce a one-step normalized cut for bipartite graphs, especially with linear-time complexity. In this paper, we first characterize a novel one-step bipartite graph cut (OBCut) criterion with normalized constraints, and theoretically prove its equivalence to a trace maximization problem. Then we extend this cut criterion to a scalable subspa
    
[^44]: 使用对比输入解码揭示大型语言模型中的偏见

    Surfacing Biases in Large Language Models using Contrastive Input Decoding. (arXiv:2305.07378v1 [cs.CL])

    [http://arxiv.org/abs/2305.07378](http://arxiv.org/abs/2305.07378)

    本文介绍了一种叫做对比输入解码（CID）的解码算法，用于生成给定两个输入的文本，能够揭示大型语言模型中可能存在的微妙偏见和输出差异。

    

    确保大型语言模型（LM）公平、强壮和有用，需要了解对它们输入的不同修改如何影响模型的行为。然而，在开放文本生成任务的背景下，这样的评估并不简单。举例来说，当引入一个带有输入文本和扰动“对照”版本的模型时，标准解码策略可能不能揭示下一个标记预测中的实质差异。因此，我们提出了对比输入解码（CID）：一种解码算法，用于生成给定两个输入的文本，其中生成的文本可能基于一个输入而不可能基于另一个输入。这样，对比生成可以以一种简单且可解释的方式突显出LM对这两个输入的输出差异可能存在的微妙差别。我们使用CID来突显标准解码策略难以检测到的上下文特定偏见，并量化不同因素的影响。

    Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour. In the context of open-text generation tasks, however, such an evaluation is not trivial. For example, when introducing a model with an input text and a perturbed, "contrastive" version of it, meaningful differences in the next-token predictions may not be revealed with standard decoding strategies. With this motivation in mind, we propose Contrastive Input Decoding (CID): a decoding algorithm to generate text given two inputs, where the generated text is likely given one input but unlikely given the other. In this way, the contrastive generations can highlight potentially subtle differences in how the LM output differs for the two inputs in a simple and interpretable manner. We use CID to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different
    
[^45]: DAISM：基于多项式近似的SRAM内数字乘法器的DNN训练和推理加速器

    DAISM: Digital Approximate In-SRAM Multiplier-based Accelerator for DNN Training and Inference. (arXiv:2305.07376v1 [cs.AR])

    [http://arxiv.org/abs/2305.07376](http://arxiv.org/abs/2305.07376)

    本论文提出了一种基于多项式近似的SRAM内数字乘法器，在不依赖于新型存储技术和避免了比特串行计算的情况下，通过内存执行GEMM计算，从而为DNN训练和推理提供了高效的加速器。

    

    DNN是最广泛使用的深度学习模型之一。对于DNN的矩阵乘法运算会产生显著的计算成本，并受限于内存和处理单元之间的数据传输。为了优化矩阵乘法运算，提出了许多专门的加速器。一种流行的想法是使用PIM（Processing-in-Memory），其中计算是由内存存储元件执行的，从而减少了处理器和记忆体之间数据传输的开销。然而，大多数PIM解决方案要么依赖于尚未成熟的新型存储技术，要么依赖于比特串行计算，后者具有重大性能开销和可扩展性问题。在本研究中，提出了一种SRAM内数字乘法器来采用先进的GEMM计算技术，同时避免了比特串行计算的缺点。这使用户可以通过使用现有技术而实现性能显著提升的系统设计。

    DNNs are one of the most widely used Deep Learning models. The matrix multiplication operations for DNNs incur significant computational costs and are bottlenecked by data movement between the memory and the processing elements. Many specialized accelerators have been proposed to optimize matrix multiplication operations. One popular idea is to use Processing-in-Memory where computations are performed by the memory storage element, thereby reducing the overhead of data movement between processor and memory. However, most PIM solutions rely either on novel memory technologies that have yet to mature or bit-serial computations which have significant performance overhead and scalability issues. In this work, an in-SRAM digital multiplier is proposed to take the best of both worlds, i.e. performing GEMM in memory but using only conventional SRAMs without the drawbacks of bit-serial computations. This allows the user to design systems with significant performance gains using existing techno
    
[^46]: 无线网络中的分散式学习: 广播与随机接入的影响

    Decentralized Learning over Wireless Networks: The Effect of Broadcast with Random Access. (arXiv:2305.07368v1 [cs.NI])

    [http://arxiv.org/abs/2305.07368](http://arxiv.org/abs/2305.07368)

    本文研究了分散式学习在无线网络中的通信问题，发现优化接入概率以最大化成功链路数的期望值是提升系统收敛速度的有效策略。

    

    本文关注分散式学习的通信方面，涉及多个代理使用分散式随机梯度下降算法(D-SGD)在分布式数据上共同训练机器学习模型的过程。特别地，我们考虑了无线信道的广播性质以及通信拓扑中的链路动态，探究广播传输和概率性随机接入策略对D-SGD收敛性能的影响。实验结果表明，优化接入概率以最大化成功链路数的期望值是一种高效的提升系统收敛速度的策略。

    In this work, we focus on the communication aspect of decentralized learning, which involves multiple agents training a shared machine learning model using decentralized stochastic gradient descent (D-SGD) over distributed data. In particular, we investigate the impact of broadcast transmission and probabilistic random access policy on the convergence performance of D-SGD, considering the broadcast nature of wireless channels and the link dynamics in the communication topology. Our results demonstrate that optimizing the access probability to maximize the expected number of successful links is a highly effective strategy for accelerating the system convergence.
    
[^47]: S-REINFORCE：一种神经符号策略梯度方法以实现可解释强化学习

    S-REINFORCE: A Neuro-Symbolic Policy Gradient Approach for Interpretable Reinforcement Learning. (arXiv:2305.07367v1 [cs.LG])

    [http://arxiv.org/abs/2305.07367](http://arxiv.org/abs/2305.07367)

    S-REINFORCE是一种神经符号策略梯度方法，利用神经网络和符号回归器生成数字和符号策略，从而为强化学习任务提供可解释解决方案。

    

    本文提出了一种新颖的强化学习算法S-REINFORCE，旨在为动态决策任务生成可解释策略。该算法利用两种类型的函数逼近器，即神经网络（NN）和符号回归器（SR），分别生成数字和符号策略。NN组件通过策略梯度学习生成可能操作的数字概率分布，而SR组件则捕获与操作概率相关的状态间关系的函数形式。然后通过重要性抽样利用SR生成的策略表达式改进学习过程中接收的奖励。我们在具有低和高维行动空间的各种动态决策问题上测试了提出的S-REINFORCE算法，结果展示了其实现可解释解决方案的有效性和影响力。通过利用NN和SR的优势，S-REINFORCE提供了一种新方法来生成强化学习任务的可解释策略。

    This paper presents a novel RL algorithm, S-REINFORCE, which is designed to generate interpretable policies for dynamic decision-making tasks. The proposed algorithm leverages two types of function approximators, namely Neural Network (NN) and Symbolic Regressor (SR), to produce numerical and symbolic policies, respectively. The NN component learns to generate a numerical probability distribution over the possible actions using a policy gradient, while the SR component captures the functional form that relates the associated states with the action probabilities. The SR-generated policy expressions are then utilized through importance sampling to improve the rewards received during the learning process. We have tested the proposed S-REINFORCE algorithm on various dynamic decision-making problems with low and high dimensional action spaces, and the results demonstrate its effectiveness and impact in achieving interpretable solutions. By leveraging the strengths of both NN and SR, S-REINF
    
[^48]: 基于模型的编程：为深度学习时代重新定义程序的基本单位

    Model-based Programming: Redefining the Atomic Unit of Programming for the Deep Learning Era. (arXiv:2305.07341v1 [cs.LG])

    [http://arxiv.org/abs/2305.07341](http://arxiv.org/abs/2305.07341)

    本文提出一种新的编程范式——基于模型的编程，旨在解决深度学习模型部署过程中的问题。推出的M语言将模型作为基本的计算单位，加强了开发人员进行关键任务的效率，这种创新的编程范式将彻底改变我们使用深度学习模型的方式。

    

    本文介绍并探讨了一种新的编程范式——基于模型的编程，旨在解决将深度学习模型应用于实际应用时所面临的挑战。尽管深度学习模型在各种任务上取得了重大成功，但将它们部署到实际业务场景中仍然存在困难，如复杂的模型训练、大量的计算资源需求以及与现有编程语言的集成问题。为了缓解这些挑战，我们提出了“基于模型的编程”概念，并推出了一种新颖的编程语言——M语言，该语言针对预期的以模型为中心的编程范式而设计。M语言将模型视为基本的计算单位，使开发人员能够更专注于关键任务，如模型加载、微调、评估和部署，从而增强创建深度学习应用程序的效率。我们认为，这种创新的编程范式将彻底改变我们使用深度学习模型的方式。

    This paper introduces and explores a new programming paradigm, Model-based Programming, designed to address the challenges inherent in applying deep learning models to real-world applications. Despite recent significant successes of deep learning models across a range of tasks, their deployment in real business scenarios remains fraught with difficulties, such as complex model training, large computational resource requirements, and integration issues with existing programming languages. To ameliorate these challenges, we propose the concept of 'Model-based Programming' and present a novel programming language - M Language, tailored to a prospective model-centered programming paradigm. M Language treats models as basic computational units, enabling developers to concentrate more on crucial tasks such as model loading, fine-tuning, evaluation, and deployment, thereby enhancing the efficiency of creating deep learning applications. We posit that this innovative programming paradigm will 
    
[^49]: 锁定与叠层：通过对数池化和超叠加堆叠贝叶斯模型预测

    Locking and Quacking: Stacking Bayesian model predictions by log-pooling and superposition. (arXiv:2305.07334v1 [stat.ML])

    [http://arxiv.org/abs/2305.07334](http://arxiv.org/abs/2305.07334)

    本文提出了两种新的贝叶斯模型组合工具，可以通过对数池化和超叠加来组合后验密度，避免了标准化常数的负担，并在预测准确性方面优于传统方法。

    

    将来自不同模型的预测结合起来是贝叶斯推理和机器学习中的一个核心问题。目前，这些预测分布几乎仅使用线性组合进行组合，例如贝叶斯模型平均、贝叶斯堆叠和专家混合。这种线性混合可能对某些应用程序不利，例如多峰性。本文提出了两种新的贝叶斯模型组合工具。这些工具是模型堆叠的推广，但是通过对数线性汇集（锁定）和量子叠加（quacking）来合并后验密度。为了优化模型权重而避免标准化常数的负担，我们研究了组合后验预测的Hyvarinen得分。我们通过一个示例说明了锁定，并将两种方法应用于来自不同连续密度的模拟数据集，将它们与传统的模型组合工具进行比较。结果表明，这两种方法在预测准确性方面优于传统方法，同时具有高效计算的特点。

    Combining predictions from different models is a central problem in Bayesian inference and machine learning more broadly. Currently, these predictive distributions are almost exclusively combined using linear mixtures such as Bayesian model averaging, Bayesian stacking, and mixture of experts. Such linear mixtures impose idiosyncrasies that might be undesirable for some applications, such as multi-modality. While there exist alternative strategies (e.g. geometric bridge or superposition), optimising their parameters usually involves computing an intractable normalising constant repeatedly. We present two novel Bayesian model combination tools. These are generalisations of model stacking, but combine posterior densities by log-linear pooling (locking) and quantum superposition (quacking). To optimise model weights while avoiding the burden of normalising constants, we investigate the Hyvarinen score of the combined posterior predictions. We demonstrate locking with an illustrative examp
    
[^50]: ActUp: 分析和整合tSNE和UMAP

    ActUp: Analyzing and Consolidating tSNE and UMAP. (arXiv:2305.07320v1 [cs.LG])

    [http://arxiv.org/abs/2305.07320](http://arxiv.org/abs/2305.07320)

    本文分析了tSNE和UMAP的参数空间，并发现归一化参数可以在两者之间切换，并提出了一种方法(\ourmethod)可以结合之前不兼容的tSNE和UMAP技术，即可以复制任意算法的结果，同时还可以加速获得UMAP的输出结果。

    

    tSNE和UMAP是由于它们的速度和可解释的低维嵌入而受欢迎的降维算法。然而，尽管它们很受欢迎，但很少有人研究它们的全部差异。我们在理论和实验上评估了tSNE和UMAP中的参数空间，并发现只有一个参数-规范化-可以在它们之间切换。这反过来意味着，大多数算法差异可以切换而不影响嵌入。我们讨论了这对UMAP背后的几个理论主张的影响，以及如何将它们与现有的tSNE解释相和谐。根据我们的分析，我们提供了一种方法(\ourmethod)，它结合了之前不兼容的tSNE和UMAP技术，并可以复制任一算法的结果。这使得我们的方法可以进一步改进，例如加速，可以更快地获得UMAP的输出结果。

    tSNE and UMAP are popular dimensionality reduction algorithms due to their speed and interpretable low-dimensional embeddings. Despite their popularity, however, little work has been done to study their full span of differences. We theoretically and experimentally evaluate the space of parameters in both tSNE and UMAP and observe that a single one -- the normalization -- is responsible for switching between them. This, in turn, implies that a majority of the algorithmic differences can be toggled without affecting the embeddings. We discuss the implications this has on several theoretic claims behind UMAP, as well as how to reconcile them with existing tSNE interpretations.  Based on our analysis, we provide a method (\ourmethod) that combines previously incompatible techniques from tSNE and UMAP and can replicate the results of either algorithm. This allows our method to incorporate further improvements, such as an acceleration that obtains either method's outputs faster than UMAP. We
    
[^51]: $\partial\mathbb{B}$ 神经网络：通过梯度下降学习离散函数

    $\partial\mathbb{B}$ nets: learning discrete functions by gradient descent. (arXiv:2305.07315v1 [cs.LG])

    [http://arxiv.org/abs/2305.07315](http://arxiv.org/abs/2305.07315)

    研究人员提出了一种通过梯度下降学习离散函数的可微分神经网络 $\partial\mathbb{B}$ 网络，它通过软网络和硬网络相结合的方式实现学习到的离散函数具有可解释性和较高的精度。

    

    $\partial\mathbb{B}$ 神经网络是一种可以通过梯度下降学习离散布尔型函数的可微分神经网络。$\partial\mathbb{B}$ 网络有两个语义上等效的方面：可微分的软网络和布尔权重的不可微硬网络。我们通过反向传播训练软网络，然后“硬化”学习到的权重，以获得与硬网络相结合的布尔权重，从而得到一个离散函数。与现有的神经网络二值化方法不同，硬化不会导致精度损失。初步实验表明，$\partial\mathbb{B}$ 网络在标准机器学习问题上实现了可比较的性能，同时具有紧凑性（由于 1 位权重）和可解释性（由于学习到函数的逻辑性质）。

    $\partial\mathbb{B}$ nets are differentiable neural networks that learn discrete boolean-valued functions by gradient descent. $\partial\mathbb{B}$ nets have two semantically equivalent aspects: a differentiable soft-net, with real weights, and a non-differentiable hard-net, with boolean weights. We train the soft-net by backpropagation and then `harden' the learned weights to yield boolean weights that bind with the hard-net. The result is a learned discrete function. `Hardening' involves no loss of accuracy, unlike existing approaches to neural network binarization. Preliminary experiments demonstrate that $\partial\mathbb{B}$ nets achieve comparable performance on standard machine learning problems yet are compact (due to 1-bit weights) and interpretable (due to the logical nature of the learnt functions).
    
[^52]: 从自然语言定义中学习多关系双曲词向量

    Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])

    [http://arxiv.org/abs/2305.07303](http://arxiv.org/abs/2305.07303)

    本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。

    

    仅使用分布信息的神经词向量一直以来都能为下游任务提供有用的含义表示。然而，现有的方法通常会导致难以解释和控制的表示。相反，自然语言定义具有递归的，自说明的语义结构，可以支持能够保留向量空间中显式概念关系和约束的新型表示学习范 paradigm。本文提出了一个神经符号、多关系框架，通过联合映射定义和定义术语及其相应的语义关系，仅从自然语言定义中学习词向量。通过自动从定义语料库中提取关系，并通过一个翻译目标规范化学习问题，我们将框架专门设定为在双曲空间中捕获由定义引起的分层和多分辨率结构。

    Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
    
[^53]: 适用于个性化膳食干预的中亚食品数据集，扩展摘要

    A Central Asian Food Dataset for Personalized Dietary Interventions, Extended Abstract. (arXiv:2305.07257v1 [cs.CV])

    [http://arxiv.org/abs/2305.07257](http://arxiv.org/abs/2305.07257)

    这项工作创建了一个可靠的中亚地区食品数据集，包含42个类别和超过16,000张属于该地区独特的国家美食的图像，并达到了88.70％（42个类别）的分类准确率。

    

    当今，人们普遍会拍摄他们所吃的每一种饮料、零食或餐点的照片，然后将这些照片发布在社交媒体平台上。利用这些社交趋势，实时的食品识别和这些捕获的食品图像的可靠分类可以在一定程度上替换一些单调的膳食记录和编码，以实现个性化的膳食干预。虽然中亚菜肴在文化和历史上具有独特性，但对这个地区的人们的食品和膳食习惯的发表数据很少。为填补这个空白，我们旨在创建一个可靠的地区食品数据集，这个数据集对广大消费者和研究人员都很容易获取。就我们所知，这是第一份关于创建中亚食品数据集（CAFD）的工作。最终数据集包含42个食品类别和超过16,000张属于该地区独特的国家美食的图像。我们在CAFD上实现了88.70％（42个类别）的分类准确率。

    Nowadays, it is common for people to take photographs of every beverage, snack, or meal they eat and then post these photographs on social media platforms. Leveraging these social trends, real-time food recognition and reliable classification of these captured food images can potentially help replace some of the tedious recording and coding of food diaries to enable personalized dietary interventions. Although Central Asian cuisine is culturally and historically distinct, there has been little published data on the food and dietary habits of people in this region. To fill this gap, we aim to create a reliable dataset of regional foods that is easily accessible to both public consumers and researchers. To the best of our knowledge, this is the first work on creating a Central Asian Food Dataset (CAFD). The final dataset contains 42 food categories and over 16,000 images of national dishes unique to this region. We achieved a classification accuracy of 88.70\% (42 classes) on the CAFD us
    
[^54]: 机器学习加速的模拟使得无经验表面重建成为可能

    Machine-learning-accelerated simulations enable heuristic-free surface reconstruction. (arXiv:2305.07251v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2305.07251](http://arxiv.org/abs/2305.07251)

    本文提出了一种双重计算环路的方法来预测多组分材料的表面相图，通过机器学习交互作用势加速了能量评分和统计采样方法，在NiAl (110)上预测了新颖的表面结构。

    

    在催化或电子等领域，理解物质表面和界面至关重要。从电子结构中的能量和统计力学相结合的第一性原理模拟可以在原则上预测材料表面的结构与热力学变量的关系。然而，当与必须进行统计采样的广阔相空间耦合时，精确的能量模拟是禁止的。在这里，我们提出了一个双重计算环路来预测多组分材料的表面相图，同时加速能量评分和统计采样方法。通过闭环主动学习，使用快速、可扩展和数据高效的机器学习相互作用势在高通量密度泛函理论计算上进行训练。在假想表面位点上使用马尔可夫链蒙特卡罗采样，在半正则系综中实现采样。在这种方法的引导下，预测的GaN（0001）和SrTiO3（001）表面与过去的工作一致，并且在NiAl(110)上预测了新颖的表面结构，其中发现在空位的存在下，反位点杂质是稳定的。

    Understanding material surfaces and interfaces is vital in applications like catalysis or electronics. Ab initio simulations, combining energies from electronic structure with statistical mechanics, can, in principle, predict the structure of material surfaces as a function of thermodynamic variables. However, accurate energy simulations are prohibitive when coupled to the vast phase space that must be statistically sampled. Here, we present a bi-faceted computational loop to predict surface phase diagrams of multi-component materials that accelerates both the energy scoring and statistical sampling methods. Fast, scalable, and data-efficient machine learning interatomic potentials are trained on high-throughput density-functional theory calculations through closed-loop active learning. Markov-chain Monte Carlo sampling in the semi-grand canonical ensemble is enabled by using virtual surface sites. The predicted surfaces for GaN(0001) and SrTiO3(001) are in agreement with past work and
    
[^55]: 基于分位数的深度强化学习及其两时间标度策略梯度算法

    Quantile-Based Deep Reinforcement Learning using Two-Timescale Policy Gradient Algorithms. (arXiv:2305.07248v1 [cs.LG])

    [http://arxiv.org/abs/2305.07248](http://arxiv.org/abs/2305.07248)

    本文探讨了优化累积奖励分位数的强化学习设置，提出了QPO和其变体QPPO算法，并使用神经网络对控制动作的策略进行参数化。实验结果表明该算法优于现有基线算法。

    

    传统的强化学习（RL）旨在优化期望累积奖励。本文中，我们考虑了优化累积奖励分位数的RL设置。我们使用神经网络对控制动作的策略进行参数化，并提出了一种称为Quantile-Based Policy Optimization（QPO）的新型策略梯度算法及其变体Quantile-Based Proximal Policy Optimization（QPPO），用于解决具有量化目标的深度RL问题。QPO使用两个耦合迭代同时在不同的时间标度上更新分位数和策略参数，而QPPO是QPO的离线版本，允许在一个模拟回合中多次更新参数，从而提高了算法效率。我们的数值实验表明，在分位数标准下，所提出的算法优于现有基线算法。

    Classical reinforcement learning (RL) aims to optimize the expected cumulative reward. In this work, we consider the RL setting where the goal is to optimize the quantile of the cumulative reward. We parameterize the policy controlling actions by neural networks, and propose a novel policy gradient algorithm called Quantile-Based Policy Optimization (QPO) and its variant Quantile-Based Proximal Policy Optimization (QPPO) for solving deep RL problems with quantile objectives. QPO uses two coupled iterations running at different timescales for simultaneously updating quantiles and policy parameters, whereas QPPO is an off-policy version of QPO that allows multiple updates of parameters during one simulation episode, leading to improved algorithm efficiency. Our numerical results indicate that the proposed algorithms outperform the existing baseline algorithms under the quantile criterion.
    
[^56]: 应用于概率时间序列填充的Schr\"odinger bridge问题的收敛性分析和算法

    Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation. (arXiv:2305.07247v1 [cs.LG])

    [http://arxiv.org/abs/2305.07247](http://arxiv.org/abs/2305.07247)

    本论文提出了一种基于近似投影的Schr\"odinger bridge算法，它能够应用于概率时间序列填充，并在医疗保健和环境数据方面实现最先进的结果。

    

    Schr\"odinger bridge问题（SBP）在生成建模中引起了越来越多的关注。然而，近似的投影是唯一可用的，其收敛性还不是十分清楚。我们提出了一种基于近似投影的Schr\"odinger bridge算法的第一个收敛分析。我们将SBP应用于概率时间序列填充，展示了优化传输成本可以提高性能，提出的算法在医疗保健和环境数据方面实现了最先进的结果。

    The Schr\"odinger bridge problem (SBP) is gaining increasing attention in generative modeling and showing promising potential even in comparison with the score-based generative models (SGMs). SBP can be interpreted as an entropy-regularized optimal transport problem, which conducts projections onto every other marginal alternatingly. However, in practice, only approximated projections are accessible and their convergence is not well understood. To fill this gap, we present a first convergence analysis of the Schr\"odinger bridge algorithm based on approximated projections. As for its practical applications, we apply SBP to probabilistic time series imputation by generating missing values conditioned on observed data. We show that optimizing the transport cost improves the performance and the proposed algorithm achieves the state-of-the-art result in healthcare and environmental data while exhibiting the advantage of exploring both temporal and feature patterns in probabilistic time ser
    
[^57]: 关于误匹配核岭回归的最优性研究

    On the Optimality of Misspecified Kernel Ridge Regression. (arXiv:2305.07241v1 [cs.LG])

    [http://arxiv.org/abs/2305.07241](http://arxiv.org/abs/2305.07241)

    本文证明当使用Sobolev RKHS时，对于误匹配核岭回归问题，KRR对于任何$s\in (0,1)$都是最优的

    

    在误匹配的核岭回归问题中，研究人员通常假定地下真实函数$f_{\rho}^{*} \in [\mathcal{H}]^{s}$，其中$[\mathcal{H}]^{s}$是一个再生核希尔伯特空间（RKHS）$\mathcal{H}$的较平滑插值空间，$s\in (0,1)$。现有的极小极大优化结果要求$\|f_{\rho}^{*}\|_{L^{\infty}}<\infty$，这意味着需要$s > \alpha_{0}$，其中$\alpha_{0}\in (0,1)$是嵌入指数，是一个依赖于$\mathcal{H}$的常数。KRR是否对所有的$s\in (0,1)$都是最优的，这是一个长期存在的问题。在这篇论文中，我们展示了当$\mathcal{H}$是Sobolev RKHS的时候，KRR对于任何的$s\in (0,1)$都是极小极大最优的。

    In the misspecified kernel ridge regression problem, researchers usually assume the underground true function $f_{\rho}^{*} \in [\mathcal{H}]^{s}$, a less-smooth interpolation space of a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$ for some $s\in (0,1)$. The existing minimax optimal results require $\|f_{\rho}^{*}\|_{L^{\infty}}<\infty$ which implicitly requires $s > \alpha_{0}$ where $\alpha_{0}\in (0,1)$ is the embedding index, a constant depending on $\mathcal{H}$. Whether the KRR is optimal for all $s\in (0,1)$ is an outstanding problem lasting for years. In this paper, we show that KRR is minimax optimal for any $s\in (0,1)$ when the $\mathcal{H}$ is a Sobolev RKHS.
    
[^58]: 处理情感回归和分类任务中单模态和多模态的通用视听学习

    Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks. (arXiv:2305.07216v1 [cs.LG])

    [http://arxiv.org/abs/2305.07216](http://arxiv.org/abs/2305.07216)

    本文提出了一个通用视听学习（VAVL）框架，可用于处理情感回归和情感分类任务中的单模态和多模态系统，即使数据缺失或不匹配也能进行有效训练和切换。

    

    大多数当前的音视频情感识别模型缺乏实际应用所需的灵活性。我们设想了一个多模态系统，即使只有一个模态可用，也可以互换地实现预测情感属性或识别分类情感。在一个多模态情感识别系统中实现这样的灵活性存在困难，因为准确解释和整合各种数据来源是困难的。同时，允许在回归和分类任务之间直接切换，同时处理缺失或部分信息也是一个挑战。本研究提出了一个用于处理情感回归和情感分类任务的通用视听学习（VAVL）框架，实现了处理单模态和多模态系统的音视频框架，即使音频和视觉数据不匹配，也可以进行训练。

    Most current audio-visual emotion recognition models lack the flexibility needed for deployment in practical applications. We envision a multimodal system that works even when only one modality is available and can be implemented interchangeably for either predicting emotional attributes or recognizing categorical emotions. Achieving such flexibility in a multimodal emotion recognition system is difficult due to the inherent challenges in accurately interpreting and integrating varied data sources. It is also a challenge to robustly handle missing or partial information while allowing direct switch between regression and classification tasks. This study proposes a \emph{versatile audio-visual learning} (VAVL) framework for handling unimodal and multimodal systems for emotion regression and emotion classification tasks. We implement an audio-visual framework that can be trained even when audio and visual paired data is not available for part of the training set (i.e., audio only or only
    
[^59]: 从流形学习的角度重新思考k-means

    Rethinking k-means from manifold learning perspective. (arXiv:2305.07213v1 [cs.LG])

    [http://arxiv.org/abs/2305.07213](http://arxiv.org/abs/2305.07213)

    该论文从流形学习的角度出发，重新思考k-means算法，提出了一种新的聚类算法，可以直接检测数据的聚类而不需要均值估计，并且能充分利用不同视图中的信息。

    

    尽管已经开发了许多聚类算法，但许多现有方法仍然利用k-means技术来检测数据点的聚类。然而，k-means的性能严重依赖于聚类中心的估计，这很难达到最优解。另一个主要缺点是对噪声和异常数据敏感。在本文中，从流形学习的角度出发，我们重新思考了k-means，并提出了一种新的聚类算法，可以直接检测数据的聚类而不需要均值估计。具体地，我们通过Butterworth滤波器构造数据点之间的距离矩阵，使得同一聚类中任意两个数据点之间的距离等于一个小常数，同时增加来自不同聚类的其他数据对之间的距离。为了充分利用嵌入在不同视图中的补充信息，我们在由指示器矩阵组成的三阶张量上利用了张量Schatten p-范数正则化。

    Although numerous clustering algorithms have been developed, many existing methods still leverage k-means technique to detect clusters of data points. However, the performance of k-means heavily depends on the estimation of centers of clusters, which is very difficult to achieve an optimal solution. Another major drawback is that it is sensitive to noise and outlier data. In this paper, from manifold learning perspective, we rethink k-means and present a new clustering algorithm which directly detects clusters of data without mean estimation. Specifically, we construct distance matrix between data points by Butterworth filter such that distance between any two data points in the same clusters equals to a small constant, while increasing the distance between other data pairs from different clusters. To well exploit the complementary information embedded in different views, we leverage the tensor Schatten p-norm regularization on the 3rd-order tensor which consists of indicator matrices 
    
[^60]: MEGABYTE: 基于多尺度Transformer的百万字节序列预测

    MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers. (arXiv:2305.07185v1 [cs.LG])

    [http://arxiv.org/abs/2305.07185](http://arxiv.org/abs/2305.07185)

    MEGABYTE是一种基于多尺度Transformer的解码器架构，能够对超过一百万字节的序列进行端到端的可微建模，在训练和生成过程中提高了性能并降低了成本，同时证明了在大规模上下文无需标记的自回归序列建模的可行性。

    

    自回归transformer模型在短序列上表现良好，但对于高分辨率图像、播客、代码或图书等长序列的处理能力较差。我们提出了Megabyte，一种多尺度解码器架构，能够对超过一百万字节的序列进行端到端的可微建模。Megabyte将序列分为图块，并在图块内使用局部子模型，在图块之间使用全局模型。这使得子二次自注意、更大的前馈层和更好的解码并行性得以实现，提高了训练和生成过程的性能，同时降低了成本。广泛的实验表明，Megabyte可以使基于字节的模型在长上下文语言建模方面与基于子词的模型相媲美，在ImageNet上实现了最先进的密度估计，可以模拟来自原始文件的音频。这些结果证明了在大规模上下文无需标记的自回归序列建模的可行性。

    Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.
    
[^61]: 基于单元级注意力状态表示的价值分解方法提升多智能体协作强化学习

    Boosting Value Decomposition via Unit-Wise Attentive State Representation for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2305.07182v1 [cs.MA])

    [http://arxiv.org/abs/2305.07182](http://arxiv.org/abs/2305.07182)

    本文提出了一种基于UNSR的简单而强大的价值分解方法，缓解了多智能体协作强化学习中的局部可观察性问题，采用多头注意力机制提供有效的信用分配，实现了最先进的性能。

    

    在多智能体协作强化学习中，当智能体数量增加时，环境的随机性和不确定性会呈指数级增长，这就对如何从部分观测中得出紧凑的潜在表示以提升价值分解方法提出了难题。为了解决这些问题，我们提出了一种简单而强大的方法，通过引入单元级注意力状态表示（UNSR），缓解了局部可观察性并有效促进协同性。在UNSR中，每个智能体学习一个紧凑而解耦的单元级状态表示，该表示由Transformer块输出，并生成其局部动作价值函数。提出的UNSR用于通过多头注意力机制提升价值分解，以在混合网络中提供有效的信用分配，为个体值函数和联合值函数之间的有效推理路径提供支持。实验结果表明，我们的方法在几个基准环境中实现了最先进的性能。

    In cooperative multi-agent reinforcement learning (MARL), the environmental stochasticity and uncertainties will increase exponentially when the number of agents increases, which puts hard pressure on how to come up with a compact latent representation from partial observation for boosting value decomposition. To tackle these issues, we propose a simple yet powerful method that alleviates partial observability and efficiently promotes coordination by introducing the UNit-wise attentive State Representation (UNSR). In UNSR, each agent learns a compact and disentangled unit-wise state representation outputted from transformer blocks, and produces its local action-value function. The proposed UNSR is used to boost the value decomposition with a multi-head attention mechanism for producing efficient credit assignment in the mixing network, providing an efficient reasoning path between the individual value function and joint value function. Experimental results demonstrate that our method a
    
[^62]: 通过学习不断困难负样本实现自动放射学报告生成

    Automatic Radiology Report Generation by Learning with Increasingly Hard Negatives. (arXiv:2305.07176v1 [cs.CV])

    [http://arxiv.org/abs/2305.07176](http://arxiv.org/abs/2305.07176)

    本文提出了一种通过学习不断困难负样本的方法实现自动放射学报告生成的框架，以获得更具判别能力的特征，从而避免产生不期望或不匹配的报告。

    

    自动放射学报告生成面临着挑战，因为医学图像或报告通常由于解剖结构的相同内容而相似。这使得模型难以捕捉个体图像的独特性，容易产生不期望的通用或不匹配的报告。为了解决这个问题，本文提出了一个新的框架，通过区分最接近的负样本，即困难负样本来学习判别图像和报告特征。特别地，为了获得更具判别能力的特征，在训练期间逐渐提高这种学习任务的难度，为特征空间中的每个图像创建越来越困难的负样本。通过将不断困难的负样本视为辅助变量，我们将这个过程制定为一个最小-最大交替优化问题。在每次迭代中，条件

    Automatic radiology report generation is challenging as medical images or reports are usually similar to each other due to the common content of anatomy. This makes a model hard to capture the uniqueness of individual images and is prone to producing undesired generic or mismatched reports. This situation calls for learning more discriminative features that could capture even fine-grained mismatches between images and reports. To achieve this, this paper proposes a novel framework to learn discriminative image and report features by distinguishing them from their closest peers, i.e., hard negatives. Especially, to attain more discriminative features, we gradually raise the difficulty of such a learning task by creating increasingly hard negative reports for each image in the feature space during training, respectively. By treating the increasingly hard negatives as auxiliary variables, we formulate this process as a min-max alternating optimisation problem. At each iteration, condition
    
[^63]: 关于理解和改善GFlowNet训练的研究

    Towards Understanding and Improving GFlowNet Training. (arXiv:2305.07170v1 [cs.LG])

    [http://arxiv.org/abs/2305.07170](http://arxiv.org/abs/2305.07170)

    该论文研究了生成流网络的训练问题，提出了三种改进方法，包括优先回放训练、相对边缘流、引导轨迹平衡目标，并在大幅度提高样本效率的同时改善了代理训练的效果。

    

    生成流网络（GFlowNets）是一类算法，学习生成策略以从非负奖励x的目标分布$p^*（x）\propto R（x）$中采样离散对象。学习目标保证GFlowNet对所有状态或轨迹全局最小化损失时，从目标分布$p^*（x）$中采样x，但在训练资源有限的情况下，其性能如何仍不清楚。我们引入了一种高效的评估策略，将所学的采样分布与目标奖励分布进行比较。由于流在给定训练数据时可能被欠定，因此我们阐明了学习流对于实践中的泛化和匹配$p^*（x）$的重要性。我们研究了如何学习更好的流，并提出了（i）优先回放训练高奖励$x$，（ii）相对边缘流策略参数化，和（iii）一种新的引导轨迹平衡目标，并展示了它如何解决子结构学分分配问题。我们显着提高了样本效率，从而提高了代理训练的效果。

    Generative flow networks (GFlowNets) are a family of algorithms that learn a generative policy to sample discrete objects $x$ with non-negative reward $R(x)$. Learning objectives guarantee the GFlowNet samples $x$ from the target distribution $p^*(x) \propto R(x)$ when loss is globally minimized over all states or trajectories, but it is unclear how well they perform with practical limits on training resources. We introduce an efficient evaluation strategy to compare the learned sampling distribution to the target reward distribution. As flows can be underdetermined given training data, we clarify the importance of learned flows to generalization and matching $p^*(x)$ in practice. We investigate how to learn better flows, and propose (i) prioritized replay training of high-reward $x$, (ii) relative edge flow policy parametrization, and (iii) a novel guided trajectory balance objective, and show how it can solve a substructure credit assignment problem. We substantially improve sample e
    
[^64]: OneCAD: 使用多模态学习的单分类器适用于所有图像数据集

    OneCAD: One Classifier for All image Datasets using multimodal learning. (arXiv:2305.07167v1 [cs.CV])

    [http://arxiv.org/abs/2305.07167](http://arxiv.org/abs/2305.07167)

    本文提出了一种训练和推断框架OneCAD，通过Mask-Image-Modeling(MIM)和多模态学习解决了当前架构(如ViTs和CNNs)存在的问题，并创建了一种可以适用于所有图像数据集且与类别数无关的DNN模型架构。

    

    视觉变换器(ViTs)和卷积神经网络(CNNs)是广泛使用的用于分类任务的深度神经网络(DNNs)。这些模型架构依赖于它所训练的数据集中类别数的数量。类别数的任何改变都会导致模型架构的改变(部分或全部)。本文探讨了一个问题：是否可能创建一个与类别数无关的模型架构？这样可以使模型架构与其所训练的数据集无关。本文强调了当前架构(ViTs和CNNs)存在的问题。同时，提出了一个训练和推断框架- OneCAD(适用于所有图像数据集的单分类器)，以实现接近与类别数无关的变压器模型。据我们所知，这是第一个使用多模态学习进行分类任务的Mask-Image-Modeling(MIM)，以创建一个与类别数无关的DNN模型架构的工作。初步结果已在自然图像分类任务上展示。

    Vision-Transformers (ViTs) and Convolutional neural networks (CNNs) are widely used Deep Neural Networks (DNNs) for classification task. These model architectures are dependent on the number of classes in the dataset it was trained on. Any change in number of classes leads to change (partial or full) in the model's architecture. This work addresses the question: Is it possible to create a number-of-class-agnostic model architecture?. This allows model's architecture to be independent of the dataset it is trained on. This work highlights the issues with the current architectures (ViTs and CNNs). Also, proposes a training and inference framework OneCAD (One Classifier for All image Datasets) to achieve close-to number-of-class-agnostic transformer model. To best of our knowledge this is the first work to use Mask-Image-Modeling (MIM) with multimodal learning for classification task to create a DNN model architecture agnostic to the number of classes. Preliminary results are shown on natu
    
[^65]: 学习增强的在线数据包调度算法

    Learning-Augmented Online Packet Scheduling with Deadlines. (arXiv:2305.07164v1 [cs.DS])

    [http://arxiv.org/abs/2305.07164](http://arxiv.org/abs/2305.07164)

    本研究提出了一种学习增强的在线数据包调度算法，能有效解决网络缓冲区管理问题。

    

    现代网络的目标是优先处理关键流量并有效地管理流量。这需要适当的缓冲区管理以防止丢失重要流量，同时最小化对非关键流量的影响。因此，算法的目标是控制每步要传输哪些数据包、哪些数据包要丢弃。在本研究中，我们提出了一种学习增强的在线数据包调度算法，并提供了一种新的算法框架来应对预测问题。我们证明了，当预测误差很小时，我们的算法会提高竞争比率，同时仍保持有界的竞争比率。

    The modern network aims to prioritize critical traffic over non-critical traffic and effectively manage traffic flow. This necessitates proper buffer management to prevent the loss of crucial traffic while minimizing the impact on non-critical traffic. Therefore, the algorithm's objective is to control which packets to transmit and which to discard at each step. In this study, we initiate the learning-augmented online packet scheduling with deadlines and provide a novel algorithmic framework to cope with the prediction. We show that when the prediction error is small, our algorithm improves the competitive ratio while still maintaining a bounded competitive ratio, regardless of the prediction error.
    
[^66]: 一种用于全组织切片组织学图像的深度学习压缩与分类技术

    A Deep Learning-based Compression and Classification Technique for Whole Slide Histopathology Images. (arXiv:2305.07161v1 [eess.IV])

    [http://arxiv.org/abs/2305.07161](http://arxiv.org/abs/2305.07161)

    本文提出了一种能够压缩组织学图像并保留更有意义表征的自编码器压缩神经网络，用于全组织切片图像。测试结果证明其有效性。

    

    本文提出了基于自编码器的神经网络架构，以压缩组织学图像并保留原始图像更密集、更有意义的表征。当前改进压缩算法的研究集中在允许更低的区域感兴趣（ROI）压缩率的方法上。神经网络能够从图像中提取有意义的语义表征，因此能够选择要在压缩过程中考虑的区域。本文关注全组织切片组织学图像的压缩。我们的目标是构建一组神经网络使有监督的压缩自编码器以更密集、更有意义的方式保留输入组织学图像。我们提出的系统是一种简单而新颖的方法来监督压缩神经网络。我们使用基于迁移学习的分类器测试压缩图像，并展示了测试结果。

    This paper presents an autoencoder-based neural network architecture to compress histopathological images while retaining the denser and more meaningful representation of the original images. Current research into improving compression algorithms is focused on methods allowing lower compression rates for Regions of Interest (ROI-based approaches). Neural networks are great at extracting meaningful semantic representations from images, therefore are able to select the regions to be considered of interest for the compression process. In this work, we focus on the compression of whole slide histopathology images. The objective is to build an ensemble of neural networks that enables a compressive autoencoder in a supervised fashion to retain a denser and more meaningful representation of the input histology images. Our proposed system is a simple and novel method to supervise compressive neural networks. We test the compressed images using transfer learning-based classifiers and show that 
    
[^67]: 机器学习在岩石物理研究中的应用：异质储层渗透率预测实例研究

    Enhancing Petrophysical Studies with Machine Learning: A Field Case Study on Permeability Prediction in Heterogeneous Reservoirs. (arXiv:2305.07145v1 [physics.geo-ph])

    [http://arxiv.org/abs/2305.07145](http://arxiv.org/abs/2305.07145)

    本实例研究利用机器学习算法中的ANN、RFC和SVM预测渗透率曲线，并将其与岩心数据相匹配，结果表明FZI方法和机器学习算法在改善储层预测方面是有效的。

    

    本研究旨在解决异质储层地层中准确预测岩石物理性质的挑战，该问题会严重影响储层性能预测。该研究采用了三种机器学习算法，即人工神经网络（ANN）、随机森林分类器（RFC）和支持向量机（SVM），以预测常规测井数据中的渗透率曲线，并将其与岩心数据相匹配。本研究的主要目的是比较三种机器学习算法在预测渗透率方面的有效性，并确定最佳的预测方法。该研究利用流动区带指数（FZI）岩石分类技术来了解影响储集层质量的因素。研究结果将用于改进储层模拟，更精确地定位未来的井位。研究得出结论，FZI方法和机器学习算法在预测渗透率曲线和改善储层预测方面是有效的。

    This field case study aims to address the challenge of accurately predicting petrophysical properties in heterogeneous reservoir formations, which can significantly impact reservoir performance predictions. The study employed three machine learning algorithms, namely Artificial Neural Network (ANN), Random Forest Classifier (RFC), and Support Vector Machine (SVM), to predict permeability log from conventional logs and match it with core data. The primary objective of this study was to compare the effectiveness of the three machine learning algorithms in predicting permeability and determine the optimal prediction method. The study utilized the Flow Zone Indicator (FZI) rock typing technique to understand the factors influencing reservoir quality. The findings will be used to improve reservoir simulation and locate future wells more accurately. The study concluded that the FZI approach and machine learning algorithms are effective in predicting permeability log and improving reservoir p
    
[^68]: ConceptARC基准：评估ARC领域的理解和泛化能力

    The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain. (arXiv:2305.07141v1 [cs.LG])

    [http://arxiv.org/abs/2305.07141](http://arxiv.org/abs/2305.07141)

    本研究提出了一个新的基准数据集ConceptARC，针对ARC领域的抽象和推理问题进行了深入评估，以提高人工智能系统的抽象和泛化能力。

    

    形成和抽象概念的能力是人类智能的关键，但现有的人工智能系统在这方面仍然欠缺。在人工智能中进行了大量关于概念抽象的研究，特别是使用理想化的领域，如Raven的渐进矩阵和Bongard问题，但即使在人工智能系统成功解决这些问题时，这些系统的实际理解情况也很少被评估。本文描述了一种针对抽象和推理数据集（ARC）的深入评估基准，ARC是Chollet [2019]开发的一组少量抽象和类比问题集。具体而言，我们描述了一个名为ConceptARC的新的、公开可用的ARC基准，它在许多基本空间和语义概念上系统地评估了抽象和泛化能力。与原始的ARC数据集不同，ConceptARC特别围绕“概念组”进行组织。

    The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture.  In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet [2019]. In particular, we describe ConceptARC, a new, publicly available benchmark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around "concept groups" -- se
    
[^69]: 基于信息论度量的受监督最优输运图形概括的承诺和限制

    Promise and Limitations of Supervised Optimal Transport-Based Graph Summarization via Information Theoretic Measures. (arXiv:2305.07138v1 [cs.LG])

    [http://arxiv.org/abs/2305.07138](http://arxiv.org/abs/2305.07138)

    本文介绍了一个最优输运框架的图形概括方法，可以将节点、边缘和属性重要性纳入概括过程中。为解决受监督图形概括问题，本文将其制定为最大化所概括图形与类标签之间的香农互信息的问题，并找到了其近似的NP难问题。

    

    图形概括是将输入的图形数据集产生更小的图形表示的问题，以使较小的压缩图形对下游任务捕获相关的结构信息。最近有一种图形概括方法，它制定了一种最优输运框架，允许将有关节点、边缘和属性重要性（在该工作中未定义）的先前信息纳入图形概括过程中。然而，对于这个框架的统计特性，我们知道的很少。为了阐明这个问题，我们考虑受监督的图形概括问题，通过使用信息论度量，我们寻求保留与类标签相关的信息。为了从理论上看待受监督概括问题本身，我们首先将其制定为最大化所概括图形与类标签之间的香农互信息的问题。我们展示了近似的NP难问题。

    Graph summarization is the problem of producing smaller graph representations of an input graph dataset, in such a way that the smaller compressed graphs capture relevant structural information for downstream tasks. There is a recent graph summarization method that formulates an optimal transport-based framework that allows prior information about node, edge, and attribute importance (never defined in that work) to be incorporated into the graph summarization process. However, very little is known about the statistical properties of this framework. To elucidate this question, we consider the problem of supervised graph summarization, wherein by using information theoretic measures we seek to preserve relevant information about a class label. To gain a theoretical perspective on the supervised summarization problem itself, we first formulate it in terms of maximizing the Shannon mutual information between the summarized graph and the class label. We show an NP-hardness of approximation 
    
[^70]: 通过非负矩阵分解解决音频分类网络的可解释性问题

    Tackling Interpretability in Audio Classification Networks with Non-negative Matrix Factorization. (arXiv:2305.07132v1 [cs.SD])

    [http://arxiv.org/abs/2305.07132](http://arxiv.org/abs/2305.07132)

    本文通过引入非负矩阵分解(NMF)的解释器设计，提出了一种具有高性能的本质上可解释模型，解决了音频分类网络的可解释性问题。

    

    本文针对音频处理网络解释性的两个主要问题，即事后和设计时解释性，提出了处理方法。针对事后解释性，我们旨在解释网络决策方面的高级音频对象，最终为终端用户提供可听的对象。本文通过引入非负矩阵分解(NMF)的解释器设计，提出了一种具有高性能的本质上可解释模型。具体而言，解释器通过学习预先学习NMF字典的时间激活作为目标网络隐藏层的规则化中间嵌入来生成。本文的方法允许我们生成直观的基于音频的解释，明确增强与网络决策最相关的输入信号的部分。我们在多个分类任务上演示了我们方法的适用性，包括真实世界的音频和音乐多标签数据。

    This paper tackles two major problem settings for interpretability of audio processing networks, post-hoc and by-design interpretation. For post-hoc interpretation, we aim to interpret decisions of a network in terms of high-level audio objects that are also listenable for the end-user. This is extended to present an inherently interpretable model with high performance. To this end, we propose a novel interpreter design that incorporates non-negative matrix factorization (NMF). In particular, an interpreter is trained to generate a regularized intermediate embedding from hidden layers of a target network, learnt as time-activations of a pre-learnt NMF dictionary. Our methodology allows us to generate intuitive audio-based interpretations that explicitly enhance parts of the input signal most relevant for a network's decision. We demonstrate our method's applicability on a variety of classification tasks, including multi-label data for real-world audio and music.
    
[^71]: k-匿名和合成数据技术的能量成本和机器学习准确性影响。

    Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques. (arXiv:2305.07116v1 [cs.LG])

    [http://arxiv.org/abs/2305.07116](http://arxiv.org/abs/2305.07116)

    本文研究了隐私增强技术对机器学习模型准确性和能量消耗的影响，发现k-匿名化训练的模型准确性较低，但能源消耗较少，合成数据是一种有潜力的选择，因为它能够在能源消耗更少的情况下取得可比的准确性。

    

    为了解决与隐私和气候变化有关的愈发增长的社会关切，欧盟颁布了《通用数据保护条例》(GDPR)并承诺了绿色协议。大量研究探究了运用匿名数据集训练机器学习模型的能效和准确性。最近的研究开始探究隐私增强技术（PET）对机器学习模型的能量消耗和准确性的影响，重点关注k-匿名。由于合成数据越来越受欢迎，因此本文分析了两个阶段的能量消耗和准确性：a）将隐私增强技术应用于相关数据集，b）在相关隐私增强数据集上训练模型。我们使用两种隐私增强技术：k-匿名化（使用泛化和抑制）和合成数据，以及三种机器学习模型。每个模型都在每个隐私增强数据集上进行训练。结果显示，在经过k-匿名化的数据上训练的模型具有较低的准确性，并且消耗的能量较少，与在非匿名化数据上训练的模型相比。然而，k-匿名化过程中消耗的能量非常可观，在评估其有用性时必须将其考虑在内。合成数据证明是一种有前途的选择，因为它在消耗更少能源的情况下实现了与非匿名化数据可比的准确性。

    To address increasing societal concerns regarding privacy and climate, the EU adopted the General Data Protection Regulation (GDPR) and committed to the Green Deal. Considerable research studied the energy efficiency of software and the accuracy of machine learning models trained on anonymised data sets. Recent work began exploring the impact of privacy-enhancing techniques (PET) on both the energy consumption and accuracy of the machine learning models, focusing on k-anonymity. As synthetic data is becoming an increasingly popular PET, this paper analyses the energy consumption and accuracy of two phases: a) applying privacy-enhancing techniques to the concerned data set, b) training the models on the concerned privacy-enhanced data set. We use two privacy-enhancing techniques: k-anonymisation (using generalisation and suppression) and synthetic data, and three machine-learning models. Each model is trained on each privacy-enhanced data set. Our results show that models trained on k-a
    
[^72]: $\mathrm{E}(n)$等变消息传递单纯网络

    $\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks. (arXiv:2305.07100v1 [cs.LG])

    [http://arxiv.org/abs/2305.07100](http://arxiv.org/abs/2305.07100)

    本文提出了$\mathrm{E}(n)$等变消息传递单纯网络(EMPSNs)，一种同时将消息传递单纯网络和$\mathrm{E}(n)$等变图神经网络的优势结合，在处理高维数据时利用几何信息防止过度平滑的方法。

    

    本文提出了$\mathrm{E}(n)$等变消息传递单纯网络(EMPSNs)，这是一种学习在几何图形和点云上的方法，其等变于旋转、平移和反射。EMPSNs可以学习在图形中的高维单纯面（如三角形），并以$\mathrm{E}(n)$等变方式利用更高维单纯体的几何信息。EMPSNs同时将$\mathrm{E}(n)$等变图神经网络推广到更加复杂的拓扑结构领域，并提供了一种在消息传递单纯网络中包含几何信息的方法。结果表明，EMPSNs可以利用两种方法的优势，相较于单独使用其中一种方法，性能有了普遍提高。此外，结果表明，在高维操作中，包含几何信息是防止消息传递网络过度平滑的有效措施。

    This paper presents $\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks (EMPSNs), a novel approach to learning on geometric graphs and point clouds that is equivariant to rotations, translations, and reflections. EMPSNs can learn high-dimensional simplex features in graphs (e.g. triangles), and use the increase of geometric information of higher-dimensional simplices in an $\mathrm{E}(n)$ equivariant fashion. EMPSNs simultaneously generalize $\mathrm{E}(n)$ Equivariant Graph Neural Networks to a topologically more elaborate counterpart and provide an approach for including geometric information in Message Passing Simplicial Networks. The results indicate that EMPSNs can leverage the benefits of both approaches, leading to a general increase in performance when compared to either method. Furthermore, the results suggest that incorporating geometric information serves as an effective measure against over-smoothing in message passing networks, especially when operating on high
    
[^73]: 机器理由对人类是否有用？评估和提高自然文本理由的人类效用

    Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales. (arXiv:2305.07095v1 [cs.CL])

    [http://arxiv.org/abs/2305.07095](http://arxiv.org/abs/2305.07095)

    该论文研究了机器产生的自然语言理由对人类是否有用，发现现有理由的人类效用远低于理想状态，并提出通过估计理由在回答给定问题中的有用性来提高机器生成理由的人类效用。

    

    在大型语言模型（LMs）的显着出现能力中，自由文本理由是其中之一；超过某个规模后，大型LMs能够生成看似有用的理由，进而可以极大地增强它们在领导榜上的表现。这种现象引发了一个问题：机器生成的理由是否也能对人类有用，特别是当普通人尝试根据这些机器理由回答问题时？我们观察到现有理由的人类效用远未令人满意，并且昂贵的人类研究才能估计。现有的评估指标，如生成理由LM的任务表现或生成理由与黄金理由之间的相似性，并不能很好地表明它们的人类效用。虽然我们观察到，理由的某些属性，如简洁性和新颖性，与它们的人类效用有关，但在没有人类参与的情况下估计它们是具有挑战性的。我们展示了如何通过估计理由在回答给定问题中的有用性来提高机器生成理由的人类效用，从而解决这个问题。

    Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond a certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory, and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales, or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale's helpfulness in ans
    
[^74]: 具有大型无界随机信息延迟的分布式随机逼近的稳定性和收敛性

    Stability and Convergence of Distributed Stochastic Approximations with large Unbounded Stochastic Information Delays. (arXiv:2305.07091v1 [math.OC])

    [http://arxiv.org/abs/2305.07091](http://arxiv.org/abs/2305.07091)

    本文通过引入信息年龄过程，将Borkar-Meyn稳定性定理推广到具有任意矩界的信息延迟的分布式随机逼近中，并讨论了分布式基于梯度的优化和分析SA的新方法。

    

    本文将Borkar-Meyn稳定性定理（BMT）推广到具有任意矩界的信息延迟的分布式随机逼近（SA）中。为了模拟延迟，我们引入了信息年龄过程（AoIP）：具有单位增长属性的非负整数上的随机过程。我们证明AoIP不能无限地超过任何时段所占比例。结合适当选择的步长，这个性质足以保证分布式SA的稳定性。与BMT相比，我们的分析需要关键修正和新的论证方式来处理由AoI导致的SA误差。在分析中，我们展示这些SA误差满足递归不等式。为了评估这个递归，我们提出了一种新的Grönwall-type不等式，用于时变下限求和。作为我们分布式BMT的应用，我们讨论了分布式基于梯度的优化和分析SA的新方法。

    We generalize the Borkar-Meyn stability Theorem (BMT) to distributed stochastic approximations (SAs) with information delays that possess an arbitrary moment bound. To model the delays, we introduce Age of Information Processes (AoIPs): stochastic processes on the non-negative integers with a unit growth property. We show that AoIPs with an arbitrary moment bound cannot exceed any fraction of time infinitely often. In combination with a suitably chosen stepsize, this property turns out to be sufficient for the stability of distributed SAs. Compared to the BMT, our analysis requires crucial modifications and a new line of argument to handle the SA errors caused by AoI. In our analysis, we show that these SA errors satisfy a recursive inequality. To evaluate this recursion, we propose a new Gronwall-type inequality for time-varying lower limits of summations. As applications to our distributed BMT, we discuss distributed gradient-based optimization and a new approach to analyzing SAs wit
    
[^75]: HINT:层次混合网络用于一致概率预测

    HINT: Hierarchical Mixture Networks For Coherent Probabilistic Forecasting. (arXiv:2305.07089v1 [stat.ML])

    [http://arxiv.org/abs/2305.07089](http://arxiv.org/abs/2305.07089)

    HINT是一种用于概率预测的新型模型族，能够有效、准确地进行一致性预测，通过引入Bootstrap方法并为网络加入规范化特征提取和输出规范化来保证其性能，在多个数据集上的预测精度比现有技术更高。

    

    我们提出了一种名为"Hierarchical Mixture Networks"（HINT）的模型族，用于有效而准确的一致性预测。我们通过多元混合并使用复合似然函数进行优化来专门针对该任务进行网络特化，并通过引入Bootstrap方法加以协调。此外，我们在网络中引入了规范化特征提取和输出规范化，以应对时间序列尺度变化。与现有最先进技术相比，我们展示了在五个数据集上的8％ sCRPS增强精度。我们对模型部件进行了消融研究并广泛研究了多元混合的理论性质。 HINT的代码可以在https://github.com/Nixtla/neuralforecast上获得。

    We present the Hierarchical Mixture Networks (HINT), a model family for efficient and accurate coherent forecasting. We specialize the networks on the task via a multivariate mixture optimized with composite likelihood and made coherent via bootstrap reconciliation. Additionally, we robustify the networks to stark time series scale variations, incorporating normalized feature extraction and recomposition of output scales within their architecture. We demonstrate 8% sCRPS improved accuracy across five datasets compared to the existing state-of-the-art. We conduct ablation studies on our model's components and extensively investigate the theoretical properties of the multivariate mixture. HINT's code is available at this https://github.com/Nixtla/neuralforecast.
    
[^76]: 应用参数模型的主动学习：基于顺序实验设计的光谱测量

    Sequential Experimental Design for Spectral Measurement: Active Learning Using a Parametric Model. (arXiv:2305.07040v1 [cs.LG])

    [http://arxiv.org/abs/2305.07040](http://arxiv.org/abs/2305.07040)

    本文提出了一种利用参数模型的主动学习方法，可用于光谱测量的顺序实验设计，提高了实验效率，缩短了测量时间。

    

    本研究提出了一种利用参数模型作为预测因素的顺序实验设计方法，通过主动学习来进行光谱测量。在光谱测量中，由于样本易损坏和高能量成本的原因，需要缩短测量时间。为了提高实验效率，本研究提出了顺序实验设计，利用之前获得的数据进行主动学习，设计后续测量方式。然而，由于公式的复杂性，一般的参数模型在实现顺序实验设计方面面临困难。因此，我们采用基于Bayesian推断的数据分析方法，利用交换Monte Carlo方法实现了使用一般参数模型的顺序实验设计。

    In this study, we demonstrate a sequential experimental design for spectral measurements by active learning using parametric models as predictors. In spectral measurements, it is necessary to reduce the measurement time because of sample fragility and high energy costs. To improve the efficiency of experiments, sequential experimental designs are proposed, in which the subsequent measurement is designed by active learning using the data obtained before the measurement. Conventionally, parametric models are employed in data analysis; when employed for active learning, they are expected to afford a sequential experimental design that improves the accuracy of data analysis. However, due to the complexity of the formulas, a sequential experimental design using general parametric models has not been realized. Therefore, we applied Bayesian inference-based data analysis using the exchange Monte Carlo method to realize a sequential experimental design with general parametric models. In this s
    
[^77]: 具有门控汇总模块的值迭代网络研究

    Value Iteration Networks with Gated Summarization Module. (arXiv:2305.07039v1 [cs.LG])

    [http://arxiv.org/abs/2305.07039](http://arxiv.org/abs/2305.07039)

    本文提出了一种具有门控汇总模块的值迭代网络（GS-VIN）来解决值迭代网络在处理更大的输入地图和减轻累积误差方面的挑战。通过自适应迭代策略和门控汇总模块，这种模型可以在保持准确性的同时，减少网络深度并提高训练稳定性。

    

    本文针对值迭代网络（VIN）在处理更大的输入地图，减轻由增加迭代次数引起的累积误差的挑战提出了一种新方法——具有门控汇总模块的值迭代网络（GS-VIN）。我们提出自适应迭代策略，利用更大的卷积核减少迭代次数，减少网络深度，提高训练稳定性，同时保持计划过程的准确性。我们还引入了门控汇总模块，使得网络可以强调整个规划过程，而不仅仅依赖于最终的全局规划结果。

    In this paper, we address the challenges faced by Value Iteration Networks (VIN) in handling larger input maps and mitigating the impact of accumulated errors caused by increased iterations. We propose a novel approach, Value Iteration Networks with Gated Summarization Module (GS-VIN), which incorporates two main improvements: (1) employing an Adaptive Iteration Strategy in the Value Iteration module to reduce the number of iterations, and (2) introducing a Gated Summarization module to summarize the iterative process. The adaptive iteration strategy uses larger convolution kernels with fewer iteration times, reducing network depth and increasing training stability while maintaining the accuracy of the planning process. The gated summarization module enables the network to emphasize the entire planning process, rather than solely relying on the final global planning outcome, by temporally and spatially resampling the entire planning process within the VI module. We conduct experiments 
    
[^78]: 用3D卷积自编码器揭示帕金森病的症状模式：潜空间分析

    Revealing Patterns of Symptomatology in Parkinson's Disease: A Latent Space Analysis with 3D Convolutional Autoencoders. (arXiv:2305.07038v1 [eess.IV])

    [http://arxiv.org/abs/2305.07038](http://arxiv.org/abs/2305.07038)

    本文提出一种使用卷积自编码器(CVAEs)检测和量化DaT的浓度及其空间模式的新方法，结合回归算法将其与不同症状类别相关联，成功地将UPDRS与低维表示相关联，有望在PD的早期诊断及理解神经退行性和症状学方面发挥重要作用。

    

    本文提出使用3D卷积变分自编码器(CVAEs)追踪帕金森病(PD)神经退化产生的变化和症状。我们提出了一种新的方法，利用3D CVAEs检测和量化多巴胺转运体(DaT)浓度及其空间模式的变化。我们的方法利用深度学习学习脑部成像数据的低维表示，然后使用回归算法将其与不同的症状类别相关联。我们在PD患者和健康对照组的数据集上展示了我们方法的有效性，并显示普遍症状(UPDRS)通过具有R2>0.25的CVAE的d维分解与之相关联。我们的研究展示了表示学习在早期诊断以及理解神经退行性和症状学中的潜力。

    This work proposes the use of 3D convolutional variational autoencoders (CVAEs) to trace the changes and symptomatology produced by neurodegeneration in Parkinson's disease (PD). In this work, we present a novel approach to detect and quantify changes in dopamine transporter (DaT) concentration and its spatial patterns using 3D CVAEs on Ioflupane (FPCIT) imaging. Our approach leverages the power of deep learning to learn a low-dimensional representation of the brain imaging data, which then is linked to different symptom categories using regression algorithms. We demonstrate the effectiveness of our approach on a dataset of PD patients and healthy controls, and show that general symptomatology (UPDRS) is linked to a d-dimensional decomposition via the CVAE with R2>0.25. Our work shows the potential of representation learning not only in early diagnosis but in understanding neurodegeneration processes and symptomatology.
    
[^79]: 通过内部层连接重新思考深度分离

    Rethink Depth Separation with Intra-layer Links. (arXiv:2305.07037v1 [cs.LG])

    [http://arxiv.org/abs/2305.07037](http://arxiv.org/abs/2305.07037)

    添加内部层连接可以显著提高网络的表示能力，并修改深度分离理论，使得带有内部层连接的浅层网络可以表示深层网络的一些困难函数。

    

    深度分离理论现在被广泛认为是深度神经网络优越性的一个有效解释，它由两部分组成：i）存在一种可以由深度网络表示的函数；ii）这样的函数不能由宽度低于某一阈值的浅层网络表示。然而，这个理论是建立在前馈网络上的。很少有研究在向解决现实问题的最常见的网络类型——快捷网络中考虑深度分离理论。本文发现，添加内部层连接可以修改深度分离理论。首先，我们报告了通过界限估计、显式构造和功能空间分析可以通过添加内部层连接显著提高网络的表示能力。然后，我们通过展示一个带有内部层连接的浅层网络不需要像之前一样变得宽来表示由深层网络构造的一些困难函数来修改深度分离理论。

    The depth separation theory is nowadays widely accepted as an effective explanation for the power of depth, which consists of two parts: i) there exists a function representable by a deep network; ii) such a function cannot be represented by a shallow network whose width is lower than a threshold. However, this theory is established for feedforward networks. Few studies, if not none, considered the depth separation theory in the context of shortcuts which are the most common network types in solving real-world problems. Here, we find that adding intra-layer links can modify the depth separation theory. First, we report that adding intra-layer links can greatly improve a network's representation capability through bound estimation, explicit construction, and functional space analysis. Then, we modify the depth separation theory by showing that a shallow network with intra-layer links does not need to go as wide as before to express some hard functions constructed by a deep network. Such
    
[^80]: 带人类反馈的GFlowNets

    GFlowNets with Human Feedback. (arXiv:2305.07036v1 [cs.LG])

    [http://arxiv.org/abs/2305.07036](http://arxiv.org/abs/2305.07036)

    GFlowNets框架通过人类反馈来改善AI模型的探索能力，通过适应不同轨迹上的人类评估，可以学习到严格与人类评级成比例的策略，实验结果表明比RLHF更出色。

    

    我们提出了带有人类反馈的GFlowNets (GFlowHF) 框架来改进训练 AI 模型的探索能力。对于奖励未知的任务，我们通过不同轨迹上的人类评估来适应奖励函数。GFlowHF 的目标是学习一个严格与人类评级成比例的策略，而不仅仅关注于类似 RLHF 的人类喜好评级。实验表明，GFlowHF 比 RLHF 可以实现更好的探索能力。

    We propose the GFlowNets with Human Feedback (GFlowHF) framework to improve the exploration ability when training AI models. For tasks where the reward is unknown, we fit the reward function through human evaluations on different trajectories. The goal of GFlowHF is to learn a policy that is strictly proportional to human ratings, instead of only focusing on human favorite ratings like RLHF. Experiments show that GFlowHF can achieve better exploration ability than RLHF.
    
[^81]: 基于端到端深度学习的古兰经朗诵识别

    Quran Recitation Recognition using End-to-End Deep Learning. (arXiv:2305.07034v1 [eess.AS])

    [http://arxiv.org/abs/2305.07034](http://arxiv.org/abs/2305.07034)

    本文提出了一种基于端到端深度学习模型，使用CTC作为目标函数，来识别古兰经的朗诵。采用公共数据集进行实验。

    

    古兰经是伊斯兰教的圣书，其朗诵是该宗教信仰的一个重要方面。由于古兰经的独特规则不适用于正常的演讲，所以自动识别古兰经的朗诵是一项具有挑战性的任务。在此之前，已进行了许多研究，但以往的研究将朗诵错误检测视为分类任务或使用传统的自动语音识别（ASR）。在本文中，我们提出了一个新的基于端到端深度学习模型来识别古兰经的朗诵。该模型是一个CNN-Bidirectional GRU编码器，使用CTC作为目标函数和基于字符的解码器，即波束搜索解码器。此外，所有以往的研究都是在由短节和几章古兰经组成的小型私人数据集上进行的。由于使用私人数据集，因此没有进行任何比较。为了解决这个问题，我们使用了最近发布的公共数据集（Ar-DAD）作为实验数据。

    The Quran is the holy scripture of Islam, and its recitation is an important aspect of the religion. Recognizing the recitation of the Holy Quran automatically is a challenging task due to its unique rules that are not applied in normal speaking speeches. A lot of research has been done in this domain, but previous works have detected recitation errors as a classification task or used traditional automatic speech recognition (ASR). In this paper, we proposed a novel end-to-end deep learning model for recognizing the recitation of the Holy Quran. The proposed model is a CNN-Bidirectional GRU encoder that uses CTC as an objective function, and a character-based decoder which is a beam search decoder. Moreover, all previous works were done on small private datasets consisting of short verses and a few chapters of the Holy Quran. As a result of using private datasets, no comparisons were done. To overcome this issue, we used a public dataset that has recently been published (Ar-DAD) and co
    
[^82]: 基于控制微分方程的Hawkes过程

    Hawkes Process based on Controlled Differential Equations. (arXiv:2305.07031v1 [cs.LG])

    [http://arxiv.org/abs/2305.07031](http://arxiv.org/abs/2305.07031)

    本文提出了一种基于控制微分方程的Hawkes过程模型，可精确计算对数似然，并能够正确处理不规则时间序列，适用于社会扩散和地震预测。

    

    Hawkes过程是一种常用的模型框架，用于对多个领域的序贯事件发生动态进行建模，例如社会扩散。在现实场景中，事件之间的间隔时间是不规则的。然而，现有基于神经网络的Hawkes过程模型不仅难以捕捉这种复杂的不规则动态，而且还会使用启发式方法计算事件的对数似然，因为它们大多基于设计用于规则离散输入的神经网络。为此，我们提出了基于控制微分方程(CDE)的Hawkes过程概念，通过采用类似于连续RNN的神经CDE技术。由于HP-CDE不断地读取数据，因此可以适当地处理不规则时间序列数据集，保留它们的不均匀时间空间，并且对数似然可以准确计算。此外，由于Hawkes过程和神经CDE都是在连续的时间域中首先开发的，它们具有相似的背景。因此，HP-CDE具有透明的结构，可以轻松适应实际场景，例如社会扩散，其中事件之间的间隔时间是不规则的。我们使用合成和真实的社交扩散和地震数据集演示了我们提出的模型的优势，并超过了现有的最先进的Hawkes过程模型。

    Hawkes processes are a popular framework to model the occurrence of sequential events, i.e., occurrence dynamics, in several fields such as social diffusion. In real-world scenarios, the inter-arrival time among events is irregular. However, existing neural network-based Hawkes process models not only i) fail to capture such complicated irregular dynamics, but also ii) resort to heuristics to calculate the log-likelihood of events since they are mostly based on neural networks designed for regular discrete inputs. To this end, we present the concept of Hawkes process based on controlled differential equations (HP-CDE), by adopting the neural controlled differential equation (neural CDE) technology which is an analogue to continuous RNNs. Since HP-CDE continuously reads data, i) irregular time-series datasets can be properly treated preserving their uneven temporal spaces, and ii) the log-likelihood can be exactly computed. Moreover, as both Hawkes processes and neural CDEs are first de
    
[^83]: 基于模仿学习的算法在现代电力市场中实现先验知识传递以进行贝叶斯纳什均衡估计

    An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation. (arXiv:2305.06924v1 [cs.GT])

    [http://arxiv.org/abs/2305.06924](http://arxiv.org/abs/2305.06924)

    本论文提出了一种基于模仿学习的算法，利用先验知识和与变化环境的交互实现了GENCO的投标策略优化和贝叶斯纳什均衡估计，针对现代电力市场中先验知识未被充分利用导致现有方法不准确和低效的问题进行了改进。

    

    在电力市场的投标游戏中，纳什均衡（NE）估计是发电公司（GENCO）进行投标策略优化和独立系统运营商（ISO）进行市场监视的关键问题。然而，现有的NE估计方法在新兴现代电力市场（FEM）中是不准确和低效的，因为在任何环境变化之前，如负载需求变化、网络拥堵和市场设计的修改，投标策略的先验知识没有充分利用。为此，本文针对FEM开发了Bayes自适应马尔科夫决策过程（BAMDP-FEM），以考虑先验知识来建模GENCO的投标策略优化。随后提出了一种新颖的多智能体生成对抗模仿学习算法（MAGAIL-FEM），使GENCO能够同时从先验知识和与变化环境的交互中进行学习。得到的NE是一种贝叶斯纳什均衡（BNE）。

    The Nash Equilibrium (NE) estimation in bidding games of electricity markets is the key concern of both generation companies (GENCOs) for bidding strategy optimization and the Independent System Operator (ISO) for market surveillance. However, existing methods for NE estimation in emerging modern electricity markets (FEM) are inaccurate and inefficient because the priori knowledge of bidding strategies before any environment changes, such as load demand variations, network congestion, and modifications of market design, is not fully utilized. In this paper, a Bayes-adaptive Markov Decision Process in FEM (BAMDP-FEM) is therefore developed to model the GENCOs' bidding strategy optimization considering the priori knowledge. A novel Multi-Agent Generative Adversarial Imitation Learning algorithm (MAGAIL-FEM) is then proposed to enable GENCOs to learn simultaneously from priori knowledge and interactions with changing environments. The obtained NE is a Bayesian Nash Equilibrium (BNE) with 
    
[^84]: 如何使用强化学习促进未来的电力市场设计？第二部分：方法和应用

    How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications. (arXiv:2305.06921v1 [cs.GT])

    [http://arxiv.org/abs/2305.06921](http://arxiv.org/abs/2305.06921)

    本文开发了一种基于强化学习的模拟方法来联合设计电力市场，详细阐述了设计电力现货市场、辅助服务市场中的保留能力产品和金融市场中的虚拟竞标产品的方法，并通过案例研究演示了如何选择最佳市场设计选项。

    

    本为两部分的论文发展了一种范式理论和详细的方法，利用基于强化学习（RL）的模拟来联合电力市场设计。在第二部分中，通过阐述详细的方法设计电力现货市场（ESM）、辅助服务市场（ASM）中的保留能力产品（RC）和金融市场（FM）中的虚拟竞标（VB）产品来进一步演示这一理论。根据第一部分提出的理论，首先确定联合市场中的市场设计选项。接着，开发了马尔科夫博弈模型，展示了如何将市场设计选项和不确定风险纳入模型公式中。详细阐述了一种多智能体策略近端优化（MAPPO）算法，作为第一部分开发的广义市场模拟方法的实际实现。最后，通过使用一些市场运行绩效指标，案例研究演示如何选择最佳市场设计选项。

    This two-part paper develops a paradigmatic theory and detailed methods of the joint electricity market design using reinforcement-learning (RL)-based simulation. In Part 2, this theory is further demonstrated by elaborating detailed methods of designing an electricity spot market (ESM), together with a reserved capacity product (RC) in the ancillary service market (ASM) and a virtual bidding (VB) product in the financial market (FM). Following the theory proposed in Part 1, firstly, market design options in the joint market are specified. Then, the Markov game model is developed, in which we show how to incorporate market design options and uncertain risks in model formulation. A multi-agent policy proximal optimization (MAPPO) algorithm is elaborated, as a practical implementation of the generalized market simulation method developed in Part 1. Finally, the case study demonstrates how to pick the best market design options by using some of the market operation performance indicators 
    
[^85]: ChatGPT式的大规模基础模型在预测与健康管理中的应用：综述与路线图

    ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v1 [cs.LG])

    [http://arxiv.org/abs/2305.06472](http://arxiv.org/abs/2305.06472)

    该论文综述了基于大规模基础模型（LSF-Models）如ChatGPT和DALLE-E的人工智能（AI）技术在预测与健康管理（PHM）中的广泛应用。这种技术可以实现多模态、多任务、大量数据和超大模型范式，成为AI-2.0的新时代的标志之一。

    

    预测与健康管理技术在工业生产和设备维护中扮演着至关重要的角色，通过基于人工智能的PHM技术识别和预测设备故障和损坏。现在，基于大规模基础模型（LSF-Models）如ChatGPT和DALLE-E的AI技术，可以实现多模态、多任务、大规模数据和超大模型范式，成为AI-2.0的新时代的标志之一。这种技术广泛应用于各种工业领域，如铁路、能源和航空等，以提高设备的服务寿命和可靠性，同时降低生产成本和停机时间。

    Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT r
    
[^86]: 多智能体强化学习: 异步通信和线性函数逼近

    Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])

    [http://arxiv.org/abs/2305.06446](http://arxiv.org/abs/2305.06446)

    该论文探讨了多智能体强化学习在情节式马尔可夫决策过程中的协作问题，提出了一种基于值迭代的算法，可以实现异步通信，在保证合作优势的同时降低通信开销。通过提供和证明的算法和复杂度界限，为多智能体强化学习在实际应用中提供理论依据。

    

    我们研究了多智能体强化学习在情节式马尔科夫决策过程中的设置，多个智能体通过中央服务器进行通信以合作。我们提出了一种基于值迭代的可证明有效的算法，可以实现异步通信，同时确保合作优势且通信开销低。我们证明了在使用线性函数逼近的情况下，我们的算法具有 $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ 的遗憾值和 $\tilde{\mathcal{O}}(dHM^2)$ 的通信复杂度，其中 $d$ 是特征维数，$H$ 是时间跨度，$M$ 是智能体总数，$K$ 是总情节数。我们还提供了一个下限证明，表明通过协作至少需要 $\Omega(dM)$ 的通信复杂度才能改善性能。

    We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\Omega(dM)$ communication complexity is required to improve the performance through collaboration.
    
[^87]: 稀疏和密集神经网络中的小批量大小的相变

    Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2305.06435](http://arxiv.org/abs/2305.06435)

    本文系统地研究了小批量大小对稀疏和密集神经网络训练的影响，发现在临界值处会出现尖锐的相变，阐明了神经网络优化的基本机制。

    

    在训练人工神经网络时，使用小批量数据现在非常普遍。尽管已经广泛使用，但缺少定量解释最佳小批量大小应该是多大的理论。本文尝试系统地理解小批量大小在训练两层神经网络中的作用。在教师-学生情境下，使用稀疏教师，并聚焦于不同复杂度的任务，我们量化了改变小批量大小m的影响。我们发现，通常情况下，学生的泛化性能强烈依赖于m，并且可能在临界值mc处经历尖锐的相变，这样当m< mc时，训练过程失败，而当m> mc时，学生可以完美地学习或很好地泛化教师。相变是由统计力学首次发现的集体现象，并在许多科学领域观察到。找到在深度学习中改变小批量大小的相变，可以阐明神经网络优化的基本机制。

    The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m<m_c$ the training process fails, while for $m>m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the 
    
[^88]: 搜索UGLE真相：无监督GNN学习环境的调查

    Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])

    [http://arxiv.org/abs/2305.06026](http://arxiv.org/abs/2305.06026)

    本文提出了一种GNN学习环境下的社区检测算法比较框架，包括数据集和评估指标，以解决目前文献中对于基于GNN的社区检测缺乏公平且严谨评估的问题。

    

    图神经网络 (GNN) 是任何机器学习任务中的一个重要工具，因为它们能够学习图结构上的函数，这是一种强大和表达性强的数据表示。社区检测是一种无监督任务，越来越多地使用GNN进行。利用节点特征的多维度与图的连接性对图中的节点进行聚类，对从社交网络到基因组学的真实世界任务有许多应用。不幸的是，目前文献中缺乏公平且严谨评估基于GNN的社区检测的充分基准环境，从而可能阻碍这一新兴领域的进展。我们观察到这种情况下的特定困难是模糊的超参数调整环境与性能和评估数据集的冲突指标。在这项工作中，我们提出和评估了框架，用于在GNN学习环境中进行一致的社区检测算法比较。我们提供了一个基准数据集，并提出了评估指标，反映了检测到的社区的内在质量以及聚类的准确性。

    Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
    
[^89]: 核梯度下降学习中的随机平滑正则化

    Random Smoothing Regularization in Kernel Gradient Descent Learning. (arXiv:2305.03531v1 [stat.ML])

    [http://arxiv.org/abs/2305.03531](http://arxiv.org/abs/2305.03531)

    本文提出了一种随机平滑正则化的框架，能够自适应地、有效地学习属于经典Sobolev空间范围内的各种真实函数，通过引入噪声避免过拟合，该方法可以在较快的速度下实现最优收敛率。

    

    随机平滑数据增强是一种独特的正则化形式，可以通过向输入数据引入噪声来防止过拟合，鼓励模型学习更广泛的特征。尽管在各种应用中都取得了成功，但随机平滑的正则化能力缺乏系统的研究。在本文中，我们旨在通过提出一个随机平滑正则化的框架，能够自适应地、有效地学习属于经典 Sobolev 空间范围内的各种真实函数。具体而言，我们研究了两种基础的函数空间：低固有维度的 Sobolev 空间，其中包括 $D$ 维欧几里德空间或低维子流形作为特例，以及具有张量结构的混合平滑 Sobolev 空间。通过使用随机平滑正则化作为新型卷积平滑核，我们可以在这些情况下实现最优收敛率。

    Random smoothing data augmentation is a unique form of regularization that can prevent overfitting by introducing noise to the input data, encouraging the model to learn more generalized features. Despite its success in various applications, there has been a lack of systematic study on the regularization ability of random smoothing. In this paper, we aim to bridge this gap by presenting a framework for random smoothing regularization that can adaptively and effectively learn a wide range of ground truth functions belonging to the classical Sobolev spaces. Specifically, we investigate two underlying function spaces: the Sobolev space of low intrinsic dimension, which includes the Sobolev space in $D$-dimensional Euclidean space or low-dimensional sub-manifolds as special cases, and the mixed smooth Sobolev space with a tensor structure. By using random smoothing regularization as novel convolution-based smoothing kernels, we can attain optimal convergence rates in these cases using a ke
    
[^90]: 如何利用强化学习促进未来电力市场设计？第一部分：范型理论。（arXiv:2305.02485v1 [cs.AI]）

    How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 1: A Paradigmatic Theory. (arXiv:2305.02485v1 [cs.AI])

    [http://arxiv.org/abs/2305.02485](http://arxiv.org/abs/2305.02485)

    本文提出基于强化学习的方法，设计联合市场以应对电力行业脱碳，实现电力系统的安全和经济效益，并为环境做出贡献。该范型理论的框架将在两部分中详细介绍。

    

    面对电力行业脱碳的迫切需求，重新设计电力市场是一种宏观层面的方法，以适应可再生能源的高渗透率，并实现电力系统的操作安全、经济效率和环境友好性。然而，现有的市场设计方法学存在于能源现货市场（ESM）、辅助服务市场（ASM）和金融市场（FM）之间协调不足，即“联合市场”，以及缺乏可靠的基于模拟的验证。为了解决这些缺陷，本文将基于强化学习（RL）的模拟，开发联合市场设计的范型理论和详细方法。第一部分提出了这种新型市场设计哲学的理论和框架。首先，总结了在设计联合市场时存在的有争议的市场设计选项作为目标研究问题。其次，提出了马尔可夫博弈模型。

    In face of the pressing need of decarbonization in the power sector, the re-design of electricity market is necessary as a Marco-level approach to accommodate the high penetration of renewable generations, and to achieve power system operation security, economic efficiency, and environmental friendliness. However, existing market design methodologies suffer from the lack of coordination among energy spot market (ESM), ancillary service market (ASM) and financial market (FM), i.e., the "joint market", and the lack of reliable simulation-based verification. To tackle these deficiencies, this two-part paper develops a paradigmatic theory and detailed methods of the joint market design using reinforcement-learning (RL)-based simulation. In Part 1, the theory and framework of this novel market design philosophy are proposed. First, the controversial market design options while designing the joint market are summarized as the targeted research questions. Second, the Markov game model is deve
    
[^91]: GAMIVAL：移动云游戏内容的视频质量预测

    GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content. (arXiv:2305.02422v1 [eess.IV])

    [http://arxiv.org/abs/2305.02422](http://arxiv.org/abs/2305.02422)

    GAMIVAL是一种新型的游戏专用无参考视频质量评估模型，结合了多种优点。在移动云游戏内容的主观质量评估数据库上进行测试，表现出更好的NR VQA性能。

    

    近年来，移动云游戏行业迅速增长。当游戏视频从云服务器传输到客户端设备时，需要一种可以监测失真视频质量而无需参考视频的算法。然而，创建可以准确预测由计算机图形引擎渲染的流式游戏视频质量的无参考视频质量评估（NR VQA）模型是一项具有挑战性的问题，因为游戏内容通常在统计上与自然视频不同，缺乏细节，并包含许多平滑区域。我们创建了一种名为Gaming Video Quality Evaluator（GAMIVAL）的新型游戏专用NR VQA模型，结合和利用空间和时间游戏失真场景统计模型、神经噪声模型和客观质量模型的优点。GAMIVAL已经在一个大型的移动云游戏内容主观质量评估数据库上进行了训练和测试，并在游戏内容的NR VQA模型方面超越了最先进的模型。

    The mobile cloud gaming industry has been rapidly growing over the last decade. When streaming gaming videos are transmitted to customers' client devices from cloud servers, algorithms that can monitor distorted video quality without having any reference video available are desirable tools. However, creating No-Reference Video Quality Assessment (NR VQA) models that can accurately predict the quality of streaming gaming videos rendered by computer graphics engines is a challenging problem, since gaming content generally differs statistically from naturalistic videos, often lacks detail, and contains many smooth regions. Until recently, the problem has been further complicated by the lack of adequate subjective quality databases of mobile gaming content. We have created a new gaming-specific NR VQA model called the Gaming Video Quality Evaluator (GAMIVAL), which combines and leverages the advantages of spatial and temporal gaming distorted scene statistics models, a neural noise model, 
    
[^92]: 通过流形展平和重构进行表示学习

    Representation Learning via Manifold Flattening and Reconstruction. (arXiv:2305.01777v1 [cs.LG])

    [http://arxiv.org/abs/2305.01777](http://arxiv.org/abs/2305.01777)

    本文提出了一种通过神经网络构建展平流形的算法FlatNet，具有可解释性和可扩展性，同时在测试数据上具有良好的泛化性能。

    

    本文提出了一种算法，可以从流形的有限样本中显式构建一对神经网络，用于线性化和重构嵌入子流形。我们所生成的神经网络称为展平网络（FlatNet），在理论上具有可解释性，在计算上可扩展性强，并且在测试数据上具有良好的泛化性能，这种平衡通常在基于流形的学习方法中难以实现。我们基于合成的高维流形数据和2D图像数据进行了实证实验，并与其他模型进行了比较。我们的代码是公开的。

    This work proposes an algorithm for explicitly constructing a pair of neural networks that linearize and reconstruct an embedded submanifold, from finite samples of this manifold. Our such-generated neural networks, called flattening networks (FlatNet), are theoretically interpretable, computationally feasible at scale, and generalize well to test data, a balance not typically found in manifold-based learning methods. We present empirical results and comparisons to other models on synthetic high-dimensional manifold data and 2D image data. Our code is publicly available.
    
[^93]: 分层对抗鲁棒性与拒绝

    Stratified Adversarial Robustness with Rejection. (arXiv:2305.01139v1 [cs.LG])

    [http://arxiv.org/abs/2305.01139](http://arxiv.org/abs/2305.01139)

    本文提出了一种新的防御方法——基于一致预测的拒绝对抗训练（CPR），用于构建鲁棒的选择性分类器。该方法可以在分层拒绝设置下进行对抗鲁棒分类，并且在实验中表现出很好的性能。

    

    最近出现了一种对分类器进行有选择性地训练的对抗性方法——拒绝预测，用于增强对抗鲁棒性。虽然在许多应用中，拒绝预测会带来一定的成本，但现有研究通常将被扰动的输入的拒绝与零成本相关联，这可能会导致拒绝大量可以被正确分类的轻度扰动输入。本文研究了在分层拒绝设置下的对抗鲁棒分类，并且通过拒绝损失函数在扰动幅度上单调不减地建模来模拟拒绝成本。我们从理论上分析了分层拒绝设置，并提出了一种新的防御方法——基于一致预测的拒绝对抗训练（CPR）——用于构建鲁棒的选择性分类器。针对图像数据集进行的实验表明，所提出的方法在强适应性下明显优于现有方法。

    Recently, there is an emerging interest in adversarially training a classifier with a rejection option (also known as a selective classifier) for boosting adversarial robustness. While rejection can incur a cost in many applications, existing studies typically associate zero cost with rejecting perturbed inputs, which can result in the rejection of numerous slightly-perturbed inputs that could be correctly classified. In this work, we study adversarially-robust classification with rejection in the stratified rejection setting, where the rejection cost is modeled by rejection loss functions monotonically non-increasing in the perturbation magnitude. We theoretically analyze the stratified rejection setting and propose a novel defense method -- Adversarial Training with Consistent Prediction-based Rejection (CPR) -- for building a robust selective classifier. Experiments on image datasets demonstrate that the proposed method significantly outperforms existing methods under strong adaptiv
    
[^94]: 神经崩溃现象的研究：Grassmannian Frame、对称性和泛化

    A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization. (arXiv:2304.08914v1 [cs.LG])

    [http://arxiv.org/abs/2304.08914](http://arxiv.org/abs/2304.08914)

    本文提出了广义神经崩溃假设，发现了Grassmannian Frame结构和对称泛化现象，这对特征选择和神经网络设计都具有重要作用。

    

    本文通过证明广义神经崩溃假设推广了原始的神经崩溃现象。我们通过分类的优化和泛化得到了Grassmannian Frame结构。该结构在球面上最大化地分离了每两个类别的特征，并且不需要一个更大的特征维度。出于对Grassmannian Frame对称性的好奇，我们进行了一系列实验，探索不同Grassmannian Frame模型是否会产生不同的表现。结果我们发现了对称泛化现象。我们提出了一个关于置换对称泛化的定理。然而，为什么特征的不同方向会导致如此不同的泛化现象的问题仍然需要进一步研究。

    In this paper, we extends original Neural Collapse Phenomenon by proving Generalized Neural Collapse hypothesis. We obtain Grassmannian Frame structure from the optimization and generalization of classification. This structure maximally separates features of every two classes on a sphere and does not require a larger feature dimension than the number of classes. Out of curiosity about the symmetry of Grassmannian Frame, we conduct experiments to explore if models with different Grassmannian Frames have different performance. As a result, we discover the Symmetric Generalization phenomenon. We provide a theorem to explain Symmetric Generalization of permutation. However, the question of why different directions of features can lead to such different generalization is still open for future investigation.
    
[^95]: 通过解释不变性和等变性评估解释方法的健壮性

    Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v1 [cs.LG])

    [http://arxiv.org/abs/2304.06715](http://arxiv.org/abs/2304.06715)

    本文提出了解释不变性和等变性的概念，通过对对称群下具有不变性的神经网络，建立两种度量方法来提高解释方法对于不变性的健壮性并证明为一些流行的解释方法提供了理论健壮性保证。

    

    只有当解释方法忠实地描述所解释的模型时，解释方法才有价值。本文考虑了神经网络，其预测在特定对称群下具有不变性，这包括从卷积神经网络到图神经网络的流行架构。任何忠实描述这种类型模型的解释都需要与该不变性属性一致。我们通过运用几何深度学习的形式化方法，通过解释不变性和等变性的概念来形式化这种直觉。通过这种严格的形式化方法，我们得出了（1）两个度量来衡量任何解释方法相对于模型对称群的健壮性;（2）一些流行的解释方法的理论健壮性保证；（3）提高任何解释方法相对于对称群的不变性的系统方法。通过在与不同对称群相关的模型的解释中经验地测量我们的度量标准，我们展示了解释不变性和等变性对于强大的解释方法是重要的属性。

    Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associate
    
[^96]: 理解神经网络中的可塑性

    Understanding plasticity in neural networks. (arXiv:2303.01486v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01486](http://arxiv.org/abs/2303.01486)

    本文通过对失去可塑性问题进行系统实证分析，发现其深度与损失梯度曲率变化密切相关，饱和单元或发散梯度范数并非原因。基于这一发现，识别了一系列参数化和优化方法，有效提高神经网络保持可塑性的能力，在深度强化学习问题中具有显著的适应性和鲁棒性。

    

    可塑性是神经网络能够快速根据新信息更改其预测的能力，是深度强化学习系统适应性和鲁棒性的关键。深度神经网络即使在相对简单的学习问题中也会在训练过程中失去可塑性，但驱动这种现象的机制仍然不清楚。本文通过系统的实证分析，旨在深度理解可塑性的丧失，以引导未来对有针对性的解决方案的发展。我们发现可塑性的丧失与损失梯度曲率的变化密切相关，但通常发生在无饱和单元或发散梯度范数的情况下。基于这一洞见，我们识别出一些参数化和优化设计选择，使网络能够在训练过程中更好地保持可塑性。我们验证了这些基于特征的干预措施在一系列深度强化学习问题中的效用，证明它们显著提高了学习系统的适应性和鲁棒性。

    Plasticity, the ability of a neural network to quickly change its predictions in response to new information, is essential for the adaptability and robustness of deep reinforcement learning systems. Deep neural networks are known to lose plasticity over the course of training even in relatively simple learning problems, but the mechanisms driving this phenomenon are still poorly understood. This paper conducts a systematic empirical analysis into plasticity loss, with the goal of understanding the phenomenon mechanistically in order to guide the future development of targeted solutions. We find that loss of plasticity is deeply connected to changes in the curvature of the loss landscape, but that it typically occurs in the absence of saturated units or divergent gradient norms. Based on this insight, we identify a number of parameterization and optimization design choices which enable networks to better preserve plasticity over the course of training. We validate the utility of these f
    
[^97]: 用于因果效应估计的本地因果关系发现算法

    Local Causal Discovery for Estimating Causal Effects. (arXiv:2302.08070v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08070](http://arxiv.org/abs/2302.08070)

    本文介绍了一种新的本地因果关系发现算法 LDECC，可以提高算法的效率，实现因果效应估计。

    

    即使我们的数据背后的因果图形是未知的，我们仍然可以使用观察数据来缩小可能的平均处理效应（ATE）取值范围，方式是（1）识别到一个马尔可夫等价类; 和（2）估计该类中每个图的ATE。尽管PC算法在强保真度假设下可以识别该类别，但计算上的限制让人望而却步。幸运的是，仅需要关于处理的局部图结构即可识别出可能的ATE值集，这是由本地发现算法用于提高计算效率的事实。在本文中，我们提出了一种新的本地因果关系发现算法——使用急切碰撞检查的本地发现（LDECC），使用未屏蔽的碰撞器来使处理的父项与现有方法不同方向。我们展示了存在图形，在这些图形中，LDECC呈指数级性能优于现有的本地发现算法，反之亦然。此外，我们证明LDECC可以在某些情况下实现有效的估计并且具有更好的高斯误差下限性质。

    Even when the causal graph underlying our data is unknown, we can use observational data to narrow down the possible values that an average treatment effect (ATE) can take by (1) identifying the graph up to a Markov equivalence class; and (2) estimating that ATE for each graph in the class. While the PC algorithm can identify this class under strong faithfulness assumptions, it can be computationally prohibitive. Fortunately, only the local graph structure around the treatment is required to identify the set of possible ATE values, a fact exploited by local discovery algorithms to improve computational efficiency. In this paper, we introduce Local Discovery using Eager Collider Checks (LDECC), a new local causal discovery algorithm that leverages unshielded colliders to orient the treatment's parents differently from existing methods. We show that there exist graphs where LDECC exponentially outperforms existing local discovery algorithms and vice versa. Moreover, we show that LDECC an
    
[^98]: 基于随机先验网络的高维输出可扩展贝叶斯优化

    Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07260](http://arxiv.org/abs/2302.07260)

    本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。

    

    科学和工程中的一些基本问题涉及到未知的高维度映射一组可控变量到昂贵实验结果的黑盒函数的全局优化任务。贝叶斯优化（BO）技术已被证明在使用相对较少的目标函数评估时处理全局优化问题时非常有效，但当处理高维输出时，其性能受到影响。为克服维度主要挑战，本文提出了一个基于带随机先验的神经网络的自举集成的BO和序贯决策制定的深度学习框架。使用适当的体系结构选择，我们证明了所提出的框架可以近似设计变量和感兴趣量之间的功能关系，即使在后者取值于高维向量空间或甚至无限维函数空间的情况下。在贝叶斯优化的背景下，该方法允许高效和可扩展的处理高维度黑盒函数的全局优化。

    Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
    
[^99]: 坚固且可扩展的贝叶斯在线变点检测

    Robust and Scalable Bayesian Online Changepoint Detection. (arXiv:2302.04759v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.04759](http://arxiv.org/abs/2302.04759)

    该论文提出了一种在线变点检测方法，具有坚固和可扩展性，通过利用广义贝叶斯视角提供了可证明的坚固性，并通过扩散得分匹配解决了可扩展性问题。所得算法是精确的，更新简单，速度较之前的算法快10倍以上。

    

    本文提出了一种在线、可证明坚固且可扩展的贝叶斯方法用于变点检测。所得算法相对于之前的工作具有重要优势：通过利用广义贝叶斯视角提供了可证明的坚固性，并解决了之前尝试中的可扩展性问题。具体来说，所提出的广义贝叶斯形式主义通过利用扩散得分匹配导致共轭后验的参数可以通过封闭形式获得。所得算法是精确的，可以通过简单的代数更新，并且比其最接近的竞争对手快10倍以上。

    This paper proposes an online, provably robust, and scalable Bayesian approach for changepoint detection. The resulting algorithm has key advantages over previous work: it provides provable robustness by leveraging the generalised Bayesian perspective, and also addresses the scalability issues of previous attempts. Specifically, the proposed generalised Bayesian formalism leads to conjugate posteriors whose parameters are available in closed form by leveraging diffusion score matching. The resulting algorithm is exact, can be updated through simple algebra, and is more than 10 times faster than its closest competitor.
    
[^100]: GPS++: 恢复消息传递艺术以进行分子属性预测

    GPS++: Reviving the Art of Message Passing for Molecular Property Prediction. (arXiv:2302.02947v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02947](http://arxiv.org/abs/2302.02947)

    GPS++是一种混合消息传递神经网络/图形转换器模型，通过集成本地消息传递组件和全局关注，以及过去文献中的其他关键思想，在分子属性预测方面实现了最先进的结果。

    

    我们提出了 GPS++，一种混合消息传递神经网络/图形转换器模型，用于分子属性预测。我们的模型集成了经过良好调整的本地消息传递组件和有偏向性的全局关注以及来自先前文献的其他关键思想，以在大规模分子数据集 PCQM4Mv2 上实现最先进的结果。通过彻底的消融研究，我们强调了个别组件的影响，并发现即使在最近图形转换器的主导下，消息传递仍然是进行三维分子属性预测的竞争方法，几乎所有模型的性能都可以在不使用全局自我关注的情况下保持。 我们还发现，当没有三维位置信息时，我们的方法比以前的技术更准确。

    We present GPS++, a hybrid Message Passing Neural Network / Graph Transformer model for molecular property prediction. Our model integrates a well-tuned local message passing component and biased global attention with other key ideas from prior literature to achieve state-of-the-art results on large-scale molecular dataset PCQM4Mv2. Through a thorough ablation study we highlight the impact of individual components and find that nearly all of the model's performance can be maintained without any use of global self-attention, showing that message passing is still a competitive approach for 3D molecular property prediction despite the recent dominance of graph transformers. We also find that our approach is significantly more accurate than prior art when 3D positional information is not available.
    
[^101]: 在交互环境中使用在线强化学习对大型语言模型进行基础设施建设

    Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning. (arXiv:2302.02662v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02662](http://arxiv.org/abs/2302.02662)

    本文研究了一种名为GLAM的方法，通过功能基础设施建设，利用在线强化学习提高LLM代理程序的性能来实现LLMs与环境之间的对齐，解决决策问题。

    

    最近的研究成功地利用了大型语言模型（LLM）捕捉世界物理的抽象知识，以解决决策问题。然而，LLMs的知识与环境之间的对齐可能是错误的，并且由于缺乏基础设施建设而限制了其功能能力。在本文中，我们研究了一种通过功能基础设施建设实现这种对齐的方法（称为GLAM）：我们考虑一个使用LLM作为策略的代理程序，随着代理程序与环境进行交互而逐步更新，并利用在线强化学习来提高其解决目标的性能。使用一个交互式的文本环境设计来研究更高级形式的基础设施建设，以及一组空间和导航任务，我们研究了几个科学问题：1）LLMs能否提高各种RL任务的在线学习的样本效率？2）它如何提高不同形式的泛化？3）在线学习的影响是什么？我们通过功能方式研究这些问题。

    Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functio
    
[^102]: AdaptDiffuser: 扩散模型作为自适应自进化规划器

    AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners. (arXiv:2302.01877v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01877](http://arxiv.org/abs/2302.01877)

    本文介绍了AdaptDiffuser方法,通过使用奖励梯度的指导生成富有丰富的目标条件任务的合成专家数据，并通过鉴别器选择高质量数据来微调扩散模型，从而改善对未知任务的泛化能力。

    

    扩散模型在许多任务中展示了强大的生成能力，具有作为离线强化学习范例的潜力。然而，扩散模型的质量受到训练数据不足的限制，这阻碍了规划的性能和对新任务的泛化能力。本文介绍了AdaptDiffuser，一种带有扩散的进化规划方法，可以自我进化以改进扩散模型，从而获得更好的规划器，不仅适用于已知任务，而且还可以适应未知任务。AdaptDiffuser使用奖励梯度的指导生成富有丰富的目标条件任务的合成专家数据。然后通过鉴别器选择高质量数据来微调扩散模型，从而改善对未知任务的泛化能力。在两个基准环境以及 KUKA 工业机器人臂和 Maze2D 环境中对两个精心设计的未知任务进行的实证实验表明，该方法在性能上具有优势。

    Diffusion models have demonstrated their powerful generative capability in many tasks, with great potential to serve as a paradigm for offline reinforcement learning. However, the quality of the diffusion model is limited by the insufficient diversity of training data, which hinders the performance of planning and the generalizability to new tasks. This paper introduces AdaptDiffuser, an evolutionary planning method with diffusion that can self-evolve to improve the diffusion model hence a better planner, not only for seen tasks but can also adapt to unseen tasks. AdaptDiffuser enables the generation of rich synthetic expert data for goal-conditioned tasks using guidance from reward gradients. It then selects high-quality data via a discriminator to finetune the diffusion model, which improves the generalization ability to unseen tasks. Empirical experiments on two benchmark environments and two carefully designed unseen tasks in KUKA industrial robot arm and Maze2D environments demons
    
[^103]: 强化学习中的知识转移——不适用动作学习

    Inapplicable Actions Learning for Knowledge Transfer in Reinforcement Learning. (arXiv:2211.15589v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15589](http://arxiv.org/abs/2211.15589)

    本文提出了一种新的强化学习方法，在学习过程中通过奖励设计让智能体学习屏蔽无关的动作，从而降低了样本数量，并获得了比传统RL方法更好的最优策略。

    

    强化学习算法在具有许多可能动作的环境中很难扩展，需要大量的样本才能学习到最优策略。传统的方法是在每个可能的状态下考虑相同的固定动作空间，这意味着智能体必须理解、同时学习如何最大化其奖励，忽略无关的动作，比如“不适用动作”（即在给定状态下执行时不会影响环境的动作）。了解这些信息可以通过屏蔽与寻找最优策略相关的动作，从策略分布中去探索无关的动作，从而帮助降低强化学习算法的样本复杂度。本文提出了一种新的方法，在学习阶段通过一种新颖的奖励设计方法学习屏蔽不适用的动作，智能体可以学习预测一个动作是否适用，并根据其预测获得奖励。实验结果表明，相比于传统的RL方法，我们的方法可以使用较少的样本学习到最优策略。

    Reinforcement Learning (RL) algorithms are known to scale poorly to environments with many available actions, requiring numerous samples to learn an optimal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as $\textit{inapplicable actions}$ (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. While this technique has been formalized for quite some time within the Automated Planning community with the concept of precondition in the STRIPS language, RL algorithms have never formally taken advantage of this information to prune the search space to explore. This is typically done in an 
    
[^104]: 弱连接网络系统中学习协同聚类方法

    Learning Coherent Clusters in Weakly-Connected Network Systems. (arXiv:2211.15301v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2211.15301](http://arxiv.org/abs/2211.15301)

    本文提出一种学习协同聚类方法，通过基于图拉普拉斯矩阵的谱聚类算法识别协同群组，构建减少网络，并给出逼近误差上界。

    

    本文提出一种结构保持的模型简化方法，用于具有紧密连接组件的大规模动态网络。首先，使用基于图拉普拉斯矩阵的谱聚类算法识别协同群组。然后，构建减少网络，其中每个节点表示每个协同组的聚合动态，并且减少网络捕捉组之间的动态耦合。当网络图从权重随机块模型随机生成时，给出了逼近误差的上界。最后，数值实验验证了我们的理论发现。

    We propose a structure-preserving model-reduction methodology for large-scale dynamic networks with tightly-connected components. First, the coherent groups are identified by a spectral clustering algorithm on the graph Laplacian matrix that models the network feedback. Then, a reduced network is built, where each node represents the aggregate dynamics of each coherent group, and the reduced network captures the dynamic coupling between the groups. We provide an upper bound on the approximation error when the network graph is randomly generated from a weight stochastic block model. Finally, numerical experiments align with and validate our theoretical findings.
    
[^105]: GLUE-X: 从ODD普适性角度评估自然语言理解模型

    GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08073](http://arxiv.org/abs/2211.08073)

    本文提出了第一个创建名为方法的统一基准的尝试，用于评估NLP模型中的OOD鲁棒性，该基准包括13个公开可用的OOD测试数据集，并在21个常用的PLMs上对8个经典NLP任务进行评估。

    

    预训练语言模型（PLMs）通过利用大量的训练数据，已知可以提高自然语言理解模型的泛化性能。然而，许多NLP任务中的ODD普适性问题仍然存在，这限制了这些方法在现实世界中的部署。本文提出了第一个创建名为方法的统一基准的尝试，用于评估NLP模型中的OOD鲁棒性，强调OOD鲁棒性的重要性，并提供如何衡量模型的鲁棒性以及如何改善模型的见解。该基准包括13个公开可用的OOD测试数据集，并在21个常用的PLMs（包括GPT-3和GPT-3.5）上对8个经典NLP任务进行评估。我们的研究结果确认了在所有设置下，与ID准确度相比，存在显着的性能下降，需要改善NLP任务中的OOD准确度。

    Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named \method for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.
    
[^106]: 以训练数据生成器为调整语言模型的增强学习少样本方法

    Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning. (arXiv:2211.03044v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.03044](http://arxiv.org/abs/2211.03044)

    该论文通过调整预训练语言模型生成大量新的训练样本，从而增强原始训练集，提高了少样本学习的性能。

    

    最近的研究揭示了预训练语言模型（PLM）惊人的少样本学习能力：它们可以在以提示形式表达的少量标记数据上微调后快速适应新任务，而无需丰富的任务特定注释。尽管有着很有前途的表现，但大多数仅从少量训练集学习的现有少样本方法仍然比非平凡的全监督训练表现不佳。在本文中，我们从不同的角度研究了使用PLMs进行少样本学习：我们首先调整自回归PLM，然后使用它作为生成器，合成大量新的训练样本，以增强原始训练集。为了鼓励生成器产生具有标签区分能力的样本，我们通过加权最大似然度量训练它，在其中每个令牌的权重基于一个区分性元学习目标自动调整。然后可以在增加后的训练集上微调分类PLM。

    Recent studies have revealed the intriguing few-shot learning ability of pretrained language models (PLMs): They can quickly adapt to a new task when fine-tuned on a small amount of labeled data formulated as prompts, without requiring abundant task-specific annotations. Despite their promising performance, most existing few-shot approaches that only learn from the small training set still underperform fully supervised training by nontrivial margins. In this work, we study few-shot learning with PLMs from a different perspective: We first tune an autoregressive PLM on the few-shot samples and then use it as a generator to synthesize a large amount of novel training samples which augment the original training set. To encourage the generator to produce label-discriminative samples, we train it via weighted maximum likelihood where the weight of each token is automatically adjusted based on a discriminative meta-learning objective. A classification PLM can then be fine-tuned on both the f
    
[^107]: 网络流的图神经建模

    Graph Neural Modeling of Network Flows. (arXiv:2209.05208v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05208](http://arxiv.org/abs/2209.05208)

    本文提出了一种新颖的网络流问题图学习架构 PEW，相较于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。

    

    网络流问题涉及将流量分布在网络中，以使基础设施得到有效利用，这在交通运输和物流中是无处不在的。其中，多商品网络流 (MCNF) 问题是普遍感兴趣的，因为它涉及在多个源和汇之间分配不同大小的多个流，同时实现链路的有效利用。由于数据驱动优化的吸引力，这些问题越来越多地使用图学习方法来解决。本文提出了一种新颖的网络流问题图学习架构 PEW (Per-Edge Weights)。此方法基于图注意力网络，并沿着每个链接使用不同参数化的消息函数。我们通过使用 $17$ 个服务提供商拓扑和 $2$ 个路由方案进行互联网流量路由案例研究，对所提出的解决方案进行了广泛的评估。我们展示了 PEW 相对于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。

    Network flow problems, which involve distributing traffic over a network such that the underlying infrastructure is used effectively, are ubiquitous in transportation and logistics. Among them, the Multi-Commodity Network Flow (MCNF) problem is of general interest, as it concerns the distribution of multiple flows of different sizes between several sources and sinks, while achieving effective utilization of the links. Due to the appeal of data-driven optimization, these problems have increasingly been approached using graph learning methods. In this paper, we propose a novel graph learning architecture for network flow problems called Per-Edge Weights (PEW). This method builds on a Graph Attention Network and uses distinctly parametrized message functions along each link. We extensively evaluate the proposed solution through an Internet flow routing case study using $17$ Service Provider topologies and $2$ routing schemes. We show that PEW yields substantial gains over architectures wh
    
[^108]: 强化学习在去监管电力市场中的应用：综合回顾

    Applications of Reinforcement Learning in Deregulated Power Market: A Comprehensive Review. (arXiv:2205.08369v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.08369](http://arxiv.org/abs/2205.08369)

    本文综述了RL在去监管电力市场中应用的最新研究成果和未来的研究方向，RL相较于传统优化工具具有优势，能够解决特征不确定、计算效率低等难题，是一种有效的方法。

    

    随着可再生能源的渗透和电力行业的去监管与市场化，电力市场运营范式正在发生转变。在这些新范式下，优化投标策略和分配方法成为市场参与者和电力系统运营商的优先考虑问题，同时也存在着特征不确定、计算效率低、超前决策要求等障碍。为了解决这些问题，作为一种新兴的机器学习技术，相较于传统的优化工具，强化学习（RL）在学术界和工业界中扮演着越来越重要的角色。本文综述了RL在去监管电力市场运营中的应用，包括投标和分配策略优化等方面，基于150余篇精选的文献。对于每个应用，除了总结广义方法的范例之外，还重点阐述了最新的研究成果和未来的研究方向。

    The increasing penetration of renewable generations, along with the deregulation and marketization of power industry, promotes the transformation of power market operation paradigms. The optimal bidding strategy and dispatching methodology under these new paradigms are prioritized concerns for both market participants and power system operators, with obstacles of uncertain characteristics, computational efficiency, as well as requirements of hyperopic decision-making. To tackle these problems, the Reinforcement Learning (RL), as an emerging machine learning technique with advantages compared with conventional optimization tools, is playing an increasingly significant role in both academia and industry. This paper presents a comprehensive review of RL applications in deregulated power market operation including bidding and dispatching strategy optimization, based on more than 150 carefully selected literatures. For each application, apart from a paradigmatic summary of generalized metho
    
[^109]: 模型量化在深度神经网络中的应用：综述与分析

    A Comprehensive Survey on Model Quantization for Deep Neural Networks. (arXiv:2205.07877v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.07877](http://arxiv.org/abs/2205.07877)

    本文综述了深度神经网络中的模型量化，这是一种用低位宽存储全精度值以实现节约内存和操作成本的压缩方法。文章分类介绍了各种量化方法，并讨论了使用比例因子匹配数据范围和适当的训练方法的重要性。本文还回顾了模型量化的最新研究，并强调了其优缺点和当前的挑战和未来研究方向。

    

    深度神经网络在机器学习领域中的应用取得了重大进展，但是需要大量的参数存储和运算会带来硬件成本的增加和挑战。对此，提出了压缩方法以设计高效的加速器，其中最重要的方法是把全精度的值存储在低位宽中，这就可以节约内存同时用低成本的简单运算代替原本的操作。由于模型量化的灵活性和对设计高效硬件的影响，最近几年提出了许多深度神经网络量化方法，因此需要进行综合性的调查以更好地理解、分析和比较。本文提供了一份全面的综述，介绍了量化概念并从不同角度分类方法，讨论了使用比例因子匹配数据范围的重要性以及使用适当的训练方法避免精度损失的方法。我们还回顾了近年来对模型量化的研究，并强调其优点和缺点。最后，我们讨论了当前的挑战和未来的研究方向。

    Recent advances in machine learning by deep neural networks are significant. But using these networks has been accompanied by a huge number of parameters for storage and computations that leads to an increase in the hardware cost and posing challenges. Therefore, compression approaches have been proposed to design efficient accelerators. One important approach for deep neural network compression is quantization that full-precision values are stored in low bit-width. In this way, in addition to memory saving, the operations will be replaced by simple ones with low cost. Many methods are suggested for DNNs Quantization in recent years, because of flexibility and influence in designing efficient hardware. Therefore, an integrated report is essential for better understanding, analysis, and comparison. In this paper, we provide a comprehensive survey. We describe the quantization concepts and categorize the methods from different perspectives. We discuss using the scale factor to match the 
    
[^110]: 分层贝叶斯建模在工程机群间多任务学习中的知识转移应用

    Hierarchical Bayesian Modelling for Knowledge Transfer Across Engineering Fleets via Multitask Learning. (arXiv:2204.12404v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.12404](http://arxiv.org/abs/2204.12404)

    本文提出了一种分层贝叶斯建模方法，利用操作机群中的领域专业知识，在不同的子群之间自动地共享信息。该方法成功地解决了卡车机群的生存分析和风电场的功率预测问题。

    

    本论文提出了一种基于群体级别分析的方法来解决工程基础设施预测建模中数据稀疏的问题。利用可解释的分层贝叶斯方法和操作机群数据，自然地将领域专业知识编码（和适当共享）到不同子群之间，分别代表（i）使用类型，（ii）部件或（iii）运行条件。具体而言，通过假设（和先验分布），利用领域专业知识来限制模型，使得方法能够自动在类似资产之间共享信息，以改善卡车机群的生存分析和风电场的功率预测。在每个资产管理示例中，通过合并推理，在机群上学习一组相关函数，以学习群体模型。当子机群在层次结构的不同级别上共享相关信息时，参数估计得到改进。反过来，具有不完整数据的群体自动借用统计强度。

    A population-level analysis is proposed to address data sparsity when building predictive models for engineering infrastructure. Utilising an interpretable hierarchical Bayesian approach and operational fleet data, domain expertise is naturally encoded (and appropriately shared) between different sub-groups, representing (i) use-type, (ii) component, or (iii) operating condition. Specifically, domain expertise is exploited to constrain the model via assumptions (and prior distributions) allowing the methodology to automatically share information between similar assets, improving the survival analysis of a truck fleet and power prediction in a wind farm. In each asset management example, a set of correlated functions is learnt over the fleet, in a combined inference, to learn a population model. Parameter estimation is improved when sub-fleets share correlated information at different levels of the hierarchy. In turn, groups with incomplete data automatically borrow statistical strength
    
[^111]: BoMD：适用于嘈杂X光分类的多标签描述符包

    BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification. (arXiv:2203.01937v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2203.01937](http://arxiv.org/abs/2203.01937)

    本文提出了一种适用于多标签、嘈杂CXR学习的方法，使用基于袋的多标签描述符平滑地重新标记数据集中的样本，并进行训练以提高模型性能。

    

    深度学习方法在医学图像问题的分类精度方面表现出色，这在很大程度上归功于具有清洁标签的大规模数据集的可用性。然而，考虑到这种手动注释的高成本，新的医学图像分类问题可能需要依赖于从放射学报告中提取的机器生成的嘈杂标签。事实上，许多胸部X光分类器已经从带有嘈杂标签的数据集中建模，但它们的训练过程通常不具有噪声标签样本的鲁棒性，导致次优模型。此外，CXR数据集大多是多标记的，因此当前设计用于多类问题的嘈杂标签学习方法不能轻松地进行调整。本文提出了一种新方法，用于嘈杂多标签CXR学习，其中检测并平滑地重新标记数据集中的样本，然后用于训练常见的多标签分类器。该方法优化了一个基于袋的多标签表示方法，以便有效地使用从放射学报告中提取的信息。

    Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a ba
    
[^112]: Transformer在时间序列中的应用概述

    Transformers in Time Series: A Survey. (arXiv:2202.07125v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07125](http://arxiv.org/abs/2202.07125)

    本文针对Transformer在时间序列建模中的应用进行了系统回顾，总结了其优点及局限性，从网络结构和应用两个角度审视了其适应和改进。

    

    Transformer在自然语言处理和计算机视觉中已经取得了优越的性能，也引起了时间序列社区的极大兴趣。Transformer的多个优势之一是能够捕捉长程依赖和相互作用，特别适合于时间序列建模，极大地推动了时间序列应用的发展。本文系统地回顾了Transformer在时间序列建模中的应用，并强调了其优点及局限性。具体而言，我们从网络结构和应用两个层面审视了Transformer在时间序列上的适应和改进。从网络结构的角度，我们总结了为了适应时间序列分析中的挑战而做出的改变和调整。从应用的角度，我们根据常见任务（包括预测、异常检测和分类）对时间序列Transformer进行了分类。

    Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically,
    
[^113]: 图上信息传播模型

    Models for information propagation on graphs. (arXiv:2201.07577v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2201.07577](http://arxiv.org/abs/2201.07577)

    本文提出了统一的图上信息传播模型，其中包括三种不同的类别，利用波、路径行程时间和eikonal方程来描述信息的传播，并给出了它们之间的等价性。此外，本文还提出了一种新的混合模型，用于描述波和eikonal模型的结合。作者在随机图形、小世界图和实际网络上进行了数值模拟。

    

    我们提出和统一了不同的图上信息传播模型。第一类模型将传播建模为一种波，它从一组已知节点在初始时间开始向所有其他未知节点传播，传播的顺序由信息波前的到达时间确定。第二类模型基于路径上的行程时间的概念。从一组初始已知节点到一个节点的信息传播时间被定义为所有可以到达该节点的路径的子集上的广义旅行时间的最小值。最后一个模型类是通过在每个未知节点上施加一个eikonal形式的局部方程，并在已知节点处施加边界条件来给出的。在一个节点的解的值与具有较低值的相邻节点的解的值耦合。我们提供了模型类的精确公式，并证明了它们之间的等价性。受到第一到达时间模型和eikonal方程之间的联系的启发，我们提出了一种新的混合形式，结合了波和eikonal模型。最后，我们在各种图形上展示了模型的数值模拟，包括随机图形、小世界图和实际网络。

    We propose and unify classes of different models for information propagation over graphs. In a first class, propagation is modelled as a wave which emanates from a set of known nodes at an initial time, to all other unknown nodes at later times with an ordering determined by the arrival time of the information wave front. A second class of models is based on the notion of a travel time along paths between nodes. The time of information propagation from an initial known set of nodes to a node is defined as the minimum of a generalised travel time over subsets of all admissible paths. A final class is given by imposing a local equation of an eikonal form at each unknown node, with boundary conditions at the known nodes. The solution value of the local equation at a node is coupled to those of neighbouring nodes with lower values. We provide precise formulations of the model classes and prove equivalences between them. Motivated by the connection between first arrival time model and the e
    
[^114]: 基于连接敏感性的训练免费DARTS：从架构级评分到操作级敏感性分析

    Connection Sensitivity Matters for Training-free DARTS: From Architecture-Level Scoring to Operation-Level Sensitivity Analysis. (arXiv:2106.11542v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11542](http://arxiv.org/abs/2106.11542)

    本论文提出一种叫做ZEROS的连接概念来评估DARTS中的操作重要性，使得整个架构的搜索过程更高效，提出了一种基于NTK理论的全新框架FreeDARTS。

    

    最近提出的训练免费NAS方法放弃了训练阶段并设计了各种零成本代理作为评分，以识别出优秀的架构，引起了神经架构搜索的极高计算效率。本文提出了一个有趣的问题：我们能否通过训练免费的方式适当地测量DARTS中的操作重要性，避免参数密集的偏差？我们通过边缘连通性的角度来研究这个问题，并提供了一个肯定的答案，通过定义一个连接概念“ZERo-cost Operation Sensitivity (ZEROS)”，来评分DARTS中候选操作的重要性。通过设计一种迭代和数据不可知的方式来利用ZEROS进行NAS，我们的新尝试导致了一个名为“training free differentiable architecture search (FreeDARTS)”的框架。基于神经切向核(NTK)的理论，我们证明了所提出的连接评分与泛化下降呈负相关。

    The recently proposed training-free NAS methods abandon the training phase and design various zero-cost proxies as scores to identify excellent architectures, arousing extreme computational efficiency for neural architecture search. In this paper, we raise an interesting problem: can we properly measure the operation importance in DARTS through a training-free way, with avoiding the parameter-intensive bias? We investigate this question through the lens of edge connectivity, and provide an affirmative answer by defining a connectivity concept, ZERo-cost Operation Sensitivity (ZEROS), to score the importance of candidate operations in DARTS at initialization. By devising an iterative and data-agnostic manner in utilizing ZEROS for NAS, our novel trial leads to a framework called training free differentiable architecture search (FreeDARTS). Based on the theory of Neural Tangent Kernel (NTK), we show the proposed connectivity score provably negatively correlated with the generalization bo
    
[^115]: 无限不平衡下的线性分类器研究

    Linear Classifiers Under Infinite Imbalance. (arXiv:2106.05797v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.05797](http://arxiv.org/abs/2106.05797)

    研究了在无限不平衡情况下的线性分类器，通过权重函数指定的经验损失最小化系数。截距发散但其余系数向量有一个有限的几乎肯定的极限，极限依赖于权重函数的左尾增长速率。极限系数向量反映稳健性或保守性属性，而在亚指数情况下，极限等价于少数类的上采样分布的隐式选择。

    

    本研究探讨了在一个类别的样本数量增长到无穷大而另一个类别的样本数量保持不变的情况下，二元分类中的线性判别函数的行为。分类器的系数通过一个权重函数指定的经验损失最小化。我们展示了，对于广泛的权重函数类，截距发散但其余系数向量在无穷不平衡情况下具有一个有限的几乎肯定的极限，这扩展了之前对逻辑回归的研究。极限依赖于权重函数的左尾增长速率，对此我们区分了两种情况：亚指数和指数。极限系数向量反映了稳健性或保守性属性，因为它们优化了某些最坏情况的替代方法。在亚指数情况下，极限等价于少数类的上采样分布的隐式选择。我们在信用风险设置中应用了这些思想。

    We study the behavior of linear discriminant functions for binary classification in the infinite-imbalance limit, where the sample size of one class grows without bound while the sample size of the other remains fixed. The coefficients of the classifier minimize an empirical loss specified through a weight function. We show that for a broad class of weight functions, the intercept diverges but the rest of the coefficient vector has a finite almost sure limit under infinite imbalance, extending prior work on logistic regression. The limit depends on the left-tail growth rate of the weight function, for which we distinguish two cases: subexponential and exponential. The limiting coefficient vectors reflect robustness or conservatism properties in the sense that they optimize against certain worst-case alternatives. In the subexponential case, the limit is equivalent to an implicit choice of upsampling distribution for the minority class. We apply these ideas in a credit risk setting, wit
    
[^116]: 深度回归中的变量误差模型的随机不确定性

    Aleatoric uncertainty for Errors-in-Variables models in deep regression. (arXiv:2105.09095v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.09095](http://arxiv.org/abs/2105.09095)

    本文提出了一种基于贝叶斯深度回归的方法，利用变量误差模型考虑所使用神经网络的输入所关联的不确定性，并将预测不确定性分解为随机和认识部分。相比于不使用该模型，使用错误变量模型能够提高对已知回归函数的覆盖率，且保持预测性能。

    

    深度学习的贝叶斯处理可以计算与深度神经网络预测相关的不确定性。我们展示了如何在贝叶斯深度回归中使用变量误差的概念，以考虑所使用神经网络的输入所关联的不确定性。所提出的方法利用了一个相关但通常被忽视的不确定性源，并将预测不确定性分解为随机和认识部分，这在统计学角度更完整，而且在很多情况下更一致。我们通过各种模拟和真实的例子讨论了这种方法，并观察到使用变量误差模型会增加不确定性，同时保持不使用变量误差模型的模型的预测性能。对于已知回归函数的例子，我们观察到变量误差模型大大提高了对基础事实的覆盖，表明该方法在回归问题中表现良好。

    A Bayesian treatment of deep learning allows for the computation of uncertainties associated with the predictions of deep neural networks. We show how the concept of Errors-in-Variables can be used in Bayesian deep regression to also account for the uncertainty associated with the input of the employed neural network. The presented approach thereby exploits a relevant, but generally overlooked, source of uncertainty and yields a decomposition of the predictive uncertainty into an aleatoric and epistemic part that is more complete and, in many cases, more consistent from a statistical perspective. We discuss the approach along various simulated and real examples and observe that using an Errors-in-Variables model leads to an increase in the uncertainty while preserving the prediction performance of models without Errors-in-Variables. For examples with known regression function we observe that this ground truth is substantially better covered by the Errors-in-Variables model, indicating 
    
[^117]: 不确定性的不平等影响：平权行动与平权信息

    The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information. (arXiv:2102.10019v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.10019](http://arxiv.org/abs/2102.10019)

    本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。

    This paper proves that uncertainty has a disparate impact on different demographic groups, with varying types of errors. The proposed strategy, called Affirmative Information, can eliminate this disparity and broaden access to opportunity, serving as an alternative to Affirmative Action.

    像贷款批准、医疗干预和大学录取这样的关键决策是在存在不确定性的情况下进行预测的。在本文中，我们证明了不确定性具有不平等的影响。虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化：平均结果较高的群体通常被分配更高的假阳性率，而平均结果较低的群体则被分配更高的假阴性率。我们展示了额外的数据获取可以消除这种差异并扩大机会的获取。我们称之为平权信息的策略可以作为平权行动的替代方案。

    Critical decisions like loan approvals, medical interventions, and college admissions are guided by predictions made in the presence of uncertainty. In this paper, we prove that uncertainty has a disparate impact. While it imparts errors across all demographic groups, the types of errors vary systematically: Groups with higher average outcomes are typically assigned higher false positive rates, while those with lower average outcomes are assigned higher false negative rates. We show that additional data acquisition can eliminate the disparity and broaden access to opportunity. The strategy, which we call Affirmative Information, could stand as an alternative to Affirmative Action.
    
[^118]: 线性递归神经网络的力量

    The Power of Linear Recurrent Neural Networks. (arXiv:1802.03308v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1802.03308](http://arxiv.org/abs/1802.03308)

    本研究展示了线性递归神经网络(LRNNs)可以逼近任何时变函数f(t)。通过检查网络转移矩阵的主要特征值，可以显著降低LRNN的规模。LRNNs具有以椭圆轨迹结束的有趣特性，并允许预测进一步的值和函数的紧凑表示。

    

    循环神经网络是处理时间序列的有力工具。我们展示了autoregressive linear,即线性激活循环神经网络(LRNNs)可以逼近由多个函数值给出的任何时变函数f(t)。逼近可以通过简单地解决一个线性方程组来有效学习；不需要反向传播或类似的方法。此外，这可能是本文的主要贡献，通过检查网络转移矩阵的频谱，即它的特征值，只取最相关的组件，可以在一步中显著降低LRNN的规模。因此，与其他方法不同，我们不仅可以学习网络权重，还可以学习网络架构。LRNNs具有有趣的特性：它们最终会以椭圆轨迹结束，并允许预测进一步的值和函数的紧凑表示。我们通过几个实验演示了这一点。

    Recurrent neural networks are a powerful means to cope with time series. We show how autoregressive linear, i.e., linearly activated recurrent neural networks (LRNNs) can approximate any time-dependent function f(t) given by a number of function values. The approximation can effectively be learned by simply solving a linear equation system; no backpropagation or similar methods are needed. Furthermore, and this is probably the main contribution of this article, the size of an LRNN can be reduced significantly in one step after inspecting the spectrum of the network transition matrix, i.e., its eigenvalues, by taking only the most relevant components. Therefore, in contrast to other approaches, we do not only learn network weights but also the network architecture. LRNNs have interesting properties: They end up in ellipse trajectories in the long run and allow the prediction of further values and compact representations of functions. We demonstrate this by several experiments, among the
    

