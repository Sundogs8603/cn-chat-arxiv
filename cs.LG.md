# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics.](http://arxiv.org/abs/2309.07120) | 多模态训练的MLLM在纯NLP任务中表现出卓越的真实性和伦理对齐能力，这得益于视觉指导调优和优秀的指导质量。 |
| [^2] | [PILOT: A Pre-Trained Model-Based Continual Learning Toolbox.](http://arxiv.org/abs/2309.07117) | 本论文介绍了一个名为PILOT的基于预训练模型的持续学习工具箱，为在处理流式数据并适应新数据到来的现实场景中，利用预训练模型进行增量学习提供了一种有前景的方法。 |
| [^3] | [Weakly-Supervised Multi-Task Learning for Audio-Visual Speaker Verification.](http://arxiv.org/abs/2309.07115) | 本文介绍了一种弱监督多任务学习的方法，用于音视频说话人验证。我们通过引入辅助任务和非同步采样策略来提高距离度量学习方法的性能。实验结果表明，我们的网络在说话人验证中取得了最先进的性能。 |
| [^4] | [Contrastive Deep Encoding Enables Uncertainty-aware Machine-learning-assisted Histopathology.](http://arxiv.org/abs/2309.07113) | 对比性深度编码能够利用公共领域的大型数据集进行预训练，并以少量随机选择注释进行微调，实现基于补丁的分类的最新水平。此外，引入不确定性感知的损失函数用于量化模型置信度，帮助专家进行选择。 |
| [^5] | [Data Augmentation via Subgroup Mixup for Improving Fairness.](http://arxiv.org/abs/2309.07110) | 本论文提出了一种通过子组混合的数据增强方法，以提高机器学习系统的群体公平性。通过添加代表低比例群体的新样本，我们可以实现数据的平衡，并且利用该方法提高公平性和准确性。 |
| [^6] | [Characterizing Speed Performance of Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2309.07108) | 本文对多智能体强化学习（MARL）的速度性能进行了特征化研究。作者通过引入分类法，提出了三种最先进的MARL算法作为目标基准，以解决目前优化奖励的算法在训练时间速度性能方面的不足。 |
| [^7] | [Mitigating Group Bias in Federated Learning for Heterogeneous Devices.](http://arxiv.org/abs/2309.07085) | 本文提出了一种在分布式边缘应用中减轻联邦学习中群体偏见的方法，该方法可以通过计算跨域群体重要性来减轻全局模型的偏见，并保持隐私和资源利用效率。 |
| [^8] | [Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain.](http://arxiv.org/abs/2309.07080) | 本文介绍了贝叶斯动态有向无环图学习方法（BDyMA）来解决在发现大脑动态效应连接组中的两个主要挑战。该方法通过无约束框架实现更准确的结果和更稀疏的网络结构，使其特别适用于提取动态效应连接组。 |
| [^9] | [The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning.](http://arxiv.org/abs/2309.07072) | 本文评估了在分类任务中确定神经网络稳定性和准确性的理论限制，发现在一定的神经架构类别中，计算和验证理想的稳定和准确的神经网络是极具挑战性的甚至可能是不可能的。 |
| [^10] | [Large Language Models for Compiler Optimization.](http://arxiv.org/abs/2309.07062) | 本论文研究了将大型语言模型应用于代码优化的新颖方法，以7B参数的transformer模型为例，通过预测指令计数和生成优化代码等辅助学习任务，显著提高了模型的优化性能。在大量测试程序上的评估中，该方法相对编译器的优化效果提高了3.0%，并展现出令人惊喜的强大代码推理能力。 |
| [^11] | [Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments.](http://arxiv.org/abs/2309.07056) | 本文使用了一种名为“梦境”的可解释人工智能技术，探索神经网络对量子光学实验的学习，发现网络可以改变量子系统的属性分布，并揭示了神经网络的学习策略。 |
| [^12] | [An Extreme Learning Machine-Based Method for Computational PDEs in Higher Dimensions.](http://arxiv.org/abs/2309.07049) | 本文提出了两种基于随机神经网络的高维偏微分方程计算方法，利用极限学习机扩展了低维到高维的逼近，通过随机插值点来满足PDE和边界条件，并通过最小二乘解得到网络参数的训练值。 |
| [^13] | [Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks.](http://arxiv.org/abs/2309.07030) | 本文提出了两种基于最优输运的距离度量，用于比较有向图，并通过仿真图数据和单细胞RNA-seq数据推断的实际细胞间通讯图对其相对表现进行了评估。 |
| [^14] | [Unsupervised Contrast-Consistent Ranking with Language Models.](http://arxiv.org/abs/2309.06991) | 无监督的对比一致排序与语言模型，通过训练一个受逻辑约束引导的探测模型，实现在多个语句中始终映射到对比的真-假极点的排序任务。 |
| [^15] | [MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems.](http://arxiv.org/abs/2309.06981) | 这项工作提出了一种名为MASTERKEY的后门攻击，针对说话人验证系统。与以往攻击不同，在实际环境下攻击者对目标用户没有任何了解。通过嵌入说话人特征和语义信息，以及集成信道失真，我们的攻击可以成功破坏多个流行的SV模型，达到100％的攻击成功率。 |
| [^16] | [Auto-Regressive Next-Token Predictors are Universal Learners.](http://arxiv.org/abs/2309.06979) | 自回归的下一个标记预测器可以有效地近似图灵机计算的任何函数，并且在文本生成和算术任务上展现出非平凡的性能。 |
| [^17] | [DNNShifter: An Efficient DNN Pruning System for Edge Computing.](http://arxiv.org/abs/2309.06973) | DNNShifter是一种高效的边缘计算DNN剪枝系统，通过快速推导出合适的模型变体来提供高推理准确性，适应系统和网络条件变化的工作负载需求。 |
| [^18] | [Setting the Right Expectations: Algorithmic Recourse Over Time.](http://arxiv.org/abs/2309.06969) | 这项研究关注算法补救措施中忽视的关键要素 - 不断变化的环境对补救效果的影响。研究发现，在时间推移和个体间竞争的情况下，初始的补救建议可能变得不可靠，因此需要考虑时间变化来确保补救的有效性。 |
| [^19] | [Implicit Neural Multiple Description for DNA-based data storage.](http://arxiv.org/abs/2309.06956) | 本论文提出了一种基于DNA的数据存储方法，采用隐式神经多描述技术编码数据。通过创新的压缩方案和神经网络，有效地解决了DNA存储中的错误问题，并在性能上超过了传统的MDC方法。 |
| [^20] | [Effect of hyperparameters on variable selection in random forests.](http://arxiv.org/abs/2309.06943) | 这项研究评估了随机森林中超参数对变量选择的影响，在高维组学研究中，适当设置RF超参数对选择重要变量具有重要意义。 |
| [^21] | [Collectionless Artificial Intelligence.](http://arxiv.org/abs/2309.06938) | 本文提出了无集合原则的学习协议的思路，其中机器在环境交互背景中掌握认知技能，避免了数据集集中化的风险。 |
| [^22] | [Modeling Dislocation Dynamics Data Using Semantic Web Technologies.](http://arxiv.org/abs/2309.06930) | 本文介绍了如何使用语义网技术对位错动力学模拟数据进行建模，并通过添加缺失的概念和与相关本体对齐来扩展位错本体。 |
| [^23] | [Investigating the Impact of Action Representations in Policy Gradient Algorithms.](http://arxiv.org/abs/2309.06921) | 本论文讨论了强化学习中行动表示的影响，并通过实验发现行动表示对流行的强化学习基准任务的学习性能有着显著影响，其中性能差异可归因于优化问题空间的复杂性的变化。 |
| [^24] | [Continual Learning with Dirichlet Generative-based Rehearsal.](http://arxiv.org/abs/2309.06917) | 该论文提出了一种新颖的基于狄利克雷生成的回顾策略，用于解决连续学习中伪样本生成的挑战。 |
| [^25] | [Towards the TopMost: A Topic Modeling System Toolkit.](http://arxiv.org/abs/2309.06908) | 本文提出了一个名为TopMost的主题建模系统工具包，通过涵盖更广泛的主题建模场景和具有高度凝聚力和解耦模块化设计的特点，可以促进主题模型的研究和应用。 |
| [^26] | [Domain-Aware Augmentations for Unsupervised Online General Continual Learning.](http://arxiv.org/abs/2309.06896) | 本文提出了一种新颖的方法，通过定义和使用流依赖的数据扩增方式以及一些实现技巧，增强了UOGCL中的对比学习，达到了与其他无监督方法相比的最新成果，并减小了监督和无监督连续学习之间的差距。 |
| [^27] | [MagiCapture: High-Resolution Multi-Concept Portrait Customization.](http://arxiv.org/abs/2309.06895) | MagiCapture是一种高分辨率多概念人像定制方法，通过几个主题和风格参考，能够生成高质量的特定风格人像图像。 |
| [^28] | [Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?.](http://arxiv.org/abs/2309.06891) | 该论文提出了一种通用的池化框架SimPool，用于替代卷积和Transformer编码器的默认池化机制，无论是有监督还是自监督的方法都能提高性能，并提供能够描绘对象边界的注意力图。 |
| [^29] | [ProMap: Datasets for Product Mapping in E-commerce.](http://arxiv.org/abs/2309.06882) | 该论文介绍了两个新的产品映射数据集：ProMapCz和ProMapEn，分别包含捷克和英文产品对，这些数据集具有较为完整的产品信息，并解决了目前现有数据集无法区分非常相似但不匹配产品对的问题。 |
| [^30] | [Dynamic control of self-assembly of quasicrystalline structures through reinforcement learning.](http://arxiv.org/abs/2309.06869) | 本研究提出使用强化学习来控制具有多边形单元颗粒的动态自组装过程，形成十二边准晶体。我们通过估计最佳的温度控制策略，成功地生成了几乎没有缺陷的结构。强化学习获得的温度调度比传统的预设温度调度更有效地重现了期望的结构。 |
| [^31] | [Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy.](http://arxiv.org/abs/2309.06838) | 本研究提出了监督机器学习和基于物理的机器学习相结合的方法，用于预测搅拌摩擦增材制造中的峰值温度分布。实验结果表明，集成的机器学习方法在预测中表现出了较好的性能，最佳的SML方法为梯度提升法，最低的均方误差为165.78。 |
| [^32] | [UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training.](http://arxiv.org/abs/2309.06828) | UniBrain是一种用于通用脑部MRI诊断的分层知识增强预训练框架，利用大规模数据集和分层对齐机制提高了特征学习的效率。 |
| [^33] | [Comparative Analysis of Contextual Relation Extraction based on Deep Learning Models.](http://arxiv.org/abs/2309.06814) | 本文比较分析了基于深度学习模型的上下文关系提取方法。现有技术无法高效预测由多于两个关系和未指定实体组成的句子中的复杂关系。研究采用深度学习技术从多个句子的语境中识别语义关系。现有机器学习模型在二元关系中表现较好，但随着关系数量的增加，预测准确率降低。 |
| [^34] | [FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization.](http://arxiv.org/abs/2309.06805) | FedDIP是一个结合了动态模型修剪和增量正则化的联邦学习框架，通过消除冗余信息交换和实现极端稀疏模型来显著提高性能。 |
| [^35] | [Uncertainty-aware Traffic Prediction under Missing Data.](http://arxiv.org/abs/2309.06800) | 本研究提出了一种考虑不确定性的交通预测方法，可以处理缺失数据和测量不确定性，并适用于风险敏感任务和决策导向问题。 |
| [^36] | [Cognitive Mirage: A Review of Hallucinations in Large Language Models.](http://arxiv.org/abs/2309.06794) | 这篇论文综述了大规模语言模型中幻觉的现象，并提出了幻觉的分类、理论分析、检测方法和改进方法，同时还设想了未来的研究方向。 |
| [^37] | [Electricity Demand Forecasting through Natural Language Processing with Long Short-Term Memory Networks.](http://arxiv.org/abs/2309.06793) | 本文提出了一种将文本新闻特征纳入长短期记忆（LSTM）网络的方法，成功预测了英国国家电力需求的确定性和概率任务，并发现公众情绪和与交通和地缘政治相关的词向量表示对电力需求具有时间连续性的影响。该模型相对于纯LSTM基准模型提高了超过3%，相对于官方基准模型提高了接近10%，并通过缩小置信区间有效地减少了预测不确定性。 |
| [^38] | [Scalable neural network models and terascale datasets for particle-flow reconstruction.](http://arxiv.org/abs/2309.06782) | 本研究针对高能电子-正电子碰撞中的粒子流重建，使用可扩展的机器学习模型，并通过超参数调优和硬件处理器的高度可移植性，取得了真实且具有竞争力的物理性能。 |
| [^39] | [Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss.](http://arxiv.org/abs/2309.06774) | 本文揭示了基于Hinge Loss训练的深度学习二分类器的基本测试性能限制。 |
| [^40] | [MTD: Multi-Timestep Detector for Delayed Streaming Perception.](http://arxiv.org/abs/2309.06742) | 提出了多时间步检测器（MTD）用于解决自动驾驶系统延迟问题，包括动态路由和延迟分析模块（DAM），以及时间步骤分支模块（TBM）来适应性地预测未来时间步。 |
| [^41] | [MCNS: Mining Causal Natural Structures Inside Time Series via A Novel Internal Causality Scheme.](http://arxiv.org/abs/2309.06739) | 本文提出了一种名为MCNS的自动领域无关框架，通过内部因果关系方案帮助挖掘时间序列中的因果自然结构。 |
| [^42] | [Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense.](http://arxiv.org/abs/2309.06724) | 该论文提出了一种使用深度非参数凸化滤波（DNCF）的通用框架，用于计算摄影领域中的图像恢复。DNCF具有强大的泛化性和对抗性图像处理的鲁棒性，同时能够实现实时的对抗性图像分类网络防御。 |
| [^43] | [Bias Amplification Enhances Minority Group Performance.](http://arxiv.org/abs/2309.06717) | 本论文提出了一种名为BAM的两阶段训练算法，通过引入可学习的辅助变量来放大偏见，提高了少数群体的表现。 |
| [^44] | [Crystal structure prediction using neural network potential and age-fitness Pareto genetic algorithm.](http://arxiv.org/abs/2309.06710) | 这篇论文介绍了一种使用神经网络势和年龄适应Pareto遗传算法进行晶体结构预测的新算法，该算法在能够找到能量最优的晶体结构方面表现出比其他算法更好的预测能力和搜索效果。 |
| [^45] | [Predicting Fatigue Crack Growth via Path Slicing and Re-Weighting.](http://arxiv.org/abs/2309.06708) | 通过路径切片和重新加权的统计学习框架，可以预测带有不确定性的加载条件下的疲劳裂纹增长和组件的失效寿命。通过构建数字库并引入路径切片和重新加权技术，该方法可以有效处理疲劳复杂性和统计噪声。 |
| [^46] | [VLSlice: Interactive Vision-and-Language Slice Discovery.](http://arxiv.org/abs/2309.06703) | 这项工作提出了一种交互式系统VLSlice，可以通过用户引导发现一致的视觉和语言行为的表示级子组，以解决自动发现子组时的困难。 |
| [^47] | [Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization.](http://arxiv.org/abs/2309.06692) | 本研究通过梯度协调方法解决了异构联邦学习中的非独立同分布问题，提出了FedGH，通过减轻本地漂移来增强性能。实验证明，在多个基准和非独立同分布场景下，FedGH始终能够显著提升联邦学习的性能。 |
| [^48] | [Attention Loss Adjusted Prioritized Experience Replay.](http://arxiv.org/abs/2309.06684) | 本文提出了一种改进的Attention Loss Adjusted Prioritized Experience Replay (ALAP)算法，通过结合改进的自注意力网络和双采样机制，调节重要性采样权重，消除了先进的经验回放算法中的估计误差。在OPENAI gym环境中的测试和对比研究验证了该算法的优势和效率。 |
| [^49] | [Federated PAC-Bayesian Learning on Non-IID data.](http://arxiv.org/abs/2309.06683) | 本文提出了一种非独立同分布的联邦PAC-Bayesian界限，通过考虑每个客户端的唯一先验知识和可变聚合权重，以及引入目标函数和创新的基于Gibbs的算法，解决了联邦学习中的非独立同分布挑战。 |
| [^50] | [Generalizable improvement of the Spalart-Allmaras model through assimilation of experimental data.](http://arxiv.org/abs/2309.06679) | 本研究通过实验数据同化改进了Spalart-Allmaras模型，实现了对分离流体的雷诺平均纳维-斯托克斯解的泛化，提高了计算模型的性能。 |
| [^51] | [Sound field decomposition based on two-stage neural networks.](http://arxiv.org/abs/2309.06661) | 该论文提出了一种基于两阶段神经网络的声场分解方法，通过第一阶段将合成的麦克风处的声压分离成每个声源激发的声压，然后通过第二阶段从单一声源处的声压回归得到源位置，实现了更高的源定位精度和声场重构精度。 |
| [^52] | [Generalizable Neural Fields as Partially Observed Neural Processes.](http://arxiv.org/abs/2309.06660) | 本研究提出了一种新的范式，将大规模的神经表示训练视为部分观测的神经过程框架的一部分，并利用神经过程算法进行优化。 |
| [^53] | [Dissipative Imitation Learning for Discrete Dynamic Output Feedback Control with Sparse Data Sets.](http://arxiv.org/abs/2309.06658) | 这项研究探索了一种用于稀疏数据集和对植物模型了解有限的情况下实现稳定性的耗散型仿真学习方法，并成功应用于两个未知的植物。 |
| [^54] | [Out of Distribution Detection via Domain-Informed Gaussian Process State Space Models.](http://arxiv.org/abs/2309.06655) | 本文提出了一种通过嵌入领域知识的高斯过程状态空间模型，以实现未知领域的检测，并基于递归预测构建了在线的监视器。数值结果表明，带有领域信息的内核可以提供更好的回归质量。 |
| [^55] | [ConR: Contrastive Regularizer for Deep Imbalanced Regression.](http://arxiv.org/abs/2309.06651) | ConR是一种对比正则化器，通过建模全局和局部标签相似性，防止少数样本的特征被折叠到其多数邻居中，有效地处理深度不平衡回归问题。 |
| [^56] | [Bregman Graph Neural Network.](http://arxiv.org/abs/2309.06645) | Bregman GNNs提出了一种受Bregman距离概念启发的双层优化框架，能够有效缓解过度平滑问题并在同构图和异构图中性能优于原始的GNNs。 |
| [^57] | [Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models.](http://arxiv.org/abs/2309.06642) | 本文提出了一种自适应扩散方法，通过潜在扩散模型实现样本自适应重建。该方法解决了现有求解器在适应重建任务困难程度、推理时间和资源分配方面的不足。 |
| [^58] | [Quantum Data Center: Perspectives.](http://arxiv.org/abs/2309.06641) | 本文提出了量子数据中心(QDC)的概念，它是现有经典数据中心的量子版本，通过结合量子随机访问存储器(QRAM)和量子网络，QDC可以提供客户在效率、安全性和精度方面的显著优势，对于量子计算、通信和传感领域具有重要意义。该研究探讨了硬件实现和特定应用方面的潜在科学和商业机会，并展示了QDC在机器学习和大数据行业等领域的潜在影响。 |
| [^59] | [$G$-Mapper: Learning a Cover in the Mapper Construction.](http://arxiv.org/abs/2309.06634) | 本论文介绍了一种基于统计检验和聚类算法的优化Mapper图覆盖的方法，通过分割覆盖选择生成了保留数据集本质的Mapper图。 |
| [^60] | [Epistemic Modeling Uncertainty of Rapid Neural Network Ensembles for Adaptive Learning.](http://arxiv.org/abs/2309.06628) | 本研究提出了一种利用快速神经网络进行认知建模的方法，用于评估航空航天工程系统设计探索中的不确定性。该方法克服了训练集成模型的成本和计算挑战，为自适应学习提供了重要信息。 |
| [^61] | [A Sequentially Fair Mechanism for Multiple Sensitive Attributes.](http://arxiv.org/abs/2309.06627) | 本论文提出了一个顺序框架来逐步实现对多个敏感特征的公平性，通过利用多边际Wasserstein重心扩展了标准的强人口平等概念，并提供了闭式解来解释敏感特征之间的相关性。 |
| [^62] | [Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity.](http://arxiv.org/abs/2309.06626) | 通过小型运行时修改引入半结构激活稀疏性，我们设计了一种稀疏训练过程，在保持精度下降最小的情况下，实现了深度神经网络的高效处理和推断加速。 |
| [^63] | [On the Contraction Coefficient of the Schr\"odinger Bridge for Stochastic Linear Systems.](http://arxiv.org/abs/2309.06622) | 本研究对与Schr\"{o}dinger系统的收敛性相关的收缩系数进行了先验估计，并提供了新的几何和控制理论解释。我们指出通过预条件化终点支持集可以改善线性SBPs的最坏情况收缩系数的计算。 |
| [^64] | [RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models.](http://arxiv.org/abs/2309.06619) | 这篇论文介绍了RT-LM，它是一种针对语言模型的实时推理进行优化的不确定性感知资源管理方法。该方法能够理解、量化和优化由于语言的不确定性而导致的推理延迟性能变化，以提高LMs的整体性能。 |
| [^65] | [Unsupervised Learning of Nanoindentation Data to Infer Microstructural Details of Complex Materials.](http://arxiv.org/abs/2309.06613) | 本研究使用无监督学习分析了Cu-Cr复合材料的纳米压痕数据，推断了材料的微观结构细节和力学性质，并引入了交叉验证方法解决了数据数量和可靠预测的问题。 |
| [^66] | [Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices.](http://arxiv.org/abs/2309.06612) | 本文提出了基于硬件感知的资源受限设备上的多模态神经架构搜索框架Harmonic-NAS，通过两层优化实现了单模态骨干和多模态融合网络的联合优化。 |
| [^67] | [Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach.](http://arxiv.org/abs/2309.06604) | 本文提出了一种基于代理的层级机器学习平台，用于选择分布式组织的机器学习算法并同时调整其超参数。该方法具有可伸缩性、灵活性和鲁棒性，并支持自动化和协同的功能。 |
| [^68] | [Reasoning with Latent Diffusion in Offline Reinforcement Learning.](http://arxiv.org/abs/2309.06599) | 本研究提出了一种新颖的离线强化学习方法，利用潜在扩散建模轨迹序列，并通过批处理约束避免外推误差。该方法能够处理多模态数据并编码更丰富的任务特定特征。 |
| [^69] | [Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning.](http://arxiv.org/abs/2309.06597) | Rank2Tell是一个多模态驾驶数据集，用于联合重要性排序和推理，为研究人员提供了复杂交通情景中各种重要对象的密集注释和独特属性。 |
| [^70] | [Convergence of Gradient-based MAML in LQR.](http://arxiv.org/abs/2309.06588) | 本研究调查了在LQR中应用MAML时的局部收敛特性，并提供了保持动态系统稳定性的局部收敛保证。论文通过简单的数值结果展示了MAML在LQR任务中的收敛性质。 |
| [^71] | [Explainable Graph Neural Network for Alzheimer's Disease And Related Dementias Risk Prediction.](http://arxiv.org/abs/2309.06584) | 这项研究提出了一种可解释的图神经网络方法来预测阿尔茨海默病和相关痴呆症的风险。通过将机器学习与索赔数据相结合，不仅能发现额外的风险因素，还能揭示不同医学代码之间的关联。通过评估关系重要性和其对风险预测的影响，该方法能提供全面的解释。 |
| [^72] | [Electron Energy Regression in the CMS High-Granularity Calorimeter Prototype.](http://arxiv.org/abs/2309.06582) | 这项研究介绍了CMS高精度量热器原型中的电子能量回归，利用机器学习方法从三维点的能量重建入射电子的能量，并希望通过公开数据促进机器学习应用专家对电子图像重建的研究。 |
| [^73] | [Efficient Finite Initialization for Tensorized Neural Networks.](http://arxiv.org/abs/2309.06577) | 这种方法提出了一种高效有限初始化张量化神经网络层的方法，避免了参数爆炸问题，并通过使用弗罗贝尼乌斯范数的迭代部分形式来计算范数，使其具有有限范围。应用于不同层的实验表明其性能良好。 |
| [^74] | [Circle Feature Graphormer: Can Circle Features Stimulate Graph Transformer?.](http://arxiv.org/abs/2309.06574) | 本文介绍了一种新的圆形特征图转换器（CFG），用于丢失链路预测任务，并实现了改进的图自注意机制。实验结果表明，CFG在ogbl-citation2数据集上取得了最先进的性能。 |
| [^75] | [Promises of Deep Kernel Learning for Control Synthesis.](http://arxiv.org/abs/2309.06569) | 深度核学习（DKL）结合了神经网络的表示能力与高斯过程的不确定性量化，为复杂动态系统的学习和控制合成提供了潜力。本研究提出了一个基于抽象的框架，使用DKL与区间马尔可夫决策过程（IMDP）实现对随机动态系统的控制合成，同时采用了高效的深度架构和正确性保证的方法。 |
| [^76] | [Unsupervised Bias Detection in College Student Newspapers.](http://arxiv.org/abs/2309.06557) | 本文提出了一个几乎没有人为影响的流程，用于从大学报纸档案中获取并检测偏见。该方法通过比较大型语言模型摘要的情感与原文来计算偏见，不需要大量标记数据，为客观理解学生报纸来源中的偏见提供了方法。 |
| [^77] | [Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning.](http://arxiv.org/abs/2309.06553) | 这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。 |
| [^78] | [Commands as AI Conversations.](http://arxiv.org/abs/2309.06551) | "ai-cli"是一个开源系统，通过将自然语言提示转化为可执行的Linux命令行工具命令，利用OpenAI的API实现命令行界面的智能化和用户友好化。 |
| [^79] | [Online Infinite-Dimensional Regression: Learning Linear Operators.](http://arxiv.org/abs/2309.06548) | 在这篇论文中，我们研究了在线设置下学习无限维线性算子的问题。我们证明了在一定的条件下，线性算子是可以在线学习的，而在另一些条件下则不可以。我们还证明了在线均一收敛和学习能力之间的分离，并在PAC设置下得到了相同的结果。 |
| [^80] | [Desenvolvimento de modelo para predi\c{c}\~ao de cota\c{c}\~oes de a\c{c}\~ao baseada em an\'alise de sentimentos de tweets.](http://arxiv.org/abs/2309.06538) | 本研究开发了一个基于推特情感分析的股票报价预测模型，通过从Twitter上的帖子中提取情感特征，使用这些特征训练模型，并在实际交易中取得了88.82雷亚尔的净收益。 |
| [^81] | [Distributionally Robust Transfer Learning.](http://arxiv.org/abs/2309.06534) | 这篇论文介绍了一种分布鲁棒的迁移学习方法，通过优化一个不确定性集合内最具对抗性的损失来实现，该集合是由源分布的凸组合生成的目标人口集合，能够有效地将迁移学习和分布鲁棒的预测模型联系起来。 |
| [^82] | [Hierarchical Multi-Task Learning Framework for Session-based Recommendations.](http://arxiv.org/abs/2309.06533) | 本文提出了一种面向会话推荐的层次化多任务学习框架HierSRec，通过在预测任务之间设置层次结构，并利用辅助任务的输出来提供更丰富的输入特征和更高的预测可解释性，进一步增强了预测准确性和可泛化性。 |
| [^83] | [Machine Translation Models Stand Strong in the Face of Adversarial Attacks.](http://arxiv.org/abs/2309.06527) | 本研究探讨了对抗攻击对机器翻译模型的影响，证明了机器翻译模型在面对已知的最佳对抗攻击时表现出强大的稳定性。同时，我们提出的攻击算法在相对性能上超过其他替代选择。 |
| [^84] | [Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers.](http://arxiv.org/abs/2309.06526) | 本文探索了对表格转换器进行差分隐私预训练和参数高效微调的好处，并展示了这些方法在准确性和可训练参数数量方面优于传统方法，从而在参数效率、隐私和准确度之间实现了改进的权衡。 |
| [^85] | [A Q-learning Approach for Adherence-Aware Recommendations.](http://arxiv.org/abs/2309.06519) | 本文提出了一种针对依从性推荐的Q学习算法，以解决在高风险场景中人类决策者接受人工智能推荐的问题。算法通过学习“依从水平”来捕捉决策者遵循推荐的频率，并实时生成最佳推荐策略。研究证明该算法可以收敛到最优值，在不同场景下表现良好。 |
| [^86] | [Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets.](http://arxiv.org/abs/2309.06503) | 本研究评估了使用大型语言模型和弱监督方法，通过GPT-4模型在单次模式下提供标签的方式，来识别COVID-19疫苗相关推文，并与人工注释员的性能进行比较。 |
| [^87] | [A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale.](http://arxiv.org/abs/2309.06497) | 这是一个针对大规模训练神经网络的PyTorch分布式数据并行实现的分布式Shampoo优化算法。这种算法利用了块对角先验矩阵和搜索方向的AllGather原语操作，在性能上实现了显著的优化，相比于传统的对角缩放的自适应梯度方法仅有最多10%的性能损失。 |
| [^88] | [Learning topological operations on meshes with application to block decomposition of polygons.](http://arxiv.org/abs/2309.06484) | 本文介绍了一种学习基于框架，用于改善非结构化网格的质量，通过自主对弈强化学习来学习减小节点度数与理想值之间的差异，从而达到减少不规则节点的目标。 |
| [^89] | [Flows for Flows: Morphing one Dataset into another with Maximum Likelihood Estimation.](http://arxiv.org/abs/2309.06472) | 该论文提出了一种名为"flows for flows"的协议，用于通过最大似然估计训练规范化流将一个数据集变形成另一个数据集，即使两个数据集的概率密度都不明确。 |
| [^90] | [Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model.](http://arxiv.org/abs/2309.06453) | 本文通过实验比较了监督和无监督句子表示学习在训练过程中的行为，并探讨了如何缩小性能差距。 |
| [^91] | [Quantized Non-Volatile Nanomagnetic Synapse based Autoencoder for Efficient Unsupervised Network Anomaly Detection.](http://arxiv.org/abs/2309.06449) | 本论文提出了一种使用量子化的非挥发性纳米磁阻自编码器进行高效无监督网络异常检测的方法。通过设计低分辨率的自编码器和有效的量化神经网络学习算法，解决了边缘设备资源限制的挑战。使用特殊的磁性域壁作为突触，通过自旋轨道力电流脉冲操纵突触权重，并在NSL-KDD数据集上进行了评估，得到了与浮点精度权重自编码器相当的异常检测性能。 |
| [^92] | [Selection of contributing factors for predicting landslide susceptibility using machine learning and deep learning models.](http://arxiv.org/abs/2309.06062) | 使用机器学习和深度学习模型对滑坡易发性进行预测，通过选择更重要的贡献因素提高预测准确性。 |
| [^93] | [Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models.](http://arxiv.org/abs/2309.05605) | 本文提出了一种通过向Transformer-Based语言模型的LLM注意力头部定向注入内存来纠正多跳推理错误的方法，从而提高了模型在处理多跳推理问题时的表现。 |
| [^94] | [Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations.](http://arxiv.org/abs/2309.05575) | 该论文研究了一个大型的有限差分离散化家族，在非均匀扩散过程中通过将二维扩散分解为四个一维扩散得出了一个3 x 3的模板。该模板类包含一个自由参数，涵盖了广泛的现有离散化。同时，论文还建立了与模板相对应的矩阵的谱范数上界，并将显式方案转化为ResNet块。 |
| [^95] | [NExT-GPT: Any-to-Any Multimodal LLM.](http://arxiv.org/abs/2309.05519) | NExT-GPT是一个任何到任何的多模态语言模型系统，通过连接多模态适配器和不同扩散解码器，能够接受和生成任意组合的文本、图像、视频和音频内容。 |
| [^96] | [Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving.](http://arxiv.org/abs/2309.05282) | 本研究提出了将预训练语言编码器整合到自动驾驶的轨迹预测模型中的新方法。通过基于文本的场景表示和经典的栅格化图像表示相结合，得到了描述性的场景嵌入，并在实验中验证了显著的性能改进。 |
| [^97] | [Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood.](http://arxiv.org/abs/2309.05153) | 本文通过协同扩散恢复似然（CDRL）提出了一种方法，用于学习和采样一系列基于能量的模型（EBMs），通过在不断嘈杂化的数据集版本上定义不同噪声水平的EBMs，并与初始化模型配对协同训练。这种方法旨在关闭EBMs和其他生成框架之间的样本质量差距。 |
| [^98] | [Generalization error bounds for iterative learning algorithms with bounded updates.](http://arxiv.org/abs/2309.05077) | 本文研究了具有有界更新的迭代学习算法在非凸损失函数上的泛化特性，提出了一种新颖的泛化误差界限，利用了信息论技术。研究表明，在模型维度和训练数据样本数量相等的情况下，界限得到了改善。 |
| [^99] | [Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation.](http://arxiv.org/abs/2309.04434) | 本研究提出了一种利用物理信息神经网络（PINNs）解决量子电路反对角（CD）协议优化问题的方法，通过嵌入物理信息到神经网络中，并利用最小作用量原理和厄米特性条件来获取最适当的反对角项，从而提供了一种可靠的替代方案，摆脱了以往依赖于经典数值逼近的约束。 |
| [^100] | [Online Submodular Maximization via Online Convex Optimization.](http://arxiv.org/abs/2309.04339) | 本论文研究了在线设置下的一般性子模最大化问题，并将一类大型子模函数归约到在线凸优化问题中。这种归约方式可在组合优化中实现次线性遗憾，并且适用于许多不同版本的在线学习问题。 |
| [^101] | [Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning.](http://arxiv.org/abs/2309.03581) | 本文提出了一个以人为中心的交互式超参数优化方法，通过应用偏好学习来解决多目标机器学习中的问题。 |
| [^102] | [Unifying over-smoothing and over-squashing in graph neural networks: A physics informed approach and beyond.](http://arxiv.org/abs/2309.02769) | 本研究提出了一种基于物理信息的方法，通过反转图热方程的时间方向，提高了图神经网络的节点特征清晰度，并引入了多尺度热核滤波函数，增强了GNNs的表达能力。进一步推广为G-MHKG模型，探索了更灵活的滤波条件，以解决过度平滑和过度压缩等计算挑战。这些方法和模型在增强GNNs性能方面具有潜力。 |
| [^103] | [Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test.](http://arxiv.org/abs/2309.02422) | 本文将最大均差相似度应用于神经网络，并提出了一种称为Radon-Kolmogorov-Smirnov（RKS）检验的方法，该方法将样本均值差异最大化的问题推广到多维空间和更高平滑度顺序，同时与神经网络密切相关。 |
| [^104] | [MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval.](http://arxiv.org/abs/2309.01516) | 多途径适配器是一个创新的框架，利用"对齐增强器"加深模态对齐，实现高可转移性，可有效减少调整参数的时间并提高零样本图像-文本检索性能。 |
| [^105] | [Switch and Conquer: Efficient Algorithms By Switching Stochastic Gradient Oracles For Decentralized Saddle Point Problems.](http://arxiv.org/abs/2309.00997) | 该论文提出了一种切换随机梯度预言的高效算法，用于解决分散鞍点问题。通过在初始阶段使用广义随机梯度计算预言加速迭代进展，然后切换到随机方差减少梯度预言，该算法取得了较好的性能。 |
| [^106] | [Elucidating the Exposure Bias in Diffusion Models.](http://arxiv.org/abs/2308.15321) | 本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。 |
| [^107] | [Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning.](http://arxiv.org/abs/2308.06851) | 本文研究了如何用机器学习优化国家篮球协会的进攻战术规划。通过建立模型，采用特定的特征进行评估和分析，可以帮助决策者确定战术执行的具体细节。 |
| [^108] | [Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer.](http://arxiv.org/abs/2308.00507) | 本文提出了一种新的方法来改善胰腺癌的预后预测。该方法使用了可学习的神经距离来描述肿瘤与血管之间的关系，并通过融合局部和全局特征来改进多相CT影像中的肿瘤纹理特征提取。实验表明，该方法在预后预测中取得了较好的效果。 |
| [^109] | [An Empirical Evaluation of Temporal Graph Benchmark.](http://arxiv.org/abs/2307.12510) | 本文对Temporal Graph Benchmark进行了实证评估，通过扩展动态图库(DyGLib)到TGB，并使用十一种动态图学习方法进行全面比较。实验发现不同模型在不同数据集上表现出不同的性能，一些基线方法的性能可以通过使用DyGLib显著提高。 |
| [^110] | [Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information.](http://arxiv.org/abs/2307.02799) | 本文提出了一种使用张量回归进行少样本个性化显著性预测的方法，以保留个性化显著性图的结构全局信息。 |
| [^111] | [RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark.](http://arxiv.org/abs/2306.17100) | RL4CO是一个用于组合优化的广泛强化学习基准测试，着重于可扩展性和泛化能力的评估，并展示了一些最新方法在样本效率和适应不同数据分布方面的表现相对较差，强调了对神经CO求解器性能的平衡评估的重要性。 |
| [^112] | [Adversaries with Limited Information in the Friedkin--Johnsen Model.](http://arxiv.org/abs/2306.10313) | 这篇论文研究了在社交网络中，对手在只知道网络拓扑的情况下是否能够种下不和谐，并以此为基础提出了解决用户意见估计困难问题的方法。 |
| [^113] | [Optimal transport for automatic alignment of untargeted metabolomic data.](http://arxiv.org/abs/2306.03218) | 本文提出了一种名为GromovMatcher的算法，通过使用最优输运自动合并LC-MS数据集，可提高数据对齐的准确性和鲁棒性，有效解决代谢组学数据合并的挑战。 |
| [^114] | [Learning Horn Envelopes via Queries from Large Language Models.](http://arxiv.org/abs/2305.12143) | 该研究提出了一种从经过训练的神经网络中提取知识的方法，可以学习到最紧Horn逼近目标理论的新算法，并在预训练的语言模型中运用于揭示基于职业的性别偏见规则。 |
| [^115] | [Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species.](http://arxiv.org/abs/2305.06695) | 本文提出了一种利用对齐的视觉-遗传推理空间来提高少量图像数据珍稀物种分类的方法，该方法通过深度嵌入模型实现对齐，适用于提高稀有物种的长尾识别，并且可以显著有益于仅基于视觉的稀有物种识别。 |
| [^116] | [Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects.](http://arxiv.org/abs/2304.11075) | 本项目在瑞士德语方言ASR模型的研究中提供了有价值的思路，通过提出考虑语义距离的新颖损失函数，对OpenAI的Whisper模型进行微调，取得了优于当前先进成果的效果。 |
| [^117] | [Primal-Dual Contextual Bayesian Optimization for Control System Online Optimization with Time-Average Constraints.](http://arxiv.org/abs/2304.06104) | 提出了一种基于原始-对偶语境贝叶斯优化算法，可以实现对约束闭环控制系统的在线性能优化，同时满足所需的约束条件。 |
| [^118] | [Learning a Universal Human Prior for Dexterous Manipulation from Human Preference.](http://arxiv.org/abs/2304.04602) | 通过借鉴最新的强化学习从人类反馈中的方法，本论文提出了一种学习普适人类先验的框架，通过直接人类偏好反馈视频来调整机器人在各种任务中的策略。结果表明，该方法能够在仿真中表现出更加类人的行为，甚至在未见过的任务中也能取得良好效果。 |
| [^119] | [Your Diffusion Model is Secretly a Zero-Shot Classifier.](http://arxiv.org/abs/2303.16203) | 扩散模型的密度估计可以被用作零样本分类，作者的生成式分类方法在各种基准测试中取得强大的结果，并具有更强的多模式关系推理能力。 |
| [^120] | [Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces.](http://arxiv.org/abs/2303.00028) | 本文提出了一种用稀疏高斯过程解决连续空间下传感器放置问题的方法，避免离散化环境并降低了计算复杂度，可通过贪婪算法找到好的解决方案。 |
| [^121] | [Imprecise Bayesian Neural Networks.](http://arxiv.org/abs/2302.09656) | 在机器学习和人工智能领域，该论文提出了一种新的算法——不精确的贝叶斯神经网络(IBNNs)。这种算法使用可信区间先验分布集合和似然分布集合进行训练，相比标准的BNNs，可以区分先验和后验的不确定性并量化。此外，IBNNs在贝叶斯灵敏度分析方面具有更强的鲁棒性，并且对分布变化也更加鲁棒。 |
| [^122] | [On Penalty-based Bilevel Gradient Descent Method.](http://arxiv.org/abs/2302.05185) | 本文提出了基于惩罚的双层梯度下降算法，解决了下层非强凸约束双层问题，实验表明该算法有效。 |
| [^123] | [Distilling Cognitive Backdoor Patterns within an Image: A SOTA Method for Backdoor Sample Detection.](http://arxiv.org/abs/2301.10908) | 本文提出了一种用于提取和检测图像中后门模式的简单方法，称为认知精炼（CD）。通过优化输入掩码，我们可以提取一个小模式，该模式可以导致模型产生相同的输出。使用CD和提取的模式，我们发现了后门攻击的一个有趣现象：后门样本的模式都非常小。所以可以利用学到的掩码从污染的训练数据集中检测和删除后门样本。 |
| [^124] | [Solar Coronal Hole Analysis and Prediction using Computer Vision and LSTM Neural Network.](http://arxiv.org/abs/2301.06732) | 本研究使用计算机视觉和LSTM神经网络分析和预测太阳冕空洞的大小和趋势，并据此为地球对太阳冕空洞的影响做准备。 |
| [^125] | [Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels.](http://arxiv.org/abs/2301.00545) | 本文提出了一种理论上保证的在学习中选择干净样本的框架Knockoffs-SPR，通过Scalable Penalized Regression（SPR）方法模型化网络特征和标签之间的线性关系，并且通过Knockoff过滤器控制了干净数据的误选择率。 |
| [^126] | [ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning.](http://arxiv.org/abs/2212.01378) | ColD Fusion是一种协同下降的分布式多任务微调方法，通过利用分布式计算，可以不断改进预训练模型，并在各种数据集上表现良好，优于RoBERTa模型。 |
| [^127] | [Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges.](http://arxiv.org/abs/2211.08413) | 本文提出去中心化联邦学习（DFL）来解决传统中心化FL（CFL）模型中的问题，主要研究DFL与CFL的差异、DFL的基础理论、DFL框架的设计与评估以及DFL在实际应用中的场景。 |
| [^128] | [A Spectral Analysis of Graph Neural Networks on Dense and Sparse Graphs.](http://arxiv.org/abs/2211.03231) | 本研究通过分析稀疏性对图谱的影响，发现图神经网络在稀疏图上能够优于谱方法的性能。 |
| [^129] | [CTRL: Clustering Training Losses for Label Error Detection.](http://arxiv.org/abs/2208.08464) | CTRL是一种用于检测多类别数据集中标签错误的新框架，通过聚类训练损失将样本分为干净标记和有噪声标记两类，并通过去除有噪声标签的样本来提高模型的效果。实验结果表明，该方法在图像和文本数据集上具有最先进的错误检测准确性。 |
| [^130] | [Data centers with quantum random access memory and quantum networks.](http://arxiv.org/abs/2207.14336) | 本文提出了将量子随机访问存储器（QRAM）和量子网络结合起来的量子数据中心（QDC）架构，并讨论了其在量子计算、量子通信和量子感知方面的应用。这将为未来的数据中心提供高效、私密和快速的服务。 |
| [^131] | [Perseus: A Simple and Optimal High-Order Method for Variational Inequalities.](http://arxiv.org/abs/2205.03202) | Perseus 解决了一个关于光滑和单调变分不等式的简单且最优高阶方法的设计问题。在大规模应用中具有实际应用价值。 |
| [^132] | [Distributed Out-of-Memory NMF on CPU/GPU Architectures.](http://arxiv.org/abs/2202.09518) | 提出了一种分布式超内存非负矩阵分解(NMF)算法，可以在CPU/GPU架构上实现高效计算。算法通过稀疏和稠密矩阵操作以及批处理/平铺策略，有效地处理超内存问题，并利用CUDA流进行数据传输和异步计算。 |
| [^133] | [A Worker-Task Specialization Model for Crowdsourcing: Efficient Inference and Fundamental Limits.](http://arxiv.org/abs/2111.12550) | 本论文介绍了一种用于众包的工人-任务特化模型，该模型解决了在多个不准确答案中推断正确标签的问题，并考虑了任务和工人的特化类型以及其可靠性变化。用于估计纠正答案达到最优样本复杂度的能力。 |
| [^134] | [Sign and Relevance learning.](http://arxiv.org/abs/2110.07292) | 本文提出了一种新型网络，在整个网络中仅传播可塑性变化的符号，同时使用神经调制控制学习速率。研究结果表明该范例可以成功学习具有多层表示的复杂任务，学习速度和稳定性优于标准模型。 |
| [^135] | [RIFLE: Imputation and Robust Inference from Low Order Marginals.](http://arxiv.org/abs/2109.00644) | RIFLE是一个统计推断框架，可以在缺失数据的情况下进行回归和分类。它通过估计底层数据分布的低阶矩和置信区间来学习一个分布鲁棒的模型，而无需进行数据补全。 |
| [^136] | [Fixed points of nonnegative neural networks.](http://arxiv.org/abs/2106.16239) | 本文利用不动点理论分析非负神经网络，证明了具有非负权重和偏置的非负神经网络存在输入和输出维度相同的不动点，并证明了其不动点集形状为区间。这些结果有助于对非负神经网络的理解。 |
| [^137] | [Neural Vortex Method: from Finite Lagrangian Particles to Infinite Dimensional Eulerian Dynamics.](http://arxiv.org/abs/2006.04178) | 神经涡旋方法（NVM）是一种新颖的学习-based 框架，用于重构高分辨率的欧拉流场，它通过构建神经网络描述拉格朗日涡旋结构和其相互作用动力学，解决了将连续流场映射到离散涡旋粒子的挑战。 |
| [^138] | [Nearest Neighbor Sampling of Point Sets using Rays.](http://arxiv.org/abs/1911.10737) | 本论文提出了一种新的框架用于最近邻的点集抽样，所涉及的RaySense草图可以捕捉点的基本几何形态以及提取与之相关的统计信息，且可高效地进行点集上的线积分计算。 |

# 详细

[^1]: 超越文本视野：多模态训练提升了在真实性和伦理道德方面的MLLM

    Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics. (arXiv:2309.07120v1 [cs.CL])

    [http://arxiv.org/abs/2309.07120](http://arxiv.org/abs/2309.07120)

    多模态训练的MLLM在纯NLP任务中表现出卓越的真实性和伦理对齐能力，这得益于视觉指导调优和优秀的指导质量。

    

    多模态大语言模型（MLLM）基于大型语言模型（LLM）进行训练，具备理解多模态输入和生成文本响应的增强能力。虽然它们在多模态任务中表现出色，但对MLLM的纯NLP能力常常低估并未经测试。本研究中，我们采用了新颖的方法，揭示了MLLM的一个引人注目的特性——初步结果表明，视觉指导调优，一种将LLM转换为MLLM的流行策略，出乎意料地帮助模型在纯NLP环境中取得了提高真实性和伦理对齐的效果。例如，经过视觉指导调优的LLaMA2 7B模型在TruthfulQA-mc和伦理道德基准上超过了经过超过一百万人工标注的LLaMA2-chat 7B模型的性能。进一步的分析表明，这种改进的对齐可以归因于视觉-文本数据固有的优秀指导质量。

    Multi-modal large language models (MLLMs) are trained based on large language models (LLM), with an enhanced capability to comprehend multi-modal inputs and generate textual responses. While they excel in multi-modal tasks, the pure NLP abilities of MLLMs are often underestimated and left untested. In this study, we get out of the box and unveil an intriguing characteristic of MLLMs -- our preliminary results suggest that visual instruction tuning, a prevailing strategy for transitioning LLMs into MLLMs, unexpectedly and interestingly helps models attain both improved truthfulness and ethical alignment in the pure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model surpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one million human annotations, on TruthfulQA-mc and Ethics benchmarks. Further analysis reveals that the improved alignment can be attributed to the superior instruction quality inherent to visual-text data. In releasing our code at 
    
[^2]: PILOT：一个基于预训练模型的持续学习工具箱

    PILOT: A Pre-Trained Model-Based Continual Learning Toolbox. (arXiv:2309.07117v1 [cs.LG])

    [http://arxiv.org/abs/2309.07117](http://arxiv.org/abs/2309.07117)

    本论文介绍了一个名为PILOT的基于预训练模型的持续学习工具箱，为在处理流式数据并适应新数据到来的现实场景中，利用预训练模型进行增量学习提供了一种有前景的方法。

    

    传统机器学习可以有效地解决各种问题，但主要在封闭环境中运作，处理流式数据时存在局限性。作为解决方案，增量学习应运而生，用于处理涉及新数据到来的现实场景。最近，预训练在不断取得重要进展，并引起了众多研究人员的关注。这些预训练模型（PTMs）的强大性能为开发能够有效适应现实场景的持续学习算法提供了有希望的途径。因此，探索在增量学习中利用PTMs已经成为必需。本文介绍了一个名为PILOT的基于预训练模型的持续学习工具箱。一方面，PILOT实施了一些基于预训练模型的最新班级增量学习算法，如L2P、DualPrompt和CODA-Prompt。另一方面，PILOT也适应了典型的班级增量学习场景。

    While traditional machine learning can effectively tackle a wide range of problems, it primarily operates within a closed-world setting, which presents limitations when dealing with streaming data. As a solution, incremental learning emerges to address real-world scenarios involving new data's arrival. Recently, pre-training has made significant advancements and garnered the attention of numerous researchers. The strong performance of these pre-trained models (PTMs) presents a promising avenue for developing continual learning algorithms that can effectively adapt to real-world scenarios. Consequently, exploring the utilization of PTMs in incremental learning has become essential. This paper introduces a pre-trained model-based continual learning toolbox known as PILOT. On the one hand, PILOT implements some state-of-the-art class-incremental learning algorithms based on pre-trained models, such as L2P, DualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical class-incre
    
[^3]: 面向音视频说话人验证的弱监督多任务学习

    Weakly-Supervised Multi-Task Learning for Audio-Visual Speaker Verification. (arXiv:2309.07115v1 [cs.SD])

    [http://arxiv.org/abs/2309.07115](http://arxiv.org/abs/2309.07115)

    本文介绍了一种弱监督多任务学习的方法，用于音视频说话人验证。我们通过引入辅助任务和非同步采样策略来提高距离度量学习方法的性能。实验结果表明，我们的网络在说话人验证中取得了最先进的性能。

    

    本文提出了一种方法，用于实现针对开放集音视频说话人验证的鲁棒多模态个人表示。距离度量学习（DML）方法通常在该问题领域占据主导地位，因为在新的和未见过的类上表现出很强的性能。在我们的工作中，我们探索了多任务学习技术，进一步提高了DML方法的性能，并展示了一个带有弱标签的辅助任务可以增加学习到的说话人表示的紧凑性。我们还将广义端到端损失（GE2E）扩展到多模态输入，并证明它可以在音视频空间中实现竞争性能。最后，我们在训练期间引入了一种非同步音视频采样随机策略，已经显示可以提高泛化能力。我们的网络在说话人验证方面达到了最先进的性能，报告了VoxCeleb1-O/E的三个官方试验列表上的0.244％，0.252％，0.441％的等误差率（EER）。

    In this paper, we present a methodology for achieving robust multimodal person representations optimized for open-set audio-visual speaker verification. Distance Metric Learning (DML) approaches have typically dominated this problem space, owing to strong performance on new and unseen classes. In our work, we explored multitask learning techniques to further boost performance of the DML approach and show that an auxiliary task with weak labels can increase the compactness of the learned speaker representation. We also extend the Generalized end-to-end loss (GE2E) to multimodal inputs and demonstrate that it can achieve competitive performance in an audio-visual space. Finally, we introduce a non-synchronous audio-visual sampling random strategy during training time that has shown to improve generalization. Our network achieves state of the art performance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal Error Rate (EER) on the three official trial lists of VoxCeleb1-O/E
    
[^4]: 对比性深度编码实现了具有不确定性的机器学习辅助组织病理学

    Contrastive Deep Encoding Enables Uncertainty-aware Machine-learning-assisted Histopathology. (arXiv:2309.07113v1 [cs.CV])

    [http://arxiv.org/abs/2309.07113](http://arxiv.org/abs/2309.07113)

    对比性深度编码能够利用公共领域的大型数据集进行预训练，并以少量随机选择注释进行微调，实现基于补丁的分类的最新水平。此外，引入不确定性感知的损失函数用于量化模型置信度，帮助专家进行选择。

    

    深度神经网络模型可以从数百万个组织病理学图像中学习临床相关的特征。然而，为每个医院、每种癌症类型和每个诊断任务生成高质量的注释来训练这些模型是非常费时费力的。另一方面，在某些情况下，虽然缺乏可靠的注释，但是公共领域中有可用的千兆字节级训练数据。在这项工作中，我们探讨了如何有意识地利用这些大型数据集来预训练深度网络以编码信息丰富的表示。然后，我们在一部分带有注释的训练数据上对预训练模型进行微调，以执行特定的下游任务。我们展示了相比于其他最新方法，我们的方法可以在只有1-10%的随机选择注释情况下达到基于补丁的分类的最新水平（SOTA）。此外，我们提出了一种不确定性感知的损失函数，用于量化模型在推断过程中的置信度。量化的不确定性有助于专家进行选择。

    Deep neural network models can learn clinically relevant features from millions of histopathology images. However generating high-quality annotations to train such models for each hospital, each cancer type, and each diagnostic task is prohibitively laborious. On the other hand, terabytes of training data -- while lacking reliable annotations -- are readily available in the public domain in some cases. In this work, we explore how these large datasets can be consciously utilized to pre-train deep networks to encode informative representations. We then fine-tune our pre-trained models on a fraction of annotated training data to perform specific downstream tasks. We show that our approach can reach the state-of-the-art (SOTA) for patch-level classification with only 1-10% randomly selected annotations compared to other SOTA approaches. Moreover, we propose an uncertainty-aware loss function, to quantify the model confidence during inference. Quantified uncertainty helps experts select th
    
[^5]: 通过子组混合实现数据增强以提高公平性

    Data Augmentation via Subgroup Mixup for Improving Fairness. (arXiv:2309.07110v1 [stat.ML])

    [http://arxiv.org/abs/2309.07110](http://arxiv.org/abs/2309.07110)

    本论文提出了一种通过子组混合的数据增强方法，以提高机器学习系统的群体公平性。通过添加代表低比例群体的新样本，我们可以实现数据的平衡，并且利用该方法提高公平性和准确性。

    

    在这项工作中，我们提出了通过子组间混合来增强数据以提高群体公平性。许多现实世界中的机器学习系统应用都存在着某些群体的偏见，这是由于训练数据的不平衡或反映了社会偏见。受到mixup在提高分类性能方面的成功启发，我们开发了一种对数据进行两两混合的方案，以增强训练数据，并鼓励为所有子组实现公平和准确的决策边界。针对群体公平性进行数据增强允许我们添加新的代表低比例群体的样本，以平衡亚群体。此外，我们的方法允许我们利用mixup的泛化能力来提高公平性和准确性。我们将我们提出的混合方法与现有的数据增强和偏见缓解方法在合成模拟和实际基准公平分类数据上进行比较，结果表明我们能够实现公平的结果，并且在准确性上具有鲁棒性，甚至有所提高。

    In this work, we propose data augmentation via pairwise mixup across subgroups to improve group fairness. Many real-world applications of machine learning systems exhibit biases across certain groups due to under-representation or training data that reflects societal biases. Inspired by the successes of mixup for improving classification performance, we develop a pairwise mixup scheme to augment training data and encourage fair and accurate decision boundaries for all subgroups. Data augmentation for group fairness allows us to add new samples of underrepresented groups to balance subpopulations. Furthermore, our method allows us to use the generalization ability of mixup to improve both fairness and accuracy. We compare our proposed mixup to existing data augmentation and bias mitigation approaches on both synthetic simulations and real-world benchmark fair classification data, demonstrating that we are able to achieve fair outcomes with robust if not improved accuracy.
    
[^6]: 多智能体强化学习的速度性能特征化

    Characterizing Speed Performance of Multi-Agent Reinforcement Learning. (arXiv:2309.07108v1 [cs.LG])

    [http://arxiv.org/abs/2309.07108](http://arxiv.org/abs/2309.07108)

    本文对多智能体强化学习（MARL）的速度性能进行了特征化研究。作者通过引入分类法，提出了三种最先进的MARL算法作为目标基准，以解决目前优化奖励的算法在训练时间速度性能方面的不足。

    

    多智能体强化学习（MARL）在大规模人工智能系统和大数据应用（如智能电网、监控等）取得了显著成功。现有的MARL算法的进展主要集中在通过引入各种机制来改进智能体之间的合作以提高奖励。然而，这些优化通常会在计算和内存方面产生较大负担，从而导致端到端训练时间的速度性能不佳。在本研究中，我们分析了速度性能（即延迟受限吞吐量）作为MARL实现的关键指标。具体而言，我们首先从加速的角度引入了一个MARL算法的分类法，包括（1）训练方案和（2）通信方法。利用我们的分类法，我们确定了三种最先进的MARL算法—多智能体深度确定性策略梯度（MADDPG）、面向目标的多智能体通信与合作（ToM2C）和网络多智能体RL（NeurComm）—作为目标基准。

    Multi-Agent Reinforcement Learning (MARL) has achieved significant success in large-scale AI systems and big-data applications such as smart grids, surveillance, etc. Existing advancements in MARL algorithms focus on improving the rewards obtained by introducing various mechanisms for inter-agent cooperation. However, these optimizations are usually compute- and memory-intensive, thus leading to suboptimal speed performance in end-to-end training time. In this work, we analyze the speed performance (i.e., latency-bounded throughput) as the key metric in MARL implementations. Specifically, we first introduce a taxonomy of MARL algorithms from an acceleration perspective categorized by (1) training scheme and (2) communication method. Using our taxonomy, we identify three state-of-the-art MARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG), Target-oriented Multi-agent Communication and Cooperation (ToM2C), and Networked Multi-Agent RL (NeurComm) - as target benchmar
    
[^7]: 在异构设备上减轻联邦学习中的群体偏见

    Mitigating Group Bias in Federated Learning for Heterogeneous Devices. (arXiv:2309.07085v1 [cs.LG])

    [http://arxiv.org/abs/2309.07085](http://arxiv.org/abs/2309.07085)

    本文提出了一种在分布式边缘应用中减轻联邦学习中群体偏见的方法，该方法可以通过计算跨域群体重要性来减轻全局模型的偏见，并保持隐私和资源利用效率。

    

    联邦学习正在分布式边缘应用中崭露头角作为一种保护隐私的模型训练方法。然而，大多数边缘部署是异构的，即它们的感知能力和环境在部署中各不相同。这种边缘异构违反了本地数据在客户端之间独立且分布相同 (IID) 的特性，产生了有偏见的全局模型，即对特定社区或群体做出不公平的决策和歧视。现有的偏见缓解技术只关注非IID数据中由标签异构引起的偏见，并没有考虑由特征异构导致的领域变化，也没有解决全局群体公平的问题。我们的工作提出了一种在保护隐私和不增加资源利用开销的情况下，减少群体偏见的联邦学习框架。我们的主要思想是利用平均条件概率来计算跨域群体重要性。

    Federated Learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients and produces biased global models i.e. models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity and do not address global group-fairness property.  Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy and without resource utilization overhead. Our main idea is to leverage average conditional probabilities to compute a cross-domain group \textit{importance we
    
[^8]: 贝叶斯动态有向无环图学习：在发现大脑动态效应连接组中的应用

    Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain. (arXiv:2309.07080v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.07080](http://arxiv.org/abs/2309.07080)

    本文介绍了贝叶斯动态有向无环图学习方法（BDyMA）来解决在发现大脑动态效应连接组中的两个主要挑战。该方法通过无约束框架实现更准确的结果和更稀疏的网络结构，使其特别适用于提取动态效应连接组。

    

    通过提取动态效应连接组（DEC）可以揭示大脑的复杂机制。最近，基于评分的有向无环图（DAG）发现方法在提取因果结构和推断有效连接方面表现出显著改进。然而，通过这些方法学习DEC仍然面临两个主要挑战：一个是高维动态DAG发现方法的根本无能力，另一个是fMRI数据质量低下。在本文中，我们引入了基于M-矩阵无环特性的贝叶斯动态DAG学习（BDyMA）方法来解决发现DEC中的挑战。所提出的动态因果模型使我们能够发现双向边缘。利用BDyMA方法中的无约束框架在检测高维网络方面可以获得更准确的结果，实现更稀疏的结果，使其特别适用于提取DEC。

    Understanding the complex mechanisms of the brain can be unraveled by extracting the Dynamic Effective Connectome (DEC). Recently, score-based Directed Acyclic Graph (DAG) discovery methods have shown significant improvements in extracting the causal structure and inferring effective connectivity. However, learning DEC through these methods still faces two main challenges: one with the fundamental impotence of high-dimensional dynamic DAG discovery methods and the other with the low quality of fMRI data. In this paper, we introduce Bayesian Dynamic DAG learning with M-matrices Acyclicity characterization \textbf{(BDyMA)} method to address the challenges in discovering DEC. The presented dynamic causal model enables us to discover bidirected edges as well. Leveraging an unconstrained framework in the BDyMA method leads to more accurate results in detecting high-dimensional networks, achieving sparser outcomes, making it particularly suitable for extracting DEC. Additionally, the score f
    
[^9]: 在深度学习中的可验证准确性、鲁棒性和泛化的边界

    The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning. (arXiv:2309.07072v1 [cs.LG])

    [http://arxiv.org/abs/2309.07072](http://arxiv.org/abs/2309.07072)

    本文评估了在分类任务中确定神经网络稳定性和准确性的理论限制，发现在一定的神经架构类别中，计算和验证理想的稳定和准确的神经网络是极具挑战性的甚至可能是不可能的。

    

    在这项工作中，我们评估了在分类任务中确定神经网络稳定性和准确性的理论限制。我们考虑了经典的分布无关框架和最小化经验风险的算法，可能还受到一些权重正则化的影响。我们证明，对于很多任务来说，在上述设置下计算和验证理想的稳定和准确的神经网络是极具挑战性的，甚至可能是不可能的，即使在给定的神经架构类别中存在这样的理想解决方案。

    In this work, we assess the theoretical limitations of determining guaranteed stability and accuracy of neural networks in classification tasks. We consider classical distribution-agnostic framework and algorithms minimising empirical risks and potentially subjected to some weights regularisation. We show that there is a large family of tasks for which computing and verifying ideal stable and accurate neural networks in the above settings is extremely challenging, if at all possible, even when such ideal solutions exist within the given class of neural architectures.
    
[^10]: 用于编译优化的大型语言模型

    Large Language Models for Compiler Optimization. (arXiv:2309.07062v1 [cs.PL])

    [http://arxiv.org/abs/2309.07062](http://arxiv.org/abs/2309.07062)

    本论文研究了将大型语言模型应用于代码优化的新颖方法，以7B参数的transformer模型为例，通过预测指令计数和生成优化代码等辅助学习任务，显著提高了模型的优化性能。在大量测试程序上的评估中，该方法相对编译器的优化效果提高了3.0%，并展现出令人惊喜的强大代码推理能力。

    

    我们探索了将大型语言模型应用于代码优化的新颖方法。我们展示了一个从头开始训练的7B参数的transformer模型，用于优化LLVM汇编的代码大小。该模型以未优化的汇编作为输入，并输出一组最佳优化程序的编译器选项。在训练过程中，我们要求模型预测优化前后的指令计数和优化后的代码本身。这些辅助学习任务显著提高了模型的优化性能，并提高了模型的理解深度。我们在一套大型测试程序上进行了评估。我们的方法在减少指令计数方面比编译器提高了3.0%，超过了需要数千次编译的两个最先进的基准方法。此外，该模型显示出令人惊讶的强大的代码推理能力，91%的时间生成可编译的代码，并70%的时间能完美模拟编译器的输出。

    We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding.  We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time.
    
[^11]: 深度量子图像模拟：解析神经网络对量子实验的见解

    Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments. (arXiv:2309.07056v1 [quant-ph])

    [http://arxiv.org/abs/2309.07056](http://arxiv.org/abs/2309.07056)

    本文使用了一种名为“梦境”的可解释人工智能技术，探索神经网络对量子光学实验的学习，发现网络可以改变量子系统的属性分布，并揭示了神经网络的学习策略。

    

    尽管神经网络在促进新的科学发现方面很有前景，但其逻辑背后的不透明性给解释其发现的挑战带来了困难。在本文中，我们使用一种名为“inception”或“深度梦境”的可解释人工智能（XAI）技术，该技术被发明用于计算机视觉的机器学习。我们使用这种技术来探索神经网络对量子光学实验的学习。我们的故事从对量子系统属性进行深度神经网络训练开始。经过训练后，我们“反转”神经网络--实际上是询问它如何想象具有特定属性的量子系统，以及如何连续修改量子系统以改变属性。我们发现网络可以改变量子系统的初始属性分布，我们可以概念化神经网络的学习策略。有趣的是，在较浅层，神经网络识别简单的属性，而在较深层次上...（内容省略）

    Despite their promise to facilitate new scientific discoveries, the opaqueness of neural networks presents a challenge in interpreting the logic behind their findings. Here, we use a eXplainable-AI (XAI) technique called $inception$ or $deep$ $dreaming$, which has been invented in machine learning for computer vision. We use this techniques to explore what neural networks learn about quantum optics experiments. Our story begins by training a deep neural networks on the properties of quantum systems. Once trained, we "invert" the neural network -- effectively asking how it imagines a quantum system with a specific property, and how it would continuously modify the quantum system to change a property. We find that the network can shift the initial distribution of properties of the quantum system, and we can conceptualize the learned strategies of the neural network. Interestingly, we find that, in the first layers, the neural network identifies simple properties, while in the deeper ones
    
[^12]: 基于极限学习机的高维计算PDE方法

    An Extreme Learning Machine-Based Method for Computational PDEs in Higher Dimensions. (arXiv:2309.07049v1 [math.NA])

    [http://arxiv.org/abs/2309.07049](http://arxiv.org/abs/2309.07049)

    本文提出了两种基于随机神经网络的高维偏微分方程计算方法，利用极限学习机扩展了低维到高维的逼近，通过随机插值点来满足PDE和边界条件，并通过最小二乘解得到网络参数的训练值。

    

    我们提出了两种基于随机神经网络的解决高维偏微分方程(PDE)的有效方法。受到这种类型网络的普遍逼近性质的启发，两种方法都将极限学习机(ELM)方法从低维扩展到高维。第一种方法中，d维未知解场由一个随机前馈神经网络表示，其中隐藏层参数被随机赋值并固定，而输出层参数则被训练。PDE和边界/初始条件，以及连续性条件(对于方法的局部变体)都在一组随机内部/边界插值点上得到满足。最小二乘解提供了网络参数的训练值，得到了一个线性或非线性代数方程组的解。第二种方法通过一个约束表达式基于一个...

    We present two effective methods for solving high-dimensional partial differential equations (PDE) based on randomized neural networks. Motivated by the universal approximation property of this type of networks, both methods extend the extreme learning machine (ELM) approach from low to high dimensions. With the first method the unknown solution field in $d$ dimensions is represented by a randomized feed-forward neural network, in which the hidden-layer parameters are randomly assigned and fixed while the output-layer parameters are trained. The PDE and the boundary/initial conditions, as well as the continuity conditions (for the local variant of the method), are enforced on a set of random interior/boundary collocation points. The resultant linear or nonlinear algebraic system, through its least squares solution, provides the trained values for the network parameters. With the second method the high-dimensional PDE problem is reformulated through a constrained expression based on an 
    
[^13]: 有向加权图的最优输运距离：以细胞间通讯网络为案例研究

    Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks. (arXiv:2309.07030v1 [cs.LG])

    [http://arxiv.org/abs/2309.07030](http://arxiv.org/abs/2309.07030)

    本文提出了两种基于最优输运的距离度量，用于比较有向图，并通过仿真图数据和单细胞RNA-seq数据推断的实际细胞间通讯图对其相对表现进行了评估。

    

    近年来，比较最优输运图引起了相当大的关注，因为最优输运引起的距离既提供了图之间的合理度量，又通过输运计划的可解释描述了图之间相关变化。然而，由于缺乏对称性，通常考虑的最优输运距离主要用于无向图。本文提出了两种基于最优输运变体的距离度量来比较有向图：（i）地球移动距离（Wasserstein）和（ii）Gromov-Wasserstein（GW）距离。我们评估了这两种距离，并讨论了它们在仿真图数据和基于单细胞RNA-seq数据推断的实际有向细胞间通讯图上的相对表现。

    Comparing graphs of optimal transport has recently gained significant attention, as the distances induced by optimal transport provide both a principled metric between graphs as well as an interpretable description of the associated changes between graphs in terms of a transport plan. As the lack of symmetry introduces challenges in the typically considered formulations, optimal transport distances for graphs have mostly been developed for undirected graphs. Here, we propose two distance measures to compare directed graphs based on variants of optimal transport: (i) an earth movers distance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate these two distances and discuss their relative performance for both simulated graph data and real-world directed cell-cell communication graphs, inferred from single-cell RNA-seq data.
    
[^14]: 无监督的对比一致排序与语言模型

    Unsupervised Contrast-Consistent Ranking with Language Models. (arXiv:2309.06991v1 [cs.LG])

    [http://arxiv.org/abs/2309.06991](http://arxiv.org/abs/2309.06991)

    无监督的对比一致排序与语言模型，通过训练一个受逻辑约束引导的探测模型，实现在多个语句中始终映射到对比的真-假极点的排序任务。

    

    语言模型包含基于排序的知识，并且是处理上下文排名任务的强大解决者。最近的研究关注于配对、点对和列表提示技术，以揭示语言模型的排序知识。然而，我们发现，即使在仔细校准和限制解码的情况下，基于提示的技术在产生的排序中也不总是自洽的。这促使我们探索一种受无监督探测方法Contrast-Consistent Search（CCS）启发的替代方法。这个想法是训练一个受逻辑约束引导的探测模型：模型对一个语句及其否定的表示必须在多个语句中始终映射到对比的真-假极点。我们假设类似的约束适用于所有项通过一致性对相关排序任务。

    Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank reviews by sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting techniques to elicit a language model's ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probing model guided by a logical constraint: a model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent pair
    
[^15]: MASTERKEY: 对说话人验证系统的实际后门攻击

    MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems. (arXiv:2309.06981v1 [cs.CR])

    [http://arxiv.org/abs/2309.06981](http://arxiv.org/abs/2309.06981)

    这项工作提出了一种名为MASTERKEY的后门攻击，针对说话人验证系统。与以往攻击不同，在实际环境下攻击者对目标用户没有任何了解。通过嵌入说话人特征和语义信息，以及集成信道失真，我们的攻击可以成功破坏多个流行的SV模型，达到100％的攻击成功率。

    

    说话人验证（SV）广泛应用于移动系统中，通过使用用户的语音特征来认证合法用户。在这项工作中，我们提出了一种名为MASTERKEY的后门攻击，以破坏SV模型。与以往的攻击不同，我们关注的是在攻击者对目标用户没有任何了解的实际环境下进行攻击。为了设计MASTERKEY，我们调查了现有的针对未见目标的中毒攻击的局限性。然后，我们优化了一个通用的后门，可以攻击任意目标。接下来，我们将说话人的特征和语义信息嵌入到后门中，使其不可察觉。最后，我们估计了信道失真，并将其集成到后门中。我们验证了我们的攻击对6个流行的SV模型。具体而言，我们中毒了共计53个模型，并使用我们的触发器攻击了16,430个注册说话人，其中包括在53个中毒模型中注册的310个目标说话人。我们的攻击成功率达到100％。

    Speaker Verification (SV) is widely deployed in mobile systems to authenticate legitimate users by using their voice traits. In this work, we propose a backdoor attack MASTERKEY, to compromise the SV models. Different from previous attacks, we focus on a real-world practical setting where the attacker possesses no knowledge of the intended victim. To design MASTERKEY, we investigate the limitation of existing poisoning attacks against unseen targets. Then, we optimize a universal backdoor that is capable of attacking arbitrary targets. Next, we embed the speaker's characteristics and semantics information into the backdoor, making it imperceptible. Finally, we estimate the channel distortion and integrate it into the backdoor. We validate our attack on 6 popular SV models. Specifically, we poison a total of 53 models and use our trigger to attack 16,430 enrolled speakers, composed of 310 target speakers enrolled in 53 poisoned models. Our attack achieves 100% attack success rate with a
    
[^16]: 自回归的下一个标记预测器是通用学习器。

    Auto-Regressive Next-Token Predictors are Universal Learners. (arXiv:2309.06979v1 [cs.LG])

    [http://arxiv.org/abs/2309.06979](http://arxiv.org/abs/2309.06979)

    自回归的下一个标记预测器可以有效地近似图灵机计算的任何函数，并且在文本生成和算术任务上展现出非平凡的性能。

    

    大型语言模型展现出在逻辑和数学推理方面的非凡能力，使其能够解决复杂任务。有趣的是，这些能力在训练于下一个标记预测的简单任务上的网络中出现。在这项工作中，我们提出了一个用于研究自回归下一个标记预测器的理论框架。我们证明了即使是简单的模型，如线性下一个标记预测器，当其在思维链数据上训练时，可以有效地近似图灵机计算的任何函数。我们引入了一个新的复杂度度量——长度复杂度，它衡量了在近似某个目标函数时，思维链序列中所需的中间标记的数量，并分析了长度复杂度和其他复杂性概念之间的相互关系。最后，我们通过实验证明简单的下一个标记预测器，如线性网络和浅层多层感知机（MLP），在文本生成和算术任务上展示出非平凡的性能。

    Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our resul
    
[^17]: DNNShifter: 一种高效的边缘计算DNN剪枝系统

    DNNShifter: An Efficient DNN Pruning System for Edge Computing. (arXiv:2309.06973v1 [cs.LG])

    [http://arxiv.org/abs/2309.06973](http://arxiv.org/abs/2309.06973)

    DNNShifter是一种高效的边缘计算DNN剪枝系统，通过快速推导出合适的模型变体来提供高推理准确性，适应系统和网络条件变化的工作负载需求。

    

    深度神经网络（DNN）是许多机器学习应用的基础。生产质量的DNN模型通过训练数百万个DNN参数来实现高推理准确性，但这占用了大量的计算资源。这对于在网络的极端边缘处工作的资源（如具有有限计算和内存资源的移动和嵌入式设备）构成挑战。为了解决这个问题，需要对模型进行剪枝，以创建轻量级、更适合这些设备的变体。现有的剪枝方法无法在不引入显著时间成本和负担的情况下提供与未剪枝模型相似的质量模型，或者只限于离线使用场景。我们的工作通过保持原始模型的准确性，快速推导出适合的模型变体。模型变体可以在系统和网络条件发生变化以匹配工作负载需求时快速切换。本文介绍了DNNShifter，一种端到端的DNN训练、空间剪枝和模型切换系统。

    Deep neural networks (DNNs) underpin many machine learning applications. Production quality DNN models achieve high inference accuracy by training millions of DNN parameters which has a significant resource footprint. This presents a challenge for resources operating at the extreme edge of the network, such as mobile and embedded devices that have limited computational and memory resources. To address this, models are pruned to create lightweight, more suitable variants for these devices. Existing pruning methods are unable to provide similar quality models compared to their unpruned counterparts without significant time costs and overheads or are limited to offline use cases. Our work rapidly derives suitable model variants while maintaining the accuracy of the original model. The model variants can be swapped quickly when system and network conditions change to match workload demand. This paper presents DNNShifter, an end-to-end DNN training, spatial pruning, and model switching syst
    
[^18]: 设定正确的期望：随时间变化的算法补救措施

    Setting the Right Expectations: Algorithmic Recourse Over Time. (arXiv:2309.06969v1 [cs.LG])

    [http://arxiv.org/abs/2309.06969](http://arxiv.org/abs/2309.06969)

    这项研究关注算法补救措施中忽视的关键要素 - 不断变化的环境对补救效果的影响。研究发现，在时间推移和个体间竞争的情况下，初始的补救建议可能变得不可靠，因此需要考虑时间变化来确保补救的有效性。

    

    算法系统经常被用于协助高风险决策。鉴于此，算法补救措施，即个体应能够针对算法系统产生的不良结果采取行动，受到越来越多的关注。迄今为止，关于算法补救措施的大部分文献主要关注如何为单个个体提供补救，而忽略了一个关键要素：不断变化的环境的影响。忽视这些对补救措施的影响是一个重大的疏忽，因为几乎所有情况下，补救措施都包括个体首次做出不利尝试，然后在以后的某个时间点提供一次或多次尝试的机会 - 当时环境可能已经发生了变化。这可能会产生虚假的期望，因为初始的补救建议随时间的推移可能变得不太可靠，原因是模型漂移和个体之间对有利结果的竞争导致的。在这项工作中，我们提出了一种考虑时间变化的算法补救措施的方法。

    Algorithmic systems are often called upon to assist in high-stakes decision making. In light of this, algorithmic recourse, the principle wherein individuals should be able to take action against an undesirable outcome made by an algorithmic system, is receiving growing attention. The bulk of the literature on algorithmic recourse to-date focuses primarily on how to provide recourse to a single individual, overlooking a critical element: the effects of a continuously changing context. Disregarding these effects on recourse is a significant oversight, since, in almost all cases, recourse consists of an individual making a first, unfavorable attempt, and then being given an opportunity to make one or several attempts at a later date - when the context might have changed. This can create false expectations, as initial recourse recommendations may become less reliable over time due to model drift and competition for access to the favorable outcome between individuals.  In this work we prop
    
[^19]: 基于DNA的数据存储的隐式神经多描述方法

    Implicit Neural Multiple Description for DNA-based data storage. (arXiv:2309.06956v1 [eess.IV])

    [http://arxiv.org/abs/2309.06956](http://arxiv.org/abs/2309.06956)

    本论文提出了一种基于DNA的数据存储方法，采用隐式神经多描述技术编码数据。通过创新的压缩方案和神经网络，有效地解决了DNA存储中的错误问题，并在性能上超过了传统的MDC方法。

    

    DNA由于其卓越的存储密度和长期稳定性，基于其固有的生物分子结构，展示出作为数据存储解决方案的巨大潜力。然而，开发这种新型介质面临着一系列挑战，特别是处理存储和生物操作引起的错误。这些挑战进一步受限于DNA序列的结构约束和成本考虑。为了应对这些限制，我们首创了一种新型的压缩方案和一种利用神经网络进行DNA数据存储的尖端多描述编码（MDC）技术。我们的MDC方法引入了一种创新的将数据编码成DNA的方法，特别设计来有效地抵抗错误。值得注意的是，我们的新压缩方案在DNA数据存储方面超过了经典图像压缩方法。此外，我们的方法表现出优于依赖自动编码器的传统MDC方法的特点。

    DNA exhibits remarkable potential as a data storage solution due to its impressive storage density and long-term stability, stemming from its inherent biomolecular structure. However, developing this novel medium comes with its own set of challenges, particularly in addressing errors arising from storage and biological manipulations. These challenges are further conditioned by the structural constraints of DNA sequences and cost considerations. In response to these limitations, we have pioneered a novel compression scheme and a cutting-edge Multiple Description Coding (MDC) technique utilizing neural networks for DNA data storage. Our MDC method introduces an innovative approach to encoding data into DNA, specifically designed to withstand errors effectively. Notably, our new compression scheme overperforms classic image compression methods for DNA-data storage. Furthermore, our approach exhibits superiority over conventional MDC methods reliant on auto-encoders. Its distinctive streng
    
[^20]: 随机森林中超参数对变量选择的影响

    Effect of hyperparameters on variable selection in random forests. (arXiv:2309.06943v1 [stat.ML])

    [http://arxiv.org/abs/2309.06943](http://arxiv.org/abs/2309.06943)

    这项研究评估了随机森林中超参数对变量选择的影响，在高维组学研究中，适当设置RF超参数对选择重要变量具有重要意义。

    

    随机森林（RF）在高维组学研究中适用于预测建模和变量选择。先前研究了RF算法的超参数对预测性能和变量重要性估计的影响，但超参数对基于RF的变量选择的影响尚不清楚。我们利用理论分布和实证基因表达数据进行了两个模拟研究，评估了Vita和Boruta变量选择 procedures 在选择重要变量（敏感性）的同时控制虚警率（FDR）的能力。我们的结果表明，在训练数据集中，要比训练数据集的抽取策略和最小终端节点大小更能影响选择 procedures。RF超参数的合适设置取决于

    Random forests (RFs) are well suited for prediction modeling and variable selection in high-dimensional omics studies. The effect of hyperparameters of the RF algorithm on prediction performance and variable importance estimation have previously been investigated. However, how hyperparameters impact RF-based variable selection remains unclear. We evaluate the effects on the Vita and the Boruta variable selection procedures based on two simulation studies utilizing theoretical distributions and empirical gene expression data. We assess the ability of the procedures to select important variables (sensitivity) while controlling the false discovery rate (FDR). Our results show that the proportion of splitting candidate variables (mtry.prop) and the sample fraction (sample.fraction) for the training dataset influence the selection procedures more than the drawing strategy of the training datasets and the minimal terminal node size. A suitable setting of the RF hyperparameters depends on the
    
[^21]: 无集合的人工智能

    Collectionless Artificial Intelligence. (arXiv:2309.06938v1 [cs.AI])

    [http://arxiv.org/abs/2309.06938](http://arxiv.org/abs/2309.06938)

    本文提出了无集合原则的学习协议的思路，其中机器在环境交互背景中掌握认知技能，避免了数据集集中化的风险。

    

    大体上，处理庞大数据集被认为是机器学习进展和相关领域中壮观结果的基本组成部分，对于这种数据集的集中化存在着越来越多的风险意识。本文支持一种新的学习协议思路，其中机器在真正以环境交互为中心的类人认知背景下掌握认知技能。这意味着学习协议需要遵循无集合原则，即在每个时间点，从环境中获取的数据被用于更新当前环境内部表示，并且代理不能对时间流进行记录。基本上，不能存储来自传感器的时间信息，从而促进了无集合原则的发展。

    By and large, the professional handling of huge data collections is regarded as a fundamental ingredient of the progress of machine learning and of its spectacular results in related disciplines, with a growing agreement on risks connected to the centralization of such data collections. This paper sustains the position that the time has come for thinking of new learning protocols where machines conquer cognitive skills in a truly human-like context centered on environmental interactions. This comes with specific restrictions on the learning protocol according to the collectionless principle, which states that, at each time instant, data acquired from the environment is processed with the purpose of contributing to update the current internal representation of the environment, and that the agent is not given the privilege of recording the temporal stream. Basically, there is neither permission to store the temporal information coming from the sensors, thus promoting the development of s
    
[^22]: 使用语义网技术对位错动力学数据进行建模

    Modeling Dislocation Dynamics Data Using Semantic Web Technologies. (arXiv:2309.06930v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.06930](http://arxiv.org/abs/2309.06930)

    本文介绍了如何使用语义网技术对位错动力学模拟数据进行建模，并通过添加缺失的概念和与相关本体对齐来扩展位错本体。

    

    材料科学与工程领域的研究着眼于材料的设计、合成、性能和性能。被广泛研究的一个重要材料类别是晶体材料，包括金属和半导体。晶体材料通常包含一种称为“位错”的特殊缺陷。这种缺陷显著影响各种材料性能，包括强度、断裂韧性和延展性。近年来，研究人员通过实验表征技术和模拟（如位错动力学模拟）致力于理解位错行为。本文介绍了如何通过使用语义网技术以本体方式对位错动力学模拟数据进行建模。我们通过添加缺失的概念并将其与其他两个与该领域相关的本体（即Elementary Multi-perspectiv）进行对齐来扩展已有的位错本体。

    Research in the field of Materials Science and Engineering focuses on the design, synthesis, properties, and performance of materials. An important class of materials that is widely investigated are crystalline materials, including metals and semiconductors. Crystalline material typically contains a distinct type of defect called "dislocation". This defect significantly affects various material properties, including strength, fracture toughness, and ductility. Researchers have devoted a significant effort in recent years to understanding dislocation behavior through experimental characterization techniques and simulations, e.g., dislocation dynamics simulations. This paper presents how data from dislocation dynamics simulations can be modeled using semantic web technologies through annotating data with ontologies. We extend the already existing Dislocation Ontology by adding missing concepts and aligning it with two other domain-related ontologies (i.e., the Elementary Multi-perspectiv
    
[^23]: 探究行动表示在策略梯度算法中的影响

    Investigating the Impact of Action Representations in Policy Gradient Algorithms. (arXiv:2309.06921v1 [cs.LG])

    [http://arxiv.org/abs/2309.06921](http://arxiv.org/abs/2309.06921)

    本论文讨论了强化学习中行动表示的影响，并通过实验发现行动表示对流行的强化学习基准任务的学习性能有着显著影响，其中性能差异可归因于优化问题空间的复杂性的变化。

    

    强化学习（RL）是一个适用于学习解决复杂实际任务的多功能框架。然而，在实践中，对RL算法学习性能的影响往往不太清楚。我们讨论了不同的分析技术，并评估它们在调查RL中行动表示的影响方面的有效性。我们的实验证明，行动表示可以显著影响流行的RL基准任务的学习性能。分析结果表明，一些性能差异可以归因于优化问题空间的复杂性的改变。最后，我们讨论了RL算法分析技术的开放挑战。

    Reinforcement learning~(RL) is a versatile framework for learning to solve complex real-world tasks. However, influences on the learning performance of RL algorithms are often poorly understood in practice. We discuss different analysis techniques and assess their effectiveness for investigating the impact of action representations in RL. Our experiments demonstrate that the action representation can significantly influence the learning performance on popular RL benchmark tasks. The analysis results indicate that some of the performance differences can be attributed to changes in the complexity of the optimization landscape. Finally, we discuss open challenges of analysis techniques for RL algorithms.
    
[^24]: 使用狄利克雷生成基础的回顾的连续学习

    Continual Learning with Dirichlet Generative-based Rehearsal. (arXiv:2309.06917v1 [cs.CL])

    [http://arxiv.org/abs/2309.06917](http://arxiv.org/abs/2309.06917)

    该论文提出了一种新颖的基于狄利克雷生成的回顾策略，用于解决连续学习中伪样本生成的挑战。

    

    最近在面向任务的数据驱动对话系统（ToDs）方面的进展由于计算约束和耗时问题而困扰着增量学习。连续学习（CL）试图通过避免密集的预训练来解决这个问题，但它面临着灾难性遗忘（CF）的问题。虽然基于生成的回顾CL方法取得了显著进展，但生成能准确反映底层任务特定分布的伪样本仍然是一个挑战。在本文中，我们提出了狄利克雷连续学习（DCL），这是一种新颖的基于生成的回顾策略用于CL。与传统上在条件变分自动编码器（CVAE）中使用的高斯潜变量不同，DCL利用狄利克雷分布的灵活性和多样性来建模潜变量先验。这使得它能够高效地捕捉先前任务的句级特征，并有效地指导伪样本的生成。此外还引入了Jensen-苏彻利散度作为训练目标，以进一步提高DCL的性能。

    Recent advancements in data-driven task-oriented dialogue systems (ToDs) struggle with incremental learning due to computational constraints and time-consuming issues. Continual Learning (CL) attempts to solve this by avoiding intensive pre-training, but it faces the problem of catastrophic forgetting (CF). While generative-based rehearsal CL methods have made significant strides, generating pseudo samples that accurately reflect the underlying task-specific distribution is still a challenge. In this paper, we present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal strategy for CL. Unlike the traditionally used Gaussian latent variable in the Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and versatility of the Dirichlet distribution to model the latent prior variable. This enables it to efficiently capture sentence-level features of previous tasks and effectively guide the generation of pseudo samples. In addition, we introduce Jensen-
    
[^25]: 走向TopMost：一个主题建模系统工具包

    Towards the TopMost: A Topic Modeling System Toolkit. (arXiv:2309.06908v1 [cs.CL])

    [http://arxiv.org/abs/2309.06908](http://arxiv.org/abs/2309.06908)

    本文提出了一个名为TopMost的主题建模系统工具包，通过涵盖更广泛的主题建模场景和具有高度凝聚力和解耦模块化设计的特点，可以促进主题模型的研究和应用。

    

    主题模型已经在过去几十年中被提出，并且具有各种应用，在神经变分推断的推动下近期得到了更新。然而，这些主题模型采用完全不同的数据集、实现和评估设置，这阻碍了它们的快速利用和公平比较。这严重阻碍了主题模型的研究进展。为了解决这些问题，本文提出了一个主题建模系统工具包（TopMost）。与现有的工具包相比，TopMost通过涵盖更广泛的主题建模场景，包括数据集预处理、模型训练、测试和评估的完整生命周期，脱颖而出。TopMost的高度凝聚力和解耦模块化设计可以快速利用，公平比较，并灵活扩展不同的主题模型，这可以促进主题模型的研究和应用。我们的代码、教程和文档可在https://github.com/bobxwu/topmost 上获得。

    Topic models have been proposed for decades with various applications and recently refreshed by the neural variational inference. However, these topic models adopt totally distinct dataset, implementation, and evaluation settings, which hinders their quick utilization and fair comparisons. This greatly hinders the research progress of topic models. To address these issues, in this paper we propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by covering a wider range of topic modeling scenarios including complete lifecycles with dataset pre-processing, model training, testing, and evaluations. The highly cohesive and decoupled modular design of TopMost enables quick utilization, fair comparisons, and flexible extensions of different topic models. This can facilitate the research and applications of topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.
    
[^26]: 针对无监督在线一般连续学习的领域感知扩增策略

    Domain-Aware Augmentations for Unsupervised Online General Continual Learning. (arXiv:2309.06896v1 [cs.LG])

    [http://arxiv.org/abs/2309.06896](http://arxiv.org/abs/2309.06896)

    本文提出了一种新颖的方法，通过定义和使用流依赖的数据扩增方式以及一些实现技巧，增强了UOGCL中的对比学习，达到了与其他无监督方法相比的最新成果，并减小了监督和无监督连续学习之间的差距。

    

    连续学习一直是一项具有挑战性的任务，尤其是在处理无监督场景时，例如无监督在线一般连续学习（UOGCL），其中学习代理没有先验知识关于类别边界或任务更改的信息。虽然以前的研究集中在减少等价学习中的遗忘，但最近的研究表明，自监督学习者对遗忘更具鲁棒性。本文提出了一种新的方法，通过定义和使用流依赖的数据扩增方式以及一些实现技巧，增强了UOGCL中对比学习的记忆使用。我们提出的方法简单而有效，在所有考虑的设置中实现了与其他无监督方法相比的最新成果，并减小了监督和无监督连续学习之间的差距。我们的领域感知扩增过程可以适应其他基于回放的方法，使它成为一种有前途的连续学习策略。

    Continual Learning has been challenging, especially when dealing with unsupervised scenarios such as Unsupervised Online General Continual Learning (UOGCL), where the learning agent has no prior knowledge of class boundaries or task change information. While previous research has focused on reducing forgetting in supervised setups, recent studies have shown that self-supervised learners are more resilient to forgetting. This paper proposes a novel approach that enhances memory usage for contrastive learning in UOGCL by defining and using stream-dependent data augmentations together with some implementation tricks. Our proposed method is simple yet effective, achieves state-of-the-art results compared to other unsupervised approaches in all considered setups, and reduces the gap between supervised and unsupervised continual learning. Our domain-aware augmentation procedure can be adapted to other replay-based methods, making it a promising strategy for continual learning.
    
[^27]: MagiCapture: 高分辨率多概念人像定制

    MagiCapture: High-Resolution Multi-Concept Portrait Customization. (arXiv:2309.06895v1 [cs.CV])

    [http://arxiv.org/abs/2309.06895](http://arxiv.org/abs/2309.06895)

    MagiCapture是一种高分辨率多概念人像定制方法，通过几个主题和风格参考，能够生成高质量的特定风格人像图像。

    

    大规模的文本到图像模型，包括稳定扩散，能够生成高保真度的逼真人像照片。有一个专门研究个性化这些模型的领域，旨在使用提供的参考图像集合合成特定主题或风格。然而，尽管这些个性化方法产生的结果令人满意，但其生成的图像往往缺乏真实感，并且尚未达到商业可行的水平。在人像图像生成中尤为明显，因为由于我们内在的人类偏见，人脸中的任何不自然的痕迹都很容易被识别出来。为了解决这个问题，我们引入了MagiCapture，一种个性化方法，用于将主题和风格概念融合，仅使用几个主题和风格参考即可生成高分辨率的人像图像。例如，给定一些随机的自拍照，我们经过调优的模型就可以生成特定风格（如护照或个人资料）的高质量人像图像。

    Large-scale text-to-image models including Stable Diffusion are capable of generating high-fidelity photorealistic portrait images. There is an active research area dedicated to personalizing these models, aiming to synthesize specific subjects or styles using provided sets of reference images. However, despite the plausible results from these personalization methods, they tend to produce images that often fall short of realism and are not yet on a commercially viable level. This is particularly noticeable in portrait image generation, where any unnatural artifact in human faces is easily discernible due to our inherent human bias. To address this, we introduce MagiCapture, a personalization method for integrating subject and style concepts to generate high-resolution portrait images using just a few subject and style references. For instance, given a handful of random selfies, our fine-tuned model can generate high-quality portrait images in specific styles, such as passport or profil
    
[^28]: 保持简单：谁说有监督的Transformer模型注意力不集中? (arXiv:2309.06891v1 [cs.CV])

    Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?. (arXiv:2309.06891v1 [cs.CV])

    [http://arxiv.org/abs/2309.06891](http://arxiv.org/abs/2309.06891)

    该论文提出了一种通用的池化框架SimPool，用于替代卷积和Transformer编码器的默认池化机制，无论是有监督还是自监督的方法都能提高性能，并提供能够描绘对象边界的注意力图。

    

    卷积网络和视觉Transformer通过不同形式的成对交互来进行池化，包括在网络层之间进行池化和在网络末端进行池化。后者真的需要与前者不同吗？作为池化的副产品，视觉Transformer可以提供免费的空间注意力，但通常质量较低，除非是自监督的，而这方面的研究并不充分。监督是真正的问题吗？在这项工作中，我们开发了一个通用的池化框架，然后将一些现有方法作为实例化。通过讨论每组方法的特性，我们得到了SimPool，一个简单的基于注意力的池化机制，用于替代卷积和Transformer编码器的默认机制。我们发现，无论是有监督还是自监督，这都改善了预训练和下游任务的性能，并提供了能够描绘对象边界的注意力图。因此，可以称SimPool是通用的。据我们所知，我们是第一个这样做的。

    Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem?  In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal. To our knowledge, we are the first to o
    
[^29]: ProMap：电子商务产品映射数据集

    ProMap: Datasets for Product Mapping in E-commerce. (arXiv:2309.06882v1 [cs.LG])

    [http://arxiv.org/abs/2309.06882](http://arxiv.org/abs/2309.06882)

    该论文介绍了两个新的产品映射数据集：ProMapCz和ProMapEn，分别包含捷克和英文产品对，这些数据集具有较为完整的产品信息，并解决了目前现有数据集无法区分非常相似但不匹配产品对的问题。

    

    产品映射的目标是确定两个不同电子商店中的两个列表是否描述相同的产品。然而，现有的匹配和非匹配产品对的数据集经常受到产品信息不完整或者只包含非常远的非匹配产品的问题。因此，尽管在这些数据集上训练的预测模型取得了良好的结果，但实际上它们无法区分非常相似但不匹配的产品对，因此无法使用。本文介绍了两个新的产品映射数据集：ProMapCz包含1495对捷克产品，ProMapEn包含1555对英文产品，这些产品对来自两个电子商店，包含了产品的图像和文字描述，包括规格，使它们成为最完整的产品映射数据集之一。此外，非匹配产品是通过两个阶段进行选择的。

    The goal of product mapping is to decide, whether two listings from two different e-shops describe the same products. Existing datasets of matching and non-matching pairs of products, however, often suffer from incomplete product information or contain only very distant non-matching products. Therefore, while predictive models trained on these datasets achieve good results on them, in practice, they are unusable as they cannot distinguish very similar but non-matching pairs of products. This paper introduces two new datasets for product mapping: ProMapCz consisting of 1,495 Czech product pairs and ProMapEn consisting of 1,555 English product pairs of matching and non-matching products manually scraped from two pairs of e-shops. The datasets contain both images and textual descriptions of the products, including their specifications, making them one of the most complete datasets for product mapping. Additionally, the non-matching products were selected in two phases, creating two types 
    
[^30]: 通过强化学习动态控制拟晶结构的自组装

    Dynamic control of self-assembly of quasicrystalline structures through reinforcement learning. (arXiv:2309.06869v1 [cond-mat.soft])

    [http://arxiv.org/abs/2309.06869](http://arxiv.org/abs/2309.06869)

    本研究提出使用强化学习来控制具有多边形单元颗粒的动态自组装过程，形成十二边准晶体。我们通过估计最佳的温度控制策略，成功地生成了几乎没有缺陷的结构。强化学习获得的温度调度比传统的预设温度调度更有效地重现了期望的结构。

    

    我们提出使用强化学习来控制具有多边形单元的颗粒的动态自组装过程，形成十二边准晶体（DDQC）。这些具有多边形单元的颗粒与其他颗粒具有各向异性相互作用，从而形成DDQC。然而，它们在稳态下的结构受其结构形成的动力学路径的显著影响。我们通过Q学习方法估计了最佳的温度控制策略，并证明我们可以使用估计的策略生成几乎没有缺陷的DDQC。通过强化学习获得的温度调度比传统的预设温度调度（如退火）更有效地重现了期望的结构。为了阐明学习的成功，我们还分析了一个描述结构变化动力学的简单模型，其中的运动是在三井势能中进行的。我们发现强化学习能够自主地发现增强结构波动的临界温度。

    We propose reinforcement learning to control the dynamical self-assembly of the dodecagonal quasicrystal (DDQC) from patchy particles. The patchy particles have anisotropic interactions with other particles and form DDQC. However, their structures at steady states are significantly influenced by the kinetic pathways of their structural formation. We estimate the best policy of temperature control trained by the Q-learning method and demonstrate that we can generate DDQC with few defects using the estimated policy. The temperature schedule obtained by reinforcement learning can reproduce the desired structure more efficiently than the conventional pre-fixed temperature schedule, such as annealing. To clarify the success of the learning, we also analyse a simple model describing the kinetics of structural changes through the motion in a triple-well potential. We have found that reinforcement learning autonomously discovers the critical temperature at which structural fluctuations enhance
    
[^31]: 监督机器学习和基于物理的机器学习方法用于预测铝合金搅拌摩擦增材制造中的峰值温度分布

    Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy. (arXiv:2309.06838v1 [cs.LG])

    [http://arxiv.org/abs/2309.06838](http://arxiv.org/abs/2309.06838)

    本研究提出了监督机器学习和基于物理的机器学习相结合的方法，用于预测搅拌摩擦增材制造中的峰值温度分布。实验结果表明，集成的机器学习方法在预测中表现出了较好的性能，最佳的SML方法为梯度提升法，最低的均方误差为165.78。

    

    增材搅拌摩擦沉积（AFSD）是一种新型的固态增材制造技术，它解决了传统粉末床熔炼和定向能量沉积方法中存在的孔隙率、开裂和性能各向异性等问题。然而，AFSD中的工艺参数、热量分布和得到的显微结构之间的相关性仍然不够清楚，这妨碍了性能的工艺优化。本研究运用了一种先进的框架，将监督机器学习（SML）和基于物理的神经网络（PINNs）相结合，以从工艺参数预测AFSD中的峰值温度分布。对于SML建模，使用了八种回归算法，而对于PINNs，使用了运输、波传播、热传导和量子力学的控制方程。在多个统计指标上，集成的机器学习方法表现出了较好的性能，梯度提升法是最佳的SML方法，最低的均方误差为165.78。

    Additive friction stir deposition (AFSD) is a novel solid-state additive manufacturing technique that circumvents issues of porosity, cracking, and properties anisotropy that plague traditional powder bed fusion and directed energy deposition approaches. However, correlations between process parameters, thermal profiles, and resulting microstructure in AFSD remain poorly understood. This hinders process optimization for properties. This work employs a cutting-edge framework combining supervised machine learning (SML) and physics-informed neural networks (PINNs) to predict peak temperature distribution in AFSD from process parameters. Eight regression algorithms were implemented for SML modeling, while four PINNs leveraged governing equations for transport, wave propagation, heat transfer, and quantum mechanics. Across multiple statistical measures, ensemble techniques like gradient boosting proved superior for SML, with lowest MSE of 165.78. The integrated ML approach was also applied 
    
[^32]: UniBrain:通用脑部MRI诊断与分层知识增强预训练

    UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training. (arXiv:2309.06828v1 [cs.CV])

    [http://arxiv.org/abs/2309.06828](http://arxiv.org/abs/2309.06828)

    UniBrain是一种用于通用脑部MRI诊断的分层知识增强预训练框架，利用大规模数据集和分层对齐机制提高了特征学习的效率。

    

    磁共振成像（MRI）在脑部疾病诊断中起着关键作用，因此提出了一系列计算机辅助人工智能方法。然而，早期的探索通常集中于一个研究中的有限类型的脑部疾病，并在小规模数据上训练模型，导致泛化的瓶颈。为了更有效和可扩展的范例，我们提出了一种用于通用脑部MRI诊断的分层知识增强预训练框架，称为UniBrain。具体而言，UniBrain利用了一个包含24,770个图像-报告配对的大规模数据集进行常规诊断。不同于先前用于单一视觉或文本特征的预训练技术，或者通过视觉和语言信息之间的蛮力对齐，我们利用不同粒度的报告信息的独特特征构建了一种分层对齐机制，从而增强了特征学习的效率。

    Magnetic resonance imaging~(MRI) have played a crucial role in brain disease diagnosis, with which a range of computer-aided artificial intelligence methods have been proposed. However, the early explorations usually focus on the limited types of brain diseases in one study and train the model on the data in a small scale, yielding the bottleneck of generalization. Towards a more effective and scalable paradigm, we propose a hierarchical knowledge-enhanced pre-training framework for the universal brain MRI diagnosis, termed as UniBrain. Specifically, UniBrain leverages a large-scale dataset of 24,770 imaging-report pairs from routine diagnostics. Different from previous pre-training techniques for the unitary vision or textual feature, or with the brute-force alignment between vision and language information, we leverage the unique characteristic of report information in different granularity to build a hierarchical alignment mechanism, which strengthens the efficiency in feature learn
    
[^33]: 基于深度学习模型的上下文关系提取的比较分析

    Comparative Analysis of Contextual Relation Extraction based on Deep Learning Models. (arXiv:2309.06814v1 [cs.CL])

    [http://arxiv.org/abs/2309.06814](http://arxiv.org/abs/2309.06814)

    本文比较分析了基于深度学习模型的上下文关系提取方法。现有技术无法高效预测由多于两个关系和未指定实体组成的句子中的复杂关系。研究采用深度学习技术从多个句子的语境中识别语义关系。现有机器学习模型在二元关系中表现较好，但随着关系数量的增加，预测准确率降低。

    

    上下文关系提取主要用于借助本体构建知识图谱，在语义搜索、查询回答和文本蕴涵等方面起到重要作用。关系提取识别原始文本中的实体及其之间的关系。在生物医药行业中，高效准确的上下文关系提取系统对于创建领域知识至关重要。现有的机器学习和自然语言处理技术无法高效地从由多于两个关系和未指定实体组成的句子中预测复杂关系。本研究使用深度学习技术，从多个句子的语境中识别出适当的语义关系。尽管关系提取中使用了各种机器学习模型，但它们只对二元关系（即在句子中完全发生在两个实体之间的关系）提供更好的结果。机器学习模型的预测准确率会随着关系的数量增加而降低。

    Contextual Relation Extraction (CRE) is mainly used for constructing a knowledge graph with a help of ontology. It performs various tasks such as semantic search, query answering, and textual entailment. Relation extraction identifies the entities from raw texts and the relations among them. An efficient and accurate CRE system is essential for creating domain knowledge in the biomedical industry. Existing Machine Learning and Natural Language Processing (NLP) techniques are not suitable to predict complex relations from sentences that consist of more than two relations and unspecified entities efficiently. In this work, deep learning techniques have been used to identify the appropriate semantic relation based on the context from multiple sentences. Even though various machine learning models have been used for relation extraction, they provide better results only for binary relations, i.e., relations occurred exactly between the two entities in a sentence. Machine learning models are
    
[^34]: FedDIP: 采用极端动态修剪和增量正则化的联邦学习

    FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization. (arXiv:2309.06805v1 [cs.LG])

    [http://arxiv.org/abs/2309.06805](http://arxiv.org/abs/2309.06805)

    FedDIP是一个结合了动态模型修剪和增量正则化的联邦学习框架，通过消除冗余信息交换和实现极端稀疏模型来显著提高性能。

    

    联邦学习（FL）已成功应用于大规模深度神经网络（DNN）的分布式训练和推理。然而，DNN具有极大的参数数量，因此在分布式节点之间交换这些参数和管理内存方面面临着重大挑战。尽管最近的DNN压缩方法（例如稀疏化、修剪）解决了这些挑战，但它们并未全面考虑在保持高精度水平的同时自适应地控制参数交换的减少。因此，我们提出了一种新颖的FL框架（称为FedDIP），它结合了（i）动态模型修剪和误差反馈来消除冗余信息交换，从而显著提高性能，以及（ii）增量正则化，可以实现“极端”稀疏模型。我们提供了FedDIP的收敛性分析，并对其进行了全面的性能和比较评估。

    Federated Learning (FL) has been successfully adopted for distributed training and inference of large-scale Deep Neural Networks (DNNs). However, DNNs are characterized by an extremely large number of parameters, thus, yielding significant challenges in exchanging these parameters among distributed nodes and managing the memory. Although recent DNN compression methods (e.g., sparsification, pruning) tackle such challenges, they do not holistically consider an adaptively controlled reduction of parameter exchange while maintaining high accuracy levels. We, therefore, contribute with a novel FL framework (coined FedDIP), which combines (i) dynamic model pruning with error feedback to eliminate redundant information exchange, which contributes to significant performance improvement, with (ii) incremental regularization that can achieve \textit{extreme} sparsity of models. We provide convergence analysis of FedDIP and report on a comprehensive performance and comparative assessment against
    
[^35]: 缺失数据下的不确定性交通预测

    Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])

    [http://arxiv.org/abs/2309.06800](http://arxiv.org/abs/2309.06800)

    本研究提出了一种考虑不确定性的交通预测方法，可以处理缺失数据和测量不确定性，并适用于风险敏感任务和决策导向问题。

    

    交通预测是一个重要的课题，因为它在交通领域有广泛的应用。近期，许多研究取得了很好的结果。然而，大多数研究假设预测位置有完整或至少部分的历史记录，不能扩展到无历史记录的位置。在现实场景中，由于预算限制和安装可行性问题，传感器的部署可能受限，这使得大多数当前模型不适用。虽然少数文献尝试在缺失位置上插补交通状态，但这些方法需要与传感器位置同时观测的数据，使它们不适用于预测任务。另一个缺点是缺乏对预测不确定性的测量，使得之前的工作不适用于风险敏感的任务或涉及决策的情况。为了填补这一空白，受到先前的归纳图神经网络的启发，本文提出了一种考虑不确定性的方法。

    Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
    
[^36]: 认知幻觉：大规模语言模型中幻觉现象的综述

    Cognitive Mirage: A Review of Hallucinations in Large Language Models. (arXiv:2309.06794v1 [cs.CL])

    [http://arxiv.org/abs/2309.06794](http://arxiv.org/abs/2309.06794)

    这篇论文综述了大规模语言模型中幻觉的现象，并提出了幻觉的分类、理论分析、检测方法和改进方法，同时还设想了未来的研究方向。

    

    随着人工智能领域中大规模语言模型的发展，文本生成系统容易受到一种令人担忧的现象，即幻觉。在本研究中，我们总结了最近关于大规模语言模型中幻觉的引人注目的见解。我们提出了一种针对各种文本生成任务的幻觉的新分类体系，从而提供了理论性的洞见、检测方法和改进方法。基于此，我们提出了未来的研究方向。我们的贡献有三个方面：（1）我们为出现在文本生成任务中的幻觉提供了详细和完整的分类体系；（2）我们对大规模语言模型中的幻觉进行了理论分析，并提供了现有的检测和改进方法；（3）我们提出了几个未来可以发展的研究方向。由于幻觉受到了学术界的广泛关注，我们将维护与相关研究进展的更新。

    As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.
    
[^37]: 通过自然语言处理和长短期记忆网络进行电力需求预测

    Electricity Demand Forecasting through Natural Language Processing with Long Short-Term Memory Networks. (arXiv:2309.06793v1 [cs.LG])

    [http://arxiv.org/abs/2309.06793](http://arxiv.org/abs/2309.06793)

    本文提出了一种将文本新闻特征纳入长短期记忆（LSTM）网络的方法，成功预测了英国国家电力需求的确定性和概率任务，并发现公众情绪和与交通和地缘政治相关的词向量表示对电力需求具有时间连续性的影响。该模型相对于纯LSTM基准模型提高了超过3%，相对于官方基准模型提高了接近10%，并通过缩小置信区间有效地减少了预测不确定性。

    

    电力需求预测是一个已经建立起来的研究领域。通常情况下，这项任务是通过考虑历史负荷、天气预报、日历信息和已知的重大事件来完成的。最近，人们开始关注如何利用来自文本新闻的新信息来提高这些预测的性能。本文提出了一种将文本新闻特征纳入其中的长短期记忆（LSTM）网络，成功预测了英国国家电力需求的确定性和概率任务。研究发现，公众情绪以及与交通和地缘政治相关的词向量表示对电力需求具有时间连续性的影响。实验结果表明，带有文本特征的LSTM相对于纯LSTM基准模型提高了超过3%，相对于官方基准模型提高了接近10%。此外，所提出的模型通过缩小置信区间有效地减少了预测不确定性。

    Electricity demand forecasting is a well established research field. Usually this task is performed considering historical loads, weather forecasts, calendar information and known major events. Recently attention has been given on the possible use of new sources of information from textual news in order to improve the performance of these predictions. This paper proposes a Long and Short-Term Memory (LSTM) network incorporating textual news features that successfully predicts the deterministic and probabilistic tasks of the UK national electricity demand. The study finds that public sentiment and word vector representations related to transport and geopolitics have time-continuity effects on electricity demand. The experimental results show that the LSTM with textual features improves by more than 3% compared to the pure LSTM benchmark and by close to 10% over the official benchmark. Furthermore, the proposed model effectively reduces forecasting uncertainty by narrowing the confidence
    
[^38]: 可扩展的神经网络模型和千兆级数据集用于粒子流重建

    Scalable neural network models and terascale datasets for particle-flow reconstruction. (arXiv:2309.06782v1 [physics.data-an])

    [http://arxiv.org/abs/2309.06782](http://arxiv.org/abs/2309.06782)

    本研究针对高能电子-正电子碰撞中的粒子流重建，使用可扩展的机器学习模型，并通过超参数调优和硬件处理器的高度可移植性，取得了真实且具有竞争力的物理性能。

    

    本研究针对高能电子-正电子碰撞中基于高度粒度探测器模拟的完整事件重建，研究了可扩展的机器学习模型。粒子流（PF）重建可通过跟踪和量能器团簇或击中来构建监督学习任务。我们比较了图神经网络和基于内核的变换器，并证明两者都避免了二次内存分配和计算成本，同时实现了真实的粒子流重建。我们展示了在超级计算机上进行的超参数调优显著提高了模型的物理性能。我们还展示了所得模型在硬件处理器上具有高度可移植性，支持NVIDIA, AMD和英特尔 Habana卡。最后，我们证明了模型可以在由跟踪和量能器击中组成的高粒度输入上进行训练，从而获得与基准相竞争的物理性能。有关复现研究的数据集和软件已发布。

    We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following 
    
[^39]: 基于Hinge Loss训练的深度学习二分类器的基本限制

    Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss. (arXiv:2309.06774v1 [cs.LG])

    [http://arxiv.org/abs/2309.06774](http://arxiv.org/abs/2309.06774)

    本文揭示了基于Hinge Loss训练的深度学习二分类器的基本测试性能限制。

    

    深度学习在化学、计算机科学、电子工程、数学、医学、神经科学和物理学等多个领域取得了重大突破，但对于为什么和如何获得经验成功的全面理解仍然基本难以把握。为了解决这一根本问题并揭示深度学习背后的奥秘，已经在建立统一理论的方向上取得了重大创新。这些创新包括优化、泛化和近似等基础性进展。然而，迄今为止还没有一个工作提供了一种方法来量化深度学习算法在解决模式分类问题时的测试性能。为了在一定程度上克服这个基本挑战，本文揭示了基于Hinge Loss训练的深度学习二分类器的基本测试性能限制。

    Although deep learning (DL) has led to several breakthroughs in many disciplines as diverse as chemistry, computer science, electrical engineering, mathematics, medicine, neuroscience, and physics, a comprehensive understanding of why and how DL is empirically successful remains fundamentally elusive. To attack this fundamental problem and unravel the mysteries behind DL's empirical successes, significant innovations toward a unified theory of DL have been made. These innovations encompass nearly fundamental advances in optimization, generalization, and approximation. Despite these advances, however, no work to date has offered a way to quantify the testing performance of a DL-based algorithm employed to solve a pattern classification problem. To overcome this fundamental challenge in part, this paper exposes the fundamental testing performance limits of DL-based binary classifiers trained with hinge loss. For binary classifiers that are based on deep rectified linear unit (ReLU) feedf
    
[^40]: MTD: 多时间步检测器用于延迟流式感知

    MTD: Multi-Timestep Detector for Delayed Streaming Perception. (arXiv:2309.06742v1 [cs.CV])

    [http://arxiv.org/abs/2309.06742](http://arxiv.org/abs/2309.06742)

    提出了多时间步检测器（MTD）用于解决自动驾驶系统延迟问题，包括动态路由和延迟分析模块（DAM），以及时间步骤分支模块（TBM）来适应性地预测未来时间步。

    

    自动驾驶系统需要实时环境感知以确保用户安全和体验。流式感知是报告世界当前状态的任务，用于评估自动驾驶系统的延迟和准确性。在实际应用中，硬件限制和高温等因素不可避免地导致自动驾驶系统的延迟，从而导致模型输出与世界状态之间的偏差。为了解决这个问题，本文提出了多时间步检测器（MTD），它是一个端到端的检测器，使用动态路由进行多分支未来预测，使模型具有抵抗延迟波动的能力。提出了一个延迟分析模块（DAM），优化现有的延迟感知方法，不断监测模型推断堆栈并计算延迟趋势。此外，构建了一个新颖的时间步骤分支模块（TBM），包括静态流和自适应流，以自适应地预测未来时间步。

    Autonomous driving systems require real-time environmental perception to ensure user safety and experience. Streaming perception is a task of reporting the current state of the world, which is used to evaluate the delay and accuracy of autonomous driving systems. In real-world applications, factors such as hardware limitations and high temperatures inevitably cause delays in autonomous driving systems, resulting in the offset between the model output and the world state. In order to solve this problem, this paper propose the Multi- Timestep Detector (MTD), an end-to-end detector which uses dynamic routing for multi-branch future prediction, giving model the ability to resist delay fluctuations. A Delay Analysis Module (DAM) is proposed to optimize the existing delay sensing method, continuously monitoring the model inference stack and calculating the delay trend. Moreover, a novel Timestep Branch Module (TBM) is constructed, which includes static flow and adaptive flow to adaptively pr
    
[^41]: MCNS: 通过一种新的内部因果方案挖掘时间序列中的因果自然结构

    MCNS: Mining Causal Natural Structures Inside Time Series via A Novel Internal Causality Scheme. (arXiv:2309.06739v1 [cs.LG])

    [http://arxiv.org/abs/2309.06739](http://arxiv.org/abs/2309.06739)

    本文提出了一种名为MCNS的自动领域无关框架，通过内部因果关系方案帮助挖掘时间序列中的因果自然结构。

    

    因果推断使我们能够发现时间序列中各个变量之间的隐藏关系。然而，在大部分现有研究中，提到的变量是维度，维度之间的因果关系可能是肤浅的，这阻碍了对内部关系的理解以及因果图对神经网络的好处。本文发现，因果关系不仅存在于时间序列之外，也存在于时间序列之内，因为这反映了现实世界中一系列事件的顺序。这启发我们寻找内部子序列之间的关系。然而，挑战在于从子序列中发现因果关系，并利用因果自然结构改进神经网络的困难。为了解决这些挑战，我们提出了一种名为MCNS的新框架，它是自动的、领域无关的，并通过内部因果方案帮助发现时间序列中的因果自然结构。

    Causal inference permits us to discover covert relationships of various variables in time series. However, in most existing works, the variables mentioned above are the dimensions. The causality between dimensions could be cursory, which hinders the comprehension of the internal relationship and the benefit of the causal graph to the neural networks (NNs). In this paper, we find that causality exists not only outside but also inside the time series because it reflects a succession of events in the real world. It inspires us to seek the relationship between internal subsequences. However, the challenges are the hardship of discovering causality from subsequences and utilizing the causal natural structures to improve NNs. To address these challenges, we propose a novel framework called Mining Causal Natural Structure (MCNS), which is automatic and domain-agnostic and helps to find the causal natural structures inside time series via the internal causality scheme. We evaluate the MCNS fra
    
[^42]: 深度非参数凸化滤波在计算摄影，图像合成和对抗性防御中的应用

    Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense. (arXiv:2309.06724v1 [cs.CV])

    [http://arxiv.org/abs/2309.06724](http://arxiv.org/abs/2309.06724)

    该论文提出了一种使用深度非参数凸化滤波（DNCF）的通用框架，用于计算摄影领域中的图像恢复。DNCF具有强大的泛化性和对抗性图像处理的鲁棒性，同时能够实现实时的对抗性图像分类网络防御。

    

    我们旨在提供一个通用框架，用于从不完美的图像中恢复真实场景的计算摄影，通过深度非参数凸化滤波（DNCF）。它由一个非参数深度网络组成，以模拟图像形成背后的物理方程，如降噪、超分辨率、修复和闪光。DNCF没有依赖于训练数据的参数化，因此具有强大的泛化性和对抗性图像处理的鲁棒性。在推理过程中，我们还鼓励网络参数为非负，并在输入和参数上创建一个双凸函数，这适应于运行时间不足的二阶优化算法，相对于Deep Image Prior有10倍的加速。通过这些工具，我们在实时中实验证明了其对抗图像分类深度网络攻击算法的能力。

    We aim to provide a general framework of for computational photography that recovers the real scene from imperfect images, via the Deep Nonparametric Convexified Filtering (DNCF). It is consists of a nonparametric deep network to resemble the physical equations behind the image formation, such as denoising, super-resolution, inpainting, and flash. DNCF has no parameterization dependent on training data, therefore has a strong generalization and robustness to adversarial image manipulation. During inference, we also encourage the network parameters to be nonnegative and create a bi-convex function on the input and parameters, and this adapts to second-order optimization algorithms with insufficient running time, having 10X acceleration over Deep Image Prior. With these tools, we empirically verify its capability to defend image classification deep networks against adversary attack algorithms in real-time.
    
[^43]: 偏见放大增强了少数群体的表现

    Bias Amplification Enhances Minority Group Performance. (arXiv:2309.06717v1 [cs.LG])

    [http://arxiv.org/abs/2309.06717](http://arxiv.org/abs/2309.06717)

    本论文提出了一种名为BAM的两阶段训练算法，通过引入可学习的辅助变量来放大偏见，提高了少数群体的表现。

    

    由标准训练产生的神经网络在罕见的子群上的准确性较差，尽管在平均水平上取得了较高准确性，这是由于某些虚假特征与标签之间的关联。之前基于最差群体损失最小化的方法（例如Group-DRO）在改善最差群体准确性方面是有效的，但需要为所有训练样本提供昂贵的群体注释。在本文中，我们关注更具挑战性和现实性的情景，即群体注释仅在一个小的验证集上可用，或者根本不可用。我们提出了BAM，一种新的两阶段训练算法：在第一阶段，通过引入可学习的辅助变量为每个训练样本训练一个偏见放大方案的模型；在第二阶段，我们对偏见放大的模型误分类的样本进行加权，然后在重新加权的数据集上继续训练同一模型。实验证明，BAM相对于其他方法在性能上取得了竞争性的表现。

    Neural networks produced by standard training are known to suffer from poor accuracy on rare subgroups despite achieving high accuracy on average, due to the correlations between certain spurious features and labels. Previous approaches based on worst-group loss minimization (e.g. Group-DRO) are effective in improving worse-group accuracy but require expensive group annotations for all the training samples. In this paper, we focus on the more challenging and realistic setting where group annotations are only available on a small validation set or are not available at all. We propose BAM, a novel two-stage training algorithm: in the first stage, the model is trained using a bias amplification scheme via introducing a learnable auxiliary variable for each training sample; in the second stage, we upweight the samples that the bias-amplified model misclassifies, and then continue training the same model on the reweighted dataset. Empirically, BAM achieves competitive performance compared w
    
[^44]: 使用神经网络势和年龄适应Pareto遗传算法进行晶体结构预测

    Crystal structure prediction using neural network potential and age-fitness Pareto genetic algorithm. (arXiv:2309.06710v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.06710](http://arxiv.org/abs/2309.06710)

    这篇论文介绍了一种使用神经网络势和年龄适应Pareto遗传算法进行晶体结构预测的新算法，该算法在能够找到能量最优的晶体结构方面表现出比其他算法更好的预测能力和搜索效果。

    

    尽管晶体结构预测是一个长期存在的挑战，我们介绍了ParetoCSP，这是一种用于晶体结构预测的新算法，它将多目标遗传算法（MOGA）与神经网络间原子势模型（IAP）结合起来，以找到给定化学成分的能量最优的晶体结构。我们通过将基因型年龄作为独立优化标准来增强NSGA-III算法，并采用M3GNet通用IAP来引导遗传算法的搜索。与GN-OA，一种基于神经势的晶体结构预测算法相比，ParetoCSP在55个不同的基准结构上通过七个性能指标的评估表明，具有显著更好的预测能力，优势因子为2.562。所有算法所遍历结构的轨迹分析表明，ParetoCSP产生了更多的有效结构，这有助于指导遗传算法更有效地搜索最优结构。

    While crystal structure prediction (CSP) remains a longstanding challenge, we introduce ParetoCSP, a novel algorithm for CSP, which combines a multi-objective genetic algorithm (MOGA) with a neural network inter-atomic potential (IAP) model to find energetically optimal crystal structures given chemical compositions. We enhance the NSGA-III algorithm by incorporating the genotypic age as an independent optimization criterion and employ the M3GNet universal IAP to guide the GA search. Compared to GN-OA, a state-of-the-art neural potential based CSP algorithm, ParetoCSP demonstrated significantly better predictive capabilities, outperforming by a factor of $2.562$ across $55$ diverse benchmark structures, as evaluated by seven performance metrics. Trajectory analysis of the traversed structures of all algorithms shows that ParetoCSP generated more valid structures than other algorithms, which helped guide the GA to search more effectively for the optimal structures
    
[^45]: 通过路径切片和重新加权预测疲劳裂纹扩展

    Predicting Fatigue Crack Growth via Path Slicing and Re-Weighting. (arXiv:2309.06708v1 [cs.LG])

    [http://arxiv.org/abs/2309.06708](http://arxiv.org/abs/2309.06708)

    通过路径切片和重新加权的统计学习框架，可以预测带有不确定性的加载条件下的疲劳裂纹增长和组件的失效寿命。通过构建数字库并引入路径切片和重新加权技术，该方法可以有效处理疲劳复杂性和统计噪声。

    

    在工程设计中，预测与关键结构组件疲劳相关的潜在风险至关重要。然而，疲劳常常涉及材料微观结构和使用条件的复杂性，使疲劳损伤的诊断和预测具有挑战性。我们报道了一个统计学习框架，用于预测在带有不确定性的加载条件下疲劳裂纹的增长和组件的失效寿命。通过高保真物理模拟构建了疲劳裂纹模式和剩余寿命的数字库。然后使用降维和神经网络架构来学习疲劳裂纹增长的历史依赖性和非线性。引入了路径切片和重新加权技术来处理统计噪声和罕见事件。预测的疲劳裂纹模式通过不断演化的裂纹模式进行自更新和自校正。代表性实例验证了端到端方法的有效性。

    Predicting potential risks associated with the fatigue of key structural components is crucial in engineering design. However, fatigue often involves entangled complexities of material microstructures and service conditions, making diagnosis and prognosis of fatigue damage challenging. We report a statistical learning framework to predict the growth of fatigue cracks and the life-to-failure of the components under loading conditions with uncertainties. Digital libraries of fatigue crack patterns and the remaining life are constructed by high-fidelity physical simulations. Dimensionality reduction and neural network architectures are then used to learn the history dependence and nonlinearity of fatigue crack growth. Path-slicing and re-weighting techniques are introduced to handle the statistical noises and rare events. The predicted fatigue crack patterns are self-updated and self-corrected by the evolving crack patterns. The end-to-end approach is validated by representative examples 
    
[^46]: VLSlice：交互式视觉和语言切片发现

    VLSlice: Interactive Vision-and-Language Slice Discovery. (arXiv:2309.06703v1 [cs.CV])

    [http://arxiv.org/abs/2309.06703](http://arxiv.org/abs/2309.06703)

    这项工作提出了一种交互式系统VLSlice，可以通过用户引导发现一致的视觉和语言行为的表示级子组，以解决自动发现子组时的困难。

    

    最近的视觉和语言研究表明，大规模预训练可以学习出具有通用性的模型，可以有效地迁移到下游任务。尽管这可能改善数据集规模的聚合指标，但通过分析针对特定偏差维度的手工子组时，发现了系统性的不良行为。然而，这种子组分析通常会因为注释工作而停滞，而收集所需数据需要大量的时间和资源。先前的方法尝试自动发现子组以规避这些限制，但通常利用现有任务特定注释上的模型行为，并在超出“表格”数据的更复杂输入上快速降级，其中没有研究视觉和语言模型。本文介绍了VLSlice，一种交互式系统，可以通过用户引导发现一致的表示级子组，具有一致的视觉语言行为，被称为视觉和语言切片，从未标记的图像中获取。

    Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond "tabular" data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled ima
    
[^47]: 解决异构联邦学习中非独立同分布问题的梯度协调方法

    Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization. (arXiv:2309.06692v1 [cs.LG])

    [http://arxiv.org/abs/2309.06692](http://arxiv.org/abs/2309.06692)

    本研究通过梯度协调方法解决了异构联邦学习中的非独立同分布问题，提出了FedGH，通过减轻本地漂移来增强性能。实验证明，在多个基准和非独立同分布场景下，FedGH始终能够显著提升联邦学习的性能。

    

    联邦学习是一种保护隐私的范式，用于从分散的客户端协作训练全局模型。然而，联邦学习的性能受到非独立同分布的数据和设备异构性的影响。在本研究中，我们通过服务器端的梯度冲突视角重新思考这个关键挑战。具体而言，我们首先调查了多个客户端之间的梯度冲突现象，并揭示了更强的异构性会导致更严重的梯度冲突。为了解决这个问题，我们提出了FedGH，一种简单而有效的方法，通过梯度协调来减轻本地漂移。这种技术将一个梯度向量投影到与其他冲突客户端对之间的正交平面上。广泛的实验表明，FedGH在不同基准和非独立同分布场景下始终能够显著提升多个最先进的联邦学习基线。值得注意的是，FedGH在特定场景中取得了更显著的改进。

    Federated learning (FL) is a privacy-preserving paradigm for collaboratively training a global model from decentralized clients. However, the performance of FL is hindered by non-independent and identically distributed (non-IID) data and device heterogeneity. In this work, we revisit this key challenge through the lens of gradient conflicts on the server side. Specifically, we first investigate the gradient conflict phenomenon among multiple clients and reveal that stronger heterogeneity leads to more severe gradient conflicts. To tackle this issue, we propose FedGH, a simple yet effective method that mitigates local drifts through Gradient Harmonization. This technique projects one gradient vector onto the orthogonal plane of the other within conflicting client pairs. Extensive experiments demonstrate that FedGH consistently enhances multiple state-of-the-art FL baselines across diverse benchmarks and non-IID scenarios. Notably, FedGH yields more significant improvements in scenarios 
    
[^48]: 改进的Attention Loss Adjusted Prioritized Experience Replay算法

    Attention Loss Adjusted Prioritized Experience Replay. (arXiv:2309.06684v1 [cs.LG])

    [http://arxiv.org/abs/2309.06684](http://arxiv.org/abs/2309.06684)

    本文提出了一种改进的Attention Loss Adjusted Prioritized Experience Replay (ALAP)算法，通过结合改进的自注意力网络和双采样机制，调节重要性采样权重，消除了先进的经验回放算法中的估计误差。在OPENAI gym环境中的测试和对比研究验证了该算法的优势和效率。

    

    先进的经验回放算法(Prioritized Experience Replay, PER)通过选择具有更多知识量的经验样本来改善神经网络的训练速度。然而，PER中使用的非均匀采样不可避免地使状态-动作空间分布偏移，并带来Q值函数的估计误差。本文提出了一种Attention Loss Adjusted Prioritized (ALAP) Experience Replay算法，该算法将改进的自注意力网络和双采样机制结合起来，以适应能够调节重要性采样权重的超参数，从而消除因PER引起的估计误差。为了验证该算法的有效性和通用性，我们在OPENAI gym环境中对基于值函数、基于策略梯度和多主体强化学习算法进行了测试，并进行了对比研究，验证了所提出的训练框架的优势和效率。

    Prioritized Experience Replay (PER) is a technical means of deep reinforcement learning by selecting experience samples with more knowledge quantity to improve the training rate of neural network. However, the non-uniform sampling used in PER inevitably shifts the state-action space distribution and brings the estimation error of Q-value function. In this paper, an Attention Loss Adjusted Prioritized (ALAP) Experience Replay algorithm is proposed, which integrates the improved Self-Attention network with Double-Sampling mechanism to fit the hyperparameter that can regulate the importance sampling weights to eliminate the estimation error caused by PER. In order to verify the effectiveness and generality of the algorithm, the ALAP is tested with value-function based, policy-gradient based and multi-agent reinforcement learning algorithms in OPENAI gym, and comparison studies verify the advantage and efficiency of the proposed training framework.
    
[^49]: 基于非独立同分布数据的联邦PAC-Bayesian学习

    Federated PAC-Bayesian Learning on Non-IID data. (arXiv:2309.06683v1 [cs.LG])

    [http://arxiv.org/abs/2309.06683](http://arxiv.org/abs/2309.06683)

    本文提出了一种非独立同分布的联邦PAC-Bayesian界限，通过考虑每个客户端的唯一先验知识和可变聚合权重，以及引入目标函数和创新的基于Gibbs的算法，解决了联邦学习中的非独立同分布挑战。

    

    现有的研究要么将Probably Approximately Correct（PAC）贝叶斯框架应用于联邦学习（FL），要么在引入定理时使用信息论的PAC-Bayesian界限，但很少考虑FL中的非独立同分布挑战。我们的工作提出了第一个适用于非独立同分布本地数据的真实联邦PAC-Bayesian界限。该界限假设每个客户端具有唯一的先验知识和可变聚合权重。我们还引入了一种目标函数和一种创新的基于Gibbs的算法来优化得到的界限。结果在真实世界数据集上进行了验证。

    Existing research has either adapted the Probably Approximately Correct (PAC) Bayesian framework for federated learning (FL) or used information-theoretic PAC-Bayesian bounds while introducing their theorems, but few considering the non-IID challenges in FL. Our work presents the first non-vacuous federated PAC-Bayesian bound tailored for non-IID local data. This bound assumes unique prior knowledge for each client and variable aggregation weights. We also introduce an objective function and an innovative Gibbs-based algorithm for the optimization of the derived bound. The results are validated on real-world datasets.
    
[^50]: 通过实验数据同化实现对Spalart-Allmaras模型的可普适改进

    Generalizable improvement of the Spalart-Allmaras model through assimilation of experimental data. (arXiv:2309.06679v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2309.06679](http://arxiv.org/abs/2309.06679)

    本研究通过实验数据同化改进了Spalart-Allmaras模型，实现了对分离流体的雷诺平均纳维-斯托克斯解的泛化，提高了计算模型的性能。

    

    本研究旨在利用模型和数据融合改进分离流体的雷诺平均纳维-斯托克斯解的Spalart-Allmaras（SA）闭合模型。特别是，我们的目标是开发模型，不仅能将稀疏的实验数据同化以改善计算模型的性能，还能通过恢复经典的SA行为来推广到未见过的情况。我们使用数据同化，即集合卡尔曼滤波方法（EnKF），通过将SA模型的系数校准到分离流体中来实现我们的目标。通过参数化产生、扩散和破坏项，实现了一种全面的校准策略。该校准依赖于采集的分离流体速度剖面、壁擦力和压力系数的实验数据的同化。尽管仅使用了来自单一流动条件（环绕一个背面台阶）的观测数据，但重新校准的SA模型表现出泛化能力。

    This study focuses on the use of model and data fusion for improving the Spalart-Allmaras (SA) closure model for Reynolds-averaged Navier-Stokes solutions of separated flows. In particular, our goal is to develop of models that not-only assimilate sparse experimental data to improve performance in computational models, but also generalize to unseen cases by recovering classical SA behavior. We achieve our goals using data assimilation, namely the Ensemble Kalman Filtering approach (EnKF), to calibrate the coefficients of the SA model for separated flows. A holistic calibration strategy is implemented via a parameterization of the production, diffusion, and destruction terms. This calibration relies on the assimilation of experimental data collected velocity profiles, skin friction, and pressure coefficients for separated flows. Despite using of observational data from a single flow condition around a backward-facing step (BFS), the recalibrated SA model demonstrates generalization to o
    
[^51]: 基于两阶段神经网络的声场分解

    Sound field decomposition based on two-stage neural networks. (arXiv:2309.06661v1 [eess.AS])

    [http://arxiv.org/abs/2309.06661](http://arxiv.org/abs/2309.06661)

    该论文提出了一种基于两阶段神经网络的声场分解方法，通过第一阶段将合成的麦克风处的声压分离成每个声源激发的声压，然后通过第二阶段从单一声源处的声压回归得到源位置，实现了更高的源定位精度和声场重构精度。

    

    提出了一种基于神经网络的声场分解方法。该方法包括两个阶段：声场分离阶段和单源定位阶段。在第一阶段，由多个声源合成的麦克风处的声压被分离成每个声源激发的声压。在第二阶段，通过由单一声源组成的麦克风处的声压进行回归，获得源位置。由于第二阶段设计为回归而不是分类，因此估计的位置不受离散化的影响。使用Green函数进行模拟生成数据集，并针对每个频率训练神经网络。数值实验表明，与传统方法相比，所提出的方法可以实现更高的源定位精度和声场重构精度。

    A method for sound field decomposition based on neural networks is proposed. The method comprises two stages: a sound field separation stage and a single-source localization stage. In the first stage, the sound pressure at microphones synthesized by multiple sources is separated into one excited by each sound source. In the second stage, the source location is obtained as a regression from the sound pressure at microphones consisting of a single sound source. The estimated location is not affected by discretization because the second stage is designed as a regression rather than a classification. Datasets are generated by simulation using Green's function, and the neural network is trained for each frequency. Numerical experiments reveal that, compared with conventional methods, the proposed method can achieve higher source-localization accuracy and higher sound-field-reconstruction accuracy.
    
[^52]: 通用的神经场作为部分观测的神经过程

    Generalizable Neural Fields as Partially Observed Neural Processes. (arXiv:2309.06660v1 [cs.LG])

    [http://arxiv.org/abs/2309.06660](http://arxiv.org/abs/2309.06660)

    本研究提出了一种新的范式，将大规模的神经表示训练视为部分观测的神经过程框架的一部分，并利用神经过程算法进行优化。

    

    神经场是使用神经网络参数化的函数来表示信号的一种有希望替代传统离散向量或基于网格的表示方法。与离散表示相比，神经表示不仅能够很好地随着分辨率的增加而扩展，而且是连续的，并且可以进行多次可微分。然而，对于我们想要表示的信号数据集来说，必须对每个信号优化一个单独的神经场是低效的，而且不能利用共享信息或信号之间的结构。现有的泛化方法将这视为元学习问题，并采用基于梯度的元学习来学习初始化值，然后通过测试时优化来微调，或者学习超网络来产生神经场的权重。相比之下，我们提出了一种新的范式，将大规模的神经表示训练视为部分观测的神经过程框架的一部分，并利用神经过程算法进行优化。

    Neural fields, which represent signals as a function parameterized by a neural network, are a promising alternative to traditional discrete vector or grid-based representations. Compared to discrete representations, neural representations both scale well with increasing resolution, are continuous, and can be many-times differentiable. However, given a dataset of signals that we would like to represent, having to optimize a separate neural field for each signal is inefficient, and cannot capitalize on shared information or structures among signals. Existing generalization methods view this as a meta-learning problem and employ gradient-based meta-learning to learn an initialization which is then fine-tuned with test-time optimization, or learn hypernetworks to produce the weights of a neural field. We instead propose a new paradigm that views the large-scale training of neural representations as a part of a partially-observed neural process framework, and leverage neural process algorit
    
[^53]: 带稀疏数据集的离散动态输出反馈控制的耗散型仿真学习

    Dissipative Imitation Learning for Discrete Dynamic Output Feedback Control with Sparse Data Sets. (arXiv:2309.06658v1 [eess.SY])

    [http://arxiv.org/abs/2309.06658](http://arxiv.org/abs/2309.06658)

    这项研究探索了一种用于稀疏数据集和对植物模型了解有限的情况下实现稳定性的耗散型仿真学习方法，并成功应用于两个未知的植物。

    

    仿真学习使得可以为复杂目标和高度不确定的植物模型合成控制器。然而，为了给仿真学习的控制器提供稳定性保证，通常需要大量的数据和/或已知的植物模型。本文中，我们探索了一种用于耗散型仿真学习的输入-输出（IO）稳定性方法，该方法在稀疏数据集和对植物模型了解有限的情况下实现稳定性。使用专家数据、粗糙的IO植物模型和新的约束来强制学习到的控制器具有耗散性，从而学习到了一个闭环稳定的动态输出反馈控制器。虽然学习目标是非凸的，但本文探索了迭代凸过估计（ICO）和投影梯度下降（PGD）作为成功学习控制器的方法。将这种新的仿真学习方法应用于两个未知的植物，并与传统学习的动态输出反馈控制器和神经网络控制器进行了比较。

    Imitation learning enables the synthesis of controllers for complex objectives and highly uncertain plant models. However, methods to provide stability guarantees to imitation learned controllers often rely on large amounts of data and/or known plant models. In this paper, we explore an input-output (IO) stability approach to dissipative imitation learning, which achieves stability with sparse data sets and with little known about the plant model. A closed-loop stable dynamic output feedback controller is learned using expert data, a coarse IO plant model, and a new constraint to enforce dissipativity on the learned controller. While the learning objective is nonconvex, iterative convex overbounding (ICO) and projected gradient descent (PGD) are explored as methods to successfully learn the controller. This new imitation learning method is applied to two unknown plants and compared to traditionally learned dynamic output feedback controller and neural network controller. With little kn
    
[^54]: 通过基于领域信息的高斯过程状态空间模型实现未知领域检测

    Out of Distribution Detection via Domain-Informed Gaussian Process State Space Models. (arXiv:2309.06655v1 [cs.RO])

    [http://arxiv.org/abs/2309.06655](http://arxiv.org/abs/2309.06655)

    本文提出了一种通过嵌入领域知识的高斯过程状态空间模型，以实现未知领域的检测，并基于递归预测构建了在线的监视器。数值结果表明，带有领域信息的内核可以提供更好的回归质量。

    

    为了使机器人能够使用基于学习的方法在未知情景中安全导航，准确地在线检测训练集之外的情况是非常重要的。近年来，高斯过程状态空间模型（GPSSM）已被证明是一种有效的方法，通过将不确定的观测与概率预测进行比较来区分它们。然而，模型能够正确区分训练集内外的观测取决于预测的准确性，主要受到GPSSM内核所能表示的函数类的影响。本文提出了一种新颖的方法，将现有领域知识嵌入内核中，并基于递归地预测构建了一个在线运行时的未知领域监视器。领域知识假设以在模拟中收集的数据集或使用名义模型。数值结果表明，对于较小的数据集，带有领域信息的内核提供了更好的回归质量，与标准内核相比。

    In order for robots to safely navigate in unseen scenarios using learning-based methods, it is important to accurately detect out-of-training-distribution (OoD) situations online. Recently, Gaussian process state-space models (GPSSMs) have proven useful to discriminate unexpected observations by comparing them against probabilistic predictions. However, the capability for the model to correctly distinguish between in- and out-of-training distribution observations hinges on the accuracy of these predictions, primarily affected by the class of functions the GPSSM kernel can represent. In this paper, we propose (i) a novel approach to embed existing domain knowledge in the kernel and (ii) an OoD online runtime monitor, based on receding-horizon predictions. Domain knowledge is assumed given as a dataset collected either in simulation or using a nominal model. Numerical results show that the informed kernel yields better regression quality with smaller datasets, as compared to standard ker
    
[^55]: ConR: 用于深度不平衡回归的对比正则化器

    ConR: Contrastive Regularizer for Deep Imbalanced Regression. (arXiv:2309.06651v1 [cs.LG])

    [http://arxiv.org/abs/2309.06651](http://arxiv.org/abs/2309.06651)

    ConR是一种对比正则化器，通过建模全局和局部标签相似性，防止少数样本的特征被折叠到其多数邻居中，有效地处理深度不平衡回归问题。

    

    不平衡分布在现实世界的数据中很常见。它们对深度神经网络提出了约束，以表示少数类别标签并避免对多数类别的偏见。大量的不平衡方法处理了分类标签空间，但在连续标签空间的回归问题上未能有效应用。相反，连续标签之间的局部和全局关联为在特征空间中有效建模关系提供了有价值的见解。在这项工作中，我们提出了ConR，一种对比正则化器，它在特征空间中建模全局和局部标签相似性，防止少数样本的特征被折叠到它们的多数邻居中。通过将预测的相似性作为特征相似性的指示器，ConR区分了标签空间和特征空间之间的不一致，并对这些不一致施加惩罚。ConR通过两个主要策略关注标签空间的连续性。

    Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to regression problems where the label space is continuous. Conversely, local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. Serving the similarities of the predictions as an indicator of feature similarities, ConR discerns the dissagreements between the label space and feature space and imposes a penalty on these disagreements. ConR minds the continuous nature of label space with two main strategi
    
[^56]: Bregman图神经网络

    Bregman Graph Neural Network. (arXiv:2309.06645v1 [cs.LG])

    [http://arxiv.org/abs/2309.06645](http://arxiv.org/abs/2309.06645)

    Bregman GNNs提出了一种受Bregman距离概念启发的双层优化框架，能够有效缓解过度平滑问题并在同构图和异构图中性能优于原始的GNNs。

    

    近期关于图神经网络（GNNs）的众多研究主要集中在将GNN架构建模为具有平滑假设的优化问题上。然而，在节点分类任务中，GNNs引起的平滑效果往往会使连接节点的表示和标签过于同质化，导致过度平滑和错误分类等不利影响。在本文中，我们提出了一种受Bregman距离概念启发的GNNs双层优化框架。我们展示了相应提出的GNNs层可以通过引入类似“跳跃连接”的机制有效缓解过度平滑问题。通过全面的实证研究，我们验证了我们的理论结果，在同构图和异构图中，Bregman增强的GNNs在性能上优于原始的GNNs。此外，我们的实验还显示出，即使层数较多，Bregman GNNs也能产生更稳健的学习准确度。

    Numerous recent research on graph neural networks (GNNs) has focused on formulating GNN architectures as an optimization problem with the smoothness assumption. However, in node classification tasks, the smoothing effect induced by GNNs tends to assimilate representations and over-homogenize labels of connected nodes, leading to adverse effects such as over-smoothing and misclassification. In this paper, we propose a novel bilevel optimization framework for GNNs inspired by the notion of Bregman distance. We demonstrate that the GNN layer proposed accordingly can effectively mitigate the over-smoothing issue by introducing a mechanism reminiscent of the "skip connection". We validate our theoretical results through comprehensive empirical studies in which Bregman-enhanced GNNs outperform their original counterparts in both homophilic and heterophilic graphs. Furthermore, our experiments also show that Bregman GNNs can produce more robust learning accuracy even when the number of layers
    
[^57]: 自适应扩散：通过潜在扩散模型进行样本自适应重建

    Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models. (arXiv:2309.06642v1 [eess.IV])

    [http://arxiv.org/abs/2309.06642](http://arxiv.org/abs/2309.06642)

    本文提出了一种自适应扩散方法，通过潜在扩散模型实现样本自适应重建。该方法解决了现有求解器在适应重建任务困难程度、推理时间和资源分配方面的不足。

    

    反问题在许多应用中出现，其目标是从嘈杂和可能是（非）线性的观测中恢复出一个干净的信号。重建问题的困难取决于多个因素，如原始信号的结构，退化的严重程度，重建模型的隐式偏差以及上述因素之间复杂的交互。这导致重建任务的困难在样本间存在自然的变化，这在现代技术中经常被忽视。最近，基于扩散的反问题求解器在各种重建任务中取得了新的最先进水平。然而，它们的缺点是计算复杂，难以实施。本文的关键观察是大多数现有求解器缺乏根据重建任务的困难程度自适应计算能力的能力，导致推理时间长，性能不佳且资源分配浪费。我们提出了一个新的自适应扩散方法来解决这个问题。

    Inverse problems arise in a multitude of applications, where the goal is to recover a clean signal from noisy and possibly (non)linear observations. The difficulty of a reconstruction problem depends on multiple factors, such as the structure of the ground truth signal, the severity of the degradation, the implicit bias of the reconstruction model and the complex interactions between the above factors. This results in natural sample-by-sample variation in the difficulty of a reconstruction task, which is often overlooked by contemporary techniques. Recently, diffusion-based inverse problem solvers have established new state-of-the-art in various reconstruction tasks. However, they have the drawback of being computationally prohibitive. Our key observation in this paper is that most existing solvers lack the ability to adapt their compute power to the difficulty of the reconstruction task, resulting in long inference times, subpar performance and wasteful resource allocation. We propose
    
[^58]: 量子数据中心: 展望

    Quantum Data Center: Perspectives. (arXiv:2309.06641v1 [quant-ph])

    [http://arxiv.org/abs/2309.06641](http://arxiv.org/abs/2309.06641)

    本文提出了量子数据中心(QDC)的概念，它是现有经典数据中心的量子版本，通过结合量子随机访问存储器(QRAM)和量子网络，QDC可以提供客户在效率、安全性和精度方面的显著优势，对于量子计算、通信和传感领域具有重要意义。该研究探讨了硬件实现和特定应用方面的潜在科学和商业机会，并展示了QDC在机器学习和大数据行业等领域的潜在影响。

    

    量子版本的数据中心可能在量子时代具有重要意义。本文介绍了量子数据中心(QDC)，这是现有经典数据中心的量子版本，特别强调了量子随机访问存储器(QRAM)和量子网络的结合。我们认为QDC将为客户提供在效率、安全性和精度方面的显著好处，并对量子计算、通信和传感方面具有帮助。通过硬件实现和可能的特定应用，我们研究了这一新颖研究方向的潜在科学和商业机会。我们展示了QDC在商业和科学领域的潜在影响，尤其是机器学习和大数据行业。

    A quantum version of data centers might be significant in the quantum era. In this paper, we introduce Quantum Data Center (QDC), a quantum version of existing classical data centers, with a specific emphasis on combining Quantum Random Access Memory (QRAM) and quantum networks. We argue that QDC will provide significant benefits to customers in terms of efficiency, security, and precision, and will be helpful for quantum computing, communication, and sensing. We investigate potential scientific and business opportunities along this novel research direction through hardware realization and possible specific applications. We show the possible impacts of QDCs in business and science, especially the machine learning and big data industries.
    
[^59]: $G$-Mapper：学习Mapper构造中的覆盖

    $G$-Mapper: Learning a Cover in the Mapper Construction. (arXiv:2309.06634v1 [cs.LG])

    [http://arxiv.org/abs/2309.06634](http://arxiv.org/abs/2309.06634)

    本论文介绍了一种基于统计检验和聚类算法的优化Mapper图覆盖的方法，通过分割覆盖选择生成了保留数据集本质的Mapper图。

    

    Mapper算法是拓扑数据分析(TDA)中一种反映给定数据集结构的可视化技术。Mapper算法需要调整多个参数以生成一个"好看的"Mapper图。该论文关注于选择覆盖参数。我们提出了一种通过根据正态性的统计检验反复分割覆盖来优化Mapper图的算法。我们的算法基于$G$-means聚类，通过迭代地进行Anderson-Darling检验来寻找$k$-means中最佳的簇数。我们的分割过程利用高斯混合模型，根据给定数据的分布精心选择覆盖。对于合成和真实数据集的实验表明，我们的算法生成的覆盖使Mapper图保留了数据集的本质。

    The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. The Mapper algorithm requires tuning several parameters in order to generate a "nice" Mapper graph. The paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on $G$-means clustering which searches for the optimal number of clusters in $k$-means by conducting iteratively the Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model in order to choose carefully the cover based on the distribution of a given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets.
    
[^60]: 快速神经网络集成的认知建模不确定性用于自适应学习

    Epistemic Modeling Uncertainty of Rapid Neural Network Ensembles for Adaptive Learning. (arXiv:2309.06628v1 [cs.LG])

    [http://arxiv.org/abs/2309.06628](http://arxiv.org/abs/2309.06628)

    本研究提出了一种利用快速神经网络进行认知建模的方法，用于评估航空航天工程系统设计探索中的不确定性。该方法克服了训练集成模型的成本和计算挑战，为自适应学习提供了重要信息。

    

    嵌入解析器的神经网络是一种利用多种真实性数据源进行航空航天工程系统高效设计探索的神经网络类型。使用不同的随机初始化训练多个神经网络模型实现。模型实现的集成用于评估由于缺乏训练样本而引起的认知建模的不确定性。这种不确定性估计是航空航天系统设计探索中成功目标导向自适应学习的关键信息。然而，训练集成模型的成本通常变得难以承受，并且在自适应学习期间，特别是当模型不是并行训练时，会带来计算挑战。在这项工作中，提出了一种使用快速神经网络范式的新型嵌入解析器的神经网络。与传统的神经网络训练不同，该方法通过使用基于梯度的方法仅优化所有网络层的权重和偏置。

    Emulator embedded neural networks, which are a type of physics informed neural network, leverage multi-fidelity data sources for efficient design exploration of aerospace engineering systems. Multiple realizations of the neural network models are trained with different random initializations. The ensemble of model realizations is used to assess epistemic modeling uncertainty caused due to lack of training samples. This uncertainty estimation is crucial information for successful goal-oriented adaptive learning in an aerospace system design exploration. However, the costs of training the ensemble models often become prohibitive and pose a computational challenge, especially when the models are not trained in parallel during adaptive learning. In this work, a new type of emulator embedded neural network is presented using the rapid neural network paradigm. Unlike the conventional neural network training that optimizes the weights and biases of all the network layers by using gradient-bas
    
[^61]: 多个敏感属性的顺序公平机制

    A Sequentially Fair Mechanism for Multiple Sensitive Attributes. (arXiv:2309.06627v1 [stat.ML])

    [http://arxiv.org/abs/2309.06627](http://arxiv.org/abs/2309.06627)

    本论文提出了一个顺序框架来逐步实现对多个敏感特征的公平性，通过利用多边际Wasserstein重心扩展了标准的强人口平等概念，并提供了闭式解来解释敏感特征之间的相关性。

    

    在算法公平性的标准用例中，目标是消除敏感变量和相应分数之间的关系。然而，在多个敏感属性的情况下，这些工具和定义的适用性和有效性变得更加复杂。为了解决这个问题，我们提出了一个顺序框架，可以逐步实现对一组敏感特征的公平性。我们通过利用多边际Wasserstein重心来实现这一点，将标准的强人口平等概念扩展到具有多个敏感特征的情况。这种方法还为最优的顺序公平预测器提供了闭式解，可以清楚地解释敏感特征之间的相关性。我们的方法也可以无缝扩展到近似解决方案。

    In the standard use case of Algorithmic Fairness, the goal is to eliminate the relationship between a sensitive variable and a corresponding score. Throughout recent years, the scientific community has developed a host of definitions and tools to solve this task, which work well in many practical applications. However, the applicability and effectivity of these tools and definitions becomes less straightfoward in the case of multiple sensitive attributes. To tackle this issue, we propose a sequential framework, which allows to progressively achieve fairness across a set of sensitive features. We accomplish this by leveraging multi-marginal Wasserstein barycenters, which extends the standard notion of Strong Demographic Parity to the case with multiple sensitive characteristics. This method also provides a closed-form solution for the optimal, sequentially fair predictor, permitting a clear interpretation of inter-sensitive feature correlations. Our approach seamlessly extends to approx
    
[^62]: 通过半结构激活稀疏加速深度神经网络

    Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity. (arXiv:2309.06626v1 [cs.CV])

    [http://arxiv.org/abs/2309.06626](http://arxiv.org/abs/2309.06626)

    通过小型运行时修改引入半结构激活稀疏性，我们设计了一种稀疏训练过程，在保持精度下降最小的情况下，实现了深度神经网络的高效处理和推断加速。

    

    在嵌入式设备上高效处理深度神经网络（DNNs）的需求是限制其部署的重要挑战之一。利用网络特征图中的稀疏性是减少推断延迟的一种方式。已知非结构化稀疏性与结构化稀疏性相比对精度下降的影响较小，但前者需要进行广泛的推断引擎更改以获得延迟优势。为了解决这个问题，我们提出了一种通过小型运行时修改引入半结构激活稀疏性的解决方案。为了在推断时获得高加速度水平，我们设计了一种稀疏训练过程，同时在计算广义矩阵乘法（GEMM）时考虑激活的最终位置。我们对各种图像分类和目标检测任务的模型对所提出的解决方案进行了广泛评估。值得注意的是，我们的方法在保持精度下降最小的情况下提供了1.25倍的速度提升。

    The demand for efficient processing of deep neural networks (DNNs) on embedded devices is a significant challenge limiting their deployment. Exploiting sparsity in the network's feature maps is one of the ways to reduce its inference latency. It is known that unstructured sparsity results in lower accuracy degradation with respect to structured sparsity but the former needs extensive inference engine changes to get latency benefits. To tackle this challenge, we propose a solution to induce semi-structured activation sparsity exploitable through minor runtime modifications. To attain high speedup levels at inference time, we design a sparse training procedure with awareness of the final position of the activations while computing the General Matrix Multiplication (GEMM). We extensively evaluate the proposed solution across various models for image classification and object detection tasks. Remarkably, our approach yields a speed improvement of $1.25 \times$ with a minimal accuracy drop 
    
[^63]: 关于随机线性系统Schr\"odinger桥收缩系数的研究

    On the Contraction Coefficient of the Schr\"odinger Bridge for Stochastic Linear Systems. (arXiv:2309.06622v1 [math.OC])

    [http://arxiv.org/abs/2309.06622](http://arxiv.org/abs/2309.06622)

    本研究对与Schr\"{o}dinger系统的收敛性相关的收缩系数进行了先验估计，并提供了新的几何和控制理论解释。我们指出通过预条件化终点支持集可以改善线性SBPs的最坏情况收缩系数的计算。

    

    Schr\"{o}dinger桥是一个随机最优控制问题，目的是在控制扩散和截止约束条件下将给定的初始状态密度转变为另一个状态密度。在经典和线性系统设置中，解决Schr\"{o}dinger桥问题的一种常用方法是通过收缩不动点递归进行数值计算。这些递归可以看作是著名的Sinkhorn迭代的动态版本，在温和的假设下，它们解决了所谓的具有线性收敛性的Schr\"{o}dinger系统。在这项工作中，我们研究了与Schr\"{o}dinger系统的收敛性相关的收缩系数的先验估计。我们提供了收缩系数的新几何和控制理论解释。基于这些新发现的解释，我们指出通过预条件化终点支持集可以改善线性SBPs的最坏情况收缩系数的计算。

    Schr\"{o}dinger bridge is a stochastic optimal control problem to steer a given initial state density to another, subject to controlled diffusion and deadline constraints. A popular method to numerically solve the Schr\"{o}dinger bridge problems, in both classical and in the linear system settings, is via contractive fixed point recursions. These recursions can be seen as dynamic versions of the well-known Sinkhorn iterations, and under mild assumptions, they solve the so-called Schr\"{o}dinger systems with guaranteed linear convergence. In this work, we study a priori estimates for the contraction coefficients associated with the convergence of respective Schr\"{o}dinger systems. We provide new geometric and control-theoretic interpretations for the same. Building on these newfound interpretations, we point out the possibility of improved computation for the worst-case contraction coefficients of linear SBPs by preconditioning the endpoint support sets.
    
[^64]: RT-LM: 面向实时推理的语言模型不确定性感知资源管理

    RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models. (arXiv:2309.06619v1 [cs.LG])

    [http://arxiv.org/abs/2309.06619](http://arxiv.org/abs/2309.06619)

    这篇论文介绍了RT-LM，它是一种针对语言模型的实时推理进行优化的不确定性感知资源管理方法。该方法能够理解、量化和优化由于语言的不确定性而导致的推理延迟性能变化，以提高LMs的整体性能。

    

    最近语言模型（LMs）的进展引起了人们的广泛关注，因为它们具备生成类似人类回应的能力。尽管在对话AI等各种应用中展示了良好的前景，但由于计算成本极高且推理延迟无法预测，这些LMs在各种设备上的部署面临挑战。由于语言的本质导致的不确定性引发的不同推理延迟可能导致计算效率不高，从而降低LMs的整体性能，特别是在高流量的工作负载下。不幸的是，这些不确定性源的带宽非常广泛，给延迟的预测和由此产生的效果带来了复杂性。为了了解和减轻不确定性对实时响应需求系统的影响，我们首先要理解、量化和优化LMs中这些不确定性导致的延迟性能变化。具体而言，我们提出了RT-LM

    Recent advancements in language models (LMs) have gained substantial attentions on their capability to generate human-like responses. Though exhibiting a promising future for various applications such as conversation AI, these LMs face deployment challenges on various devices due to their extreme computational cost and unpredictable inference latency. Such varied inference latency, identified as a consequence of uncertainty intrinsic to the nature of language, can lead to computational inefficiency and degrade the overall performance of LMs, especially under high-traffic workloads. Unfortunately, the bandwidth of these uncertainty sources is extensive, complicating the prediction of latency and the effects emanating from such uncertainties. To understand and mitigate the impact of uncertainty on real-time response-demanding systems, we take the first step to comprehend, quantify and optimize these uncertainty-induced latency performance variations in LMs. Specifically, we present RT-LM
    
[^65]: 通过无监督学习推断复杂材料的微观结构细节的纳米压痕数据

    Unsupervised Learning of Nanoindentation Data to Infer Microstructural Details of Complex Materials. (arXiv:2309.06613v1 [cs.LG])

    [http://arxiv.org/abs/2309.06613](http://arxiv.org/abs/2309.06613)

    本研究使用无监督学习分析了Cu-Cr复合材料的纳米压痕数据，推断了材料的微观结构细节和力学性质，并引入了交叉验证方法解决了数据数量和可靠预测的问题。

    

    本研究使用纳米压痕研究了Cu-Cr复合材料。样品上放置了大面积的压痕数组，产生了包含数百个在压痕深度变化时Young模量和硬度的测量值的数据集。采用无监督学习技术，高斯混合模型进行数据分析，帮助确定了“力学相”的数量及其相应的力学性质。此外，引入了交叉验证方法，推断数据数量是否足够，并建议可靠预测所需的数据量 - 这是材料科学机器学习中经常遇到但难以解决的问题之一。

    In this study, Cu-Cr composites were studied by nanoindentation. Arrays of indents were placed over large areas of the samples resulting in datasets consisting of several hundred measurements of Young's modulus and hardness at varying indentation depths. The unsupervised learning technique, Gaussian mixture model, was employed to analyze the data, which helped to determine the number of "mechanical phases" and the respective mechanical properties. Additionally, a cross-validation approach was introduced to infer whether the data quantity was adequate and to suggest the amount of data required for reliable predictions -- one of the often encountered but difficult to resolve issues in machine learning of materials science problems.
    
[^66]: 基于硬件感知的资源受限设备上的多模态神经架构搜索(Harmonic-NAS)

    Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices. (arXiv:2309.06612v1 [cs.LG])

    [http://arxiv.org/abs/2309.06612](http://arxiv.org/abs/2309.06612)

    本文提出了基于硬件感知的资源受限设备上的多模态神经架构搜索框架Harmonic-NAS，通过两层优化实现了单模态骨干和多模态融合网络的联合优化。

    

    最近，对多模态神经网络（MM-NN）的兴趣激增，这归功于它们有效处理和整合来自不同数据源的信息的能力。在MM-NN中，使用适当的单模态骨干和特定的融合网络从多个模态提取和融合特征。尽管这有助于增强多模态信息表达，但设计此类网络是劳动密集型的。它需要调整单模态骨干的架构参数，选择融合点，并选择融合的操作。此外，多模性人工智能正在成为物联网系统中的一种尖端选择，其中推断延迟和能量消耗是除准确性外的关键指标。在本文中，我们提出了一种名为Harmonic-NAS的框架，用于在资源受限设备上具有硬件感知的单模态骨干和多模态融合网络的联合优化。

    The recent surge of interest surrounding Multimodal Neural Networks (MM-NN) is attributed to their ability to effectively process and integrate information from diverse data sources. In MM-NN, features are extracted and fused from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this helps strengthen the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Furthermore, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy. In this paper, we propose Harmonic-NAS, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. Harmonic-NAS involves a two-tier optimi
    
[^67]: 分布式机器学习资源上的混合算法选择和超参数调整: 一种基于层级代理的方法

    Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach. (arXiv:2309.06604v1 [cs.LG])

    [http://arxiv.org/abs/2309.06604](http://arxiv.org/abs/2309.06604)

    本文提出了一种基于代理的层级机器学习平台，用于选择分布式组织的机器学习算法并同时调整其超参数。该方法具有可伸缩性、灵活性和鲁棒性，并支持自动化和协同的功能。

    

    算法选择和超参数调整是学术界和应用机器学习中关键的步骤。然而，由于机器学习资源数量的大幅增加、多样性和分布性，这些步骤变得越来越复杂。当将多智能体系统应用于机器学习平台的设计时，会带来可伸缩性、灵活性和鲁棒性等多个独特特性。本文提出了一种完全自动和协同的基于代理的机制，用于选择分布式组织的机器学习算法，并同时调整其超参数。我们的方法基于现有的基于代理的层级机器学习平台，并通过增强其查询结构来支持上述功能，而不限于特定的学习、选择和调整机制。我们进行了理论评估、形式验证和分析。

    Algorithm selection and hyperparameter tuning are critical steps in both academic and applied machine learning. On the other hand, these steps are becoming ever increasingly delicate due to the extensive rise in the number, diversity, and distributedness of machine learning resources. Multi-agent systems, when applied to the design of machine learning platforms, bring about several distinctive characteristics such as scalability, flexibility, and robustness, just to name a few. This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical st
    
[^68]: 离线强化学习中的潜在扩散推理

    Reasoning with Latent Diffusion in Offline Reinforcement Learning. (arXiv:2309.06599v1 [cs.LG])

    [http://arxiv.org/abs/2309.06599](http://arxiv.org/abs/2309.06599)

    本研究提出了一种新颖的离线强化学习方法，利用潜在扩散建模轨迹序列，并通过批处理约束避免外推误差。该方法能够处理多模态数据并编码更丰富的任务特定特征。

    

    离线强化学习（RL）有望通过静态数据集学习高奖励策略，而无需进一步的环境交互。然而，离线RL中的一个关键挑战在于有效地将静态数据集中的子优化轨迹片段连接起来，同时避免由于数据集中的支持不足而产生的外推误差。现有方法使用保守的方法进行调整，这些方法难以调节，并且在多模态数据上存在困难（如我们所示），或者依赖于嘈杂的蒙特卡洛回报样本进行奖励条件。在这项工作中，我们提出了一种新颖的方法，利用潜在扩散的表达能力，将支持内的轨迹序列建模为压缩的潜在技能。这有助于通过批处理约束学习Q函数，同时避免外推误差。潜在空间也具有表达能力，并且能够优雅地处理多模态数据。我们展示了学习到的时间抽象潜在空间编码更丰富的任务特定特征。

    Offline reinforcement learning (RL) holds promise as a means to learn high-reward policies from a static dataset, without the need for further environment interactions. However, a key challenge in offline RL lies in effectively stitching portions of suboptimal trajectories from the static dataset while avoiding extrapolation errors arising due to a lack of support in the dataset. Existing approaches use conservative methods that are tricky to tune and struggle with multi-modal data (as we show) or rely on noisy Monte Carlo return-to-go samples for reward conditioning. In this work, we propose a novel approach that leverages the expressiveness of latent diffusion to model in-support trajectory sequences as compressed latent skills. This facilitates learning a Q-function while avoiding extrapolation error via batch-constraining. The latent space is also expressive and gracefully copes with multi-modal data. We show that the learned temporally-abstract latent space encodes richer task-spe
    
[^69]: Rank2Tell: 一个用于联合重要性排序和推理的多模态驾驶数据集

    Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning. (arXiv:2309.06597v1 [cs.CV])

    [http://arxiv.org/abs/2309.06597](http://arxiv.org/abs/2309.06597)

    Rank2Tell是一个多模态驾驶数据集，用于联合重要性排序和推理，为研究人员提供了复杂交通情景中各种重要对象的密集注释和独特属性。

    

    商用自动驾驶车辆（AVs）和先进驾驶辅助系统（ADAS）的广泛应用可能在很大程度上取决于社会对它们的接受程度，而对骑车人来说，它们被视为可信和可解释性是至关重要的。一般来说，这个任务是具有挑战性的，因为现代自主系统软件严重依赖于黑盒人工智能模型。为了实现这个目标，本文介绍了一种新的数据集，Rank2Tell，这是一个用于重要性级别排序和原因解释的多模态驾驶数据集。使用各种闭合和开放式视觉问答，该数据集提供了复杂交通情景中各种重要对象的各种语义、空间、时间和关系属性的密集注释。数据集的密集注释和独特属性使其成为从事视觉场景理解和相关领域研究的研究人员的宝贵资源。此外，我们还介绍了一个联合模型，用于联合表示和推理重要性和原因。

    The widespread adoption of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS) may largely depend on their acceptance by society, for which their perceived trustworthiness and interpretability to riders are crucial. In general, this task is challenging because modern autonomous systems software relies heavily on black-box artificial intelligence models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios. The dense annotations and unique attributes of the dataset make it a valuable resource for researchers working on visual scene understanding and related fields. Further, we introduce a joint model for joint i
    
[^70]: Gradient-based MAML在LQR中的收敛性研究

    Convergence of Gradient-based MAML in LQR. (arXiv:2309.06588v1 [eess.SY])

    [http://arxiv.org/abs/2309.06588](http://arxiv.org/abs/2309.06588)

    本研究调查了在LQR中应用MAML时的局部收敛特性，并提供了保持动态系统稳定性的局部收敛保证。论文通过简单的数值结果展示了MAML在LQR任务中的收敛性质。

    

    本研究的主要目标是探索在线性系统二次优化控制（LQR）中应用Model-agnostic Meta-learning（MAML）时的局部收敛特性。MAML及其变体已成为快速适应新任务的流行技术，通过利用在回归、分类和强化学习等领域的先前学习知识。然而，由于非凸性和其结构，MAML的理论保证仍然未知，这使得在动态系统设置中确保稳定性更具挑战性。本研究重点研究了MAML在LQR设置中的局部收敛性保证，同时保持动态系统的稳定性。该论文还提供了简单的数值结果，以展示MAML在LQR任务中的收敛性质。

    The main objective of this research paper is to investigate the local convergence characteristics of Model-agnostic Meta-learning (MAML) when applied to linear system quadratic optimal control (LQR). MAML and its variations have become popular techniques for quickly adapting to new tasks by leveraging previous learning knowledge in areas like regression, classification, and reinforcement learning. However, its theoretical guarantees remain unknown due to non-convexity and its structure, making it even more challenging to ensure stability in the dynamic system setting. This study focuses on exploring MAML in the LQR setting, providing its local convergence guarantees while maintaining the stability of the dynamical system. The paper also presents simple numerical results to demonstrate the convergence properties of MAML in LQR tasks.
    
[^71]: 可解释的图神经网络用于阿尔茨海默病和相关痴呆症风险预测

    Explainable Graph Neural Network for Alzheimer's Disease And Related Dementias Risk Prediction. (arXiv:2309.06584v1 [cs.LG])

    [http://arxiv.org/abs/2309.06584](http://arxiv.org/abs/2309.06584)

    这项研究提出了一种可解释的图神经网络方法来预测阿尔茨海默病和相关痴呆症的风险。通过将机器学习与索赔数据相结合，不仅能发现额外的风险因素，还能揭示不同医学代码之间的关联。通过评估关系重要性和其对风险预测的影响，该方法能提供全面的解释。

    

    阿尔茨海默病和相关痴呆症（ADRD）在美国是第六大死亡原因，准确的ADRD风险预测具有重要意义。虽然最近在ADRD风险预测方面取得了一定进展，但大部分依赖于图像分析，而并非所有患者在ADRD诊断前都接受医学影像检查。将机器学习与索赔数据相结合可以揭示额外的风险因素并发现不同医学代码之间的相互关联。我们的目标是利用图神经网络（GNN）和索赔数据进行ADRD风险预测。为了解决这些预测背后缺乏可解释原因的问题，我们引入了一种创新方法来评估关系重要性及其对ADRD风险预测的影响，确保全面解释。我们使用变分正则化编码器-解码器图神经网络（VGNN）来估计ADRD可能性。我们创建了三种情景来评估模型的效率，使用了随机森林和轻梯度...

    Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading cause of death in the US, underlining the importance of accurate ADRD risk prediction. While recent advancement in ADRD risk prediction have primarily relied on imaging analysis, yet not all patients undergo medical imaging before an ADRD diagnosis. Merging machine learning with claims data can reveal additional risk factors and uncover interconnections among diverse medical codes. Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD risk prediction. Addressing the lack of human-interpretable reasons behind these predictions, we introduce an innovative method to evaluate relationship importance and its influence on ADRD risk prediction, ensuring comprehensive interpretation.  We employed Variationally Regularized Encoder-decoder Graph Neural Network (VGNN) for estimating ADRD likelihood. We created three scenarios to assess the model's efficiency, using Random Forest and Light Gradient 
    
[^72]: CMS高精度量热器原型中的电子能量回归

    Electron Energy Regression in the CMS High-Granularity Calorimeter Prototype. (arXiv:2309.06582v1 [hep-ex])

    [http://arxiv.org/abs/2309.06582](http://arxiv.org/abs/2309.06582)

    这项研究介绍了CMS高精度量热器原型中的电子能量回归，利用机器学习方法从三维点的能量重建入射电子的能量，并希望通过公开数据促进机器学习应用专家对电子图像重建的研究。

    

    我们提出了一个新的公开的数据集，其中包含将安装在CERN大型强子对撞机上的一种新型量热器的模拟数据。该探测器将拥有超过600万个通道，每个通道都能够进行位置、电离和精确时间测量。以高效的方式重建这些事件是一个巨大的挑战，该挑战正在应用最新的机器学习技术进行解决。作为这一发展的一部分，我们建造了一个拥有12,000个通道的大型原型，并对其进行了高能电子束入射。利用机器学习方法，我们从三维点的能量重建了入射电子的能量，这些能量已知到一定的精度。通过公开发布这些数据，我们希望鼓励机器学习应用专家开发出高效准确的这些电子图像重建方法。

    We present a new publicly available dataset that contains simulated data of a novel calorimeter to be installed at the CERN Large Hadron Collider. This detector will have more than six-million channels with each channel capable of position, ionisation and precision time measurement. Reconstructing these events in an efficient way poses an immense challenge which is being addressed with the latest machine learning techniques. As part of this development a large prototype with 12,000 channels was built and a beam of high-energy electrons incident on it. Using machine learning methods we have reconstructed the energy of incident electrons from the energies of three-dimensional hits, which is known to some precision. By releasing this data publicly we hope to encourage experts in the application of machine learning to develop efficient and accurate image reconstruction of these electrons.
    
[^73]: 高效有限初始化张量化神经网络的方法

    Efficient Finite Initialization for Tensorized Neural Networks. (arXiv:2309.06577v1 [cs.LG])

    [http://arxiv.org/abs/2309.06577](http://arxiv.org/abs/2309.06577)

    这种方法提出了一种高效有限初始化张量化神经网络层的方法，避免了参数爆炸问题，并通过使用弗罗贝尼乌斯范数的迭代部分形式来计算范数，使其具有有限范围。应用于不同层的实验表明其性能良好。

    

    我们提出了一种新的方法，用于初始化张量化神经网络的层，以避免参数爆炸。该方法适用于具有大量节点的层，其中所有或大多数节点与输入或输出有连接。该方法的核心是使用该层的弗罗贝尼乌斯范数的迭代部分形式，使其具有有限的范围。这个范数的计算是高效的，对于大多数情况都可以完全或部分计算。我们将这个方法应用于不同的层，并检查其性能。我们创建了一个Python函数，在i3BQuantum存储库的Jupyter Notebook中可以运行它：https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb

    We present a novel method for initializing layers of tensorized neural networks in a way that avoids the explosion of the parameters of the matrix it emulates. The method is intended for layers with a high number of nodes in which there is a connection to the input or output of all or most of the nodes. The core of this method is the use of the Frobenius norm of this layer in an iterative partial form, so that it has to be finite and within a certain range. This norm is efficient to compute, fully or partially for most cases of interest. We apply the method to different layers and check its performance. We create a Python function to run it on an arbitrary layer, available in a Jupyter Notebook in the i3BQuantum repository: https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
    
[^74]: Circle Feature Graphormer: 能够刺激图转换器的圆形特征吗？

    Circle Feature Graphormer: Can Circle Features Stimulate Graph Transformer?. (arXiv:2309.06574v1 [cs.SI])

    [http://arxiv.org/abs/2309.06574](http://arxiv.org/abs/2309.06574)

    本文介绍了一种新的圆形特征图转换器（CFG），用于丢失链路预测任务，并实现了改进的图自注意机制。实验结果表明，CFG在ogbl-citation2数据集上取得了最先进的性能。

    

    在本文中，我们介绍了两种用于ogbl-citation2中丢失链路预测任务的本地图特征。我们将这些特征定义为圆形特征，借鉴了朋友圈的概念。我们提出了上述特征的详细计算公式。首先，我们将第一个圆形特征定义为常见图中的改进振荡特征，它来自于二分图。其次，我们将第二个圆形特征定义为桥梁，它表示不同朋友圈中两个节点的重要性。此外，我们首次将上述特征作为偏置来增强图转换器神经网络，从而改进了图自注意机制。我们基于SIEG网络实现了一个基于圆形特征的图转换器（CFG）模型，它利用双塔结构来捕捉全局和局部结构特征。实验结果表明，CFG在ogbl-citation2数据集上达到了最先进的性能。

    In this paper, we introduce two local graph features for missing link prediction tasks on ogbl-citation2. We define the features as Circle Features, which are borrowed from the concept of circle of friends. We propose the detailed computing formulas for the above features. Firstly, we define the first circle feature as modified swing for common graph, which comes from bipartite graph. Secondly, we define the second circle feature as bridge, which indicates the importance of two nodes for different circle of friends. In addition, we firstly propose the above features as bias to enhance graph transformer neural network, such that graph self-attention mechanism can be improved. We implement a Circled Feature aware Graph transformer (CFG) model based on SIEG network, which utilizes a double tower structure to capture both global and local structure features. Experimental results show that CFG achieves the state-of-the-art performance on dataset ogbl-citation2.
    
[^75]: 深度核学习在控制合成中的应用前景

    Promises of Deep Kernel Learning for Control Synthesis. (arXiv:2309.06569v1 [eess.SY])

    [http://arxiv.org/abs/2309.06569](http://arxiv.org/abs/2309.06569)

    深度核学习（DKL）结合了神经网络的表示能力与高斯过程的不确定性量化，为复杂动态系统的学习和控制合成提供了潜力。本研究提出了一个基于抽象的框架，使用DKL与区间马尔可夫决策过程（IMDP）实现对随机动态系统的控制合成，同时采用了高效的深度架构和正确性保证的方法。

    

    深度核学习（DKL）将神经网络的表示能力与高斯过程的不确定性量化相结合。因此，它是一个有潜力的工具，可以学习和控制复杂的动态系统。本研究开发了一个可扩展的基于抽象的框架，使得可以在复杂规范下使用DKL进行随机动态系统的控制合成。具体而言，我们考虑时态逻辑规范，并创建一个端到端的框架，使用DKL从数据中学习未知系统，并将DKL模型正式抽象成区间马尔可夫决策过程（IMDP），以进行具有正确性保证的控制合成。此外，我们还确定了一种深度架构，可以实现准确的学习和高效的抽象计算。我们通过多个基准测试来说明我们方法的有效性，包括一个5维非线性随机系统，展示了使用DKL进行控制合成可以大大优于状态-

    Deep Kernel Learning (DKL) combines the representational power of neural networks with the uncertainty quantification of Gaussian Processes. Hence, it is potentially a promising tool to learn and control complex dynamical systems. In this work, we develop a scalable abstraction-based framework that enables the use of DKL for control synthesis of stochastic dynamical systems against complex specifications. Specifically, we consider temporal logic specifications and create an end-to-end framework that uses DKL to learn an unknown system from data and formally abstracts the DKL model into an Interval Markov Decision Process (IMDP) to perform control synthesis with correctness guarantees. Furthermore, we identify a deep architecture that enables accurate learning and efficient abstraction computation. The effectiveness of our approach is illustrated on various benchmarks, including a 5-D nonlinear stochastic system, showing how control synthesis with DKL can substantially outperform state-
    
[^76]: 在大学学生报纸中无监督检测偏见

    Unsupervised Bias Detection in College Student Newspapers. (arXiv:2309.06557v1 [cs.CL])

    [http://arxiv.org/abs/2309.06557](http://arxiv.org/abs/2309.06557)

    本文提出了一个几乎没有人为影响的流程，用于从大学报纸档案中获取并检测偏见。该方法通过比较大型语言模型摘要的情感与原文来计算偏见，不需要大量标记数据，为客观理解学生报纸来源中的偏见提供了方法。

    

    本文提出了一个几乎没有人为影响的流程，用于从大学报纸档案中获取并检测偏见。该文介绍了一个从自动化工具无法获取数据的复杂档案网站上获取数据的框架，并生成了一个包含23,154个条目的14个学生报纸数据集。通过将大型语言模型摘要的情感与原文进行比较，还可以通过关键字查询来计算偏见。这种方法的优势在于它比重构偏见更少比较，并且比生成关键字情绪需要更少的标记数据。通过在政治性词汇以及控制词上计算结果，展示了如何得出结论。该完整方法有助于在假设和分类较少的情况下提取细致入微的见解，为客观理解学生报纸来源中的偏见铺平了道路。

    This paper presents a pipeline with minimal human influence for scraping and detecting bias on college newspaper archives. This paper introduces a framework for scraping complex archive sites that automated tools fail to grab data from, and subsequently generates a dataset of 14 student papers with 23,154 entries. This data can also then be queried by keyword to calculate bias by comparing the sentiment of a large language model summary to the original article. The advantages of this approach are that it is less comparative than reconstruction bias and requires less labelled data than generating keyword sentiment. Results are calculated on politically charged words as well as control words to show how conclusions can be drawn. The complete method facilitates the extraction of nuanced insights with minimal assumptions and categorizations, paving the way for a more objective understanding of bias within student newspaper sources.
    
[^77]: 离线逆向强化学习下的提示评估与优化

    Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])

    [http://arxiv.org/abs/2309.06553](http://arxiv.org/abs/2309.06553)

    这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。

    

    最近，像ChatGPT这样的大型语言模型（LLM）的发展取得了显著的性能，通过利用人类专业知识。然而，充分揭示LLMs在复杂任务中的潜力需要在自然语言提示的广阔搜索空间中进行导航。虽然提示工程显示出潜力，但试错尝试中所需的人工设计提示和相关成本带来了重大挑战。关键是，提示优化的效率取决于昂贵的提示评估过程。本工作介绍了Prompt-OIRL，这是一种基于离线逆向强化学习的方法，旨在弥合有效提示评估和可负担性之间的差距。我们的方法利用专家评估的离线数据集，运用逆向强化学习获得一个针对离线、查询依赖型提示评估的奖励模型。Prompt-OIRL的优点是多方面的：它预测提示的性能，成本高效，生成易读的结果。

    The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
    
[^78]: 命令作为AI对话：一个开源系统将自然语言提示转化为可执行的Linux命令行工具命令

    Commands as AI Conversations. (arXiv:2309.06551v1 [cs.SE])

    [http://arxiv.org/abs/2309.06551](http://arxiv.org/abs/2309.06551)

    "ai-cli"是一个开源系统，通过将自然语言提示转化为可执行的Linux命令行工具命令，利用OpenAI的API实现命令行界面的智能化和用户友好化。

    

    开发人员和数据科学家经常在编写命令行输入时遇到困难，尽管图形界面或像ChatGPT这样的工具可以提供帮助。解决方案是开源系统"ai-cli"，受GitHub Copilot启发，将自然语言提示转化为各种Linux命令行工具的可执行命令。通过利用OpenAI的API，该系统可以通过JSON HTTP请求进行交互，将用户查询转化为可操作的命令行指令。然而，在多个命令行工具之间集成AI辅助，特别是在开源环境中，可能会很复杂。历史上，操作系统可以进行中介，但各个工具的功能和缺乏统一的方法使得集中化集成具有挑战性。通过通过动态加载和与每个程序的Readline库API链接，"ai-cli"工具填补了这一差距，使命令行界面更智能、更用户友好，为进一步的增强和跨平台提供了可能性。

    Developers and data scientists often struggle to write command-line inputs, even though graphical interfaces or tools like ChatGPT can assist. The solution? "ai-cli," an open-source system inspired by GitHub Copilot that converts natural language prompts into executable commands for various Linux command-line tools. By tapping into OpenAI's API, which allows interaction through JSON HTTP requests, "ai-cli" transforms user queries into actionable command-line instructions. However, integrating AI assistance across multiple command-line tools, especially in open source settings, can be complex. Historically, operating systems could mediate, but individual tool functionality and the lack of a unified approach have made centralized integration challenging. The "ai-cli" tool, by bridging this gap through dynamic loading and linking with each program's Readline library API, makes command-line interfaces smarter and more user-friendly, opening avenues for further enhancement and cross-platfor
    
[^79]: 在在线设置下学习线性算子的无限维回归

    Online Infinite-Dimensional Regression: Learning Linear Operators. (arXiv:2309.06548v1 [stat.ML])

    [http://arxiv.org/abs/2309.06548](http://arxiv.org/abs/2309.06548)

    在这篇论文中，我们研究了在线设置下学习无限维线性算子的问题。我们证明了在一定的条件下，线性算子是可以在线学习的，而在另一些条件下则不可以。我们还证明了在线均一收敛和学习能力之间的分离，并在PAC设置下得到了相同的结果。

    

    我们考虑在线设置下学习两个无限维希尔伯特空间之间的线性算子问题，通过最小二乘损失函数进行学习。我们证明了在$p \in [1, \infty)$范围内，具有均匀有界$p$-Schatten范数的线性算子类是可以在线学习的。另一方面，我们证明了具有均匀有界算子范数的线性算子类\textit{不}是可以在线学习的。此外，我们通过找到一类有界线性算子，证明了在线均一收敛和学习能力之间的分离。最后，我们证明了不可能性结果和均一收敛与学习能力之间的分离在PAC设置下同样成立。

    We consider the problem of learning linear operators under squared loss between two infinite-dimensional Hilbert spaces in the online setting. We show that the class of linear operators with uniformly bounded $p$-Schatten norm is online learnable for any $p \in [1, \infty)$. On the other hand, we prove an impossibility result by showing that the class of uniformly bounded linear operators with respect to the operator norm is \textit{not} online learnable. Moreover, we show a separation between online uniform convergence and online learnability by identifying a class of bounded linear operators that is online learnable but uniform convergence does not hold. Finally, we prove that the impossibility result and the separation between uniform convergence and learnability also hold in the agnostic PAC setting.
    
[^80]: 基于推特情感分析的股票报价预测模型的开发

    Desenvolvimento de modelo para predi\c{c}\~ao de cota\c{c}\~oes de a\c{c}\~ao baseada em an\'alise de sentimentos de tweets. (arXiv:2309.06538v1 [q-fin.ST])

    [http://arxiv.org/abs/2309.06538](http://arxiv.org/abs/2309.06538)

    本研究开发了一个基于推特情感分析的股票报价预测模型，通过从Twitter上的帖子中提取情感特征，使用这些特征训练模型，并在实际交易中取得了88.82雷亚尔的净收益。

    

    自动化交易这样的研究领域中，训练机器学习模型来预测股票市场的股价是一个活跃的研究领域。虽然大部分的研究工作是基于过去的股票价格训练神经网络，但在本研究中，我们使用iFeel 2.0平台从微博平台Twitter上获得的提及巴西石油公司的帖子中提取了19个情感特征。然后，我们使用这些特征来训练XBoot模型，以预测该公司未来的股票价格。随后，我们基于模型的输出模拟了巴西石油公司股票的交易，并与100个随机模型的平均表现相比，在250天的时间段内获得了88.82雷亚尔（净）的收益。

    Training machine learning models for predicting stock market share prices is an active area of research since the automatization of trading such papers was available in real time. While most of the work in this field of research is done by training Neural networks based on past prices of stock shares, in this work, we use iFeel 2.0 platform to extract 19 sentiment features from posts obtained from microblog platform Twitter that mention the company Petrobras. Then, we used those features to train XBoot models to predict future stock prices for the referred company. Later, we simulated the trading of Petrobras' shares based on the model's outputs and determined the gain of R$88,82 (net) in a 250-day period when compared to a 100 random models' average performance.
    
[^81]: 分布鲁棒的迁移学习

    Distributionally Robust Transfer Learning. (arXiv:2309.06534v1 [cs.LG])

    [http://arxiv.org/abs/2309.06534](http://arxiv.org/abs/2309.06534)

    这篇论文介绍了一种分布鲁棒的迁移学习方法，通过优化一个不确定性集合内最具对抗性的损失来实现，该集合是由源分布的凸组合生成的目标人口集合，能够有效地将迁移学习和分布鲁棒的预测模型联系起来。

    

    许多现有的迁移学习方法依赖于利用与目标数据相似的源数据的信息。然而，这种方法经常忽视了可能存在于不同但潜在相关的辅助样本中的有价值的知识。当处理有限的目标数据和多样化的源模型时，我们的论文引入了一种新颖的方法，分布鲁棒迁移学习（TransDRO），它摆脱了严格的相似性约束。TransDRO通过在一个不确定性集合内优化最具对抗性的损失来设计，该集合定义为由源分布的凸组合生成的目标人口的集合，保证了对目标数据的出色预测性能。TransDRO有效地将迁移学习和分布鲁棒的预测模型联系起来。我们建立了TransDRO的可辨识性和其作为最接近源模型的加权平均值的解释。

    Many existing transfer learning methods rely on leveraging information from source data that closely resembles the target data. However, this approach often overlooks valuable knowledge that may be present in different yet potentially related auxiliary samples. When dealing with a limited amount of target data and a diverse range of source models, our paper introduces a novel approach, Distributionally Robust Optimization for Transfer Learning (TransDRO), that breaks free from strict similarity constraints. TransDRO is designed to optimize the most adversarial loss within an uncertainty set, defined as a collection of target populations generated as a convex combination of source distributions that guarantee excellent prediction performances for the target data. TransDRO effectively bridges the realms of transfer learning and distributional robustness prediction models. We establish the identifiability of TransDRO and its interpretation as a weighted average of source models closest to
    
[^82]: 面向会话推荐的层次化多任务学习框架

    Hierarchical Multi-Task Learning Framework for Session-based Recommendations. (arXiv:2309.06533v1 [cs.IR])

    [http://arxiv.org/abs/2309.06533](http://arxiv.org/abs/2309.06533)

    本文提出了一种面向会话推荐的层次化多任务学习框架HierSRec，通过在预测任务之间设置层次结构，并利用辅助任务的输出来提供更丰富的输入特征和更高的预测可解释性，进一步增强了预测准确性和可泛化性。

    

    虽然会话推荐系统（SBRS）已经表现出卓越的推荐性能，但多任务学习（MTL）已经被SBRS采用以进一步提高其预测准确性和可泛化性。层次化多任务学习（H-MTL）在预测任务之间设置了层次结构，并将辅助任务的输出馈送给主任务。与现有的MTL框架相比，这种层次结构为主任务提供了更丰富的输入特征和更高的预测可解释性。然而，H-MTL框架在SBRS中尚未进行研究。在本文中，我们提出了HierSRec，将H-MTL架构纳入SBRS中。HierSRec使用元数据感知Transformer对给定会话进行编码，并使用会话编码进行下一类别预测（即辅助任务）。接下来，HierSRec使用类别预测结果和会话编码进行下一个物品预测（即主任务）。为了可扩展的推断，HierSRec创建了一个紧凑的候选物品集合。

    While session-based recommender systems (SBRSs) have shown superior recommendation performance, multi-task learning (MTL) has been adopted by SBRSs to enhance their prediction accuracy and generalizability further. Hierarchical MTL (H-MTL) sets a hierarchical structure between prediction tasks and feeds outputs from auxiliary tasks to main tasks. This hierarchy leads to richer input features for main tasks and higher interpretability of predictions, compared to existing MTL frameworks. However, the H-MTL framework has not been investigated in SBRSs yet. In this paper, we propose HierSRec which incorporates the H-MTL architecture into SBRSs. HierSRec encodes a given session with a metadata-aware Transformer and performs next-category prediction (i.e., auxiliary task) with the session encoding. Next, HierSRec conducts next-item prediction (i.e., main task) with the category prediction result and session encoding. For scalable inference, HierSRec creates a compact set of candidate items (
    
[^83]: 机器翻译模型在面对对抗攻击时表现出强大的稳定性

    Machine Translation Models Stand Strong in the Face of Adversarial Attacks. (arXiv:2309.06527v1 [cs.CL])

    [http://arxiv.org/abs/2309.06527](http://arxiv.org/abs/2309.06527)

    本研究探讨了对抗攻击对机器翻译模型的影响，证明了机器翻译模型在面对已知的最佳对抗攻击时表现出强大的稳定性。同时，我们提出的攻击算法在相对性能上超过其他替代选择。

    

    对抗攻击通过向输入引入微小扰动来暴露深度学习模型的漏洞，这导致输出结果发生重大变化。我们的研究关注这种对抗攻击对序列到序列（seq2seq）模型，特别是机器翻译模型的影响。我们引入了一些算法，包括基本文本扰动启发式和更高级的策略，如基于梯度的攻击，它利用可微分逼近非可微翻译度量。通过我们的调查，我们提供证据表明机器翻译模型对已知的最佳对抗攻击表现出了强大的稳定性，因为输出中的扰动程度与输入中的扰动成比例。然而，在不利情况下，我们的攻击胜过其他选择，提供了最佳的相对性能。另一个强大的候选是基于个体混合的攻击。

    Adversarial attacks expose vulnerabilities of deep learning models by introducing minor perturbations to the input, which lead to substantial alterations in the output. Our research focuses on the impact of such adversarial attacks on sequence-to-sequence (seq2seq) models, specifically machine translation models. We introduce algorithms that incorporate basic text perturbation heuristics and more advanced strategies, such as the gradient-based attack, which utilizes a differentiable approximation of the inherently non-differentiable translation metric. Through our investigation, we provide evidence that machine translation models display robustness displayed robustness against best performed known adversarial attacks, as the degree of perturbation in the output is directly proportional to the perturbation in the input. However, among underdogs, our attacks outperform alternatives, providing the best relative performance. Another strong candidate is an attack based on mixing of individu
    
[^84]: 探索差分隐私预训练和参数高效微调在表格转换器中的受益

    Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers. (arXiv:2309.06526v1 [cs.LG])

    [http://arxiv.org/abs/2309.06526](http://arxiv.org/abs/2309.06526)

    本文探索了对表格转换器进行差分隐私预训练和参数高效微调的好处，并展示了这些方法在准确性和可训练参数数量方面优于传统方法，从而在参数效率、隐私和准确度之间实现了改进的权衡。

    

    对于使用表格数据的机器学习，Table Transformer是一种最先进的神经网络模型，而差分隐私是确保数据隐私的重要组成部分。在本文中，我们探索了将这两个方面结合起来应用于迁移学习场景中的好处——差分隐私预训练和对Table Transformer进行多种参数高效微调方法（包括Adapter、LoRA和Prompt Tuning）。我们在ACSIncome数据集上进行了大量实验证明，这些参数高效微调方法在下游任务准确度和可训练参数数量方面优于传统方法，从而在参数效率、隐私和准确度之间实现了改进的权衡。我们的代码可在github.com/IBM/DP-TabTransformer中找到。

    For machine learning with tabular data, Table Transformer (TabTransformer) is a state-of-the-art neural network model, while Differential Privacy (DP) is an essential component to ensure data privacy. In this paper, we explore the benefits of combining these two aspects together in the scenario of transfer learning -- differentially private pre-training and fine-tuning of TabTransformers with a variety of parameter-efficient fine-tuning (PEFT) methods, including Adapter, LoRA, and Prompt Tuning. Our extensive experiments on the ACSIncome dataset show that these PEFT methods outperform traditional approaches in terms of the accuracy of the downstream task and the number of trainable parameters, thus achieving an improved trade-off among parameter efficiency, privacy, and accuracy. Our code is available at github.com/IBM/DP-TabTransformer.
    
[^85]: 一种针对依从性感知推荐的Q学习方法

    A Q-learning Approach for Adherence-Aware Recommendations. (arXiv:2309.06519v1 [cs.LG])

    [http://arxiv.org/abs/2309.06519](http://arxiv.org/abs/2309.06519)

    本文提出了一种针对依从性推荐的Q学习算法，以解决在高风险场景中人类决策者接受人工智能推荐的问题。算法通过学习“依从水平”来捕捉决策者遵循推荐的频率，并实时生成最佳推荐策略。研究证明该算法可以收敛到最优值，在不同场景下表现良好。

    

    在许多涉及高风险和安全问题的实际场景中，人类决策者（HDM）可能会在担负最终决策责任的同时接收到来自人工智能的推荐。在这封信中，我们开发了一种“依从性感知的Q学习”算法来解决这个问题。该算法学习了“依从水平”，即捕捉HDM遵循推荐行动的频率，并实时推导出最佳推荐策略。我们证明了所提出的Q学习算法收敛到最优值，并评估了它在各种场景中的性能。

    In many real-world scenarios involving high-stakes and safety implications, a human decision-maker (HDM) may receive recommendations from an artificial intelligence while holding the ultimate responsibility of making decisions. In this letter, we develop an "adherence-aware Q-learning" algorithm to address this problem. The algorithm learns the "adherence level" that captures the frequency with which an HDM follows the recommended actions and derives the best recommendation policy in real time. We prove the convergence of the proposed Q-learning algorithm to the optimal value and evaluate its performance across various scenarios.
    
[^86]: 利用大型语言模型和弱监督对社交媒体数据进行注释:以COVID-19自我报告的疫苗推文为例

    Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets. (arXiv:2309.06503v1 [cs.CL])

    [http://arxiv.org/abs/2309.06503](http://arxiv.org/abs/2309.06503)

    本研究评估了使用大型语言模型和弱监督方法，通过GPT-4模型在单次模式下提供标签的方式，来识别COVID-19疫苗相关推文，并与人工注释员的性能进行比较。

    

    COVID-19大流行给医疗行业和整个社会带来了重大挑战。随着COVID-19疫苗的快速发展，社交媒体平台已成为讨论疫苗相关话题的热门媒介。识别疫苗相关推文并进行分析可以为公共卫生研究人员和政策制定者提供宝贵的见解。然而，手动注释大量推文耗时且昂贵。在这项研究中，我们评估了使用大型语言模型，本例中为GPT-4（3月23日版本），和弱监督来识别COVID-19疫苗相关推文，目的是与人工注释员的性能进行比较。我们利用了一个手动策划的黄金标准数据集，并使用GPT-4在单次模式下（无需额外提示）提供标签，而无需任何额外的微调或指示。

    The COVID-19 pandemic has presented significant challenges to the healthcare industry and society as a whole. With the rapid development of COVID-19 vaccines, social media platforms have become a popular medium for discussions on vaccine-related topics. Identifying vaccine-related tweets and analyzing them can provide valuable insights for public health research-ers and policymakers. However, manual annotation of a large number of tweets is time-consuming and expensive. In this study, we evaluate the usage of Large Language Models, in this case GPT-4 (March 23 version), and weak supervision, to identify COVID-19 vaccine-related tweets, with the purpose of comparing performance against human annotators. We leveraged a manu-ally curated gold-standard dataset and used GPT-4 to provide labels without any additional fine-tuning or instructing, in a single-shot mode (no additional prompting).
    
[^87]: PyTorch分布式数据并行的分布式Shampoo优化器实现用于大规模训练神经网络

    A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale. (arXiv:2309.06497v1 [cs.LG])

    [http://arxiv.org/abs/2309.06497](http://arxiv.org/abs/2309.06497)

    这是一个针对大规模训练神经网络的PyTorch分布式数据并行实现的分布式Shampoo优化算法。这种算法利用了块对角先验矩阵和搜索方向的AllGather原语操作，在性能上实现了显著的优化，相比于传统的对角缩放的自适应梯度方法仅有最多10%的性能损失。

    

    Shampoo是一种在线和随机优化算法，属于AdaGrad方法家族，用于训练神经网络。它构建一个块对角先验矩阵，其中每个块由神经网络每个参数的全矩阵AdaGrad的粗略Kronecker积近似构成。在这项工作中，我们提供了算法的完整描述，以及我们的实现利用PyTorch来训练深度网络的性能优化。我们的实现通过PyTorch的DTensor数据结构来分布每个参数块的内存和计算，并在每次迭代中对计算得到的搜索方向进行AllGather原语操作，从而实现快速的多GPU分布式数据并行训练。这一主要性能提升使我们在每步墙钟时间上相比于标准的基于对角缩放的自适应梯度方法最多只有10%的性能下降。我们通过实验验证了我们的实现。

    Shampoo is an online and stochastic optimization algorithm belonging to the AdaGrad family of methods for training neural networks. It constructs a block-diagonal preconditioner where each block consists of a coarse Kronecker product approximation to full-matrix AdaGrad for each parameter of the neural network. In this work, we provide a complete description of the algorithm as well as the performance optimizations that our implementation leverages to train deep networks at-scale in PyTorch. Our implementation enables fast multi-GPU distributed data-parallel training by distributing the memory and computation associated with blocks of each parameter via PyTorch's DTensor data structure and performing an AllGather primitive on the computed search directions at each iteration. This major performance enhancement enables us to achieve at most a 10% performance reduction in per-step wall-clock time compared against standard diagonal-scaling-based adaptive gradient methods. We validate our i
    
[^88]: 学习在网格上的拓扑操作及其在多边形区块分解中的应用

    Learning topological operations on meshes with application to block decomposition of polygons. (arXiv:2309.06484v1 [cs.CG])

    [http://arxiv.org/abs/2309.06484](http://arxiv.org/abs/2309.06484)

    本文介绍了一种学习基于框架，用于改善非结构化网格的质量，通过自主对弈强化学习来学习减小节点度数与理想值之间的差异，从而达到减少不规则节点的目标。

    

    我们提出了一个基于学习的框架，用于改善非结构化三角形和四边形网格的质量。我们的模型通过自主对弈强化学习来学习改善网格质量，没有先验的启发函数。网格上进行的操作是标准的局部和全局元素操作。目标是最小化节点度数与理想值之间的偏差，在内部节点的情况下，可以最小化不规则节点。

    We present a learning based framework for mesh quality improvement on unstructured triangular and quadrilateral meshes. Our model learns to improve mesh quality according to a prescribed objective function purely via self-play reinforcement learning with no prior heuristics. The actions performed on the mesh are standard local and global element operations. The goal is to minimize the deviation of the node degrees from their ideal values, which in the case of interior vertices leads to a minimization of irregular nodes.
    
[^89]: 数据流转：用最大似然估计将一个数据集变形成另一个数据集

    Flows for Flows: Morphing one Dataset into another with Maximum Likelihood Estimation. (arXiv:2309.06472v1 [hep-ph])

    [http://arxiv.org/abs/2309.06472](http://arxiv.org/abs/2309.06472)

    该论文提出了一种名为"flows for flows"的协议，用于通过最大似然估计训练规范化流将一个数据集变形成另一个数据集，即使两个数据集的概率密度都不明确。

    

    在高能物理和其他领域的数据分析中，许多组件需要将一个数据集变形成另一个数据集。通常通过重新加权来解决，但保留权重并移动数据点有许多优势。规范化流是具有令人印象深刻的精度的机器学习模型，在各种粒子物理任务上表现出色。但规范化流不能直接用于数据流转，因为它们需要对起始数据集的概率密度有所了解。在大多数粒子物理案例中，我们可以生成更多示例，但我们并不明确知道密度。我们提出了一种称为"flows for flows"的协议，用于训练规范化流将一个数据集变形成另一个数据集，即使两个数据集的概率密度都不明确。这使得可以使用最大似然估计训练变形策略，而该设置已证明在相关任务中非常有效。我们研究了这个协议的多种变体，以探索数据流转的极限程度。

    Many components of data analysis in high energy physics and beyond require morphing one dataset into another. This is commonly solved via reweighting, but there are many advantages of preserving weights and shifting the data points instead. Normalizing flows are machine learning models with impressive precision on a variety of particle physics tasks. Naively, normalizing flows cannot be used for morphing because they require knowledge of the probability density of the starting dataset. In most cases in particle physics, we can generate more examples, but we do not know densities explicitly. We propose a protocol called flows for flows for training normalizing flows to morph one dataset into another even if the underlying probability density of neither dataset is known explicitly. This enables a morphing strategy trained with maximum likelihood estimation, a setup that has been shown to be highly effective in related tasks. We study variations on this protocol to explore how far the dat
    
[^90]: 缩小监督和无监督句子表示学习的差距：大规模语言模型

    Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model. (arXiv:2309.06453v1 [cs.CL])

    [http://arxiv.org/abs/2309.06453](http://arxiv.org/abs/2309.06453)

    本文通过实验比较了监督和无监督句子表示学习在训练过程中的行为，并探讨了如何缩小性能差距。

    

    句子表示学习是自然语言处理中的一项基本任务，对比学习的句子嵌入（CSE）作为主流技术具有出色的性能。然而，在CSE中有一个有趣的现象，即监督和无监督方法之间存在显著的性能差距，即使它们的句子编码器和损失函数相同。本文通过实证实验回答“发生了什么导致了性能差距”和“如何缩小性能差距”的问题。我们首先通过彻底比较监督和无监督CSE在各自的训练过程中的行为来回答“发生了什么”这个问题。

    Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with Contrastive learning of Sentence Embeddings (CSE) as the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, even when their sentence encoder and loss function are the same. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, alignment and uniformity only measure the results, which means they cannot answer "What happens during the training process that leads to the performance gap?" and "How can the performance gap be narrowed?". In this paper, we conduct empirical experiments to answer these "What" and "How" questions. We first answer the "What" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, We o
    
[^91]: 量子化的非挥发性纳米磁阻自编码器用于高效无监督网络异常检测

    Quantized Non-Volatile Nanomagnetic Synapse based Autoencoder for Efficient Unsupervised Network Anomaly Detection. (arXiv:2309.06449v1 [cond-mat.mes-hall])

    [http://arxiv.org/abs/2309.06449](http://arxiv.org/abs/2309.06449)

    本论文提出了一种使用量子化的非挥发性纳米磁阻自编码器进行高效无监督网络异常检测的方法。通过设计低分辨率的自编码器和有效的量化神经网络学习算法，解决了边缘设备资源限制的挑战。使用特殊的磁性域壁作为突触，通过自旋轨道力电流脉冲操纵突触权重，并在NSL-KDD数据集上进行了评估，得到了与浮点精度权重自编码器相当的异常检测性能。

    

    在基于自编码器的异常检测范式中，由于硬件、能源和计算资源有限，将自编码器实现在能够实时学习的边缘设备中是极具挑战性的。我们展示了通过设计具有低分辨率的非挥发性内存基础突触的自编码器，并采用有效的量化神经网络学习算法来解决这些限制。我们提出了一个具有工程缺口的铁磁赛道，其中承载着一个磁性域壁(DW)作为自编码器的突触，在这里通过自旋轨道力(SOT)电流脉冲操纵有限状态(5状态)的突触权重。我们在NSL-KDD数据集上评估了所提出的自编码器模型的异常检测性能。进行了有限分辨率和DW器件随机性感知训练，其异常检测性能与具有浮点精度权重的自编码器相当。

    In the autoencoder based anomaly detection paradigm, implementing the autoencoder in edge devices capable of learning in real-time is exceedingly challenging due to limited hardware, energy, and computational resources. We show that these limitations can be addressed by designing an autoencoder with low-resolution non-volatile memory-based synapses and employing an effective quantized neural network learning algorithm. We propose a ferromagnetic racetrack with engineered notches hosting a magnetic domain wall (DW) as the autoencoder synapses, where limited state (5-state) synaptic weights are manipulated by spin orbit torque (SOT) current pulses. The performance of anomaly detection of the proposed autoencoder model is evaluated on the NSL-KDD dataset. Limited resolution and DW device stochasticity aware training of the autoencoder is performed, which yields comparable anomaly detection performance to the autoencoder having floating-point precision weights. While the limited number of 
    
[^92]: 使用机器学习和深度学习模型预测滑坡易发性时的贡献因素选择

    Selection of contributing factors for predicting landslide susceptibility using machine learning and deep learning models. (arXiv:2309.06062v1 [cs.LG])

    [http://arxiv.org/abs/2309.06062](http://arxiv.org/abs/2309.06062)

    使用机器学习和深度学习模型对滑坡易发性进行预测，通过选择更重要的贡献因素提高预测准确性。

    

    滑坡是常见的自然灾害，可能导致人员伤亡、财产安全威胁和经济损失。因此，了解或预测潜在风险地点的滑坡发生概率非常重要。常用的方法是根据滑坡清单和一组滑坡贡献因素进行滑坡易发性评估，可以通过机器学习（ML）模型（如逻辑回归，支持向量机，随机森林，极限梯度提升或深度学习（DL）模型（如卷积神经网络和长短记忆）来实现。作为这些模型的输入数据，滑坡贡献因素对滑坡发生有不同的影响。因此，有逻辑的选择更重要的贡献因素并消除不相关的因素，从而提高这些模型的预测准确性。

    Landslides are a common natural disaster that can cause casualties, property safety threats and economic losses. Therefore, it is important to understand or predict the probability of landslide occurrence at potentially risky sites. A commonly used means is to carry out a landslide susceptibility assessment based on a landslide inventory and a set of landslide contributing factors. This can be readily achieved using machine learning (ML) models such as logistic regression (LR), support vector machine (SVM), random forest (RF), extreme gradient boosting (Xgboost), or deep learning (DL) models such as convolutional neural network (CNN) and long short time memory (LSTM). As the input data for these models, landslide contributing factors have varying influences on landslide occurrence. Therefore, it is logically feasible to select more important contributing factors and eliminate less relevant ones, with the aim of increasing the prediction accuracy of these models. However, selecting more
    
[^93]: 内存注入：在Transformer-Based语言模型中纠正多跳推理错误

    Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. (arXiv:2309.05605v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05605](http://arxiv.org/abs/2309.05605)

    本文提出了一种通过向Transformer-Based语言模型的LLM注意力头部定向注入内存来纠正多跳推理错误的方法，从而提高了模型在处理多跳推理问题时的表现。

    

    回答多跳推理问题需要从多个信息源中检索和综合信息。大语言模型(LLMs)往往难以保持一致的推理能力。本文提出了一种通过在LLM注意力头部进行定向内存注入来确定和纠正多跳推理错误的方法。首先，我们分析了GPT-2模型在单跳和多跳提示下各层的激活情况。然后，我们提出了一种机制，允许用户在推理过程中向关键LLM位置注入相关的提示特定信息，我们将其称为“记忆”。通过在推理过程中使LLM能够整合额外的相关信息，我们提高了多跳提示生成的质量。我们实证表明，将简单、高效且定向的记忆注入到关键注意力层中往往能够提高多跳任务中所需下一个标记的概率，提高了达到424%。

    Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
    
[^94]: 一个示例题形的非均匀扩散模板: 从简单推导到稳定性评估再到ResNet实现

    Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations. (arXiv:2309.05575v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2309.05575](http://arxiv.org/abs/2309.05575)

    该论文研究了一个大型的有限差分离散化家族，在非均匀扩散过程中通过将二维扩散分解为四个一维扩散得出了一个3 x 3的模板。该模板类包含一个自由参数，涵盖了广泛的现有离散化。同时，论文还建立了与模板相对应的矩阵的谱范数上界，并将显式方案转化为ResNet块。

    

    非均匀扩散过程与扩散张量在图像分析、物理学和工程学中具有重要意义。然而，它们的数值近似对耗散性伪影和与旋转不变性偏离有强烈影响。在这项工作中，我们研究了一个大型的有限差分离散化家族，使用了一个3 x 3的模板。我们通过将二维非均匀扩散分解为四个一维扩散来推导出它。结果的模板类包含一个自由参数，涵盖了广泛的现有离散化。它包括Weickert等人(2013)的完整模板家族，并表明它们的两个参数包含冗余。此外，我们建立了与模板相对应的矩阵的谱范数上界。这给出了在欧几里得范数中保证显式方案稳定性的时间步长限制。我们的方向分割还允许将显式方案非常自然地转化为ResNet块。使用神经网络库实现简单。

    Anisotropic diffusion processes with a diffusion tensor are important in image analysis, physics, and engineering. However, their numerical approximation has a strong impact on dissipative artefacts and deviations from rotation invariance. In this work, we study a large family of finite difference discretisations on a 3 x 3 stencil. We derive it by splitting 2-D anisotropic diffusion into four 1-D diffusions. The resulting stencil class involves one free parameter and covers a wide range of existing discretisations. It comprises the full stencil family of Weickert et al. (2013) and shows that their two parameters contain redundancy. Furthermore, we establish a bound on the spectral norm of the matrix corresponding to the stencil. This gives time step size limits that guarantee stability of an explicit scheme in the Euclidean norm. Our directional splitting also allows a very natural translation of the explicit scheme into ResNet blocks. Employing neural network libraries enables simple
    
[^95]: NExT-GPT: 任何到任何的多模态语言模型

    NExT-GPT: Any-to-Any Multimodal LLM. (arXiv:2309.05519v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.05519](http://arxiv.org/abs/2309.05519)

    NExT-GPT是一个任何到任何的多模态语言模型系统，通过连接多模态适配器和不同扩散解码器，能够接受和生成任意组合的文本、图像、视频和音频内容。

    

    最近，多模态大型语言模型（MM-LLM）取得了令人振奋的进展，但它们主要存在一个限制，即只能在输入端进行多模态理解，无法以多种模式生成内容。由于我们人类总是通过各种模态感知世界和与人交流，因此开发能够接受和传递任何模态内容的任何到任何的MM-LLM系统对于实现人级AI至关重要。为了填补这一空白，我们提出了一个端到端的通用任何到任何的多模态语言模型系统，NExT-GPT。我们通过连接一个含有多模态适配器和不同扩散解码器的LLM，使得NExT-GPT能够以任意的文本、图像、视频和音频的组合进行输入和输出。通过利用现有训练有素的高性能编码器和解码器，NExT-GPT仅通过调整某些投影层的少量参数（1%）进行调优，这不仅有利于低成本训练，还有助于方便的扩展性。

    While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansi
    
[^96]: 可以通过短信传输发生的事情吗？将预训练语言编码器整合到自动驾驶的轨迹预测模型中

    Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving. (arXiv:2309.05282v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.05282](http://arxiv.org/abs/2309.05282)

    本研究提出了将预训练语言编码器整合到自动驾驶的轨迹预测模型中的新方法。通过基于文本的场景表示和经典的栅格化图像表示相结合，得到了描述性的场景嵌入，并在实验中验证了显著的性能改进。

    

    在自动驾驶任务中，场景理解是预测周围交通参与者未来行为的第一步。然而，如何表示给定的场景并提取其特征仍然是开放的研究问题。本研究提出了一种新颖的基于文本的交通场景表示，并通过预训练的语言编码器进行处理。首先，我们展示了基于文本的表示与传统的栅格化图像表示相结合，可以得到描述性的场景嵌入。其次，我们在nuScenes数据集上对我们的预测进行了基准测试，并与基准模型相比，显示出显著的改进。第三，我们通过消融研究证明，文本和栅格化图像的联合编码器胜过单独的编码器，确认了两种表示具有互补的优势。

    In autonomous driving tasks, scene understanding is the first step towards predicting the future behavior of the surrounding traffic participants. Yet, how to represent a given scene and extract its features are still open research questions. In this study, we propose a novel text-based representation of traffic scenes and process it with a pre-trained language encoder.  First, we show that text-based representations, combined with classical rasterized image representations, lead to descriptive scene embeddings. Second, we benchmark our predictions on the nuScenes dataset and show significant improvements compared to baselines. Third, we show in an ablation study that a joint encoder of text and rasterized images outperforms the individual encoders confirming that both representations have their complementary strengths.
    
[^97]: 通过协同扩散恢复似然学习基于能量的模型

    Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood. (arXiv:2309.05153v1 [stat.ML])

    [http://arxiv.org/abs/2309.05153](http://arxiv.org/abs/2309.05153)

    本文通过协同扩散恢复似然（CDRL）提出了一种方法，用于学习和采样一系列基于能量的模型（EBMs），通过在不断嘈杂化的数据集版本上定义不同噪声水平的EBMs，并与初始化模型配对协同训练。这种方法旨在关闭EBMs和其他生成框架之间的样本质量差距。

    

    在高维数据上使用最大似然估计训练能量基准模型（EBMs）可能具有挑战性且耗时较长。因此，EBMs和其他生成框架（如GANs和扩散模型）之间存在明显的样本质量差距。为了弥补这一差距，受最近通过最大化扩散恢复似然（DRL）来学习EBMs的努力的启发，我们提出了协同扩散恢复似然（CDRL），一种有效的方法来可行地学习和从一系列EBMs中进行采样，这些EBMs定义在越来越嘈杂的数据集版本上，并与每个EBM的初始化模型配对。在每个噪声水平上，初始化模型学习在EBM的采样过程中分摊，而两个模型在协同训练框架内共同估计。初始化模型生成的样本作为起始点，经过EBM的几个采样步骤进行改进。通过改进后的样本，通过最大化恢复似然来优化EBM。

    Training energy-based models (EBMs) with maximum likelihood estimation on high-dimensional data can be both challenging and time-consuming. As a result, there a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximimizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versons of a dataset, paired with an initializer model for each EBM. At each noise level, the initializer model learns to amortize the sampling process of the EBM, and the two models are jointly estimated within a cooperative training framework. Samples from the initializer serve as starting points that are refined by a few sampling steps from the EBM. With the refined samples, the EBM is optimized by maximizing recovery likelihood, while t
    
[^98]: 具有有界更新的迭代学习算法的泛化误差界限

    Generalization error bounds for iterative learning algorithms with bounded updates. (arXiv:2309.05077v1 [cs.LG])

    [http://arxiv.org/abs/2309.05077](http://arxiv.org/abs/2309.05077)

    本文研究了具有有界更新的迭代学习算法在非凸损失函数上的泛化特性，提出了一种新颖的泛化误差界限，利用了信息论技术。研究表明，在模型维度和训练数据样本数量相等的情况下，界限得到了改善。

    

    本文探讨了具有有界更新的迭代学习算法在非凸损失函数上的泛化特性，采用了信息论技术。我们的主要贡献是针对具有有界更新的算法提出了一种新颖的泛化误差界限，超出了以前只关注随机梯度下降（SGD）的范围。我们的方法引入了两个主要的创新之处：1）我们将互信息重新定义为更新的不确定性，提供了一种新的视角；2）我们不使用互信息的链式法则，而是采用方差分解技术来将信息分解到迭代中，从而允许简化的代理过程。我们在各种设置下分析了我们的泛化界限，并在模型维度以与训练数据样本数量相同的速率增加时展示了改进的界限。为了弥合理论与实践之间的差距，我们还研究了先前观察到的情况。

    This paper explores the generalization characteristics of iterative learning algorithms with bounded updates for non-convex loss functions, employing information-theoretic techniques. Our key contribution is a novel bound for the generalization error of these algorithms with bounded updates, extending beyond the scope of previous works that only focused on Stochastic Gradient Descent (SGD). Our approach introduces two main novelties: 1) we reformulate the mutual information as the uncertainty of updates, providing a new perspective, and 2) instead of using the chaining rule of mutual information, we employ a variance decomposition technique to decompose information across iterations, allowing for a simpler surrogate process. We analyze our generalization bound under various settings and demonstrate improved bounds when the model dimension increases at the same rate as the number of training data samples. To bridge the gap between theory and practice, we also examine the previously obse
    
[^99]: 用物理信息神经网络进行最优反对角量子计算的研究

    Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation. (arXiv:2309.04434v1 [quant-ph] CROSS LISTED)

    [http://arxiv.org/abs/2309.04434](http://arxiv.org/abs/2309.04434)

    本研究提出了一种利用物理信息神经网络（PINNs）解决量子电路反对角（CD）协议优化问题的方法，通过嵌入物理信息到神经网络中，并利用最小作用量原理和厄米特性条件来获取最适当的反对角项，从而提供了一种可靠的替代方案，摆脱了以往依赖于经典数值逼近的约束。

    

    我们引入了一种新的方法，利用物理信息神经网络（PINNs）的优势来解决由$N_{Q}$比特系统组成的量子电路中的反对角（CD）协议优化的问题。主要目标是利用受物理启发的深度学习技术精确地解决量子系统中不同物理可观测量的时间演化。为了实现这个目标，我们将必要的物理信息嵌入到底层神经网络中，以有效地解决这个问题。特别地，我们对所有物理可观测量施加厄米特性条件，并利用最小作用量原理，保证基于物理学的最适当反对角项的获取。所提出的方法提供了一个可靠的替代选择来解决CD驱动问题，摆脱了以往依赖于经典数值逼近的约束。

    We introduce a novel methodology that leverages the strength of Physics-Informed Neural Networks (PINNs) to address the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits. The primary objective is to utilize physics-inspired deep learning techniques to accurately solve the time evolution of the different physical observables within the quantum system. To accomplish this objective, we embed the necessary physical information into an underlying neural network to effectively tackle the problem. In particular, we impose the hermiticity condition on all physical observables and make use of the principle of least action, guaranteeing the acquisition of the most appropriate counterdiabatic terms based on the underlying physics. The proposed approach offers a dependable alternative to address the CD driving problem, free from the constraints typically encountered in previous methodologies relying on classical numerical approximations.
    
[^100]: 通过在线凸优化实现在线子模最大化

    Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])

    [http://arxiv.org/abs/2309.04339](http://arxiv.org/abs/2309.04339)

    本论文研究了在线设置下的一般性子模最大化问题，并将一类大型子模函数归约到在线凸优化问题中。这种归约方式可在组合优化中实现次线性遗憾，并且适用于许多不同版本的在线学习问题。

    

    我们研究了在线设置下的一般性子模最大化问题在一般性模性约束下。我们证明了在线优化一类大型子模函数，即加权阈值势函数，可以归约到在线凸优化(OCO)问题。这是因为这个类别的函数可以进行凹松弛;因此，结合适当的舍入方案，OCO策略可以在组合设置中实现次线性遗憾。我们还展示了我们的简化方式可以应用在许多不同版本的在线学习问题中，包括动态遗憾、强盗和乐观学习等设置。

    We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
    
[^101]: 通过偏好学习在多目标问题中进行交互式超参数优化

    Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])

    [http://arxiv.org/abs/2309.03581](http://arxiv.org/abs/2309.03581)

    本文提出了一个以人为中心的交互式超参数优化方法，通过应用偏好学习来解决多目标机器学习中的问题。

    

    超参数优化对于发挥机器学习的潜力至关重要。在实践中，用户通常对多目标问题感兴趣，即优化可能存在冲突的目标，比如准确性和能耗。为了解决这个问题，绝大多数多目标机器学习算法将一组非支配的机器学习模型的帕累托前沿返回给用户。然而，优化这种算法的超参数并不容易，因为评估一个超参数配置涉及评估得到的帕累托前沿的质量。在文献中，已有一些指标可以通过量化不同属性（如体积、与参考点的接近程度）来评估帕累托前沿的质量（例如超体积、R2）。然而，对于用户来说，选择导致期望的帕累托前沿的指标可能是一项困难的任务。在本文中，我们提出了一个以人为中心的交互式超参数优化方法，针对多目标机器学习应用偏好学习。

    Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learnin
    
[^102]: 统一图神经网络中的过度平滑和过度压缩：一种物理信息驱动的方法和更多扩展

    Unifying over-smoothing and over-squashing in graph neural networks: A physics informed approach and beyond. (arXiv:2309.02769v1 [cs.LG])

    [http://arxiv.org/abs/2309.02769](http://arxiv.org/abs/2309.02769)

    本研究提出了一种基于物理信息的方法，通过反转图热方程的时间方向，提高了图神经网络的节点特征清晰度，并引入了多尺度热核滤波函数，增强了GNNs的表达能力。进一步推广为G-MHKG模型，探索了更灵活的滤波条件，以解决过度平滑和过度压缩等计算挑战。这些方法和模型在增强GNNs性能方面具有潜力。

    

    图神经网络（GNNs）已经成为处理图结构数据的领先方法之一。尽管取得了巨大的成功，但是GNNs仍然面临着过度平滑、过度压缩和有限的表达能力等关键计算挑战，这些问题会影响GNNs的性能。本研究从经典和量子物理中常用的时间反演原理得到启发，将图热方程的时间方向进行反转，得到了一类高通滤波函数，可以增强图节点特征的清晰度。基于这个概念，我们引入了基于多尺度热核的图神经网络（MHKG），通过合并多种滤波函数对节点特征的影响来增强其表达能力。为了探索更灵活的滤波条件，我们进一步将MHKG推广到一个称为G-MHKG的模型，并详细展示了每个元素在控制过度平滑、过度压缩和表达能力方面的作用。值得注意的是，我们说明了所有的观测可达一个目标。

    Graph Neural Networks (GNNs) have emerged as one of the leading approaches for machine learning on graph-structured data. Despite their great success, critical computational challenges such as over-smoothing, over-squashing, and limited expressive power continue to impact the performance of GNNs. In this study, inspired from the time-reversal principle commonly utilized in classical and quantum physics, we reverse the time direction of the graph heat equation. The resulted reversing process yields a class of high pass filtering functions that enhance the sharpness of graph node features. Leveraging this concept, we introduce the Multi-Scaled Heat Kernel based GNN (MHKG) by amalgamating diverse filtering functions' effects on node features. To explore more flexible filtering conditions, we further generalize MHKG into a model termed G-MHKG and thoroughly show the roles of each element in controlling over-smoothing, over-squashing and expressive power. Notably, we illustrate that all afo
    
[^103]: 最大均差相似度遇上神经网络：Radon-Kolmogorov-Smirnov检验

    Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test. (arXiv:2309.02422v1 [stat.ML])

    [http://arxiv.org/abs/2309.02422](http://arxiv.org/abs/2309.02422)

    本文将最大均差相似度应用于神经网络，并提出了一种称为Radon-Kolmogorov-Smirnov（RKS）检验的方法，该方法将样本均值差异最大化的问题推广到多维空间和更高平滑度顺序，同时与神经网络密切相关。

    

    最大均差相似度（MMD）是一类基于最大化两个分布$P$和$Q$之间样本均值差异的非参数双样本检验，其中考虑了所有在某个函数空间$\mathcal{F}$中的数据变换$f$的选择。受到最近将所谓的Radon有界变差函数（RBV）和神经网络联系起来的工作的启发（Parhi和Nowak, 2021, 2023），我们研究了将$\mathcal{F}$取为给定平滑度顺序$k \geq 0$下的RBV空间中的单位球的MMD。这个检验被称为Radon-Kolmogorov-Smirnov（RKS）检验，可以看作是对多维空间和更高平滑度顺序的经典Kolmogorov-Smirnov（KS）检验的一般化。它还与神经网络密切相关：我们证明RKS检验中的证据函数$f$，即达到最大均差的函数，总是一个二次样条函数。

    Maximum mean discrepancy (MMD) refers to a general class of nonparametric two-sample tests that are based on maximizing the mean difference over samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of $\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the $\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree
    
[^104]: 多途径适配器：为可扩展的图像-文本检索调整大规模多模态模型

    MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval. (arXiv:2309.01516v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.01516](http://arxiv.org/abs/2309.01516)

    多途径适配器是一个创新的框架，利用"对齐增强器"加深模态对齐，实现高可转移性，可有效减少调整参数的时间并提高零样本图像-文本检索性能。

    

    随着大规模多模态模型（LMMs）的规模不断增加，将这些预训练模型调整到专门的任务上已成为一个计算和内存密集的挑战。传统的微调方法需要为每个新任务进行孤立、穷举的重新调整，限制了模型的多功能性。此外，当前的高效调整技术经常忽视模态对齐，仅关注新任务的知识提取。为了解决这些问题，我们引入了多途径适配器，这是一个创新的框架，它包含了一个“对齐增强器”，可以加深模态对齐，实现高度的可转移性而无需调整预训练参数。我们的方法仅向LMMs添加了不到1.25%的额外参数，以BEiT-3模型为例。与完全微调的模型相比，我们的方法在零样本图像-文本检索性能上具有优势，同时缩短了高达57%的微调时间。我们的方法提供了一种资源高效和高效的方法。

    As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25\% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57\% reduction in fine-tuning time. Our approach offers a resource-efficient and ef
    
[^105]: 切换与征服：通过切换随机梯度预言求解分散鞍点问题的高效算法

    Switch and Conquer: Efficient Algorithms By Switching Stochastic Gradient Oracles For Decentralized Saddle Point Problems. (arXiv:2309.00997v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.00997](http://arxiv.org/abs/2309.00997)

    该论文提出了一种切换随机梯度预言的高效算法，用于解决分散鞍点问题。通过在初始阶段使用广义随机梯度计算预言加速迭代进展，然后切换到随机方差减少梯度预言，该算法取得了较好的性能。

    

    我们考虑在没有中央服务器的分散设置下处理一类非光滑强凸-强凹鞍点问题。为了解决这一类问题的共识形式，我们开发了一种不精确的原始对偶杂交梯度（不精确PDHG）过程，允许通用的梯度计算预言更新原始变量和对偶变量。我们首先研究了带有随机方差减少梯度（SVRG）预言的不精确PDHG的性能。我们的数值研究揭示了IPDHG与SVRG预言迭代的初始保守进展现象。为了解决这个问题，我们提出了一个简单而有效的切换思想，在初始更新阶段使用广义随机梯度（GSG）计算预言来加速迭代的进展到鞍点解，然后在适当时刻切换到SVRG预言。我们提出的算法命名为分散近端切换随机梯度

    We consider a class of non-smooth strongly convex-strongly concave saddle point problems in a decentralized setting without a central server. To solve a consensus formulation of problems in this class, we develop an inexact primal dual hybrid gradient (inexact PDHG) procedure that allows generic gradient computation oracles to update the primal and dual variables. We first investigate the performance of inexact PDHG with stochastic variance reduction gradient (SVRG) oracle. Our numerical study uncovers a significant phenomenon of initial conservative progress of iterates of IPDHG with SVRG oracle. To tackle this, we develop a simple and effective switching idea, where a generalized stochastic gradient (GSG) computation oracle is employed to hasten the iterates' progress to a saddle point solution during the initial phase of updates, followed by a switch to the SVRG oracle at an appropriate juncture. The proposed algorithm is named Decentralized Proximal Switching Stochastic Gradient me
    
[^106]: 阐明扩散模型中的曝光偏差问题

    Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])

    [http://arxiv.org/abs/2308.15321](http://arxiv.org/abs/2308.15321)

    本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。

    

    扩散模型展示了令人印象深刻的生成能力，但它们的“曝光偏差”问题，即训练和采样之间的输入不匹配，缺乏深入探索。本文通过首先对采样分布进行分析建模，然后将每个采样步骤的预测误差归因为曝光偏差问题的根本原因，系统地研究了扩散模型中的曝光偏差问题。此外，我们讨论了解决这个问题的潜在方法，并提出了一个直观的度量标准。除了阐明曝光偏差问题，我们提出了一种简单但有效的免训练方法，称为Epsilon Scaling，以减轻曝光偏差。我们展示了Epsilon Scaling通过缩小网络输出（Epsilon）明确地将采样轨迹移近训练阶段学习到的向量场，从而减轻了训练和采样之间的输入不匹配。在各种扩散框架上进行了实验。

    Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
    
[^107]: 用机器学习优化国家篮球协会的进攻战术规划

    Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning. (arXiv:2308.06851v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06851](http://arxiv.org/abs/2308.06851)

    本文研究了如何用机器学习优化国家篮球协会的进攻战术规划。通过建立模型，采用特定的特征进行评估和分析，可以帮助决策者确定战术执行的具体细节。

    

    在NBA发生的分析革命中，特定的指标和公式的发展为球队、教练和球员提供了一种新的看待比赛的方式。然而，一个问题出现了，我们如何验证这些指标呢？一种方法可能是简单地凭眼球推测（尝试许多不同的战术计划）和/或试错法-一种估计性的昂贵方法。另一种方法是尝试用机器学习技术对已有指标进行建模，并使用一组独特的特征来模拟。这种方法的关键在于，通过选择这些特征，我们可以尝试评估这些特征的组合效果，而不是单独分析简单的指标评估。如果我们有一个准确的模型，它可以帮助我们确定战术执行的具体细节。在本文中，统计指标ORTG（Dean Oliver开发的进攻评分）发现与不同的NBA比赛类型存在相关性，通过使用线性回归方法进行建模。

    Throughout the analytical revolution that has occurred in the NBA, the development of specific metrics and formulas has given teams, coaches, and players a new way to see the game. However - the question arises - how can we verify any metrics? One method would simply be eyeball approximation (trying out many different gameplans) and/or trial and error - an estimation-based and costly approach. Another approach is to try to model already existing metrics with a unique set of features using machine learning techniques. The key to this approach is that with these features that are selected, we can try to gauge the effectiveness of these features combined, rather than using individual analysis in simple metric evaluation. If we have an accurate model, it can particularly help us determine the specifics of gameplan execution. In this paper, the statistic ORTG (Offensive Rating, developed by Dean Oliver) was found to have a correlation with different NBA playtypes using both a linear regress
    
[^108]: 使用多相CT结合神经距离和纹理感知变压器，提高胰腺癌预后预测的方法

    Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer. (arXiv:2308.00507v1 [eess.IV])

    [http://arxiv.org/abs/2308.00507](http://arxiv.org/abs/2308.00507)

    本文提出了一种新的方法来改善胰腺癌的预后预测。该方法使用了可学习的神经距离来描述肿瘤与血管之间的关系，并通过融合局部和全局特征来改进多相CT影像中的肿瘤纹理特征提取。实验表明，该方法在预后预测中取得了较好的效果。

    

    胰腺导管腺癌（PDAC）是一种高度致命的癌症，肿瘤-血管受累极大影响患者的可切除性和总体生存率。然而，当前的预后预测方法未能明确准确地调查肿瘤与附近重要血管之间的关系。本文提出了一种新颖的可学习的神经距离，描述了不同患者CT影像中肿瘤与血管之间的精确关系，并将其作为预后预测的主要特征。此外，不同于现有模型在动态对比增强CT成像上利用CNN或LSTM来利用肿瘤增强模式，我们通过融合局部和全局特征使用CNN和变压器模块，在多相对比增强CT中改进了动态与肿瘤相关的纹理特征提取，进一步增强了跨多相CT影像提取的特征。我们对所提出的方法进行了广泛评估和比较。

    Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal cancer in which the tumor-vascular involvement greatly affects the resectability and, thus, overall survival of patients. However, current prognostic prediction methods fail to explicitly and accurately investigate relationships between the tumor and nearby important vessels. This paper proposes a novel learnable neural distance that describes the precise relationship between the tumor and vessels in CT images of different patients, adopting it as a major feature for prognosis prediction. Besides, different from existing models that used CNNs or LSTMs to exploit tumor enhancement patterns on dynamic contrast-enhanced CT imaging, we improved the extraction of dynamic tumor-related texture features in multi-phase contrast-enhanced CT by fusing local and global features using CNN and transformer modules, further enhancing the features extracted across multi-phase CT images. We extensively evaluated and compared the proposed method
    
[^109]: Temporal Graph Benchmark的实证评估

    An Empirical Evaluation of Temporal Graph Benchmark. (arXiv:2307.12510v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12510](http://arxiv.org/abs/2307.12510)

    本文对Temporal Graph Benchmark进行了实证评估，通过扩展动态图库(DyGLib)到TGB，并使用十一种动态图学习方法进行全面比较。实验发现不同模型在不同数据集上表现出不同的性能，一些基线方法的性能可以通过使用DyGLib显著提高。

    

    本文通过将我们的动态图库(DyGLib)扩展到Temporal Graph Benchmark (TGB)，对TGB进行了实证评估。与TGB相比，我们包括了十一种流行的动态图学习方法进行更全面的比较。通过实验，我们发现：（1）不同模型在不同数据集上表现出不同的性能，这与之前的观察一致；（2）使用DyGLib时，一些基线方法的性能可以显著提高。本工作旨在方便研究人员在TGB上评估各种动态图学习方法，并试图提供可直接参考的结果供后续研究使用。本项目中使用的所有资源均可在https://github.com/yule-BUAA/DyGLib_TGB上公开获取。本工作正在进行中，欢迎社区提供反馈以进行改进。

    In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
    
[^110]: 用张量回归进行少样本个性化显著性预测，保留结构全局信息。

    Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information. (arXiv:2307.02799v1 [eess.IV])

    [http://arxiv.org/abs/2307.02799](http://arxiv.org/abs/2307.02799)

    本文提出了一种使用张量回归进行少样本个性化显著性预测的方法，以保留个性化显著性图的结构全局信息。

    

    本文提出了一种使用张量到矩阵回归进行少样本个性化显著性预测的方法，以保留个性化显著性图（PSM）的结构全局信息。与一般的显著性图相比，PSM具有巨大的潜力，因为它的映射指示了个体特定的视觉注意力，对于从凝视区域的异质性中获取个体视觉偏好非常有用。PSM的预测是为了获取未见图像的PSM，但由于个体凝视模式的复杂性，其预测仍然是一项具有挑战性的任务。为了从有限的眼动数据中识别个体凝视模式，先前的方法采用个体之间凝视趋势的相似性。然而，在先前的方法中，PSMs被向量化以适应预测模型，从而忽视了与图像对应的PSMs的结构全局信息。为了自动揭示PSMs之间的关系，我们聚焦于...

    This paper presents a few-shot personalized saliency prediction using tensor-to-matrix regression for preserving the structural global information of personalized saliency maps (PSMs). In contrast to a general saliency map, a PSM has been great potential since its map indicates the person-specific visual attention that is useful for obtaining individual visual preferences from heterogeneity of gazed areas. The PSM prediction is needed for acquiring the PSM for the unseen image, but its prediction is still a challenging task due to the complexity of individual gaze patterns. For recognizing individual gaze patterns from the limited amount of eye-tracking data, the previous methods adopt the similarity of gaze tendency between persons. However, in the previous methods, the PSMs are vectorized for the prediction model. In this way, the structural global information of the PSMs corresponding to the image is ignored. For automatically revealing the relationship between PSMs, we focus on the
    
[^111]: RL4CO: 用于组合优化的广泛强化学习基准测试

    RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark. (arXiv:2306.17100v1 [cs.LG])

    [http://arxiv.org/abs/2306.17100](http://arxiv.org/abs/2306.17100)

    RL4CO是一个用于组合优化的广泛强化学习基准测试，着重于可扩展性和泛化能力的评估，并展示了一些最新方法在样本效率和适应不同数据分布方面的表现相对较差，强调了对神经CO求解器性能的平衡评估的重要性。

    

    我们引入了RL4CO，这是一个广泛的强化学习（RL）用于组合优化（CO）的基准测试。RL4CO采用最先进的软件库和最佳实践，如模块化和配置管理，以便研究人员可以轻松修改神经网络架构、环境和算法。与现有的专注于特定任务（如旅行推销员问题）进行性能评估的方法不同，我们强调可扩展性和泛化能力对于各种优化任务的重要性。我们还系统地评估了各种模型在样本效率、零-shot泛化和适应不同数据分布方面的表现。我们的实验结果表明，一些最新的最先进方法在使用这些新指标进行评估时落后于之前的方法，这表明有必要更加平衡地评估神经CO求解器的性能。我们希望RL4CO能够为研究人员提供一个综合性的基准测试工具，以进一步推动强化学习在组合优化领域的研究。

    We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse optimization tasks. We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these new metrics, suggesting the necessity for a more balanced view of the performance of neural CO solvers. We hope RL4CO will 
    
[^112]: 在弗里德金－约翰森模型中的信息有限的对手

    Adversaries with Limited Information in the Friedkin--Johnsen Model. (arXiv:2306.10313v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2306.10313](http://arxiv.org/abs/2306.10313)

    这篇论文研究了在社交网络中，对手在只知道网络拓扑的情况下是否能够种下不和谐，并以此为基础提出了解决用户意见估计困难问题的方法。

    

    近年来，网络社交平台成为了寻求在社会中引入不和谐，破坏民主和破坏社区稳定的对手的目标。通常，目标不是支持某一方的冲突，而是增加分歧和极化。为了对这些攻击有数学上的理解，研究人员使用社会学中的意见形成模型，例如弗里德金－约翰森模型，并且正式研究当改变仅一小部分用户的意见时，对手可以产生多大的不和谐。然而，在实践中，这种假设通常是不现实的，用户的意见通常无法获得或者很难准确估计。为了解决这个问题，我们提出了以下问题：即使只知道网络拓扑，攻击者是否可以在社交网络中种下不和谐？

    In recent years, online social networks have been the target of adversaries who seek to introduce discord into societies, to undermine democracies and to destabilize communities. Often the goal is not to favor a certain side of a conflict but to increase disagreement and polarization. To get a mathematical understanding of such attacks, researchers use opinion-formation models from sociology, such as the Friedkin--Johnsen model, and formally study how much discord the adversary can produce when altering the opinions for only a small set of users. In this line of work, it is commonly assumed that the adversary has full knowledge about the network topology and the opinions of all users. However, the latter assumption is often unrealistic in practice, where user opinions are not available or simply difficult to estimate accurately.  To address this concern, we raise the following question: Can an attacker sow discord in a social network, even when only the network topology is known? We an
    
[^113]: 用于无目标代谢组学数据自动对齐的最优输运

    Optimal transport for automatic alignment of untargeted metabolomic data. (arXiv:2306.03218v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.03218](http://arxiv.org/abs/2306.03218)

    本文提出了一种名为GromovMatcher的算法，通过使用最优输运自动合并LC-MS数据集，可提高数据对齐的准确性和鲁棒性，有效解决代谢组学数据合并的挑战。

    

    液相色谱-质谱（LC-MS）通过测量生物标本中的大量代谢物推动药物研发，疾病诊断和风险预测的进展。然而，LC-MS的低通量对于生物标记物发现，注释和实验比较构成了主要挑战，需要合并多个数据集。当前的数据池化方法由于对数据变化和超参数依赖性的脆弱性而遇到实际限制。本文介绍了GromovMatcher，一种灵活且用户友好的算法，使用最优输运自动结合LC-MS数据集。通过利用特征强度相关结构，GromovMatcher提供了比现有方法更高的对齐准确性和鲁棒性。该算法可扩展到需要最小超参数调整的数千个特征。将我们的方法应用于肝癌和胰腺癌的实验患者研究

    Untargeted metabolomic profiling through liquid chromatography-mass spectrometry (LC-MS) measures a vast array of metabolites within biospecimens, advancing drug development, disease diagnosis, and risk prediction. However, the low throughput of LC-MS poses a major challenge for biomarker discovery, annotation, and experimental comparison, necessitating the merging of multiple datasets. Current data pooling methods encounter practical limitations due to their vulnerability to data variations and hyperparameter dependence. Here we introduce GromovMatcher, a flexible and user-friendly algorithm that automatically combines LC-MS datasets using optimal transport. By capitalizing on feature intensity correlation structures, GromovMatcher delivers superior alignment accuracy and robustness compared to existing approaches. This algorithm scales to thousands of features requiring minimal hyperparameter tuning. Applying our method to experimental patient studies of liver and pancreatic cancer, 
    
[^114]: 通过与大型语言模型的查询学习Horn包络

    Learning Horn Envelopes via Queries from Large Language Models. (arXiv:2305.12143v1 [cs.LG])

    [http://arxiv.org/abs/2305.12143](http://arxiv.org/abs/2305.12143)

    该研究提出了一种从经过训练的神经网络中提取知识的方法，可以学习到最紧Horn逼近目标理论的新算法，并在预训练的语言模型中运用于揭示基于职业的性别偏见规则。

    

    我们研究了一种从训练的神经网络中提取知识的方法，该方法基于Angluin的精确学习模型，使用成员和等价性查询到一个oracle。在这种方法中，oracle是一个经过训练的神经网络。我们考虑了Angluin用于学习Horn理论的经典算法，并研究了必要的变化，以使其适用于从神经网络中学习。特别地，我们必须考虑到经过训练的神经网络可能不会像Horn oracle那样行事，这意味着它们的潜在目标理论可能不是Horn。我们提出了一种旨在提取目标理论“最紧Horn逼近”的新算法，并保证在指数时间（在最坏情况下）内终止，在目标具有多项式数量的非Horn示例的情况下，在多项式时间内终止。为了展示这种方法的适用性，我们对预训练的语言模型进行实验，提取揭示基于职业的性别偏见的规则。

    We investigate an approach for extracting knowledge from trained neural networks based on Angluin's exact learning model with membership and equivalence queries to an oracle. In this approach, the oracle is a trained neural network. We consider Angluin's classical algorithm for learning Horn theories and study the necessary changes to make it applicable to learn from neural networks. In particular, we have to consider that trained neural networks may not behave as Horn oracles, meaning that their underlying target theory may not be Horn. We propose a new algorithm that aims at extracting the ``tightest Horn approximation'' of the target theory and that is guaranteed to terminate in exponential time (in the worst case) and in polynomial time if the target has polynomially many non-Horn examples. To showcase the applicability of the approach, we perform experiments on pre-trained language models and extract rules that expose occupation-based gender biases.
    
[^115]: 深度视觉和遗传生物测定用于少量图像数据珍稀物种分类

    Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species. (arXiv:2305.06695v1 [cs.CV])

    [http://arxiv.org/abs/2305.06695](http://arxiv.org/abs/2305.06695)

    本文提出了一种利用对齐的视觉-遗传推理空间来提高少量图像数据珍稀物种分类的方法，该方法通过深度嵌入模型实现对齐，适用于提高稀有物种的长尾识别，并且可以显著有益于仅基于视觉的稀有物种识别。

    

    在生物应用中，视觉和遗传生物测定通常用于识别物种和个体。然而，在计算上增强少量图像数据稀有类别的视觉分类方面，该领域尚未进行尝试。因此，本文提出了对齐的视觉-遗传推理空间，旨在隐式编码跨域关联以提高性能。我们首次证明了这种对齐可以通过深度嵌入模型实现，并且该方法直接适用于提高稀有物种的长尾识别（LTR）。我们通过应用于32个物种、超过30,000个浮游有孔虫壳的显微图像并与独立的遗传数据样本一起使用来实验室展现了该概念的效力。最重要的是，对从业者而言，我们展示了视觉-遗传对齐可以显著有益于仅基于视觉的稀有物种识别。

    Visual as well as genetic biometrics are routinely employed to identify species and individuals in biological applications. However, no attempts have been made in this domain to computationally enhance visual classification of rare classes with little image data via genetics. In this paper, we thus propose aligned visual-genetic inference spaces with the aim to implicitly encode cross-domain associations for improved performance. We demonstrate for the first time that such alignment can be achieved via deep embedding models and that the approach is directly applicable to boosting long-tailed recognition (LTR) particularly for rare species. We experimentally demonstrate the efficacy of the concept via application to microscopic imagery of 30k+ planktic foraminifer shells across 32 species when used together with independent genetic data samples. Most importantly for practitioners, we show that visual-genetic alignment can significantly benefit visual-only recognition of the rarest speci
    
[^116]: Spaiche: 将最先进的ASR模型扩展到瑞士德语方言

    Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects. (arXiv:2304.11075v1 [cs.CL])

    [http://arxiv.org/abs/2304.11075](http://arxiv.org/abs/2304.11075)

    本项目在瑞士德语方言ASR模型的研究中提供了有价值的思路，通过提出考虑语义距离的新颖损失函数，对OpenAI的Whisper模型进行微调，取得了优于当前先进成果的效果。

    

    最近自然语言处理方面的突破大大增加了ASR系统在我们日常生活中的存在。然而，对于许多低资源语言，由于难以获取相关数据，ASR模型仍需要改进。本项目旨在通过提供关于最近发布的瑞士德语语音数据集上最先进的ASR模型性能的见解，帮助推进瑞士德语方言ASR模型的研究。我们提出了一种新颖的损失函数，考虑了预测和基准标签之间的语义距离。通过对瑞士德语数据集对OpenAI的Whisper模型进行微调，我们超越了当前先进的成果。

    Recent breakthroughs in NLP largely increased the presence of ASR systems in our daily lives. However, for many low-resource languages, ASR models still need to be improved due in part to the difficulty of acquiring pertinent data. This project aims to help advance research in ASR models for Swiss German dialects, by providing insights about the performance of state-of-the-art ASR models on recently published Swiss German speech datasets. We propose a novel loss that takes into account the semantic distance between the predicted and the ground-truth labels. We outperform current state-of-the-art results by fine-tuning OpenAI's Whisper model on Swiss-German datasets.
    
[^117]: 基于原始-对偶语境贝叶斯优化的带时间平均约束的控制系统在线优化

    Primal-Dual Contextual Bayesian Optimization for Control System Online Optimization with Time-Average Constraints. (arXiv:2304.06104v1 [cs.LG])

    [http://arxiv.org/abs/2304.06104](http://arxiv.org/abs/2304.06104)

    提出了一种基于原始-对偶语境贝叶斯优化算法，可以实现对约束闭环控制系统的在线性能优化，同时满足所需的约束条件。

    

    本文研究带有外生时间变化上下文干扰的未知黑盒函数的约束闭环控制系统在线性能优化问题。提出了一种原始-对偶语境贝叶斯优化算法，在满足一定正则条件下，实现了对动态最优解的亚线性累积遗憾。此外，该算法可以实现零时间平均约束违规，确保了约束函数的平均值满足所需的约束条件。该方法应用于高斯过程的采样实例和连续搅拌槽反应器参数调节问题。仿真结果表明，该方法同时提供接近最优的性能和平均保持约束可行性，这与当前的最先进方法形成对比，后者要么遭受大量累积遗憾，要么存在严重约束违规问题。

    This paper studies the problem of online performance optimization of constrained closed-loop control systems, where both the objective and the constraints are unknown black-box functions affected by exogenous time-varying contextual disturbances. A primal-dual contextual Bayesian optimization algorithm is proposed that achieves sublinear cumulative regret with respect to the dynamic optimal solution under certain regularity conditions. Furthermore, the algorithm achieves zero time-average constraint violation, ensuring that the average value of the constraint function satisfies the desired constraint. The method is applied to both sampled instances from Gaussian processes and a continuous stirred tank reactor parameter tuning problem; simulation results show that the method simultaneously provides close-to-optimal performance and maintains constraint feasibility on average. This contrasts current state-of-the-art methods, which either suffer from large cumulative regret or severe const
    
[^118]: 从人类偏好学习灵巧机器人操作的普适人类先验

    Learning a Universal Human Prior for Dexterous Manipulation from Human Preference. (arXiv:2304.04602v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2304.04602](http://arxiv.org/abs/2304.04602)

    通过借鉴最新的强化学习从人类反馈中的方法，本论文提出了一种学习普适人类先验的框架，通过直接人类偏好反馈视频来调整机器人在各种任务中的策略。结果表明，该方法能够在仿真中表现出更加类人的行为，甚至在未见过的任务中也能取得良好效果。

    

    在机器人手灵巧操作任务中生成类似人类行为是一个巨大的挑战。由于高维的控制空间，从头开始编写策略很难，使用强化学习（RL）和手动奖励设计训练策略也难以实现并导致不自然的动作。借鉴强化学习从人类反馈中的最新进展，我们提出了一个框架，通过直接人类偏好反馈视频来学习一个普适的人类先验，以在仿真中有效调整20个双手机器人操作任务的RL策略，而无需进行任何人类示范。通过生成多样的策略并收集轨迹上的人类偏好，训练了一个任务不可知的奖励模型，然后在微调阶段用于规范策略的行为。我们的方法在各种任务中经验性地展示了机器人手的更类人行为，甚至包括未见过的任务。

    Generating human-like behavior on robots is a great challenge especially in dexterous manipulation tasks with robotic hands. Scripting policies from scratch is intractable due to the high-dimensional control space, and training policies with reinforcement learning (RL) and manual reward engineering can also be hard and lead to unnatural motions. Leveraging the recent progress on RL from Human Feedback, we propose a framework that learns a universal human prior using direct human preference feedback over videos, for efficiently tuning the RL policies on 20 dual-hand robot manipulation tasks in simulation, without a single human demonstration. A task-agnostic reward model is trained through iteratively generating diverse polices and collecting human preference over the trajectories; it is then applied for regularizing the behavior of polices in the fine-tuning stage. Our method empirically demonstrates more human-like behaviors on robot hands in diverse tasks including even unseen tasks,
    
[^119]: 您的扩散模型暗中是一种零样本分类器。

    Your Diffusion Model is Secretly a Zero-Shot Classifier. (arXiv:2303.16203v1 [cs.LG])

    [http://arxiv.org/abs/2303.16203](http://arxiv.org/abs/2303.16203)

    扩散模型的密度估计可以被用作零样本分类，作者的生成式分类方法在各种基准测试中取得强大的结果，并具有更强的多模式关系推理能力。

    

    最近大规模的文本到图像扩散模型极大地增强了我们的基于文本生成图像的能力。这些模型可以为大量提示生成逼真的图像，并展示出令人印象深刻的组合泛化能力。几乎所有的用例到目前为止都只关注抽样，然而，扩散模型还可以提供有用于图像生成之外的条件密度估计。在本文中，我们展示了类似于Stable Diffusion的大规模文本到图像扩散模型的密度估计可以被利用来执行零样本分类，而无需额外的训练。我们的生成式分类方法在各种基准测试中取得了强大的结果，并优于从扩散模型中提取知识的替代方法。我们还发现，我们基于扩散的方法比竞争性的对比方法具有更强的多模式关系推理能力。最后，我们评估了我们方法的可解释性，并呈现了定性结果，证明它学习了有意义的图像-文本对齐。

    The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. We also find that our diffusion-based approach has stronger multimodal relational reasoning abilities than competing contrastive approaches. Finally, we eva
    
[^120]: 基于稀疏高斯过程的连续和离散空间的回归传感器放置优化

    Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.00028](http://arxiv.org/abs/2303.00028)

    本文提出了一种用稀疏高斯过程解决连续空间下传感器放置问题的方法，避免离散化环境并降低了计算复杂度，可通过贪婪算法找到好的解决方案。

    

    本文提出了一种基于稀疏高斯过程方法的传感器放置方案，用于监测温度、降水等空间（或时空）相关现象。与现有的基于高斯过程的传感器放置方法不同，我们将已知内核函数参数的稀疏高斯过程拟合到环境中随机采样的未标记位置，并通过学习得到的诱导点来解决连续空间的传感器放置问题。使用稀疏高斯过程避免了对环境进行离散化，并将计算复杂度从立方级别降低到线性级别。在候选传感器放置点集合的限制下，我们可以使用贪婪顺序选择算法来找到较好的解决方案。

    We present a novel approach based on sparse Gaussian processes (SGPs) to address the sensor placement problem for monitoring spatially (or spatiotemporally) correlated phenomena such as temperature and precipitation. Existing Gaussian process (GP) based sensor placement approaches use GPs with known kernel function parameters to model a phenomenon and subsequently optimize the sensor locations in a discretized representation of the environment. In our approach, we fit an SGP with known kernel function parameters to randomly sampled unlabeled locations in the environment and show that the learned inducing points of the SGP inherently solve the sensor placement problem in continuous spaces. Using SGPs avoids discretizing the environment and reduces the computation cost from cubic to linear complexity. When restricted to a candidate set of sensor placement locations, we can use greedy sequential selection algorithms on the SGP's optimization bound to find good solutions. We also present a
    
[^121]: 不精确的贝叶斯神经网络

    Imprecise Bayesian Neural Networks. (arXiv:2302.09656v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09656](http://arxiv.org/abs/2302.09656)

    在机器学习和人工智能领域，该论文提出了一种新的算法——不精确的贝叶斯神经网络(IBNNs)。这种算法使用可信区间先验分布集合和似然分布集合进行训练，相比标准的BNNs，可以区分先验和后验的不确定性并量化。此外，IBNNs在贝叶斯灵敏度分析方面具有更强的鲁棒性，并且对分布变化也更加鲁棒。

    

    在机器学习和人工智能中, 确定不确定性和鲁棒性是重要的目标。虽然贝叶斯神经网络使得预测中的不确定性能够被评估，不同来源的不确定性是无法区分的。我们提出了不精确的贝叶斯神经网络（IBNNs），它们可以概括和克服标准BNNs的某些缺点。标准BNNs使用单一的先验分布和似然分布进行训练，而IBNNs使用可信区间先验分布和似然分布进行训练。它们允许区分先验和后验不确定性，并对其进行量化。此外，IBNNs在贝叶斯灵敏度分析方面具有鲁棒性，并且对分布变化比标准BNNs更加鲁棒。它们还可以用于计算具有PAC样本复杂性的结果集。我们将IBNNs应用于两个案例研究：一个是为了人工胰腺控制模拟血糖和胰岛素动力学，另一个是运动规划。

    Uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. Although Bayesian neural networks (BNNs) allow for uncertainty in the predictions to be assessed, different sources of uncertainty are indistinguishable. We present imprecise Bayesian neural networks (IBNNs); they generalize and overcome some of the drawbacks of standard BNNs. These latter are trained using a single prior and likelihood distributions, whereas IBNNs are trained using credal prior and likelihood sets. They allow to distinguish between aleatoric and epistemic uncertainties, and to quantify them. In addition, IBNNs are robust in the sense of Bayesian sensitivity analysis, and are more robust than BNNs to distribution shift. They can also be used to compute sets of outcomes that enjoy PAC-like properties. We apply IBNNs to two case studies. One, to model blood glucose and insulin dynamics for artificial pancreas control, and two, for motion p
    
[^122]: 基于惩罚的双层梯度下降方法研究

    On Penalty-based Bilevel Gradient Descent Method. (arXiv:2302.05185v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05185](http://arxiv.org/abs/2302.05185)

    本文提出了基于惩罚的双层梯度下降算法，解决了下层非强凸约束双层问题，实验表明该算法有效。

    This paper proposes a penalty-based bilevel gradient descent algorithm to solve the constrained bilevel problem without lower-level strong convexity, and experiments show its efficiency.

    双层优化在超参数优化、元学习和强化学习等领域有广泛应用，但是双层优化问题难以解决。最近的可扩展双层算法主要集中在下层目标函数是强凸或无约束的双层优化问题上。在本文中，我们通过惩罚方法来解决双层问题。我们证明，在一定条件下，惩罚重构可以恢复原始双层问题的解。此外，我们提出了基于惩罚的双层梯度下降（PBGD）算法，并证明了其在下层非强凸约束双层问题上的有限时间收敛性。实验展示了所提出的PBGD算法的效率。

    Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel optimization problems are difficult to solve. Recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent (PBGD) algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. Experiments showcase the efficiency of the proposed PBGD algorithm.
    
[^123]: 在图像中提取认知后门模式的方法: 一种用于后门样本检测的 SOTA 方法

    Distilling Cognitive Backdoor Patterns within an Image: A SOTA Method for Backdoor Sample Detection. (arXiv:2301.10908v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10908](http://arxiv.org/abs/2301.10908)

    本文提出了一种用于提取和检测图像中后门模式的简单方法，称为认知精炼（CD）。通过优化输入掩码，我们可以提取一个小模式，该模式可以导致模型产生相同的输出。使用CD和提取的模式，我们发现了后门攻击的一个有趣现象：后门样本的模式都非常小。所以可以利用学到的掩码从污染的训练数据集中检测和删除后门样本。

    

    本文提出了一种简单的方法来提取和检测图像中的后门模式: 认知精炼 (CD)。该方法通过优化输入掩码来提取输入图像中的小模式，该模式可以导致模型产生相同的输出。通过使用CD和提取的模式，我们揭示了后门攻击的一个有趣现象：尽管不同攻击使用不同形式和大小的触发模式，但后门样本的模式都惊人地小。因此，可以利用学到的掩码从被污染的训练数据集中检测和删除后门样本。我们进行了大量实验证明CD可以稳健地检测各种高级后门样本。

    This paper proposes a simple method to distill and detect backdoor patterns within an image: \emph{Cognitive Distillation} (CD). The idea is to extract the "minimal essence" from an input image responsible for the model's prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. One thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. We conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor a
    
[^124]: 使用计算机视觉和LSTM神经网络分析和预测太阳冕空洞

    Solar Coronal Hole Analysis and Prediction using Computer Vision and LSTM Neural Network. (arXiv:2301.06732v4 [astro-ph.SR] UPDATED)

    [http://arxiv.org/abs/2301.06732](http://arxiv.org/abs/2301.06732)

    本研究使用计算机视觉和LSTM神经网络分析和预测太阳冕空洞的大小和趋势，并据此为地球对太阳冕空洞的影响做准备。

    

    随着人类开始探索太空，太空天气的重要性变得明显。已经确立了太阳冕空洞这一种太空天气现象会对飞机和卫星的运作产生影响。太阳冕空洞是太阳上的一片区域，其特点是磁场线开放而温度相对较低，导致太阳风以高于平均速率进行发射。在本研究中，为了准备好应对太阳冕空洞对地球的影响，我们使用计算机视觉检测太阳动力学观测卫星（SDO）图像中的太阳冕空洞区域并计算其大小。我们对太阳每个区域的太阳冕空洞进行比较和相关性分析。然后，我们实施深度学习技术，特别是使用长短期记忆（LSTM）方法分析太阳冕空洞面积数据的趋势，并预测不同太阳区域未来7天的太阳冕空洞大小。通过分析太阳冕空洞面积的时间序列数据，本研究旨在辨识太阳冕空洞的变化趋势和预测其大小。

    As humanity has begun to explore space, the significance of space weather has become apparent. It has been established that coronal holes, a type of space weather phenomenon, can impact the operation of aircraft and satellites. The coronal hole is an area on the sun characterized by open magnetic field lines and relatively low temperatures, which result in the emission of the solar wind at higher than average rates. In this study, To prepare for the impact of coronal holes on the Earth, we use computer vision to detect the coronal hole region and calculate its size based on images from the Solar Dynamics Observatory (SDO). We compare the coronal holes for each region of the Sun and analyze the correlation. We then implement deep learning techniques, specifically the Long Short-Term Memory (LSTM) method, to analyze trends in the coronal hole area data and predict its size for different sun regions over 7 days. By analyzing time series data on the coronal hole area, this study aims to id
    
[^125]: Knockoffs-SPR: 无噪声标签学习中的干净样本选择

    Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels. (arXiv:2301.00545v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00545](http://arxiv.org/abs/2301.00545)

    本文提出了一种理论上保证的在学习中选择干净样本的框架Knockoffs-SPR，通过Scalable Penalized Regression（SPR）方法模型化网络特征和标签之间的线性关系，并且通过Knockoff过滤器控制了干净数据的误选择率。

    

    噪声训练集通常会导致神经网络的泛化能力和鲁棒性的降低。本文提出了一种新颖的在带有噪声标签的学习中，理论上保证的干净样本选择框架。具体地，我们首先提出了一种可扩展的惩罚回归（SPR）方法，用于建模网络特征和独热标签之间的线性关系。在SPR中，通过在回归模型中求解的零均值漂移参数来确定干净数据。我们理论上证明了在一些条件下SPR能够恢复干净数据。在一般情况下，这些条件可能不再满足；并且一些噪声数据被错误地选择为干净数据。为了解决这个问题，我们提出了一种基于Knockoff过滤器的数据自适应方法来控制选取的干净数据的误选择率（FSR）的Knockoffs-SPR。为了提高效率，我们进一步提出了一种分割算法，将整个训练集分割为多个子集。

    A noisy training set usually leads to the degradation of the generalization and robustness of neural networks. In this paper, we propose a novel theoretically guaranteed clean sample selection framework for learning with noisy labels. Specifically, we first present a Scalable Penalized Regression (SPR) method, to model the linear relation between network features and one-hot labels. In SPR, the clean data are identified by the zero mean-shift parameters solved in the regression model. We theoretically show that SPR can recover clean data under some conditions. Under general scenarios, the conditions may be no longer satisfied; and some noisy data are falsely selected as clean data. To solve this problem, we propose a data-adaptive method for Scalable Penalized Regression with Knockoff filters (Knockoffs-SPR), which is provable to control the False-Selection-Rate (FSR) in the selected clean data. To improve the efficiency, we further present a split algorithm that divides the whole trai
    
[^126]: ColD Fusion: 协同下降的分布式多任务微调方法

    ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning. (arXiv:2212.01378v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01378](http://arxiv.org/abs/2212.01378)

    ColD Fusion是一种协同下降的分布式多任务微调方法，通过利用分布式计算，可以不断改进预训练模型，并在各种数据集上表现良好，优于RoBERTa模型。

    

    我们提出了一种新的范式来不断演进预训练模型，称为ColD Fusion。它具有多任务学习的优势，但利用有限通信的分布式计算，并且消除了共享数据的需求。因此，ColD Fusion可以形成一个协同循环，其中微调模型可以循环利用，不断改进它们所基于的预训练模型。我们展示了ColD Fusion产生了与多任务训练相当的好处，通过产生一个在所有训练数据集上表现良好并且在未见数据集上进行微调的更好的起点模型。我们展示了ColD Fusion优于RoBERTa甚至以前的多任务模型。具体来说，在使用35个不同数据集进行训练和测试时，ColD Fusion-based模型在不改变架构的情况下平均优于RoBERTa 2.33个点。

    We propose a new paradigm to continually evolve pretrained models, denoted ColD Fusion. It provides the benefits of multitask learning but leverages distributed computation with limited communication and eliminates the need for shared data. Consequentially, ColD Fusion can give rise to a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based upon. We show that ColD Fusion yields comparable benefits to multitask training by producing a model that (a) attains strong performance on all of the datasets it was trained on; and (b) is a better starting point for finetuning on unseen datasets. We show that ColD Fusion outperforms RoBERTa and even previous multitask models. Specifically, when training and testing on 35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.33 points on average without any changes to the architecture.
    
[^127]: 基于去中心化的联邦学习: 基础、现状、框架、趋势和挑战 (arXiv:2211.08413v2 [cs.LG] UPDATED)

    Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges. (arXiv:2211.08413v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08413](http://arxiv.org/abs/2211.08413)

    本文提出去中心化联邦学习（DFL）来解决传统中心化FL（CFL）模型中的问题，主要研究DFL与CFL的差异、DFL的基础理论、DFL框架的设计与评估以及DFL在实际应用中的场景。

    

    在过去的十年中，联邦学习（FL）已经成为在不共享敏感数据的情况下训练协作模型的一种重要方法。自问世以来，中心化FL（CFL）一直是文献中最常见的方法，其中一个中心化实体创建全局模型。然而，中心化方法会导致瓶颈增加、系统故障风险增高，影响负责创建全局模型的实体的可信度。去中心化联邦学习（DFL）应运而生，通过推广去中心化模型聚合并最小化对中心化架构的依赖，解决了这些问题。但是，尽管在DFL方面有所努力，文献还没有研究(i)DFL和CFL之间的主要差异;(ii)分析DFL框架以创建和评估新解决方案;(iii)回顾使用DFL的应用场景。因此，本文在联邦架构、安全性、通信等方面识别并分析了DFL的主要基础。

    In the last decade, Federated Learning (FL) has gained relevance in training collaborative models without sharing sensitive data. Since its birth, Centralized FL (CFL) has been the most common approach in the literature, where a central entity creates a global model. However, a centralized approach leads to increased latency due to bottlenecks, heightened vulnerability to system failures, and trustworthiness concerns affecting the entity responsible for the global model creation. Decentralized Federated Learning (DFL) emerged to address these concerns by promoting decentralized model aggregation and minimizing reliance on centralized architectures. However, despite the work done in DFL, the literature has not (i) studied the main aspects differentiating DFL and CFL; (ii) analyzed DFL frameworks to create and evaluate new solutions; and (iii) reviewed application scenarios using DFL. Thus, this article identifies and analyzes the main fundamentals of DFL in terms of federation architect
    
[^128]: 图神经网络在稠密和稀疏图上的谱分析

    A Spectral Analysis of Graph Neural Networks on Dense and Sparse Graphs. (arXiv:2211.03231v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2211.03231](http://arxiv.org/abs/2211.03231)

    本研究通过分析稀疏性对图谱的影响，发现图神经网络在稀疏图上能够优于谱方法的性能。

    

    本研究提出了一个可以产生不同稀疏程度的随机图模型。我们分析了稀疏性对图谱的影响，以及这对于节点分类任务中图神经网络（GNN）的性能影响在稠密和稀疏图上的表现。我们将GNN与已知在稠密图上提供一致估计的谱方法进行比较，这也是一个相关的任务。我们表明，GNN在稀疏图上可以超越谱方法的性能，并通过合成和真实图的数值示例来说明这些结果。

    In this work we propose a random graph model that can produce graphs at different levels of sparsity. We analyze how sparsity affects the graph spectra, and thus the performance of graph neural networks (GNNs) in node classification on dense and sparse graphs. We compare GNNs with spectral methods known to provide consistent estimators for community detection on dense graphs, a closely related task. We show that GNNs can outperform spectral methods on sparse graphs, and illustrate these results with numerical examples on both synthetic and real graphs.
    
[^129]: CTRL: 针对标签错误检测进行聚类训练损失

    CTRL: Clustering Training Losses for Label Error Detection. (arXiv:2208.08464v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.08464](http://arxiv.org/abs/2208.08464)

    CTRL是一种用于检测多类别数据集中标签错误的新框架，通过聚类训练损失将样本分为干净标记和有噪声标记两类，并通过去除有噪声标签的样本来提高模型的效果。实验结果表明，该方法在图像和文本数据集上具有最先进的错误检测准确性。

    

    在监督式机器学习中，正确的标签使用非常重要，以确保高准确性。不幸的是，大多数数据集都包含有损标签。在这些数据集上训练的机器学习模型不具备良好的泛化能力。因此，检测其标签错误可以极大地提高其效果。我们提出了一种名为CTRL（Clustering TRaining Losses for label error detection）的新框架，用于检测多类别数据集中的标签错误。它根据模型以不同方式学习干净和有噪声标签的观察，通过两个步骤检测标签错误。首先，我们使用有噪声训练数据集训练神经网络并获得每个样本的损失曲线。然后，我们将聚类算法应用于训练损失，将样本分为两类：干净标记和有噪声标记。在标签错误检测后，我们删除具有噪声标签的样本并重新训练模型。我们的实验结果在图像数据集和文本数据集上展现出了最先进的错误检测准确性。

    In supervised machine learning, use of correct labels is extremely important to ensure high accuracy. Unfortunately, most datasets contain corrupted labels. Machine learning models trained on such datasets do not generalize well. Thus, detecting their label errors can significantly increase their efficacy. We propose a novel framework, called CTRL (Clustering TRaining Losses for label error detection), to detect label errors in multi-class datasets. It detects label errors in two steps based on the observation that models learn clean and noisy labels in different ways. First, we train a neural network using the noisy training dataset and obtain the loss curve for each sample. Then, we apply clustering algorithms to the training losses to group samples into two categories: cleanly-labeled and noisily-labeled. After label error detection, we remove samples with noisy labels and retrain the model. Our experimental results demonstrate state-of-the-art error detection accuracy on both image
    
[^130]: 具有量子随机访问存储器和量子网络的数据中心

    Data centers with quantum random access memory and quantum networks. (arXiv:2207.14336v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2207.14336](http://arxiv.org/abs/2207.14336)

    本文提出了将量子随机访问存储器（QRAM）和量子网络结合起来的量子数据中心（QDC）架构，并讨论了其在量子计算、量子通信和量子感知方面的应用。这将为未来的数据中心提供高效、私密和快速的服务。

    

    本文提出了量子数据中心（QDC）的架构，将量子随机访问存储器（QRAM）和量子网络结合起来。我们给出了QDC的精确定义，并讨论了它的可能实现和扩展。我们讨论了QDC在量子计算、量子通信和量子感知中的应用，重点关注了用于$T$门资源的QDC、用于多方私密量子通信的QDC以及通过数据压缩进行分布式感知的QDC。我们展示了QDC将作为数据中心的未来版本，提供高效、私密和快速的服务。

    In this paper, we propose the Quantum Data Center (QDC), an architecture combining Quantum Random Access Memory (QRAM) and quantum networks. We give a precise definition of QDC, and discuss its possible realizations and extensions. We discuss applications of QDC in quantum computation, quantum communication, and quantum sensing, with a primary focus on QDC for $T$-gate resources, QDC for multi-party private quantum communication, and QDC for distributed sensing through data compression. We show that QDC will provide efficient, private, and fast services as a future version of data centers.
    
[^131]: Perseus:一个求解变分不等式的简单且最优高阶方法

    Perseus: A Simple and Optimal High-Order Method for Variational Inequalities. (arXiv:2205.03202v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2205.03202](http://arxiv.org/abs/2205.03202)

    Perseus 解决了一个关于光滑和单调变分不等式的简单且最优高阶方法的设计问题。在大规模应用中具有实际应用价值。

    

    该论文解决了一个关于设计简单和最优高阶方法以求解光滑且单调的变分不等式的开放且具有挑战性的问题。变分不等式涉及寻找$x^\star \in \mathcal{X}$，使得对于所有的$x \in \mathcal{X}$，$\langle F(x), x - x^\star\rangle \geq 0$。我们考虑了$F$具有最多$(p-1)$阶导数的情况。对于$p=2$，我们扩展了立方正则化的牛顿方法以满足全局速率为$O(\epsilon^{-1})$的变分不等式。通过一种另外的二阶方法，我们可以获得$O(\epsilon^{-2/3}\log\log(1/\epsilon))$的改进速率，但该方法需要一个非平凡的内循环线搜索过程。类似地，基于线搜索过程的高阶方法已被证明可以达到$O(\epsilon^{-2/(p+1)}\log\log(1/\epsilon))$的速率。然而，正如Nesterov所强调的，这样的过程不一定意味着在大规模应用中具有实际应用价值，因此我们需要一种简单且最优的高阶方法。

    This paper settles an open and challenging question pertaining to the design of simple and optimal high-order methods for solving smooth and monotone variational inequalities (VIs). A VI involves finding $x^\star \in \mathcal{X}$ such that $\langle F(x), x - x^\star\rangle \geq 0$ for all $x \in \mathcal{X}$. We consider the setting in which $F$ is smooth with up to $(p-1)^{th}$-order derivatives. For $p = 2$, the cubic regularized Newton method was extended to VIs with a global rate of $O(\epsilon^{-1})$. An improved rate of $O(\epsilon^{-2/3}\log\log(1/\epsilon))$ can be obtained via an alternative second-order method, but this method requires a nontrivial line-search procedure as an inner loop. Similarly, high-order methods based on line-search procedures have been shown to achieve a rate of $O(\epsilon^{-2/(p+1)}\log\log(1/\epsilon))$. As emphasized by Nesterov, however, such procedures do not necessarily imply practical applicability in large-scale applications, and it would be de
    
[^132]: 分布式CPU/GPU架构上的超内存非负矩阵分解(NMF)

    Distributed Out-of-Memory NMF on CPU/GPU Architectures. (arXiv:2202.09518v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2202.09518](http://arxiv.org/abs/2202.09518)

    提出了一种分布式超内存非负矩阵分解(NMF)算法，可以在CPU/GPU架构上实现高效计算。算法通过稀疏和稠密矩阵操作以及批处理/平铺策略，有效地处理超内存问题，并利用CUDA流进行数据传输和异步计算。

    

    我们提出了一种高效的分布式超内存实现的非负矩阵分解(NMF)算法，用于异构高性能计算(HPC)系统。该实现基于NMFk的先前工作，可以自动进行模型选择并从数据中提取潜在变量和模式。在本研究中，我们通过添加对多节点、多GPU系统的稠密和稀疏矩阵操作支持，扩展了NMFk。得到的算法针对超内存问题进行了优化，其中所需内存大于可用的GPU内存来进行矩阵分解。通过批处理/平铺策略降低内存复杂度，并使用GPU核心(或者可用的张量核心)显著加速稀疏和稠密矩阵操作。使用CUDA流隐藏了主机和设备之间的批处理复制的输入/输出(I/O)延迟，以实现数据传输和异步计算的重叠，以及与收集相关的延迟。

    We propose an efficient distributed out-of-memory implementation of the Non-negative Matrix Factorization (NMF) algorithm for heterogeneous high-performance-computing (HPC) systems. The proposed implementation is based on prior work on NMFk, which can perform automatic model selection and extract latent variables and patterns from data. In this work, we extend NMFk by adding support for dense and sparse matrix operation on multi-node, multi-GPU systems. The resulting algorithm is optimized for out-of-memory (OOM) problems where the memory required to factorize a given matrix is greater than the available GPU memory. Memory complexity is reduced by batching/tiling strategies, and sparse and dense matrix operations are significantly accelerated with GPU cores (or tensor cores when available). Input/Output (I/O) latency associated with batch copies between host and device is hidden using CUDA streams to overlap data transfers and compute asynchronously, and latency associated with collect
    
[^133]: 一种用于众包的工人-任务特化模型：高效推断与基本限制

    A Worker-Task Specialization Model for Crowdsourcing: Efficient Inference and Fundamental Limits. (arXiv:2111.12550v3 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2111.12550](http://arxiv.org/abs/2111.12550)

    本论文介绍了一种用于众包的工人-任务特化模型，该模型解决了在多个不准确答案中推断正确标签的问题，并考虑了任务和工人的特化类型以及其可靠性变化。用于估计纠正答案达到最优样本复杂度的能力。

    

    众包系统已成为一种有效的平台，通过使用非专家工人以相对较低的成本为数据标注。然而，从多个不准确答案中推断正确的标签一直是一个具有挑战性的问题，因为答案的质量在任务和工人之间差异较大。许多现有工作假设工人的技能水平存在固定的排序，并着重于估计工人技能以汇总来自具有不同权重的工人的答案。然而，在实践中，特别是在任务异质性很大时，工人的技能在任务间变化很大。在本文中，我们考虑了一种新模型，称为$d$-type特化模型，其中每个任务和工人都有自己的（未知）类型，并且每个工人的可靠性可以在给定任务的类型和工人的类型上变化。我们允许类型数$d$随着任务数量的增加而扩展。在这个模型中，我们表征了使纠正答案达到最优样本复杂度的能力。

    Crowdsourcing system has emerged as an effective platform for labeling data with relatively low cost by using non-expert workers. Inferring correct labels from multiple noisy answers on data, however, has been a challenging problem, since the quality of the answers varies widely across tasks and workers. Many existing works have assumed that there is a fixed ordering of workers in terms of their skill levels, and focused on estimating worker skills to aggregate the answers from workers with different weights. In practice, however, the worker skill changes widely across tasks, especially when the tasks are heterogeneous. In this paper, we consider a new model, called $d$-type specialization model, in which each task and worker has its own (unknown) type and the reliability of each worker can vary in the type of a given task and that of a worker. We allow that the number $d$ of types can scale in the number of tasks. In this model, we characterize the optimal sample complexity to correct
    
[^134]: 签名和相关性学习

    Sign and Relevance learning. (arXiv:2110.07292v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.07292](http://arxiv.org/abs/2110.07292)

    本文提出了一种新型网络，在整个网络中仅传播可塑性变化的符号，同时使用神经调制控制学习速率。研究结果表明该范例可以成功学习具有多层表示的复杂任务，学习速度和稳定性优于标准模型。

    

    传统的生物仿真和生物启发式强化学习模型应用全局误差信号，这意味着使用浅层网络。然而，误差反向传播允许使用多层网络。本研究引入了一种新型网络，通过在整个网络中仅传播可塑性变化的符号（即LTP / LTD），同时使用神经调制来控制学习速率来解决这个问题。神经调制可以被理解为一个修正的误差或相关信号，而错误信号的自上而下符号决定长期增强还是长期抑制将发生。为了证明这种方法的有效性，我们进行了一个真实的机器人任务作为概念验证。我们的结果表明，该范例可以成功学习具有多层表示的复杂任务，并在学习速度和稳定性方面优于标准模型。

    Standard models of biologically realistic or biologically inspired reinforcement learning employ a global error signal, which implies the use of shallow networks. On the other hand, error backpropagation allows the use of networks with multiple layers. However, precise error backpropagation is difficult to justify in biologically realistic networks because it requires precise weighted error backpropagation from layer to layer. In this study, we introduce a novel network that solves this problem by propagating only the sign of the plasticity change (i.e., LTP/LTD) throughout the whole network, while neuromodulation controls the learning rate. Neuromodulation can be understood as a rectified error or relevance signal, while the top-down sign of the error signal determines whether long-term potentiation or long-term depression will occur. To demonstrate the effectiveness of this approach, we conducted a real robotic task as proof of concept. Our results show that this paradigm can success
    
[^135]: RIFLE: 从低阶边缘估计中补全缺失值和稳健推断

    RIFLE: Imputation and Robust Inference from Low Order Marginals. (arXiv:2109.00644v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.00644](http://arxiv.org/abs/2109.00644)

    RIFLE是一个统计推断框架，可以在缺失数据的情况下进行回归和分类。它通过估计底层数据分布的低阶矩和置信区间来学习一个分布鲁棒的模型，而无需进行数据补全。

    

    实际数据集中缺失值的普遍存在给统计推断带来了挑战，并且可能阻止相似数据集进行相同的研究分析，使得许多现有的数据集不能用于新的分析。目前已经开发了大量的数据补全包和算法，但绝大多数在存在大量缺失值和样本量较小的情况下表现不佳，而这在实证数据中是常见的特征。这种低准确度的估计会对下游的统计模型性能产生不利影响。我们开发了一个统计推断框架，用于在缺失数据的情况下进行回归和分类。我们的框架RIFLE（通过低阶矩估计进行稳健推断）通过估计底层数据分布的低阶矩和相应的置信区间来学习一个分布鲁棒的模型，而无需进行数据补全。我们将我们的框架专门应用于线性回归和...

    The ubiquity of missing values in real-world datasets poses a challenge for statistical inference and can prevent similar datasets from being analyzed in the same study, precluding many existing datasets from being used for new analyses. While an extensive collection of packages and algorithms have been developed for data imputation, the overwhelming majority perform poorly if there are many missing values and low sample sizes, which are unfortunately common characteristics in empirical data. Such low-accuracy estimations adversely affect the performance of downstream statistical models. We develop a statistical inference framework for regression and classification in the presence of missing data without imputation. Our framework, RIFLE (Robust InFerence via Low-order moment Estimations), estimates low-order moments of the underlying data distribution with corresponding confidence intervals to learn a distributionally robust model. We specialize our framework to linear regression and n
    
[^136]: 非负神经网络的不动点

    Fixed points of nonnegative neural networks. (arXiv:2106.16239v7 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.16239](http://arxiv.org/abs/2106.16239)

    本文利用不动点理论分析非负神经网络，证明了具有非负权重和偏置的非负神经网络存在输入和输出维度相同的不动点，并证明了其不动点集形状为区间。这些结果有助于对非负神经网络的理解。

    

    我们利用不动点理论分析非负神经网络，将其定义为将非负向量映射为非负向量的神经网络。我们首先证明了具有非负权重和偏置的非负神经网络可以在非线性Perron-Frobenius理论框架下被认为是单调且(弱)可扩展的函数。这个事实使我们能够提供非负神经网络存在输入和输出维度相同的不动点的条件，这些条件比最近在凸分析中使用的论证要弱。此外，我们证明了具有非负权重和偏置的非负神经网络的不动点集的形状是一个区间，在温和条件下退化为一个点。然后，我们利用这些结果得到更一般的非负神经网络存在不动点的结论。从实际的角度来看，我们的结果有助于对非负神经网络的理解。

    We use fixed point theory to analyze nonnegative neural networks, which we define as neural networks that map nonnegative vectors to nonnegative vectors. We first show that nonnegative neural networks with nonnegative weights and biases can be recognized as monotonic and (weakly) scalable functions within the framework of nonlinear Perron-Frobenius theory. This fact enables us to provide conditions for the existence of fixed points of nonnegative neural networks having inputs and outputs of the same dimension, and these conditions are weaker than those recently obtained using arguments in convex analysis. Furthermore, we prove that the shape of the fixed point set of nonnegative neural networks with nonnegative weights and biases is an interval, which under mild conditions degenerates to a point. These results are then used to obtain the existence of fixed points of more general nonnegative neural networks. From a practical perspective, our results contribute to the understanding of th
    
[^137]: 神经涡旋方法：从有限拉格朗日粒子到无限维欧拉动力学

    Neural Vortex Method: from Finite Lagrangian Particles to Infinite Dimensional Eulerian Dynamics. (arXiv:2006.04178v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2006.04178](http://arxiv.org/abs/2006.04178)

    神经涡旋方法（NVM）是一种新颖的学习-based 框架，用于重构高分辨率的欧拉流场，它通过构建神经网络描述拉格朗日涡旋结构和其相互作用动力学，解决了将连续流场映射到离散涡旋粒子的挑战。

    

    在流体数值分析领域，存在一个长期以来的问题：缺乏一种严格的数学工具将连续流场映射到离散的涡旋粒子，从而使拉格朗日粒子无法继承大规模欧拉求解器的高分辨率。为了解决这个挑战，我们提出了一种新颖的基于学习的框架，即神经涡旋方法（NVM），它通过构建拉格朗日涡旋结构及其相互作用动力学的神经网络描述，以物理精确的方式重构高分辨率的欧拉流场。我们基础设施的关键组成部分包括两个网络：一个涡旋表示网络用于识别基于网格的速度场中的拉格朗日涡旋，以及一个涡旋相互作用网络用于学习这些有限结构的潜在控制动力学。通过将这两个网络嵌入涡度到速度泊松求解器并使用高保真度的数据训练其参数。

    In the field of fluid numerical analysis, there has been a long-standing problem: lacking of a rigorous mathematical tool to map from a continuous flow field to discrete vortex particles, hurdling the Lagrangian particles from inheriting the high resolution of a large-scale Eulerian solver. To tackle this challenge, we propose a novel learning-based framework, the Neural Vortex Method (NVM), which builds a neural-network description of the Lagrangian vortex structures and their interaction dynamics to reconstruct the high-resolution Eulerian flow field in a physically-precise manner. The key components of our infrastructure consist of two networks: a vortex representation network to identify the Lagrangian vortices from a grid-based velocity field and a vortex interaction network to learn the underlying governing dynamics of these finite structures. By embedding these two networks with a vorticity-to-velocity Poisson solver and training its parameters using the high-fidelity data obtai
    
[^138]: 用射线进行最近邻点集抽样

    Nearest Neighbor Sampling of Point Sets using Rays. (arXiv:1911.10737v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/1911.10737](http://arxiv.org/abs/1911.10737)

    本论文提出了一种新的框架用于最近邻的点集抽样，所涉及的RaySense草图可以捕捉点的基本几何形态以及提取与之相关的统计信息，且可高效地进行点集上的线积分计算。

    

    我们提出了一个新的框架，用于抽样、压缩和分析欧几里得空间中嵌入的点集和其他几何对象的分布。我们的方法涉及构建一种称为RaySense草图的张量，该张量捕捉沿着一组射线的点的基本几何形态的最近邻居。我们探讨了可以在RaySense草图上执行的各种操作，从而导致不同的属性和潜在的应用。可以在不考虑射线集的情况下从草图中提取有关数据集的统计信息。使用草图可以高效地计算点集上的线积分。我们还提供了几个示例，说明了所提出策略在实际情况下的应用。

    We propose a new framework for the sampling, compression, and analysis of distributions of point sets and other geometric objects embedded in Euclidean spaces. Our approach involves the construction of a tensor called the RaySense sketch, which captures the nearest neighbors from the underlying geometry of points along a set of rays. We explore various operations that can be performed on the RaySense sketch, leading to different properties and potential applications. Statistical information about the data set can be extracted from the sketch, independent of the ray set. Line integrals on point sets can be efficiently computed using the sketch. We also present several examples illustrating applications of the proposed strategy in practical scenarios.
    

