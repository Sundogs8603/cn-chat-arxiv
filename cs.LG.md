# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning.](http://arxiv.org/abs/2309.06440) | LEAP Hand是一种低成本、高效且拟人化的机器学习手。其新颖的运动结构可以实现最大的灵活性，其成本为其他竞争对手的1/8。LEAP Hand在现实世界中执行多个操作任务，并在所有实验中表现出色。 |
| [^2] | [Unveiling the potential of large language models in generating semantic and cross-language clones.](http://arxiv.org/abs/2309.06424) | 本研究利用GPT-3模型在语义和跨语言克隆生成方面的潜力，通过评估、分析和验证，揭示了其在生成给定代码片段的语义和跨语言克隆变体方面的表现。 |
| [^3] | [Verifiable Reinforcement Learning Systems via Compositionality.](http://arxiv.org/abs/2309.06420) | 本研究提出了一个可验证和组合的强化学习框架，通过将多个强化学习子系统组合起来实现整体任务。通过定义子系统之间的接口，实现了任务规范的自动分解，并允许子系统的独立训练和测试。 |
| [^4] | [On Computationally Efficient Learning of Exponential Family Distributions.](http://arxiv.org/abs/2309.06413) | 本研究提出了一种计算高效的估计器，用于准确学习具有任意精度的自然参数的指数族分布。该估计器是一致的、渐近正态的，并可视为最大似然估计的重新参数化分布。 |
| [^5] | [Computational Approaches for Predicting Drug-Disease Associations: A Comprehensive Review.](http://arxiv.org/abs/2309.06388) | 这篇综述文章介绍了计算方法在药物再定位中预测药物-疾病关联方面的最新进展，并比较了不同方法的预测性能。 |
| [^6] | [Ensemble Mask Networks.](http://arxiv.org/abs/2309.06382) | 本研究引入了两种机制，灵活的掩模和独特的网络剪枝，使得一个前馈网络能够学习矩阵向量乘法，并且在图形模型中可以用来测试依赖关系或交互顺序。 |
| [^7] | [InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation.](http://arxiv.org/abs/2309.06380) | 本研究提出了InstaFlow，一种基于扩散的文本到图像生成方法，将稳定扩散模型转化为一步模型，通过修正的流动方法提高了噪声与图像之间的对应关系，实现了高质量、高速度的图像生成。 |
| [^8] | [Adversarial attacks on hybrid classical-quantum Deep Learning models for Histopathological Cancer Detection.](http://arxiv.org/abs/2309.06377) | 该论文介绍了对组织病理学癌症检测的混合经典-量子深度学习模型进行对抗攻击的研究，通过使用多个传统模型和量子迁移学习模型的结合，提供了对比分析性能，并评估了在各种对抗攻击下的鲁棒性。 |
| [^9] | [Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models.](http://arxiv.org/abs/2309.06375) | 建模推荐系统生态系统需要考虑参与者激励、行为以及策略引发的相互作用，通过强化学习进行长期优化，使用社会选择方法进行权衡，并减少信息不对称。 |
| [^10] | [Using Reed-Muller Codes for Classification with Rejection and Recovery.](http://arxiv.org/abs/2309.06359) | 本文提出了一种使用Reed-Muller码的分类方法，该方法可以纠正和拒绝输入，并在不同程度的对抗攻击下具有良好的正确性。 |
| [^11] | [Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors.](http://arxiv.org/abs/2309.06349) | 这项研究对使用分数后验概率的汤普森抽样算法进行了广义遗憾分析，获得了依赖于实例和实例独立的频率遗憾界。这对多臂赌博问题的解决有重要意义。 |
| [^12] | [Band-gap regression with architecture-optimized message-passing neural networks.](http://arxiv.org/abs/2309.06348) | 这项工作使用优化的消息传递神经网络（MPNN）进行带隙回归预测，通过对密度泛函理论数据进行分类和神经架构搜索，实现了对非金属材料带隙的准确预测，并在不确定性量化方面取得了显著进展。 |
| [^13] | [Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning.](http://arxiv.org/abs/2309.06315) | 本论文提出了一种新的Tsetlin机器（TM）反馈方案，通过引入马尔科夫边界来学习最简化的Tsetlin机器子句，以提供最佳的特征集。 |
| [^14] | [Semantic and Articulated Pedestrian Sensing Onboard a Moving Vehicle.](http://arxiv.org/abs/2309.06313) | 该论文研究了在行驶车辆上进行语义和关节人体感知的挑战，指出车载视频的大量前进运动导致难以进行3D重建，以及物体检测和人体感知模型在车载视频上表现较差的原因。作者提出通过LiDAR数据进行关节人体感知的基准可以促进人体感知和交通预测的研究，并有可能提高行人交通安全性。 |
| [^15] | [Modeling Supply and Demand in Public Transportation Systems.](http://arxiv.org/abs/2309.06299) | 该论文在公共交通系统中建立了供需模型，利用数据分析和机器学习技术揭示了运营服务中的空缺。 |
| [^16] | [Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition.](http://arxiv.org/abs/2309.06286) | 本论文提出了一种三步知识迁移性分析框架来支持数据驱动的增材制造知识的迁移。研究发现，通过利用各种增材制造技术之间的相似性和应用迁移学习的方法，可以将现有的解决方案从一个工艺或问题转移到另一个工艺或问题中。 |
| [^17] | [ELRA: Exponential learning rate adaption gradient descent optimization method.](http://arxiv.org/abs/2309.06274) | 提出了一种快速、从基础原理出发且无超参数依赖的基于梯度的优化算法，通过情境感知调整学习率，具有高成功率和快速收敛速度，并且具备旋转不变性。 |
| [^18] | [ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation.](http://arxiv.org/abs/2309.06268) | 本研究引入了一种自监督DNN方法，用于估计前列腺肿瘤的VERDICT模型参数并减少计算成本。 |
| [^19] | [Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning.](http://arxiv.org/abs/2309.06260) | 本研究提出了一种使用强化学习方法开发离散一致性闭塞方案的新方法，其中将大涡模拟中的闭塞模型系数调整任务定义为马尔可夫决策过程，通过后期的强化学习解决。这一方法能够将模型调整到实际的离散化中并考虑离散化和模型本身之间的相互作用。通过优化显式和隐式闭塞模型中的局部涡粘度模型和混合策略，实现了闭塞模型的优化。 |
| [^20] | [Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models.](http://arxiv.org/abs/2309.06256) | 本研究实证了基础模型微调中的灾难性遗忘现象，微调过程中追求专业性会导致模型的广泛性损失。 |
| [^21] | [Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation.](http://arxiv.org/abs/2309.06255) | 本文提出了一种精细的模态评估指标，用于评估每个模态在样本级别的贡献，并发现多模态模型倾向于依赖一个特定的模态，导致其他模态的贡献较低。 |
| [^22] | [Rethinking Evaluation Metric for Probability Estimation Models Using Esports Data.](http://arxiv.org/abs/2309.06248) | 本研究首次提出了使用布里尔分数和预期校准误差作为评估电竞领域胜率估计模型性能的指标，并且提出了一种新的平衡分数指标。 |
| [^23] | [Consistency and adaptivity are complementary targets for the validation of variance-based uncertainty quantification metrics in machine learning regression tasks.](http://arxiv.org/abs/2309.06240) | 这篇论文研究了机器学习回归任务中基于方差的不确定性量化度量的验证，发现一致性和适应性是互补的验证目标，并提出了适应性验证方法。 |
| [^24] | [Risk-Aware Reinforcement Learning through Optimal Transport Theory.](http://arxiv.org/abs/2309.06239) | 本文将最优输运理论与强化学习相结合，创建了一个风险感知的框架，通过修改目标函数，在最大化期望奖励的同时捕捉潜在风险，提供了数学精确的方法来提升强化学习中风险考量的重要性。 |
| [^25] | [The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models.](http://arxiv.org/abs/2309.06236) | 这项研究讨论了在大语言模型中表示和分词时间数据的困难，并提出了解决方案，如使用轻量级嵌入层进行提示调整和多模态适配器。 |
| [^26] | [A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models.](http://arxiv.org/abs/2309.06230) | 该论文提出了针对高维单指数模型中最佳子集选择的一致性和可扩展算法，通过使用广义信息准则来确定支持的回归系数大小，消除了模型选择的调优需求，并具有子集选择一致性和高概率下的理想属性。 |
| [^27] | [Long-term drought prediction using deep neural networks based on geospatial weather data.](http://arxiv.org/abs/2309.06212) | 基于地理气象数据的深度神经网络用于长期干旱预测，提出了一种端到端解决方案来预测特定地区干旱概率。采用卷积LSTM和Transformer模型相比其他基线模型能够获得更高的准确性。 |
| [^28] | [Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding.](http://arxiv.org/abs/2309.06195) | 本文研究了通过平滑软阈值函数的展开ISTA和ADMM网络在过参数化区域中的优化保证，通过满足特定区域的PL$^*$条件实现接近零训练损失。 |
| [^29] | [Assessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments.](http://arxiv.org/abs/2309.06183) | 本研究介绍了一种泛化评估框架，用于评估学习型语音增强系统在噪声和混响环境中的泛化能力，并提出了使用在测试条件下训练的参考模型作为难度代理。 |
| [^30] | [Efficient Memory Management for Large Language Model Serving with PagedAttention.](http://arxiv.org/abs/2309.06180) | 本论文提出了PagedAttention算法和vLLM系统，通过类似虚拟内存和分页技术的方法，实现了大型语言模型服务中键-值缓存内存的高效管理和灵活共享，大大提高了吞吐量，减少了内存使用量，相对于最先进的系统（如FasterTransformer和Orca）改进了2-4倍的吞吐量。 |
| [^31] | [Elucidating the solution space of extended reverse-time SDE for diffusion models.](http://arxiv.org/abs/2309.06169) | 这项工作介绍了扩展反向时间随机微分方程（ER SDE）用于解决扩散模型中的采样问题，并提供了精确解和高阶近似解，并解释了在快速采样方面ODE求解器优于SDE求解器的数学洞察力。 |
| [^32] | [Certified Robust Models with Slack Control and Large Lipschitz Constants.](http://arxiv.org/abs/2309.06166) | 本文提出了一种校准的Lipschitz边界误差（CLL）来提高认证鲁棒性，通过解决边界误差不会根据收缩的输出分布调整惩罚和最小化Lipschitz常数导致过度平滑的问题。 |
| [^33] | [Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines.](http://arxiv.org/abs/2309.06157) | 本文提出了一种稳健的多支路深度学习模型，用于旋转机器的剩余寿命预测和运行状态识别。该模型包括LSTM-Autoencoder对振动数据进行去噪、特征提取和多支路深度学习网络结构等组件，并在实验中证明了它在轴承机器应用中的优越性和潜力。 |
| [^34] | [Towards Reliable Domain Generalization: A New Dataset and Evaluations.](http://arxiv.org/abs/2309.06142) | 这项研究提出了一个新的领域泛化任务，用于手写中文字符识别，该任务的目标是丰富领域泛化方法的应用场景。通过在一个特定的数据集上评估18种领域泛化方法，研究发现现有方法在该数据集上的性能仍然不理想。此外，通过设计动态领域泛化设置，揭示了只有留出一个领域的方法才能在这种设置下实现良好性能。 |
| [^35] | [Accelerating Edge AI with Morpher: An Integrated Design, Compilation and Simulation Framework for CGRAs.](http://arxiv.org/abs/2309.06127) | 形变器是一个集成的设计、编译和仿真框架，用于加速边缘人工智能应用。它能自动将AI应用内核编译到用户定义的CGRA架构上，并验证其功能。 |
| [^36] | [AstroLLaMA: Towards Specialized Foundation Models in Astronomy.](http://arxiv.org/abs/2309.06126) | AstroLLaMA是一个专门用于天文学的模型，通过从arXiv中的天文学摘要fine-tuned得到，其在因果语言建模中表现出色，生成的文本完成和嵌入提取比其他基础模型更具洞察力和科学相关性。 |
| [^37] | [A robust synthetic data generation framework for machine learning in High-Resolution Transmission Electron Microscopy (HRTEM).](http://arxiv.org/abs/2309.06122) | 本论文提出了一个用于高分辨率透射电子显微镜（HRTEM）的鲁棒合成数据生成框架，该框架包括Construction Zone软件包和一个端到端的工作流程。利用该框架，可以快速生成大规模的模拟数据库，并用于训练神经网络进行纳米颗粒的分割等任务。 |
| [^38] | [Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning.](http://arxiv.org/abs/2309.06097) | 本文提出了一种基于保真度诱导的策略提取方法（FIPE）来解决深度强化学习代理不透明的决策问题。实验证明，该方法在星际争霸 II 这样的复杂控制环境下具有良好的性能。 |
| [^39] | [A General Verification Framework for Dynamical and Control Models via Certificate Synthesis.](http://arxiv.org/abs/2309.06090) | 这个论文提出了一个通用的框架来通过证书合成验证动态和控制模型。研究者们提供了一种自动化方法来设计控制器并分析复杂规范。这个方法利用神经网络和SMT求解器来提供候选控制和证书函数，并为控制的安全学习领域做出了贡献。 |
| [^40] | [Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies.](http://arxiv.org/abs/2309.06089) | 该研究比较了不同的微调和跨语言转移策略在解决跨语言任务时的表现，评估了灾难性遗忘的程度和转移的成功程度。 |
| [^41] | [Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning.](http://arxiv.org/abs/2309.06086) | 本研究提出了一种优化塑性的互补网络方法，用于无监督连续学习。通过解除专家网络对保留先前知识的要求，并通过适应-回顾阶段与之结合，解决了无监督学习中塑性和稳定性之间的权衡问题。 |
| [^42] | [A Machine Learning Framework to Deconstruct the Primary Drivers for Electricity Market Price Events.](http://arxiv.org/abs/2309.06082) | 该论文提出了一个基于机器学习的分析框架，用于解析现代高可再生能源电力市场中价格飙升事件的主要驱动因素。 |
| [^43] | [Information Flow in Graph Neural Networks: A Clinical Triage Use Case.](http://arxiv.org/abs/2309.06081) | 本文研究了图神经网络在知识图谱中的链接预测任务中的信息流动问题，提出了一个数学模型来解耦网络连接性与图数据连接性，并在临床分诊应用场景中评估了网络性能。研究结果表明，将领域知识纳入网络连接性能够提升预测性能，负边对于良好的预测起重要作用，但使用过多的网络层会降低性能。 |
| [^44] | [A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel Segmentation via Two-Phase Training Angiography-to-Venography Translation.](http://arxiv.org/abs/2309.06075) | A2V是一种半监督领域自适应框架，用于通过图像到图像转换实现脑血管的跨模态分割。它通过离散化和语义丰富的潜在空间实现源域到目标域的图像级自适应，并提高了计算效率和训练稳定性。 |
| [^45] | [Selection of contributing factors for predicting landslide susceptibility using machine learning and deep learning models.](http://arxiv.org/abs/2309.06062) | 使用机器学习和深度学习模型对滑坡易发性进行预测，通过选择更重要的贡献因素提高预测准确性。 |
| [^46] | [Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems.](http://arxiv.org/abs/2309.06061) | 提出了公平性即服务（FaaS）的安全、可验证和隐私保护协议，用于计算和验证任何机器学习（ML）模型的公平性。FaaS是模型无关的，可以支持多种公平性指标；因此，它可以用作审计任何ML模型的服务。 |
| [^47] | [How does representation impact in-context learning: A exploration on a synthetic task.](http://arxiv.org/abs/2309.06054) | 本研究通过探索表示学习的角度，研究了表示对上下文学习的影响。实验结果表明，在上下文学习中，上下文内部成分对学习性能起到重要作用。 |
| [^48] | [A Perceptron-based Fine Approximation Technique for Linear Separation.](http://arxiv.org/abs/2309.06049) | 本文提出了一种基于感知机的在线学习方法，通过精细调整神经元权重来找到数据点之间的分离超平面，从而降低了大型或不平衡数据集的计算复杂性。该方法通过适当转换初始数据集来将分离问题转化为一类分类问题。 |
| [^49] | [BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise.](http://arxiv.org/abs/2309.06046) | 本研究对少样本元学习器受标签噪声影响的性能进行了全面分析，发现在受到标签噪声影响的元训练中，Reptile、iMAML和foMAML在Omniglot和CifarFS数据集上的准确性下降了最高达42%。为了增强对标签噪声的鲁棒性，提出了Man和BatMan两种采样技术，将有噪声的有监督学习器转变为半监督学习器。 |
| [^50] | [Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning.](http://arxiv.org/abs/2309.06034) | 本文提出了一种基于正态学习的图异常检测框架NLGAD，通过多尺度对比学习网络来增强学习正常模式的能力，以改进异常检测性能。 |
| [^51] | [Energy-Aware Federated Learning with Distributed User Sampling and Multichannel ALOHA.](http://arxiv.org/abs/2309.06033) | 本研究考虑了能量收集设备与多信道ALOHA相结合的联邦学习网络，提出了一种方法来解决能量耗尽问题，并确保任务的顺利执行。实验证明该方法在关键设置中的有效性和性能优于基于规范的解决方案。 |
| [^52] | [Emergent Communication in Multi-Agent Reinforcement Learning for Future Wireless Networks.](http://arxiv.org/abs/2309.06021) | 多智能体强化学习中的紧急通信（EC-MARL）是解决未来无线网络中高维连续控制问题的合作方式，它赋予网络实体自主决策能力来解决复杂任务。 |
| [^53] | [Interpolation, Approximation and Controllability of Deep Neural Networks.](http://arxiv.org/abs/2309.06015) | 本文研究了深度神经网络的插值、逼近和可控性。通过控制理论分析，我们发现深度残差神经网络具有通用插值和通用逼近的能力。然而，这两个性质不能互相推导，但在一定条件下，两个性质也是等价的。 |
| [^54] | [ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation.](http://arxiv.org/abs/2309.05994) | 本文提出了一种双层场外分布检测框架来处理领域转移和语义转移问题。该框架利用全局低级特征和密集高级特征图来适应模型到未见领域，并增强模型在检测新类别方面的能力。 |
| [^55] | [Learning Unbiased News Article Representations: A Knowledge-Infused Approach.](http://arxiv.org/abs/2309.05981) | 提出了一种基于知识的深度学习模型，可以学习无偏见的新闻文章表示，从而准确预测新闻文章的政治倾向，并解决了现有学习模型受新闻发布者政治偏见影响的问题。 |
| [^56] | [CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram.](http://arxiv.org/abs/2309.05975) | CleanUNet 2是一种结合了波形和频谱图的混合语音降噪模型，通过两个阶段的框架，它构建在当前最先进的波形降噪器CleanUNet的基础上，并通过使用预测的频谱图作为输入来进一步提升性能。 |
| [^57] | [Circuit Breaking: Removing Model Behaviors with Targeted Ablation.](http://arxiv.org/abs/2309.05973) | 本论文提出了一种通过有针对性的消融模型组件之间的因果路径来去除语言模型中不良行为的新方法。在减少GPT-2毒性语言生成方面，仅消融12条因果边中的11.6K可以有效减轻毒性生成，并在其他输入上的性能下降很小。 |
| [^58] | [Neural Network Layer Matrix Decomposition reveals Latent Manifold Encoding and Memory Capacity.](http://arxiv.org/abs/2309.05968) | 通过神经网络层矩阵分解，我们揭示了神经网络层编码训练数据集的潜在流形和数学运算的几何性质，这对于理解神经网络如何突破维度诅咒具有重要意义。 |
| [^59] | [Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms.](http://arxiv.org/abs/2309.05961) | 本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。 |
| [^60] | [GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection.](http://arxiv.org/abs/2309.05953) | GLAD是一个内容感知的动态图用于日志异常检测的框架，它综合了日志语义、关系模式和顺序模式，通过识别关键字段和构建动态日志图来检测系统日志中的关联异常。 |
| [^61] | [Language Models as Black-Box Optimizers for Vision-Language Models.](http://arxiv.org/abs/2309.05950) | 本论文介绍了一种新的视觉-语言模型 (VLMs) 微调方法，通过自然语言提示来避免访问模型参数，采用聊天式的语言模型作为黑盒优化器，在少样本图像分类任务中达到效果。 |
| [^62] | [Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals.](http://arxiv.org/abs/2309.05927) | 本研究提出了一种名为$\texttt{bio}$FAME的频率感知掩码自编码器，用于多模态生物信号的预训练。其通过在频率空间中对生物信号进行表示参数化，利用固定大小的傅里叶变换运算符进行全局令牌混合，并通过频率维持预训练策略保持每个输入通道中的频率成分。 |
| [^63] | [On Regularized Sparse Logistic Regression.](http://arxiv.org/abs/2309.05925) | 本文提出了解决正则稀疏逻辑回归的方法，包括$\ell_1$正则化稀疏逻辑回归和一些满足先决条件的非凸惩罚正则化稀疏逻辑回归。经验实验表明，这些算法能够以较低的计算成本有效地进行分类和特征选择。 |
| [^64] | [ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning.](http://arxiv.org/abs/2309.05915) | 这篇论文提出了一种通过将动态规划应用于决策Transformer来增强其能力的方法。作者提出了三个步骤来实现这一目标：使用样本内值迭代获得近似值函数，结合估计的优势评估动作质量，并训练ACT生成基于估计优势的动作。该方法在测试中表现出良好的性能。 |
| [^65] | [Adversarial Attacks Assessment of Salient Object Detection via Symbolic Learning.](http://arxiv.org/abs/2309.05900) | 这项工作证明了通过符号学习的显著目标检测方法在面对对抗攻击时具有鲁棒性。 |
| [^66] | [Hierarchical Conditional Semi-Paired Image-to-Image Translation For Multi-Task Image Defect Correction On Shopping Websites.](http://arxiv.org/abs/2309.05883) | 本论文提出了一个统一的图像到图像转换模型，可以在不同产品类型上校正多个缺陷。通过引入分层的注意力机制，模型可以有效地减轻缺陷对图像质量的影响。与最先进的方法相比，模型在缺陷校正上取得了显著的性能提升。 |
| [^67] | [Generalized Attacks on Face Verification Systems.](http://arxiv.org/abs/2309.05879) | 这篇论文研究了对面部验证系统的广义攻击。通过引入DodgePersonation攻击，创造了模仿给定身份的面部图像，同时避免被识别为不同的身份。提出了一种分类法，对不同类型的对抗攻击进行统一总结。并提出了“一张面孔统治所有”的攻击，具有最先进的性能。 |
| [^68] | [Reaction coordinate flows for model reduction of molecular kinetics.](http://arxiv.org/abs/2309.05878) | 该论文介绍了一种基于流的机器学习方法，称为反应坐标流，用于发现分子系统低维动力学模型，该方法能够以连续时间和空间中的可训练和可处理的方式进行模型简化，产生准确和可解释的低维表示。 |
| [^69] | [Force-directed graph embedding with hops distance.](http://arxiv.org/abs/2309.05865) | 本文提出了一种新颖的基于跳跃距离的力导向图嵌入方法，通过模拟自定义的引力和斥力以及使用牛顿第二定律，嵌入节点以保持图的拓扑结构和结构特征，并在多个图分析任务上取得了竞争性能。 |
| [^70] | [The bionic neural network for external simulation of human locomotor system.](http://arxiv.org/abs/2309.05863) | 本文提出了一种基于肌肉骨骼模拟和物理信息深度学习的方法，用于预测关节运动和肌肉力量。 |
| [^71] | [Uncovering mesa-optimization algorithms in Transformers.](http://arxiv.org/abs/2309.05858) | 本研究揭示了Transformer模型中的mesa-optimization算法，该算法通过内部学习目标和相应的优化解决方案驱动预测生成。研究还发现，这种学习的优化算法可以被应用于解决监督式少样本任务，暗示了mesa-optimization可能是大型语言模型上下文学习能力的基础。 |
| [^72] | [Energy Preservation and Stability of Random Filterbanks.](http://arxiv.org/abs/2309.05855) | 本文从随机卷积算子的数学角度详细阐述了简单卷积神经网络的统计属性，发现具有随机高斯权重的FIR滤波器组在大滤波器和局部周期输入信号的情况下是病态的，并推导了其期望帧边界的理论界限。 |
| [^73] | [ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation.](http://arxiv.org/abs/2309.05853) | 这项工作提出了一种新颖而高效的半监督主动学习方法，通过策略性地操作样本空间表示，可以使生成模型相对于客观函数进行微调。在目标分子生成的背景下，通过在化学空间代理内部策略性地操作，实现了最大化生成分子与蛋白质靶标之间吸引力相互作用的能力。 |
| [^74] | [Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data.](http://arxiv.org/abs/2309.05845) | 在多元时间序列医疗数据上，我们提出了一种基于残差的异常检测方法，用于有效的表示学习和异常活动检测。实验结果显示F1分数为0.839。 |
| [^75] | [Optimizing Audio Augmentations for Contrastive Learning of Health-Related Acoustic Signals.](http://arxiv.org/abs/2309.05843) | 本文通过利用自监督学习框架SimCLR和Slowfast NFNet背骨，优化健康声学对比学习。我们针对该应用识别了有效的音频增强策略，并发现当这些增强方法相结合时能够产生超过单独应用每个增强的 synergistic effects。 |
| [^76] | [The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems.](http://arxiv.org/abs/2309.05837) | 这篇文章提供了对安全过滤器方法的综述，提出了一个统一的技术框架来理解、比较和结合现有的技术，为成功部署下一代自主机器人提供了重要的指导。 |
| [^77] | [PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis.](http://arxiv.org/abs/2309.05833) | 本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。 |
| [^78] | [Instance-Agnostic Geometry and Contact Dynamics Learning.](http://arxiv.org/abs/2309.05832) | 本文提出了一个实例无关的学习框架，通过几何作为共享表示，将视觉与动力学相结合，从RGBD视频中学习物体的几何和动力学属性。实验结果表明，该框架能够学习刚性和凸物体的几何和动力学，并改进了跟踪框架。 |
| [^79] | [Studying Accuracy of Machine Learning Models Trained on Lab Lifting Data in Solving Real-World Problems Using Wearable Sensors for Workplace Safety.](http://arxiv.org/abs/2309.05831) | 本文研究了将实验室训练的机器学习模型应用于真实世界时的准确性问题，并提出了四种潜在解决方案。 |
| [^80] | [Exploring Geometric Deep Learning For Precipitation Nowcasting.](http://arxiv.org/abs/2309.05828) | 本论文探索了几何深度学习在降水预测中的应用，采用了基于时间的图卷积网络（GCN）模型，该模型能够更好地捕捉地理网格之间的动态空间关系。 |
| [^81] | [KD-FixMatch: Knowledge Distillation Siamese Neural Networks.](http://arxiv.org/abs/2309.05826) | KD-FixMatch是一种半监督学习算法，在FixMatch的基础上引入了知识蒸馏，通过顺序和并行训练SNNs的组合来提高性能并降低性能下降。 |
| [^82] | [Ensemble-based modeling abstractions for modern self-optimizing systems.](http://arxiv.org/abs/2309.05823) | 本文在基于集成的组件模型DEECo的基础上，扩展了机器学习和优化启发式方法以建立和重新配置自主组件集。这种方法对于现代智能系统来说是一个关键特性，能够在学习过程中优化行为，并在运行时处理环境中的不确定性。 |
| [^83] | [Interpretable learning of effective dynamics for multiscale systems.](http://arxiv.org/abs/2309.05812) | 该论文提出了一种新的可解释学习有效动力学（iLED）框架，它通过引入深度循环神经网络技术，在保持准确性的同时提供了可解释性，解决了现有神经网络在复杂系统中应用受限的问题。 |
| [^84] | [Predicting the Radiation Field of Molecular Clouds using Denoising Diffusion Probabilistic Models.](http://arxiv.org/abs/2309.05811) | 使用去噪扩散概率模型（DDPMs）预测分子云的辐射场强度，通过合成尘埃发射图来估计星际辐射场（ISRF），并且在不同物理参数的新模拟中展现出一致的预测结果。 |
| [^85] | [SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors.](http://arxiv.org/abs/2309.05810) | 我们提出了SHIFT3D，这是一个可微分的方法，用于生成具有挑战性的三维形状以欺骗三维物体检测器，并且具有预防性发现潜在安全风险的潜力。 |
| [^86] | [Divergences in Color Perception between Deep Neural Networks and Humans.](http://arxiv.org/abs/2309.05809) | 本研究通过对比深度神经网络(DNNs)和人类的颜色感知差异，发现最先进的DNN结构在颜色相似性判断方面与人类存在显著差异。这对于深入理解DNN在人类视觉方面的模拟能力具有重要意义。 |
| [^87] | [Online ML Self-adaptation in Face of Traps.](http://arxiv.org/abs/2309.05805) | 这篇论文报告了将在线机器学习应用于自适应中的经验，讨论了涉及规范、在线训练和评估的陷阱，并给出了一系列的经验教训，可以为其他研究人员和实践者提供指导。 |
| [^88] | [Revisiting Energy Based Models as Policies: Ranking Noise Contrastive Estimation and Interpolating Energy Models.](http://arxiv.org/abs/2309.05803) | 这项工作提出了一种实用的训练目标和算法，用于能量模型，证明了现有的普遍传言是错误的。同时，定义了一个能够在高维连续空间中有效训练的指标函数。 |
| [^89] | [Enhancing Hyperedge Prediction with Context-Aware Self-Supervised Learning.](http://arxiv.org/abs/2309.05798) | 该论文提出了一种增强超边预测的方法，通过上下文感知的节点聚合和自监督对比学习来解决超边预测中的问题。这种方法可以准确捕捉节点之间的复杂关系，并缓解数据稀疏问题。 |
| [^90] | [On the Fine-Grained Hardness of Inverting Generative Models.](http://arxiv.org/abs/2309.05795) | 本文提供了反转生成模型的计算难度的细粒度视图，建立了对精确和近似模型反转的新的难度下界。 |
| [^91] | [Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems.](http://arxiv.org/abs/2309.05787) | 这篇论文提出了一种自适应用户中心的神经符号学习方法，用于支持多模态交互中自主系统对物体和环境的符号化理解能力的提升。 |
| [^92] | [Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments.](http://arxiv.org/abs/2309.05784) | 本研究提出了一种基于灰盒贝叶斯优化和仿真评估的方法，通过捕捉关于活动的空间分布的领域特定知识，找到高质量的传感器布置，并在跌倒检测、室内定位和活动识别等领域取得了优于黑盒优化技术的性能。 |
| [^93] | [Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday Functioning.](http://arxiv.org/abs/2309.05777) | 该研究探讨了使用智能手表收集声学特征作为检测认知缺陷引起的日常功能缺陷的客观标记的可行性。机器学习模型使用声学特征可以以高准确率检测到存在日常功能缺陷的个体。 |
| [^94] | [The Effect of Intrinsic Dimension on Metric Learning under Compression.](http://arxiv.org/abs/2309.05751) | 本论文研究了内在维度对压缩下的度量学习的影响，提出了在对数据进行随机压缩后在低维空间内训练全秩度量的方法。理论保证了在不依赖环境维度的情况下，度量学习的误差可以被控制，并且在存在良性几何结构时效果更好。 |
| [^95] | [CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation.](http://arxiv.org/abs/2309.05704) | CaloClouds II是一个超快速几何独立的高分辨率量能器模拟，通过连续时间得分建模，相比传统模拟更快且具有可比的保真度。 |
| [^96] | [Unsupervised Machine Learning Techniques for Exploring Tropical Coamoeba, Brane Tilings and Seiberg Duality.](http://arxiv.org/abs/2309.05702) | 我们利用无监督机器学习技术，并采用主成分分析（PCA）和t-分布随机邻域嵌入（t-SNE）等技术，成功地将标记的共平面空间投影到具有相界限的低维相空间中，以识别与相同泛型Calabi-Yau 3折对应的4维N=1超对称规范理论的泛型相。 |
| [^97] | [Temporal Patience: Efficient Adaptive Deep Learning for Embedded Radar Data Processing.](http://arxiv.org/abs/2309.05686) | 该论文提出了一种利用流式雷达数据的时间相关性来增强嵌入式设备上深度学习推理效率的新技术。通过在网络中添加额外的分类器分支，实现根据运行时决策机制提前终止推理，从而降低计算成本而保持最小的准确度损失。实验结果表明，该技术能够比单路退出网络和基于置信度的早期退出版本节省高达26%和12%的推理操作。这些技术适用于通用硬件并可以结合使用。 |
| [^98] | [EANet: Expert Attention Network for Online Trajectory Prediction.](http://arxiv.org/abs/2309.05683) | 本论文提出了一个专家注意力网络用于在线轨迹预测的在线学习框架。通过引入专家注意力机制，调整网络层的权重，解决了梯度问题，能够快速学习新场景的知识以提高预测准确度。 |
| [^99] | [A compendium of data sources for data science, machine learning, and artificial intelligence.](http://arxiv.org/abs/2309.05682) | 这篇论文提供了一个跨多个领域的数据源大全，包括金融、法律、生命科学、新闻社交等，以满足数据科学家和机器学习专家的需求。 |
| [^100] | [Knowledge-based Refinement of Scientific Publication Knowledge Graphs.](http://arxiv.org/abs/2309.05681) | 本论文提出了一种基于知识的方法来细化科学出版物知识图谱，通过学习概率逻辑模型并使用功能梯度提升和人类知识引导来识别作者身份。实验证明了人类知识在作者身份领域的定量和定性作用。 |
| [^101] | [Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing.](http://arxiv.org/abs/2309.05679) | 本文通过基于趋势的测试方法评估了解释模型决策的忠诚度，并提出了三种新的趋势测试方法，从实证结果来看，这些新测试方法在图像、自然语言和安全任务中可以更好地评估解释模型的忠诚度。 |
| [^102] | [Gromov-Hausdorff Distances for Comparing Product Manifolds of Model Spaces.](http://arxiv.org/abs/2309.05678) | 本论文提出了一种用于比较模型空间的产品流形的Gromov-Hausdorff距离的新方法，通过估计距离来搜索最佳的潜在几何。这为提高机器学习模型的性能提供了一种原则性的技术。 |
| [^103] | [MultiCaM-Vis: Visual Exploration of Multi-Classification Model with High Number of Classes.](http://arxiv.org/abs/2309.05676) | MultiCaM-Vis是一种可视化工具，能够帮助机器学习专家在大量类别的多分类模型中识别出问题的根本原因，并探索和检查类别级别的实例错误分类。 |
| [^104] | [SHAPE: A Sample-adaptive Hierarchical Prediction Network for Medication Recommendation.](http://arxiv.org/abs/2309.05675) | 本文提出了一种名为SHAPE的适应样本的层次药物预测网络，旨在解决医疗保健中复杂多发病条件下的药物推荐问题。通过设计紧凑的内部病患就诊事件关系编码器和纵向病历编码器，我们能够获得准确的病患表示和纵向序列学习策略。 |
| [^105] | [Circles: Inter-Model Comparison of Multi-Classification Problems with High Number of Classes.](http://arxiv.org/abs/2309.05672) | 这篇论文介绍了一种名为"Circles"的交互式可视分析工具，允许在一个视图中进行对具有1K类别的多个分类模型进行模型间比较。使用同心径向线布局来解决视觉混乱的问题。 |
| [^106] | [tSPM+; a high-performance algorithm for mining transitive sequential patterns from clinical data.](http://arxiv.org/abs/2309.05671) | tSPM+算法是一种高性能算法，通过在时间模式中加入持续时间维度，提供了高速运行和内存消耗改进。 |
| [^107] | [Robot Parkour Learning.](http://arxiv.org/abs/2309.05665) | 该论文提出了一个学习基于视觉的多样化公园our技能的系统，该系统使用简单的奖励而不使用参考动作数据，通过强化学习方法生成不同的公园our技能，并将其转移到四足机器人中。 |
| [^108] | [Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck.](http://arxiv.org/abs/2309.05649) | 广义对称信息瓶颈是一种同时压缩两个随机变量以保留信息的维度约简技术，相较于逐个压缩变量，它需要更少的数据来达到相同的误差。 |
| [^109] | [Machine Learning for maximizing the memristivity of single and coupled quantum memristors.](http://arxiv.org/abs/2309.05062) | 本文使用机器学习方法来研究单个和耦合量子忆阻器的记忆阻性，并发现最大化记忆阻性可以提高两个量子忆阻器的纠缠程度，揭示了量子相关性与记忆之间的关系。这一发现增强了将量子忆阻器应用于神经形态的量子计算的潜力。 |
| [^110] | [Knowledge Distillation-Empowered Digital Twin for Anomaly Detection.](http://arxiv.org/abs/2309.04616) | 本文提出了基于知识蒸馏的数字孪生模型KDDT，用于列车控制和管理系统（TCMS）的异常检测。KDDT利用语言模型和长短期记忆网络分别提取上下文和时间顺序特征，通过知识蒸馏丰富数据量。实验结果表明KDDT在两个数据集上取得了较高的F1分数0.931和0.91。 |
| [^111] | [Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation.](http://arxiv.org/abs/2309.04434) | 本研究提出了一种利用物理信息神经网络（PINNs）解决量子电路反对角（CD）协议优化问题的方法，通过嵌入物理信息到神经网络中，并利用最小作用量原理和厄米特性条件来获取最适当的反对角项，从而提供了一种可靠的替代方案，摆脱了以往依赖于经典数值逼近的约束。 |
| [^112] | [Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility.](http://arxiv.org/abs/2309.04296) | 本研究提出了一种利用人类移动数据和持续学习技术的方法来解决COVID-19期间非分布期间的电力负荷预测问题，通过保留过去的见解并整合新的数据，提高了模型的准确性和鲁棒性。 |
| [^113] | [Multimodal Transformer for Material Segmentation.](http://arxiv.org/abs/2309.04001) | 本文提出了一种新的多模态分割方法MMSFormer，该方法有效地融合四种不同模态的信息，并在MCubeS数据集上取得了显著的性能提升。 |
| [^114] | [ImageBind-LLM: Multi-modality Instruction Tuning.](http://arxiv.org/abs/2309.03905) | ImageBind-LLM是一种多模态指令调优的方法，通过图像-文本对齐训练，并利用联合嵌入实现了优秀的多模态指令跟随能力。 |
| [^115] | [GPT Can Solve Mathematical Problems Without a Calculator.](http://arxiv.org/abs/2309.03241) | 本研究表明，通过充分训练，一个20亿参数的语言模型可以在没有计算器工具的情况下以几乎100%的准确度执行多位数的算术运算，超越了之前的GPT-4。这项研究还通过在附加的多步骤算术运算和数学问题的数据集上进行微调，展示了一个与GPT-4在中文数学问题上相似的性能。 |
| [^116] | [Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference.](http://arxiv.org/abs/2309.03239) | 本文提出了一种针对POI级别人群流推断的时空对比自监督学习模型，通过自监督属性图表示学习以解决数据标记不足、POI间时空依赖性复杂和人群流量与GPS报告之间相关性多样等挑战。 |
| [^117] | [No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function.](http://arxiv.org/abs/2309.03224) | 该论文提出了一种通过蒙特卡洛树搜索和能量函数引导来释放大型语言模型的数学推理能力的方法，以解决当前在数学推理任务中的不足和错误。该方法不需要进一步的微调步骤，通过重新定义模型和引入路径验证器的方式，实现了对输出空间的搜索和推理路径的评估。 |
| [^118] | [Diffusion on the Probability Simplex.](http://arxiv.org/abs/2309.02530) | 本文提出了一种在概率单纯形上执行扩散的方法，通过使用softmax函数应用于阿恩斯坦-乌伦贝克过程，可以在处理连续性和离散性对象之间的紧张关系时取得良好效果。这种方法也可以扩展到单位立方体上，从而在有界图像生成方面具有应用前景。 |
| [^119] | [MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision.](http://arxiv.org/abs/2308.16139) | MedShapeNet是一个大规模的三维医学形状数据集，作为一种对于常用形状基准的替代品，为计算机视觉研究提供了新的选择。 |
| [^120] | [LLaSM: Large Language and Speech Model.](http://arxiv.org/abs/2308.15930) | LLaSM是一个大型语言和语音模型，具有跨模态对话能力，通过遵循语音和语言指令，提供了一种方便自然的人机交互方式。 |
| [^121] | [JL-lemma derived Optimal Projections for Discriminative Dictionary Learning.](http://arxiv.org/abs/2308.13991) | 本文提出了一种名为JLSPCADL的新方法，通过利用Johnson-Lindenstrauss引理选择一个维数的转换空间，从而实现判别式字典学习并提供更好的分类结果。 |
| [^122] | [Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models.](http://arxiv.org/abs/2308.11890) | 本文提出了一个基于形状的分子生成问题，通过等变形状引导的生成模型ShapeMol成功生成了新颖、多样且类似给定形状条件的药物样分子。 |
| [^123] | [Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning.](http://arxiv.org/abs/2308.09544) | 本研究提出了一种适应教师的方法（TA）用于无样本连续学习，解决了知识蒸馏方法在这种情况下的性能下降问题，并在多个基准测试中持续提升模型性能。 |
| [^124] | [From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space.](http://arxiv.org/abs/2308.09437) | 该论文提出了一种通过梯度减小模型对偏见的敏感性的方法，从而在概念级别上确保正确原因，有效减轻深度神经网络中的偏见。该方法在多个数据集和环境中被验证有效。 |
| [^125] | [Multi-Modality Multi-Loss Fusion Network.](http://arxiv.org/abs/2308.00264) | 多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。 |
| [^126] | [eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review.](http://arxiv.org/abs/2307.13704) | 本综述探讨了可解释人工智能（XAI）在年龄预测任务中的应用。通过系统性综述，我们讨论了XAI方法在医疗应用和年龄预测领域的益处。 |
| [^127] | [TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars.](http://arxiv.org/abs/2307.10705) | 本文提出了一种轻量级模型TwinLiteNet，用于自动驾驶车辆中的可驱动区域和车道分割。该模型成本低廉且高效准确，并在实验中表现出明显的计算资源节约。 |
| [^128] | [Safe Reinforcement Learning for Strategic Bidding of Virtual Power Plants in Day-Ahead Markets.](http://arxiv.org/abs/2307.05812) | 本文提出了一种新颖的安全增强学习算法，用于虚拟电力厂在日前电力市场中的战略竞标。该算法不需要精确的市场模型，通过深度确定性策略梯度方法学习具有竞争力的竞标策略，并通过引入安全屏蔽和奖励函数的惩罚机制来考虑虚拟电力厂的物理约束，使代理能够学习到更安全的策略。 |
| [^129] | [A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark.](http://arxiv.org/abs/2307.04570) | 该论文提出反思评估年龄估计实践的呼吁，对现有技术进行了比较分析，并提出了统一基准。研究发现当前评估协议存在问题，并描述了如何解决它们。通过比较分析，发现方法之间的性能差异微不足道，而其他因素的影响更大。研究利用得到的见解提出使用FaRL方法来解决这些问题。 |
| [^130] | [ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models.](http://arxiv.org/abs/2307.00398) | ProbVLM是一种概率适配器，用于估计大规模视觉-语言模型中嵌入的概率分布，以解决固有的嵌入歧义问题，并在多个数据集上展示了其在检索任务中的优越性能表现。 |
| [^131] | [Deep Learning Models for Water Stage Predictions in South Florida.](http://arxiv.org/abs/2306.15907) | 本论文利用深度学习模型训练代理模型，快速预测南佛罗里达州迈阿密河下游的水位，并与基于物理的模型进行比较。 |
| [^132] | [Pure Exploration in Bandits with Linear Constraints.](http://arxiv.org/abs/2306.12774) | 本文提出了两种相对于线性约束下的多臂赌博问题都是渐进最优的算法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。 |
| [^133] | [Breaking On-device Training Memory Wall: A Systematic Survey.](http://arxiv.org/abs/2306.10388) | 本研究调查了打破设备上训练内存壁垒的最先进技术，并提出了解决资源受限设备上训练更大更复杂模型的方法。调查分析了内存壁垒的关键因素，并总结了设备上训练的开放问题。 |
| [^134] | [RescueSpeech: A German Corpus for Speech Recognition in Search and Rescue Domain.](http://arxiv.org/abs/2306.04054) | RescueSpeech是一个用于搜救领域语音识别的德语语音数据集，但目前最先进的方法仍无法令人满意。 |
| [^135] | [Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates.](http://arxiv.org/abs/2305.13409) | 该研究提出了一种能有效学习几个非克利福德门制备的量子状态的算法，并给出了一个随着稳定维数增大而学习所有状态的算法。 |
| [^136] | [Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning.](http://arxiv.org/abs/2305.06295) | 本文采用深度强化学习算法，基于电子病历来学习获得正确诊断所需的观察序列的最优顺序。因为诊断指南的缺陷，尤其对罕见病或患有多种病的患者，DRL算法具有重要现实意义。 |
| [^137] | [PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel.](http://arxiv.org/abs/2304.11277) | 本论文介绍了基于PyTorch的Fully Sharded Data Parallel（FSDP）解决方案，该方案可扩展大型模型训练，并优化各种硬件配置的资源利用率。 |
| [^138] | [Multi-granulariy Time-based Transformer for Knowledge Tracing.](http://arxiv.org/abs/2304.05257) | 本文提出了一种基于Transformer的架构用于准确地预测学生在标准化测试中的表现。该模型考虑了学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，并在解码器输入中使用了多个时间特征粒度以显著提高模型性能。与LightGBM相比，该方法更加准确，为教育领域的AI发展提供了一个可伸缩和准确的预测学生成果的工具。 |
| [^139] | [Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis.](http://arxiv.org/abs/2304.01347) | 本文提出了一种基于动态功能连接的脑网络分析模型，通过构建动态同步特征和革命性的图卷积方法实现精神分裂症诊断和侧化分析，并在实验证明其表现优于其他最先进模型。 |
| [^140] | [BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware.](http://arxiv.org/abs/2303.17727) | BOLT是一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库，它提供了一个灵活的高级API，使用户可以构建模型并抽象掉稀疏网络训练的算法细节。 |
| [^141] | [Plant Disease Detection using Region-Based Convolutional Neural Network.](http://arxiv.org/abs/2303.09063) | 本文提出了一种新的基于区域卷积神经网络（R-CNN）的轻量级深度学习模型，可用于检测番茄植物的叶病害。与传统方法相比，该模型利用整个番茄植物图像的特征进行检测，具有更高的准确性和效率。 |
| [^142] | [Tradeoff of generalization error in unsupervised learning.](http://arxiv.org/abs/2303.05718) | 无监督学习中存在一个权衡，使用更复杂的模型可以降低模型误差，但会增加数据误差，特别是在训练数据集较小的情况下。 |
| [^143] | [Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks.](http://arxiv.org/abs/2303.05445) | 该论文提出了一种名为带吸收的泛洪（FwA）的新协议，用于解决复杂网络上的异构赌博机问题。通过严格的遗憾分析，证明了该协议的有效性。 |
| [^144] | [Hierarchical Optimization-Derived Learning.](http://arxiv.org/abs/2302.05587) | 该论文提出了一个名为HODL的新框架，用于同时研究优化导出模型构建和相应学习过程的内在行为，并证明了这两个子任务的联合收敛性。这是对这两个任务提供的首个理论保证。 |
| [^145] | [Large Language Models for Code: Security Hardening and Adversarial Testing.](http://arxiv.org/abs/2302.05319) | 本研究针对大型语言模型在生成代码时缺乏安全意识，从安全加固和对抗测试的角度入手，提出了一项新的安全任务——受控代码生成，通过一种新型基于学习的方法SVEN，实现生成既安全又功能正确的代码，并对当前的LM进行对抗测试，强调了在LM的培训和评估中考虑安全因素的必要性。 |
| [^146] | [Deep-OSG: Deep Learning of Operators in Semigroup.](http://arxiv.org/abs/2302.03358) | 本文提出了一种深度学习方法，用于学习半群中的运算符，可以将未知自主动力系统建模为时间序列数据，在不同时间滞后下收集。这种方法能够学习具有可变时间步长的演化算符，构成自主系统的半群。 |
| [^147] | [Semantic-Guided Generative Image Augmentation Method with Diffusion Models for Image Classification.](http://arxiv.org/abs/2302.02070) | SGID是一种使用扩散模型的语义引导生成图像增强方法，旨在在图像分类中平衡图像多样性和语义一致性。实验证明，SGID在ResNet-5上的效果优于最佳增强基线1.72％。 |
| [^148] | [Robust Markov Decision Processes without Model Estimation.](http://arxiv.org/abs/2302.01248) | 这篇论文提出了一种无需模型估计的鲁棒MDPs算法，通过将原始问题转化为另一种形式，并使用随机梯度方法求解，从而去除了对优化器的依赖。 |
| [^149] | [A prediction and behavioural analysis of machine learning methods for modelling travel mode choice.](http://arxiv.org/abs/2301.04404) | 本研究分析了各种机器学习方法和传统随机效用模型在建模出行方式选择方面的性能和特点，并确定哪些模型最适合于建模，出行行为可解释性和解释性、计算复杂度和数据效率等因素需要进行整体考虑。 |
| [^150] | [StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation.](http://arxiv.org/abs/2212.10229) | 本文对GAN的领域适应问题进行了深入研究并提出了StyleGAN用于领域适应的新的高效轻量级参数化方案。 |
| [^151] | [Biomedical image analysis competitions: The state of current participation practice.](http://arxiv.org/abs/2212.08568) | 本研究调查了生物医学图像分析竞赛的现状和参与实践。调查结果显示，参与者主要动机是知识交流，而奖金的获得只起到次要作用。 |
| [^152] | [ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning.](http://arxiv.org/abs/2212.07919) | ROSCOE是一套度量指标，用于评分逐步推理的正确性和质量。它可以衡量语义一致性、逻辑性、信息量、流畅度和事实等特征，并提供可解释的评估方法。 |
| [^153] | [Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules.](http://arxiv.org/abs/2211.16691) | 本研究提出了一种基于简单规则的有针对性探索方法，通过避免已知子优的状态-动作空间区域来更快地加速强化学习代理程序的收敛，并在一个房间温度控制案例研究中实现了比传统方法快6-7倍的速度收敛。 |
| [^154] | [A Survey of Deep Graph Clustering: Taxonomy, Challenge, Application, and Open Resource.](http://arxiv.org/abs/2211.12875) | 在这篇论文中，作者对深度图聚类进行了综述研究。首先介绍了该领域的定义、评估和发展，然后提出了深度图聚类方法的分类学，并对现有方法进行了分析，总结出了挑战和机会。 |
| [^155] | [Brand New K-FACs: Speeding up K-FAC with Online Decomposition Updates.](http://arxiv.org/abs/2210.08494) | 本文提出了全新的K-FACs算法，利用在线分解更新来加速K-FAC的计算速度。这种算法比传统的K-FAC和RS-KFAC更廉价，虽然精度较低，但在预调节部分的计算复杂度上只有线性缩放。 |
| [^156] | [Fairness and robustness in anti-causal prediction.](http://arxiv.org/abs/2209.09423) | 这项研究通过因果关系的视角探讨了反因果预测中公平性和鲁棒性的关系，并提出了将分离准则应用于反因果设置的新动机。该研究还发现，以鲁棒性为驱动的方法可以用来实现分离，且通常比直接设计公平性方法的方法更有效。 |
| [^157] | [From latent dynamics to meaningful representations.](http://arxiv.org/abs/2209.00905) | 本文提出了一个动力学约束的表示学习框架，通过限制潜在表示遵循特定动态规律以使得学习到的表示具有意义。在真实世界的DNA荧光电影数据集上验证了该算法，表明其可以准确地学习动态规律，并获得有意义的表示。 |
| [^158] | [GEDI: A Graph-based End-to-end Data Imputation Framework.](http://arxiv.org/abs/2208.06573) | GEDI是一种基于图的端到端数据插补框架，通过使用Transformer网络和图结构学习来改进特征之间的上下文关系和观测之间的相似性，以及使用元学习来选择对下游标签预测任务有影响力的特征。实验证明，该框架在各种基准方法上都能提高插补和标签预测性能。 |
| [^159] | [Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs.](http://arxiv.org/abs/2206.00979) | 这篇论文提出了一种名为多尺度Wasserstein最短路径过滤图核心（MWSPF）的新型最短路径图核心，解决了传统核心的信息丢失和缺乏多个尺度考虑的问题。 |
| [^160] | [PSO-Convolutional Neural Networks with Heterogeneous Learning Rate.](http://arxiv.org/abs/2205.10456) | 本论文提出了一种基于粒子群优化的PSO卷积神经网络训练方法，通过在训练的不同阶段实现卷积神经网络之间的协同学习，提高了训练性能和泛化能力。 |
| [^161] | [Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks.](http://arxiv.org/abs/2203.02928) | 本文评估了几种常见的深度神经网络可解释性方法，展示了扰动伪影对可解释性方法评估的影响，强调在评估中需要考虑伪影的存在。 |
| [^162] | [Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features.](http://arxiv.org/abs/2203.01881) | 从自监督学习模型中提取区分特征，并使用它们压缩表示空间，提出了一种Q-Score自监督表示质量分数，可以可靠地预测线性评估期间的错误分类。 |
| [^163] | [Spatiotemporal Clustering with Neyman-Scott Processes via Connections to Bayesian Nonparametric Mixture Models.](http://arxiv.org/abs/2201.05044) | 这篇论文介绍了Neyman-Scott过程（NSPs）和贝叶斯非参数混合模型（DPMM）之间的新颖联系，并探讨了NSP在时空数据建模中的应用。 |
| [^164] | [Axiomatic Aggregations of Abductive Explanations.](http://arxiv.org/abs/2109.03890) | 本论文提出了三种聚合方法，将各种可能的推断解释聚合成特征重要性分数，解决了推断解释中多个有效解释的问题。这些方法基于合作博弈理论的权力指数和已知的因果强度度量。 |
| [^165] | [Task-Oriented Communication for Multi-Device Cooperative Edge Inference.](http://arxiv.org/abs/2109.00172) | 本文研究了多设备合作边缘推理的任务导向通信，通过优化本地特征提取和分布式特征编码，实现低延迟的合作推理。 |
| [^166] | [Stability to Deformations of Manifold Filters and Manifold Neural Networks.](http://arxiv.org/abs/2106.03725) | 本文定义了流形滤波器和流形神经网络，并通过分析它们在流形变形下的稳定性，推广了图滤波器和标准卷积滤波器的已知稳定性性质。 |
| [^167] | [Graph Barlow Twins: A self-supervised representation learning framework for graphs.](http://arxiv.org/abs/2106.02466) | 图形Barlow Twins是一种自监督表示学习框架，使用交叉相关的损失函数来学习图形表示，克服了负样本定义困难的问题。相比其他方法，它不依赖非对称神经网络结构，在需要较少超参数的情况下取得了竞争性的结果。 |
| [^168] | [Privacy-Preserving Constrained Domain Generalization for Medical Image Classification.](http://arxiv.org/abs/2105.08511) | 该论文提出了一种隐私保护约束域泛化的方法，通过改进信息聚合过程来提高模型的泛化能力，并解决了由于数据隐私保护问题导致的医学影像分类的挑战。 |
| [^169] | [Out-of-distribution detection for regression tasks: parameter versus predictor entropy.](http://arxiv.org/abs/2010.12995) | 本研究针对回归任务中的离群样本检测进行了实证评估，发现通过学习多样的预测器可以估计新观测实例的认识不确定性，但参数的多样性并不一定能转化为预测器的多样性。 |
| [^170] | [GTAdam: Gradient Tracking with Adaptive Momentum for Distributed Online Optimization.](http://arxiv.org/abs/2009.01745) | 本文提出了一种名为GTAdam的分布式算法，通过梯度跟踪和自适应动量估计结合，解决了分布式在线优化问题，并在强凸成本函数的在线设置中提供了动态遗憾上界和静态设置下的线性收敛速度保证。 |
| [^171] | [Distributionally Robust Batch Contextual Bandits.](http://arxiv.org/abs/2006.05630) | 本文提出了一种方法，在不完整的观察数据下学习分布鲁棒的策略，通过引入策略评估过程和中心极限定理类型的保证，实现了针对最坏情况下的环境转变的策略学习。 |
| [^172] | [Empirical and Instance-Dependent Estimation of Markov Chain and Mixing Time.](http://arxiv.org/abs/1912.06845) | 我们提出了一种实证和实例依赖的方法来估计马尔可夫链的混合时间。我们基于收缩系数来估计混合时间，该系数能够控制混合时间直到强的普遍常数，并且适用于非可逆链。与现有方法相比，我们的方法计算更容易且置信区间更精确，还引入了一种新的分析方法来考虑转移矩阵的附加信息。 |
| [^173] | [Multiplayer Bandit Learning, from Competition to Cooperation.](http://arxiv.org/abs/1908.01135) | 这篇论文研究了多人赌博学习中竞争和合作对探索和利用权衡的影响，模型考虑了不同合作参数下玩家的效用函数，并使用Gittins指数简化了单人问题。 |

# 详细

[^1]: LEAP Hand: 低成本、高效和拟人化机器学习手

    LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning. (arXiv:2309.06440v1 [cs.RO])

    [http://arxiv.org/abs/2309.06440](http://arxiv.org/abs/2309.06440)

    LEAP Hand是一种低成本、高效且拟人化的机器学习手。其新颖的运动结构可以实现最大的灵活性，其成本为其他竞争对手的1/8。LEAP Hand在现实世界中执行多个操作任务，并在所有实验中表现出色。

    

    在机器人学中，灵巧操控一直是一个长期存在的挑战。虽然机器学习技术在这方面显示出一些希望，但结果主要限于模拟。这主要归因于缺乏适当的硬件。在本文中，我们提出了LEAP Hand，这是一种低成本的灵巧和拟人化机器学习手。与以往的手相比，LEAP Hand具有一种新颖的运动结构，无论手指的姿势如何都可以实现最大的灵活性。LEAP Hand成本低廉，并且可以在4个小时内使用现有零件组装而成，成本为2000美元。它能够持续长时间地施加大扭矩。我们展示了LEAP Hand可以在真实世界中执行多个操作任务，从视觉远程操作到从被动视频数据和模拟到真实世界的学习。LEAP Hand在所有实验中都明显优于最接近的竞争对手Allegro Hand，同时成本只有其1/8。我们公开了详细的设计和组装说明，以便其他研究人员可以重复我们的工作。

    Dexterous manipulation has been a long-standing challenge in robotics. While machine learning techniques have shown some promise, results have largely been currently limited to simulation. This can be mostly attributed to the lack of suitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous and anthropomorphic hand for machine learning research. In contrast to previous hands, LEAP Hand has a novel kinematic structure that allows maximal dexterity regardless of finger pose. LEAP Hand is low-cost and can be assembled in 4 hours at a cost of 2000 USD from readily available parts. It is capable of consistently exerting large torques over long durations of time. We show that LEAP Hand can be used to perform several manipulation tasks in the real world -- from visual teleoperation to learning from passive video data and sim2real. LEAP Hand significantly outperforms its closest competitor Allegro Hand in all our experiments while being 1/8th of the cost. We release detailed
    
[^2]: 揭示大型语言模型在生成语义和跨语言克隆方面的潜力

    Unveiling the potential of large language models in generating semantic and cross-language clones. (arXiv:2309.06424v1 [cs.SE])

    [http://arxiv.org/abs/2309.06424](http://arxiv.org/abs/2309.06424)

    本研究利用GPT-3模型在语义和跨语言克隆生成方面的潜力，通过评估、分析和验证，揭示了其在生成给定代码片段的语义和跨语言克隆变体方面的表现。

    

    语义和跨语言代码克隆生成对于代码重用、代码理解、重构和基准测试可能非常有用。OpenAI的GPT模型在这种克隆生成方面有潜力，因为GPT被用于文本生成。当开发人员从Stack Overflow（SO）或系统内部复制/粘贴代码时，可能会出现不一致的更改导致意外行为。同样，如果某人拥有一种编程语言中的代码片段，但在不同的语言中寻找等效功能，则语义跨语言代码克隆生成方法可能会提供有价值的帮助。 在这项研究中，我们使用SemanticCloneBench作为工具，评估了GPT-3模型在生成给定代码片段的语义和跨语言克隆变体方面的表现。我们收集了一组多样化的代码片段，并评估了GPT-3在生成代码变体方面的性能。通过广泛的实验和分析，我们通过9位法官花费了158个小时进行验证。

    Semantic and Cross-language code clone generation may be useful for code reuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model has potential in such clone generation as GPT is used for text generation. When developers copy/paste codes from Stack Overflow (SO) or within a system, there might be inconsistent changes leading to unexpected behaviours. Similarly, if someone possesses a code snippet in a particular programming language but seeks equivalent functionality in a different language, a semantic cross-language code clone generation approach could provide valuable assistance.In this study, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3 model could help generate semantic and cross-language clone variants for a given fragment.We have comprised a diverse set of code fragments and assessed GPT-3s performance in generating code variants.Through extensive experimentation and analysis, where 9 judges spent 158 hours to validate, we investigate 
    
[^3]: 可验证的组合强化学习系统

    Verifiable Reinforcement Learning Systems via Compositionality. (arXiv:2309.06420v1 [eess.SY])

    [http://arxiv.org/abs/2309.06420](http://arxiv.org/abs/2309.06420)

    本研究提出了一个可验证和组合的强化学习框架，通过将多个强化学习子系统组合起来实现整体任务。通过定义子系统之间的接口，实现了任务规范的自动分解，并允许子系统的独立训练和测试。

    

    我们提出了一个可验证和组合的强化学习（RL）框架，其中一组RL子系统被组合在一起以完成一个整体任务。该框架由一个高级模型和一组低级子系统组成。高级模型作为参数化的马尔可夫决策过程用于规划和分析子系统的组合，而低级子系统则作为部分可观测性下操作的深度RL代理实现。通过定义子系统之间的接口，该框架能够自动分解任务规范成独立的子任务规范，并允许子系统的独立训练和测试。我们提出了理论结果保证了

    We propose a framework for verifiable and compositional reinforcement learning (RL) in which a collection of RL subsystems, each of which learns to accomplish a separate subtask, are composed to achieve an overall task. The framework consists of a high-level model, represented as a parametric Markov decision process, which is used to plan and analyze compositions of subsystems, and of the collection of low-level subsystems themselves. The subsystems are implemented as deep RL agents operating under partial observability. By defining interfaces between the subsystems, the framework enables automatic decompositions of task specifications, e.g., reach a target set of states with a probability of at least 0.95, into individual subtask specifications, i.e. achieve the subsystem's exit conditions with at least some minimum probability, given that its entry conditions are met. This in turn allows for the independent training and testing of the subsystems. We present theoretical results guaran
    
[^4]: 计算有效学习指数族分布

    On Computationally Efficient Learning of Exponential Family Distributions. (arXiv:2309.06413v1 [cs.LG])

    [http://arxiv.org/abs/2309.06413](http://arxiv.org/abs/2309.06413)

    本研究提出了一种计算高效的估计器，用于准确学习具有任意精度的自然参数的指数族分布。该估计器是一致的、渐近正态的，并可视为最大似然估计的重新参数化分布。

    

    本研究考虑了以计算和统计的高效方式，准确学习具有任意精度的自然参数的$k$参数截断\textit{最小}指数族分布。我们关注的是支持和自然参数适当有界的情况。虽然传统的最大似然估计器对于这类指数族分布是一致的、渐近正态的和渐近有效的，但其计算复杂度很高。在这项工作中，我们提出了一种新的损失函数和计算高效的估计器，在温和条件下一致且渐近正态。我们证明，在总体水平上，我们的方法可以被看作是同一类指数族分布的参数化分布的最大似然估计。此外，我们还证明了我们的估计器可以解释为最小化特定Bregman得分的解决方案。

    We consider the classical problem of learning, with arbitrary accuracy, the natural parameters of a $k$-parameter truncated \textit{minimal} exponential family from i.i.d. samples in a computationally and statistically efficient manner. We focus on the setting where the support as well as the natural parameters are appropriately bounded. While the traditional maximum likelihood estimator for this class of exponential family is consistent, asymptotically normal, and asymptotically efficient, evaluating it is computationally hard. In this work, we propose a novel loss function and a computationally efficient estimator that is consistent as well as asymptotically normal under mild conditions. We show that, at the population level, our method can be viewed as the maximum likelihood estimation of a re-parameterized distribution belonging to the same class of exponential family. Further, we show that our estimator can be interpreted as a solution to minimizing a particular Bregman score as w
    
[^5]: 用于预测药物-疾病关联的计算方法：综述

    Computational Approaches for Predicting Drug-Disease Associations: A Comprehensive Review. (arXiv:2309.06388v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.06388](http://arxiv.org/abs/2309.06388)

    这篇综述文章介绍了计算方法在药物再定位中预测药物-疾病关联方面的最新进展，并比较了不同方法的预测性能。

    

    在近几十年中，传统的药物研发面临着高成本、长周期和高风险等挑战。为了解决这些问题，许多计算方法已被提出用于预测药物和疾病之间的关系，从而通过药物再定位来降低新药研发的成本、周期和风险。研究人员探索了不同的计算方法来预测药物-疾病关联，包括药物副作用-疾病关联、药物靶点关联和miRNA-疾病关联。在这篇综述中，我们关注于预测药物再定位中药物-疾病关联方法的最新进展。我们首先将这些方法分为几个类别，包括基于神经网络的算法、基于矩阵的算法、推荐算法、基于链接的推理算法以及文本挖掘和语义推理。然后，我们比较了已有方法的预测性能。

    In recent decades, traditional drug research and development have been facing challenges such as high cost, long timelines, and high risks. To address these issues, many computational approaches have been suggested for predicting the relationship between drugs and diseases through drug repositioning, aiming to reduce the cost, development cycle, and risks associated with developing new drugs. Researchers have explored different computational methods to predict drug-disease associations, including drug side effects-disease associations, drug-target associations, and miRNAdisease associations. In this comprehensive review, we focus on recent advances in predicting drug-disease association methods for drug repositioning. We first categorize these methods into several groups, including neural network-based algorithms, matrixbased algorithms, recommendation algorithms, link-based reasoning algorithms, and text mining and semantic reasoning. Then, we compare the prediction performance of exi
    
[^6]: 集成掩模网络

    Ensemble Mask Networks. (arXiv:2309.06382v1 [cs.LG])

    [http://arxiv.org/abs/2309.06382](http://arxiv.org/abs/2309.06382)

    本研究引入了两种机制，灵活的掩模和独特的网络剪枝，使得一个前馈网络能够学习矩阵向量乘法，并且在图形模型中可以用来测试依赖关系或交互顺序。

    

    一个$\mathbb{R}^n\rightarrow \mathbb{R}^n$的前馈网络能够学习矩阵向量乘法吗？本研究引入了两种机制：灵活的掩模用于接收矩阵输入，以及一种独特的网络剪枝方法以尊重掩模的依赖结构。网络可以近似固定操作，如矩阵向量乘法$\phi(A,x) \rightarrow Ax$，这激发了引入的机制在基于图的模型中测试依赖关系或交互顺序的应用。

    Can an $\mathbb{R}^n\rightarrow \mathbb{R}^n$ feedforward network learn matrix-vector multiplication? This study introduces two mechanisms - flexible masking to take matrix inputs, and a unique network pruning to respect the mask's dependency structure. Networks can approximate fixed operations such as matrix-vector multiplication $\phi(A,x) \rightarrow Ax$, motivating the mechanisms introduced with applications towards litmus-testing dependencies or interaction order in graph-based models.
    
[^7]: InstaFlow: 一步即可实现高质量基于扩散的文本到图像生成

    InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation. (arXiv:2309.06380v1 [cs.LG])

    [http://arxiv.org/abs/2309.06380](http://arxiv.org/abs/2309.06380)

    本研究提出了InstaFlow，一种基于扩散的文本到图像生成方法，将稳定扩散模型转化为一步模型，通过修正的流动方法提高了噪声与图像之间的对应关系，实现了高质量、高速度的图像生成。

    

    扩散模型以其出色的质量和创造力彻底改变了文本到图像生成领域。然而，其多步采样过程被认为很慢，通常需要十几步推断才能获得令人满意的结果。以往试图通过蒸馏来提高采样速度和减少计算成本的尝试都未能实现功能齐全的一步模型。本文中，我们探索了一种最近的方法，即修正的流动方法，这种方法到目前为止只应用于小数据集。修正的流动方法的核心在于其重新流动的过程，它将概率流的轨迹变得直线，改进了噪声与图像之间的耦合关系，并通过学生模型便于蒸馏过程。我们提出了一种新的文本条件的流程，将稳定扩散模型（SD）转化为超快速的一步模型，在其中我们发现重新流动在改善噪声与图像之间的对应关系方面起着关键作用。凭借我们的新流程，我们能够以较快的速度直接生成高质量的图像。

    Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce computational costs through distillation have been unsuccessful in achieving a functional one-step model. In this paper, we explore a recent method called Rectified Flow, which, thus far, has only been applied to small datasets. The core of Rectified Flow lies in its \emph{reflow} procedure, which straightens the trajectories of probability flows, refines the coupling between noises and images, and facilitates the distillation process with student models. We propose a novel text-conditioned pipeline to turn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find reflow plays a critical role in improving the assignment between noise and images. Leveraging our new pipeline, w
    
[^8]: 对组织病理学癌症检测的混合经典-量子深度学习模型的对抗攻击

    Adversarial attacks on hybrid classical-quantum Deep Learning models for Histopathological Cancer Detection. (arXiv:2309.06377v1 [quant-ph])

    [http://arxiv.org/abs/2309.06377](http://arxiv.org/abs/2309.06377)

    该论文介绍了对组织病理学癌症检测的混合经典-量子深度学习模型进行对抗攻击的研究，通过使用多个传统模型和量子迁移学习模型的结合，提供了对比分析性能，并评估了在各种对抗攻击下的鲁棒性。

    

    我们在组织病理学癌症检测中提出了量子机器学习的有效应用。本研究强调了混合经典-量子深度学习模型的两个主要应用。第一个应用是使用量子迁移学习策略构建组织病理学癌症检测的分类模型。第二个应用是测试该模型在各种对抗攻击下的性能。我们不仅使用单个迁移学习模型测试混合经典-量子模型，还尤其使用ResNet18、VGG-16、Inception-v3和AlexNet作为特征提取器，并集成了多个基于量子电路的变分量子电路（VQC）以增强表达能力。因此，我们对经典模型和混合经典-量子迁移学习模型在组织病理学癌症检测中的性能进行了对比分析，考虑了多种对抗攻击。

    We present an effective application of quantum machine learning in histopathological cancer detection. The study here emphasizes two primary applications of hybrid classical-quantum Deep Learning models. The first application is to build a classification model for histopathological cancer detection using the quantum transfer learning strategy. The second application is to test the performance of this model for various adversarial attacks. Rather than using a single transfer learning model, the hybrid classical-quantum models are tested using multiple transfer learning models, especially ResNet18, VGG-16, Inception-v3, and AlexNet as feature extractors and integrate it with several quantum circuit-based variational quantum circuits (VQC) with high expressibility. As a result, we provide a comparative analysis of classical models and hybrid classical-quantum transfer learning models for histopathological cancer detection under several adversarial attacks. We compared the performance accu
    
[^9]: 建模推荐系统生态系统：机制设计、强化学习和生成模型的交叉研究挑战

    Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models. (arXiv:2309.06375v1 [cs.AI])

    [http://arxiv.org/abs/2309.06375](http://arxiv.org/abs/2309.06375)

    建模推荐系统生态系统需要考虑参与者激励、行为以及策略引发的相互作用，通过强化学习进行长期优化，使用社会选择方法进行权衡，并减少信息不对称。

    

    现代推荐系统位于涵盖用户、内容提供商、广告商和其他参与者行为的复杂生态系统的核心。尽管如此，大多数推荐系统研究的重点，以及大多数重要实用推荐系统，仅限于个别用户推荐的局部、短视优化。这给推荐系统可能为用户带来的长期效用带来了重大成本。我们认为，如果要最大化系统对这些参与者的价值并提高整体生态系统的“健康”状况，有必要明确地对系统中所有参与者的激励和行为进行建模，并对其策略引发的相互作用进行建模。为此需要：使用强化学习等技术进行长期优化；使用社会选择方法为不同参与者的效用进行不可避免的权衡；减少信息不对称。

    Modern recommender systems lie at the heart of complex ecosystems that couple the behavior of users, content providers, advertisers, and other actors. Despite this, the focus of the majority of recommender research -- and most practical recommenders of any import -- is on the local, myopic optimization of the recommendations made to individual users. This comes at a significant cost to the long-term utility that recommenders could generate for its users. We argue that explicitly modeling the incentives and behaviors of all actors in the system -- and the interactions among them induced by the recommender's policy -- is strictly necessary if one is to maximize the value the system brings to these actors and improve overall ecosystem "health". Doing so requires: optimization over long horizons using techniques such as reinforcement learning; making inevitable tradeoffs in the utility that can be generated for different actors using the methods of social choice; reducing information asymm
    
[^10]: 使用Reed-Muller码进行具有拒绝和恢复功能的分类

    Using Reed-Muller Codes for Classification with Rejection and Recovery. (arXiv:2309.06359v1 [cs.LG])

    [http://arxiv.org/abs/2309.06359](http://arxiv.org/abs/2309.06359)

    本文提出了一种使用Reed-Muller码的分类方法，该方法可以纠正和拒绝输入，并在不同程度的对抗攻击下具有良好的正确性。

    

    在实际应用中部署分类器时，用户希望它们能够适当地响应输入。然而，传统的分类器无法处理远离其所训练分布的输入。恶意行为者可以利用这个缺陷，通过进行有针对性的扰动来导致分类器给出错误的输出。拒绝分类方法试图通过允许网络在对输入的置信度较低时拒绝进行分类来解决这个问题。这对于强对抗样本效果很好，但也导致了对轻微扰动图像的拒绝，而直观上这些图像可能是可以正确分类的。为了解决这些问题，我们提出了Reed-Muller聚合网络（RMAggNet），这是一种受到Reed-Muller纠错码启发的分类器，可以纠正和拒绝输入。本文表明，RMAggNet可以在多个不同程度的对抗攻击下，最小化错误并保持很好的正确性。

    When deploying classifiers in the real world, users expect them to respond to inputs appropriately. However, traditional classifiers are not equipped to handle inputs which lie far from the distribution they were trained on. Malicious actors can exploit this defect by making adversarial perturbations designed to cause the classifier to give an incorrect output. Classification-with-rejection methods attempt to solve this problem by allowing networks to refuse to classify an input in which they have low confidence. This works well for strongly adversarial examples, but also leads to the rejection of weakly perturbed images, which intuitively could be correctly classified. To address these issues, we propose Reed-Muller Aggregation Networks (RMAggNet), a classifier inspired by Reed-Muller error-correction codes which can correct and reject inputs. This paper shows that RMAggNet can minimise incorrectness while maintaining good correctness over multiple adversarial attacks at different per
    
[^11]: 使用分数后验概率对汤普森抽样进行广义遗憾分析

    Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors. (arXiv:2309.06349v1 [stat.ML])

    [http://arxiv.org/abs/2309.06349](http://arxiv.org/abs/2309.06349)

    这项研究对使用分数后验概率的汤普森抽样算法进行了广义遗憾分析，获得了依赖于实例和实例独立的频率遗憾界。这对多臂赌博问题的解决有重要意义。

    

    汤普森抽样（TS）是解决随机多臂赌博问题的最流行和最早的算法之一。我们考虑了TS的一个变种，称为α-TS，其中我们使用分数或α-后验（α∈（0,1））代替标准后验分布。为了计算α-后验，标准后验的定义中的似然函数被一个因子α搅拌。对于α-TS，我们在非常温和的先验和奖励分布条件下获得了既依赖于实例的Ο（∑_{k≠i^*}Δ_k（\frac{\log(T)}{C(α)Δ_k^2}+\frac{1}{2}））也依赖于实例独立的Ο（\sqrt{KT\log K}）频率遗憾界，其中Δ_k是第k个和最好的臂的真实均值奖励之间的差，而C(α)是已知的常数。子高斯和指数族模型都满足我们对奖励分布的一般条件。我们对先验的条件是...

    Thompson sampling (TS) is one of the most popular and earliest algorithms to solve stochastic multi-armed bandit problems. We consider a variant of TS, named $\alpha$-TS, where we use a fractional or $\alpha$-posterior ($\alpha\in(0,1)$) instead of the standard posterior distribution. To compute an $\alpha$-posterior, the likelihood in the definition of the standard posterior is tempered with a factor $\alpha$. For $\alpha$-TS we obtain both instance-dependent $\mathcal{O}\left(\sum_{k \neq i^*} \Delta_k\left(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2} \right)\right)$ and instance-independent $\mathcal{O}(\sqrt{KT\log K})$ frequentist regret bounds under very mild conditions on the prior and reward distributions, where $\Delta_k$ is the gap between the true mean rewards of the $k^{th}$ and the best arms, and $C(\alpha)$ is a known constant. Both the sub-Gaussian and exponential family models satisfy our general conditions on the reward distribution. Our conditions on the prior di
    
[^12]: 使用优化的消息传递神经网络进行带隙回归

    Band-gap regression with architecture-optimized message-passing neural networks. (arXiv:2309.06348v1 [physics.comp-ph])

    [http://arxiv.org/abs/2309.06348](http://arxiv.org/abs/2309.06348)

    这项工作使用优化的消息传递神经网络（MPNN）进行带隙回归预测，通过对密度泛函理论数据进行分类和神经架构搜索，实现了对非金属材料带隙的准确预测，并在不确定性量化方面取得了显著进展。

    

    基于图的神经网络，特别是消息传递神经网络（MPNNs），在预测固体材料的物理性质方面显示出巨大的潜力。在这项工作中，我们首先使用来自AFLOW数据库的密度泛函理论数据通过MPNN对材料进行分类，判断其金属或半导体/绝缘体属性。然后，我们进行神经架构搜索，探索MPNN的模型架构和超参数空间，以预测被识别为非金属材料的带隙。搜索中的参数包括消息传递步骤的数量、潜在空间的大小和激活函数等。从搜索中筛选出的表现最佳的模型被汇总成一个集合，显著优于现有文献中的模型。使用蒙特卡洛Dropout和集成方法对不确定性进行量化，集成方法表现更好。我们分析了集成模型在晶体系统、晶格尺寸和其他材料特性方面的适用性范围。

    Graph-based neural networks and, specifically, message-passing neural networks (MPNNs) have shown great potential in predicting physical properties of solids. In this work, we train an MPNN to first classify materials through density functional theory data from the AFLOW database as being metallic or semiconducting/insulating. We then perform a neural-architecture search to explore the model architecture and hyperparameter space of MPNNs to predict the band gaps of the materials identified as non-metals. The parameters in the search include the number of message-passing steps, latent size, and activation-function, among others. The top-performing models from the search are pooled into an ensemble that significantly outperforms existing models from the literature. Uncertainty quantification is evaluated with Monte-Carlo Dropout and ensembling, with the ensemble method proving superior. The domain of applicability of the ensemble model is analyzed with respect to the crystal systems, the
    
[^13]: 使用马尔科夫边界引导修剪学习最简化Tsetlin机器子句

    Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning. (arXiv:2309.06315v1 [cs.LG])

    [http://arxiv.org/abs/2309.06315](http://arxiv.org/abs/2309.06315)

    本论文提出了一种新的Tsetlin机器（TM）反馈方案，通过引入马尔科夫边界来学习最简化的Tsetlin机器子句，以提供最佳的特征集。

    

    如果包含了预测变量所需的所有信息，那么一组变量就是随机变量的马尔科夫盖被。如果盖被无法减少而不丢失有用信息，则被称为马尔科夫边界。识别随机变量的马尔科夫边界是有优势的，因为边界外的所有变量都是多余的。因此，马尔科夫边界提供了最佳的特征集。然而，从数据中学习马尔科夫边界具有两个挑战。如果从马尔科夫边界中移除一个或多个变量，边界外的变量可能开始提供信息。相反，边界内的变量可能停止提供信息。每个候选变量的真正作用只有在识别了马尔科夫边界后才会显现。在本文中，我们提出了一种新的Tsetlin机器（TM）反馈方案，以补充类型I和类型II的反馈。该方案引入了一种新颖的有限状态自动机 - 一种上下文特定的机器。

    A set of variables is the Markov blanket of a random variable if it contains all the information needed for predicting the variable. If the blanket cannot be reduced without losing useful information, it is called a Markov boundary. Identifying the Markov boundary of a random variable is advantageous because all variables outside the boundary are superfluous. Hence, the Markov boundary provides an optimal feature set. However, learning the Markov boundary from data is challenging for two reasons. If one or more variables are removed from the Markov boundary, variables outside the boundary may start providing information. Conversely, variables within the boundary may stop providing information. The true role of each candidate variable is only manifesting when the Markov boundary has been identified. In this paper, we propose a new Tsetlin Machine (TM) feedback scheme that supplements Type I and Type II feedback. The scheme introduces a novel Finite State Automaton - a Context-Specific I
    
[^14]: 在行驶车辆上的语义和关节人体感知

    Semantic and Articulated Pedestrian Sensing Onboard a Moving Vehicle. (arXiv:2309.06313v1 [cs.CV])

    [http://arxiv.org/abs/2309.06313](http://arxiv.org/abs/2309.06313)

    该论文研究了在行驶车辆上进行语义和关节人体感知的挑战，指出车载视频的大量前进运动导致难以进行3D重建，以及物体检测和人体感知模型在车载视频上表现较差的原因。作者提出通过LiDAR数据进行关节人体感知的基准可以促进人体感知和交通预测的研究，并有可能提高行人交通安全性。

    

    由于车辆的大量前进运动，从车载视频中进行3D重建非常困难。即使与标准基准相比，物体检测和人体感知模型在车载视频上的表现也明显较差，因为与标准物体检测基准相比，物体往往距离摄像机较远，图像质量常常受到运动模糊的影响，且经常发生遮挡。这导致交通数据特定的基准的普及。最近，光检测与测距（LiDAR）传感器已经在直接估计深度方面变得流行，而无需进行3D重建。然而，与基于图像的方法相比，基于LiDAR的方法在远程关节人体检测方面仍然不足。我们假设针对LiDAR数据中的关节人体感知的基准可以促进人体感知和交通预测方面的增加研究，并有可能提高行人交通安全性。

    It is difficult to perform 3D reconstruction from on-vehicle gathered video due to the large forward motion of the vehicle. Even object detection and human sensing models perform significantly worse on onboard videos when compared to standard benchmarks because objects often appear far away from the camera compared to the standard object detection benchmarks, image quality is often decreased by motion blur and occlusions occur often. This has led to the popularisation of traffic data-specific benchmarks. Recently Light Detection And Ranging (LiDAR) sensors have become popular to directly estimate depths without the need to perform 3D reconstructions. However, LiDAR-based methods still lack in articulated human detection at a distance when compared to image-based methods. We hypothesize that benchmarks targeted at articulated human sensing from LiDAR data could bring about increased research in human sensing and prediction in traffic and could lead to improved traffic safety for pedestr
    
[^15]: 在公共交通系统中建模供需

    Modeling Supply and Demand in Public Transportation Systems. (arXiv:2309.06299v1 [cs.LG])

    [http://arxiv.org/abs/2309.06299](http://arxiv.org/abs/2309.06299)

    该论文在公共交通系统中建立了供需模型，利用数据分析和机器学习技术揭示了运营服务中的空缺。

    

    哈里森堡公共交通部门旨在利用其数据提高运营效率和效果。我们构建了两个供需模型，帮助部门识别服务中的空缺。模型考虑了许多变量，包括哈里森堡市向联邦政府报告的方式以及最脆弱人口聚集的区域。我们采用数据分析和机器学习技术进行预测。

    The Harrisonburg Department of Public Transportation (HDPT) aims to leverage their data to improve the efficiency and effectiveness of their operations. We construct two supply and demand models that help the department identify gaps in their service. The models take many variables into account, including the way that the HDPT reports to the federal government and the areas with the most vulnerable populations in Harrisonburg City. We employ data analysis and machine learning techniques to make our predictions.
    
[^16]: 数据驱动的增材制造知识的可迁移性分析：粉床熔化和定向能量沉积之间的案例研究

    Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition. (arXiv:2309.06286v1 [cs.AI])

    [http://arxiv.org/abs/2309.06286](http://arxiv.org/abs/2309.06286)

    本论文提出了一种三步知识迁移性分析框架来支持数据驱动的增材制造知识的迁移。研究发现，通过利用各种增材制造技术之间的相似性和应用迁移学习的方法，可以将现有的解决方案从一个工艺或问题转移到另一个工艺或问题中。

    

    最近几年，数据驱动的增材制造（AM）研究取得了显著的成功，产生了大量的科学文献。这些研究中的知识涉及到AM和人工智能（AI）领域，但没有以一种整合的方式进行挖掘和形式化。此外，目前没有支持数据驱动知识从一个上下文迁移到另一个上下文的工具或指南。因此，仅针对特定的AM工艺技术开发和验证了特定的AI技术的数据驱动解决方案。有潜力利用各种AM技术之间的内在相似性，利用AI（如迁移学习）从一个工艺或问题中适应现有的解决方案。我们提出了一种三步知识迁移性分析框架来支持数据驱动的AM知识迁移。作为迁移性分析的先决条件，AM知识被转化为识别出的知识组件。

    Data-driven research in Additive Manufacturing (AM) has gained significant success in recent years. This has led to a plethora of scientific literature to emerge. The knowledge in these works consists of AM and Artificial Intelligence (AI) contexts that have not been mined and formalized in an integrated way. Moreover, no tools or guidelines exist to support data-driven knowledge transfer from one context to another. As a result, data-driven solutions using specific AI techniques are being developed and validated only for specific AM process technologies. There is a potential to exploit the inherent similarities across various AM technologies and adapt the existing solutions from one process or problem to another using AI, such as Transfer Learning. We propose a three-step knowledge transferability analysis framework in AM to support data-driven AM knowledge transfer. As a prerequisite to transferability analysis, AM knowledge is featurized into identified knowledge components. The fra
    
[^17]: ELRA: 指数学习率自适应梯度下降优化方法

    ELRA: Exponential learning rate adaption gradient descent optimization method. (arXiv:2309.06274v1 [cs.LG])

    [http://arxiv.org/abs/2309.06274](http://arxiv.org/abs/2309.06274)

    提出了一种快速、从基础原理出发且无超参数依赖的基于梯度的优化算法，通过情境感知调整学习率，具有高成功率和快速收敛速度，并且具备旋转不变性。

    

    我们提出了一种新颖的，快速（指数学习率自适应），从基础原理出发（无超参数依赖）的基于梯度的优化算法。该方法的主要思想是通过情境感知来调整学习率α，主要是追求正交邻近梯度。该方法具有高成功率和快速收敛速度，并且不依赖于手动调节的参数，具有更大的通用性。它可以应用于任意维度n的问题，并且与问题的维度线性（O(n)阶）扩展。它优化凸和非凸连续景观，并提供一定程度的梯度。与Ada系列（AdaGrad，AdaMax，AdaDelta，Adam等）相比，该方法具有旋转不变性：优化路径和性能与坐标选择无关。通过在MNIST基准数据集上与最先进的优化器进行广泛实验，展示了其出色的性能。我们根据其核心思想将这种新类优化器命名为指数学习率自适应梯度下降优化方法（ELRA）。

    We present a novel, fast (exponential rate adaption), ab initio (hyper-parameter-free) gradient based optimizer algorithm. The main idea of the method is to adapt the learning rate $\alpha$ by situational awareness, mainly striving for orthogonal neighboring gradients. The method has a high success and fast convergence rate and does not rely on hand-tuned parameters giving it greater universality. It can be applied to problems of any dimensions n and scales only linearly (of order O(n)) with the dimension of the problem. It optimizes convex and non-convex continuous landscapes providing some kind of gradient. In contrast to the Ada-family (AdaGrad, AdaMax, AdaDelta, Adam, etc.) the method is rotation invariant: optimization path and performance are independent of coordinate choices. The impressive performance is demonstrated by extensive experiments on the MNIST benchmark data-set against state-of-the-art optimizers. We name this new class of optimizers after its core idea Exponential 
    
[^18]: ssVERDICT：用于增强前列腺肿瘤表征的自监督VERDICT-MRI

    ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation. (arXiv:2309.06268v1 [eess.IV])

    [http://arxiv.org/abs/2309.06268](http://arxiv.org/abs/2309.06268)

    本研究引入了一种自监督DNN方法，用于估计前列腺肿瘤的VERDICT模型参数并减少计算成本。

    

    MRI在前列腺癌（PCa）的诊断中越来越被使用，其中扩散MRI（dMRI）起着重要作用。当与计算模型结合时，dMRI可以估计细胞大小等微观结构信息。传统上，这些模型使用非线性最小二乘（NLLS）曲线拟合方法进行拟合，与高计算成本相关。监督式深度神经网络（DNN）是一种高效的替代方法，但其性能受到合成训练数据的底层分布的显著影响。自监督学习是一种有吸引力的替代方法，网络在此方法中学习输入数据的特征，而不使用单独的训练数据集。到目前为止，这种方法仅应用于对微不足道的dMRI模型的拟合。在这里，我们引入了一种自监督DNN，用于估计前列腺VERDICT（Tumours中的血管、细胞外和受限扩散）模型的参数。我们展示了，通过这种方法，用于前列腺肿瘤的VERDICT模型的参数可以有效估计并且具有较低的计算成本。

    MRI is increasingly being used in the diagnosis of prostate cancer (PCa), with diffusion MRI (dMRI) playing an integral role. When combined with computational models, dMRI can estimate microstructural information such as cell size. Conventionally, such models are fit with a nonlinear least squares (NLLS) curve fitting approach, associated with a high computational cost. Supervised deep neural networks (DNNs) are an efficient alternative, however their performance is significantly affected by the underlying distribution of the synthetic training data. Self-supervised learning is an attractive alternative, where instead of using a separate training dataset, the network learns the features of the input data itself. This approach has only been applied to fitting of trivial dMRI models thus far. Here, we introduce a self-supervised DNN to estimate the parameters of the VERDICT (Vascular, Extracellular and Restricted DIffusion for Cytometry in Tumours) model for prostate. We demonstrate, for
    
[^19]: 通过强化学习实现离散一致性闭塞方案用于大涡模拟

    Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning. (arXiv:2309.06260v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2309.06260](http://arxiv.org/abs/2309.06260)

    本研究提出了一种使用强化学习方法开发离散一致性闭塞方案的新方法，其中将大涡模拟中的闭塞模型系数调整任务定义为马尔可夫决策过程，通过后期的强化学习解决。这一方法能够将模型调整到实际的离散化中并考虑离散化和模型本身之间的相互作用。通过优化显式和隐式闭塞模型中的局部涡粘度模型和混合策略，实现了闭塞模型的优化。

    

    我们提出了一种新的方法来开发用于隐式滤波的大涡模拟（LES）中的离散一致性闭塞方案。在隐式滤波的LES中，感应滤波核和闭塞项是由网格和离散化操作符的性质决定的，从而产生额外的未知计算细网项。因此，将LES闭塞模型的系数调整任务定义为马尔可夫决策过程，并通过强化学习在后期解决。这允许将模型调整到实际的离散化中，同时还包括离散化和模型本身之间的相互作用。这个优化框架应用于显式和隐式闭塞模型。通过优化局部涡粘度模型作为显式模型，通过强化学习来识别混合不连续G的最优混合策略。

    We propose a novel method for developing discretization-consistent closure schemes for implicitly filtered Large Eddy Simulation (LES). In implicitly filtered LES, the induced filter kernel, and thus the closure terms, are determined by the properties of the grid and the discretization operator, leading to additional computational subgrid terms that are generally unknown in a priori analysis. Therefore, the task of adapting the coefficients of LES closure models is formulated as a Markov decision process and solved in an a posteriori manner with Reinforcement Learning (RL). This allows to adjust the model to the actual discretization as it also incorporates the interaction between the discretization and the model itself. This optimization framework is applied to both explicit and implicit closure models. An element-local eddy viscosity model is optimized as the explicit model. For the implicit modeling, RL is applied to identify an optimal blending strategy for a hybrid discontinuous G
    
[^20]: 专业性与广泛性：关于基础模型微调中灾难性遗忘的实证研究

    Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models. (arXiv:2309.06256v1 [cs.LG])

    [http://arxiv.org/abs/2309.06256](http://arxiv.org/abs/2309.06256)

    本研究实证了基础模型微调中的灾难性遗忘现象，微调过程中追求专业性会导致模型的广泛性损失。

    

    基础模型，包括视觉语言模型(VLMs)和大型语言模型(LLMs)，具有处理多样分布和任务的广泛性，这源于它们广泛的预训练数据集。对基础模型进行微调是提高任务性能或调整模型行为与人类期望一致的常见做法，使其获得专业性。然而，用于微调的小型数据集可能无法充分覆盖预训练过程中遇到的多样分布和任务。因此，追求微调过程中的专业性可能导致模型的广泛性损失，这与深度学习中的灾难性遗忘(Catastrophic Forgetting, CF)相关。在本研究中，我们展示了这种现象在VLMs和LLMs中的存在。例如，对像CLIP这样的VLM进行在ImageNet上的微调会导致处理多样分布的广泛性损失，对医学领域的Galactica进行微调则会导致遵循指令的能力损失。

    Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions
    
[^21]: 通过精细的模态评估增强多模态协作

    Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v1 [cs.CV])

    [http://arxiv.org/abs/2309.06255](http://arxiv.org/abs/2309.06255)

    本文提出了一种精细的模态评估指标，用于评估每个模态在样本级别的贡献，并发现多模态模型倾向于依赖一个特定的模态，导致其他模态的贡献较低。

    

    多模态学习的一个主要问题是如何将来自不同模态的异质信息共同结合起来。然而，大多数模型在多模态协作方面常常存在不尽人意的问题，不能很好地共同利用所有模态。一些方法被提出来识别和增强学习效果较差的模态，但往往难以在理论上提供对样本级别多模态协作的细粒度观察和支持。因此，合理观察和改进模态之间细粒度的协作尤为重要，尤其是在面对模态差异在不同样本之间可能变化的实际场景时。为了实现这一目标，我们引入了一种精细的模态评估指标，以评估每个模态在样本级别的贡献。通过模态评估，我们遗憾地发现多模态模型倾向于依赖一个特定的模态，导致其他模态的贡献较低。我们进一步分析了这个问题。

    One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this iss
    
[^22]: 重新思考使用电竞数据评估概率估计模型的评价指标

    Rethinking Evaluation Metric for Probability Estimation Models Using Esports Data. (arXiv:2309.06248v1 [cs.LG])

    [http://arxiv.org/abs/2309.06248](http://arxiv.org/abs/2309.06248)

    本研究首次提出了使用布里尔分数和预期校准误差作为评估电竞领域胜率估计模型性能的指标，并且提出了一种新的平衡分数指标。

    

    概率估计模型在天气预报、推荐系统和体育分析等领域发挥着重要作用。在众多概率估计模型中，由于没有真实概率数据可用，很难评估哪个模型提供可靠的概率。电竞中的胜率估计模型是概率估计领域中一个正在被积极研究的领域，其计算在特定游戏状态下的胜率。然而，大多数之前的研究使用准确率作为评估模型的指标，准确率只能衡量区分性能。在这项工作中，我们首次研究了布里尔分数和预期校准误差（ECE）作为替代准确性的评估指标，用于电竞领域中胜率估计模型的性能评估。基于分析，我们提出了一种名为平衡分数的新指标，它在六个好属性方面是简单而有效的指标。

    Probability estimation models play an important role in various fields, such as weather forecasting, recommendation systems, and sports analysis. Among several models estimating probabilities, it is difficult to evaluate which model gives reliable probabilities since the ground-truth probabilities are not available. The win probability estimation model for esports, which calculates the win probability under a certain game state, is also one of the fields being actively studied in probability estimation. However, most of the previous works evaluated their models using accuracy, a metric that only can measure the performance of discrimination. In this work, we firstly investigate the Brier score and the Expected Calibration Error (ECE) as a replacement of accuracy used as a performance evaluation metric for win probability estimation models in esports field. Based on the analysis, we propose a novel metric called Balance score which is a simple yet effective metric in terms of six good p
    
[^23]: 一致性和适应性是验证机器学习回归任务中基于方差的不确定性量化度量的互补目标

    Consistency and adaptivity are complementary targets for the validation of variance-based uncertainty quantification metrics in machine learning regression tasks. (arXiv:2309.06240v1 [stat.ML])

    [http://arxiv.org/abs/2309.06240](http://arxiv.org/abs/2309.06240)

    这篇论文研究了机器学习回归任务中基于方差的不确定性量化度量的验证，发现一致性和适应性是互补的验证目标，并提出了适应性验证方法。

    

    可靠的不确定性量化是材料和化学科学中许多研究的焦点。目前已经认识到平均校准是不足够的，大多数研究都使用额外的方法来测试条件校准，即一致性。一致性主要通过可靠性图来评估。然而，除了平均校准之外还存在一种方法，即基于输入特征的条件校准，也就是适应性。实际上，适应性是ML-UQ方法的最终用户关注的主要问题，他们寻求对特征空间中的任何点的预测和不确定性的可靠性。本文旨在展示一致性和适应性是互补的验证目标，并且好的一致性并不意味着好的适应性。文章提出并在一个典型示例上进行了适应性验证方法的说明。

    Reliable uncertainty quantification (UQ) in machine learning (ML) regression tasks is becoming the focus of many studies in materials and chemical science. It is now well understood that average calibration is insufficient, and most studies implement additional methods testing the conditional calibration with respect to uncertainty, i.e. consistency. Consistency is assessed mostly by so-called reliability diagrams. There exists however another way beyond average calibration, which is conditional calibration with respect to input features, i.e. adaptivity. In practice, adaptivity is the main concern of the final users of a ML-UQ method, seeking for the reliability of predictions and uncertainties for any point in features space. This article aims to show that consistency and adaptivity are complementary validation targets, and that a good consistency does not imply a good adaptivity. Adapted validation methods are proposed and illustrated on a representative example.
    
[^24]: 通过最优输运理论实现风险感知的强化学习

    Risk-Aware Reinforcement Learning through Optimal Transport Theory. (arXiv:2309.06239v1 [cs.LG])

    [http://arxiv.org/abs/2309.06239](http://arxiv.org/abs/2309.06239)

    本文将最优输运理论与强化学习相结合，创建了一个风险感知的框架，通过修改目标函数，在最大化期望奖励的同时捕捉潜在风险，提供了数学精确的方法来提升强化学习中风险考量的重要性。

    

    在强化学习（RL）操作的动态和不确定环境中，风险管理成为确保可靠决策的关键因素。传统RL方法在奖励优化方面有效，但常常忽视潜在风险的情况。针对这一问题，本文首次将最优输运（OT）理论与RL相结合，创建了一个风险感知的框架。我们的方法修改了目标函数，确保得到的策略不仅最大化期望奖励，还遵守OT距离所指示的状态访问分布和期望风险配置之间的风险约束。通过利用OT的数学精确性，我们提供了一个公式，将风险考量与传统RL目标并列。通过一系列定理，我们证实了我们的贡献，揭示了风险分布、最优值函数和策略行为之间的关系。通过OT的视角，这项工作揭示了风险感知的强化学习的重要性。

    In the dynamic and uncertain environments where reinforcement learning (RL) operates, risk management becomes a crucial factor in ensuring reliable decision-making. Traditional RL approaches, while effective in reward optimization, often overlook the landscape of potential risks. In response, this paper pioneers the integration of Optimal Transport (OT) theory with RL to create a risk-aware framework. Our approach modifies the objective function, ensuring that the resulting policy not only maximizes expected rewards but also respects risk constraints dictated by OT distances between state visitation distributions and the desired risk profiles. By leveraging the mathematical precision of OT, we offer a formulation that elevates risk considerations alongside conventional RL objectives. Our contributions are substantiated with a series of theorems, mapping the relationships between risk distributions, optimal value functions, and policy behaviors. Through the lens of OT, this work illumin
    
[^25]: 踏出的第一步最困难：在大语言模型中表示和分词时间数据的陷阱

    The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. (arXiv:2309.06236v1 [cs.LG])

    [http://arxiv.org/abs/2309.06236](http://arxiv.org/abs/2309.06236)

    这项研究讨论了在大语言模型中表示和分词时间数据的困难，并提出了解决方案，如使用轻量级嵌入层进行提示调整和多模态适配器。

    

    大型语言模型(LLMs)在各种任务中展示了出色的泛化能力，导致人们越来越多地将它们用作个人助手和通用计算引擎。然而，将数值/时间数据输入到这些模型中时，会出现一个明显的障碍，比如从可穿戴设备或电子健康记录中获取的数据。LLMs在其输入中使用分词器将文本分解为较小的单位。然而，分词器并不设计用于表示数值，并可能难以理解重复模式和上下文，将连续的值视为单独的标记并忽略它们的时间关系。在这里，我们讨论了最近使用LLMs进行以人为中心任务的研究，并提出了一个案例研究，展示了流行的LLMs错误地对时间数据进行分词。为了解决这个问题，我们强调了一些潜在的解决方案，例如使用轻量级嵌入层进行提示调整和多模态适配器。

    Large Language Models (LLMs) have demonstrated remarkable generalization across diverse tasks, leading individuals to increasingly use them as personal assistants and universal computing engines. Nevertheless, a notable obstacle emerges when feeding numerical/temporal data into these models, such as data sourced from wearables or electronic health records. LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships. Here, we discuss recent works that employ LLMs for human-centric tasks such as in mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that c
    
[^26]: 单指数模型中最佳子集选择的一致性和可扩展算法

    A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models. (arXiv:2309.06230v1 [stat.ML])

    [http://arxiv.org/abs/2309.06230](http://arxiv.org/abs/2309.06230)

    该论文提出了针对高维单指数模型中最佳子集选择的一致性和可扩展算法，通过使用广义信息准则来确定支持的回归系数大小，消除了模型选择的调优需求，并具有子集选择一致性和高概率下的理想属性。

    

    高维数据的分析引发了对单指数模型（SIMs）和最佳子集选择的增加兴趣。SIMs为高维数据提供了一种可解释和灵活的建模框架，而最佳子集选择旨在从大量的预测因子中找到稀疏模型。然而，在高维模型中的最佳子集选择被认为是计算上难以处理的。现有的方法倾向于放宽选择，但不能得到最佳子集解。在本文中，我们通过提出第一个经过证明的针对高维SIMs中最佳子集选择的可扩展算法，直接解决了计算难题。我们的算法解具有子集选择一致性，并且几乎肯定具有用于参数估计的虚拟属性。该算法包括一个广义信息准则来确定回归系数的支持大小，消除模型选择调整。此外，我们的方法不假设误差分布或特定参数。

    Analysis of high-dimensional data has led to increased interest in both single index models (SIMs) and best subset selection. SIMs provide an interpretable and flexible modeling framework for high-dimensional data, while best subset selection aims to find a sparse model from a large set of predictors. However, best subset selection in high-dimensional models is known to be computationally intractable. Existing methods tend to relax the selection, but do not yield the best subset solution. In this paper, we directly tackle the intractability by proposing the first provably scalable algorithm for best subset selection in high-dimensional SIMs. Our algorithmic solution enjoys the subset selection consistency and has the oracle property with a high probability. The algorithm comprises a generalized information criterion to determine the support size of the regression coefficients, eliminating the model selection tuning. Moreover, our method does not assume an error distribution or a specif
    
[^27]: 基于地理气象数据的深度神经网络用于长期干旱预测

    Long-term drought prediction using deep neural networks based on geospatial weather data. (arXiv:2309.06212v1 [cs.LG])

    [http://arxiv.org/abs/2309.06212](http://arxiv.org/abs/2309.06212)

    基于地理气象数据的深度神经网络用于长期干旱预测，提出了一种端到端解决方案来预测特定地区干旱概率。采用卷积LSTM和Transformer模型相比其他基线模型能够获得更高的准确性。

    

    在农业实践中，准确预测特定地区干旱概率对于决策具有重要性。尤其对于长期决策，提前一年进行预测至关重要。然而，由于感兴趣区域及其相邻区域内各种因素的复杂相互作用，预测这一概率存在挑战。在本研究中，我们提出了一种基于各种时空神经网络的端到端解决方案来解决这个问题。所考虑的模型主要是根据Palmer干旱严重指数（PDSI）预测感兴趣亚区的干旱强度，利用气候模型的内在因素和见解来提高干旱预测的准确性。比较评估结果表明，与基准梯度提升和逻辑回归解决方案相比，卷积LSTM（ConvLSTM）和Transformer模型的准确性更高。前两种模型取得了令人印象深刻的ROC AUC分数，高达0.90

    The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.  Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 
    
[^28]: 通过平滑软阈值函数的展开ISTA和ADMM网络的优化保证

    Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding. (arXiv:2309.06195v1 [cs.LG])

    [http://arxiv.org/abs/2309.06195](http://arxiv.org/abs/2309.06195)

    本文研究了通过平滑软阈值函数的展开ISTA和ADMM网络在过参数化区域中的优化保证，通过满足特定区域的PL$^*$条件实现接近零训练损失。

    

    解决线性反问题在许多应用中起着关键作用。基于算法展开的，模型感知的数据驱动方法在有效解决这些问题方面引起了广泛关注。学习的迭代软阈值算法（LISTA）和交替方向乘子压缩感知网络（ADMM-CSNet）是两种广泛使用的这种方法，分别基于ISTA和ADMM算法。本文研究了具有平滑软阈值在过参数化（OP）区域的有限层展开网络（如LISTA和ADMM-CSNet）的优化保证，即通过学习迭代次数的增加实现接近零训练损失。我们通过利用Polyak-Lojasiewicz的一个修改版本，即PL$^*$条件，实现了这一目标。在损失函数图景的特定区域内满足PL$^*$条件可以确保全局最小值的存在，并实现从初始化时的指数收敛。

    Solving linear inverse problems plays a crucial role in numerous applications. Algorithm unfolding based, model-aware data-driven approaches have gained significant attention for effectively addressing these problems. Learned iterative soft-thresholding algorithm (LISTA) and alternating direction method of multipliers compressive sensing network (ADMM-CSNet) are two widely used such approaches, based on ISTA and ADMM algorithms, respectively. In this work, we study optimization guarantees, i.e., achieving near-zero training loss with the increase in the number of learning epochs, for finite-layer unfolded networks such as LISTA and ADMM-CSNet with smooth soft-thresholding in an over-parameterized (OP) regime. We achieve this by leveraging a modified version of the Polyak-Lojasiewicz, denoted PL$^*$, condition. Satisfying the PL$^*$ condition within a specific region of the loss landscape ensures the existence of a global minimum and exponential convergence from initialization using gra
    
[^29]: 评估学习型语音增强系统在噪声和混响环境中的泛化差距

    Assessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments. (arXiv:2309.06183v1 [eess.AS])

    [http://arxiv.org/abs/2309.06183](http://arxiv.org/abs/2309.06183)

    本研究介绍了一种泛化评估框架，用于评估学习型语音增强系统在噪声和混响环境中的泛化能力，并提出了使用在测试条件下训练的参考模型作为难度代理。

    

    噪声和混响语音混合物的声学变异性受多个因素影响，例如目标说话者和干扰噪声的频谱时域特性，信噪比和房间特性。这种大的变异性给基于学习的语音增强系统带来了很大的挑战，因为训练和测试条件之间的不匹配会大大降低系统的性能。通常通过使用与训练期间使用的不同的新语音、噪声或双耳房间脉冲响应（BRIR）数据库对系统进行测试来评估对未知条件的泛化能力。然而，语音增强任务的难度可能会随着数据库的变化而改变，这可能会对结果产生重大影响。本研究引入了一种泛化评估框架，该框架使用在测试条件下训练的参考模型作为测试条件难度的代理。

    The acoustic variability of noisy and reverberant speech mixtures is influenced by multiple factors, such as the spectro-temporal characteristics of the target speaker and the interfering noise, the signal-to-noise ratio (SNR) and the room characteristics. This large variability poses a major challenge for learning-based speech enhancement systems, since a mismatch between the training and testing conditions can substantially reduce the performance of the system. Generalization to unseen conditions is typically assessed by testing the system with a new speech, noise or binaural room impulse response (BRIR) database different from the one used during training. However, the difficulty of the speech enhancement task can change across databases, which can substantially influence the results. The present study introduces a generalization assessment framework that uses a reference model trained on the test condition, such that it can be used as a proxy for the difficulty of the test conditio
    
[^30]: 大型语言模型服务的高效内存管理：基于分页注意力的方法

    Efficient Memory Management for Large Language Model Serving with PagedAttention. (arXiv:2309.06180v1 [cs.LG])

    [http://arxiv.org/abs/2309.06180](http://arxiv.org/abs/2309.06180)

    本论文提出了PagedAttention算法和vLLM系统，通过类似虚拟内存和分页技术的方法，实现了大型语言模型服务中键-值缓存内存的高效管理和灵活共享，大大提高了吞吐量，减少了内存使用量，相对于最先进的系统（如FasterTransformer和Orca）改进了2-4倍的吞吐量。

    

    高吞吐量的大型语言模型（LLMs）服务需要一次批处理足够多的请求。然而，现有系统存在困难，因为每个请求的键-值缓存（KV缓存）内存非常庞大且动态增长和收缩。当管理不当时，这些内存可能会因为碎片化和冗余复制而被大量浪费，从而限制了批处理的规模。为了解决这个问题，我们提出了基于经典虚拟内存和分页技术的注意力算法PagedAttention。在此基础上，我们构建了vLLM，一个LLM服务系统，它能够实现（1）KV缓存内存几乎零浪费和（2）灵活共享KV缓存，以进一步减少内存使用量。我们的评估结果显示，与最先进的系统（如FasterTransformer和Orca）相比，vLLM在保持相同延迟水平的情况下，将流行的LLMs的吞吐量提高了2-4倍。对于更长的序列，这种改进效果更为明显。

    High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequence
    
[^31]: 阐明扩展反向时间随机微分方程在扩散模型中的解空间

    Elucidating the solution space of extended reverse-time SDE for diffusion models. (arXiv:2309.06169v1 [cs.LG])

    [http://arxiv.org/abs/2309.06169](http://arxiv.org/abs/2309.06169)

    这项工作介绍了扩展反向时间随机微分方程（ER SDE）用于解决扩散模型中的采样问题，并提供了精确解和高阶近似解，并解释了在快速采样方面ODE求解器优于SDE求解器的数学洞察力。

    

    扩散模型在各种生成建模任务中展示出强大的图像生成能力。然而，它们的主要限制在于采样速度较慢，需要通过大型神经网络进行数百或数千次连续函数评估才能生成高质量的图像。从扩散模型中采样可以看作是解相应的随机微分方程（SDE）或常微分方程（ODE）。在这项工作中，我们将采样过程形式化为扩展反向时间 SDE（ER SDE），将之前对ODE和SDE的探索统一起来。利用ER SDE解的半线性结构，我们为VP SDE提供了精确解和任意高阶近似解，为VE SDE提供了高阶近似解。基于ER SDE的解空间，我们揭示了ODE求解器在快速采样方面优于SDE求解器的数学洞察力。此外，我们还揭示了VP SDE求解器与其VE SDE求解器在性能上相当。

    Diffusion models (DMs) demonstrate potent image generation capabilities in various generative modeling tasks. Nevertheless, their primary limitation lies in slow sampling speed, requiring hundreds or thousands of sequential function evaluations through large neural networks to generate high-quality images. Sampling from DMs can be seen as solving corresponding stochastic differential equations (SDEs) or ordinary differential equations (ODEs). In this work, we formulate the sampling process as an extended reverse-time SDE (ER SDE), unifying prior explorations into ODEs and SDEs. Leveraging the semi-linear structure of ER SDE solutions, we offer exact solutions and arbitrarily high-order approximate solutions for VP SDE and VE SDE, respectively. Based on the solution space of the ER SDE, we yield mathematical insights elucidating the superior performance of ODE solvers over SDE solvers in terms of fast sampling. Additionally, we unveil that VP SDE solvers stand on par with their VE SDE c
    
[^32]: 具有弹性控制和较大Lipschitz常数的认证鲁棒模型

    Certified Robust Models with Slack Control and Large Lipschitz Constants. (arXiv:2309.06166v1 [cs.LG])

    [http://arxiv.org/abs/2309.06166](http://arxiv.org/abs/2309.06166)

    本文提出了一种校准的Lipschitz边界误差（CLL）来提高认证鲁棒性，通过解决边界误差不会根据收缩的输出分布调整惩罚和最小化Lipschitz常数导致过度平滑的问题。

    

    尽管最近取得了成功，但目前最先进的基于学习的模型仍然对输入变化，如对抗样本，非常容易受到攻击。为了获得对这种扰动的可证明的鲁棒性，最近的研究考虑了基于Lipschitz的正则化器或约束，同时增加了预测边界。不幸的是，这样做会显著降低准确性。在本文中，我们提出了一个校准的Lipschitz边界误差（CLL）来解决这个问题，并通过解决两个问题来提高认证鲁棒性：首先，常用的边界误差不会根据收缩的输出分布调整惩罚，这是由于最小化Lipschitz常数K所造成的。其次，最重要的是，我们观察到最小化K可以导致决策函数过度平滑。这限制了模型的复杂性，从而降低了准确性。我们的CLL通过明确校准损失与边界和Lipschitz常数的关系来解决这些问题，从而确保模型具有较高的准确性。

    Despite recent success, state-of-the-art learning-based models remain highly vulnerable to input changes such as adversarial examples. In order to obtain certifiable robustness against such perturbations, recent work considers Lipschitz-based regularizers or constraints while at the same time increasing prediction margin. Unfortunately, this comes at the cost of significantly decreased accuracy. In this paper, we propose a Calibrated Lipschitz-Margin Loss (CLL) that addresses this issue and improves certified robustness by tackling two problems: Firstly, commonly used margin losses do not adjust the penalties to the shrinking output distribution; caused by minimizing the Lipschitz constant $K$. Secondly, and most importantly, we observe that minimization of $K$ can lead to overly smooth decision functions. This limits the model's complexity and thus reduces accuracy. Our CLL addresses these issues by explicitly calibrating the loss w.r.t. margin and Lipschitz constant, thereby establis
    
[^33]: Robust-MBDL:一种用于旋转机器剩余寿命预测和运行状态鉴别的稳健的多支路深度学习模型

    Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines. (arXiv:2309.06157v1 [cs.LG])

    [http://arxiv.org/abs/2309.06157](http://arxiv.org/abs/2309.06157)

    本文提出了一种稳健的多支路深度学习模型，用于旋转机器的剩余寿命预测和运行状态识别。该模型包括LSTM-Autoencoder对振动数据进行去噪、特征提取和多支路深度学习网络结构等组件，并在实验中证明了它在轴承机器应用中的优越性和潜力。

    

    本文提出了一种用于旋转机器剩余寿命(RUL)预测和运行状态(CO)鉴别的稳健的多支路深度学习系统。具体而言，该系统包括主要组件：(1)采用LSTM-Autoencoder对振动数据进行去噪；(2)使用特征提取从去噪数据中生成时域、频域和时频域特征；(3)采用新颖而稳健的多支路深度学习网络结构来利用多个特征。我们的系统在XJTU-SY和PRONOSTIA两个基准数据集上与现有技术水平进行了评估和比较。实验结果证明我们的系统表现优于现有技术水平，并具有在轴承机器实际应用中的潜力。

    In this paper, a Robust Multi-branch Deep learning-based system for remaining useful life (RUL) prediction and condition operations (CO) identification of rotating machines is proposed. In particular, the proposed system comprises main components: (1) an LSTM-Autoencoder to denoise the vibration data; (2) a feature extraction to generate time-domain, frequency-domain, and time-frequency based features from the denoised data; (3) a novel and robust multi-branch deep learning network architecture to exploit the multiple features. The performance of our proposed system was evaluated and compared to the state-of-the-art systems on two benchmark datasets of XJTU-SY and PRONOSTIA. The experimental results prove that our proposed system outperforms the state-of-the-art systems and presents potential for real-life applications on bearing machines.
    
[^34]: 面向可靠的领域泛化：一个新的数据集和评估

    Towards Reliable Domain Generalization: A New Dataset and Evaluations. (arXiv:2309.06142v1 [cs.CV])

    [http://arxiv.org/abs/2309.06142](http://arxiv.org/abs/2309.06142)

    这项研究提出了一个新的领域泛化任务，用于手写中文字符识别，该任务的目标是丰富领域泛化方法的应用场景。通过在一个特定的数据集上评估18种领域泛化方法，研究发现现有方法在该数据集上的性能仍然不理想。此外，通过设计动态领域泛化设置，揭示了只有留出一个领域的方法才能在这种设置下实现良好性能。

    

    现实世界中存在着普遍的分布偏移。然而，深度神经网络(DNN)很容易对训练集产生偏见，这在接收到超出分布的数据时会导致性能严重下降。在领域泛化(DG)的文献中，研究了许多方法以训练在不同分布偏移下具有广泛泛化能力的模型。然而，最近的DomainBed和WILDS基准挑战了这些方法的有效性。针对现有研究中存在的问题，我们提出了一个新的领域泛化任务，用于丰富DG方法研究的应用场景，即手写中文字符识别(HCCR)。我们在提出的PaHCC（印刷和手写中文字符）数据集上评估了18种DG方法，并显示现有方法在该数据集上的性能仍然不令人满意。此外，在设计的动态DG设置下，我们揭示了DG方法的更多属性，并认为只有留出一个领域的方法可以在这种设置下实现良好的性能。

    There are ubiquitous distribution shifts in the real world. However, deep neural networks (DNNs) are easily biased towards the training set, which causes severe performance degradation when they receive out-of-distribution data. Many methods are studied to train models that generalize under various distribution shifts in the literature of domain generalization (DG). However, the recent DomainBed and WILDS benchmarks challenged the effectiveness of these methods. Aiming at the problems in the existing research, we propose a new domain generalization task for handwritten Chinese character recognition (HCCR) to enrich the application scenarios of DG method research. We evaluate eighteen DG methods on the proposed PaHCC (Printed and Handwritten Chinese Characters) dataset and show that the performance of existing methods on this dataset is still unsatisfactory. Besides, under a designed dynamic DG setting, we reveal more properties of DG methods and argue that only the leave-one-domain-out
    
[^35]: 使用形变器加速边缘人工智能：一个集成的设计、编译和仿真框架用于粗粒度可重构数组

    Accelerating Edge AI with Morpher: An Integrated Design, Compilation and Simulation Framework for CGRAs. (arXiv:2309.06127v1 [cs.AR])

    [http://arxiv.org/abs/2309.06127](http://arxiv.org/abs/2309.06127)

    形变器是一个集成的设计、编译和仿真框架，用于加速边缘人工智能应用。它能自动将AI应用内核编译到用户定义的CGRA架构上，并验证其功能。

    

    粗粒度可重构数组（CGRAs）作为功耗高效的边缘加速器具有巨大潜力，提供超越人工智能应用的多样性。形变器是一个开源的、架构自适应的CGRA设计框架，专门设计用于探索CGRAs的广阔设计空间。形变器的全面生态系统包括定制的编译器、模拟器、加速器综合和验证框架。本研究概述了形变器，重点介绍了它在自动编译AI应用内核到用户定义的CGRA架构上并验证其功能方面的能力。通过形变器框架，利用CGRAs的多样性，实现了边缘人工智能应用的高效编译和验证，覆盖了广泛嵌入式AI工作负载的重要内核。形变器可以在https://github.com/ecolab-nus/morpher-v2 上在线获取。

    Coarse-Grained Reconfigurable Arrays (CGRAs) hold great promise as power-efficient edge accelerator, offering versatility beyond AI applications. Morpher, an open-source, architecture-adaptive CGRA design framework, is specifically designed to explore the vast design space of CGRAs. The comprehensive ecosystem of Morpher includes a tailored compiler, simulator, accelerator synthesis, and validation framework. This study provides an overview of Morpher, highlighting its capabilities in automatically compiling AI application kernels onto user-defined CGRA architectures and verifying their functionality. Through the Morpher framework, the versatility of CGRAs is harnessed to facilitate efficient compilation and verification of edge AI applications, covering important kernels representative of a wide range of embedded AI workloads. Morpher is available online at https://github.com/ecolab-nus/morpher-v2.
    
[^36]: AstroLLaMA: 面向天文学的专业基础模型

    AstroLLaMA: Towards Specialized Foundation Models in Astronomy. (arXiv:2309.06126v1 [astro-ph.IM])

    [http://arxiv.org/abs/2309.06126](http://arxiv.org/abs/2309.06126)

    AstroLLaMA是一个专门用于天文学的模型，通过从arXiv中的天文学摘要fine-tuned得到，其在因果语言建模中表现出色，生成的文本完成和嵌入提取比其他基础模型更具洞察力和科学相关性。

    

    大型语言模型在许多人类语言任务中表现出色，但在学术天文学等高度专业化领域往往难以胜任。为了弥合这个差距，我们介绍了AstroLLaMA，这是一个从arXiv上的超过300,000个天文学摘要中使用LLaMA-2 fine-tuned得到的70亿参数模型。AstroLLaMA针对传统因果语言建模进行了优化，其困惑度比Llama-2低30％，表现出明显的领域适应性。尽管参数明显较少，但我们的模型生成的文本完成和嵌入提取比最先进的基础模型更具洞察力和科学相关性。AstroLLaMA作为一个强大的领域特定模型，具有广泛的fine-tuning潜力。其公开发布旨在推动围绕天文学的研究，包括自动论文摘要和对话代理的开发。

    Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.
    
[^37]: 用于高分辨率透射电子显微镜(HRTEM)的鲁棒合成数据生成框架

    A robust synthetic data generation framework for machine learning in High-Resolution Transmission Electron Microscopy (HRTEM). (arXiv:2309.06122v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.06122](http://arxiv.org/abs/2309.06122)

    本论文提出了一个用于高分辨率透射电子显微镜（HRTEM）的鲁棒合成数据生成框架，该框架包括Construction Zone软件包和一个端到端的工作流程。利用该框架，可以快速生成大规模的模拟数据库，并用于训练神经网络进行纳米颗粒的分割等任务。

    

    机器学习技术对于开发高度准确的纳米材料表征的自动化分析工具，包括高分辨率透射电子显微镜（HRTEM），具有吸引力。然而，成功实施这类机器学习工具可能很困难，因为从实验中获取足够大、高质量的训练数据集面临挑战。本研究中，我们介绍了Construction Zone，一个用于快速生成复杂纳米尺度原子结构的Python软件包，并开发了一个端到端的工作流程，用于创建大型模拟数据库以训练神经网络。Construction Zone能够快速而系统地采样现实纳米材料结构，并可用作模拟数据库的随机结构生成器，这对于生成大型多样化的合成数据集至关重要。以HRTEM显微镜图像为例，我们在我们的模拟数据库的各个子集上训练了一系列神经网络以分割纳米颗粒和...

    Machine learning techniques are attractive options for developing highly-accurate automated analysis tools for nanomaterials characterization, including high-resolution transmission electron microscopy (HRTEM). However, successfully implementing such machine learning tools can be difficult due to the challenges in procuring sufficiently large, high-quality training datasets from experiments. In this work, we introduce Construction Zone, a Python package for rapidly generating complex nanoscale atomic structures, and develop an end-to-end workflow for creating large simulated databases for training neural networks. Construction Zone enables fast, systematic sampling of realistic nanomaterial structures, and can be used as a random structure generator for simulated databases, which is important for generating large, diverse synthetic datasets. Using HRTEM imaging as an example, we train a series of neural networks on various subsets of our simulated databases to segment nanoparticles and
    
[^38]: 基于保真度诱导的可解释策略提取方法用于强化学习

    Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning. (arXiv:2309.06097v1 [cs.AI])

    [http://arxiv.org/abs/2309.06097](http://arxiv.org/abs/2309.06097)

    本文提出了一种基于保真度诱导的策略提取方法（FIPE）来解决深度强化学习代理不透明的决策问题。实验证明，该方法在星际争霸 II 这样的复杂控制环境下具有良好的性能。

    

    深度强化学习在顺序决策问题上取得了显著的成功。然而，现有的深度强化学习代理以不透明的方式进行决策，阻碍了用户建立信任和审视代理的弱点。虽然最近的研究开发了一些可解释策略提取方法来解释代理的行为，但它们的解释常常与代理的行为不一致，因此经常无法解释。为了解决这个问题，我们提出了一种新方法，即基于保真度诱导的策略提取（FIPE）。具体而言，我们首先分析了现有可解释策略提取方法的优化机制，阐述了在增加累积奖励时忽视一致性的问题。然后，我们将一个保真度量集成到强化学习反馈中，设计了一个保真度诱导机制。我们在星际争霸 II 的复杂控制环境下进行实验，这是当前可解释策略提取方法通常避免的领域。

    Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making problems. However, existing DRL agents make decisions in an opaque fashion, hindering the user from establishing trust and scrutinizing weaknesses of the agents. While recent research has developed Interpretable Policy Extraction (IPE) methods for explaining how an agent takes actions, their explanations are often inconsistent with the agent's behavior and thus, frequently fail to explain. To tackle this issue, we propose a novel method, Fidelity-Induced Policy Extraction (FIPE). Specifically, we start by analyzing the optimization mechanism of existing IPE methods, elaborating on the issue of ignoring consistency while increasing cumulative rewards. We then design a fidelity-induced mechanism by integrate a fidelity measurement into the reinforcement learning feedback. We conduct experiments in the complex control environment of StarCraft II, an arena typically avoided by current IPE method
    
[^39]: 通过证书合成的动态与控制模型的通用验证框架

    A General Verification Framework for Dynamical and Control Models via Certificate Synthesis. (arXiv:2309.06090v1 [eess.SY])

    [http://arxiv.org/abs/2309.06090](http://arxiv.org/abs/2309.06090)

    这个论文提出了一个通用的框架来通过证书合成验证动态和控制模型。研究者们提供了一种自动化方法来设计控制器并分析复杂规范。这个方法利用神经网络和SMT求解器来提供候选控制和证书函数，并为控制的安全学习领域做出了贡献。

    

    控制论的一个新兴分支专门研究证书学习，涉及对自主或控制模型的所需（可能是复杂的）系统行为的规范，并通过基于函数的证明进行分析验证。然而，满足这些复杂要求的控制器的合成通常是一个非常困难的任务，可能超出了大多数专家控制工程师的能力。因此，需要自动技术能够设计控制器并分析各种复杂规范。在本文中，我们提供了一个通用框架来编码系统规范并定义相应的证书，并提出了一种自动化方法来正式合成控制器和证书。我们的方法为控制的安全学习领域做出了贡献，利用神经网络的灵活性提供候选的控制和证书函数，同时使用SMT求解器来提供形式化的保证。

    An emerging branch of control theory specialises in certificate learning, concerning the specification of a desired (possibly complex) system behaviour for an autonomous or control model, which is then analytically verified by means of a function-based proof. However, the synthesis of controllers abiding by these complex requirements is in general a non-trivial task and may elude the most expert control engineers. This results in a need for automatic techniques that are able to design controllers and to analyse a wide range of elaborate specifications. In this paper, we provide a general framework to encode system specifications and define corresponding certificates, and we present an automated approach to formally synthesise controllers and certificates. Our approach contributes to the broad field of safe learning for control, exploiting the flexibility of neural networks to provide candidate control and certificate functions, whilst using SMT-solvers to offer a formal guarantee of co
    
[^40]: 在跨语言转移范式中测量灾难性遗忘：探索调优策略

    Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies. (arXiv:2309.06089v1 [cs.CL])

    [http://arxiv.org/abs/2309.06089](http://arxiv.org/abs/2309.06089)

    该研究比较了不同的微调和跨语言转移策略在解决跨语言任务时的表现，评估了灾难性遗忘的程度和转移的成功程度。

    

    跨语言转移是一种解决资源匮乏语言任务的有希望的技术。在这个实证研究中，我们比较了两种与零射和全射学习方法相结合的大型语言模型在跨语言设置下的微调方法。作为微调策略，我们比较了参数效率适配器方法与所有参数微调。作为跨语言转移策略，我们比较了使用每个语言依次的中间训练（IT）和在微调的验证阶段已经使用目标语言的跨语言验证（CLV）。我们评估了转移的成功程度以及源语言中由于跨语言转移而导致的灾难性遗忘的程度，即在学习不同语言中的新信息时之前获得的知识损失了多少。在两个不同的分类问题上，包括仇恨言论检测和产品评论，分别包含了多个语种数据集的结果。

    The cross-lingual transfer is a promising technique to solve tasks in less-resourced languages. In this empirical study, we compare two fine-tuning approaches combined with zero-shot and full-shot learning approaches for large language models in a cross-lingual setting. As fine-tuning strategies, we compare parameter-efficient adapter methods with fine-tuning of all parameters. As cross-lingual transfer strategies, we compare the intermediate-training (\textit{IT}) that uses each language sequentially and cross-lingual validation (\textit{CLV}) that uses a target language already in the validation phase of fine-tuning. We assess the success of transfer and the extent of catastrophic forgetting in a source language due to cross-lingual transfer, i.e., how much previously acquired knowledge is lost when we learn new information in a different language. The results on two different classification problems, hate speech detection and product reviews, each containing datasets in several lang
    
[^41]: 优化塑性的互补网络用于无监督连续学习

    Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning. (arXiv:2309.06086v1 [cs.LG])

    [http://arxiv.org/abs/2309.06086](http://arxiv.org/abs/2309.06086)

    本研究提出了一种优化塑性的互补网络方法，用于无监督连续学习。通过解除专家网络对保留先前知识的要求，并通过适应-回顾阶段与之结合，解决了无监督学习中塑性和稳定性之间的权衡问题。

    

    连续无监督表示学习（CURL）研究受益于自监督学习（SSL）技术的改进。因此，使用SSL的现有CURL方法可以在没有任何标签的情况下学习高质量的表示，但在学习多任务数据流时性能下降明显。我们假设这是由于为了防止遗忘而施加的正则化损失造成的，导致塑性和稳定性之间的权衡不够优化：它们要么不完全适应新的数据（塑性低），要么在完全适应新的SSL预训练任务时产生显著遗忘（稳定性低）。在这项工作中，我们提出了训练一个专家网络，它不再需要保留先前的知识，而是可以专注于在新任务上表现最佳（优化塑性）。在第二阶段，我们将这个新知识与先前的网络结合在一个适应-回顾阶段，以避免遗忘。

    Continuous unsupervised representation learning (CURL) research has greatly benefited from improvements in self-supervised learning (SSL) techniques. As a result, existing CURL methods using SSL can learn high-quality representations without any labels, but with a notable performance drop when learning on a many-tasks data stream. We hypothesize that this is caused by the regularization losses that are imposed to prevent forgetting, leading to a suboptimal plasticity-stability trade-off: they either do not adapt fully to the incoming data (low plasticity), or incur significant forgetting when allowed to fully adapt to a new SSL pretext-task (low stability). In this work, we propose to train an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity). In the second phase, we combine this new knowledge with the previous network in an adaptation-retrospection phase to avoid forgetting and i
    
[^42]: 一个机器学习框架用于解析电力市场价格事件的主要驱动因素

    A Machine Learning Framework to Deconstruct the Primary Drivers for Electricity Market Price Events. (arXiv:2309.06082v1 [cs.LG])

    [http://arxiv.org/abs/2309.06082](http://arxiv.org/abs/2309.06082)

    该论文提出了一个基于机器学习的分析框架，用于解析现代高可再生能源电力市场中价格飙升事件的主要驱动因素。

    

    电力网络正在向100%可再生能源的大容量电力网络转变，电力系统运行和电力市场的整体动态也在发生变化。电力市场不仅在经济上调度资源，还考虑了各种可控行动，如可再生能源限制、输电阻塞缓解和能量储存优化等，以确保电网可靠性。因此，电力市场的价格形成变得非常复杂。传统的根本原因分析和统计方法无法分析和推断现代电网和具有可变可再生能源（VRE）的市场价格形成背后的主要驱动因素。本文提出了一个基于机器学习的分析框架，用于解析现代高可再生能源电力市场中价格飙升事件的主要驱动因素。这些结果可以应用于市场设计、可再生能源调度等各个关键方面。

    Power grids are moving towards 100% renewable energy source bulk power grids, and the overall dynamics of power system operations and electricity markets are changing. The electricity markets are not only dispatching resources economically but also taking into account various controllable actions like renewable curtailment, transmission congestion mitigation, and energy storage optimization to ensure grid reliability. As a result, price formations in electricity markets have become quite complex. Traditional root cause analysis and statistical approaches are rendered inapplicable to analyze and infer the main drivers behind price formation in the modern grid and markets with variable renewable energy (VRE). In this paper, we propose a machine learning-based analysis framework to deconstruct the primary drivers for price spike events in modern electricity markets with high renewable energy. The outcomes can be utilized for various critical aspects of market design, renewable dispatch an
    
[^43]: 图神经网络中的信息流动：临床分诊应用案例研究

    Information Flow in Graph Neural Networks: A Clinical Triage Use Case. (arXiv:2309.06081v1 [cs.LG])

    [http://arxiv.org/abs/2309.06081](http://arxiv.org/abs/2309.06081)

    本文研究了图神经网络在知识图谱中的链接预测任务中的信息流动问题，提出了一个数学模型来解耦网络连接性与图数据连接性，并在临床分诊应用场景中评估了网络性能。研究结果表明，将领域知识纳入网络连接性能够提升预测性能，负边对于良好的预测起重要作用，但使用过多的网络层会降低性能。

    

    图神经网络（GNNs）由于能够处理多模态和多关系图而在医疗保健等领域引起了广泛关注。然而，GNNs的有效训练仍然具有挑战性，存在一些未解决的研究问题。本文研究了嵌入信息在GNNs内部传播如何影响知识图谱（KGs）中链接的预测。具体而言，我们提出了一个数学模型，将GNN的连接性与图数据的连接性解耦，并在临床分诊应用案例中评估了GNNs的性能。我们的结果表明，将领域知识纳入到GNN的连接性中比使用与KG相同的连接性或允许无限制的嵌入传播导致更好的性能。此外，我们还证明了负边在实现良好预测中起到了至关重要的作用，并且使用太多的GNN层会降低性能。

    Graph Neural Networks (GNNs) have gained popularity in healthcare and other domains due to their ability to process multi-modal and multi-relational graphs. However, efficient training of GNNs remains challenging, with several open research questions. In this paper, we investigate how the flow of embedding information within GNNs affects the prediction of links in Knowledge Graphs (KGs). Specifically, we propose a mathematical model that decouples the GNN connectivity from the connectivity of the graph data and evaluate the performance of GNNs in a clinical triage use case. Our results demonstrate that incorporating domain knowledge into the GNN connectivity leads to better performance than using the same connectivity as the KG or allowing unconstrained embedding propagation. Moreover, we show that negative edges play a crucial role in achieving good predictions, and that using too many GNN layers can degrade performance.
    
[^44]: A2V: 一种半监督领域自适应框架用于不同图像模态的脑血管分割，通过二阶段训练从血管造影到静脉造影的转换

    A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel Segmentation via Two-Phase Training Angiography-to-Venography Translation. (arXiv:2309.06075v1 [eess.IV])

    [http://arxiv.org/abs/2309.06075](http://arxiv.org/abs/2309.06075)

    A2V是一种半监督领域自适应框架，用于通过图像到图像转换实现脑血管的跨模态分割。它通过离散化和语义丰富的潜在空间实现源域到目标域的图像级自适应，并提高了计算效率和训练稳定性。

    

    我们提出了一种半监督领域自适应框架，用于从不同图像模态中分割脑血管。现有的最先进方法集中于单一模态，忽视了广泛可用的脑血管成像技术。这可能导致显著的分布变化，对跨模态的泛化产生负面影响。通过依赖注释的血管造影和有限数量的注释的静脉造影，我们的框架实现图像到图像的转换和语义分割，利用离散化和语义丰富的潜在空间来表示异构数据，并进行源域到目标域的图像级自适应。此外，我们减少了基于循环的架构的典型复杂性，最小化了对抗性训练的使用，这使我们能够构建一个稳定训练的高效直观模型。我们在磁共振血管造影和静脉造影上评估了我们的方法。在实现最先进水平的同时，我们减少了所提出方法的计算复杂度和模型稳定性的改进。

    We present a semi-supervised domain adaptation framework for brain vessel segmentation from different image modalities. Existing state-of-the-art methods focus on a single modality, despite the wide range of available cerebrovascular imaging techniques. This can lead to significant distribution shifts that negatively impact the generalization across modalities. By relying on annotated angiographies and a limited number of annotated venographies, our framework accomplishes image-to-image translation and semantic segmentation, leveraging a disentangled and semantically rich latent space to represent heterogeneous data and perform image-level adaptation from source to target domains. Moreover, we reduce the typical complexity of cycle-based architectures and minimize the use of adversarial training, which allows us to build an efficient and intuitive model with stable training. We evaluate our method on magnetic resonance angiographies and venographies. While achieving state-of-the-art pe
    
[^45]: 使用机器学习和深度学习模型预测滑坡易发性时的贡献因素选择

    Selection of contributing factors for predicting landslide susceptibility using machine learning and deep learning models. (arXiv:2309.06062v1 [cs.LG])

    [http://arxiv.org/abs/2309.06062](http://arxiv.org/abs/2309.06062)

    使用机器学习和深度学习模型对滑坡易发性进行预测，通过选择更重要的贡献因素提高预测准确性。

    

    滑坡是常见的自然灾害，可能导致人员伤亡、财产安全威胁和经济损失。因此，了解或预测潜在风险地点的滑坡发生概率非常重要。常用的方法是根据滑坡清单和一组滑坡贡献因素进行滑坡易发性评估，可以通过机器学习（ML）模型（如逻辑回归，支持向量机，随机森林，极限梯度提升或深度学习（DL）模型（如卷积神经网络和长短记忆）来实现。作为这些模型的输入数据，滑坡贡献因素对滑坡发生有不同的影响。因此，有逻辑的选择更重要的贡献因素并消除不相关的因素，从而提高这些模型的预测准确性。

    Landslides are a common natural disaster that can cause casualties, property safety threats and economic losses. Therefore, it is important to understand or predict the probability of landslide occurrence at potentially risky sites. A commonly used means is to carry out a landslide susceptibility assessment based on a landslide inventory and a set of landslide contributing factors. This can be readily achieved using machine learning (ML) models such as logistic regression (LR), support vector machine (SVM), random forest (RF), extreme gradient boosting (Xgboost), or deep learning (DL) models such as convolutional neural network (CNN) and long short time memory (LSTM). As the input data for these models, landslide contributing factors have varying influences on landslide occurrence. Therefore, it is logically feasible to select more important contributing factors and eliminate less relevant ones, with the aim of increasing the prediction accuracy of these models. However, selecting more
    
[^46]: 验证公平性：隐私保护的机器学习系统公平性计算

    Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems. (arXiv:2309.06061v1 [cs.CR])

    [http://arxiv.org/abs/2309.06061](http://arxiv.org/abs/2309.06061)

    提出了公平性即服务（FaaS）的安全、可验证和隐私保护协议，用于计算和验证任何机器学习（ML）模型的公平性。FaaS是模型无关的，可以支持多种公平性指标；因此，它可以用作审计任何ML模型的服务。

    

    公平机器学习是一个蓬勃发展且充满活力的研究课题。本文提出了公平性即服务（FaaS）的安全、可验证和隐私保护协议，用于计算和验证任何机器学习（ML）模型的公平性。在FaaS的设计中，通过密文表示数据和结果，以确保隐私。此外，零知识证明保证了密文和底层数据的良好性。FaaS是模型无关的，可以支持多种公平性指标；因此，它可以用作审计任何ML模型的服务。我们的解决方案不需要可信任的第三方或私密通道来计算公平度量。安全保证和承诺以确保每个步骤都是安全透明的，并且整个过程从开始到结束都可以进行验证。所有输入数据的密文对于每个人，例如审计员、社会活动家和专家来说都是公开可用的，以验证其正确性。

    Fair machine learning is a thriving and vibrant research topic. In this paper, we propose Fairness as a Service (FaaS), a secure, verifiable and privacy-preserving protocol to computes and verify the fairness of any machine learning (ML) model. In the deisgn of FaaS, the data and outcomes are represented through cryptograms to ensure privacy. Also, zero knowledge proofs guarantee the well-formedness of the cryptograms and underlying data. FaaS is model--agnostic and can support various fairness metrics; hence, it can be used as a service to audit the fairness of any ML model. Our solution requires no trusted third party or private channels for the computation of the fairness metric. The security guarantees and commitments are implemented in a way that every step is securely transparent and verifiable from the start to the end of the process. The cryptograms of all input data are publicly available for everyone, e.g., auditors, social activists and experts, to verify the correctness of 
    
[^47]: 表示对上下文学习的影响：对合成任务的探索

    How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])

    [http://arxiv.org/abs/2309.06054](http://arxiv.org/abs/2309.06054)

    本研究通过探索表示学习的角度，研究了表示对上下文学习的影响。实验结果表明，在上下文学习中，上下文内部成分对学习性能起到重要作用。

    

    上下文学习，即从上下文样本中学习，是Transformer的一项引人注目的能力。然而，驱动上下文学习的机制尚未被充分理解。本研究旨在从一个未被充分探索的表示学习角度进行调查。在上下文学习场景中，表示更加复杂，表示可以受到模型权重和上下文样本的影响。我们将上述两个概念方面的表示分别称为权重内部成分和上下文内部成分。为了研究这两个成分如何影响上下文学习能力，我们构建了一个新颖的合成任务，从而可以设计两个探针，即权重内部探针和上下文探针，分别评估这两个成分。我们证明上下文内部成分的好坏与上下文学习性能高度相关，这表明上下文学习与表示学习之间的纠缠关系。

    In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation lear
    
[^48]: 一种基于感知机的线性分离精细逼近技术

    A Perceptron-based Fine Approximation Technique for Linear Separation. (arXiv:2309.06049v1 [cs.LG])

    [http://arxiv.org/abs/2309.06049](http://arxiv.org/abs/2309.06049)

    本文提出了一种基于感知机的在线学习方法，通过精细调整神经元权重来找到数据点之间的分离超平面，从而降低了大型或不平衡数据集的计算复杂性。该方法通过适当转换初始数据集来将分离问题转化为一类分类问题。

    

    本文提出了一种新颖的在线学习方法，旨在找到标记为正或负的数据点之间的分离超平面。由于人工神经元的权重和偏置可以直接与高维空间中的超平面相关联，所以该技术适用于机器学习中训练基于感知机的二分类器。在大型或不平衡的数据集情况下，使用解析或基于梯度的解决方案可能变得禁止和不实际，而启发式和近似技术仍然适用。所提出的方法基于感知机算法，但在搜索分离超平面期间只调整神经元权重的必要程度。通过适当转换初始数据集，我们不需要考虑数据标签，也不需要考虑偏置项，将可分性降低为一类分类问题。该方法已被证明收敛；实证结果表明，它可以m

    This paper presents a novel online learning method that aims at finding a separator hyperplane between data points labelled as either positive or negative. Since weights and biases of artificial neurons can directly be related to hyperplanes in high-dimensional spaces, the technique is applicable to train perceptron-based binary classifiers in machine learning. In case of large or imbalanced data sets, use of analytical or gradient-based solutions can become prohibitive and impractical, where heuristics and approximation techniques are still applicable. The proposed method is based on the Perceptron algorithm, however, it tunes neuron weights in just the necessary extent during searching the separator hyperplane. Due to an appropriate transformation of the initial data set we need not to consider data labels, neither the bias term. respectively, reducing separability to a one-class classification problem. The presented method has proven converge; empirical results show that it can be m
    
[^49]: BatMan-CLR: 使得少样本元学习器对标签噪声具有鲁棒性

    BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise. (arXiv:2309.06046v1 [cs.LG])

    [http://arxiv.org/abs/2309.06046](http://arxiv.org/abs/2309.06046)

    本研究对少样本元学习器受标签噪声影响的性能进行了全面分析，发现在受到标签噪声影响的元训练中，Reptile、iMAML和foMAML在Omniglot和CifarFS数据集上的准确性下降了最高达42%。为了增强对标签噪声的鲁棒性，提出了Man和BatMan两种采样技术，将有噪声的有监督学习器转变为半监督学习器。

    

    标签噪声对于经典的监督学习已经有了深入研究，但是在元学习领域仍然是一个开放的研究问题。元学习器旨在通过在元训练中学习一个良好的初始模型，并在元测试期间根据新任务进行连续微调，以适应未知的学习任务。本文首次全面分析了不同程度的标签噪声对最先进的元学习器（特别是基于梯度的N-way K-shot学习器）性能的影响。结果表明，当元训练受到标签噪声的影响时，Reptile、iMAML和foMAML在Omniglot和CifarFS数据集上的准确性下降了最高达42%。为了增强对标签噪声的鲁棒性，我们提出了两种采样技术，即流形（Man）和批次流形（BatMan），将有噪声的有监督学习器转变为半监督学习器，以增加噪声标签的效用。我们首先构建了N-way 2-contrastiv的流形样本

    The negative impact of label noise is well studied in classical supervised learning yet remains an open research question in meta-learning. Meta-learners aim to adapt to unseen learning tasks by learning a good initial model in meta-training and consecutively fine-tuning it according to new tasks during meta-testing. In this paper, we present the first extensive analysis of the impact of varying levels of label noise on the performance of state-of-the-art meta-learners, specifically gradient-based $N$-way $K$-shot learners. We show that the accuracy of Reptile, iMAML, and foMAML drops by up to 42% on the Omniglot and CifarFS datasets when meta-training is affected by label noise. To strengthen the resilience against label noise, we propose two sampling techniques, namely manifold (Man) and batch manifold (BatMan), which transform the noisy supervised learners into semi-supervised ones to increase the utility of noisy labels. We first construct manifold samples of $N$-way $2$-contrastiv
    
[^50]: 基于正态学习的多尺度对比学习的图异常检测

    Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning. (arXiv:2309.06034v1 [cs.LG])

    [http://arxiv.org/abs/2309.06034](http://arxiv.org/abs/2309.06034)

    本文提出了一种基于正态学习的图异常检测框架NLGAD，通过多尺度对比学习网络来增强学习正常模式的能力，以改进异常检测性能。

    

    图异常检测（GAD）在机器学习和数据挖掘领域引起了越来越多的关注。最近的研究主要集中在如何捕捉更丰富的信息，以提高GAD中节点嵌入的质量。尽管它们在检测性能方面取得了显著进展，但对于任务的特性仍然相对不足。GAD旨在识别与大多数节点有所偏离的异常。然而，模型很容易学习到组成大多数样本的正常样本的模式。与此同时，当异常行为与正常性不同的时候，异常可以很容易被检测到。因此，通过增强学习正常模式的能力，性能可以进一步提高。为此，我们提出了一种基于正态学习的GAD框架，使用多尺度对比学习网络（简称NLGAD）。具体而言，我们首先使用不同尺度的对比网络初始化模型，以提供充足可靠的正常节点样本。

    Graph anomaly detection (GAD) has attracted increasing attention in machine learning and data mining. Recent works have mainly focused on how to capture richer information to improve the quality of node embeddings for GAD. Despite their significant advances in detection performance, there is still a relative dearth of research on the properties of the task. GAD aims to discern the anomalies that deviate from most nodes. However, the model is prone to learn the pattern of normal samples which make up the majority of samples. Meanwhile, anomalies can be easily detected when their behaviors differ from normality. Therefore, the performance can be further improved by enhancing the ability to learn the normal pattern. To this end, we propose a normality learning-based GAD framework via multi-scale contrastive learning networks (NLGAD for abbreviation). Specifically, we first initialize the model with the contrastive networks on different scales. To provide sufficient and reliable normal nod
    
[^51]: 具有分布式用户采样和多信道ALOHA的能源感知联邦学习

    Energy-Aware Federated Learning with Distributed User Sampling and Multichannel ALOHA. (arXiv:2309.06033v1 [eess.SP])

    [http://arxiv.org/abs/2309.06033](http://arxiv.org/abs/2309.06033)

    本研究考虑了能量收集设备与多信道ALOHA相结合的联邦学习网络，提出了一种方法来解决能量耗尽问题，并确保任务的顺利执行。实验证明该方法在关键设置中的有效性和性能优于基于规范的解决方案。

    

    随着联邦学习（FL）的出现，分布式设备上的分布式学习引起了越来越多的关注。值得注意的是，边缘设备通常具有有限的电池和异构的能源可用性，而FL需要多轮迭代才能收敛，加大了对能源效率的需求。能源耗尽可能会阻碍训练过程和训练模型的高效利用。为了解决这些问题，本文考虑将能量收集（EH）设备集成到具有多信道ALOHA的FL网络中，并提出一种方法来保证低能源中断概率和成功执行未来任务。数值结果表明这种方法的有效性，特别是在平均能量收入无法覆盖迭代成本的关键设置中。该方法在收敛时间和电池电量方面优于基于规范的解决方案。

    Distributed learning on edge devices has attracted increased attention with the advent of federated learning (FL). Notably, edge devices often have limited battery and heterogeneous energy availability, while multiple rounds are required in FL for convergence, intensifying the need for energy efficiency. Energy depletion may hinder the training process and the efficient utilization of the trained model. To solve these problems, this letter considers the integration of energy harvesting (EH) devices into a FL network with multi-channel ALOHA, while proposing a method to ensure both low energy outage probability and successful execution of future tasks. Numerical results demonstrate the effectiveness of this method, particularly in critical setups where the average energy income fails to cover the iteration cost. The method outperforms a norm based solution in terms of convergence time and battery level.
    
[^52]: 未来无线网络中的多智能体强化学习中的紧急通信

    Emergent Communication in Multi-Agent Reinforcement Learning for Future Wireless Networks. (arXiv:2309.06021v1 [cs.LG])

    [http://arxiv.org/abs/2309.06021](http://arxiv.org/abs/2309.06021)

    多智能体强化学习中的紧急通信（EC-MARL）是解决未来无线网络中高维连续控制问题的合作方式，它赋予网络实体自主决策能力来解决复杂任务。

    

    在不同的无线网络场景中，多个网络实体需要合作以实现共同的任务，同时尽量减少延迟和能源消耗。未来的无线网络要求在动态和不确定的环境中交换高维数据，因此实施通信控制任务变得具有挑战性和高度复杂。多智能体强化学习与紧急通信（EC-MARL）是解决部分可观测状态下的高维连续控制问题的合作方式，其中智能体构建紧急通信协议来解决复杂任务。本文阐述了EC-MARL在未来6G无线网络背景下的重要性，该网络赋予网络实体自主决策能力来解决自动驾驶、机器人导航、飞行基站网络规划和智能城市应用等复杂任务。本文概述了EC-MARL算法和

    In different wireless network scenarios, multiple network entities need to cooperate in order to achieve a common task with minimum delay and energy consumption. Future wireless networks mandate exchanging high dimensional data in dynamic and uncertain environments, therefore implementing communication control tasks becomes challenging and highly complex. Multi-agent reinforcement learning with emergent communication (EC-MARL) is a promising solution to address high dimensional continuous control problems with partially observable states in a cooperative fashion where agents build an emergent communication protocol to solve complex tasks. This paper articulates the importance of EC-MARL within the context of future 6G wireless networks, which imbues autonomous decision-making capabilities into network entities to solve complex tasks such as autonomous driving, robot navigation, flying base stations network planning, and smart city applications. An overview of EC-MARL algorithms and the
    
[^53]: 深度神经网络的插值、逼近和可控性研究

    Interpolation, Approximation and Controllability of Deep Neural Networks. (arXiv:2309.06015v1 [cs.LG])

    [http://arxiv.org/abs/2309.06015](http://arxiv.org/abs/2309.06015)

    本文研究了深度神经网络的插值、逼近和可控性。通过控制理论分析，我们发现深度残差神经网络具有通用插值和通用逼近的能力。然而，这两个性质不能互相推导，但在一定条件下，两个性质也是等价的。

    

    本文通过控制理论，研究了将深度残差神经网络理想化为连续动力系统的表达能力。具体而言，我们考虑了来自监督学习的两个性质，即通用插值 - 能够匹配任意输入和目标训练样本 - 以及紧密相关的通用逼近 - 能够通过流动图近似输入-目标函数关系。在控制家族具有仿射不变性的假设下，我们给出了通用插值的刻画，证明了对于具有非线性的任何架构，它都成立。此外，我们阐明了通用插值和通用逼近在一般控制系统背景下的关系，表明这两个性质不能从彼此推导出来。同时，我们确定了控制家族和目标函数的条件，确保了这两个概念的等价性。

    We investigate the expressive power of deep residual neural networks idealized as continuous dynamical systems through control theory. Specifically, we consider two properties that arise from supervised learning, namely universal interpolation - the ability to match arbitrary input and target training samples - and the closely related notion of universal approximation the ability to approximate input-target functional relationships via flow maps. Under the assumption of affine invariance of the control family, we give a characterisation of universal interpolation, showing that it holds for essentially any architecture with non-linearity. Furthermore, we elucidate the relationship between universal interpolation and universal approximation in the context of general control systems, showing that the two properties cannot be deduced from each other. At the same time, we identify conditions on the control family and the target function that ensures the equivalence of the two notions.
    
[^54]: ATTA: 一种针对分割中的区分场外分布检测的异常感知的测试时适应方法

    ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation. (arXiv:2309.05994v1 [cs.CV])

    [http://arxiv.org/abs/2309.05994](http://arxiv.org/abs/2309.05994)

    本文提出了一种双层场外分布检测框架来处理领域转移和语义转移问题。该框架利用全局低级特征和密集高级特征图来适应模型到未见领域，并增强模型在检测新类别方面的能力。

    

    最近在稠密场外分布检测方面的进展主要集中在训练和测试数据集具有相似领域的情况下，假设它们之间不存在领域转移。然而，实际情况下常常存在领域转移，并且显著影响现有场外分布检测模型的准确性。在这项工作中，我们提出了一个双层场外分布检测框架，同时处理领域转移和语义转移。第一层利用全局低级特征区分图像中是否存在领域转移，而第二层利用密集高级特征图识别具有语义转移的像素。通过这种方式，我们可以有选择地适应模型到未见领域，并增强模型在检测新类别方面的能力。我们在几个场外分割基准上验证了我们提出的方法的有效性，包括 those with significant domain shifts and those without。

    Recent advancements in dense out-of-distribution (OOD) detection have primarily focused on scenarios where the training and testing datasets share a similar domain, with the assumption that no domain shift exists between them. However, in real-world situations, domain shift often exits and significantly affects the accuracy of existing out-of-distribution (OOD) detection models. In this work, we propose a dual-level OOD detection framework to handle domain shift and semantic shift jointly. The first level distinguishes whether domain shift exists in the image by leveraging global low-level features, while the second level identifies pixels with semantic shift by utilizing dense high-level feature maps. In this way, we can selectively adapt the model to unseen domains as well as enhance model's capacity in detecting novel classes. We validate the efficacy of our proposed method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, obse
    
[^55]: 学习无偏见的新闻文章表征：一种基于知识的方法。

    Learning Unbiased News Article Representations: A Knowledge-Infused Approach. (arXiv:2309.05981v1 [cs.LG])

    [http://arxiv.org/abs/2309.05981](http://arxiv.org/abs/2309.05981)

    提出了一种基于知识的深度学习模型，可以学习无偏见的新闻文章表示，从而准确预测新闻文章的政治倾向，并解决了现有学习模型受新闻发布者政治偏见影响的问题。

    

    在理解社会群体中政治意识形态的动态和减轻其影响的措施方面，对在线新闻文章的政治倾向进行量化有助于。然而，使用机器学习模型准确预测新闻文章的政治倾向是一项具有挑战性的任务。这是因为(i)新闻文章的政治意识形态由多个因素定义，以及(ii)现有学习模型在模型训练过程中受新闻发布者的政治偏见的影响而偏倚。目前只有有限的方法来研究新闻文章的政治倾向，这些方法也没有考虑算法的政治偏见，这降低了机器学习模型对任何新闻发布者发布的新闻文章的政治倾向的泛化能力。在这项工作中，我们提出了一种基于知识的深度学习模型，利用相对可靠的外部数据资源来学习无偏见的新闻文章表示。

    Quantification of the political leaning of online news articles can aid in understanding the dynamics of political ideology in social groups and measures to mitigating them. However, predicting the accurate political leaning of a news article with machine learning models is a challenging task. This is due to (i) the political ideology of a news article is defined by several factors, and (ii) the innate nature of existing learning models to be biased with the political bias of the news publisher during the model training. There is only a limited number of methods to study the political leaning of news articles which also do not consider the algorithmic political bias which lowers the generalization of machine learning models to predict the political leaning of news articles published by any new news publishers. In this work, we propose a knowledge-infused deep learning model that utilizes relatively reliable external data resources to learn unbiased representations of news articles usin
    
[^56]: CleanUNet 2：一种基于波形和频谱图的混合语音降噪模型

    CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram. (arXiv:2309.05975v1 [cs.LG])

    [http://arxiv.org/abs/2309.05975](http://arxiv.org/abs/2309.05975)

    CleanUNet 2是一种结合了波形和频谱图的混合语音降噪模型，通过两个阶段的框架，它构建在当前最先进的波形降噪器CleanUNet的基础上，并通过使用预测的频谱图作为输入来进一步提升性能。

    

    在这项工作中，我们提出了CleanUNet 2，一种结合了波形降噪器和频谱图降噪器优点并且取得了最佳效果的语音降噪模型。CleanUNet 2采用了一个受流行的语音合成方法启发的两阶段框架，包括一个波形模型和一个频谱图模型。具体而言，CleanUNet 2在当前最先进的波形降噪器CleanUNet的基础上，通过使用从频谱图降噪器预测的频谱图作为输入来进一步提升性能。我们证明了CleanUNet 2在各种客观和主观评估方面优于先前的方法。

    In this work, we present CleanUNet 2, a speech denoising model that combines the advantages of waveform denoiser and spectrogram denoiser and achieves the best of both worlds. CleanUNet 2 uses a two-stage framework inspired by popular speech synthesis methods that consist of a waveform model and a spectrogram model. Specifically, CleanUNet 2 builds upon CleanUNet, the state-of-the-art waveform denoiser, and further boosts its performance by taking predicted spectrograms from a spectrogram denoiser as the input. We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations.
    
[^57]: 切断电路: 通过有针对性的消融去除模型行为

    Circuit Breaking: Removing Model Behaviors with Targeted Ablation. (arXiv:2309.05973v1 [cs.CL])

    [http://arxiv.org/abs/2309.05973](http://arxiv.org/abs/2309.05973)

    本论文提出了一种通过有针对性的消融模型组件之间的因果路径来去除语言模型中不良行为的新方法。在减少GPT-2毒性语言生成方面，仅消融12条因果边中的11.6K可以有效减轻毒性生成，并在其他输入上的性能下降很小。

    

    语言模型通常会表现出在预训练目标上提高性能但在下游任务上降低性能的行为。我们提出了一种新颖的方法，通过消融模型组件之间的一小部分因果路径，以禁用与不良行为有关的计算电路，从而去除不良行为。在拥有模型表现差的小型输入数据集的情况下，我们学会了消融一小部分重要的因果路径。在减少GPT-2毒性语言生成方面，我们发现消融仅仅12条因果边中的11.6K，可以减轻毒性生成，同时在其他输入上的性能下降很小。

    Language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. We propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. Given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. In the setting of reducing GPT-2 toxic language generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
    
[^58]: 神经网络层矩阵分解揭示潜在流形编码和存储容量

    Neural Network Layer Matrix Decomposition reveals Latent Manifold Encoding and Memory Capacity. (arXiv:2309.05968v1 [cs.LG])

    [http://arxiv.org/abs/2309.05968](http://arxiv.org/abs/2309.05968)

    通过神经网络层矩阵分解，我们揭示了神经网络层编码训练数据集的潜在流形和数学运算的几何性质，这对于理解神经网络如何突破维度诅咒具有重要意义。

    

    我们证明了通用逼近定理的逆定理，即神经网络(NN)编码定理，它表明对于每个稳定收敛的NN和连续激活函数，其权重矩阵实际上编码了一个连续函数，该函数在有界域内近似于训练数据集，并且误差在有限范围内。我们进一步展示了使用特征值分解和奇异值分解来对每个NN层的权重矩阵进行矩阵分解，可以揭示训练数据集所编码和表示的潜在空间流形的性质，以及每个NN层执行的几何操作的性质。我们的结果对于理解NN如何通过利用存储容量来突破维度诅咒具有意义，并且这两者是互补的。这种层矩阵分解(LMD)进一步揭示了NN层的特征值分解与最新的研究有密切关联。

    We prove the converse of the universal approximation theorem, i.e. a neural network (NN) encoding theorem which shows that for every stably converged NN of continuous activation functions, its weight matrix actually encodes a continuous function that approximates its training dataset to within a finite margin of error over a bounded domain. We further show that using the Eckart-Young theorem for truncated singular value decomposition of the weight matrix for every NN layer, we can illuminate the nature of the latent space manifold of the training dataset encoded and represented by every NN layer, and the geometric nature of the mathematical operations performed by each NN layer. Our results have implications for understanding how NNs break the curse of dimensionality by harnessing memory capacity for expressivity, and that the two are complementary. This Layer Matrix Decomposition (LMD) further suggests a close relationship between eigen-decomposition of NN layers and the latest advanc
    
[^59]: 评估潮起潮落：对不同平台间问答趋势的深入分析

    Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])

    [http://arxiv.org/abs/2309.05961](http://arxiv.org/abs/2309.05961)

    本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。

    

    社区问答平台因其快速回答用户查询的能力而越来越受欢迎。这些回答速度的快慢取决于查询特定和用户相关的因素的综合。本文通过研究六个高度流行的社区问答平台，分析了这些因素在其中的作用。我们的调查揭示了问题的第一个回答所花费的时间与元数据、问题的构成方式和用户之间的互动水平之间的关联。此外，通过使用传统的机器学习模型分析这些元数据和用户互动模式，我们试图预测哪些查询将迅速获得初始回答。

    Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
    
[^60]: GLAD: 内容感知的动态图用于日志异常检测

    GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection. (arXiv:2309.05953v1 [cs.LG])

    [http://arxiv.org/abs/2309.05953](http://arxiv.org/abs/2309.05953)

    GLAD是一个内容感知的动态图用于日志异常检测的框架，它综合了日志语义、关系模式和顺序模式，通过识别关键字段和构建动态日志图来检测系统日志中的关联异常。

    

    记录了有关系统信息的日志在系统监控和调试中起着关键作用，包括事件和状态。尽管已经提出了各种方法来检测日志序列中的异常，但它们往往忽视了考虑系统组件之间的关系的重要性，例如服务和用户，这些可以从日志内容中识别出来。理解这些关系对于检测异常及其潜在原因至关重要。为了解决这个问题，我们引入了GLAD，一个基于图的日志异常检测框架，用于检测系统日志中的关联异常。GLAD将日志语义、关系模式和顺序模式纳入统一框架进行异常检测。具体而言，GLAD首先引入一个字段提取模块，利用基于提示的少样本学习从日志内容中识别出关键字段。然后，GLAD通过互连提取的字段和日志事件构建滑动窗口的动态日志图。

    Logs play a crucial role in system monitoring and debugging by recording valuable system information, including events and states. Although various methods have been proposed to detect anomalies in log sequences, they often overlook the significance of considering relations among system components, such as services and users, which can be identified from log contents. Understanding these relations is vital for detecting anomalies and their underlying causes. To address this issue, we introduce GLAD, a Graph-based Log Anomaly Detection framework designed to detect relational anomalies in system logs. GLAD incorporates log semantics, relational patterns, and sequential patterns into a unified framework for anomaly detection. Specifically, GLAD first introduces a field extraction module that utilizes prompt-based few-shot learning to identify essential fields from log contents. Then GLAD constructs dynamic log graphs for sliding windows by interconnecting extracted fields and log events p
    
[^61]: 语言模型作为视觉-语言模型的黑盒优化器

    Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])

    [http://arxiv.org/abs/2309.05950](http://arxiv.org/abs/2309.05950)

    本论文介绍了一种新的视觉-语言模型 (VLMs) 微调方法，通过自然语言提示来避免访问模型参数，采用聊天式的语言模型作为黑盒优化器，在少样本图像分类任务中达到效果。

    

    预训练在大规模网络数据集上的视觉-语言模型 (VLMs) 展示了在各种视觉和多模态任务中的显著能力。目前，VLMs 的微调方法主要在白盒环境中操作，需要访问模型参数进行反向传播。然而，许多 VLMs 依赖于专有数据且不开源，限制了使用白盒方法进行微调。鉴于像 ChatGPT 这样的受欢迎私有大型语言模型 (LLMs) 仍然提供基于语言的用户界面，我们旨在通过自然语言提示开发一种新的 VLMs 微调方法，从而避免访问模型参数、特征嵌入或输出 logits 的需要。在这种设置下，我们提出使用基于聊天的 LLMs 作为黑盒优化器，以在使用 CLIP 进行少样本图像分类的示例任务中寻找最佳文本提示。具体而言，我们采用自动"爬山"程序，它能收敛到有效的提示上。

    Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
    
[^62]: 针对生物信号的频率感知掩码自编码器的多模态预训练

    Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals. (arXiv:2309.05927v1 [cs.LG])

    [http://arxiv.org/abs/2309.05927](http://arxiv.org/abs/2309.05927)

    本研究提出了一种名为$\texttt{bio}$FAME的频率感知掩码自编码器，用于多模态生物信号的预训练。其通过在频率空间中对生物信号进行表示参数化，利用固定大小的傅里叶变换运算符进行全局令牌混合，并通过频率维持预训练策略保持每个输入通道中的频率成分。

    

    利用来自生物信号的多模态信息对人们的身心状态进行综合建模非常重要。然而，多模态生物信号通常在预训练和推断数据集之间存在重大的分布偏移，这源于任务规范的变化或者模态组合的差异。为了在潜在分布偏移的情况下实现有效的预训练，我们提出了一种频率感知的掩码自编码器（$\texttt{bio}$FAME），该自编码器学习在频率空间中对生物信号的表示进行参数化。$\texttt{bio}$FAME包含一个频率感知变压器，利用基于傅里叶变换的固定大小的运算符进行全局令牌混合，与输入的长度和采样率无关。为了保持每个输入通道中的频率成分，我们还采用了一种频率维持预训练策略，在潜空间中执行掩码自编码。最终的架构有效地捕获不同任务间的频率特征和模态组合的变化。

    Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder ($\texttt{bio}$FAME) that learns to parameterize the representation of biosignals in the frequency space. $\texttt{bio}$FAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. The resulting architecture effectivel
    
[^63]: 关于正则稀疏逻辑回归的研究

    On Regularized Sparse Logistic Regression. (arXiv:2309.05925v1 [cs.LG])

    [http://arxiv.org/abs/2309.05925](http://arxiv.org/abs/2309.05925)

    本文提出了解决正则稀疏逻辑回归的方法，包括$\ell_1$正则化稀疏逻辑回归和一些满足先决条件的非凸惩罚正则化稀疏逻辑回归。经验实验表明，这些算法能够以较低的计算成本有效地进行分类和特征选择。

    

    稀疏逻辑回归旨在同时进行高维数据的分类和特征选择。虽然有许多研究解决了$\ell_1$正则化逻辑回归问题，但对于与非凸惩罚相关的稀疏逻辑回归解决方案并没有等量的文献。本文提出了解决$\ell_1$正则化稀疏逻辑回归和一些满足一定先决条件的非凸惩罚正则化稀疏逻辑回归的方法，并采用类似的优化框架。在提出的优化框架中，我们利用不同的线搜索准则来保证不同正则化项的良好收敛性能。通过对真实世界数据集的二元分类任务进行经验实验，我们证明了我们提出的算法能够以较低的计算成本有效地进行分类和特征选择。

    Sparse logistic regression aims to perform classification and feature selection simultaneously for high-dimensional data. Although many studies have been done to solve $\ell_1$-regularized logistic regression, there is no equivalently abundant literature about solving sparse logistic regression associated with nonconvex penalties. In this paper, we propose to solve $\ell_1$-regularized sparse logistic regression and some nonconvex penalties-regularized sparse logistic regression, when the nonconvex penalties satisfy some prerequisites, with similar optimization frameworks. In the proposed optimization frameworks, we utilize different line search criteria to guarantee good convergence performance for different regularization terms. Empirical experiments on binary classification tasks with real-world datasets demonstrate our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.
    
[^64]: 通过优势调节使用动态规划增强决策Transformer

    ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning. (arXiv:2309.05915v1 [cs.LG])

    [http://arxiv.org/abs/2309.05915](http://arxiv.org/abs/2309.05915)

    这篇论文提出了一种通过将动态规划应用于决策Transformer来增强其能力的方法。作者提出了三个步骤来实现这一目标：使用样本内值迭代获得近似值函数，结合估计的优势评估动作质量，并训练ACT生成基于估计优势的动作。该方法在测试中表现出良好的性能。

    

    决策Transformer (DT) 利用表达丰富的序列建模技术来执行动作生成，已成为离线策略优化的一种有前景的方法。然而，DT 生成的动作是基于期望未来回报的条件，已知具有某些弱点，比如易受环境随机性影响。为了克服DT的弱点，我们提出了在DT中增加动态规划能力的方法。我们的方法包括三个步骤。首先，我们使用样本内值迭代来获得近似值函数，这涉及到MDP结构上的动态规划。第二，我们结合估计的优势来评估动作的质量。我们引入了两种优势估计器，分别适用于不同的任务。第三，我们训练了一个以估计的优势为条件生成动作的优势条件Transformer (ACT)。最后，在测试阶段，ACT根据所需的优势生成动作。我们的评估结果表明...

    Decision Transformer (DT), which employs expressive sequence modeling techniques to perform action generation, has emerged as a promising approach to offline policy optimization. However, DT generates actions conditioned on a desired future return, which is known to bear some weaknesses such as the susceptibility to environmental stochasticity. To overcome DT's weaknesses, we propose to empower DT with dynamic programming. Our method comprises three steps. First, we employ in-sample value iteration to obtain approximated value functions, which involves dynamic programming over the MDP structure. Second, we evaluate action quality in context with estimated advantages. We introduce two types of advantage estimators, IAE and GAE, which are suitable for different tasks. Third, we train an Advantage-Conditioned Transformer (ACT) to generate actions conditioned on the estimated advantages. Finally, during testing, ACT generates actions conditioned on a desired advantage. Our evaluation resul
    
[^65]: 通过符号学习评估显著目标检测的对抗攻击

    Adversarial Attacks Assessment of Salient Object Detection via Symbolic Learning. (arXiv:2309.05900v1 [cs.CV])

    [http://arxiv.org/abs/2309.05900](http://arxiv.org/abs/2309.05900)

    这项工作证明了通过符号学习的显著目标检测方法在面对对抗攻击时具有鲁棒性。

    

    机器学习是主流技术的核心，优于手工特征设计的传统方法。除了对人工特征提取的学习过程外，它具有从输入到输出的端到端范 Paradigm，可以实现非常准确的结果。然而，关于其对恶意和难以察觉的扰动的稳健性的安全问题已经引起了关注，因为其预测可能会完全改变。显著目标检测是一个研究领域，深度卷积神经网络在其中已被证明非常有效，但其可信性代表了一个需要分析和解决黑客攻击的重要问题。脑程序设计是一种符号学习方法，在传统的人工智能中是非常重要的。这项工作提供了证据，表明符号学习的鲁棒性对于设计可靠的视觉注意系统至关重要，因为它可以经受住最强烈的扰动。我们测试了这种进化计算方法论。

    Machine learning is at the center of mainstream technology and outperforms classical approaches to handcrafted feature design. Aside from its learning process for artificial feature extraction, it has an end-to-end paradigm from input to output, reaching outstandingly accurate results. However, security concerns about its robustness to malicious and imperceptible perturbations have drawn attention since its prediction can be changed entirely. Salient object detection is a research area where deep convolutional neural networks have proven effective but whose trustworthiness represents a significant issue requiring analysis and solutions to hackers' attacks. Brain programming is a kind of symbolic learning in the vein of good old-fashioned artificial intelligence. This work provides evidence that symbolic learning robustness is crucial in designing reliable visual attention systems since it can withstand even the most intense perturbations. We test this evolutionary computation methodolo
    
[^66]: 分层条件半配对图像到图像转换：针对购物网站的多任务图像缺陷校正

    Hierarchical Conditional Semi-Paired Image-to-Image Translation For Multi-Task Image Defect Correction On Shopping Websites. (arXiv:2309.05883v1 [cs.CV])

    [http://arxiv.org/abs/2309.05883](http://arxiv.org/abs/2309.05883)

    本论文提出了一个统一的图像到图像转换模型，可以在不同产品类型上校正多个缺陷。通过引入分层的注意力机制，模型可以有效地减轻缺陷对图像质量的影响。与最先进的方法相比，模型在缺陷校正上取得了显著的性能提升。

    

    在购物网站上，低质量的产品图片会对客户的体验产生负面影响。尽管在检测具有不同缺陷的图像方面已经有大量的工作，但很少有工作致力于在规模上纠正这些缺陷。一个主要的挑战是，有成千上万种产品类型，每种类型都有特定的缺陷，因此构建特定缺陷的模型是不可扩展的。在本文中，我们提出了一个统一的图像到图像（I2I）转换模型，以纠正不同产品类型上的多个缺陷。我们的模型利用注意力机制，以分层方式将高级缺陷组和特定缺陷类型合并，引导网络将焦点放在与缺陷相关的图像区域上。在八个公共数据集上进行评估，我们的模型与目前最先进的I2I方法MoNCE相比，平均将Frechet Inception Distance (FID)降低了24.6%。与公共数据不同，购物网站上的另一个实际挑战是有些配对图像质量较低。因此，

    On shopping websites, product images of low quality negatively affect customer experience. Although there are plenty of work in detecting images with different defects, few efforts have been dedicated to correct those defects at scale. A major challenge is that there are thousands of product types and each has specific defects, therefore building defect specific models is unscalable. In this paper, we propose a unified Image-to-Image (I2I) translation model to correct multiple defects across different product types. Our model leverages an attention mechanism to hierarchically incorporate high-level defect groups and specific defect types to guide the network to focus on defect-related image regions. Evaluated on eight public datasets, our model reduces the Frechet Inception Distance (FID) by 24.6% in average compared with MoNCE, the state-of-the-art I2I method. Unlike public data, another practical challenge on shopping websites is that some paired images are of low quality. Therefore 
    
[^67]: 面部验证系统的广义攻击

    Generalized Attacks on Face Verification Systems. (arXiv:2309.05879v1 [cs.CR])

    [http://arxiv.org/abs/2309.05879](http://arxiv.org/abs/2309.05879)

    这篇论文研究了对面部验证系统的广义攻击。通过引入DodgePersonation攻击，创造了模仿给定身份的面部图像，同时避免被识别为不同的身份。提出了一种分类法，对不同类型的对抗攻击进行统一总结。并提出了“一张面孔统治所有”的攻击，具有最先进的性能。

    

    近年来，使用深度神经网络模型的面部验证系统取得了巨大的进展，超越了人类的准确性，并在边境控制和智能手机解锁等各种应用中得到部署。然而，面部验证系统容易受到对抗攻击的影响，这些攻击会操纵输入图像以欺骗这些系统，而这种操纵对人类通常是难以察觉的。本文深入研究了对面部验证系统的攻击。我们引入了DodgePersonation攻击，并提出了一种方法来创建模仿一组给定身份的面部图像，同时避免被识别为任何一个不同的身份。我们提出了一种分类法，以提供对抗面部验证系统的不同类型攻击的统一视角，包括躲避攻击、冒名攻击和主要面部攻击。最后，我们提出了“一张面孔统治所有”的攻击，它使用DodgePersonation攻击并在众所周知的场景中具有最先进的性能。

    Face verification (FV) using deep neural network models has made tremendous progress in recent years, surpassing human accuracy and seeing deployment in various applications such as border control and smartphone unlocking. However, FV systems are vulnerable to Adversarial Attacks, which manipulate input images to deceive these systems in ways usually unnoticeable to humans. This paper provides an in-depth study of attacks on FV systems. We introduce the DodgePersonation Attack that formulates the creation of face images that impersonate a set of given identities while avoiding being identified as any of the identities in a separate, disjoint set. A taxonomy is proposed to provide a unified view of different types of Adversarial Attacks against FV systems, including Dodging Attacks, Impersonation Attacks, and Master Face Attacks. Finally, we propose the ''One Face to Rule Them All'' Attack which implements the DodgePersonation Attack with state-of-the-art performance on a well-known sce
    
[^68]: 反应坐标流在分子动力学模型简化中的应用

    Reaction coordinate flows for model reduction of molecular kinetics. (arXiv:2309.05878v1 [cs.LG])

    [http://arxiv.org/abs/2309.05878](http://arxiv.org/abs/2309.05878)

    该论文介绍了一种基于流的机器学习方法，称为反应坐标流，用于发现分子系统低维动力学模型，该方法能够以连续时间和空间中的可训练和可处理的方式进行模型简化，产生准确和可解释的低维表示。

    

    在这项工作中，我们引入了一种基于流的机器学习方法，称为反应坐标（RC）流，用于发现分子系统低维动力学模型。RC流利用归一化流设计坐标变换，并使用布朗动力学模型来近似RC的动力学，所有模型参数可以以数据驱动的方式进行估计。与现有的分子动力学模型简化方法不同，由于归一化流的可逆性，RC流在连续时间和空间中提供了可训练和可处理的简化动力学模型。此外，本文研究的基于布朗动力学的简化动力学模型在分子系统的相空间中产生了易于辨别的亚稳态表示。数值实验证明了所提方法如何有效地发现给定的完整状态动力学的可解释和准确的低维表示。

    In this work, we introduce a flow based machine learning approach, called reaction coordinate (RC) flow, for discovery of low-dimensional kinetic models of molecular systems. The RC flow utilizes a normalizing flow to design the coordinate transformation and a Brownian dynamics model to approximate the kinetics of RC, where all model parameters can be estimated in a data-driven manner. In contrast to existing model reduction methods for molecular kinetics, RC flow offers a trainable and tractable model of reduced kinetics in continuous time and space due to the invertibility of the normalizing flow. Furthermore, the Brownian dynamics-based reduced kinetic model investigated in this work yields a readily discernible representation of metastable states within the phase space of the molecular system. Numerical experiments demonstrate how effectively the proposed method discovers interpretable and accurate low-dimensional representations of given full-state kinetics from simulations.
    
[^69]: 基于跳跃距离的力导向图嵌入

    Force-directed graph embedding with hops distance. (arXiv:2309.05865v1 [cs.LG])

    [http://arxiv.org/abs/2309.05865](http://arxiv.org/abs/2309.05865)

    本文提出了一种新颖的基于跳跃距离的力导向图嵌入方法，通过模拟自定义的引力和斥力以及使用牛顿第二定律，嵌入节点以保持图的拓扑结构和结构特征，并在多个图分析任务上取得了竞争性能。

    

    图嵌入已成为分析图结构数据的一种越来越重要的技术。通过将图中节点表示为低维空间中的向量，图嵌入可以实现高效的图处理和分析任务，如节点分类、链接预测和可视化。本文提出了一种新颖的力导向图嵌入方法，利用稳定加速度动力学公式将节点嵌入以保持图的拓扑结构和结构特征。我们的方法模拟了针对每对节点的自定义引力和斥力，这些力是根据节点之间的跳跃距离计算的。然后使用牛顿第二定律来获得每个节点的加速度。该方法直观、可并行化和高度可扩展。我们在几个图分析任务上评估了我们的方法，并展示了与最先进的无监督嵌入技术相比的竞争性能。

    Graph embedding has become an increasingly important technique for analyzing graph-structured data. By representing nodes in a graph as vectors in a low-dimensional space, graph embedding enables efficient graph processing and analysis tasks like node classification, link prediction, and visualization. In this paper, we propose a novel force-directed graph embedding method that utilizes the steady acceleration kinetic formula to embed nodes in a way that preserves graph topology and structural features. Our method simulates a set of customized attractive and repulsive forces between all node pairs with respect to their hop distance. These forces are then used in Newton's second law to obtain the acceleration of each node. The method is intuitive, parallelizable, and highly scalable. We evaluate our method on several graph analysis tasks and show that it achieves competitive performance compared to state-of-the-art unsupervised embedding techniques.
    
[^70]: 仿生神经网络用于外部模拟人类运动系统

    The bionic neural network for external simulation of human locomotor system. (arXiv:2309.05863v1 [cs.LG])

    [http://arxiv.org/abs/2309.05863](http://arxiv.org/abs/2309.05863)

    本文提出了一种基于肌肉骨骼模拟和物理信息深度学习的方法，用于预测关节运动和肌肉力量。

    

    用肌肉骨骼模拟技术估计的肌肉力量和关节动力学提供了描述运动质量的有用指标。基于模型的计算机肌肉骨骼模型能够解释神经驱动肌肉、肌肉动力学、身体和关节动力学以及动力学之间的动态相互作用。然而，这些解决方案在复杂模型中存在计算时间长和肌肉招募问题。近年来，基于数据驱动的方法已经成为一种有前途的替代方案，因为它具有灵活性和适应性的优势。然而，获得大量标记的训练数据并不容易。本文提出了一种基于肌肉骨骼模拟的物理信息深度学习方法，用于预测关节运动和肌肉力量。将肌肉骨骼模型嵌入神经网络中作为一个带有生理参数的常微分方程（ODE）损失函数，用于识别肌肉激活动力学和肌肉收缩动力学。

    Muscle forces and joint kinematics estimated with musculoskeletal (MSK) modeling techniques offer useful metrics describing movement quality. Model-based computational MSK models can interpret the dynamic interaction between the neural drive to muscles, muscle dynamics, body and joint kinematics, and kinetics. Still, such a set of solutions suffers from high computational time and muscle recruitment problems, especially in complex modeling. In recent years, data-driven methods have emerged as a promising alternative due to the benefits of flexibility and adaptability. However, a large amount of labeled training data is not easy to be acquired. This paper proposes a physics-informed deep learning method based on MSK modeling to predict joint motion and muscle forces. The MSK model is embedded into the neural network as an ordinary differential equation (ODE) loss function with physiological parameters of muscle activation dynamics and muscle contraction dynamics to be identified. These 
    
[^71]: 揭示Transformer中的mesa-optimization算法

    Uncovering mesa-optimization algorithms in Transformers. (arXiv:2309.05858v1 [cs.LG])

    [http://arxiv.org/abs/2309.05858](http://arxiv.org/abs/2309.05858)

    本研究揭示了Transformer模型中的mesa-optimization算法，该算法通过内部学习目标和相应的优化解决方案驱动预测生成。研究还发现，这种学习的优化算法可以被应用于解决监督式少样本任务，暗示了mesa-optimization可能是大型语言模型上下文学习能力的基础。

    

    Transformer已经成为深度学习中主导的模型，但其卓越性能的原因尚不清楚。我们假设Transformer的强大性能源于其架构中对mesa-optimization的偏好，即一个学习过程在模型的前向传递中运行，由以下两个步骤组成：（i）构建内部学习目标，和（ii）通过优化找到相应的解决方案。为了验证这个假设，我们对一系列在简单序列建模任务上训练的自回归Transformer进行了逆向工程，揭示了驱动预测生成的基于梯度的底层mesa-optimization算法。此外，我们还展示了学习的前向传递优化算法可以立即被重新应用于解决监督式少样本任务，这表明mesa-optimization可能是大型语言模型的上下文学习能力的基础。最后，我们提出了一种新颖的自注意力机制。

    Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention 
    
[^72]: 随机滤波器组的能量保持和稳定性

    Energy Preservation and Stability of Random Filterbanks. (arXiv:2309.05855v1 [cs.LG])

    [http://arxiv.org/abs/2309.05855](http://arxiv.org/abs/2309.05855)

    本文从随机卷积算子的数学角度详细阐述了简单卷积神经网络的统计属性，发现具有随机高斯权重的FIR滤波器组在大滤波器和局部周期输入信号的情况下是病态的，并推导了其期望帧边界的理论界限。

    

    波形为基础的深度学习为什么如此困难？尽管有多次尝试训练卷积神经网络(convnets)进行滤波器设计，但它们往往无法超越手工创建的基线。这更令人惊讶，因为这些基线是线性时不变系统：因此，它们的传递函数可以通过具有大感受野的卷积神经网络准确表示。在本文中，我们从随机卷积算子的数学角度详细阐述了简单卷积神经网络的统计属性。我们发现，具有随机高斯权重的FIR滤波器组在大滤波器和局部周期输入信号的情况下是病态的，这在音频信号处理应用中是典型的。此外，我们观察到随机滤波器组的期望能量保持对于数值稳定性是不足够的，并推导了其期望帧边界的理论界限。

    What makes waveform-based deep learning so hard? Despite numerous attempts at training convolutional neural networks (convnets) for filterbank design, they often fail to outperform hand-crafted baselines. This is all the more surprising because these baselines are linear time-invariant systems: as such, their transfer functions could be accurately represented by a convnet with a large receptive field. In this article, we elaborate on the statistical properties of simple convnets from the mathematical perspective of random convolutional operators. We find that FIR filterbanks with random Gaussian weights are ill-conditioned for large filters and locally periodic input signals, which both are typical in audio signal processing applications. Furthermore, we observe that expected energy preservation of a random filterbank is not sufficient for numerical stability and derive theoretical bounds for its expected frame bounds.
    
[^73]: ChemSpaceAL:一个应用于特定蛋白质分子生成的高效主动学习方法

    ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation. (arXiv:2309.05853v1 [cs.LG])

    [http://arxiv.org/abs/2309.05853](http://arxiv.org/abs/2309.05853)

    这项工作提出了一种新颖而高效的半监督主动学习方法，通过策略性地操作样本空间表示，可以使生成模型相对于客观函数进行微调。在目标分子生成的背景下，通过在化学空间代理内部策略性地操作，实现了最大化生成分子与蛋白质靶标之间吸引力相互作用的能力。

    

    生成人工智能模型的不可思议能力不可避免地导致了它们在药物发现领域的应用。因此，开发能够增强这些强大工具的能力和适用性的方法论具有巨大的兴趣。在这项工作中，我们提出了一种新颖而高效的半监督主动学习方法，该方法通过在构建的样本空间表示中策略性地操作，可以使生成模型相对于客观函数进行微调。在目标分子生成的背景下，我们展示了一种能够相对于基于吸引力相互作用评分函数进行微调的GPT基础分子生成模型的能力，通过在化学空间代理内部策略性地操作，从而最大化生成的分子与蛋白质靶标之间的吸引力相互作用。重要的是，我们的方法不需要对用于微调的所有数据点进行单独的评估。

    The incredible capabilities of generative artificial intelligence models have inevitably led to their application in the domain of drug discovery. It is therefore of tremendous interest to develop methodologies that enhance the abilities and applicability of these powerful tools. In this work, we present a novel and efficient semi-supervised active learning methodology that allows for the fine-tuning of a generative model with respect to an objective function by strategically operating within a constructed representation of the sample space. In the context of targeted molecular generation, we demonstrate the ability to fine-tune a GPT-based molecular generator with respect to an attractive interaction-based scoring function by strategically operating within a chemical space proxy, thereby maximizing attractive interactions between the generated molecules and a protein target. Importantly, our approach does not require the individual evaluation of all data points that are used for fine-
    
[^74]: 在多元时间序列医疗数据上进行有效的异常活动检测

    Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data. (arXiv:2309.05845v1 [cs.LG])

    [http://arxiv.org/abs/2309.05845](http://arxiv.org/abs/2309.05845)

    在多元时间序列医疗数据上，我们提出了一种基于残差的异常检测方法，用于有效的表示学习和异常活动检测。实验结果显示F1分数为0.839。

    

    多元时间序列（MTS）数据通过多个传感器收集，为智能医疗场景中准确的异常活动检测提供了潜力。然而，异常表现出多样的模式，并且在MTS数据中变得不容易察觉。因此，实现准确的异常检测是具有挑战性的，我们需要捕捉时间序列的时间依赖性和变量之间的相互关系。为了解决这个问题，我们提出了一种基于残差的异常检测方法Rs-AD，用于有效的表示学习和异常活动检测。我们在一个真实的步态数据集上评估了我们的方案，实验结果显示F1分数为0.839。

    Multivariate time series (MTS) data collected from multiple sensors provide the potential for accurate abnormal activity detection in smart healthcare scenarios. However, anomalies exhibit diverse patterns and become unnoticeable in MTS data. Consequently, achieving accurate anomaly detection is challenging since we have to capture both temporal dependencies of time series and inter-relationships among variables. To address this problem, we propose a Residual-based Anomaly Detection approach, Rs-AD, for effective representation learning and abnormal activity detection. We evaluate our scheme on a real-world gait dataset and the experimental results demonstrate an F1 score of 0.839.
    
[^75]: 优化健康相关声音对比学习的音频增强

    Optimizing Audio Augmentations for Contrastive Learning of Health-Related Acoustic Signals. (arXiv:2309.05843v1 [cs.LG])

    [http://arxiv.org/abs/2309.05843](http://arxiv.org/abs/2309.05843)

    本文通过利用自监督学习框架SimCLR和Slowfast NFNet背骨，优化健康声学对比学习。我们针对该应用识别了有效的音频增强策略，并发现当这些增强方法相结合时能够产生超过单独应用每个增强的 synergistic effects。

    

    健康相关声音（如咳嗽和呼吸声）对于医学诊断和持续健康监测非常重要。大多数现有的健康声学机器学习方法针对特定任务进行训练和评估，限制了它们在各种医疗应用中的泛化能力。本文利用自监督学习框架SimCLR和Slowfast NFNet背骨进行健康声学的对比学习。优化Slowfast NFNet在这个应用中的一个关键方面在于识别有效的音频增强。我们对各种音频增强策略进行了深入分析，并证明适当的增强策略可以提高Slowfast NFNet音频编码器在各种健康声学任务上的性能。我们的研究结果表明，当增强方法相结合时，它们可以产生超过单独应用每个增强的效果。

    Health-related acoustic signals, such as cough and breathing sounds, are relevant for medical diagnosis and continuous health monitoring. Most existing machine learning approaches for health acoustics are trained and evaluated on specific tasks, limiting their generalizability across various healthcare applications. In this paper, we leverage a self-supervised learning framework, SimCLR with a Slowfast NFNet backbone, for contrastive learning of health acoustics. A crucial aspect of optimizing Slowfast NFNet for this application lies in identifying effective audio augmentations. We conduct an in-depth analysis of various audio augmentation strategies and demonstrate that an appropriate augmentation strategy enhances the performance of the Slowfast NFNet audio encoder across a diverse set of health acoustic tasks. Our findings reveal that when augmentations are combined, they can produce synergistic effects that exceed the benefits seen when each is applied individually.
    
[^76]: 安全过滤器: 自主系统中安全关键控制的统一视图

    The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems. (arXiv:2309.05837v1 [eess.SY])

    [http://arxiv.org/abs/2309.05837](http://arxiv.org/abs/2309.05837)

    这篇文章提供了对安全过滤器方法的综述，提出了一个统一的技术框架来理解、比较和结合现有的技术，为成功部署下一代自主机器人提供了重要的指导。

    

    近年来，机器人自主领域取得了显著进展，并伴随着机器人技术的不断扩展。然而，新部署领域的出现给保证这些系统安全运行带来了前所未有的挑战，这仍然非常重要。传统的基于模型的安全控制方法在泛化性和可扩展性方面存在困难，新兴的数据驱动方法则往往缺乏明确的保证，这可能导致不可预测的灾难性故障。成功部署下一代自主机器人将需要整合两种范式的优势。本文提供了安全过滤器方法的综述，突出了现有技术之间的重要联系，并提出了一个统一的技术框架来理解、比较和结合它们。这个新的统一视图揭示了一系列看似不相关的安全过滤器类之间的共享模块结构，并自然地提出了进一步的研究方向。

    Recent years have seen significant progress in the realm of robot autonomy, accompanied by the expanding reach of robotic technologies. However, the emergence of new deployment domains brings unprecedented challenges in ensuring safe operation of these systems, which remains as crucial as ever. While traditional model-based safe control methods struggle with generalizability and scalability, emerging data-driven approaches tend to lack well-understood guarantees, which can result in unpredictable catastrophic failures. Successful deployment of the next generation of autonomous robots will require integrating the strengths of both paradigms. This article provides a review of safety filter approaches, highlighting important connections between existing techniques and proposing a unified technical framework to understand, compare, and combine them. The new unified view exposes a shared modular structure across a range of seemingly disparate safety filter classes and naturally suggests dir
    
[^77]: PACE: 使用GPT-4进行云事件根本原因分析中的提示和增加以进行校准的置信度估计

    PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])

    [http://arxiv.org/abs/2309.05833](http://arxiv.org/abs/2309.05833)

    本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。

    

    近年来，IT行业向基于云的平台的转变强调了云事件根本原因分析的重要性，以确保服务的可靠性和维护客户信任。核心问题是有效确定根本原因，由于当代云基础设施的复杂性，这一任务变得具有挑战性。尽管出现了许多用于根本原因识别的基于AI的工具，但它们的适用性仍受到其输出质量不一致的限制。本文介绍了一种通过提示检索增强的大语言模型（LLM）来增强根本原因分析工具中置信度估计的方法。此方法分为两个阶段。首先，模型根据历史事件数据评估自身的置信度，考虑其对证据的评估强度。然后，模型审核由预测器生成的根本原因。然后，优化步骤将这些评估结合起来确定最终的置信度估计。

    In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
    
[^78]: 实例无关的几何与接触动力学学习

    Instance-Agnostic Geometry and Contact Dynamics Learning. (arXiv:2309.05832v1 [cs.CV])

    [http://arxiv.org/abs/2309.05832](http://arxiv.org/abs/2309.05832)

    本文提出了一个实例无关的学习框架，通过几何作为共享表示，将视觉与动力学相结合，从RGBD视频中学习物体的几何和动力学属性。实验结果表明，该框架能够学习刚性和凸物体的几何和动力学，并改进了跟踪框架。

    

    本文提出了一个实例无关的学习框架，通过几何作为共享表示，将视觉与动力学相结合，同时学习形状、姿态轨迹和物理性质。与许多接触学习方法不同，我们的框架从RGBD视频中学习物体的几何和动力学属性，而不需要类别级别或实例级别的形状先验知识。我们将视觉系统BundleSDF与动力学系统ContactNets集成，并提出了一个循环训练管道，使用动力学模块的输出通过透视重投影来改进视觉模块的姿态和几何。实验证明了我们的框架学习刚性和凸物体的几何和动力学，并改进了当前的跟踪框架。

    This work presents an instance-agnostic learning framework that fuses vision with dynamics to simultaneously learn shape, pose trajectories and physical properties via the use of geometry as a shared representation. Unlike many contact learning approaches that assume motion capture input and a known shape prior for the collision model, our proposed framework learns an object's geometric and dynamic properties from RGBD video, without requiring either category-level or instance-level shape priors. We integrate a vision system, BundleSDF, with a dynamics system, ContactNets and propose a cyclic training pipeline to use the output from the dynamics module to refine the poses and the geometry from the vision module, using perspective reprojection. Experiments demonstrate our framework's ability to learn the geometry and dynamics of rigid and convex objects and improve upon the current tracking framework.
    
[^79]: 研究使用可穿戴传感器解决工作场所安全问题中，基于实验室承载数据训练的机器学习模型的准确性

    Studying Accuracy of Machine Learning Models Trained on Lab Lifting Data in Solving Real-World Problems Using Wearable Sensors for Workplace Safety. (arXiv:2309.05831v1 [cs.LG])

    [http://arxiv.org/abs/2309.05831](http://arxiv.org/abs/2309.05831)

    本文研究了将实验室训练的机器学习模型应用于真实世界时的准确性问题，并提出了四种潜在解决方案。

    

    将实验室数据训练的机器学习模型应用于现实世界一直以来都是一个挑战。本文讨论了将实验室训练的举重识别模型移植到现实世界的问题。与训练数据相比，模型的性能要低得多，我们探讨了失败的原因，并提出了四种潜在解决方案来提高模型性能。

    Porting ML models trained on lab data to real-world situations has long been a challenge. This paper discusses porting a lab-trained lifting identification model to the real-world. With performance much lower than on training data, we explored causes of the failure and proposed four potential solutions to increase model performance
    
[^80]: 探索几何深度学习在降水预测中的应用

    Exploring Geometric Deep Learning For Precipitation Nowcasting. (arXiv:2309.05828v1 [cs.LG])

    [http://arxiv.org/abs/2309.05828](http://arxiv.org/abs/2309.05828)

    本论文探索了几何深度学习在降水预测中的应用，采用了基于时间的图卷积网络（GCN）模型，该模型能够更好地捕捉地理网格之间的动态空间关系。

    

    降水预测（几小时内）仍然是一项挑战，因为需要准确捕捉复杂的局部相互作用。卷积神经网络依赖于卷积核与网格数据进行卷积，并且提取的特征受限于有限的感知域，通常表现为与真实数据相比过度平滑的输出。因此，它们缺乏对网格之间复杂空间关系的建模能力。几何深度学习旨在将神经网络模型推广到非欧几里德域。这些模型在定义节点和边缘方面更加灵活，并且能够有效地捕捉地理网格之间的动态空间关系。受此启发，我们探索了一种基于几何深度学习的时间图卷积网络（GCN）应用于降水预测。自动学习描述网格单元之间相互作用的邻接矩阵，通过最小化预测值与真实像素值之间的L1损失来实现。

    Precipitation nowcasting (up to a few hours) remains a challenge due to the highly complex local interactions that need to be captured accurately. Convolutional Neural Networks rely on convolutional kernels convolving with grid data and the extracted features are trapped by limited receptive field, typically expressed in excessively smooth output compared to ground truth. Thus they lack the capacity to model complex spatial relationships among the grids. Geometric deep learning aims to generalize neural network models to non-Euclidean domains. Such models are more flexible in defining nodes and edges and can effectively capture dynamic spatial relationship among geographical grids. Motivated by this, we explore a geometric deep learning-based temporal Graph Convolutional Network (GCN) for precipitation nowcasting. The adjacency matrix that simulates the interactions among grid cells is learned automatically by minimizing the L1 loss between prediction and ground truth pixel value durin
    
[^81]: KD-FixMatch: 知识蒸馏的孪生神经网络

    KD-FixMatch: Knowledge Distillation Siamese Neural Networks. (arXiv:2309.05826v1 [cs.LG])

    [http://arxiv.org/abs/2309.05826](http://arxiv.org/abs/2309.05826)

    KD-FixMatch是一种半监督学习算法，在FixMatch的基础上引入了知识蒸馏，通过顺序和并行训练SNNs的组合来提高性能并降低性能下降。

    

    半监督学习（SSL）作为解决有限标注数据挑战的一种方法，在深度学习中变得至关重要。深度神经网络的成功严重依赖于大规模高质量标注数据的可用性。然而，数据标注的过程耗时且不可扩展，导致标注数据不足。SSL旨在通过利用额外的未标注数据来解决这个问题。FixMatch是一种流行的SSL算法，通过使用孪生神经网络（SNN）同时训练相同权重共享的教师和学生网络。然而，在早期训练阶段，如果伪标签存在较大噪声，该算法容易导致性能下降。我们提出了KD-FixMatch，一种新颖的SSL算法，通过引入知识蒸馏来解决FixMatch的局限性。该算法利用顺序和并行训练SNNs的组合来提高性能并降低性能下降。

    Semi-supervised learning (SSL) has become a crucial approach in deep learning as a way to address the challenge of limited labeled data. The success of deep neural networks heavily relies on the availability of large-scale high-quality labeled data. However, the process of data labeling is time-consuming and unscalable, leading to shortages in labeled data. SSL aims to tackle this problem by leveraging additional unlabeled data in the training process. One of the popular SSL algorithms, FixMatch, trains identical weight-sharing teacher and student networks simultaneously using a siamese neural network (SNN). However, it is prone to performance degradation when the pseudo labels are heavily noisy in the early training stage. We present KD-FixMatch, a novel SSL algorithm that addresses the limitations of FixMatch by incorporating knowledge distillation. The algorithm utilizes a combination of sequential and simultaneous training of SNNs to enhance performance and reduce performance degra
    
[^82]: 基于集成模型的抽象化方法用于现代自我优化系统

    Ensemble-based modeling abstractions for modern self-optimizing systems. (arXiv:2309.05823v1 [cs.LG])

    [http://arxiv.org/abs/2309.05823](http://arxiv.org/abs/2309.05823)

    本文在基于集成的组件模型DEECo的基础上，扩展了机器学习和优化启发式方法以建立和重新配置自主组件集。这种方法对于现代智能系统来说是一个关键特性，能够在学习过程中优化行为，并在运行时处理环境中的不确定性。

    

    在本文中，我们扩展了基于集成的组件模型DEECo，使其具有使用机器学习和优化启发式方法建立和重新配置自主组件集的能力。我们展示了如何在模型层次上捕捉这些概念，并举例说明了在工业4.0环境中，这样的模型如何有益地用于建模访问控制相关问题。我们认为，将机器学习和优化启发式方法纳入现代智能系统是一个关键特性，这些系统需要在学习过程中并在运行时优化其行为，以应对环境中的不确定性。

    In this paper, we extend our ensemble-based component model DEECo with the capability to use machine-learning and optimization heuristics in establishing and reconfiguration of autonomic component ensembles. We show how to capture these concepts on the model level and give an example of how such a model can be beneficially used for modeling access-control related problem in the Industry 4.0 settings. We argue that incorporating machine-learning and optimization heuristics is a key feature for modern smart systems which are to learn over the time and optimize their behavior at runtime to deal with uncertainty in their environment.
    
[^83]: 可解释多尺度系统有效动力学学习

    Interpretable learning of effective dynamics for multiscale systems. (arXiv:2309.05812v1 [stat.ML])

    [http://arxiv.org/abs/2309.05812](http://arxiv.org/abs/2309.05812)

    该论文提出了一种新的可解释学习有效动力学（iLED）框架，它通过引入深度循环神经网络技术，在保持准确性的同时提供了可解释性，解决了现有神经网络在复杂系统中应用受限的问题。

    

    高维多尺度系统的建模和仿真是科学和工程领域面临的重要挑战。尽管现今的计算机技术不断进步，解决由控制方程描述的所有时空尺度仍然是一个遥不可及的目标。这种认识促使人们大力发展模型降阶技术。近年来，基于深度循环神经网络的技术在复杂时空系统的建模和仿真方面取得了令人鼓舞的成果，并且具有模型开发的灵活性，因为它们可以结合实验和计算数据。然而，神经网络缺乏可解释性，限制了它们在复杂系统中的实用性和普适性。在这里，我们提出了一种新的可解释学习有效动力学（iLED）框架，它具有与基于循环神经网络的最新方法相当的准确性，并提供了额外的好处。

    The modeling and simulation of high-dimensional multiscale systems is a critical challenge across all areas of science and engineering. It is broadly believed that even with today's computer advances resolving all spatiotemporal scales described by the governing equations remains a remote target. This realization has prompted intense efforts to develop model order reduction techniques. In recent years, techniques based on deep recurrent neural networks have produced promising results for the modeling and simulation of complex spatiotemporal systems and offer large flexibility in model development as they can incorporate experimental and computational data. However, neural networks lack interpretability, which limits their utility and generalizability across complex systems. Here we propose a novel framework of Interpretable Learning Effective Dynamics (iLED) that offers comparable accuracy to state-of-the-art recurrent neural network-based approaches while providing the added benefit o
    
[^84]: 使用去噪扩散概率模型预测分子云的辐射场

    Predicting the Radiation Field of Molecular Clouds using Denoising Diffusion Probabilistic Models. (arXiv:2309.05811v1 [astro-ph.GA])

    [http://arxiv.org/abs/2309.05811](http://arxiv.org/abs/2309.05811)

    使用去噪扩散概率模型（DDPMs）预测分子云的辐射场强度，通过合成尘埃发射图来估计星际辐射场（ISRF），并且在不同物理参数的新模拟中展现出一致的预测结果。

    

    准确量化辐射反馈对恒星形成的影响是具有挑战性的。为了解决这个复杂的问题，我们利用深度学习技术，即去噪扩散概率模型（DDPMs），根据4.5um、24um和250um三频段的尘埃发射来预测星际辐射场（ISRF）强度。我们采用了STARFORGE（星际环境中的恒星形成）项目中的磁流体力学模拟来模拟恒星形成和巨大分子云（GMC）的演化。我们生成了与Monoceros R2（MonR2）GMC观测到的谱能分布相匹配的合成尘埃发射图。我们训练DDPMs来使用合成的三频段尘埃发射来估计ISRF。预测值与真实值之间的离散度在测试集中在0.1倍以内。我们将扩散模型的评估扩展到包含不同物理参数的新模拟中。在这些超出分布范围的模拟中观察到了一致的偏移。

    Accurately quantifying the impact of radiation feedback in star formation is challenging. To address this complex problem, we employ deep learning techniques, denoising diffusion probabilistic models (DDPMs), to predict the interstellar radiation field (ISRF) strength based on three-band dust emission at 4.5 \um, 24 \um, and 250 \um. We adopt magnetohydrodynamic simulations from the STARFORGE (STAR FORmation in Gaseous Environments) project that model star formation and giant molecular cloud (GMC) evolution. We generate synthetic dust emission maps matching observed spectral energy distributions in the Monoceros R2 (MonR2) GMC. We train DDPMs to estimate the ISRF using synthetic three-band dust emission. The dispersion between the predictions and true values is within a factor of 0.1 for the test set. We extended our assessment of the diffusion model to include new simulations with varying physical parameters. While there is a consistent offset observed in these out-of-distribution sim
    
[^85]: SHIFT3D: 合成困惑三维检测器的强输入

    SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors. (arXiv:2309.05810v1 [cs.CV])

    [http://arxiv.org/abs/2309.05810](http://arxiv.org/abs/2309.05810)

    我们提出了SHIFT3D，这是一个可微分的方法，用于生成具有挑战性的三维形状以欺骗三维物体检测器，并且具有预防性发现潜在安全风险的潜力。

    

    我们提出了SHIFT3D，一个可微分的流水线，用于生成在结构上合理且具有挑战性的三维形状，以欺骗三维物体检测器。在自动驾驶等安全关键应用中，发现这种新颖的具有挑战性的物体可以揭示三维检测器的未知漏洞。通过使用有符号距离函数（SDF）来表示物体，我们展示了梯度误差信号使我们能够平滑地改变三维物体的形状或姿态，以困惑下游的三维检测器。重要的是，SHIFT3D生成的物体在物理上与基准物体不同，但保留了语义可识别的形状。我们的方法为现代三维物体检测器提供了可解释的故障模式，并且可以在这些风险变成严重故障之前，有助于预防性地发现潜在的安全风险。

    We present SHIFT3D, a differentiable pipeline for generating 3D shapes that are structurally plausible yet challenging to 3D object detectors. In safety-critical applications like autonomous driving, discovering such novel challenging objects can offer insight into unknown vulnerabilities of 3D detectors. By representing objects with a signed distanced function (SDF), we show that gradient error signals allow us to smoothly deform the shape or pose of a 3D object in order to confuse a downstream 3D detector. Importantly, the objects generated by SHIFT3D physically differ from the baseline object yet retain a semantically recognizable shape. Our approach provides interpretable failure modes for modern 3D object detectors, and can aid in preemptive discovery of potential safety risks within 3D perception systems before these risks become critical failures.
    
[^86]: 深度神经网络与人类在颜色感知方面的差异研究

    Divergences in Color Perception between Deep Neural Networks and Humans. (arXiv:2309.05809v1 [cs.CV])

    [http://arxiv.org/abs/2309.05809](http://arxiv.org/abs/2309.05809)

    本研究通过对比深度神经网络(DNNs)和人类的颜色感知差异，发现最先进的DNN结构在颜色相似性判断方面与人类存在显著差异。这对于深入理解DNN在人类视觉方面的模拟能力具有重要意义。

    

    深度神经网络(DNNs)被越来越多地提出作为人类视觉模型，这得益于它们在图像分类和物体识别任务中的出色性能。然而，DNNs能否捕捉到人类视觉中诸如颜色感知等基本方面仍然不清楚。在这里，我们开展了新颖的实验证明了DNN中颜色嵌入的感知一致性，并评估了这些算法对通过在线调查收集的人类颜色相似性判断的预测能力。我们发现，包括卷积神经网络和视觉变换器在内的最先进DNN结构在对控制颜色特性的图像、在线搜索生成的图像以及经典的CIFAR-10数据集中真实世界图像的颜色相似性判断方面与人类颜色判断显著不同。我们将DNN性能与基于小波分解的可解释且符合认知的颜色感知模型进行了比较。

    Deep neural networks (DNNs) are increasingly proposed as models of human vision, bolstered by their impressive performance on image classification and object recognition tasks. Yet, the extent to which DNNs capture fundamental aspects of human vision such as color perception remains unclear. Here, we develop novel experiments for evaluating the perceptual coherence of color embeddings in DNNs, and we assess how well these algorithms predict human color similarity judgments collected via an online survey. We find that state-of-the-art DNN architectures $-$ including convolutional neural networks and vision transformers $-$ provide color similarity judgments that strikingly diverge from human color judgments of (i) images with controlled color properties, (ii) images generated from online searches, and (iii) real-world images from the canonical CIFAR-10 dataset. We compare DNN performance against an interpretable and cognitively plausible model of color perception based on wavelet decomp
    
[^87]: 在陷阱面前的在线机器学习自适应

    Online ML Self-adaptation in Face of Traps. (arXiv:2309.05805v1 [cs.LG])

    [http://arxiv.org/abs/2309.05805](http://arxiv.org/abs/2309.05805)

    这篇论文报告了将在线机器学习应用于自适应中的经验，讨论了涉及规范、在线训练和评估的陷阱，并给出了一系列的经验教训，可以为其他研究人员和实践者提供指导。

    

    在自适应系统中，在线机器学习（ML）经常被用来加强适应机制，提高系统效用。尽管有这些好处，应用在线ML进行自适应可能具有挑战性，而且很少有论文报告其限制。最近，我们尝试将在线ML应用于智能农业场景的自适应中，并且我们面临了一些意外的困难 - 陷阱 - 这在我们的了解中在社区中没有得到足够的讨论。在本文中，我们报告了我们在这些陷阱中的经验。具体而言，我们讨论了与基于ML的估计器的规范和在线训练有关的几个陷阱，它们对自适应的影响以及用于评估估计器的方法。我们对这些陷阱的概述提供了一系列的经验教训，可以为其他研究人员和实践者在应用在线ML进行自适应时提供指导。

    Online machine learning (ML) is often used in self-adaptive systems to strengthen the adaptation mechanism and improve the system utility. Despite such benefits, applying online ML for self-adaptation can be challenging, and not many papers report its limitations. Recently, we experimented with applying online ML for self-adaptation of a smart farming scenario and we had faced several unexpected difficulties -- traps -- that, to our knowledge, are not discussed enough in the community. In this paper, we report our experience with these traps. Specifically, we discuss several traps that relate to the specification and online training of the ML-based estimators, their impact on self-adaptation, and the approach used to evaluate the estimators. Our overview of these traps provides a list of lessons learned, which can serve as guidance for other researchers and practitioners when applying online ML for self-adaptation.
    
[^88]: 重访能量模型作为策略：评级噪声对比估计和插值能量模型

    Revisiting Energy Based Models as Policies: Ranking Noise Contrastive Estimation and Interpolating Energy Models. (arXiv:2309.05803v1 [cs.RO])

    [http://arxiv.org/abs/2309.05803](http://arxiv.org/abs/2309.05803)

    这项工作提出了一种实用的训练目标和算法，用于能量模型，证明了现有的普遍传言是错误的。同时，定义了一个能够在高维连续空间中有效训练的指标函数。

    

    任何机器人学习流程的关键设计决策是选择策略表示形式：应该使用什么类型的模型来生成下一步的机器人动作？由于许多机器人任务具有固有的多模态特性，并且结合了生成模型的最新成功，研究人员已经转向使用最先进的概率模型，如扩散模型进行策略表示。在这项工作中，我们重访了将能量模型（EBM）作为一种策略类别的选择。我们证明了现有的普遍传言——高维连续空间中的能量模型不容易训练是错误的。我们开发了一种实用的训练目标和算法，用于能量模型，其中结合了几个关键要素：(i) 评级噪声对比估计 (R-NCE)，(ii) 可学习的负采样器，以及 (iii) 非对抗性联合训练。我们证明了我们提出的目标函数是渐近一致的，并量化了其极限方差。

    A crucial design decision for any robot learning pipeline is the choice of policy representation: what type of model should be used to generate the next set of robot actions? Owing to the inherent multi-modal nature of many robotic tasks, combined with the recent successes in generative modeling, researchers have turned to state-of-the-art probabilistic models such as diffusion models for policy representation. In this work, we revisit the choice of energy-based models (EBM) as a policy class. We show that the prevailing folklore -- that energy models in high dimensional continuous spaces are impractical to train -is false. We develop a practical training objective and algorithm for energy models which combines several key ingredients: (i) ranking noise contrastive estimation (R-NCE), (ii) learnable negative samplers, and (iii) non-adversarial joint training. We prove that our proposed objective function is asymptotically consistent and quantify its limiting variance. On the other ha
    
[^89]: 增强上下文感知自监督学习的超边预测

    Enhancing Hyperedge Prediction with Context-Aware Self-Supervised Learning. (arXiv:2309.05798v1 [cs.LG])

    [http://arxiv.org/abs/2309.05798](http://arxiv.org/abs/2309.05798)

    该论文提出了一种增强超边预测的方法，通过上下文感知的节点聚合和自监督对比学习来解决超边预测中的问题。这种方法可以准确捕捉节点之间的复杂关系，并缓解数据稀疏问题。

    

    超图可以自然地建模群组关系（例如，一组共同购买物品的用户），hyperedge预测是预测未来或未观察到的超边的任务，在许多实际应用中都非常重要。然而，目前的研究中很少探讨以下挑战：（C1）如何聚合每个超边候选中的节点以准确预测超边？（C2）如何缓解超边预测中固有的数据稀疏问题？为了同时解决这两个挑战，本文提出了一种新颖的超边预测框架CASH，它采用了（1）上下文感知节点聚合，精确捕捉每个超边中节点之间的复杂关系，用于解决挑战（C1），以及（2）自监督对比学习在超边预测上下文中增强超图表示，以应对挑战（C2）。此外，针对挑战（C2），我们提出了超边感知的数据增强方法。

    Hypergraphs can naturally model group-wise relations (e.g., a group of users who co-purchase an item) as hyperedges. Hyperedge prediction is to predict future or unobserved hyperedges, which is a fundamental task in many real-world applications (e.g., group recommendation). Despite the recent breakthrough of hyperedge prediction methods, the following challenges have been rarely studied: (C1) How to aggregate the nodes in each hyperedge candidate for accurate hyperedge prediction? and (C2) How to mitigate the inherent data sparsity problem in hyperedge prediction? To tackle both challenges together, in this paper, we propose a novel hyperedge prediction framework (CASH) that employs (1) context-aware node aggregation to precisely capture complex relations among nodes in each hyperedge for (C1) and (2) self-supervised contrastive learning in the context of hyperedge prediction to enhance hypergraph representations for (C2). Furthermore, as for (C2), we propose a hyperedge-aware augmenta
    
[^90]: 关于反转生成模型的细粒度难度

    On the Fine-Grained Hardness of Inverting Generative Models. (arXiv:2309.05795v1 [stat.ML])

    [http://arxiv.org/abs/2309.05795](http://arxiv.org/abs/2309.05795)

    本文提供了反转生成模型的计算难度的细粒度视图，建立了对精确和近似模型反转的新的难度下界。

    

    生成模型反转的目标是识别一个大小为$n$的潜在向量，该向量能够产生与给定目标密切匹配的生成模型输出。这个问题在涉及计算机视觉和自然语言处理的许多现代应用中是核心的计算原语。然而，该问题在最坏情况下被认为是计算上具有挑战性和NP难解的。本文旨在提供对这个问题的计算难度的细粒度视图。我们针对精确和近似模型反转建立了几个新的难度下界。在精确反转中，目标是确定一个目标是否包含在给定生成模型的范围内。在强指数时间假设（SETH）下，我们通过从$k$-SAT的约简来证明，精确反转的计算复杂度下界为$\Omega(2^n)$；这是已知结果的加强版。对于更具实际意义的近似反转问题，目标是寻找一个潜在向量，使得生成模型输出与给定目标尽可能接近。在这种情况下，我们证明了存在一个常数$\gamma$，使得除非$\text{NP}\subseteq \text{BPTIME}(2^{O(n^\gamma)})$，否则近似反转问题的计算复杂度无法突破$\Omega(2^{n/2})$。

    The objective of generative model inversion is to identify a size-$n$ latent vector that produces a generative model output that closely matches a given target. This operation is a core computational primitive in numerous modern applications involving computer vision and NLP. However, the problem is known to be computationally challenging and NP-hard in the worst case. This paper aims to provide a fine-grained view of the landscape of computational hardness for this problem. We establish several new hardness lower bounds for both exact and approximate model inversion. In exact inversion, the goal is to determine whether a target is contained within the range of a given generative model. Under the strong exponential time hypothesis (SETH), we demonstrate that the computational complexity of exact inversion is lower bounded by $\Omega(2^n)$ via a reduction from $k$-SAT; this is a strengthening of known results. For the more practically relevant problem of approximate inversion, the goal 
    
[^91]: 自适应用户中心的神经符号学习在多模态交互中的应用于自主系统

    Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems. (arXiv:2309.05787v1 [cs.AI])

    [http://arxiv.org/abs/2309.05787](http://arxiv.org/abs/2309.05787)

    这篇论文提出了一种自适应用户中心的神经符号学习方法，用于支持多模态交互中自主系统对物体和环境的符号化理解能力的提升。

    

    最近在机器学习，特别是深度学习方面取得的进展使得自主系统能够以感知的非符号化方式识别和理解对象及其环境。这些系统现在可以执行物体检测、传感器数据融合和语言理解任务。然而，要提升这些系统对对象及其环境的概念和符号理解能力，需要综合考虑人类提供的显性教导（例如描述情况或解释如何行动）和通过观察人类行为（通过系统的传感器）获得的隐性教导。因此，系统必须设计具有多模态输入和输出能力，以支持隐性和显性交互模型。在这篇文章中，我们主张同时考虑这两种类型的输入，以及人在循环中的参与和增量学习技术，以推进这一强大的人工智能水平的实现。

    Recent advances in machine learning, particularly deep learning, have enabled autonomous systems to perceive and comprehend objects and their environments in a perceptual subsymbolic manner. These systems can now perform object detection, sensor data fusion, and language understanding tasks. However, there is a growing need to enhance these systems to understand objects and their environments more conceptually and symbolically. It is essential to consider both the explicit teaching provided by humans (e.g., describing a situation or explaining how to act) and the implicit teaching obtained by observing human behavior (e.g., through the system's sensors) to achieve this level of powerful artificial intelligence. Thus, the system must be designed with multimodal input and output capabilities to support implicit and explicit interaction models. In this position paper, we argue for considering both types of inputs, as well as human-in-the-loop and incremental learning techniques, for advan
    
[^92]: 辅助生活环境中传感器布置的灰盒贝叶斯优化

    Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments. (arXiv:2309.05784v1 [cs.LG])

    [http://arxiv.org/abs/2309.05784](http://arxiv.org/abs/2309.05784)

    本研究提出了一种基于灰盒贝叶斯优化和仿真评估的方法，通过捕捉关于活动的空间分布的领域特定知识，找到高质量的传感器布置，并在跌倒检测、室内定位和活动识别等领域取得了优于黑盒优化技术的性能。

    

    优化传感器的配置和布置对于可靠的跌倒检测、室内定位和活动识别在辅助生活空间中至关重要。我们提出了一种新颖的、样本高效的方法，基于灰盒贝叶斯优化和基于仿真的评估，在任意室内空间中找到高质量的传感器布置。我们的关键技术贡献在于捕捉关于活动的空间分布的领域特定知识，并将其纳入贝叶斯优化的迭代查询点选择中。通过考虑两个模拟室内环境和包含人类活动和传感器触发的真实数据集，我们表明我们提出的方法在识别高质量传感器布置方面比现有的黑盒优化技术表现更好，在F1得分方面实现了准确的活动识别，同时还需要大幅度减少(平均减少51.3%)昂贵的函数次数。

    Optimizing the configuration and placement of sensors is crucial for reliable fall detection, indoor localization, and activity recognition in assisted living spaces. We propose a novel, sample-efficient approach to find a high-quality sensor placement in an arbitrary indoor space based on grey-box Bayesian optimization and simulation-based evaluation. Our key technical contribution lies in capturing domain-specific knowledge about the spatial distribution of activities and incorporating it into the iterative selection of query points in Bayesian optimization. Considering two simulated indoor environments and a real-world dataset containing human activities and sensor triggers, we show that our proposed method performs better compared to state-of-the-art black-box optimization techniques in identifying high-quality sensor placements, leading to accurate activity recognition in terms of F1-score, while also requiring a significantly lower (51.3% on average) number of expensive function 
    
[^93]: 智能手表派生的声学标记用于认知相关日常功能的缺陷

    Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday Functioning. (arXiv:2309.05777v1 [eess.AS])

    [http://arxiv.org/abs/2309.05777](http://arxiv.org/abs/2309.05777)

    该研究探讨了使用智能手表收集声学特征作为检测认知缺陷引起的日常功能缺陷的客观标记的可行性。机器学习模型使用声学特征可以以高准确率检测到存在日常功能缺陷的个体。

    

    检测由认知缺陷引起的日常功能微妙缺陷对早期发现神经退行性疾病，尤其是阿尔茨海默病，非常重要。然而，当前日常功能评估的标准基于定性的主观评级。已经证明，语音提供了良好的客观标记来评估认知缺陷，但与认知相关的日常功能的关联尚未得到研究。在这项研究中，我们展示了利用基于智能手表的应用程序收集声学特征作为检测日常功能缺陷的客观标记的可行性。我们从54名老年人中收集了在执行认知任务和日常对话中的语音数据，作为可能的应用场景，并对日常功能进行了测量。使用声学特征的机器学习模型可以以高达77.8%的准确率检测到日常功能缺陷的个体，这比当前标准评估方法要高得多。

    Detection of subtle deficits in everyday functioning due to cognitive impairment is important for early detection of neurodegenerative diseases, particularly Alzheimer's disease. However, current standards for assessment of everyday functioning are based on qualitative, subjective ratings. Speech has been shown to provide good objective markers for cognitive impairments, but the association with cognition-relevant everyday functioning remains uninvestigated. In this study, we demonstrate the feasibility of using a smartwatch-based application to collect acoustic features as objective markers for detecting deficits in everyday functioning. We collected voice data during the performance of cognitive tasks and daily conversation, as possible application scenarios, from 54 older adults, along with a measure of everyday functioning. Machine learning models using acoustic features could detect individuals with deficits in everyday functioning with up to 77.8% accuracy, which was higher than 
    
[^94]: 内在维度对压缩下的度量学习的影响

    The Effect of Intrinsic Dimension on Metric Learning under Compression. (arXiv:2309.05751v1 [cs.LG])

    [http://arxiv.org/abs/2309.05751](http://arxiv.org/abs/2309.05751)

    本论文研究了内在维度对压缩下的度量学习的影响，提出了在对数据进行随机压缩后在低维空间内训练全秩度量的方法。理论保证了在不依赖环境维度的情况下，度量学习的误差可以被控制，并且在存在良性几何结构时效果更好。

    

    度量学习旨在在输入空间中找到适当的距离度量，以改善基于距离的学习算法的性能。在高维环境中，度量学习还可以作为降维的手段，通过对学习的度量施加一个低秩约束。本文中，我们考虑的是对数据的一个随机压缩版本，然后在其中训练一个全秩的度量。我们给出了关于距离度量学习的误差的理论保证，这些保证不依赖于环境维度。我们的边界除了对来自有界支持的独立同分布数据没有显式的假设之外，并且在存在良性几何结构时自动收敛。在合成和真实数据集上的实验结果支持我们在高维环境中的理论发现。

    Metric learning aims at finding a suitable distance metric over the input space, to improve the performance of distance-based learning algorithms. In high-dimensional settings, metric learning can also play the role of dimensionality reduction, by imposing a low-rank restriction to the learnt metric. In this paper, instead of training a low-rank metric on high-dimensional data, we consider a randomly compressed version of the data, and train a full-rank metric there. We give theoretical guarantees on the error of distance-based metric learning, with respect to the random compression, which do not depend on the ambient dimension. Our bounds do not make any explicit assumptions, aside from i.i.d. data from a bounded support, and automatically tighten when benign geometrical structures are present. Experimental results on both synthetic and real data sets support our theoretical findings in high-dimensional settings.
    
[^95]: CaloClouds II: 超快速几何独立的高分辨率量能器模拟

    CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation. (arXiv:2309.05704v1 [physics.ins-det])

    [http://arxiv.org/abs/2309.05704](http://arxiv.org/abs/2309.05704)

    CaloClouds II是一个超快速几何独立的高分辨率量能器模拟，通过连续时间得分建模，相比传统模拟更快且具有可比的保真度。

    

    高分辨率探测器的能量沉积的快速模拟对于未来具有不断增加亮度的对撞机实验至关重要。生成式机器学习（ML）模型已经被证明可以加快和增强物理分析中的传统模拟链。然而，大多数以前的努力局限于依赖于固定、规则的探测器读出几何的模型。最近提出的CaloClouds模型是一个几何独立的扩散模型，为设想中的国际大型探测器（ILD）的电磁量能器生成块状点云。在这项工作中，我们介绍了CaloClouds II，它具有一些关键的改进。这包括基于连续时间得分的建模，它允许进行25步采样，与CaloClouds的保真度相当，同时在单个CPU上比Geant4快6倍（比CaloClouds快5倍）。我们进一步将扩散模型提炼成一种新的模型。

    Fast simulation of the energy depositions in high-granular detectors is needed for future collider experiments with ever increasing luminosities. Generative machine learning (ML) models have been shown to speed up and augment the traditional simulation chain in physics analysis. However, the majority of previous efforts were limited to models relying on fixed, regular detector readout geometries. A major advancement is the recently introduced CaloClouds model, a geometry-independent diffusion model, which generates calorimeter showers as point clouds for the electromagnetic calorimeter of the envisioned International Large Detector (ILD).  In this work, we introduce CaloClouds II which features a number of key improvements. This includes continuous time score-based modelling, which allows for a 25 step sampling with comparable fidelity to CaloClouds while yielding a $6\times$ speed-up over Geant4 on a single CPU ($5\times$ over CaloClouds). We further distill the diffusion model into a
    
[^96]: 无监督机器学习技术用于探索热带共平面，膜图和Seiberg对偶

    Unsupervised Machine Learning Techniques for Exploring Tropical Coamoeba, Brane Tilings and Seiberg Duality. (arXiv:2309.05702v1 [hep-th])

    [http://arxiv.org/abs/2309.05702](http://arxiv.org/abs/2309.05702)

    我们利用无监督机器学习技术，并采用主成分分析（PCA）和t-分布随机邻域嵌入（t-SNE）等技术，成功地将标记的共平面空间投影到具有相界限的低维相空间中，以识别与相同泛型Calabi-Yau 3折对应的4维N=1超对称规范理论的泛型相。

    

    我们引入无监督机器学习技术，以识别与相同的泛型Calabi-Yau 3折对应的4维N=1超对称规范理论的泛型相。这些4维N=1超对称规范理论是探测泛型Calabi-Yau 3折的D3-膜的世界体理论，并且以称为膜图的Type IIB膜配置形式实现。它对应于与泛型Calabi-Yau 3折关联的镜面曲线的共平面投影的骨架图。当我们改变镜面Calabi-Yau 3折的复结构模量时，共平面和相应的膜图会改变其形状，产生由Seiberg对偶相联系的不同泛型相。我们证明通过使用主成分分析（PCA）和t-分布随机邻域嵌入（t-SNE）等技术，我们可以将由复结构模量标记的共平面空间投影到具有相界限的低维相空间中。

    We introduce unsupervised machine learning techniques in order to identify toric phases of 4d N=1 supersymmetric gauge theories corresponding to the same toric Calabi-Yau 3-fold. These 4d N=1 supersymmetric gauge theories are worldvolume theories of a D3-brane probing a toric Calabi-Yau 3-fold and are realized in terms of a Type IIB brane configuration known as a brane tiling. It corresponds to the skeleton graph of the coamoeba projection of the mirror curve associated to the toric Calabi-Yau 3-fold. When we vary the complex structure moduli of the mirror Calabi-Yau 3-fold, the coamoeba and the corresponding brane tilings change their shape, giving rise to different toric phases related by Seiberg duality. We illustrate that by employing techniques such as principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE), we can project the space of coamoeba labelled by complex structure moduli down to a lower dimensional phase space with phase boundaries corr
    
[^97]: 时间耐心：用于嵌入式雷达数据处理的高效自适应深度学习

    Temporal Patience: Efficient Adaptive Deep Learning for Embedded Radar Data Processing. (arXiv:2309.05686v1 [cs.LG])

    [http://arxiv.org/abs/2309.05686](http://arxiv.org/abs/2309.05686)

    该论文提出了一种利用流式雷达数据的时间相关性来增强嵌入式设备上深度学习推理效率的新技术。通过在网络中添加额外的分类器分支，实现根据运行时决策机制提前终止推理，从而降低计算成本而保持最小的准确度损失。实验结果表明，该技术能够比单路退出网络和基于置信度的早期退出版本节省高达26%和12%的推理操作。这些技术适用于通用硬件并可以结合使用。

    

    雷达传感器为常开智能设备提供了功耗高效的解决方案，但在资源受限的嵌入式平台上处理数据流仍然具有挑战性。本文介绍了一种利用流式雷达数据中的时间相关性来增强嵌入式设备上深度学习推理的早期退出神经网络的效率的新技术。这些网络在架构的隐藏层之间添加了额外的分类器分支，允许根据运行时决策机制的结果，在推理过程中提前终止。我们的方法使得在何时终止推理有更明智的决策，从而降低计算成本同时保持最小的准确度损失。我们的结果表明，与单路退出网络相比，我们的技术每次推理节省了高达26%的操作，与基于置信度的早期退出版本相比节省了12%。我们提出的技术适用于通用硬件并可以结合使用。

    Radar sensors offer power-efficient solutions for always-on smart devices, but processing the data streams on resource-constrained embedded platforms remains challenging. This paper presents novel techniques that leverage the temporal correlation present in streaming radar data to enhance the efficiency of Early Exit Neural Networks for Deep Learning inference on embedded devices. These networks add additional classifier branches between the architecture's hidden layers that allow for an early termination of the inference if their result is deemed sufficient enough by an at-runtime decision mechanism. Our methods enable more informed decisions on when to terminate the inference, reducing computational costs while maintaining a minimal loss of accuracy.  Our results demonstrate that our techniques save up to 26% of operations per inference over a Single Exit Network and 12% over a confidence-based Early Exit version. Our proposed techniques work on commodity hardware and can be combined
    
[^98]: EANet: 专家注意力网络用于在线轨迹预测

    EANet: Expert Attention Network for Online Trajectory Prediction. (arXiv:2309.05683v1 [cs.LG])

    [http://arxiv.org/abs/2309.05683](http://arxiv.org/abs/2309.05683)

    本论文提出了一个专家注意力网络用于在线轨迹预测的在线学习框架。通过引入专家注意力机制，调整网络层的权重，解决了梯度问题，能够快速学习新场景的知识以提高预测准确度。

    

    轨迹预测在自动驾驶中起着至关重要的作用。现有的主流研究和基于持续学习的方法都需要在完整的数据集上训练，导致在场景突然变化时预测准确度较低，无法及时响应和更新模型。这些方法是否能够实时预测并使用数据实例立即更新模型（即在线学习环境）仍然是个问题。还需要解决由数据实例流引起的梯度爆炸或消失的问题。受到Hedge Propagation算法的启发，我们提出了一个完整的在线学习框架——专家注意力网络，用于轨迹预测。我们引入专家注意力机制，调整网络层的不同深度的权重，避免由于梯度问题导致模型更新缓慢，并能够快速学习新场景的知识以恢复预测准确度。此外，我们还提出了一种短期运动模型。

    Trajectory prediction plays a crucial role in autonomous driving. Existing mainstream research and continuoual learning-based methods all require training on complete datasets, leading to poor prediction accuracy when sudden changes in scenarios occur and failing to promptly respond and update the model. Whether these methods can make a prediction in real-time and use data instances to update the model immediately(i.e., online learning settings) remains a question. The problem of gradient explosion or vanishing caused by data instance streams also needs to be addressed. Inspired by Hedge Propagation algorithm, we propose Expert Attention Network, a complete online learning framework for trajectory prediction. We introduce expert attention, which adjusts the weights of different depths of network layers, avoiding the model updated slowly due to gradient problem and enabling fast learning of new scenario's knowledge to restore prediction accuracy. Furthermore, we propose a short-term mot
    
[^99]: 数据科学、机器学习和人工智能数据源大全

    A compendium of data sources for data science, machine learning, and artificial intelligence. (arXiv:2309.05682v1 [cs.LG])

    [http://arxiv.org/abs/2309.05682](http://arxiv.org/abs/2309.05682)

    这篇论文提供了一个跨多个领域的数据源大全，包括金融、法律、生命科学、新闻社交等，以满足数据科学家和机器学习专家的需求。

    

    数据科学、机器学习和人工智能的最新进展，如大型语言模型的出现，导致对可供这些模型处理的数据的需求不断增加。虽然数据源是应用特定的，无法列出详尽无遗的数据源列表，但一个全面而不完整的列表仍然有利于各级别的数据科学家和机器学习专家。本文的目标是提供这样一个（必然不完整的）列表，即跨多个应用领域的数据源大全或概览，包括金融和经济、法律（法律和法规）、生命科学（医学和药物发现）、新闻情绪和社交媒体、零售和电子商务、卫星图像以及航运和物流，以满足各种需求。

    Recent advances in data science, machine learning, and artificial intelligence, such as the emergence of large language models, are leading to an increasing demand for data that can be processed by such models. While data sources are application-specific, and it is impossible to produce an exhaustive list of such data sources, it seems that a comprehensive, rather than complete, list would still benefit data scientists and machine learning experts of all levels of seniority. The goal of this publication is to provide just such an (inevitably incomplete) list -- or compendium -- of data sources across multiple areas of applications, including finance and economics, legal (laws and regulations), life sciences (medicine and drug discovery), news sentiment and social media, retail and ecommerce, satellite imagery, and shipping and logistics, and sports.
    
[^100]: 基于知识的科学出版物知识图谱的细化

    Knowledge-based Refinement of Scientific Publication Knowledge Graphs. (arXiv:2309.05681v1 [cs.LG])

    [http://arxiv.org/abs/2309.05681](http://arxiv.org/abs/2309.05681)

    本论文提出了一种基于知识的方法来细化科学出版物知识图谱，通过学习概率逻辑模型并使用功能梯度提升和人类知识引导来识别作者身份。实验证明了人类知识在作者身份领域的定量和定性作用。

    

    我们将识别作者身份的问题作为一个知识图构建和细化的问题来考虑。为此，我们将这个问题建模为在人类指导（基于知识的学习）的情况下学习概率逻辑模型。具体而言，我们使用功能梯度提升学习关系回归树，并输出可解释的规则。为了引入人类知识，我们将一阶子句的形式的建议注入到树中进行细化。我们在七个作者身份领域定量和定性地展示了人类知识的有用性。

    We consider the problem of identifying authorship by posing it as a knowledge graph construction and refinement. To this effect, we model this problem as learning a probabilistic logic model in the presence of human guidance (knowledge-based learning). Specifically, we learn relational regression trees using functional gradient boosting that outputs explainable rules. To incorporate human knowledge, advice in the form of first-order clauses is injected to refine the trees. We demonstrate the usefulness of human knowledge both quantitatively and qualitatively in seven authorship domains.
    
[^101]: 外表优美但缺乏忠诚度：通过基于趋势的测试理解局部解释方法

    Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing. (arXiv:2309.05679v1 [cs.LG])

    [http://arxiv.org/abs/2309.05679](http://arxiv.org/abs/2309.05679)

    本文通过基于趋势的测试方法评估了解释模型决策的忠诚度，并提出了三种新的趋势测试方法，从实证结果来看，这些新测试方法在图像、自然语言和安全任务中可以更好地评估解释模型的忠诚度。

    

    尽管深度学习（DL）带来了巨大的成就，但人们也对DL模型的决策感到担忧，因为DL模型的高度非线性使得决策极其难以理解。因此，攻击（如对抗攻击）很容易进行，但很难检测和解释，这导致了局部解释方法的研究激增，以解释模型决策。在本文中，我们评估了解释方法的忠诚度，并发现传统的忠诚度测试遇到了随机优势问题，即随机选择效果最好，尤其是对于复杂的数据。为了进一步解决这个问题，我们提出了三种基于趋势的忠诚度测试，并从实证上证明了新的趋势测试可以比传统的图像、自然语言和安全任务的测试更好地评估忠诚度。我们实现了评估系统，并评估了十种流行的解释方法。

    While enjoying the great achievements brought by deep learning (DL), people are also worried about the decision made by DL models, since the high degree of non-linearity of DL models makes the decision extremely difficult to understand. Consequently, attacks such as adversarial attacks are easy to carry out, but difficult to detect and explain, which has led to a boom in the research on local explanation methods for explaining model decisions. In this paper, we evaluate the faithfulness of explanation methods and find that traditional tests on faithfulness encounter the random dominance problem, \ie, the random selection performs the best, especially for complex data. To further solve this problem, we propose three trend-based faithfulness tests and empirically demonstrate that the new trend tests can better assess faithfulness than traditional tests on image, natural language and security tasks. We implement the assessment system and evaluate ten popular explanation methods. Benefitin
    
[^102]: 用于比较模型空间的产品流形的Gromov-Hausdorff距离

    Gromov-Hausdorff Distances for Comparing Product Manifolds of Model Spaces. (arXiv:2309.05678v1 [cs.LG])

    [http://arxiv.org/abs/2309.05678](http://arxiv.org/abs/2309.05678)

    本论文提出了一种用于比较模型空间的产品流形的Gromov-Hausdorff距离的新方法，通过估计距离来搜索最佳的潜在几何。这为提高机器学习模型的性能提供了一种原则性的技术。

    

    最近的研究提出通过将潜在空间的几何特征与底层数据结构对齐来增强机器学习模型。研究人员建议使用具有常曲率的双曲和球面空间，或它们的组合（称为产品流形），以提高模型性能，而不仅仅依赖欧几里得空间。然而，目前没有原则性的技术来确定最佳的潜在产品流形特征，即流形组件的选择和维度。为了解决这个问题，我们引入了一种新的候选潜在几何之间的距离概念，使用度量几何中的Gromov-Hausdorff距离。我们建议使用一个图搜索空间，利用估计的Gromov-Hausdorff距离来搜索最佳的潜在几何。在这项工作中，我们重点介绍了一种计算模型空间之间的Gromov-Hausdorff距离及其计算实现的算法描述。

    Recent studies propose enhancing machine learning models by aligning the geometric characteristics of the latent space with the underlying data structure. Instead of relying solely on Euclidean space, researchers have suggested using hyperbolic and spherical spaces with constant curvature, or their combinations (known as product manifolds), to improve model performance. However, there exists no principled technique to determine the best latent product manifold signature, which refers to the choice and dimensionality of manifold components. To address this, we introduce a novel notion of distance between candidate latent geometries using the Gromov-Hausdorff distance from metric geometry. We propose using a graph search space that uses the estimated Gromov-Hausdorff distances to search for the optimal latent geometry. In this work we focus on providing a description of an algorithm to compute the Gromov-Hausdorff distance between model spaces and its computational implementation.
    
[^103]: MultiCaM-Vis: 具有大量类别的多分类模型的可视化探索

    MultiCaM-Vis: Visual Exploration of Multi-Classification Model with High Number of Classes. (arXiv:2309.05676v1 [cs.HC])

    [http://arxiv.org/abs/2309.05676](http://arxiv.org/abs/2309.05676)

    MultiCaM-Vis是一种可视化工具，能够帮助机器学习专家在大量类别的多分类模型中识别出问题的根本原因，并探索和检查类别级别的实例错误分类。

    

    针对大量类别的多分类模型的可视化探索有助于机器学习专家在学习阶段识别出问题的根本原因，如实例的错误分类。以往的大多数可视化分析解决方案只针对少数类别。本文介绍了我们的交互式可视化分析工具MultiCaM-Vis，它提供了总览+详细信息的平行坐标视图和弦图，用于探索和检查类别级别的实例错误分类。我们还介绍了一项初步用户研究的结果，共有12名参与者。

    Visual exploration of multi-classification models with large number of classes would help machine learning experts in identifying the root cause of a problem that occurs during learning phase such as miss-classification of instances. Most of the previous visual analytics solutions targeted only a few classes. In this paper, we present our interactive visual analytics tool, called MultiCaM-Vis, that provides \Emph{overview+detail} style parallel coordinate views and a Chord diagram for exploration and inspection of class-level miss-classification of instances. We also present results of a preliminary user study with 12 participants.
    
[^104]: SHAPE：一种适应样本的层次预测网络用于药物推荐

    SHAPE: A Sample-adaptive Hierarchical Prediction Network for Medication Recommendation. (arXiv:2309.05675v1 [cs.LG])

    [http://arxiv.org/abs/2309.05675](http://arxiv.org/abs/2309.05675)

    本文提出了一种名为SHAPE的适应样本的层次药物预测网络，旨在解决医疗保健中复杂多发病条件下的药物推荐问题。通过设计紧凑的内部病患就诊事件关系编码器和纵向病历编码器，我们能够获得准确的病患表示和纵向序列学习策略。

    

    在医疗保健中，针对复杂的多发病条件进行有效的药物推荐是一项关键任务。大多数现有的工作基于纵向记录预测药物，这种方法假设学习纵向序列数据的信息传输模式是稳定的，并且病患在就诊期间的医疗事件是有序的。然而，他们忽视了以下条件：1）急需一种更紧凑的内部病患就诊事件关系编码器；2）学习病人可变纵向序列的准确表示的策略是不同的。在本文中，我们提出了一种新颖的适应样本的层次药物预测网络，称为SHAPE，来解决药物推荐任务中的上述挑战。具体来说，我们设计了一个紧凑的内部病患就诊事件关系编码器，用于获得就诊级别的表示，并且发展了一个纵向病历编码器，以学习患者的纵向序列表示。

    Effectively medication recommendation with complex multimorbidity conditions is a critical task in healthcare. Most existing works predicted medications based on longitudinal records, which assumed the information transmitted patterns of learning longitudinal sequence data are stable and intra-visit medical events are serialized. However, the following conditions may have been ignored: 1) A more compact encoder for intra-relationship in the intra-visit medical event is urgent; 2) Strategies for learning accurate representations of the variable longitudinal sequences of patients are different. In this paper, we proposed a novel Sample-adaptive Hierarchical medicAtion Prediction nEtwork, termed SHAPE, to tackle the above challenges in the medication recommendation task. Specifically, we design a compact intra-visit set encoder to encode the relationship in the medical event for obtaining visit-level representation and then develop an inter-visit longitudinal encoder to learn the patient-
    
[^105]: 圈圈：对具有大量类别的多分类问题的模型间比较

    Circles: Inter-Model Comparison of Multi-Classification Problems with High Number of Classes. (arXiv:2309.05672v1 [cs.HC])

    [http://arxiv.org/abs/2309.05672](http://arxiv.org/abs/2309.05672)

    这篇论文介绍了一种名为"Circles"的交互式可视分析工具，允许在一个视图中进行对具有1K类别的多个分类模型进行模型间比较。使用同心径向线布局来解决视觉混乱的问题。

    

    机器学习的最新进展，激发了研究人员生成处理包含数百类的分类模型，比如图像数据集。然而，对高数量类别的分类模型的可视化和这些分类问题的模型间比较这两个领域在文献中并没有得到太多关注，尽管分类模型越来越多地被用来解决具有非常大类别的问题。在本文中，我们提出了我们的交互式可视分析工具，称为"Circles"，它允许在一个视图中对1K类别的多个分类模型进行视觉模型间比较。为了减少视觉混乱的棘手问题，我们选择了同心径向线布局作为我们的模型间比较任务。我们的原型展示了9个具有1K类别的模型的结果。

    The recent advancements in machine learning have motivated researchers to generate classification models dealing with hundreds of classes such as in the case of image datasets. However, visualization of classification models with high number of classes and inter-model comparison in such classification problems are two areas that have not received much attention in the literature, despite the ever-increasing use of classification models to address problems with very large class categories. In this paper, we present our interactive visual analytics tool, called Circles, that allows a visual inter-model comparison of numerous classification models with 1K classes in one view. To mitigate the tricky issue of visual clutter, we chose concentric a radial line layout for our inter-model comparison task. Our prototype shows the results of 9 models with 1K classes
    
[^106]: tSPM+：一种用于从临床数据中挖掘传递顺序模式的高性能算法

    tSPM+; a high-performance algorithm for mining transitive sequential patterns from clinical data. (arXiv:2309.05671v1 [cs.LG])

    [http://arxiv.org/abs/2309.05671](http://arxiv.org/abs/2309.05671)

    tSPM+算法是一种高性能算法，通过在时间模式中加入持续时间维度，提供了高速运行和内存消耗改进。

    

    越来越多从患者那里收集到的大型临床数据集的可用性，使得使用不同的分析算法对复杂疾病进行计算化特征化成为可能。一种从大型临床数据集中提取知识的有前途的新方法是将时间模式挖掘与机器学习工作流程相结合。然而，挖掘这些时间模式是一项计算密集型任务，并且会对存储器产生影响。目前的算法，如时间序列模式挖掘（tSPM）算法，已经提供了有希望的结果，但仍有优化的空间。在本文中，我们提出了tSPM+算法，这是tSPM算法的一种高性能实现，通过在时间模式中添加持续时间维度，增加了一种新的维度。我们展示了tSPM+算法提供了高达980倍的加速和高达48倍的内存使用改进。此外，我们提供了一个包含R包的docker容器，我们还提供了完整的实验代码。

    The increasing availability of large clinical datasets collected from patients can enable new avenues for computational characterization of complex diseases using different analytic algorithms. One of the promising new methods for extracting knowledge from large clinical datasets involves temporal pattern mining integrated with machine learning workflows. However, mining these temporal patterns is a computational intensive task and has memory repercussions. Current algorithms, such as the temporal sequence pattern mining (tSPM) algorithm, are already providing promising outcomes, but still leave room for optimization. In this paper, we present the tSPM+ algorithm, a high-performance implementation of the tSPM algorithm, which adds a new dimension by adding the duration to the temporal patterns. We show that the tSPM+ algorithm provides a speed up to factor 980 and a up to 48 fold improvement in memory consumption. Moreover, we present a docker container with an R-package, We also provi
    
[^107]: 机器人公园our学习

    Robot Parkour Learning. (arXiv:2309.05665v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2309.05665](http://arxiv.org/abs/2309.05665)

    该论文提出了一个学习基于视觉的多样化公园our技能的系统，该系统使用简单的奖励而不使用参考动作数据，通过强化学习方法生成不同的公园our技能，并将其转移到四足机器人中。

    

    公园our是一个对于机器人来说需要在复杂环境中迅速克服各种障碍的长期挑战。现有的方法通过使用参考动物数据或复杂的奖励，可以生成多样但盲目的运动技能或基于视觉的特殊技能。然而，自主公园our需要机器人学习基于视觉的、多样化的可推广技能，以感知和应对各种情况。在这项工作中，我们提出了一个系统，通过使用简单的奖励而不使用任何参考动作数据，来学习多样化的、基于视觉的公园our技能的单一端到端策略。我们开发了一种受到直接协作的强化学习方法，用于生成公园our技能，包括攀爬高障碍物、跨越大距离间隙、爬行低壁垒、穿越狭窄缝隙和奔跑。我们将这些技能提炼成一个单一基于视觉的公园our策略，并将其转移到四足机器人上，利用其自我中心视角。

    Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric 
    
[^108]: 数据效率、维度约简和广义对称信息瓶颈

    Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck. (arXiv:2309.05649v1 [cs.IT] CROSS LISTED)

    [http://arxiv.org/abs/2309.05649](http://arxiv.org/abs/2309.05649)

    广义对称信息瓶颈是一种同时压缩两个随机变量以保留信息的维度约简技术，相较于逐个压缩变量，它需要更少的数据来达到相同的误差。

    

    对称信息瓶颈（SIB）是一种维度约简技术，它是更常见的信息瓶颈的扩展，同时压缩两个随机变量以保留它们的压缩版本之间的信息。我们引入了广义对称信息瓶颈（GSIB），探索了不同功能形式的同时约简成本。然后，我们探索了同时压缩的数据集大小需求。我们通过推导涉及损失函数的统计波动的界限和均方根估计来实现这一点。我们表明，在典型情况下，与逐个压缩变量相比，同时的GSIB压缩在达到相同误差时需要更少的数据。我们认为这是一个更一般的原则的例子，即同时压缩比独立压缩输入变量更具数据效率。

    The Symmetric Information Bottleneck (SIB), an extension of the more familiar Information Bottleneck, is a dimensionality reduction technique that simultaneously compresses two random variables to preserve information between their compressed versions. We introduce the Generalized Symmetric Information Bottleneck (GSIB), which explores different functional forms of the cost of such simultaneous reduction. We then explore the dataset size requirements of such simultaneous compression. We do this by deriving bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions. We show that, in typical situations, the simultaneous GSIB compression requires qualitatively less data to achieve the same errors compared to compressing variables one at a time. We suggest that this is an example of a more general principle that simultaneous compression is more data efficient than independent compression of each of the input variables.
    
[^109]: 机器学习用于最大化单个和耦合量子忆阻器的记忆阻性

    Machine Learning for maximizing the memristivity of single and coupled quantum memristors. (arXiv:2309.05062v1 [quant-ph] CROSS LISTED)

    [http://arxiv.org/abs/2309.05062](http://arxiv.org/abs/2309.05062)

    本文使用机器学习方法来研究单个和耦合量子忆阻器的记忆阻性，并发现最大化记忆阻性可以提高两个量子忆阻器的纠缠程度，揭示了量子相关性与记忆之间的关系。这一发现增强了将量子忆阻器应用于神经形态的量子计算的潜力。

    

    我们提出了机器学习方法来表征单个和耦合量子忆阻器的记忆阻性。我们展示了最大化记忆阻性会导致两个量子忆阻器的纠缠程度较大，揭示了量子相关性与记忆之间的密切关系。我们的结果增强了将量子忆阻器作为神经形态的量子计算的关键组件的可能性。

    We propose machine learning (ML) methods to characterize the memristive properties of single and coupled quantum memristors. We show that maximizing the memristivity leads to large values in the degree of entanglement of two quantum memristors, unveiling the close relationship between quantum correlations and memory. Our results strengthen the possibility of using quantum memristors as key components of neuromorphic quantum computing.
    
[^110]: 基于知识蒸馏的数字孪生模型在异常检测中的应用

    Knowledge Distillation-Empowered Digital Twin for Anomaly Detection. (arXiv:2309.04616v1 [cs.LG])

    [http://arxiv.org/abs/2309.04616](http://arxiv.org/abs/2309.04616)

    本文提出了基于知识蒸馏的数字孪生模型KDDT，用于列车控制和管理系统（TCMS）的异常检测。KDDT利用语言模型和长短期记忆网络分别提取上下文和时间顺序特征，通过知识蒸馏丰富数据量。实验结果表明KDDT在两个数据集上取得了较高的F1分数0.931和0.91。

    

    物联网系统，在关键基础设施中越来越普及，如列车控制和管理系统（TCMS）。作为安全关键系统，确保其在操作过程中的可靠性至关重要。数字孪生（DTs）由于具有运行时监控和警告、异常预测和检测等能力而越来越受到研究关注。然而，在TCMS中构建用于异常检测的数字孪生模型需要充足的训练数据，并提取具有高质量的时间顺序和上下文特征。因此，在本文中，我们提出了一种名为KDDT的新方法，用于TCMS的异常检测。KDDT利用语言模型（LM）和长短期记忆（LSTM）网络分别提取上下文和时间顺序特征。为了增加数据量，KDDT利用知识蒸馏（KD）的方法来利用领域外数据。我们使用我们的工业合作伙伴阿尔斯通的两个数据集对KDDT进行了评估，并获得了0.931和0.91的F1分数。

    Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming ubiquitous in critical infrastructures. As safety-critical systems, ensuring their dependability during operation is crucial. Digital twins (DTs) have been increasingly studied for this purpose owing to their capability of runtime monitoring and warning, prediction and detection of anomalies, etc. However, constructing a DT for anomaly detection in TCMS necessitates sufficient training data and extracting both chronological and context features with high quality. Hence, in this paper, we propose a novel method named KDDT for TCMS anomaly detection. KDDT harnesses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To enrich data volume, KDDT benefits from out-of-domain data with knowledge distillation (KD). We evaluated KDDT with two datasets from our industry partner Alstom and obtained the F1 scores of 0.931 and 0.91
    
[^111]: 用物理信息神经网络进行最优反对角量子计算的研究

    Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation. (arXiv:2309.04434v1 [quant-ph] CROSS LISTED)

    [http://arxiv.org/abs/2309.04434](http://arxiv.org/abs/2309.04434)

    本研究提出了一种利用物理信息神经网络（PINNs）解决量子电路反对角（CD）协议优化问题的方法，通过嵌入物理信息到神经网络中，并利用最小作用量原理和厄米特性条件来获取最适当的反对角项，从而提供了一种可靠的替代方案，摆脱了以往依赖于经典数值逼近的约束。

    

    我们引入了一种新的方法，利用物理信息神经网络（PINNs）的优势来解决由$N_{Q}$比特系统组成的量子电路中的反对角（CD）协议优化的问题。主要目标是利用受物理启发的深度学习技术精确地解决量子系统中不同物理可观测量的时间演化。为了实现这个目标，我们将必要的物理信息嵌入到底层神经网络中，以有效地解决这个问题。特别地，我们对所有物理可观测量施加厄米特性条件，并利用最小作用量原理，保证基于物理学的最适当反对角项的获取。所提出的方法提供了一个可靠的替代选择来解决CD驱动问题，摆脱了以往依赖于经典数值逼近的约束。

    We introduce a novel methodology that leverages the strength of Physics-Informed Neural Networks (PINNs) to address the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits. The primary objective is to utilize physics-inspired deep learning techniques to accurately solve the time evolution of the different physical observables within the quantum system. To accomplish this objective, we embed the necessary physical information into an underlying neural network to effectively tackle the problem. In particular, we impose the hermiticity condition on all physical observables and make use of the principle of least action, guaranteeing the acquisition of the most appropriate counterdiabatic terms based on the underlying physics. The proposed approach offers a dependable alternative to address the CD driving problem, free from the constraints typically encountered in previous methodologies relying on classical numerical approximations.
    
[^112]: 在COVID-19期间导航不在分布范围内的电力负荷预测：利用人类移动的持续学习方法

    Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility. (arXiv:2309.04296v1 [cs.LG])

    [http://arxiv.org/abs/2309.04296](http://arxiv.org/abs/2309.04296)

    本研究提出了一种利用人类移动数据和持续学习技术的方法来解决COVID-19期间非分布期间的电力负荷预测问题，通过保留过去的见解并整合新的数据，提高了模型的准确性和鲁棒性。

    

    在传统的深度学习算法中，一个关键假设是数据分布在训练和部署过程中保持不变。然而，在面对非分布期间时，如COVID-19的封锁期，数据分布与模型在训练过程中所见的明显偏离。本文采用双重策略：利用持续学习技术更新模型的新数据，并利用在建筑物外部的保护隐私的行人计数器收集的人类移动数据。与在线学习相比，后者常常会遭受“灾难性遗忘”的困扰，因为新获得的知识常常会抹去先前的信息，持续学习则通过保留过去的见解并整合新的数据，提供了一个整体的方法。本研究将FSNet，一种强大的持续学习算法，应用于墨尔本市13个建筑群的真实数据。

    In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the s
    
[^113]: 多模态变换器用于材料分割

    Multimodal Transformer for Material Segmentation. (arXiv:2309.04001v1 [cs.CV])

    [http://arxiv.org/abs/2309.04001](http://arxiv.org/abs/2309.04001)

    本文提出了一种新的多模态分割方法MMSFormer，该方法有效地融合四种不同模态的信息，并在MCubeS数据集上取得了显著的性能提升。

    

    利用不同模态的信息可以提高多模态分割任务的性能。然而，由于每个模态的独特特性，有效地融合不同模态的信息仍然具有挑战性。在本文中，我们提出了一种新的融合策略，可以有效地融合四种不同模态的信息：RGB、线性偏振角（AoLP）、线性偏振度（DoLP）和近红外（NIR）。我们还提出了一种名为多模态分割变换器（MMSFormer）的新模型，该模型将所提出的融合策略结合起来进行多模态材料分割。MMSFormer在多模态材料分割（MCubeS）数据集上取得了52.05％的mIoU，超过了当前最先进的方法。例如，我们的方法在检测砾石（+10.4％）和人类（+9.1％）类上提供了显着的改进。消融研究表明融合块中的不同模块对结果至关重要。

    Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes. Ablation studies show that different modules in the fusion block are crucial for
    
[^114]: ImageBind-LLM: 多模态指令调优的方法

    ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])

    [http://arxiv.org/abs/2309.03905](http://arxiv.org/abs/2309.03905)

    ImageBind-LLM是一种多模态指令调优的方法，通过图像-文本对齐训练，并利用联合嵌入实现了优秀的多模态指令跟随能力。

    

    我们提出了一种通过ImageBind对大型语言模型（LLMs）进行多模态指令调优的方法。现有的工作主要集中在语言和图像指令调优方面，与此不同，我们的ImageBind-LLM可以响应多模态条件，包括音频、3D点云、视频以及它们的嵌入空间算术，只需进行图像-文本对齐训练。在训练过程中，我们采用可学习的Bind网络来对齐LLaMA和ImageBind的图像编码器之间的嵌入空间。然后，通过Bind网络转换的图像特征被添加到LLaMA的所有层的单词标记中，通过一个无注意力和零初始化的门控机制逐步注入视觉指令。在ImageBind的联合嵌入的帮助下，简单的图像-文本训练使我们的模型展示出了卓越的多模态指令跟随能力。在推断过程中，多模态输入被送入相应的ImageBind编码器，并被处理。

    We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by 
    
[^115]: GPT可以在没有计算器的情况下解决数学问题

    GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])

    [http://arxiv.org/abs/2309.03241](http://arxiv.org/abs/2309.03241)

    本研究表明，通过充分训练，一个20亿参数的语言模型可以在没有计算器工具的情况下以几乎100%的准确度执行多位数的算术运算，超越了之前的GPT-4。这项研究还通过在附加的多步骤算术运算和数学问题的数据集上进行微调，展示了一个与GPT-4在中文数学问题上相似的性能。

    

    以往的研究通常认为大型语言模型无法在没有计算器工具的情况下准确执行算术运算，特别是超过8位数字的乘法，以及涉及小数和分数的运算。本文旨在挑战这种误解。通过充分的训练数据，一个拥有20亿参数的语言模型可以以近乎100%的准确度执行多位数的算术运算，而且没有数据泄露，显著超过了GPT-4（其多位数乘法准确率仅为4.3%）。我们还演示了我们的MathGLM，它是通过在包含了文本描述的附加多步骤算术运算和数学问题的数据集上从GLM-10B微调而成的，它在一个包含5000个样本的中文数学问题测试集上的表现与GPT-4相似。

    Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of >8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.
    
[^116]: POI级别人群流推断的时空对比自监督学习

    Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference. (arXiv:2309.03239v1 [cs.LG])

    [http://arxiv.org/abs/2309.03239](http://arxiv.org/abs/2309.03239)

    本文提出了一种针对POI级别人群流推断的时空对比自监督学习模型，通过自监督属性图表示学习以解决数据标记不足、POI间时空依赖性复杂和人群流量与GPS报告之间相关性多样等挑战。

    

    准确获取兴趣点（POI）的人群流量对于有效的交通管理、公共服务和城市规划至关重要。尽管如此重要，但由于城市感知技术的限制，大多数数据源的数据质量不足以监测每个POI的人群流动。这使得从低质量数据中推断准确的人群流量成为一项关键且具有挑战性的任务。这一复杂性主要由三个关键因素引起：1）标记数据的稀缺性和罕见性；2）POI之间复杂的时空依赖关系；3）精确人群流量与GPS报告之间的众多相关性。为了应对这些挑战，我们将人群流推断问题重新构建为自监督属性图表示学习任务，并引入一种新的时空数据对比自监督学习框架（model）。我们的方法从构建一个空间图开始。

    Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad correlations between precise crowd flow and GPS reports}.  To address these challenges, we recast the crowd flow inference problem as a self-supervised attributed graph representation learning task and introduce a novel \underline{C}ontrastive \underline{S}elf-learning framework for \underline{S}patio-\underline{T}emporal data (\model). Our approach initiates with the construction of a spati
    
[^117]: 无需训练依然能获益：通过蒙特卡洛树搜索和能量函数引导实现大型语言模型的数学推理

    No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])

    [http://arxiv.org/abs/2309.03224](http://arxiv.org/abs/2309.03224)

    该论文提出了一种通过蒙特卡洛树搜索和能量函数引导来释放大型语言模型的数学推理能力的方法，以解决当前在数学推理任务中的不足和错误。该方法不需要进一步的微调步骤，通过重新定义模型和引入路径验证器的方式，实现了对输出空间的搜索和推理路径的评估。

    

    大型语言模型（LLMs）展现出令人印象深刻的语言理解和背景学习能力，包括自然语言处理（NLP）任务和具有挑战性的数学推理。然而，由于缺乏过程监督，将PLMs应用于数学推理任务通常无法生成正确的推理步骤和最终答案，即使解决方案概率很高。为了在没有进一步的微调步骤的情况下发挥微调的LLMs的数学推理能力，我们提出了一种方法，通过蒙特卡洛树搜索（MCTS）和轻量级能量函数为LLMs赋予即时反应和精细推理系统。具体而言，我们首先将微调的LLMs重新定义为基于残差的能量模型（Residual-EBM），并应用噪声对比估计来估计能量函数的参数。然后，我们使用带有能量函数的MCTS作为路径验证器来搜索输出空间并评估推理路径。通过广泛的实验证明了我们方法的有效性。

    Large language models (LLMs) exhibit impressive language understanding and in-context learning abilities including natural language processing (NLP) tasks and challenging mathematical reasoning. However, due to the lack of process-supervision, applying PLMs to mathematical reasoning tasks often fail to generate correct reasoning steps and final answer even though solutions have high probabilities. To unleash the mathematical reasoning of finetuned-LLMs without any further fineutuning steps, we propose a method to endow LLMs with immediate reaction and delicate reasoning system via Monte Carlo Tree Search(MCTS) and a light energy function to rank the decision steps. In particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy Model~(Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function . Then we use MCTS with energy function as path verifier to search the output space and evaluating the reasoning path. Through extensive 
    
[^118]: 概率单纯形上的扩散

    Diffusion on the Probability Simplex. (arXiv:2309.02530v1 [cs.LG])

    [http://arxiv.org/abs/2309.02530](http://arxiv.org/abs/2309.02530)

    本文提出了一种在概率单纯形上执行扩散的方法，通过使用softmax函数应用于阿恩斯坦-乌伦贝克过程，可以在处理连续性和离散性对象之间的紧张关系时取得良好效果。这种方法也可以扩展到单位立方体上，从而在有界图像生成方面具有应用前景。

    

    扩散模型通过学习逆转数据分布的逐渐噪声化来创建一个生成模型。然而，连续的噪声化过程与离散数据之间的期望不一致。为了解决连续性和离散性对象之间的紧张关系，我们提出了在概率单纯形上执行扩散的方法。使用概率单纯形自然地创建了一种解释，其中点对应于分类概率分布。我们的方法使用对阿恩斯坦-乌伦贝克过程之间进行softmax函数的应用，这是一个众所周知的随机微分方程。我们发现我们的方法也自然地扩展到包括对单位立方体的扩散，这对于有界图像生成应用具有意义。

    Diffusion models learn to reverse the progressive noising of a data distribution to create a generative model. However, the desired continuous nature of the noising process can be at odds with discrete data. To deal with this tension between continuous and discrete objects, we propose a method of performing diffusion on the probability simplex. Using the probability simplex naturally creates an interpretation where points correspond to categorical probability distributions. Our method uses the softmax function applied to an Ornstein-Unlenbeck Process, a well-known stochastic differential equation. We find that our methodology also naturally extends to include diffusion on the unit cube which has applications for bounded image generation.
    
[^119]: MedShapeNet - 一个用于计算机视觉的大规模三维医学形状数据集

    MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v1 [cs.CV])

    [http://arxiv.org/abs/2308.16139](http://arxiv.org/abs/2308.16139)

    MedShapeNet是一个大规模的三维医学形状数据集，作为一种对于常用形状基准的替代品，为计算机视觉研究提供了新的选择。

    

    我们提出了MedShapeNet，一个包含了解剖形状（如骨骼、器官、血管）和三维手术器械模型的大型数据集。在深度学习时代之前，统计形状模型在医学图像分析中的广泛应用证明了形状常被用来描述医学数据。然而，当前医学图像领域的最先进深度学习算法主要是基于体素的。相反，在计算机视觉领域，形状（包括体素占据网格、网格、点云和隐式表面模型）是三维数据的首选表示方法，这一点可以从大量关于形状的文章及在顶级计算机视觉会议（如IEEE/CVF计算机视觉与模式识别会议（CVPR））中见到，同时ShapeNet（约51300个模型）和普林斯顿ModelNet（127,915个模型）的流行度也在不断增加。MedShapeNet的创建是为了作为这些常用形状基准的替代品。

    We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to f
    
[^120]: LLaSM: 大型语言和语音模型

    LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])

    [http://arxiv.org/abs/2308.15930](http://arxiv.org/abs/2308.15930)

    LLaSM是一个大型语言和语音模型，具有跨模态对话能力，通过遵循语音和语言指令，提供了一种方便自然的人机交互方式。

    

    最近，多模态大型语言模型引起了广泛关注。然而，大部分研究都集中在视觉-语言多模态模型上，提供了强大的能力来遵循视觉和语言指令。然而，我们认为语音也是人类与世界互动的重要方式。因此，对于一个通用的助手来说，能够遵循多模态语音和语言指令是至关重要的。在这项工作中，我们提出了大型语言和语音模型（LLaSM）。LLaSM是一个端到端训练的大型多模态语音-语言模型，具有跨模态对话能力，能够遵循语音和语言指令。我们的初步实验表明，LLaSM展示了一种更方便自然的人机交互方式。为了支持研究，我们还发布了一个大型的语音指令数据集LLaSM-Audio-Instructions。代码和演示可在https://github.com/LinkSoul-AI/LLaSM和ht上查看

    Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and ht
    
[^121]: JL-引理推导的用于判别式字典学习的最优投影

    JL-lemma derived Optimal Projections for Discriminative Dictionary Learning. (arXiv:2308.13991v1 [cs.CV])

    [http://arxiv.org/abs/2308.13991](http://arxiv.org/abs/2308.13991)

    本文提出了一种名为JLSPCADL的新方法，通过利用Johnson-Lindenstrauss引理选择一个维数的转换空间，从而实现判别式字典学习并提供更好的分类结果。

    

    为了克服在具有大量类别的大维度数据分类中的困难，我们提出了一种名为JLSPCADL的新方法。本文利用Johnson-Lindenstrauss(JL)引理，在一个转换空间中选择一个维数，可以在其中学习用于信号分类的判别式字典。与通常使用JL进行降维的随机投影不同，我们使用从Modified Supervised PC Analysis (M-SPCA)推导得出的投影转换矩阵，其维数遵循JL的规定。JLSPCADL提供了一种启发式方法来推断适当的失真水平和相应的字典原子的适当描述长度(SDL)，以推导出一个最优的特征空间，从而提供更好的分类的字典原子的SDL。与最先进的基于降维的字典学习方法不同，从M-SPCA中一步得出的投影转换矩阵提供了最大的特征-标签一致性。

    To overcome difficulties in classifying large dimensionality data with a large number of classes, we propose a novel approach called JLSPCADL. This paper uses the Johnson-Lindenstrauss (JL) Lemma to select the dimensionality of a transformed space in which a discriminative dictionary can be learned for signal classification. Rather than reducing dimensionality via random projections, as is often done with JL, we use a projection transformation matrix derived from Modified Supervised PC Analysis (M-SPCA) with the JL-prescribed dimension.  JLSPCADL provides a heuristic to deduce suitable distortion levels and the corresponding Suitable Description Length (SDL) of dictionary atoms to derive an optimal feature space and thus the SDL of dictionary atoms for better classification. Unlike state-of-the-art dimensionality reduction-based dictionary learning methods, a projection transformation matrix derived in a single step from M-SPCA provides maximum feature-label consistency of the transfor
    
[^122]: 通过等变扩散模型进行基于形状的3D分子生成

    Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models. (arXiv:2308.11890v1 [cs.LG])

    [http://arxiv.org/abs/2308.11890](http://arxiv.org/abs/2308.11890)

    本文提出了一个基于形状的分子生成问题，通过等变形状引导的生成模型ShapeMol成功生成了新颖、多样且类似给定形状条件的药物样分子。

    

    配体基药物设计旨在识别与已知活性分子形状相似的新型药物候选物。本文提出了一个基于形状的分子生成问题，即在给定分子的形状条件下生成3D分子结构。为了解决这个问题，我们开发了一个等变形状引导的生成模型ShapeMol。ShapeMol由一个等变形状编码器和一个基于这些编码生成3D分子的等变扩散模型组成。实验结果表明，ShapeMol能够生成新颖、多样且类似给定形状条件的药物样分子。这些结果展示了ShapeMol在设计具有所需3D形状并与蛋白靶点结合的药物候选物方面的潜力。

    Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.
    
[^123]: 适应您的教师: 改进知识蒸馏用于无样本连续学习

    Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning. (arXiv:2308.09544v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.09544](http://arxiv.org/abs/2308.09544)

    本研究提出了一种适应教师的方法（TA）用于无样本连续学习，解决了知识蒸馏方法在这种情况下的性能下降问题，并在多个基准测试中持续提升模型性能。

    

    在这项工作中，我们研究了无样本类别增量学习（CIL），并使用知识蒸馏（KD）作为正则化策略，旨在防止遗忘。KD方法在CIL中取得了成功，但是在没有访问先前任务的训练数据示例的情况下，它们往往很难对模型进行正则化。我们的分析发现，这个问题源于处理分布外数据时教师网络中的显著表示转换。这导致KD损失成分中出现较大的错误，从而导致CIL模型性能下降。受最近的测试时适应方法的启发，我们引入了教师适应（TA）方法，该方法在增量训练过程中同时更新教师和主模型。我们的方法与基于KD的CIL方法无缝集成，能够在多个无样本CIL基准测试中持续提升它们的性能。

    In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL models. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main models during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.
    
[^124]: 从期望到安全：通过在潜在空间中强制正确的原因来消除深度模型的偏见

    From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space. (arXiv:2308.09437v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.09437](http://arxiv.org/abs/2308.09437)

    该论文提出了一种通过梯度减小模型对偏见的敏感性的方法，从而在概念级别上确保正确原因，有效减轻深度神经网络中的偏见。该方法在多个数据集和环境中被验证有效。

    

    深度神经网络容易学习训练数据中潜藏的错误相关性，从而导致可能有偏见的预测。这在将这些模型部署于高风险决策场景（如医学应用）时存在风险。目前的后处理模型校正方法要么需要输入级别的注释，这只适用于局部化偏见，要么通过扩充潜在特征空间，希望能实现正确的原因。我们提出了一种新的方法，通过梯度减小模型对偏见的敏感性，从而在概念级别上确保正确的原因。当通过概念激活向量来建模偏见时，我们强调选择稳健的方向的重要性，因为传统的基于回归的方法（如支持向量机）往往会导致发散的方向。我们使用VGG、ResNet和EfficientN在ISIC、骨龄、ImageNet和CelebA数据集上在受控和真实环境中有效减轻偏见。

    Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientN
    
[^125]: 多模态多损失融合网络

    Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])

    [http://arxiv.org/abs/2308.00264](http://arxiv.org/abs/2308.00264)

    多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。

    

    在这项工作中，我们研究了跨多个模态选择和融合特征的最佳方法，并将其组合在神经网络中以改善情感检测。我们比较了不同的融合方法并且研究了多损失训练在多模态融合网络中的影响，从而确定了与子网性能相关的有用发现。我们最好的模型在三个数据集（CMU-MOSI、CMU-MOSEI和CH-SIMS）上实现了最先进的性能，并且在大多数指标上优于其他方法。我们发现，训练多模态特征可以改善单模态测试，并且基于数据集注释模式设计融合方法可以增强模型性能。这些结果表明了在神经网络中增强情感检测的优化特征选择和融合方法的发展方向。

    In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
    
[^126]: 可解释人工智能（XAI）在年龄预测中的应用：一项系统综述

    eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review. (arXiv:2307.13704v1 [cs.AI])

    [http://arxiv.org/abs/2307.13704](http://arxiv.org/abs/2307.13704)

    本综述探讨了可解释人工智能（XAI）在年龄预测任务中的应用。通过系统性综述，我们讨论了XAI方法在医疗应用和年龄预测领域的益处。

    

    可解释人工智能（XAI）现在是机器学习中的重要组成部分，能够解释复杂模型的预测结果。XAI特别适用于危险应用，特别是在医疗保健领域，人类的生命依赖于AI系统的决策。医疗研究的一个领域是年龄预测和衰老及与年龄相关疾病的生物标志物鉴定。然而，在年龄预测任务中，XAI的作用尚未直接探讨。在本综述中，我们讨论了XAI方法在年龄预测任务中的应用。我们通过器官系统进行了系统性综述，并讨论了XAI在医疗应用以及特别是年龄预测领域的益处。

    eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
    
[^127]: TwinLiteNet：自动驾驶汽车中可驱动区域和车道分割的高效轻量模型

    TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])

    [http://arxiv.org/abs/2307.10705](http://arxiv.org/abs/2307.10705)

    本文提出了一种轻量级模型TwinLiteNet，用于自动驾驶车辆中的可驱动区域和车道分割。该模型成本低廉且高效准确，并在实验中表现出明显的计算资源节约。

    

    语义分割是自动驾驶中一个常见的任务，用于理解周围环境。对于道路上的安全和高效导航来说，可驱动区域分割和车道检测尤为重要。然而，原始的语义分割模型计算开销大，需要高端硬件，这对于嵌入式系统的自动驾驶车辆来说是不可行的。本文提出了一种轻量级的可驱动区域和车道线分割模型。TwinLiteNet设计成成本低廉，但能够实现准确和高效的分割结果。我们在BDD100K数据集上评估了TwinLiteNet，并与现代模型进行了比较。实验结果表明，我们的TwinLiteNet与现有方法表现相似，但所需的计算资源显著减少。具体而言，TwinLiteNet在可驱动区域任务上实现了91.3%的mIoU评分，在车道检测任务上实现了31.08%的IoU评分，仅使用了40万个参数，在GPU RTX上实现了415 FPS。

    Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX 
    
[^128]: 虚拟电力厂在日前电力市场中的战略竞标的安全增强学习算法

    Safe Reinforcement Learning for Strategic Bidding of Virtual Power Plants in Day-Ahead Markets. (arXiv:2307.05812v1 [eess.SY])

    [http://arxiv.org/abs/2307.05812](http://arxiv.org/abs/2307.05812)

    本文提出了一种新颖的安全增强学习算法，用于虚拟电力厂在日前电力市场中的战略竞标。该算法不需要精确的市场模型，通过深度确定性策略梯度方法学习具有竞争力的竞标策略，并通过引入安全屏蔽和奖励函数的惩罚机制来考虑虚拟电力厂的物理约束，使代理能够学习到更安全的策略。

    

    本文提出了一种新颖的安全增强学习算法，用于虚拟电力厂在日前电力市场中的战略竞标。提出的算法采用深度确定性策略梯度（DDPG）方法，学习具有竞争力的竞标策略，而不需要精确的市场模型。此外，为了考虑虚拟电力厂的复杂内部物理约束，我们对DDPG方法进行了两个增强。首先，推导了一种基于投影的安全屏蔽，将代理的行为限制在分布式能源资源的非线性功率流方程和运行约束所定义的可行空间内。其次，引入了奖励函数中的屏蔽激活惩罚，鼓励代理学习更安全的策略。基于IEEE 13-bus网络的案例研究证明了所提方法在使代理学习到高竞争力，安全的战略策略方面的有效性。

    This paper presents a novel safe reinforcement learning algorithm for strategic bidding of Virtual Power Plants (VPPs) in day-ahead electricity markets. The proposed algorithm utilizes the Deep Deterministic Policy Gradient (DDPG) method to learn competitive bidding policies without requiring an accurate market model. Furthermore, to account for the complex internal physical constraints of VPPs we introduce two enhancements to the DDPG method. Firstly, a projection-based safety shield that restricts the agent's actions to the feasible space defined by the non-linear power flow equations and operating constraints of distributed energy resources is derived. Secondly, a penalty for the shield activation in the reward function that incentivizes the agent to learn a safer policy is introduced. A case study based on the IEEE 13-bus network demonstrates the effectiveness of the proposed approach in enabling the agent to learn a highly competitive, safe strategic policy.
    
[^129]: 评估年龄估计实践的反思呼吁：现有技术的比较分析和统一基准

    A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark. (arXiv:2307.04570v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.04570](http://arxiv.org/abs/2307.04570)

    该论文提出反思评估年龄估计实践的呼吁，对现有技术进行了比较分析，并提出了统一基准。研究发现当前评估协议存在问题，并描述了如何解决它们。通过比较分析，发现方法之间的性能差异微不足道，而其他因素的影响更大。研究利用得到的见解提出使用FaRL方法来解决这些问题。

    

    由于基准流程的不一致性导致的发布结果的不可靠性，比较不同的年龄估计方法面临着挑战。先前的研究报告指出，使用专门的方法在过去十年中持续改善了性能，然而我们的发现对这些声明提出质疑。本文识别出当前使用的评估协议中存在的两个琐碎但持续存在的问题，并描述了如何解决它们。我们详细描述了我们的评估协议，并提供了使用该协议的具体示例。我们利用该协议对最先进的面部年龄估计方法进行了广泛的比较分析。令人惊讶的是，我们发现方法之间的性能差异与其他因素（如面部对齐、面部覆盖、图像分辨率、模型架构或用于预训练的数据量）相比微不足道。我们利用这些见解提出使用FaRL方法来解决这些问题。

    Comparing different age estimation methods poses a challenge due to the unreliability of published results stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. This paper identifies two trivial, yet persistent issues with the currently used evaluation protocol and describes how to resolve them. We describe our evaluation protocol in detail and provide specific examples of how the protocol should be used. We utilize the protocol to offer an extensive comparative analysis for state-of-the-art facial age estimation methods. Surprisingly, we find that the performance differences between the methods are negligible compared to the effect of other factors, such as facial alignment, facial coverage, image resolution, model architecture, or the amount of data used for pretraining. We use the gained insights to propose using FaRL a
    
[^130]: ProbVLM: 冻结视觉-语言模型的概率适配器

    ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models. (arXiv:2307.00398v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.00398](http://arxiv.org/abs/2307.00398)

    ProbVLM是一种概率适配器，用于估计大规模视觉-语言模型中嵌入的概率分布，以解决固有的嵌入歧义问题，并在多个数据集上展示了其在检索任务中的优越性能表现。

    

    大规模视觉-语言模型（VLM）如CLIP成功地在图像和文本之间找到对应关系。通过标准的确定性映射过程，将图像或文本样本映射到嵌入空间中的一个向量。这是有问题的：由于多个样本（图像或文本）可以抽象出物理世界中的相同概念，确定性嵌入不反映嵌入空间中的固有歧义性。我们提出了ProbVLM，一种概率适配器，通过事后方式在预训练的VLM中通过内部/外部模态对齐估计嵌入的概率分布，而无需大规模数据集或计算。在四个具有挑战性的数据集上，即COCO、Flickr、CUB和Oxford-flowers，我们估计了两个VLM（CLIP和BLIP）的多模态嵌入不确定性，量化了嵌入不确定性在检索任务中的校准，并表明ProbVLM优于其他方法。此外，我们提出了主动学习和模型...

    Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model 
    
[^131]: 南佛罗里达州水位预测的深度学习模型

    Deep Learning Models for Water Stage Predictions in South Florida. (arXiv:2306.15907v1 [cs.LG])

    [http://arxiv.org/abs/2306.15907](http://arxiv.org/abs/2306.15907)

    本论文利用深度学习模型训练代理模型，快速预测南佛罗里达州迈阿密河下游的水位，并与基于物理的模型进行比较。

    

    模拟和预测河流系统的水位对于洪水警报、水力操作和洪水减轻至关重要。在工程领域中，使用HEC-RAS、MIKE和SWMM等工具建立详细的基于物理的水文和水力计算模型来模拟整个流域，从而预测系统中任意点的水位。然而，这些基于物理的模型计算量大，尤其对于大流域和长时间模拟来说。为了解决这个问题，我们训练了几个深度学习（DL）模型作为代理模型，快速预测水位。本文以南佛罗里达州迈阿密河的下游水位为案例研究。数据集来自南佛罗里达水管理区（SFWMD）的DBHYDRO数据库，时间跨度为2010年1月1日至2020年12月31日。广泛的实验表明，DL模型的性能与基于物理的模型相当。

    Simulating and predicting water levels in river systems is essential for flood warnings, hydraulic operations, and flood mitigations. In the engineering field, tools such as HEC-RAS, MIKE, and SWMM are used to build detailed physics-based hydrological and hydraulic computational models to simulate the entire watershed, thereby predicting the water stage at any point in the system. However, these physics-based models are computationally intensive, especially for large watersheds and for longer simulations. To overcome this problem, we train several deep learning (DL) models for use as surrogate models to rapidly predict the water stage. The downstream stage of the Miami River in South Florida is chosen as a case study for this paper. The dataset is from January 1, 2010, to December 31, 2020, downloaded from the DBHYDRO database of the South Florida Water Management District (SFWMD). Extensive experiments show that the performance of the DL models is comparable to that of the physics-bas
    
[^132]: 线性约束下的多臂赌博纯探索算法

    Pure Exploration in Bandits with Linear Constraints. (arXiv:2306.12774v1 [cs.LG])

    [http://arxiv.org/abs/2306.12774](http://arxiv.org/abs/2306.12774)

    本文提出了两种相对于线性约束下的多臂赌博问题都是渐进最优的算法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。

    

    本文解决多臂赌博问题中存在线性约束的情况下，如何在一定置信度下确定最优策略的问题。与标准的最优臂识别问题不同，这种情况下的最优策略可能不是确定性的，而是可能在多个臂之间进行混合。这种情况改变了问题的几何形状，我们通过信息论下界进行了描述。我们提出了两种相对于此设置都是渐进最优的算法，其中一个基于“跟踪停止”方法，另一个基于博弈理论的方法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。最后，我们提供了实证结果，验证了我们的界限，并展示了约束如何改变问题的难度。

    We address the problem of identifying the optimal policy with a fixed confidence level in a multi-armed bandit setup, when \emph{the arms are subject to linear constraints}. Unlike the standard best-arm identification problem which is well studied, the optimal policy in this case may not be deterministic and could mix between several arms. This changes the geometry of the problem which we characterize via an information-theoretic lower bound. We introduce two asymptotically optimal algorithms for this setting, one based on the Track-and-Stop method and the other based on a game-theoretic approach. Both these algorithms try to track an optimal allocation based on the lower bound and computed by a weighted projection onto the boundary of a normal cone. Finally, we provide empirical results that validate our bounds and visualize how constraints change the hardness of the problem.
    
[^133]: 打破设备上训练内存壁垒：一项系统性调查

    Breaking On-device Training Memory Wall: A Systematic Survey. (arXiv:2306.10388v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2306.10388](http://arxiv.org/abs/2306.10388)

    本研究调查了打破设备上训练内存壁垒的最先进技术，并提出了解决资源受限设备上训练更大更复杂模型的方法。调查分析了内存壁垒的关键因素，并总结了设备上训练的开放问题。

    

    设备上的训练已经成为机器学习中越来越流行的方法，可以直接在移动设备和边缘设备上进行模型训练。然而，在这个领域面临的一个主要挑战是这些设备上有限的内存，这会严重限制可以进行训练的模型的大小和复杂性。在这个系统性调查中，我们旨在探索打破设备上训练内存壁垒的当前最先进技术，重点关注可以在资源受限设备上训练更大更复杂的模型的方法。具体而言，我们首先分析了设备上训练中遇到的内存壁垒的关键因素。然后，我们提出了关于设备上训练的综合文献综述，地址了内存限制的问题。最后，我们总结了设备上训练，并突出了未来研究的开放问题。通过提供这些技术及其效果的全面概述，我们的调查对于解决设备上训练中的内存限制问题具有重要意义。

    On-device training has become an increasingly popular approach to machine learning, enabling models to be trained directly on mobile and edge devices. However, a major challenge in this area is the limited memory available on these devices, which can severely restrict the size and complexity of the models that can be trained. In this systematic survey, we aim to explore the current state-of-the-art techniques for breaking on-device training memory walls, focusing on methods that can enable larger and more complex models to be trained on resource-constrained devices.  Specifically, we first analyze the key factors that contribute to the phenomenon of memory walls encountered during on-device training. Then, we present a comprehensive literature review of on-device training, which addresses the issue of memory limitations. Finally, we summarize on-device training and highlight the open problems for future research.  By providing a comprehensive overview of these techniques and their effe
    
[^134]: RescueSpeech：用于搜救领域语音识别的德语语料库

    RescueSpeech: A German Corpus for Speech Recognition in Search and Rescue Domain. (arXiv:2306.04054v1 [eess.AS])

    [http://arxiv.org/abs/2306.04054](http://arxiv.org/abs/2306.04054)

    RescueSpeech是一个用于搜救领域语音识别的德语语音数据集，但目前最先进的方法仍无法令人满意。

    

    尽管语音识别在最近得到了进一步发展，但在嘈杂、回声的环境中准确转录对话和情感表达仍然存在一定难度。在搜救领域尤其如此，因为转录救援队成员之间的对话对支持实时决策至关重要。搜救场景中语音数据和相关背景噪声的稀缺性使得部署健壮的语音识别系统变得困难。为了解决这个问题，我们创建并公开了一个名为RescueSpeech的德语语音数据集。该数据集包括模拟救援演习的真实语音录音。此外，我们还发布了竞争性训练配方和预训练模型。我们的研究表明，目前最先进的方法所达到的性能水平仍远未能令人满意。

    Despite recent advancements in speech recognition, there are still difficulties in accurately transcribing conversational and emotional speech in noisy and reverberant acoustic environments. This poses a particular challenge in the search and rescue (SAR) domain, where transcribing conversations among rescue team members is crucial to support real-time decision-making. The scarcity of speech data and associated background noise in SAR scenarios make it difficult to deploy robust speech recognition systems.  To address this issue, we have created and made publicly available a German speech dataset called RescueSpeech. This dataset includes real speech recordings from simulated rescue exercises. Additionally, we have released competitive training recipes and pre-trained models. Our study indicates that the current level of performance achieved by state-of-the-art methods is still far from being acceptable.
    
[^135]: 几个非克利福德门制备的量子状态的有效学习

    Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates. (arXiv:2305.13409v1 [quant-ph])

    [http://arxiv.org/abs/2305.13409](http://arxiv.org/abs/2305.13409)

    该研究提出了一种能有效学习几个非克利福德门制备的量子状态的算法，并给出了一个随着稳定维数增大而学习所有状态的算法。

    

    我们提出了一种算法，可以有效地学习通过克利福德门和$O(\log(n))$个非克利福德门制备的量子状态。具体而言，对于最多使用$t$个非克利福德门制备的$n$量子比特状态$|\psi\rangle$，我们证明可以用$\mathsf{poly}(n,2^t,1/\epsilon)$时间和$|\psi\rangle$的复制来学习$|\psi\rangle$，使其跟真实状态的距离不超过$\epsilon$。该结果是一个稳定维数较大的状态学习算法的特例，当一个量子状态的稳定子维数为$k$，表示它被一个由$2^k$个Pauli算子的Abel群稳定。

    We give an algorithm that efficiently learns a quantum state prepared by Clifford gates and $O(\log(n))$ non-Clifford gates. Specifically, for an $n$-qubit state $\lvert \psi \rangle$ prepared with at most $t$ non-Clifford gates, we show that $\mathsf{poly}(n,2^t,1/\epsilon)$ time and copies of $\lvert \psi \rangle$ suffice to learn $\lvert \psi \rangle$ to trace distance at most $\epsilon$. This result follows as a special case of an algorithm for learning states with large stabilizer dimension, where a quantum state has stabilizer dimension $k$ if it is stabilized by an abelian group of $2^k$ Pauli operators. We also develop an efficient property testing algorithm for stabilizer dimension, which may be of independent interest.
    
[^136]: 使用深度强化学习从电子病历中提取诊断路径

    Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning. (arXiv:2305.06295v1 [cs.LG])

    [http://arxiv.org/abs/2305.06295](http://arxiv.org/abs/2305.06295)

    本文采用深度强化学习算法，基于电子病历来学习获得正确诊断所需的观察序列的最优顺序。因为诊断指南的缺陷，尤其对罕见病或患有多种病的患者，DRL算法具有重要现实意义。

    

    临床诊断指南旨在说明可能导致诊断的步骤。指南能够理性地规范化临床决策，但由于它们的建立是为了覆盖大多数人群，因此在指导罕见病或患有多种病的患者获得正确诊断方面，存在缺陷。本文受指南启发，将诊断任务形式化为序列决策问题，并研究了使用电子病历(EHRs)训练的深度强化学习算法来学习获得正确诊断所需的观察序列的最优顺序。由于DRL算法的多样性和对上下文的敏感性，我们考虑了几种方法和设置，并将它们与彼此和经典分类器进行了比较。我们在一个合成但逼真的数据集上进行了实验。

    Clinical diagnosis guidelines aim at specifying the steps that may lead to a diagnosis. Guidelines enable rationalizing and normalizing clinical decisions but suffer drawbacks as they are built to cover the majority of the population and may fail in guiding to the right diagnosis for patients with uncommon conditions or multiple pathologies. Moreover, their updates are long and expensive, making them unsuitable to emerging practices. Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms trained on Electronic Health Records (EHRs) to learn the optimal sequence of observations to perform in order to obtain a correct diagnosis. Because of the variety of DRL algorithms and of their sensitivity to the context, we considered several approaches and settings that we compared to each other, and to classical classifiers. We experimented on a synthetic but realistic dataset to differenti
    
[^137]: PyTorch FSDP：全面分片数据并行规模化的经验

    PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. (arXiv:2304.11277v1 [cs.DC])

    [http://arxiv.org/abs/2304.11277](http://arxiv.org/abs/2304.11277)

    本论文介绍了基于PyTorch的Fully Sharded Data Parallel（FSDP）解决方案，该方案可扩展大型模型训练，并优化各种硬件配置的资源利用率。

    

    众所周知，大型模型在广泛领域内具有优异的性能潜力。尽管机器学习系统研究领域取得了显著进展，使得开发和探索大型模型成为可能，但这些能力仍受限于少数高级用户和行业领袖，导致技术上的隐含壁垒阻碍广泛社区访问和利用这些技术。本文介绍了PyTorch Fully Sharded Data Parallel（FSDP）作为大型模型训练的产业级解决方案。FSDP已与几个关键PyTorch核心组件（包括张量实现、分发器系统和CUDA内存缓存分配器）密切协作，以提供非侵入式用户体验和高训练效率。此外，FSDP本地集成了一系列技术和设置，优化了各种硬件配置的资源利用率。

    It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The 
    
[^138]: 多粒度时间变换器用于知识追踪

    Multi-granulariy Time-based Transformer for Knowledge Tracing. (arXiv:2304.05257v1 [cs.LG])

    [http://arxiv.org/abs/2304.05257](http://arxiv.org/abs/2304.05257)

    本文提出了一种基于Transformer的架构用于准确地预测学生在标准化测试中的表现。该模型考虑了学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，并在解码器输入中使用了多个时间特征粒度以显著提高模型性能。与LightGBM相比，该方法更加准确，为教育领域的AI发展提供了一个可伸缩和准确的预测学生成果的工具。

    

    本文提出了一种基于Transformer的架构，用于预测标准化测试中学生的表现。具体来说，我们利用学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，为每个学生创建一个个性化的模型。然后，我们使用这些模型来预测学生在给定测试中的未来表现。将该模型应用于RIIID数据集，我们证明使用多个时间特征粒度作为解码器输入可以显着提高模型性能。我们的结果还表明了我们方法的有效性，相对于LightGBM方法有很大的改进。我们的工作为教育领域的AI发展做出了贡献，提供了一个可伸缩和准确的预测学生成果的工具。

    In this paper, we present a transformer architecture for predicting student performance on standardized tests. Specifically, we leverage students historical data, including their past test scores, study habits, and other relevant information, to create a personalized model for each student. We then use these models to predict their future performance on a given test. Applying this model to the RIIID dataset, we demonstrate that using multiple granularities for temporal features as the decoder input significantly improve model performance. Our results also show the effectiveness of our approach, with substantial improvements over the LightGBM method. Our work contributes to the growing field of AI in education, providing a scalable and accurate tool for predicting student outcomes.
    
[^139]: 时间动态同步功能脑网络在精神分裂症诊断和侧化分析中的应用

    Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis. (arXiv:2304.01347v1 [q-bio.NC])

    [http://arxiv.org/abs/2304.01347](http://arxiv.org/abs/2304.01347)

    本文提出了一种基于动态功能连接的脑网络分析模型，通过构建动态同步特征和革命性的图卷积方法实现精神分裂症诊断和侧化分析，并在实验证明其表现优于其他最先进模型。

    

    有证据表明，动态功能连接可以捕捉静息态功能磁共振成像数据中的脑活动时变异常，并在揭示精神分裂症（SZ）患者异常脑活动机制方面具有天然优势。因此，本文采用了一种先进的动态脑网络分析模型——时态脑类别图卷积网络（temporal-BCGCN）。首先设计了独特的动态脑网络分析模块DSF-BrainNet，用于构建动态同步特征。随后，提出了一种革命性的图卷积方法TemporalConv，基于特征的同步时间属性。最后，提出了一种基于静息态功能磁共振成像数据的深度学习模块化异常半球侧化检测工具，称为CategoryPool。该研究在COBRE和UCLA数据集上进行验证，分别达到83.62％和89.71％的平均准确率，优于基线模型和其他最先进模型，在精神分裂症诊断和侧化分析方面表现出色。

    Available evidence suggests that dynamic functional connectivity (dFC) can capture time-varying abnormalities in brain activity in rs-fMRI data and has a natural advantage in uncovering mechanisms of abnormal brain activity in schizophrenia(SZ) patients. Hence, an advanced dynamic brain network analysis model called the temporal brain category graph convolutional network (temporal-BCGCN) was employed. Firstly, a unique dynamic brain network analysis module, DSF-BrainNet, was designed to construct dynamic synchronization features. Subsequently, a revolutionary graph convolution method, TemporalConv, was proposed, based on the synchronous temporal properties of feature. Finally, the first modular abnormal hemispherical lateralization test tool in deep learning based on rs-fMRI data, named CategoryPool, was proposed. This study was validated on COBRE and UCLA datasets and achieved 83.62% and 89.71% average accuracy, respectively, outperforming the baseline model and other State-of-the-Art
    
[^140]: BOLT：一种用于在普通CPU硬件上训练和部署大规模神经网络的自动化深度学习框架。

    BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware. (arXiv:2303.17727v1 [cs.LG])

    [http://arxiv.org/abs/2303.17727](http://arxiv.org/abs/2303.17727)

    BOLT是一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库，它提供了一个灵活的高级API，使用户可以构建模型并抽象掉稀疏网络训练的算法细节。

    

    有效地在普通CPU硬件上进行大规模神经网络的训练和推理对于民主化深度学习能力具有巨大的实际意义。目前，由数十亿个参数组成的大规模模型的训练过程需要广泛使用专用硬件加速器（例如GPU），这些加速器仅限于少数具有相当财务资源的机构。此外，训练和部署这些模型通常会带来惊人的碳足迹。在本文中，我们通过引入BOLT，一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库来解决这些挑战。BOLT提供了一个灵活的高级API，用于构建模型，该API对于现有流行的深度学习框架的用户来说是熟悉的。通过自动调整专用超参数，BOLT也抽象掉了稀疏网络训练的算法细节。

    Efficient large-scale neural network training and inference on commodity CPU hardware is of immense practical significance in democratizing deep learning (DL) capabilities. Presently, the process of training massive models consisting of hundreds of millions to billions of parameters requires the extensive use of specialized hardware accelerators, such as GPUs, which are only accessible to a limited number of institutions with considerable financial resources. Moreover, there is often an alarming carbon footprint associated with training and deploying these models. In this paper, we address these challenges by introducing BOLT, a sparse deep learning library for training massive neural network models on standard CPU hardware. BOLT provides a flexible, high-level API for constructing models that will be familiar to users of existing popular DL frameworks. By automatically tuning specialized hyperparameters, BOLT also abstracts away the algorithmic details of sparse network training. We e
    
[^141]: 基于区域卷积神经网络的植物病害检测

    Plant Disease Detection using Region-Based Convolutional Neural Network. (arXiv:2303.09063v1 [cs.CV])

    [http://arxiv.org/abs/2303.09063](http://arxiv.org/abs/2303.09063)

    本文提出了一种新的基于区域卷积神经网络（R-CNN）的轻量级深度学习模型，可用于检测番茄植物的叶病害。与传统方法相比，该模型利用整个番茄植物图像的特征进行检测，具有更高的准确性和效率。

    

    农业在孟加拉国的食品和经济中起着重要作用，但细菌，病毒和真菌病害限制了作物生产。本论文旨在构建一个轻量级深度学习模型以预测番茄植物的叶病害。通过修改区域卷积神经网络（R-CNN）的结构，本文提出了一个架构，利用整个番茄植物图像的特征来检测植物病害，而不仅仅是感兴趣区域（ROI）。所提出的模型在检测四种常见的番茄植物病害方面表现出94.3％的准确性。

    Agriculture plays an important role in the food and economy of Bangladesh. The rapid growth of population over the years also has increased the demand for food production. One of the major reasons behind low crop production is numerous bacteria, virus and fungal plant diseases. Early detection of plant diseases and proper usage of pesticides and fertilizers are vital for preventing the diseases and boost the yield. Most of the farmers use generalized pesticides and fertilizers in the entire fields without specifically knowing the condition of the plants. Thus the production cost oftentimes increases, and, not only that, sometimes this becomes detrimental to the yield. Deep Learning models are found to be very effective to automatically detect plant diseases from images of plants, thereby reducing the need for human specialists. This paper aims at building a lightweight deep learning model for predicting leaf disease in tomato plants. By modifying the region-based convolutional neural n
    
[^142]: 无监督学习中泛化错误的权衡

    Tradeoff of generalization error in unsupervised learning. (arXiv:2303.05718v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2303.05718](http://arxiv.org/abs/2303.05718)

    无监督学习中存在一个权衡，使用更复杂的模型可以降低模型误差，但会增加数据误差，特别是在训练数据集较小的情况下。

    

    寻找最小化泛化错误（GE）的最佳模型复杂度是机器学习的关键问题。对于传统的监督学习，这个任务通常涉及偏差-方差权衡：通过使模型更复杂来降低偏差会导致方差的增加。与此同时，关于无监督学习是否存在相同的权衡问题研究较少。在本研究中，我们提出无监督学习通常表现出GE的两个组成要素的权衡，即模型误差和数据误差 - 使用更复杂的模型减小模型误差的同时以数据误差为代价，数据误差对于更小的训练数据集起到更重要的作用。通过训练受限玻尔兹曼机生成给定温度下的二维伊辛模型的配置以及给定入口和出口速率的完全非对称简单排斥过程，我们证实了这一点。我们的结果还表明，

    Finding the optimal model complexity that minimizes the generalization error (GE) is a key issue of machine learning. For the conventional supervised learning, this task typically involves the bias-variance tradeoff: lowering the bias by making the model more complex entails an increase in the variance. Meanwhile, little has been studied about whether the same tradeoff exists for unsupervised learning. In this study, we propose that unsupervised learning generally exhibits a two-component tradeoff of the GE, namely the model error and the data error -- using a more complex model reduces the model error at the cost of the data error, with the data error playing a more significant role for a smaller training dataset. This is corroborated by training the restricted Boltzmann machine to generate the configurations of the two-dimensional Ising model at a given temperature and the totally asymmetric simple exclusion process with given entry and exit rates. Our results also indicate that the 
    
[^143]: 带吸收的泛洪：复杂网络上异构赌博机的高效协议

    Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks. (arXiv:2303.05445v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05445](http://arxiv.org/abs/2303.05445)

    该论文提出了一种名为带吸收的泛洪（FwA）的新协议，用于解决复杂网络上的异构赌博机问题。通过严格的遗憾分析，证明了该协议的有效性。

    

    多臂赌博机广泛用于建模顺序决策，在许多现实应用中如在线推荐系统和无线网络中无处不在。我们考虑一个多代理的场景，每个代理解决自己的赌博机问题，赌博机拥有不同的臂。他们的目标是在通过给定网络的通信协议协作的同时最小化他们的集体遗憾。先前关于此问题的文献只考虑了臂的异质性和网络化代理问题。在这项工作中，我们引入了一个同时包含这两个特性的设置。针对这一新颖的设置，我们首先对标准泛洪协议结合经典的上置信界策略提供了严格的遗憾分析。然后，为了减轻在复杂网络中泛洪造成的高通信成本问题，我们提出了一种新的协议，称为带吸收的泛洪（FwA）。我们对由此产生的遗憾上界进行了理论分析，并讨论了该协议的优点。

    Multi-armed bandits are extensively used to model sequential decision-making, making them ubiquitous in many real-life applications such as online recommender systems and wireless networking. We consider a multi-agent setting where each agent solves their own bandit instance endowed with a different set of arms. Their goal is to minimize their group regret while collaborating via some communication protocol over a given network. Previous literature on this problem only considered arm heterogeneity and networked agents separately. In this work, we introduce a setting that encompasses both features. For this novel setting, we first provide a rigorous regret analysis for a standard flooding protocol combined with the classic UCB policy. Then, to mitigate the issue of high communication costs incurred by flooding in complex networks, we propose a new protocol called Flooding with Absorption (FwA). We provide a theoretical analysis of the resulting regret bound and discuss the advantages of
    
[^144]: 分层优化导出学习

    Hierarchical Optimization-Derived Learning. (arXiv:2302.05587v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05587](http://arxiv.org/abs/2302.05587)

    该论文提出了一个名为HODL的新框架，用于同时研究优化导出模型构建和相应学习过程的内在行为，并证明了这两个子任务的联合收敛性。这是对这两个任务提供的首个理论保证。

    

    近年来，通过利用优化技术来规划深度模型的传播，提出了各种所谓的优化导出学习（ODL）方法来解决不同的学习和视觉任务。尽管在实践中取得了令人满意的性能，但现有的ODL方法仍存在一些基本问题。特别地，当前的ODL方法倾向于将模型构建和学习视为两个独立的阶段，因此未能准确表达它们之间的相互关系。在这项研究中，我们首先建立了一个名为Hierarchical ODL（HODL）的新框架，同时研究了优化导出模型构建和相应学习过程的内在行为。然后，我们严格证明了这两个子任务的联合收敛性，从逼近质量和稳定性分析的角度展示了证明。据我们所知，这是对这两个相互关联的任务提供的首个理论保证。

    In recent years, by utilizing optimization techniques to formulate the propagation of deep model, a variety of so-called Optimization-Derived Learning (ODL) approaches have been proposed to address diverse learning and vision tasks. Although having achieved relatively satisfying practical performance, there still exist fundamental issues in existing ODL methods. In particular, current ODL methods tend to consider model construction and learning as two separate phases, and thus fail to formulate their underlying coupling and depending relationship. In this work, we first establish a new framework, named Hierarchical ODL (HODL), to simultaneously investigate the intrinsic behaviors of optimization-derived model construction and its corresponding learning process. Then we rigorously prove the joint convergence of these two sub-tasks, from the perspectives of both approximation quality and stationary analysis. To our best knowledge, this is the first theoretical guarantee for these two cou
    
[^145]: 用于编码的大型语言模型：安全加固和对抗测试

    Large Language Models for Code: Security Hardening and Adversarial Testing. (arXiv:2302.05319v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.05319](http://arxiv.org/abs/2302.05319)

    本研究针对大型语言模型在生成代码时缺乏安全意识，从安全加固和对抗测试的角度入手，提出了一项新的安全任务——受控代码生成，通过一种新型基于学习的方法SVEN，实现生成既安全又功能正确的代码，并对当前的LM进行对抗测试，强调了在LM的培训和评估中考虑安全因素的必要性。

    

    大型语言模型(LMs)越来越多地预先在大规模代码库上进行预处理，用于生成代码。然而，LM缺乏安全意识，并经常生成不安全的代码。本研究沿着两个重要方向研究了LM的安全性:(i)安全加固，旨在增强LM在生成安全代码方面的可靠性;(ii)对抗测试，旨在在对抗性立场评估LM的安全性。我们通过制定一项称为受控代码生成的新安全任务来同时解决这两个问题。该任务是参数化的，将一个二进制属性作为输入，以指导LM生成安全或不安全的代码，同时保留LM生成功能正确代码的能力。我们提出了一种称为SVEN的新型基于学习的方法来解决这个任务。SVEN利用属性特定的连续向量来引导程序生成达到给定的属性，而不修改LM的权重。我们的训练过程通过可微分的投影损失来优化这些连续向量，实现端到端的训练。此外，我们使用SVEN进行对抗测试，并表明当前的LM容易受到攻击，在测试时修改它们的输入而保留功能。我们的工作强调需要在LM的培训和评估中考虑安全因素。

    Large language models (LMs) are increasingly pretrained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous ve
    
[^146]: Deep-OSG：半群中的运算符的深度学习方法

    Deep-OSG: Deep Learning of Operators in Semigroup. (arXiv:2302.03358v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03358](http://arxiv.org/abs/2302.03358)

    本文提出了一种深度学习方法，用于学习半群中的运算符，可以将未知自主动力系统建模为时间序列数据，在不同时间滞后下收集。这种方法能够学习具有可变时间步长的演化算符，构成自主系统的半群。

    

    本文提出了一种新颖的深度学习方法，用于学习半群中的运算符，并应用于使用在不同时间滞后下收集的时间序列数据对未知自主动力系统进行建模。本文是之前流图学习(FML)工作的续集，该工作主要集中在学习具有固定时间步长的单一演化算符。本文旨在学习一族具有可变时间步长的演化算符，构成自主系统的半群。半群性质对于联系系统在不同时间尺度上的演化行为非常重要，但在以前的研究中没有考虑到这一点。我们首次提出了将半群性质嵌入数据驱动学习过程的框架，通过一种新颖的神经网络

    This paper proposes a novel deep learning approach for learning operators in semigroup, with applications to modeling unknown autonomous dynamical systems using time series data collected at varied time lags. It is a sequel to the previous flow map learning (FML) works [T. Qin, K. Wu, and D. Xiu, J. Comput. Phys., 395:620--635, 2019], [K. Wu and D. Xiu, J. Comput. Phys., 408:109307, 2020], and [Z. Chen, V. Churchill, K. Wu, and D. Xiu, J. Comput. Phys., 449:110782, 2022], which focused on learning single evolution operator with a fixed time step. This paper aims to learn a family of evolution operators with variable time steps, which constitute a semigroup for an autonomous system. The semigroup property is very crucial and links the system's evolutionary behaviors across varying time scales, but it was not considered in the previous works. We propose for the first time a framework of embedding the semigroup property into the data-driven learning process, through a novel neural network
    
[^147]: 使用扩散模型的语义引导生成图像增强方法用于图像分类

    Semantic-Guided Generative Image Augmentation Method with Diffusion Models for Image Classification. (arXiv:2302.02070v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.02070](http://arxiv.org/abs/2302.02070)

    SGID是一种使用扩散模型的语义引导生成图像增强方法，旨在在图像分类中平衡图像多样性和语义一致性。实验证明，SGID在ResNet-5上的效果优于最佳增强基线1.72％。

    

    现有的图像增强方法分为两类：扰动方法和生成方法。扰动方法对原始图像应用预定义的扰动来增强图像，但只是局部变化图像，因此缺乏图像多样性。相反，生成方法在增强图像中带来更多的图像多样性，但可能无法保持语义一致性，从而错误地改变了原始图像的基本语义。为了在增强图像中平衡图像多样性和语义一致性，我们提出了SGID，一种语义引导生成图像增强方法，使用扩散模型用于图像分类。具体而言，SGID采用扩散模型生成具有良好图像多样性的增强图像。更重要的是，SGID以图像标签和标题作为指导，以保持增强和原始图像之间的语义一致性。实验证明，SGID在ResNet-5上的表现优于最佳增强基线1.72％。

    Existing image augmentation methods consist of two categories: perturbation-based methods and generative methods. Perturbation-based methods apply pre-defined perturbations to augment an original image, but only locally vary the image, thus lacking image diversity. In contrast, generative methods bring more image diversity in the augmented images but may not preserve semantic consistency, thus incorrectly changing the essential semantics of the original image. To balance image diversity and semantic consistency in augmented images, we propose SGID, a Semantic-guided Generative Image augmentation method with Diffusion models for image classification. Specifically, SGID employs diffusion models to generate augmented images with good image diversity. More importantly, SGID takes image labels and captions as guidance to maintain semantic consistency between the augmented and original images. Experimental results show that SGID outperforms the best augmentation baseline by 1.72% on ResNet-5
    
[^148]: 无需模型估计的鲁棒马尔科夫决策过程

    Robust Markov Decision Processes without Model Estimation. (arXiv:2302.01248v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.01248](http://arxiv.org/abs/2302.01248)

    这篇论文提出了一种无需模型估计的鲁棒MDPs算法，通过将原始问题转化为另一种形式，并使用随机梯度方法求解，从而去除了对优化器的依赖。

    

    鲁棒马尔科夫决策过程（MDPs）在学习一个对环境变化不敏感的鲁棒策略方面受到了广泛关注。目前有越来越多的工作分析鲁棒MDPs的采样效率。然而，在实际应用中应用鲁棒MDPs存在两个主要障碍。首先，大多数工作都是在模型为基础的情况下研究鲁棒MDPs，其中转移概率需要进行估计，需要大量的记忆（O(|S|²|A|)）。其次，之前的工作通常假设存在一个强大的优化器来获得最优解，用作解决鲁棒MDPs的中间步骤。然而，在实践中，通常并不存在这样的优化器。为了去除优化器的依赖，我们将原始的鲁棒MDPs转化为另一种形式，使我们能够使用随机梯度方法来求解鲁棒MDPs。此外，我们证明了这种替代形式仍然具有类似的作用。通过这种新的公式，我们设计了一种采样有效的算法来解决鲁棒MDPs。

    Robust Markov Decision Processes (MDPs) are receiving much attention in learning a robust policy which is less sensitive to environment changes. There are an increasing number of works analyzing sample-efficiency of robust MDPs. However, there are two major barriers to applying robust MDPs in practice. First, most works study robust MDPs in a model-based regime, where the transition probability needs to be estimated and requires a large amount of memories $\mathcal{O}(|\mathcal{S}|^2|\mathcal{A}|)$. Second, prior work typically assumes a strong oracle to obtain the optimal solution as an intermediate step to solve robust MDPs. However, in practice, such an oracle does not exist usually. To remove the oracle, we transform the original robust MDPs into an alternative form, which allows us to use stochastic gradient methods to solve the robust MDPs. Moreover, we prove the alternative form still plays a similar role as the original form. With this new formulation, we devise a sample-effici
    
[^149]: 对出行方式选择建模的机器学习方法进行预测和行为分析

    A prediction and behavioural analysis of machine learning methods for modelling travel mode choice. (arXiv:2301.04404v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04404](http://arxiv.org/abs/2301.04404)

    本研究分析了各种机器学习方法和传统随机效用模型在建模出行方式选择方面的性能和特点，并确定哪些模型最适合于建模，出行行为可解释性和解释性、计算复杂度和数据效率等因素需要进行整体考虑。

    

    为了出行方式选择问题，各种机器学习方法的出现提出了一个有趣的问题：哪些模型适用于哪些应用？这个问题的答案不仅仅在于简单的预测性能，还涉及行为可解释性和解释性、计算复杂度和数据效率等多方面的平衡。许多研究试图比较不同机器学习分类器与传统随机效用模型的预测性能。然而，现有研究通常只分析离散预测性能，忽略其他影响模型选择的方面。此外，许多研究受技术限制的影响，例如使用不恰当的验证方案、分层数据的错误抽样、缺乏外部验证以及仅使用离散度量等。为了解决这些限制，我们进行了系统性的行为分析，旨在比较不同机器学习方法的表现和特点，以便确定最适合建模出行选择的模型类型。

    The emergence of a variety of Machine Learning (ML) approaches for travel mode choice prediction poses an interesting question to transport modellers: which models should be used for which applications? The answer to this question goes beyond simple predictive performance, and is instead a balance of many factors, including behavioural interpretability and explainability, computational complexity, and data efficiency. There is a growing body of research which attempts to compare the predictive performance of different ML classifiers with classical random utility models. However, existing studies typically analyse only the disaggregate predictive performance, ignoring other aspects affecting model choice. Furthermore, many studies are affected by technical limitations, such as the use of inappropriate validation schemes, incorrect sampling for hierarchical data, lack of external validation, and the exclusive use of discrete metrics. We address these limitations by conducting a systemati
    
[^150]: StyleDomain：用于单次和少次领域适应的StyleGAN高效轻量化参数化

    StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation. (arXiv:2212.10229v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.10229](http://arxiv.org/abs/2212.10229)

    本文对GAN的领域适应问题进行了深入研究并提出了StyleGAN用于领域适应的新的高效轻量级参数化方案。

    

    GAN的领域适应是fine-tuning在大规模数据集上预训练的先进GAN模型（例如StyleGAN）以适应具有少量样本的特定领域（例如绘画面孔、素描等）。本文对GAN的领域适应问题（特别是StyleGAN模型）进行系统和深入的分析，并提出了StyleGAN用于领域适应的新的高效轻量级参数化方案。

    Domain adaptation of GANs is a problem of fine-tuning the state-of-the-art GAN models (e.g. StyleGAN) pretrained on a large dataset to a specific domain with few samples (e.g. painting faces, sketches, etc.). While there are a great number of methods that tackle this problem in different ways, there are still many important questions that remain unanswered.  In this paper, we provide a systematic and in-depth analysis of the domain adaptation problem of GANs, focusing on the StyleGAN model. First, we perform a detailed exploration of the most important parts of StyleGAN that are responsible for adapting the generator to a new domain depending on the similarity between the source and target domains. As a result of this in-depth study, we propose new efficient and lightweight parameterizations of StyleGAN for domain adaptation. Particularly, we show there exist directions in StyleSpace (StyleDomain directions) that are sufficient for adapting to similar domains and they can be reduced fu
    
[^151]: 生物医学图像分析竞赛：当前参与实践的现状

    Biomedical image analysis competitions: The state of current participation practice. (arXiv:2212.08568v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.08568](http://arxiv.org/abs/2212.08568)

    本研究调查了生物医学图像分析竞赛的现状和参与实践。调查结果显示，参与者主要动机是知识交流，而奖金的获得只起到次要作用。

    

    在各个机器学习（ML）研究和实践领域，国际基准竞赛的数量在稳步增加。然而，迄今为止，对于社区在应对提出的研究问题时的常见实践和遇到的瓶颈了解甚少。为了揭示生物医学图像分析领域算法开发的现状，我们设计了一项国际调查，该调查针对与IEEE ISBI 2021和MICCAI 2021会议（总共80个竞赛）相关的挑战的所有参与者进行。调查涵盖了参与者的专业知识和工作环境、他们选择的策略以及算法特性。其中有中位数为72%的竞赛参与者参与了这项调查。根据我们的结果，知识交流是参与的主要动机（70%），而奖金的获得只起到次要作用（16%）。参与者平均花费80个工作小时进行算法开发。

    The number of international benchmarking competitions is steadily increasing in various fields of machine learning (ML) research and practice. So far, however, little is known about the common practice as well as bottlenecks faced by the community in tackling the research questions posed. To shed light on the status quo of algorithm development in the specific field of biomedical imaging analysis, we designed an international survey that was issued to all participants of challenges conducted in conjunction with the IEEE ISBI 2021 and MICCAI 2021 conferences (80 competitions in total). The survey covered participants' expertise and working environments, their chosen strategies, as well as algorithm characteristics. A median of 72% challenge participants took part in the survey. According to our results, knowledge exchange was the primary incentive (70%) for participation, while the reception of prize money played only a minor role (16%). While a median of 80 working hours was spent on m
    
[^152]: ROSCOE: 用于评分逐步推理的一套度量指标

    ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. (arXiv:2212.07919v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.07919](http://arxiv.org/abs/2212.07919)

    ROSCOE是一套度量指标，用于评分逐步推理的正确性和质量。它可以衡量语义一致性、逻辑性、信息量、流畅度和事实等特征，并提供可解释的评估方法。

    

    当大型语言模型被要求生成逐步推理来解释其最终答案时，它们在下游任务性能上显示出了改进。这些推理步骤大大提高了模型的可解释性和验证性，但在没有可靠的自动评估方法的情况下，独立于最终答案来研究它们的正确性是困难的。我们并不知道所述的推理步骤实际上有多少支持最终任务预测结果。在这项工作中，我们提出了ROSCOE，这是一套可解释的无监督自动评分指标，它改进并扩展了先前的文本生成评估指标。为了评估ROSCOE与基线指标的差异，我们设计了一种推理错误的分类，并在常用的推理数据集上收集了合成和人工评估得分。与现有指标相比，ROSCOE可以通过利用逐步推理的特性来衡量语义一致性、逻辑性、信息量、流畅度和事实等特征。

    Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rat
    
[^153]: 计算效率高的强化学习：基于简单规则的有针对性探索

    Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules. (arXiv:2211.16691v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16691](http://arxiv.org/abs/2211.16691)

    本研究提出了一种基于简单规则的有针对性探索方法，通过避免已知子优的状态-动作空间区域来更快地加速强化学习代理程序的收敛，并在一个房间温度控制案例研究中实现了比传统方法快6-7倍的速度收敛。

    

    强化学习通常由于需要穷举探索状态-动作空间以找到表现良好的策略而导致样本复杂度不太好。然而，我们认为系统的专家知识通常允许我们设计简单规则，我们期望良好的策略始终遵循这些规则。因此，在本研究中，我们提出了一种简单而有效的连续演员-评论家框架的修改版本，以纳入这些规则并避免已知子优的状态-动作空间区域，从而显着加速强化学习代理程序的改进。具体而言，如果代理程序选择的动作不符合我们的直觉，我们会饱和这些动作，关键是修改策略的梯度更新步骤，以确保学习流程不受饱和步骤的影响。在一个房间温度控制案例研究中，它使代理程序以比传统代理程序快6-7倍的速度收敛到表现良好的策略，而不需要消耗额外的计算资源。

    Reinforcement Learning (RL) generally suffers from poor sample complexity, mostly due to the need to exhaustively explore the state-action space to find well-performing policies. On the other hand, we postulate that expert knowledge of the system often allows us to design simple rules we expect good policies to follow at all times. In this work, we hence propose a simple yet effective modification of continuous actor-critic frameworks to incorporate such rules and avoid regions of the state-action space that are known to be suboptimal, thereby significantly accelerating the convergence of RL agents. Concretely, we saturate the actions chosen by the agent if they do not comply with our intuition and, critically, modify the gradient update step of the policy to ensure the learning process is not affected by the saturation step. On a room temperature control case study, it allows agents to converge to well-performing policies up to 6-7x faster than classical agents without computational o
    
[^154]: 深度图聚类综述：分类、挑战、应用和开放资源

    A Survey of Deep Graph Clustering: Taxonomy, Challenge, Application, and Open Resource. (arXiv:2211.12875v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12875](http://arxiv.org/abs/2211.12875)

    在这篇论文中，作者对深度图聚类进行了综述研究。首先介绍了该领域的定义、评估和发展，然后提出了深度图聚类方法的分类学，并对现有方法进行了分析，总结出了挑战和机会。

    

    图聚类旨在将图中的节点划分为若干不同的簇，是一项基础且具有挑战性的任务。近年来，借助深度学习强大的表示能力，深度图聚类方法取得了巨大成功。然而，对于这个领域的综述论文相对较少，综述该领域的工作势在必行。基于此动机，我们进行了深度图聚类的综述研究。首先，我们介绍了该领域的公式化定义、评估和发展。其次，基于图类型、网络架构、学习范式和聚类方法等四个不同的标准，提出了深度图聚类方法的分类学。第三，我们通过广泛的实验对现有方法进行了详细分析，并从图数据质量、稳定性、可扩展性、辨别能力和未知簇数等五个角度总结了挑战和机会。

    Graph clustering, which aims to divide nodes in the graph into several distinct clusters, is a fundamental yet challenging task. Benefiting from the powerful representation capability of deep learning, deep graph clustering methods have achieved great success in recent years. However, the corresponding survey paper is relatively scarce, and it is imminent to make a summary of this field. From this motivation, we conduct a comprehensive survey of deep graph clustering. Firstly, we introduce formulaic definition, evaluation, and development in this field. Secondly, the taxonomy of deep graph clustering methods is presented based on four different criteria, including graph type, network architecture, learning paradigm, and clustering method. Thirdly, we carefully analyze the existing methods via extensive experiments and summarize the challenges and opportunities from five perspectives, including graph data quality, stability, scalability, discriminative capability, and unknown cluster nu
    
[^155]: 全新的K-FACs：利用在线分解更新加速K-FAC

    Brand New K-FACs: Speeding up K-FAC with Online Decomposition Updates. (arXiv:2210.08494v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08494](http://arxiv.org/abs/2210.08494)

    本文提出了全新的K-FACs算法，利用在线分解更新来加速K-FAC的计算速度。这种算法比传统的K-FAC和RS-KFAC更廉价，虽然精度较低，但在预调节部分的计算复杂度上只有线性缩放。

    

    K-FAC是Deep Learning中可行的Natural Gradient的实现，其瓶颈在于计算所谓的“Kronecker-Factors”（K-factors）的逆。RS-KFAC是K-FAC的改进，提供了估计K-factors逆的一种廉价方法。本文利用K-factors的指数平均构造范式，并使用在线数值线性代数技术，提出了一种更为廉价（但精度较低）的估计K-factors逆的方法。特别地，我们提出了一个在层大小上线性缩放的K-factor逆更新方法。我们还提出了一个在逆应用过程中也线性缩放的方法（K-FAC的逆应用过程是三次方缩放的，RS-KFAC的是二次缩放的）。总体上，我们提出的算法提供了一个近似的K-FAC实现，其预调节部分在层大小上线性缩放（与K-FAC的三次方和RS-KFAC的二次方相比）。

    K-FAC (arXiv:1503.05671, arXiv:1602.01407) is a tractable implementation of Natural Gradient (NG) for Deep Learning (DL), whose bottleneck is computing the inverses of the so-called ``Kronecker-Factors'' (K-factors). RS-KFAC (arXiv:2206.15397) is a K-FAC improvement which provides a cheap way of estimating the K-factors inverses.  In this paper, we exploit the exponential-average construction paradigm of the K-factors, and use online numerical linear algebra techniques to propose an even cheaper (but less accurate) way of estimating the K-factors inverses. In particular, we propose a K-factor inverse update which scales linearly in layer size. We also propose an inverse application procedure which scales linearly as well (the one of K-FAC scales cubically and the one of RS-KFAC scales quadratically). Overall, our proposed algorithm gives an approximate K-FAC implementation whose preconditioning part scales linearly in layer size (compare to cubic for K-FAC and quadratic for RS-KFAC). I
    
[^156]: 公平性和鲁棒性在反因果预测中的应用

    Fairness and robustness in anti-causal prediction. (arXiv:2209.09423v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.09423](http://arxiv.org/abs/2209.09423)

    这项研究通过因果关系的视角探讨了反因果预测中公平性和鲁棒性的关系，并提出了将分离准则应用于反因果设置的新动机。该研究还发现，以鲁棒性为驱动的方法可以用来实现分离，且通常比直接设计公平性方法的方法更有效。

    

    鲁棒性和公平性作为现代机器学习模型的重要要求，独立地出现。虽然这两个要求似乎相关，但在实践中它们之间的连接常常不清楚。在本文中，我们通过因果关系的视角讨论这些连接，重点关注反因果预测任务，其中分类器的输入（例如图像）被假定为由目标标签和受保护属性的函数生成。通过采取这种视角，我们明确了一个常见的公平性准则 - 分离性 - 与一种常见的鲁棒性概念 - 风险不变性之间的联系。这些连接为在反因果设置中应用分离准则提供了新的动机，并为关于公平性-性能权衡的旧讨论提供了信息。此外，我们的研究结果表明，鲁棒性驱动的方法可以用来实现分离，并且通常比直接设计公平性方法的方法在实践中表现更好。

    Robustness to distribution shift and fairness have independently emerged as two important desiderata required of modern machine learning models. While these two desiderata seem related, the connection between them is often unclear in practice. Here, we discuss these connections through a causal lens, focusing on anti-causal prediction tasks, where the input to a classifier (e.g., an image) is assumed to be generated as a function of the target label and the protected attribute. By taking this perspective, we draw explicit connections between a common fairness criterion - separation - and a common notion of robustness - risk invariance. These connections provide new motivation for applying the separation criterion in anticausal settings, and inform old discussions regarding fairness-performance tradeoffs. In addition, our findings suggest that robustness-motivated approaches can be used to enforce separation, and that they often work better in practice than methods designed to directly 
    
[^157]: 从潜在动力学到有意义的表示法

    From latent dynamics to meaningful representations. (arXiv:2209.00905v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.00905](http://arxiv.org/abs/2209.00905)

    本文提出了一个动力学约束的表示学习框架，通过限制潜在表示遵循特定动态规律以使得学习到的表示具有意义。在真实世界的DNA荧光电影数据集上验证了该算法，表明其可以准确地学习动态规律，并获得有意义的表示。

    

    虽然表示学习已成为机器学习和人工智能崛起的核心，但一个关键问题仍然是使学习到的表示具有意义。为此，典型的方法是通过先验概率分布来规范学习到的表示。然而，这样的先验通常是不可用或临时的。为了解决这个问题，我们提出了一个动力学约束的表示学习框架。我们不使用预定义的概率，而是限制潜在表示遵循特定的动态规律，这是动态系统表示学习更自然的约束。我们的信仰源于物理学中的一个基本观察，即虽然不同的系统可以有不同的边际概率分布，但通常遵循相同的动态规律，例如牛顿和薛定谔方程。我们对不同系统验证了我们的框架，包括一个真实世界的荧光DNA电影数据集。我们展示了我们的算法可以准确地学习动态规律，并获得有意义的表示。

    While representation learning has been central to the rise of machine learning and artificial intelligence, a key problem remains in making the learnt representations meaningful. For this the typical approach is to regularize the learned representation through prior probability distributions. However such priors are usually unavailable or ad hoc. To deal with this, we propose a dynamics-constrained representation learning framework. Instead of using predefined probabilities, we restrict the latent representation to follow specific dynamics, which is a more natural constraint for representation learning in dynamical systems. Our belief stems from a fundamental observation in physics that though different systems can have different marginalized probability distributions, they typically obey the same dynamics, such as Newton's and Schrodinger's equations. We validate our framework for different systems including a real-world fluorescent DNA movie dataset. We show that our algorithm can un
    
[^158]: GEDI:一种基于图的端到端数据插补框架

    GEDI: A Graph-based End-to-end Data Imputation Framework. (arXiv:2208.06573v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.06573](http://arxiv.org/abs/2208.06573)

    GEDI是一种基于图的端到端数据插补框架，通过使用Transformer网络和图结构学习来改进特征之间的上下文关系和观测之间的相似性，以及使用元学习来选择对下游标签预测任务有影响力的特征。实验证明，该框架在各种基准方法上都能提高插补和标签预测性能。

    

    数据插补是在实际应用中常见的处理缺失数据的有效方法。本研究提出并测试了一种新颖的数据插补过程，实现了两个重要目标：（1）保持特征矩阵中观测之间的行相似性和特征之间的列上下文关系，（2）将插补过程调整为特定的下游标签预测任务。所提出的插补过程使用Transformer神经网络和图结构学习来迭代地改进特征之间的上下文关系和观测之间的相似性。此外，它使用元学习框架来选择对下游感兴趣的预测任务有影响力的特征。我们在真实世界的大型数据集上进行实验证明，所提出的插补过程在各种基准方法上始终提高了插补和标签预测性能。

    Data imputation is an effective way to handle missing data, which is common in practical applications. In this study, we propose and test a novel data imputation process that achieve two important goals: (1) preserve the row-wise similarities among observations and column-wise contextual relationships among features in the feature matrix, and (2) tailor the imputation process to specific downstream label prediction task. The proposed imputation process uses Transformer network and graph structure learning to iteratively refine the contextual relationships among features and similarities among observations. Moreover, it uses a meta-learning framework to select features that are influential to the downstream prediction task of interest. We conduct experiments on real-world large data sets, and show that the proposed imputation process consistently improves imputation and label prediction performance over a variety of benchmark methods.
    
[^159]: 图上的多尺度Wasserstein最短路径过滤核心

    Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs. (arXiv:2206.00979v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00979](http://arxiv.org/abs/2206.00979)

    这篇论文提出了一种名为多尺度Wasserstein最短路径过滤图核心（MWSPF）的新型最短路径图核心，解决了传统核心的信息丢失和缺乏多个尺度考虑的问题。

    

    传统的最短路径图核心（SP）是最受欢迎的图核心之一。它将图分解为最短路径，并计算每个图中最短路径的频率。然而，SP面临两个主要挑战：首先，最短路径的三元表示失去了信息。其次，SP比较图时没有考虑到图结构的多个不同尺度，而这在现实世界的图中很常见，例如社交网络中的链状结构、环状结构和星状结构。为了克服这两个挑战，我们开发了一种新颖的最短路径图核心，称为多尺度Wasserstein最短路径过滤图核心（MWSPF）。它使用以每个顶点为根的某个深度的BFS树来限制考虑最短路径的最大长度，考虑到小世界特性。它考虑了最短路径中所有顶点的标签。为了方便在多个不同尺度上比较图，它从顶点和

    The traditional shortest-path graph kernel (SP) is one of the most popular graph kernels. It decomposes graphs into shortest paths and computes their frequencies in each graph. However, SP has two main challenges: Firstly, the triplet representation of the shortest path loses information. Secondly, SP compares graphs without considering the multiple different scales of the graph structure which is common in real-world graphs, e.g., the chain-, ring-, and star-structures in social networks. To overcome these two challenges, we develop a novel shortest-path graph kernel called the Multi-scale Wasserstein Shortest-Path Filtration graph kernel (MWSPF). It uses a BFS tree of a certain depth rooted at each vertex to restrict the maximum length of the shortest path considering the small world property. It considers the labels of all the vertices in the shortest path. To facilitate the comparison of graphs at multiple different scales, it augments graphs from both the aspects of the vertex and
    
[^160]: 具有异构学习率的PSO卷积神经网络

    PSO-Convolutional Neural Networks with Heterogeneous Learning Rate. (arXiv:2205.10456v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.10456](http://arxiv.org/abs/2205.10456)

    本论文提出了一种基于粒子群优化的PSO卷积神经网络训练方法，通过在训练的不同阶段实现卷积神经网络之间的协同学习，提高了训练性能和泛化能力。

    

    卷积神经网络（ConvNets或CNN）在计算机视觉和相关领域中已被广泛应用。然而，这些神经网络训练的动态仍然难以捉摸：训练它们是困难且计算开销大的。为了解决这个挑战并解决图像处理中的几个问题，如语音、图像和动作识别以及物体检测，已经提出了许多架构和训练策略。在本文中，我们提出了一种基于粒子群优化（PSO）的ConvNets训练方法。在这种框架中，每个ConvNet的权重向量通常被视为相位空间中粒子的位置，其中PSO协同动力学与随机梯度下降（SGD）相结合，以提高训练性能和泛化能力。我们的方法如下：i) [常规阶段]每个ConvNet通过SGD独立训练；ii) [协同阶段]ConvNets之间共享学习。

    Convolutional Neural Networks (ConvNets or CNNs) have been candidly deployed in the scope of computer vision and related fields. Nevertheless, the dynamics of training of these neural networks lie still elusive: it is hard and computationally expensive to train them. A myriad of architectures and training strategies have been proposed to overcome this challenge and address several problems in image processing such as speech, image and action recognition as well as object detection. In this article, we propose a novel Particle Swarm Optimization (PSO) based training for ConvNets. In such framework, the vector of weights of each ConvNet is typically cast as the position of a particle in phase space whereby PSO collaborative dynamics intertwines with Stochastic Gradient Descent (SGD) in order to boost training performance and generalization. Our approach goes as follows: i) [regular phase] each ConvNet is trained independently via SGD; ii) [collaborative phase] ConvNets share among themse
    
[^161]: 深度神经网络的可解释性方法和扰动伪影的评估

    Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks. (arXiv:2203.02928v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.02928](http://arxiv.org/abs/2203.02928)

    本文评估了几种常见的深度神经网络可解释性方法，展示了扰动伪影对可解释性方法评估的影响，强调在评估中需要考虑伪影的存在。

    

    深度神经网络在图像分类、检测和预测方面表现出色，但如何解释其决策仍然是一个未解决的问题，因此出现了许多可解释性方法。评估这些方法是一个重要的挑战，其中一种流行的方法是通过扰动输入特征来评估可解释性方法，但是扰动本身可能会引入伪影。本文提出了一种估计伪影影响的方法，使用此方法评估了几种流行的可解释性方法在不同数据集上的表现，并展示了扰动伪影对可解释性方法评估的影响。我们的结果突出了在评估可解释性方法时考虑伪影存在的重要性。

    Despite excellent performance of deep neural networks (DNNs) in image classification, detection, and prediction, characterizing how DNNs make a given decision remains an open problem, resulting in a number of interpretability methods. Post-hoc interpretability methods primarily aim to quantify the importance of input features with respect to the class probabilities. However, due to the lack of ground truth and the existence of interpretability methods with diverse operating characteristics, evaluating these methods is a crucial challenge. A popular approach to evaluate interpretability methods is to perturb input features deemed important for a given prediction and observe the decrease in accuracy. However, perturbation itself may introduce artifacts. We propose a method for estimating the impact of such artifacts on the fidelity estimation by utilizing model accuracy curves from perturbing input features according to the Most Import First (MIF) and Least Import First (LIF) orders. Usi
    
[^162]: 使用区分特征度量下游分类的自监督表示质量

    Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features. (arXiv:2203.01881v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01881](http://arxiv.org/abs/2203.01881)

    从自监督学习模型中提取区分特征，并使用它们压缩表示空间，提出了一种Q-Score自监督表示质量分数，可以可靠地预测线性评估期间的错误分类。

    

    自监督学习在下游分类任务中展现出了惊人的结果。然而，对于它们的失败模式和学习表示的解释，存在着有限的研究。本文研究了 SimCLR、SwaV、MoCo、BYOL、DINO、SimSiam、VICReg 和 Barlow Twins 等最先进的自监督模型的表示空间。在不使用类标签信息的情况下，我们发现了对应于图像中独特物理属性的区分特征，这些区分特征主要存在于正确分类的表示中。使用这些特征，我们可以将表示空间压缩多达 40%，而不会显著影响线性分类性能。然后，我们提出了自监督表示质量分数（或 Q-Score），这是一种模型无关、无监督的分数，可以可靠地预测一个给定样本在线性评估期间是否可能被错误分类，并在 ImageNet-100 和 ImageNet-1K 上实现了 AUPRC 分别为 91.45 和 78.78。

    Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score
    
[^163]: 通过与贝叶斯非参数混合模型的联系，使用Neyman-Scott过程进行时空聚类

    Spatiotemporal Clustering with Neyman-Scott Processes via Connections to Bayesian Nonparametric Mixture Models. (arXiv:2201.05044v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2201.05044](http://arxiv.org/abs/2201.05044)

    这篇论文介绍了Neyman-Scott过程（NSPs）和贝叶斯非参数混合模型（DPMM）之间的新颖联系，并探讨了NSP在时空数据建模中的应用。

    

    Neyman-Scott过程（NSPs）是生成时间或空间中点簇的点过程模型。它们是一种适用于广泛现象的自然模型，从神经脉冲序列到文档流。聚类属性是通过双重随机公式实现的：首先，从泊松过程中绘制一组潜在事件；然后，每个潜在事件根据另一个泊松过程生成一组观测数据点。这个结构类似于贝叶斯非参数混合模型，如狄利克雷过程混合模型（DPMM），其中潜在事件（即簇）的数量是一个随机变量，但点过程的构造使得NSP特别适合于建模时空数据。虽然为DPMM开发了许多专门算法，但相对较少的工作集中在NSP的推断上。在这里，我们介绍了NSP与DPMM之间的新颖联系，关键的连接是第三类贝叶斯混合模型

    Neyman-Scott processes (NSPs) are point process models that generate clusters of points in time or space. They are natural models for a wide range of phenomena, ranging from neural spike trains to document streams. The clustering property is achieved via a doubly stochastic formulation: first, a set of latent events is drawn from a Poisson process; then, each latent event generates a set of observed data points according to another Poisson process. This construction is similar to Bayesian nonparametric mixture models like the Dirichlet process mixture model (DPMM) in that the number of latent events (i.e. clusters) is a random variable, but the point process formulation makes the NSP especially well suited to modeling spatiotemporal data. While many specialized algorithms have been developed for DPMMs, comparatively fewer works have focused on inference in NSPs. Here, we present novel connections between NSPs and DPMMs, with the key link being a third class of Bayesian mixture models c
    
[^164]: 《推断解释的公理聚合》

    Axiomatic Aggregations of Abductive Explanations. (arXiv:2109.03890v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.03890](http://arxiv.org/abs/2109.03890)

    本论文提出了三种聚合方法，将各种可能的推断解释聚合成特征重要性分数，解决了推断解释中多个有效解释的问题。这些方法基于合作博弈理论的权力指数和已知的因果强度度量。

    

    对后续模型逼近解释方法（如LIME和SHAP）的鲁棒性的近期批评导致了模型精确的推断解释的兴起。对于每个数据点，推断解释提供了一个足以生成结果的最小子集特征。尽管在理论上是严格和可靠的，但推断解释存在一个主要问题：同一数据点可以有多个有效的推断解释。在这种情况下，提供一个单一的推断解释可能是不足够的；另一方面，提供所有有效的推断解释可能由于其规模而难以理解。在这项工作中，我们通过将各种可能的推断解释聚合成特征重要性分数来解决这个问题。我们提出了三种聚合方法：两种基于合作博弈理论的权力指数方法和一种基于著名的因果强度度量的方法。我们从公理上对这三种方法进行了表征，证明每个方法都是良定义的且符合公理。

    The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
    
[^165]: 多设备合作边缘推理的任务导向通信

    Task-Oriented Communication for Multi-Device Cooperative Edge Inference. (arXiv:2109.00172v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2109.00172](http://arxiv.org/abs/2109.00172)

    本文研究了多设备合作边缘推理的任务导向通信，通过优化本地特征提取和分布式特征编码，实现低延迟的合作推理。

    

    本文研究了多设备合作边缘推理的任务导向通信，在这种场景下，一组分布式低端边缘设备将本地样本提取的特征传输到强大的边缘服务器进行推理。尽管合作边缘推理可以克服单个设备的有限感知能力，但它大大增加了通信开销并可能引起过大的延迟。为了实现低延迟的合作推理，我们提出了一种基于学习的通信方案，通过任务导向的方式优化本地特征提取和分布式特征编码，即去除数据冗余并传输对下游推理任务而言至关重要的信息，而不是在边缘服务器上重新构建数据样本。具体而言，我们利用信息瓶颈 (IB) 原理在每个边缘设备上提取任务相关特征，并采用分布式信息瓶颈 (DIB) 框架对单个样本的相关特征进行形式化处理。

    This paper investigates task-oriented communication for multi-device cooperative edge inference, where a group of distributed low-end edge devices transmit the extracted features of local samples to a powerful edge server for inference. While cooperative edge inference can overcome the limited sensing capability of a single device, it substantially increases the communication overhead and may incur excessive latency. To enable low-latency cooperative inference, we propose a learning-based communication scheme that optimizes local feature extraction and distributed feature encoding in a task-oriented manner, i.e., to remove data redundancy and transmit information that is essential for the downstream inference task rather than reconstructing the data samples at the edge server. Specifically, we leverage an information bottleneck (IB) principle to extract the task-relevant feature at each edge device and adopt a distributed information bottleneck (DIB) framework to formalize a single-let
    
[^166]: 《流形滤波器和流形神经网络的变形稳定性》

    Stability to Deformations of Manifold Filters and Manifold Neural Networks. (arXiv:2106.03725v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.03725](http://arxiv.org/abs/2106.03725)

    本文定义了流形滤波器和流形神经网络，并通过分析它们在流形变形下的稳定性，推广了图滤波器和标准卷积滤波器的已知稳定性性质。

    

    本文定义并研究了流形（M）卷积滤波器和神经网络（NN）。流形滤波器和MNN是基于拉普拉斯-贝尔特拉米算子指数定义的，并且在流形被采样时，可以恢复为图（G）滤波器和神经网络（NN）的离散近似。这些滤波器具有谱表示，它是图滤波器的谱表示和连续时间中标准卷积滤波器的频率响应的推广。本文的主要技术贡献是分析流形滤波器和MNN在流形光滑变形下的稳定性。这种分析推广了图滤波器和GNN的已知稳定性性质，并且也是连续时间中标准卷积滤波器和神经网络已知稳定性性质的推广。从这种分析中得出的最重要的观察是，流形滤波器和图滤波器一样，同时也具有稳定性。

    The paper defines and studies manifold (M) convolutional filters and neural networks (NNs). \emph{Manifold} filters and MNNs are defined in terms of the Laplace-Beltrami operator exponential and are such that \emph{graph} (G) filters and neural networks (NNs) are recovered as discrete approximations when the manifold is sampled. These filters admit a spectral representation which is a generalization of both the spectral representation of graph filters and the frequency response of standard convolutional filters in continuous time. The main technical contribution of the paper is to analyze the stability of manifold filters and MNNs to smooth deformations of the manifold. This analysis generalizes known stability properties of graph filters and GNNs and it is also a generalization of known stability properties of standard convolutional filters and neural networks in continuous time. The most important observation that follows from this analysis is that manifold filters, same as graph fil
    
[^167]: 图形Barlow Twins：一种用于图形的自监督表示学习框架

    Graph Barlow Twins: A self-supervised representation learning framework for graphs. (arXiv:2106.02466v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.02466](http://arxiv.org/abs/2106.02466)

    图形Barlow Twins是一种自监督表示学习框架，使用交叉相关的损失函数来学习图形表示，克服了负样本定义困难的问题。相比其他方法，它不依赖非对称神经网络结构，在需要较少超参数的情况下取得了竞争性的结果。

    

    自监督学习（SSL）范式是一个重要的探索领域，旨在消除昂贵的数据标注需求。尽管自监督学习方法在计算机视觉和自然语言处理领域取得了巨大成功，但大多数方法使用对比学习目标，需要难以定义的负样本。在图形的情况下，这更具挑战性，是实现稳健表示的瓶颈。为了克服这些限制，我们提出了一种自监督图形表示学习框架 - 图形Barlow Twins，它使用基于交叉相关的损失函数，而不是负样本。此外，与最先进的自监督图形表示学习方法BGRL相比，它不依赖非对称神经网络体系结构。我们展示了我们的方法在需要较少超参数和实质性的情况下取得了与最好的自监督方法和全监督方法相当的结果。

    The self-supervised learning (SSL) paradigm is an essential exploration area, which tries to eliminate the need for expensive data labeling. Despite the great success of SSL methods in computer vision and natural language processing, most of them employ contrastive learning objectives that require negative samples, which are hard to define. This becomes even more challenging in the case of graphs and is a bottleneck for achieving robust representations. To overcome such limitations, we propose a framework for self-supervised graph representation learning - Graph Barlow Twins, which utilizes a cross-correlation-based loss function instead of negative samples. Moreover, it does not rely on non-symmetric neural network architectures - in contrast to state-of-the-art self-supervised graph representation learning method BGRL. We show that our method achieves as competitive results as the best self-supervised methods and fully supervised ones while requiring fewer hyperparameters and substan
    
[^168]: 面向医学图像分类的隐私保护约束域泛化

    Privacy-Preserving Constrained Domain Generalization for Medical Image Classification. (arXiv:2105.08511v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.08511](http://arxiv.org/abs/2105.08511)

    该论文提出了一种隐私保护约束域泛化的方法，通过改进信息聚合过程来提高模型的泛化能力，并解决了由于数据隐私保护问题导致的医学影像分类的挑战。

    

    深度神经网络在医学影像应用中取得了前所未有的成功。然而，由于数据集有限和对患者隐私保护的严格法律和伦理要求，由大规模训练数据驱动的深度神经网络在医学影像分类的广泛应用受到了很大阻碍。在这篇论文中，我们旨在通过开发隐私保护约束域泛化方法来解决这个问题，以在隐私保护条件下提高泛化能力。具体而言，我们提出了一种新的梯度对齐损失，以改善集中式服务器端的信息聚合过程，期望训练的模型可以更好地泛化到“未见过”的数据。

    Deep neural networks (DNN) have demonstrated unprecedented success for medical imaging applications. However, due to the issue of limited dataset availability and the strict legal and ethical requirements for patient privacy protection, the broad applications of medical imaging classification driven by DNN with large-scale training data have been largely hindered. For example, when training the DNN from one domain (e.g., with data only from one hospital), the generalization capability to another domain (e.g., data from another hospital) could be largely lacking. In this paper, we aim to tackle this problem by developing the privacy-preserving constrained domain generalization method, aiming to improve the generalization capability under the privacy-preserving condition. In particular, We propose to improve the information aggregation process on the centralized server-side with a novel gradient alignment loss, expecting that the trained model can be better generalized to the "unseen" bu
    
[^169]: 回归任务中的离群样本检测：参数与预测器熵比较

    Out-of-distribution detection for regression tasks: parameter versus predictor entropy. (arXiv:2010.12995v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.12995](http://arxiv.org/abs/2010.12995)

    本研究针对回归任务中的离群样本检测进行了实证评估，发现通过学习多样的预测器可以估计新观测实例的认识不确定性，但参数的多样性并不一定能转化为预测器的多样性。

    

    对于机器学习模型来说，检测样本与训练样本相距太远时至关重要，这被称为离群样本（OOD）检测。对于神经网络而言，一种处理这个任务的方法是学习多样的预测器，这些预测器都能解释训练数据。这些信息可以用来估计新观测实例的认识不确定性，通过预测结果的不一致性来衡量。评估和认证方法检测OOD的能力需要指定在部署中可能发生但没有可用预测的实例。我们选择回归任务作为研究重点，在此任务中选择一个简单而有洞察力的模型来表示OOD分布，并对各种方法在区分OOD样本和数据中的能力进行实证评估。此外，我们还提供证据表明，参数的多样性可能无法转化为预测器的多样性。

    It is crucial to detect when an instance lies downright too far from the training samples for the machine learning model to be trusted, a challenge known as out-of-distribution (OOD) detection. For neural networks, one approach to this task consists of learning a diversity of predictors that all can explain the training data. This information can be used to estimate the epistemic uncertainty at a given newly observed instance in terms of a measure of the disagreement of the predictions. Evaluation and certification of the ability of a method to detect OOD require specifying instances which are likely to occur in deployment yet on which no prediction is available. Focusing on regression tasks, we choose a simple yet insightful model for this OOD distribution and conduct an empirical evaluation of the ability of various methods to discriminate OOD samples from the data. Moreover, we exhibit evidence that a diversity of parameters may fail to translate to a diversity of predictors. Based 
    
[^170]: GTAdam：带有自适应动量的梯度跟踪用于分布式在线优化

    GTAdam: Gradient Tracking with Adaptive Momentum for Distributed Online Optimization. (arXiv:2009.01745v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2009.01745](http://arxiv.org/abs/2009.01745)

    本文提出了一种名为GTAdam的分布式算法，通过梯度跟踪和自适应动量估计结合，解决了分布式在线优化问题，并在强凸成本函数的在线设置中提供了动态遗憾上界和静态设置下的线性收敛速度保证。

    

    本文涉及一种计算代理网络，旨在通过本地计算和通信的方式，在没有任何中央协调者的情况下，分布式地解决在线优化问题。我们提出了梯度跟踪与自适应动量估计（GTAdam）分布式算法，该算法将梯度跟踪机制与梯度的一阶和二阶动量估计相结合。算法在具有Lipschitz连续梯度的强凸成本函数的在线设置中进行分析。我们给出了与初始条件和目标函数的时间变化相关的动态遗憾的上界。此外，在静态设置中保证线性收敛速度。该算法在时间变化的分类问题、（移动）目标定位问题以及图像分类的随机优化设置中进行了测试。

    This paper deals with a network of computing agents aiming to solve an online optimization problem in a distributed fashion, i.e., by means of local computation and communication, without any central coordinator. We propose the gradient tracking with adaptive momentum estimation (GTAdam) distributed algorithm, which combines a gradient tracking mechanism with first and second order momentum estimates of the gradient. The algorithm is analyzed in the online setting for strongly convex cost functions with Lipschitz continuous gradients. We provide an upper bound for the dynamic regret given by a term related to the initial conditions and another term related to the temporal variations of the objective functions. Moreover, a linear convergence rate is guaranteed in the static setup. The algorithm is tested on a time-varying classification problem, on a (moving) target localization problem, and in a stochastic optimization setup from image classification. In these numerical experiments fro
    
[^171]: 分布鲁棒的批次情境强化学习

    Distributionally Robust Batch Contextual Bandits. (arXiv:2006.05630v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.05630](http://arxiv.org/abs/2006.05630)

    本文提出了一种方法，在不完整的观察数据下学习分布鲁棒的策略，通过引入策略评估过程和中心极限定理类型的保证，实现了针对最坏情况下的环境转变的策略学习。

    

    使用历史观察数据进行策略学习是一个重要的问题，已经在广泛的应用中得到应用。例如，选择向客户发送的优惠、价格、广告，以及选择给患者开具哪种药物。然而，现有的文献基于一个关键假设，即学习到的策略将被部署到的未来环境与生成数据的过去环境相同，而这个假设往往是错误的或者过于粗略的近似。在本文中，我们放宽了这个假设，并旨在学习一个具有不完整观察数据的分布鲁棒策略。我们首先提出了一个策略评估过程，以评估策略在最坏情况下的环境转变下的表现。然后，我们建立了这个提出的策略评估方案的中心极限定理类型的保证。利用这个评估方案，我们进一步提出了一种新颖的学习算法，能够学习一个策略。

    Policy learning using historical observational data is an important problem that has found widespread applications. Examples include selecting offers, prices, advertisements to send to customers, as well as selecting which medication to prescribe to a patient. However, existing literature rests on the crucial assumption that the future environment where the learned policy will be deployed is the same as the past environment that has generated the data -- an assumption that is often false or too coarse an approximation. In this paper, we lift this assumption and aim to learn a distributionally robust policy with incomplete observational data. We first present a policy evaluation procedure that allows us to assess how well the policy does under the worst-case environment shift. We then establish a central limit theorem type guarantee for this proposed policy evaluation scheme. Leveraging this evaluation scheme, we further propose a novel learning algorithm that is able to learn a policy 
    
[^172]: 马尔可夫链和混合时间的实证和实例依赖估计

    Empirical and Instance-Dependent Estimation of Markov Chain and Mixing Time. (arXiv:1912.06845v4 [math.PR] UPDATED)

    [http://arxiv.org/abs/1912.06845](http://arxiv.org/abs/1912.06845)

    我们提出了一种实证和实例依赖的方法来估计马尔可夫链的混合时间。我们基于收缩系数来估计混合时间，该系数能够控制混合时间直到强的普遍常数，并且适用于非可逆链。与现有方法相比，我们的方法计算更容易且置信区间更精确，还引入了一种新的分析方法来考虑转移矩阵的附加信息。

    

    我们解决了从单个观测轨迹估计马尔可夫链混合时间的问题。与大多数先前使用希尔伯特空间方法估计谱缺口的工作不同，我们选择采用基于总变差收缩的方法。具体地，我们估计了Wolfer [2020]中引入的收缩系数，受到Dobrushin的启发。与谱缺口不同，这个数量控制着混合时间直到强的普遍常数，并且适用于非可逆链。我们改进了现有的完全依赖数据的置信区间，这些区间比谱相关数量更容易计算且更薄。此外，我们通过利用关于转移矩阵的附加信息，引入了一种超过最坏情况分析的新方法。这使我们能够针对诱导均匀范数和一些混合属性，导出与矩阵估计有关的实例依赖的速率。

    We address the problem of estimating the mixing time of a Markov chain from a single trajectory of observations. Unlike most previous works which employed Hilbert space methods to estimate spectral gaps, we opt for an approach based on contraction with respect to total variation. Specifically, we estimate the contraction coefficient introduced in Wolfer [2020], inspired from Dobrushin's. This quantity, unlike the spectral gap, controls the mixing time up to strong universal constants and remains applicable to non-reversible chains. We improve existing fully data-dependent confidence intervals around this contraction coefficient, which are both easier to compute and thinner than spectral counterparts. Furthermore, we introduce a novel analysis beyond the worst-case scenario by leveraging additional information about the transition matrix. This allows us to derive instance-dependent rates for estimating the matrix with respect to the induced uniform norm, and some of its mixing propertie
    
[^173]: 多人赌博学习，从竞争到合作

    Multiplayer Bandit Learning, from Competition to Cooperation. (arXiv:1908.01135v3 [cs.GT] UPDATED)

    [http://arxiv.org/abs/1908.01135](http://arxiv.org/abs/1908.01135)

    这篇论文研究了多人赌博学习中竞争和合作对探索和利用权衡的影响，模型考虑了不同合作参数下玩家的效用函数，并使用Gittins指数简化了单人问题。

    

    随机多臂赌博模型捕捉到了探索和利用的权衡。我们研究了竞争和合作对这种权衡的影响。假设有k个臂和两名玩家，Alice和Bob。在每一轮中，每个玩家拉动一个臂，接收到相应的奖励，并观察到对方的选择但不知道他们的奖励。Alice的效用函数为$\Gamma_A + \lambda \Gamma_B$（Bob的效用函数类似），其中$\Gamma_A$是Alice的总奖励，$\lambda \in [-1, 1]$是合作参数。当$\lambda = -1$时，玩家在一个零和游戏中竞争；当$\lambda = 1$时，他们完全合作；当$\lambda = 0$时，他们是中立的：每个玩家的效用函数为他们自己的奖励。该模型与战略实验经济学文献中关于观察对方奖励的研究相关。使用折扣因子$\beta$，Gittins指数将单人问题简化为对一个带有先验$\mu$的有风险臂的比较。

    The stochastic multi-armed bandit model captures the tradeoff between exploration and exploitation. We study the effects of competition and cooperation on this tradeoff. Suppose there are $k$ arms and two players, Alice and Bob. In every round, each player pulls an arm, receives the resulting reward, and observes the choice of the other player but not their reward. Alice's utility is $\Gamma_A + \lambda \Gamma_B$ (and similarly for Bob), where $\Gamma_A$ is Alice's total reward and $\lambda \in [-1, 1]$ is a cooperation parameter. At $\lambda = -1$ the players are competing in a zero-sum game, at $\lambda = 1$, they are fully cooperating, and at $\lambda = 0$, they are neutral: each player's utility is their own reward. The model is related to the economics literature on strategic experimentation, where usually players observe each other's rewards.  With discount factor $\beta$, the Gittins index reduces the one-player problem to the comparison between a risky arm, with a prior $\mu$, 
    

