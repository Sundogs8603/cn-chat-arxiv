# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Enhancing Representation Learning on High-Dimensional, Small-Size Tabular Data: A Divide and Conquer Method with Ensembled VAEs.](http://arxiv.org/abs/2306.15661) | 这项研究使用了基于集成VAE的分而治之方法，在高维度、低样本数量的任务中学习到更好的潜在表示，提高了样本效率，并在下游分类任务中取得了更高的准确性。 |
| [^2] | [SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design.](http://arxiv.org/abs/2306.15656) | SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。 |
| [^3] | [Effective resistance in metric spaces.](http://arxiv.org/abs/2306.15649) | 本研究通过考虑区域间的有效电阻，而不是点对间的有效电阻，并根据每个区域的密度适当缩放边权重，解决了在度量空间中有效电阻收敛到微不足道量的问题。 |
| [^4] | [On the Usefulness of Synthetic Tabular Data Generation.](http://arxiv.org/abs/2306.15636) | 本研究对合成表格数据在数据共享、数据增强、类别平衡和数据汇总等用例中进行了基准测试，发现合成表格数据对于ML训练的实用性目前尚缺乏足够的证据。 |
| [^5] | [Asynchronous Algorithmic Alignment with Cocycles.](http://arxiv.org/abs/2306.15632) | 该论文提出了一种将节点状态更新和消息函数调用分离的数学框架，以实现异步计算，并以此作为基础，进行了异步算法和神经网络的对齐。 |
| [^6] | [Coupling parameter and particle dynamics for adaptive sampling in Neural Galerkin schemes.](http://arxiv.org/abs/2306.15630) | 本研究引入了神经Galerkin方案，通过使用自适应分布的数据来估计训练损失，以应对具有局部特征和高方差的输运主导的高维问题。粒子集合通过与解场的非线性参数化耦合的动力学演化来主动调整，以保持信息丰富。 |
| [^7] | [Machine-learning based noise characterization and correction on neutral atoms NISQ devices.](http://arxiv.org/abs/2306.15628) | 本研究基于机器学习技术，提出了两种方法来表征和校正中性原子NISQ设备上的噪声参数，为进一步提高计算结果的准确性提供了新的思路。 |
| [^8] | [LeanDojo: Theorem Proving with Retrieval-Augmented Language Models.](http://arxiv.org/abs/2306.15626) | 本文引入了LeanDojo，该工具通过提取Lean的数据，为定理证明研究提供了一个开放源代码的平台。利用LeanDojo的数据，开发了ReProver，它是第一个使用检索增强的语言模型的证明器，可以从庞大的数学库中选择命题，训练成本低，并且只需要一周的GPU训练时间。 |
| [^9] | [Value-aware Importance Weighting for Off-policy Reinforcement Learning.](http://arxiv.org/abs/2306.15625) | 本文提出了一种称为“价值感知重要性权重”的方法，用于校正离策略学习中的样本。这种方法可以降低重要性采样权重的方差并保持无偏性，从而提高实践中的稳定性和效果。 |
| [^10] | [SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating Reproducible Scenes.](http://arxiv.org/abs/2306.15620) | SCENEREPLICA是一个基于YCB对象的可重复性基准测试，用于评估现实世界中的机器人操纵能力。此基准测试易于重复并允许研究人员比较不同的技术和算法，有助于加快机器人操纵方法的发展。 |
| [^11] | [DCID: Deep Canonical Information Decomposition.](http://arxiv.org/abs/2306.15619) | DCID是一种用于识别一维目标变量之间共享信号的方法，传统方法在一元情况下效果不佳。我们提出了ICM评估指标，用于评估学习到的共享特征，并进一步提出了深度规范信息分解（DCID）方法。 |
| [^12] | [Learning Nonautonomous Systems via Dynamic Mode Decomposition.](http://arxiv.org/abs/2306.15618) | 通过动态模态分解，我们提出了一种学习未知非自治系统的数据驱动方法，并通过修改系统和降维插值构建参数化代理模型来解决时间相关库普曼算子的近似问题。 |
| [^13] | [Extending Context Window of Large Language Models via Positional Interpolation.](http://arxiv.org/abs/2306.15595) | 通过位置插值方法，我们可以在最小微调的情况下将RoPE-based预训练语言模型的上下文窗口扩展到最多32768，并在多个任务上获得强有力的实证结果。通过线性降低输入位置索引的大小，我们保持了扩展模型在原始上下文窗口内任务的质量。 |
| [^14] | [Learning to Sail Dynamic Networks: The MARLIN Reinforcement Learning Framework for Congestion Control in Tactical Environments.](http://arxiv.org/abs/2306.15591) | 本论文提出了一个强化学习框架MARLIN，用于在战术环境中进行拥塞控制。该框架利用准确且可并行化的仿真环境来模拟战术网络条件，并引入了适用于复杂场景中的代理的精细化强化学习公式和评估方法。通过训练MARLIN智能体在模拟条件下进行瓶颈链路转换的性能评估，我们证明了该框架的有效性。 |
| [^15] | [Optimizing Credit Limit Adjustments Under Adversarial Goals Using Reinforcement Learning.](http://arxiv.org/abs/2306.15585) | 本研究使用强化学习技术，通过平衡最大化投资组合收入和最小化准备金的对抗目标，自动化寻找最优信用卡额度调整策略。 |
| [^16] | [Approximate Message Passing for the Matrix Tensor Product Model.](http://arxiv.org/abs/2306.15580) | 本文提出了一种针对矩阵张量乘积模型的近似传递消息（AMP）算法，并分析了其性能。该算法通过权衡和组合多个估计来优化算法迭代过程。通过对非可分函数的AMP收敛定理和状态演化的研究，我们给出了恢复目标信号所需的必要和充分条件。该算法适用于多种类型的两两观测。 |
| [^17] | [PyBADS: Fast and robust black-box optimization in Python.](http://arxiv.org/abs/2306.15576) | PyBADS是Python中一种快速而稳健的黑盒优化算法，适用于解决目标函数粗糙、计算代价高、可能存在噪声且梯度信息不可用的困难优化问题。它支持高达20个连续输入参数的黑盒函数，并提供易于使用的Python接口。 |
| [^18] | [See Through the Fog: Curriculum Learning with Progressive Occlusion in Medical Imaging.](http://arxiv.org/abs/2306.15574) | 本文提出了一种基于课程学习的方法，用于训练深度学习模型有效处理医学图像中的遮挡情况。通过逐步引入遮挡，模型首先学习简单、可辨别的模式，然后逐渐理解更复杂的遮挡场景。 |
| [^19] | [Generating Elementary Integrable Expressions.](http://arxiv.org/abs/2306.15572) | 本文介绍了如何利用Risch算法生成初等可积表达式的数据集，并展示了这种方式生成的数据可以减轻早期方法中存在的一些缺陷。 |
| [^20] | [RansomAI: AI-powered Ransomware for Stealthy Encryption.](http://arxiv.org/abs/2306.15559) | 本文提出了一种名为RansomAI的基于人工智能的隐蔽加密勒索软件，它通过强化学习来适应加密行为，最小化被检测的可能性，并在加密时最大化损害功能。 |
| [^21] | [Simple Steps to Success: Axiomatics of Distance-Based Algorithmic Recourse.](http://arxiv.org/abs/2306.15557) | 我们提出了一种基于距离的算法补偿的新方法，通过在数据流形中提供用户可以采取的方向来改变其预测结果。该方法具有高效性、可证明的隐私和鲁棒性保证，并在实验证明上优于现有技术。 |
| [^22] | [A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms.](http://arxiv.org/abs/2306.15552) | 本综述调查了面向异构HPC平台的深度学习硬件加速器，包括GPU、TPU、FPGA、ASIC、神经处理单元和RISC-V等，同时也涵盖了新兴内存技术和计算范式。 |
| [^23] | [CrunchGPT: A chatGPT assisted framework for scientific machine learning.](http://arxiv.org/abs/2306.15551) | CrunchGPT是一个基于ChatGPT的科学机器学习辅助框架，通过简单的用户提示来协调整个科学机器学习的工作流程，实现无缝集成数据和物理知识，解决了SciML在预处理、问题建模、代码生成、后处理和分析等方面的耗时问题，拓展了其工业应用和数字孪生框架的适用性。 |
| [^24] | [Geometric Ultrasound Localization Microscopy.](http://arxiv.org/abs/2306.15548) | 这项研究提出了一种基于几何框架的超声定位显微镜方法，通过仅依赖到达时间差信息实现微气泡的定位，并在精度和可靠性方面超越了现有的方法。 |
| [^25] | [When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions.](http://arxiv.org/abs/2306.15546) | 基础模型与联邦学习的交叉提供了解锁新可能性的独特机会，扩展了数据可用性，促进了协作式模型发展，并提高了性能和隐私保护。 |
| [^26] | [DataCI: A Platform for Data-Centric AI on Streaming Data.](http://arxiv.org/abs/2306.15538) | DataCI是一个开源平台，专为流数据中的数据中心人工智能而设计，提供丰富的API和版本控制功能，具有易于使用和有效性，可以改变流数据背景下的数据中心人工智能实践。 |
| [^27] | [Higher-order Graph Attention Network for Stock Selection with Joint Analysis.](http://arxiv.org/abs/2306.15526) | 我们提出了一种基于高阶图注意力网络和联合分析的股票选股方法（H-GAT），它能够捕捉涉及超过两个节点的复杂高阶结构，并同时结合基本分析因素和技术分析因素，对股票选股具有较好的效果。 |
| [^28] | [Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning.](http://arxiv.org/abs/2306.15503) | 本研究提出了一种名为优先轨迹回放的回放记忆方法，将数据采样的视角扩展到轨迹中，从有限的数据中提取更全面的信息。这种方法通过反向采样轨迹来提高学习效率，并利用加权评论目标避免采样未见过的动作。优先轨迹回放还能根据不同的优先度指标优先采样效率更高的轨迹。 |
| [^29] | [A novel structured argumentation framework for improved explainability of classification tasks.](http://arxiv.org/abs/2306.15500) | 本文提出了一种新的结构化论证框架$xADG$，通过使用布尔逻辑运算符和多个支持来构建简洁且可理解的论证图，从而改进了分类任务的可解释性和预测能力。 |
| [^30] | [Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets.](http://arxiv.org/abs/2306.15482) | 本文研究了多目标鲁棒性中的合作与竞争之间的平衡问题，提出了一种通过调整对手预算来避免玩家主导的新框架，从而显著提高了多目标鲁棒性。 |
| [^31] | [Causal Inference via Predictive Coding.](http://arxiv.org/abs/2306.15479) | 通过预测编码进行因果推理的技术使我们能够在已知和未知因果图的情况下进行端到端的因果推理和因果发现。 |
| [^32] | [Large-scale unsupervised audio pre-training for video-to-speech synthesis.](http://arxiv.org/abs/2306.15464) | 本文提出了一种利用大规模无监督音频预训练的方法，用于视频到语音合成。通过训练编码器-解码器模型，我们可以在不需要视频对应的情况下，使用丰富的仅音频数据集进行合成。 |
| [^33] | [Are aligned neural networks adversarially aligned?.](http://arxiv.org/abs/2306.15447) | 我们研究了大型语言模型在面对对抗用户构建的对抗性输入时是否仍能保持对齐。我们发现现有的攻击手法不足以可靠攻击对齐文本模型，并通过蛮力方法找到了对抗性输入。 |
| [^34] | [Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate.](http://arxiv.org/abs/2306.15444) | 有限内存贪婪拟牛顿方法提出了一种解决标准拟牛顿方法计算成本和内存需求过高问题的方法，同时还有具有非渐进超线性收敛速率的性能优势。 |
| [^35] | [On-device modeling of user's social context and familiar places from smartphone-embedded sensor data.](http://arxiv.org/abs/2306.15437) | 本研究提出了一种利用嵌入式传感器数据在设备上对用户的社交环境和熟悉地点进行建模的策略，解决了中心化架构处理上下文信息存在隐私泄露和缺乏个性化的问题。 |
| [^36] | [Adversarial Training for Graph Neural Networks.](http://arxiv.org/abs/2306.15427) | 该论文通过克服图学习设置的限制，引入可学习图扩散的灵活GNNs以及针对结构扰动的攻击方法，证明了对抗训练是针对对抗结构扰动的最先进防御方法。 |
| [^37] | [Verifying Safety of Neural Networks from Topological Perspectives.](http://arxiv.org/abs/2306.15403) | 本研究提出了一种从拓扑角度研究神经网络安全性的方法，利用神经网络的同胚性质和开映射性质建立了输入集和输出集之间的严格保证，从而解决了神经网络在安全验证中的不确定性问题。 |
| [^38] | [Length Generalization in Arithmetic Transformers.](http://arxiv.org/abs/2306.15400) | 本研究通过引入相对位置嵌入和训练集引导方法，提高了Transformer模型在学习整数算术和推广到更长序列上的性能，在简单任务中表现良好；同时展示了在乘法任务上引导方法的有效性，并探讨了引导方法在其他领域的潜在应用。 |
| [^39] | [Assessing Dataset Quality Through Decision Tree Characteristics in Autoencoder-Processed Spaces.](http://arxiv.org/abs/2306.15392) | 本文研究了机器学习分类任务中数据集质量评估的关键方面，通过多个数据集的实验，揭示了数据集质量对模型训练和性能的深远影响，并提出了一个全面的数据集质量评估框架。 |
| [^40] | [Multi-perspective Information Fusion Res2Net with RandomSpecmix for Fake Speech Detection.](http://arxiv.org/abs/2306.15389) | 本文提出了多角度信息融合Res2Net与随机Specmix的系统用于假冒语音检测，在低质量场景下提高了模型学习准确伪造信息的能力，并通过数据增强和减少冗余的干扰信息来提高模型的性能。 |
| [^41] | [LeCo: Lightweight Compression via Learning Serial Correlations.](http://arxiv.org/abs/2306.15374) | 使用机器学习来自动消除序列冗余以实现出色的压缩比和解压缩性能的LeCo是一种轻量级压缩框架，通过学习序列相关性，它能够在压缩比和随机访问速度上实现帕累托改进。 |
| [^42] | [A Meta-analytical Comparison of Naive Bayes and Random Forest for Software Defect Prediction.](http://arxiv.org/abs/2306.15369) | 通过系统性文献回顾和元分析，本研究比较了朴素贝叶斯和随机森林模型在软件缺陷预测中的性能。结果表明，没有统计证据表明朴素贝叶斯在召回率、F-度量和精确度方面与随机森林有显著的差异。 |
| [^43] | [Mean Field Theory in Deep Metric Learning.](http://arxiv.org/abs/2306.15368) | 本文将均场理论应用于深度度量学习，提出了一种基于对比度的损失函数设计分类损失函数的方法，通过降低训练复杂度，在图像检索任务中取得了比基线方法更好的效果。 |
| [^44] | [CellViT: Vision Transformers for Precise Cell Segmentation and Classification.](http://arxiv.org/abs/2306.15350) | CellViT是一种基于Vision Transformer的深度学习架构，用于自动实例分割苏木精和伊红染色的组织样本中的细胞核。通过在PanNuke数据集上训练和评估，CellViT展示了其优越性。 |
| [^45] | [FedET: A Communication-Efficient Federated Class-Incremental Learning Framework Based on Enhanced Transformer.](http://arxiv.org/abs/2306.15347) | FedET是一种通信高效的联邦类增量学习框架，利用增强Transformer和增强器来解决联邦学习中的灾难性遗忘和通信成本问题，保证了高精度和数据隐私。 |
| [^46] | [A Toolbox for Fast Interval Arithmetic in numpy with an Application to Formal Verification of Neural Network Controlled Systems.](http://arxiv.org/abs/2306.15340) | 本文介绍了一个使用numpy的区间分析工具箱，通过构建自然包含函数来计算广泛映射类的区间界限，并将其应用于神经网络控制系统的形式验证。 |
| [^47] | [Homological Neural Networks: A Sparse Architecture for Multivariate Complexity.](http://arxiv.org/abs/2306.15337) | 本研究提出了一种新颖的深度神经网络架构，同调神经网络，通过应用网络过滤技术构建稀疏的高阶图结构，解决了深度学习中的计算复杂性和能源效率挑战，并在表格数据和时间序列回归问题上展示了优越的性能。 |
| [^48] | [Simulating counterfactuals.](http://arxiv.org/abs/2306.15328) | 该论文提出了一种算法，可以模拟反事实分布中的值，可对离散和连续变量设定条件，并应用于信用评分中的公平性分析。 |
| [^49] | [Anomaly Detection in Networks via Score-Based Generative Models.](http://arxiv.org/abs/2306.15324) | 该论文提出了一种利用基于得分的生成模型进行网络异常检测的方法，该方法在小规模图上取得了竞争性的结果。 |
| [^50] | [FAIRER: Fairness as Decision Rationale Alignment.](http://arxiv.org/abs/2306.15299) | 本文针对深度神经网络中的公平性问题，从决策合理性的角度研究，并定义了参数平等得分来表征公平决策过程。实证研究表明现有的公平性规范项无法实现决策合理性的对齐。 |
| [^51] | [Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task.](http://arxiv.org/abs/2306.15298) | 该论文通过引入新的偏见度量方法，并在一个现实的IMDB电影分类器的例子中对BERT的性别偏见进行了全面分析。研究结果表明，公开的BERT模型中存在着显著的性别偏见，并强调了负责任使用的重要性。 |
| [^52] | [Adaptive Annealed Importance Sampling with Constant Rate Progress.](http://arxiv.org/abs/2306.15283) | 本文提出了一种自适应退火重要性采样算法，通过推导出恒定速率离散化进展计划，实现了在退火过程中样本在各个分布之间的自适应移动，从而提升了采样效率。 |
| [^53] | [Delivering Inflated Explanations.](http://arxiv.org/abs/2306.15272) | 本文提出了夸大的解释，它是为了解释人工智能系统的决策原因所定义的一种方法。传统的解释方法只显示选择的特征对决策的影响，而夸大的解释考虑了更多特征的可能性。 |
| [^54] | [Hyper-parameter Adaptation of Conformer ASR Systems for Elderly and Dysarthric Speech Recognition.](http://arxiv.org/abs/2306.15265) | 本文研究了在Librispeech语料库上预训练的Conformer ASR系统的超参数适应性，并将其应用于DementiaBank老年人和UASpeech发音障碍者语音数据集。实验结果表明，超参数适应可以显著降低词错误率，并发现了超参数适应性能改进与源域和目标域数据的话语长度比之间的相关性。 |
| [^55] | [[Re] Double Sampling Randomized Smoothing.](http://arxiv.org/abs/2306.15221) | 该论文提出了一种名为双抽样随机平滑的方法，通过使用额外的平滑分布改善了神经网络对抗扰动的鲁棒性认证，实验证明了该方法的有效性。 |
| [^56] | [Unsupervised Episode Generation for Graph Meta-learning.](http://arxiv.org/abs/2306.15217) | 本文研究了无监督的剧集生成方法，通过元学习解决没有标签的少样本节点分类问题。它们充分利用所有节点信息，并且通过泛化能力提高性能。 |
| [^57] | [TranssionADD: A multi-frame reinforcement based sequence tagging model for audio deepfake detection.](http://arxiv.org/abs/2306.15212) | TranssionADD是一种基于多帧强化学习的序列标注模型，用于音频深度伪造检测，通过适应序列标注任务、改进模型泛化能力和引入多帧检测，提供了解决模型鲁棒性和音频片段异常值的方法。 |
| [^58] | [Chronic pain detection from resting-state raw EEG signals using improved feature selection.](http://arxiv.org/abs/2306.15194) | 本论文提出了一种改进的特征选择算法，用于从静息态原始脑电信号中检测慢性疼痛，该算法在类别可分性方面表现优秀，并且在测试中取得了高准确率。 |
| [^59] | [One-class systems seamlessly fit in the forward-forward algorithm.](http://arxiv.org/abs/2306.15188) | 本文研究了在前向训练方式下训练深度单类目标函数的性能，发现这些函数可以处理动态网络大小，为无缝在线训练带来了许多好处。 |
| [^60] | [Automatic Truss Design with Reinforcement Learning.](http://arxiv.org/abs/2306.15182) | 本文提出了一个两阶段的AutoTruss框架，利用蒙特卡洛树搜索和强化学习方法，高效生成轻型和合法的桁架布局。在实验中，AutoTruss显示出优越的性能，超越了之前报道的方法。 |
| [^61] | [Exploiting Inferential Structure in Neural Processes.](http://arxiv.org/abs/2306.15169) | 这项工作提供了一个框架，利用推理结构在神经过程中进行扩展。我们为NPs的潜变量提供了丰富的先验，并提出了适当的上下文集聚合策略。此外，我们还描述了一种消息传递过程，可以进行端到端优化，并通过使用混合和学生-t假设改善了函数建模和测试时间的鲁棒性。 |
| [^62] | [Learning from Invalid Data: On Constraint Satisfaction in Generative Models.](http://arxiv.org/abs/2306.15166) | 本论文提出了一种新的训练机制，利用包含无效数据点的数据集进行生成模型的训练，以提高生成结果的精度和满足约束条件的能力。实验证明，与只使用有效数据点进行训练的标准模型相比，基于无效数据训练的模型明显优于标准模型。 |
| [^63] | [DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization.](http://arxiv.org/abs/2306.15164) | 本论文介绍了一种新颖的方法DSRM，通过最小化分布偏移风险而不是使用对抗样本来对抗训练，从而提高了深度语言模型的鲁棒性，减少了时间消耗。 |
| [^64] | [Wasserstein Generative Regression.](http://arxiv.org/abs/2306.15163) | 我们提出了一种新的统一的非参数回归和条件分布学习方法，使用生成学习框架同时估计回归函数和条件生成器，并使用深度神经网络建模条件生成器。我们的方法可以处理具有多元输出和协变量的问题，可以构建预测区间，并通过理论保证和数值实验证明了方法的有效性和优越性。 |
| [^65] | [Evaluation of machine learning architectures on the quantification of epistemic and aleatoric uncertainties in complex dynamical systems.](http://arxiv.org/abs/2306.15159) | 这项研究评估了多种机器学习技术在复杂动态系统的UQ中的准确性，为降低训练数据集规模和安全因子提供了成本节约的机会。 |
| [^66] | [Revisiting Tropical Polynomial Division: Theory, Algorithms and Application to Neural Networks.](http://arxiv.org/abs/2306.15157) | 本文重新审视了热带多项式除法的问题，并将其应用于神经网络简化中。我们分析了具有实系数的热带多项式，证明了唯一商余对的存在，并提出了精确算法和近似算法。 |
| [^67] | [Learning non-Markovian Decision-Making from State-only Sequences.](http://arxiv.org/abs/2306.15156) | 本文提出了一种从仅状态序列学习非马尔科夫决策的方法，通过深度生成建模和最大似然估计实现基于模型的模仿。学习的模型能够实现“推理式决策”，并在路径规划任务中展示了有效性。 |
| [^68] | [Input-sensitive dense-sparse primitive compositions for GNN acceleration.](http://arxiv.org/abs/2306.15155) | 本文提出了一种在不同的输入图和GNN嵌入大小上使用代数重组的方法，通过选择最佳组合来提高GNN加速的性能。 |
| [^69] | [Contrastive Meta-Learning for Few-shot Node Classification.](http://arxiv.org/abs/2306.15154) | 这篇论文提出了一种对比元学习的方法来解决少样本节点分类的问题，通过从大量有标记节点的类别中抽取可迁移的知识，并将其推广到具有有限标记节点的其他类别，学习通用的节点嵌入。 |
| [^70] | [A Restarted Large-Scale Spectral Clustering with Self-Guiding and Block Diagonal Representation.](http://arxiv.org/abs/2306.15138) | 本论文提出了一个具有自我引导和块对角表示的重启聚类方法，该方法在谱聚类中首次应用重启策略，并且通过在每个周期中重新分类样本来获得更好的聚类效果。 |
| [^71] | [MIMIC: Masked Image Modeling with Image Correspondences.](http://arxiv.org/abs/2306.15128) | MIMIC是一种基于图像对应关系的遮蔽图像建模方法，通过挖掘不需要任何注释的数据集，使用多个自监督模型进行训练，达到了在多个下游任务上优于使用注释挖掘的表示的效果。 |
| [^72] | [Off-Policy Evaluation of Ranking Policies under Diverse User Behavior.](http://arxiv.org/abs/2306.15098) | 本文提出了一种新的离策略评估方法Adaptive IPS (AIPS)，针对排名策略的离策略评估问题，通过考虑用户行为的多样性和上下文的变化，有效降低了估计中的偏差和方差。 |
| [^73] | [C3S Micro-architectural Enhancement: Spike Encoder Block and Relaxing Gamma Clock (Asynchronous).](http://arxiv.org/abs/2306.15093) | C3S微体系结构增强的创新是通过引入刺激编码器模块和放松伽马时钟来改进现有的皮层列架构。 |
| [^74] | [Energy Modelling and Forecasting for an Underground Agricultural Farm using a Higher Order Dynamic Mode Decomposition Approach.](http://arxiv.org/abs/2306.15089) | 本文基于高阶动态模态分解方法提出了一种能量建模和预测的方法，应用于伦敦地下农业农场。这种方法可以处理受噪声和瞬态条件影响的复杂数据，并分析和预测农场的环境行为。 |
| [^75] | [Balanced Filtering via Non-Disclosive Proxies.](http://arxiv.org/abs/2306.15083) | 本文研究了在群组成员资格不可用或被禁止使用时，如何以非泄露方式收集平衡的数据样本。通过使用代理函数和抽样概率，实现了对个体样本的分类和选择，同时保护个体样本的敏感群组成员资格不被泄露。 |
| [^76] | [Molecular geometric deep learning.](http://arxiv.org/abs/2306.15065) | 分子几何深度学习（Mol-GDL）提出了一种更通用的分子表示方法，通过仅使用非共价键构建的分子图，在分子性质预测中取得了与基于共价键模型相似甚至更好的结果，展现了超越基于共价键的分子图的巨大潜力。 |
| [^77] | [Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression.](http://arxiv.org/abs/2306.15063) | 预训练的transformer在回归问题中展现了非贝叶斯上下文学习能力，其在任务多样性阈值以下表现类似于贝叶斯估计器，而在阈值以上明显优于贝叶斯估计器，与岭回归一致。 |
| [^78] | [BatchGFN: Generative Flow Networks for Batch Active Learning.](http://arxiv.org/abs/2306.15058) | BatchGFN是一种用于批量主动学习的新颖方法，通过使用生成流网络根据批量奖励采样数据点集合，能够以原则性的方式构建高度信息量的批量，用于主动学习。通过在推理时间内进行单次前向传递来采样近乎最优效用的批量，减轻了面向批量的算法的计算复杂性，并消除了贪婪近似的需求。提出了跨获取步骤分摊训练的早期结果，实现了对实际任务的扩展。 |
| [^79] | [Optimal Differentially Private Learning with Public Data.](http://arxiv.org/abs/2306.15056) | 本论文研究了具有公共数据的最优差分隐私学习，并解决了在训练差分隐私模型时如何利用公共数据提高准确性的问题。 |
| [^80] | [Towards Sybil Resilience in Decentralized Learning.](http://arxiv.org/abs/2306.15044) | 本研究针对去中心化学习的Sybil毒化攻击问题，提出了一种名为SybilWall的创新算法，旨在增强去中心化学习对这类攻击的鲁棒性。 |
| [^81] | [Equivariant flow matching.](http://arxiv.org/abs/2306.15030) | 本文介绍了一种基于最优输运流匹配的等变CNF训练目标，可以提高等变CNF的可扩展性和实际应用。 |
| [^82] | [Beyond dynamic programming.](http://arxiv.org/abs/2306.15029) | 提出了一种新的理论方法——得分寿命规划，用于解决强化学习问题。方法可以搜索非稳态策略函数，并直接计算最优无限时间区间动作序列。这种方法的核心思想是建立动作序列和实数之间的映射，并通过优化问题计算最优动作序列。方法在非线性最优控制问题中证明了有效性。 |
| [^83] | [Scaling and Resizing Symmetry in Feedforward Networks.](http://arxiv.org/abs/2306.15015) | 本研究提出了关于前馈网络中的缩放和调整对称性的发现，并说明了在临界点上的初始化方式对于提高训练速度具有重要意义。 |
| [^84] | [Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures.](http://arxiv.org/abs/2306.15012) | 本论文提出了一种用于从噪声混合物中恢复目标信号的统计分量分离方法，并且在图像降噪任务中展示了其优于标准降噪方法的表现。 |
| [^85] | [Spectral Analysis of Marine Debris in Simulated and Observed Sentinel-2/MSI Images using Unsupervised Classification.](http://arxiv.org/abs/2306.15008) | 本研究使用无监督分类方法对模拟和观测的Sentinel-2/MSI图像中的海洋垃圾进行光谱分析，发现污染物的光谱行为受多种因素影响，并揭示了元素之间的光谱特征和趋势。 |
| [^86] | [Quality Issues in Machine Learning Software Systems.](http://arxiv.org/abs/2306.15007) | 本文通过采访实践者和进行调查的方式，研究了机器学习软件系统中的质量问题，并确定了一个质量问题目录。 |
| [^87] | [LM4HPC: Towards Effective Language Model Application in High-Performance Computing.](http://arxiv.org/abs/2306.14979) | 本文设计了LM4HPC框架，旨在通过使用语言模型LMs来分析和优化高性能计算（HPC）软件。通过支持HPC数据集、AI模型和流水线，LM4HPC可以帮助用户快速评估最先进的模型并生成有见地的排行榜。 |
| [^88] | [Fairness Aware Counterfactuals for Subgroups.](http://arxiv.org/abs/2306.14978) | 本文提出了针对子群的公平感知反事实论证（FACTS）框架，用于通过反事实解释来审查子群的公平性。该框架重新定义了子群公平性概念，旨在从微观和宏观两个角度考虑实现期望结果的困难，并引入了对于实现成本具有鲁棒性的子群公平性概念。通过实验评估，证明了该方法的优势、适用性和效率。 |
| [^89] | [The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets.](http://arxiv.org/abs/2306.14975) | 本文研究了复杂数据集中的底层缩放定律和普适统计结构。通过将数据类比为物理系统，并应用统计物理学和随机矩阵理论的方法，揭示了特征-特征协方差矩阵的局部和全局特征值统计量的规律。研究发现，在无关随机数据和真实数据之间存在显著差异，并且可以通过引入长程相关性完全恢复缩放行为。同时，生成的数据和真实世界数据都属于混沌系统，并在较小的数据集大小上即可体现随机矩阵理论的统计行为。 |
| [^90] | [SIMF: Semantics-aware Interactive Motion Forecasting for Autonomous Driving.](http://arxiv.org/abs/2306.14941) | 本文提出了一种名为SIMF的方法，用于自动驾驶车辆中语义感知的交互式运动预测。该方法通过实现基于语义的行为体选择和注意力机制提取全局编码，能够捕捉空间信息和语义信息，并优选相关的行为体进行运动预测。 |
| [^91] | [The Art of Embedding Fusion: Optimizing Hate Speech Detection.](http://arxiv.org/abs/2306.14939) | 这项工作研究了优化仇恨言论检测的方法。研究发现，尽管嵌入的组合会略微提高性能，但计算成本很高，并且组合方式对结果的影响较小。 |
| [^92] | [Integrating Bidirectional Long Short-Term Memory with Subword Embedding for Authorship Attribution.](http://arxiv.org/abs/2306.14933) | 这项研究提出了一种将双向长短期记忆网络与子词嵌入相结合的方法，用于解决作者归属问题。该方法能够在处理文本中的隐含词问题的同时保留词的顺序上下文。 |
| [^93] | [GloptiNets: Scalable Non-Convex Optimization with Certificates.](http://arxiv.org/abs/2306.14932) | GloptiNets是一种新方法，可以处理具有证明的非凸优化问题，通过利用目标函数的正则性和并行计算的优势，取得了比现有方法更好的性能。 |
| [^94] | [LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding.](http://arxiv.org/abs/2306.14924) | 本研究探索了使用大型语言模型（LLMs）来减少演绎编码所需时间的方法，同时保留传统内容分析的灵活性。通过一个案例研究和经验基准测试，证明了在不同演绎编码任务上，GPT-3.5在LLM辅助内容分析（LACA）中的有效性。 |
| [^95] | [A Cosine Similarity-based Method for Out-of-Distribution Detection.](http://arxiv.org/abs/2306.14920) | 本文提出了一种基于余弦相似度的离群分布检测方法，通过计算测试特征和内部数据特征之间的余弦相似度来判断离群数据。实验证明，该方法在离群数据检测方面优于现有方法。 |
| [^96] | [Beyond Chemical Language: A Multimodal Approach to Enhance Molecular Property Prediction.](http://arxiv.org/abs/2306.14919) | 一种多模态方法用于预测分子性质，结合了化学语言表示和物理化学特征，通过因果特征选择和分子嵌入向量空间的集成来实现准确预测。相比现有算法，该方法在生物降解性和PFAS毒性评估等复杂任务中表现出卓越性能。 |
| [^97] | [Utilizing Natural Language Processing for Automated Assessment of Classroom Discussion.](http://arxiv.org/abs/2306.14918) | 本研究利用自然语言处理技术，通过自动生成细化标准得分，实现对课堂讨论质量的自动评估。实验结果令人鼓舞，同时指出标准仍有改进空间，并发现不同的NLP方法对不同的标准更有效。 |
| [^98] | [Towards Enriched Controllability for Educational Question Generation.](http://arxiv.org/abs/2306.14917) | 本研究旨在通过引入新的引导属性（问题明确性）来丰富教育问题生成的可控性。我们提出了通过控制生成明确和隐含wh-问题的方法。研究展示了通过问题明确性和叙事要素同时控制问题生成的初步证据。 |
| [^99] | [Uncertainty Estimation for Molecules: Desiderata and Methods.](http://arxiv.org/abs/2306.14916) | 该论文研究了分子力场中不确定性估计的要求和方法，并发现先前的研究方法无法满足所有标准。为了解决这个问题，提出了一种基于局部神经核的高斯过程模型（LNK）。 |
| [^100] | [The Importance of Human-Labeled Data in the Era of LLMs.](http://arxiv.org/abs/2306.14910) | 本文论述了在LLMs时代，人标记数据仍然具有重要性的论据和支持。 |
| [^101] | [Molecule Design by Latent Space Energy-Based Modeling and Gradual Distribution Shifting.](http://arxiv.org/abs/2306.14902) | 本文提出了一种概率生成模型来设计具有所需化学和生物性质的分子，并通过逐步分布变换采样算法搜索具有所需性质的分子。实验证明该方法在分子设计任务上表现出很强的性能。 |
| [^102] | [Phonon dynamic behaviors induced by amorphous interlayer at heterointerfaces.](http://arxiv.org/abs/2306.14901) | 本研究使用声子波包模拟研究发现，非晶性中间层显著阻碍声子在GaN/AlN界面上的传输，导致声子模式转换和高频声子部分穿过非晶性中间层，这为界面热传导提供了额外的热传输通道。 |
| [^103] | [InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback.](http://arxiv.org/abs/2306.14898) | InterCode是一个交互式编码的标准化和基准测试框架，它使用执行反馈作为观察，并提供了安全可重现的执行环境，可以用于开发新的交互式代码生成方法。 |
| [^104] | [PMaF: Deep Declarative Layers for Principal Matrix Features.](http://arxiv.org/abs/2306.14759) | 本文介绍了PMaF框架，使用声明性的深度层来学习主要矩阵特征，通过迭代优化解决问题并应用双层优化框架进行反向传播，从而提高效率。实验证明了该框架优于现有的基线模型。 |
| [^105] | [A General Framework for Sequential Decision-Making under Adaptivity Constraints.](http://arxiv.org/abs/2306.14468) | 本论文提出了一个通用框架，研究了在适应性约束下的顺序决策问题。具体地，我们提供了Eluder Condition类，并针对稀缺策略切换和批次学习约束分别提供了相应的算法。此工作是第一个考虑通用函数类别下稀缺策略切换和批次学习的工作，涵盖了之前研究中的大部分模型。 |
| [^106] | [DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing.](http://arxiv.org/abs/2306.14435) | DragDiffusion是一个利用扩散模型进行交互式点基图像编辑的方法，通过优化扩散潜在实现精确的空间控制，以提高实际场景中的应用性。 |
| [^107] | [Feature Adversarial Distillation for Point Cloud Classification.](http://arxiv.org/abs/2306.14221) | 本文提出了一种特征对抗蒸馏方法（FAD），用于解决点云分类中知识传递的信息损失问题。在实验证明，该方法在模型压缩的同时保持了竞争性能。 |
| [^108] | [QNNRepair: Quantized Neural Network Repair.](http://arxiv.org/abs/2306.13793) | QNNRepair 是一种用于修复量化神经网络的方法，通过解决神经元权重参数以修复在神经网络量化过程中导致性能下降的神经元，从而在不影响通过测试的性能的同时，提高在失败测试上的性能。 |
| [^109] | [Margin Maximization in Attention Mechanism.](http://arxiv.org/abs/2306.13596) | 这篇论文证明了，在softmax-attention模型中，通过在p或等价的W上运行梯度下降，可以收敛到一个最大边缘解，这将局部最优的标记与非最优的标记分隔开。这明确地将注意力机制形式化为标记分离机制。 |
| [^110] | [Minibatch training of neural network ensembles via trajectory sampling.](http://arxiv.org/abs/2306.13442) | 本文介绍了一种轨迹采样下的神经网络集合小批量训练方法，通过对MNIST数据集实验，发现相较于传统方法，该方法提高了两个数量级的计算效率，同时还提高了推断准确性。 |
| [^111] | [FuXi: A cascade machine learning forecasting system for 15-day global weather forecast.](http://arxiv.org/abs/2306.12873) | 逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。 |
| [^112] | [A Bayesian Take on Gaussian Process Networks.](http://arxiv.org/abs/2306.11380) | 该论文提出了一种基于高斯过程和贝叶斯方法的网络模型，通过蒙特卡罗和马尔可夫链蒙特卡罗方法采样网络结构的后验分布。该方法在恢复网络的图形结构方面优于最先进的算法，并提供了后验概率的准确近似。 |
| [^113] | [Top-down machine learning of coarse-grained protein force-fields.](http://arxiv.org/abs/2306.11375) | 通过分子动力学模拟和可微分轨迹重加权训练神经网络势能，实现了自上而下的粗粒化蛋白质力场建模，仅需蛋白质的天然构象即可展示其外推能力。 |
| [^114] | [Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs.](http://arxiv.org/abs/2306.11330) | 本论文介绍了一种基于FPGA的GNN架构，在粒子跟踪中实现了低延迟和资源高效性，并实现了对现有CPU和GPU的数千倍上的性能提升。 |
| [^115] | [Co-design Hardware and Algorithm for Vector Search.](http://arxiv.org/abs/2306.11182) | 本论文提出了一个在FPGA上的向量搜索框架FANNS，实现了硬件和算法的共同设计，可以根据用户需求和硬件预算生成相应的加速器。与FPGA和CPU基准相比，FANNS实现了显著的加速，并展现了卓越的可扩展性。 |
| [^116] | [Effect-Invariant Mechanisms for Policy Generalization.](http://arxiv.org/abs/2306.10983) | 本文提出了一种松弛了完全不变性的方法，称为效果不变性，证明它足以进行零样本策略概括，并讨论了基于少量样本的扩展。 |
| [^117] | [Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network.](http://arxiv.org/abs/2306.10946) | 本文提出了一种基于注意力知识图卷积网络的旅游景点推荐模型，通过自动语义发掘目标景点的相邻实体，根据旅客的喜好选择，预测类似景点的概率，实验中取得良好效果。 |
| [^118] | [Conditional expectation via compact kernels.](http://arxiv.org/abs/2306.10592) | 本文提出了一种基于紧核的算子理论方法来解决条件期望估计问题，在再生核希尔伯特空间中实现，易于实现，且成功应用于实际问题中。 |
| [^119] | [The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions.](http://arxiv.org/abs/2306.10404) | 本文提出了一个高维RL模型，推导出该模型的典型动力学为一组闭式ODE方程组，并通过实验与神经RL代理进行了比较，结果表明该模型捕捉了现实世界RL的关键特征。 |
| [^120] | [Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction.](http://arxiv.org/abs/2306.10045) | 本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。 |
| [^121] | [Survey on Sociodemographic Bias in Natural Language Processing.](http://arxiv.org/abs/2306.08158) | 本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。 |
| [^122] | [Learning World Models with Identifiable Factorization.](http://arxiv.org/abs/2306.06561) | 本文提出了一个通用框架IFactor，用于学习具有可辨识分解的世界模型，以提取稳定且紧凑的环境表示，并揭示了所有与奖励相关的因素对策略学习的重要性。 |
| [^123] | [One-step Multi-view Clustering with Diverse Representation.](http://arxiv.org/abs/2306.05437) | 本文提出了一种一步多视角聚类与多样性表征的方法，将多视角学习和k-means聚类融合到一个统一框架中，实验结果表明其在各种标准的多视角数据集上都优于现有算法。 |
| [^124] | [EMO: Episodic Memory Optimization for Few-Shot Meta-Learning.](http://arxiv.org/abs/2306.05189) | EMO是一种元学习的情节记忆优化方案，通过在外部存储器中记录过去任务的梯度历史，实现小样本学习，无论提供的梯度信息是否可靠，都可以推动参数更新朝着正确的方向前进。 |
| [^125] | [Fast Optimal Locally Private Mean Estimation via Random Projections.](http://arxiv.org/abs/2306.04444) | 提出了一种名为ProjUnit的算法框架，用于实现高效的本地隐私均值估计，通过随机投影低维空间实现最优解，且具有低通信复杂度和快速的服务器运行时间。 |
| [^126] | [Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning.](http://arxiv.org/abs/2306.04265) | 本文介绍了一个用于异质半监督学习的新型图神经网络模型PEGFAN，它使用置换等变图框架实现了多尺度特征提取，表现优于其他最先进模型，特别是在相对较大和密集连接的数据集中。 |
| [^127] | [LLMZip: Lossless Text Compression using Large Language Models.](http://arxiv.org/abs/2306.04050) | 本研究使用大型语言模型提出了一种结合预测和无损压缩方案的英文文本压缩算法，并在初步实验中表现优于当前最先进的文本压缩方案。 |
| [^128] | [Language Models are Bounded Pragmatic Speakers.](http://arxiv.org/abs/2305.17760) | 本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。 |
| [^129] | [On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models.](http://arxiv.org/abs/2305.17583) | 本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决深度神经网络(DNNs)缺乏PGMs的精确语义和明确定义的概率解释的问题。研究发现DNNs在前向传播时确实执行PGM推断的近似，这与现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似，潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。 |
| [^130] | [Levin Tree Search with Context Models.](http://arxiv.org/abs/2305.16945) | 本文提出了一种新的具有上下文模型的Levin树搜索算法，通过将神经网络替换为上下文模型，实现了LTS损失的凸优化，并在多个基准测试中取得了明显优于LTS+NN的结果。 |
| [^131] | [Replicable Reinforcement Learning.](http://arxiv.org/abs/2305.15284) | 本篇论文提供了可复制的强化学习算法，是控制问题的第一个正式的可复制性结果 |
| [^132] | [Distribution-aware Fairness Test Generation.](http://arxiv.org/abs/2305.13935) | 本文介绍了一种名为DistroFair的分布感知的公平性测试方法，可以从图像分类器中检测到类级别的公平性违规。 |
| [^133] | [Fast Convergence in Learning Two-Layer Neural Networks with Separable Data.](http://arxiv.org/abs/2305.13471) | 本文研究了使用归一化梯度下降算法在双层神经网络中进行训练的方法，证明了对于指数尾部损失函数，其收敛速率为线性，同时建立了有限时间的泛化边界。 |
| [^134] | [Copy Recurrent Neural Network Structure Network.](http://arxiv.org/abs/2305.13250) | 本研究提出了一种名为复制循环神经网络结构网络（CRNNet）的新型粗到细的ICD路径生成框架，通过使用RNN生成顺序输出并结合复制模块，有效识别复杂疾病，相比于最先进的和先前的方法，在预测中实现了更高的复杂疾病比率（57.30%）。 |
| [^135] | [Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test.](http://arxiv.org/abs/2305.13108) | 本文提出了一种样本重新加权与样本关联测试（Re-SAT）的新方法，用于缓解失语症患者的偏差问题，在不影响健康患者语音的ASR性能的情况下，有效提高了ASR的性能表现。 |
| [^136] | [Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach.](http://arxiv.org/abs/2305.12837) | 本论文提出了一种名为HDR的新方法，通过重复使用历史促销数据，来捕捉促销转化模式，达到更好地适应促销模式的目的。 |
| [^137] | [Towards Complex Dynamic Physics System Simulation with Graph Neural ODEs.](http://arxiv.org/abs/2305.12334) | 本研究提出了一种基于学习的模拟模型，称为GNSTODE，通过利用统一的端到端框架描述了粒子系统中不同时间和不同空间条件下的变化。 |
| [^138] | [The Waymo Open Sim Agents Challenge.](http://arxiv.org/abs/2305.12032) | Waymo开放模拟代理挑战赛提出使用真实、互动的智能体仿真以促进自动驾驶行为模型的评估和训练，是该领域的首个公开挑战赛，旨在推动逼真模拟器的设计。 |
| [^139] | [Large-Scale Package Manipulation via Learned Metrics of Pick Success.](http://arxiv.org/abs/2305.10272) | 本文讨论了基于学习度量的大规模包裹操作，通过训练拾取成功预测器和学习拾取质量度量，实现了能够大规模部署的强力抓握策略。 |
| [^140] | [Molecule-Morphology Contrastive Pretraining for Transferable Molecular Representation.](http://arxiv.org/abs/2305.09790) | 本文提出了Molecule-Morphology Contrastive Pretraining (MoCoP)框架，用于学习分子图形和细胞形态的多模态表示。实验结果表明，MoCoP可以提高图神经网络在分子属性预测任务上的表现，具有良好的实用性。 |
| [^141] | [Quantified Semantic Comparison of Convolutional Neural Networks.](http://arxiv.org/abs/2305.07663) | 本研究提出了两种方法来量化卷积神经网络潜在空间中语义信息之间的相似性，从而揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。 |
| [^142] | [Device-Robust Acoustic Scene Classification via Impulse Response Augmentation.](http://arxiv.org/abs/2305.07499) | 本篇论文提出了一种基于冲击响应增强的方法，用于解决音频分类模型泛化到未被训练设备上时性能下降的问题。 |
| [^143] | [Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation.](http://arxiv.org/abs/2305.06563) | 本文提出了一种基于流形正则化Tucker分解的时空交通数据填充方法，该方法利用稀疏正则化项改善了Tucker核的稀疏性，并引入流形正则化和时间约束项来优化张量的填充性能。 |
| [^144] | [Limits of Model Selection under Transfer Learning.](http://arxiv.org/abs/2305.00152) | 这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。 |
| [^145] | [Discovering Object-Centric Generalized Value Functions From Pixels.](http://arxiv.org/abs/2304.13892) | 本文介绍了一种从像素中学习物体中心化的广义值函数的方法。该方法从物体中发现有意义的特征，转化为“问题”函数，并利用随后学习的广义值函数来进行控制，在静态和非静态设置下表现良好。学到的表示不仅是可解释的，而且围绕着具有不变性的物体，有助于快速适应。 |
| [^146] | [Smart Learning to Find Dumb Contracts.](http://arxiv.org/abs/2304.10726) | DLVA是一种用于以太坊智能合约的强大深度学习漏洞检测工具，其算法涵盖了源代码到字节码的扩展，并且速度比传统漏洞检测工具提高了10-500倍，并成功地发现了一些Slither误标记的易受攻击的合约。 |
| [^147] | [EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition.](http://arxiv.org/abs/2304.01508) | EPVT是一种基于环境感知的提示视觉Transformer，用于解决皮肤病变识别中深度神经网络可能过度依赖疾病不相关图像特征的问题，通过嵌入一组领域提示和一个共享提示来进行领域一般化，并且引入了领域提示生成器促进知识共享。 |
| [^148] | [mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection.](http://arxiv.org/abs/2303.09901) | 本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。 |
| [^149] | [SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks.](http://arxiv.org/abs/2302.13939) | 本论文提出了一种称之为SpikeGPT的生成语言模型，使用二进制、事件驱动脉冲激活单元进行训练，克服了SNN训练中的挑战性。该模型可以用于大规模语言生成任务。 |
| [^150] | [GraphSR: A Data Augmentation Algorithm for Imbalanced Node Classification.](http://arxiv.org/abs/2302.12814) | 本文提出了一种名为GraphSR的自动节点分类数据增强算法，通过使用具有显著差异的未标记节点来增强少数类的训练样本。 |
| [^151] | [Out-of-Domain Robustness via Targeted Augmentations.](http://arxiv.org/abs/2302.11861) | 本文研究了为领域外泛化设计数据增强的原则，通过有针对性的增强方法，在保留鲁棒特征的同时随机化虚假的领域相关特征，提高了领域外性能。 |
| [^152] | [Learning to Play Text-based Adventure Games with Maximum Entropy Reinforcement Learning.](http://arxiv.org/abs/2302.10720) | 本文提出了一种使用最大熵强化学习玩文本基础冒险游戏的方法，通过结合奖励塑形技术，提供更多信息密集的奖励信号给强化学习机器人，从而使其在许多游戏中达到更高的得分，且只需一半的训练步骤。 |
| [^153] | [Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics.](http://arxiv.org/abs/2302.04841) | 本文研究了文本到图像个性化方法的训练动态，并提出了一种简单的早停准则来加快训练速度 |
| [^154] | [The Sample Complexity of Approximate Rejection Sampling with Applications to Smoothed Online Learning.](http://arxiv.org/abs/2302.04658) | 本研究展示了在有界f-散度约束下，近似拒绝采样的样本复杂度可以通过Θ(~(D/f'(n)))函数来表示，并且应用于平滑在线学习中的相关算法的性能依然成立。 |
| [^155] | [Toward Physically Plausible Data-Driven Models: A Novel Neural Network Approach to Symbolic Regression.](http://arxiv.org/abs/2302.00773) | 这项研究提出了一种新颖的基于神经网络的符号回归方法，该方法通过基于梯度的优化算法来学习整个解析模型的结构和系数，克服了传统遗传编程方法在变量和样本数量增加时模型规模和复杂度增长快、准确性提升不足以及模型系数调整困难的问题。 |
| [^156] | [GFlowNets for AI-Driven Scientific Discovery.](http://arxiv.org/abs/2302.00615) | GFlowNets是一种新的概率机器学习框架，可以帮助加快科学探索的速度，尤其是在高维搜索空间中估计不确定性和生成多样性的实验集。 |
| [^157] | [DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models.](http://arxiv.org/abs/2301.13629) | 本论文提出了一种新的方法DiffSTG，该方法结合了STGNN的时空学习能力和扩散模型的不确定性测量，可以有效减小STG预测中的排名概率分数和均方根误差。 |
| [^158] | [GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration.](http://arxiv.org/abs/2301.12686) | GibbsDDRM是一种扩展的局部折叠Gibbs采样方法，用于解决线性逆问题中线性算子未知的盲场景。它利用预训练的扩散模型构建了数据、测量和线性算子的联合分布，并通过后验采样来解决问题。该方法在盲图像去模糊和语音去混响任务上表现出了高性能，而且适用于各种逆问题。 |
| [^159] | [Conformal inference is (almost) free for neural networks trained with early stopping.](http://arxiv.org/abs/2301.11556) | 本文介绍了一种将早停与conformal校准相结合的新方法，以解决使用早停训练的神经网络在缺乏独立校准数据时无法提供准确统计保证的问题。 |
| [^160] | [A Semi-supervised Sensing Rate Learning based CMAB Scheme to Combat COVID-19 by Trustful Data Collection in the Crowd.](http://arxiv.org/abs/2301.08563) | 本文提出了一种基于半监督学习的组合多臂赌博反向拍卖方案，用于解决移动众包系统中在招募多个未知和有策略的工作者时出现的数据可信问题。 |
| [^161] | [Input Normalized Stochastic Gradient Descent Training of Deep Neural Networks.](http://arxiv.org/abs/2212.09921) | 本文提出了一种名为输入归一化随机梯度下降（INSGD）的优化算法，通过在学习率上应用归一化来更新网络权重，以实现在大型数据集上训练复杂模型时避免发散并提高准确性水平。 |
| [^162] | [Faster Maximum Inner Product Search in High Dimensions.](http://arxiv.org/abs/2212.07551) | 该论文提出了一种在高维度中更快的最大内积搜索算法 BanditMIPS，该算法的复杂度与维度无关，并且通过自适应采样策略提供理论保证和实验证明。 |
| [^163] | [Interpretable Diabetic Retinopathy Diagnosis based on Biomarker Activation Map.](http://arxiv.org/abs/2212.06299) | 本文提出了一种基于生物标志物激活地图（BAM）的诊断框架，使用两个U形生成器为临床医生提供意义明确的可解释性，以验证和理解分类器的决策，可应用于自动诊断糖尿病视网膜病变。 |
| [^164] | [Explainable Performance: Measuring the Driving Forces of Predictive Performance.](http://arxiv.org/abs/2212.05866) | XPER方法能衡量输入特征对模型预测性能的具体贡献，并可用于处理异质性问题，构建同质化个体群体，从而提高预测精度。 |
| [^165] | [Training Data Influence Analysis and Estimation: A Survey.](http://arxiv.org/abs/2212.04612) | 本文对训练数据影响分析与估计进行了全面调查，通过量化每个训练实例对最终模型的改变程度揭示了训练的基本相互作用。调查了当前的影响分析方法，并提出了未来的研究方向。 |
| [^166] | [Clustering with Neural Network and Index.](http://arxiv.org/abs/2212.03853) | 介绍了一种新的带有神经网络和索引的聚类模型CNNI，该模型使用神经网络对数据点进行聚类，实现了第一个能够处理非凸形状数据的参数化聚类模型。 |
| [^167] | [Latent Graph Inference using Product Manifolds.](http://arxiv.org/abs/2211.16199) | 本文提出了一种利用积流形进行推断的潜在图学习方法，以动态学习问题内在的图结构。通过使用Riemannian几何学和生成更复杂的嵌入空间，可以提高系统性能并产生更丰富的表示。 |
| [^168] | [SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control.](http://arxiv.org/abs/2210.17432) | SSD-LM是一种半自回归的扩散语言模型，通过在解码时灵活生成文本块并实现本地上下文更新，以及在自然词汇空间上进行扩散，实现了分类器指导和模块化控制。在无约束文本生成基准上，SSD-LM与自回归模型相比，在质量和多样性方面表现出色，并且显著超越了其他基于扩散的模型。 |
| [^169] | [Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers.](http://arxiv.org/abs/2210.02317) | 本文实现了一个名为ReLoD的实时学习系统，利用本地和远程计算机来分配深度强化学习算法的计算，并在基于视觉的控制任务上进行了评估。结果显示SAC的性能下降了。 |
| [^170] | [CADet: Fully Self-Supervised Out-Of-Distribution Detection With Contrastive Learning.](http://arxiv.org/abs/2210.01742) | 本文介绍了一种使用自监督对比学习进行带有对比学习的全自主分布检测的方法，能够同时检测未见过的类别和对抗性扰动样本。通过将自监督对比学习与最大均值差异（MMD）相结合，提出了CADet方法，该方法通过利用同一样本的对比变换之间的相似性进行OOD检测，并在对抗性扰动的识别方面比现有方法表现更好。 |
| [^171] | [MGG: Accelerating Graph Neural Networks with Fine-grained intra-kernel Communication-Computation Pipelining on Multi-GPU Platforms.](http://arxiv.org/abs/2209.06800) | MGG是一种软件流水线设计，可在多GPU平台上加速GNNs，通过采用GNN特殊的流水线构建和GPU感知的流水线映射，实现精细计算通信重叠以提高性能。 |
| [^172] | [Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning.](http://arxiv.org/abs/2208.11361) | 本文在强化学习中提出了一种新的内在奖励方法，通过比较当前观察与历史知识的差异来评估好奇心，并利用时间不一致性作为内在奖励。实验证明该方法在稀疏外在奖励的情况下具有更高的性能和噪声容忍度。 |
| [^173] | [Asynchronous Execution of Heterogeneous Tasks in ML-driven HPC Workflows.](http://arxiv.org/abs/2208.11069) | 该论文研究了机器学习驱动的高性能计算工作流中异步任务执行的要求和特性，并提出了用于确定异步执行收益的关键指标。通过在Summit上进行大规模实验，作者证明了异步执行对性能的增强与模型一致。 |
| [^174] | [Event-Triggered Time-Varying Bayesian Optimization.](http://arxiv.org/abs/2208.10790) | 本文提出了一种事件触发算法ET-GP-UCB，用于解决时变贝叶斯优化中的探索和开发的权衡问题。通过基于高斯过程回归的概率均匀误差界，算法能够在未知变化速率的情况下自适应地适应实际的时间变化。数值实验结果表明，ET-GP-UCB在合成和实际数据上优于现有算法。 |
| [^175] | [What Do Compressed Multilingual Machine Translation Models Forget?.](http://arxiv.org/abs/2205.10828) | 本研究评估了压缩方法对多语言神经机器翻译模型在不同语言群体、性别和语义偏差方面的影响，并发现代表性不足的语言性能显著下降。 |
| [^176] | [Bilinear value networks.](http://arxiv.org/abs/2204.13695) | 提出了一种通过点积低秩近似来表示Q值的双线性分解方法，其中第一个向量场捕捉状态的局部动态，第二个部分捕捉当前状态和目标之间的全局关系，该方法能够显著提高数据效率，并具有很好的泛化性能。 |
| [^177] | [Topological Experience Replay.](http://arxiv.org/abs/2203.15845) | 本文提出了一种拓扑经验回放的方法，通过构建图来明确状态的 Q 值之间的依赖关系，解决了传统采样策略忽视状态间依赖关系的问题，提高了学习深度 Q 函数时的性能和准确性。 |
| [^178] | [Explainable AI Integrated Feature Selection for Landslide Susceptibility Mapping using TreeSHAP.](http://arxiv.org/abs/2201.03225) | 本研究使用最先进的机器学习方法来描述滑坡易发性，并通过XgBoost算法和TreeSHAP方法进行优化。经过实验证明，优化后的XgBoost表现优于其他分类器，具有较高的交叉验证加权F1分数。 |
| [^179] | [The Dual PC Algorithm and the Role of Gaussianity for Structure Learning of Bayesian Networks.](http://arxiv.org/abs/2112.09036) | 双重PC算法通过利用协方差和精度矩阵之间的反向关系，实现了CI测试，能够恢复正确的等价类，并可对互补调节集的偏相关进行测试。 |
| [^180] | [Convergence and Stability of the Stochastic Proximal Point Algorithm with Momentum.](http://arxiv.org/abs/2111.06171) | 本论文研究了带有动量的随机近端点算法（SPPAM）的收敛性和稳定性，展示了SPPAM相比于随机近端点算法（SPPA）具有更快的线性收敛和更好的收缩因子。 |
| [^181] | [When Does Translation Require Context? A Data-driven, Multilingual Exploration.](http://arxiv.org/abs/2109.07446) | 本研究开发了多语言语篇感知基准，系统性地确定了需要上下文翻译的现象。发现上下文感知机器翻译模型对于解决这些现象的困难程度有限，为进一步研究提供了挑战。 |
| [^182] | [FedPower: Privacy-Preserving Distributed Eigenspace Estimation.](http://arxiv.org/abs/2103.00704) | 本文提出了一种称为FedPower的算法，在联邦学习框架内解决了特征空间估计的隐私和通信效率问题。算法利用幂法进行本地迭代和全局聚合，采用正交Procrustes变换加权以实现对齐，并引入差分隐私以保护数据隐私。 |
| [^183] | [Learning Deep Features in Instrumental Variable Regression.](http://arxiv.org/abs/2010.07154) | 本论文提出了一种名为深度特征工具变量回归（DFIV）的方法，用于处理观测数据中工具变量、处理变量和结果变量之间的非线性关系。通过训练深度神经网络来定义工具变量和处理变量上的非线性特征，我们提供了一种交替训练机制以获得良好的端到端性能。这种方法可以在计算上获得高度灵活的特征映射。 |
| [^184] | [Neural Topic Modeling with Continual Lifelong Learning.](http://arxiv.org/abs/2006.10909) | 本研究提出了一个具有持续终身学习的神经主题建模框架，可以处理数据稀疏性，并通过知识的持续积累和转移来提高主题建模的效果。 |
| [^185] | [Explainable and Discourse Topic-aware Neural Language Understanding.](http://arxiv.org/abs/2006.10632) | 该论文提出了一个新颖的神经复合语言模型，通过引入可解释性的主题表示和句子级的主题对话，将主题模型和语言模型相结合。实验结果表明，在多个任务上，该模型显示出了良好的性能。 |

# 详细

[^1]: 在高维度、小样本表格数据上增强表示学习：一种基于集成VAE的分而治之方法

    Enhancing Representation Learning on High-Dimensional, Small-Size Tabular Data: A Divide and Conquer Method with Ensembled VAEs. (arXiv:2306.15661v1 [cs.LG])

    [http://arxiv.org/abs/2306.15661](http://arxiv.org/abs/2306.15661)

    这项研究使用了基于集成VAE的分而治之方法，在高维度、低样本数量的任务中学习到更好的潜在表示，提高了样本效率，并在下游分类任务中取得了更高的准确性。

    

    变分自动编码器及其各种变体展现了在降维任务中的出色性能，通常能达到最新水平。然而，在高维度、低样本数量(HDLSS)任务中，许多当前方法在学习良好的表示方面存在困难，这是一个固有的具有挑战性的设置。我们通过使用轻量级VAE的集成来学习特征空间子集上的后验概率，从而在新颖的分而治之方法中聚合到一个联合后验中，从而应对这一挑战。具体而言，我们提出了联合后验的一种替代分解方式，引入了一种隐式数据增强形式，提高了样本效率。通过对八个真实数据集进行一系列实验，我们展示了我们的方法在HDLSS设置中学习到更好的潜在表示，这导致了下游分类任务中更高的准确性。此外，我们验证我们的方法对解缠缚效果有积极影响。

    Variational Autoencoders and their many variants have displayed impressive ability to perform dimensionality reduction, often achieving state-of-the-art performance. Many current methods however, struggle to learn good representations in High Dimensional, Low Sample Size (HDLSS) tasks, which is an inherently challenging setting. We address this challenge by using an ensemble of lightweight VAEs to learn posteriors over subsets of the feature-space, which get aggregated into a joint posterior in a novel divide-and-conquer approach. Specifically, we present an alternative factorisation of the joint posterior that induces a form of implicit data augmentation that yields greater sample efficiency. Through a series of experiments on eight real-world datasets, we show that our method learns better latent representations in HDLSS settings, which leads to higher accuracy in a downstream classification task. Furthermore, we verify that our approach has a positive effect on disentanglement and a
    
[^2]: SparseOptimizer: 通过Moreau-Yosida正则化来降低语言模型的稀疏性，并通过编译器共同设计来加速

    SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])

    [http://arxiv.org/abs/2306.15656](http://arxiv.org/abs/2306.15656)

    SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。

    

    本文介绍了SparseOptimizer，一种新颖的深度学习优化器，通过Moreau-Yosida正则化在大型语言模型（如BERT，ALBERT和GPT）中自然地引入稀疏性。SparseOptimizer设计的关键是嵌入的收缩操作符，它在优化过程中直接引入稀疏性。这个操作符通过坚实的理论框架支持，并包含了一个分析解，从而增强了优化器的鲁棒性和效果。重要的是，SparseOptimizer的即插即用功能消除了对代码修改的需求，使其成为适用于各种大型语言模型的通用适应工具。在GLUE、RACE、SQuAD1和SQuAD2等基准数据集上的实证评估表明，通过SparseOptimizer稀疏化后的SparseBERT和SparseALBERT在性能上与密集型的BERT和ALBERT相当，同时显著减少了参数数量。

    This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
    
[^3]: 在度量空间中的有效电阻

    Effective resistance in metric spaces. (arXiv:2306.15649v1 [cs.LG])

    [http://arxiv.org/abs/2306.15649](http://arxiv.org/abs/2306.15649)

    本研究通过考虑区域间的有效电阻，而不是点对间的有效电阻，并根据每个区域的密度适当缩放边权重，解决了在度量空间中有效电阻收敛到微不足道量的问题。

    

    有效电阻（ER）是一种探测图结构的有效方法。它是计算图拉普拉斯算子的特征向量的一种替代方法。ER的一个有吸引力的应用是对点云进行分析，即图的顶点对应于度量空间中的IID样本的分布。不幸的是，研究表明，随着样本大小趋近于无穷大，任意两点之间的ER收敛到一个没有关于图结构的信息的微不足道的量。在本研究中，我们通过考虑小区域之间的基于区域的ER，而不是点对之间的ER，并根据每个区域中的潜在密度适当地缩放边权重，证明了这个微不足道的解决方案可以被绕过。通过保持区域不变，我们理论上证明了基于区域的ER在点的数量趋近于无穷大时收敛到一个非微不足道的极限，即度量空间中的ER。我们用实验证明了我们的理论发现。

    Effective resistance (ER) is an attractive way to interrogate the structure of graphs. It is an alternative to computing the eigenvectors of the graph Laplacian.  One attractive application of ER is to point clouds, i.e. graphs whose vertices correspond to IID samples from a distribution over a metric space. Unfortunately, it was shown that the ER between any two points converges to a trivial quantity that holds no information about the graph's structure as the size of the sample increases to infinity.  In this study, we show that this trivial solution can be circumvented by considering a region-based ER between pairs of small regions rather than pairs of points and by scaling the edge weights appropriately with respect to the underlying density in each region. By keeping the regions fixed, we show analytically that the region-based ER converges to a non-trivial limit as the number of points increases to infinity. Namely the ER on a metric space. We support our theoretical findings wit
    
[^4]: 关于合成表格数据生成的实用性研究

    On the Usefulness of Synthetic Tabular Data Generation. (arXiv:2306.15636v1 [cs.LG])

    [http://arxiv.org/abs/2306.15636](http://arxiv.org/abs/2306.15636)

    本研究对合成表格数据在数据共享、数据增强、类别平衡和数据汇总等用例中进行了基准测试，发现合成表格数据对于ML训练的实用性目前尚缺乏足够的证据。

    

    尽管合成数据生成方面取得了一些进展，但科学界对其实用性仍缺乏一致的共识。普遍认为，合成数据可以用于数据交换和加强机器学习（ML）训练。隐私保护的合成数据生成可以加速下游任务的数据交换，但目前尚缺乏足够的证据来说明合成数据如何或为何能够提升ML训练。本研究对四种用例使用合成表格数据对ML性能进行了基准测试：数据共享、数据增强、类别平衡和数据汇总。我们观察到在某些数据集上平衡用例有轻微改进。然而，我们得出结论，目前并没有足够的证据来声称合成表格数据对ML训练有用。

    Despite recent advances in synthetic data generation, the scientific community still lacks a unified consensus on its usefulness. It is commonly believed that synthetic data can be used for both data exchange and boosting machine learning (ML) training. Privacy-preserving synthetic data generation can accelerate data exchange for downstream tasks, but there is not enough evidence to show how or why synthetic data can boost ML training. In this study, we benchmarked ML performance using synthetic tabular data for four use cases: data sharing, data augmentation, class balancing, and data summarization. We observed marginal improvements for the balancing use case on some datasets. However, we conclude that there is not enough evidence to claim that synthetic tabular data is useful for ML training.
    
[^5]: 异步算法与Cocycles的对齐

    Asynchronous Algorithmic Alignment with Cocycles. (arXiv:2306.15632v1 [cs.LG])

    [http://arxiv.org/abs/2306.15632](http://arxiv.org/abs/2306.15632)

    该论文提出了一种将节点状态更新和消息函数调用分离的数学框架，以实现异步计算，并以此作为基础，进行了异步算法和神经网络的对齐。

    

    最先进的神经算法推理器使用图神经网络（GNN）中的消息传递。但是，典型的GNN在定义和调用消息函数之间模糊了区别，迫使节点在每一层都向其邻居发送消息，同步地进行。然而，当将GNN应用于学习执行动态规划算法时，大多数步骤只有少数几个节点会有有意义的更新要发送。因此，通过在图中发送太多无关的数据，可能导致低效率，而许多中间的GNN步骤必须学习身份函数。在这项工作中，我们明确地分离了节点状态更新和消息函数调用的概念。通过这种分离，我们得到了一个数学表达，可以让我们思考算法和神经网络中的异步计算。

    State-of-the-art neural algorithmic reasoners make use of message passing in graph neural networks (GNNs). But typical GNNs blur the distinction between the definition and invocation of the message function, forcing a node to send messages to its neighbours at every layer, synchronously. When applying GNNs to learn to execute dynamic programming algorithms, however, on most steps only a handful of the nodes would have meaningful updates to send. One, hence, runs the risk of inefficiencies by sending too much irrelevant data across the graph -- with many intermediate GNN steps having to learn identity functions. In this work, we explicitly separate the concepts of node state update and message function invocation. With this separation, we obtain a mathematical formulation that allows us to reason about asynchronous computation in both algorithms and neural networks.
    
[^6]: 神经Galerkin方案中自适应采样的耦合参数和粒子动力学

    Coupling parameter and particle dynamics for adaptive sampling in Neural Galerkin schemes. (arXiv:2306.15630v1 [math.NA])

    [http://arxiv.org/abs/2306.15630](http://arxiv.org/abs/2306.15630)

    本研究引入了神经Galerkin方案，通过使用自适应分布的数据来估计训练损失，以应对具有局部特征和高方差的输运主导的高维问题。粒子集合通过与解场的非线性参数化耦合的动力学演化来主动调整，以保持信息丰富。

    

    训练非线性参数化，如深度神经网络，以数值逼近偏微分方程的解通常基于最小化包括残差的损失，该残差在有限的情况下是可以解析得到的。同时，由于存在局部特征(如波和相干结构)的以输运为主的高维问题的残差和相关量可能具有较高的方差，因此经验估计训练损失是具有挑战性的。因此，基于来自无信息均匀分布的数据样本的估计器是低效的。本研究引入了神经Galerkin方案，该方案使用来自自适应分布的数据来估计训练损失，这些分布通过粒子集合进行经验表示。粒子集合通过与解场的非线性参数化耦合的动力学演化来主动调整，使得粒子集合保持信息丰富以用于估计...

    Training nonlinear parametrizations such as deep neural networks to numerically approximate solutions of partial differential equations is often based on minimizing a loss that includes the residual, which is analytically available in limited settings only. At the same time, empirically estimating the training loss is challenging because residuals and related quantities can have high variance, especially for transport-dominated and high-dimensional problems that exhibit local features such as waves and coherent structures. Thus, estimators based on data samples from un-informed, uniform distributions are inefficient. This work introduces Neural Galerkin schemes that estimate the training loss with data from adaptive distributions, which are empirically represented via ensembles of particles. The ensembles are actively adapted by evolving the particles with dynamics coupled to the nonlinear parametrizations of the solution fields so that the ensembles remain informative for estimating t
    
[^7]: 基于机器学习的中性原子NISQ设备噪声特性和校正研究

    Machine-learning based noise characterization and correction on neutral atoms NISQ devices. (arXiv:2306.15628v1 [quant-ph])

    [http://arxiv.org/abs/2306.15628](http://arxiv.org/abs/2306.15628)

    本研究基于机器学习技术，提出了两种方法来表征和校正中性原子NISQ设备上的噪声参数，为进一步提高计算结果的准确性提供了新的思路。

    

    中性原子器件利用光镊排列原子和调制激光脉冲控制量子态，代表了一种有前景的技术。Pasqal发展了一种使用铷原子的中性原子噪声中等规模量子（NISQ）设备，可处理多达100个量子比特。所有NISQ设备都受到噪声的影响，这对计算结果有着一定影响。因此，更好地了解和表征噪声源并可能纠正它们非常重要。本文提出了两种方法来表征和校正中性原子NISQ设备上的噪声参数。特别关注Pasqal设备，并采用机器学习（ML）技术来实现这些目标。为了表征噪声参数，训练了多个ML模型，只使用原子最终量子态的测量结果作为输入，来预测激光强度波动和腰围、温度以及假阳性和假阴性测量值等。

    Neutral atoms devices represent a promising technology that uses optical tweezers to geometrically arrange atoms and modulated laser pulses to control the quantum states. A neutral atoms Noisy Intermediate Scale Quantum (NISQ) device is developed by Pasqal with rubidium atoms that will allow to work with up to 100 qubits. All NISQ devices are affected by noise that have an impact on the computations results. Therefore it is important to better understand and characterize the noise sources and possibly to correct them. Here, two approaches are proposed to characterize and correct noise parameters on neutral atoms NISQ devices. In particular the focus is on Pasqal devices and Machine Learning (ML) techniques are adopted to pursue those objectives. To characterize the noise parameters, several ML models are trained, using as input only the measurements of the final quantum state of the atoms, to predict laser intensity fluctuation and waist, temperature and false positive and negative mea
    
[^8]: LeanDojo: 检索增强语言模型的定理证明

    LeanDojo: Theorem Proving with Retrieval-Augmented Language Models. (arXiv:2306.15626v1 [cs.LG])

    [http://arxiv.org/abs/2306.15626](http://arxiv.org/abs/2306.15626)

    本文引入了LeanDojo，该工具通过提取Lean的数据，为定理证明研究提供了一个开放源代码的平台。利用LeanDojo的数据，开发了ReProver，它是第一个使用检索增强的语言模型的证明器，可以从庞大的数学库中选择命题，训练成本低，并且只需要一周的GPU训练时间。

    

    大型语言模型（LLM）已经显示出在使用Lean等证明助手证明形式定理方面的潜力。然而，由于私有代码、数据和大量计算要求，现有的方法很难复制或建立在其基础上，这给定理证明的机器学习方法的研究带来了巨大的障碍。本文通过引入LeanDojo来消除这些障碍：一个包含工具包、数据、模型和基准测试的开放源代码的Lean游乐场。LeanDojo从Lean中提取数据，并使得可以通过编程与证明环境进行交互。它包含证明中命题的细粒度注释，为命题选择提供了有价值的数据：这是定理证明中的一个关键瓶颈。利用这些数据，我们开发出了ReProver（检索增强的证明器）：它是第一个使用LLM的证明器，通过检索从庞大的数学库中选择命题。它成本低廉，只需要一周的GPU训练时间。我们的检索器利用了LeanDojo的pro相关功能。

    Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): the first LLM-based prover that is augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's prog
    
[^9]: 针对离策略强化学习的价值感知重要性加权

    Value-aware Importance Weighting for Off-policy Reinforcement Learning. (arXiv:2306.15625v1 [cs.LG])

    [http://arxiv.org/abs/2306.15625](http://arxiv.org/abs/2306.15625)

    本文提出了一种称为“价值感知重要性权重”的方法，用于校正离策略学习中的样本。这种方法可以降低重要性采样权重的方差并保持无偏性，从而提高实践中的稳定性和效果。

    

    重要性采样是强化学习中离策略预测的一个核心思想，它提供了一种策略，可以通过对一个分布中的样本进行重新加权，从而在另一个分布下获得无偏的估计值。然而，重要性采样权重往往会表现出极端的方差，常常导致实践中的稳定性问题。在这项工作中，我们考虑了一类更广泛的重要性权重来校正离策略学习中的样本。我们提出了使用“价值感知重要性权重”，它考虑了样本空间，从而在目标分布下提供更低的方差，但仍然是无偏的估计。我们推导了如何计算这样的权重，并详细说明了结果重要性权重的关键属性。然后，我们将几个强化学习预测算法扩展到使用这些权重的离策略设置，并进行了实证评估。

    Importance sampling is a central idea underlying off-policy prediction in reinforcement learning. It provides a strategy for re-weighting samples from a distribution to obtain unbiased estimates under another distribution. However, importance sampling weights tend to exhibit extreme variance, often leading to stability issues in practice. In this work, we consider a broader class of importance weights to correct samples in off-policy learning. We propose the use of $\textit{value-aware importance weights}$ which take into account the sample space to provide lower variance, but still unbiased, estimates under a target distribution. We derive how such weights can be computed, and detail key properties of the resulting importance weights. We then extend several reinforcement learning prediction algorithms to the off-policy setting with these weights, and evaluate them empirically.
    
[^10]: SCENEREPLICA：通过创建可重复的场景来评估现实世界中的机器人操纵能力的基准测试

    SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating Reproducible Scenes. (arXiv:2306.15620v1 [cs.RO])

    [http://arxiv.org/abs/2306.15620](http://arxiv.org/abs/2306.15620)

    SCENEREPLICA是一个基于YCB对象的可重复性基准测试，用于评估现实世界中的机器人操纵能力。此基准测试易于重复并允许研究人员比较不同的技术和算法，有助于加快机器人操纵方法的发展。

    

    我们提出了一个新的可重复性基准测试，用于评估现实世界中的机器人操纵能力，特别关注抓取和放置任务。我们的基准测试使用了YCB对象，这是机器人学界常用的数据集，确保我们的结果可以与其他研究进行比较。此外，此基准测试还被设计为在现实世界中易于重复，使其可供研究人员和实践者使用。我们还提供了对基准测试中基于模型和无模型的6D机器人抓取的实验结果和分析，其中评估了代表性算法在物体感知、抓取规划和运动规划方面的性能。我们相信我们的基准测试将成为推动机器人操纵领域发展的宝贵工具。通过提供一个标准化的评估框架，研究人员可以更容易地比较不同的技术和算法，从而加快发展机器人操纵方法的进展。

    We present a new reproducible benchmark for evaluating robot manipulation in the real world, specifically focusing on pick-and-place. Our benchmark uses the YCB objects, a commonly used dataset in the robotics community, to ensure that our results are comparable to other studies. Additionally, the benchmark is designed to be easily reproducible in the real world, making it accessible to researchers and practitioners. We also provide our experimental results and analyzes for model-based and model-free 6D robotic grasping on the benchmark, where representative algorithms are evaluated for object perception, grasping planning, and motion planning. We believe that our benchmark will be a valuable tool for advancing the field of robot manipulation. By providing a standardized evaluation framework, researchers can more easily compare different techniques and algorithms, leading to faster progress in developing robot manipulation methods.
    
[^11]: DCID: 深度规范信息分解

    DCID: Deep Canonical Information Decomposition. (arXiv:2306.15619v1 [cs.LG])

    [http://arxiv.org/abs/2306.15619](http://arxiv.org/abs/2306.15619)

    DCID是一种用于识别一维目标变量之间共享信号的方法，传统方法在一元情况下效果不佳。我们提出了ICM评估指标，用于评估学习到的共享特征，并进一步提出了深度规范信息分解（DCID）方法。

    

    我们考虑在存在额外的多变量观测的情况下，识别两个一维目标变量之间共享的信号的问题。传统上，基于典型相关分析（CCA）的方法被用于识别共享变量，然而，它们被设计用于多变量目标，并且对于一元情况只提供微不足道的解决方案。在多任务学习（MTL）的背景下，提出了各种模型来学习稀疏且共享的特征。然而，这些方法通常通过他们的预测性能进行评估。据我们所知，以往的研究没有系统地评估模型是否能正确地恢复共享的信号。在这里，我们形式化了一元共享信息检索的设置，并提出了ICM，一种可以在存在真实标签的情况下使用的评估指标，评估学习到的共享特征的三个方面。我们进一步提出了深度规范信息分解（DCID）方法，该方法可以将多变量目标一致扩展到一维情况。

    We consider the problem of identifying the signal shared between two one-dimensional target variables, in the presence of additional multivariate observations. Canonical Correlation Analysis (CCA)-based methods have traditionally been used to identify shared variables, however, they were designed for multivariate targets and only offer trivial solutions for univariate cases. In the context of Multi-Task Learning (MTL), various models were postulated to learn features that are sparse and shared across multiple tasks. However, these methods were typically evaluated by their predictive performance. To the best of our knowledge, no prior studies systematically evaluated models in terms of correctly recovering the shared signal. Here, we formalize the setting of univariate shared information retrieval, and propose ICM, an evaluation metric which can be used in the presence of ground-truth labels, quantifying 3 aspects of the learned shared features. We further propose Deep Canonical Informa
    
[^12]: 通过动态模态分解学习非自治系统

    Learning Nonautonomous Systems via Dynamic Mode Decomposition. (arXiv:2306.15618v1 [math.NA])

    [http://arxiv.org/abs/2306.15618](http://arxiv.org/abs/2306.15618)

    通过动态模态分解，我们提出了一种学习未知非自治系统的数据驱动方法，并通过修改系统和降维插值构建参数化代理模型来解决时间相关库普曼算子的近似问题。

    

    我们提出了一种基于动态模态分解（DMD）的数据驱动学习方法，用于未知的非自治动力系统，该系统具有时间相关的输入。为了规避对非自治系统中的时间相关库普曼算子进行近似的困难，我们采用了从外部时间相关输入的局部参数化导出的修改系统作为对原始非自治系统的近似。修改后的系统由一系列局部参数化系统组成，可以通过我们先前提出的在参数空间中进行降维和插值的框架（DRIPS）使用参数化代理模型进行很好的近似。DRIPS的离线步骤依赖于DMD来构建线性代理模型，并赋予从训练数据映射而来的可观测值降序基（ROBs）。然后，离线步骤从适当流形上的插值构造一系列迭代参数代理模型，其中目标/测试参数点可以从插值和正交操作中回归。

    We present a data-driven learning approach for unknown nonautonomous dynamical systems with time-dependent inputs based on dynamic mode decomposition (DMD). To circumvent the difficulty of approximating the time-dependent Koopman operators for nonautonomous systems, a modified system derived from local parameterization of the external time-dependent inputs is employed as an approximation to the original nonautonomous system. The modified system comprises a sequence of local parametric systems, which can be well approximated by a parametric surrogate model using our previously proposed framework for dimension reduction and interpolation in parameter space (DRIPS). The offline step of DRIPS relies on DMD to build a linear surrogate model, endowed with reduced-order bases (ROBs), for the observables mapped from training data. Then the offline step constructs a sequence of iterative parametric surrogate models from interpolations on suitable manifolds, where the target/test parameter point
    
[^13]: 通过位置插值扩展大型语言模型的上下文窗口

    Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v1 [cs.CL])

    [http://arxiv.org/abs/2306.15595](http://arxiv.org/abs/2306.15595)

    通过位置插值方法，我们可以在最小微调的情况下将RoPE-based预训练语言模型的上下文窗口扩展到最多32768，并在多个任务上获得强有力的实证结果。通过线性降低输入位置索引的大小，我们保持了扩展模型在原始上下文窗口内任务的质量。

    

    我们提出了一种位置插值（PI）方法，可以在最小微调的情况下将RoPE-based预训练语言模型（如LLaMA模型）的上下文窗口大小扩展到最多32768，并且在需要长上下文的各种任务（包括密钥检索、语言建模和长篇文档摘要等）上展现出良好的实证结果。同时，通过位置插值扩展的模型在原始上下文窗口内的任务中相对保持良好的质量。为了实现这一目标，位置插值线性地降低输入位置索引的大小，以匹配原始的上下文窗口大小，而不是超过训练时上下文长度，这可能会导致严重的高注意力分数，完全破坏自注意机制。我们的理论研究表明，插值的上界至少是推断的上界的$\sim 600 \times$要小，进一步证明了其稳定性。

    We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\sim 600 \times$ smaller than that of extrapolation, further demonstrating its stability. Models extend
    
[^14]: 学习在动态网络中航行：MARLIN强化学习框架在战术环境中的拥塞控制

    Learning to Sail Dynamic Networks: The MARLIN Reinforcement Learning Framework for Congestion Control in Tactical Environments. (arXiv:2306.15591v1 [cs.LG])

    [http://arxiv.org/abs/2306.15591](http://arxiv.org/abs/2306.15591)

    本论文提出了一个强化学习框架MARLIN，用于在战术环境中进行拥塞控制。该框架利用准确且可并行化的仿真环境来模拟战术网络条件，并引入了适用于复杂场景中的代理的精细化强化学习公式和评估方法。通过训练MARLIN智能体在模拟条件下进行瓶颈链路转换的性能评估，我们证明了该框架的有效性。

    

    传统的拥塞控制算法，如TCP Cubic，在战术环境中遇到困难，因为它们错误地将丢包和网络性能波动视为拥塞症状。最近的努力，包括我们自己的MARLIN，已经探索了使用强化学习（RL）进行拥塞控制，但在竞争激烈、不稳定和意外情况下往往不能很好地泛化。为了解决这些挑战，本文提出了一个RL框架，利用准确且可并行化的仿真环境重现战术网络的条件。我们还引入了适用于在这种复杂场景中运行的代理的精细化RL公式和性能评估方法。我们通过训练一个在卫星通信（SATCOM）和UHF宽带（UHF）无线电链路之间模拟瓶颈链路转换条件的MARLIN智能体来评估我们的RL学习框架。最后，我们将其在文件传输任务中与Transm的性能进行了比较。

    Conventional Congestion Control (CC) algorithms,such as TCP Cubic, struggle in tactical environments as they misinterpret packet loss and fluctuating network performance as congestion symptoms. Recent efforts, including our own MARLIN, have explored the use of Reinforcement Learning (RL) for CC, but they often fall short of generalization, particularly in competitive, unstable, and unforeseen scenarios. To address these challenges, this paper proposes an RL framework that leverages an accurate and parallelizable emulation environment to reenact the conditions of a tactical network. We also introduce refined RL formulation and performance evaluation methods tailored for agents operating in such intricate scenarios. We evaluate our RL learning framework by training a MARLIN agent in conditions replicating a bottleneck link transition between a Satellite Communication (SATCOM) and an UHF Wide Band (UHF) radio link. Finally, we compared its performance in file transfer tasks against Transm
    
[^15]: 使用强化学习优化对抗目标下的信用额度调整

    Optimizing Credit Limit Adjustments Under Adversarial Goals Using Reinforcement Learning. (arXiv:2306.15585v1 [q-fin.GN])

    [http://arxiv.org/abs/2306.15585](http://arxiv.org/abs/2306.15585)

    本研究使用强化学习技术，通过平衡最大化投资组合收入和最小化准备金的对抗目标，自动化寻找最优信用卡额度调整策略。

    

    强化学习已经在很多问题中得到应用，从具有确定性环境的视频游戏到具有随机场景的投资组合和运营管理；然而，在银行问题中对这些方法的测试尝试很少。在本研究中，我们试图通过使用强化学习技术找到并自动化最优信用卡额度调整策略。具体而言，由于有历史数据可用，我们考虑每个客户的两种可能操作，即增加或保持个人当前的信用额度。为了找到这个策略，我们首先将这个决策问题形式化为一个优化问题，在其中最大化预期利润；因此，我们平衡了两个对抗目标：最大化投资组合的收入和最小化投资组合的准备金。其次，考虑到我们问题的特殊性，我们使用了离线学习策略，以基于历史数据模拟行动的影响。

    Reinforcement learning has been explored for many problems, from video games with deterministic environments to portfolio and operations management in which scenarios are stochastic; however, there have been few attempts to test these methods in banking problems. In this study, we sought to find and automatize an optimal credit card limit adjustment policy by employing reinforcement learning techniques. In particular, because of the historical data available, we considered two possible actions per customer, namely increasing or maintaining an individual's current credit limit. To find this policy, we first formulated this decision-making question as an optimization problem in which the expected profit was maximized; therefore, we balanced two adversarial goals: maximizing the portfolio's revenue and minimizing the portfolio's provisions. Second, given the particularities of our problem, we used an offline learning strategy to simulate the impact of the action based on historical data f
    
[^16]: 近似传递消息在矩阵张量乘积模型中的应用

    Approximate Message Passing for the Matrix Tensor Product Model. (arXiv:2306.15580v1 [stat.ML])

    [http://arxiv.org/abs/2306.15580](http://arxiv.org/abs/2306.15580)

    本文提出了一种针对矩阵张量乘积模型的近似传递消息（AMP）算法，并分析了其性能。该算法通过权衡和组合多个估计来优化算法迭代过程。通过对非可分函数的AMP收敛定理和状态演化的研究，我们给出了恢复目标信号所需的必要和充分条件。该算法适用于多种类型的两两观测。

    

    我们提出了一种针对矩阵张量乘积模型的近似传递消息（AMP）算法，并进行了分析。该模型是标准尖峰矩阵模型的推广，允许对一组潜在变量进行多种类型的两两观测。该算法的一个关键创新是在每次迭代中通过一种方法对多个估计进行最优权重和组合。借助非可分函数的AMP收敛定理，我们证明了非可分函数的状态演化，从而在高维极限下提供了其性能的渐近精确描述。我们利用这个状态演化结果给出了恢复目标信号所需的必要和充分条件。这些条件取决于从我们模型的适当推广中导出的线性算子的奇异值，这个线性算子是一个适当推广的信噪比的一个重要组成部分。我们的结果作为特殊情况涵盖了一些最近提出的背景模型方法（例如，covariance-）

    We propose and analyze an approximate message passing (AMP) algorithm for the matrix tensor product model, which is a generalization of the standard spiked matrix models that allows for multiple types of pairwise observations over a collection of latent variables. A key innovation for this algorithm is a method for optimally weighing and combining multiple estimates in each iteration. Building upon an AMP convergence theorem for non-separable functions, we prove a state evolution for non-separable functions that provides an asymptotically exact description of its performance in the high-dimensional limit. We leverage this state evolution result to provide necessary and sufficient conditions for recovery of the signal of interest. Such conditions depend on the singular values of a linear operator derived from an appropriate generalization of a signal-to-noise ratio for our model. Our results recover as special cases a number of recently proposed methods for contextual models (e.g., cova
    
[^17]: PyBADS：Python中快速而稳健的黑盒优化

    PyBADS: Fast and robust black-box optimization in Python. (arXiv:2306.15576v1 [stat.ML])

    [http://arxiv.org/abs/2306.15576](http://arxiv.org/abs/2306.15576)

    PyBADS是Python中一种快速而稳健的黑盒优化算法，适用于解决目标函数粗糙、计算代价高、可能存在噪声且梯度信息不可用的困难优化问题。它支持高达20个连续输入参数的黑盒函数，并提供易于使用的Python接口。

    

    PyBADS是Bayesian Adaptive Direct Search（BADS）算法的Python实现，用于快速而稳健的黑盒优化（Acerbi和Ma，2017）。BADS是一种针对目标函数粗糙（非凸、非光滑）、计算代价较高（例如函数评估需要超过0.1秒）、可能存在噪声且梯度信息不可用的困难优化问题而设计的优化算法。通过BADS，这些问题得到了很好的解决，使其成为使用最大似然估计等方法拟合计算模型的优秀选择。该算法在具有高达$D \approx 20$个连续输入参数的黑盒函数上具有高效的扩展性，并支持边界约束或无约束。PyBADS提供了一个易于使用的Python接口，用于运行算法和检查其结果。PyBADS只需要用户提供一个用于评估目标函数的Python函数，以及其他约束（可选）。

    PyBADS is a Python implementation of the Bayesian Adaptive Direct Search (BADS) algorithm for fast and robust black-box optimization (Acerbi and Ma 2017). BADS is an optimization algorithm designed to efficiently solve difficult optimization problems where the objective function is rough (non-convex, non-smooth), mildly expensive (e.g., the function evaluation requires more than 0.1 seconds), possibly noisy, and gradient information is unavailable. With BADS, these issues are well addressed, making it an excellent choice for fitting computational models using methods such as maximum-likelihood estimation. The algorithm scales efficiently to black-box functions with up to $D \approx 20$ continuous input parameters and supports bounds or no constraints. PyBADS comes along with an easy-to-use Pythonic interface for running the algorithm and inspecting its results. PyBADS only requires the user to provide a Python function for evaluating the target function, and optionally other constraint
    
[^18]: 透过迷雾看清楚：在医学影像中采用渐进遮挡的课程学习

    See Through the Fog: Curriculum Learning with Progressive Occlusion in Medical Imaging. (arXiv:2306.15574v1 [cs.CV])

    [http://arxiv.org/abs/2306.15574](http://arxiv.org/abs/2306.15574)

    本文提出了一种基于课程学习的方法，用于训练深度学习模型有效处理医学图像中的遮挡情况。通过逐步引入遮挡，模型首先学习简单、可辨别的模式，然后逐渐理解更复杂的遮挡场景。

    

    近年来，深度学习模型在医学图像解释方面取得了革命性的进展，显著提高了诊断准确性。然而，这些模型在具有部分或完全遮挡的复杂图像上往往表现不佳，而这在临床实践中非常常见。在本文中，我们提出了一种基于课程学习的全新方法，用于训练深度学习模型有效处理遮挡医学图像。我们的方法逐步引入遮挡，从清晰、无遮挡的图像开始，逐渐过渡到遮挡程度不断增加的图像。这种有序的学习过程类似于人类学习的过程，使模型首先掌握简单、可辨别的模式，然后在此基础上逐渐理解更复杂的遮挡场景。此外，我们还提出了三种全新的遮挡生成方法，分别是Wasserstein课程学习（WCL）、信息自适应学习（IAL）和测地线课程学习（Geodesic Curriculum Learn）。

    In recent years, deep learning models have revolutionized medical image interpretation, offering substantial improvements in diagnostic accuracy. However, these models often struggle with challenging images where critical features are partially or fully occluded, which is a common scenario in clinical practice. In this paper, we propose a novel curriculum learning-based approach to train deep learning models to handle occluded medical images effectively. Our method progressively introduces occlusion, starting from clear, unobstructed images and gradually moving to images with increasing occlusion levels. This ordered learning process, akin to human learning, allows the model to first grasp simple, discernable patterns and subsequently build upon this knowledge to understand more complicated, occluded scenarios. Furthermore, we present three novel occlusion synthesis methods, namely Wasserstein Curriculum Learning (WCL), Information Adaptive Learning (IAL), and Geodesic Curriculum Learn
    
[^19]: 生成初等可积表达式

    Generating Elementary Integrable Expressions. (arXiv:2306.15572v1 [cs.SC])

    [http://arxiv.org/abs/2306.15572](http://arxiv.org/abs/2306.15572)

    本文介绍了如何利用Risch算法生成初等可积表达式的数据集，并展示了这种方式生成的数据可以减轻早期方法中存在的一些缺陷。

    

    近年来，机器学习在计算机代数领域的应用数量不断增长，包括在符号积分的重要子领域中的应用。然而，机器学习模型需要大量的数据才能取得成功，但对于它们而言，目前存在着很少符合实际需求的基准。虽然已经存在一些生成新数据的方法，但它们存在一些缺陷，可能导致训练在其上的机器学习模型出现偏差。在本文中，我们描述了如何使用Risch算法进行符号积分，从而创建一个初等可积表达式的数据集。此外，我们还展示了以这种方式生成的数据可以减轻早期方法中发现的一些缺陷。

    There has been an increasing number of applications of machine learning to the field of Computer Algebra in recent years, including to the prominent sub-field of Symbolic Integration. However, machine learning models require an abundance of data for them to be successful and there exist few benchmarks on the scale required. While methods to generate new data already exist, they are flawed in several ways which may lead to bias in machine learning models trained upon them. In this paper, we describe how to use the Risch Algorithm for symbolic integration to create a dataset of elementary integrable expressions. Further, we show that data generated this way alleviates some of the flaws found in earlier methods.
    
[^20]: RansomAI: 基于人工智能的隐蔽加密勒索软件

    RansomAI: AI-powered Ransomware for Stealthy Encryption. (arXiv:2306.15559v1 [cs.CR])

    [http://arxiv.org/abs/2306.15559](http://arxiv.org/abs/2306.15559)

    本文提出了一种名为RansomAI的基于人工智能的隐蔽加密勒索软件，它通过强化学习来适应加密行为，最小化被检测的可能性，并在加密时最大化损害功能。

    

    近期，由于人工智能技术的爆发, 勒索软件（包括恶意软件）将会运用人工智能技术来智能而动态地适应其加密行为以避免被检测。这将导致现有的网络安全解决方案变得无效和过时，但是文献中缺乏基于人工智能的勒索软件来验证此观点。因此，本文提出了一种名为RansomAI的强化学习框架，它可以集成到现有的勒索软件样本中，使其加密行为具有适应性，并保持隐蔽性。RansomAI引入了一个智能代理，通过奖励机制和指纹智能检测系统来学习最佳的加密算法、速率和持续时间，以降低被检测的概率同时最大化其损害功能。

    Cybersecurity solutions have shown promising performance when detecting ransomware samples that use fixed algorithms and encryption rates. However, due to the current explosion of Artificial Intelligence (AI), sooner than later, ransomware (and malware in general) will incorporate AI techniques to intelligently and dynamically adapt its encryption behavior to be undetected. It might result in ineffective and obsolete cybersecurity solutions, but the literature lacks AI-powered ransomware to verify it. Thus, this work proposes RansomAI, a Reinforcement Learning-based framework that can be integrated into existing ransomware samples to adapt their encryption behavior and stay stealthy while encrypting files. RansomAI presents an agent that learns the best encryption algorithm, rate, and duration that minimizes its detection (using a reward mechanism and a fingerprinting intelligent detection system) while maximizing its damage function. The proposed framework was validated in a ransomwar
    
[^21]: 简单成功的步骤：基于距离的算法补偿的公理化方法

    Simple Steps to Success: Axiomatics of Distance-Based Algorithmic Recourse. (arXiv:2306.15557v1 [cs.LG])

    [http://arxiv.org/abs/2306.15557](http://arxiv.org/abs/2306.15557)

    我们提出了一种基于距离的算法补偿的新方法，通过在数据流形中提供用户可以采取的方向来改变其预测结果。该方法具有高效性、可证明的隐私和鲁棒性保证，并在实验证明上优于现有技术。

    

    我们提出了一个新颖的数据驱动框架，用于算法补偿，提供给用户改变其预测结果的干预措施。现有的计算补偿方法找到满足某些期望的点集，例如在基础因果图中的干预，或者最小化代价函数。然而，满足这些标准需要对基础模型结构有广泛的了解，在几个领域中往往需要大量的不切实际的信息。我们提出了一种数据驱动的、计算高效的算法补偿方法，通过在数据流形中提供用户可以采取的方向来改变其预测结果。我们提出了一种公理化合理化的方法，Stepwise Explainable Paths (StEP)，用于计算基于方向的算法补偿。我们对StEP进行了彻底的实证和理论研究。StEP提供了可证明的隐私和鲁棒性保证，并在几个已建立的补偿方法中表现优于现有技术。

    We propose a novel data-driven framework for algorithmic recourse that offers users interventions to change their predicted outcome. Existing approaches to compute recourse find a set of points that satisfy some desiderata -- e.g. an intervention in the underlying causal graph, or minimizing a cost function. Satisfying these criteria, however, requires extensive knowledge of the underlying model structure, often an unrealistic amount of information in several domains. We propose a data-driven, computationally efficient approach to computing algorithmic recourse. We do so by suggesting directions in the data manifold that users can take to change their predicted outcome. We present Stepwise Explainable Paths (StEP), an axiomatically justified framework to compute direction-based algorithmic recourse. We offer a thorough empirical and theoretical investigation of StEP. StEP offers provable privacy and robustness guarantees, and outperforms the state-of-the-art on several established reco
    
[^22]: 面向异构HPC平台的深度学习硬件加速器综述

    A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms. (arXiv:2306.15552v1 [cs.AR])

    [http://arxiv.org/abs/2306.15552](http://arxiv.org/abs/2306.15552)

    本综述调查了面向异构HPC平台的深度学习硬件加速器，包括GPU、TPU、FPGA、ASIC、神经处理单元和RISC-V等，同时也涵盖了新兴内存技术和计算范式。

    

    最近，深度学习在高性能计算（HPC）应用中，如图像分类、计算机视觉和语音识别中成为硬件加速器最可行的解决方案。本综述总结和分类了设计深度学习加速器的最新进展，以满足HPC应用的性能要求。特别地，它强调了支持深度学习加速的最先进方法，包括不仅限于基于GPU和TPU的加速器，还包括基于FPGA和ASIC的特定设计的硬件加速器、神经处理单元、基于开放硬件RISC-V的加速器和协处理器。本综述还描述了基于新兴内存技术和计算范式的加速器，例如3D堆叠处理器内存、非易失性存储器（主要是电阻式随机存取存储器和相变存储器）实现内存计算，神经形态学处理单元等。

    Recent trends in deep learning (DL) imposed hardware accelerators as the most viable solution for several classes of high-performance computing (HPC) applications such as image classification, computer vision, and speech recognition. This survey summarizes and classifies the most recent advances in designing DL accelerators suitable to reach the performance requirements of HPC applications. In particular, it highlights the most advanced approaches to support deep learning accelerations including not only GPU and TPU-based accelerators but also design-specific hardware accelerators such as FPGA-based and ASIC-based accelerators, Neural Processing Units, open hardware RISC-V-based accelerators and co-processors. The survey also describes accelerators based on emerging memory technologies and computing paradigms, such as 3D-stacked Processor-In-Memory, non-volatile memories (mainly, Resistive RAM and Phase Change Memories) to implement in-memory computing, Neuromorphic Processing Units, a
    
[^23]: CrunchGPT：基于ChatGPT的科学机器学习辅助框架

    CrunchGPT: A chatGPT assisted framework for scientific machine learning. (arXiv:2306.15551v1 [cs.LG])

    [http://arxiv.org/abs/2306.15551](http://arxiv.org/abs/2306.15551)

    CrunchGPT是一个基于ChatGPT的科学机器学习辅助框架，通过简单的用户提示来协调整个科学机器学习的工作流程，实现无缝集成数据和物理知识，解决了SciML在预处理、问题建模、代码生成、后处理和分析等方面的耗时问题，拓展了其工业应用和数字孪生框架的适用性。

    

    科学机器学习（SciML）近年来在计算科学和工程的许多不同领域取得了进展。其目标是在不需要复杂和计算密集的数据同化方案的情况下，无缝地将数据和物理知识集成起来。然而，预处理、问题建模、代码生成、后处理和分析仍然是耗时的，并且可能限制SciML在工业应用和数字孪生框架中的广泛适用性。在这里，我们将SciML的各个阶段整合到ChatGPT的伞下，形成CrunchGPT，它通过用户简单的提示来协调整个SciML的工作流程。具体而言，我们提供了两个示例，演示了CrunchGPT在气动学中优化机翼和在各种几何形状中获得流场的潜在用途，并强调了验证阶段。为了演示CrunchGPT的流程和

    Scientific Machine Learning (SciML) has advanced recently across many different areas in computational science and engineering. The objective is to integrate data and physics seamlessly without the need of employing elaborate and computationally taxing data assimilation schemes. However, preprocessing, problem formulation, code generation, postprocessing and analysis are still time consuming and may prevent SciML from wide applicability in industrial applications and in digital twin frameworks. Here, we integrate the various stages of SciML under the umbrella of ChatGPT, to formulate CrunchGPT, which plays the role of a conductor orchestrating the entire workflow of SciML based on simple prompts by the user. Specifically, we present two examples that demonstrate the potential use of CrunchGPT in optimizing airfoils in aerodynamics, and in obtaining flow fields in various geometries in interactive mode, with emphasis on the validation stage. To demonstrate the flow of the CrunchGPT, and
    
[^24]: 几何超声定位显微镜

    Geometric Ultrasound Localization Microscopy. (arXiv:2306.15548v1 [cs.CV])

    [http://arxiv.org/abs/2306.15548](http://arxiv.org/abs/2306.15548)

    这项研究提出了一种基于几何框架的超声定位显微镜方法，通过仅依赖到达时间差信息实现微气泡的定位，并在精度和可靠性方面超越了现有的方法。

    

    对比增强超声（CEUS）已成为无创动态可视化医学诊断方法，然而超声定位显微镜（ULM）通过提供十倍更高的分辨率实现了突破性进展。到目前为止，延迟和求和（DAS）波束形成器被用于渲染ULM帧，最终确定图像的分辨率能力。为了充分利用ULM，本研究质疑波束形成是否是ULM最有效的处理步骤，提出了一种仅依赖到达时间差（TDoA）信息的替代方法。为此，提出了一种新颖的通过椭圆交点定位微气泡的几何框架，以克服现有波束形成的局限性。我们基于一个公共数据集进行了基准比较，结果表明我们的几何ULM在精度和可靠性方面优于现有的基准方法，仅利用了部分可用的换能器数据。

    Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound Localization Microscopy (ULM) has enabled a revolutionary breakthrough by offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers are used to render ULM frames, ultimately determining the image resolution capability. To take full advantage of ULM, this study questions whether beamforming is the most effective processing step for ULM, suggesting an alternative approach that relies solely on Time-Difference-of-Arrival (TDoA) information. To this end, a novel geometric framework for micro bubble localization via ellipse intersections is proposed to overcome existing beamforming limitations. We present a benchmark comparison based on a public dataset for which our geometric ULM outperforms existing baseline methods in terms of accuracy and reliability while only utilizing a portion of the available transducer data.
    
[^25]: 当基础模型遇到联邦学习：动机、挑战和未来方向

    When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions. (arXiv:2306.15546v1 [cs.LG])

    [http://arxiv.org/abs/2306.15546](http://arxiv.org/abs/2306.15546)

    基础模型与联邦学习的交叉提供了解锁新可能性的独特机会，扩展了数据可用性，促进了协作式模型发展，并提高了性能和隐私保护。

    

    基础模型（FM）与联邦学习（FL）的交叉提供了相互的好处，在AI研究中提供了解锁新可能性的独特机会，解决了AI和现实世界应用中的关键挑战。FL扩展了FM的数据可用性，并实现了计算共享，分散了训练过程，并减轻了FL参与者的负担。它促进了协作式FM发展，民主化了这一过程，促进了包容性和创新。另一方面，FM以其庞大的规模、预训练的知识和出色的性能，为FL提供了一个强大的起点，促进了在非独立同分布数据下更快的收敛和更好的性能。此外，利用FM生成合成数据可以丰富数据多样性，减少过拟合，保护隐私。通过研究FL和FM之间的相互作用，本文旨在加深对它们协同关系的理解，强调动机和挑战。

    The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations,
    
[^26]: DataCI: 一个用于流数据中数据中心人工智能的平台

    DataCI: A Platform for Data-Centric AI on Streaming Data. (arXiv:2306.15538v1 [cs.DC])

    [http://arxiv.org/abs/2306.15538](http://arxiv.org/abs/2306.15538)

    DataCI是一个开源平台，专为流数据中的数据中心人工智能而设计，提供丰富的API和版本控制功能，具有易于使用和有效性，可以改变流数据背景下的数据中心人工智能实践。

    

    我们介绍了DataCI，这是一个专门用于动态流数据场景中的数据中心人工智能的综合开源平台。DataCI提供了基础设施，具有丰富的API，用于无缝流数据集管理、数据中心流程的开发和评估，并且提供了精心设计的版本控制功能，以跟踪流程的衍生。另外，DataCI还提供了直观的图形界面，提供更好的交互体验。初步研究和演示证明了DataCI易于使用和有效性，凸显了它在流数据背景下改变数据中心人工智能实践的潜力。

    We introduce DataCI, a comprehensive open-source platform designed specifically for data-centric AI in dynamic streaming data settings. DataCI provides 1) an infrastructure with rich APIs for seamless streaming dataset management, data-centric pipeline development and evaluation on streaming scenarios, 2) an carefully designed versioning control function to track the pipeline lineage, and 3) an intuitive graphical interface for a better interactive user experience. Preliminary studies and demonstrations attest to the easy-to-use and effectiveness of DataCI, highlighting its potential to revolutionize the practice of data-centric AI in streaming data contexts.
    
[^27]: 基于高阶图注意力网络的股票选股方法及联合分析

    Higher-order Graph Attention Network for Stock Selection with Joint Analysis. (arXiv:2306.15526v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.15526](http://arxiv.org/abs/2306.15526)

    我们提出了一种基于高阶图注意力网络和联合分析的股票选股方法（H-GAT），它能够捕捉涉及超过两个节点的复杂高阶结构，并同时结合基本分析因素和技术分析因素，对股票选股具有较好的效果。

    

    股票选股对于投资者构建有利可图的投资组合至关重要。图神经网络（GNN）由于其在关系建模和泛化方面的强大能力，越来越受到研究人员的关注。然而，现有的GNN方法仅关注简单的股票配对关系，并不能捕捉涉及超过两个节点的复杂高阶结构模型。此外，它们只考虑技术分析因素，忽视了可能对股票趋势产生显著影响的基本分析因素。基于这些动机，我们提出了一种基于高阶图注意力网络和联合分析的股票选股方法（H-GAT）。H-GAT能够捕捉高阶结构，并同时结合基本分析因素和技术分析因素。具体而言，H-GAT的顺序层将这两种类型的因素作为长短期记忆模型的输入。H-GAT的关系嵌入层构建了一个高阶图且学习了

    Stock selection is important for investors to construct profitable portfolios. Graph neural networks (GNNs) are increasingly attracting researchers for stock prediction due to their strong ability of relation modelling and generalisation. However, the existing GNN methods only focus on simple pairwise stock relation and do not capture complex higher-order structures modelling relations more than two nodes. In addition, they only consider factors of technical analysis and overlook factors of fundamental analysis that can affect the stock trend significantly. Motivated by them, we propose higher-order graph attention network with joint analysis (H-GAT). H-GAT is able to capture higher-order structures and jointly incorporate factors of fundamental analysis with factors of technical analysis. Specifically, the sequential layer of H-GAT take both types of factors as the input of a long-short term memory model. The relation embedding layer of H-GAT constructs a higher-order graph and learn 
    
[^28]: 优先轨迹回放：一种用于数据驱动强化学习的回放记忆方法

    Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning. (arXiv:2306.15503v1 [cs.LG])

    [http://arxiv.org/abs/2306.15503](http://arxiv.org/abs/2306.15503)

    本研究提出了一种名为优先轨迹回放的回放记忆方法，将数据采样的视角扩展到轨迹中，从有限的数据中提取更全面的信息。这种方法通过反向采样轨迹来提高学习效率，并利用加权评论目标避免采样未见过的动作。优先轨迹回放还能根据不同的优先度指标优先采样效率更高的轨迹。

    

    近年来，数据驱动的强化学习（RL），也称为离线RL，引起了广泛关注。然而，尽管其具有提升在线RL性能的潜力，但离线RL中的数据采样技术的作用却被忽视了。最近的研究表明，直接将采样技术应用于状态转换并不能始终提高离线RL的性能。因此，在本研究中，我们提出了一种记忆技术——优先轨迹回放（TR/PTR），它将采样的视角扩展到轨迹中，以从有限的数据中提取更全面的信息。TR通过反向采样轨迹来提高学习效率，优化后续状态信息的使用。在TR的基础上，我们构建了加权评论目标，以避免在离线训练中采样未见过的动作，并且引入了优先轨迹回放（PTR）来实现更高效的轨迹采样，根据不同的轨迹优先度指标进行优先设置。我们演示了...

    In recent years, data-driven reinforcement learning (RL), also known as offline RL, have gained significant attention. However, the role of data sampling techniques in offline RL has been overlooked despite its potential to enhance online RL performance. Recent research suggests applying sampling techniques directly to state-transitions does not consistently improve performance in offline RL. Therefore, in this study, we propose a memory technique, (Prioritized) Trajectory Replay (TR/PTR), which extends the sampling perspective to trajectories for more comprehensive information extraction from limited data. TR enhances learning efficiency by backward sampling of trajectories that optimizes the use of subsequent state information. Building on TR, we build the weighted critic target to avoid sampling unseen actions in offline training, and Prioritized Trajectory Replay (PTR) that enables more efficient trajectory sampling, prioritized by various trajectory priority metrics. We demonstrat
    
[^29]: 一种用于改进分类任务可解释性的新型结构化论证框架

    A novel structured argumentation framework for improved explainability of classification tasks. (arXiv:2306.15500v1 [cs.AI])

    [http://arxiv.org/abs/2306.15500](http://arxiv.org/abs/2306.15500)

    本文提出了一种新的结构化论证框架$xADG$，通过使用布尔逻辑运算符和多个支持来构建简洁且可理解的论证图，从而改进了分类任务的可解释性和预测能力。

    

    本文提出了一种名为扩展论证决策图（$xADG$）的新型结构化论证框架。它是基于Dung的抽象论证图构建的论证决策图的拓展。$xADG$框架允许论证使用布尔逻辑运算符和多个前提（支持）在内部结构中，从而得到更简洁的论证图，使用户更容易理解。该研究提出了一种构建$xADG$的方法，并评估了它们在不同规模分类任务中的大小和预测能力。结果显示，得到的$xADG$具有强（平衡的）准确性，这是通过输入决策树实现的，同时还减少了达到结论所需的平均支持数。结果进一步表明，可以构建出在预测能力和总体大小方面优于其他构建$ADG$技术的可理解的$xADG$。

    This paper presents a novel framework for structured argumentation, named extend argumentative decision graph ($xADG$). It is an extension of argumentative decision graphs built upon Dung's abstract argumentation graphs. The $xADG$ framework allows for arguments to use boolean logic operators and multiple premises (supports) within their internal structure, resulting in more concise argumentation graphs that may be easier for users to understand. The study presents a methodology for construction of $xADGs$ and evaluates their size and predictive capacity for classification tasks of varying magnitudes. Resulting $xADGs$ achieved strong (balanced) accuracy, which was accomplished through an input decision tree, while also reducing the average number of supports needed to reach a conclusion. The results further indicated that it is possible to construct plausibly understandable $xADGs$ that outperform other techniques for building $ADGs$ in terms of predictive capacity and overall size. I
    
[^30]: 合作还是竞争：通过自适应预算避免多目标鲁棒性中的玩家主导

    Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets. (arXiv:2306.15482v1 [cs.AI])

    [http://arxiv.org/abs/2306.15482](http://arxiv.org/abs/2306.15482)

    本文研究了多目标鲁棒性中的合作与竞争之间的平衡问题，提出了一种通过调整对手预算来避免玩家主导的新框架，从而显著提高了多目标鲁棒性。

    

    尽管取得了令人难以置信的进展，深度学习已被证明容易受到对抗性攻击的影响。已经提出了许多方法来以经验和可证明的方式训练鲁棒网络。但是，大多数方法只能防御一种类型的攻击，而最近的研究在防御多种攻击方面有所进展。本文将多目标鲁棒性问题视为一个博弈过程，在这个过程中，不同的玩家（对手）通过协商在参数更新的方向上达成一致。我们发现了一个被称为玩家主导的现象，在这个博弈中，现有的最大值为基础的方法，如MAX和MSD，无法收敛。基于我们的理论分析，我们设计了一个新的框架，通过调整不同对手的预算来避免任何玩家的主导。在标准基准测试上的实验证明，将提出的框架应用于现有方法显著提高了多目标鲁棒性。

    Despite incredible advances, deep learning has been shown to be susceptible to adversarial attacks. Numerous approaches have been proposed to train robust networks both empirically and certifiably. However, most of them defend against only a single type of attack, while recent work takes steps forward in defending against multiple attacks. In this paper, to understand multi-target robustness, we view this problem as a bargaining game in which different players (adversaries) negotiate to reach an agreement on a joint direction of parameter updating. We identify a phenomenon named player domination in the bargaining game, namely that the existing max-based approaches, such as MAX and MSD, do not converge. Based on our theoretical analysis, we design a novel framework that adjusts the budgets of different adversaries to avoid any player dominance. Experiments on standard benchmarks show that employing the proposed framework to the existing approaches significantly advances multi-target ro
    
[^31]: 通过预测编码进行因果推理

    Causal Inference via Predictive Coding. (arXiv:2306.15479v1 [cs.LG])

    [http://arxiv.org/abs/2306.15479](http://arxiv.org/abs/2306.15479)

    通过预测编码进行因果推理的技术使我们能够在已知和未知因果图的情况下进行端到端的因果推理和因果发现。

    

    贝叶斯推理和因果推理是智能的基本过程。贝叶斯推理模型观察：如果我们观察到一个相关变量x，我们能推断出关于y的信息吗？因果推理模型干预：如果我们直接改变x，y会如何变化？预测编码是一种受神经科学启发的方法，用于使用仅局部信息对连续状态变量进行贝叶斯推理。在这项工作中，我们超越了贝叶斯推理，并展示了如何通过对预测编码的推理过程进行简单更改，实现已知因果图的干预和反事实推理。然后我们扩展了我们的结果，并展示了如何将预测编码推广到因果图未知且需要通过数据推断的情况，从而进行因果发现。结果是一种新颖且直接的技术，可以在基于预测编码的结构因果模型上进行端到端的因果推理，并展示了其实用性。

    Bayesian and causal inference are fundamental processes for intelligence. Bayesian inference models observations: what can be inferred about y if we observe a related variable x? Causal inference models interventions: if we directly change x, how will y change? Predictive coding is a neuroscience-inspired method for performing Bayesian inference on continuous state variables using local information only. In this work, we go beyond Bayesian inference, and show how a simple change in the inference process of predictive coding enables interventional and counterfactual inference in scenarios where the causal graph is known. We then extend our results, and show how predictive coding can be generalized to cases where this graph is unknown, and has to be inferred from data, hence performing causal discovery. What results is a novel and straightforward technique that allows us to perform end-to-end causal inference on predictive-coding-based structural causal models, and demonstrate its utilit
    
[^32]: 大规模无监督音频预训练用于视频到语音合成

    Large-scale unsupervised audio pre-training for video-to-speech synthesis. (arXiv:2306.15464v1 [cs.SD])

    [http://arxiv.org/abs/2306.15464](http://arxiv.org/abs/2306.15464)

    本文提出了一种利用大规模无监督音频预训练的方法，用于视频到语音合成。通过训练编码器-解码器模型，我们可以在不需要视频对应的情况下，使用丰富的仅音频数据集进行合成。

    

    视频到语音合成是从无声视频中重建语音信号的任务。目前大多数已建立的方法都采用了一个两步法，首先从视频中提取中间表示，如谱图，然后传递给声码器生成原始音频。最近的一些工作专注于端到端合成，即同时生成原始音频和任何中间表示。所有这些方法都需要在几乎完全是音频-视觉数据集上进行训练，即每个音频样本都有对应的视频样本。这排除了使用丰富的仅音频数据集的可能性，这些数据集可能没有对应的视觉模态（例如有声读物、广播播客、语音识别数据集等），以及多年来由音频机器学习社区开发的仅音频架构。在本文中，我们建议在超过3500小时的数据上训练编码器-解码器模型。

    Video-to-speech synthesis is the task of reconstructing the speech signal from a silent video of a speaker. Most established approaches to date involve a two-step process, whereby an intermediate representation from the video, such as a spectrogram, is extracted first and then passed to a vocoder to produce the raw audio. Some recent work has focused on end-to-end synthesis, whereby the generation of raw audio and any intermediate representations is performed jointly. All such approaches involve training on data from almost exclusively audio-visual datasets, i.e. every audio sample has a corresponding video sample. This precludes the use of abundant audio-only datasets which may not have a corresponding visual modality (e.g. audiobooks, radio podcasts, speech recognition datasets etc.), as well as audio-only architectures that have been developed by the audio machine learning community over the years. In this paper we propose to train encoder-decoder models on more than 3,500 hours of 
    
[^33]: 对齐的神经网络是否对抗对齐？

    Are aligned neural networks adversarially aligned?. (arXiv:2306.15447v1 [cs.CL])

    [http://arxiv.org/abs/2306.15447](http://arxiv.org/abs/2306.15447)

    我们研究了大型语言模型在面对对抗用户构建的对抗性输入时是否仍能保持对齐。我们发现现有的攻击手法不足以可靠攻击对齐文本模型，并通过蛮力方法找到了对抗性输入。

    

    大型语言模型现在被调整为与其创建者的目标保持一致，即"有益且无害"。这些模型应该对用户的问题给出有益的回答，但拒绝回答可能会造成伤害的请求。然而，对抗用户可以构建绕过对齐尝试的输入。在这项工作中，我们研究了在与构造最坏情况输入（对抗性样本）的对抗用户交互时，这些模型保持多大程度的对齐。这些输入被设计成导致模型发出本应禁止的有害内容。我们展示了现有基于自然语言处理的优化攻击手法在可靠攻击对齐文本模型方面的不足之处：即使在当前基于自然语言处理的攻击失败时，我们仍然可以通过蛮力方法找到对抗性输入。因此，当前攻击的失败不应被视为对齐文本模型在面对对抗性输入时仍然保持对齐的证明。但是近期大规模机器学习模型的趋势是多模态的。

    Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.  However the recent trend in large-scale ML models is multim
    
[^34]: 有限内存贪婪拟牛顿方法与非渐进超线性收敛速率

    Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate. (arXiv:2306.15444v1 [math.OC])

    [http://arxiv.org/abs/2306.15444](http://arxiv.org/abs/2306.15444)

    有限内存贪婪拟牛顿方法提出了一种解决标准拟牛顿方法计算成本和内存需求过高问题的方法，同时还有具有非渐进超线性收敛速率的性能优势。

    

    非渐进收敛分析表明，拟牛顿方法的显式超线性速率为O$((1/\sqrt{t})^t)$。然而，获得这一速率的方法存在一个众所周知的缺点：它们需要存储先前的黑塞近似矩阵，或者存储所有过去的曲率信息以形成当前的黑塞逆近似。有限内存的拟牛顿方法（如著名的L-BFGS）通过利用有限窗口的过去曲率信息来构造黑塞逆近似，从而缓解了这个问题。因此，它们的每次迭代复杂度和存储需求为O$(\tau d)$，其中$\tau \le d$ 是窗口的大小，$d$ 是问题的维数，从而降低了标准拟牛顿方法的O$(d^2)$ 计算成本和内存需求。然而，据我们所知，没有结果表明有限内存拟牛顿方法存在非渐进超线性收敛速率。

    Non-asymptotic convergence analysis of quasi-Newton methods has gained attention with a landmark result establishing an explicit superlinear rate of O$((1/\sqrt{t})^t)$. The methods that obtain this rate, however, exhibit a well-known drawback: they require the storage of the previous Hessian approximation matrix or instead storing all past curvature information to form the current Hessian inverse approximation. Limited-memory variants of quasi-Newton methods such as the celebrated L-BFGS alleviate this issue by leveraging a limited window of past curvature information to construct the Hessian inverse approximation. As a result, their per iteration complexity and storage requirement is O$(\tau d)$ where $\tau \le d$ is the size of the window and $d$ is the problem dimension reducing the O$(d^2)$ computational cost and memory requirement of standard quasi-Newton methods. However, to the best of our knowledge, there is no result showing a non-asymptotic superlinear convergence rate for a
    
[^35]: 利用嵌入式传感器数据在设备上进行用户社交环境和熟悉地点建模

    On-device modeling of user's social context and familiar places from smartphone-embedded sensor data. (arXiv:2306.15437v1 [cs.LG])

    [http://arxiv.org/abs/2306.15437](http://arxiv.org/abs/2306.15437)

    本研究提出了一种利用嵌入式传感器数据在设备上对用户的社交环境和熟悉地点进行建模的策略，解决了中心化架构处理上下文信息存在隐私泄露和缺乏个性化的问题。

    

    上下文建模和识别对于自适应移动和普适计算至关重要。移动环境中的上下文感知依赖于对上下文变化的及时反应。然而，当前的解决方案主要关注在集中式架构上处理有限的上下文信息，存在隐私泄露和缺乏个性化的风险。设备上的上下文建模和识别是一个新兴的研究趋势，可以解决这些问题。社交互动和访问位置在描述日常生活场景中起着重要作用。本文提出了一种无监督、轻量级的方法，直接在移动设备上建模用户的社交环境和位置。利用ego-network模型，系统从手机嵌入式传感器数据中提取高级、语义丰富的上下文特征。对于社交环境，该方法利用用户和设备之间的物理和网络社交互动数据。在位置方面，它优先考虑建模熟悉的地点。

    Context modeling and recognition are crucial for adaptive mobile and ubiquitous computing. Context-awareness in mobile environments relies on prompt reactions to context changes. However, current solutions focus on limited context information processed on centralized architectures, risking privacy leakage and lacking personalization. On-device context modeling and recognition are emerging research trends, addressing these concerns. Social interactions and visited locations play significant roles in characterizing daily life scenarios. This paper proposes an unsupervised and lightweight approach to model the user's social context and locations directly on the mobile device. Leveraging the ego-network model, the system extracts high-level, semantic-rich context features from smartphone-embedded sensor data. For the social context, the approach utilizes data on physical and cyber social interactions among users and their devices. Regarding location, it prioritizes modeling the familiarity
    
[^36]: 针对图神经网络的对抗训练

    Adversarial Training for Graph Neural Networks. (arXiv:2306.15427v1 [cs.LG])

    [http://arxiv.org/abs/2306.15427](http://arxiv.org/abs/2306.15427)

    该论文通过克服图学习设置的限制，引入可学习图扩散的灵活GNNs以及针对结构扰动的攻击方法，证明了对抗训练是针对对抗结构扰动的最先进防御方法。

    

    尽管在图像领域取得了成功，但对抗训练在图神经网络（GNNs）对抗图结构扰动方面并没有明显效果。在修复对抗训练的过程中，我们发现并克服了之前工作中采用的图学习设置的基本理论和实际限制；我们揭示了基于可学习图扩散的更灵活 GNNs 能够适应对抗扰动，同时学习到的消息传递方案具有自然的可解释性；我们还引入了第一种针对结构扰动的攻击方法，能够同时对多个节点进行攻击，并能处理全局（图级别）和局部（节点级别）的约束。通过这些贡献，我们证明对抗训练是对抗结构扰动的最先进防御方法。

    Despite its success in the image domain, adversarial training does not (yet) stand out as an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations. In the pursuit of fixing adversarial training (1) we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) we reveal that more flexible GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable; (3) we introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) constraints. Including these contributions, we demonstrate that adversarial training is a state-of-the-art defense against adversarial structure perturbations.
    
[^37]: 从拓扑角度验证神经网络的安全性

    Verifying Safety of Neural Networks from Topological Perspectives. (arXiv:2306.15403v1 [cs.LG])

    [http://arxiv.org/abs/2306.15403](http://arxiv.org/abs/2306.15403)

    本研究提出了一种从拓扑角度研究神经网络安全性的方法，利用神经网络的同胚性质和开映射性质建立了输入集和输出集之间的严格保证，从而解决了神经网络在安全验证中的不确定性问题。

    

    神经网络越来越多地应用于自动驾驶等安全关键系统中，然而它们易受损并且常常表现不良。因此，在实际部署之前，它们的行为应该经过严格的保证。在本文中，我们提出了一种从拓扑角度研究神经网络安全验证问题的集边界可达性方法。给定一个具有输入集和安全集的神经网络，安全验证问题是确定所有来自输入集的神经网络输出是否落在安全集中。在我们的方法中，主要利用了神经网络的同胚性质和开映射性质，这些性质在输入集的边界和输出集的边界之间建立了严格的保证。利用这两个性质可以通过提取输入集的子集而不是整个输入集来进行可达性计算，从而控制可达性分析中的包裹效应。

    Neural networks (NNs) are increasingly applied in safety-critical systems such as autonomous vehicles. However, they are fragile and are often ill-behaved. Consequently, their behaviors should undergo rigorous guarantees before deployment in practice. In this paper, we propose a set-boundary reachability method to investigate the safety verification problem of NNs from a topological perspective. Given an NN with an input set and a safe set, the safety verification problem is to determine whether all outputs of the NN resulting from the input set fall within the safe set. In our method, the homeomorphism property and the open map property of NNs are mainly exploited, which establish rigorous guarantees between the boundaries of the input set and the boundaries of the output set. The exploitation of these two properties facilitates reachability computations via extracting subsets of the input set rather than the entire input set, thus controlling the wrapping effect in reachability analy
    
[^38]: 算术Transformer中的长度推广

    Length Generalization in Arithmetic Transformers. (arXiv:2306.15400v1 [cs.LG])

    [http://arxiv.org/abs/2306.15400](http://arxiv.org/abs/2306.15400)

    本研究通过引入相对位置嵌入和训练集引导方法，提高了Transformer模型在学习整数算术和推广到更长序列上的性能，在简单任务中表现良好；同时展示了在乘法任务上引导方法的有效性，并探讨了引导方法在其他领域的潜在应用。

    

    我们研究了Transformer在两个挑战中的表现：学习基本整数运算和推广到训练中未见过的更长序列。我们发现，相对位置嵌入可以使简单任务（如加法）的长度推广：在训练中使用5位数的模型可以执行15位数的求和。然而，这种方法在乘法上失效，我们提出了训练集引导：将几个（10到50个）长序列添加到训练集中。我们展示了引导可以使训练在5位数乘以3位数的模型推广到35乘以3的例子。我们还展示了模型可以针对不同的推广长度进行引导，而引导样本量的缩放与训练集大小的对数成比例。最后，我们讨论了引导在算术之外的潜在应用。

    We examine how transformers cope with two challenges: learning basic integer arithmetic, and generalizing to longer sequences than seen during training. We find that relative position embeddings enable length generalization for simple tasks, such as addition: models trained on $5$-digit numbers can perform $15$-digit sums. However, this method fails for multiplication, and we propose train set priming: adding a few ($10$ to $50$) long sequences to the training set. We show that priming allows models trained on $5$-digit $\times$ $3$-digit multiplications to generalize to $35\times 3$ examples. We also show that models can be primed for different generalization lengths, and that the priming sample size scales as the logarithm of the training set size. Finally, we discuss potential applications of priming beyond arithmetic.
    
[^39]: 通过决策树特征在自动编码器处理空间中评估数据集质量

    Assessing Dataset Quality Through Decision Tree Characteristics in Autoencoder-Processed Spaces. (arXiv:2306.15392v1 [cs.LG])

    [http://arxiv.org/abs/2306.15392](http://arxiv.org/abs/2306.15392)

    本文研究了机器学习分类任务中数据集质量评估的关键方面，通过多个数据集的实验，揭示了数据集质量对模型训练和性能的深远影响，并提出了一个全面的数据集质量评估框架。

    

    本文深入探讨机器学习分类任务中数据集质量评估的关键方面。借助九个不同的数据集，每个数据集都经过分类任务需求的精心设计，我们展示了数据集质量对模型训练和性能的深远影响。我们还引入了两个额外的数据集，分别用于表示具体的数据条件 - 一个最大化熵，另一个展示高冗余性。我们的研究结果强调了适当的特征选择、充足的数据量和数据质量在实现高性能机器学习模型中的重要性。为了帮助研究人员和实践者，我们提出了一个全面的数据集质量评估框架，可以帮助评估手头的数据集是否足够且具备特定任务所需的质量。这项研究为数据评估实践提供了宝贵的见解，并有助于开发更准确和强大的模型。

    In this paper, we delve into the critical aspect of dataset quality assessment in machine learning classification tasks. Leveraging a variety of nine distinct datasets, each crafted for classification tasks with varying complexity levels, we illustrate the profound impact of dataset quality on model training and performance. We further introduce two additional datasets designed to represent specific data conditions - one maximizing entropy and the other demonstrating high redundancy. Our findings underscore the importance of appropriate feature selection, adequate data volume, and data quality in achieving high-performing machine learning models. To aid researchers and practitioners, we propose a comprehensive framework for dataset quality assessment, which can help evaluate if the dataset at hand is sufficient and of the required quality for specific tasks. This research offers valuable insights into data assessment practices, contributing to the development of more accurate and robus
    
[^40]: 多角度信息融合Res2Net与随机Specmix用于假冒语音检测

    Multi-perspective Information Fusion Res2Net with RandomSpecmix for Fake Speech Detection. (arXiv:2306.15389v1 [cs.SD])

    [http://arxiv.org/abs/2306.15389](http://arxiv.org/abs/2306.15389)

    本文提出了多角度信息融合Res2Net与随机Specmix的系统用于假冒语音检测，在低质量场景下提高了模型学习准确伪造信息的能力，并通过数据增强和减少冗余的干扰信息来提高模型的性能。

    

    本文提出了多角度信息融合Res2Net与随机Specmix用于假冒语音检测。该系统的主要目的是提高模型在低质量场景下学习准确的伪造信息以完成假冒语音检测任务的能力。随机Specmix是一种数据增强方法，其目的是改善模型的泛化能力并增强模型定位辨别信息的能力。Specmix在同一批次的样本中剪切和粘贴频率维度的声谱图信息，不引入其他数据，有助于模型定位真正有用的信息。同时，我们随机选择样本进行增强，以减少数据增强直接改变所有数据的影响。一旦达到帮助模型定位信息的目的，减少不必要的信息也变得重要。MPIF-Res2Net的作用是减少冗余的干扰信息。

    In this paper, we propose the multi-perspective information fusion (MPIF) Res2Net with random Specmix for fake speech detection (FSD). The main purpose of this system is to improve the model's ability to learn precise forgery information for FSD task in low-quality scenarios. The task of random Specmix, a data augmentation, is to improve the generalization ability of the model and enhance the model's ability to locate discriminative information. Specmix cuts and pastes the frequency dimension information of the spectrogram in the same batch of samples without introducing other data, which helps the model to locate the really useful information. At the same time, we randomly select samples for augmentation to reduce the impact of data augmentation directly changing all the data. Once the purpose of helping the model to locate information is achieved, it is also important to reduce unnecessary information. The role of MPIF-Res2Net is to reduce redundant interference information. Deceptiv
    
[^41]: LeCo：通过学习序列相关性实现轻量级压缩

    LeCo: Lightweight Compression via Learning Serial Correlations. (arXiv:2306.15374v1 [cs.DB])

    [http://arxiv.org/abs/2306.15374](http://arxiv.org/abs/2306.15374)

    使用机器学习来自动消除序列冗余以实现出色的压缩比和解压缩性能的LeCo是一种轻量级压缩框架，通过学习序列相关性，它能够在压缩比和随机访问速度上实现帕累托改进。

    

    轻量级数据压缩是一种关键技术，它使得列式存储在分析查询方面展示出卓越的性能。尽管之前有关基于字典编码来逼近Shannon熵的研究已经很全面，但鲜有之前的工作系统地利用列的序列相关性来进行压缩。在本文中，我们提出了LeCo（即学习压缩），这是一种使用机器学习自动消除序列冗余以实现出色的压缩比和解压缩性能的框架。LeCo提供了一种通用的方法来实现这一目标，在我们的框架下，现有的（临时的）算法，如参考帧（Frame-of-Reference），Delta编码和游程编码（Run-Length Encoding）都是特例。我们对三个合成数据集和六个真实数据集进行的微基准测试显示，LeCo原型在压缩比和随机访问速度上相比现有解决方案取得了帕累托改进。当将LeCo集成时

    Lightweight data compression is a key technique that allows column stores to exhibit superior performance for analytical queries. Despite a comprehensive study on dictionary-based encodings to approach Shannon's entropy, few prior works have systematically exploited the serial correlation in a column for compression. In this paper, we propose LeCo (i.e., Learned Compression), a framework that uses machine learning to remove the serial redundancy in a value sequence automatically to achieve an outstanding compression ratio and decompression performance simultaneously. LeCo presents a general approach to this end, making existing (ad-hoc) algorithms such as Frame-of-Reference (FOR), Delta Encoding, and Run-Length Encoding (RLE) special cases under our framework. Our microbenchmark with three synthetic and six real-world data sets shows that a prototype of LeCo achieves a Pareto improvement on both compression ratio and random access speed over the existing solutions. When integrating LeC
    
[^42]: 基于元分析的朴素贝叶斯和随机森林在软件缺陷预测中的比较

    A Meta-analytical Comparison of Naive Bayes and Random Forest for Software Defect Prediction. (arXiv:2306.15369v1 [cs.SE])

    [http://arxiv.org/abs/2306.15369](http://arxiv.org/abs/2306.15369)

    通过系统性文献回顾和元分析，本研究比较了朴素贝叶斯和随机森林模型在软件缺陷预测中的性能。结果表明，没有统计证据表明朴素贝叶斯在召回率、F-度量和精确度方面与随机森林有显著的差异。

    

    在软件缺陷预测中，朴素贝叶斯和随机森林在召回率、F-度量和精确度方面是否存在统计差异？通过利用系统性文献回顾和元分析，我们正在回答这个问题。我们通过建立搜索和选择论文的标准进行了系统性文献回顾，共包括了五个研究。之后，我们使用了五个选择的论文的元数据和森林图进行了元分析来比较两个模型。结果显示，没有显著的统计证据表明朴素贝叶斯在召回率、F-度量和精确度方面与随机森林表现不同。

    Is there a statistical difference between Naive Bayes and Random Forest in terms of recall, f-measure, and precision for predicting software defects? By utilizing systematic literature review and meta-analysis, we are answering this question. We conducted a systematic literature review by establishing criteria to search and choose papers, resulting in five studies. After that, using the meta-data and forest-plots of five chosen papers, we conducted a meta-analysis to compare the two models. The results have shown that there is no significant statistical evidence that Naive Bayes perform differently from Random Forest in terms of recall, f-measure, and precision.
    
[^43]: 深度度量学习中的均场理论

    Mean Field Theory in Deep Metric Learning. (arXiv:2306.15368v1 [cs.LG])

    [http://arxiv.org/abs/2306.15368](http://arxiv.org/abs/2306.15368)

    本文将均场理论应用于深度度量学习，提出了一种基于对比度的损失函数设计分类损失函数的方法，通过降低训练复杂度，在图像检索任务中取得了比基线方法更好的效果。

    

    本文探讨了在深度度量学习中应用统计物理中的均场理论，并解决了传统度量学习损失函数所普遍存在的高训练复杂度问题。通过将均场理论应用于深度度量学习，我们提出了一种从基于对比度的损失函数设计分类损失函数的方法，这可以被认为是对基于代理的方法的补充。将均场理论应用于两种基于对比度的损失函数，我们推导出了两个新的损失函数，MeanFieldContrastive和MeanFieldClassWiseMultiSimilarity损失函数，其训练复杂度得到了降低。我们在三个图像检索数据集上对这些衍生损失函数进行了广泛评估，并证明我们的损失函数在三个数据集中的两个上胜过了基线方法。

    In this paper, we explore the application of mean field theory, a technique from statistical physics, to deep metric learning and address the high training complexity commonly associated with conventional metric learning loss functions. By adapting mean field theory for deep metric learning, we develop an approach to design classification-based loss functions from pair-based ones, which can be considered complementary to the proxy-based approach. Applying the mean field theory to two pair-based loss functions, we derive two new loss functions, MeanFieldContrastive and MeanFieldClassWiseMultiSimilarity losses, with reduced training complexity. We extensively evaluate these derived loss functions on three image-retrieval datasets and demonstrate that our loss functions outperform baseline methods in two out of the three datasets.
    
[^44]: CellViT:用于精确细胞分割和分类的视觉Transformer

    CellViT: Vision Transformers for Precise Cell Segmentation and Classification. (arXiv:2306.15350v1 [eess.IV])

    [http://arxiv.org/abs/2306.15350](http://arxiv.org/abs/2306.15350)

    CellViT是一种基于Vision Transformer的深度学习架构，用于自动实例分割苏木精和伊红染色的组织样本中的细胞核。通过在PanNuke数据集上训练和评估，CellViT展示了其优越性。

    

    细胞核在苏木精和伊红染色的组织图像中的检测和分割是重要的临床任务，并且对各种应用至关重要。然而，由于细胞核染色和大小的差异、边界重叠和核聚集，这是一项具有挑战性的任务。虽然卷积神经网络在这个任务中已经被广泛使用，但我们探索了Transformer-based网络在这个领域的潜力。因此，我们引入了一种使用基于Vision Transformer的深度学习架构CellViT对数字化组织样本中的细胞核进行自动实例分割的新方法。CellViT在PanNuke数据集上进行训练和评估，该数据集是一个最具挑战性的细胞核实例分割数据集，包含了近20万个注释的细胞核，分为19种组织类型的5个临床重要类别。我们通过利用最近提出的大规模领域内和领域外预训练的Vision Transformer，展示了其优越性。

    Nuclei detection and segmentation in hematoxylin and eosin-stained (H&E) tissue images are important clinical tasks and crucial for a wide range of applications. However, it is a challenging task due to nuclei variances in staining and size, overlapping boundaries, and nuclei clustering. While convolutional neural networks have been extensively used for this task, we explore the potential of Transformer-based networks in this domain. Therefore, we introduce a new method for automated instance segmentation of cell nuclei in digitized tissue samples using a deep learning architecture based on Vision Transformer called CellViT. CellViT is trained and evaluated on the PanNuke dataset, which is one of the most challenging nuclei instance segmentation datasets, consisting of nearly 200,000 annotated Nuclei into 5 clinically important classes in 19 tissue types. We demonstrate the superiority of large-scale in-domain and out-of-domain pre-trained Vision Transformers by leveraging the recently
    
[^45]: FedET: 一种基于增强Transformer的通信高效的联邦类增量学习框架

    FedET: A Communication-Efficient Federated Class-Incremental Learning Framework Based on Enhanced Transformer. (arXiv:2306.15347v1 [cs.LG])

    [http://arxiv.org/abs/2306.15347](http://arxiv.org/abs/2306.15347)

    FedET是一种通信高效的联邦类增量学习框架，利用增强Transformer和增强器来解决联邦学习中的灾难性遗忘和通信成本问题，保证了高精度和数据隐私。

    

    联邦学习(Federated Learning, FL)由于能够在保证数据隐私的前提下进行分散学习而受到广泛关注。然而，大多数现有方法都不切实际地假设本地客户端遇到的类别随时间固定。在学习新类别后，这个假设会导致模型对旧类别的灾难性遗忘变得更加严重。此外，受通信成本的限制，在FL中使用大规模模型是具有挑战性的，这将影响预测精度。为了解决这些挑战，我们提出了一种新颖的框架FedET，它同时实现了高精度和低通信成本。具体而言，FedET使用增强器(Enhancer)这个小型模块来吸收和传递新的知识，并将预训练的Transformer与不同的增强器结合使用，以在各种任务上保证高精度。为了解决新任务的新类别引起的本地遗忘问题和非i.i.d（非独立同分布）所带来的全局遗忘问题，FedET使用了动态扩展方法和重放机制。

    Federated Learning (FL) has been widely concerned for it enables decentralized learning while ensuring data privacy. However, most existing methods unrealistically assume that the classes encountered by local clients are fixed over time. After learning new classes, this assumption will make the model's catastrophic forgetting of old classes significantly severe. Moreover, due to the limitation of communication cost, it is challenging to use large-scale models in FL, which will affect the prediction accuracy. To address these challenges, we propose a novel framework, Federated Enhanced Transformer (FedET), which simultaneously achieves high accuracy and low communication cost. Specifically, FedET uses Enhancer, a tiny module, to absorb and communicate new knowledge, and applies pre-trained Transformers combined with different Enhancers to ensure high precision on various tasks. To address local forgetting caused by new classes of new tasks and global forgetting brought by non-i.i.d (non
    
[^46]: 使用numpy的快速区间算术工具箱及其在神经网络控制系统的形式验证中的应用

    A Toolbox for Fast Interval Arithmetic in numpy with an Application to Formal Verification of Neural Network Controlled Systems. (arXiv:2306.15340v1 [eess.SY])

    [http://arxiv.org/abs/2306.15340](http://arxiv.org/abs/2306.15340)

    本文介绍了一个使用numpy的区间分析工具箱，通过构建自然包含函数来计算广泛映射类的区间界限，并将其应用于神经网络控制系统的形式验证。

    

    本文介绍了一个在numpy中进行区间分析的工具箱，并应用于神经网络控制系统的形式验证。通过使用自然包含函数的概念，我们系统地构建了一个广泛的映射类的区间界限。该工具箱提供了使用编译的C代码高效计算自然包含函数的方法，以及在numpy中提供了熟悉的接口和其经典功能，例如n维数组、矩阵/向量操作和向量化。

    In this paper, we present a toolbox for interval analysis in numpy, with an application to formal verification of neural network controlled systems. Using the notion of natural inclusion functions, we systematically construct interval bounds for a general class of mappings. The toolbox offers efficient computation of natural inclusion functions using compiled C code, as well as a familiar interface in numpy with its canonical features, such as n-dimensional arrays, matrix/vector operations, and vectorization. We then use this toolbox in formal verification of dynamical systems with neural network controllers, through the composition of their inclusion functions.
    
[^47]: 同调神经网络：多元复杂性的稀疏架构

    Homological Neural Networks: A Sparse Architecture for Multivariate Complexity. (arXiv:2306.15337v1 [cs.LG])

    [http://arxiv.org/abs/2306.15337](http://arxiv.org/abs/2306.15337)

    本研究提出了一种新颖的深度神经网络架构，同调神经网络，通过应用网络过滤技术构建稀疏的高阶图结构，解决了深度学习中的计算复杂性和能源效率挑战，并在表格数据和时间序列回归问题上展示了优越的性能。

    

    随着人工智能研究的快速发展，越来越复杂的深度学习模型的开发带来了计算复杂性、能源效率和可解释性方面日益增长的挑战。在本研究中，我们应用先进的基于网络的信息过滤技术，设计了一种新颖的深度神经网络单元，该单元以基础数据的同调结构为基础构建了一个稀疏的高阶图结构。我们在两个传统上对深度学习具有挑战性的应用领域中展示了其有效性：表格数据和时间序列回归问题。结果表明，这种新颖设计的优势在于能够仅使用部分参数与最先进的机器学习和深度学习模型并列或超越其结果。

    The rapid progress of Artificial Intelligence research came with the development of increasingly complex deep learning models, leading to growing challenges in terms of computational complexity, energy efficiency and interpretability. In this study, we apply advanced network-based information filtering techniques to design a novel deep neural network unit characterized by a sparse higher-order graphical architecture built over the homological structure of underlying data. We demonstrate its effectiveness in two application domains which are traditionally challenging for deep learning: tabular data and time series regression problems. Results demonstrate the advantages of this novel design which can tie or overcome the results of state-of-the-art machine learning and deep learning models using only a fraction of parameters.
    
[^48]: 模拟反事实情况

    Simulating counterfactuals. (arXiv:2306.15328v1 [stat.ML])

    [http://arxiv.org/abs/2306.15328](http://arxiv.org/abs/2306.15328)

    该论文提出了一种算法，可以模拟反事实分布中的值，可对离散和连续变量设定条件，并应用于信用评分中的公平性分析。

    

    反事实推断考虑了在与实际世界存在一些证据的平行世界中进行的假设性干预。如果证据在流形上指定了条件分布，反事实可能是解析难解的。我们提出了一种算法，用于从反事实分布中模拟值，其中可以对离散和连续变量设定条件。我们表明，所提出的算法可以被呈现为粒子滤波器，从而导致渐近有效的推断。该算法被应用于信用评分中的公平性分析。

    Counterfactual inference considers a hypothetical intervention in a parallel world that shares some evidence with the factual world. If the evidence specifies a conditional distribution on a manifold, counterfactuals may be analytically intractable. We present an algorithm for simulating values from a counterfactual distribution where conditions can be set on both discrete and continuous variables. We show that the proposed algorithm can be presented as a particle filter leading to asymptotically valid inference. The algorithm is applied to fairness analysis in credit scoring.
    
[^49]: 通过基于得分的生成模型进行网络异常检测

    Anomaly Detection in Networks via Score-Based Generative Models. (arXiv:2306.15324v1 [cs.LG])

    [http://arxiv.org/abs/2306.15324](http://arxiv.org/abs/2306.15324)

    该论文提出了一种利用基于得分的生成模型进行网络异常检测的方法，该方法在小规模图上取得了竞争性的结果。

    

    在带属性的图中对节点异常进行检测是一个具有挑战性的问题，目前还没有一种方法能够在不同的数据集上表现良好。受图生成建模中基于得分模型的最新结果的启发，我们提出将其应用于上述问题中。我们的方法在小规模图上取得了竞争性的结果。我们对狄利克雷能量进行了实证分析，并表明生成模型可能难以准确重构它。

    Node outlier detection in attributed graphs is a challenging problem for which there is no method that would work well across different datasets. Motivated by the state-of-the-art results of score-based models in graph generative modeling, we propose to incorporate them into the aforementioned problem. Our method achieves competitive results on small-scale graphs. We provide an empirical analysis of the Dirichlet energy, and show that generative models might struggle to accurately reconstruct it.
    
[^50]: FAIRER: 公平作为决策合理性对齐原则

    FAIRER: Fairness as Decision Rationale Alignment. (arXiv:2306.15299v1 [cs.LG])

    [http://arxiv.org/abs/2306.15299](http://arxiv.org/abs/2306.15299)

    本文针对深度神经网络中的公平性问题，从决策合理性的角度研究，并定义了参数平等得分来表征公平决策过程。实证研究表明现有的公平性规范项无法实现决策合理性的对齐。

    

    深度神经网络(DNNs)取得了显著的进展，但往往存在公平性问题，因为深度模型通常在某些子群体（例如男性和女性）之间显示出明显的准确性差异。现有研究通过使用公平感知损失函数来约束最后一层的输出并直接规范化DNNs来解决这个关键问题。虽然DNN的公平性得到了改善，但不清楚经过训练的网络如何进行公平预测，这限制了未来的公平性改进。在本文中，我们从决策合理性的角度研究了公平性，并通过分析各个子群体中的神经元影响来定义参数平等得分来表征网络的公平决策过程。广泛的实证研究表明，不公平问题可能源于子群体的不对齐决策合理性。现有的公平性规范项无法实现决策合理性的对齐，因为它们只约束最后一层的输出，而忽视了之前的层次。

    Deep neural networks (DNNs) have made significant progress, but often suffer from fairness issues, as deep models typically show distinct accuracy differences among certain subgroups (e.g., males and females). Existing research addresses this critical issue by employing fairness-aware loss functions to constrain the last-layer outputs and directly regularize DNNs. Although the fairness of DNNs is improved, it is unclear how the trained network makes a fair prediction, which limits future fairness improvements. In this paper, we investigate fairness from the perspective of decision rationale and define the parameter parity score to characterize the fair decision process of networks by analyzing neuron influence in various subgroups. Extensive empirical studies show that the unfair issue could arise from the unaligned decision rationales of subgroups. Existing fairness regularization terms fail to achieve decision rationale alignment because they only constrain last-layer outputs while i
    
[^51]: BERT中的性别偏见——通过情感评分在现实的下游分类任务中测量和分析偏见

    Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task. (arXiv:2306.15298v1 [cs.CL])

    [http://arxiv.org/abs/2306.15298](http://arxiv.org/abs/2306.15298)

    该论文通过引入新的偏见度量方法，并在一个现实的IMDB电影分类器的例子中对BERT的性别偏见进行了全面分析。研究结果表明，公开的BERT模型中存在着显著的性别偏见，并强调了负责任使用的重要性。

    

    预训练语言模型在各种现实应用中公开可用，并不断进行微调。随着它们具备抓取复杂上下文信息的能力，有害偏见很可能越来越多地与这些模型交织在一起。本文通过两个主要贡献来分析BERT模型中的性别偏见：首先，引入了一种新的偏见度量方法，将偏见定义为女性和男性样本版本在情感评估上的差异。其次，我们对一个现实的IMDB电影分类器的例子中BERT的偏见进行了全面分析。通过系统地变化训练流程的各个元素，我们可以对最终模型偏见的影响做出结论。我们比较了七个不同的公开BERT模型的九种训练条件，即总共63个模型。几乎所有的条件都产生了显著的性别偏见。结果表明，反映的偏见源于公开的BERT模型而不是任务特定数据，强调了负责任使用的重要性。

    Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT's biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.
    
[^52]: 自适应退火重要性采样与恒定速率进展

    Adaptive Annealed Importance Sampling with Constant Rate Progress. (arXiv:2306.15283v1 [stat.ML])

    [http://arxiv.org/abs/2306.15283](http://arxiv.org/abs/2306.15283)

    本文提出了一种自适应退火重要性采样算法，通过推导出恒定速率离散化进展计划，实现了在退火过程中样本在各个分布之间的自适应移动，从而提升了采样效率。

    

    退火重要性采样(AIS)能够在给定难以计算的概率分布的非规范化密度函数的情况下合成加权样本。该算法依赖于一系列插值分布，将目标分布与初始易于计算的分布进行衔接，其中著名的几何平均路径被认为通常是次优的。本文证明了几何退火是当前粒子分布与目标分布之间KL散度最小的路径。我们基于这一观察，推导出了这一退火序列的恒定速率离散化进展计划，根据样本在初始和目标分布之间移动的难度调整进展计划。我们进一步将结果推广至f-散度，并提出了相应的退火序列动力学。

    Annealed Importance Sampling (AIS) synthesizes weighted samples from an intractable distribution given its unnormalized density function. This algorithm relies on a sequence of interpolating distributions bridging the target to an initial tractable distribution such as the well-known geometric mean path of unnormalized distributions which is assumed to be suboptimal in general. In this paper, we prove that the geometric annealing corresponds to the distribution path that minimizes the KL divergence between the current particle distribution and the desired target when the feasible change in the particle distribution is constrained. Following this observation, we derive the constant rate discretization schedule for this annealing sequence, which adjusts the schedule to the difficulty of moving samples between the initial and the target distributions. We further extend our results to $f$-divergences and present the respective dynamics of annealing sequences based on which we propose the C
    
[^53]: 提供夸大的解释

    Delivering Inflated Explanations. (arXiv:2306.15272v1 [cs.AI])

    [http://arxiv.org/abs/2306.15272](http://arxiv.org/abs/2306.15272)

    本文提出了夸大的解释，它是为了解释人工智能系统的决策原因所定义的一种方法。传统的解释方法只显示选择的特征对决策的影响，而夸大的解释考虑了更多特征的可能性。

    

    在追求可解释的人工智能（XAI）的过程中，经常出现一个问题，即 AI 系统做出决策的原因是什么。解释性的正式方法建立了 AI 系统的形式模型，并利用该模型推理系统的特性。给定一个要解释的实例的特征值集和相应的决策，一个形式推理解释是一组特征，如果它们采取给定值，将始终导致同样的决策。这种解释是有用的，它显示只有一些特征被用于做出最终决策。但它是狭义的，它只显示如果选择的特征采取它们给定的值，决策就不会改变。可能有些特征的值会改变，但仍然导致相同的决策。在本文中，我们正式定义了夸大的解释，它是一组特征，对于每个特征有一组值（始终包括该特征的值）。

    In the quest for Explainable Artificial Intelligence (XAI) one of the questions that frequently arises given a decision made by an AI system is, ``why was the decision made in this way?'' Formal approaches to explainability build a formal model of the AI system and use this to reason about the properties of the system. Given a set of feature values for an instance to be explained, and a resulting decision, a formal abductive explanation is a set of features, such that if they take the given value will always lead to the same decision. This explanation is useful, it shows that only some features were used in making the final decision. But it is narrow, it only shows that if the selected features take their given values the decision is unchanged. It's possible that some features may change values and still lead to the same decision. In this paper we formally define inflated explanations which is a set of features, and for each feature of set of values (always including the value of the i
    
[^54]: 适应老年和发音障碍者语音识别的Conformer ASR系统的超参数调整

    Hyper-parameter Adaptation of Conformer ASR Systems for Elderly and Dysarthric Speech Recognition. (arXiv:2306.15265v1 [eess.AS])

    [http://arxiv.org/abs/2306.15265](http://arxiv.org/abs/2306.15265)

    本文研究了在Librispeech语料库上预训练的Conformer ASR系统的超参数适应性，并将其应用于DementiaBank老年人和UASpeech发音障碍者语音数据集。实验结果表明，超参数适应可以显著降低词错误率，并发现了超参数适应性能改进与源域和目标域数据的话语长度比之间的相关性。

    

    自动识别混乱和老年人的语音仍然是一项具有挑战性的任务，因为数据稀缺。通常使用参数微调来利用大量的非老年和健康语音预训练模型，而神经网络架构的超参数则使用专家知识设置并保持不变。本文研究了在Librispeech语料库上预训练的Conformer ASR系统的超参数适应性，然后将其领域适应到DementiaBank老年人和UASpeech发音障碍者语音数据集。实验结果表明，与仅参数微调相比，超参数适应在DBank和UASpeech任务中分别减少了0.45%和0.67%的词错误率（WER）。在超参数领域适应的性能改进和源域数据与目标域数据的相对话语长度比之间存在直观的相关性。

    Automatic recognition of disordered and elderly speech remains highly challenging tasks to date due to data scarcity. Parameter fine-tuning is often used to exploit the large quantities of non-aged and healthy speech pre-trained models, while neural architecture hyper-parameters are set using expert knowledge and remain unchanged. This paper investigates hyper-parameter adaptation for Conformer ASR systems that are pre-trained on the Librispeech corpus before being domain adapted to the DementiaBank elderly and UASpeech dysarthric speech datasets. Experimental results suggest that hyper-parameter adaptation produced word error rate (WER) reductions of 0.45% and 0.67% over parameter-only fine-tuning on DBank and UASpeech tasks respectively. An intuitive correlation is found between the performance improvements by hyper-parameter domain adaptation and the relative utterance length ratio between the source and target domain data.
    
[^55]: [Re]双抽样随机平滑方法

    [Re] Double Sampling Randomized Smoothing. (arXiv:2306.15221v1 [cs.LG])

    [http://arxiv.org/abs/2306.15221](http://arxiv.org/abs/2306.15221)

    该论文提出了一种名为双抽样随机平滑的方法，通过使用额外的平滑分布改善了神经网络对抗扰动的鲁棒性认证，实验证明了该方法的有效性。

    

    本文是机器学习领域可重复性挑战的一个贡献，特别针对神经网络(NNs)对抗扰动的鲁棒性认证问题。提出的双抽样随机平滑(DSRS)框架通过使用额外的平滑分布来改善鲁棒性认证，克服了现有方法的局限性。本文清晰地阐述了DSRS在广义高斯平滑族的案例，并提供了一种计算高效的实现方法。在MNIST和CIFAR-10上的实验证明了DSRS的有效性，相比其他方法，始终证明了更大的鲁棒半径。此外，还进行了各种消融研究，以进一步分析超参数和对抗训练方法对所提出框架所认证半径的影响。

    This paper is a contribution to the reproducibility challenge in the field of machine learning, specifically addressing the issue of certifying the robustness of neural networks (NNs) against adversarial perturbations. The proposed Double Sampling Randomized Smoothing (DSRS) framework overcomes the limitations of existing methods by using an additional smoothing distribution to improve the robustness certification. The paper provides a clear manifestation of DSRS for a generalized family of Gaussian smoothing and a computationally efficient method for implementation. The experiments on MNIST and CIFAR-10 demonstrate the effectiveness of DSRS, consistently certifying larger robust radii compared to other methods. Also various ablations studies are conducted to further analyze the hyperparameters and effect of adversarial training methods on the certified radius by the proposed framework.
    
[^56]: 无监督的剧集生成方法用于图元学习

    Unsupervised Episode Generation for Graph Meta-learning. (arXiv:2306.15217v1 [cs.LG])

    [http://arxiv.org/abs/2306.15217](http://arxiv.org/abs/2306.15217)

    本文研究了无监督的剧集生成方法，通过元学习解决没有标签的少样本节点分类问题。它们充分利用所有节点信息，并且通过泛化能力提高性能。

    

    本文研究了无监督的剧集生成方法，通过元学习来解决没有标签的少样本节点分类问题。主流的少样本节点分类的元学习方法是在存在大量有标签节点用于训练的情况下开发的，然而在现实世界中可能无法获得这样的数据。虽然已经提出了一些解决标签稀缺性问题的研究，但它们仍然依赖于有限数量的有标签数据，这限制了对图中所有节点信息的充分利用。尽管自监督学习方法在没有标签的节点分类问题上很有效，但它们主要学习通用的节点嵌入，没有考虑要解决的下游任务，这可能限制了其性能。在这项工作中，我们提出了无监督的剧集生成方法，以利用它们在少样本节点分类任务中的泛化能力，同时解决标签稀缺性问题。我们首先提出了一种利用图增强方法的方法

    In this paper, we investigate Unsupervised Episode Generation methods to solve Few-Shot Node-Classification (FSNC) problem via Meta-learning without labels. Dominant meta-learning methodologies for FSNC were developed under the existence of abundant labeled nodes for training, which however may not be possible to obtain in the real-world. Although few studies have been proposed to tackle the label-scarcity problem, they still rely on a limited amount of labeled data, which hinders the full utilization of the information of all nodes in a graph. Despite the effectiveness of Self-Supervised Learning (SSL) approaches on FSNC without labels, they mainly learn generic node embeddings without consideration on the downstream task to be solved, which may limit its performance. In this work, we propose unsupervised episode generation methods to benefit from their generalization ability for FSNC tasks while resolving label-scarcity problem. We first propose a method that utilizes graph augmentat
    
[^57]: TranssionADD:一种基于多帧强化学习的序列标注模型用于音频深度伪造检测

    TranssionADD: A multi-frame reinforcement based sequence tagging model for audio deepfake detection. (arXiv:2306.15212v1 [cs.SD])

    [http://arxiv.org/abs/2306.15212](http://arxiv.org/abs/2306.15212)

    TranssionADD是一种基于多帧强化学习的序列标注模型，用于音频深度伪造检测，通过适应序列标注任务、改进模型泛化能力和引入多帧检测，提供了解决模型鲁棒性和音频片段异常值的方法。

    

    由于最近端到端语音建模技术的进步，模仿和克隆用户的声音变得越来越可行。这导致区分真实和伪造音频片段面临着重大挑战。为了解决用户声音滥用和误用的问题，第二届音频深度伪造检测挑战（ADD 2023）旨在检测和分析伪造的语音表达。具体而言，Track 2，名为操纵区域定位（RL），旨在准确定位音频中被操纵的区域，这些区域可能存在于真实和生成的音频片段中。我们提出了一种名为TranssionADD的新颖系统，作为对模型鲁棒性和音频片段异常值的追踪竞赛中一个具有挑战性的问题的解决方案。我们的系统提供了三个独特的贡献：1）我们将序列标注任务应用于音频深度伪造检测；2）我们通过各种数据增强技术提高模型的泛化能力；3）我们结合多帧检测（MFD）改进了检测性能。

    Thanks to recent advancements in end-to-end speech modeling technology, it has become increasingly feasible to imitate and clone a user`s voice. This leads to a significant challenge in differentiating between authentic and fabricated audio segments. To address the issue of user voice abuse and misuse, the second Audio Deepfake Detection Challenge (ADD 2023) aims to detect and analyze deepfake speech utterances. Specifically, Track 2, named the Manipulation Region Location (RL), aims to pinpoint the location of manipulated regions in audio, which can be present in both real and generated audio segments. We propose our novel TranssionADD system as a solution to the challenging problem of model robustness and audio segment outliers in the trace competition. Our system provides three unique contributions: 1) we adapt sequence tagging task for audio deepfake detection; 2) we improve model generalization by various data augmentation techniques; 3) we incorporate multi-frame detection (MFD) 
    
[^58]: 使用改进的特征选择从静息态原始脑电信号中检测慢性疼痛

    Chronic pain detection from resting-state raw EEG signals using improved feature selection. (arXiv:2306.15194v1 [cs.LG])

    [http://arxiv.org/abs/2306.15194](http://arxiv.org/abs/2306.15194)

    本论文提出了一种改进的特征选择算法，用于从静息态原始脑电信号中检测慢性疼痛，该算法在类别可分性方面表现优秀，并且在测试中取得了高准确率。

    

    我们提出了一种自动化方法，用于从静息态原始脑电数据中检测慢性疼痛。我们提出了一种新的特征选择算法-改进的顺序浮动前向选择（mSFFS）。改进的特征选择方案虽然较为简洁，但显示出更好的类别可分性，这由Bhattacharyya距离度量和更好的可视化结果所示。它也优于其他基准方法生成的选择，将测试准确率提高到97.5％，并在包含不同类型慢性疼痛的外部数据集上产生81.4％的测试准确率。

    We present an automatic approach that works on resting-state raw EEG data for chronic pain detection. A new feature selection algorithm - modified Sequential Floating Forward Selection (mSFFS) - is proposed. The improved feature selection scheme is rather compact but displays better class separability as indicated by the Bhattacharyya distance measures and better visualization results. It also outperforms selections generated by other benchmark methods, boosting the test accuracy to 97.5% and yielding a test accuracy of 81.4% on an external dataset that contains different types of chronic pain
    
[^59]: 一类系统完美适用于前向算法

    One-class systems seamlessly fit in the forward-forward algorithm. (arXiv:2306.15188v1 [cs.LG])

    [http://arxiv.org/abs/2306.15188](http://arxiv.org/abs/2306.15188)

    本文研究了在前向训练方式下训练深度单类目标函数的性能，发现这些函数可以处理动态网络大小，为无缝在线训练带来了许多好处。

    

    前向算法提出了一种新的神经网络训练方法，通过在推理过程中更新权重，逐层进行参数更新。这一方法在训练过程中立即降低了内存需求，可能带来更多好处，比如无缝在线训练。这种方法依赖于一个损失（“好度”）函数，该函数可以在每个层的激活上进行评估，这些层的参数大小可以根据网络的超参数化而变化。在开创性论文中，提出了一个好度函数来满足这一需求；然而，如果将其置于一个单类问题的背景下，就无需开创新的损失函数，因为这些函数本身可以处理动态网络大小。在本文中，我们研究了在前向训练方式下训练深度单类目标函数的性能。代码可以在 \url{https://github.com/MichaelHopwood/ForwardForwardOneclass} 找到。

    The forward-forward algorithm presents a new method of training neural networks by updating weights during an inference, performing parameter updates for each layer individually. This immediately reduces memory requirements during training and may lead to many more benefits, like seamless online training. This method relies on a loss ("goodness") function that can be evaluated on the activations of each layer, of which can have a varied parameter size, depending on the hyperparamaterization of the network. In the seminal paper, a goodness function was proposed to fill this need; however, if placed in a one-class problem context, one need not pioneer a new loss because these functions can innately handle dynamic network sizes. In this paper, we investigate the performance of deep one-class objective functions when trained in a forward-forward fashion. The code is available at \url{https://github.com/MichaelHopwood/ForwardForwardOneclass}.
    
[^60]: 使用强化学习的自动桁架设计

    Automatic Truss Design with Reinforcement Learning. (arXiv:2306.15182v1 [cs.AI])

    [http://arxiv.org/abs/2306.15182](http://arxiv.org/abs/2306.15182)

    本文提出了一个两阶段的AutoTruss框架，利用蒙特卡洛树搜索和强化学习方法，高效生成轻型和合法的桁架布局。在实验中，AutoTruss显示出优越的性能，超越了之前报道的方法。

    

    桁架布局设计是建筑行业中的一个基本问题，即寻找一个满足所有物理约束条件的轻型桁架布局。生成最优布局是一个具有挑战性的组合优化问题，通过穷举搜索求解可能非常昂贵。将端到端强化学习方法直接应用于桁架布局设计也是不可行的，因为在物理约束条件下只有一个很小的布局空间是有效的，导致强化学习训练的奖励非常稀疏。在本文中，我们开发了AutoTruss，一个两阶段的框架，可以高效地生成轻型和合法的桁架布局。AutoTruss首先采用蒙特卡洛树搜索来发现多样化的合法布局，然后利用强化学习来逐步优化有效的解决方案。我们在2D和3D设置下的流行桁架布局设计测试案例中进行了实验和消融研究。AutoTruss在表现上超过了最佳报道的方法。

    Truss layout design, namely finding a lightweight truss layout satisfying all the physical constraints, is a fundamental problem in the building industry. Generating the optimal layout is a challenging combinatorial optimization problem, which can be extremely expensive to solve by exhaustive search. Directly applying end-to-end reinforcement learning (RL) methods to truss layout design is infeasible either, since only a tiny portion of the entire layout space is valid under the physical constraints, leading to particularly sparse rewards for RL training. In this paper, we develop AutoTruss, a two-stage framework to efficiently generate both lightweight and valid truss layouts. AutoTruss first adopts Monte Carlo tree search to discover a diverse collection of valid layouts. Then RL is applied to iteratively refine the valid solutions. We conduct experiments and ablation studies in popular truss layout design test cases in both 2D and 3D settings. AutoTruss outperforms the best-reported
    
[^61]: 利用推理结构在神经过程中进行扩展

    Exploiting Inferential Structure in Neural Processes. (arXiv:2306.15169v1 [cs.LG])

    [http://arxiv.org/abs/2306.15169](http://arxiv.org/abs/2306.15169)

    这项工作提供了一个框架，利用推理结构在神经过程中进行扩展。我们为NPs的潜变量提供了丰富的先验，并提出了适当的上下文集聚合策略。此外，我们还描述了一种消息传递过程，可以进行端到端优化，并通过使用混合和学生-t假设改善了函数建模和测试时间的鲁棒性。

    

    神经过程(NPs)由于其能够基于上下文集执行快速适应而具有吸引力。这个集合由一个潜变量编码，通常假设该变量遵循一个简单的分布。然而，在现实世界的情况下，上下文集可能来自具有多个模式、重尾等丰富分布的抽样。在这项工作中，我们提供了一个框架，允许给予NPs潜变量一个由图模型定义的丰富先验。这些分布假设直接转化为适合上下文集的适当聚合策略。此外，我们描述了一种消息传递过程，仍然可以通过随机梯度进行端到端优化。我们通过使用混合和学生-t假设来证明我们框架的普适性，从而改善了函数建模和测试时间的鲁棒性。

    Neural Processes (NPs) are appealing due to their ability to perform fast adaptation based on a context set. This set is encoded by a latent variable, which is often assumed to follow a simple distribution. However, in real-word settings, the context set may be drawn from richer distributions having multiple modes, heavy tails, etc. In this work, we provide a framework that allows NPs' latent variable to be given a rich prior defined by a graphical model. These distributional assumptions directly translate into an appropriate aggregation strategy for the context set. Moreover, we describe a message-passing procedure that still allows for end-to-end optimization with stochastic gradients. We demonstrate the generality of our framework by using mixture and Student-t assumptions that yield improvements in function modelling and test-time robustness.
    
[^62]: 从无效数据中学习：关于生成模型中的约束满足问题

    Learning from Invalid Data: On Constraint Satisfaction in Generative Models. (arXiv:2306.15166v1 [cs.LG])

    [http://arxiv.org/abs/2306.15166](http://arxiv.org/abs/2306.15166)

    本论文提出了一种新的训练机制，利用包含无效数据点的数据集进行生成模型的训练，以提高生成结果的精度和满足约束条件的能力。实验证明，与只使用有效数据点进行训练的标准模型相比，基于无效数据训练的模型明显优于标准模型。

    

    生成模型在视觉、语言和语音等领域取得了令人印象深刻的结果。然而，即使有大量的数据集，它们仍然在精度上存在困难，生成出物理上无效或事实上不正确的数据。当生成的数据必须满足约束条件时，这一问题尤为严重，例如，在工程设计中满足产品规格或者在自然场景中遵守物理定律。为了提高精度并保持多样性和保真度，我们提出了一种新的训练机制，利用包含无效数据点的数据集进行训练。我们的方法最小化了生成分布与有效先验之间的差异，同时最大化了与无效分布之间的差异。我们演示了将GAN和DDPM等生成模型与无效数据一起训练的方法明显优于仅使用有效数据点进行训练的标准模型。例如，我们的训练过程生成了……

    Generative models have demonstrated impressive results in vision, language, and speech. However, even with massive datasets, they struggle with precision, generating physically invalid or factually incorrect data. This is particularly problematic when the generated data must satisfy constraints, for example, to meet product specifications in engineering design or to adhere to the laws of physics in a natural scene. To improve precision while preserving diversity and fidelity, we propose a novel training mechanism that leverages datasets of constraint-violating data points, which we consider invalid. Our approach minimizes the divergence between the generative distribution and the valid prior while maximizing the divergence with the invalid distribution. We demonstrate how generative models like GANs and DDPMs that we augment to train with invalid data vastly outperform their standard counterparts which solely train on valid data points. For example, our training procedure generates up 
    
[^63]: 用分布偏移风险最小化增强文本对抗训练（DSRM）

    DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization. (arXiv:2306.15164v1 [cs.CL])

    [http://arxiv.org/abs/2306.15164](http://arxiv.org/abs/2306.15164)

    本论文介绍了一种新颖的方法DSRM，通过最小化分布偏移风险而不是使用对抗样本来对抗训练，从而提高了深度语言模型的鲁棒性，减少了时间消耗。

    

    对抗训练是改善深度语言模型鲁棒性的最佳方法之一。然而，鲁棒模型的代价是时间消耗高，因为它们需要多步梯度上升或单词替换来获取对抗样本。此外，这些生成的样本在语法质量和语义一致性方面存在缺陷，影响了对抗训练的有效性。为了解决这些问题，我们引入了一种新颖、有效的过程，将对抗训练改为只使用干净数据。我们的方法DSRM通过扰动输入数据的概率分布而不是它们的嵌入来估计对抗损失。这种公式化结果导致了一个在对抗攻击下最小化期望全局损失的鲁棒模型。我们的方法在训练过程中不需要对抗样本，并且与当前最佳对抗训练相比，减少了高达70\%的时间消耗。

    Adversarial training is one of the best-performing methods in improving the robustness of deep language models. However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training. To address these problems, we introduce a novel, effective procedure for instead adversarial training with only clean data. Our procedure, distribution shift risk minimization (DSRM), estimates the adversarial loss by perturbing the input data's probability distribution rather than their embeddings. This formulation results in a robust model that minimizes the expected global loss under adversarial attacks. Our approach requires zero adversarial samples for training and reduces time consumption by up to 70\% compared to current best-performing adversarial traini
    
[^64]: Wasserstein生成回归

    Wasserstein Generative Regression. (arXiv:2306.15163v1 [stat.ML])

    [http://arxiv.org/abs/2306.15163](http://arxiv.org/abs/2306.15163)

    我们提出了一种新的统一的非参数回归和条件分布学习方法，使用生成学习框架同时估计回归函数和条件生成器，并使用深度神经网络建模条件生成器。我们的方法可以处理具有多元输出和协变量的问题，可以构建预测区间，并通过理论保证和数值实验证明了方法的有效性和优越性。

    

    在这篇论文中，我们提出了一种新的统一的非参数回归和条件分布学习方法。我们的方法同时估计回归函数和条件生成器，使用生成学习框架，其中条件生成器是一个可以从条件分布生成样本的函数。主要思想是估计一个满足产生良好回归函数估计的约束条件的条件生成器。我们使用深度神经网络来建模条件生成器。我们的方法可以处理具有多元输出和协变量的问题，并且可以用来构建预测区间。我们通过得出非渐近误差界和在适当的假设下我们方法的分布一致性来提供理论保证。我们还使用模拟和实际数据进行数值实验，以展示我们的方法在某些现有方法上的有效性和优越性。

    In this paper, we propose a new and unified approach for nonparametric regression and conditional distribution learning. Our approach simultaneously estimates a regression function and a conditional generator using a generative learning framework, where a conditional generator is a function that can generate samples from a conditional distribution. The main idea is to estimate a conditional generator that satisfies the constraint that it produces a good regression function estimator. We use deep neural networks to model the conditional generator. Our approach can handle problems with multivariate outcomes and covariates, and can be used to construct prediction intervals. We provide theoretical guarantees by deriving non-asymptotic error bounds and the distributional consistency of our approach under suitable assumptions. We also perform numerical experiments with simulated and real data to demonstrate the effectiveness and superiority of our approach over some existing approaches in va
    
[^65]: 在复杂动态系统的UQ中评估机器学习架构对关于认知和测试误差的量化

    Evaluation of machine learning architectures on the quantification of epistemic and aleatoric uncertainties in complex dynamical systems. (arXiv:2306.15159v1 [stat.ML])

    [http://arxiv.org/abs/2306.15159](http://arxiv.org/abs/2306.15159)

    这项研究评估了多种机器学习技术在复杂动态系统的UQ中的准确性，为降低训练数据集规模和安全因子提供了成本节约的机会。

    

    机器学习方法用于构建数据驱动的降阶模型，被广泛应用于工程领域，特别是作为昂贵计算流体力学的替代方法。对于模型可靠性的重要检验是不确定性量化（UQ），它是对模型误差的自我评估。准确的UQ可以通过减少训练数据集的规模和安全因子来实现成本节约，而较差的UQ则会阻碍用户对模型预测的信任。我们研究了几种机器学习技术，包括高斯过程和一系列增强UQ的神经网络：集成神经网络（ENN）、贝叶斯神经网络（BNN）、Dropout神经网络（D-NN）和高斯神经网络（G-NN）。我们使用两个指标来评估UQ准确性（与模型准确性不同）：验证数据上归一化残差的分布和估计分布。

    Machine learning methods for the construction of data-driven reduced order model models are used in an increasing variety of engineering domains, especially as a supplement to expensive computational fluid dynamics for design problems. An important check on the reliability of surrogate models is Uncertainty Quantification (UQ), a self assessed estimate of the model error. Accurate UQ allows for cost savings by reducing both the required size of training data sets and the required safety factors, while poor UQ prevents users from confidently relying on model predictions. We examine several machine learning techniques, including both Gaussian processes and a family UQ-augmented neural networks: Ensemble neural networks (ENN), Bayesian neural networks (BNN), Dropout neural networks (D-NN), and Gaussian neural networks (G-NN). We evaluate UQ accuracy (distinct from model accuracy) using two metrics: the distribution of normalized residuals on validation data, and the distribution of estima
    
[^66]: 重新审视热带多项式除法：理论、算法与神经网络应用

    Revisiting Tropical Polynomial Division: Theory, Algorithms and Application to Neural Networks. (arXiv:2306.15157v1 [cs.LG])

    [http://arxiv.org/abs/2306.15157](http://arxiv.org/abs/2306.15157)

    本文重新审视了热带多项式除法的问题，并将其应用于神经网络简化中。我们分析了具有实系数的热带多项式，证明了唯一商余对的存在，并提出了精确算法和近似算法。

    

    最近热带几何在分析具有分段线性激活函数的神经网络中发现了几个应用。本文对热带多项式除法问题及其在神经网络简化中的应用进行了新的研究。我们分析了具有实系数的热带多项式，扩展了早期对整数系数多项式的思想和方法。我们首先证明了唯一商余对的存在，并以一个相关函数的凸双共轭来表征商。有趣的是，具有整数系数的热带多项式的商不一定具有整数系数。此外，我们建立了热带多项式除法与计算凸多面体并集的凸包的关系，并用它来推导了热带多项式除法的精确算法。还提出了一种基于数据的交替的近似算法。

    Tropical geometry has recently found several applications in the analysis of neural networks with piecewise linear activation functions. This paper presents a new look at the problem of tropical polynomial division and its application to the simplification of neural networks. We analyze tropical polynomials with real coefficients, extending earlier ideas and methods developed for polynomials with integer coefficients. We first prove the existence of a unique quotient-remainder pair and characterize the quotient in terms of the convex bi-conjugate of a related function. Interestingly, the quotient of tropical polynomials with integer coefficients does not necessarily have integer coefficients. Furthermore, we develop a relationship of tropical polynomial division with the computation of the convex hull of unions of convex polyhedra and use it to derive an exact algorithm for tropical polynomial division. An approximate algorithm is also presented, based on an alternation between data pa
    
[^67]: 从仅状态序列学习非马尔科夫决策

    Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v1 [cs.LG])

    [http://arxiv.org/abs/2306.15156](http://arxiv.org/abs/2306.15156)

    本文提出了一种从仅状态序列学习非马尔科夫决策的方法，通过深度生成建模和最大似然估计实现基于模型的模仿。学习的模型能够实现“推理式决策”，并在路径规划任务中展示了有效性。

    

    传统的模仿学习假设能够获得展示者的动作，但是在自然环境中这些动作通常无法观测。此外，在这些环境中的序列决策行为可能偏离标准马尔科夫决策过程（MDP）的假设。为了解决这些挑战，我们探索了非马尔科夫决策过程（nMDP）中仅状态序列的深度生成建模，其中策略是潜在状态转移生成器的能量先验。我们开发了最大似然估计来实现基于模型的模仿，其中包括对先验进行短期MCMC采样和对后验进行重要性采样。学习的模型实现了“推理式决策”，即无模型策略执行等价于先验采样，基于模型的规划则是从策略初始化的后验采样。我们在一个具有非马尔科夫特征的原型路径规划任务中证明了所提方法的有效性。

    Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables \textit{decision-making as inference}: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Mark
    
[^68]: 针对GNN加速的输入敏感的稠密-稀疏基本组成元素

    Input-sensitive dense-sparse primitive compositions for GNN acceleration. (arXiv:2306.15155v1 [cs.LG])

    [http://arxiv.org/abs/2306.15155](http://arxiv.org/abs/2306.15155)

    本文提出了一种在不同的输入图和GNN嵌入大小上使用代数重组的方法，通过选择最佳组合来提高GNN加速的性能。

    

    图神经网络（GNN）已成为一类重要的神经网络模型，在社交和金融网络分析等领域越来越受欢迎。GNN计算的不同阶段可以使用稠密和稀疏矩阵运算来建模。文献中已经提出了许多框架和优化技术来加速GNN。然而，在许多具有不同稀疏模式和GNN嵌入大小的输入图上实现一致高性能仍然困难。在本文中，我们提出了对GNN计算进行不同的代数重组，导致了新的密集和稀疏矩阵基本选择和组合。我们表明，这些组合的盈利能力取决于输入图、嵌入大小和目标硬件。我们开发了一个名为SENSEi的系统，它使用数据驱动的自适应策略来选择给定输入图和GNN嵌入大小的最佳组合。我们在广泛的范围上进行了评估。

    Graph neural networks (GNN) have become an important class of neural network models that have gained popularity in domains such as social and financial network analysis. Different phases of GNN computations can be modeled using both dense and sparse matrix operations. There have been many frameworks and optimization techniques proposed in the literature to accelerate GNNs. However, getting consistently high performance across many input graphs with different sparsity patterns and GNN embedding sizes has remained difficult.  In this paper, we propose different algebraic reassociations of GNN computations that lead to novel dense and sparse matrix primitive selections and compositions. We show that the profitability of these compositions depends on the input graph, embedding size, and the target hardware. We developed SENSEi, a system that uses a data-driven adaptive strategy to select the best composition given the input graph and GNN embedding sizes. Our evaluations on a wide range of 
    
[^69]: 少样本节点分类的对比元学习

    Contrastive Meta-Learning for Few-shot Node Classification. (arXiv:2306.15154v1 [cs.LG])

    [http://arxiv.org/abs/2306.15154](http://arxiv.org/abs/2306.15154)

    这篇论文提出了一种对比元学习的方法来解决少样本节点分类的问题，通过从大量有标记节点的类别中抽取可迁移的知识，并将其推广到具有有限标记节点的其他类别，学习通用的节点嵌入。

    

    少样本节点分类是在只有有限标记节点作为参考的图上为节点预测标签的任务，在真实世界的图挖掘任务中具有重要意义。本文针对少样本节点分类问题，通过元学习框架利用大量的episode从有大量标记节点的类别中抽取可迁移的知识，并将这些知识推广到具有有限标记节点的其他类别。本质上，少样本节点分类的主要目标是学习可推广到不同类别的节点嵌入。为了实现这一目标，GNN编码器必须能够区分不同类别之间的节点嵌入，同时还要对同一类别中的节点嵌入进行对齐。因此，在这项工作中，我们提出考虑类内和类间的节点嵌入对比。

    Few-shot node classification, which aims to predict labels for nodes on graphs with only limited labeled nodes as references, is of great significance in real-world graph mining tasks. Particularly, in this paper, we refer to the task of classifying nodes in classes with a few labeled nodes as the few-shot node classification problem. To tackle such a label shortage issue, existing works generally leverage the meta-learning framework, which utilizes a number of episodes to extract transferable knowledge from classes with abundant labeled nodes and generalizes the knowledge to other classes with limited labeled nodes. In essence, the primary aim of few-shot node classification is to learn node embeddings that are generalizable across different classes. To accomplish this, the GNN encoder must be able to distinguish node embeddings between different classes, while also aligning embeddings for nodes in the same class. Thus, in this work, we propose to consider both the intra-class and int
    
[^70]: 一个具有自我引导和块对角表示的重启大规模谱聚类方法

    A Restarted Large-Scale Spectral Clustering with Self-Guiding and Block Diagonal Representation. (arXiv:2306.15138v1 [cs.LG])

    [http://arxiv.org/abs/2306.15138](http://arxiv.org/abs/2306.15138)

    本论文提出了一个具有自我引导和块对角表示的重启聚类方法，该方法在谱聚类中首次应用重启策略，并且通过在每个周期中重新分类样本来获得更好的聚类效果。

    

    谱聚类是最流行的无监督机器学习方法之一。构建相似性矩阵对于这类方法至关重要。在大多数现有方法中，相似性矩阵只计算一次或者是交替更新。然而，前者很难反映数据点之间的全面关系，而后者耗时且在大规模问题中甚至难以实施。在本文中，我们提出了一个具有自我引导和块对角表示的重启聚类框架。该策略的优势在于尽可能保留从先前周期中获得的有用聚类信息。据我们所知，这是第一个将重启策略应用于谱聚类的工作。关键区别在于我们在方法的每个周期中重新对样本进行分类，而现有方法只进行一次分类。为了进一步减少开销，我们引入了一个块对角表示方法。

    Spectral clustering is one of the most popular unsupervised machine learning methods. Constructing similarity matrix is crucial to this type of method. In most existing works, the similarity matrix is computed once for all or is updated alternatively. However, the former is difficult to reflect comprehensive relationships among data points, and the latter is time-consuming and is even infeasible for large-scale problems. In this work, we propose a restarted clustering framework with self-guiding and block diagonal representation. An advantage of the strategy is that some useful clustering information obtained from previous cycles could be preserved as much as possible. To the best of our knowledge, this is the first work that applies restarting strategy to spectral clustering. The key difference is that we reclassify the samples in each cycle of our method, while they are classified only once in existing methods. To further release the overhead, we introduce a block diagonal representa
    
[^71]: MIMIC: 基于图像对应关系的遮蔽图像建模

    MIMIC: Masked Image Modeling with Image Correspondences. (arXiv:2306.15128v1 [cs.CV])

    [http://arxiv.org/abs/2306.15128](http://arxiv.org/abs/2306.15128)

    MIMIC是一种基于图像对应关系的遮蔽图像建模方法，通过挖掘不需要任何注释的数据集，使用多个自监督模型进行训练，达到了在多个下游任务上优于使用注释挖掘的表示的效果。

    

    许多像素级的密集预测任务——如计算机视觉中的深度估计和语义分割——如今依赖于预训练的图像表示。因此，筛选有效的预训练数据集至关重要。不幸的是，有效的预训练数据集仅通过模拟环境中的带有注释的3D网格、点云和相机参数筛选而来，并不具备多视角场景。我们提出了一种不需要任何注释的数据集筛选机制。我们从开源视频数据集和合成的3D环境中挖掘了两个数据集：MIMIC-1M(包含1.3M个多视角图像对)和MIMIC-3M(包含3.1M个多视角图像对)。我们使用多个自监督模型进行训练，采用不同的遮蔽图像建模目标，展示了以下发现：在多个下游任务中，基于MIMIC-3M训练的表示优于使用注释挖掘的表示，包括深度估计、语义分割、表面法线和姿态估计等。

    Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representati
    
[^72]: 不同用户行为下的排名策略的离策略评估

    Off-Policy Evaluation of Ranking Policies under Diverse User Behavior. (arXiv:2306.15098v1 [stat.ML])

    [http://arxiv.org/abs/2306.15098](http://arxiv.org/abs/2306.15098)

    本文提出了一种新的离策略评估方法Adaptive IPS (AIPS)，针对排名策略的离策略评估问题，通过考虑用户行为的多样性和上下文的变化，有效降低了估计中的偏差和方差。

    

    在线平台上到处都是排名界面。因此，对于使用记录数据进行排名策略的准确性能评估的离策略评估（OPE）越来越感兴趣。OPE的一种事实上的方法是倒数倾向得分法（IPS），它提供了一个无偏且一致的值估计。然而，在排名设置中，由于在大型行为空间下具有较高的方差，它变得极其不准确。为了解决这个问题，先前的研究假设用户行为是独立的或级联的，从而产生了一些IPS的排名版本。尽管这些估计器在减少方差方面有一定的效果，但所有现有的估计器都对每个用户应用了一个单一的通用假设，导致过度的偏差和方差。因此，这项工作探索了更通用的公式，其中用户行为是多样的，并且可以根据用户上下文的不同而变化。我们展示出由此产生的估计器，我们称之为自适应IPS（AIPS），可以更准确地进行离策略评估和排名策略。

    Ranking interfaces are everywhere in online platforms. There is thus an ever growing interest in their Off-Policy Evaluation (OPE), aiming towards an accurate performance evaluation of ranking policies using logged data. A de-facto approach for OPE is Inverse Propensity Scoring (IPS), which provides an unbiased and consistent value estimate. However, it becomes extremely inaccurate in the ranking setup due to its high variance under large action spaces. To deal with this problem, previous studies assume either independent or cascade user behavior, resulting in some ranking versions of IPS. While these estimators are somewhat effective in reducing the variance, all existing estimators apply a single universal assumption to every user, causing excessive bias and variance. Therefore, this work explores a far more general formulation where user behavior is diverse and can vary depending on the user context. We show that the resulting estimator, which we call Adaptive IPS (AIPS), can be unb
    
[^73]: C3S微体系结构增强：刺激编码器模块和放松伽马时钟（异步）。

    C3S Micro-architectural Enhancement: Spike Encoder Block and Relaxing Gamma Clock (Asynchronous). (arXiv:2306.15093v1 [cs.AR])

    [http://arxiv.org/abs/2306.15093](http://arxiv.org/abs/2306.15093)

    C3S微体系结构增强的创新是通过引入刺激编码器模块和放松伽马时钟来改进现有的皮层列架构。

    

    神经形态计算领域正在快速发展。随着对生物准确性和实际实现的探索，现有架构被修改和改进以适应这两个目的。时间神经网络（TNN）式的架构是近似生物神经元的好基础，因为它使用定时脉冲来编码数据和电压阈值系统。基于时间神经网络皮层模型C3S架构设计，本项目旨在增强网络设计。本项目注意到两个想法，并提出了它们的设计，以改善现有的皮层列架构。该领域的一个需要是拥有一种能够在常见数字格式和定时神经脉冲之间转换的编码器，因为生物准确网络的本质是时间性的。为此，本项目提出了一种编码器，用于将二进制编码值和定时脉冲进行转换，以供神经网络处理。另一个需求是放松伽马时钟。

    The field of neuromorphic computing is rapidly evolving. As both biological accuracy and practical implementations are explored, existing architectures are modified and improved for both purposes. The Temporal Neural Network(TNN) style of architecture is a good basis for approximating biological neurons due to its use of timed pulses to encode data and a voltage-threshold-like system. Using the Temporal Neural Network cortical column C3S architecture design as a basis, this project seeks to augment the network's design. This project takes note of two ideas and presents their designs with the goal of improving existing cortical column architecture. One need in this field is for an encoder that could convert between common digital formats and timed neuronal spikes, as biologically accurate networks are temporal in nature. To this end, this project presents an encoder to translate between binary encoded values and timed spikes to be processed by the neural network. Another need is for the
    
[^74]: 使用高阶动态模态分解方法对地下农业农场进行能量建模和预测

    Energy Modelling and Forecasting for an Underground Agricultural Farm using a Higher Order Dynamic Mode Decomposition Approach. (arXiv:2306.15089v1 [cs.LG])

    [http://arxiv.org/abs/2306.15089](http://arxiv.org/abs/2306.15089)

    本文基于高阶动态模态分解方法提出了一种能量建模和预测的方法，应用于伦敦地下农业农场。这种方法可以处理受噪声和瞬态条件影响的复杂数据，并分析和预测农场的环境行为。

    

    本文提出了一种基于高阶动态模态分解（HODMD）的方法，用于对位于伦敦地下隧道中的改造农业农场的能量行为进行建模、分析和预测。在这个农场中，观测到的测量数据受到噪声和偶尔的瞬态条件的影响。HODMD 是一种数据驱动的降阶建模方法，通常用于分析和预测流体动力学中高噪声和复杂流动，或者来自动力系统的任何类型的复杂数据。HODMD 是经典动态模态分解方法（DMD）的最新扩展，专门用于处理测量数据的谱复杂度高于其空间复杂度的场景，例如农场的环境行为。HODMD将时间数据分解为物理含义的DMD模态的线性展开，采用时延嵌入方法进行半自动处理。我们使用实际测量的数据将HODMD应用于三个季节的情景中。

    This paper presents an approach based on higher order dynamic mode decomposition (HODMD) to model, analyse, and forecast energy behaviour in an urban agriculture farm situated in a retrofitted London underground tunnel, where observed measurements are influenced by noisy and occasionally transient conditions. HODMD is a data-driven reduced order modelling method typically used to analyse and predict highly noisy and complex flows in fluid dynamics or any type of complex data from dynamical systems. HODMD is a recent extension of the classical dynamic mode decomposition method (DMD), customised to handle scenarios where the spectral complexity underlying the measurement data is higher than its spatial complexity, such as is the environmental behaviour of the farm. HODMD decomposes temporal data as a linear expansion of physically-meaningful DMD-modes in a semi-automatic approach, using a time-delay embedded approach. We apply HODMD to three seasonal scenarios using real data measured by
    
[^75]: 平衡过滤使用非泄露代理

    Balanced Filtering via Non-Disclosive Proxies. (arXiv:2306.15083v1 [cs.LG])

    [http://arxiv.org/abs/2306.15083](http://arxiv.org/abs/2306.15083)

    本文研究了在群组成员资格不可用或被禁止使用时，如何以非泄露方式收集平衡的数据样本。通过使用代理函数和抽样概率，实现了对个体样本的分类和选择，同时保护个体样本的敏感群组成员资格不被泄露。

    

    当群组成员资格在收集时不可用或被禁止使用时，我们研究了非泄露方式收集与敏感群组平衡的数据样本的问题。具体而言，我们的收集机制不会比基本比率能够确定的任何个体样本的群组成员资格更多地透露相关信息。为了做到这一点，我们采用了公平流程的观点，即学习者可以使用少量的标记数据训练代理函数，这个代理函数以后可以用于这个过滤任务。然后，我们将代理函数的范围与抽样概率相关联；给定一个新的候选样本，我们使用代理函数对其进行分类，然后根据与其代理分类对应的抽样概率选择它作为我们的样本。重要的是，我们要求代理分类本身不会透露任何个体样本的敏感群组成员资格的重要信息（即，它应该是非泄露的）。

    We study the problem of non-disclosively collecting a sample of data that is balanced with respect to sensitive groups when group membership is unavailable or prohibited from use at collection time. Specifically, our collection mechanism does not reveal significantly more about group membership of any individual sample than can be ascertained from base rates alone. To do this, we adopt a fairness pipeline perspective, in which a learner can use a small set of labeled data to train a proxy function that can later be used for this filtering task. We then associate the range of the proxy function with sampling probabilities; given a new candidate, we classify it using our proxy function, and then select it for our sample with probability proportional to the sampling probability corresponding to its proxy classification. Importantly, we require that the proxy classification itself not reveal significant information about the sensitive group membership of any individual sample (i.e., it sho
    
[^76]: 分子几何深度学习

    Molecular geometric deep learning. (arXiv:2306.15065v1 [physics.comp-ph])

    [http://arxiv.org/abs/2306.15065](http://arxiv.org/abs/2306.15065)

    分子几何深度学习（Mol-GDL）提出了一种更通用的分子表示方法，通过仅使用非共价键构建的分子图，在分子性质预测中取得了与基于共价键模型相似甚至更好的结果，展现了超越基于共价键的分子图的巨大潜力。

    

    几何深度学习（GDL）在分子数据分析中展示了巨大的威力和潜力。然而，高效的分子表示仍然是一个巨大的挑战。目前，基于共价键的分子图已成为表示原子层次的分子拓扑的事实标准。我们首次证明，仅由非共价键构建的分子图可以在分子性质预测中取得与基于共价键模型相似甚至更好的结果。这证明了超越基于共价键的分子图的新型分子表示的巨大潜力。基于这一发现，我们提出了分子几何深度学习（Mol-GDL）。其核心思想是将更通用的分子表示融入GDL模型中。在我们的Mol-GDL中，分子拓扑被建模为一系列的分子图，每个分子图聚焦于不同尺度的原子相互作用。

    Geometric deep learning (GDL) has demonstrated huge power and enormous potential in molecular data analysis. However, a great challenge still remains for highly efficient molecular representations. Currently, covalent-bond-based molecular graphs are the de facto standard for representing molecular topology at the atomic level. Here we demonstrate, for the first time, that molecular graphs constructed only from non-covalent bonds can achieve similar or even better results than covalent-bond-based models in molecular property prediction. This demonstrates the great potential of novel molecular representations beyond the de facto standard of covalent-bond-based molecular graphs. Based on the finding, we propose molecular geometric deep learning (Mol-GDL). The essential idea is to incorporate a more general molecular representation into GDL models. In our Mol-GDL, molecular topology is modeled as a series of molecular graphs, each focusing on a different scale of atomic interactions. In th
    
[^77]: 预训练任务多样性与回归问题中非贝叶斯上下文学习的出现

    Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. (arXiv:2306.15063v1 [cs.LG])

    [http://arxiv.org/abs/2306.15063](http://arxiv.org/abs/2306.15063)

    预训练的transformer在回归问题中展现了非贝叶斯上下文学习能力，其在任务多样性阈值以下表现类似于贝叶斯估计器，而在阈值以上明显优于贝叶斯估计器，与岭回归一致。

    

    预训练的transformer表现出了令人钦佩的上下文学习能力（ICL）：它们可以从仅提供在提示中的少量示例中学习任务，而无需更新任何权重。这引发了一个基本问题：ICL能够解决在预训练期间未见过的、在本质上与之前任务非常不同的新任务吗？为了探索这个问题，我们在预训练数据集中改变任务的多样性，研究了ICL在线性回归中的表现。我们经验性地证明了出现ICL的任务多样性阈值。在这个阈值以下，预训练的transformer无法解决未见的回归任务，因为它的行为类似于具有非多样性预训练任务分布作为先验的贝叶斯估计器。超过这个阈值后，transformer明显优于这个估计器；它的行为与岭回归一致，对$\textit{所有任务}$，包括在预训练期间未见过的任务，具有高斯先验。

    Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally $\textit{new}$ tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a $\textit{task diversity threshold}$ for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks as it behaves like a Bayesian estimator with the $\textit{non-diverse pretraining task distribution}$ as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over $\textit{all tasks}$, including those not seen during pretraining. 
    
[^78]: BatchGFN: 用于批量主动学习的生成流网络

    BatchGFN: Generative Flow Networks for Batch Active Learning. (arXiv:2306.15058v1 [cs.LG])

    [http://arxiv.org/abs/2306.15058](http://arxiv.org/abs/2306.15058)

    BatchGFN是一种用于批量主动学习的新颖方法，通过使用生成流网络根据批量奖励采样数据点集合，能够以原则性的方式构建高度信息量的批量，用于主动学习。通过在推理时间内进行单次前向传递来采样近乎最优效用的批量，减轻了面向批量的算法的计算复杂性，并消除了贪婪近似的需求。提出了跨获取步骤分摊训练的早期结果，实现了对实际任务的扩展。

    

    我们引入了BatchGFN - 一种新颖的基于池的主动学习方法，该方法使用生成流网络根据批量奖励采样数据点集合。通过适当的奖励函数来量化获取批量的效用，如批量与模型参数之间的联合互信息，BatchGFN能够以原则性的方式构建高度信息量的批量，用于主动学习。我们展示了我们的方法在玩具回归问题中可以在推理时间内通过对批量中每个点进行单次前向传递来采样近乎最优效用的批量，这减轻了面向批量的算法的计算复杂性，并消除了寻找批量奖励最大化器的贪婪近似的需求。我们还提出了跨获取步骤分摊训练的早期结果，这将实现对实际任务的扩展。

    We introduce BatchGFN -- a novel approach for pool-based active learning that uses generative flow networks to sample sets of data points proportional to a batch reward. With an appropriate reward function to quantify the utility of acquiring a batch, such as the joint mutual information between the batch and the model parameters, BatchGFN is able to construct highly informative batches for active learning in a principled way. We show our approach enables sampling near-optimal utility batches at inference time with a single forward pass per point in the batch in toy regression problems. This alleviates the computational complexity of batch-aware algorithms and removes the need for greedy approximations to find maximizers for the batch reward. We also present early results for amortizing training across acquisition steps, which will enable scaling to real-world tasks.
    
[^79]: 具有公共数据的最优差分隐私学习

    Optimal Differentially Private Learning with Public Data. (arXiv:2306.15056v1 [cs.LG])

    [http://arxiv.org/abs/2306.15056](http://arxiv.org/abs/2306.15056)

    本论文研究了具有公共数据的最优差分隐私学习，并解决了在训练差分隐私模型时如何利用公共数据提高准确性的问题。

    

    差分隐私能够确保训练机器学习模型不泄漏私密数据。然而，差分隐私的代价是模型的准确性降低或样本复杂度增加。在实践中，我们可能可以访问不涉及隐私问题的辅助公共数据。这促使了最近研究公共数据在提高差分隐私模型准确性方面的作用。在本研究中，我们假设有一定数量的公共数据，并解决以下基本开放问题：1.在有公共数据的情况下，训练基于私有数据集的差分隐私模型的最优（最坏情况）误差是多少？哪些算法是最优的？2.如何利用公共数据在实践中改进差分隐私模型训练？我们在本地模型和中心模型的差分隐私问题下考虑这些问题。为了回答第一个问题，我们证明了对三个基本问题的最优误差率的紧密（最高常数因子）下界和上界。这三个问题是：均值估计，经验风险最小化和凸奇化。

    Differential Privacy (DP) ensures that training a machine learning model does not leak private data. However, the cost of DP is lower model accuracy or higher sample complexity. In practice, we may have access to auxiliary public data that is free of privacy concerns. This has motivated the recent study of what role public data might play in improving the accuracy of DP models. In this work, we assume access to a given amount of public data and settle the following fundamental open questions: 1. What is the optimal (worst-case) error of a DP model trained over a private data set while having access to side public data? What algorithms are optimal? 2. How can we harness public data to improve DP model training in practice? We consider these questions in both the local and central models of DP. To answer the first question, we prove tight (up to constant factors) lower and upper bounds that characterize the optimal error rates of three fundamental problems: mean estimation, empirical ris
    
[^80]: 实现去中心化学习中的Sybil鲁棒性

    Towards Sybil Resilience in Decentralized Learning. (arXiv:2306.15044v1 [cs.DC])

    [http://arxiv.org/abs/2306.15044](http://arxiv.org/abs/2306.15044)

    本研究针对去中心化学习的Sybil毒化攻击问题，提出了一种名为SybilWall的创新算法，旨在增强去中心化学习对这类攻击的鲁棒性。

    

    联邦学习是一种保护隐私的机器学习技术，但受到可扩展性的限制。这种限制主要来自于中央参数服务器的互联网连接和内存容量，以及模型聚合函数的复杂性。近年来，去中心化学习作为联邦学习的一种有前景的替代方案逐渐兴起。这种新技术通过将模型聚合分散到所有参与节点中，消除了对中央参数服务器的需求。许多研究已经针对提高联邦学习对毒化攻击和Sybil攻击的鲁棒性进行了研究，而去中心化学习的鲁棒性仍然鲜有研究。这个研究空白是本研究的主要动机，我们的目标是提高去中心化学习的Sybil毒化鲁棒性。我们提出了SybilWall，这是一种专注于增强去中心化学习对Sybil攻击的鲁棒性的创新算法。

    Federated learning is a privacy-enforcing machine learning technology but suffers from limited scalability. This limitation mostly originates from the internet connection and memory capacity of the central parameter server, and the complexity of the model aggregation function. Decentralized learning has recently been emerging as a promising alternative to federated learning. This novel technology eliminates the need for a central parameter server by decentralizing the model aggregation across all participating nodes. Numerous studies have been conducted on improving the resilience of federated learning against poisoning and Sybil attacks, whereas the resilience of decentralized learning remains largely unstudied. This research gap serves as the main motivator for this study, in which our objective is to improve the Sybil poisoning resilience of decentralized learning.  We present SybilWall, an innovative algorithm focused on increasing the resilience of decentralized learning against t
    
[^81]: 等变流匹配

    Equivariant flow matching. (arXiv:2306.15030v1 [stat.ML])

    [http://arxiv.org/abs/2306.15030](http://arxiv.org/abs/2306.15030)

    本文介绍了一种基于最优输运流匹配的等变CNF训练目标，可以提高等变CNF的可扩展性和实际应用。

    

    标准化流是一类特别适用于物理学中概率分布建模的深度生成模型。其中，流的准确似然性质可以实现对已知目标能量函数的加权重重和无偏观测量的计算。例如，Boltzmann生成器通过训练流生成处于平衡状态的多体系统（如小分子和蛋白质）样本，解决了统计物理学中长期存在的采样问题。为了构建有效的模型，也很关键将目标能量的对称性纳入模型中，这可以通过等变连续标准化流（CNF）来实现。然而，CNF的训练和样本生成的计算开销较大，这限制了它们的可扩展性和实际应用。在本文中，我们引入了等变流匹配，一种新的等变CNF训练目标，其基于最近提出的最优输运流匹配方法。

    Normalizing flows are a class of deep generative models that are especially interesting for modeling probability distributions in physics, where the exact likelihood of flows allows reweighting to known target energy functions and computing unbiased observables. For instance, Boltzmann generators tackle the long-standing sampling problem in statistical physics by training flows to produce equilibrium samples of many-body systems such as small molecules and proteins. To build effective models for such systems, it is crucial to incorporate the symmetries of the target energy into the model, which can be achieved by equivariant continuous normalizing flows (CNFs). However, CNFs can be computationally expensive to train and generate samples from, which has hampered their scalability and practical application. In this paper, we introduce equivariant flow matching, a new training objective for equivariant CNFs that is based on the recently proposed optimal transport flow matching. Equivarian
    
[^82]: 超越动态规划

    Beyond dynamic programming. (arXiv:2306.15029v1 [cs.LG])

    [http://arxiv.org/abs/2306.15029](http://arxiv.org/abs/2306.15029)

    提出了一种新的理论方法——得分寿命规划，用于解决强化学习问题。方法可以搜索非稳态策略函数，并直接计算最优无限时间区间动作序列。这种方法的核心思想是建立动作序列和实数之间的映射，并通过优化问题计算最优动作序列。方法在非线性最优控制问题中证明了有效性。

    

    本文提出了一种新颖的理论方法——得分寿命规划，用于解决强化学习问题。与传统的基于动态规划的方法相比，我们的方法可以搜索非稳态策略函数，并且可以直接计算给定状态下的最优无限时间区间动作序列。我们方法的核心思想是建立无限时间区间动作序列和有界区间内实数之间的映射。这种构造使我们能够制定一个优化问题，直接计算最优无限时间区间动作序列，无需策略函数。我们通过将该方法应用于非线性最优控制问题，验证了其有效性。总的来说，我们的贡献为制定和解决强化学习问题提供了一种新颖的理论框架。

    In this paper, we present Score-life programming, a novel theoretical approach for solving reinforcement learning problems. In contrast with classical dynamic programming-based methods, our method can search over non-stationary policy functions, and can directly compute optimal infinite horizon action sequences from a given state. The central idea in our method is the construction of a mapping between infinite horizon action sequences and real numbers in a bounded interval. This construction enables us to formulate an optimization problem for directly computing optimal infinite horizon action sequences, without requiring a policy function. We demonstrate the effectiveness of our approach by applying it to nonlinear optimal control problems. Overall, our contributions provide a novel theoretical framework for formulating and solving reinforcement learning problems.
    
[^83]: 在前馈网络中的缩放和调整对称性

    Scaling and Resizing Symmetry in Feedforward Networks. (arXiv:2306.15015v1 [cs.LG])

    [http://arxiv.org/abs/2306.15015](http://arxiv.org/abs/2306.15015)

    本研究提出了关于前馈网络中的缩放和调整对称性的发现，并说明了在临界点上的初始化方式对于提高训练速度具有重要意义。

    

    深度神经网络中的权重初始化对学习映射的收敛速度有很大影响。最近的研究表明，在随机初始化的情况下，随机权重和偏差的方差空间会发生混沌/有序相变。实验证明，如果在临界线上的数值上初始化神经网络，可以在训练速度方面取得很大改进。本文提供了证据，表明在临界线上的未训练前馈网络中也存在物理系统在临界点所表现出的缩放特性。此外，我们还提出了一个额外的数据调整对称性，直接继承自临界点的缩放对称性。

    Weights initialization in deep neural networks have a strong impact on the speed of converge of the learning map. Recent studies have shown that in the case of random initializations, a chaos/order phase transition occur in the space of variances of random weights and biases. Experiments then had shown that large improvements can be made, in terms of the training speed, if a neural network is initialized on values along the critical line of such phase transition. In this contribution, we show evidence that the scaling property exhibited by physical systems at criticality, is also present in untrained feedforward networks with random weights initialization at the critical line. Additionally, we suggest an additional data-resizing symmetry, which is directly inherited from the scaling symmetry at criticality.
    
[^84]: 用于噪声混合物中目标信号恢复的统计分量分离

    Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures. (arXiv:2306.15012v1 [stat.ML])

    [http://arxiv.org/abs/2306.15012](http://arxiv.org/abs/2306.15012)

    本论文提出了一种用于从噪声混合物中恢复目标信号的统计分量分离方法，并且在图像降噪任务中展示了其优于标准降噪方法的表现。

    

    当只对给定信号的特定属性感兴趣时，从一个加性混合物中分离信号可能是一个不必要地困难的问题。在本工作中，我们解决了更简单的“统计分量分离”问题，该问题专注于从噪声混合物中恢复目标信号的预定义统计描述量。假设可以获得噪声过程的样本，我们研究了一种方法，该方法旨在使受噪声样本污染的解决方案候选的统计特性与观测的混合物的统计特性匹配。首先，我们使用具有解析可追踪计算的简单示例分析了该方法的行为。然后，我们将其应用于图像降噪环境中，使用了1）基于小波的描述符，2）针对天体物理和ImageNet数据的ConvNet-based描述符。在第一种情况下，我们展示了我们的方法在大多数情况下比标准降噪方法更好地恢复了目标数据的描述符。此外，尽管不是为此目的构建的，它也表现出对目标信号描述符恢复的潜力。

    Separating signals from an additive mixture may be an unnecessarily hard problem when one is only interested in specific properties of a given signal. In this work, we tackle simpler "statistical component separation" problems that focus on recovering a predefined set of statistical descriptors of a target signal from a noisy mixture. Assuming access to samples of the noise process, we investigate a method devised to match the statistics of the solution candidate corrupted by noise samples with those of the observed mixture. We first analyze the behavior of this method using simple examples with analytically tractable calculations. Then, we apply it in an image denoising context employing 1) wavelet-based descriptors, 2) ConvNet-based descriptors on astrophysics and ImageNet data. In the case of 1), we show that our method better recovers the descriptors of the target data than a standard denoising method in most situations. Additionally, despite not constructed for this purpose, it pe
    
[^85]: 使用无监督分类对模拟和观测的Sentinel-2/MSI图像中的海洋垃圾进行光谱分析

    Spectral Analysis of Marine Debris in Simulated and Observed Sentinel-2/MSI Images using Unsupervised Classification. (arXiv:2306.15008v1 [cs.CV])

    [http://arxiv.org/abs/2306.15008](http://arxiv.org/abs/2306.15008)

    本研究使用无监督分类方法对模拟和观测的Sentinel-2/MSI图像中的海洋垃圾进行光谱分析，发现污染物的光谱行为受多种因素影响，并揭示了元素之间的光谱特征和趋势。

    

    海洋垃圾对海洋和沿海环境造成重大威胁，其影响不断增长。遥感技术通过广泛的覆盖范围和频繁的观测提供了传统缓解技术的有利补充，如当地清理行动和拖网调查。在本研究中，我们使用了辐射传输模型（RTM）模拟数据和Sentinel-2任务的多光谱仪（MSI）的数据结合机器学习算法。我们的目标是研究海洋塑料污染的光谱行为，并评估RTM在该研究领域的适用性。探索性分析和使用KMeans算法的无监督分类的结果表明，污染物的光谱行为受到聚合物类型和像素覆盖百分比等因素的影响。研究结果还揭示了元素之间的光谱特征、关联和差异的趋势。

    Marine litter poses significant threats to marine and coastal environments, with its impacts ever-growing. Remote sensing provides an advantageous supplement to traditional mitigation techniques, such as local cleaning operations and trawl net surveys, due to its capabilities for extensive coverage and frequent observation. In this study, we used Radiative Transfer Model (RTM) simulated data and data from the Multispectral Instrument (MSI) of the Sentinel-2 mission in combination with machine learning algorithms. Our aim was to study the spectral behavior of marine plastic pollution and evaluate the applicability of RTMs within this research area. The results from the exploratory analysis and unsupervised classification using the KMeans algorithm indicate that the spectral behavior of pollutants is influenced by factors such as the type of polymer and pixel coverage percentage. The findings also reveal spectral characteristics and trends of association and differentiation among element
    
[^86]: 机器学习软件系统中的质量问题

    Quality Issues in Machine Learning Software Systems. (arXiv:2306.15007v1 [cs.SE])

    [http://arxiv.org/abs/2306.15007](http://arxiv.org/abs/2306.15007)

    本文通过采访实践者和进行调查的方式，研究了机器学习软件系统中的质量问题，并确定了一个质量问题目录。

    

    上下文：在各个领域中，越来越多的需求是使用机器学习（ML）来解决复杂的问题。ML模型被实现为软件组件并部署在机器学习软件系统（MLSSs）中。问题：有必要确保MLSSs的服务质量。这些系统的错误或不良决策可能导致其他系统的故障，造成巨大的财务损失，甚至对人类生命构成威胁。 MLSSs的质量保证被认为是一项具有挑战性的任务，目前是一个热门的研究课题。目标：本文旨在从实践者的角度研究MLSSs中真实质量问题的特征。这项实证研究旨在确定MLSSs中的质量问题目录。方法：我们与实践者/专家进行了一系列采访，以获取他们处理质量问题时的经验和做法。我们通过对ML从业者的调查验证了所确定的质量问题。结果

    Context: An increasing demand is observed in various domains to employ Machine Learning (ML) for solving complex problems. ML models are implemented as software components and deployed in Machine Learning Software Systems (MLSSs). Problem: There is a strong need for ensuring the serving quality of MLSSs. False or poor decisions of such systems can lead to malfunction of other systems, significant financial losses, or even threats to human life. The quality assurance of MLSSs is considered a challenging task and currently is a hot research topic. Objective: This paper aims to investigate the characteristics of real quality issues in MLSSs from the viewpoint of practitioners. This empirical study aims to identify a catalog of quality issues in MLSSs. Method: We conduct a set of interviews with practitioners/experts, to gather insights about their experience and practices when dealing with quality issues. We validate the identified quality issues via a survey with ML practitioners. Result
    
[^87]: LM4HPC：面向高性能计算的有效语言模型应用

    LM4HPC: Towards Effective Language Model Application in High-Performance Computing. (arXiv:2306.14979v1 [cs.LG])

    [http://arxiv.org/abs/2306.14979](http://arxiv.org/abs/2306.14979)

    本文设计了LM4HPC框架，旨在通过使用语言模型LMs来分析和优化高性能计算（HPC）软件。通过支持HPC数据集、AI模型和流水线，LM4HPC可以帮助用户快速评估最先进的模型并生成有见地的排行榜。

    

    最近几年，诸如GPT-4之类的语言模型（LMs）已经广泛应用于多个领域，包括自然语言处理、可视化等。然而，由于缺乏高性能计算（HPC）特定的支持，将它们应用于分析和优化HPC软件仍然具有挑战性。在本文中，我们设计了LM4HPC框架，以便利用LMs进行HPC软件分析和优化的研究和开发。我们的框架专为支持HPC数据集、AI模型和流水线而设计，构建在机器学习软件栈的不同层级组件之上，并具有Hugging Face兼容的API。通过三个代表性任务，我们评估了我们的框架的原型。结果表明，LM4HPC可以帮助用户快速评估一组最先进的模型并生成有见地的排行榜。

    In recent years, language models (LMs), such as GPT-4, have been widely used in multiple domains, including natural language processing, visualization, and so on. However, applying them for analyzing and optimizing high-performance computing (HPC) software is still challenging due to the lack of HPC-specific support. In this paper, we design the LM4HPC framework to facilitate the research and development of HPC software analyses and optimizations using LMs. Tailored for supporting HPC datasets, AI models, and pipelines, our framework is built on top of a range of components from different levels of the machine learning software stack, with Hugging Face-compatible APIs. Using three representative tasks, we evaluated the prototype of our framework. The results show that LM4HPC can help users quickly evaluate a set of state-of-the-art models and generate insightful leaderboards.
    
[^88]: 针对子群的公平感知反事实论证

    Fairness Aware Counterfactuals for Subgroups. (arXiv:2306.14978v1 [cs.LG])

    [http://arxiv.org/abs/2306.14978](http://arxiv.org/abs/2306.14978)

    本文提出了针对子群的公平感知反事实论证（FACTS）框架，用于通过反事实解释来审查子群的公平性。该框架重新定义了子群公平性概念，旨在从微观和宏观两个角度考虑实现期望结果的困难，并引入了对于实现成本具有鲁棒性的子群公平性概念。通过实验评估，证明了该方法的优势、适用性和效率。

    

    在这项工作中，我们提出了针对子群的公平感知反事实论证（FACTS），这是一个通过反事实解释来审查子群公平性的框架。我们重新审视（并泛化）现有概念，并引入更精细的子群公平性概念。我们的目标是（a）在微观层面上考虑到子群成员个体，或在宏观层面上考虑整个子群，来表达某些子群个体在实现补救（即实现期望结果）方面的困难不同方面；以及（b）引入对于实现补救成本坚固的（如果不是完全无视的）子群公平性概念。我们结合这些概念提出了一个高效、模型无关、高度可参数化和可解释的框架来评估子群公平性。通过对不同基准测试的彻底实验评估，我们展示了我们的方法的优势、广泛适用性和效率。

    In this work, we present Fairness Aware Counterfactuals for Subgroups (FACTS), a framework for auditing subgroup fairness through counterfactual explanations. We start with revisiting (and generalizing) existing notions and introducing new, more refined notions of subgroup fairness. We aim to (a) formulate different aspects of the difficulty of individuals in certain subgroups to achieve recourse, i.e. receive the desired outcome, either at the micro level, considering members of the subgroup individually, or at the macro level, considering the subgroup as a whole, and (b) introduce notions of subgroup fairness that are robust, if not totally oblivious, to the cost of achieving recourse. We accompany these notions with an efficient, model-agnostic, highly parameterizable, and explainable framework for evaluating subgroup fairness. We demonstrate the advantages, the wide applicability, and the efficiency of our approach through a thorough experimental evaluation of different benchmark d
    
[^89]: 复杂数据集的底层缩放定律和普适统计结构

    The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets. (arXiv:2306.14975v1 [cs.LG])

    [http://arxiv.org/abs/2306.14975](http://arxiv.org/abs/2306.14975)

    本文研究了复杂数据集中的底层缩放定律和普适统计结构。通过将数据类比为物理系统，并应用统计物理学和随机矩阵理论的方法，揭示了特征-特征协方差矩阵的局部和全局特征值统计量的规律。研究发现，在无关随机数据和真实数据之间存在显著差异，并且可以通过引入长程相关性完全恢复缩放行为。同时，生成的数据和真实世界数据都属于混沌系统，并在较小的数据集大小上即可体现随机矩阵理论的统计行为。

    

    我们研究了在真实世界的复杂数据集和人工生成的数据集中都出现的普遍特征。我们将数据类比为物理系统，并利用统计物理学和随机矩阵理论的工具揭示其底层结构。我们重点分析了特征-特征协方差矩阵，分析了其局部和全局特征值统计量。我们的主要观察结果是：(i) 大部分特征值呈现的幂律缩放在无相关随机数据和真实数据之间存在显著差异，(ii) 通过简单地引入长程相关性，可以完全恢复这种缩放行为到合成数据中，(iii) 从随机矩阵理论的角度看，生成的数据集和真实世界数据集属于同一个普适性类别，都是混沌系统而非可积系统，(iv) 预期的随机矩阵理论统计行为在相对较小的数据集大小上就已经在经验协方差矩阵中得到体现。

    We study universal traits which emerge both in real-world complex datasets, as well as in artificially generated ones. Our approach is to analogize data to a physical system and employ tools from statistical physics and Random Matrix Theory (RMT) to reveal their underlying structure. We focus on the feature-feature covariance matrix, analyzing both its local and global eigenvalue statistics. Our main observations are: (i) The power-law scalings that the bulk of its eigenvalues exhibit are vastly different for uncorrelated random data compared to real-world data, (ii) this scaling behavior can be completely recovered by introducing long range correlations in a simple way to the synthetic data, (iii) both generated and real-world datasets lie in the same universality class from the RMT perspective, as chaotic rather than integrable systems, (iv) the expected RMT statistical behavior already manifests for empirical covariance matrices at dataset sizes significantly smaller than those conv
    
[^90]: SIMF: 自动驾驶的语义感知交互式运动预测

    SIMF: Semantics-aware Interactive Motion Forecasting for Autonomous Driving. (arXiv:2306.14941v1 [cs.CV])

    [http://arxiv.org/abs/2306.14941](http://arxiv.org/abs/2306.14941)

    本文提出了一种名为SIMF的方法，用于自动驾驶车辆中语义感知的交互式运动预测。该方法通过实现基于语义的行为体选择和注意力机制提取全局编码，能够捕捉空间信息和语义信息，并优选相关的行为体进行运动预测。

    

    自动驾驶车辆需要对周围多个行为体（行人和车辆）进行运动预测，以做出最优导航决策。现有的方法主要关注如何利用这些行为体的位置和速度，并未能捕捉到场景中的语义信息。此外，为了减少与场景中行为体数量增加相关的计算复杂度，一些方法利用欧氏距离来剪枝远离的行为体。然而，仅仅基于距离的度量无法选择相关的行为体并准确进行预测。为了解决这些问题，我们提出了一种称为SIMF的方法，用于捕捉空间信息以及语义信息，并优选相关的行为体进行运动预测。具体而言，我们通过实现一种基于语义的行为体选择方法，将其通过注意力机制传递，以提取全局编码。

    Autonomous vehicles require motion forecasting of their surrounding multi-agents (pedestrians and vehicles) to make optimal decisions for navigation. The existing methods focus on techniques to utilize the positions and velocities of these agents and fail to capture semantic information from the scene. Moreover, to mitigate the increase in computational complexity associated with the number of agents in the scene, some works leverage Euclidean distance to prune far-away agents. However, distance-based metric alone is insufficient to select relevant agents and accurately perform their predictions. To resolve these issues, we propose Semantics-aware Interactive Motion Forecasting (SIMF) method to capture semantics along with spatial information, and optimally select relevant agents for motion prediction. Specifically, we achieve this by implementing a semantic-aware selection of relevant agents from the scene and passing them through an attention mechanism to extract global encodings. Th
    
[^91]: 嵌入融合的艺术：优化仇恨言论检测

    The Art of Embedding Fusion: Optimizing Hate Speech Detection. (arXiv:2306.14939v1 [cs.CL])

    [http://arxiv.org/abs/2306.14939](http://arxiv.org/abs/2306.14939)

    这项工作研究了优化仇恨言论检测的方法。研究发现，尽管嵌入的组合会略微提高性能，但计算成本很高，并且组合方式对结果的影响较小。

    

    仇恨言论检测是一项具有挑战性的自然语言处理任务，需要捕捉语言和语境细微差别。预训练语言模型（PLMs）提供了丰富的文本语义表示，可以改进这个任务。然而，对于有效地组合PLMs的表示和利用它们的互补优势的方法还知之甚少。在这项工作中，我们揭示了几种PLMs组合技术的方式，并全面分析了它们的有效性。我们的研究结果表明，组合嵌入可以略微改善性能，但计算成本较高，组合方式对最终结果的影响较小。我们还在https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection上公开了我们的代码库。

    Hate speech detection is a challenging natural language processing task that requires capturing linguistic and contextual nuances. Pre-trained language models (PLMs) offer rich semantic representations of text that can improve this task. However there is still limited knowledge about ways to effectively combine representations across PLMs and leverage their complementary strengths. In this work, we shed light on various combination techniques for several PLMs and comprehensively analyze their effectiveness. Our findings show that combining embeddings leads to slight improvements but at a high computational cost and the choice of combination has marginal effect on the final outcome. We also make our codebase public at https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection .
    
[^92]: 将双向长短期记忆网络与子词嵌入相结合用于作者归属的研究

    Integrating Bidirectional Long Short-Term Memory with Subword Embedding for Authorship Attribution. (arXiv:2306.14933v1 [cs.CL])

    [http://arxiv.org/abs/2306.14933](http://arxiv.org/abs/2306.14933)

    这项研究提出了一种将双向长短期记忆网络与子词嵌入相结合的方法，用于解决作者归属问题。该方法能够在处理文本中的隐含词问题的同时保留词的顺序上下文。

    

    揭示给定文本文档的作者身份是作者归属问题。深度学习方法已成功地使用多样的基于词的风格标记来处理作者归属的内在问题。然而，基于词的作者归属系统的性能受到训练语料库词汇的限制。文献推荐了以字符为基础的风格标记作为克服隐含词问题的替代方法。然而，基于字符的方法经常无法捕捉文本中的词的顺序关系，这是进一步改善的难题。本文讨论的问题是是否可以解决文本文档中隐含词的歧义性，同时保留词的顺序上下文。因此，提出了一种基于双向长短期记忆网络（BLSTM）与二维卷积神经网络（CNN）相结合的方法来捕捉顺序上下文。

    The problem of unveiling the author of a given text document from multiple candidate authors is called authorship attribution. Manifold word-based stylistic markers have been successfully used in deep learning methods to deal with the intrinsic problem of authorship attribution. Unfortunately, the performance of word-based authorship attribution systems is limited by the vocabulary of the training corpus. Literature has recommended character-based stylistic markers as an alternative to overcome the hidden word problem. However, character-based methods often fail to capture the sequential relationship of words in texts which is a chasm for further improvement. The question addressed in this paper is whether it is possible to address the ambiguity of hidden words in text documents while preserving the sequential context of words. Consequently, a method based on bidirectional long short-term memory (BLSTM) with a 2-dimensional convolutional neural network (CNN) is proposed to capture sequ
    
[^93]: GloptiNets：具有证明的可扩展非凸优化

    GloptiNets: Scalable Non-Convex Optimization with Certificates. (arXiv:2306.14932v1 [math.OC])

    [http://arxiv.org/abs/2306.14932](http://arxiv.org/abs/2306.14932)

    GloptiNets是一种新方法，可以处理具有证明的非凸优化问题，通过利用目标函数的正则性和并行计算的优势，取得了比现有方法更好的性能。

    

    我们提出了一种处理超立方体或环面上的光滑函数的具有证明的非凸优化的新方法。与依赖代数性质的传统方法不同，我们的算法利用了目标函数的正则性，该正则性在其傅里叶谱的衰减中体现出来。通过定义一个易处理的模型族，我们既能够获得精确的证明，又能够利用用于优化神经网络的先进和强大的计算技术。通过与GPU的并行计算，我们自然地增强了我们方法的可扩展性。我们的方法在应用于具有数千个系数但维度适中的多项式的情况下，优于基于Lasserre层次的证明最先进的优化方法，解决了竞争者难以处理的问题。

    We present a novel approach to non-convex optimization with certificates, which handles smooth functions on the hypercube or on the torus. Unlike traditional methods that rely on algebraic properties, our algorithm exploits the regularity of the target function intrinsic in the decay of its Fourier spectrum. By defining a tractable family of models, we allow at the same time to obtain precise certificates and to leverage the advanced and powerful computational techniques developed to optimize neural networks. In this way the scalability of our approach is naturally enhanced by parallel computing with GPUs. Our approach, when applied to the case of polynomials of moderate dimensions but with thousands of coefficients, outperforms the state-of-the-art optimization methods with certificates, as the ones based on Lasserre's hierarchy, addressing problems intractable for the competitors.
    
[^94]: LLM辅助内容分析：利用大型语言模型支持演绎编码

    LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding. (arXiv:2306.14924v1 [cs.CL])

    [http://arxiv.org/abs/2306.14924](http://arxiv.org/abs/2306.14924)

    本研究探索了使用大型语言模型（LLMs）来减少演绎编码所需时间的方法，同时保留传统内容分析的灵活性。通过一个案例研究和经验基准测试，证明了在不同演绎编码任务上，GPT-3.5在LLM辅助内容分析（LACA）中的有效性。

    

    演绎编码是一种广泛使用的定性研究方法，用于确定文档中主题的普遍性。尽管有用，演绎编码通常是繁琐且耗时的，因为它要求研究人员阅读、解释并可靠地对大量非结构化文本进行分类。大型语言模型（LLMs），如ChatGPT，是一类快速发展的人工智能工具，可以执行各种自然语言处理和推理任务。在这项研究中，我们探索了使用LLMs来减少演绎编码所需的时间，同时保留传统内容分析的灵活性。我们概述了所提出的方法，称为LLM辅助内容分析（LACA），并使用GPT-3.5在一个公开可用的演绎编码数据集上进行了深入案例研究。此外，我们还进行了一个经验基准测试，使用LACA在4个公开可用的数据集上评估GPT-3.5在不同演绎编码任务上的表现。

    Deductive coding is a widely used qualitative research method for determining the prevalence of themes across documents. While useful, deductive coding is often burdensome and time consuming since it requires researchers to read, interpret, and reliably categorize a large body of unstructured text documents. Large language models (LLMs), like ChatGPT, are a class of quickly evolving AI tools that can perform a range of natural language processing and reasoning tasks. In this study, we explore the use of LLMs to reduce the time it takes for deductive coding while retaining the flexibility of a traditional content analysis. We outline the proposed approach, called LLM-assisted content analysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a publicly available deductive coding data set. Additionally, we conduct an empirical benchmark using LACA on 4 publicly available data sets to assess the broader question of how well GPT-3.5 performs across a range of deductive co
    
[^95]: 基于余弦相似度的离群分布检测方法

    A Cosine Similarity-based Method for Out-of-Distribution Detection. (arXiv:2306.14920v1 [cs.LG])

    [http://arxiv.org/abs/2306.14920](http://arxiv.org/abs/2306.14920)

    本文提出了一种基于余弦相似度的离群分布检测方法，通过计算测试特征和内部数据特征之间的余弦相似度来判断离群数据。实验证明，该方法在离群数据检测方面优于现有方法。

    

    检测离群数据的能力是实际机器学习应用中的关键要素。在这项工作中，我们展示了测试特征和典型内部数据特征之间的余弦相似度是检测离群数据的良好指标。我们提出了类典型匹配（CTM），一种使用余弦相似度评分函数的事后离群数据检测算法。在多个基准测试上进行的广泛实验表明，CTM优于现有的事后离群数据检测方法。

    The ability to detect OOD data is a crucial aspect of practical machine learning applications. In this work, we show that cosine similarity between the test feature and the typical ID feature is a good indicator of OOD data. We propose Class Typical Matching (CTM), a post hoc OOD detection algorithm that uses a cosine similarity scoring function. Extensive experiments on multiple benchmarks show that CTM outperforms existing post hoc OOD detection methods.
    
[^96]: 超越化学语言: 一种增强分子性质预测的多模态方法

    Beyond Chemical Language: A Multimodal Approach to Enhance Molecular Property Prediction. (arXiv:2306.14919v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.14919](http://arxiv.org/abs/2306.14919)

    一种多模态方法用于预测分子性质，结合了化学语言表示和物理化学特征，通过因果特征选择和分子嵌入向量空间的集成来实现准确预测。相比现有算法，该方法在生物降解性和PFAS毒性评估等复杂任务中表现出卓越性能。

    

    我们提出了一种新颖的多模态语言模型方法，通过将化学语言表示与物理化学特征相结合，用于预测分子性质。我们的方法MULTIMODAL-MOLFORMER采用因果多阶段特征选择方法，根据其对特定目标性质的直接因果效应来识别物理化学特征。然后，这些因果特征与由MOLFORMER生成的分子嵌入向量空间相结合。特别是，我们使用Mordred描述符作为物理化学特征，并识别目标性质的马尔可夫毯，理论上包含最相关的特征以实现准确预测。我们的结果表明，与现有的业界领先算法（包括基于化学语言的MOLFORMER和图神经网络）相比，我们提出的方法在预测生物降解性和PFAS毒性评估等复杂任务中表现出卓越的性能。

    We present a novel multimodal language model approach for predicting molecular properties by combining chemical language representation with physicochemical features. Our approach, MULTIMODAL-MOLFORMER, utilizes a causal multistage feature selection method that identifies physicochemical features based on their direct causal effect on a specific target property. These causal features are then integrated with the vector space generated by molecular embeddings from MOLFORMER. In particular, we employ Mordred descriptors as physicochemical features and identify the Markov blanket of the target property, which theoretically contains the most relevant features for accurate prediction. Our results demonstrate a superior performance of our proposed approach compared to existing state-of-the-art algorithms, including the chemical language-based MOLFORMER and graph neural networks, in predicting complex tasks such as biodegradability and PFAS toxicity estimation. Moreover, we demonstrate the ef
    
[^97]: 利用自然语言处理进行课堂讨论的自动评估

    Utilizing Natural Language Processing for Automated Assessment of Classroom Discussion. (arXiv:2306.14918v1 [cs.CL])

    [http://arxiv.org/abs/2306.14918](http://arxiv.org/abs/2306.14918)

    本研究利用自然语言处理技术，通过自动生成细化标准得分，实现对课堂讨论质量的自动评估。实验结果令人鼓舞，同时指出标准仍有改进空间，并发现不同的NLP方法对不同的标准更有效。

    

    严格而互动的课堂讨论对于学习至关重要，同时也是大多数教学干预的核心组成部分。然而，对讨论质量进行规模化的正式评估对于大多数研究者来说是昂贵且不可行的。在这项工作中，我们尝试了各种现代自然语言处理（NLP）技术，以自动生成课堂文本讨论质量的细化标准得分。具体而言，我们使用了包含超过18000轮次、注释有详细的教学分析运动（ATM）代码的90个课堂讨论记录数据集，并聚焦于四个教学质量评估（IQA）标准。尽管数据量有限，我们的工作在一些标准上取得了令人鼓舞的结果，同时也暗示其他标准仍有改进空间。我们还发现，某些NLP方法在某些标准上效果更好。

    Rigorous and interactive class discussions that support students to engage in high-level thinking and reasoning are essential to learning and are a central component of most teaching interventions. However, formally assessing discussion quality 'at scale' is expensive and infeasible for most researchers. In this work, we experimented with various modern natural language processing (NLP) techniques to automatically generate rubric scores for individual dimensions of classroom text discussion quality. Specifically, we worked on a dataset of 90 classroom discussion transcripts consisting of over 18000 turns annotated with fine-grained Analyzing Teaching Moves (ATM) codes and focused on four Instructional Quality Assessment (IQA) rubrics. Despite the limited amount of data, our work shows encouraging results in some of the rubrics while suggesting that there is room for improvement in the others. We also found that certain NLP approaches work better for certain rubrics.
    
[^98]: 迈向教育问题生成的丰富可控性

    Towards Enriched Controllability for Educational Question Generation. (arXiv:2306.14917v1 [cs.CL])

    [http://arxiv.org/abs/2306.14917](http://arxiv.org/abs/2306.14917)

    本研究旨在通过引入新的引导属性（问题明确性）来丰富教育问题生成的可控性。我们提出了通过控制生成明确和隐含wh-问题的方法。研究展示了通过问题明确性和叙事要素同时控制问题生成的初步证据。

    

    生成问题（QG）是自然语言处理（NLP）中的一个任务，它涉及根据输入（通常由文本和目标答案组成）自动生成问题。近期关于QG的研究旨在控制生成问题的类型，以满足教育需求。教育QG中可控性的一个显著例子是生成涉及特定叙事要素的问题，例如因果关系、结果解决或预测。本研究旨在通过引入新的引导属性（问题明确性）来丰富QG的可控性。我们提议通过控制从适合儿童的故事中生成明确和隐含的wh-问题。我们展示了仅通过问题明确性以及与另一个目标属性（问题的叙事要素）同时控制QG的初步证据。代码公开可在github.com/bernardoleite/question-generation-control获取。

    Question Generation (QG) is a task within Natural Language Processing (NLP) that involves automatically generating questions given an input, typically composed of a text and a target answer. Recent work on QG aims to control the type of generated questions so that they meet educational needs. A remarkable example of controllability in educational QG is the generation of questions underlying certain narrative elements, e.g., causal relationship, outcome resolution, or prediction. This study aims to enrich controllability in QG by introducing a new guidance attribute: question explicitness. We propose to control the generation of explicit and implicit wh-questions from children-friendly stories. We show preliminary evidence of controlling QG via question explicitness alone and simultaneously with another target attribute: the question's narrative element. The code is publicly available at github.com/bernardoleite/question-generation-control.
    
[^99]: 分子的不确定性估计: 期望和方法

    Uncertainty Estimation for Molecules: Desiderata and Methods. (arXiv:2306.14916v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.14916](http://arxiv.org/abs/2306.14916)

    该论文研究了分子力场中不确定性估计的要求和方法，并发现先前的研究方法无法满足所有标准。为了解决这个问题，提出了一种基于局部神经核的高斯过程模型（LNK）。

    

    图神经网络（GNNs）作为量子力学计算的有希望替代方法，在分子动力学（MD）轨迹集合上建立了前所未有的低误差。由于它们快速的推理时间，它们承诺加速计算化学应用。然而，尽管在分布（ID）误差上可低，但这些GNNs在分布外（OOD）样本上可能完全错误。不确定性估计（UE）可以在这种情况下帮助通过传达模型对其预测的确定性。在这里，我们着眼于这个问题，并确定了分子力场不确定性估计中的六个关键要求，三个是“物理信息”方面的，三个是“应用焦点”方面的。为了概述该领域，我们调查了现有的UE方法，并分析它们如何符合这些要求。通过我们的分析，我们得出结论，先前的研究都无法满足所有标准。为了填补这个空白，我们提出了基于局部神经核（LNK）的高斯过程（GP）模型。

    Graph Neural Networks (GNNs) are promising surrogates for quantum mechanical calculations as they establish unprecedented low errors on collections of molecular dynamics (MD) trajectories. Thanks to their fast inference times they promise to accelerate computational chemistry applications. Unfortunately, despite low in-distribution (ID) errors, such GNNs might be horribly wrong for out-of-distribution (OOD) samples. Uncertainty estimation (UE) may aid in such situations by communicating the model's certainty about its prediction. Here, we take a closer look at the problem and identify six key desiderata for UE in molecular force fields, three 'physics-informed' and three 'application-focused' ones. To overview the field, we survey existing methods from the field of UE and analyze how they fit to the set desiderata. By our analysis, we conclude that none of the previous works satisfies all criteria. To fill this gap, we propose Localized Neural Kernel (LNK) a Gaussian Process (GP)-based
    
[^100]: 机器学习语言模型时代中人标记数据的重要性

    The Importance of Human-Labeled Data in the Era of LLMs. (arXiv:2306.14910v1 [cs.CL])

    [http://arxiv.org/abs/2306.14910](http://arxiv.org/abs/2306.14910)

    本文论述了在LLMs时代，人标记数据仍然具有重要性的论据和支持。

    

    大型语言模型（LLMs）的出现在定制机器学习模型的开发方面带来了一场革命，并引发了关于重新定义数据要求的讨论。LLMs的训练和实施所带来的自动化引发了对人工标记干预可能不再具有与监督学习时代相同重要性的讨论和期望。本文提出了有力的论据，支持在LLMs时代人标记数据的持续重要性。

    The advent of large language models (LLMs) has brought about a revolution in the development of tailored machine learning models and sparked debates on redefining data requirements. The automation facilitated by the training and implementation of LLMs has led to discussions and aspirations that human-level labeling interventions may no longer hold the same level of importance as in the era of supervised learning. This paper presents compelling arguments supporting the ongoing relevance of human-labeled data in the era of LLMs.
    
[^101]: 通过潜在空间能量建模和分布逐步变换进行分子设计

    Molecule Design by Latent Space Energy-Based Modeling and Gradual Distribution Shifting. (arXiv:2306.14902v1 [q-bio.BM])

    [http://arxiv.org/abs/2306.14902](http://arxiv.org/abs/2306.14902)

    本文提出了一种概率生成模型来设计具有所需化学和生物性质的分子，并通过逐步分布变换采样算法搜索具有所需性质的分子。实验证明该方法在分子设计任务上表现出很强的性能。

    

    为了药物研发，生成具有所需化学和生物性质（如高药物样性，高亲和力）的分子至关重要。本文提出了一个概率生成模型，用于捕捉分子和其性质的联合分布。我们的模型在潜在空间中基于能量建模（EBM）。在潜在向量的条件下，分子及其性质被分别建模为分子生成模型和性质回归模型。为了搜索具有所需性质的分子，我们提出了逐步分布变换采样（SGDS）算法，使得在最初学习现有分子及其性质的训练数据后，该算法逐渐将模型分布转移到支持所需性质值的分子区域。我们的实验结果表明，我们的方法在各种分子设计任务上表现出很强的性能。

    Generation of molecules with desired chemical and biological properties such as high drug-likeness, high binding affinity to target proteins, is critical for drug discovery. In this paper, we propose a probabilistic generative model to capture the joint distribution of molecules and their properties. Our model assumes an energy-based model (EBM) in the latent space. Conditional on the latent vector, the molecule and its properties are modeled by a molecule generation model and a property regression model respectively. To search for molecules with desired properties, we propose a sampling with gradual distribution shifting (SGDS) algorithm, so that after learning the model initially on the training data of existing molecules and their properties, the proposed algorithm gradually shifts the model distribution towards the region supported by molecules with desired values of properties. Our experiments show that our method achieves very strong performances on various molecule design tasks.
    
[^102]: 受非晶性中间层影响的异质界面声子动态行为

    Phonon dynamic behaviors induced by amorphous interlayer at heterointerfaces. (arXiv:2306.14901v1 [physics.app-ph])

    [http://arxiv.org/abs/2306.14901](http://arxiv.org/abs/2306.14901)

    本研究使用声子波包模拟研究发现，非晶性中间层显著阻碍声子在GaN/AlN界面上的传输，导致声子模式转换和高频声子部分穿过非晶性中间层，这为界面热传导提供了额外的热传输通道。

    

    界面阻碍异质结构中的热流，并且界面热阻已成为电子器件热耗散的关键问题。为了探索界面热阻的机制，本研究使用声子波包模拟研究了具有非晶性中间层的GaN/AlN界面上通过的声子的动态行为。发现非晶性中间层显著阻碍声子在界面上的传输，并导致显著的声子模式转换，如LA→TA、TA→LA和LA→TO转换。然而，由于模式转换和非弹性散射，我们发现部分高频TA声子，高于截止频率且无法穿过理想尖锐界面，可以部分穿过非晶性中间层，这引入了额外的热传输通道并对界面热传导起到积极作用

    Interface impedes heat flow in heterostructures and the interfacial thermal resistance (ITR) has become a critical issue for thermal dissipation in electronic devices. To explore the mechanism leading to the ITR, in this work, the dynamic behaviors of phonons passing through the GaN/AlN interface with an amorphous interlayer is investigated by using phonon wave packet simulation. It is found the amorphous interlayer significantly impedes phonon transport across the interface, and leads to remarkable phonon mode conversions, such as LA$\rightarrow$TA, TA$\rightarrow$LA, and LA$\rightarrow$TO conversion. However, due to mode conversion and inelastic scattering, we found a portion of high-frequency TA phonons, which are higher than the cut-off frequency and cannot transmit across the ideal sharp interface, can partially transmit across the amorphous interlayer, which introduces additional thermal transport channels through the interface and has positive effect on interfacial thermal condu
    
[^103]: InterCode:标准化和基准测试具有执行反馈的交互编码

    InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback. (arXiv:2306.14898v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.14898](http://arxiv.org/abs/2306.14898)

    InterCode是一个交互式编码的标准化和基准测试框架，它使用执行反馈作为观察，并提供了安全可重现的执行环境，可以用于开发新的交互式代码生成方法。

    

    人类以基本交互方式编写代码，并依赖于持续的执行反馈来纠正错误，解决歧义和分解任务。尽管最近的LLM展示出了有希望的编码能力，但目前的编码基准主要考虑静态的指令到代码序列转换过程，这可能导致错误传播和生成的代码与其最终执行环境之间的脱节。为了填补这一差距，我们引入了InterCode，这是一个轻量级、灵活且易于使用的交互式编码框架，作为一个标准强化学习（RL）环境，使用代码作为行动，执行反馈作为观察。我们的框架与语言和平台无关，使用独立的Docker环境提供安全和可重现的执行，并且与传统的seq2seq编码方法开箱即用，同时还可以开发新的交互式代码生成方法。我们使用InterCode创建...

    Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create t
    
[^104]: PMaF:用于主要矩阵特征的深度声明性层

    PMaF: Deep Declarative Layers for Principal Matrix Features. (arXiv:2306.14759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14759](http://arxiv.org/abs/2306.14759)

    本文介绍了PMaF框架，使用声明性的深度层来学习主要矩阵特征，通过迭代优化解决问题并应用双层优化框架进行反向传播，从而提高效率。实验证明了该框架优于现有的基线模型。

    

    本文探讨了两种不同iable的深度声明性层，即球上的最小二乘法(LESS)和隐式特征值分解(IED)，用于学习主要矩阵特征(PMaF)。这可以用一个低维向量表示包含来自高维矩阵的主要信息的数据特征。我们首先通过前向传递的迭代优化来解决问题，然后在一个双层优化框架下反向传播解决方案进行隐式梯度。具体来说，我们研究了自适应下降步骤和反向线搜索方法以及切线空间中的下降衰减，以提高LESS的前向通过程的效率。与此同时，在LESS和IED的反向传递中使用了优化的数据结构，大大降低了计算复杂度。在经验上，通过比较解决方案的最优性和计算要求，我们证明了我们的层优于现成的基线模型。

    We explore two differentiable deep declarative layers, namely least squares on sphere (LESS) and implicit eigen decomposition (IED), for learning the principal matrix features (PMaF). This can be used to represent data features with a low-dimension vector containing dominant information from a high-dimension matrix. We first solve the problems with iterative optimization in the forward pass and then backpropagate the solution for implicit gradients under a bi-level optimization framework. Particularly, adaptive descent steps with the backtracking line search method and descent decay in the tangent space are studied to improve the forward pass efficiency of LESS. Meanwhile, exploited data structures are used to greatly reduce the computational complexity in the backward pass of LESS and IED. Empirically, we demonstrate the superiority of our layers over the off-the-shelf baselines by comparing the solution optimality and computational requirements.
    
[^105]: 通用框架下适应性约束下的顺序决策问题研究

    A General Framework for Sequential Decision-Making under Adaptivity Constraints. (arXiv:2306.14468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14468](http://arxiv.org/abs/2306.14468)

    本论文提出了一个通用框架，研究了在适应性约束下的顺序决策问题。具体地，我们提供了Eluder Condition类，并针对稀缺策略切换和批次学习约束分别提供了相应的算法。此工作是第一个考虑通用函数类别下稀缺策略切换和批次学习的工作，涵盖了之前研究中的大部分模型。

    

    我们在研究通用的顺序决策问题下对两个适应性约束进行了首次探索：策略切换稀缺和批次学习。首先，我们提供了一个称为Eluder Condition类的通用类别，其中包括了广泛的强化学习类别。然后，对于策略切换稀缺约束，我们提供了一个通用算法，在EC类别上实现了大约$ \widetilde{\mathcal{O}}(\log K)$的切换代价和$\widetilde{\mathcal{O}}(\sqrt{K})$的后悔代价。对于批次学习约束，我们提供了一个算法，在$B$个批次的情况下，提供了大约$\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$的后悔代价。这篇论文是第一篇考虑通用函数类别下稀缺策略切换和批次学习的工作，涵盖了之前研究中几乎所有的模型，如表格MDP (Bai et al. 2019; Zhang et al. 2020)、线性MDP (Wang et al. 2021; Gao et al. 2021)、低Eluder维度MDP (Kong et al. 2021; Gao et al. 2021)、广义线性函数类别等。

    We take the first step in studying general sequential decision-making under two adaptivity constraints: rare policy switch and batch learning. First, we provide a general class called the Eluder Condition class, which includes a wide range of reinforcement learning classes. Then, for the rare policy switch constraint, we provide a generic algorithm to achieve a $\widetilde{\mathcal{O}}(\log K) $ switching cost with a $\widetilde{\mathcal{O}}(\sqrt{K})$ regret on the EC class. For the batch learning constraint, we provide an algorithm that provides a $\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$ regret with the number of batches $B.$ This paper is the first work considering rare policy switch and batch learning under general function classes, which covers nearly all the models studied in the previous works such as tabular MDP (Bai et al. 2019; Zhang et al. 2020), linear MDP (Wang et al. 2021; Gao et al. 2021), low eluder dimension MDP (Kong et al. 2021; Gao et al. 2021), generalized linear fu
    
[^106]: DragDiffusion: 利用扩散模型进行交互式点基图像编辑

    DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.14435](http://arxiv.org/abs/2306.14435)

    DragDiffusion是一个利用扩散模型进行交互式点基图像编辑的方法，通过优化扩散潜在实现精确的空间控制，以提高实际场景中的应用性。

    

    精确可控的图像编辑是一个具有挑战性的任务，引起了广泛的关注。最近，DragGAN实现了一个交互式的基于点的图像编辑框架，并以像素级精度实现了令人印象深刻的编辑结果。然而，由于该方法基于生成对抗网络（GAN），其通用性受限于预先训练的GAN模型的容量。在这项工作中，我们将这样的编辑框架扩展到扩散模型，并提出了DragDiffusion。通过利用大规模预训练的扩散模型，我们极大地提高了交互式基于点的编辑在实际场景中的适用性。虽然大多数现有的基于扩散的图像编辑方法基于文本嵌入，DragDiffusion优化扩散潜在来实现精确的空间控制。尽管扩散模型以迭代方式生成图像，但我们凭经验表明，在一个单独的步骤中优化扩散潜在已足以生成连贯的结果，从而使得该方法成为可能。

    Precise and controllable image editing is a challenging task that has attracted significant attention. Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive editing results with pixel-level precision. However, since this method is based on generative adversarial networks (GAN), its generality is upper-bounded by the capacity of the pre-trained GAN models. In this work, we extend such an editing framework to diffusion models and propose DragDiffusion. By leveraging large-scale pretrained diffusion models, we greatly improve the applicability of interactive point-based editing in real world scenarios. While most existing diffusion-based image editing methods work on text embeddings, DragDiffusion optimizes the diffusion latent to achieve precise spatial control. Although diffusion models generate images in an iterative manner, we empirically show that optimizing diffusion latent at one single step suffices to generate coherent results, enabl
    
[^107]: 特征对抗蒸馏用于点云分类

    Feature Adversarial Distillation for Point Cloud Classification. (arXiv:2306.14221v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.14221](http://arxiv.org/abs/2306.14221)

    本文提出了一种特征对抗蒸馏方法（FAD），用于解决点云分类中知识传递的信息损失问题。在实验证明，该方法在模型压缩的同时保持了竞争性能。

    

    由于点云的不规则和无序的几何结构，传统的知识蒸馏技术在直接应用于点云任务时丢失了很多信息。本文提出了一种特征对抗蒸馏（Feature Adversarial Distillation，简称FAD）方法，这是一种在点云蒸馏中使用的通用对抗损失函数，可减少知识传递过程中的信息损失。在特征提取阶段，利用教师提取的特征作为鉴别器，在训练阶段学生不断生成新特征。学生的特征通过攻击教师的反馈得到得分，以判断学生是否学习了知识。在ModelNet40和ScanObjectNN数据集上进行的标准点云分类实验中，我们的方法在40倍模型压缩的同时降低了知识传递中的信息损失，并保持了竞争性能。

    Due to the point cloud's irregular and unordered geometry structure, conventional knowledge distillation technology lost a lot of information when directly used on point cloud tasks. In this paper, we propose Feature Adversarial Distillation (FAD) method, a generic adversarial loss function in point cloud distillation, to reduce loss during knowledge transfer. In the feature extraction stage, the features extracted by the teacher are used as the discriminator, and the students continuously generate new features in the training stage. The feature of the student is obtained by attacking the feedback from the teacher and getting a score to judge whether the student has learned the knowledge well or not. In experiments on standard point cloud classification on ModelNet40 and ScanObjectNN datasets, our method reduced the information loss of knowledge transfer in distillation in 40x model compression while maintaining competitive performance.
    
[^108]: QNNRepair：量化神经网络修复的方法

    QNNRepair: Quantized Neural Network Repair. (arXiv:2306.13793v1 [cs.LG])

    [http://arxiv.org/abs/2306.13793](http://arxiv.org/abs/2306.13793)

    QNNRepair 是一种用于修复量化神经网络的方法，通过解决神经元权重参数以修复在神经网络量化过程中导致性能下降的神经元，从而在不影响通过测试的性能的同时，提高在失败测试上的性能。

    

    本文提出了QNNRepair，这是文献中第一个修正量化神经网络（QNNs）的方法。其旨在提高在量化之后的神经网络模型的准确性，其接受全精度和权重量化的神经网络以及一个修复数据集。QNNRepair将修复问题转化为线性规划，通过解决神经元权重参数以修复在神经网络量化过程中导致性能下降的神经元，从而在不影响通过测试的性能的同时，提高在失败测试上的性能。

    We present QNNRepair, the first method in the literature for repairing quantized neural networks (QNNs). QNNRepair aims to improve the accuracy of a neural network model after quantization. It accepts the full-precision and weight-quantized neural networks and a repair dataset of passing and failing tests. At first, QNNRepair applies a software fault localization method to identify the neurons that cause performance degradation during neural network quantization. Then, it formulates the repair problem into a linear programming problem of solving neuron weights parameters, which corrects the QNN's performance on failing tests while not compromising its performance on passing tests. We evaluate QNNRepair with widely used neural network architectures such as MobileNetV2, ResNet, and VGGNet on popular datasets, including high-resolution images. We also compare QNNRepair with the state-of-the-art data-free quantization method SQuant. According to the experiment results, we conclude that QNN
    
[^109]: 注意力机制中的边缘最大化

    Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])

    [http://arxiv.org/abs/2306.13596](http://arxiv.org/abs/2306.13596)

    这篇论文证明了，在softmax-attention模型中，通过在p或等价的W上运行梯度下降，可以收敛到一个最大边缘解，这将局部最优的标记与非最优的标记分隔开。这明确地将注意力机制形式化为标记分离机制。

    

    注意力机制是Transformer架构的核心组件，也是大型语言模型取得惊人成功的原因之一。然而，注意力机制背后的理论原则尚不清楚，特别是它的非凸优化动力学。本文探讨了开创性的softmax-attention模型$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$，其中$\boldsymbol{X}$是标记序列，$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$是可调参数。我们证明了在$\boldsymbol{p}$或等价的$\boldsymbol{W}$上运行梯度下降会沿着方向收敛到分隔“局部最优”标记和“非最优”标记的最大边缘解。这明确地形式化了注意力作为一种标记分离机制。值得注意的是，我们的结果适用于一般数据，并使用嵌入$\boldsymbol{Xv}$和$\texttt{softmax}(\boldsymbol{XWp})$精细地表征标记的“最优性”。

    Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
    
[^110]: 轨迹采样下的神经网络集合小批量训练

    Minibatch training of neural network ensembles via trajectory sampling. (arXiv:2306.13442v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2306.13442](http://arxiv.org/abs/2306.13442)

    本文介绍了一种轨迹采样下的神经网络集合小批量训练方法，通过对MNIST数据集实验，发现相较于传统方法，该方法提高了两个数量级的计算效率，同时还提高了推断准确性。

    

    大多数迭代神经网络训练方法使用数据的小随机子集（或小批量）的损失函数估计来更新参数，在训练时间与庞大的训练数据集大小之间解耦，提高了效率。我们展示了一种小批量方法可以以高效的方式通过轨迹方法训练神经网络集合(NNEs)。我们通过训练NNE来分类MNIST数据集中的图像来说明这种方法。这种方法可以提高训练时间，使其能够缩放为数据集大小与平均小批量大小之比，对于MNIST来说，计算效率通常提高两个数量级。我们强调使用较长的轨迹来表示NNE的优点，既可以提高推断的准确性，又可以在小批量更新所需的样本方面降低更新成本。

    Most iterative neural network training methods use estimates of the loss function over small random subsets (or minibatches) of the data to update the parameters, which aid in decoupling the training time from the (often very large) size of the training datasets. Here, we show that a minibatch approach can also be used to train neural network ensembles (NNEs) via trajectory methods in a highly efficent manner. We illustrate this approach by training NNEs to classify images in the MNIST datasets. This method gives an improvement to the training times, allowing it to scale as the ratio of the size of the dataset to that of the average minibatch size which, in the case of MNIST, gives a computational improvement typically of two orders of magnitude. We highlight the advantage of using longer trajectories to represent NNEs, both for improved accuracy in inference and reduced update cost in terms of the samples needed in minibatch updates.
    
[^111]: FuXi: 一个15天全球天气预报级联机器学习系统

    FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])

    [http://arxiv.org/abs/2306.12873](http://arxiv.org/abs/2306.12873)

    逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。

    

    近年来，随着机器学习模型在天气预报中的快速发展，最先进的机器学习模型在0.25度空间分辨率下的10天天气预报中已经表现出比欧洲中期天气预报中心(ECMWF)的高分辨率预报(HRES)更优越的性能。然而，挑战在于在15天预报中表现与ECMWF集合平均(EM)相当。以前的研究表明，缓解预报误差的积累对于有效的长期预报非常重要。尽管有许多减少积累误差的努力，包括自回归多时间步长损失，但使用单个模型发现无法在短和长导出时间上达到最佳性能。因此，我们提出了FuXi，这是一个级联机器学习天气预测系统，提供了分辨率为0.25度、时间分辨率为6小时的15天全球预测。FuXi基于级联集合模型开发，它集成了多种模型的优势，并减少了预测误差的积累。使用空气温度，比湿度和位势高度的均方根误差(RMSE)和异常相关系数(ACC)评估了FuXi的性能。结果表明，与ECMWF HRES相比，FuXi在15天预报中表现出更好的性能，并显著减少了积累误差。

    Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
    
[^112]: 高斯过程网络的贝叶斯方法

    A Bayesian Take on Gaussian Process Networks. (arXiv:2306.11380v1 [stat.ML])

    [http://arxiv.org/abs/2306.11380](http://arxiv.org/abs/2306.11380)

    该论文提出了一种基于高斯过程和贝叶斯方法的网络模型，通过蒙特卡罗和马尔可夫链蒙特卡罗方法采样网络结构的后验分布。该方法在恢复网络的图形结构方面优于最先进的算法，并提供了后验概率的准确近似。

    

    高斯过程网络（GPNs）是一类有向图模型，其使用高斯过程作为网络中每个变量给定其父变量的条件期望的先验分布。该模型允许以紧凑但灵活的方式描述连续联合分布，对变量之间的依赖关系仅做最少的参数假设。GPNs的贝叶斯结构学习需要计算网络结构的后验分布，即使在低维情况下，这也是计算上不可行的。本文实现了蒙特卡罗和马尔可夫链蒙特卡罗方法来从网络结构的后验分布中采样。因此，该方法遵循贝叶斯范式，通过边缘似然比较模型，并计算GPN特征的后验概率。模拟研究表明，我们的方法在恢复网络的图形结构方面优于最先进的算法，并提供其后验的准确近似。

    Gaussian Process Networks (GPNs) are a class of directed graphical models which employ Gaussian processes as priors for the conditional expectation of each variable given its parents in the network. The model allows describing continuous joint distributions in a compact but flexible manner with minimal parametric assumptions on the dependencies between variables. Bayesian structure learning of GPNs requires computing the posterior over graphs of the network and is computationally infeasible even in low dimensions. This work implements Monte Carlo and Markov Chain Monte Carlo methods to sample from the posterior distribution of network structures. As such, the approach follows the Bayesian paradigm, comparing models via their marginal likelihood and computing the posterior probability of the GPN features. Simulation studies show that our method outperforms state-of-the-art algorithms in recovering the graphical structure of the network and provides an accurate approximation of its poste
    
[^113]: 自上而下的机器学习用于粗粒化蛋白质力场

    Top-down machine learning of coarse-grained protein force-fields. (arXiv:2306.11375v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2306.11375](http://arxiv.org/abs/2306.11375)

    通过分子动力学模拟和可微分轨迹重加权训练神经网络势能，实现了自上而下的粗粒化蛋白质力场建模，仅需蛋白质的天然构象即可展示其外推能力。

    

    开发准确和高效的蛋白质粗粒化表征对于理解它们的折叠、功能和在长时间尺度下的相互作用至关重要。我们的方法包括通过分子动力学模拟蛋白质，并利用得到的轨迹通过可微分轨迹重加权来训练神经网络势能。令人惊讶的是，该方法仅需要蛋白质的天然构象，消除了从广泛模拟或内存密集型端到端可微分模拟导出标记数据的需求。一旦训练完成，模型可以用于并行分子动力学模拟，并对训练分布内外的蛋白质进行折叠事件采样，展示其外推能力。通过应用马尔可夫状态模型，可以预测模拟蛋白质的与天然构象相似的构象。由于其理论可转移性和仅使用蛋白质的天然构象的能力，可以在不同尺度上应用该方法。

    Developing accurate and efficient coarse-grained representations of proteins is crucial for understanding their folding, function, and interactions over extended timescales. Our methodology involves simulating proteins with molecular dynamics and utilizing the resulting trajectories to train a neural network potential through differentiable trajectory reweighting. Remarkably, this method requires only the native conformation of proteins, eliminating the need for labeled data derived from extensive simulations or memory-intensive end-to-end differentiable simulations. Once trained, the model can be employed to run parallel molecular dynamics simulations and sample folding events for proteins both within and beyond the training distribution, showcasing its extrapolation capabilities. By applying Markov State Models, native-like conformations of the simulated proteins can be predicted from the coarse-grained simulations. Owing to its theoretical transferability and ability to use solely e
    
[^114]: 基于FPGA的粒子轨迹跟踪的低延迟边缘分类GNN

    Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs. (arXiv:2306.11330v1 [cs.AR] CROSS LISTED)

    [http://arxiv.org/abs/2306.11330](http://arxiv.org/abs/2306.11330)

    本论文介绍了一种基于FPGA的GNN架构，在粒子跟踪中实现了低延迟和资源高效性，并实现了对现有CPU和GPU的数千倍上的性能提升。

    

    大型强子对撞机中实时粒子轨迹重建面临高碰撞率和众多粒子撞击的挑战。使用FPGA上的GNN（图神经网络）可以实现更高的准确率和灵活的轨迹分类。然而，现有的GNN架构在边缘分类方面资源利用效率低且并行性不足。本文在FPGA上引入了一种资源高效的GNN架构，用于低延迟粒子跟踪。模块化的架构便于设计扩展以支持大型图形。利用击中探测器的几何特性进一步减少了图形的复杂性和资源利用率。我们在Xilinx UltraScale+ VU9P上的实验结果表明，相对于CPU和GPU，性能提高了1625倍和1574倍。

    In-time particle trajectory reconstruction in the Large Hadron Collider is challenging due to the high collision rate and numerous particle hits. Using GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible trajectory classification. However, existing GNN architectures have inefficient resource usage and insufficient parallelism for edge classification. This paper introduces a resource-efficient GNN architecture on FPGAs for low latency particle tracking. The modular architecture facilitates design scalability to support large graphs. Leveraging the geometric properties of hit detectors further reduces graph complexity and resource usage. Our results on Xilinx UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU and GPU respectively.
    
[^115]: 为向量搜索进行硬件和算法的共同设计

    Co-design Hardware and Algorithm for Vector Search. (arXiv:2306.11182v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11182](http://arxiv.org/abs/2306.11182)

    本论文提出了一个在FPGA上的向量搜索框架FANNS，实现了硬件和算法的共同设计，可以根据用户需求和硬件预算生成相应的加速器。与FPGA和CPU基准相比，FANNS实现了显著的加速，并展现了卓越的可扩展性。

    

    向量搜索已成为大规模信息检索和机器学习系统的基础，像Google和Bing这样的搜索引擎通过评估编码查询文本和网络文档之间的向量相似度，每秒处理数万个查询，在拥有PB级文档数据集的情况下。随着对向量搜索系统性能的需求激增，在摩尔定律时代后，加速硬件成为了一个有前景的解决方案。我们介绍了一个在FPGA上的端到端可扩展向量搜索框架FANNS。给定用户提供的对数据集的召回要求和硬件资源预算，FANNS自动进行硬件和算法的共同设计，随后生成相应的加速器。该框架还通过在加速器中引入硬件TCP/IP堆栈来支持规模扩展。与FPGA和CPU基准相比，FANNS分别实现了23.0倍和37.2倍的加速，并展现了卓越的可扩展性。

    Vector search has emerged as the foundation for large-scale information retrieval and machine learning systems, with search engines like Google and Bing processing tens of thousands of queries per second on petabyte-scale document datasets by evaluating vector similarities between encoded query texts and web documents. As performance demands for vector search systems surge, accelerated hardware offers a promising solution in the post-Moore's Law era. We introduce \textit{FANNS}, an end-to-end and scalable vector search framework on FPGAs. Given a user-provided recall requirement on a dataset and a hardware resource budget, \textit{FANNS} automatically co-designs hardware and algorithm, subsequently generating the corresponding accelerator. The framework also supports scale-out by incorporating a hardware TCP/IP stack in the accelerator. \textit{FANNS} attains up to 23.0$\times$ and 37.2$\times$ speedup compared to FPGA and CPU baselines, respectively, and demonstrates superior scalabil
    
[^116]: 策略概括中的效果不变机制

    Effect-Invariant Mechanisms for Policy Generalization. (arXiv:2306.10983v1 [stat.ML])

    [http://arxiv.org/abs/2306.10983](http://arxiv.org/abs/2306.10983)

    本文提出了一种松弛了完全不变性的方法，称为效果不变性，证明它足以进行零样本策略概括，并讨论了基于少量样本的扩展。

    

    策略学习是许多实际学习系统的重要组成部分。策略学习的一个主要挑战是如何有效地适应未见过的环境或任务。最近，有人建议利用不变的条件分布来学习更好地概括未见过环境的模型。然而，假设整个条件分布是不变的（我们称之为完全不变性），在实践中可能是一个太强的假设。在本文中，我们引入了一种松弛完全不变性的方法，称为效果不变性（简称e-不变性），并证明它是足够的（在适当的假设下），用于零样本策略概括。我们还讨论了一种扩展，它在测试环境中只有少量样本时利用e-不变性，从而实现了少样本策略推广。我们的工作不假设存在一个基础因果图，也不假设数据是由结构因果模型生成的。相反，我们开发了测试过程来测试e-不变性。

    Policy learning is an important component of many real-world learning systems. A major challenge in policy learning is how to adapt efficiently to unseen environments or tasks. Recently, it has been suggested to exploit invariant conditional distributions to learn models that generalize better to unseen environments. However, assuming invariance of entire conditional distributions (which we call full invariance) may be too strong of an assumption in practice. In this paper, we introduce a relaxation of full invariance called effect-invariance (e-invariance for short) and prove that it is sufficient, under suitable assumptions, for zero-shot policy generalization. We also discuss an extension that exploits e-invariance when we have a small sample from the test environment, enabling few-shot policy generalization. Our work does not assume an underlying causal graph or that the data are generated by a structural causal model; instead, we develop testing procedures to test e-invariance dir
    
[^117]: 基于注意力知识图卷积网络的旅游景点推荐

    Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2306.10946](http://arxiv.org/abs/2306.10946)

    本文提出了一种基于注意力知识图卷积网络的旅游景点推荐模型，通过自动语义发掘目标景点的相邻实体，根据旅客的喜好选择，预测类似景点的概率，实验中取得良好效果。

    

    基于知识图谱的推荐算法在相对成熟阶段，但在特定领域的推荐仍存在问题。例如在旅游领域，选择适合的旅游景点属性流程作为推荐基础较为复杂。本文提出改进的注意力知识图卷积网络模型(Att-KGCN)，自动语义地发掘目标景点的相邻实体，利用注意力层将相对相似的位置进行聚合，并通过推理旅客喜好选择，预测类似景点的概率作为推荐系统。实验中，采用索科特拉岛-也门的旅游数据，证明了注意力知识图卷积网络在旅游领域的景点推荐效果良好。

    The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
    
[^118]: 基于紧核的条件期望估计

    Conditional expectation via compact kernels. (arXiv:2306.10592v1 [stat.ML])

    [http://arxiv.org/abs/2306.10592](http://arxiv.org/abs/2306.10592)

    本文提出了一种基于紧核的算子理论方法来解决条件期望估计问题，在再生核希尔伯特空间中实现，易于实现，且成功应用于实际问题中。

    

    去噪、条件期望和流形学习任务通常可以在寻找两个随机变量积的条件期望的公共环境下表述。本文针对这个更一般的问题，描述了一种算子理论方法来估计条件期望。核积分算子被用作紧致化工具，将估计问题设置为在再生核希尔伯特空间中的线性逆问题。该方程的解被证明对数值逼近是稳定的，从而确保了数据驱动实现的收敛性。总体技术易于实现，还展示了其在一些实际问题中的成功应用。

    The separate tasks of denoising, conditional expectation and manifold learning can often be posed in a common setting of finding the conditional expectations arising from a product of two random variables. This paper focuses on this more general problem and describes an operator theoretic approach to estimating the conditional expectation. Kernel integral operators are used as a compactification tool, to set up the estimation problem as a linear inverse problem in a reproducing kernel Hilbert space. This equation is shown to have solutions that are stable to numerical approximation, thus guaranteeing the convergence of data-driven implementations. The overall technique is easy to implement, and their successful application to some real-world problems are also shown.
    
[^119]: RL感知机：高维策略学习的泛化动力学

    The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions. (arXiv:2306.10404v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10404](http://arxiv.org/abs/2306.10404)

    本文提出了一个高维RL模型，推导出该模型的典型动力学为一组闭式ODE方程组，并通过实验与神经RL代理进行了比较，结果表明该模型捕捉了现实世界RL的关键特征。

    

    强化学习算法已被证明在许多领域中具有变革性，为了解决真实世界的问题，这些系统通常使用神经网络直接从像素或其他高维感官输入中学习策略。然而，许多强化学习理论都集中于离散状态空间或最坏情况分析，关于高维情况下策略学习的动力学基本问题仍有待解决。在这里，我们提出了一个可解的高维RL模型，它可以捕捉多种学习协议，并将其典型动力学导出为一组闭式常微分方程。我们推导出最佳的学习率和任务难度调度，类似于训练中的退火方案和课程表，并表明该模型表现出丰富的行为，包括在稀疏奖励下的延迟学习；根据奖励基线不同的各种学习方案；以及由奖励严格程度驱动的速度-准确度权衡。与神经RL代理进行的实验比较表明，该模型捕捉了现实世界RL的关键特征。

    Reinforcement learning (RL) algorithms have proven transformative in a range of domains. To tackle real-world domains, these systems often use neural networks to learn policies directly from pixels or other high-dimensional sensory input. By contrast, much theory of RL has focused on discrete state spaces or worst-case analysis, and fundamental questions remain about the dynamics of policy learning in high-dimensional settings. Here, we propose a solvable high-dimensional model of RL that can capture a variety of learning protocols, and derive its typical dynamics as a set of closed-form ordinary differential equations (ODEs). We derive optimal schedules for the learning rates and task difficulty - analogous to annealing schemes and curricula during training in RL - and show that the model exhibits rich behaviour, including delayed learning under sparse rewards; a variety of learning regimes depending on reward baselines; and a speed-accuracy trade-off driven by reward stringency. Expe
    
[^120]: 预测晶体性质的完整相互作用势的高效逼近

    Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10045](http://arxiv.org/abs/2306.10045)

    本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。

    

    我们研究了晶体材料的性质预测。晶体结构由一个最小的单元格组成，在三维空间中无限重复。如何在机器学习模型中准确表示这种重复结构仍然没有解决。当前的方法只在附近的节点之间建立边缘来构建图形，因此无法忠实地捕捉无限重复的模式和远距离的原子间相互作用。在这项工作中，我们提出了几个创新来克服这些限制。首先，我们建议直接建模物理原理的相互作用势，而不仅仅使用距离，如许多现有方法所做的。这些势包括库仑势，伦敦分散势和Pauli斥力势。其次，我们建模所有原子之间的完整势，而不仅仅是现有方法中的附近原子之间的势。这得益于我们用可证明的误差界逼近无限势和的方法。我们进一步开发了...

    We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
    
[^121]: 自然语言处理中社会人口统计偏见的调查

    Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])

    [http://arxiv.org/abs/2306.08158](http://arxiv.org/abs/2306.08158)

    本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。

    

    深度神经网络在训练过程中往往会学习到非预期的偏见，这在实际应用中可能会产生有害的影响。本文对209篇关于NLP模型中偏见的论文进行了调查，其中大部分论文涉及社会人口统计偏见。为了更好地理解偏见与真实世界的危害之间的区别，我们借鉴心理学和行为经济学的思想，提出了社会人口统计偏见的定义。我们确定了NLP偏见研究的三个主要类别：偏见类型、量化偏见和去偏见。我们认为当前对于量化偏见的方法存在可靠性问题，许多偏见度量并不涉及真实世界中的偏见，当前的去偏见技术是表面的，只是隐藏了偏见，而不是真正去除它。最后，我们提供了未来工作的建议。

    Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
    
[^122]: 学习具有可辨识分解的世界模型

    Learning World Models with Identifiable Factorization. (arXiv:2306.06561v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06561](http://arxiv.org/abs/2306.06561)

    本文提出了一个通用框架IFactor，用于学习具有可辨识分解的世界模型，以提取稳定且紧凑的环境表示，并揭示了所有与奖励相关的因素对策略学习的重要性。

    

    在高维、噪声和非平稳环境中，提取环境稳定而紧凑的表示对于高效的强化学习至关重要。这样的环境中存在不同类别的信息-如何有效提取和区分这些信息仍然是一个具有挑战性的问题。在本文中，我们提出了一个通用框架IFactor，用于建模四种不同类别的潜在状态变量，这些变量通过与动作和奖励之间的相互作用捕捉RL系统中的各个信息方面。我们的分析建立了这些潜在变量的分块可识别性，这不仅提供了稳定且紧凑的表示，还揭示了所有与奖励相关的因素对策略学习的重要性。我们还提出了一种学习具有可辨识分块的世界模型的实用方法，确保去除冗余但保留最小且足够的信息以进行策略优化。

    Extracting a stable and compact representation of the environment is crucial for efficient reinforcement learning in high-dimensional, noisy, and non-stationary environments. Different categories of information coexist in such environments -- how to effectively extract and disentangle these information remains a challenging problem. In this paper, we propose IFactor, a general framework to model four distinct categories of latent state variables that capture various aspects of information within the RL system, based on their interactions with actions and rewards. Our analysis establishes block-wise identifiability of these latent variables, which not only provides a stable and compact representation but also discloses that all reward-relevant factors are significant for policy learning. We further present a practical approach to learning the world model with identifiable blocks, ensuring the removal of redundants but retaining minimal and sufficient information for policy optimization.
    
[^123]: 多视角聚类方法：一步多视角聚类与多样性表征

    One-step Multi-view Clustering with Diverse Representation. (arXiv:2306.05437v1 [cs.LG])

    [http://arxiv.org/abs/2306.05437](http://arxiv.org/abs/2306.05437)

    本文提出了一种一步多视角聚类与多样性表征的方法，将多视角学习和k-means聚类融合到一个统一框架中，实验结果表明其在各种标准的多视角数据集上都优于现有算法。

    

    多视角聚类因其能够利用不同视角的信息来提高效果而备受关注。本文提出了一种一步多视角聚类与多样性表征的方法，将多视角学习和k-means聚类融合到一个统一框架中。实验结果表明，在各种标准的多视角数据集上，我们的方法在效果和效率方面都优于现有的同类算法。

    Multi-view clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-view clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, which limits the expressiveness of the model. Moreover, a range of methods suffer from a two-step process, i.e., multimodal learning and the subsequent $k$-means, inevitably causing a sub-optimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation method, which incorporates multi-view learning and $k$-means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervis
    
[^124]: EMO：用于小样本元学习的情节记忆优化

    EMO: Episodic Memory Optimization for Few-Shot Meta-Learning. (arXiv:2306.05189v1 [cs.LG])

    [http://arxiv.org/abs/2306.05189](http://arxiv.org/abs/2306.05189)

    EMO是一种元学习的情节记忆优化方案，通过在外部存储器中记录过去任务的梯度历史，实现小样本学习，无论提供的梯度信息是否可靠，都可以推动参数更新朝着正确的方向前进。

    

    小样本元学习由于任务训练样本数量的限制对梯度下降优化提出了挑战。为了解决这个问题，本文提出了一种元学习的情节记忆优化方案，称为EMO。EMO受到人类从脑内记忆中回忆过去学习经验的能力的启发，将过去任务的梯度历史记录在外部存储器中，以增强记忆的方式进行小样本学习。通过学习保留和回忆过去训练任务的学习过程，即使仅有有限数量的示例提供了不可靠的梯度，EMO也可以推动参数更新朝着正确的方向前进。我们在理论上证明了该算法对于平滑、强凸目标函数会收敛。EMO是通用的、灵活的、与模型无关的优化器，可无缝嵌入现有的基于优化的小样本元学习方法。实证结果表明EMO可以提高准确性和收敛速度。

    Few-shot meta-learning presents a challenge for gradient descent optimization due to the limited number of training samples per task. To address this issue, we propose an episodic memory optimization for meta-learning, we call \emph{EMO}, which is inspired by the human ability to recall past learning experiences from the brain's memory. EMO retains the gradient history of past experienced tasks in external memory, enabling few-shot learning in a memory-augmented way. By learning to retain and recall the learning process of past training tasks, EMO nudges parameter updates in the right direction, even when the gradients provided by a limited number of examples are uninformative. We prove theoretically that our algorithm converges for smooth, strongly convex objectives. EMO is generic, flexible, and model-agnostic, making it a simple plug-and-play optimizer that can be seamlessly embedded into existing optimization-based few-shot meta-learning approaches. Empirical results show that EMO 
    
[^125]: 通过随机投影快速获得最优的本地隐私均值估计

    Fast Optimal Locally Private Mean Estimation via Random Projections. (arXiv:2306.04444v1 [cs.LG])

    [http://arxiv.org/abs/2306.04444](http://arxiv.org/abs/2306.04444)

    提出了一种名为ProjUnit的算法框架，用于实现高效的本地隐私均值估计，通过随机投影低维空间实现最优解，且具有低通信复杂度和快速的服务器运行时间。

    

    本文研究了欧几里得空间中高维向量的本地隐私均值估计问题。现有算法要么产生次优误差，要么具有高通信和/或运行时间复杂度。我们提出了一种新的算法框架ProjUnit，用于隐私均值估计的算法具有计算效率高、通信复杂度低且误差与最优解之间的差距最大为1 + o(1)。我们的框架实现起来非常简单：每个随机化器将其输入投影到一个随机的低维子空间中，对结果进行归一化，然后在低维空间中运行一个最优算法，例如PrivUnitG。此外，我们展示了通过适当地协调设备之间的随机投影矩阵，可以实现快速的服务器运行时间。我们通过随机投影的性质分析了算法的误差，并研究了两种实例。最后，我们的实验结果表明，ProjUnit相比现有方法具有显著的性能优势，特别是在高维高斯混合数据集上表现出色。

    We study the problem of locally private mean estimation of high-dimensional vectors in the Euclidean ball. Existing algorithms for this problem either incur sub-optimal error or have high communication and/or run-time complexity. We propose a new algorithmic framework, ProjUnit, for private mean estimation that yields algorithms that are computationally efficient, have low communication complexity, and incur optimal error up to a $1+o(1)$-factor. Our framework is deceptively simple: each randomizer projects its input to a random low-dimensional subspace, normalizes the result, and then runs an optimal algorithm such as PrivUnitG in the lower-dimensional space. In addition, we show that, by appropriately correlating the random projection matrices across devices, we can achieve fast server run-time. We mathematically analyze the error of the algorithm in terms of properties of the random projections, and study two instantiations. Lastly, our experiments for private mean estimation and pr
    
[^126]: 置换等变图框架在异质半监督学习中的应用

    Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning. (arXiv:2306.04265v1 [cs.LG])

    [http://arxiv.org/abs/2306.04265](http://arxiv.org/abs/2306.04265)

    本文介绍了一个用于异质半监督学习的新型图神经网络模型PEGFAN，它使用置换等变图框架实现了多尺度特征提取，表现优于其他最先进模型，特别是在相对较大和密集连接的数据集中。

    

    异质图的本质与同质图显著不同，这表明1-hop以外的聚合方式并引起早期图神经网络模型的困难。本文展示了一种新的多尺度提取方法，通过构建具有置换等变性，高效性和稀疏性的Haar-type图框架，在图上深度学习任务中实现。我们进一步使用我们构建的图框架设计了图框架神经网络模型PEGFAN。实验在合成数据集和9个基准数据集上进行，与其他最先进的模型进行性能比较。结果表明，我们的模型在某些异质图数据集（包括相对较大和更密集的连接的大部分异质数据集）上可以达到最佳性能，并在其余数据集上具有竞争性能。

    The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.
    
[^127]: LLMZip：使用大型语言模型的无损文本压缩

    LLMZip: Lossless Text Compression using Large Language Models. (arXiv:2306.04050v1 [cs.IT])

    [http://arxiv.org/abs/2306.04050](http://arxiv.org/abs/2306.04050)

    本研究使用大型语言模型提出了一种结合预测和无损压缩方案的英文文本压缩算法，并在初步实验中表现优于当前最先进的文本压缩方案。

    

    本文使用大型语言模型LLaMA-7B对英语熵的渐近上界提出了新估计值，并提出了一种结合大型语言模型预测与无损压缩方案的英文文本压缩算法。初步实验结果显示，我们的算法优于当前最先进的文本压缩方案，如BSC、ZPAQ和paq8h。

    We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in \cite{cover1978convergent}, \cite{lutati2023focus}. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.
    
[^128]: 语言模型是有限实用说话者

    Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17760](http://arxiv.org/abs/2305.17760)

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。

    

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。特别地，我们展示了经过人类反馈的强化学习微调的大型语言模型（Ouyang等人，2022）具有概念上类似于 快与慢思考模型（Kahneman，2011）的思维模型，而这种思维模型被心理学家们归因于人类。我们讨论了从人类反馈中的强化学习作为快与慢思考模型的局限性，并提出了扩展这个框架的途径。本研究实质上凸显了采用认知概率建模方法来获得对语言模型的理解、评估和推进方面的深刻见解的价值。

    How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
    
[^129]: 关于神经网络作为无限树状概率图模型的论文研究

    On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models. (arXiv:2305.17583v1 [stat.ML])

    [http://arxiv.org/abs/2305.17583](http://arxiv.org/abs/2305.17583)

    本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决深度神经网络(DNNs)缺乏PGMs的精确语义和明确定义的概率解释的问题。研究发现DNNs在前向传播时确实执行PGM推断的近似，这与现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似，潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。

    

    深度神经网络(DNNs)缺乏概率图模型(PGMs)的精确语义和明确定义的概率解释。本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决这个问题。我们的研究揭示了DNNs在前向传播期间确实执行PGM推断的近似，这与曾经的神经网络描述为核机器或无限大小的高斯过程的现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似。潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。

    Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs.
    
[^130]: 具有上下文模型的Levin树搜索

    Levin Tree Search with Context Models. (arXiv:2305.16945v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16945](http://arxiv.org/abs/2305.16945)

    本文提出了一种新的具有上下文模型的Levin树搜索算法，通过将神经网络替换为上下文模型，实现了LTS损失的凸优化，并在多个基准测试中取得了明显优于LTS+NN的结果。

    

    Levin Tree Search（LTS）是一种利用策略（动作的概率分布）的搜索算法，并具有关于达到目标节点之前扩展次数的理论保证，这取决于策略的质量。我们将这个保证称为LTS损失，可以将其作为优化表示策略的神经网络（LTS+NN）的损失函数。在这项工作中，我们展示了神经网络可以用在线压缩文献中的参数化上下文模型来替代（LTS+CM）。我们证明了在这种新模型下LTS损失是凸的，从而可以使用标准凸优化工具，并且对于给定的解轨迹集合，在在线设置中可以获得到最优参数的收敛保证——而神经网络无法提供这样的保证。新的LTS+CM算法在几个基准测试中与LTS+NN相比表现出明显优势：Sokoban（Boxoban）、The Witness和24-Sliding Tile Puzzle（STP）。

    Levin Tree Search (LTS) is a search algorithm that makes use of a policy (a probability distribution over actions) and comes with a theoretical guarantee on the number of expansions before reaching a goal node, depending on the quality of the policy. This guarantee can be used as a loss function, which we call the LTS loss, to optimize neural networks representing the policy (LTS+NN). In this work we show that the neural network can be substituted with parameterized context models originating from the online compression literature (LTS+CM). We show that the LTS loss is convex under this new model, which allows for using standard convex optimization tools, and obtain convergence guarantees to the optimal parameters in an online setting for a given set of solution trajectories -- guarantees that cannot be provided for neural networks. The new LTS+CM algorithm compares favorably against LTS+NN on several benchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle (STP). The
    
[^131]: 可复现强化学习

    Replicable Reinforcement Learning. (arXiv:2305.15284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15284](http://arxiv.org/abs/2305.15284)

    本篇论文提供了可复制的强化学习算法，是控制问题的第一个正式的可复制性结果

    

    在社会、行为和数据科学中，可重复性危机导致了算法框架的形成，即要求算法在从相同的底层分布提取的两个不同样本上运行时产生相同的输出（概率高）。虽然仍处于初期阶段，但在机器学习和统计学中的许多基本任务，包括统计查询学习、重要项问题和分布测试，都已经开发出了可证明可复现算法。在这项工作中，我们开始研究可复现强化学习，并提供了并行值迭代的可证复制算法以及一个在连续设置中可证复制的R-max。这是控制问题的第一个正式可复制性结果，这些问题在批量学习环境中提出了不同的复制挑战。

    The replicability crisis in the social, behavioral, and data sciences has led to the formulation of algorithm frameworks for replicability -- i.e., a requirement that an algorithm produce identical outputs (with high probability) when run on two different samples from the same underlying distribution. While still in its infancy, provably replicable algorithms have been developed for many fundamental tasks in machine learning and statistics, including statistical query learning, the heavy hitters problem, and distribution testing. In this work we initiate the study of replicable reinforcement learning, providing a provably replicable algorithm for parallel value iteration, and a provably replicable version of R-max in the episodic setting. These are the first formal replicability results for control problems, which present different challenges for replication than batch learning settings.
    
[^132]: 分布感知的公平性测试生成

    Distribution-aware Fairness Test Generation. (arXiv:2305.13935v1 [cs.CV])

    [http://arxiv.org/abs/2305.13935](http://arxiv.org/abs/2305.13935)

    本文介绍了一种名为DistroFair的分布感知的公平性测试方法，可以从图像分类器中检测到类级别的公平性违规。

    

    本文探讨如何验证图像识别软件中的组公平性。我们提出了一种分布感知的公平性测试方法（称为DistroFair），通过将超出分布范围的对象引入到图像识别器中，通过三种语义保留图像变换 - 对象删除，对象插入和对象旋转来系统性地暴露图像分类器中的类级别公平性违规。我们使用两个知名数据集（CityScapes和MS-COCO）和三个主要的商业图像识别软件（即Amazon Rekognition，Google Cloud Vision和Azure计算机视觉）对DistroFair进行评估。结果显示，DistroFair生成的图像中，约有21％通过真实标准或元测试标准显露出了类级别的公平性违规。

    This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DistroFair) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DistroFair automatically learns the distribution (e.g., number/orientation) of objects in a set of images. Then it systematically mutates objects in the images to become OOD using three semantic-preserving image mutations -- object deletion, object insertion and object rotation. We evaluate DistroFair using two well-known datasets (CityScapes and MS-COCO) and three major, commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision). Results show that about 21% of images generated by DistroFair reveal class-level fairness violations using either ground truth or metamorphic oracles. DistroFair 
    
[^133]: 基于可分数据的双层神经网络的快速收敛

    Fast Convergence in Learning Two-Layer Neural Networks with Separable Data. (arXiv:2305.13471v1 [cs.LG])

    [http://arxiv.org/abs/2305.13471](http://arxiv.org/abs/2305.13471)

    本文研究了使用归一化梯度下降算法在双层神经网络中进行训练的方法，证明了对于指数尾部损失函数，其收敛速率为线性，同时建立了有限时间的泛化边界。

    

    在具有可分数据的线性分类器上，归一化梯度下降在加速指数尾部损失函数（包括指数和逻辑损失）收敛方面取得了显着成功。本文通过研究归一化 GD 对双层神经网络的作用超越了线性模型。对于指数尾部损失，我们证明了使用归一化 GD 导致训练损失对全局最优解的线性收敛速率。这是通过展示一定的梯度自限制条件和对数利普希茨特性实现的。我们还通过算法稳定性分析研究了用于凸目标的归一化 GD 的泛化。特别是，我们通过建立有限时间的泛化边界证明了训练期间归一化 GD 不会过拟合。

    Normalized gradient descent has shown substantial success in speeding up the convergence of exponentially-tailed loss functions (which includes exponential and logistic losses) on linear classifiers with separable data. In this paper, we go beyond linear models by studying normalized GD on two-layer neural nets. We prove for exponentially-tailed losses that using normalized GD leads to linear rate of convergence of the training loss to the global optimum. This is made possible by showing certain gradient self-boundedness conditions and a log-Lipschitzness property. We also study generalization of normalized GD for convex objectives via an algorithmic-stability analysis. In particular, we show that normalized GD does not overfit during training by establishing finite-time generalization bounds.
    
[^134]: 复制循环神经网络结构网络

    Copy Recurrent Neural Network Structure Network. (arXiv:2305.13250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13250](http://arxiv.org/abs/2305.13250)

    本研究提出了一种名为复制循环神经网络结构网络（CRNNet）的新型粗到细的ICD路径生成框架，通过使用RNN生成顺序输出并结合复制模块，有效识别复杂疾病，相比于最先进的和先前的方法，在预测中实现了更高的复杂疾病比率（57.30%）。

    

    电子健康记录（EHR）编码涉及将EHR自动分类为诊断代码。虽然大多数先前的研究将其视为多标签分类任务，生成每个代码的概率并选择超过一定阈值的标签，但这些方法常常忽视了识别复杂疾病的挑战。本研究的重点是在EHR中检测并发疾病。我们提出了一种称为复制循环神经网络结构网络（CRNNet）的新型粗到细的ICD路径生成框架，该框架使用路径生成器（PG）和路径鉴别器（PD）进行EHR编码。通过使用RNN生成顺序输出并结合复制模块，我们能够有效地识别复杂疾病。我们的方法在预测中实现了57.30％的复杂疾病比率，优于最先进的和先前的方法。此外，通过消融研究，我们证明复制机制起着至关重要的作用。

    Electronic Health Record (EHR) coding involves automatically classifying EHRs into diagnostic codes. While most previous research treats this as a multi-label classification task, generating probabilities for each code and selecting those above a certain threshold as labels, these approaches often overlook the challenge of identifying complex diseases. In this study, our focus is on detecting complication diseases within EHRs.  We propose a novel coarse-to-fine ICD path generation framework called the Copy Recurrent Neural Network Structure Network (CRNNet), which employs a Path Generator (PG) and a Path Discriminator (PD) for EHR coding. By using RNNs to generate sequential outputs and incorporating a copy module, we efficiently identify complication diseases. Our method achieves a 57.30\% ratio of complex diseases in predictions, outperforming state-of-the-art and previous approaches.  Additionally, through an ablation study, we demonstrate that the copy mechanism plays a crucial rol
    
[^135]: 通过样本重新加权与样本关联测试进行失语症语音的无偏自动语音识别

    Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test. (arXiv:2305.13108v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.13108](http://arxiv.org/abs/2305.13108)

    本文提出了一种样本重新加权与样本关联测试（Re-SAT）的新方法，用于缓解失语症患者的偏差问题，在不影响健康患者语音的ASR性能的情况下，有效提高了ASR的性能表现。

    

    基于深度学习的自动语音识别系统主要是通过经验风险最小化（ERM）进行训练的。由于ERM利用数据样本的平均表现而不考虑一个群体，例如健康或失语症患者，因此ASR系统无法识别跨群体的性能差异，导致ASR系统存在偏差且其群体性能差异严重。本研究旨在提高语音识别系统的群体稳健性，针对失语症患者进行改进。为了实现我们的目标，我们提出了一种新方法，即样本重新加权与样本关联测试（Re-SAT）。 Re-SAT系统地衡量所给数据样本的去偏帮助性，并通过去偏帮助性加权来缓解偏差。实验结果表明， Re-SAT有助于改善失语症语音的ASR性能，而不会影响健康语音的ASR性能。

    Automatic speech recognition systems based on deep learning are mainly trained under empirical risk minimization (ERM). Since ERM utilizes the averaged performance on the data samples regardless of a group such as healthy or dysarthric speakers, ASR systems are unaware of the performance disparities across the groups. This results in biased ASR systems whose performance differences among groups are severe. In this study, we aim to improve the ASR system in terms of group robustness for dysarthric speakers. To achieve our goal, we present a novel approach, sample reweighting with sample affinity test (Re-SAT). Re-SAT systematically measures the debiasing helpfulness of the given data sample and then mitigates the bias by debiasing helpfulness-based sample reweighting. Experimental results demonstrate that Re-SAT contributes to improved ASR performance on dysarthric speech without performance degradation on healthy speech.
    
[^136]: 捕捉促销期间的转化率波动：一种新颖的历史数据再利用方法

    Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach. (arXiv:2305.12837v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.12837](http://arxiv.org/abs/2305.12837)

    本论文提出了一种名为HDR的新方法，通过重复使用历史促销数据，来捕捉促销转化模式，达到更好地适应促销模式的目的。

    

    转化率（CVR）预测是在线推荐系统的核心组件之一，已经提出了各种方法以获得准确和一致的CVR估计。然而，我们观察到，即使训练良好的CVR预测模型，在促销期间也经常表现出次优的性能。这主要归因于数据分布转移问题，其中传统方法不再起作用。因此，我们寻求开发替代建模技术用于CVR预测。观察到不同促销之间存在相似的购买模式，我们提出了重用历史促销数据以捕捉促销转化模式的方法。因此，我们提出了一种新颖的历史数据再利用（HDR）方法，该方法首先检索历史上相似的促销数据，然后使用获取的数据微调CVR预测模型以更好地适应促销模式。HDR由三个组件组成：自动数据

    Conversion rate (CVR) prediction is one of the core components in online recommender systems, and various approaches have been proposed to obtain accurate and well-calibrated CVR estimation. However, we observe that a well-trained CVR prediction model often performs sub-optimally during sales promotions. This can be largely ascribed to the problem of the data distribution shift, in which the conventional methods no longer work. To this end, we seek to develop alternative modeling techniques for CVR prediction. Observing similar purchase patterns across different promotions, we propose reusing the historical promotion data to capture the promotional conversion patterns. Herein, we propose a novel \textbf{H}istorical \textbf{D}ata \textbf{R}euse (\textbf{HDR}) approach that first retrieves historically similar promotion data and then fine-tunes the CVR prediction model with the acquired data for better adaptation to the promotion mode. HDR consists of three components: an automated data 
    
[^137]: 采用图神经ODE模型实现复杂动态物理系统的模拟

    Towards Complex Dynamic Physics System Simulation with Graph Neural ODEs. (arXiv:2305.12334v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12334](http://arxiv.org/abs/2305.12334)

    本研究提出了一种基于学习的模拟模型，称为GNSTODE，通过利用统一的端到端框架描述了粒子系统中不同时间和不同空间条件下的变化。

    

    深度学习模型具有很强的学习能力，使我们能够理解真实的物理世界，因此学习模拟复杂的粒子系统是一个很有前途的努力。然而，物理世界的复杂规律给基于学习的模拟带来了巨大的挑战，如相互作用粒子之间的不同空间依赖性以及不同时间戳之间粒子系统状态的不同时间依赖性，这些因素决定了粒子的相互作用行为和物理系统的演化模式。现有的基于学习的模拟方法无法充分考虑这些复杂性，因此无法产生令人满意的模拟结果。为了更好地理解复杂的物理法则，本文提出了一种新的基于学习的模拟模型——具有时空建模的图神经ODE模型（GNSTODE），该模型利用统一的端到端框架描述了粒子系统中不同时间和不同空间条件下的变化。

    The great learning ability of deep learning models facilitates us to comprehend the real physical world, making learning to simulate complicated particle systems a promising endeavour. However, the complex laws of the physical world pose significant challenges to the learning based simulations, such as the varying spatial dependencies between interacting particles and varying temporal dependencies between particle system states in different time stamps, which dominate particles' interacting behaviour and the physical systems' evolution patterns. Existing learning based simulation methods fail to fully account for the complexities, making them unable to yield satisfactory simulations. To better comprehend the complex physical laws, this paper proposes a novel learning based simulation model- Graph Networks with Spatial-Temporal neural Ordinary Equations (GNSTODE)- that characterizes the varying spatial and temporal dependencies in particle systems using a united end-to-end framework. Th
    
[^138]: Waymo开放模拟代理挑战赛

    The Waymo Open Sim Agents Challenge. (arXiv:2305.12032v1 [cs.CV])

    [http://arxiv.org/abs/2305.12032](http://arxiv.org/abs/2305.12032)

    Waymo开放模拟代理挑战赛提出使用真实、互动的智能体仿真以促进自动驾驶行为模型的评估和训练，是该领域的首个公开挑战赛，旨在推动逼真模拟器的设计。

    

    本文定义了Waymo开放模拟代理挑战赛(WOSAC)。通过与真实、互动的智能体进行仿真是自动驾驶软件开发的关键任务。WOSAC是第一个公开的挑战赛，旨在解决该任务并提出相应的评估指标。该挑战的目标是激发设计逼真模拟器的兴趣，以用于评估和训练自动驾驶的行为模型。我们概述了评估方法，并展示了几种基准仿真代理方法的初步结果。

    In this work, we define the Waymo Open Sim Agents Challenge (WOSAC). Simulation with realistic, interactive agents represents a key task for autonomous vehicle software development. WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The goal of the challenge is to stimulate the design of realistic simulators that can be used to evaluate and train a behavior model for autonomous driving. We outline our evaluation methodology and present preliminary results for a number of different baseline simulation agent methods.
    
[^139]: 基于学习度量的大规模包裹操作

    Large-Scale Package Manipulation via Learned Metrics of Pick Success. (arXiv:2305.10272v1 [cs.RO])

    [http://arxiv.org/abs/2305.10272](http://arxiv.org/abs/2305.10272)

    本文讨论了基于学习度量的大规模包裹操作，通过训练拾取成功预测器和学习拾取质量度量，实现了能够大规模部署的强力抓握策略。

    

    自动化仓储操作可以降低物流成本，最终降低消费品价格，提高交货速度，并增强对劳动力波动的抵抗能力。近年来，自动化重复任务的兴趣增加，但大多数是在受控环境中进行的。从杂乱的堆堆中挑选物品等任务直到最近才变得足够强大，可以在最小人工干预下进行大规模部署。本文展示了亚马逊机器人的Robot Induction（Robin）群的大规模包裹操作，该群利用在实际生产数据上训练的拾取成功预测器。具体而言，该系统在超过394K个拾取上进行了训练。它用于把每天高达5百万个包裹进行了分离，本文的评估期间操作了超过2亿个包裹。开发的学习拾取质量度量实时排名各种拾取替代方案，并采用高成功率的强力抓握策略。

    Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.  This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.  The developed learned pick quality measure ranks various pick alternatives in real-time and p
    
[^140]: 分子形态对比预训练提高分子表示迁移能力

    Molecule-Morphology Contrastive Pretraining for Transferable Molecular Representation. (arXiv:2305.09790v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.09790](http://arxiv.org/abs/2305.09790)

    本文提出了Molecule-Morphology Contrastive Pretraining (MoCoP)框架，用于学习分子图形和细胞形态的多模态表示。实验结果表明，MoCoP可以提高图神经网络在分子属性预测任务上的表现，具有良好的实用性。

    

    过去十年中，基于图像的分析技术因其在目标鉴定、作用机制推断和测定发展中的应用而越来越受欢迎。这些技术产生了大量细胞形态的数据集，通常用于研究小分子干扰物的效果。本文通过引入Molecule-Morphology Contrastive Pretraining (MoCoP)，提出了一种学习分子图形和细胞形态的多模态表示的框架，将这样的数据集的影响扩展到了改进量化结构-活性关系 (QSAR) 模型。使用来自JUMP-CP联盟的数据，将MoCoP扩展到了约100K的分子和约600K的形态文件，结果表明，MoCoP在ChEMBL20上的分子属性预测任务中，始终提高了图神经网络 (GNNs) 的表现，且在内部 GSK药代动力学数据和其他数据集上具有良好的实用性。

    Image-based profiling techniques have become increasingly popular over the past decade for their applications in target identification, mechanism-of-action inference, and assay development. These techniques have generated large datasets of cellular morphologies, which are typically used to investigate the effects of small molecule perturbagens. In this work, we extend the impact of such dataset to improving quantitative structure-activity relationship (QSAR) models by introducing Molecule-Morphology Contrastive Pretraining (MoCoP), a framework for learning multi-modal representation of molecular graphs and cellular morphologies. We scale MoCoP to approximately 100K molecules and 600K morphological profiles using data from the JUMP-CP Consortium and show that MoCoP consistently improves performances of graph neural networks (GNNs) on molecular property prediction tasks in ChEMBL20 across all dataset sizes. The pretrained GNNs are also evaluated on internal GSK pharmacokinetic data and s
    
[^141]: 卷积神经网络的定量语义比较

    Quantified Semantic Comparison of Convolutional Neural Networks. (arXiv:2305.07663v1 [cs.CV])

    [http://arxiv.org/abs/2305.07663](http://arxiv.org/abs/2305.07663)

    本研究提出了两种方法来量化卷积神经网络潜在空间中语义信息之间的相似性，从而揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。

    

    卷积神经网络（CNN）在计算机视觉领域的应用处于领先地位，具有出色的性能，然而它们的工作原理却很难阐明。但是，对于自动驾驶这类安全关键应用，模型选择还应考虑候选模型在模型透明性方面如何表示语义信息。为了解决这一尚未解决的问题，我们的工作提出了两种方法来量化CNN潜在空间中语义信息之间的相似性，旨在揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。我们使用了可解释人工智能（XAI）领域的著名技术作为基础，这些技术用于获得每个潜在空间中语义概念的全局向量表示，并基于它们在测试输入上的激活进行比较。本工作在三个不同的目标检测器和两个不同范围的图像数据集上进行了评估。

    The state-of-the-art in convolutional neural networks (CNNs) for computer vision excels in performance, while remaining opaque. But due to safety regulations for safety-critical applications, like perception for automated driving, the choice of model should also take into account how candidate models represent semantic information for model transparency reasons. To tackle this yet unsolved problem, our work proposes two methods for quantifying the similarity between semantic information in CNN latent spaces. These allow insights into both the flow and similarity of semantic information within CNN layers, and into the degree of their similitude between different networks. As a basis, we use renown techniques from the field of explainable artificial intelligence (XAI), which are used to obtain global vector representations of semantic concepts in each latent space. These are compared with respect to their activation on test inputs. When applied to three diverse object detectors and two d
    
[^142]: 基于冲击响应增强的设备鲁棒声学场景分类

    Device-Robust Acoustic Scene Classification via Impulse Response Augmentation. (arXiv:2305.07499v1 [cs.SD])

    [http://arxiv.org/abs/2305.07499](http://arxiv.org/abs/2305.07499)

    本篇论文提出了一种基于冲击响应增强的方法，用于解决音频分类模型泛化到未被训练设备上时性能下降的问题。

    

    对于音频分类模型而言，广泛适用于各种录音设备是关键性能因素。不同类型的麦克风特性由于其不同的频率响应，会引入数字化音频信号的分布差异。如果在训练期间不考虑此领域偏移，那么当它用于未见过的设备记录音频时，模型的性能可能会严重下降。特别是，在少数不同麦克风上录制音频信号的模型训练可能会使泛化到未被训练的设备困难。为解决这个问题，我们使用预录制设备脉冲响应(DIR)卷积训练集中的音频信号，从而人工增加录音设备的多样性。我们系统地研究了使用CNN和音频光谱变换进行Acoustic Scene Classification任务的DIR增强效果。结果表明，仅使用DIR增强就能提升模型在未见过设备上的性能。

    The ability to generalize to a wide range of recording devices is a crucial performance factor for audio classification models. The characteristics of different types of microphones introduce distributional shifts in the digitized audio signals due to their varying frequency responses. If this domain shift is not taken into account during training, the model's performance could degrade severely when it is applied to signals recorded by unseen devices. In particular, training a model on audio signals recorded with a small number of different microphones can make generalization to unseen devices difficult. To tackle this problem, we convolve audio signals in the training set with pre-recorded device impulse responses (DIRs) to artificially increase the diversity of recording devices. We systematically study the effect of DIR augmentation on the task of Acoustic Scene Classification using CNNs and Audio Spectrogram Transformers. The results show that DIR augmentation in isolation performs
    
[^143]: 基于流形正则化 Tucker 分解的时空交通数据填充方法

    Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation. (arXiv:2305.06563v1 [stat.ML])

    [http://arxiv.org/abs/2305.06563](http://arxiv.org/abs/2305.06563)

    本文提出了一种基于流形正则化Tucker分解的时空交通数据填充方法，该方法利用稀疏正则化项改善了Tucker核的稀疏性，并引入流形正则化和时间约束项来优化张量的填充性能。

    

    时空交通数据填充(STDI)是数据驱动智能交通系统中不可避免和具有挑战性的任务，在部分观测到的交通数据中估计丢失数据。由于交通数据具有多维和时空性质，我们将丢失数据填充视为张量完成问题。过去十年中，许多关于基于张量分解的 STDI 的研究已经展开。然而，如何利用时空相关性和核张量稀疏性来改善填充性能仍然需要解决。本文重新构造了3/4阶汉克尔张量，并提出了一种创新的流形正则化 Tucker 分解(maniRTD)模型用于STDI。明确地，我们通过引入多维延迟嵌入变换将传感交通状态数据表示为3/4阶张量。然后，ManiRTD使用稀疏正则化项改善了Tucker核的稀疏性，并使用流形正则化和时间约束项来优化张量的填充性能。

    Spatiotemporal traffic data imputation (STDI), estimating the missing data from partially observed traffic data, is an inevitable and challenging task in data-driven intelligent transportation systems (ITS). Due to traffic data's multidimensional and spatiotemporal properties, we treat the missing data imputation as a tensor completion problem. Many studies have been on STDI based on tensor decomposition in the past decade. However, how to use spatiotemporal correlations and core tensor sparsity to improve the imputation performance still needs to be solved. This paper reshapes a 3rd/4th order Hankel tensor and proposes an innovative manifold regularized Tucker decomposition (ManiRTD) model for STDI. Expressly, we represent the sensory traffic state data as the 3rd/4th tensors by introducing Multiway Delay Embedding Transforms. Then, ManiRTD improves the sparsity of the Tucker core using a sparse regularization term and employs manifold regularization and temporal constraint terms of f
    
[^144]: 转移学习下的模型选择限制

    Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v1 [stat.ML])

    [http://arxiv.org/abs/2305.00152](http://arxiv.org/abs/2305.00152)

    这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。

    

    目前，关于转移学习或领域自适应的理论研究主要关注已知假设类或模型的情况；然而，在实践中，通常涉及一定程度的模型选择，这经常出现在超参数调整的总体范畴下：例如，我们可以考虑调整针对目标任务的正确神经网络架构的问题，同时利用来自相关源任务的数据。除了与模型选择有关的近似与估计误差的通常权衡之外，这个问题还带来了新的复杂度，即源分布与目标分布之间的转移距离，这个距离随着假设类的选择而发生变化。我们首次研究了这个问题，重点关注分类问题。特别的，分析揭示了一些引人注目的现象：自适应速率，即没有分布式信息时可达到的速率，可以任意慢于oracle速率，即在给定知识的情况下。

    Theoretical studies on transfer learning or domain adaptation have so far focused on situations with a known hypothesis class or model; however in practice, some amount of model selection is usually involved, often appearing under the umbrella term of hyperparameter-tuning: for example, one may think of the problem of tuning for the right neural network architecture towards a target task, while leveraging data from a related source task.  Now, in addition to the usual tradeoffs on approximation vs estimation errors involved in model selection, this problem brings in a new complexity term, namely, the transfer distance between source and target distributions, which is known to vary with the choice of hypothesis class.  We present a first study of this problem, focusing on classification; in particular, the analysis reveals some remarkable phenomena: adaptive rates, i.e., those achievable with no distributional information, can be arbitrarily slower than oracle rates, i.e., when given kn
    
[^145]: 从像素中学习物体中心化的广义值函数

    Discovering Object-Centric Generalized Value Functions From Pixels. (arXiv:2304.13892v1 [cs.LG])

    [http://arxiv.org/abs/2304.13892](http://arxiv.org/abs/2304.13892)

    本文介绍了一种从像素中学习物体中心化的广义值函数的方法。该方法从物体中发现有意义的特征，转化为“问题”函数，并利用随后学习的广义值函数来进行控制，在静态和非静态设置下表现良好。学到的表示不仅是可解释的，而且围绕着具有不变性的物体，有助于快速适应。

    

    深度强化学习展现了从高维输入中提取有用表示的显著进展，尽管使用的是手工辅助任务和伪奖励。自动化地以物体为中心学习此类表示，以期实现控制和快速适应，仍然是一个未解决的研究问题。在本文中，我们介绍了一种方法，试图从物体中发现有意义的特征，将它们转化为时间上连贯的“问题”函数，并利用随后学习的广义值函数来进行控制。我们将我们的方法与最先进的技术进行比较，并展示了在静态和非静态设置下的竞争性表现。最后，我们还调查了被发现的广义值函数，并通过定性分析表明，学到的表示不仅是可解释的，而且围绕着物体，这些物体对任务的变化具有不变性，有助于快速适应。

    Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatically learning such representations in an object-centric manner geared towards control and fast adaptation remains an open research problem. In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent "question" functions and leveraging the subsequent learned general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings. Finally, we also investigate the discovered general value functions and through qualitative analysis show that the learned representations are not only interpretable but also, centered around objects that are invariant to changes across tasks facilitating fast adaptatio
    
[^146]: 智能学习发现 愚笨合约

    Smart Learning to Find Dumb Contracts. (arXiv:2304.10726v1 [cs.CR])

    [http://arxiv.org/abs/2304.10726](http://arxiv.org/abs/2304.10726)

    DLVA是一种用于以太坊智能合约的强大深度学习漏洞检测工具，其算法涵盖了源代码到字节码的扩展，并且速度比传统漏洞检测工具提高了10-500倍，并成功地发现了一些Slither误标记的易受攻击的合约。

    

    我们引入了基于强大深度学习技术的 Deep Learning Vulnerability Analyzer （DLVA），它是一种针对以字节码为基础的以太坊智能合约的漏洞检测工具。我们在没有手动特征工程、预定义模式或专家规则的情况下，将源代码分析扩展到字节码，训练DLVA判断字节码。DLVA训练算法的鲁棒性也很强：它克服了1.25%误标记合约的错误率，学生超越了老师，并发现了Slither误标记的易受攻击的合约。DLVA比基于形式方法的传统智能合约漏洞检测工具快得多：DLVA检查了29个漏洞所需的时间为0.2秒，速度提高了10-500倍。DLVA有三个关键组成部分：Smart Contract to Vector（SC2Vec）将智能合约转换为深度学习模型的向量表示。Bytecode Tokenizer（BCT）将底层字节码转换为神经网络的有意义的标记，DLVA是神经网络模型，可预测智能合约是否包含漏洞。我们对Etherscan的28,505个经过验证的智能合约数据集进行了DLVA评估，发现它取得了0.964的AUC（真阳率/假阳率曲线下的面积）得分。与基线方法相比，DLVA在F1分数上显示了30.7%的改进，它是精度和召回的调和平均值。

    We introduce Deep Learning Vulnerability Analyzer (DLVA), a vulnerability detection tool for Ethereum smart contracts based on powerful deep learning techniques for sequential data adapted for bytecode. We train DLVA to judge bytecode even though the supervising oracle, Slither, can only judge source code. DLVA's training algorithm is general: we "extend" a source code analysis to bytecode without any manual feature engineering, predefined patterns, or expert rules. DLVA's training algorithm is also robust: it overcame a 1.25% error rate mislabeled contracts, and the student surpassing the teacher; found vulnerable contracts that Slither mislabeled. In addition to extending a source code analyzer to bytecode, DLVA is much faster than conventional tools for smart contract vulnerability detection based on formal methods: DLVA checks contracts for 29 vulnerabilities in 0.2 seconds, a speedup of 10-500x+ compared to traditional tools.  DLVA has three key components. Smart Contract to Vecto
    
[^147]: EPVT: 基于环境感知的提示视觉Transformer在皮肤病变识别领域一般化中的应用

    EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition. (arXiv:2304.01508v1 [cs.CV])

    [http://arxiv.org/abs/2304.01508](http://arxiv.org/abs/2304.01508)

    EPVT是一种基于环境感知的提示视觉Transformer，用于解决皮肤病变识别中深度神经网络可能过度依赖疾病不相关图像特征的问题，通过嵌入一组领域提示和一个共享提示来进行领域一般化，并且引入了领域提示生成器促进知识共享。

    

    利用深度学习进行皮肤病变识别已取得重大进展，而在现实世界场景中部署这些系统的需求不断增加。然而，最近的研究表明，用于皮肤病变识别的深度神经网络可能过度依赖于与疾病不相关的图像特征（如暗角、浓密毛发），导致在看不见的环境中表现不佳。为了解决这个问题，我们提出了一种新颖的领域一般化方法——EPVT，它将提示嵌入到Vision Transformer中，以协同学习来自不同领域的知识。具体而言，EPVT利用一组领域提示，每个领域提示都扮演领域专家的角色，以捕获领域特定的知识；以及一个共享提示来获得整个数据集的通用知识。为了促进知识共享和不同提示之间的交互，我们引入了一个领域提示生成器，它使得领域提示与共享提示之间可以进行低秩乘性更新。

    Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on disease-irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A
    
[^148]: SemEval-2023任务3上的mCPT：用于零样本和少样本框架检测的多语言标签感知对比预训练变压器

    mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])

    [http://arxiv.org/abs/2303.09901](http://arxiv.org/abs/2303.09901)

    本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。

    

    本文介绍了零样本的西班牙语框架检测任务的获胜系统，并在另外八种语言中取得了良好的成绩。框架检测任务的挑战在于在只有少量或零个样本的情况下识别一组14个框架，即多语言多标签的少样本和零样本设置。我们开发的解决方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。除了描述系统外，我们还进行了嵌入空间分析和消融研究，以展示我们的预训练程序如何支持框架检测以推进计算框架分析。

    This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
    
[^149]: SpikeGPT：带有脉冲神经网络的生成预训练语言模型

    SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13939](http://arxiv.org/abs/2302.13939)

    本论文提出了一种称之为SpikeGPT的生成语言模型，使用二进制、事件驱动脉冲激活单元进行训练，克服了SNN训练中的挑战性。该模型可以用于大规模语言生成任务。

    

    随着大型语言模型的规模不断扩大，所需的计算资源也随之增加。脉冲神经网络（SNN）已成为一种能够利用稀疏和事件驱动激活减少模型推理计算开销的节能深度学习方法。虽然它们在许多计算机视觉任务上已经具有竞争力，但SNN的训练也被证明更具挑战性。因此，它们的性能落后于现代深度学习，我们尚未看到SNN在语言生成方面的有效性。在本文中，我们受到Receptance Weighted Key Value（RWKV）语言模型的启发，成功实现了“SpikeGPT”，它是一种具有二进制、事件驱动脉冲激活单元的生成语言模型。我们在两种模型变体上训练了所提出的模型：45M和216M参数。据我们所知，SpikeGPT是迄今最大的反向传播训练SNN模型，使其适用于非脉冲模型通常解决的大规模语言生成任务。

    As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suita
    
[^150]: GraphSR：一种用于不平衡节点分类的数据增强算法

    GraphSR: A Data Augmentation Algorithm for Imbalanced Node Classification. (arXiv:2302.12814v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12814](http://arxiv.org/abs/2302.12814)

    本文提出了一种名为GraphSR的自动节点分类数据增强算法，通过使用具有显著差异的未标记节点来增强少数类的训练样本。

    

    图神经网络（GNN）在节点分类任务中取得了巨大成功。然而，现有的GNN自然偏向于具有更多标记数据的多数类，并忽略那些具有相对较少标记数据的少数类。传统的技术往往采用过采样方法，但可能会导致过拟合问题。最近的一些工作提出从标记节点中合成少数类的附加节点，然而，并没有保证这些生成的节点真正代表相应的少数类。事实上，不恰当地合成节点可能导致算法的泛化能力不足。为了解决这个问题，本文提出了一种自动从图的大量未标记节点中增强少数类的方法——GraphSR。具体来说，我们提出了一种新颖的自训练策略，通过使用具有显著差异的未标记节点增加少数类，该策略基于相似度。

    Graph neural networks (GNNs) have achieved great success in node classification tasks. However, existing GNNs naturally bias towards the majority classes with more labelled data and ignore those minority classes with relatively few labelled ones. The traditional techniques often resort over-sampling methods, but they may cause overfitting problem. More recently, some works propose to synthesize additional nodes for minority classes from the labelled nodes, however, there is no any guarantee if those generated nodes really stand for the corresponding minority classes. In fact, improperly synthesized nodes may result in insufficient generalization of the algorithm. To resolve the problem, in this paper we seek to automatically augment the minority classes from the massive unlabelled nodes of the graph. Specifically, we propose \textit{GraphSR}, a novel self-training strategy to augment the minority classes with significant diversity of unlabelled nodes, which is based on a Similarity-bas
    
[^151]: 通过有针对性的增强实现领域外的鲁棒性

    Out-of-Domain Robustness via Targeted Augmentations. (arXiv:2302.11861v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11861](http://arxiv.org/abs/2302.11861)

    本文研究了为领域外泛化设计数据增强的原则，通过有针对性的增强方法，在保留鲁棒特征的同时随机化虚假的领域相关特征，提高了领域外性能。

    

    在一个领域上训练的模型往往在未见过的领域上表现下降，比如野生动物监测模型在新的摄像机位置上部署时会出现这种情况。在这项工作中，我们研究了为领域外泛化设计数据增强的原则。特别地，我们关注现实世界中的场景，在这些场景中，一些与领域相关的特征是鲁棒的，即一些在不同领域间变化的特征对泛化有预测能力。例如，在上述的野生动物监测应用中，图像背景在摄像机位置上不同，但可以指示栖息地类型，从而帮助预测被摄动物的物种。在对线性设置进行理论分析的基础上，我们提出了有针对性的增强方法，即有选择性地随机化虚假的领域相关特征，同时保留鲁棒的特征。我们证明了有针对性的增强可以提高领域外性能，使模型在较少领域上具有更好的泛化能力。相比之下，现有的通用增强方法未能实现领域外的鲁棒性。

    Models trained on one set of domains often suffer performance drops on unseen domains, e.g., when wildlife monitoring models are deployed in new camera locations. In this work, we study principles for designing data augmentations for out-of-domain (OOD) generalization. In particular, we focus on real-world scenarios in which some domain-dependent features are robust, i.e., some features that vary across domains are predictive OOD. For example, in the wildlife monitoring application above, image backgrounds vary across camera locations but indicate habitat type, which helps predict the species of photographed animals. Motivated by theoretical analysis on a linear setting, we propose targeted augmentations, which selectively randomize spurious domain-dependent features while preserving robust ones. We prove that targeted augmentations improve OOD performance, allowing models to generalize better with fewer domains. In contrast, existing approaches such as generic augmentations, which fai
    
[^152]: 使用最大熵强化学习玩文本基础冒险游戏

    Learning to Play Text-based Adventure Games with Maximum Entropy Reinforcement Learning. (arXiv:2302.10720v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10720](http://arxiv.org/abs/2302.10720)

    本文提出了一种使用最大熵强化学习玩文本基础冒险游戏的方法，通过结合奖励塑形技术，提供更多信息密集的奖励信号给强化学习机器人，从而使其在许多游戏中达到更高的得分，且只需一半的训练步骤。

    

    文本基础游戏是语言强化学习的流行测试平台。本文中，我们将软演员-评论家算法(SAC)适应到文本环境中，并结合基于潜力的奖励塑形技术，以提供更多信息密集的奖励信号给强化学习机器人。我们应用该方法来玩具有挑战性的文本游戏。SAC方法在许多游戏中的得分比Q学习方法高，并且只需一半的训练步骤。这表明该方法非常适用于文本游戏。此外，我们还展示了奖励塑形技术可以帮助机器人更快地学习策略并获得更高的分数。

    Text-based games are a popular testbed for language-based reinforcement learning (RL). In previous work, deep Q-learning is commonly used as the learning agent. Q-learning algorithms are challenging to apply to complex real-world domains due to, for example, their instability in training. Therefore, in this paper, we adapt the soft-actor-critic (SAC) algorithm to the text-based environment. To deal with sparse extrinsic rewards from the environment, we combine it with a potential-based reward shaping technique to provide more informative (dense) reward signals to the RL agent. We apply our method to play difficult text-based games. The SAC method achieves higher scores than the Q-learning methods on many games with only half the number of training steps. This shows that it is well-suited for text-based games. Moreover, we show that the reward shaping technique helps the agent to learn the policy faster and achieve higher scores. In particular, we consider a dynamically learned value fu
    
[^153]: 这是一篇关于通过跟踪目标动态来实现更快的文本到图像定制的论文

    Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics. (arXiv:2302.04841v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.04841](http://arxiv.org/abs/2302.04841)

    本文研究了文本到图像个性化方法的训练动态，并提出了一种简单的早停准则来加快训练速度

    

    文本到图像生成模型代表了图像合成的下一个发展阶段，为实现灵活但精细的控制结果提供了一种自然的方式。研究的一个新兴领域是将大型文本到图像模型快速适应到较小的数据集或新的视觉概念。然而，许多高效的适应方法需要长时间的训练，这限制了它们的实际应用，降低了研究实验的速度，并消耗了过多的GPU资源。在这项工作中，我们研究了流行的文本到图像个性化方法（如文本倒转或梦幻小屋）的训练动态，旨在加速它们。我们观察到大多数概念在早期阶段就已经学习到了，并且质量在后期没有得到改善，但是标准的模型收敛指标未能指示这一点。相反，我们提出了一种简单的即插即用的早停准则，该准则只需要在所有训练迭代中对一组固定输入计算常规训练目标。我们对...进行了实验

    Text-to-image generation models represent the next step of evolution in image synthesis, offering a natural way to achieve flexible yet fine-grained control over the result. One emerging area of research is the fast adaptation of large text-to-image models to smaller datasets or new visual concepts. However, many efficient methods of adaptation have a long training time, which limits their practical applications, slows down research experiments, and spends excessive GPU resources. In this work, we study the training dynamics of popular text-to-image personalization methods (such as Textual Inversion or DreamBooth), aiming to speed them up. We observe that most concepts are learned at early stages and do not improve in quality later, but standard model convergence metrics fail to indicate that. Instead, we propose a simple drop-in early stopping criterion that only requires computing the regular training objective on a fixed set of inputs for all training iterations. Our experiments on 
    
[^154]: 近似拒绝采样的样本复杂度及其在平滑在线学习中的应用

    The Sample Complexity of Approximate Rejection Sampling with Applications to Smoothed Online Learning. (arXiv:2302.04658v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.04658](http://arxiv.org/abs/2302.04658)

    本研究展示了在有界f-散度约束下，近似拒绝采样的样本复杂度可以通过Θ(~(D/f'(n)))函数来表示，并且应用于平滑在线学习中的相关算法的性能依然成立。

    

    假设我们可以访问来自分布μ的n个独立样本，并且我们希望输出其中一个样本，使得输出的分布尽可能接近目标分布ν。在这项工作中，我们展示了在所有具有有界f-散度Df(ν|μ)≤D的ν,μ对中，关于n的最优总变差距离由Θ(~(D/f'(n)))给出。之前，这个问题只研究了ν相对于μ的Radon-Nikodym导数一致有界的情况。我们还考虑了似乎非常不同的平滑在线学习领域的一个应用，我们展示了最小化遗憾和具有oracle效率的算法的遗憾即使在对手有边界f-散度（而不是有界Radon-Nikodym导数）的松弛约束下，仍然成立。最后，我们还研究了在均匀估计中用于平均估计的重要性采样的效果。

    Suppose we are given access to $n$ independent samples from distribution $\mu$ and we wish to output one of them with the goal of making the output distributed as close as possible to a target distribution $\nu$. In this work we show that the optimal total variation distance as a function of $n$ is given by $\tilde\Theta(\frac{D}{f'(n)})$ over the class of all pairs $\nu,\mu$ with a bounded $f$-divergence $D_f(\nu\|\mu)\leq D$. Previously, this question was studied only for the case when the Radon-Nikodym derivative of $\nu$ with respect to $\mu$ is uniformly bounded. We then consider an application in the seemingly very different field of smoothed online learning, where we show that recent results on the minimax regret and the regret of oracle-efficient algorithms still hold even under relaxed constraints on the adversary (to have bounded $f$-divergence, as opposed to bounded Radon-Nikodym derivative). Finally, we also study efficacy of importance sampling for mean estimates uniform o
    
[^155]: 迈向物理合理的数据驱动模型：一种新颖的神经网络方法用于符号回归

    Toward Physically Plausible Data-Driven Models: A Novel Neural Network Approach to Symbolic Regression. (arXiv:2302.00773v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2302.00773](http://arxiv.org/abs/2302.00773)

    这项研究提出了一种新颖的基于神经网络的符号回归方法，该方法通过基于梯度的优化算法来学习整个解析模型的结构和系数，克服了传统遗传编程方法在变量和样本数量增加时模型规模和复杂度增长快、准确性提升不足以及模型系数调整困难的问题。

    

    许多现实世界的系统可以由人类可理解的数学模型描述，这些模型易于分析并有助于解释系统的行为。符号回归是一种可以从数据中自动生成这种模型的方法。历史上，符号回归主要通过遗传编程来实现，这种方法通过遗传操作交叉和突变对候选解的群体进行进化和修改。然而，这种方法存在一些问题：随着训练数据中变量和样本数量的增加，模型的规模和复杂性会增长，但准确性提升不足；仅通过遗传操作很难对模型系数进行微调。最近，人们开始将神经网络应用于学习整个解析模型，即学习模型的结构和系数，采用基于梯度的优化算法。本文提出了一种新颖的基于神经网络的符号回归方法。

    Many real-world systems can be described by mathematical models that are human-comprehensible, easy to analyze and help explain the system's behavior. Symbolic regression is a method that can automatically generate such models from data. Historically, symbolic regression has been predominantly realized by genetic programming, a method that evolves populations of candidate solutions that are subsequently modified by genetic operators crossover and mutation. However, this approach suffers from several deficiencies: it does not scale well with the number of variables and samples in the training data - models tend to grow in size and complexity without an adequate accuracy gain, and it is hard to fine-tune the model coefficients using just genetic operators. Recently, neural networks have been applied to learn the whole analytic model, i.e., its structure and the coefficients, using gradient-based optimization algorithms. This paper proposes a novel neural network-based symbolic regression
    
[^156]: GFlowNets用于基于AI的科学探索

    GFlowNets for AI-Driven Scientific Discovery. (arXiv:2302.00615v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00615](http://arxiv.org/abs/2302.00615)

    GFlowNets是一种新的概率机器学习框架，可以帮助加快科学探索的速度，尤其是在高维搜索空间中估计不确定性和生成多样性的实验集。

    

    解决人类最紧迫的问题，如气候危机和全球大流行的威胁，需要加快科学探索的速度。尽管科学传统上在很大程度上依赖于试错和偶然性，但过去几十年来，数据驱动的科学发现出现了激增。然而，为了真正利用大规模数据集和高通量实验装置，机器学习方法需要进一步改进并更好地融入科学发现流程。当前机器学习方法在这个背景下面临的一个关键挑战是高效探索非常大的搜索空间，这需要估计可减少的（认知）不确定性和生成多样性和信息丰富的实验集。这促使提出了一种新的概率机器学习框架称为GFlowNets，它可以应用于建模、假设生成和实验设计。

    Tackling the most pressing problems for humanity, such as the climate crisis and the threat of global pandemics, requires accelerating the pace of scientific discovery. While science has traditionally relied on trial and error and even serendipity to a large extent, the last few decades have seen a surge of data-driven scientific discoveries. However, in order to truly leverage large-scale data sets and high-throughput experimental setups, machine learning methods will need to be further improved and better integrated in the scientific discovery pipeline. A key challenge for current machine learning methods in this context is the efficient exploration of very large search spaces, which requires techniques for estimating reducible (epistemic) uncertainty and generating sets of diverse and informative experiments to perform. This motivated a new probabilistic machine learning framework called GFlowNets, which can be applied in the modeling, hypotheses generation and experimental design s
    
[^157]: DiffSTG: 带有去噪扩散模型的概率时空图预测

    DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models. (arXiv:2301.13629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13629](http://arxiv.org/abs/2301.13629)

    本论文提出了一种新的方法DiffSTG，该方法结合了STGNN的时空学习能力和扩散模型的不确定性测量，可以有效减小STG预测中的排名概率分数和均方根误差。

    

    时空图神经网络（STGNN）已成为时空图（STG）预测的主要模型。然而，它们无法对STG数据中的内在不确定性进行建模，这使得它们在决策任务中的实用性受到限制。为此，本文关注概率STG预测，由于建模不确定性和复杂的ST依赖关系的困难，这是一个具有挑战性的问题。在本研究中，我们首次尝试将流行的去噪扩散概率模型推广到STG，提出了一种称为DiffSTG的新的非自回归框架，并在该框架中引入了第一个STG去噪网络UGnet。我们的方法将STGNN的时空学习能力与扩散模型的不确定性测量相结合。大量实验证实DiffSTG将持续排名概率分数（CRPS）降低了4%-14%，均方根误差（RMSE）降低了2%-7%。

    Spatio-temporal graph neural networks (STGNN) have emerged as the dominant model for spatio-temporal graph (STG) forecasting. Despite their success, they fail to model intrinsic uncertainties within STG data, which cripples their practicality in downstream tasks for decision-making. To this end, this paper focuses on probabilistic STG forecasting, which is challenging due to the difficulty in modeling uncertainties and complex ST dependencies. In this study, we present the first attempt to generalize the popular denoising diffusion probabilistic models to STGs, leading to a novel non-autoregressive framework called DiffSTG, along with the first denoising network UGnet for STG in the framework. Our approach combines the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models. Extensive experiments validate that DiffSTG reduces the Continuous Ranked Probability Score (CRPS) by 4%-14%, and Root Mean Squared Error (RMSE) by 2%-7% over existing 
    
[^158]: GibbsDDRM:一种用于解决盲逆问题的局部折叠Gibbs采样器，带有去噪扩散恢复的先验。(arXiv:2301.12686v2 [cs.LG] 更新)

    GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration. (arXiv:2301.12686v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12686](http://arxiv.org/abs/2301.12686)

    GibbsDDRM是一种扩展的局部折叠Gibbs采样方法，用于解决线性逆问题中线性算子未知的盲场景。它利用预训练的扩散模型构建了数据、测量和线性算子的联合分布，并通过后验采样来解决问题。该方法在盲图像去模糊和语音去混响任务上表现出了高性能，而且适用于各种逆问题。

    

    在各种线性逆问题中，预训练扩散模型已成功用作先验，目标是从噪声线性测量中重构信号。然而，现有方法需要了解线性算子。在本文中，我们提出了GibbsDDRM，它是将Denoising Diffusion Restoration Models(DDRM)扩展到线性测量算子未知的盲场景。GibbsDDRM利用预训练的扩散模型构建了数据、测量和线性算子的联合分布，并通过采用一种高效的Gibbs采样器的变体进行后验采样来解决问题。所提出的方法是问题不可知的，意味着预训练的扩散模型可以应用于各种逆问题而无需微调。在实验证明，尽管使用了简单的通用先验来处理底层线性算子，该方法在盲图像去模糊和语音去混响任务上均取得了高性能。

    Pre-trained diffusion models have been successfully used as priors in a variety of linear inverse problems, where the goal is to reconstruct a signal from noisy linear measurements. However, existing approaches require knowledge of the linear operator. In this paper, we propose GibbsDDRM, an extension of Denoising Diffusion Restoration Models (DDRM) to a blind setting in which the linear measurement operator is unknown. GibbsDDRM constructs a joint distribution of the data, measurements, and linear operator by using a pre-trained diffusion model for the data prior, and it solves the problem by posterior sampling with an efficient variant of a Gibbs sampler. The proposed method is problem-agnostic, meaning that a pre-trained diffusion model can be applied to various inverse problems without fine-tuning. In experiments, it achieved high performance on both blind image deblurring and vocal dereverberation tasks, despite the use of simple generic priors for the underlying linear operators.
    
[^159]: 对于使用早停的神经网络，conformal推理是几乎免费的。

    Conformal inference is (almost) free for neural networks trained with early stopping. (arXiv:2301.11556v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11556](http://arxiv.org/abs/2301.11556)

    本文介绍了一种将早停与conformal校准相结合的新方法，以解决使用早停训练的神经网络在缺乏独立校准数据时无法提供准确统计保证的问题。

    

    基于保留数据的早停是一种流行的正则化技术，用于减轻过拟合并提高神经网络的预测准确性。使用早停训练的模型通常提供相对准确的预测，但除非进一步使用独立的保留数据进行校准，否则它们通常仍然缺乏精确的统计保证。本文通过将早停与conformal校准相结合，同时高效地重复使用相同的保留数据，解决了上述限制。这导致了既准确又能够提供精确预测推断的模型，而无需多次数据拆分或过于保守的调整。为不同的学习任务（异常值检测，多类分类，回归）开发了实际实现，并在真实数据上展示了它们的竞争性能。

    Early stopping based on hold-out data is a popular regularization technique designed to mitigate overfitting and increase the predictive accuracy of neural networks. Models trained with early stopping often provide relatively accurate predictions, but they generally still lack precise statistical guarantees unless they are further calibrated using independent hold-out data. This paper addresses the above limitation with conformalized early stopping: a novel method that combines early stopping with conformal calibration while efficiently recycling the same hold-out data. This leads to models that are both accurate and able to provide exact predictive inferences without multiple data splits nor overly conservative adjustments. Practical implementations are developed for different learning tasks -- outlier detection, multi-class classification, regression -- and their competitive performance is demonstrated on real data.
    
[^160]: 一种基于半监督感知率学习的CMAB方案，通过可信数据收集在人群中抗击COVID-19

    A Semi-supervised Sensing Rate Learning based CMAB Scheme to Combat COVID-19 by Trustful Data Collection in the Crowd. (arXiv:2301.08563v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2301.08563](http://arxiv.org/abs/2301.08563)

    本文提出了一种基于半监督学习的组合多臂赌博反向拍卖方案，用于解决移动众包系统中在招募多个未知和有策略的工作者时出现的数据可信问题。

    

    对于移动众包系统，招募可信和高质量的工作者是一个重要的研究问题。先前的研究要么假设工作者的能力是事先已知的，要么假设平台一旦接收到他们收集的数据就知道他们的能力。实际上，为了降低成本并最大程度地提高收入，许多有策略的工作者不诚实地执行其感知任务，并向平台报告虚假数据，这被称为虚假数据攻击。对于平台来说，评估所收到的数据的真实性十分困难。本文提出了一种名为基于半监督组合多臂赌博反向拍卖（SCMABA）的激励机制来解决MCS中多个未知和有策略的工作者的招聘问题。首先，我们将工作者招募建模为多臂赌博反向拍卖问题，并设计了一种基于UCB的算法来分离探索和开发，将已招募的工作者的感知率视为“臂“。

    The recruitment of trustworthy and high-quality workers is an important research issue for MCS. Previous studies either assume that the qualities of workers are known in advance, or assume that the platform knows the qualities of workers once it receives their collected data. In reality, to reduce costs and thus maximize revenue, many strategic workers do not perform their sensing tasks honestly and report fake data to the platform, which is called False data attacks. And it is very hard for the platform to evaluate the authenticity of the received data. In this paper, an incentive mechanism named Semi-supervision based Combinatorial Multi-Armed Bandit reverse Auction (SCMABA) is proposed to solve the recruitment problem of multiple unknown and strategic workers in MCS. First, we model the worker recruitment as a multi-armed bandit reverse auction problem and design an UCB-based algorithm to separate the exploration and exploitation, regarding the Sensing Rates (SRs) of recruited worke
    
[^161]: 输入归一化随机梯度下降训练深度神经网络

    Input Normalized Stochastic Gradient Descent Training of Deep Neural Networks. (arXiv:2212.09921v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.09921](http://arxiv.org/abs/2212.09921)

    本文提出了一种名为输入归一化随机梯度下降（INSGD）的优化算法，通过在学习率上应用归一化来更新网络权重，以实现在大型数据集上训练复杂模型时避免发散并提高准确性水平。

    

    本文提出了一种用于训练机器学习模型的新型优化算法，称为输入归一化随机梯度下降（INSGD），受到自适应滤波中使用的归一化最小均方（NLMS）算法的启发。当在大型数据集上训练复杂模型时，优化器参数的选择，尤其是学习率，对于避免发散是至关重要的。我们的算法使用随机梯度下降更新网络权重，其中学习率使用基于$\ell_1$和$\ell_2$的归一化，类似于NLMS。然而，与现有的归一化方法不同的是，我们在归一化过程中排除了误差项，而是使用输入向量对更新项进行归一化。我们的实验证明，相比于不同的初始化设置，我们的优化算法实现了更高的准确性水平。我们使用ResNet-18、WResNet-20和ResNet-50等基准数据集评估了我们的训练算法的效率。

    In this paper, we propose a novel optimization algorithm for training machine learning models called Input Normalized Stochastic Gradient Descent (INSGD), inspired by the Normalized Least Mean Squares (NLMS) algorithm used in adaptive filtering. When training complex models on large datasets, the choice of optimizer parameters, particularly the learning rate, is crucial to avoid divergence. Our algorithm updates the network weights using stochastic gradient descent with $\ell_1$ and $\ell_2$-based normalizations applied to the learning rate, similar to NLMS. However, unlike existing normalization methods, we exclude the error term from the normalization process and instead normalize the update term using the input vector to the neuron. Our experiments demonstrate that our optimization algorithm achieves higher accuracy levels compared to different initialization settings. We evaluate the efficiency of our training algorithm on benchmark datasets using ResNet-18, WResNet-20, ResNet-50, 
    
[^162]: 在高维度中更快的最大内积搜索

    Faster Maximum Inner Product Search in High Dimensions. (arXiv:2212.07551v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07551](http://arxiv.org/abs/2212.07551)

    该论文提出了一种在高维度中更快的最大内积搜索算法 BanditMIPS，该算法的复杂度与维度无关，并且通过自适应采样策略提供理论保证和实验证明。

    

    最大内积搜索（MIPS）是机器学习应用中普遍存在的任务，如推荐系统。给定一个查询向量和d维空间中的n个原子向量，MIPS的目标是找到与查询向量具有最高内积的原子。现有的MIPS算法的复杂度至少为O(√d)，在高维设置下变得计算上禁止。在这项工作中，我们提出了BanditMIPS，一种新颖的随机MIPS算法，其复杂度与d无关。BanditMIPS通过子采样坐标来估计每个原子的内积，并适应性地评估更有前景的原子的坐标。具体的自适应采样策略受到了多臂老虎机问题的启发。我们提供理论保证，BanditMIPS以高概率返回正确的答案，同时将复杂度从O(√d)改进到O(1)。我们还在四个合成和实际数据集上进行了实验证明。

    Maximum Inner Product Search (MIPS) is a ubiquitous task in machine learning applications such as recommendation systems. Given a query vector and $n$ atom vectors in $d$-dimensional space, the goal of MIPS is to find the atom that has the highest inner product with the query vector. Existing MIPS algorithms scale at least as $O(\sqrt{d})$, which becomes computationally prohibitive in high-dimensional settings. In this work, we present BanditMIPS, a novel randomized MIPS algorithm whose complexity is independent of $d$. BanditMIPS estimates the inner product for each atom by subsampling coordinates and adaptively evaluates more coordinates for more promising atoms. The specific adaptive sampling strategy is motivated by multi-armed bandits. We provide theoretical guarantees that BanditMIPS returns the correct answer with high probability, while improving the complexity in $d$ from $O(\sqrt{d})$ to $O(1)$. We also perform experiments on four synthetic and real-world datasets and demonst
    
[^163]: 基于生物标志物激活地图的可解释性糖尿病视网膜病变诊断

    Interpretable Diabetic Retinopathy Diagnosis based on Biomarker Activation Map. (arXiv:2212.06299v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2212.06299](http://arxiv.org/abs/2212.06299)

    本文提出了一种基于生物标志物激活地图（BAM）的诊断框架，使用两个U形生成器为临床医生提供意义明确的可解释性，以验证和理解分类器的决策，可应用于自动诊断糖尿病视网膜病变。

    

    深度学习分类器提供了基于光学相干断层扫描（OCT）及其血管影像学（OCTA）自动诊断糖尿病视网膜病变（DR）的最准确的手段。这些模型的优势部分归功于包含的隐藏层所提供的复杂性，从而实现了所需任务。然而，隐藏层也使算法输出难以解释。在这里，我们引入了一种基于生成对抗学习的新型生物标志物激活地图（BAM）框架，使临床医生能够验证和理解分类器的决策。一个数据集包括456个黄斑扫描根据当前临床标准被分级为非可转诊或可转诊DR。首先，基于此数据集使用DR分类器来评估我们的BAM。BAM生成框架是通过组合两个U形生成器来设计的，从而为此分类器提供有意义的可解释性。主要生成器是根据可转诊扫描进行训练的。

    Deep learning classifiers provide the most accurate means of automatically diagnosing diabetic retinopathy (DR) based on optical coherence tomography (OCT) and its angiography (OCTA). The power of these models is attributable in part to the inclusion of hidden layers that provide the complexity required to achieve a desired task. However, hidden layers also render algorithm outputs difficult to interpret. Here we introduce a novel biomarker activation map (BAM) framework based on generative adversarial learning that allows clinicians to verify and understand classifiers decision-making. A data set including 456 macular scans were graded as non-referable or referable DR based on current clinical standards. A DR classifier that was used to evaluate our BAM was first trained based on this data set. The BAM generation framework was designed by combing two U-shaped generators to provide meaningful interpretability to this classifier. The main generator was trained to take referable scans as
    
[^164]: 可解释的性能：衡量预测性能的驱动力

    Explainable Performance: Measuring the Driving Forces of Predictive Performance. (arXiv:2212.05866v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.05866](http://arxiv.org/abs/2212.05866)

    XPER方法能衡量输入特征对模型预测性能的具体贡献，并可用于处理异质性问题，构建同质化个体群体，从而提高预测精度。

    

    我们引入了XPER（eXplainable PERformance）方法来衡量输入特征对模型预测性能的具体贡献。我们的方法在理论上基于Shapley值，既不依赖于模型，也不依赖于性能度量。此外，XPER可在模型级别或个体级别实现。我们证明XPER具有标准解释性方法（SHAP）的特殊情况。在贷款违约预测应用中，我们展示了如何利用XPER处理异质性问题，并显著提高样本外性能。为此，我们通过基于个体XPER值对他们进行聚类来构建同质化的个体群体。我们发现估计群体特定的模型比一个模型适用于所有个体具有更高的预测精度。

    We introduce the XPER (eXplainable PERformance) methodology to measure the specific contribution of the input features to the predictive performance of a model. Our methodology is theoretically grounded on Shapley values and is both model-agnostic and performance metric-agnostic. Furthermore, XPER can be implemented either at the model level or at the individual level. We demonstrate that XPER has as a special case the standard explainability method in machine learning (SHAP). In a loan default forecasting application, we show how XPER can be used to deal with heterogeneity issues and significantly boost out-of-sample performance. To do so, we build homogeneous groups of individuals by clustering them based on their individual XPER values. We find that estimating group-specific models yields a much higher predictive accuracy than with a one-fits-all model.
    
[^165]: 训练数据影响分析与估计：一项调查

    Training Data Influence Analysis and Estimation: A Survey. (arXiv:2212.04612v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04612](http://arxiv.org/abs/2212.04612)

    本文对训练数据影响分析与估计进行了全面调查，通过量化每个训练实例对最终模型的改变程度揭示了训练的基本相互作用。调查了当前的影响分析方法，并提出了未来的研究方向。

    

    好的模型需要好的训练数据。对于过度参数化的深度模型，训练数据与模型预测之间的因果关系变得越来越不透明和难以理解。通过量化每个训练实例对最终模型的改变程度，影响分析部分揭示了训练的基本相互作用。在最坏情况下，准确测量训练数据的影响力是有可能证明困难的；因此，发展和使用影响力估计器，仅对真实影响进行近似估计。本文首次全面调查了训练数据影响分析与估计。首先，我们系统化各种关于训练数据影响的定义，并在某些方面进行解析。然后，我们将最先进的影响分析方法分类整理；详细描述每种方法，比较其基本假设、渐近复杂度以及整体优缺点。最后，我们提出未来的研究方向。

    Good models require good training data. For overparameterized deep models, the causal relationship between training data and model predictions is increasingly opaque and poorly understood. Influence analysis partially demystifies training's underlying interactions by quantifying the amount each training instance alters the final model. Measuring the training data's influence exactly can be provably hard in the worst case; this has led to the development and use of influence estimators, which only approximate the true influence. This paper provides the first comprehensive survey of training data influence analysis and estimation. We begin by formalizing the various, and in places orthogonal, definitions of training data influence. We then organize state-of-the-art influence analysis methods into a taxonomy; we describe each of these methods in detail and compare their underlying assumptions, asymptotic complexities, and overall strengths and weaknesses. Finally, we propose future resear
    
[^166]: 带有神经网络和索引的聚类模型

    Clustering with Neural Network and Index. (arXiv:2212.03853v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03853](http://arxiv.org/abs/2212.03853)

    介绍了一种新的带有神经网络和索引的聚类模型CNNI，该模型使用神经网络对数据点进行聚类，实现了第一个能够处理非凸形状数据的参数化聚类模型。

    

    介绍一种新的聚类模型，称为带有神经网络和索引的聚类模型（CNNI）。CNNI使用神经网络对数据点进行聚类。神经网络的训练模仿监督学习，使用内部聚类评估指标作为损失函数。进行了一项实验来测试新模型的可行性，并与K均值和高斯混合模型（GMM）等其他聚类模型的结果进行了比较。结果表明CNNI可以正确地对数据进行聚类；CNNI配备了MMJ-SC，成为第一个能够处理非凸形状（非平面几何）数据的参数化（归纳式）聚类模型。

    A new model called Clustering with Neural Network and Index (CNNI) is introduced. CNNI uses a Neural Network to cluster data points. Training of the Neural Network mimics supervised learning, with an internal clustering evaluation index acting as the loss function. An experiment is conducted to test the feasibility of the new model, and compared with results of other clustering models like K-means and Gaussian Mixture Model (GMM). The result shows CNNI can work properly for clustering data; CNNI equipped with MMJ-SC, achieves the first parametric (inductive) clustering model that can deal with non-convex shaped (non-flat geometry) data.
    
[^167]: 利用积流形进行潜在图推断

    Latent Graph Inference using Product Manifolds. (arXiv:2211.16199v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16199](http://arxiv.org/abs/2211.16199)

    本文提出了一种利用积流形进行推断的潜在图学习方法，以动态学习问题内在的图结构。通过使用Riemannian几何学和生成更复杂的嵌入空间，可以提高系统性能并产生更丰富的表示。

    

    图神经网络通常依赖于图拓扑结构对网络的可用性以及对下游任务的最优性。而潜在图推断允许模型动态学习问题内在的图结构，这些数据的连通性模式可能无法直接访问。本文将离散可微图模块（dDGM）推广到潜在图学习。原始的dDGM架构使用欧几里得平面来编码潜在特征，基于此生成潜在图。通过将黎曼几何引入模型并生成更复杂的嵌入空间，我们可以提高潜在图推断系统的性能。特别是，我们提出了一种计算上可行的方法来产生常曲率模型空间的积流形，可以编码不同结构的潜在特征。映射到推断出的积流形上的潜在表示用于计算更丰富的表示。

    Graph Neural Networks usually rely on the assumption that the graph topology is available to the network as well as optimal for the downstream task. Latent graph inference allows models to dynamically learn the intrinsic graph structure of problems where the connectivity patterns of data may not be directly accessible. In this work, we generalize the discrete Differentiable Graph Module (dDGM) for latent graph learning. The original dDGM architecture used the Euclidean plane to encode latent features based on which the latent graphs were generated. By incorporating Riemannian geometry into the model and generating more complex embedding spaces, we can improve the performance of the latent graph inference system. In particular, we propose a computationally tractable approach to produce product manifolds of constant curvature model spaces that can encode latent features of varying structure. The latent representations mapped onto the inferred product manifold are used to compute richer s
    
[^168]: SSD-LM: 基于半自回归简单形式的扩散语言模型用于文本生成和模块化控制

    SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control. (arXiv:2210.17432v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.17432](http://arxiv.org/abs/2210.17432)

    SSD-LM是一种半自回归的扩散语言模型，通过在解码时灵活生成文本块并实现本地上下文更新，以及在自然词汇空间上进行扩散，实现了分类器指导和模块化控制。在无约束文本生成基准上，SSD-LM与自回归模型相比，在质量和多样性方面表现出色，并且显著超越了其他基于扩散的模型。

    

    尽管扩散模型在连续值领域（如图像）取得了很大成功，但在离散领域（如文本）中，类似的努力却还没有达到自回归语言模型的性能水平。在这项工作中，我们提出了 SSD-LM，一个基于扩散的语言模型，使用了两个关键设计选择。首先，SSD-LM是半自回归的，通过迭代生成文本块，在解码时允许灵活的输出长度，同时实现本地双向上下文更新。其次，它采用基于单纯形的方法，在自然词汇空间上进行扩散，而不是在学习到的潜在空间上，这使得我们能够在不进行任何自适应的情况下，利用现成的分类器实现分类器指导和模块化控制。我们在无约束文本生成基准上评估了 SSD-LM，并展示它在标准质量和多样性度量上与强大的自回归 GPT-2 模型相匹配或超越，并且远远优于基于扩散的基准模型。

    Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM -- a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generatio
    
[^169]: 实时强化学习用于基于视觉的机器人，利用本地和远程计算机

    Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers. (arXiv:2210.02317v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.02317](http://arxiv.org/abs/2210.02317)

    本文实现了一个名为ReLoD的实时学习系统，利用本地和远程计算机来分配深度强化学习算法的计算，并在基于视觉的控制任务上进行了评估。结果显示SAC的性能下降了。

    

    实时学习对于适应不断变化的非稳定环境的机器人代理至关重要。一个常见的机器人代理设置是同时有两台不同的计算机：与机器人相连的资源受限本地计算机和无线连接的强大远程计算机。在这样的设置下，尚不清楚学习系统的性能在资源限制方面能受到多大的影响，以及如何高效利用无线连接的强大计算机来弥补性能损失。在本文中，我们实现了一个实时学习系统，称为Remote-Local Distributed (ReLoD)系统，用于在本地和远程计算机之间分配两个深度强化学习算法Soft Actor-Critic (SAC)和Proximal Policy Optimization (PPO)的计算。该系统的性能在使用机械臂和移动机器人开发的两个基于视觉的控制任务上进行了评估。我们的结果表明SAC的性能下降了。

    Real-time learning is crucial for robotic agents adapting to ever-changing, non-stationary environments. A common setup for a robotic agent is to have two different computers simultaneously: a resource-limited local computer tethered to the robot and a powerful remote computer connected wirelessly. Given such a setup, it is unclear to what extent the performance of a learning system can be affected by resource limitations and how to efficiently use the wirelessly connected powerful computer to compensate for any performance loss. In this paper, we implement a real-time learning system called the Remote-Local Distributed (ReLoD) system to distribute computations of two deep reinforcement learning (RL) algorithms, Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO), between a local and a remote computer. The performance of the system is evaluated on two vision-based control tasks developed using a robotic arm and a mobile robot. Our results show that SAC's performance degrades
    
[^170]: CADet:全自监督对比学习用于带有对比学习的全自主分布检测

    CADet: Fully Self-Supervised Out-Of-Distribution Detection With Contrastive Learning. (arXiv:2210.01742v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01742](http://arxiv.org/abs/2210.01742)

    本文介绍了一种使用自监督对比学习进行带有对比学习的全自主分布检测的方法，能够同时检测未见过的类别和对抗性扰动样本。通过将自监督对比学习与最大均值差异（MMD）相结合，提出了CADet方法，该方法通过利用同一样本的对比变换之间的相似性进行OOD检测，并在对抗性扰动的识别方面比现有方法表现更好。

    

    处理带有分布之外（OOD）样本已成为机器学习系统在现实世界部署中的主要问题。本文探讨了使用自监督对比学习同时检测两种类型的OOD样本：未见过的类别和对抗性扰动。首先，我们将自监督对比学习与最大均值差异（MMD）双样本检验相结合。这种方法使我们能够鲁棒地测试两个独立样本集是否来自相同的分布，并且我们证明了相较于之前的工作，它在区分CIFAR-10和CIFAR-10.1时具有更高的置信度。在此成功的基础上，我们引入了CADet（对比异常检测），一种用于单样本OOD检测的新方法。CADet借鉴了MMD的思想，但利用了同一样本的对比变换之间的相似性。CADet在识别被对抗性扰动干扰的样本方面胜过现有的对抗检测方法。

    Handling out-of-distribution (OOD) samples has become a major stake in the real-world deployment of machine learning systems. This work explores the use of self-supervised contrastive learning to the simultaneous detection of two types of OOD samples: unseen classes and adversarial perturbations. First, we pair self-supervised contrastive learning with the maximum mean discrepancy (MMD) two-sample test. This approach enables us to robustly test whether two independent sets of samples originate from the same distribution, and we demonstrate its effectiveness by discriminating between CIFAR-10 and CIFAR-10.1 with higher confidence than previous work. Motivated by this success, we introduce CADet (Contrastive Anomaly Detection), a novel method for OOD detection of single samples. CADet draws inspiration from MMD, but leverages the similarity between contrastive transformations of a same sample. CADet outperforms existing adversarial detection methods in identifying adversarially perturbed
    
[^171]: MGG: 多GPU平台上通过精细的内核通信计算流水线加速图神经网络

    MGG: Accelerating Graph Neural Networks with Fine-grained intra-kernel Communication-Computation Pipelining on Multi-GPU Platforms. (arXiv:2209.06800v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2209.06800](http://arxiv.org/abs/2209.06800)

    MGG是一种软件流水线设计，可在多GPU平台上加速GNNs，通过采用GNN特殊的流水线构建和GPU感知的流水线映射，实现精细计算通信重叠以提高性能。

    

    图神经网络（GNNs）输入图的大小越来越大，需要使用多GPU平台。然而，现有的多GPU GNN系统仅基于传统做法缩放稠密DNN，优化计算和通信操作，对于不规则稀疏的GNN工作负载，缺失同时调度优化计算和通信操作以提高性能的机会。因此，我们提出了MGG，这是一种新的系统设计，可以在多GPU平台上加速全图GNN。MGG的核心是其新颖的动态软件流水线，以促进GPU内部精细的计算通信重叠。具体而言，MGG引入了适用于GNN的流水线构建和GPU感知的流水线映射，以促进工作负载平衡和操作重叠。MGG还结合了智能的运行时设计和分析建模和优化启发式方法，以动态改进性能。

    The increasing size of input graphs for graph neural networks (GNNs) highlights the demand for using multi-GPU platforms. However, existing multi-GPU GNN systems optimize the computation and communication individually based on the conventional practice of scaling dense DNNs. For irregularly sparse and fine-grained GNN workloads, such solutions miss the opportunity to jointly schedule/optimize the computation and communication operations for high-performance delivery. To this end, we propose MGG, a novel system design to accelerate full-graph GNNs on multi-GPU platforms. The core of MGG is its novel dynamic software pipeline to facilitate fine-grained computation-communication overlapping within a GPU kernel. Specifically, MGG introduces GNN-tailored pipeline construction and GPU-aware pipeline mapping to facilitate workload balancing and operation overlapping. MGG also incorporates an intelligent runtime design with analytical modeling and optimization heuristics to dynamically improve
    
[^172]: 基于时间不一致的自监督探索在强化学习中的应用

    Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning. (arXiv:2208.11361v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11361](http://arxiv.org/abs/2208.11361)

    本文在强化学习中提出了一种新的内在奖励方法，通过比较当前观察与历史知识的差异来评估好奇心，并利用时间不一致性作为内在奖励。实验证明该方法在稀疏外在奖励的情况下具有更高的性能和噪声容忍度。

    

    在稀疏外在奖励的情况下，强化学习仍然具有挑战性，尽管对这个领域的兴趣不断增加。先前的尝试表明，内在奖励可以缓解稀疏性带来的问题。在本文中，我们提出了一种受人类学习启发的新型内在奖励，人类通过将当前观察与历史知识进行比较来评估好奇心。我们的方法涉及训练一个自监督预测模型，保存模型参数的快照，并使用核范数来评估不同快照之间预测的时间不一致性作为内在奖励。我们还提出了一种变分加权机制，以自适应的方式为不同的快照分配权重。我们在各种基准环境上的实验结果表明了我们方法的有效性，该方法在没有额外训练成本且具有更高噪声容忍度的情况下优于其他基于内在奖励的方法。

    Under sparse extrinsic reward settings, reinforcement learning has remained challenging, despite surging interests in this field. Previous attempts suggest that intrinsic reward can alleviate the issue caused by sparsity. In this article, we present a novel intrinsic reward that is inspired by human learning, as humans evaluate curiosity by comparing current observations with historical knowledge. Our method involves training a self-supervised prediction model, saving snapshots of the model parameters, and using nuclear norm to evaluate the temporal inconsistency between the predictions of different snapshots as intrinsic rewards. We also propose a variational weighting mechanism to assign weight to different snapshots in an adaptive manner. Our experimental results on various benchmark environments demonstrate the efficacy of our method, which outperforms other intrinsic reward-based methods without additional training costs and with higher noise tolerance. This work has been submitte
    
[^173]: 异构任务在机器学习驱动的高性能计算工作流中的异步执行

    Asynchronous Execution of Heterogeneous Tasks in ML-driven HPC Workflows. (arXiv:2208.11069v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2208.11069](http://arxiv.org/abs/2208.11069)

    该论文研究了机器学习驱动的高性能计算工作流中异步任务执行的要求和特性，并提出了用于确定异步执行收益的关键指标。通过在Summit上进行大规模实验，作者证明了异步执行对性能的增强与模型一致。

    

    异构科学工作流由许多类型的任务组成，这些任务需要在异构资源上执行。异步执行这些任务对于提高资源利用率、任务吞吐量和减少工作流的时间非常重要。因此，能够在异构资源上调度和执行不同类型任务的中间件必须支持异步执行任务。本文研究了机器学习驱动的高性能计算工作流的异步任务执行的要求和特性。我们模型化了任意工作流的异步性程度，并提出了可用于确定异步执行带来的定性收益的关键指标。我们的实验代表了相关科学驱动，我们在Summit上进行了大规模实验，并且我们展示了由于异步执行所带来的性能增强与我们的模型一致。

    Heterogeneous scientific workflows consist of numerous types of tasks that require executing on heterogeneous resources. Asynchronous execution of those tasks is crucial to improve resource utilization, task throughput and reduce workflows' makespan. Therefore, middleware capable of scheduling and executing different task types across heterogeneous resources must enable asynchronous execution of tasks. In this paper, we investigate the requirements and properties of the asynchronous task execution of machine learning (ML)-driven high performance computing (HPC) workflows. We model the degree of asynchronicity permitted for arbitrary workflows and propose key metrics that can be used to determine qualitative benefits when employing asynchronous execution. Our experiments represent relevant scientific drivers, we perform them at scale on Summit, and we show that the performance enhancements due to asynchronous execution are consistent with our model.
    
[^174]: 事件触发的时变贝叶斯优化

    Event-Triggered Time-Varying Bayesian Optimization. (arXiv:2208.10790v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10790](http://arxiv.org/abs/2208.10790)

    本文提出了一种事件触发算法ET-GP-UCB，用于解决时变贝叶斯优化中的探索和开发的权衡问题。通过基于高斯过程回归的概率均匀误差界，算法能够在未知变化速率的情况下自适应地适应实际的时间变化。数值实验结果表明，ET-GP-UCB在合成和实际数据上优于现有算法。

    

    我们考虑使用时变贝叶斯优化（TVBO）顺序优化时变目标函数的问题。其中，关键挑战是在时间变化下的勘探与开发的权衡。当前的TVBO方法需要对变化速率有先验知识。然而，在实践中，变化速率通常是未知的。我们提出了一种事件触发算法ET-GP-UCB，它将优化问题视为静态问题，直到在线检测到目标函数的变化并重置数据集。这使得算法能够适应实际的时间变化，而不需要先验知识。事件触发基于高斯过程回归中使用的概率均匀误差界。我们给出了ET-GP-UCB的遗憾界，并通过数值实验表明，它在合成和实际数据上优于现有算法。此外，这些结果表明ET-GP-UCB可广泛应用于不同的设定。

    We consider the problem of sequentially optimizing a time-varying objective function using time-varying Bayesian optimization (TVBO). Here, the key challenge is the exploration-exploitation trade-off under time variations. Current approaches to TVBO require prior knowledge of a constant rate of change. However, in practice, the rate of change is usually unknown. We propose an event-triggered algorithm, ET-GP-UCB, that treats the optimization problem as static until it detects changes in the objective function online and then resets the dataset. This allows the algorithm to adapt to realized temporal changes without the need for prior knowledge. The event-trigger is based on probabilistic uniform error bounds used in Gaussian process regression. We provide regret bounds for ET-GP-UCB and show in numerical experiments that it outperforms state-of-the-art algorithms on synthetic and real-world data. Furthermore, these results demonstrate that ET-GP-UCB is readily applicable to various set
    
[^175]: 压缩型多语言机器翻译模型会忽略什么？

    What Do Compressed Multilingual Machine Translation Models Forget?. (arXiv:2205.10828v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.10828](http://arxiv.org/abs/2205.10828)

    本研究评估了压缩方法对多语言神经机器翻译模型在不同语言群体、性别和语义偏差方面的影响，并发现代表性不足的语言性能显著下降。

    

    最近，非常庞大的预训练模型在各种自然语言处理（NLP）任务中取得了最先进的结果，但是它们的大小使得在资源受限的环境中应用它们更具挑战性。压缩技术可以大幅减小模型的大小，从而减少推理时间，并对顶级指标几乎没有影响。然而，对多个任务和/或语言进行平均的综合性能可能掩盖了在代表性不足的功能上的严重性能下降，这可能导致模型所编码的偏见的放大。在这项工作中，我们通过对不同机器翻译基准（FLORES-101、MT-Gender和DiBiMT）上的压缩模型进行全面分析，评估了压缩方法对多语言神经机器翻译模型（MNMT）在不同语言群体、性别和语义偏差方面的影响。我们发现，代表性不足的语言的性能显著下降，而平均BLEU度量值则没什么变化。

    Recently, very large pre-trained models achieve state-of-the-art results in various natural language processing (NLP) tasks, but their size makes it more challenging to apply them in resource-constrained environments. Compression techniques allow to drastically reduce the size of the models and therefore their inference time with negligible impact on top-tier metrics. However, the general performance averaged across multiple tasks and/or languages may hide a drastic performance drop on under-represented features, which could result in the amplification of biases encoded by the models. In this work, we assess the impact of compression methods on Multilingual Neural Machine Translation models (MNMT) for various language groups, gender, and semantic biases by extensive analysis of compressed models on different machine translation benchmarks, i.e. FLORES-101, MT-Gender, and DiBiMT. We show that the performance of under-represented languages drops significantly, while the average BLEU metr
    
[^176]: 双线性价值网络

    Bilinear value networks. (arXiv:2204.13695v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.13695](http://arxiv.org/abs/2204.13695)

    提出了一种通过点积低秩近似来表示Q值的双线性分解方法，其中第一个向量场捕捉状态的局部动态，第二个部分捕捉当前状态和目标之间的全局关系，该方法能够显著提高数据效率，并具有很好的泛化性能。

    

    多目标强化学习的主流框架涉及估计目标条件下的Q值函数。当学习实现多个不同目标时，数据效率与Q函数对新目标的泛化密切相关。目前的范式是使用单一的神经网络来近似Q(s, a, g)。为了改进Q函数的泛化，我们提出了一种双线性分解方法，通过两个向量场之间的点积的低秩近似来表示Q值。第一个向量场f(s, a)捕捉状态s处的环境局部动态；而第二个部分{ϕ}(s, g)则捕捉当前状态和目标之间的全局关系。我们展示了双线性分解方案显著提高了数据效率，相比先前的方法，对于处于分布范围之外的目标具有更好的转移性。我们在模拟的Fetch机器人任务套件和DeepMind Control Suite上提供了实证证据。

    The dominant framework for off-policy multi-goal reinforcement learning involves estimating goal conditioned Q-value function. When learning to achieve multiple goals, data efficiency is intimately connected with the generalization of the Q-function to new goals. The de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks. To improve the generalization of the Q-function, we propose a bilinear decomposition that represents the Q-value via a low-rank approximation in the form of a dot product between two vector fields. The first vector field, f(s, a), captures the environment's local dynamics at the state s; whereas the second component, {\phi}(s, g), captures the global relationship between the current state and the goal. We show that our bilinear decomposition scheme substantially improves data efficiency, and has superior transfer to out-of-distribution goals compared to prior methods. Empirical evidence is provided on the simulated Fetch robot task-suite and d
    
[^177]: 拓扑经验回放

    Topological Experience Replay. (arXiv:2203.15845v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.15845](http://arxiv.org/abs/2203.15845)

    本文提出了一种拓扑经验回放的方法，通过构建图来明确状态的 Q 值之间的依赖关系，解决了传统采样策略忽视状态间依赖关系的问题，提高了学习深度 Q 函数时的性能和准确性。

    

    最先进的深度 Q 学习方法使用从经验重放缓冲区中采样的状态转换元组更新 Q 值。这种策略通常均匀和随机地采样，或基于诸如时间差（TD）误差等度量优先。这样的采样策略在学习 Q 函数时可能效率低下，因为一个状态的 Q 值取决于继承状态的 Q 值。如果数据采样策略忽略了下一个状态的 Q 值估计的精度，它可能会导致无用和常常不正确的 Q 值更新。为了减轻这个问题，我们将智能体的经验组织成一个图，明确跟踪状态的 Q 值之间的依赖关系。图中的每条边代表通过执行单个操作在两个状态之间的转换。我们通过从一组终端状态开始扩展图中的顶点，并逐步向后移动的广度优先搜索来执行值备份。

    State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often uniformly and randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function because a state's Q-value depends on the Q-value of successor states. If the data sampling strategy ignores the precision of the Q-value estimate of the next state, it can lead to useless and often incorrect updates to the Q-values. To mitigate this issue, we organize the agent's experience into a graph that explicitly tracks the dependency between Q-values of states. Each edge in the graph represents a transition between two states by executing a single action. We perform value backups via a breadth-first search starting from that expands vertices in the graph starting from the set of terminal states and successively moving backward. We empirically sho
    
[^178]: 使用TreeSHAP的可解释人工智能集成特征选择进行滑坡易发性制图

    Explainable AI Integrated Feature Selection for Landslide Susceptibility Mapping using TreeSHAP. (arXiv:2201.03225v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.03225](http://arxiv.org/abs/2201.03225)

    本研究使用最先进的机器学习方法来描述滑坡易发性，并通过XgBoost算法和TreeSHAP方法进行优化。经过实验证明，优化后的XgBoost表现优于其他分类器，具有较高的交叉验证加权F1分数。

    

    在人为全球变暖时代，滑坡频繁发生并对人类生命和财产构成威胁。利用数据驱动方法进行早期滑坡易发性预测是时代的需求。本研究探索了最能描述滑坡易发性的优秀特征，并采用了最先进的机器学习方法进行分析。我们使用了XgBoost、LR、KNN、SVM和Adaboost等最先进的机器学习算法进行滑坡易发性预测。为了找到每个分类器的最佳超参数以达到最优性能，我们采用了网格搜索方法和10折交叉验证。在此情况下，经过优化的XgBoost表现优于其他所有分类器，交叉验证加权F1分数达到94.62％。在这个经验的基础上，我们通过引入TreeSHAP方法探索了XgBoost分类器，该方法是一种基于博弈论的统计算法，用于解释机器学习模型。

    Landslides have been a regular occurrence and an alarming threat to human life and property in the era of anthropogenic global warming. An early prediction of landslide susceptibility using a data-driven approach is a demand of time. In this study, we explored the eloquent features that best describe landslide susceptibility with state-of-the-art machine learning methods. In our study, we employed state-of-the-art machine learning algorithms including XgBoost, LR, KNN, SVM, and Adaboost for landslide susceptibility prediction. To find the best hyperparameters of each individual classifier for optimized performance, we have incorporated the Grid Search method, with 10 Fold Cross-Validation. In this context, the optimized version of XgBoost outperformed all other classifiers with a Cross-validation Weighted F1 score of 94.62 %. Followed by this empirical evidence, we explored the XgBoost classifier by incorporating TreeSHAP, a game-theory-based statistical algorithm used to explain Machi
    
[^179]: 双重PC算法及其对高斯性质在贝叶斯网络结构学习中的作用

    The Dual PC Algorithm and the Role of Gaussianity for Structure Learning of Bayesian Networks. (arXiv:2112.09036v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.09036](http://arxiv.org/abs/2112.09036)

    双重PC算法通过利用协方差和精度矩阵之间的反向关系，实现了CI测试，能够恢复正确的等价类，并可对互补调节集的偏相关进行测试。

    

    学习贝叶斯网络的图形结构是描述许多复杂应用程序中的数据生成机制的关键，但面临着巨大的计算挑战。在某些假设下，流行的PC算法可以通过逆向工程变量分布中所具有的条件独立关系来一致地恢复正确的等价类。双重PC算法是一种新颖的方案，通过利用协方差和精度矩阵之间的反向关系来进行PC算法中的CI测试。通过利用块矩阵求逆，我们还可以对互补（或双重）调节集的偏相关进行测试。双重PC算法的多个CI测试首先考虑边缘和完全排序CI关系。

    Learning the graphical structure of Bayesian networks is key to describing data-generating mechanisms in many complex applications but poses considerable computational challenges. Observational data can only identify the equivalence class of the directed acyclic graph underlying a Bayesian network model, and a variety of methods exist to tackle the problem. Under certain assumptions, the popular PC algorithm can consistently recover the correct equivalence class by reverse-engineering the conditional independence (CI) relationships holding in the variable distribution. The dual PC algorithm is a novel scheme to carry out the CI tests within the PC algorithm by leveraging the inverse relationship between covariance and precision matrices. By exploiting block matrix inversions we can also perform tests on partial correlations of complementary (or dual) conditioning sets. The multiple CI tests of the dual PC algorithm proceed by first considering marginal and full-order CI relationships a
    
[^180]: 带有动量的随机近端点算法的收敛性和稳定性研究

    Convergence and Stability of the Stochastic Proximal Point Algorithm with Momentum. (arXiv:2111.06171v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2111.06171](http://arxiv.org/abs/2111.06171)

    本论文研究了带有动量的随机近端点算法（SPPAM）的收敛性和稳定性，展示了SPPAM相比于随机近端点算法（SPPA）具有更快的线性收敛和更好的收缩因子。

    

    随机梯度下降与动量（SGDM）是许多优化场景中的主要算法，包括凸优化和非凸神经网络训练。然而，在随机设置中，动量会干扰梯度噪声，通常需要特定的步长和动量选择才能保证收敛，但加速往往被忽略。相对而言，近端点方法因其数值稳定性和对不完美调整的弹性而受到了广泛关注。然而，它们的随机加速变体却受到了限制的关注：动量如何影响（随机）近端点方法的稳定性仍然未经研究。为了解决这个问题，我们关注带有动量的随机近端点算法（SPPAM）的收敛性和稳定性，并展示了SPPAM相比于带有更好收缩因子的随机近端点算法（SPPA）具有更快的线性收敛到一个邻域。

    Stochastic gradient descent with momentum (SGDM) is the dominant algorithm in many optimization scenarios, including convex optimization instances and non-convex neural network training. Yet, in the stochastic setting, momentum interferes with gradient noise, often leading to specific step size and momentum choices in order to guarantee convergence, set aside acceleration. Proximal point methods, on the other hand, have gained much attention due to their numerical stability and elasticity against imperfect tuning. Their stochastic accelerated variants though have received limited attention: how momentum interacts with the stability of (stochastic) proximal point methods remains largely unstudied. To address this, we focus on the convergence and stability of the stochastic proximal point algorithm with momentum (SPPAM), and show that SPPAM allows a faster linear convergence to a neighborhood compared to the stochastic proximal point algorithm (SPPA) with a better contraction factor, und
    
[^181]: 何时需要上下文翻译？一项数据驱动的、多语言的探索

    When Does Translation Require Context? A Data-driven, Multilingual Exploration. (arXiv:2109.07446v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2109.07446](http://arxiv.org/abs/2109.07446)

    本研究开发了多语言语篇感知基准，系统性地确定了需要上下文翻译的现象。发现上下文感知机器翻译模型对于解决这些现象的困难程度有限，为进一步研究提供了挑战。

    

    虽然正确处理语篇对机器翻译的质量有很大影响，但这些改进在常见的翻译质量评估指标中得不到充分衡量。最近的上下文感知机器翻译研究试图针对一小部分语篇现象进行评估，但缺乏完全系统化的方式。在本文中，我们开发了多语言语篇感知（MuDA）基准，这是一系列标记器，用于识别和评估给定数据集中的语篇现象的模型性能。选择的语篇现象受到一种新颖的方法的启发，该方法可系统地确定需要上下文的翻译。我们确认了之前研究的语篇现象的难度，并揭示了之前未解决的其他现象。我们发现常见的上下文感知机器翻译模型仅对上下文无关模型进行了轻微的改进，这表明这些模型并没有有效处理这些歧义。我们发布了14种语言对的代码和数据，以鼓励进一步研究。

    Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations requiring context. We confirm the difficulty of previously studied phenomena while uncovering others that were previously unaddressed. We find that common context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage th
    
[^182]: FedPower: 隐私保护的分布式特征空间估计

    FedPower: Privacy-Preserving Distributed Eigenspace Estimation. (arXiv:2103.00704v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2103.00704](http://arxiv.org/abs/2103.00704)

    本文提出了一种称为FedPower的算法，在联邦学习框架内解决了特征空间估计的隐私和通信效率问题。算法利用幂法进行本地迭代和全局聚合，采用正交Procrustes变换加权以实现对齐，并引入差分隐私以保护数据隐私。

    

    特征空间估计是机器学习和统计学中的基础问题，在主成分分析、维度约简和聚类等领域都有应用。现代机器学习通常假设数据来自不同的组织且属于不同组织。数据的低通信能力和可能的隐私泄漏使得特征空间的计算具有挑战性。为了解决这些挑战，我们在联邦学习（FL）框架内提出了一类算法，称为FedPower。FedPower利用了著名的幂法，通过交替进行多个本地幂迭代和全局聚合步骤，从而提高通信效率。在聚合中，我们提出使用正交Procrustes变换（OPT）对每个本地特征向量矩阵进行加权以实现更好的对齐。为了确保强大的隐私保护，我们在每次迭代中添加高斯噪声，采用差分隐私（DP）的概念。我们提供了...

    Eigenspace estimation is fundamental in machine learning and statistics, which has found applications in PCA, dimension reduction, and clustering, among others. The modern machine learning community usually assumes that data come from and belong to different organizations. The low communication power and the possible privacy breaches of data make the computation of eigenspace challenging. To address these challenges, we propose a class of algorithms called \textsf{FedPower} within the federated learning (FL) framework. \textsf{FedPower} leverages the well-known power method by alternating multiple local power iterations and a global aggregation step, thus improving communication efficiency. In the aggregation, we propose to weight each local eigenvector matrix with {\it Orthogonal Procrustes Transformation} (OPT) for better alignment. To ensure strong privacy protection, we add Gaussian noise in each iteration by adopting the notion of \emph{differential privacy} (DP). We provide conve
    
[^183]: 在工具变量回归中学习深度特征

    Learning Deep Features in Instrumental Variable Regression. (arXiv:2010.07154v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.07154](http://arxiv.org/abs/2010.07154)

    本论文提出了一种名为深度特征工具变量回归（DFIV）的方法，用于处理观测数据中工具变量、处理变量和结果变量之间的非线性关系。通过训练深度神经网络来定义工具变量和处理变量上的非线性特征，我们提供了一种交替训练机制以获得良好的端到端性能。这种方法可以在计算上获得高度灵活的特征映射。

    

    工具变量（IV）回归是一种在观测数据中利用仅通过处理影响结果的工具变量来学习混淆的处理和结果变量之间因果关系的标准策略。在传统的IV回归中，学习分为两个阶段：阶段1从工具变量到处理变量进行线性回归；阶段2在工具变量的条件下，从处理变量到结果变量进行线性回归。我们提出了一种新的方法，即深度特征工具变量回归（DFIV），用于处理工具变量、处理变量和结果变量之间可能是非线性关系的情况。在这种情况下，我们训练深度神经网络来定义工具变量和处理变量上的信息性非线性特征。我们提出了一种交替训练机制来训练这些特征，以保证通过组合阶段1和阶段2获得良好的端到端性能，从而在计算上获得高度灵活的特征映射。

    Instrumental variable (IV) regression is a standard strategy for learning causal relationships between confounded treatment and outcome variables from observational data by utilizing an instrumental variable, which affects the outcome only through the treatment. In classical IV regression, learning proceeds in two stages: stage 1 performs linear regression from the instrument to the treatment; and stage 2 performs linear regression from the treatment to the outcome, conditioned on the instrument. We propose a novel method, deep feature instrumental variable regression (DFIV), to address the case where relations between instruments, treatments, and outcomes may be nonlinear. In this case, deep neural nets are trained to define informative nonlinear features on the instruments and treatments. We propose an alternating training regime for these features to ensure good end-to-end performance when composing stages 1 and 2, thus obtaining highly flexible feature maps in a computationally eff
    
[^184]: 具有持续终身学习的神经主题建模

    Neural Topic Modeling with Continual Lifelong Learning. (arXiv:2006.10909v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2006.10909](http://arxiv.org/abs/2006.10909)

    本研究提出了一个具有持续终身学习的神经主题建模框架，可以处理数据稀疏性，并通过知识的持续积累和转移来提高主题建模的效果。

    

    终身学习吸引了人们对建立机器学习系统的关注，这些系统可以不断积累和传递知识，以帮助未来的学习。无监督主题建模广泛用于从文档集合中发现主题。然而，由于数据稀疏性，例如在一个小的（短）文档集合中，主题建模的应用具有挑战性，从而产生不连贯的主题和次优的文档表示。为了解决这个问题，我们提出了一个神经主题建模的终身学习框架，可以持续处理文档集合的流，积累主题，并通过从多个源的知识转移指导未来的主题建模任务，以更好地处理稀疏数据。在终身过程中，我们特别研究了共享的生成同源性（潜在主题）以在终身中传递先前的知识，以及通过新的选择性遗忘来最小化灾难性遗忘，以保留过去的学习。

    Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel sele
    
[^185]: 可解释的、对话主题感知的神经语言理解

    Explainable and Discourse Topic-aware Neural Language Understanding. (arXiv:2006.10632v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2006.10632](http://arxiv.org/abs/2006.10632)

    该论文提出了一个新颖的神经复合语言模型，通过引入可解释性的主题表示和句子级的主题对话，将主题模型和语言模型相结合。实验结果表明，在多个任务上，该模型显示出了良好的性能。

    

    将主题模型与语言模型相结合，通过主题将语言理解扩展到句子之外的更广泛的文档级上下文。现有的方法在语言模型中引入了主题语义，但忽略了文档中句子的主题对话。本研究通过为每个潜在主题的比例相对应地引入可解释性主题表示来扩展研究领域。此外，我们通过为文档中的每个句子建模主题对话，保留句子-主题关联以及文档-主题关联。我们提出了一种新颖的神经复合语言模型，在主题模型和语言模型的联合学习框架中同时利用潜在主题和可解释主题以及句子级的主题对话。我们在多个任务上进行实验，如语言建模、词义消歧等。

    Marrying topic models and language models exposes language understanding to a broader source of document-level context beyond sentences via topics. While introducing topical semantics in language models, existing approaches incorporate latent document topic proportions and ignore topical discourse in sentences of the document. This work extends the line of research by additionally introducing an explainable topic representation in language understanding, obtained from a set of key terms correspondingly for each latent topic of the proportion. Moreover, we retain sentence-topic associations along with document-topic association by modeling topical discourse for every sentence in the document. We present a novel neural composite language model that exploits both the latent and explainable topics along with topical discourse at sentence-level in a joint learning framework of topic and language models. Experiments over a range of tasks such as language modeling, word sense disambiguation, 
    

