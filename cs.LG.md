# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Control-Centric Benchmark for Video Prediction.](http://arxiv.org/abs/2304.13723) | 本文提出了一个以控制为中心的视频预测基准，评估给定模型在通过采样规划对模拟机器人操作的表现。该基准包含有11个任务类别和310个任务实例定义的模拟环境，以及完整的规划实现和训练数据集，以解决现有指标在预测任务执行成功方面不可靠的问题。 |
| [^2] | [Sparsified Model Zoo Twins: Investigating Populations of Sparsified Neural Network Models.](http://arxiv.org/abs/2304.13718) | 本文研究了两种稀疏化方法在模型族中的表现，并发现幅度修剪方法优于变分丢失方法，除了高于80%的较高稀疏化比率之外，两种方法都非常稳健。 |
| [^3] | [Association Rules Mining with Auto-Encoders.](http://arxiv.org/abs/2304.13717) | 本文提出了一种基于自编码器的关联规则挖掘算法ARM-AE，可在保证规则质量的前提下，挖掘出高支持度和置信度规则集，并具有比传统方法更好的执行时间。 |
| [^4] | [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.](http://arxiv.org/abs/2304.13712) | 本文提供了一个LLMs的使用综述，探讨了在各种自然语言处理任务中的使用和限制。 |
| [^5] | [Hopfield model with planted patterns: a teacher-student self-supervised learning model.](http://arxiv.org/abs/2304.13710) | 该论文提出了一种基于师生自我监督学习问题的Hopfield模型，能够帮助机器利用结构化的模式来学习，虽然一些条件对于学习非常重要，但这种学习模式在特定条件下可以实现泛化。 |
| [^6] | [Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware.](http://arxiv.org/abs/2304.13705) | 本文介绍了一种低成本系统，它能够在没有高端机器人、准确的传感器或精心的校准的情况下进行精细双手操作的模仿学习。作者开发了一种名为Action Chunking with Transformers (ACT)的新颖算法，它学习了一个带有动作序列的生成模型，从而允许机器人学习复杂的双手操作。 |
| [^7] | [Measuring Bias in AI Models with Application to Face Biometrics: An Statistical Approach.](http://arxiv.org/abs/2304.13680) | 本文提出使用N-Sigma方法来统计测量机器学习模型中的偏差，以开发基于偏差分析的新风险评估框架，尤其在人脸识别技术方面具有一定的优势和缺点。 |
| [^8] | [Learning battery model parameter dynamics from data with recursive Gaussian process regression.](http://arxiv.org/abs/2304.13666) | 本文提出了一种结合数据和模型驱动技术的混合方法来估计电池健康状况。通过递归高斯过程回归学习参数动态，并且对间隙和变化的操作条件具有鲁棒性。 |
| [^9] | [Data-driven Piecewise Affine Decision Rules for Stochastic Programming with Covariate Information.](http://arxiv.org/abs/2304.13646) | 本研究提出一种嵌入非凸分段仿射决策规则的经验风险最小化方法，用于学习特征与最优决策之间的直接映射。所提出的方法可用于广泛的非凸型SP问题，并且在数值研究中表现出优越的性能。 |
| [^10] | [PVP: Pre-trained Visual Parameter-Efficient Tuning.](http://arxiv.org/abs/2304.13639) | 本文提出了一种名为PVP的新方法，通过利用预训练模型的统计信息和无监督聚类方法初始化提示模块，可以在极少的标记数据（每类一到两个示例）的情况下获得具有竞争力的性能。 |
| [^11] | [ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries.](http://arxiv.org/abs/2304.13620) | 本文提出了ChartSumm数据集，用于长短摘要自动生成任务，包括84000多个图表及其元数据和描述。研究发现，现有的自动摘要模型虽然得分不错，但经常面临错觉、漏掉重要数据点以及不正确解释复杂趋势等问题。 |
| [^12] | [CROP: Towards Distributional-Shift Robust Reinforcement Learning using Compact Reshaped Observation Processing.](http://arxiv.org/abs/2304.13616) | CROP是一种新的强化学习算法，通过使用紧凑重塑观察处理来减少用于政策优化的状态信息，避免过度拟合特定的训练布局，并提高在未知环境中的泛化能力。 |
| [^13] | [Diffsurv: Differentiable sorting for censored time-to-event data.](http://arxiv.org/abs/2304.13594) | 为了处理带有审核任务的生存分析，我们提出了一种基于Diffsurv的新方法，通过预测可能组合矩阵，考虑到引入的标签不确定性，实现可区分的排序。 |
| [^14] | [Thompson Sampling Regret Bounds for Contextual Bandits with sub-Gaussian rewards.](http://arxiv.org/abs/2304.13593) | 本文研究了子高斯奖励情境下的 Thompson 抽样算法在情境 Bandit 问题中的性能，并引入了提高信息比率的新边界。 |
| [^15] | [Energy-Based Sliced Wasserstein Distance.](http://arxiv.org/abs/2304.13586) | 本文提出了一种能量为基础的切片Wasserstein距离，并将其参数化，以克服传统方法中的固定先验分布缺乏信息和优化最佳分布昂贵不稳定的局限。 |
| [^16] | ["I'm" Lost in Translation: Pronoun Missteps in Crowdsourced Data Sets.](http://arxiv.org/abs/2304.13557) | 这项研究发现在英语和日语之间的翻译中存在男性代词偏见，同时也检测到了对女性、中性和/或非二元代词存在的微妙反应的偏见。他们提出了针对代词翻译偏见的问题，并提供了将复数嵌入NLP数据集的解决方案。 |
| [^17] | [FLCC: Efficient Distributed Federated Learning on IoMT over CSMA/CA.](http://arxiv.org/abs/2304.13549) | 本文研究了在无线网络中实现FLCC的方法，该方法利用频率复用和空间聚类技术，提高了分布式学习的吞吐量和性能。 |
| [^18] | [Killing Two Birds with One Stone: Quantization Achieves Privacy in Distributed Learning.](http://arxiv.org/abs/2304.13545) | 本文提出了一种分布式学习中的综合解决方案，基于量化同时实现通讯效率和隐私保护，并且向均匀量化的梯度添加二项噪声以达到所需的差分隐私级别。 |
| [^19] | [Byzantine-Resilient Learning Beyond Gradients: Distributing Evolutionary Search.](http://arxiv.org/abs/2304.13540) | 本文介绍了一种新的拜占庭容错ML定义——模型共识，并展示了如何利用一类无梯度ML算法和经典分布式共识算法生成无梯度拜占庭容错学习算法。 |
| [^20] | [Tensor Decomposition for Model Reduction in Neural Networks: A Review.](http://arxiv.org/abs/2304.13539) | 本文综述了六种张量分解方法，并讨论了将神经网络层替换为低秩张量逼近的方案。实验结果表明这种方法能够显著降低模型大小、运行时间和能量消耗，并且适合于在边缘设备上实现神经网络。 |
| [^21] | [Bridging the Gap: Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models.](http://arxiv.org/abs/2304.13536) | 本文使用眼动事件作为可解释概念，对深度神经序列模型进行解释，研究结果表明扫视输入特征比固定点输入特征更为重要，且接近扫视峰值速度的注视样本最具影响力。 |
| [^22] | [A mean-field games laboratory for generative modeling.](http://arxiv.org/abs/2304.13534) | 本文提出了使用均场博弈作为实验室对生成模型进行设计和分析的方法，并建立了这种方法与主要流动和扩散型生成模型之间的关联。通过研究每个生成模型与它们相关的 MFG 的最优条件，本文提出了一个基于双人 MFG 的新的生成模型，该模型在提高样本多样性和逼真度的同时改善了解缠结和公平性。 |
| [^23] | [Cluster Entropy: Active Domain Adaptation in Pathological Image Segmentation.](http://arxiv.org/abs/2304.13513) | 本文提出了一种对于病理图像分割领域偏移问题的解决方法，通过使用簇熵来选择有效的WSI并可以显著提高半监督域自适应的性能。 |
| [^24] | [Mixing Data Augmentation with Preserving Foreground Regions in Medical Image Segmentation.](http://arxiv.org/abs/2304.13490) | 该研究提出了两种新的数据增强方法KeepMask和KeepMix，这些方法可以在不消耗额外计算资源的情况下更好地识别器官边界，从而为医学图像分割提供高精度的性能和更精确的分割边界。 |
| [^25] | [Fundamental Tradeoffs in Learning with Prior Information.](http://arxiv.org/abs/2304.13479) | 本文研究了先验信息准确性和学习性能之间的基本权衡，引入了优先风险概念，并为统计估计问题提供了下界，展现了框架在不同问题中的应用。 |
| [^26] | [Effect of latent space distribution on the segmentation of images with multiple annotations.](http://arxiv.org/abs/2304.13476) | 本文研究了多次标注图像的分割问题，提出了广义概率U-Net。研究表明，选择合适的潜在空间分布可以提高预测结果的多样性和参考分割的重叠度。 |
| [^27] | [Unsupervised classification of fully kinetic simulations of plasmoid instability using Self-Organizing Maps (SOMs).](http://arxiv.org/abs/2304.13469) | 本文采用自组织映射（SOM）的聚类方法应用于等离子体磁陀螺不稳定完全动力学模拟，我们获得了良好的聚类结果，证明该方法可作为模拟和观测数据可靠分析工具，并给出了有益的物理见解和空间区域的信息。 |
| [^28] | [Polynomial-Time Solvers for the Discrete $\infty$-Optimal Transport Problems.](http://arxiv.org/abs/2304.13467) | 本文提出了离散无穷范最优输运问题的多项式时间求解器。 |
| [^29] | [A Comparative Analysis of Multiple Methods for Predicting a Specific Type of Crime in the City of Chicago.](http://arxiv.org/abs/2304.13464) | 本研究考察了预测芝加哥市盗窃犯罪的多种方法，并发现采用XGBoost方法能够得到最佳的预测结果，F1分数为0.86。 |
| [^30] | [From Chaos Comes Order: Ordering Event Representations for Object Detection.](http://arxiv.org/abs/2304.13455) | 本文提出了一种基于Gromov-Wasserstein Discrepancy选择最佳事件表示的方法，这种方法可以在多个表示、网络骨干和数据集上保持任务性能排名的一致性。利用这一方法，本文对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示法，取得了优于最先进的基于事件的对象检测方法的成果。 |
| [^31] | [Implicit Counterfactual Data Augmentation for Deep Neural Networks.](http://arxiv.org/abs/2304.13431) | 本研究提出了隐式反事实数据增强（ICDA）方法，通过新的样本增强策略、易于计算的代理损失和具体方案，消除了虚假关联并进行了稳健预测。 |
| [^32] | [GENIE-NF-AI: Identifying Neurofibromatosis Tumors using Liquid Neural Network (LTC) trained on AACR GENIE Datasets.](http://arxiv.org/abs/2304.13429) | 本研究提出了一个使用基于AACR GENIE数据集的液态神经网络（LTC）诊断神经纤维瘤的可解释AI方法，以99.86%的准确率优于现有模型，并提供了解释性和黑盒模型。 |
| [^33] | [FLEX: an Adaptive Exploration Algorithm for Nonlinear Systems.](http://arxiv.org/abs/2304.13426) | FLEX是一种非线性系统的自适应探索算法，使用最优实验设计方法，需要最少的资源，并用于下游的基于模型的经典控制任务中。 |
| [^34] | [Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories.](http://arxiv.org/abs/2304.13424) | 本文研究了强化学习代理对超出分布的“可控”状态的“接力泛化”性能。通过让测试代理从其他陌生代理的轨迹中间开始，发现这种泛化普遍存在泛化失效问题。 |
| [^35] | [Regression with Sensor Data Containing Incomplete Observations.](http://arxiv.org/abs/2304.13415) | 本文提出了一种能处理不完整观测传感器数据的回归算法，解决了标签值由于不完整观测导致学习结果偏低的问题。 |
| [^36] | [Secure Communication Model For Quantum Federated Learning: A Post Quantum Cryptography (PQC) Framework.](http://arxiv.org/abs/2304.13413) | 本论文提出了一种后量子密码的量子联邦学习模型，并研究了其收敛和安全条件。 |
| [^37] | [Improving Adversarial Transferability by Intermediate-level Perturbation Decay.](http://arxiv.org/abs/2304.13410) | 本文提出了一种名为中间层扰动衰减（ILPD）的新型中间层方法，可以通过单个优化阶段制作可转移的对抗性样本，并在过程中鼓励中间层扰动处于有效的敌对方向，并同时具有很大的幅度。 |
| [^38] | [FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models.](http://arxiv.org/abs/2304.13407) | 该论文提出FedVS，一种同时解决垂直联邦学习中滞后客户端和数据泄露问题的方法，通过设计本地数据和模型的秘密共享方案，以保证信息理论隐私，并通过解密计算股份，无损重构所有客户端的嵌入的汇总。 |
| [^39] | [SEAL: Simultaneous Label Hierarchy Exploration And Learning.](http://arxiv.org/abs/2304.13374) | 提出了一种新的框架，Simultaneous label hierarchy Exploration And Learning (SEAL)，通过增加遵循先验层次结构的潜在标签来探索标签层次结构，通过1-Wasserstein度量在树度量空间上学习数据驱动的标签层次结构和执行（半）监督学习，并在实验中展示了优越的结果和富有见地的标签结构。 |
| [^40] | [Feed-Forward Optimization With Delayed Feedback for Neural Networks.](http://arxiv.org/abs/2304.13372) | 本文提出了一种延迟反馈的前馈神经网络优化方法F^3，使用延迟的误差信息来缩放梯度从而提高生物可行性和计算效率，具有较高的预测性能，为低能量训练和并行化提供了新思路。 |
| [^41] | [LoRaWAN-enabled Smart Campus: The Dataset and a People Counter Use Case.](http://arxiv.org/abs/2304.13366) | 本文介绍一个基于LoRaWAN的智能校园数据集，使用k近邻和LSTM方法处理丢失值和预测未来读数，并构建一个深度神经网络来预测房间内人数，准确率达到95％。 |
| [^42] | [Concept-Monitor: Understanding DNN training through individual neurons.](http://arxiv.org/abs/2304.13346) | 通过 Concept-Monitor，我们提出了一种通用框架来自动解密黑匣子 DNN 训练过程，它可以帮助人们理解 DNN 在训练期间如何发展，同时我们提出了一种新的正则化器来激励隐藏神经元学习多样的概念，这可以提高训练性能。我们还运用 Concept-Monitor 对不同训练范式进行了多个案例研究，包括对抗性训练、微调和网络修剪。 |
| [^43] | [OpenBox: A Python Toolkit for Generalized Black-box Optimization.](http://arxiv.org/abs/2304.13339) | OpenBox是一个通用黑盒优化的Python工具包，提供了用户友好的接口和可视化功能，模块化设计能够在现有系统中灵活部署，并且实验证明其比现有系统更有效和高效。 |
| [^44] | [Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs.](http://arxiv.org/abs/2304.13312) | 本文提出了一种通过量化输入变量之间的编码交互来准确且简明地解释深度神经网络(DNN)的推理逻辑的方法。针对此目的，作者提出了两种交互方式，即AND交互和OR交互，并利用它们设计出一系列技术来提高解释的简洁性，同时不会损害准确性。 |
| [^45] | [HiQ -- A Declarative, Non-intrusive, Dynamic and Transparent Observability and Optimization System.](http://arxiv.org/abs/2304.13302) | HiQ是一种可透明监控Python程序运行时信息的系统，具有非侵入性和动态性，可应用于离线/在线应用程序和分布式系统，我们可以使用它来优化神经网络模型并捕捉瓶颈，而不影响代码的干净程度和性能表现。 |
| [^46] | [Membrane Potential Distribution Adjustment and Parametric Surrogate Gradient in Spiking Neural Networks.](http://arxiv.org/abs/2304.13289) | 本论文提出了一种参数化代理梯度（PSG）方法和潜在分布调整（PDA）方法，以解决尖峰神经网络（SNN）中梯度下降训练的问题，并在基准数据集上实现了优异的性能。 |
| [^47] | [The Closeness of In-Context Learning and Weight Shifting for Softmax Regression.](http://arxiv.org/abs/2304.13276) | 本论文讨论了大型语言模型中关键组件注意机制的 softmax 单元以及上下文学习，探究了在上下文学习中softmax单元的权重调整。 |
| [^48] | [Federated Learning with Uncertainty-Based Client Clustering for Fleet-Wide Fault Diagnosis.](http://arxiv.org/abs/2304.13275) | 本文提出了一种基于不确定性客户聚类的联邦学习框架，用于车队故障诊断，通过完全去中心化的架构和客户聚类，提高了模型的精确性和效率，并在真实数据集上证明了其优于现有联邦学习方法的表现。 |
| [^49] | [Making Models Shallow Again: Jointly Learning to Reduce Non-Linearity and Depth for Latency-Efficient Private Inference.](http://arxiv.org/abs/2304.13274) | 本文提出了一种联合优化方法，可以学习深度神经网络变得更浅，具体地，利用卷积块的ReLU灵敏度来合并卷积层和去除ReLU层，可以在减少ReLU和线性操作的情况下提高模型计算效率，同时减小模型模型延迟和大小，准确率不显著下降。 |
| [^50] | [From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping.](http://arxiv.org/abs/2304.13273) | 本研究提出了一种从关联到生成的零-shot方法：通过将图像/视频投影到语言模态并在生成任务中生成描述性字幕。该方法在多个基准数据集上显著优于现有的最先进方法，为无监督跨模态映射提供了一个新的视角，并具有在视频字幕，图像合成和文本到图像生成等领域的潜在应用。 |
| [^51] | [Bayesian Federated Learning: A Survey.](http://arxiv.org/abs/2304.13267) | 本文综述了贝叶斯联邦学习（BFL）的基本概念和分类，包括客户端、服务器端和基于FL的BFL方法。BFL是解决现有FL方法中受限和动态的数据和条件、异质性和不确定性以及分析解释能力挑战的有前途方法。 |
| [^52] | [SHIELD: Thwarting Code Authorship Attribution.](http://arxiv.org/abs/2304.13255) | 本研究设计了 SHIELD 来检查代码作者归属方法对抗性代码示例的鲁棒性。实验结果表明当前的作者归属方法对于对抗性攻击和扰动具有脆弱性。 |
| [^53] | [Analyzing In-browser Cryptojacking.](http://arxiv.org/abs/2304.13253) | 本文研究了浏览器中加密挖矿的静态、动态和经济方面，对加密挖矿样本进行分类，采用机器学习进行区分并分析了其对于计算机系统资源的影响。 |
| [^54] | [A Security Verification Framework of Cryptographic Protocols Using Machine Learning.](http://arxiv.org/abs/2304.13249) | 该论文提出了一种使用机器学习方法以较短时间验证密码协议安全性的框架。 |
| [^55] | [Learning to Predict Navigational Patterns from Partial Observations.](http://arxiv.org/abs/2304.13242) | 本文提出了一种仅通过局部观测学习预测真实环境中导航模式的自监督学习方法，能够胜过两个有监督模型，并且可以在无限数据的情况下预测无偏本地方向软车道概率场。 |
| [^56] | [Structure Diagram Recognition in Financial Announcements.](http://arxiv.org/abs/2304.13240) | 本文提出了一种新的识别金融公告中结构图的方法，并通过两阶段方法生成了行业的第一个基准。实验结果显示，与现有方法相比，本文所提出的方法具有更高的有效性和效率。 |
| [^57] | [Multi-criteria Hardware Trojan Detection: A Reinforcement Learning Approach.](http://arxiv.org/abs/2304.13232) | 本文提出了一种多准则强化学习（RL）HT检测工具，可适用于不同的HT检测场景，提高了HT检测的成功率。 |
| [^58] | [UNADON: Transformer-based model to predict genome-wide chromosome spatial position.](http://arxiv.org/abs/2304.13230) | UNADON是一种基于Transformer的深度学习模型，可以预测全基因组的染色体空间位置。通过使用序列特征和表观遗传信号，UNADON在训练单个细胞系时能够高度准确地预测染色质空间定位到核体，并揭示了潜在影响染色质区隔的序列和表观遗传因素。 |
| [^59] | [Generating Adversarial Examples with Task Oriented Multi-Objective Optimization.](http://arxiv.org/abs/2304.13229) | 本文提出了“基于任务导向的MOO”方法来实现对抗样本生成以同时实现多个目标，避免了朴素MOO最大化所有目标的弊端。 |
| [^60] | [Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation.](http://arxiv.org/abs/2304.13224) | 本文提出了一种基于BSDE的扩散模型，可以通过调整现有的分数函数确定到达所需终端分布所需的初始条件，为扩散建模提供了一种新的方法，并在扩散反演，条件扩散和不确定性量化等领域具有潜在应用。 |
| [^61] | [Reinforcement Learning with Partial Parametric Model Knowledge.](http://arxiv.org/abs/2304.13223) | 该论文研究了在环境完全无知和完美知识之间的机遇，提出了一种利用局部模型和保持数据驱动调整的强化学习方法，已在线性二次调节器上得到验证。 |
| [^62] | [ZRG: A High Resolution 3D Residential Rooftop Geometry Dataset for Machine Learning.](http://arxiv.org/abs/2304.13219) | 该论文介绍了 ZRG 数据集，其中包含成千上万个样本的高分辨率住宅屋顶正交图像拼接、对应 DSM、3D 屋顶框架和多视图图像生成的点云。该数据集可以用于住宅屋顶结构的几何建模和场景理解，如屋顶轮廓提取、单目高度估计和平面屋顶结构提取等任务。 |
| [^63] | [Single-View Height Estimation with Conditional Diffusion Probabilistic Models.](http://arxiv.org/abs/2304.13214) | 本文提出了一种使用条件扩散概率模型进行单视图高度估计的方法，通过训练一个生成模型来学习光学和DSM图像之间的联合分布，实现生成逼真的高分辨率3D表面。 |
| [^64] | [Splitting physics-informed neural networks for inferring the dynamics of integer- and fractional-order neuron models.](http://arxiv.org/abs/2304.13205) | 本文提出了一种用于正向动力学系统的分裂物理信息神经网络方法，应用于求解整数和分数阶神经元模型并展现出更高的准确性和效率。 |
| [^65] | [Kernel Methods are Competitive for Operator Learning.](http://arxiv.org/abs/2304.13202) | 本文提出了一个核方法算子学习框架，在对多组数据进行全面比较后，结果表明该方法在多种设置下都是一种具有竞争力的算子学习方法。 |
| [^66] | [Connector 0.5: A unified framework for graph representation learning.](http://arxiv.org/abs/2304.13195) | 本文提出了一种名为Connector的图表示框架，可以覆盖从浅层到最先进的各种图嵌入模型，并考虑通过构建具有不同结构关系的各种类型的图来生成图。 |
| [^67] | [Towards Reliable Colorectal Cancer Polyps Classification via Vision Based Tactile Sensing and Confidence-Calibrated Neural Networks.](http://arxiv.org/abs/2304.13192) | 本研究针对基于人工智能的结直肠癌（CRC）息肉分类技术输出过于自信的问题，提出了一种利用视觉触觉传感系统和独特的CRC息肉模拟体的残差神经网络分类器，并通过温度缩放的后处理方法解决其过度自信的输出问题，从而实现了可靠的CRC息肉诊断。 |
| [^68] | [TABLET: Learning From Instructions For Tabular Data.](http://arxiv.org/abs/2304.13188) | 该论文提出了TABLET，这是一个由20个不同的包含指令注释的表格数据集组成的基准测试，可以提高大型语言模型在表格预测问题上的效果，并评估了指令在保真度和LLM在表格预测方面的局限性。 |
| [^69] | [Sample-Specific Debiasing for Better Image-Text Models.](http://arxiv.org/abs/2304.13181) | 发现从训练数据集中均匀地抽取负样本会引入错误的负面样本。我们提出了一种纠正错误负面样本的新方法，即针对图文模型的样本特异性去偏方法。 |
| [^70] | [SAFE: Machine Unlearning With Shard Graphs.](http://arxiv.org/abs/2304.13169) | 本论文提出了一种使用 shard graph 进行机器遗忘的方法，以实现在最小化遗忘成本的情况下适应多样数据的大型模型，并取得了较高的准确性。 |
| [^71] | [Towards Compute-Optimal Transfer Learning.](http://arxiv.org/abs/2304.13164) | 本文提出一种计算优化的迁移学习方法，通过对预训练模型进行零-shot结构剪枝，使其在最小降低性能的情况下提高计算效率，实现了20%以上的性能提升。 |
| [^72] | [LumiGAN: Unconditional Generation of Relightable 3D Human Faces.](http://arxiv.org/abs/2304.13153) | LumiGAN是一种无条件的生成对抗网络，具有基于物理学的照明模块，可以在推理时实现在新的光照下的再照明，它可以生成可再照明的人脸的逼真物理特性，并比现有的方法具有更好的逼真度和几何生成能力。 |
| [^73] | [T Cell Receptor Protein Sequences and Sparse Coding: A Novel Approach to Cancer Classification.](http://arxiv.org/abs/2304.13145) | 本研究探索了利用稀疏编码的方法对具有癌症分类目标的TCR蛋白序列进行多类别分类，为基于TCR的免疫治疗提供理论支持。 |
| [^74] | [Self-Supervised Temporal Analysis of Spatiotemporal Data.](http://arxiv.org/abs/2304.13143) | 本文提出了一种自监督方法，能根据移动活动时间序列对景观进行分层，通过深度语义分割实现了对地理空间任务的多模态建模，适用于分类居民区和商业区等不同任务。 |
| [^75] | [Quantum Machine Learning Approach for the Prediction of Surface Roughness in Additive Manufactured Specimens.](http://arxiv.org/abs/2304.13142) | 首次针对增材制造试件的表面粗糙度使用三种量子算法（QNN、Q-Forest和VQC）进行回归预测，其中Q-Forest算法表现最优，具有较低的MSE和MAE和较高的EVS。 |
| [^76] | [ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode.](http://arxiv.org/abs/2304.13140) | 本文提出ESimCSE无监督比较学习和UDA半监督比较学习模型相结合，通过联合训练技术解决了大标签系统文本分类的多个问题，并在公共数据集上实现了准确率提高。 |
| [^77] | [The Update Equivalence Framework for Decision-Time Planning.](http://arxiv.org/abs/2304.13138) | 该论文提出了一个基于更新等价的决策时间规划框架，使得决策时间规划算法不依赖于公共信息，在更大范围的不完全信息决策环境中实现超人类表现。 |
| [^78] | [MEDNC: Multi-ensemble deep neural network for COVID-19 diagnosis.](http://arxiv.org/abs/2304.13135) | MEDNC是一种基于深度学习的COVID-19自动诊断方法，使用计算机断层扫描图像预测和诊断COVID-19，准确性高达98.79%和99.82%。同时，该方法还能适用于其他问题，如大脑肿瘤和血细胞数据的预测，具有广泛的适用性。 |
| [^79] | [Directed Chain Generative Adversarial Networks.](http://arxiv.org/abs/2304.13131) | 本文提出了有向链生成对抗网络（DC-GANs），使用邻域过程作为关键步骤生成同样分布的多模态时间序列数据。 |
| [^80] | [Precision Spectroscopy of Fast, Hot Exotic Isotopes Using Machine Learning Assisted Event-by-Event Doppler Correction.](http://arxiv.org/abs/2304.13120) | 提出一种新的实验方案，结合机器学习辅助的事件逐事件多普勒修正，实现对快速、热的异位同位素进行高精度的激光光谱学研究，在极端温度下仍能实现kHz级别的不确定性。 |
| [^81] | [Application of Transformers for Nonlinear Channel Compensation in Optical Systems.](http://arxiv.org/abs/2304.13119) | 本文提出了一种利用Transformer进行光学系统非线性通道补偿的新方法，这种方法利用了Transformer的记忆关注能力和并行结构，实现了高效的非线性补偿。同时，作者还提出了一种物理学信息掩码，用于降低计算复杂度。 |
| [^82] | [Federated Deep Reinforcement Learning for THz-Beam Search with Limited CSI.](http://arxiv.org/abs/2304.13109) | 本文提出了一个联邦深度强化学习的方法，可以用于在有限CSI的情况下迅速进行THz波束搜索，以克服THz信号的传播衰减。 |
| [^83] | [Time-Selective RNN for Device-Free Multi-Room Human Presence Detection Using WiFi CSI.](http://arxiv.org/abs/2304.13107) | 这篇论文提出了一种使用基于WiFi信道状态信息提取人体移动和空间特征的无设备多房间人体存在检测系统，能够通过时间-selective特征提取算法区分有直觉视线路径阻塞和无视线路径阻塞的情况。 |
| [^84] | [Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System.](http://arxiv.org/abs/2304.13105) | 本文提出了一种利用WiFi信号进行人员存在检测的新系统，采用了关注机制和双向LSTM网络来提高准确性，并证明了其在现实场景中的稳健性。 |
| [^85] | [LSTM-based Load Forecasting Robustness Against Noise Injection Attack in Microgrid.](http://arxiv.org/abs/2304.13104) | 本文研究了LSTM神经网络在微电网负荷预测中受到噪声注入攻击的鲁棒性。使用低通滤波器消除了攻击，以提高模型的性能。 |
| [^86] | [Uncovering the Representation of Spiking Neural Networks Trained with Surrogate Gradient.](http://arxiv.org/abs/2304.13098) | 本研究使用中心核对齐分析了使用替代梯度训练的脉冲神经网络（SNN）与传统人工神经网络（ANN）之间的表示相似性，并发现SNN中的时间维度提供了独特的表示学习能力。 |
| [^87] | [Making Video Quality Assessment Models Robust to Bit Depth.](http://arxiv.org/abs/2304.13092) | 本文提出了HDRMAX特征集，可以提高视频质量评估算法对于不同位深视频的质量预测性能，尤其在HDR视频上表现尤佳。 |
| [^88] | [Model Extraction Attacks Against Reinforcement Learning Based Controllers.](http://arxiv.org/abs/2304.13090) | 本文研究了在强化学习控制器中的模型提取攻击，提出了一个两阶段算法，第一阶段使用侧信道信息进行估计候选项的识别，第二阶段使用轨迹优化算法选择最佳估计。 |
| [^89] | [Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations.](http://arxiv.org/abs/2304.13089) | 本文分析了联合嵌入学习和重建学习两种自监督学习视觉transformers的目标对所学表示的影响及其转换性能差异，发现联合嵌入学习特征更利于线性探测转移分类，进而提供了该领域未来研究的方向。 |
| [^90] | [Organizational Governance of Emerging Technologies: AI Adoption in Healthcare.](http://arxiv.org/abs/2304.13081) | 该研究通过与美国主要医疗保健系统的领导人和相关领域的主要知情人合作，制定了AI在医疗保健中的组织治理框架，包括关键控制点和决策标准，为卫生系统领导人做出更加明智的决策提供了支持。 |
| [^91] | [iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer.](http://arxiv.org/abs/2304.13061) | 本文推广了 Hopkins field 分层网络，并介绍了 iMixer，MLP-Mixer 模型的新概括，不同于普通的前馈网络，iMixer 涉及到从输出到输入的传播的 MLP 层，被特征化为一个可逆、隐式、迭代的 mixing block。 |
| [^92] | [GULP: Solar-Powered Smart Garbage Segregation Bins with SMS Notification and Machine Learning Image Processing.](http://arxiv.org/abs/2304.13040) | 本研究利用太阳能驱动智能垃圾分类桶，并通过短信通知和机器学习图像处理功能实现废物分类，提高废物管理效率，同时使终端用户更加关注环保。 |
| [^93] | [Optimizing Deep Learning Models For Raspberry Pi.](http://arxiv.org/abs/2304.13039) | 针对树莓派优化深度学习模型包括修剪技术和模型参数结构优化，以适应其硬件特点并提高能效。 |
| [^94] | [VeML: An End-to-End Machine Learning Lifecycle for Large-scale and High-dimensional Data.](http://arxiv.org/abs/2304.13037) | VeML是一种专门用于大规模高维数据的端到端机器学习生命周期的版本管理系统，在解决生命周期高成本问题、数据相似性计算和数据模式分析等关键问题方面表现出色。 |
| [^95] | [SmartChoices: Augmenting Software with Learned Implementations.](http://arxiv.org/abs/2304.13033) | SmartChoices 提出了一种将机器学习与现有软件系统轻松、安全、有效地结合的新方法。 |
| [^96] | [A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction.](http://arxiv.org/abs/2304.13032) | 提出了一个针对软件性能预测的主动学习框架，通过将源代码解析成流增强抽象语法树图形式，构造各种无监督和有监督的图嵌入，进行主动学习，实现对未标注数据的高效利用，并比现有方法更加优越。 |
| [^97] | [Awesome-META+: Meta-Learning Research and Learning Platform.](http://arxiv.org/abs/2304.12921) | Awesome-META+是一个元学习框架集成和学习平台，旨在提供完整可靠的元学习框架应用和面向初学者的学习材料，进而促进元学习的发展并将其从小众领域转化为主流的研究方向。 |
| [^98] | [Learning Robust Deep Equilibrium Models.](http://arxiv.org/abs/2304.12707) | 本论文提出一种名为LyaDEQ的鲁棒DEQ模型，通过Lyapunov理论提供了保证的稳定性以抵抗微小的初始扰动，并在不同的固定点之间加入全连接层以避免不良对抗性防御。 |
| [^99] | [Efficient Bayesian inference using physics-informed invertible neural networks for inverse problems.](http://arxiv.org/abs/2304.12541) | 本文提出了一种使用物理信息可逆神经网络(PI-INN)解决贝叶斯反问题的新方法，该方法可以高效地进行抽样和准确的密度评估。研究通过残差项和独立性损失项确保了INN输出的统计独立性，并在多项实验中证明了其有效性和准确性。 |
| [^100] | [ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain.](http://arxiv.org/abs/2304.11960) | 本文提出了一种基于BERT的焦点爬虫ThreatCrawl，使用主题建模和关键词提取技术来筛选出最可能包含有价值CTI信息的网页。 |
| [^101] | [Unsupervised Machine Learning to Classify the Confinement of Waves in Periodic Superstructures.](http://arxiv.org/abs/2304.11901) | 通过对比聚类算法和直接应用方法，本研究发现应先采用直接比例方法找到正确的约束维度集，再用聚类来细化结果，同时基于模型的算法优于标准的k-means++聚类算法。 |
| [^102] | [Learning Partial Correlation based Deep Visual Representation for Image Classification.](http://arxiv.org/abs/2304.11597) | 本文提出了一种基于偏相关的深度视觉表示学习方法，解决了使用协方差矩阵表征相关性在存在混淆效应时的误导问题。 |
| [^103] | [The Isotonic Mechanism for Exponential Family Estimation.](http://arxiv.org/abs/2304.11160) | 本文利用扩展的保序机制，将其应用于指数族分布以提高同行评审的质量，并发现作者的同行评分可以较准确地在不需要知道具体分布情况下进行调整。 |
| [^104] | [Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance.](http://arxiv.org/abs/2304.11127) | 该论文介绍了一种广泛使用的贝叶斯优化方法 Tree-structured Parzen estimator (TPE)，并对其控制参数的作用和算法直觉进行了讨论和分析，提供了一组推荐设置并证明其能够提高TPE的性能表现。 |
| [^105] | [Segment Anything Model for Medical Image Analysis: an Experimental Study.](http://arxiv.org/abs/2304.10517) | 本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。 |
| [^106] | [Attention Scheme Inspired Softmax Regression.](http://arxiv.org/abs/2304.10411) | 本研究从 softmax 单元中获得灵感，提出了注意机制 inspired 的 softmax 回归问题，该问题可用于控制潜在函数的进展和稳定性。 |
| [^107] | [Lady and the Tramp Nextdoor: Online Manifestations of Real-World Inequalities in the Nextdoor Social Network.](http://arxiv.org/abs/2304.05232) | 本文第一个大规模研究了Nextdoor社交网络，发现不同收入水平的社区在社交网络上的在线行为不同，更富裕的社区情感更积极，更多地讨论犯罪，尽管实际犯罪率要低得多，同时用户生成内容能够预测收入和不平等。 |
| [^108] | [Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond.](http://arxiv.org/abs/2304.04968) | 本研究提出了Perp-Neg算法，通过利用得分空间的几何特性来解决目前文本到图像扩散模型中负面提示算法存在的问题，使得用户能够编辑掉初始生成图像中不想要的概念，从而提供了更大的灵活性。同时，我们还通过提出基于Perp-Neg的3D负面提示算法，将算法扩展到3D应用中。 |
| [^109] | [MHfit: Mobile Health Data for Predicting Athletics Fitness Using Machine Learning.](http://arxiv.org/abs/2304.04839) | 本文提出了利用移动健康数据来比较多种机器学习算法，预测人体健康行为和健身情况的方法；结果表明，XGBoost算法表现优于其他算法。 |
| [^110] | [DiffMimic: Efficient Motion Mimicking with Differentiable Physics.](http://arxiv.org/abs/2304.03274) | 本文提出了DiffMimic，一种基于可微分物理的高效运动模仿方法。与传统强化学习方法相比，其有更快更稳定的收敛速度；同时通过演示重播机制避免陷入局部最优解。 |
| [^111] | [Causal Repair of Learning-enabled Cyber-physical Systems.](http://arxiv.org/abs/2304.02813) | 本文提出了一种学习增强型物联网系统的因果诊断和修复方法，通过矫正有问题的输入/输出行为子集，识别真正的属性违规原因并修复。 |
| [^112] | [The expressive power of pooling in Graph Neural Networks.](http://arxiv.org/abs/2304.01575) | 本文研究了池化算子在图神经网络中的表达能力，并提供了一个通用标准来选择或设计池化算子。 |
| [^113] | [Inferring networks from time series: a neural approach.](http://arxiv.org/abs/2303.18059) | 本论文提出了一种基于神经网络的快速计算方法，可以从时间序列数据中推断大型网络的相邻矩阵，并对不确定性进行量化，解决了网络推断问题的不足。 |
| [^114] | [Efficient Alternating Minimization Solvers for Wyner Multi-View Unsupervised Learning.](http://arxiv.org/abs/2303.15866) | 本文提出了适用于Wyner多视图无监督学习的高效交替最小化求解器，包括变分形式和表现形式，通过多重乘数交替方向算法可以解决由此造成的非凸优化问题。 |
| [^115] | [Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems: A systematic scientific machine learning approach.](http://arxiv.org/abs/2303.13555) | 本研究提出了一种机器学习方法，可用于创建非线性平流-扩散-吸附系统的高效混合模型和发现吸附摄取模型的吸附动力学定律结构。 |
| [^116] | [On the Risks of Stealing the Decoding Algorithms of Language Models.](http://arxiv.org/abs/2303.04729) | 这项工作首次展示，一个拥有典型API访问权限的对手可以以极低的金钱成本窃取GPT-2和GPT-3等LM的解码算法的类型和超参数。 |
| [^117] | [Towards provably efficient quantum algorithms for large-scale machine-learning models.](http://arxiv.org/abs/2303.03428) | 本论文提出了一种可能的针对通用（随机）梯度下降算法的高效量子解决方案，只要模型足够耗散和稀疏，具有小的学习率，并且可以缩放至 $O(T^2 \times \text{polylog}(n))$。在实践中，证明了在稀疏训练的情况下，量子计算可以显著提高效率。 |
| [^118] | [Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most.](http://arxiv.org/abs/2302.09195) | 该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。 |
| [^119] | [Generative Causal Representation Learning for Out-of-Distribution Motion Forecasting.](http://arxiv.org/abs/2302.08635) | 该论文提出了一种生成因果表示学习方法，通过利用因果关系来实现分布转移下的知识迁移，主要应用于面向多样性预测的问题。 |
| [^120] | [Listen2Scene: Interactive material-aware binaural soundbpropagation for reconstructed 3D scenes.](http://arxiv.org/abs/2302.02809) | 本文提出了一种交互式的物质感知双耳音频传播方法，能够生成真实环境下的渲染音频，利用图神经网络和条件生成对抗网络，处理重构三维模型中的缺陷，并且能够精确生成与真实环境相符的声学输出。 |
| [^121] | [Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation.](http://arxiv.org/abs/2302.02561) | 该论文介绍了一种新的域自适应方法，该方法利用了从多域数据中生成的域索引，提供额外的洞察视角，并在各种任务中实现了最佳性能。 |
| [^122] | [Fast, Sample-Efficient, Affine-Invariant Private Mean and Covariance Estimation for Subgaussian Distributions.](http://arxiv.org/abs/2301.12250) | 本论文提出了一种快速的差分私有算法，用于具有几乎最优样本复杂度的高维协方差感知均值估计。在Mahalanobis误差度量中，也就是相对于协方差的平均误差中，我们的算法使得$\hat \mu$更接近$\mu$。 |
| [^123] | [Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2.](http://arxiv.org/abs/2301.11719) | 本论文研究了将事实知识纳入生成的摘要的可能性，具体采用前缀调整的方法，实验结果表明，此方法可以生成保留知识的摘要，而且可以提升整体性能。 |
| [^124] | [From Pseudorandomness to Multi-Group Fairness and Back.](http://arxiv.org/abs/2301.08837) | 本文探索了预测算法中多组公平性和伪随机性的联系，提供了新的多校准算法和实值函数核引理证明算法。 |
| [^125] | [SEQUENT: Towards Traceable Quantum Machine Learning using Sequential Quantum Enhanced Training.](http://arxiv.org/abs/2301.02601) | SEQUENT提出了一种新方法，通过序列量子增强训练实现混合机器学习模型的追溯性，可以追踪到所选的电路架构和参数化对模型的贡献，解决了现有方法中无法严格分离经典和量子影响的问题。 |
| [^126] | [Deep Statistical Solver for Distribution System State Estimation.](http://arxiv.org/abs/2301.01835) | 该论文提出了基于图神经网络的深度统计求解器（DSS$^2$），应用于配电系统状态估计。DSS$^2$利用超图和节点消息传递方案更新潜在表示，并通过弱监督学习方法进行训练。实验结果证明了DSS$^2$在实际数据集上优于其他方法。 |
| [^127] | [Dynamic Feature Engineering and model selection methods for temporal tabular datasets with regime changes.](http://arxiv.org/abs/2301.00790) | 本文提出了一种新的机器学习管道，用于在数据制度变化下对时序面板数据集的预测进行排名。使用梯度提升决策树（GBDT）并结合dropout技术的模型具有良好的性能和泛化能力，而动态特征中和则是一种高效而不需要重新训练模型就可以应用于任何机器学习模型中的后处理技术。 |
| [^128] | [BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI.](http://arxiv.org/abs/2212.10802) | 本文提出了一种基于半监督学习的双折叠师生网络，该网络通过利用部分标记和未标记的数据集智能地学习空间和时间特征，有效地解决了基于CSI的室内存在检测受到环境变化和有监督学习方法需要耗时标注的问题。 |
| [^129] | [Towards Understanding Fairness and its Composition in Ensemble Machine Learning.](http://arxiv.org/abs/2212.04593) | 该论文研究集成机器学习中的公平性及其组成方式，提出了一种新颖的方法来衡量集成模型的公平性，并评估了单个学习器和集成超参数对公平性的影响。实验表明，公平性在集成中的组成方式是非线性的，超参数的选择可以显著影响集成模型的公平性。 |
| [^130] | [VISEM-Tracking, a human spermatozoa tracking dataset.](http://arxiv.org/abs/2212.02842) | 本文提供了人类精子跟踪数据集VISEM-Tracking，包含手动注释的包围框坐标和由专家分析的精子特征，并提供未标记的视频以供易于访问和分析，有助于训练监督式机器学习方法，提高在评估精子运动和运动学方面的精度和可靠性。 |
| [^131] | [CrossSplit: Mitigating Label Noise Memorization through Data Splitting.](http://arxiv.org/abs/2212.01674) | 本文提出了一种名为CrossSplit的新训练程序，通过使用交叉分割的标签修正和半监督训练两个主要组成部分，缓解了深度学习算法中标签噪声记忆问题，具有良好的效果。 |
| [^132] | [Time-shift selection for reservoir computing using a rank-revealing QR algorithm.](http://arxiv.org/abs/2211.17095) | 本论文提出了一种基于QR分解的时间偏移选择技术，通过最大化蓄水池矩阵秩来提高性能准确性，在模拟硬件蓄水池计算中具有广泛应用。 |
| [^133] | [Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models.](http://arxiv.org/abs/2211.17091) | 本文提出了“鉴别器引导”方法，通过在评分训练之后训练鉴别器，使模型评估更加准确，从而改善预训练扩散模型的样本生成。在 ImageNet 256x256 数据集上实现了 FID 1.83 和召回率 0.64 的最新结果，类似于验证数据的 FID 和召回率。 |
| [^134] | [SimVP: Towards Simple yet Powerful Spatiotemporal Predictive Learning.](http://arxiv.org/abs/2211.12509) | SimVP是一种简单且强大的时空预测学习基准模型，完全基于卷积神经网络构建且没有循环结构，并且使用均方误差损失为训练目标。SimVP在各种基准数据集上均能取得优异的表现，并且具有强大的泛化性和可扩展性。 |
| [^135] | [Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version).](http://arxiv.org/abs/2211.11434) | 该研究提出了通过差分隐私保护COVID-19检测模型，解决数据分析和患者隐私保护的问题。通过黑盒成员推理攻击，实现了对实际隐私的评估，结论表明所需的隐私等级可能因受到实际威胁的任务而异。 |
| [^136] | [CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI.](http://arxiv.org/abs/2211.10354) | 本文介绍了一种名为CRONOS的系统，可以通过彩色化和对比学习来基于Wi-Fi CSI实现无人设备NLoS人体检测，可以区分房间中的移动人员和空置。实验结果表明该系统在NLoS条件下能够准确地检测出房间中的人物存在。 |
| [^137] | [Mechanistic Mode Connectivity.](http://arxiv.org/abs/2211.08422) | 本文从模式连通性的视角研究神经网络损失景观，提出了机制相似性的定义，并演示了两个模型之间缺乏线性连通性意味着它们使用不同的机制来做出预测。此外，本文还提出了一种名为基于连通性的微调（CBFT）的方法，用于目标修改模型机制，有助于消除模型对虚假特征的依赖。 |
| [^138] | [RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure.](http://arxiv.org/abs/2211.05239) | RecD 是一种为 DLRM 训练提供去重功能的端到端基础设施优化，解决了由于特征重复造成的海量存储、预处理和训练开销，引入了新的张量格式 InverseKeyedJaggedTensors (IKJTs) 来去除特征值的重复，使 DLRM 模型架构能够更好地利用数据的重复性提高训练吞吐量。 |
| [^139] | [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models.](http://arxiv.org/abs/2211.05105) | 该论文提出了一种名为安全潜向扩散的方法，可以在图像生成过程中移除和抑制不当的图像部分，从而缓解基于文本的图像生成模型因不当数据集带来的不良影响。 |
| [^140] | [Efficacy of MRI data harmonization in the age of machine learning. A multicenter study across 36 datasets.](http://arxiv.org/abs/2211.04125) | 本文描述了一项多中心研究，旨在评估MRI数据协调在机器学习中的效用;作者提出了一种“调和器变压器”方法，在不泄露信息的前提下，在机器学习的预处理步骤中实现了数据协调。 |
| [^141] | [FingerFlex: Inferring Finger Trajectories from ECoG signals.](http://arxiv.org/abs/2211.01960) | 本文介绍了FingerFlex模型，该模型通过深度学习算法在ECoG信号上实现手指运动回归，取得了最先进的表现，为开发高精度皮层运动脑机接口提供了机会。 |
| [^142] | [Study of Manifold Geometry using Multiscale Non-Negative Kernel Graphs.](http://arxiv.org/abs/2210.17475) | 本文提出了一个框架，利用非负核图的方法研究数据流形的几何结构，并且通过多尺度的迭代合并构建出更准确的结构，实验证明了其有效性。 |
| [^143] | [Learning image representations for anomaly detection: application to discovery of histological alterations in drug development.](http://arxiv.org/abs/2210.07675) | 该论文提出了一种基于CNN的异常检测系统，通过对健康组织进行辅助任务训练，使表示适应组织中的相关细节，实现对组织学图像中的异常情况检测。 |
| [^144] | [Computationally-efficient initialisation of GPs: The generalised variogram method.](http://arxiv.org/abs/2210.05394) | 该论文提出了一种计算有效的策略，避免了计算似然函数，可用作初始化超参数，使模型得到更好的训练，该策略在实践中证明了对于高斯过程是有效的。 |
| [^145] | [Efficient Learning of Mesh-Based Physical Simulation with BSMS-GNN.](http://arxiv.org/abs/2210.02573) | 该论文提出了一种名为bi-stride的新型池化策略，利用二部图决策实现高效的基于网格的物理仿真，无需手动绘制粗网格，并避免了空间接近性带来的错误边缘。 |
| [^146] | [FADE: Enabling Federated Adversarial Training on Heterogeneous Resource-Constrained Edge Devices.](http://arxiv.org/abs/2209.03839) | 这篇论文提出了一个名为FADE的联邦对抗训练框架，能够在资源受限的边缘设备上实现对抗训练，框架将整个模型差分分解成小模块来适应设备的资源预算，并提出辅助权重衰减方法以获得更好的准确度-鲁棒性平衡。 |
| [^147] | [TransPolymer: a Transformer-based language model for polymer property predictions.](http://arxiv.org/abs/2209.01307) | 本文提出了一种基于Transformer的高分子属性预测语言模型TransPolymer，它利用化学感知能力的高分子分词器学习高分子序列的表示，并通过预训练获得了更好的性能。 |
| [^148] | [Positive Difference Distribution for Image Outlier Detection using Normalizing Flows and Contrastive Data.](http://arxiv.org/abs/2208.14024) | 本研究提出了一种使用未标记的辅助数据集和概率异常得分进行异常检测的方法，可以有效地检测与训练数据有偏差的测试数据。该方法使用自监督特征提取器训练，通过学习正差分布来提高检测效果并在基准数据集上进行验证。 |
| [^149] | [One-vs-the-Rest Loss to Focus on Important Samples in Adversarial Training.](http://arxiv.org/abs/2207.10283) | 本文提出了一种名为SOVR的对抗训练损失函数，可以聚焦重要样本，增加对抗攻击下的对数几率间隔，从而在实验中表现出对抗攻击的有效性。 |
| [^150] | [Language Modelling with Pixels.](http://arxiv.org/abs/2207.06991) | 本文介绍了一个名为PIXEL的基于像素的预训练语言模型，它可以将文本渲染为图像，解决了扩展支持的语言数量时出现的词汇瓶颈问题，且在形态学和语义任务上显著优于BERT。 |
| [^151] | [Data-driven reduced order models using invariant foliations, manifolds and autoencoders.](http://arxiv.org/abs/2206.12269) | 本文研究了从物理系统中识别简化模型的方法，发现使用不变叶片是从现有数据中识别ROM的常见方法。其余方法只能识别完整模型。研究表明找到不变叶片需要近似高维函数，使用带有压缩张量系数的多项式即可。 |
| [^152] | [Flexible Differentiable Optimization via Model Transformations.](http://arxiv.org/abs/2206.06135) | 本文介绍了DiffOpt.jl，一个基于MathOptInterface构建的Julia库，它可以对任何模型的参数进行微分求解，不仅限于凸锥规划和二次规划标准形式。使用该库, 可以实现灵活高效地求解一系列模型的优化问题，为基于优化的机器学习提供了新的机会。 |
| [^153] | [A Review and Evaluation of Elastic Distance Functions for Time Series Clustering.](http://arxiv.org/abs/2205.15181) | 本文针对时间序列聚类中的弹性距离度量算法进行了综述和评估，发现常用的距离度量方法之一-DTW，在与k-means结合时表现不如欧几里得距离。使用k-medoids可以改善聚类效果。 |
| [^154] | [A Rule Search Framework for the Early Identification of Chronic Emergency Homeless Shelter Clients.](http://arxiv.org/abs/2205.09883) | 本研究使用规则搜索技术，早期识别有可能成为长期或慢性无家可归者收容所的高风险人群。在实时交付支持性住房计划的框架内，应用本文方法使得早期识别处于慢性无家可归者风险的客户的中位时间从297天降至162天。 |
| [^155] | [DECONET: an Unfolding Network for Analysis-based Compressed Sensing with Generalization Error Bounds.](http://arxiv.org/abs/2205.07050) | 本文提出了一种名为DECONET的新型深度展开网络，可以用于基于分析稀疏性的压缩感知，能有效地重构向量并优于现有的展开网络，在估计了其泛化误差的基础上得出相关结论。 |
| [^156] | [Detection of sepsis during emergency department triage using machine learning.](http://arxiv.org/abs/2204.07657) | 本研究利用机器学习开发出一种检测急诊科分诊前败血症的模型，其性能优于标准败血症筛查算法。 |
| [^157] | [Graph Neural Networks Designed for Different Graph Types: A Survey.](http://arxiv.org/abs/2204.03080) | 本综述详细介绍了已经存在的GNN，并根据其处理不同图类型和属性的能力对它们进行分类。 |
| [^158] | [Quantum compiling with variational instruction set for accurate and fast quantum computing.](http://arxiv.org/abs/2203.15574) | 本文提出了基于灵活设计的多量子比特门操作的量子变分指令集(QuVIS)，通过细粒度的时间优化算法，实现了快速和精确的量子计算，并且通过实验展示出了更低的误差积累和时间成本。 |
| [^159] | [Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation.](http://arxiv.org/abs/2203.11740) | 该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。 |
| [^160] | [Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions.](http://arxiv.org/abs/2202.00992) | 本文提出了一种新的谱条件，用于提供具有幂律优化轨迹的问题的更紧密上界，演示了如何统一获得最优加速方法及其计划和收敛上界。 |
| [^161] | [Leveraging Bitstream Metadata for Fast, Accurate, Generalized Compressed Video Quality Enhancement.](http://arxiv.org/abs/2202.00011) | 本文提出了一种利用视频比特流元数据进行深度学习的压缩视频质量增强方法，在实现更高吞吐量的同时提高了压缩视频的还原准确性，具有重要的应用价值。 |
| [^162] | [Restarted Nonconvex Accelerated Gradient Descent: No More Polylogarithmic Factor in the $O(\epsilon^{-7/4})$ Complexity.](http://arxiv.org/abs/2201.11411) | 本文提出了重启式加速梯度下降（AGD）和重启式重球（HB）方法来解决非凸优化问题，并通过基本证明证明了这些方法可以在$O(\epsilon^{-7/4})$个梯度评估内达到$\epsilon$近似的一阶稳定点。这项工作的复杂度没有多项式对数因子，并优于以前的最佳复杂度。 |
| [^163] | [On the effectiveness of Randomized Signatures as Reservoir for Learning Rough Dynamics.](http://arxiv.org/abs/2201.00384) | 本文研究了随机签名方法在学习粗糙动态中的表现，并发现它比截断签名方法和其他深度学习技术更具优势。 |
| [^164] | [Abstract Interpretation of Fixpoint Iterators with Applications to Neural Networks.](http://arxiv.org/abs/2110.08260) | 该论文提出了一种新的抽象解释框架，使我们能够精确地过度逼近数值Fixpoint迭代器。使用一种新的抽象域，CH-Zonotope，可以实现有效的传播和包含检查。该框架在研究monDEQ神经网络结构时表现出色，是一种有前途的验证技术。 |
| [^165] | [Transfer-Recursive-Ensemble Learning for Multi-Day COVID-19 Prediction in India using Recurrent Neural Networks.](http://arxiv.org/abs/2108.09131) | 本文提出了基于循环神经网络的预测新冠病例数量的方法，并利用递归转移集成学习和预训练模型，实现了印度多日COVID-19疫情趋势的预测。 |
| [^166] | [FairBalance: How to Achieve Equalized Odds With Data Pre-processing.](http://arxiv.org/abs/2107.08310) | 本研究提供了一种简单而有效的预处理方法，旨在实现机器学习软件的等几率公平性问题，通过平衡每个族群中的类别分布，以达到这一目标。 |
| [^167] | [Connection Sensitivity Matters for Training-free DARTS: From Architecture-Level Scoring to Operation-Level Sensitivity Analysis.](http://arxiv.org/abs/2106.11542) | 本论文提出一种叫做ZEROS的连接概念来评估DARTS中的操作重要性，使得整个架构的搜索过程更高效，提出了一种基于NTK理论的全新框架FreeDARTS。 |
| [^168] | [Benchmarking Multivariate Time Series Classification Algorithms.](http://arxiv.org/abs/2007.13156) | 该论文比较了单变量时间序列分类（TSC）与多元TSC(MTSC)问题的算法。作者测试了基于深度学习、形状和单词袋方法的算法，并将其与维度无关的方法进行比较。 |
| [^169] | [A tale of two toolkits, report the third: on the usage and performance of HIVE-COTE v1.0.](http://arxiv.org/abs/2004.06069) | 介绍了用于时间序列分类的异构元集成算法 HIVE-COTE 的最新稳定版本 1.0，提供了使用指南，并通过实验评估了其性能和资源使用情况，并与三种近期提出的算法进行了比较。 |

# 详细

[^1]: 一种以控制为中心的视频预测基准

    A Control-Centric Benchmark for Video Prediction. (arXiv:2304.13723v1 [cs.CV])

    [http://arxiv.org/abs/2304.13723](http://arxiv.org/abs/2304.13723)

    本文提出了一个以控制为中心的视频预测基准，评估给定模型在通过采样规划对模拟机器人操作的表现。该基准包含有11个任务类别和310个任务实例定义的模拟环境，以及完整的规划实现和训练数据集，以解决现有指标在预测任务执行成功方面不可靠的问题。

    

    视频是学习世界动态模型的体现代理人的有希望的知识来源。大型深度网络在自我监督的情况下越来越有效地建模复杂的视频数据，评估基于人类感知相似性或像素比较的指标。然而，目前的指标是否准确预测下游任务的表现仍不清楚。我们实验证明，对于规划机器人操作来说，现有指标在预测任务执行成功方面可能不可靠。为了解决这个问题，我们提出了一种行动条件下的视频预测基准，即通过采样规划对给定模型进行评估的控制基准。我们的基准，用于视觉规划的视频预测 ($VP^2$)，包括11个任务类别和310个任务实例定义的模拟环境、完整的规划实现和包含脚本交互轨迹的训练数据集。

    Video is a promising source of knowledge for embodied agents to learn models of the world's dynamics. Large deep networks have become increasingly effective at modeling complex video data in a self-supervised manner, as evaluated by metrics based on human perceptual similarity or pixel-wise comparison. However, it remains unclear whether current metrics are accurate indicators of performance on downstream tasks. We find empirically that for planning robotic manipulation, existing metrics can be unreliable at predicting execution success. To address this, we propose a benchmark for action-conditioned video prediction in the form of a control benchmark that evaluates a given model for simulated robotic manipulation through sampling-based planning. Our benchmark, Video Prediction for Visual Planning ($VP^2$), includes simulated environments with 11 task categories and 310 task instance definitions, a full planning implementation, and training datasets containing scripted interaction traje
    
[^2]: 稀疏化模型动物园双胞胎：研究稀疏神经网络模型族的行为和鲁棒性

    Sparsified Model Zoo Twins: Investigating Populations of Sparsified Neural Network Models. (arXiv:2304.13718v1 [cs.LG])

    [http://arxiv.org/abs/2304.13718](http://arxiv.org/abs/2304.13718)

    本文研究了两种稀疏化方法在模型族中的表现，并发现幅度修剪方法优于变分丢失方法，除了高于80%的较高稀疏化比率之外，两种方法都非常稳健。

    

    随着神经网络（NNs）的规模增长，模型稀疏化以减少模型推理的计算成本和内存需求已经成为研究和生产的关键兴趣。虽然许多稀疏化方法已被提出并成功应用于个体模型，但据我们所知，它们的行为和鲁棒性尚未在大量模型族上进行研究。本文通过将两种流行的稀疏化方法应用于模型族（所谓的模型动物园）上，创建原始动物园的稀疏化版本，来填补这个空白。我们研究了这两种方法在每个动物园中的表现，逐层比较稀疏化，并分析原始族群和稀疏化后族群之间的一致性。我们发现，除了高于80%的较高稀疏化比率之外，两种方法都非常稳健，而幅度修剪能够优于变分丢失。此外，我们发现稀疏化模型与原始模型高度一致。

    With growing size of Neural Networks (NNs), model sparsification to reduce the computational cost and memory demand for model inference has become of vital interest for both research and production. While many sparsification methods have been proposed and successfully applied on individual models, to the best of our knowledge their behavior and robustness has not yet been studied on large populations of models. With this paper, we address that gap by applying two popular sparsification methods on populations of models (so called model zoos) to create sparsified versions of the original zoos. We investigate the performance of these two methods for each zoo, compare sparsification layer-wise, and analyse agreement between original and sparsified populations. We find both methods to be very robust with magnitude pruning able outperform variational dropout with the exception of high sparsification ratios above 80%. Further, we find sparsified models agree to a high degree with their origin
    
[^3]: 自编码器实现的关联规则挖掘

    Association Rules Mining with Auto-Encoders. (arXiv:2304.13717v1 [cs.LG])

    [http://arxiv.org/abs/2304.13717](http://arxiv.org/abs/2304.13717)

    本文提出了一种基于自编码器的关联规则挖掘算法ARM-AE，可在保证规则质量的前提下，挖掘出高支持度和置信度规则集，并具有比传统方法更好的执行时间。

    

    关联规则挖掘是数据挖掘中研究最广泛的领域之一，应用领域从杂货篮问题到可解释的分类系统均有涉及。然而，传统的关联规则挖掘算法存在多个限制，特别是执行时间和生成规则数方面。近十年来，神经网络已被用于解决各种优化问题，如分类、回归或聚类。尽管如此，在挖掘关联规则方面仍然没有有效的神经网络解决方案。本文提出了一种基于自编码器的关联规则挖掘算法ARM-AE，并以三个分类数据集为例将其与FP-Growth和NSGAII进行了比较。结果表明，我们的算法在保持所产生的规则集质量的同时，挖掘出高支持度和置信度规则集，并具有比传统方法更好的执行时间。

    Association rule mining is one of the most studied research fields of data mining, with applications ranging from grocery basket problems to explainable classification systems. Classical association rule mining algorithms have several limitations, especially with regards to their high execution times and number of rules produced. Over the past decade, neural network solutions have been used to solve various optimization problems, such as classification, regression or clustering. However there are still no efficient way association rules using neural networks. In this paper, we present an auto-encoder solution to mine association rule called ARM-AE. We compare our algorithm to FP-Growth and NSGAII on three categorical datasets, and show that our algorithm discovers high support and confidence rule set and has a better execution time than classical methods while preserving the quality of the rule set produced.
    
[^4]: 发挥LLMs在实践中的力量：ChatGPT及其应用的综述调查

    Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])

    [http://arxiv.org/abs/2304.13712](http://arxiv.org/abs/2304.13712)

    本文提供了一个LLMs的使用综述，探讨了在各种自然语言处理任务中的使用和限制。

    

    本文为从事下游自然语言处理（NLP）任务的从业人员和最终用户提供了一个全面实用的指南，介绍了如何利用Large Language Models（LLMs）。我们从模型、数据和下游任务的角度提供了LLMs的使用讨论和见解。首先，我们介绍了当前的GPT和BERT样式的LLMs。然后，讨论了预训练数据、训练数据和测试数据的影响。最重要的是，我们详细讨论了大型语言模型在各种自然语言处理任务中的使用和非使用情况，例如知识密集型任务、传统自然语言理解任务、自然语言生成任务、紧急能力以及特定任务的考虑。我们呈现了各种使用和非使用情况，以说明LLMs在实际情况下的实际应用和限制。我们还试图了解数据对于LLMs应用的重要性。

    This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of dat
    
[^5]: 带种植模式的Hopfield模型：一种师生自我监督学习模型

    Hopfield model with planted patterns: a teacher-student self-supervised learning model. (arXiv:2304.13710v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2304.13710](http://arxiv.org/abs/2304.13710)

    该论文提出了一种基于师生自我监督学习问题的Hopfield模型，能够帮助机器利用结构化的模式来学习，虽然一些条件对于学习非常重要，但这种学习模式在特定条件下可以实现泛化。

    

    尽管Hopfield网络被认为是记忆存储和检索的典型模型，但现代人工智能系统主要基于机器学习范式。我们展示了如何利用具有结构化模式的Hopfield模型的适当推广来构建Boltzmann机的师生自我监督学习问题，其中自旋变量是机器权重，模式对应于训练集的示例。我们通过研究相图来分析学习性能，这些相图是通过训练集大小、数据集噪声和推断温度（即权重正则化）来构建的。使用小而富信息的数据集，机器可以通过记忆来学习。使用嘈杂的数据集，则需要大量的示例数以超过临界阈值。在这个区域，系统的存储限制成为产生一种学习模式的机会，在这种模式下，系统可以进行泛化。

    While Hopfield networks are known as paradigmatic models for memory storage and retrieval, modern artificial intelligence systems mainly stand on the machine learning paradigm. We show that it is possible to formulate a teacher-student self-supervised learning problem with Boltzmann machines in terms of a suitable generalization of the Hopfield model with structured patterns, where the spin variables are the machine weights and patterns correspond to the training set's examples. We analyze the learning performance by studying the phase diagram in terms of the training set size, the dataset noise and the inference temperature (i.e. the weight regularization). With a small but informative dataset the machine can learn by memorization. With a noisy dataset, an extensive number of examples above a critical threshold is needed. In this regime the memory storage limits of the system becomes an opportunity for the occurrence of a learning regime in which the system can generalize.
    
[^6]: 使用低成本硬件学习精细的双手操作

    Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware. (arXiv:2304.13705v1 [cs.RO])

    [http://arxiv.org/abs/2304.13705](http://arxiv.org/abs/2304.13705)

    本文介绍了一种低成本系统，它能够在没有高端机器人、准确的传感器或精心的校准的情况下进行精细双手操作的模仿学习。作者开发了一种名为Action Chunking with Transformers (ACT)的新颖算法，它学习了一个带有动作序列的生成模型，从而允许机器人学习复杂的双手操作。

    

    机器人进行精细操纵任务，如穿缆带或插入电池之类的任务，一直是难题，因为它们需要精确的协调、紧闭回路视觉反馈。通常需要高端机器人、准确的传感器或精心的校准，这可能是昂贵且难以安装。机器学习是否能够使低成本和不精确的硬件也能执行这些精细操纵任务？我们提出了一种低成本系统，直接从使用自定义远程操作界面收集的真实演示中进行端到端的模仿学习。然而，模仿学习本身也存在着挑战，特别是在高精度领域：策略中的错误可能会随着时间的推移而不断累积，并且人类演示可能是非静态的。为了解决这些挑战，我们开发了一种简单而新颖的算法，Action Chunking with Transformers (ACT)，它学习了一个带有动作序列的生成模型。ACT允许机器人学习复杂的双手操作。

    Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to 
    
[^7]: 应用于人脸生物识别的AI模型偏差的统计方法测量

    Measuring Bias in AI Models with Application to Face Biometrics: An Statistical Approach. (arXiv:2304.13680v1 [cs.LG])

    [http://arxiv.org/abs/2304.13680](http://arxiv.org/abs/2304.13680)

    本文提出使用N-Sigma方法来统计测量机器学习模型中的偏差，以开发基于偏差分析的新风险评估框架，尤其在人脸识别技术方面具有一定的优势和缺点。

    

    欧盟委员会发布了新的关于人工智能（AI）的法规框架草案，提出了一种新的基于风险的法律方法。草案强调需要为不同的AI用途开发充分的风险评估。这种风险评估应包括检测和减轻AI中的偏差。本文分析了测量自动决策系统中偏差的统计方法。我们的实验重点是人脸识别技术。我们提出了一种新的测量机器学习模型中偏差的方式，该方式基于N-Sigma方法的统计方法。N-Sigma是一种流行的统计方法，用于验证一般科学（如物理和社会领域）中的假设，其应用于机器学习尚未得到探索。在本文中，我们研究如何应用这种方法来开发基于偏差分析的新风险评估框架，并讨论其主要优缺点。

    The new regulatory framework proposal on Artificial Intelligence (AI) published by the European Commission establishes a new risk-based legal approach. The proposal highlights the need to develop adequate risk assessments for the different uses of AI. This risk assessment should address, among others, the detection and mitigation of bias in AI. In this work we analyze statistical approaches to measure biases in automatic decision-making systems. We focus our experiments in face recognition technologies. We propose a novel way to measure the biases in machine learning models using a statistical approach based on the N-Sigma method. N-Sigma is a popular statistical approach used to validate hypotheses in general science such as physics and social areas and its application to machine learning is yet unexplored. In this work we study how to apply this methodology to develop new risk assessment frameworks based on bias analysis and we discuss the main advantages and drawbacks with respect t
    
[^8]: 从数据学习电池模型参数动态的递归高斯过程回归

    Learning battery model parameter dynamics from data with recursive Gaussian process regression. (arXiv:2304.13666v1 [eess.SY])

    [http://arxiv.org/abs/2304.13666](http://arxiv.org/abs/2304.13666)

    本文提出了一种结合数据和模型驱动技术的混合方法来估计电池健康状况。通过递归高斯过程回归学习参数动态，并且对间隙和变化的操作条件具有鲁棒性。

    

    电池管理系统中估计健康状态是一个关键的功能，但由于实际应用的操作条件和使用要求的变化性，仍然具有挑战性。因此，基于等效电路模型的技术在性能极端和长期老化时可能出现不准确性或参数估计不稳定。另一方面，完全数据驱动的技术受限于其训练数据集之外的泛化性。本文提出了一种结合数据和模型驱动技术的混合方法来估计电池健康状况。具体而言，我们演示了一种贝叶斯数据驱动方法，高斯过程回归，将模型参数估计为状态、操作条件和寿命的函数。通过一种递归方法确保计算效率，得到一个统一的联合状态-参数估计器，从数据中学习参数动态，并且对间隙和变化的操作条件具有鲁棒性。

    Estimating state of health is a critical function of a battery management system but remains challenging due to the variability of operating conditions and usage requirements of real applications. As a result, techniques based on fitting equivalent circuit models may exhibit inaccuracy at extremes of performance and over long-term ageing, or instability of parameter estimates. Pure data-driven techniques, on the other hand, suffer from lack of generality beyond their training dataset. In this paper, we propose a hybrid approach combining data- and model-driven techniques for battery health estimation. Specifically, we demonstrate a Bayesian data-driven method, Gaussian process regression, to estimate model parameters as functions of states, operating conditions, and lifetime. Computational efficiency is ensured through a recursive approach yielding a unified joint state-parameter estimator that learns parameter dynamics from data and is robust to gaps and varying operating conditions. 
    
[^9]: 基于数据驱动的分段仿射决策规则用于带协变信息的随机规划

    Data-driven Piecewise Affine Decision Rules for Stochastic Programming with Covariate Information. (arXiv:2304.13646v1 [math.OC])

    [http://arxiv.org/abs/2304.13646](http://arxiv.org/abs/2304.13646)

    本研究提出一种嵌入非凸分段仿射决策规则的经验风险最小化方法，用于学习特征与最优决策之间的直接映射。所提出的方法可用于广泛的非凸型SP问题，并且在数值研究中表现出优越的性能。

    

    本文针对带协变信息的随机规划，提出了一种嵌入非凸分段仿射决策规则(PADR)的经验风险最小化(ERM)方法，旨在学习特征与最优决策之间的直接映射。我们建立了基于PADR的ERM模型的非渐近一致性结果，可用于无约束问题，以及约束问题的渐近一致性结果。为了解决非凸和非可微的ERM问题，我们开发了一个增强的随机主导下降算法，并建立了沿（复合强）方向稳定性的渐近收敛以及复杂性分析。我们表明，所提出的PADR-based ERM方法适用于广泛的非凸型SP问题，并具有理论一致性保证和计算可处理性。数值研究表明，在各种设置下，PADR-based ERM方法相对于最先进的方法具有优越的性能。

    Focusing on stochastic programming (SP) with covariate information, this paper proposes an empirical risk minimization (ERM) method embedded within a nonconvex piecewise affine decision rule (PADR), which aims to learn the direct mapping from features to optimal decisions. We establish the nonasymptotic consistency result of our PADR-based ERM model for unconstrained problems and asymptotic consistency result for constrained ones. To solve the nonconvex and nondifferentiable ERM problem, we develop an enhanced stochastic majorization-minimization algorithm and establish the asymptotic convergence to (composite strong) directional stationarity along with complexity analysis. We show that the proposed PADR-based ERM method applies to a broad class of nonconvex SP problems with theoretical consistency guarantees and computational tractability. Our numerical study demonstrates the superior performance of PADR-based ERM methods compared to state-of-the-art approaches under various settings,
    
[^10]: PVP: 预训练的视觉参数高效微调

    PVP: Pre-trained Visual Parameter-Efficient Tuning. (arXiv:2304.13639v1 [cs.CV])

    [http://arxiv.org/abs/2304.13639](http://arxiv.org/abs/2304.13639)

    本文提出了一种名为PVP的新方法，通过利用预训练模型的统计信息和无监督聚类方法初始化提示模块，可以在极少的标记数据（每类一到两个示例）的情况下获得具有竞争力的性能。

    

    大规模预训练变换器在各种计算机视觉任务中取得了显著的成功。然而，由于其高计算和存储成本，在下游任务中完全微调这些模型仍然面临着极大的挑战。我们首次通过经验探究发现，大多数PETuning方法仍需要大量的下游任务训练数据才能取得良好的结果。为了克服这个问题，我们提出了PVP，利用预训练模型的统计信息，利用无监督聚类方法初始化提示模块。PVP可在各种图像分类和少样本学习基准上仅使用少量标记数据（例如每类一到两个示例）实现具有竞争力的性能。

    Large-scale pre-trained transformers have demonstrated remarkable success in various computer vision tasks. However, it is still highly challenging to fully fine-tune these models for downstream tasks due to their high computational and storage costs. Recently, Parameter-Efficient Tuning (PETuning) techniques, e.g., Visual Prompt Tuning (VPT) and Low-Rank Adaptation (LoRA), have significantly reduced the computation and storage cost by inserting lightweight prompt modules into the pre-trained models and tuning these prompt modules with a small number of trainable parameters, while keeping the transformer backbone frozen. Although only a few parameters need to be adjusted, most PETuning methods still require a significant amount of downstream task training data to achieve good results. The performance is inadequate on low-data regimes, especially when there are only one or two examples per class. To this end, we first empirically identify the poor performance is mainly due to the inappr
    
[^11]: ChartSumm：长短摘要自动生成任务的全面基准数据集

    ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])

    [http://arxiv.org/abs/2304.13620](http://arxiv.org/abs/2304.13620)

    本文提出了ChartSumm数据集，用于长短摘要自动生成任务，包括84000多个图表及其元数据和描述。研究发现，现有的自动摘要模型虽然得分不错，但经常面临错觉、漏掉重要数据点以及不正确解释复杂趋势等问题。

    

    自动将图表转换为文本摘要是视障人士的有效工具，同时为用户提供表格数据的自然语言精确洞察力。大型、结构良好的数据集始终是数据驱动模型的关键部分。本文提出了ChartSumm：一个大规模基准数据集，包括共84363个图表及其元数据和描述，涵盖广泛的主题和图表类型，可生成长短摘要。强基线模型的广泛实验表明，尽管这些模型通过实现各种自动评估指标的得分来生成流畅且信息丰富的摘要，但它们经常遇到一些问题，例如产生错觉，漏掉重要的数据点，以及不正确地解释图表中的复杂趋势。我们还通过自动翻译工具探讨了将ChartSumm扩展到其他语言的潜力。这使得我们的数据集成为一个有挑战的任务。

    Automatic chart to text summarization is an effective tool for the visually impaired people along with providing precise insights of tabular data in natural language to the user. A large and well-structured dataset is always a key part for data driven models. In this paper, we propose ChartSumm: a large-scale benchmark dataset consisting of a total of 84,363 charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. Extensive experiments with strong baseline models show that even though these models generate fluent and informative summaries by achieving decent scores in various automatic evaluation metrics, they often face issues like suffering from hallucination, missing out important data points, in addition to incorrect explanation of complex trends in the charts. We also investigated the potential of expanding ChartSumm to other languages using automated translation tools. These make our dataset a challeng
    
[^12]: CROP: 使用紧凑重塑观察处理实现分布偏移鲁棒强化学习

    CROP: Towards Distributional-Shift Robust Reinforcement Learning using Compact Reshaped Observation Processing. (arXiv:2304.13616v1 [cs.LG])

    [http://arxiv.org/abs/2304.13616](http://arxiv.org/abs/2304.13616)

    CROP是一种新的强化学习算法，通过使用紧凑重塑观察处理来减少用于政策优化的状态信息，避免过度拟合特定的训练布局，并提高在未知环境中的泛化能力。

    

    强化学习的安全应用需要从有限的训练数据中推广到未知情境。然而，应对不断变化的情况是强化学习中的一项关键挑战。当前最先进的通用化方法应用数据增强技术来增加训练数据的多样性。尽管这可以防止过度拟合训练环境，但也会阻碍政策优化。设计一个合适的观察信息，只包含关键信息，已被证明是一个困难的任务。为了提高数据效率和泛化能力，我们提出了紧凑重塑观察处理（CROP），以减少用于政策优化的状态信息。通过提供只有相关信息，可以避免过度拟合特定的训练布局，并提高在未知环境中的泛化能力。我们制定了三种CROP，可应用于完全可观察的观察和行动空间，并提供方法ologically地进行评估。

    The safe application of reinforcement learning (RL) requires generalization from limited training data to unseen scenarios. Yet, fulfilling tasks under changing circumstances is a key challenge in RL. Current state-of-the-art approaches for generalization apply data augmentation techniques to increase the diversity of training data. Even though this prevents overfitting to the training environment(s), it hinders policy optimization. Crafting a suitable observation, only containing crucial information, has been shown to be a challenging task itself. To improve data efficiency and generalization capabilities, we propose Compact Reshaped Observation Processing (CROP) to reduce the state information used for policy optimization. By providing only relevant information, overfitting to a specific training layout is precluded and generalization to unseen environments is improved. We formulate three CROPs that can be applied to fully observable observation- and action-spaces and provide methodi
    
[^13]: Diffsurv: 可区分的排序用于有审查时间的数据

    Diffsurv: Differentiable sorting for censored time-to-event data. (arXiv:2304.13594v1 [cs.LG])

    [http://arxiv.org/abs/2304.13594](http://arxiv.org/abs/2304.13594)

    为了处理带有审核任务的生存分析，我们提出了一种基于Diffsurv的新方法，通过预测可能组合矩阵，考虑到引入的标签不确定性，实现可区分的排序。

    

    生存分析是一项重要的半监督任务，在机器学习中具有很多现实世界的应用，尤其是在医疗领域。目前，生存分析最常见的方法是基于Cox的部分似然，可解释为在一致性指数的下限上优化的排序模型。这种排序模型和Cox的部分似然之间的关系仅考虑了成对比较。最近的工作发展了可区分排序的方法，放松了这种成对独立假设，使得能够对样本集进行排序。然而，当前的可区分排序方法不能考虑到许多真实世界数据中的关键因素——审查。为了解决这个限制，我们提出了一种新的方法Diffsurv。我们通过预测可能排列矩阵来扩展不同iable排序方法以处理审查任务，这些矩阵考虑到审查样本引入的标签不确定性。我们将这种方法与方法进行比较...

    Survival analysis is a crucial semi-supervised task in machine learning with numerous real-world applications, particularly in healthcare. Currently, the most common approach to survival analysis is based on Cox's partial likelihood, which can be interpreted as a ranking model optimized on a lower bound of the concordance index. This relation between ranking models and Cox's partial likelihood considers only pairwise comparisons. Recent work has developed differentiable sorting methods which relax this pairwise independence assumption, enabling the ranking of sets of samples. However, current differentiable sorting methods cannot account for censoring, a key factor in many real-world datasets. To address this limitation, we propose a novel method called Diffsurv. We extend differentiable sorting methods to handle censored tasks by predicting matrices of possible permutations that take into account the label uncertainty introduced by censored samples. We contrast this approach with meth
    
[^14]: 基于互信息比例的 Thompson 抽样算法在子高斯奖励情境下的遗憾界研究

    Thompson Sampling Regret Bounds for Contextual Bandits with sub-Gaussian rewards. (arXiv:2304.13593v1 [stat.ML])

    [http://arxiv.org/abs/2304.13593](http://arxiv.org/abs/2304.13593)

    本文研究了子高斯奖励情境下的 Thompson 抽样算法在情境 Bandit 问题中的性能，并引入了提高信息比率的新边界。

    

    本文研究了基于 Neu et al. 的框架和其提出的互信息比例概念的情境 Bandit 问题中的 Thompson 抽样算法表现。首先，我们证明了 Thompson 抽样期望累计遗憾的全面边界取决于环境参数和历史的互信息。然后，我们引入了对子高斯奖励成立的提高信息比率的新边界，从而推广了 Neu 等人的结果，其分析要求二进制奖励。最后，我们为非结构化有界情境 Bandit、结构化有界情境 Bandit（拉普拉斯似然函数）、结构化 Bernoulli Bandit 和有界线性情境 Bandit 提供了明确的遗憾界。

    In this work, we study the performance of the Thompson Sampling algorithm for Contextual Bandit problems based on the framework introduced by Neu et al. and their concept of lifted information ratio. First, we prove a comprehensive bound on the Thompson Sampling expected cumulative regret that depends on the mutual information of the environment parameters and the history. Then, we introduce new bounds on the lifted information ratio that hold for sub-Gaussian rewards, thus generalizing the results from Neu et al. which analysis requires binary rewards. Finally, we provide explicit regret bounds for the special cases of unstructured bounded contextual bandits, structured bounded contextual bandits with Laplace likelihood, structured Bernoulli bandits, and bounded linear contextual bandits.
    
[^15]: 能量为基础的切片Wasserstein距离

    Energy-Based Sliced Wasserstein Distance. (arXiv:2304.13586v1 [stat.ML])

    [http://arxiv.org/abs/2304.13586](http://arxiv.org/abs/2304.13586)

    本文提出了一种能量为基础的切片Wasserstein距离，并将其参数化，以克服传统方法中的固定先验分布缺乏信息和优化最佳分布昂贵不稳定的局限。

    

    切片Wasserstein（SW）距离被广泛认为是两个概率测度之间的一种统计有效且计算高效的度量。SW距离的一个关键部分是切片分布。目前有两种方法来选择这个分布。第一种方法是使用固定的先验分布。第二种是优化归属于参数分布族的最佳分布，并且可以最大化期望的距离。然而，这两种方法都有局限性。固定的先验分布在突出能够区分两个常规概率测度的投影方向方面缺乏信息。而优化最佳分布通常是昂贵和不稳定的。此外，设计候选分布的参数分布族可能会很容易被错误指定。为了解决这些问题，我们提出将切片分布设计为基于能量的分布，并将其参数化，从而使其更加通用而稳健。

    The sliced Wasserstein (SW) distance has been widely recognized as a statistically effective and computationally efficient metric between two probability measures. A key component of the SW distance is the slicing distribution. There are two existing approaches for choosing this distribution. The first approach is using a fixed prior distribution. The second approach is optimizing for the best distribution which belongs to a parametric family of distributions and can maximize the expected distance. However, both approaches have their limitations. A fixed prior distribution is non-informative in terms of highlighting projecting directions that can discriminate two general probability measures. Doing optimization for the best distribution is often expensive and unstable. Moreover, designing the parametric family of the candidate distribution could be easily misspecified. To address the issues, we propose to design the slicing distribution as an energy-based distribution that is parameter
    
[^16]: 在众包数据集中“我”迷失在翻译中：代词错误步骤的问题

    "I'm" Lost in Translation: Pronoun Missteps in Crowdsourced Data Sets. (arXiv:2304.13557v1 [cs.CL])

    [http://arxiv.org/abs/2304.13557](http://arxiv.org/abs/2304.13557)

    这项研究发现在英语和日语之间的翻译中存在男性代词偏见，同时也检测到了对女性、中性和/或非二元代词存在的微妙反应的偏见。他们提出了针对代词翻译偏见的问题，并提供了将复数嵌入NLP数据集的解决方案。

    

    随着虚拟助手在全球范围内的普及，越来越需要这些语音系统以各种语言自然地进行交流。众包倡议已经专注于对大型开放数据集进行多语言翻译，以用于自然语言处理（NLP）。然而，语言翻译通常不是一对一的，并且偏见可能会逐渐渗入。在这项最新工作中，我们关注了在众包Tatoeba数据库中英语和日语之间翻译的代词问题。我们发现整体上存在男性代词偏见，即使在其他方式中考虑到语言的复数。重要的是，我们检测到翻译过程中反映了对女性、中性和/或非二元代词存在的微妙反应的偏见。我们提出了代词翻译中的偏见问题，并提供了将复数嵌入NLP数据集的实际解决方案。

    As virtual assistants continue to be taken up globally, there is an ever-greater need for these speech-based systems to communicate naturally in a variety of languages. Crowdsourcing initiatives have focused on multilingual translation of big, open data sets for use in natural language processing (NLP). Yet, language translation is often not one-to-one, and biases can trickle in. In this late-breaking work, we focus on the case of pronouns translated between English and Japanese in the crowdsourced Tatoeba database. We found that masculine pronoun biases were present overall, even though plurality in language was accounted for in other ways. Importantly, we detected biases in the translation process that reflect nuanced reactions to the presence of feminine, neutral, and/or non-binary pronouns. We raise the issue of translation bias for pronouns and offer a practical solution to embed plurality in NLP data sets.
    
[^17]: FLCC: 无线CSMA/CA网络上高效的分布式联邦学习

    FLCC: Efficient Distributed Federated Learning on IoMT over CSMA/CA. (arXiv:2304.13549v1 [cs.DC])

    [http://arxiv.org/abs/2304.13549](http://arxiv.org/abs/2304.13549)

    本文研究了在无线网络中实现FLCC的方法，该方法利用频率复用和空间聚类技术，提高了分布式学习的吞吐量和性能。

    

    联邦学习（FL）已成为隐私保护的一种有前途的方法，允许在用户和云服务器之间共享模型参数，而不是原始的本地数据。在分布式无线节点中实现FL时，FL方法在通信和机器学习性能之间呈现出有趣的相互作用。网络动态和学习都发挥着重要作用。本文研究了在应用程序上使用FL来改进远程医疗系统的性能，该系统在采用CSMA/CA进行调度时使用自组织网络。我们提出的FLCC（在CSMA/CA上实现的FL）模型旨在消除不可信的设备，并利用频率复用和空间聚类技术，提高协调分布式实现FL所需的吞吐量。在我们提出的模型中，频率和位置的选择对实现高效的分布式学习至关重要。

    Federated Learning (FL) has emerged as a promising approach for privacy preservation, allowing sharing of the model parameters between users and the cloud server rather than the raw local data. FL approaches have been adopted as a cornerstone of distributed machine learning (ML) to solve several complex use cases. FL presents an interesting interplay between communication and ML performance when implemented over distributed wireless nodes. Both the dynamics of networking and learning play an important role. In this article, we investigate the performance of FL on an application that might be used to improve a remote healthcare system over ad hoc networks which employ CSMA/CA to schedule its transmissions. Our FL over CSMA/CA (FLCC) model is designed to eliminate untrusted devices and harness frequency reuse and spatial clustering techniques to improve the throughput required for coordinating a distributed implementation of FL in the wireless network.  In our proposed model, frequency a
    
[^18]: 一箭双雕：量化在分布式学习中实现隐私保护与通讯效率

    Killing Two Birds with One Stone: Quantization Achieves Privacy in Distributed Learning. (arXiv:2304.13545v1 [cs.LG])

    [http://arxiv.org/abs/2304.13545](http://arxiv.org/abs/2304.13545)

    本文提出了一种分布式学习中的综合解决方案，基于量化同时实现通讯效率和隐私保护，并且向均匀量化的梯度添加二项噪声以达到所需的差分隐私级别。

    

    分布式机器学习中的通讯效率和隐私保护是两个关键问题。现有的方法分开解决这两个问题，可能在资源有限的环境下应用上受到限制。本文提出了一种基于量化的综合解决方案，能够同时实现通讯效率和隐私保护，并提供了与通讯和隐私相关性质的新见解。具体而言，我们向均匀量化的梯度添加二项噪声以达到所需的差分隐私级别，从而在稍微牺牲通讯效率的情况下证明了所提出的解决方案在分布式随机梯度下降 (SGD) 框架中的有效性。我们理论上捕捉了通讯、隐私和学习性能之间的新权衡。

    Communication efficiency and privacy protection are two critical issues in distributed machine learning. Existing methods tackle these two issues separately and may have a high implementation complexity that constrains their application in a resource-limited environment. We propose a comprehensive quantization-based solution that could simultaneously achieve communication efficiency and privacy protection, providing new insights into the correlated nature of communication and privacy. Specifically, we demonstrate the effectiveness of our proposed solutions in the distributed stochastic gradient descent (SGD) framework by adding binomial noise to the uniformly quantized gradients to reach the desired differential privacy level but with a minor sacrifice in communication efficiency. We theoretically capture the new trade-offs between communication, privacy, and learning performance.
    
[^19]: 基于演化搜索的拜占庭容错学习：超越梯度下降

    Byzantine-Resilient Learning Beyond Gradients: Distributing Evolutionary Search. (arXiv:2304.13540v1 [cs.DC])

    [http://arxiv.org/abs/2304.13540](http://arxiv.org/abs/2304.13540)

    本文介绍了一种新的拜占庭容错ML定义——模型共识，并展示了如何利用一类无梯度ML算法和经典分布式共识算法生成无梯度拜占庭容错学习算法。

    

    现代机器学习（ML）模型表现出色，其能力的提升不仅因于其架构和训练算法的改进，还在于用于训练它们的计算能力的巨大提升。这种巨大提升导致对分布式ML的兴趣日益增长，而这反过来又使工人故障和对抗性攻击变得越来越紧迫。虽然在可微设置中提出了分布式拜占庭容错算法，但在无梯度设置中不存在。本文的目标就是解决这个缺陷。为此，我们引入了一个更一般的拜占庭容错ML定义——模型共识，该定义扩展了经典分布式共识的定义。然后，我们利用这个定义来展示一般类的无梯度ML算法——（1，λ）-演化搜索可以与经典分布式共识算法相结合，生成无梯度拜占庭容错学习算法。

    Modern machine learning (ML) models are capable of impressive performances. However, their prowess is not due only to the improvements in their architecture and training algorithms but also to a drastic increase in computational power used to train them.  Such a drastic increase led to a growing interest in distributed ML, which in turn made worker failures and adversarial attacks an increasingly pressing concern. While distributed byzantine resilient algorithms have been proposed in a differentiable setting, none exist in a gradient-free setting.  The goal of this work is to address this shortcoming. For that, we introduce a more general definition of byzantine-resilience in ML - the \textit{model-consensus}, that extends the definition of the classical distributed consensus. We then leverage this definition to show that a general class of gradient-free ML algorithms - ($1,\lambda$)-Evolutionary Search - can be combined with classical distributed consensus algorithms to generate gradi
    
[^20]: 张量分解用于神经网络模型简化：一项综述

    Tensor Decomposition for Model Reduction in Neural Networks: A Review. (arXiv:2304.13539v1 [cs.LG])

    [http://arxiv.org/abs/2304.13539](http://arxiv.org/abs/2304.13539)

    本文综述了六种张量分解方法，并讨论了将神经网络层替换为低秩张量逼近的方案。实验结果表明这种方法能够显著降低模型大小、运行时间和能量消耗，并且适合于在边缘设备上实现神经网络。

    

    现代神经网络在计算机视觉和自然语言处理等领域取得了革命性的进展。它们被广泛用于解决复杂的计算机视觉任务和自然语言处理任务，如图像分类、图像生成和机器翻译。大多数最先进的神经网络都是过度参数化的，需要高昂的计算成本。一种直接的解决方案是使用不同的张量分解方法将网络的层替换为其低秩张量近似。本文综述了六种张量分解方法并阐述了它们在卷积神经网络、循环神经网络和Transformer的模型参数压缩中的能力。一些压缩模型的准确性甚至可以高于原始版本。评估表明，张量分解可以在模型大小、运行时间和能量消耗方面实现显著降低，并且非常适合在边缘设备上实现神经网络。

    Modern neural networks have revolutionized the fields of computer vision (CV) and Natural Language Processing (NLP). They are widely used for solving complex CV tasks and NLP tasks such as image classification, image generation, and machine translation. Most state-of-the-art neural networks are over-parameterized and require a high computational cost. One straightforward solution is to replace the layers of the networks with their low-rank tensor approximations using different tensor decomposition methods. This paper reviews six tensor decomposition methods and illustrates their ability to compress model parameters of convolutional neural networks (CNNs), recurrent neural networks (RNNs) and Transformers. The accuracy of some compressed models can be higher than the original versions. Evaluations indicate that tensor decompositions can achieve significant reductions in model size, run-time and energy consumption, and are well suited for implementing neural networks on edge devices.
    
[^21]: 立足眼动事件：使用可解释概念用于解释深度神经序列模型

    Bridging the Gap: Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models. (arXiv:2304.13536v1 [cs.LG])

    [http://arxiv.org/abs/2304.13536](http://arxiv.org/abs/2304.13536)

    本文使用眼动事件作为可解释概念，对深度神经序列模型进行解释，研究结果表明扫视输入特征比固定点输入特征更为重要，且接近扫视峰值速度的注视样本最具影响力。

    

    最近在眼动数据的可解释人工智能方面的工作对于解释用于眼动测量生物识别的深度神经序列模型的输出的特征归因方法的适用性进行了评估。这些方法提供显着性地图来突出特定眼注视序列的重要输入特征。但是，到目前为止，它的本地化分析在整个数据集上缺少数量化的方法。在这项工作中，我们采用了已经建立的固定点和扫视眼动事件检测算法，并通过确定它们的概念影响来定量评估这些事件的影响。属于扫视的输入特征比属于固定点的特征重要得多。通过将扫视事件分解为子事件，我们能够展示接近扫视峰值速度的注视样本是最有影响力的。我们进一步研究了事件属性如扫视幅度或固定离散度对深度神经模型准确性的影响。我们的结果展示了使用眼动事件作为可解释概念解释深度神经序列模型的有用性。

    Recent work in XAI for eye tracking data has evaluated the suitability of feature attribution methods to explain the output of deep neural sequence models for the task of oculomotric biometric identification. These methods provide saliency maps to highlight important input features of a specific eye gaze sequence. However, to date, its localization analysis has been lacking a quantitative approach across entire datasets. In this work, we employ established gaze event detection algorithms for fixations and saccades and quantitatively evaluate the impact of these events by determining their concept influence. Input features that belong to saccades are shown to be substantially more important than features that belong to fixations. By dissecting saccade events into sub-events, we are able to show that gaze samples that are close to the saccadic peak velocity are most influential. We further investigate the effect of event properties like saccadic amplitude or fixational dispersion on the 
    
[^22]: 用均场博弈为生成模型搭建实验室

    A mean-field games laboratory for generative modeling. (arXiv:2304.13534v1 [stat.ML])

    [http://arxiv.org/abs/2304.13534](http://arxiv.org/abs/2304.13534)

    本文提出了使用均场博弈作为实验室对生成模型进行设计和分析的方法，并建立了这种方法与主要流动和扩散型生成模型之间的关联。通过研究每个生成模型与它们相关的 MFG 的最优条件，本文提出了一个基于双人 MFG 的新的生成模型，该模型在提高样本多样性和逼真度的同时改善了解缠结和公平性。

    

    本文展示了均场博弈 (MFGs) 作为一种数学框架用于解释、增强和设计生成模型的多功能性。我们建立了 MFGs 与主要流动和扩散型生成模型之间关联，并通过不同的粒子动力学和代价函数推导了这三个类别的生成模型。此外，我们通过研究它们相关的 MFG 的最优条件——一组耦合的非线性偏微分方程，来研究每个生成模型的数学结构和特性。本文还提出了一个新的基于双人 MFG 的生成模型，其中一个代理合成样本，另一个代理对样本进行识别，理论和实验结果表明，该模型生成的样本多样且逼真，同时与基准模型相比，改善了解缠结和公平性。总之，本文突显了 MFGs 作为设计和分析生成模型的实验室的潜力。

    In this paper, we demonstrate the versatility of mean-field games (MFGs) as a mathematical framework for explaining, enhancing, and designing generative models. There is a pervasive sense in the generative modeling community that the various flow and diffusion-based generative models have some foundational common structure and interrelationships. We establish connections between MFGs and major classes of flow and diffusion-based generative models including continuous-time normalizing flows, score-based models, and Wasserstein gradient flows. We derive these three classes of generative models through different choices of particle dynamics and cost functions. Furthermore, we study the mathematical structure and properties of each generative model by studying their associated MFG's optimality condition, which is a set of coupled nonlinear partial differential equations (PDEs). The theory of MFGs, therefore, enables the study of generative models through the theory of nonlinear PDEs. Throu
    
[^23]: 簇熵：病理图像分割中的主动域自适应

    Cluster Entropy: Active Domain Adaptation in Pathological Image Segmentation. (arXiv:2304.13513v1 [cs.CV])

    [http://arxiv.org/abs/2304.13513](http://arxiv.org/abs/2304.13513)

    本文提出了一种对于病理图像分割领域偏移问题的解决方法，通过使用簇熵来选择有效的WSI并可以显著提高半监督域自适应的性能。

    

    病理图像分割中的域偏移是一个重要问题，源域（在特定医院收集的图像）训练的网络在目标域（来自不同医院）中由于不同的图像特征表现不佳。由于病理学中的类别不平衡和不同类别先验分布的问题，典型的无监督域自适应方法不能通过对齐源域和目标域的分布来很好地处理该问题。本文提出了一种簇熵方法，用于选择用于半监督域自适应的有效WSI，该方法可以通过计算每个簇的熵来度量WSI的图像特征如何覆盖目标域的整个分布，并可以显著提高域自适应的性能。我们的方法在来自两家医院的数据集上取得了竞争性的结果。

    The domain shift in pathological segmentation is an important problem, where a network trained by a source domain (collected at a specific hospital) does not work well in the target domain (from different hospitals) due to the different image features. Due to the problems of class imbalance and different class prior of pathology, typical unsupervised domain adaptation methods do not work well by aligning the distribution of source domain and target domain. In this paper, we propose a cluster entropy for selecting an effective whole slide image (WSI) that is used for semi-supervised domain adaptation. This approach can measure how the image features of the WSI cover the entire distribution of the target domain by calculating the entropy of each cluster and can significantly improve the performance of domain adaptation. Our approach achieved competitive results against the prior arts on datasets collected from two hospitals.
    
[^24]: 在医学图像分割中混合数据增强与前景区域保护

    Mixing Data Augmentation with Preserving Foreground Regions in Medical Image Segmentation. (arXiv:2304.13490v1 [eess.IV])

    [http://arxiv.org/abs/2304.13490](http://arxiv.org/abs/2304.13490)

    该研究提出了两种新的数据增强方法KeepMask和KeepMix，这些方法可以在不消耗额外计算资源的情况下更好地识别器官边界，从而为医学图像分割提供高精度的性能和更精确的分割边界。

    

    深度学习在医学图像分割方面的应用可以显著支持医生的诊断。深度学习需要大量的训练数据，因此需要数据增强来扩展多样性以防止过拟合。然而，现有的医学图像分割数据增强方法主要基于需要更新参数和消耗额外计算资源的模型。我们提出了一种旨在训练高精度深度学习网络进行医学图像分割的数据增强方法。所提出的数据增强方法被称为KeepMask和KeepMix，可以通过更好地识别器官边界来创建医学图像，而不需要额外的参数。我们的方法在数据集上实现了更好的性能，并获得了更精确的医学图像分割边界。在CHAOS上，我们的方法的Dice系数达到了94.15％（比基线高3.04％），在MSD脾脏上达到了74.70％（比基线高5.25％）。

    The development of medical image segmentation using deep learning can significantly support doctors' diagnoses. Deep learning needs large amounts of data for training, which also requires data augmentation to extend diversity for preventing overfitting. However, the existing methods for data augmentation of medical image segmentation are mainly based on models which need to update parameters and cost extra computing resources. We proposed data augmentation methods designed to train a high accuracy deep learning network for medical image segmentation. The proposed data augmentation approaches are called KeepMask and KeepMix, which can create medical images by better identifying the boundary of the organ with no more parameters. Our methods achieved better performance and obtained more precise boundaries for medical image segmentation on datasets. The dice coefficient of our methods achieved 94.15% (3.04% higher than baseline) on CHAOS and 74.70% (5.25% higher than baseline) on MSD splee
    
[^25]: 先验信息学习中的基本权衡

    Fundamental Tradeoffs in Learning with Prior Information. (arXiv:2304.13479v1 [cs.LG])

    [http://arxiv.org/abs/2304.13479](http://arxiv.org/abs/2304.13479)

    本文研究了先验信息准确性和学习性能之间的基本权衡，引入了优先风险概念，并为统计估计问题提供了下界，展现了框架在不同问题中的应用。

    

    本文旨在探讨学习者在所学问题上的先验信息的准确性和其学习性能之间的基本权衡。我们引入了优先风险的概念，它不同于传统的极小极大和贝叶斯风险，可以让我们研究现实不一定符合学习者先验的情况下这些基本权衡。我们提出了一种基于缩减的方法来扩展经典的极小极大下界技术，以便为统计估计问题的优先风险提供下界。我们还介绍了一种新颖的法诺不等式的推广（可能具有独立的兴趣），用于在涉及无限损失的更一般的设置下，下界优先风险。我们展示了我们的框架揭示了在估计、回归和强化学习问题中，先验信息与学习性能之间权衡的能力。

    We seek to understand fundamental tradeoffs between the accuracy of prior information that a learner has on a given problem and its learning performance. We introduce the notion of prioritized risk, which differs from traditional notions of minimax and Bayes risk by allowing us to study such fundamental tradeoffs in settings where reality does not necessarily conform to the learner's prior. We present a general reduction-based approach for extending classical minimax lower-bound techniques in order to lower bound the prioritized risk for statistical estimation problems. We also introduce a novel generalization of Fano's inequality (which may be of independent interest) for lower bounding the prioritized risk in more general settings involving unbounded losses. We illustrate the ability of our framework to provide insights into tradeoffs between prior information and learning performance for problems in estimation, regression, and reinforcement learning.
    
[^26]: 多次标注图像分割中潜在空间分布对效果的影响

    Effect of latent space distribution on the segmentation of images with multiple annotations. (arXiv:2304.13476v1 [cs.CV])

    [http://arxiv.org/abs/2304.13476](http://arxiv.org/abs/2304.13476)

    本文研究了多次标注图像的分割问题，提出了广义概率U-Net。研究表明，选择合适的潜在空间分布可以提高预测结果的多样性和参考分割的重叠度。

    

    我们提出了广义概率U-Net，其通过允许更一般形式的高斯分布作为潜在空间分布来扩展概率U-Net，这可以更好地近似于参考分割的不确定性。我们研究了潜在空间分布的选择对肺肿瘤和脑白质高信号灶参考分割变化捕捉的影响。我们表明，分布的选择会影响预测的样本多样性以及与参考分割的重叠度。我们在https://github.com/ishaanb92/GeneralizedProbabilisticUNet上发布了我们的实现。

    We propose the Generalized Probabilistic U-Net, which extends the Probabilistic U-Net by allowing more general forms of the Gaussian distribution as the latent space distribution that can better approximate the uncertainty in the reference segmentations. We study the effect the choice of latent space distribution has on capturing the variation in the reference segmentations for lung tumors and white matter hyperintensities in the brain. We show that the choice of distribution affects the sample diversity of the predictions and their overlap with respect to the reference segmentations. We have made our implementation available at https://github.com/ishaanb92/GeneralizedProbabilisticUNet
    
[^27]: 采用自组织映射（SOM）的等离子体磁陀螺不稳定完全动力学模拟的无监督分类

    Unsupervised classification of fully kinetic simulations of plasmoid instability using Self-Organizing Maps (SOMs). (arXiv:2304.13469v1 [physics.plasm-ph])

    [http://arxiv.org/abs/2304.13469](http://arxiv.org/abs/2304.13469)

    本文采用自组织映射（SOM）的聚类方法应用于等离子体磁陀螺不稳定完全动力学模拟，我们获得了良好的聚类结果，证明该方法可作为模拟和观测数据可靠分析工具，并给出了有益的物理见解和空间区域的信息。

    

    空间物理过程模拟和观测产生的数据量增加，鼓励使用机器学习方法进行数据分析和物理研究。我们将基于自组织映射（SOM）的聚类方法应用于等离子体磁陀螺不稳定完全动力学模拟，旨在评估其作为模拟和观测数据可靠分析工具的适用性。我们获得了良好的聚类结果，与我们对过程的知识相符：聚类清楚地识别了进流区域、内部磁陀螺区域、分离区域以及与磁陀螺合并相关的区域。SOM特定的分析工具，例如特征映射和统一距离矩阵，为我们提供了有益的关于物理和特定空间区域的见解。该方法似乎是分析模拟和观测数据，且潜在地也可以用于显示从传统方法到基于机器学习分析方法的转换的有前途的选择。

    The growing amount of data produced by simulations and observations of space physics processes encourages the use of methods rooted in Machine Learning for data analysis and physical discovery. We apply a clustering method based on Self-Organizing Maps (SOM) to fully kinetic simulations of plasmoid instability, with the aim of assessing its suitability as a reliable analysis tool for both simulated and observed data. We obtain clusters that map well, a posteriori, to our knowledge of the process: the clusters clearly identify the inflow region, the inner plasmoid region, the separatrices, and regions associated with plasmoid merging. SOM-specific analysis tools, such as feature maps and Unified Distance Matrix, provide one with valuable insights into both the physics at work and specific spatial regions of interest. The method appears as a promising option for the analysis of data, both from simulations and from observations, and could also potentially be used to trigger the switch to 
    
[^28]: 离散无穷范最优输运问题的多项式时间求解器

    Polynomial-Time Solvers for the Discrete $\infty$-Optimal Transport Problems. (arXiv:2304.13467v1 [math.OC])

    [http://arxiv.org/abs/2304.13467](http://arxiv.org/abs/2304.13467)

    本文提出了离散无穷范最优输运问题的多项式时间求解器。

    

    本文提出了求解离散有限的$\infty$-最优输运问题中的Monge和Kantorovich表述的多项式时间算法。据我们所知，这是第一次提出这些问题的有效数值方法。

    In this note, we propose polynomial-time algorithms solving the Monge and Kantorovich formulations of the $\infty$-optimal transport problem in the discrete and finite setting. It is the first time, to the best of our knowledge, that efficient numerical methods for these problems have been proposed.
    
[^29]: 芝加哥市预测特定犯罪类型的多种方法的比较分析

    A Comparative Analysis of Multiple Methods for Predicting a Specific Type of Crime in the City of Chicago. (arXiv:2304.13464v1 [cs.LG])

    [http://arxiv.org/abs/2304.13464](http://arxiv.org/abs/2304.13464)

    本研究考察了预测芝加哥市盗窃犯罪的多种方法，并发现采用XGBoost方法能够得到最佳的预测结果，F1分数为0.86。

    

    研究者认为，犯罪是受到多种物理、社会和经济因素影响的社会现象。不同类型的犯罪据说有不同的动机。例如，盗窃是一种基于机会的犯罪，而谋杀是出于情感驱动的。因此，我们考察了当预测单一犯罪时，仅使用时空信息时模型的表现。具体来说，我们旨在预测盗窃，因为这是一种可以使用时空信息进行预测的犯罪。我们的研究问题是：“我们能用空间和时间特征来预测盗窃的表现如何？”为了回答这个问题，我们使用不同的不平衡技术和超参数，考察了支持向量机、线性回归、XGBoost、随机森林和k-最近邻等方法的有效性。XGBoost表现最佳，F1分数为0.86。

    Researchers regard crime as a social phenomenon that is influenced by several physical, social, and economic factors. Different types of crimes are said to have different motivations. Theft, for instance, is a crime that is based on opportunity, whereas murder is driven by emotion. In accordance with this, we examine how well a model can perform with only spatiotemporal information at hand when it comes to predicting a single crime. More specifically, we aim at predicting theft, as this is a crime that should be predictable using spatiotemporal information. We aim to answer the question: "How well can we predict theft using spatial and temporal features?". To answer this question, we examine the effectiveness of support vector machines, linear regression, XGBoost, Random Forest, and k-nearest neighbours, using different imbalanced techniques and hyperparameters. XGBoost showed the best results with an F1-score of 0.86.
    
[^30]: 从混沌中迸发出秩序：为物体检测排序事件表示法

    From Chaos Comes Order: Ordering Event Representations for Object Detection. (arXiv:2304.13455v1 [cs.CV])

    [http://arxiv.org/abs/2304.13455](http://arxiv.org/abs/2304.13455)

    本文提出了一种基于Gromov-Wasserstein Discrepancy选择最佳事件表示的方法，这种方法可以在多个表示、网络骨干和数据集上保持任务性能排名的一致性。利用这一方法，本文对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示法，取得了优于最先进的基于事件的对象检测方法的成果。

    

    如今，处理事件的顶尖深度神经网络在使用现成网络之前，首先将其转换为稠密的网格状输入表示。然而，传统上为任务选择适当的表示需要针对每个表示训练一个神经网络，并根据验证分数选择最佳表示，这非常耗时。在这项工作中，我们通过基于原始事件及其表示之间的Gromov-Wasserstein Discrepancy (GWD)选择最佳表示来消除这个瓶颈。它的计算速度大约比训练神经网络快200倍，同时在多个表示、网络骨干和数据集上保持事件表示法任务性能排名的一致性。这意味着找到具有高任务分数的表示相当于找到具有低GWD的表示。我们利用这一观察结果，首次对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示。我们的方法在Moving MNIST和N-Caltech101数据集上都优于最先进的基于事件的对象检测方法，在后者达到了83.0%的1%误报率下的mAP新的最高水平。

    Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. In this work, we eliminate this bottleneck by selecting the best representation based on the Gromov-Wasserstein Discrepancy (GWD) between the raw events and their representation. It is approximately 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, and datasets. This means that finding a representation with a high task score is equivalent to finding a representation with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family o
    
[^31]: 深度神经网络的隐式反事实数据增强

    Implicit Counterfactual Data Augmentation for Deep Neural Networks. (arXiv:2304.13431v1 [cs.LG])

    [http://arxiv.org/abs/2304.13431](http://arxiv.org/abs/2304.13431)

    本研究提出了隐式反事实数据增强（ICDA）方法，通过新的样本增强策略、易于计算的代理损失和具体方案，消除了虚假关联并进行了稳健预测。

    

    机器学习模型易于捕捉非因果属性和类别之间的虚假相关性，使用反事实数据增强是破除这些虚假的联想的有效方法。然而，明确生成反事实数据很具挑战性，训练效率会降低。因此，本研究提出了一种隐式反事实数据增强（Implicit Counterfactual Data Augmentation，ICDA）方法来消除虚假关联并进行稳健预测。具体而言，首先，开发了一种新的样本增强策略，为每个样本生成在语义和反事实意义上有意义的深度特征，并具有不同的增强强度。其次，当增广样本数变为无穷大时，我们推导出对于增广特征集的易于计算的代理损失。第三，提出了两种具体的方案，包括直接量化和元学习，以确定鲁棒性损失的关键参数。此外，还从实验的角度解释了ICDA的作用。

    Machine-learning models are prone to capturing the spurious correlations between non-causal attributes and classes, with counterfactual data augmentation being a promising direction for breaking these spurious associations. However, explicitly generating counterfactual data is challenging, with the training efficiency declining. Therefore, this study proposes an implicit counterfactual data augmentation (ICDA) method to remove spurious correlations and make stable predictions. Specifically, first, a novel sample-wise augmentation strategy is developed that generates semantically and counterfactually meaningful deep features with distinct augmentation strength for each sample. Second, we derive an easy-to-compute surrogate loss on the augmented feature set when the number of augmented samples becomes infinite. Third, two concrete schemes are proposed, including direct quantification and meta-learning, to derive the key parameters for the robust loss. In addition, ICDA is explained from 
    
[^32]: GENIE-NF-AI: 使用基于AACR GENIE数据集的液态神经网络（LTC）识别神经纤维瘤肿瘤

    GENIE-NF-AI: Identifying Neurofibromatosis Tumors using Liquid Neural Network (LTC) trained on AACR GENIE Datasets. (arXiv:2304.13429v1 [cs.LG])

    [http://arxiv.org/abs/2304.13429](http://arxiv.org/abs/2304.13429)

    本研究提出了一个使用基于AACR GENIE数据集的液态神经网络（LTC）诊断神经纤维瘤的可解释AI方法，以99.86%的准确率优于现有模型，并提供了解释性和黑盒模型。

    

    近年来，医学领域越来越多地采用人工智能（AI）技术来提供更快，更准确的疾病检测、预测和评估。本研究提出了一种可解释的AI方法，使用血液检测和致病变量来诊断患有神经纤维瘤的患者。我们使用AACR GENIE项目的数据集评估了所提出的方法，并将其性能与现代方法进行了比较。我们所提出的方法以99.86%的准确率优于现有模型。我们还进行了NF1和可解释的AI测试来验证我们的方法。我们的工作提供了一个使用逻辑回归和说明性刺激的可解释模型以及一个黑盒模型。可说明的模型有助于解释黑盒模型的预测，而玻璃盒模型提供最佳特征的信息。总之，我们的研究提出了一种可解释的AI方法，用于诊断神经纤维瘤患者。

    In recent years, the field of medicine has been increasingly adopting artificial intelligence (AI) technologies to provide faster and more accurate disease detection, prediction, and assessment. In this study, we propose an interpretable AI approach to diagnose patients with neurofibromatosis using blood tests and pathogenic variables. We evaluated the proposed method using a dataset from the AACR GENIE project and compared its performance with modern approaches. Our proposed approach outperformed existing models with 99.86% accuracy. We also conducted NF1 and interpretable AI tests to validate our approach. Our work provides an explainable approach model using logistic regression and explanatory stimulus as well as a black-box model. The explainable models help to explain the predictions of black-box models while the glass-box models provide information about the best-fit features. Overall, our study presents an interpretable AI approach for diagnosing patients with neurofibromatosis 
    
[^33]: FLEX：一种适用于非线性系统的自适应探索算法

    FLEX: an Adaptive Exploration Algorithm for Nonlinear Systems. (arXiv:2304.13426v1 [cs.LG])

    [http://arxiv.org/abs/2304.13426](http://arxiv.org/abs/2304.13426)

    FLEX是一种非线性系统的自适应探索算法，使用最优实验设计方法，需要最少的资源，并用于下游的基于模型的经典控制任务中。

    

    基于模型的加强学习是一种强大的工具，但收集适合系统的精确模型的数据可能很昂贵。因此以样本有效的方式探索未知环境非常重要。然而，动力学的复杂性以及实际系统的计算限制使得这一任务具有挑战性。在本文中，我们介绍了FLEX，这是一种基于最优实验设计的非线性动力学探索算法。我们的策略最大化下一步信息，从而得到自适应探索算法，与通用参数化学习模型兼容，并且需要最少的资源。我们在涵盖不同设置的若干非线性环境中测试了我们的方法，包括时变动力学。牢记探索是为了服务于开发性目标，我们还将我们的算法应用于下游基于模型的经典控制任务，并将其与其他最先进的基于模型和模型自由方法进行了比较。

    Model-based reinforcement learning is a powerful tool, but collecting data to fit an accurate model of the system can be costly. Exploring an unknown environment in a sample-efficient manner is hence of great importance. However, the complexity of dynamics and the computational limitations of real systems make this task challenging. In this work, we introduce FLEX, an exploration algorithm for nonlinear dynamics based on optimal experimental design. Our policy maximizes the information of the next step and results in an adaptive exploration algorithm, compatible with generic parametric learning models and requiring minimal resources. We test our method on a number of nonlinear environments covering different settings, including time-varying dynamics. Keeping in mind that exploration is intended to serve an exploitation objective, we also test our algorithm on downstream model-based classical control tasks and compare it to other state-of-the-art model-based and model-free approaches. T
    
[^34]: 与陌生人一起跑接力赛？强化学习在超出分布轨迹上的泛化能力

    Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories. (arXiv:2304.13424v1 [cs.LG])

    [http://arxiv.org/abs/2304.13424](http://arxiv.org/abs/2304.13424)

    本文研究了强化学习代理对超出分布的“可控”状态的“接力泛化”性能。通过让测试代理从其他陌生代理的轨迹中间开始，发现这种泛化普遍存在泛化失效问题。

    

    本文定义、评估和改进各种状态下的强化学习（RL）代理对超出分布的“可控”状态的“接力泛化”性能。通过将测试代理从其他独立训练良好的“陌生”代理的轨迹的中间开始，我们实际评估了这种泛化类型。通过大量实验评估，我们展示了来自陌生代理的可控状态几乎普遍存在泛化失效。例如，在人形环境中，我们观察到一个经过良好训练的PPO代理，在正常测试期间只有3.9％的失败率，但在10个陌生代理的轨迹中，失败率升高到31.4％。

    In this paper, we define, evaluate, and improve the ``relay-generalization'' performance of reinforcement learning (RL) agents on the out-of-distribution ``controllable'' states. Ideally, an RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. For example, a self-driving system should be able to take over the control from humans in the middle of driving and must continue to drive the car safely. To practically evaluate this type of generalization, we start the test agent from the middle of other independently well-trained \emph{stranger} agents' trajectories. With extensive experimental evaluation, we show the prevalence of \emph{generalization failure} on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\% failure rate during regular testing, failed on
    
[^35]: 包含不完整观测的传感器数据回归问题

    Regression with Sensor Data Containing Incomplete Observations. (arXiv:2304.13415v1 [cs.LG])

    [http://arxiv.org/abs/2304.13415](http://arxiv.org/abs/2304.13415)

    本文提出了一种能处理不完整观测传感器数据的回归算法，解决了标签值由于不完整观测导致学习结果偏低的问题。

    

    本文解决了一个回归问题，其中输出标签值是感应现象幅度的结果。这种标签值低可能意味着现象的实际幅度低或传感器做出了不完整的观测。这导致标签值偏低，学习结果也偏低，因为标签值可能由于不完整的观测而偏低，即使现象的实际幅度高。为了解决这个问题，我们提出了一个学习算法，显式地模拟了带有负值的不对称噪声的不完整观测。我们证明了我们的算法是无偏的，就像从不包含不完整观测的未损坏数据学习一样。我们通过数值实验证明了我们算法的优势。

    This paper addresses a regression problem in which output label values are the results of sensing the magnitude of a phenomenon. A low value of such labels can mean either that the actual magnitude of the phenomenon was low or that the sensor made an incomplete observation. This leads to a bias toward lower values in labels and its resultant learning because labels may have lower values due to incomplete observations, even if the actual magnitude of the phenomenon was high. Moreover, because an incomplete observation does not provide any tags indicating incompleteness, we cannot eliminate or impute them. To address this issue, we propose a learning algorithm that explicitly models incomplete observations corrupted with an asymmetric noise that always has a negative value. We show that our algorithm is unbiased as if it were learned from uncorrupted data that does not involve incomplete observations. We demonstrate the advantages of our algorithm through numerical experiments.
    
[^36]: 量子联邦学习的安全通信模型：一种后量子密码（PQC）框架

    Secure Communication Model For Quantum Federated Learning: A Post Quantum Cryptography (PQC) Framework. (arXiv:2304.13413v1 [cs.CR])

    [http://arxiv.org/abs/2304.13413](http://arxiv.org/abs/2304.13413)

    本论文提出了一种后量子密码的量子联邦学习模型，并研究了其收敛和安全条件。

    

    我们设计了一种后量子密码（PQC）量子联邦学习（QFL）模型。我们开发了一个具有动态服务器选择的框架，并研究了收敛和安全条件。实现和结果可公开获得。

    We design a model of Post Quantum Cryptography (PQC) Quantum Federated Learning (QFL). We develop a framework with a dynamic server selection and study convergence and security conditions. The implementation and results are publicly available1.
    
[^37]: 使用中间层扰动衰减来提高敌对迁移能力

    Improving Adversarial Transferability by Intermediate-level Perturbation Decay. (arXiv:2304.13410v1 [cs.LG])

    [http://arxiv.org/abs/2304.13410](http://arxiv.org/abs/2304.13410)

    本文提出了一种名为中间层扰动衰减（ILPD）的新型中间层方法，可以通过单个优化阶段制作可转移的对抗性样本，并在过程中鼓励中间层扰动处于有效的敌对方向，并同时具有很大的幅度。

    

    中间层攻击是指通过遵循对抗方向，尝试扰动特征表示的攻击方法，在制作可转移的对抗样本中展现出良好的性能。现有的这类攻击方法通常由两个分离的阶段构成，首先需要确定一个方向向导，然后扩大中间层扰动对该方向向导的标量投影。然而，得到的扰动在特征空间中难免会偏离向导。本文发现，这种偏离可能导致次优的攻击效果。为了解决这个问题，我们开发了一种名为中间层扰动衰减（ILPD）的新型中间层方法，可以通过单个优化阶段制作可转移的对抗性样本。具体来说，该方法鼓励中间层扰动处于有效的敌对方向，并同时具有很大的幅度。

    Intermediate-level attacks that attempt to perturb feature representations following an adversarial direction drastically have shown favorable performance in crafting transferable adversarial examples. Existing methods in this category are normally formulated with two separate stages, where a directional guide is required to be determined at first and the scalar projection of the intermediate-level perturbation onto the directional guide is enlarged thereafter. The obtained perturbation deviates from the guide inevitably in the feature space, and it is revealed in this paper that such a deviation may lead to sub-optimal attack. To address this issue, we develop a novel intermediate-level method that crafts adversarial examples within a single stage of optimization. In particular, the proposed method, named intermediate-level perturbation decay (ILPD), encourages the intermediate-level perturbation to be in an effective adversarial direction and to possess a great magnitude simultaneous
    
[^38]: FedVS: 面向分割模型的容错和隐私保护垂直联邦学习

    FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models. (arXiv:2304.13407v1 [cs.LG])

    [http://arxiv.org/abs/2304.13407](http://arxiv.org/abs/2304.13407)

    该论文提出FedVS，一种同时解决垂直联邦学习中滞后客户端和数据泄露问题的方法，通过设计本地数据和模型的秘密共享方案，以保证信息理论隐私，并通过解密计算股份，无损重构所有客户端的嵌入的汇总。

    

    在一个由中央服务器和许多分布式客户端组成的垂直联邦学习系统中，训练数据被垂直分割，不同的特征存储在不同的客户端上。分割垂直联邦学习的问题是训练一个在服务器和客户端之间划分的模型。本文旨在解决分割垂直联邦学习中的两个主要挑战：1）由于训练过程中存在迟滞的客户端造成的性能下降；2）客户端上传数据嵌入导致的数据和模型隐私泄露。我们提出了FedVS来同时解决这两个挑战。FedVS的关键思想是设计本地数据和模型的秘密共享方案，从而保证针对勾结客户和好奇服务器的信息理论隐私，并且通过解密计算股份，无损重构所有客户端的嵌入的汇总。在各种类型的VFL数据集（包括表格，CV，图像，NLP）上进行了广泛的实验，证明了FedVS的有效性。

    In a vertical federated learning (VFL) system consisting of a central server and many distributed clients, the training data are vertically partitioned such that different features are privately stored on different clients. The problem of split VFL is to train a model split between the server and the clients. This paper aims to address two major challenges in split VFL: 1) performance degradation due to straggling clients during training; and 2) data and model privacy leakage from clients' uploaded data embeddings. We propose FedVS to simultaneously address these two challenges. The key idea of FedVS is to design secret sharing schemes for the local data and models, such that information-theoretical privacy against colluding clients and curious server is guaranteed, and the aggregation of all clients' embeddings is reconstructed losslessly, via decrypting computation shares from the non-straggling clients. Extensive experiments on various types of VFL datasets (including tabular, CV, a
    
[^39]: SEAL:同时探索和学习标签层次结构的框架

    SEAL: Simultaneous Label Hierarchy Exploration And Learning. (arXiv:2304.13374v1 [cs.LG])

    [http://arxiv.org/abs/2304.13374](http://arxiv.org/abs/2304.13374)

    提出了一种新的框架，Simultaneous label hierarchy Exploration And Learning (SEAL)，通过增加遵循先验层次结构的潜在标签来探索标签层次结构，通过1-Wasserstein度量在树度量空间上学习数据驱动的标签层次结构和执行（半）监督学习，并在实验中展示了优越的结果和富有见地的标签结构。

    

    标签层次结构是增强分类性能的重要外部知识源。然而，大多数现有方法都依赖于预定义的标签层次结构，可能无法匹配数据分布。为解决这个问题，我们提出了Simultaneous label hierarchy Exploration And Learning (SEAL)，一种新的框架，通过增加遵循先验层次结构的潜在标签来探索标签层次结构。我们的方法使用树度量空间上的1-Wasserstein度量作为目标函数，使我们能够同时学习数据驱动的标签层次结构和执行（半）监督学习。我们在几个数据集上评估了我们的方法，并展示了它在有监督和半监督情况下都取得了优越的结果，并揭示了富有见地的标签结构。我们的实现可在https://github.com/tzq1999/SEAL上找到。

    Label hierarchy is an important source of external knowledge that can enhance classification performance. However, most existing methods rely on predefined label hierarchies that may not match the data distribution. To address this issue, we propose Simultaneous label hierarchy Exploration And Learning (SEAL), a new framework that explores the label hierarchy by augmenting the observed labels with latent labels that follow a prior hierarchical structure. Our approach uses a 1-Wasserstein metric over the tree metric space as an objective function, which enables us to simultaneously learn a data-driven label hierarchy and perform (semi-)supervised learning. We evaluate our method on several datasets and show that it achieves superior results in both supervised and semi-supervised scenarios and reveals insightful label structures. Our implementation is available at https://github.com/tzq1999/SEAL.
    
[^40]: 延迟反馈的前馈优化神经网络

    Feed-Forward Optimization With Delayed Feedback for Neural Networks. (arXiv:2304.13372v1 [cs.LG])

    [http://arxiv.org/abs/2304.13372](http://arxiv.org/abs/2304.13372)

    本文提出了一种延迟反馈的前馈神经网络优化方法F^3，使用延迟的误差信息来缩放梯度从而提高生物可行性和计算效率，具有较高的预测性能，为低能量训练和并行化提供了新思路。

    

    反向传播长期以来一直受到生物学上的批评，因为它依赖于自然学习过程中不可行的概念。本文提出了一种替代方法来解决两个核心问题，即权重传输和更新锁定，以实现生物可行性和计算效率。我们引入了延迟反馈的前馈（F^3），通过利用延迟的误差信息作为样本级缩放因子来更准确地近似梯度，改进了先前的工作。我们发现，F^3将生物可行性训练算法和反向传播之间的预测性能差距缩小了高达96％。这证明了生物可行性训练的适用性，并为低能量训练和并行化开辟了有 promising 的新方向。

    Backpropagation has long been criticized for being biologically implausible, relying on concepts that are not viable in natural learning processes. This paper proposes an alternative approach to solve two core issues, i.e., weight transport and update locking, for biological plausibility and computational efficiency. We introduce Feed-Forward with delayed Feedback (F$^3$), which improves upon prior work by utilizing delayed error information as a sample-wise scaling factor to approximate gradients more accurately. We find that F$^3$ reduces the gap in predictive performance between biologically plausible training algorithms and backpropagation by up to 96%. This demonstrates the applicability of biologically plausible training and opens up promising new avenues for low-energy training and parallelization.
    
[^41]: 基于LoRaWAN的智能校园：数据集和人流计数器案例

    LoRaWAN-enabled Smart Campus: The Dataset and a People Counter Use Case. (arXiv:2304.13366v1 [cs.LG])

    [http://arxiv.org/abs/2304.13366](http://arxiv.org/abs/2304.13366)

    本文介绍一个基于LoRaWAN的智能校园数据集，使用k近邻和LSTM方法处理丢失值和预测未来读数，并构建一个深度神经网络来预测房间内人数，准确率达到95％。

    

    物联网在智能校园中具有重要作用。本文提供了一个基于LoRaWAN的智能校园数据集的详细描述。LoRaWAN是一种新兴技术，可以为数百个物联网设备提供服务。首先，我们描述了将设备连接到服务器的LoRa网络。然后，我们分析了丢失的传输并提出了k近邻解决方案来处理缺失值。然后，我们使用长短期记忆（LSTM）来预测未来的读数。最后，作为一个示例应用程序，我们构建了一个深度神经网络来基于所选传感器的读数预测房间内人数。我们的结果显示，在预测人数方面，我们的模型达到了95％的准确率。此外，数据集是公开可用的，并且有详细说明，这是探索其他功能和应用的机会。

    IoT has a significant role in the smart campus. This paper presents a detailed description of the Smart Campus dataset based on LoRaWAN. LoRaWAN is an emerging technology that enables serving hundreds of IoT devices. First, we describe the LoRa network that connects the devices to the server. Afterward, we analyze the missing transmissions and propose a k-nearest neighbor solution to handle the missing values. Then, we predict future readings using a long short-term memory (LSTM). Finally, as one example application, we build a deep neural network to predict the number of people inside a room based on the selected sensor's readings. Our results show that our model achieves an accuracy of $95 \: \%$ in predicting the number of people. Moreover, the dataset is openly available and described in detail, which is opportunity for exploration of other features and applications.
    
[^42]: Concept-Monitor: 通过个别神经元理解 DNN 训练

    Concept-Monitor: Understanding DNN training through individual neurons. (arXiv:2304.13346v1 [cs.LG])

    [http://arxiv.org/abs/2304.13346](http://arxiv.org/abs/2304.13346)

    通过 Concept-Monitor，我们提出了一种通用框架来自动解密黑匣子 DNN 训练过程，它可以帮助人们理解 DNN 在训练期间如何发展，同时我们提出了一种新的正则化器来激励隐藏神经元学习多样的概念，这可以提高训练性能。我们还运用 Concept-Monitor 对不同训练范式进行了多个案例研究，包括对抗性训练、微调和网络修剪。

    

    在这项工作中，我们提出了一个称为 Concept-Monitor 的通用框架，使用新颖的统一嵌入空间和概念多样性度量来帮助自动化地解密黑匣子 DNN 训练过程。Concept-Monitor 可以呈现人类可解释的可视化和指标，促进透明度和深入了解 DNN 在训练期间如何发展。受这些发现启发，我们还提出了一种新的训练正则化器，激励隐藏神经元学习多样的概念，我们发现这种正则化可以提高训练性能。最后，我们运用 Concept-Monitor 来对不同训练范式进行多个案例研究，包括对抗性训练、微调和通过“中奖彩票假设”进行网络修剪。

    In this work, we propose a general framework called Concept-Monitor to help demystify the black-box DNN training processes automatically using a novel unified embedding space and concept diversity metric. Concept-Monitor enables human-interpretable visualization and indicators of the DNN training processes and facilitates transparency as well as deeper understanding on how DNNs develop along the during training. Inspired by these findings, we also propose a new training regularizer that incentivizes hidden neurons to learn diverse concepts, which we show to improve training performance. Finally, we apply Concept-Monitor to conduct several case studies on different training paradigms including adversarial training, fine-tuning and network pruning via the Lottery Ticket Hypothesis
    
[^43]: OpenBox：通用黑盒优化的 Python 工具包

    OpenBox: A Python Toolkit for Generalized Black-box Optimization. (arXiv:2304.13339v1 [cs.LG])

    [http://arxiv.org/abs/2304.13339](http://arxiv.org/abs/2304.13339)

    OpenBox是一个通用黑盒优化的Python工具包，提供了用户友好的接口和可视化功能，模块化设计能够在现有系统中灵活部署，并且实验证明其比现有系统更有效和高效。

    

    黑盒优化具有广泛的应用，包括自动机器学习、实验设计和数据库参数调整。然而，使用现有软件包时，用户在适用性、性能和效率方面仍面临挑战。本文介绍了 OpenBox，这是一个开源的黑盒优化工具包，提高了其可用性。它实现了用户友好的接口和可视化功能，让用户能够定义和管理任务。OpenBox 的模块化设计有助于在现有系统中灵活部署。实验结果表明，OpenBox比现有系统更有效和高效。OpenBox 的源代码可在 https://github.com/PKU-DAIR/open-box 中找到。

    Black-box optimization (BBO) has a broad range of applications, including automatic machine learning, experimental design, and database knob tuning. However, users still face challenges when applying BBO methods to their problems at hand with existing software packages in terms of applicability, performance, and efficiency. This paper presents OpenBox, an open-source BBO toolkit with improved usability. It implements user-friendly inferfaces and visualization for users to define and manage their tasks. The modular design behind OpenBox facilitates its flexible deployment in existing systems. Experimental results demonstrate the effectiveness and efficiency of OpenBox over existing systems. The source code of OpenBox is available at https://github.com/PKU-DAIR/open-box.
    
[^44]: 技术笔记：定义和量化DNN的AND-OR交互以进行准确和简明的解释

    Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs. (arXiv:2304.13312v1 [cs.LG])

    [http://arxiv.org/abs/2304.13312](http://arxiv.org/abs/2304.13312)

    本文提出了一种通过量化输入变量之间的编码交互来准确且简明地解释深度神经网络(DNN)的推理逻辑的方法。针对此目的，作者提出了两种交互方式，即AND交互和OR交互，并利用它们设计出一系列技术来提高解释的简洁性，同时不会损害准确性。

    

    本文旨在通过量化输入变量之间的编码交互来解释深度神经网络(DNN)的推理逻辑。具体而言，我们首先重新思考交互的定义，然后正式定义了基于交互的解释的准确性和简洁性。为此，我们提出了两种交互方式，即AND交互和OR交互。针对准确性，我们证明了AND（OR）交互在量化输入变量之间的AND（OR）关系效应方面的唯一性。此外，基于AND-OR交互，我们设计了技术来提高解释的简洁性，同时不会损害准确性。因此，DNN的推理逻辑可以通过一组符号概念准确而简明地解释。

    In this technical note, we aim to explain a deep neural network (DNN) by quantifying the encoded interactions between input variables, which reflects the DNN's inference logic. Specifically, we first rethink the definition of interactions, and then formally define faithfulness and conciseness for interaction-based explanation. To this end, we propose two kinds of interactions, i.e., the AND interaction and the OR interaction. For faithfulness, we prove the uniqueness of the AND (OR) interaction in quantifying the effect of the AND (OR) relationship between input variables. Besides, based on AND-OR interactions, we design techniques to boost the conciseness of the explanation, while not hurting the faithfulness. In this way, the inference logic of a DNN can be faithfully and concisely explained by a set of symbolic concepts.
    
[^45]: HiQ -- 一种声明性、非侵入式、动态和透明的可观察性和优化系统。

    HiQ -- A Declarative, Non-intrusive, Dynamic and Transparent Observability and Optimization System. (arXiv:2304.13302v1 [cs.DC])

    [http://arxiv.org/abs/2304.13302](http://arxiv.org/abs/2304.13302)

    HiQ是一种可透明监控Python程序运行时信息的系统，具有非侵入性和动态性，可应用于离线/在线应用程序和分布式系统，我们可以使用它来优化神经网络模型并捕捉瓶颈，而不影响代码的干净程度和性能表现。

    

    本文提出了一种名为“HiQ”的非侵入式、声明性、动态和透明系统，用于跟踪Python程序的运行时信息，而不会影响运行时系统性能和损失洞察力。 HiQ可以用于单片和分布式系统、离线和在线应用程序，我们已经将其用于优化使用Python编写的大型深度神经网络（DNN）模型，并可推广到任何Python程序或分布式系统，甚至是Java等其他语言。 我们已经在深度学习模型生命周期管理系统中实现了HiQ，并采用它来捕捉瓶颈，同时保持我们的生产代码干净且高性能。 我们的实现已经在 [https://github.com/oracle/hiq](https://github.com/oracle/hiq) 开源。

    This paper proposes a non-intrusive, declarative, dynamic and transparent system called `HiQ` to track Python program runtime information without compromising on the run-time system performance and losing insight. HiQ can be used for monolithic and distributed systems, offline and online applications. HiQ is developed when we optimize our large deep neural network (DNN) models which are written in Python, but it can be generalized to any Python program or distributed system, or even other languages like Java. We have implemented the system and adopted it in our deep learning model life cycle management system to catch the bottleneck while keeping our production code clean and highly performant. The implementation is open-sourced at: [https://github.com/oracle/hiq](https://github.com/oracle/hiq).
    
[^46]: 膜电位分布调整和参数化代理梯度在尖峰神经网络中的应用

    Membrane Potential Distribution Adjustment and Parametric Surrogate Gradient in Spiking Neural Networks. (arXiv:2304.13289v1 [cs.LG])

    [http://arxiv.org/abs/2304.13289](http://arxiv.org/abs/2304.13289)

    本论文提出了一种参数化代理梯度（PSG）方法和潜在分布调整（PDA）方法，以解决尖峰神经网络（SNN）中梯度下降训练的问题，并在基准数据集上实现了优异的性能。

    

    作为一种新兴的网络模型，尖峰神经网络（SNN）近年来引起了重大的研究关注。然而，高效能的二进制脉冲信号并不适用于基于梯度下降的训练方法。代理梯度（SG）策略被研究和应用于绕过这个问题并从零开始训练SNN。由于缺乏公认的SG选择规则，大多数SG被直观地选择。我们提出了参数化代理梯度（PSG）方法来迭代地更新SG，并最终确定最佳代理梯度参数，该参数校准了候选SG的形状。在SNN中，由于量化误差，神经电位分布往往会出现不可预测的偏差。我们评估了这种潜在的偏移并提出了潜在分布调整（PDA）的方法来最小化不希望的预激活损失。实验结果表明，所提出的方法可以轻松地与时间反向传播（BPTT）算法集成，并在基准数据集上实现优异的性能。

    As an emerging network model, spiking neural networks (SNNs) have aroused significant research attentions in recent years. However, the energy-efficient binary spikes do not augur well with gradient descent-based training approaches. Surrogate gradient (SG) strategy is investigated and applied to circumvent this issue and train SNNs from scratch. Due to the lack of well-recognized SG selection rule, most SGs are chosen intuitively. We propose the parametric surrogate gradient (PSG) method to iteratively update SG and eventually determine an optimal surrogate gradient parameter, which calibrates the shape of candidate SGs. In SNNs, neural potential distribution tends to deviate unpredictably due to quantization error. We evaluate such potential shift and propose methodology for potential distribution adjustment (PDA) to minimize the loss of undesired pre-activations. Experimental results demonstrate that the proposed methods can be readily integrated with backpropagation through time (B
    
[^47]: 基于Softmax回归的上下文学习和权重调整的关系

    The Closeness of In-Context Learning and Weight Shifting for Softmax Regression. (arXiv:2304.13276v1 [cs.CL])

    [http://arxiv.org/abs/2304.13276](http://arxiv.org/abs/2304.13276)

    本论文讨论了大型语言模型中关键组件注意机制的 softmax 单元以及上下文学习，探究了在上下文学习中softmax单元的权重调整。

    

    大型语言模型因其在自然语言处理中的出色表现而闻名，适用于许多与人类生活或工作相关的任务。Transformer架构中的注意机制是LLMs的一个关键组件，因为它允许模型有选择地关注特定的输入部分。softmax单元作为注意机制的关键部分，规范化了注意得分。因此，LLMs在各种NLP任务中的表现很大程度上取决于softmax单元与注意机制发挥的关键作用。最近，许多工作[RTH+22，ASA+22，GTLV22，ONR+22]研究了上下文学习。

    Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related or even job-related tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit.  In-context learning, as one of the celebrated abilities of recent LLMs, is an important concept in querying LLMs such as ChatGPT. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the in-context learni
    
[^48]: 基于不确定性客户聚类的联邦学习用于车队故障诊断

    Federated Learning with Uncertainty-Based Client Clustering for Fleet-Wide Fault Diagnosis. (arXiv:2304.13275v1 [cs.LG])

    [http://arxiv.org/abs/2304.13275](http://arxiv.org/abs/2304.13275)

    本文提出了一种基于不确定性客户聚类的联邦学习框架，用于车队故障诊断，通过完全去中心化的架构和客户聚类，提高了模型的精确性和效率，并在真实数据集上证明了其优于现有联邦学习方法的表现。

    

    各行各业的操作员一直在推广无线传感器节点用于工业监控，这些努力产生了可用于构建诊断算法的大规模条件监测数据集，以警告维护工程师即将发生的故障或识别当前系统的健康状况。然而，单个操作员可能没有足够大的系统或部件数量来收集足够的数据以开发数据驱动的算法。收集安全关键系统的满意数量的故障模式特别困难，因为故障的稀有性。联邦学习 (FL) 已经成为一种有前途的解决方案，利用多个操作员的数据集来训练去中心化资产故障诊断模型，同时保持数据的机密性。然而，在优化联邦策略时仍有相当大的障碍需要克服，以避免泄漏敏感数据并解决由数据异质性引起的问题。

    Operators from various industries have been pushing the adoption of wireless sensing nodes for industrial monitoring, and such efforts have produced sizeable condition monitoring datasets that can be used to build diagnosis algorithms capable of warning maintenance engineers of impending failure or identifying current system health conditions. However, single operators may not have sufficiently large fleets of systems or component units to collect sufficient data to develop data-driven algorithms. Collecting a satisfactory quantity of fault patterns for safety-critical systems is particularly difficult due to the rarity of faults. Federated learning (FL) has emerged as a promising solution to leverage datasets from multiple operators to train a decentralized asset fault diagnosis model while maintaining data confidentiality. However, there are still considerable obstacles to overcome when it comes to optimizing the federation strategy without leaking sensitive data and addressing the i
    
[^49]: 让模型变得更浅：联合学习降低深度和非线性以实现延迟高效的私人推断

    Making Models Shallow Again: Jointly Learning to Reduce Non-Linearity and Depth for Latency-Efficient Private Inference. (arXiv:2304.13274v1 [cs.LG])

    [http://arxiv.org/abs/2304.13274](http://arxiv.org/abs/2304.13274)

    本文提出了一种联合优化方法，可以学习深度神经网络变得更浅，具体地，利用卷积块的ReLU灵敏度来合并卷积层和去除ReLU层，可以在减少ReLU和线性操作的情况下提高模型计算效率，同时减小模型模型延迟和大小，准确率不显著下降。

    

    深度神经网络的大量ReLU和MAC操作使它们不适合进行延迟和计算高效的私人推断。本文提出了一种模型优化方法，允许模型学习变得浅。具体地，我们利用卷积块的ReLU灵敏度来移除ReLU层，并将其前后的卷积层合并为一个浅层。与现有的ReLU降低方法不同，我们的联合降低方法可以获得更好的ReLU和线性操作降低，分别提高了1.73倍和1.47倍，使用CIFAR-100上的ResNet18进行评估时，几乎没有显著的准确度下降。

    Large number of ReLU and MAC operations of Deep neural networks make them ill-suited for latency and compute-efficient private inference. In this paper, we present a model optimization method that allows a model to learn to be shallow. In particular, we leverage the ReLU sensitivity of a convolutional block to remove a ReLU layer and merge its succeeding and preceding convolution layers to a shallow block. Unlike existing ReLU reduction methods, our joint reduction method can yield models with improved reduction of both ReLUs and linear operations by up to 1.73x and 1.47x, respectively, evaluated with ResNet18 on CIFAR-100 without any significant accuracy-drop.
    
[^50]: 从关联到生成：无监督跨模态映射的纯文本字幕生成

    From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v1 [cs.CV])

    [http://arxiv.org/abs/2304.13273](http://arxiv.org/abs/2304.13273)

    本研究提出了一种从关联到生成的零-shot方法：通过将图像/视频投影到语言模态并在生成任务中生成描述性字幕。该方法在多个基准数据集上显著优于现有的最先进方法，为无监督跨模态映射提供了一个新的视角，并具有在视频字幕，图像合成和文本到图像生成等领域的潜在应用。

    

    随着以CLIP和ALIGN为代表的视觉-语言预训练模型的发展，CLIP的零-shot能力在图像分类和图像-文本检索等基于关联的视觉任务中取得了重大突破。但是，CLIP难以应用于基于生成的任务。这是由于缺乏解码器架构和生成的预训练任务。我们提出了K最近邻跨模态映射（Knight），一种从关联到生成的零-shot方法。通过窄字幕任务的纯文本无监督预训练来有效地将图像/视频投影到语言模态并在生成任务中生成描述性字幕。实验结果表明，Knight在多个基准数据集上显著优于现有的最先进方法。我们的方法为无监督跨模态映射提供了一个新的视角，并且将在视频字幕，图像合成和文本到图像生成等领域具有潜在应用。

    With the development of Vision-Language Pre-training Models (VLPMs) represented by CLIP and ALIGN, significant breakthroughs have been achieved for association-based visual tasks such as image classification and image-text retrieval by the zero-shot capability of CLIP without fine-tuning. However, CLIP is hard to apply to generation-based tasks. This is due to the lack of decoder architecture and pre-training tasks for generation. Although previous works have created generation capacity for CLIP through additional language models, a modality gap between the CLIP representations of different modalities and the inability of CLIP to model the offset of this gap, which fails the concept to transfer across modalities. To solve the problem, we try to map images/videos to the language modality and generate captions from the language modality. In this paper, we propose the K-nearest-neighbor Cross-modality Mapping (Knight), a zero-shot method from association to generation. With text-only unsu
    
[^51]: 贝叶斯联邦学习：综述

    Bayesian Federated Learning: A Survey. (arXiv:2304.13267v1 [cs.LG])

    [http://arxiv.org/abs/2304.13267](http://arxiv.org/abs/2304.13267)

    本文综述了贝叶斯联邦学习（BFL）的基本概念和分类，包括客户端、服务器端和基于FL的BFL方法。BFL是解决现有FL方法中受限和动态的数据和条件、异质性和不确定性以及分析解释能力挑战的有前途方法。

    

    联邦学习（FL）以隐私保护的方式整合了分布式基础设施、通信、计算和学习，展示了其优势。然而，现有FL方法的鲁棒性和能力受到有限和动态的数据和条件，包括异质性和不确定性，以及分析解释能力的挑战。贝叶斯联邦学习（BFL）已成为解决这些问题的一种有前途的方法。本综述对BFL进行了批判性的概述，包括其基本概念，其在FL上下文中与贝叶斯学习的关系，以及从贝叶斯和联邦学习的角度对BFL进行分类和讨论。我们将客户端和服务器端的BFL方法以及基于FL的BFL方法进行了分类和讨论，分析了它们的优缺点。本文还讨论了现有BFL方法的局限性以及BFL研究的未来方向，以进一步满足现实生活中FL应用的复杂需求。

    Federated learning (FL) demonstrates its advantages in integrating distributed infrastructure, communication, computing and learning in a privacy-preserving manner. However, the robustness and capabilities of existing FL methods are challenged by limited and dynamic data and conditions, complexities including heterogeneities and uncertainties, and analytical explainability. Bayesian federated learning (BFL) has emerged as a promising approach to address these issues. This survey presents a critical overview of BFL, including its basic concepts, its relations to Bayesian learning in the context of FL, and a taxonomy of BFL from both Bayesian and federated perspectives. We categorize and discuss client- and server-side and FL-based BFL methods and their pros and cons. The limitations of the existing BFL methods and the future directions of BFL research further address the intricate requirements of real-life FL applications.
    
[^52]: SHIELD：防御代码作者归属。

    SHIELD: Thwarting Code Authorship Attribution. (arXiv:2304.13255v1 [cs.CR])

    [http://arxiv.org/abs/2304.13255](http://arxiv.org/abs/2304.13255)

    本研究设计了 SHIELD 来检查代码作者归属方法对抗性代码示例的鲁棒性。实验结果表明当前的作者归属方法对于对抗性攻击和扰动具有脆弱性。

    

    作者归属的准确性不断提高，这对于希望保持匿名的程序员构成了严重的隐私风险。本文介绍了 SHIELD，用于检查不同的代码作者归属方法对抗性代码示例的鲁棒性。我们定义了四种攻击归属技术的方法，包括有针对性和非有针对性的攻击，并使用对抗性代码扰动来实现它们。我们使用来自 Google Code Jam 竞赛的 200 个程序员的数据集来验证我们的方法，针对采用各种技术从源代码中提取作者属性的六种最新作者归属方法进行攻击，包括 RNN、CNN 和代码样式学。我们的实验证明了当前作者归属方法对于对抗性攻击的漏洞。对于非有针对性攻击，我们的实验表明当前作者归属方法在攻击下的脆弱性。

    Authorship attribution has become increasingly accurate, posing a serious privacy risk for programmers who wish to remain anonymous. In this paper, we introduce SHIELD to examine the robustness of different code authorship attribution approaches against adversarial code examples. We define four attacks on attribution techniques, which include targeted and non-targeted attacks, and realize them using adversarial code perturbation. We experiment with a dataset of 200 programmers from the Google Code Jam competition to validate our methods targeting six state-of-the-art authorship attribution methods that adopt a variety of techniques for extracting authorship traits from source-code, including RNN, CNN, and code stylometry. Our experiments demonstrate the vulnerability of current authorship attribution methods against adversarial attacks. For the non-targeted attack, our experiments demonstrate the vulnerability of current authorship attribution methods against the attack with an attack 
    
[^53]: 分析浏览器中的加密挖矿

    Analyzing In-browser Cryptojacking. (arXiv:2304.13253v1 [cs.CR])

    [http://arxiv.org/abs/2304.13253](http://arxiv.org/abs/2304.13253)

    本文研究了浏览器中加密挖矿的静态、动态和经济方面，对加密挖矿样本进行分类，采用机器学习进行区分并分析了其对于计算机系统资源的影响。

    

    加密挖矿是未经许可使用目标设备进行隐秘挖掘加密货币的行为。攻击者使用恶意JavaScript代码强制浏览器解决工作量证明问题，从而利用网站访客的资源赚钱。为了理解和对抗这样的攻击，我们系统地分析了浏览器中加密挖掘的静态、动态和经济方面。对于静态分析，我们对加密挖掘样本进行内容、货币和基于代码的分类，以1）测量它们在网站上的分布，2）突出它们的平台亲和性，以及3）研究它们的代码复杂性。我们采用机器学习技术将加密挖掘脚本与良性和恶意JavaScript样本区分开来，准确率达到100％。对于动态分析，我们分析了加密挖掘对关键系统资源（如CPU和电池使用）的影响。我们还执行Web浏览器指纹识别以分析信息交换。

    Cryptojacking is the permissionless use of a target device to covertly mine cryptocurrencies. With cryptojacking, attackers use malicious JavaScript codes to force web browsers into solving proof-of-work puzzles, thus making money by exploiting the resources of the website visitors. To understand and counter such attacks, we systematically analyze the static, dynamic, and economic aspects of in-browser cryptojacking. For static analysis, we perform content, currency, and code-based categorization of cryptojacking samples to 1) measure their distribution across websites, 2) highlight their platform affinities, and 3) study their code complexities. We apply machine learning techniques to distinguish cryptojacking scripts from benign and malicious JavaScript samples with 100\% accuracy. For dynamic analysis, we analyze the effect of cryptojacking on critical system resources, such as CPU and battery usage. We also perform web browser fingerprinting to analyze the information exchange betw
    
[^54]: 一种使用机器学习进行密码协议安全验证的框架

    A Security Verification Framework of Cryptographic Protocols Using Machine Learning. (arXiv:2304.13249v1 [cs.CR])

    [http://arxiv.org/abs/2304.13249](http://arxiv.org/abs/2304.13249)

    该论文提出了一种使用机器学习方法以较短时间验证密码协议安全性的框架。

    

    我们提出了一种使用机器学习进行密码协议安全验证的框架。随着密码协议越来越复杂，自动验证技术的研究已经变得越来越重要。主要的技术是形式化验证。然而，形式化验证存在两个问题：它需要大量的计算时间，并且不能保证可决定性。我们提出了一种方法，使用机器学习以线性时间复杂度验证密码协议的安全性。

    We propose a security verification framework for cryptographic protocols using machine learning. In recent years, as cryptographic protocols have become more complex, research on automatic verification techniques has been focused on. The main technique is formal verification. However, the formal verification has two problems: it requires a large amount of computational time and does not guarantee decidability. We propose a method that allows security verification with computational time on the order of linear with respect to the size of the protocol using machine learning. In training machine learning models for security verification of cryptographic protocols, a sufficient amount of data, i.e., a set of protocol data with security labels, is difficult to collect from academic papers and other sources. To overcome this issue, we propose a way to create arbitrarily large datasets by automatically generating random protocols and assigning security labels to them using formal verification
    
[^55]: 从局部观测学习预测导航模式

    Learning to Predict Navigational Patterns from Partial Observations. (arXiv:2304.13242v1 [cs.CV])

    [http://arxiv.org/abs/2304.13242](http://arxiv.org/abs/2304.13242)

    本文提出了一种仅通过局部观测学习预测真实环境中导航模式的自监督学习方法，能够胜过两个有监督模型，并且可以在无限数据的情况下预测无偏本地方向软车道概率场。

    

    人类通过遵守相互知晓的导航模式在遵循规则的环境下进行合作导航，这些模式可以表示为方向路径或道路车道。从不完全观测到的环境中推断出这些导航模式是智能移动机器人在未映射位置操作所必需的。然而，算法定义这些导航模式是非常困难的。本文提出了第一个仅从局部观测中学习推断真实环境中导航模式的自监督学习（SSL）方法。我们解释了如何使用几何数据增强，预测世界建模和信息论正则化器实现了在无限数据情况下预测无偏本地方向软车道概率（DSLP）场。我们演示了如何通过将最大似然图拟合到DSLP场中来推断全局导航模式。实验证明，在模拟和移动机器人的真实世界实验中，我们的自监督模型在从局部观测中学习预测导航模式的任务中胜过了两个SOTA有监督模型。

    Human beings cooperatively navigate rule-constrained environments by adhering to mutually known navigational patterns, which may be represented as directional pathways or road lanes. Inferring these navigational patterns from incompletely observed environments is required for intelligent mobile robots operating in unmapped locations. However, algorithmically defining these navigational patterns is nontrivial. This paper presents the first self-supervised learning (SSL) method for learning to infer navigational patterns in real-world environments from partial observations only. We explain how geometric data augmentation, predictive world modeling, and an information-theoretic regularizer enables our model to predict an unbiased local directional soft lane probability (DSLP) field in the limit of infinite data. We demonstrate how to infer global navigational patterns by fitting a maximum likelihood graph to the DSLP field. Experiments show that our SSL model outperforms two SOTA supervis
    
[^56]: 金融公告中结构图识别

    Structure Diagram Recognition in Financial Announcements. (arXiv:2304.13240v1 [cs.CV])

    [http://arxiv.org/abs/2304.13240](http://arxiv.org/abs/2304.13240)

    本文提出了一种新的识别金融公告中结构图的方法，并通过两阶段方法生成了行业的第一个基准。实验结果显示，与现有方法相比，本文所提出的方法具有更高的有效性和效率。

    

    在构建金融知识图谱并进一步提高各种金融应用的效率方面，准确从金融公告中的结构图中提取结构化数据具有很大的实际意义。本文提出了一种新的方法，能更好地检测和提取不同类型的连接线，包括不同方向和角度的直线、曲线和折线。我们开发了一个两阶段的方法，使用自动化工具合成并注释大量图表以训练初步识别模型，并自动标注真实结构图以进行少量手动更正，最终实验验证了我们提出的方法在基准数据集上的有效性和效率，并与现有方法相比取得了最先进的性能。

    Accurately extracting structured data from structure diagrams in financial announcements is of great practical importance for building financial knowledge graphs and further improving the efficiency of various financial applications. First, we proposed a new method for recognizing structure diagrams in financial announcements, which can better detect and extract different types of connecting lines, including straight lines, curves, and polylines of different orientations and angles. Second, we developed a two-stage method to efficiently generate the industry's first benchmark of structure diagrams from Chinese financial announcements, where a large number of diagrams were synthesized and annotated using an automated tool to train a preliminary recognition model with fairly good performance, and then a high-quality benchmark can be obtained by automatically annotating the real-world structure diagrams using the preliminary model and then making few manual corrections. Finally, we experi
    
[^57]: 多准则硬件特洛伊检测：一种强化学习方法

    Multi-criteria Hardware Trojan Detection: A Reinforcement Learning Approach. (arXiv:2304.13232v1 [cs.AR])

    [http://arxiv.org/abs/2304.13232](http://arxiv.org/abs/2304.13232)

    本文提出了一种多准则强化学习（RL）HT检测工具，可适用于不同的HT检测场景，提高了HT检测的成功率。

    

    硬件特洛伊（HT）是一种不受欢迎的设计或制造修改，会严重影响数字集成电路的安全性和功能。HT可以根据各种设计准则插入，例如，网络切换活动，可观测性，可控性等。然而，据我们所知，大多数HT检测方法只基于单一标准，即网络切换活动。本文提出了一种多准则强化学习（RL）HT检测工具，该工具具有可调节的奖励函数，可用于不同的HT检测场景。该工具允许探索现有的检测策略，并可以以最小的努力适应新的检测场景。我们还提出了一种公正比较HT检测方法的通用方法。我们的初步结果显示ISCAS-85基准测试中84.2％的HT检测成功。

    Hardware Trojans (HTs) are undesired design or manufacturing modifications that can severely alter the security and functionality of digital integrated circuits. HTs can be inserted according to various design criteria, e.g., nets switching activity, observability, controllability, etc. However, to our knowledge, most HT detection methods are only based on a single criterion, i.e., nets switching activity. This paper proposes a multi-criteria reinforcement learning (RL) HT detection tool that features a tunable reward function for different HT detection scenarios. The tool allows for exploring existing detection strategies and can adapt new detection scenarios with minimal effort. We also propose a generic methodology for comparing HT detection methods fairly. Our preliminary results show an average of 84.2% successful HT detection in ISCAS-85 benchmark
    
[^58]: UNADON：用于预测全基因组染色体空间位置的基于Transformer的模型

    UNADON: Transformer-based model to predict genome-wide chromosome spatial position. (arXiv:2304.13230v1 [q-bio.GN])

    [http://arxiv.org/abs/2304.13230](http://arxiv.org/abs/2304.13230)

    UNADON是一种基于Transformer的深度学习模型，可以预测全基因组的染色体空间位置。通过使用序列特征和表观遗传信号，UNADON在训练单个细胞系时能够高度准确地预测染色质空间定位到核体，并揭示了潜在影响染色质区隔的序列和表观遗传因素。

    

    染色体相对于功能性核体的空间定位与基因组功能（如转录）密切相关。然而，影响全基因组范围内染色质空间定位的序列模式和表观遗传特征尚未被很好地理解。在这里，我们开发了一种新的基于Transformer的深度学习模型UNADON，它使用序列特征和表观遗传信号，预测了通过TSA-seq测量的特定类型核体的全基因组细胞学距离。在四种细胞系（K562，H1，HFFc6，HCT116）中对UNADON的评估表明，在单个细胞系训练时，高度准确地预测了染色质空间定位到核体。UNADON在未见过的细胞类型中表现良好。重要的是，我们揭示了影响大尺度染色质区隔到核体的潜在序列和表观遗传因素。综上，UNADON为了解基因组中的序列特征和染色质空间定位的原理提供了新的见解，并为研究核的功能组织提供了有价值的工具。

    The spatial positioning of chromosomes relative to functional nuclear bodies is intertwined with genome functions such as transcription. However, the sequence patterns and epigenomic features that collectively influence chromatin spatial positioning in a genome-wide manner are not well understood. Here, we develop a new transformer-based deep learning model called UNADON, which predicts the genome-wide cytological distance to a specific type of nuclear body, as measured by TSA-seq, using both sequence features and epigenomic signals. Evaluations of UNADON in four cell lines (K562, H1, HFFc6, HCT116) show high accuracy in predicting chromatin spatial positioning to nuclear bodies when trained on a single cell line. UNADON also performed well in an unseen cell type. Importantly, we reveal potential sequence and epigenomic factors that affect large-scale chromatin compartmentalization to nuclear bodies. Together, UNADON provides new insights into the principles between sequence features a
    
[^59]: 基于任务导向多目标优化的对抗样本生成

    Generating Adversarial Examples with Task Oriented Multi-Objective Optimization. (arXiv:2304.13229v1 [cs.LG])

    [http://arxiv.org/abs/2304.13229](http://arxiv.org/abs/2304.13229)

    本文提出了“基于任务导向的MOO”方法来实现对抗样本生成以同时实现多个目标，避免了朴素MOO最大化所有目标的弊端。

    

    深度学习模型，即使是最先进的模型，也很容易受到对抗样本的攻击。对抗训练是提高模型稳健性的最有效方法之一。对于对抗训练的成功来说，关键因素是要能够生成满足某些目标/目标的合格且有差异的对抗样本（例如，找到最大化模型损失以同时攻击多个模型的对抗样本）。因此，多目标优化（MOO）是实现对抗性样本生成以同时实现多个目标/目标的自然工具。本文观察到，MOO的朴素应用往往会平等地最大化所有目标/目标，而不关心目标/目标是否已经实现。这导致了在已实现目标/目标的任务上做无用的努力，而在未实现目标/目标的任务上则投入了较少的关注。因此，本文提出了“基于任务导向的MOO”来解决这一问题，在此情况下...

    Deep learning models, even the-state-of-the-art ones, are highly vulnerable to adversarial examples. Adversarial training is one of the most efficient methods to improve the model's robustness. The key factor for the success of adversarial training is the capability to generate qualified and divergent adversarial examples which satisfy some objectives/goals (e.g., finding adversarial examples that maximize the model losses for simultaneously attacking multiple models). Therefore, multi-objective optimization (MOO) is a natural tool for adversarial example generation to achieve multiple objectives/goals simultaneously. However, we observe that a naive application of MOO tends to maximize all objectives/goals equally, without caring if an objective/goal has been achieved yet. This leads to useless effort to further improve the goal-achieved tasks, while putting less focus on the goal-unachieved tasks. In this paper, we propose \emph{Task Oriented MOO} to address this issue, in the contex
    
[^60]: 基于分数的BSDE扩散模型：反演和生成。

    Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation. (arXiv:2304.13224v1 [cs.LG])

    [http://arxiv.org/abs/2304.13224](http://arxiv.org/abs/2304.13224)

    本文提出了一种基于BSDE的扩散模型，可以通过调整现有的分数函数确定到达所需终端分布所需的初始条件，为扩散建模提供了一种新的方法，并在扩散反演，条件扩散和不确定性量化等领域具有潜在应用。

    

    所提出的基于BSDE的扩散模型为扩散建模提供了一种新的方法，扩展了随机微分方程在机器学习中的应用。与传统的SDE-based扩散模型不同，我们的模型可以通过调整现有的分数函数确定到达所需终端分布所需的初始条件。我们展示了该模型的理论保证，使用Lipschitz网络进行分数匹配的优势，以及在扩散反演，条件扩散和不确定性量化等各个领域的潜在应用。我们的工作对于基于分数的生成学习领域做出了贡献，为解决现实问题提供了有前途的方向。

    The proposed BSDE-based diffusion model represents a novel approach to diffusion modeling, which extends the application of stochastic differential equations (SDEs) in machine learning. Unlike traditional SDE-based diffusion models, our model can determine the initial conditions necessary to reach a desired terminal distribution by adapting an existing score function. We demonstrate the theoretical guarantees of the model, the benefits of using Lipschitz networks for score matching, and its potential applications in various areas such as diffusion inversion, conditional diffusion, and uncertainty quantification. Our work represents a contribution to the field of score-based generative learning and offers a promising direction for solving real-world problems.
    
[^61]: 具有局部参数模型知识的强化学习

    Reinforcement Learning with Partial Parametric Model Knowledge. (arXiv:2304.13223v1 [eess.SY])

    [http://arxiv.org/abs/2304.13223](http://arxiv.org/abs/2304.13223)

    该论文研究了在环境完全无知和完美知识之间的机遇，提出了一种利用局部模型和保持数据驱动调整的强化学习方法，已在线性二次调节器上得到验证。

    

    我们将强化学习方法应用于连续控制，以填补在环境完全无知和完美知识之间的差距。我们的方法，Partial Knowledge Least Squares Policy Iteration(PLSPI)，既借鉴了模型无关的强化学习，也借鉴了模型基础控制。它利用局部模型的不完全信息，并保留强化学习朝向最优性能的数据驱动调整。我们以线性二次调节器为案例研究；数值实验证明了所提方法的有效性和带来的好处。

    We adapt reinforcement learning (RL) methods for continuous control to bridge the gap between complete ignorance and perfect knowledge of the environment. Our method, Partial Knowledge Least Squares Policy Iteration (PLSPI), takes inspiration from both model-free RL and model-based control. It uses incomplete information from a partial model and retains RL's data-driven adaption towards optimal performance. The linear quadratic regulator provides a case study; numerical experiments demonstrate the effectiveness and resulting benefits of the proposed method.
    
[^62]: ZRG: 一个用于机器学习的高分辨率住宅屋顶三维几何数据集

    ZRG: A High Resolution 3D Residential Rooftop Geometry Dataset for Machine Learning. (arXiv:2304.13219v1 [cs.CV])

    [http://arxiv.org/abs/2304.13219](http://arxiv.org/abs/2304.13219)

    该论文介绍了 ZRG 数据集，其中包含成千上万个样本的高分辨率住宅屋顶正交图像拼接、对应 DSM、3D 屋顶框架和多视图图像生成的点云。该数据集可以用于住宅屋顶结构的几何建模和场景理解，如屋顶轮廓提取、单目高度估计和平面屋顶结构提取等任务。

    

    本文介绍了 Zeitview Rooftop Geometry (ZRG) 数据集，其中包含成千上万个高分辨率住宅屋顶正交图像拼接、对应数字表面模型 (DSM)、3D 屋顶框架和多视图图像生成的点云用于住宅屋顶几何和场景理解。我们进行了详细的基准测试，展示了此数据集解锁的众多应用，并为屋顶轮廓提取、单目高度估计和平面屋顶结构提取等任务提供了基线。

    In this paper we present the Zeitview Rooftop Geometry (ZRG) dataset. ZRG contains thousands of samples of high resolution orthomosaics of aerial imagery of residential rooftops with corresponding digital surface models (DSM), 3D rooftop wireframes, and multiview imagery generated point clouds for the purpose of residential rooftop geometry and scene understanding. We perform thorough benchmarks to illustrate the numerous applications unlocked by this dataset and provide baselines for the tasks of roof outline extraction, monocular height estimation, and planar roof structure extraction.
    
[^63]: 带有条件扩散概率模型的单视图高度估计

    Single-View Height Estimation with Conditional Diffusion Probabilistic Models. (arXiv:2304.13214v1 [cs.CV])

    [http://arxiv.org/abs/2304.13214](http://arxiv.org/abs/2304.13214)

    本文提出了一种使用条件扩散概率模型进行单视图高度估计的方法，通过训练一个生成模型来学习光学和DSM图像之间的联合分布，实现生成逼真的高分辨率3D表面。

    

    数字地表模型（DSM）提供了丰富的高度信息，可用于理解地球表面以及监测自然和人造结构的存在或变化。传统的高度估计需要多视角地理空间影像或激光雷达点云，而这可能是昂贵的。使用神经网络模型的单视图高度估计显示出良好的前景，但它可能难以重建高分辨率特征。尚未利用高分辨率图像合成和编辑的最新扩散模型的进展来进行遥感图像，特别是高度估计。我们的方法涉及训练生成扩散模型，以学习光学和DSM图像在两个域中的联合分布，作为马尔科夫链。这是通过最小化去噪评分匹配目标来实现的，同时以源图像为条件生成逼真的高分辨率3D表面。

    Digital Surface Models (DSM) offer a wealth of height information for understanding the Earth's surface as well as monitoring the existence or change in natural and man-made structures. Classical height estimation requires multi-view geospatial imagery or LiDAR point clouds which can be expensive to acquire. Single-view height estimation using neural network based models shows promise however it can struggle with reconstructing high resolution features. The latest advancements in diffusion models for high resolution image synthesis and editing have yet to be utilized for remote sensing imagery, particularly height estimation. Our approach involves training a generative diffusion model to learn the joint distribution of optical and DSM images across both domains as a Markov chain. This is accomplished by minimizing a denoising score matching objective while being conditioned on the source image to generate realistic high resolution 3D surfaces. In this paper we experiment with condition
    
[^64]: 将物理信息神经网络应用于整数和分数阶神经元模型的动力学推断的分裂方法

    Splitting physics-informed neural networks for inferring the dynamics of integer- and fractional-order neuron models. (arXiv:2304.13205v1 [math.NA])

    [http://arxiv.org/abs/2304.13205](http://arxiv.org/abs/2304.13205)

    本文提出了一种用于正向动力学系统的分裂物理信息神经网络方法，应用于求解整数和分数阶神经元模型并展现出更高的准确性和效率。

    

    我们提出了一种新的方法，将拆分方法和物理信息神经网络（PINN）相结合，用于求解微分方程的正向系统。所提出的分裂PINN方法有效地解决了在正向动力学系统中应用PINN所面临的挑战，并通过其在神经元模型中的应用表现出更高的准确性。具体而言，我们应用算子拆分将原始神经元模型分解为子问题，然后使用PINN来解决这些子问题。此外，我们开发了$L^1$方案，用于离散分数导数在分数神经元模型中的应用，从而实现更高的准确性和效率。本研究的结果凸显了分裂PINN在解决整数和分数阶神经元模型以及其他类似的计算科学和工程系统中的潜力。

    We introduce a new approach for solving forward systems of differential equations using a combination of splitting methods and physics-informed neural networks (PINNs). The proposed method, splitting PINN, effectively addresses the challenge of applying PINNs to forward dynamical systems and demonstrates improved accuracy through its application to neuron models. Specifically, we apply operator splitting to decompose the original neuron model into sub-problems that are then solved using PINNs. Moreover, we develop an $L^1$ scheme for discretizing fractional derivatives in fractional neuron models, leading to improved accuracy and efficiency. The results of this study highlight the potential of splitting PINNs in solving both integer- and fractional-order neuron models, as well as other similar systems in computational science and engineering.
    
[^65]: 核方法在算子学习中表现竞争力

    Kernel Methods are Competitive for Operator Learning. (arXiv:2304.13202v1 [stat.ML])

    [http://arxiv.org/abs/2304.13202](http://arxiv.org/abs/2304.13202)

    本文提出了一个核方法算子学习框架，在对多组数据进行全面比较后，结果表明该方法在多种设置下都是一种具有竞争力的算子学习方法。

    

    我们提出了一个基于核的算子学习框架，并提供了先验误差分析和与流行的神经网络方法（如Deep Operator Net（DeepONet）[Lu et al.]和Fourier神经算子（FNO）[Li et al.]）的全面数字比较。我们考虑目标算子$\mathcal{G}^\dagger:\mathcal{U}\to\mathcal{V}$的输入/输出空间是再生核希尔伯特空间（RKHS）的情况，数据以输入/输出函数的部分观测$\varphi(v_i),\phi(u_i)$的形式出现，其中$v_i=\mathcal{G}^\dagger(u_i)$（$i=1,\ldots,N$），测量算子$\varphi:\mathcal{V}\to\mathbb{R}^m$和$\phi:\mathcal{U}\to\mathbb{R}^n$是线性的。在写$\psi:\mathbb{R}^n\to\mathcal{U}$和$\chi:\mathbb{R}^m\to\mathcal{V}$作为与$\phi$和$\varphi$相关的最佳恢复映射时，我们使用$\bar{f}$ 核映射 $L^2(\mathcal{U},\mathbb{R}^n)$ 定义一个$k$ 类型的最小二乘模型， 然后用 $\bar{\mathcal{G}}=\chi\circ\bar{f}\circ\psi$ 来近似$\mathcal{G}^\dagger$。 我们的分析涉及多个例子，包括常见的偏微分方程的算子近似，结果表明在多种设置下核方法都是一种具有竞争力的算子学习方法。

    We present a general kernel-based framework for learning operators between Banach spaces along with a priori error analysis and comprehensive numerical comparisons with popular neural net (NN) approaches such as Deep Operator Net (DeepONet) [Lu et al.] and Fourier Neural Operator (FNO) [Li et al.]. We consider the setting where the input/output spaces of target operator $\mathcal{G}^\dagger\,:\, \mathcal{U}\to \mathcal{V}$ are reproducing kernel Hilbert spaces (RKHS), the data comes in the form of partial observations $\phi(u_i), \varphi(v_i)$ of input/output functions $v_i=\mathcal{G}^\dagger(u_i)$ ($i=1,\ldots,N$), and the measurement operators $\phi\,:\, \mathcal{U}\to \mathbb{R}^n$ and $\varphi\,:\, \mathcal{V} \to \mathbb{R}^m$ are linear. Writing $\psi\,:\, \mathbb{R}^n \to \mathcal{U}$ and $\chi\,:\, \mathbb{R}^m \to \mathcal{V}$ for the optimal recovery maps associated with $\phi$ and $\varphi$, we approximate $\mathcal{G}^\dagger$ with $\bar{\mathcal{G}}=\chi \circ \bar{f} \ci
    
[^66]: Connector 0.5: 一种用于图表示学习的统一框架

    Connector 0.5: A unified framework for graph representation learning. (arXiv:2304.13195v1 [cs.LG])

    [http://arxiv.org/abs/2304.13195](http://arxiv.org/abs/2304.13195)

    本文提出了一种名为Connector的图表示框架，可以覆盖从浅层到最先进的各种图嵌入模型，并考虑通过构建具有不同结构关系的各种类型的图来生成图。

    

    图表示学习模型旨在将图结构及其特征表示为潜在空间中的低维向量，该方法有助于进行各种下游任务，如节点分类和链接预测。鉴于其强大的图数据建模能力，已提出了各种图嵌入模型和库来学习嵌入，并帮助研究人员轻松开展实验。在本文中，我们介绍了一种新颖的图表示框架Connector，该框架涵盖各种图嵌入模型，从浅层模型到最先进的模型。我们首先考虑通过构建具有不同结构关系的各种类型的图来生成图，包括同质、带符号、异质和知识图。其次，我们介绍了各种图表示学习模型，从浅层到深层图嵌入模型。最后，我们计划建立一个高效的开源框架，可提供深度图嵌入模型。

    Graph representation learning models aim to represent the graph structure and its features into low-dimensional vectors in a latent space, which can benefit various downstream tasks, such as node classification and link prediction. Due to its powerful graph data modelling capabilities, various graph embedding models and libraries have been proposed to learn embeddings and help researchers ease conducting experiments. In this paper, we introduce a novel graph representation framework covering various graph embedding models, ranging from shallow to state-of-the-art models, namely Connector. First, we consider graph generation by constructing various types of graphs with different structural relations, including homogeneous, signed, heterogeneous, and knowledge graphs. Second, we introduce various graph representation learning models, ranging from shallow to deep graph embedding models. Finally, we plan to build an efficient open-source framework that can provide deep graph embedding mode
    
[^67]: 基于视觉触觉传感与置信度校准神经网络的可靠结直肠癌息肉诊断

    Towards Reliable Colorectal Cancer Polyps Classification via Vision Based Tactile Sensing and Confidence-Calibrated Neural Networks. (arXiv:2304.13192v1 [cs.CV])

    [http://arxiv.org/abs/2304.13192](http://arxiv.org/abs/2304.13192)

    本研究针对基于人工智能的结直肠癌（CRC）息肉分类技术输出过于自信的问题，提出了一种利用视觉触觉传感系统和独特的CRC息肉模拟体的残差神经网络分类器，并通过温度缩放的后处理方法解决其过度自信的输出问题，从而实现了可靠的CRC息肉诊断。

    

    本研究针对现有基于人工智能的结直肠癌（CRC）息肉分类技术输出过于自信的问题，提出了一种置信度校准残差神经网络。利用一种新型的视觉触觉传感系统和独特的CRC息肉模拟体，我们证明了传统的准确度和精度等指标不足以全面评估模型在处理敏感CRC息肉诊断方面的性能。为此，我们设计了一个残差神经网络分类器，并通过温度缩放的后处理方法解决其过度自信的输出问题。为评估所提出方法的可靠性，我们通过 reliability diagrams 和其他统计指标，引入了噪声和模糊效果来测试 VS-TS 所获取的纹理图像的非理想输入下的模型可靠性。

    In this study, toward addressing the over-confident outputs of existing artificial intelligence-based colorectal cancer (CRC) polyp classification techniques, we propose a confidence-calibrated residual neural network. Utilizing a novel vision-based tactile sensing (VS-TS) system and unique CRC polyp phantoms, we demonstrate that traditional metrics such as accuracy and precision are not sufficient to encapsulate model performance for handling a sensitive CRC polyp diagnosis. To this end, we develop a residual neural network classifier and address its over-confident outputs for CRC polyps classification via the post-processing method of temperature scaling. To evaluate the proposed method, we introduce noise and blur to the obtained textural images of the VS-TS and test the model's reliability for non-ideal inputs through reliability diagrams and other statistical metrics.
    
[^68]: TABLET：基于指令学习表格数据

    TABLET: Learning From Instructions For Tabular Data. (arXiv:2304.13188v1 [cs.LG])

    [http://arxiv.org/abs/2304.13188](http://arxiv.org/abs/2304.13188)

    该论文提出了TABLET，这是一个由20个不同的包含指令注释的表格数据集组成的基准测试，可以提高大型语言模型在表格预测问题上的效果，并评估了指令在保真度和LLM在表格预测方面的局限性。

    

    在训练表格预测的机器学习模型时，获取高质量的数据通常是一项重大挑战，特别是在隐私敏感和成本高的领域，比如医学和金融。向大型语言模型（LLM）提供自然语言指令提供了另一种解决方案。然而，指令如何有效地利用LLM中的知识来解决表格预测问题仍不清楚。为了弥补这一差距，我们介绍了TABLET，这是一个由20个不同的包含指令注释的表格数据集组成的基准测试，这些指令在措辞、细节和技术性方面各不相同。此外，TABLET还包括指令的逻辑和结构修改。我们发现，在上下文指令的帮助下，Flan-T5 11b的零示例F1性能平均提高了44％，在TABLET上，ChatGPT的提升为13％。此外，我们评估了指令保真度，探讨了使用LLM进行表格预测的局限性。我们发现LLM通常会忽略指令并依赖于数据中存在的偏差。

    Acquiring high-quality data is often a significant challenge in training machine learning (ML) models for tabular prediction, particularly in privacy-sensitive and costly domains like medicine and finance. Providing natural language instructions to large language models (LLMs) offers an alternative solution. However, it is unclear how effectively instructions leverage the knowledge in LLMs for solving tabular prediction problems. To address this gap, we introduce TABLET, a benchmark of 20 diverse tabular datasets annotated with instructions that vary in their phrasing, granularity, and technicality. Additionally, TABLET includes the instructions' logic and structured modifications to the instructions. We find in-context instructions increase zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% for ChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabular prediction in our benchmark by evaluating instruction faithfulness. We find LLMs often ignore instr
    
[^69]: 针对图文模型的样本特异性去偏方法

    Sample-Specific Debiasing for Better Image-Text Models. (arXiv:2304.13181v1 [cs.LG])

    [http://arxiv.org/abs/2304.13181](http://arxiv.org/abs/2304.13181)

    发现从训练数据集中均匀地抽取负样本会引入错误的负面样本。我们提出了一种纠正错误负面样本的新方法，即针对图文模型的样本特异性去偏方法。

    

    对于图文数据的自监督表征学习在医学应用中具有重要意义，例如图像分类、视觉定位和跨模态检索。一种常见的方法涉及对语义上相似（正）和不相似（负）的数据点进行对比。从训练数据集中均匀地抽取负样本会引入错误的负面样本，即将同属一类的样本视为不相似。在医疗保健数据中，潜在的类别分布是不均匀的，意味着错误的负面样本出现的比例高度不同。为了提高学得的表示质量，我们提出了一种纠正错误负面样本的新方法。我们的方法可以被看作是使用估计的样本特异性类别概率的去偏对比学习的变体。我们提供了目标函数的理论分析，并在图像和配对的图文数据集上展示了所提出的方法。我们的实验证明了我们的方法取得的更好的效果。

    Self-supervised representation learning on image-text data facilitates crucial medical applications, such as image classification, visual grounding, and cross-modal retrieval. One common approach involves contrasting semantically similar (positive) and dissimilar (negative) pairs of data points. Drawing negative samples uniformly from the training data set introduces false negatives, i.e., samples that are treated as dissimilar but belong to the same class. In healthcare data, the underlying class distribution is nonuniform, implying that false negatives occur at a highly variable rate. To improve the quality of learned representations, we develop a novel approach that corrects for false negatives. Our method can be viewed as a variant of debiased constrastive learning that uses estimated sample-specific class probabilities. We provide theoretical analysis of the objective function and demonstrate the proposed approach on both image and paired image-text data sets. Our experiments demo
    
[^70]: SAFE: 使用 Shard Graphs 进行机器遗忘

    SAFE: Machine Unlearning With Shard Graphs. (arXiv:2304.13169v1 [cs.LG])

    [http://arxiv.org/abs/2304.13169](http://arxiv.org/abs/2304.13169)

    本论文提出了一种使用 shard graph 进行机器遗忘的方法，以实现在最小化遗忘成本的情况下适应多样数据的大型模型，并取得了较高的准确性。

    

    我们提出了 Synergy Aware Forgetting Ensemble（SAFE）方法，该方法可以在最小化从训练模型中消除训练样本影响的预期成本的同时，适应各种数据的大型模型。这个过程也被称为选择性遗忘或遗忘，通常是通过将数据集分成碎片，对每个碎片进行完全独立的模型训练，然后将所得模型合成来进行的。增加碎片的数量可以降低遗忘的预期成本，但同时也增加了推理成本，并降低了模型的最终准确性，因为独立的模型训练过程中失去了样本之间的协同信息。SAFE 引入了 shard graph 的概念，它允许在训练过程中从其他碎片中引入有限的信息，以牺牲一定的预期遗忘成本增加明显的准确性，同时仍然实现完全消除残留影响。

    We present Synergy Aware Forgetting Ensemble (SAFE), a method to adapt large models on a diverse collection of data while minimizing the expected cost to remove the influence of training samples from the trained model. This process, also known as selective forgetting or unlearning, is often conducted by partitioning a dataset into shards, training fully independent models on each, then ensembling the resulting models. Increasing the number of shards reduces the expected cost to forget but at the same time it increases inference cost and reduces the final accuracy of the model since synergistic information between samples is lost during the independent model training. Rather than treating each shard as independent, SAFE introduces the notion of a shard graph, which allows incorporating limited information from other shards during training, trading off a modest increase in expected forgetting cost with a significant increase in accuracy, all while still attaining complete removal of resi
    
[^71]: 迈向计算优化的迁移学习

    Towards Compute-Optimal Transfer Learning. (arXiv:2304.13164v1 [cs.LG])

    [http://arxiv.org/abs/2304.13164](http://arxiv.org/abs/2304.13164)

    本文提出一种计算优化的迁移学习方法，通过对预训练模型进行零-shot结构剪枝，使其在最小降低性能的情况下提高计算效率，实现了20%以上的性能提升。

    

    预训练模型的出现使得迁移学习领域发生了重大变革，这些模型在下游任务中展现了强大的适应性。但是，微调或使用这些模型的高计算和存储要求可能会阻碍它们的广泛使用。在本研究中，我们提出了一个解决方案，通过提出一种简单而有效的方法，将计算效率与渐近性能之间的权衡，我们将其定义为学习算法在计算趋近于无穷大时实现的性能。具体而言，我们认为对预训练模型进行零-shot结构剪枝，可以使它们在最小降低性能的情况下提高计算效率。我们在提供多种迁移场景的Nevis'22连续学习基准上评估了我们的方法。我们的结果表明，在低计算范围内，剪枝预先训练模型的卷积过滤器可以带来超过20%的性能提高。

    The field of transfer learning is undergoing a significant shift with the introduction of large pretrained models which have demonstrated strong adaptability to a variety of downstream tasks. However, the high computational and memory requirements to finetune or use these models can be a hindrance to their widespread use. In this study, we present a solution to this issue by proposing a simple yet effective way to trade computational efficiency for asymptotic performance which we define as the performance a learning algorithm achieves as compute tends to infinity. Specifically, we argue that zero-shot structured pruning of pretrained models allows them to increase compute efficiency with minimal reduction in performance. We evaluate our method on the Nevis'22 continual learning benchmark that offers a diverse set of transfer scenarios. Our results show that pruning convolutional filters of pretrained models can lead to more than 20% performance improvement in low computational regimes.
    
[^72]: LumiGAN：生成可再照明的三维人脸的无条件生成

    LumiGAN: Unconditional Generation of Relightable 3D Human Faces. (arXiv:2304.13153v1 [cs.CV])

    [http://arxiv.org/abs/2304.13153](http://arxiv.org/abs/2304.13153)

    LumiGAN是一种无条件的生成对抗网络，具有基于物理学的照明模块，可以在推理时实现在新的光照下的再照明，它可以生成可再照明的人脸的逼真物理特性，并比现有的方法具有更好的逼真度和几何生成能力。

    

    从非结构化的二维图像数据中无监督地学习三维人脸是一个活跃的研究领域。尽管最近的研究已经实现了令人印象深刻的逼真度，但它们通常缺乏对照明的控制，这样生成的模型就无法在新环境中使用。为此，我们介绍了LumiGAN，一种无条件的生成对抗网络（GAN）用于生成三维人脸，并带有一个基于物理学的照明模块，该模块可以在推理时实现在新的光照下的再照明。与以往的工作不同，LumiGAN可以使用一种在自监督学习中学习的高效可见性公式来产生逼真的阴影效果。LumiGAN可以生成可再照明的人脸的逼真物理特性，包括表面法线、漫反射反照率和高光色调，而无需任何ground truth数据。除了可再照明性之外，我们还展示了与最先进的不可再照明的三维GAN相比，显著改进的几何生成，并且比现有的可再照明方法具有明显更好的逼真度。

    Unsupervised learning of 3D human faces from unstructured 2D image data is an active research area. While recent works have achieved an impressive level of photorealism, they commonly lack control of lighting, which prevents the generated assets from being deployed in novel environments. To this end, we introduce LumiGAN, an unconditional Generative Adversarial Network (GAN) for 3D human faces with a physically based lighting module that enables relighting under novel illumination at inference time. Unlike prior work, LumiGAN can create realistic shadow effects using an efficient visibility formulation that is learned in a self-supervised manner. LumiGAN generates plausible physical properties for relightable faces, including surface normals, diffuse albedo, and specular tint without any ground truth data. In addition to relightability, we demonstrate significantly improved geometry generation compared to state-of-the-art non-relightable 3D GANs and notably better photorealism than exi
    
[^73]: T细胞受体蛋白序列和稀疏编码：癌症分类的一种新方法。

    T Cell Receptor Protein Sequences and Sparse Coding: A Novel Approach to Cancer Classification. (arXiv:2304.13145v1 [cs.LG])

    [http://arxiv.org/abs/2304.13145](http://arxiv.org/abs/2304.13145)

    本研究探索了利用稀疏编码的方法对具有癌症分类目标的TCR蛋白序列进行多类别分类，为基于TCR的免疫治疗提供理论支持。

    

    癌症是一种以不受控制的细胞生长和增殖为特征的复杂疾病。T细胞受体（TCR）是适应性免疫系统中的重要蛋白质，它们对抗原的特异性识别在免疫反应中发挥着关键作用，包括对抗癌症。TCR的多样性和特异性使它们成为瞄准癌细胞的理想选择，近期测序技术的进步使得TCR库进行了全面的分析，这导致发现了具有强有力的抗癌活性的TCR，并开发了基于TCR的免疫治疗方法。在本研究中，我们探讨了利用稀疏编码对具有癌症分类作为目标标签的TCR蛋白序列进行多类别分类的应用。稀疏编码是一种机器学习中流行的技术，它能够利用一组信息丰富的特征来表示数据，可以捕捉氨基酸之间的复杂关系，识别序列中的微小模式。

    Cancer is a complex disease characterized by uncontrolled cell growth and proliferation. T cell receptors (TCRs) are essential proteins for the adaptive immune system, and their specific recognition of antigens plays a crucial role in the immune response against diseases, including cancer. The diversity and specificity of TCRs make them ideal for targeting cancer cells, and recent advancements in sequencing technologies have enabled the comprehensive profiling of TCR repertoires. This has led to the discovery of TCRs with potent anti-cancer activity and the development of TCR-based immunotherapies. In this study, we investigate the use of sparse coding for the multi-class classification of TCR protein sequences with cancer categories as target labels. Sparse coding is a popular technique in machine learning that enables the representation of data with a set of informative features and can capture complex relationships between amino acids and identify subtle patterns in the sequence tha
    
[^74]: 自监督的时空数据分析

    Self-Supervised Temporal Analysis of Spatiotemporal Data. (arXiv:2304.13143v1 [cs.AI])

    [http://arxiv.org/abs/2304.13143](http://arxiv.org/abs/2304.13143)

    本文提出了一种自监督方法，能根据移动活动时间序列对景观进行分层，通过深度语义分割实现了对地理空间任务的多模态建模，适用于分类居民区和商业区等不同任务。

    

    地理活动的时间模式与用地类型之间存在着相关性。本文提出了一种新的自监督方法，根据移动活动时间序列对景观进行分层。首先，将时间序列信号转换为频域，然后通过压缩自编码器将其压缩为任务无关的时间嵌入，该嵌入保留了时间序列中观察到的周期性时间模式。像素级的嵌入被转换为类似图像的通道，可以用于基于任务的下游地理空间任务的多模态建模，其中采用了深度语义分割。实验证明，时间嵌入是时间序列数据的语义有意义的表征，并且对于分类居民区和商业区等不同任务是有效的。

    There exists a correlation between geospatial activity temporal patterns and type of land use. A novel self-supervised approach is proposed to stratify landscape based on mobility activity time series. First, the time series signal is transformed to the frequency domain and then compressed into task-agnostic temporal embeddings by a contractive autoencoder, which preserves cyclic temporal patterns observed in time series. The pixel-wise embeddings are converted to image-like channels that can be used for task-based, multimodal modeling of downstream geospatial tasks using deep semantic segmentation. Experiments show that temporal embeddings are semantically meaningful representations of time series data and are effective across different tasks such as classifying residential area and commercial areas.
    
[^75]: 面向增材制造试件表面粗糙度预测的量子机器学习方法

    Quantum Machine Learning Approach for the Prediction of Surface Roughness in Additive Manufactured Specimens. (arXiv:2304.13142v1 [quant-ph])

    [http://arxiv.org/abs/2304.13142](http://arxiv.org/abs/2304.13142)

    首次针对增材制造试件的表面粗糙度使用三种量子算法（QNN、Q-Forest和VQC）进行回归预测，其中Q-Forest算法表现最优，具有较低的MSE和MAE和较高的EVS。

    

    表面粗糙度是影响增材制造零件性能和功能的重要因素。准确预测表面粗糙度对于优化制造过程和确保最终产品的质量至关重要。最近，量子计算作为解决复杂问题和创建精确预测模型的潜在解决方案引起了关注。在本研究论文中，我们首次针对增材制造试件的表面粗糙度进行了三种量子算法，包括量子神经网络（QNN）、量子森林（Q-Forest）和变分量子分类器（VQC）的回归适应比较。我们使用均方误差（MSE）、平均绝对误差（MAE）和解释方差得分（EVS）作为评估指标来评估算法的性能。我们的研究结果表明，Q-Forest算法超越了其他算法，MSE为56.905，MAE为7.479，EVS为0.2957。

    Surface roughness is a crucial factor influencing the performance and functionality of additive manufactured components. Accurate prediction of surface roughness is vital for optimizing manufacturing processes and ensuring the quality of the final product. Quantum computing has recently gained attention as a potential solution for tackling complex problems and creating precise predictive models. In this research paper, we conduct an in-depth comparison of three quantum algorithms i.e. the Quantum Neural Network (QNN), Quantum Forest (Q-Forest), and Variational Quantum Classifier (VQC) adapted for regression for predicting surface roughness in additive manufactured specimens for the first time. We assess the algorithms performance using Mean Squared Error (MSE), Mean Absolute Error (MAE), and Explained Variance Score (EVS) as evaluation metrics. Our findings show that the Q-Forest algorithm surpasses the other algorithms, achieving an MSE of 56.905, MAE of 7.479, and an EVS of 0.2957. I
    
[^76]: ESimCSE无监督对比学习联合UDA半监督学习用于大标签系统文本分类模型

    ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode. (arXiv:2304.13140v1 [cs.LG])

    [http://arxiv.org/abs/2304.13140](http://arxiv.org/abs/2304.13140)

    本文提出ESimCSE无监督比较学习和UDA半监督比较学习模型相结合，通过联合训练技术解决了大标签系统文本分类的多个问题，并在公共数据集上实现了准确率提高。

    

    在自然语言处理任务中，文本分类面临的挑战包括多个标签系统、数据分布不均匀和高噪声。为了解决这些问题，本文通过使用联合训练技术，将ESimCSE无监督比较学习和UDA半监督比较学习模型相结合。ESimCSE模型利用无标签数据高效地学习文本向量表示，从而实现更好的分类结果；而UDA则通过半监督学习方法使用无标签数据进行训练，提高模型的预测性能和稳定性，并进一步提高模型的泛化能力。此外，模型训练过程中采用了对抗训练技术FGM和PGD，以提高模型的鲁棒性和可靠性。实验结果表明，与基线模型相比，在公共数据集Ruesters上有8%和10%的准确率提高。

    The challenges faced by text classification with large tag systems in natural language processing tasks include multiple tag systems, uneven data distribution, and high noise. To address these problems, the ESimCSE unsupervised comparative learning and UDA semi-supervised comparative learning models are combined through the use of joint training techniques in the models.The ESimCSE model efficiently learns text vector representations using unlabeled data to achieve better classification results, while UDA is trained using unlabeled data through semi-supervised learning methods to improve the prediction performance of the models and stability, and further improve the generalization ability of the model. In addition, adversarial training techniques FGM and PGD are used in the model training process to improve the robustness and reliability of the model. The experimental results show that there is an 8% and 10% accuracy improvement relative to Baseline on the public dataset Ruesters as we
    
[^77]: 决策时间规划的更新等价框架

    The Update Equivalence Framework for Decision-Time Planning. (arXiv:2304.13138v1 [cs.AI])

    [http://arxiv.org/abs/2304.13138](http://arxiv.org/abs/2304.13138)

    该论文提出了一个基于更新等价的决策时间规划框架，使得决策时间规划算法不依赖于公共信息，在更大范围的不完全信息决策环境中实现超人类表现。

    

    在棋类游戏等完全信息环境中，即时修正（或构建）策略的决策时间规划是实现超人类表现的关键。一些研究将决策时间规划扩展到更普遍的不完全信息环境，从而实现了扑克中的超人类表现。但是，这些方法需要考虑随着非公共信息量的增加而快速增长的子游戏，使得它们在非公共信息量较大时不起作用。为了解决这个问题，我们引入了一种基于更新等价而不是子游戏概念的决策时间规划框架。在这个框架中，决策时间规划算法模拟同步学习算法的更新。这个框架使我们能够引入一系列原则上的决策时间规划算法，这些算法不依赖于公共信息，并为新的一个系列的决策时间规划算法打开了大门。

    The process of revising (or constructing) a policy immediately prior to execution -- known as decision-time planning -- is key to achieving superhuman performance in perfect-information settings like chess and Go. A recent line of work has extended decision-time planning to more general imperfect-information settings, leading to superhuman performance in poker. However, these methods requires considering subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on subgames but rather on the notion of update equivalence. In this framework, decision-time planning algorithms simulate updates of synchronous learning algorithms. This framework enables us to introduce a new family of principled decision-time planning algorithms that do not rely on public information, opening the door to sound and
    
[^78]: MEDNC：多集成深度神经网络用于COVID-19诊断

    MEDNC: Multi-ensemble deep neural network for COVID-19 diagnosis. (arXiv:2304.13135v1 [eess.IV])

    [http://arxiv.org/abs/2304.13135](http://arxiv.org/abs/2304.13135)

    MEDNC是一种基于深度学习的COVID-19自动诊断方法，使用计算机断层扫描图像预测和诊断COVID-19，准确性高达98.79%和99.82%。同时，该方法还能适用于其他问题，如大脑肿瘤和血细胞数据的预测，具有广泛的适用性。

    

    新型冠状病毒病2019(COVID-19)已经在全球蔓延了三年，但是许多地区的医疗设施仍然不足。需要快速的COVID-19诊断来识别高危患者并最大化利用有限的医疗资源。因此，我们提出了深度学习框架MEDNC，用于使用计算机断层扫描(CT)图像进行 COVID-19 的自动预测和诊断。我们的模型是使用两个公共可用的COVID-19数据集进行训练的。模型的构建灵感源于迁移学习。结果表明，MEDNC极大地增强了COVID-19感染的检测能力，分别达到了98.79%和99.82%的准确率。我们在大脑肿瘤和血细胞数据集上测试了MEDNC，以展示我们的模型适用于广泛的问题。结果表明，我们提出的模型分别达到了99.39%和99.28%的准确率。这种COVID-19识别工具可以帮助优化医疗资源的利用。

    Coronavirus disease 2019 (COVID-19) has spread all over the world for three years, but medical facilities in many areas still aren't adequate. There is a need for rapid COVID-19 diagnosis to identify high-risk patients and maximize the use of limited medical resources. Motivated by this fact, we proposed the deep learning framework MEDNC for automatic prediction and diagnosis of COVID-19 using computed tomography (CT) images. Our model was trained using two publicly available sets of COVID-19 data. And it was built with the inspiration of transfer learning. Results indicated that the MEDNC greatly enhanced the detection of COVID-19 infections, reaching an accuracy of 98.79% and 99.82% respectively. We tested MEDNC on a brain tumor and a blood cell dataset to show that our model applies to a wide range of problems. The outcomes demonstrated that our proposed models attained an accuracy of 99.39% and 99.28%, respectively. This COVID-19 recognition tool could help optimize healthcare reso
    
[^79]: 有向链生成对抗网络

    Directed Chain Generative Adversarial Networks. (arXiv:2304.13131v1 [cs.LG])

    [http://arxiv.org/abs/2304.13131](http://arxiv.org/abs/2304.13131)

    本文提出了有向链生成对抗网络（DC-GANs），使用邻域过程作为关键步骤生成同样分布的多模态时间序列数据。

    

    现实世界中的数据分布通常是多模态的，例如描述社区意见分歧、神经元的间隔分布以及振荡器的固有频率的数据。生成多模态分布的现实世界数据已成为现有生成对抗网络（GAN）的挑战。例如，将神经随机微分方程（Neural SDEs）视为无限维GAN的情况下，它们已经展示了成功的性能，主要用于生成单峰时间序列数据。在本文中，我们提出了一种新的时间序列生成器——有向链GAN（DC-GAN），它将时间序列数据集（称为有向链的邻域过程或输入）插入具有分布约束的有向链SDE的漂移和扩散系数中。DC-GAN可以生成与邻域过程相同分布的新时间序列，而邻域过程将提供学习和生成多模态分布数据的关键步骤。

    Real-world data can be multimodal distributed, e.g., data describing the opinion divergence in a community, the interspike interval distribution of neurons, and the oscillators natural frequencies. Generating multimodal distributed real-world data has become a challenge to existing generative adversarial networks (GANs). For example, neural stochastic differential equations (Neural SDEs), treated as infinite-dimensional GANs, have demonstrated successful performance mainly in generating unimodal time series data. In this paper, we propose a novel time series generator, named directed chain GANs (DC-GANs), which inserts a time series dataset (called a neighborhood process of the directed chain or input) into the drift and diffusion coefficients of the directed chain SDEs with distributional constraints. DC-GANs can generate new time series of the same distribution as the neighborhood process, and the neighborhood process will provide the key step in learning and generating multimodal di
    
[^80]: 利用机器学习辅助的事件逐事件多普勒修正进行快速、热的异位同位素的高精度光谱学研究

    Precision Spectroscopy of Fast, Hot Exotic Isotopes Using Machine Learning Assisted Event-by-Event Doppler Correction. (arXiv:2304.13120v1 [nucl-ex])

    [http://arxiv.org/abs/2304.13120](http://arxiv.org/abs/2304.13120)

    提出一种新的实验方案，结合机器学习辅助的事件逐事件多普勒修正，实现对快速、热的异位同位素进行高精度的激光光谱学研究，在极端温度下仍能实现kHz级别的不确定性。

    

    我们提出了一个实验方案,用于在快速的异位同位素上进行敏感、高精度的激光光谱学研究。通过在电场内诱导原子的逐步共振电离,然后检测离子和相应的电子,可以进行时间和位置敏感的结果粒子的测量。使用混合密度网络 (MDN),我们可以利用这些信息对单个原子的初始能量进行预测,从而在事件逐事件的基础上应用多普勒修正所观察到的跃迁频率。我们对该提议的实验方案进行数值模拟,并表明可以实现kHz级别的不确定性,对在极端温度 ($> 10^8$ K) 下产生的离子束进行，在能量分散最大可达10 keV和速度分布不均匀的情况下。能够直接在高能束上进行飞行谱学研究,为研究短寿命的离子的物理和化学性质提供了独特的机会。

    We propose an experimental scheme for performing sensitive, high-precision laser spectroscopy studies on fast exotic isotopes. By inducing a step-wise resonant ionization of the atoms travelling inside an electric field and subsequently detecting the ion and the corresponding electron, time- and position-sensitive measurements of the resulting particles can be performed. Using a Mixture Density Network (MDN), we can leverage this information to predict the initial energy of individual atoms and thus apply a Doppler correction of the observed transition frequencies on an event-by-event basis. We conduct numerical simulations of the proposed experimental scheme and show that kHz-level uncertainties can be achieved for ion beams produced at extreme temperatures ($> 10^8$ K), with energy spreads as large as $10$ keV and non-uniform velocity distributions. The ability to perform in-flight spectroscopy, directly on highly energetic beams, offers unique opportunities to studying short-lived i
    
[^81]: 利用Transformer进行光学系统非线性通道补偿的应用

    Application of Transformers for Nonlinear Channel Compensation in Optical Systems. (arXiv:2304.13119v1 [cs.IT])

    [http://arxiv.org/abs/2304.13119](http://arxiv.org/abs/2304.13119)

    本文提出了一种利用Transformer进行光学系统非线性通道补偿的新方法，这种方法利用了Transformer的记忆关注能力和并行结构，实现了高效的非线性补偿。同时，作者还提出了一种物理学信息掩码，用于降低计算复杂度。

    

    本文介绍了一种基于Transformer的新型非线性通道均衡方法，用于相干长距离传输。我们证明，由于其能够直接关注一系列符号之间的记忆，因此Transformer可以与并行结构有效地配合使用。我们展示了一个编码器部分的Transformer实现，用于非线性均衡，并分析了其在不同超参数范围内的性能。通过在每次迭代中处理符号块，并仔细选择要一起处理的编码器输出子集，可以实现高效的非线性补偿。我们还提出了一种基于非线性扰动理论的物理学信息掩码，用于降低Transformer非线性均衡的计算复杂度。

    In this paper, we introduce a new nonlinear channel equalization method for the coherent long-haul transmission based on Transformers. We show that due to their capability to attend directly to the memory across a sequence of symbols, Transformers can be used effectively with a parallelized structure. We present an implementation of encoder part of Transformer for nonlinear equalization and analyze its performance over a wide range of different hyper-parameters. It is shown that by processing blocks of symbols at each iteration and carefully selecting subsets of the encoder's output to be processed together, an efficient nonlinear compensation can be achieved. We also propose the use of a physic-informed mask inspired by nonlinear perturbation theory for reducing the computational complexity of Transformer nonlinear equalization.
    
[^82]: 有限CSI下的THz波束搜索的联邦深度强化学习

    Federated Deep Reinforcement Learning for THz-Beam Search with Limited CSI. (arXiv:2304.13109v1 [cs.AI])

    [http://arxiv.org/abs/2304.13109](http://arxiv.org/abs/2304.13109)

    本文提出了一个联邦深度强化学习的方法，可以用于在有限CSI的情况下迅速进行THz波束搜索，以克服THz信号的传播衰减。

    

    THz通信是下一代无线网络中高数据率的有前途的技术，然而其严重的传播衰减限制了其实际应用。本文提出了一种联邦深度强化学习（FDRL）的新方法，用于在运营商网络中，由边缘服务器协调的多个基站（BS）迅速执行THz波束搜索。所有BS都进行基于DDPG（深度确定性策略梯度）的DRL以获得具有有限信道状态信息（CSI）的THz波束成形策略。他们使用隐藏信息更新他们的DDPG模型，以减轻跨小区干扰。

    Terahertz (THz) communication with ultra-wide available spectrum is a promising technique that can achieve the stringent requirement of high data rate in the next-generation wireless networks, yet its severe propagation attenuation significantly hinders its implementation in practice. Finding beam directions for a large-scale antenna array to effectively overcome severe propagation attenuation of THz signals is a pressing need. This paper proposes a novel approach of federated deep reinforcement learning (FDRL) to swiftly perform THz-beam search for multiple base stations (BSs) coordinated by an edge server in a cellular network. All the BSs conduct deep deterministic policy gradient (DDPG)-based DRL to obtain THz beamforming policy with limited channel state information (CSI). They update their DDPG models with hidden information in order to mitigate inter-cell interference. We demonstrate that the cell network can achieve higher throughput as more THz CSI and hidden neurons of DDPG a
    
[^83]: 基于WiFi CSI的无设备多房间人体存在检测的时间选择性循环神经网络

    Time-Selective RNN for Device-Free Multi-Room Human Presence Detection Using WiFi CSI. (arXiv:2304.13107v1 [cs.AI])

    [http://arxiv.org/abs/2304.13107](http://arxiv.org/abs/2304.13107)

    这篇论文提出了一种使用基于WiFi信道状态信息提取人体移动和空间特征的无设备多房间人体存在检测系统，能够通过时间-selective特征提取算法区分有直觉视线路径阻塞和无视线路径阻塞的情况。

    

    人类存在检测是各种应用的重要技术，包括家居自动化、安全和医疗保健。虽然传统上采用基于摄像机的系统来实现这一目的，但会引发隐私问题。为了解决这个问题，最近的研究探讨了利用商用WiFi接入点提供的信道状态信息(CSI)方法，提供详细的信道特征。在本论文中，我们提出了一个基于时间选择性条件双特征提取递归网络(TCD-FERN)的无设备多房间人体存在检测系统。我们的系统旨在使用动态和静态(DaS)数据预处理技术，在条件人体特征下捕捉重要的时间特征，提取人的移动和空间特征，并区分有直接视线路径阻塞和无视线路径阻塞的情况。为了减少房间隔断造成的特征衰减问题，我们使用基于 LSTM 的 NCoV-DaS 技术。

    Human presence detection is a crucial technology for various applications, including home automation, security, and healthcare. While camera-based systems have traditionally been used for this purpose, they raise privacy concerns. To address this issue, recent research has explored the use of channel state information (CSI) approaches that can be extracted from commercial WiFi access points (APs) and provide detailed channel characteristics. In this thesis, we propose a device-free human presence detection system for multi-room scenarios using a time-selective conditional dual feature extract recurrent Network (TCD-FERN). Our system is designed to capture significant time features with the condition on current human features using a dynamic and static (DaS) data preprocessing technique to extract moving and spatial features of people and differentiate between line-of-sight (LoS) path blocking and non-blocking cases. To mitigate the feature attenuation problem caused by room partitions,
    
[^84]: 利用室内WiFi系统进行无设备穿墙存在检测的注意增强深度学习

    Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v1 [cs.LG])

    [http://arxiv.org/abs/2304.13105](http://arxiv.org/abs/2304.13105)

    本文提出了一种利用WiFi信号进行人员存在检测的新系统，采用了关注机制和双向LSTM网络来提高准确性，并证明了其在现实场景中的稳健性。

    

    在室内环境中准确检测人员存在对于各种应用非常重要，例如能源管理和安全。本文提出了一种利用WiFi信号的通道状态信息（CSI）进行人员存在检测的新系统。我们的系统名为注意力增强深度学习（ALPD），采用关注机制从CSI数据中自动选择有信息量的子载波，并采用双向长短时记忆（LSTM）网络捕捉CSI中的时间依赖性。此外，我们利用一个静态特征来提高静态状态下人员存在检测的准确性。我们通过部署一对WiFi接入点（AP）来收集CSI数据集来评估所提出的ALPD系统，该系统进一步与几个基准进行比较。结果表明，我们的ALPD系统在准确性方面优于基准，特别是在存在干扰的情况下。此外，双向传输数据不会影响我们系统的性能，证明了其在现实场景中的稳健性。

    Accurate detection of human presence in indoor environments is important for various applications, such as energy management and security. In this paper, we propose a novel system for human presence detection using the channel state information (CSI) of WiFi signals. Our system named attention-enhanced deep learning for presence detection (ALPD) employs an attention mechanism to automatically select informative subcarriers from the CSI data and a bidirectional long short-term memory (LSTM) network to capture temporal dependencies in CSI. Additionally, we utilize a static feature to improve the accuracy of human presence detection in static states. We evaluate the proposed ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI dataset, which is further compared with several benchmarks. The results demonstrate that our ALPD system outperforms the benchmarks in terms of accuracy, especially in the presence of interference. Moreover, bidirectional transmission data 
    
[^85]: 基于LSTM的微电网负荷预测对抗注入攻击的鲁棒性研究

    LSTM-based Load Forecasting Robustness Against Noise Injection Attack in Microgrid. (arXiv:2304.13104v1 [cs.LG])

    [http://arxiv.org/abs/2304.13104](http://arxiv.org/abs/2304.13104)

    本文研究了LSTM神经网络在微电网负荷预测中受到噪声注入攻击的鲁棒性。使用低通滤波器消除了攻击，以提高模型的性能。

    

    本文研究了LSTM神经网络在理想微电网中的电力负荷预测中对抗注入攻击的鲁棒性。研究了在不同信噪比下进行的黑盒高斯噪声攻击下LSTM模型的性能。假设攻击者只能访问LSTM模型的输入数据。结果表明，噪声攻击影响了LSTM模型的性能。对于正常预测，负荷预测的平均绝对误差（MAE）为0.047 MW，而对于信噪比为6 dB的高斯噪声插入，该值增加到了0.097 MW。为了使LSTM模型对噪声攻击具有鲁棒性，将最佳截止频率的低通滤波器应用于模型的输入以消除噪声攻击。该滤波器在低信噪比噪声的情况下表现更好，对于小噪声的效果较差。

    In this paper, we investigate the robustness of an LSTM neural network against noise injection attacks for electric load forecasting in an ideal microgrid. The performance of the LSTM model is investigated under a black-box Gaussian noise attack with different SNRs. It is assumed that attackers have just access to the input data of the LSTM model. The results show that the noise attack affects the performance of the LSTM model. The load prediction means absolute error (MAE) is 0.047 MW for a healthy prediction, while this value increases up to 0.097 MW for a Gaussian noise insertion with SNR= 6 dB. To robustify the LSTM model against noise attack, a low-pass filter with optimal cut-off frequency is applied at the model's input to remove the noise attack. The filter performs better in case of noise with lower SNR and is less promising for small noises.
    
[^86]: 用替代梯度训练的脉冲神经网络的表示学习探究

    Uncovering the Representation of Spiking Neural Networks Trained with Surrogate Gradient. (arXiv:2304.13098v1 [cs.LG])

    [http://arxiv.org/abs/2304.13098](http://arxiv.org/abs/2304.13098)

    本研究使用中心核对齐分析了使用替代梯度训练的脉冲神经网络（SNN）与传统人工神经网络（ANN）之间的表示相似性，并发现SNN中的时间维度提供了独特的表示学习能力。

    

    脉冲神经网络（SNN）由于其类生物特性和能量效率而被认为是下一代神经网络的候选者。最近的研究表明，使用替代梯度训练，SNN能够在图像识别任务中实现接近于最先进水平的性能。然而，关于SNN的一些基本问题尚未得到充分研究，如：使用替代梯度训练的SNN是否学习了不同于传统的人工神经网络（ANN）的表示学习？SNN中的时间维度是否提供了独特的表示学习能力？本文旨在通过使用中心核对齐（CKA）进行SNN和ANN之间的表示相似性分析来回答这些问题。我们首先分析网络的空间维度，包括宽度和深度。此外，我们发现SNN学习了周期模式的残差连接，从而使SNN的表示类似于ANN。此外，我们发现SNN中的时间维度提供了在ANN中不存在的独特的表示学习能力。

    Spiking Neural Networks (SNNs) are recognized as the candidate for the next-generation neural networks due to their bio-plausibility and energy efficiency. Recently, researchers have demonstrated that SNNs are able to achieve nearly state-of-the-art performance in image recognition tasks using surrogate gradient training. However, some essential questions exist pertaining to SNNs that are little studied: Do SNNs trained with surrogate gradient learn different representations from traditional Artificial Neural Networks (ANNs)? Does the time dimension in SNNs provide unique representation power? In this paper, we aim to answer these questions by conducting a representation similarity analysis between SNNs and ANNs using Centered Kernel Alignment (CKA). We start by analyzing the spatial dimension of the networks, including both the width and the depth. Furthermore, our analysis of residual connections shows that SNNs learn a periodic pattern, which rectifies the representations in SNNs to
    
[^87]: 使视频质量评估模型对位深具有稳健性

    Making Video Quality Assessment Models Robust to Bit Depth. (arXiv:2304.13092v1 [eess.IV])

    [http://arxiv.org/abs/2304.13092](http://arxiv.org/abs/2304.13092)

    本文提出了HDRMAX特征集，可以提高视频质量评估算法对于不同位深视频的质量预测性能，尤其在HDR视频上表现尤佳。

    

    我们引入了一种新的特征集，称为HDRMAX特征。当将其包含在为标准动态范围（SDR）视频设计的视频质量评估（VQA）算法中时，它们会对高动态范围（HDR）视频的失真进行敏感处理，这些失真未被这些算法充分考虑。虽然这些特征不仅限于HDR，并且还可以增强VQA模型对SDR内容的质量预测性能，但它们在HDR上尤为有效。HDRMAX特征通过增强自然视频统计模型（NVS）中从视觉影响视频的最亮和最暗的局部中提取的强大先验信息的可测性来捕捉通常由现有VQA模型难以考虑的失真。作为我们方法有效性的证明，我们展示了，在10位HDR数据库上，当前最先进的VQA模型的表现很差，但当使用HDRMAX特征时，其性能显著提高。

    We introduce a novel feature set, which we call HDRMAX features, that when included into Video Quality Assessment (VQA) algorithms designed for Standard Dynamic Range (SDR) videos, sensitizes them to distortions of High Dynamic Range (HDR) videos that are inadequately accounted for by these algorithms. While these features are not specific to HDR, and also augment the equality prediction performances of VQA models on SDR content, they are especially effective on HDR. HDRMAX features modify powerful priors drawn from Natural Video Statistics (NVS) models by enhancing their measurability where they visually impact the brightest and darkest local portions of videos, thereby capturing distortions that are often poorly accounted for by existing VQA models. As a demonstration of the efficacy of our approach, we show that, while current state-of-the-art VQA models perform poorly on 10-bit HDR databases, their performances are greatly improved by the inclusion of HDRMAX features when tested on
    
[^88]: 面向强化学习控制器的模型提取攻击

    Model Extraction Attacks Against Reinforcement Learning Based Controllers. (arXiv:2304.13090v1 [cs.LG])

    [http://arxiv.org/abs/2304.13090](http://arxiv.org/abs/2304.13090)

    本文研究了在强化学习控制器中的模型提取攻击，提出了一个两阶段算法，第一阶段使用侧信道信息进行估计候选项的识别，第二阶段使用轨迹优化算法选择最佳估计。

    

    我们提出了在网络-物理系统中的模型提取攻击问题，攻击者试图估计或获取系统的反馈控制器。提取或估计控制器可以给攻击者带来无与伦比的优势，因为它允许他们预测系统的未来控制动作并相应地计划攻击。因此，了解攻击者执行此类攻击的能力非常重要。在本文中，我们着重研究了使用强化学习算法训练深度神经网络控制器并用于控制随机系统的情况。我们扮演攻击者的角色，旨在估计这种未知的深度神经网络控制器，并提出了一个两阶段算法。在第一阶段，即离线阶段，攻击者使用关于RL奖励函数和系统动态的侧信道信息来识别一组候选的未知DNN估计。在第二阶段，也称为在线阶段，攻击者使用轨迹优化算法选择最佳估计。

    We introduce the problem of model-extraction attacks in cyber-physical systems in which an attacker attempts to estimate (or extract) the feedback controller of the system. Extracting (or estimating) the controller provides an unmatched edge to attackers since it allows them to predict the future control actions of the system and plan their attack accordingly. Hence, it is important to understand the ability of the attackers to perform such an attack. In this paper, we focus on the setting when a Deep Neural Network (DNN) controller is trained using Reinforcement Learning (RL) algorithms and is used to control a stochastic system. We play the role of the attacker that aims to estimate such an unknown DNN controller, and we propose a two-phase algorithm. In the first phase, also called the offline phase, the attacker uses side-channel information about the RL-reward function and the system dynamics to identify a set of candidate estimates of the unknown DNN. In the second phase, also ca
    
[^89]: 目标很重要：理解自监督目标对视觉Transformer表示形式的影响

    Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations. (arXiv:2304.13089v1 [cs.LG])

    [http://arxiv.org/abs/2304.13089](http://arxiv.org/abs/2304.13089)

    本文分析了联合嵌入学习和重建学习两种自监督学习视觉transformers的目标对所学表示的影响及其转换性能差异，发现联合嵌入学习特征更利于线性探测转移分类，进而提供了该领域未来研究的方向。

    

    联合嵌入学习(SimCLR、MoCo、DINO等)和重建学习(BEiT、SimMIM、MAE等)是自监督学习视觉transformers的两种主要范例，但它们在转换性能上有很大差异。本文旨在通过分析这些目标对所学表示的结构和可转移性的影响来解释这些差异。我们的分析揭示了重建学习特征与联合嵌入学习特征相比较显著的不同，并且即使在不同架构下也能通过类似目标来训练。这些差异早在网络的早期就产生了，并且主要受到注意力和归一化层的驱动。我们发现，联合嵌入特征产生更好的线性探测转移分类，因为不同的目标驱动不同的信息分布和不变性在所学表示中。本文分析提供了自监督学习目标设计的见解，并为未来的研究提供了方向。

    Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading paradigms for self-supervised learning of vision transformers, but they differ substantially in their transfer performance. Here, we aim to explain these differences by analyzing the impact of these objectives on the structure and transferability of the learned representations. Our analysis reveals that reconstruction-based learning features are significantly dissimilar to joint-embedding based learning features and that models trained with similar objectives learn similar features even across architectures. These differences arise early in the network and are primarily driven by attention and normalization layers. We find that joint-embedding features yield better linear probe transfer for classification because the different objectives drive different distributions of information and invariances in the learned representation. These differences expl
    
[^90]: 新兴技术的组织治理：AI在医疗保健中的应用

    Organizational Governance of Emerging Technologies: AI Adoption in Healthcare. (arXiv:2304.13081v1 [cs.AI])

    [http://arxiv.org/abs/2304.13081](http://arxiv.org/abs/2304.13081)

    该研究通过与美国主要医疗保健系统的领导人和相关领域的主要知情人合作，制定了AI在医疗保健中的组织治理框架，包括关键控制点和决策标准，为卫生系统领导人做出更加明智的决策提供了支持。

    

    私营和公共部门的结构和规范精细化了新兴技术的实际应用。在医疗保健中，尽管出现了大量的AI采用方式，但是其使用和整合周围的组织治理往往被认为不可行。健康AI合作伙伴关系（HAIP）旨在通过此研究更好地定义医疗保健中AI系统的充分组织治理要求，并支持卫生系统领导人做出更加明智的决策。要达到这个目标，我们首先确定了AI在医疗保健中采用的标准如何易于使用和高效运作。然后，我们在特定的卫生系统中，绘制出实际机构采用AI技术的具体决策点。在实践中，我们通过与美国主要医疗保健系统的领导人和相关领域的主要知情人合作，实现了这一目标。通过这种合作，我们制定了AI在医疗保健中的组织治理框架，其中包括关键控制点和决策标准。

    Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working w
    
[^91]: iMixer: 分层Hopfield网络暗示了可逆、隐式和迭代的MLP-Mixer

    iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer. (arXiv:2304.13061v1 [cs.LG])

    [http://arxiv.org/abs/2304.13061](http://arxiv.org/abs/2304.13061)

    本文推广了 Hopkins field 分层网络，并介绍了 iMixer，MLP-Mixer 模型的新概括，不同于普通的前馈网络，iMixer 涉及到从输出到输入的传播的 MLP 层，被特征化为一个可逆、隐式、迭代的 mixing block。

    

    在过去的几年中，Transformer在计算机视觉领域的成功促使寻找可以与之竞争的许多替代模型，如MLP-Mixer。尽管这些模型的引入偏差较弱，但它们的表现可与研究较多的卷积神经网络相媲美。最近对现代Hopfield网络的研究表明了某些基于能量的关联记忆模型与Transformer或MLP-Mixer之间的对应关系，并揭示了Transformer类型架构设计的理论背景。在本文中，我们将该对应关系推广到最近引入的分层Hopfield网络，并找到了iMixer，这是MLP-Mixer模型的新的概括。与普通的前馈神经网络不同，iMixer涉及从输出侧向输入侧传播的MLP层。我们将该模块特征化为可逆、隐式和迭代混合模块的一个例子。我们通过各种任务评估了模型的性能。

    In the last few years, the success of Transformers in computer vision has stimulated the discovery of many alternative models that compete with Transformers, such as the MLP-Mixer. Despite their weak induced bias, these models have achieved performance comparable to well-studied convolutional neural networks. Recent studies on modern Hopfield networks suggest the correspondence between certain energy-based associative memory models and Transformers or MLP-Mixer, and shed some light on the theoretical background of the Transformer-type architectures design. In this paper we generalize the correspondence to the recently introduced hierarchical Hopfield network, and find iMixer, a novel generalization of MLP-Mixer model. Unlike ordinary feedforward neural networks, iMixer involves MLP layers that propagate forward from the output side to the input side. We characterize the module as an example of invertible, implicit, and iterative mixing module. We evaluate the model performance with var
    
[^92]: GULP: 太阳能驱动的智能垃圾分类桶，具备短信通知和机器学习图像处理功能

    GULP: Solar-Powered Smart Garbage Segregation Bins with SMS Notification and Machine Learning Image Processing. (arXiv:2304.13040v1 [cs.LG])

    [http://arxiv.org/abs/2304.13040](http://arxiv.org/abs/2304.13040)

    本研究利用太阳能驱动智能垃圾分类桶，并通过短信通知和机器学习图像处理功能实现废物分类，提高废物管理效率，同时使终端用户更加关注环保。

    

    本研究旨在建立一个智能垃圾桶，将固体废物分类到其相应的容器中。通过利用可再生的太阳能源，使终端用户对废物管理过程更加感兴趣，当智能垃圾桶需要卸载时通知相关工作人员，鼓励使用环境友好型智能垃圾桶。研究人员采用敏捷开发方法，因为它可以使团队成功管理工作量，创建最高质量的产品，并在分配预算内完成工作。六个基本阶段是规划、设计、开发、测试、发布和反馈。通过ISO/IEC 25010评估提供的总体质量测试结果，得出积极的结论。整体平均值为4.55，口头解释为优秀。此外，该应用程序还可以利用太阳能源独立运行。用户可以通过其有趣的机制享受整个废物处理过程。

    This study intends to build a smartbin that segregates solid waste into its respective bins. To make the waste management process more interesting for the end-users; to notify the utility staff when the smart bin needs to be unloaded; to encourage an environment-friendly smart bin by utilizing renewable solar energy source. The researchers employed an Agile Development approach because it enables teams to manage their workloads successfully and create the highest-quality product while staying within their allocated budget. The six fundamental phases are planning, design, development, test, release, and feedback. The Overall quality testing result that was provided through the ISO/IEC 25010 evaluation which concludes a positive outcome. The overall average was 4.55, which is verbally interpreted as excellent. Additionally, the application can also independently run with its solar energy source. Users were able to enjoy the whole process of waste disposal through its interesting mechanis
    
[^93]: 针对树莓派优化深度学习模型

    Optimizing Deep Learning Models For Raspberry Pi. (arXiv:2304.13039v1 [eess.SY])

    [http://arxiv.org/abs/2304.13039](http://arxiv.org/abs/2304.13039)

    针对树莓派优化深度学习模型包括修剪技术和模型参数结构优化，以适应其硬件特点并提高能效。

    

    深度学习模型在计算机视觉、自然语言处理和语音识别等领域越来越受欢迎。但这类模型需要大量的计算资源，使得在低功耗设备上运行（如树莓派）变得具有挑战性。针对这个问题的解决方案包括修剪（pruning）技术和优化模型以适合树莓派等硬件架构。在训练期间或训练后修剪可以减小模型大小，让模型更高效。优化模型则包括调整模型参数和结构适应树莓派硬件特点，例如树莓派的CPU和GPU，并通过最小化计算量以实现能耗的优化等方面。

    Deep learning models have become increasingly popular for a wide range of applications, including computer vision, natural language processing, and speech recognition. However, these models typically require large amounts of computational resources, making them challenging to run on low-power devices such as the Raspberry Pi. One approach to addressing this challenge is to use pruning techniques to reduce the size of the deep learning models. Pruning involves removing unimportant weights and connections from the model, resulting in a smaller and more efficient model. Pruning can be done during training or after the model has been trained. Another approach is to optimize the deep learning models specifically for the Raspberry Pi architecture. This can include optimizing the model's architecture and parameters to take advantage of the Raspberry Pi's hardware capabilities, such as its CPU and GPU. Additionally, the model can be optimized for energy efficiency by minimizing the amount of c
    
[^94]: VeML：大规模高维数据的端到端机器学习生命周期

    VeML: An End-to-End Machine Learning Lifecycle for Large-scale and High-dimensional Data. (arXiv:2304.13037v1 [cs.LG])

    [http://arxiv.org/abs/2304.13037](http://arxiv.org/abs/2304.13037)

    VeML是一种专门用于大规模高维数据的端到端机器学习生命周期的版本管理系统，在解决生命周期高成本问题、数据相似性计算和数据模式分析等关键问题方面表现出色。

    

    端到端的机器学习生命周期包含许多迭代过程，从数据准备和机器学习模型设计到模型训练，再到部署训练好的模型用于推理。当构建一个机器学习问题的端到端生命周期时，必须设计和执行许多机器学习管道，这会产生大量的生命周期版本。因此，本文介绍了VeML，一种专门用于端到端机器学习生命周期的版本管理系统。我们的系统解决了其他系统没有解决的几个关键问题。首先，我们解决了构建机器学习生命周期的高成本问题，特别是针对大规模和高维数据集。我们通过提议将在我们系统中管理的类似数据集的生命周期转移到新的训练数据来解决这个问题。我们设计了一种基于核心集的算法，可以有效地计算大规模高维数据的相似性。另一个关键问题是由于训练数据和测试数据的差异而导致模型准确性下降。我们开发了一种数据模式分析方法来检测先前使用的数据和新数据之间的差异。我们的系统使用户可以自定义机器学习生命周期工作流，并将生命周期的各个阶段与其API连接起来，作为用户运行自定义代码的桥梁。 VeML已应用于处理多个真实世界的机器学习问题，结果证明了我们的系统的有效性。

    An end-to-end machine learning (ML) lifecycle consists of many iterative processes, from data preparation and ML model design to model training and then deploying the trained model for inference. When building an end-to-end lifecycle for an ML problem, many ML pipelines must be designed and executed that produce a huge number of lifecycle versions. Therefore, this paper introduces VeML, a Version management system dedicated to end-to-end ML Lifecycle. Our system tackles several crucial problems that other systems have not solved. First, we address the high cost of building an ML lifecycle, especially for large-scale and high-dimensional dataset. We solve this problem by proposing to transfer the lifecycle of similar datasets managed in our system to the new training data. We design an algorithm based on the core set to compute similarity for large-scale, high-dimensional data efficiently. Another critical issue is the model accuracy degradation by the difference between training data a
    
[^95]: SmartChoices: 学习实现增强软件

    SmartChoices: Augmenting Software with Learned Implementations. (arXiv:2304.13033v1 [cs.SE])

    [http://arxiv.org/abs/2304.13033](http://arxiv.org/abs/2304.13033)

    SmartChoices 提出了一种将机器学习与现有软件系统轻松、安全、有效地结合的新方法。

    

    我们正处于机器学习的黄金时代。强大的模型正在训练中，远比仅使用传统软件工程方法更好地执行许多任务。然而，将这些模型开发并部署到现有软件系统中仍然很困难。在本文中，我们提出了 SmartChoices，一种将机器学习轻松、安全、有效地结合到成熟软件堆栈中的新方法。我们解释了总体设计理念，并展示了使用 SmartChoices 在大型工业系统中的案例研究。

    We are living in a golden age of machine learning. Powerful models are being trained to perform many tasks far better than is possible using traditional software engineering approaches alone. However, developing and deploying those models in existing software systems remains difficult. In this paper we present SmartChoices, a novel approach to incorporating machine learning into mature software stacks easily, safely, and effectively. We explain the overall design philosophy and present case studies using SmartChoices within large scale industrial systems.
    
[^96]: 一个统一的主动学习框架，用于注释图形数据并应用于软件代码性能预测

    A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction. (arXiv:2304.13032v1 [cs.SE])

    [http://arxiv.org/abs/2304.13032](http://arxiv.org/abs/2304.13032)

    提出了一个针对软件性能预测的主动学习框架，通过将源代码解析成流增强抽象语法树图形式，构造各种无监督和有监督的图嵌入，进行主动学习，实现对未标注数据的高效利用，并比现有方法更加优越。

    

    大多数机器学习和数据分析应用程序，包括软件系统中的性能工程，需要大量注释和标记数据，这些可能事先并不可用。获取注释通常需要显着的时间、精力和计算资源，这使得任务变得具有挑战性。我们开发了一个统一的主动学习框架，专门针对软件性能预测来解决这个任务。我们从将源代码解析成抽象语法树（AST）开始，然后用数据和控制流边来增强它。然后，我们将源代码的树形表示转换为流增强抽象语法树图（FA-AST）表示法。基于图形表示，我们将各种图嵌入（无监督和有监督）构造成一个潜在空间。鉴于这样的嵌入，该框架变得任务不可知，因为可以使用任何回归方法和适用于回归的查询策略来执行主动学习。在该框架内，我们引入并评估了几种用于软件性能预测的主动学习查询策略和回归算法，证明了我们的方法优于现有方法。

    Most machine learning and data analytics applications, including performance engineering in software systems, require a large number of annotations and labelled data, which might not be available in advance. Acquiring annotations often requires significant time, effort, and computational resources, making it challenging. We develop a unified active learning framework, specializing in software performance prediction, to address this task. We begin by parsing the source code to an Abstract Syntax Tree (AST) and augmenting it with data and control flow edges. Then, we convert the tree representation of the source code to a Flow Augmented-AST graph (FA-AST) representation. Based on the graph representation, we construct various graph embeddings (unsupervised and supervised) into a latent space. Given such an embedding, the framework becomes task agnostic since active learning can be performed using any regression method and query strategy suited for regression. Within this framework, we in
    
[^97]: Awesome-META+: 元学习研究与学习平台

    Awesome-META+: Meta-Learning Research and Learning Platform. (arXiv:2304.12921v1 [cs.LG])

    [http://arxiv.org/abs/2304.12921](http://arxiv.org/abs/2304.12921)

    Awesome-META+是一个元学习框架集成和学习平台，旨在提供完整可靠的元学习框架应用和面向初学者的学习材料，进而促进元学习的发展并将其从小众领域转化为主流的研究方向。

    

    人工智能已经在经济、产业、教育等各个领域产生了深远的影响，但还存在诸多限制。元学习，也称为“学习如何学习”，为通用人工智能提供了突破目前瓶颈的机会。然而，元学习起步较晚，相比CV、NLP等领域，项目数量较少。每次部署都需要大量的经验去配置环境、调试代码甚至重写，而且框架之间相对孤立。此外，目前针对元学习的专门平台和面向初学者的学习材料相对较少，门槛相对较高。基于此，Awesome-META+提出了一个元学习框架集成和学习平台，旨在解决上述问题并提供完整可靠的元学习框架应用和学习平台。该项目旨在促进元学习的发展，并将其从一个小众领域转化为一个主流的研究方向。

    Artificial intelligence technology has already had a profound impact in various fields such as economy, industry, and education, but still limited. Meta-learning, also known as "learning to learn", provides an opportunity for general artificial intelligence, which can break through the current AI bottleneck. However, meta learning started late and there are fewer projects compare with CV, NLP etc. Each deployment requires a lot of experience to configure the environment, debug code or even rewrite, and the frameworks are isolated. Moreover, there are currently few platforms that focus exclusively on meta-learning, or provide learning materials for novices, for which the threshold is relatively high. Based on this, Awesome-META+, a meta-learning framework integration and learning platform is proposed to solve the above problems and provide a complete and reliable meta-learning framework application and learning platform. The project aims to promote the development of meta-learning and t
    
[^98]: 学习鲁棒的深度平衡模型

    Learning Robust Deep Equilibrium Models. (arXiv:2304.12707v1 [cs.LG])

    [http://arxiv.org/abs/2304.12707](http://arxiv.org/abs/2304.12707)

    本论文提出一种名为LyaDEQ的鲁棒DEQ模型，通过Lyapunov理论提供了保证的稳定性以抵抗微小的初始扰动，并在不同的固定点之间加入全连接层以避免不良对抗性防御。

    

    深度平衡(DEQ)模型已成为深度学习中一种有前途的隐式层模型，它通过解决单个非线性层的固定点来放弃了传统深度。尽管这些模型很成功，但对于这些模型的固定点的稳定性仍然知之甚少。最近，将Lyapunov理论应用于另一种类型的隐式层模型——神经ODE，可以赋予其对抗鲁棒性。通过将DEQ模型视为非线性动态系统，我们提出了一种名为LyaDEQ的鲁棒DEQ模型，通过Lyapunov理论提供了保证的稳定性。我们方法的关键是确保DEQ模型的固定点是Lyapunov稳定的，这使得LyaDEQ模型能够抵抗微小的初始扰动。为了避免由于Lyapunov稳定的固定点彼此靠近而导致的不良对抗性防御，我们在Lyapunov稳定性模块之后加入了一个正交的全连接层，以分离不同的固定点。我们在各种基准上评估了LyaDEQ模型。

    Deep equilibrium (DEQ) models have emerged as a promising class of implicit layer models in deep learning, which abandon traditional depth by solving for the fixed points of a single nonlinear layer. Despite their success, the stability of the fixed points for these models remains poorly understood. Recently, Lyapunov theory has been applied to Neural ODEs, another type of implicit layer model, to confer adversarial robustness. By considering DEQ models as nonlinear dynamic systems, we propose a robust DEQ model named LyaDEQ with guaranteed provable stability via Lyapunov theory. The crux of our method is ensuring the fixed points of the DEQ models are Lyapunov stable, which enables the LyaDEQ models to resist the minor initial perturbations. To avoid poor adversarial defense due to Lyapunov-stable fixed points being located near each other, we add an orthogonal fully connected layer after the Lyapunov stability module to separate different fixed points. We evaluate LyaDEQ models on se
    
[^99]: 使用物理信息可逆神经网络进行高效贝叶斯推断的研究

    Efficient Bayesian inference using physics-informed invertible neural networks for inverse problems. (arXiv:2304.12541v1 [math.NA])

    [http://arxiv.org/abs/2304.12541](http://arxiv.org/abs/2304.12541)

    本文提出了一种使用物理信息可逆神经网络(PI-INN)解决贝叶斯反问题的新方法，该方法可以高效地进行抽样和准确的密度评估。研究通过残差项和独立性损失项确保了INN输出的统计独立性，并在多项实验中证明了其有效性和准确性。

    

    本文提出了一种利用物理信息可逆神经网络(PI-INN)解决贝叶斯反问题的新方法。PI-INN的结构包括两个子网络：一个可逆神经网络(INN)和一个神经基础网络(NB-Net)。通过NB-Net帮助建立参数输入和INN输出之间的可逆映射，以提供可行的后验分布估计，从而实现高效的抽样和准确的密度评估。此外，PI-INN的损失函数包含两个部分：一部分是基于残差的物理信息损失项，另一部分是新的独立性损失项。提出的独立性损失项可以高斯化随机潜变量，并通过有效利用估计的密度函数，确保INN输出的两个部分之间的统计独立性。通过进行反向运动学和反向扩散等多项实验验证了所提出的PI-INN的有效性和准确性。

    In the paper, we propose a novel approach for solving Bayesian inverse problems with physics-informed invertible neural networks (PI-INN). The architecture of PI-INN consists of two sub-networks: an invertible neural network (INN) and a neural basis network (NB-Net). The invertible map between the parametric input and the INN output with the aid of NB-Net is constructed to provide a tractable estimation of the posterior distribution, which enables efficient sampling and accurate density evaluation. Furthermore, the loss function of PI-INN includes two components: a residual-based physics-informed loss term and a new independence loss term. The presented independence loss term can Gaussianize the random latent variables and ensure statistical independence between two parts of INN output by effectively utilizing the estimated density function. Several numerical experiments are presented to demonstrate the efficiency and accuracy of the proposed PI-INN, including inverse kinematics, inver
    
[^100]: ThreatCrawl：基于BERT的网络安全焦点爬虫

    ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain. (arXiv:2304.11960v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2304.11960](http://arxiv.org/abs/2304.11960)

    本文提出了一种基于BERT的焦点爬虫ThreatCrawl，使用主题建模和关键词提取技术来筛选出最可能包含有价值CTI信息的网页。

    

    可公开获取的信息对于网络威胁情报（CTI）来说包含有价值的信息。这可以用于预防已经在其他系统上发生的攻击。但是，虽然有不同的标准来交流这些信息，但很多信息是以非标准化的方式在文章或博客帖子中共享的。手动浏览多个在线门户和新闻页面以发现新威胁并提取它们是一项耗时的任务。为了自动化这个扫描过程的一部分，多篇论文提出了使用自然语言处理（NLP）从文档中提取威胁指示器（IOCs）的提取器。然而，虽然这已经解决了从文档中提取信息的问题，但很少考虑搜索这些文档。本文提出了一种新的焦点爬虫ThreatCrawl，它使用双向编码器表示（BERT）搜索网络安全领域中的相关文档。ThreatCrawl使用主题建模和关键词提取技术来识别相关网站和网页，然后应用基于BERT的分类器来优先考虑最可能包含有价值CTI信息的网页。

    Publicly available information contains valuable information for Cyber Threat Intelligence (CTI). This can be used to prevent attacks that have already taken place on other systems. Ideally, only the initial attack succeeds and all subsequent ones are detected and stopped. But while there are different standards to exchange this information, a lot of it is shared in articles or blog posts in non-standardized ways. Manually scanning through multiple online portals and news pages to discover new threats and extracting them is a time-consuming task. To automize parts of this scanning process, multiple papers propose extractors that use Natural Language Processing (NLP) to extract Indicators of Compromise (IOCs) from documents. However, while this already solves the problem of extracting the information out of documents, the search for these documents is rarely considered. In this paper, a new focused crawler is proposed called ThreatCrawl, which uses Bidirectional Encoder Representations 
    
[^101]: 无监督机器学习用于分类周期性超结构中波的约束

    Unsupervised Machine Learning to Classify the Confinement of Waves in Periodic Superstructures. (arXiv:2304.11901v2 [physics.optics] UPDATED)

    [http://arxiv.org/abs/2304.11901](http://arxiv.org/abs/2304.11901)

    通过对比聚类算法和直接应用方法，本研究发现应先采用直接比例方法找到正确的约束维度集，再用聚类来细化结果，同时基于模型的算法优于标准的k-means++聚类算法。

    

    我们利用无监督机器学习增强了我们最近提出的波约束分析的比例方法的准确性[1]。我们采用标准的k-means ++算法以及我们自己的基于模型的算法。我们研究了簇有效性指数作为一种手段来找到正确的约束维度号，以用作聚类算法的输入。随后，我们分析了两种聚类算法相对于不使用聚类的直接应用比例方法的性能。我们发现聚类方法提供了更具物理意义的结果，但可能难以识别正确的约束维度集。我们得出结论，最准确的结果是先应用直接比例方法找到正确的约束维度集，然后再采用聚类来细化结果。此外，我们的基于模型的算法优于标准的k-means ++聚类。

    We employ unsupervised machine learning to enhance the accuracy of our recently presented scaling method for wave confinement analysis [1]. We employ the standard k-means++ algorithm as well as our own model-based algorithm. We investigate cluster validity indices as a means to find the correct number of confinement dimensionalities to be used as an input to the clustering algorithms. Subsequently, we analyze the performance of the two clustering algorithms when compared to the direct application of the scaling method without clustering. We find that the clustering approach provides more physically meaningful results, but may struggle with identifying the correct set of confinement dimensionalities. We conclude that the most accurate outcome is obtained by first applying the direct scaling to find the correct set of confinement dimensionalities and subsequently employing clustering to refine the results. Moreover, our model-based algorithm outperforms the standard k-means++ clustering.
    
[^102]: 基于偏相关的深度视觉表示学习用于图像分类

    Learning Partial Correlation based Deep Visual Representation for Image Classification. (arXiv:2304.11597v1 [cs.CV])

    [http://arxiv.org/abs/2304.11597](http://arxiv.org/abs/2304.11597)

    本文提出了一种基于偏相关的深度视觉表示学习方法，解决了使用协方差矩阵表征相关性在存在混淆效应时的误导问题。

    

    基于协方差矩阵的视觉表示已经证明了其在图像分类中的有效性，通过对卷积特征映射中不同通道之间的成对相关性进行建模。然而，如果存在另一个通道与感兴趣的两个通道相关，则成对相关性将变得误导人，导致“混淆”效应。针对这种情况，应该估计“偏相关”，以消除混淆效应。然而，可靠地估计偏相关需要解决一个对称正定矩阵优化问题，即稀疏逆协方差矩阵估计（SICE）。如何将此过程融入CNN中仍然是一个开放问题。在这项工作中，我们将SICE制定为CNN的一个新结构层。为确保端到端的可训练性，我们开发了一种迭代方法，在前向和后向传播步骤中解决上述矩阵优化问题。我们的工作获得了基于偏相关的深度视觉表示。

    Visual representation based on covariance matrix has demonstrates its efficacy for image classification by characterising the pairwise correlation of different channels in convolutional feature maps. However, pairwise correlation will become misleading once there is another channel correlating with both channels of interest, resulting in the ``confounding'' effect. For this case, ``partial correlation'' which removes the confounding effect shall be estimated instead. Nevertheless, reliably estimating partial correlation requires to solve a symmetric positive definite matrix optimisation, known as sparse inverse covariance estimation (SICE). How to incorporate this process into CNN remains an open issue. In this work, we formulate SICE as a novel structured layer of CNN. To ensure end-to-end trainability, we develop an iterative method to solve the above matrix optimisation during forward and backward propagation steps. Our work obtains a partial correlation based deep visual representa
    
[^103]: 利用保序机制提高机器学习和人工智能会议的同行评审

    The Isotonic Mechanism for Exponential Family Estimation. (arXiv:2304.11160v1 [math.ST])

    [http://arxiv.org/abs/2304.11160](http://arxiv.org/abs/2304.11160)

    本文利用扩展的保序机制，将其应用于指数族分布以提高同行评审的质量，并发现作者的同行评分可以较准确地在不需要知道具体分布情况下进行调整。

    

    本文致力于扩展保序机制，将其应用于指数族分布以提高同行评审的质量。该机制可生成与原始评分接近的调整分数，并符合作者指定的排名要求，得到广泛的指数族分布应用，而且不需要知道具体的分布形式。研究表明，在一定的指数族分布下，如果作者的效用函数采用简单的凸可加函数，则激励作者提供准确的排名建议。

    In 2023, the International Conference on Machine Learning (ICML) required authors with multiple submissions to rank their submissions based on perceived quality. In this paper, we aim to employ these author-specified rankings to enhance peer review in machine learning and artificial intelligence conferences by extending the Isotonic Mechanism (Su, 2021, 2022) to exponential family distributions. This mechanism generates adjusted scores closely align with the original scores while adhering to author-specified rankings. Despite its applicability to a broad spectrum of exponential family distributions, this mechanism's implementation does not necessitate knowledge of the specific distribution form. We demonstrate that an author is incentivized to provide accurate rankings when her utility takes the form of a convex additive function of the adjusted review scores. For a certain subclass of exponential family distributions, we prove that the author reports truthfully only if the question in
    
[^104]: 树状Parzen估计器：理解其算法组成部分及其在提高实证表现中的作用

    Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance. (arXiv:2304.11127v1 [cs.LG])

    [http://arxiv.org/abs/2304.11127](http://arxiv.org/abs/2304.11127)

    该论文介绍了一种广泛使用的贝叶斯优化方法 Tree-structured Parzen estimator (TPE)，并对其控制参数的作用和算法直觉进行了讨论和分析，提供了一组推荐设置并证明其能够提高TPE的性能表现。

    

    许多领域中最近的进展要求更加复杂的实验设计。这种复杂的实验通常有许多参数，需要参数调整。Tree-structured Parzen estimator (TPE) 是一种贝叶斯优化方法，在最近的参数调整框架中被广泛使用。尽管它很受欢迎，但控制参数的角色和算法直觉尚未得到讨论。在本教程中，我们将确定每个控制参数的作用以及它们对超参数优化的影响，使用多种基准测试。我们将从剖析研究中得出的推荐设置与基准方法进行比较，并证明我们的推荐设置提高了TPE的性能。我们的TPE实现可在https://github.com/nabenabe0928/tpe/tree/single-opt中获得。

    Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.
    
[^105]: 医学图像分析中的任意分割模型：一项实验研究

    Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])

    [http://arxiv.org/abs/2304.10517](http://arxiv.org/abs/2304.10517)

    本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。

    

    由于数据注释的有限可用性和获取成本，训练医学图像分割模型仍然具有挑战性。Segment Anything Model（SAM）是一种基础模型，经过超过10亿个注释的训练，主要用于自然图像，旨在能够以交互方式分割用户定义的感兴趣的对象。尽管SAM在自然图像上表现出色，但不清楚该模型在转换到医学图像领域时会受到多大影响。在这里，我们对SAM在各种模态和解剖学的11个医学图像数据集上进行了广泛的评估。在我们的实验中，我们使用标准方法生成点提示来模拟交互分割。实验结果表明，SAM基于单点提示的表现在任务和数据集方面高度变化，即从脊柱MRI数据集的0.1135到髋关节X射线数据集的0.8650。

    Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
    
[^106]: 基于注意机制的 softmax 回归

    Attention Scheme Inspired Softmax Regression. (arXiv:2304.10411v1 [cs.LG])

    [http://arxiv.org/abs/2304.10411](http://arxiv.org/abs/2304.10411)

    本研究从 softmax 单元中获得灵感，提出了注意机制 inspired 的 softmax 回归问题，该问题可用于控制潜在函数的进展和稳定性。

    

    大型语言模型（LLMs）已经给人类社会带来了巨大的变革。LLMs 的关键计算之一是 softmax 单元。这个操作在 LLMs 中非常重要，因为它允许模型在给定输入单词序列的情况下生成可能的下一个单词或短语的分布。这个分布然后用来选择最有可能的下一个单词或短语，基于模型分配的概率。softmax 单元在训练 LLMs 中起着至关重要的作用，因为它允许模型通过调整神经网络的权重和偏差从数据中学习。在凸优化领域，例如使用中心路径法解决线性规划，softmax 函数已经成为控制潜在函数的进展和稳定性的关键工具。在这项工作中，我们从 softmax 单元中获得灵感，定义了一个 softmax 回归问题。形式上讲，给定一个矩阵 $A \in \mathbb{R}^{n \times d}$ 和...

    Large language models (LLMs) have made transformed changes for human society. One of the key computation in LLMs is the softmax unit. This operation is important in LLMs because it allows the model to generate a distribution over possible next words or phrases, given a sequence of input words. This distribution is then used to select the most likely next word or phrase, based on the probabilities assigned by the model. The softmax unit plays a crucial role in training LLMs, as it allows the model to learn from the data by adjusting the weights and biases of the neural network.  In the area of convex optimization such as using central path method to solve linear programming. The softmax function has been used a crucial tool for controlling the progress and stability of potential function [Cohen, Lee and Song STOC 2019, Brand SODA 2020].  In this work, inspired the softmax unit, we define a softmax regression problem. Formally speaking, given a matrix $A \in \mathbb{R}^{n \times d}$ and 
    
[^107]: 邻里犬与街头流浪汉：Nextdoor社交网络中真实世界不平等的在线体现

    Lady and the Tramp Nextdoor: Online Manifestations of Real-World Inequalities in the Nextdoor Social Network. (arXiv:2304.05232v1 [cs.SI])

    [http://arxiv.org/abs/2304.05232](http://arxiv.org/abs/2304.05232)

    本文第一个大规模研究了Nextdoor社交网络，发现不同收入水平的社区在社交网络上的在线行为不同，更富裕的社区情感更积极，更多地讨论犯罪，尽管实际犯罪率要低得多，同时用户生成内容能够预测收入和不平等。

    

    收入影响着生活中很多选择，从健康到教育。许多研究都利用在线社交网络的数据研究此问题。但在本文中，我们提出了一个相反的问题：不同收入水平是否导致不同的在线行为？我们证明了确实是如此。我们展示了第一个Nextdoor社交网络的大规模研究，在美国的64,283个社区和英国的3,325个社区收集了2.6万篇帖子，以研究在线话语是否反映了社区的收入和收入不平等。我们发现不同收入的社区的帖子确实不同，比如更富裕的社区情感更积极，更多地讨论犯罪，尽管实际犯罪率要低得多。然后，我们展示了用户生成内容可预测收入和不平等。我们训练了多个机器学习模型，预测了收入（R-Square=0.841）和不平等（R-Sq…

    From health to education, income impacts a huge range of life choices. Many papers have leveraged data from online social networks to study precisely this. In this paper, we ask the opposite question: do different levels of income result in different online behaviors? We demonstrate it does. We present the first large-scale study of Nextdoor, a popular location-based social network. We collect 2.6 Million posts from 64,283 neighborhoods in the United States and 3,325 neighborhoods in the United Kingdom, to examine whether online discourse reflects the income and income inequality of a neighborhood. We show that posts from neighborhoods with different income indeed differ, e.g. richer neighborhoods have a more positive sentiment and discuss crimes more, even though their actual crime rates are much lower. We then show that user-generated content can predict both income and inequality. We train multiple machine learning models and predict both income (R-Square=0.841) and inequality (R-Sq
    
[^108]: 重新构想负提示算法：将2D扩散转化为3D，缓解“扬尼斯问题”等等

    Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond. (arXiv:2304.04968v1 [cs.CV])

    [http://arxiv.org/abs/2304.04968](http://arxiv.org/abs/2304.04968)

    本研究提出了Perp-Neg算法，通过利用得分空间的几何特性来解决目前文本到图像扩散模型中负面提示算法存在的问题，使得用户能够编辑掉初始生成图像中不想要的概念，从而提供了更大的灵活性。同时，我们还通过提出基于Perp-Neg的3D负面提示算法，将算法扩展到3D应用中。

    

    尽管文本到图像扩散模型在从文本生成图像方面取得了显著进展，但它们有时更倾向于生成类似于模型训练数据的图像，而不是提供的文本。这限制了它们在2D和3D应用中的使用。为了解决这个问题，我们探索了使用负面提示，但发现当前的实现无法产生期望的结果，特别是当主提示和负面提示之间存在重叠时。为了克服这个问题，我们提出Perp-Neg，一种利用得分空间的几何特性来解决当前负性提示算法缺点的新算法。Perp-Neg不需要对模型进行任何训练或微调。此外，我们通过实验表明，Perp-Neg通过在2D情况下使用户能够编辑掉初始生成的图像中不想要的概念，提供了生成图像更大的灵活性。此外，为了扩展我们的算法到3D应用，我们还提出了一种基于Perp-Neg的3D负面提示算法。

    Although text-to-image diffusion models have made significant strides in generating images from text, they are sometimes more inclined to generate images like the data on which the model was trained rather than the provided text. This limitation has hindered their usage in both 2D and 3D applications. To address this problem, we explored the use of negative prompts but found that the current implementation fails to produce desired results, particularly when there is an overlap between the main and negative prompts. To overcome this issue, we propose Perp-Neg, a new algorithm that leverages the geometrical properties of the score space to address the shortcomings of the current negative prompts algorithm. Perp-Neg does not require any training or fine-tuning of the model. Moreover, we experimentally demonstrate that Perp-Neg provides greater flexibility in generating images by enabling users to edit out unwanted concepts from the initially generated images in 2D cases. Furthermore, to e
    
[^109]: MHfit：使用机器学习预测运动员健康状况的移动健康数据

    MHfit: Mobile Health Data for Predicting Athletics Fitness Using Machine Learning. (arXiv:2304.04839v1 [cs.LG])

    [http://arxiv.org/abs/2304.04839](http://arxiv.org/abs/2304.04839)

    本文提出了利用移动健康数据来比较多种机器学习算法，预测人体健康行为和健身情况的方法；结果表明，XGBoost算法表现优于其他算法。

    

    移动电话和其他电子设备已经帮助人们在不需要进行数据输入的情况下收集数据。本文将特别关注移动健康数据。移动健康数据使用移动设备实时收集临床健康数据并跟踪患者生命体征。我们的研究旨在使用从移动设备和患者身上的传感器收集的数据来比较多种机器学习算法，以预测人类行为和健康，并为小型或大型运动队提供关于某个运动员是否适合参与特定比赛的决策。本研究从一项类似的移动健康研究中获得了包含来自不同背景的10名志愿者的生命体征记录的数据集。他们必须在身体上放置传感器并进行多项体力活动。我们使用了5种机器学习算法（XGBoost，朴素贝叶斯，决策树，随机森林和逻辑回归）来分析和预测人类健康行为和健身情况。我们的研究结果表明，XGBoost算法表现优于其他算法。

    Mobile phones and other electronic gadgets or devices have aided in collecting data without the need for data entry. This paper will specifically focus on Mobile health data. Mobile health data use mobile devices to gather clinical health data and track patient vitals in real-time. Our study is aimed to give decisions for small or big sports teams on whether one athlete good fit or not for a particular game with the compare several machine learning algorithms to predict human behavior and health using the data collected from mobile devices and sensors placed on patients. In this study, we have obtained the dataset from a similar study done on mhealth. The dataset contains vital signs recordings of ten volunteers from different backgrounds. They had to perform several physical activities with a sensor placed on their bodies. Our study used 5 machine learning algorithms (XGBoost, Naive Bayes, Decision Tree, Random Forest, and Logistic Regression) to analyze and predict human health behav
    
[^110]: DiffMimic: 基于可微分物理的高效运动模仿

    DiffMimic: Efficient Motion Mimicking with Differentiable Physics. (arXiv:2304.03274v1 [cs.CV])

    [http://arxiv.org/abs/2304.03274](http://arxiv.org/abs/2304.03274)

    本文提出了DiffMimic，一种基于可微分物理的高效运动模仿方法。与传统强化学习方法相比，其有更快更稳定的收敛速度；同时通过演示重播机制避免陷入局部最优解。

    

    运动模仿是基于物理的角色动画中的基础任务，然而大多数现有的运动模仿方法都建立在强化学习（RL）之上，存在重度奖励工程、高方差和难以探索的收敛速度缓慢等问题。本文提出了一种基于可微分物理模拟器（DPS）的运动模仿方法，名为DiffMimic，通过分析梯度和基于真实物理先验学习稳定策略，从而实现显著更快和更稳定的收敛。此外，为了避免陷入局部最优解，我们还利用演示重播机制，在长时间跨度内实现稳定梯度反向传播。

    Motion mimicking is a foundational task in physics-based character animation. However, most existing motion mimicking methods are built upon reinforcement learning (RL) and suffer from heavy reward engineering, high variance, and slow convergence with hard explorations. Specifically, they usually take tens of hours or even days of training to mimic a simple motion sequence, resulting in poor scalability. In this work, we leverage differentiable physics simulators (DPS) and propose an efficient motion mimicking method dubbed DiffMimic. Our key insight is that DPS casts a complex policy learning task to a much simpler state matching problem. In particular, DPS learns a stable policy by analytical gradients with ground-truth physical priors hence leading to significantly faster and stabler convergence than RL-based methods. Moreover, to escape from local optima, we utilize a Demonstration Replay mechanism to enable stable gradient backpropagation in a long horizon. Extensive experiments o
    
[^111]: 学习增强型物联网系统的因果修复

    Causal Repair of Learning-enabled Cyber-physical Systems. (arXiv:2304.02813v1 [eess.SY])

    [http://arxiv.org/abs/2304.02813](http://arxiv.org/abs/2304.02813)

    本文提出了一种学习增强型物联网系统的因果诊断和修复方法，通过矫正有问题的输入/输出行为子集，识别真正的属性违规原因并修复。

    

    实际因果模型使用领域知识生成导致结果的事件的令人信服的诊断。将这些模型应用于具有学习增强组件（LEC）的物联网系统（CPS）中诊断和修复运行时属性违规是有希望的。然而，鉴于LEC的高多样性和复杂性，将领域知识（例如CPS动态）编码成可生成有用修复建议的可扩展实际因果模型是具有挑战性的。在本文中，我们将因果诊断集中于LEC的输入/输出行为。具体而言，我们的目标是确定LEC的哪个输入/输出行为子集是导致属性违规的真正原因。一个重要的副产品是矫正识别出有问题的行为来修复运行时属性的LEC的反事实版本。基于这些结果，我们设计了一个两步诊断流程：（1）构建Halpern-Pearl因果模型以反映属性结果的依赖关系。

    Models of actual causality leverage domain knowledge to generate convincing diagnoses of events that caused an outcome. It is promising to apply these models to diagnose and repair run-time property violations in cyber-physical systems (CPS) with learning-enabled components (LEC). However, given the high diversity and complexity of LECs, it is challenging to encode domain knowledge (e.g., the CPS dynamics) in a scalable actual causality model that could generate useful repair suggestions. In this paper, we focus causal diagnosis on the input/output behaviors of LECs. Specifically, we aim to identify which subset of I/O behaviors of the LEC is an actual cause for a property violation. An important by-product is a counterfactual version of the LEC that repairs the run-time property by fixing the identified problematic behaviors. Based on this insights, we design a two-step diagnostic pipeline: (1) construct and Halpern-Pearl causality model that reflects the dependency of property outcom
    
[^112]: 图神经网络中池化的表达能力

    The expressive power of pooling in Graph Neural Networks. (arXiv:2304.01575v1 [cs.LG])

    [http://arxiv.org/abs/2304.01575](http://arxiv.org/abs/2304.01575)

    本文研究了池化算子在图神经网络中的表达能力，并提供了一个通用标准来选择或设计池化算子。

    

    在图神经网络（GNNs）中，分层池化算子通过创建图结构和其顶点特征的本地摘要来生成输入数据的更粗糙的表示。虽然已经致力于研究GNN中消息传递（MP）层的表达能力，但缺乏关于池化算子如何影响GNN表达能力的研究。此外，尽管最近在有效池化算子的设计方面取得了进展，但没有一个原则性的标准来比较它们。我们的工作旨在通过提供足够的条件使池化算子在其之前的MP层中完全保留表达能力来填补这一空白。这些条件作为选择现有池化算子或设计新的池化算子的通用和理论基础的标准。基于我们的理论发现，我们审查了几个现有的池化算子，并确定了那些不能满足表达性假设的算子。

    In Graph Neural Networks (GNNs), hierarchical pooling operators generate a coarser representation of the input data by creating local summaries of the graph structure and its vertex features. Considerable attention has been devoted to studying the expressive power of message-passing (MP) layers in GNNs, while a study on how pooling operators affect the expressivity of a GNN is still lacking. Additionally, despite the recent advances in the design of effective pooling operators, there is not a principled criterion to compare them. Our work aims to fill this gap by providing sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically-grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we reviewed several existing pooling operators and identified those that fail to satisfy the expressiveness assumptions. Finally,
    
[^113]: 从时间序列中推断网络结构的神经方法

    Inferring networks from time series: a neural approach. (arXiv:2303.18059v1 [cs.LG])

    [http://arxiv.org/abs/2303.18059](http://arxiv.org/abs/2303.18059)

    本论文提出了一种基于神经网络的快速计算方法，可以从时间序列数据中推断大型网络的相邻矩阵，并对不确定性进行量化，解决了网络推断问题的不足。

    

    网络结构是许多复杂现象的动态基础，包括基因调控、食物链、电力网络和社交媒体。然而，由于网络结构通常无法直接观测到，因此必须从其紧急动态的观测数据中推断它们的相互连接性。在本研究中，我们提出了一种快速计算方法，使用神经网络从时间序列数据中推断大型网络相邻矩阵。使用神经网络提供了预测的不确定性量化方法，反映了推断问题的非凸性和数据上的噪声。这是有用的，因为网络推断问题通常是欠定的，并且在网络推断方法中缺乏这个特征。我们通过从观测其响应断电的情况下推断英国电力网络的线路故障位置来展示我们的方法的能力。

    Network structures underlie the dynamics of many complex phenomena, from gene regulation and foodwebs to power grids and social media. Yet, as they often cannot be observed directly, their connectivities must be inferred from observations of their emergent dynamics. In this work we present a powerful and fast computational method to infer large network adjacency matrices from time series data using a neural network. Using a neural network provides uncertainty quantification on the prediction in a manner that reflects both the non-convexity of the inference problem as well as the noise on the data. This is useful since network inference problems are typically underdetermined, and a feature that has hitherto been lacking from network inference methods. We demonstrate our method's capabilities by inferring line failure locations in the British power grid from observations of its response to a power cut. Since the problem is underdetermined, many classical statistical tools (e.g. regressio
    
[^114]: Wyner多视图无监督学习的高效交替最小化求解器

    Efficient Alternating Minimization Solvers for Wyner Multi-View Unsupervised Learning. (arXiv:2303.15866v1 [cs.IT])

    [http://arxiv.org/abs/2303.15866](http://arxiv.org/abs/2303.15866)

    本文提出了适用于Wyner多视图无监督学习的高效交替最小化求解器，包括变分形式和表现形式，通过多重乘数交替方向算法可以解决由此造成的非凸优化问题。

    

    在本文中，我们采用Wyner通用信息框架进行无监督的多视图表示学习。在这个框架内，我们提出了两种新的公式，使得可以基于交替最小化原则开发计算高效的求解器。第一种公式被称为“变分形式”，其复杂度随着视图数量的增加而线性增长，并基于变分推理紧束缚和拉格朗日优化目标函数。第二种公式，即“表现形式”，被证明包括已知结果的特殊情况。在这里，我们开发了一个定制版本的多重乘数交替方向算法（ADMM），用于解决由此产生的非凸优化问题。在两种情况下，所提出的求解器的收敛性在某些相关范围内得到了证明。此外，我们的实证结果证明了提出的方法的有效性。

    In this work, we adopt Wyner common information framework for unsupervised multi-view representation learning. Within this framework, we propose two novel formulations that enable the development of computational efficient solvers based on the alternating minimization principle. The first formulation, referred to as the {\em variational form}, enjoys a linearly growing complexity with the number of views and is based on a variational-inference tight surrogate bound coupled with a Lagrangian optimization objective function. The second formulation, i.e., the {\em representational form}, is shown to include known results as special cases. Here, we develop a tailored version from the alternating direction method of multipliers (ADMM) algorithm for solving the resulting non-convex optimization problem. In the two cases, the convergence of the proposed solvers is established in certain relevant regimes. Furthermore, our empirical results demonstrate the effectiveness of the proposed methods 
    
[^115]: 非线性平流-扩散-吸附系统的高效混合建模和吸附模型探索：系统科学机器学习方法

    Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems: A systematic scientific machine learning approach. (arXiv:2303.13555v1 [cs.CE])

    [http://arxiv.org/abs/2303.13555](http://arxiv.org/abs/2303.13555)

    本研究提出了一种机器学习方法，可用于创建非线性平流-扩散-吸附系统的高效混合模型和发现吸附摄取模型的吸附动力学定律结构。

    

    本研究提出了一种系统化的机器学习方法，用于创建非线性平流-扩散-吸附系统的高效混合模型和发现吸附摄取模型。它演示了使用基于梯度的优化器、伴随灵敏度分析和JIT编译的向量雅各布乘积，结合空间离散和自适应积分器来训练这些复杂系统的有效方法。稀疏和符号回归被用来识别人工神经网络中缺失的函数。该方法的鲁棒性在一个模拟数据集上得到了测试，该数据集包含固定床吸附的噪声突破曲线观测结果，得出了拟合良好的混合模型。该研究成功地利用稀疏和符号回归重建了吸附摄取动力学，并利用确定的多项式准确地预测了突破曲线，突显了该框架发现吸附动力学定律结构的潜力。

    This study presents a systematic machine learning approach for creating efficient hybrid models and discovering sorption uptake models in non-linear advection-diffusion-sorption systems. It demonstrates an effective method to train these complex systems using gradientbased optimizers, adjoint sensitivity analysis, and JIT-compiled vector Jacobian products, combined with spatial discretization and adaptive integrators. Sparse and symbolic regression were employed to identify missing functions in the artificial neural network. The robustness of the proposed method was tested on an in-silico data set of noisy breakthrough curve observations of fixed-bed adsorption, resulting in a well-fitted hybrid model. The study successfully reconstructed sorption uptake kinetics using sparse and symbolic regression, and accurately predicted breakthrough curves using identified polynomials, highlighting the potential of the proposed framework for discovering sorption kinetic law structures.
    
[^116]: 论盗用语言模型解码算法的风险

    On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04729](http://arxiv.org/abs/2303.04729)

    这项工作首次展示，一个拥有典型API访问权限的对手可以以极低的金钱成本窃取GPT-2和GPT-3等LM的解码算法的类型和超参数。

    

    现代语言模型（LM）生成文本的关键组成部分是选择和调整解码算法。这些算法确定如何从LM生成的内部概率分布中生成文本。选择解码算法并调整其超参数的过程需要显著的时间、手动工作和计算，还需要进行广泛的人类评估。因此，解码算法的身份和超参数被认为是极其有价值的。在这项工作中，我们首次展示了一个拥有典型API访问权限的对手可以以极低的金钱成本窃取其解码算法的类型和超参数。我们的攻击对用于文本生成API的流行LM有效，包括GPT-2和GPT-3。我们证明了只需花费几美元，例如0.8美元、1美元、4美元和40美元，就可以盗取此类信息。

    A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
    
[^117]: 面向大规模机器学习模型的可证明高效量子算法

    Towards provably efficient quantum algorithms for large-scale machine-learning models. (arXiv:2303.03428v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2303.03428](http://arxiv.org/abs/2303.03428)

    本论文提出了一种可能的针对通用（随机）梯度下降算法的高效量子解决方案，只要模型足够耗散和稀疏，具有小的学习率，并且可以缩放至 $O(T^2 \times \text{polylog}(n))$。在实践中，证明了在稀疏训练的情况下，量子计算可以显著提高效率。

    

    大型机器学习模型是人工智能的革命性技术，其瓶颈包括巨大的计算开销、功耗和时间，既用于预训练，也用于微调过程。本研究表明，容错量子计算可能会针对通用（随机）梯度下降算法提供可证明的高效解决方案，其缩放为 $\mathcal{O}(T^2 \times \text{polylog}(n))$，其中 $n$ 是模型的大小，$T$ 是训练中的迭代次数，只要模型足够耗散和稀疏，并具有较小的学习率。基于早期用于耗散微分方程的高效量子算法，我们发现并证明了类似的算法可用于（随机）梯度下降，这是机器学习的主要算法。在实践中，我们对拥有从700万到1.03亿个参数的大型机器学习模型进行了基准测试。我们发现，在稀疏训练的情况下，量子计算显然可以在一定程度上提高效率。

    Large machine learning models are revolutionary technologies of artificial intelligence whose bottlenecks include huge computational expenses, power, and time used both in the pre-training and fine-tuning process. In this work, we show that fault-tolerant quantum computing could possibly provide provably efficient resolutions for generic (stochastic) gradient descent algorithms, scaling as $\mathcal{O}(T^2 \times \text{polylog}(n))$, where $n$ is the size of the models and $T$ is the number of iterations in the training, as long as the models are both sufficiently dissipative and sparse, with small learning rates. Based on earlier efficient quantum algorithms for dissipative differential equations, we find and prove that similar algorithms work for (stochastic) gradient descent, the primary algorithm for machine learning. In practice, we benchmark instances of large machine learning models from 7 million to 103 million parameters. We find that, in the context of sparse training, a quan
    
[^118]: 数据高效的对比自监督学习：易于学习的样本起到最大的作用。

    Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09195](http://arxiv.org/abs/2302.09195)

    该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。

    

    自监督学习（SSL）从大量的无标签训练数据中学习高质量的表示。随着数据集变得越来越大，识别对学习此类表示最有用的示例变得至关重要。这可以通过减少学习高质量表示所需的数据量来实现有效的SSL。然而，对于SSL的价值如何量化一直是一个悬而未决的问题。在本文中，我们首次解决了这个问题，证明在期望意义下，对比SSL中对学习做出最大贡献的示例是具有最相似数据增强的示例。我们对这些子集的SSL的广义性能提供了严格的保证。实验证明，令人惊讶的是，对SSL做出最大贡献的子集是对监督学习做出最小贡献的子集。通过广泛的实验，我们证明了我们的子集在CIFAR100、CIFAR中的表现优于随机子集3%以上。

    Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
    
[^119]: 面向多样性预测的生成因果表示学习

    Generative Causal Representation Learning for Out-of-Distribution Motion Forecasting. (arXiv:2302.08635v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08635](http://arxiv.org/abs/2302.08635)

    该论文提出了一种生成因果表示学习方法，通过利用因果关系来实现分布转移下的知识迁移，主要应用于面向多样性预测的问题。

    

    传统的有监督学习方法通常假定样本是独立同分布的，但对于超出分布的数据很敏感。我们提出了利用因果关系实现分布转移下的知识迁移的生成因果表示学习方法。我们评估了该方法在人体轨迹预测模型中的有效性，并且该方法也可以应用于其他领域。

    Conventional supervised learning methods typically assume i.i.d samples and are found to be sensitive to out-of-distribution (OOD) data. We propose Generative Causal Representation Learning (GCRL) which leverages causality to facilitate knowledge transfer under distribution shifts. While we evaluate the effectiveness of our proposed method in human trajectory prediction models, GCRL can be applied to other domains as well. First, we propose a novel causal model that explains the generative factors in motion forecasting datasets using features that are common across all environments and with features that are specific to each environment. Selection variables are used to determine which parts of the model can be directly transferred to a new environment without fine-tuning. Second, we propose an end-to-end variational learning paradigm to learn the causal mechanisms that generate observations from features. GCRL is supported by strong theoretical results that imply identifiability of the
    
[^120]: Listen2Scene：交互式物质感知双耳音频传播重构三维场景

    Listen2Scene: Interactive material-aware binaural soundbpropagation for reconstructed 3D scenes. (arXiv:2302.02809v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2302.02809](http://arxiv.org/abs/2302.02809)

    本文提出了一种交互式的物质感知双耳音频传播方法，能够生成真实环境下的渲染音频，利用图神经网络和条件生成对抗网络，处理重构三维模型中的缺陷，并且能够精确生成与真实环境相符的声学输出。

    

    本文提出了一种用于虚拟现实（VR）和增强现实（AR）应用的端到端双耳音频渲染方法（Listen2Scene）。我们提出了一种新颖的基于神经网络的双耳声学传播方法，以生成真实环境的3D模型的声学效果。任何清洁音频或干音频都可以与生成的声学效果卷积，以渲染与真实环境相对应的音频。我们提出了一个图神经网络，利用3D场景的材料和拓扑信息生成场景潜在向量。此外，我们使用条件生成对抗网络（CGAN）从场景潜在向量生成声学效果。我们的网络能够处理重构的三维网格模型中的孔洞或其他伪像。我们提出了一种高效的成本函数，用于生成器网络以整合空间音频效果。给定源和听者位置，我们的基于学习的双耳声音传播方法可以生成与真实环境精度相符的声学输出。

    We present an end-to-end binaural audio rendering approach (Listen2Scene) for virtual reality (VR) and augmented reality (AR) applications. We propose a novel neural-network-based binaural sound propagation method to generate acoustic effects for 3D models of real environments. Any clean audio or dry audio can be convolved with the generated acoustic effects to render audio corresponding to the real environment. We propose a graph neural network that uses both the material and the topology information of the 3D scenes and generates a scene latent vector. Moreover, we use a conditional generative adversarial network (CGAN) to generate acoustic effects from the scene latent vector. Our network is able to handle holes or other artifacts in the reconstructed 3D mesh model. We present an efficient cost function to the generator network to incorporate spatial audio effects. Given the source and the listener position, our learning-based binaural sound propagation approach can generate an acou
    
[^121]: 域索引变分贝叶斯：可解释的域索引用于域自适应

    Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation. (arXiv:2302.02561v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02561](http://arxiv.org/abs/2302.02561)

    该论文介绍了一种新的域自适应方法，该方法利用了从多域数据中生成的域索引，提供额外的洞察视角，并在各种任务中实现了最佳性能。

    

    先前的研究表明，利用域索引可以显著提高域自适应性能。然而，并非总是有这样的域索引可用。为解决这一挑战，我们首先从概率角度提供了域索引的正式定义，然后提出了一个对抗性变分贝叶斯框架，从多域数据中推断出域索引，从而提供额外的域关系洞察，并提高域自适应性能。我们的理论分析表明，我们的对抗性变分贝叶斯框架在平衡点处找到了最优的域索引。对合成和真实数据的实证结果验证了我们的模型可以产生可解释的域索引，使我们可以实现优于现有域适应方法的性能。代码可在https://github.com/Wang-ML-Lab/VDI获得。

    Previous studies have shown that leveraging domain index can significantly boost domain adaptation performance (arXiv:2007.01807, arXiv:2202.03628). However, such domain indices are not always available. To address this challenge, we first provide a formal definition of domain index from the probabilistic perspective, and then propose an adversarial variational Bayesian framework that infers domain indices from multi-domain data, thereby providing additional insight on domain relations and improving domain adaptation performance. Our theoretical analysis shows that our adversarial variational Bayesian framework finds the optimal domain index at equilibrium. Empirical results on both synthetic and real data verify that our model can produce interpretable domain indices which enable us to achieve superior performance compared to state-of-the-art domain adaptation methods. Code is available at https://github.com/Wang-ML-Lab/VDI.
    
[^122]: 高维次高斯分布的快速，样本有效，仿射不变私有均值和协方差估计

    Fast, Sample-Efficient, Affine-Invariant Private Mean and Covariance Estimation for Subgaussian Distributions. (arXiv:2301.12250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12250](http://arxiv.org/abs/2301.12250)

    本论文提出了一种快速的差分私有算法，用于具有几乎最优样本复杂度的高维协方差感知均值估计。在Mahalanobis误差度量中，也就是相对于协方差的平均误差中，我们的算法使得$\hat \mu$更接近$\mu$。

    

    我们提出了一种快速的差分私有算法，用于具有几乎最优样本复杂度的高维协方差感知均值估计。以前已知只有指数时间估计器才能实现此保证。给定从具有未知均值 $μ$ 和协方差 $Σ$ 的（亚）高斯分布中抽取的$n$个样本，我们的 $(\varepsilon,\delta)$-差分式私有估计器生成$\tilde{\mu}$，使得只要 $n \gtrsim \tfrac d {\alpha^2} + \tfrac{d \sqrt{\log 1/\delta}}{\alpha \varepsilon}+\frac{d\log 1/\delta}{\varepsilon}$，就满足 $\|\mu - \tilde{\mu}\|_{\Sigma} \leq \alpha$。Mahalanobis误差度量 $\|\mu - \hat{\mu}\|_{\Sigma}$ 衡量了$\hat \mu$与$\mu$在$\Sigma$相对距离; 它表征了样本平均值的误差。我们的算法运行时间为$\tilde{O}(nd^{\omega - 1} + nd/\varepsilon)$，其中$\omega < 2.38$是矩阵乘法指数。我们使用 Brown、Gaboardi、Smith、Ullman 和 Zakynthiadaki[BGSUZ18] 的指数时间方法来计算问题的最优估计的足够统计量，并将其用于通过随机线性代数构造线性时间估计器。

    We present a fast, differentially private algorithm for high-dimensional covariance-aware mean estimation with nearly optimal sample complexity. Only exponential-time estimators were previously known to achieve this guarantee. Given $n$ samples from a (sub-)Gaussian distribution with unknown mean $\mu$ and covariance $\Sigma$, our $(\varepsilon,\delta)$-differentially private estimator produces $\tilde{\mu}$ such that $\|\mu - \tilde{\mu}\|_{\Sigma} \leq \alpha$ as long as $n \gtrsim \tfrac d {\alpha^2} + \tfrac{d \sqrt{\log 1/\delta}}{\alpha \varepsilon}+\frac{d\log 1/\delta}{\varepsilon}$. The Mahalanobis error metric $\|\mu - \hat{\mu}\|_{\Sigma}$ measures the distance between $\hat \mu$ and $\mu$ relative to $\Sigma$; it characterizes the error of the sample mean. Our algorithm runs in time $\tilde{O}(nd^{\omega - 1} + nd/\varepsilon)$, where $\omega < 2.38$ is the matrix multiplication exponent.  We adapt an exponential-time approach of Brown, Gaboardi, Smith, Ullman, and Zakynthi
    
[^123]: 将知识纳入文档摘要生成中：基于GPT-2的前缀调整应用

    Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11719](http://arxiv.org/abs/2301.11719)

    本论文研究了将事实知识纳入生成的摘要的可能性，具体采用前缀调整的方法，实验结果表明，此方法可以生成保留知识的摘要，而且可以提升整体性能。

    

    尽管现在文档摘要技术得到了很大的发展，但是生成的摘要和原始文本之间的事实不一致仍然时有发生。本研究探索了采用提示来将事实知识纳入生成的摘要的可能性。我们具体研究了前缀调整，它使用一组可训练的连续前缀提示和离散自然语言提示来帮助摘要生成。实验结果表明，可训练的前缀可以帮助摘要模型准确地从离散提示中提取信息，从而生成保留知识的摘要，这些摘要在事实上与离散提示一致。生成的摘要的ROUGE改进表明，将事实知识明确地添加到摘要生成过程中可以提升整体性能，显示出在其他自然语言处理任务中应用的巨大潜力。

    Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.
    
[^124]: 从伪随机性到多组公平性再到回来

    From Pseudorandomness to Multi-Group Fairness and Back. (arXiv:2301.08837v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08837](http://arxiv.org/abs/2301.08837)

    本文探索了预测算法中多组公平性和伪随机性的联系，提供了新的多校准算法和实值函数核引理证明算法。

    

    本文探讨了预测算法中多组公平性和泄露-韧性和图形规则之间的联系，在一些参数范围内提供了新的多校准和实值函数核引理证明算法。

    We identify and explore connections between the recent literature on multi-group fairness for prediction algorithms and the pseudorandomness notions of leakage-resilience and graph regularity. We frame our investigation using new, statistical distance-based variants of multicalibration that are closely related to the concept of outcome indistinguishability. Adopting this perspective leads us naturally not only to our graph theoretic results, but also to new, more efficient algorithms for multicalibration in certain parameter regimes and a novel proof of a hardcore lemma for real-valued functions.
    
[^125]: SEQUENT: 序列量子增强训练实现量子机器学习可追溯性

    SEQUENT: Towards Traceable Quantum Machine Learning using Sequential Quantum Enhanced Training. (arXiv:2301.02601v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2301.02601](http://arxiv.org/abs/2301.02601)

    SEQUENT提出了一种新方法，通过序列量子增强训练实现混合机器学习模型的追溯性，可以追踪到所选的电路架构和参数化对模型的贡献，解决了现有方法中无法严格分离经典和量子影响的问题。

    

    近期，将量子计算等新的计算范式应用于机器学习领域引起了广泛关注。然而，由于高维实际应用程序目前尚无法使用纯量子硬件进行求解，因此提出了使用经典和量子机器学习范式的混合方法。例如，迁移学习方法已成功应用于混合图像分类任务中。然而，仍需要探索有益的电路架构。因此，跟踪所选电路架构和参数化的影响对于开发有益的混合方法至关重要。然而，当前的方法包括同时训练两个部分的过程，因此不能严格分离经典和量子影响。因此，这些架构可能会产生具有卓越预测准确性的模型，同时采用最少量的量子影响。

    Applying new computing paradigms like quantum computing to the field of machine learning has recently gained attention. However, as high-dimensional real-world applications are not yet feasible to be solved using purely quantum hardware, hybrid methods using both classical and quantum machine learning paradigms have been proposed. For instance, transfer learning methods have been shown to be successfully applicable to hybrid image classification tasks. Nevertheless, beneficial circuit architectures still need to be explored. Therefore, tracing the impact of the chosen circuit architecture and parameterization is crucial for the development of beneficially applicable hybrid methods. However, current methods include processes where both parts are trained concurrently, therefore not allowing for a strict separability of classical and quantum impact. Thus, those architectures might produce models that yield a superior prediction accuracy whilst employing the least possible quantum impact. 
    
[^126]: 深度统计求解器用于配电系统状态估计

    Deep Statistical Solver for Distribution System State Estimation. (arXiv:2301.01835v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01835](http://arxiv.org/abs/2301.01835)

    该论文提出了基于图神经网络的深度统计求解器（DSS$^2$），应用于配电系统状态估计。DSS$^2$利用超图和节点消息传递方案更新潜在表示，并通过弱监督学习方法进行训练。实验结果证明了DSS$^2$在实际数据集上优于其他方法。

    

    实现准确的配电系统状态估计（DSSE）面临着许多挑战，其中包括可观测性不足和配电系统密度高。虽然基于机器学习模型的数据驱动替代方案可能是一种选择，但由于缺乏标注数据，它们在DSSE中受到影响。实际上，配电系统中的测量往往是嘈杂、损坏和不可用的。为解决这些问题，我们提出了基于图神经网络（GNN）的深度学习模型——用于配电系统状态估计的深度统计求解器（DSS$^2$），它考虑到了配电系统的网络结构和物理控制功率流方程。DSS$^2$利用超图来表示配电系统的异构组件，并通过以节点为中心的消息传递方案更新其潜在表示。提出了一种弱监督学习方法，在不需要标记数据的情况下以学习优化的方式训练DSS$^2$。实验结果表明，DSS$^2$在实际数据集上提高了估计精度，优于现有方法。

    Implementing accurate Distribution System State Estimation (DSSE) faces several challenges, among which the lack of observability and the high density of the distribution system. While data-driven alternatives based on Machine Learning models could be a choice, they suffer in DSSE because of the lack of labeled data. In fact, measurements in the distribution system are often noisy, corrupted, and unavailable. To address these issues, we propose the Deep Statistical Solver for Distribution System State Estimation (DSS$^2$), a deep learning model based on graph neural networks (GNNs) that accounts for the network structure of the distribution system and for the physical governing power flow equations. DSS$^2$ leverages hypergraphs to represent the heterogeneous components of the distribution systems and updates their latent representations via a node-centric message-passing scheme. A weakly supervised learning approach is put forth to train the DSS$^2$ in a learning-to-optimize fashion w
    
[^127]: 面向时序表格数据的动态特征工程和模型选择方法在制度变化下的应用

    Dynamic Feature Engineering and model selection methods for temporal tabular datasets with regime changes. (arXiv:2301.00790v2 [q-fin.CP] UPDATED)

    [http://arxiv.org/abs/2301.00790](http://arxiv.org/abs/2301.00790)

    本文提出了一种新的机器学习管道，用于在数据制度变化下对时序面板数据集的预测进行排名。使用梯度提升决策树（GBDT）并结合dropout技术的模型具有良好的性能和泛化能力，而动态特征中和则是一种高效而不需要重新训练模型就可以应用于任何机器学习模型中的后处理技术。

    

    由于严重的非平稳性，将深度学习算法应用于时序面板数据集是困难的，这可能导致过度拟合的模型在制度变化下性能不佳。在本文中，我们提出了一种新的机器学习管道，用于在数据制度变化下对时序面板数据集的预测进行排名。管道评估不同的机器学习模型，包括梯度提升决策树（GBDT）和具有和不具有简单特征工程的神经网络。我们发现，具有dropout的GBDT模型具有高性能、稳健性和泛化能力，而且相对复杂度较低、计算成本较低。然后，我们展示了在线学习技术可以在预测后处理中用于增强结果。特别地，我们提出了动态特征中和，这是一种无需重新训练模型就可以应用于任何机器学习模型的高效过程。

    The application of deep learning algorithms to temporal panel datasets is difficult due to heavy non-stationarities which can lead to over-fitted models that under-perform under regime changes. In this work we propose a new machine learning pipeline for ranking predictions on temporal panel datasets which is robust under regime changes of data. Different machine-learning models, including Gradient Boosting Decision Trees (GBDTs) and Neural Networks with and without simple feature engineering are evaluated in the pipeline with different settings. We find that GBDT models with dropout display high performance, robustness and generalisability with relatively low complexity and reduced computational cost. We then show that online learning techniques can be used in post-prediction processing to enhance the results. In particular, dynamic feature neutralisation, an efficient procedure that requires no retraining of models and can be applied post-prediction to any machine learning model, impr
    
[^128]: BTS：基于半监督学习的室内两房间存在检测中的双折叠师生网络

    BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI. (arXiv:2212.10802v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.10802](http://arxiv.org/abs/2212.10802)

    本文提出了一种基于半监督学习的双折叠师生网络，该网络通过利用部分标记和未标记的数据集智能地学习空间和时间特征，有效地解决了基于CSI的室内存在检测受到环境变化和有监督学习方法需要耗时标注的问题。

    

    近年来，基于有监督学习和信道状态信息（CSI）的室内人体存在检测引起了广泛的关注。然而，现有的研究依赖于CSI的空间信息，容易受到环境变化的影响，如物体移动、大气因素和机器重启，从而降低了预测精度。此外，基于有监督学习的方法需要进行耗时的标注来重新训练模型。因此，使用半监督学习方案设计一个连续监控的模型生命周期是必要的。在本文中，我们构思了一种双折叠师生（BTS）学习方法来检测存在于系统中的存在。该方法结合了半监督学习，利用部分标记和未标记的数据集。所提出的原始对偶师生网络从标记和未标记的CSI中智能地学习空间和时间特征。此外，增强的惩罚损失函数利用熵和距离测量来区分深层特征，降低噪声的影响。

    In recent years, indoor human presence detection based on supervised learning (SL) and channel state information (CSI) has attracted much attention. However, the existing studies that rely on spatial information of CSI are susceptible to environmental changes, such as object movement, atmospheric factors, and machine rebooting, which degrade prediction accuracy. Moreover, SL-based methods require time-consuming labeling for retraining models. Therefore, it is imperative to design a continuously monitored model life-cycle using a semi-supervised learning (SSL) based scheme. In this paper, we conceive a bifold teacher-student (BTS) learning approach for presence detection systems that combines SSL by utilizing partially labeled and unlabeled datasets. The proposed primal-dual teacher-student network intelligently learns spatial and temporal features from labeled and unlabeled CSI. Additionally, the enhanced penalized loss function leverages entropy and distance measures to distinguish dr
    
[^129]: 探索集成机器学习中的公平性及其组成方式

    Towards Understanding Fairness and its Composition in Ensemble Machine Learning. (arXiv:2212.04593v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04593](http://arxiv.org/abs/2212.04593)

    该论文研究集成机器学习中的公平性及其组成方式，提出了一种新颖的方法来衡量集成模型的公平性，并评估了单个学习器和集成超参数对公平性的影响。实验表明，公平性在集成中的组成方式是非线性的，超参数的选择可以显著影响集成模型的公平性。

    

    机器学习软件在现代社会被广泛采用，但存在可能基于种族、性别、年龄等因素影响少数群体公平性的问题。最近的许多研究提出了衡量和减轻机器学习模型算法偏见的方法，但现实中的机器学习模型往往由多个独立或相关联的学习器组成（如随机森林），其中的公平性组合方式非常复杂。公平性如何在集成中组合？学习器对集成最终公平性的影响是什么？公平的学习器是否会导致不公平的集成？此外，研究表明超参数对机器学习模型的公平性产生影响。集成超参数更为复杂，因为它们影响学习器在不同类别的集成中的组合方式。深入了解集成超参数对公平性的影响将有助于程序员设计公平的集成模型。本文研究了集成机器学习中的公平性及其组成方式，并基于条件人口平等的概念提出了一种新颖的方法来衡量集成模型的公平性。此外，我们还评估了单个学习器和集成超参数对公平性的影响。我们的实验表明，公平性在集成中的组成方式是非线性的，超参数的选择可以显著影响集成模型的公平性。

    Machine Learning (ML) software has been widely adopted in modern society, with reported fairness implications for minority groups based on race, sex, age, etc. Many recent works have proposed methods to measure and mitigate algorithmic bias in ML models. The existing approaches focus on single classifier-based ML models. However, real-world ML models are often composed of multiple independent or dependent learners in an ensemble (e.g., Random Forest), where the fairness composes in a non-trivial way. How does fairness compose in ensembles? What are the fairness impacts of the learners on the ultimate fairness of the ensemble? Can fair learners result in an unfair ensemble? Furthermore, studies have shown that hyperparameters influence the fairness of ML models. Ensemble hyperparameters are more complex since they affect how learners are combined in different categories of ensembles. Understanding the impact of ensemble hyperparameters on fairness will help programmers design fair ensem
    
[^130]: VISEM-Tracking，一份人类精子跟踪数据集

    VISEM-Tracking, a human spermatozoa tracking dataset. (arXiv:2212.02842v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02842](http://arxiv.org/abs/2212.02842)

    本文提供了人类精子跟踪数据集VISEM-Tracking，包含手动注释的包围框坐标和由专家分析的精子特征，并提供未标记的视频以供易于访问和分析，有助于训练监督式机器学习方法，提高在评估精子运动和运动学方面的精度和可靠性。

    

    精子运动的手动评估需要显微镜观察，由于所观察的精子在视野中的快速移动，这是具有挑战性的。为了获得正确的结果，手动评估需要进行广泛的培训。因此，在诊所中，计算机辅助精子分析（CASA）变得越来越常用。尽管如此，需要更多数据来训练监督式机器学习方法，以提高在评估精子运动和运动学方面的精度和可靠性。在这方面，我们提供了一个名为VISEM-Tracking的数据集，其中包含20个30秒的视频记录（包括29,196帧）的湿性精子制备物，具备手动注释的包围框坐标和由该领域的专家分析的一组精子特征。除了已注释的数据，我们还提供了未标记的视频剪辑，以便通过自监督或无监督学习等方法轻松访问和分析数据。作为本文的一部分，我们提出了基线精子检测性能。

    A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as selfor unsupervised learning. As part of this paper, we present baseline sperm detection performan
    
[^131]: CrossSplit: 通过数据分割缓解标签噪声记忆问题

    CrossSplit: Mitigating Label Noise Memorization through Data Splitting. (arXiv:2212.01674v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.01674](http://arxiv.org/abs/2212.01674)

    本文提出了一种名为CrossSplit的新训练程序，通过使用交叉分割的标签修正和半监督训练两个主要组成部分，缓解了深度学习算法中标签噪声记忆问题，具有良好的效果。

    

    我们的研究旨在解决标签噪声存在情况下深度学习算法鲁棒性不足的问题。基于现有的标签修正和共同教学方法，我们提出了一种新的训练程序——CrossSplit，以缓解噪声标签的记忆问题。CrossSplit使用在两个标记数据集的不相交部分上训练的一对神经网络。该方法组合了两个主要组成部分：(i)交叉分割标签修正：由于在数据集的一部分上训练的模型不能记忆来自其他部分的示例-标签对，因此可以使用对等网络的预测平滑调整每个网络呈现的训练标签；(ii)交叉分割半监督训练：在一个部分的数据上训练的网络也使用另一个部分的未标记输入。在CIFAR-10、CIFAR-100、Tiny-ImageNet和mini-WebVision数据集上的大量实验表明，我们的方法可以在广泛的噪声和干扰下比当前最先进的方法更好地完成任务。

    We approach the problem of improving robustness of deep learning algorithms in the presence of label noise. Building upon existing label correction and co-teaching methods, we propose a novel training procedure to mitigate the memorization of noisy labels, called CrossSplit, which uses a pair of neural networks trained on two disjoint parts of the labelled dataset. CrossSplit combines two main ingredients: (i) Cross-split label correction. The idea is that, since the model trained on one part of the data cannot memorize example-label pairs from the other part, the training labels presented to each network can be smoothly adjusted by using the predictions of its peer network; (ii) Cross-split semi-supervised training. A network trained on one part of the data also uses the unlabeled inputs of the other part. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of no
    
[^132]: 基于QR分解的时间偏移选择技术在蓄水池计算中的应用

    Time-shift selection for reservoir computing using a rank-revealing QR algorithm. (arXiv:2211.17095v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.17095](http://arxiv.org/abs/2211.17095)

    本论文提出了一种基于QR分解的时间偏移选择技术，通过最大化蓄水池矩阵秩来提高性能准确性，在模拟硬件蓄水池计算中具有广泛应用。

    

    蓄水池计算是一种仅对输出层进行训练的递归神经网络，已经在非线性系统预测和控制等任务中表现出卓越的性能。最近，研究发现在蓄水池中加入时间偏移可以显著提高性能准确性。本文提出了一种使用QR分解算法最大化蓄水池矩阵秩来选择时间偏移的技术。该技术不依赖于特定任务，不需要系统模型，因此直接适用于模拟硬件蓄水池计算。我们将时间偏移选择技术应用于两种类型的蓄水池计算：基于光电振荡器和传统递归网络的$tanh$激活函数。研究结果表明，相比随机选择时间偏移，我们的技术在几乎所有情况下提供了更高的准确性。

    Reservoir computing, a recurrent neural network paradigm in which only the output layer is trained, has demonstrated remarkable performance on tasks such as prediction and control of nonlinear systems. Recently, it was demonstrated that adding time-shifts to the signals generated by a reservoir can provide large improvements in performance accuracy. In this work, we present a technique to choose the time-shifts by maximizing the rank of the reservoir matrix using a rank-revealing QR algorithm. This technique, which is not task dependent, does not require a model of the system, and therefore is directly applicable to analog hardware reservoir computers. We demonstrate our time-shift selection technique on two types of reservoir computer: one based on an opto-electronic oscillator and the traditional recurrent network with a $tanh$ activation function. We find that our technique provides improved accuracy over random time-shift selection in essentially all cases.
    
[^133]: 利用鉴别器引导在基于评分的扩散模型中完善生成过程

    Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models. (arXiv:2211.17091v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.17091](http://arxiv.org/abs/2211.17091)

    本文提出了“鉴别器引导”方法，通过在评分训练之后训练鉴别器，使模型评估更加准确，从而改善预训练扩散模型的样本生成。在 ImageNet 256x256 数据集上实现了 FID 1.83 和召回率 0.64 的最新结果，类似于验证数据的 FID 和召回率。

    

    本文提出的“鉴别器引导”方法旨在改善预训练扩散模型的样本生成。该方法引入了一个鉴别器，明确地监督去噪样本路径是否真实。与 GAN 不同的是，我们的方法不需要联合训练评分和鉴别器网络。相反，在评分训练之后训练鉴别器，使鉴别器训练稳定且快速收敛。在样本生成中，我们向预训练的评分添加一个辅助项以欺骗鉴别器。该项将模型评分矫正为最优鉴别器处的数据评分，这意味着鉴别器以补充的方式帮助更好地评估分数。使用我们的算法，在 ImageNet 256x256 上实现了 FID 1.83 和召回率 0.64 的最新结果，类似于验证数据的 FID（1.68）和召回率（0.66）。我们在 https://github.com/alsdudrla10/DG 上公开了代码。

    The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.
    
[^134]: SimVP: 较为简单却强大的时空预测学习的探索

    SimVP: Towards Simple yet Powerful Spatiotemporal Predictive Learning. (arXiv:2211.12509v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12509](http://arxiv.org/abs/2211.12509)

    SimVP是一种简单且强大的时空预测学习基准模型，完全基于卷积神经网络构建且没有循环结构，并且使用均方误差损失为训练目标。SimVP在各种基准数据集上均能取得优异的表现，并且具有强大的泛化性和可扩展性。

    

    近年来，时空预测学习在辅助输入、复杂神经结构和精细化训练策略等方面取得了显著进展。然而，目前主流方法的系统复杂性不断增加，这可能会妨碍它们的便捷应用。本文提出SimVP，一种完全基于卷积神经网络构建且没有循环结构、以均方误差损失为训练目标的简单时空预测基准模型。不需要任何额外的技巧和策略，SimVP在各种基准数据集上都能取得优异的表现。为了进一步提高性能，我们从SimVP中提取了具有门式时空注意力翻译器的变体，可以实现更好的性能。通过大量实验，我们证明了SimVP在真实世界数据集上具有强大的泛化性和可扩展性。 而且训练成本显著降低了。

    Recent years have witnessed remarkable advances in spatiotemporal predictive learning, incorporating auxiliary inputs, elaborate neural architectures, and sophisticated training strategies. Although impressive, the system complexity of mainstream methods is increasing as well, which may hinder the convenient applications. This paper proposes SimVP, a simple spatiotemporal predictive baseline model that is completely built upon convolutional networks without recurrent architectures and trained by common mean squared error loss in an end-to-end fashion. Without introducing any extra tricks and strategies, SimVP can achieve superior performance on various benchmark datasets. To further improve the performance, we derive variants with the gated spatiotemporal attention translator from SimVP that can achieve better performance. We demonstrate that SimVP has strong generalization and extensibility on real-world datasets through extensive experiments. The significant reduction in training cos
    
[^135]: 实践中的隐私：X射线图像中的COVID-19检测的私有化（扩展版）

    Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version). (arXiv:2211.11434v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11434](http://arxiv.org/abs/2211.11434)

    该研究提出了通过差分隐私保护COVID-19检测模型，解决数据分析和患者隐私保护的问题。通过黑盒成员推理攻击，实现了对实际隐私的评估，结论表明所需的隐私等级可能因受到实际威胁的任务而异。

    

    机器学习（ML）可以通过使大量图像快速筛选来帮助抗击COVID-19等全球大流行病。为了在保护患者隐私的同时进行数据分析，我们创建了满足差分隐私（DP）要求的ML模型。以往探索私有COVID-19模型的研究在一定程度上基于小型数据集，提供较弱或不明确的隐私保证，并且没有研究实际隐私。我们提出改进措施以解决这些空缺。我们考虑天生的类别不平衡，并更广泛地评估效用-隐私权衡以及更严格的隐私预算。我们的评估得到黑盒成员推理攻击（MIAs）的实际隐私估计支持。引入的DP应有助于限制MIAs带来的泄漏威胁，而我们的实际分析是第一个在COVID-19分类任务上测试这个假设的。

    Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat 
    
[^136]: CRONOS：基于Wi-Fi CSI的无人设备NLoS人体检测的彩色化和对比学习

    CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI. (arXiv:2211.10354v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.10354](http://arxiv.org/abs/2211.10354)

    本文介绍了一种名为CRONOS的系统，可以通过彩色化和对比学习来基于Wi-Fi CSI实现无人设备NLoS人体检测，可以区分房间中的移动人员和空置。实验结果表明该系统在NLoS条件下能够准确地检测出房间中的人物存在。

    

    近年来，对于全面智能化服务和应用的需求迅速增长。通过传感器或摄像头进行无人检测已被广泛采用，但存在隐私问题以及对静止人员的错误检测。为了解决这些缺点，商用Wi-Fi设备捕获的信道状态信息(CSI)提供了丰富的信号特征，以进行准确的检测。然而，现有系统在非直视(NLoS)和静态场景下存在分类不准确的问题，例如当一个人静止站在房间角落时。在本文中，我们提出了一个名为CRONOS(基于彩色化和对比度学习增强的NLoS人体存在检测)的系统，它生成动态的复发图(RPs)和颜色编码的CSI比率以区分房间中的移动人员和空置。我们还结合监督对比学习来检索实质性的表征，其中咨询损失被制定为区分同类和异类的嵌入点之间距离度量的损失。在数据集上的实验结果表明，提出的系统在NLoS条件下能够准确地检测出房间中的人物存在。

    In recent years, the demand for pervasive smart services and applications has increased rapidly. Device-free human detection through sensors or cameras has been widely adopted, but it comes with privacy issues as well as misdetection for motionless people. To address these drawbacks, channel state information (CSI) captured from commercialized Wi-Fi devices provides rich signal features for accurate detection. However, existing systems suffer from inaccurate classification under a non-line-of-sight (NLoS) and stationary scenario, such as when a person is standing still in a room corner. In this work, we propose a system called CRONOS (Colorization and Contrastive Learning Enhanced NLoS Human Presence Detection), which generates dynamic recurrence plots (RPs) and color-coded CSI ratios to distinguish mobile people from vacancy in a room, respectively. We also incorporate supervised contrastive learning to retrieve substantial representations, where consultation loss is formulated to dif
    
[^137]: 机制模式连通性

    Mechanistic Mode Connectivity. (arXiv:2211.08422v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08422](http://arxiv.org/abs/2211.08422)

    本文从模式连通性的视角研究神经网络损失景观，提出了机制相似性的定义，并演示了两个模型之间缺乏线性连通性意味着它们使用不同的机制来做出预测。此外，本文还提出了一种名为基于连通性的微调（CBFT）的方法，用于目标修改模型机制，有助于消除模型对虚假特征的依赖。

    

    我们从模式连通性的视角研究神经网络损失景观，即通过在数据集上训练而检索到的神经网络的最小化器通过低损失的简单路径相互连接。具体而言，我们提出了机制相似性的定义，即与输入转换的共享不变性，并演示了两个模型之间缺乏线性连通性意味着它们使用不同的机制来做出预测。与实践相关的是，这个结果帮助我们证明了在下游数据集上的简单微调可能无法改变模型的机制，例如，微调可能无法消除模型对虚假特征的依赖。我们的分析还推动了一种名为基于连通性的微调（CBFT）的目标变化模型机制的方法，我们使用几个合成数据集对其进行了分析。

    We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model's mechanisms, e.g., fine-tuning can fail to eliminate a model's reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model's mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthe
    
[^138]: RecD：为端到端深度学习推荐模型训练基础设施提供去重功能

    RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure. (arXiv:2211.05239v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05239](http://arxiv.org/abs/2211.05239)

    RecD 是一种为 DLRM 训练提供去重功能的端到端基础设施优化，解决了由于特征重复造成的海量存储、预处理和训练开销，引入了新的张量格式 InverseKeyedJaggedTensors (IKJTs) 来去除特征值的重复，使 DLRM 模型架构能够更好地利用数据的重复性提高训练吞吐量。

    

    我们提出了 RecD（推荐去重），它是一组针对深度学习推荐模型 (DLRM) 训练流程的端到端基础设施优化。RecD解决了由于特征重复造成的海量存储、预处理和训练开销，这是大规模 DLRM 训练数据集内在的问题，因为 DLRM 数据集是从交互中生成的。我们展示了 RecD 如何利用此属性来优化生产数据的流程，减少数据集存储和预处理需求，并最大限度地在训练批次中重复。RecD 引入了一种新的张量格式 InverseKeyedJaggedTensors (IKJTs)，以在每个批次中去除特征值的重复。我们展示了 DLRM 模型架构如何利用 IKJTs 来显著提高训练吞吐量。

    We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. Re
    
[^139]: 安全潜向扩散：缓解扩散模型中不当退化

    Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. (arXiv:2211.05105v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.05105](http://arxiv.org/abs/2211.05105)

    该论文提出了一种名为安全潜向扩散的方法，可以在图像生成过程中移除和抑制不当的图像部分，从而缓解基于文本的图像生成模型因不当数据集带来的不良影响。

    

    最近，基于文本的图像生成模型在图像质量和文本对齐方面取得了惊人的成果，并因此被广泛应用于越来越多的应用程序。由于它们高度依赖于随机从互联网上抓取的数十亿大小的数据集，因此它们也面临来自退化和偏见的人类行为的不良影响，正如我们所展示的那样。反过来，它们甚至可能强化这些偏见。为了帮助应对这些不良影响，我们提出了安全潜向扩散（SLD）。具体而言，为了衡量由于未过滤和不平衡的训练集而引起的不当退化，我们建立了一个新颖的图像生成测试平台——包含专门的、覆盖裸露和暴力等概念的实际图像到文本提示的不当图像提示（I2P）。正如我们详尽的实证评估所证明的那样，引入的SLD在扩散过程中移除和抑制了不当的图像部分，无需额外的训练，并且对图像质量没有不良影响。

    Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse e
    
[^140]: 机器学习时代MRI数据协调的有效性：一项跨36个数据集的多中心研究

    Efficacy of MRI data harmonization in the age of machine learning. A multicenter study across 36 datasets. (arXiv:2211.04125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04125](http://arxiv.org/abs/2211.04125)

    本文描述了一项多中心研究，旨在评估MRI数据协调在机器学习中的效用;作者提出了一种“调和器变压器”方法，在不泄露信息的前提下，在机器学习的预处理步骤中实现了数据协调。

    

    从多个网站汇集公开可用的MRI数据可以组装大量受试对象，增加统计功率，并通过机器学习技术促进数据重用。多中心数据的协调是减少数据中与非生物来源的变异度量的混杂效应的必要条件。然而，将协调应用于机器学习之前的整个数据集会导致数据泄漏，因为训练集之外的信息可能会影响模型构建并可能导致性能过高。我们提出了1）数据协调的有效性测量方法；2）“调和器变压器”，即ComBat协调方法的一个实现，它允许将其封装在机器学习的预处理步骤中，避免数据泄漏。我们使用了来自36个网站的1740名健康受试者的大脑T1加权MRI数据来测试这些工具。经过协调后，网站效应被删除或减少了，

    Pooling publicly-available MRI data from multiple sites allows to assemble extensive groups of subjects, increase statistical power, and promote data reuse with machine learning techniques. The harmonization of multicenter data is necessary to reduce the confounding effect associated with non-biological sources of variability in the data. However, when applied to the entire dataset before machine learning, the harmonization leads to data leakage, because information outside the training set may affect model building, and potentially falsely overestimate performance. We propose a 1) measurement of the efficacy of data harmonization; 2) harmonizer transformer, i.e., an implementation of the ComBat harmonization allowing its encapsulation among the preprocessing steps of a machine learning pipeline, avoiding data leakage. We tested these tools using brain T1-weighted MRI data from 1740 healthy subjects acquired at 36 sites. After harmonization, the site effect was removed or reduced, and 
    
[^141]: FingerFlex：从ECoG信号推断手指轨迹

    FingerFlex: Inferring Finger Trajectories from ECoG signals. (arXiv:2211.01960v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2211.01960](http://arxiv.org/abs/2211.01960)

    本文介绍了FingerFlex模型，该模型通过深度学习算法在ECoG信号上实现手指运动回归，取得了最先进的表现，为开发高精度皮层运动脑机接口提供了机会。

    

    运动脑机接口（BCI）的发展关键在于神经时间序列解码算法。深度学习结构的最新进展允许自动选择特征以逼近数据中的高阶依赖关系。本文提出了FingerFlex模型——一种卷积编码器-解码器架构，用于在电导层图（ECoG）脑数据上进行手指运动回归。在公开可用的BCI竞赛IV数据集4上实现了最先进的表现，真实和预测轨迹之间的相关系数高达0.74。该方法为开发完全功能的高精度皮层运动脑机接口提供了机会。

    Motor brain-computer interface (BCI) development relies critically on neural time series decoding algorithms. Recent advances in deep learning architectures allow for automatic feature selection to approximate higher-order dependencies in data. This article presents the FingerFlex model - a convolutional encoder-decoder architecture adapted for finger movement regression on electrocorticographic (ECoG) brain data. State-of-the-art performance was achieved on a publicly available BCI competition IV dataset 4 with a correlation coefficient between true and predicted trajectories up to 0.74. The presented method provides the opportunity for developing fully-functional high-precision cortical motor brain-computer interfaces.
    
[^142]: 使用多尺度非负核图研究流形几何

    Study of Manifold Geometry using Multiscale Non-Negative Kernel Graphs. (arXiv:2210.17475v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17475](http://arxiv.org/abs/2210.17475)

    本文提出了一个框架，利用非负核图的方法研究数据流形的几何结构，并且通过多尺度的迭代合并构建出更准确的结构，实验证明了其有效性。

    

    现代机器学习系统越来越倾向于在高维空间中嵌入大量数据进行训练，然而通常这样做时并没有对数据集的结构进行分析。在本文中，我们提出了一个研究数据几何结构的框架。我们利用我们最近引入的非负核回归 (NNK) 图来估计数据流形的点密度、内在维度和线性性 (曲率)。我们进一步将图构造和几何估计推广到多个尺度，通过迭代合并输入数据中的邻域。我们的实验证明了我们所提出的方法在合成数据集和真实数据集上估计数据流形的局部几何的有效性，超过了其他基线方法。

    Modern machine learning systems are increasingly trained on large amounts of data embedded in high-dimensional spaces. Often this is done without analyzing the structure of the dataset. In this work, we propose a framework to study the geometric structure of the data. We make use of our recently introduced non-negative kernel (NNK) regression graphs to estimate the point density, intrinsic dimension, and the linearity of the data manifold (curvature). We further generalize the graph construction and geometric estimation to multiple scale by iteratively merging neighborhoods in the input data. Our experiments demonstrate the effectiveness of our proposed approach over other baselines in estimating the local geometry of the data manifolds on synthetic and real datasets.
    
[^143]: 学习图像表示以进行异常检测：在药物开发中发现组织学改变的应用

    Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.07675](http://arxiv.org/abs/2210.07675)

    该论文提出了一种基于CNN的异常检测系统，通过对健康组织进行辅助任务训练，使表示适应组织中的相关细节，实现对组织学图像中的异常情况检测。

    

    我们提出了一种用于组织病理学图像异常检测的系统。在组织学中，正常样本通常是大量存在的，而异常（病理）情况通常很少或不可用。在这种情况下，使用在健康数据上训练的单类分类器可以检测到分布外的异常样本。这样的方法与预训练的卷积神经网络（CNN）图像表示相结合，以前已经用于异常检测（AD）。但是，预训练的现成CNN表示可能对组织中的异常情况不敏感，而健康组织的自然变异可能导致远离的表示。为了使表示适应健康组织中的相关细节，我们建议在辅助任务上训练CNN，该任务区分不同物种、器官和染色试剂的健康组织。几乎不需要额外的标注工作量，因为健康样本可以自动获得上述标签。在训练中，我们强制执行

    We present a system for anomaly detection in histopathological images. In histology, normal samples are usually abundant, whereas anomalous (pathological) cases are scarce or not available. Under such settings, one-class classifiers trained on healthy data can detect out-of-distribution anomalous samples. Such approaches combined with pre-trained Convolutional Neural Network (CNN) representations of images were previously employed for anomaly detection (AD). However, pre-trained off-the-shelf CNN representations may not be sensitive to abnormal conditions in tissues, while natural variations of healthy tissue may result in distant representations. To adapt representations to relevant details in healthy tissue we propose training a CNN on an auxiliary task that discriminates healthy tissue of different species, organs, and staining reagents. Almost no additional labeling workload is required, since healthy samples come automatically with aforementioned labels. During training we enforce
    
[^144]: 高斯过程的计算有效初始化：广义变异函数方法

    Computationally-efficient initialisation of GPs: The generalised variogram method. (arXiv:2210.05394v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05394](http://arxiv.org/abs/2210.05394)

    该论文提出了一种计算有效的策略，避免了计算似然函数，可用作初始化超参数，使模型得到更好的训练，该策略在实践中证明了对于高斯过程是有效的。

    

    我们提出了一种旨在初始化高斯过程（GP）超参数的计算有效策略，避免了计算似然函数。我们的策略可以用作预训练阶段，以查找最大似然（ML）训练的初始条件，或作为独立方法来计算超参数值，以直接插入GP模型中。我们受到这样一个事实的启发，即通过ML训练来训练GP与最小化真实模型和学习模型之间的KL散度（平均而言）是等效的，因此我们开始探索不同的GP度量/散度，它们计算起来廉价，并提供接近通过ML发现的超参数值。在实践中，我们通过将经验协方差或（傅里叶）功率谱投影到参数族上来识别GP超参数，因此提出并研究在时间和频率域上运作的各种差异度量。我们的贡献扩展了由th开发的变异函数方法

    We present a computationally-efficient strategy to initialise the hyperparameters of a Gaussian process (GP) avoiding the computation of the likelihood function. Our strategy can be used as a pretraining stage to find initial conditions for maximum-likelihood (ML) training, or as a standalone method to compute hyperparameters values to be plugged in directly into the GP model. Motivated by the fact that training a GP via ML is equivalent (on average) to minimising the KL-divergence between the true and learnt model, we set to explore different metrics/divergences among GPs that are computationally inexpensive and provide hyperparameter values that are close to those found via ML. In practice, we identify the GP hyperparameters by projecting the empirical covariance or (Fourier) power spectrum onto a parametric family, thus proposing and studying various measures of discrepancy operating on the temporal and frequency domains. Our contribution extends the variogram method developed by th
    
[^145]: 用BSMS-GNN高效学习基于网格的物理仿真

    Efficient Learning of Mesh-Based Physical Simulation with BSMS-GNN. (arXiv:2210.02573v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02573](http://arxiv.org/abs/2210.02573)

    该论文提出了一种名为bi-stride的新型池化策略，利用二部图决策实现高效的基于网格的物理仿真，无需手动绘制粗网格，并避免了空间接近性带来的错误边缘。

    

    基于平面图神经网络（GNNs）和堆叠的消息传递（MPs）学习大规模网格上的物理仿真具有与节点数量相关的缩放复杂度和过度平滑的挑战。引入“多尺度”结构到GNNs以进行物理仿真引起了社区的越来越多的兴趣。然而，当前最先进的方法受到依赖于繁琐的绘制粗网格或基于空间邻近性构建粗略级别的限制，这可能在几何边界处引入错误的边缘。受到二部图决策的启发，我们提出了一种新的汇集策略——双步跨（bi-stride），以解决上述限制。当进行广度优先搜索（BFS）时，双步跨在每个其他前沿的节点上进行池化，而无需手动绘制较粗的网格，并通过空间邻近性避免错误的边缘。此外，它还实现了每层的单一MP方案和无参数的池化。

    Learning the physical simulation on large-scale meshes with flat Graph Neural Networks (GNNs) and stacking Message Passings (MPs) is challenging due to the scaling complexity w.r.t. the number of nodes and over-smoothing. There has been growing interest in the community to introduce \textit{multi-scale} structures to GNNs for physical simulation. However, current state-of-the-art methods are limited by their reliance on the labor-intensive drawing of coarser meshes or building coarser levels based on spatial proximity, which can introduce wrong edges across geometry boundaries. Inspired by the bipartite graph determination, we propose a novel pooling strategy, \textit{bi-stride} to tackle the aforementioned limitations. Bi-stride pools nodes on every other frontier of the breadth-first search (BFS), without the need for the manual drawing of coarser meshes and avoiding the wrong edges by spatial proximity. Additionally, it enables a one-MP scheme per level and non-parametrized pooling 
    
[^146]: FADE：实现异构资源受限边缘设备上联邦对抗训练

    FADE: Enabling Federated Adversarial Training on Heterogeneous Resource-Constrained Edge Devices. (arXiv:2209.03839v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.03839](http://arxiv.org/abs/2209.03839)

    这篇论文提出了一个名为FADE的联邦对抗训练框架，能够在资源受限的边缘设备上实现对抗训练，框架将整个模型差分分解成小模块来适应设备的资源预算，并提出辅助权重衰减方法以获得更好的准确度-鲁棒性平衡。

    

    联邦对抗训练能够有效将对抗性鲁棒性引入保护隐私的联邦学习系统。然而，由于内存容量和计算资源的高需求，大规模联邦对抗训练在资源受限的边缘设备上无法实现。本文提出一种名为FADE的新框架，用于实现异构资源受限边缘设备上的对抗训练。FADE将整个模型差分分解成小模块以适应每个设备的资源预算，每个设备每次通信只需要对单个模块执行对抗训练。我们还提出了一种辅助权重衰减方法，在FADE中缓解目标不一致性并实现更好的准确度-鲁棒性平衡。FADE在收敛和对抗的收敛率方面提供了理论保证。

    Federated adversarial training can effectively complement adversarial robustness into the privacy-preserving federated learning systems. However, the high demand for memory capacity and computing power makes large-scale federated adversarial training infeasible on resource-constrained edge devices. Few previous studies in federated adversarial training have tried to tackle both memory and computational constraints simultaneously. In this paper, we propose a new framework named Federated Adversarial Decoupled Learning (FADE) to enable AT on heterogeneous resource-constrained edge devices. FADE differentially decouples the entire model into small modules to fit into the resource budget of each device, and each device only needs to perform AT on a single module in each communication round. We also propose an auxiliary weight decay to alleviate objective inconsistency and achieve better accuracy-robustness balance in FADE. FADE offers theoretical guarantees for convergence and adversarial 
    
[^147]: 基于Transformer的高分子属性预测语言模型TransPolymer

    TransPolymer: a Transformer-based language model for polymer property predictions. (arXiv:2209.01307v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.01307](http://arxiv.org/abs/2209.01307)

    本文提出了一种基于Transformer的高分子属性预测语言模型TransPolymer，它利用化学感知能力的高分子分词器学习高分子序列的表示，并通过预训练获得了更好的性能。

    

    在高分子设计中，准确且高效地预测高分子的属性具有重要意义。传统上，需要进行昂贵且耗时的实验或模拟才能评估高分子的功能。最近，在自注意力机制的支持下，Transformer模型在自然语言处理方面表现出了出色的性能。然而，这种方法在高分子科学中尚未得到研究。在此，我们报道了TransPolymer，一种基于Transformer的高分子属性预测语言模型。我们提出了一种具有化学感知能力的高分子分词器，使其能够学习高分子序列的表示。在十个高分子属性预测基准测试中的严格实验表明了TransPolymer的优越性能。此外，我们展示了TransPolymer从大型无标记数据集的预训练中受益。实验结果进一步表明了自注意力在建模高分子序列中的重要作用。我们强调了这一点。

    Accurate and efficient prediction of polymer properties is of great significance in polymer design. Conventionally, expensive and time-consuming experiments or simulations are required to evaluate polymer functions. Recently, Transformer models, equipped with self-attention mechanisms, have exhibited superior performance in natural language processing. However, such methods have not been investigated in polymer sciences. Herein, we report TransPolymer, a Transformer-based language model for polymer property prediction. Our proposed polymer tokenizer with chemical awareness enables learning representations from polymer sequences. Rigorous experiments on ten polymer property prediction benchmarks demonstrate the superior performance of TransPolymer. Moreover, we show that TransPolymer benefits from pretraining on large unlabeled dataset via Masked Language Modeling. Experimental results further manifest the important role of self-attention in modeling polymer sequences. We highlight this
    
[^148]: 基于归一化流和对比数据的图像异常检测中的正差分布

    Positive Difference Distribution for Image Outlier Detection using Normalizing Flows and Contrastive Data. (arXiv:2208.14024v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14024](http://arxiv.org/abs/2208.14024)

    本研究提出了一种使用未标记的辅助数据集和概率异常得分进行异常检测的方法，可以有效地检测与训练数据有偏差的测试数据。该方法使用自监督特征提取器训练，通过学习正差分布来提高检测效果并在基准数据集上进行验证。

    

    检测与训练数据有偏差的测试数据是安全和稳健机器学习的一个中心问题。通过标准对数似然训练，例如归一化流学习的可能性作为异常得分效果不好。本文提出使用未标记的辅助数据集和概率异常得分进行异常检测。我们使用自监督特征提取器对辅助数据集进行训练，并通过最大化在分布数据上的可能性和最小化在对比数据上的可能性训练提取特征的归一化流。我们证明这相当于学习分布数据和对比数据之间的正常化正差分布。我们在基准数据集上进行实验，并将其与似然、似然比和最先进的异常检测方法进行比较。

    Detecting test data deviating from training data is a central problem for safe and robust machine learning. Likelihoods learned by a generative model, e.g., a normalizing flow via standard log-likelihood training, perform poorly as an outlier score. We propose to use an unlabelled auxiliary dataset and a probabilistic outlier score for outlier detection. We use a self-supervised feature extractor trained on the auxiliary dataset and train a normalizing flow on the extracted features by maximizing the likelihood on in-distribution data and minimizing the likelihood on the contrastive dataset. We show that this is equivalent to learning the normalized positive difference between the in-distribution and the contrastive feature density. We conduct experiments on benchmark datasets and compare to the likelihood, the likelihood ratio and state-of-the-art anomaly detection methods.
    
[^149]: 一对其余损失函数在对抗训练中聚焦重要样本的作用

    One-vs-the-Rest Loss to Focus on Important Samples in Adversarial Training. (arXiv:2207.10283v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10283](http://arxiv.org/abs/2207.10283)

    本文提出了一种名为SOVR的对抗训练损失函数，可以聚焦重要样本，增加对抗攻击下的对数几率间隔，从而在实验中表现出对抗攻击的有效性。

    

    本文提出了一种新的对抗训练损失函数。由于对抗训练存在困难，如需要高模型容量，通过加权交叉熵损失关注重要数据点已引起广泛关注。然而，它们容易受到复杂攻击的影响，如Auto-Attack。本文实验表明，它们的易受攻击的原因是真实标签和其他标签之间的对数几率之间的较小间隔。由于神经网络是根据对数几率对数据点进行分类的，所以对数几率的间隔应该足够大，以避免攻击翻转最大的对数几率。重要性感知方法不会增加重要样本的对数几率间隔，但与交叉熵损失相比会减少较不重要样本的对数几率间隔。为了增加重要样本的对数几率间隔，我们提出了一种切换一对其余（SOVR）损失函数，该损失函数在具有较小对数几率间隔的重要样本中从交叉熵切换到一对其余损失。我们提供理论分析、消融研究和实验，证明SOVR对抗抗击和其他最先进的攻击方法的有效性。

    This paper proposes a new loss function for adversarial training. Since adversarial training has difficulties, e.g., necessity of high model capacity, focusing on important data points by weighting cross-entropy loss has attracted much attention. However, they are vulnerable to sophisticated attacks, e.g., Auto-Attack. This paper experimentally reveals that the cause of their vulnerability is their small margins between logits for the true label and the other labels. Since neural networks classify the data points based on the logits, logit margins should be large enough to avoid flipping the largest logit by the attacks. Importance-aware methods do not increase logit margins of important samples but decrease those of less-important samples compared with cross-entropy loss. To increase logit margins of important samples, we propose switching one-vs-the-rest loss (SOVR), which switches from cross-entropy to one-vs-the-rest loss for important samples that have small logit margins. We prov
    
[^150]: 使用像素的语言建模

    Language Modelling with Pixels. (arXiv:2207.06991v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.06991](http://arxiv.org/abs/2207.06991)

    本文介绍了一个名为PIXEL的基于像素的预训练语言模型，它可以将文本渲染为图像，解决了扩展支持的语言数量时出现的词汇瓶颈问题，且在形态学和语义任务上显著优于BERT。

    

    语言模型在有限的输入集合上进行定义，这导致在尝试扩展支持的语言数量时出现词汇瓶颈。解决这个瓶颈会导致在嵌入矩阵中所能表示的内容与输出层的计算问题之间的权衡。本文引入了名为PIXEL的基于像素的语言编码器。PIXEL是预训练的语言模型，可以将文本渲染为图像，从而可以基于拼写相似性或像素的共同激活来跨语言传递表示。PIXEL训练时不是预测标记分布，而是重构被屏蔽的块的像素。我们对与BART相同的英语数据进行了86M参数PIXEL模型的预训练，并在形态学和语义任务上进行了评估，涵盖了不同类型的语言，包括各种非拉丁文字。我们发现，在语法和语义处理方面，PIXEL的性能显著优于BERT。

    Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing
    
[^151]: 基于不变叶片、流形和自编码器的数据驱动简化模型

    Data-driven reduced order models using invariant foliations, manifolds and autoencoders. (arXiv:2206.12269v3 [math.DS] UPDATED)

    [http://arxiv.org/abs/2206.12269](http://arxiv.org/abs/2206.12269)

    本文研究了从物理系统中识别简化模型的方法，发现使用不变叶片是从现有数据中识别ROM的常见方法。其余方法只能识别完整模型。研究表明找到不变叶片需要近似高维函数，使用带有压缩张量系数的多项式即可。

    

    本文研究了如何从物理系统中识别出简化模型（ROM）。ROM捕捉了观察到的动态的不变子集。我们发现，物理系统与数学模型有四种关系：不变叶片、不变流形、自编码器和无方程模型。识别不变流形和无方程模型需要对系统进行闭环操作。不变叶片和自编码器也可以使用离线数据。只有不变叶片和不变流形可以识别ROM，其余只能识别完整模型。因此，从现有数据中识别ROM的常见情况只能使用不变叶片。找到不变叶片需要近似高维函数。在函数逼近中，我们使用带有压缩张量系数的多项式，其复杂性随维数增加而线性增加。不变流形也可以被找到作为修正但不完全刻画ROM的手段。

    This paper explores how to identify a reduced order model (ROM) from a physical system. A ROM captures an invariant subset of the observed dynamics. We find that there are four ways a physical system can be related to a mathematical model: invariant foliations, invariant manifolds, autoencoders and equation-free models. Identification of invariant manifolds and equation-free models require closed-loop manipulation of the system. Invariant foliations and autoencoders can also use off-line data. Only invariant foliations and invariant manifolds can identify ROMs, the rest identify complete models. Therefore, the common case of identifying a ROM from existing data can only be achieved using invariant foliations.  Finding an invariant foliation requires approximating high-dimensional functions. For function approximation, we use polynomials with compressed tensor coefficients, whose complexity increases linearly with increasing dimensions. An invariant manifold can also be found as the fix
    
[^152]: 通过模型变换实现灵活可导优化

    Flexible Differentiable Optimization via Model Transformations. (arXiv:2206.06135v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06135](http://arxiv.org/abs/2206.06135)

    本文介绍了DiffOpt.jl，一个基于MathOptInterface构建的Julia库，它可以对任何模型的参数进行微分求解，不仅限于凸锥规划和二次规划标准形式。使用该库, 可以实现灵活高效地求解一系列模型的优化问题，为基于优化的机器学习提供了新的机会。

    

    本文介绍了DiffOpt.jl，这是一个Julia库，它可以对包含目标值和/或约束条件的任意参数进行优化求解，并进行微分。这个库基于MathOptInterface构建，因此可以利用丰富的求解器生态系统，并与像JuMP这样的建模语言组合得很好。DiffOpt提供前向和反向微分模式，使得可以应用于超参数优化、反向传播和敏感性分析等多种用途，通过将约束优化与端到端可导性编程相结合。DiffOpt是基于两个已知的规则来求解的，分别是凸锥规划和二次规划标准形式的微分。然而，由于可以通过模型转换进行微分，因此用户不仅限于这些形式，还可以针对可以转换为这些标准形式的任何模型的参数进行微分。这特别包括混合仿射锥约束和凸四次约束的程序，这些程序无法直接建模为标准锥形程序。因此，DiffOpt使得可以灵活高效地求解一系列模型的优化问题，为基于优化的机器学习提供了新的机会。

    We introduce DiffOpt.jl, a Julia library to differentiate through the solution of optimization problems with respect to arbitrary parameters present in the objective and/or constraints. The library builds upon MathOptInterface, thus leveraging the rich ecosystem of solvers and composing well with modeling languages like JuMP. DiffOpt offers both forward and reverse differentiation modes, enabling multiple use cases from hyperparameter optimization to backpropagation and sensitivity analysis, bridging constrained optimization with end-to-end differentiable programming. DiffOpt is built on two known rules for differentiating quadratic programming and conic programming standard forms. However, thanks ability to differentiate through model transformation, the user is not limited to these forms and can differentiate with respect to the parameters of any model that can be reformulated into these standard forms. This notably includes programs mixing affine conic constraints and convex quadrat
    
[^153]: 弹性距离函数在时间序列聚类中的综述和评估

    A Review and Evaluation of Elastic Distance Functions for Time Series Clustering. (arXiv:2205.15181v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15181](http://arxiv.org/abs/2205.15181)

    本文针对时间序列聚类中的弹性距离度量算法进行了综述和评估，发现常用的距离度量方法之一-DTW，在与k-means结合时表现不如欧几里得距离。使用k-medoids可以改善聚类效果。

    

    时间序列聚类是将时间序列数据分组的行为，不需要标签。聚类时间序列数据的算法可以分为两组：使用时间序列特定距离度量的算法和从时间序列推导特征的算法。两种方法通常都依赖于传统的聚类算法，比如$k$-means。我们的重点是基于弹性距离度量的时间序列距离算法，即在测量距离时执行某种实际对其操作的距离度量。我们描述了九种常用的弹性距离度量，并将它们与k-means和k-medoids聚类的性能进行了比较。我们的发现令人意外。最受欢迎的技术-动态时间规整(DTW)，在与k-means结合时比欧几里得距离表现更差，即使进行了调整，效果也不好。使用k-medoids而不是k-means对所有九个距离度量都改善了聚类效果。DTW与k-medoids中的欧几里得距离没有显著差异。一般而言，距离度量方法的影响取决于聚类的算法和数据集的特性。

    Time series clustering is the act of grouping time series data without recourse to a label. Algorithms that cluster time series can be classified into two groups: those that employ a time series specific distance measure; and those that derive features from time series. Both approaches usually rely on traditional clustering algorithms such as $k$-means. Our focus is on distance based time series that employ elastic distance measures, i.e. distances that perform some kind of realignment whilst measuring distance. We describe nine commonly used elastic distance measures and compare their performance with k-means and k-medoids clustering. Our findings are surprising. The most popular technique, dynamic time warping (DTW), performs worse than Euclidean distance with k-means, and even when tuned, is no better. Using k-medoids rather than k-means improved the clusterings for all nine distance measures. DTW is not significantly better than Euclidean distance with k-medoids. Generally, distanc
    
[^154]: 一个规则搜索框架用于早期识别慢性紧急无家可归者

    A Rule Search Framework for the Early Identification of Chronic Emergency Homeless Shelter Clients. (arXiv:2205.09883v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2205.09883](http://arxiv.org/abs/2205.09883)

    本研究使用规则搜索技术，早期识别有可能成为长期或慢性无家可归者收容所的高风险人群。在实时交付支持性住房计划的框架内，应用本文方法使得早期识别处于慢性无家可归者风险的客户的中位时间从297天降至162天。

    

    本文利用规则搜索技术，早期识别那些有可能成为长期或慢性住在无家可归者收容所的高风险人群。使用一家北美主要收容所服务与超过40,000人的12年交互数据集，采用优化修剪无序搜索(OPUS)算法，开发直观而有效的规则。在能够实现实时交付支持性住房计划的框架内，对这些规则进行了评估。结果表明，应用本文方法使得早期识别处于慢性无家可归者风险的客户的中位时间从297天降至162天。

    This paper uses rule search techniques for the early identification of emergency homeless shelter clients who are at risk of becoming long term or chronic shelter users. Using a data set from a major North American shelter containing 12 years of service interactions with over 40,000 individuals, the optimized pruning for unordered search (OPUS) algorithm is used to develop rules that are both intuitive and effective. The rules are evaluated within a framework compatible with the real-time delivery of a housing program meant to transition high risk clients to supportive housing. Results demonstrate that the median time to identification of clients at risk of chronic shelter use drops from 297 days to 162 days when the methods in this paper are applied.
    
[^155]: DECONET：一种基于分析稀疏性压缩感知的展开网络及其泛化误差界

    DECONET: an Unfolding Network for Analysis-based Compressed Sensing with Generalization Error Bounds. (arXiv:2205.07050v6 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2205.07050](http://arxiv.org/abs/2205.07050)

    本文提出了一种名为DECONET的新型深度展开网络，可以用于基于分析稀疏性的压缩感知，能有效地重构向量并优于现有的展开网络，在估计了其泛化误差的基础上得出相关结论。

    

    我们提出了一种新的深度展开网络DECONET，用于基于分析稀疏性的压缩感知。DECONET联合学习一个解码器，用于从不完整、噪声测量中重构向量，以及一个冗余的稀疏分析算子，该算子在DECONET的各个层之间共享。此外，我们构建了DECONET的假设类并估计其相关的Rademacher复杂度。然后，我们利用这个估计结果为DECONET提供有意义的上限，用于评估DECONET的泛化误差。最后，在合成和真实数据集上评估了我们的理论结果的有效性，并与现有的展开网络进行比较。实验结果表明，我们提出的网络在所有数据集上始终优于基线，并且其行为符合我们的理论发现。

    We present a new deep unfolding network for analysis-sparsity-based Compressed Sensing. The proposed network coined Decoding Network (DECONET) jointly learns a decoder that reconstructs vectors from their incomplete, noisy measurements and a redundant sparsifying analysis operator, which is shared across the layers of DECONET. Moreover, we formulate the hypothesis class of DECONET and estimate its associated Rademacher complexity. Then, we use this estimate to deliver meaningful upper bounds for the generalization error of DECONET. Finally, the validity of our theoretical results is assessed and comparisons to state-of-the-art unfolding networks are made, on both synthetic and real-world datasets. Experimental results indicate that our proposed network outperforms the baselines, consistently for all datasets, and its behaviour complies with our theoretical findings.
    
[^156]: 利用机器学习在急诊科分诊中检测败血症

    Detection of sepsis during emergency department triage using machine learning. (arXiv:2204.07657v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.07657](http://arxiv.org/abs/2204.07657)

    本研究利用机器学习开发出一种检测急诊科分诊前败血症的模型，其性能优于标准败血症筛查算法。

    

    身体器官功能障碍的败血症是全球死亡和危重疾病的主要原因之一。本研究的目的是比较标准败血症筛查算法和在电子健康记录分诊数据上训练的机器学习算法在急诊科分诊前（未使用实验室诊断）的败血症检测性能。研究得出机器学习模型的 AUC 为 0.9423，敏感性为 71.09%。

    Sepsis is a life-threatening condition with organ dysfunction and is a leading cause of death and critical illness worldwide. Even a few hours of delay in the treatment of sepsis results in increased mortality. Early detection of sepsis during emergency department triage would allow early initiation of lab analysis, antibiotic administration, and other sepsis treatment protocols. The purpose of this study was to compare sepsis detection performance at ED triage (prior to the use of laboratory diagnostics) of the standard sepsis screening algorithm (SIRS with source of infection) and a machine learning algorithm trained on EHR triage data. A machine learning model (KATE Sepsis) was developed using patient encounters with triage data from 16participating hospitals. KATE Sepsis and standard screening were retrospectively evaluated on the adult population of 512,949 medical records. KATE Sepsis demonstrates an AUC of 0.9423 (0.9401 - 0.9441) with sensitivity of 71.09% (70.12% - 71.98%) and
    
[^157]: 面向不同图类型的图神经网络综述

    Graph Neural Networks Designed for Different Graph Types: A Survey. (arXiv:2204.03080v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.03080](http://arxiv.org/abs/2204.03080)

    本综述详细介绍了已经存在的GNN，并根据其处理不同图类型和属性的能力对它们进行分类。

    

    图在自然界中无处不在，因此可以作为许多实际但也是理论问题的模型。为此，它们可以定义为适合反映所表示问题的各个上下文的许多不同类型。为了解决基于图数据的尖端问题，出现了图神经网络（GNN）研究领域。尽管该领域很年轻，新模型的开发速度很快，但许多最近的调查已经发布，以跟踪它们。然而，还没有收集到哪个GNN可以处理什么类型的图。在本调查中，我们详细介绍了已经存在的GNN，与以前的调查不同的是，我们根据它们处理不同图类型和属性的能力对它们进行分类。我们考虑操作具有不同结构构成的静态和动态图的GNN，具有或不具有节点或边属性。此外，我们区分离散时间或连续时间的GNN模型。

    Graphs are ubiquitous in nature and can therefore serve as models for many practical but also theoretical problems. For this purpose, they can be defined as many different types which suitably reflect the individual contexts of the represented problem. To address cutting-edge problems based on graph data, the research field of Graph Neural Networks (GNNs) has emerged. Despite the field's youth and the speed at which new models are developed, many recent surveys have been published to keep track of them. Nevertheless, it has not yet been gathered which GNN can process what kind of graph types. In this survey, we give a detailed overview of already existing GNNs and, unlike previous surveys, categorize them according to their ability to handle different graph types and properties. We consider GNNs operating on static and dynamic graphs of different structural constitutions, with or without node or edge attributes. Moreover, we distinguish between GNN models for discrete-time or continuou
    
[^158]: 基于变分指令集的量子编译优化：实现高精度与快速量子计算

    Quantum compiling with variational instruction set for accurate and fast quantum computing. (arXiv:2203.15574v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2203.15574](http://arxiv.org/abs/2203.15574)

    本文提出了基于灵活设计的多量子比特门操作的量子变分指令集(QuVIS)，通过细粒度的时间优化算法，实现了快速和精确的量子计算，并且通过实验展示出了更低的误差积累和时间成本。

    

    量子指令集(QIS)定义为在控制量子比特状态下可以物理实现的一系列量子门操作，其在量子计算中起着基础性作用。本文提出了基于灵活设计的多量子比特门操作的量子变分指令集(QuVIS)，通过细粒度的时间优化算法来变分实现量子比特的控制，从而实现快速和精确的量子计算。与标准QIS 如量子微指令集(QuMIS)相比，QuVIS 用于多量子比特交换和量子傅里叶变换等门操作具有更低的误差积累和时间成本。在相同量子硬件要求下，本文的方法可大幅提升量子计算的效率。

    The quantum instruction set (QIS) is defined as the quantum gates that are physically realizable by controlling the qubits in a quantum hardware. Compiling quantum circuits into the product of the gates in a properly-defined QIS is a fundamental step in quantum computing. We here propose the \R{quantum variational instruction set (QuVIS)} formed by flexibly-designed multi-qubit gates for higher speed and accuracy of quantum computing. The controlling of qubits for realizing the gates in a QuVIS are variationally achieved using the fine-grained time optimization algorithm. Significant reductions on both the error accumulation and time cost are demonstrated in realizing the swaps of multiple qubits and quantum Fourier transformations, compared with the compiling by the standard QIS such as \RR{the quantum microinstruction set} (QuMIS, formed by several one- and two-qubit gates including the one-qubit rotations and controlled-NOT gate). With the same requirement on quantum hardware, the t
    
[^159]: 基于星形细胞对关键期的神经可塑性神经网络，通过现有和记忆性的大脑可塑性和突触形成实现突触竞争和强度平衡。（arXiv: 2203.11740v12 [cs.NE] UPDATED）

    Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2203.11740](http://arxiv.org/abs/2203.11740)

    该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。

    

    除了突触共享连接权重之外，PNN还包括突触有效范围的权重[14-25]。PNN考虑突触强度平衡在突触吞噬的动态和长度常数之和的静态中[14]，并包含了鱼群行为的先导行为。突触形成在实验和模拟中会抑制树突生成[15]。类似于Spring Boot中的强制韧性，反向回路的记忆持久度梯度也存在。相对较好和较差的梯度信息存储在类似于脑褶的记忆痕迹细胞中，在反向回路的突触形成中。争议认为人类海马神经元的再生能力是否持续到老年，并可能在后期迭代中形成新的更长的回路[17,18]。关闭关键期会导致神经紊乱在实验和模拟中[19]。考虑到负面和正面记忆的持久性，有助于更好地激活突触。

    In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
    
[^160]: 基于幂律谱条件下的优化收敛率紧密上界

    Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions. (arXiv:2202.00992v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2202.00992](http://arxiv.org/abs/2202.00992)

    本文提出了一种新的谱条件，用于提供具有幂律优化轨迹的问题的更紧密上界，演示了如何统一获得最优加速方法及其计划和收敛上界。

    

    对于二次问题的优化性能，取决于谱的低能部分。对于大型（有效无限维）问题，这部分谱通常可以通过幂律分布自然表示或近似，导致梯度算法的迭代解表现出幂律收敛率。本文提出了一种新的谱条件，用于提供具有幂律优化轨迹的问题的更紧密上界。我们利用这个条件来建立一张广泛优化算法的上下界完整图像——梯度下降、最陡下降、重球、共轭梯度——并强调了学习率和动量的基本计划。特别的，我们演示了如何统一获得最优加速方法及其计划和收敛上界，对于给定谱形状。此外，我们还提供了对于首个证明。

    Performance of optimization on quadratic problems sensitively depends on the low-lying part of the spectrum. For large (effectively infinite-dimensional) problems, this part of the spectrum can often be naturally represented or approximated by power law distributions, resulting in power law convergence rates for iterative solutions of these problems by gradient-based algorithms. In this paper, we propose a new spectral condition providing tighter upper bounds for problems with power law optimization trajectories. We use this condition to build a complete picture of upper and lower bounds for a wide range of optimization algorithms -- Gradient Descent, Steepest Descent, Heavy Ball, and Conjugate Gradients -- with an emphasis on the underlying schedules of learning rate and momentum. In particular, we demonstrate how an optimally accelerated method, its schedule, and convergence upper bound can be obtained in a unified manner for a given shape of the spectrum. Also, we provide first proo
    
[^161]: 利用比特流元数据快速、准确、泛化地压缩视频质量增强

    Leveraging Bitstream Metadata for Fast, Accurate, Generalized Compressed Video Quality Enhancement. (arXiv:2202.00011v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.00011](http://arxiv.org/abs/2202.00011)

    本文提出了一种利用视频比特流元数据进行深度学习的压缩视频质量增强方法，在实现更高吞吐量的同时提高了压缩视频的还原准确性，具有重要的应用价值。

    

    视频压缩是现代互联网的核心功能，涵盖从社交媒体到视频会议等技术。虽然视频压缩不断成熟，但在许多压缩设置中，质量损失仍然很明显。然而，这些设置对于在带宽受限或不稳定的连接上高效传输视频具有重要应用。在本文中，我们开发了一个深度学习架构，能够利用嵌入视频比特流中的底层结构和动态信息，为压缩视频恢复细节。我们展示了这种方法相比于以前的压缩校正方法提高了恢复准确性，并且在实现更高吞吐量的同时，与最近的基于深度学习的视频压缩方法在速率-失真比上具有竞争力。此外，我们在模型上进行量化数据条件化，这可轻松地从比特流中获取。这使得我们的单一模型能够处理各种不同的压缩设置。

    Video compression is a central feature of the modern internet powering technologies from social media to video conferencing. While video compression continues to mature, for many compression settings, quality loss is still noticeable. These settings nevertheless have important applications to the efficient transmission of videos over bandwidth constrained or otherwise unstable connections. In this work, we develop a deep learning architecture capable of restoring detail to compressed videos which leverages the underlying structure and motion information embedded in the video bitstream. We show that this improves restoration accuracy compared to prior compression correction methods and is competitive when compared with recent deep-learning-based video compression methods on rate-distortion while achieving higher throughput. Furthermore, we condition our model on quantization data which is readily available in the bitstream. This allows our single model to handle a variety of different c
    
[^162]: 重启的非凸加速梯度下降：在$O(\epsilon^{-7/4})$复杂度中不再需要多项式对数因子

    Restarted Nonconvex Accelerated Gradient Descent: No More Polylogarithmic Factor in the $O(\epsilon^{-7/4})$ Complexity. (arXiv:2201.11411v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2201.11411](http://arxiv.org/abs/2201.11411)

    本文提出了重启式加速梯度下降（AGD）和重启式重球（HB）方法来解决非凸优化问题，并通过基本证明证明了这些方法可以在$O(\epsilon^{-7/4})$个梯度评估内达到$\epsilon$近似的一阶稳定点。这项工作的复杂度没有多项式对数因子，并优于以前的最佳复杂度。

    

    本文研究具有Lipschitz连续梯度和Hessian的非凸优化的加速梯度方法。我们提出了两种简单的加速梯度方法，重启式加速梯度下降（AGD）和重启式重球（HB）方法，并通过基本证明证明了我们的方法可以在$O(\epsilon^{-7/4})$个梯度评估内达到$\epsilon$近似的一阶稳定点。在理论上，我们的复杂度没有隐藏任何的多项式对数因子，并因此比已知的最佳复杂度提高了$O(\log \frac{1}{\epsilon})$的因子。我们的算法非常简单，因为它们只是由Nesterov的经典AGD或Polyak的HB迭代以及重启机制组成。与现有的分析相比，我们的基本证明使用了不那么先进的技术，并且不涉及强凸性的分析或正则化代理函数的最小化作为子例程。

    This paper studies accelerated gradient methods for nonconvex optimization with Lipschitz continuous gradient and Hessian. We propose two simple accelerated gradient methods, restarted accelerated gradient descent (AGD) and restarted heavy ball (HB) method, and establish that our methods achieve an $\epsilon$-approximate first-order stationary point within $O(\epsilon^{-7/4})$ number of gradient evaluations by elementary proofs. Theoretically, our complexity does not hide any polylogarithmic factors, and thus it improves over the best known one by the $O(\log\frac{1}{\epsilon})$ factor. Our algorithms are simple in the sense that they only consist of Nesterov's classical AGD or Polyak's HB iterations, as well as a restart mechanism. They do not invoke negative curvature exploitation or minimization of regularized surrogate functions as the subroutines. In contrast with existing analysis, our elementary proofs use less advanced techniques and do not invoke the analysis of strongly conve
    
[^163]: 随机签名作为学习粗糙动态的储备库的有效性研究

    On the effectiveness of Randomized Signatures as Reservoir for Learning Rough Dynamics. (arXiv:2201.00384v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.00384](http://arxiv.org/abs/2201.00384)

    本文研究了随机签名方法在学习粗糙动态中的表现，并发现它比截断签名方法和其他深度学习技术更具优势。

    

    许多金融、物理和工程现象都由高度不规则（随机）输入驱动的连续时间动态系统建模。由于粗路径理论，并利用所谓的签名变换，在这种情况下执行时间序列分析的强有力工具已经出现。该算法享有强大的理论保证，但很难扩展到高维数据。本文研究了最近推导的随机投影变量，称为随机签名。我们对随机签名方法的有效性进行了深入的实验评估，旨在展示这种储备库对社区的优势。我们发现，在模型复杂性、训练时间、准确性、稳健性和数据需求方面，这种方法优于截断签名方法和其他深度学习技术。

    Many finance, physics, and engineering phenomena are modeled by continuous-time dynamical systems driven by highly irregular (stochastic) inputs. A powerful tool to perform time series analysis in this context is rooted in rough path theory and leverages the so-called Signature Transform. This algorithm enjoys strong theoretical guarantees but is hard to scale to high-dimensional data. In this paper, we study a recently derived random projection variant called Randomized Signature, obtained using the Johnson-Lindenstrauss Lemma. We provide an in-depth experimental evaluation of the effectiveness of the Randomized Signature approach, in an attempt to showcase the advantages of this reservoir to the community. Specifically, we find that this method is preferable to the truncated Signature approach and alternative deep learning techniques in terms of model complexity, training time, accuracy, robustness, and data hungriness.
    
[^164]: 抽象解释下的Fixpoint迭代器及其在神经网络中的应用

    Abstract Interpretation of Fixpoint Iterators with Applications to Neural Networks. (arXiv:2110.08260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.08260](http://arxiv.org/abs/2110.08260)

    该论文提出了一种新的抽象解释框架，使我们能够精确地过度逼近数值Fixpoint迭代器。使用一种新的抽象域，CH-Zonotope，可以实现有效的传播和包含检查。该框架在研究monDEQ神经网络结构时表现出色，是一种有前途的验证技术。

    

    我们提出了一个新的抽象解释框架，以精确地过度逼近数值Fixpoint迭代器。我们的关键观察是，在这种情况下，与通常用于逼近所有可达程序状态的标准抽象解释（AI）不同，人们只需要抽象具体的Fixpoints，即最终的程序状态。我们的框架针对具体收敛性和唯一性保证的数值Fixpoint迭代器，基于两个主要的技术贡献：（i）理论上的洞察力，允许我们计算出不使用交汇点的声音和精确的Fixpoint抽象，（ii）一种新的抽象域，CH- Zonotope，在保持高精度的同时，允许有效的传播和包含检查。我们在一个名为CRAFT的工具中实现了我们的框架，并在一个特别具有挑战性的基于Fixpoint的神经网络架构（monDEQ）上进行了评估。我们的广泛评估表明，CRAFT超出了最先进的工具。

    We present a new abstract interpretation framework for the precise over-approximation of numerical fixpoint iterators. Our key observation is that unlike in standard abstract interpretation (AI), typically used to over-approximate all reachable program states, in this setting, one only needs to abstract the concrete fixpoints, i.e., the final program states. Our framework targets numerical fixpoint iterators with convergence and uniqueness guarantees in the concrete and is based on two major technical contributions: (i) theoretical insights which allow us to compute sound and precise fixpoint abstractions without using joins, and (ii) a new abstract domain, CH-Zonotope, which admits efficient propagation and inclusion checks while retaining high precision. We implement our framework in a tool called CRAFT and evaluate it on a novel fixpoint-based neural network architecture (monDEQ) that is particularly challenging to verify. Our extensive evaluation demonstrates that CRAFT exceeds the
    
[^165]: 基于递归转移集成学习的循环神经网络预测印度多日COVID-19疫情趋势

    Transfer-Recursive-Ensemble Learning for Multi-Day COVID-19 Prediction in India using Recurrent Neural Networks. (arXiv:2108.09131v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.09131](http://arxiv.org/abs/2108.09131)

    本文提出了基于循环神经网络的预测新冠病例数量的方法，并利用递归转移集成学习和预训练模型，实现了印度多日COVID-19疫情趋势的预测。

    

    当前的COVID-19大流行给印度的医疗基础设施带来了巨大挑战。在第二波疫情中，越来越多的人感染，医院负担过重，物资和氧气短缺。在这种情况下，提前预测COVID-19病例数量可能有助于更好地利用有限的资源和物资。本文涉及多日预测新的COVID-19病例、新的死亡人数和总活动病例。所提出的方法主要使用门控循环单元网络作为预测模型。通过建立四个预先训练在不同国家（美国、巴西、西班牙和孟加拉国）数据上的模型，并在印度的数据上进行微调或重新训练的研究。由于所选择的四个国家经历了不同类型的感染曲线，因此预训练为模型提供了迁移学习，考虑到了不同的情况。

    The current COVID-19 pandemic has put a huge challenge on the Indian health infrastructure. With more and more people getting affected during the second wave, the hospitals were over-burdened, running out of supplies and oxygen. In this scenario, prediction of the number of COVID-19 cases beforehand might have helped in the better utilization of limited resources and supplies. This manuscript deals with the prediction of new COVID-19 cases, new deaths and total active cases for multiple days in advance. The proposed method uses gated recurrent unit networks as the main predicting model. A study is conducted by building four models that are pre-trained on the data from four different countries (United States of America, Brazil, Spain and Bangladesh) and are fine-tuned or retrained on India's data. Since the four countries chosen have experienced different types of infection curves, the pre-training provides a transfer learning to the models incorporating diverse situations into account.
    
[^166]: FairBalance：如何通过数据预处理实现等几率性。

    FairBalance: How to Achieve Equalized Odds With Data Pre-processing. (arXiv:2107.08310v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.08310](http://arxiv.org/abs/2107.08310)

    本研究提供了一种简单而有效的预处理方法，旨在实现机器学习软件的等几率公平性问题，通过平衡每个族群中的类别分布，以达到这一目标。

    

    本研究旨在通过提供一种简单而有效的预处理方法，在机器学习软件中实现平等机会平等。随着机器学习软件在高风险和高风险决策中的应用越来越多，公平性问题引起了越来越多的关注。在所有现有的公平性概念中，本文特别针对“等几率性”，因为它在始终允许完美分类器方面具有优势。等几率要求每个族群的成员都不会受到不同的待遇。先前的工作要么在学习过程中优化与等几率有关的指标，如黑盒，要么遵循一些直觉来操纵训练数据。本文研究了违反等几率的根本原因以及如何解决它。我们发现，使用样本权重平衡每个族群中的类别分布是实现等几率所必需的条件。

    This research seeks to benefit the software engineering society by providing a simple yet effective pre-processing approach to achieve equalized odds fairness in machine learning software. Fairness issues have attracted increasing attention since machine learning software is increasingly used for high-stakes and high-risk decisions. Amongst all the existing fairness notions, this work specifically targets "equalized odds" given its advantage in always allowing perfect classifiers. Equalized odds requires that members of every demographic group do not receive disparate mistreatment. Prior works either optimize for an equalized odds related metric during the learning process like a black-box, or manipulate the training data following some intuition. This work studies the root cause of the violation of equalized odds and how to tackle it. We found that equalizing the class distribution in each demographic group with sample weights is a necessary condition for achieving equalized odds with
    
[^167]: 基于连接敏感性的训练免费DARTS：从架构级评分到操作级敏感性分析

    Connection Sensitivity Matters for Training-free DARTS: From Architecture-Level Scoring to Operation-Level Sensitivity Analysis. (arXiv:2106.11542v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11542](http://arxiv.org/abs/2106.11542)

    本论文提出一种叫做ZEROS的连接概念来评估DARTS中的操作重要性，使得整个架构的搜索过程更高效，提出了一种基于NTK理论的全新框架FreeDARTS。

    

    最近提出的训练免费NAS方法放弃了训练阶段并设计了各种零成本代理作为评分，以识别出优秀的架构，引起了神经架构搜索的极高计算效率。本文提出了一个有趣的问题：我们能否通过训练免费的方式适当地测量DARTS中的操作重要性，避免参数密集的偏差？我们通过边缘连通性的角度来研究这个问题，并提供了一个肯定的答案，通过定义一个连接概念“ZERo-cost Operation Sensitivity (ZEROS)”，来评分DARTS中候选操作的重要性。通过设计一种迭代和数据不可知的方式来利用ZEROS进行NAS，我们的新尝试导致了一个名为“training free differentiable architecture search (FreeDARTS)”的框架。基于神经切向核(NTK)的理论，我们证明了所提出的连接评分与泛化下降呈负相关。

    The recently proposed training-free NAS methods abandon the training phase and design various zero-cost proxies as scores to identify excellent architectures, arousing extreme computational efficiency for neural architecture search. In this paper, we raise an interesting problem: can we properly measure the operation importance in DARTS through a training-free way, with avoiding the parameter-intensive bias? We investigate this question through the lens of edge connectivity, and provide an affirmative answer by defining a connectivity concept, ZERo-cost Operation Sensitivity (ZEROS), to score the importance of candidate operations in DARTS at initialization. By devising an iterative and data-agnostic manner in utilizing ZEROS for NAS, our novel trial leads to a framework called training free differentiable architecture search (FreeDARTS). Based on the theory of Neural Tangent Kernel (NTK), we show the proposed connectivity score provably negatively correlated with the generalization bo
    
[^168]: 多元时间序列分类算法基准测试

    Benchmarking Multivariate Time Series Classification Algorithms. (arXiv:2007.13156v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.13156](http://arxiv.org/abs/2007.13156)

    该论文比较了单变量时间序列分类（TSC）与多元TSC(MTSC)问题的算法。作者测试了基于深度学习、形状和单词袋方法的算法，并将其与维度无关的方法进行比较。

    

    时间序列分类（TSC）涉及从有序的实值属性中构建用于离散目标变量的预测模型。最近几年，一组新的TSC算法已经开发出来，这些算法在之前的技术水平上取得了重大进展。主要关注的是单变量TSC，即每个案例都有一个单一序列和一个类标签的问题。但实际上，更常见的是遇到多元TSC(MTSC)问题，其中多个序列与单一标签关联。然而，对于MTSC的考虑远不如单变量情况那么多。2018年推出了30个MTSC问题的UEA存档库，使算法之间的比较更容易。我们回顾了基于深度学习、形状和单词袋方法提出的定制MTSC算法。 MTSC的最简单方法是通过多元维度上的集成单变量分类器。我们将这些定制算法与这些维度无关方法进行比较。

    Time Series Classification (TSC) involved building predictive models for a discrete target variable from ordered, real valued, attributes. Over recent years, a new set of TSC algorithms have been developed which have made significant improvement over the previous state of the art. The main focus has been on univariate TSC, i.e. the problem where each case has a single series and a class label. In reality, it is more common to encounter multivariate TSC (MTSC) problems where multiple series are associated with a single label. Despite this, much less consideration has been given to MTSC than the univariate case. The UEA archive of 30 MTSC problems released in 2018 has made comparison of algorithms easier. We review recently proposed bespoke MTSC algorithms based on deep learning, shapelets and bag of words approaches. The simplest approach to MTSC is to ensemble univariate classifiers over the multivariate dimensions. We compare the bespoke algorithms to these dimension independent appro
    
[^169]: 两个工具箱的故事——第三次报告：关于HIVE-COTE v1.0的使用和性能

    A tale of two toolkits, report the third: on the usage and performance of HIVE-COTE v1.0. (arXiv:2004.06069v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2004.06069](http://arxiv.org/abs/2004.06069)

    介绍了用于时间序列分类的异构元集成算法 HIVE-COTE 的最新稳定版本 1.0，提供了使用指南，并通过实验评估了其性能和资源使用情况，并与三种近期提出的算法进行了比较。

    

    层次化转换集合法（HIVE-COTE）是一种用于时间序列分类的异构元集成算法。该算法最初于2016年提出，经历了一些小的改变，并在两个开源代码库中推出了可配置、可扩展和易于使用的版本。本文介绍了最新稳定的 HIVE-COTE 版本 1.0 的概述，并阐述了它与最初版本的区别。我们提供了一个使用该分类器的指南，并对其预测性能和资源使用情况进行了广泛的实验评估。我们使用 aeon 工具包比较了 HIVE-COTE 与三种近期提出的算法的性能。

    The Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE) is a heterogeneous meta ensemble for time series classification. Since it was first proposed in 2016, the algorithm has undergone some minor changes and there is now a configurable, scalable and easy to use version available in two open source repositories. We present an overview of the latest stable HIVE-COTE, version 1.0, and describe how it differs to the original. We provide a walkthrough guide of how to use the classifier, and conduct extensive experimental evaluation of its predictive performance and resource usage. We compare the performance of HIVE-COTE to three recently proposed algorithms using the aeon toolkit.
    

