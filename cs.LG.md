# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Unveiling the Role of Message Passing in Dual-Privacy Preservation on GNNs.](http://arxiv.org/abs/2308.13513) | 本论文揭示了GNN中消息传递在隐私泄露中的核心作用，并提出了一种有原则的双隐私保护GNN框架，有效保护节点和链路的隐私。 |
| [^2] | [Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models.](http://arxiv.org/abs/2308.13507) | 通过在生成代码之前提问澄清问题，大型语言模型的代码生成能力可以得到提升，增加了对生成代码的信心。 |
| [^3] | [A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance.](http://arxiv.org/abs/2308.13504) | A2Q是一种用于训练量化神经网络的累加器感知量化方法，通过引入一种独特的公式，并根据累加器位宽边界约束模型权重的L1范数，以避免在低精度累加器上发生溢出。该方法在计算机视觉任务中得到验证，能够有效地训练QNNs并保持模型准确性，特别适用于部署在可编程硬件如FPGAs上。 |
| [^4] | [Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators.](http://arxiv.org/abs/2308.13498) | 本文介绍了使用配对距离估计器对集成模型进行认识不确定性估计的新方法，相比于常用的深度学习方法，该方法能够更快速、更准确地在更大的空间和更高维度上估计认识不确定性。 |
| [^5] | [Ngambay-French Neural Machine Translation (sba-Fr).](http://arxiv.org/abs/2308.13497) | 该论文介绍了Ngambay-French神经机器翻译系统，并创造了第一个sba-Fr数据集，用于低资源语言的翻译研究。 |
| [^6] | [TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs.](http://arxiv.org/abs/2308.13490) | TpuGraphs是一种关于大型张量计算图的性能预测数据集，可用于优化编译器或自动调优工具的决策，并提供了图的执行时间信息。 |
| [^7] | [Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction.](http://arxiv.org/abs/2308.13466) | 本文提出了一种新颖且可扩展的分布式GNN训练方法SAT，通过在线动态嵌入预测减轻了过时性带来的问题，解决了分布式GNN训练中的并发性和通信开销的困扰。 |
| [^8] | [Learning to Intervene on Concept Bottlenecks.](http://arxiv.org/abs/2308.13453) | 该论文提出了一种扩展了概念瓶颈模型的概念瓶颈记忆模型（CB2M），通过学习将干预推广到不同情境并重新应用先前干预来自动改善模型性能。当没有先前的人类干预信息时，CB2M能够检测错误并请求有针对性的干预。 |
| [^9] | [Gotta match 'em all: Solution diversification in graph matching matched filters.](http://arxiv.org/abs/2308.13451) | 本文提出了一种在大规模背景图中查找多个嵌入的模板图的新方法，通过迭代惩罚相似度矩阵来实现多样化匹配的发现，并提出了算法加速措施。在理论验证和实验证明中，证明了该方法的可行性和实用性。 |
| [^10] | [Six Lectures on Linearized Neural Networks.](http://arxiv.org/abs/2308.13431) | 这篇论文通过分析线性模型，探讨了多层神经网络行为的学习。研究使用了线性回归、核岭回归、随机特征模型和神经切线模型等四个线性化神经网络模型。研究还讨论了线性理论的局限性和其他方法如何克服这些局限性。 |
| [^11] | [Nougat: Neural Optical Understanding for Academic Documents.](http://arxiv.org/abs/2308.13418) | Nougat是一种用于学术文档的神经光学理解模型，通过光学字符识别任务将科学文档转换成标记语言，提高了科学知识的可访问性，并提供了模型和代码以加速未来的科学文本识别工作。 |
| [^12] | [An investigation into the impact of deep learning model choice on sex and race bias in cardiac MR segmentation.](http://arxiv.org/abs/2308.13415) | 这项研究调查了深度学习模型选择对心脏磁共振分割中性别和种族偏差的影响，发现不同模型之间偏差的严重性和性质不同，强调了模型选择在训练公平的基于AI的医学成像分割模型时的重要性。 |
| [^13] | [Using Visual and Vehicular Sensors for Driver Behavior Analysis: A Survey.](http://arxiv.org/abs/2308.13406) | 该综述介绍了使用视觉和车辆传感器分析驾驶行为的技术，指出将视觉和车辆信息相结合可以提高分析的准确性和效果，从而改善驾驶安全和减少交通事故。 |
| [^14] | [EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression.](http://arxiv.org/abs/2308.13399) | 该论文提出了一种无监督的关键词提取方法，通过利用预训练语言模型和信息论方法，在文本中提取具有最高条件熵的短语作为关键词。实验证明，该方法在关键词提取任务上取得了与常用方法相当的结果。 |
| [^15] | [TFDNet: Time-Frequency Enhanced Decomposed Network for Long-term Time Series Forecasting.](http://arxiv.org/abs/2308.13386) | 这篇论文提出了一种称为TFDNet的时频增强分解网络，用于从时频域捕获长期时间序列的基本模式和时间周期性。 |
| [^16] | [In-context learning for model-free system identification.](http://arxiv.org/abs/2308.13380) | 本文提出了一种基于上下文学习的无模型系统辨识方法，通过观察同一类别中其他系统的行为来理解动态系统的复杂性。 |
| [^17] | [EOG Artifact Removal from Single and Multi-channel EEG Recordings through the combination of Long Short-Term Memory Networks and Independent Component Analysis.](http://arxiv.org/abs/2308.13371) | 本文提出了一种将长短期记忆网络与独立成分分析相结合的方法，用于从受污染的脑电图信号中去除眼电图伪迹。 |
| [^18] | [A topological model for partial equivariance in deep learning and data analysis.](http://arxiv.org/abs/2308.13357) | 本文提出了一种拓扑模型，用于在神经网络中编码部分等变性，并研究了相应的测量空间和P-GENEO空间的性质。 |
| [^19] | [On the Impact of Language Selection for Training and Evaluating Programming Language Models.](http://arxiv.org/abs/2308.13354) | 这项研究根据使用CodeBERT模型分析编程语言的表示，发现编程语言之间在标记表示方面存在差异，建议使用这种相似度度量方法来选择跨多种语言的模型。 |
| [^20] | [A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data.](http://arxiv.org/abs/2308.13352) | 这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。 |
| [^21] | [Compressor-Based Classification for Atrial Fibrillation Detection.](http://arxiv.org/abs/2308.13328) | 本文研究了基于压缩机的文本分类方法在房颤检测中的应用。通过对心律间隔序列进行压缩距离计算和最近邻分类器模型优化，我们实现了良好的分类性能，接近于最佳的房颤检测算法。这表明gzip分类器适用于生物医学数据和连续随机序列的分类。 |
| [^22] | [Fine-tuning can cripple your foundation model; preserving features may be the solution.](http://arxiv.org/abs/2308.13320) | 在微调过程中，基础模型可能会遗忘概念，我们提出了一种名为LDIFS的方法，用于解决这个问题，该方法在实验证明效果显著。 |
| [^23] | [Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics.](http://arxiv.org/abs/2308.13317) | 本文介绍了一种名为PGI的新方法，在实际商业问题中应用于GPT模型。该方法利用GPT模型的能力来理解复杂的语言结构，并生成上下文相关的回应。实验证实了PGI策略的有效性，并帮助解决了人类智能低度利用的问题。 |
| [^24] | [Bang and the Artefacts are Gone! Rapid Artefact Removal and Tissue Segmentation in Haematoxylin and Eosin Stained Biopsies.](http://arxiv.org/abs/2308.13304) | 该论文提出了一种基于H&E Otsu thresholding的方案，可以快速去除标本残留物和分割组织，在血染乙酸洋红染色的活检中取得了良好的效果。 |
| [^25] | [Learning Compact Neural Networks with Deep Overparameterised Multitask Learning.](http://arxiv.org/abs/2308.13300) | 本文提出了一种使用深度超参数化多任务学习来学习紧凑的神经网络的方法，通过在训练中超参数化模型架构并更有效地共享跨任务的超参数化模型参数，以实现更好的优化和泛化。 |
| [^26] | [Federated Linear Bandit Learning via Over-the-Air Computation.](http://arxiv.org/abs/2308.13298) | 本研究针对联邦线性赌博学习提出了一种通过无线计算的方案，以减少通信开销。通过在噪声衰落信道上进行的模拟信号传输，我们的方案在降低累积遗憾方面表现出竞争力。 |
| [^27] | [Training normalizing flows with computationally intensive target probability distributions.](http://arxiv.org/abs/2308.13294) | 本文提出了一种基于REINFORCE算法的归一化流估计器，用于训练具有计算密集型目标概率分布的问题。在二维Schwinger模型中的应用结果表明，相较于重新参数化技巧估计器，该方法能够在墙时钟时间上快10倍，且内存使用上节省30%。 |
| [^28] | [A Bayesian Active Learning Approach to Comparative Judgement.](http://arxiv.org/abs/2308.13292) | 这项研究提出了一种基于贝叶斯主动学习的比较评判方法，用于解决传统教育评估中存在的一致性和偏见等问题，并探索了如何选择比较项目的可靠数量。 |
| [^29] | [JAX-LOB: A GPU-Accelerated limit order book simulator to unlock large scale reinforcement learning for trading.](http://arxiv.org/abs/2308.13289) | JAX-LOB是第一个GPU加速的LOB模拟器，可以并行处理数千个订单簿，以较低的处理时间实现大规模强化学习交易，为金融交易研究提供了重要工具。 |
| [^30] | [AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning.](http://arxiv.org/abs/2308.13280) | 提出了一种称为AtmoRep的大气动力学随机模型，它利用大规模表示学习和人工智能技术来确定复杂的大气动力学的通用描述，从而为各种应用提供技能结果。 |
| [^31] | [Hyperbolic Random Forests.](http://arxiv.org/abs/2308.13279) | 该论文提出了一种在非欧几里得空间中将随机森林推广的方法，并使用水平球重新定义了分割的概念。为了处理多类数据和不平衡实验，论文还提出了一种新的类组合方法。 |
| [^32] | [Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity.](http://arxiv.org/abs/2308.13278) | 本文提出了一种将LLMs和Decision Transformers集成到语言驱动的生成质量多样性问题中的方法，通过利用大型语言模型增加具有轨迹的自然语言描述的库，并训练一个依赖于这些描述的策略来解决问题。 |
| [^33] | [Heterogeneous Decentralized Machine Unlearning with Seed Model Distillation.](http://arxiv.org/abs/2308.13269) | 该论文介绍了一种名为HDUS的分布式遗忘框架，使用种子模型蒸馏构建可擦除的模型集成，适用于异构的设备端模型，具有卓越的性能。 |
| [^34] | [Heterogeneous Federated Learning via Personalized Generative Networks.](http://arxiv.org/abs/2308.13265) | 本文通过个性化生成网络实现了异构联邦学习，解决了数据统计异质性的问题，并通过对客户端之间知识传递的方法，提高了全局模型的收敛效果。 |
| [^35] | [Kissing to Find a Match: Efficient Low-Rank Permutation Representation.](http://arxiv.org/abs/2308.13252) | 本文提出了一种使用低秩矩阵分解和非线性逼近来解决大型置换矩阵的维度灾难的方法。通过利用"亲吻数"理论，我们可以推断表示置换矩阵所需的最小秩，从而大幅降低计算和内存成本。这种方法可以实现准确的表达，并在大规模问题中取得了显著的内存节省。 |
| [^36] | [Model-free Reinforcement Learning with Stochastic Reward Stabilization for Recommender Systems.](http://arxiv.org/abs/2308.13246) | 本文研究了在推荐系统中应用无模型强化学习的问题。针对推荐系统中随机奖励的特性，我们设计了两种随机奖励稳定化框架，用于更有效地处理随机反馈。我们的实验证明了这些框架的优越性。 |
| [^37] | [Optimizing Group-Fair Plackett-Luce Ranking Models for Relevance and Ex-Post Fairness.](http://arxiv.org/abs/2308.13242) | 提出了一种优化群组公平Plackett-Luce排序模型的方法，该方法最大化预期相关性并满足表示约束以确保后期公平性。 |
| [^38] | [Bayesian Reasoning for Physics Informed Neural Networks.](http://arxiv.org/abs/2308.13222) | 本文提出了一种基于贝叶斯推理的物理信息神经网络方法（PINN）。该方法采用贝叶斯神经网络框架，通过计算证据来优化模型并解决不确定性问题。 |
| [^39] | [GEMTrans: A General, Echocardiography-based, Multi-Level Transformer Framework for Cardiovascular Diagnosis.](http://arxiv.org/abs/2308.13217) | GEMTrans是一种通用的、基于超声心动图的多级Transformer框架，用于心血管诊断。该框架能够提供解释性，并能处理多个心脏视图的echo视频以进行准确的心血管测量或解释任务的预测。 |
| [^40] | [Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation.](http://arxiv.org/abs/2308.13212) | 提出一种受物理启发的神经图ODE算法（PINGO），用于模拟和建模多对象物理系统的长期动态，并解决了当前模型泛化能力差的问题。 |
| [^41] | [Physics-inspired Equivariant Descriptors of Non-bonded Interactions.](http://arxiv.org/abs/2308.13208) | 基于物理启发，我们提出了一种受物理启发的非键相互作用等变描述符框架，该框架能够模拟长程物理相互作用，并且能够生成类似非键位势的局部描述符。 |
| [^42] | [Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon.](http://arxiv.org/abs/2308.13182) | 本研究提出了一种新的生成模型SC-GAN，用于合成H&E图像和IHC染色之间的转换。该模型利用边缘结构信息和注意力模块，增强了特征定位并保留了上下文信息。 |
| [^43] | [Using Adamic-Adar Index Algorithm to Predict Volunteer Collaboration: Less is More.](http://arxiv.org/abs/2308.13176) | 本研究使用Adamic-Adar指数算法预测了志愿者合作，发现AAI算法在分析图形时表现优于传统的JC、CNC和其他机器学习算法。 |
| [^44] | [IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization.](http://arxiv.org/abs/2308.13168) | IOMatch是一个新颖的开放集半监督学习框架，能够在内点和外点难以区分的情况下共同利用它们。 |
| [^45] | [DAG-ACFL: Asynchronous Clustered Federated Learning based on DAG-DLT.](http://arxiv.org/abs/2308.13158) | DAG-ACFL是一种基于DAG-DLT的异步聚类联邦学习框架，通过选择相似分布的客户端模型来聚合全局模型，同时采用自适应的tip选择算法降低通信和存储成本。 |
| [^46] | [Federated Learning in IoT: a Survey from a Resource-Constrained Perspective.](http://arxiv.org/abs/2308.13157) | 本研究论文调研了在资源受限的物联网环境中实施联邦学习所面临的挑战和解决方案。重点关注了有限的客户端资源、异构客户端数据存在、服务器容量和高通信成本等问题，并评估了各个解决方案的有效性。 |
| [^47] | [Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism.](http://arxiv.org/abs/2308.13150) | 本文介绍了一种使用ResNet模型和轻量级注意机制框架的图像分类方法，通过优化特征表示、增强分类能力和改善特征可辨别性，在乳腺癌分类任务上显示出卓越性能和潜在应用前景。 |
| [^48] | [MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification.](http://arxiv.org/abs/2308.13139) | MatchXML是一种高效的文本-标签匹配框架，用于极端多标签文本分类。它通过label2vec方法生成语义密集的标签嵌入，并利用这些嵌入构建层次化标签树。通过微调预训练的Transformer模型，MatchXML将多标签文本分类问题转化为文本-标签匹配问题，并提取出密集的文本表示和静态的句子嵌入。 |
| [^49] | [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models.](http://arxiv.org/abs/2308.13137) | OmniQuant是一种用于大型语言模型的全向校准量化技术，通过优化各种量化参数实现了良好的性能，并保持了计算效率。 |
| [^50] | [Nonparametric Additive Value Functions: Interpretable Reinforcement Learning with an Application to Surgical Recovery.](http://arxiv.org/abs/2308.13135) | 该论文提出了一种非参数可加模型，用于估计可解释的值函数，并在强化学习中具有应用价值。该方法能够克服传统模型的线性假设限制，同时提供较强的决策建议解释性。 |
| [^51] | [Business Metric-Aware Forecasting for Inventory Management.](http://arxiv.org/abs/2308.13118) | 本研究针对库存管理设置，通过端到端优化预测指标，相较于优化传统标准的与业务无关的指标，能够显著提升业务绩效。 |
| [^52] | [Bayesian low-rank adaptation for large language models.](http://arxiv.org/abs/2308.13111) | 本研究提出了一种名为Laplace-LoRA的贝叶斯方法，通过应用拉普拉斯近似来增强经过微调的大型语言模型的校准能力。 |
| [^53] | [Contrastive Learning of Temporal Distinctiveness for Survival Analysis in Electronic Health Records.](http://arxiv.org/abs/2308.13104) | 本文提出了一种新的基于对比学习的生存分析框架，充分利用截断和观察数据的生存时长来定义时序区别度，构建负样本对。 |
| [^54] | [Racing Towards Reinforcement Learning based control of an Autonomous Formula SAE Car.](http://arxiv.org/abs/2308.13088) | 本文介绍了使用深度强化学习控制自主Formula SAE赛车的研究。实验结果表明，这种方法可以在模拟环境中成功学习赛车驾驶，并成功应用于真实赛道上的物理平台。该研究提供对方法的局限性和未来研究方向的指导。 |
| [^55] | [SHIELD: Sustainable Hybrid Evolutionary Learning Framework for Carbon, Wastewater, and Energy-Aware Data Center Management.](http://arxiv.org/abs/2308.13086) | 这项研究提出了一种名为SHIELD的混合学习框架，用于数据中心管理，旨在优化碳排放、水足迹和能源成本。通过整合机器学习引导的局部搜索和分解式进化算法，该框架可以智能地管理工作负载分配，实现较高的效率和速度。 |
| [^56] | [Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology.](http://arxiv.org/abs/2308.13068) | 多元时间序列异常检测是一个研究领域，但目前存在组织不够有序和评估协议有缺陷的问题。文章评估了许多最近算法的性能，并指出了针对多元时间序列异常检测的评估协议存在的问题及如何缓解这些问题的方法。 |
| [^57] | [Objective-Agnostic Enhancement of Molecule Properties via Multi-Stage VAE.](http://arxiv.org/abs/2308.13066) | 本文提出了一种多阶段VAE方法，可以改善在药物发现领域中恢复低维流形的问题。实验结果表明，该方法显著改善了生成分子的属性统计，而无需集成属性预测器。 |
| [^58] | [ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching.](http://arxiv.org/abs/2308.13062) | 本研究在安全关键的软件中探索了使用LLMs生成具有侧信道泄漏的脆弱代码的修补程序。通过零-shot学习方法和动态分析，我们成功地生成了具有泄漏鲁棒性的候选替换。 |
| [^59] | [Bayesian Exploration Networks.](http://arxiv.org/abs/2308.13049) | 这篇论文提出了一种贝叶斯探索网络的方法，通过在一维Bellman算子中建模不确定性，解决了贝叶斯强化学习中学习贝叶斯最优策略的计算复杂性的挑战。 |
| [^60] | [Federated Learning of Causal Effects from Incomplete Observational Data.](http://arxiv.org/abs/2308.13047) | 我们提出了一种联邦学习的方法，可以从多个分布式和不完整的数据源中进行因果推断，估计因果效应并解决因为缺失值引入的偏差问题。 |
| [^61] | [The intersection of video capsule endoscopy and artificial intelligence: addressing unique challenges using machine learning.](http://arxiv.org/abs/2308.13035) | 视频胶囊内镜和人工智能的交叉领域面临着技术负担、解释成本高昂、数据不平衡等挑战。研究使用机器学习方法解决了这些挑战，包括使用卷积神经网络对VCE数据分类以及创建辅助专家的注释工具。 |
| [^62] | [Financial News Analytics Using Fine-Tuned Llama 2 GPT Model.](http://arxiv.org/abs/2308.13032) | 本研究通过精细调整的Llama 2模型实现了金融新闻的多任务分析，包括文本分析、摘要和情感提取等。实验结果显示，提取的命名实体情感可以作为有监督机器学习模型的预测特征。 |
| [^63] | [Training Neural Networks with Universal Adiabatic Quantum Computing.](http://arxiv.org/abs/2308.13028) | 该论文提出了一种使用通用绝热量子计算来训练神经网络的新方法，通过利用绝热演化原理，该方法可以高效地找到损失函数的全局最小值，为经典训练方法提供了有希望的替代方案。 |
| [^64] | [Extreme Risk Mitigation in Reinforcement Learning using Extreme Value Theory.](http://arxiv.org/abs/2308.13011) | 本研究提出了一种使用极值理论在强化学习中缓解极端风险的方法，通过改进状态-动作值函数分布预测的极值来增强RL智能体在面对非常罕见和危险事件时的韧性。 |
| [^65] | [Performance Comparison of Design Optimization and Deep Learning-based Inverse Design.](http://arxiv.org/abs/2308.13000) | 本文对比了传统设计优化方法与基于深度学习的逆向设计方法的性能，通过采用各种情景的基准问题来进行比较。根据研究结果，证明了... |
| [^66] | [Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning.](http://arxiv.org/abs/2308.12843) | 本文研究了使用强化学习控制无人机顶部机械臂执行器轨迹的方法，并提出了基于时间到碰撞的运动规划模型以绕过障碍物。同时，利用基于模型的Q-learning模型独立追踪和控制机械臂末端执行器的期望轨迹。通过这种方法可以实现一系列在高难度和危险环境中的执行任务。 |
| [^67] | [Match-And-Deform: Time Series Domain Adaptation through Optimal Transport and Temporal Alignment.](http://arxiv.org/abs/2308.12686) | 本文提出了一种名为Match-And-Deform（MAD）的方法，通过最优传输和时间对齐，在时间序列领域自适应问题中找到对应关系并允许时间失真，实验结果表明MAD可以生成对齐领域并最大化网络判别能力的新时间序列表示形式。 |
| [^68] | [Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition.](http://arxiv.org/abs/2308.12673) | 本文提出了一种新的遮罩特征建模方法(MFM)，用于无监督预训练图注意力网络块。通过利用预训练的视觉分词器来重构视频中对象的遮罩特征，将预训练的GAT块融入到视频事件识别架构ViGAT中，以改善模型的性能。 |
| [^69] | [Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning.](http://arxiv.org/abs/2308.12219) | 本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。 |
| [^70] | [Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models.](http://arxiv.org/abs/2308.11521) | 这篇论文研究了大型语言模型的越狱问题，并提出了一种自动越狱方法，介绍了语义防火墙的概念和三种技术实现方法。 |
| [^71] | [Can Authorship Representation Learning Capture Stylistic Features?.](http://arxiv.org/abs/2308.11490) | 本论文研究了作者身份表征学习能否捕捉文体特征的问题，并通过实验验证了这些表征能够有效地捕捉写作风格的特征。 |
| [^72] | [Bayesian polynomial neural networks and polynomial neural ordinary differential equations.](http://arxiv.org/abs/2308.10892) | 本研究提出了贝叶斯推断方法来改善多项式神经网络和多项式神经常微分方程在方程恢复问题中的表现，其中拉普拉斯近似是最佳的方法。这种方法可以推广到更广泛的符号神经网络类别。 |
| [^73] | [Deep Reinforcement Learning for Artificial Upwelling Energy Management.](http://arxiv.org/abs/2308.10199) | 通过使用深度强化学习算法，我们提出了一种新颖的能源管理方法来优化操作人工上涌系统。通过将问题建模为马尔可夫决策过程，并结合分位网络和深度竞争网络的思想，我们的方法在提高能源效率方面取得了显著成效。 |
| [^74] | [Resource-Adaptive Newton's Method for Distributed Learning.](http://arxiv.org/abs/2308.10154) | 本文介绍了一种名为RANL的新颖和高效的算法，通过使用简单的Hessian初始化和自适应的训练区域分配，克服了牛顿法在大规模和异构学习环境中的限制，并实现了线性收敛率。 |
| [^75] | [Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection.](http://arxiv.org/abs/2308.08915) | 这篇论文提出了一种冲突感知的多变量时间序列异常检测算法，该算法通过为每个指标提供独特的结构来缓解指标回归目标之间的冲突。 |
| [^76] | [Characteristics of networks generated by kernel growing neural gas.](http://arxiv.org/abs/2308.08163) | 本研究开发了核化的生长神经气体算法，并研究了由此算法生成的网络的特性。研究发现，核生长神经气体算法可以将数据集转化为无向图，并提取数据集的特征作为图形。五种核函数被用于此研究。 |
| [^77] | [How to Mask in Error Correction Code Transformer: Systematic and Double Masking.](http://arxiv.org/abs/2308.08128) | 该论文介绍了在纠错码变压器中使用系统化编码和双重遮蔽的方法，以提高性能和减少计算复杂性。 |
| [^78] | [AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes.](http://arxiv.org/abs/2308.07221) | AudioFormer是一种学习音频特征表示的方法，通过生成离散的声学代码并利用它们来训练掩码语言模型，从而将音频分类任务视为自然语言理解的形式。此外，引入了多正样本对比学习方法，通过学习联合表示来捕捉音频中的相关性。 |
| [^79] | [Experts Weights Averaging: A New General Training Scheme for Vision Transformers.](http://arxiv.org/abs/2308.06093) | 本文提出了一种新的通用训练策略，利用专家权重平均化实现了对ViTs的性能提升，而不增加推断成本。 |
| [^80] | [Symmetry-Preserving Program Representations for Learning Code Semantics.](http://arxiv.org/abs/2308.03312) | 本文提出了一种在代码中保持对称性的程序表示方法，通过引入循环层来提高在程序分析和建模中的效果。 |
| [^81] | [ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks.](http://arxiv.org/abs/2308.01423) | ChatMOF是一种自主AI系统，用于预测和生成金属-有机骨架。通过利用大规模语言模型，它能够从文本输入中提取关键细节，并提供适当的回应。该系统通过组合代理、工具包和评估器的核心组件，实现了数据检索、性质预测和结构生成等多个任务。研究进一步展示了在材料科学中使用大型语言模型的优势和潜力。 |
| [^82] | [BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning.](http://arxiv.org/abs/2307.14623) | BubbleML是一个用于机器学习的多物理数据集，通过物理驱动模拟获得准确的地面真实信息，并在各种沸腾场景中验证了其可靠性和潜力。 |
| [^83] | [Towards Understanding Adversarial Transferability From Surrogate Training.](http://arxiv.org/abs/2307.07873) | 本论文探索了对抗性可转移性的理解，特别关注替代训练。通过研究模型的平滑性和梯度相似性之间的权衡，发现对抗训练可以提高模型的替代能力。研究结果对数据分布的转变提出了新的推测。 |
| [^84] | [Min-Max Optimization under Delays.](http://arxiv.org/abs/2307.06886) | 在大规模机器学习中，研究了min-max优化在延迟下的性能。对于简单实例，即使是小的延迟也可能导致Extra-gradient算法发散，因此需要对延迟版本的min-max优化算法进行仔细分析。为此，我们证明了在适当的技术假设下，梯度下降-上升算法在延迟情况下的收敛性和性能。 |
| [^85] | [IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation.](http://arxiv.org/abs/2307.06698) | IntelliGraphs是一组新的知识图谱数据集，用于评估知识图谱生成。其中包含具有逻辑规则表达的语义的子图，用于评估子图推断的模型。 |
| [^86] | [Federated Ensemble YOLOv5 - A Better Generalized Object Detection Algorithm.](http://arxiv.org/abs/2306.17829) | 本文研究了联合学习算法在目标检测中的应用，并将其与集中式训练方法进行性能比较。实验结果表明，使用联合学习训练的YOLOv5模型在生成准确的未见过目标的边界框方面具有显著优势。 |
| [^87] | [Federated Object Detection for Quality Inspection in Shared Production.](http://arxiv.org/abs/2306.17645) | 本文提出了一个利用联邦学习进行质量检验任务中的目标检测的算法，该算法使用YOLOv5作为目标检测算法和Federated Averaging作为联邦学习算法。实验结果表明，该联邦学习方法在整体客户测试数据集上具有更好的泛化性能，并生成相对于使用本地客户数据集训练的模型更精确的边界框。 |
| [^88] | [To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration.](http://arxiv.org/abs/2306.15749) | 神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。 |
| [^89] | [Rapid building damage assessment workflow: An implementation for the 2023 Rolling Fork, Mississippi tornado event.](http://arxiv.org/abs/2306.12589) | 本文介绍了一种人在循环工作流，用于自然灾害后快速训练建筑损伤评估模型，并通过在2023年密西西比州滚动叉口龙卷风事件中的案例研究获得了较高的精度和召回率。 |
| [^90] | [Fix Fairness, Don't Ruin Accuracy: Performance Aware Fairness Repair using AutoML.](http://arxiv.org/abs/2306.09297) | 本文提出了一个使用AutoML技术来减少偏见的新方法，该方法通过改进优化函数和搜索空间，并结合公平目标，在几乎不损失准确性的情况下减少基于ML的软件中的偏见。同时提出了一个适用于AutoML的公平感知搜索空间修剪方法，以减少计算成本和修复时间。 |
| [^91] | [Overcoming Adversarial Attacks for Human-in-the-Loop Applications.](http://arxiv.org/abs/2306.05952) | 通过人类视觉注意力模型改善人机图像分析系统的可解释性和鲁棒性，克服针对人机交互应用的对抗攻击。 |
| [^92] | [SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts.](http://arxiv.org/abs/2306.02207) | 本文探索了一个名为SpeechGen的统一框架，通过提示调节，解锁了语音语言模型的生成能力，成功地实现了直接适应连续语音到离散标记的任务，使得语音生成成为可能。 |
| [^93] | [Quantum Kernel Mixtures for Probabilistic Deep Learning.](http://arxiv.org/abs/2305.18204) | 本文提出了一种量子核混合方法，可以用于表示连续和离散随机变量的联合概率分布。该框架允许构建可微分的模型，适用于密度估计、推理和采样，以及各种机器学习任务，包括生成建模和判别学习。 |
| [^94] | [In Defense of Pure 16-bit Floating-Point Neural Networks.](http://arxiv.org/abs/2305.10947) | 本文探讨了纯16位浮点神经网络的被忽视的效率，提供了理论分析来探讨16位和32位模型的差异，并可以定量解释16位模型与其32位对应物之间的条件。 |
| [^95] | [Uncertainty Estimation for Deep Learning Image Reconstruction using a Local Lipschitz Metric.](http://arxiv.org/abs/2305.07618) | 本文提出了一种基于局部Lipschitz度量的方法，可以用于估计深度学习图像重建模型的不确定性。该方法可用于确定特定深度学习重建方法的适用性，识别分布外的测试样本并指导适当的数据增强。 |
| [^96] | [PGB: A PubMed Graph Benchmark for Heterogeneous Network Representation Learning.](http://arxiv.org/abs/2305.02691) | 介绍了一个基于PubMed数据库的PGB基准数据集，用于评估生物医学文献的异构图嵌入。该数据集包含丰富的元数据和来自不同数据集的21个系统性评价主题的评估任务。 |
| [^97] | [On the lifting and reconstruction of nonlinear systems with multiple attractors.](http://arxiv.org/abs/2304.11860) | 本文研究了具有多个吸引子的非线性系统的Koopman算子提升和重构机制，通过利用吸引域之间的固有对称性，只需三个自由度的线性重构就可以全局线性化系统。 |
| [^98] | [Coarse race data conceals disparities in clinical risk score performance.](http://arxiv.org/abs/2304.09270) | 研究发现仅依赖粗糙的种族类别可能掩盖了临床风险评分表现中的重要差异，需要更精细的种族数据采集。 |
| [^99] | [Scale Federated Learning for Label Set Mismatch in Medical Image Classification.](http://arxiv.org/abs/2304.06931) | 本文提出了FedLSM框架以解决医学图像分类中标签集不匹配问题，该框架采用不同的训练策略以有效利用未标记或部分标记的数据，并在分类层采用逐类别自适应聚合，适用于多个标签集的场景。 |
| [^100] | [Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box.](http://arxiv.org/abs/2304.05527) | 本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。 |
| [^101] | [StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables.](http://arxiv.org/abs/2304.03853) | StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。 |
| [^102] | [Arrhythmia Classifier Based on Ultra-Lightweight Binary Neural Network.](http://arxiv.org/abs/2304.01568) | 本研究提出了一种能够通过ECG信号进行5类和17类心律失常分类的超轻量级二值神经网络(BNN)，在存储使用率最低的情况下，分别达到了96.90%和97.50%的准确率。 |
| [^103] | [DRIP: Deep Regularizers for Inverse Problems.](http://arxiv.org/abs/2304.00015) | 提出了一种基于变分法的深度神经正则化器家族，保证可以适配数据并解决逆问题。在图像去模糊和小角度层析成像等问题上可行。 |
| [^104] | [Learning Causal Attributions in Neural Networks: Beyond Direct Effects.](http://arxiv.org/abs/2303.13850) | 本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。 |
| [^105] | [Random Inverse Problems Over Graphs: Decentralized Online Learning.](http://arxiv.org/abs/2303.11789) | 本文提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来，并发展了一种新的L2-渐近稳定性理论。该算法在网络图为连通且正向算子序列满足无限维度时空励磁条件的情况下，能够实现均方和几乎必然的强一致估计。 |
| [^106] | [Feature Unlearning for Pre-trained GANs and VAEs.](http://arxiv.org/abs/2303.05699) | 本文提出了一种从预训练的GAN和VAE模型中消除特定特征的方法，并通过实验证明了方法的有效性。 |
| [^107] | [PDSketch: Integrated Planning Domain Programming and Learning.](http://arxiv.org/abs/2303.05501) | 本文通过PDSketch语言和可训练的神经网络，实现了模型的学习和在线规划，加速了机器人的灵活性和通用性。 |
| [^108] | [Cross-domain Transfer Learning and State Inference for Soft Robots via a Semi-supervised Sequential Variational Bayes Framework.](http://arxiv.org/abs/2303.01693) | 本文提出了一个半监督顺序变分贝叶斯框架，用于解决软机器人领域的跨域迁移学习和状态推断问题。该框架可以处理某些机器人配置下存在缺失状态标签的情况，同时引入了特征空间迁移策略，提高了在多个配置下的潜在特征的适应性。 |
| [^109] | [Learning to Control Autonomous Fleets from Observation via Offline Reinforcement Learning.](http://arxiv.org/abs/2302.14833) | 本文提出了通过离线强化学习从观察中学习控制自主机群，并利用离线数据学习有效控制策略的方法。通过在真实出行系统数据上的实证研究，展示了离线学习恢复AMoD控制策略的能力。 |
| [^110] | [On marginal feature attributions of tree-based models.](http://arxiv.org/abs/2302.08434) | 该论文讨论了基于树模型的边际特征归因方法，与流行的TreeSHAP算法相比，边际Shapley值在相同函数的情况下保持一致，并且介绍了如何利用树模型的内部结构计算边际特征归因。 |
| [^111] | [LExecutor: Learning-Guided Execution.](http://arxiv.org/abs/2302.02343) | LExecutor是一个学习引导的执行方法，通过让神经模型预测缺失值，并将其注入到执行中，可以以自由约束的方式执行任意代码片段。该方法在Python代码和从Stack Overflow提取的代码片段上表现良好。 |
| [^112] | [NeuRI: Diversifying DNN Generation via Inductive Rule Inference.](http://arxiv.org/abs/2302.02261) | NeuRI是一种全自动化生成由数百种操作符组成的有效且多样化的DL模型的方法。它通过归纳式程序合成推断操作符约束条件，并采用符号和具体操作的混合模型生成。 |
| [^113] | [Vectorized Scenario Description and Motion Prediction for Scenario-Based Testing.](http://arxiv.org/abs/2302.01161) | 本论文提出了一种基于向量化的场景描述和运动预测的场景测试方法，通过合并不同场景的数据并利用向量化场景描述中的空间和时间细微差别，可以更准确地预测自动化车辆在未见过场景下的轨迹。 |
| [^114] | [Internally Rewarded Reinforcement Learning.](http://arxiv.org/abs/2302.00270) | 这项研究探讨了一类强化学习问题，其中策略的奖励信号由与之相关且同时优化的判别器生成，导致学习过程不稳定。实验结果表明，修剪线性奖励函数可以稳定训练过程。 |
| [^115] | [E-commerce users' preferences for delivery options.](http://arxiv.org/abs/2301.00666) | 研究通过设计陈述选择调查，调查了电子商务用户对交付选择的偏好，发现交付方式的费用、时间和时间段大小是用户选择的重要决定因素，同时也发现用户的偏好与年龄、性别、远程办公频率和是否有快递柜等社会人口特征有关。 |
| [^116] | [A System-Level View on Out-of-Distribution Data in Robotics.](http://arxiv.org/abs/2212.14020) | 本文旨在揭示机器人中视域外数据的系统层面观点，强调考虑机器人在OOD条件下的整体系统层面的能力，并为未来安全可靠的学习启用自主性研究提出关键研究问题。 |
| [^117] | [Robust Design and Evaluation of Predictive Algorithms under Unobserved Confounding.](http://arxiv.org/abs/2212.09844) | 本文提出了一种统一的方法来设计和评估在存在未观察到的混淆数据中的预测算法，通过对预测性能估计量的边界进行去偏倚的机器学习估计，从而解决了预测算法在选择性观察情境中的问题。 |
| [^118] | [Rethinking the Role of Pre-Trained Networks in Source-Free Domain Adaptation.](http://arxiv.org/abs/2212.07585) | 本研究提出将预训练网络整合到目标领域自适应中，通过共同学习策略提取有用的目标领域信息，以改善源模型的自适应性能。实验证明这种策略能够成功地提高自适应性能，并与现有的方法集成。 |
| [^119] | [A Neural-Network-Based Convex Regularizer for Image Reconstruction.](http://arxiv.org/abs/2211.12461) | 本研究提出了基于神经网络的凸规则化器用于图像重建。该方法通过重新审视由凸脊函数组成的规则化器，并使用具有单个隐藏层的神经网络对其梯度进行参数化。实验证明，在去噪、CT和MRI重建方面，该方法在提供类似可靠性保证的情况下，能够取得更好的结果。 |
| [^120] | [Distributed Graph Neural Network Training: A Survey.](http://arxiv.org/abs/2211.00216) | 这项调查研究了分布式图神经网络训练中的挑战，并提出了解决方案来优化特征通信、模型精度和分布式同步。 |
| [^121] | [Differentially Private Diffusion Models.](http://arxiv.org/abs/2210.09929) | 本研究提出了一种差分隐私扩散模型(DPDMs)，通过差分隐私训练生成模型，实现对隐私的保护，在图像生成基准测试中表现优越，能够在标准测试中与特定任务的DP-SGD训练的分类器相媲美。 |
| [^122] | [Pathology Steered Stratification Network for Subtype Identification in Alzheimer's Disease.](http://arxiv.org/abs/2210.05880) | 本文提出了一种病理学引导的分层网络（PSSN），通过反应扩散模型将已建立的AD病理学知识与机器学习相结合，预测了阿尔茨海默病患者的长期轨迹。这一方法填补了当前缺失的神经病理学信息。 |
| [^123] | [Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving.](http://arxiv.org/abs/2208.12263) | 本研究提出了Scene-Rep Transformer来提升强化学习决策能力，通过改进场景表示编码和顺序预测潜在蒸馏。采用多阶段Transformer编码器建模交互意识和意图意识，并使用顺序潜在Transformer进行自监督学习，加速训练和减少探索空间。 |
| [^124] | [Grammar-Based Grounded Lexicon Learning.](http://arxiv.org/abs/2202.08806) | 基于语法的基础词汇学习（G2L2）是一种从基础数据中学习语言含义表示的方法，通过将单词映射到语法类型和神经符号语义程序，利用基于语法的组合推导句子的含义，最终可以在基础输入上执行。 |
| [^125] | [On the in vivo recognition of kidney stones using machine learning.](http://arxiv.org/abs/2201.08865) | 本研究通过比较不同机器学习方法和深度学习架构在体内图像上识别肾结石的性能，为肾结石诊断提供了重要的参考。 |
| [^126] | [Early Stopping for Deep Image Prior.](http://arxiv.org/abs/2112.06074) | 本文提出了一种早停策略来解决深度图像先验中的过拟合问题，通过在多个视觉任务和DIP变体中持续检测接近最佳性能，突破了DIP实用性的限制。 |
| [^127] | [Dynamics of Local Elasticity During Training of Neural Nets.](http://arxiv.org/abs/2111.01166) | 本论文研究了神经网络训练过程中的局部弹性动力学。通过对现有$S_{\rm rel}$定义的全面研究并提出新的定义，我们发现新的定义能更敏锐地检测出权重更新更偏向于在与样本数据同一类别内进行预测变化的特性。 |
| [^128] | [Scenario generation for market risk models using generative neural networks.](http://arxiv.org/abs/2109.10072) | 本研究展示了如何使用生成性神经网络作为经济场景生成器，将其应用于整个内部市场风险模型，具有与欧洲监管批准的内部模型相似的结果。 |
| [^129] | [Implicit Regularization of Bregman Proximal Point Algorithm and Mirror Descent on Separable Data.](http://arxiv.org/abs/2108.06808) | 通过对可分数据使用Bregman proximal point算法和Mirror Descent进行学习线性分类器的研究, 我们发现BPPA具有可验证的算法正则化性质, 并且证明了边界与Bregman距离之间的关联性, 这揭示了BPPA对于学习分类器质量的影响和重要性 |
| [^130] | [On Model Identification and Out-of-Sample Prediction of Principal Component Regression: Applications to Synthetic Controls.](http://arxiv.org/abs/2010.14449) | 该论文在高维度的误差变量固定设计环境中分析了主成分回归模型识别和样本外预测问题，并提出了优于已知最佳速率的非渐进预测保证。在分析过程中，引入了一种自然的线性代数条件来避免样本外预测的分布假设，并构建了一个假设检验来检查该条件在实践中的可行性。同时，该论文的结果也对合成对照研究提供了新的发现。 |
| [^131] | [Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks and Autoregressive Policy Decomposition.](http://arxiv.org/abs/2009.12462) | 这篇论文介绍了一种基于图神经网络和自回归策略分解的深度强化学习框架，能够处理符号关系问题的可变状态和动作空间，并在多个领域展现了广泛的适用性和令人印象深刻的零-shot泛化能力。 |
| [^132] | [Frequentist Regret Bounds for Randomized Least-Squares Value Iteration.](http://arxiv.org/abs/1911.00567) | 本文引入了一种改进版本的随机最小二乘值迭代（RLSVI）算法，通过对行动值函数的最小二乘逼近进行扰动，诱导出了探索过程。在马尔可夫决策过程具有低秩转移动态的假设下，我们证明了RLSVI的频率后悔上界为$\widetilde O(d^2 H^2 \sqrt{T})$。这是对于带有函数逼近的随机探索的首个频率后悔分析。 |
| [^133] | [A Multilayer Framework for Online Metric Learning.](http://arxiv.org/abs/1805.05510) | 这项研究提出了一种多层框架的在线度量学习方法，用于捕捉实例之间的非线性相似性。不同于传统方法只能学习一个度量空间，该方法使用多个层次的度量空间来处理复杂的数据分布。 |

# 详细

[^1]: 揭示GNN中消息传递在双隐私保护上的作用

    Unveiling the Role of Message Passing in Dual-Privacy Preservation on GNNs. (arXiv:2308.13513v1 [cs.LG])

    [http://arxiv.org/abs/2308.13513](http://arxiv.org/abs/2308.13513)

    本论文揭示了GNN中消息传递在隐私泄露中的核心作用，并提出了一种有原则的双隐私保护GNN框架，有效保护节点和链路的隐私。

    

    图神经网络（GNN）是学习图形表示的强大工具，如社交网络。然而，它们易受隐私推断攻击的影响限制了它们的实用性，特别是在高风险领域。为了解决这个问题，提出了保护隐私的GNN，并着重于保护节点和/或链路隐私。本研究回顾了GNN对隐私泄露的贡献，并通过理论分析和模拟，确定了结构偏差下的消息传递作为GNN“传播”和“放大”隐私泄露的核心组成部分。基于这些发现，我们提出了一种有原则的双隐私保护GNN框架，有效地保护节点和链路隐私。该框架包括三个主要模块：一个敏感信息混淆模块，从节点嵌入中删除敏感信息；一个动态结构去偏模块。

    Graph Neural Networks (GNNs) are powerful tools for learning representations on graphs, such as social networks. However, their vulnerability to privacy inference attacks restricts their practicality, especially in high-stake domains. To address this issue, privacy-preserving GNNs have been proposed, focusing on preserving node and/or link privacy. This work takes a step back and investigates how GNNs contribute to privacy leakage. Through theoretical analysis and simulations, we identify message passing under structural bias as the core component that allows GNNs to \textit{propagate} and \textit{amplify} privacy leakage. Building upon these findings, we propose a principled privacy-preserving GNN framework that effectively safeguards both node and link privacy, referred to as dual-privacy preservation. The framework comprises three major modules: a Sensitive Information Obfuscation Module that removes sensitive information from node embeddings, a Dynamic Structure Debiasing Module th
    
[^2]: 提问澄清问题是否增加了生成代码的信心？关于大型语言模型的沟通能力的研究

    Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models. (arXiv:2308.13507v1 [cs.SE])

    [http://arxiv.org/abs/2308.13507](http://arxiv.org/abs/2308.13507)

    通过在生成代码之前提问澄清问题，大型语言模型的代码生成能力可以得到提升，增加了对生成代码的信心。

    

    大型语言模型(LLMs)显著提高了代码生成任务的能力。然而，LLMs在成为顶级软件工程师方面仍存在差距。基于观察到顶级软件工程师通常会提出澄清问题以减少需求和编码解决方案的不确定性，我们认为在代码生成任务中，LLMs也应该采用同样的方法。通过在生成最终代码之前提出深入的问题，可以减轻使用LLMs进行编程所面临的挑战，如意图规范不明确、计算思维不足和代码质量不理想。这反过来增加了对生成代码的自信。在这项工作中，我们探讨如何利用更好的沟通技巧来增加对生成代码的信心。我们提出了一个以沟通为中心的过程，利用LLM生成的沟通器来识别高度不确定或信心低的问题。

    Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, we explore how to leverage better communication skills to achieve greater confidence in generated code. We propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low conf
    
[^3]: A2Q:带有保证避免溢出功能的累加器感知量化

    A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance. (arXiv:2308.13504v1 [cs.LG])

    [http://arxiv.org/abs/2308.13504](http://arxiv.org/abs/2308.13504)

    A2Q是一种用于训练量化神经网络的累加器感知量化方法，通过引入一种独特的公式，并根据累加器位宽边界约束模型权重的L1范数，以避免在低精度累加器上发生溢出。该方法在计算机视觉任务中得到验证，能够有效地训练QNNs并保持模型准确性，特别适用于部署在可编程硬件如FPGAs上。

    

    我们提出了一种累加器感知量化（A2Q）方法，用于训练量化神经网络（QNNs），以避免在推理过程中使用低精度累加器时发生溢出。A2Q引入了一种受权值规范化启发的独特公式，根据我们导出的累加器位宽边界约束模型权重的L1范数。因此，在训练低精度累加的QNNs时，A2Q还自动促进非结构化权重稀疏性，以确保避免溢出。我们将该方法应用于基于深度学习的计算机视觉任务，展示了A2Q可以训练低精度累加器的QNNs，并保持模型准确性与浮点数基准相竞争。在我们的评估中，我们考虑了A2Q对通用平台和可编程硬件的影响。但是，我们主要针对在FPGAs上部署模型，因为它们可以被编程以充分利用自定义累加器位宽。我们的实验证明了A2Q在使用低精度累加器时可以有效地训练QNNs，并保持模型的准确性。

    We present accumulator-aware quantization (A2Q), a novel weight quantization method designed to train quantized neural networks (QNNs) to avoid overflow when using low-precision accumulators during inference. A2Q introduces a unique formulation inspired by weight normalization that constrains the L1-norm of model weights according to accumulator bit width bounds that we derive. Thus, in training QNNs for low-precision accumulation, A2Q also inherently promotes unstructured weight sparsity to guarantee overflow avoidance. We apply our method to deep learning-based computer vision tasks to show that A2Q can train QNNs for low-precision accumulators while maintaining model accuracy competitive with a floating-point baseline. In our evaluations, we consider the impact of A2Q on both general-purpose platforms and programmable hardware. However, we primarily target model deployment on FPGAs because they can be programmed to fully exploit custom accumulator bit widths. Our experimentation sho
    
[^4]: 逃离样本陷阱：使用配对距离估计器快速准确地估计认识不确定性

    Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators. (arXiv:2308.13498v1 [cs.LG])

    [http://arxiv.org/abs/2308.13498](http://arxiv.org/abs/2308.13498)

    本文介绍了使用配对距离估计器对集成模型进行认识不确定性估计的新方法，相比于常用的深度学习方法，该方法能够更快速、更准确地在更大的空间和更高维度上估计认识不确定性。

    

    本文介绍了一种使用配对距离估计器（PaiDEs）对集成模型进行认识不确定性估计的新方法。这些估计器利用模型组件之间的配对距离来建立熵的边界，并将这些边界作为基于信息准则的估计值。与最近基于样本的蒙特卡洛估计器用于认识不确定性估计的深度学习方法不同，PaiDEs能够在更大的空间（最多100倍）上以更快的速度（最多100倍）估计认识不确定性，并在更高维度上具有更准确的性能。为了验证我们的方法，我们进行了一系列用于评估认识不确定性估计的实验：一维正弦数据，摆动物体（Pendulum-v0），跳跃机器人（Hopper-v2），蚂蚁机器人（Ant-v2）和人形机器人（Humanoid-v2）。对于每个实验设置，我们应用了主动学习框架来展示PaiDEs在认识不确定性估计中的优势。

    This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
    
[^5]: Ngambay-French神经机器翻译（sba-Fr）论文

    Ngambay-French Neural Machine Translation (sba-Fr). (arXiv:2308.13497v1 [cs.CL])

    [http://arxiv.org/abs/2308.13497](http://arxiv.org/abs/2308.13497)

    该论文介绍了Ngambay-French神经机器翻译系统，并创造了第一个sba-Fr数据集，用于低资源语言的翻译研究。

    

    在非洲和全球范围内，越来越多的关注于开发神经机器翻译（NMT）系统，以克服语言障碍。对于低资源语言的NMT尤为引人注目，因为它涉及到有限标记数据的学习。然而，对于低资源语言来说，获取一个良好对齐的平行语料库可能有挑战性。查德地区全球少数语言的技术先进程度与NMT研究的不足之间存在明显差距。尚未尝试过在低资源查德语言上进行端到端NMT实验。此外，与一些非洲语言不同，自然语言处理领域的在线和结构化数据收集的数据匮乏。然而，通过指导性的数据收集方法，可以产生许多查德语言翻译对的双语数据，其中另一种语言是有大量数据的知名语言。在这个项目中，我们创建了第一个sba-Fr数据集，它是一个Ngambay到French的语料库。

    In Africa, and the world at large, there is an increasing focus on developing Neural Machine Translation (NMT) systems to overcome language barriers. NMT for Low-resource language is particularly compelling as it involves learning with limited labelled data. However, obtaining a well-aligned parallel corpus for low-resource languages can be challenging. The disparity between the technological advancement of a few global languages and the lack of research on NMT for local languages in Chad is striking. End-to-end NMT trials on low-resource Chad languages have not been attempted. Additionally, there is a dearth of online and well-structured data gathering for research in Natural Language Processing, unlike some African languages. However, a guided approach for data gathering can produce bitext data for many Chadian language translation pairs with well-known languages that have ample data. In this project, we created the first sba-Fr Dataset, which is a corpus of Ngambay-to-French transla
    
[^6]: TpuGraphs:一种关于大型张量计算图的性能预测数据集

    TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs. (arXiv:2308.13490v1 [cs.LG])

    [http://arxiv.org/abs/2308.13490](http://arxiv.org/abs/2308.13490)

    TpuGraphs是一种关于大型张量计算图的性能预测数据集，可用于优化编译器或自动调优工具的决策，并提供了图的执行时间信息。

    

    精确的硬件性能模型在代码优化中起着关键作用。它们可以帮助编译器做出启发性决策，或者帮助自动调优工具找到给定程序的最佳配置。本文介绍了TpuGraphs，一种在Tensor Processing Units（TPUs）上运行的全张量程序的性能预测数据集，这些程序以计算图的形式表示。数据集中的每个图表示机器学习工作负载的主要计算，例如训练周期或推断步骤。每个数据样本包含一个计算图、一个编译配置，以及使用该配置编译时图的执行时间。

    Precise hardware performance models play a crucial role in code optimizations. They can assist compilers in making heuristic decisions or aid autotuners in identifying the optimal configuration for a given program. For example, the autotuner for XLA, a machine learning compiler, discovered 10-20% speedup on state-of-the-art models serving substantial production traffic at Google. Although there exist a few datasets for program performance prediction, they target small sub-programs such as basic blocks or kernels. This paper introduces TpuGraphs, a performance prediction dataset on full tensor programs, represented as computational graphs, running on Tensor Processing Units (TPUs). Each graph in the dataset represents the main computation of a machine learning workload, e.g., a training epoch or an inference step. Each data sample contains a computational graph, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the datas
    
[^7]: 可通过在线动态嵌入预测减轻过时性的分布式GNN训练

    Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction. (arXiv:2308.13466v1 [cs.LG])

    [http://arxiv.org/abs/2308.13466](http://arxiv.org/abs/2308.13466)

    本文提出了一种新颖且可扩展的分布式GNN训练方法SAT，通过在线动态嵌入预测减轻了过时性带来的问题，解决了分布式GNN训练中的并发性和通信开销的困扰。

    

    尽管图神经网络（GNNs）取得了近期的成功，但由于邻居扩散，仍然难以在大规模图上进行训练。分布式计算成为一种有希望的解决方案，通过利用丰富的计算资源（如GPU）。然而，在分布式GNN训练中，由于图数据的节点依赖性增加，实现高并发性变得困难，这会导致巨大的通信开销。为了解决这个问题，历史值近似被认为是一种有希望的分布式训练技术类别。它利用离线内存缓存历史信息（如节点嵌入），作为精确值的可承受的近似，并实现了高并发性。然而，这些好处是以使用过时的训练信息为代价的，导致过时性、不准确性和收敛性问题。为了克服这些挑战，本文提出了SAT（减轻过时性训练），这是一种新颖且可扩展的分布式GNN训练方法。

    Despite the recent success of Graph Neural Networks (GNNs), it remains challenging to train GNNs on large-scale graphs due to neighbor explosions. As a remedy, distributed computing becomes a promising solution by leveraging abundant computing resources (e.g., GPU). However, the node dependency of graph data increases the difficulty of achieving high concurrency in distributed GNN training, which suffers from the massive communication overhead. To address it, Historical value approximation is deemed a promising class of distributed training techniques. It utilizes an offline memory to cache historical information (e.g., node embedding) as an affordable approximation of the exact value and achieves high concurrency. However, such benefits come at the cost of involving dated training information, leading to staleness, imprecision, and convergence issues. To overcome these challenges, this paper proposes SAT (Staleness-Alleviated Training), a novel and scalable distributed GNN training fr
    
[^8]: 学习干预概念瓶颈

    Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])

    [http://arxiv.org/abs/2308.13453](http://arxiv.org/abs/2308.13453)

    该论文提出了一种扩展了概念瓶颈模型的概念瓶颈记忆模型（CB2M），通过学习将干预推广到不同情境并重新应用先前干预来自动改善模型性能。当没有先前的人类干预信息时，CB2M能够检测错误并请求有针对性的干预。

    

    传统的深度学习模型缺乏解释性，而概念瓶颈模型（CBM）通过其概念表示提供固有的解释。具体而言，它们允许用户通过更新概念值并纠正模型的预测输出来进行干预交互。然而，传统方法中这些干预仅应用于模型一次后即被丢弃。为了纠正这一问题，我们提出了概念瓶颈记忆模型（CB2M），这是CBM的一个扩展。具体而言，CB2M通过双折叠记忆学习将干预的推广到适当的新情境中，从而能够学习检测错误并重新应用先前的干预。通过这种方式，CB2M能够从最初获得的少量干预中自动提高模型的性能。如果没有先前的人类干预信息，CB2M可以检测到CBM瓶颈的潜在错误并请求有针对性的干预。

    While traditional deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Specifically, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Traditionally, however, these interventions are applied to the model only once and discarded afterward. To rectify this, we present concept bottleneck memory models (CB2M), an extension to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate novel situations via a two-fold memory with which it can learn to detect mistakes and to reapply previous interventions. In this way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. In our experime
    
[^9]: 抓住它们：图匹配匹配滤波中的解决方案多样化

    Gotta match 'em all: Solution diversification in graph matching matched filters. (arXiv:2308.13451v1 [stat.ML])

    [http://arxiv.org/abs/2308.13451](http://arxiv.org/abs/2308.13451)

    本文提出了一种在大规模背景图中查找多个嵌入的模板图的新方法，通过迭代惩罚相似度矩阵来实现多样化匹配的发现，并提出了算法加速措施。在理论验证和实验证明中，证明了该方法的可行性和实用性。

    

    我们提出了一种在非常大的背景图中查找多个嵌入在其中的模板图的新方法。我们的方法基于Sussman等人提出的图匹配匹配滤波技术，通过在匹配滤波算法中迭代地惩罚合适的节点对相似度矩阵来实现多样化匹配的发现。此外，我们提出了算法加速，极大地提高了我们的匹配滤波方法的可扩展性。我们在相关的Erdos-Renyi图设置中对我们的方法进行了理论上的验证，显示其在温和的模型条件下能够顺序地发现多个模板。我们还通过使用模拟模型和真实世界数据集（包括人脑连接组和大型交易知识库）进行了大量实验证明了我们方法的实用性。

    We present a novel approach for finding multiple noisily embedded template graphs in a very large background graph. Our method builds upon the graph-matching-matched-filter technique proposed in Sussman et al., with the discovery of multiple diverse matchings being achieved by iteratively penalizing a suitable node-pair similarity matrix in the matched filter algorithm. In addition, we propose algorithmic speed-ups that greatly enhance the scalability of our matched-filter approach. We present theoretical justification of our methodology in the setting of correlated Erdos-Renyi graphs, showing its ability to sequentially discover multiple templates under mild model conditions. We additionally demonstrate our method's utility via extensive experiments both using simulated models and real-world dataset, include human brain connectomes and a large transactional knowledge base.
    
[^10]: 线性化神经网络的六个讲座

    Six Lectures on Linearized Neural Networks. (arXiv:2308.13431v1 [stat.ML])

    [http://arxiv.org/abs/2308.13431](http://arxiv.org/abs/2308.13431)

    这篇论文通过分析线性模型，探讨了多层神经网络行为的学习。研究使用了线性回归、核岭回归、随机特征模型和神经切线模型等四个线性化神经网络模型。研究还讨论了线性理论的局限性和其他方法如何克服这些局限性。

    

    在这六个讲座中，我们从线性模型的分析中探讨了对多层神经网络行为的学习。我们首先回顾了通过所谓的懒惰模式将神经网络与线性模型相对应的情况。然后，我们回顾了线性化神经网络的四个模型：带有集中特征的线性回归，核岭回归，随机特征模型和神经切线模型。最后，我们强调了线性理论的局限性，并讨论了其他方法如何克服这些局限性。

    In these six lectures, we examine what can be learnt about the behavior of multi-layer neural networks from the analysis of linear models. We first recall the correspondence between neural networks and linear models via the so-called lazy regime. We then review four models for linearized neural networks: linear regression with concentrated features, kernel ridge regression, random feature model and neural tangent model. Finally, we highlight the limitations of the linear theory and discuss how other approaches can overcome them.
    
[^11]: Nougat: 用于学术文档的神经光学理解

    Nougat: Neural Optical Understanding for Academic Documents. (arXiv:2308.13418v1 [cs.LG])

    [http://arxiv.org/abs/2308.13418](http://arxiv.org/abs/2308.13418)

    Nougat是一种用于学术文档的神经光学理解模型，通过光学字符识别任务将科学文档转换成标记语言，提高了科学知识的可访问性，并提供了模型和代码以加速未来的科学文本识别工作。

    

    科学知识主要存储在图书和科学期刊中，通常以PDF格式存在。然而，PDF格式会导致语义信息的丢失，尤其是数学表达式。我们提出了Nougat（用于学术文档的神经光学理解），一种利用视觉转换模型进行光学字符识别（OCR）任务，将科学文档处理成标记语言，并在新数据集上展示了我们模型的有效性。这种方法为在数字时代增强科学知识的可访问性提供了有希望的解决方案，弥合了人类可读文档和机器可读文本之间的差距。我们发布了模型和代码，以加速未来科学文本识别工作的进行。

    Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.
    
[^12]: 对深度学习模型选择对心脏磁共振分割中性别和种族偏差的影响的研究

    An investigation into the impact of deep learning model choice on sex and race bias in cardiac MR segmentation. (arXiv:2308.13415v1 [eess.IV])

    [http://arxiv.org/abs/2308.13415](http://arxiv.org/abs/2308.13415)

    这项研究调查了深度学习模型选择对心脏磁共振分割中性别和种族偏差的影响，发现不同模型之间偏差的严重性和性质不同，强调了模型选择在训练公平的基于AI的医学成像分割模型时的重要性。

    

    在医学成像中，人工智能（AI）越来越多地被用于自动化例行任务。然而，这些算法可能会出现并加剧偏差，导致受保护的群体之间的不平等表现。我们研究了模型选择对基于AI的心脏磁共振图像分割中训练数据集中主体性别和种族不平衡所产生的影响。我们评估了三个基于卷积神经网络的模型和一个视觉变换器模型。我们发现四个模型中有三个存在显著的性别偏差，而所有模型中都存在种族偏差。然而，偏差的严重程度和性质因模型而异，这凸显了模型选择在尝试训练公平的基于AI的医学成像分割模型时的重要性。

    In medical imaging, artificial intelligence (AI) is increasingly being used to automate routine tasks. However, these algorithms can exhibit and exacerbate biases which lead to disparate performances between protected groups. We investigate the impact of model choice on how imbalances in subject sex and race in training datasets affect AI-based cine cardiac magnetic resonance image segmentation. We evaluate three convolutional neural network-based models and one vision transformer model. We find significant sex bias in three of the four models and racial bias in all of the models. However, the severity and nature of the bias varies between the models, highlighting the importance of model choice when attempting to train fair AI-based segmentation models for medical imaging tasks.
    
[^13]: 使用视觉和车辆传感器进行驾驶行为分析：一项综述

    Using Visual and Vehicular Sensors for Driver Behavior Analysis: A Survey. (arXiv:2308.13406v1 [cs.LG])

    [http://arxiv.org/abs/2308.13406](http://arxiv.org/abs/2308.13406)

    该综述介绍了使用视觉和车辆传感器分析驾驶行为的技术，指出将视觉和车辆信息相结合可以提高分析的准确性和效果，从而改善驾驶安全和减少交通事故。

    

    在美国，高风险驾驶者占致命事故的70%。随着传感器和智能车载系统的最新进展，对于评估驾驶行为以改善驾驶体验和道路安全的研究显著增加。本文通过检查使用视觉和车辆数据分析驾驶行为的各种技术，概述了该领域的最新研究。本文还讨论了该领域面临的挑战和未解决问题，并提供了未来研究的潜在建议。综述得出结论，将视觉和车辆信息相结合可以显著提高驾驶行为分析的准确性和效果，从而改善安全措施并减少交通事故。

    Risky drivers account for 70% of fatal accidents in the United States. With recent advances in sensors and intelligent vehicular systems, there has been significant research on assessing driver behavior to improve driving experiences and road safety. This paper examines the various techniques used to analyze driver behavior using visual and vehicular data, providing an overview of the latest research in this field. The paper also discusses the challenges and open problems in the field and offers potential recommendations for future research. The survey concludes that integrating vision and vehicular information can significantly enhance the accuracy and effectiveness of driver behavior analysis, leading to improved safety measures and reduced traffic accidents.
    
[^14]: EntropyRank: 通过基于语言模型的文本压缩的副信息优化来进行无监督关键词提取

    EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression. (arXiv:2308.13399v1 [cs.CL])

    [http://arxiv.org/abs/2308.13399](http://arxiv.org/abs/2308.13399)

    该论文提出了一种无监督的关键词提取方法，通过利用预训练语言模型和信息论方法，在文本中提取具有最高条件熵的短语作为关键词。实验证明，该方法在关键词提取任务上取得了与常用方法相当的结果。

    

    我们提出了一种无监督的方法，基于预训练的语言模型（LM）和Shannon的信息最大化，从文本中提取关键词和关键词短语。具体来说，我们的方法提取在LM下具有最高条件熵的短语。得到的关键词短语集合解决了一个相关的信息论问题：如果作为副信息提供，它会导致使用LM和熵编码器对文本进行压缩时的预期最小二进制码长度。另外，得到的集合是通过因果LM对在给定条件下最小化文本熵的短语集合的近似。在实证上，该方法在各种关键词提取基准挑战中提供了与最常用方法可比较的结果。

    We propose an unsupervised method to extract keywords and keyphrases from texts based on a pre-trained language model (LM) and Shannon's information maximization. Specifically, our method extracts phrases having the highest conditional entropy under the LM. The resulting set of keyphrases turns out to solve a relevant information-theoretic problem: if provided as side information, it leads to the expected minimal binary code length in compressing the text using the LM and an entropy encoder. Alternately, the resulting set is an approximation via a causal LM to the set of phrases that minimize the entropy of the text when conditioned upon it. Empirically, the method provides results comparable to the most commonly used methods in various keyphrase extraction benchmark challenges.
    
[^15]: TFDNet：增强时频分解网络用于长期时间序列预测

    TFDNet: Time-Frequency Enhanced Decomposed Network for Long-term Time Series Forecasting. (arXiv:2308.13386v1 [cs.LG])

    [http://arxiv.org/abs/2308.13386](http://arxiv.org/abs/2308.13386)

    这篇论文提出了一种称为TFDNet的时频增强分解网络，用于从时频域捕获长期时间序列的基本模式和时间周期性。

    

    长期时间序列预测是一项重要任务，并且具有广泛的实际应用。最近的方法专注于捕获来自单一域（例如时间域或频率域）的基本模式，并且没有从时频域综合处理长期时间序列。在本文中，我们提出了一种称为时频增强分解网络（TFDNet）的方法，可以在时频域捕获长期的基本模式和时间周期性。在TFDNet中，我们设计了一个多尺度时频增强编码器主干，并开发两个分别用于捕获多分辨率中分解趋势和季节分量中的不同模式的趋势和季节时频块。通过研究和整合多变量时间序列的潜在不同通道相关模式，我们探索了时频块中核操作的多样化内核学习策略。

    Long-term time series forecasting is a vital task and has a wide range of real applications. Recent methods focus on capturing the underlying patterns from one single domain (e.g. the time domain or the frequency domain), and have not taken a holistic view to process long-term time series from the time-frequency domains. In this paper, we propose a Time-Frequency Enhanced Decomposed Network (TFDNet) to capture both the long-term underlying patterns and temporal periodicity from the time-frequency domain. In TFDNet, we devise a multi-scale time-frequency enhanced encoder backbone and develop two separate trend and seasonal time-frequency blocks to capture the distinct patterns within the decomposed trend and seasonal components in multi-resolutions. Diverse kernel learning strategies of the kernel operations in time-frequency blocks have been explored, by investigating and incorporating the potential different channel-wise correlation patterns of multivariate time series. Experimental e
    
[^16]: 基于上下文学习的无模型系统辨识

    In-context learning for model-free system identification. (arXiv:2308.13380v1 [eess.SY])

    [http://arxiv.org/abs/2308.13380](http://arxiv.org/abs/2308.13380)

    本文提出了一种基于上下文学习的无模型系统辨识方法，通过观察同一类别中其他系统的行为来理解动态系统的复杂性。

    

    在传统的系统辨识中，我们通过给定的输入/输出序列和可用的物理知识来估计未知动态系统的模型。然而，是否还可以通过观察同一类别中其他系统的行为，而不仅仅是从它们的输入/输出模式中理解动态系统的复杂性呢？这个核心问题驱动着本文的研究。作为对这个问题的回应，我们引入了一种新的系统辨识范式，解决了两个主要任务：一步预测和多步模拟。与传统方法不同的是，我们不直接对特定系统进行模型估计，而是预先训练一个代表动态系统类别的元模型。该元模型是通过从某个分布中随机抽取的系统生成的潜在无限流的合成数据进行训练的。在其核心，元模型作为对主要特征的隐式表示，

    In traditional system identification, we estimate a model of an unknown dynamical system based on given input/output sequences and available physical knowledge. Yet, is it also possible to understand the intricacies of dynamical systems not solely from their input/output patterns, but by observing the behavior of other systems within the same class? This central question drives the study presented in this paper.  In response to this query, we introduce a novel paradigm for system identification, addressing two primary tasks: one-step-ahead prediction and multi-step simulation. Unlike conventional methods, we do not directly estimate a model for the specific system. Instead, we pretrain a meta model that represents a class of dynamical systems. This meta model is trained from a potentially infinite stream of synthetic data, generated by systems randomly extracted from a certain distribution. At its core, the meta model serves as an implicit representation of the main characteristics of 
    
[^17]: 通过长短期记忆网络和独立成分分析结合的方法，从单通道和多通道脑电图记录中去除眼电图伪迹

    EOG Artifact Removal from Single and Multi-channel EEG Recordings through the combination of Long Short-Term Memory Networks and Independent Component Analysis. (arXiv:2308.13371v1 [eess.SP])

    [http://arxiv.org/abs/2308.13371](http://arxiv.org/abs/2308.13371)

    本文提出了一种将长短期记忆网络与独立成分分析相结合的方法，用于从受污染的脑电图信号中去除眼电图伪迹。

    

    引言：由于其丰富的信息内容，脑电图(EEG)信号在各种应用中获得了显著的流行度。然而，这些信号容易受到来自各种伪迹源的污染，尤其是由眼睛运动引起的眼电图(EOG)伪迹。解决EOG伪迹的最有效方法是同时记录EOG信号和EEG信号，并使用盲源分离技术，如独立成分分析(ICA)。然而，在预先录制的数据集中，EOG记录的可用性并不总是可行的。目标：在本文中，我们提出了一种新的方法，该方法将基于长短期记忆(LSTM)的神经网络与ICA相结合，以解决从受污染的EEG信号中去除EOG伪迹的挑战。方法：我们的方法旨在实现两个主要目标：1) 从受污染的EEG数据中估计水平和垂直的EOG信号；2) 使用ICA对估计的EOG信号进行盲源分离。

    Introduction: Electroencephalogram (EEG) signals have gained significant popularity in various applications due to their rich information content. However, these signals are prone to contamination from various sources of artifacts, notably the electrooculogram (EOG) artifacts caused by eye movements. The most effective approach to mitigate EOG artifacts involves recording EOG signals simultaneously with EEG and employing blind source separation techniques, such as independent component analysis (ICA). Nevertheless, the availability of EOG recordings is not always feasible, particularly in pre-recorded datasets. Objective: In this paper, we present a novel methodology that combines a long short-term memory (LSTM)-based neural network with ICA to address the challenge of EOG artifact removal from contaminated EEG signals. Approach: Our approach aims to accomplish two primary objectives: 1) estimate the horizontal and vertical EOG signals from the contaminated EEG data, and 2) employ ICA 
    
[^18]: 深度学习和数据分析中的部分等变性的拓扑模型

    A topological model for partial equivariance in deep learning and data analysis. (arXiv:2308.13357v1 [stat.ML])

    [http://arxiv.org/abs/2308.13357](http://arxiv.org/abs/2308.13357)

    本文提出了一种拓扑模型，用于在神经网络中编码部分等变性，并研究了相应的测量空间和P-GENEO空间的性质。

    

    本文提出了一种拓扑模型，用于在神经网络中编码部分等变性。为此，我们引入了一类称为P-GENEO的运算符，以非扩张的方式改变通过测量表示的数据，并遵循一定集合的变换作用。如果变换作用的集合是一个群，则得到所谓的GENEO。然后，我们研究了受某些自映射作用的测量空间以及这些空间之间的P-GENEO空间。我们在它们上定义了伪度量，并展示了一些结果空间的性质。特别地，我们展示了这些空间具有便利的逼近和凸性质。

    In this article, we propose a topological model to encode partial equivariance in neural networks. To this end, we introduce a class of operators, called P-GENEOs, that change data expressed by measurements, respecting the action of certain sets of transformations, in a non-expansive way. If the set of transformations acting is a group, then we obtain the so-called GENEOs. We then study the spaces of measurements, whose domains are subject to the action of certain self-maps, and the space of P-GENEOs between these spaces. We define pseudo-metrics on them and show some properties of the resulting spaces. In particular, we show how such spaces have convenient approximation and convexity properties.
    
[^19]: 关于语言选择对训练和评估编程语言模型的影响

    On the Impact of Language Selection for Training and Evaluating Programming Language Models. (arXiv:2308.13354v1 [cs.SE])

    [http://arxiv.org/abs/2308.13354](http://arxiv.org/abs/2308.13354)

    这项研究根据使用CodeBERT模型分析编程语言的表示，发现编程语言之间在标记表示方面存在差异，建议使用这种相似度度量方法来选择跨多种语言的模型。

    

    基于Transformer的语言模型的最新进展显示出在增强这些模型的多语言能力方面具有显著潜力。在自然语言任务中取得的显著进展不仅适用于编程语言领域，而且还扩展到编程语言领域。尽管这些模型具备从多种语言中学习的能力，但评估通常只关注同一种语言的特定组合。在本研究中，我们使用基于CodeBERT模型的编程语言表示分析来评估编程语言的相似性。我们的实验揭示了像C++、Python和Java这样的语言中的标记表示之间存在相近性，而像Mathematica和R这样的语言中的相同标记显示出显著的不相似性。我们的研究结果表明，当处理多种语言时，这种现象可能导致性能挑战。因此，我们建议使用我们的相似度度量来选择一个可以平衡多种语言的模型。

    The recent advancements in Transformer-based Language Models have demonstrated significant potential in enhancing the multilingual capabilities of these models. The remarkable progress made in this domain not only applies to natural language tasks but also extends to the domain of programming languages. Despite the ability of these models to learn from multiple languages, evaluations typically focus on particular combinations of the same languages. In this study, we evaluate the similarity of programming languages by analyzing their representations using a CodeBERT-based model. Our experiments reveal that token representation in languages such as C++, Python, and Java exhibit proximity to one another, whereas the same tokens in languages such as Mathematica and R display significant dissimilarity. Our findings suggest that this phenomenon can potentially result in performance challenges when dealing with diverse languages. Thus, we recommend using our similarity measure to select a div
    
[^20]: 一个用于完全无监督异常检测的通用机器学习框架，适用于污染数据

    A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])

    [http://arxiv.org/abs/2308.13352](http://arxiv.org/abs/2308.13352)

    这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。

    

    在各个领域和应用中，机器学习算法已经解决了异常检测（AD）任务。这些算法中大多数使用正常数据对一个基于残差的模型进行训练，并根据未见样本与学习到的正常范围的不相似性来分配异常分数。这些方法的基本假设是可以用无异常的数据进行训练。然而，在真实世界中的操作环境中，训练数据通常会与一定比例的异常样本混合。而利用污染数据进行训练必然会导致基于残差的算法的AD性能下降。本文介绍了一个完全无监督的用于AD任务的污染训练数据的改进框架。该框架是通用的，可应用于任何基于残差的机器学习模型。我们展示了该框架在两个多元时间数据集上的应用。

    Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
    
[^21]: 基于压缩机的房颤检测分类器

    Compressor-Based Classification for Atrial Fibrillation Detection. (arXiv:2308.13328v1 [eess.SP])

    [http://arxiv.org/abs/2308.13328](http://arxiv.org/abs/2308.13328)

    本文研究了基于压缩机的文本分类方法在房颤检测中的应用。通过对心律间隔序列进行压缩距离计算和最近邻分类器模型优化，我们实现了良好的分类性能，接近于最佳的房颤检测算法。这表明gzip分类器适用于生物医学数据和连续随机序列的分类。

    

    房颤是最常见的心律失常之一，对公共健康有重大影响。自动检测房颤发作是生物医学工程中最重要的任务之一。本文将最近引入的基于压缩机的文本分类方法应用于房颤检测任务（心律之间的二分类）。我们研究了对ΔRR和RR间期序列应用归一化压缩距离，k-最近邻分类器的配置，以及最佳窗口长度。我们实现了良好的分类结果（平均敏感度=97.1%，平均特异度=91.7%，最佳敏感度为99.8%，最佳特异度为97.6% 5折交叉验证）。所获得的性能接近于最佳的专门的房颤检测算法。我们的结果表明，gzip分类器，最初用于文本，也适用于生物医学数据和连续随机序列。

    Atrial fibrillation (AF) is one of the most common arrhythmias with challenging public health implications. Automatic detection of AF episodes is therefore one of the most important tasks in biomedical engineering. In this paper, we apply the recently introduced method of compressor-based text classification to the task of AF detection (binary classification between heart rhythms). We investigate the normalised compression distance applied to $\Delta$RR and RR-interval sequences, the configuration of the k-Nearest Neighbour classifier, and an optimal window length. We achieve good classification results (avg. sensitivity = 97.1%, avg. specificity = 91.7%, best sensitivity of 99.8%, best specificity of 97.6% with 5-fold cross-validation). Obtained performance is close to the best specialised AF detection algorithms. Our results suggest that gzip classification, originally proposed for texts, is suitable for biomedical data and continuous stochastic sequences in general.
    
[^22]: 微调可能削弱基础模型；保留特征可能是解决方案

    Fine-tuning can cripple your foundation model; preserving features may be the solution. (arXiv:2308.13320v1 [cs.LG])

    [http://arxiv.org/abs/2308.13320](http://arxiv.org/abs/2308.13320)

    在微调过程中，基础模型可能会遗忘概念，我们提出了一种名为LDIFS的方法，用于解决这个问题，该方法在实验证明效果显著。

    

    预训练的基础模型主要由于其巨大的容量和对从互联网上爬取的大量训练数据的暴露，享有存储关于许多现实世界概念的知识的优势。这些模型通常在下游数据集上进行微调，以产生出色的最新性能。然而，我们观察到，与预训练模型相比，微调模型在与下游任务不同的任务上识别概念的能力显著降低。这显然是不可取的，因为在首次学习这些概念时，投入了大量的时间和金钱。我们将这种不可取的现象称为“概念遗忘”，通过实验证明大多数端到端微调方法都严重受到这种副作用的影响。为此，我们还提出了一个相当简单的解决方法，即设计了一种名为LDIFS的方法。

    Pre-trained foundation models, owing primarily to their enormous capacity and exposure to vast amount of training data scraped from the internet, enjoy the advantage of storing knowledge about plenty of real-world concepts. Such models are typically fine-tuned on downstream datasets to produce remarkable state-of-the-art performances. While various fine-tuning methods have been devised and are shown to be highly effective, we observe that a fine-tuned model's ability to recognize concepts on tasks $\textit{different}$ from the downstream one is reduced significantly compared to its pre-trained counterpart. This is clearly undesirable as a huge amount of time and money went into learning those very concepts in the first place. We call this undesirable phenomenon "concept forgetting" and via experiments show that most end-to-end fine-tuning approaches suffer heavily from this side effect. To this end, we also propose a rather simple fix to this problem by designing a method called LDIFS 
    
[^23]: 改造生成预训练变换器的输出: PGI框架对注意力动态的影响

    Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics. (arXiv:2308.13317v1 [cs.AI])

    [http://arxiv.org/abs/2308.13317](http://arxiv.org/abs/2308.13317)

    本文介绍了一种名为PGI的新方法，在实际商业问题中应用于GPT模型。该方法利用GPT模型的能力来理解复杂的语言结构，并生成上下文相关的回应。实验证实了PGI策略的有效性，并帮助解决了人类智能低度利用的问题。

    

    本文提出了一种名为Persona-Grouping-Intelligence (PGI)的新方法，旨在解决GPT模型在实际商业问题中的应用所带来的挑战。PGI利用GPT模型的内在能力来理解复杂的语言结构，并生成与上下文相关的回应。实验在一个商业场景中进行，该场景存在人类智能被低效的商业流程低度利用的问题。该方法的主要目标是利用GPT模型来减轻人类在广泛、单调和重复的任务中的工作负荷，将重点转向决策活动。该实验生成的4,000个回应的验证准确率为93.81%，突出了PGI策略的有效性。这种范式转变有效地解决了人类智能低度利用的问题，使企业环境与决策活动相一致。

    This paper presents a novel approach named Persona-Grouping-Intelligence (PGI), which has been crafted to tackle the challenges posed by GPT models when applied to real-world business issues. PGI leverages the inherent capabilities of the GPT model to comprehend intricate language structures and generate responses that are contextually relevant. The experiment occurred in a business scenario where human intelligence was being underutilized due to less optimized business processes. The primary objective of this approach is to leverage GPT models to reduce the workload on humans in tasks that are extensive, monotonous, and repetitive. Instead, the focus is redirected toward decision-making activities. Remarkably, the experiment yielded an accuracy rate of 93.81% in validating 4,000 responses generated by the model, underscoring the effectiveness of the PGI strategies. Effectively addressing the issue of underutilized human intelligence, this paradigm shift aligns business environments wi
    
[^24]: 激光和标本消失了！快速去除标本残留物和组织分割在血染乙酸洋红染色活检中

    Bang and the Artefacts are Gone! Rapid Artefact Removal and Tissue Segmentation in Haematoxylin and Eosin Stained Biopsies. (arXiv:2308.13304v1 [eess.IV])

    [http://arxiv.org/abs/2308.13304](http://arxiv.org/abs/2308.13304)

    该论文提出了一种基于H&E Otsu thresholding的方案，可以快速去除标本残留物和分割组织，在血染乙酸洋红染色的活检中取得了良好的效果。

    

    我们提出了一种H&E Otsu thresholding方案，用于快速检测全幻灯片图像中的组织，可以消除笔迹和扫描残留等各种不良标本残留物。我们的方法涉及到获取低放大倍率RGB全景图像的双峰表示，从而可以简单地使用Otsu thresholding方法将组织与背景和残留物分离。我们在来自各种机构和WSI数字扫描仪的WSI制备的图像上演示了我们的方法，每张图像都包含大量的残留物，其他方法都无法处理。我们方法的美妙之处在于其简洁性：通过操作RGB颜色空间并使用Otsu thresholding可以快速去除残留物并分割组织。

    We present H&E Otsu thresholding, a scheme for rapidly detecting tissue in whole-slide images (WSIs) that eliminates a wide range of undesirable artefacts such as pen marks and scanning artefacts. Our method involves obtaining a bid-modal representation of a low-magnification RGB overview image which enables simple Otsu thresholding to separate tissue from background and artefacts. We demonstrate our method on WSIs prepared from a wide range of institutions and WSI digital scanners, each containing substantial artefacts that cause other methods to fail. The beauty of our approach lies in its simplicity: manipulating RGB colour space and using Otsu thresholding allows for the rapid removal of artefacts and segmentation of tissue.
    
[^25]: 使用深度超参数化多任务学习来学习紧凑的神经网络

    Learning Compact Neural Networks with Deep Overparameterised Multitask Learning. (arXiv:2308.13300v1 [cs.LG])

    [http://arxiv.org/abs/2308.13300](http://arxiv.org/abs/2308.13300)

    本文提出了一种使用深度超参数化多任务学习来学习紧凑的神经网络的方法，通过在训练中超参数化模型架构并更有效地共享跨任务的超参数化模型参数，以实现更好的优化和泛化。

    

    紧凑的神经网络在实际应用中具有许多优点。然而，用小参数大小和低计算成本来训练紧凑的神经网络以达到与更复杂、更强大的体系结构相同或更好的模型性能通常是具有挑战性的。这在多任务学习中尤其如此，因为不同的任务竞争资源。我们提出了一种简单、高效、有效的多任务学习超参数化神经网络设计，通过在训练中超参数化模型架构并更有效地共享跨任务的超参数化模型参数，以实现更好的优化和泛化。在两个具有挑战性的多任务数据集（NYUv2和COCO）上的实验证明了所提方法在各种卷积网络和参数大小上的有效性。

    Compact neural network offers many benefits for real-world applications. However, it is usually challenging to train the compact neural networks with small parameter sizes and low computational costs to achieve the same or better model performance compared to more complex and powerful architecture. This is particularly true for multitask learning, with different tasks competing for resources. We present a simple, efficient and effective multitask learning overparameterisation neural network design by overparameterising the model architecture in training and sharing the overparameterised model parameters more effectively across tasks, for better optimisation and generalisation. Experiments on two challenging multitask datasets (NYUv2 and COCO) demonstrate the effectiveness of the proposed method across various convolutional networks and parameter sizes.
    
[^26]: 通过无线计算实现联邦线性赌博学习

    Federated Linear Bandit Learning via Over-the-Air Computation. (arXiv:2308.13298v1 [cs.LG])

    [http://arxiv.org/abs/2308.13298](http://arxiv.org/abs/2308.13298)

    本研究针对联邦线性赌博学习提出了一种通过无线计算的方案，以减少通信开销。通过在噪声衰落信道上进行的模拟信号传输，我们的方案在降低累积遗憾方面表现出竞争力。

    

    本文研究了在由服务器和多个设备组成的无线系统中的联邦背景下的线性赌博学习。每个设备与环境交互，在接收到奖励后选择一个动作，并将模型更新发送到服务器。主要目标是在有限的时间范围内最小化所有设备的累积遗憾。为了减少通信开销，设备通过无线计算（AirComp）在噪声衰落信道上与服务器通信，其中通道噪声可能会扭曲信号。在这个背景下，我们提出了一种定制的联邦线性赌博方案，其中每个设备传输一个模拟信号，服务器接收到的是这些信号的叠加，受到信道噪声的扭曲。我们进行了严格的数学分析，确定了该方案的遗憾上限。理论分析和数值实验都证明了我们提出的方案在性能方面的竞争力。

    In this paper, we investigate federated contextual linear bandit learning within a wireless system that comprises a server and multiple devices. Each device interacts with the environment, selects an action based on the received reward, and sends model updates to the server. The primary objective is to minimize cumulative regret across all devices within a finite time horizon. To reduce the communication overhead, devices communicate with the server via over-the-air computation (AirComp) over noisy fading channels, where the channel noise may distort the signals. In this context, we propose a customized federated linear bandits scheme, where each device transmits an analog signal, and the server receives a superposition of these signals distorted by channel noise. A rigorous mathematical analysis is conducted to determine the regret bound of the proposed scheme. Both theoretical analysis and numerical experiments demonstrate the competitive performance of our proposed scheme in terms o
    
[^27]: 训练具有计算密集型目标概率分布的归一化流

    Training normalizing flows with computationally intensive target probability distributions. (arXiv:2308.13294v1 [cs.LG])

    [http://arxiv.org/abs/2308.13294](http://arxiv.org/abs/2308.13294)

    本文提出了一种基于REINFORCE算法的归一化流估计器，用于训练具有计算密集型目标概率分布的问题。在二维Schwinger模型中的应用结果表明，相较于重新参数化技巧估计器，该方法能够在墙时钟时间上快10倍，且内存使用上节省30%。

    

    机器学习技术，特别是所谓的归一化流，在蒙特卡洛模拟中越来越受欢迎，因为它们可以有效地近似目标概率分布。在格点场论（LFT）的情况下，目标分布由作用的指数给出。基于“重新参数化技巧”的常见损失函数的梯度估计器需要对场的导数进行计算。对于复杂的非局部作用，如QCD中的费米子作用，这可能会带来显著的计算成本。在本文中，我们提出了一种基于REINFORCE算法的归一化流估计器，以避免这个问题。我们将其应用于关键性的二维Schwinger模型与Wilson费米子，并展示了它相对于重新参数化技巧估计器在墙时钟时间上快10倍以及在内存使用上节省30%的优势。

    Machine learning techniques, in particular the so-called normalizing flows, are becoming increasingly popular in the context of Monte Carlo simulations as they can effectively approximate target probability distributions. In the case of lattice field theories (LFT) the target distribution is given by the exponential of the action. The common loss function's gradient estimator based on the "reparametrization trick" requires the calculation of the derivative of the action with respect to the fields. This can present a significant computational cost for complicated, non-local actions like e.g. fermionic action in QCD. In this contribution, we propose an estimator for normalizing flows based on the REINFORCE algorithm that avoids this issue. We apply it to two dimensional Schwinger model with Wilson fermions at criticality and show that it is up to ten times faster in terms of the wall-clock time as well as requiring up to $30\%$ less memory than the reparameterization trick estimator. It 
    
[^28]: 基于贝叶斯主动学习的比较评判方法

    A Bayesian Active Learning Approach to Comparative Judgement. (arXiv:2308.13292v1 [cs.LG])

    [http://arxiv.org/abs/2308.13292](http://arxiv.org/abs/2308.13292)

    这项研究提出了一种基于贝叶斯主动学习的比较评判方法，用于解决传统教育评估中存在的一致性和偏见等问题，并探索了如何选择比较项目的可靠数量。

    

    评估是教育的关键部分。传统的评分方法存在一些问题，如一致性不足，存在无意识的偏见，给评估者带来较大的认知负担。比较评判（CJ）是一种解决这些问题的方法。在CJ中，评估者以一对项目为单位，选择哪个更好。通过一系列比较，可以使用排名模型（例如BTM）推导出一个排名。虽然CJ被认为是一种可靠的评分方法，但仍存在透明度和生成可靠排名所需的对比次数的理想数量等问题。此外，已有尝试提出了一些方法以有效方式选择下一个应比较的项目，但某些现有方法会在结果中产生自己的偏见，从而增加了所使用的可靠性度量。因此，通常使用随机选择的方法。本文提出了一种新颖的基于贝叶斯的方法。

    Assessment is a crucial part of education. Traditional marking is a source of inconsistencies and unconscious bias, placing a high cognitive load on the assessors. An approach to address these issues is comparative judgement (CJ). In CJ, the assessor is presented with a pair of items and is asked to select the better one. Following a series of comparisons, a rank is derived using a ranking model, for example, the BTM, based on the results. While CJ is considered a reliable method for marking, there are concerns around transparency, and the ideal number of pairwise comparisons to generate a reliable estimation of the rank order is not known. Additionally, there have been attempts to generate a method of selecting pairs that should be compared next in an informative manner, but some existing methods are known to have created their own bias within results inflating the reliability metric used. As a result, a random selection approach is usually deployed.  We propose a novel Bayesian appro
    
[^29]: JAX-LOB：一种基于GPU加速的限价单簿模拟器，为大规模强化学习交易解锁

    JAX-LOB: A GPU-Accelerated limit order book simulator to unlock large scale reinforcement learning for trading. (arXiv:2308.13289v1 [q-fin.TR])

    [http://arxiv.org/abs/2308.13289](http://arxiv.org/abs/2308.13289)

    JAX-LOB是第一个GPU加速的LOB模拟器，可以并行处理数千个订单簿，以较低的处理时间实现大规模强化学习交易，为金融交易研究提供了重要工具。

    

    全球金融交易所使用限价单簿（LOB）来处理订单和匹配交易。为了研究目的，需要具有大规模高效的LOB动态模拟器。以前曾在基于代理模型（ABMs），强化学习（RL）环境和生成模型的上下文中实现了LOB模拟器，处理来自历史数据集和手工代理的订单流。对于许多应用程序，需要处理多个订单簿，无论是用于ABM的校准还是RL代理的训练。我们展示了首个能够并行处理数千本订单簿且每个消息处理时间显著减少的GPU-enabled LOB模拟器-JAX-LOB的实现。我们的模拟器JAX-LOB的实现基于设计选择，旨在充分利用JAX的功能，同时不损害与LOB相关的机制的真实性。我们将JAX-LOB与其他JAX包集成，以提供如何适用的示例。

    Financial exchanges across the world use limit order books (LOBs) to process orders and match trades. For research purposes it is important to have large scale efficient simulators of LOB dynamics. LOB simulators have previously been implemented in the context of agent-based models (ABMs), reinforcement learning (RL) environments, and generative models, processing order flows from historical data sets and hand-crafted agents alike. For many applications, there is a requirement for processing multiple books, either for the calibration of ABMs or for the training of RL agents. We showcase the first GPU-enabled LOB simulator designed to process thousands of books in parallel, with a notably reduced per-message processing time. The implementation of our simulator - JAX-LOB - is based on design choices that aim to best exploit the powers of JAX without compromising on the realism of LOB-related mechanisms. We integrate JAX-LOB with other JAX packages, to provide an example of how one may ad
    
[^30]: AtmoRep:一种利用大规模表示学习的大气动力学随机模型

    AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning. (arXiv:2308.13280v1 [physics.ao-ph])

    [http://arxiv.org/abs/2308.13280](http://arxiv.org/abs/2308.13280)

    提出了一种称为AtmoRep的大气动力学随机模型，它利用大规模表示学习和人工智能技术来确定复杂的大气动力学的通用描述，从而为各种应用提供技能结果。

    

    大气对人类有多种影响，从因天气不良而丧生的损失到对社会的长期社会和经济影响。因此，对大气动力学进行计算机模拟对我们和未来的世代的福祉非常重要。在这里，我们提出了AtmoRep，一种新颖的、与任务无关的大气动力学随机计算机模型，可以为广泛的应用提供技能结果。AtmoRep利用人工智能的大规模表示学习来确定大气高度复杂、随机动力学的通用描述，该描述基于历史轨迹的最佳可用估计，这些历史轨迹受观测约束。这是通过一种新颖的自监督学习目标和一个独特的集合实现的，该集合从随机模型中采样，其可变性受历史记录中的可变性启发。AtmoRep的任务无关性使其能够为各种应用提供灵活的结果。

    The atmosphere affects humans in a multitude of ways, from loss of life due to adverse weather effects to long-term social and economic impacts on societies. Computer simulations of atmospheric dynamics are, therefore, of great importance for the well-being of our and future generations. Here, we propose AtmoRep, a novel, task-independent stochastic computer model of atmospheric dynamics that can provide skillful results for a wide range of applications. AtmoRep uses large-scale representation learning from artificial intelligence to determine a general description of the highly complex, stochastic dynamics of the atmosphere from the best available estimate of the system's historical trajectory as constrained by observations. This is enabled by a novel self-supervised learning objective and a unique ensemble that samples from the stochastic model with a variability informed by the one in the historical record. The task-independent nature of AtmoRep enables skillful results for a divers
    
[^31]: 非欧几里得随机森林

    Hyperbolic Random Forests. (arXiv:2308.13279v1 [cs.LG])

    [http://arxiv.org/abs/2308.13279](http://arxiv.org/abs/2308.13279)

    该论文提出了一种在非欧几里得空间中将随机森林推广的方法，并使用水平球重新定义了分割的概念。为了处理多类数据和不平衡实验，论文还提出了一种新的类组合方法。

    

    非欧几里得空间由于许多现实世界数据集的分层结构（无论是隐式还是显式）而成为表示数据的流行选择。随之而来的是需要能够在非欧几里得空间中解决分类等基本任务的算法。最近，有多篇论文研究了非欧几里得空间中基于超平面的分类器（如逻辑回归和支持向量机）的替代方法。虽然有效，但这些方法在处理更复杂的分层数据时存在困难。因此，我们提出将众所周知的随机森林推广到非欧几里得空间。我们通过使用水平球重新定义了分割的概念来实现这一点。由于找到全局最优分割是计算上难以处理的，我们通过一个大边界分类器找到候选的水平球。为了使非欧几里得随机森林适用于多类数据和不平衡实验，我们还概述了一种基于它们的最低公共祖先和类平衡的类组合方法。

    Hyperbolic space is becoming a popular choice for representing data due to the hierarchical structure - whether implicit or explicit - of many real-world datasets. Along with it comes a need for algorithms capable of solving fundamental tasks, such as classification, in hyperbolic space. Recently, multiple papers have investigated hyperbolic alternatives to hyperplane-based classifiers, such as logistic regression and SVMs. While effective, these approaches struggle with more complex hierarchical data. We, therefore, propose to generalize the well-known random forests to hyperbolic space. We do this by redefining the notion of a split using horospheres. Since finding the globally optimal split is computationally intractable, we find candidate horospheres through a large-margin classifier. To make hyperbolic random forests work on multi-class data and imbalanced experiments, we furthermore outline a new method for combining classes based on their lowest common ancestor and a class-balan
    
[^32]: 将LLMs和Decision Transformers集成到语言驱动的生成质量多样性中

    Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity. (arXiv:2308.13278v1 [cs.LG])

    [http://arxiv.org/abs/2308.13278](http://arxiv.org/abs/2308.13278)

    本文提出了一种将LLMs和Decision Transformers集成到语言驱动的生成质量多样性问题中的方法，通过利用大型语言模型增加具有轨迹的自然语言描述的库，并训练一个依赖于这些描述的策略来解决问题。

    

    质量多样性是一种用于解决强化学习和控制领域问题的随机优化分支，其目的是构建表现良好且在行为空间上具有多样性的政策/技能库。这样的存档通常由有限数量的反应代理组成，每个代理都与唯一的行为描述符相关联，而在粗略离散化的空间之外实例化行为描述符并不直观。虽然最近有一些工作提出了解决这个问题的方法，但生成的轨迹在目标行为描述符规定之外很难进行定制。我们提出在具有静态场景元素的环境中，通过利用大型语言模型增加具有轨迹的自然语言描述的库，并训练一个依赖于这些描述的策略来共同解决这些问题。

    Quality-Diversity is a branch of stochastic optimization that is often applied to problems from the Reinforcement Learning and control domains in order to construct repertoires of well-performing policies/skills that exhibit diversity with respect to a behavior space. Such archives are usually composed of a finite number of reactive agents which are each associated to a unique behavior descriptor, and instantiating behavior descriptors outside of that coarsely discretized space is not straight-forward. While a few recent works suggest solutions to that issue, the trajectory that is generated is not easily customizable beyond the specification of a target behavior descriptor. We propose to jointly solve those problems in environments where semantic information about static scene elements is available by leveraging a Large Language Model to augment the repertoire with natural language descriptions of trajectories, and training a policy conditioned on those descriptions. Thus, our method 
    
[^33]: 异构分布式机器遗忘和种子模型蒸馏

    Heterogeneous Decentralized Machine Unlearning with Seed Model Distillation. (arXiv:2308.13269v1 [cs.LG])

    [http://arxiv.org/abs/2308.13269](http://arxiv.org/abs/2308.13269)

    该论文介绍了一种名为HDUS的分布式遗忘框架，使用种子模型蒸馏构建可擦除的模型集成，适用于异构的设备端模型，具有卓越的性能。

    

    随着一些最近的信息安全法规赋予用户对任何经过训练的机器学习模型拥有被遗忘的无条件权利，个性化物联网服务提供商必须考虑到遗忘功能。取消学习用户贡献的最直接方法是从初始状态重新训练模型，但在频繁的遗忘请求中，这在高吞吐量应用中是不现实的。尽管提出了一些机器遗忘框架来加速重新训练过程，但它们无法适应分布式学习场景。本文中，我们设计了一个名为HDUS的分布式遗忘框架，它使用蒸馏的种子模型为所有客户端构建可擦除的集成模型。此外，该框架与异构的设备端模型兼容，具有更强的可伸缩性，适用于真实世界的应用。在三个真实数据集上的大量实验表明，我们的HDUS实现了最先进的性能。

    As some recent information security legislation endowed users with unconditional rights to be forgotten by any trained machine learning model, personalized IoT service providers have to put unlearning functionality into their consideration. The most straightforward method to unlearn users' contribution is to retrain the model from the initial state, which is not realistic in high throughput applications with frequent unlearning requests. Though some machine unlearning frameworks have been proposed to speed up the retraining process, they fail to match decentralized learning scenarios. In this paper, we design a decentralized unlearning framework called HDUS, which uses distilled seed models to construct erasable ensembles for all clients. Moreover, the framework is compatible with heterogeneous on-device models, representing stronger scalability in real-world applications. Extensive experiments on three real-world datasets show that our HDUS achieves state-of-the-art performance.
    
[^34]: 个性化生成网络实现异构联邦学习

    Heterogeneous Federated Learning via Personalized Generative Networks. (arXiv:2308.13265v1 [cs.LG])

    [http://arxiv.org/abs/2308.13265](http://arxiv.org/abs/2308.13265)

    本文通过个性化生成网络实现了异构联邦学习，解决了数据统计异质性的问题，并通过对客户端之间知识传递的方法，提高了全局模型的收敛效果。

    

    联邦学习允许多个客户端构建一个共同的全局机器学习模型，而无需共享数据。然而，联邦学习面临客户端数据的统计异质性的挑战，这降低了性能并减慢了向全局模型的收敛速度。本文提供了理论证明，最小化客户端之间的异质性有助于每个单独客户端的全局模型的收敛。这在客户端之间出现经验概念转变非常重要，而不仅仅是考虑到已被研究过的不平衡类别。因此，我们提出了一种知识传递方法，其中服务器训练客户端特定的生成器。每个生成器为相应的客户端生成样本，以消除与其他客户端模型的冲突。在合成和真实数据上进行的实验证明以及理论研究支持了我们方法的有效性。

    Federated Learning (FL) allows several clients to construct a common global machine-learning model without having to share their data. FL, however, faces the challenge of statistical heterogeneity between the client's data, which degrades performance and slows down the convergence toward the global model. In this paper, we provide theoretical proof that minimizing heterogeneity between clients facilitates the convergence of a global model for every single client. This becomes particularly important under empirical concept shifts among clients, rather than merely considering imbalanced classes, which have been studied until now. Therefore, we propose a method for knowledge transfer between clients where the server trains client-specific generators. Each generator generates samples for the corresponding client to remove the conflict with other clients' models. Experiments conducted on synthetic and real data, along with a theoretical study, support the effectiveness of our method in cons
    
[^35]: 亲吻寻找匹配：高效的低秩置换表示

    Kissing to Find a Match: Efficient Low-Rank Permutation Representation. (arXiv:2308.13252v1 [cs.LG])

    [http://arxiv.org/abs/2308.13252](http://arxiv.org/abs/2308.13252)

    本文提出了一种使用低秩矩阵分解和非线性逼近来解决大型置换矩阵的维度灾难的方法。通过利用"亲吻数"理论，我们可以推断表示置换矩阵所需的最小秩，从而大幅降低计算和内存成本。这种方法可以实现准确的表达，并在大规模问题中取得了显著的内存节省。

    

    置换矩阵在匹配和分配问题中起着关键作用，尤其在计算机视觉和机器人领域。然而，显式表示置换矩阵的内存随问题规模的增大呈二次增长，限制了大规模问题的解决。本文提出通过使用低秩矩阵分解和非线性逼近来解决大型置换矩阵的维度灾难。为此，我们依靠"亲吻数"理论来推断表示给定大小的置换矩阵所需的最小秩，该秩明显小于问题的大小。这导致计算和内存成本的大幅降低，例如，在表示$n=20000$的问题时，仅使用两个小矩阵中的$8.4\times10^5$个元素，而不是使用一个包含$4\times 10^8$个元素的巨大矩阵，内存节省了$3$个数量级。所提出的表示方法可以实现准确的表达。

    Permutation matrices play a key role in matching and assignment problems across the fields, especially in computer vision and robotics. However, memory for explicitly representing permutation matrices grows quadratically with the size of the problem, prohibiting large problem instances. In this work, we propose to tackle the curse of dimensionality of large permutation matrices by approximating them using low-rank matrix factorization, followed by a nonlinearity. To this end, we rely on the Kissing number theory to infer the minimal rank required for representing a permutation matrix of a given size, which is significantly smaller than the problem size. This leads to a drastic reduction in computation and memory costs, e.g., up to $3$ orders of magnitude less memory for a problem of size $n=20000$, represented using $8.4\times10^5$ elements in two small matrices instead of using a single huge matrix with $4\times 10^8$ elements. The proposed representation allows for accurate represent
    
[^36]: 无模型强化学习在推荐系统中的应用: 随机奖励稳定化

    Model-free Reinforcement Learning with Stochastic Reward Stabilization for Recommender Systems. (arXiv:2308.13246v1 [cs.LG])

    [http://arxiv.org/abs/2308.13246](http://arxiv.org/abs/2308.13246)

    本文研究了在推荐系统中应用无模型强化学习的问题。针对推荐系统中随机奖励的特性，我们设计了两种随机奖励稳定化框架，用于更有效地处理随机反馈。我们的实验证明了这些框架的优越性。

    

    最近，基于无模型的强化学习（RL）的推荐系统因其处理部分反馈和长期奖励的能力而受到越来越多的研究关注。然而，大多数现有研究忽略了推荐系统中的一个关键特征：同一用户在不同时间对同一项的反馈是随机的。随机奖励的特性与具有确定性奖励的经典RL场景本质上不同，这使得基于RL的推荐系统更具挑战性。本文首先在一个模拟环境中展示了直接使用随机反馈会导致性能显著下降。为了更有效地处理随机反馈，我们设计了两种随机奖励稳定化框架，用于用监督模型学习到的奖励替代直接的随机反馈。这两个框架都是模型无关的，即它们可以有效地利用各种监督模型。我们证明了所提出的框架的优越性。

    Model-free RL-based recommender systems have recently received increasing research attention due to their capability to handle partial feedback and long-term rewards. However, most existing research has ignored a critical feature in recommender systems: one user's feedback on the same item at different times is random. The stochastic rewards property essentially differs from that in classic RL scenarios with deterministic rewards, which makes RL-based recommender systems much more challenging. In this paper, we first demonstrate in a simulator environment where using direct stochastic feedback results in a significant drop in performance. Then to handle the stochastic feedback more efficiently, we design two stochastic reward stabilization frameworks that replace the direct stochastic feedback with that learned by a supervised model. Both frameworks are model-agnostic, i.e., they can effectively utilize various supervised models. We demonstrate the superiority of the proposed framework
    
[^37]: 优化关于相关性和后期公平性的群组公平Plackett-Luce排序模型

    Optimizing Group-Fair Plackett-Luce Ranking Models for Relevance and Ex-Post Fairness. (arXiv:2308.13242v1 [cs.LG])

    [http://arxiv.org/abs/2308.13242](http://arxiv.org/abs/2308.13242)

    提出了一种优化群组公平Plackett-Luce排序模型的方法，该方法最大化预期相关性并满足表示约束以确保后期公平性。

    

    在学习排名中，仅优化相关性（或预期排名效用）可能对某些类别的项目造成表现性损害。此外，如果相关性分数中存在隐性偏见，则学习排名模型可能无法优化真实的相关性。以前的研究提出了有效的算法来训练随机排名模型，以达到群组预期暴露的公平性（即期望值），但可能无法保证群组后期的表现公平性，即在从随机排序模型中实现排名之后。通常，通过后期处理实现后期公平性，但是以前的工作不训练意识到此后期处理的随机排序模型。在本文中，我们提出了一种新颖的目标函数，仅在满足给定表示约束的排名中最大化预期相关性，以确保后期公平性。基于最近关于后期群组公平排名的有效抽样器的工作，我们提出了一个新颖的目标函数，仅在满足给定表示约束的排名中最大化预期相关性，以确保后期公正性。建立在一个高效的抽样器的基础上，我们提出了一种新的目标函数，它最大化在满足给定的表示约束的排名中的预期相关性，以确保后期的公平性。

    In learning-to-rank (LTR), optimizing only the relevance (or the expected ranking utility) can cause representational harm to certain categories of items. Moreover, if there is implicit bias in the relevance scores, LTR models may fail to optimize for true relevance. Previous works have proposed efficient algorithms to train stochastic ranking models that achieve fairness of exposure to the groups ex-ante (or, in expectation), which may not guarantee representation fairness to the groups ex-post, that is, after realizing a ranking from the stochastic ranking model. Typically, ex-post fairness is achieved by post-processing, but previous work does not train stochastic ranking models that are aware of this post-processing.  In this paper, we propose a novel objective that maximizes expected relevance only over those rankings that satisfy given representation constraints to ensure ex-post fairness. Building upon recent work on an efficient sampler for ex-post group-fair rankings, we propo
    
[^38]: 用于物理信息神经网络的贝叶斯推理

    Bayesian Reasoning for Physics Informed Neural Networks. (arXiv:2308.13222v1 [physics.comp-ph])

    [http://arxiv.org/abs/2308.13222](http://arxiv.org/abs/2308.13222)

    本文提出了一种基于贝叶斯推理的物理信息神经网络方法（PINN）。该方法采用贝叶斯神经网络框架，通过计算证据来优化模型并解决不确定性问题。

    

    本文提出了一种基于贝叶斯公式的物理信息神经网络（PINN）方法。我们采用了MacKay在Neural Computation（1992年）中提出的贝叶斯神经网络框架。通过拉普拉斯近似法，得到后验密度。对于每个模型（拟合），计算所谓的证据。它是一种分类假设的度量。最优解具有最大的证据值。贝叶斯框架使我们能够控制边界对总损失的影响。事实上，贝叶斯算法通过微调损失组件的相对权重。我们解决了热力学、波动和Burger方程。所得结果与精确解基本一致。所有解都提供了在贝叶斯框架内计算的不确定性。

    Physics informed neural network (PINN) approach in Bayesian formulation is presented. We adopt the Bayesian neural network framework formulated by MacKay (Neural Computation 4 (3) (1992) 448). The posterior densities are obtained from Laplace approximation. For each model (fit), the so-called evidence is computed. It is a measure that classifies the hypothesis. The most optimal solution has the maximal value of the evidence. The Bayesian framework allows us to control the impact of the boundary contribution to the total loss. Indeed, the relative weights of loss components are fine-tuned by the Bayesian algorithm. We solve heat, wave, and Burger's equations. The obtained results are in good agreement with the exact solutions. All solutions are provided with the uncertainties computed within the Bayesian framework.
    
[^39]: GEMTrans: 一种用于心血管诊断的通用、基于超声心动图的多级Transformer框架

    GEMTrans: A General, Echocardiography-based, Multi-Level Transformer Framework for Cardiovascular Diagnosis. (arXiv:2308.13217v1 [cs.CV])

    [http://arxiv.org/abs/2308.13217](http://arxiv.org/abs/2308.13217)

    GEMTrans是一种通用的、基于超声心动图的多级Transformer框架，用于心血管诊断。该框架能够提供解释性，并能处理多个心脏视图的echo视频以进行准确的心血管测量或解释任务的预测。

    

    超声心动图（echo）是一种广泛用于各种心血管诊断任务的超声成像技术。由于基于echo的诊断存在观察者间变异性，这是由于echo图像采集的变异性和基于临床经验对echo图像的解释导致的，基于视觉的机器学习（ML）方法越来越受欢迎作为二次验证层。对于这种安全关键应用，任何提出的ML方法都必须具备一定的可解释性和良好的准确性。此外，这种方法必须能够处理来自各种心脏视图的多个echo视频以及它们之间的交互，以适当地对多种心血管测量或解释任务进行预测。现有工作缺乏可解释性，或者在范围上受限于专注于单一心血管任务。为了解决这个问题，我们提出了一种通用的基于超声心动图的多级Transformer（GEMTrans）框架。

    Echocardiography (echo) is an ultrasound imaging modality that is widely used for various cardiovascular diagnosis tasks. Due to inter-observer variability in echo-based diagnosis, which arises from the variability in echo image acquisition and the interpretation of echo images based on clinical experience, vision-based machine learning (ML) methods have gained popularity to act as secondary layers of verification. For such safety-critical applications, it is essential for any proposed ML method to present a level of explainability along with good accuracy. In addition, such methods must be able to process several echo videos obtained from various heart views and the interactions among them to properly produce predictions for a variety of cardiovascular measurements or interpretation tasks. Prior work lacks explainability or is limited in scope by focusing on a single cardiovascular task. To remedy this, we propose a General, Echo-based, Multi-Level Transformer (GEMTrans) framework tha
    
[^40]: 受物理启发的神经图ODE用于长期动力学模拟

    Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation. (arXiv:2308.13212v1 [cs.LG])

    [http://arxiv.org/abs/2308.13212](http://arxiv.org/abs/2308.13212)

    提出一种受物理启发的神经图ODE算法（PINGO），用于模拟和建模多对象物理系统的长期动态，并解决了当前模型泛化能力差的问题。

    

    模拟和建模多对象物理系统的长期动态是一项重要且具有挑战性的任务。目前的研究利用具有等变性质的图神经网络(GNNs)对物理系统进行建模。具体而言，他们将动力学建模为一系列具有固定时间间隔的离散状态，并学习所有相邻状态之间的直接映射。然而，这种直接映射忽略了两个状态之间的连续性。换句话说，我们已经验证了在当前基于GNN的直接映射模型中，两个离散动态状态之间存在无数可能的轨迹。这个问题极大地阻碍了模型的泛化能力，导致长期模拟的性能较差。在本文中，为了更好地通过离散监督信号建模潜在轨迹，我们提出了一个受物理启发的神经图ODE(PINGO)算法。

    Simulating and modeling the long-term dynamics of multi-object physical systems is an essential and challenging task. Current studies model the physical systems utilizing Graph Neural Networks (GNNs) with equivariant properties. Specifically, they model the dynamics as a sequence of discrete states with a fixed time interval and learn a direct mapping for all the two adjacent states. However, this direct mapping overlooks the continuous nature between the two states. Namely, we have verified that there are countless possible trajectories between two discrete dynamic states in current GNN-based direct mapping models. This issue greatly hinders the model generalization ability, leading to poor performance of the long-term simulation. In this paper, to better model the latent trajectory through discrete supervision signals, we propose a Physics-Inspired Neural Graph ODE (PINGO) algorithm. In PINGO, to ensure the uniqueness of the trajectory, we construct a Physics-Inspired Neural ODE fram
    
[^41]: 受物理启发的非键相互作用等变描述符

    Physics-inspired Equivariant Descriptors of Non-bonded Interactions. (arXiv:2308.13208v1 [physics.chem-ph])

    [http://arxiv.org/abs/2308.13208](http://arxiv.org/abs/2308.13208)

    基于物理启发，我们提出了一种受物理启发的非键相互作用等变描述符框架，该框架能够模拟长程物理相互作用，并且能够生成类似非键位势的局部描述符。

    

    大多数应用于原子尺度模拟的现有机器学习方案依赖于对结构几何的局部描述，并且在建模由长程物理相互作用驱动的效应方面困难重重。克服这些限制的努力集中于直接将静电引入，这是最突出的效应，通常依赖于与显式物理模型的功能形式相似的体系结构。包括其他形式的非键相互作用，或者预测除了原子间势能之外的性质，需要进行临时修改。我们提出了一种替代方法，将远程等变（LODE）框架扩展到生成类似任意渐近行为的非键位势的原子环境的局部描述符，从点电荷静电到色散力。我们证明，LODE形式主义可通过广义多极展开直观地解释。

    Most of the existing machine-learning schemes applied to atomic-scale simulations rely on a local description of the geometry of a structure, and struggle to model effects that are driven by long-range physical interactions. Efforts to overcome these limitations have focused on the direct incorporation of electrostatics, which is the most prominent effect, often relying on architectures that mirror the functional form of explicit physical models. Including other forms of non-bonded interactions, or predicting properties other than the interatomic potential, requires ad hoc modifications. We propose an alternative approach that extends the long-distance equivariant (LODE) framework to generate local descriptors of an atomic environment that resemble non-bonded potentials with arbitrary asymptotic behaviors, ranging from point-charge electrostatics to dispersion forces. We show that the LODE formalism is amenable to a direct physical interpretation in terms of a generalized multipole exp
    
[^42]: 结构Cycle GAN用于在结肠中对腺体标记进行虚拟免疫组织化学染色

    Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon. (arXiv:2308.13182v1 [cs.CV])

    [http://arxiv.org/abs/2308.13182](http://arxiv.org/abs/2308.13182)

    本研究提出了一种新的生成模型SC-GAN，用于合成H&E图像和IHC染色之间的转换。该模型利用边缘结构信息和注意力模块，增强了特征定位并保留了上下文信息。

    

    随着数字扫描仪和深度学习的出现，诊断操作可能从显微镜移到桌面上。血红素和伊红染色（H&E）是最常用于疾病分析、诊断和分级的染色方法之一，但病理学家确实需要不同的免疫组织化学（IHC）染色来分析特定的结构或细胞。在单个标本上获得所有这些染色（H&E和不同的IHC）是一项繁琐耗时的任务。因此，虚拟染色已成为一个重要的研究方向。在这里，我们提出了一种新颖的生成模型，结构Cycle GAN（SC-GAN），用于从H&E图像合成IHC染色反之亦然。我们的方法明确地将边缘结构信息（除了颜色数据）纳入到所提出的生成模型的解码器中，并且在生成过程中仅使用注意力模块。这种集成增强了特征定位并在生成过程中保留了上下文信息。

    With the advent of digital scanners and deep learning, diagnostic operations may move from a microscope to a desktop. Hematoxylin and Eosin (H&E) staining is one of the most frequently used stains for disease analysis, diagnosis, and grading, but pathologists do need different immunohistochemical (IHC) stains to analyze specific structures or cells. Obtaining all of these stains (H&E and different IHCs) on a single specimen is a tedious and time-consuming task. Consequently, virtual staining has emerged as an essential research direction. Here, we propose a novel generative model, Structural Cycle-GAN (SC-GAN), for synthesizing IHC stains from H&E images, and vice versa. Our method expressly incorporates structural information in the form of edges (in addition to color data) and employs attention modules exclusively in the decoder of the proposed generator model. This integration enhances feature localization and preserves contextual information during the generation process. In additi
    
[^43]: 使用Adamic-Adar指数算法预测志愿者合作：少即是多

    Using Adamic-Adar Index Algorithm to Predict Volunteer Collaboration: Less is More. (arXiv:2308.13176v1 [cs.SI])

    [http://arxiv.org/abs/2308.13176](http://arxiv.org/abs/2308.13176)

    本研究使用Adamic-Adar指数算法预测了志愿者合作，发现AAI算法在分析图形时表现优于传统的JC、CNC和其他机器学习算法。

    

    社交网络由于参与者之间潜在合作的不确定性而呈现出复杂的图形结构。机器学习算法在多个实际预测任务中具有优秀的性能。然而，对于机器学习算法是否胜过专门设计用于图链接预测的算法，我们还不清楚。为了解决这个问题，本文使用Adamic-Adar指数算法（AAI），杰卡德系数（JC）和共同邻居中心性（CNC）作为图形特定算法的代表，应用于预测志愿者活动期间深圳市的潜在合作，同时还使用了经典的机器学习算法如随机森林、支持向量机和梯度提升来作为单一预测器和集成学习的组成部分。本文介绍了AAI算法在分析图形时优于传统的JC、CNC和其他机器学习算法的表现。

    Social networks exhibit a complex graph-like structure due to the uncertainty surrounding potential collaborations among participants. Machine learning algorithms possess generic outstanding performance in multiple real-world prediction tasks. However, whether machine learning algorithms outperform specific algorithms designed for graph link prediction remains unknown to us. To address this issue, the Adamic-Adar Index (AAI), Jaccard Coefficient (JC) and common neighbour centrality (CNC) as representatives of graph-specific algorithms were applied to predict potential collaborations, utilizing data from volunteer activities during the Covid-19 pandemic in Shenzhen city, along with the classical machine learning algorithms such as random forest, support vector machine, and gradient boosting as single predictors and components of ensemble learning. This paper introduces that the AAI algorithm outperformed the traditional JC and CNC, and other machine learning algorithms in analyzing grap
    
[^44]: IOMatch:简化开放集半监督学习，同时利用内点和外点

    IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization. (arXiv:2308.13168v1 [cs.CV])

    [http://arxiv.org/abs/2308.13168](http://arxiv.org/abs/2308.13168)

    IOMatch是一个新颖的开放集半监督学习框架，能够在内点和外点难以区分的情况下共同利用它们。

    

    半监督学习旨在在标签昂贵的情况下利用大量未标记的数据。然而，在许多真实世界的应用中，收集到的未标记数据不可避免地会包含不属于任何已标记类别的未见类别的异常值。为了应对具有挑战性的开放集半监督学习任务，主流方法往往首先检测异常值，然后过滤它们。然而，我们观察到一个令人惊讶的事实，当标签极为稀缺时，这种方法可能导致更严重的性能下降，因为不可靠的异常值检测器可能错误地排除了大量有价值的内点。为了解决这个问题，我们引入了一种新颖的开放集半监督学习框架——IOMatch，它可以在内点和外点难以区分的情况下共同利用它们。具体而言，我们提出了一种多二值分类器，结合标准的闭集分类器，用于生成统一的开放集分类结果。

    Semi-supervised learning (SSL) aims to leverage massive unlabeled data when labels are expensive to obtain. Unfortunately, in many real-world applications, the collected unlabeled data will inevitably contain unseen-class outliers not belonging to any of the labeled classes. To deal with the challenging open-set SSL task, the mainstream methods tend to first detect outliers and then filter them out. However, we observe a surprising fact that such approach could result in more severe performance degradation when labels are extremely scarce, as the unreliable outlier detector may wrongly exclude a considerable portion of valuable inliers. To tackle with this issue, we introduce a novel open-set SSL framework, IOMatch, which can jointly utilize inliers and outliers, even when it is difficult to distinguish exactly between them. Specifically, we propose to employ a multi-binary classifier in combination with the standard closed-set classifier for producing unified open-set classification t
    
[^45]: DAG-ACFL：基于DAG-DLT的异步聚类联邦学习

    DAG-ACFL: Asynchronous Clustered Federated Learning based on DAG-DLT. (arXiv:2308.13158v1 [cs.LG])

    [http://arxiv.org/abs/2308.13158](http://arxiv.org/abs/2308.13158)

    DAG-ACFL是一种基于DAG-DLT的异步聚类联邦学习框架，通过选择相似分布的客户端模型来聚合全局模型，同时采用自适应的tip选择算法降低通信和存储成本。

    

    联邦学习旨在在保护客户数据隐私的同时协同训练全局模型。然而，由于客户数据的非独立同分布，联邦学习面临挑战。聚类联邦学习(CFL)作为一种有前景的解决方案已经出现，但大多数现有的CFL框架都采用同步框架缺乏异步性。提出了一种基于有向无环图分布式账本技术(DAG-DLT)的异步CFL框架SDAGFL，但其完全去中心化导致了高通信和存储成本。我们提出了DAG-ACFL，这是一种基于DAG-DLT的异步聚类FL框架。首先详细介绍了DAG-ACFL的组成部分。然后设计了一种基于模型参数的余弦相似度的tip选择算法，以聚合具有相似分布的客户端的模型。采用自适应tip选择算法，利用变点检测动态确定所选tip的数量。我们评估了DAG-ACFL的性能并与其他方法进行了比较。

    Federated learning (FL) aims to collaboratively train a global model while ensuring client data privacy. However, FL faces challenges from the non-IID data distribution among clients. Clustered FL (CFL) has emerged as a promising solution, but most existing CFL frameworks adopt synchronous frameworks lacking asynchrony. An asynchronous CFL framework called SDAGFL based on directed acyclic graph distributed ledger techniques (DAG-DLT) was proposed, but its complete decentralization leads to high communication and storage costs. We propose DAG-ACFL, an asynchronous clustered FL framework based on directed acyclic graph distributed ledger techniques (DAG-DLT). We first detail the components of DAG-ACFL. A tip selection algorithm based on the cosine similarity of model parameters is then designed to aggregate models from clients with similar distributions. An adaptive tip selection algorithm leveraging change-point detection dynamically determines the number of selected tips. We evaluate t
    
[^46]: 物联网中的联邦学习：从资源限制的角度进行调研

    Federated Learning in IoT: a Survey from a Resource-Constrained Perspective. (arXiv:2308.13157v1 [cs.LG])

    [http://arxiv.org/abs/2308.13157](http://arxiv.org/abs/2308.13157)

    本研究论文调研了在资源受限的物联网环境中实施联邦学习所面临的挑战和解决方案。重点关注了有限的客户端资源、异构客户端数据存在、服务器容量和高通信成本等问题，并评估了各个解决方案的有效性。

    

    物联网生态系统能够利用大量数据进行智能决策。联邦学习（FL）是一种分散式机器学习技术，广泛用于从各种分布式数据源中收集和训练机器学习模型。物联网和联邦学习系统可以相互补充并共同使用。然而，物联网设备资源有限的特性限制了FL在现实世界中的大规模部署。本研究论文从客户端和服务器两个层面，全面调研了在资源受限的物联网环境中实施联邦学习（FL）面临的挑战和解决方案。我们关注有关有限的客户端资源、异构客户端数据存在、服务器容量和高通信成本的解决方案，并评估了它们在不同场景中的有效性。此外，我们根据应用位置（即物联网客户端和FL服务器）对这些解决方案进行了分类。

    The IoT ecosystem is able to leverage vast amounts of data for intelligent decision-making. Federated Learning (FL), a decentralized machine learning technique, is widely used to collect and train machine learning models from a variety of distributed data sources. Both IoT and FL systems can be complementary and used together. However, the resource-constrained nature of IoT devices prevents the widescale deployment FL in the real world. This research paper presents a comprehensive survey of the challenges and solutions associated with implementing Federated Learning (FL) in resource-constrained Internet of Things (IoT) environments, viewed from 2 levels, client and server. We focus on solutions regarding limited client resources, presence of heterogeneous client data, server capacity, and high communication costs, and assess their effectiveness in various scenarios. Furthermore, we categorize the solutions based on the location of their application, i.e., the IoT client, and the FL ser
    
[^47]: 使用带轻量级注意机制的迁移ResNet增强乳腺癌分类

    Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism. (arXiv:2308.13150v1 [eess.IV])

    [http://arxiv.org/abs/2308.13150](http://arxiv.org/abs/2308.13150)

    本文介绍了一种使用ResNet模型和轻量级注意机制框架的图像分类方法，通过优化特征表示、增强分类能力和改善特征可辨别性，在乳腺癌分类任务上显示出卓越性能和潜在应用前景。

    

    深度学习模型通过学习原始像素数据中的复杂特征层次结构，彻底改变了图像分类。本文介绍了一种基于ResNet模型的图像分类方法，并引入了轻量级注意机制框架来提高性能。该框架优化特征表示，增强分类能力，改善特征可辨别性。我们在Breakhis数据集上验证了算法的有效性，在许多方面显示出卓越的性能。我们的方法不仅在传统模型方面表现优越，在当代视觉变换器等最新方法上也显示出优势。在诸如精度、准确度、召回率、F1分数和G-means等指标上取得了显著改进，同时在收敛时间方面表现良好。这些结果增强了算法的性能，巩固了其在实际图像分类任务中的应用前景。

    Deep learning models have revolutionized image classification by learning complex feature hierarchies in raw pixel data. This paper introduces an image classification method based on the ResNet model, and introduces a lightweight attention mechanism framework to improve performance. The framework optimizes feature representation, enhances classification capabilities, and improves feature discriminativeness. We verified the effectiveness of the algorithm on the Breakhis dataset, showing its superior performance in many aspects. Not only in terms of conventional models, our method also shows advantages on state-of-the-art methods such as contemporary visual transformers. Significant improvements have been achieved in metrics such as precision, accuracy, recall, F1-score, and G-means, while also performing well in terms of convergence time. These results strengthen the performance of the algorithm and solidify its application prospects in practical image classification tasks. Keywords: Re
    
[^48]: MatchXML: 高效的文本-标签匹配框架，用于极端多标签文本分类

    MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification. (arXiv:2308.13139v1 [cs.CL])

    [http://arxiv.org/abs/2308.13139](http://arxiv.org/abs/2308.13139)

    MatchXML是一种高效的文本-标签匹配框架，用于极端多标签文本分类。它通过label2vec方法生成语义密集的标签嵌入，并利用这些嵌入构建层次化标签树。通过微调预训练的Transformer模型，MatchXML将多标签文本分类问题转化为文本-标签匹配问题，并提取出密集的文本表示和静态的句子嵌入。

    

    极端多标签文本分类（XMC）是指训练一个分类器，从一个非常大规模的标签集中（例如数百万个标签）为文本样本分配相关标签。我们提出了MatchXML，一种用于XMC的高效文本-标签匹配框架。我们观察到，由稀疏的词频-逆文档频率（TF-IDF）特征生成的标签嵌入存在一些限制。因此，我们提出了label2vec，通过Skip-gram模型来有效训练语义密集的标签嵌入。然后，使用这些密集的标签嵌入来构建一个层次化标签树。在微调预训练的编码器Transformer时，我们将多标签文本分类问题制定为一个在二分图中的文本-标签匹配问题。然后，从微调后的Transformer中提取密集的文本表示。除了微调后的密集文本嵌入之外，我们还从预训练的Sentence Transformer中提取静态的密集句子嵌入。

    The eXtreme Multi-label text Classification(XMC) refers to training a classifier that assigns a text sample with relevant labels from an extremely large-scale label set (e.g., millions of labels). We propose MatchXML, an efficient text-label matching framework for XMC. We observe that the label embeddings generated from the sparse Term Frequency-Inverse Document Frequency(TF-IDF) features have several limitations. We thus propose label2vec to effectively train the semantic dense label embeddings by the Skip-gram model. The dense label embeddings are then used to build a Hierarchical Label Tree by clustering. In fine-tuning the pre-trained encoder Transformer, we formulate the multi-label text classification as a text-label matching problem in a bipartite graph. We then extract the dense text representations from the fine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also extract the static dense sentence embeddings from a pre-trained Sentence Transformer. Finally,
    
[^49]: OmniQuant：用于大型语言模型的全向校准量化

    OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])

    [http://arxiv.org/abs/2308.13137](http://arxiv.org/abs/2308.13137)

    OmniQuant是一种用于大型语言模型的全向校准量化技术，通过优化各种量化参数实现了良好的性能，并保持了计算效率。

    

    大型语言模型（LLM）已经在自然语言处理任务中带来了革命性的变化。然而，它们的实际部署受到了其庞大的内存和计算需求的限制。虽然最近的后训练量化（PTQ）方法在减少内存占用和提高LLM的计算效率方面非常有效，但它们手工制定量化参数，导致性能较低并且不能处理极低位量化。为了解决这个问题，我们介绍了一种全向校准量化（OmniQuant）技术，用于LLMs，它在多种量化设置下实现了良好的性能，并通过高效优化各种量化参数来保持PTQ的计算效率。OmniQuant包含两个创新组件，包括可学习的权重剪裁（LWC）和可学习的等效变换（LET）。LWC通过优化剪裁阈值来调节权重的极值。与此同时，LET处理激活函数。

    Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activa
    
[^50]: 非参数可加值函数：具有可解释性的强化学习方法及其在外科手术恢复中的应用

    Nonparametric Additive Value Functions: Interpretable Reinforcement Learning with an Application to Surgical Recovery. (arXiv:2308.13135v1 [stat.ML])

    [http://arxiv.org/abs/2308.13135](http://arxiv.org/abs/2308.13135)

    该论文提出了一种非参数可加模型，用于估计可解释的值函数，并在强化学习中具有应用价值。该方法能够克服传统模型的线性假设限制，同时提供较强的决策建议解释性。

    

    我们提出了一种非参数可加模型，用于在强化学习中估计可解释的值函数。学习依靠数字表型特征的有效自适应临床干预是医务人员重视的问题。在脊柱手术方面，关于患者运动能力恢复的不同术后恢复建议可能会导致患者恢复程度的显著变化。虽然强化学习在游戏等领域取得了广泛成功，但最近的方法严重依赖于黑盒方法，如神经网络。不幸的是，这些方法阻碍了考察每个特征对于产生最终建议决策的贡献。虽然在经典算法（如最小二乘策略迭代）中可以轻松提供这样的解释，但基本的线性假设阻止了学习特征之间的高阶灵活交互作用。在本文中，我们提出了一种新颖的方法，提供了一种灵活的技术来克服这些限制，并能够得到解释性强的决策建议模型。

    We propose a nonparametric additive model for estimating interpretable value functions in reinforcement learning. Learning effective adaptive clinical interventions that rely on digital phenotyping features is a major for concern medical practitioners. With respect to spine surgery, different post-operative recovery recommendations concerning patient mobilization can lead to significant variation in patient recovery. While reinforcement learning has achieved widespread success in domains such as games, recent methods heavily rely on black-box methods, such neural networks. Unfortunately, these methods hinder the ability of examining the contribution each feature makes in producing the final suggested decision. While such interpretations are easily provided in classical algorithms such as Least Squares Policy Iteration, basic linearity assumptions prevent learning higher-order flexible interactions between features. In this paper, we present a novel method that offers a flexible techniq
    
[^51]: 面向库存管理的业务指标感知预测

    Business Metric-Aware Forecasting for Inventory Management. (arXiv:2308.13118v1 [cs.LG])

    [http://arxiv.org/abs/2308.13118](http://arxiv.org/abs/2308.13118)

    本研究针对库存管理设置，通过端到端优化预测指标，相较于优化传统标准的与业务无关的指标，能够显著提升业务绩效。

    

    时间序列预测在业务规划中起着至关重要的作用。然而，预测者通常优化与业务目标无关的目标，从而可能产生与业务偏好不一致的预测结果。在这项工作中，我们证明传统预测指标的优化通常会导致次优的业务绩效。针对库存管理的情境，我们在一个端到端可微分的方式下推导出了计算和优化常见业务指标代理的高效过程。我们探索了各种可能的成本权衡情境，并通过实证研究表明，端到端优化通常优于优化标准的与业务无关的预测指标（对于简单的缩放模型，提升高达45.7%，对于LSTM编码器-解码器模型，提升高达54.0%）。最后，我们讨论了我们的研究结果如何在其他业务环境中受益。

    Time-series forecasts play a critical role in business planning. However, forecasters typically optimize objectives that are agnostic to downstream business goals and thus can produce forecasts misaligned with business preferences. In this work, we demonstrate that optimization of conventional forecasting metrics can often lead to sub-optimal downstream business performance. Focusing on the inventory management setting, we derive an efficient procedure for computing and optimizing proxies of common downstream business metrics in an end-to-end differentiable manner. We explore a wide range of plausible cost trade-off scenarios, and empirically demonstrate that end-to-end optimization often outperforms optimization of standard business-agnostic forecasting metrics (by up to 45.7% for a simple scaling model, and up to 54.0% for an LSTM encoder-decoder model). Finally, we discuss how our findings could benefit other business contexts.
    
[^52]: 基于贝叶斯低秩适应的大型语言模型

    Bayesian low-rank adaptation for large language models. (arXiv:2308.13111v1 [cs.LG])

    [http://arxiv.org/abs/2308.13111](http://arxiv.org/abs/2308.13111)

    本研究提出了一种名为Laplace-LoRA的贝叶斯方法，通过应用拉普拉斯近似来增强经过微调的大型语言模型的校准能力。

    

    参数高效的微调（PEFT）已成为大型语言模型（LLMs）成本高效微调的新范式，其中低秩适应（LoRA）被广泛采用。然而，经过微调的LLMs往往变得过于自信，尤其是在较小数据集上进行微调时。贝叶斯方法具有估计不确定性的固有能力，可作为减轻过度自信并增强校准能力的有力工具。在这项工作中，我们引入了Laplace-LoRA，一种直观而有效的贝叶斯方法，它将拉普拉斯近似应用于LoRA参数，并显著提升了经过微调的LLMs的校准能力。

    Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially on when fine-tuned on smaller datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, a straightforward yet effective Bayesian method, which applies the Laplace approximation to the LoRA parameters and, considerably boosts the calibration of fine-tuned LLMs.
    
[^53]: 电子健康记录中的生存分析的对比学习：构建时序区别度

    Contrastive Learning of Temporal Distinctiveness for Survival Analysis in Electronic Health Records. (arXiv:2308.13104v1 [cs.LG])

    [http://arxiv.org/abs/2308.13104](http://arxiv.org/abs/2308.13104)

    本文提出了一种新的基于对比学习的生存分析框架，充分利用截断和观察数据的生存时长来定义时序区别度，构建负样本对。

    

    生存分析在许多医疗决策中起着至关重要的作用，可以支持对患者医疗过程中感兴趣事件的风险预测。鉴于数据截断的存在，一种有效的生存分析方法是强制保持截断和观察数据之间的时序一致性，旨在利用截断前的时间间隔作为部分观测的事件发生时间标签用于监督学习。尽管现有研究大多采用排序方法来追求排序目标，但尚未对对比方法进行深入探索，这些方法通过数据之间的对比来学习有区别性的嵌入。因此，在本文中，我们提出了一种新颖的基于本体感知的时序对比生存分析（OTCSurv）框架，利用截断和观察数据的生存时长定义时序区别度，并构建负样本对。

    Survival analysis plays a crucial role in many healthcare decisions, where the risk prediction for the events of interest can support an informative outlook for a patient's medical journey. Given the existence of data censoring, an effective way of survival analysis is to enforce the pairwise temporal concordance between censored and observed data, aiming to utilize the time interval before censoring as partially observed time-to-event labels for supervised learning. Although existing studies mostly employed ranking methods to pursue an ordering objective, contrastive methods which learn a discriminative embedding by having data contrast against each other, have not been explored thoroughly for survival analysis. Therefore, in this paper, we propose a novel Ontology-aware Temporality-based Contrastive Survival (OTCSurv) analysis framework that utilizes survival durations from both censored and observed data to define temporal distinctiveness and construct negative sample pairs with adj
    
[^54]: 对一辆自主Formula SAE赛车的基于强化学习的控制进行竞赛

    Racing Towards Reinforcement Learning based control of an Autonomous Formula SAE Car. (arXiv:2308.13088v1 [cs.RO])

    [http://arxiv.org/abs/2308.13088](http://arxiv.org/abs/2308.13088)

    本文介绍了使用深度强化学习控制自主Formula SAE赛车的研究。实验结果表明，这种方法可以在模拟环境中成功学习赛车驾驶，并成功应用于真实赛道上的物理平台。该研究提供对方法的局限性和未来研究方向的指导。

    

    随着自主导航研究的日益流行，Formula Student（FS）赛事在其比赛列表中引入了自动驾驶车辆（DV）类别。本文介绍了对这些比赛中的自主FS赛车利用深度强化学习（RL）进行端到端控制的初步研究。我们在类似于实际设计的赛道上，通过在Turtlebot2平台上进行模拟进行训练，使用两种最先进的RL算法。结果表明，我们的方法可以成功地在模拟环境中学习赛车驾驶，并将其转移到真实世界的赛道上的物理平台。最后，我们对所提出方法的限制进行了探讨，并提出了将强化学习应用于全尺度自主FS赛车的未来方向的指导。

    With the rising popularity of autonomous navigation research, Formula Student (FS) events are introducing a Driverless Vehicle (DV) category to their event list. This paper presents the initial investigation into utilising Deep Reinforcement Learning (RL) for end-to-end control of an autonomous FS race car for these competitions. We train two state-of-the-art RL algorithms in simulation on tracks analogous to the full-scale design on a Turtlebot2 platform. The results demonstrate that our approach can successfully learn to race in simulation and then transfer to a real-world racetrack on the physical platform. Finally, we provide insights into the limitations of the presented approach and guidance into the future directions for applying RL toward full-scale autonomous FS racing.
    
[^55]: SHIELD: 可持续混合进化学习框架用于碳，废水和能源感知的数据中心管理

    SHIELD: Sustainable Hybrid Evolutionary Learning Framework for Carbon, Wastewater, and Energy-Aware Data Center Management. (arXiv:2308.13086v1 [cs.DC])

    [http://arxiv.org/abs/2308.13086](http://arxiv.org/abs/2308.13086)

    这项研究提出了一种名为SHIELD的混合学习框架，用于数据中心管理，旨在优化碳排放、水足迹和能源成本。通过整合机器学习引导的局部搜索和分解式进化算法，该框架可以智能地管理工作负载分配，实现较高的效率和速度。

    

    当今的云数据中心通常分布在地理上，以提供强大的数据服务。但是，这些地理分布的数据中心（GDDC）由于不断增加的碳排放和水使用量而对环境产生重大影响，需要加以控制。此外，操作这些数据中心的能源成本不断上升。本文提出了一种新颖的框架，使用称为SHIELD的混合工作负载管理框架，通过将机器学习引导的局部搜索与基于分解的进化算法相结合，来协同优化GDDC的碳排放量，水足迹和能源成本。我们的框架考虑了地理因素和时间差异对功率生成/使用成本和环境影响的影响，以智能地管理GDDC和数据中心的工作负载分配。实验结果表明，SHIELD可以实现34.4倍的加速和2.1倍的帕累托超体积改进，在减少碳足迹的同时，实现了速度和效率的提升。

    Today's cloud data centers are often distributed geographically to provide robust data services. But these geo-distributed data centers (GDDCs) have a significant associated environmental impact due to their increasing carbon emissions and water usage, which needs to be curtailed. Moreover, the energy costs of operating these data centers continue to rise. This paper proposes a novel framework to co-optimize carbon emissions, water footprint, and energy costs of GDDCs, using a hybrid workload management framework called SHIELD that integrates machine learning guided local search with a decomposition-based evolutionary algorithm. Our framework considers geographical factors and time-based differences in power generation/use, costs, and environmental impacts to intelligently manage workload distribution across GDDCs and data center operation. Experimental results show that SHIELD can realize 34.4x speedup and 2.1x improvement in Pareto Hypervolume while reducing the carbon footprint by u
    
[^56]: 多元时间序列异常检测: 炫酷算法和有缺陷的评估方法

    Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology. (arXiv:2308.13068v1 [cs.LG])

    [http://arxiv.org/abs/2308.13068](http://arxiv.org/abs/2308.13068)

    多元时间序列异常检测是一个研究领域，但目前存在组织不够有序和评估协议有缺陷的问题。文章评估了许多最近算法的性能，并指出了针对多元时间序列异常检测的评估协议存在的问题及如何缓解这些问题的方法。

    

    多元时间序列（MVTS）的异常检测是一个长期存在且具有挑战性的研究课题，近年来吸引了工业界和学术界的大量研究努力。然而，对文献的仔细研究让我们意识到：1）该领域的社区活跃，但并不像计算机视觉（CV）和自然语言处理（NLP）等其他机器学习领域那样组织有序；2）大多数提出的解决方案使用不合适或存在明显缺陷的评估协议进行评估，缺乏科学基础。其中一个非常流行的协议，即所谓的 \pa 协议，是如此有缺陷，以至于随机猜测可以显示系统地优于迄今为止开发的\emph{所有}算法。在本文中，我们使用更健壮的协议对许多最近的算法进行回顾和评估，并讨论在MVTS异常检测的背景下，一个本来很好的协议可能存在的问题以及如何减轻这些问题。我们还对基准数据集表达了关切。

    Multivariate Time Series (MVTS) anomaly detection is a long-standing and challenging research topic that has attracted tremendous research effort from both industry and academia recently. However, a careful study of the literature makes us realize that 1) the community is active but not as organized as other sibling machine learning communities such as Computer Vision (CV) and Natural Language Processing (NLP), and 2) most proposed solutions are evaluated using either inappropriate or highly flawed protocols, with an apparent lack of scientific foundation. So flawed is one very popular protocol, the so-called \pa protocol, that a random guess can be shown to systematically outperform \emph{all} algorithms developed so far. In this paper, we review and evaluate many recent algorithms using more robust protocols and discuss how a normally good protocol may have weaknesses in the context of MVTS anomaly detection and how to mitigate them. We also share our concerns about benchmark dataset
    
[^57]: 通过多阶段VAE对分子属性进行客观无关的改进

    Objective-Agnostic Enhancement of Molecule Properties via Multi-Stage VAE. (arXiv:2308.13066v1 [cs.LG])

    [http://arxiv.org/abs/2308.13066](http://arxiv.org/abs/2308.13066)

    本文提出了一种多阶段VAE方法，可以改善在药物发现领域中恢复低维流形的问题。实验结果表明，该方法显著改善了生成分子的属性统计，而无需集成属性预测器。

    

    变分自编码器（VAE）是药物发现的一种流行方法，已经提出了各种架构和流程来改善其性能。然而，已知VAE方法在数据位于高维环境空间中嵌入的低维流形上时，很难恢复流形。在药物发现领域，这一问题尚未得到充分探索。本文中，我们探索了一种可以改善合成数据集上的流形恢复的多阶段VAE方法，并将其应用到药物发现领域。我们使用ChEMBL数据集对我们的多阶段VAE方法进行了实验评估，并展示了该方法在不将属性预测器纳入训练流程的前提下，显著改善了所生成分子的属性统计。我们进一步在两个精心策划且较小的分子数据集上对模型进行了微调，这两个数据集针对不同的蛋白质。实验结果显示了属性的增加。

    Variational autoencoder (VAE) is a popular method for drug discovery and various architectures and pipelines have been proposed to improve its performance. However, VAE approaches are known to suffer from poor manifold recovery when the data lie on a low-dimensional manifold embedded in a higher dimensional ambient space [Dai and Wipf, 2019]. The consequences of it in drug discovery are somewhat under-explored. In this paper, we explore applying a multi-stage VAE approach, that can improve manifold recovery on a synthetic dataset, to the field of drug discovery. We experimentally evaluate our multi-stage VAE approach using the ChEMBL dataset and demonstrate its ability to improve the property statistics of generated molecules substantially from pre-existing methods without incorporating property predictors into the training pipeline. We further fine-tune our models on two curated and much smaller molecule datasets that target different proteins. Our experiments show an increase in the 
    
[^58]: ZeroLeak: 使用LLMs进行可扩展和成本有效的侧信道补丁

    ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching. (arXiv:2308.13062v1 [cs.CR])

    [http://arxiv.org/abs/2308.13062](http://arxiv.org/abs/2308.13062)

    本研究在安全关键的软件中探索了使用LLMs生成具有侧信道泄漏的脆弱代码的修补程序。通过零-shot学习方法和动态分析，我们成功地生成了具有泄漏鲁棒性的候选替换。

    

    安全关键的软件，如OpenSSL，由于缺乏资源或专家，存在许多未修补的侧信道泄漏。随着代码开发速度的加快，开发人员依赖于大型语言模型(LLMs)自动生成代码，情况只会变得更糟。在这项工作中，我们探索了使用LLMs生成具有微体系结构侧信道泄漏的脆弱代码的修补程序。为此，我们通过精心设计的提示来探索强大的LLMs的生成能力，采用了零-shot学习方法。所有生成的代码都通过泄漏检测工具进行动态分析，这些工具能够准确指示从秘密相关访问或分支或脆弱的Spectre gadget泄漏的指令级的信息泄漏。精心设计的提示被用来生成脆弱代码的候选替换，然后对其进行正确性和泄漏鲁棒性的分析。

    Security critical software, e.g., OpenSSL, comes with numerous side-channel leakages left unpatched due to a lack of resources or experts. The situation will only worsen as the pace of code development accelerates, with developers relying on Large Language Models (LLMs) to automatically generate code. In this work, we explore the use of LLMs in generating patches for vulnerable code with microarchitectural side-channel leakages. For this, we investigate the generative abilities of powerful LLMs by carefully crafting prompts following a zero-shot learning approach. All generated code is dynamically analyzed by leakage detection tools, which are capable of pinpointing information leakage at the instruction level leaked either from secret dependent accesses or branches or vulnerable Spectre gadgets, respectively. Carefully crafted prompts are used to generate candidate replacements for vulnerable code, which are then analyzed for correctness and for leakage resilience. From a cost/perform
    
[^59]: 贝叶斯探索网络

    Bayesian Exploration Networks. (arXiv:2308.13049v1 [cs.LG])

    [http://arxiv.org/abs/2308.13049](http://arxiv.org/abs/2308.13049)

    这篇论文提出了一种贝叶斯探索网络的方法，通过在一维Bellman算子中建模不确定性，解决了贝叶斯强化学习中学习贝叶斯最优策略的计算复杂性的挑战。

    

    贝叶斯强化学习为不确定性下的顺序决策提供了一种原则性和优雅的方法。最显著的是，贝叶斯代理不会面临频率方法的探索/开发困境，这是一个重大的问题。贝叶斯强化学习的一个关键挑战是学习贝叶斯最优策略的计算复杂性，这在玩具领域中是可计算的。在本文中，我们提出了一种新颖的无模型方法来解决这一挑战。与基于模型的方法不同，我们在一维Bellman算子中建模不确定性而不是在高维状态转移分布中建模。我们的理论分析揭示了现有的无模型方法要么不通过MDP传播认知不确定性，要么在一组语境策略中优化而不是所有历史条件策略。这两个近似得到的策略可能是任意贝叶斯次优的。为了克服这些问题，我们引入了贝叶斯探索网络（Bayesian exploration network）。

    Bayesian reinforcement learning (RL) offers a principled and elegant approach for sequential decision making under uncertainty. Most notably, Bayesian agents do not face an exploration/exploitation dilemma, a major pathology of frequentist methods. A key challenge for Bayesian RL is the computational complexity of learning Bayes-optimal policies, which is only tractable in toy domains. In this paper we propose a novel model-free approach to address this challenge. Rather than modelling uncertainty in high-dimensional state transition distributions as model-based approaches do, we model uncertainty in a one-dimensional Bellman operator. Our theoretical analysis reveals that existing model-free approaches either do not propagate epistemic uncertainty through the MDP or optimise over a set of contextual policies instead of all history-conditioned policies. Both approximations yield policies that can be arbitrarily Bayes-suboptimal. To overcome these issues, we introduce the Bayesian explo
    
[^60]: 不完整观测数据的联邦因果效应学习

    Federated Learning of Causal Effects from Incomplete Observational Data. (arXiv:2308.13047v1 [cs.LG])

    [http://arxiv.org/abs/2308.13047](http://arxiv.org/abs/2308.13047)

    我们提出了一种联邦学习的方法，可以从多个分布式和不完整的数据源中进行因果推断，估计因果效应并解决因为缺失值引入的偏差问题。

    

    分布式和不完整的数据源在实际应用中很常见，对因果推断提出了巨大挑战。由于隐私限制，这些数据源无法合并为一个实体，而其中的缺失值可能会引入偏差到因果估计中。我们提出了一种新的方法，可以从多个分布式和不完整的数据源中进行联邦因果推断，从而估计因果效应。我们的方法将损失函数拆分为多个组件，每个组件对应于具有缺失值的特定数据源。我们的方法在缺失随机假设下考虑了缺失数据，并估计了因果估计的高阶统计量。我们的方法从分散的数据源中恢复观察到的混淆变量的条件分布，以识别因果效应。我们的框架估计了异质的条件分布以应对不完整的数据源。

    Decentralized and incomplete data sources are prevalent in real-world applications, posing a formidable challenge for causal inference. These sources cannot be consolidated into a single entity owing to privacy constraints, and the presence of missing values within them can potentially introduce bias to the causal estimands. We introduce a new approach for federated causal inference from incomplete data, enabling the estimation of causal effects from multiple decentralized and incomplete data sources. Our approach disentangles the loss function into multiple components, each corresponding to a specific data source with missing values. Our approach accounts for the missing data under the missing at random assumption, while also estimating higher-order statistics of the causal estimands. Our method recovers the conditional distribution of missing confounders given the observed confounders from the decentralized data sources to identify causal effects. Our framework estimates heterogeneou
    
[^61]: 视频胶囊内镜和人工智能的交叉领域：利用机器学习解决独特挑战

    The intersection of video capsule endoscopy and artificial intelligence: addressing unique challenges using machine learning. (arXiv:2308.13035v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.13035](http://arxiv.org/abs/2308.13035)

    视频胶囊内镜和人工智能的交叉领域面临着技术负担、解释成本高昂、数据不平衡等挑战。研究使用机器学习方法解决了这些挑战，包括使用卷积神经网络对VCE数据分类以及创建辅助专家的注释工具。

    

    引言：视频胶囊内镜（VCE）的技术负担和耗时的审查过程限制了其实用性。人工智能（AI）有望解决这些限制，但AI与VCE的交叉领域揭示了必须先克服的挑战。我们确定了要解决的五个挑战。挑战1：VCE数据是随机的且包含显著伪迹。挑战2：VCE解释成本高昂。挑战3：VCE数据本质上是不平衡的。挑战4：现有的VCE AIMLT计算上繁琐。挑战5：临床医生抗拒不能解释过程的AIMLT。方法：采用解剖地标检测模型，测试了卷积神经网络（CNN）在对VCE数据进行分类任务中的应用。我们还创建了一种辅助专家对VCE数据进行注释的工具。然后，我们使用不同的方法创建了更复杂的模型，包括多帧方法、基于图的CNN模型等。

    Introduction: Technical burdens and time-intensive review processes limit the practical utility of video capsule endoscopy (VCE). Artificial intelligence (AI) is poised to address these limitations, but the intersection of AI and VCE reveals challenges that must first be overcome. We identified five challenges to address. Challenge #1: VCE data are stochastic and contains significant artifact. Challenge #2: VCE interpretation is cost-intensive. Challenge #3: VCE data are inherently imbalanced. Challenge #4: Existing VCE AIMLT are computationally cumbersome. Challenge #5: Clinicians are hesitant to accept AIMLT that cannot explain their process.  Methods: An anatomic landmark detection model was used to test the application of convolutional neural networks (CNNs) to the task of classifying VCE data. We also created a tool that assists in expert annotation of VCE data. We then created more elaborate models using different approaches including a multi-frame approach, a CNN based on graph 
    
[^62]: 使用精细调整的Llama 2 GPT模型进行金融新闻分析

    Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])

    [http://arxiv.org/abs/2308.13032](http://arxiv.org/abs/2308.13032)

    本研究通过精细调整的Llama 2模型实现了金融新闻的多任务分析，包括文本分析、摘要和情感提取等。实验结果显示，提取的命名实体情感可以作为有监督机器学习模型的预测特征。

    

    本文考虑了使用精细调整的Llama 2 Large Language Model (LLM) 对金融新闻进行多任务分析的可能性。通过PEFT/LoRA方法对模型进行精细调整，主要包括从金融市场角度分析文本、突出文本的主要观点、对文本进行摘要和提取具有适当情感的命名实体等任务。实验结果表明，经过精细调整的Llama 2模型能够进行多任务的金融新闻分析，其响应的结构可以部分为结构化文本，另一部分数据可以采用JSON格式进一步处理。提取的命名实体情感可以被视为具有定量目标变量的监督机器学习模型的预测特征。

    The paper considers the possibility to fine-tune Llama 2 Large Language Model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.
    
[^63]: 使用通用绝热量子计算训练神经网络

    Training Neural Networks with Universal Adiabatic Quantum Computing. (arXiv:2308.13028v1 [quant-ph])

    [http://arxiv.org/abs/2308.13028](http://arxiv.org/abs/2308.13028)

    该论文提出了一种使用通用绝热量子计算来训练神经网络的新方法，通过利用绝热演化原理，该方法可以高效地找到损失函数的全局最小值，为经典训练方法提供了有希望的替代方案。

    

    神经网络（NNs）的训练是一个计算密集型的任务，需要大量的时间和资源。本文提出了一种使用绝热量子计算（AQC）来训练NN的新方法，AQC 是利用绝热演化原理来解决优化问题的一种范式。我们提出了一种可以在门量子计算机上实现的通用 AQC 方法，可以应用于广泛的哈密顿量，从而实现对具有表达力的神经网络进行训练。我们将这种方法应用于具有连续、离散和二进制权重的各种神经网络。我们的结果表明，AQC可以非常高效地找到损失函数的全局最小值，为经典训练方法提供了有希望的替代方案。

    The training of neural networks (NNs) is a computationally intensive task requiring significant time and resources. This paper presents a novel approach to NN training using Adiabatic Quantum Computing (AQC), a paradigm that leverages the principles of adiabatic evolution to solve optimisation problems. We propose a universal AQC method that can be implemented on gate quantum computers, allowing for a broad range of Hamiltonians and thus enabling the training of expressive neural networks. We apply this approach to various neural networks with continuous, discrete, and binary weights. Our results indicate that AQC can very efficiently find the global minimum of the loss function, offering a promising alternative to classical training methods.
    
[^64]: 使用极值理论在强化学习中的极端风险缓解

    Extreme Risk Mitigation in Reinforcement Learning using Extreme Value Theory. (arXiv:2308.13011v1 [cs.LG])

    [http://arxiv.org/abs/2308.13011](http://arxiv.org/abs/2308.13011)

    本研究提出了一种使用极值理论在强化学习中缓解极端风险的方法，通过改进状态-动作值函数分布预测的极值来增强RL智能体在面对非常罕见和危险事件时的韧性。

    

    近年来，由于在现实世界中部署强化学习智能体的兴趣增加，风险敏感的强化学习 (RL) 引起了广泛关注。风险意识的一个关键方面是对可能导致灾难性结果的高度罕见的风险事件 (奖励) 进行建模。这些罕见事件对于旨在准确捕捉此类风险事件的数据驱动方法来说是一个巨大的挑战。尽管存在风险感知的 RL 技术，当建模这些罕见事件时，它们的风险规避水平严重依赖于状态-动作值函数估计的精度。我们的工作提出通过专注于改进状态-动作值函数分布预测的极值来增强 RL 智能体在面对非常罕见和危险事件时的韧性。为了实现这一目标，我们将状态-动作值函数分布的极值表示为参数化分布，并从中获得灵感

    Risk-sensitive reinforcement learning (RL) has garnered significant attention in recent years due to the growing interest in deploying RL agents in real-world scenarios. A critical aspect of risk awareness involves modeling highly rare risk events (rewards) that could potentially lead to catastrophic outcomes. These infrequent occurrences present a formidable challenge for data-driven methods aiming to capture such risky events accurately. While risk-aware RL techniques do exist, their level of risk aversion heavily relies on the precision of the state-action value function estimation when modeling these rare occurrences. Our work proposes to enhance the resilience of RL agents when faced with very rare and risky events by focusing on refining the predictions of the extreme values predicted by the state-action value function distribution. To achieve this, we formulate the extreme values of the state-action value function distribution as parameterized distributions, drawing inspiration 
    
[^65]: 设计优化和基于深度学习的逆向设计性能比较

    Performance Comparison of Design Optimization and Deep Learning-based Inverse Design. (arXiv:2308.13000v1 [math.OC])

    [http://arxiv.org/abs/2308.13000](http://arxiv.org/abs/2308.13000)

    本文对比了传统设计优化方法与基于深度学习的逆向设计方法的性能，通过采用各种情景的基准问题来进行比较。根据研究结果，证明了...

    

    基于代理模型的优化方法在工程设计领域越来越广泛应用。它涉及使用仿真或实际实验数据创建基于目标函数或约束条件的代理模型，然后使用数值优化方法从模型中找到最优解。近年来，基于深度学习的逆向设计方法的进展使得实时生成工程设计问题的最优解成为可能，从而消除了迭代优化过程的要求。然而，迄今为止，还没有一项全面的研究仔细比较了这种新方法与传统设计优化方法之间的具体优缺点。本文的目的是通过采用各种情景的基准问题来比较传统设计优化方法和基于深度学习的逆向设计方法的性能。根据本研究的发现，我们证明了...

    Surrogate model-based optimization has been increasingly used in the field of engineering design. It involves creating a surrogate model with objective functions or constraints based on the data obtained from simulations or real-world experiments, and then finding the optimal solution from the model using numerical optimization methods. Recent advancements in deep learning-based inverse design methods have made it possible to generate real-time optimal solutions for engineering design problems, eliminating the requirement for iterative optimization processes. Nevertheless, no comprehensive study has yet closely examined the specific advantages and disadvantages of this novel approach compared to the traditional design optimization method. The objective of this paper is to compare the performance of traditional design optimization methods with deep learning-based inverse design methods by employing benchmark problems across various scenarios. Based on the findings of this study, we prov
    
[^66]: 使用强化学习的无人机顶部机械臂执行器轨迹规划

    Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning. (arXiv:2308.12843v1 [cs.RO])

    [http://arxiv.org/abs/2308.12843](http://arxiv.org/abs/2308.12843)

    本文研究了使用强化学习控制无人机顶部机械臂执行器轨迹的方法，并提出了基于时间到碰撞的运动规划模型以绕过障碍物。同时，利用基于模型的Q-learning模型独立追踪和控制机械臂末端执行器的期望轨迹。通过这种方法可以实现一系列在高难度和危险环境中的执行任务。

    

    本文研究了一种空中机械臂系统，即装备有可控制的二自由度臂的无人机，以实现即时执行任务。我们的解决方案基于使用Q-learning方法来控制臂尖端（即末端执行器）的轨迹。我们开发了一个基于时间到碰撞（TTC）的运动规划模型，使四旋翼无人机能够在保证机械臂可达性的同时绕过障碍物航行。此外，我们利用基于模型的Q-learning模型独立追踪和控制机械臂末端执行器的期望轨迹，给定无人机平台的任意基准轨迹。这种组合使得可以执行各种执行任务，如高空焊接、结构监测和修复、电池更换、排水沟清理、摩天大楼清洁和电力线路维护在难以到达和危险的环境中。

    In this paper, we investigate the operation of an aerial manipulator system, namely an Unmanned Aerial Vehicle (UAV) equipped with a controllable arm with two degrees of freedom to carry out actuation tasks on the fly. Our solution is based on employing a Q-learning method to control the trajectory of the tip of the arm, also called \textit{end-effector}. More specifically, we develop a motion planning model based on Time To Collision (TTC), which enables a quadrotor UAV to navigate around obstacles while ensuring the manipulator's reachability. Additionally, we utilize a model-based Q-learning model to independently track and control the desired trajectory of the manipulator's end-effector, given an arbitrary baseline trajectory for the UAV platform. Such a combination enables a variety of actuation tasks such as high-altitude welding, structural monitoring and repair, battery replacement, gutter cleaning, sky scrapper cleaning, and power line maintenance in hard-to-reach and risky en
    
[^67]: Match-And-Deform: 通过最优传输和时间对齐进行时间序列领域自适应

    Match-And-Deform: Time Series Domain Adaptation through Optimal Transport and Temporal Alignment. (arXiv:2308.12686v1 [cs.LG])

    [http://arxiv.org/abs/2308.12686](http://arxiv.org/abs/2308.12686)

    本文提出了一种名为Match-And-Deform（MAD）的方法，通过最优传输和时间对齐，在时间序列领域自适应问题中找到对应关系并允许时间失真，实验结果表明MAD可以生成对齐领域并最大化网络判别能力的新时间序列表示形式。

    

    尽管通常存在大量的无标签数据，但相关联的标签往往很少。无监督领域自适应问题旨在利用来自源领域的标签来对来自相关但不同的目标领域的数据进行分类。当涉及时间序列时，除了标准特征分布偏移之外，还会出现时间偏移的新难题。在本文中，我们介绍了Match-And-Deform（MAD）方法，该方法旨在在允许时间失真的同时在源时间序列和目标时间序列之间找到对应关系。通过最优传输损失和动态时间扭曲同时对齐了系列。当嵌入到深度神经网络中时，MAD有助于学习时间序列的新表示，既可以对齐领域又可以最大化网络的判别能力。对基准数据集和遥感数据进行的实证研究表明，MAD具有意义。

    While large volumes of unlabeled data are usually available, associated labels are often scarce. The unsupervised domain adaptation problem aims at exploiting labels from a source domain to classify data from a related, yet different, target domain. When time series are at stake, new difficulties arise as temporal shifts may appear in addition to the standard feature distribution shift. In this paper, we introduce the Match-And-Deform (MAD) approach that aims at finding correspondences between the source and target time series while allowing temporal distortions. The associated optimization problem simultaneously aligns the series thanks to an optimal transport loss and the time stamps through dynamic time warping. When embedded into a deep neural network, MAD helps learning new representations of time series that both align the domains and maximize the discriminative power of the network. Empirical studies on benchmark datasets and remote sensing data demonstrate that MAD makes meanin
    
[^68]: 遮罩特征建模：无监督预训练图注意力网络块的特征遮罩方法

    Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition. (arXiv:2308.12673v1 [cs.CV])

    [http://arxiv.org/abs/2308.12673](http://arxiv.org/abs/2308.12673)

    本文提出了一种新的遮罩特征建模方法(MFM)，用于无监督预训练图注意力网络块。通过利用预训练的视觉分词器来重构视频中对象的遮罩特征，将预训练的GAT块融入到视频事件识别架构ViGAT中，以改善模型的性能。

    

    本文介绍了一种新颖的方法，即遮罩特征建模(MFM)，用于无监督预训练图注意力网络(GAT)块。MFM利用预训练的视觉分词器来重构视频中对象的遮罩特征，利用MiniKinetics数据集。然后将预训练的GAT块融入到最先进的自底向上有监督视频事件识别架构ViGAT中，以改善模型的起点和整体准确性。在YLI-MED数据集上的实验评估表明，MFM在提高事件识别性能方面具有有效性。

    In this paper, we introduce Masked Feature Modelling (MFM), a novel approach for the unsupervised pre-training of a Graph Attention Network (GAT) block. MFM utilizes a pretrained Visual Tokenizer to reconstruct masked features of objects within a video, leveraging the MiniKinetics dataset. We then incorporate the pre-trained GAT block into a state-of-the-art bottom-up supervised video-event recognition architecture, ViGAT, to improve the model's starting point and overall accuracy. Experimental evaluations on the YLI-MED dataset demonstrate the effectiveness of MFM in improving event recognition performance.
    
[^69]: 扩展性和指导调优的扩散语言模型能够完成多种任务

    Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])

    [http://arxiv.org/abs/2308.12219](http://arxiv.org/abs/2308.12219)

    本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。

    

    最近生成式人工智能的兴起得益于扩散概率模型的生成能力和大规模语言模型的可扩展性。尽管具有潜力，但扩散语言模型是否能够解决与自回归模型相媲美的通用语言任务仍然不明确。本文证明了在数据、规模和任务方面扩展扩散模型能够有效使其成为强大的语言学习者。我们通过先通过掩码语言建模预训练从大规模数据中获取知识，再通过扩散适应将预训练的掩码语言模型改进为扩散语言模型，通过任务特定的微调和指导调优来发掘其在解决通用语言任务方面的多样性。实验证明，扩展扩散语言模型能够在下游语言任务中持续提高性能。

    The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
    
[^70]: 自我欺骗：逆向破解大型语言模型的语义防火墙

    Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])

    [http://arxiv.org/abs/2308.11521](http://arxiv.org/abs/2308.11521)

    这篇论文研究了大型语言模型的越狱问题，并提出了一种自动越狱方法，介绍了语义防火墙的概念和三种技术实现方法。

    

    大型语言模型（LLM），如ChatGPT，具有接近人工通用智能的惊人能力。虽然为各种社会需求提供了便利，但LLM也降低了生成有害内容的成本。因此，LLM开发人员已经部署了语义级的防御机制，用于识别和拒绝可能导致不适当内容的提示。不幸的是，这些防御机制并不完全可靠，一些攻击者已经设计出了“越狱”提示，临时使LLM忘记内容防御规则并回答任何不适当的问题。迄今为止，业界和学术界尚无关于这些语义级攻击和防御原则的明确解释。本文研究了LLM越狱问题，并首次提出了一种自动越狱方法。我们提出了语义防火墙的概念，并提供了三种技术实现方法。

    Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.  This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates trad
    
[^71]: 作者身份表征学习能够捕捉文体特征吗？

    Can Authorship Representation Learning Capture Stylistic Features?. (arXiv:2308.11490v1 [cs.CL])

    [http://arxiv.org/abs/2308.11490](http://arxiv.org/abs/2308.11490)

    本论文研究了作者身份表征学习能否捕捉文体特征的问题，并通过实验验证了这些表征能够有效地捕捉写作风格的特征。

    

    在计算语言学中，自动将作者的风格从其写作内容中分离出来是一个长期存在且可能不可解决的问题。同时，最近有大量带有作者标签的文本语料库可用，使得以纯数据驱动的方式学习作者身份表征成为可能，用于作者归属这一任务，该任务显然更多地依赖于编码写作风格而不是编码内容。然而，对这一替代任务的成功并不能确保这些表征能够捕捉写作风格，因为作者身份也可能与其他潜在变量（如主题）相关。为了更好地理解这些表征所传递的信息的本质，特别是为了验证其主要编码的是写作风格的假设，我们通过一系列有针对性的实验系统地检查了这些表征。这些实验的结果表明，为作者表示学习的表征能够有效地捕捉写作风格的特征。

    Automatically disentangling an author's style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the 
    
[^72]: 贝叶斯多项式神经网络和多项式神经常微分方程

    Bayesian polynomial neural networks and polynomial neural ordinary differential equations. (arXiv:2308.10892v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10892](http://arxiv.org/abs/2308.10892)

    本研究提出了贝叶斯推断方法来改善多项式神经网络和多项式神经常微分方程在方程恢复问题中的表现，其中拉普拉斯近似是最佳的方法。这种方法可以推广到更广泛的符号神经网络类别。

    

    多项式神经网络和多项式神经常微分方程是最近用于科学和工程问题方程恢复的两种强大方法。然而，这些方法只能提供模型参数的点估计，并且目前不能适应噪声数据。我们通过开发和验证以下贝叶斯推断方法来解决这个挑战: 拉普拉斯近似、马尔可夫链蒙特卡洛(MCMC)采样方法和变分推断。我们发现拉普拉斯近似是这类问题的最佳方法。我们的工作可以轻松扩展到多项式神经网络所属的更广泛的符号神经网络类别。

    Symbolic regression with polynomial neural networks and polynomial neural ordinary differential equations (ODEs) are two recent and powerful approaches for equation recovery of many science and engineering problems. However, these methods provide point estimates for the model parameters and are currently unable to accommodate noisy data. We address this challenge by developing and validating the following Bayesian inference methods: the Laplace approximation, Markov Chain Monte Carlo (MCMC) sampling methods, and variational inference. We have found the Laplace approximation to be the best method for this class of problems. Our work can be easily extended to the broader class of symbolic neural networks to which the polynomial neural network belongs.
    
[^73]: 人工上涌能源管理的深度强化学习

    Deep Reinforcement Learning for Artificial Upwelling Energy Management. (arXiv:2308.10199v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10199](http://arxiv.org/abs/2308.10199)

    通过使用深度强化学习算法，我们提出了一种新颖的能源管理方法来优化操作人工上涌系统。通过将问题建模为马尔可夫决策过程，并结合分位网络和深度竞争网络的思想，我们的方法在提高能源效率方面取得了显著成效。

    

    近年来，人工上涌（AU）作为一种将富含营养的底层水提升到海面、刺激海藻生长并增加海洋碳封存的方法，受到了越来越多的关注。这导致在中国开发了第一个太阳能供电和空气增压的AU系统（AUS）。然而，在复杂的海洋环境中高效调度气体喷射系统仍然是操作AUS的关键挑战，因为它有潜力显著提高能源效率。为了解决这一挑战，我们提出了一种利用深度强化学习（DRL）算法开发AUS运行的高效策略的新能源管理方法。具体而言，我们将最大化AUS能源效率的问题建模为马尔可夫决策过程，并将分布式强化学习（QR-DQN）中的分位网络与深度竞争网络相结合以解决这个问题。通过大量的仿真实验，我们证明了我们的方法可以显著提高AUS的能源效率。

    The potential of artificial upwelling (AU) as a means of lifting nutrient-rich bottom water to the surface, stimulating seaweed growth, and consequently enhancing ocean carbon sequestration, has been gaining increasing attention in recent years. This has led to the development of the first solar-powered and air-lifted AU system (AUS) in China. However, efficient scheduling of air injection systems in complex marine environments remains a crucial challenge in operating AUS, as it holds the potential to significantly improve energy efficiency. To tackle this challenge, we propose a novel energy management approach that utilizes deep reinforcement learning (DRL) algorithm to develop efficient strategies for operating AUS. Specifically, we formulate the problem of maximizing the energy efficiency of AUS as a Markov decision process and integrate the quantile network in distributional reinforcement learning (QR-DQN) with the deep dueling network to solve it. Through extensive simulations, w
    
[^74]: 分布式学习中的资源自适应牛顿法

    Resource-Adaptive Newton's Method for Distributed Learning. (arXiv:2308.10154v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10154](http://arxiv.org/abs/2308.10154)

    本文介绍了一种名为RANL的新颖和高效的算法，通过使用简单的Hessian初始化和自适应的训练区域分配，克服了牛顿法在大规模和异构学习环境中的限制，并实现了线性收敛率。

    

    基于牛顿方法的分布式随机优化方法通过利用曲率信息提供了比一阶方法更好的性能。然而，在大规模和异构学习环境中，牛顿方法的实际适用性受到了诸多挑战的限制，例如与Hessian矩阵相关的高计算和通信成本、子模型多样性、训练的过时性和数据的异构性。为了解决这些挑战，本文引入了一种名为RANL的新颖高效算法，通过采用简单的Hessian初始化和自适应的训练区域分配来克服牛顿方法的限制。该算法表现出令人印象深刻的收敛性质，在随机优化的标准假设下进行了严格分析。理论分析证明了RANL实现了线性收敛率，同时有效地适应了可用资源。

    Distributed stochastic optimization methods based on Newton's method offer significant advantages over first-order methods by leveraging curvature information for improved performance. However, the practical applicability of Newton's method is hindered in large-scale and heterogeneous learning environments due to challenges such as high computation and communication costs associated with the Hessian matrix, sub-model diversity, staleness in training, and data heterogeneity. To address these challenges, this paper introduces a novel and efficient algorithm called RANL, which overcomes the limitations of Newton's method by employing a simple Hessian initialization and adaptive assignments of training regions. The algorithm demonstrates impressive convergence properties, which are rigorously analyzed under standard assumptions in stochastic optimization. The theoretical analysis establishes that RANL achieves a linear convergence rate while effectively adapting to available resources and 
    
[^75]: 超越共享：冲突感知的多变量时间序列异常检测

    Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection. (arXiv:2308.08915v1 [cs.LG])

    [http://arxiv.org/abs/2308.08915](http://arxiv.org/abs/2308.08915)

    这篇论文提出了一种冲突感知的多变量时间序列异常检测算法，该算法通过为每个指标提供独特的结构来缓解指标回归目标之间的冲突。

    

    大量的关键绩效指标(KPI)以多变量时间序列数据(MTS)的形式进行监测，以确保软件应用程序和服务系统的可靠性。准确检测MTS的异常对于后续的故障排除非常关键。异常的稀缺性和手动标记导致了各种自监督的MTS异常检测方法的发展，这些方法优化了一个涵盖所有指标回归目标/损失的整体目标/损失。然而，我们的实证研究发现了指标回归目标之间冲突的普遍存在，导致MTS模型在不同的损失中挣扎。这一关键方面显著影响检测性能，但在现有方法中被忽视了。为了解决这个问题，通过模仿多门专家混合模型(MMoE)的设计，我们引入了CAD，一种冲突感知的多变量KPI异常检测算法。CAD为每个指标提供了一个独特的结构，以缓解冲突。

    Massive key performance indicators (KPIs) are monitored as multivariate time series data (MTS) to ensure the reliability of the software applications and service system. Accurately detecting the abnormality of MTS is very critical for subsequent fault elimination. The scarcity of anomalies and manual labeling has led to the development of various self-supervised MTS anomaly detection (AD) methods, which optimize an overall objective/loss encompassing all metrics' regression objectives/losses. However, our empirical study uncovers the prevalence of conflicts among metrics' regression objectives, causing MTS models to grapple with different losses. This critical aspect significantly impacts detection performance but has been overlooked in existing approaches. To address this problem, by mimicking the design of multi-gate mixture-of-experts (MMoE), we introduce CAD, a Conflict-aware multivariate KPI Anomaly Detection algorithm. CAD offers an exclusive structure for each metric to mitigate
    
[^76]: 由核生长神经气体生成的网络的特性

    Characteristics of networks generated by kernel growing neural gas. (arXiv:2308.08163v1 [cs.LG])

    [http://arxiv.org/abs/2308.08163](http://arxiv.org/abs/2308.08163)

    本研究开发了核化的生长神经气体算法，并研究了由此算法生成的网络的特性。研究发现，核生长神经气体算法可以将数据集转化为无向图，并提取数据集的特征作为图形。五种核函数被用于此研究。

    

    本研究旨在开发核化的生长神经气体（GNG）算法的核GNG，并研究由核GNG生成的网络的特征。GNG是一种无监督的人工神经网络，可以将数据集转化为无向图，从而提取数据集的特征作为图形。GNG被广泛应用于向量量化、聚类和三维图形中。核方法常用于将数据集映射到特征空间，其中支持向量机是最重要的应用之一。本文介绍了核GNG方法，并探索了由核GNG生成的网络的特性。本研究中使用了五种核函数，包括高斯核、拉普拉斯核、柯西核、反多项式核和对数核。

    This research aims to develop kernel GNG, a kernelized version of the growing neural gas (GNG) algorithm, and to investigate the features of the networks generated by the kernel GNG. The GNG is an unsupervised artificial neural network that can transform a dataset into an undirected graph, thereby extracting the features of the dataset as a graph. The GNG is widely used in vector quantization, clustering, and 3D graphics. Kernel methods are often used to map a dataset to feature space, with support vector machines being the most prominent application. This paper introduces the kernel GNG approach and explores the characteristics of the networks generated by kernel GNG. Five kernels, including Gaussian, Laplacian, Cauchy, inverse multiquadric, and log kernels, are used in this study.
    
[^77]: 如何在纠错码变压器中进行遮蔽：系统化与双重遮蔽

    How to Mask in Error Correction Code Transformer: Systematic and Double Masking. (arXiv:2308.08128v1 [cs.LG])

    [http://arxiv.org/abs/2308.08128](http://arxiv.org/abs/2308.08128)

    该论文介绍了在纠错码变压器中使用系统化编码和双重遮蔽的方法，以提高性能和减少计算复杂性。

    

    在通信和存储系统中，纠错码（ECC）对于确保数据可靠性至关重要。随着深度学习在不同领域的应用广泛扩展，神经网络解码器已成为研究的焦点，超越传统解码算法。在这些神经解码器中，纠错码变压器（ECCT）已经实现了最先进的性能，大幅超过其他方法。为了进一步提高ECCT的性能，我们提出了两种新方法。首先，利用ECC的系统编码技术，我们引入了一个新的遮蔽矩阵来改善ECCT的性能并减少计算复杂性。其次，我们提出了一种新的ECCT变压器架构，称为双重遮蔽的ECCT。该架构以并行方式使用两个不同的遮蔽矩阵，以学习遮蔽自注意力块中编码字位之间更多样的特征关系。大量实验证明了我们方法的有效性。

    In communication and storage systems, error correction codes (ECCs) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer (ECCT) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of ECCT, we propose two novel methods. First, leveraging the systematic encoding technique of ECCs, we introduce a new masking matrix for ECCT, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of ECCT called a double-masked ECCT. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive si
    
[^78]: AudioFormer: 通过离散的声学代码学习音频特征表示的音频变换器

    AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes. (arXiv:2308.07221v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2308.07221](http://arxiv.org/abs/2308.07221)

    AudioFormer是一种学习音频特征表示的方法，通过生成离散的声学代码并利用它们来训练掩码语言模型，从而将音频分类任务视为自然语言理解的形式。此外，引入了多正样本对比学习方法，通过学习联合表示来捕捉音频中的相关性。

    

    我们提出了一种名为AudioFormer的方法，通过获取离散的声学代码来学习音频特征表示，并随后对其进行微调以用于音频分类任务。我们首先将音频分类任务视为一种自然语言理解 (NLU) 的形式，借助现有的神经音频编解码模型，我们生成了离散的声学代码，并利用它们来训练一个掩码语言模型 (MLM)，从而获得音频特征表示。此外，我们首创了一种多正样本对比 (MPC) 学习方法的整合，该方法能够学习同一音频输入中多个离散声学代码间的联合表示。在实验中，我们将离散的声学代码视为文本数据，并使用类似填空题的方法训练一个掩码语言模型，最终得到高质量的音频表示。值得注意的是，MPC学习技术能够有效捕捉到音频中的相关性。

    We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures c
    
[^79]: 专家权重平均化: 一种视觉Transformer的新通用训练方案

    Experts Weights Averaging: A New General Training Scheme for Vision Transformers. (arXiv:2308.06093v1 [cs.CV])

    [http://arxiv.org/abs/2308.06093](http://arxiv.org/abs/2308.06093)

    本文提出了一种新的通用训练策略，利用专家权重平均化实现了对ViTs的性能提升，而不增加推断成本。

    

    结构重参数化是一种用于卷积神经网络（CNNs）的通用训练方案，可以在不增加推断成本的情况下提高性能。随着视觉Transformer (ViTs)在各种视觉任务中逐渐超越CNNs，一个问题出现了: 是否存在一种专门用于ViTs的训练方案，可以在不增加推断成本的情况下提高性能？最近，混合专家（MoE）引起了越来越多的关注，因为它可以通过稀疏激活的专家有效地扩展Transformer的容量，而成本保持不变。考虑到MoE也可以看作是一种多支系结构，我们能否利用MoE来实现类似结构重参数化的ViT训练方案呢？在本文中，我们肯定地回答了这些问题，并提出了一种新的ViTs通用训练策略。具体来说，我们分离了ViTs的训练和推断阶段。在训练阶段，我们替换了一些前馈网络（FFNs）

    Structural re-parameterization is a general training scheme for Convolutional Neural Networks (CNNs), which achieves performance improvement without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may question: if a training scheme specifically for ViTs exists that can also achieve performance improvement without increasing inference cost? Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we affirmatively answer these questions, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the 
    
[^80]: 保持对称性的程序表示法用于学习代码语义

    Symmetry-Preserving Program Representations for Learning Code Semantics. (arXiv:2308.03312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03312](http://arxiv.org/abs/2308.03312)

    本文提出了一种在代码中保持对称性的程序表示方法，通过引入循环层来提高在程序分析和建模中的效果。

    

    大型语言模型(LLMs)在自动程序推理方面显示出潜力，这是许多安全任务的关键方面。然而，现有的用于代码的LLM架构通常从其他领域（如自然语言处理）借用，引发对其泛化能力和对未知代码的健壮性的担忧。一个关键的泛化挑战是将代码语义的知识，包括控制和数据流，纳入LLM架构中。受到利用平移对称性的卷积层的启发，我们探索了代码对称性如何增强程序分析和建模的LLM架构。我们提供了一个严格的群论框架，形式化地定义了代码对称性作为保持语义的变换，并提供了在LLM架构中精确推理对称性保持的技术。利用这个框架，我们引入了一种保持程序对称性的新型自注意力变体，并展示了其有效性。

    Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.  Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiv
    
[^81]: ChatMOF: 一种自主人工智能系统用于预测和生成金属-有机骨架

    ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])

    [http://arxiv.org/abs/2308.01423](http://arxiv.org/abs/2308.01423)

    ChatMOF是一种自主AI系统，用于预测和生成金属-有机骨架。通过利用大规模语言模型，它能够从文本输入中提取关键细节，并提供适当的回应。该系统通过组合代理、工具包和评估器的核心组件，实现了数据检索、性质预测和结构生成等多个任务。研究进一步展示了在材料科学中使用大型语言模型的优势和潜力。

    

    ChatMOF是一个自主人工智能系统，用于预测和生成金属-有机骨架（MOFs）。通过利用大规模语言模型（gpt-3.5-turbo），ChatMOF从文本输入中提取关键细节并提供适当的回应，从而消除了对刚性结构化查询的需求。该系统由三个核心组件（即代理、工具包和评估器）组成，形成一个强大的流水线，管理多种任务，包括数据检索、性质预测和结构生成。该研究进一步探讨了在材料科学中使用大型语言模型（LLMs）人工智能系统的优点和限制，并展示了其对未来发展的变革潜力。

    ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
    
[^82]: BubbleML: 用于机器学习的多物理数据集和基准

    BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning. (arXiv:2307.14623v1 [cs.LG])

    [http://arxiv.org/abs/2307.14623](http://arxiv.org/abs/2307.14623)

    BubbleML是一个用于机器学习的多物理数据集，通过物理驱动模拟获得准确的地面真实信息，并在各种沸腾场景中验证了其可靠性和潜力。

    

    在相变现象领域，缺乏适用于机器学习训练的可访问和多样化的数据集是一个重要挑战。现有的实验数据集通常受限，可用性有限且地面真实数据稀缺，阻碍了我们对这种复杂多物理现象的理解。为了弥补这一差距，我们提出了BubbleML数据集（https://github.com/HPCForge/BubbleML），它利用物理驱动的模拟为各种沸腾场景提供准确的地面真实信息，包括核泡池沸腾、流动沸腾和亚冷沸腾。这个广泛的数据集涵盖了各种参数，包括不同的重力条件、流量、亚冷水平和壁面过热，总共有51个模拟。BubbleML已经通过实验观察和趋势进行了验证，被确认为机器学习研究的宝贵资源。此外，我们展示了它促进多样化降低温度沸腾研究的潜力。

    In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth data, impeding our understanding of this complex multi-physics phenomena. To bridge this gap, we present the BubbleML Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 51 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate exploration of diverse dow
    
[^83]: 探索从替代训练中理解对抗性可转移性

    Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])

    [http://arxiv.org/abs/2307.07873](http://arxiv.org/abs/2307.07873)

    本论文探索了对抗性可转移性的理解，特别关注替代训练。通过研究模型的平滑性和梯度相似性之间的权衡，发现对抗训练可以提高模型的替代能力。研究结果对数据分布的转变提出了新的推测。

    

    对DNNs的对抗样本(AEs)已经表明是可转移的：成功欺骗白盒子替代模型的AEs也可以欺骗具有不同架构的其他黑盒模型。虽然许多经验研究提供了生成高度可转移AE的指导，但这些研究缺乏解释甚至导致不一致的建议。本文在理解对抗性可转移性方面迈出了一步，特别关注替代方面。从着名的小健壮性现象开始，通过以轻微扰动的对抗性样本对模型进行对抗训练可以得到更好的替代模型，我们将其归因于两个主要因素之间的权衡：模型的平滑性和梯度相似性。我们的研究集中在它们的共同效果上，而不是它们与可转移性的单独相关性。通过一系列理论和实证分析，我们推测数据分布的转变。

    Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in
    
[^84]: Min-Max优化在延迟下的研究

    Min-Max Optimization under Delays. (arXiv:2307.06886v1 [cs.LG])

    [http://arxiv.org/abs/2307.06886](http://arxiv.org/abs/2307.06886)

    在大规模机器学习中，研究了min-max优化在延迟下的性能。对于简单实例，即使是小的延迟也可能导致Extra-gradient算法发散，因此需要对延迟版本的min-max优化算法进行仔细分析。为此，我们证明了在适当的技术假设下，梯度下降-上升算法在延迟情况下的收敛性和性能。

    

    在通信起重要作用的大规模机器学习问题中，延迟和异步是不可避免的。因此，一些研究团队广泛分析了具有延迟梯度的随机优化问题。但据我们所知，尚无类似的理论可用于min-max优化，这个话题由于在对抗鲁棒性、博弈论和强化学习中的应用而越来越受关注。针对这一差距，我们对带有延迟梯度更新的标准min-max优化算法的性能进行了研究。首先，我们（经验性地）展示了即使是小的延迟也可能导致像Extra-gradient (EG) 这样的杰出算法在简单实例上发散，而在没有延迟的情况下EG可以保证收敛。因此，我们的经验研究表明有必要对延迟版本的min-max优化算法进行仔细分析。相应地，在适当的技术假设下，我们证明了梯度下降 - 上升 (GDA)算法在延迟情况下的收敛性和性能。

    Delays and asynchrony are inevitable in large-scale machine-learning problems where communication plays a key role. As such, several works have extensively analyzed stochastic optimization with delayed gradients. However, as far as we are aware, no analogous theory is available for min-max optimization, a topic that has gained recent popularity due to applications in adversarial robustness, game theory, and reinforcement learning. Motivated by this gap, we examine the performance of standard min-max optimization algorithms with delayed gradient updates. First, we show (empirically) that even small delays can cause prominent algorithms like Extra-gradient (\texttt{EG}) to diverge on simple instances for which \texttt{EG} guarantees convergence in the absence of delays. Our empirical study thus suggests the need for a careful analysis of delayed versions of min-max optimization algorithms. Accordingly, under suitable technical assumptions, we prove that Gradient Descent-Ascent (\texttt{G
    
[^85]: IntelliGraphs: 用于评估知识图谱生成的数据集

    IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v1 [cs.AI])

    [http://arxiv.org/abs/2307.06698](http://arxiv.org/abs/2307.06698)

    IntelliGraphs是一组新的知识图谱数据集，用于评估知识图谱生成。其中包含具有逻辑规则表达的语义的子图，用于评估子图推断的模型。

    

    知识图谱嵌入（KGE）模型用于学习实体和关系的连续表示。文献中一个关键的任务是预测实体之间的缺失链接。然而，知识图谱不仅仅是链接的集合，还具有其结构中的语义。语义在多个下游任务中至关重要，例如查询回答或推理。我们引入了子图推断任务，其中一个模型必须生成可能的并且语义上有效的子图。我们提出了IntelliGraphs，一个包含五个新的知识图谱数据集的集合。IntelliGraphs数据集包含具有逻辑规则表达的语义的子图，用于评估子图推断。我们还设计了产生合成数据集的数据集生成器。我们设计了四个新的基准模型，其中包括基于传统KGE的三个模型。我们评估了它们的表达能力，并展示了这些模型无法捕捉到语义。我们相信这一基准将促进该领域的发展。

    Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development
    
[^86]: 联合集成YOLOv5 - 一种更好的广义目标检测算法

    Federated Ensemble YOLOv5 - A Better Generalized Object Detection Algorithm. (arXiv:2306.17829v1 [cs.CV])

    [http://arxiv.org/abs/2306.17829](http://arxiv.org/abs/2306.17829)

    本文研究了联合学习算法在目标检测中的应用，并将其与集中式训练方法进行性能比较。实验结果表明，使用联合学习训练的YOLOv5模型在生成准确的未见过目标的边界框方面具有显著优势。

    

    联合学习（FL）作为一种保护隐私的算法已经获得了显著的关注，但是联合学习算法如联合平均（FED Avg）或联合SGD（FED SGD）与集成学习算法的相似性尚未完全探索。本文旨在研究FL在目标检测中的应用作为一种增强泛化性能的方法，并将其与集中式训练方法的目标检测算法进行性能比较。具体来说，我们研究了使用FL训练的YOLOv5模型在多个客户端上的性能，并采用无替换的随机抽样策略，使每个客户端持有一部分用于集中式训练的相同数据集。我们的实验结果展示了FL目标检测器的全局模型在生成准确的未见过目标的边界框方面的卓越效率，测试集是来自两个不同客户端的对象的混合。

    Federated learning (FL) has gained significant traction as a privacy-preserving algorithm, but the underlying resembles of federated learning algorithm like Federated averaging (FED Avg) or Federated SGD (FED SGD) to ensemble learning algorithms has not been fully explored. The purpose of this paper is to examine the application of FL to object detection as a method to enhance generalizability, and to compare its performance against a centralized training approach for an object detection algorithm. Specifically, we investigate the performance of a YOLOv5 model trained using FL across multiple clients and employ a random sampling strategy without replacement, so each client holds a portion of the same dataset used for centralized training. Our experimental results showcase the superior efficiency of the FL object detector's global model in generating accurate bounding boxes for unseen objects, with the test set being a mixture of objects from two distinct clients not represented in the 
    
[^87]: 共享生产中的联邦目标检测用于质量检验

    Federated Object Detection for Quality Inspection in Shared Production. (arXiv:2306.17645v1 [cs.LG])

    [http://arxiv.org/abs/2306.17645](http://arxiv.org/abs/2306.17645)

    本文提出了一个利用联邦学习进行质量检验任务中的目标检测的算法，该算法使用YOLOv5作为目标检测算法和Federated Averaging作为联邦学习算法。实验结果表明，该联邦学习方法在整体客户测试数据集上具有更好的泛化性能，并生成相对于使用本地客户数据集训练的模型更精确的边界框。

    

    联邦学习（FL）已经成为一种在分散数据的条件下训练机器学习模型的有希望方法，而不会损害数据隐私。在本文中，我们提出了一种利用YOLOv5作为目标检测算法和联邦平均（FedAvg）作为FL算法的联邦学习算法，用于质量检验任务中的目标检测。我们将此方法应用于制造业中的一个使用案例，多个工厂/客户共享数据以训练全局目标检测模型，同时在非IID数据集上保护数据隐私。我们的实验表明，我们的联邦学习方法在整体客户测试数据集上实现了更好的泛化性能，并且生成的边界框相对于使用本地客户数据集训练的模型更加精确。这项工作展示了联邦学习在制造业质量检验任务中的潜力，并为利用YOLOv5和FedAvg进行联邦目标检测提供了有价值的见解。

    Federated learning (FL) has emerged as a promising approach for training machine learning models on decentralized data without compromising data privacy. In this paper, we propose a FL algorithm for object detection in quality inspection tasks using YOLOv5 as the object detection algorithm and Federated Averaging (FedAvg) as the FL algorithm. We apply this approach to a manufacturing use-case where multiple factories/clients contribute data for training a global object detection model while preserving data privacy on a non-IID dataset. Our experiments demonstrate that our FL approach achieves better generalization performance on the overall clients' test dataset and generates improved bounding boxes around the objects compared to models trained using local clients' datasets. This work showcases the potential of FL for quality inspection tasks in the manufacturing industry and provides valuable insights into the performance and feasibility of utilizing YOLOv5 and FedAvg for federated ob
    
[^88]: 何去何从：深度学习加速的数字硬件视角

    To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])

    [http://arxiv.org/abs/2306.15749](http://arxiv.org/abs/2306.15749)

    神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。

    

    随着深度学习模型规模的增加，它们在涵盖计算机视觉到自然语言处理等领域变得越来越有竞争力；然而，这是以效率为代价的，因为它们需要越来越多的内存和计算能力。生物脑的功耗效率超过任何大规模深度学习（DL）模型；因此，神经形态计算试图模仿脑部操作，例如基于脉冲的信息处理，以提高DL模型的效率。尽管脑部有诸如高效的信息传输、密集的神经元连接和计算与存储的共同位置等优势，但可用的生物基底严重限制了生物大脑的进化。电子硬件没有相同的约束；因此，虽然建模脉冲神经网络（SNNs）可能揭示了一个谜题的一部分，但对于SNNs的高效硬件后端设计需要进一步研究。

    As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
    
[^89]: 快速建筑损伤评估工作流：针对2023年密西西比州滚动叉口龙卷风事件的实现

    Rapid building damage assessment workflow: An implementation for the 2023 Rolling Fork, Mississippi tornado event. (arXiv:2306.12589v1 [cs.CV])

    [http://arxiv.org/abs/2306.12589](http://arxiv.org/abs/2306.12589)

    本文介绍了一种人在循环工作流，用于自然灾害后快速训练建筑损伤评估模型，并通过在2023年密西西比州滚动叉口龙卷风事件中的案例研究获得了较高的精度和召回率。

    

    自然灾害后通过高分辨率卫星图像进行快速准确的建筑损伤评估对于指导和优化第一应答者的努力至关重要。然而，由于不同的灾害损伤、卫星图像的多样性以及缺乏广泛的标记数据集等问题，以自动化方式执行此类建筑损伤评估并不容易。为了解决这些问题，本文介绍了一种人在循环工作流，用于在自然灾害后快速训练建筑损伤评估模型。该文章详细介绍了一个使用此工作流的案例研究，该工作流是与美国红十字会合作执行的，针对2023年3月密西西比州滚动叉口龙卷风事件。根据后灾情收集的地面真实数据，人在循环模型过程的输出在受损建筑方面实现了0.86的精度和0.80的召回率。这个工作流的端到端实现时间不到2个小时。

    Rapid and accurate building damage assessments from high-resolution satellite imagery following a natural disaster is essential to inform and optimize first responder efforts. However, performing such building damage assessments in an automated manner is non-trivial due to the challenges posed by variations in disaster-specific damage, diversity in satellite imagery, and the dearth of extensive, labeled datasets. To circumvent these issues, this paper introduces a human-in-the-loop workflow for rapidly training building damage assessment models after a natural disaster. This article details a case study using this workflow, executed in partnership with the American Red Cross during a tornado event in Rolling Fork, Mississippi in March, 2023. The output from our human-in-the-loop modeling process achieved a precision of 0.86 and recall of 0.80 for damaged buildings when compared to ground truth data collected post-disaster. This workflow was implemented end-to-end in under 2 hours per s
    
[^90]: 修复公平性，而不是破坏准确性：使用AutoML的性能感知公平修复

    Fix Fairness, Don't Ruin Accuracy: Performance Aware Fairness Repair using AutoML. (arXiv:2306.09297v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2306.09297](http://arxiv.org/abs/2306.09297)

    本文提出了一个使用AutoML技术来减少偏见的新方法，该方法通过改进优化函数和搜索空间，并结合公平目标，在几乎不损失准确性的情况下减少基于ML的软件中的偏见。同时提出了一个适用于AutoML的公平感知搜索空间修剪方法，以减少计算成本和修复时间。

    

    机器学习（ML）越来越多地被用于关键决策软件中，但事故引发了关于ML预测公平性的质疑。为了解决这个问题，需要新的工具和方法来减少基于ML的软件中的偏见。先前的研究提出了偏见缓解算法，但这些算法只能在特定情况下工作，并且通常会导致准确性损失。我们提出了一种新颖的方法，利用自动机器学习（AutoML）技术来减少偏见。我们的方法包括两个关键创新：一种新颖的优化函数和一个公平感知的搜索空间。通过改进AutoML的默认优化函数并结合公平目标，我们能够在几乎不损失准确性的情况下减少偏见。此外，我们还提出了一种适用于AutoML的公平感知搜索空间修剪方法，以减少计算成本和修复时间。我们的方法建立在最先进的Auto-Sklearn工具上，旨在减少偏见。

    Machine learning (ML) is increasingly being used in critical decision-making software, but incidents have raised questions about the fairness of ML predictions. To address this issue, new tools and methods are needed to mitigate bias in ML-based software. Previous studies have proposed bias mitigation algorithms that only work in specific situations and often result in a loss of accuracy. Our proposed solution is a novel approach that utilizes automated machine learning (AutoML) techniques to mitigate bias. Our approach includes two key innovations: a novel optimization function and a fairness-aware search space. By improving the default optimization function of AutoML and incorporating fairness objectives, we are able to mitigate bias with little to no loss of accuracy. Additionally, we propose a fairness-aware search space pruning method for AutoML to reduce computational cost and repair time. Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed to reduce bias i
    
[^91]: 克服针对人机交互应用的对抗攻击

    Overcoming Adversarial Attacks for Human-in-the-Loop Applications. (arXiv:2306.05952v1 [cs.LG])

    [http://arxiv.org/abs/2306.05952](http://arxiv.org/abs/2306.05952)

    通过人类视觉注意力模型改善人机图像分析系统的可解释性和鲁棒性，克服针对人机交互应用的对抗攻击。

    

    包含人类分析可能对深度神经网络的强韧性产生积极影响，在对抗机器学习领域相对较少被探索。神经网络视觉解释图经常容易遭受对抗性攻击。为了让图像分析者评估给定模型，需要进一步研究选择稳健的解释可视化。这些因素极大地影响了人机交互（HITL）评估工具，因为它们依赖于对抗性图像，包括解释图和鲁棒性测量。我们相信，人类视觉注意力模型可以改善人机图像分析系统的可解释性和鲁棒性。我们面临的挑战是，在这种对抗性环境下，如何使HITL评估更加鲁棒。

    Including human analysis has the potential to positively affect the robustness of Deep Neural Networks and is relatively unexplored in the Adversarial Machine Learning literature. Neural network visual explanation maps have been shown to be prone to adversarial attacks. Further research is needed in order to select robust visualizations of explanations for the image analyst to evaluate a given model. These factors greatly impact Human-In-The-Loop (HITL) evaluation tools due to their reliance on adversarial images, including explanation maps and measurements of robustness. We believe models of human visual attention may improve interpretability and robustness of human-machine imagery analysis systems. Our challenge remains, how can HITL evaluation be robust in this adversarial landscape?
    
[^92]: SpeechGen: 利用提示解锁语音语言模型的生成能力

    SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.02207](http://arxiv.org/abs/2306.02207)

    本文探索了一个名为SpeechGen的统一框架，通过提示调节，解锁了语音语言模型的生成能力，成功地实现了直接适应连续语音到离散标记的任务，使得语音生成成为可能。

    

    大型语言模型（LLM）在人工智能生成内容（AIGC）中引起了相当大的关注，特别是随着ChatGPT的出现。然而，将连续语音直接适应于处理离散标记的LLM仍然是一个未解决的挑战，这妨碍了LLM在语音生成方面的应用。高级语音LM们无法充分利用语音信号所包含的丰富信息，包括说话者和情感等，这些信息仅通过文本数据无法获取。在一些语音分类任务中，简单的提示调整已经表现出明显的参数效率和竞争性能的提高。但在多大程度上提示能够有效地激发语音LM的生成任务仍然是一个未知的问题。本文提出了一项先驱性研究，该研究在称为SpeechGen的统一框架中使用提示调节来刺激语音LM进行各种生成任务，并具有约10M可训练参数。

    Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unif
    
[^93]: 概率深度学习的量子核混合方法

    Quantum Kernel Mixtures for Probabilistic Deep Learning. (arXiv:2305.18204v1 [cs.LG])

    [http://arxiv.org/abs/2305.18204](http://arxiv.org/abs/2305.18204)

    本文提出了一种量子核混合方法，可以用于表示连续和离散随机变量的联合概率分布。该框架允许构建可微分的模型，适用于密度估计、推理和采样，以及各种机器学习任务，包括生成建模和判别学习。

    

    本文提出了一种新的概率深度学习方法——量子核混合，它是从量子密度矩阵的数学形式中推导出来的。该方法提供了一种简单而有效的机制，用于表示连续和离散随机变量的联合概率分布。该框架允许构建可微分的模型，用于密度估计、推理和采样，从而能够整合到端到端的深度神经模型中。通过这样做，我们提供了一种多功能的边际和联合概率分布表示，可以开发一种可微分的、组合的和可逆的推理过程，涵盖了广泛的机器学习任务，包括密度估计、判别学习和生成建模。我们通过两个示例来说明该框架的广泛适用性：一个图像分类模型，它可以自然地转化为条件生成模型，得益于量子核混合的表示能力。

    This paper presents a novel approach to probabilistic deep learning (PDL), quantum kernel mixtures, derived from the mathematical formalism of quantum density matrices, which provides a simpler yet effective mechanism for representing joint probability distributions of both continuous and discrete random variables. The framework allows for the construction of differentiable models for density estimation, inference, and sampling, enabling integration into end-to-end deep neural models. In doing so, we provide a versatile representation of marginal and joint probability distributions that allows us to develop a differentiable, compositional, and reversible inference procedure that covers a wide range of machine learning tasks, including density estimation, discriminative learning, and generative modeling. We illustrate the broad applicability of the framework with two examples: an image classification model, which can be naturally transformed into a conditional generative model thanks to
    
[^94]: 关于纯16位浮点神经网络的辩护

    In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])

    [http://arxiv.org/abs/2305.10947](http://arxiv.org/abs/2305.10947)

    本文探讨了纯16位浮点神经网络的被忽视的效率，提供了理论分析来探讨16位和32位模型的差异，并可以定量解释16位模型与其32位对应物之间的条件。

    

    减少编码神经网络权重和激活所需的位数是非常可取的，因为它可以加快神经网络的训练和推理时间，同时减少内存消耗。因此，这一领域的研究引起了广泛关注，以开发利用更低精度计算的神经网络，比如混合精度训练。有趣的是，目前不存在纯16位浮点设置的方法。本文揭示了纯16位浮点神经网络被忽视的效率。我们通过提供全面的理论分析来探讨造成16位和32位模型的差异的因素。我们规范化了浮点误差和容忍度的概念，从而可以定量解释16位模型与其32位对应物之间密切逼近结果的条件。这种理论探索提供了新的视角。

    Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
    
[^95]: 基于局部Lipschitz度量的深度学习图像重建的不确定性估计

    Uncertainty Estimation for Deep Learning Image Reconstruction using a Local Lipschitz Metric. (arXiv:2305.07618v1 [cs.CV])

    [http://arxiv.org/abs/2305.07618](http://arxiv.org/abs/2305.07618)

    本文提出了一种基于局部Lipschitz度量的方法，可以用于估计深度学习图像重建模型的不确定性。该方法可用于确定特定深度学习重建方法的适用性，识别分布外的测试样本并指导适当的数据增强。

    

    深度学习在医学成像中广泛应用于解决与成像相关的反问题，尤其是图像重建。在模型部署时，可能会遇到与训练数据差异较大的输入分布，部分原因是数据偏差或漂移。本文提出一种基于从单个训练模型中确定的局部Lipschitz度量的方法，可以用于估计图像重建的模型不确定性。我们展示了局部Lipschitz值与平均绝对误差之间的单调关系，并表明可以使用此方法提供确定是否适合特定深度学习重建方法的阈值。我们的不确定性估计方法可用于识别分布外的测试样本，关联关于认识不确定性的信息，并指导适当的数据增强。在医学成像应用中特别重要的是，量化学习重构方法的不确定性，因为诊断和治疗可能会受到重建图像的准确性和可靠性的影响。

    The use of deep learning approaches for image reconstruction is of contemporary interest in radiology, especially for approaches that solve inverse problems associated with imaging. In deployment, these models may be exposed to input distributions that are widely shifted from training data, due in part to data biases or drifts. We propose a metric based on local Lipschitz determined from a single trained model that can be used to estimate the model uncertainty for image reconstructions. We demonstrate a monotonic relationship between the local Lipschitz value and Mean Absolute Error and show that this method can be used to provide a threshold that determines whether a given DL reconstruction approach was well suited to the task. Our uncertainty estimation method can be used to identify out-of-distribution test samples, relate information regarding epistemic uncertainties, and guide proper data augmentation. Quantifying uncertainty of learned reconstruction approaches is especially pert
    
[^96]: PGB：用于异构网络表示学习的PubMed图数据集基准

    PGB: A PubMed Graph Benchmark for Heterogeneous Network Representation Learning. (arXiv:2305.02691v1 [cs.LG])

    [http://arxiv.org/abs/2305.02691](http://arxiv.org/abs/2305.02691)

    介绍了一个基于PubMed数据库的PGB基准数据集，用于评估生物医学文献的异构图嵌入。该数据集包含丰富的元数据和来自不同数据集的21个系统性评价主题的评估任务。

    

    生物医学文献的数量快速增长，但是捕捉这些文章的文献信息的异质性仍然相对较少研究。尽管通过异构图神经网络的图挖掘研究已经成为研究热点，但它们是否捕捉到了PubMed数据库的异质性仍不清楚，而这是一个包含超过3300万篇文章的庞大数字资料库。我们介绍了PubMed Graph Benchmark（PGB），这是一个用于评估生物医学文献异构图嵌入的新的基准数据集。PGB是迄今为止最大的异构网络之一，包含3000万篇英文文章。该基准数据集包含丰富的元数据，包括摘要、作者、引用、MeSH术语、MeSH层次结构和其他一些信息。基准数据集包含来自3个不同数据集的21个系统性评价主题的评估任务。在PGB中，我们将与PubMed生物医学文章相关的元数据聚合成一致的。

    There has been a rapid growth in biomedical literature, yet capturing the heterogeneity of the bibliographic information of these articles remains relatively understudied. Although graph mining research via heterogeneous graph neural networks has taken center stage, it remains unclear whether these approaches capture the heterogeneity of the PubMed database, a vast digital repository containing over 33 million articles. We introduce PubMed Graph Benchmark (PGB), a new benchmark dataset for evaluating heterogeneous graph embeddings for biomedical literature. PGB is one of the largest heterogeneous networks to date and consists of 30 million English articles. The benchmark contains rich metadata including abstract, authors, citations, MeSH terms, MeSH hierarchy, and some other information. The benchmark contains an evaluation task of 21 systematic reviews topics from 3 different datasets. In PGB, we aggregate the metadata associated with the biomedical articles from PubMed into a unified
    
[^97]: 关于多个吸引子非线性系统的提升和重构

    On the lifting and reconstruction of nonlinear systems with multiple attractors. (arXiv:2304.11860v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2304.11860](http://arxiv.org/abs/2304.11860)

    本文研究了具有多个吸引子的非线性系统的Koopman算子提升和重构机制，通过利用吸引域之间的固有对称性，只需三个自由度的线性重构就可以全局线性化系统。

    

    Koopman算子通过关注不变子空间中的观测量的演化，提供了非线性动力学的线性视角。感兴趣的观测量通常是从Koopman特征函数线性重构出来的。尽管Koopman算子在过去几年广泛使用，但对于具有多个稳定点的动力系统，关于Koopman算子的适用性存在一些误解。本研究解释了具有多个吸引子的非线性系统的Koopman算子提升机制。通过考虑Duffing振荡器的例子，我们表明通过利用吸引域之间的固有对称性，Koopman可观测空间中具有三个自由度的线性重构就足以全局线性化系统。

    The Koopman operator provides a linear perspective on non-linear dynamics by focusing on the evolution of observables in an invariant subspace. Observables of interest are typically linearly reconstructed from the Koopman eigenfunctions. Despite the broad use of Koopman operators over the past few years, there exist some misconceptions about the applicability of Koopman operators to dynamical systems with more than one fixed point. In this work, an explanation is provided for the mechanism of lifting for the Koopman operator of nonlinear systems with multiple attractors. Considering the example of the Duffing oscillator, we show that by exploiting the inherent symmetry between the basins of attraction, a linear reconstruction with three degrees of freedom in the Koopman observable space is sufficient to globally linearize the system.
    
[^98]: 粗糙的种族数据掩盖了临床风险评分表现的差异

    Coarse race data conceals disparities in clinical risk score performance. (arXiv:2304.09270v1 [cs.CY])

    [http://arxiv.org/abs/2304.09270](http://arxiv.org/abs/2304.09270)

    研究发现仅依赖粗糙的种族类别可能掩盖了临床风险评分表现中的重要差异，需要更精细的种族数据采集。

    

    美国的医疗保健数据通常只记录病人的粗略种族组：例如，印度和中国病人通常都被编码为“亚洲人”。然而，目前还不清楚这种粗略编码是否掩盖了精细种族组之间的临床风险评分表现的显著差异。本文利用418K紧急科室就诊的数据，评估了三种结局、五种风险评分和四种表现指标的精细种族组之间的临床风险评分表现差异。在各种结局和指标中，我们表明，粗略种族类别内存在重要的表现差异。事实上，在粗略类别内，性能指标的变异常常超过粗略类别之间的变异。我们探讨了这些差异的原因，发现结局率、特征分布以及特征与结果之间的关系在不同精细种族类别之间都有显着差异。我们的结果表明，仅依赖粗糙的种族类别可能掩盖了临床风险评分表现中的重要差异，并强调了医疗保健环境中需要收集更精细的种族数据。

    Healthcare data in the United States often records only a patient's coarse race group: for example, both Indian and Chinese patients are typically coded as ``Asian.'' It is unknown, however, whether this coarse coding conceals meaningful disparities in the performance of clinical risk scores across granular race groups. Here we show that it does. Using data from 418K emergency department visits, we assess clinical risk score performance disparities across granular race groups for three outcomes, five risk scores, and four performance metrics. Across outcomes and metrics, we show that there are significant granular disparities in performance within coarse race categories. In fact, variation in performance metrics within coarse groups often exceeds the variation between coarse groups. We explore why these disparities arise, finding that outcome rates, feature distributions, and the relationships between features and outcomes all vary significantly across granular race categories. Our res
    
[^99]: 面向医学图像分类中标签集不匹配的规模化联邦学习

    Scale Federated Learning for Label Set Mismatch in Medical Image Classification. (arXiv:2304.06931v1 [cs.CV])

    [http://arxiv.org/abs/2304.06931](http://arxiv.org/abs/2304.06931)

    本文提出了FedLSM框架以解决医学图像分类中标签集不匹配问题，该框架采用不同的训练策略以有效利用未标记或部分标记的数据，并在分类层采用逐类别自适应聚合，适用于多个标签集的场景。

    

    联邦学习（FL）作为一种去中心化的学习范式，在医疗保健领域得到了广泛应用，允许多个参与方在不泄露隐私的前提下协同训练模型。然而，以往的研究大多假定每个客户端拥有相同的标签集。事实上，医学专家往往只注释其知识领域或兴趣范围内的疾病。这意味着每个客户端的标签集可能是不同甚至是不相交的。本文提出了FedLSM框架以解决标签集不匹配问题。FedLSM采用不同的训练策略处理不同不确定性程度的数据，以有效利用未标记或部分标记的数据，并在分类层采用逐类别自适应聚合以避免客户端缺失标签时的不准确聚合。我们在两个公共实际医学图像数据集上评估了FedLSM，包括拥有112,120张胸透图像的胸部X射线诊断和皮肤病变诊断。

    Federated learning (FL) has been introduced to the healthcare domain as a decentralized learning paradigm that allows multiple parties to train a model collaboratively without privacy leakage. However, most previous studies have assumed that every client holds an identical label set. In reality, medical specialists tend to annotate only diseases within their knowledge domain or interest. This implies that label sets in each client can be different and even disjoint. In this paper, we propose the framework FedLSM to solve the problem Label Set Mismatch. FedLSM adopts different training strategies on data with different uncertainty levels to efficiently utilize unlabeled or partially labeled data as well as class-wise adaptive aggregation in the classification layer to avoid inaccurate aggregation when clients have missing labels. We evaluate FedLSM on two public real-world medical image datasets, including chest x-ray (CXR) diagnosis with 112,120 CXR images and skin lesion diagnosis wit
    
[^100]: 一种使用确定性目标的黑匣子变分推断：更快，更精确，更黑。

    Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box. (arXiv:2304.05527v1 [cs.LG])

    [http://arxiv.org/abs/2304.05527](http://arxiv.org/abs/2304.05527)

    本文提出了“确定性ADVI”（DADVI），它用一种固定的蒙特卡罗近似替换了均值场变分贝叶斯（MFVB）的不可解目标，可以使用现成的二阶优化，适用于更准确的后验线性响应（LR）协方差估计，在某些常见的统计问题类别上效果更好。

    

    自动微分变分推断（ADVI）提供了多种现代概率编程语言中快速易用的后验近似方法。然而它的随机优化器缺乏明确的收敛标准，并且需要调整参数。此外，ADVI继承了均值场变分贝叶斯（MFVB）的较差后验不确定性估计。我们引入了“确定性ADVI”（DADVI）来解决这些问题。DADVI用固定的蒙特卡罗近似替换了MFVB的不可解目标，这一技术在随机优化文献中被称为“样本平均近似”（SAA）。通过优化近似但确定的目标，DADVI可以使用现成的二阶优化，而且与标准均值场ADVI不同的是，可以适用于更准确的后验线性响应（LR）协方差估计。与现有的最坏情况理论相反，我们表明，在某些常见的统计问题类别上，DADVI和SAA可以表现得更好。

    Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce ``deterministic ADVI'' (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the ``sample average approximation'' (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior linear response (LR) covariance estimates. In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform 
    
[^101]: StepMix: 一个用于外部变量广义混合模型的伪似然估计的Python包

    StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])

    [http://arxiv.org/abs/2304.03853](http://arxiv.org/abs/2304.03853)

    StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。

    

    StepMix是一个用于广义有限混合模型(潜在剖面和潜在类分析)与外部变量(协变量和远程结果)的伪似然估计(单步、两步和三步方法)的开源软件包。在许多社会科学的应用中，主要目标不仅是将个体聚类成潜在类别，还包括使用这些类别来开发更复杂的统计模型。这些模型通常分为一个将潜在类别与观察指标相关联的测量模型和一个将协变量和结果变量与潜在类别相关联的结构模型。测量和结构模型可以使用所谓的一步法共同估计，也可以使用逐步方法逐步估计，对于从业人员来说，这些方法在估计潜在类别的可解释性方面具有显著优势。除了一步法，StepMix还实现了文献中提出的最重要的逐步估计方法，提供了用户友好的界面，方便模型的估计、选择和解释。

    StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
    
[^102]: 基于超轻量级二值神经网络的心律失常分类器

    Arrhythmia Classifier Based on Ultra-Lightweight Binary Neural Network. (arXiv:2304.01568v1 [eess.SP])

    [http://arxiv.org/abs/2304.01568](http://arxiv.org/abs/2304.01568)

    本研究提出了一种能够通过ECG信号进行5类和17类心律失常分类的超轻量级二值神经网络(BNN)，在存储使用率最低的情况下，分别达到了96.90%和97.50%的准确率。

    

    合理有效地通过心电图(ECG)信号监测心律失常对人类健康具有重要意义。随着深度学习的发展，出现了许多基于深度学习的ECG分类算法。但是，大多数现有算法在高精度和复杂模型之间进行权衡，导致存储使用率和功耗很高。这也不可避免地增加了在资源有限的可穿戴人工智能物联网设备上实现的难度。本研究提出了一种通用的超轻量级二值神经网络(BNN)，能够通过ECG信号进行5类和17类心律失常分类。我们的BNN在5类和17类分类中分别达到了96.90% (完全精度97.09%)和97.50% (完全精度98.00%)的准确率，存储使用率最低(3.76 KB和4.45 KB)。与其他二值化作品相比，我们的方法在支持两个多分类方面表现出色。

    Reasonably and effectively monitoring arrhythmias through ECG signals has significant implications for human health. With the development of deep learning, numerous ECG classification algorithms based on deep learning have emerged. However, most existing algorithms trade off high accuracy for complex models, resulting in high storage usage and power consumption. This also inevitably increases the difficulty of implementation on wearable Artificial Intelligence-of-Things (AIoT) devices with limited resources. In this study, we proposed a universally applicable ultra-lightweight binary neural network(BNN) that is capable of 5-class and 17-class arrhythmia classification based on ECG signals. Our BNN achieves 96.90% (full precision 97.09%) and 97.50% (full precision 98.00%) accuracy for 5-class and 17-class classification, respectively, with state-of-the-art storage usage (3.76 KB and 4.45 KB). Compared to other binarization works, our approach excels in supporting two multi-classificatio
    
[^103]: DRIP: 逆问题的深度正则化器

    DRIP: Deep Regularizers for Inverse Problems. (arXiv:2304.00015v1 [cs.LG])

    [http://arxiv.org/abs/2304.00015](http://arxiv.org/abs/2304.00015)

    提出了一种基于变分法的深度神经正则化器家族，保证可以适配数据并解决逆问题。在图像去模糊和小角度层析成像等问题上可行。

    

    逆问题在数学上是不良定义的，因此对于一些（带有噪声的）数据，可能会有不止一个与数据匹配的解。近年来，一些能够找到最合适解决方案的深度神经技术得到了发展，但它们存在一些缺点：大多数技术无法保证解决方案能够在推理时匹配数据；虽然这些技术的推导是基于一个有效的标量正则化函数存在的基础之上，但在实际运用中这些技术并没有依赖于这样一个函数，因此与传统的变分技术有所偏离。本文提出了一种新的神经正则化器家族来解决逆问题，这些正则化器基于变分形式，并保证适配数据。我们演示了它们在一些高度ill-posed问题上的使用，包括图像去模糊和小角度层析成像。

    Inverse problems are mathematically ill-posed. Thus, given some (noisy) data, there is more than one solution that fits the data. In recent years, deep neural techniques that find the most appropriate solution, in the sense that it contains a-priori information, were developed. However, they suffer from several shortcomings. First, most techniques cannot guarantee that the solution fits the data at inference. Second, while the derivation of the techniques is inspired by the existence of a valid scalar regularization function, such techniques do not in practice rely on such a function, and therefore veer away from classical variational techniques. In this work we introduce a new family of neural regularizers for the solution of inverse problems. These regularizers are based on a variational formulation and are guaranteed to fit the data. We demonstrate their use on a number of highly ill-posed problems, from image deblurring to limited angle tomography.
    
[^104]: 神经网络中的因果归因学习：超越直接影响

    Learning Causal Attributions in Neural Networks: Beyond Direct Effects. (arXiv:2303.13850v1 [cs.LG])

    [http://arxiv.org/abs/2303.13850](http://arxiv.org/abs/2303.13850)

    本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。

    

    近年来，捕捉和维护神经网络模型中的因果关系引起了越来越多的关注。本文研究了用因果方法估计和维护神经网络模型中的输入-输出属性。特别地，现有的研究仅假设输入变量独立（由于神经网络架构），因此仅研究直接影响。我们将神经网络视为结构性因果模型，并提出在输入特征中引入边缘以捕捉和维护直接和间接因果效应的简单而有效的方法，同时训练神经网络模型。我们还提出有效的近似策略来量化高维数据的因果归因。我们在合成和真实数据集上进行了各种实验，结果表明，所提出的方法学习了接近基本事实效果的直接和间接因果归因。

    There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. We also propose effective approximation strategies to quantify causal attributions in high dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
    
[^105]: 图上随机逆问题：分布式在线学习

    Random Inverse Problems Over Graphs: Decentralized Online Learning. (arXiv:2303.11789v1 [cs.LG])

    [http://arxiv.org/abs/2303.11789](http://arxiv.org/abs/2303.11789)

    本文提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来，并发展了一种新的L2-渐近稳定性理论。该算法在网络图为连通且正向算子序列满足无限维度时空励磁条件的情况下，能够实现均方和几乎必然的强一致估计。

    

    我们建立了一个随机逆问题的框架，该问题具有实时的图上观测，并提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来。我们将算法收敛性转化为带有L2有界鞅差分项的希尔伯特空间中随机时变差分方程的渐近稳定性，并发展了L2-渐近稳定性理论。结果表明，如果网络图是连通的，并且正向算子序列满足无限维度时空励磁条件，则所有节点的估计均为均方和几乎必然的强一致的。通过将RKHS中的分布式学习问题等效地转化为图上随机逆问题，我们提出了一种基于无中心节点的RKHS分布式在线学习算法。

    We establish a framework of random inverse problems with real-time observations over graphs, and present a decentralized online learning algorithm based on online data streams, which unifies the distributed parameter estimation in Hilbert space and the least mean square problem in reproducing kernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into the asymptotic stability of randomly time-varying difference equations in Hilbert space with L2-bounded martingale difference terms and develop the L2 -asymptotic stability theory. It is shown that if the network graph is connected and the sequence of forward operators satisfies the infinitedimensional spatio-temporal persistence of excitation condition, then the estimates of all nodes are mean square and almost surely strongly consistent. By equivalently transferring the distributed learning problem in RKHS to the random inverse problem over graphs, we propose a decentralized online learning algorithm in RKHS based on no
    
[^106]: 预训练GAN和VAE的特征消除

    Feature Unlearning for Pre-trained GANs and VAEs. (arXiv:2303.05699v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05699](http://arxiv.org/abs/2303.05699)

    本文提出了一种从预训练的GAN和VAE模型中消除特定特征的方法，并通过实验证明了方法的有效性。

    

    我们解决了从预训练的图像生成模型（GAN和VAE）中消除特征的问题。与常见的消除任务不同，我们的目标是从预训练的生成模型中消除特定的特征，例如面部图像中的发型。由于目标特征仅出现在图像的局部区域中，从预训练模型中消除整个图像可能导致失去图像剩余区域中的其他细节。为了指定要消除的特征，我们收集包含目标特征的随机生成图像。然后，我们识别与目标特征对应的潜在表示，并使用表示来微调预训练模型。通过对MNIST和CelebA数据集进行实验，我们展示了成功删除目标特征同时保持原始模型的可信度。进一步的对抗性攻击实验证明了消除后的模型的鲁棒性。

    We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST and CelebA datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is mo
    
[^107]: PDSketch: 集成规划领域编程和学习

    PDSketch: Integrated Planning Domain Programming and Learning. (arXiv:2303.05501v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.05501](http://arxiv.org/abs/2303.05501)

    本文通过PDSketch语言和可训练的神经网络，实现了模型的学习和在线规划，加速了机器人的灵活性和通用性。

    

    本文研究了一种模型学习和在线规划方法，以构建灵活和通用的机器人。具体而言，我们研究了如何利用底层环境转换模型中的局部性和稀疏结构，以提高模型泛化能力、数据效率和运行效率。我们提出了一种新的域定义语言，名为PDSketch。它允许用户灵活地定义转换模型中的高级结构，例如对象和特征的依赖关系，类似于程序员使用TensorFlow或PyTorch指定卷积神经网络的核大小和隐藏维度的方式。转换模型的细节将由可训练的神经网络填充。基于定义的结构和学习参数，PDSketch自动生成与域无关的规划启发式算法，无需额外的训练。衍生的启发式算法加速了对新目标的规划性能。

    This paper studies a model learning and online planning approach towards building flexible and general robots. Specifically, we investigate how to exploit the locality and sparsity structures in the underlying environmental transition model to improve model generalization, data-efficiency, and runtime-efficiency. We present a new domain definition language, named PDSketch. It allows users to flexibly define high-level structures in the transition models, such as object and feature dependencies, in a way similar to how programmers use TensorFlow or PyTorch to specify kernel sizes and hidden dimensions of a convolutional neural network. The details of the transition model will be filled in by trainable neural networks. Based on the defined structures and learned parameters, PDSketch automatically generates domain-independent planning heuristics without additional training. The derived heuristics accelerate the performance-time planning for novel goals.
    
[^108]: 通过半监督顺序变分贝叶斯框架实现软机器人的跨域迁移学习和状态推断

    Cross-domain Transfer Learning and State Inference for Soft Robots via a Semi-supervised Sequential Variational Bayes Framework. (arXiv:2303.01693v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.01693](http://arxiv.org/abs/2303.01693)

    本文提出了一个半监督顺序变分贝叶斯框架，用于解决软机器人领域的跨域迁移学习和状态推断问题。该框架可以处理某些机器人配置下存在缺失状态标签的情况，同时引入了特征空间迁移策略，提高了在多个配置下的潜在特征的适应性。

    

    最近，基于数据驱动模型（如深度神经网络）的软机器人建模和状态推断显示出了很大的潜力。然而，深度模型需要大量的数据才能有效地运行，这需要进行详尽和质量良好的数据采集，尤其是状态标签的采集。因此，由于软机器人的传感器化困难和在非结构化环境中收集数据的不便等原因，获取标注的软机器人系统状态数据存在挑战。为了解决这个挑战，本文提出了一个半监督顺序变分贝叶斯（DSVB）框架，用于处理某些机器人配置中存在缺失状态标签的软机器人的迁移学习和状态推断。考虑到软机器人在不同的机器人配置下可能展现出不同的动力学特性，我们还引入了特征空间迁移策略，以促进在多个配置下的潜在特征的适应。

    Recently, data-driven models such as deep neural networks have shown to be promising tools for modelling and state inference in soft robots. However, voluminous amounts of data are necessary for deep models to perform effectively, which requires exhaustive and quality data collection, particularly of state labels. Consequently, obtaining labelled state data for soft robotic systems is challenged for various reasons, including difficulty in the sensorization of soft robots and the inconvenience of collecting data in unstructured environments. To address this challenge, in this paper, we propose a semi-supervised sequential variational Bayes (DSVB) framework for transfer learning and state inference in soft robots with missing state labels on certain robot configurations. Considering that soft robots may exhibit distinct dynamics under different robot configurations, a feature space transfer strategy is also incorporated to promote the adaptation of latent features across multiple config
    
[^109]: 通过离线强化学习从观察中学习控制自主机群

    Learning to Control Autonomous Fleets from Observation via Offline Reinforcement Learning. (arXiv:2302.14833v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2302.14833](http://arxiv.org/abs/2302.14833)

    本文提出了通过离线强化学习从观察中学习控制自主机群，并利用离线数据学习有效控制策略的方法。通过在真实出行系统数据上的实证研究，展示了离线学习恢复AMoD控制策略的能力。

    

    自主移动出行（AMoD）系统是一种不断发展的交通方式，其中由中央协调的自动驾驶车辆组成的车队动态地提供出行服务。这些系统的控制通常被形式化为一个大规模网络优化问题，而强化学习（RL）最近被提出作为解决该领域的挑战的有希望的方法。最近的集中式RL方法关注在线学习数据，忽视了实际交通系统中每个样本交互的成本。为了解决这些限制，我们提出通过离线强化学习的视角来形式化AMoD系统的控制，并仅使用可用于当前出行运营商的离线数据学习有效的控制策略。我们进一步研究设计决策，并基于真实出行系统的数据提供了实证证据，展示了离线学习如何恢复AMoD控制策略。

    Autonomous Mobility-on-Demand (AMoD) systems are an evolving mode of transportation in which a centrally coordinated fleet of self-driving vehicles dynamically serves travel requests. The control of these systems is typically formulated as a large network optimization problem, and reinforcement learning (RL) has recently emerged as a promising approach to solve the open challenges in this space. Recent centralized RL approaches focus on learning from online data, ignoring the per-sample-cost of interactions within real-world transportation systems. To address these limitations, we propose to formalize the control of AMoD systems through the lens of offline reinforcement learning and learn effective control strategies using solely offline data, which is readily available to current mobility operators. We further investigate design decisions and provide empirical evidence based on data from real-world mobility systems showing how offline learning allows to recover AMoD control policies t
    
[^110]: 基于树模型的边际特征归因研究

    On marginal feature attributions of tree-based models. (arXiv:2302.08434v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08434](http://arxiv.org/abs/2302.08434)

    该论文讨论了基于树模型的边际特征归因方法，与流行的TreeSHAP算法相比，边际Shapley值在相同函数的情况下保持一致，并且介绍了如何利用树模型的内部结构计算边际特征归因。

    

    由于其强大和易于使用的特点，随机森林和梯度提升树集成等基于树的机器学习模型变得非常流行。为了解释这些模型，可以使用基于边际期望的局部特征归因方法，例如边际（干预）Shapley、Owen或Banzhaf值。这些方法对模型真实且实现不变，即仅依赖于模型的输入输出函数。通过提供两个（具有相似统计性质的）决策树来对比这一点，这两个决策树计算完全相同的函数，但“路径相关”的TreeSHAP方法给出了不同的特征排序，而边际Shapley值重合。此外，我们讨论了如何利用基于树模型的内部结构来帮助计算它们的边际特征归因，以得到线性博弈值。一个重要的观察是，这些函数在某个常数区间内是简单的（分段常数）函数。

    Due to their power and ease of use, tree-based machine learning models, such as random forests and gradient-boosted tree ensembles, have become very popular. To interpret them, local feature attributions based on marginal expectations, e.g. marginal (interventional) Shapley, Owen or Banzhaf values, may be employed. Such methods are true to the model and implementation invariant, i.e. dependent only on the input-output function of the model. We contrast this with the popular TreeSHAP algorithm by presenting two (statistically similar) decision trees that compute the exact same function for which the "path-dependent" TreeSHAP yields different rankings of features, whereas the marginal Shapley values coincide. Furthermore, we discuss how the internal structure of tree-based models may be leveraged to help with computing their marginal feature attributions according to a linear game value. One important observation is that these are simple (piecewise-constant) functions with respect to a c
    
[^111]: LExecutor: 学习引导的执行

    LExecutor: Learning-Guided Execution. (arXiv:2302.02343v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2302.02343](http://arxiv.org/abs/2302.02343)

    LExecutor是一个学习引导的执行方法，通过让神经模型预测缺失值，并将其注入到执行中，可以以自由约束的方式执行任意代码片段。该方法在Python代码和从Stack Overflow提取的代码片段上表现良好。

    

    执行代码对于各种程序分析任务是必不可少的，例如通过异常检测错误或获取执行跟踪以进行进一步的动态分析。然而，在实践中执行任意代码片段通常很困难，例如由于缺少变量定义、缺少用户输入和缺少第三方依赖。本文介绍了LExecutor，一种学习引导的方法，用于以自由约束的方式执行任意代码片段。关键思想是让神经模型预测否则会导致程序停滞的缺失值，并将这些值注入到执行中。例如，LExecutor为未定义的变量注入可能的值，并为缺失的函数调用返回预测可能的返回值。我们在来自流行开源项目的Python代码和从Stack Overflow中提取的代码片段上评估了该方法。神经模型以79.5%到98.2%的准确率预测真实值。

    Executing code is essential for various program analysis tasks, e.g., to detect bugs that manifest through exceptions or to obtain execution traces for further dynamic analysis. However, executing an arbitrary piece of code is often difficult in practice, e.g., because of missing variable definitions, missing user inputs, and missing third-party dependencies. This paper presents LExecutor, a learning-guided approach for executing arbitrary code snippets in an underconstrained way. The key idea is to let a neural model predict missing values that otherwise would cause the program to get stuck, and to inject these values into the execution. For example, LExecutor injects likely values for otherwise undefined variables and likely return values of calls to otherwise missing functions. We evaluate the approach on Python code from popular open-source projects and on code snippets extracted from Stack Overflow. The neural model predicts realistic values with an accuracy between 79.5% and 98.2
    
[^112]: NeuRI：通过归纳规则推断实现DNN生成的多样化

    NeuRI: Diversifying DNN Generation via Inductive Rule Inference. (arXiv:2302.02261v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2302.02261](http://arxiv.org/abs/2302.02261)

    NeuRI是一种全自动化生成由数百种操作符组成的有效且多样化的DL模型的方法。它通过归纳式程序合成推断操作符约束条件，并采用符号和具体操作的混合模型生成。

    

    深度学习(DL)在各个行业中被广泛应用于改善决策和自动化流程，其推动力来自不断发展的DL库和编译器。DL系统的正确性对于信任DL应用非常重要。因此，最近的研究浪潮一直在研究用于模糊DL系统的自动化测试用例合成（即DNN模型和其输入）。然而，现有的模型生成器只涵盖了有限数量的操作符，缺乏广泛建模操作符约束的能力。为了解决这个挑战，我们提出了NeuRI，一种全自动生成由数百种操作符组成的有效且多样化的DL模型的方法。NeuRI采用了三步过程：(i)从各种来源收集有效和无效的API追踪；(ii)在追踪数据上应用归纳式程序合成，推断构建有效模型的约束条件；(iii)通过结合符号和具体操作执行混合模型生成。

    Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications. As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints. To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process: (i) collecting valid and invalid API traces from various sources; (ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and (iii) performing hybrid model generation by incorporating both symbolic and concrete ope
    
[^113]: 基于向量化的场景描述和运动预测的场景测试

    Vectorized Scenario Description and Motion Prediction for Scenario-Based Testing. (arXiv:2302.01161v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01161](http://arxiv.org/abs/2302.01161)

    本论文提出了一种基于向量化的场景描述和运动预测的场景测试方法，通过合并不同场景的数据并利用向量化场景描述中的空间和时间细微差别，可以更准确地预测自动化车辆在未见过场景下的轨迹。

    

    自动化车辆(AVs)通常在多样化的场景中进行测试，这些场景通常由速度、距离或曲线半径等参数来确定。为了能够统一描述这些场景，独立于这些参数，本文提出了一个基于道路几何和车辆轨迹的向量化场景描述方法。利用这种形式的数据，我们生成了三个场景的数据，并将它们合并起来用于训练运动预测模型VectorNet，从而可以预测未见过场景下AV的轨迹。通过预测场景评估指标，VectorNet在一定程度上比单独处理三个场景数据的回归模型具有较低的误差。然而，为了全面的泛化，必须确保训练数据中具有足够的变化。因此，与现有方法不同，我们提出的方法可以合并不同场景的数据并利用向量化场景描述中的空间和时间细微差别。这样一来，我们可以使用指定的测试场景和真实场景的数据。

    Automated vehicles (AVs) are tested in diverse scenarios, typically specified by parameters such as velocities, distances, or curve radii. To describe scenarios uniformly independent of such parameters, this paper proposes a vectorized scenario description defined by the road geometry and vehicles' trajectories. Data of this form are generated for three scenarios, merged, and used to train the motion prediction model VectorNet, allowing to predict an AV's trajectory for unseen scenarios. Predicting scenario evaluation metrics, VectorNet partially achieves lower errors than regression models that separately process the three scenarios' data. However, for comprehensive generalization, sufficient variance in the training data must be ensured. Thus, contrary to existing methods, our proposed method can merge diverse scenarios' data and exploit spatial and temporal nuances in the vectorized scenario description. As a result, data from specified test scenarios and real-world scenarios can be
    
[^114]: 内部奖励的强化学习

    Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00270](http://arxiv.org/abs/2302.00270)

    这项研究探讨了一类强化学习问题，其中策略的奖励信号由与之相关且同时优化的判别器生成，导致学习过程不稳定。实验结果表明，修剪线性奖励函数可以稳定训练过程。

    

    我们研究了一类强化学习问题，其中用于策略学习的奖励信号由一个与策略相关且与策略同时优化的判别器生成。策略和判别器之间的相互依赖导致了不稳定的学习过程，因为来自不成熟判别器的奖励信号是嘈杂的，阻碍了策略的学习；反过来，未经优化的策略也会阻碍判别器的学习。我们将这种学习设置称为“内部奖励的强化学习”（IRRL），因为奖励不是直接来自环境，而是由判别器“内部”提供的。在本文中，我们正式地表述了IRRL，并提出了一类属于IRRL的问题。我们从理论上推导并经验性地分析了IRRL中奖励函数的影响，并基于这些分析提出了修剪线性奖励函数。实验结果表明，所提出的奖励函数可以持续稳定训练过程。

    We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the tra
    
[^115]: 电子商务用户对交付选择的偏好

    E-commerce users' preferences for delivery options. (arXiv:2301.00666v2 [econ.GN] UPDATED)

    [http://arxiv.org/abs/2301.00666](http://arxiv.org/abs/2301.00666)

    研究通过设计陈述选择调查，调查了电子商务用户对交付选择的偏好，发现交付方式的费用、时间和时间段大小是用户选择的重要决定因素，同时也发现用户的偏好与年龄、性别、远程办公频率和是否有快递柜等社会人口特征有关。

    

    许多电子商务市场为了满足用户日益增长的需求，提供免费的快速交付选项，这给城市物流带来了过重的负担。因此，了解电子商务用户对交付选择的偏好是设计物流政策的关键。为此，本研究设计了一项陈述选择调查，受访者面临不同交付选择和时间段之间的选择任务，在日本三个主要都市地区的4062名用户完成了调查。为了分析数据，我们估计了捕捉品味异质性以及灵活替代模式的混合逻辑模型。模型估计结果表明，包括费用、时间和时间段大小在内的交付属性是交付选择的重要决定因素。还提出了用户偏好与年龄、性别、远程办公频率和是否有快递柜等社会人口特征之间的关联。

    Many e-commerce marketplaces offer their users fast delivery options for free to meet the increasing needs of users, imposing an excessive burden on city logistics. Therefore, understanding e-commerce users' preference for delivery options is a key to designing logistics policies. To this end, this study designs a stated choice survey in which respondents are faced with choice tasks among different delivery options and time slots, which was completed by 4,062 users from the three major metropolitan areas in Japan. To analyze the data, mixed logit models capturing taste heterogeneity as well as flexible substitution patterns have been estimated. The model estimation results indicate that delivery attributes including fee, time, and time slot size are significant determinants of the delivery option choices. Associations between users' preferences and socio-demographic characteristics, such as age, gender, teleworking frequency and the presence of a delivery box, were also suggested. More
    
[^116]: 机器人中视域外数据的系统层面观点

    A System-Level View on Out-of-Distribution Data in Robotics. (arXiv:2212.14020v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.14020](http://arxiv.org/abs/2212.14020)

    本文旨在揭示机器人中视域外数据的系统层面观点，强调考虑机器人在OOD条件下的整体系统层面的能力，并为未来安全可靠的学习启用自主性研究提出关键研究问题。

    

    当测试条件与训练数据不一致时，所谓的视域外（OOD）输入可能影响现代机器人自主堆栈中的学习组件的可靠性。因此，应对OOD数据是信任学习启用的开放世界自主性的重要挑战。本文旨在揭示与数据驱动机器人系统中OOD数据及其相关挑战有关的主题，并将其与ML社区中关于OOD数据对孤立的学习模型的影响研究的新兴范式联系起来。我们认为，作为机器人学家，我们应该考虑机器人在OOD条件下的整体系统层面的能力。我们强调围绕OOD问题的系统层面观点的关键研究问题，以指导未来安全可靠的学习启用自主性研究。

    When testing conditions differ from those represented in training data, so-called out-of-distribution (OOD) inputs can mar the reliability of learned components in the modern robot autonomy stack. Therefore, coping with OOD data is an important challenge on the path towards trustworthy learning-enabled open-world autonomy. In this paper, we aim to demystify the topic of OOD data and its associated challenges in the context of data-driven robotic systems, drawing connections to emerging paradigms in the ML community that study the effect of OOD data on learned models in isolation. We argue that as roboticists, we should reason about the overall \textit{system-level} competence of a robot as it operates in OOD conditions. We highlight key research questions around this system-level view of OOD problems to guide future research toward safe and reliable learning-enabled autonomy.
    
[^117]: 在未观察到的混淆下，预测算法的鲁棒设计和评估

    Robust Design and Evaluation of Predictive Algorithms under Unobserved Confounding. (arXiv:2212.09844v4 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2212.09844](http://arxiv.org/abs/2212.09844)

    本文提出了一种统一的方法来设计和评估在存在未观察到的混淆数据中的预测算法，通过对预测性能估计量的边界进行去偏倚的机器学习估计，从而解决了预测算法在选择性观察情境中的问题。

    

    预测算法在人类决策者作出选择后选择性地观察到结果的情境中进行重要决策。通常存在着未观察到的混淆因素影响决策者的选择和结果。我们提出了一种统一的方法，用于在存在未观察到的混淆数据下对预测算法进行鲁棒设计和评估。我们的方法对于预测算法的条件平均结果在已观察到的协变量和已识别的干扰参数上可能的变化程度提出了一般性的假设，从而形式化了用于填充缺失数据的常用实证策略，如代理结果和工具变量。我们开发了去偏倚的机器学习估计器，用于对大类预测性能估计量的边界，例如结果的条件似然、预测算法的均方误差、真/假阳性率等等。

    Predictive algorithms inform consequential decisions in settings where the outcome is selectively observed given some choices made by human decision makers. There often exists unobserved confounders that affected the decision maker's choice and the outcome. We propose a unified methodology for the robust design and evaluation of predictive algorithms in selectively observed data under such unobserved confounding. Our approach imposes general assumptions on how much the outcome may vary on average between unselected and selected units conditional on observed covariates and identified nuisance parameters, formalizing popular empirical strategies for imputing missing data such as proxy outcomes and instrumental variables. We develop debiased machine learning estimators for the bounds on a large class of predictive performance estimands, such as the conditional likelihood of the outcome, a predictive algorithm's mean square error, true/false positive rate, and many others, under these assu
    
[^118]: 重新思考预训练网络在无源领域自适应中的作用

    Rethinking the Role of Pre-Trained Networks in Source-Free Domain Adaptation. (arXiv:2212.07585v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.07585](http://arxiv.org/abs/2212.07585)

    本研究提出将预训练网络整合到目标领域自适应中，通过共同学习策略提取有用的目标领域信息，以改善源模型的自适应性能。实验证明这种策略能够成功地提高自适应性能，并与现有的方法集成。

    

    无源领域自适应（SFDA）旨在将在完全标记的源领域上训练的源模型适应到无标签的目标领域。在源训练期间，使用大数据预训练网络来初始化源模型，然后将其丢弃。然而，源训练可能导致模型在源数据分布上过拟合，并丧失适用于目标领域的知识。我们提议将预训练网络整合到目标自适应过程中，因为它具有重要的泛化性能的多样化特征，并提供与源模型不同的特征和分类决策的另一视角。我们提出通过共同学习策略提炼有用的目标领域信息，以改善微调源模型的目标伪标签质量。对4个基准数据集的评估结果显示，我们提出的策略改善了自适应性能，并可以成功地与现有的SFDA方法集成。利用现代预训练网络具有多样化的特征来实现领域自适应具有重要意义。

    Source-free domain adaptation (SFDA) aims to adapt a source model trained on a fully-labeled source domain to an unlabeled target domain. Large-data pre-trained networks are used to initialize source models during source training, and subsequently discarded. However, source training can cause the model to overfit to source data distribution and lose applicable target domain knowledge. We propose to integrate the pre-trained network into the target adaptation process as it has diversified features important for generalization and provides an alternate view of features and classification decisions different from the source model. We propose to distil useful target domain information through a co-learning strategy to improve target pseudolabel quality for finetuning the source model. Evaluation on 4 benchmark datasets show that our proposed strategy improves adaptation performance and can be successfully integrated with existing SFDA methods. Leveraging modern pre-trained networks that ha
    
[^119]: 基于神经网络的凸规则化器用于图像重建

    A Neural-Network-Based Convex Regularizer for Image Reconstruction. (arXiv:2211.12461v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.12461](http://arxiv.org/abs/2211.12461)

    本研究提出了基于神经网络的凸规则化器用于图像重建。该方法通过重新审视由凸脊函数组成的规则化器，并使用具有单个隐藏层的神经网络对其梯度进行参数化。实验证明，在去噪、CT和MRI重建方面，该方法在提供类似可靠性保证的情况下，能够取得更好的结果。

    

    深度学习方法在解决图像重建问题上的出现，使得重建质量有了显著提高。然而，这些新方法往往缺乏可靠性和可解释性，因此人们越来越关注如何在保持性能提升的同时解决这些问题。本研究通过重新审视由凸脊函数组成的规则化器来解决这个问题。这些规则化器的梯度由一个具有单个隐藏层的神经网络参数化，其中包含逐渐增加和可学习的激活函数。该神经网络作为多步高斯去噪器在几分钟内进行训练。去噪、CT和MRI重建的数值实验显示，与提供类似可靠性保证的方法相比，本方法具有改进。

    The emergence of deep-learning-based methods to solve image-reconstruction problems has enabled a significant increase in reconstruction quality. Unfortunately, these new methods often lack reliability and explainability, and there is a growing interest to address these shortcomings while retaining the boost in performance. In this work, we tackle this issue by revisiting regularizers that are the sum of convex-ridge functions. The gradient of such regularizers is parameterized by a neural network that has a single hidden layer with increasing and learnable activation functions. This neural network is trained within a few minutes as a multistep Gaussian denoiser. The numerical experiments for denoising, CT, and MRI reconstruction show improvements over methods that offer similar reliability guarantees.
    
[^120]: 分布式图神经网络训练：一项调查

    Distributed Graph Neural Network Training: A Survey. (arXiv:2211.00216v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00216](http://arxiv.org/abs/2211.00216)

    这项调查研究了分布式图神经网络训练中的挑战，并提出了解决方案来优化特征通信、模型精度和分布式同步。

    

    图神经网络（GNNs）是一种在图上进行训练的深度学习模型，在多个领域取得了成功应用。尽管GNNs的有效性，但是将其扩展到大规模图依然具有挑战性。分布式计算成为训练大规模GNNs的有希望的解决方案，因为它能提供丰富的计算资源。然而，图结构的依赖性使得实现高效的分布式GNN训练变得困难，存在大量的通信和负载不平衡。近年来，人们对分布式GNN训练进行了许多努力，并提出了一系列训练算法和系统。然而，在分布式执行GNN训练的优化技术方面缺乏系统性的综述。本调查分析了分布式GNN训练中的三个主要挑战：大规模特征通信、模型精度损失和分布式同步。

    Graph neural networks (GNNs) are a type of deep learning models that are trained on graphs and have been successfully applied in various domains. Despite the effectiveness of GNNs, it is still challenging for GNNs to efficiently scale to large graphs. As a remedy, distributed computing becomes a promising solution of training large-scale GNNs, since it is able to provide abundant computing resources. However, the dependency of graph structure increases the difficulty of achieving high-efficiency distributed GNN training, which suffers from the massive communication and workload imbalance. In recent years, many efforts have been made on distributed GNN training, and an array of training algorithms and systems have been proposed. Yet, there is a lack of systematic review on the optimization techniques for the distributed execution of GNN training. In this survey, we analyze three major challenges in distributed GNN training that are massive feature communication, the loss of model accura
    
[^121]: 差分隐私扩散模型

    Differentially Private Diffusion Models. (arXiv:2210.09929v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.09929](http://arxiv.org/abs/2210.09929)

    本研究提出了一种差分隐私扩散模型(DPDMs)，通过差分隐私训练生成模型，实现对隐私的保护，在图像生成基准测试中表现优越，能够在标准测试中与特定任务的DP-SGD训练的分类器相媲美。

    

    现代机器学习模型依赖于越来越大的训练数据集，然而在涉及隐私的领域，数据通常是有限的。通过差分隐私训练的生成模型可以绕过这一挑战，提供对合成数据的访问。本文在扩散模型的最新成功基础上，引入了差分隐私扩散模型(DPDMs)，使用差分隐私随机梯度下降(DP-SGD)来实现隐私保护。我们研究了DPDM中的参数化和采样算法，并提出了噪声多样性，这是一个针对DM训练的强大改进。我们在图像生成基准测试中验证了我们的新颖DPDMs，并在所有实验证明了最先进的性能。此外，在标准基准测试中，使用DPDM生成的合成数据训练的分类器表现与特定任务的DP-SGD训练的分类器相当，这在以往的研究中没有被证明。

    While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been dem
    
[^122]: Alzheimer病中的病理学引导分层网络用于亚型识别

    Pathology Steered Stratification Network for Subtype Identification in Alzheimer's Disease. (arXiv:2210.05880v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2210.05880](http://arxiv.org/abs/2210.05880)

    本文提出了一种病理学引导的分层网络（PSSN），通过反应扩散模型将已建立的AD病理学知识与机器学习相结合，预测了阿尔茨海默病患者的长期轨迹。这一方法填补了当前缺失的神经病理学信息。

    

    Alzheimer病(AD)是一种异质性的、多因素引起的神经退行性疾病，以β-淀粉样蛋白、病理性tau蛋白和神经退化为特征。目前对于晚期阿尔茨海默病没有有效的治疗方法，迫切需要早期干预。然而，现有的AD亚型识别的统计推断方法忽视了病理学领域的知识，可能会导致病情不明确且与基本神经学原则不一致的结果。我们提出了一种新的基于病理学引导的分层网络(PSSN)的方法，通过反应扩散模型将已建立的AD病理学知识与机器学习相结合，考虑主要生物标志物之间的非线性相互作用和沿脑结构网络的扩散。通过对纵向多模态神经影像数据进行训练，生物模型可以预测捕捉到个体发展模式的长期轨迹，填补了当前缺失的神经病理学信息。

    Alzheimer's disease (AD) is a heterogeneous, multifactorial neurodegenerative disorder characterized by beta-amyloid, pathologic tau, and neurodegeneration. There are no effective treatments for Alzheimer's disease at a late stage, urging for early intervention. However, existing statistical inference approaches of AD subtype identification ignore the pathological domain knowledge, which could lead to ill-posed results that are sometimes inconsistent with the essential neurological principles. Integrating systems biology modeling with machine learning, we propose a novel pathology steered stratification network (PSSN) that incorporates established domain knowledge in AD pathology through a reaction-diffusion model, where we consider non-linear interactions between major biomarkers and diffusion along brain structural network. Trained on longitudinal multimodal neuroimaging data, the biological model predicts long-term trajectories that capture individual progression pattern, filling in
    
[^123]: 使用基于Transformer的场景表示学习增强强化学习用于自动驾驶决策

    Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving. (arXiv:2208.12263v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12263](http://arxiv.org/abs/2208.12263)

    本研究提出了Scene-Rep Transformer来提升强化学习决策能力，通过改进场景表示编码和顺序预测潜在蒸馏。采用多阶段Transformer编码器建模交互意识和意图意识，并使用顺序潜在Transformer进行自监督学习，加速训练和减少探索空间。

    

    城市自动驾驶的决策是具有挑战性的，由于交通参与者的随机性和道路结构的复杂性。尽管基于强化学习（RL）的决策方案在处理城市驾驶场景方面很有前景，但它的采样效率低且适应性差。在本文中，我们提出了Scene-Rep Transformer来改善RL决策能力，通过更好的场景表示编码和顺序预测潜在蒸馏。具体而言，我们构建了一个多阶段Transformer（MST）编码器，用于建模自车与其邻居之间的交互意识以及代理者与候选路径之间的意图意识。我们采用了一个具有自监督学习目标的顺序潜在Transformer（SLT），将未来的预测信息蒸馏到潜在的场景表示中，以减少探索空间并加快训练。

    Decision-making for urban autonomous driving is challenging due to the stochastic nature of interactive traffic participants and the complexity of road structures. Although reinforcement learning (RL)-based decision-making scheme is promising to handle urban driving scenarios, it suffers from low sample efficiency and poor adaptability. In this paper, we propose Scene-Rep Transformer to improve the RL decision-making capabilities with better scene representation encoding and sequential predictive latent distillation. Specifically, a multi-stage Transformer (MST) encoder is constructed to model not only the interaction awareness between the ego vehicle and its neighbors but also intention awareness between the agents and their candidate routes. A sequential latent Transformer (SLT) with self-supervised learning objectives is employed to distill the future predictive information into the latent scene representation, in order to reduce the exploration space and speed up training. The fina
    
[^124]: 基于语法的基础词汇学习

    Grammar-Based Grounded Lexicon Learning. (arXiv:2202.08806v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.08806](http://arxiv.org/abs/2202.08806)

    基于语法的基础词汇学习（G2L2）是一种从基础数据中学习语言含义表示的方法，通过将单词映射到语法类型和神经符号语义程序，利用基于语法的组合推导句子的含义，最终可以在基础输入上执行。

    

    我们提出了一种基于语法的基础词汇学习方法（G2L2），用于从基础数据（如图像和文本的配对）中学习语言的组合和基于基础的含义表示。G2L2的核心是一组词汇条目，将每个单词映射到一个语法类型和神经符号语义程序的元组。给定一个输入句子，G2L2首先查找与每个标记相关联的词汇条目。然后通过基于语法的组合词汇含义来推导句子的含义作为可执行的神经符号程序。恢复的含义程序可以在基础输入上执行。为了在指数级增长的组合空间中促进学习，我们引入了一种基于逻辑回归的channel pruning方法。

    We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist approach toward learning a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of G2L2 is a collection of lexicon entries, which map each word to a tuple of a syntactic type and a neuro-symbolic semantic program. For example, the word shiny has a syntactic type of adjective; its neuro-symbolic semantic program has the symbolic form {\lambda}x. filter(x, SHINY), where the concept SHINY is associated with a neural network embedding, which will be used to classify shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries associated with each token. It then derives the meaning of the sentence as an executable neuro-symbolic program by composing lexical meanings based on syntax. The recovered meaning programs can be executed on grounded inputs. To facilitate learning in an exponentially-growing compositional space, we introd
    
[^125]: 关于使用机器学习在体内识别肾结石的研究

    On the in vivo recognition of kidney stones using machine learning. (arXiv:2201.08865v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2201.08865](http://arxiv.org/abs/2201.08865)

    本研究通过比较不同机器学习方法和深度学习架构在体内图像上识别肾结石的性能，为肾结石诊断提供了重要的参考。

    

    确定肾结石的类型可以帮助泌尿科医生预防肾结石的复发。自动化的体内基于图像的分类方法将是诊断的第一阶段中立即识别所需肾结石类型的重要一步。文献中已经证明，在体外的数据上，自动化的肾结石分类确实是可行的。本研究比较了六种浅层机器学习方法和三种深度学习架构在体内图像上进行肾结石识别的性能，并使用内窥镜在标准输尿管镜检术期间获得的四种最常见尿路结石类型的图像进行测试。本文详细介绍了数据库构建和测试的肾结石分类器的设计。

    Determining the type of kidney stones allows urologists to prescribe a treatment to avoid recurrence of renal lithiasis. An automated in-vivo image-based classification method would be an important step towards an immediate identification of the kidney stone type required as a first phase of the diagnosis. In the literature it was shown on ex-vivo data (i.e., in very controlled scene and image acquisition conditions) that an automated kidney stone classification is indeed feasible. This pilot study compares the kidney stone recognition performances of six shallow machine learning methods and three deep-learning architectures which were tested with in-vivo images of the four most frequent urinary calculi types acquired with an endoscope during standard ureteroscopies. This contribution details the database construction and the design of the tested kidney stones classifiers. Even if the best results were obtained by the Inception v3 architecture (weighted precision, recall and F1-score o
    
[^126]: 深度图像先验的早停法

    Early Stopping for Deep Image Prior. (arXiv:2112.06074v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.06074](http://arxiv.org/abs/2112.06074)

    本文提出了一种早停策略来解决深度图像先验中的过拟合问题，通过在多个视觉任务和DIP变体中持续检测接近最佳性能，突破了DIP实用性的限制。

    

    深度图像先验(DIP)及其变体在计算机视觉中解决逆问题方面显示出了显著的潜力，无需额外的训练数据。实际的DIP模型通常存在过度参数化的问题。在拟合过程中，这些模型首先学习到大部分期望的视觉内容，然后逐渐捕捉到潜在的建模和观测噪声，即过拟合。因此，DIP的实用性往往关键取决于良好的早停策略，以捕捉过渡期。在这方面，大多数用于视觉任务的DIP工作只展示了模型的潜力，报告了与真实结果的最佳性能，但对于如何在没有真实结果的情况下操作性地获得接近最佳性能并没有给出线索。在本文中，我们致力于突破DIP的实用性障碍，并提出了一种高效的早停策略，该策略在多个视觉任务和DIP变体中持续地检测到接近最佳性能。

    Deep image prior (DIP) and its variants have showed remarkable potential for solving inverse problems in computer vision, without any extra training data. Practical DIP models are often substantially overparameterized. During the fitting process, these models learn mostly the desired visual content first, and then pick up the potential modeling and observational noise, i.e., overfitting. Thus, the practicality of DIP often depends critically on good early stopping (ES) that captures the transition period. In this regard, the majority of DIP works for vision tasks only demonstrates the potential of the models -- reporting the peak performance against the ground truth, but provides no clue about how to operationally obtain near-peak performance without access to the groundtruth. In this paper, we set to break this practicality barrier of DIP, and propose an efficient ES strategy, which consistently detects near-peak performance across several vision tasks and DIP variants. Based on a sim
    
[^127]: 训练神经网络时的局部弹性动力学。

    Dynamics of Local Elasticity During Training of Neural Nets. (arXiv:2111.01166v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.01166](http://arxiv.org/abs/2111.01166)

    本论文研究了神经网络训练过程中的局部弹性动力学。通过对现有$S_{\rm rel}$定义的全面研究并提出新的定义，我们发现新的定义能更敏锐地检测出权重更新更偏向于在与样本数据同一类别内进行预测变化的特性。

    

    在最近的研究中，我们发现了神经网络训练轨迹在权重空间中的一个属性，即“局部弹性”（表示为$S_{\rm rel}$）。局部弹性试图量化样本数据点对预测结果在其他数据点上的影响传播。在本研究中，我们对已有的$S_{\rm rel}$定义进行了全面研究，并提出了一个新的定义，解决了原始定义在分类设置中的局限性。通过在SVHN，CIFAR-10和CIFAR-100上进行各种最先进的神经网络训练实验，我们展示了我们新的$S_{\rm rel}$定义相比于原始定义更加敏锐地检测出权重更新更偏向于在与样本数据同一类别内进行预测变化的特性。在神经回归实验中，我们展示了原始$S_{\rm rel}$显示出一个2阶段行为--通过初始弹性阶段进行训练。

    In the recent past, a property of neural training trajectories in weight-space had been isolated, that of "local elasticity" (denoted as $S_{\rm rel}$). Local elasticity attempts to quantify the propagation of the influence of a sampled data point on the prediction at another data. In this work, we embark on a comprehensive study of the existing notion of $S_{\rm rel}$ and also propose a new definition that addresses the limitations that we point out for the original definition in the classification setting. On various state-of-the-art neural network training on SVHN, CIFAR-10 and CIFAR-100 we demonstrate how our new proposal of $S_{\rm rel}$, as opposed to the original definition, much more sharply detects the property of the weight updates preferring to make prediction changes within the same class as the sampled data.  In neural regression experiments we demonstrate that the original $S_{\rm rel}$ reveals a $2-$phase behavior -- that the training proceeds via an initial elastic phas
    
[^128]: 使用生成性神经网络进行市场风险模型的场景生成

    Scenario generation for market risk models using generative neural networks. (arXiv:2109.10072v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.10072](http://arxiv.org/abs/2109.10072)

    本研究展示了如何使用生成性神经网络作为经济场景生成器，将其应用于整个内部市场风险模型，具有与欧洲监管批准的内部模型相似的结果。

    

    本研究展示了如何扩展使用生成对抗网络（GAN）作为经济场景生成器（ESG）的现有方法，将其应用于整个内部市场风险模型，以模拟保险公司的全方位投资风险因素，并符合 Solvency 2 所要求的一年时间范围。我们证明了基于 GAN 的内部模型的结果与欧洲监管批准的内部模型类似。因此，可以将基于 GAN 的模型视为一种数据驱动的市场风险建模的替代方式。

    In this research, we show how to expand existing approaches of using generative adversarial networks (GANs) as economic scenario generators (ESG) to a whole internal market risk model - with enough risk factors to model the full band-width of investments for an insurance company and for a one year time horizon as required in Solvency 2. We demonstrate that the results of a GAN-based internal model are similar to regulatory approved internal models in Europe. Therefore, GAN-based models can be seen as a data-driven alternative way of market risk modeling.
    
[^129]: Bregman Proximal Point算法和Mirror Descent在可分数据上的隐式正则化

    Implicit Regularization of Bregman Proximal Point Algorithm and Mirror Descent on Separable Data. (arXiv:2108.06808v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.06808](http://arxiv.org/abs/2108.06808)

    通过对可分数据使用Bregman proximal point算法和Mirror Descent进行学习线性分类器的研究, 我们发现BPPA具有可验证的算法正则化性质, 并且证明了边界与Bregman距离之间的关联性, 这揭示了BPPA对于学习分类器质量的影响和重要性

    

    Bregman proximal point算法（BPPA）在机器学习中具有广泛应用，但其理论理解尚未完全探索。我们通过学习具有可分数据的线性分类器，研究了BPPA的计算性质，并证明了BPPA的可验证算法正则化。对于任何使用固定Bregman距离实例化的BPPA，我们提供了BPPA所获得的边界的下界，该下界与任意选择的范数相关。所得到的边界下界与最大边界之间存在一个乘法因子的差异，该乘法因子与以对偶范数度量的距离生成函数的条件数成反比。我们证明了对条件数的依赖是紧致的，从而证明了差异对于影响学习分类器的质量的重要性。然后，我们将我们的发现推广到了mirror descent，对于该算法，我们建立了边界和Bregman距离之间的类似联系，以及一个非-a

    Bregman proximal point algorithm (BPPA) has witnessed emerging machine learning applications, yet its theoretical understanding has been largely unexplored. We study the computational properties of BPPA through learning linear classifiers with separable data, and demonstrate provable algorithmic regularization of BPPA. For any BPPA instantiated with a fixed Bregman divergence, we provide a lower bound of the margin obtained by BPPA with respect to an arbitrarily chosen norm. The obtained margin lower bound differs from the maximal margin by a multiplicative factor, which inversely depends on the condition number of the distance-generating function measured in the dual norm. We show that the dependence on the condition number is tight, thus demonstrating the importance of divergence in affecting the quality of the learned classifiers. We then extend our findings to mirror descent, for which we establish similar connections between the margin and Bregman divergence, together with a non-a
    
[^130]: 关于主成分回归模型识别和样本外预测的研究: 应用于合成对照研究

    On Model Identification and Out-of-Sample Prediction of Principal Component Regression: Applications to Synthetic Controls. (arXiv:2010.14449v5 [math.ST] UPDATED)

    [http://arxiv.org/abs/2010.14449](http://arxiv.org/abs/2010.14449)

    该论文在高维度的误差变量固定设计环境中分析了主成分回归模型识别和样本外预测问题，并提出了优于已知最佳速率的非渐进预测保证。在分析过程中，引入了一种自然的线性代数条件来避免样本外预测的分布假设，并构建了一个假设检验来检查该条件在实践中的可行性。同时，该论文的结果也对合成对照研究提供了新的发现。

    

    我们在高维度的误差变量固定设计环境中分析了主成分回归(PCR)。在适当的条件下，我们证明了PCR能够一致地识别出具有最小L2范数的唯一模型。这些结果使我们能够建立起优于已知最佳速率的非渐进的样本外预测保证。在我们的分析过程中，我们引入了一种自然的线性代数条件，联系了样本内和样本外的协变量，从而避免了对样本外预测的分布假设。我们的模拟实验证明了此条件在泛化方面的重要性，即使在协变量漂移的情况下也是如此。因此，我们构建了一个假设检验来检查这个条件在实践中是否成立。此外，我们的结果还为合成对照文献提供了新的发现，合成对照是一种主要的政策评估方法。据我们所知，我们在固定设计环境中的预测保证尚未被研究过。

    We analyze principal component regression (PCR) in a high-dimensional error-in-variables setting with fixed design. Under suitable conditions, we show that PCR consistently identifies the unique model with minimum $\ell_2$-norm. These results enable us to establish non-asymptotic out-of-sample prediction guarantees that improve upon the best known rates. In the course of our analysis, we introduce a natural linear algebraic condition between the in- and out-of-sample covariates, which allows us to avoid distributional assumptions for out-of-sample predictions. Our simulations illustrate the importance of this condition for generalization, even under covariate shifts. Accordingly, we construct a hypothesis test to check when this conditions holds in practice. As a byproduct, our results also lead to novel results for the synthetic controls literature, a leading approach for policy evaluation. To the best of our knowledge, our prediction guarantees for the fixed design setting have been 
    
[^131]: 基于图神经网络和自回归策略分解的符号关系深度强化学习

    Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks and Autoregressive Policy Decomposition. (arXiv:2009.12462v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.12462](http://arxiv.org/abs/2009.12462)

    这篇论文介绍了一种基于图神经网络和自回归策略分解的深度强化学习框架，能够处理符号关系问题的可变状态和动作空间，并在多个领域展现了广泛的适用性和令人印象深刻的零-shot泛化能力。

    

    我们关注于以对象、它们之间的关系和以对象为中心的动作来自然定义的符号关系问题中的强化学习。这些问题具有可变的状态和动作空间，对于大多数现有的强化学习方法而言，找到一个固定长度的表示是困难的，甚至不可能的。我们提出了一个基于图神经网络和自回归策略分解的深度强化学习框架，可以自然地应用于这些问题，并且完全是领域无关的。我们在三个不同的领域展示了该框架的广泛适用性，并展示了在不同问题大小上引人注目的零-shot泛化效果。

    We focus on reinforcement learning (RL) in relational problems that are naturally defined in terms of objects, their relations, and object-centric actions. These problems are characterized by variable state and action spaces, and finding a fixed-length representation, required by most existing RL methods, is difficult, if not impossible. We present a deep RL framework based on graph neural networks and auto-regressive policy decomposition that naturally works with these problems and is completely domain-independent. We demonstrate the framework's broad applicability in three distinct domains and show impressive zero-shot generalization over different problem sizes.
    
[^132]: 随机最小二乘值迭代的频率后悔界限

    Frequentist Regret Bounds for Randomized Least-Squares Value Iteration. (arXiv:1911.00567v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1911.00567](http://arxiv.org/abs/1911.00567)

    本文引入了一种改进版本的随机最小二乘值迭代（RLSVI）算法，通过对行动值函数的最小二乘逼近进行扰动，诱导出了探索过程。在马尔可夫决策过程具有低秩转移动态的假设下，我们证明了RLSVI的频率后悔上界为$\widetilde O(d^2 H^2 \sqrt{T})$。这是对于带有函数逼近的随机探索的首个频率后悔分析。

    

    我们考虑有限时间域强化学习中的探索-利用困境。当状态空间很大或连续时，传统的表格方法不可行，必须采用函数逼近的形式。在本文中，我们引入了一种乐观初始化的改进版本的随机最小二乘值迭代（RLSVI）算法，该算法是一种无模型算法，其中探索是通过扰动行动值函数的最小二乘逼近来诱导的。在假设马尔可夫决策过程具有低秩转移动态的情况下，我们证明了RLSVI的频率后悔将上界为$\widetilde O(d^2 H^2 \sqrt{T})$，其中$ d $是特征维度，$ H $是时间限制，$ T $是总步数。据我们所知，这是对于带有函数逼近的随机探索的第一个频率后悔分析。

    We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning (RL). When the state space is large or continuous, traditional tabular approaches are unfeasible and some form of function approximation is mandatory. In this paper, we introduce an optimistically-initialized variant of the popular randomized least-squares value iteration (RLSVI), a model-free algorithm where exploration is induced by perturbing the least-squares approximation of the action-value function. Under the assumption that the Markov decision process has low-rank transition dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by $\widetilde O(d^2 H^2 \sqrt{T})$ where $ d $ are the feature dimension, $ H $ is the horizon, and $ T $ is the total number of steps. To the best of our knowledge, this is the first frequentist regret analysis for randomized exploration with function approximation.
    
[^133]: 在线度量学习的多层框架

    A Multilayer Framework for Online Metric Learning. (arXiv:1805.05510v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/1805.05510](http://arxiv.org/abs/1805.05510)

    这项研究提出了一种多层框架的在线度量学习方法，用于捕捉实例之间的非线性相似性。不同于传统方法只能学习一个度量空间，该方法使用多个层次的度量空间来处理复杂的数据分布。

    

    在线度量学习在分类和检索中得到了广泛应用。它可以通过将相似实例与不相似实例分开一定边距来自动从数据中学习适合的度量。然而，现有的在线度量学习算法在复杂数据分布的实际分类中性能有限。为此，本文提出了一种多层在线度量学习框架，以捕捉实例之间的非线性相似性。与传统的在线度量学习不同，该提出的多层在线度量学习（MLOML）将在线度量学习算法作为度量层，并学习多个分层度量空间，其中每个度量层均遵循复杂数据分布的非线性关系。此外，使用正向传播（FP）策略和反向传播（BP）策略来训练层次结构。

    Online metric learning has been widely applied in classification and retrieval. It can automatically learn a suitable metric from data by restricting similar instances to be separated from dissimilar instances with a given margin. However, the existing online metric learning algorithms have limited performance in real-world classifications, especially when data distributions are complex. To this end, this paper proposes a multilayer framework for online metric learning to capture the nonlinear similarities among instances. Different from the traditional online metric learning, which can only learn one metric space, the proposed Multi-Layer Online Metric Learning (MLOML) takes an online metric learning algorithm as a metric layer and learns multiple hierarchical metric spaces, where each metric layer follows a nonlinear layers for the complicated data distribution. Moreover, the forward propagation (FP) strategy and backward propagation (BP) strategy are employed to train the hierarchic
    

