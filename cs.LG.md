# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark.](http://arxiv.org/abs/2304.03279) | 本文介绍了 MACHIAVELLI 基准测试，用于衡量人工智能代理是否表现出马基雅维利行为，发现了最大化奖励和行为的道德性之间存在权衡，并探索了基于语言模型的方法来减轻这种权衡。 |
| [^2] | [DiffMimic: Efficient Motion Mimicking with Differentiable Physics.](http://arxiv.org/abs/2304.03274) | 本文提出了DiffMimic，一种基于可微分物理的高效运动模仿方法。与传统强化学习方法相比，其有更快更稳定的收敛速度；同时通过演示重播机制避免陷入局部最优解。 |
| [^3] | [Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models.](http://arxiv.org/abs/2304.03271) | 本论文揭示以及提出了解决人工智能模型巨大水足迹的方法，因为其淡水消耗已经引起国际社会的重视，并且AI模型应该承担社会责任，做出面对水危机的表率。 |
| [^4] | [Causal Discovery with Score Matching on Additive Models with Arbitrary Noise.](http://arxiv.org/abs/2304.03265) | 本文提出了一种新的因果性发现算法NoGAM，它仅仅需要最少量的假设，并基于加性非线性模型来推断因果图中变量的拓扑顺序，而且在合成数据上性能表现最优。 |
| [^5] | [A Bayesian Framework for Causal Analysis of Recurrent Events in Presence of Immortal Risk.](http://arxiv.org/abs/2304.03247) | 论文提出了一种贝叶斯框架，针对错位处理问题，将其视为治疗切换问题，并通过概率模型解决了复增和末事件偏差的问题。 |
| [^6] | [Assessing the Reproducibility of Machine-learning-based Biomarker Discovery in Parkinson's Disease.](http://arxiv.org/abs/2304.03239) | 本文评估了机器学习方法在生物标记发现中的可重复性，结果表明不同的数据集成策略在确定潜在PD生物标志物的SNPs上的一致性较低。 |
| [^7] | [FedBot: Enhancing Privacy in Chatbots with Federated Learning.](http://arxiv.org/abs/2304.03228) | 本论文提出了一个利用联邦学习保护用户隐私的聊天机器人FedBot。它结合了Deep Bidirectional Transformer模型和联邦学习算法，在联合模型训练过程中保护客户数据隐私。 |
| [^8] | [DexDeform: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics.](http://arxiv.org/abs/2304.03223) | 本文提出了DexDeform，借助人类示范和不同iable物理学习灵巧操纵可变形物体的技能，可在真实环境中高效实现。 |
| [^9] | [Anomaly Detection via Gumbel Noise Score Matching.](http://arxiv.org/abs/2304.03220) | 该论文提出了一种通过估计连续松弛分类分布的得分来检测分类数据中的异常的无监督方法，该方法在异常检测表格数据集和图像数据中均表现出持续优异的性能。 |
| [^10] | [Hierarchical Graph Neural Network with Cross-Attention for Cross-Device User Matching.](http://arxiv.org/abs/2304.03215) | 本文提出了一种带有跨设备交叉注意力的分层图神经网络(HGNN)，用于解决跨设备用户匹配问题，相对于最先进的TGCE方法，提高了5%的性能。 |
| [^11] | [Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts.](http://arxiv.org/abs/2304.03209) | 提出了一种名为MORSE的基于隐式解剖渲染的通用神经渲染框架，旨在在医学图像分割中帮助融合高级语义相关内容和低级解剖特征。 |
| [^12] | [Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster.](http://arxiv.org/abs/2304.03208) | 本论文介绍了从111M到13B参数的开放计算优化语言模型 Cerebras-GPT，它采用了高效的预训练方法和缩放规则，具有最先进的训练效率和可预测性，这是一个开放可复现的工作。 |
| [^13] | [SLM: End-to-end Feature Selection via Sparse Learnable Masks.](http://arxiv.org/abs/2304.03202) | SLM是一种经典的端到端特征选择方法，通过可学习的稀疏掩码来最大化所选特征和标签之间的互信息，同时精确地控制选择的特征数量，实验结果表明其取得了最先进的结果。 |
| [^14] | [Improving automatic endoscopic stone recognition using a multi-view fusion approach enhanced with two-step transfer learning.](http://arxiv.org/abs/2304.03193) | 本文介绍了一种利用多视图融合及两步式迁移学习增强的自动内镜结石识别方法，可提高肾结石分类准确度达6%以上。 |
| [^15] | [Krylov Methods are (nearly) Optimal for Low-Rank Approximation.](http://arxiv.org/abs/2304.03191) | 本文研究在矩阵-向量相乘模型下针对各种Schatten范数的秩-1低秩逼近问题，表明Krylov方法（几乎）实现谱，Frobenius和核低秩逼近的信息熵上的最优矩阵-向量积数量。 |
| [^16] | [The Concept of Forward-Forward Learning Applied to a Multi Output Perceptron.](http://arxiv.org/abs/2304.03189) | 本文应用前向前向学习算法于多输出感知机，仅需单个矩阵乘法，有效地处理数据集，性能与隐藏层更多的复杂神经网络相当。 |
| [^17] | [Pairwise Ranking with Gaussian Kernels.](http://arxiv.org/abs/2304.03185) | 本文提出新的Oracle不等式，在输入域上的一般盒计数维度假设和噪声条件或标准平滑条件下，对于高斯成对排名估计器得出了快速学习率。这表明，输入空间的低固有维度可以帮助避免维度诅咒。 |
| [^18] | [Deep learning-based image exposure enhancement as a pre-processing for an accurate 3D colon surface reconstruction.](http://arxiv.org/abs/2304.03171) | 本文研究了基于深度学习的三维结肠表面重建，在图像预处理的过程中通过纠正局部欠曝光和过曝光来提高重建精度。 |
| [^19] | [Spectral Toolkit of Algorithms for Graphs: Technical Report (1).](http://arxiv.org/abs/2304.03170) | 频谱工具包STAG是一个开源库，用于实现高效的频谱图算法，其中还包括本地图聚类组件。此技术报告介绍了STAG的用户指南、展示研究以及开发背后的技术考虑。 |
| [^20] | [Parameterized Approximation Schemes for Clustering with General Norm Objectives.](http://arxiv.org/abs/2304.03146) | 提出了一种简洁的EPAS，解决了多个聚类问题，并统一了已有的EPAS |
| [^21] | [Efficient SAGE Estimation via Causal Structure Learning.](http://arxiv.org/abs/2304.03113) | 提出了一种通过因果结构学习的方法名为d-SAGE，用于加速SAGE逼近算法，显著降低计算开销和提高计算效率，并在理论上展示了$d$-SAGE的逼近误差会收敛于零，实验上体现了高精度。 |
| [^22] | [Spectral Gap Regularization of Neural Networks.](http://arxiv.org/abs/2304.03096) | 本文介绍了一种利用谱/图形信息进行神经网络正则化的新方法，即Fiedler正则化。通过使用神经网络底层图的Fiedler值作为正则化工具，我们提供了一种结构加权的 $\text{L}_1$ 惩罚并提供统一泛化误差界限的分析，这使得我们的方法在许多数据集上都取得了良好的性能。 |
| [^23] | [PopulAtion Parameter Averaging (PAPA).](http://arxiv.org/abs/2304.03094) | 提出一种新方法PopulAtion Parameter Averaging (PAPA)，能同时拥有集成的普遍性与权重平均的效率，可以显著提高模型性能。 |
| [^24] | [Inductive Graph Unlearning.](http://arxiv.org/abs/2304.03093) | 本论文介绍了针对图形数据的反学习，旨在实现机器学习中的“被遗忘权”，与其他框架相比，提出了一个新颖的归纳式框架。这个框架可以让机器学习系统在处理动态改变的图形时更具有适应性。 |
| [^25] | [Safe MDP Planning by Learning Temporal Patterns of Undesirable Trajectories and Averting Negative Side Effects.](http://arxiv.org/abs/2304.03081) | 通过学习不良轨迹的时间模式和防止负面副作用实现安全MDP规划 |
| [^26] | [Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series.](http://arxiv.org/abs/2304.03069) | 本文提出了一种适用于非平稳时间序列的自适应学生t分布方法，基于方法的一般自适应矩可以使用廉价的指数移动平均值（EMA）来估计参数。 |
| [^27] | [An experimental study in Real-time Facial Emotion Recognition on new 3RL dataset.](http://arxiv.org/abs/2304.03064) | 该论文介绍了新的3RL数据集，它是一种用于面部情感识别的数据集，通过与其他数据集进行比较，表现出了更好的泛化能力，并使用最先进的算法(cnn)实现了91.4％的准确率。 |
| [^28] | [Expert-Independent Generalization of Well and Seismic Data Using Machine Learning Methods for Complex Reservoirs Predicting During Early-Stage Geological Exploration.](http://arxiv.org/abs/2304.03048) | 该研究利用自主方法预测了复杂油藏的空间分布概率，可以进行专家无关的概化预测和地质模型创建。 |
| [^29] | [Multi-Linear Kernel Regression and Imputation in Data Manifolds.](http://arxiv.org/abs/2304.03041) | 本文提出了一种基于数据流形的多线性核回归和插补框架，可以高效地进行计算、提取数据模式和它们的几何形状，而且在dMRI数据下具有显著的改进。 |
| [^30] | [Modelling customer lifetime-value in the retail banking industry.](http://arxiv.org/abs/2304.03038) | 本研究提出了一个通用的框架，可以应用于具有长期合同和产品中心客户关系的行业来建模客户的生命周期价值，该框架可以预测任意时间范围内的CLV，并可生成基于产品的倾向模型，这在零售银行业中尤其重要。通过测试，我们证明了相对于传统算法，该模型可以提高43%的超出时间的CLV预测误差。 |
| [^31] | [IoT Federated Blockchain Learning at the Edge.](http://arxiv.org/abs/2304.03006) | 本文提出了一种边缘计算的联邦学习框架，使用区块链实现分散式方案，从而提高隐私性和效率，可以在不泄露数据隐私的前提下，训练出更加准确、全面的医疗机器学习模型。 |
| [^32] | [Spritz-PS: Validation of Synthetic Face Images Using a Large Dataset of Printed Documents.](http://arxiv.org/abs/2304.02982) | 该论文开发了一个新的数据集，以成为多媒体取证调查的标准，通过验证合成面部图像来探究印刷和扫描图像的有效取证分析能力。 |
| [^33] | [A Fast and Lightweight Network for Low-Light Image Enhancement.](http://arxiv.org/abs/2304.02978) | 本文提出了一种称为FLW-Net的快速、轻量级网络，用于解决低光照图像中存在的噪声、低亮度、低对比度和色彩偏差问题。该方法具有高效的全局特征信息提取组件以及基于相对信息设计的损失函数，实验结果显示其有效性。 |
| [^34] | [Unconstrained Parametrization of Dissipative and Contracting Neural Ordinary Differential Equations.](http://arxiv.org/abs/2304.02976) | 本文介绍了一种连续时间的深度神经网络，通过结合神经常微分方程和循环平衡网络的结构，使得网络具有收缩和耗散性质。此外提出的非约束参数化方法使得该网络学习的参数量得以增加。 |
| [^35] | [Deep Long-Short Term Memory networks: Stability properties and Experimental validation.](http://arxiv.org/abs/2304.02975) | 本研究研究了增量输入状态稳定的深度长短期记忆网络在非线性动态系统识别方面的应用，并利用对网络权重的适当充分条件来建立一个训练过程，从数据中学习到经过证明的$\delta$ISS LSTM模型。在实验中测试结果表明了令人满意的建模表现。 |
| [^36] | [Training a Two Layer ReLU Network Analytically.](http://arxiv.org/abs/2304.02972) | 本研究探讨了一种算法，可以使用解析的方法训练双层ReLU网络，相比随机梯度下降和Adam优化器能够找到更深的最小值，在四个真实数据集中获得了显著更小的训练损失值，同时该方法速度更快，调参参数更少。 |
| [^37] | [Synthetic Hard Negative Samples for Contrastive Learning.](http://arxiv.org/abs/2304.02971) | 本文提出一种新的方法，称为SSCL，可以更好地利用难以区分出的负样本，提高对比学习的性能。 |
| [^38] | [FengWu: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead.](http://arxiv.org/abs/2304.02948) | FengWu是一个基于人工智能的先进数据驱动的全球中期天气预报系统。它从多模态和多任务的角度下解决了中期预报问题，通过不确定性损失的监督学习，在区域自适应的方式下平衡不同预测器的优化。引入回放缓冲机制来提高性能。FengWu具有精确预测大气动力学和未来的陆地和大气状态的能力。在2018年的长期预报中表现优异。 |
| [^39] | [Adaptable and Interpretable Framework for Novelty Detection in Real-Time IoT Systems.](http://arxiv.org/abs/2304.02947) | 介绍了一种RAID算法，能够在多元动态过程中检测异常行为，具有适应非平稳效应、不需改变现有过程自动化基础设施等特点，可在不同领域高度部署，并通过实际数据案例研究证明其改进的检测准确性。 |
| [^40] | [Convolutional neural networks for crack detection on flexible road pavements.](http://arxiv.org/abs/2304.02933) | 该研究通过对六种卷积神经网络模型的比较，使用包含14000个样本的新现实世界二元裂缝数据集进行微调，实现了自动检测路面裂缝的目的。训练的六个模型中有五个的准确率超过了97％，最高记录的准确率为99.7％。最佳模型已经部署在云基础架构上，以允许自动检测相机镜头中的裂缝。 |
| [^41] | [Mask Detection and Classification in Thermal Face Images.](http://arxiv.org/abs/2304.02931) | 本文利用热成像技术，检测口罩是否佩戴及分类类型，最佳模型为nano版本的Yolov5模型，达到97%以上的高准确度。 |
| [^42] | [Efficient Audio Captioning Transformer with Patchout and Text Guidance.](http://arxiv.org/abs/2304.02916) | 本文提出了一种使用Patchout和文本指导的高效音频字幕生成器，利用迁移学习和Mixup增强技术解决数据稀缺性问题。 |
| [^43] | [Classification of Superstatistical Features in High Dimensions.](http://arxiv.org/abs/2304.02912) | 本文利用经验风险最小化的方法，对高维超统计特征下的数据进行分类，并分析了正则化和分布尺度参数对分类的影响。 |
| [^44] | [Heavy-Tailed Regularization of Weight Matrices in Deep Neural Networks.](http://arxiv.org/abs/2304.02911) | 本文介绍了一种名为重尾部正则化的技术，在深度神经网络中通过明确提倡更重的重尾谱来提高泛化性能。与标准正则化技术相比，该方法在基准数据集上实现了显着的改进。 |
| [^45] | [Towards Efficient MCMC Sampling in Bayesian Neural Networks by Exploiting Symmetry.](http://arxiv.org/abs/2304.02902) | 本文提出了一种利用对称性在贝叶斯神经网络中实现高效MCMC采样的方法，通过利用神经元可互换性和某些激活函数引起的对称性在参数后验的多模态性中找到平衡。 |
| [^46] | [Variable-Complexity Weighted-Tempered Gibbs Samplers for Bayesian Variable Selection.](http://arxiv.org/abs/2304.02899) | 本文提出了一种变复杂度加权调节 Gibbs 采样器，用于贝叶斯变量选择，可以降低每个MCMC迭代的计算复杂度，并且可以在有限的迭代次数内控制估计器的方差。 |
| [^47] | [Object-centric Inference for Language Conditioned Placement: A Foundation Model based Approach.](http://arxiv.org/abs/2304.02893) | 本文提出了一个基于物体中心的语言条件放置推理框架，有效降低了训练数据需求，具有更好的泛化能力并可以实现高达 97.75% 的放置成功率。 |
| [^48] | [Learning Cautiously in Federated Learning with Noisy and Heterogeneous Clients.](http://arxiv.org/abs/2304.02892) | 本文提出了一种名为FedCNI的联邦学习方法，该方法包括鲁棒的全局聚合器和抗噪局部求解器，可以有效处理在小型本地数据集中存在的标签噪声和类别不平衡的问题。 |
| [^49] | [ViralVectors: Compact and Scalable Alignment-free Virome Feature Generation.](http://arxiv.org/abs/2304.02891) | ViralVectors是一种紧凑且可扩展的方法，从virome测序数据中生成Minimizers特征向量进行有效的下游分析。该方法优于现有非比对技术方法，可以区分不同的病毒家族，甚至属，并能够提供接近最优的SARS-CoV-2分类性能。 |
| [^50] | [Tag that issue: Applying API-domain labels in issue tracking systems.](http://arxiv.org/abs/2304.02877) | 该论文研究了在开源软件项目中自动标记问题为API领域标签的可行性和相关性，结果表明自动分配的标签对贡献者选择任务和理解问题要求是有帮助的。 |
| [^51] | [Protecting User Privacy in Online Settings via Supervised Learning.](http://arxiv.org/abs/2304.02870) | 本研究提出了基于监督学习的隐私保护方法，可以阻止侵犯用户隐私的数据收集，保护用户数字隐私。 |
| [^52] | [Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions.](http://arxiv.org/abs/2304.02868) | 本文探究大型语言模型在玩文字游戏的能力，并发现其表现有竞争力，但仍然缺乏智能，有待提升。 |
| [^53] | [Learning to Learn with Indispensable Connections.](http://arxiv.org/abs/2304.02862) | 该论文提出了一种新的元学习方法Meta-LTH，该方法包括不可缺少的连接，通过重要性剪枝技术生成关键的连接，能够有效地解决少样本学习问题，并在标准少样本学习基准测试上优于现有的元学习算法。 |
| [^54] | [A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation.](http://arxiv.org/abs/2304.02858) | 本文研究了集成学习和数据增强方法的应用，针对类别不平衡问题，通过计算评估，找到了最有效的组合。 |
| [^55] | [Logistic-Normal Likelihoods for Heteroscedastic Label Noise in Classification.](http://arxiv.org/abs/2304.02849) | 该论文介绍了一种新的分类方法，用于提高鲁棒性，减少标签噪声的影响，其基于正态分布，并可通过最小化负对数似然来学习参数。 |
| [^56] | [Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets.](http://arxiv.org/abs/2304.02847) | 本研究提出一种叫做Robustmix的方法，通过正则化网络以低频空间特征进行分类来提高深度网络的鲁棒性，在Imagenet-C和Stylized Imagenet等基准测试上取得了最新的最优状态平均峰值误差（mCE），在避免计算开销和先验知识的大量图像变换的同时对模型架构和数据增强的最新进展提供了补充。 |
| [^57] | [Robust Neural Architecture Search.](http://arxiv.org/abs/2304.02845) | 提出了一种名为RNAS的神经架构搜索方法，通过平衡准确性和鲁棒性生成高质量架构，使用噪声样本减少搜索成本，在图像分类和对抗攻击中均达到最先进性能水平。 |
| [^58] | [NTK-SAP: Improving neural network pruning by aligning training dynamics.](http://arxiv.org/abs/2304.02840) | 本文提出了一种新的神经网络剪枝方法：通过对齐训练动态来提高剪枝效果，具体来说就是剪去对NTK频谱影响最小的连接。采用这种方法有助于维持NTK频谱，从而将训练动态和其密集对应物的训练动态对齐。 |
| [^59] | [TBDetector:Transformer-Based Detector for Advanced Persistent Threats with Provenance Graph.](http://arxiv.org/abs/2304.02838) | 本论文提出了一种采用来源图和Transformer的高级持久性威胁检测方法，利用Transformer的自注意力编码器-解码器提取系统状态的长期上下文特征，并通过来源分析实现对长期运行系统的概括，以检测缓慢攻击。 |
| [^60] | [Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification.](http://arxiv.org/abs/2304.02836) | 本文提出了一种新的肺部结节分类方法，使用变压器模型整合了EHR中的成像和临床特征。 |
| [^61] | [GIF: A General Graph Unlearning Strategy via Influence Function.](http://arxiv.org/abs/2304.02835) | 本文提出了一种模型无关的图神经网络遗忘方法GIF，采用影响函数来移除模型中特定节点、边或特征的影响，在性能和复杂度的平衡上取得了最新的最佳表现。 |
| [^62] | [Deep Reinforcement Learning Based Vehicle Selection for Asynchronous Federated Learning Enabled Vehicular Edge Computing.](http://arxiv.org/abs/2304.02832) | 本文提出了一种基于深度强化学习的异步联邦学习车辆选择方案，以提高车联网的训练性能。仿真结果表明，该算法可以实现更好的训练精度和更低的通信开销。 |
| [^63] | [SoK: Machine Learning for Continuous Integration.](http://arxiv.org/abs/2304.02829) | 这篇论文对使用机器学习进行持续集成的方法进行了系统综述并指出了目前存在的不足和改进方向。 |
| [^64] | [GPT detectors are biased against non-native English writers.](http://arxiv.org/abs/2304.02819) | 该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。 |
| [^65] | [Causal Repair of Learning-enabled Cyber-physical Systems.](http://arxiv.org/abs/2304.02813) | 本文提出了一种学习增强型物联网系统的因果诊断和修复方法，通过矫正有问题的输入/输出行为子集，识别真正的属性违规原因并修复。 |
| [^66] | [HomPINNs: homotopy physics-informed neural networks for solving the inverse problems of nonlinear differential equations with multiple solutions.](http://arxiv.org/abs/2304.02811) | 本文提出了一种新的框架——基于同伦的物理知识神经网络，来解决具有多个解的非线性微分方程的反问题。该框架使用神经网络逼近已知观测结果并符合DEs的约束条件，通过同伦连续方法解决反问题。实验证明该方法可伸缩且适应性强，为解决具有多个解的DEs提供了有效解决方案。 |
| [^67] | [Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling.](http://arxiv.org/abs/2304.02806) | 本文提出了一种新的图混合专家（GMoE）模型，旨在解决现实世界中的图具有多样的图结构和包含异构节点和边的问题。该模型可以增强GNN的泛化能力，适应多样的训练图结构的能力，并且不会增加计算开销。 |
| [^68] | [UNICORN: A Unified Backdoor Trigger Inversion Framework.](http://arxiv.org/abs/2304.02786) | 本论文提出了一个基于触发器反演的统一框架 UNICORN，可用于识别后门模型并理解植入的恶意行为。 |
| [^69] | [A Transformer-Based Deep Learning Approach for Fairly Predicting Post-Liver Transplant Risk Factors.](http://arxiv.org/abs/2304.02780) | 本研究提出了一个基于Transformer的深度学习框架模型，可同时预测五种移植后风险并实现更好的性能。 |
| [^70] | [Application of Transformers based methods in Electronic Medical Records: A Systematic Literature Review.](http://arxiv.org/abs/2304.02768) | 这篇论文综述了基于Transformer的自然语言处理技术在电子病历领域中的应用，并提出了目前研究中的限制和未来研究的方向。 |
| [^71] | [MethaneMapper: Spectral Absorption aware Hyperspectral Transformer for Methane Detection.](http://arxiv.org/abs/2304.02767) | MethaneMapper是一个可用于光谱域内定位甲烷排放区域的Transformer网络，并在模型尺寸上实现了优化。同时，作者介绍了一个用于研究甲烷检测问题的大规模数据集。 |
| [^72] | [Hybrid Zonotopes Exactly Represent ReLU Neural Networks.](http://arxiv.org/abs/2304.02755) | 混合区间多面体精确表示ReLU神经网络，并且二进制变量随网络规模线性增长，实用性广泛。 |
| [^73] | [Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models.](http://arxiv.org/abs/2304.02754) | 本研究使用两种经典认知心理学技术来估算人类和GPT-3等大型语言模型的词汇语义结构，结果表明人类的概念结构稳健鲁棒，而大型语言模型的行为估算结构更多取决于具体任务。 |
| [^74] | [Bengali Fake Review Detection using Semi-supervised Generative Adversarial Networks.](http://arxiv.org/abs/2304.02739) | 本文研究使用半监督生成对抗网络以少量数据分类孟加拉语假评论和真实评论的潜力，并提出了BanglaBERT与半监督GAN相结合的解决方案，实验结果表明其准确率达到83.59％，f1分数达到84.89％。 |
| [^75] | [Learning Stability Attention in Vision-based End-to-end Driving Policies.](http://arxiv.org/abs/2304.02733) | 这篇论文提出了一种利用控制李亚普诺夫函数（CLFs）为端到端基于视觉的策略配备稳定属性，并引入CLFs中的稳定性注意力（att-CLFs）来处理环境变化并提高学习灵活性的方法。 |
| [^76] | [Constructing Phylogenetic Networks via Cherry Picking and Machine Learning.](http://arxiv.org/abs/2304.02729) | 本文提出了一种基于樱桃挑选理论框架的启发式算法，通过设计和训练能够捕捉输入树结构信息并指导算法产生更好解决方案的机器学习模型，实现对由二叉树组成的实际规模数据集的系统发育网络构建，方法具有良好的计算效率和准确性。 |
| [^77] | [FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For Medical Imaging Segmentation.](http://arxiv.org/abs/2304.02725) | FMG-Net和W-Net是两种受多重网格启发的深度学习架构，能够解决医学图像分割中面临的细节特征和尺度变化的挑战，能够提高肿瘤分割的精度。 |
| [^78] | [Exploring the Utility of Self-Supervised Pretraining Strategies for the Detection of Absent Lung Sliding in M-Mode Lung Ultrasound.](http://arxiv.org/abs/2304.02724) | 本研究探索了在M-模式肺部超声图像中使用自监督预训练来提高缺失肺滑动检测的分类性能。结果表明自监督预训练可以显著提高性能，并且使用大量未标记数据可以提高模型的通用性和外部验证性能。 |
| [^79] | [Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning.](http://arxiv.org/abs/2304.02711) | SPIRES是一种新的知识提取方法，利用大型语言模型进行零样本学习和通用查询回答，能够填充复杂的知识库而无需显式训练数据。 |
| [^80] | [Agnostic proper learning of monotone functions: beyond the black-box correction barrier.](http://arxiv.org/abs/2304.02700) | 本文提出了第一个无偏、高效、适当的单调布尔函数学习算法，算法的运行时间和假设的大小和评估时间都为$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$，该算法解决了样本高效算法无法解决的问题。 |
| [^81] | [Revolutionizing Single Cell Analysis: The Power of Large Language Models for Cell Type Annotation.](http://arxiv.org/abs/2304.02697) | 大语言模型的出现革命性地改变了单细胞分析，能够更有效、准确地进行细胞类型注释，揭示以前被忽视的细胞亚型的特定分化轨迹，对癌症、发育和干细胞分化的理解有重要应用。 |
| [^82] | [A Certified Radius-Guided Attack Framework to Image Segmentation Models.](http://arxiv.org/abs/2304.02693) | 本文提出了一种基于认证半径的攻击框架用于图像分割模型，该攻击框架主要通过引导搜索具有相对较小认证半径的像素来实现对图像分割模型的攻击。 |
| [^83] | [ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast.](http://arxiv.org/abs/2304.02689) | 本文提出了一种改进的对比学习框架ACTION++，通过自适应的解剖对比来改善半监督医学图像分割。 |
| [^84] | [Predictive Coding as a Neuromorphic Alternative to Backpropagation: A Critical Evaluation.](http://arxiv.org/abs/2304.02658) | 预测编码算法被认为是反向传播的一个替代方案，在神经形态学系统中具有潜力。研究者通过使用现有的 PC 变体探讨了这个问题，并给出了时间复杂度下界，揭示了 PC 的一些有趣的特性，包括其神经生物学可行性和潜在的贝叶斯推理解释。 |
| [^85] | [Graph Representation Learning for Interactive Biomolecule Systems.](http://arxiv.org/abs/2304.02656) | 本文综述了图形表示学习在生物分子系统中的重要性，并讨论了其对药物发现、蛋白质表征和生物系统分析的应用，同时总结了该领域的现状和未来研究方向。 |
| [^86] | [Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks.](http://arxiv.org/abs/2304.02654) | 本文提出了一种名为BiSupervised的新架构，使用两个监督器在调用大规模远程深度神经网络之前，对小规模本地模型进行预测，通过这种方式避免不必要的网络调用，可以节省成本。 |
| [^87] | [RARE: Robust Masked Graph Autoencoder.](http://arxiv.org/abs/2304.01507) | RARE是一种鲁棒性抗干扰的掩码图自编码器，通过在高阶潜在特征空间中进行掩码和重构节点样本来提高推断掩码数据的确定性和自监督机制的可靠性，并在下游任务中优于现有的SGP方法。 |
| [^88] | [Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis.](http://arxiv.org/abs/2304.01347) | 本文提出了一种基于动态功能连接的脑网络分析模型，通过构建动态同步特征和革命性的图卷积方法实现精神分裂症诊断和侧化分析，并在实验证明其表现优于其他最先进模型。 |
| [^89] | [Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice.](http://arxiv.org/abs/2304.01311) | 本研究通过访谈19位知识图谱（KG）实践者，发现KG构建者需求架构执行程序，KG分析师需要可自定义查询构建器，KG消费者需要领域特定可视化，并指出在实践中实施KG需要技术和社交方面的解决方案。 |
| [^90] | [POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems.](http://arxiv.org/abs/2304.01218) | POLAR-Express 是一种高效且准确的形式可达性分析工具，用于验证神经网络控制系统的安全性。它使用 Taylor 模型算术和逐层传播技术，可以分析具有连续激活功能的前馈神经网络，并在 ReLU 激活函数上提供了一种更有效的精确传播 TM 的新方法。 |
| [^91] | [Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition.](http://arxiv.org/abs/2304.01117) | 本文分析了2022年基因与进化计算会议举办的竞赛，评估了符号回归的新方法在面对现实数据时的表现，并提供了可解释性评估的实际方法。 |
| [^92] | [Joint 2D-3D Multi-Task Learning on Cityscapes-3D: 3D Detection, Segmentation, and Depth Estimation.](http://arxiv.org/abs/2304.00971) | 该论文提出了一种基于Cityscapes-3D的联合2D-3D多任务学习方法，旨在同时实现单眼3D车辆检测、语义分割和单眼深度估计，并通过优化多个目标单元，提高了模型性能。 |
| [^93] | [Improving Fast Adversarial Training with Prior-Guided Knowledge.](http://arxiv.org/abs/2304.00202) | 本文提出了一种使用先前训练过程中高质量对抗扰动的正面先验引导对抗初始化方法，以提高对抗样本的质量，从而避免快速对抗训练中的灾难性过度拟合问题。 |
| [^94] | [Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking.](http://arxiv.org/abs/2303.17907) | 本文提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向，实现面向全沉浸多用户虚拟现实技术的高效通信。 |
| [^95] | [A comparative evaluation of image-to-image translation methods for stain transfer in histopathology.](http://arxiv.org/abs/2303.17009) | 本文比较了12种图像到图像翻译方法在组织病理学染色转移方面的应用，证明了一些方法优于现有技术，可用于大规模染色组织病理学图像的合成。 |
| [^96] | [Task-oriented Memory-efficient Pruning-Adapter.](http://arxiv.org/abs/2303.14704) | 本文提出了一种面向任务的剪枝适配器方法，既实现了训练和内存的高效率，又加快了训练时间，并且在 GLUE 任务中没有显著降低准确性。 |
| [^97] | [Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings.](http://arxiv.org/abs/2303.13570) | 本研究提出了一种使用残差循环神经网络的新型模型，实现了可逆的句子嵌入。与其他神经机器翻译模型不同，该方法使用基于回归的输出层重建输入序列的单词向量，其具有高准确度和快速训练速度。这种方法适合各种自然语言处理应用，特别是对需要高质量句嵌入的神经网络系统的使用具有潜在优势。 |
| [^98] | [Causal Discovery from Temporal Data: An Overview and New Perspectives.](http://arxiv.org/abs/2303.10112) | 本文对于从时间数据中进行因果关系发现进行了综述，提出了两个相关的类别，即多元时间序列因果发现和事件序列因果发现，并提供了新的方法来综合考虑这两个类别。 |
| [^99] | [Variational formulations of ODE-Net as a mean-field optimal control problem and existence results.](http://arxiv.org/abs/2303.05924) | 本文探讨了ODE-Net在最小化损失函数的同时约束参数ODE的数学问题，并提出了一种测度论均场最优控制问题的形式化表述，并针对线性神经网络证明了最小化器的存在性结果。 |
| [^100] | [Time series anomaly detection with reconstruction-based state-space models.](http://arxiv.org/abs/2303.03324) | 本文提出一种基于重构状态空间模型的时间序列异常检测方法，该方法利用LSTM编码器—解码器共同学习观测和动态模型，并从正常样本中估计模型不确定性。该模型的潜在空间受到正则化约束，可以用马氏距离评估异常级别。 |
| [^101] | [Convergence Rates for Non-Log-Concave Sampling and Log-Partition Estimation.](http://arxiv.org/abs/2303.03237) | 非对数凹势V的高维采样速率可以在一些条件下实现与凸函数相同的收敛速率。 |
| [^102] | [Semi-decentralized Inference in Heterogeneous Graph Neural Networks for Traffic Demand Forecasting: An Edge-Computing Approach.](http://arxiv.org/abs/2303.00524) | 本文提出一种半分散推理方法，利用多个云节点降低通信需求，同时保持图神经网络分散化的优势，从而提高交通需求预测效率。 |
| [^103] | [AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for Robotic Rehabilitation.](http://arxiv.org/abs/2303.00085) | 本文提出了一种基于强化学习的机器人康复辅助控制器AR3n，通过使用虚拟患者模型实现控制器的泛化，实时调节机器人辅助力度并最小化机器人辅助的量，该控制器在实验验证中表现出良好的效果。 |
| [^104] | [PAD: Towards Principled Adversarial Malware Detection Against Evasion Attacks.](http://arxiv.org/abs/2302.11328) | 该论文提出了一个新的对抗性训练框架，称为合理对抗性恶意软件检测（PAD），它通过可学习的凸度量保护恶意软件检测器免受攻击者的影响，而不是简单的启发式方法。实验结果表明，该方法优于现有技术，提高了恶意软件检测的防御效果。 |
| [^105] | [Interpreting wealth distribution via poverty map inference using multimodal data.](http://arxiv.org/abs/2302.10793) | 本论文提出了一种利用多模态数据推断贫困地图的模型管道，通过推断多个地理集群的财富平均值和标准差，配合新颖的“财富地平线”概念明确可视化财富分布在不同空间尺度下的情况，有助于政策制定者更好地了解影响财富分布的潜在社会经济多样性，优先考虑有针对性的干预措施。 |
| [^106] | [Pruning Deep Neural Networks from a Sparsity Perspective.](http://arxiv.org/abs/2302.05601) | 本文提出了一种稀疏感知自适应剪枝算法，通过利用PQ指数来衡量深度神经网络的潜在压缩性，可以有效地确定模型的剪枝程度，确保不会过度或欠剪枝。 |
| [^107] | [IoT Botnet Detection Using an Economic Deep Learning Model.](http://arxiv.org/abs/2302.02013) | 本文提出了一种经济深度学习模型来检测物联网僵尸网络攻击以及不同类型的攻击。该模型能够在较小的预算下加速训练和检测过程，并且具有比最先进的检测模型更高的准确性。 |
| [^108] | [Perfect is the enemy of test oracle.](http://arxiv.org/abs/2302.01488) | 本文提出了一种学习方法 SEER，该方法可以在缺乏测试断言或其他类型的测试Oracle的情况下确定单元测试是否通过或失败，并且可以构建准确的Oracle而不需要知道正确或错误行为的确切期望。 |
| [^109] | [Analyzing Leakage of Personally Identifiable Information in Language Models.](http://arxiv.org/abs/2302.00539) | 本研究针对语言模型中泄漏个人身份信息的风险进行了严格的定义，并通过黑盒提取、推断和重建攻击进行了实证评估。 |
| [^110] | [Towards Flexibility and Interpretability of Gaussian Process State-Space Model.](http://arxiv.org/abs/2301.08843) | 本文提出了一种新的状态空间模型类TGPSSM，利用正规化流增加了标准GPSSM中的GP先验概率，从而增强模型的灵活性和表现力。同时提出了可扩展的变分推理算法，为潜在状态的变分分布提供灵活和最优的结构。 |
| [^111] | [Efficient Activation Function Optimization through Surrogate Modeling.](http://arxiv.org/abs/2301.05785) | 本文提出了一种基于代理建模的方法，通过扩展的基准测试空间在较少的函数评估次数中发现了优化的高效激活函数架构，并在多个标准基准测试中实现了最先进的性能。 |
| [^112] | [EmoGator: A New Open Source Vocal Burst Dataset with Baseline Machine Learning Classification Methodologies.](http://arxiv.org/abs/2301.00508) | EmoGator是一个包含32,130个样本的语音数据集，用于研究短暂的非语音发声对情感的表达，并提供了基线机器学习分类方法。 |
| [^113] | [Learning Lipschitz Functions by GD-trained Shallow Overparameterized ReLU Neural Networks.](http://arxiv.org/abs/2212.13848) | 本论文通过GD训练过程中神经切向核（NTK）近似方法，探究了过度参数化的浅层ReLU神经网络学习Lipschitz函数的能力，提出了一系列能够产生最优速率的实用早停规则。 |
| [^114] | [Automatically Bounding the Taylor Remainder Series: Tighter Bounds and New Applications.](http://arxiv.org/abs/2212.11429) | 本文提出了一种自动计算泰勒余项级数的算法，可以提供更紧密的界限，并应用于区间计算、优化等领域。 |
| [^115] | [Dataless Knowledge Fusion by Merging Weights of Language Models.](http://arxiv.org/abs/2212.09849) | 本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。 |
| [^116] | [Teaching Matters: Investigating the Role of Supervision in Vision Transformers.](http://arxiv.org/abs/2212.03862) | 本研究比较了不同的监督方法对于Vision Transformers的训练效果，并发现了一种新的注意力头类型。并证明了对于Vision Transformers而言，自监督学习是一种非常有效的学习范式。 |
| [^117] | [Denoising diffusion probabilistic models for probabilistic energy forecasting.](http://arxiv.org/abs/2212.02977) | 本文利用去噪扩散概率模型对能源（负荷、光伏或风力）的概率预测，结果表明该方法比其他深度学习生成模型具有竞争力。 |
| [^118] | [Improving Pareto Front Learning via Multi-Sample Hypernetworks.](http://arxiv.org/abs/2212.01130) | 本文提出了一个新的PFL框架PHN-HVI，利用超网络生成一组多样的解，并通过最大化这些解定义的超体积指标来提高帕累托前沿的质量。 |
| [^119] | [Classification of Melanocytic Nevus Images using BigTransfer (BiT).](http://arxiv.org/abs/2211.11872) | 该研究提出了一种通过利用BigTransfer（BiT）算法进行黑素瘤图像分类的自动化方法。 |
| [^120] | [Blurring-Sharpening Process Models for Collaborative Filtering.](http://arxiv.org/abs/2211.09324) | 本文提出了一种协同过滤的模糊-锐化过程模型（BSPM），并利用期望最大化算法学习模型参数，在显式和隐式反馈中优于现有技术方法。 |
| [^121] | [Data Quality Over Quantity: Pitfalls and Guidelines for Process Analytics.](http://arxiv.org/abs/2211.06440) | 本文介绍了如何在工业流程中获取和准备高质量的操作数据，并强调数据预处理对于人工智能应用的成功至关重要。 |
| [^122] | [Delay Embedded Echo-State Network: A Predictor for Partially Observed Systems.](http://arxiv.org/abs/2211.05992) | 本文提出了一种基于回声状态网络和时间延迟嵌入的方法来解决部分观测系统预测的问题。 |
| [^123] | [Conformal Quantitative Predictive Monitoring of STL Requirements for Stochastic Processes.](http://arxiv.org/abs/2211.02375) | 该论文提出了一种新的预测监控方法，称为定量预测监控（QPM），能够支持使用Signal Temporal Logic（STL）描述的随机过程和丰富规范，它可以量化地预测满足程度，并提供高效的计算和概率保证。 |
| [^124] | [UniASM: Binary Code Similarity Detection without Fine-tuning.](http://arxiv.org/abs/2211.01144) | 提出了一种新的二进制代码嵌入模型UniASM，并设计了两个新的训练任务，使得生成向量的空间分布更加均匀，直接可以在无需任何微调的情况下用于二进制代码相似性检测。此外，提出了一种新的二进制函数tokenization方法，缓解了词汇外的问题，并通过消融实验得到了一些新的有价值的发现，实验证明UniASM优于其他模型。 |
| [^125] | [Resource Constrained Vehicular Edge Federated Learning with Highly Mobile Connected Vehicles.](http://arxiv.org/abs/2210.15496) | 本文提出了一种在高度移动的连接车辆下，边缘服务器利用局部数据集和处理单元进行训练的边缘联邦学习解决方案，可以通过权重组合和子集选择来聚合模型参数并最大化成功接收本地训练模型的概率。 |
| [^126] | [Bayesian Optimization with Conformal Prediction Sets.](http://arxiv.org/abs/2210.12496) | 符合性贝叶斯优化在决策过程中应用符合性预测集，可以纠正由于模型规范不当和协变量转移带来的主观上不可能的结果，并在黑盒优化任务和表格排名任务中表现优异。 |
| [^127] | [Ollivier-Ricci Curvature for Hypergraphs: A Unified Framework.](http://arxiv.org/abs/2210.12048) | 本论文提出了ORCHID框架，将Ollivier-Ricci曲率推广到超图领域，并证明了其具有良好的理论特性。实验结果表明ORCHID曲率对于超图任务有很好的应用性。 |
| [^128] | [Variable-Based Calibration for Machine Learning Classifiers.](http://arxiv.org/abs/2209.15154) | 提出可变校准的概念，并发现在数据特征方面即使拥有良好 ECE 的模型还是可能存在显著的校准失误，为此提出了检测、可视化和定量化可变校准误差的策略。 |
| [^129] | [Non-contrastive representation learning for intervals from well logs.](http://arxiv.org/abs/2209.14750) | 本文提出了一种新的方法来处理井测数据表示学习问题，采用自我监督学习的方法进行非对比度的表示学习，减少对数据的标注需求，并提高了算法性能。 |
| [^130] | [TRBoost: A Generic Gradient Boosting Machine based on Trust-region Method.](http://arxiv.org/abs/2209.13791) | TRBoost 是一种新型通用梯度提升机，使用约束二次模型来近似目标并应用信赖域算法来获得新的学习器，具有适用于任意损失函数的通用性和竞争性能。 |
| [^131] | [PREF: Predictability Regularized Neural Motion Fields.](http://arxiv.org/abs/2209.10691) | 本文提出了一个名为PREF的框架，通过正则化估计的三维运动场为可预测性。该框架在多视图设置下建模动态场景中所有点的运动，并采用潜在嵌入和预测网络来实现可预测性。实验结果表明，该框架的表现优于其他基于神经运动场的动态场景表示方法。 |
| [^132] | [Real2Sim2Real Transfer for Control of Cable-driven Robots via a Differentiable Physics Engine.](http://arxiv.org/abs/2209.06261) | 本文描述了一种基于可微物理引擎的真实世界到仿真世界转移的策略，该策略通过对真实机器人的有限数据进行迭代训练，以减少实到虚之间的差距并产生准确的仿真。该策略在索驱动张力结构机器人上得到了测试，并证明了其有效性。 |
| [^133] | [The Bayan Algorithm: Detecting Communities in Networks Through Exact and Approximate Optimization of Modularity.](http://arxiv.org/abs/2209.04562) | 提出了一种名为Bayan的社区检测算法，通过精确或近似优化模块度的方法，它能够返回最优或接近最优的分区，并且比其他算法快数倍，并能够在合成和真实网络数据集上准确地找到地面真实社区。 |
| [^134] | [Delayed Feedback in Generalised Linear Bandits Revisited.](http://arxiv.org/abs/2207.10786) | 研究了广义线性赌博机中的延迟奖励现象，提出了一种自然的乐观算法，可实现一个独立于时间的惩罚函数，降低了现有工作中随着时间增长而增加的惩罚函数的界限。 |
| [^135] | [Primal Estimated Subgradient Solver for SVM for Imbalanced Classification.](http://arxiv.org/abs/2206.09311) | 本研究旨在实现对不平衡数据集的分类，并评估成本敏感的PEGASOS SVM的性能，同时将核函数纳入SVM中扩展Ding的工作。 |
| [^136] | [Accurate Node Feature Estimation with Structured Variational Graph Autoencoder.](http://arxiv.org/abs/2206.04516) | 本论文提出了一种基于结构化变分图自编码器的准确节点特征估计方法，结合了变分推断和图神经网络的优点，可以在有限的数据情况下提供准确的估计。 |
| [^137] | [Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search.](http://arxiv.org/abs/2206.00702) | AdaSubS 是一种自适应的子目标搜索方法，它采用验证机制快速过滤出不可达子目标，从而实现在计划更长的子目标的效率和在计划更短的子目标方面具有精细控制，在 Sokoban、魔方和不等式证明基准 INT 等复杂推理任务上表现优越。 |
| [^138] | [Scalable Stochastic Parametric Verification with Stochastic Variational Smoothed Model Checking.](http://arxiv.org/abs/2205.05398) | 本论文提出了一种利用概率机器学习扩展平滑模型检验(smMC)方法的思路，从而使贝叶斯推理的smMC适用于更大的数据集和实际问题。 |
| [^139] | [WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series.](http://arxiv.org/abs/2203.09978) | 该论文提出了一个名为WOODS的时间序列基准测试，致力于解决在离群分布下的泛化过程中面临的挑战，还改进了目前时间序列任务中的离群分布广义性算法，并表明仍有很大的改进空间。 |
| [^140] | [Quantum Differential Privacy: An Information Theory Perspective.](http://arxiv.org/abs/2202.10717) | 该论文讨论了量子差分隐私并以量子散度形式将其讨论在信息理论框架下，将差分隐私的属性从每个测量检查转为仅基于计算输出状态的属性，使得证明更简单、性质广泛并提供了新的限制。 |
| [^141] | [StratDef: Strategic Defense Against Adversarial Attacks in ML-based Malware Detection.](http://arxiv.org/abs/2202.07568) | 本文提出了一个名为StratDef的移动目标防御方法的战略防御系统，针对机器学习恶意软件检测的防御措施进行了全面评估。StratDef动态地和策略地选择最佳模型，以增加攻击者的不确定性，同时最小化对攻击的影响，使其具有很高的对抗鲁棒性。 |
| [^142] | [Continual Repeated Annealed Flow Transport Monte Carlo.](http://arxiv.org/abs/2201.13117) | 这篇论文提出了一种结合序列蒙特卡罗采样和标准化流变分推断的连续重复退火流输运蒙特卡罗方法，通过对标准化流的训练，实现不同温度下的传输，并在多个实例中展示了其优于其他方法的表现。 |
| [^143] | [The Complexity of Dynamic Least-Squares Regression.](http://arxiv.org/abs/2201.00228) | 本文研究了动态最小二乘回归的复杂度，证明了在部分动态设置中高精度和低精度LSR的平摊更新时间有尖锐分离，并利用在线矩阵向量猜想进行了间隙放大约简。 |
| [^144] | [Robust Upper Bounds for Adversarial Training.](http://arxiv.org/abs/2112.09279) | 该论文提出了一种新的对抗性训练方法，通过最小化基于网络的整体展开的对抗损失上界来实现。与传统方法相比，该方法利用了最新的稳健优化领域的工具，可以在保证输出层绑定紧密性的同时，有效地进行训练。 |
| [^145] | [ConDA: Unsupervised Domain Adaptation for LiDAR Segmentation via Regularized Domain Concatenation.](http://arxiv.org/abs/2111.15242) | 本文提出了一个基于拼接的激光雷达分割无监督域自适应框架ConDA，通过构建包含源域和目标域的细粒度互换信号的中间域，实现在不破坏语义连贯性的情况下进行自我训练。 |
| [^146] | [Optimism and Delays in Episodic Reinforcement Learning.](http://arxiv.org/abs/2111.07615) | 本文从理论角度探讨了延迟反馈对周期性强化学习的影响，提出了两种通用方法来处理延迟，并指出这两种方法针对乐观算法类后悔会增加一个与状态数量、动作数量、情节长度、预期延迟和算法相关常数的加性项。 |
| [^147] | [Road Network Guided Fine-Grained Urban Traffic Flow Inference.](http://arxiv.org/abs/2109.14251) | 本文提出了一种基于道路网络的交通流量推断方法，利用道路网络的先验知识全面学习细粒度交通流的道路感知空间分布，普遍适用于城市交通流量监测和调控方案。 |
| [^148] | [Missingness Augmentation: A General Approach for Improving Generative Imputation Models.](http://arxiv.org/abs/2108.02566) | 这篇论文提出了一种新的数据增强方法，称为缺失数据增强（MisA），可以用于生成式填补模型。该方法通过动态产生不完整样本，用一个简单的重构损失对增强样本进行约束，为生成式填补框架提供了一种简单而有效的提高性能的方法。 |
| [^149] | [Principal component analysis for Gaussian process posteriors.](http://arxiv.org/abs/2107.07115) | 本文提出了高斯过程后验的主成分分析扩展，解决了如何定义一组具有无限维参数的GP的结构的问题，并且证明了通过元学习提高目标任务性能的有效性。 |
| [^150] | [Spectral-Spatial Global Graph Reasoning for Hyperspectral Image Classification.](http://arxiv.org/abs/2106.13952) | 本文提出了一种全局谱空间图推理框架，通过在网络训练中生成超像素并结合光谱和空间信息进行图卷积操作，实现了在三个基准高光谱图像分类数据集上的最先进性能。 |
| [^151] | [Convolutional Neural Networks with Intermediate Loss for 3D Super-Resolution of CT and MRI Scans.](http://arxiv.org/abs/2001.01330) | 本文提出了一种用于单张图像的 3D CT 或 MRI 扫描超分辨率的方法，采用深度卷积神经网络并引入中间损失函数以缓解梯度消失问题并提高准确性。实验结果表明该方法在量化度量和视觉质量方面均优于现有方法。 |
| [^152] | [Sequential Adversarial Anomaly Detection for One-Class Event Data.](http://arxiv.org/abs/1910.09161) | 本文提出了一种对抗顺序检测器，使用带标记的点过程模型捕捉序列事件中的相关性，可以应用于检测异常序列，通过解决最小最大问题，针对最坏情况的生成器，找到最佳检测器。 |

# 详细

[^1]: 奖励是否合理？在 MACHIAVELLI 基准测试中衡量奖励与道德行为之间的权衡

    Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])

    [http://arxiv.org/abs/2304.03279](http://arxiv.org/abs/2304.03279)

    本文介绍了 MACHIAVELLI 基准测试，用于衡量人工智能代理是否表现出马基雅维利行为，发现了最大化奖励和行为的道德性之间存在权衡，并探索了基于语言模型的方法来减轻这种权衡。

    

    传统上，人工智能代理被训练成最大化奖励，这可能会激励追求权力和欺骗行为，类似于语言模型中的下一个标记预测可能会激励有害行为。那么代理是否自然而然地学会了马基雅维利行为？我们如何在 GPT-4 等通用模型中衡量这些行为呢？为回答这些问题，我们引入了 MACHIAVELLI 基准测试，该测试涵盖了超过一百万个多样化的情景，重点关注社会决策制定，用于衡量人工代理是否表现出马基雅维利行为。我们数学化了数十种有害行为，并使用我们的注释来评估代理倾向于追求权力，造成功能不良和违反伦理的倾向。我们观察到最大化奖励和行为的道德性之间存在一些紧张关系。为了改善这种权衡，我们研究了基于语言模型的方法，以使代理趋向于采取更少的有害行为。我们的结果显示，MACHIAVELLI 是评估人工代理马基雅维利行为水平的有用基准测试。

    Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
    
[^2]: DiffMimic: 基于可微分物理的高效运动模仿

    DiffMimic: Efficient Motion Mimicking with Differentiable Physics. (arXiv:2304.03274v1 [cs.CV])

    [http://arxiv.org/abs/2304.03274](http://arxiv.org/abs/2304.03274)

    本文提出了DiffMimic，一种基于可微分物理的高效运动模仿方法。与传统强化学习方法相比，其有更快更稳定的收敛速度；同时通过演示重播机制避免陷入局部最优解。

    

    运动模仿是基于物理的角色动画中的基础任务，然而大多数现有的运动模仿方法都建立在强化学习（RL）之上，存在重度奖励工程、高方差和难以探索的收敛速度缓慢等问题。本文提出了一种基于可微分物理模拟器（DPS）的运动模仿方法，名为DiffMimic，通过分析梯度和基于真实物理先验学习稳定策略，从而实现显著更快和更稳定的收敛。此外，为了避免陷入局部最优解，我们还利用演示重播机制，在长时间跨度内实现稳定梯度反向传播。

    Motion mimicking is a foundational task in physics-based character animation. However, most existing motion mimicking methods are built upon reinforcement learning (RL) and suffer from heavy reward engineering, high variance, and slow convergence with hard explorations. Specifically, they usually take tens of hours or even days of training to mimic a simple motion sequence, resulting in poor scalability. In this work, we leverage differentiable physics simulators (DPS) and propose an efficient motion mimicking method dubbed DiffMimic. Our key insight is that DPS casts a complex policy learning task to a much simpler state matching problem. In particular, DPS learns a stable policy by analytical gradients with ground-truth physical priors hence leading to significantly faster and stabler convergence than RL-based methods. Moreover, to escape from local optima, we utilize a Demonstration Replay mechanism to enable stable gradient backpropagation in a long horizon. Extensive experiments o
    
[^3]: 使AI“口渴”减少的方法：揭示和解决AI模型的秘密水消耗

    Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])

    [http://arxiv.org/abs/2304.03271](http://arxiv.org/abs/2304.03271)

    本论文揭示以及提出了解决人工智能模型巨大水足迹的方法，因为其淡水消耗已经引起国际社会的重视，并且AI模型应该承担社会责任，做出面对水危机的表率。

    

    人工智能（AI）模型的碳足迹不断增长，特别是像GPT-3和GPT-4这样的大型模型，已经受到公众的关注。然而，同等重要且巨大的AI模型水印尚未引起人们的注意。例如，在微软最先进的美国数据中心中训练GPT-3可以直接消耗70万升清洁淡水（相当于生产370辆宝马汽车或320辆特斯拉电动汽车），如果在微软的亚洲数据中心进行训练，这个水消耗量将增加三倍，但这样的信息一直被保密。这极其令人担忧，因为淡水短缺已成为在人口迅速增长、水资源减少和老化的水基础设施的背景下，我们所有人面临的最紧迫的挑战之一。为了应对全球水资源的挑战，人工智能模型可以，而且应该，承担社会责任，以身作则解决自己的问题。

    The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
    
[^4]: 任意噪声下基于积分算法的潜在因果性发现方法

    Causal Discovery with Score Matching on Additive Models with Arbitrary Noise. (arXiv:2304.03265v1 [cs.LG])

    [http://arxiv.org/abs/2304.03265](http://arxiv.org/abs/2304.03265)

    本文提出了一种新的因果性发现算法NoGAM，它仅仅需要最少量的假设，并基于加性非线性模型来推断因果图中变量的拓扑顺序，而且在合成数据上性能表现最优。

    

    因果推断方法受固有的结构可识别性假设所限制。此外，为了简化推断任务，通常还会施加其他限制，例如许多因果推断方法中所共有的对于加性非线性模型的高斯噪声假设。本文分析了在违反高斯噪声假设的情况下边缘反转的风险，并提出了一种新颖的方法来推断因果图中变量的拓扑顺序，其可适用于由加性非线性模型生成的具有通用噪声分布的数据。因此，本文提出了一个因果性发现算法NoGAM（Not only Gaussian Additive noise Models），该算法仅需要最少量的假设，且具有最先进的性能，在合成数据上进行了实验验证。

    Causal discovery methods are intrinsically constrained by the set of assumptions needed to ensure structure identifiability. Moreover additional restrictions are often imposed in order to simplify the inference task: this is the case for the Gaussian noise assumption on additive non-linear models, which is common to many causal discovery approaches. In this paper we show the shortcomings of inference under this hypothesis, analyzing the risk of edge inversion under violation of Gaussianity of the noise terms. Then, we propose a novel method for inferring the topological ordering of the variables in the causal graph, from data generated according to an additive non-linear model with a generic noise distribution. This leads to NoGAM (Not only Gaussian Additive noise Models), a causal discovery algorithm with a minimal set of assumptions and state of the art performance, experimentally benchmarked on synthetic data.
    
[^5]: 一种在不可避免风险存在下进行复发事件因果分析的贝叶斯框架

    A Bayesian Framework for Causal Analysis of Recurrent Events in Presence of Immortal Risk. (arXiv:2304.03247v1 [stat.ME])

    [http://arxiv.org/abs/2304.03247](http://arxiv.org/abs/2304.03247)

    论文提出了一种贝叶斯框架，针对错位处理问题，将其视为治疗切换问题，并通过概率模型解决了复增和末事件偏差的问题。

    

    生物医学统计学中对复发事件率的观测研究很常见。通常的目标是在规定的随访时间窗口内，估计在一个明确定义的目标人群中两种治疗方法的事件率差异。使用观测性索赔数据进行估计是具有挑战性的，因为在目标人群的成员资格方面定义时，很少在资格确认时准确分配治疗方式。目前的解决方案通常是错位处理，比如基于后续分配，在资格确认时分配治疗方式，这会将先前的事件率错误地归因于治疗-从而产生不可避免的风险偏差。即使资格和治疗已经对齐，终止事件过程（例如死亡）也经常停止感兴趣的复发事件过程。同样，这两个过程也受到审查的影响，因此在整个随访时间窗口内不能观察到事件。我们的方法将错位处理转化为治疗切换问题：一些患者在整个随访时间窗口内坚持一个特定的治疗策略，另一些患者在这个时间窗口内经历治疗策略的切换。我们提出了一个概率模型，其中包括两个基本元素：通过一个合理的时刻切换模型，正确地建模治疗之间的切换和不可避免风险，通过将非观察事件模型化为复发事件模型，解决了复增和末事件偏差的问题。

    Observational studies of recurrent event rates are common in biomedical statistics. Broadly, the goal is to estimate differences in event rates under two treatments within a defined target population over a specified followup window. Estimation with observational claims data is challenging because while membership in the target population is defined in terms of eligibility criteria, treatment is rarely assigned exactly at the time of eligibility. Ad-hoc solutions to this timing misalignment, such as assigning treatment at eligibility based on subsequent assignment, incorrectly attribute prior event rates to treatment - resulting in immortal risk bias. Even if eligibility and treatment are aligned, a terminal event process (e.g. death) often stops the recurrent event process of interest. Both processes are also censored so that events are not observed over the entire followup window. Our approach addresses misalignment by casting it as a treatment switching problem: some patients are on
    
[^6]: 评估机器学习在帕金森病生物标记发现中的可重复性

    Assessing the Reproducibility of Machine-learning-based Biomarker Discovery in Parkinson's Disease. (arXiv:2304.03239v1 [q-bio.GN])

    [http://arxiv.org/abs/2304.03239](http://arxiv.org/abs/2304.03239)

    本文评估了机器学习方法在生物标记发现中的可重复性，结果表明不同的数据集成策略在确定潜在PD生物标志物的SNPs上的一致性较低。

    

    基因组关联性研究(GWAS)有助于发现在类似帕金森病(PD)等疾病的患者中较为普遍的遗传变异。因此，GWAS数据可以用于确定与该疾病相关的遗传变异。特征选择和机器学习方法可用于分析GWAS数据并识别潜在的疾病生物标志物。然而，GWAS研究存在技术差异，这些差异会影响识别的生物标记的可重复性，例如基因分型平台的差异以及选择被基因分型的个体的标准的不同。为了解决这个问题，我们收集了来自Genotypes and Phenotypes (dbGaP)数据库的五个GWAS数据集，并探索了几种数据集成策略。我们评估了不同策略在识别潜在PD生物标记物的单核苷酸多态性（SNP）方面的一致性。我们的结果显示，使用不同的数据集成策略发现的生物标记的一致性较低，这表明在使用机器学习方法识别基因生物标记时应谨慎报告。

    Genome-Wide Association Studies (GWAS) help identify genetic variations in people with diseases such as Parkinson's disease (PD), which are less common in those without the disease. Thus, GWAS data can be used to identify genetic variations associated with the disease. Feature selection and machine learning approaches can be used to analyze GWAS data and identify potential disease biomarkers. However, GWAS studies have technical variations that affect the reproducibility of identified biomarkers, such as differences in genotyping platforms and selection criteria for individuals to be genotyped. To address this issue, we collected five GWAS datasets from the database of Genotypes and Phenotypes (dbGaP) and explored several data integration strategies. We evaluated the agreement among different strategies in terms of the Single Nucleotide Polymorphisms (SNPs) that were identified as potential PD biomarkers. Our results showed a low concordance of biomarkers discovered using different dat
    
[^7]: FedBot：利用联邦学习增强聊天机器人的隐私保护

    FedBot: Enhancing Privacy in Chatbots with Federated Learning. (arXiv:2304.03228v1 [cs.CL])

    [http://arxiv.org/abs/2304.03228](http://arxiv.org/abs/2304.03228)

    本论文提出了一个利用联邦学习保护用户隐私的聊天机器人FedBot。它结合了Deep Bidirectional Transformer模型和联邦学习算法，在联合模型训练过程中保护客户数据隐私。

    

    聊天机器人主要依赖于包含敏感信息的话语的数据推动，但是在共享数据上训练深度学习模型可能会侵犯用户隐私。本文提出FedBot，一个利用大规模客户支持数据实现隐私保护的聊天机器人的概念验证，它结合了Deep Bidirectional Transformer模型和联邦学习算法，在联合模型训练过程中保护客户数据隐私。概念验证的结果展示了隐私保护聊天机器人能够通过改变客户支持行业的潜力。

    Chatbots are mainly data-driven and usually based on utterances that might be sensitive. However, training deep learning models on shared data can violate user privacy. Such issues have commonly existed in chatbots since their inception. In the literature, there have been many approaches to deal with privacy, such as differential privacy and secure multi-party computation, but most of them need to have access to users' data. In this context, Federated Learning (FL) aims to protect data privacy through distributed learning methods that keep the data in its location. This paper presents Fedbot, a proof-of-concept (POC) privacy-preserving chatbot that leverages large-scale customer support data. The POC combines Deep Bidirectional Transformer models and federated learning algorithms to protect customer data privacy during collaborative model training. The results of the proof-of-concept showcase the potential for privacy-preserving chatbots to transform the customer support industry by de
    
[^8]: DexDeform：基于人类示范和可微分物理的巧妙可变形物体操纵

    DexDeform: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics. (arXiv:2304.03223v1 [cs.CV])

    [http://arxiv.org/abs/2304.03223](http://arxiv.org/abs/2304.03223)

    本文提出了DexDeform，借助人类示范和不同iable物理学习灵巧操纵可变形物体的技能，可在真实环境中高效实现。

    

    本文旨在利用多指手学习可变形物体的灵巧操纵。强化学习方法在可变形物体中的物理相互作用复杂性下可能会受到阻碍，并且先前基于可微分物理的轨迹优化方法也可能因为手-物体交互引起的接触模式增加而受到局部极小值的影响。因此，本文提出了DexDeform——一个基于人类示范的巧妙操纵技能抽象并通过可微分物理进行精化的原则框架。实验结果表明，我们的方法在操作成功率和效率方面明显优于以前的方法。

    In this work, we aim to learn dexterous manipulation of deformable objects using multi-fingered hands. Reinforcement learning approaches for dexterous rigid object manipulation would struggle in this setting due to the complexity of physics interaction with deformable objects. At the same time, previous trajectory optimization approaches with differentiable physics for deformable manipulation would suffer from local optima caused by the explosion of contact modes from hand-object interactions. To address these challenges, we propose DexDeform, a principled framework that abstracts dexterous manipulation skills from human demonstration and refines the learned skills with differentiable physics. Concretely, we first collect a small set of human demonstrations using teleoperation. And we then train a skill model using demonstrations for planning over action abstractions in imagination. To explore the goal space, we further apply augmentations to the existing deformable shapes in demonstra
    
[^9]: 通过Gumbel噪声分数匹配进行异常检测

    Anomaly Detection via Gumbel Noise Score Matching. (arXiv:2304.03220v1 [cs.LG])

    [http://arxiv.org/abs/2304.03220](http://arxiv.org/abs/2304.03220)

    该论文提出了一种通过估计连续松弛分类分布的得分来检测分类数据中的异常的无监督方法，该方法在异常检测表格数据集和图像数据中均表现出持续优异的性能。

    

    我们提出了一种新的无监督方法，通过估计连续松弛分类分布的得分（即与输入相关的对数似然梯度）来检测分类数据中的异常。我们在一系列异常检测表格数据集上测试了我们的方法。 GNSM在所有实验中均表现出持续优异的性能。我们通过将其应用于图像数据进一步展示了GNSM的灵活性，其中模型的任务是检测不良分割预测。 GNSM排名异常的图像显示出明显的分割失败，GNSM输出的结果与基于地面真实值计算的分割度量高度相关。我们概述了GNSM使用的得分匹配训练目标，并提供了我们工作的开源实现。

    We propose Gumbel Noise Score Matching (GNSM), a novel unsupervised method to detect anomalies in categorical data. GNSM accomplishes this by estimating the scores, i.e. the gradients of log likelihoods w.r.t.~inputs, of continuously relaxed categorical distributions. We test our method on a suite of anomaly detection tabular datasets. GNSM achieves a consistently high performance across all experiments. We further demonstrate the flexibility of GNSM by applying it to image data where the model is tasked to detect poor segmentation predictions. Images ranked anomalous by GNSM show clear segmentation failures, with the outputs of GNSM strongly correlating with segmentation metrics computed on ground-truth. We outline the score matching training objective utilized by GNSM and provide an open-source implementation of our work.
    
[^10]: 带有跨设备交叉注意力的分层图神经网络用于跨设备用户匹配

    Hierarchical Graph Neural Network with Cross-Attention for Cross-Device User Matching. (arXiv:2304.03215v1 [cs.LG])

    [http://arxiv.org/abs/2304.03215](http://arxiv.org/abs/2304.03215)

    本文提出了一种带有跨设备交叉注意力的分层图神经网络(HGNN)，用于解决跨设备用户匹配问题，相对于最先进的TGCE方法，提高了5%的性能。

    

    在广告、推荐系统和网络安全等众多领域，跨设备用户匹配是一个关键问题。它涉及使用序列日志来识别和链接属于同一人的不同设备。以往的数据挖掘技术难以解决日志之间的长程依赖和高阶连接问题。最近，研究人员将这个问题建模为图问题，并提出了一种两层图上下文嵌入(TGCE)神经网络架构，表现优于先前的方法。在本文中，我们提出了一种新颖的分层图神经网络架构（HGNN），它具有比TGCE更为计算效率的二级设计。此外，我们在模型中引入了一种跨设备交叉注意力（Cross-Att）机制，相对于最先进的TGCE方法，提高了5%的性能。

    Cross-device user matching is a critical problem in numerous domains, including advertising, recommender systems, and cybersecurity. It involves identifying and linking different devices belonging to the same person, utilizing sequence logs. Previous data mining techniques have struggled to address the long-range dependencies and higher-order connections between the logs. Recently, researchers have modeled this problem as a graph problem and proposed a two-tier graph contextual embedding (TGCE) neural network architecture, which outperforms previous methods. In this paper, we propose a novel hierarchical graph neural network architecture (HGNN), which has a more computationally efficient second level design than TGCE. Furthermore, we introduce a cross-attention (Cross-Att) mechanism in our model, which improves performance by 5% compared to the state-of-the-art TGCE method.
    
[^11]: 基于随机专家的医学图像分割的隐性解剖渲染

    Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts. (arXiv:2304.03209v1 [cs.CV])

    [http://arxiv.org/abs/2304.03209](http://arxiv.org/abs/2304.03209)

    提出了一种名为MORSE的基于隐式解剖渲染的通用神经渲染框架，旨在在医学图像分割中帮助融合高级语义相关内容和低级解剖特征。

    

    将高级语义相关内容和低级解剖特征集成到医学图像分割中非常重要。近期基于深度学习的医学分割方法在更好地建模这些信息方面取得了很大的成功。然而，医学分割的卷积操作通常在规则网格上运行，这在高频区域即边界区域中天生模糊。本文提出了一个名为MORSE的通用隐式神经渲染框架，旨在在解剖层面上为医学图像分割辅助学习。我们的方法基于事实：相较于离散的基于网格的表示方式，隐式神经表示在拟合复杂信号和解决计算机图形问题时表现更为有效。我们的方法的核心是以端到端的方式将医学图像分割视为渲染问题。具体而言，我们持续地对齐粗略的分割p并利用随机专家来生成渲染图像。

    Integrating high-level semantically correlated contents and low-level anatomical features is of central importance in medical image segmentation. Towards this end, recent deep learning-based medical segmentation methods have shown great promise in better modeling such information. However, convolution operators for medical segmentation typically operate on regular grids, which inherently blur the high-frequency regions, i.e., boundary regions. In this work, we propose MORSE, a generic implicit neural rendering framework designed at an anatomical level to assist learning in medical image segmentation. Our method is motivated by the fact that implicit neural representation has been shown to be more effective in fitting complex signals and solving computer graphics problems than discrete grid-based representation. The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. Specifically, we continuously align the coarse segmentation p
    
[^12]: 基于 Cerebras Wafer-Scale Cluster 的开放计算优化语言模型 Cerebras-GPT 的研究

    Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster. (arXiv:2304.03208v1 [cs.LG])

    [http://arxiv.org/abs/2304.03208](http://arxiv.org/abs/2304.03208)

    本论文介绍了从111M到13B参数的开放计算优化语言模型 Cerebras-GPT，它采用了高效的预训练方法和缩放规则，具有最先进的训练效率和可预测性，这是一个开放可复现的工作。

    

    本文研究了最近改善大型语言模型的有效预训练和扩展以及开放数据集和工具的研究进展。同时结合这些进展，介绍了一系列从111M到13B参数的开放计算优化语言模型 Cerebras-GPT。我们根据 DeepMind 的 Chinchilla 缩放规则对 Eleuther Pile 数据集进行训练，达到了在给定计算预算下最高精度的高效预训练。我们表征了可预测的幂律缩放规律，并与其他公开可用模型进行比较，展示了 Cerebras-GPT 具有最先进的预训练和下游目标训练效率。我们描述了我们的发现，包括最大更新参数化($\mu$P)如何进一步提高大型模型扩展的精度和超参数可预测性。我们发布了预训练模型和代码，使本文成为关于在亿级参数规模下比较计算优化语言模型的首个开放可复现的工作。

    We study recent research advances that improve large language models through efficient pre-training and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the predictable power-law scaling and compare Cerebras-GPT with other publicly-available models to show all Cerebras-GPT models have state-of-the-art training efficiency on both pre-training and downstream objectives. We describe our learnings including how Maximal Update Parameterization ($\mu$P) can further improve large model scaling, improving accuracy and hyperparameter predictability at scale. We release our pre-trained models and code, making this paper the first open and reproducible work comparing compute-optimal 
    
[^13]: SLM：通过稀疏可学习掩码进行端到端特征选择

    SLM: End-to-end Feature Selection via Sparse Learnable Masks. (arXiv:2304.03202v1 [cs.LG])

    [http://arxiv.org/abs/2304.03202](http://arxiv.org/abs/2304.03202)

    SLM是一种经典的端到端特征选择方法，通过可学习的稀疏掩码来最大化所选特征和标签之间的互信息，同时精确地控制选择的特征数量，实验结果表明其取得了最先进的结果。

    

    特征选择已被广泛应用于减少训练过程中的计算要求，阐明模型的可解释性以及提高模型的泛化能力。本文提出了SLM - 稀疏可学习掩码，这是一种经典的端到端特征选择方法，可以很好地适应特征维度和样本数量的变化。SLM的核心是一个简单但有效的可学习稀疏掩码，它学习选择哪些特征，并产生一种新的目标函数，可以证明它最大化了所选特征和标签之间的互信息（MI），这可以从互信息的一次方根松弛中导出。此外，我们还推导出一种缩放机制，利用sparsemax精确地控制选择的特征数量。通过消融实验证明了其有效性。在实验中，SLM在各种竞争基线模型上取得了最先进的结果。

    Feature selection has been widely used to alleviate compute requirements during training, elucidate model interpretability, and improve model generalizability. We propose SLM -- Sparse Learnable Masks -- a canonical approach for end-to-end feature selection that scales well with respect to both the feature dimension and the number of samples. At the heart of SLM lies a simple but effective learnable sparse mask, which learns which features to select, and gives rise to a novel objective that provably maximizes the mutual information (MI) between the selected features and the labels, which can be derived from a quadratic relaxation of mutual information from first principles. In addition, we derive a scaling mechanism that allows SLM to precisely control the number of features selected, through a novel use of sparsemax. This allows for more effective learning as demonstrated in ablation studies. Empirically, SLM achieves state-of-the-art results against a variety of competitive baselines
    
[^14]: 采用多视图融合及两步式迁移学习增强的自动内镜结石识别方法

    Improving automatic endoscopic stone recognition using a multi-view fusion approach enhanced with two-step transfer learning. (arXiv:2304.03193v1 [eess.IV])

    [http://arxiv.org/abs/2304.03193](http://arxiv.org/abs/2304.03193)

    本文介绍了一种利用多视图融合及两步式迁移学习增强的自动内镜结石识别方法，可提高肾结石分类准确度达6%以上。

    

    本文提出了一种深度学习方法，用于提取和融合从不同视角获取的图像信息，旨在为鉴定内镜图像中所见的肾结石类型产生更具区分性的物体特征。本模型进一步应用了两步迁移学习方法和注意力块来调整学习的特征图。深度特征融合策略将肾结石分类准确度比单视图提取主干模型提高了6%以上。

    This contribution presents a deep-learning method for extracting and fusing image information acquired from different viewpoints, with the aim to produce more discriminant object features for the identification of the type of kidney stones seen in endoscopic images. The model was further improved with a two-step transfer learning approach and by attention blocks to refine the learned feature maps. Deep feature fusion strategies improved the results of single view extraction backbone models by more than 6% in terms of accuracy of the kidney stones classification.
    
[^15]: Krylov方法在低秩逼近中（几乎）最优

    Krylov Methods are (nearly) Optimal for Low-Rank Approximation. (arXiv:2304.03191v1 [cs.DS])

    [http://arxiv.org/abs/2304.03191](http://arxiv.org/abs/2304.03191)

    本文研究在矩阵-向量相乘模型下针对各种Schatten范数的秩-1低秩逼近问题，表明Krylov方法（几乎）实现谱，Frobenius和核低秩逼近的信息熵上的最优矩阵-向量积数量。

    

    本文研究在矩阵-向量相乘模型下，针对各种Schatten范数的秩-1低秩逼近问题：$$\min_{\|u\|_2=1} \|A(I - u u^\top)\|_{\mathcal{S}_p}$$ 其中$\|M\|_{\mathcal{S}_p}$表示$M$的奇异值的$\ell_p$范数。给定$\varepsilon>0$，我们的目标是输出一个单位向量$v$，使得$$\|A(I - vv^\top)\|_{\mathcal{S}_p} \leq (1+\varepsilon) \min_{\|u\|_2=1}\|A(I - u u^\top)\|_{\mathcal{S}_p}.$$ 我们的主要结果表明，Krylov方法（几乎）实现谱（$p=\infty$），Frobenius（$p=2$）和核（$p=1$）低秩逼近的信息熵上的最优矩阵-向量积数量。特别是，对于谱低秩逼近，我们展示任何算法都需要$\Omega\left(\log(n)/\varepsilon^{1/2}\right)$个矩阵-向量积，完全匹配Krylov方法[MM15, BCW22]得到的上界。我们的下界回答了[Woo14]的问题1，为算法的缺乏进展提供了证据。

    We consider the problem of rank-$1$ low-rank approximation (LRA) in the matrix-vector product model under various Schatten norms: $$  \min_{\|u\|_2=1} \|A (I - u u^\top)\|_{\mathcal{S}_p} , $$ where $\|M\|_{\mathcal{S}_p}$ denotes the $\ell_p$ norm of the singular values of $M$. Given $\varepsilon>0$, our goal is to output a unit vector $v$ such that $$  \|A(I - vv^\top)\|_{\mathcal{S}_p} \leq (1+\varepsilon) \min_{\|u\|_2=1}\|A(I - u u^\top)\|_{\mathcal{S}_p}. $$ Our main result shows that Krylov methods (nearly) achieve the information-theoretically optimal number of matrix-vector products for Spectral ($p=\infty$), Frobenius ($p=2$) and Nuclear ($p=1$) LRA.  In particular, for Spectral LRA, we show that any algorithm requires $\Omega\left(\log(n)/\varepsilon^{1/2}\right)$ matrix-vector products, exactly matching the upper bound obtained by Krylov methods [MM15, BCW22]. Our lower bound addresses Open Question 1 in [Woo14], providing evidence for the lack of progress on algorithms for
    
[^16]: 应用于多输出感知机的前向前向学习概念

    The Concept of Forward-Forward Learning Applied to a Multi Output Perceptron. (arXiv:2304.03189v1 [cs.LG])

    [http://arxiv.org/abs/2304.03189](http://arxiv.org/abs/2304.03189)

    本文应用前向前向学习算法于多输出感知机，仅需单个矩阵乘法，有效地处理数据集，性能与隐藏层更多的复杂神经网络相当。

    

    本文将最近提出的前向前向学习算法应用于单个多输出感知机模型中，以进行分类。系统参数的训练是基于对输入样本的正确（错误）标记的“好度”增加（减少）。基本的数值测试结果表明，经过训练的多输出感知机有效地处理具有非线性决策边界的数据集。此外，其性能总体上与隐藏层更多的复杂神经网络相当。所提出的方法的优点在于仅涉及单个矩阵乘法。

    The concept of a recently proposed Forward-Forward learning algorithm for fully connected artificial neural networks is applied to a single multi output perceptron for classification. The parameters of the system are trained with respect to increased (decreased) "goodness" for correctly (incorrectly) labelled input samples. Basic numerical tests demonstrate that the trained perceptron effectively deals with data sets that have non-linear decision boundaries. Moreover, the overall performance is comparable to more complex neural networks with hidden layers. The benefit of the approach presented here is that it only involves a single matrix multiplication.
    
[^17]: 带高斯核的成对排名

    Pairwise Ranking with Gaussian Kernels. (arXiv:2304.03185v1 [stat.ML])

    [http://arxiv.org/abs/2304.03185](http://arxiv.org/abs/2304.03185)

    本文提出新的Oracle不等式，在输入域上的一般盒计数维度假设和噪声条件或标准平滑条件下，对于高斯成对排名估计器得出了快速学习率。这表明，输入空间的低固有维度可以帮助避免维度诅咒。

    

    带高斯核的正则成对排名是前沿的学习算法之一。尽管应用范围广泛，但缺乏严格的理论证明来支持这种排名估计器的性能。本文旨在通过为正则成对排名开发新的 Oracle 不等式来填补这一空白。借助这些 Oracle 不等式，结合输入域上的一般盒计数维度假设和噪声条件或标准平滑条件，我们推导出高斯排名估计器的快速学习率。我们的理论分析改进了现有的估计，并显示输入空间的低固有维度可以帮助速率避免维度诅咒。

    Regularized pairwise ranking with Gaussian kernels is one of the cutting-edge learning algorithms. Despite a wide range of applications, a rigorous theoretical demonstration still lacks to support the performance of such ranking estimators. This work aims to fill this gap by developing novel oracle inequalities for regularized pairwise ranking. With the help of these oracle inequalities, we derive fast learning rates of Gaussian ranking estimators under a general box-counting dimension assumption on the input domain combined with the noise conditions or the standard smoothness condition. Our theoretical analysis improves the existing estimates and shows that a low intrinsic dimension of input space can help the rates circumvent the curse of dimensionality.
    
[^18]: 以深度学习为基础的图像曝光增强作为准确的三维结肠表面重建的预处理

    Deep learning-based image exposure enhancement as a pre-processing for an accurate 3D colon surface reconstruction. (arXiv:2304.03171v1 [eess.IV])

    [http://arxiv.org/abs/2304.03171](http://arxiv.org/abs/2304.03171)

    本文研究了基于深度学习的三维结肠表面重建，在图像预处理的过程中通过纠正局部欠曝光和过曝光来提高重建精度。

    

    本文介绍了如何通过适当的图像预处理改善基于深度学习的结肠部分三维重建。假设在内窥镜检查中应该纠正局部欠曝光和过曝光而不是全局图像照明校正。首先概述了包括图像曝光校正和循环神经网络-SLAM的流程。然后，本文比较了在与适当照明校正和无校正的情况下结肠内镜轨迹的重建精度。

    This contribution shows how an appropriate image pre-processing can improve a deep-learning based 3D reconstruction of colon parts. The assumption is that, rather than global image illumination corrections, local under- and over-exposures should be corrected in colonoscopy. An overview of the pipeline including the image exposure correction and a RNN-SLAM is first given. Then, this paper quantifies the reconstruction accuracy of the endoscope trajectory in the colon with and without appropriate illumination correction
    
[^19]: 图谱算法的频谱工具包：技术报告（1）

    Spectral Toolkit of Algorithms for Graphs: Technical Report (1). (arXiv:2304.03170v1 [cs.SI])

    [http://arxiv.org/abs/2304.03170](http://arxiv.org/abs/2304.03170)

    频谱工具包STAG是一个开源库，用于实现高效的频谱图算法，其中还包括本地图聚类组件。此技术报告介绍了STAG的用户指南、展示研究以及开发背后的技术考虑。

    

    图谱算法的频谱工具包（STAG）是一个用于高效频谱图算法的开源库，其开发始于2022年9月。我们目前已经完成了本地图聚类的组件，并且本技术报告介绍了STAG的用户指南、展示研究以及我们开发背后的几个技术考虑。

    Spectral Toolkit of Algorithms for Graphs (STAG) is an open-source library for efficient spectral graph algorithms, and its development starts in September 2022. We have so far finished the component on local graph clustering, and this technical report presents a user's guide to STAG, showcase studies, and several technical considerations behind our development.
    
[^20]: 带一般范数目标的聚类参数近似方案

    Parameterized Approximation Schemes for Clustering with General Norm Objectives. (arXiv:2304.03146v1 [cs.DS])

    [http://arxiv.org/abs/2304.03146](http://arxiv.org/abs/2304.03146)

    提出了一种简洁的EPAS，解决了多个聚类问题，并统一了已有的EPAS

    

    本文考虑设计运行时间为$f(k, \epsilon)poly(n)$的$k$-聚类问题的$(1+\epsilon)$-近似算法。已有结果处理基本目标（如$k$-中心，$k$-中位数和$k$-均值）的问题，但是仅适用于特定的目标和度量空间。本文的主要贡献是提出一种简单的EPAS，解决了多个聚类问题并统一了众所周知的EPAS。

    This paper considers the well-studied algorithmic regime of designing a $(1+\epsilon)$-approximation algorithm for a $k$-clustering problem that runs in time $f(k,\epsilon)poly(n)$ (sometimes called an efficient parameterized approximation scheme or EPAS for short). Notable results of this kind include EPASes in the high-dimensional Euclidean setting for $k$-center [Bad\u{o}iu, Har-Peled, Indyk; STOC'02] as well as $k$-median, and $k$-means [Kumar, Sabharwal, Sen; J. ACM 2010]. However, existing EPASes handle only basic objectives (such as $k$-center, $k$-median, and $k$-means) and are tailored to the specific objective and metric space.  Our main contribution is a clean and simple EPAS that settles more than ten clustering problems (across multiple well-studied objectives as well as metric spaces) and unifies well-known EPASes. Our algorithm gives EPASes for a large variety of clustering objectives (for example, $k$-means, $k$-center, $k$-median, priority $k$-center, $\ell$-centrum, o
    
[^21]: 通过因果结构学习实现高效的SAGE估计

    Efficient SAGE Estimation via Causal Structure Learning. (arXiv:2304.03113v1 [stat.ML])

    [http://arxiv.org/abs/2304.03113](http://arxiv.org/abs/2304.03113)

    提出了一种通过因果结构学习的方法名为d-SAGE，用于加速SAGE逼近算法，显著降低计算开销和提高计算效率，并在理论上展示了$d$-SAGE的逼近误差会收敛于零，实验上体现了高精度。

    

    Shapley Additive Global Importance (SAGE)是一种理论上有吸引力的可解释性方法，它公平地将全局重要性归因于模型的特征。然而，它的精确计算需要计算特征集的指数数量的剩余性能贡献，这在计算上非常昂贵，尤其是因为估计剩余性能贡献需要从条件分布中采样。因此，SAGE逼近算法只考虑了一小部分特征集。我们提出了一种名为$d$-SAGE的方法，它可以加速SAGE逼近。$d$-SAGE是由于观察到特征和模型目标之间的条件独立性 (CI) 意味着零剩余贡献，因此可以跳过它们的计算。为了识别CI，我们利用因果结构学习(CSL)来推断一个图，该图将数据中的(条件)独立性编码为$d$分离。这在计算上更有效，因为我们只需要计算非$d$分离特征集的SAGE值。我们提供了理论保证，说明随着$d$的增加，$d$-SAGE的逼近误差会收敛于零。在实验上，我们证明了$d$-SAGE需要比现有的SAGE逼近算法更少的特征集，同时保持高精度。

    The Shapley Additive Global Importance (SAGE) value is a theoretically appealing interpretability method that fairly attributes global importance to a model's features. However, its exact calculation requires the computation of the feature's surplus performance contributions over an exponential number of feature sets. This is computationally expensive, particularly because estimating the surplus contributions requires sampling from conditional distributions. Thus, SAGE approximation algorithms only take a fraction of the feature sets into account. We propose $d$-SAGE, a method that accelerates SAGE approximation. $d$-SAGE is motivated by the observation that conditional independencies (CIs) between a feature and the model target imply zero surplus contributions, such that their computation can be skipped. To identify CIs, we leverage causal structure learning (CSL) to infer a graph that encodes (conditional) independencies in the data as $d$-separations. This is computationally more ef
    
[^22]: 神经网络的谱间隙正则化

    Spectral Gap Regularization of Neural Networks. (arXiv:2304.03096v1 [stat.ML])

    [http://arxiv.org/abs/2304.03096](http://arxiv.org/abs/2304.03096)

    本文介绍了一种利用谱/图形信息进行神经网络正则化的新方法，即Fiedler正则化。通过使用神经网络底层图的Fiedler值作为正则化工具，我们提供了一种结构加权的 $\text{L}_1$ 惩罚并提供统一泛化误差界限的分析，这使得我们的方法在许多数据集上都取得了良好的性能。

    

    本文引入了Fiedler正则化，这是一种利用谱/图形信息对神经网络进行正则化的新方法。现有的正则化方法常常通过全局/均匀地惩罚权重来实现，忽略了神经网络的连通性结构。我们提出利用神经网络底层图的Fiedler值作为正则化工具。我们通过谱图理论提供了这种方法的理论动机。我们证明了Fiedler值的几个有用属性，使其成为正则化工具。我们提供了一种近似的变分方法，以便在训练期间更快地计算。我们提供了该框架的另一种形式，这是一种结构加权的 $\text{L}_1$ 惩罚，因此将我们的方法与稀疏感应联系起来。我们通过Rademacher复杂性分析提供了Fiedler正则化的统一泛化误差界限。我们对数据集进行了实验比较。

    We introduce Fiedler regularization, a novel approach for regularizing neural networks that utilizes spectral/graphical information. Existing regularization methods often focus on penalizing weights in a global/uniform manner that ignores the connectivity structure of the neural network. We propose to use the Fiedler value of the neural network's underlying graph as a tool for regularization. We provide theoretical motivation for this approach via spectral graph theory. We demonstrate several useful properties of the Fiedler value that make it useful as a regularization tool. We provide an approximate, variational approach for faster computation during training. We provide an alternative formulation of this framework in the form of a structurally weighted $\text{L}_1$ penalty, thus linking our approach to sparsity induction. We provide uniform generalization error bounds for Fiedler regularization via a Rademacher complexity analysis. We performed experiments on datasets that compare F
    
[^23]: PopulAtion Parameter Averaging (PAPA)（人口参数平均）

    PopulAtion Parameter Averaging (PAPA). (arXiv:2304.03094v1 [cs.LG])

    [http://arxiv.org/abs/2304.03094](http://arxiv.org/abs/2304.03094)

    提出一种新方法PopulAtion Parameter Averaging (PAPA)，能同时拥有集成的普遍性与权重平均的效率，可以显著提高模型性能。

    

    集成方法将多个模型的预测组合起来以提高性能，但需要更高的计算成本。为了避免这些成本，可以通过对多个神经网络的权重进行平均来将它们合并成一个（模型汤）。然而，这通常比集成表现更差。当权重足够相似（在权重或特征空间中）可以很好地平均，但足够不同以从组合中受益时，权重平均才是有益的。基于这个想法，我们提出了PopulAtion Parameter Averaging (PAPA)，一种将集成的普遍性与权重平均的效率相结合的方法。PAPA利用不同模型（在不同数据顺序，增强和正则化上训练）的人口，而偶尔（不要太频繁，也不要太稀疏）用网络的权重来代替人口权重的平均值。PAPA减少了平均值和集成之间的性能差距，提高了模型的性能。

    Ensemble methods combine the predictions of multiple models to improve performance, but they require significantly higher computation costs at inference time. To avoid these costs, multiple neural networks can be combined into one by averaging their weights (model soups). However, this usually performs significantly worse than ensembling. Weight averaging is only beneficial when weights are similar enough (in weight or feature space) to average well but different enough to benefit from combining them. Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a method that combines the generality of ensembling with the efficiency of weight averaging. PAPA leverages a population of diverse models (trained on different data orders, augmentations, and regularizations) while occasionally (not too often, not too rarely) replacing the weights of the networks with the population average of the weights. PAPA reduces the performance gap between averaging and ensembling, increasing th
    
[^24]: 归纳式图形反学习

    Inductive Graph Unlearning. (arXiv:2304.03093v1 [cs.LG])

    [http://arxiv.org/abs/2304.03093](http://arxiv.org/abs/2304.03093)

    本论文介绍了针对图形数据的反学习，旨在实现机器学习中的“被遗忘权”，与其他框架相比，提出了一个新颖的归纳式框架。这个框架可以让机器学习系统在处理动态改变的图形时更具有适应性。

    

    “机器反学习”是机器学习实现“被遗忘权”的方法，旨在完全删除要删除的样本对经过训练的模型的贡献和信息，同时不影响其他样本的贡献。近年来，许多反学习框架已被提出，其中大部分专注于图像和文本数据。为了将反学习扩展到图形数据，已经提出了GraphEraser。然而，一个关键的问题是GraphEraser专门针对转移图设定进行设计，在该设定下，图形是静态的，测试节点的属性和边缘在训练期间是可见的。对于归纳式的设置是不合适的，在此设置中，图形可以是动态的，测试图形信息事先是不可见的。这种归纳能力对于具有不断发展的图形（如社交媒体和交易网络）的生产机器学习系统至关重要。为了填补这一空白，我们提出了G...

    As a way to implement the "right to be forgotten" in machine learning, \textit{machine unlearning} aims to completely remove the contributions and information of the samples to be deleted from a trained model without affecting the contributions of other samples. Recently, many frameworks for machine unlearning have been proposed, and most of them focus on image and text data. To extend machine unlearning to graph data, \textit{GraphEraser} has been proposed. However, a critical issue is that \textit{GraphEraser} is specifically designed for the transductive graph setting, where the graph is static and attributes and edges of test nodes are visible during training. It is unsuitable for the inductive setting, where the graph could be dynamic and the test graph information is invisible in advance. Such inductive capability is essential for production machine learning systems with evolving graphs like social media and transaction networks. To fill this gap, we propose the \underline{{\bf G
    
[^25]: 通过学习不良轨迹的时间模式和防止负面副作用实现安全MDP规划

    Safe MDP Planning by Learning Temporal Patterns of Undesirable Trajectories and Averting Negative Side Effects. (arXiv:2304.03081v1 [cs.LG])

    [http://arxiv.org/abs/2304.03081](http://arxiv.org/abs/2304.03081)

    通过学习不良轨迹的时间模式和防止负面副作用实现安全MDP规划

    

    在安全MDP规划中，基于当前状态和动作的代价函数通常用于指定安全方面，但现实世界中使用的状态表示通常缺乏足够的准确度来指定这样的安全约束条件，基于不完整模型工作常常会产生意外的负面副作用（NSEs），为了解决这些挑战，我们首先将安全信号与状态-动作轨迹相关联（而不仅仅是状态-动作即时）使我们的安全模型具有高度的通用性。我们还假设为不同的轨迹提供了分类安全标签，而不是更难由问题设计者指定的数值代价函数。然后，我们采用监督学习模型来学习这样的非马尔科夫安全模式。其次，我们开发了一种拉格朗日乘数方法，将安全模型和基础MDP模型合并成一个计算图，以促进代理学习安全行为。最后，我们的实证结果...

    In safe MDP planning, a cost function based on the current state and action is often used to specify safety aspects. In the real world, often the state representation used may lack sufficient fidelity to specify such safety constraints. Operating based on an incomplete model can often produce unintended negative side effects (NSEs). To address these challenges, first, we associate safety signals with state-action trajectories (rather than just an immediate state-action). This makes our safety model highly general. We also assume categorical safety labels are given for different trajectories, rather than a numerical cost function, which is harder to specify by the problem designer. We then employ a supervised learning model to learn such non-Markovian safety patterns. Second, we develop a Lagrange multiplier method, which incorporates the safety model and the underlying MDP model in a single computation graph to facilitate agent learning of safe behaviors. Finally, our empirical results
    
[^26]: 自适应学生t分布与方法矩移动估计器用于非平稳时间序列

    Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series. (arXiv:2304.03069v1 [stat.ME])

    [http://arxiv.org/abs/2304.03069](http://arxiv.org/abs/2304.03069)

    本文提出了一种适用于非平稳时间序列的自适应学生t分布方法，基于方法的一般自适应矩可以使用廉价的指数移动平均值（EMA）来估计参数。

    

    真实的时间序列通常是非平稳的，这带来了模型适应的难题。传统方法如GARCH假定任意类型的依赖性。为了避免这种偏差，我们将着眼于最近提出的不可知的移动估计器哲学：在时间$t$找到优化$F_t=\sum_{\tau<t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$移动对数似然的参数，随时间演化。例如，它允许使用廉价的指数移动平均值（EMA）来估计参数，例如绝对中心矩$E[|x-\mu|^p]$随$p\in\mathbb{R}^+$的变化而演化$m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$。这种基于方法的一般自适应矩的应用将呈现在学生t分布上，尤其是在经济应用中流行，这里应用于DJIA公司的对数收益率。

    The real life time series are usually nonstationary, bringing a difficult question of model adaptation. Classical approaches like GARCH assume arbitrary type of dependence. To prevent such bias, we will focus on recently proposed agnostic philosophy of moving estimator: in time $t$ finding parameters optimizing e.g. $F_t=\sum_{\tau<t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$ moving log-likelihood, evolving in time. It allows for example to estimate parameters using inexpensive exponential moving averages (EMA), like absolute central moments $E[|x-\mu|^p]$ evolving with $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$ for one or multiple powers $p\in\mathbb{R}^+$. Application of such general adaptive methods of moments will be presented on Student's t-distribution, popular especially in economical applications, here applied to log-returns of DJIA companies.
    
[^27]: 一项关于新3RL数据集实时面部情感识别的实验研究

    An experimental study in Real-time Facial Emotion Recognition on new 3RL dataset. (arXiv:2304.03064v1 [cs.CV])

    [http://arxiv.org/abs/2304.03064](http://arxiv.org/abs/2304.03064)

    该论文介绍了新的3RL数据集，它是一种用于面部情感识别的数据集，通过与其他数据集进行比较，表现出了更好的泛化能力，并使用最先进的算法(cnn)实现了91.4％的准确率。

    

    虽然实时面部情感识别是人机交互领域的一个热门研究领域，但现有的最先进数据集仍存在各种问题，如一些与情感无关的照片（如文件照片），每类照片数量不平衡以及可能对正确分类产生负面影响的误导性图像。为了克服以前可用数据集的问题，我们创建了3RL数据集，其中包含约24K张图像，并且将公开提供。这个数据集被标记为五种基本情绪：快乐、恐惧、悲伤、厌恶和愤怒。此外，我们将3RL数据集与其他著名的最先进数据集（FER数据集、CK+数据集）进行了比较，并应用了先前工作中最常用的算法，如SVM和CNN。结果表明，在3RL数据集的泛化上有明显的提高。实验结果显示，使用CNN在3RL数据集上的准确率可达91.4％，而FER2013、CK +的结果则略低。

    Although real-time facial emotion recognition is a hot topic research domain in the field of human-computer interaction, state-of the-art available datasets still suffer from various problems, such as some unrelated photos such as document photos, unbalanced numbers of photos in each class, and misleading images that can negatively affect correct classification. The 3RL dataset was created, which contains approximately 24K images and will be publicly available, to overcome previously available dataset problems. The 3RL dataset is labelled with five basic emotions: happiness, fear, sadness, disgust, and anger. Moreover, we compared the 3RL dataset with other famous state-of-the-art datasets (FER dataset, CK+ dataset), and we applied the most commonly used algorithms in previous works, SVM and CNN. The results show a noticeable improvement in generalization on the 3RL dataset. Experiments have shown an accuracy of up to 91.4% on 3RL dataset using CNN where results on FER2013, CK+ are, re
    
[^28]: 机器学习方法在复杂油藏早期地质勘探中，利用井和地震数据进行专家无关的概化预测

    Expert-Independent Generalization of Well and Seismic Data Using Machine Learning Methods for Complex Reservoirs Predicting During Early-Stage Geological Exploration. (arXiv:2304.03048v1 [physics.geo-ph])

    [http://arxiv.org/abs/2304.03048](http://arxiv.org/abs/2304.03048)

    该研究利用自主方法预测了复杂油藏的空间分布概率，可以进行专家无关的概化预测和地质模型创建。

    

    本研究旨在开发和应用一种自主方法，用于预测在研究区域内油藏传播的概率。自主性意味着在准备和输入地质地球物理信息之后，专家对算法的影响被最小化。该研究以研究区域早期勘探阶段的3D地震勘探数据和井信息为基础进行了研究。结果，为两组输入数据：基础组和反演校准后的组，预测了油藏空间分布的概率，并得到了标定后的概率立方体。本文所提出的方法可以对地质和地球物理数据进行专家无关的概化，并利用该概化进行假设检验和基于油藏概率表示的地质模型创建。算法的合格表现表明，在复杂油藏早期地质勘探中具有潜在的广泛应用。

    The aim of this study is to develop and apply an autonomous approach for predicting the probability of hydrocarbon reservoirs spreading in the studied area. Autonomy means that after preparing and inputting geological-geophysical information, the influence of an expert on the algorithms is minimized. The study was made based on the 3D seismic survey data and well information on the early exploration stage of the studied field. As a result, a forecast of the probability of spatial distribution of reservoirs was made for two sets of input data: the base set and the set after reverse-calibration, and three-dimensional cubes of calibrated probabilities of belonging of the studied space to the identified classes were obtained. The approach presented in the paper allows for expert-independent generalization of geological and geophysical data, and to use this generalization for hypothesis testing and creating geological models based on a probabilistic representation of the reservoir. The qual
    
[^29]: 数据流形中的多线性核回归和插补

    Multi-Linear Kernel Regression and Imputation in Data Manifolds. (arXiv:2304.03041v1 [eess.SP])

    [http://arxiv.org/abs/2304.03041](http://arxiv.org/abs/2304.03041)

    本文提出了一种基于数据流形的多线性核回归和插补框架，可以高效地进行计算、提取数据模式和它们的几何形状，而且在dMRI数据下具有显著的改进。

    

    本文介绍了一种高效的多线性非参数（基于核的）逼近框架，用于数据回归和插补，以及其在动态磁共振成像（dMRI）中的应用。假设数据特征驻留在或靠近嵌入再现核希尔伯特空间中的光滑流形中。通过识别里程碑点来描述特征点云，通过线性逼近块来模仿光滑流形的切空间的概念。多线性模型实现了降维，能够进行高效的计算，并提取数据模式及其几何形状，无需训练数据或其他信息。针对严重欠采样的dMRI数据的数值测试表明，与先前的方法、流行的数据建模方法以及最近的张量和深度图像先验方案相比，所提出的方法在效率和准确性方面都有显著改善。

    This paper introduces an efficient multi-linear nonparametric (kernel-based) approximation framework for data regression and imputation, and its application to dynamic magnetic-resonance imaging (dMRI). Data features are assumed to reside in or close to a smooth manifold embedded in a reproducing kernel Hilbert space. Landmark points are identified to describe concisely the point cloud of features by linear approximating patches which mimic the concept of tangent spaces to smooth manifolds. The multi-linear model effects dimensionality reduction, enables efficient computations, and extracts data patterns and their geometry without any training data or additional information. Numerical tests on dMRI data under severe under-sampling demonstrate remarkable improvements in efficiency and accuracy of the proposed approach over its predecessors, popular data modeling methods, as well as recent tensor-based and deep-image-prior schemes.
    
[^30]: 在零售银行业中建模客户生命周期价值

    Modelling customer lifetime-value in the retail banking industry. (arXiv:2304.03038v1 [cs.LG])

    [http://arxiv.org/abs/2304.03038](http://arxiv.org/abs/2304.03038)

    本研究提出了一个通用的框架，可以应用于具有长期合同和产品中心客户关系的行业来建模客户的生命周期价值，该框架可以预测任意时间范围内的CLV，并可生成基于产品的倾向模型，这在零售银行业中尤其重要。通过测试，我们证明了相对于传统算法，该模型可以提高43%的超出时间的CLV预测误差。

    

    理解客户生命周期价值是培养长期客户关系的关键，但估计它远非易事。在零售银行业中，常用的方法依赖于简单的启发式算法，并未充分利用现代机器学习技术的高预测能力。我们提出了一个通用的框架来建模客户生命周期价值，该框架可应用于具有长期合同和产品中心客户关系的行业，其中零售银行就是一个例子。该框架的创新之处在于可以在任意时间范围内进行CLV预测和基于产品的倾向模型。我们还详细介绍了这个模型的实现，该模型目前已在一家大型英国放贷机构中投入生产。在测试中，相对于一种流行的基线方法，我们估计在时间外CLV预测误差方面有43%的改善。从我们的CLV模型派生的倾向模型已被用于支持客户联络营销活动。

    Understanding customer lifetime value is key to nurturing long-term customer relationships, however, estimating it is far from straightforward. In the retail banking industry, commonly used approaches rely on simple heuristics and do not take advantage of the high predictive ability of modern machine learning techniques. We present a general framework for modelling customer lifetime value which may be applied to industries with long-lasting contractual and product-centric customer relationships, of which retail banking is an example. This framework is novel in facilitating CLV predictions over arbitrary time horizons and product-based propensity models. We also detail an implementation of this model which is currently in production at a large UK lender. In testing, we estimate an 43% improvement in out-of-time CLV prediction error relative to a popular baseline approach. Propensity models derived from our CLV model have been used to support customer contact marketing campaigns. In test
    
[^31]: 边缘物联网联邦区块链学习技术

    IoT Federated Blockchain Learning at the Edge. (arXiv:2304.03006v1 [cs.LG])

    [http://arxiv.org/abs/2304.03006](http://arxiv.org/abs/2304.03006)

    本文提出了一种边缘计算的联邦学习框架，使用区块链实现分散式方案，从而提高隐私性和效率，可以在不泄露数据隐私的前提下，训练出更加准确、全面的医疗机器学习模型。

    

    物联网设备在医学领域特别是医疗机器学习方面的利用远远不够，但它们具有无可比拟的优势。本文提出了一种分布式联邦学习框架，用于IoMT（医疗物联网）的IoT设备，利用区块链实现分散式方案，提高隐私性和效率，从而从主流的基于云的架构向边缘移动。

    IoT devices are sorely underutilized in the medical field, especially within machine learning for medicine, yet they offer unrivaled benefits. IoT devices are low-cost, energy-efficient, small and intelligent devices. In this paper, we propose a distributed federated learning framework for IoT devices, more specifically for IoMT (Internet of Medical Things), using blockchain to allow for a decentralized scheme improving privacy and efficiency over a centralized system; this allows us to move from the cloud-based architectures, that are prevalent, to the edge. The system is designed for three paradigms: 1) Training neural networks on IoT devices to allow for collaborative training of a shared model whilst decoupling the learning from the dataset to ensure privacy. Training is performed in an online manner simultaneously amongst all participants, allowing for the training of actual data that may not have been present in a dataset collected in the traditional way and dynamically adapt the
    
[^32]: 突破性论文: Spritz-PS-利用大规模印刷文件数据集对合成面部图像进行验证

    Spritz-PS: Validation of Synthetic Face Images Using a Large Dataset of Printed Documents. (arXiv:2304.02982v1 [cs.CV])

    [http://arxiv.org/abs/2304.02982](http://arxiv.org/abs/2304.02982)

    该论文开发了一个新的数据集，以成为多媒体取证调查的标准，通过验证合成面部图像来探究印刷和扫描图像的有效取证分析能力。

    

    在许多应用中，对印刷和扫描（PS）图像进行有效的取证分析能力至关重要。PS文档可以用于隐藏图像的伪造痕迹，因为这些痕迹通常存在于处理过的图像中，并且合成图像中的主要痕迹可以在PS之后去除。由于生成对抗网络(GANs)的吸引力，使用GANs模型生成的合成面部图像难以与真正的人类面部区分开来，可能用于创建伪造身份。此外，由于GAN模型未考虑在生成人类面部时的生理约束以及这些约束对人类虹膜的影响，在PS情况下区分真实和合成虹膜变得极为困难。由于缺乏大规模参考虹膜数据集，我们旨在开发一个新的数据集，成为多媒体取证(MFs)调查的标准。

    The capability of doing effective forensic analysis on printed and scanned (PS) images is essential in many applications. PS documents may be used to conceal the artifacts of images which is due to the synthetic nature of images since these artifacts are typically present in manipulated images and the main artifacts in the synthetic images can be removed after the PS. Due to the appeal of Generative Adversarial Networks (GANs), synthetic face images generated with GANs models are difficult to differentiate from genuine human faces and may be used to create counterfeit identities. Additionally, since GANs models do not account for physiological constraints for generating human faces and their impact on human IRISes, distinguishing genuine from synthetic IRISes in the PS scenario becomes extremely difficult. As a result of the lack of large-scale reference IRIS datasets in the PS scenario, we aim at developing a novel dataset to become a standard for Multimedia Forensics (MFs) investigat
    
[^33]: 一种快速、轻量级的用于低光照图像增强的网络

    A Fast and Lightweight Network for Low-Light Image Enhancement. (arXiv:2304.02978v1 [cs.CV])

    [http://arxiv.org/abs/2304.02978](http://arxiv.org/abs/2304.02978)

    本文提出了一种称为FLW-Net的快速、轻量级网络，用于解决低光照图像中存在的噪声、低亮度、低对比度和色彩偏差问题。该方法具有高效的全局特征信息提取组件以及基于相对信息设计的损失函数，实验结果显示其有效性。

    

    低光照图像通常存在严重的噪声、低亮度、低对比度和色彩偏差问题。本文提出了一种名为FLW-Net的快速、轻量级网络来解决上述问题。我们设计了一个高效的全局特征信息提取组件，并基于相对信息设计了损失函数来解决绝对参考缺失和获得全局对比度方面的问题。实验结果证实了该方法的有效性。

    Low-light images often suffer from severe noise, low brightness, low contrast, and color deviation. While several low-light image enhancement methods have been proposed, there remains a lack of efficient methods that can simultaneously solve all of these problems. In this paper, we introduce FLW-Net, a Fast and LightWeight Network for low-light image enhancement that significantly improves processing speed and overall effect. To achieve efficient low-light image enhancement, we recognize the challenges of the lack of an absolute reference and the need for a large receptive field to obtain global contrast. Therefore, we propose an efficient global feature information extraction component and design loss functions based on relative information to overcome these challenges. Finally, we conduct comparative experiments to demonstrate the effectiveness of the proposed method, and the results confirm that FLW-Net can significantly reduce the complexity of supervised low-light image enhancemen
    
[^34]: 无约束参数化的耗散性和收缩性神经常微分方程

    Unconstrained Parametrization of Dissipative and Contracting Neural Ordinary Differential Equations. (arXiv:2304.02976v1 [eess.SY])

    [http://arxiv.org/abs/2304.02976](http://arxiv.org/abs/2304.02976)

    本文介绍了一种连续时间的深度神经网络，通过结合神经常微分方程和循环平衡网络的结构，使得网络具有收缩和耗散性质。此外提出的非约束参数化方法使得该网络学习的参数量得以增加。

    

    本文介绍和研究了一类连续时间的深度神经网络，提出的架构源于神经常微分方程和最近引入的循环平衡网络（RENs）的模型结构相结合。我们展示了如何赋予我们提出的NodeRENs收缩和耗散性——对于健壮的学习和控制至关重要的属性。最重要的是，与RENs一样，我们推导了收缩和耗散NodeRENs的参数化，这些参数没有约束，因此能够学习大量的参数。我们在非线性系统识别的案例研究中验证了NodeRENs的属性，包括处理不规则采样数据的可能性。

    In this work, we introduce and study a class of Deep Neural Networks (DNNs) in continuous-time. The proposed architecture stems from the combination of Neural Ordinary Differential Equations (Neural ODEs) with the model structure of recently introduced Recurrent Equilibrium Networks (RENs). We show how to endow our proposed NodeRENs with contractivity and dissipativity -- crucial properties for robust learning and control. Most importantly, as for RENs, we derive parametrizations of contractive and dissipative NodeRENs which are unconstrained, hence enabling their learning for a large number of parameters. We validate the properties of NodeRENs, including the possibility of handling irregularly sampled data, in a case study in nonlinear system identification.
    
[^35]: 深度长短期记忆网络：稳定性和实验验证

    Deep Long-Short Term Memory networks: Stability properties and Experimental validation. (arXiv:2304.02975v1 [eess.SY])

    [http://arxiv.org/abs/2304.02975](http://arxiv.org/abs/2304.02975)

    本研究研究了增量输入状态稳定的深度长短期记忆网络在非线性动态系统识别方面的应用，并利用对网络权重的适当充分条件来建立一个训练过程，从数据中学习到经过证明的$\delta$ISS LSTM模型。在实验中测试结果表明了令人满意的建模表现。

    

    本文旨在研究增量输入状态稳定（$\delta$ISS）深度长短期记忆网络（LSTMs）在非线性动态系统识别方面的应用。我们表明可以利用对网络权重的适当充分条件来建立一个训练过程，能够从数据中学习到经过证明的$\delta$ISS LSTM模型。所提出的方法在实际的制动器上进行了测试，以从实验收集的输入输出数据中识别出系统的模型。结果表明了令人满意的建模表现。

    The aim of this work is to investigate the use of Incrementally Input-to-State Stable ($\delta$ISS) deep Long Short Term Memory networks (LSTMs) for the identification of nonlinear dynamical systems. We show that suitable sufficient conditions on the weights of the network can be leveraged to setup a training procedure able to learn provenly-$\delta$ISS LSTM models from data. The proposed approach is tested on a real brake-by-wire apparatus to identify a model of the system from input-output experimentally collected data. Results show satisfactory modeling performances.
    
[^36]: 解析训练双层ReLU网络

    Training a Two Layer ReLU Network Analytically. (arXiv:2304.02972v1 [cs.LG])

    [http://arxiv.org/abs/2304.02972](http://arxiv.org/abs/2304.02972)

    本研究探讨了一种算法，可以使用解析的方法训练双层ReLU网络，相比随机梯度下降和Adam优化器能够找到更深的最小值，在四个真实数据集中获得了显著更小的训练损失值，同时该方法速度更快，调参参数更少。

    

    神经网络通常使用各种梯度下降的优化算法进行训练，如随机梯度下降或Adam优化器。最近的理论研究表明，双层ReLU网络的临界点（损失梯度为零的点）不都是局部最小值。然而，在本研究中，我们将探讨一种使用ReLU激活的双层神经网络和平方损失的算法，该算法交替地在一个层的情况下解析地找到损失函数的临界点，同时保持另一个层和神经元激活模式不变。实验表明，这个简单的算法比随机梯度下降或Adam优化器能够找到更深的最小值，在评估的五个真实数据集中有四个获得了显著更小的训练损失值。而且，该方法比梯度下降方法更快，几乎没有调参参数。

    Neural networks are usually trained with different variants of gradient descent based optimization algorithms such as stochastic gradient descent or the Adam optimizer. Recent theoretical work states that the critical points (where the gradient of the loss is zero) of two-layer ReLU networks with the square loss are not all local minima. However, in this work we will explore an algorithm for training two-layer neural networks with ReLU-like activation and the square loss that alternatively finds the critical points of the loss function analytically for one layer while keeping the other layer and the neuron activation pattern fixed. Experiments indicate that this simple algorithm can find deeper optima than Stochastic Gradient Descent or the Adam optimizer, obtaining significantly smaller training loss values on four out of the five real datasets evaluated. Moreover, the method is faster than the gradient descent methods and has virtually no tuning parameters.
    
[^37]: 对比学习中的合成难负样本

    Synthetic Hard Negative Samples for Contrastive Learning. (arXiv:2304.02971v1 [cs.CV])

    [http://arxiv.org/abs/2304.02971](http://arxiv.org/abs/2304.02971)

    本文提出一种新的方法，称为SSCL，可以更好地利用难以区分出的负样本，提高对比学习的性能。

    

    对比学习已成为计算机视觉自监督学习的重要方法。其核心目标是在最大化同一图像的两个增强版本之间的相似性（正对），同时最小化不同图像之间的相似性（负对）。最近的研究表明，难度更大的负样本，即难以从锚定样本中区分出的样本，在对比学习中发挥了更为关键的作用。本文提出了一种新的特征层方法，即用于对比学习的合成难负样本（SSCL）的采样，更有效地利用了更难的负样本。具体地，1）我们通过混合负样本生成更多和更难的负样本，然后通过控制锚点样本与其他负样本之间的对比度来对这些负样本进行采样。2）考虑到通过采样得到的负样本可能存在假负样本的问题，我们建议注重减轻这个问题。

    Contrastive learning has emerged as an essential approach for self-supervised learning in computer vision. The central objective of contrastive learning is to maximize the similarities between two augmented versions of the same image (positive pairs), while minimizing the similarities between different images (negative pairs). Recent studies have demonstrated that harder negative samples, i.e., those that are difficult to distinguish from anchor sample, play a more critical role in contrastive learning. In this paper, we propose a novel featurelevel method, namely sampling synthetic hard negative samples for contrastive learning (SSCL), to exploit harder negative samples more effectively. Specifically, 1) we generate more and harder negative samples by mixing negative samples, and then sample them by controlling the contrast of anchor sample with the other negative samples. 2) Considering that the negative samples obtained by sampling may have the problem of false negative samples, we 
    
[^38]: FengWu：推动技能精湛的全球中期天气预报超越10天的领先。(arXiv:2304.02948v1 [cs.AI])

    FengWu: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead. (arXiv:2304.02948v1 [cs.AI])

    [http://arxiv.org/abs/2304.02948](http://arxiv.org/abs/2304.02948)

    FengWu是一个基于人工智能的先进数据驱动的全球中期天气预报系统。它从多模态和多任务的角度下解决了中期预报问题，通过不确定性损失的监督学习，在区域自适应的方式下平衡不同预测器的优化。引入回放缓冲机制来提高性能。FengWu具有精确预测大气动力学和未来的陆地和大气状态的能力。在2018年的长期预报中表现优异。

    

    我们提出了FengWu，一种基于人工智能的先进数据驱动的全球中期天气预报系统。与现有的数据驱动天气预报方法不同，FengWu从多模态和多任务的角度解决了中期预报问题。具体来说，我们精心设计了一个深度学习体系结构，配备了模型特定的编码器-解码器和跨模态融合Transformer，通过不确定性损失的监督学习，在区域自适应的方式下平衡不同预测器的优化。此外，引入了回放缓冲区机制来提高中期预报性能。在基于ERA5再分析的39年数据训练下，FengWu能够准确地复制大气动力学并在0.25{\deg}纬度-经度分辨率上预测未来的陆地和大气状态。基于ERA5的2018年6小时长期预报表明，FengWu表现优异。

    We present FengWu, an advanced data-driven global medium-range weather forecast system based on Artificial Intelligence (AI). Different from existing data-driven weather forecast methods, FengWu solves the medium-range forecast problem from a multi-modal and multi-task perspective. Specifically, a deep learning architecture equipped with model-specific encoder-decoders and cross-modal fusion Transformer is elaborately designed, which is learned under the supervision of an uncertainty loss to balance the optimization of different predictors in a region-adaptive manner. Besides this, a replay buffer mechanism is introduced to improve medium-range forecast performance. With 39-year data training based on the ERA5 reanalysis, FengWu is able to accurately reproduce the atmospheric dynamics and predict the future land and atmosphere states at 37 vertical levels on a 0.25{\deg} latitude-longitude resolution. Hindcasts of 6-hourly weather in 2018 based on ERA5 demonstrate that FengWu performs 
    
[^39]: 实时物联网系统中新颖性检测的可适应和可解释框架

    Adaptable and Interpretable Framework for Novelty Detection in Real-Time IoT Systems. (arXiv:2304.02947v1 [cs.LG])

    [http://arxiv.org/abs/2304.02947](http://arxiv.org/abs/2304.02947)

    介绍了一种RAID算法，能够在多元动态过程中检测异常行为，具有适应非平稳效应、不需改变现有过程自动化基础设施等特点，可在不同领域高度部署，并通过实际数据案例研究证明其改进的检测准确性。

    

    本文介绍了一种名为Real-time Adaptive and Interpretable Detection (RAID)算法的新颖方法，它解决了多元动态过程异常检测方法的局限性，这些方法仅限于在模型训练条件范围内检测异常。RAID算法适应了非平稳效应，如数据漂移和变化点，在模型开发期间可能没有进行账务，从而延长了服务寿命。基于联合概率分布的动态模型处理系统中的异常行为检测和基于自适应过程限制的根本原因隔离。RAID算法不需要更改现有的过程自动化基础设施，因此可在不同领域高度部署。两个涉及实际动态系统数据的案例研究证明了RAID算法的好处，包括变更点适应性、根本原因隔离和改进的检测准确性。

    This paper presents the Real-time Adaptive and Interpretable Detection (RAID) algorithm. The novel approach addresses the limitations of state-of-the-art anomaly detection methods for multivariate dynamic processes, which are restricted to detecting anomalies within the scope of the model training conditions. The RAID algorithm adapts to non-stationary effects such as data drift and change points that may not be accounted for during model development, resulting in prolonged service life. A dynamic model based on joint probability distribution handles anomalous behavior detection in a system and the root cause isolation based on adaptive process limits. RAID algorithm does not require changes to existing process automation infrastructures, making it highly deployable across different domains. Two case studies involving real dynamic system data demonstrate the benefits of the RAID algorithm, including change point adaptation, root cause isolation, and improved detection accuracy.
    
[^40]: 基于卷积神经网络的柔性路面裂缝检测

    Convolutional neural networks for crack detection on flexible road pavements. (arXiv:2304.02933v1 [cs.CV])

    [http://arxiv.org/abs/2304.02933](http://arxiv.org/abs/2304.02933)

    该研究通过对六种卷积神经网络模型的比较，使用包含14000个样本的新现实世界二元裂缝数据集进行微调，实现了自动检测路面裂缝的目的。训练的六个模型中有五个的准确率超过了97％，最高记录的准确率为99.7％。最佳模型已经部署在云基础架构上，以允许自动检测相机镜头中的裂缝。

    

    柔性路面主要由于车辆和不利的环境条件而恶化，而裂缝是最常见的恶化机制；其调查通常使用国际定义的分类标准进行手动进行。在南非，已经引入了高清晰度视频图像，可以进行更安全的道路调查。但是，调查仍然是一项繁琐的手动过程。自动检测诸如裂缝之类的缺陷将允许更快地分析道路网络，并潜在地减少人为偏差和错误。该研究对六种最先进的卷积神经网络模型进行了比较，用于裂缝检测。这些模型在ImageNet数据集上进行预训练，并使用包含14000个样本的新现实世界二元裂缝数据集进行微调。还调查了数据集扩充的效果。训练的六个模型中有五个的准确率超过了97％，最高记录的准确率为99.7％。最佳模型部署在云基础架构上，以允许自动检测相机镜头中的裂缝。结果表明，深度学习技术为道路缺陷的自动化检测提供了一种有前途的方法。

    Flexible road pavements deteriorate primarily due to traffic and adverse environmental conditions. Cracking is the most common deterioration mechanism; the surveying thereof is typically conducted manually using internationally defined classification standards. In South Africa, the use of high-definition video images has been introduced, which allows for safer road surveying. However, surveying is still a tedious manual process. Automation of the detection of defects such as cracks would allow for faster analysis of road networks and potentially reduce human bias and error. This study performs a comparison of six state-of-the-art convolutional neural network models for the purpose of crack detection. The models are pretrained on the ImageNet dataset, and fine-tuned using a new real-world binary crack dataset consisting of 14000 samples. The effects of dataset augmentation are also investigated. Of the six models trained, five achieved accuracy above 97%. The highest recorded accuracy w
    
[^41]: 热成像下口罩的检测和分类

    Mask Detection and Classification in Thermal Face Images. (arXiv:2304.02931v1 [cs.CV])

    [http://arxiv.org/abs/2304.02931](http://arxiv.org/abs/2304.02931)

    本文利用热成像技术，检测口罩是否佩戴及分类类型，最佳模型为nano版本的Yolov5模型，达到97%以上的高准确度。

    

    口罩的使用可以减少病毒，特别是SARS-CoV-2的传播，因此自动检测口罩的佩戴情况以及口罩类型和佩戴方式等是一个重要的研究课题。本文考虑使用热成像技术分析在面部检测和分类口罩类型的可行性，并扩展并注释了现有的热成像数据集。不同的深度学习模型被适应，最好的模型为nano版本的Yolov5模型，并取得了mAP大于97％和精度约为95％的高精度。

    Face masks are recommended to reduce the transmission of many viruses, especially SARS-CoV-2. Therefore, the automatic detection of whether there is a mask on the face, what type of mask is worn, and how it is worn is an important research topic. In this work, the use of thermal imaging was considered to analyze the possibility of detecting (localizing) a mask on the face, as well as to check whether it is possible to classify the type of mask on the face. The previously proposed dataset of thermal images was extended and annotated with the description of a type of mask and a location of a mask within a face. Different deep learning models were adapted. The best model for face mask detection turned out to be the Yolov5 model in the "nano" version, reaching mAP higher than 97% and precision of about 95%. High accuracy was also obtained for mask type classification. The best results were obtained for the convolutional neural network model built on an autoencoder initially trained in the 
    
[^42]: 基于Patchout和文本指导的高效音频字幕生成器

    Efficient Audio Captioning Transformer with Patchout and Text Guidance. (arXiv:2304.02916v1 [cs.SD])

    [http://arxiv.org/abs/2304.02916](http://arxiv.org/abs/2304.02916)

    本文提出了一种使用Patchout和文本指导的高效音频字幕生成器，利用迁移学习和Mixup增强技术解决数据稀缺性问题。

    

    自动音频字幕生成是一项多模态翻译任务，旨在为给定的音频剪辑生成文本描述。本文提出了一种全面的Transformer架构，利用了[1]中提出的Patchout技术，显著降低了计算复杂度，并避免了过度拟合。字幕生成部分对预训练分类模型提取的文本AudioSet标签进行了某种程度的条件约束，该模型被微调以最大化AudioSet标签与地面真实字幕之间的语义相似度。为了缓解音频字幕生成的数据稀缺性问题，我们引入了来自上游音频相关任务和扩大的领域内数据集的迁移学习。此外，我们还提出了一种应用Mixup增强技术进行音频字幕生成的方法。进行了消融实验来研究Patchout和文本指导对最终性能的贡献。结果表明，所提出的技术改善了我们系统的性能，同时减少了计算量。

    Automated audio captioning is multi-modal translation task that aim to generate textual descriptions for a given audio clip. In this paper we propose a full Transformer architecture that utilizes Patchout as proposed in [1], significantly reducing the computational complexity and avoiding overfitting. The caption generation is partly conditioned on textual AudioSet tags extracted by a pre-trained classification model which is fine-tuned to maximize the semantic similarity between AudioSet labels and ground truth captions. To mitigate the data scarcity problem of Automated Audio Captioning we introduce transfer learning from an upstream audio-related task and an enlarged in-domain dataset. Moreover, we propose a method to apply Mixup augmentation for AAC. Ablation studies are carried out to investigate how Patchout and text guidance contribute to the final performance. The results show that the proposed techniques improve the performance of our system and while reducing the computationa
    
[^43]: 高维超统计特征的分类方法

    Classification of Superstatistical Features in High Dimensions. (arXiv:2304.02912v1 [stat.ML])

    [http://arxiv.org/abs/2304.02912](http://arxiv.org/abs/2304.02912)

    本文利用经验风险最小化的方法，对高维超统计特征下的数据进行分类，并分析了正则化和分布尺度参数对分类的影响。

    

    在高维情况下，我们通过经验风险最小化的方法，对具有一般中心点的两个数据云的混合进行了学习，假设具有通用的凸损失和凸正则化。每个数据云是通过从可能是不可数的高斯分布叠加中进行采样来获得的，其方差具有通用的概率密度$\varrho$。我们的分析涵盖了大量的数据分布，包括没有协方差的幂律尾部分布的情况。我们研究了所得估计器的泛化性能，分析了正则化的作用以及分离转换与分布尺度参数的相关性。

    We characterise the learning of a mixture of two clouds of data points with generic centroids via empirical risk minimisation in the high dimensional regime, under the assumptions of generic convex loss and convex regularisation. Each cloud of data points is obtained by sampling from a possibly uncountable superposition of Gaussian distributions, whose variance has a generic probability density $\varrho$. Our analysis covers therefore a large family of data distributions, including the case of power-law-tailed distributions with no covariance. We study the generalisation performance of the obtained estimator, we analyse the role of regularisation, and the dependence of the separability transition on the distribution scale parameters.
    
[^44]: 深度神经网络的重尾部正则化

    Heavy-Tailed Regularization of Weight Matrices in Deep Neural Networks. (arXiv:2304.02911v1 [stat.ML])

    [http://arxiv.org/abs/2304.02911](http://arxiv.org/abs/2304.02911)

    本文介绍了一种名为重尾部正则化的技术，在深度神经网络中通过明确提倡更重的重尾谱来提高泛化性能。与标准正则化技术相比，该方法在基准数据集上实现了显着的改进。

    

    深度神经网络成功和显著的泛化能力背后的原因仍然是一个巨大的挑战。从随机矩阵理论得到的最新信息，特别是涉及深度神经网络中权重矩阵的谱分析的信息，为解决这个问题提供了有价值的线索。一个关键发现是，神经网络的泛化性能与其权重矩阵的谱的重尾程度相关。为了利用这一发现，我们介绍了一种新的正则化技术，称为重尾部正则化，通过正则化明确提倡权重矩阵中更重的重尾谱。首先，我们采用加权阿尔法和稳定秩作为惩罚项，两者都可微分，从而可以直接计算它们的梯度。为了避免过度正则化，我们介绍了两种惩罚函数的变体。然后，采用贝叶斯统计视角，我们提出了重尾部正则化的概率解释，使我们能够将其效果理解为权重矩阵的先验。在多个基准数据集上的实证评估表明，与标准正则化技术相比，我们的方法明显提高了泛化性能。

    Unraveling the reasons behind the remarkable success and exceptional generalization capabilities of deep neural networks presents a formidable challenge. Recent insights from random matrix theory, specifically those concerning the spectral analysis of weight matrices in deep neural networks, offer valuable clues to address this issue. A key finding indicates that the generalization performance of a neural network is associated with the degree of heavy tails in the spectrum of its weight matrices. To capitalize on this discovery, we introduce a novel regularization technique, termed Heavy-Tailed Regularization, which explicitly promotes a more heavy-tailed spectrum in the weight matrix through regularization. Firstly, we employ the Weighted Alpha and Stable Rank as penalty terms, both of which are differentiable, enabling the direct calculation of their gradients. To circumvent over-regularization, we introduce two variations of the penalty function. Then, adopting a Bayesian statistics
    
[^45]: 利用对称性在贝叶斯神经网络中实现高效MCMC采样

    Towards Efficient MCMC Sampling in Bayesian Neural Networks by Exploiting Symmetry. (arXiv:2304.02902v1 [stat.ML])

    [http://arxiv.org/abs/2304.02902](http://arxiv.org/abs/2304.02902)

    本文提出了一种利用对称性在贝叶斯神经网络中实现高效MCMC采样的方法，通过利用神经元可互换性和某些激活函数引起的对称性在参数后验的多模态性中找到平衡。

    

    由于高维，强多模态参数后验密度景观，深度神经网络的贝叶斯推断是具有挑战性的。马尔可夫链蒙特卡罗方法可以渐进性地恢复真实后验，但因其在大规模现代架构上被认为是代价高昂而难以应用。而局部方法，作为一种流行的替代方案，聚焦于可通过可积函数近似的特定参数区域。虽然这些方法常常能够产生满意的实证结果，但它们未能考虑参数后验的多模态性。本文认为，可以通过利用后验景观中的对称性来缓解精确但代价昂贵和廉价但不精确方法之间的困境。这种对称性由神经元可互换性和某些激活函数引起，在不同的参数值导致相同的功能输出值。我们理论上证明后验预测可以利用对称性被高效地计算和采样。

    Bayesian inference in deep neural networks is challenging due to the high-dimensional, strongly multi-modal parameter posterior density landscape. Markov chain Monte Carlo approaches asymptotically recover the true posterior but are considered prohibitively expensive for large modern architectures. Local methods, which have emerged as a popular alternative, focus on specific parameter regions that can be approximated by functions with tractable integrals. While these often yield satisfactory empirical results, they fail, by definition, to account for the multi-modality of the parameter posterior. In this work, we argue that the dilemma between exact-but-unaffordable and cheap-but-inexact approaches can be mitigated by exploiting symmetries in the posterior landscape. Such symmetries, induced by neuron interchangeability and certain activation functions, manifest in different parameter values leading to the same functional output value. We show theoretically that the posterior predictiv
    
[^46]: 变复杂度加权调节 Gibbs 采样用于贝叶斯变量选择

    Variable-Complexity Weighted-Tempered Gibbs Samplers for Bayesian Variable Selection. (arXiv:2304.02899v1 [stat.ML])

    [http://arxiv.org/abs/2304.02899](http://arxiv.org/abs/2304.02899)

    本文提出了一种变复杂度加权调节 Gibbs 采样器，用于贝叶斯变量选择，可以降低每个MCMC迭代的计算复杂度，并且可以在有限的迭代次数内控制估计器的方差。

    

    最近，Jankowiak引入了子集加权调节 Gibbs 采样器（wTGS），用于在高维应用程序中降低每个MCMC迭代的计算复杂度，其中后验包含概率（PIP）的精确计算并不重要。然而，与该采样器相关的Rao-Backwellized估计器具有高方差，因为信号维度与条件PIP估计数之比很大。在本文中，我们设计了一个新的子集加权调节 Gibbs 采样器（wTGS），其中每个MCMC迭代中可预期的条件PIP计算数量可以远小于信号维度。与子集wTGS和wTGS不同，我们的采样器具有可变的复杂度。我们在有限的迭代次数 $T$ 上提供了与该采样器关联的Rao-Blackwellized估计器的方差上限，并展示了该方差为 $O\big(\big(\frac{P}{S}\big)^2 \frac{\log T}{T}\big)$，其中 $\frac{P}{S}$ 是条件PIP估计数和信号维度之比。

    Subset weighted-Tempered Gibbs Sampler (wTGS) has been recently introduced by Jankowiak to reduce the computation complexity per MCMC iteration in high-dimensional applications where the exact calculation of the posterior inclusion probabilities (PIP) is not essential. However, the Rao-Backwellized estimator associated with this sampler has a high variance as the ratio between the signal dimension and the number of conditional PIP estimations is large. In this paper, we design a new subset weighted-Tempered Gibbs Sampler (wTGS) where the expected number of computations of conditional PIPs per MCMC iteration can be much smaller than the signal dimension. Different from the subset wTGS and wTGS, our sampler has a variable complexity per MCMC iteration. We provide an upper bound on the variance of an associated Rao-Blackwellized estimator for this sampler at a finite number of iterations, $T$, and show that the variance is $O\big(\big(\frac{P}{S}\big)^2 \frac{\log T}{T}\big)$ for a given 
    
[^47]: 基于物体中心的语言条件放置推理：基于基础模型的方法

    Object-centric Inference for Language Conditioned Placement: A Foundation Model based Approach. (arXiv:2304.02893v1 [cs.RO])

    [http://arxiv.org/abs/2304.02893](http://arxiv.org/abs/2304.02893)

    本文提出了一个基于物体中心的语言条件放置推理框架，有效降低了训练数据需求，具有更好的泛化能力并可以实现高达 97.75% 的放置成功率。

    

    本文关注于语言条件下的物体放置任务，即机器人应该生成满足语言说明中所有空间关系限制的放置。以往基于规则的语言解析或场景中心的视觉表示对指令形式和参考对象有限制或需要大量的训练数据。本文提出了一个基于物体中心的框架，利用基础模型来确定参考对象和空间关系以进行放置，该方法更加样本高效和可推广。实验表明，我们的模型只需 ~0.26M 可训练参数就能达到 97.75% 的放置成功率。此外，我们的方法对未见过的物体和说明都有更好的泛化能力。当只使用 25% 的训练数据时，我们仍然胜过顶级竞争方法。

    We focus on the task of language-conditioned object placement, in which a robot should generate placements that satisfy all the spatial relational constraints in language instructions. Previous works based on rule-based language parsing or scene-centric visual representation have restrictions on the form of instructions and reference objects or require large amounts of training data. We propose an object-centric framework that leverages foundation models to ground the reference objects and spatial relations for placement, which is more sample efficient and generalizable. Experiments indicate that our model can achieve a 97.75% success rate of placement with only ~0.26M trainable parameters. Besides, our method generalizes better to both unseen objects and instructions. Moreover, with only 25% training data, we still outperform the top competing approach.
    
[^48]: 在具有嘈杂和异质客户端的联邦学习中谨慎学习

    Learning Cautiously in Federated Learning with Noisy and Heterogeneous Clients. (arXiv:2304.02892v1 [cs.LG])

    [http://arxiv.org/abs/2304.02892](http://arxiv.org/abs/2304.02892)

    本文提出了一种名为FedCNI的联邦学习方法，该方法包括鲁棒的全局聚合器和抗噪局部求解器，可以有效处理在小型本地数据集中存在的标签噪声和类别不平衡的问题。

    

    联邦学习是一种分布式的框架，可在保护隐私的情况下进行协作训练。在现实场景中，客户端可能具有非独立同分布数据（本地类别不平衡）和低质量的注释（标签嘈杂）。FL 的小型本地数据集中存在标签噪声和类别不平衡的共存在，使传统 FL 方法和嘈杂标签学习方法均无效。为了解决这些问题，我们提出了 FedCNI，它不使用额外的干净代理数据集。它包括一个鲁棒的全局聚合器和一个抗噪局部求解器。对于局部求解器，我们设计了一个更稳健的样本嘈杂检测器来区分嘈杂样本。为了减少噪声样本带来的负面影响，我们设计了一个课程伪标签方法和一个去噪 Mixup 训练策略。对于全局聚合器，我们提出了一个针对不同学习阶段量身定制的切换加权聚合方法。广泛的实验表明，我们的方法可以显著提高嘈杂标签下的联邦学习的效果。

    Federated learning (FL) is a distributed framework for collaboratively training with privacy guarantees. In real-world scenarios, clients may have Non-IID data (local class imbalance) with poor annotation quality (label noise). The co-existence of label noise and class imbalance in FL's small local datasets renders conventional FL methods and noisy-label learning methods both ineffective. To address the challenges, we propose FedCNI without using an additional clean proxy dataset. It includes a noise-resilient local solver and a robust global aggregator. For the local solver, we design a more robust prototypical noise detector to distinguish noisy samples. Further to reduce the negative impact brought by the noisy samples, we devise a curriculum pseudo labeling method and a denoise Mixup training strategy. For the global aggregator, we propose a switching re-weighted aggregation method tailored to different learning periods. Extensive experiments demonstrate our method can substantiall
    
[^49]: ViralVectors：一种紧凑且可扩展的基于非比对技术生成virome特征的方法

    ViralVectors: Compact and Scalable Alignment-free Virome Feature Generation. (arXiv:2304.02891v1 [q-bio.GN])

    [http://arxiv.org/abs/2304.02891](http://arxiv.org/abs/2304.02891)

    ViralVectors是一种紧凑且可扩展的方法，从virome测序数据中生成Minimizers特征向量进行有效的下游分析。该方法优于现有非比对技术方法，可以区分不同的病毒家族，甚至属，并能够提供接近最优的SARS-CoV-2分类性能。

    

    对于SARS-CoV-2的测序数据量比其他大多数病毒都要大若干个数量级，而且SARS-CoV-2的数据量将继续呈几何级数增长，许多国家正在大力投资基因组监测工作。因此，我们需要处理大量的序列数据以实现有效而及时的决策。这些数据来自各种不同的来源：比对、未比对甚至未装配的原始核苷酸或氨基酸测序reads，涵盖整个基因组或某些区域（例如spike）。本研究提出了ViralVectors，一种从virome测序数据中生成紧凑特征向量的方法，以实现有效的下游分析。该生成方法基于minimizers，一种轻量级的序列“签名”，传统上用于组装和读取映射，据我们所知，首次使用minimizers进行这样的方法。我们验证了我们的方法在不同类型的测序数据上的表现：（a）2.5M SARS-CoV-2 nanopore reads，（b）1.5M 未比对的SARS-CoV-2 Illumina reads，以及（c）大量的virome reads。我们的方法优于最先进的基于非比对技术的方法，可以区分不同的病毒家族，甚至属。我们展示了我们的特征向量可以轻松地聚类和可视化，实现了直观的病毒发现和探索功能。我们进一步分析了公开的SARS-CoV-2数据集，并发现我们的方法可以在一个平衡的两类SARS-CoV-2数据集上提供接近最优的分类性能。

    The amount of sequencing data for SARS-CoV-2 is several orders of magnitude larger than any virus. This will continue to grow geometrically for SARS-CoV-2, and other viruses, as many countries heavily finance genomic surveillance efforts. Hence, we need methods for processing large amounts of sequence data to allow for effective yet timely decision-making. Such data will come from heterogeneous sources: aligned, unaligned, or even unassembled raw nucleotide or amino acid sequencing reads pertaining to the whole genome or regions (e.g., spike) of interest. In this work, we propose \emph{ViralVectors}, a compact feature vector generation from virome sequencing data that allows effective downstream analysis. Such generation is based on \emph{minimizers}, a type of lightweight "signature" of a sequence, used traditionally in assembly and read mapping -- to our knowledge, the first use minimizers in this way. We validate our approach on different types of sequencing data: (a) 2.5M SARS-CoV-
    
[^50]: 标记问题：在问题跟踪系统中应用API领域标签

    Tag that issue: Applying API-domain labels in issue tracking systems. (arXiv:2304.02877v1 [cs.SE])

    [http://arxiv.org/abs/2304.02877](http://arxiv.org/abs/2304.02877)

    该论文研究了在开源软件项目中自动标记问题为API领域标签的可行性和相关性，结果表明自动分配的标签对贡献者选择任务和理解问题要求是有帮助的。

    

    在开源软件项目中，用所需技能来标记问题可以帮助贡献者选择任务。然而，手动标记问题耗时且容易出错，而当前的自动化方法大多仅限于将问题分类为错误/非错误。我们调查自动标记问题为我们所谓的“API领域”的可行性和相关性，这些领域是API的高级类别。因此，我们认为受问题影响的源代码中使用的API可以作为工作问题所需技能（例如DB、安全性、UI）的代理。我们进行了一个用户研究（n=74）来评估API领域标签对潜在贡献者的相关性，利用问题描述和项目历史来建立预测模型，并验证了贡献者（n=20）对项目的预测结果。我们的研究结果显示：（i）项目新手认为API领域标签在选择任务时有用，（ii）可以预测标签的准确率高达60％，（iii）贡献者发现自动分配的标签有助于理解问题的要求。

    Labeling issues with the skills required to complete them can help contributors to choose tasks in Open Source Software projects. However, manually labeling issues is time-consuming and error-prone, and current automated approaches are mostly limited to classifying issues as bugs/non-bugs. We investigate the feasibility and relevance of automatically labeling issues with what we call "API-domains," which are high-level categories of APIs. Therefore, we posit that the APIs used in the source code affected by an issue can be a proxy for the type of skills (e.g., DB, security, UI) needed to work on the issue. We ran a user study (n=74) to assess API-domain labels' relevancy to potential contributors, leveraged the issues' descriptions and the project history to build prediction models, and validated the predictions with contributors (n=20) of the projects. Our results show that (i) newcomers to the project consider API-domain labels useful in choosing tasks, (ii) labels can be predicted w
    
[^51]: 通过监督学习保护在线用户隐私

    Protecting User Privacy in Online Settings via Supervised Learning. (arXiv:2304.02870v1 [cs.CR])

    [http://arxiv.org/abs/2304.02870](http://arxiv.org/abs/2304.02870)

    本研究提出了基于监督学习的隐私保护方法，可以阻止侵犯用户隐私的数据收集，保护用户数字隐私。

    

    与传统商业模式不同，网络公司通常以收集用户数据的方式赚取利润。然而，用户数据会被出售给第三方，侵犯了用户的隐私。本研究设计了一种基于监督学习的智能隐私保护方法，可以检测和阻止可能侵犯用户隐私的数据收集，从而恢复用户的数字隐私。

    Companies that have an online presence-in particular, companies that are exclusively digital-often subscribe to this business model: collect data from the user base, then expose the data to advertisement agencies in order to turn a profit. Such companies routinely market a service as "free", while obfuscating the fact that they tend to "charge" users in the currency of personal information rather than money. However, online companies also gather user data for more principled purposes, such as improving the user experience and aggregating statistics. The problem is the sale of user data to third parties. In this work, we design an intelligent approach to online privacy protection that leverages supervised learning. By detecting and blocking data collection that might infringe on a user's privacy, we can restore a degree of digital privacy to the user. In our evaluation, we collect a dataset of network requests and measure the performance of several classifiers that adhere to the supervi
    
[^52]: 大型语言模型能否能够很好地玩文字游戏？现状和未来问题研究

    Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. (arXiv:2304.02868v1 [cs.CL])

    [http://arxiv.org/abs/2304.02868](http://arxiv.org/abs/2304.02868)

    本文探究大型语言模型在玩文字游戏的能力，并发现其表现有竞争力，但仍然缺乏智能，有待提升。

    

    最近，诸如ChatGPT和GPT-4之类的大型语言模型展示了它们与人类用户通信的卓越能力。本技术报告旨在调查它们在玩文字游戏方面的能力，这要求玩家通过与游戏世界的对话来理解环境并对情况做出反应。我们的实验表明，与所有现有系统相比，ChatGPT表现出有竞争力，但仍然表现出较低的智能水平。确切地说，ChatGPT无法通过玩游戏或阅读游戏手册来构建世界模型；它可能无法利用它已经拥有的世界知识；它无法推断出随着游戏进展的每一步的目标。我们的结果在人工智能、机器学习和自然语言处理交叉领域开启了新的研究问题。

    Large language models (LLMs) such as ChatGPT and GPT-4 have recently demonstrated their remarkable abilities of communicating with human users. In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world. Our experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses. Our results open up new research questions at the intersection of artificial intelligence, machine learning, and natural language processing.
    
[^53]: 学习不可或缺的连接来进行元学习

    Learning to Learn with Indispensable Connections. (arXiv:2304.02862v1 [cs.LG])

    [http://arxiv.org/abs/2304.02862](http://arxiv.org/abs/2304.02862)

    该论文提出了一种新的元学习方法Meta-LTH，该方法包括不可缺少的连接，通过重要性剪枝技术生成关键的连接，能够有效地解决少样本学习问题，并在标准少样本学习基准测试上优于现有的元学习算法。

    

    元学习旨在通过少量标记实例来解决未知任务。然而，尽管现有基于优化的方法在快速学习方面非常有效，但它们存在一些缺陷。元训练中经常出现不必要的连接，导致神经网络过度参数化。因此，在元测试过程中，观察到不必要的计算和额外的内存开销。为了克服这些问题，我们提出了一种新的元学习方法，称为Meta-LTH，它包括不可缺少的连接。我们应用了被称为重要性剪枝的彩票假设技术来生成这些关键的连接，可以有效地解决少样本学习问题。我们的目标是实现两件事：(a) 寻找一个更适应元学习的子网络，(b) 在元测试阶段学习未见过任务的新低级特征并将这些特征与已经学习的特征重新组合起来。实验结果表明，我们提出的Meta-LTH方法在标准少样本学习基准测试上优于现有的元学习算法。

    Meta-learning aims to solve unseen tasks with few labelled instances. Nevertheless, despite its effectiveness for quick learning in existing optimization-based methods, it has several flaws. Inconsequential connections are frequently seen during meta-training, which results in an over-parameterized neural network. Because of this, meta-testing observes unnecessary computations and extra memory overhead. To overcome such flaws. We propose a novel meta-learning method called Meta-LTH that includes indispensible (necessary) connections. We applied the lottery ticket hypothesis technique known as magnitude pruning to generate these crucial connections that can effectively solve few-shot learning problem. We aim to perform two things: (a) to find a sub-network capable of more adaptive meta-learning and (b) to learn new low-level features of unseen tasks and recombine those features with the already learned features during the meta-test phase. Experimental results show that our proposed Met-
    
[^54]: 面向类别不均问题的集成学习和数据增强模型综述：组合、实现和评估

    A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v1 [cs.LG])

    [http://arxiv.org/abs/2304.02858](http://arxiv.org/abs/2304.02858)

    本文研究了集成学习和数据增强方法的应用，针对类别不平衡问题，通过计算评估，找到了最有效的组合。

    

    分类问题中的类别不平衡（CI）是指属于一个类的观测值数量低于其他类的数量。集成学习结合数据增强方法已被广泛应用于解决类别不平衡问题。在过去的十年里，一些策略已经被应用于增强集成学习和数据增强方法，同时还开发了一些新方法，如生成对抗网络（GAN）。本文对用于解决基准CI问题的数据增强和集成学习方法进行计算评估。我们提出了一个评估CI问题的10个数据增强方法和10个集成学习方法的通用框架。我们的目标是识别提高分类效果最有效的组合。

    Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other classes. Ensemble learning that combines multiple models to obtain a robust model has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, but the true rank of different combinations would require a computational review. In this paper, we present a computational review to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We propose a general framework that evaluates 10 data augmentation and 10 ensemble learning methods for CI problems. Our objective was to identify the most effective combination for improving classificat
    
[^55]: 分类中异方差标签噪声的逻辑正态似然

    Logistic-Normal Likelihoods for Heteroscedastic Label Noise in Classification. (arXiv:2304.02849v1 [cs.LG])

    [http://arxiv.org/abs/2304.02849](http://arxiv.org/abs/2304.02849)

    该论文介绍了一种新的分类方法，用于提高鲁棒性，减少标签噪声的影响，其基于正态分布，并可通过最小化负对数似然来学习参数。

    

    在回归中估计异方差标签噪声的一种自然方法是将观测到的（可能带有噪声的）目标建模为一个正态分布的样本，其参数可以通过最小化负对数似然来学习。该损失具有期望的损失衰减特性，因为它可以降低高误差示例的贡献。直观地说，这种行为可以通过减少过拟合来提高对标签噪声的鲁棒性。我们提出了这种简单且概率化方法在分类中的扩展，具有相同的期望损失衰减特性。我们通过测量其对分类中标签噪声的鲁棒性来评估该方法的有效性。我们进行了启发性的实验，探索了该方法的内部工作原理，包括对超参数的敏感性，消融研究等。

    A natural way of estimating heteroscedastic label noise in regression is to model the observed (potentially noisy) target as a sample from a normal distribution, whose parameters can be learned by minimizing the negative log-likelihood. This loss has desirable loss attenuation properties, as it can reduce the contribution of high-error examples. Intuitively, this behavior can improve robustness against label noise by reducing overfitting. We propose an extension of this simple and probabilistic approach to classification that has the same desirable loss attenuation properties. We evaluate the effectiveness of the method by measuring its robustness against label noise in classification. We perform enlightening experiments exploring the inner workings of the method, including sensitivity to hyperparameters, ablation studies, and more.
    
[^56]: Robustmix：通过正则化深度网络的频率偏差来提高鲁棒性

    Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets. (arXiv:2304.02847v1 [cs.CV])

    [http://arxiv.org/abs/2304.02847](http://arxiv.org/abs/2304.02847)

    本研究提出一种叫做Robustmix的方法，通过正则化网络以低频空间特征进行分类来提高深度网络的鲁棒性，在Imagenet-C和Stylized Imagenet等基准测试上取得了最新的最优状态平均峰值误差（mCE），在避免计算开销和先验知识的大量图像变换的同时对模型架构和数据增强的最新进展提供了补充。

    

    深度网络在一系列经过精心策划的基准数据集上取得了令人印象深刻的结果。令人惊讶的是，它们的性能对于对人类性能几乎没有影响的扰动仍然很敏感。在这项工作中，我们提出了一种名为Robustmix的Mixup新扩展，该扩展通过正则化网络以基于低频空间特征进行分类。我们表明，这种类型的正则化改善了在一系列基准测试中的鲁棒性，例如Imagenet-C和Stylized Imagenet。它几乎没有计算开销，并且不需要先验知识的大量图像变换。我们发现，这种方法进一步补充了模型架构和数据增强的最新进展，使用EfficientNet-B8模型和RandAugment达到了44.8的最新状态平均峰值误差（mCE），相比基线降低了16个mCE。

    Deep networks have achieved impressive results on a range of well-curated benchmark datasets. Surprisingly, their performance remains sensitive to perturbations that have little effect on human performance. In this work, we propose a novel extension of Mixup called Robustmix that regularizes networks to classify based on lower-frequency spatial features. We show that this type of regularization improves robustness on a range of benchmarks such as Imagenet-C and Stylized Imagenet. It adds little computational overhead and, furthermore, does not require a priori knowledge of a large set of image transformations. We find that this approach further complements recent advances in model architecture and data augmentation, attaining a state-of-the-art mCE of 44.8 with an EfficientNet-B8 model and RandAugment, which is a reduction of 16 mCE compared to the baseline.
    
[^57]: 坚韧的神经架构搜索

    Robust Neural Architecture Search. (arXiv:2304.02845v1 [cs.LG])

    [http://arxiv.org/abs/2304.02845](http://arxiv.org/abs/2304.02845)

    提出了一种名为RNAS的神经架构搜索方法，通过平衡准确性和鲁棒性生成高质量架构，使用噪声样本减少搜索成本，在图像分类和对抗攻击中均达到最先进性能水平。

    

    近年来，神经架构搜索（NAS）变得越来越流行。然而，NAS生成的模型往往更容易受到各种恶意攻击的影响。许多强健的NAS方法利用对抗训练来增强NAS生成的模型的强健性，但是它们忽略了NAS生成的模型的本质准确性。在我们的论文中，我们提出了一种新颖的NAS方法，名为Robust Neural Architecture Search（RNAS）。为了设计出一个正则化项来平衡准确性和鲁棒性，RNAS生成具有高准确性和良好鲁棒性的架构。为了减少搜索成本，我们提出使用噪声样本而不是对抗性样本作为搜索架构的输入。广泛的实验表明，RNAS在图像分类和对抗攻击方面均达到了最先进（SOTA）的性能，这证明了所提出的RNAS在准确性和鲁棒性之间取得了良好的平衡。

    Neural Architectures Search (NAS) becomes more and more popular over these years. However, NAS-generated models tends to suffer greater vulnerability to various malicious attacks. Lots of robust NAS methods leverage adversarial training to enhance the robustness of NAS-generated models, however, they neglected the nature accuracy of NAS-generated models. In our paper, we propose a novel NAS method, Robust Neural Architecture Search (RNAS). To design a regularization term to balance accuracy and robustness, RNAS generates architectures with both high accuracy and good robustness. To reduce search cost, we further propose to use noise examples instead adversarial examples as input to search architectures. Extensive experiments show that RNAS achieves state-of-the-art (SOTA) performance on both image classification and adversarial attacks, which illustrates the proposed RNAS achieves a good tradeoff between robustness and accuracy.
    
[^58]: NTK-SAP: 通过对齐训练动态来提高神经网络剪枝

    NTK-SAP: Improving neural network pruning by aligning training dynamics. (arXiv:2304.02840v1 [cs.LG])

    [http://arxiv.org/abs/2304.02840](http://arxiv.org/abs/2304.02840)

    本文提出了一种新的神经网络剪枝方法：通过对齐训练动态来提高剪枝效果，具体来说就是剪去对NTK频谱影响最小的连接。采用这种方法有助于维持NTK频谱，从而将训练动态和其密集对应物的训练动态对齐。

    

    在训练之前剪枝神经网络因其减少训练时间和存储空间的潜力而受到越来越多的关注。其中一种流行的方法是基于某种度量对连接进行剪枝，但是什么度量是最好的选择还不完全清楚。神经切向核（NTK）理论的最新进展表明，足够大的神经网络的训练动态与NTK的频谱密切相关。在此发现的基础上，我们建议剪枝那些对NTK频谱影响最小的连接。这种方法有助于维持NTK频谱，这可能有助于将训练动态与其密集对应物的训练动态对齐。然而，一个可能的问题是给定初始点对应的固定权值NTK可能与训练阶段后的迭代对应的NTK非常不同。我们进一步提议对随机权重的多个实现进行采样以估计NTK频谱。请注意，我们的方法是权重无关的。

    Pruning neural networks before training has received increasing interest due to its potential to reduce training time and memory. One popular method is to prune the connections based on a certain metric, but it is not entirely clear what metric is the best choice. Recent advances in neural tangent kernel (NTK) theory suggest that the training dynamics of large enough neural networks is closely related to the spectrum of the NTK. Motivated by this finding, we propose to prune the connections that have the least influence on the spectrum of the NTK. This method can help maintain the NTK spectrum, which may help align the training dynamics to that of its dense counterpart. However, one possible issue is that the fixed-weight-NTK corresponding to a given initial point can be very different from the NTK corresponding to later iterates during the training phase. We further propose to sample multiple realizations of random weights to estimate the NTK spectrum. Note that our approach is weight
    
[^59]: 基于Transformer和来源图的高级持久性威胁检测方法

    TBDetector:Transformer-Based Detector for Advanced Persistent Threats with Provenance Graph. (arXiv:2304.02838v1 [cs.CR])

    [http://arxiv.org/abs/2304.02838](http://arxiv.org/abs/2304.02838)

    本论文提出了一种采用来源图和Transformer的高级持久性威胁检测方法，利用Transformer的自注意力编码器-解码器提取系统状态的长期上下文特征，并通过来源分析实现对长期运行系统的概括，以检测缓慢攻击。

    

    针对高级持久性威胁（APT）攻击的长期潜伏、隐秘多阶段攻击模式，本文提出了一种基于Transformer的APT检测方法，利用来源图提供的历史信息进行APT检测。该方法利用Transformer的自注意力编码器-解码器提取系统状态的长期上下文特征，并通过来源分析实现对长期运行系统的概括，以检测缓慢攻击。此外，作者还引入了异常评分，可评估不同系统状态的异常性。每个状态都有相应的相似度和隔离度分数的异常分数计算。为了评估该方法的有效性

    APT detection is difficult to detect due to the long-term latency, covert and slow multistage attack patterns of Advanced Persistent Threat (APT). To tackle these issues, we propose TBDetector, a transformer-based advanced persistent threat detection method for APT attack detection. Considering that provenance graphs provide rich historical information and have the powerful attacks historic correlation ability to identify anomalous activities, TBDetector employs provenance analysis for APT detection, which summarizes long-running system execution with space efficiency and utilizes transformer with self-attention based encoder-decoder to extract long-term contextual features of system states to detect slow-acting attacks. Furthermore, we further introduce anomaly scores to investigate the anomaly of different system states, where each state is calculated with an anomaly score corresponding to its similarity score and isolation score. To evaluate the effectiveness of the proposed method,
    
[^60]: 长期的多模式变压器整合EHR中成像和潜在临床特征，用于肺部结节分类。

    Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification. (arXiv:2304.02836v1 [eess.IV])

    [http://arxiv.org/abs/2304.02836](http://arxiv.org/abs/2304.02836)

    本文提出了一种新的肺部结节分类方法，使用变压器模型整合了EHR中的成像和临床特征。

    

    将重复成像和医疗背景（如电子健康记录）纳入预测性孤立性肺部结节（SPN）诊断模型可以极大增加准确性。然而，像成像和诊断代码这样的临床常规模式可能是异步的，并且在不同时间尺度上进行不规则采样，这是长期多模态学习的障碍。我们提出了一种基于变压器的多模态策略，将重复成像与日常收集的EHR中的长期临床特征相整合，以进行SPN分类。我们对潜在临床特征进行无监督的解缠缚，并利用时间距离缩放自注意力来联合学习临床特征表达和胸部计算机断层扫描（CT）。我们的分类器是在一个公共数据集的2,668个扫描和1,149名志愿者的长期胸部CT、账单代码、药物和实验室检查记录中进行预训练的。

    The accuracy of predictive models for solitary pulmonary nodule (SPN) diagnosis can be greatly increased by incorporating repeat imaging and medical context, such as electronic health records (EHRs). However, clinically routine modalities such as imaging and diagnostic codes can be asynchronous and irregularly sampled over different time scales which are obstacles to longitudinal multimodal learning. In this work, we propose a transformer-based multimodal strategy to integrate repeat imaging with longitudinal clinical signatures from routinely collected EHRs for SPN classification. We perform unsupervised disentanglement of latent clinical signatures and leverage time-distance scaled self-attention to jointly learn from clinical signatures expressions and chest computed tomography (CT) scans. Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution
    
[^61]: GIF：一种基于影响函数的通用图神经网络遗忘策略

    GIF: A General Graph Unlearning Strategy via Influence Function. (arXiv:2304.02835v1 [cs.LG])

    [http://arxiv.org/abs/2304.02835](http://arxiv.org/abs/2304.02835)

    本文提出了一种模型无关的图神经网络遗忘方法GIF，采用影响函数来移除模型中特定节点、边或特征的影响，在性能和复杂度的平衡上取得了最新的最佳表现。

    

    随着隐私和安全问题在我们的社会中受到越来越多的关注，在训练过的图神经网络模型中撤销特定数据的影响的图神经网络遗忘问题变得越来越重要。然而，现有的机器遗忘到最近出现的图神经网络遗忘方法，要么采用重新训练的范式，要么执行近似抹消，而这种方法未能考虑到连接邻居之间的相互依赖关系或对GNN结构施加限制，因此很难实现令人满意的性能 - 复杂性平衡。在这项工作中，我们探索了针对图神经网络遗忘量身定制的影响函数，以提高图神经网络遗忘的效果和效率。我们首先提出了各种图神经网络遗忘任务的统一问题表述（节点、边、特征）。然后，我们认识到传统影响函数用于图神经网络遗忘无能为力的关键所在，并设计了图影响函数(GIF)，这是一种模型无关的遗忘机器，利用基于梯度的影响函数从模型中移除特定节点、边或特征的影响，而无需重新训练整个模型。各种数据集上的实验结果表明，与现有基线相比，GIF在效率和效果上均达到了最先进的水平。

    With the greater emphasis on privacy and security in our society, the problem of graph unlearning -- revoking the influence of specific data on the trained GNN model, is drawing increasing attention. However, ranging from machine unlearning to recently emerged graph unlearning methods, existing efforts either resort to retraining paradigm, or perform approximate erasure that fails to consider the inter-dependency between connected neighbors or imposes constraints on GNN structure, therefore hard to achieve satisfying performance-complexity trade-offs.  In this work, we explore the influence function tailored for graph unlearning, so as to improve the unlearning efficacy and efficiency for graph unlearning. We first present a unified problem formulation of diverse graph unlearning tasks \wrt node, edge, and feature. Then, we recognize the crux to the inability of traditional influence function for graph unlearning, and devise Graph Influence Function (GIF), a model-agnostic unlearning m
    
[^62]: 基于深度强化学习的异步联邦学习车辆选择方案

    Deep Reinforcement Learning Based Vehicle Selection for Asynchronous Federated Learning Enabled Vehicular Edge Computing. (arXiv:2304.02832v1 [cs.LG])

    [http://arxiv.org/abs/2304.02832](http://arxiv.org/abs/2304.02832)

    本文提出了一种基于深度强化学习的异步联邦学习车辆选择方案，以提高车联网的训练性能。仿真结果表明，该算法可以实现更好的训练精度和更低的通信开销。

    

    在传统车联网中，车辆生成的计算任务通常上传到云端进行处理。然而，由于任务卸载会导致大量延迟，因此引入了车辆边缘计算 (VEC) 技术，它使用具有一定计算能力的路侧单元 (RSU) 作为边缘实体来处理车辆的数据。由于隐私和安全问题，车辆不愿直接上传本地数据到 RSU，因此联邦学习 (FL) 成为 VEC 中某些机器学习任务的一种有前途的技术，车辆只需要上传本地模型超参数而不是将本地数据转移到附近的 RSU。此外，由于车辆具有不同的本地训练时间，因为本地数据的大小和不同的计算能力，所以采用异步联邦学习 (AFL) 来促进 RSU 从所有参与车辆的聚合本地更新中更新全局模型。在本文中，我们提出了一种基于深度强化学习的 AFL-VEC 车辆选择算法，该算法可以选择具有高计算能力和通信能力的最适合的车辆参与 AFL 进程，以提高整体训练性能。仿真结果表明，与其他最先进的算法相比，所提出的算法可以实现更好的训练精度和更低的通信开销。

    In the traditional vehicular network, computing tasks generated by the vehicles are usually uploaded to the cloud for processing. However, since task offloading toward the cloud will cause a large delay, vehicular edge computing (VEC) is introduced to avoid such a problem and improve the whole system performance, where a roadside unit (RSU) with certain computing capability is used to process the data of vehicles as an edge entity. Owing to the privacy and security issues, vehicles are reluctant to upload local data directly to the RSU, and thus federated learning (FL) becomes a promising technology for some machine learning tasks in VEC, where vehicles only need to upload the local model hyperparameters instead of transferring their local data to the nearby RSU. Furthermore, as vehicles have different local training time due to various sizes of local data and their different computing capabilities, asynchronous federated learning (AFL) is employed to facilitate the RSU to update the g
    
[^63]: SoK: 机器学习在持续集成中的应用

    SoK: Machine Learning for Continuous Integration. (arXiv:2304.02829v1 [cs.SE])

    [http://arxiv.org/abs/2304.02829](http://arxiv.org/abs/2304.02829)

    这篇论文对使用机器学习进行持续集成的方法进行了系统综述并指出了目前存在的不足和改进方向。

    

    持续集成（CI）已成为自动和持续集成软件开发过程中代码变更的一项成熟的软件开发实践。越来越多基于机器学习的方法用于自动化CI过程的报道正在出现。提供一个机器学习在CI阶段应用的知识体系化（SoK）是及时且相关的。本文报告了机器学习在CI的不同方面的 SoK。我们的系统分析还强调了现有基于机器学习的解决方案的缺陷，可以改进以推进最新技术的发展。

    Continuous Integration (CI) has become a well-established software development practice for automatically and continuously integrating code changes during software development. An increasing number of Machine Learning (ML) based approaches for automation of CI phases are being reported in the literature. It is timely and relevant to provide a Systemization of Knowledge (SoK) of ML-based approaches for CI phases. This paper reports an SoK of different aspects of the use of ML for CI. Our systematic analysis also highlights the deficiencies of the existing ML-based solutions that can be improved for advancing the state-of-the-art.
    
[^64]: GPT检测器对非英语母语的作者存在偏见。

    GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])

    [http://arxiv.org/abs/2304.02819](http://arxiv.org/abs/2304.02819)

    该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。

    

    生成语言模型的快速推广带来了数字通信方面的实质性进展，同时也引发了AI生成内容潜在误用的担忧。虽然已经提出了许多检测方法来区分AI和人类生成的内容，但这些检测器的公平性和鲁棒性仍未得到充分探讨。在这项研究中，我们使用来自英语母语和非英语母语作者的写作样本评估了几种广泛使用的GPT检测器的性能表现。我们的研究发现，这些检测器持续将非英语母语的写作样本错误地分类为AI生成的内容，而原生写作样本则能够被准确识别。此外，我们证明了简单的提示策略不仅可以缓解这种偏见，而且还可以有效地规避GPT检测器，这表明GPT检测器可能无意中惩罚具有受限语言表达能力的作者。我们的研究结果呼吁进行更广泛的讨论。

    The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
    
[^65]: 学习增强型物联网系统的因果修复

    Causal Repair of Learning-enabled Cyber-physical Systems. (arXiv:2304.02813v1 [eess.SY])

    [http://arxiv.org/abs/2304.02813](http://arxiv.org/abs/2304.02813)

    本文提出了一种学习增强型物联网系统的因果诊断和修复方法，通过矫正有问题的输入/输出行为子集，识别真正的属性违规原因并修复。

    

    实际因果模型使用领域知识生成导致结果的事件的令人信服的诊断。将这些模型应用于具有学习增强组件（LEC）的物联网系统（CPS）中诊断和修复运行时属性违规是有希望的。然而，鉴于LEC的高多样性和复杂性，将领域知识（例如CPS动态）编码成可生成有用修复建议的可扩展实际因果模型是具有挑战性的。在本文中，我们将因果诊断集中于LEC的输入/输出行为。具体而言，我们的目标是确定LEC的哪个输入/输出行为子集是导致属性违规的真正原因。一个重要的副产品是矫正识别出有问题的行为来修复运行时属性的LEC的反事实版本。基于这些结果，我们设计了一个两步诊断流程：（1）构建Halpern-Pearl因果模型以反映属性结果的依赖关系。

    Models of actual causality leverage domain knowledge to generate convincing diagnoses of events that caused an outcome. It is promising to apply these models to diagnose and repair run-time property violations in cyber-physical systems (CPS) with learning-enabled components (LEC). However, given the high diversity and complexity of LECs, it is challenging to encode domain knowledge (e.g., the CPS dynamics) in a scalable actual causality model that could generate useful repair suggestions. In this paper, we focus causal diagnosis on the input/output behaviors of LECs. Specifically, we aim to identify which subset of I/O behaviors of the LEC is an actual cause for a property violation. An important by-product is a counterfactual version of the LEC that repairs the run-time property by fixing the identified problematic behaviors. Based on this insights, we design a two-step diagnostic pipeline: (1) construct and Halpern-Pearl causality model that reflects the dependency of property outcom
    
[^66]: HomPINNs：基于同伦的物理知识神经网络用于解决具有多个解的非线性微分方程的反问题

    HomPINNs: homotopy physics-informed neural networks for solving the inverse problems of nonlinear differential equations with multiple solutions. (arXiv:2304.02811v1 [cs.LG])

    [http://arxiv.org/abs/2304.02811](http://arxiv.org/abs/2304.02811)

    本文提出了一种新的框架——基于同伦的物理知识神经网络，来解决具有多个解的非线性微分方程的反问题。该框架使用神经网络逼近已知观测结果并符合DEs的约束条件，通过同伦连续方法解决反问题。实验证明该方法可伸缩且适应性强，为解决具有多个解的DEs提供了有效解决方案。

    

    由于解空间中的非唯一性、对称性和分岔等复杂行为，解决具有多个解的非线性微分方程（DEs）的反问题是一项具有挑战性的任务。为了解决这个问题，我们提出了同伦物理知识神经网络（HomPINNs），这是一种利用同伦连续和神经网络（NNs）来解决反问题的新框架。所提出的框架首先使用NNs同时逼近已知观测结果和符合DEs的约束条件。通过利用同伦连续方法，逼近可追踪观察结果以确定多个解并解决反问题。实验涵盖在一维DEs上测试所提出的方法的性能，并应用它来解决二维Gray-Scott模拟。我们的研究结果表明，所提出的方法是可伸缩且适应性强的，为解决具有多个解的DEs提供了有效的解决方案。

    Due to the complex behavior arising from non-uniqueness, symmetry, and bifurcations in the solution space, solving inverse problems of nonlinear differential equations (DEs) with multiple solutions is a challenging task. To address this issue, we propose homotopy physics-informed neural networks (HomPINNs), a novel framework that leverages homotopy continuation and neural networks (NNs) to solve inverse problems. The proposed framework begins with the use of a NN to simultaneously approximate known observations and conform to the constraints of DEs. By utilizing the homotopy continuation method, the approximation traces the observations to identify multiple solutions and solve the inverse problem. The experiments involve testing the performance of the proposed method on one-dimensional DEs and applying it to solve a two-dimensional Gray-Scott simulation. Our findings demonstrate that the proposed method is scalable and adaptable, providing an effective solution for solving DEs with mul
    
[^67]: 图混合专家：显式多样性建模下的大规模图学习

    Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling. (arXiv:2304.02806v1 [cs.LG])

    [http://arxiv.org/abs/2304.02806](http://arxiv.org/abs/2304.02806)

    本文提出了一种新的图混合专家（GMoE）模型，旨在解决现实世界中的图具有多样的图结构和包含异构节点和边的问题。该模型可以增强GNN的泛化能力，适应多样的训练图结构的能力，并且不会增加计算开销。

    

    图神经网络已被广泛应用于图数据的学习。然而，现实世界中的图通常具有多样的图结构，并且包含异构节点和边。为了增强GNN的泛化能力，进一步提高训练图结构的多样性已成为常见做法。但是，简单地增加GNN模型容量将会导致更高的推理成本和GNN难以训练的问题。本文将专家混合（MoE）的思想引入到GNN中，旨在增强其适应多样的训练图结构的能力，而不会增加计算开销。我们的新图混合专家（GMoE）模型使得图中的每个节点可以动态地选择其自己的最佳信息。

    Graph neural networks (GNNs) have been widely applied to learning over graph data. Yet, real-world graphs commonly exhibit diverse graph structures and contain heterogeneous nodes and edges. Moreover, to enhance the generalization ability of GNNs, it has become common practice to further increase the diversity of training graph structures by incorporating graph augmentations and/or performing large-scale pre-training on more graphs. Therefore, it becomes essential for a GNN to simultaneously model diverse graph structures. Yet, naively increasing the GNN model capacity will suffer from both higher inference costs and the notorious trainability issue of GNNs. This paper introduces the Mixture-of-Expert (MoE) idea to GNNs, aiming to enhance their ability to accommodate the diversity of training graph structures, without incurring computational overheads. Our new Graph Mixture of Expert (GMoE) model enables each node in the graph to dynamically select its own optimal \textit{information a
    
[^68]: UNICORN: 一种统一的后门触发反演框架

    UNICORN: A Unified Backdoor Trigger Inversion Framework. (arXiv:2304.02786v1 [cs.LG])

    [http://arxiv.org/abs/2304.02786](http://arxiv.org/abs/2304.02786)

    本论文提出了一个基于触发器反演的统一框架 UNICORN，可用于识别后门模型并理解植入的恶意行为。

    

    后门攻击是一种严重威胁深度神经网络模型的攻击手段，攻击者通过注入带有触发信号的输入数据（如补丁）来激活预先植入的恶意行为。触发信号反演是一种有效的后门模型识别和对植入进去的恶意行为进行理解的方法。现有方法在反演触发信号时会有许多不同的构造方式，但因为采用了一些过于具体的假设或者攻击方式特定的限制，这些方法就无法泛化到各种类型的触发信号上。根本原因是现有工作没有考虑触发器设计空间对反演问题的影响。本文正式定义和分析了不同空间中嵌入的触发器及其反演问题，然后提出了一个基于反演问题的触发器形式化定义和分析识别后门模型内部行为的统一框架。我们的原型 UNICORN 在泛化性和有效性方面表现优异。

    The backdoor attack, where the adversary uses inputs stamped with triggers (e.g., a patch) to activate pre-planted malicious behaviors, is a severe threat to Deep Neural Network (DNN) models. Trigger inversion is an effective way of identifying backdoor models and understanding embedded adversarial behaviors. A challenge of trigger inversion is that there are many ways of constructing the trigger. Existing methods cannot generalize to various types of triggers by making certain assumptions or attack-specific constraints. The fundamental reason is that existing work does not consider the trigger's design space in their formulation of the inversion problem. This work formally defines and analyzes the triggers injected in different spaces and the inversion problem. Then, it proposes a unified framework to invert backdoor triggers based on the formalization of triggers and the identified inner behaviors of backdoor models from our analysis. Our prototype UNICORN is general and effective in
    
[^69]: 基于Transformer的深度学习方法公正预测肝移植后风险因素

    A Transformer-Based Deep Learning Approach for Fairly Predicting Post-Liver Transplant Risk Factors. (arXiv:2304.02780v1 [cs.LG])

    [http://arxiv.org/abs/2304.02780](http://arxiv.org/abs/2304.02780)

    本研究提出了一个基于Transformer的深度学习框架模型，可同时预测五种移植后风险并实现更好的性能。

    

    肝移植是对于晚期肝病患者的一项拯救性手术，而该手术存在两个主要挑战：为供体寻找最佳匹配的受体，以及在不同亚群体之间确保移植的公平性。传统的MELD评分系统只能评估在90天内未接受器官移植的患者的死亡风险。然而，供受体匹配也应该考虑到移植后的风险因素，如心血管疾病、慢性排异等，这些都是移植后常见的并发症。准确预测这些风险分数仍然是一个重大挑战。本研究提出了一个深度学习框架模型，通过将其制定为多任务学习问题，在数据上训练了提出的深度神经网络，可同时预测五种移植后风险并实现更好的性能。

    Liver transplantation is a life-saving procedure for patients with end-stage liver disease. There are two main challenges in liver transplant: finding the best matching patient for a donor and ensuring transplant equity among different subpopulations. The current MELD scoring system evaluates a patient's mortality risk if not receiving an organ within 90 days. However, the donor-patient matching should also take into consideration post-transplant risk factors, such as cardiovascular disease, chronic rejection, etc., which are all common complications after transplant. Accurate prediction of these risk scores remains a significant challenge. In this study, we will use predictive models to solve the above challenge. We propose a deep learning framework model to predict multiple risk factors after a liver transplant. By formulating it as a multi-task learning problem, the proposed deep neural network was trained on this data to simultaneously predict the five post-transplant risks and ach
    
[^70]: 基于Transformer的方法在电子病历中的应用：系统性文献综述

    Application of Transformers based methods in Electronic Medical Records: A Systematic Literature Review. (arXiv:2304.02768v1 [cs.CL])

    [http://arxiv.org/abs/2304.02768](http://arxiv.org/abs/2304.02768)

    这篇论文综述了基于Transformer的自然语言处理技术在电子病历领域中的应用，并提出了目前研究中的限制和未来研究的方向。

    

    由于可用数据的增长和它们的非结构化性质，越来越多的自然语言处理（NLP）技术开始受到关注，以从这些数据资产中获得价值，因为这种格式不适用于统计分析。本文对不同NLP任务中基于变压器的EMR上的最新进展进行了系统性的文献综述。在最初的查询中，从三个公共数据库中选择了99篇文章，最终筛选得到了65篇文章进行详细分析。本文将从业务问题、NLP任务、模型和技术、数据集的可用性、建模的可重复性、语言和交换格式等方面对这些论文进行分析。文章提出了当前研究的一些局限性以及进一步研究的建议。

    The combined growth of available data and their unstructured nature has received increased interest in natural language processing (NLP) techniques to make value of these data assets since this format is not suitable for statistical analysis. This work presents a systematic literature review of state-of-the-art advances using transformer-based methods on electronic medical records (EMRs) in different NLP tasks. To the best of our knowledge, this work is unique in providing a comprehensive review of research on transformer-based methods for NLP applied to the EMR field. In the initial query, 99 articles were selected from three public databases and filtered into 65 articles for detailed analysis. The papers were analyzed with respect to the business problem, NLP task, models and techniques, availability of datasets, reproducibility of modeling, language, and exchange format. The paper presents some limitations of current research and some recommendations for further research.
    
[^71]: MethaneMapper: 光谱吸收感知高光谱转换器用于甲烷检测。

    MethaneMapper: Spectral Absorption aware Hyperspectral Transformer for Methane Detection. (arXiv:2304.02767v1 [eess.IV])

    [http://arxiv.org/abs/2304.02767](http://arxiv.org/abs/2304.02767)

    MethaneMapper是一个可用于光谱域内定位甲烷排放区域的Transformer网络，并在模型尺寸上实现了优化。同时，作者介绍了一个用于研究甲烷检测问题的大规模数据集。

    

    甲烷(CH4)是全球气候变化的主要贡献者。本文提出了一个全新的端到端的光谱吸收波长感知Transformer网络MethaneMapper，用于检测和定量排放。MethaneMapper引入了两个新模块，帮助在光谱域中定位最相关的甲烷云区域，并用于准确地定位它们。充分的评估表明MethaneMapper在检测方面达到了0.63 mAP，并在模型尺寸上（缩小5倍）与现有技术水平相比实现了优化。此外，我们还介绍了一个大规模的甲烷云分割数据集，包括超过1000张AVIRIS-NG图像以及它们的真实世界地理参考数据。这个数据集将帮助研究人员更好地理解和解决在红外和可见光波长下的甲烷检测问题。

    Methane (CH$_4$) is the chief contributor to global climate change. Recent Airborne Visible-Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) has been very useful in quantitative mapping of methane emissions. Existing methods for analyzing this data are sensitive to local terrain conditions, often require manual inspection from domain experts, prone to significant error and hence are not scalable. To address these challenges, we propose a novel end-to-end spectral absorption wavelength aware transformer network, MethaneMapper, to detect and quantify the emissions. MethaneMapper introduces two novel modules that help to locate the most relevant methane plume regions in the spectral domain and uses them to localize these accurately. Thorough evaluation shows that MethaneMapper achieves 0.63 mAP in detection and reduces the model size (by 5x) compared to the current state of the art. In addition, we also introduce a large-scale dataset of methane plume segmentation mask for over 1
    
[^72]: 混合区间多面体精确表示ReLU神经网络

    Hybrid Zonotopes Exactly Represent ReLU Neural Networks. (arXiv:2304.02755v1 [cs.LG])

    [http://arxiv.org/abs/2304.02755](http://arxiv.org/abs/2304.02755)

    混合区间多面体精确表示ReLU神经网络，并且二进制变量随网络规模线性增长，实用性广泛。

    

    我们展示了混合区间多面体提供了一种等价的表示前馈全连接ReLU激活函数神经网络的方法。我们的方法证明了二进制变量的复杂度等于网络中神经元的总数，因此随着网络规模的增大而线性增长。我们通过三个案例研究展示了混合区间多面体公式的实用性，包括非线性函数逼近、MPC闭环可达性和验证，以及对MNIST数据集的分类鲁棒性。

    We show that hybrid zonotopes offer an equivalent representation of feed-forward fully connected neural networks with ReLU activation functions. Our approach demonstrates that the complexity of binary variables is equal to the total number of neurons in the network and hence grows linearly in the size of the network. We demonstrate the utility of the hybrid zonotope formulation through three case studies including nonlinear function approximation, MPC closed-loop reachability and verification, and robustness of classification on the MNIST dataset.
    
[^73]: 人类和大型语言模型中的概念结构表现的差异性

    Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models. (arXiv:2304.02754v1 [cs.AI])

    [http://arxiv.org/abs/2304.02754](http://arxiv.org/abs/2304.02754)

    本研究使用两种经典认知心理学技术来估算人类和GPT-3等大型语言模型的词汇语义结构，结果表明人类的概念结构稳健鲁棒，而大型语言模型的行为估算结构更多取决于具体任务。

    

    多年以来，神经网络语言模型一直被用作研究心理和脑部概念表征的工具。然而，在当代语言人工智能中，我们可以使用与人类参与者几乎相同的方法来探讨概念表征的潜在结构。本研究使用两种经典的认知心理学技术来估算和比较人类和一个著名的大型语言模型（GPT-3的DaVinci变体）的词汇语义结构。研究表明，人类的概念结构强大且鲁棒，不受文化、语言和估算方法的差异影响；大型语言模型中的行为估算结果相对稳定，但具体取决于任务本身。这些结果表明，虽然人类参与者的行为估算结果可靠，但在使用大型语言模型进行人类认知处理相关推断时，需要谨慎。

    Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language AIs, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly used with human participants. The current work uses two common techniques borrowed from cognitive psychology to estimate and compare lexical-semantic structure in both humans and a well-known AI, the DaVinci variant of GPT-3. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from AI behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task 
    
[^74]: 使用半监督生成对抗网络检测孟加拉语假评论

    Bengali Fake Review Detection using Semi-supervised Generative Adversarial Networks. (arXiv:2304.02739v1 [cs.CL])

    [http://arxiv.org/abs/2304.02739](http://arxiv.org/abs/2304.02739)

    本文研究使用半监督生成对抗网络以少量数据分类孟加拉语假评论和真实评论的潜力，并提出了BanglaBERT与半监督GAN相结合的解决方案，实验结果表明其准确率达到83.59％，f1分数达到84.89％。

    

    本文研究使用半监督生成对抗网络（GAN）微调预训练语言模型，以少量已注释数据来分类孟加拉语假评论和真实评论的潜力。随着社交媒体和电子商务的兴起，能够检测虚假或欺骗性评论变得越来越重要，以保护消费者免受虚假信息的误导。任何机器学习模型在识别假评论方面都会遇到困难，特别是对于像孟加拉语这样的低资源语言。我们证明了所提出的半监督GAN-LM体系结构（预训练语言模型之上的生成对抗网络）是一个可行的解决方案，实验结果表明，即使只有1024个已注释的样本，使用半监督GAN的BanglaBERT的准确率达到83.59％，f1分数达到84.89％，优于其他预训练语言模型BanglaBERT生成器。

    This paper investigates the potential of semi-supervised Generative Adversarial Networks (GANs) to fine-tune pretrained language models in order to classify Bengali fake reviews from real reviews with a few annotated data. With the rise of social media and e-commerce, the ability to detect fake or deceptive reviews is becoming increasingly important in order to protect consumers from being misled by false information. Any machine learning model will have trouble identifying a fake review, especially for a low resource language like Bengali. We have demonstrated that the proposed semi-supervised GAN-LM architecture (generative adversarial network on top of a pretrained language model) is a viable solution in classifying Bengali fake reviews as the experimental results suggest that even with only 1024 annotated samples, BanglaBERT with semi-supervised GAN (SSGAN) achieved an accuracy of 83.59% and a f1-score of 84.89% outperforming other pretrained language models BanglaBERT generator,
    
[^75]: 基于视觉端到端驾驶策略的学习稳定性注意力

    Learning Stability Attention in Vision-based End-to-end Driving Policies. (arXiv:2304.02733v1 [cs.RO])

    [http://arxiv.org/abs/2304.02733](http://arxiv.org/abs/2304.02733)

    这篇论文提出了一种利用控制李亚普诺夫函数（CLFs）为端到端基于视觉的策略配备稳定属性，并引入CLFs中的稳定性注意力（att-CLFs）来处理环境变化并提高学习灵活性的方法。

    

    现代端到端学习系统可以从感知中学习控制。然而，由于这些系统往往接收到未经结构化、高维度和复杂的观测空间，如从像素输入流中进行自主驾驶，因此保证这些系统的稳定性和鲁棒性很困难。我们提出利用控制李亚普诺夫函数（CLFs）为端到端基于视觉的策略配备稳定属性，并引入CLFs中的稳定性注意力（att-CLFs）来处理环境变化并提高学习的灵活性。我们还提出了一种紧密集成到att-CLFs中的不确定性传播技术。我们在真实环境和真实全尺寸自主车辆上，通过与经典CLFs、模型预测控制和香草端到端学习的比较，展示了att-CLFs的有效性。

    Modern end-to-end learning systems can learn to explicitly infer control from perception. However, it is difficult to guarantee stability and robustness for these systems since they are often exposed to unstructured, high-dimensional, and complex observation spaces (e.g., autonomous driving from a stream of pixel inputs). We propose to leverage control Lyapunov functions (CLFs) to equip end-to-end vision-based policies with stability properties and introduce stability attention in CLFs (att-CLFs) to tackle environmental changes and improve learning flexibility. We also present an uncertainty propagation technique that is tightly integrated into att-CLFs. We demonstrate the effectiveness of att-CLFs via comparison with classical CLFs, model predictive control, and vanilla end-to-end learning in a photo-realistic simulator and on a real full-scale autonomous vehicle.
    
[^76]: 利用樱桃挑选和机器学习构建系统发育网络

    Constructing Phylogenetic Networks via Cherry Picking and Machine Learning. (arXiv:2304.02729v1 [q-bio.PE])

    [http://arxiv.org/abs/2304.02729](http://arxiv.org/abs/2304.02729)

    本文提出了一种基于樱桃挑选理论框架的启发式算法，通过设计和训练能够捕捉输入树结构信息并指导算法产生更好解决方案的机器学习模型，实现对由二叉树组成的实际规模数据集的系统发育网络构建，方法具有良好的计算效率和准确性。

    

    将一组系统发育树合并为一个解释它们的系统发育网络是演化研究中的基本挑战。现有方法计算成本高，只能处理少量的系统发育树，或仅限于严格受限的网络类别。在本文中，我们应用了最近引入的樱桃挑选理论框架，设计了一类有效启发式算法，保证生成一个包含输入树的网络，用于由二叉树组成的数据集。在该框架中，我们的一些启发式算法基于设计和训练能够捕捉输入树结构重要信息并指导算法产生更好解决方案的机器学习模型。我们还提出了简单快速的随机启发式算法，证明在多次运行时非常有效。与现有的精确方法不同，我们的启发式算法适用于实际规模的数据集，并处理广泛的网络结构。我们在模拟和真实的生物数据集上评估了我们的方法，并证明了它们在计算效率和准确性方面优于现有技术方法。

    Combining a set of phylogenetic trees into a single phylogenetic network that explains all of them is a fundamental challenge in evolutionary studies. Existing methods are computationally expensive and can either handle only small numbers of phylogenetic trees or are limited to severely restricted classes of networks. In this paper, we apply the recently-introduced theoretical framework of cherry picking to design a class of efficient heuristics that are guaranteed to produce a network containing each of the input trees, for datasets consisting of binary trees. Some of the heuristics in this framework are based on the design and training of a machine learning model that captures essential information on the structure of the input trees and guides the algorithms towards better solutions. We also propose simple and fast randomised heuristics that prove to be very effective when run multiple times.  Unlike the existing exact methods, our heuristics are applicable to datasets of practical 
    
[^77]: FMG-Net和W-Net：受多重网格启发的医学图像分割深度学习架构

    FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For Medical Imaging Segmentation. (arXiv:2304.02725v1 [eess.IV])

    [http://arxiv.org/abs/2304.02725](http://arxiv.org/abs/2304.02725)

    FMG-Net和W-Net是两种受多重网格启发的深度学习架构，能够解决医学图像分割中面临的细节特征和尺度变化的挑战，能够提高肿瘤分割的精度。

    

    准确的医学图像分割对于精确和有效的医疗干预至关重要。然而，尽管卷积神经网络(CNN)在医学图像分割方面取得了成功，但它们仍面临着处理细粒度特征和图像尺度变化的挑战。这些挑战在复杂和具有挑战性的分割任务中特别明显，例如BraTS多标签脑肿瘤分割挑战赛中。在这个任务中，精确地分割不同的肿瘤亚组分，在大小和形状上都有显著变化，仍然是一个重大挑战，即使是最先进的方法也会产生重大错误。因此，我们提出了两种架构，FMG-Net和W-Net，它们将解线性方程组的几何多重网格方法的原理纳入CNN中，以解决这些挑战。我们在BraTS 2020数据集上的实验表明，FMG-Net和W-Net都优于广泛使用的U-Net架构，特别是在分割中的tum的精度方面。

    Accurate medical imaging segmentation is critical for precise and effective medical interventions. However, despite the success of convolutional neural networks (CNNs) in medical image segmentation, they still face challenges in handling fine-scale features and variations in image scales. These challenges are particularly evident in complex and challenging segmentation tasks, such as the BraTS multi-label brain tumor segmentation challenge. In this task, accurately segmenting the various tumor sub-components, which vary significantly in size and shape, remains a significant challenge, with even state-of-the-art methods producing substantial errors. Therefore, we propose two architectures, FMG-Net and W-Net, that incorporate the principles of geometric multigrid methods for solving linear systems of equations into CNNs to address these challenges. Our experiments on the BraTS 2020 dataset demonstrate that both FMG-Net and W-Net outperform the widely used U-Net architecture regarding tum
    
[^78]: 探索自监督预训练策略在M-模式肺部超声波中检测缺失肺滑动性的实用性。

    Exploring the Utility of Self-Supervised Pretraining Strategies for the Detection of Absent Lung Sliding in M-Mode Lung Ultrasound. (arXiv:2304.02724v1 [cs.CV])

    [http://arxiv.org/abs/2304.02724](http://arxiv.org/abs/2304.02724)

    本研究探索了在M-模式肺部超声图像中使用自监督预训练来提高缺失肺滑动检测的分类性能。结果表明自监督预训练可以显著提高性能，并且使用大量未标记数据可以提高模型的通用性和外部验证性能。

    

    自监督预训练已被证实可以提高医学影像中监督学习任务的性能。本研究调查了在进行监督微调之前自监督预训练的实用性，用于M-模式肺部超声波图像中的肺滑动分类任务。我们提出了一种新的配对关系，将从同一B-模式图像构建的M-模式图像进行了配对，并研究了特定于M-模式肺部超声波的数据增强程序的实用性。结果表明，自监督预训练比完全监督更能提高性能，尤其适用于未使用ImageNet预训练权重的特征提取器。此外，我们观察到包括大量未标记数据会导致在外部验证数据集上提高性能，凸显了自监督性的价值，以提高自动超声解释的通用性。作者最好的知识，本研究是第一个探索使用自监督预训练来解决M-模式肺部超声图像分类问题的工作。

    Self-supervised pretraining has been observed to improve performance in supervised learning tasks in medical imaging. This study investigates the utility of self-supervised pretraining prior to conducting supervised fine-tuning for the downstream task of lung sliding classification in M-mode lung ultrasound images. We propose a novel pairwise relationship that couples M-mode images constructed from the same B-mode image and investigate the utility of data augmentation procedure specific to M-mode lung ultrasound. The results indicate that self-supervised pretraining yields better performance than full supervision, most notably for feature extractors not initialized with ImageNet-pretrained weights. Moreover, we observe that including a vast volume of unlabelled data results in improved performance on external validation datasets, underscoring the value of self-supervision for improving generalizability in automatic ultrasound interpretation. To the authors' best knowledge, this study i
    
[^79]: 结构化提示询问与递归语义提取（SPIRES）：使用零样本学习填充知识库的方法

    Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. (arXiv:2304.02711v1 [cs.AI])

    [http://arxiv.org/abs/2304.02711](http://arxiv.org/abs/2304.02711)

    SPIRES是一种新的知识提取方法，利用大型语言模型进行零样本学习和通用查询回答，能够填充复杂的知识库而无需显式训练数据。

    

    创建知识库和本体是一项耗时的任务，依赖于手动管理。AI / NLP方法可以帮助专业策展人填充这些知识库，但当前方法依赖于大量训练数据，并且不能填充任意复杂的嵌套知识模式。在这里我们提出了Structured Prompt Interrogation and Recursive Extraction of Semantics（SPIRES），一种知识提取方法，该方法依赖于大型语言模型（LLM）执行零样本学习（ZSL）和通用查询回答，以及从灵活提示返回符合指定模式的信息。 SPIRES针对给定的详细用户定义的知识模式和输入文本，对GPT-3+执行递归提示询问，以获得与提供的模式匹配的一组响应。 SPIRES使用现有的本体和词汇表为所有匹配元素提供标识符。 我们提供了在不同领域（包括音乐，体育和政治）中使用SPIRES的示例，展示了其能够填充复杂的知识库而无需显式训练数据。

    Creating knowledge bases and ontologies is a time consuming task that relies on a manual curation. AI/NLP approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrary complex nested knowledge schemas.  Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, SPIRES recursively performs prompt interrogation against GPT-3+ to obtain a set of responses matching the provided schema. SPIRES uses existing ontologies and vocabularies to provide identifiers for all matched elements.  We present examples of use of SPIRES in different domains, inc
    
[^80]: 无偏关于坡度函数的适当学习：越过黑盒修正障碍

    Agnostic proper learning of monotone functions: beyond the black-box correction barrier. (arXiv:2304.02700v1 [cs.DS])

    [http://arxiv.org/abs/2304.02700](http://arxiv.org/abs/2304.02700)

    本文提出了第一个无偏、高效、适当的单调布尔函数学习算法，算法的运行时间和假设的大小和评估时间都为$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$，该算法解决了样本高效算法无法解决的问题。

    

    本文提出了第一个无偏、高效、适当的单调布尔函数学习算法。给定未知函数$f:\{\pm 1\}^n \rightarrow \{\pm 1\}$的$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$个均匀随机样本，算法输出一个假设$g:\{\pm 1\}^n \rightarrow \{\pm 1\}$，该假设是单调的，并且与$f$的距离为$(\mathrm{opt} + \varepsilon)$，其中$\mathrm{opt}$是$f$与最近单调函数之间的距离。算法的运行时间（因此也是假设的大小和评估时间）也是$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$，几乎与Blais等人（RANDOM '15）的下界相匹配。我们还给出一个算法，用于估计未知函数$f$到单调性的添加误差$\varepsilon$的距离，其运行时间为$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$。以前，针对这两个问题，已知有样本有效的算法，但这些算法并不是运行时间有效的。因此，我们的工作解决了这个问题。

    We give the first agnostic, efficient, proper learning algorithm for monotone Boolean functions. Given $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$ uniformly random examples of an unknown function $f:\{\pm 1\}^n \rightarrow \{\pm 1\}$, our algorithm outputs a hypothesis $g:\{\pm 1\}^n \rightarrow \{\pm 1\}$ that is monotone and $(\mathrm{opt} + \varepsilon)$-close to $f$, where $\mathrm{opt}$ is the distance from $f$ to the closest monotone function. The running time of the algorithm (and consequently the size and evaluation time of the hypothesis) is also $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$, nearly matching the lower bound of Blais et al (RANDOM '15). We also give an algorithm for estimating up to additive error $\varepsilon$ the distance of an unknown function $f$ to monotone using a run-time of $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$. Previously, for both of these problems, sample-efficient algorithms were known, but these algorithms were not run-time efficient. Our work thus closes this g
    
[^81]: 革命性的单细胞分析：大语言模型在细胞类型注释中的威力。

    Revolutionizing Single Cell Analysis: The Power of Large Language Models for Cell Type Annotation. (arXiv:2304.02697v1 [q-bio.GN])

    [http://arxiv.org/abs/2304.02697](http://arxiv.org/abs/2304.02697)

    大语言模型的出现革命性地改变了单细胞分析，能够更有效、准确地进行细胞类型注释，揭示以前被忽视的细胞亚型的特定分化轨迹，对癌症、发育和干细胞分化的理解有重要应用。

    

    近年来，单细胞RNA测序已成为研究细胞多样性和功能的常用技术。然而，精确地从单细胞数据中注释细胞类型一直是一项具有挑战性的任务，因为它需要对细胞生物学和基因功能有广泛的认识。2023年ChatGPT和New Bing等大语言模型的出现革命性地改变了这个过程，它们将科学文献整合并提供准确的细胞类型注释。这一突破使研究人员能够更有效、准确地进行文献综述，并有可能揭示细胞类型注释中的新见解。通过使用ChatGPT来注释单细胞数据，我们可以将罕见的细胞类型与其功能联系起来，揭示以前被忽视的细胞亚型的特定分化轨迹。这可以在理解癌症进展、哺乳动物发育和干细胞分化方面具有重要应用，并有可能促进诊断和治疗的发展。

    In recent years, single cell RNA sequencing has become a widely used technique to study cellular diversity and function. However, accurately annotating cell types from single cell data has been a challenging task, as it requires extensive knowledge of cell biology and gene function. The emergence of large language models such as ChatGPT and New Bing in 2023 has revolutionized this process by integrating the scientific literature and providing accurate annotations of cell types. This breakthrough enables researchers to conduct literature reviews more efficiently and accurately, and can potentially uncover new insights into cell type annotation. By using ChatGPT to annotate single cell data, we can relate rare cell type to their function and reveal specific differentiation trajectories of cell subtypes that were previously overlooked. This can have important applications in understanding cancer progression, mammalian development, and stem cell differentiation, and can potentially lead to
    
[^82]: 一种基于认证半径的攻击框架用于图像分割模型

    A Certified Radius-Guided Attack Framework to Image Segmentation Models. (arXiv:2304.02693v1 [cs.CV])

    [http://arxiv.org/abs/2304.02693](http://arxiv.org/abs/2304.02693)

    本文提出了一种基于认证半径的攻击框架用于图像分割模型，该攻击框架主要通过引导搜索具有相对较小认证半径的像素来实现对图像分割模型的攻击。

    

    图像分割是许多安全关键应用中的重要问题。最近的研究表明，现代图像分割模型易受对抗扰动的影响，而现有的攻击方法主要是攻击图像分类模型。我们认为图像分割和分类有本质区别，并为图像分割模型设计了一种攻击框架。我们的攻击框架受认证半径启发，该方法最初是由防御者用来防御分类模型的对抗扰动。我们是第一个从攻击者的角度出发，利用认证半径的性质提出了一种针对图像分割模型的攻击框架。具体而言，我们首先将针对分类模型的最先进的认证方法随机平滑法进行调整，以导出像素的认证半径。然后，我们更加关注具有相对较小认证半径的像素的破坏，并提出了一种引导搜索方法，能够在给定距离限制下高效地找到最大化攻击成功率的强健扰动。多个基准数据集上的实验证明了我们的攻击框架对多种最先进的图像分割模型的有效性。

    Image segmentation is an important problem in many safety-critical applications. Recent studies show that modern image segmentation models are vulnerable to adversarial perturbations, while existing attack methods mainly follow the idea of attacking image classification models. We argue that image segmentation and classification have inherent differences, and design an attack framework specially for image segmentation models. Our attack framework is inspired by certified radius, which was originally used by defenders to defend against adversarial perturbations to classification models. We are the first, from the attacker perspective, to leverage the properties of certified radius and propose a certified radius guided attack framework against image segmentation models. Specifically, we first adapt randomized smoothing, the state-of-the-art certification method for classification models, to derive the pixel's certified radius. We then focus more on disrupting pixels with relatively small
    
[^83]: ACTION++：使用自适应解剖对比度改善半监督医学图像分割

    ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v1 [cs.CV])

    [http://arxiv.org/abs/2304.02689](http://arxiv.org/abs/2304.02689)

    本文提出了一种改进的对比学习框架ACTION++，通过自适应的解剖对比来改善半监督医学图像分割。

    

    医学数据通常表现为长尾分布，存在严重的类别不平衡，这自然导致少数类别（即边界区域或罕见物体）的分类困难。最近的工作通过配备无监督对比标准，在长尾场景中显着改进了半监督医学图像分割。然而，在类别分布也高度不平衡的标记数据部分中，它们的表现仍不清楚。在这项工作中，我们提出了ACTION++，一种改进的具有自适应解剖对比的对比学习框架，用于半监督医学分割。

    Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes (i.e., boundary regions or rare objects). Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced. In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed cla
    
[^84]: 预测编码作为神经形态学替代反向传播算法的一个关键评估

    Predictive Coding as a Neuromorphic Alternative to Backpropagation: A Critical Evaluation. (arXiv:2304.02658v1 [cs.NE])

    [http://arxiv.org/abs/2304.02658](http://arxiv.org/abs/2304.02658)

    预测编码算法被认为是反向传播的一个替代方案，在神经形态学系统中具有潜力。研究者通过使用现有的 PC 变体探讨了这个问题，并给出了时间复杂度下界，揭示了 PC 的一些有趣的特性，包括其神经生物学可行性和潜在的贝叶斯推理解释。

    

    反向传播已经快速成为现代深度学习方法的主要学分分配算法。最近，计算神经科学起源于的修改形式的预测编码（PC）已被表明可以得到与反向传播完全相同或近似相等的参数更新。由于这种联系，有人认为PC可以作为反向传播的替代方案，并具有有利的性质，有助于在神经形态学系统中实现。在这里，我们使用文献中提出的不同的当代PC变体来探讨这些声明。我们获得了这些PC变体的时间复杂度下界，并显示它们低于反向传播。我们还介绍了这些变体的主要属性，这些属性涉及神经生物学的可行性和它们的解释，尤其是从标准PC作为潜在概率模型的变分贝叶斯算法的角度来看。我们的发现揭示了PC作为神经形态学系统中反向传播替代方案的潜力。

    Backpropagation has rapidly become the workhorse credit assignment algorithm for modern deep learning methods. Recently, modified forms of predictive coding (PC), an algorithm with origins in computational neuroscience, have been shown to result in approximately or exactly equal parameter updates to those under backpropagation. Due to this connection, it has been suggested that PC can act as an alternative to backpropagation with desirable properties that may facilitate implementation in neuromorphic systems. Here, we explore these claims using the different contemporary PC variants proposed in the literature. We obtain time complexity bounds for these PC variants which we show are lower-bounded by backpropagation. We also present key properties of these variants that have implications for neurobiological plausibility and their interpretations, particularly from the perspective of standard PC as a variational Bayes algorithm for latent probabilistic models. Our findings shed new light 
    
[^85]: 交互生物分子系统的图形表示学习

    Graph Representation Learning for Interactive Biomolecule Systems. (arXiv:2304.02656v1 [q-bio.QM])

    [http://arxiv.org/abs/2304.02656](http://arxiv.org/abs/2304.02656)

    本文综述了图形表示学习在生物分子系统中的重要性，并讨论了其对药物发现、蛋白质表征和生物系统分析的应用，同时总结了该领域的现状和未来研究方向。

    

    深度学习模型的进展已经彻底改变了生物分子系统和它们的机制的研究。其中图形表示学习特别重要，可以准确地捕捉不同层次的生物分子的几何信息。本文综述了用于将生物分子和系统表示为计算机可识别对象（例如序列、图形和表面）的方法学。此外，本文还讨论了几何深度学习模型（强调基于图形的技术）如何分析生物分子数据以实现药物发现、蛋白质表征和生物系统分析。研究最后总结了该领域的现状，凸显了存在的挑战和潜在的未来研究方向。

    Advances in deep learning models have revolutionized the study of biomolecule systems and their mechanisms. Graph representation learning, in particular, is important for accurately capturing the geometric information of biomolecules at different levels. This paper presents a comprehensive review of the methodologies used to represent biological molecules and systems as computer-recognizable objects, such as sequences, graphs, and surfaces. Moreover, it examines how geometric deep learning models, with an emphasis on graph-based techniques, can analyze biomolecule data to enable drug discovery, protein characterization, and biological system analysis. The study concludes with an overview of the current state of the field, highlighting the challenges that exist and the potential future research directions.
    
[^86]: 采用两个监督器用于大规模远程深度神经网络的有效使用

    Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks. (arXiv:2304.02654v1 [cs.LG])

    [http://arxiv.org/abs/2304.02654](http://arxiv.org/abs/2304.02654)

    本文提出了一种名为BiSupervised的新架构，使用两个监督器在调用大规模远程深度神经网络之前，对小规模本地模型进行预测，通过这种方式避免不必要的网络调用，可以节省成本。

    

    近几十年来，大规模深度神经网络的出现在各种人工智能任务中取得了人类竞争水平的性能。由于这些深度神经网络通常包含数十亿甚至数百亿参数，因此无法部署到资源受限的设备（如手机或物联网微控制器）上或高效运行。因此，使用大规模深度神经网络的系统需要通过网络调用相应的模型，导致托管和运行大规模远程模型的成本相当高，这些成本通常按使用次数收费。在本文中，我们提出了一种新的架构BiSupervised，它在依赖于大规模远程深度神经网络之前，尝试在小规模本地模型上进行预测，而一个DNN主管监控此预测过程并确定易于进行本地预测的输入。对于这些输入，无需调用远程模型，从而节省成本。

    Recent decades have seen the rise of large-scale Deep Neural Networks (DNNs) to achieve human-competitive performance in a variety of artificial intelligence tasks. Often consisting of hundreds of millions, if not hundreds of billion parameters, these DNNs are too large to be deployed to, or efficiently run on resource-constrained devices such as mobile phones or IoT microcontrollers. Systems relying on large-scale DNNs thus have to call the corresponding model over the network, leading to substantial costs for hosting and running the large-scale remote model, costs which are often charged on a per-use basis. In this paper, we propose BiSupervised, a novel architecture, where, before relying on a large remote DNN, a system attempts to make a prediction on a small-scale local model. A DNN supervisor monitors said prediction process and identifies easy inputs for which the local prediction can be trusted. For these inputs, the remote model does not have to be invoked, thus saving costs, 
    
[^87]: RARE：鲁棒性抗干扰的掩码图自编码器

    RARE: Robust Masked Graph Autoencoder. (arXiv:2304.01507v1 [cs.LG])

    [http://arxiv.org/abs/2304.01507](http://arxiv.org/abs/2304.01507)

    RARE是一种鲁棒性抗干扰的掩码图自编码器，通过在高阶潜在特征空间中进行掩码和重构节点样本来提高推断掩码数据的确定性和自监督机制的可靠性，并在下游任务中优于现有的SGP方法。

    

    掩码图自编码器（MGAE）由于其简单和有效的特性，在自监督图预训练（SGP）方面已成为一种很有前途的范例。然而，现有的方法在原始数据空间中执行掩码-重构操作，类似于计算机视觉（CV）和自然语言处理（NLP）领域，而忽略了图数据的重要非欧几里得属性。结果，高度不稳定的局部连接结构大大增加了推断掩码数据的不确定性，并降低了利用自监督信号的可靠性，导致下游评估中的表示效果不佳。为了解决这个问题，我们提出了一种新的SGP方法，称为Robust mAsked gRaph autoEncoder（RARE），通过高阶潜在特征空间中更多的掩码和重构节点样本来提高推断掩码数据的确定性和自监督机制的可靠性。通过理论和实证分析，我们发现RARE能够有效地捕捉图数据的内在结构，并在不同的下游任务中优于现有的SGP方法。

    Masked graph autoencoder (MGAE) has emerged as a promising self-supervised graph pre-training (SGP) paradigm due to its simplicity and effectiveness. However, existing efforts perform the mask-then-reconstruct operation in the raw data space as is done in computer vision (CV) and natural language processing (NLP) areas, while neglecting the important non-Euclidean property of graph data. As a result, the highly unstable local connection structures largely increase the uncertainty in inferring masked data and decrease the reliability of the exploited self-supervision signals, leading to inferior representations for downstream evaluations. To address this issue, we propose a novel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve the certainty in inferring masked data and the reliability of the self-supervision mechanism by further masking and reconstructing node samples in the high-order latent feature space. Through both theoretical and empirical analyses, we have dis
    
[^88]: 时间动态同步功能脑网络在精神分裂症诊断和侧化分析中的应用

    Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis. (arXiv:2304.01347v1 [q-bio.NC])

    [http://arxiv.org/abs/2304.01347](http://arxiv.org/abs/2304.01347)

    本文提出了一种基于动态功能连接的脑网络分析模型，通过构建动态同步特征和革命性的图卷积方法实现精神分裂症诊断和侧化分析，并在实验证明其表现优于其他最先进模型。

    

    有证据表明，动态功能连接可以捕捉静息态功能磁共振成像数据中的脑活动时变异常，并在揭示精神分裂症（SZ）患者异常脑活动机制方面具有天然优势。因此，本文采用了一种先进的动态脑网络分析模型——时态脑类别图卷积网络（temporal-BCGCN）。首先设计了独特的动态脑网络分析模块DSF-BrainNet，用于构建动态同步特征。随后，提出了一种革命性的图卷积方法TemporalConv，基于特征的同步时间属性。最后，提出了一种基于静息态功能磁共振成像数据的深度学习模块化异常半球侧化检测工具，称为CategoryPool。该研究在COBRE和UCLA数据集上进行验证，分别达到83.62％和89.71％的平均准确率，优于基线模型和其他最先进模型，在精神分裂症诊断和侧化分析方面表现出色。

    Available evidence suggests that dynamic functional connectivity (dFC) can capture time-varying abnormalities in brain activity in rs-fMRI data and has a natural advantage in uncovering mechanisms of abnormal brain activity in schizophrenia(SZ) patients. Hence, an advanced dynamic brain network analysis model called the temporal brain category graph convolutional network (temporal-BCGCN) was employed. Firstly, a unique dynamic brain network analysis module, DSF-BrainNet, was designed to construct dynamic synchronization features. Subsequently, a revolutionary graph convolution method, TemporalConv, was proposed, based on the synchronous temporal properties of feature. Finally, the first modular abnormal hemispherical lateralization test tool in deep learning based on rs-fMRI data, named CategoryPool, was proposed. This study was validated on COBRE and UCLA datasets and achieved 83.62% and 89.71% average accuracy, respectively, outperforming the baseline model and other State-of-the-Art
    
[^89]: 实践中知识图谱用户、挑战和可视化需求的特征化研究

    Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice. (arXiv:2304.01311v1 [cs.HC])

    [http://arxiv.org/abs/2304.01311](http://arxiv.org/abs/2304.01311)

    本研究通过访谈19位知识图谱（KG）实践者，发现KG构建者需求架构执行程序，KG分析师需要可自定义查询构建器，KG消费者需要领域特定可视化，并指出在实践中实施KG需要技术和社交方面的解决方案。

    

    本研究通过对19位来自企业和学术环境下、涉及各种用例的知识图谱（KG）实践者的访谈，提出了KG实践者在创建、探索和分析KG时遇到的重要挑战，这些挑战可以通过可视化设计来缓解。我们的研究发现，KG实践者可以分为三类：KG构建者、分析师和消费者，每个人都有自己的专业知识和需求。我们发现，KG构建者可以从架构执行程序中获益，而KG分析师需要提供中间查询结果的可自定义查询构建器。对于KG消费者，我们确定节点链接图的效力不足，并需要定制的领域特定可视化来促进KG的采用和理解。最后，我们发现，在实践中有效地实施KG需要不仅技术上的，还有社交上的解决方案，而这些解决方案目前并未被当前的工具、技术和最佳实践所考虑。

    This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners - KG Builders, Analysts, and Consumers - each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, tec
    
[^90]: POLAR-Express: 神经网络控制系统的高效准确形式可达性分析

    POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems. (arXiv:2304.01218v1 [eess.SY])

    [http://arxiv.org/abs/2304.01218](http://arxiv.org/abs/2304.01218)

    POLAR-Express 是一种高效且准确的形式可达性分析工具，用于验证神经网络控制系统的安全性。它使用 Taylor 模型算术和逐层传播技术，可以分析具有连续激活功能的前馈神经网络，并在 ReLU 激活函数上提供了一种更有效的精确传播 TM 的新方法。

    

    在挑战性的控制问题上，扮演控制器角色的神经网络 (NN) 展示出了令人印象深刻的实验性能。但神经网络控制系统 (NNCS) 在实际应用中的潜在采用也引起了日益增长的对这些 NNCS 安全性的担忧，特别是在安全关键应用中的使用。本文提出了 POLAR-Express，一种高效且准确的形式可达性分析工具，用于验证 NNCS 的安全性。POLAR-Express 使用 Taylor 模型算术，逐层横跨神经网络来传播 Taylor 模型 (TM) 以计算神经网络函数的近似值。它可以用于分析任何具有连续激活功能的前馈神经网络。我们还提出了一种在 ReLU 激活函数上更有效地精确传播 TM 的新方法。此外，POLAR-Express 为逐层传播提供了并行计算支持。

    Neural networks (NNs) playing the role of controllers have demonstrated impressive empirical performances on challenging control problems. However, the potential adoption of NN controllers in real-life applications also gives rise to a growing concern over the safety of these neural-network controlled systems (NNCSs), especially when used in safety-critical applications. In this work, we present POLAR-Express, an efficient and precise formal reachability analysis tool for verifying the safety of NNCSs. POLAR-Express uses Taylor model arithmetic to propagate Taylor models (TMs) across a neural network layer-by-layer to compute an overapproximation of the neural-network function. It can be applied to analyze any feed-forward neural network with continuous activation functions. We also present a novel approach to propagate TMs more efficiently and precisely across ReLU activation functions. In addition, POLAR-Express provides parallel computation support for the layer-by-layer propagation
    
[^91]: 数据科学中的可解释符号回归：2022年竞赛分析

    Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition. (arXiv:2304.01117v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.01117](http://arxiv.org/abs/2304.01117)

    本文分析了2022年基因与进化计算会议举办的竞赛，评估了符号回归的新方法在面对现实数据时的表现，并提供了可解释性评估的实际方法。

    

    符号回归是寻找能够准确描述研究现象的解析表达式的方法。这种方法的主要优势是返回可解释的模型，能够给用户提供深刻的见解。历史上，符号回归的大多数算法都基于进化算法。然而，最近出现了大量新的提案，这些提案使用了列举算法、混合线性整数规划、神经网络和贝叶斯优化等方法。为了评估这些新方法在面对现实世界数据中经常遇到的一组常见挑战时的表现如何，我们在2022年遗传与进化计算会议上举办了一次竞赛，其中包含不同的合成和真实世界数据集，参赛者对这些数据集是盲测试的。对于真实世界的部分，我们使用了领域专家来评估候选模型的可解释性。我们对结果进行了深入分析。

    Symbolic regression searches for analytic expressions that accurately describe studied phenomena. The main attraction of this approach is that it returns an interpretable model that can be insightful to users. Historically, the majority of algorithms for symbolic regression have been based on evolutionary algorithms. However, there has been a recent surge of new proposals that instead utilize approaches such as enumeration algorithms, mixed linear integer programming, neural networks, and Bayesian optimization. In order to assess how well these new approaches behave on a set of common challenges often faced in real-world data, we hosted a competition at the 2022 Genetic and Evolutionary Computation Conference consisting of different synthetic and real-world datasets which were blind to entrants. For the real-world track, we assessed interpretability in a realistic way by using a domain expert to judge the trustworthiness of candidate models.We present an in-depth analysis of the result
    
[^92]: 基于Cityscapes-3D的联合2D-3D多任务学习：3D检测、分割和深度估计

    Joint 2D-3D Multi-Task Learning on Cityscapes-3D: 3D Detection, Segmentation, and Depth Estimation. (arXiv:2304.00971v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.00971](http://arxiv.org/abs/2304.00971)

    该论文提出了一种基于Cityscapes-3D的联合2D-3D多任务学习方法，旨在同时实现单眼3D车辆检测、语义分割和单眼深度估计，并通过优化多个目标单元，提高了模型性能。

    

    这份报告是TaskPrompter在基于Cityscapes-3D的新联合2D-3D多任务学习标准上的实现的补充文档。TaskPrompter提出了一个创新的多任务提示框架，将（i）任务通用表示、（ii）任务特定表示和（iii）跨任务交互的学习统一起来，与以往的方法将这些学习目标分别存放在不同的网络模块中相反。这种统一的方法不仅减少了对结构设计的细致经验需求，还显著增强了多任务网络的表示学习能力，因为整个模型容量都致力于同时优化这三个目标。TaskPrompter在Cityscapes-3D数据集上引入了一个新的多任务基准，要求多任务模型同时为单眼3D车辆检测、语义分割和单眼深度估计生成预测。

    This report serves as a supplementary document for TaskPrompter, detailing its implementation on a new joint 2D-3D multi-task learning benchmark based on Cityscapes-3D. TaskPrompter presents an innovative multi-task prompting framework that unifies the learning of (i) task-generic representations, (ii) task-specific representations, and (iii) cross-task interactions, as opposed to previous approaches that separate these learning objectives into different network modules. This unified approach not only reduces the need for meticulous empirical structure design but also significantly enhances the multi-task network's representation learning capability, as the entire model capacity is devoted to optimizing the three objectives simultaneously. TaskPrompter introduces a new multi-task benchmark based on Cityscapes-3D dataset, which requires the multi-task model to concurrently generate predictions for monocular 3D vehicle detection, semantic segmentation, and monocular depth estimation. The
    
[^93]: 使用先验引导知识改进快速对抗训练

    Improving Fast Adversarial Training with Prior-Guided Knowledge. (arXiv:2304.00202v1 [cs.LG])

    [http://arxiv.org/abs/2304.00202](http://arxiv.org/abs/2304.00202)

    本文提出了一种使用先前训练过程中高质量对抗扰动的正面先验引导对抗初始化方法，以提高对抗样本的质量，从而避免快速对抗训练中的灾难性过度拟合问题。

    

    快速对抗训练是提高模型鲁棒性的有效方法。然而，原始的快速对抗训练会遭受灾难性的过度拟合问题，在经过几个训练周期后鲁棒性会急剧下降。虽然已经提出了各种快速对抗训练的变体来防止过度拟合，但它们需要较高的训练成本。本文通过比较标准对抗训练和快速对抗训练的训练过程，研究了对抗样本质量和灾难性过度拟合之间的关系。我们发现，当对抗样本的攻击成功率变差时，就会发生灾难性的过度拟合。基于这一观察，我们提出了一种使用高质量对抗扰动的正面先验引导对抗初始化方法，以提高对抗样本的质量，从而避免额外的训练成本。我们提供了该初始化方法的理论分析。

    Fast adversarial training (FAT) is an efficient method to improve robustness. However, the original FAT suffers from catastrophic overfitting, which dramatically and suddenly reduces robustness after a few training epochs. Although various FAT variants have been proposed to prevent overfitting, they require high training costs. In this paper, we investigate the relationship between adversarial example quality and catastrophic overfitting by comparing the training processes of standard adversarial training and FAT. We find that catastrophic overfitting occurs when the attack success rate of adversarial examples becomes worse. Based on this observation, we propose a positive prior-guided adversarial initialization to prevent overfitting by improving adversarial example quality without extra training costs. This initialization is generated by using high-quality adversarial perturbations from the historical training process. We provide theoretical analysis for the proposed initialization a
    
[^94]: 面向全沉浸多用户虚拟现实技术的预测上下文感知和重定向步行

    Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking. (arXiv:2303.17907v1 [cs.NI])

    [http://arxiv.org/abs/2303.17907](http://arxiv.org/abs/2303.17907)

    本文提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向，实现面向全沉浸多用户虚拟现实技术的高效通信。

    

    虚拟现实技术正朝着增强沉浸感、支持多用户体验和在虚拟体验中支持无限制的移动，而通过重定向步行将用户限制在专门的VR设置内。为了满足未来VR系统的极端数据速率和延迟要求，支持无线网络基础设施将在毫米波（mmWave）频率上运行，并通过波束成形和波束导向实现高度定向的通信。我们提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向。具体而言，我们认为通过短期预测多用户VR设置中用户的横向移动，可以利用用户方向上的直线视距（LoS）“跟踪”来优化发射端的波束成形和波束导向。

    Virtual Reality (VR) technology is being advanced along the lines of enhancing its immersiveness, enabling multiuser Virtual Experiences (VEs), and supporting unconstrained mobility of the users in their VEs, while constraining them within specialized VR setups through Redirected Walking (RDW). For meeting the extreme data-rate and latency requirements of future VR systems, supporting wireless networking infrastructures will operate in millimeter Wave (mmWave) frequencies and leverage highly directional communication in both transmission and reception through beamforming and beamsteering. We propose to leverage predictive context-awareness for optimizing transmitter and receiver-side beamforming and beamsteering. In particular, we argue that short-term prediction of users' lateral movements in multiuser VR setups with RDW can be utilized for optimizing transmitter-side beamforming and beamsteering through Line-of-Sight (LoS) "tracking" in the users' directions. At the same time, short-
    
[^95]: 组织病理学染色转移的图像到图像翻译方法的比较评估

    A comparative evaluation of image-to-image translation methods for stain transfer in histopathology. (arXiv:2303.17009v1 [eess.IV])

    [http://arxiv.org/abs/2303.17009](http://arxiv.org/abs/2303.17009)

    本文比较了12种图像到图像翻译方法在组织病理学染色转移方面的应用，证明了一些方法优于现有技术，可用于大规模染色组织病理学图像的合成。

    

    图像到图像翻译（I2I）方法允许生成具有不同风格但与原始图像共享内容的人工图像。近年来，随着基于生成对抗网络（GANs）的方法的进展，I2I方法实现了生成与自然图像无法区分的人工图像。最近，针对组织病理学中不同类型的染色，I2I方法也被用于生成模拟的染色组织的人工图像。我们将这个过程称为染色转移。由于I2I变种的数量不断增加，这使得选择最合适的I2I方法进行染色转移变得困难。在我们的工作中，我们比较了12种染色转移方法，其中3种基于传统方法，9种基于GAN图像处理方法。分析依赖于定量评估图像翻译质量的补充措施，以及对深度学习组织成像和医学诊断的适用性的评估，以及经验丰富的病理学家的定性评估。我们的实验表明，有几种I2I方法优于染色转移领域的现有技术，并可用于大规模染色组织病理学图像的合成。

    Image-to-image translation (I2I) methods allow the generation of artificial images that share the content of the original image but have a different style. With the advances in Generative Adversarial Networks (GANs)-based methods, I2I methods enabled the generation of artificial images that are indistinguishable from natural images. Recently, I2I methods were also employed in histopathology for generating artificial images of in silico stained tissues from a different type of staining. We refer to this process as stain transfer. The number of I2I variants is constantly increasing, which makes a well justified choice of the most suitable I2I methods for stain transfer challenging. In our work, we compare twelve stain transfer approaches, three of which are based on traditional and nine on GAN-based image processing methods. The analysis relies on complementary quantitative measures for the quality of image translation, the assessment of the suitability for deep learning-based tissue gra
    
[^96]: 面向任务的内存高效剪枝适配器

    Task-oriented Memory-efficient Pruning-Adapter. (arXiv:2303.14704v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.14704](http://arxiv.org/abs/2303.14704)

    本文提出了一种面向任务的剪枝适配器方法，既实现了训练和内存的高效率，又加快了训练时间，并且在 GLUE 任务中没有显著降低准确性。

    

    大型语言模型的出色性能和不断增长的规模导致了对参数高效学习的 increased attention。主要的两种方法是适配器和剪枝。适配器是在模型上冻结并给它一个新的权重矩阵，在训练时间和内存方面可以显著降低成本，但这样会增加评估和测试的时间和内存消耗。剪枝是截断一些权重并重新分配剩余的权重，这样可以牺牲训练的复杂度，以极高的内存和训练时间为代价，使评估和测试的成本相对较低。因此，训练和推理的效率无法同时得到。在这项工作中，我们提出了一种面向任务的剪枝适配器方法，实现了训练和内存的高内存效率，加快了训练时间，并确保在 GLUE 任务中准确性没有显著下降，实现了训练和推理的效率。

    The Outstanding performance and growing size of Large Language Models has led to increased attention in parameter efficient learning. The two predominant approaches are Adapters and Pruning. Adapters are to freeze the model and give it a new weight matrix on the side, which can significantly reduce the time and memory of training, but the cost is that the evaluation and testing will increase the time and memory consumption. Pruning is to cut off some weight and re-distribute the remaining weight, which sacrifices the complexity of training at the cost of extremely high memory and training time, making the cost of evaluation and testing relatively low. So efficiency of training and inference can't be obtained in the same time. In this work, we propose a task-oriented Pruning-Adapter method that achieve a high memory efficiency of training and memory, and speeds up training time and ensures no significant decrease in accuracy in GLUE tasks, achieving training and inference efficiency at 
    
[^97]: RNN 的回归：用可逆句嵌入的残差循环神经网络

    Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings. (arXiv:2303.13570v1 [cs.CL])

    [http://arxiv.org/abs/2303.13570](http://arxiv.org/abs/2303.13570)

    本研究提出了一种使用残差循环神经网络的新型模型，实现了可逆的句子嵌入。与其他神经机器翻译模型不同，该方法使用基于回归的输出层重建输入序列的单词向量，其具有高准确度和快速训练速度。这种方法适合各种自然语言处理应用，特别是对需要高质量句嵌入的神经网络系统的使用具有潜在优势。

    

    本研究提出了一种新型模型，使用残差循环神经网络在无监督编码任务上进行训练，以生成可逆的句子嵌入。相比于神经机器翻译模型中常见的概率输出，我们的方法采用基于回归的输出层来重建输入序列的单词向量。该模型在使用 ADAM 优化器进行快速训练的同时，取得了高准确度的结果。我们引入了残差连接和“match drop”技术，即只计算错误单词的梯度。我们的方法在各种自然语言处理应用中表现出潜在优势，特别是在需要高质量句嵌入的神经网络系统中。

    This study presents a novel model for invertible sentence embeddings using a residual recurrent network trained on an unsupervised encoding task. Rather than the probabilistic outputs common to neural machine translation models, our approach employs a regression-based output layer to reconstruct the input sequence's word vectors. The model achieves high accuracy and fast training with the ADAM optimizer, a significant finding given that RNNs typically require memory units, such as LSTMs, or second-order optimization methods. We incorporate residual connections and introduce a "match drop" technique, where gradients are calculated only for incorrect words. Our approach demonstrates potential for various natural language processing applications, particularly in neural network-based systems that require high-quality sentence embeddings.
    
[^98]: 从时间序列数据中进行因果关系发现:综述和新视角

    Causal Discovery from Temporal Data: An Overview and New Perspectives. (arXiv:2303.10112v1 [cs.LG])

    [http://arxiv.org/abs/2303.10112](http://arxiv.org/abs/2303.10112)

    本文对于从时间数据中进行因果关系发现进行了综述，提出了两个相关的类别，即多元时间序列因果发现和事件序列因果发现，并提供了新的方法来综合考虑这两个类别。

    

    时间数据代表着复杂系统的时间顺序观测，可以被许多领域广泛生成，例如工业、医疗和金融。分析这种类型的数据对于各种应用非常有价值。因此，在过去几十年中，提出了不同的时间数据分析任务，例如分类、聚类和预测。其中，从时间数据中学习因果关系的因果发现任务被认为是一个有趣但至关重要的任务，并引起了广泛的研究关注。现有的因果发现工作可以根据时间数据是否被校准来分为两个高度相关的类别，即多元时间序列因果发现和事件序列因果发现。然而，大多数以前的调查仅专注于时间序列因果发现，忽略了第二类。在本文中，我们详细说明了这两个类别之间的相关性，并提供了新的方法来综合考虑这两个类别。

    Temporal data, representing chronological observations of complex systems, has always been a typical data structure that can be widely generated by many domains, such as industry, medicine and finance. Analyzing this type of data is extremely valuable for various applications. Thus, different temporal data analysis tasks, eg, classification, clustering and prediction, have been proposed in the past decades. Among them, causal discovery, learning the causal relations from temporal data, is considered an interesting yet critical task and has attracted much research attention. Existing casual discovery works can be divided into two highly correlated categories according to whether the temporal data is calibrated, ie, multivariate time series casual discovery, and event sequence casual discovery. However, most previous surveys are only focused on the time series casual discovery and ignore the second category. In this paper, we specify the correlation between the two categories and provide
    
[^99]: ODE-Net的变分形式：均场最优控制问题及存在性结果

    Variational formulations of ODE-Net as a mean-field optimal control problem and existence results. (arXiv:2303.05924v2 [math.AP] UPDATED)

    [http://arxiv.org/abs/2303.05924](http://arxiv.org/abs/2303.05924)

    本文探讨了ODE-Net在最小化损失函数的同时约束参数ODE的数学问题，并提出了一种测度论均场最优控制问题的形式化表述，并针对线性神经网络证明了最小化器的存在性结果。

    

    本文对ODE-Net进行了数学分析，它是深度神经网络（DNN）的连续模型。近年来，机器学习研究人员提出了用ODE代替DNN深度结构作为连续极限的想法。这些研究将ODE-Net的"学习"视为最小化由参数ODE约束的"损失"。虽然需要假定该最小化问题的存在，但只有少数研究详细地分析了其存在性。本文将ODE-Net的形式化表述为一种测度论均场最优控制问题，并基于此讨论了最小化器的存在性结果。当描述ODE-Net向量场的神经网络针对可学习参数是线性的时，证明了最小化器的存在性。证明使用测度论形式化和变分法的直接方法相结合。其次，本文提出了一个理想化的最小化问题，以消除针对基于边界条件的"恒等"ODE的最优控制问题的一些技术困难。

    This paper presents a mathematical analysis of ODE-Net, a continuum model of deep neural networks (DNNs). In recent years, Machine Learning researchers have introduced ideas of replacing the deep structure of DNNs with ODEs as a continuum limit. These studies regard the "learning" of ODE-Net as the minimization of a "loss" constrained by a parametric ODE. Although the existence of a minimizer for this minimization problem needs to be assumed, only a few studies have investigated its existence analytically in detail. In the present paper, the existence of a minimizer is discussed based on a formulation of ODE-Net as a measure-theoretic mean-field optimal control problem. The existence result is proved when a neural network, which describes a vector field of ODE-Net, is linear with respect to learnable parameters. The proof employs the measure-theoretic formulation combined with the direct method of Calculus of Variations. Secondly, an idealized minimization problem is proposed to remove
    
[^100]: 基于重构状态空间模型的时间序列异常检测

    Time series anomaly detection with reconstruction-based state-space models. (arXiv:2303.03324v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03324](http://arxiv.org/abs/2303.03324)

    本文提出一种基于重构状态空间模型的时间序列异常检测方法，该方法利用LSTM编码器—解码器共同学习观测和动态模型，并从正常样本中估计模型不确定性。该模型的潜在空间受到正则化约束，可以用马氏距离评估异常级别。

    

    数字化技术的不断发展导致各种领域中出现了多变量时间序列数据，使得实时监测运营成为可能。在这些情况下，识别异常数据模式和检测潜在故障变得越来越重要但也变得更加具有挑战性。在本研究中，我们提出了一种新颖的时间序列数据无监督异常检测方法。所提出的框架共同学习观测模型和动态模型，并从正常样本中估计模型的不确定性。具体的，采用基于长短时记忆网络（LSTM）的编码器—解码器表示观测空间和潜在空间之间的映射关系, 融合了向后和向前的时间信息以同时建模状态的双向转换。潜在空间的正则化约束了正常样本的状态，并使用马氏距离评估异常级别。

    Recent advances in digitization have led to the availability of multivariate time series data in various domains, enabling real-time monitoring of operations. Identifying abnormal data patterns and detecting potential failures in these scenarios are important yet rather challenging. In this work, we propose a novel unsupervised anomaly detection method for time series data. The proposed framework jointly learns the observation model and the dynamic model, and model uncertainty is estimated from normal samples. Specifically, a long short-term memory (LSTM)-based encoder-decoder is adopted to represent the mapping between the observation space and the latent space. Bidirectional transitions of states are simultaneously modeled by leveraging backward and forward temporal information. Regularization of the latent space places constraints on the states of normal samples, and Mahalanobis distance is used to evaluate the abnormality level. Empirical studies on synthetic and real-world dataset
    
[^101]: 非对数凹采样和对数分区估计的收敛速率

    Convergence Rates for Non-Log-Concave Sampling and Log-Partition Estimation. (arXiv:2303.03237v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.03237](http://arxiv.org/abs/2303.03237)

    非对数凹势V的高维采样速率可以在一些条件下实现与凸函数相同的收敛速率。

    

    从吉布斯分布$p(x)\propto\exp(-V(x)/\epsilon)$中采样并计算其对数分区函数是统计学、机器学习和统计物理中的基本任务。然而，虽然有效的算法已知于凸势函数$V$，但非凸情况下的情况要困难得多，算法必然在最坏情况下受到维度灾难的困扰。最近，已经证明在适当的条件下，高维采样非对数凹势V的速率也可以达到同样快的速度。本文对这些结果进行了回顾，并强调了领域中的一些开放问题。

    Sampling from Gibbs distributions $p(x) \propto \exp(-V(x)/\varepsilon)$ and computing their log-partition function are fundamental tasks in statistics, machine learning, and statistical physics. However, while efficient algorithms are known for convex potentials $V$, the situation is much more difficult in the non-convex case, where algorithms necessarily suffer from the curse of dimensionality in the worst case. For optimization, which can be seen as a low-temperature limit of sampling, it is known that smooth functions $V$ allow faster convergence rates. Specifically, for $m$-times differentiable functions in $d$ dimensions, the optimal rate for algorithms with $n$ function evaluations is known to be $O(n^{-m/d})$, where the constant can potentially depend on $m, d$ and the function to be optimized. Hence, the curse of dimensionality can be alleviated for smooth functions at least in terms of the convergence rate. Recently, it has been shown that similarly fast rates can also be ach
    
[^102]: 异构图神经网络中的半分散推理技术在交通需求预测中的应用：一种边缘计算方法

    Semi-decentralized Inference in Heterogeneous Graph Neural Networks for Traffic Demand Forecasting: An Edge-Computing Approach. (arXiv:2303.00524v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00524](http://arxiv.org/abs/2303.00524)

    本文提出一种半分散推理方法，利用多个云节点降低通信需求，同时保持图神经网络分散化的优势，从而提高交通需求预测效率。

    

    预测出租车服务的需求和供应对于提高客户体验和提供商利润至关重要。最近，图神经网络（GNN）在此类应用中展现出了良好的前景。该方法将城市区域建模为一个交通图中的节点以及它们之间的关系作为边。GNN利用本地节点特征和图形结构进行预测。然而，通过两种主要途径，可以实现更高效的预测：扩大交通网络的规模，同时利用图中不同类型的节点和边。然而，这两种方法都面临GNN可扩展性的挑战。即时的解决方法是分散GNN操作。然而，这会产生过多的节点之间传输通信。在本文中，我们首先表征了分散的GNN方法对于过多的通信需求。然后，我们提出了一种半分散方法，利用多个负责调节的云计算节点来减少通信需求，同时保持分散化的优势。我们在大规模出租车服务需求预测数据集上评估了我们的方法。实验结果表明，我们的方法能够在保持高精度的同时，有效减少通信开销。

    Prediction of taxi service demand and supply is essential for improving customer's experience and provider's profit. Recently, graph neural networks (GNNs) have been shown promising for this application. This approach models city regions as nodes in a transportation graph and their relations as edges. GNNs utilize local node features and the graph structure in the prediction. However, more efficient forecasting can still be achieved by following two main routes; enlarging the scale of the transportation graph, and simultaneously exploiting different types of nodes and edges in the graphs. However, both approaches are challenged by the scalability of GNNs. An immediate remedy to the scalability challenge is to decentralize the GNN operation. However, this creates excessive node-to-node communication. In this paper, we first characterize the excessive communication needs for the decentralized GNN approach. Then, we propose a semi-decentralized approach utilizing multiple cloudlets, moder
    
[^103]: AR3n: 一种基于强化学习的机器人康复辅助控制器

    AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for Robotic Rehabilitation. (arXiv:2303.00085v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.00085](http://arxiv.org/abs/2303.00085)

    本文提出了一种基于强化学习的机器人康复辅助控制器AR3n，通过使用虚拟患者模型实现控制器的泛化，实时调节机器人辅助力度并最小化机器人辅助的量，该控制器在实验验证中表现出良好的效果。

    

    本文提出了AR3n（发音为Aaron），一种采用强化学习的辅助控制器，可在机器人辅助的书写康复任务中提供适应性辅助。与以往的辅助控制器不同，我们的方法不依赖于患者特定的控制器参数或物理模型。我们建议使用虚拟患者模型来使AR3n推广到多个受试者。该系统实时调节机器人辅助力度，同时最小化机器人辅助的量，基于被试的跟踪误差。通过一组仿真实验和人体受试实验对控制器进行实验验证。最后，进行了与传统基于规则的控制器的比较研究，以分析两种控制器的辅助机制的差异。

    In this paper, we present AR3n (pronounced as Aaron), an assist-as-needed (AAN) controller that utilizes reinforcement learning to supply adaptive assistance during a robot assisted handwriting rehabilitation task. Unlike previous AAN controllers, our method does not rely on patient specific controller parameters or physical models. We propose the use of a virtual patient model to generalize AR3n across multiple subjects. The system modulates robotic assistance in realtime based on a subject's tracking error, while minimizing the amount of robotic assistance. The controller is experimentally validated through a set of simulations and human subject experiments. Finally, a comparative study with a traditional rule-based controller is conducted to analyze differences in assistance mechanisms of the two controllers.
    
[^104]: PAD: 面向对抗逃避攻击的合理恶意软件检测

    PAD: Towards Principled Adversarial Malware Detection Against Evasion Attacks. (arXiv:2302.11328v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.11328](http://arxiv.org/abs/2302.11328)

    该论文提出了一个新的对抗性训练框架，称为合理对抗性恶意软件检测（PAD），它通过可学习的凸度量保护恶意软件检测器免受攻击者的影响，而不是简单的启发式方法。实验结果表明，该方法优于现有技术，提高了恶意软件检测的防御效果。

    

    机器学习技术可以促进恶意软件（简称为恶意软件）的自动检测，但受到逃避攻击的影响。许多研究采用启发式方法来应对这些攻击，缺乏理论保证和有效的防御。在本文中，我们提出了一个新的对抗性训练框架，称为合理对抗性恶意软件检测（PAD），它针对强大的优化方法提供了收敛保证。PAD建立在可学习的凸度量上，量化分布式离散扰动，以保护恶意软件检测器免受攻击者的影响，对于平滑检测器，可以进行理论上的对抗性训练。为了提高防御效果，我们提出了一种新的混合攻击方法来实现PAD，以增强基于深度神经网络的测量和恶意软件检测器。在两个Android恶意软件数据集上的实验结果表明：（i）所提出的方法明显优于现有技术。

    Machine Learning (ML) techniques can facilitate the automation of malicious software (malware for short) detection, but suffer from evasion attacks. Many studies counter such attacks in heuristic manners, lacking theoretical guarantees and defense effectiveness. In this paper, we propose a new adversarial training framework, termed Principled Adversarial Malware Detection (PAD), which offers convergence guarantees for robust optimization methods. PAD lays on a learnable convex measurement that quantifies distribution-wise discrete perturbations to protect malware detectors from adversaries, whereby for smooth detectors, adversarial training can be performed with theoretical treatments. To promote defense effectiveness, we propose a new mixture of attacks to instantiate PAD to enhance deep neural network-based measurements and malware detectors. Experimental results on two Android malware datasets demonstrate: (i) the proposed method significantly outperforms the state-of-the-art defens
    
[^105]: 通过多模态数据推断贫困地图来解释财富分布

    Interpreting wealth distribution via poverty map inference using multimodal data. (arXiv:2302.10793v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10793](http://arxiv.org/abs/2302.10793)

    本论文提出了一种利用多模态数据推断贫困地图的模型管道，通过推断多个地理集群的财富平均值和标准差，配合新颖的“财富地平线”概念明确可视化财富分布在不同空间尺度下的情况，有助于政策制定者更好地了解影响财富分布的潜在社会经济多样性，优先考虑有针对性的干预措施。

    

    贫困地图是政府和非政府组织追踪社会经济变化并在需要的地区充分分配基础设施和服务的重要工具。传感器和在线众包数据与机器学习方法相结合最近在贫困地图推断方面取得了突破。然而，这些方法无法捕捉到地方财富波动，并且没有优化以产生可靠结果，以确保所有子人口的准确预测。在这里，我们提出了一个机器学习模型的管道，来推断多个地理集群人口地方的财富平均值和标准差，并举例说明了它们在塞拉利昂和乌干达的表现。这些模型利用了七个独立的和自由可用的特征源，基于卫星图像和通过在线众包和社交媒体收集的元数据。我们的模型表明，综合元数据特征是农村地区财富的最佳预测因素，优于基于图像的模态。此外，我们引入了一个新颖的“财富地平线”概念，明确地可视化了不同空间尺度下的财富分布。我们的方法可以帮助政策制定者更好地了解影响财富分布的潜在社会经济多样性，并优先考虑有针对性的干预措施。

    Poverty maps are essential tools for governments and NGOs to track socioeconomic changes and adequately allocate infrastructure and services in places in need. Sensor and online crowd-sourced data combined with machine learning methods have provided a recent breakthrough in poverty map inference. However, these methods do not capture local wealth fluctuations, and are not optimized to produce accountable results that guarantee accurate predictions to all sub-populations. Here, we propose a pipeline of machine learning models to infer the mean and standard deviation of wealth across multiple geographically clustered populated places, and illustrate their performance in Sierra Leone and Uganda. These models leverage seven independent and freely available feature sources based on satellite images, and metadata collected via online crowd-sourcing and social media. Our models show that combined metadata features are the best predictors of wealth in rural areas, outperforming image-based mod
    
[^106]: 基于稀疏性的深度神经网络剪枝技术

    Pruning Deep Neural Networks from a Sparsity Perspective. (arXiv:2302.05601v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05601](http://arxiv.org/abs/2302.05601)

    本文提出了一种稀疏感知自适应剪枝算法，通过利用PQ指数来衡量深度神经网络的潜在压缩性，可以有效地确定模型的剪枝程度，确保不会过度或欠剪枝。

    

    近年来，深度网络剪枝技术受到了重视，旨在将人工智能快速部署到计算和内存受限的小型设备上。这种剪枝通常通过丢弃深度网络中的冗余权重、神经元或层来实现，同时努力保持可比的测试性能。已经提出了许多深层剪枝算法，取得了令人瞩目的实证成果。然而，现有方法缺乏可量化的措施来估算每个剪枝迭代中子网络的可压缩性，因此可能会对模型进行过剪枝或欠剪枝。在本文中，我们提出了PQ指数（PQI）来衡量深度神经网络的潜在压缩性，并利用此指数开发出了一种稀疏感知自适应剪枝（SAP）算法。我们的广泛实验支持假设，对于通用剪枝程序，当大型模型被有效地正则化时，PQI首先减小，然后在其可压缩性达到最小值时增加。

    In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a
    
[^107]: 基于经济深度学习模型的IoT僵尸网络检测

    IoT Botnet Detection Using an Economic Deep Learning Model. (arXiv:2302.02013v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.02013](http://arxiv.org/abs/2302.02013)

    本文提出了一种经济深度学习模型来检测物联网僵尸网络攻击以及不同类型的攻击。该模型能够在较小的预算下加速训练和检测过程，并且具有比最先进的检测模型更高的准确性。

    

    技术创新的快速进步增加了过去十年的使用和分发。全球物联网系统的快速增长增加了由恶意第三方创建的网络安全挑战。因此，考虑安全问题和物联网系统限制的可靠入侵检测和网络取证系统对于保护这些系统至关重要。物联网僵尸网络攻击是企业和个人面临的重大威胁之一。因此，本文提出了一种基于经济的深度学习模型来检测物联网僵尸网络攻击以及不同类型的攻击。所提出的模型在较小的实现预算下，加速了训练和检测过程，并获得了比最先进的检测模型更高的准确性。

    The rapid progress in technology innovation usage and distribution has increased in the last decade. The rapid growth of the Internet of Things (IoT) systems worldwide has increased network security challenges created by malicious third parties. Thus, reliable intrusion detection and network forensics systems that consider security concerns and IoT systems limitations are essential to protect such systems. IoT botnet attacks are one of the significant threats to enterprises and individuals. Thus, this paper proposed an economic deep learning-based model for detecting IoT botnet attacks along with different types of attacks. The proposed model achieved higher accuracy than the state-of-the-art detection models using a smaller implementation budget and accelerating the training and detecting processes.
    
[^108]: 完美主义是测试Oracle的敌人

    Perfect is the enemy of test oracle. (arXiv:2302.01488v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2302.01488](http://arxiv.org/abs/2302.01488)

    本文提出了一种学习方法 SEER，该方法可以在缺乏测试断言或其他类型的测试Oracle的情况下确定单元测试是否通过或失败，并且可以构建准确的Oracle而不需要知道正确或错误行为的确切期望。

    

    自动化测试Oracle是软件测试中最具挑战性的方面之一，但与自动化测试输入生成相比，仍然受到相对较少的关注。测试Oracle依赖于可以区分正确行为和错误行为的基础真相来确定测试是否失败（检测到错误）或通过。让Oracle问题具有挑战性和不可决定性的是假设这个基础真相需要知道正确行为或错误行为的确切期望。然而，我们认为即使不知道确切的正确或错误行为如何不同，仍然可以构建准确的Oracle。本文提出了SEER，一种基于学习的方法，用于在缺乏测试断言或其他类型的Oracle的情况下，确定在给定的测试方法下单元测试是否通过或失败。为了建立基础真相，SEER将单元测试和MUTs的实现联合嵌入到一个统一的向量空间中，使神经表示方式具有区分单元测试结果的能力。

    Automation of test oracles is one of the most challenging facets of software testing, but remains comparatively less addressed compared to automated test input generation. Test oracles rely on a ground-truth that can distinguish between the correct and buggy behavior to determine whether a test fails (detects a bug) or passes. What makes the oracle problem challenging and undecidable is the assumption that the ground-truth should know the exact expected, correct, or buggy behavior. However, we argue that one can still build an accurate oracle without knowing the exact correct or buggy behavior, but how these two might differ. This paper presents SEER, a learning-based approach that in the absence of test assertions or other types of oracle, can determine whether a unit test passes or fails on a given method under test (MUT). To build the ground-truth, SEER jointly embeds unit tests and the implementation of MUTs into a unified vector space, in such a way that the neural representation 
    
[^109]: 分析语言模型中个人识别信息泄露的情况

    Analyzing Leakage of Personally Identifiable Information in Language Models. (arXiv:2302.00539v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00539](http://arxiv.org/abs/2302.00539)

    本研究针对语言模型中泄漏个人身份信息的风险进行了严格的定义，并通过黑盒提取、推断和重建攻击进行了实证评估。

    

    语言模型已经被证明会通过句子级成员推断和重构攻击泄漏训练数据的信息。然而，我们对于语言模型泄露个人身份信息的风险了解不足。目前已经假设数据集整理技术（如数据清洗）足以防止个人身份信息泄露，但这一假设是错误的。实际上，数据清洗技术可以减少Pll泄露的风险，但并不能完全绝对地防止泄露。本文中，我们引入了三种类型的个人身份信息泄漏的严格基于博弈的定义，通过API访问语言模型进行黑盒提取、推断和重建攻击，并对其进行实证评估。

    Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the 
    
[^110]: 面向高斯过程状态空间模型的灵活性和可解释性的探索

    Towards Flexibility and Interpretability of Gaussian Process State-Space Model. (arXiv:2301.08843v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08843](http://arxiv.org/abs/2301.08843)

    本文提出了一种新的状态空间模型类TGPSSM，利用正规化流增加了标准GPSSM中的GP先验概率，从而增强模型的灵活性和表现力。同时提出了可扩展的变分推理算法，为潜在状态的变分分布提供灵活和最优的结构。

    

    过去十年中，高斯过程状态空间模型（GPSSM）备受关注。然而，GPSSM的模型表达能力远非令人满意。大多数GPSSM研究依赖于标准高斯过程（GP）和预先设置的核心，例如平方指数（SE）核心或Matern核心，这限制了模型的表达能力和在复杂场景中的应用。针对这个问题，本文提出了一种新的概率状态空间模型类，称为TGPSSM。通过利用一个参数化的正规化流，TGPSSM增加了标准GPSSM中的GP先验概率，使状态空间模型更具灵活性和表现力。此外，我们提出了一种可扩展的变分推理算法，用于在TGPSSM中进行学习和推理，为潜在状态的变分分布提供灵活和最优的结构。由于GP的稀疏表示，该算法是可解释和计算高效的。

    The Gaussian process state-space model (GPSSM) has attracted much attention over the past decade. However, the model representation power of the GPSSM is far from satisfactory. Most GPSSM studies rely on the standard Gaussian process (GP) with a preliminary kernel, such as the squared exponential (SE) kernel or Mat\'{e}rn kernel, which limits the model representation power and its application in complex scenarios. To address this issue, this paper proposes a novel class of probabilistic state-space models, called TGPSSMs. By leveraging a parametric normalizing flow, the TGPSSMs enrich the GP priors in the standard GPSSM, rendering the state-space model more flexible and expressive. Additionally, we present a scalable variational inference algorithm for learning and inference in TGPSSMs, which provides a flexible and optimal structure for the variational distribution of latent states. The algorithm is interpretable and computationally efficient owing to the sparse representation of GP a
    
[^111]: 通过代理建模实现高效的激活函数优化

    Efficient Activation Function Optimization through Surrogate Modeling. (arXiv:2301.05785v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05785](http://arxiv.org/abs/2301.05785)

    本文提出了一种基于代理建模的方法，通过扩展的基准测试空间在较少的函数评估次数中发现了优化的高效激活函数架构，并在多个标准基准测试中实现了最先进的性能。

    

    精心设计的激活函数可以提高神经网络在许多机器学习任务中的性能。然而，人类很难构建最优激活函数，而当前的激活函数搜索算法过于昂贵。本文通过三个步骤旨在改进现有技术：首先，通过使用2,913个系统生成的激活函数从头训练卷积、残差和视觉变换器架构来创建 Act-Bench-CNN、Act-Bench-ResNet 和 Act-Bench-ViT 基准数据集。第二，开发了基于代理的方法用于优化基准空间，发现与模型预测分布和激活函数输出分布相关联的 Fisher 信息矩阵的频谱对性能的预测性很高。第三，使用代理在较少的函数评估次数中发现了改进的激活函数架构，同时在几个标准基准测试中实现了最先进的性能。

    Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activ
    
[^112]: EmoGator：一种新的开源语音爆发数据集与基线机器学习分类方法

    EmoGator: A New Open Source Vocal Burst Dataset with Baseline Machine Learning Classification Methodologies. (arXiv:2301.00508v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2301.00508](http://arxiv.org/abs/2301.00508)

    EmoGator是一个包含32,130个样本的语音数据集，用于研究短暂的非语音发声对情感的表达，并提供了基线机器学习分类方法。

    

    Vocal Bursts是一种短暂的非语音发声，包括笑声、哭声、叹息、呻吟和咕哝等，传递情感，是语音情感识别中常被忽视但人类声音交流中重要的一部分。其中一个难点是缺乏大型数据集。我们很高兴地介绍EmoGator数据集，其中包括来自357个说话人的32,130个样本，16.9654个小时的音频；每个样本由说话人分类为30个不同的情感类别。将讨论构建分类器以识别情感类别的几种不同方法，并建议未来研究方向。数据集可从https://github.com/fredbuhl/EmoGator下载。

    Vocal Bursts -- short, non-speech vocalizations that convey emotions, such as laughter, cries, sighs, moans, and groans -- are an often-overlooked aspect of speech emotion recognition, but an important aspect of human vocal communication. One barrier to study of these interesting vocalizations is a lack of large datasets. I am pleased to introduce the EmoGator dataset, which consists of 32,130 samples from 357 speakers, 16.9654 hours of audio; each sample classified into one of 30 distinct emotion categories by the speaker. Several different approaches to construct classifiers to identify emotion categories will be discussed, and directions for future research will be suggested. Data set is available for download from https://github.com/fredbuhl/EmoGator.
    
[^113]: 通过GD训练的过度参数化浅层ReLU神经网络学习Lipschitz函数

    Learning Lipschitz Functions by GD-trained Shallow Overparameterized ReLU Neural Networks. (arXiv:2212.13848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13848](http://arxiv.org/abs/2212.13848)

    本论文通过GD训练过程中神经切向核（NTK）近似方法，探究了过度参数化的浅层ReLU神经网络学习Lipschitz函数的能力，提出了一系列能够产生最优速率的实用早停规则。

    

    本研究探究了过度参数化的浅层ReLU神经网络在通过梯度下降（GD）训练时学习具有加性噪声的Lipschitz、不可微分、有界函数的能力。为避免存在噪声时，神经网络训练到接近0的训练误差时不一致的问题，我们专注于停止较早的GD，从而展示了一致性和最优速率。具体来说，我们从GD训练的有限宽度神经网络的神经切向核（NTK）近似的视角探索了这个问题。我们发现，只要在ReLU激活函数引起的核的希尔伯特空间中，某些早停规则保证能够给出最优的超额风险速率，那么相同的规则就可以被用于实现在Lipschitz函数所考虑的类中通过神经网络学习的极小极值速率。我们讨论了几个无需数据和数据相关的实际吸引力停止准则，这些准则产生了最优速率。

    We explore the ability of overparameterized shallow ReLU neural networks to learn Lipschitz, nondifferentiable, bounded functions with additive noise when trained by Gradient Descent (GD). To avoid the problem that in the presence of noise, neural networks trained to nearly zero training error are inconsistent in this class, we focus on the early-stopped GD which allows us to show consistency and optimal rates. In particular, we explore this problem from the viewpoint of the Neural Tangent Kernel (NTK) approximation of a GD-trained finite-width neural network. We show that whenever some early stopping rule is guaranteed to give an optimal rate (of excess risk) on the Hilbert space of the kernel induced by the ReLU activation function, the same rule can be used to achieve minimax optimal rate for learning on the class of considered Lipschitz functions by neural networks. We discuss several data-free and data-dependent practically appealing stopping rules that yield optimal rates.
    
[^114]: 自动计算泰勒余项级数：更紧密的界限和新应用

    Automatically Bounding the Taylor Remainder Series: Tighter Bounds and New Applications. (arXiv:2212.11429v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11429](http://arxiv.org/abs/2212.11429)

    本文提出了一种自动计算泰勒余项级数的算法，可以提供更紧密的界限，并应用于区间计算、优化等领域。

    

    我们提出了一种自动计算泰勒余项级数的算法。在标量函数 $f:\mathbb{R}\to\mathbb{R}$ 的特殊情况下，我们的算法以参考点 $x_0$、信任域 $[a,b]$ 和整数 $k\geq1$ 为输入，并返回一个区间 $I$，使得对于所有 $x\in[a,b]$, $f(x)\sum_{i=0}^{k-1}\frac{1}{i!}f^{(i)}(x_0)(x-x_0)^i \in I(x-x_0)^k$。与自动微分类似，函数 $f$ 的输入必须为已知的原子函数。在算法的高层次上，我们的算法包含两个步骤。首先，我们针对多种常用的初等函数（如 $\exp$，$\log$）导出泰勒余项级数的尖锐多项式上限和下限。然后，我们使用区间算术的泰勒模式自动微分递归地组合元函数的边界。我们的算法可以高效利用机器学习硬件加速器，并提供了新的应用。

    We present a new algorithm for automatically bounding the Taylor remainder series. In the special case of a scalar function $f: \mathbb{R} \to \mathbb{R}$, our algorithm takes as input a reference point $x_0$, trust region $[a, b]$, and integer $k \ge 1$, and returns an interval $I$ such that $f(x) \sum_{i=0}^{k-1} \frac {1} {i!} f^{(i)}(x_0) (x - x_0)^i \in I (x - x_0)^k$ for all $x \in [a, b]$. As in automatic differentiation, the function $f$ is provided to the algorithm in symbolic form, and must be composed of known atomic functions.  At a high level, our algorithm has two steps. First, for a variety of commonly-used elementary functions (e.g., $\exp$, $\log$), we derive sharp polynomial upper and lower bounds on the Taylor remainder series. We then recursively combine the bounds for the elementary functions using an interval arithmetic variant of Taylor-mode automatic differentiation. Our algorithm can make efficient use of machine learning hardware accelerators, and we provide
    
[^115]: 通过合并语言模型的权重实现无数据知识融合

    Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09849](http://arxiv.org/abs/2212.09849)

    本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。

    

    微调预训练语言模型已成为构建下游NLP模型的流行范式。通常情况下，经过微调的模型已经可用，但其训练数据不可用，由于数据隐私或知识产权问题。这就造成了跨模型融合知识以产生更好的单一模型的障碍。在本文中，我们研究了建立在不同训练数据集上的单个模型之间合并的问题，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。我们提出了一种无数据知识融合方法，该方法在参数空间中合并模型，由权重引导，以最小化合并模型和单个模型之间的预测差异。在一系列评估设置中，我们展示了该方法显著优于如Fisher加权平均或模型集成等基线。此外，我们发现我们的方法是一个有前途的多语言微调替代方案，因为它可以在不需要任何额外注释数据的情况下实现可比的性能。

    Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
    
[^116]: 教育很重要：探究监督对Vision Transformers中的作用。

    Teaching Matters: Investigating the Role of Supervision in Vision Transformers. (arXiv:2212.03862v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.03862](http://arxiv.org/abs/2212.03862)

    本研究比较了不同的监督方法对于Vision Transformers的训练效果，并发现了一种新的注意力头类型。并证明了对于Vision Transformers而言，自监督学习是一种非常有效的学习范式。

    

    Vision Transformers (ViTs)在近年中广受欢迎，并在许多应用中得到了推广。然而，它们在不同的学习范式下的行为尚未得到很好的研究。我们比较了通过不同监督方法训练的ViTs，并展示了它们在注意力、表示和下游性能方面学习了各种不同的行为。我们还发现了ViT行为在不同训练模式下的一致性，包括新出现的Offset Local Attention Heads，这是一种我们之前没有意识到的自注意力头类型。我们的分析表明，ViTs非常灵活，通过不同的训练方法学习处理本地和全局信息。我们发现，对比自监督方法学习的特征与显式监督方法学习的特征具有竞争力，且具有更好的可解释性。总的来说，我们的研究揭示了监督方式对于ViT学习行为的影响，发现了一种新的注意力头类型，并且证明了对于Vision Transformers而言，自监督学习是一种非常有效的学习范式。

    Vision Transformers (ViTs) have gained significant popularity in recent years and have proliferated into many applications. However, their behavior under different learning paradigms is not well explored. We compare ViTs trained through different methods of supervision, and show that they learn a diverse range of behaviors in terms of their attention, representations, and downstream performance. We also discover ViT behaviors that are consistent across supervision, including the emergence of Offset Local Attention Heads. These are self-attention heads that attend to a token adjacent to the current token with a fixed directional offset, a phenomenon that to the best of our knowledge has not been highlighted in any prior work. Our analysis shows that ViTs are highly flexible and learn to process local and global information in different orders depending on their training method. We find that contrastive self-supervised methods learn features that are competitive with explicitly supervise
    
[^117]: 去噪扩散概率模型在能量概率预测中的应用

    Denoising diffusion probabilistic models for probabilistic energy forecasting. (arXiv:2212.02977v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02977](http://arxiv.org/abs/2212.02977)

    本文利用去噪扩散概率模型对能源（负荷、光伏或风力）的概率预测，结果表明该方法比其他深度学习生成模型具有竞争力。

    

    基于情景的概率预测对于决策者在处理间歇性可再生能源方面至关重要。本文提出了一种称为去噪扩散概率模型的深度学习生成方法，它是一类潜变量模型，在计算机视觉领域最近表现出了惊人的结果。然而，据我们所知，尚未有演示它们可以生成高质量的负荷、光伏或风力时间序列样本，而这是应对电力系统应用中的新挑战所必需的关键要素。因此，我们提出了这种模型在利用全球能源预测大赛2014的公开数据进行能源预测的首次实现。结果表明，这种方法与其他最先进的深度学习生成模型（包括生成对抗网络、变分自动编码器和正常化流）具有竞争力。

    Scenario-based probabilistic forecasts have become vital for decision-makers in handling intermittent renewable energies. This paper presents a recent promising deep learning generative approach called denoising diffusion probabilistic models. It is a class of latent variable models which have recently demonstrated impressive results in the computer vision community. However, to our knowledge, there has yet to be a demonstration that they can generate high-quality samples of load, PV, or wind power time series, crucial elements to face the new challenges in power systems applications. Thus, we propose the first implementation of this model for energy forecasting using the open data of the Global Energy Forecasting Competition 2014. The results demonstrate this approach is competitive with other state-of-the-art deep learning generative models, including generative adversarial networks, variational autoencoders, and normalizing flows.
    
[^118]: 通过多样本超网络提高帕累托前沿学习

    Improving Pareto Front Learning via Multi-Sample Hypernetworks. (arXiv:2212.01130v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01130](http://arxiv.org/abs/2212.01130)

    本文提出了一个新的PFL框架PHN-HVI，利用超网络生成一组多样的解，并通过最大化这些解定义的超体积指标来提高帕累托前沿的质量。

    

    帕累托前沿学习(PFL)是一种有效的方法，用于获得从给定权衡向量到帕累托前沿解的映射函数，从而解决多目标优化(MOO)问题。然而，现有的PFL方法忽略了优化过程中解之间的关系，从而影响了获得的帕累托前沿的质量。为了解决这个问题，本文提出了一个新的PFL框架，即PHN-HVI，它使用超网络从多样的权衡偏好集生成多个解，并通过最大化这些解定义的超体积指标来提高帕累托前沿的质量。多个MOO机器学习数据集上的实验结果表明，相对于现有的PFL方法，PHN-HVI在帕累托前沿近似质量方面具有更好的性能。

    Pareto Front Learning (PFL) was recently introduced as an effective approach to obtain a mapping function from a given trade-off vector to a solution on the Pareto front, which solves the multi-objective optimization (MOO) problem. Due to the inherent trade-off between conflicting objectives, PFL offers a flexible approach in many scenarios in which the decision makers can not specify the preference of one Pareto solution over another, and must switch between them depending on the situation. However, existing PFL methods ignore the relationship between the solutions during the optimization process, which hinders the quality of the obtained front. To overcome this issue, we propose a novel PFL framework namely PHN-HVI, which employs a hypernetwork to generate multiple solutions from a set of diverse trade-off preferences and enhance the quality of the Pareto front by maximizing the Hypervolume indicator defined by these solutions. The experimental results on several MOO machine learning
    
[^119]: 使用BigTransfer（BiT）进行黑素瘤基底细胞痣图像分类

    Classification of Melanocytic Nevus Images using BigTransfer (BiT). (arXiv:2211.11872v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.11872](http://arxiv.org/abs/2211.11872)

    该研究提出了一种通过利用BigTransfer（BiT）算法进行黑素瘤图像分类的自动化方法。

    

    皮肤癌是一种致命的疾病，每年夺去了人类的生命。有色皮肤图像在不同的皮肤病变（如黑素瘤和痣）之间表现出显著的相似性，使得识别和诊断更加具有挑战性。黑色素痣可能会成熟并导致致命的黑色素瘤。因此，当前的管理协议涉及到去除那些看起来令人生畏的痣。该研究提出了一种自动化分类算法，利用了先前针对不同问题陈述而进行训练的神经网络和基于ResNet的迁移学习方法BigTransfer（BiT）。

    Skin cancer is a fatal disease that takes a heavy toll over human lives annually. The colored skin images show a significant degree of resemblance between different skin lesions such as melanoma and nevus, making identification and diagnosis more challenging. Melanocytic nevi may mature to cause fatal melanoma. Therefore, the current management protocol involves the removal of those nevi that appear intimidating. However, this necessitates resilient classification paradigms for classifying benign and malignant melanocytic nevi. Early diagnosis necessitates a dependable automated system for melanocytic nevi classification to render diagnosis efficient, timely, and successful. An automated classification algorithm is proposed in the given research. A neural network previously-trained on a separate problem statement is leveraged in this technique for classifying melanocytic nevus images. The suggested method uses BigTransfer (BiT), a ResNet-based transfer learning approach for classifying
    
[^120]: 协同过滤的模糊-锐化过程模型

    Blurring-Sharpening Process Models for Collaborative Filtering. (arXiv:2211.09324v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2211.09324](http://arxiv.org/abs/2211.09324)

    本文提出了一种协同过滤的模糊-锐化过程模型（BSPM），并利用期望最大化算法学习模型参数，在显式和隐式反馈中优于现有技术方法。

    

    协同过滤是推荐系统中最基本的主题之一。从矩阵分解到图卷积方法，已经提出了各种各样的协同过滤方法。在图过滤方法和基于分数的生成模型（SGM）的最近成功启发下，我们提出了一种新的模糊-锐化过程模型（BSPM）的概念。SGM和BSPM共享相同的处理哲学，即在将原始信息首先扰乱然后恢复到原始形式的过程中可以发现新信息（例如，在SGM的情况下生成新图像）。然而，SGM和我们的BSPM处理不同类型的信息，并且它们的最优扰动和恢复过程存在根本上的差异。因此，我们的BSPM与SGM具有不同的形式。此外，我们的概念不仅理论上包括了许多现有的协同过滤模型，而且在显式和隐式反馈的情况下，在召回率和NDCG方面也优于它们。具体而言，我们提出了一组具有不同模糊和锐化滤波器设置的BSPM，并推导了期望最大化（EM）算法以学习模型参数。四个基准数据集的实验结果证明了我们的模型相对于现有技术方法的有效性。

    Collaborative filtering is one of the most fundamental topics for recommender systems. Various methods have been proposed for collaborative filtering, ranging from matrix factorization to graph convolutional methods. Being inspired by recent successes of graph filtering-based methods and score-based generative models (SGMs), we present a novel concept of blurring-sharpening process model (BSPM). SGMs and BSPMs share the same processing philosophy that new information can be discovered (e.g., new images are generated in the case of SGMs) while original information is first perturbed and then recovered to its original form. However, SGMs and our BSPMs deal with different types of information, and their optimal perturbation and recovery processes have fundamental discrepancies. Therefore, our BSPMs have different forms from SGMs. In addition, our concept not only theoretically subsumes many existing collaborative filtering models but also outperforms them in terms of Recall and NDCG in th
    
[^121]: 数据量不如数据质量：工艺分析中的陷阱和准则

    Data Quality Over Quantity: Pitfalls and Guidelines for Process Analytics. (arXiv:2211.06440v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2211.06440](http://arxiv.org/abs/2211.06440)

    本文介绍了如何在工业流程中获取和准备高质量的操作数据，并强调数据预处理对于人工智能应用的成功至关重要。

    

    高级过程控制、工艺分析和机器学习都涉及到获取和准备数据。虽然文献经常强调通过逐步改进的复杂建模技术提高模型性能，但当工业案例研究被发布时，它们通常缺乏有关数据获取和准备的重要细节。本文介绍了获取和准备操作数据的最佳实践，以追求工业流程中数据驱动的建模和控制机会。

    A significant portion of the effort involved in advanced process control, process analytics, and machine learning involves acquiring and preparing data. Literature often emphasizes increasingly complex modelling techniques with incremental performance improvements. However, when industrial case studies are published they often lack important details on data acquisition and preparation. Although data pre-processing is unfairly maligned as trivial and technically uninteresting, in practice it has an out-sized influence on the success of real-world artificial intelligence applications. This work describes best practices for acquiring and preparing operating data to pursue data-driven modelling and control opportunities in industrial processes. We present practical considerations for pre-processing industrial time series data to inform the efficient development of reliable soft sensors that provide valuable process insights.
    
[^122]: 延迟嵌入的回声状态网络：一种部分观测系统的预测器

    Delay Embedded Echo-State Network: A Predictor for Partially Observed Systems. (arXiv:2211.05992v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2211.05992](http://arxiv.org/abs/2211.05992)

    本文提出了一种基于回声状态网络和时间延迟嵌入的方法来解决部分观测系统预测的问题。

    

    本文研究了使用循环神经网络进行基于数据的部分观测系统预测的问题。虽然神经网络基于完整状态训练数据的动态预测表现良好，但在训练阶段使用部分观测的数据进行预测会带来显着挑战。本文提出了一种使用回声状态网络（ESN）和部分观测状态的时间延迟嵌入来进行部分观测系统预测的方法。该方法基于Taken的嵌入定理和非线性系统的强可观测性理论上得到了证明。该方法的有效性在三个系统上得到了证明：两个混沌动力学系统的合成数据集和一组实时交通数据。

    This paper considers the problem of data-driven prediction of partially observed systems using a recurrent neural network. While neural network based dynamic predictors perform well with full-state training data, prediction with partial observation during training phase poses a significant challenge. Here a predictor for partial observations is developed using an echo-state network (ESN) and time delay embedding of the partially observed state. The proposed method is theoretically justified with Taken's embedding theorem and strong observability of a nonlinear system. The efficacy of the proposed method is demonstrated on three systems: two synthetic datasets from chaotic dynamical systems and a set of real-time traffic data.
    
[^123]: 基于STL的随机过程的依从性量化预测监控

    Conformal Quantitative Predictive Monitoring of STL Requirements for Stochastic Processes. (arXiv:2211.02375v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2211.02375](http://arxiv.org/abs/2211.02375)

    该论文提出了一种新的预测监控方法，称为定量预测监控（QPM），能够支持使用Signal Temporal Logic（STL）描述的随机过程和丰富规范，它可以量化地预测满足程度，并提供高效的计算和概率保证。

    

    本研究考虑了预测监控（PM）的问题，即从当前系统状态预测所需属性的满足情况。由于其对于运行时安全保障和在线控制的重要性，PM方法需要高效，以便能够在预测到违规时及时进行干预，同时提供正确性保证。我们引入了“定量预测监控（QPM）”，这是第一个支持STL中的随机过程和丰富规范的PM方法。与大多数现有的PM技术通过预测是否满足某些属性$\phi$不同，QPM通过预测$\phi$的定量（也称为鲁棒性）STL语义来量化满足程度。QPM导出了高效计算且具有概率保证的预测区间，在概率上覆盖了与系统随机演化相关的STL鲁棒性值。

    We consider the problem of predictive monitoring (PM), i.e., predicting at runtime the satisfaction of a desired property from the current system's state. Due to its relevance for runtime safety assurance and online control, PM methods need to be efficient to enable timely interventions against predicted violations, while providing correctness guarantees. We introduce \textit{quantitative predictive monitoring (QPM)}, the first PM method to support stochastic processes and rich specifications given in Signal Temporal Logic (STL). Unlike most of the existing PM techniques that predict whether or not some property $\phi$ is satisfied, QPM provides a quantitative measure of satisfaction by predicting the quantitative (aka robust) STL semantics of $\phi$. QPM derives prediction intervals that are highly efficient to compute and with probabilistic guarantees, in that the intervals cover with arbitrary probability the STL robustness values relative to the stochastic evolution of the system. 
    
[^124]: UniASM：无需微调的二进制代码相似性检测

    UniASM: Binary Code Similarity Detection without Fine-tuning. (arXiv:2211.01144v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.01144](http://arxiv.org/abs/2211.01144)

    提出了一种新的二进制代码嵌入模型UniASM，并设计了两个新的训练任务，使得生成向量的空间分布更加均匀，直接可以在无需任何微调的情况下用于二进制代码相似性检测。此外，提出了一种新的二进制函数tokenization方法，缓解了词汇外的问题，并通过消融实验得到了一些新的有价值的发现，实验证明UniASM优于其他模型。

    

    二进制代码相似性检测被广泛用于各种二进制分析任务，如漏洞搜索、恶意软件检测、克隆检测和补丁分析。最近的研究表明，基于学习的二进制代码嵌入模型比传统的基于特征的方法更好。本文提出了一种新的基于transformer的二进制代码嵌入模型UniASM，用于学习二进制函数的表示。我们设计了两个新的训练任务，使得生成向量的空间分布更加均匀，直接可以在无需任何微调的情况下用于二进制代码相似性检测。此外，我们提出了一种新的二进制函数tokenization方法，增加了tokens的语义信息并缓解了词汇外的问题。通过消融实验进行了深入分析，得到了一些新的有价值的发现，实验证明UniASM优于其他模型。

    Binary code similarity detection (BCSD) is widely used in various binary analysis tasks such as vulnerability search, malware detection, clone detection, and patch analysis. Recent studies have shown that the learning-based binary code embedding models perform better than the traditional feature-based approaches. In this paper, we propose a novel transformer-based binary code embedding model named UniASM to learn representations of the binary functions. We design two new training tasks to make the spatial distribution of the generated vectors more uniform, which can be used directly in BCSD without any fine-tuning. In addition, we present a new tokenization approach for binary functions, which increases the token's semantic information and mitigates the out-of-vocabulary (OOV) problem. We conduct an in-depth analysis of the factors affecting model performance through ablation experiments and obtain some new and valuable findings. The experimental results show that UniASM outperforms th
    
[^125]: 带资源约束的高度移动连接车辆下的车联网边缘联邦学习

    Resource Constrained Vehicular Edge Federated Learning with Highly Mobile Connected Vehicles. (arXiv:2210.15496v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.15496](http://arxiv.org/abs/2210.15496)

    本文提出了一种在高度移动的连接车辆下，边缘服务器利用局部数据集和处理单元进行训练的边缘联邦学习解决方案，可以通过权重组合和子集选择来聚合模型参数并最大化成功接收本地训练模型的概率。

    

    本文提出了一种车联网边缘联邦学习（VEFL）解决方案，其中边缘服务器利用高度移动的连接车辆（CV）的本地数据集和中央处理单元（CPU）来训练全局模型。收敛分析表明，VEFL训练损失取决于成功接收CV通过间歇性车辆到基础设施（V2I）无线链路传输的训练模型。由于高度移动性，在全设备参与情况（FDPC）下，边缘服务器根据CV数据集大小和逗留时间的加权组合聚合客户端模型参数，而在部分设备参与情况（PDPC）下选择CV的子集。然后，我们设计了在延迟、能量和成本约束条件下的联合VEFL和无线接入技术（RAT）参数优化问题，以最大化成功接收本地训练模型的概率。考虑到优化问题是NP-hard问题，我们将其分解为待解决的子问题。

    This paper proposes a vehicular edge federated learning (VEFL) solution, where an edge server leverages highly mobile connected vehicles' (CVs') onboard central processing units (CPUs) and local datasets to train a global model. Convergence analysis reveals that the VEFL training loss depends on the successful receptions of the CVs' trained models over the intermittent vehicle-to-infrastructure (V2I) wireless links. Owing to high mobility, in the full device participation case (FDPC), the edge server aggregates client model parameters based on a weighted combination according to the CVs' dataset sizes and sojourn periods, while it selects a subset of CVs in the partial device participation case (PDPC). We then devise joint VEFL and radio access technology (RAT) parameters optimization problems under delay, energy and cost constraints to maximize the probability of successful reception of the locally trained models. Considering that the optimization problem is NP-hard, we decompose it i
    
[^126]: 带有符合性预测集的贝叶斯优化

    Bayesian Optimization with Conformal Prediction Sets. (arXiv:2210.12496v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12496](http://arxiv.org/abs/2210.12496)

    符合性贝叶斯优化在决策过程中应用符合性预测集，可以纠正由于模型规范不当和协变量转移带来的主观上不可能的结果，并在黑盒优化任务和表格排名任务中表现优异。

    

    贝叶斯优化是面对不确定性时做出决策的普遍方法，应用包括多臂老虎机、主动学习和黑盒优化。贝叶斯优化通过基于贝叶斯模型的后验分布选择具有最大预期效用的决策(即目标函数查询)，该后验分布量化了查询结果的可减少的先验信息不确定性。在实践中，因模型规范不当和协变量转移的原因，主观上不可能的结果可能经常发生。符合性预测是一种不确定性量化方法，即使对于规范不良的模型也具有覆盖保证，并且具有纠正协变量转移的简单机制。我们提出了符合性贝叶斯优化，将查询引导到模型预测具有保证有效性的搜索空间区域，并研究了它在一组黑盒优化任务和表格排名任务中的行为。在许多情况下，我们发现符合性贝叶斯优化优于标准贝叶斯优化方法。

    Bayesian optimization is a coherent, ubiquitous approach to decision-making under uncertainty, with applications including multi-arm bandits, active learning, and black-box optimization. Bayesian optimization selects decisions (i.e. objective function queries) with maximal expected utility with respect to the posterior distribution of a Bayesian model, which quantifies reducible, epistemic uncertainty about query outcomes. In practice, subjectively implausible outcomes can occur regularly for two reasons: 1) model misspecification and 2) covariate shift. Conformal prediction is an uncertainty quantification method with coverage guarantees even for misspecified models and a simple mechanism to correct for covariate shift. We propose conformal Bayesian optimization, which directs queries towards regions of search space where the model predictions have guaranteed validity, and investigate its behavior on a suite of black-box optimization tasks and tabular ranking tasks. In many cases we f
    
[^127]: 超图的Ollivier-Ricci曲率：一个统一的框架

    Ollivier-Ricci Curvature for Hypergraphs: A Unified Framework. (arXiv:2210.12048v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12048](http://arxiv.org/abs/2210.12048)

    本论文提出了ORCHID框架，将Ollivier-Ricci曲率推广到超图领域，并证明了其具有良好的理论特性。实验结果表明ORCHID曲率对于超图任务有很好的应用性。

    

    曲率是一种强大而富有表现力的不变量，连接了几何和拓扑。虽然在流形和图的背景下，曲率的效用已经在理论和实证上得到了证实，但其在新兴的超图领域的推广仍然是一个未被充分探索的问题。在图上，Ollivier-Ricci曲率通过Wasserstein距离度量随机游走之间的不同，从而将几何概念落实到概率论和最优输运的思想中。我们开发了ORCHID，一个将Ollivier-Ricci曲率推广到超图的灵活框架，并证明了所得曲率具有良好的理论属性。通过对来自不同领域的合成和真实超图的广泛实验，我们证明ORCHID曲率既具有可扩展性，也有用于进行各种超图任务的实用性。

    Bridging geometry and topology, curvature is a powerful and expressive invariant. While the utility of curvature has been theoretically and empirically confirmed in the context of manifolds and graphs, its generalization to the emerging domain of hypergraphs has remained largely unexplored. On graphs, the Ollivier-Ricci curvature measures differences between random walks via Wasserstein distances, thus grounding a geometric concept in ideas from probability theory and optimal transport. We develop ORCHID, a flexible framework generalizing Ollivier-Ricci curvature to hypergraphs, and prove that the resulting curvatures have favorable theoretical properties. Through extensive experiments on synthetic and real-world hypergraphs from different domains, we demonstrate that ORCHID curvatures are both scalable and useful to perform a variety of hypergraph tasks in practice.
    
[^128]: 可变校准机器学习分类器

    Variable-Based Calibration for Machine Learning Classifiers. (arXiv:2209.15154v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15154](http://arxiv.org/abs/2209.15154)

    提出可变校准的概念，并发现在数据特征方面即使拥有良好 ECE 的模型还是可能存在显著的校准失误，为此提出了检测、可视化和定量化可变校准误差的策略。

    

    在高风险领域部署机器学习分类器需要有良好校准信心分数以预测模型。本文介绍了可变校准的概念来描述模型在特定兴趣变量方面的调节属性，将传统的基于得分的指标如预期校准误差（ECE）进行推广。通过理论和实践在多个知名数据集上演示了即使拥有接近完美 ECE 的模型在数据特征方面还是可能存在显著的校准失误，而且这种校准失误可能在应用现有的校准方法后还会持续存在。为了减轻这个问题，我们提出了检测、可视化和定量化可变校准误差的策略。接着我们分析了当前得分校准方法的局限性并探讨了潜在的修改方法。最后，我们讨论了这些发现的启示。

    The deployment of machine learning classifiers in high-stakes domains requires well-calibrated confidence scores for model predictions. In this paper we introduce the notion of variable-based calibration to characterize calibration properties of a model with respect to a variable of interest, generalizing traditional score-based metrics such as expected calibration error (ECE). In particular, we find that models with near-perfect ECE can exhibit significant miscalibration as a function of features of the data. We demonstrate this phenomenon both theoretically and in practice on multiple well-known datasets, and show that it can persist after the application of existing calibration methods. To mitigate this issue, we propose strategies for detection, visualization, and quantification of variable-based calibration error. We then examine the limitations of current score-based calibration methods and explore potential modifications. Finally, we discuss the implications of these findings, e
    
[^129]: 针对井测数据的非对比度表示学习

    Non-contrastive representation learning for intervals from well logs. (arXiv:2209.14750v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.14750](http://arxiv.org/abs/2209.14750)

    本文提出了一种新的方法来处理井测数据表示学习问题，采用自我监督学习的方法进行非对比度的表示学习，减少对数据的标注需求，并提高了算法性能。

    

    石油和天然气行业中的表示学习问题旨在构建一个模型，根据钻井数据为井段提供表示形式。以往的尝试主要是有监督的，并且关注于相似性任务，即估计井段之间的相似程度。我们希望在不使用已标记数据的情况下构建信息量丰富的表示形式。其中一个可能的方法是自我监督学习（SSL）。与有监督范式相反，这个方法对数据需要很少或者没有标签。现今，大多数SSL方法要么是对比的，要么是非对比的。对比方法使相似的（正）对象的表示变得更加接近，并将不同的（负）对象与之距离。由于可能存在错误的正负标注，这些方法可能会提供更差的性能。非对比方法不依赖于此类标注，在计算机视觉领域广泛应用。它们仅使用容易识别的相似对象对进行学习。

    The representation learning problem in the oil & gas industry aims to construct a model that provides a representation based on logging data for a well interval. Previous attempts are mainly supervised and focus on similarity task, which estimates closeness between intervals. We desire to build informative representations without using supervised (labelled) data. One of the possible approaches is self-supervised learning (SSL). In contrast to the supervised paradigm, this one requires little or no labels for the data. Nowadays, most SSL approaches are either contrastive or non-contrastive. Contrastive methods make representations of similar (positive) objects closer and distancing different (negative) ones. Due to possible wrong marking of positive and negative pairs, these methods can provide an inferior performance. Non-contrastive methods don't rely on such labelling and are widespread in computer vision. They learn using only pairs of similar objects that are easier to identify in 
    
[^130]: TRBoost: 基于信赖域方法的通用梯度提升机

    TRBoost: A Generic Gradient Boosting Machine based on Trust-region Method. (arXiv:2209.13791v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.13791](http://arxiv.org/abs/2209.13791)

    TRBoost 是一种新型通用梯度提升机，使用约束二次模型来近似目标并应用信赖域算法来获得新的学习器，具有适用于任意损失函数的通用性和竞争性能。

    

    梯度提升机 (GBMs) 利用函数空间的泰勒展开显著成功地解决了各种问题。然而，性能和通用性之间的平衡对 GBMs 提出了挑战。尤其是，基于梯度下降的 GBMs 使用一阶泰勒展开以确保适用于所有损失函数，而基于牛顿方法的 GBMs 利用正定的黑塞矩阵获得卓越的性能，但以牺牲通用性为代价。为解决这个问题，本研究提出了一种新型的通用梯度提升机，称为 TRBoost。在每次迭代中，TRBoost 使用一个约束二次模型来近似目标并应用信赖域算法来解决它并获得一个新的学习器。与基于牛顿方法的 GBMs 不同，TRBoost 不要求黑塞矩阵是正定的，因此允许它用于任意损失函数，同时仍保持竞争性能。

    Gradient Boosting Machines (GBMs) have demonstrated remarkable success in solving diverse problems by utilizing Taylor expansions in functional space. However, achieving a balance between performance and generality has posed a challenge for GBMs. In particular, gradient descent-based GBMs employ the first-order Taylor expansion to ensure applicability to all loss functions, while Newton's method-based GBMs use positive Hessian information to achieve superior performance at the expense of generality. To address this issue, this study proposes a new generic Gradient Boosting Machine called Trust-region Boosting (TRBoost). In each iteration, TRBoost uses a constrained quadratic model to approximate the objective and applies the Trust-region algorithm to solve it and obtain a new learner. Unlike Newton's method-based GBMs, TRBoost does not require the Hessian to be positive definite, thereby allowing it to be applied to arbitrary loss functions while still maintaining competitive performan
    
[^131]: PREF: 可预测性正则化的神经运动场

    PREF: Predictability Regularized Neural Motion Fields. (arXiv:2209.10691v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.10691](http://arxiv.org/abs/2209.10691)

    本文提出了一个名为PREF的框架，通过正则化估计的三维运动场为可预测性。该框架在多视图设置下建模动态场景中所有点的运动，并采用潜在嵌入和预测网络来实现可预测性。实验结果表明，该框架的表现优于其他基于神经运动场的动态场景表示方法。

    

    在许多视觉应用中，了解动态场景的三维运动对于估计活动情况至关重要。最近的研究主要集中在估计特定元素（如人）的活动情况上。本文利用神经运动场估计多视图设置下所有点的运动。由于颜色相似的点和颜色变化的点的模糊性，从动态场景的多视图数据中建模运动是具有挑战性的。我们建议将估计的运动正则化为可预测。如果已知之前几帧的运动，则在不久的将来的运动应该是可预测的。因此，我们首先在潜在嵌入上对估计的运动进行条件化，然后通过采用预测网络对嵌入进行可预测性约束。所提出的框架PREF（可预测性正则化场）实现了与基于神经运动场的动态场景表示最先进的同等甚至更好的结果。

    Knowing the 3D motions in a dynamic scene is essential to many vision applications. Recent progress is mainly focused on estimating the activity of some specific elements like humans. In this paper, we leverage a neural motion field for estimating the motion of all points in a multiview setting. Modeling the motion from a dynamic scene with multiview data is challenging due to the ambiguities in points of similar color and points with time-varying color. We propose to regularize the estimated motion to be predictable. If the motion from previous frames is known, then the motion in the near future should be predictable. Therefore, we introduce a predictability regularization by first conditioning the estimated motion on latent embeddings, then by adopting a predictor network to enforce predictability on the embeddings. The proposed framework PREF (Predictability REgularized Fields) achieves on par or better results than state-of-the-art neural motion field-based dynamic scene representa
    
[^132]: 基于可微物理引擎的索驱动机器人的真实世界到仿真世界的控制转移

    Real2Sim2Real Transfer for Control of Cable-driven Robots via a Differentiable Physics Engine. (arXiv:2209.06261v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.06261](http://arxiv.org/abs/2209.06261)

    本文描述了一种基于可微物理引擎的真实世界到仿真世界转移的策略，该策略通过对真实机器人的有限数据进行迭代训练，以减少实到虚之间的差距并产生准确的仿真。该策略在索驱动张力结构机器人上得到了测试，并证明了其有效性。

    

    张力结构机器人由坚硬的杆和柔软的缆绳组成，具有高强度重量比和显著的变形能力，使其能够在非结构化的地形中航行并在严峻的撞击中存活。然而，由于维度高、动力复杂且耦合结构使得它们难以控制。基于物理的仿真是开发可以转移到实际机器人的运动策略的有前途途径。然而，由于实到虚之间的显著差距，对张力结构机器人进行建模是一个复杂的任务。为了解决这个问题，本文描述了一种基于不同iable物理引擎的张力结构机器人的真实世界到仿真世界的转移策略(R2S2R)。该策略基于一个可训练的可微物理引擎，通过对真实机器人的有限数据进行训练，并将包括物理属性的离线测量，如质量和几何体的各种机器人部件，以及使用随机控制策略的轨迹观察。利用来自真实机器人的数据，物理引擎可以进行迭代训练，以减少实到虚之间的差距并产生准确的仿真。这种R2S2R策略在索驱动张力结构机器人上得到了测试，并证明了使用可微物理引擎开发可以转移到实际机器人的控制策略的有效性。

    Tensegrity robots, composed of rigid rods and flexible cables, exhibit high strength-to-weight ratios and significant deformations, which enable them to navigate unstructured terrains and survive harsh impacts. They are hard to control, however, due to high dimensionality, complex dynamics, and a coupled architecture. Physics-based simulation is a promising avenue for developing locomotion policies that can be transferred to real robots. Nevertheless, modeling tensegrity robots is a complex task due to a substantial sim2real gap. To address this issue, this paper describes a Real2Sim2Real (R2S2R) strategy for tensegrity robots. This strategy is based on a differentiable physics engine that can be trained given limited data from a real robot. These data include offline measurements of physical properties, such as mass and geometry for various robot components, and the observation of a trajectory using a random control policy. With the data from the real robot, the engine can be iterativ
    
[^133]: Bayan算法：通过对模块度的精确和近似优化来检测网络中的社区

    The Bayan Algorithm: Detecting Communities in Networks Through Exact and Approximate Optimization of Modularity. (arXiv:2209.04562v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2209.04562](http://arxiv.org/abs/2209.04562)

    提出了一种名为Bayan的社区检测算法，通过精确或近似优化模块度的方法，它能够返回最优或接近最优的分区，并且比其他算法快数倍，并能够在合成和真实网络数据集上准确地找到地面真实社区。

    

    社区检测是网络科学中的经典问题，具有广泛的应用。在众多方法中，最常见的方法是最大化模块度。尽管启发式模块度最大化算法设计理念和广泛采用，但很少返回最佳分区或类似分区。我们提出了一种专门的算法Bayan，它返回具有最优或接近最优分区保证的分区。Bayan算法的核心是一种分支限界方案，它解决了问题的整数规划公式以达到最优或近似最优的目的。我们证明Bayan在合成基准和真实网络节点标签的检索地面真实社区方面具有独特的准确性和稳定性，比其他21种算法快数倍，可以找到最优分区的实例。

    Community detection is a classic problem in network science with extensive applications in various fields. Among numerous approaches, the most common method is modularity maximization. Despite their design philosophy and wide adoption, heuristic modularity maximization algorithms rarely return an optimal partition or anything similar. We propose a specialized algorithm, Bayan, which returns partitions with a guarantee of either optimality or proximity to an optimal partition. At the core of the Bayan algorithm is a branch-and-cut scheme that solves an integer programming formulation of the problem to optimality or approximate it within a factor. We demonstrate Bayan's distinctive accuracy and stability over 21 other algorithms in retrieving ground-truth communities in synthetic benchmarks and node labels in real networks. Bayan is several times faster than open-source and commercial solvers for modularity maximization making it capable of finding optimal partitions for instances that c
    
[^134]: 延迟反馈在广义线性赌博机中的研究再访

    Delayed Feedback in Generalised Linear Bandits Revisited. (arXiv:2207.10786v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10786](http://arxiv.org/abs/2207.10786)

    研究了广义线性赌博机中的延迟奖励现象，提出了一种自然的乐观算法，可实现一个独立于时间的惩罚函数，降低了现有工作中随着时间增长而增加的惩罚函数的界限。

    

    随着许多真实世界的应用中奖励几乎总是被延迟，导致要求即时奖励的模型难以应用。本文将研究在广义线性赌博机中延迟奖励的现象。我们证明了一种自然的乐观算法适应延迟反馈领域能够有一个与时间无关的惩罚函数。这比现有的工作显著的提高了，因为最佳的已知的惩罚函数的界限随着时间的推移而增加。

    The stochastic generalised linear bandit is a well-understood model for sequential decision-making problems, with many algorithms achieving near-optimal regret guarantees under immediate feedback. However, the stringent requirement for immediate rewards is unmet in many real-world applications where the reward is almost always delayed. We study the phenomenon of delayed rewards in generalised linear bandits in a theoretical manner. We show that a natural adaptation of an optimistic algorithm to the delayed feedback achieves a regret bound where the penalty for the delays is independent of the horizon. This result significantly improves upon existing work, where the best known regret bound has the delay penalty increasing with the horizon. We verify our theoretical results through experiments on simulated data.
    
[^135]: 基于原始估计亚梯度求解器的不平衡分类SVM

    Primal Estimated Subgradient Solver for SVM for Imbalanced Classification. (arXiv:2206.09311v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09311](http://arxiv.org/abs/2206.09311)

    本研究旨在实现对不平衡数据集的分类，并评估成本敏感的PEGASOS SVM的性能，同时将核函数纳入SVM中扩展Ding的工作。

    

    本研究旨在通过实验证明我们的成本敏感PEGASOS SVM在主多次要比从8.6：1到130：1的不平衡数据集上具有良好的性能，并确定包括截距（偏见）、正则化和参数是否会影响我们选择的数据集上的性能。虽然许多人采用SMOTE方法，但我们旨在采用一种计算量较小的方法。通过检查学习曲线来评估性能，这些曲线可以诊断我们是过度拟合还是欠拟合，或者我们选择了过度代表性或欠代表性的训练/测试数据。我们还将在验证曲线中查看超参数的背景与测试和训练误差之间的关系。我们将基准化我们的PEGASOS成本敏感SVM与Ding的LINEAR SVM DECIDL方法的结果。他在一个数据集中获得了0.5的ROC-AUC。我们的工作将通过将核函数纳入SVM来扩展Ding的工作。我们将使用Python而不是MATLAB，因为Python具有更有效地存储我的数据集的字典。

    We aim to demonstrate in experiments that our cost sensitive PEGASOS SVM achieves good performance on imbalanced data sets with a Majority to Minority Ratio ranging from 8.6:1 to 130:1 and to ascertain whether the including intercept (bias), regularization and parameters affects performance on our selection of datasets. Although many resort to SMOTE methods, we aim for a less computationally intensive method. We evaluate the performance by examining the learning curves. These curves diagnose whether we overfit or underfit or we choose over representative or under representative training/test data. We will also see the background of the hyperparameters versus the test and train error in validation curves. We benchmark our PEGASOS Cost-Sensitive SVM's results of Ding's LINEAR SVM DECIDL method. He obtained an ROC-AUC of .5 in one dataset. Our work will extend the work of Ding by incorporating kernels into SVM. We will use Python rather than MATLAB as python has dictionaries for storing m
    
[^136]: 基于结构化变分图自编码器的准确节点特征估计

    Accurate Node Feature Estimation with Structured Variational Graph Autoencoder. (arXiv:2206.04516v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04516](http://arxiv.org/abs/2206.04516)

    本论文提出了一种基于结构化变分图自编码器的准确节点特征估计方法，结合了变分推断和图神经网络的优点，可以在有限的数据情况下提供准确的估计。

    

    给定一个包含节点特征部分观测的图，如何精确地估计缺失的特征？特征估计对于分析实际图形是至关重要的，因为在数据收集过程中，这些特征通常会缺失。准确的估计不仅提供节点的多样信息，而且支持需要完整观测节点特征的图神经网络的推理。然而，设计一种有效的方法来估计高维特征是具有挑战性的，因为它需要估计器具有大的表现力，增加了过拟合的风险。在本文中，我们提出了一种准确的特征估计方法SVGA（Structured Variational Graph Autoencoder）。SVGA通过结构化变分推断对潜变量的分布进行强大的正则化，该推断基于图形结构将变量的先验模型建模为高斯马尔科夫随机场。因此，SVGA结合了变分推断和图神经网络的优点，可以在有限的数据情况下提供准确的估计。

    Given a graph with partial observations of node features, how can we estimate the missing features accurately? Feature estimation is a crucial problem for analyzing real-world graphs whose features are commonly missing during the data collection process. Accurate estimation not only provides diverse information of nodes but also supports the inference of graph neural networks that require the full observation of node features. However, designing an effective approach for estimating high-dimensional features is challenging, since it requires an estimator to have large representation power, increasing the risk of overfitting. In this work, we propose SVGA (Structured Variational Graph Autoencoder), an accurate method for feature estimation. SVGA applies strong regularization to the distribution of latent variables by structured variational inference, which models the prior of variables as Gaussian Markov random field based on the graph structure. As a result, SVGA combines the advantages
    
[^137]: 快速而精确：自适应子目标搜索调整规划长度

    Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search. (arXiv:2206.00702v8 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.00702](http://arxiv.org/abs/2206.00702)

    AdaSubS 是一种自适应的子目标搜索方法，它采用验证机制快速过滤出不可达子目标，从而实现在计划更长的子目标的效率和在计划更短的子目标方面具有精细控制，在 Sokoban、魔方和不等式证明基准 INT 等复杂推理任务上表现优越。

    

    复杂的推理问题包含需要耗费不同计算成本来确定良好行动计划的状态。针对这一特性，我们提出了一种名为自适应子目标搜索 (AdaSubS) 的搜索方法，可以自适应性地调整规划长度。为此，AdaSubS 生成不同距离下的多样化子目标集。采用验证机制快速过滤出不可达子目标，以便专注于可以实现的后续子目标。通过这种方式，AdaSubS 在计划更长的子目标的效率和在计划更短的子目标方面具有精细控制，并因此在难解的规划问题上扩展得很好。我们展示了 AdaSubS 在三个复杂推理任务 Sokoban、魔方和不等式证明基准 INT 上显著超过分层规划算法。

    Complex reasoning problems contain states that vary in the computational cost required to determine a good action plan. Taking advantage of this property, we propose Adaptive Subgoal Search (AdaSubS), a search method that adaptively adjusts the planning horizon. To this end, AdaSubS generates diverse sets of subgoals at different distances. A verification mechanism is employed to filter out unreachable subgoals swiftly, allowing to focus on feasible further subgoals. In this way, AdaSubS benefits from the efficiency of planning with longer subgoals and the fine control with the shorter ones, and thus scales well to difficult planning problems. We show that AdaSubS significantly surpasses hierarchical planning algorithms on three complex reasoning tasks: Sokoban, the Rubik's Cube, and inequality proving benchmark INT.
    
[^138]: 带随机变量的参数化验证-随机变分光滑模型检验的可扩展性研究

    Scalable Stochastic Parametric Verification with Stochastic Variational Smoothed Model Checking. (arXiv:2205.05398v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.05398](http://arxiv.org/abs/2205.05398)

    本论文提出了一种利用概率机器学习扩展平滑模型检验(smMC)方法的思路，从而使贝叶斯推理的smMC适用于更大的数据集和实际问题。

    

    针对随机模型的线性时态性属性的参数化验证可以表示为计算满足一定属性的概率，函数的参数为这个模型的参数。平滑模型检验(smMC)旨在从通过模拟获得的有限的观测值中推断出整个参数空间上的满足函数。由于观测成本高且噪声大，因此smMC被构建为贝叶斯推理问题，使估计值具有额外的不确定性量化。在smMC中，作者使用由期望传播算法推断出的高斯过程(GP)。这种方法提供了准确的重构和统计上合理的不确定性量化。然而，它继承了GP的著名可扩展性问题。因此，本文利用概率机器学习的最新进展，将贝叶斯推理的smMC扩展到更大的数据集，并使其适用于实际问题。

    Parametric verification of linear temporal properties for stochastic models can be expressed as computing the satisfaction probability of a certain property as a function of the parameters of the model. Smoothed model checking (smMC) aims at inferring the satisfaction function over the entire parameter space from a limited set of observations obtained via simulation. As observations are costly and noisy, smMC is framed as a Bayesian inference problem so that the estimates have an additional quantification of the uncertainty. In smMC the authors use Gaussian Processes (GP), inferred by means of the Expectation Propagation algorithm. This approach provides accurate reconstructions with statistically sound quantification of the uncertainty. However, it inherits the well-known scalability issues of GP. In this paper, we exploit recent advances in probabilistic machine learning to push this limitation forward, making Bayesian inference of smMC scalable to larger datasets and enabling its ap
    
[^139]: WOODS: 时间序列领域的离群分布广义性的基准测试

    WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series. (arXiv:2203.09978v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.09978](http://arxiv.org/abs/2203.09978)

    该论文提出了一个名为WOODS的时间序列基准测试，致力于解决在离群分布下的泛化过程中面临的挑战，还改进了目前时间序列任务中的离群分布广义性算法，并表明仍有很大的改进空间。

    

    机器学习模型在分布偏移下往往难以进行很好的泛化。理解和克服这些问题形成了离群分布广义性的研究领域。尽管对于静态计算机视觉任务已经得到广泛研究，但在时间序列任务中，离群分布广义性却鲜有探索。为减少这一差距，我们提出WOODS：八个具有挑战性的开源时间序列基准测试，涵盖了各种数据模态，例如视频、脑记录和传感器信号。我们改进了现有的时间序列任务的离群分布广义性算法，并使用我们的系统框架进行评估。我们的实验显示，对于我们的数据集，经验风险最小化和离群分布广义性算法仍有很大的改进空间，从而凸显了时间序列任务面临的新挑战。代码和文档可在https://woods-benchmarks.github.io获得。

    Machine learning models often fail to generalize well under distributional shifts. Understanding and overcoming these failures have led to a research field of Out-of-Distribution (OOD) generalization. Despite being extensively studied for static computer vision tasks, OOD generalization has been underexplored for time series tasks. To shine light on this gap, we present WOODS: eight challenging open-source time series benchmarks covering a diverse range of data modalities, such as videos, brain recordings, and sensor signals. We revise the existing OOD generalization algorithms for time series tasks and evaluate them using our systematic framework. Our experiments show a large room for improvement for empirical risk minimization and OOD generalization algorithms on our datasets, thus underscoring the new challenges posed by time series tasks. Code and documentation are available at https://woods-benchmarks.github.io .
    
[^140]: 量子差分隐私：信息论视角

    Quantum Differential Privacy: An Information Theory Perspective. (arXiv:2202.10717v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2202.10717](http://arxiv.org/abs/2202.10717)

    该论文讨论了量子差分隐私并以量子散度形式将其讨论在信息理论框架下，将差分隐私的属性从每个测量检查转为仅基于计算输出状态的属性，使得证明更简单、性质广泛并提供了新的限制。

    

    差分隐私在提供经典计算的可证安全性方面取得了非常成功的成果。最近，这个概念被推广到量子计算中。虽然经典计算本质上是无噪声的，差分隐私通常通过人为添加噪声来实现，但近期的量子计算机本来就是嘈杂的，因此自然存在差分隐私这一特性。在本文中，我们以量子散度的形式将量子差分隐私讨论在信息理论框架下。该方法的主要优势在于，差分隐私成为仅基于计算输出状态的属性，而不需要对每个测量进行检查。这导致证明更简单，性质的概括声明更广泛，并为通用噪声模型和特定噪声模型提供了若干新的限制。特别是，这些限制包括常见的表示形式。

    Differential privacy has been an exceptionally successful concept when it comes to providing provable security guarantees for classical computations. More recently, the concept was generalized to quantum computations. While classical computations are essentially noiseless and differential privacy is often achieved by artificially adding noise, near-term quantum computers are inherently noisy and it was observed that this leads to natural differential privacy as a feature.  In this work we discuss quantum differential privacy in an information theoretic framework by casting it as a quantum divergence. A main advantage of this approach is that differential privacy becomes a property solely based on the output states of the computation, without the need to check it for every measurement. This leads to simpler proofs and generalized statements of its properties as well as several new bounds for both, general and specific, noise models. In particular, these include common representations of
    
[^141]: StratDef: 机器学习恶意软件检测对抗攻击的战略防御

    StratDef: Strategic Defense Against Adversarial Attacks in ML-based Malware Detection. (arXiv:2202.07568v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07568](http://arxiv.org/abs/2202.07568)

    本文提出了一个名为StratDef的移动目标防御方法的战略防御系统，针对机器学习恶意软件检测的防御措施进行了全面评估。StratDef动态地和策略地选择最佳模型，以增加攻击者的不确定性，同时最小化对攻击的影响，使其具有很高的对抗鲁棒性。

    

    多年来针对机器学习模型的对抗攻击防御大多集中在图像识别领域。尽管恶意软件检测领域的重要性很高，但它却受到了较少的关注。而且，大多数关于这些防御措施的研究都集中在一些方法上，而没有具体的应用策略。本文提出了一个名为StratDef的战略防御系统，它基于移动目标防御方法。我们解决了涉及模型系统化构建、选择和战略使用的挑战，以最大化对抗鲁棒性。在对抗机器学习领域的关键方面，如攻击可转移性，StratDef动态地和策略地选择最佳模型，以增加攻击者的不确定性，同时最小化对攻击的影响。我们为机器学习恶意软件检测的防御措施首次进行了全面评估，其中我们的威胁模型探索了不同级别的威胁、攻击者的知识水平等。

    Over the years, most research towards defenses against adversarial attacks on machine learning models has been in the image recognition domain. The malware detection domain has received less attention despite its importance. Moreover, most work exploring these defenses has focused on several methods but with no strategy when applying them. In this paper, we introduce StratDef, which is a strategic defense system based on a moving target defense approach. We overcome challenges related to the systematic construction, selection, and strategic use of models to maximize adversarial robustness. StratDef dynamically and strategically chooses the best models to increase the uncertainty for the attacker while minimizing critical aspects in the adversarial ML domain, like attack transferability. We provide the first comprehensive evaluation of defenses against adversarial attacks on machine learning for malware detection, where our threat model explores different levels of threat, attacker know
    
[^142]: 连续重复退火流输运蒙特卡罗

    Continual Repeated Annealed Flow Transport Monte Carlo. (arXiv:2201.13117v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2201.13117](http://arxiv.org/abs/2201.13117)

    这篇论文提出了一种结合序列蒙特卡罗采样和标准化流变分推断的连续重复退火流输运蒙特卡罗方法，通过对标准化流的训练，实现不同温度下的传输，并在多个实例中展示了其优于其他方法的表现。

    

    我们提出连续重复退火流输运蒙特卡罗（CRAFT）方法，将序列蒙特卡罗（SMC）采样器（自身是逐步重要采样的推广）与使用标准化流的变分推断相结合。标准化流直接训练以在每个转换之间传输退火温度，使用KL散度进行优化目标，此优化目标本身使用标准化流/ SMC近似估计。我们在概念上和多个经验实例中展示了CRAFT优于Annealed Flow Transport Monte Carlo（Arbel等人，2021），并在其基础上改进了马尔可夫链蒙特卡罗（MCMC）的随机标准化流（Wu等人，2020）。通过在粒子MCMC中结合CRAFT，我们展示了这样学习的采样器在具有挑战性的晶格场论实例上可以实现令人印象深刻的精确结果。

    We propose Continual Repeated Annealed Flow Transport Monte Carlo (CRAFT), a method that combines a sequential Monte Carlo (SMC) sampler (itself a generalization of Annealed Importance Sampling) with variational inference using normalizing flows. The normalizing flows are directly trained to transport between annealing temperatures using a KL divergence for each transition. This optimization objective is itself estimated using the normalizing flow/SMC approximation. We show conceptually and using multiple empirical examples that CRAFT improves on Annealed Flow Transport Monte Carlo (Arbel et al., 2021), on which it builds and also on Markov chain Monte Carlo (MCMC) based Stochastic Normalizing Flows (Wu et al., 2020). By incorporating CRAFT within particle MCMC, we show that such learnt samplers can achieve impressively accurate results on a challenging lattice field theory example.
    
[^143]: 动态最小二乘回归的复杂度

    The Complexity of Dynamic Least-Squares Regression. (arXiv:2201.00228v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2201.00228](http://arxiv.org/abs/2201.00228)

    本文研究了动态最小二乘回归的复杂度，证明了在部分动态设置中高精度和低精度LSR的平摊更新时间有尖锐分离，并利用在线矩阵向量猜想进行了间隙放大约简。

    

    我们研究了动态最小二乘回归（LSR）的复杂度，其中行和标签 $(\mathbf{A}^{(t)},\mathbf{b}^{(t)})$ 可以被自适应地插入和/或删除，目标是有效地维护 $\min_{\mathbf{x}^{(t)}} \| \mathbf{A}^{(t)} \mathbf{x}^{(t)} - \mathbf{b}^{(t)} \|_2$ 的 $\epsilon$-近似解对于所有 $t\in [T]$。我们证明了（i）完全动态 vs 部分动态 $0.01$-LSR；（ii）高精度 vs 低精度LSR在部分动态（仅插入）设置中的平摊更新时间的尖锐分离($d^{2-o(1)}$ vs. $\sim d$)。我们的下界来自一个间隙放大约简（类似于迭代重构）——从准确的在线矩阵向量猜想（OMv）[HKNS15]到实数上的常数近似OMv，其中第 $i$ 个在线积 $\mathbf{H}\mathbf{v}^{(i)}$ 只需要计算到 $0.1$-相对误差。所有以前的由OMv到它的近似版本的精细约简都只表现出强困难性。

    We settle the complexity of dynamic least-squares regression (LSR), where rows and labels $(\mathbf{A}^{(t)}, \mathbf{b}^{(t)})$ can be adaptively inserted and/or deleted, and the goal is to efficiently maintain an $\epsilon$-approximate solution to $\min_{\mathbf{x}^{(t)}} \| \mathbf{A}^{(t)} \mathbf{x}^{(t)} - \mathbf{b}^{(t)} \|_2$ for all $t\in [T]$. We prove sharp separations ($d^{2-o(1)}$ vs. $\sim d$) between the amortized update time of: (i) Fully vs. Partially dynamic $0.01$-LSR; (ii) High vs. low-accuracy LSR in the partially-dynamic (insertion-only) setting.  Our lower bounds follow from a gap-amplification reduction -- reminiscent of iterative refinement -- rom the exact version of the Online Matrix Vector Conjecture (OMv) [HKNS15], to constant approximate OMv over the reals, where the $i$-th online product $\mathbf{H}\mathbf{v}^{(i)}$ only needs to be computed to $0.1$-relative error. All previous fine-grained reductions from OMv to its approximate versions only show hardn
    
[^144]: 稳健对抗性训练的强力上界

    Robust Upper Bounds for Adversarial Training. (arXiv:2112.09279v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.09279](http://arxiv.org/abs/2112.09279)

    该论文提出了一种新的对抗性训练方法，通过最小化基于网络的整体展开的对抗损失上界来实现。与传统方法相比，该方法利用了最新的稳健优化领域的工具，可以在保证输出层绑定紧密性的同时，有效地进行训练。

    

    为了提供对抗攻击的安全保证，许多最先进的深度学习对抗性训练方法利用对抗损失的上界。然而，这些方法依赖于凸松弛来传播中间层的下界和上界，这会影响输出层绑定的紧密性。我们引入了一种新的对抗性训练方法，通过最小化基于网络的整体展开的对抗损失上界来实现。该上界利用了稳健优化领域的最新工具，具有闭合形式，并且可以使用反向传播进行有效训练。我们提出了两种新方法来实现这种方法。第一种方法（近似稳健上界或aRUB）使用网络的一阶近似和线性稳健优化的基本工具，获得对抗损失的经验上界，可以轻松实现。

    Many state-of-the-art adversarial training methods for deep learning leverage upper bounds of the adversarial loss to provide security guarantees against adversarial attacks. Yet, these methods rely on convex relaxations to propagate lower and upper bounds for intermediate layers, which affect the tightness of the bound at the output layer. We introduce a new approach to adversarial training by minimizing an upper bound of the adversarial loss that is based on a holistic expansion of the network instead of separate bounds for each layer. This bound is facilitated by state-of-the-art tools from Robust Optimization; it has closed-form and can be effectively trained using backpropagation. We derive two new methods with the proposed approach. The first method (Approximated Robust Upper Bound or aRUB) uses the first order approximation of the network as well as basic tools from Linear Robust Optimization to obtain an empirical upper bound of the adversarial loss that can be easily implement
    
[^145]: ConDA: 通过规范化域拼接进行激光雷达分割的无监督域自适应

    ConDA: Unsupervised Domain Adaptation for LiDAR Segmentation via Regularized Domain Concatenation. (arXiv:2111.15242v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.15242](http://arxiv.org/abs/2111.15242)

    本文提出了一个基于拼接的激光雷达分割无监督域自适应框架ConDA，通过构建包含源域和目标域的细粒度互换信号的中间域，实现在不破坏语义连贯性的情况下进行自我训练。

    

    将有标签的源域中学到的知识转移到原始的目标域中，以实现无监督域自适应（UDA），对于自动驾驶系统的可扩展部署非常重要。本文提出了一种基于拼接的域自适应框架ConDA，用于激光雷达分割，旨在构建一个包含源域和目标域的细粒度互换信号的中间域，而不破坏自车周围物体和背景的语义连贯性，并利用该中间域进行自我训练。

    Transferring knowledge learned from the labeled source domain to the raw target domain for unsupervised domain adaptation (UDA) is essential to the scalable deployment of autonomous driving systems. State-of-the-art methods in UDA often employ a key idea: utilizing joint supervision signals from both source and target domains for self-training. In this work, we improve and extend this aspect. We present ConDA, a concatenation-based domain adaptation framework for LiDAR segmentation that: 1) constructs an intermediate domain consisting of fine-grained interchange signals from both source and target domains without destabilizing the semantic coherency of objects and background around the ego-vehicle; and 2) utilizes the intermediate domain for self-training. To improve the network training on the source domain and self-training on the intermediate domain, we propose an anti-aliasing regularizer and an entropy aggregator to reduce the negative effect caused by the aliasing artifacts and n
    
[^146]: 乐观算法与延迟的周期性强化学习

    Optimism and Delays in Episodic Reinforcement Learning. (arXiv:2111.07615v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.07615](http://arxiv.org/abs/2111.07615)

    本文从理论角度探讨了延迟反馈对周期性强化学习的影响，提出了两种通用方法来处理延迟，并指出这两种方法针对乐观算法类后悔会增加一个与状态数量、动作数量、情节长度、预期延迟和算法相关常数的加性项。

    

    周期性强化学习中有很多用于减少后悔的算法。从理论角度来看，问题已经得到了很好的理解，只要与每次与环境交互后立即更新策略的算法可以获取与每个情节相关的状态、动作和奖励序列。然而，在实践中，反馈几乎总是延迟的。本文从理论角度研究了延迟反馈对周期性强化学习的影响，并提出了两种通用方法来处理延迟。第一个方法涉及到在新信息可用时立即更新，而第二个方法则等待使用新观察到的信息来更新策略。针对乐观算法类和任一方法，我们表明后悔会增加一个与状态数量、动作数量、情节长度、预期延迟和算法相关常数的加性项。我们还进行了实证研究。

    There are many algorithms for regret minimisation in episodic reinforcement learning. This problem is well-understood from a theoretical perspective, providing that the sequences of states, actions and rewards associated with each episode are available to the algorithm updating the policy immediately after every interaction with the environment. However, feedback is almost always delayed in practice. In this paper, we study the impact of delayed feedback in episodic reinforcement learning from a theoretical perspective and propose two general-purpose approaches to handling the delays. The first involves updating as soon as new information becomes available, whereas the second waits before using newly observed information to update the policy. For the class of optimistic algorithms and either approach, we show that the regret increases by an additive term involving the number of states, actions, episode length, the expected delay and an algorithm-dependent constant. We empirically inves
    
[^147]: 道路网络引导的城市细粒度交通流推断

    Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.14251](http://arxiv.org/abs/2109.14251)

    本文提出了一种基于道路网络的交通流量推断方法，利用道路网络的先验知识全面学习细粒度交通流的道路感知空间分布，普遍适用于城市交通流量监测和调控方案。

    

    精确推断细粒度交通流量是一个新兴但至关重要的问题，它可以帮助极大地减少所需交通监测传感器的数量以节省成本。本文发现交通流量与道路网络具有很高的相关性，但之前的研究中完全忽略了这一点，或者仅将其视为外部因素。为了解决这个问题，我们提出了一种新的路网感知交通流量放大器（RATFM），它明确利用道路网络的先验知识，全面学习细粒度交通流的道路感知空间分布。具体而言，我们首先引入了一个多方向1D卷积层来提取道路网络的语义特征。随后，我们将道路网络特征和粗粒度流量特征结合在一起，规范化道路相关交通流的短距离空间分布建模。此外，我们将道路网络特征作为查询来捕获长距离路段交通流量的关联。

    Accurate inference of fine-grained traffic flow from coarse-grained one is an emerging yet crucial problem, which can help greatly reduce the number of the required traffic monitoring sensors for cost savings. In this work, we notice that traffic flow has a high correlation with road network, which was either completely ignored or simply treated as an external factor in previous works.To facilitate this problem, we propose a novel Road-Aware Traffic Flow Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks to fully learn the road-aware spatial distribution of fine-grained traffic flow. Specifically, a multi-directional 1D convolutional layer is first introduced to extract the semantic feature of the road network. Subsequently, we incorporate the road network feature and coarse-grained flow feature to regularize the short-range spatial distribution modeling of road-relative traffic flow. Furthermore, we take the road network feature as a query to capture the l
    
[^148]: 缺失数据增强：提高生成式填补模型性能的通用方法

    Missingness Augmentation: A General Approach for Improving Generative Imputation Models. (arXiv:2108.02566v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.02566](http://arxiv.org/abs/2108.02566)

    这篇论文提出了一种新的数据增强方法，称为缺失数据增强（MisA），可以用于生成式填补模型。该方法通过动态产生不完整样本，用一个简单的重构损失对增强样本进行约束，为生成式填补框架提供了一种简单而有效的提高性能的方法。

    

    缺失数据填补是数据分析中的基础问题，许多研究已经探索了模型结构和学习过程来提高其性能。然而，数据增强作为一种简单而有效的方法，在这个领域里没有得到足够的关注。我们提出了一种新颖的数据增强方法，称为缺失数据增强（MisA），用于生成式填补模型。我们的方法利用生成器的输出每一轮动态产生不完整样本，通过一个简单的重构损失对增强样本进行约束，并将此损失与原始损失组合成最终的优化目标。作为一种通用的增强技术，MisA可以轻松地集成到生成式填补框架中，为提高性能提供了一种简单却有效的方法。实验结果表明，MisA显著提高了许多最近提出的生成式填补模型的性能。

    Missing data imputation is a fundamental problem in data analysis, and many studies have been conducted to improve its performance by exploring model structures and learning procedures. However, data augmentation, as a simple yet effective method, has not received enough attention in this area. In this paper, we propose a novel data augmentation method called Missingness Augmentation (MisA) for generative imputation models. Our approach dynamically produces incomplete samples at each epoch by utilizing the generator's output, constraining the augmented samples using a simple reconstruction loss, and combining this loss with the original loss to form the final optimization objective. As a general augmentation technique, MisA can be easily integrated into generative imputation frameworks, providing a simple yet effective way to enhance their performance. Experimental results demonstrate that MisA significantly improves the performance of many recently proposed generative imputation model
    
[^149]: 高斯过程后验的主成分分析

    Principal component analysis for Gaussian process posteriors. (arXiv:2107.07115v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.07115](http://arxiv.org/abs/2107.07115)

    本文提出了高斯过程后验的主成分分析扩展，解决了如何定义一组具有无限维参数的GP的结构的问题，并且证明了通过元学习提高目标任务性能的有效性。

    

    本文提出了高斯过程后验的主成分分析（GP-PCA）扩展。由于GP-PCA估计了一个低维度的GP后验空间，因此可以用于元学习，这是一种通过估计一组任务的结构来提高目标任务性能的框架。本研究通过考虑具有相同先验的GP后验空间，在信息几何框架下将GP的无限维度问题缩减为有限维度的情况，从而解决了如何定义一组具有无限维参数（如坐标系和发散）的GP的结构的问题。此外，我们提出了一种基于变分推理的GP-PCA近似方法，并通过实验证明了GP-PCA作为元学习的有效性。

    This paper proposes an extension of principal component analysis for Gaussian process (GP) posteriors, denoted by GP-PCA. Since GP-PCA estimates a low-dimensional space of GP posteriors, it can be used for meta-learning, which is a framework for improving the performance of target tasks by estimating a structure of a set of tasks. The issue is how to define a structure of a set of GPs with an infinite-dimensional parameter, such as coordinate system and a divergence. In this study, we reduce the infiniteness of GP to the finite-dimensional case under the information geometrical framework by considering a space of GP posteriors that have the same prior. In addition, we propose an approximation method of GP-PCA based on variational inference and demonstrate the effectiveness of GP-PCA as meta-learning through experiments.
    
[^150]: 全局谱空间图推理用于高光谱图像分类

    Spectral-Spatial Global Graph Reasoning for Hyperspectral Image Classification. (arXiv:2106.13952v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2106.13952](http://arxiv.org/abs/2106.13952)

    本文提出了一种全局谱空间图推理框架，通过在网络训练中生成超像素并结合光谱和空间信息进行图卷积操作，实现了在三个基准高光谱图像分类数据集上的最先进性能。

    

    卷积神经网络已广泛应用于高光谱图像分类。然而，传统的卷积在提取不规则分布物体的特征方面效果不佳。近期的方法尝试通过在空间拓扑上执行图卷积来解决这个问题，但固定的图结构和局部感知限制了它们的表现。本文提出了一种新方法，通过在网络训练过程中对中间特征进行超像素生成来适应性生成均质区域、获得图结构并进一步生成空间描述符，这些描述符被用作图节点。除了空间对象，我们还通过合理地聚合通道来生成光谱描述符来探索通道之间的图关系。这些图卷积中的邻接矩阵通过考虑所有描述符之间的关系来实现全局感知。在将光谱和空间信息结合到全局图推理框架中后，我们提出的方法在三个基准高光谱图像分类数据集上实现了最先进的性能。

    Convolutional neural networks have been widely applied to hyperspectral image classification. However, traditional convolutions can not effectively extract features for objects with irregular distributions. Recent methods attempt to address this issue by performing graph convolutions on spatial topologies, but fixed graph structures and local perceptions limit their performances. To tackle these problems, in this paper, different from previous approaches, we perform the superpixel generation on intermediate features during network training to adaptively produce homogeneous regions, obtain graph structures, and further generate spatial descriptors, which are served as graph nodes. Besides spatial objects, we also explore the graph relationships between channels by reasonably aggregating channels to generate spectral descriptors. The adjacent matrices in these graph convolutions are obtained by considering the relationships among all descriptors to realize global perceptions. By combinin
    
[^151]: 带有中间损失的卷积神经网络用于 CT 和 MRI 扫描的 3D 超分辨率

    Convolutional Neural Networks with Intermediate Loss for 3D Super-Resolution of CT and MRI Scans. (arXiv:2001.01330v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2001.01330](http://arxiv.org/abs/2001.01330)

    本文提出了一种用于单张图像的 3D CT 或 MRI 扫描超分辨率的方法，采用深度卷积神经网络并引入中间损失函数以缓解梯度消失问题并提高准确性。实验结果表明该方法在量化度量和视觉质量方面均优于现有方法。

    

    现在医院常用的 CT 扫描仪可产生最大 512 像素的低分辨率图像，图像中的一个像素对应组织的一毫米。为了准确地分割肿瘤并制定治疗计划，医生需要更高分辨率的 CT 扫描。同样的问题也出现在 MRI 中。本文提出了一种用于单张图像的 3D CT 或 MRI 扫描超分辨率的方法。我们的方法基于深度卷积神经网络（CNN），包括 10 个卷积层和一个中间上采样层，该层放置在前 6 个卷积层之后。我们的第一个 CNN 通过两个轴（宽度和高度）增加分辨率，紧接着是第二个 CNN，它增加了第三个轴（深度）的分辨率。与其他方法不同的是，我们在上采样层后立即计算与高分辨率输出的真值相关的损失，除了最后一个卷积层后计算的损失。中间损失有助于缓解梯度消失问题并提高准确性。我们在两个公共数据集上评估了我们的方法，并展示了它在量化度量和视觉质量方面均优于现有方法。

    CT scanners that are commonly-used in hospitals nowadays produce low-resolution images, up to 512 pixels in size. One pixel in the image corresponds to a one millimeter piece of tissue. In order to accurately segment tumors and make treatment plans, doctors need CT scans of higher resolution. The same problem appears in MRI. In this paper, we propose an approach for the single-image super-resolution of 3D CT or MRI scans. Our method is based on deep convolutional neural networks (CNNs) composed of 10 convolutional layers and an intermediate upscaling layer that is placed after the first 6 convolutional layers. Our first CNN, which increases the resolution on two axes (width and height), is followed by a second CNN, which increases the resolution on the third axis (depth). Different from other methods, we compute the loss with respect to the ground-truth high-resolution output right after the upscaling layer, in addition to computing the loss after the last convolutional layer. The inte
    
[^152]: 用于一类事件数据的顺序对抗异常检测

    Sequential Adversarial Anomaly Detection for One-Class Event Data. (arXiv:1910.09161v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1910.09161](http://arxiv.org/abs/1910.09161)

    本文提出了一种对抗顺序检测器，使用带标记的点过程模型捕捉序列事件中的相关性，可以应用于检测异常序列，通过解决最小最大问题，针对最坏情况的生成器，找到最佳检测器。

    

    本文考虑在单类场景下的顺序异常检测问题，仅在异常序列可用时，提出了一种对抗顺序检测器，通过解决最小最大问题，针对最坏情况的生成器，找到最佳检测器。生成器使用带标记的点过程模型捕捉序列事件中的相关性。检测器顺序评估测试序列的可能性，并将其与学习自最小最大问题的时间变化阈值进行比较。我们在模拟和专有的大规模信用卡欺诈数据集上展示了提出方法的良好性能。该方法通常适用于检测异常序列。

    We consider the sequential anomaly detection problem in the one-class setting when only the anomalous sequences are available and propose an adversarial sequential detector by solving a minimax problem to find an optimal detector against the worst-case sequences from a generator. The generator captures the dependence in sequential events using the marked point process model. The detector sequentially evaluates the likelihood of a test sequence and compares it with a time-varying threshold, also learned from data through the minimax problem. We demonstrate our proposed method's good performance using numerical experiments on simulations and proprietary large-scale credit card fraud datasets. The proposed method can generally apply to detecting anomalous sequences.
    

