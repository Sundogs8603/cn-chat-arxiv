# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Reasoning in Large Language Models Through Symbolic Math Word Problems.](http://arxiv.org/abs/2308.01906) | 本文通过研究数学问题的符号化版本来解决大型语言模型（LLMs）的推理能力问题，该方法探索了一种自我提示的方法，鼓励符号化推理与数值答案保持一致。 |
| [^2] | [Revisiting Deformable Convolution for Depth Completion.](http://arxiv.org/abs/2308.01905) | 本文通过重新思考可变形卷积的思想，提出了一种以可变形核卷积作为一次通过的完善模块的有效架构，并经验性地证明了其优越性。研究表明，可变形卷积在深度完成中具有重要作用。 |
| [^3] | [How many preprints have actually been printed and why: a case study of computer science preprints on arXiv.](http://arxiv.org/abs/2308.01899) | 这个研究通过计算机科学预印本在arXiv上的案例研究，量化了有多少预印本最终被印刷在同行评审的期刊上。为了解决预印本和最终发表版本的对应问题，引入了基于语义的映射方法使用BERT。 |
| [^4] | [Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning.](http://arxiv.org/abs/2308.01895) | 本文研究了改进持续学习中回放样本选择和存储的方法，通过对蓄水池抽样和其他替代策略进行比较，并提供了寻找最佳存储样本数量的详细分析。 |
| [^5] | [Exact identification of nonlinear dynamical systems by Trimmed Lasso.](http://arxiv.org/abs/2308.01891) | 本文通过修整的Lasso（TRIM）方法，提出了一种能够处理噪声、有限数据和多重共线性等问题的稳健识别非线性动力系统的方法。 |
| [^6] | [DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations.](http://arxiv.org/abs/2308.01890) | DualCoOp++是一种快速有效的方法，用于在有限注释情况下进行多标签识别。它通过利用强大的文本和视觉特征对齐，采用Evidence-guided Dual Context Optimization框架来解决部分标签和零样本多标签识别的问题。 |
| [^7] | [Multi-variable Hard Physical Constraints for Climate Model Downscaling.](http://arxiv.org/abs/2308.01868) | 该研究探讨了气候模式降尺度中多变量之间的物理约束问题，并通过应用于温度的案例，提出了一种引入多变量硬约束的框架，确保了降尺度的气候变量群之间的物理关系。 |
| [^8] | [MRQ:Support Multiple Quantization Schemes through Model Re-Quantization.](http://arxiv.org/abs/2308.01867) | 该论文提出了一种名为MRQ的模型重新量化方法，通过将现有的量化模型快速转换以满足不同的量化要求，解决了在固定点硬件上部署深度学习模型的挑战。 |
| [^9] | [Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory.](http://arxiv.org/abs/2308.01853) | 这篇论文研究了统计估计中的分布偏移问题，主要关注Wasserstein分布偏移，提出了联合分布偏移概念，并分析了几个统计问题的解决方法。论文发现了最优的极小极大风险和最不利的扰动，并证明了样本均值和最小二乘估计量的优越性。 |
| [^10] | [Curricular Transfer Learning for Sentence Encoded Tasks.](http://arxiv.org/abs/2308.01849) | 本文提出了一种课程迁移学习的方法，通过数据入侵和语法分析引导，逐步适应预训练分布，并在MultiWoZ任务中取得了显着的改进。 |
| [^11] | [URET: Universal Robustness Evaluation Toolkit (for Evasion).](http://arxiv.org/abs/2308.01840) | 本论文提出了一个名为URET的通用鲁棒性评估工具包，该工具包可以生成各种类型和任务领域下的对抗性输入，以确保关键AI任务的安全和鲁棒性。 |
| [^12] | [Distribution-Free Inference for the Regression Function of Binary Classification.](http://arxiv.org/abs/2308.01835) | 本文提出了一种分布无关的方法来推断二元分类问题中的回归函数，通过构建置信区间来解决该问题，相关算法经过验证具有可靠性。 |
| [^13] | [The Capability of Large Language Models to Measure Psychiatric Functioning.](http://arxiv.org/abs/2308.01834) | 本研究调查了大型语言模型在通过患者采访和临床描述预测精神功能方面的能力，结果显示这些模型在预测抑郁症评分方面表现最好，与人类临床评估者的结果相近，揭示了通用临床语言模型在评估精神状况方面的潜力。 |
| [^14] | [Hard Adversarial Example Mining for Improving Robust Fairness.](http://arxiv.org/abs/2308.01823) | 该研究针对对抗训练模型容易出现不公平问题的限制，提出了通过自适应的困难对抗样本挖掘来改善深度神经网络的鲁棒性和公平性。 |
| [^15] | [Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit.](http://arxiv.org/abs/2308.01814) | 本论文研究了在无限宽度极限中使用自适应优化器训练宽神经网络时的新现象，通过推导相应的“神经切线”和“最大更新”极限，展示了特征学习和核行为之间的二分法，同时引入了简化计算的bra-ket符号。 |
| [^16] | [Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach.](http://arxiv.org/abs/2308.01797) | 这篇论文介绍了一种原始的深度强化学习方法，采用序列到序列的方法自动学习调度规则，适用于作业车间调度问题和其他最优作业调度任务。 |
| [^17] | [Benchmarking Adaptative Variational Quantum Algorithms on QUBO Instances.](http://arxiv.org/abs/2308.01789) | 本文通过分析三种自适应变分量子算法（EVQE, VAns, RA-VQE）填补了对不同方法的系统比较的研究空白。 |
| [^18] | [Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment.](http://arxiv.org/abs/2308.01771) | 本研究使用深度学习工具基于动脉壁横截面的空间排列和钙化信息，准确预测了动脉壁的应力和应变分布，以提高心血管风险评估的效果。 |
| [^19] | [Bag of Policies for Distributional Deep Exploration.](http://arxiv.org/abs/2308.01759) | 本论文提出了一种名为"袋子策略"的通用方法，用于分布式强化学习中的深度探索。该方法通过维护一个副本的种群，利用独立更新的头部和离线更新策略，为每个头部提供不同的学习信号，从而实现了多样化的学习和行为。 |
| [^20] | [Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants.](http://arxiv.org/abs/2308.01746) | 本文提出了一种神经崩溃终止点的统一解决方案，用于解决类别增量学习及其变体中的问题，通过固定的结构和原型演化方案来保持对新类别的可学习性，同时避免灾难性遗忘问题。 |
| [^21] | [Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning.](http://arxiv.org/abs/2308.01744) | 本文提出了一种在不可知情况下的多任务学习新方法，通过对多任务信息增益的分析，获得了新的遗憾保证，并提出了一种能够自动适应任务相似性的在线学习算法。 |
| [^22] | [Finding the Optimum Design of Large Gas Engines Prechambers Using CFD and Bayesian Optimization.](http://arxiv.org/abs/2308.01743) | 本研究使用CFD模拟结合贝叶斯优化方法，高效地寻找大型燃气发动机预燃室的最佳设计。 |
| [^23] | [Exploiting Multi-Label Correlation in Label Distribution Learning.](http://arxiv.org/abs/2308.01742) | 该论文介绍了一种利用多标签相关性的标签分布学习方法，通过在多标签学习过程中捕捉低秩标签相关性来提高学习效果，并通过实验证明了该方法优于现有的标签分布学习方法。 |
| [^24] | [MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction.](http://arxiv.org/abs/2308.01737) | 提出了一个模型无关的预训练框架，用于点击率预测，可以更好地利用多字段分类数据和大量用户点击日志，学习更广义和有效的特征和实例表示。 |
| [^25] | [Quantification of Predictive Uncertainty via Inference-Time Sampling.](http://arxiv.org/abs/2308.01731) | 该论文提出了一种后续采样策略，用于估计考虑数据模糊性的预测不确定性，可以生成不同的合理输出，并且不假设预测分布的参数形式。 |
| [^26] | [Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data.](http://arxiv.org/abs/2308.01729) | 本论文提出了一种基于联合精算神经网络框架的横断面和纵向索赔计数模型，通过结合传统精算模型和神经网络，充分利用了两个模型的优势。 |
| [^27] | [Evaluating Link Prediction Explanations for Graph Neural Networks.](http://arxiv.org/abs/2308.01682) | 本文评估了图神经网络的链路预测解释的质量，并提供了定量指标来衡量解释的质量。我们发现底层假设和技术细节对解释的质量有影响。 |
| [^28] | [Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity.](http://arxiv.org/abs/2308.01677) | 本研究探讨了基于张量核范数的约束最小化方法在低秩张量恢复中的有效性，提出了适当的严格互补性条件，并且得到了在此条件下的主要结果：1.对于特定形式的目标函数，标准投影梯度方法具有线性收敛速度，尽管目标函数不一定是强凸的。2.对于光滑的目标函数，标准梯度方法的收敛速度和每次迭代的运行时间可能显著提高。 |
| [^29] | [End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC.](http://arxiv.org/abs/2308.01674) | 本论文提出了一种用于经济非线性MPC的Koopman模型的端到端强化学习方法，旨在实现控制性能和计算需求之间的平衡。 |
| [^30] | [UniG-Encoder: A Universal Feature Encoder for Graph and Hypergraph Node Classification.](http://arxiv.org/abs/2308.01650) | UniG-Encoder是一种通用特征编码器，用于图和超图节点分类。它通过将连接节点的拓扑关系转换为边或超边特征，并结合原始节点特征，利用神经网络生成编码后的节点嵌入。 |
| [^31] | [MARLIM: Multi-Agent Reinforcement Learning for Inventory Management.](http://arxiv.org/abs/2308.01649) | 本文提出了一个名为MARLIM的新型多智能体强化学习框架，用于解决具有随机需求和交货时间的单层多产品供应链的库存管理问题，并通过数值实验证明了强化学习方法相对于传统基准方法的优势。 |
| [^32] | [Interleaving GANs with knowledge graphs to support design creativity for book covers.](http://arxiv.org/abs/2308.01626) | 本文将生成对抗网络与知识图谱相互交错应用于书籍封面设计，通过改变输入标题获得多个选项并利用鉴别器选择最佳图像，表现出比之前方法更好的生成效果和更好的选择。 |
| [^33] | [A Novel Convolutional Neural Network Architecture with a Continuous Symmetry.](http://arxiv.org/abs/2308.01621) | 本文介绍了一种新的卷积神经网络架构，通过连续的对称性修改权重，具有与传统模型不同的特点，希望将对称性作为神经网络的新期望特性，并推广将偏微分方程的视角应用于ConvNet的分析和解释。 |
| [^34] | [Assessing Systematic Weaknesses of DNNs using Counterfactuals.](http://arxiv.org/abs/2308.01614) | 通过反事实验证评估DNNs的系统性缺陷。 |
| [^35] | [Feature Noise Boosts DNN Generalization under Label Noise.](http://arxiv.org/abs/2308.01609) | 本研究提出一种特征噪声方法来增强深度神经网络在标签噪声下的泛化能力。理论上证明标签噪声会削弱泛化能力，而特征噪声通过约束模型权重和特征之间的互信息来提高泛化能力。为了确保有效泛化，我们进行了应用分析并确定了合适的特征噪声类型和水平。 |
| [^36] | [Deep Learning-based surrogate models for parametrized PDEs: handling geometric variability through graph neural networks.](http://arxiv.org/abs/2308.01602) | 本研究提出了一种使用基于深度学习的替代模型来处理参数化偏微分方程中的几何变异的策略，其中使用了图神经网络来高效演化系统。 |
| [^37] | [Unsupervised Representation Learning for Time Series: A Review.](http://arxiv.org/abs/2308.01578) | 本文对时间序列领域的无监督表示学习方法进行了综述，提出了基于ULTS库的快速实现与统一评估，并为未来的研究提供了参考。 |
| [^38] | [Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS.](http://arxiv.org/abs/2308.01573) | 本文提出了一种具有双鉴别器的语音合成模型，以进一步提升 DiffGAN-TTS 的性能。该模型利用扩散鉴别器学习逆过程分布，并利用声谱图鉴别器学习声谱图分布。 |
| [^39] | [Fast Slate Policy Optimization: Going Beyond Plackett-Luce.](http://arxiv.org/abs/2308.01566) | 本文介绍了一种快速Slate策略优化方法，通过提出一种新的策略类，可以在大规模决策系统中有效地优化任意奖励函数，结果表明该方法在百万级别动作空间问题上具有很好的效果。 |
| [^40] | [Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity.](http://arxiv.org/abs/2308.01562) | 本研究提出了一种剪枝增强的分层联邦学习（PHFL）方法，用于解决无线网络中的带宽稀缺和系统异构性问题。通过模型剪枝和无线通信的优化，实现了在严格的延迟和能耗约束下收敛速度的最优化。 |
| [^41] | [Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models.](http://arxiv.org/abs/2308.01557) | 本文提出了一种新的机器人运动学习与规划方法，通过学习扩散模型作为先验知识，可以加速运动规划优化过程。扩散模型能够在高维环境中有效地编码数据的多模态性，并可以直接从任务目标条件下的后验轨迹分布中进行采样。 |
| [^42] | [InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent.](http://arxiv.org/abs/2308.01552) | 本研究将OpenAI的ChatGPT集成到具身智能体系统中，通过将ChatGPT分配不同角色并与原始语言模型集成，实现了98%的成功率，并在实际环境中展现了ChatGPT在理解和执行复杂任务方面的能力。 |
| [^43] | [MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies.](http://arxiv.org/abs/2308.01546) | MusicLDM通过稳定扩散和AudioLDM架构，结合重新训练对比语言-音频预训练模型和Hifi-GAN声码器，以解决音乐生成中的挑战。通过节奏同步的混合策略，对训练数据进行增广，提高新颖性并避免抄袭问题。 |
| [^44] | [Lode Enhancer: Level Co-creation Through Scaling.](http://arxiv.org/abs/2308.01543) | 本文探索了使用AI增强的上采样作为设计辅助工具，在2D游戏关卡设计中实现协同创作。我们使用深度神经网络来将人工降低分辨率的关卡片段上采样，并为此引入了一种能够学习上采样并处理不太常见图块的神经网络架构。经过与设计师的合作研究，我们发现设计师们喜欢这个工具的共同设计过程，并认为它具有潜力推动更多的开发工作。 |
| [^45] | [MFIM: Megapixel Facial Identity Manipulation.](http://arxiv.org/abs/2308.01536) | MFIM是一种新颖的人脸交换框架，它能够生成高质量的百万像素图像，并且能够有效地将给定图像的身份转换为另一个人的身份，同时保留身份无关的属性。 |
| [^46] | [Circumventing Concept Erasure Methods For Text-to-Image Generative Models.](http://arxiv.org/abs/2308.01508) | 本论文研究了绕过文本到图像生成模型中概念删除方法的问题，发现这些方法无法完全删除目标概念，并对其使用的脆弱性提出了质疑。 |
| [^47] | [Minimax Optimal $Q$ Learning with Nearest Neighbors.](http://arxiv.org/abs/2308.01490) | 本文提出了两种新的具有最近邻的$Q$学习方法，用于解决连续状态空间下收敛速度差的问题。 |
| [^48] | [Efficient neural supersampling on a novel gaming dataset.](http://arxiv.org/abs/2308.01483) | 该论文介绍了一种新颖的神经算法，用于在游戏内容上进行超采样，比现有方法高效4倍，同时保持相同的精度水平。此外，还引入了一个新的数据集，填补了当前数据集领域的空白，并可以帮助推动游戏内容超分辨率技术的最新进展。 |
| [^49] | [Online covariance estimation for stochastic gradient descent under Markovian sampling.](http://arxiv.org/abs/2308.01481) | 本文研究了在马尔可夫采样下的随机梯度下降中的在线重叠批次均值协方差估计器，并证明了其收敛速率为$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$和$O\big(\sqrt{d}\,n^{-1/8}\big)$，分别对应于状态相关和状态无关的马尔可夫采样。这些速率与独立同分布情况下的最佳收敛速率相匹配，并且克服了由于马尔可夫采样而引起的挑战。 |
| [^50] | [Interpretable Machine Learning for Discovery: Statistical Challenges \& Opportunities.](http://arxiv.org/abs/2308.01475) | 可解释的机器学习技术被广泛用于处理大数据集、可视化预测和数据驱动的发现，该论文回顾了这一领域并探讨了验证发现的挑战。 |
| [^51] | [Reverse Stable Diffusion: What prompt was used to generate this image?.](http://arxiv.org/abs/2308.01472) | 本论文介绍了一种新的任务，即在给定由生成扩散模型生成的图像的情况下预测文本提示。为了解决这个问题，作者结合了多种白盒和黑盒模型，提出了一个新颖的学习框架，该框架能够生成改进的提示，并采用课程学习和无监督领域自适应核学习方法来进一步提高方法的性能。 |
| [^52] | [Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving.](http://arxiv.org/abs/2308.01471) | 该论文提出了一种统一的感知和未来预测方法，利用隐式占用流场来表示自动驾驶车辆对周围环境的感知并预测其他交通参与者的行为。这种方法避免了不必要的计算和信息丢失的问题。 |
| [^53] | [VertexSerum: Poisoning Graph Neural Networks for Link Inference.](http://arxiv.org/abs/2308.01469) | VertexSerum是一种针对链路推理的图神经网络毒化攻击，通过放大链接连接性泄漏来增加图链接窃取的效果，并提出了一种可以嵌入到链接检测网络中的注意力机制。在实验中，VertexSerum在四个真实世界数据集和三种不同的GNN结构上平均提高了9.8％的AUC分数，且在黑盒和在线学习环境中均表现出有效性。 |
| [^54] | [A digital twin framework for civil engineering structures.](http://arxiv.org/abs/2308.01445) | 本研究提出了一个用于土木工程结构的预测数字孪生方法，它采用概率图模型编码资产孪生耦合动态系统，通过深度学习模型同化感测数据来提供实时的结构健康诊断，不断更新数字孪生状态并用于优化维护和管理规划。 |
| [^55] | [Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations.](http://arxiv.org/abs/2308.01438) | 本研究提出了六种新颖的基于物理学的机器学习模型，用于准确近似室内污染物浓度。所提出的模型结合了物理学中的状态空间概念、门控循环单元和分解技术。通过在加利福尼亚州一栋商业建筑中五个办公室收集的数据进行验证，结果表明所提出的模型具有较低的复杂度。 |
| [^56] | [Price-Aware Deep Learning for Electricity Markets.](http://arxiv.org/abs/2308.01436) | 本文提出了一种价格感知深度学习方法，通过将电力市场清算优化嵌入到深度学习层中，平衡预测误差和定价误差，提高公平性，并控制价格误差的空间分布。 |
| [^57] | [COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography.](http://arxiv.org/abs/2308.01433) | COVID-VR是一种使用体积渲染图像识别肺部疾病的深度学习模型。相较于其他方法，COVID-VR在整个肺部提供了综合视图，并能有效识别肺部病变。 |
| [^58] | [ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks.](http://arxiv.org/abs/2308.01423) | ChatMOF是一种自主AI系统，用于预测和生成金属-有机骨架。通过利用大规模语言模型，它能够从文本输入中提取关键细节，并提供适当的回应。该系统通过组合代理、工具包和评估器的核心组件，实现了数据检索、性质预测和结构生成等多个任务。研究进一步展示了在材料科学中使用大型语言模型的优势和潜力。 |
| [^59] | [Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting.](http://arxiv.org/abs/2308.01421) | 这项工作提出了一种处理神经网络泛化和过拟合的新方法，通过正则化损失函数和提前停止策略来优化网络参数，并通过数值实验验证了该方法的有效性。 |
| [^60] | [SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text.](http://arxiv.org/abs/2308.01420) | SAP-sLDA提出了一种半监督人类参与的LDA方法，可以在低维投影中学习主题并保留文档之间的语义关系。 |
| [^61] | [Graph Neural Networks for Forecasting Multivariate Realized Volatility with Spillover Effects.](http://arxiv.org/abs/2308.01419) | 本研究提出了一种使用图神经网络建模和预测多变量实现波动的新方法，能够有效地捕捉非线性关系，并利用多跳邻居的溢出效应提高实现波动的预测准确性。此外，使用拟似似然损失进行训练可以显著提升模型性能。 |
| [^62] | [An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model.](http://arxiv.org/abs/2308.01415) | 本文提出了一个精心设计的数据创建流水线，通过与金融专家的对话和反馈，在大型语言模型中生成了一个高质量的金融指令数据集。实验结果表明，该方法在生成准确、相关和金融风格响应方面取得了显著进展。 |
| [^63] | [Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models.](http://arxiv.org/abs/2308.01404) | 通过引入一款名为"Hoodwinked"的文本游戏，研究了当前语言模型是否具有欺骗和识别谎言的能力。实验证据表明，杀手经常否认罪行并指责他人，导致投票结果受到影响。更先进的模型在杀手效果上表现出优势。实验证据表明，这种改进是通过在讨论中更强的欺骗能力实现的。 |
| [^64] | [Learning to Model the World with Language.](http://arxiv.org/abs/2308.01399) | 本论文提出了一种通过语言学习对世界进行建模的方法，利用语言帮助代理器预测未来并进行行动。通过学习多模态世界模型，代理器可以预测未来的文本和图像表示，并在模型回滚中进行行动。 |
| [^65] | [OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models.](http://arxiv.org/abs/2308.01390) | OpenFlamingo是一个开源框架，用于训练大型自回归视觉语言模型。它在多个数据集上表现良好，达到了对应模型性能的80%至89%。 |
| [^66] | [Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning.](http://arxiv.org/abs/2308.01389) | 本论文的主要创新是通过优化的单次多框检测和强化学习追踪士兵，实现了使用DeepRacer构建一个自主系统。通过使用SSD Lite替代SSD，我们在推理速度上取得了显著提升，而准确性并未受到影响。 |
| [^67] | [Computational Long Exposure Mobile Photography.](http://arxiv.org/abs/2308.01379) | 本文提出了一种计算爆发摄影系统，可以在手持智能手机相机应用中实现长曝光摄影的效果，包括前景模糊和背景模糊。通过检测和分割显著主体，并跟踪场景中的动态元素，系统能够自动合成出令人惊叹的图像。 |
| [^68] | [Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE.](http://arxiv.org/abs/2308.01362) | 该论文介绍了一种可解释的深度学习方法，使用神经-ODE进行肿瘤动力建模和整体生存预测。该方法能够从截断数据中进行无偏预测，并提供了一种融合多模态数据的有原则的方式。 |
| [^69] | [Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning.](http://arxiv.org/abs/2308.01358) | 本文研究了压缩对分布式和联邦学习中随机梯度算法的影响，通过比较不同的无偏压缩操作符的收敛速度，超越了经典的最坏情况分析。针对最小二乘回归，我们提出了一个随机逼近算法，并考虑了随机场的一般假设和噪声协方差的限制，以分析各种随机化机制。 |
| [^70] | [EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding.](http://arxiv.org/abs/2308.01329) | 本文提出了一种用于嵌入学习的层次探索算法EmbeddingTree和相应的可视化工具，能够解释具有语义的实体特征和嵌入向量之间的关联，并通过实验验证了其有效性。 |
| [^71] | [Careful Whisper -- leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification.](http://arxiv.org/abs/2308.01327) | 本文提出了一种利用自动语音识别对失语症亚型进行分类的方法，通过结合不同模型的技术，能够利用声音录音自动识别并评估言语障碍。该方法在区分失语症患者和健康对照组的录音时表现出与人类水平相近的准确性，并且可以以较高的准确率区分最常见的失语症类型。该方法还可以应用于其他疾病和语言，并有望稳健地提取诊断性的言语生物标志。 |
| [^72] | [Evaluation of network-guided random forest for disease gene discovery.](http://arxiv.org/abs/2308.01323) | 本论文评估了网络引导的随机森林在疾病基因发现中的应用，结果表明在疾病预测方面与标准随机森林相比并无优势，但在疾病基因发现方面，网络引导的随机森林能更准确地识别基因模块，但对于中心基因可能存在虚假选择的情况。 |
| [^73] | [DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales.](http://arxiv.org/abs/2308.01320) | DeepSpeed-Chat是一个新颖的系统，使得ChatGPT-like模型的RLHF培训易于访问，高效且经济实惠。它具有易于使用的训练和推断体验，复制了InstructGPT的训练流程，并集成了各种训练和推断优化，提供了无与伦比的效率和可扩展性。 |
| [^74] | [Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges.](http://arxiv.org/abs/2308.01319) | 本文综述了近年来计算机辅助诊断领域在疾病诊断方面使用机器学习的最新进展，探讨了机器学习算法对于疾病检测和诊断的重要性和应用，以及其在分析生物医学数据方面的优势和挑战。 |
| [^75] | [An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning.](http://arxiv.org/abs/2308.00788) | 本论文介绍了双层优化在信号处理和机器学习中的基本概念和应用。双层优化是一个经典的优化问题，涉及到两个层次的优化，并在建模问题中展现了强大的能力。它在无线系统资源分配和对抗性机器学习等领域有广泛的应用。 |
| [^76] | [On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey.](http://arxiv.org/abs/2307.16680) | 本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。 |
| [^77] | [An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid.](http://arxiv.org/abs/2307.16149) | 这篇论文提出了一种利用LSTM和DDPM相结合的方案来解决智能电网系统中的能量盗窃检测和预测问题。通过重构和预测误差，系统能够准确识别能量盗窃的实例，并在实验中表现出较好的性能。 |
| [^78] | [Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search.](http://arxiv.org/abs/2307.13831) | 这项研究分析了使用Armijo线搜索的随机梯度下降非凸优化中批大小和步数的关系，并发现随着批大小的增加，所需的步数减少。 |
| [^79] | [Model Calibration in Dense Classification with Adaptive Label Perturbation.](http://arxiv.org/abs/2307.13539) | 本文提出了一种自适应标签扰动的模型校准方法，使用自校准二进制交叉熵损失来统一不同形式的标签扰动过程。该方法通过最大化预测熵来改善模型校准，并在保持分类准确性的同时纠正校准问题。 |
| [^80] | [Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities.](http://arxiv.org/abs/2307.10803) | 该研究总结了海洋科学领域的时空数据挖掘研究，包括广泛使用的ST海洋数据集和其特点，以及ST海洋数据质量增强技术和现有STDM分类。 |
| [^81] | [AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System.](http://arxiv.org/abs/2307.04577) | AnyTeleop是一个通用的视觉导向的远程操作系统，支持多个不同的机械臂、手部、环境和摄像头配置，在实际实验和模拟中表现出色。 |
| [^82] | [Nearest Neighbour with Bandit Feedback.](http://arxiv.org/abs/2306.13773) | 本论文提出一种新的最近邻算法，能够应用于上下文Bandit问题并处理完全对抗的设置，具有高效运行、快速搜索和准线性空间的优点。 |
| [^83] | [An efficient, provably exact algorithm for the 0-1 loss linear classification problem.](http://arxiv.org/abs/2306.12344) | 该研究详细介绍了一种名为增量单元枚举（ICE）的算法，该算法可以精确解决定维度0-1损失线性分类问题。 |
| [^84] | [Masked Diffusion Models Are Fast and Privacy-Aware Learners.](http://arxiv.org/abs/2306.11363) | 该论文提出了一种基于先验的去噪训练框架，通过遮蔽学习和扩散模型的结合，实现了更高效的训练和生成更高质量的图像。 |
| [^85] | [Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction.](http://arxiv.org/abs/2306.10045) | 本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。 |
| [^86] | [Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models.](http://arxiv.org/abs/2306.05357) | 本论文提出了一种无监督的方法，用于从图像中自动地发现不同的生成概念，并且这些生成概念可以被用于重新组合和生成新的艺术和混合图像，并作为一种表示用于下游的分类任务。 |
| [^87] | [From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module.](http://arxiv.org/abs/2305.16174) | 该论文研究了一种从潜在图到潜在拓扑推断的方法，通过引入可微分的单复形模块（DCM），学习描述数据点之间多向交互的高阶单复形的稀疏且不规则的拓扑结构，并展示了如何将其与单复形消息传递网络层集成以提高下游任务的效果。 |
| [^88] | [Variational Classification.](http://arxiv.org/abs/2305.10406) | 提出一种新的变分分类方法，通过引入潜变量建模来优化训练，允许灵活的设计选择以改善校准和对抗鲁棒性，实验结果表明其对于域外数据的分类准确性得到了保持。 |
| [^89] | [Mlinear: Rethink the Linear Model for Time-series Forecasting.](http://arxiv.org/abs/2305.04800) | 该论文重新思考了时间序列预测的线性模型，提出了Mlinear方法，通过动态调节通道独立性和通道依赖性属性以实现更好的预测性能。 |
| [^90] | [Classification and Online Clustering of Zero-Day Malware.](http://arxiv.org/abs/2305.00605) | 本文研究了零日恶意软件的分类和在线聚类。实验使用 EMBER 数据集，对有流入的恶意软件样本进行了分类，得到了 95.33% 的平衡准确度。在剩下的数据中，使用自组织映射实现了纯度从 47.61% 到 77.68% 的聚类。 |
| [^91] | [Morphological Classification of Extragalactic Radio Sources Using Gradient Boosting Methods.](http://arxiv.org/abs/2304.12729) | 本文提出了基于梯度提升机器学习方法和主成分分析的自动分类方法，用于解决星系外射电源形态分类问题。实验结果表明，在具有表格数据的分类问题中，该方法优于卷积神经网络。 |
| [^92] | [Reconstructing Turbulent Flows Using Physics-Aware Spatio-Temporal Dynamics and Test-Time Refinement.](http://arxiv.org/abs/2304.12130) | 本文提出了一种物理引导的神经网络方法，用于从低分辨率LES数据重建连续的DNS，可有效解决湍流流场的时空复杂性。 |
| [^93] | [Model sparsification can simplify machine unlearning.](http://arxiv.org/abs/2304.04934) | 本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。 |
| [^94] | [OpenAGI: When LLM Meets Domain Experts.](http://arxiv.org/abs/2304.04370) | 基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。 |
| [^95] | [Computer Vision Estimation of Emotion Reaction Intensity in the Wild.](http://arxiv.org/abs/2303.10741) | 本研究旨在利用计算机视觉模型在野外环境中估计情绪反应强度。通过训练深度神经网络和多模态模型，我们在Hume-Reaction数据集上取得了不错的结果。 |
| [^96] | [Causal Discovery from Temporal Data: An Overview and New Perspectives.](http://arxiv.org/abs/2303.10112) | 本文对于从时间数据中进行因果关系发现进行了综述，提出了两个相关的类别，即多元时间序列因果发现和事件序列因果发现，并提供了新的方法来综合考虑这两个类别。 |
| [^97] | [Exploiting 4D CT Perfusion for segmenting infarcted areas in patients with suspected acute ischemic stroke.](http://arxiv.org/abs/2303.08757) | 该研究提出了一种新颖的方法，通过利用四维CTP全面利用时空信息，以分割疑似急性缺血性卒中患者的梗死区。实验证明，该方法明显优于现有的最先进方法，有潜力为改善AIS患者的早期诊断和治疗规划的准确性和效率做出贡献。 |
| [^98] | [SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model.](http://arxiv.org/abs/2303.05118) | SLCA是一种用于连续学习的简单但极其有效的方法。它通过慢学习和分类器对齐来在预训练模型上提高泛化能力和解决渐进过拟合问题。 |
| [^99] | [Optimal Training of Mean Variance Estimation Neural Networks.](http://arxiv.org/abs/2302.08875) | 本文研究了均方差估计网络的最优实现，并发现通过使用预热期可以避免收敛困难。 |
| [^100] | [Merging satellite and gauge-measured precipitation using LightGBM with an emphasis on extreme quantiles.](http://arxiv.org/abs/2302.03606) | 本文将应用机器学习算法将卫星和测站降水数据合并，重点关注极值量化。通过将观测降水作为因变量，卫星数据作为预测变量，以提高降水估计精度。 |
| [^101] | [Matrix Estimation for Individual Fairness.](http://arxiv.org/abs/2302.02096) | 本文研究了个体公平性(IF)和矩阵估计(ME)之间的联系。结果表明，在适当条件下使用ME方法进行数据预处理可以改善算法的个体公平性，并且不会牺牲性能。 |
| [^102] | [Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models.](http://arxiv.org/abs/2301.06267) | 通过跨模态适应方法，在多模态模型下利用少样本示例（包括文本和声音）进行狗的视觉分类，并取得了最先进的结果。 |
| [^103] | [Learning from Data Streams: An Overview and Update.](http://arxiv.org/abs/2212.14720) | 这篇文章探讨了在数据流上的机器学习任务的定义和设置存在的问题，针对这些问题，提出了重新构思监督数据流学习的基本定义和设置，并重新考虑了一些关于数据流学习的基本假设。 |
| [^104] | [Automatically Bounding the Taylor Remainder Series: Tighter Bounds and New Applications.](http://arxiv.org/abs/2212.11429) | 本文提出了一种自动计算泰勒余项级数的算法，可以提供更紧密的界限，并应用于区间计算、优化等领域。 |
| [^105] | [A Neural Network Warm-Start Approach for the Inverse Acoustic Obstacle Scattering Problem.](http://arxiv.org/abs/2212.08736) | 本文提出了一种神经网络热启动方法，用于解决声学障碍物散射问题的逆问题。该方法通过在计算散射场和给定测量数据之间的$L^2$距离的区域边界中找到良好的初始猜测，以克服计算上的挑战。 |
| [^106] | [An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection.](http://arxiv.org/abs/2212.03188) | 本论文提出了一种无监督机器学习方法，用于提取地震动谱的定义特征，以辅助地震动选择。它结合了机器发现的潜在特征和传统强度测量，通过聚类分析选择代表性的地震动记录。验证结果表明该方法的有效性。 |
| [^107] | [No Agreement Without Loss: Learning and Social Choice in Peer Review.](http://arxiv.org/abs/2211.02144) | 在本文中，我们挑战了Nothigattu、Shah和Procaccia的框架，该框架旨在通过最小化损失函数来聚合评审人的映射，我们发现了一些负面结果。 |
| [^108] | [Hebbian Deep Learning Without Feedback.](http://arxiv.org/abs/2209.11883) | 本文提出了无反馈的Hebbian深度学习算法，通过避免传统方法中的反馈信号，实现了高效性、生物兼容性和准确性的同时提升。 |
| [^109] | [A Missing Value Filling Model Based on Feature Fusion Enhanced Autoencoder.](http://arxiv.org/abs/2208.13495) | 本文提出了一种基于特征融合增强自编码器的缺失值填充模型，通过引入去轨迹神经元和径向基神经元的隐藏层，实现了同时学习相关特征和共同特征的能力。 |
| [^110] | [UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering.](http://arxiv.org/abs/2208.11435) | 本文提出了UniCon方法，用于解决多客户VQA任务的保密性约束和客户有限标记训练数据的问题。该方法通过模型共享学习跨模态表示，采用分裂学习架构确保隐私。 |
| [^111] | [Collaborative causal inference on distributed data.](http://arxiv.org/abs/2208.07898) | 提出了一种数据协作准实验（DC-QE）方法，可以在保护隐私的前提下对分布式数据进行因果推断。通过共享中间表示而不是私有数据，估计倾向分数和处理效应，能够减少随机误差和偏差，相比现有方法有更好的估计结果。 |
| [^112] | [ProMix: Combating Label Noise via Maximizing Clean Sample Utility.](http://arxiv.org/abs/2207.10276) | 论文提出了一种名为ProMix的新颖LNL框架，通过最大化干净样本的效用来对抗标签噪声。采用一种匹配高置信度选择技术来动态扩展基础干净样本集，并设计了一种平衡和无偏的SSL框架来提高性能。 |
| [^113] | [Distributed Online Private Learning of Convex Nondecomposable Objectives.](http://arxiv.org/abs/2206.07944) | 该论文提出了一种分布式在线隐私学习的算法框架DPSDA，通过差分私有方式和对偶平均法来处理具有隐私和时间变化特性的约束在线学习问题，并提出了两种算法DPSDA-C和DPSDA-PS，其分别采用了循环式通信和具有噪声干扰梯度的对偶更新策略。 |
| [^114] | [Fairness in Recommendation: Foundations, Methods and Applications.](http://arxiv.org/abs/2205.13619) | 这篇论文对推荐系统中的公平性问题进行了系统调查，针对推荐过程中可能出现的数据或算法偏见，提供了一些方法和应用来提升推荐中的公平性。 |
| [^115] | [Confident Neural Network Regression with Bootstrapped Deep Ensembles.](http://arxiv.org/abs/2202.10903) | 本文提出了一种称为Bootstrapped Deep Ensembles的新方法，通过引入经典的有限数据效应，明确考虑神经网络回归中的不确定性，并通过实验证明了该方法的显著改进。 |
| [^116] | [Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition.](http://arxiv.org/abs/2202.07901) | 本文提出了一种在在线手写识别中使用三元组损失函数的辅助跨模态表示学习方法，通过最小化模态嵌入之间的距离，利用图像分类任务的附加信息，提高了时间序列分类任务的准确性。 |
| [^117] | [Relational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship.](http://arxiv.org/abs/2112.15402) | 本文提出了关系经验回放（RER）的方法来进行持续学习，通过自适应调整任务之间的关系和样本重要性，以平衡稳定性和可塑性的要求，从而减轻了灾难性遗忘问题并积累新知识。 |
| [^118] | [Sharing to learn and learning to share -- Fitting together Meta-Learning, Multi-Task Learning, and Transfer Learning: A meta review.](http://arxiv.org/abs/2111.12146) | 本文是一篇对Meta-Learning，Multi-Task Learning和Transfer Learning的元回顾，总结了这些学习范例的特点及其比较分析。通过将这些技术结合起来，可以解决各种不同领域的问题。 |
| [^119] | [Successor Feature Neural Episodic Control.](http://arxiv.org/abs/2111.03110) | 本文研究了记忆控制和后继特征两个框架的整合，通过结合这两种方法，提高了强化学习的样本效率和策略重用的优雅性。 |
| [^120] | [How to Evaluate Uncertainty Estimates in Machine Learning for Regression?.](http://arxiv.org/abs/2106.03395) | 本文研究了如何评估机器学习回归中的不确定性估计，发现目前的评估方法存在严重缺陷，无法准确评估估计质量和预测区间的关系。 |
| [^121] | [Random Planted Forest: a directly interpretable tree ensemble.](http://arxiv.org/abs/2012.14563) | 提出了一种名为"随机种植森林"的算法，通过修改随机森林算法，保留切分后的某些叶子，形成非二进制树，实现直接可解释的树集算法。该算法具有较好的预测和可视化特性。 |
| [^122] | [ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation.](http://arxiv.org/abs/2011.11233) | ROME是一种鲁棒的内存高效的NAS方法，通过拓扑解耦和梯度累积解决了单路径DARTS中的性能下降问题。 |
| [^123] | [Stable and consistent density-based clustering via multiparameter persistence.](http://arxiv.org/abs/2005.09048) | 这篇论文通过引入一种度量层次聚类的对应交错距离，研究了一种稳定一致的密度-based聚类算法，提供了一个从一参数层次聚类中提取单个聚类的算法，并证明了该算法的一致性和稳定性。 |
| [^124] | [RAB: Provable Robustness Against Backdoor Attacks.](http://arxiv.org/abs/2003.08904) | 本文提出了一种证实机器学习模型鲁棒性的统一框架，通过随机平滑技术实现对规避和后门攻击的鲁棒性。同时，我们提出了鲁棒训练过程RAB，并证明其有效性和紧密性。在理论上证明了对后门攻击进行鲁棒性保护的可行性。 |

# 详细

[^1]: 大型语言模型通过符号化数学问题进行推理

    Reasoning in Large Language Models Through Symbolic Math Word Problems. (arXiv:2308.01906v1 [cs.CL])

    [http://arxiv.org/abs/2308.01906](http://arxiv.org/abs/2308.01906)

    本文通过研究数学问题的符号化版本来解决大型语言模型（LLMs）的推理能力问题，该方法探索了一种自我提示的方法，鼓励符号化推理与数值答案保持一致。

    

    大型语言模型（LLMs）通过解决几乎没有标记数据的下游任务，改变了自然语言处理（NLP）的方式。尽管它们具有多功能的能力，但对它们的推理能力的问题仍然不太清楚。本文通过研究数学问题的符号化版本来解决数学问题的推理问题，因为符号表达是对数值答案的“简明解释”。我们创建并使用了SVAMP数据集的符号化版本，并发现GPT-3的davinci-002模型在符号化数学问题上也具有良好的零样本准确性。为了评估模型推理的准确性，我们不仅考虑准确率，还评估最终答案和推理结果之间的一致性，分别对应于数值和符号化答案的数学问题。我们探索了一种自我提示的方法，鼓励符号化推理与数值答案保持一致，从而使LLM能够提供简明且可验证的推理。

    Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a "concise explanation" of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and
    
[^2]: 重新思考深度完成的可变形卷积

    Revisiting Deformable Convolution for Depth Completion. (arXiv:2308.01905v1 [cs.CV])

    [http://arxiv.org/abs/2308.01905](http://arxiv.org/abs/2308.01905)

    本文通过重新思考可变形卷积的思想，提出了一种以可变形核卷积作为一次通过的完善模块的有效架构，并经验性地证明了其优越性。研究表明，可变形卷积在深度完成中具有重要作用。

    

    深度完成旨在从稀疏深度图生成高质量的密集深度图，近年来引起了越来越多的关注。以往的工作通常使用RGB图像作为引导，并引入迭代的空间传播来完善估计的粗糙深度图。然而，大多数传播完善方法需要多次迭代，并且存在固定的感受野，可能包含与非常稀疏输入无关和无用的信息。本文通过重新思考可变形卷积的思想，同时解决了这两个挑战。我们提出了一种有效的架构，利用可变形核卷积作为一次通过的完善模块，并经验性地证明了其优越性。为了更好地理解可变形卷积的功能并将其应用于深度完成，我们进一步系统地研究了各种代表性的策略。我们的研究揭示了与以往工作不同，可变形卷积在深度完成中的作用。

    Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years. Previous work usually employs RGB images as guidance, and introduces iterative spatial propagation to refine estimated coarse depth maps. However, most of the propagation refinement methods require several iterations and suffer from a fixed receptive field, which may contain irrelevant and useless information with very sparse input. In this paper, we address these two challenges simultaneously by revisiting the idea of deformable convolution. We propose an effective architecture that leverages deformable kernel convolution as a single-pass refinement module, and empirically demonstrate its superiority. To better understand the function of deformable convolution and exploit it for depth completion, we further systematically investigate a variety of representative strategies. Our study reveals that, different from prior work, deformable convol
    
[^3]: 有多少预印本实际上被印刷了，以及为什么：计算机科学预印本在arXiv上的案例研究

    How many preprints have actually been printed and why: a case study of computer science preprints on arXiv. (arXiv:2308.01899v1 [cs.DL])

    [http://arxiv.org/abs/2308.01899](http://arxiv.org/abs/2308.01899)

    这个研究通过计算机科学预印本在arXiv上的案例研究，量化了有多少预印本最终被印刷在同行评审的期刊上。为了解决预印本和最终发表版本的对应问题，引入了基于语义的映射方法使用BERT。

    

    预印本在学术界扮演着越来越重要的角色。研究人员在正式提交到期刊或会议之前将他们的手稿发布到预印本服务器上的原因有很多，但预印本的使用也引发了不少争议，尤其是与优先权的声明有关。本文对2008年至2017年期间提交到arXiv的计算机科学预印本进行了案例研究，以量化最终有多少预印本在同行评审的场合中被印刷。在这些已发表的手稿中，有些以不同的标题发表，且未更新arXiv上的预印本。对于这些手稿，传统的模糊匹配方法无法将预印本与最终发表版本对应起来。鉴于这个问题，我们引入了一种基于语义的映射方法，利用Transformers中的Bidirectional Encoder Representations (BERT)。利用这种新的映射方法和多种数据来源...

    Preprints play an increasingly critical role in academic communities. There are many reasons driving researchers to post their manuscripts to preprint servers before formal submission to journals or conferences, but the use of preprints has also sparked considerable controversy, especially surrounding the claim of priority. In this paper, a case study of computer science preprints submitted to arXiv from 2008 to 2017 is conducted to quantify how many preprints have eventually been printed in peer-reviewed venues. Among those published manuscripts, some are published under different titles and without an update to their preprints on arXiv. In the case of these manuscripts, the traditional fuzzy matching method is incapable of mapping the preprint to the final published version. In view of this issue, we introduce a semantics-based mapping method with the employment of Bidirectional Encoder Representations from Transformers (BERT). With this new mapping method and a plurality of data sou
    
[^4]: 改进回放样本选择和存储以减少持续学习中的遗忘问题

    Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning. (arXiv:2308.01895v1 [cs.LG])

    [http://arxiv.org/abs/2308.01895](http://arxiv.org/abs/2308.01895)

    本文研究了改进持续学习中回放样本选择和存储的方法，通过对蓄水池抽样和其他替代策略进行比较，并提供了寻找最佳存储样本数量的详细分析。

    

    持续学习旨在使深度学习者能够在一系列未知长度的任务上进行训练，而不会遗忘以前的任务。其中一种有效的解决方案是回放，即将少量先前的经验存储在内存中，并在学习当前任务时重新播放它们。然而，在选择最有信息量的样本进行存储以及确定所需存储样本的最佳数量方面仍有改进的空间。本研究旨在通过对常用的蓄水池抽样与各种替代方法进行比较，提供一个新颖的详细分析，以解决这些问题，并找到最佳的存储样本数。

    Continual learning seeks to enable deep learners to train on a series of tasks of unknown length without suffering from the catastrophic forgetting of previous tasks. One effective solution is replay, which involves storing few previous experiences in memory and replaying them when learning the current task. However, there is still room for improvement when it comes to selecting the most informative samples for storage and determining the optimal number of samples to be stored. This study aims to address these issues with a novel comparison of the commonly used reservoir sampling to various alternative population strategies and providing a novel detailed analysis of how to find the optimal number of stored samples.
    
[^5]: 通过修整的Lasso精确识别非线性动力系统

    Exact identification of nonlinear dynamical systems by Trimmed Lasso. (arXiv:2308.01891v1 [cs.LG])

    [http://arxiv.org/abs/2308.01891](http://arxiv.org/abs/2308.01891)

    本文通过修整的Lasso（TRIM）方法，提出了一种能够处理噪声、有限数据和多重共线性等问题的稳健识别非线性动力系统的方法。

    

    非线性动力系统的识别已经被广泛应用于通过顺序阈值最小二乘（STLS）算法进行非线性动力学的稀疏识别（SINDy）。许多SINDy的扩展在文献中出现，以处理长度有限且噪音较大的实验数据。最近，提出了集成引导SINDy模型（E-SINDy）的计算密集型方法来进行模型识别，处理有限且噪音较大的数据。虽然SINDy的扩展很多，但它们促进稀疏估计的估计器有时会提供动态的稀疏近似，而不是精确恢复。此外，这些估计器在多重共线性下存在问题，例如Lasso的不可表示条件。在本文中，我们证明修剪的Lasso用于对模型进行稳健识别（TRIM）可以在较严重的噪声、有限数据和多重共线性下提供精确恢复，而不同于E-SINDy。

    Identification of nonlinear dynamical systems has been popularized by sparse identification of the nonlinear dynamics (SINDy) via the sequentially thresholded least squares (STLS) algorithm. Many extensions SINDy have emerged in the literature to deal with experimental data which are finite in length and noisy. Recently, the computationally intensive method of ensembling bootstrapped SINDy models (E-SINDy) was proposed for model identification, handling finite, highly noisy data. While the extensions of SINDy are numerous, their sparsity-promoting estimators occasionally provide sparse approximations of the dynamics as opposed to exact recovery. Furthermore, these estimators suffer under multicollinearity, e.g. the irrepresentable condition for the Lasso. In this paper, we demonstrate that the Trimmed Lasso for robust identification of models (TRIM) can provide exact recovery under more severe noise, finite data, and multicollinearity as opposed to E-SINDy. Additionally, the computatio
    
[^6]: DualCoOp++: 针对有限注释的多标签识别进行快速有效的适应

    DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations. (arXiv:2308.01890v1 [cs.CV])

    [http://arxiv.org/abs/2308.01890](http://arxiv.org/abs/2308.01890)

    DualCoOp++是一种快速有效的方法，用于在有限注释情况下进行多标签识别。它通过利用强大的文本和视觉特征对齐，采用Evidence-guided Dual Context Optimization框架来解决部分标签和零样本多标签识别的问题。

    

    在低标签情况下进行多标签图像识别是一个极具挑战和实际意义的任务。之前的研究主要关注学习文本和视觉空间之间的对齐，以弥补图像标签有限所带来的准确性降低的问题。然而，由于高质量的多标签注释稀缺，这种方法可能会导致准确性下降。在这项研究中，我们利用了用数百万个辅助图像-文本对预训练的文本和视觉特征之间的强大对齐。我们提出了一种高效且有效的框架，称为Evidence-guided Dual Context Optimization (DualCoOp++)，它作为一种统一的方法来解决部分标签和零样本多标签识别的问题。在DualCoOp++中，我们单独对目标类别的证据、正面和负面上下文进行编码，作为语言输入（即提示）的参数组件。证据上下文旨在发现与目标类别相关的所有视觉内容，并作为聚合正面和负面上下文的指导。

    Multi-label image recognition in the low-label regime is a task of great challenge and practical significance. Previous works have focused on learning the alignment between textual and visual spaces to compensate for limited image labels, yet may suffer from reduced accuracy due to the scarcity of high-quality multi-label annotations. In this research, we leverage the powerful alignment between textual and visual features pretrained with millions of auxiliary image-text pairs. We introduce an efficient and effective framework called Evidence-guided Dual Context Optimization (DualCoOp++), which serves as a unified approach for addressing partial-label and zero-shot multi-label recognition. In DualCoOp++ we separately encode evidential, positive, and negative contexts for target classes as parametric components of the linguistic input (i.e., prompts). The evidential context aims to discover all the related visual content for the target class, and serves as guidance to aggregate positive 
    
[^7]: 气候模式降尺度的多变量困难物理约束

    Multi-variable Hard Physical Constraints for Climate Model Downscaling. (arXiv:2308.01868v1 [physics.ao-ph])

    [http://arxiv.org/abs/2308.01868](http://arxiv.org/abs/2308.01868)

    该研究探讨了气候模式降尺度中多变量之间的物理约束问题，并通过应用于温度的案例，提出了一种引入多变量硬约束的框架，确保了降尺度的气候变量群之间的物理关系。

    

    全球气候模型（GCM）是模拟气候演变和评估气候变化影响的主要工具。然而，它们通常以粗糙的空间分辨率运行，限制了它们在再现局部尺度现象方面的准确性。利用深度学习的统计降尺度方法提供了一种解决这一问题的方法，通过从粗糙变量中近似局部尺度气候场，从而实现区域GCM预测。通常，不同感兴趣的气候场被独立降尺度，导致互连变量之间基本物理特性的违反。本研究调查了这个问题的范围，并通过对温度的应用，为引入多变量硬约束的框架奠定了基础，以确保降尺度的气候变量群之间的物理关系。

    Global Climate Models (GCMs) are the primary tool to simulate climate evolution and assess the impacts of climate change. However, they often operate at a coarse spatial resolution that limits their accuracy in reproducing local-scale phenomena. Statistical downscaling methods leveraging deep learning offer a solution to this problem by approximating local-scale climate fields from coarse variables, thus enabling regional GCM projections. Typically, climate fields of different variables of interest are downscaled independently, resulting in violations of fundamental physical properties across interconnected variables. This study investigates the scope of this problem and, through an application on temperature, lays the foundation for a framework introducing multi-variable hard constraints that guarantees physical relationships between groups of downscaled climate variables.
    
[^8]: MRQ:通过模型重新量化支持多种量化方案

    MRQ:Support Multiple Quantization Schemes through Model Re-Quantization. (arXiv:2308.01867v1 [cs.LG])

    [http://arxiv.org/abs/2308.01867](http://arxiv.org/abs/2308.01867)

    该论文提出了一种名为MRQ的模型重新量化方法，通过将现有的量化模型快速转换以满足不同的量化要求，解决了在固定点硬件上部署深度学习模型的挑战。

    

    尽管各种硬件加速器（如NPU，TPU，DPU）的普及，但在固定点硬件上部署深度学习模型仍然具有挑战性，原因是复杂的模型量化和转换。现有的模型量化框架（如Tensorflow QAT，TFLite PTQ和Qualcomm AIMET）只支持有限的量化方案（如仅在TF1.x QAT中的非对称每张量量化）。因此，由于稍微不同的量化要求，深度学习模型不能轻松地为各种固定点硬件进行量化。在本文中，我们设想了一种新型的模型量化方法，称为MRQ（模型重新量化），它可以采用现有的量化模型，并快速将模型转换为满足不同量化要求（如非对称->对称，非2的幂次->2的幂次）。重新量化比从头开始进行量化要简单得多，因为它避免了昂贵的重新训练。

    Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU, DPU), deploying deep learning models on edge devices with fixed-point hardware is still challenging due to complex model quantization and conversion. Existing model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g., only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep learning models cannot be easily quantized for diverse fixed-point hardwares, mainly due to slightly different quantization requirements. In this paper, we envision a new type of model quantization approach called MRQ (model re-quantization), which takes existing quantized models and quickly transforms the models to meet different quantization requirements (e.g., asymmetric -> symmetric, non-power-of-2 scale -> power-of-2 scale). Re-quantization is much simpler than quantizing from scratch because it avoids costly re-training and
    
[^9]: 统计估计中的分布偏移: Wasserstein扰动与极小极大理论

    Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory. (arXiv:2308.01853v1 [stat.ML])

    [http://arxiv.org/abs/2308.01853](http://arxiv.org/abs/2308.01853)

    这篇论文研究了统计估计中的分布偏移问题，主要关注Wasserstein分布偏移，提出了联合分布偏移概念，并分析了几个统计问题的解决方法。论文发现了最优的极小极大风险和最不利的扰动，并证明了样本均值和最小二乘估计量的优越性。

    

    分布偏移是现代统计学习中的一个严重问题，因为它们可以将数据的特性从真实情况中系统地改变。我们专注于Wasserstein分布偏移，其中每个数据点可能会发生轻微扰动，而不是Huber污染模型，其中一部分观测值是异常值。我们提出并研究了超出独立扰动的偏移，探索了联合分布偏移，其中每个观测点的扰动可以协调进行。我们分析了几个重要的统计问题，包括位置估计、线性回归和非参数密度估计。在均值估计和线性回归的预测误差方差下，我们找到了精确的极小极大风险、最不利的扰动，并证明了样本均值和最小二乘估计量分别是最优的。这适用于独立和联合偏移，但最不利的扰动和极小极大风险是不同的。

    Distribution shifts are a serious concern in modern statistical learning as they can systematically change the properties of the data away from the truth. We focus on Wasserstein distribution shifts, where every data point may undergo a slight perturbation, as opposed to the Huber contamination model where a fraction of observations are outliers. We formulate and study shifts beyond independent perturbations, exploring Joint Distribution Shifts, where the per-observation perturbations can be coordinated. We analyze several important statistical problems, including location estimation, linear regression, and non-parametric density estimation. Under a squared loss for mean estimation and prediction error in linear regression, we find the exact minimax risk, a least favorable perturbation, and show that the sample mean and least squares estimators are respectively optimal. This holds for both independent and joint shifts, but the least favorable perturbations and minimax risks differ. For
    
[^10]: 句子编码任务的课程迁移学习

    Curricular Transfer Learning for Sentence Encoded Tasks. (arXiv:2308.01849v1 [cs.CL])

    [http://arxiv.org/abs/2308.01849](http://arxiv.org/abs/2308.01849)

    本文提出了一种课程迁移学习的方法，通过数据入侵和语法分析引导，逐步适应预训练分布，并在MultiWoZ任务中取得了显着的改进。

    

    微调语言模型是自然语言处理领域许多最先进方法的标准做法。然而，当源任务和目标任务之间的分布漂移时，例如，对话环境下，这些收益往往会减少。本文提出了一种由“数据入侵”和语法分析引导的预训练步骤序列（课程），允许在预训练分布之间进一步逐渐适应。在我们的实验中，与其他已知的预训练方法相比，我们的方法在MultiWoZ任务中取得了显着的改进。

    Fine-tuning language models in a downstream task is the standard approach for many state-of-the-art methodologies in the field of NLP. However, when the distribution between the source task and target task drifts, \textit{e.g.}, conversational environments, these gains tend to be diminished. This article proposes a sequence of pre-training steps (a curriculum) guided by "data hacking" and grammar analysis that allows further gradual adaptation between pre-training distributions. In our experiments, we acquire a considerable improvement from our method compared to other known pre-training approaches for the MultiWoZ task.
    
[^11]: URET: 通用鲁棒性评估工具包（用于逃避攻击）

    URET: Universal Robustness Evaluation Toolkit (for Evasion). (arXiv:2308.01840v1 [cs.LG])

    [http://arxiv.org/abs/2308.01840](http://arxiv.org/abs/2308.01840)

    本论文提出了一个名为URET的通用鲁棒性评估工具包，该工具包可以生成各种类型和任务领域下的对抗性输入，以确保关键AI任务的安全和鲁棒性。

    

    众所周知，机器学习模型容易受到逃避攻击的影响，如图像分类模型所示。充分了解这种攻击对于确保关键AI任务的安全和鲁棒性至关重要。然而，大多数逃避攻击很难对大多数AI系统进行部署，因为它们仅集中在图像领域并具有少数约束。与实践中使用的其他输入类型不同，图像由均匀的、数值的、连续的和独立的特征组成。此外，某些输入类型包含额外的语义和功能约束，必须遵守以生成逼真的对抗性输入。在这项工作中，我们提出了一个新的框架，可以生成与输入类型和任务领域无关的对抗性输入。给定一个输入和一组预定义的输入转换，我们的框架可以发现一系列转换，从而得到一个符合语义和功能要求的正确结果。

    Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functi
    
[^12]: 分布无关推断二元分类的回归函数

    Distribution-Free Inference for the Regression Function of Binary Classification. (arXiv:2308.01835v1 [stat.ML])

    [http://arxiv.org/abs/2308.01835](http://arxiv.org/abs/2308.01835)

    本文提出了一种分布无关的方法来推断二元分类问题中的回归函数，通过构建置信区间来解决该问题，相关算法经过验证具有可靠性。

    

    二元分类的一个关键对象是回归函数，即给定输入的类别标签的条件期望。通过回归函数，不仅可以定义贝叶斯最优分类器，还可以编码对应的错误分类概率。本文提出了一种重采样框架，用于构建精确、分布无关且非渐近保证的真实回归函数的置信区间，根据用户选择的置信水平。然后，提出了特定的算法来演示该框架。证明了构建的置信区间是强一致的，也就是说，任何错误的模型最终被排除的概率为1。排除的程度也通过可能近似正确类型的界限进行了量化。最后，通过数值实验验证了算法，并将方法与近似渐近置信椭圆进行了比较。

    One of the key objects of binary classification is the regression function, i.e., the conditional expectation of the class labels given the inputs. With the regression function not only a Bayes optimal classifier can be defined, but it also encodes the corresponding misclassification probabilities. The paper presents a resampling framework to construct exact, distribution-free and non-asymptotically guaranteed confidence regions for the true regression function for any user-chosen confidence level. Then, specific algorithms are suggested to demonstrate the framework. It is proved that the constructed confidence regions are strongly consistent, that is, any false model is excluded in the long run with probability one. The exclusion is quantified with probably approximately correct type bounds, as well. Finally, the algorithms are validated via numerical experiments, and the methods are compared to approximate asymptotic confidence ellipsoids.
    
[^13]: 大型语言模型在评估精神状况方面的能力

    The Capability of Large Language Models to Measure Psychiatric Functioning. (arXiv:2308.01834v1 [cs.CL])

    [http://arxiv.org/abs/2308.01834](http://arxiv.org/abs/2308.01834)

    本研究调查了大型语言模型在通过患者采访和临床描述预测精神功能方面的能力，结果显示这些模型在预测抑郁症评分方面表现最好，与人类临床评估者的结果相近，揭示了通用临床语言模型在评估精神状况方面的潜力。

    

    本研究探讨了基于大规模医学知识（Med-PaLM 2）训练的大型语言模型（LLM）在没有经过特定训练的情况下，通过患者采访和临床描述来预测精神功能的能力。通过使用提示来提取估计的临床评分和诊断，分析了145例抑郁症和115例创伤后应激障碍评估以及46例临床案例研究。结果表明，Med-PaLM 2能够在一系列精神疾病中评估精神功能，其中对基于标准评估的抑郁症评分的预测表现最佳（准确率范围= 0.80-0.84），这些预测结果在统计上与人类临床评估者的结果无法区分（t(1,144) = 1.20，p = 0.23）。结果显示了通用临床语言模型在评估精神状况方面的潜力。

    The current work investigates the capability of Large language models (LLMs) that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2) to predict psychiatric functioning from patient interviews and clinical descriptions without being trained to do so. To assess this, n = 145 depression and n =115 PTSD assessments and n = 46 clinical case studies across high prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma and stress, Addictive disorders) were analyzed using prompts to extract estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is capable of assessing psychiatric functioning across a range of psychiatric conditions with the strongest performance being the prediction of depression scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which were statistically indistinguishable from human clinical raters t(1,144) = 1.20; p = 0.23. Results show the potential for general clinical language models to f
    
[^14]: 提高深度神经网络鲁棒性和公平性的困难对抗样本挖掘

    Hard Adversarial Example Mining for Improving Robust Fairness. (arXiv:2308.01823v1 [cs.LG])

    [http://arxiv.org/abs/2308.01823](http://arxiv.org/abs/2308.01823)

    该研究针对对抗训练模型容易出现不公平问题的限制，提出了通过自适应的困难对抗样本挖掘来改善深度神经网络的鲁棒性和公平性。

    

    对抗训练（AT）被广泛认为是提高深度神经网络（DNNs）对抗性样本（AE）鲁棒性的最先进技术。然而，最近的研究发现，经过对抗训练的模型容易出现不公平问题，限制了它们的适用性。在本文中，我们通过经验观察发现，这个限制可能是由于严重的对抗置信过拟合，即某些具有过度自信的对抗性样本。为了缓解这个问题，我们提出了HAM，这是一个简单但有效的框架，通过自适应的困难对抗样本挖掘。HAM集中于以适应性的方式挖掘困难对抗性样本，同时丢弃容易的样本。具体地，HAM根据计算损失值时需要穿过决策边界的步长来识别困难的AE。此外，还引入了早期丢弃机制来在AE生成的初期丢弃容易的样本，从而使得网络更加鲁棒和公平。

    Adversarial training (AT) is widely considered the state-of-the-art technique for improving the robustness of deep neural networks (DNNs) against adversarial examples (AE). Nevertheless, recent studies have revealed that adversarially trained models are prone to unfairness problems, restricting their applicability. In this paper, we empirically observe that this limitation may be attributed to serious adversarial confidence overfitting, i.e., certain adversarial examples with overconfidence. To alleviate this problem, we propose HAM, a straightforward yet effective framework via adaptive Hard Adversarial example Mining.HAM concentrates on mining hard adversarial examples while discarding the easy ones in an adaptive fashion. Specifically, HAM identifies hard AEs in terms of their step sizes needed to cross the decision boundary when calculating loss value. Besides, an early-dropping mechanism is incorporated to discard the easy examples at the initial stages of AE generation, resulting
    
[^15]: Tensor程序IVb：无限宽度极限中的自适应优化

    Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit. (arXiv:2308.01814v1 [cs.LG])

    [http://arxiv.org/abs/2308.01814](http://arxiv.org/abs/2308.01814)

    本论文研究了在无限宽度极限中使用自适应优化器训练宽神经网络时的新现象，通过推导相应的“神经切线”和“最大更新”极限，展示了特征学习和核行为之间的二分法，同时引入了简化计算的bra-ket符号。

    

    超越随机梯度下降（SGD），当使用Adam等自适应优化器训练宽神经网络时，会出现新的现象吗？在这里，我们展示了以下结果：与SGD一样，对于包括Adam在内的一般优化器，特征学习和核行为之间存在着相同的二分法 - 尽管有一种非线性的“核”概念。我们推导了对于任何架构的相应的“神经切线”和“最大更新”极限。上述结果的两个基础性进展是：1）一种新的Tensor程序语言，NEXORT，可以表达自适应优化器如何将梯度处理为更新。2）引入bra-ket符号来极大地简化Tensor程序中的表达式和计算。该工作总结并概括了Tensor程序系列论文中的所有先前结果。

    Going beyond stochastic gradient descent (SGD), what new phenomena emerge in wide neural networks trained by adaptive optimizers like Adam? Here we show: The same dichotomy between feature learning and kernel behaviors (as in SGD) holds for general optimizers as well, including Adam -- albeit with a nonlinear notion of "kernel." We derive the corresponding "neural tangent" and "maximal update" limits for any architecture. Two foundational advances underlie the above results: 1) A new Tensor Program language, NEXORT, that can express how adaptive optimizers process gradients into updates. 2) The introduction of bra-ket notation to drastically simplify expressions and calculations in Tensor Programs. This work summarizes and generalizes all previous results in the Tensor Programs series of papers.
    
[^16]: 用深度强化学习解决作业车间调度问题: 一种序列到序列的方法。

    Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach. (arXiv:2308.01797v1 [cs.AI])

    [http://arxiv.org/abs/2308.01797](http://arxiv.org/abs/2308.01797)

    这篇论文介绍了一种原始的深度强化学习方法，采用序列到序列的方法自动学习调度规则，适用于作业车间调度问题和其他最优作业调度任务。

    

    作业调度是一个众所周知的组合优化问题，具有无尽的应用。良好规划的调度在自动化系统中带来许多好处：它们限制生产成本和浪费。然而，这个问题的NP难度使得使用启发式算法至关重要，其设计困难，需要专门的知识，并且通常会产生针对特定任务的方法。本文提出了一种原始的端到端深度强化学习方法来进行调度，自动学习调度规则。我们的技术受到自然语言编码器-解码器模型在序列处理中的启发，并据我们所知，从未用于调度目的。我们将我们的方法应用和测试到作业车间问题的一些基准实例上，但该技术足够通用，可以有可能用于处理其他不同的最优作业调度任务，减少干预。结果证明...

    Job scheduling is a well-known Combinatorial Optimization problem with endless applications. Well planned schedules bring many benefits in the context of automated systems: among others, they limit production costs and waste. Nevertheless, the NP-hardness of this problem makes it essential to use heuristics whose design is difficult, requires specialized knowledge and often produces methods tailored to the specific task. This paper presents an original end-to-end Deep Reinforcement Learning approach to scheduling that automatically learns dispatching rules. Our technique is inspired by natural language encoder-decoder models for sequence processing and has never been used, to the best of our knowledge, for scheduling purposes. We applied and tested our method in particular to some benchmark instances of Job Shop Problem, but this technique is general enough to be potentially used to tackle other different optimal job scheduling tasks with minimal intervention. Results demonstrate that 
    
[^17]: 基于QUBO实例的自适应变分量子算法基准测试

    Benchmarking Adaptative Variational Quantum Algorithms on QUBO Instances. (arXiv:2308.01789v1 [quant-ph])

    [http://arxiv.org/abs/2308.01789](http://arxiv.org/abs/2308.01789)

    本文通过分析三种自适应变分量子算法（EVQE, VAns, RA-VQE）填补了对不同方法的系统比较的研究空白。

    

    近年来，变分量子算法（VQAs）在NISQ时代的量子计算机上解决优化问题方面已经成为一种有前景的方法。然而，VQAs的一个限制是它们依赖于固定结构的电路，这些电路可能无法针对特定的问题或硬件配置进行定制。解决这个问题的一种主要策略是自适应VQAs，它通过添加和删除门来动态修改电路结构，并在训练过程中优化参数。已经在文献中提出了几种基于启发式的自适应VQAs，例如电路浅度、纠缠能力和硬件兼容性，但目前仍缺乏对不同方法进行系统比较的研究。本文旨在通过分析三种自适应VQAs来填补这一空白：进化变分量子本征求解器（EVQE）、可变模型（VAns）和随机自适应-VQE（RA-VQE）。

    In recent years, Variational Quantum Algorithms (VQAs) have emerged as a promising approach for solving optimization problems on quantum computers in the NISQ era. However, one limitation of VQAs is their reliance on fixed-structure circuits, which may not be taylored for specific problems or hardware configurations. A leading strategy to address this issue are Adaptative VQAs, which dynamically modify the circuit structure by adding and removing gates, and optimize their parameters during the training. Several Adaptative VQAs, based on heuristics such as circuit shallowness, entanglement capability and hardware compatibility, have already been proposed in the literature, but there is still lack of a systematic comparison between the different methods. In this paper, we aim to fill this gap by analyzing three Adaptative VQAs: Evolutionary Variational Quantum Eigensolver (EVQE), Variable Ansatz (VAns), already proposed in the literature, and Random Adapt-VQE (RA-VQE), a random approach 
    
[^18]: 基于深度学习的动脉壁应力应变分布预测以提高心血管风险评估

    Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment. (arXiv:2308.01771v1 [cs.LG])

    [http://arxiv.org/abs/2308.01771](http://arxiv.org/abs/2308.01771)

    本研究使用深度学习工具基于动脉壁横截面的空间排列和钙化信息，准确预测了动脉壁的应力和应变分布，以提高心血管风险评估的效果。

    

    本研究探讨了将端到端深度学习工具作为有限元方法(FEM)的更有效替代品来预测动脉壁二维横截面内的应力和应变场的潜力。我们首先提出了基于U-Net的全卷积神经网络(CNN)，根据动脉壁横截面中钙化的空间排列来预测von Mises应力和应变分布。进一步，我们开发了一个条件生成对抗网络(cGAN)，从感知角度特别是增强了应力和应变场地图对于具有不同钙化数量和空间配置的动脉壁的预测准确性。在U-Net和cGAN之上，我们还分别提出了它们的集成方法，进一步提高了场地图的预测准确性。我们的数据集由实施边界条件并提取应力和应变场地图而生成的输入图像和输出图像组成。经过训练的U-Net模型可以准确地预测动脉壁的应力和应变分布。

    This study investigated the potential of end-to-end deep learning tools as a more effective substitute for FEM in predicting stress-strain fields within 2D cross sections of arterial wall. We first proposed a U-Net based fully convolutional neural network (CNN) to predict the von Mises stress and strain distribution based on the spatial arrangement of calcification within arterial wall cross-sections. Further, we developed a conditional generative adversarial network (cGAN) to enhance, particularly from the perceptual perspective, the prediction accuracy of stress and strain field maps for arterial walls with various calcification quantities and spatial configurations. On top of U-Net and cGAN, we also proposed their ensemble approaches, respectively, to further improve the prediction accuracy of field maps. Our dataset, consisting of input and output images, was generated by implementing boundary conditions and extracting stress-strain field maps. The trained U-Net models can accurate
    
[^19]: Bag of Policies for Distributional Deep Exploration（分布式深度探索中的一种策略组合方法）

    Bag of Policies for Distributional Deep Exploration. (arXiv:2308.01759v1 [cs.LG])

    [http://arxiv.org/abs/2308.01759](http://arxiv.org/abs/2308.01759)

    本论文提出了一种名为"袋子策略"的通用方法，用于分布式强化学习中的深度探索。该方法通过维护一个副本的种群，利用独立更新的头部和离线更新策略，为每个头部提供不同的学习信号，从而实现了多样化的学习和行为。

    

    在强化学习中，高效地探索复杂环境仍然是一个重大挑战。与以往的启发式机制相比，我们关注的是在分布式强化学习中进行深度探索。我们提出了一种通用的方法，袋子策略（Bag of Policies，BoP），它可以在任何返回分布估计器的基础上构建，通过维护一个副本的种群。BoP由多个独立更新的头部组成。在训练过程中，每个episode由一个头部控制，收集到的状态-动作对被用来离线更新所有头部，为每个头部提供不同的学习信号，从而多样化学习和行为。我们通过使用贝叶斯分布进行分布式的actor-critic群体实现了BoP方法，以测试乐观集成方法是否能够像Bootstrapped DQN在标量强化学习中一样提高分布式强化学习的效果。

    Efficient exploration in complex environments remains a major challenge for reinforcement learning (RL). Compared to previous Thompson sampling-inspired mechanisms that enable temporally extended exploration, i.e., deep exploration, we focus on deep exploration in distributional RL. We develop here a general purpose approach, Bag of Policies (BoP), that can be built on top of any return distribution estimator by maintaining a population of its copies. BoP consists of an ensemble of multiple heads that are updated independently. During training, each episode is controlled by only one of the heads and the collected state-action pairs are used to update all heads off-policy, leading to distinct learning signals for each head which diversify learning and behaviour. To test whether optimistic ensemble method can improve on distributional RL as did on scalar RL, by e.g. Bootstrapped DQN, we implement the BoP approach with a population of distributional actor-critics using Bayesian Distributi
    
[^20]: 神经崩溃终止点：一种统一的解决方案，用于类别增量学习及其变体

    Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants. (arXiv:2308.01746v1 [cs.LG])

    [http://arxiv.org/abs/2308.01746](http://arxiv.org/abs/2308.01746)

    本文提出了一种神经崩溃终止点的统一解决方案，用于解决类别增量学习及其变体中的问题，通过固定的结构和原型演化方案来保持对新类别的可学习性，同时避免灾难性遗忘问题。

    

    如何在保持对旧类别的能力的同时使新类别具有可学习性，一直是类别增量学习的一项重要挑战。除了常规情况之外，还提出了长尾类别增量学习和少样本类别增量学习，以考虑数据不平衡和数据稀缺性问题，这在实际应用中很常见，进一步加剧了灾难性遗忘问题。现有的方法专门针对这三个任务中的一个进行提出。本文提出了一种统一的解决方案来解决这三个任务中的问题。具体来说，我们提出了神经崩溃终止点，它是一个具有整个标签空间中最大等角互类间隔的固定结构。它在整个增量训练过程中充当一致的目标，以避免逐步划分特征空间。针对类别增量学习和长尾类别增量学习，我们还提出了一种原型演化方案，将主干特征引入我们的神经崩溃终止点。

    How to enable learnability for new classes while keeping the capability well on old classes has been a crucial challenge for class incremental learning. Beyond the normal case, long-tail class incremental learning and few-shot class incremental learning are also proposed to consider the data imbalance and data scarcity, respectively, which are common in real-world implementations and further exacerbate the well-known problem of catastrophic forgetting. Existing methods are specifically proposed for one of the three tasks. In this paper, we offer a unified solution to the misalignment dilemma in the three tasks. Concretely, we propose neural collapse terminus that is a fixed structure with the maximal equiangular inter-class separation for the whole label space. It serves as a consistent target throughout the incremental training to avoid dividing the feature space incrementally. For CIL and LTCIL, we further propose a prototype evolving scheme to drive the backbone features into our ne
    
[^21]: 具有无悔的多任务学习：从改进的置信区间到主动学习

    Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning. (arXiv:2308.01744v1 [cs.LG])

    [http://arxiv.org/abs/2308.01744](http://arxiv.org/abs/2308.01744)

    本文提出了一种在不可知情况下的多任务学习新方法，通过对多任务信息增益的分析，获得了新的遗憾保证，并提出了一种能够自动适应任务相似性的在线学习算法。

    

    多任务学习是一个强大的框架，通过共享信息来同时学习多个相关任务。对估计任务的不确定性进行量化对许多下游应用非常重要，例如在线或主动学习。在这项工作中，我们在具有挑战性的不可知设置下提供了新颖的多任务置信区间，即当学习者无法获得任务之间的相似性和任务特征时。所得到的区间不需要独立同分布的数据，并且可以直接应用于在线学习中的遗憾边界。通过对多任务信息增益的精细分析，我们获得了新的遗憾保证，可以根据任务相似性参数明显改进独立处理任务的方法。我们进一步提出了一种新的在线学习算法，可以在不事先知道这个参数的情况下实现这种改进的遗憾，即自动适应任务相似性。

    Multitask learning is a powerful framework that enables one to simultaneously learn multiple related tasks by sharing information between them. Quantifying uncertainty in the estimated tasks is of pivotal importance for many downstream applications, such as online or active learning. In this work, we provide novel multitask confidence intervals in the challenging agnostic setting, i.e., when neither the similarity between tasks nor the tasks' features are available to the learner. The obtained intervals do not require i.i.d. data and can be directly applied to bound the regret in online learning. Through a refined analysis of the multitask information gain, we obtain new regret guarantees that, depending on a task similarity parameter, can significantly improve over treating tasks independently. We further propose a novel online learning algorithm that achieves such improved regret without knowing this parameter in advance, i.e., automatically adapting to task similarity. As a second k
    
[^22]: 使用CFD和贝叶斯优化找到大型燃气发动机预燃室的最佳设计

    Finding the Optimum Design of Large Gas Engines Prechambers Using CFD and Bayesian Optimization. (arXiv:2308.01743v1 [cs.CE])

    [http://arxiv.org/abs/2308.01743](http://arxiv.org/abs/2308.01743)

    本研究使用CFD模拟结合贝叶斯优化方法，高效地寻找大型燃气发动机预燃室的最佳设计。

    

    利用预燃室的湍流射流点火概念是在大型燃气发动机中实现稳定燃烧的一种有前景的解决方案，可以在贫燃烧条件下实现高效率和低排放水平。由于大型燃气发动机预燃室的设计和工作参数范围广泛，评估不同设计的首选方法是计算流体力学（CFD），因为在试验台测量活动中进行测试耗时且昂贵。然而，由于解决底层物理学的复杂性，详细CFD模拟需要很长的计算时间，这也限制了它的适用性。在类似于本例的优化设置中，即在目标函数的计算成本较高的情况下，贝叶斯优化已在很大程度上取代了经典的试验设计。因此，本研究涉及使用CFD模拟进行计算高效的大型燃气发动机预燃室设计的贝叶斯优化。

    The turbulent jet ignition concept using prechambers is a promising solution to achieve stable combustion at lean conditions in large gas engines, leading to high efficiency at low emission levels. Due to the wide range of design and operating parameters for large gas engine prechambers, the preferred method for evaluating different designs is computational fluid dynamics (CFD), as testing in test bed measurement campaigns is time-consuming and expensive. However, the significant computational time required for detailed CFD simulations due to the complexity of solving the underlying physics also limits its applicability. In optimization settings similar to the present case, i.e., where the evaluation of the objective function(s) is computationally costly, Bayesian optimization has largely replaced classical design-of-experiment. Thus, the present study deals with the computationally efficient Bayesian optimization of large gas engine prechambers design using CFD simulation. Reynolds-av
    
[^23]: 利用多标签相关性在标签分布学习中的应用

    Exploiting Multi-Label Correlation in Label Distribution Learning. (arXiv:2308.01742v1 [cs.LG])

    [http://arxiv.org/abs/2308.01742](http://arxiv.org/abs/2308.01742)

    该论文介绍了一种利用多标签相关性的标签分布学习方法，通过在多标签学习过程中捕捉低秩标签相关性来提高学习效果，并通过实验证明了该方法优于现有的标签分布学习方法。

    

    标签分布学习（LDL）是一种新的机器学习范式，它为每个实例分配一个标签分布。许多LDL方法提出在学习过程中利用标签相关性来解决指数大小的输出空间；其中许多方法利用标签分布的低秩结构来捕捉标签相关性。然而，最近的研究表明，标签分布矩阵通常是满秩的，给利用低秩标签相关性的工作带来了挑战。需要注意的是，多标签通常是低秩的；低秩标签相关性在多标签学习（MLL）文献中被广泛采用。受此启发，我们在LDL中引入了辅助的MLL过程，并在MLL而不是LDL中捕捉低秩标签相关性。通过这种方式，我们恰当地利用了低秩标签相关性在我们的LDL方法中。我们进行了全面的实验，并证明了我们的方法优于现有的LDL方法。

    Label Distribution Learning (LDL) is a novel machine learning paradigm that assigns label distribution to each instance. Many LDL methods proposed to leverage label correlation in the learning process to solve the exponential-sized output space; among these, many exploited the low-rank structure of label distribution to capture label correlation. However, recent studies disclosed that label distribution matrices are typically full-rank, posing challenges to those works exploiting low-rank label correlation. Note that multi-label is generally low-rank; low-rank label correlation is widely adopted in multi-label learning (MLL) literature. Inspired by that, we introduce an auxiliary MLL process in LDL and capture low-rank label correlation on that MLL rather than LDL. In such a way, low-rank label correlation is appropriately exploited in our LDL methods. We conduct comprehensive experiments and demonstrate that our methods are superior to existing LDL methods. Besides, the ablation studi
    
[^24]: MAP: 一个模型无关的预训练框架用于点击率预测

    MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction. (arXiv:2308.01737v1 [cs.IR])

    [http://arxiv.org/abs/2308.01737](http://arxiv.org/abs/2308.01737)

    提出了一个模型无关的预训练框架，用于点击率预测，可以更好地利用多字段分类数据和大量用户点击日志，学习更广义和有效的特征和实例表示。

    

    随着个性化在线服务的广泛应用，点击率（CTR）预测越来越受到关注和研究。CTR预测的最突出特点是其多字段分类数据格式和庞大而日益增长的数据量。神经模型的大容量有助于在监督学习范式下消化如此大量的数据，但是它们未能充分利用大量数据的潜力，因为1比特的点击信号不足以指导模型学习功能强大的特征和实例表示。自我监督学习范式提供了更有前景的预训练-微调解决方案，以更好地利用大量用户点击日志并学习更广义和有效的表示。然而，对于CTR预测的自我监督学习仍然是一个开放的问题，因为当前在这方面的工作仅仅是初步和基础的。为此，我们提出了一个模型无关的预训练框架。

    With the widespread application of personalized online services, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume. The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since the 1-bit click signal is not sufficient to guide the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs, and learn more generalized and effective representations. However, self-supervised learning for CTR prediction is still an open question, since current works on this line are only preliminary and rudimentary. To this end, we propose a Model-agnostic pretrain
    
[^25]: 通过推断时间采样量化预测不确定性

    Quantification of Predictive Uncertainty via Inference-Time Sampling. (arXiv:2308.01731v1 [cs.LG])

    [http://arxiv.org/abs/2308.01731](http://arxiv.org/abs/2308.01731)

    该论文提出了一种后续采样策略，用于估计考虑数据模糊性的预测不确定性，可以生成不同的合理输出，并且不假设预测分布的参数形式。

    

    由于数据模糊性而导致的预测变异性通常通过构建具有内置概率能力的专用模型来解决，这些模型被训练以预测不确定性估计作为感兴趣的变量。这些方法需要不同的结构组件和训练机制，可能包含限制性的假设并表现出过于自信，即对不精确预测的高置信度。在这项工作中，我们提出了一种后续采样策略，用于估计考虑数据模糊性的预测不确定性。该方法可以为给定的输入生成不同的合理输出，并且不假设预测分布的参数形式。它是架构不可知的，并且可以应用于任何前馈确定性网络，而无需对架构或训练过程进行更改。在图像和非图像输入数据上的回归任务实验显示该方法生成多样化和多模态的预测分布的能力，以及-

    Predictive variability due to data ambiguities has typically been addressed via construction of dedicated models with built-in probabilistic capabilities that are trained to predict uncertainty estimates as variables of interest. These approaches require distinct architectural components and training mechanisms, may include restrictive assumptions and exhibit overconfidence, i.e., high confidence in imprecise predictions. In this work, we propose a post-hoc sampling strategy for estimating predictive uncertainty accounting for data ambiguity. The method can generate different plausible outputs for a given input and does not assume parametric forms of predictive distributions. It is architecture agnostic and can be applied to any feed-forward deterministic network without changes to the architecture or training procedure. Experiments on regression tasks on imaging and non-imaging input data show the method's ability to generate diverse and multi-modal predictive distributions, and a des
    
[^26]: 基于联合精算神经网络的横断面和纵向索赔计数数据的车载通信技术

    Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data. (arXiv:2308.01729v1 [stat.ML])

    [http://arxiv.org/abs/2308.01729](http://arxiv.org/abs/2308.01729)

    本论文提出了一种基于联合精算神经网络框架的横断面和纵向索赔计数模型，通过结合传统精算模型和神经网络，充分利用了两个模型的优势。

    

    我们提出了一种基于Mario W\"uthrich和Michael Merz提出的联合精算神经网络（CANN）框架的横断面和纵向索赔计数模型。CANN方法将传统的精算模型（如广义线性模型）与神经网络相结合，形成了一个包含经典回归模型和神经网络部分的双组件模型。CANN模型充分利用了两个模型的优势，既可以提供经典模型的可靠性和可解释性，又可以利用神经网络的灵活性和对复杂关系和交互作用的捕捉能力。在我们提出的模型中，我们使用了广为人知的对数线性索赔计数回归模型作为经典回归部分，使用了多层感知器（MLP）作为神经网络部分。MLP部分用于处理以向量形式表示的车辆驾驶行为的车载通信数据。

    We present novel cross-sectional and longitudinal claim count models for vehicle insurance built upon the Combined Actuarial Neural Network (CANN) framework proposed by Mario W\"uthrich and Michael Merz. The CANN approach combines a classical actuarial model, such as a generalized linear model, with a neural network. This blending of models results in a two-component model comprising a classical regression model and a neural network part. The CANN model leverages the strengths of both components, providing a solid foundation and interpretability from the classical model while harnessing the flexibility and capacity to capture intricate relationships and interactions offered by the neural network. In our proposed models, we use well-known log-linear claim count regression models for the classical regression part and a multilayer perceptron (MLP) for the neural network part. The MLP part is used to process telematics car driving data given as a vector characterizing the driving behavior 
    
[^27]: 评估图神经网络的链路预测解释

    Evaluating Link Prediction Explanations for Graph Neural Networks. (arXiv:2308.01682v1 [cs.LG])

    [http://arxiv.org/abs/2308.01682](http://arxiv.org/abs/2308.01682)

    本文评估了图神经网络的链路预测解释的质量，并提供了定量指标来衡量解释的质量。我们发现底层假设和技术细节对解释的质量有影响。

    

    图机器学习（GML）在现实世界的领域中有许多应用，比如节点/图分类和链路预测。为GML模型提供人类可理解的解释是一项具有挑战性但基础的任务，但对链路预测模型的解释验证却受到了很少的关注。在本文中，我们提供了定量指标来评估链路预测解释的质量，无论是否有基准数据。我们使用这些指标评估了图神经网络的最先进可解释性方法。我们讨论了底层假设和链路预测任务特定的技术细节，比如节点嵌入之间的距离选择，如何影响解释的质量。

    Graph Machine Learning (GML) has numerous applications, such as node/graph classification and link prediction, in real-world domains. Providing human-understandable explanations for GML models is a challenging yet fundamental task to foster their adoption, but validating explanations for link prediction models has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.
    
[^28]: 利用张量核范数和严格互补性进行低秩张量恢复的一阶方法的效率

    Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity. (arXiv:2308.01677v1 [math.OC])

    [http://arxiv.org/abs/2308.01677](http://arxiv.org/abs/2308.01677)

    本研究探讨了基于张量核范数的约束最小化方法在低秩张量恢复中的有效性，提出了适当的严格互补性条件，并且得到了在此条件下的主要结果：1.对于特定形式的目标函数，标准投影梯度方法具有线性收敛速度，尽管目标函数不一定是强凸的。2.对于光滑的目标函数，标准梯度方法的收敛速度和每次迭代的运行时间可能显著提高。

    

    我们考虑基于张量核范数诱导的球上的约束最小化的凸松弛方法，用于低秩张量恢复。我们借鉴了最近的一系列结果，这些结果考虑了用于恢复低秩矩阵的凸松弛方法，并且已经建立了在严格互补性条件下，标准梯度方法的收敛速度和每次迭代的运行时间可能显著提高。我们针对张量核范数球体开发了适当的严格互补性条件，并获得了以下主要结果：1. 当要最小化的目标具有形式$f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$，其中$g$是强凸函数，$\mA$是一个线性映射（例如最小二乘法），存在二次增长界，这意味着标准投影梯度方法具有线性收敛速度，尽管$f$不一定是强凸的。2.对于光滑的目标函数，

    We consider convex relaxations for recovering low-rank tensors based on constrained minimization over a ball induced by the tensor nuclear norm, recently introduced in \cite{tensor_tSVD}. We build on a recent line of results that considered convex relaxations for the recovery of low-rank matrices and established that under a strict complementarity condition (SC), both the convergence rate and per-iteration runtime of standard gradient methods may improve dramatically. We develop the appropriate strict complementarity condition for the tensor nuclear norm ball and obtain the following main results under this condition: 1. When the objective to minimize is of the form $f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$ , where $g$ is strongly convex and $\mA$ is a linear map (e.g., least squares), a quadratic growth bound holds, which implies linear convergence rates for standard projected gradient methods, despite the fact that $f$ need not be strongly convex. 2. For a smooth objective function,
    
[^29]: 经济非线性MPC的Koopman模型的端到端强化学习

    End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC. (arXiv:2308.01674v1 [cs.LG])

    [http://arxiv.org/abs/2308.01674](http://arxiv.org/abs/2308.01674)

    本论文提出了一种用于经济非线性MPC的Koopman模型的端到端强化学习方法，旨在实现控制性能和计算需求之间的平衡。

    

    （经济）非线性模型预测控制（（e）NMPC）需要在所有相关状态空间区域都具有足够准确性的动态系统模型。这些模型还必须计算成本足够低以确保实时可行性。基于数据驱动的替代机制模型可以用来减少（e）NMPC的计算负担；但是，这些模型通常通过系统辨识以在模拟样本上获得最大平均预测准确性进行训练，并作为实际（e）NMPC的一部分表现不佳。我们提出了一种用于实现最佳（e）NMPC性能的动态替代模型的端到端强化学习方法，从而得到具有控制性能和计算需求之间良好平衡的预测控制器。我们通过两个基于已建立的非线性连续搅拌反应器模型的应用来验证我们的方法。

    (Economic) nonlinear model predictive control ((e)NMPC) requires dynamic system models that are sufficiently accurate in all relevant state-space regions. These models must also be computationally cheap enough to ensure real-time tractability. Data-driven surrogate models for mechanistic models can be used to reduce the computational burden of (e)NMPC; however, such models are typically trained by system identification for maximum average prediction accuracy on simulation samples and perform suboptimally as part of actual (e)NMPC. We present a method for end-to-end reinforcement learning of dynamic surrogate models for optimal performance in (e)NMPC applications, resulting in predictive controllers that strike a favorable balance between control performance and computational demand. We validate our method on two applications derived from an established nonlinear continuous stirred-tank reactor model. We compare the controller performance to that of MPCs utilizing models trained by the 
    
[^30]: UniG-Encoder: 一种用于图与超图节点分类的通用特征编码器

    UniG-Encoder: A Universal Feature Encoder for Graph and Hypergraph Node Classification. (arXiv:2308.01650v1 [cs.LG])

    [http://arxiv.org/abs/2308.01650](http://arxiv.org/abs/2308.01650)

    UniG-Encoder是一种通用特征编码器，用于图和超图节点分类。它通过将连接节点的拓扑关系转换为边或超边特征，并结合原始节点特征，利用神经网络生成编码后的节点嵌入。

    

    图和超图表示学习受到各个研究领域的广泛关注。尽管图神经网络（GNNs）、超图神经网络（HGNNs）及其精心设计的变种在一些常用基准图和超图上有不错的性能和丰富的应用，但它们在某些情况下却不如简单的多层感知器（Multi-Layer Perceptron）表现好。这一观察促使人们重新审视当前GNNs和HGNNs的设计范式，并提出了有效提取图特征的挑战。本文设计了一种用于图和超图表示学习的通用特征编码器，称为UniG-Encoder。该架构从连接节点的拓扑关系的前向转换开始，通过一个归一化投影矩阵将其转换为边或超边特征。得到的边/超边特征以及原始节点特征一起输入神经网络。编码后的节点嵌入然后被生成。

    Graph and hypergraph representation learning has attracted increasing attention from various research fields. Despite the decent performance and fruitful applications of Graph Neural Networks (GNNs), Hypergraph Neural Networks (HGNNs), and their well-designed variants, on some commonly used benchmark graphs and hypergraphs, they are outperformed by even a simple Multi-Layer Perceptron. This observation motivates a reexamination of the design paradigm of the current GNNs and HGNNs and poses challenges of extracting graph features effectively. In this work, a universal feature encoder for both graph and hypergraph representation learning is designed, called UniG-Encoder. The architecture starts with a forward transformation of the topological relationships of connected nodes into edge or hyperedge features via a normalized projection matrix. The resulting edge/hyperedge features, together with the original node features, are fed into a neural network. The encoded node embeddings are then
    
[^31]: MARLIM: 多智能体强化学习在库存管理中的应用

    MARLIM: Multi-Agent Reinforcement Learning for Inventory Management. (arXiv:2308.01649v1 [cs.LG])

    [http://arxiv.org/abs/2308.01649](http://arxiv.org/abs/2308.01649)

    本文提出了一个名为MARLIM的新型多智能体强化学习框架，用于解决具有随机需求和交货时间的单层多产品供应链的库存管理问题，并通过数值实验证明了强化学习方法相对于传统基准方法的优势。

    

    在供应链行业中，通过优化补货决策来维持产品供需平衡是一个重要的挑战。本文提出了一种名为MARLIM的新型强化学习框架，用于解决具有随机需求和交货时间的单层多产品供应链的库存管理问题。在这个背景下，通过单个或多个智能体在合作环境中开发控制器。对真实数据进行的数值实验证明了强化学习方法相对于传统基准方法的优势。

    Maintaining a balance between the supply and demand of products by optimizing replenishment decisions is one of the most important challenges in the supply chain industry. This paper presents a novel reinforcement learning framework called MARLIM, to address the inventory management problem for a single-echelon multi-products supply chain with stochastic demands and lead-times. Within this context, controllers are developed through single or multiple agents in a cooperative setting. Numerical experiments on real data demonstrate the benefits of reinforcement learning methods over traditional baselines.
    
[^32]: 将知识图谱与生成对抗网络相互交错应用于书籍封面设计的创意支持

    Interleaving GANs with knowledge graphs to support design creativity for book covers. (arXiv:2308.01626v1 [cs.CV])

    [http://arxiv.org/abs/2308.01626](http://arxiv.org/abs/2308.01626)

    本文将生成对抗网络与知识图谱相互交错应用于书籍封面设计，通过改变输入标题获得多个选项并利用鉴别器选择最佳图像，表现出比之前方法更好的生成效果和更好的选择。

    

    书籍封面的吸引力对于一本书的成功至关重要。本文将生成对抗网络（GANs）应用于书籍封面领域，使用不同的训练方法以获得更好的生成图像。我们将知识图谱与GANs相互交错，改变输入标题以获得给定标题的多个可能选项，然后将其作为生成器的增强输入。最后，我们利用训练阶段获得的鉴别器选择使用新标题生成的最佳图像。相比仅使用GANs，我们的方法在生成书籍封面方面表现更好，而知识图谱与书籍作者或编辑相比提供了更好的选择。

    An attractive book cover is important for the success of a book. In this paper, we apply Generative Adversarial Networks (GANs) to the book covers domain, using different methods for training in order to obtain better generated images. We interleave GANs with knowledge graphs to alter the input title to obtain multiple possible options for any given title, which are then used as an augmented input to the generator. Finally, we use the discriminator obtained during the training phase to select the best images generated with new titles. Our method performed better at generating book covers than previous attempts, and the knowledge graph gives better options to the book author or editor compared to using GANs alone.
    
[^33]: 一种具有连续对称性的新型卷积神经网络架构

    A Novel Convolutional Neural Network Architecture with a Continuous Symmetry. (arXiv:2308.01621v1 [cs.CV])

    [http://arxiv.org/abs/2308.01621](http://arxiv.org/abs/2308.01621)

    本文介绍了一种新的卷积神经网络架构，通过连续的对称性修改权重，具有与传统模型不同的特点，希望将对称性作为神经网络的新期望特性，并推广将偏微分方程的视角应用于ConvNet的分析和解释。

    

    本文介绍了一种新的卷积神经网络(ConvNet)架构，其灵感来自于一类称为拟线性双曲系统的偏微分方程(PDEs)。在图像分类任务上具有可比较的性能，它允许通过连续的对称性修改权重。这是与传统模型中基本固定的架构和权重相比的重大转变。我们希望将(内部)对称性作为一种新的神经网络期望特性，并在更广泛的深度学习社区中吸引对PDE视角分析和解释ConvNet的关注。

    This paper introduces a new Convolutional Neural Network (ConvNet) architecture inspired by a class of partial differential equations (PDEs) called quasi-linear hyperbolic systems. With comparable performance on image classification task, it allows for the modification of the weights via a continuous group of symmetry. This is a significant shift from traditional models where the architecture and weights are essentially fixed. We wish to promote the (internal) symmetry as a new desirable property for a neural network, and to draw attention to the PDE perspective in analyzing and interpreting ConvNets in the broader Deep Learning community.
    
[^34]: 通过反事实验证评估DNNs的系统性缺陷

    Assessing Systematic Weaknesses of DNNs using Counterfactuals. (arXiv:2308.01614v1 [cs.LG])

    [http://arxiv.org/abs/2308.01614](http://arxiv.org/abs/2308.01614)

    通过反事实验证评估DNNs的系统性缺陷。

    

    随着DNNs进入安全关键应用，对这种模型的测试方法越来越受到关注。目前的一个方向是寻找和识别系统性弱点，这些弱点使基于平均性能值的安全假设处于危险之中。这些弱点可以表现为（语义上连贯的）子集或输入空间中的区域，在这些区域中，DNN的性能比预期的平均性能要差。然而，将观察到的低性能归因于描述该子集的特定语义特征是非常困难的。例如，与其他（未考虑）属性相关的数据不均匀性可能会扭曲结果。然而，考虑所有（可用）属性及其相互作用通常计算成本很高。受反事实解释的启发，我们提出了一种有效且计算成本低的算法来验证现有子集的语义归因，即检查...

    With the advancement of DNNs into safety-critical applications, testing approaches for such models have gained more attention. A current direction is the search for and identification of systematic weaknesses that put safety assumptions based on average performance values at risk. Such weaknesses can take on the form of (semantically coherent) subsets or areas in the input space where a DNN performs systematically worse than its expected average. However, it is non-trivial to attribute the reason for such observed low performances to the specific semantic features that describe the subset. For instance, inhomogeneities within the data w.r.t. other (non-considered) attributes might distort results. However, taking into account all (available) attributes and their interaction is often computationally highly expensive. Inspired by counterfactual explanations, we propose an effective and computationally cheap algorithm to validate the semantic attribution of existing subsets, i.e., to chec
    
[^35]: 特征噪声提升带有标签噪声下的深度神经网络泛化能力

    Feature Noise Boosts DNN Generalization under Label Noise. (arXiv:2308.01609v1 [cs.LG])

    [http://arxiv.org/abs/2308.01609](http://arxiv.org/abs/2308.01609)

    本研究提出一种特征噪声方法来增强深度神经网络在标签噪声下的泛化能力。理论上证明标签噪声会削弱泛化能力，而特征噪声通过约束模型权重和特征之间的互信息来提高泛化能力。为了确保有效泛化，我们进行了应用分析并确定了合适的特征噪声类型和水平。

    

    训练数据中的标签噪声对深度神经网络（DNNs）的泛化能力有深远影响。本研究引入并理论上证明了一种简单的特征噪声方法，直接给训练数据的特征添加噪声，可以提升带有标签噪声下的DNNs的泛化能力。具体来说，我们进行了理论分析，揭示了标签噪声通过放宽PAC-Bayes泛化界限而导致DNN泛化能力减弱，而特征噪声通过对模型权重与特征之间的互信息设置上界来约束PAC-Bayes泛化界限，从而得到更好的DNN泛化能力。此外，为了在标签噪声存在的情况下确保DNN的有效泛化能力，我们进行了应用分析，确定了添加合适的特征噪声类型和水平以获得理想的标签噪声泛化能力。最后，我们在几个流行的数据集上进行了大量实验结果验证。

    The presence of label noise in the training data has a profound impact on the generalization of deep neural networks (DNNs). In this study, we introduce and theoretically demonstrate a simple feature noise method, which directly adds noise to the features of training data, can enhance the generalization of DNNs under label noise. Specifically, we conduct theoretical analyses to reveal that label noise leads to weakened DNN generalization by loosening the PAC-Bayes generalization bound, and feature noise results in better DNN generalization by imposing an upper bound on the mutual information between the model weights and the features, which constrains the PAC-Bayes generalization bound. Furthermore, to ensure effective generalization of DNNs in the presence of label noise, we conduct application analyses to identify the optimal types and levels of feature noise to add for obtaining desirable label noise generalization. Finally, extensive experimental results on several popular datasets
    
[^36]: 基于深度学习的参数化偏微分方程的替代模型：通过图神经网络处理几何变化

    Deep Learning-based surrogate models for parametrized PDEs: handling geometric variability through graph neural networks. (arXiv:2308.01602v1 [math.NA])

    [http://arxiv.org/abs/2308.01602](http://arxiv.org/abs/2308.01602)

    本研究提出了一种使用基于深度学习的替代模型来处理参数化偏微分方程中的几何变异的策略，其中使用了图神经网络来高效演化系统。

    

    在建模复杂物理系统时，基于网格的模拟在科学和工程的许多学科中都起着关键作用，这些系统需要求解参数化的时间相关非线性偏微分方程（PDEs）。在这种情况下，全阶模型（FOMs），比如基于有限元法的模型，可以达到很高的精度，但通常需要大量计算资源。因此，为了在精度和效率之间找到有利的平衡，我们发展了替代模型，用来取代计算成本高昂的求解器。本研究探索了在几何变异存在的情况下，使用图神经网络（GNNs）进行时间相关PDEs仿真的潜力。具体而言，我们提出了一种基于数据驱动时间步进方案的系统策略，在其中使用GNN架构来高效演化系统。

    Mesh-based simulations play a key role when modeling complex physical systems that, in many disciplines across science and engineering, require the solution of parametrized time-dependent nonlinear partial differential equations (PDEs). In this context, full order models (FOMs), such as those relying on the finite element method, can reach high levels of accuracy, however often yielding intensive simulations to run. For this reason, surrogate models are developed to replace computationally expensive solvers with more efficient ones, which can strike favorable trade-offs between accuracy and efficiency. This work explores the potential usage of graph neural networks (GNNs) for the simulation of time-dependent PDEs in the presence of geometrical variability. In particular, we propose a systematic strategy to build surrogate models based on a data-driven time-stepping scheme where a GNN architecture is used to efficiently evolve the system. With respect to the majority of surrogate models
    
[^37]: 无监督表示学习对时间序列的研究综述

    Unsupervised Representation Learning for Time Series: A Review. (arXiv:2308.01578v1 [cs.LG])

    [http://arxiv.org/abs/2308.01578](http://arxiv.org/abs/2308.01578)

    本文对时间序列领域的无监督表示学习方法进行了综述，提出了基于ULTS库的快速实现与统一评估，并为未来的研究提供了参考。

    

    无监督表示学习方法旨在从无标签数据中学习有区分性的特征表示，而无需对每个样本进行注释。对于时间序列数据来说，实现无监督表示学习非常关键，因为其复杂特性和缺乏与其他数据形态相比的视觉提示导致了标注困境。近年来，无监督表示学习技术在各个领域取得了快速发展。然而，对于时间序列的无监督表示学习方法缺乏系统性的分析。为了填补这个空白，我们进行了对时间序列领域中现有快速发展的无监督表示学习方法的全面文献综述。此外，我们还开发了一个统一和标准化的库，名为ULTS（即无监督时间序列学习），以便对各种模型进行快速实现和统一评估。通过ULTS，我们经验性地评估了无监督表示学习方法在不同任务上的性能，并为未来的研究提供了参考。

    Unsupervised representation learning approaches aim to learn discriminative feature representations from unlabeled data, without the requirement of annotating every sample. Enabling unsupervised representation learning is extremely crucial for time series data, due to its unique annotation bottleneck caused by its complex characteristics and lack of visual cues compared with other data modalities. In recent years, unsupervised representation learning techniques have advanced rapidly in various domains. However, there is a lack of systematic analysis of unsupervised representation learning approaches for time series. To fill the gap, we conduct a comprehensive literature review of existing rapidly evolving unsupervised representation learning approaches for time series. Moreover, we also develop a unified and standardized library, named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast implementations and unified evaluations on various models. With ULTS, we empirica
    
[^38]: 使用双对抗器对去噪扩散模型进行对抗训练以实现高保真多人说话人 TTS

    Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS. (arXiv:2308.01573v1 [cs.SD])

    [http://arxiv.org/abs/2308.01573](http://arxiv.org/abs/2308.01573)

    本文提出了一种具有双鉴别器的语音合成模型，以进一步提升 DiffGAN-TTS 的性能。该模型利用扩散鉴别器学习逆过程分布，并利用声谱图鉴别器学习声谱图分布。

    

    扩散模型通过概率方法能够生成高质量的数据。然而，由于需要大量的时间步骤，其生成速度缓慢。为了解决这个限制，最近的模型，如去噪扩散隐式模型 (DDIM)，专注于在不直接建模概率分布的情况下生成样本，而像去噪扩散生成对抗网络 (GAN) 这样的模型将扩散过程与 GAN 结合。在语音合成领域，一种最近的扩散语音合成模型叫做 DiffGAN-TTS，利用了 GAN 的结构，展现了在语音质量和生成速度方面的卓越性能。在本文中，为了进一步提升 DiffGAN-TTS 的性能，我们提出了一个具有两个鉴别器的语音合成模型：一个负责学习逆过程分布的扩散鉴别器和一个负责学习声谱图分布的声谱图鉴别器。

    The diffusion model is capable of generating high-quality data through a probabilistic approach. However, it suffers from the drawback of slow generation speed due to the requirement of a large number of time steps. To address this limitation, recent models such as denoising diffusion implicit models (DDIM) focus on generating samples without directly modeling the probability distribution, while models like denoising diffusion generative adversarial networks (GAN) combine diffusion processes with GANs. In the field of speech synthesis, a recent diffusion speech synthesis model called DiffGAN-TTS, utilizing the structure of GANs, has been introduced and demonstrates superior performance in both speech quality and generation speed. In this paper, to further enhance the performance of DiffGAN-TTS, we propose a speech synthesis model with two discriminators: a diffusion discriminator for learning the distribution of the reverse process and a spectrogram discriminator for learning the distr
    
[^39]: 快速Slate策略优化：超越Plackett-Luce

    Fast Slate Policy Optimization: Going Beyond Plackett-Luce. (arXiv:2308.01566v1 [cs.LG])

    [http://arxiv.org/abs/2308.01566](http://arxiv.org/abs/2308.01566)

    本文介绍了一种快速Slate策略优化方法，通过提出一种新的策略类，可以在大规模决策系统中有效地优化任意奖励函数，结果表明该方法在百万级别动作空间问题上具有很好的效果。

    

    大规模机器学习系统中一个越来越重要的构建模块是返回Slate，即给定一个查询返回有序的项目列表。该技术的应用包括搜索、信息检索和推荐系统。当行动空间很大时，决策系统会限制在特定结构中以快速完成在线查询。本文解决了这些大规模决策系统在给定任意奖励函数下的优化问题。我们将这个学习问题转化为策略优化框架，并提出了一种新的策略类，它源于决策函数的一种新颖放松。这导致了一个简单而高效的学习算法，可以扩展到大规模的动作空间。我们将我们的方法与常用的Plackett-Luce策略类进行比较，并展示了我们的方法在动作空间大小达到百万级别的问题上的有效性。

    An increasingly important building block of large scale machine learning systems is based on returning slates; an ordered lists of items given a query. Applications of this technology include: search, information retrieval and recommender systems. When the action space is large, decision systems are restricted to a particular structure to complete online queries quickly. This paper addresses the optimization of these large scale decision systems given an arbitrary reward function. We cast this learning problem in a policy optimization framework and propose a new class of policies, born from a novel relaxation of decision functions. This results in a simple, yet efficient learning algorithm that scales to massive action spaces. We compare our method to the commonly adopted Plackett-Luce policy class and demonstrate the effectiveness of our approach on problems with action space sizes in the order of millions.
    
[^40]: 在无线网络中的分层联邦学习：剪枝解决带宽稀缺和系统异构性问题

    Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity. (arXiv:2308.01562v1 [eess.SY])

    [http://arxiv.org/abs/2308.01562](http://arxiv.org/abs/2308.01562)

    本研究提出了一种剪枝增强的分层联邦学习（PHFL）方法，用于解决无线网络中的带宽稀缺和系统异构性问题。通过模型剪枝和无线通信的优化，实现了在严格的延迟和能耗约束下收敛速度的最优化。

    

    在实际无线网络中，由于终端用户与中央服务器之间存在多个层级，用户设备的计算和电池能力有限，而服务基站具有固定的带宽。鉴于这些实际约束和系统模型，本文利用模型剪枝，并提出一种适用于异构网络的剪枝增强分层联邦学习（PHFL）方法。我们首先推导出收敛速度的上界，清晰地展示了模型剪枝和客户端与关联基站之间的无线通信的影响。然后，我们联合优化模型剪枝比率、客户端的中央处理器（CPU）频率和传输功率，以在严格的延迟和能耗约束下最小化收敛界的可控项。然而，由于原始问题不是凸问题，我们采用逐步凸逼近（SCA）方法，联合优化参数。

    While a practical wireless network has many tiers where end users do not directly communicate with the central server, the users' devices have limited computation and battery powers, and the serving base station (BS) has a fixed bandwidth. Owing to these practical constraints and system models, this paper leverages model pruning and proposes a pruning-enabled hierarchical federated learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper bound of the convergence rate that clearly demonstrates the impact of the model pruning and wireless communications between the clients and the associated BS. Then we jointly optimize the model pruning ratio, central processing unit (CPU) frequency and transmission power of the clients in order to minimize the controllable terms of the convergence bound under strict delay and energy constraints. However, since the original problem is not convex, we perform successive convex approximation (SCA) and jointly optimize the parameters fo
    
[^41]: 运动规划扩散：基于扩散模型的机器人运动学习与规划

    Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. (arXiv:2308.01557v1 [cs.RO])

    [http://arxiv.org/abs/2308.01557](http://arxiv.org/abs/2308.01557)

    本文提出了一种新的机器人运动学习与规划方法，通过学习扩散模型作为先验知识，可以加速运动规划优化过程。扩散模型能够在高维环境中有效地编码数据的多模态性，并可以直接从任务目标条件下的后验轨迹分布中进行采样。

    

    学习轨迹分布的先验知识可以加快机器人运动规划的优化。在给定先前成功的规划方案的情况下，学习轨迹生成模型作为新规划问题的先验知识是非常理想的。之前的研究提出了几种利用这种先验知识进行运动规划问题引导的方法。可以通过从先验知识中采样初始化，或者在最大后验优化的过程中使用先验分布。在本文中，我们提出了学习扩散模型作为先验知识的方法。然后，我们可以通过利用扩散模型的逆去噪过程，直接从任务目标条件下的后验轨迹分布中采样。此外，最近的研究表明，扩散在高维环境中可以有效地编码数据的多模态性，这对于大量的轨迹数据集非常适用。为了展示我们的方法的有效性，我们将我们提出的方法-运动规划扩散与几种基准方发进行了比较。

    Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multimodality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several ba
    
[^42]: InterAct: 探索将ChatGPT作为合作代理人的潜力

    InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent. (arXiv:2308.01552v1 [cs.AI])

    [http://arxiv.org/abs/2308.01552](http://arxiv.org/abs/2308.01552)

    本研究将OpenAI的ChatGPT集成到具身智能体系统中，通过将ChatGPT分配不同角色并与原始语言模型集成，实现了98%的成功率，并在实际环境中展现了ChatGPT在理解和执行复杂任务方面的能力。

    

    本研究论文探讨了将OpenAI的ChatGPT集成到具身智能体系统中，评估其对交互决策基准的影响。我们引入InterAct这一概念，将其类比于人们根据自己独特的优势扮演角色的概念。在这种方法中，我们给ChatGPT提供各种提示，将其分配为像检查员和分类员这样的多种角色，然后将它们与原始语言模型集成。我们的研究在AlfWorld中展示了98%的显著成功率，AlfWorld是一个模拟家庭环境中包含6个不同任务的基准测试，强调了良好的提示工程的重要性。结果强调了ChatGPT在理解和高效地执行复杂任务方面的能力，为任务规划的进一步发展铺平了道路。

    This research paper delves into the integration of OpenAI's ChatGPT into embodied agent systems, evaluating its influence on interactive decision-making benchmark. Drawing a parallel to the concept of people assuming roles according to their unique strengths, we introduce InterAct. In this approach, we feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model. Our research shows a remarkable success rate of 98% in AlfWorld, which consists of 6 different tasks in a simulated household environment, emphasizing the significance of proficient prompt engineering. The results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.
    
[^43]: MusicLDM：使用节奏同步的混合策略增强文本转音乐生成中的新颖性

    MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies. (arXiv:2308.01546v1 [cs.SD])

    [http://arxiv.org/abs/2308.01546](http://arxiv.org/abs/2308.01546)

    MusicLDM通过稳定扩散和AudioLDM架构，结合重新训练对比语言-音频预训练模型和Hifi-GAN声码器，以解决音乐生成中的挑战。通过节奏同步的混合策略，对训练数据进行增广，提高新颖性并避免抄袭问题。

    

    扩散模型在跨模态生成任务中展现了令人期待的结果，包括文本到图像和文本到音频的生成。然而，由于音乐数据的有限可用性以及与版权和抄袭相关的敏感问题，生成音乐作为一种特殊类型的音频，面临着独特的挑战。在本文中，为了解决这些挑战，我们首先构建了一种最先进的文本到音乐模型MusicLDM，将稳定扩散和AudioLDM架构适应到音乐领域。我们通过对一系列音乐数据样本进行重新训练对比语言-音频预训练模型(CLAP)和Hifi-GAN声码器这些MusicLDM的组成部分进行了实现。然后，为了解决训练数据的限制并避免抄袭，我们利用节拍追踪模型并提出了两种不同的数据增广混合策略：节奏同步音频混合和节奏同步潜在混合，分别通过直接重新组合训练音频或通过潜在嵌入空间重新组合。

    Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respe
    
[^44]: Lode Enhancer: 通过扩展促进关卡的协同创作

    Lode Enhancer: Level Co-creation Through Scaling. (arXiv:2308.01543v1 [cs.LG])

    [http://arxiv.org/abs/2308.01543](http://arxiv.org/abs/2308.01543)

    本文探索了使用AI增强的上采样作为设计辅助工具，在2D游戏关卡设计中实现协同创作。我们使用深度神经网络来将人工降低分辨率的关卡片段上采样，并为此引入了一种能够学习上采样并处理不太常见图块的神经网络架构。经过与设计师的合作研究，我们发现设计师们喜欢这个工具的共同设计过程，并认为它具有潜力推动更多的开发工作。

    

    我们探索了在创建2D游戏关卡时，使用AI增强的上采样作为设计辅助工具。我们使用深度神经网络将来自谜题平台游戏Lode Runner的人工降低分辨率的关卡片段进行上采样。训练得到的网络被整合到一个基于Web的编辑器中，用户可以以3个不同的分辨率（4x4、8x8和16x16）创建和编辑关卡。在任何分辨率上的编辑都会立即传输到其他分辨率上。由于上采样需要发明在较低分辨率下可能不存在的特征，我们训练神经网络来复制这些特征。我们引入了一种神经网络架构，它不仅能够学习上采样，还能够更高优先级地处理不太常见的图块。为了调查这个工具的潜力并指导进一步的开发，我们进行了一个定性研究，与3位设计师协作，了解他们如何使用它。设计师享受与这个工具的共同设计过程，喜欢它的基本概念，并提供了反馈。

    We explore AI-powered upscaling as a design assistance tool in the context of creating 2D game levels. Deep neural networks are used to upscale artificially downscaled patches of levels from the puzzle platformer game Lode Runner. The trained networks are incorporated into a web-based editor, where the user can create and edit levels at three different levels of resolution: 4x4, 8x8, and 16x16. An edit at any resolution instantly transfers to the other resolutions. As upscaling requires inventing features that might not be present at lower resolutions, we train neural networks to reproduce these features. We introduce a neural network architecture that is capable of not only learning upscaling but also giving higher priority to less frequent tiles. To investigate the potential of this tool and guide further development, we conduct a qualitative study with 3 designers to understand how they use it. Designers enjoyed co-designing with the tool, liked its underlying concept, and provided 
    
[^45]: MFIM: 百万像素人脸身份操纵

    MFIM: Megapixel Facial Identity Manipulation. (arXiv:2308.01536v1 [cs.CV])

    [http://arxiv.org/abs/2308.01536](http://arxiv.org/abs/2308.01536)

    MFIM是一种新颖的人脸交换框架，它能够生成高质量的百万像素图像，并且能够有效地将给定图像的身份转换为另一个人的身份，同时保留身份无关的属性。

    

    人脸交换是一项将给定图像的人脸身份更改为另一个人的任务。在这项工作中，我们提出了一种名为百万像素人脸身份操纵（MFIM）的新颖人脸交换框架。人脸交换模型应该能够实现两个目标。首先，它应该能够生成高质量的图像。我们认为，擅长生成百万像素图像的模型可以实现这个目标。然而，要想生成百万像素图像通常会很困难，需要仔细设计模型。因此，我们的模型利用预训练的StyleGAN以GAN-逆向的方式来有效地生成百万像素图像。其次，它应该能够有效地转换给定图像的身份。具体来说，它应该能够将给定图像的身份属性（如脸型和眼睛）积极地转换为另一个人的属性，同时保留身份无关的属性（如姿势和表情）。为了实现这个目标，我们利用了能够捕捉各种面部特征的3DMM模型。

    Face swapping is a task that changes a facial identity of a given image to that of another person. In this work, we propose a novel face-swapping framework called Megapixel Facial Identity Manipulation (MFIM). The face-swapping model should achieve two goals. First, it should be able to generate a high-quality image. We argue that a model which is proficient in generating a megapixel image can achieve this goal. However, generating a megapixel image is generally difficult without careful model design. Therefore, our model exploits pretrained StyleGAN in the manner of GAN-inversion to effectively generate a megapixel image. Second, it should be able to effectively transform the identity of a given image. Specifically, it should be able to actively transform ID attributes (e.g., face shape and eyes) of a given image into those of another person, while preserving ID-irrelevant attributes (e.g., pose and expression). To achieve this goal, we exploit 3DMM that can capture various facial att
    
[^46]: 绕过文本到图像生成模型的概念删除方法

    Circumventing Concept Erasure Methods For Text-to-Image Generative Models. (arXiv:2308.01508v1 [cs.LG])

    [http://arxiv.org/abs/2308.01508](http://arxiv.org/abs/2308.01508)

    本论文研究了绕过文本到图像生成模型中概念删除方法的问题，发现这些方法无法完全删除目标概念，并对其使用的脆弱性提出了质疑。

    

    文本到图像生成模型可以为极其广泛的概念生成逼真的图像，并且它们的使用在普通公众中广泛普及。但是，这些模型存在许多缺点，包括可能生成具有性别暴露内容的图像，未经许可地模仿艺术风格，甚至是深度伪造名人的样貌。因此，为了从文本到图像模型中“擦除”敏感概念，提出了各种方法。在这项工作中，我们研究了五种最近提出的概念删除方法，并显示这些方法都无法完全删除目标概念。具体而言，我们利用了特殊的学习词嵌入的存在，可以在无需对其权重进行任何修改的情况下，从经过处理的模型中检索“删除”概念。我们的结果突出了事后概念删除方法的脆弱性，并对它们在算法工具包中的使用提出了质疑。

    Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. On the flip side, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to "erase" sensitive concepts from text-to-image models. In this work, we examine five recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we leverage the existence of special learned word embeddings that can retrieve "erased" concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit 
    
[^47]: 具有最近邻的极小极大最优$Q$学习

    Minimax Optimal $Q$ Learning with Nearest Neighbors. (arXiv:2308.01490v1 [cs.LG])

    [http://arxiv.org/abs/2308.01490](http://arxiv.org/abs/2308.01490)

    本文提出了两种新的具有最近邻的$Q$学习方法，用于解决连续状态空间下收敛速度差的问题。

    

    $Q$学习是一种常见的无模型强化学习方法。现有的大部分工作集中在分析有限状态和动作空间的$Q$学习。如果状态空间是连续的，那么原始的$Q$学习方法就无法直接使用。(Shah and Xie, 2018) 提出了原始$Q$学习方法的修改版，用最近邻方法估计$Q$值。这种修改使得$Q$学习适用于连续状态空间。该论文指出估计$Q$函数的收敛速度为$\tilde{O}(T^{-1/(d+3)})$，比极小极大下界$\tilde{\Omega}(T^{-1/(d+2)})$慢，说明该方法效率不高。本文提出了两种新的$Q$学习方法，来弥合(Shah and Xie, 2018)中的收敛速度差距，其中一种是离线的，另一种是在线的。尽管我们仍然使用最近邻方法来估计$Q$函数，但算法与(Shah and Xie, 2018)有显著区别。

    $Q$ learning is a popular model free reinforcement learning method. Most of existing works focus on analyzing $Q$ learning for finite state and action spaces. If the state space is continuous, then the original $Q$ learning method can not be directly used. A modification of the original $Q$ learning method was proposed in (Shah and Xie, 2018), which estimates $Q$ values with nearest neighbors. Such modification makes $Q$ learning suitable for continuous state space. (Shah and Xie, 2018) shows that the convergence rate of estimated $Q$ function is $\tilde{O}(T^{-1/(d+3)})$, which is slower than the minimax lower bound $\tilde{\Omega}(T^{-1/(d+2)})$, indicating that this method is not efficient. This paper proposes two new $Q$ learning methods to bridge the gap of convergence rates in (Shah and Xie, 2018), with one of them being offline, while the other is online. Despite that we still use nearest neighbor approach to estimate $Q$ function, the algorithms are crucially different from (Sh
    
[^48]: 在新型游戏数据集上的高效神经超采样

    Efficient neural supersampling on a novel gaming dataset. (arXiv:2308.01483v1 [cs.CV])

    [http://arxiv.org/abs/2308.01483](http://arxiv.org/abs/2308.01483)

    该论文介绍了一种新颖的神经算法，用于在游戏内容上进行超采样，比现有方法高效4倍，同时保持相同的精度水平。此外，还引入了一个新的数据集，填补了当前数据集领域的空白，并可以帮助推动游戏内容超分辨率技术的最新进展。

    

    由于需要更高的分辨率、帧率和逼真度，实时渲染视频游戏变得越来越具有挑战性。超采样已经成为解决这一挑战的有效方法。我们的工作引入了一种新颖的神经算法，用于超采样渲染内容，其效率比现有方法高4倍，同时保持相同的精度水平。此外，我们还引入了一个新的数据集，它提供了辅助模态，例如使用图形渲染功能生成的运动矢量和深度，如视口抖动和多级纹理偏差在不同分辨率下。我们相信，这个数据集填补了当前数据集领域的空白，可以作为一个宝贵的资源，帮助衡量该领域的进展，并推动游戏内容超分辨率技术的最新进展。

    Real-time rendering for video games has become increasingly challenging due to the need for higher resolutions, framerates and photorealism. Supersampling has emerged as an effective solution to address this challenge. Our work introduces a novel neural algorithm for supersampling rendered content that is 4 times more efficient than existing methods while maintaining the same level of accuracy. Additionally, we introduce a new dataset which provides auxiliary modalities such as motion vectors and depth generated using graphics rendering features like viewport jittering and mipmap biasing at different resolutions. We believe that this dataset fills a gap in the current dataset landscape and can serve as a valuable resource to help measure progress in the field and advance the state-of-the-art in super-resolution techniques for gaming content.
    
[^49]: 在马尔可夫采样下，用于随机梯度下降的在线协方差估计

    Online covariance estimation for stochastic gradient descent under Markovian sampling. (arXiv:2308.01481v1 [math.ST])

    [http://arxiv.org/abs/2308.01481](http://arxiv.org/abs/2308.01481)

    本文研究了在马尔可夫采样下的随机梯度下降中的在线重叠批次均值协方差估计器，并证明了其收敛速率为$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$和$O\big(\sqrt{d}\,n^{-1/8}\big)$，分别对应于状态相关和状态无关的马尔可夫采样。这些速率与独立同分布情况下的最佳收敛速率相匹配，并且克服了由于马尔可夫采样而引起的挑战。

    

    我们研究了用于马尔可夫采样下随机梯度下降的在线重叠批次均值协方差估计器。我们证明了协方差估计器的收敛速率分别为$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$和$O\big(\sqrt{d}\,n^{-1/8}\big)$，其中$d$代表维度，$n$表示观测数量或SGD迭代次数。值得注意的是，这些速率与先前由\cite{zhu2021online}在独立同分布($\iid$)情况下建立的最佳收敛速率相匹配，除了对数因子。我们的分析克服了由于马尔可夫采样而产生的重要挑战，引入了额外的误差项和批次均值协方差估计器的复杂依赖关系。此外，我们还建立了SGD动态误差$\ell_2$范数的前四阶矩的收敛速率。

    We study the online overlapping batch-means covariance estimator for Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the convergence rates of the covariance estimator are $O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$ and $O\big(\sqrt{d}\,n^{-1/8}\big)$ under state-dependent and state-independent Markovian sampling, respectively, with $d$ representing dimensionality and $n$ denoting the number of observations or SGD iterations. Remarkably, these rates match the best-known convergence rate previously established for the independent and identically distributed ($\iid$) case by \cite{zhu2021online}, up to logarithmic factors. Our analysis overcomes significant challenges that arise due to Markovian sampling, leading to the introduction of additional error terms and complex dependencies between the blocks of the batch-means covariance estimator. Moreover, we establish the convergence rate for the first four moments of the $\ell_2$ norm of the error of SGD dynamics u
    
[^50]: 可解释的机器学习用于发现：统计挑战和机遇

    Interpretable Machine Learning for Discovery: Statistical Challenges \& Opportunities. (arXiv:2308.01475v1 [stat.ML])

    [http://arxiv.org/abs/2308.01475](http://arxiv.org/abs/2308.01475)

    可解释的机器学习技术被广泛用于处理大数据集、可视化预测和数据驱动的发现，该论文回顾了这一领域并探讨了验证发现的挑战。

    

    新技术导致了许多科学领域和行业的庞大、复杂的数据集。人们经常使用机器学习技术来处理、可视化和预测这些大数据，并通过可解释的机器学习模型和技术来进行数据驱动的发现。本文讨论和回顾了可解释的机器学习领域，特别关注这些技术在从大数据集中生成新知识或进行发现时的应用。我们概述了可解释的机器学习在监督和无监督场景下可以实现的发现类型。此外，我们重点讨论了如何以数据驱动的方式验证这些发现，以促进对机器学习系统的信任和科学中的可重复性。我们讨论了验证的挑战。

    New technologies have led to vast troves of large and complex datasets across many scientific domains and industries. People routinely use machine learning techniques to not only process, visualize, and make predictions from this big data, but also to make data-driven discoveries. These discoveries are often made using Interpretable Machine Learning, or machine learning models and techniques that yield human understandable insights. In this paper, we discuss and review the field of interpretable machine learning, focusing especially on the techniques as they are often employed to generate new knowledge or make discoveries from large data sets. We outline the types of discoveries that can be made using Interpretable Machine Learning in both supervised and unsupervised settings. Additionally, we focus on the grand challenge of how to validate these discoveries in a data-driven manner, which promotes trust in machine learning systems and reproducibility in science. We discuss validation f
    
[^51]: 反向稳定扩散：生成该图像所使用的提示是什么？

    Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])

    [http://arxiv.org/abs/2308.01472](http://arxiv.org/abs/2308.01472)

    本论文介绍了一种新的任务，即在给定由生成扩散模型生成的图像的情况下预测文本提示。为了解决这个问题，作者结合了多种白盒和黑盒模型，提出了一个新颖的学习框架，该框架能够生成改进的提示，并采用课程学习和无监督领域自适应核学习方法来进一步提高方法的性能。

    

    文本到图像扩散模型，如稳定扩散，最近吸引了许多研究人员的兴趣，反向扩散过程在更好地理解生成过程和如何设计提示以获得所需图像方面起着重要作用。为此，我们引入了一种新的任务，即在给定由生成扩散模型生成的图像的情况下预测文本提示。我们结合了一系列白盒和黑盒模型（有和无对扩散网络权重进行访问）来处理所提出的任务。我们提出了一个新颖的学习框架，包括联合提示回归和多标签词汇分类目标，生成改进的提示。为了进一步改进我们的方法，我们采用了一个课程学习过程，促进了具有更低标注噪声（即更好对齐）的图像提示对的学习，并且使用相似性进行无监督领域自适应核学习方法。

    Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities b
    
[^52]: 自动驾驶中的隐式占用流场应用于感知和预测

    Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving. (arXiv:2308.01471v1 [cs.CV])

    [http://arxiv.org/abs/2308.01471](http://arxiv.org/abs/2308.01471)

    该论文提出了一种统一的感知和未来预测方法，利用隐式占用流场来表示自动驾驶车辆对周围环境的感知并预测其他交通参与者的行为。这种方法避免了不必要的计算和信息丢失的问题。

    

    自动驾驶车辆必须能够感知周围环境并预测其他交通参与者的未来行为。现有的方法要么进行目标检测，然后对检测到的目标进行轨迹预测，要么预测整个场景的密集占用和流格。前者由于效率原因需要保持检测数量较少，从而牺牲了对象的回忆率，存在安全问题。后者由于输出格的高维性和完全卷积网络固有的有限感受野而计算成本高。此外，这两种方法都使用了大量的计算资源来预测可能永远不会被运动规划器查询的区域或对象。鉴于这一点，我们提出了一种统一的感知和未来预测方法，利用单个神经网络隐式表示随时间变化的占用和流动。我们的方法避免了不必要的计算，因为它可以直接被运动规划器查询。

    A self-driving vehicle (SDV) must be able to perceive its surroundings and predict the future behavior of other traffic participants. Existing works either perform object detection followed by trajectory forecasting of the detected objects, or predict dense occupancy and flow grids for the whole scene. The former poses a safety concern as the number of detections needs to be kept low for efficiency reasons, sacrificing object recall. The latter is computationally expensive due to the high-dimensionality of the output grid, and suffers from the limited receptive field inherent to fully convolutional networks. Furthermore, both approaches employ many computational resources predicting areas or objects that might never be queried by the motion planner. This motivates our unified approach to perception and future prediction that implicitly represents occupancy and flow over time with a single neural network. Our method avoids unnecessary computation, as it can be directly queried by the mo
    
[^53]: VertexSerum: 针对链路推理的图神经网络毒化攻击

    VertexSerum: Poisoning Graph Neural Networks for Link Inference. (arXiv:2308.01469v1 [cs.LG])

    [http://arxiv.org/abs/2308.01469](http://arxiv.org/abs/2308.01469)

    VertexSerum是一种针对链路推理的图神经网络毒化攻击，通过放大链接连接性泄漏来增加图链接窃取的效果，并提出了一种可以嵌入到链接检测网络中的注意力机制。在实验中，VertexSerum在四个真实世界数据集和三种不同的GNN结构上平均提高了9.8％的AUC分数，且在黑盒和在线学习环境中均表现出有效性。

    

    图神经网络（GNNs）在利用图结构数据的各种应用中（如社交分析和欺诈检测）取得了出色的性能。图的链接（例如社交关系和交易历史）是敏感和有价值的信息，使用GNNs时会引起隐私问题。为了利用这些漏洞，我们提出了VertexSerum，一种新颖的图毒化攻击，通过放大链接连接性泄漏来增加图链接窃取的效果。为了更准确地推断节点邻接关系，我们提出了一种可以嵌入到链接检测网络中的注意力机制。我们的实验结果表明，在四个真实世界数据集和三种不同的GNN结构上，VertexSerum的AUC分数平均提高了9.8％，显著优于SOTA链路推理攻击方法。此外，我们的实验结果还验证了VertexSerum在黑盒和在线学习环境中的有效性。

    Graph neural networks (GNNs) have brought superb performance to various applications utilizing graph structural data, such as social analysis and fraud detection. The graph links, e.g., social relationships and transaction history, are sensitive and valuable information, which raises privacy concerns when using GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel graph poisoning attack that increases the effectiveness of graph link stealing by amplifying the link connectivity leakage. To infer node adjacency more accurately, we propose an attention mechanism that can be embedded into the link detection network. Our experiments demonstrate that VertexSerum significantly outperforms the SOTA link inference attack, improving the AUC scores by an average of $9.8\%$ across four real-world datasets and three different GNN structures. Furthermore, our experiments reveal the effectiveness of VertexSerum in both black-box and online learning settings, further validating its a
    
[^54]: 一个用于土木工程结构的数字孪生框架

    A digital twin framework for civil engineering structures. (arXiv:2308.01445v1 [math.NA])

    [http://arxiv.org/abs/2308.01445](http://arxiv.org/abs/2308.01445)

    本研究提出了一个用于土木工程结构的预测数字孪生方法，它采用概率图模型编码资产孪生耦合动态系统，通过深度学习模型同化感测数据来提供实时的结构健康诊断，不断更新数字孪生状态并用于优化维护和管理规划。

    

    数字孪生概念为土木工程系统的基于条件和预测维护范式的推进提供了有吸引力的机会，从而实现降低生命周期成本、增加系统安全性和提高系统可用性。本文提出了一种预测性的数字孪生方法，用于土木工程结构的健康监测、维护和管理规划。资产孪生耦合动态系统采用概率图模型进行编码，从而可以考虑到所有相关的不确定性来源。特别是，采用动态贝叶斯网络对时间重复的观测-决策流进行建模。通过将感测数据与深度学习模型进行同化，实时的结构健康诊断得以提供。数字孪生状态以顺序贝叶斯推理的方式不断更新。然后利用这些信息来动态决策下的维护和管理行动的最优规划。

    The digital twin concept represents an appealing opportunity to advance condition-based and predictive maintenance paradigms for civil engineering systems, thus allowing reduced lifecycle costs, increased system safety, and increased system availability. This work proposes a predictive digital twin approach to the health monitoring, maintenance, and management planning of civil engineering structures. The asset-twin coupled dynamical system is encoded employing a probabilistic graphical model, which allows all relevant sources of uncertainty to be taken into account. In particular, the time-repeating observations-to-decisions flow is modeled using a dynamic Bayesian network. Real-time structural health diagnostics are provided by assimilating sensed data with deep learning models. The digital twin state is continually updated in a sequential Bayesian inference fashion. This is then exploited to inform the optimal planning of maintenance and management actions within a dynamic decision-
    
[^55]: 室内空气质量近似的新颖基于物理的机器学习模型

    Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations. (arXiv:2308.01438v1 [cs.LG])

    [http://arxiv.org/abs/2308.01438](http://arxiv.org/abs/2308.01438)

    本研究提出了六种新颖的基于物理学的机器学习模型，用于准确近似室内污染物浓度。所提出的模型结合了物理学中的状态空间概念、门控循环单元和分解技术。通过在加利福尼亚州一栋商业建筑中五个办公室收集的数据进行验证，结果表明所提出的模型具有较低的复杂度。

    

    成本低廉的传感器能够实时捕捉到不同污染物浓度、室内/室外湿度和温度等与空气质量相关的多种模态。机器学习模型能够提前近似室内空气质量。毫无疑问，准确的室内空气质量近似有助于提供健康的室内环境，优化相关能耗，并提供人体舒适度。然而，设计一个能够捕捉所谓的问题物理学领域知识的机器学习架构至关重要。在本研究中，我们提出了六种新颖的基于物理学的机器学习模型，用于准确近似室内污染物浓度。所提出的模型包括物理学中的状态空间概念、门控循环单元和分解技术的巧妙组合。所提出的模型使用从加利福尼亚州一栋商业建筑中五个办公室收集的数据进行了验证。结果显示，所提出的模型较为简单。

    Cost-effective sensors are capable of real-time capturing a variety of air quality-related modalities from different pollutant concentrations to indoor/outdoor humidity and temperature. Machine learning (ML) models are capable of performing air-quality "ahead-of-time" approximations. Undoubtedly, accurate indoor air quality approximation significantly helps provide a healthy indoor environment, optimize associated energy consumption, and offer human comfort. However, it is crucial to design an ML architecture to capture the domain knowledge, so-called problem physics. In this study, we propose six novel physics-based ML models for accurate indoor pollutant concentration approximations. The proposed models include an adroit combination of state-space concepts in physics, Gated Recurrent Units, and Decomposition techniques. The proposed models were illustrated using data collected from five offices in a commercial building in California. The proposed models are shown to be less complex, 
    
[^56]: 电力市场的价格感知深度学习

    Price-Aware Deep Learning for Electricity Markets. (arXiv:2308.01436v1 [cs.LG])

    [http://arxiv.org/abs/2308.01436](http://arxiv.org/abs/2308.01436)

    本文提出了一种价格感知深度学习方法，通过将电力市场清算优化嵌入到深度学习层中，平衡预测误差和定价误差，提高公平性，并控制价格误差的空间分布。

    

    尽管深度学习逐渐渗透到运营规划中，但其固有的预测误差可能会显著影响电力价格。本文考察了预测误差如何传播到电力价格中，揭示了拥挤电力系统中显著的定价误差及其空间差异。为了提高公平性，我们提出将电力市场清算优化嵌入到深度学习层中。通过此层的区分，可以在预测误差和定价误差之间平衡，而不仅仅是最小化预测误差。该层隐式地优化公平性，并控制系统中价格误差的空间分布。我们展示了在风力发电预测和短期电力市场清算中的价格感知深度学习。

    While deep learning gradually penetrates operational planning, its inherent prediction errors may significantly affect electricity prices. This letter examines how prediction errors propagate into electricity prices, revealing notable pricing errors and their spatial disparity in congested power systems. To improve fairness, we propose to embed electricity market-clearing optimization as a deep learning layer. Differentiating through this layer allows for balancing between prediction and pricing errors, as oppose to minimizing prediction errors alone. This layer implicitly optimizes fairness and controls the spatial distribution of price errors across the system. We showcase the price-aware deep learning in the nexus of wind power forecasting and short-term electricity market clearing.
    
[^57]: COVID-VR:一种使用体积渲染计算机断层扫描的深度学习COVID-19分类模型

    COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography. (arXiv:2308.01433v1 [eess.IV])

    [http://arxiv.org/abs/2308.01433](http://arxiv.org/abs/2308.01433)

    COVID-VR是一种使用体积渲染图像识别肺部疾病的深度学习模型。相较于其他方法，COVID-VR在整个肺部提供了综合视图，并能有效识别肺部病变。

    

    COVID-19流行对全球的医疗系统提出了众多挑战。鉴于肺部感染在COVID-19患者中普遍存在，胸部计算机断层扫描（CT）经常被用作识别COVID-19病情和其他肺部疾病的替代方法。深度学习结构利用CT扫描切片作为分类模型的输入，自动识别肺部疾病类型。本文介绍了COVID-VR，一种基于从多个角度捕获的肺部体积渲染图像分类肺部疾病的新方法，从而在每个图像中提供对整个肺部的综合视图。为了评估我们提出的方法的有效性，我们将其与使用合作医院获取的私有数据和公开可用的数据集的竞争策略进行了比较。结果表明，我们的方法能够有效识别肺部病变并

    The COVID-19 pandemic presented numerous challenges to healthcare systems worldwide. Given that lung infections are prevalent among COVID-19 patients, chest Computer Tomography (CT) scans have frequently been utilized as an alternative method for identifying COVID-19 conditions and various other types of pulmonary diseases. Deep learning architectures have emerged to automate the identification of pulmonary disease types by leveraging CT scan slices as inputs for classification models. This paper introduces COVID-VR, a novel approach for classifying pulmonary diseases based on volume rendering images of the lungs captured from multiple angles, thereby providing a comprehensive view of the entire lung in each image. To assess the effectiveness of our proposal, we compared it against competing strategies utilizing both private data obtained from partner hospitals and a publicly available dataset. The results demonstrate that our approach effectively identifies pulmonary lesions and perfo
    
[^58]: ChatMOF: 一种自主人工智能系统用于预测和生成金属-有机骨架

    ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])

    [http://arxiv.org/abs/2308.01423](http://arxiv.org/abs/2308.01423)

    ChatMOF是一种自主AI系统，用于预测和生成金属-有机骨架。通过利用大规模语言模型，它能够从文本输入中提取关键细节，并提供适当的回应。该系统通过组合代理、工具包和评估器的核心组件，实现了数据检索、性质预测和结构生成等多个任务。研究进一步展示了在材料科学中使用大型语言模型的优势和潜力。

    

    ChatMOF是一个自主人工智能系统，用于预测和生成金属-有机骨架（MOFs）。通过利用大规模语言模型（gpt-3.5-turbo），ChatMOF从文本输入中提取关键细节并提供适当的回应，从而消除了对刚性结构化查询的需求。该系统由三个核心组件（即代理、工具包和评估器）组成，形成一个强大的流水线，管理多种任务，包括数据检索、性质预测和结构生成。该研究进一步探讨了在材料科学中使用大型语言模型（LLMs）人工智能系统的优点和限制，并展示了其对未来发展的变革潜力。

    ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
    
[^59]: 正则化、提前停止和梦想：一种处理泛化和过拟合的类Hopfield设置

    Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting. (arXiv:2308.01421v1 [cs.LG])

    [http://arxiv.org/abs/2308.01421](http://arxiv.org/abs/2308.01421)

    这项工作提出了一种处理神经网络泛化和过拟合的新方法，通过正则化损失函数和提前停止策略来优化网络参数，并通过数值实验验证了该方法的有效性。

    

    在这项工作中，我们从机器学习的角度来处理吸引子神经网络：通过对正则化损失函数进行梯度下降来寻找最优网络参数。在这个框架中，最优的神经元交互矩阵被证明是一类通过迭代应用某些取消学习协议修订的Hebbian核矩阵。值得注意的是，取消学习步骤的数量被证明与损失函数的正则化超参数和训练时间有关。因此，我们可以设计避免过拟合的策略，这些策略可以用交互矩阵的代数性质来描述，或者等价地用正则化调整和提前停止策略来描述。还研究了这些吸引子网络的泛化能力：针对随机合成数据集获得了分析结果，随后用数值实验来验证了所得到的整体情况。

    In this work we approach attractor neural networks from a machine learning perspective: we look for optimal network parameters by applying a gradient descent over a regularized loss function. Within this framework, the optimal neuron-interaction matrices turn out to be a class of matrices which correspond to Hebbian kernels revised by iteratively applying some unlearning protocols. Remarkably, the number of unlearning steps is proved to be related to the regularization hyperparameters of the loss function and to the training time. Thus, we can design strategies to avoid overfitting that are formulated in terms of the algebraic properties of the interaction matrix, or, equivalently, in terms of regularization tuning and early-stopping strategies. The generalization capabilities of these attractor networks are also investigated: analytical results are obtained for random synthetic datasets, next, the emerging picture is corroborated by numerical experiments that highlight the existence o
    
[^60]: SAP-sLDA: 一个可解释的界面用于探索非结构化文本

    SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text. (arXiv:2308.01420v1 [cs.CL])

    [http://arxiv.org/abs/2308.01420](http://arxiv.org/abs/2308.01420)

    SAP-sLDA提出了一种半监督人类参与的LDA方法，可以在低维投影中学习主题并保留文档之间的语义关系。

    

    一种探索文本语料库的常见方法是通过文档的低维投影，希望主题相似的文档能够在投影空间中聚类在一起。然而，常用的用于降维文本语料库的算法，如潜在狄利克雷分配（LDA），往往会产生无法捕捉文档相似性的人类概念的投影。我们提出了一种半监督人类参与的基于LDA的方法来学习主题，在低维投影中保留语义上有意义的文档之间的关系。在合成语料库上，我们的方法比仅提供少量标签的基线方法产生更易解释的投影。在实际语料库上，我们获得了类似的结果。

    A common way to explore text corpora is through low-dimensional projections of the documents, where one hopes that thematically similar documents will be clustered together in the projected space. However, popular algorithms for dimensionality reduction of text corpora, like Latent Dirichlet Allocation (LDA), often produce projections that do not capture human notions of document similarity. We propose a semi-supervised human-in-the-loop LDA-based method for learning topics that preserve semantically meaningful relationships between documents in low-dimensional projections. On synthetic corpora, our method yields more interpretable projections than baseline methods with only a fraction of labels provided. On a real corpus, we obtain qualitatively similar results.
    
[^61]: 使用图神经网络预测具有溢出效应的多变量实现波动

    Graph Neural Networks for Forecasting Multivariate Realized Volatility with Spillover Effects. (arXiv:2308.01419v1 [q-fin.ST])

    [http://arxiv.org/abs/2308.01419](http://arxiv.org/abs/2308.01419)

    本研究提出了一种使用图神经网络建模和预测多变量实现波动的新方法，能够有效地捕捉非线性关系，并利用多跳邻居的溢出效应提高实现波动的预测准确性。此外，使用拟似似然损失进行训练可以显著提升模型性能。

    

    我们提出了一种新的方法，使用定制的图神经网络来建模和预测多变量的实际波动，以整合股票间的溢出效应。所提出的模型具有将多跳邻居的溢出效应纳入考虑、捕捉非线性关系和使用不同损失函数进行灵活训练的优势。我们的实证结果提供了有力的证据，表明仅考虑多跳邻居的溢出效应并不能在预测准确性方面明显优势。然而，建模非线性溢出效应可以提高实现波动的预测准确性，尤其是对于最长一周的短期预测。此外，我们的结果不断表明，使用拟似似然损失进行训练比常用的均方误差具有明显的模型性能改善。在其他设置中进行的全面一系列实证评估证实了该模型的稳健性。

    We present a novel methodology for modeling and forecasting multivariate realized volatilities using customized graph neural networks to incorporate spillover effects across stocks. The proposed model offers the benefits of incorporating spillover effects from multi-hop neighbors, capturing nonlinear relationships, and flexible training with different loss functions. Our empirical findings provide compelling evidence that incorporating spillover effects from multi-hop neighbors alone does not yield a clear advantage in terms of predictive accuracy. However, modeling nonlinear spillover effects enhances the forecasting accuracy of realized volatilities, particularly for short-term horizons of up to one week. Moreover, our results consistently indicate that training with the Quasi-likelihood loss leads to substantial improvements in model performance compared to the commonly-used mean squared error. A comprehensive series of empirical evaluations in alternative settings confirm the robus
    
[^62]: 一个有效的数据创建流水线用于为大型语言模型生成高质量的金融指令数据

    An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model. (arXiv:2308.01415v1 [cs.CL])

    [http://arxiv.org/abs/2308.01415](http://arxiv.org/abs/2308.01415)

    本文提出了一个精心设计的数据创建流水线，通过与金融专家的对话和反馈，在大型语言模型中生成了一个高质量的金融指令数据集。实验结果表明，该方法在生成准确、相关和金融风格响应方面取得了显著进展。

    

    在大型语言模型的初期阶段，为金融相关任务精调大型语言模型生成高质量的金融数据集非常关键。因此，本文提出了一个精心设计的数据创建流水线来实现这一目的。具体而言，我们使用ChatGPT引发了一个AI投资者和金融专家之间的对话，并融入了人工金融专家的反馈，从而改进了数据集。该流水线产生了一个由103k个多轮对话组成的稳定的指令精调数据集。通过采用外部的GPT-4作为评判者，在该数据集上进行了大量实验来评估模型的性能。有希望的实验结果验证了我们的方法在生成准确、相关和金融风格响应方面取得了显著进展，从而为金融领域的应用提供了一个强大的工具。

    At the beginning era of large language model, it is quite critical to generate a high-quality financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model's performance by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models, and thus providing a powerful tool for applications within the financial sector.
    
[^63]: 蒙骗：基于文本的游戏中的欺骗与合作研究

    Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models. (arXiv:2308.01404v1 [cs.CL])

    [http://arxiv.org/abs/2308.01404](http://arxiv.org/abs/2308.01404)

    通过引入一款名为"Hoodwinked"的文本游戏，研究了当前语言模型是否具有欺骗和识别谎言的能力。实验证据表明，杀手经常否认罪行并指责他人，导致投票结果受到影响。更先进的模型在杀手效果上表现出优势。实验证据表明，这种改进是通过在讨论中更强的欺骗能力实现的。

    

    当前的语言模型是否具有欺骗和识别谎言的能力？我们通过引入一款名为“Hoodwinked”的基于文本的游戏，受到“黑帮”和“谁是卧底”游戏的启发，来研究这个问题。玩家们被锁在一个房子里，必须找到一把钥匙才能逃脱，但其中一个玩家被派任务杀死其他人。每次发生谋杀，幸存的玩家们会进行自然语言讨论，然后投票将一名玩家放逐出游戏。我们使用GPT-3、GPT-3.5和GPT-4操控代理进行实验，并发现了欺骗和识别谎言的能力证据。杀手经常否认自己的罪行并指责他人，导致投票结果受到可测量的影响。更先进的模型在24个两两比较中的18个中表现出更高的杀手效果，超越了更小的模型。次要指标提供了证据，表明这种改进并不是通过不同的行动实现的，而是通过在讨论中更强的欺骗能力实现的。总的来说，我们发现了实质性的创新。

    Are current language models capable of deception and lie detection? We study this question by introducing a text-based game called $\textit{Hoodwinked}$, inspired by $\textit{Mafia}$ and $\textit{Among Us}$. Players are locked in a house and must find a key to escape, but one player is tasked with killing the others. Each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. We conduct experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger deception capabilities during discussions. Overall, we find substantial
    
[^64]: 通过语言学习对世界建模

    Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])

    [http://arxiv.org/abs/2308.01399](http://arxiv.org/abs/2308.01399)

    本论文提出了一种通过语言学习对世界进行建模的方法，利用语言帮助代理器预测未来并进行行动。通过学习多模态世界模型，代理器可以预测未来的文本和图像表示，并在模型回滚中进行行动。

    

    为了与人类在世界中相互作用，代理器需要理解人们使用的多样化的语言类型，并将其与视觉世界关联起来，并基于语言行动。虽然当前的代理器可以通过任务奖励学习执行简单的语言指令，但我们的目标是建立可以利用传达一般知识、描述世界状态、提供互动反馈等多样化语言的代理器。我们的核心思想是语言帮助代理器预测未来：将会被观察到什么、世界将如何运行以及哪些情况将获得奖励。这个观点将语言理解与未来预测统一为一个强大的自监督学习目标。我们提出了Dynalang，一种学习多模态世界模型的代理器，它可以预测未来的文本和图像表示，并在想像的模型回滚中学习行动。与只使用语言预测动作的传统代理器不同，Dynalang通过过去的语言还可以获取丰富的语言理解能力。

    To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to 
    
[^65]: OpenFlamingo: 一个用于训练大型自回归视觉语言模型的开源框架

    OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. (arXiv:2308.01390v1 [cs.CV])

    [http://arxiv.org/abs/2308.01390](http://arxiv.org/abs/2308.01390)

    OpenFlamingo是一个开源框架，用于训练大型自回归视觉语言模型。它在多个数据集上表现良好，达到了对应模型性能的80%至89%。

    

    我们介绍了OpenFlamingo，这是一系列自回归的视觉语言模型，参数范围从3B到9B。 OpenFlamingo是一个持续努力的项目，旨在复制DeepMind的Flamingo模型的开源版本。在七个视觉语言数据集上，OpenFlamingo模型的性能介于对应的Flamingo性能的80%至89%之间。本技术报告介绍了我们的模型、训练数据、超参数和评估套件。我们在https://github.com/mlfoundations/open_flamingo上分享我们的模型和代码。

    We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at https://github.com/mlfoundations/open_flamingo.
    
[^66]: 通过优化的单次多框检测和强化学习追踪士兵

    Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning. (arXiv:2308.01389v1 [cs.RO])

    [http://arxiv.org/abs/2308.01389](http://arxiv.org/abs/2308.01389)

    本论文的主要创新是通过优化的单次多框检测和强化学习追踪士兵，实现了使用DeepRacer构建一个自主系统。通过使用SSD Lite替代SSD，我们在推理速度上取得了显著提升，而准确性并未受到影响。

    

    如今，由于在战场上和解决各种现实世界的挑战都有许多潜在应用，自动驾驶汽车正变得越来越受关注。我们项目的主要目标是使用DeepRacer构建一个自主系统，该系统将在士兵（项目中的特定人物）在任何方向移动时追踪他们。实现此项目的两个主要组成部分是优化的单次多框检测（SSD）目标检测模型和强化学习（RL）模型。我们使用SSD Lite而不是SSD完成了任务，并最终比较了SSD、具有神经计算棒（NCS）的SSD和SSD Lite的结果。实验结果表明SSD Lite在这三种技术中表现更好，并且在不降低准确性的情况下，推理速度提高了约2-3倍。

    Nowadays, autonomous cars are gaining traction due to their numerous potential applications on battlefields and in resolving a variety of other real-world challenges. The main goal of our project is to build an autonomous system using DeepRacer which will follow a specific person (for our project, a soldier) when they will be moving in any direction. Two main components to accomplish this project is an optimized Single-Shot Multibox Detection (SSD) object detection model and a Reinforcement Learning (RL) model. We accomplished the task using SSD Lite instead of SSD and at the end, compared the results among SSD, SSD with Neural Computing Stick (NCS), and SSD Lite. Experimental results show that SSD Lite gives better performance among these three techniques and exhibits a considerable boost in inference speed (~2-3 times) without compromising accuracy.
    
[^67]: 计算长曝光移动摄影

    Computational Long Exposure Mobile Photography. (arXiv:2308.01379v1 [cs.CV])

    [http://arxiv.org/abs/2308.01379](http://arxiv.org/abs/2308.01379)

    本文提出了一种计算爆发摄影系统，可以在手持智能手机相机应用中实现长曝光摄影的效果，包括前景模糊和背景模糊。通过检测和分割显著主体，并跟踪场景中的动态元素，系统能够自动合成出令人惊叹的图像。

    

    长曝光摄影产生令人惊叹的图像，用运动模糊来表现场景中的移动元素。它通常分为两种模式，一种是产生前景模糊效果，一种是产生背景模糊效果。传统上，前景模糊图像是在三脚架上的相机上拍摄的，描绘出模糊的移动前景元素（如丝绸般的水流或光迹），而背景则呈现出完全清晰的背景景观。背景模糊图像，也称为跟拍摄影，是在相机追踪移动对象时拍摄的，以产生一个清晰的主体图像和一个因相对运动而模糊的背景。这两种技术都非常具有挑战性，需要额外的设备和高级技能。在本文中，我们描述了一种在手持智能手机相机应用中运行的计算爆发摄影系统，可以在按下快门按钮时完全自动地实现这些效果。我们的方法首先检测和分割显著主体。我们跟踪场景中的所有动态元素，以估计它们在图像序列中的运动轨迹，并通过对图像序列的帧进行融合，生成合成图像。

    Long exposure photography produces stunning imagery, representing moving elements in a scene with motion-blur. It is generally employed in two modalities, producing either a foreground or a background blur effect. Foreground blur images are traditionally captured on a tripod-mounted camera and portray blurred moving foreground elements, such as silky water or light trails, over a perfectly sharp background landscape. Background blur images, also called panning photography, are captured while the camera is tracking a moving subject, to produce an image of a sharp subject over a background blurred by relative motion. Both techniques are notoriously challenging and require additional equipment and advanced skills. In this paper, we describe a computational burst photography system that operates in a hand-held smartphone camera app, and achieves these effects fully automatically, at the tap of the shutter button. Our approach first detects and segments the salient subject. We track the sce
    
[^68]: 可解释的深度学习用于肿瘤动力建模和使用神经-ODE进行整体生存预测

    Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE. (arXiv:2308.01362v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.01362](http://arxiv.org/abs/2308.01362)

    该论文介绍了一种可解释的深度学习方法，使用神经-ODE进行肿瘤动力建模和整体生存预测。该方法能够从截断数据中进行无偏预测，并提供了一种融合多模态数据的有原则的方式。

    

    虽然肿瘤动力建模已被广泛应用于支持肿瘤药物的开发，但仍然需要增加预测能力，实现个性化治疗并改善决策。我们提出使用肿瘤动力神经-ODE（TDNODE）作为一种药理学信息的神经网络，以从纵向肿瘤大小数据中实现模型发现。我们展示了TDNODE在克服现有模型的一个关键限制上的能力，即能够从截断数据中进行无偏预测。编码器-解码器架构设计用于表达具有时间的广义齐次性这一基本特性的基础动力学定律。因此，建模形式使得编码器输出可以被解释为动力学速率指标，其中倒数时间作为物理单位。我们展示了生成的指标可以高准确度地用于预测患者的整体生存。所提出的建模形式为融合多模态数据提供了一个有原则的方式。

    While tumor dynamic modeling has been widely applied to support the development of oncology drugs, there remains a need to increase predictivity, enable personalized therapy, and improve decision-making. We propose the use of Tumor Dynamic Neural-ODE (TDNODE) as a pharmacology-informed neural network to enable model discovery from longitudinal tumor size data. We show that TDNODE overcomes a key limitation of existing models in its ability to make unbiased predictions from truncated data. The encoder-decoder architecture is designed to express an underlying dynamical law which possesses the fundamental property of generalized homogeneity with respect to time. Thus, the modeling formalism enables the encoder output to be interpreted as kinetic rate metrics, with inverse time as the physical unit. We show that the generated metrics can be used to predict patients' overall survival (OS) with high accuracy. The proposed modeling formalism provides a principled way to integrate multimodal d
    
[^69]: 压缩和分布式最小二乘回归：收敛速度及其在联邦学习中的应用

    Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning. (arXiv:2308.01358v1 [cs.LG])

    [http://arxiv.org/abs/2308.01358](http://arxiv.org/abs/2308.01358)

    本文研究了压缩对分布式和联邦学习中随机梯度算法的影响，通过比较不同的无偏压缩操作符的收敛速度，超越了经典的最坏情况分析。针对最小二乘回归，我们提出了一个随机逼近算法，并考虑了随机场的一般假设和噪声协方差的限制，以分析各种随机化机制。

    

    本文研究了在机器学习中广泛应用的分布式和联邦学习中，压缩对随机梯度算法的影响。我们强调了几种无偏压缩操作符之间的收敛速度差异，这些操作符都满足相同的方差条件，从而超越了经典的最坏情况分析。为此，我们专注于最小二乘回归（LSR）的情况，并分析了一个依赖于随机场的最小二乘回归的随机逼近算法。我们对随机场的一般性假设进行了详细分析（特别是期望的Hölder正则性）并对噪声协方差进行了限制，以便分析各种随机化机制，包括压缩。然后，我们将结果扩展到联邦学习的情况下。具体而言，我们强调了对加性噪声的协方差𝖢𝖠𝖭𝖨𝖠对收敛性的影响。

    In this paper, we investigate the impact of compression on stochastic gradient algorithms for machine learning, a technique widely used in distributed and federated learning. We underline differences in terms of convergence rates between several unbiased compression operators, that all satisfy the same condition on their variance, thus going beyond the classical worst-case analysis. To do so, we focus on the case of least-squares regression (LSR) and analyze a general stochastic approximation algorithm for minimizing quadratic functions relying on a random field. We consider weak assumptions on the random field, tailored to the analysis (specifically, expected H\"older regularity), and on the noise covariance, enabling the analysis of various randomizing mechanisms, including compression. We then extend our results to the case of federated learning.  More formally, we highlight the impact on the convergence of the covariance $\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced 
    
[^70]: EmbeddingTree：嵌入式中实体特征的层次探索

    EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding. (arXiv:2308.01329v1 [cs.LG])

    [http://arxiv.org/abs/2308.01329](http://arxiv.org/abs/2308.01329)

    本文提出了一种用于嵌入学习的层次探索算法EmbeddingTree和相应的可视化工具，能够解释具有语义的实体特征和嵌入向量之间的关联，并通过实验验证了其有效性。

    

    嵌入学习将离散的数据实体转换为连续的数值表示，编码了实体的特征/属性。尽管有多种嵌入学习算法报告了出色的性能，但很少有工作投入到对学习到的嵌入空间中的特征如何编码的结构解释。本文提出了EmbeddingTree，一种层次嵌入探索算法，将实体特征的语义与较难解释的嵌入向量相关联。还开发了一种基于EmbeddingTree的交互式可视化工具来探索高维嵌入。该工具可帮助用户发现数据实体的微妙特征，在嵌入训练中执行特征去噪/注入，并为未见实体生成嵌入。我们通过对工业规模的商户数据和公共的30Music听歌/播放列表数据集生成的嵌入来证明EmbeddingTree和我们的可视化工具的有效性。

    Embedding learning transforms discrete data entities into continuous numerical representations, encoding features/properties of the entities. Despite the outstanding performance reported from different embedding learning algorithms, few efforts were devoted to structurally interpreting how features are encoded in the learned embedding space. This work proposes EmbeddingTree, a hierarchical embedding exploration algorithm that relates the semantics of entity features with the less-interpretable embedding vectors. An interactive visualization tool is also developed based on EmbeddingTree to explore high-dimensional embeddings. The tool helps users discover nuance features of data entities, perform feature denoising/injecting in embedding training, and generate embeddings for unseen entities. We demonstrate the efficacy of EmbeddingTree and our visualization tool through embeddings generated for industry-scale merchant data and the public 30Music listening/playlists dataset.
    
[^71]: Careful Whisper - 利用自动语音识别的进展来进行健壮且易解释的失语症亚型分类

    Careful Whisper -- leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification. (arXiv:2308.01327v1 [cs.SD])

    [http://arxiv.org/abs/2308.01327](http://arxiv.org/abs/2308.01327)

    本文提出了一种利用自动语音识别对失语症亚型进行分类的方法，通过结合不同模型的技术，能够利用声音录音自动识别并评估言语障碍。该方法在区分失语症患者和健康对照组的录音时表现出与人类水平相近的准确性，并且可以以较高的准确率区分最常见的失语症类型。该方法还可以应用于其他疾病和语言，并有望稳健地提取诊断性的言语生物标志。

    

    本文提出了一种全自动的方法，通过声音录音识别来辅助评估言语障碍。通过结合连接主义时间分类（CTC）和基于编码器-解码器的自动语音识别模型，我们生成了丰富的声学和清晰的转录。然后，我们应用几种自然语言处理方法从这些转录中提取特征，以生成健康语音的原型。基于这些原型的基本距离度量作为标准机器学习分类器的输入特征，能够在人类水平上区分失语症患者的录音和健康对照组。此外，最常见的失语症类型可以以90%的准确率进行区分。该流程可直接适用于其他疾病和语言，展示了强大的潜力来稳健地提取诊断性言语生物标志。

    This paper presents a fully automated approach for identifying speech anomalies from voice recordings to aid in the assessment of speech impairments. By combining Connectionist Temporal Classification (CTC) and encoder-decoder-based automatic speech recognition models, we generate rich acoustic and clean transcripts. We then apply several natural language processing methods to extract features from these transcripts to produce prototypes of healthy speech. Basic distance measures from these prototypes serve as input features for standard machine learning classifiers, yielding human-level accuracy for the distinction between recordings of people with aphasia and a healthy control group. Furthermore, the most frequently occurring aphasia types can be distinguished with 90% accuracy. The pipeline is directly applicable to other diseases and languages, showing promise for robustly extracting diagnostic speech biomarkers.
    
[^72]: 评估网络引导的随机森林在疾病基因发现中的应用

    Evaluation of network-guided random forest for disease gene discovery. (arXiv:2308.01323v1 [q-bio.MN])

    [http://arxiv.org/abs/2308.01323](http://arxiv.org/abs/2308.01323)

    本论文评估了网络引导的随机森林在疾病基因发现中的应用，结果表明在疾病预测方面与标准随机森林相比并无优势，但在疾病基因发现方面，网络引导的随机森林能更准确地识别基因模块，但对于中心基因可能存在虚假选择的情况。

    

    基因网络信息被认为对于疾病模块和通路的识别有益，但在标准的随机森林（RF）算法中尚未明确利用基因表达数据分析。我们研究了网络引导的RF的性能，在构建RF时，网络信息被总结为预测变量的采样概率，并在其中使用。我们的结果表明，网络引导的RF在疾病预测方面并不比标准RF更好。在疾病基因发现方面，如果疾病基因形成模块，则网络引导的RF可以更准确地识别它们。此外，当给定网络中的基因与疾病状态无关时，使用网络信息可能会产生虚假的基因选择结果，特别是对于中心基因。我们对来自The Cancer Genome Atlas (TCGA)的两个平衡的微阵列和RNA-Seq乳腺癌数据集进行了实证分析，用于分类孕激素受体（PR）。

    Gene network information is believed to be beneficial for disease module and pathway identification, but has not been explicitly utilized in the standard random forest (RF) algorithm for gene expression data analysis. We investigate the performance of a network-guided RF where the network information is summarized into a sampling probability of predictor variables which is further used in the construction of the RF. Our results suggest that network-guided RF does not provide better disease prediction than the standard RF. In terms of disease gene discovery, if disease genes form module(s), network-guided RF identifies them more accurately. In addition, when disease status is independent from genes in the given network, spurious gene selection results can occur when using network information, especially on hub genes. Our empirical analysis on two balanced microarray and RNA-Seq breast cancer datasets from The Cancer Genome Atlas (TCGA) for classification of progesterone receptor (PR) st
    
[^73]: DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales.

    DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. (arXiv:2308.01320v1 [cs.LG])

    [http://arxiv.org/abs/2308.01320](http://arxiv.org/abs/2308.01320)

    DeepSpeed-Chat是一个新颖的系统，使得ChatGPT-like模型的RLHF培训易于访问，高效且经济实惠。它具有易于使用的训练和推断体验，复制了InstructGPT的训练流程，并集成了各种训练和推断优化，提供了无与伦比的效率和可扩展性。

    

    ChatGPT-like模型在人工智能的各种应用中带来了革命，从摘要和编码到翻译，甚至超越了人类表现。然而，当前环境还缺乏一种易于访问、高效且经济实惠的端到端RLHF（强化学习与人类反馈）训练流程，特别是当训练规模达到数十亿参数时。本文介绍了DeepSpeed-Chat，这是一个新颖的系统，使RLHF培训对AI社区可用。DeepSpeed-Chat提供了三个关键能力：一个易于使用的ChatGPT-like模型的训练和推断体验，一个复制InstructGPT训练流程的DeepSpeed-RLHF流水线，以及一个集成了各种训练和推断优化的强大DeepSpeed-RLHF系统。该系统提供了无与伦比的效率和可扩展性，可以训练具有数千亿参数的模型。

    ChatGPT-like models have revolutionized various applications in artificial intelligence, from summarization and coding to translation, matching or even surpassing human performance. However, the current landscape lacks an accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement Learning with Human Feedback) training pipeline for these powerful models, particularly when training at the scale of billions of parameters. This paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. DeepSpeed-Chat offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way. The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of
    
[^74]: 近年来使用机器学习进行疾病诊断的最新进展: 系统性调查、比较和挑战

    Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges. (arXiv:2308.01319v1 [cs.LG])

    [http://arxiv.org/abs/2308.01319](http://arxiv.org/abs/2308.01319)

    本文综述了近年来计算机辅助诊断领域在疾病诊断方面使用机器学习的最新进展，探讨了机器学习算法对于疾病检测和诊断的重要性和应用，以及其在分析生物医学数据方面的优势和挑战。

    

    计算机辅助诊断(CAD)作为一个充满活力的医学成像研究领域，正在迅速发展。由于医学诊断系统的错误可能导致严重误导的医疗治疗，近年来已经付出了重要努力来改进计算机辅助诊断应用。机器学习在计算机辅助诊断中的应用至关重要。一个简单的等式可能会导致关于器官等项目的错误指示。因此，从示例中学习是模式识别的一个重要组成部分。生物医学领域中的模式识别和机器学习有望提高疾病检测和诊断的精确性，同时支持决策过程的客观性。机器学习为创建优雅且自主的算法来分析高维和多模态生物医学数据提供了一种实际方法。本综述文章探讨了用于检测疾病的机器学习算法，包括肝炎、糖尿病、肝脏疾病、登革热等。

    Computer-aided diagnosis (CAD), a vibrant medical imaging research field, is expanding quickly. Because errors in medical diagnostic systems might lead to seriously misleading medical treatments, major efforts have been made in recent years to improve computer-aided diagnostics applications. The use of machine learning in computer-aided diagnosis is crucial. A simple equation may result in a false indication of items like organs. Therefore, learning from examples is a vital component of pattern recognition. Pattern recognition and machine learning in the biomedical area promise to increase the precision of disease detection and diagnosis. They also support the decision-making process's objectivity. Machine learning provides a practical method for creating elegant and autonomous algorithms to analyze high-dimensional and multimodal bio-medical data. This review article examines machine-learning algorithms for detecting diseases, including hepatitis, diabetes, liver disease, dengue fever
    
[^75]: 一种双层优化的介绍：在信号处理和机器学习中的基础和应用

    An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning. (arXiv:2308.00788v1 [cs.LG])

    [http://arxiv.org/abs/2308.00788](http://arxiv.org/abs/2308.00788)

    本论文介绍了双层优化在信号处理和机器学习中的基本概念和应用。双层优化是一个经典的优化问题，涉及到两个层次的优化，并在建模问题中展现了强大的能力。它在无线系统资源分配和对抗性机器学习等领域有广泛的应用。

    

    最近，双层优化（BLO）在信号处理和机器学习领域的一些激动人心的发展中占据了中心舞台。粗略地说，BLO是一个经典的优化问题，涉及到两个层次（即上层和下层），其中解决上层问题需要解决下层问题。BLO之所以受到欢迎，在很大程度上是因为它在建模涉及优化嵌套目标函数的SP和ML等问题方面非常强大。BLO的显著应用范围从无线系统的资源分配到对抗性机器学习。在这项工作中，我们重点研究了一类在SP和ML应用中经常出现的可解BLO问题。我们提供了这类BLO问题的一些基本概念的概述，例如它们的最优性条件、标准算法（包括它们的优化原理和实际实现方法），以及它们如何能够被利用。

    Recently, bi-level optimization (BLO) has taken center stage in some very exciting developments in the area of signal processing (SP) and machine learning (ML). Roughly speaking, BLO is a classical optimization problem that involves two levels of hierarchy (i.e., upper and lower levels), wherein obtaining the solution to the upper-level problem requires solving the lower-level one. BLO has become popular largely because it is powerful in modeling problems in SP and ML, among others, that involve optimizing nested objective functions. Prominent applications of BLO range from resource allocation for wireless systems to adversarial machine learning. In this work, we focus on a class of tractable BLO problems that often appear in SP and ML applications. We provide an overview of some basic concepts of this class of BLO problems, such as their optimality conditions, standard algorithms (including their optimization principles and practical implementations), as well as how they can be levera
    
[^76]: 关于最先进生成模型的可信度景观：一项综合调查

    On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16680](http://arxiv.org/abs/2307.16680)

    本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。

    

    扩散模型和大规模语言模型已经成为领先的生成模型，并对人类生活的各个方面产生了革命性的影响。然而，这些模型的实际应用也暴露出固有的风险，突显了它们的双重性质，并引发了对它们可信度的担忧。尽管有大量关于这个主题的文献，但针对大规模生成模型及其可信度的综合调查仍然很少见。为了弥补这一空白，本文调查了涉及这些模型的长期和新兴威胁，涵盖了隐私、安全、公平和责任这四个基本维度。通过这种方式，我们构建了一张详尽的地图，概述了这些模型的可信度，并提供了实际建议和未来的发展方向。这些努力对于促进这些模型的可信度部署至关重要。

    Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
    
[^77]: 智能电网中一种有效的用于能量盗窃检测和预测的LSTM-DDPM方案

    An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v1 [cs.LG])

    [http://arxiv.org/abs/2307.16149](http://arxiv.org/abs/2307.16149)

    这篇论文提出了一种利用LSTM和DDPM相结合的方案来解决智能电网系统中的能量盗窃检测和预测问题。通过重构和预测误差，系统能够准确识别能量盗窃的实例，并在实验中表现出较好的性能。

    

    能量盗窃检测（ETD）和能量消耗预测（ECF）是智能电网系统中两个相互关联的挑战。共同解决这些问题对于确保系统安全至关重要。本论文解决了智能电网系统中的ETD和ECF的相互关联挑战。所提出的解决方案结合了长短期记忆（LSTM）和去噪扩散概率模型（DDPM），用于生成输入重构和预测。通过利用重构和预测误差，系统能够识别能量盗窃的实例，基于重构误差和预测误差的方法相互补充，可以检测不同类型的攻击。通过在真实和合成数据集上进行大量实验，所提出的方案在ETD和ECF问题上表现优于基准方法。集成方法显著提升了ETD性能，能够准确检测到基准方法未能检测到的能量盗窃攻击。该研究提供了一种可行的解决方案来解决智能电网系统中ETD和ECF的挑战。

    Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers
    
[^78]: 批大小和步数与使用Armijo线搜索的随机梯度下降非凸优化之间的关系

    Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search. (arXiv:2307.13831v1 [cs.LG])

    [http://arxiv.org/abs/2307.13831](http://arxiv.org/abs/2307.13831)

    这项研究分析了使用Armijo线搜索的随机梯度下降非凸优化中批大小和步数的关系，并发现随着批大小的增加，所需的步数减少。

    

    随机梯度下降（SGD）是训练深度神经网络最简单的深度学习优化器。虽然SGD可以使用各种学习率，如常数或递减的学习率，但之前的数值结果表明，当SGD使用线搜索方法给出的学习率时，它的表现优于其他深度学习优化器。本文对使用Armijo线搜索给出学习率的非凸优化中的SGD进行了收敛性分析。分析表明，当步数和批大小都很大时，全梯度的平方范数的期望上界变小。接下来，我们展示了对于使用Armijo线搜索学习率的SGD来说，非凸优化所需的步数是批大小的单调递减凸函数；也就是说，随着批大小的增加，非凸优化所需的步数减少。此外，我们还展示了随机火灾的贡献。

    Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to train deep neural networks. While SGD can use various learning rates, such as constant or diminishing rates, the previous numerical results showed that SGD performs better than other deep learning optimizers using when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization. The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic fir
    
[^79]: 在密集分类中使用自适应标签扰动进行模型校准

    Model Calibration in Dense Classification with Adaptive Label Perturbation. (arXiv:2307.13539v1 [cs.CV])

    [http://arxiv.org/abs/2307.13539](http://arxiv.org/abs/2307.13539)

    本文提出了一种自适应标签扰动的模型校准方法，使用自校准二进制交叉熵损失来统一不同形式的标签扰动过程。该方法通过最大化预测熵来改善模型校准，并在保持分类准确性的同时纠正校准问题。

    

    对于安全相关的应用程序来说，产生可信赖的深度神经网络是至关重要的，其预测与置信度相关，可以代表正确性的可能性，以供后续决策使用。现有的密集二分类模型容易过于自信。为了改善模型校准，我们提出了自适应随机标签扰动（ASLP），它为每个训练图像学习一个独特的标签扰动级别。ASLP使用我们提出的自校准二进制交叉熵（SC-BCE）损失，将包括随机方法（如扰动标签）和标签平滑在内的标签扰动过程统一起来，以在保持分类率的同时纠正校准问题。ASLP采用经典统计力学的最大熵推断，以最大化相对于缺失信息的预测熵。它可以在以下情况下执行：（1）在已知数据上保持分类准确性作为保守解决方案，或（2）专门改善模型校准。

    For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates. ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information. It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model cali
    
[^80]: 海洋科学的时空数据挖掘：数据、方法和机遇

    Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities. (arXiv:2307.10803v1 [cs.LG])

    [http://arxiv.org/abs/2307.10803](http://arxiv.org/abs/2307.10803)

    该研究总结了海洋科学领域的时空数据挖掘研究，包括广泛使用的ST海洋数据集和其特点，以及ST海洋数据质量增强技术和现有STDM分类。

    

    随着海洋时空数据量的增加，已经进行了大量的空间-时间数据挖掘（STDM）研究来解决各种海洋问题，如气候预测和灾害警示。与典型的ST数据（如交通数据）相比，ST海洋数据更加复杂，具有一些独特的特征，例如多样化的区域性和高稀疏性。这些特点使得设计和训练STDM模型变得困难。不幸的是，还缺乏对这些研究的概述，这阻碍了计算机科学家在海洋领域识别研究问题，同时也使海洋科学研究人员不愿应用先进的STDM技术。为了解决这个问题，我们提供了一份综合调查，总结了海洋领域中现有的STDM研究。具体而言，我们首先总结了广泛使用的ST海洋数据集并确定了它们的独特特点。然后，讨论了典型的ST海洋数据质量增强技术。接下来，我们对现有的STDM进行分类。

    With the increasing amount of spatial-temporal~(ST) ocean data, numerous spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, e.g., climate forecasting and disaster warning. Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated with some unique characteristics, e.g., diverse regionality and high sparsity. These characteristics make it difficult to design and train STDM models. Unfortunately, an overview of these studies is still missing, hindering computer scientists to identify the research issues in ocean while discouraging researchers in ocean science from applying advanced STDM techniques. To remedy this situation, we provide a comprehensive survey to summarize existing STDM studies in ocean. Concretely, we first summarize the widely-used ST ocean datasets and identify their unique characteristics. Then, typical ST ocean data quality enhancement techniques are discussed. Next, we classify existing STDM st
    
[^81]: AnyTeleop: 通用视觉导向的灵巧机械臂手操作系统

    AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System. (arXiv:2307.04577v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2307.04577](http://arxiv.org/abs/2307.04577)

    AnyTeleop是一个通用的视觉导向的远程操作系统，支持多个不同的机械臂、手部、环境和摄像头配置，在实际实验和模拟中表现出色。

    

    基于视觉的远程操作为机器人提供了与环境进行物理交互的可能性，同时只需要低成本的摄像头传感器。然而，目前的基于视觉的远程操作系统是针对特定机器人模型和部署环境进行设计和工程化的，随着机器人模型的增加和操作环境的多样化，其扩展性较差。本文提出了AnyTeleop，一个统一和通用的远程操作系统，支持在单个系统中使用多个不同的机械臂、手部、现实环境和摄像头配置。尽管设计具有选择模拟器和真实硬件的灵活性，我们的系统仍然可以实现出色的性能。在实际实验中，AnyTeleop可以以更高的成功率击败为特定机器人硬件设计的以前系统，使用相同的机器人。在模拟中进行远程操作时，AnyTeleop可以实现更好的效果。

    Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better 
    
[^82]: 带有Bandit反馈的最近邻算法

    Nearest Neighbour with Bandit Feedback. (arXiv:2306.13773v1 [cs.LG])

    [http://arxiv.org/abs/2306.13773](http://arxiv.org/abs/2306.13773)

    本论文提出一种新的最近邻算法，能够应用于上下文Bandit问题并处理完全对抗的设置，具有高效运行、快速搜索和准线性空间的优点。

    

    本文将最近邻算法应用于上下文Bandit问题中。我们的算法处理了完全对抗的设置，即不对数据生成过程做任何假设。当与快速数据结构（可能是近似的自适应最近邻搜索，如导航网络）相结合时，我们的算法非常高效-每次试验的运行时间对动作数和试验数呈对数多项式增长，并且只需要准线性空间。

    In this paper we adapt the nearest neighbour rule to the contextual bandit problem. Our algorithm handles the fully adversarial setting in which no assumptions at all are made about the data-generation process. When combined with a sufficiently fast data-structure for (perhaps approximate) adaptive nearest neighbour search, such as a navigating net, our algorithm is extremely efficient - having a per trial running time polylogarithmic in both the number of trials and actions, and taking only quasi-linear space.
    
[^83]: 一种有效且可证明精确的0-1损失线性分类问题算法

    An efficient, provably exact algorithm for the 0-1 loss linear classification problem. (arXiv:2306.12344v1 [cs.LG])

    [http://arxiv.org/abs/2306.12344](http://arxiv.org/abs/2306.12344)

    该研究详细介绍了一种名为增量单元枚举（ICE）的算法，该算法可以精确解决定维度0-1损失线性分类问题。

    

    解决线性分类问题的算法具有悠久的历史，至少可以追溯到1936年的线性判别分析。对于线性可分数据，许多算法可以有效地得到相应的0-1损失分类问题的精确解，但对于非线性可分数据，已经证明这个问题在完全范围内是NP难的。所有替代方法都涉及某种形式的近似，包括使用0-1损失的代理（例如hinge或logistic损失）或近似的组合搜索，这些都不能保证完全解决问题。找到解决定维度0-1损失线性分类问题的全局最优解的有效算法仍然是一个未解决的问题。在本研究中，我们详细介绍了一个新算法的构建过程，增量单元枚举（ICE），它可以精确解决0-1损失分类问题。

    Algorithms for solving the linear classification problem have a long history, dating back at least to 1936 with linear discriminant analysis. For linearly separable data, many algorithms can obtain the exact solution to the corresponding 0-1 loss classification problem efficiently, but for data which is not linearly separable, it has been shown that this problem, in full generality, is NP-hard. Alternative approaches all involve approximations of some kind, including the use of surrogates for the 0-1 loss (for example, the hinge or logistic loss) or approximate combinatorial search, none of which can be guaranteed to solve the problem exactly. Finding efficient algorithms to obtain an exact i.e. globally optimal solution for the 0-1 loss linear classification problem with fixed dimension, remains an open problem. In research we report here, we detail the construction of a new algorithm, incremental cell enumeration (ICE), that can solve the 0-1 loss classification problem exactly in po
    
[^84]: 受遮蔽扩散模型是快速和注重隐私的学习器

    Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.11363](http://arxiv.org/abs/2306.11363)

    该论文提出了一种基于先验的去噪训练框架，通过遮蔽学习和扩散模型的结合，实现了更高效的训练和生成更高质量的图像。

    

    扩散模型已成为图像生成的事实上技术，然而它们具有显著的计算开销，限制了该技术在研究社区中的广泛应用。我们提出了一种基于先验的去噪训练框架，首次将预训练和微调范式纳入扩散模型训练过程中，大大提升了训练效率，并在促进各种下游任务方面显示出潜力。我们的方法主要是通过遮蔽输入图像的高比例（例如高达90％），并利用遮蔽去噪得分匹配来去噪可见区域，从而引导扩散模型从训练数据中学习更显著的特征作为先验知识。通过在预训练阶段使用遮蔽学习，我们在CelebA-HQ $256 \times 256$像素空间上高效地训练了基于ViT的扩散模型，实现了4倍加速，并提高了生成图像的质量，与去噪相比。

    Diffusion models have emerged as the \emph{de-facto} technique for image generation, yet they entail significant computational overhead, hindering the technique's broader application in the research community. We propose a prior-based denoising training framework, the first to incorporate the pre-train and fine-tune paradigm into the diffusion model training process, which substantially improves training efficiency and shows potential in facilitating various downstream tasks. Our approach centers on masking a high proportion (e.g., up to 90\%) of the input image and employing masked denoising score matching to denoise the visible areas, thereby guiding the diffusion model to learn more salient features from training data as prior knowledge. By utilizing masked learning in a pre-training stage, we efficiently train the ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space, achieving a 4x acceleration and enhancing the quality of generated images compared to denoisin
    
[^85]: 预测晶体性质的完整相互作用势的高效逼近

    Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10045](http://arxiv.org/abs/2306.10045)

    本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。

    

    我们研究了晶体材料的性质预测。晶体结构由一个最小的单元格组成，在三维空间中无限重复。如何在机器学习模型中准确表示这种重复结构仍然没有解决。当前的方法只在附近的节点之间建立边缘来构建图形，因此无法忠实地捕捉无限重复的模式和远距离的原子间相互作用。在这项工作中，我们提出了几个创新来克服这些限制。首先，我们建议直接建模物理原理的相互作用势，而不仅仅使用距离，如许多现有方法所做的。这些势包括库仑势，伦敦分散势和Pauli斥力势。其次，我们建模所有原子之间的完整势，而不仅仅是现有方法中的附近原子之间的势。这得益于我们用可证明的误差界逼近无限势和的方法。我们进一步开发了...

    We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
    
[^86]: 无监督的文本到图像生成模型下的组合式概念发现

    Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v1 [cs.CV])

    [http://arxiv.org/abs/2306.05357](http://arxiv.org/abs/2306.05357)

    本论文提出了一种无监督的方法，用于从图像中自动地发现不同的生成概念，并且这些生成概念可以被用于重新组合和生成新的艺术和混合图像，并作为一种表示用于下游的分类任务。

    

    文本到图像生成模型使得在不同领域实现高分辨率的图像合成成为可能，但需要用户指定他们想要生成的内容。本文考虑了相反的问题——在给出的不同图像集合中，我们能否发现代表每个图像的生成概念？我们提出了一种无监督的方法来从一组图像中发现生成的概念，将绘画中不同的艺术风格，对象和照明从厨房场景中分解出来，并通过给定的ImageNet图像发现图像类。我们展示了这样的生成概念能够准确地表示图像的内容，能够重新组合和组成以生成新的艺术和混合图像，并可以作为下游分类任务的一种表示来使用。

    Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem -- given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks.
    
[^87]: 从潜在图到潜在拓扑推断：可微分的单复形模块

    From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module. (arXiv:2305.16174v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16174](http://arxiv.org/abs/2305.16174)

    该论文研究了一种从潜在图到潜在拓扑推断的方法，通过引入可微分的单复形模块（DCM），学习描述数据点之间多向交互的高阶单复形的稀疏且不规则的拓扑结构，并展示了如何将其与单复形消息传递网络层集成以提高下游任务的效果。

    

    潜在图推断（LGI）通过动态学习来减少图神经网络（GNNs）对给定图拓扑的依赖。然而，大多数LGI方法假设存在（噪声、不完整、可改进的...）输入图来重新连接，并且只能学习常规的图拓扑。在拓扑深度学习（TDL）取得成功之后，我们研究了用于学习描述数据点之间多向交互的高阶单复形（具有稀疏且不规则的拓扑结构）的潜在拓扑推断（LTI）。为此，我们引入了可微分的单复形模块（DCM），一种计算复杂中单元概率的新型可学习函数，用于改进下游任务。我们展示了如何将DCM与单复形消息传递网络层集成，以及如何通过两步推断过程在端到端的方式进行训练，避免对输入中所有可能的单元进行详尽搜索，从而保持可伸缩性。我们的模型在多个高阶拓扑推断任务上进行了测试。

    Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks (GNNs) on a given graph topology by dynamically learning it. However, most of LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake of the success of Topological Deep Learning (TDL), we study Latent Topology Inference (LTI) for learning higher-order cell complexes (with sparse and not regular topology) describing multi-way interactions between data points. To this aim, we introduce the Differentiable Cell Complex Module (DCM), a novel learnable function that computes cell probabilities in the complex to improve the downstream task. We show how to integrate DCM with cell complex message passing networks layers and train it in a end-to-end fashion, thanks to a two-step inference procedure that avoids an exhaustive search across all possible cells in the input, thus maintaining scalability. Our model is tested on several ho
    
[^88]: 变分分类

    Variational Classification. (arXiv:2305.10406v1 [cs.LG])

    [http://arxiv.org/abs/2305.10406](http://arxiv.org/abs/2305.10406)

    提出一种新的变分分类方法，通过引入潜变量建模来优化训练，允许灵活的设计选择以改善校准和对抗鲁棒性，实验结果表明其对于域外数据的分类准确性得到了保持。

    

    我们提出了一种传统神经网络方法的新型扩展，称为变分分类 (VC)。通过引入潜变量建模，类似于变分自编码器和传统自编码器之间的关系，我们得到了一个基于证据下界 (ELBO) 的训练目标，采用对抗性方法优化。我们的VC模型允许在设计选择方面更加灵活，特别是类条件潜先验，而不是在现成的softmax分类器中做出的隐式假设。在图像和文本分类数据集上的实证评估表明，我们的方法在保持预测准确性的同时，改善了其他良好特性，如校准和对抗鲁棒性，即使应用于域外数据。

    We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
    
[^89]: Mlinear:重新思考时间序列预测的线性模型

    Mlinear: Rethink the Linear Model for Time-series Forecasting. (arXiv:2305.04800v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04800](http://arxiv.org/abs/2305.04800)

    该论文重新思考了时间序列预测的线性模型，提出了Mlinear方法，通过动态调节通道独立性和通道依赖性属性以实现更好的预测性能。

    

    最近，在时间序列预测研究中取得了显著的进展，越来越关注分析时间序列数据的性质，例如通道独立性（CI）和通道依赖性（CD），而不仅仅关注设计复杂的预测模型。然而，当前研究主要集中在单独的CI或CD上，有效地结合这两个相反的属性以实现协同效应的挑战仍然是一个未解决的问题。在本文中，我们仔细研究了CI和CD的相反属性，并提出了一个迄今未能有效回答的实际问题，即“如何有效地混合时间序列的CI和CD属性以实现更好的预测性能？”为了回答这个问题，我们提出了Mlinear（MIX-Linear），这是一种简单而有效的基于线性层的方法。Mlinear的设计理念主要包括两个方面：（1）基于动态调节CI和CD属性的机制

    Recently, significant advancements have been made in time-series forecasting research, with an increasing focus on analyzing the nature of time-series data, e.g, channel-independence (CI) and channel-dependence (CD), rather than solely focusing on designing sophisticated forecasting models. However, current research has primarily focused on either CI or CD in isolation, and the challenge of effectively combining these two opposing properties to achieve a synergistic effect remains an unresolved issue. In this paper, we carefully examine the opposing properties of CI and CD, and raise a practical question that has not been effectively answered, e.g.,"How to effectively mix the CI and CD properties of time series to achieve better predictive performance?" To answer this question, we propose Mlinear (MIX-Linear), a simple yet effective method based mainly on linear layers. The design philosophy of Mlinear mainly includes two aspects:(1) dynamically tuning the CI and CD properties based on
    
[^90]: 零日恶意软件的分类和在线聚类

    Classification and Online Clustering of Zero-Day Malware. (arXiv:2305.00605v1 [cs.CR])

    [http://arxiv.org/abs/2305.00605](http://arxiv.org/abs/2305.00605)

    本文研究了零日恶意软件的分类和在线聚类。实验使用 EMBER 数据集，对有流入的恶意软件样本进行了分类，得到了 95.33% 的平衡准确度。在剩下的数据中，使用自组织映射实现了纯度从 47.61% 到 77.68% 的聚类。

    

    不断产生大量新的恶意软件，我们需要将其与良性样本区分开来，并将其分类到恶意软件家族中。为此，我们需要研究现有恶意软件家族是如何发展，以及如何检查新出现的恶意软件家族。本文重点研究对入侵样本进行在线处理，将其分配给现有家族，或在新家族的情况下对其进行聚类。我们使用 EMBER 数据集中的七个流行恶意软件家族，其中四个在训练集中，另外三个在测试集中。通过多层感知器的分类得分，我们确定哪些样本将被分类，哪些将被聚类到新的恶意软件家族中。我们以平衡准确度为 95.33% 对 97.21% 的流数据进行了分类，然后使用自组织映射对剩余数据进行了聚类，实现了纯度从四个聚类的 47.61% 到十个聚类的 77.68%。

    A large amount of new malware is constantly being generated, which must not only be distinguished from benign samples, but also classified into malware families. For this purpose, investigating how existing malware families are developed and examining emerging families need to be explored. This paper focuses on the online processing of incoming malicious samples to assign them to existing families or, in the case of samples from new families, to cluster them. We experimented with seven prevalent malware families from the EMBER dataset, with four in the training set and three additional new families in the test set. Based on the classification score of the multilayer perceptron, we determined which samples would be classified and which would be clustered into new malware families. We classified 97.21% of streaming data with a balanced accuracy of 95.33%. Then, we clustered the remaining data using a self-organizing map, achieving a purity from 47.61% for four clusters to 77.68% for ten 
    
[^91]: 使用梯度提升方法对星系外射电源进行形态分类

    Morphological Classification of Extragalactic Radio Sources Using Gradient Boosting Methods. (arXiv:2304.12729v1 [astro-ph.IM])

    [http://arxiv.org/abs/2304.12729](http://arxiv.org/abs/2304.12729)

    本文提出了基于梯度提升机器学习方法和主成分分析的自动分类方法，用于解决星系外射电源形态分类问题。实验结果表明，在具有表格数据的分类问题中，该方法优于卷积神经网络。

    

    随着新的射电望远镜的建成，无线电天文学的数据量急剧增加。形态分类脱星系外射电源的自动分类是该领域中最关键的问题之一。大部分关于该领域的最近的成果都是基于卷积神经网络的分类器。而本文则提出了梯度提升机器学习方法和主成分分析作为卷积神经网络计算成本较低的替代方法。最近的研究发现，在具有表格数据的分类问题中，梯度提升方法的有效性优于深度学习方法。本文研究了基于XGBoost，LightGBM和CatBoost实现的梯度提升方法对数据集大小的分类器性能的影响。该分类器将星系外射电源分为三个类别。

    The field of radio astronomy is witnessing a boom in the amount of data produced per day due to newly commissioned radio telescopes. One of the most crucial problems in this field is the automatic classification of extragalactic radio sources based on their morphologies. Most recent contributions in the field of morphological classification of extragalactic radio sources have proposed classifiers based on convolutional neural networks. Alternatively, this work proposes gradient boosting machine learning methods accompanied by principal component analysis as data-efficient alternatives to convolutional neural networks. Recent findings have shown the efficacy of gradient boosting methods in outperforming deep learning methods for classification problems with tabular data. The gradient boosting methods considered in this work are based on the XGBoost, LightGBM, and CatBoost implementations. This work also studies the effect of dataset size on classifier performance. A three-class classifi
    
[^92]: 使用物理感知的时空动力学和测试时间细化重建湍流流场

    Reconstructing Turbulent Flows Using Physics-Aware Spatio-Temporal Dynamics and Test-Time Refinement. (arXiv:2304.12130v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2304.12130](http://arxiv.org/abs/2304.12130)

    本文提出了一种物理引导的神经网络方法，用于从低分辨率LES数据重建连续的DNS，可有效解决湍流流场的时空复杂性。

    

    模拟湍流对于航空航天工程、环境科学、能源行业和生物医学等许多重要应用至关重要。大涡模拟（LES）由于其较低的计算成本而被广泛用作模拟湍流流场的一种替代方法，相比直接数值模拟（DNS）。然而，LES无法准确捕捉湍流运输的所有尺度。从低分辨率LES重建DNS对于许多科学和工程学科至关重要，但由于湍流流场的时空复杂性，这给现有的超分辨率方法提出了许多挑战。本文提出了一种新的物理引导神经网络，用于从低分辨率LES数据重建连续的DNS。所提出的方法利用了流动动力学底层的偏微分方程，在时空模型架构的设计中进行建模。还开发了一种基于降级的细化方法，以强制实施p

    Simulating turbulence is critical for many societally important applications in aerospace engineering, environmental science, the energy industry, and biomedicine. Large eddy simulation (LES) has been widely used as an alternative to direct numerical simulation (DNS) for simulating turbulent flows due to its reduced computational cost. However, LES is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the spatio-temporal complexity of turbulent flows. In this work, we propose a new physics-guided neural network for reconstructing the sequential DNS from low-resolution LES data. The proposed method leverages the partial differential equation that underlies the flow dynamics in the design of spatio-temporal model architecture. A degradation-based refinement method is also developed to enforce p
    
[^93]: 模型稀疏化可以简化机器反学习

    Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])

    [http://arxiv.org/abs/2304.04934](http://arxiv.org/abs/2304.04934)

    本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。

    

    最近的数据管制要求机器反学习（MU）：从模型中移除指定样例的影响。虽然可以通过使用剩余数据从头开始进行模型重新训练来进行精确反学习，但是其计算成本导致了近似但高效的反学习方案的开发。除了数据中心的MU解决方案，我们通过一种新颖的基于模型的视角推进MU：通过权值修剪进行稀疏化。我们的理论和实践结果表明，模型稀疏性可以提高近似反学习器的多标准反学习性能，缩小近似间隙，同时保持高效。有了这个认识，我们制定了两个新的稀疏感知反学习元方案，称为“先修剪，然后反学习”和“稀疏感知反学习”。广泛的实验表明，我们的发现和提议在各种场景下始终有益于MU，包括按类数据擦除、随机数据擦除和后门数据伪造等。

    Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
    
[^94]: OpenAGI：当LLM遇到领域专家

    OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])

    [http://arxiv.org/abs/2304.04370](http://arxiv.org/abs/2304.04370)

    基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。

    

    人类具有将基本技能组合成复杂技能以解决复杂任务的显著能力。这种能力对于人工智能同样重要，因此，我们断言，除了开发大型综合智能模型外，将不同领域专家模型应用于复杂任务解决能力同样关键，以在人工智能通用智能的追求中使其具备这种能力。最近的大型语言模型（LLM）的发展证明其具有出色的学习和推理能力，使它们成为选择、综合和执行外部模型以解决复杂任务的控制器的有前途的选择。在这个项目中，我们开发了一个名为OpenAGI的开源AGI研究平台，专门设计为提供复杂的多步骤任务，并配有任务特定的数据集、评估指标和各种可扩展模型。OpenAGI将复杂任务阐释为自然语言问答，旨在促进领域专家和语言模型之间的协同作用。

    Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
    
[^95]: 在野外环境中使用计算机视觉估计情绪反应强度

    Computer Vision Estimation of Emotion Reaction Intensity in the Wild. (arXiv:2303.10741v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10741](http://arxiv.org/abs/2303.10741)

    本研究旨在利用计算机视觉模型在野外环境中估计情绪反应强度。通过训练深度神经网络和多模态模型，我们在Hume-Reaction数据集上取得了不错的结果。

    

    情绪在人类沟通中扮演着至关重要的角色。开发用于自动识别情绪表达的计算机视觉模型可以在各个领域中提供帮助，包括机器人技术、数字化行为医疗和媒体分析。在情感计算研究中，传统上有三种情绪表达的建模方式：行动单元、情感价值与唤起度（VA）和分类情绪。作为超越这些表达方式，朝着更精细标签的努力的一部分，我们描述了我们在野外情绪行为分析（ABAW）第五次比赛中提交的模型，该比赛引入了情绪反应强度（ERI）估计挑战。我们开发了四个在视觉领域训练的深度神经网络和一个结合了视觉和音频特征训练的多模态模型来预测情绪反应强度。在Hume-Reaction数据集上，我们最佳的模型在测试集上达到了0.4080的平均皮尔逊相关系数。

    Emotions play an essential role in human communication. Developing computer vision models for automatic recognition of emotion expression can aid in a variety of domains, including robotics, digital behavioral healthcare, and media analytics. There are three types of emotional representations which are traditionally modeled in affective computing research: Action Units, Valence Arousal (VA), and Categorical Emotions. As part of an effort to move beyond these representations towards more fine-grained labels, we describe our submission to the newly introduced Emotional Reaction Intensity (ERI) Estimation challenge in the 5th competition for Affective Behavior Analysis in-the-Wild (ABAW). We developed four deep neural networks trained in the visual domain and a multimodal model trained with both visual and audio features to predict emotion reaction intensity. Our best performing model on the Hume-Reaction dataset achieved an average Pearson correlation coefficient of 0.4080 on the test se
    
[^96]: 从时间序列数据中进行因果关系发现:综述和新视角

    Causal Discovery from Temporal Data: An Overview and New Perspectives. (arXiv:2303.10112v1 [cs.LG])

    [http://arxiv.org/abs/2303.10112](http://arxiv.org/abs/2303.10112)

    本文对于从时间数据中进行因果关系发现进行了综述，提出了两个相关的类别，即多元时间序列因果发现和事件序列因果发现，并提供了新的方法来综合考虑这两个类别。

    

    时间数据代表着复杂系统的时间顺序观测，可以被许多领域广泛生成，例如工业、医疗和金融。分析这种类型的数据对于各种应用非常有价值。因此，在过去几十年中，提出了不同的时间数据分析任务，例如分类、聚类和预测。其中，从时间数据中学习因果关系的因果发现任务被认为是一个有趣但至关重要的任务，并引起了广泛的研究关注。现有的因果发现工作可以根据时间数据是否被校准来分为两个高度相关的类别，即多元时间序列因果发现和事件序列因果发现。然而，大多数以前的调查仅专注于时间序列因果发现，忽略了第二类。在本文中，我们详细说明了这两个类别之间的相关性，并提供了新的方法来综合考虑这两个类别。

    Temporal data, representing chronological observations of complex systems, has always been a typical data structure that can be widely generated by many domains, such as industry, medicine and finance. Analyzing this type of data is extremely valuable for various applications. Thus, different temporal data analysis tasks, eg, classification, clustering and prediction, have been proposed in the past decades. Among them, causal discovery, learning the causal relations from temporal data, is considered an interesting yet critical task and has attracted much research attention. Existing casual discovery works can be divided into two highly correlated categories according to whether the temporal data is calibrated, ie, multivariate time series casual discovery, and event sequence casual discovery. However, most previous surveys are only focused on the time series casual discovery and ignore the second category. In this paper, we specify the correlation between the two categories and provide
    
[^97]: 利用四维CT灌注成像对疑似急性缺血性卒中患者的梗死区进行分割

    Exploiting 4D CT Perfusion for segmenting infarcted areas in patients with suspected acute ischemic stroke. (arXiv:2303.08757v1 [eess.IV])

    [http://arxiv.org/abs/2303.08757](http://arxiv.org/abs/2303.08757)

    该研究提出了一种新颖的方法，通过利用四维CTP全面利用时空信息，以分割疑似急性缺血性卒中患者的梗死区。实验证明，该方法明显优于现有的最先进方法，有潜力为改善AIS患者的早期诊断和治疗规划的准确性和效率做出贡献。

    

    精确、快速的急性缺血性卒中（AIS）患者缺血区（核心和半影区）预测方法对于改进诊断和治疗规划具有重要的临床意义。计算机断层扫描（CT）是疑似AIS患者早期评估的主要模式之一。CT灌注成像（CTP）通常用作主要评估手段，以确定卒中位置、严重程度和缺血性病灶体积。目前，大多数CTP自动分割方法都使用已经处理过的三维彩色地图作为放射科医师常规视觉评估的输入。或者，基于切片的二维+时间输入使用原始CTP数据，其中忽略了在体积上的空间信息。在本文中，我们研究不同方法来利用整个四维CTP作为输入，以充分利用时空信息。这使我们提出了一种新颖的4D卷积层。我们在大型数据集上进行的全面实验表明，所提出的方法明显优于现有的最先进方法。该方法有潜力为改善AIS患者的早期诊断和治疗规划的准确性和效率做出贡献。

    Precise and fast prediction methods for ischemic areas (core and penumbra) in acute ischemic stroke (AIS) patients are of significant clinical interest: they play an essential role in improving diagnosis and treatment planning. Computed Tomography (CT) scan is one of the primary modalities for early assessment in patients with suspected AIS. CT Perfusion (CTP) is often used as a primary assessment to determine stroke location, severity, and volume of ischemic lesions. Current automatic segmentation methods for CTP mostly use already processed 3D color maps conventionally used for visual assessment by radiologists as input. Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time input, where the spatial information over the volume is ignored. In this paper, we investigate different methods to utilize the entire 4D CTP as input to fully exploit the spatio-temporal information. This leads us to propose a novel 4D convolution layer. Our comprehensive experiments on a l
    
[^98]: SLCA: 预训练模型上用于连续学习的慢学习者与分类器对齐

    SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05118](http://arxiv.org/abs/2303.05118)

    SLCA是一种用于连续学习的简单但极其有效的方法。它通过慢学习和分类器对齐来在预训练模型上提高泛化能力和解决渐进过拟合问题。

    

    连续学习的目标是在学习顺序到达的数据中提高识别模型的性能。尽管大部分现有工作都建立在从头学习的前提下，但越来越多的努力已经致力于融入预训练的好处。然而，如何在每个增量任务中自适应地利用预训练的知识，同时保持其泛化能力，仍然是一个未解决的问题。在这项工作中，我们对预训练模型上的连续学习进行了广泛的分析，并将关键挑战归因于渐进过拟合问题。观察到在表征层次上选择性降低学习率几乎可以解决这个问题，我们提出了一种简单但极其有效的方法，名为慢学习者与分类器对齐（SLCA），通过建模类别分布并在事后对齐分类层次，进一步改进了分类层次。在各种实验中，我们证明了SLCA在连续学习任务中的有效性和性能优势。

    The goal of continual learning is to improve the performance of recognition models in learning sequentially arrived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training. However, how to adaptively exploit the pre-trained knowledge for each incremental task while maintaining its generalizability remains an open question. In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selectively reducing the learning rate can almost resolve this issue in the representation layer, we propose a simple but extremely effective approach named Slow Learner with Classifier Alignment (SLCA), which further improves the classification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fashion. Across a variety o
    
[^99]: 最优训练均方差估计神经网络

    Optimal Training of Mean Variance Estimation Neural Networks. (arXiv:2302.08875v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08875](http://arxiv.org/abs/2302.08875)

    本文研究了均方差估计网络的最优实现，并发现通过使用预热期可以避免收敛困难。

    

    本文研究了均方差估计网络（MVE网络）的最优实现。这种网络经常被用作回归设置中不确定性估计方法的构建模块，例如Concrete dropout和Deep Ensembles。具体而言，MVE网络假设数据是从一个具有均值函数和方差函数的正态分布产生的。MVE网络输出均值和方差的估计，通过最小化负对数似然函数来优化网络参数。在本文中，我们提出了两个重要的见解。首先，最近的研究中报告的收敛困难可以通过遵循原始作者的简单但经常被忽视的建议来相对容易地避免，即使用一个预热期。在这个期间，只优化均值，方差保持固定。我们通过实验证明了这一步骤的有效性。

    This paper focusses on the optimal implementation of a Mean Variance Estimation network (MVE network) (Nix and Weigend, 1994). This type of network is often used as a building block for uncertainty estimation methods in a regression setting, for instance Concrete dropout (Gal et al., 2017) and Deep Ensembles (Lakshminarayanan et al., 2017). Specifically, an MVE network assumes that the data is produced from a normal distribution with a mean function and variance function. The MVE network outputs a mean and variance estimate and optimizes the network parameters by minimizing the negative loglikelihood. In our paper, we present two significant insights. Firstly, the convergence difficulties reported in recent work can be relatively easily prevented by following the simple yet often overlooked recommendation from the original authors that a warm-up period should be used. During this period, only the mean is optimized with a fixed variance. We demonstrate the effectiveness of this step thr
    
[^100]: 使用LightGBM合并卫星和测站降水数据，并强调极值量化

    Merging satellite and gauge-measured precipitation using LightGBM with an emphasis on extreme quantiles. (arXiv:2302.03606v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.03606](http://arxiv.org/abs/2302.03606)

    本文将应用机器学习算法将卫星和测站降水数据合并，重点关注极值量化。通过将观测降水作为因变量，卫星数据作为预测变量，以提高降水估计精度。

    

    在水文模型应用中了解空间和时间上的降水情况至关重要，然而由于经济限制，雨量观测站的空间覆盖有限。网格化卫星降水数据集提供了一种替代选择，可以通过覆盖大面积的均匀地区来估计实际降水，尽管相关估计不准确。为了提高降水估计精度，将机器学习应用于合并基于测站观测和网格化卫星降水产品。在这种情况下，观测降水充当因变量，卫星数据充当预测变量。在相关应用中，随机森林是主要的机器学习算法。在这些空间预测设置中，通常发布因变量的点预测（主要是条件分布的均值或中位数）。本文旨在解决降水概率预测问题。

    Knowing the actual precipitation in space and time is critical in hydrological modelling applications, yet the spatial coverage with rain gauge stations is limited due to economic constraints. Gridded satellite precipitation datasets offer an alternative option for estimating the actual precipitation by covering uniformly large areas, albeit related estimates are not accurate. To improve precipitation estimates, machine learning is applied to merge rain gauge-based measurements and gridded satellite precipitation products. In this context, observed precipitation plays the role of the dependent variable, while satellite data play the role of predictor variables. Random forests is the dominant machine learning algorithm in relevant applications. In those spatial predictions settings, point predictions (mostly the mean or the median of the conditional distribution) of the dependent variable are issued. The aim of the manuscript is to solve the problem of probabilistic prediction of precip
    
[^101]: 个体公平的矩阵估计

    Matrix Estimation for Individual Fairness. (arXiv:2302.02096v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02096](http://arxiv.org/abs/2302.02096)

    本文研究了个体公平性(IF)和矩阵估计(ME)之间的联系。结果表明，在适当条件下使用ME方法进行数据预处理可以改善算法的个体公平性，并且不会牺牲性能。

    

    近年来，出现了多种算法公平性的概念。其中一种概念是个体公平性(IF)，要求相似的个体接受相似的对待。与此同时，矩阵估计(ME)作为处理具有缺失值的噪声数据的一种自然范式也出现了。在这项工作中，我们将这两个概念进行了联系。我们表明，使用ME方法对数据进行预处理可以在不牺牲性能的情况下改善算法的IF。具体而言，我们表明，在适当的条件下，使用一种名为奇异值阈值(SVT)的流行ME方法对数据进行预处理可以提供强有力的IF保证。然后，我们表明，在类似的条件下，SVT预处理还产生了一致且近似最小化敌对风险的估计。因此，在所述条件下，ME预处理步骤不会增加基本算法的预测误差，即不会给公平性与性能之间带来权衡。我们通过合成数据和真实数据集验证了这些结果。

    In recent years, multiple notions of algorithmic fairness have arisen. One such notion is individual fairness (IF), which requires that individuals who are similar receive similar treatment. In parallel, matrix estimation (ME) has emerged as a natural paradigm for handling noisy data with missing values. In this work, we connect the two concepts. We show that pre-processing data using ME can improve an algorithm's IF without sacrificing performance. Specifically, we show that using a popular ME method known as singular value thresholding (SVT) to pre-process the data provides a strong IF guarantee under appropriate conditions. We then show that, under analogous conditions, SVT pre-processing also yields estimates that are consistent and approximately minimax optimal. As such, the ME pre-processing step does not, under the stated conditions, increase the prediction error of the base algorithm, i.e., does not impose a fairness-performance trade-off. We verify these results on synthetic a
    
[^102]: 多模态有助于单模态：多模态模型下的交叉模态少样本学习

    Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models. (arXiv:2301.06267v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.06267](http://arxiv.org/abs/2301.06267)

    通过跨模态适应方法，在多模态模型下利用少样本示例（包括文本和声音）进行狗的视觉分类，并取得了最先进的结果。

    

    快速学习新任务的能力是智能代理的核心要素，也被称为少样本学习。传统的少样本学习基准使用来自单模态的少样本样本，但这些样本可能不足以描述整个概念类。相比之下，人类使用跨模态信息高效地学习新概念。在这项工作中，我们展示了通过阅读关于狗并听它们吠叫的声音来构建更好的视觉狗分类器的可能性。为此，我们利用最近的多模态基础模型（如CLIP）是固有的跨模态的特性，将不同的模态映射到相同的表示空间。具体而言，我们提出了一种简单的跨模态适应方法，从跨越不同模态的少样本示例中进行学习。通过将类名重新用作额外的一次性训练样本，我们使用一个极其简单的线性分类器实现了最先进的结果。

    The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better ${\bf visual}$ dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for visi
    
[^103]: 从数据流中学习：概述与更新

    Learning from Data Streams: An Overview and Update. (arXiv:2212.14720v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.14720](http://arxiv.org/abs/2212.14720)

    这篇文章探讨了在数据流上的机器学习任务的定义和设置存在的问题，针对这些问题，提出了重新构思监督数据流学习的基本定义和设置，并重新考虑了一些关于数据流学习的基本假设。

    

    在数据流上的机器学习文献非常广泛且不断增长。然而，关于数据流学习任务的定义假设通常太强，在实践中难以满足或者甚至相互矛盾，尤其在监督学习的背景下。算法的选择和设计基于通常不明确说明的标准，针对不明确定义的问题设置，在不现实的环境中进行测试，并且与更广泛的文献中的相关方法孤立地进行。这对于在这种背景下构思的许多方法产生了真实世界影响的可能性提出了质疑，并且存在传播误导性研究焦点的风险。我们提议通过重新构思监督数据流学习的基本定义和设置，以考虑相关概念漂移和时间依赖的现代思考方式来解决这些问题；同时，我们重新审视了什么构成了监督数据流学习任务，以及重新考虑一些关于数据流学习的基本假设。

    The literature on machine learning in the context of data streams is vast and growing. However, many of the defining assumptions regarding data-stream learning tasks are too strong to hold in practice, or are even contradictory such that they cannot be met in the contexts of supervised learning. Algorithms are chosen and designed based on criteria which are often not clearly stated, for problem settings not clearly defined, tested in unrealistic settings, and/or in isolation from related approaches in the wider literature. This puts into question the potential for real-world impact of many approaches conceived in such contexts, and risks propagating a misguided research focus. We propose to tackle these issues by reformulating the fundamental definitions and settings of supervised data-stream learning with regard to contemporary considerations of concept drift and temporal dependence; and we take a fresh look at what constitutes a supervised data-stream learning task, and a reconsidera
    
[^104]: 自动计算泰勒余项级数：更紧密的界限和新应用

    Automatically Bounding the Taylor Remainder Series: Tighter Bounds and New Applications. (arXiv:2212.11429v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11429](http://arxiv.org/abs/2212.11429)

    本文提出了一种自动计算泰勒余项级数的算法，可以提供更紧密的界限，并应用于区间计算、优化等领域。

    

    我们提出了一种自动计算泰勒余项级数的算法。在标量函数 $f:\mathbb{R}\to\mathbb{R}$ 的特殊情况下，我们的算法以参考点 $x_0$、信任域 $[a,b]$ 和整数 $k\geq1$ 为输入，并返回一个区间 $I$，使得对于所有 $x\in[a,b]$, $f(x)\sum_{i=0}^{k-1}\frac{1}{i!}f^{(i)}(x_0)(x-x_0)^i \in I(x-x_0)^k$。与自动微分类似，函数 $f$ 的输入必须为已知的原子函数。在算法的高层次上，我们的算法包含两个步骤。首先，我们针对多种常用的初等函数（如 $\exp$，$\log$）导出泰勒余项级数的尖锐多项式上限和下限。然后，我们使用区间算术的泰勒模式自动微分递归地组合元函数的边界。我们的算法可以高效利用机器学习硬件加速器，并提供了新的应用。

    We present a new algorithm for automatically bounding the Taylor remainder series. In the special case of a scalar function $f: \mathbb{R} \to \mathbb{R}$, our algorithm takes as input a reference point $x_0$, trust region $[a, b]$, and integer $k \ge 1$, and returns an interval $I$ such that $f(x) \sum_{i=0}^{k-1} \frac {1} {i!} f^{(i)}(x_0) (x - x_0)^i \in I (x - x_0)^k$ for all $x \in [a, b]$. As in automatic differentiation, the function $f$ is provided to the algorithm in symbolic form, and must be composed of known atomic functions.  At a high level, our algorithm has two steps. First, for a variety of commonly-used elementary functions (e.g., $\exp$, $\log$), we derive sharp polynomial upper and lower bounds on the Taylor remainder series. We then recursively combine the bounds for the elementary functions using an interval arithmetic variant of Taylor-mode automatic differentiation. Our algorithm can make efficient use of machine learning hardware accelerators, and we provide
    
[^105]: 神经网络热启动方法用于声学障碍物散射问题的逆问题

    A Neural Network Warm-Start Approach for the Inverse Acoustic Obstacle Scattering Problem. (arXiv:2212.08736v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2212.08736](http://arxiv.org/abs/2212.08736)

    本文提出了一种神经网络热启动方法，用于解决声学障碍物散射问题的逆问题。该方法通过在计算散射场和给定测量数据之间的$L^2$距离的区域边界中找到良好的初始猜测，以克服计算上的挑战。

    

    我们考虑在二维中的声学障碍物散射问题的逆问题，在这个问题中，通过对来自物体外部的一系列接收器接收到的散射场的测量来确定障碍物的边界。解决该问题的标准方法之一是将其重新表述为一个优化问题：找到最小化计算的散射场值与给定测量数据之间的$L^2$距离的区域边界。由于局部凸性随着频率增加而收缩，并在真解附近产生越来越多的局部最小值，这个优化问题在计算上具有挑战性。在许多实际实验设置中，由于实验装置或测量传感器的限制，低频测量是不可用的。因此，在这种环境中获得一个良好的优化问题初始猜测至关重要。

    We consider the inverse acoustic obstacle problem for sound-soft star-shaped obstacles in two dimensions wherein the boundary of the obstacle is determined from measurements of the scattered field at a collection of receivers outside the object. One of the standard approaches for solving this problem is to reformulate it as an optimization problem: finding the boundary of the domain that minimizes the $L^2$ distance between computed values of the scattered field and the given measurement data. The optimization problem is computationally challenging since the local set of convexity shrinks with increasing frequency and results in an increasing number of local minima in the vicinity of the true solution. In many practical experimental settings, low frequency measurements are unavailable due to limitations of the experimental setup or the sensors used for measurement. Thus, obtaining a good initial guess for the optimization problem plays a vital role in this environment.  We present a ne
    
[^106]: 一种无监督机器学习方法用于地震动谱的聚类和选择

    An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection. (arXiv:2212.03188v2 [physics.geo-ph] UPDATED)

    [http://arxiv.org/abs/2212.03188](http://arxiv.org/abs/2212.03188)

    本论文提出了一种无监督机器学习方法，用于提取地震动谱的定义特征，以辅助地震动选择。它结合了机器发现的潜在特征和传统强度测量，通过聚类分析选择代表性的地震动记录。验证结果表明该方法的有效性。

    

    随着机器学习在应用科学中的快速发展，序列数据的聚类分析在工程设计中仍然具有许多应用。本文提出了一种无监督机器学习算法，用于提取地震动谱的定义特征，也称为潜在特征，以辅助地震动选择。在这个背景下，潜在特征是通过神经网络自动编码器学习的低维度机器发现的谱特征。机器发现的潜在特征可以与传统定义的强度测量相结合，进行聚类分析，从大量的地震动样本中选择一个代表性的子集。高效的地震动选择的目标是选择代表性的记录，以概率性地反映结构在其寿命周期内将经历的情况。本文提供了三个示例来验证该方法，包括使用合成和现场记录。

    Clustering analysis of sequence data continues to address many applications in engineering design, aided with the rapid growth of machine learning in applied science. This paper presents an unsupervised machine learning algorithm to extract defining characteristics of earthquake ground-motion spectra, also called latent features, to aid in ground-motion selection (GMS). In this context, a latent feature is a low-dimensional machine-discovered spectral characteristic learned through nonlinear relationships of a neural network autoencoder. Machine discovered latent features can be combined with traditionally defined intensity measures and clustering can be performed to select a representative subgroup from a large ground-motion suite. The objective of efficient GMS is to choose characteristic records representative of what the structure will probabilistically experience in its lifetime. Three examples are presented to validate this approach, including the use of synthetic and field recor
    
[^107]: 无损失即无协议：学习与同行评审的社会选择

    No Agreement Without Loss: Learning and Social Choice in Peer Review. (arXiv:2211.02144v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.02144](http://arxiv.org/abs/2211.02144)

    在本文中，我们挑战了Nothigattu、Shah和Procaccia的框架，该框架旨在通过最小化损失函数来聚合评审人的映射，我们发现了一些负面结果。

    

    在同行评审系统中，评审人经常被要求评估提交稿件的各种特征，如技术质量或新颖性。给定每个预定义特征的评分，评审人必须提供一个整体的定量建议。可以假设每个评审人对特征集合到建议的映射都有自己的看法，并且不同的评审人心中有着不同的映射。这引入了一个称为比例偏差的任选性因素。本文讨论了一个由Noothigattu、Shah和Procaccia引入的框架，并且该框架在AAAI2022会议的组织者中应用。Noothigattu、Shah和Procaccia提出通过最小化特定损失函数来聚合评审人的映射，并研究了这种方法的社会选择理论中的公理性质。我们对他们的工作中使用的结果和假设进行了挑战，并报告了一些负面结果。

    In peer review systems, reviewers are often asked to evaluate various features of submissions, such as technical quality or novelty. A score is given to each of the predefined features and based on these the reviewer has to provide an overall quantitative recommendation. It may be assumed that each reviewer has her own mapping from the set of features to a recommendation, and that different reviewers have different mappings in mind. This introduces an element of arbitrariness known as commensuration bias. In this paper we discuss a framework, introduced by Noothigattu, Shah and Procaccia, and then applied by the organizers of the AAAI 2022 conference. Noothigattu, Shah and Procaccia proposed to aggregate reviewer's mapping by minimizing certain loss functions, and studied axiomatic properties of this approach, in the sense of social choice theory. We challenge several of the results and assumptions used in their work and report a number of negative results. On the one hand, we study a 
    
[^108]: 无反馈的Hebbian深度学习

    Hebbian Deep Learning Without Feedback. (arXiv:2209.11883v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2209.11883](http://arxiv.org/abs/2209.11883)

    本文提出了无反馈的Hebbian深度学习算法，通过避免传统方法中的反馈信号，实现了高效性、生物兼容性和准确性的同时提升。

    

    近期对反向传播算法的近似方法在减少计算效率和与生物学的不兼容性方面有所改进，但仍存在重要限制。此外，这些近似方法显著降低了基准测试的准确性，这表明完全不同的方法可能更有成效。基于最近关于软赢者全拓扑网络中Hebbian学习的理论，我们提出了多层SoftHebb算法，即训练深度神经网络的一种算法，无需任何反馈、目标或错误信号。因此，与其他方法相比，它通过避免权重传输、非局部可塑性、层更新的时间锁定、迭代平衡以及（自身）监督或其他反馈信号来实现效率提升和生物兼容性，而不是以准确性为代价。添加线性分类器后，它在最多五个隐藏层的情况下达到了与最先进的可生物学习相当的准确性。

    Recent approximations to backpropagation (BP) have mitigated many of BP's computational inefficiencies and incompatibilities with biology, but important limitations still remain. Moreover, the approximations significantly decrease accuracy in benchmarks, suggesting that an entirely different approach may be more fruitful. Here, grounded on recent theory for Hebbian learning in soft winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm that trains deep neural networks, without any feedback, target, or error signals. As a result, it achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals -- which were necessary in other approaches. Its increased efficiency and biological compatibility do not trade off accuracy compared to state-of-the-art bio-plausible learning, but rather improve it. With up to five hidden layers and an added linear classifier, acc
    
[^109]: 基于特征融合增强自编码器的缺失值填充模型

    A Missing Value Filling Model Based on Feature Fusion Enhanced Autoencoder. (arXiv:2208.13495v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.13495](http://arxiv.org/abs/2208.13495)

    本文提出了一种基于特征融合增强自编码器的缺失值填充模型，通过引入去轨迹神经元和径向基神经元的隐藏层，实现了同时学习相关特征和共同特征的能力。

    

    随着大数据时代的到来，数据质量问题变得更加关键。在众多因素中，缺失值数据是一个主要问题，因此开发有效的填充模型成为研究社区的一个关键课题。最近，一项主要的研究方向是利用神经网络模型，如自组织映射或自动编码器来填充缺失值。然而，这些传统方法很难同时发现数据属性间的相关特征和共同特征。特别是，对于经典的自动编码器来说，他们常常学习到无效的常量映射，这严重影响了填充性能。为了解决上述问题，我们提出了一种基于特征融合增强自编码器的缺失值填充模型。我们首先在自编码器中引入了一个包含去轨迹神经元和径向基神经元的隐藏层，这可以增强学习相关特征和共同特征的能力。

    With the advent of the big data era, the data quality problem is becoming more critical. Among many factors, data with missing values is one primary issue, and thus developing effective imputation models is a key topic in the research community. Recently, a major research direction is to employ neural network models such as self-organizing mappings or automatic encoders for filling missing values. However, these classical methods can hardly discover interrelated features and common features simultaneously among data attributes. Especially, it is a very typical problem for classical autoencoders that they often learn invalid constant mappings, which dramatically hurts the filling performance. To solve the above-mentioned problems, we propose a missing-value-filling model based on a feature-fusion-enhanced autoencoder. We first incorporate into an autoencoder a hidden layer that consists of de-tracking neurons and radial basis function neurons, which can enhance the ability of learning i
    
[^110]: UniCon: 带有对比损失的单向分歧学习用于视觉问答

    UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering. (arXiv:2208.11435v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.11435](http://arxiv.org/abs/2208.11435)

    本文提出了UniCon方法，用于解决多客户VQA任务的保密性约束和客户有限标记训练数据的问题。该方法通过模型共享学习跨模态表示，采用分裂学习架构确保隐私。

    

    多模式数据的视觉问答（VQA）有助于现实应用，如家庭机器人和医学诊断。然而，面临的一个重要挑战是为各种客户任务设计强大的学习方法。其中一个关键方面是确保隐私，因为由于保密问题，客户数据共享受到限制。本文致力于解决多客户VQA任务的保密性约束和客户有限标记训练数据的问题。我们提出了带有对比损失的单向分歧学习（UniCon）方法来克服这些限制。所提出的方法在不同客户的整个数据分布上训练全局模型，通过模型共享学习精细的跨模态表示来实现隐私保证，利用分裂学习架构确保隐私，其中完整模型分为两个组件进行独立训练。此外，最近发现自我监督学习技术与我们的方法高度兼容。

    Visual Question Answering (VQA) using multi-modal data facilitates real-life applications, such as home robots and medical diagnoses. However, one significant challenge is to design a robust learning method for various client tasks. One critical aspect is to ensure privacy, as client data sharing is limited due to confidentiality concerns. This work focuses on addressing the issue of confidentiality constraints in multi-client VQA tasks and limited labeled training data of clients. We propose the Unidirectional Split Learning with Contrastive Loss (UniCon) method to overcome these limitations. The proposed method trains a global model on the entire data distribution of different clients, learning refined cross-modal representations through model sharing. Privacy is ensured by utilizing a split learning architecture in which a complete model is partitioned into two components for independent training. Moreover, recent self-supervised learning techniques were found to be highly compatibl
    
[^111]: 分布式数据上的协同因果推断

    Collaborative causal inference on distributed data. (arXiv:2208.07898v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.07898](http://arxiv.org/abs/2208.07898)

    提出了一种数据协作准实验（DC-QE）方法，可以在保护隐私的前提下对分布式数据进行因果推断。通过共享中间表示而不是私有数据，估计倾向分数和处理效应，能够减少随机误差和偏差，相比现有方法有更好的估计结果。

    

    近年来，基于隐私保护的分布式数据因果推断技术的发展引起了广泛关注。为了解决这个问题，我们提出了一种数据协作准实验（DC-QE）方法，可以在保护隐私的前提下对分布式数据进行因果推断。在我们的方法中，首先，本地各方从私有数据中构建降维的中间表示。其次，他们共享中间表示，而不是私有数据，以保护隐私。然后，从共享的中间表示中估计倾向分数。最后，从倾向分数中估计处理效应。我们的方法能够减少随机误差和偏差，而现有方法只能减少处理效应估计中的随机误差。通过在人工数据和实际数据上进行数值实验，我们确认我们的方法可以得到比单独分析更好的估计结果。

    The development of technologies for causal inference with the privacy preservation of distributed data has attracted considerable attention in recent years. To address this issue, we propose a data collaboration quasi-experiment (DC-QE) that enables causal inference from distributed data with privacy preservation. In our method, first, local parties construct dimensionality-reduced intermediate representations from the private data. Second, they share intermediate representations, instead of private data for privacy preservation. Third, propensity scores were estimated from the shared intermediate representations. Finally, the treatment effects were estimated from propensity scores. Our method can reduce both random errors and biases, whereas existing methods can only reduce random errors in the estimation of treatment effects. Through numerical experiments on both artificial and real-world data, we confirmed that our method can lead to better estimation results than individual analyse
    
[^112]: ProMix：通过最大化干净样本效用来对抗标签噪声

    ProMix: Combating Label Noise via Maximizing Clean Sample Utility. (arXiv:2207.10276v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10276](http://arxiv.org/abs/2207.10276)

    论文提出了一种名为ProMix的新颖LNL框架，通过最大化干净样本的效用来对抗标签噪声。采用一种匹配高置信度选择技术来动态扩展基础干净样本集，并设计了一种平衡和无偏的SSL框架来提高性能。

    

    学习具有噪声标签（LNL）已成为一个吸引人的话题，因为带有不完整标注的数据相对较便宜易得。最近的最先进方法采用特定的选择机制来区分干净样本和噪声样本，然后应用半监督学习（SSL）技术以提高性能。然而，选择步骤主要提供一个中等大小和足够好的清洁子集，忽视了丰富的干净样本集。为了实现这一点，我们提出了一种新颖的LNL框架ProMix，试图通过最大化干净样本的效用来提高性能。我们方法的关键在于提出了一种匹配高置信度选择技术，该技术选择那些具有高置信度得分和与给定标签匹配预测的示例，以动态扩展基础干净样本集。为了克服过度选择清洁样本集程序的潜在副作用，我们进一步设计了一种新颖的SSL框架，能够在训练时得到平衡和无偏的分类器。

    Learning with Noisy Labels (LNL) has become an appealing topic, as imperfectly annotated data are relatively cheaper to obtain. Recent state-of-the-art approaches employ specific selection mechanisms to separate clean and noisy samples and then apply Semi-Supervised Learning (SSL) techniques for improved performance. However, the selection step mostly provides a medium-sized and decent-enough clean subset, which overlooks a rich set of clean samples. To fulfill this, we propose a novel LNL framework ProMix that attempts to maximize the utility of clean samples for boosted performance. Key to our method, we propose a matched high confidence selection technique that selects those examples with high confidence scores and matched predictions with given labels to dynamically expand a base clean sample set. To overcome the potential side effect of excessive clean set selection procedure, we further devise a novel SSL framework that is able to train balanced and unbiased classifiers on the se
    
[^113]: 分布式在线隐私学习的凸非可分目标

    Distributed Online Private Learning of Convex Nondecomposable Objectives. (arXiv:2206.07944v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2206.07944](http://arxiv.org/abs/2206.07944)

    该论文提出了一种分布式在线隐私学习的算法框架DPSDA，通过差分私有方式和对偶平均法来处理具有隐私和时间变化特性的约束在线学习问题，并提出了两种算法DPSDA-C和DPSDA-PS，其分别采用了循环式通信和具有噪声干扰梯度的对偶更新策略。

    

    我们处理了一个通用的分布式约束在线学习问题，该问题涉及随时间变化的网络隐私，考虑了一类非可分目标。在这种设置下，每个节点只控制全局决策的一部分，所有节点的目标是在时间范围T内协同最小化全局成本，并保证传输信息的安全性。针对这种问题，我们首先设计了一种新颖的通用算法框架，称为DPSDA，使用Laplace机制和随机变量的对偶平均法进行差分私有分布式在线学习。注意，在对偶更新中，DPSDA的所有节点都使用受噪声干扰的梯度，以提高通用性。然后，我们在该框架下提出了两种算法，分别称为DPSDA-C和DPSDA-PS。在DPSDA-C中，节点在原始更新中实现基于循环的通信，以减轻随时间变化的无向网络上的分歧。

    We deal with a general distributed constrained online learning problem with privacy over time-varying networks, where a class of nondecomposable objectives are considered. Under this setting, each node only controls a part of the global decision, and the goal of all nodes is to collaboratively minimize the global cost over a time horizon $T$ while guarantees the security of the transmitted information. For such problems, we first design a novel generic algorithm framework, named as DPSDA, of differentially private distributed online learning using the Laplace mechanism and the stochastic variants of dual averaging method. Note that in the dual updates, all nodes of DPSDA employ the noise-corrupted gradients for more generality. Then, we propose two algorithms, named as DPSDA-C and DPSDA-PS, under this framework. In DPSDA-C, the nodes implement a circulation-based communication in the primal updates so as to alleviate the disagreements over time-varying undirected networks. In addition,
    
[^114]: 推荐系统中的公平性：基础、方法和应用

    Fairness in Recommendation: Foundations, Methods and Applications. (arXiv:2205.13619v5 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2205.13619](http://arxiv.org/abs/2205.13619)

    这篇论文对推荐系统中的公平性问题进行了系统调查，针对推荐过程中可能出现的数据或算法偏见，提供了一些方法和应用来提升推荐中的公平性。

    

    作为机器学习最普遍的应用之一，推荐系统在辅助人类决策中起着重要作用。用户的满意度和平台的利益与生成的推荐结果的质量密切相关。然而，作为一个高度数据驱动的系统，推荐系统可能受到数据或算法偏见的影响，从而产生不公平的结果，这可能削弱系统的可信赖性。因此，在推荐设置中解决潜在的不公平问题至关重要。最近，对推荐系统的公平性考虑引起了越来越多的关注，涉及提升推荐中的公平性的方法越来越多。然而，这些研究相对零散且缺乏系统化整理，因此对于新研究人员来说难以深入领域。这促使我们对推荐中现有公平性作品进行系统调查。

    As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This
    
[^115]: 自信的神经网络回归与引导深度集成

    Confident Neural Network Regression with Bootstrapped Deep Ensembles. (arXiv:2202.10903v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.10903](http://arxiv.org/abs/2202.10903)

    本文提出了一种称为Bootstrapped Deep Ensembles的新方法，通过引入经典的有限数据效应，明确考虑神经网络回归中的不确定性，并通过实验证明了该方法的显著改进。

    

    随着神经网络的流行和使用增加，可信的不确定性估计变得越来越重要。其中一个最突出的不确定性估计方法是Deep Ensembles（Lakshminarayanan等人，2017）。一个经典的参数模型由于建模数据是随机样本，因此其参数存在不确定性。现代神经网络由于网络优化的随机性也具有额外的不确定性成分。Lakshminarayanan等人（2017）指出，Deep Ensembles未考虑到由有限数据效应引起的经典不确定性。在本文中，我们提出了一种用于回归设置的计算廉价性扩展Deep Ensembles的方法，称为Bootstrapped Deep Ensembles，它使用改进版的参数自助法明确考虑了有限数据的经典效应。通过实验研究，我们证明了我们的方法在标准方法的基础上明显改进。

    With the rise of the popularity and usage of neural networks, trustworthy uncertainty estimation is becoming increasingly essential. One of the most prominent uncertainty estimation methods is Deep Ensembles (Lakshminarayanan et al., 2017) . A classical parametric model has uncertainty in the parameters due to the fact that the data on which the model is build is a random sample. A modern neural network has an additional uncertainty component since the optimization of the network is random. Lakshminarayanan et al. (2017) noted that Deep Ensembles do not incorporate the classical uncertainty induced by the effect of finite data. In this paper, we present a computationally cheap extension of Deep Ensembles for the regression setting, called Bootstrapped Deep Ensembles, that explicitly takes this classical effect of finite data into account using a modified version of the parametric bootstrap. We demonstrate through an experimental study that our method significantly improves upon standar
    
[^116]: 在线手写识别中使用三元组损失函数的辅助跨模态表示学习

    Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition. (arXiv:2202.07901v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07901](http://arxiv.org/abs/2202.07901)

    本文提出了一种在在线手写识别中使用三元组损失函数的辅助跨模态表示学习方法，通过最小化模态嵌入之间的距离，利用图像分类任务的附加信息，提高了时间序列分类任务的准确性。

    

    跨模态表示学习通过学习两种或多种模态之间的共享嵌入，比仅使用其中一种模态能够提高给定任务的性能。不同数据类型的跨模态表示学习，如图像和时间序列数据（例如音频或文本数据），需要最小化模态嵌入之间的距离的深度度量学习损失函数。本文提出使用对比损失或三元组损失，利用正负标签创建具有不同标签的样本对，进行图像和时间序列模态之间的跨模态表示学习（CMR-IS）。通过调整三元组损失进行跨模态表示学习，可以利用辅助（图像分类）任务的附加信息，从而在主要的时间序列分类任务中实现更高的准确性。我们提出了适用于单标签和序列到序列分类任务的动态边界三元组损失。

    Cross-modal representation learning learns a shared embedding between two or more modalities to improve performance in a given task compared to using only one of the modalities. Cross-modal representation learning from different data types -- such as images and time-series data (e.g., audio or text data) -requires a deep metric learning loss that minimizes the distance between the modality embeddings. In this paper, we propose to use the contrastive or triplet loss, which uses positive and negative identities to create sample pairs with different labels, for cross-modal representation learning between image and time-series modalities (CMR-IS). By adapting the triplet loss for cross-modal representation learning, higher accuracy in the main (time-series classification) task can be achieved by exploiting additional information of the auxiliary (image classification) task. We present a triplet loss with a dynamic margin for single label and sequence-to-sequence classification tasks. We 
    
[^117]: 关系经验回放：通过自适应调整任务之间的关系进行持续学习

    Relational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship. (arXiv:2112.15402v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.15402](http://arxiv.org/abs/2112.15402)

    本文提出了关系经验回放（RER）的方法来进行持续学习，通过自适应调整任务之间的关系和样本重要性，以平衡稳定性和可塑性的要求，从而减轻了灾难性遗忘问题并积累新知识。

    

    持续学习是一种有前景的机器学习范例，可以在流式训练数据上学习新任务的同时保留先前学到的知识。目前，基于回放的方法通过保留来自旧任务的一小部分数据作为内存缓冲区，在减轻先前学到的知识的灾难性遗忘方面表现良好。然而，大多数这些方法通常将每个新任务视为平等的，这可能不充分考虑旧任务和新任务之间的关系或相似性。此外，这些方法通常忽视了持续训练过程中样本的重要性，导致某些任务的性能不佳。为了解决这个具有挑战性的问题，我们提出了关系经验回放（RER），这是一个双层学习框架，通过自适应调整每个任务内的任务之间的关系和样本重要性，以实现更好的“稳定性”和“可塑性”之间的权衡。因此，所提出的方法能够在积累新知识的同时保持对先前知识的记忆。

    Continual learning is a promising machine learning paradigm to learn new tasks while retaining previously learned knowledge over streaming training data. Till now, rehearsal-based methods, keeping a small part of data from old tasks as a memory buffer, have shown good performance in mitigating catastrophic forgetting for previously learned knowledge. However, most of these methods typically treat each new task equally, which may not adequately consider the relationship or similarity between old and new tasks. Furthermore, these methods commonly neglect sample importance in the continual training process and result in sub-optimal performance on certain tasks. To address this challenging problem, we propose Relational Experience Replay (RER), a bi-level learning framework, to adaptively tune task-wise relationships and sample importance within each task to achieve a better `stability' and `plasticity' trade-off. As such, the proposed method is capable of accumulating new knowledge while 
    
[^118]: 分享学习与学习分享 - Meta-Learning，Multi-Task Learning和Transfer Learning的有机结合: 一篇元回顾

    Sharing to learn and learning to share -- Fitting together Meta-Learning, Multi-Task Learning, and Transfer Learning: A meta review. (arXiv:2111.12146v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.12146](http://arxiv.org/abs/2111.12146)

    本文是一篇对Meta-Learning，Multi-Task Learning和Transfer Learning的元回顾，总结了这些学习范例的特点及其比较分析。通过将这些技术结合起来，可以解决各种不同领域的问题。

    

    跨不同领域整合知识是人类学习的重要特征。转移学习、元学习和多任务学习等学习范例通过利用先前知识来为新任务提供更快的学习和良好的泛化能力，反映了人类学习过程。本文详细介绍了这些学习范例及其比较分析。一个学习算法的弱点往往是另一个算法的优势，因此将它们合并是文献中普遍存在的特征。已有大量研究论文独立关注于这些学习范例，并对它们提供了全面的概述。然而，本文回顾了将（两个）这些学习算法结合起来的研究。本调研描述了如何将这些技术结合起来来解决各种不同领域的问题，包括计算机视觉、自然语言处理、高光谱成像等。

    Integrating knowledge across different domains is an essential feature of human learning. Learning paradigms such as transfer learning, meta learning, and multi-task learning reflect the human learning process by exploiting the prior knowledge for new tasks, encouraging faster learning and good generalization for new tasks. This article gives a detailed view of these learning paradigms and their comparative analysis. The weakness of one learning algorithm turns out to be a strength of another, and thus merging them is a prevalent trait in the literature. There are numerous research papers that focus on each of these learning paradigms separately and provide a comprehensive overview of them. However, this article provides a review of research studies that combine (two of) these learning algorithms. This survey describes how these techniques are combined to solve problems in many different fields of study, including computer vision, natural language processing, hyperspectral imaging, and
    
[^119]: 后继特征神经记忆控制

    Successor Feature Neural Episodic Control. (arXiv:2111.03110v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.03110](http://arxiv.org/abs/2111.03110)

    本文研究了记忆控制和后继特征两个框架的整合，通过结合这两种方法，提高了强化学习的样本效率和策略重用的优雅性。

    

    强化学习中一个长期目标是构建智能代理，展示类似于人类和动物的快速学习和灵活的技能转移。本文研究了解决这些目标的两个框架的整合：记忆控制和后继特征。记忆控制是一种受认知启发的方法，依赖于情节记忆，即代理的经验的基于实例的内存模型。同时，后继特征和广义策略改进（SF&amp;GPI）是一种元学习和迁移学习框架，可以为具有不同奖励函数的后续任务学习策略，并且可以高效地重用先前学到的策略。这两种技术分别在大大提高样本效率和优雅地重用先前学到的策略方面显示出令人印象深刻的结果。因此，我们概述了将这两种方法结合在一个单一的强化学习框架中，并通过实证研究其好处。

    A longstanding goal in reinforcement learning is to build intelligent agents that show fast learning and a flexible transfer of skills akin to humans and animals. This paper investigates the integration of two frameworks for tackling those goals: episodic control and successor features. Episodic control is a cognitively inspired approach relying on episodic memory, an instance-based memory model of an agent's experiences. Meanwhile, successor features and generalized policy improvement (SF&GPI) is a meta and transfer learning framework allowing to learn policies for tasks that can be efficiently reused for later tasks which have a different reward function. Individually, these two techniques have shown impressive results in vastly improving sample efficiency and the elegant reuse of previously learned policies. Thus, we outline a combination of both approaches in a single reinforcement learning framework and empirically illustrate its benefits.
    
[^120]: 如何评估机器学习回归中的不确定性估计？

    How to Evaluate Uncertainty Estimates in Machine Learning for Regression?. (arXiv:2106.03395v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.03395](http://arxiv.org/abs/2106.03395)

    本文研究了如何评估机器学习回归中的不确定性估计，发现目前的评估方法存在严重缺陷，无法准确评估估计质量和预测区间的关系。

    

    随着神经网络的普及，对于相应的不确定性估计的需求也越来越大。目前有两种主要的方法来测试这些估计的质量。大多数方法输出一个概率密度，可以通过在测试集上评估其对数似然来进行比较。其他方法直接输出一个预测区间，通常通过检查落入相应预测区间的测试点的比例来进行测试。直观上看，这两种方法都是合理的。然而，我们通过理论论证和模拟实验表明，评估不确定性估计质量的这两种方式都存在严重缺陷。首先，这两种方法无法分离共同产生预测不确定性的各个组成部分，从而难以评估这些组成部分的估计质量。其次，更好的对数似然并不保证更好的预测区间，而这通常是这些方法在实践中所用的。

    As neural networks become more popular, the need for accompanying uncertainty estimates increases. There are currently two main approaches to test the quality of these estimates. Most methods output a density. They can be compared by evaluating their loglikelihood on a test set. Other methods output a prediction interval directly. These methods are often tested by examining the fraction of test points that fall inside the corresponding prediction intervals. Intuitively both approaches seem logical. However, we demonstrate through both theoretical arguments and simulations that both ways of evaluating the quality of uncertainty estimates have serious flaws. Firstly, both approaches cannot disentangle the separate components that jointly create the predictive uncertainty, making it difficult to evaluate the quality of the estimates of these components. Secondly, a better loglikelihood does not guarantee better prediction intervals, which is what the methods are often used for in practice
    
[^121]: 随机种植森林：一种直接可解释的树集算法

    Random Planted Forest: a directly interpretable tree ensemble. (arXiv:2012.14563v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2012.14563](http://arxiv.org/abs/2012.14563)

    提出了一种名为"随机种植森林"的算法，通过修改随机森林算法，保留切分后的某些叶子，形成非二进制树，实现直接可解释的树集算法。该算法具有较好的预测和可视化特性。

    

    我们介绍了一种新颖的可解释的基于树的回归算法。我们的动机是从功能分解的角度估计未知的回归函数，其中功能组件对应于低阶交互项。我们的思路是修改随机森林算法，通过在切分后保留某些叶子而不是删除它们。这导致非二进制树，我们称之为种植树。将其扩展为一个森林，我们得到了我们的随机种植森林算法。此外，可以对叶子内可以相互作用的协变量数量进行限制。如果我们将交互限制设置为1，得到的估计量是一维函数的和。在另一个极端情况下，如果我们不设置限制，得到的估计量和相应的模型对回归函数的形式不加限制。在模拟研究中，我们发现我们的随机种植森林算法具有鼓励人的预测和可视化特性。

    We introduce a novel interpretable tree based algorithm for prediction in a regression setting. Our motivation is to estimate the unknown regression function from a functional decomposition perspective in which the functional components correspond to lower order interaction terms. The idea is to modify the random forest algorithm by keeping certain leaves after they are split instead of deleting them. This leads to non-binary trees which we refer to as planted trees. An extension to a forest leads to our random planted forest algorithm. Additionally, the maximum number of covariates which can interact within a leaf can be bounded. If we set this interaction bound to one, the resulting estimator is a sum of one-dimensional functions. In the other extreme case, if we do not set a limit, the resulting estimator and corresponding model place no restrictions on the form of the regression function. In a simulation study we find encouraging prediction and visualisation properties of our rando
    
[^122]: ROME: 通过拓扑解耦和梯度累积实现鲁棒的内存高效NAS

    ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation. (arXiv:2011.11233v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.11233](http://arxiv.org/abs/2011.11233)

    ROME是一种鲁棒的内存高效的NAS方法，通过拓扑解耦和梯度累积解决了单路径DARTS中的性能下降问题。

    

    尽管可微架构搜索（DARTS）是一种流行的架构搜索方法，但由于整个超网络存放在内存中，它受到了严重的内存开销的限制。这就是单路径DARTS的优势所在，它在每个步骤中只选择一个单路径子模型。虽然它对内存友好，但计算成本较低。然而，我们发现单路径DARTS存在一个关键问题，这一问题尚未得到足够的关注。也就是说，它也会出现严重的性能下降，因为像DARTS一样，它会导出太多无参数操作，例如跳跃连接。在本文中，我们提出了一种名为ROME的新算法来解决这个问题。首先，我们通过拓扑搜索和操作搜索解耦，使搜索和评估保持一致。然后，我们采用Gumbel-Top2重新参数化和梯度累积来增强具有挑战性的双层优化。我们对ROME进行了大量验证。

    Albeit being a prevalent architecture searching approach, differentiable architecture search (DARTS) is largely hindered by its substantial memory cost since the entire supernet resides in the memory. This is where the single-path DARTS comes in, which only chooses a single-path submodel at each step. While being memory-friendly, it also comes with low computational costs. Nonetheless, we discover a critical issue of single-path DARTS that has not been primarily noticed. Namely, it also suffers from severe performance collapse since too many parameter-free operations like skip connections are derived, just like DARTS does. In this paper, we propose a new algorithm called RObustifying Memory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology search from the operation search to make searching and evaluation consistent. We then adopt Gumbel-Top2 reparameterization and gradient accumulation to robustify the unwieldy bi-level optimization. We verify ROME extensively acr
    
[^123]: 稳定一致的密度-based聚类算法通过多参数持续性

    Stable and consistent density-based clustering via multiparameter persistence. (arXiv:2005.09048v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2005.09048](http://arxiv.org/abs/2005.09048)

    这篇论文通过引入一种度量层次聚类的对应交错距离，研究了一种稳定一致的密度-based聚类算法，提供了一个从一参数层次聚类中提取单个聚类的算法，并证明了该算法的一致性和稳定性。

    

    我们考虑了拓扑数据分析中的度-Rips构造，它提供了一种密度敏感的多参数层次聚类算法。我们使用我们引入的一种度量层次聚类的对应交错距离，分析了它对输入数据的扰动的稳定性。从度-Rips中取某些一参数切片可以恢复出已知的基于密度的聚类方法，但我们证明了这些方法是不稳定的。然而，我们证明了作为多参数对象的度-Rips是稳定的，并提出了一种从度-Rips中取切片的替代方法，该方法产生一个具有更好稳定性属性的一参数层次聚类算法。我们使用对应交错距离证明了该算法的一致性。我们提供了从一参数层次聚类中提取单个聚类的算法，该算法在对应交错距离方面是稳定的。

    We consider the degree-Rips construction from topological data analysis, which provides a density-sensitive, multiparameter hierarchical clustering algorithm. We analyze its stability to perturbations of the input data using the correspondence-interleaving distance, a metric for hierarchical clusterings that we introduce. Taking certain one-parameter slices of degree-Rips recovers well-known methods for density-based clustering, but we show that these methods are unstable. However, we prove that degree-Rips, as a multiparameter object, is stable, and we propose an alternative approach for taking slices of degree-Rips, which yields a one-parameter hierarchical clustering algorithm with better stability properties. We prove that this algorithm is consistent, using the correspondence-interleaving distance. We provide an algorithm for extracting a single clustering from one-parameter hierarchical clusterings, which is stable with respect to the correspondence-interleaving distance. And, we
    
[^124]: RAB: 可证实抵抗后门攻击的方法

    RAB: Provable Robustness Against Backdoor Attacks. (arXiv:2003.08904v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2003.08904](http://arxiv.org/abs/2003.08904)

    本文提出了一种证实机器学习模型鲁棒性的统一框架，通过随机平滑技术实现对规避和后门攻击的鲁棒性。同时，我们提出了鲁棒训练过程RAB，并证明其有效性和紧密性。在理论上证明了对后门攻击进行鲁棒性保护的可行性。

    

    最近的研究表明，深度神经网络（DNNs）容易受到对抗性攻击，包括规避攻击和后门（毒化）攻击。在防御方面，对于规避攻击已经进行了密集的改进，包括经验和可证实的鲁棒性；然而，对于后门攻击的可证实鲁棒性仍然很少被探索。本文针对通用威胁模型，特别是后门攻击，提出了一种证实机器学习模型鲁棒性的统一框架，并展示了如何利用随机平滑技术来保证对规避和后门攻击的鲁棒性。我们还提出了首个鲁棒训练过程RAB，使训练模型平滑并保证其对后门攻击的鲁棒性。我们证明了使用RAB训练的机器学习模型的鲁棒性界限，并证明我们的鲁棒性界限是紧密的。此外，我们在理论上展示了实现对后门攻击进行鲁棒性保护是可能的。

    Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible
    

