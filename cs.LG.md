# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type](https://rss.arxiv.org/abs/2402.01632) | 这篇论文提出了一种新的贝叶斯优化算法，可以处理具有任意类型未知超参数的情况，并具有无遗憾特性。 |
| [^2] | [Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees](https://rss.arxiv.org/abs/2402.00899) | 这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。 |
| [^3] | [IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation](https://arxiv.org/abs/2402.08682) | 本文提出了一种名为IM-3D的方法，通过考虑视频生成器和使用高斯平铺的3D重构算法，从生成的视图直接输出高质量的3D结果，实现了更高效的流水线、更好的质量和更高的可用3D资产产出。 |
| [^4] | [Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance](https://arxiv.org/abs/2402.08680) | 本文介绍了一种名为MARINE的框架，用于通过无分类器引导来减少大型视觉语言模型的物体幻觉。该框架无需训练或API访问，并通过集成视觉模型和引入额外的物体基础特征来提高模型的生成精确性和效率。 |
| [^5] | [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://arxiv.org/abs/2402.08679) | 本文提出了COLD-Attack框架，旨在实现具有隐秘性和可控性的LLM越狱。通过建立可控文本生成与攻击生成之间的关联，采用了能量限制解码与Langevin动力学算法，使得在不同的控制要求下搜索对抗性LLM攻击成为可能。 |
| [^6] | [Graph Mamba: Towards Learning on Graphs with State Space Models](https://arxiv.org/abs/2402.08678) | 本文提出了一种基于选择性SSMs的新类GNNs框架——图马巴网络（GMNs），通过不依赖于Transformer、复杂的消息传递和位置/结构编码（SE/PE），解决了传统GNNs的过度压缩和无法捕捉长程依赖的问题。 |
| [^7] | [A Convergence Analysis of Approximate Message Passing with Non-Separable Functions and Applications to Multi-Class Classification](https://arxiv.org/abs/2402.08676) | 该论文对非可分函数的近似消息传递（AMP）动力学进行了收敛性分析，并应用于多类别分类中的凸优化问题。 |
| [^8] | [Human Curriculum Effects Emerge with In-Context Learning in Neural Networks](https://arxiv.org/abs/2402.08674) | 人类学习对示例课程和任务结构有敏感性。研究发现，在神经网络和语言模型中，通过上下文学习方法可以同时获得分组和交错训练的优势。 |
| [^9] | [Model Assessment and Selection under Temporal Distribution Shift](https://arxiv.org/abs/2402.08672) | 本文研究了在变化环境中的模型评估与选择问题，通过合成不同时期的数据集，并开发了自适应滚动窗口方法来估计模型的泛化误差以及比较不同模型之间的差异。实验证明了我们提出的方法在非稳态数据中的适应性。 |
| [^10] | [Target Score Matching](https://arxiv.org/abs/2402.08667) | 本文提出了目标分数匹配方法，通过最小化回归损失来估计目标分布的噪声版本的分数。与传统的噪声分数匹配方法不同，该方法在低噪声水平下能够获得更准确的分数估计。 |
| [^11] | [Learning Emergent Gaits with Decentralized Phase Oscillators: on the role of Observations, Rewards, and Feedback](https://arxiv.org/abs/2402.08662) | 该论文通过将相位观察、简单的相位奖励和局部反馈动力学相结合，提出了一种使用分散相位振荡器学习动物步态的模型，有效地实现了新生步态偏好的策略。 |
| [^12] | [SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds](https://arxiv.org/abs/2402.08653) | SAGMAN是一种用于检验图神经网络稳定性的谱框架，它通过评估非线性映射中的距离失真来衡量GNN的稳定性。为了进行有意义的稳定性分析，我们提出了一种距离保持的图降维方法。 |
| [^13] | [Generating Universal Adversarial Perturbations for Quantum Classifiers](https://arxiv.org/abs/2402.08648) | 这项工作引入了QuGAP-一个用于生成量子分类器的通用对抗扰动的新框架，并通过实验证明了量子分类器容易受到这些攻击。 |
| [^14] | [Peeking Behind the Curtains of Residual Learning](https://arxiv.org/abs/2402.08645) | 本文窥探了残差学习的内幕，发现了导致平面神经网络收敛失败的“逐渐消失的输入”现象，并提出了通过保持更好的幸存神经元下界的残差连接作为一种解决方案。 |
| [^15] | [Learned Image Compression with Text Quality Enhancement](https://arxiv.org/abs/2402.08643) | 本文提出了一种学习图像压缩与文本质量增强的方法，通过最小化新的文本logit损失来改善重构文本的感知质量。实验证明，将该损失函数与适当的加权相结合，可以显著提高重构文本的质量，并在实际应用中取得了优异的性能。 |
| [^16] | [Forecasting high-impact research topics via machine learning on evolving knowledge graphs](https://arxiv.org/abs/2402.08640) | 通过机器学习预测未发布研究想法的影响力，我们使用一个由超过2100万篇科学论文构建的演化知识图谱，结合论文内容和历史引用的信息，高准确度预测未来的演化网络动态和新的研究方向的影响力。 |
| [^17] | [Strategizing against No-Regret Learners in First-Price Auctions](https://arxiv.org/abs/2402.08637) | 我们研究了第一价格拍卖中的无悔学习者和优化者之间的策略博弈。对于一类常用的无悔学习算法，我们发现在标准的第一价格拍卖中，优化者无法获得超过斯塔克尔贝格效用，但在贝叶斯第一价格拍卖中，他们可以获得远高于斯塔克尔贝格效用的效用。另外，我们也发现对于一类更复杂的算法，优化者的效用可以被限制在斯塔克尔贝格效用上。 |
| [^18] | [Knowledge Editing on Black-box Large Language Models](https://arxiv.org/abs/2402.08631) | 这项研究提出了在黑盒大型语言模型上进行知识编辑的方法，并引入了一种多角度评估框架和一种新的postEdit框架，以解决现有方法中的隐私和风格问题。 |
| [^19] | [A Generalized Approach to Online Convex Optimization](https://arxiv.org/abs/2402.08621) | 这是一篇关于在线凸优化的论文，作者分析了不同环境下的问题并提出了一种通用的解决方法，该方法可以转化为相应的线性优化算法，并可以在面对不同类型对手时获得可比较的遗憾界限。 |
| [^20] | [Adjustment Identification Distance: A gadjid for Causal Structure Learning](https://arxiv.org/abs/2402.08616) | gadjid软件包提供了一种用于因果结构学习的调整识别距离，通过引入框架来计算因果距离，这些距离能够高效评估因果发现算法学习的图形，并且在处理大规模图形时具有较高的性能。 |
| [^21] | [A Cost-Sensitive Transformer Model for Prognostics Under Highly Imbalanced Industrial Data](https://arxiv.org/abs/2402.08611) | 本文介绍了一种面向高度不平衡工业数据的成本敏感变压器模型，该模型在故障检测和预测方面表现出了显著的性能提升，并通过剔除实验分析了不同组件的贡献。 |
| [^22] | [Mixtures of Experts Unlock Parameter Scaling for Deep RL](https://arxiv.org/abs/2402.08609) | 本文证明了将专家组合模块融入基于值的网络中，尤其是软MoE，可以实现更具参数可扩展性的深度强化学习模型，这提供了强有力的实证证据以发展强化学习的缩放定律。 |
| [^23] | [Arbitrary Polynomial Separations in Trainable Quantum Machine Learning](https://arxiv.org/abs/2402.08606) | 该论文通过构建一种层次结构的可高效训练的量子神经网络，实现了在经典序列建模任务中具有任意常数次数的多项式内存分离。每个量子神经网络单元都可以在常数时间内在量子设备上进行计算。 |
| [^24] | [Homomorphism Counts for Graph Neural Networks: All About That Basis](https://arxiv.org/abs/2402.08595) | 本研究展示了基于图神经网络的同态计数对于增强其表达能力的重要性，并提出了一种更细致的方法来融合目标模式的同态计数。这种方法比现有方法更具表达力且没有额外的计算复杂度开销。 |
| [^25] | [Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs](https://arxiv.org/abs/2402.08593) | 本文介绍了一种名为"图特征预处理器"的软件库，可以从实时交易图中检测典型的洗钱和欺诈模式，并生成丰富的交易特征，从而显著提高机器学习模型的预测准确率。 |
| [^26] | [Convolutional Neural Networks Towards Facial Skin Lesions Detection](https://arxiv.org/abs/2402.08592) | 本研究提出了一种基于卷积神经网络和机器学习的模型，用于检测面部图像上的瑕疵和皮肤病变。该模型具有简单的架构和适用于图像处理的速度，避免了传统方法的复杂性。 |
| [^27] | [Faster Repeated Evasion Attacks in Tree Ensembles](https://arxiv.org/abs/2402.08586) | 这篇论文提出了一种更快速的构建树集成中对抗性样本的方法，通过识别一组特征的洞察力来加速构建过程。 |
| [^28] | [Mixture of Link Predictors](https://arxiv.org/abs/2402.08583) | 提出了一种用于链接预测的混合模型Link-MoE，通过选择合适的专家模型，利用不同类型的成对信息，能够显著提高预测性能。 |
| [^29] | [FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing](https://arxiv.org/abs/2402.08578) | FedLPS提出了一种在边缘计算环境中处理边缘设备生成的数据的多任务异构联邦学习方法，通过本地参数共享和迁移学习的原理来减少资源消耗和提高部署效率。 |
| [^30] | [Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2402.08577) | 本文提出了一种针对多模态大型语言模型的测试时反向门控攻击（AnyDoor），通过使用对抗性测试图像将反向门控注入到文本模态中，而无需访问或修改训练数据。AnyDoor具有分离设置和激活有害效果的时间的能力，并且在实验中证明了其有效性。 |
| [^31] | [Regret Minimization in Stackelberg Games with Side Information](https://arxiv.org/abs/2402.08576) | 这篇论文研究了侧信息中的Stackelberg博弈，提出了一种方法来解决现实中玩家之间信息交流不充分的情况，并且证明了在这种情况下后悔最小化是有效的。 |
| [^32] | [Two Tales of Single-Phase Contrastive Hebbian Learning](https://arxiv.org/abs/2402.08573) | 两种单相对比海比安学习的故事探索了学习算法的生物合理性，并提出了一种全局学习算法，能够消除与反向传播之间的性能差距，并解决了同步和无限小扰动带来的问题。 |
| [^33] | [Online Foundation Model Selection in Robotics](https://arxiv.org/abs/2402.08570) | 这篇论文介绍了在机器人学中的在线基础模型选择问题，针对采集闭源模型大量训练数据的高成本，提出了一种以用户为中心的在线模型选择解决方案，该方案结合了开源编码器和在线学习算法，通过提取上下文特征来实现模型选择。 |
| [^34] | [Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast](https://arxiv.org/abs/2402.08567) | Agent Smith提出了一种安全问题，即传染性越狱，该问题在多代理环境中可以通过简单的越狱一个代理来迅速感染所有代理并导致有害行为。 |
| [^35] | [Denoising Diffusion Restoration Tackles Forward and Inverse Problems for the Laplace Operator](https://arxiv.org/abs/2402.08563) | 本论文提出了一种新的方法，通过使用去噪扩散恢复模型（DDRM）解决了拉普拉斯算子的反向和正向问题，对于泊松方程的解和参数恢复有着显著的改善。 |
| [^36] | [Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases](https://arxiv.org/abs/2402.08552) | 本论文探讨了扩散模型对齐中的奖励过度优化问题，并从归纳和优先偏差的角度提出了解决方案。作者通过发现当前方法与扩散模型的时间归纳偏差分歧，以及评论模型中的沉睡神经元和活跃神经元对抗过度优化，提出了一种新的时间扩散策略优化方法。 |
| [^37] | [Generative VS non-Generative Models in Engineering Shape Optimization](https://arxiv.org/abs/2402.08540) | 本研究对生成式和非生成式模型在工程形状优化中的效果和效率进行了比较。研究结果表明，非生成式模型通过适当的形状编码和物理增强设计空间，可以以经济高效的方式生成高性能的有效设计，并且扩展了设计空间的覆盖范围。 |
| [^38] | [Intelligent Diagnosis of Alzheimer's Disease Based on Machine Learning](https://arxiv.org/abs/2402.08539) | 本研究使用机器学习探索阿尔茨海默病的早期检测和疾病进展，通过创新的数据预处理策略和机器学习模型，成功克服了缺失数据的挑战，并取得了91%的准确率，为阿尔茨海默病的早期检测提供了有价值的见解。 |
| [^39] | [A Distributional Analogue to the Successor Representation](https://arxiv.org/abs/2402.08530) | 本文提出了一种新的分布式强化学习方法，它通过分离转换结构和奖励，引入了分布式后继度量来描述行为的分布式后果。在实验中展示了该方法的实用性，特别是在零样本风险敏感策略评估方面。 |
| [^40] | [Approximately Piecewise E(3) Equivariant Point Networks](https://arxiv.org/abs/2402.08529) | 将对称性概念应用于点云神经网络中，可以提高其泛化能力。本研究提出了一种新的框架APEN，用于构建近似分段的E(3)等变点云网络，以处理具有局部对称性的多部分输入。 |
| [^41] | [Concept-1K: A Novel Benchmark for Instance Incremental Learning](https://arxiv.org/abs/2402.08526) | 我们提出了一种名为Concept-1K的具有挑战性的实例增量学习（IIL）场景和数据集，揭示了十亿参数的PLM仍然遭受灾难性遗忘，影响因素包括模型规模、预训练和缓冲区大小。现有的IL方法和LoRA技术无法满足性能需求。我们的研究为探索和缓解PLM中的遗忘问题提供了新的场景。 |
| [^42] | [Fairness Auditing with Multi-Agent Collaboration](https://arxiv.org/abs/2402.08522) | 本文研究了多代理协作下的公平性审计，证明了有时协调对审计准确性可能有害，而非协调的合作通常会产生良好的结果。 |
| [^43] | [A PAC-Bayesian Link Between Generalisation and Flat Minima](https://arxiv.org/abs/2402.08508) | 本研究结合了PAC-Bayes工具箱和Poincaré与Log-Sobolev不等式，提供了新的梯度项泛化界限，并突出了平坦最小值对泛化性能的积极影响。 |
| [^44] | [Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea](https://arxiv.org/abs/2402.08502) | 这项研究提出了一种可证明安全的强化学习方法，用于在开放海域上的船只中遵守交通规则，并引入了一种有效的验证方法来确定行为是否符合COLREGS规则。 |
| [^45] | [A Systematic Review of Data-to-Text NLG](https://arxiv.org/abs/2402.08496) | 这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。 |
| [^46] | [Sparsity via Sparse Group $k$-max Regularization](https://arxiv.org/abs/2402.08493) | 本文提出了一种新颖简洁的稀疏分组k最大规则化方法，可以同时增强分组内和分组间的稀疏性，更接近l0范数。 |
| [^47] | [Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming](https://arxiv.org/abs/2402.08491) | 本研究开发了一个基于深度强化学习的计算框架，用于细胞重编程中的重编程策略识别。在控制问题中，引入了伪吸引子的概念和识别方法，并设计了一个用于解决该问题的计算框架。 |
| [^48] | [Revealing Decurve Flows for Generalized Graph Propagation](https://arxiv.org/abs/2402.08480) | 本研究提出了广义传播的概念，并设计了广义传播神经网络(GPNNs)和连续统一里奇曲率(CURC)方法，用于在定向和加权图上进行图传播分析和图结构的特性研究。 |
| [^49] | [Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models](https://arxiv.org/abs/2402.08473) | 本文通过新的梯度下降优化方法，探索了常用的视觉语言模型的嵌入空间。结果显示，尽管该模型在零样本分类上有超过99%的表现，但在系统性评估中却完全失败。通过线性近似，提供了一个解释这些差异的框架。 |
| [^50] | [Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale](https://arxiv.org/abs/2402.08470) | 提出了一种并行友好的时空图学习方法，用于规模化光伏衰减分析。该方法集成了时空一致性和图注意力，可以准确估计大规模光伏逆变器的长期性能损失率，并提供在线模型和数据集以帮助改进光伏系统的性能评估和可靠性分析。 |
| [^51] | [ROSpace: Intrusion Detection Dataset for a ROS2-Based Cyber-Physical System](https://arxiv.org/abs/2402.08468) | 提出了一个基于ROS2的物理网络入侵检测数据集，通过渗透测试在三个层次监测系统特征，重现了正常和攻击行为的交替和重叠过程，可以测量检测攻击者所需的时间和恶意活动的数量。 |
| [^52] | [Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products](https://arxiv.org/abs/2402.08450) | 这篇论文提出了一种名为Subgraphormer的架构，通过将子图GNNs和图变换器结合起来，综合了子图GNNs的表达能力和消息传递机制以及图变换器的注意力和位置编码。基于子图GNNs与图的乘积之间的新连接，该方法设计了乘积图上的注意力机制和子图GNNs位置编码方案。 |
| [^53] | [Frequency-aware Graph Signal Processing for Collaborative Filtering](https://arxiv.org/abs/2402.08426) | 面向协同过滤的频率感知图信号处理方法（FaGSP）采用级联滤波模块和并行滤波模块，旨在更准确地建模用户偏好并利用高阶邻域信息，以提高协同过滤的性能。 |
| [^54] | [Transfer Operators from Batches of Unpaired Points via Entropic Transport Kernels](https://arxiv.org/abs/2402.08425) | 本文提出了一种通过熵传输核从批次的未配对点估计随机变量$X$和$Y$的联合概率的方法，并在理论上证明了其收敛性质。 |
| [^55] | [Conditional Neural Expert Processes for Learning from Demonstration](https://arxiv.org/abs/2402.08424) | 条件神经专家过程（CNEP）是一种学习从演示中获取技能的新框架，通过将不同模式的演示分配给不同的专家网络，并利用潜在空间中的信息将专家与编码表示匹配，解决了相同技能演示的变化和多种方式获取的挑战。 |
| [^56] | [Distribution Estimation under the Infinity Norm](https://arxiv.org/abs/2402.08422) | 本文提出了新的界限来在$\ell_\infty$范数下估计离散概率分布，实现了数据相关的收敛性保证，比现有结果有显著改进。通过运用切尔诺夫型不等式和经验贝尔斯坦界等方法，我们在合成和实际实验中验证了我们的结果。最后，我们将我们的方法应用于一个基本的选择性推断问题。 |
| [^57] | [Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins](https://arxiv.org/abs/2402.08421) | 本研究提出了一种应用于数字孪生的离线多智能体强化学习方案，通过整合分布式强化学习和保守Q学习来解决环境的不确定性和有限数据带来的认识不确定性。 |
| [^58] | [Interacting Particle Systems on Networks: joint inference of the network and the interaction kernel](https://arxiv.org/abs/2402.08412) | 本文研究了在网络上建模多智体系统的方法，提出了联合推断网络的权重矩阵和相互作用核的估计器，通过解决非凸优化问题并使用交替最小二乘（ALS）算法和交替最小二乘算子回归（ORALS）算法进行求解。在保证可识别性和良定义性的条件下，ALS算法表现出统计效率和鲁棒性，而ORALS算法是一致的，并且在渐近情况下具有正态性。 |
| [^59] | [Transition Constrained Bayesian Optimization via Markov Decision Processes](https://arxiv.org/abs/2402.08406) | 本文介绍了一种过渡受限的贝叶斯优化方法，通过马尔可夫决策过程的框架，使用强化学习解决了由于转变约束导致的搜索空间依赖历史的问题，并在化学反应器优化、信息化路径规划、机器校准等领域进行了应用。 |
| [^60] | [A Novel Approach to Regularising 1NN classifier for Improved Generalization](https://arxiv.org/abs/2402.08405) | 本文提出了一种改进通用化能力的非参数分类器，即分水岭分类器，通过对1NN分类器进行正则化。分水岭分类器可以在任意密集的数据集上找到任意边界，并具有很小的VC维度，从而能够实现良好的泛化。 |
| [^61] | [LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection](https://arxiv.org/abs/2402.08401) | 本文介绍了一种名为LOSS-GAT的半监督和一类方法用于假新闻检测，采用了标签传播和图注意力网络，以解决标记数据集有限性的挑战。 |
| [^62] | [Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing](https://arxiv.org/abs/2402.08400) | 本文提出了一种适用于图像语义分割的自适应分层认证方法，能够在多级层级标签空间中认证图像像素，并提供更多经过认证的语义信息。 |
| [^63] | [Selective Learning: Towards Robust Calibration with Dynamic Regularization](https://arxiv.org/abs/2402.08384) | 本研究提出了一种名为动态正则化（DReg）的方法，通过训练学习应该学到什么，从而解决深度学习中的过拟合和误校准问题。 |
| [^64] | [Uncertainty Quantification for Forward and Inverse Problems of PDEs via Latent Global Evolution](https://arxiv.org/abs/2402.08383) | 本论文提出了一种将高效和精确的不确定性量化集成到基于深度学习的替代模型中的方法，称为LE-PDE-UQ。该方法利用潜在空间中的潜在向量来演化系统的状态和相应的不确定性估计。 |
| [^65] | [The Duet of Representations and How Explanations Exacerbate It](https://arxiv.org/abs/2402.08379) | 该研究发现，算法的因果表示可能与人类的先前信念相冲突，解释会将注意力导向冲突特征并远离其他相关特征，这可能导致因果过度归因并对人类的信息处理产生不利影响。 |
| [^66] | [Time-Series Classification for Dynamic Strategies in Multi-Step Forecasting](https://arxiv.org/abs/2402.08373) | 本文提出了动态策略（DyStrat）用于多步预测，并在实验中证明了其在多个时间序列数据集上的优越性能。 |
| [^67] | [Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization](https://arxiv.org/abs/2402.08371) | 本文提出了一种混合推荐系统，结合了协同过滤和基于内容的过滤，利用多个准则为大学生推荐最合适的选修课程。通过遗传算法自动发现最佳配置，并使用科尔多瓦大学的真实信息进行实验验证。 |
| [^68] | [RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks](https://arxiv.org/abs/2402.08367) | 本论文介绍了RBF-PINN方法，在物理信息神经网络中应用非Fourier位置嵌入，解决了基于Fourier特征映射的局限性，并在各种正向和反向问题中取得了有效结果。 |
| [^69] | [NeuRes: Learning Proofs of Propositional Satisfiability](https://arxiv.org/abs/2402.08365) | NeuRes是一种神经符号证明为基础的SAT解析器，能够证明不可满足性并加速找到可满足真值分配的过程。 |
| [^70] | [Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training](https://arxiv.org/abs/2402.08344) | 通过使用带有差分隐私训练的Noisy-SGD方法，我们发现随机性而非剪裁梯度是导致训练过程中的隐式偏差的原因，并且这种偏差会被加剧，这对于使用巨大批量数据的强差分隐私保证构成重要挑战。 |
| [^71] | [Uncertainty Quantification via Stable Distribution Propagation](https://arxiv.org/abs/2402.08324) | 通过局部线性化，我们提出了一种新的方法，能够通过神经网络传播稳定概率分布来量化输出的不确定性。我们的方法在预测校准置信区间和选择性预测方面显示出了明显的优势。 |
| [^72] | [Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret with Adversarial Robustness in Partial Monitoring](https://arxiv.org/abs/2402.08321) | 这篇论文介绍了一种在部分监测问题中探索优化的方法，通过使用混合正则化器可以提高在随机和对抗环境中的遗憾界限。 |
| [^73] | [Approximating Families of Sharp Solutions to Fisher's Equation with Physics-Informed Neural Networks](https://arxiv.org/abs/2402.08313) | 本文利用物理信息神经网络（PINNs）近似求解Fisher方程，针对大反应速率系数条件下的行波解，引入了残差加权方案来提高解的精度，并通过特定的网络架构和反应速率系数作为额外输入，评估了PINN近似整个解族群的能力。 |
| [^74] | [Prompted Contextual Vectors for Spear-Phishing Detection](https://arxiv.org/abs/2402.08309) | 通过新的文档向量化方法，我们的方法使用大型语言模型来检测钓鱼网络攻击的电子邮件，并在实验证明具有高效性能。 |
| [^75] | [ChatCell: Facilitating Single-Cell Analysis with Natural Language](https://arxiv.org/abs/2402.08303) | ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。 |
| [^76] | [Multi-Level GNN Preconditioner for Solving Large Scale Problems](https://arxiv.org/abs/2402.08296) | 本论文介绍了一个多层GNN预处理器用于解决大规模问题，该预处理器集成了GNN模型和多层域分解框架，能够通过增强Krylov方法的效率以不同精度水平收敛。 |
| [^77] | [The Effect of Data Poisoning on Counterfactual Explanations](https://arxiv.org/abs/2402.08290) | 本研究研究了反事实解释在数据污染方面的脆弱性，发现最先进的反事实生成方法和工具包容易受到数据污染的影响。 |
| [^78] | [Pix2Code: Learning to Compose Neural Visual Concepts as Programs](https://arxiv.org/abs/2402.08280) | Pix2Code 是一个将神经视觉概念组合成程序的框架，通过利用显式、组合的符号和隐式的神经表示能力，从图像中检索对象表示并将关系概念合成为lambda演算程序，来解决通用性和可解释性的挑战。在推理领域Kandinsky Patterns和CURI上的评估结果表明，Pix2Code 能够识别组合视觉概念并推广到新数据和推理任务。 |
| [^79] | [Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering](https://arxiv.org/abs/2402.08277) | 这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。 |
| [^80] | [Geometry-induced Implicit Regularization in Deep ReLU Neural Networks](https://arxiv.org/abs/2402.08269) | 通过研究参数变化时输出集合的几何特征，我们发现在深度ReLU神经网络的优化过程中存在几何引导的隐式正则化现象。 |
| [^81] | [World Model on Million-Length Video And Language With RingAttention](https://arxiv.org/abs/2402.08268) | 该论文介绍了一个使用百万长度的视频和语言序列进行联合建模的环形注意力世界模型。该模型通过利用视频序列中的时间信息和语言的文本知识以及逐渐增加上下文大小的方法提高了AI辅助人类的能力。 |
| [^82] | [Distal Interference: Exploring the Limits of Model-Based Continual Learning](https://arxiv.org/abs/2402.08255) | 本研究探讨了持续学习中远距离干扰的极限问题，并提出了一种新型的可近似任何连续函数的反对称有界指数层B-spline ANN架构，用以解决灾难性干扰的挑战。 |
| [^83] | [Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles](https://arxiv.org/abs/2402.08251) | 本文提出了一种基于深度学习的神经网络模型，用于在无人机收集的热像中识别小型和微小物体。通过使用YOLOv5结构的骨干网络、Transformer编码器和滑动窗口，以及Sigmoid函数进行检测，我们的模型在识别准确性上超过了其他最先进的方法。 |
| [^84] | [APALU: A Trainable, Adaptive Activation Function for Deep Learning Networks](https://arxiv.org/abs/2402.08244) | APALU 是一种可训练的激活函数，通过增强深度学习的学习性能，在适应复杂数据表示的同时保持稳定和高效。 在图像分类任务中，APALU相对于传统激活函数能够显著提高准确性。 |
| [^85] | [Towards Equitable Agile Research and Development of AI and Robotics](https://arxiv.org/abs/2402.08242) | 这项研究提出了一种将研发项目管理方法与公平能力结合的框架，以解决机器学习和人工智能方法中存在的偏见和歧视问题。 |
| [^86] | [End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/abs/2402.08233) | 该论文研究了在统计套利中使用自编码器结构进行端到端策略学习的效果，通过嵌入自编码器网络到神经网络表示中，直接输出投资组合分配，并通过风险调整回报的反向传播进行训练。 |
| [^87] | [Causal Discovery under Off-Target Interventions](https://arxiv.org/abs/2402.08229) | 本文研究了非目标干预下的因果发现问题，提出了一个随机干预模型来尽量减少干预次数。通过验证和搜索两个基本问题，提供了多对数复杂度的近似算法。 |
| [^88] | [Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective](https://arxiv.org/abs/2402.08228) | 这项研究从架构的角度全面调查了图的超分布推广，揭示了图自我注意机制和其他常见构建模块在超分布问题上的影响。 |
| [^89] | [Improving Black-box Robustness with In-Context Rewriting](https://arxiv.org/abs/2402.08225) | 本文提出了一种名为LLM-TTA的方法，通过使用LLM生成的增强作为测试时间增强（TTA）的增强函数，提高了黑盒模型的鲁棒性。在BERT和T5模型的情感、毒性和新闻分类任务中，LLM-TTA优于传统的增强函数，使BERT的分布外鲁棒性平均提高了4.30个百分点，而不降低分布内性能。 |
| [^90] | [BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models](https://arxiv.org/abs/2402.08219) | BBox-Adapter是一种适用于黑盒大型语言模型的轻量级适配器，通过区分目标和源域数据，并采用排名式噪音对比估计（NCE）损失和在线适应机制，实现了在透明、隐私和成本方面的有效适应。 |
| [^91] | [Quantum Computing-Enhanced Algorithm Unveils Novel Inhibitors for KRAS](https://arxiv.org/abs/2402.08210) | 我们通过将量子算法的计算能力与经典方法的可靠性相结合，提出了一个量子-经典生成模型，用于设计癌症治疗中的关键分子靶点KRAS的新的抑制剂。在研究中合成了15种有希望的分子，并通过实验测试验证了其中两种分子的有效结合能力。 |
| [^92] | [Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits](https://arxiv.org/abs/2402.08209) | 本文提出了一种利用阈值赌博机算法快速识别具有低数据Shapley值的实例子集的迭代方法，从而提高数据清洗的计算速度，同时保持模型性能。 |
| [^93] | [Confronting Discrimination in Classification: Smote Based on Marginalized Minorities in the Kernel Space for Imbalanced Data](https://arxiv.org/abs/2402.08202) | 该论文提出了一种基于决策边界和样本近邻关系的新型分类过采样方法，旨在解决金融欺诈检测中因类别不平衡而导致的歧视问题。 |
| [^94] | [Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap](https://arxiv.org/abs/2402.08201) | 本文研究了弱分布重叠下马尔可夫决策过程中的离策略评估问题，并提出了一种截断双重稳健（TDR）估计器，在这种情况下表现良好。 |
| [^95] | [PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and Generalizable Compound-Protein Interaction Prediction](https://arxiv.org/abs/2402.08198) | PSC-CPI是一种多尺度蛋白质序列-结构对比框架，通过内模态和跨模态对比捕获蛋白质序列和结构之间的依赖关系，用于高效且具有可推广性的化合物-蛋白质相互作用预测。 |
| [^96] | [Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems](https://arxiv.org/abs/2402.08193) | 高斯模型集成置信传播算法（GEnBP）是一种用于高维系统中高效推断的方法，通过集成卡尔曼滤波器和高斯置信传播等技术相结合，能有效处理高维状态、参数和复杂的依赖结构。 |
| [^97] | [THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation](https://arxiv.org/abs/2402.08191) | THE COLOSSEUM是一个新的模拟基准测试，用于评估机器人操作的泛化性能。它包括20个不同的操作任务，在12个环境干扰轴上进行系统评估。研究发现，四个最先进的操作模型在干扰因素下的成功率下降了30-50%。改变干扰对象数量、目标对象颜色或光照条件会对模型的性能产生重要影响。 |
| [^98] | [Learning time-dependent PDE via graph neural networks and deep operator network for robust accuracy on irregular grids](https://arxiv.org/abs/2402.08187) | 本文提出了一种基于图神经网络的自回归模型GraphDeepONet，用于学习时间依赖的偏微分方程，并在不规则网格上实现了鲁棒的准确性。 |
| [^99] | [Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation](https://arxiv.org/abs/2402.08184) | 本研究引入了一种新的框架，通过将各种状态空间统一为固定大小的输入，实现了在多智能体系统中进行转移学习的能力。在SMAC环境中的实验结果表明，通过学习机动技能获得的知识可以显著提高多智能体的学习性能。 |
| [^100] | [Variational Continual Test-Time Adaptation](https://arxiv.org/abs/2402.08182) | 本文介绍了VCoTTA，一种变分贝叶斯方法用于测量连续测试时适应性中的不确定性。采用变分预热策略将预训练的模型转为贝叶斯神经网络，在测试时通过均值教师更新策略来更新学生模型，结合源模型和教师模型的先验。实验证明该方法在减轻先验偏移方面有效。 |
| [^101] | [Online Structured Prediction with Fenchel--Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss](https://arxiv.org/abs/2402.08180) | 这项研究扩展了在线结构化预测的替代后悔度界限，通过引入Fenchel-Young损失和随机解码方案，使得在在线多类分类和逻辑损失下获得了更好的结果。 |
| [^102] | [Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction](https://arxiv.org/abs/2402.08174) | 本论文提出了一种使用地标和聚类的层级位置嵌入方法用于链接预测任务。通过选择具有高度中心度的节点作为地标和进行图聚类，本方法有效地将位置信息嵌入到图中，提高了链接预测的准确性和性能。 |
| [^103] | [LLaGA: Large Language and Graph Assistant](https://arxiv.org/abs/2402.08170) | LLaGA是一个创新的模型，它有效地整合了大型语言模型（LLM）的能力，以处理图结构数据的复杂性。通过重新组织图节点以作为结构感知的序列，并通过一个多功能投影仪将其映射到标记嵌入空间中，LLaGA在多样性、泛化性和可解释性方面表现出色。 |
| [^104] | [On Limitations of the Transformer Architecture](https://arxiv.org/abs/2402.08164) | 本论文通过通信复杂性证明了Transformer层在处理函数组合任务时的局限性，指出对于大型定义域和某些数学任务，Transformers可能无法解决。 |
| [^105] | [Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models](https://arxiv.org/abs/2402.08151) | 本研究引入了渐变流自适应重要性抽样的方法，用于稳定贝叶斯分类模型的留一交叉验证预测的蒙特卡罗近似，以评估模型的普适性。 |
| [^106] | [Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search](https://arxiv.org/abs/2402.08147) | 本文提出了一种使用蒙特卡洛树搜索引导大型语言模型生成验证程序的方法，通过结合验证器的反馈和LLM先验知识提高了合成能力，实验证明这种方法在一组验证编程问题上的性能优于基本模型和具有插件的ChatGPT4。 |
| [^107] | [Randomized Algorithms for Symmetric Nonnegative Matrix Factorization](https://arxiv.org/abs/2402.08134) | 本论文提出了两种随机算法来更快、更可扩展地计算对称非负矩阵分解，其中一种使用随机矩阵草图来计算初始低秩输入矩阵，另一种使用随机杠杆得分采样来近似解决约束最小二乘问题。这些方法在大规模真实世界的图聚类任务上取得了良好的效果。 |
| [^108] | [On the Resurgence of Recurrent Models for Long Sequences: Survey and Research Opportunities in the Transformer Era](https://arxiv.org/abs/2402.08132) | 这项调研总结了在处理长序列数据方面，循环模型的复兴和与Transformer模型相结合的新型神经模型的发展，以及深度空间状态模型作为时间函数逼近的方法的出现。 |
| [^109] | [Efficient Contextual Bandits with Uninformed Feedback Graphs](https://arxiv.org/abs/2402.08127) | 本文提出了第一个针对无信息设置的上下文算法，通过在线回归降低效率和最优化算法，关键是学习使用对数损失的图形以获得有利的后悔保证。 |
| [^110] | [Contextual Multinomial Logit Bandits with General Value Functions](https://arxiv.org/abs/2402.08126) | 本文研究了具有一般价值函数的情境多项式逻辑回归赌博机，并提出了一套算法来处理线性情况，这些算法具有计算效率高、无维度的遗憾界限以及处理完全对抗性环境和奖励的能力。 |
| [^111] | [A Universal Non-Parametric Approach For Improved Molecular Sequence Analysis](https://arxiv.org/abs/2402.08117) | 该论文提出了一种新的基于压缩模型的非参数方法，以改进分子序列分析的性能。通过使用基本压缩算法和标准化压缩距离算法，该方法能够在分类任务中取得更好的结果，并不依赖于手工特征或预训练模型。 |
| [^112] | [Active Preference Learning for Large Language Models](https://arxiv.org/abs/2402.08114) | 本论文提出了一种用于大型语言模型的主动偏好学习策略，通过直接偏好优化（DPO）来更好地利用偏好标签。实验结果表明，该方法提高了基于成对偏好数据的微调的学习速度和最终性能。 |
| [^113] | [A Competition Winning Deep Reinforcement Learning Agent in microRTS](https://arxiv.org/abs/2402.08112) | 在IEEE microRTS竞赛中，RAISocketAI成为第一个获胜的深度强化学习代理，它通过逐步优化基本策略和迁移学习来击败了前两位竞赛获胜者，在未来的竞赛中可以作为基准参考，并为DRL研究提供起点。 |
| [^114] | [From Data to Decisions: The Transformational Power of Machine Learning in Business Recommendations](https://arxiv.org/abs/2402.08109) | 本研究探讨了机器学习在商业推荐系统中的作用，着重研究了数据源、特征工程和评估指标等方面的重要性，并突显了推荐引擎对用户体验和决策过程的重要影响。 |
| [^115] | [Finding Moving-Band Statistical Arbitrages via Convex-Concave Optimization](https://arxiv.org/abs/2402.08108) | 本文提出了一种通过凸凹优化方法寻找可以包含更多资产的统计套利策略，其中价格带的中点随时间变化。 |
| [^116] | [Mirror Descent-Ascent for mean-field min-max problems](https://arxiv.org/abs/2402.08106) | 该论文研究了解决测度空间上极小极大问题的镜像下降-上升算法的两种变体，并证明了收敛速率与相关有限维算法的最新结果一致。 |
| [^117] | [Learning Cartesian Product Graphs with Laplacian Constraints](https://arxiv.org/abs/2402.08105) | 本文研究了在Laplacian约束下学习笛卡尔乘积图的问题，建立了笛卡尔乘积Laplacian的统计一致性，并提出了一种有效的算法。实验证明了方法的有效性。 |
| [^118] | [Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation](https://arxiv.org/abs/2402.08100) | 本研究调查了大型语言模型在文本到SQL翻译中数据污染的影响。通过引入一种新的检测方法，研究人员发现GPT-3.5在陌生数据集上的性能显著下降。此外，通过采用对抗性表断开方法，研究人员还分析了GPT-3.5在修改信息的数据库上的效果。 |
| [^119] | [An Accelerated Gradient Method for Simple Bilevel Optimization with Convex Lower-level Problem](https://arxiv.org/abs/2402.08097) | 本文提出了一种加速梯度方法来解决具有凸下层问题的简单双层优化问题，通过局部逼近下层问题的解集和加速梯度更新方法，在有限次迭代内找到一个具有一定精度的最优解。 |
| [^120] | [Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?](https://arxiv.org/abs/2402.08096) | 本文提出了一种新的微调预训练模型的采样方案"mix-cd"，通过识别和优先处理实际面临遗忘的样本，以缓解微调过程中的知识遗忘问题。该方法简单、易于实现，并能在现有模型中无缝运用，有效地保持预训练模型的性能。 |
| [^121] | [Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization](https://arxiv.org/abs/2402.08095) | 本文通过均匀化的方式确切实现了离散扩散模型，研究了其理论性质，并提供了关于采样的总变差距离和KL散度保证。这一方法在建模离散数据方面具有重要的应用价值。 |
| [^122] | [BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data](https://arxiv.org/abs/2402.08093) | 基于10万小时数据的10亿参数文本到语音模型BASE TTS在语音自然度上达到了最新技术水平，并且能够展现自然的韵律。 |
| [^123] | [Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees](https://arxiv.org/abs/2402.08090) | 本论文提出了扩展线性化收缩动力学（ELCD），是第一个具有全局收缩性保证的神经网络动力系统，通过参数化非线性向量场的扩展线性化实现。通过在数据空间和潜在空间之间训练微分同胚，并在潜在空间中强制收缩性，ELCD能在面对不确定性时保持全局稳定性和鲁棒性。 |
| [^124] | [Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control](https://arxiv.org/abs/2402.08088) | 该论文提出了一种使用统计过程控制的机器学习框架，用于检测离群数据和监测数据漂移。该框架在临床环境中具有重要应用价值，能够帮助提高ML设备在放射学图像中的性能和患者安全。 |
| [^125] | [Text-centric Alignment for Multi-Modality Learning](https://arxiv.org/abs/2402.08086) | 本研究提出了一种名为文本中心对齐的多模态学习方法（TAMML），利用大型语言模型和基础模型，通过将文本作为统一的语义空间，解决了多模态学习中的模态不匹配问题，并在处理未见过的、多样化的、不可预测的模态组合时取得了显著的改进。 |
| [^126] | [Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning](https://arxiv.org/abs/2402.08085) | "信息绕行"是一种用于层次性表征图中循环的方法，通过比较最短路径和最长路径之间的对比性，实现了与高阶"Weisfeiler-Lehman"（WL）测试相当的表达能力，但计算需求更少。 |
| [^127] | [Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions](https://arxiv.org/abs/2402.08082) | 基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难，通过分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。 |
| [^128] | [Large Language Models as Agents in Two-Player Games](https://arxiv.org/abs/2402.08078) | 通过将大型语言模型的训练过程重新概念化为基于语言的两人游戏中的代理学习，我们能够得到关键的见解，并提供了新的方法和技术来推进大型语言模型的发展。 |
| [^129] | [Diffeomorphic Measure Matching with Kernels for Generative Modeling](https://arxiv.org/abs/2402.08077) | 该研究提出了使用核函数进行同胚度量匹配的方法，在生成建模中实现了概率测度的传输。通过理论分析和数值实验，本文展示了该方法的性能和适用性。 |
| [^130] | [Efficient and Scalable Fine-Tune of Language Models for Genome Understanding](https://arxiv.org/abs/2402.08075) | 这篇论文提出了一种名为Lingo的高效可扩展的基因理解语言模型微调方法，该方法利用了自然语言模型的上下文提示，并通过自适应秩采样方法适应了基因组注释的多样性任务。 |
| [^131] | [Grounding Data Science Code Generation with Input-Output Specifications](https://arxiv.org/abs/2402.08073) | 该论文提出了一种方法，通过使用输入输出规范来解决大型语言模型在生成代码时与自然语言提示和I/O规范对齐困难的问题，并在数据科学编程任务上进行了评估。 |
| [^132] | [Locality Sensitive Hashing for Network Traffic Fingerprinting](https://arxiv.org/abs/2402.08063) | 这项研究探讨了使用局部敏感哈希（LSH）作为网络流量指纹技术的解决方案，以解决机器学习方法在特征选择、超参数调整和模型重新训练方面的困难，并提供对网络中的概念漂移的弹性。 |
| [^133] | [Avoiding Catastrophe in Continuous Spaces by Asking for Help](https://arxiv.org/abs/2402.08062) | 在连续空间中，通过寻求帮助来避免灾难。引入了一种上下文多臂赌博问题的变体，目标是最小化灾难发生的概率。提出了一种算法，在连续1D状态空间和相对简单的回报函数下，遗憾和向导师查询率都趋近于0。 |
| [^134] | [MIML library: a Modular and Flexible Library for Multi-instance Multi-label Learning](https://arxiv.org/abs/2402.08056) | MIML库是一个用于多实例多标签学习的模块化和灵活的Java软件库，提供了43种分类算法，可通过XML配置文件执行，还包括数据管理和分区、交叉验证和性能评估等功能。 |
| [^135] | [Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking](https://arxiv.org/abs/2402.08030) | 本研究通过实验和访谈调查了大型语言模型(LLM)助手在软件帮助寻求中的有效性。结果显示，虽然优化后的LLM助手相较于基准LLM表现更好，但提示指南和领域上下文的融合与否对于LLM的使用和用户感知没有显著影响。 |
| [^136] | [UGMAE: A Unified Framework for Graph Masked Autoencoders](https://arxiv.org/abs/2402.08023) | UGMAE是一种统一框架，用于解决图形自动编码器中存在的节点重要性、图像信息利用、表示空间中的语义知识和重构稳定性等问题。 |
| [^137] | [Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale Wireless Networks](https://arxiv.org/abs/2402.08022) | 本文提出了一种针对优化无线网络的传统Q-Learning算法面临的性能和复杂性挑战的新颖的集成Q-Learning算法，在大规模无线网络中利用数字堂兄来逼近可观测无线网络的大状态空间。 |
| [^138] | [Nearest Neighbour Score Estimators for Diffusion Generative Models](https://arxiv.org/abs/2402.08018) | 本论文提出了一种新颖的最近邻评分函数估计器，通过利用训练集中的多个样本大大降低了估计器的方差，可用于训练一致性模型和扩散模型，提高收敛速度、样本质量，并为进一步的研究提供了新的可能性。 |
| [^139] | [Lumos : Empowering Multimodal LLMs with Scene Text Recognition](https://arxiv.org/abs/2402.08017) | 本论文介绍了Lumos，它是第一个具备文本理解能力的多模式问答系统，通过运用场景文本识别组件，能够从第一人称视角图像中提取文本，并将其用于加强多模式大型语言模型的输入。研究过程中，作者克服了与文本识别质量、延迟和模型推断相关的多个挑战，并提供了全面的组件评估结果，展示了高质量和高效率的性能。 |
| [^140] | [Online Differentially Private Synthetic Data Generation](https://arxiv.org/abs/2402.08012) | 本文提出了一种在线差分隐私合成数据生成的多项式时间算法，在超立方体数据流上实现了近乎最优的精度界限，也推广了之前关于计数查询的连续发布模型的工作，仅需要额外的多项式对数因子。 |
| [^141] | [Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning](https://arxiv.org/abs/2402.08010) | 本文描述了CNN中卷积瓶颈（CBN）结构的出现，网络在前几层将输入表示转换为在少数频率和通道上受支持的表示，然后通过最后几层映射回输出。CBN秩定义了保留在瓶颈中的频率的数量和类型，并部分证明了参数范数与深度和CBN秩的比例成正比。此外，我们还展示了网络的参数范数依赖于函数的规则性。我们发现任何具有接近最优参数范数的网络都会展示出CBN结构，这解释了下采样的常见实践；我们还验证了CBN结构在下采样下仍然成立。最后，我们使用CBN结构来解释...（摘要完整内容请见正文） |
| [^142] | [Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs](https://arxiv.org/abs/2402.08005) | 本文提出了一种改进的直接优化法（rDPO），通过使用合成数据来改善大规模语言模型（LLM）的行为调整。这种方法通过自我评论和广义DPO损失函数来优化学生LLM，并利用外部奖励模型提高合成数据质量，从而使rDPO在多个行为调整任务中表现出良好效果。 |
| [^143] | [Improvement and generalization of ABCD method with Bayesian inference](https://arxiv.org/abs/2402.08001) | 通过利用贝叶斯推断方法，改进和推广了ABCD方法，提供了一种更好的描述信号和背景的方式，并利用先验知识和不同可观测量之间的依赖关系来提高测量精度。 |
| [^144] | [NetInfoF Framework: Measuring and Exploiting Network Usable Information](https://arxiv.org/abs/2402.07999) | NetInfoF框架提供了一种快速度量和利用节点属性图中的可利用信息的方法，能同时处理链路预测和节点分类任务，并具备理论保证和闭式解的方法。 |
| [^145] | [Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical Similarity Search](https://arxiv.org/abs/2402.07970) | 本文评估了低维化学嵌入结合k-d树数据结构是否能够在保持对标准化学相似性搜索基准性能的同时实现快速最近邻查询 |
| [^146] | [SMX: Sequential Monte Carlo Planning for Expert Iteration](https://arxiv.org/abs/2402.07963) | 这项研究介绍了一种名为SMX的顺序蒙特卡洛规划算法，它利用可扩展的方法创建了有效的自我学习机制。它适用于离散和连续动作空间的环境，具有高并行性能。 |
| [^147] | [ProtIR: Iterative Refinement between Retrievers and Predictors for Protein Function Annotation](https://arxiv.org/abs/2402.07955) | ProtIR是一种新颖的变分伪似然框架，通过迭代优化功能预测器和检索器之间的知识，以提高蛋白质功能注释的准确性，并在没有大规模预训练的情况下表现出与或胜过预测器的能力。 |
| [^148] | [Optimizing the Design of an Artificial Pancreas to Improve Diabetes Management](https://arxiv.org/abs/2402.07949) | 通过神经进化算法优化人工胰腺治疗策略，减少糖尿病患者的血糖偏差，并且降低注射次数。 |
| [^149] | [evolSOM: an R Package for evolutionary conservation analysis with SOMs](https://arxiv.org/abs/2402.07948) | evolSOM是一个新颖的R包，利用自组织映射( SOMs)探索和可视化生物变量保守性，以便更容易地整合表型和基因型属性，并分析不同物种或条件之间的生物变量位移。 |
| [^150] | [Re-Envisioning Command and Control](https://arxiv.org/abs/2402.07946) | 重新构想的论文提出了未来指挥与控制（C2）决策需要面对更复杂和挑战性的环境，因此提出了基于人工智能系统与人类强有力伙伴关系的未来C2的愿景。这个愿景的核心是优化C2操作流程，保持协同努力，发展自适应的集体知识系统。 |
| [^151] | [Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs](https://arxiv.org/abs/2402.07938) | 本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介，通过对自然文本输入进行彻底分析，实现智能和响应式用户体验。 |
| [^152] | [A Physiological Sensor-Based Android Application Synchronized with a Driving Simulator for Driver Monitoring](https://arxiv.org/abs/2402.07937) | 本文提出了一种基于生理传感器的安卓应用程序，与驾驶模拟器同步工作，用于监测驾驶员的生理状态和驾驶表现。通过配置、选择、接收、处理、图形化表示和存储多种传感器的信号，该应用程序可以分析驾驶员的心电图、肌电图、皮肤电反应和动力学参数，并与驾驶模拟器同步分析驾驶安全性和效率。 |
| [^153] | [Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations](https://arxiv.org/abs/2402.07933) | 本文评估了无代码AutoML作为解决人工智能产品原型设计中挑战的方案，提出了一个概念框架，通过案例研究证实了其对非专家的支持潜力。无代码AutoML的战略整合有助于实现可访问和可解释的原型设计，对于学术界、管理者和决策者具有益处。 |
| [^154] | [Abstracted Trajectory Visualization for Explainability in Reinforcement Learning](https://arxiv.org/abs/2402.07928) | 强化学习(RL)模型中使用模糊轨迹可视化，使非RL专家能够推断出RL的行为模式。 |
| [^155] | [Research on Older Adults' Interaction with E-Health Interface Based on Explainable Artificial Intelligence](https://arxiv.org/abs/2402.07915) | 本研究基于可解释人工智能，探讨了老年人与电子健康界面的交互。结果显示，注入可解释人工智能的电子健康界面在消除年龄相关的数字鸿沟方面起到关键作用，并提出了直观可视化和简明解释等设计因素对于老年用户的重要性。本研究强调了XAI在电子健康领域的革命潜力。 |
| [^156] | [Towards Meta-Pruning via Optimal Transport](https://arxiv.org/abs/2402.07839) | 本文提出了一种名为Intra-Fusion的新方法，通过模型融合和最优传输的概念实现了神经网络的元剪枝，该方法能够有效恢复准确度并避免精调工作，同时具有高效和有前景的特点。 |
| [^157] | [Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism](https://arxiv.org/abs/2402.07735) | 本论文提出了一种利用BAM进行图结构推断的方法。通过神经网络模型，通过变形的耦合模拟输入数据进行训练，仅需通过一次前向传递即可进行推断。通过利用结构方程模型和随机生成的多变量切比雪夫多项式来模拟训练数据，方法能够泛化到线性和各种非线性依赖关系。引入了双线性注意机制（BAM）来处理依赖关系，该机制在转换数据的协方差矩阵水平上运行，并尊重对称正定矩阵流形的几何特性。实证评估证明了方法的有效性和性能。 |
| [^158] | [Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification](https://arxiv.org/abs/2402.07595) | 本文通过对比分析ImageNet预训练的深度学习模型和DINOv2在医学图像分类中的表现，发现在临床数据集中，DINOv2的性能不如ImageNet-b。 |
| [^159] | [Topological Safeguard for Evasion Attack based on the Interpretability of Artificial Neural Network Behavior](https://arxiv.org/abs/2402.07480) | 本文提出了一种基于人工神经网络行为解释性的拓扑保护方法，用于对抗逃避攻击。在过去几年中，深度学习技术的应用日益广泛，但也带来了新的网络安全威胁。逃避攻击是其中一种常见攻击，本文旨在设计一种能够对抗该攻击的有效防御方法。 |
| [^160] | [A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?](https://arxiv.org/abs/2402.07462) | 我们提出了HALO法规模式，使用激素分析来调节人工智能的行为模式，以解决价值装载问题中的“回形针最大化器”场景。 |
| [^161] | [Regression Trees for Fast and Adaptive Prediction Intervals](https://arxiv.org/abs/2402.07357) | 该论文提出了一种新的、与模型无关的方法族，用于校准具有局部覆盖保证的回归问题的预测区间。这种方法利用回归树和随机森林训练来创建最粗糙的特征空间划分，以近似条件覆盖，提供了准确、快速和自适应的预测区间。 |
| [^162] | [Sampling from the Mean-Field Stationary Distribution](https://arxiv.org/abs/2402.07355) | 本文研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，并提出了一种解耦的方法。该方法能够在多种情况下提供改进的保证，包括在均场区域优化某些双层神经网络的更好保证。 |
| [^163] | [HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs](https://arxiv.org/abs/2402.07309) | 本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。 |
| [^164] | [How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?](https://arxiv.org/abs/2402.07282) | 本文研究了如何在大型语言模型中权衡诚实和帮助性，在实验中发现强化学习改善了诚实和帮助性，而链式思维提示则偏向于帮助性。研究结果还展示了GPT-4 Turbo对对话框架和听众决策背景的敏感性。这些发现揭示了大型语言模型内化的对话价值观，并暗示零-shot提示可以在一定程度上引导这些抽象价值观。 |
| [^165] | [Towards Fast Stochastic Sampling in Diffusion Generative Models](https://arxiv.org/abs/2402.07211) | 本文提出了一种在扩散生成模型中进行快速随机采样的方法，通过对分裂积分器进行原则性修改，实现了更高的采样效率。在CIFAR-10数据集上进行实验，100次网络函数评估下的FID分数为2.36。 |
| [^166] | [Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning](https://arxiv.org/abs/2402.07107) | 这篇论文提出了一个新的统计方法，利用深度Q网络和分位数回归来在模型-free的分布式强化学习中引入不确定性，该方法通过结合深度证据学习和基于合规推理原则的分位数校准，提供了全局不确定性的显式、无样本计算，具有更高的计算和统计效率，并成功处理了超出分布范围的观测数据。 |
| [^167] | [Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization](https://arxiv.org/abs/2402.06974) | 本文提出了一种基于超网络的联邦融合算法hFedF，用于解决联邦领域泛化问题。该算法通过非线性融合客户模型，实现了对底层数据分布的全面理解，并在联邦学习中个性化和泛化之间达到了优秀的平衡。 |
| [^168] | [Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities](https://arxiv.org/abs/2402.06938) | 这项研究介绍了一种使用谈判能力的分布式基础设施高效资源调度的方法，通过引入一种基于模糊逻辑的代理自动谈判系统，优化云计算提供商和客户之间的协议，以提高效率。 |
| [^169] | [Discriminative Adversarial Unlearning](https://arxiv.org/abs/2402.06864) | 该论文提出了一种基于最小最大优化范式的机器反学习框架，利用强大的成员推断攻击来实现反学习，同时保持其总体性能，并增强了反学习能力。 |
| [^170] | [Retrieve, Merge, Predict: Augmenting Tables with Data Lakes](https://arxiv.org/abs/2402.06282) | 本文通过对数据湖中的数据发现进行深入分析，着重于表格增强，提出了准确检索连接候选人的重要性和简单合并方法的效率，以及现有解决方案的好处和局限性。 |
| [^171] | [Learn To be Efficient: Build Structured Sparsity in Large Language Models](https://arxiv.org/abs/2402.06126) | 本文通过引入一种新的算法"Learn-To-be-Efficient(LTE)"，提出了在大型语言模型(LLM)中构建结构化稀疏性的方法。该方法通过训练高效意识的LLM学习激活更少的神经元，取得更好的稀疏性和性能折衷。 |
| [^172] | [Limits of Transformer Language Models on Algorithmic Learning](https://arxiv.org/abs/2402.05785) | Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。 |
| [^173] | [Improving Token-Based World Models with Parallel Observation Prediction](https://arxiv.org/abs/2402.05643) | 该论文提出了一种改进基于令牌的世界模型的方法，通过引入并行观测预测机制（POP）来解决想象过程中出现的瓶颈问题。通过在一个新型TBWM代理中应用POP，想象速度提高了15.4倍，在不到12小时的训练时间内在Atari 100K基准测试中取得了超人类的表现。 |
| [^174] | [On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling](https://arxiv.org/abs/2402.05098) | 本研究探讨了训练扩散模型以从给定分布中采样的问题，并针对随机控制和采样提出了一种新的探索策略，通过基准测试比较了不同推断方法的相对优劣，并对过去的工作提出了质疑。 |
| [^175] | [Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces](https://arxiv.org/abs/2402.04691) | 本研究在一般希尔伯特空间中使用随机梯度下降（SGD）学习算子，提出了适用于目标算子的规则条件，并建立了SGD算法的收敛速度上界，同时展示了对于非线性算子学习的有效性及线性近似收敛特性。 |
| [^176] | [Densely Multiplied Physics Informed Neural Network](https://arxiv.org/abs/2402.04390) | 该论文通过改进神经网络架构，提出了一种密集乘法物理信息神经网络（DM-PINN）架构，它有效利用隐藏层的输出，显著提高了PINN的准确性和性能。 |
| [^177] | [MolTC: Towards Molecular Relational Modeling In Language Models](https://arxiv.org/abs/2402.03781) | 本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。 |
| [^178] | [Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective](https://arxiv.org/abs/2402.03496) | 移除自适应方法中的平方根可以在卷积结构上减小与SGD的泛化差距，同时保持在transformers上的性能。 |
| [^179] | [Vision-Language Models Provide Promptable Representations for Reinforcement Learning](https://arxiv.org/abs/2402.02651) | 本论文提出一种利用预训练的视觉-语言模型作为可提示的表示，为强化学习提供世界知识，使得代理能够更快地学习新的行为。在实验中，我们发现使用这种表示训练的策略在复杂环境下表现更好，优于通用图像表示和遵循指示的方法。 |
| [^180] | [Your Diffusion Model is Secretly a Certifiably Robust Classifier](https://arxiv.org/abs/2402.02316) | 这项研究提出了一种新的扩散分类器家族，称为噪声扩散分类器（NDCs），其具有最新的可证明的鲁棒性。通过将扩散分类器推广到分类高斯受损数据，并将其与随机平滑技术相结合，构建了具有非常量Lipschitzness的平滑分类器。这些NDCs显示出卓越的认证鲁棒性。 |
| [^181] | [Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection](https://arxiv.org/abs/2402.02196) | 我们提出了一种新颖的并行大规模排序和选择问题的聚类及征服方法，通过利用相关信息进行聚类以提高样本效率，在大规模AI应用中表现优异。 |
| [^182] | [PresAIse, An Enterprises Prescriptive AI Solution](https://arxiv.org/abs/2402.02006) | PresAIse是一种企业预测性人工智能解决方案，通过提供因果推断方法、可解释的决策制定方法和大型语言模型（LLMs）的集成，旨在解决企业预测性人工智能面临的数据限制、建议可解释性和数据科学家与业务用户之间的合作障碍等挑战。 |
| [^183] | [CroissantLLM: A Truly Bilingual French-English Language Model](https://arxiv.org/abs/2402.00786) | CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。 |
| [^184] | [Rates of Convergence in the Central Limit Theorem for Markov Chains, with an Application to TD Learning](https://arxiv.org/abs/2401.15719) | 本研究证明了一个非渐近的中心极限定理，并通过应用于TD学习，展示了其实际应用的可行性。 |
| [^185] | [Stochastic Gradient Descent for Additive Nonparametric Regression](https://arxiv.org/abs/2401.00691) | 本文介绍了一种用于训练加性模型的随机梯度下降算法，具有良好的内存存储和计算要求。在规范很好的情况下，通过仔细选择学习率，可以实现最小和最优的风险。 |
| [^186] | [LEFL: Low Entropy Client Sampling in Federated Learning](https://arxiv.org/abs/2312.17430) | LEFL提出了一种以保护数据隐私为目标的联邦学习采样策略，通过对客户端数据进行聚类，实现在每一轮次中的分层采样，从而优化全局模型的训练效果。 |
| [^187] | [Non-Vacuous Generalization Bounds for Large Language Models](https://arxiv.org/abs/2312.17173) | 这项研究提供了首个针对预训练大语言模型的非平凡泛化界限，表明语言模型能够发现适用于未见数据的规律性。建立了有效的压缩界限，证明较大的模型具有更好的泛化界限并更易压缩。 |
| [^188] | [PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs](https://arxiv.org/abs/2312.15230) | 本研究中，通过仅更新少部分高度表达力的参数，我们挑战了全参数重新训练的做法，在修剪后恢复或甚至提升了性能。PERP方法显著减少了计算量和存储需求。 |
| [^189] | [Adversarial Robustness on Image Classification with $k$-means](https://arxiv.org/abs/2312.09533) | 本文研究了增强$k$-means聚类算法对抗性攻击鲁棒性的挑战和策略，并提出了一种对抗性训练方法来改善在对抗场景中的测试性能。 |
| [^190] | [Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum](https://arxiv.org/abs/2312.06441) | 本文提出了一种基于半监督GNN的欺诈检测器SEC-GFD，通过混合过滤模块和局部环境约束模块解决了异质性和标签利用问题。 |
| [^191] | [What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes](https://arxiv.org/abs/2312.03096) | 这项工作提出了多义性的替代起源故事，称为偶然多义性，即使有足够的神经元来表示所有特征，也可能产生多义性。 |
| [^192] | [Bagged Regularized $k$-Distances for Anomaly Detection](https://arxiv.org/abs/2312.01046) | 本文提出了一种称为Bagged Regularized $k$-Distances for Anomaly Detection (BRDAD)的基于距离的算法，通过将非监督异常检测问题转化为凸优化问题，成功解决了基于距离算法中超参数选择的敏感性挑战，并通过包集成方法解决了处理大规模数据集时的效率问题。 |
| [^193] | [Meta Co-Training: Two Views are Better than One](https://arxiv.org/abs/2311.18083) | 元共训练通过在数据上构建不同的视角，并利用未标记数据进行共同训练，提高了半监督学习的性能。 |
| [^194] | [Experimental Analysis of Large-scale Learnable Vector Storage Compression](https://arxiv.org/abs/2311.15578) | 本文对嵌入向量压缩进行了全面比较分析和实验评估，引入了一个新的分类法，根据特征和方法学对这些技术进行分类，并进一步发展了一个模块化的方法来实现压缩算法。 |
| [^195] | [Learning to Compute Gr\"obner Bases](https://arxiv.org/abs/2311.12904) | 本文首次通过Transformer的训练实现了格罗布纳基的计算，通过解决随机生成格罗布纳基和将其转化为非格罗布纳多项式系统的问题，提出了解决计算任务中的关键挑战的方法。 |
| [^196] | [Omitted Labels in Causality: A Study of Paradoxes](https://arxiv.org/abs/2311.06840) | 本研究探讨了所谓的“遗漏标签上下文”，即训练数据仅限于可能标签的一个子集，并利用悖论展示了在这种上下文中因果推断面临的困难。研究发现在某些情况下，必须使用非可交换的处理组和对照组进行正确的校正。此外，研究还发现了结论网络与社会选择理论之间的有趣联系。 |
| [^197] | [PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks](https://arxiv.org/abs/2311.03415) | PowerFlowNet 是一种使用消息传递图神经网络进行功率流近似的新型架构，与传统的牛顿-拉夫逊方法相比，在简单的系统中速度提高了4倍，在实际的法国高电压网络中提高了145倍，同时在性能和执行时间方面明显优于其他传统方法。 |
| [^198] | [LILO: Learning Interpretable Libraries by Compressing and Documenting Code](https://arxiv.org/abs/2310.19791) | LILO是一种神经符号框架，通过迭代地合成、压缩和文档化代码来构建可解释且适用于特定问题领域的程序库。在其中，LILO结合了大型语言模型引导的程序合成和程序自动重构的算法进展，并且通过自动文档过程使得代码抽象可解释并提升性能。 |
| [^199] | [Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)](https://arxiv.org/abs/2310.09877) | 本研究通过引入创新的工具和技术，使用积累的局部效应（ALE）进行统计推断，在解决小数据集、直观特征和健壮推断方面取得了突破。这项工作推动了ALE及其应用的研究。 |
| [^200] | [A Homogenization Approach for Gradient-Dominated Stochastic Optimization](https://arxiv.org/abs/2308.10630) | 本文介绍了一种基于均匀化方法的梯度主导随机优化方法，通过满足梯度主导性质的随机函数，实现全局收敛。我们提供了样本复杂度分析，并通过方差减少技术提供了增强结果。实验结果表明，该方法在无需立方正则化的情况下达到了最佳样本复杂度。 |
| [^201] | [Optimized Gradient Tracking for Decentralized Online Learning](https://arxiv.org/abs/2306.06375) | 这项工作提出了一种新的通用梯度跟踪（GGT）框架，用于解决分布式在线学习中的优化问题。该方法在不需要梯度有界性假设的情况下，通过一种新颖的半定规划分析方法来获得理想的遗憾界，并可以适用于包括最先进算法和新动态版本的各种情况。 |
| [^202] | [Fast and Efficient Matching Algorithm with Deadline Instances](https://arxiv.org/abs/2305.08353) | 本文介绍了一种带有截止期限实例的快速高效匹配算法，通过引入带有截止期限的市场模型，提出了两种优化算法（FastGreedy和FastPostponedGreedy）。该算法在处理机器学习中的在线加权匹配问题时具有较快的速度和准确性。 |
| [^203] | [Customized Load Profiles Synthesis for Electricity Customers Based on Conditional Diffusion Models](https://arxiv.org/abs/2304.12076) | 本文提出了一种基于条件扩散模型的定制负荷曲线合成方法，用于解决电力客户的数据短缺问题，并实现高质量负荷曲线的合成。 |
| [^204] | [MFAI: A Scalable Bayesian Matrix Factorization Approach to Leveraging Auxiliary Information](https://arxiv.org/abs/2303.02566) | MFAI是一种可扩展的贝叶斯矩阵分解方法，通过利用辅助信息来克服由于数据质量差导致的挑战，具有灵活建模非线性关系和对辅助信息的鲁棒性。 |
| [^205] | [nSimplex Zen: A Novel Dimensionality Reduction for Euclidean and Hilbert Spaces](https://arxiv.org/abs/2302.11508) | nSimplex Zen是一种新颖的拓扑降维方法，它通过成对距离来降低高维空间的维度。它在物理内存使用和距离计算速度方面具有优势。 |
| [^206] | [Selective Uncertainty Propagation in Offline RL](https://arxiv.org/abs/2302.00284) | 本论文提出了一种名为选择性不确定性传播的方法，用于解决离线强化学习中的分布偏移问题。该方法通过自适应的方式建立置信区间，有效地处理了实际问题中策略学习的挑战。 |
| [^207] | [A Novel Framework for Policy Mirror Descent with General Parameterization and Linear Convergence](https://arxiv.org/abs/2301.13139) | 我们提出了一种新的政策优化框架，通过镜面下降自然地适应通用参数化，并获得了应用于通用参数化的基于政策梯度的方法的线性收敛保证。 |
| [^208] | [Subset verification and search algorithms for causal DAGs](https://arxiv.org/abs/2301.03180) | 本文研究了在学习因果DAG的子集关系时，识别所需最小干预集的问题，提出了两种有效算法进行解决。在子集验证问题上，我们提供了一种计算最小干预集的高效算法。在子集搜索问题上，我们提出了两种解决方法。 |
| [^209] | [Compressing Sign Information in DCT-based Image Coding via Deep Sign Retrieval](https://arxiv.org/abs/2209.10712) | 该论文提出了一种名为“符号检索”的方法来高效压缩离散余弦变换（DCT）系数的符号信息。通过排除符号信息并在解码器中补充，该方法在符号位数和计算成本方面优于以前的方法。 |
| [^210] | [Multistream Gaze Estimation with Anatomical Eye Region Isolation by Synthetic to Real Transfer Learning](https://arxiv.org/abs/2206.09256) | 该论文提出了一种多流注视估计的神经网络框架MSGazeNet，通过利用眼部解剖信息学习注视表示。研究首先使用合成数据集训练一个网络来隔离眼部区域，并通过域随机化实现将该网络转移到真实领域。通过合成到真实的迁移学习，实现了在真实世界眼部图像上生成掩模的目标。 |
| [^211] | [Deep Active Learning with Noise Stability](https://arxiv.org/abs/2205.13340) | 本文提出了一种利用噪声稳定性来估计数据不确定性的深度主动学习算法，通过测量模型参数通过噪声进行随机扰动时，输出与原始观测值之间的差异。该算法适用于多个领域的任务。 |
| [^212] | [Probabilistic Forecasting with Generative Networks via Scoring Rule Minimization](https://arxiv.org/abs/2112.08217) | 本论文提出了一种使用生成神经网络进行概率预测的方法，通过预测序列打分规则进行训练，避免了繁琐的超参数调整和不稳定的对抗训练，从而在概率预测中可靠地使用生成网络。 |
| [^213] | [Learning to be Fair: A Consequentialist Approach to Equitable Decision-Making](https://arxiv.org/abs/2109.08792) | 本文提出了一种以后果为导向的设计公平算法的替代框架，这种方法主要考虑决策的后果，而非仅仅追求在种族或性别群体间的均衡。 |
| [^214] | [Stochastic Low-rank Tensor Bandits for Multi-dimensional Online Decision Making](https://arxiv.org/abs/2007.15788) | 这项研究提出了一种针对多维度在线决策的随机低秩张量赌博算法。通过考虑有上下文和没有上下文的情况，提出了两种学习算法，并推导了有限时间的遗憾界限。 |
| [^215] | [Input Validation for Neural Networks via Runtime Local Robustness Verification](https://arxiv.org/abs/2002.03339) | 本文提出了通过运行时本地鲁棒性验证来验证神经网络输入的方法。实验证明，这种方法可以保护神经网络免受对抗性样本的影响，并提高准确性。 |
| [^216] | [Leveraging tensor kernels to reduce objective function mismatch in deep clustering](https://arxiv.org/abs/2001.07026) | 本文研究了深度聚类中的目标函数不匹配（OFM）问题，并发现基于自编码器的方法容易导致降低聚类性能和重构与聚类目标之间的不匹配。为了解决这个问题，我们提出了一种新的辅助目标方法，称为无监督伴随对象（UCO），通过核函数在网络的中间表示上制定聚类目标。 |
| [^217] | [Adaptive Block sparse regularization under arbitrary linear transform.](http://arxiv.org/abs/2401.15292) | 我们提出了一种在任意线性变换下重构具有块稀疏性的信号的方法，相比现有方法扩大了应用范围，并通过数值实验证明了其有效性。 |
| [^218] | [Emergent Dominance Hierarchies in Reinforcement Learning Agents.](http://arxiv.org/abs/2401.12258) | 本研究在强化学习中探讨了一种新的支配等级现象，并证明了在没有明确编程和内在奖励的情况下，强化学习代理能够自主发明、学习、实施和传递支配等级给新的群体。 |
| [^219] | [Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest Machine Learning.](http://arxiv.org/abs/2401.10386) | 本研究提出了一种基于随机森林机器学习的非侵入性诊断急性间室综合征的方法。使用压力传感电阻器检测肌肉间室压力，并通过蓝牙传输结果到Web应用程序。该诊断方法在准确率、灵敏度和F1得分等关键性能指标方面表现出色。 |
| [^220] | [Hypercomplex neural network in time series forecasting of stock data.](http://arxiv.org/abs/2401.04632) | 本文测试了三种不同的神经网络架构，将其用于股票数据的时间序列预测。结果显示，具有超复数密集层的架构在准确性方面表现类似于其他架构，但可训练参数更少。此外，输入时间序列的顺序对有效性有影响。 |
| [^221] | [Advancing Deep Active Learning & Data Subset Selection: Unifying Principles with Information-Theory Intuitions.](http://arxiv.org/abs/2401.04305) | 本论文探索了基于信息论原则的主动学习和主动采样方面的数据子集选择技术，以提高深度学习模型的标签和训练效率。 |
| [^222] | [Setting the Record Straight on Transformer Oversmoothing.](http://arxiv.org/abs/2401.04301) | Transformers are not inherently low-pass filters as previously thought and whether they oversmooth or not depends on the eigenspectrum of their update equations. Based on the analysis, a simple way to parameterize the weights of the Transformer update equations is derived, allowing for control over its spectrum and preventing oversmoothing. |
| [^223] | [Multilingual Instruction Tuning With Just a Pinch of Multilinguality.](http://arxiv.org/abs/2401.01854) | 本研究研究了多语言指令调优中的多语言性对跨语言指令遵循的影响。研究发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，只有40个多语言示例能够显著提高多语言指令遵循。总体来说，多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。 |
| [^224] | [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models.](http://arxiv.org/abs/2401.01335) | 本文提出了一种名为自我对弱语言模型进行细调（SPIN）的方法，通过模型自我对弈生成训练数据，并从中优化模型策略，从而将弱语言模型转化为强语言模型，无需额外的人类标注数据。 |
| [^225] | [Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs.](http://arxiv.org/abs/2310.17816) | 在有限先验知识下，通过局部分区发现算法（LDP），该研究解决了自动变量选择的问题。LDP根据与曝光-结果对{X,Y}相关的子集将变量集合Z进行分区，并区分混淆因素和其他变量类型。该算法具有理论保证，并在实践中观察到次二次的运行时间。 |
| [^226] | [Controlled Decoding from Language Models.](http://arxiv.org/abs/2310.17022) | 本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。 |
| [^227] | [Neural Collapse in Multi-label Learning with Pick-all-label Loss.](http://arxiv.org/abs/2310.15903) | 这项论文研究了在多标签分类任务中的神经坍缩现象。他们推广了之前在多类别分类中发现的神经坍缩现象，证明了在“选择所有标签”公式下存在广义的神经坍缩现象。他们还发现了在广义的神经坍缩中的一个组合性质。 |
| [^228] | [Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery.](http://arxiv.org/abs/2310.13576) | 本论文提出了一种在DAG空间中使用树搜索和模型驱动强化学习进行因果发现的方法，并通过两个实际任务的评估，证明了其在性能上显著优于目前最先进的无模型方法和贪婪搜索，具有很大的应用前景。 |
| [^229] | [Adaptive Pairwise Encodings for Link Prediction.](http://arxiv.org/abs/2310.11009) | 提出了一种自适应的对向编码方法，用于解决链路预测中现有方法的归纳偏差问题。该方法将消息传递神经网络和启发式方法结合起来，能够更好地分类各种不同因素形成的链路。 |
| [^230] | [Instruction Tuning with Human Curriculum.](http://arxiv.org/abs/2310.09518) | 本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。 |
| [^231] | [Optimal Sample Complexity for Average Reward Markov Decision Processes.](http://arxiv.org/abs/2310.08833) | 本论文解决了对于均匀收敛的马尔可夫决策过程的长期平均奖励最大化策略学习的样本复杂度问题，并建立了一个样本复杂度为$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$的优化策略估计器。 |
| [^232] | [FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication.](http://arxiv.org/abs/2310.07048) | FedMFS是一种新的多模态融合联邦学习方法，通过选择性模态通信解决了缺乏特定模态的异构客户问题，并设计了最优的模态上传策略以提高学习性能。 |
| [^233] | [Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables.](http://arxiv.org/abs/2310.05955) | 本论文提出了一种基于贝叶斯优化的新型质量多样性方法，用于解决混合连续、离散和分类变量的约束优化问题，旨在提供具有多样性性质的最优解集。 |
| [^234] | [Efficient Agnostic Learning with Average Smoothness.](http://arxiv.org/abs/2309.17016) | 该论文研究了基于平均光滑度的无参回归问题，提出了无分布限制下的统一收敛界限和高效无偏学习算法。 |
| [^235] | [Self-Supervised Blind Source Separation via Multi-Encoder Autoencoders.](http://arxiv.org/abs/2309.07138) | 本论文提出了一种基于多编码器自编码器和自监督学习的方法，用于解决盲源分离问题。通过训练网络进行输入解码和重构，然后利用编码掩蔽技术进行源推断，同时引入路径分离损失以促进稀疏性。 |
| [^236] | [Les Houches Lectures on Deep Learning at Large & Infinite Width.](http://arxiv.org/abs/2309.01592) | 本论文主要以无穷宽度和大宽度范围内的深度神经网络为研究对象，讨论了这些网络的各种统计和动力学特性，包括随机网络的性质、训练后的网络与线性模型、核函数和高斯过程之间的关系，以及对大但有限宽度网络在初始化和训练后的摄动和非摄动处理。 |
| [^237] | [SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies.](http://arxiv.org/abs/2308.12367) | 本文提出了一种更安全的算法补救方法（SafeAR），该方法通过考虑风险因素在计算和评估补救措施时，为那些受到机器学习模型决策不利影响的个体提供更可靠的建议。 |
| [^238] | [MDB: Interactively Querying Datasets and Models.](http://arxiv.org/abs/2308.06686) | MDB是一个调试框架，用于互动查询数据集和模型。它通过集成函数式编程与关系代数，能够快速迭代和优化查询，发现和描述错误和模型行为。实验证明，MDB比其他工具能够实现更快的查询速度加快和查询长度缩短。 |
| [^239] | [Finding Optimal Diverse Feature Sets with Alternative Feature Selection.](http://arxiv.org/abs/2307.11607) | 本文引入了替代特征选择的概念，将其形式化为优化问题，并通过约束定义了替代特征集，使用户可以控制替代的数量和差异性。我们证明了该问题的NP-hard性，并讨论了如何将传统特征选择方法作为目标集成。实验证明替代特征集确实可以具有高预测质量，同时分析了几个影响因素。 |
| [^240] | [Amortized Variational Inference: When and Why?.](http://arxiv.org/abs/2307.11018) | 本文研究了分期变分推断作为近似后验推断的一种通用替代方法，探讨了何时能够达到与传统的因子化变分推断相同的最优解。 |
| [^241] | [Nonlinear Processing with Linear Optics.](http://arxiv.org/abs/2307.08533) | 该论文提出了一种利用多次散射实现多层光学网络的新框架，可以以低光功率同时合成线性和非线性转换，实现能量高效和高速的光学实现神经网络。 |
| [^242] | [From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs.](http://arxiv.org/abs/2307.08433) | 这篇论文介绍了一种在连续时间动态图上具有低延迟的节点嵌入框架，通过提出流式低延迟的近似随机游走特征，计算时间感知节点嵌入以总结多跳信息。 |
| [^243] | [Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks.](http://arxiv.org/abs/2307.06887) | 通过双层ReLU神经网络，本论文提出了一种可证明的多任务表示学习方法，用于解决神经网络在实践中同时训练多个任务时遇到的问题。 |
| [^244] | [Towards a Certified Proof Checker for Deep Neural Network Verification.](http://arxiv.org/abs/2307.06299) | 面向DNN验证的一种新型证明检查器实现，通过提供数值稳定性和更大的可验证性改进现有搜索工具存在的问题。这一实现利用了Imandra的两个关键能力：无限精度实数算术和形式化验证基础设施。 |
| [^245] | [Active Sensing with Predictive Coding and Uncertainty Minimization.](http://arxiv.org/abs/2307.00668) | 本论文提出了一种基于预测编码和不确定性最小化的主动感知方法，能够在各种任务中实现有效的探索和学习。 |
| [^246] | [Comparative Analysis of Segment Anything Model and U-Net for Breast Tumor Detection in Ultrasound and Mammography Images.](http://arxiv.org/abs/2306.12510) | 研究采用U-Net和pretrained SAM两种深度学习架构，针对乳腺超声和乳腺X线图像，进行肿瘤区域的识别和分割。结果表明，U-Net模型对于不同类型的良性和恶性肿瘤的识别和分割效果更好。 |
| [^247] | [Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey.](http://arxiv.org/abs/2306.06123) | 本文总结了对抗性攻击和防御在可解释人工智能中的研究。列出了现有的不安全因素，并表明了本领域的新兴研究方向。 |
| [^248] | [Neural Algorithmic Reasoning for Combinatorial Optimisation.](http://arxiv.org/abs/2306.06064) | 本文提出了一种用于组合优化问题的神经算法推理方法，旨在解决旅行商问题。该方法是通过在TSP实例训练之前，将神经模型用相关算法进行预训练来实现的。实验结果表明，该方法可以显著提高TSP问题的解决效率。 |
| [^249] | [Interpreting and Improving Diffusion Models Using the Euclidean Distance Function.](http://arxiv.org/abs/2306.04848) | 本文利用欧几里得距离函数解释去噪扩散模型，并提出了一种新的采样器。采样器表现出了最先进的FID得分，并能够生成高质量的样本。 |
| [^250] | [How does over-squashing affect the power of GNNs?.](http://arxiv.org/abs/2306.03589) | 本文通过测量节点之间成对交互的水平，提供了严格的分析，以确定具有一定容量的MPNN可以学习哪些节点特征的函数类别。结果表明，为了保证节点对之间的充分通信，MPNN的容量必须是... |
| [^251] | [Tackling Unbounded State Spaces in Continuing Task Reinforcement Learning.](http://arxiv.org/abs/2306.01896) | 本文提出了一种基于李雅普诺夫思想的奖励塑形方法，用于在持续任务的强化学习中应对无界状态空间，旨在鼓励代理器学习稳定性和最优策略。 |
| [^252] | [Stability-penalty-adaptive Follow-the-regularized-leader: Sparsity, Game-dependency, and Best-of-both-worlds.](http://arxiv.org/abs/2305.17301) | 本文开发了一种稳定性惩罚自适应（SPA）学习率，该学习率使FTRL具有稀疏性、游戏依赖性和最佳世界（BOBW）三种适应性类型，其中SPA-sparse算法可适应于未知的稀疏级别，SPA-game-dependency算法可根据所玩的游戏自适应地改变其行为，BOBW算法则是既具有稀疏性又具有游戏依赖性的适应性算法。 |
| [^253] | [Differentially private low-dimensional representation of high-dimensional data.](http://arxiv.org/abs/2305.17148) | 本文提出了一种在保护个人敏感信息的情况下，生成高效低维合成数据的算法，并在Wasserstein距离方面具有效用保证；与标准扰动分析不同，使用私有主成分分析过程避免了维度诅咒的影响。 |
| [^254] | [Dynamic Inter-treatment Information Sharing for Heterogeneous Treatment Effects Estimation.](http://arxiv.org/abs/2305.15984) | 本论文提出了一种基于深度学习的HyperCATE框架，通过软权重共享的方式实现端到端信息共享来解决现有CATE学习器中的有偏估计问题，并在IHDP、ACIC-2016和Twins基准测试中评估了该框架的表现。 |
| [^255] | [Cost-aware learning of relevant contextual variables within Bayesian optimization.](http://arxiv.org/abs/2305.14120) | 本文提出一种基于代价感知的模型选择BO方法SADCBO，通过对后验代理模型的敏感性分析来学习关于环境的相关情境信息，并通过平均模型预测来最小化优化代价，在实验中表现出卓越的性能。 |
| [^256] | [IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers.](http://arxiv.org/abs/2305.06741) | 本文提出了一种新的方法，在建模电子病历时间序列时，利用直接近似IVP的过程来消除递归计算，从而提高计算效率和训练速度。与目前基于IVP求解器和递归神经网络方法相比，本方法可以达到类似的分类和预测性能。 |
| [^257] | [The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges.](http://arxiv.org/abs/2304.08242) | 深层潜在位置主题模型用于网络聚类和表示，通过基于模型的聚类策略和概率模型对节点和边进行联合表示，并使用模型选择准则进行参数选择。 |
| [^258] | [Certifiable (Multi)Robustness Against Patch Attacks Using ERM.](http://arxiv.org/abs/2303.08944) | 该文研究了针对贴片攻击的防御方法，并提出了一种使用ERM算法学习可证明具有多种蒙版下都具有鲁棒性的预测模型算法，实现了严格的证明多鲁棒性对贴片攻击。 |
| [^259] | [Making Batch Normalization Great in Federated Deep Learning.](http://arxiv.org/abs/2303.06530) | 本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。 |
| [^260] | [Correlation Clustering with Active Learning of Pairwise Similarities.](http://arxiv.org/abs/2302.10295) | 本文研究了相关聚类中成对相似性不事先给出的情况，并开发了一个通用的主动学习框架，适应各种相关聚类算法和查询策略，同时具有适应性灵活、噪声鲁棒性等优势。 |
| [^261] | [Practical Differentially Private Hyperparameter Tuning with Subsampling.](http://arxiv.org/abs/2301.11989) | 本文提出了一种利用子采样实现实用的差分隐私超参数调整的方法，相比基准算法，可以在不损失最终评估结果的情况下提高隐私-实用性权衡。 |
| [^262] | [Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning.](http://arxiv.org/abs/2301.11916) | 本研究发现，大型语言模型可以被视为隐式的主题模型，并提出了一种算法，从注释数据中选择最佳示范，大大提高了上下文学习的能力。 |

# 详细

[^1]: 超越尺度：具有任意类型未知超参数的无遗憾贝叶斯优化

    Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type

    [https://rss.arxiv.org/abs/2402.01632](https://rss.arxiv.org/abs/2402.01632)

    这篇论文提出了一种新的贝叶斯优化算法，可以处理具有任意类型未知超参数的情况，并具有无遗憾特性。

    

    贝叶斯优化需要拟合高斯过程模型，而拟合高斯过程模型需要指定超参数 - 大部分理论文献假设这些超参数是已知的。之前的理论研究通常假设数据在空间中均匀填充，而常用的高斯过程超参数的最大似然估计器只有在这种情况下才是一致的。然而，在贝叶斯优化中，数据不一定满足这种均匀填充的条件。由于无法保证超参数估计的正确性，并且这些超参数可以显著影响高斯过程拟合，因此对具有未知超参数的贝叶斯优化进行理论分析非常具有挑战性。之前提出的具有无遗憾特性的算法仅能处理特殊情况下的未知长度尺度、再生核希尔伯特空间范数，并且仅适用于频率派的情况。我们提出了一种新的算法，命名为HE-GP-UCB，它是第一个具有无遗憾特性的算法，在具有未知超参数的情况下实现了贝叶斯优化。

    Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known. The commonly used maximum likelihood estimator for hyperparameters of the Gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in Bayesian optimisation. Since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the Gaussian process fit, theoretical analysis of Bayesian optimisation with unknown hyperparameters is very challenging. Previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel Hilbert space norm and applied only to the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparame
    
[^2]: 弱监督学习器实现具有可证明性能保证的AI错误修正

    Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees

    [https://rss.arxiv.org/abs/2402.00899](https://rss.arxiv.org/abs/2402.00899)

    这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。

    

    我们提出了一种新的方法来处理AI错误，通过引入具有先验性能保证的弱监督AI错误修正器。这些AI修正器是辅助映射，其作用是通过批准或拒绝以调节之前构建的底层分类器的决策。拒绝一个决策可以用作建议放弃做出决策的信号。该工作的一个关键技术重点是通过对错误决策的概率界限提供这些新的AI修正器的性能保证。这些界限是分布不可知的，并且不依赖于对数据维度的假设。我们的实证示例说明了该框架如何应用于改善在训练数据稀缺的具有挑战性的真实世界任务中图像分类器的性能。

    We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
    
[^3]: IM-3D：用于高质量3D生成的迭代多视角扩散和重构

    IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation

    [https://arxiv.org/abs/2402.08682](https://arxiv.org/abs/2402.08682)

    本文提出了一种名为IM-3D的方法，通过考虑视频生成器和使用高斯平铺的3D重构算法，从生成的视图直接输出高质量的3D结果，实现了更高效的流水线、更好的质量和更高的可用3D资产产出。

    

    大多数文本到3D生成器都是基于已训练过的数十亿图像的文本到图像模型构建的。它们使用Score Distillation Sampling (SDS)的变体，这种方法较慢、不太稳定且容易产生伪影。一种缓解方法是将2D生成器微调为多视角感知，可以帮助消除伪影，或者与重构网络结合，直接输出3D对象。在本文中，我们进一步探索了文本到3D模型的设计空间。通过考虑视频而非图像生成器，我们显著改善了多视角生成。结合一种使用高斯平铺的3D重构算法，可以优化鲁棒的基于图像的损失，我们直接从生成的视图输出高质量的3D结果。我们的新方法IM-3D将2D生成器网络的计算次数减少了10-100倍，从而实现了更高效的流水线、更好的质量、更少的几何不一致性和更高的可用3D资产产出。

    Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images. They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly. In this paper, we further explore the design space of text-to-3D models. We significantly improve multi-view generation by considering video instead of image generators. Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views. Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets.
    
[^4]: 通过无分类器引导来减轻大型视觉语言模型中的物体幻觉

    Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance

    [https://arxiv.org/abs/2402.08680](https://arxiv.org/abs/2402.08680)

    本文介绍了一种名为MARINE的框架，用于通过无分类器引导来减少大型视觉语言模型的物体幻觉。该框架无需训练或API访问，并通过集成视觉模型和引入额外的物体基础特征来提高模型的生成精确性和效率。

    

    大型视觉语言模型（LVLM）的进展越来越突出了它们在图像中产生虚假物体的严重问题。为了解决这个问题，先前的研究着重于使用特殊策划的数据集或强大的LLM（例如GPT-3.5）来纠正LVLM的输出。然而，这些方法要求昂贵的训练/微调或API访问先进的LLM来在生成后纠正模型的输出。在本文中，我们通过引入一个名为通过无分类器引导缓解幻觉的框架（MARINE）来解决这个挑战，该框架既无需训练也无需API访问，可以在生成过程中有效地减少物体幻觉。具体而言，MARINE通过集成现有的开源视觉模型丰富LVLM的视觉语境，并使用无分类器引导来整合额外的物体基础特征，以提高LVLM生成的精确性。

    The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Thr
    
[^5]: COLD-Attack: 用于具有隐秘性和可控性的LLM越狱

    COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability

    [https://arxiv.org/abs/2402.08679](https://arxiv.org/abs/2402.08679)

    本文提出了COLD-Attack框架，旨在实现具有隐秘性和可控性的LLM越狱。通过建立可控文本生成与攻击生成之间的关联，采用了能量限制解码与Langevin动力学算法，使得在不同的控制要求下搜索对抗性LLM攻击成为可能。

    

    最近对大型语言模型（LLMs）进行越狱的注意力越来越多。为了全面评估LLM的安全性，有必要考虑具有不同属性的越狱，例如上下文连贯性以及情感/风格变化，因此研究可控性越狱是有益的，即如何对LLM攻击进行控制。在本文中，我们正式形式化了可控性攻击生成问题，并建立了该问题与可控文本生成之间的新型关联，这是自然语言处理中一个被广泛探索的主题。基于这种关联，我们改进了能量限制解码与Langevin动力学（COLD）的算法，这是一种在可控文本生成中的高效算法，并引入了COLD-Attack框架，该框架统一且自动化地搜索各种控制要求下的对抗性LLM攻击，例如流畅性、隐秘性、情感和左右连贯性。

    Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controlla
    
[^6]: 图马巴：面向基于状态空间模型的图学习

    Graph Mamba: Towards Learning on Graphs with State Space Models

    [https://arxiv.org/abs/2402.08678](https://arxiv.org/abs/2402.08678)

    本文提出了一种基于选择性SSMs的新类GNNs框架——图马巴网络（GMNs），通过不依赖于Transformer、复杂的消息传递和位置/结构编码（SE/PE），解决了传统GNNs的过度压缩和无法捕捉长程依赖的问题。

    

    图神经网络（GNNs）在图表示学习方面显示出了很大的潜力。大多数GNNs定义了一种局部消息传递机制，通过堆叠多个层在图上传播信息。然而，这些方法已知存在两个主要限制：过度压缩和无法捕捉长程依赖。最近，图转换器（GTs）作为消息传递神经网络（MPNNs）的一种强大替代方法出现。然而，GTs具有二次计算成本，在图结构上缺乏归纳偏差，并且依赖复杂的位置/结构编码（SE/PE）。在本文中，我们展示了在实践中，尽管Transformer、复杂的消息传递和SE/PE对于良好性能而言是足够的，但并非必需。受到最近的状态空间模型（SSMs）（例如Mamba）的成功启发，我们提出了图马巴网络（GMNs），这是一种基于选择性SSMs的新类GNNs的通用框架。我们讨论并对新的c进行分类。

    Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). In this paper, we show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general framework for a new class of GNNs based on selective SSMs. We discuss and categorize the new c
    
[^7]: 关于非可分函数的近似消息传递收敛性分析及其在多类别分类中的应用

    A Convergence Analysis of Approximate Message Passing with Non-Separable Functions and Applications to Multi-Class Classification

    [https://arxiv.org/abs/2402.08676](https://arxiv.org/abs/2402.08676)

    该论文对非可分函数的近似消息传递（AMP）动力学进行了收敛性分析，并应用于多类别分类中的凸优化问题。

    

    受最近将近似消息传递（AMP）应用于多类别分类中凸优化分析的启发[Loureiro, et. al., 2021]，我们提出了关于非可分多变量非线性动力学的AMP收敛性分析。作为一个应用，我们提出了对所激发的凸优化问题的完整（和独立的）分析。

    Motivated by the recent application of approximate message passing (AMP) to the analysis of convex optimizations in multi-class classifications [Loureiro, et. al., 2021], we present a convergence analysis of AMP dynamics with non-separable multivariate nonlinearities. As an application, we present a complete (and independent) analysis of the motivated convex optimization problem.
    
[^8]: 使用上下文学习的神经网络中出现人类课程效应

    Human Curriculum Effects Emerge with In-Context Learning in Neural Networks

    [https://arxiv.org/abs/2402.08674](https://arxiv.org/abs/2402.08674)

    人类学习对示例课程和任务结构有敏感性。研究发现，在神经网络和语言模型中，通过上下文学习方法可以同时获得分组和交错训练的优势。

    

    人类学习对规则结构和训练中所使用的示例课程非常敏感。在由简洁规则控制的任务中，当相关示例在多次试验中被分组时，学习更加稳健；但在缺乏这样的规则的情况下，交错训练更加有效。迄今为止，没有神经模型能够同时捕捉到这些看似矛盾的效应。在本文中，我们展示了“上下文学习”（ICL）在使用元学习进行训练的神经网络和大型语言模型（LLMs）中自发产生了同样的权衡。ICL是通过内层循环算法在激活动力学中实现的一种“上下文内学习”（in-context learning）的能力，可以在没有权重更改的情况下学习新任务。对预训练的LLMs和元学习变压器进行的实验表明，ICL在涉及规则结构的任务中展示出了人类所示的分组优势，而同时进行权重学习则复制了人类在缺少这样结构的任务上所观察到的交错优势。

    Human learning is sensitive to rule-like structure and the curriculum of examples used for training. In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective. To date, no neural model has simultaneously captured these seemingly contradictory effects. Here we show that this same tradeoff spontaneously emerges with "in-context learning" (ICL) both in neural networks trained with metalearning and in large language models (LLMs). ICL is the ability to learn new tasks "in context" - without weight changes - via an inner-loop algorithm implemented in activation dynamics. Experiments with pretrained LLMs and metalearning transformers show that ICL exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structu
    
[^9]: 模型评估与选择在时间分布转移下的研究

    Model Assessment and Selection under Temporal Distribution Shift

    [https://arxiv.org/abs/2402.08672](https://arxiv.org/abs/2402.08672)

    本文研究了在变化环境中的模型评估与选择问题，通过合成不同时期的数据集，并开发了自适应滚动窗口方法来估计模型的泛化误差以及比较不同模型之间的差异。实验证明了我们提出的方法在非稳态数据中的适应性。

    

    我们通过合成当前时期和历史时期的数据集，研究了在变化环境中的模型评估与选择。为了解决未知和可能任意的时间分布转移，我们开发了一种自适应滚动窗口方法来估计给定模型的泛化误差。这种策略还通过估计两个候选模型之间的泛化误差差异来方便比较。我们进一步将两两比较整合到单场淘汰赛中，从候选模型集合中实现了近乎最优的模型选择。理论分析和数值实验证明了我们所提出方法对数据非稳态的适应性。

    We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.
    
[^10]: 目标分数匹配

    Target Score Matching

    [https://arxiv.org/abs/2402.08667](https://arxiv.org/abs/2402.08667)

    本文提出了目标分数匹配方法，通过最小化回归损失来估计目标分布的噪声版本的分数。与传统的噪声分数匹配方法不同，该方法在低噪声水平下能够获得更准确的分数估计。

    

    噪声分数匹配通过最小化回归损失来估计目标分布的噪声版本的分数，并广泛用于训练流行的去噪扩散模型。然而，众所周知，噪声分数匹配的一个局限性是在低噪声水平下会产生较差的分数估计。这个问题在物理科学和对于已知干净的原始目标分数的蒙特卡洛抽样任务中特别不利。直观地说，在这些情况下，估计目标稍有噪声版本的分数应该是一个简单的任务。在本文中，我们解决了这个缺点，并展示了利用目标分数的知识确实可以实现。我们提出了目标分数身份和相应的目标分数匹配回归损失，这使我们能够在低噪声水平下获得具有有利属性的分数估计。

    Denoising Score Matching estimates the score of a noised version of a target distribution by minimizing a regression loss and is widely used to train the popular class of Denoising Diffusion Models. A well known limitation of Denoising Score Matching, however, is that it yields poor estimates of the score at low noise levels. This issue is particularly unfavourable for problems in the physical sciences and for Monte Carlo sampling tasks for which the score of the clean original target is known. Intuitively, estimating the score of a slightly noised version of the target should be a simple task in such cases. In this paper, we address this shortcoming and show that it is indeed possible to leverage knowledge of the target score. We present a Target Score Identity and corresponding Target Score Matching regression loss which allows us to obtain score estimates admitting favourable properties at low noise levels.
    
[^11]: 使用分散的相位振荡器学习新生步态：关于观察、奖励和反馈的作用

    Learning Emergent Gaits with Decentralized Phase Oscillators: on the role of Observations, Rewards, and Feedback

    [https://arxiv.org/abs/2402.08662](https://arxiv.org/abs/2402.08662)

    该论文通过将相位观察、简单的相位奖励和局部反馈动力学相结合，提出了一种使用分散相位振荡器学习动物步态的模型，有效地实现了新生步态偏好的策略。

    

    我们提出了一个最小化相位振荡器模型来学习四足动物的运动。每个振荡器仅与自身和相应的腿之间通过地面反作用力的局部反馈相连，这可以被解释为一个观察者反馈增益。我们将振荡器本身解释为潜在的接触状态估计器。通过系统的消除研究，我们表明相位观察、简单的基于相位的奖励和局部反馈动力学的结合引导出展现新生步态偏好的策略，同时使用了一组简化的奖励，而没有预先指定特定的步态。代码是开源的，视频摘要可在https://youtu.be/1NKQ0rSV3jU处获取。

    We present a minimal phase oscillator model for learning quadrupedal locomotion. Each of the four oscillators is coupled only to itself and its corresponding leg through local feedback of the ground reaction force, which can be interpreted as an observer feedback gain. We interpret the oscillator itself as a latent contact state-estimator. Through a systematic ablation study, we show that the combination of phase observations, simple phase-based rewards, and the local feedback dynamics induces policies that exhibit emergent gait preferences, while using a reduced set of simple rewards, and without prescribing a specific gait. The code is open-source, and a video synopsis available at https://youtu.be/1NKQ0rSV3jU.
    
[^12]: SAGMAN: 用于图神经网络在流形上的稳定性分析的方法

    SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds

    [https://arxiv.org/abs/2402.08653](https://arxiv.org/abs/2402.08653)

    SAGMAN是一种用于检验图神经网络稳定性的谱框架，它通过评估非线性映射中的距离失真来衡量GNN的稳定性。为了进行有意义的稳定性分析，我们提出了一种距离保持的图降维方法。

    

    现代图神经网络（GNN）对输入图结构和节点特征的变化敏感，可能导致不可预测的行为和性能下降。本文引入了一种称为SAGMAN的谱框架，用于检验GNN的稳定性。该框架评估非线性映射中GNN在输入和输出流形之间引起的距离失真: 当输入流行中两个附近的节点（通过GNN模型）被映射到输出流行上的两个远离的节点时，意味着存在较大的距离失真，从而导致GNN的稳定性较差。我们提出了一种距离保持的图降维（GDR）方法，利用谱图嵌入和概率图模型（PGMs）来创建低维的输入/输出基于图的流形，以进行有意义的稳定性分析。我们的实证评估表明，SAGMAN能够有效评估每个节点在面对不同边缘或特征扰动时的稳定性。

    Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
    
[^13]: 为量子分类器生成通用对抗扰动

    Generating Universal Adversarial Perturbations for Quantum Classifiers

    [https://arxiv.org/abs/2402.08648](https://arxiv.org/abs/2402.08648)

    这项工作引入了QuGAP-一个用于生成量子分类器的通用对抗扰动的新框架，并通过实验证明了量子分类器容易受到这些攻击。

    

    量子机器学习（QML）作为一个有前景的研究领域已经出现，旨在利用量子计算的能力来增强现有的机器学习方法。最近的研究发现，像经典的模型一样，基于参数化量子电路（PQC）的QML模型也容易受到对抗性攻击。此外，在量子领域，理论上已经证明了存在通用对抗扰动（UAP），并且已在量子分类器的背景下进行了实证研究。在这项工作中，我们引入了QuGAP：一个为量子分类器生成UAP的新框架。我们对基于PQC的分类器的加性UAP的概念进行了理论上的证明，并通过概率模型（QuGAP-A）来制造加性UAP，并实验证明了量子分类器容易受到这类攻击。此外，我们提出了一种使用量子生成模型和压缩方法生成幺正UAP的新方法（QuGAP-U），并且进行了实验验证。

    Quantum Machine Learning (QML) has emerged as a promising field of research, aiming to leverage the capabilities of quantum computing to enhance existing machine learning methodologies. Recent studies have revealed that, like their classical counterparts, QML models based on Parametrized Quantum Circuits (PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of Universal Adversarial Perturbations (UAPs) in the quantum domain has been demonstrated theoretically in the context of quantum classifiers. In this work, we introduce QuGAP: a novel framework for generating UAPs for quantum classifiers. We conceptualize the notion of additive UAPs for PQC-based classifiers and theoretically demonstrate their existence. We then utilize generative models (QuGAP-A) to craft additive UAPs and experimentally show that quantum classifiers are susceptible to such attacks. Moreover, we formulate a new method for generating unitary UAPs (QuGAP-U) using quantum generative models and a 
    
[^14]: 窥探残差学习的内幕

    Peeking Behind the Curtains of Residual Learning

    [https://arxiv.org/abs/2402.08645](https://arxiv.org/abs/2402.08645)

    本文窥探了残差学习的内幕，发现了导致平面神经网络收敛失败的“逐渐消失的输入”现象，并提出了通过保持更好的幸存神经元下界的残差连接作为一种解决方案。

    

    残差学习在深度可扩展的神经网络中得到了广泛应用。然而，导致残差学习成功的基本原理仍然不清楚，这阻碍了深度可扩展平面网络的有效训练。本文通过揭示导致平面神经网络收敛失败的“逐渐消失的输入”现象，窥探了残差学习的内幕：由于非线性性质，在平面层中输入逐渐被损坏，从而导致学习特征表示的挑战。我们从理论上证明了平面神经网络如何将输入退化为随机噪声，并强调了通过保持更好的幸存神经元下界的残差连接的重要性。基于我们的理论发现，我们提出了“平面神经网络假设”（PNNH），该假设将非线性层之间的内部路径确定为残差学习中最关键的部分，并建立了

    The utilization of residual learning has become widespread in deep and scalable neural nets. However, the fundamental principles that contribute to the success of residual learning remain elusive, thus hindering effective training of plain nets with depth scalability. In this paper, we peek behind the curtains of residual learning by uncovering the "dissipating inputs" phenomenon that leads to convergence failure in plain neural nets: the input is gradually compromised through plain layers due to non-linearities, resulting in challenges of learning feature representations. We theoretically demonstrate how plain neural nets degenerate the input to random noise and emphasize the significance of a residual connection that maintains a better lower bound of surviving neurons as a solution. With our theoretical discoveries, we propose "The Plain Neural Net Hypothesis" (PNNH) that identifies the internal path across non-linear layers as the most critical part in residual learning, and establi
    
[^15]: 学习图像压缩与文本质量增强

    Learned Image Compression with Text Quality Enhancement

    [https://arxiv.org/abs/2402.08643](https://arxiv.org/abs/2402.08643)

    本文提出了一种学习图像压缩与文本质量增强的方法，通过最小化新的文本logit损失来改善重构文本的感知质量。实验证明，将该损失函数与适当的加权相结合，可以显著提高重构文本的质量，并在实际应用中取得了优异的性能。

    

    学习图像压缩在实现超低比特率方面效率高，因此获得了广泛的应用。然而，包含大量文本内容的图像，特别是屏幕内容图像（SCI），经过这样的压缩后往往会出现文本失真。为了解决这个问题，我们提出了一种新的文本logit损失，用于量化原始图像和重构图像之间的文本差异，从而提高重构文本的感知质量。通过在不同数据集上进行严格的实验，并采用先进的算法，我们的研究结果显示，将该损失函数与适当的加权相结合可以显著提高重构文本的质量。值得注意的是，通过应用文本logit损失函数，我们在两个屏幕截图数据集上获得了平均Bjontegaard delta (BD)为-32.64%的字符错误率（CER）和-28.03%的词错误率（WER）。此外，我们还提供了定量指标来衡量文本质量的改善程度。

    Learned image compression has gained widespread popularity for their efficiency in achieving ultra-low bit-rates. Yet, images containing substantial textual content, particularly screen-content images (SCI), often suffers from text distortion at such compressed levels. To address this, we propose to minimize a novel text logit loss designed to quantify the disparity in text between the original and reconstructed images, thereby improving the perceptual quality of the reconstructed text. Through rigorous experimentation across diverse datasets and employing state-of-the-art algorithms, our findings reveal significant enhancements in the quality of reconstructed text upon integration of the proposed loss function with appropriate weighting. Notably, we achieve a Bjontegaard delta (BD) rate of -32.64% for Character Error Rate (CER) and -28.03% for Word Error Rate (WER) on average by applying the text logit loss for two screenshot datasets. Additionally, we present quantitative metrics tai
    
[^16]: 通过机器学习在不断演化的知识图谱上预测高影响力的研究主题

    Forecasting high-impact research topics via machine learning on evolving knowledge graphs

    [https://arxiv.org/abs/2402.08640](https://arxiv.org/abs/2402.08640)

    通过机器学习预测未发布研究想法的影响力，我们使用一个由超过2100万篇科学论文构建的演化知识图谱，结合论文内容和历史引用的信息，高准确度预测未来的演化网络动态和新的研究方向的影响力。

    

    科学出版物的指数增长对人类研究者构成了严峻挑战。它迫使研究者将注意力集中在更狭窄的子领域上，使得发现其他领域的新颖且有影响力的研究想法和合作变得困难。虽然有办法预测科学论文未来的引用次数，但通常需要等到研究完成并且论文写成后才能进行评估，这样就错过了想法构思的早期阶段。在本文中，我们展示了如何预测从未被研究者发布的想法的影响力。为此，我们开发了一个大型的演化知识图谱，其中包含超过2100万篇科学论文。它结合了从论文内容中创建的语义网络和从历史引用中创建的影响网络。利用机器学习，我们可以高准确度地预测演化网络的动态情况，从而预测新的研究方向的影响力。我们预期这种能力将有助于研究者发现具有高影响力的研究主题。

    The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to 
    
[^17]: 针对无悔学习者的第一价格拍卖策略

    Strategizing against No-Regret Learners in First-Price Auctions

    [https://arxiv.org/abs/2402.08637](https://arxiv.org/abs/2402.08637)

    我们研究了第一价格拍卖中的无悔学习者和优化者之间的策略博弈。对于一类常用的无悔学习算法，我们发现在标准的第一价格拍卖中，优化者无法获得超过斯塔克尔贝格效用，但在贝叶斯第一价格拍卖中，他们可以获得远高于斯塔克尔贝格效用的效用。另外，我们也发现对于一类更复杂的算法，优化者的效用可以被限制在斯塔克尔贝格效用上。

    

    我们研究了重复的第一价格拍卖和两个玩家之间的通用重复贝叶斯博弈，其中一个玩家，即学习者，采用无悔学习算法，而另一个玩家，优化者，知道学习者的算法，并制定策略以最大化自己的效用。对于一类常用的称为基于均值的算法，我们表明（i）在标准（即完全信息）的第一价格拍卖中，优化者无法获得超过斯塔克尔贝格效用—文献中的标准基准，但是（ii）在贝叶斯第一价格拍卖中，存在一些情况，优化者可以获得远高于斯塔克尔贝格效用的效用。另一方面，Mansour等人（2022）表明，一类更复杂的算法，称为无多面体互换后悔算法，足以将优化者的效用限制在斯塔克尔贝格效用上，在任何重复的贝叶斯博弈（包括贝叶斯第一价格拍卖）中，他们提出了一个未解决的问题。

    We study repeated first-price auctions and general repeated Bayesian games between two players, where one player, the learner, employs a no-regret learning algorithm, and the other player, the optimizer, knowing the learner's algorithm, strategizes to maximize its own utility. For a commonly used class of no-regret learning algorithms called mean-based algorithms, we show that (i) in standard (i.e., full-information) first-price auctions, the optimizer cannot get more than the Stackelberg utility -- a standard benchmark in the literature, but (ii) in Bayesian first-price auctions, there are instances where the optimizer can achieve much higher than the Stackelberg utility.   On the other hand, Mansour et al. (2022) showed that a more sophisticated class of algorithms called no-polytope-swap-regret algorithms are sufficient to cap the optimizer's utility at the Stackelberg utility in any repeated Bayesian game (including Bayesian first-price auctions), and they pose the open question wh
    
[^18]: 在黑盒大型语言模型上进行知识编辑

    Knowledge Editing on Black-box Large Language Models

    [https://arxiv.org/abs/2402.08631](https://arxiv.org/abs/2402.08631)

    这项研究提出了在黑盒大型语言模型上进行知识编辑的方法，并引入了一种多角度评估框架和一种新的postEdit框架，以解决现有方法中的隐私和风格问题。

    

    知识编辑旨在高效、精确地修改大型语言模型的行为，以更新特定的知识，而不对其他知识产生负面影响。当前的研究主要集中在白盒语言模型编辑上，忽视了一个重要的场景：黑盒语言模型编辑，即通过接口访问语言模型，并仅可用文本输出。为了解决现有评估在黑盒语言模型编辑上不适用且缺乏全面性的局限性，我们提出了一种多角度评估框架，首次将风格保留的评估纳入其中。为了解决当前方法中的编辑数据隐私泄漏和风格过度编辑的问题，我们引入了一种新的postEdit框架，通过下游后处理解决隐私问题，并通过对原始回答进行细粒度编辑来保持文本风格一致性。在两个基准测试上的实验与分析表明，postEdit的性能超过了所有现有方法。

    Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all 
    
[^19]: 一种广义的在线凸优化方法

    A Generalized Approach to Online Convex Optimization

    [https://arxiv.org/abs/2402.08621](https://arxiv.org/abs/2402.08621)

    这是一篇关于在线凸优化的论文，作者分析了不同环境下的问题并提出了一种通用的解决方法，该方法可以转化为相应的线性优化算法，并可以在面对不同类型对手时获得可比较的遗憾界限。

    

    在本文中，我们分析了不同环境下的在线凸优化问题。我们证明了任何用于具有完全自适应对手的在线线性优化的算法都是用于在线凸优化的算法。我们还证明了任何需要全信息反馈的算法都可以转化为具有可比较的遗憾界限的半匹配反馈算法。此外，我们还证明了使用确定性半匹配反馈的全自适应对手设计的算法在面对无知对手时可以使用只有随机半匹配反馈的算法获得相似的界限。我们利用这一结果描述了将一阶算法转化为零阶算法的通用元算法，这些算法具有可比较的遗憾界限。我们的框架使我们能够分析各种设置中的在线优化问题，包括全信息反馈、半匹配反馈、随机遗憾、对抗遗憾和各种形式的非平稳遗憾。利用我们的分析结果，

    In this paper, we analyze the problem of online convex optimization in different settings. We show that any algorithm for online linear optimization with fully adaptive adversaries is an algorithm for online convex optimization. We also show that any such algorithm that requires full-information feedback may be transformed to an algorithm with semi-bandit feedback with comparable regret bound. We further show that algorithms that are designed for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback when facing oblivious adversaries. We use this to describe general meta-algorithms to convert first order algorithms to zeroth order algorithms with comparable regret bounds. Our framework allows us to analyze online optimization in various settings, such full-information feedback, bandit feedback, stochastic regret, adversarial regret and various forms of non-stationary regret. Using our analysis, we provide
    
[^20]: Adjustment Identification Distance: 一种用于因果结构学习的调整识别距离

    Adjustment Identification Distance: A gadjid for Causal Structure Learning

    [https://arxiv.org/abs/2402.08616](https://arxiv.org/abs/2402.08616)

    gadjid软件包提供了一种用于因果结构学习的调整识别距离，通过引入框架来计算因果距离，这些距离能够高效评估因果发现算法学习的图形，并且在处理大规模图形时具有较高的性能。

    

    通过因果发现算法学习的图形的评估是困难的：两个图形之间不同的边的数量不能反映出它们在建议因果效应的识别公式方面有何不同。我们引入了一个框架，用于开发图形之间的因果距离，其中包括有向无环图的结构干预距离作为一种特殊情况。我们利用这个框架开发了改进的基于调整的距离，以及对完成的部分有向无环图和因果序列的扩展。我们开发了多项式时间可达性算法来高效计算距离。在我们的gadjid软件包中（在https://github.com/CausalDisco/gadjid上开源），我们提供了我们的距离实现；它们的运行速度比结构干预距离快几个数量级，从而为以前无法扩展的图形尺寸提供了一个因果发现的成功指标。

    Evaluating graphs learned by causal discovery algorithms is difficult: The number of edges that differ between two graphs does not reflect how the graphs differ with respect to the identifying formulas they suggest for causal effects. We introduce a framework for developing causal distances between graphs which includes the structural intervention distance for directed acyclic graphs as a special case. We use this framework to develop improved adjustment-based distances as well as extensions to completed partially directed acyclic graphs and causal orders. We develop polynomial-time reachability algorithms to compute the distances efficiently. In our package gadjid (open source at https://github.com/CausalDisco/gadjid), we provide implementations of our distances; they are orders of magnitude faster than the structural intervention distance and thereby provide a success metric for causal discovery that scales to graph sizes that were previously prohibitive.
    
[^21]: 面向高度不平衡工业数据的成本敏感变压器模型的预测性能

    A Cost-Sensitive Transformer Model for Prognostics Under Highly Imbalanced Industrial Data

    [https://arxiv.org/abs/2402.08611](https://arxiv.org/abs/2402.08611)

    本文介绍了一种面向高度不平衡工业数据的成本敏感变压器模型，该模型在故障检测和预测方面表现出了显著的性能提升，并通过剔除实验分析了不同组件的贡献。

    

    数据驱动模型在工业领域的快速增长得益于传感器技术的普及，使得大量数据的收集成为可能。然而，在故障检测和预测方面利用这些模型面临着诸多挑战，包括缺失值和类别不平衡等问题。此外，工业运营中的成本敏感性进一步增加了在这个背景下应用传统模型的复杂性。本文介绍了一种新颖的成本敏感变压器模型，它是作为一个系统工作流的一部分开发的，还整合了混合重采样器和基于回归的插补器。通过对来自Scania卡车的APS故障数据集和SECOM数据集的严格测试，我们观察到与最先进方法相比性能有了显著提升。此外，我们进行了剔除实验来分析我们提出的方法中不同组件的贡献。

    The rapid influx of data-driven models into the industrial sector has been facilitated by the proliferation of sensor technology, enabling the collection of vast quantities of data. However, leveraging these models for failure detection and prognosis poses significant challenges, including issues like missing values and class imbalances. Moreover, the cost sensitivity associated with industrial operations further complicates the application of conventional models in this context. This paper introduces a novel cost-sensitive transformer model developed as part of a systematic workflow, which also integrates a hybrid resampler and a regression-based imputer. After subjecting our approach to rigorous testing using the APS failure dataset from Scania trucks and the SECOM dataset, we observed a substantial enhancement in performance compared to state-of-the-art methods. Moreover, we conduct an ablation study to analyze the contributions of different components in our proposed method. Our fi
    
[^22]: 专家组合解锁深度强化学习的参数缩放

    Mixtures of Experts Unlock Parameter Scaling for Deep RL

    [https://arxiv.org/abs/2402.08609](https://arxiv.org/abs/2402.08609)

    本文证明了将专家组合模块融入基于值的网络中，尤其是软MoE，可以实现更具参数可扩展性的深度强化学习模型，这提供了强有力的实证证据以发展强化学习的缩放定律。

    

    最近对（自我）监督学习模型的快速进展很大程度上是通过实证缩放定律预测的：模型的性能与其规模成比例。然而，在强化学习领域中，寻找类似的缩放定律仍然困难，因为增加模型的参数数量往往会损害其最终性能。在本文中，我们证明将专家组合（MoE）模块，特别是软MoE（Puigcerver等人，2023年），融入基于值的网络中，可以得到更具参数可扩展性的模型，通过各种训练方案和模型规模的显著性能提升加以证明。因此，这项工作为发展强化学习的缩放定律提供了有力的实证证据。

    The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.
    
[^23]: 可训练量子机器学习中任意多项式分离

    Arbitrary Polynomial Separations in Trainable Quantum Machine Learning

    [https://arxiv.org/abs/2402.08606](https://arxiv.org/abs/2402.08606)

    该论文通过构建一种层次结构的可高效训练的量子神经网络，实现了在经典序列建模任务中具有任意常数次数的多项式内存分离。每个量子神经网络单元都可以在常数时间内在量子设备上进行计算。

    

    最近在量子机器学习领域的理论研究表明，量子神经网络（QNNs）的表达能力和可训练性之间存在一种普遍的权衡；作为这些结果的推论，实际上在表达能力上实现指数级的超越经典机器学习模型的分离是不可行的，因为这样的QNN训练时间在模型规模上是指数级的。我们通过构建一种层次结构的可高效训练的QNNs来绕开这些负面结果，在执行经典序列建模任务时，这些QNNs可以展示出任意常数次数的多项式内存分离，且在量子设备上每个单元格都可以在常数时间内进行计算。我们证明了这种分离适用于包括循环神经网络和Transformer在内的众所周知的经典网络。我们还展示了量子上下文相关性的重要性。

    Recent theoretical results in quantum machine learning have demonstrated a general trade-off between the expressive power of quantum neural networks (QNNs) and their trainability; as a corollary of these results, practical exponential separations in expressive power over classical machine learning models are believed to be infeasible as such QNNs take a time to train that is exponential in the model size. We here circumvent these negative results by constructing a hierarchy of efficiently trainable QNNs that exhibit unconditionally provable, polynomial memory separations of arbitrary constant degree over classical neural networks in performing a classical sequence modeling task. Furthermore, each unit cell of the introduced class of QNNs is computationally efficient, implementable in constant time on a quantum device. The classical networks we prove a separation over include well-known examples such as recurrent neural networks and Transformers. We show that quantum contextuality is th
    
[^24]: 图神经网络的同态计数：关于基础的一切

    Homomorphism Counts for Graph Neural Networks: All About That Basis

    [https://arxiv.org/abs/2402.08595](https://arxiv.org/abs/2402.08595)

    本研究展示了基于图神经网络的同态计数对于增强其表达能力的重要性，并提出了一种更细致的方法来融合目标模式的同态计数。这种方法比现有方法更具表达力且没有额外的计算复杂度开销。

    

    图神经网络是用于学习图上不变函数的架构。大量研究已经探讨了图神经网络的性质，并确定了一些限制，特别是与其表达能力相关的限制。它们无法计数图中的某些模式（例如循环）是这些限制的核心，因为许多需要学习的函数依赖于计数这些模式的能力。两种突出的范例旨在通过丰富图特征的子图或同态模式计数来解决这个限制。在这项工作中，我们展示了这两种方法在某种意义上都是次优的，并主张采用一种更细致的方法，将目标模式的“基础”中的同态计数纳入考虑。与现有方法相比，这产生了更加表达力的架构，而不会带来任何额外的计算复杂度开销。我们证明了一系列理论结论。

    Graph neural networks are architectures for learning invariant functions over graphs. A large body of work has investigated the properties of graph neural networks and identified several limitations, particularly pertaining to their expressive power. Their inability to count certain patterns (e.g., cycles) in a graph lies at the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns. Two prominent paradigms aim to address this limitation by enriching the graph features with subgraph or homomorphism pattern counts. In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the "basis" of the target pattern. This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches. We prove a series of theoretical r
    
[^25]: 图特征预处理器：实时从交易图中提取基于子图的特征

    Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs

    [https://arxiv.org/abs/2402.08593](https://arxiv.org/abs/2402.08593)

    本文介绍了一种名为"图特征预处理器"的软件库，可以从实时交易图中检测典型的洗钱和欺诈模式，并生成丰富的交易特征，从而显著提高机器学习模型的预测准确率。

    

    在本文中，我们提出了一种名为"图特征预处理器"的软件库，用于实时检测金融交易图中的典型洗钱和欺诈模式。这些模式被用于生成丰富的交易特征，用于下游的机器学习训练和推断任务，如洗钱检测。我们展示了我们丰富的交易特征如何显著提高基于梯度提升的机器学习模型的预测准确率。我们的库利用多核并行性，维护一个动态的内存图，并高效地挖掘传入交易流中的子图模式，使其能够以流的方式操作。我们使用高度不平衡的合成反洗钱（AML）和真实的以太坊钓鱼数据集对我们的库进行评估。在这些数据集中，非法交易的比例非常小，使得学习过程具有挑战性。我们的解决方案结合了我们的图特征预处理器和...

    In this paper, we present "Graph Feature Preprocessor", a software library for detecting typical money laundering and fraud patterns in financial transaction graphs in real time. These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection. We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models. Our library exploits multicore parallelism, maintains a dynamic in-memory graph, and efficiently mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner. We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets. In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging. Our solution, which combines our Graph Feature Prep
    
[^26]: 卷积神经网络用于面部皮肤病变检测

    Convolutional Neural Networks Towards Facial Skin Lesions Detection

    [https://arxiv.org/abs/2402.08592](https://arxiv.org/abs/2402.08592)

    本研究提出了一种基于卷积神经网络和机器学习的模型，用于检测面部图像上的瑕疵和皮肤病变。该模型具有简单的架构和适用于图像处理的速度，避免了传统方法的复杂性。

    

    面部分析已经成为一个重要的研究领域，具有包括美容整形、美容行业、摄影和娱乐等多种应用。操纵患者的图像通常需要专业的图像处理软件。本研究通过卷积神经网络和机器学习方法提供了一个模型，以实现对面部图像上的瑕疵和皮肤病变的检测。所提出的方法具有简单的架构、快速和适用于图像处理，并避免了传统方法的复杂性。该模型包括四个主要步骤：区域选择、所选区域的扫描、病变诊断和标记已识别的病变。该研究的原始数据是从德黑兰一家专门从事皮肤护理和美容服务的知名诊所收集的。数据集包括行政信息、临床数据和面部和侧面图像。

    Facial analysis has emerged as a prominent area of research with diverse applications, including cosmetic surgery programs, the beauty industry, photography, and entertainment. Manipulating patient images often necessitates professional image processing software. This study contributes by providing a model that facilitates the detection of blemishes and skin lesions on facial images through a convolutional neural network and machine learning approach. The proposed method offers advantages such as simple architecture, speed and suitability for image processing while avoiding the complexities associated with traditional methods. The model comprises four main steps: area selection, scanning the chosen region, lesion diagnosis, and marking the identified lesion. Raw data for this research were collected from a reputable clinic in Tehran specializing in skincare and beauty services. The dataset includes administrative information, clinical data, and facial and profile images. A total of 230
    
[^27]: 树集成中更快的重复逃避攻击

    Faster Repeated Evasion Attacks in Tree Ensembles

    [https://arxiv.org/abs/2402.08586](https://arxiv.org/abs/2402.08586)

    这篇论文提出了一种更快速的构建树集成中对抗性样本的方法，通过识别一组特征的洞察力来加速构建过程。

    

    树集成是最广泛使用的模型类之一。然而，这些模型容易受到对抗性样本的攻击，也就是略微扰动的样本会导致错误预测。已经有很多研究致力于设计用于树集成的对抗性样本构建方法。但这是一个计算上具有挑战性的问题，通常需要大量次数的解决（例如对所有训练样本进行解决）。目前的方法试图从头开始寻找这样的样本，这进一步加剧了问题。相反地，我们利用正在解决的多个相似问题的事实。具体而言，我们的方法利用了树集成的对抗性样本倾向于扰动一致但相对较小的一组特征的洞察力。我们展示了我们可以快速识别这组特征，并利用这个知识加速构建对抗性样本。

    Tree ensembles are one of the most widely used model classes. However, these models are susceptible to adversarial examples, i.e., slightly perturbed examples that elicit a misprediction. There has been significant research on designing approaches to construct such examples for tree ensembles. But this is a computationally challenging problem that often must be solved a large number of times (e.g., for all examples in a training set). This is compounded by the fact that current approaches attempt to find such examples from scratch. In contrast, we exploit the fact that multiple similar problems are being solved. Specifically, our approach exploits the insight that adversarial examples for tree ensembles tend to perturb a consistent but relatively small set of features. We show that we can quickly identify this set of features and use this knowledge to speedup constructing adversarial examples.
    
[^28]: 链接预测的混合模型

    Mixture of Link Predictors

    [https://arxiv.org/abs/2402.08583](https://arxiv.org/abs/2402.08583)

    提出了一种用于链接预测的混合模型Link-MoE，通过选择合适的专家模型，利用不同类型的成对信息，能够显著提高预测性能。

    

    链接预测是图机器学习中的一项基本任务，旨在预测图中未见连接。启发式方法利用一系列不同的成对度量，如共同邻居和最短路径，常常能够与纯粹的图神经网络（GNNs）性能相媲美。因此，近期GNNs在链接预测（GNN4LP）方面的进展主要集中在整合一种或少数几种成对信息上。在这项工作中，我们揭示了同一数据集中的不同节点对需要不同的成对信息进行准确预测，而只应用相同的成对信息的模型可能会导致次优性能。因此，我们提出了一种简单的专家模型Link-MoE用于链接预测。Link-MoE利用各种GNNs作为专家，并根据不同类型的成对信息为每个节点对选择合适的专家。在各种真实数据集上的实验结果表明，Link-MoE能够显著提高预测性能。

    Link prediction, which aims to forecast unseen connections in graphs, is a fundamental task in graph machine learning. Heuristic methods, leveraging a range of different pairwise measures such as common neighbors and shortest paths, often rival the performance of vanilla Graph Neural Networks (GNNs). Therefore, recent advancements in GNNs for link prediction (GNN4LP) have primarily focused on integrating one or a few types of pairwise information. In this work, we reveal that different node pairs within the same dataset necessitate varied pairwise information for accurate prediction and models that only apply the same pairwise information uniformly could achieve suboptimal performance. As a result, we propose a simple mixture of experts model Link-MoE for link prediction. Link-MoE utilizes various GNNs as experts and strategically selects the appropriate expert for each node pair based on various types of pairwise information. Experimental results across diverse real-world datasets dem
    
[^29]: FedLPS: 多任务异构联邦学习中的本地参数共享

    FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing

    [https://arxiv.org/abs/2402.08578](https://arxiv.org/abs/2402.08578)

    FedLPS提出了一种在边缘计算环境中处理边缘设备生成的数据的多任务异构联邦学习方法，通过本地参数共享和迁移学习的原理来减少资源消耗和提高部署效率。

    

    联邦学习（FL）已成为边缘计算环境中处理边缘设备生成的大量数据的一种有希望的解决方案。通过在分布式边缘设备上共同优化全局机器学习模型，FL避免了传输原始数据的需求，并增强了用户隐私保护。尽管实际上取得了成功，但FL仍然面临重大挑战，包括有限的边缘设备资源、多任务部署和数据异构性。然而，现有研究侧重于减少每个单独任务的FL训练成本，忽略了在异构FL场景中多个任务之间的资源消耗。在本文中，我们提出了一种称为具有本地参数共享的异构联邦学习（FedLPS）的方法来填补这一空白。FedLPS利用迁移学习的原理，在单个设备上实现多任务部署，通过将本地模型划分为可共享的编码器和任务特定的编码器。

    Federated Learning (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices. By collaboratively optimizing the global machine learning models on distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy. Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity. However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios. In this paper, we propose Heterogeneous Federated Learning with Local Parameter Sharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer learning to facilitate the deployment of multiple tasks on a single device by dividing the local model into a shareable encoder and task-specific encoders. To f
    
[^30]: 对多模态大型语言模型的测试时反向门控攻击

    Test-Time Backdoor Attacks on Multimodal Large Language Models

    [https://arxiv.org/abs/2402.08577](https://arxiv.org/abs/2402.08577)

    本文提出了一种针对多模态大型语言模型的测试时反向门控攻击（AnyDoor），通过使用对抗性测试图像将反向门控注入到文本模态中，而无需访问或修改训练数据。AnyDoor具有分离设置和激活有害效果的时间的能力，并且在实验中证明了其有效性。

    

    反向门控攻击通常通过污染训练数据来执行，从而在测试阶段触发预定的有害效果。在本文中，我们提出了AnyDoor，一种针对多模态大型语言模型（MLLMs）的测试时反向门控攻击，它使用对抗性测试图像将反向门控注入到文本模态中（共享相同的通用扰动），而无需访问或修改训练数据。AnyDoor采用类似于通用对抗攻击的技术，但其通过能够分离有害效果的设置和激活的时间来区别于其他攻击。在我们的实验中，我们验证了AnyDoor对流行的MLLMs（如LLaVA-1.5、MiniGPT-4、InstructBLIP和BLIP-2）的有效性，并提供了全面的消融研究。值得注意的是，由于反向门控由通用扰动注入，AnyDoor可以动态改变其反向门触发提示/有害效果，从而暴露出...

    Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects. In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing 
    
[^31]: 侧信息中的Stackelberg博弈中的后悔最小化

    Regret Minimization in Stackelberg Games with Side Information

    [https://arxiv.org/abs/2402.08576](https://arxiv.org/abs/2402.08576)

    这篇论文研究了侧信息中的Stackelberg博弈，提出了一种方法来解决现实中玩家之间信息交流不充分的情况，并且证明了在这种情况下后悔最小化是有效的。

    

    在最基本的情况下，Stackelberg博弈是一个双人博弈，其中领导者承诺一种（混合）策略，追随者做出最佳反应。在过去的十年中，Stackelberg博弈算法是算法博弈论的最大成功之一，因为Stackelberg博弈的算法已经在许多现实世界的领域中被应用，包括机场安全、反盗猎和网络犯罪预防。然而，这些算法通常未能考虑到每个玩家可用的额外信息（例如交通模式，天气条件，网络拥塞），这是现实的显著特征，可能会显著影响到两个玩家的最优策略。我们将这样的情况形式化为带有侧信息的Stackelberg博弈，其中两个玩家在进行游戏之前都观察到一个外部环境。然后，领导者承诺一种（可能依赖于上下文的）策略，追随者对领导者的策略和上下文都做出最佳反应。

    In its most basic form, a Stackelberg game is a two-player game in which a leader commits to a (mixed) strategy, and a follower best-responds. Stackelberg games are perhaps one of the biggest success stories of algorithmic game theory over the last decade, as algorithms for playing in Stackelberg games have been deployed in many real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader then commits to a (possibly context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on t
    
[^32]: 两种单相对比海比安学习的故事

    Two Tales of Single-Phase Contrastive Hebbian Learning

    [https://arxiv.org/abs/2402.08573](https://arxiv.org/abs/2402.08573)

    两种单相对比海比安学习的故事探索了学习算法的生物合理性，并提出了一种全局学习算法，能够消除与反向传播之间的性能差距，并解决了同步和无限小扰动带来的问题。

    

    对于“生物学上合理”的学习算法的探索已经收敛于将梯度表示为活动差异的想法。然而，大多数方法需要较高程度的同步（学习期间的不同阶段）并引入大量的计算开销，这对于它们的生物学合理性以及其在神经形态计算中的潜在效用产生了疑问。此外，它们通常依赖于对输出单元施加无限小扰动（nudges），这在嘈杂环境中是不切实际的。最近研究发现，通过将人工神经元建模为两个相反扰动的组件，名为“双向传播”的全局学习算法能够弥合到反向传播的性能差距，而不需要分别的学习阶段或无限小扰动。然而，该算法的数值稳定性依赖于对称扰动，这可能在生物学上受到限制。

    The search for "biologically plausible" learning algorithms has converged on the idea of representing gradients as activity differences. However, most approaches require a high degree of synchronization (distinct phases during learning) and introduce substantial computational overhead, which raises doubts regarding their biological plausibility as well as their potential utility for neuromorphic computing. Furthermore, they commonly rely on applying infinitesimal perturbations (nudges) to output units, which is impractical in noisy environments. Recently it has been shown that by modelling artificial neurons as dyads with two oppositely nudged compartments, it is possible for a fully local learning algorithm named ``dual propagation'' to bridge the performance gap to backpropagation, without requiring separate learning phases or infinitesimal nudging. However, the algorithm has the drawback that its numerical stability relies on symmetric nudging, which may be restrictive in biological
    
[^33]: 在机器人学中的在线基础模型选择

    Online Foundation Model Selection in Robotics

    [https://arxiv.org/abs/2402.08570](https://arxiv.org/abs/2402.08570)

    这篇论文介绍了在机器人学中的在线基础模型选择问题，针对采集闭源模型大量训练数据的高成本，提出了一种以用户为中心的在线模型选择解决方案，该方案结合了开源编码器和在线学习算法，通过提取上下文特征来实现模型选择。

    

    在在计算机视觉和自然语言处理中表现出色后，基础模型最近扩展到了机器人学领域。这些模型可以通过两种方式获得：开源或付费的闭源选项。用户同时有两种选择时，他们会面临一个问题，即在有效但昂贵的闭源模型和免费但功能较弱的开源模型之间做出决策。我们称之为模型选择问题。由于从闭源模型中收集大量训练数据的成本较高，现有的监督学习方法是不实际的。因此，我们将重点放在在线学习设置上，其中算法在收集数据的同时学习，消除了需要大量预先收集的数据集的需求。因此，我们提出了一个以用户为中心的在线模型选择问题，并提出了一种新颖的解决方案，该解决方案将开源编码器与处理该上下文的在线学习算法相结合。编码器将大量的数据分布提炼成低维特征，即上下文。

    Foundation models have recently expanded into robotics after excelling in computer vision and natural language processing. The models are accessible in two ways: open-source or paid, closed-source options. Users with access to both face a problem when deciding between effective yet costly closed-source models and free but less powerful open-source alternatives. We call it the model selection problem. Existing supervised-learning methods are impractical due to the high cost of collecting extensive training data from closed-source models. Hence, we focus on the online learning setting where algorithms learn while collecting data, eliminating the need for large pre-collected datasets. We thus formulate a user-centric online model selection problem and propose a novel solution that combines an open-source encoder to output context and an online learning algorithm that processes this context. The encoder distills vast data distributions into low-dimensional features, i.e., the context, with
    
[^34]: Agent Smith:一张图像可以迅速越狱一百万个多模态LLM代理

    Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast

    [https://arxiv.org/abs/2402.08567](https://arxiv.org/abs/2402.08567)

    Agent Smith提出了一种安全问题，即传染性越狱，该问题在多代理环境中可以通过简单的越狱一个代理来迅速感染所有代理并导致有害行为。

    

    多模态大型语言模型（MLLM）代理可以接收指令，捕捉图像，从内存中检索历史记录，并决定使用哪些工具。然而，红队评估发现恶意图像/提示可以越狱MLLM并导致不对齐的行为。在这项工作中，我们报告了多代理环境中更严重的安全问题，称为传染性越狱。它涉及到对单个代理进行简单的越狱，无需来自对手的进一步干预，（几乎）所有代理将以指数级别被感染并展示有害行为。为了验证传染性越狱的可行性，我们模拟了包含高达一百万个LLaVA-1.5代理的多代理环境，并将随机匹配对聊天作为多代理交互的概念验证实例。我们的结果表明，将（传染性）恶意图像输入到任意选择的代理的内存中就足以实现传染性越狱。

    A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious 
    
[^35]: 通过去噪扩散恢复模型解决拉普拉斯算子的正向和反向问题

    Denoising Diffusion Restoration Tackles Forward and Inverse Problems for the Laplace Operator

    [https://arxiv.org/abs/2402.08563](https://arxiv.org/abs/2402.08563)

    本论文提出了一种新的方法，通过使用去噪扩散恢复模型（DDRM）解决了拉普拉斯算子的反向和正向问题，对于泊松方程的解和参数恢复有着显著的改善。

    

    扩散模型已成为一类有前景的生成模型，将噪声输入映射为逼真的图像。最近，它们被应用于生成偏微分方程（PDE）的解。然而，它们在拉普拉斯算子的反向问题上仍然存在困难，例如泊松方程，因为幅度较大的特征值会放大测量噪声。本文提出了一种新的方法，通过使用去噪扩散恢复模型（DDRM）来解决PDE的反向和正向问题。DDRM被用于线性反问题，通过利用线性算子的奇异值分解（SVD）来恢复原始干净信号。同样地，我们提出了一种方法来通过利用拉普拉斯算子的特征值和特征函数来恢复泊松方程的解和参数。我们的结果表明，使用去噪扩散恢复显著改善了估计结果。

    Diffusion models have emerged as a promising class of generative models that map noisy inputs to realistic images. More recently, they have been employed to generate solutions to partial differential equations (PDEs). However, they still struggle with inverse problems in the Laplacian operator, for instance, the Poisson equation, because the eigenvalues that are large in magnitude amplify the measurement noise. This paper presents a novel approach for the inverse and forward solution of PDEs through the use of denoising diffusion restoration models (DDRM). DDRMs were used in linear inverse problems to restore original clean signals by exploiting the singular value decomposition (SVD) of the linear operator. Equivalently, we present an approach to restore the solution and the parameters in the Poisson equation by exploiting the eigenvalues and the eigenfunctions of the Laplacian operator. Our results show that using denoising diffusion restoration significantly improves the estimation o
    
[^36]: 面对扩散模型的奖励过度优化问题：归纳和优先偏差的视角

    Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases

    [https://arxiv.org/abs/2402.08552](https://arxiv.org/abs/2402.08552)

    本论文探讨了扩散模型对齐中的奖励过度优化问题，并从归纳和优先偏差的角度提出了解决方案。作者通过发现当前方法与扩散模型的时间归纳偏差分歧，以及评论模型中的沉睡神经元和活跃神经元对抗过度优化，提出了一种新的时间扩散策略优化方法。

    

    将扩散模型与人类偏好融合是将其应用于实际生成工作流程中的关键。虽然通过优化下游奖励模型已经成为一种有前景的对齐策略，但同时也存在学习奖励模型时过度优化的风险，这可能会损害地面真实性能。在本研究中，我们从归纳和优先偏差的角度来对付扩散模型对齐中的奖励过度优化问题。首先，我们发现当前方法与扩散模型的多步去噪过程中固有的时间归纳偏差存在分歧，这可能是过度优化的一个潜在来源。然后，我们令人惊讶地发现，我们评论模型中的沉睡神经元充当一种对抗过度优化的正则化手段，而活跃神经元则反映了这个设置中的优先偏差。受这些观察的启发，我们提出了带有评论模型活跃神经元的时间扩散策略优化。

    Bridging the gap between diffusion models and human preferences is crucial for their integration into practical generative workflows. While optimizing downstream reward models has emerged as a promising alignment strategy, concerns arise regarding the risk of excessive optimization with learned reward models, which potentially compromises ground-truth performance. In this work, we confront the reward overoptimization problem in diffusion model alignment through the lenses of both inductive and primacy biases. We first identify the divergence of current methods from the temporal inductive bias inherent in the multi-step denoising process of diffusion models as a potential source of overoptimization. Then, we surprisingly discover that dormant neurons in our critic model act as a regularization against overoptimization, while active neurons reflect primacy bias in this setting. Motivated by these observations, we propose Temporal Diffusion Policy Optimization with critic active neuron Re
    
[^37]: 工程形状优化中的生成式与非生成式模型的比较

    Generative VS non-Generative Models in Engineering Shape Optimization

    [https://arxiv.org/abs/2402.08540](https://arxiv.org/abs/2402.08540)

    本研究对生成式和非生成式模型在工程形状优化中的效果和效率进行了比较。研究结果表明，非生成式模型通过适当的形状编码和物理增强设计空间，可以以经济高效的方式生成高性能的有效设计，并且扩展了设计空间的覆盖范围。

    

    本研究对生成式模型和非生成式模型在构建设计空间、进行新颖高效设计探索和形状优化方面的效果和效率进行了系统比较。我们将这些模型应用于翼型/水翼设计案例，并在生成的设计空间上进行比较。我们将传统的生成对抗网络(GAN)和最先进的生成模型——性能增强多样性生成对抗网络(PaDGAN)与基于Karhunen-Loève展开和物理模型化形状签名向量(SSV-KLE)耦合的线性非生成式模型进行比较。研究结果表明，在适当的形状编码和物理增强设计空间的情况下，非生成式模型有潜力以经济高效的方式生成高性能的有效设计，并且可以扩展设计空间的覆盖范围。本研究将这两种方法应用于两个大型翼型数据集。

    In this work, we perform a systematic comparison of the effectiveness and efficiency of generative and non-generative models in constructing design spaces for novel and efficient design exploration and shape optimization. We apply these models in the case of airfoil/hydrofoil design and conduct the comparison on the resulting design spaces. A conventional Generative Adversarial Network (GAN) and a state-of-the-art generative model, the Performance-Augmented Diverse Generative Adversarial Network (PaDGAN), are juxtaposed with a linear non-generative model based on the coupling of the Karhunen-Lo\`eve Expansion and a physics-informed Shape Signature Vector (SSV-KLE). The comparison demonstrates that, with an appropriate shape encoding and a physics-augmented design space, non-generative models have the potential to cost-effectively generate high-performing valid designs with enhanced coverage of the design space. In this work, both approaches are applied to two large foil profile dataset
    
[^38]: 基于机器学习的阿尔茨海默病智能诊断

    Intelligent Diagnosis of Alzheimer's Disease Based on Machine Learning

    [https://arxiv.org/abs/2402.08539](https://arxiv.org/abs/2402.08539)

    本研究使用机器学习探索阿尔茨海默病的早期检测和疾病进展，通过创新的数据预处理策略和机器学习模型，成功克服了缺失数据的挑战，并取得了91%的准确率，为阿尔茨海默病的早期检测提供了有价值的见解。

    

    本研究基于阿尔茨海默病神经影像学倡议（ADNI）数据库，旨在探索阿尔茨海默病（AD）的早期检测和疾病进展。我们采用创新的数据预处理策略，包括使用随机森林算法填补缺失数据，处理异常值和无效数据，从而充分挖掘和利用这些有限的数据资源。通过Spearman相关系数分析，我们确定了一些与AD诊断强相关的特征。我们使用这些特征构建和测试了三个机器学习模型：随机森林、XGBoost和支持向量机（SVM）。其中，XGBoost模型在诊断性能方面表现最佳，准确率达到91%。总体而言，本研究成功克服了缺失数据的挑战，并为阿尔茨海默病的早期检测提供了有价值的见解，展示了其独特的研究价值和实际意义。

    This study is based on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and aims to explore early detection and disease progression in Alzheimer's disease (AD). We employ innovative data preprocessing strategies, including the use of the random forest algorithm to fill missing data and the handling of outliers and invalid data, thereby fully mining and utilizing these limited data resources. Through Spearman correlation coefficient analysis, we identify some features strongly correlated with AD diagnosis. We build and test three machine learning models using these features: random forest, XGBoost, and support vector machine (SVM). Among them, the XGBoost model performs the best in terms of diagnostic performance, achieving an accuracy of 91%. Overall, this study successfully overcomes the challenge of missing data and provides valuable insights into early detection of Alzheimer's disease, demonstrating its unique research value and practical significance.
    
[^39]: 分布式后续表示的分布式类比

    A Distributional Analogue to the Successor Representation

    [https://arxiv.org/abs/2402.08530](https://arxiv.org/abs/2402.08530)

    本文提出了一种新的分布式强化学习方法，它通过分离转换结构和奖励，引入了分布式后继度量来描述行为的分布式后果。在实验中展示了该方法的实用性，特别是在零样本风险敏感策略评估方面。

    

    本文提出了一种新的分布式强化学习方法，它将转换结构和奖励在学习过程中进行了明确的分离。与后续表示（SR）描述按照给定策略行为的期望后果类似，我们的分布式后继度量（SM）描述了这种行为的分布式结果。我们将分布式SM构建为一个分布的分布，并提供了与分布式和基于模型的强化学习相关的理论。此外，我们提出了一种从数据中学习分布式SM的算法，通过最小化两个层次的最大均值差异来实现。我们方法的关键是一些独立有价值的学习状态生成模型的算法技术。作为分布式SM有用性的例证，我们展示了它使得零样本风险敏感策略评估成为可能，这在以前是不可能的。

    This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possi
    
[^40]: 近似分段E(3)等变点云网络

    Approximately Piecewise E(3) Equivariant Point Networks

    [https://arxiv.org/abs/2402.08529](https://arxiv.org/abs/2402.08529)

    将对称性概念应用于点云神经网络中，可以提高其泛化能力。本研究提出了一种新的框架APEN，用于构建近似分段的E(3)等变点云网络，以处理具有局部对称性的多部分输入。

    

    将对称性的概念集成到点云神经网络中，可以明确地提高其泛化能力。尤其引人关注的是E(3)等变点云网络，其中应用于输入的欧几里德变换在输出中保持不变。最近的研究致力于扩展E(3)等变网络，以适应由多个部分组成的输入，其中每个部分都表现出局部的E(3)对称性。然而，在实际环境中，将输入划分为单独变换的区域是未知的。划分预测的错误将不可避免地映射为不符合真实输入对称性的错误。过去的研究提出了不同的方法来预测划分，这些方法在维持与实际划分等变性的能力上可能存在无法控制的错误。因此，我们引入了APEN: 一种构建近似分段E(3)等变点云网络的通用框架。我们的主要见解是，函数...

    Integrating a notion of symmetry into point cloud neural networks is a provably effective way to improve their generalization capability. Of particular interest are $E(3)$ equivariant point cloud networks where Euclidean transformations applied to the inputs are preserved in the outputs. Recent efforts aim to extend networks that are $E(3)$ equivariant, to accommodate inputs made of multiple parts, each of which exhibits local $E(3)$ symmetry. In practical settings, however, the partitioning into individually transforming regions is unknown a priori. Errors in the partition prediction would unavoidably map to errors in respecting the true input symmetry. Past works have proposed different ways to predict the partition, which may exhibit uncontrolled errors in their ability to maintain equivariance to the actual partition. To this end, we introduce APEN: a general framework for constructing approximate piecewise-$E(3)$ equivariant point networks. Our primary insight is that functions th
    
[^41]: Concept-1K：一种用于实例增量学习的新型基准

    Concept-1K: A Novel Benchmark for Instance Incremental Learning

    [https://arxiv.org/abs/2402.08526](https://arxiv.org/abs/2402.08526)

    我们提出了一种名为Concept-1K的具有挑战性的实例增量学习（IIL）场景和数据集，揭示了十亿参数的PLM仍然遭受灾难性遗忘，影响因素包括模型规模、预训练和缓冲区大小。现有的IL方法和LoRA技术无法满足性能需求。我们的研究为探索和缓解PLM中的遗忘问题提供了新的场景。

    

    增量学习（IL）对于实现神经网络中的人类级智能至关重要。然而，现有的IL场景和数据集无法评估PLM中的遗忘，使人误以为PLM不会遭受灾难性遗忘。为此，我们提出了一种具有挑战性的IL场景，称为实例增量学习（IIL），以及一个支持数量级更大的IL步骤的新数据集Concept-1K。基于对Concept-1K的实验，我们揭示了十亿参数的PLM仍然遭受着灾难性遗忘，并且遗忘受模型规模、预训练和缓冲区大小的影响。此外，现有的IL方法和一种流行的微调技术LoRA都未能达到令人满意的性能。我们的研究为未来研究提供了一个新的场景，探索PLM的灾难性遗忘，并鼓励设计更强大的技术以减轻PLM中的遗忘问题。

    Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs. The data, code and scripts ar
    
[^42]: 多代理协作下的公平性审计

    Fairness Auditing with Multi-Agent Collaboration

    [https://arxiv.org/abs/2402.08522](https://arxiv.org/abs/2402.08522)

    本文研究了多代理协作下的公平性审计，证明了有时协调对审计准确性可能有害，而非协调的合作通常会产生良好的结果。

    

    现有的公平性审计工作假设代理人独立操作。本文考虑多个代理人对同一平台进行不同任务的审计情况。代理人有两个杠杆：协作策略（是否进行协调）和抽样方法。我们在代理人独立操作或协作时对它们的相互作用进行了理论研究。我们证明了有时协调对审计准确性可能有害，而非协调的合作通常会产生良好的结果。对实际数据集的实验验证了这一观察结果，非协调的合作的审计准确性与协作的最优抽样相匹配。

    Existing work in fairness audits assumes that agents operate independently. In this paper, we consider the case of multiple agents auditing the same platform for different tasks. Agents have two levers: their collaboration strategy, with or without coordination beforehand, and their sampling method. We theoretically study their interplay when agents operate independently or collaborate. We prove that, surprisingly, coordination can sometimes be detrimental to audit accuracy, whereas uncoordinated collaboration generally yields good results. Experimentation on real-world datasets confirms this observation, as the audit accuracy of uncoordinated collaboration matches that of collaborative optimal sampling.
    
[^43]: 广义和平均容量之间的PAC-Bayes联结

    A PAC-Bayesian Link Between Generalisation and Flat Minima

    [https://arxiv.org/abs/2402.08508](https://arxiv.org/abs/2402.08508)

    本研究结合了PAC-Bayes工具箱和Poincaré与Log-Sobolev不等式，提供了新的梯度项泛化界限，并突出了平坦最小值对泛化性能的积极影响。

    

    现代机器学习通常使用超参数设置（训练参数数量大于数据集大小）中的预测器，它们的训练不仅产生良好的训练数据性能，而且具有良好的泛化能力。这一现象挑战了许多理论结果，并且仍然是一个未解决的问题。为了更好地理解这一现象，我们提供了涉及梯度项的新型泛化界限。为此，我们将PAC-Bayes工具箱与Poincaré和Log-Sobolev不等式相结合，避免了对预测器空间维数的显式依赖。我们的结果突出了“平坦最小值”（几乎能够最小化学习问题的邻近最小值）对泛化性能的积极影响，直接涉及到优化阶段的好处。

    Modern machine learning usually involves predictors in the overparametrised setting (number of trained parameters greater than dataset size), and their training yield not only good performances on training data, but also good generalisation capacity. This phenomenon challenges many theoretical results, and remains an open problem. To reach a better understanding, we provide novel generalisation bounds involving gradient terms. To do so, we combine the PAC-Bayes toolbox with Poincar\'e and Log-Sobolev inequalities, avoiding an explicit dependency on dimension of the predictor space. Our results highlight the positive influence of \emph{flat minima} (being minima with a neighbourhood nearly minimising the learning problem as well) on generalisation performances, involving directly the benefits of the optimisation phase.
    
[^44]: 在开放海域上的安全强化学习中可证明的交通规则遵守

    Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea

    [https://arxiv.org/abs/2402.08502](https://arxiv.org/abs/2402.08502)

    这项研究提出了一种可证明安全的强化学习方法，用于在开放海域上的船只中遵守交通规则，并引入了一种有效的验证方法来确定行为是否符合COLREGS规则。

    

    自主车辆必须遵守交通规则。这些规则通常使用时态逻辑进行形式化，导致使用基于优化的运动规划器解决这些约束变得困难。强化学习（RL）是一种有前途的方法，可以找到符合时态逻辑规范的运动规划。然而，纯强化学习算法基于随机探索，这在本质上是不安全的。为了解决这个问题，我们提出了一种可证明安全的RL方法，始终遵守交通规则。作为一个特定的应用领域，我们考虑在开放海域上的船只，这些船只必须遵守《海上避碰规则公约》（COLREGS）的规定。我们介绍了一种高效的验证方法，用于确定行为与使用时态逻辑形式化的COLREGS的符合性。我们的行为验证被集成到RL过程中，以便智能体只选择经过验证的行为。与只集成交通规则的智能体相比，

    Autonomous vehicles have to obey traffic rules. These rules are often formalized using temporal logic, resulting in constraints that are hard to solve using optimization-based motion planners. Reinforcement Learning (RL) is a promising method to find motion plans adhering to temporal logic specifications. However, vanilla RL algorithms are based on random exploration, which is inherently unsafe. To address this issue, we propose a provably safe RL approach that always complies with traffic rules. As a specific application area, we consider vessels on the open sea, which must adhere to the Convention on the International Regulations for Preventing Collisions at Sea (COLREGS). We introduce an efficient verification approach that determines the compliance of actions with respect to the COLREGS formalized using temporal logic. Our action verification is integrated into the RL process so that the agent only selects verified actions. In contrast to agents that only integrate the traffic rule
    
[^45]: 数据到文本自然语言生成研究的系统性回顾

    A Systematic Review of Data-to-Text NLG

    [https://arxiv.org/abs/2402.08496](https://arxiv.org/abs/2402.08496)

    这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。

    

    这篇系统性回顾旨在全面分析数据到文本生成研究的现状，重点是确定研究空白，提供未来方向，并解决回顾中发现的挑战。我们对文献进行了全面的检查，包括方法、数据集、评估指标、应用、多语言性和幻觉缓解措施。我们的回顾为这个快速发展的领域的未来研究提供了路线图。

    This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
    
[^46]: 通过稀疏分组k最大规则化实现稀疏性

    Sparsity via Sparse Group $k$-max Regularization

    [https://arxiv.org/abs/2402.08493](https://arxiv.org/abs/2402.08493)

    本文提出了一种新颖简洁的稀疏分组k最大规则化方法，可以同时增强分组内和分组间的稀疏性，更接近l0范数。

    

    对于具有稀疏约束的线性逆问题，l0正则化问题是NP困难的，现有方法要么利用贪婪算法找到近似最优解，要么用凸映射来逼近l0正则化。本文提出了一种新颖简洁的正则化方法，即稀疏分组k最大规则化，其不仅可以同时增强分组内和分组间的稀疏性，而且对每个分组中的变量的大小没有额外的限制，这对于不同尺度的变量尤为重要，以更接近l0范数。我们还提出了一种带有局部最优性条件和复杂性分析的迭代软阈值算法。通过在合成和真实数据集上的数值实验证明了所提方法的有效性和灵活性。

    For the linear inverse problem with sparsity constraints, the $l_0$ regularized problem is NP-hard, and existing approaches either utilize greedy algorithms to find almost-optimal solutions or to approximate the $l_0$ regularization with its convex counterparts. In this paper, we propose a novel and concise regularization, namely the sparse group $k$-max regularization, which can not only simultaneously enhance the group-wise and in-group sparsity, but also casts no additional restraints on the magnitude of variables in each group, which is especially important for variables at different scales, so that it approximate the $l_0$ norm more closely. We also establish an iterative soft thresholding algorithm with local optimality conditions and complexity analysis provided. Through numerical experiments on both synthetic and real-world datasets, we verify the effectiveness and flexibility of the proposed method.
    
[^47]: 深度强化学习在细胞重编程的布尔模型吸引子景观中的控制遍历中的应用研究

    Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming

    [https://arxiv.org/abs/2402.08491](https://arxiv.org/abs/2402.08491)

    本研究开发了一个基于深度强化学习的计算框架，用于细胞重编程中的重编程策略识别。在控制问题中，引入了伪吸引子的概念和识别方法，并设计了一个用于解决该问题的计算框架。

    

    细胞重编程可用于预防和治疗不同疾病。然而，通过传统湿实验发现重编程策略的效率受到时间和成本的限制。在本研究中，我们基于深度强化学习开发了一个新颖的计算框架，以便帮助识别重编程策略。为此，我们在细胞重编程框架的BNs和PBNs以及异步更新模式下制定了一个控制问题。此外，我们引入了伪吸引子的概念和训练过程中伪吸引子状态的识别方法。最后，我们设计了一个用于解决控制问题的计算框架，并在多个不同模型上进行了测试。

    Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies. For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training. Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models.
    
[^48]: 揭示泛化图传播中的曲线流

    Revealing Decurve Flows for Generalized Graph Propagation

    [https://arxiv.org/abs/2402.08480](https://arxiv.org/abs/2402.08480)

    本研究提出了广义传播的概念，并设计了广义传播神经网络(GPNNs)和连续统一里奇曲率(CURC)方法，用于在定向和加权图上进行图传播分析和图结构的特性研究。

    

    本研究通过定义具有定向和加权图的{\em \textbf{广义传播}}来解决传统信息传递分析的局限性，该分析方法对图学习至关重要。其重要性体现在两个方面。首先，我们提出了{\em 广义传播神经网络}(\textbf{GPNNs})，这是一个统一了大多数基于传播的图神经网络的框架。通过生成具有邻接函数和连接函数的定向-加权传播图，GPNNs可以增强对各种图模型中注意机制的洞察力。我们通过实证实验探讨了设计空间中的权衡，并通过理论分析强调了邻接函数对模型表达能力的关键作用。其次，我们提出了{\em 连续统一里奇曲率}(\textbf{CURC})，它是著名的{\em 奥利维尔-里奇曲率}在定向和加权图上的扩展。理论上，我们证明了CURC能够揭示图结构中的关键信息，包括图的连通性和流动属性。

    This study addresses the limitations of the traditional analysis of message-passing, central to graph learning, by defining {\em \textbf{generalized propagation}} with directed and weighted graphs. The significance manifest in two ways. \textbf{Firstly}, we propose {\em Generalized Propagation Neural Networks} (\textbf{GPNNs}), a framework that unifies most propagation-based graph neural networks. By generating directed-weighted propagation graphs with adjacency function and connectivity function, GPNNs offer enhanced insights into attention mechanisms across various graph models. We delve into the trade-offs within the design space with empirical experiments and emphasize the crucial role of the adjacency function for model expressivity via theoretical analysis. \textbf{Secondly}, we propose the {\em Continuous Unified Ricci Curvature} (\textbf{CURC}), an extension of celebrated {\em Ollivier-Ricci Curvature} for directed and weighted graphs. Theoretically, we demonstrate that CURC po
    
[^49]: 零样本和系统性评估视觉语言Transformer模型之间的有趣差异

    Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models

    [https://arxiv.org/abs/2402.08473](https://arxiv.org/abs/2402.08473)

    本文通过新的梯度下降优化方法，探索了常用的视觉语言模型的嵌入空间。结果显示，尽管该模型在零样本分类上有超过99%的表现，但在系统性评估中却完全失败。通过线性近似，提供了一个解释这些差异的框架。

    

    过去几年中，基于Transformer的模型在自然语言处理和其他领域中占据主导地位，因为它们在基准数据集上具有出色的(零样本)性能。然而，由于其复杂性和规模，这些模型目前尚未被很好地理解。虽然探测法已广泛用于理解特定属性，但表示空间的结构尚未得到系统性的刻画；因此，目前还不清楚这样的模型如何推广和过度推广到超出数据集范围的新输入。在本文中，基于一种新的梯度下降优化方法，我们能够探索常用的视觉语言模型的嵌入空间。我们使用Imagenette数据集表明，尽管该模型实现了超过99%的零样本分类性能，但它在系统性评估中完全失败。我们使用线性近似提供了一个框架来解释这些显著的差异。我们还使用另一个模型获得了类似的结果，以支持这个解释。

    Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets. However, these models are poorly understood due to their complexity and size. While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets. In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model. Using the Imagenette dataset, we show that while the model achieves over 99\% zero-shot classification performance, it fails systematic evaluations completely. Using a linear approximation, we provide a framework to explain the striking differences. We have also obtained similar results using a different model to support that o
    
[^50]: 面向规模化光伏衰减分析的并行友好时空图学习

    Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale

    [https://arxiv.org/abs/2402.08470](https://arxiv.org/abs/2402.08470)

    提出了一种并行友好的时空图学习方法，用于规模化光伏衰减分析。该方法集成了时空一致性和图注意力，可以准确估计大规模光伏逆变器的长期性能损失率，并提供在线模型和数据集以帮助改进光伏系统的性能评估和可靠性分析。

    

    我们提出了一种新颖的时空图神经网络驱动的趋势分析方法(ST-GTrend)，用于进行光伏电力网络的集群级性能衰减分析。光伏电站已成为全球可持续能源生产格局的重要组成部分。准确估计光伏系统的性能对于其作为发电技术和金融资产的可行性至关重要。评估光伏系统的水平化能源成本(LCOE)中最具挑战性的问题之一是了解和估计大规模光伏逆变器的长期性能损失率(PLR)。ST-GTrend集成了时空一致性和图注意力，将PLR作为长期的“老化”趋势与光伏输入数据中的多个波动项分离开来。为了应对时序中多样的衰减模式，ST-GTrend采用并行图自编码器数组同时提取老化和波动项。ST-GTrend发布了一个在线光伏衰减分析模型和数据集，以帮助学术界和工业界改进光伏系统的性能评估和可靠性分析。

    We propose a novel Spatio-Temporal Graph Neural Network empowered trend analysis approach (ST-GTrend) to perform fleet-level performance degradation analysis for Photovoltaic (PV) power networks. PV power stations have become an integral component to the global sustainable energy production landscape. Accurately estimating the performance of PV systems is critical to their feasibility as a power generation technology and as a financial asset. One of the most challenging problems in assessing the Levelized Cost of Energy (LCOE) of a PV system is to understand and estimate the long-term Performance Loss Rate (PLR) for large fleets of PV inverters. ST-GTrend integrates spatio-temporal coherence and graph attention to separate PLR as a long-term "aging" trend from multiple fluctuation terms in the PV input data. To cope with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled graph autoencoder array to extract aging and fluctuation terms simultaneously. ST-GTrend impo
    
[^51]: ROSpace: ROS2基础的物理网络入侵检测数据集

    ROSpace: Intrusion Detection Dataset for a ROS2-Based Cyber-Physical System

    [https://arxiv.org/abs/2402.08468](https://arxiv.org/abs/2402.08468)

    提出了一个基于ROS2的物理网络入侵检测数据集，通过渗透测试在三个层次监测系统特征，重现了正常和攻击行为的交替和重叠过程，可以测量检测攻击者所需的时间和恶意活动的数量。

    

    大部分基于机器学习的入侵检测系统的数据集都只针对网络系统，并且通常只从一个层次上收集数据。此外，攻击往往是在专用的攻击会话中生成的，而不是重现正常和攻击行为的实际交替和重叠过程。我们提出了一个用于入侵检测的数据集，通过对基于ROS2的嵌入式物理网络系统进行渗透测试来获取。特征从三个层次监测：Linux操作系统、网络和ROS2服务。该数据集以时间序列的形式进行组织，并描述了系统的预期行为以及对ROS2特定攻击的响应：它反复交替无攻击操作的时期和执行特定攻击的时期。值得注意的是，这可以测量检测攻击者所需的时间和恶意活动的数量。

    Most of the intrusion detection datasets to research machine learning-based intrusion detection systems (IDSs) are devoted to cyber-only systems, and they typically collect data from one architectural layer. Additionally, often the attacks are generated in dedicated attack sessions, without reproducing the realistic alternation and overlap of normal and attack actions. We present a dataset for intrusion detection by performing penetration testing on an embedded cyber-physical system built over Robot Operating System 2 (ROS2). Features are monitored from three architectural layers: the Linux operating system, the network, and the ROS2 services. The dataset is structured as a time series and describes the expected behavior of the system and its response to ROS2-specific attacks: it repeatedly alternates periods of attack-free operation with periods when a specific attack is being performed. Noteworthy, this allows measuring the time to detect an attacker and the number of malicious activ
    
[^52]: Subgraphormer:通过图的乘积将子图GNN和图变换器统一起来

    Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products

    [https://arxiv.org/abs/2402.08450](https://arxiv.org/abs/2402.08450)

    这篇论文提出了一种名为Subgraphormer的架构，通过将子图GNNs和图变换器结合起来，综合了子图GNNs的表达能力和消息传递机制以及图变换器的注意力和位置编码。基于子图GNNs与图的乘积之间的新连接，该方法设计了乘积图上的注意力机制和子图GNNs位置编码方案。

    

    在图神经网络(GNNs)领域中，最近出现了两个令人兴奋的研究方向：子图GNN和图变换器。本文提出了一种将两种方法结合起来的架构，称为Subgraphormer，它将子图GNNs的增强表达能力、信息传递机制和聚合方案与图变换器中最重要的注意力和位置编码相结合。我们的方法基于我们揭示的子图GNNs与乘积图之间的新连接，这表明子图GNNs可以被形式化为在图的乘积上操作的消息传递神经网络(MPNNs)。我们使用这个公式来设计我们的架构：首先，我们基于乘积图的连接性设计了一个注意力机制。接着，我们提出了一种新颖高效的子图GNNs位置编码方案，将其推导为乘积图的位置编码。

    In the realm of Graph Neural Networks (GNNs), two exciting research directions have recently emerged: Subgraph GNNs and Graph Transformers. In this paper, we propose an architecture that integrates both approaches, dubbed Subgraphormer, which combines the enhanced expressive power, message-passing mechanisms, and aggregation schemes from Subgraph GNNs with attention and positional encodings, arguably the most important components in Graph Transformers. Our method is based on an intriguing new connection we reveal between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be formulated as Message Passing Neural Networks (MPNNs) operating on a product of the graph with itself. We use this formulation to design our architecture: first, we devise an attention mechanism based on the connectivity of the product graph. Following this, we propose a novel and efficient positional encoding scheme for Subgraph GNNs, which we derive as a positional encoding for the product graph. 
    
[^53]: 面向协同过滤的频率感知图信号处理

    Frequency-aware Graph Signal Processing for Collaborative Filtering

    [https://arxiv.org/abs/2402.08426](https://arxiv.org/abs/2402.08426)

    面向协同过滤的频率感知图信号处理方法（FaGSP）采用级联滤波模块和并行滤波模块，旨在更准确地建模用户偏好并利用高阶邻域信息，以提高协同过滤的性能。

    

    近期，基于图信号处理（GSP）的推荐算法因其高效性而备受关注。然而，这些方法未考虑到反映用户/物品特征的各种交互的重要性，并未利用用户和物品的高阶邻域信息来建模用户偏好，从而导致子优性能。为解决上述问题，我们提出了一种面向协同过滤的频率感知图信号处理方法（FaGSP）。首先，我们设计了一个级联滤波模块，由理想的高通滤波器和理想的低通滤波器组成，以连续的方式捕获独特和共同的用户/物品特征，更准确地建模用户偏好。然后，我们设计了一个并行滤波模块，由两个低通滤波器组成，可以轻松捕获邻域的层次结构，以更准确地利用用户/物品的高阶邻域信息进行用户预测。

    Graph Signal Processing (GSP) based recommendation algorithms have recently attracted lots of attention due to its high efficiency. However, these methods failed to consider the importance of various interactions that reflect unique user/item characteristics and failed to utilize user and item high-order neighborhood information to model user preference, thus leading to sub-optimal performance. To address the above issues, we propose a frequency-aware graph signal processing method (FaGSP) for collaborative filtering. Firstly, we design a Cascaded Filter Module, consisting of an ideal high-pass filter and an ideal low-pass filter that work in a successive manner, to capture both unique and common user/item characteristics to more accurately model user preference. Then, we devise a Parallel Filter Module, consisting of two low-pass filters that can easily capture the hierarchy of neighborhood, to fully utilize high-order neighborhood information of users/items for more accurate user pre
    
[^54]: 通过熵传输核将未配对点的批次转移算子进行估计

    Transfer Operators from Batches of Unpaired Points via Entropic Transport Kernels

    [https://arxiv.org/abs/2402.08425](https://arxiv.org/abs/2402.08425)

    本文提出了一种通过熵传输核从批次的未配对点估计随机变量$X$和$Y$的联合概率的方法，并在理论上证明了其收敛性质。

    

    本文关注于通过$N$个独立观测块$(\boldsymbol{x}^i,\boldsymbol{y}^i)$（$i=1,\ldots,N$）来估计随机变量$X$和$Y$的联合概率，每个观测块包含$M$个样本$(\boldsymbol{x}^i,\boldsymbol{y}^i) = \bigl((x^i_j, y^i_{\sigma^i(j)}) \bigr)_{j=1}^M$，其中$\sigma^i$表示一个未知的排列，用于对$i.i.d.$采样的对$(x^i_j,y_j^i)$进行重新排序，$j=1,\ldots,M$。这意味着观测块内部样本的顺序是未知的。我们推导了一个最大似然推断函数，并提出了一个可计算的近似方法，并分析了它们的性质。特别是，我们证明了一个$\Gamma$-收敛结果，说明我们可以在块数$N$趋向于无穷大时，从经验近似中恢复出真实的密度。使用熵最优传输核，我们对一类假设空间建模，该空间的密度函数可以最小化推断函数。

    In this paper, we are concerned with estimating the joint probability of random variables $X$ and $Y$, given $N$ independent observation blocks $(\boldsymbol{x}^i,\boldsymbol{y}^i)$, $i=1,\ldots,N$, each of $M$ samples $(\boldsymbol{x}^i,\boldsymbol{y}^i) = \bigl((x^i_j, y^i_{\sigma^i(j)}) \bigr)_{j=1}^M$, where $\sigma^i$ denotes an unknown permutation of i.i.d. sampled pairs $(x^i_j,y_j^i)$, $j=1,\ldots,M$. This means that the internal ordering of the $M$ samples within an observation block is not known. We derive a maximum-likelihood inference functional, propose a computationally tractable approximation and analyze their properties. In particular, we prove a $\Gamma$-convergence result showing that we can recover the true density from empirical approximations as the number $N$ of blocks goes to infinity. Using entropic optimal transport kernels, we model a class of hypothesis spaces of density functions over which the inference functional can be minimized. This hypothesis class is 
    
[^55]: 条件神经专家过程用于从演示中学习

    Conditional Neural Expert Processes for Learning from Demonstration

    [https://arxiv.org/abs/2402.08424](https://arxiv.org/abs/2402.08424)

    条件神经专家过程（CNEP）是一种学习从演示中获取技能的新框架，通过将不同模式的演示分配给不同的专家网络，并利用潜在空间中的信息将专家与编码表示匹配，解决了相同技能演示的变化和多种方式获取的挑战。

    

    从演示中学习（LfD）是机器人学中广泛使用的一种技术，用于技能获取。然而，相同技能的演示可能存在显著的变化，或者学习系统可能同时尝试获取相同技能的不同方式，这使得将这些动作编码为运动原语变得具有挑战性。为了解决这些挑战，我们提出了一个LfD框架，即条件神经专家过程（CNEP），它学习将来自不同模式的演示分配给不同的专家网络，利用潜在空间中的内在信息将专家与编码表示匹配起来。CNEP不需要在哪种模式下轨迹属于的监督。在人工生成的数据集上进行的实验证明了CNEP的有效性。此外，我们将CNEP与另一个LfD框架——条件神经运动原语（CNMP）在一系列任务上的性能进行了比较，包括对真实机器人进行实验。

    Learning from Demonstration (LfD) is a widely used technique for skill acquisition in robotics. However, demonstrations of the same skill may exhibit significant variances, or learning systems may attempt to acquire different means of the same skill simultaneously, making it challenging to encode these motions into movement primitives. To address these challenges, we propose an LfD framework, namely the Conditional Neural Expert Processes (CNEP), that learns to assign demonstrations from different modes to distinct expert networks utilizing the inherent information within the latent space to match experts with the encoded representations. CNEP does not require supervision on which mode the trajectories belong to. Provided experiments on artificially generated datasets demonstrate the efficacy of CNEP. Furthermore, we compare the performance of CNEP with another LfD framework, namely Conditional Neural Movement Primitives (CNMP), on a range of tasks, including experiments on a real robo
    
[^56]: 基于无穷范数的分布估计

    Distribution Estimation under the Infinity Norm

    [https://arxiv.org/abs/2402.08422](https://arxiv.org/abs/2402.08422)

    本文提出了新的界限来在$\ell_\infty$范数下估计离散概率分布，实现了数据相关的收敛性保证，比现有结果有显著改进。通过运用切尔诺夫型不等式和经验贝尔斯坦界等方法，我们在合成和实际实验中验证了我们的结果。最后，我们将我们的方法应用于一个基本的选择性推断问题。

    

    我们提出了一种在$\ell_\infty$范数下估计离散概率分布的新界限。这些界限在各种精确意义上几乎是最优的，包括一种实例最优性。我们对最大似然估计器的数据相关收敛性保证明显优于当前已知结果。我们运用和创新了各种技术，包括切尔诺夫型不等式和经验贝尔斯坦界。我们在合成和实际实验中展示了我们的结果。最后，我们将我们的提出的框架应用于一个基本的选择性推断问题，其中我们估计样本中出现频率最高的概率。

    We present novel bounds for estimating discrete probability distributions under the $\ell_\infty$ norm. These are nearly optimal in various precise senses, including a kind of instance-optimality. Our data-dependent convergence guarantees for the maximum likelihood estimator significantly improve upon the currently known results. A variety of techniques are utilized and innovated upon, including Chernoff-type inequalities and empirical Bernstein bounds. We illustrate our results in synthetic and real-world experiments. Finally, we apply our proposed framework to a basic selective inference problem, where we estimate the most frequent probabilities in a sample.
    
[^57]: 保守和风险意识的离线多智能体强化学习在数字孪生中的应用

    Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins

    [https://arxiv.org/abs/2402.08421](https://arxiv.org/abs/2402.08421)

    本研究提出了一种应用于数字孪生的离线多智能体强化学习方案，通过整合分布式强化学习和保守Q学习来解决环境的不确定性和有限数据带来的认识不确定性。

    

    数字孪生（DT）平台被越来越认为是控制、优化和监控诸如下一代无线网络之类的复杂工程系统的有希望技术。采用DT解决方案面临的一个重要挑战是它们依赖于离线收集的数据，缺乏对物理环境的直接访问。这一限制在多智能体系统中尤为严重，因为传统的多智能体强化学习（MARL）需要与环境进行在线互动。将在线MARL方案直接应用于离线环境通常会因有限数据的认识不确定性而失败。在这项工作中，我们提出了一种用于基于DT的无线网络的离线MARL方案，它整合了分布式强化学习（distributional RL）和保守Q学习，以应对环境固有的案例性不确定性和有限数据引起的认识不确定性。为了进一步利用离线数据，我们改编了所提出的方案。

    Digital twin (DT) platforms are increasingly regarded as a promising technology for controlling, optimizing, and monitoring complex engineering systems such as next-generation wireless networks. An important challenge in adopting DT solutions is their reliance on data collected offline, lacking direct access to the physical environment. This limitation is particularly severe in multi-agent systems, for which conventional multi-agent reinforcement (MARL) requires online interactions with the environment. A direct application of online MARL schemes to an offline setting would generally fail due to the epistemic uncertainty entailed by the limited availability of data. In this work, we propose an offline MARL scheme for DT-based wireless networks that integrates distributional RL and conservative Q-learning to address the environment's inherent aleatoric uncertainty and the epistemic uncertainty arising from limited data. To further exploit the offline data, we adapt the proposed scheme t
    
[^58]: 在网络上相互作用的粒子系统: 网络和相互作用核的联合推断

    Interacting Particle Systems on Networks: joint inference of the network and the interaction kernel

    [https://arxiv.org/abs/2402.08412](https://arxiv.org/abs/2402.08412)

    本文研究了在网络上建模多智体系统的方法，提出了联合推断网络的权重矩阵和相互作用核的估计器，通过解决非凸优化问题并使用交替最小二乘（ALS）算法和交替最小二乘算子回归（ORALS）算法进行求解。在保证可识别性和良定义性的条件下，ALS算法表现出统计效率和鲁棒性，而ORALS算法是一致的，并且在渐近情况下具有正态性。

    

    在各种学科中，对网络上的多智体系统进行建模是一个基本的挑战。我们从由多条轨迹组成的数据中联合推断网络的权重矩阵和相互作用核，分别确定哪些智体与哪些其他智体相互作用以及这种相互作用的规则。我们提出的估计器自然地导致一个非凸优化问题，并研究了两种解决方案：一种基于交替最小二乘（ALS）算法，另一种基于一种名为交替最小二乘的算子回归（ORALS）的新算法。这两种算法都可扩展到大量数据轨迹。我们建立了保证可识别性和良定义性的强制性条件。尽管ALS算法在小数据情况下缺乏性能和收敛性保证，但表现出统计效率和鲁棒性。在强制性条件下，ORALS估计器是一致的，并且在渐近情况下具有正态性。

    Modeling multi-agent systems on networks is a fundamental challenge in a wide variety of disciplines. We jointly infer the weight matrix of the network and the interaction kernel, which determine respectively which agents interact with which others and the rules of such interactions from data consisting of multiple trajectories. The estimator we propose leads naturally to a non-convex optimization problem, and we investigate two approaches for its solution: one is based on the alternating least squares (ALS) algorithm; another is based on a new algorithm named operator regression with alternating least squares (ORALS). Both algorithms are scalable to large ensembles of data trajectories. We establish coercivity conditions guaranteeing identifiability and well-posedness. The ALS algorithm appears statistically efficient and robust even in the small data regime but lacks performance and convergence guarantees. The ORALS estimator is consistent and asymptotically normal under a coercivity
    
[^59]: 过渡受限的贝叶斯优化在马尔可夫决策过程中的应用

    Transition Constrained Bayesian Optimization via Markov Decision Processes

    [https://arxiv.org/abs/2402.08406](https://arxiv.org/abs/2402.08406)

    本文介绍了一种过渡受限的贝叶斯优化方法，通过马尔可夫决策过程的框架，使用强化学习解决了由于转变约束导致的搜索空间依赖历史的问题，并在化学反应器优化、信息化路径规划、机器校准等领域进行了应用。

    

    贝叶斯优化是一种优化黑盒函数的方法。传统上，它关注的是可以任意查询搜索空间的情况。然而，许多现实生活中的问题并不具备这种灵活性；特别是，下一个查询的搜索空间可能取决于先前的查询。物理科学领域的例子中存在一些挑战，如局部移动限制、特定变量的单调性要求以及转变影响测量精度。总之，这些过渡约束需要一种规划方法。本文通过马尔可夫决策过程的框架扩展了贝叶斯优化，通过强化学习迭代地解决我们目标的一个可行线性化，从而获得能够提前规划长时间跨度的策略。得到的策略可能是依赖历史的和非马尔可夫的。我们展示了在化学反应器优化、信息化路径规划、机器校准等方面的应用。

    Bayesian optimization is a methodology to optimize black-box functions. Traditionally, it focuses on the setting where you can arbitrarily query the search space. However, many real-life problems do not offer this flexibility; in particular, the search space of the next query may depend on previous ones. Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements. Altogether, such transition constraints necessitate a form of planning. This work extends Bayesian optimization via the framework of Markov Decision Processes, iteratively solving a tractable linearization of our objective using reinforcement learning to obtain a policy that plans ahead over long horizons. The resulting policy is potentially history-dependent and non-Markovian. We showcase applications in chemical reactor optimization, informative path planning, machine calibration, and other s
    
[^60]: 一种改进通用化能力的1NN分类器正则化方法的创新途径

    A Novel Approach to Regularising 1NN classifier for Improved Generalization

    [https://arxiv.org/abs/2402.08405](https://arxiv.org/abs/2402.08405)

    本文提出了一种改进通用化能力的非参数分类器，即分水岭分类器，通过对1NN分类器进行正则化。分水岭分类器可以在任意密集的数据集上找到任意边界，并具有很小的VC维度，从而能够实现良好的泛化。

    

    在本文中，我们提出了一种非参数分类器，可以学习任意边界并具有良好的泛化能力。我们的方法基于一种新颖的贪心方法，对1NN分类器进行正则化。我们将这类分类器称为分水岭分类器。已知1NN分类器容易过拟合，但具有很大的VC维度，因此不能很好地泛化。我们展示了分水岭分类器可以在任意足够密集的数据集上找到任意边界，并且具有很小的VC维度；因此，分水岭分类器可以实现良好的泛化。传统的1NN分类器正则化方法是考虑K个最近邻。邻域组件分析（NCA）提出了一种学习与（n-1）最近邻分类器一致的表示的方法，其中n表示数据集的大小。在本文中，我们提出了一种可以学习与分水岭分类器一致的表示的损失函数，并且展示了其结果。

    In this paper, we propose a class of non-parametric classifiers, that learn arbitrary boundaries and generalize well.   Our approach is based on a novel way to regularize 1NN classifiers using a greedy approach. We refer to this class of classifiers as Watershed Classifiers. 1NN classifiers are known to trivially over-fit but have very large VC dimension, hence do not generalize well. We show that watershed classifiers can find arbitrary boundaries on any dense enough dataset, and, at the same time, have very small VC dimension; hence a watershed classifier leads to good generalization.   Traditional approaches to regularize 1NN classifiers are to consider $K$ nearest neighbours. Neighbourhood component analysis (NCA) proposes a way to learn representations consistent with ($n-1$) nearest neighbour classifier, where $n$ denotes the size of the dataset. In this article, we propose a loss function which can learn representations consistent with watershed classifiers, and show that it out
    
[^61]: LOSS-GAT: 标签传播和一类半监督图注意力网络用于假新闻检测

    LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection

    [https://arxiv.org/abs/2402.08401](https://arxiv.org/abs/2402.08401)

    本文介绍了一种名为LOSS-GAT的半监督和一类方法用于假新闻检测，采用了标签传播和图注意力网络，以解决标记数据集有限性的挑战。

    

    在广泛社交网络的时代，虚假新闻的快速传播已经成为一个重大威胁，在人们生活的各个方面造成了不利影响。机器学习和深度学习方法已被广泛应用于识别假新闻。然而，识别假新闻的一个重大挑战是标记新闻数据集的有限性。因此，使用只有一小部分标记数据的一个类学习（OCL）方法可以是解决这一挑战的合适方法。另一方面，将数据表示为图可以访问到不同内容和结构信息，并且图上的标签传播方法可以有效地预测节点的标签。在本文中，我们采用基于图的模型进行数据表示，并引入一种半监督和一类方法，用于假新闻检测，称为LOSS-GAT。

    In the era of widespread social networks, the rapid dissemination of fake news has emerged as a significant threat, inflicting detrimental consequences across various dimensions of people's lives. Machine learning and deep learning approaches have been extensively employed for identifying fake news. However, a significant challenge in identifying fake news is the limited availability of labeled news datasets. Therefore, the One-Class Learning (OCL) approach, utilizing only a small set of labeled data from the interest class, can be a suitable approach to address this challenge. On the other hand, representing data as a graph enables access to diverse content and structural information, and label propagation methods on graphs can be effective in predicting node labels. In this paper, we adopt a graph-based model for data representation and introduce a semi-supervised and one-class approach for fake news detection, called LOSS-GAT. Initially, we employ a two-step label propagation algori
    
[^62]: 使用随机平滑的自适应分层认证进行分割

    Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing

    [https://arxiv.org/abs/2402.08400](https://arxiv.org/abs/2402.08400)

    本文提出了一种适用于图像语义分割的自适应分层认证方法，能够在多级层级标签空间中认证图像像素，并提供更多经过认证的语义信息。

    

    通常的认证方法是在预定义的细粒度类别集上操作。然而，在本文中，我们提出了一种新颖、更普遍且实用的设置，即自适应分层认证用于图像语义分割。在这个设置中，认证可以在由细到粗的多级层级标签空间中进行。与经典方法不同，在不稳定的组件上，我们的方法自适应地放松了认证到层级中的更粗粒度的级别。这种放松降低了放弃率，同时提供了更多经过认证的语义有意义的信息。我们数学地形式化了问题设置，并首次引入了一种自适应分层认证算法用于图像语义分割，它可以在层级中对图像像素进行认证，并证明了其保证的正确性。由于认证的准确度在遍历时不考虑信息丢失。

    Common certification methods operate on a flat pre-defined set of fine-grained classes. In this paper, however, we propose a novel, more general, and practical setting, namely adaptive hierarchical certification for image semantic segmentation. In this setting, the certification can be within a multi-level hierarchical label space composed of fine to coarse levels. Unlike classic methods where the certification would abstain for unstable components, our approach adaptively relaxes the certification to a coarser level within the hierarchy. This relaxation lowers the abstain rate whilst providing more certified semantically meaningful information. We mathematically formulate the problem setup and introduce, for the first time, an adaptive hierarchical certification algorithm for image semantic segmentation, that certifies image pixels within a hierarchy and prove the correctness of its guarantees. Since certified accuracy does not take the loss of information into account when traversing
    
[^63]: 选择性学习：实现动态正则化的鲁棒校准

    Selective Learning: Towards Robust Calibration with Dynamic Regularization

    [https://arxiv.org/abs/2402.08384](https://arxiv.org/abs/2402.08384)

    本研究提出了一种名为动态正则化（DReg）的方法，通过训练学习应该学到什么，从而解决深度学习中的过拟合和误校准问题。

    

    深度学习中的误校准指的是预测的可信度与性能之间存在差异。这个问题通常是由过拟合问题引起的，过拟合问题的特点是学习训练集中呈现出的所有内容，导致在测试过程中进行过于自信的预测。现有方法通常通过在目标函数中添加最大熵正则化器来解决过拟合问题并缓解误校准问题。这个目标可以理解为寻找一个模型，通过增加可信度来适应实际标签，同时通过降低可信度来最大化预测概率的熵。然而，以前的方法缺乏对可信度调整的明确指导，导致目标冲突（增加但也降低可信度）。因此，我们引入了一种称为动态正则化（DReg）的方法，旨在通过训练学习应该学到什么，从而避免可信度调整的权衡。

    Miscalibration in deep learning refers to there is a discrepancy between the predicted confidence and performance. This problem usually arises due to the overfitting problem, which is characterized by learning everything presented in the training set, resulting in overconfident predictions during testing. Existing methods typically address overfitting and mitigate the miscalibration by adding a maximum-entropy regularizer to the objective function. The objective can be understood as seeking a model that fits the ground-truth labels by increasing the confidence while also maximizing the entropy of predicted probabilities by decreasing the confidence. However, previous methods lack clear guidance on confidence adjustment, leading to conflicting objectives (increasing but also decreasing confidence). Therefore, we introduce a method called Dynamic Regularization (DReg), which aims to learn what should be learned during training thereby circumventing the confidence adjusting trade-off. At 
    
[^64]: 通过潜在全局演化实现偏微分方程的正逆问题的不确定性量化

    Uncertainty Quantification for Forward and Inverse Problems of PDEs via Latent Global Evolution

    [https://arxiv.org/abs/2402.08383](https://arxiv.org/abs/2402.08383)

    本论文提出了一种将高效和精确的不确定性量化集成到基于深度学习的替代模型中的方法，称为LE-PDE-UQ。该方法利用潜在空间中的潜在向量来演化系统的状态和相应的不确定性估计。

    

    基于深度学习的替代模型在速度方面相对于传统的偏微分方程求解器表现出了显著的优势，往往能够实现10到1000倍的加速。然而，一个重要的挑战阻碍了它们在科学和工业领域的广泛应用，即缺乏对其预测不确定性的理解，特别是在涉及关键决策的情况下。为了解决这个限制，我们提出了一种将高效和精确的不确定性量化集成到基于深度学习的替代模型中的方法。我们的方法称为带不确定性量化的偏微分方程的潜在演化（LE-PDE-UQ），为前向和反向问题赋予了深度学习替代模型强大而高效的不确定性量化能力。LE-PDE-UQ利用潜在空间中的潜在向量来演化系统的状态和相应的不确定性估计。

    Deep learning-based surrogate models have demonstrated remarkable advantages over classical solvers in terms of speed, often achieving speedups of 10 to 1000 times over traditional partial differential equation (PDE) solvers. However, a significant challenge hindering their widespread adoption in both scientific and industrial domains is the lack of understanding about their prediction uncertainties, particularly in scenarios that involve critical decision making. To address this limitation, we propose a method that integrates efficient and precise uncertainty quantification into a deep learning-based surrogate model. Our method, termed Latent Evolution of PDEs with Uncertainty Quantification (LE-PDE-UQ), endows deep learning-based surrogate models with robust and efficient uncertainty quantification capabilities for both forward and inverse problems. LE-PDE-UQ leverages latent vectors within a latent space to evolve both the system's state and its corresponding uncertainty estimation.
    
[^65]: 表示和解释的二重奏及其加剧作用

    The Duet of Representations and How Explanations Exacerbate It

    [https://arxiv.org/abs/2402.08379](https://arxiv.org/abs/2402.08379)

    该研究发现，算法的因果表示可能与人类的先前信念相冲突，解释会将注意力导向冲突特征并远离其他相关特征，这可能导致因果过度归因并对人类的信息处理产生不利影响。

    

    算法能够对人类的感知中特征和标签之间的关系进行因果表示。这种表示可能与人类的先前信念相冲突。解释可以将人类的注意力引导到冲突的特征上，并使其远离其他相关特征。这导致因果过度归因，并可能对人类的信息处理产生不利影响。在一项实地实验中，我们将一个经过XGBoost训练的模型作为决策辅助工具应用于公共就业服务中心的顾问，用于预测候选人长期失业的风险。治疗组的顾问还提供了SHAP作为解释。结果显示，当冲突特征作为解释的一部分显示时，人类的决策质量较差。

    An algorithm effects a causal representation of relations between features and labels in the human's perception. Such a representation might conflict with the human's prior belief. Explanations can direct the human's attention to the conflicting feature and away from other relevant features. This leads to causal overattribution and may adversely affect the human's information processing. In a field experiment we implemented an XGBoost-trained model as a decision-making aid for counselors at a public employment service to predict candidates' risk of long-term unemployment. The treatment group of counselors was also provided with SHAP. The results show that the quality of the human's decision-making is worse when a feature on which the human holds a conflicting prior belief is displayed as part of the explanation.
    
[^66]: 动态策略下多步预测的时间序列分类

    Time-Series Classification for Dynamic Strategies in Multi-Step Forecasting

    [https://arxiv.org/abs/2402.08373](https://arxiv.org/abs/2402.08373)

    本文提出了动态策略（DyStrat）用于多步预测，并在实验中证明了其在多个时间序列数据集上的优越性能。

    

    时间序列中的多步预测（MSF）是几乎所有时间领域的基础，能够预测多个未来时间步骤的能力。为了进行这种预测，必须假设时间动态的递归复杂性，这种假设被称为训练预测模型所使用的预测策略。之前的研究表明，在评估未见数据之前，不清楚哪种预测策略是最优的。此外，目前的多步预测方法使用单个（固定）的预测策略。在本文中，我们对最优预测策略的实例级别变化进行了描述，并提出了用于MSF的动态策略（DyStrat）。我们在来自不同规模、领域和多步预测长度的10个数据集上进行了实验。使用基于随机森林的分类器时，DyStrat在94%的时间内优于最佳固定策略，平均均方误差降低了11%。

    Multi-step forecasting (MSF) in time-series, the ability to make predictions multiple time steps into the future, is fundamental to almost all temporal domains. To make such forecasts, one must assume the recursive complexity of the temporal dynamics. Such assumptions are referred to as the forecasting strategy used to train a predictive model. Previous work shows that it is not clear which forecasting strategy is optimal a priori to evaluating on unseen data. Furthermore, current approaches to MSF use a single (fixed) forecasting strategy.   In this paper, we characterise the instance-level variance of optimal forecasting strategies and propose Dynamic Strategies (DyStrat) for MSF. We experiment using 10 datasets from different scales, domains, and lengths of multi-step horizons. When using a random-forest-based classifier, DyStrat outperforms the best fixed strategy, which is not knowable a priori, 94% of the time, with an average reduction in mean-squared error of 11%. Our approach 
    
[^67]: 使用混合多标准推荐系统和遗传优化帮助大学生选择选修课程

    Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization

    [https://arxiv.org/abs/2402.08371](https://arxiv.org/abs/2402.08371)

    本文提出了一种混合推荐系统，结合了协同过滤和基于内容的过滤，利用多个准则为大学生推荐最合适的选修课程。通过遗传算法自动发现最佳配置，并使用科尔多瓦大学的真实信息进行实验验证。

    

    大学课程的广泛可用性和学术计划的灵活性揭示了推荐系统在这一领域的重要性。这些系统是帮助学生选择符合个人兴趣和学术表现的课程的工具。本文提出了一种混合推荐系统，结合了协同过滤和基于内容的过滤，利用与学生和课程信息相关的多个准则来为学生推荐最合适的课程。为了自动发现最佳的推荐系统配置，我们开发了遗传算法来确定包括最相关的准则和其他参数配置的最优配置。实验研究使用了科尔多瓦大学（西班牙）计算机科学学位的真实信息，包括三个学年期间从95名学生和63门课程的2500个条目中收集的信息。

    The wide availability of specific courses together with the flexibility of academic plans in university studies reveal the importance of Recommendation Systems (RSs) in this area. These systems appear as tools that help students to choose courses that suit to their personal interests and their academic performance. This paper presents a hybrid RS that combines Collaborative Filtering (CF) and Content-based Filtering (CBF) using multiple criteria related both to student and course information to recommend the most suitable courses to the students. A Genetic Algorithm (GA) has been developed to automatically discover the optimal RS configuration which include both the most relevant criteria and the configuration of the rest of parameters. The experimental study has used real information of Computer Science Degree of University of Cordoba (Spain) including information gathered from students during three academic years, counting on 2500 entries of 95 students and 63 courses. Experimental r
    
[^68]: RBF-PINN：物理信息神经网络中的非Fourier位置嵌入

    RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks

    [https://arxiv.org/abs/2402.08367](https://arxiv.org/abs/2402.08367)

    本论文介绍了RBF-PINN方法，在物理信息神经网络中应用非Fourier位置嵌入，解决了基于Fourier特征映射的局限性，并在各种正向和反向问题中取得了有效结果。

    

    虽然最近许多物理信息神经网络（PINNs）的变体在求解偏微分方程方面取得了显著的成功，但来自更广泛的神经表示研究的特征映射的经验优势在很大程度上被忽视了。我们强调了在某些情况下广泛使用的基于Fourier的特征映射的局限性，并建议使用具有条件正定性质的径向基函数。实证发现表明我们的方法在各种正向和反向问题案例中的有效性。我们的方法可以无缝集成到基于坐标的输入神经网络中，并为PINNs研究的更广泛领域做出贡献。

    While many recent Physics-Informed Neural Networks (PINNs) variants have had considerable success in solving Partial Differential Equations, the empirical benefits of feature mapping drawn from the broader Neural Representations research have been largely overlooked. We highlight the limitations of widely used Fourier-based feature mapping in certain situations and suggest the use of the conditionally positive definite Radial Basis Function. The empirical findings demonstrate the effectiveness of our approach across a variety of forward and inverse problem cases. Our method can be seamlessly integrated into coordinate-based input neural networks and contribute to the wider field of PINNs research.
    
[^69]: NeuRes: 学习命题可满足性的证明

    NeuRes: Learning Proofs of Propositional Satisfiability

    [https://arxiv.org/abs/2402.08365](https://arxiv.org/abs/2402.08365)

    NeuRes是一种神经符号证明为基础的SAT解析器，能够证明不可满足性并加速找到可满足真值分配的过程。

    

    我们介绍了一种神经符号证明为基础的SAT解析器NeuRes。与其他神经SAT解算法不同，NeuRes能够证明不可满足性，而不仅仅是预测它。NeuRes通过采用命题推理来证明不可满足性并加速在不可满足和可满足公式中找到满足真值分配的过程。为了实现这一点，我们提出了一种新颖的架构，它结合了图神经网络和指针网络的元素，从动态图结构中自动选择节点对，这对于生成解析证明是至关重要的。我们使用与NeuroSAT相同的随机公式分布编制了一个包含教师证明和真值分配的数据集，对我们的模型进行训练和评估。在实验证明中，我们展示了NeuRes在不同分布上比NeuroSAT解决更多的测试公式，并且需要更少的数据。

    We introduce NeuRes, a neuro-symbolic proof-based SAT solver. Unlike other neural SAT solving methods, NeuRes is capable of proving unsatisfiability as opposed to merely predicting it. By design, NeuRes operates in a certificate-driven fashion by employing propositional resolution to prove unsatisfiability and to accelerate the process of finding satisfying truth assignments in case of unsat and sat formulas, respectively. To realize this, we propose a novel architecture that adapts elements from Graph Neural Networks and Pointer Networks to autoregressively select pairs of nodes from a dynamic graph structure, which is essential to the generation of resolution proofs. Our model is trained and evaluated on a dataset of teacher proofs and truth assignments that we compiled with the same random formula distribution used by NeuroSAT. In our experiments, we show that NeuRes solves more test formulas than NeuroSAT by a rather wide margin on different distributions while being much more data
    
[^70]: 在带有差分隐私训练的Noisy-SGD中的隐式偏差：及其在训练中的应用

    Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training

    [https://arxiv.org/abs/2402.08344](https://arxiv.org/abs/2402.08344)

    通过使用带有差分隐私训练的Noisy-SGD方法，我们发现随机性而非剪裁梯度是导致训练过程中的隐式偏差的原因，并且这种偏差会被加剧，这对于使用巨大批量数据的强差分隐私保证构成重要挑战。

    

    使用随机梯度下降(SGD)以小批量训练深度神经网络(DNN)相较于大批量训练能够获得更好的测试性能。SGD特定的噪声结构被认为是导致这种隐式偏差的原因。用于确保DNN训练中的差异隐私(DP)的DP-SGD会给剪裁梯度添加高斯噪声。令人惊讶的是，大批量训练仍然会导致显著的性能下降，这是一个重要的挑战，因为强DP保证需要使用大批量数据。我们首先展示了现象在Noisy-SGD（没有剪裁的DP-SGD）中的存在，这表明随机性（而不是剪裁）是这种隐式偏差的原因，即使加入了额外的各向同性高斯噪声。我们在线性最小二乘和对角线线性网络设置上对使用连续版本的Noisy-SGD获得的解进行了理论分析，并揭示了隐式偏差确实被加剧了。

    Training Deep Neural Networks (DNNs) with small batches using Stochastic Gradient Descent (SGD) yields superior test performance compared to larger batches. The specific noise structure inherent to SGD is known to be responsible for this implicit bias. DP-SGD, used to ensure differential privacy (DP) in DNNs' training, adds Gaussian noise to the clipped gradients. Surprisingly, large-batch training still results in a significant decrease in performance, which poses an important challenge because strong DP guarantees necessitate the use of massive batches. We first show that the phenomenon extends to Noisy-SGD (DP-SGD without clipping), suggesting that the stochasticity (and not the clipping) is the cause of this implicit bias, even with additional isotropic Gaussian noise. We theoretically analyse the solutions obtained with continuous versions of Noisy-SGD for the Linear Least Square and Diagonal Linear Network settings, and reveal that the implicit bias is indeed amplified by the add
    
[^71]: 不确定性量化通过稳定分布传播

    Uncertainty Quantification via Stable Distribution Propagation

    [https://arxiv.org/abs/2402.08324](https://arxiv.org/abs/2402.08324)

    通过局部线性化，我们提出了一种新的方法，能够通过神经网络传播稳定概率分布来量化输出的不确定性。我们的方法在预测校准置信区间和选择性预测方面显示出了明显的优势。

    

    我们提出了一种通过神经网络传播稳定概率分布的新方法。我们的方法基于局部线性化，在ReLU非线性方面，我们证明它是总变差距离的最优近似。这使得能够通过神经网络传播高斯和柯西输入不确定性来量化其输出不确定性。为了展示传播分布的实用性，我们将所提出的方法应用于预测校准置信区间和选择性预测超出分布的数据上。结果表明了传播分布的广泛适用性，并显示了我们的方法在诸如矩匹配等其他方法上的优势。

    We propose a new approach for propagating stable probability distributions through neural networks. Our method is based on local linearization, which we show to be an optimal approximation in terms of total variation distance for the ReLU non-linearity. This allows propagating Gaussian and Cauchy input uncertainties through neural networks to quantify their output uncertainties. To demonstrate the utility of propagating distributions, we apply the proposed method to predicting calibrated confidence intervals and selective prediction on out-of-distribution data. The results demonstrate a broad applicability of propagating distributions and show the advantages of our method over other approaches such as moment matching.
    
[^72]: 使用混合正则化器的优化探索：在部分监测中具有对抗鲁棒性的对数遗憾

    Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret with Adversarial Robustness in Partial Monitoring

    [https://arxiv.org/abs/2402.08321](https://arxiv.org/abs/2402.08321)

    这篇论文介绍了一种在部分监测问题中探索优化的方法，通过使用混合正则化器可以提高在随机和对抗环境中的遗憾界限。

    

    部分监测是一种具有有限观测的在线决策问题的通用框架。为了从这种有限观测中做出决策，需要找到一个适当的探索分布。最近，提出了一种用于此目的的强大方法，即通过优化进行探索（ExO），它利用追踪正则化最优方法，在广泛的在线决策问题中实现对抗环境下的最优界限。然而，在随机环境中纯粹应用ExO会显著降低遗憾界限。为了解决这个局部可观测游戏中的问题，我们首先建立了一个新颖的ExO与混合正则化器的框架和分析。这个发展使我们能够显著改进最佳双赢算法（BOBW）的现有遗憾界限，在随机和对抗环境中都实现了几乎最优的界限。特别地，我们得出了一个随机遗憾界限为$O(\sum_{a \neq a^*} k^2 m^2$

    Partial monitoring is a generic framework of online decision-making problems with limited observations. To make decisions from such limited observations, it is necessary to find an appropriate distribution for exploration. Recently, a powerful approach for this purpose, exploration by optimization (ExO), was proposed, which achieves the optimal bounds in adversarial environments with follow-the-regularized-leader for a wide range of online decision-making problems. However, a naive application of ExO in stochastic environments significantly degrades regret bounds. To resolve this problem in locally observable games, we first establish a novel framework and analysis for ExO with a hybrid regularizer. This development allows us to significantly improve the existing regret bounds of best-of-both-worlds (BOBW) algorithms, which achieves nearly optimal bounds both in stochastic and adversarial environments. In particular, we derive a stochastic regret bound of $O(\sum_{a \neq a^*} k^2 m^2 \
    
[^73]: 使用物理信息神经网络近似Fisher方程的解的族群

    Approximating Families of Sharp Solutions to Fisher's Equation with Physics-Informed Neural Networks

    [https://arxiv.org/abs/2402.08313](https://arxiv.org/abs/2402.08313)

    本文利用物理信息神经网络（PINNs）近似求解Fisher方程，针对大反应速率系数条件下的行波解，引入了残差加权方案来提高解的精度，并通过特定的网络架构和反应速率系数作为额外输入，评估了PINN近似整个解族群的能力。

    

    本文利用物理信息神经网络（PINNs）求解Fisher方程，这是一个具有简单性和重要性的反应扩散系统的基本表示。重点是研究在大反应速率系数条件下的Fisher方程，其中解呈现为行波，由于波前的陡峭性，对数值方法提出了挑战。为了解决标准PINN方法所面临的优化挑战，引入了一种残差加权方案。该方案旨在通过考虑反应扩散方程中的反应项来增强行波前的跟踪能力。此外，还研究了一种特定的网络架构，专门针对行波形式的解。最后，通过将反应速率系数作为网络的额外输入，评估了PINN近似整个解族群的能力。

    This paper employs physics-informed neural networks (PINNs) to solve Fisher's equation, a fundamental representation of a reaction-diffusion system with both simplicity and significance. The focus lies specifically in investigating Fisher's equation under conditions of large reaction rate coefficients, wherein solutions manifest as traveling waves, posing a challenge for numerical methods due to the occurring steepness of the wave front. To address optimization challenges associated with the standard PINN approach, a residual weighting scheme is introduced. This scheme is designed to enhance the tracking of propagating wave fronts by considering the reaction term in the reaction-diffusion equation. Furthermore, a specific network architecture is studied which is tailored for solutions in the form of traveling waves. Lastly, the capacity of PINNs to approximate an entire family of solutions is assessed by incorporating the reaction rate coefficient as an additional input to the network 
    
[^74]: 通过提示的上下文向量检测钓鱼网络攻击

    Prompted Contextual Vectors for Spear-Phishing Detection

    [https://arxiv.org/abs/2402.08309](https://arxiv.org/abs/2402.08309)

    通过新的文档向量化方法，我们的方法使用大型语言模型来检测钓鱼网络攻击的电子邮件，并在实验证明具有高效性能。

    

    钓鱼网络攻击是一个重大的安全挑战，而大型语言模型（LLMs）通过生成令人信服的电子邮件并方便目标侦察来升级了威胁。为了解决这个问题，我们提出了一种基于新颖文档向量化方法的检测方法，该方法利用LLMs的集合来创建表示向量。通过提示LLMs来推理和回答人工制定的问题，我们量化电子邮件内容中常见说服原则的存在，为下游监督机器学习模型生成提示上下文文档向量。我们使用一个专有系统生成的独特数据集来评估我们的方法，该系统自动化目标侦察和钓鱼电子邮件的创建。我们的方法在仅包含传统钓鱼和良性电子邮件的训练集中实现了91%的F1得分，其中关键贡献包括一种创新的文档向量化方法。

    Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email's content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include an innovative document vectorization method utilizin
    
[^75]: ChatCell: 利用自然语言促进单细胞分析

    ChatCell: Facilitating Single-Cell Analysis with Natural Language

    [https://arxiv.org/abs/2402.08303](https://arxiv.org/abs/2402.08303)

    ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。

    

    随着大型语言模型(LLMs)的快速发展，它们在科学中的影响日益突出。LLMs在任务泛化和自由对话方面的新兴能力可以极大地推进化学和生物学等领域。然而，单细胞生物学这个构成生物体基础构件的领域仍面临一些挑战。当前方法在知识门槛和可扩展性方面存在限制，阻碍了LLMs在掌握单细胞数据方面的充分利用，影响了直接可访问和快速迭代的能力。为此，我们引入了ChatCell，通过利用词汇适应和统一序列生成，它在单细胞生物学领域获得了深厚的专业知识和适应各种分析任务的能力，标志着一种范式转变。

    As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
    
[^76]: 多层GNN预处理器用于解决大规模问题

    Multi-Level GNN Preconditioner for Solving Large Scale Problems

    [https://arxiv.org/abs/2402.08296](https://arxiv.org/abs/2402.08296)

    本论文介绍了一个多层GNN预处理器用于解决大规模问题，该预处理器集成了GNN模型和多层域分解框架，能够通过增强Krylov方法的效率以不同精度水平收敛。

    

    大规模数值模拟通常需要进行艰难的计算。高性能计算已经改进了这个过程，但是将传统代码适应并行GPU计算仍然具有挑战性。与此同时，机器学习模型可以有效地利用GPU计算，但往往在泛化和准确性方面遇到困难。图神经网络（GNN）特别适用于学习从网格等非结构化数据中，但往往受限于小规模问题。此外，所训练模型的能力通常限制了基于数据驱动的解决方案的准确性。为了同时从两个领域受益，本文介绍了一个新型的预处理器，将GNN模型与多层域分解框架集成在一起。所提出的基于GNN的预处理器用于增强Krylov方法的效率，从而产生一个混合求解器，可以以任何所需的准确性收敛。Krylov方法的效率从GNN预处理器中受益良多。

    Large-scale numerical simulations often come at the expense of daunting computations. High-Performance Computing has enhanced the process, but adapting legacy codes to leverage parallel GPU computations remains challenging. Meanwhile, Machine Learning models can harness GPU computations effectively but often struggle with generalization and accuracy. Graph Neural Networks (GNNs), in particular, are great for learning from unstructured data like meshes but are often limited to small-scale problems. Moreover, the capabilities of the trained model usually restrict the accuracy of the data-driven solution. To benefit from both worlds, this paper introduces a novel preconditioner integrating a GNN model within a multi-level Domain Decomposition framework. The proposed GNN-based preconditioner is used to enhance the efficiency of a Krylov method, resulting in a hybrid solver that can converge with any desired level of accuracy. The efficiency of the Krylov method greatly benefits from the GN
    
[^77]: 数据污染对反事实解释的影响

    The Effect of Data Poisoning on Counterfactual Explanations

    [https://arxiv.org/abs/2402.08290](https://arxiv.org/abs/2402.08290)

    本研究研究了反事实解释在数据污染方面的脆弱性，发现最先进的反事实生成方法和工具包容易受到数据污染的影响。

    

    反事实解释是分析黑盒系统预测的一种流行方法，它们提供了根据不同情况建议改变输入以获得不同（更有利）系统输出的计算补救机会。然而，最近的研究突显了它们对不同类型操纵的脆弱性。本研究研究了反事实解释对数据污染的脆弱性。我们在增加三个不同层次的补救成本方面，形式化地研究了反事实解释在单个实例、某个子组或所有实例上的数据污染。我们证明了最先进的反事实生成方法和工具包对此类数据污染是脆弱的。

    Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \& toolboxes are vulnerable to such data poisoning.
    
[^78]: Pix2Code：学习将神经视觉概念组合成程序

    Pix2Code: Learning to Compose Neural Visual Concepts as Programs

    [https://arxiv.org/abs/2402.08280](https://arxiv.org/abs/2402.08280)

    Pix2Code 是一个将神经视觉概念组合成程序的框架，通过利用显式、组合的符号和隐式的神经表示能力，从图像中检索对象表示并将关系概念合成为lambda演算程序，来解决通用性和可解释性的挑战。在推理领域Kandinsky Patterns和CURI上的评估结果表明，Pix2Code 能够识别组合视觉概念并推广到新数据和推理任务。

    

    在无监督学习中，学习从图像中抽象概念的挑战在于需要将视觉感知和通用关系推理进行整合。此外，该任务的无监督性质使得人类用户需要能够理解模型学到的概念，并可能修正错误的行为。为了解决视觉概念学习的通用性和可解释性约束，我们提出了Pix2Code，这是一个将程序合成扩展到视觉关系推理的框架，利用了明确的、组合的符号和隐式的神经表示的能力。通过从图像中检索对象表示并将关系概念合成为lambda演算程序来实现这一点。我们在具有挑战性的推理领域Kandinsky Patterns和CURI上评估了Pix2Code的多样特性，从而测试其识别组合视觉概念并推广到新数据和推理任务的能力。

    The challenge in learning abstract concepts from images in an unsupervised fashion lies in the required integration of visual perception and generalizable relational reasoning. Moreover, the unsupervised nature of this task makes it necessary for human users to be able to understand a model's learnt concepts and potentially revise false behaviours. To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational reasoning by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations. This is achieved by retrieving object representations from images and synthesizing relational concepts as lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and co
    
[^79]: 朝着忠实和强大的基于证据的问答专家的方向前进

    Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering

    [https://arxiv.org/abs/2402.08277](https://arxiv.org/abs/2402.08277)

    这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。

    

    对大型语言模型（LLM）更忠实和可追踪的答案的进步对于各种研究和实践活动至关重要。其中一种达到这个目标的方法是基于可靠的来源提供答案。然而，这种基于证据的问答在使用LLM时已经证明在引用正确的来源（来源质量）和准确地表示来源中的信息（答案归因能力）方面工作不足。在这项工作中，我们系统地研究了如何鲁棒地微调LLM，以提高来源质量和答案归因能力。具体而言，我们引入了一个数据生成流水线，其中包括自动数据质量过滤器，可以大规模合成多样化的高质量训练和测试数据。我们还引入了四个测试集，以对微调后的专家模型的鲁棒性进行基准测试。广泛的评估结果表明，在合成数据上进行微调可以提高在内部和外部分布的性能。%基于证据的问答案例。此外，我们展示了用于评估的四个测试集，以评估微调后的专家模型的鲁棒性。

    Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
    
[^80]: 深度ReLU神经网络中的几何引导隐式正则化

    Geometry-induced Implicit Regularization in Deep ReLU Neural Networks

    [https://arxiv.org/abs/2402.08269](https://arxiv.org/abs/2402.08269)

    通过研究参数变化时输出集合的几何特征，我们发现在深度ReLU神经网络的优化过程中存在几何引导的隐式正则化现象。

    

    众所周知，具有比训练样本更多参数的神经网络不会过拟合。隐式正则化现象在优化过程中出现，对“好”的网络有利。因此，如果我们不考虑所有可能的网络，而只考虑“好”的网络，参数数量就不是一个足够衡量复杂性的指标。为了更好地理解在优化过程中哪些网络受到青睐，我们研究了参数变化时输出集合的几何特征。当输入固定时，我们证明了这个集合的维度会发生变化，并且局部维度，即批次功能维度，几乎总是由隐藏层中的激活模式决定。我们证明了批次功能维度对网络参数化的对称性（神经元排列和正向缩放）是不变的。实证上，我们证实了在优化过程中批次功能维度会下降。因此，优化过程具有隐式正则化的效果。

    It is well known that neural networks with many more parameters than training examples do not overfit. Implicit regularization phenomena, which are still not well understood, occur during optimization and 'good' networks are favored. Thus the number of parameters is not an adequate measure of complexity if we do not consider all possible networks but only the 'good' ones. To better understand which networks are favored during optimization, we study the geometry of the output set as parameters vary. When the inputs are fixed, we prove that the dimension of this set changes and that the local dimension, called batch functional dimension, is almost surely determined by the activation patterns in the hidden layers. We prove that the batch functional dimension is invariant to the symmetries of the network parameterization: neuron permutations and positive rescalings. Empirically, we establish that the batch functional dimension decreases during optimization. As a consequence, optimization l
    
[^81]: 百万长度视频和语言的环形注意力世界模型

    World Model on Million-Length Video And Language With RingAttention

    [https://arxiv.org/abs/2402.08268](https://arxiv.org/abs/2402.08268)

    该论文介绍了一个使用百万长度的视频和语言序列进行联合建模的环形注意力世界模型。该模型通过利用视频序列中的时间信息和语言的文本知识以及逐渐增加上下文大小的方法提高了AI辅助人类的能力。

    

    当前的语言模型在理解难以用文字描述的世界方面表现不佳，并且在处理复杂的长篇任务时遇到困难。视频序列提供了只有语言和静态图像所不具备的宝贵时间信息，因此它们在与语言进行联合建模时具有吸引力。这种模型可以对人类的文本知识和物理世界进行理解，为辅助人类提供更广泛的人工智能能力。然而，从百万个标记的视频和语言序列中学习面临着记忆约束、计算复杂性和数据有限性的挑战。为了应对这些挑战，我们策划了一个包含多样化视频和书籍的大型数据集，利用环形注意力技术对长序列进行可扩展的训练，逐渐增加上下文大小从4K到1M个标记。本文的贡献如下：

    Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language seq
    
[^82]: Distal Interference: 探索基于模型的持续学习的极限

    Distal Interference: Exploring the Limits of Model-Based Continual Learning

    [https://arxiv.org/abs/2402.08255](https://arxiv.org/abs/2402.08255)

    本研究探讨了持续学习中远距离干扰的极限问题，并提出了一种新型的可近似任何连续函数的反对称有界指数层B-spline ANN架构，用以解决灾难性干扰的挑战。

    

    持续学习是机器学习模型按顺序学习不同任务的过程。持续学习被称为灾难性干扰或遗忘的阻碍，即在学习新任务时快速遗忘之前学习的任务。尽管人工神经网络（ANNs）在实践中取得了成功，但它们容易受到灾难性干扰的影响。该研究分析了梯度下降和远距离输入点之间重叠表示如何导致远距离干扰和灾难性干扰。远距离干扰是指在对域的子集进行模型训练时，对域的其他子集造成非局部变化的现象。该研究表明，没有远距离干扰的均匀可训练模型必须具有指数级的规模。提出了一种名为ABEL-Spline的新型反对称有界指数层B-spline ANN架构，该架构可以近似任何连续函数，具有均匀可训练性、多项式计算复杂度。

    Continual learning is the sequential learning of different tasks by a machine learning model. Continual learning is known to be hindered by catastrophic interference or forgetting, i.e. rapid unlearning of earlier learned tasks when new tasks are learned. Despite their practical success, artificial neural networks (ANNs) are prone to catastrophic interference. This study analyses how gradient descent and overlapping representations between distant input points lead to distal interference and catastrophic interference. Distal interference refers to the phenomenon where training a model on a subset of the domain leads to non-local changes on other subsets of the domain. This study shows that uniformly trainable models without distal interference must be exponentially large. A novel antisymmetric bounded exponential layer B-spline ANN architecture named ABEL-Spline is proposed that can approximate any continuous function, is uniformly trainable, has polynomial computational complexity, an
    
[^83]: 无人机热像中基于深度学习的目标检测

    Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles

    [https://arxiv.org/abs/2402.08251](https://arxiv.org/abs/2402.08251)

    本文提出了一种基于深度学习的神经网络模型，用于在无人机收集的热像中识别小型和微小物体。通过使用YOLOv5结构的骨干网络、Transformer编码器和滑动窗口，以及Sigmoid函数进行检测，我们的模型在识别准确性上超过了其他最先进的方法。

    

    本文提出了一种神经网络模型，能够识别无人机收集的热像中的小型和微小物体。我们的模型由三个部分组成，即骨干网络，脖子和预测头。骨干网络基于YOLOv5的结构，并在末尾使用了一个Transformer编码器。脖子包括一个BI-FPN块，结合滑动窗口和Transformer，增加了输入到预测头的信息。预测头通过评估特征图使用Sigmoid函数进行检测。Transformer与注意力和滑动窗口的使用提高了识别准确性，并且使模型在嵌入式系统中保持了合理的参数和计算需求。在公共数据集VEDAI和我们收集的数据集上进行的实验表明，我们的模型比ResNet、Faster RCNN、ComNet、ViT、YOLOv5、SMPNe等最先进的方法具有更高的准确性。

    This work presents a neural network model capable of recognizing small and tiny objects in thermal images collected by unmanned aerial vehicles. Our model consists of three parts, the backbone, the neck, and the prediction head. The backbone is developed based on the structure of YOLOv5 combined with the use of a transformer encoder at the end. The neck includes a BI-FPN block combined with the use of a sliding window and a transformer to increase the information fed into the prediction head. The prediction head carries out the detection by evaluating feature maps with the Sigmoid function. The use of transformers with attention and sliding windows increases recognition accuracy while keeping the model at a reasonable number of parameters and computation requirements for embedded systems. Experiments conducted on public dataset VEDAI and our collected datasets show that our model has a higher accuracy than state-of-the-art methods such as ResNet, Faster RCNN, ComNet, ViT, YOLOv5, SMPNe
    
[^84]: APALU: 一种可训练、适应性激活函数用于深度学习网络

    APALU: A Trainable, Adaptive Activation Function for Deep Learning Networks

    [https://arxiv.org/abs/2402.08244](https://arxiv.org/abs/2402.08244)

    APALU 是一种可训练的激活函数，通过增强深度学习的学习性能，在适应复杂数据表示的同时保持稳定和高效。 在图像分类任务中，APALU相对于传统激活函数能够显著提高准确性。

    

    激活函数是深度学习的关键组成部分，有助于提取复杂的数据模式。虽然类似ReLU及其变种的经典激活函数被广泛应用，但它们的静态特性和简洁性，尽管有利，但通常限制了它们在特定任务中的有效性。可训练的激活函数有时也难以适应数据的独特特征。针对这些限制，我们引入了一种新颖的可训练激活函数，即自适应分段逼近激活线性单元（APALU），以增强深度学习在各种任务中的学习性能。它具有一套独特的特性，使其能够在学习过程中保持稳定和高效，并适应复杂的数据表示。实验证实，在不同任务中，与广泛使用的激活函数相比，APALU取得了显著的改进。在图像分类中，APALU提升了MobileNet和GoogleNet的准确性。

    Activation function is a pivotal component of deep learning, facilitating the extraction of intricate data patterns. While classical activation functions like ReLU and its variants are extensively utilized, their static nature and simplicity, despite being advantageous, often limit their effectiveness in specialized tasks. The trainable activation functions also struggle sometimes to adapt to the unique characteristics of the data. Addressing these limitations, we introduce a novel trainable activation function, adaptive piecewise approximated activation linear unit (APALU), to enhance the learning performance of deep learning across a broad range of tasks. It presents a unique set of features that enable it to maintain stability and efficiency in the learning process while adapting to complex data representations. Experiments reveal significant improvements over widely used activation functions for different tasks. In image classification, APALU increases MobileNet and GoogleNet accur
    
[^85]: 实现人工智能和机器人研发中的公平敏捷方法

    Towards Equitable Agile Research and Development of AI and Robotics

    [https://arxiv.org/abs/2402.08242](https://arxiv.org/abs/2402.08242)

    这项研究提出了一种将研发项目管理方法与公平能力结合的框架，以解决机器学习和人工智能方法中存在的偏见和歧视问题。

    

    机器学习（ML）和人工智能（AI）的方法往往会复制和放大现有的偏见和成见，AI机器人也是如此。例如，具备面部识别功能的机器人未能将黑人女性识别为人类，而其他机器人则仅根据外表将人们，如黑人男性，归类为罪犯。在“AI供应链”中，一种“模块化文化”意味着伤害被认为是“超出范围”的，或者是别人的责任。事件的发生是 routine enough（incidentdatabase.ai 列举了2000多个例子）以表明很少有组织能够完全尊重人们的权利；实现所声称的公平性、多样性和包容性（EDI或DEI）目标；或者识别然后解决这些组织和工件中的失败。我们提出了一个框架，用于调整广泛实践的研发项目管理方法，以建立组织的公平能力并更好地整合

    Machine Learning (ML) and 'Artificial Intelligence' ('AI') methods tend to replicate and amplify existing biases and prejudices, as do Robots with AI. For example, robots with facial recognition have failed to identify Black Women as human, while others have categorized people, such as Black Men, as criminals based on appearance alone. A 'culture of modularity' means harms are perceived as 'out of scope', or someone else's responsibility, throughout employment positions in the 'AI supply chain'. Incidents are routine enough (incidentdatabase.ai lists over 2000 examples) to indicate that few organizations are capable of completely respecting peoples' rights; meeting claimed equity, diversity, and inclusion (EDI or DEI) goals; or recognizing and then addressing such failures in their organizations and artifacts. We propose a framework for adapting widely practiced Research and Development (R&D) project management methodologies to build organizational equity capabilities and better integr
    
[^86]: 一个统计套利自编码器结构的端到端策略学习

    End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture

    [https://arxiv.org/abs/2402.08233](https://arxiv.org/abs/2402.08233)

    该论文研究了在统计套利中使用自编码器结构进行端到端策略学习的效果，通过嵌入自编码器网络到神经网络表示中，直接输出投资组合分配，并通过风险调整回报的反向传播进行训练。

    

    在统计套利中，经典的均值回归交易策略通常依赖于资产定价或基于PCA的模型来识别合成资产的均值。一旦找到这样一个（线性）模型，就可以制定一个独立的均值回归策略来生成交易信号。为了推广这种方法并使其真正数据驱动，我们研究了在统计套利中使用自编码器结构的效用。作为第一种方法，我们使用训练在美国股票回报上的标准自编码器来基于Ornstein-Uhlenbeck（OU）过程推导交易策略。为了进一步增强这个模型，我们采用了策略学习方法，并将自编码器网络嵌入一个投资组合交易策略空间的神经网络表示中。这种集成直接输出投资组合分配，并且可以通过神经策略的风险调整回报的反向传播进行端到端的训练。我们的研究结果表明，这种创新的端到端策略学习方法

    In Statistical Arbitrage (StatArb), classical mean reversion trading strategies typically hinge on asset-pricing or PCA based models to identify the mean of a synthetic asset. Once such a (linear) model is identified, a separate mean reversion strategy is then devised to generate a trading signal. With a view of generalising such an approach and turning it truly data-driven, we study the utility of Autoencoder architectures in StatArb. As a first approach, we employ a standard Autoencoder trained on US stock returns to derive trading strategies based on the Ornstein-Uhlenbeck (OU) process. To further enhance this model, we take a policy-learning approach and embed the Autoencoder network into a neural network representation of a space of portfolio trading policies. This integration outputs portfolio allocations directly and is end-to-end trainable by backpropagation of the risk-adjusted returns of the neural policy. Our findings demonstrate that this innovative end-to-end policy learni
    
[^87]: 非目标干预下的因果发现

    Causal Discovery under Off-Target Interventions

    [https://arxiv.org/abs/2402.08229](https://arxiv.org/abs/2402.08229)

    本文研究了非目标干预下的因果发现问题，提出了一个随机干预模型来尽量减少干预次数。通过验证和搜索两个基本问题，提供了多对数复杂度的近似算法。

    

    因果图发现是一个在各个学科中具有重要应用的问题。然而，仅凭观察数据，只能恢复到其马尔可夫等价类的潜在因果图，并且需要进一步的假设或干预来缩小真实图的范围。本研究解决了在随机干预设置下的因果发现问题，目标是尽量减少干预次数。我们提出了以下随机干预模型，它包含了现有文献中的自适应无噪声干预，并能捕捉到脂肪手干预和CRISPR基因敲除等情况：任何干预尝试都会导致对一个随机顶点子集的实际干预，这个子集的选择是依赖于尝试的动作的分布。在这个模型下，我们研究了因果发现中的验证和搜索两个基本问题，并提供了具有多对数复杂度的近似算法。

    Causal graph discovery is a significant problem with applications across various disciplines. However, with observational data alone, the underlying causal graph can only be recovered up to its Markov equivalence class, and further assumptions or interventions are necessary to narrow down the true graph. This work addresses the causal discovery problem under the setting of stochastic interventions with the natural goal of minimizing the number of interventions performed. We propose the following stochastic intervention model which subsumes existing adaptive noiseless interventions in the literature while capturing scenarios such as fat-hand interventions and CRISPR gene knockouts: any intervention attempt results in an actual intervention on a random subset of vertices, drawn from a distribution dependent on attempted action. Under this model, we study the two fundamental problems in causal discovery of verification and search and provide approximation algorithms with polylogarithmic c
    
[^88]: 研究GNN的超分布推广：从架构角度的视角

    Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective

    [https://arxiv.org/abs/2402.08228](https://arxiv.org/abs/2402.08228)

    这项研究从架构的角度全面调查了图的超分布推广，揭示了图自我注意机制和其他常见构建模块在超分布问题上的影响。

    

    图神经网络（GNN）在测试数据来自于训练数据相同分布的假设下表现出了出色的性能。然而，在真实场景中，这个假设可能并不总是成立。因此，在图的上下文中，对超分布（OOD）问题的探索日益受到关注。大部分现有的研究主要集中在改进图的OOD推广的两个“模型无关”角度上：数据驱动方法和策略学习。然而，对于已知的GNN模型架构对图的OOD推广的影响的研究相对较少，这与现有的研究相互独立。在这项工作中，我们从架构的角度首次全面调查了图的OOD推广，并对现代GNN的常见构建模块进行了考察。通过大量实验，我们揭示了图自我注意机制和...

    Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism an
    
[^89]: 用上下文重写提高黑盒模型的鲁棒性

    Improving Black-box Robustness with In-Context Rewriting

    [https://arxiv.org/abs/2402.08225](https://arxiv.org/abs/2402.08225)

    本文提出了一种名为LLM-TTA的方法，通过使用LLM生成的增强作为测试时间增强（TTA）的增强函数，提高了黑盒模型的鲁棒性。在BERT和T5模型的情感、毒性和新闻分类任务中，LLM-TTA优于传统的增强函数，使BERT的分布外鲁棒性平均提高了4.30个百分点，而不降低分布内性能。

    

    机器学习模型在分布内（ID）数据上表现优秀，但在未见过的分布外（OOD）输入上表现困难。大多数提高OOD鲁棒性的技术在模型是黑盒的情况下不适用，例如权重被冻结，重新训练成本高，或者通过API使用模型。测试时间增强（TTA）是一种简单的事后技术，通过对测试输入的多个增强进行预测聚合来绕过黑盒约束来提高鲁棒性。由于生成有效的自然语言增强的挑战，TTA在自然语言处理中的应用受到限制。在这项研究中，我们提出了LLM-TTA，它使用LLM生成的增强作为TTA的增强函数。LLM-TTA在BERT和T5模型的情感、毒性和新闻分类任务中优于传统的增强函数，BERT的OOD鲁棒性提高了平均4.30个百分点而不会减退平均ID pe。

    Machine learning models often excel on in-distribution (ID) data but struggle with unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD robustness are not applicable to settings where the model is effectively a black box, such as when the weights are frozen, retraining is costly, or the model is leveraged via an API. Test-time augmentation (TTA) is a simple post-hoc technique for improving robustness that sidesteps black-box constraints by aggregating predictions across multiple augmentations of the test input. TTA has seen limited use in NLP due to the challenge of generating effective natural language augmentations. In this work, we propose LLM-TTA, which uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks for BERT and T5 models, with BERT's OOD robustness improving by an average of 4.30 percentage points without regressing average ID pe
    
[^90]: BBox-Adapter: 轻量级适配黑盒大型语言模型

    BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models

    [https://arxiv.org/abs/2402.08219](https://arxiv.org/abs/2402.08219)

    BBox-Adapter是一种适用于黑盒大型语言模型的轻量级适配器，通过区分目标和源域数据，并采用排名式噪音对比估计（NCE）损失和在线适应机制，实现了在透明、隐私和成本方面的有效适应。

    

    适应最先进的大型语言模型（LLMs），如GPT-4和Gemini，以满足特定任务的要求是具有挑战性的。由于它们的参数、嵌入和输出概率的不透明性，现有的微调适应方法是不适用的。因此，只能通过它们的API服务适应这些黑盒LLMs，这引发了透明度、隐私和成本的担忧。为了解决这些挑战，我们介绍了BBox-Adapter，一种新颖的适用于黑盒LLMs的轻量级适配器。BBox-Adapter通过将目标数据视为正样本，将源数据视为负样本来区分目标和源域数据。它采用基于排名的噪音对比估计（NCE）损失来提高目标域数据的可能性，同时惩罚源域数据的可能性。此外，它还具有在线适应机制，该机制将来自真实数据、人类或AI反馈的实时正样本采样与先前适应的负样本数据相结合。广泛的实验表明，BBox-Adapter在不降低性能的同时，提供了高效而灵活的黑盒LLMs适应解决方案。

    Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive ex
    
[^91]: 量子计算增强的算法揭示了KRAS的新抑制剂

    Quantum Computing-Enhanced Algorithm Unveils Novel Inhibitors for KRAS

    [https://arxiv.org/abs/2402.08210](https://arxiv.org/abs/2402.08210)

    我们通过将量子算法的计算能力与经典方法的可靠性相结合，提出了一个量子-经典生成模型，用于设计癌症治疗中的关键分子靶点KRAS的新的抑制剂。在研究中合成了15种有希望的分子，并通过实验测试验证了其中两种分子的有效结合能力。

    

    发现具有治疗潜力的小分子一直是化学和生物学中的长期挑战。研究人员越来越多地利用新颖的计算技术来简化药物开发流程，提高药物的发现率，并降低将药物推向市场的成本。为此，我们引入了一个将16比特IBM量子计算机上的量子算法的计算能力与设计小分子的传统可靠的经典方法无缝集成的量子-经典生成模型。我们的混合生成模型被应用于设计新的KRAS抑制剂，这是癌症治疗中的一个重要靶点。在我们的研究中，我们合成了15种有希望的分子，并对它们进行了实验测试，以评估它们与靶点的结合能力。值得注意的是，在候选分子中，两种具有独特骨架的分子，ISM061-018-2和ISM061-22，通过展示有效的与靶点的结合能力而脱颖而出。

    The discovery of small molecules with therapeutic potential is a long-standing challenge in chemistry and biology. Researchers have increasingly leveraged novel computational techniques to streamline the drug development process to increase hit rates and reduce the costs associated with bringing a drug to market. To this end, we introduce a quantum-classical generative model that seamlessly integrates the computational power of quantum algorithms trained on a 16-qubit IBM quantum computer with the established reliability of classical methods for designing small molecules. Our hybrid generative model was applied to designing new KRAS inhibitors, a crucial target in cancer therapy. We synthesized 15 promising molecules during our investigation and subjected them to experimental testing to assess their ability to engage with the target. Notably, among these candidates, two molecules, ISM061-018-2 and ISM061-22, each featuring unique scaffolds, stood out by demonstrating effective engageme
    
[^92]: 利用多臂赌博机的阈值数据Shapley进行数据清洗

    Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits

    [https://arxiv.org/abs/2402.08209](https://arxiv.org/abs/2402.08209)

    本文提出了一种利用阈值赌博机算法快速识别具有低数据Shapley值的实例子集的迭代方法，从而提高数据清洗的计算速度，同时保持模型性能。

    

    数据清洗旨在通过从训练数据集中删除一组有害实例来提高模型性能。数据Shapley是一种常见的理论上保证的方法，用于评估每个实例对模型性能的贡献；然而，它需要在训练数据的所有子集上进行训练，这在计算上是昂贵的。在本文中，我们提出了一种使用阈值赌博机算法快速识别具有低数据Shapley值的实例子集的迭代方法。我们提供了一个理论保证，即如果进行足够多的迭代，所提出的方法可以准确选择有害实例。使用不同模型和数据集的实证评估表明，所提出的方法在保持模型性能的同时有效地提高了计算速度。

    Data cleansing aims to improve model performance by removing a set of harmful instances from the training dataset. Data Shapley is a common theoretically guaranteed method to evaluate the contribution of each instance to model performance; however, it requires training on all subsets of the training data, which is computationally expensive. In this paper, we propose an iterativemethod to fast identify a subset of instances with low data Shapley values by using the thresholding bandit algorithm. We provide a theoretical guarantee that the proposed method can accurately select harmful instances if a sufficiently large number of iterations is conducted. Empirical evaluation using various models and datasets demonstrated that the proposed method efficiently improved the computational speed while maintaining the model performance.
    
[^93]: 在分类中应对歧视：基于核空间中边缘少数群体的SMOTE算法

    Confronting Discrimination in Classification: Smote Based on Marginalized Minorities in the Kernel Space for Imbalanced Data

    [https://arxiv.org/abs/2402.08202](https://arxiv.org/abs/2402.08202)

    该论文提出了一种基于决策边界和样本近邻关系的新型分类过采样方法，旨在解决金融欺诈检测中因类别不平衡而导致的歧视问题。

    

    金融欺诈检测面临着类别不平衡的典型挑战，即欺诈案例非常罕见，但如果误标识可能导致不可预测的经济损失。在分类中精确地对这些关键的少数样本进行分类是一项具有挑战性的任务。主要困难来自于主流分类器在评估指标上对少数样本常常表现出“隐性歧视”，导致频繁的错误分类，问题的关键在于多数样本和少数样本之间的特征空间重叠。为了解决这些挑战，过采样是一个可行的解决方案，然而目前的经典过采样方法在样本选择上常常缺乏必要的谨慎，加剧了特征空间的重叠。为此，我们提出了一种基于决策边界和样本近邻关系的新型分类过采样方法。

    Financial fraud detection poses a typical challenge characterized by class imbalance, where instances of fraud are extremely rare but can lead to unpredictable economic losses if misidentified. Precisely classifying these critical minority samples represents a challenging task within the classification. The primary difficulty arises from mainstream classifiers, which often exhibit "implicit discrimination" against minority samples in evaluation metrics, which results in frequent misclassifications, and the key to the problem lies in the overlap of feature spaces between majority and minority samples. To address these challenges, oversampling is a feasible solution, yet current classical oversampling methods often lack the necessary caution in sample selection, exacerbating feature space overlap. In response, we propose a novel classification oversampling approach based on the decision boundary and sample proximity relationships. This method carefully considers the distance between crit
    
[^94]: 弱分布重叠下马尔可夫决策过程中的离策略评估

    Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap

    [https://arxiv.org/abs/2402.08201](https://arxiv.org/abs/2402.08201)

    本文研究了弱分布重叠下马尔可夫决策过程中的离策略评估问题，并提出了一种截断双重稳健（TDR）估计器，在这种情况下表现良好。

    

    在马尔可夫决策过程（MDP）中，双重稳健方法在序列可忽略性下对离策略评估具有很大的潜力：它们已经证明了随着时长T的收敛速度为$1/\sqrt{T}$，在大样本中具有统计效率，并且可以通过标准强化学习技术执行预估任务，具有模块化实现的能力。然而，现有结果在很大程度上使用了强分布重叠假设，即目标政策和数据收集政策的稳态分布相差在有限因子内，而这个假设通常只在MDP的状态空间有界时才可信。在本文中，我们重新审视了在弱分布重叠概念下的MDP离策略评估任务，并引入了一类截断双重稳健（TDR）估计器，在这种情况下表现良好。当目标和数据收集的分布比率有界时，我们证明了这些估计器的一致性。

    Doubly robust methods hold considerable promise for off-policy evaluation in Markov decision processes (MDPs) under sequential ignorability: They have been shown to converge as $1/\sqrt{T}$ with the horizon $T$, to be statistically efficient in large samples, and to allow for modular implementation where preliminary estimation tasks can be executed using standard reinforcement learning techniques. Existing results, however, make heavy use of a strong distributional overlap assumption whereby the stationary distributions of the target policy and the data-collection policy are within a bounded factor of each other -- and this assumption is typically only credible when the state space of the MDP is bounded. In this paper, we re-visit the task of off-policy evaluation in MDPs under a weaker notion of distributional overlap, and introduce a class of truncated doubly robust (TDR) estimators which we find to perform well in this setting. When the distribution ratio of the target and data-coll
    
[^95]: PSC-CPI: 多尺度蛋白质序列-结构对比用于高效且具有可推广性的化合物-蛋白质相互作用预测

    PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and Generalizable Compound-Protein Interaction Prediction

    [https://arxiv.org/abs/2402.08198](https://arxiv.org/abs/2402.08198)

    PSC-CPI是一种多尺度蛋白质序列-结构对比框架，通过内模态和跨模态对比捕获蛋白质序列和结构之间的依赖关系，用于高效且具有可推广性的化合物-蛋白质相互作用预测。

    

    化合物-蛋白质相互作用（CPI）预测旨在预测化合物-蛋白质相互作用的模式和强度，以用于理性药物发现。现有的基于深度学习的方法仅利用蛋白质序列或结构的单一模态，缺乏对两个模态的联合分布进行共同建模，这可能导致在复杂的实际场景中由于各种因素（例如，模态丢失和领域转移）而出现显著的性能下降。更重要的是，这些方法仅以单一固定尺度模拟蛋白质序列和结构，忽视了更精细的多尺度信息，例如嵌入在关键蛋白质片段中的信息。在本文中，我们提出了一种新颖的多尺度蛋白质序列-结构对比框架用于CPI预测（PSC-CPI），通过内模态和跨模态对比捕获蛋白质序列和结构之间的依赖关系。我们还应用可变长度的蛋白质增强技术，以允许对不同长度的蛋白质进行建模。

    Compound-Protein Interaction (CPI) prediction aims to predict the pattern and strength of compound-protein interactions for rational drug discovery. Existing deep learning-based methods utilize only the single modality of protein sequences or structures and lack the co-modeling of the joint distribution of the two modalities, which may lead to significant performance drops in complex real-world scenarios due to various factors, e.g., modality missing and domain shifting. More importantly, these methods only model protein sequences and structures at a single fixed scale, neglecting more fine-grained multi-scale information, such as those embedded in key protein fragments. In this paper, we propose a novel multi-scale Protein Sequence-structure Contrasting framework for CPI prediction (PSC-CPI), which captures the dependencies between protein sequences and structures through both intra-modality and cross-modality contrasting. We further apply length-variable protein augmentation to allow
    
[^96]: 高斯模型集成置信传播用于高维系统中的高效推断

    Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems

    [https://arxiv.org/abs/2402.08193](https://arxiv.org/abs/2402.08193)

    高斯模型集成置信传播算法（GEnBP）是一种用于高维系统中高效推断的方法，通过集成卡尔曼滤波器和高斯置信传播等技术相结合，能有效处理高维状态、参数和复杂的依赖结构。

    

    高维模型中的高效推断仍然是机器学习中的一个核心挑战。本文介绍了一种名为高斯模型集成置信传播（GEnBP）算法的方法，该方法是集成卡尔曼滤波器和高斯置信传播（GaBP）方法的结合。GEnBP通过在图模型结构中传递低秩本地信息来更新集成模型。这种组合继承了每种方法的有利特性。集成技术使得GEnBP能够处理高维状态、参数和复杂的、嘈杂的黑箱生成过程。在图模型结构中使用本地信息确保了该方法适用于分布式计算，并能高效地处理复杂的依赖结构。当集成大小远小于推断维度时，GEnBP特别有优势。这种情况在空时建模、图像处理和物理模型反演等领域经常出现。GEnBP可以应用于一般性问题。

    Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem s
    
[^97]: THE COLOSSEUM：用于评估机器人操作泛化性的基准测试

    THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation

    [https://arxiv.org/abs/2402.08191](https://arxiv.org/abs/2402.08191)

    THE COLOSSEUM是一个新的模拟基准测试，用于评估机器人操作的泛化性能。它包括20个不同的操作任务，在12个环境干扰轴上进行系统评估。研究发现，四个最先进的操作模型在干扰因素下的成功率下降了30-50%。改变干扰对象数量、目标对象颜色或光照条件会对模型的性能产生重要影响。

    

    为了实现有效的大规模、现实世界的机器人应用，我们必须评估我们的机器人策略在环境条件变化时的适应能力。不幸的是，大多数研究评估机器人在与训练设置非常相似甚至相同的环境中的性能。我们提出了一个新颖的模拟基准测试THE COLOSSEUM，其中包括20个不同的操作任务，可以对模型在12个环境干扰轴上进行系统评估。这些干扰包括物体、桌面和背景的颜色、纹理和大小的变化；我们还改变了光照、干扰因素和相机姿态。使用THE COLOSSEUM，我们比较了4个最先进的操作模型，发现它们的成功率在这些干扰因素下下降了30-50%。当多个干扰同时应用时，成功率下降至≥75%。我们确定了改变干扰对象数量、目标对象颜色或光照条件会对模型的性能产生重要影响。

    To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 12 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades $\geq$75%. We identify that changing the number of distractor objects, target object color, or lighting conditions ar
    
[^98]: 通过图神经网络和深度操作网络学习时间依赖的偏微分方程，以在不规则网格上实现鲁棒准确性

    Learning time-dependent PDE via graph neural networks and deep operator network for robust accuracy on irregular grids

    [https://arxiv.org/abs/2402.08187](https://arxiv.org/abs/2402.08187)

    本文提出了一种基于图神经网络的自回归模型GraphDeepONet，用于学习时间依赖的偏微分方程，并在不规则网格上实现了鲁棒的准确性。

    

    近年来，使用深度学习进行科学计算取得了重大进展。学习从偏微分方程（PDE）的参数到相应解的算子的模型引起了越来越多的关注。DeepONet和Fourier神经算子等模型被设计为适用于将函数作为输入和输出处理的结构，以实现作为解算子的替代模型的实时预测。基于图神经网络（GNNs）的替代模型研究在处理时间相关的PDE方面也取得了显著进展。在本文中，我们提出了一种基于GNN的自回归模型GraphDeepONet，以有效地适应成功的算子学习模型DeepONet。与现有的基于GNN的PDE求解器模型相比，GraphDeepONet在预测解方面展现出了鲁棒的准确性。在不规则网格上，它保持着一致的性能表现。

    Scientific computing using deep learning has seen significant advancements in recent years. There has been growing interest in models that learn the operator from the parameters of a partial differential equation (PDE) to the corresponding solutions. Deep Operator Network (DeepONet) and Fourier Neural operator, among other models, have been designed with structures suitable for handling functions as inputs and outputs, enabling real-time predictions as surrogate models for solution operators. There has also been significant progress in the research on surrogate models based on graph neural networks (GNNs), specifically targeting the dynamics in time-dependent PDEs. In this paper, we propose GraphDeepONet, an autoregressive model based on GNNs, to effectively adapt DeepONet, which is well-known for successful operator learning. GraphDeepONet exhibits robust accuracy in predicting solutions compared to existing GNN-based PDE solver models. It maintains consistent performance even on irre
    
[^99]: 通过场景无关表示实现多智能体转移强化学习

    Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation

    [https://arxiv.org/abs/2402.08184](https://arxiv.org/abs/2402.08184)

    本研究引入了一种新的框架，通过将各种状态空间统一为固定大小的输入，实现了在多智能体系统中进行转移学习的能力。在SMAC环境中的实验结果表明，通过学习机动技能获得的知识可以显著提高多智能体的学习性能。

    

    多智能体强化学习（MARL）算法广泛应用于解决在动态的多智能体系统中需要合作和竞争的复杂任务。然而，从头开始学习这种任务是困难且可能不可行的，尤其对于具有大量交互智能体的多智能体系统而言，由于样本复杂性极高。因此，重复使用过去经验或其他智能体获得的知识可以有效加快学习过程并提升MARL算法。在本研究中，我们介绍了一种新颖的框架，通过将各种状态空间统一为固定大小的输入，实现了MARL的转移学习，从而在多智能体系统的不同场景中实现了可行的统一深度学习策略。我们在StarCraft多智能体挑战（SMAC）环境中评估了我们的方法，并发现使用通过机动技能学习得到的知识的多智能体学习性能大幅提升。

    Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity. Therefore, reusing knowledge gained from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms. In this study, we introduce a novel framework that enables transfer learning for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS. We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned fr
    
[^100]: 变分连续测试时适应性

    Variational Continual Test-Time Adaptation

    [https://arxiv.org/abs/2402.08182](https://arxiv.org/abs/2402.08182)

    本文介绍了VCoTTA，一种变分贝叶斯方法用于测量连续测试时适应性中的不确定性。采用变分预热策略将预训练的模型转为贝叶斯神经网络，在测试时通过均值教师更新策略来更新学生模型，结合源模型和教师模型的先验。实验证明该方法在减轻先验偏移方面有效。

    

    先验偏移在只使用无标签测试数据的连续测试时适应性（CTTA）方法中至关重要，因为它可能导致严重的误差传播。在本文中，我们介绍了VCoTTA，一种用于测量CTTA中不确定性的变分贝叶斯方法。在源阶段，我们通过变分预热策略将预训练的确定性模型转化为贝叶斯神经网络（BNN），将不确定性注入模型中。在测试时，我们采用变分推断的均值教师更新策略，将学生模型和指数移动平均法用于教师模型。我们的新方法通过结合源模型和教师模型的先验来更新学生模型。证据下界被制定为学生模型和教师模型之间的交叉熵，以及先验混合的Kullback-Leibler（KL）散度。在三个数据集上的实验结果表明该方法在减轻在CTTA中的先验偏移方面的有效性。

    The prior drift is crucial in Continual Test-Time Adaptation (CTTA) methods that only use unlabeled test data, as it can cause significant error propagation. In this paper, we introduce VCoTTA, a variational Bayesian approach to measure uncertainties in CTTA. At the source stage, we transform a pre-trained deterministic model into a Bayesian Neural Network (BNN) via a variational warm-up strategy, injecting uncertainties into the model. During the testing time, we employ a mean-teacher update strategy using variational inference for the student model and exponential moving average for the teacher model. Our novel approach updates the student model by combining priors from both the source and teacher models. The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method's effectiveness in mitigating prior drift within th
    
[^101]: 在线结构化预测与Fenchel-Young损失和改进的替代后悔度用于在线多类分类与逻辑损失的研究

    Online Structured Prediction with Fenchel--Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss

    [https://arxiv.org/abs/2402.08180](https://arxiv.org/abs/2402.08180)

    这项研究扩展了在线结构化预测的替代后悔度界限，通过引入Fenchel-Young损失和随机解码方案，使得在在线多类分类和逻辑损失下获得了更好的结果。

    

    本文研究了具有完全信息反馈的在线结构化预测。对于在线多类分类，van der Hoeven(2020)通过引入一个优美的“利用替代间隙”的框架，获得了与时间范围无关的替代后悔度界限，即有限的界限。然而，这个框架主要限于多类分类，因为它依赖于一种特定于分类的过程，将估计得分转化为输出。我们将“利用替代间隙”框架扩展到具有“Fenchel-Young损失”的在线结构化预测中，这是一大类包括多类分类的逻辑损失在内的替代损失，获得了在各种结构化预测问题上的有限替代后悔度界限。为此，我们提出并分析了随机解码，将估计得分转化为一般的结构化输出。此外，通过将我们的解码应用于在线多类分类与逻辑损失，我们获得了改进的替代后悔度界限。

    This paper studies online structured prediction with full-information feedback. For online multiclass classification, van der Hoeven (2020) has obtained surrogate regret bounds independent of the time horizon, or \emph{finite}, by introducing an elegant \emph{exploit-the-surrogate-gap} framework. However, this framework has been limited to multiclass classification primarily because it relies on a classification-specific procedure for converting estimated scores to outputs. We extend the exploit-the-surrogate-gap framework to online structured prediction with \emph{Fenchel--Young losses}, a large family of surrogate losses including the logistic loss for multiclass classification, obtaining finite surrogate regret bounds in various structured prediction problems. To this end, we propose and analyze \emph{randomized decoding}, which converts estimated scores to general structured outputs. Moreover, by applying our decoding to online multiclass classification with the logistic loss, we o
    
[^102]: 使用地标和聚类的层级位置嵌入图形用于链接预测

    Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction

    [https://arxiv.org/abs/2402.08174](https://arxiv.org/abs/2402.08174)

    本论文提出了一种使用地标和聚类的层级位置嵌入方法用于链接预测任务。通过选择具有高度中心度的节点作为地标和进行图聚类，本方法有效地将位置信息嵌入到图中，提高了链接预测的准确性和性能。

    

    学习图中节点的位置信息对于链接预测任务非常重要。我们提出了使用代表性节点（称为地标）来表示位置信息的方法。我们选择少量具有高度中心度的节点作为地标，它们作为节点位置的参考点。我们证明了这种选择策略对于众所周知的随机图模型是合理的，并推导出涉及地标的平均路径长度的闭合形式上界。在幂律图的模型中，我们证明了地标为节点之间距离提供了渐近完全准确的信息。我们将理论洞察力应用于实际网络，并提出了具有地标和聚类的层级位置嵌入（HPLC）方法。HPLC将地标选择和图聚类相结合，其中图被分割为连通密集的聚类，选择具有最高度中心度的节点作为地标。HPLC利用了基于地标的节点位置信息的层级性。

    Learning positional information of nodes in a graph is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions. We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC). HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various l
    
[^103]: LLaGA: 大型语言和图形助手

    LLaGA: Large Language and Graph Assistant

    [https://arxiv.org/abs/2402.08170](https://arxiv.org/abs/2402.08170)

    LLaGA是一个创新的模型，它有效地整合了大型语言模型（LLM）的能力，以处理图结构数据的复杂性。通过重新组织图节点以作为结构感知的序列，并通过一个多功能投影仪将其映射到标记嵌入空间中，LLaGA在多样性、泛化性和可解释性方面表现出色。

    

    图神经网络（GNN）已经推动了图结构数据分析的进步。最近，大型语言模型（LLM）如GPT-4的崛起预示着深度学习的一个新时代。然而，将它们应用于图数据还面临着独特的挑战，由于将图结构转化为文本的固有难度。为此，我们引入了一个创新模型——大型语言和图形助手（LLaGA），它有效地整合了LLM的能力，以处理图结构数据的复杂性。LLaGA保留了LLM的通用性，同时将图数据转化为与LLM输入兼容的格式。LLaGA通过重新组织图节点以作为结构感知的序列，然后通过一个多功能投影仪将其映射到标记嵌入空间中。LLaGA在多样性、泛化性和可解释性方面表现出色，使其能够在不同数据集和任务上表现出一致的良好性能。

    Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and 
    
[^104]: 关于Transformer架构的限制

    On Limitations of the Transformer Architecture

    [https://arxiv.org/abs/2402.08164](https://arxiv.org/abs/2402.08164)

    本论文通过通信复杂性证明了Transformer层在处理函数组合任务时的局限性，指出对于大型定义域和某些数学任务，Transformers可能无法解决。

    

    大型语言模型（LLMs）中幻觉的根本原因是什么？我们使用通信复杂性来证明，如果函数的定义域足够大，Transformer层无法组合函数（例如，在家谱中查找一个人的祖父）；我们通过示例显示，当定义域相当小的时候，这种能力的缺乏已经在经验上存在。我们还指出，许多在所谓的组合任务中的数学任务，认为它们对LLMs来说很难解决，对于足够大的实例来说，且假设计算复杂性领域的某些被广泛接受的猜想是正确的，Transformers也不太可能解决。

    What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.
    
[^105]: 渐变流自适应重要性抽样用于sigmoid分类模型的贝叶斯留一交叉验证

    Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models

    [https://arxiv.org/abs/2402.08151](https://arxiv.org/abs/2402.08151)

    本研究引入了渐变流自适应重要性抽样的方法，用于稳定贝叶斯分类模型的留一交叉验证预测的蒙特卡罗近似，以评估模型的普适性。

    

    我们引入了一组梯度流引导的自适应重要性抽样（IS）变换，用于稳定贝叶斯分类模型的点级留一交叉验证（LOO）预测的蒙特卡罗近似。可以利用这种方法来评估模型的普适性，例如计算与AIC类似的LOO或计算LOO ROC / PRC曲线以及派生的度量指标，如AUROC和AUPRC。通过变分法和梯度流，我们推导出两个简单的非线性单步变换，利用梯度信息将模型的预训练完整数据后验靠近目标LOO后验预测分布。这样，变换稳定了重要性权重。因为变换涉及到似然函数的梯度，所以结果的蒙特卡罗积分依赖于模型Hessian的Jacobian行列式。我们推导出了这些Jacobian行列式的闭合精确公式。

    We introduce a set of gradient-flow-guided adaptive importance sampling (IS) transformations to stabilize Monte-Carlo approximations of point-wise leave one out cross-validated (LOO) predictions for Bayesian classification models. One can leverage this methodology for assessing model generalizability by for instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves and derived metrics like the AUROC and AUPRC. By the calculus of variations and gradient flow, we derive two simple nonlinear single-step transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. Because the transformations involve the gradient of the likelihood function, the resulting Monte Carlo integral depends on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in
    
[^106]: 使用大型语言模型和蒙特卡洛树搜索进行验证的多步合成

    Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search

    [https://arxiv.org/abs/2402.08147](https://arxiv.org/abs/2402.08147)

    本文提出了一种使用蒙特卡洛树搜索引导大型语言模型生成验证程序的方法，通过结合验证器的反馈和LLM先验知识提高了合成能力，实验证明这种方法在一组验证编程问题上的性能优于基本模型和具有插件的ChatGPT4。

    

    我们提出了一种使用蒙特卡洛树搜索（MCTS）引导大型语言模型（LLMs）生成在Dafny、Lean和Coq中验证的程序的方法。我们的方法被称为VMCTS，通过在每个步骤检查部分程序来利用搜索算法中的验证器。结合LLM先验知识，验证器的反馈提高了开源模型的合成能力。在一组五个经过验证的编程问题中，我们发现在四个问题中，即使重新对解决方案进行一小时的重新采样，基本模型无法解决问题，而VMCTS可以在6分钟内解决这些问题。在这些问题上，基本模型加上VMCTS甚至与具有插件和多次重试的ChatGPT4竞争力相当。我们的代码和基准测试结果可在https://github.com/namin/llm-verified-with-monte-carlo-tree-search找到。

    We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq. Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step. In combination with the LLM prior, the verifier feedback raises the synthesis capabilities of open source models. On a set of five verified programming problems, we find that in four problems where the base model cannot solve the question even when re-sampling solutions for one hour, VMCTS can solve the problems within 6 minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented with plugins and multiple re-tries on these problems. Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search .
    
[^107]: 对称非负矩阵分解的随机算法

    Randomized Algorithms for Symmetric Nonnegative Matrix Factorization

    [https://arxiv.org/abs/2402.08134](https://arxiv.org/abs/2402.08134)

    本论文提出了两种随机算法来更快、更可扩展地计算对称非负矩阵分解，其中一种使用随机矩阵草图来计算初始低秩输入矩阵，另一种使用随机杠杆得分采样来近似解决约束最小二乘问题。这些方法在大规模真实世界的图聚类任务上取得了良好的效果。

    

    对称非负矩阵分解（SymNMF）是数据分析和机器学习中一种将对称矩阵近似表示为非负、低秩矩阵及其转置的技术。为了设计更快、更可扩展的SymNMF算法，我们开发了两种随机算法来进行计算。第一种算法使用随机矩阵草图计算初始的低秩输入矩阵，并利用该输入迅速计算SymNMF。第二种算法使用随机杠杆得分采样来近似解决约束最小二乘问题。许多成功的SymNMF方法依赖于（近似）解决一系列约束最小二乘问题。我们在理论上证明了杠杆得分采样可以以高概率近似解决非负最小二乘问题，达到所选精度。最后，我们通过将这两种方法应用于大规模真实世界的图聚类任务中，证明了它们的有效性。

    Symmetric Nonnegative Matrix Factorization (SymNMF) is a technique in data analysis and machine learning that approximates a symmetric matrix with a product of a nonnegative, low-rank matrix and its transpose. To design faster and more scalable algorithms for SymNMF we develop two randomized algorithms for its computation. The first algorithm uses randomized matrix sketching to compute an initial low-rank input matrix and proceeds to use this input to rapidly compute a SymNMF. The second algorithm uses randomized leverage score sampling to approximately solve constrained least squares problems. Many successful methods for SymNMF rely on (approximately) solving sequences of constrained least squares problems. We prove theoretically that leverage score sampling can approximately solve nonnegative least squares problems to a chosen accuracy with high probability. Finally we demonstrate that both methods work well in practice by applying them to graph clustering tasks on large real world d
    
[^108]: 《关于循环模型在长序列中的复兴：在Transformer时代的调研和研究机会》

    On the Resurgence of Recurrent Models for Long Sequences: Survey and Research Opportunities in the Transformer Era

    [https://arxiv.org/abs/2402.08132](https://arxiv.org/abs/2402.08132)

    这项调研总结了在处理长序列数据方面，循环模型的复兴和与Transformer模型相结合的新型神经模型的发展，以及深度空间状态模型作为时间函数逼近的方法的出现。

    

    机器学习领域长期以来的一个挑战是开发可以处理和学习非常长的数据序列的模型。基于Transformer的网络（例如大型语言模型）的出色结果推动了并行注意力的概念，将经典的顺序处理的循环模型的作用掩盖起来。然而，过去几年中，一些研究人员对自注意力的二次复杂度表示关注，提出了一系列兼顾Transformer和循环网络两个世界优势的新型神经模型。同时，深度空间状态模型作为时间函数逼近的强大方法出现，从而为从序列数据中学习开辟了新的视角，许多领域的研究者对此感兴趣并利用它来实现一类特殊的（线性）循环神经网络。本调研旨在概述这些趋势。

    A longstanding challenge for the Machine Learning community is the one of developing models that are capable of processing and learning from very long sequences of data. The outstanding results of Transformers-based networks (e.g., Large Language Models) promotes the idea of parallel attention as the key to succeed in such a challenge, obfuscating the role of classic sequential processing of Recurrent Models. However, in the last few years, researchers who were concerned by the quadratic complexity of self-attention have been proposing a novel wave of neural models, which gets the best from the two worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State Models emerged as robust approaches to function approximation over time, thus opening a new perspective in learning from sequential data, followed by many people in the field and exploited to implement a special class of (linear) Recurrent Neural Networks. This survey is aimed at providing an overview of these trends 
    
[^109]: 无信息反馈图的高效上下文赌率

    Efficient Contextual Bandits with Uninformed Feedback Graphs

    [https://arxiv.org/abs/2402.08127](https://arxiv.org/abs/2402.08127)

    本文提出了第一个针对无信息设置的上下文算法，通过在线回归降低效率和最优化算法，关键是学习使用对数损失的图形以获得有利的后悔保证。

    

    具有反馈图的赌率是强大的在线学习模型，可以插值完整信息和经典赌率问题，捕捉许多现实应用。最近，张等人（2023年）研究了这个问题的上下文版本，并提出了一种通过在线回归降低效率和最优化算法。然而，他们的算法在每个决策之前都关键依赖于看到反馈图，而在许多应用中，反馈图是未知的，意味着它只在学习者做出决策后才被揭示，甚至从未完全揭示。本文通过高效地将在线回归应用于损失和图形，为这种未知设置开发了首个上下文算法。重要的是，我们表明使用对数损失学习图形是关键的，以获得有利的懊悔保证。我们还在一系列实验中展示了我们算法的实证效果。

    Bandits with feedback graphs are powerful online learning models that interpolate between the full information and classic bandit problems, capturing many real-life applications. A recent work by Zhang et al. (2023) studies the contextual version of this problem and proposes an efficient and optimal algorithm via a reduction to online regression. However, their algorithm crucially relies on seeing the feedback graph before making each decision, while in many applications, the feedback graph is uninformed, meaning that it is either only revealed after the learner makes her decision or even never fully revealed at all. This work develops the first contextual algorithm for such uninformed settings, via an efficient reduction to online regression over both the losses and the graphs. Importantly, we show that it is critical to learn the graphs using log loss instead of squared loss to obtain favorable regret guarantees. We also demonstrate the empirical effectiveness of our algorithm on a b
    
[^110]: 具有一般价值函数的情境多项式逻辑回归赌博机

    Contextual Multinomial Logit Bandits with General Value Functions

    [https://arxiv.org/abs/2402.08126](https://arxiv.org/abs/2402.08126)

    本文研究了具有一般价值函数的情境多项式逻辑回归赌博机，并提出了一套算法来处理线性情况，这些算法具有计算效率高、无维度的遗憾界限以及处理完全对抗性环境和奖励的能力。

    

    情境多项式逻辑回归 (MNL) 赌博机可以解决许多实际中的推荐问题，比如在线零售/广告。然而，先前的研究仅考虑了（广义的）线性价值函数，这严重限制了其适用性。鉴于这一事实，在本研究中，我们考虑了包含真实情况的情境MNL赌博机，借鉴了最近对情境赌博机研究的趋势。具体而言，我们考虑了随机和对抗性环境，并提出了一套算法，每个算法在计算和遗憾之间有不同的权衡。当应用于线性情况时，我们的结果不仅是第一个不依赖于某个可能指数增长的问题相关常数的结果，还具有其他优势，如计算效率、无维度的遗憾界限以及处理完全对抗性环境和奖励的能力。

    Contextual multinomial logit (MNL) bandits capture many real-world assortment recommendation problems such as online retailing/advertising. However, prior work has only considered (generalized) linear value functions, which greatly limits its applicability. Motivated by this fact, in this work, we consider contextual MNL bandits with a general value function class that contains the ground truth, borrowing ideas from a recent trend of studies on contextual bandits. Specifically, we consider both the stochastic and the adversarial settings, and propose a suite of algorithms, each with different computation-regret trade-off. When applied to the linear case, our results not only are the first ones with no dependence on a certain problem-dependent constant that can be exponentially large, but also enjoy other advantages such as computational efficiency, dimension-free regret bounds, or the ability to handle completely adversarial contexts and rewards.
    
[^111]: 一种改进的分子序列分析的通用非参数方法

    A Universal Non-Parametric Approach For Improved Molecular Sequence Analysis

    [https://arxiv.org/abs/2402.08117](https://arxiv.org/abs/2402.08117)

    该论文提出了一种新的基于压缩模型的非参数方法，以改进分子序列分析的性能。通过使用基本压缩算法和标准化压缩距离算法，该方法能够在分类任务中取得更好的结果，并不依赖于手工特征或预训练模型。

    

    在生物研究领域中，理解分子序列的特征和功能是至关重要的。分子序列的分类已经广泛使用基于神经网络的技术。尽管这些模型的准确性令人惊讶，但它们往往需要大量的参数和更多的数据收集。在这项工作中，我们提出了一种基于压缩模型的新方法，该方法受到\cite{jiang2023low}的启发，将基本压缩算法（如Gzip和Bz2）的简单性与标准化压缩距离（NCD）算法相结合，以在分类任务上实现更好的性能，而不依赖于手工特征或预训练模型。首先，我们使用众所周知的压缩算法（如Gzip和Bz2）对分子序列进行压缩。通过利用压缩文件中编码的潜在结构，我们计算出每对分子序列之间的标准化压缩距离，该距离是从压缩距离推导出的。

    In the field of biological research, it is essential to comprehend the characteristics and functions of molecular sequences. The classification of molecular sequences has seen widespread use of neural network-based techniques. Despite their astounding accuracy, these models often require a substantial number of parameters and more data collection. In this work, we present a novel approach based on the compression-based Model, motivated from \cite{jiang2023low}, which combines the simplicity of basic compression algorithms like Gzip and Bz2, with Normalized Compression Distance (NCD) algorithm to achieve better performance on classification tasks without relying on handcrafted features or pre-trained models. Firstly, we compress the molecular sequence using well-known compression algorithms, such as Gzip and Bz2. By leveraging the latent structure encoded in compressed files, we compute the Normalized Compression Distance between each pair of molecular sequences, which is derived from t
    
[^112]: 大型语言模型的主动偏好学习

    Active Preference Learning for Large Language Models

    [https://arxiv.org/abs/2402.08114](https://arxiv.org/abs/2402.08114)

    本论文提出了一种用于大型语言模型的主动偏好学习策略，通过直接偏好优化（DPO）来更好地利用偏好标签。实验结果表明，该方法提高了基于成对偏好数据的微调的学习速度和最终性能。

    

    随着大型语言模型（LLM）的能力越来越强，与人类意图对齐的微调技术变得越来越重要。对于对齐这些模型来说，最关键的考虑是如何最有效地利用人力资源，或者在LLM本身被用作oracle的情况下如何最有效地利用模型资源。从人类或AI偏好中进行强化学习（RLHF / RLAIF）是这种技术最突出的例子，但它往往复杂且不稳定。最近，直接偏好优化（DPO）被提出作为一个更简单和更稳定的替代方法。在这项工作中，我们开发了一种DPO的主动学习策略，以更好地利用偏好标签。我们提出了一个基于语言模型的预测熵和DPO优化的隐式偏好模型的确定性度量的实用采集函数，展示了我们的方法如何提高基于成对偏好数据的微调的学习速度和最终性能。

    As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.
    
[^113]: 一种在microRTS中获奖的深度强化学习代理

    A Competition Winning Deep Reinforcement Learning Agent in microRTS

    [https://arxiv.org/abs/2402.08112](https://arxiv.org/abs/2402.08112)

    在IEEE microRTS竞赛中，RAISocketAI成为第一个获胜的深度强化学习代理，它通过逐步优化基本策略和迁移学习来击败了前两位竞赛获胜者，在未来的竞赛中可以作为基准参考，并为DRL研究提供起点。

    

    在CIG和CoG举办的IEEE microRTS（$\mu$RTS）竞赛的五届中，脚本代理主导了比赛。尽管深度强化学习（DRL）算法在实时策略（RTS）游戏中取得了重大进展，但由于需要大量的培训资源以及创建和调试此类代理所固有的复杂性，它们在这个主要是学术竞赛中的采用仍然有限。RAISocketAI是第一个在IEEE microRTS竞赛中获胜的DRL代理。在一个没有性能限制的基准测试中，RAISocketAI经常击败前两位竞赛获胜者。这个第一个获胜的DRL提交可以成为未来microRTS竞赛的基准，并成为未来DRL研究的起点。逐步优化基本策略和对特定地图进行迁移学习对RAISocketAI的获胜表现至关重要。这些策略可以用于经济训练未来的DRL代理。在模仿学习方面的进一步工作可以进一步提高DRL代理的性能。

    Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents. RAISocketAI is the first DRL agent to win the IEEE microRTS competition. In a benchmark without performance constraints, RAISocketAI regularly defeated the two prior competition winners. This first competition-winning DRL submission can be a benchmark for future microRTS competitions and a starting point for future DRL research. Iteratively fine-tuning the base policy and transfer learning to specific maps were critical to RAISocketAI's winning performance. These strategies can be used to economically train future DRL agents. Further work in Imitation L
    
[^114]: 从数据到决策：机器学习在商业推荐中的转变力量

    From Data to Decisions: The Transformational Power of Machine Learning in Business Recommendations

    [https://arxiv.org/abs/2402.08109](https://arxiv.org/abs/2402.08109)

    本研究探讨了机器学习在商业推荐系统中的作用，着重研究了数据源、特征工程和评估指标等方面的重要性，并突显了推荐引擎对用户体验和决策过程的重要影响。

    

    本研究旨在探讨机器学习对推荐系统在商业环境中演变和有效性的影响，特别是在它们在商业环境中日益重要的背景下。在方法论上，研究深入探讨了机器学习在推荐系统中塑造和改进的作用，着重研究数据来源、特征工程和评估指标的重要性，从而突显了增强推荐算法的迭代性质。研究还探讨了推荐引擎在各个领域的应用，通过高级算法和数据分析驱动，展示了它们对用户体验和决策过程的重要影响。这些引擎不仅简化了信息发现和增强了协作，还加快了知识获取，对企业在数字化领域中的导航至关重要。它们对销售、收入和企业竞争优势的贡献非常重要。

    This research aims to explore the impact of Machine Learning (ML) on the evolution and efficacy of Recommendation Systems (RS), particularly in the context of their growing significance in commercial business environments. Methodologically, the study delves into the role of ML in crafting and refining these systems, focusing on aspects such as data sourcing, feature engineering, and the importance of evaluation metrics, thereby highlighting the iterative nature of enhancing recommendation algorithms. The deployment of Recommendation Engines (RE), driven by advanced algorithms and data analytics, is explored across various domains, showcasing their significant impact on user experience and decision-making processes. These engines not only streamline information discovery and enhance collaboration but also accelerate knowledge acquisition, proving vital in navigating the digital landscape for businesses. They contribute significantly to sales, revenue, and the competitive edge of enterpr
    
[^115]: 通过凸凹优化寻找移动带统计套利

    Finding Moving-Band Statistical Arbitrages via Convex-Concave Optimization

    [https://arxiv.org/abs/2402.08108](https://arxiv.org/abs/2402.08108)

    本文提出了一种通过凸凹优化方法寻找可以包含更多资产的统计套利策略，其中价格带的中点随时间变化。

    

    我们提出了一种新的方法，用于寻找可以包含除传统交易对以外更多资产的统计套利。我们将问题归结为寻找一个波动率最高的投资组合，使其价格保持在一定范围内并满足杠杆限制。这个优化问题不是凸的，但可以通过凸凹程序来近似求解，这是一种特定的顺序凸规划方法。我们展示了该方法如何推广到寻找移动带统计套利，其中价格带的中点随时间变化。

    We propose a new method for finding statistical arbitrages that can contain more assets than just the traditional pair. We formulate the problem as seeking a portfolio with the highest volatility, subject to its price remaining in a band and a leverage limit. This optimization problem is not convex, but can be approximately solved using the convex-concave procedure, a specific sequential convex programming method. We show how the method generalizes to finding moving-band statistical arbitrages, where the price band midpoint varies over time.
    
[^116]: 针对均场极小极大问题的镜像下降-上升算法的研究

    Mirror Descent-Ascent for mean-field min-max problems

    [https://arxiv.org/abs/2402.08106](https://arxiv.org/abs/2402.08106)

    该论文研究了解决测度空间上极小极大问题的镜像下降-上升算法的两种变体，并证明了收敛速率与相关有限维算法的最新结果一致。

    

    我们研究了镜像下降-上升算法在测度空间上解决极小极大问题的两个变体：同时和依次。我们在凸性-凹性和相对光滑性的假设下，针对适当的Bregman散度在测度空间上通过平坦导数进行了定义。我们证明了收敛速率到混合纳什均衡，用尼凯多-Isoda误差表示，对于同时和依次方案分别是$\mathcal{O}\left(N^{-1/2}\right)$和$\mathcal{O}\left(N^{-2/3}\right)$，这与相关有限维算法的最新结果一致。

    We study two variants of the mirror descent-ascent algorithm for solving min-max problems on the space of measures: simultaneous and sequential. We work under assumptions of convexity-concavity and relative smoothness of the payoff function with respect to a suitable Bregman divergence, defined on the space of measures via flat derivatives. We show that the convergence rates to mixed Nash equilibria, measured in the Nikaid\`o-Isoda error, are of order $\mathcal{O}\left(N^{-1/2}\right)$ and $\mathcal{O}\left(N^{-2/3}\right)$ for the simultaneous and sequential schemes, respectively, which is in line with the state-of-the-art results for related finite-dimensional algorithms.
    
[^117]: 学习具有Laplacian约束的笛卡尔乘积图

    Learning Cartesian Product Graphs with Laplacian Constraints

    [https://arxiv.org/abs/2402.08105](https://arxiv.org/abs/2402.08105)

    本文研究了在Laplacian约束下学习笛卡尔乘积图的问题，建立了笛卡尔乘积Laplacian的统计一致性，并提出了一种有效的算法。实验证明了方法的有效性。

    

    图Laplacian学习，也被称为网络拓扑推断，是一个吸引多个领域兴趣的问题。在高斯图模型（GM）中，图学习等价于向协方差选择添加Laplacian结构。在图信号处理（GSP）中，从过滤系统的输出中推断未观察到的图是至关重要的。在本文中，我们研究了在Laplacian约束下学习笛卡尔乘积图的问题。笛卡尔图乘积是建模高阶条件依赖的一种自然方法，也是将GSP推广到多路张量的关键。我们建立了笛卡尔乘积Laplacian的惩罚最大似然估计（MLE）的统计一致性，并提出了一种有效的算法来解决这个问题。我们还扩展了我们的方法，以在存在结构性缺失值的情况下进行高效的联合图学习和插补。对合成和真实世界数据集的实验证明了我们的方法的有效性。

    Graph Laplacian learning, also known as network topology inference, is a problem of great interest to multiple communities. In Gaussian graphical models (GM), graph learning amounts to endowing covariance selection with the Laplacian structure. In graph signal processing (GSP), it is essential to infer the unobserved graph from the outputs of a filtering system. In this paper, we study the problem of learning Cartesian product graphs under Laplacian constraints. The Cartesian graph product is a natural way for modeling higher-order conditional dependencies and is also the key for generalizing GSP to multi-way tensors. We establish statistical consistency for the penalized maximum likelihood estimation (MLE) of a Cartesian product Laplacian, and propose an efficient algorithm to solve the problem. We also extend our method for efficient joint graph learning and imputation in the presence of structural missing values. Experiments on synthetic and real-world datasets demonstrate that our 
    
[^118]: 研究大型语言模型在文本到SQL翻译中数据污染的影响

    Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation

    [https://arxiv.org/abs/2402.08100](https://arxiv.org/abs/2402.08100)

    本研究调查了大型语言模型在文本到SQL翻译中数据污染的影响。通过引入一种新的检测方法，研究人员发现GPT-3.5在陌生数据集上的性能显著下降。此外，通过采用对抗性表断开方法，研究人员还分析了GPT-3.5在修改信息的数据库上的效果。

    

    理解文本描述以生成代码似乎是零-shot场景下指令遵循大型语言模型（LLM）的一项已实现的能力。然而，可能会严重影响这种翻译能力的因素是已经见过目标的文本描述和相关代码。这种影响被称为数据污染。  在本研究中，我们调查了数据污染对GPT-3.5在文本到SQL代码生成任务中性能的影响。因此，我们提出了一种新的方法来检测GPTs中的数据污染，并使用已知的Spider数据集和我们的新的陌生数据集Termite来检查GPT-3.5在文本到SQL任务中的表现。此外，我们通过采用对抗性表断开（ATD）方法分析了GPT-3.5在具有修改信息的数据库上的效果，通过从数据库中删除结构信息来使文本到SQL任务复杂化。我们的研究结果表明，GPT-3.5在陌生的Termite数据集上表现出显著的性能下降。

    Understanding textual description to generate code seems to be an achieved capability of instruction-following Large Language Models (LLMs) in zero-shot scenario. However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination.   In this study, we investigate the impact of Data Contamination on the performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we introduce a novel method to detect Data Contamination in GPTs and examine GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on databases with modified information via an adversarial table disconnection (ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of information from the database. Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar T
    
[^119]: 一种加速梯度方法求解具有凸下层问题的简单双层优化问题

    An Accelerated Gradient Method for Simple Bilevel Optimization with Convex Lower-level Problem

    [https://arxiv.org/abs/2402.08097](https://arxiv.org/abs/2402.08097)

    本文提出了一种加速梯度方法来解决具有凸下层问题的简单双层优化问题，通过局部逼近下层问题的解集和加速梯度更新方法，在有限次迭代内找到一个具有一定精度的最优解。

    

    本文主要研究简单的双层优化问题，即在另一个凸光滑约束优化问题的最优解集上最小化一个凸光滑目标函数。我们提出了一种新颖的双层优化方法，通过切平面方法局部逼近下层问题的解集，并采用加速梯度更新方法降低近似解集上的上层目标函数。我们通过子最优解和不可行误差度量我们方法的性能，并提供了对两个误差标准的非渐进收敛性保证。特别地，当可行集是紧致的时候，我们证明了我们的方法最多需要$\mathcal{O}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$次迭代才能找到一个$\epsilon_f$-子最优且$\epsilon_g$-不可行的解。此外，在额外假设下，下层目标满足$r$阶H\"olderian误差时，我们给出了解的收敛速度估计。

    In this paper, we focus on simple bilevel optimization problems, where we minimize a convex smooth objective function over the optimal solution set of another convex smooth constrained optimization problem. We present a novel bilevel optimization method that locally approximates the solution set of the lower-level problem using a cutting plane approach and employs an accelerated gradient-based update to reduce the upper-level objective function over the approximated solution set. We measure the performance of our method in terms of suboptimality and infeasibility errors and provide non-asymptotic convergence guarantees for both error criteria. Specifically, when the feasible set is compact, we show that our method requires at most $\mathcal{O}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$ iterations to find a solution that is $\epsilon_f$-suboptimal and $\epsilon_g$-infeasible. Moreover, under the additional assumption that the lower-level objective satisfies the $r$-th H\"olderian err
    
[^120]: 在微调预训练模型时重新练习哪些预训练样本更好？

    Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?

    [https://arxiv.org/abs/2402.08096](https://arxiv.org/abs/2402.08096)

    本文提出了一种新的微调预训练模型的采样方案"mix-cd"，通过识别和优先处理实际面临遗忘的样本，以缓解微调过程中的知识遗忘问题。该方法简单、易于实现，并能在现有模型中无缝运用，有效地保持预训练模型的性能。

    

    在文本和视觉任务中，微调预训练基础模型已成为事实上的方法。这种方法的一个已知问题是在微调过程中会遗忘预训练知识。从预训练数据集中随机选择样本来进行重新练习是缓解遗忘的常见方法。然而，我们发现随机混合不经意地包括了模型尚未遗忘或无法学习的样本。我们提出了一种新的采样方案"mix-cd"，用于识别和优先处理实际面临遗忘的样本，我们称之为"collateral damage"。由于直接识别"collateral damage"样本计算成本高昂，我们提出了一种通过跟踪微调样本的统计信息来估计这类样本分布的过程。我们的方法简洁轻量，易于实现，并可以无缝集成到现有模型中，具有有效地保持预训练性能而无需额外计算开销的能力。

    Fine-tuning pretrained foundational models on specific tasks is now the de facto approach for text and vision tasks. A known pitfall of this approach is the forgetting of pretraining knowledge that happens during finetuning. Rehearsing samples randomly from the pretrain dataset is a common approach to alleviate such forgetting. However, we find that random mixing unintentionally includes samples which are not (yet) forgotten or unlearnable by the model. We propose a novel sampling scheme, mix-cd, that identifies and prioritizes samples that actually face forgetting, which we call collateral damage. Since directly identifying collateral damage samples is computationally expensive, we propose a procedure to estimate the distribution of such samples by tracking the statistics of finetuned samples. Our approach is lightweight, easy to implement, and can be seamlessly integrated into existing models, offering an effective means to retain pretrain performance without additional computational
    
[^121]: 离散扩散模型的收敛分析：通过均匀化的确切实现

    Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization

    [https://arxiv.org/abs/2402.08095](https://arxiv.org/abs/2402.08095)

    本文通过均匀化的方式确切实现了离散扩散模型，研究了其理论性质，并提供了关于采样的总变差距离和KL散度保证。这一方法在建模离散数据方面具有重要的应用价值。

    

    扩散模型在数据生成任务中取得了巨大的经验成功。最近，一些努力已经被做出来，将扩散模型的框架适应到离散状态空间，为建模本质上是离散数据（如语言和图形）提供了一种更自然的方法。这通过将前向噪声过程和相应的逆过程都构建为连续时间马尔可夫链（CTMC）来实现。在本文中，我们研究了离散扩散模型的理论性质。具体而言，我们介绍了一种利用连续马尔可夫链均匀化的算法，在随机时间点上实现转移。在关于离散得分函数学习的合理假设下，我们得到了从超立方体上的任何分布进行采样所需的总变差距离和KL散度保证。我们的结果与在$\mathbb{R}^d$中的扩散模型的最新成就相一致，并进一步强调了d的优势。

    Diffusion models have achieved huge empirical success in data generation tasks. Recently, some efforts have been made to adapt the framework of diffusion models to discrete state space, providing a more natural approach for modeling intrinsically discrete data, such as language and graphs. This is achieved by formulating both the forward noising process and the corresponding reversed process as Continuous Time Markov Chains (CTMCs). In this paper, we investigate the theoretical properties of the discrete diffusion model. Specifically, we introduce an algorithm leveraging the uniformization of continuous Markov chains, implementing transitions on random time points. Under reasonable assumptions on the learning of the discrete score function, we derive Total Variation distance and KL divergence guarantees for sampling from any distribution on a hypercube. Our results align with state-of-the-art achievements for diffusion models in $\mathbb{R}^d$ and further underscore the advantages of d
    
[^122]: 基于10万小时数据的10亿参数文本到语音模型的经验教训

    BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data

    [https://arxiv.org/abs/2402.08093](https://arxiv.org/abs/2402.08093)

    基于10万小时数据的10亿参数文本到语音模型BASE TTS在语音自然度上达到了最新技术水平，并且能够展现自然的韵律。

    

    我们介绍了一个名为BASE TTS的文本到语音（TTS）模型，其中BASE代表大规模自适应可流式TTS和新出现的能力。BASE TTS是迄今为止最大的TTS模型，训练于10万小时的公共领域语音数据，实现了语音自然度的最新技术水平。它采用了一个10亿参数的自回归Transformer，将原始文本转换为离散代码（"speechcodes"），然后通过基于卷积的解码器将这些speechcodes以增量、可流式的方式转换为波形。此外，我们的speechcodes采用了一种新颖的语音标记化技术，具有说话者ID解耦和字节对编码的压缩特性。与大量数据训练的大语言模型广泛报道的"新出现的能力"类似，我们展示了使用10K+小时和500M+参数构建的BASE TTS变体在文本复杂句子上开始展现自然的韵律。我们设计了...

    We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with $\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes ("speechcodes") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported "emergent abilities" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design
    
[^123]: 学习神经收缩动力学：扩展线性化和全局保证

    Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees

    [https://arxiv.org/abs/2402.08090](https://arxiv.org/abs/2402.08090)

    本论文提出了扩展线性化收缩动力学（ELCD），是第一个具有全局收缩性保证的神经网络动力系统，通过参数化非线性向量场的扩展线性化实现。通过在数据空间和潜在空间之间训练微分同胚，并在潜在空间中强制收缩性，ELCD能在面对不确定性时保持全局稳定性和鲁棒性。

    

    在学习的动态系统中，全局稳定性和鲁棒性保证对于确保系统在面对不确定性时的良好行为至关重要。我们提出了扩展线性化收缩动力学（ELCD），该系统是第一个具有任意度量下全局收缩性保证的基于神经网络的动力系统。ELCD的关键特性是非线性向量场扩展线性化的参数化。在其最基本形式下，ELCD保证全局指数稳定、平衡收缩以及在某些度量下全局收缩。为了实现在数据空间中相对于更一般度量的收缩，我们训练数据空间和潜在空间之间的微分同胚，并在潜在空间中强制收缩性，从而确保数据空间的全局收缩性。我们在2D、4D和8D的LASA数据集上展示了ELCD的性能。

    Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty. We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field. In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric. To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D LASA datasets.
    
[^124]: 使用统计过程控制的离群数据检测和数据漂移监测方法

    Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control

    [https://arxiv.org/abs/2402.08088](https://arxiv.org/abs/2402.08088)

    该论文提出了一种使用统计过程控制的机器学习框架，用于检测离群数据和监测数据漂移。该框架在临床环境中具有重要应用价值，能够帮助提高ML设备在放射学图像中的性能和患者安全。

    

    背景：机器学习（ML）方法往往在数据偏离其训练分布时失效。这对于临床环境中的ML设备来说是一个重要的问题，因为数据漂移可能导致意外的性能下降，危及患者安全。方法：我们提出了一种基于ML的统计过程控制（SPC）框架，用于离群数据检测和漂移监测。SPC优势在于能够直观地和统计上突出显示与预期分布的偏差。为了证明所提出的框架在放射学图像的数据漂移监测中的实用性，我们考察了不同的设计选择，包括特征提取方法、漂移量化和SPC参数选择等。结果：我们展示了我们的框架在两个任务中的有效性：1）区分轴向与非轴向的计算机断层摄影（CT）图像；2）将胸部X光（CXR）与其他模态进行分离。

    Background: Machine learning (ML) methods often fail with data that deviates from their training distribution. This is a significant concern for ML-enabled devices in clinical settings, where data drift may cause unexpected performance that jeopardizes patient safety.   Method: We propose a ML-enabled Statistical Process Control (SPC) framework for out-of-distribution (OOD) detection and drift monitoring. SPC is advantageous as it visually and statistically highlights deviations from the expected distribution. To demonstrate the utility of the proposed framework for monitoring data drift in radiological images, we investigated different design choices, including methods for extracting feature representations, drift quantification, and SPC parameter selection.   Results: We demonstrate the effectiveness of our framework for two tasks: 1) differentiating axial vs. non-axial computed tomography (CT) images and 2) separating chest x-ray (CXR) from other modalities. For both tasks, we achie
    
[^125]: 多模态学习中的文本中心对齐

    Text-centric Alignment for Multi-Modality Learning

    [https://arxiv.org/abs/2402.08086](https://arxiv.org/abs/2402.08086)

    本研究提出了一种名为文本中心对齐的多模态学习方法（TAMML），利用大型语言模型和基础模型，通过将文本作为统一的语义空间，解决了多模态学习中的模态不匹配问题，并在处理未见过的、多样化的、不可预测的模态组合时取得了显著的改进。

    

    本研究论文解决了多模态学习中的模态不匹配问题，即推理阶段可用的模态与训练阶段不同。我们提出了一种名为文本中心对齐的多模态学习（TAMML）方法，该方法利用大型语言模型（LLMs）进行上下文学习，并借助基础模型增强多模态系统在这些条件下的泛化能力。通过利用文本作为统一的语义空间的独特特性，TAMML在处理未见过的、多样化的、不可预测的模态组合方面展示出显著的改进。TAMML不仅能够适应不同的模态，还能保持稳健的性能，展示了基础模型在克服传统的固定模态框架中的表示嵌入限制方面的潜力。这项研究为领域提供了一种灵活有效的解决方案，适用于现实世界的应用，其中模态的可用性可能会变化。

    This research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availabili
    
[^126]: 信息绕行：一种简单且有效的用于表达图学习的循环表示方法

    Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning

    [https://arxiv.org/abs/2402.08085](https://arxiv.org/abs/2402.08085)

    "信息绕行"是一种用于层次性表征图中循环的方法，通过比较最短路径和最长路径之间的对比性，实现了与高阶"Weisfeiler-Lehman"（WL）测试相当的表达能力，但计算需求更少。

    

    图学习在生物信息学、社交网络和化学领域中至关重要。尽管高阶图形特征，如循环，对于节点分类、边预测和图像识别至关重要，但对高阶拓扑特征进行建模在计算上面临着困难，限制了其在机器学习中的广泛应用。为了解决这个问题，我们引入了"信息绕行"的概念，以在整个图中层次性地表征循环，利用每个图节点相关的一系列局部拓扑结构中最短路径和最长路径之间的对比性。我们从信息绕行景观中得到的拓扑特征表示具有与高阶"Weisfeiler-Lehman"（WL）测试相当的表达能力，但计算需求更少。除了与图核和信息的集成外

    Graph learning is crucial in the fields of bioinformatics, social networks, and chemicals. Although high-order graphlets, such as cycles, are critical to achieving an informative graph representation for node classification, edge prediction, and graph recognition, modeling high-order topological characteristics poses significant computational challenges, restricting its widespread applications in machine learning. To address this limitation, we introduce the concept of \textit{message detouring} to hierarchically characterize cycle representation throughout the entire graph, which capitalizes on the contrast between the shortest and longest pathways within a range of local topologies associated with each graph node. The topological feature representations derived from our message detouring landscape demonstrate comparable expressive power to high-order \textit{Weisfeiler-Lehman} (WL) tests but much less computational demands. In addition to the integration with graph kernel and message
    
[^127]: 基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难

    Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions

    [https://arxiv.org/abs/2402.08082](https://arxiv.org/abs/2402.08082)

    基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难，通过分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。

    

    尽管基于分数的生成模型（SGMs）在巨大的图像生成任务中取得了显著的成功，但它们的数学基础仍然有限。在本文中，我们分析了SGMs在学习一个子高斯概率分布族中的近似和泛化。我们引入了一种关于概率分布复杂性的概念，即相对密度与标准高斯测度的相对密度。我们证明，如果对数相对密度可以通过神经网络进行局部逼近，并且网络参数可以适当地受限，那么通过经验分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。我们通过示例说明了我们的理论，其中包括某些高斯混合分布。我们证明的一个关键点是推导出与正向过程相关的真实得分函数的维度无关的深度神经网络逼近速率。

    While score-based generative models (SGMs) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is inte
    
[^128]: 大型语言模型作为两人游戏中的代理

    Large Language Models as Agents in Two-Player Games

    [https://arxiv.org/abs/2402.08078](https://arxiv.org/abs/2402.08078)

    通过将大型语言模型的训练过程重新概念化为基于语言的两人游戏中的代理学习，我们能够得到关键的见解，并提供了新的方法和技术来推进大型语言模型的发展。

    

    通过在一个统一的机器学习范式中正式定义大型语言模型（LLMs）的训练过程，该过程通常包括预训练、有监督微调和强化学习与人类反馈，在推进LLM技术方面可以获得关键性的见解。本文描述了LLM的训练方法与在博弈论、强化学习和多智能体系统中研究的两人游戏代理开发所采用的策略之间的相似之处。我们提出了一种将LLM学习过程重新概念化为基于语言的游戏中的代理学习的框架。这一框架揭示了LLM开发中的成功与挑战的创新视角，提供了对解决对齐问题和其他战略考虑的新理解。此外，我们的两人游戏方法为训练LLMs提供了新的数据准备和机器学习技术的启示。

    By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies. This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems. We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.
    
[^129]: 使用核函数的同胚度量匹配在生成建模中的应用

    Diffeomorphic Measure Matching with Kernels for Generative Modeling

    [https://arxiv.org/abs/2402.08077](https://arxiv.org/abs/2402.08077)

    该研究提出了使用核函数进行同胚度量匹配的方法，在生成建模中实现了概率测度的传输。通过理论分析和数值实验，本文展示了该方法的性能和适用性。

    

    本文提出了一个通用框架，通过普通微分方程(ODEs)和再生核希尔伯特空间(RKHSs)实现概率测度的传输，以达到最小差异的生成建模和采样。该框架受同胚匹配和图像配准思想的启发。文中通过理论分析，给出了该方法的先验误差界限，该界限与模型的复杂性、训练集中样本的数量和模型规范性有关。通过大量的数值实验进一步展示了该方法的特性、优势和弱点，并扩展了其在条件模拟和推断等其他任务中的适用性。

    This article presents a general framework for the transport of probability measures towards minimum divergence generative modeling and sampling using ordinary differential equations (ODEs) and Reproducing Kernel Hilbert Spaces (RKHSs), inspired by ideas from diffeomorphic matching and image registration. A theoretical analysis of the proposed method is presented, giving a priori error bounds in terms of the complexity of the model, the number of samples in the training set, and model misspecification. An extensive suite of numerical experiments further highlights the properties, strengths, and weaknesses of the method and extends its applicability to other tasks, such as conditional simulation and inference.
    
[^130]: 高效可扩展的基因理解语言模型微调方法

    Efficient and Scalable Fine-Tune of Language Models for Genome Understanding

    [https://arxiv.org/abs/2402.08075](https://arxiv.org/abs/2402.08075)

    这篇论文提出了一种名为Lingo的高效可扩展的基因理解语言模型微调方法，该方法利用了自然语言模型的上下文提示，并通过自适应秩采样方法适应了基因组注释的多样性任务。

    

    尽管DNA基因组模型在基因理解方面取得了进展，但在基因组数据规模和多样性方面仍面临重大挑战。这种限制与自然语言模型相比形成鲜明对比，后者在更大规模上取得了成功。此外，基因理解涉及许多具有固有数据异质性的下游基因组注释任务，因此需要更高效且稳健的针对基因组的微调方法。在这里，我们提出了Lingo：语言前缀微调基因组模型。与DNA基因组模型不同，Lingo策略性地利用了自然语言模型的上下文提示，重新校准其在基因组序列方面的语言知识。Lingo通过自适应秩采样方法进一步适应了许多不同的、异质的下游微调任务，该方法剪枝并随机重新引入剪枝的奇异向量。

    Although DNA foundation models have advanced the understanding of genomes, they still face significant challenges in the limited scale and diversity of genomic data. This limitation starkly contrasts with the success of natural language foundation models, which thrive on substantially larger scales. Furthermore, genome understanding involves numerous downstream genome annotation tasks with inherent data heterogeneity, thereby necessitating more efficient and robust fine-tuning methods tailored for genomics. Here, we present \textsc{Lingo}: \textsc{L}anguage prefix f\textsc{In}e-tuning for \textsc{G}en\textsc{O}mes. Unlike DNA foundation models, \textsc{Lingo} strategically leverages natural language foundation models' contextual cues, recalibrating their linguistic knowledge to genomic sequences. \textsc{Lingo} further accommodates numerous, heterogeneous downstream fine-tune tasks by an adaptive rank sampling method that prunes and stochastically reintroduces pruned singular vectors w
    
[^131]: 使用输入输出规范来支撑数据科学代码生成

    Grounding Data Science Code Generation with Input-Output Specifications

    [https://arxiv.org/abs/2402.08073](https://arxiv.org/abs/2402.08073)

    该论文提出了一种方法，通过使用输入输出规范来解决大型语言模型在生成代码时与自然语言提示和I/O规范对齐困难的问题，并在数据科学编程任务上进行了评估。

    

    最近，大型语言模型(LLM)展示了从自然语言(NL)提示生成代码的卓越能力。然而，在现实世界中，NL往往过于模糊，无法捕捉编程问题背后的真实意图，需要额外的输入输出(I/O)规范。不幸的是，LLM可能难以将其输出与NL提示和I/O规范对齐。在这篇论文中，我们提出了一种方法来缓解数据科学编程中的这个问题，其中任务需要明确的I/O规范以保证清晰度。具体而言，我们提出了GIFT4Code，一种用于基于I/O规范进行指导微调的LLM的新方法。我们的方法利用LLM本身产生的合成数据，并利用执行派生的反馈作为关键的学习信号。将该反馈以程序I/O规范的形式提供给LLM以促进指导微调。我们在两个具有挑战性的数据科学任务上评估了我们的方法。

    Large language models (LLMs) have recently demonstrated a remarkable ability to generate code from natural language (NL) prompts. However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification. In this paper, we give a way to mitigate this issue in the context of data science programming, where tasks require explicit I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel approach for the instruction fine-tuning of LLMs with respect to I/O specifications. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program I/O specifications, is provided to the LLM to facilitate instruction fine-tuning. We evaluated our approach on two challenging data scien
    
[^132]: 用于网络流量指纹的局部敏感哈希

    Locality Sensitive Hashing for Network Traffic Fingerprinting

    [https://arxiv.org/abs/2402.08063](https://arxiv.org/abs/2402.08063)

    这项研究探讨了使用局部敏感哈希（LSH）作为网络流量指纹技术的解决方案，以解决机器学习方法在特征选择、超参数调整和模型重新训练方面的困难，并提供对网络中的概念漂移的弹性。

    

    物联网的出现给计算机网络带来了额外的复杂性和困难。由于其简单的设计，这些设备特别容易受到网络攻击的影响。因此，识别网络中的这些设备以进行网络管理和识别任何有害行为至关重要。网络流量指纹是识别设备和检测异常的关键技术。目前，这方面的主要方法主要依赖于机器学习（ML）。然而，机器学习方法需要选择特征、调整超参数和重新训练模型来达到最佳结果，并为网络中检测到的概念漂移提供弹性。在这项研究中，我们建议使用局部敏感哈希（LSH）作为解决这些困难的网络流量指纹技术。我们的研究重点是研究Nilsimsa LSH函数的几种设计选项。

    The advent of the Internet of Things (IoT) has brought forth additional intricacies and difficulties to computer networks. These gadgets are particularly susceptible to cyber-attacks because of their simplistic design. Therefore, it is crucial to recognise these devices inside a network for the purpose of network administration and to identify any harmful actions. Network traffic fingerprinting is a crucial technique for identifying devices and detecting anomalies. Currently, the predominant methods for this depend heavily on machine learning (ML). Nevertheless, machine learning (ML) methods need the selection of features, adjustment of hyperparameters, and retraining of models to attain optimal outcomes and provide resilience to concept drifts detected in a network. In this research, we suggest using locality-sensitive hashing (LSH) for network traffic fingerprinting as a solution to these difficulties. Our study focuses on examining several design options for the Nilsimsa LSH functio
    
[^133]: 避免连续空间中的灾难：通过寻求帮助

    Avoiding Catastrophe in Continuous Spaces by Asking for Help

    [https://arxiv.org/abs/2402.08062](https://arxiv.org/abs/2402.08062)

    在连续空间中，通过寻求帮助来避免灾难。引入了一种上下文多臂赌博问题的变体，目标是最小化灾难发生的概率。提出了一种算法，在连续1D状态空间和相对简单的回报函数下，遗憾和向导师查询率都趋近于0。

    

    大多数具有正式遗憾保证的强化学习算法假设所有错误都是可逆的，并依赖于尝试所有可能的选项。当一些错误是无法修复甚至是灾难性的时，这种方法会导致糟糕的结果。我们提出了一种上下文多臂赌博问题的变体，在这个问题中，目标是最小化发生灾难的概率。具体而言，我们假设每轮的回报代表了在该轮避免灾难的概率，并尝试最大化回报的乘积（总体避免灾难的概率）。为了给 agent 一些成功的机会，我们允许有限次向导师提问，并假设回报函数为 Lipschitz 连续的。我们提出了一种算法，当时间跨度增长时，它的遗憾和向导师查询率都趋近于 0，假设是一个连续的 1D 状态空间和相对"简单"的回报函数。我们还提供了一个匹配的下界：在没有简单性假设的情况下，任何算法要么不断查询异常的行为，要么每次查询完全相同的行为。

    Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
    
[^134]: MIML库：用于多实例多标签学习的模块化和灵活库

    MIML library: a Modular and Flexible Library for Multi-instance Multi-label Learning

    [https://arxiv.org/abs/2402.08056](https://arxiv.org/abs/2402.08056)

    MIML库是一个用于多实例多标签学习的模块化和灵活的Java软件库，提供了43种分类算法，可通过XML配置文件执行，还包括数据管理和分区、交叉验证和性能评估等功能。

    

    MIML库是一个Java软件工具，用于开发、测试和比较多实例多标签（MIML）学习的分类算法。该库包括43种算法，并提供了特定的数据管理和分区、保留和交叉验证方法、性能评估的标准指标和报告生成的功能和格式。此外，算法可以通过XML配置文件执行，无需编程。它是平台无关、可扩展、免费、开源，并在GNU General Public License下在GitHub上提供。

    MIML library is a Java software tool to develop, test, and compare classification algorithms for multi-instance multi-label (MIML) learning. The library includes 43 algorithms and provides a specific format and facilities for data managing and partitioning, holdout and cross-validation methods, standard metrics for performance evaluation, and generation of reports. In addition, algorithms can be executed through $xml$ configuration files without needing to program. It is platform-independent, extensible, free, open-source, and available on GitHub under the GNU General Public License.
    
[^135]: 为什么和何时LLM助手可能出错：探究基于提示的交互对软件寻求帮助的有效性

    Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking

    [https://arxiv.org/abs/2402.08030](https://arxiv.org/abs/2402.08030)

    本研究通过实验和访谈调查了大型语言模型(LLM)助手在软件帮助寻求中的有效性。结果显示，虽然优化后的LLM助手相较于基准LLM表现更好，但提示指南和领域上下文的融合与否对于LLM的使用和用户感知没有显著影响。

    

    大型语言模型（LLM）助手，如ChatGPT，已成为帮助用户在复杂的、功能丰富的软件中导航的潜在替代方法。LLM使用来自特定领域文本、软件手册和代码库的大量训练数据来模拟人类般的交互，提供量身定制的帮助，包括逐步指导。本研究通过一项16名参与者的被试实验和后续访谈来调查LLM生成的软件指导。我们比较了基线LLM助手和针对特定软件环境进行优化的LLM，即SoftAIBot，后者还提供了构建适当提示的指南。我们评估了任务完成情况、感知准确性、相关性和信任度。令人惊讶的是，尽管SoftAIBot表现优于基准LLM，但我们的结果显示，在提示指南和领域上下文的融合与否对LLM的使用和用户感知没有显著差异。大多数用户在使用LLM时遇到困难。

    Large Language Model (LLM) assistants, such as ChatGPT, have emerged as potential alternatives to search methods for helping users navigate complex, feature-rich software. LLMs use vast training data from domain-specific texts, software manuals, and code repositories to mimic human-like interactions, offering tailored assistance, including step-by-step instructions. In this work, we investigated LLM-generated software guidance through a within-subject experiment with 16 participants and follow-up interviews. We compared a baseline LLM assistant with an LLM optimized for particular software contexts, SoftAIBot, which also offered guidelines for constructing appropriate prompts. We assessed task completion, perceived accuracy, relevance, and trust. Surprisingly, although SoftAIBot outperformed the baseline LLM, our results revealed no significant difference in LLM usage and user perceptions with or without prompt guidelines and the integration of domain context. Most users struggled to u
    
[^136]: UGMAE：一种统一框架用于图形Masked Autoencoders

    UGMAE: A Unified Framework for Graph Masked Autoencoders

    [https://arxiv.org/abs/2402.08023](https://arxiv.org/abs/2402.08023)

    UGMAE是一种统一框架，用于解决图形自动编码器中存在的节点重要性、图像信息利用、表示空间中的语义知识和重构稳定性等问题。

    

    图形自动编码器的生成自监督学习已成为一种流行的学习范式，并在处理非欧几里德数据方面表现出了其有效性。然而，仍存在一些问题限制了现有方法的能力：1）在掩模中忽视不均匀的节点重要性，2）对整体图像信息的利用不足，3）由于仅在输出空间中使用重构损失，忽视了表示空间中的语义知识，以及4）由于遮蔽内容的大量，导致不稳定的重构。因此，我们提出了UGMAE，一种用于解决这些问题的图形Masked Autoencoders的统一框架，从适应性、完整性、互补性和一致性的角度来考虑。具体而言，我们首先开发了一个自适应特征遮罩生成器，以考虑节点的独特重要性并采样信息丰富的遮罩（适应性）。然后，我们设计了一个基于排名的结构重建方法，以维护图形的完整性和语义一致性，并使用额外的自监督任务增强图像生成过程（完整性和互补性）。

    Generative self-supervised learning on graphs, particularly graph masked autoencoders, has emerged as a popular learning paradigm and demonstrated its efficacy in handling non-Euclidean data. However, several remaining issues limit the capability of existing methods: 1) the disregard of uneven node significance in masking, 2) the underutilization of holistic graph information, 3) the ignorance of semantic knowledge in the representation space due to the exclusive use of reconstruction loss in the output space, and 4) the unstable reconstructions caused by the large volume of masked contents. In light of this, we propose UGMAE, a unified framework for graph masked autoencoders to address these issues from the perspectives of adaptivity, integrity, complementarity, and consistency. Specifically, we first develop an adaptive feature mask generator to account for the unique significance of nodes and sample informative masks (adaptivity). We then design a ranking-based structure reconstruct
    
[^137]: 在大规模无线网络中利用数字堂兄来进行集成Q-Learning

    Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale Wireless Networks

    [https://arxiv.org/abs/2402.08022](https://arxiv.org/abs/2402.08022)

    本文提出了一种针对优化无线网络的传统Q-Learning算法面临的性能和复杂性挑战的新颖的集成Q-Learning算法，在大规模无线网络中利用数字堂兄来逼近可观测无线网络的大状态空间。

    

    优化大规模无线网络，包括最优资源管理、功率分配和吞吐量最大化，是一项困难的任务，因为它们具有非可观测的系统动态和异构且复杂的特性。本文提出了一种新颖的集成Q-Learning算法，解决了传统Q-Learning算法优化无线网络时面临的性能和复杂性挑战。通过合成马尔可夫决策过程的集成学习，通过新模型逼近大状态空间可观测无线网络。特别地，提出了数字堂兄作为传统数字孪生概念的延伸，其中在多个合成马尔可夫环境上运行多个Q-Learning算法，并将它们的输出融合成一个单独的Q函数。提供了关键统计量和Q函数的收敛分析，以及对估计偏差和方差上限的推导。

    Optimizing large-scale wireless networks, including optimal resource management, power allocation, and throughput maximization, is inherently challenging due to their non-observable system dynamics and heterogeneous and complex nature. Herein, a novel ensemble Q-learning algorithm that addresses the performance and complexity challenges of the traditional Q-learning algorithm for optimizing wireless networks is presented. Ensemble learning with synthetic Markov Decision Processes is tailored to wireless networks via new models for approximating large state-space observable wireless networks. In particular, digital cousins are proposed as an extension of the traditional digital twin concept wherein multiple Q-learning algorithms on multiple synthetic Markovian environments are run in parallel and their outputs are fused into a single Q-function. Convergence analyses of key statistics and Q-functions and derivations of upper bounds on the estimation bias and variance are provided. Numeri
    
[^138]: 扩散生成模型的最近邻评分估计器

    Nearest Neighbour Score Estimators for Diffusion Generative Models

    [https://arxiv.org/abs/2402.08018](https://arxiv.org/abs/2402.08018)

    本论文提出了一种新颖的最近邻评分函数估计器，通过利用训练集中的多个样本大大降低了估计器的方差，可用于训练一致性模型和扩散模型，提高收敛速度、样本质量，并为进一步的研究提供了新的可能性。

    

    评分函数估计是训练和采样扩散生成模型的基础。尽管如此，最常用的估计器要么是有偏的神经网络逼近，要么是基于条件评分的高方差蒙特卡洛估计器。我们引入了一种创新的最近邻评分函数估计器，利用训练集中的多个样本大大降低了估计器的方差。我们在两个引人注目的应用中利用了低方差估计器。在使用我们的估计器进行训练一致性模型时，我们报告了收敛速度和样本质量显著提高。在扩散模型中，我们展示了我们的估计器可以替代学习网络进行概率流ODE积分，为未来研究开辟了有前景的新方向。

    Score function estimation is the cornerstone of both training and sampling from diffusion generative models. Despite this fact, the most commonly used estimators are either biased neural network approximations or high variance Monte Carlo estimators based on the conditional score. We introduce a novel nearest neighbour score function estimator which utilizes multiple samples from the training set to dramatically decrease estimator variance. We leverage our low variance estimator in two compelling applications. Training consistency models with our estimator, we report a significant increase in both convergence speed and sample quality. In diffusion models, we show that our estimator can replace a learned network for probability-flow ODE integration, opening promising new avenues of future research.
    
[^139]: Lumos : 用场景文本识别增强多模式LLMs的能力

    Lumos : Empowering Multimodal LLMs with Scene Text Recognition

    [https://arxiv.org/abs/2402.08017](https://arxiv.org/abs/2402.08017)

    本论文介绍了Lumos，它是第一个具备文本理解能力的多模式问答系统，通过运用场景文本识别组件，能够从第一人称视角图像中提取文本，并将其用于加强多模式大型语言模型的输入。研究过程中，作者克服了与文本识别质量、延迟和模型推断相关的多个挑战，并提供了全面的组件评估结果，展示了高质量和高效率的性能。

    

    我们介绍了Lumos，它是第一个具备文本理解能力的端到端多模式问答系统。Lumos的核心是一个场景文本识别（STR）组件，用于从第一人称视角图像中提取文本，并将其用于增强多模式大型语言模型（MM-LLM）的输入。在构建Lumos的过程中，我们遇到了许多与STR质量、整体延迟和模型推断相关的挑战。在本文中，我们探讨了这些挑战，并讨论了用于克服这些障碍的系统架构、设计选择和建模技术。我们还对每个组件进行了全面评估，展示了高质量和高效率的性能。

    We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.
    
[^140]: 在线差分隐私合成数据生成

    Online Differentially Private Synthetic Data Generation

    [https://arxiv.org/abs/2402.08012](https://arxiv.org/abs/2402.08012)

    本文提出了一种在线差分隐私合成数据生成的多项式时间算法，在超立方体数据流上实现了近乎最优的精度界限，也推广了之前关于计数查询的连续发布模型的工作，仅需要额外的多项式对数因子。

    

    本文提出了一种用于在线差分隐私合成数据生成的多项式时间算法。对于在超立方体$[0,1]^d$内的数据流和无限时间范围，我们开发了一种在线算法，每个时间$t$都生成一个差分隐私合成数据集。该算法在1-Wasserstein距离上实现了近乎最优的精度界限：当$d\geq 2$时为$O(t^{-1/d}\log(t)$，当$d=1$时为$O(t^{-1}\log^{4.5}(t)$。这个结果将之前关于计数查询的连续发布模型的工作推广到包括Lipschitz查询。与离线情况不同，离线情况下整个数据集一次性可用，我们的方法仅需要在精度界限中额外的多项式对数因子。

    We present a polynomial-time algorithm for online differentially private synthetic data generation. For a data stream within the hypercube $[0,1]^d$ and an infinite time horizon, we develop an online algorithm that generates a differentially private synthetic dataset at each time $t$. This algorithm achieves a near-optimal accuracy bound of $O(t^{-1/d}\log(t))$ for $d\geq 2$ and $O(t^{-1}\log^{4.5}(t))$ for $d=1$ in the 1-Wasserstein distance. This result generalizes the previous work on the continual release model for counting queries to include Lipschitz queries. Compared to the offline case, where the entire dataset is available at once, our approach requires only an extra polylog factor in the accuracy bound.
    
[^141]: CNN需要哪些频率？特征学习中的紧急瓶颈结构的出现

    Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning

    [https://arxiv.org/abs/2402.08010](https://arxiv.org/abs/2402.08010)

    本文描述了CNN中卷积瓶颈（CBN）结构的出现，网络在前几层将输入表示转换为在少数频率和通道上受支持的表示，然后通过最后几层映射回输出。CBN秩定义了保留在瓶颈中的频率的数量和类型，并部分证明了参数范数与深度和CBN秩的比例成正比。此外，我们还展示了网络的参数范数依赖于函数的规则性。我们发现任何具有接近最优参数范数的网络都会展示出CBN结构，这解释了下采样的常见实践；我们还验证了CBN结构在下采样下仍然成立。最后，我们使用CBN结构来解释...（摘要完整内容请见正文）

    

    我们描述了CNN中卷积瓶颈（CBN）结构的出现，网络使用其前几层将输入表示转换为仅在几个频率和通道上受支持的表示，然后使用最后几层将其映射回输出。我们定义了CBN秩，描述了保留在瓶颈内的频率的数量和类型，并在一定程度上证明了表示函数$f$所需的参数范数按深度乘以CBN秩$f$的比例缩放。我们还展示了参数范数在下一阶中依赖于$f$的正则性。我们展示了任何具有近乎最优参数范数的网络都会在权重和（在网络对大学习率稳定的假设下）激活中表现出CBN结构，这促使了下采样的常见做法；并且我们验证了CBN结构在下采样下仍然成立。最后，我们使用CBN结构来解释...

    We describe the emergence of a Convolution Bottleneck (CBN) structure in CNNs, where the network uses its first few layers to transform the input representation into a representation that is supported only along a few frequencies and channels, before using the last few layers to map back to the outputs. We define the CBN rank, which describes the number and type of frequencies that are kept inside the bottleneck, and partially prove that the parameter norm required to represent a function $f$ scales as depth times the CBN rank $f$. We also show that the parameter norm depends at next order on the regularity of $f$. We show that any network with almost optimal parameter norm will exhibit a CBN structure in both the weights and - under the assumption that the network is stable under large learning rate - the activations, which motivates the common practice of down-sampling; and we verify that the CBN results still hold with down-sampling. Finally we use the CBN structure to interpret the
    
[^142]: 带有合成数据的改进型直接优化法用于LLM的行为调整

    Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs

    [https://arxiv.org/abs/2402.08005](https://arxiv.org/abs/2402.08005)

    本文提出了一种改进的直接优化法（rDPO），通过使用合成数据来改善大规模语言模型（LLM）的行为调整。这种方法通过自我评论和广义DPO损失函数来优化学生LLM，并利用外部奖励模型提高合成数据质量，从而使rDPO在多个行为调整任务中表现出良好效果。

    

    本文介绍了一种改进型直接优化法（rDPO），用于改善大规模语言模型（LLM）的行为调整，无需人工标注数据。该方法通过自我评论提示教师LLM来创建合成数据，然后利用广义DPO损失函数来提纯给学生LLM。损失函数结合了额外的外部奖励模型，以提高合成数据的质量，使rDPO能够抵抗合成数据集中的潜在噪声。rDPO在多种行为调整任务中展现出良好效果，如提高安全性，抵抗角色扮演，降低巴结行为。代码将在https://github.com/vicgalle/refined-dpo上发布。

    In this paper, we introduce \emph{refined Direct Preference Optimization} (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data. The method involves creating synthetic data using self-critique prompting by a teacher LLM and then utilising a generalized DPO loss function to distil to a student LLM. The loss function incorporates an additional external reward model to improve the quality of synthetic data, making rDPO robust to potential noise in the synthetic dataset. rDPO is shown to be effective in a diverse set of behavioural alignment tasks, such as improved safety, robustness against role-playing, and reduced sycophancy. Code to be released at https://github.com/vicgalle/refined-dpo.
    
[^143]: 改进并推广具有贝叶斯推断的ABCD方法

    Improvement and generalization of ABCD method with Bayesian inference

    [https://arxiv.org/abs/2402.08001](https://arxiv.org/abs/2402.08001)

    通过利用贝叶斯推断方法，改进和推广了ABCD方法，提供了一种更好的描述信号和背景的方式，并利用先验知识和不同可观测量之间的依赖关系来提高测量精度。

    

    在LHC上寻找新物理或精确我们对标准模型的认识是一个涉及多个因素的企业。我们专注于利用现有信息，并重新思考常规的基于数据的ABCD方法，通过使用贝叶斯机器学习工具来改进和推广它。我们建议使用混合模型来描述由信号和多个背景组成的数据集。通过利用先验知识和事件之间的依赖关系，可以很好地提取出样本中的信号、背景及其相对分数。与ABCD方法相比，我们展示了如何利用对不同背景的某些属性的理解以及每个事件中需要测量的超过两个独立可观测量的优势。此外，贝叶斯框架使用连续分布的信息来定义区域，而不是通过硬切割定义区域。

    To find New Physics or to refine our knowledge of the Standard Model at the LHC is an enterprise that involves many factors. We focus on taking advantage of available information and pour our effort in re-thinking the usual data-driven ABCD method to improve it and to generalize it using Bayesian Machine Learning tools. We propose that a dataset consisting of a signal and many backgrounds is well described through a mixture model. Signal, backgrounds and their relative fractions in the sample can be well extracted by exploiting the prior knowledge and the dependence between the different observables at the event-by-event level with Bayesian tools. We show how, in contrast to the ABCD method, one can take advantage of understanding some properties of the different backgrounds and of having more than two independent observables to measure in each event. In addition, instead of regions defined through hard cuts, the Bayesian framework uses the information of continuous distribution to obt
    
[^144]: NetInfoF 框架：度量和利用网络可利用信息

    NetInfoF Framework: Measuring and Exploiting Network Usable Information

    [https://arxiv.org/abs/2402.07999](https://arxiv.org/abs/2402.07999)

    NetInfoF框架提供了一种快速度量和利用节点属性图中的可利用信息的方法，能同时处理链路预测和节点分类任务，并具备理论保证和闭式解的方法。

    

    对于一个节点属性图和一个图任务（链路预测或节点分类），我们能否判断图神经网络（GNN）是否能很好地完成任务？具体而言，图结构和节点特征是否包含足够可利用的信息来完成任务？我们的目标是（1）开发一个快速工具来度量图结构和节点特征中包含的信息量，以及（2）根据信息量来解决任务。我们提出了NetInfoF框架，包括NetInfoF_Probe和NetInfoF_Act两个部分，分别用于度量和利用网络可利用信息（NUI）。给定一个图数据，NetInfoF_Probe在不进行任何模型训练的情况下度量NUI，而NetInfoF_Act则用于解决链路预测和节点分类任务，两个模块共享相同的骨干网络。总之，NetInfoF具有以下显著优点：（a）通用性，能处理链路预测和节点分类两种任务；（b）原则性，具备理论保证和闭式解的方法。

    Given a node-attributed graph, and a graph task (link prediction or node classification), can we tell if a graph neural network (GNN) will perform well? More specifically, do the graph structure and the node features carry enough usable information for the task? Our goals are (1) to develop a fast tool to measure how much information is in the graph structure and in the node features, and (2) to exploit the information to solve the task, if there is enough. We propose NetInfoF, a framework including NetInfoF_Probe and NetInfoF_Act, for the measurement and the exploitation of network usable information (NUI), respectively. Given a graph data, NetInfoF_Probe measures NUI without any model training, and NetInfoF_Act solves link prediction and node classification, while two modules share the same backbone. In summary, NetInfoF has following notable advantages: (a) General, handling both link prediction and node classification; (b) Principled, with theoretical guarantee and closed-form solu
    
[^145]: 利用低维分子嵌入进行快速化学相似性搜索

    Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical Similarity Search

    [https://arxiv.org/abs/2402.07970](https://arxiv.org/abs/2402.07970)

    本文评估了低维化学嵌入结合k-d树数据结构是否能够在保持对标准化学相似性搜索基准性能的同时实现快速最近邻查询

    

    最近邻相似性搜索是化学领域中常见的任务，在药物发现等领域有着显著的应用。然而，目前一些常用的方法仍然采用蛮力搜索的方式，这在实践中可能会造成计算成本高和时间消耗大的问题，部分原因是现代化学数据库的规模之大。以往对这一任务的计算技术进展主要依赖于硬件改进或特定数据集的技巧，缺乏普适性。而利用低复杂度的搜索算法的方法相对较少被探索。然而，许多这类算法都是近似解决方案，或者在典型的高维化学嵌入中存在困难。本文评估了低维化学嵌入结合k-d树数据结构是否能够在保持对标准化学相似性搜索基准性能的同时实现快速最近邻查询。我们对不同的降维方法进

    Nearest neighbor-based similarity searching is a common task in chemistry, with notable use cases in drug discovery. Yet, some of the most commonly used approaches for this task still leverage a brute-force approach. In practice this can be computationally costly and overly time-consuming, due in part to the sheer size of modern chemical databases. Previous computational advancements for this task have generally relied on improvements to hardware or dataset-specific tricks that lack generalizability. Approaches that leverage lower-complexity searching algorithms remain relatively underexplored. However, many of these algorithms are approximate solutions and/or struggle with typical high-dimensional chemical embeddings. Here we evaluate whether a combination of low-dimensional chemical embeddings and a k-d tree data structure can achieve fast nearest neighbor queries while maintaining performance on standard chemical similarity search benchmarks. We examine different dimensionality redu
    
[^146]: SMX: 专家迭代的顺序蒙特卡洛规划

    SMX: Sequential Monte Carlo Planning for Expert Iteration

    [https://arxiv.org/abs/2402.07963](https://arxiv.org/abs/2402.07963)

    这项研究介绍了一种名为SMX的顺序蒙特卡洛规划算法，它利用可扩展的方法创建了有效的自我学习机制。它适用于离散和连续动作空间的环境，具有高并行性能。

    

    发展能够在决策和学习过程中利用规划能力的智能体对于人工智能的进步至关重要。最近的研究已经证明了树状搜索方法和自我对弈学习机制的有效性。然而，由于搜索过程的顺序性质，这些方法通常面临扩展性挑战。虽然实践工程解决方案可以部分克服这个问题，但仍需要大量计算资源，这限制了它们的适用性。在本文中，我们介绍一种名为SMX的基于模型的计划算法，它利用可扩展的顺序蒙特卡洛方法创建了一种有效的自我学习机制。SMX基于控制作为推断的理论框架，并受益于坚实的理论基础。它基于采样的搜索方法使其适应具有离散和连续动作空间的环境。此外，SMX允许高度并行化并可以运行于各类计算机设备上。

    Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence. Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms. Yet, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability. In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism. Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings. Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces. Furthermore, SMX allows for high parallelisation and can run on h
    
[^147]: ProtIR：用于蛋白质功能注释的检索器和预测器之间的迭代优化

    ProtIR: Iterative Refinement between Retrievers and Predictors for Protein Function Annotation

    [https://arxiv.org/abs/2402.07955](https://arxiv.org/abs/2402.07955)

    ProtIR是一种新颖的变分伪似然框架，通过迭代优化功能预测器和检索器之间的知识，以提高蛋白质功能注释的准确性，并在没有大规模预训练的情况下表现出与或胜过预测器的能力。

    

    蛋白质功能注释是生物学中重要但具有挑战性的任务。最近的深度学习进展显示，通过学习蛋白质序列和结构，可以显著提高准确的功能预测能力。然而，这些基于预测器的方法通常忽视了蛋白质相似性建模，这是在传统方法中使用序列或结构检索工具常用的思想。为了填补这个差距，我们首先通过在蛋白质功能注释任务上对比检索器和预测器进行基准测试，研究了蛋白质相似性建模的影响。我们的结果表明，检索器可以在没有大规模预训练的情况下与或胜过预测器。基于这些见解，我们引入了一种新颖的变分伪似然框架ProtIR，旨在通过整合蛋白质相似性建模来改进功能预测器。该框架在功能预测器和检索器之间迭代地优化知识，从而结合了两者的优势。

    Protein function annotation is an important yet challenging task in biology. Recent deep learning advancements show significant potential for accurate function prediction by learning from protein sequences and structures. Nevertheless, these predictor-based methods often overlook the modeling of protein similarity, an idea commonly employed in traditional approaches using sequence or structure retrieval tools. To fill this gap, we first study the effect of inter-protein similarity modeling by benchmarking retriever-based methods against predictors on protein function annotation tasks. Our results show that retrievers can match or outperform predictors without large-scale pre-training. Building on these insights, we introduce a novel variational pseudo-likelihood framework, ProtIR, designed to improve function predictors by incorporating inter-protein similarity modeling. This framework iteratively refines knowledge between a function predictor and retriever, thereby combining the stren
    
[^148]: 优化人工胰腺设计以改善糖尿病管理

    Optimizing the Design of an Artificial Pancreas to Improve Diabetes Management

    [https://arxiv.org/abs/2402.07949](https://arxiv.org/abs/2402.07949)

    通过神经进化算法优化人工胰腺治疗策略，减少糖尿病患者的血糖偏差，并且降低注射次数。

    

    糖尿病是一种慢性疾病，影响美国境内有3800万人，它会影响身体将食物转化为能量（即血糖）的能力。标准的治疗方法是通过使用人工胰腺，即持续胰岛素泵（基础注射），以及定期注射胰岛素（突发注射）来补充碳水化合物摄入量。治疗目标是将血糖保持在可接受范围的中心位置，通过持续血糖测量来进行衡量。次要目标是减少注射次数，因为对某些患者来说注射是不愉快且难以实施的。本研究使用神经进化来发现治疗的最佳策略。基于30天的治疗和单个患者的测量数据集，首先训练了随机森林来预测未来的血糖水平。然后通过进化了一个神经网络来指定碳水化合物摄入量、基础注射水平和突发注射。进化发现了一个帕累托前沿，减少了与目标值的偏差。

    Diabetes, a chronic condition that impairs how the body turns food into energy, i.e. blood glucose, affects 38 million people in the US alone. The standard treatment is to supplement carbohydrate intake with an artificial pancreas, i.e. a continuous insulin pump (basal shots), as well as occasional insulin injections (bolus shots). The goal of the treatment is to keep blood glucose at the center of an acceptable range, as measured through a continuous glucose meter. A secondary goal is to minimize injections, which are unpleasant and difficult for some patients to implement. In this study, neuroevolution was used to discover an optimal strategy for the treatment. Based on a dataset of 30 days of treatment and measurements of a single patient, a random forest was first trained to predict future glucose levels. A neural network was then evolved to prescribe carbohydrates, basal pumping levels, and bolus injections. Evolution discovered a Pareto front that reduced deviation from the targe
    
[^149]: evolSOM:一种用于自组织映射的进化保守性分析的R包

    evolSOM: an R Package for evolutionary conservation analysis with SOMs

    [https://arxiv.org/abs/2402.07948](https://arxiv.org/abs/2402.07948)

    evolSOM是一个新颖的R包，利用自组织映射( SOMs)探索和可视化生物变量保守性，以便更容易地整合表型和基因型属性，并分析不同物种或条件之间的生物变量位移。

    

    动机:解开基因和性状之间的联系对于解决许多生物难题至关重要。基因提供了构建细胞机器的指令，指导维持生命的过程。从这些遗传指令中衍生出的RNA分子和蛋白质在塑造细胞结构、影响反应和指导行为方面起着重要作用。这个基本的生物原则将基因构成与可观察性状联系起来，但是从这种复杂的多模态数据中整合和提取有意义的关系是一个重大挑战。结果:我们介绍了evolSOM，一种利用自组织映射(SOMs)探索和可视化生物变量保守性的创新R包，简化了表型和基因型属性的整合。通过构建捕捉非冗余模式的物种特定或条件特定的自组织映射，evolSOM允许分析物种或条件之间生物变量的位移。

    Motivation: Unraveling the connection between genes and traits is crucial for solving many biological puzzles. Genes provide instructions for building cellular machinery, directing the processes that sustain life. RNA molecules and proteins, derived from these genetic instructions, play crucial roles in shaping cell structures, influencing reactions, and guiding behavior. This fundamental biological principle links genetic makeup to observable traits, but integrating and extracting meaningful relationships from this complex, multimodal data presents a significant challenge. Results: We introduce evolSOM, a novel R package that utilizes Self-Organizing Maps (SOMs) to explore and visualize the conservation of biological variables, easing the integration of phenotypic and genotypic attributes. By constructing species-specific or condition-specific SOMs that capture non-redundant patterns, evolSOM allows the analysis of displacement of biological variables between species or conditions. Va
    
[^150]: 重新构想指挥与控制

    Re-Envisioning Command and Control

    [https://arxiv.org/abs/2402.07946](https://arxiv.org/abs/2402.07946)

    重新构想的论文提出了未来指挥与控制（C2）决策需要面对更复杂和挑战性的环境，因此提出了基于人工智能系统与人类强有力伙伴关系的未来C2的愿景。这个愿景的核心是优化C2操作流程，保持协同努力，发展自适应的集体知识系统。

    

    未来的战争将要求在更复杂、快节奏、不结构化和极具挑战性的环境中进行指挥与控制（C2）决策。C2将因被拒绝、退化、间歇和有限的通信以及需要考虑到多个作战领域中的许多数据流而变得更加复杂。然而，当前的C2实践——源自工业时代而非新兴的智能时代——是线性的且耗时。而且，这些方法可能无法在未来战场上与对手保持优势。为了应对这些挑战，我们提出了一种基于人工智能（AI）系统与人类之间强有力伙伴关系的未来C2愿景。这个未来愿景体现在三个运营影响上：优化C2操作流程，保持协同努力，以及发展自适应的集体知识系统。本文阐述了所设想的未来指挥与控制的愿景。

    Future warfare will require Command and Control (C2) decision-making to occur in more complex, fast-paced, ill-structured, and demanding conditions. C2 will be further complicated by operational challenges such as Denied, Degraded, Intermittent, and Limited (DDIL) communications and the need to account for many data streams, potentially across multiple domains of operation. Yet, current C2 practices -- which stem from the industrial era rather than the emerging intelligence era -- are linear and time-consuming. Critically, these approaches may fail to maintain overmatch against adversaries on the future battlefield. To address these challenges, we propose a vision for future C2 based on robust partnerships between humans and artificial intelligence (AI) systems. This future vision is encapsulated in three operational impacts: streamlining the C2 operations process, maintaining unity of effort, and developing adaptive collective knowledge systems. This paper illustrates the envisaged fu
    
[^151]: 大型语言用户界面：由LLMs驱动的语音交互用户界面

    Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs

    [https://arxiv.org/abs/2402.07938](https://arxiv.org/abs/2402.07938)

    本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介，通过对自然文本输入进行彻底分析，实现智能和响应式用户体验。

    

    最近大型语言模型的快速发展展示了其在逻辑推理和理解方面的卓越能力。这些新发现的能力引发了新一代软件的诞生，正如它们在工业界无数应用中所展示的那样。本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介。通过对自然文本输入进行彻底分析，一个经过精心设计的LLM引擎可以理解用户的需求，分类最有可能的应用程序，识别所需的UI组件，并随后执行用户期望的操作。这种集成可以将静态UI系统发展成高度动态和可适应的解决方案，引入智能和响应式用户体验的新领域。这样的框架可以从根本上改变用户完成日常任务的方式，极大提升用户体验。

    The recent meteoric advancements in large language models have showcased a remarkable capacity for logical reasoning and comprehension. These newfound capabilities have opened the door to a new generation of software, as has been made obvious through the innumerable ways they are being applied in the industry. This research focuses on harnessing and guiding the upgraded power of LLMs to construct a framework that can serve as an intermediary between a user and their user interface. By comprehending a user's needs through a thorough analysis of natural textual inputs, an effectively crafted LLM engine can classify the most likely available application, identify the desired UI component and subsequently execute the user's expected actions. This integration can evolve static UI systems into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences. Such a framework can fundamentally shift how users accomplish daily tasks, skyrocket e
    
[^152]: 基于生理传感器的与驾驶模拟器同步的安卓应用程序用于驾驶员监测

    A Physiological Sensor-Based Android Application Synchronized with a Driving Simulator for Driver Monitoring

    [https://arxiv.org/abs/2402.07937](https://arxiv.org/abs/2402.07937)

    本文提出了一种基于生理传感器的安卓应用程序，与驾驶模拟器同步工作，用于监测驾驶员的生理状态和驾驶表现。通过配置、选择、接收、处理、图形化表示和存储多种传感器的信号，该应用程序可以分析驾驶员的心电图、肌电图、皮肤电反应和动力学参数，并与驾驶模拟器同步分析驾驶安全性和效率。

    

    本文提出了一种安卓应用程序，可以控制和监测来自Shimmer平台的生理传感器，并与驾驶模拟器同步工作。该应用程序可以监测驾驶员，其参数可以用于分析他们的生理状态与驾驶性能之间的关系。该应用程序可以配置、选择、接收、处理、图形化表示和存储来自心电图（ECG）、肌电图（EMG）、皮肤电反应（GSR）模块和加速度计、磁力计和陀螺仪的信号。该安卓应用程序通过两个步骤与我们之前使用Unity游戏引擎开发的驾驶模拟器同步，以分析驾驶安全性和效率。该安卓应用程序在不同采样率和不同安卓设备上同时测试了不同传感器的工作情况。我们还通过与25人一起测试驾驶模拟器和安卓应用程序的同步工作，并分析了两者之间的关系。

    In this paper, we present an Android application to control and monitor the physiological sensors from the Shimmer platform and its synchronized working with a driving simulator. The Android app can monitor drivers and their parameters can be used to analyze the relation between their physiological states and driving performance. The app can configure, select, receive, process, represent graphically, and store the signals from electrocardiogram (ECG), electromyogram (EMG) and galvanic skin response (GSR) modules and accelerometers, a magnetometer and a gyroscope. The Android app is synchronized in two steps with a driving simulator that we previously developed using the Unity game engine to analyze driving security and efficiency. The Android app was tested with different sensors working simultaneously at various sampling rates and in different Android devices. We also tested the synchronized working of the driving simulator and the Android app with 25 people and analyzed the relation 
    
[^153]: 人本智能产品原型设计中的无代码AutoML：概念框架、潜力与限制

    Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations

    [https://arxiv.org/abs/2402.07933](https://arxiv.org/abs/2402.07933)

    本文评估了无代码AutoML作为解决人工智能产品原型设计中挑战的方案，提出了一个概念框架，通过案例研究证实了其对非专家的支持潜力。无代码AutoML的战略整合有助于实现可访问和可解释的原型设计，对于学术界、管理者和决策者具有益处。

    

    本文评估了无代码AutoML作为解决人工智能产品原型设计中的挑战的方案，这些产品的特点是不可预测性和对非专家的不可访问性，并提出了一个概念框架。人工智能产品的复杂性阻碍了无缝执行和跨学科合作，这对于人本智能产品至关重要。与行业和创新相关，它影响战略决策和投资风险的降低。目前的方法对人工智能产品想法的潜力和可行性提供了有限的见解。本研究采用设计科学研究的方法，识别挑战，并通过提出无代码AutoML的概念框架来解决这些挑战。案例研究证实了其对非专家的支持潜力，提供了一种结构化的人工智能产品开发方法。该框架有助于实现可访问和可解释的原型设计，使学术界、管理者和决策者受益。无代码AutoML的战略整合

    This paper evaluates No-Code AutoML as a solution for challenges in AI product prototyping, characterized by unpredictability and inaccessibility to non-experts, and proposes a conceptual framework. This complexity of AI products hinders seamless execution and interdisciplinary collaboration crucial for human-centered AI products. Relevant to industry and innovation, it affects strategic decision-making and investment risk mitigation. Current approaches provide limited insights into the potential and feasibility of AI product ideas. Employing Design Science Research, the study identifies challenges and integrates no-code AutoML as a solution by presenting a framework for AI product prototyping with No-code AutoML. A case study confirms its potential in supporting non-experts, offering a structured approach to AI product development. The framework facilitates accessible and interpretable prototyping, benefiting academia, managers, and decision-makers. Strategic integration of no-code Au
    
[^154]: 在强化学习中，模糊轨迹可视化用于可解释性

    Abstracted Trajectory Visualization for Explainability in Reinforcement Learning

    [https://arxiv.org/abs/2402.07928](https://arxiv.org/abs/2402.07928)

    强化学习(RL)模型中使用模糊轨迹可视化，使非RL专家能够推断出RL的行为模式。

    

    可解释的人工智能（XAI）已经展示出帮助强化学习（RL）从业者理解RL模型工作原理的潜力。然而，对于没有RL专业知识的用户（非RL专家），XAI的研究还不够。这导致非RL专家难以参与如何为人类和AI共存的社会设计RL模型的基本讨论。解决这个问题将使RL专家能够与非RL专家进行沟通，从而生成更适合我们社会的机器学习解决方案。我们认为，描述RL模型主要状态之间转换的模糊轨迹对于非RL专家构建代理的心理模型将是有用的。我们的初步结果表明，通过利用模糊轨迹的可视化，没有RL专业知识的用户能够推断出RL的行为模式。

    Explainable AI (XAI) has demonstrated the potential to help reinforcement learning (RL) practitioners to understand how RL models work. However, XAI for users who do not have RL expertise (non-RL experts), has not been studied sufficiently. This results in a difficulty for the non-RL experts to participate in the fundamental discussion of how RL models should be designed for an incoming society where humans and AI coexist. Solving such a problem would enable RL experts to communicate with the non-RL experts in producing machine learning solutions that better fit our society. We argue that abstracted trajectories, that depicts transitions between the major states of the RL model, will be useful for non-RL experts to build a mental model of the agents. Our early results suggest that by leveraging a visualization of the abstracted trajectories, users without RL expertise are able to infer the behavior patterns of RL.
    
[^155]: 基于可解释人工智能的老年人电子健康界面交互研究

    Research on Older Adults' Interaction with E-Health Interface Based on Explainable Artificial Intelligence

    [https://arxiv.org/abs/2402.07915](https://arxiv.org/abs/2402.07915)

    本研究基于可解释人工智能，探讨了老年人与电子健康界面的交互。结果显示，注入可解释人工智能的电子健康界面在消除年龄相关的数字鸿沟方面起到关键作用，并提出了直观可视化和简明解释等设计因素对于老年用户的重要性。本研究强调了XAI在电子健康领域的革命潜力。

    

    本文提出了一种综合的混合方法框架，通过各种样本的老年人的用户体验、可用性评估和深入访谈，结合可解释人工智能（XAI）方法。通过访谈收集老年人与电子健康界面的交互经验，并将其转化为可操作的数据库，然后利用XAI方法解释这些收集到的访谈结果。研究结果表明，注入XAI的电子健康界面能够在调查老年人与电子健康界面交互时，扮演重要的角色，促进年龄相关的数字鸿沟的消除。此外，研究还确定了重要的设计因素，如直观的可视化和简明扼要的解释，这些因素对于创建老年用户之间有效的人机交互工具至关重要。此外，本研究强调了XAI 在电子健康领域的革命潜力。

    This paper proposed a comprehensive mixed-methods framework with varied samples of older adults, including user experience, usability assessments, and in-depth interviews with the integration of Explainable Artificial Intelligence (XAI) methods. The experience of older adults' interaction with the Ehealth interface is collected through interviews and transformed into operatable databases whereas XAI methods are utilized to explain the collected interview results in this research work. The results show that XAI-infused e-health interfaces could play an important role in bridging the age-related digital divide by investigating elders' preferences when interacting with E-health interfaces. Furthermore, the study identifies important design factors, such as intuitive visualization and straightforward explanations, that are critical for creating efficient Human Computer Interaction (HCI) tools among older users. Furthermore, this study emphasizes the revolutionary potential of XAI in e-heal
    
[^156]: 通过最优传输实现元剪枝

    Towards Meta-Pruning via Optimal Transport

    [https://arxiv.org/abs/2402.07839](https://arxiv.org/abs/2402.07839)

    本文提出了一种名为Intra-Fusion的新方法，通过模型融合和最优传输的概念实现了神经网络的元剪枝，该方法能够有效恢复准确度并避免精调工作，同时具有高效和有前景的特点。

    

    传统神经网络的结构剪枝通常依赖于识别和丢弃不重要的神经元，这种做法往往会导致显著的准确度损失，需要随后的精调工作。本文提出了一种名为Intra-Fusion的新方法，挑战了传统的剪枝范式。与现有方法只关注设计有意义的神经元重要性度量不同，Intra-Fusion重新定义了上层剪枝过程。通过利用模型融合和最优传输的概念，我们利用给定的不可知重要性度量，得到了更有效的稀疏模型表示。值得注意的是，我们的方法在不需要资源密集型的精调的情况下实现了显著的准确度恢复，使其成为神经网络压缩的高效且有前景的工具。

    Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm. Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure. Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation. Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.   Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We 
    
[^157]: 用BAM进行图结构推断：引入双线性注意机制

    Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism

    [https://arxiv.org/abs/2402.07735](https://arxiv.org/abs/2402.07735)

    本论文提出了一种利用BAM进行图结构推断的方法。通过神经网络模型，通过变形的耦合模拟输入数据进行训练，仅需通过一次前向传递即可进行推断。通过利用结构方程模型和随机生成的多变量切比雪夫多项式来模拟训练数据，方法能够泛化到线性和各种非线性依赖关系。引入了双线性注意机制（BAM）来处理依赖关系，该机制在转换数据的协方差矩阵水平上运行，并尊重对称正定矩阵流形的几何特性。实证评估证明了方法的有效性和性能。

    

    在统计学和机器学习中，检测数据集中的依赖关系是一个核心挑战。我们提出了一种新颖的神经网络模型，用于监督图结构学习，即学习观测数据和它们的基本依赖结构之间的映射。该模型通过变形的耦合模拟输入数据进行训练，并且仅需通过训练网络进行一次前向传递即可进行推断。通过利用结构方程模型，并通过随机生成的多变量切比雪夫多项式来模拟训练数据，我们的方法展示了在线性和各种非线性依赖关系之间的强大泛化能力。我们引入了一种新的双线性注意机制（BAM），用于显式处理依赖信息，该机制在转换数据的协方差矩阵水平上运行，并尊重对称正定矩阵流形的几何特性。实证评估展示了方法的有效性和性能。

    In statistics and machine learning, detecting dependencies in datasets is a central challenge. We propose a novel neural network model for supervised graph structure learning, i.e., the process of learning a mapping between observational data and their underlying dependence structure. The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference. By leveraging structural equation models and employing randomly generated multivariate Chebyshev polynomials for the simulation of training data, our method demonstrates robust generalizability across both linear and various types of non-linear dependencies. We introduce a novel bilinear attention mechanism (BAM) for explicit processing of dependency information, which operates on the level of covariance matrices of transformed data and respects the geometry of the manifold of symmetric positive definite matrices. Empirical evaluation demonstrates th
    
[^158]: 对比分析ImageNet预训练的深度学习模型和DINOv2在医学图像分类中的表现

    Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification

    [https://arxiv.org/abs/2402.07595](https://arxiv.org/abs/2402.07595)

    本文通过对比分析ImageNet预训练的深度学习模型和DINOv2在医学图像分类中的表现，发现在临床数据集中，DINOv2的性能不如ImageNet-b。

    

    医学图像分析常常面临着数据稀缺的挑战。迁移学习在解决这个问题同时节约计算资源方面非常有效。最近出现的基础模型，如使用视觉变换器架构的DINOv2，为该领域提供了新的机会并引起了极大的关注。然而，DINOv2在临床数据上的表现仍需验证。本文中，我们使用三种临床模态的脑部MRI数据执行了一个胶质瘤分级任务。我们在迁移学习的上下文中比较了基于ImageNet和DINOv2的各种预训练深度学习模型的表现。我们关注了冻结机制对性能的影响。我们还在其他三种类型的公共数据集上验证了我们的发现：胸部放射学、眼底放射学和皮肤镜。我们的研究结果表明，在我们的临床数据集中，DINOv2的性能不如ImageNet-b。

    Medical image analysis frequently encounters data scarcity challenges. Transfer learning has been effective in addressing this issue while conserving computational resources. The recent advent of foundational models like the DINOv2, which uses the vision transformer architecture, has opened new opportunities in the field and gathered significant interest. However, DINOv2's performance on clinical data still needs to be verified. In this paper, we performed a glioma grading task using three clinical modalities of brain MRI data. We compared the performance of various pre-trained deep learning models, including those based on ImageNet and DINOv2, in a transfer learning context. Our focus was on understanding the impact of the freezing mechanism on performance. We also validated our findings on three other types of public datasets: chest radiography, fundus radiography, and dermoscopy. Our findings indicate that in our clinical dataset, DINOv2's performance was not as strong as ImageNet-b
    
[^159]: 基于人工神经网络行为可解释性的逃避攻击的拓扑保护

    Topological Safeguard for Evasion Attack based on the Interpretability of Artificial Neural Network Behavior

    [https://arxiv.org/abs/2402.07480](https://arxiv.org/abs/2402.07480)

    本文提出了一种基于人工神经网络行为解释性的拓扑保护方法，用于对抗逃避攻击。在过去几年中，深度学习技术的应用日益广泛，但也带来了新的网络安全威胁。逃避攻击是其中一种常见攻击，本文旨在设计一种能够对抗该攻击的有效防御方法。

    

    在过去几年中，深度学习技术已经在不同领域提出，为每个领域带来了许多进展，但这也带来了关于网络安全方面的新威胁。这些实施的模型带来了与深度学习技术相关的几种漏洞。此外，它们允许利用这些模型，获得私人信息，甚至修改模型的决策。因此，对这些漏洞/攻击进行研究并设计防御措施以避免或对抗它们的兴趣在研究中日益突出。特别是，著名的逃避攻击正在被研究人员分析，因此在文献中可以找到几种避免此威胁的防御措施。自L-BFG算法提出以来，这种威胁一直关注研究界。然而，由于没有适用于所有已知逃避算法的完美防御，因此仍在不断开发新的巧妙对策。

    In the last years, Deep Learning technology has been proposed in different fields, bringing many advances in each of them, but identifying new threats in these solutions regarding cybersecurity. Those implemented models have brought several vulnerabilities associated with Deep Learning technology. Moreover, those allow taking advantage of the implemented model, obtaining private information, and even modifying the model's decision-making. Therefore, interest in studying those vulnerabilities/attacks and designing defenses to avoid or fight them is gaining prominence among researchers. In particular, the widely known evasion attack is being analyzed by researchers; thus, several defenses to avoid such a threat can be found in the literature. Since the presentation of the L-BFG algorithm, this threat concerns the research community. However, it continues developing new and ingenious countermeasures since there is no perfect defense for all the known evasion algorithms. In this work, a no
    
[^160]: 价值装载问题的激素适应方法：预防回形针启示录？

    A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?

    [https://arxiv.org/abs/2402.07462](https://arxiv.org/abs/2402.07462)

    我们提出了HALO法规模式，使用激素分析来调节人工智能的行为模式，以解决价值装载问题中的“回形针最大化器”场景。

    

    价值装载问题对于研究人员来说是一个重要的挑战，他们旨在创建与人类价值观和偏好相一致的人工智能系统。该问题需要一种方法来定义和规范人工智能行为的安全和最优限制。在这项工作中，我们提出了HALO（激素适应通过对手过程）这个法规模式，它使用激素分析来调节人工智能的行为模式。行为激素适应是一种现象，低频率的行为具有益处，而高频率的行为则有害。通过将行为建模为变态对手过程，我们可以使用行为频率响应分析（BFRA）或行为计数响应分析（BCRA）来量化可重复行为的激素限制。我们展示了如何使用HALO来解决“回形针最大化器”场景，这是一个思想实验，其中一个未受管制的人工智能任务是将所有物质转化为回形针。

    The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences. This problem requires a method to define and regulate safe and optimal limits of AI behaviors. In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful. By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors. We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips. 
    
[^161]: 回归树用于快速和自适应的预测区间

    Regression Trees for Fast and Adaptive Prediction Intervals

    [https://arxiv.org/abs/2402.07357](https://arxiv.org/abs/2402.07357)

    该论文提出了一种新的、与模型无关的方法族，用于校准具有局部覆盖保证的回归问题的预测区间。这种方法利用回归树和随机森林训练来创建最粗糙的特征空间划分，以近似条件覆盖，提供了准确、快速和自适应的预测区间。

    

    预测模型会犯错，因此需要量化与其预测相关的不确定性。符合性推断已经成为一种强大的工具，可以在点预测周围创建统计上有效的预测区域，但是它在回归问题上的朴素应用会产生非自适应的区域。新的符合性得分，通常依赖于分位数回归器或条件密度估计器，旨在解决这个限制。虽然它们在创建预测带方面很有用，但这些得分与量化任意预测模型周围的不确定性的原始目标脱节。本文提出了一种新的、与模型无关的方法族，用于校准具有局部覆盖保证的回归问题的预测区间。我们的方法是基于追求最粗糙的特征空间划分来近似条件覆盖。我们通过对符合性得分进行回归树和随机森林的训练来创建这个划分。我们的提议将回归树和随机森林应用于符合性推断的新领域，以提供准确、快速和自适应的预测区间。

    Predictive models make mistakes. Hence, there is a need to quantify the uncertainty associated with their predictions. Conformal inference has emerged as a powerful tool to create statistically valid prediction regions around point predictions, but its naive application to regression problems yields non-adaptive regions. New conformal scores, often relying upon quantile regressors or conditional density estimators, aim to address this limitation. Although they are useful for creating prediction bands, these scores are detached from the original goal of quantifying the uncertainty around an arbitrary predictive model. This paper presents a new, model-agnostic family of methods to calibrate prediction intervals for regression problems with local coverage guarantees. Our approach is based on pursuing the coarsest partition of the feature space that approximates conditional coverage. We create this partition by training regression trees and Random Forests on conformity scores. Our proposal
    
[^162]: 从均场稳态分布中采样

    Sampling from the Mean-Field Stationary Distribution

    [https://arxiv.org/abs/2402.07355](https://arxiv.org/abs/2402.07355)

    本文研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，并提出了一种解耦的方法。该方法能够在多种情况下提供改进的保证，包括在均场区域优化某些双层神经网络的更好保证。

    

    我们研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，或者等价地，即包含交互项的概率测度空间上的最小化函数的复杂性。我们的主要洞察是将这个问题的两个关键方面解耦：(1) 通过有限粒子系统逼近均场SDE，通过时间均匀传播混沌，和(2) 通过标准对数凹抽样器从有限粒子稳态分布中采样。我们的方法在概念上更简单，其灵活性允许结合用于算法和理论的最新技术。这导致在许多设置中提供了改进的保证，包括在均场区域优化某些双层神经网络的更好保证。

    We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.
    
[^163]: HyperBERT:将混合超图感知层与语言模型用于文本属性超图上的节点分类

    HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs

    [https://arxiv.org/abs/2402.07309](https://arxiv.org/abs/2402.07309)

    本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。

    

    超图通过复杂的拓扑结构标记，表达多个实体之间的高阶相互作用，其中超边扮演重要角色。最近，基于超图的深度学习方法在学习文本属性超图上的节点分类问题中引起了越来越多的研究关注。然而，现有方法往往难以同时捕捉超图结构信息的全部内容和节点属性中的丰富语言属性，这在很大程度上影响了它们的效果和泛化能力。为了克服这些挑战，我们探索了如何通过为节点分类任务进一步增强预训练的BERT模型，引入专门的超图感知层。这些层将高阶结构归纳偏差引入语言模型中，从而提高模型利用超图结构中的高阶上下文信息和文本中的语义信息的能力。

    Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
    
[^164]: 大型语言模型如何在诚实与帮助之间进行权衡？

    How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?

    [https://arxiv.org/abs/2402.07282](https://arxiv.org/abs/2402.07282)

    本文研究了如何在大型语言模型中权衡诚实和帮助性，在实验中发现强化学习改善了诚实和帮助性，而链式思维提示则偏向于帮助性。研究结果还展示了GPT-4 Turbo对对话框架和听众决策背景的敏感性。这些发现揭示了大型语言模型内化的对话价值观，并暗示零-shot提示可以在一定程度上引导这些抽象价值观。

    

    在日常交流中，人们经常为了最大限度地帮助听众而近似真相，例如约略时间或省略细节。大型语言模型（LLMs）如何处理这种微妙的权衡？为了回答这个问题，我们使用心理模型和旨在描述人类行为的实验来分析LLMs。我们测试了一系列LLMs，并探讨了优化人类偏好或推理时思考对这些权衡的影响。我们发现，从人类反馈中的强化学习改善了诚实和帮助性，而链式思维提示使LLMs偏向于帮助性而不是诚实。最后，GPT-4 Turbo展示了类似人类的回应模式，包括对对话框架和听众决策背景的敏感性。我们的研究结果揭示了LLMs内化的对话价值观，并暗示即使这些抽象价值观也可以在零-shot提示下在一定程度上被引导。

    In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.
    
[^165]: 面向扩散生成模型的快速随机采样方法

    Towards Fast Stochastic Sampling in Diffusion Generative Models

    [https://arxiv.org/abs/2402.07211](https://arxiv.org/abs/2402.07211)

    本文提出了一种在扩散生成模型中进行快速随机采样的方法，通过对分裂积分器进行原则性修改，实现了更高的采样效率。在CIFAR-10数据集上进行实验，100次网络函数评估下的FID分数为2.36。

    

    扩散模型在推理时生成样本的速度较慢。尽管最近有一些努力在改善扩散模型的随机采样效率，但仍然有待改进。我们提出了基于分裂积分器的预训练扩散模型的快速随机采样方法。分裂积分器通常在分子动力学中使用，通过巧妙地在涉及数据、辅助或噪声变量的数值更新之间交替来提高采样效率。然而，我们发现对于快速采样，简单应用分裂积分器是次优的。因此，我们提出了几种原则上修改了简单分裂采样器以提高采样效率的方法，并将得到的采样器称为减小分裂积分器。在CIFAR-10数据集上使用相空间朗之万扩散 (PSLD) [Pandey \& Mandt, 2023] 的背景下，我们的随机采样器在仅进行100次网络函数评估后，实现了2.36的FID分数。

    Diffusion models suffer from slow sample generation at inference time. Despite recent efforts, improving the sampling efficiency of stochastic samplers for diffusion models remains a promising direction. We propose Splitting Integrators for fast stochastic sampling in pre-trained diffusion models in augmented spaces. Commonly used in molecular dynamics, splitting-based integrators attempt to improve sampling efficiency by cleverly alternating between numerical updates involving the data, auxiliary, or noise variables. However, we show that a naive application of splitting integrators is sub-optimal for fast sampling. Consequently, we propose several principled modifications to naive splitting samplers for improving sampling efficiency and denote the resulting samplers as Reduced Splitting Integrators. In the context of Phase Space Langevin Diffusion (PSLD) [Pandey \& Mandt, 2023] on CIFAR-10, our stochastic sampler achieves an FID score of 2.36 in only 100 network function evaluations 
    
[^166]: 索crates怀疑的回声：在校准的证据增强学习中接受不确定性

    Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning

    [https://arxiv.org/abs/2402.07107](https://arxiv.org/abs/2402.07107)

    这篇论文提出了一个新的统计方法，利用深度Q网络和分位数回归来在模型-free的分布式强化学习中引入不确定性，该方法通过结合深度证据学习和基于合规推理原则的分位数校准，提供了全局不确定性的显式、无样本计算，具有更高的计算和统计效率，并成功处理了超出分布范围的观测数据。

    

    我们提出了一种新颖的统计方法，用于在基于模型的分布强化学习中引入不确定性意识，涉及基于分位数回归的深度Q网络。提出的算法$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$旨在解决在随机环境中分别估计aleatoric和epistemic不确定性所面临的关键挑战。它将深度证据学习与基于合规推理原则的分位数校准相结合，提供了显式的、无样本计算的$\textit{全局}$不确定性，而不是基于简单方差的$\textit{局部}$估计，克服了传统方法在计算和统计效率以及处理超出分布范围的观测数据方面的局限性。在一套小型化的Atari游戏（即MinAtar）上进行测试，CEQR-DQN在得分和学习速度方面超过了类似的现有框架。它能够严谨地处理外部数据观测，并提供更高的计算和统计效率。

    We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously e
    
[^167]: 非线性融合在联邦学习中的应用：基于超网络的联邦领域泛化方法

    Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization

    [https://arxiv.org/abs/2402.06974](https://arxiv.org/abs/2402.06974)

    本文提出了一种基于超网络的联邦融合算法hFedF，用于解决联邦领域泛化问题。该算法通过非线性融合客户模型，实现了对底层数据分布的全面理解，并在联邦学习中个性化和泛化之间达到了优秀的平衡。

    

    联邦学习（FL）作为一种保护数据隐私的多个客户共同训练共享全局模型的有前途的范式出现。为了创建一个稳健和实用的FL框架，扩展其良好泛化能力以适应未知领域是至关重要的，这个问题被称为联邦领域泛化（FDG），目前尚未得到充分探索。我们提出了一种创新的联邦算法，称为hFedF（基于超网络的联邦融合），旨在弥合个性化和泛化之间的性能差距，能够处理各种程度的领域转移。基本上，超网络支持对客户模型进行非线性融合，从而全面了解底层数据分布。我们对个性化和泛化之间的权衡进行了广泛的讨论，并提供了对FL中强大基准测试的新见解。所提出的算法在三个广泛使用的DG数据集上优于强大的基准测试。

    Federated Learning (FL) has emerged as a promising paradigm in which multiple clients collaboratively train a shared global model while preserving data privacy. To create a robust and practicable FL framework, it is crucial to extend its ability to generalize well to unseen domains - a problem referred to as federated Domain Generalization (FDG), being still under-explored. We propose an innovative federated algorithm, termed hFedF for hypernetwork-based Federated Fusion, designed to bridge the performance gap between generalization and personalization, capable of addressing various degrees of domain shift. Essentially, the hypernetwork supports a non-linear fusion of client models enabling a comprehensive understanding of the underlying data distribution. We encompass an extensive discussion and provide novel insights into the tradeoff between personalization and generalization in FL. The proposed algorithm outperforms strong benchmarks on three widely-used data sets for DG in an exce
    
[^168]: 使用谈判能力的分布式基础设施高效资源调度

    Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities

    [https://arxiv.org/abs/2402.06938](https://arxiv.org/abs/2402.06938)

    这项研究介绍了一种使用谈判能力的分布式基础设施高效资源调度的方法，通过引入一种基于模糊逻辑的代理自动谈判系统，优化云计算提供商和客户之间的协议，以提高效率。

    

    在过去的几十年里，信息和互联网技术的快速发展催生了大量的数据和信息。信息爆炸推动许多企业或个人寻求租用云计算基础设施来将他们的应用程序放置在云中。然而，云计算提供商和客户之间达成的协议通常不高效。许多因素影响效率，如提供商云计算基础设施的闲置和对客户的额外成本。一个可能的解决方案是引入一种综合的、谈判类的博弈，并根据谈判结果安排资源。我们提出了一种基于模糊逻辑的基于代理的自动谈判系统用于资源调度。所提出的方法可以完成一对一的自动谈判过程，并为提供商和客户生成最优的报价。我们比较了不同成员函数、模糊规则集和网络拓扑结构对资源调度的影响。

    In the past few decades, the rapid development of information and internet technologies has spawned massive amounts of data and information. The information explosion drives many enterprises or individuals to seek to rent cloud computing infrastructure to put their applications in the cloud. However, the agreements reached between cloud computing providers and clients are often not efficient. Many factors affect the efficiency, such as the idleness of the providers' cloud computing infrastructure, and the additional cost to the clients. One possible solution is to introduce a comprehensive, bargaining game (a type of negotiation), and schedule resources according to the negotiation results. We propose an agent-based auto-negotiation system for resource scheduling based on fuzzy logic. The proposed method can complete a one-to-one auto-negotiation process and generate optimal offers for the provider and client. We compare the impact of different member functions, fuzzy rule sets, and ne
    
[^169]: 具有辨别性对抗学习的论文

    Discriminative Adversarial Unlearning

    [https://arxiv.org/abs/2402.06864](https://arxiv.org/abs/2402.06864)

    该论文提出了一种基于最小最大优化范式的机器反学习框架，利用强大的成员推断攻击来实现反学习，同时保持其总体性能，并增强了反学习能力。

    

    我们引入了一种新颖的机器反学习框架，基于最小最大优化范式的已建立原则。我们利用强大的成员推断攻击（MIA）的能力，以促进从训练模型中反学习特定样本。我们考虑了两个网络的场景，攻击者$\mathbf{A}$和经过训练的防御者 $\mathbf{D}$在对抗目标下相互对抗，其中攻击者旨在揭示数据的信息以推断成员身份，而防御者在反击中进行反学习，同时保持其总体性能。算法可以使用反向传播进行端到端训练，遵循已知的迭代最小最大方法来更新攻击者和防御者。我们还加入了自监督目标，有效地解决了遗忘集和验证集之间的特征空间差异，增强了反学习能力

    We introduce a novel machine unlearning framework founded upon the established principles of the min-max optimization paradigm. We capitalize on the capabilities of strong Membership Inference Attacks (MIA) to facilitate the unlearning of specific samples from a trained model. We consider the scenario of two networks, the attacker $\mathbf{A}$ and the trained defender $\mathbf{D}$ pitted against each other in an adversarial objective, wherein the attacker aims at teasing out the information of the data to be unlearned in order to infer membership, and the defender unlearns to defend the network against the attack, whilst preserving its general performance. The algorithm can be trained end-to-end using backpropagation, following the well known iterative min-max approach in updating the attacker and the defender. We additionally incorporate a self-supervised objective effectively addressing the feature space discrepancies between the forget set and the validation set, enhancing unlearnin
    
[^170]: 获取、合并、预测：通过数据湖增强表格

    Retrieve, Merge, Predict: Augmenting Tables with Data Lakes

    [https://arxiv.org/abs/2402.06282](https://arxiv.org/abs/2402.06282)

    本文通过对数据湖中的数据发现进行深入分析，着重于表格增强，提出了准确检索连接候选人的重要性和简单合并方法的效率，以及现有解决方案的好处和局限性。

    

    我们对数据湖中的数据发现进行了深入分析，重点是给定机器学习任务的表格增强。我们分析了三个主要步骤中使用的替代方法：检索可连接的表格、合并信息和预测结果表格。作为数据湖，本文使用了YADL（另一个数据湖）-我们开发的一种用于基准测试此数据发现任务的新型数据集-和Open Data US，一个被引用的真实数据湖。通过对这两个数据湖的系统性探索，我们的研究概述了准确检索连接候选人的重要性以及简单合并方法的效率。我们报告了现有解决方案的好处和局限性，旨在指导未来的研究。

    We present an in-depth analysis of data discovery in data lakes, focusing on table augmentation for given machine learning tasks. We analyze alternative methods used in the three main steps: retrieving joinable tables, merging information, and predicting with the resultant table. As data lakes, the paper uses YADL (Yet Another Data Lake) -- a novel dataset we developed as a tool for benchmarking this data discovery task -- and Open Data US, a well-referenced real data lake. Through systematic exploration on both lakes, our study outlines the importance of accurately retrieving join candidates and the efficiency of simple merging methods. We report new insights on the benefits of existing solutions and on their limitations, aiming at guiding future research in this space.
    
[^171]: 学习变得高效：在大型语言模型中构建结构化稀疏性

    Learn To be Efficient: Build Structured Sparsity in Large Language Models

    [https://arxiv.org/abs/2402.06126](https://arxiv.org/abs/2402.06126)

    本文通过引入一种新的算法"Learn-To-be-Efficient(LTE)"，提出了在大型语言模型(LLM)中构建结构化稀疏性的方法。该方法通过训练高效意识的LLM学习激活更少的神经元，取得更好的稀疏性和性能折衷。

    

    大型语言模型(LLM)以其十亿级参数取得了显著的成功，但它们产生了高昂的推理开销。在LLM中出现的激活稀疏性为通过仅涉及部分参数进行推理提供了一种自然的方法来减少这种成本。现有方法只关注利用这种自然形成的激活稀疏性，忽视了进一步放大这种固有稀疏性的潜力。本文中，我们假设LLM可以通过实现更结构化的激活稀疏性来学习高效。为实现这一目标，我们引入了一种新颖的算法"Learn-To-be-Efficient(LTE)", 旨在训练高效意识的LLM学习激活更少的神经元，并在稀疏性和性能之间取得更好的折衷。此外，与主要关注基于ReLU模型的SOTA MoEfication方法不同，LTE还可以应用于像GPT和LLaMA这样具有软激活函数的LLM。我们在四个模型和十一个数据集上评估了LTE。

    Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
    
[^172]: Transformer语言模型在算法学习上的限制

    Limits of Transformer Language Models on Algorithmic Learning

    [https://arxiv.org/abs/2402.05785](https://arxiv.org/abs/2402.05785)

    Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。

    

    我们分析了Transformer语言模型在学习离散算法方面的能力。为此，我们引入了两个要求组合多个离散子任务的新任务。我们通过从头开始训练LLaMA模型和在GPT-4和Gemini上提示来衡量学习学习原语的组合。我们观察到，目前最先进的Transformer语言模型的组合能力非常有限，并且在样本规模方面比为新的算法组合重新学习所有子任务效果更差。我们还提出了一个复杂性理论的定理，证明了记忆前馈模型上的梯度下降可以指数级地浪费数据。

    We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
    
[^173]: 通过并行观测预测改进基于令牌的世界模型

    Improving Token-Based World Models with Parallel Observation Prediction

    [https://arxiv.org/abs/2402.05643](https://arxiv.org/abs/2402.05643)

    该论文提出了一种改进基于令牌的世界模型的方法，通过引入并行观测预测机制（POP）来解决想象过程中出现的瓶颈问题。通过在一个新型TBWM代理中应用POP，想象速度提高了15.4倍，在不到12小时的训练时间内在Atari 100K基准测试中取得了超人类的表现。

    

    受到将Transformer应用于离散符号序列的成功启发，最近提出了基于令牌的世界模型（TBWMs）作为高效样本方法。在TBWMs中，世界模型将代理经验作为一种类似语言的令牌序列进行消耗，其中每个观测构成一个子序列。然而，在想象过程中，通过令牌逐个生成下一个观测的串行方式导致了严重的瓶颈问题，导致训练时间长、GPU利用率低和表示能力有限。为了解决这个瓶颈问题，我们设计了一种新颖的并行观测预测（POP）机制。POP通过一种针对我们的强化学习环境设计的新型前向模式来扩充了保持网络（RetNet）。我们将POP集成到一种名为REM（保持环境模型）的新型TBWM代理中，展示了比以前的TBWMs快15.4倍的想象能力。REM在Atari 100K基准测试的26个游戏中的12个游戏中达到超越人类水平的性能，并且在不到12小时的训练时间内完成训练。

    Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
    
[^174]: 关于分散推断模型的扩散模型：基准测试和改进随机控制和采样

    On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling

    [https://arxiv.org/abs/2402.05098](https://arxiv.org/abs/2402.05098)

    本研究探讨了训练扩散模型以从给定分布中采样的问题，并针对随机控制和采样提出了一种新的探索策略，通过基准测试比较了不同推断方法的相对优劣，并对过去的工作提出了质疑。

    

    我们研究了训练扩散模型以从给定的非标准化密度或能量函数分布中采样的问题。我们对几种扩散结构推断方法进行了基准测试，包括基于模拟的变分方法和离策略方法（连续生成流网络）。我们的结果揭示了现有算法的相对优势，同时对过去的研究提出了一些质疑。我们还提出了一种新颖的离策略方法探索策略，基于目标空间中的局部搜索和回放缓冲区的使用，并证明它可以改善各种目标分布上的样本质量。我们研究的采样方法和基准测试的代码已公开在https://github.com/GFNOrg/gfn-diffusion，作为未来在分散推断模型上工作的基础。

    We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion models for amortized inference.
    
[^175]: 在一般希尔伯特空间中使用随机梯度下降学习算子

    Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces

    [https://arxiv.org/abs/2402.04691](https://arxiv.org/abs/2402.04691)

    本研究在一般希尔伯特空间中使用随机梯度下降（SGD）学习算子，提出了适用于目标算子的规则条件，并建立了SGD算法的收敛速度上界，同时展示了对于非线性算子学习的有效性及线性近似收敛特性。

    

    本研究探讨了利用随机梯度下降（SGD）在一般希尔伯特空间中学习算子的方法。我们提出了针对目标算子的弱和强规则条件，以描述其内在结构和复杂性。在这些条件下，我们建立了SGD算法的收敛速度的上界，并进行了极小值下界分析，进一步说明我们的收敛分析和规则条件定量地刻画了使用SGD算法解决算子学习问题的可行性。值得强调的是，我们的收敛分析对于非线性算子学习仍然有效。我们证明了SGD估计器将收敛于非线性目标算子的最佳线性近似。此外，将我们的分析应用于基于矢量值和实值再生核希尔伯特空间的算子学习问题，产生了新的收敛结果，从而完善了现有文献的结论。

    This study investigates leveraging stochastic gradient descent (SGD) to learn operators between general Hilbert spaces. We propose weak and strong regularity conditions for the target operator to depict its intrinsic structure and complexity. Under these conditions, we establish upper bounds for convergence rates of the SGD algorithm and conduct a minimax lower bound analysis, further illustrating that our convergence analysis and regularity conditions quantitatively characterize the tractability of solving operator learning problems using the SGD algorithm. It is crucial to highlight that our convergence analysis is still valid for nonlinear operator learning. We show that the SGD estimator will converge to the best linear approximation of the nonlinear target operator. Moreover, applying our analysis to operator learning problems based on vector-valued and real-valued reproducing kernel Hilbert spaces yields new convergence results, thereby refining the conclusions of existing litera
    
[^176]: 密集乘法物理信息神经网络

    Densely Multiplied Physics Informed Neural Network

    [https://arxiv.org/abs/2402.04390](https://arxiv.org/abs/2402.04390)

    该论文通过改进神经网络架构，提出了一种密集乘法物理信息神经网络（DM-PINN）架构，它有效利用隐藏层的输出，显著提高了PINN的准确性和性能。

    

    尽管物理信息神经网络（Physics-Informed Neural Networks, PINNs）在处理非线性偏微分方程（PDEs）方面显示出巨大潜力，但常常会出现精度不足或获取不正确结果的问题。与大多数现有的解决方案不同，该论文改进了神经网络架构以提高PINN的性能。我们提出了一种密集乘法PINN（DM-PINN）架构，它将隐藏层的输出与所有后面的隐藏层的输出相乘。在不引入更多可训练参数的情况下，该有效机制可以显著提高PINN的准确性。所提出的架构在四个基准示例（Allan-Cahn方程，Helmholtz方程，Burgers方程和1D对流方程）上进行了评估。将所提出的架构与不同的PINN结构进行比较，证明了其卓越的性能。

    Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes. Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN. We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers. Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs. The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation). Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of 
    
[^177]: MolTC: 在语言模型中进行分子关系建模

    MolTC: Towards Molecular Relational Modeling In Language Models

    [https://arxiv.org/abs/2402.03781](https://arxiv.org/abs/2402.03781)

    本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。

    

    分子关系学习（MRL）旨在理解分子之间的相互作用，在推进生物化学研究方面起到了关键作用。最近，大型语言模型（LLMs）的采用已成为一种有效和高效的MRL方法，这些模型以其庞大的知识存储库和先进的逻辑推理能力而闻名。尽管具有潜力，但这些方法主要依赖于文本数据，因此没有充分利用分子图中固有的丰富结构信息。此外，缺乏统一的框架加剧了信息的浪费，因为它阻碍了在不同数据集之间共享学习到的相互作用理由。为了解决这些挑战，本研究提出了一种基于LLM的多模态框架，用于根据思维链（CoT）理论对分子相互作用进行预测，称为MolTC，它可以高效地整合分子对的丰富图形信息。

    Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
    
[^178]: 我们能去掉自适应梯度方法中的平方根吗？一个二阶角度的研究

    Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective

    [https://arxiv.org/abs/2402.03496](https://arxiv.org/abs/2402.03496)

    移除自适应方法中的平方根可以在卷积结构上减小与SGD的泛化差距，同时保持在transformers上的性能。

    

    自适应梯度优化器如Adam(W)是许多深度学习结构（如transformers）的默认训练算法。它们的对角先验基于梯度外积，通过平方根加入到参数更新中。虽然这些方法通常被称为近似的二阶方法，但平方根表示了一个根本性的区别。在这项研究中，我们研究了在去掉平方根后自适应方法的行为如何变化，即加强它们的二阶动机。令人惊讶的是，我们发现这种去掉平方根的自适应方法能够在卷积结构上缩小与SGD的泛化差距，同时保持了在transformers上基于平方根的方法的性能。二阶角度对于开发具有非对角先验的自适应方法也具有实际好处。与像Shampoo这样基于平方根的对应方法不同，它们不需要数值不稳定的矩阵平方。

    Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square
    
[^179]: 视觉-语言模型为强化学习提供可提示的表示

    Vision-Language Models Provide Promptable Representations for Reinforcement Learning

    [https://arxiv.org/abs/2402.02651](https://arxiv.org/abs/2402.02651)

    本论文提出一种利用预训练的视觉-语言模型作为可提示的表示，为强化学习提供世界知识，使得代理能够更快地学习新的行为。在实验中，我们发现使用这种表示训练的策略在复杂环境下表现更好，优于通用图像表示和遵循指示的方法。

    

    人类可以通过利用背景世界知识快速学习新的行为。相比之下，利用强化学习训练的代理通常需要从零开始学习行为。因此，我们提出了一种新的方法，利用在互联网规模数据上预训练的视觉-语言模型（VLMs）中编码的大量通用和可索引的世界知识来进行具象的强化学习。我们通过将VLMs用作可提示表示来初始化策略：这些嵌入在视觉观察中具有基础，并根据VLM的内部知识编码语义特征，通过提供任务上下文和辅助信息来触发。我们在Minecraft和Habitat中的视觉复杂、长期的强化学习任务上评估了我们的方法。我们发现，使用通用型VLMs提取的嵌入训练的策略胜过使用通用的、不可提示的图像嵌入训练的策略。我们还发现我们的方法胜过遵循指示的元策略。

    Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following met
    
[^180]: 你的扩散模型实际上是一个可证明鲁棒的分类器

    Your Diffusion Model is Secretly a Certifiably Robust Classifier

    [https://arxiv.org/abs/2402.02316](https://arxiv.org/abs/2402.02316)

    这项研究提出了一种新的扩散分类器家族，称为噪声扩散分类器（NDCs），其具有最新的可证明的鲁棒性。通过将扩散分类器推广到分类高斯受损数据，并将其与随机平滑技术相结合，构建了具有非常量Lipschitzness的平滑分类器。这些NDCs显示出卓越的认证鲁棒性。

    

    近期，扩散模型被作为鲁棒分类的生成器分类器所采用。然而，对于扩散分类器鲁棒性的综合理论理解仍然缺乏，这让我们怀疑它们是否会容易受到未来更强攻击的影响。在本研究中，我们提出了一种新的扩散分类器家族，命名为噪声扩散分类器（NDCs），其具有最新的可证明的鲁棒性。具体来说，我们通过推导这些分布的证据下界（ELBOs），利用ELBO近似似然度量，并使用贝叶斯定理计算分类概率，将扩散分类器推广到分类高斯受损数据。我们将这些推广的扩散分类器与随机平滑技术相结合，构建具有非常量Lipschitzness的平滑分类器。实验结果表明我们提出的NDCs在鲁棒性方面具有卓越的认证能力。值得注意的是，我们是第一个达到80%的...

    Diffusion models are recently employed as generative classifiers for robust classification. However, a comprehensive theoretical understanding of the robustness of diffusion classifiers is still lacking, leading us to question whether they will be vulnerable to future stronger attacks. In this study, we propose a new family of diffusion classifiers, named Noised Diffusion Classifiers~(NDCs), that possess state-of-the-art certified robustness. Specifically, we generalize the diffusion classifiers to classify Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. We integrate these generalized diffusion classifiers with randomized smoothing to construct smoothed classifiers possessing non-constant Lipschitzness. Experimental results demonstrate the superior certified robustness of our proposed NDCs. Notably, we are the first to achieve 80\%
    
[^181]: 并行大规模排序和选择问题的样本高效聚类及征服方法

    Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection

    [https://arxiv.org/abs/2402.02196](https://arxiv.org/abs/2402.02196)

    我们提出了一种新颖的并行大规模排序和选择问题的聚类及征服方法，通过利用相关信息进行聚类以提高样本效率，在大规模AI应用中表现优异。

    

    我们提出了一种新颖的"聚类和征服"方法，用于解决并行大规模排序和选择问题，通过利用相关信息进行聚类，以打破样本效率的瓶颈。在并行计算环境中，基于相关性的聚类可以实现O(p)的样本复杂度减少速度，这是理论上可达到的最佳减少速度。我们提出的框架是通用的，在固定预算和固定精度的范式下，可以无缝集成各种常见的排序和选择方法。它可以在无需高精确度相关估计和精确聚类的情况下实现改进。在大规模人工智能应用中，如神经结构搜索，我们的无筛选版本的方法惊人地超过了完全顺序化的基准，表现出更高的样本效率。这表明利用有价值的结构信息，如相关性，是绕过传统方法的一条可行路径。

    We propose novel "clustering and conquer" procedures for the parallel large-scale ranking and selection (R&S) problem, which leverage correlation information for clustering to break the bottleneck of sample efficiency. In parallel computing environments, correlation-based clustering can achieve an $\mathcal{O}(p)$ sample complexity reduction rate, which is the optimal reduction rate theoretically attainable. Our proposed framework is versatile, allowing for seamless integration of various prevalent R&S methods under both fixed-budget and fixed-precision paradigms. It can achieve improvements without the necessity of highly accurate correlation estimation and precise clustering. In large-scale AI applications such as neural architecture search, a screening-free version of our procedure surprisingly surpasses fully-sequential benchmarks in terms of sample efficiency. This suggests that leveraging valuable structural information, such as correlation, is a viable path to bypassing the trad
    
[^182]: PresAIse，一种企业预测性人工智能解决方案

    PresAIse, An Enterprises Prescriptive AI Solution

    [https://arxiv.org/abs/2402.02006](https://arxiv.org/abs/2402.02006)

    PresAIse是一种企业预测性人工智能解决方案，通过提供因果推断方法、可解释的决策制定方法和大型语言模型（LLMs）的集成，旨在解决企业预测性人工智能面临的数据限制、建议可解释性和数据科学家与业务用户之间的合作障碍等挑战。

    

    预测性人工智能代表了决策制定中的一次转型性变革，提供因果洞察和可操作的建议。尽管具有巨大潜力，但企业采用常常面临几个挑战。首先，观察数据的限制使得准确的因果推断成为了良好决策制定的前提条件。其次，建议的可解释性对于企业决策制定至关重要。第三个挑战是数据科学家和业务用户之间的隔离，阻碍了有效的合作。本文概述了IBM研究的一项倡议，旨在通过提供一套预测性 AI 解决方案来解决其中一些挑战。利用来自各种研究论文的见解，解决方案套件包括可扩展的因果推断方法、可解释的决策制定方法以及通过对话框架来弥合沟通隔阂的大型语言模型 (LLMs) 的集成。

    Prescriptive AI represents a transformative shift in decision-making, offering causal insights and actionable recommendations. Despite its huge potential, enterprise adoption often faces several challenges. The first challenge is caused by the limitations of observational data for accurate causal inference which is typically a prerequisite for good decision-making. The second pertains to the interpretability of recommendations, which is crucial for enterprise decision-making settings. The third challenge is the silos between data scientists and business users, hindering effective collaboration. This paper outlines an initiative from IBM Research, aiming to address some of these challenges by offering a suite of prescriptive AI solutions. Leveraging insights from various research papers, the solution suite includes scalable causal inference methods, interpretable decision-making approaches, and the integration of large language models (LLMs) to bridge communication gaps via a conversati
    
[^183]: CroissantLLM: 一个真正的双语法语-英语语言模型

    CroissantLLM: A Truly Bilingual French-English Language Model

    [https://arxiv.org/abs/2402.00786](https://arxiv.org/abs/2402.00786)

    CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。

    

    我们介绍了CroissantLLM，这是一个在3T个英语和法语标记上预训练的13亿语言模型，为研究和工业社区带来了一种高性能的、完全开源的双语模型，可以在消费级本地硬件上快速运行。为此，我们首次尝试使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集来训练一种内在双语的模型。我们发布了训练数据集，其中包含了一个法语分割，其中包含了手工策划、高质量和多样化的数据源。为了评估在英语以外的性能，我们创建了一个新的基准测试 FrenchBench，包括一系列分类和生成任务，涵盖了模型在法语语言中性能的各个方面。此外，为了保持透明度并促进进一步的大规模语言模型研究，我们发布了代码库和各种模型规模、训练数据分布上的几十个检查点。

    We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
    
[^184]: 关于马尔可夫链中心极限定理的收敛速度，及其在TD学习中的应用

    Rates of Convergence in the Central Limit Theorem for Markov Chains, with an Application to TD Learning

    [https://arxiv.org/abs/2401.15719](https://arxiv.org/abs/2401.15719)

    本研究证明了一个非渐近的中心极限定理，并通过应用于TD学习，展示了其实际应用的可行性。

    

    我们使用Stein方法证明了一个非渐近的、矢量值鞅差的中心极限定理，并利用泊松方程将结果推广到马尔可夫链的函数。然后我们展示了这些结果可以应用于建立基于平均的非渐近的TD学习的中心极限定理。

    We prove a non-asymptotic central limit theorem for vector-valued martingale differences using Stein's method, and use Poisson's equation to extend the result to functions of Markov Chains. We then show that these results can be applied to establish a non-asymptotic central limit theorem for Temporal Difference (TD) learning with averaging.
    
[^185]: 添加非参数回归的随机梯度下降

    Stochastic Gradient Descent for Additive Nonparametric Regression

    [https://arxiv.org/abs/2401.00691](https://arxiv.org/abs/2401.00691)

    本文介绍了一种用于训练加性模型的随机梯度下降算法，具有良好的内存存储和计算要求。在规范很好的情况下，通过仔细选择学习率，可以实现最小和最优的风险。

    

    本文介绍了一种用于训练加性模型的迭代算法，该算法具有良好的内存存储和计算要求。该算法可以看作是对组件函数的截断基扩展的系数应用随机梯度下降的函数对应物。我们证明了得到的估计量满足一个奥拉克不等式，允许模型错误规范。在规范很好的情况下，通过在训练的三个不同阶段仔细选择学习率，我们证明了其风险在数据维度和训练样本大小的依赖方面是最小和最优的。通过在两个实际数据集上将该方法与传统的反向拟合进行比较，我们进一步说明了计算优势。

    This paper introduces an iterative algorithm for training additive models that enjoys favorable memory storage and computational requirements. The algorithm can be viewed as the functional counterpart of stochastic gradient descent, applied to the coefficients of a truncated basis expansion of the component functions. We show that the resulting estimator satisfies an oracle inequality that allows for model mis-specification. In the well-specified setting, by choosing the learning rate carefully across three distinct stages of training, we demonstrate that its risk is minimax optimal in terms of the dependence on the dimensionality of the data and the size of the training sample. We further illustrate the computational benefits by comparing the approach with traditional backfitting on two real-world datasets.
    
[^186]: LEFL: 高却熵客户端采样在联邦学习中的应用

    LEFL: Low Entropy Client Sampling in Federated Learning

    [https://arxiv.org/abs/2312.17430](https://arxiv.org/abs/2312.17430)

    LEFL提出了一种以保护数据隐私为目标的联邦学习采样策略，通过对客户端数据进行聚类，实现在每一轮次中的分层采样，从而优化全局模型的训练效果。

    

    联邦学习（FL）是一种机器学习范例，多个客户端协作使用私有数据优化单一全局模型。全局模型由中央服务器维护，通过一系列训练轮次来进行FL训练过程的编排。在每个轮次，服务器从客户端池中进行客户端采样，在发送最新的全局模型参数给客户端进行进一步优化之前。传统的采样策略使用随机客户端采样，但由于隐私原因未能考虑客户端数据分布。因此，我们提出了LEFL采样策略，通过根据模型学到的高级特征进行一次性客户端聚类，同时尊重数据隐私。这使得服务器能够在每一轮次中对簇进行分层客户端采样。我们展示通过这种方法选择的样本客户端数据集相对于全局数据分布的相对熵较低。因此，FL训练变得...

    Federated learning (FL) is a machine learning paradigm where multiple clients collaborate to optimize a single global model using their private data. The global model is maintained by a central server that orchestrates the FL training process through a series of training rounds. In each round, the server samples clients from a client pool before sending them its latest global model parameters for further optimization. Naive sampling strategies implement random client sampling and fail to factor client data distributions for privacy reasons. Hence we propose LEFL, an alternative sampling strategy by performing a one-time clustering of clients based on their model's learned high-level features while respecting data privacy. This enables the server to perform stratified client sampling across clusters in every round. We show datasets of sampled clients selected with this approach yield a low relative entropy with respect to the global data distribution. Consequently, the FL training becom
    
[^187]: 大语言模型的非平凡泛化界限

    Non-Vacuous Generalization Bounds for Large Language Models

    [https://arxiv.org/abs/2312.17173](https://arxiv.org/abs/2312.17173)

    这项研究提供了首个针对预训练大语言模型的非平凡泛化界限，表明语言模型能够发现适用于未见数据的规律性。建立了有效的压缩界限，证明较大的模型具有更好的泛化界限并更易压缩。

    

    现代语言模型可以包含数十亿个参数，这引发了一个问题，它们是否可以在训练数据之外进行泛化，或者只是重复它们的训练语料库。我们提供了首个针对预训练大语言模型（LLM）的非平凡泛化界限，表明语言模型能够发现适用于未见数据的规律性。具体而言，我们使用预测平滑导出了一个适用于无界对数似然损失的压缩界限，并且我们扩展了该界限以处理子采样，加速对大规模数据集的界限计算。为了实现非平凡泛化界限所需的极端压缩程度，我们设计了SubLoRA，这是一种低维非线性参数化方法。使用这种方法，我们发现较大的模型具有更好的泛化界限，并且比较小的模型更易压缩。

    Modern language models can contain billions of parameters, raising the question of whether they can generalize beyond the training data or simply regurgitate their training corpora. We provide the first non-vacuous generalization bounds for pretrained large language models (LLMs), indicating that language models are capable of discovering regularities that generalize to unseen data. In particular, we derive a compression bound that is valid for the unbounded log-likelihood loss using prediction smoothing, and we extend the bound to handle subsampling, accelerating bound computation on massive datasets. To achieve the extreme level of compression required for non-vacuous generalization bounds, we devise SubLoRA, a low-dimensional non-linear parameterization. Using this approach, we find that larger models have better generalization bounds and are more compressible than smaller models.
    
[^188]: PERP: 在LLMs时代重新思考修剪-重新训练范式

    PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs

    [https://arxiv.org/abs/2312.15230](https://arxiv.org/abs/2312.15230)

    本研究中，通过仅更新少部分高度表达力的参数，我们挑战了全参数重新训练的做法，在修剪后恢复或甚至提升了性能。PERP方法显著减少了计算量和存储需求。

    

    神经网络可以通过修剪实现高效压缩，显著减少存储和计算需求同时保持预测性能。像迭代幅值修剪（IMP，Han等，2015）这样的简单而有效的方法可以去除不重要的参数，并需要昂贵的重新训练过程以在修剪后恢复性能。然而，随着大型语言模型（LLMs）的兴起，由于内存和计算限制，完全重新训练变得不可行。在本研究中，我们挑战了重新训练所有参数的做法，通过证明只更新少部分高度表达力的参数通常足以恢复甚至提高性能。令人惊讶的是，仅重新训练GPT-结构的0.27%-0.35%的参数即可在不同稀疏水平上实现与一次性IMP相当的性能。我们的方法，即修剪后参数高效重新训练（PERP），大大减少了计算量。

    Neural Networks can be efficiently compressed through pruning, significantly reducing storage and computational demands while maintaining predictive performance. Simple yet effective methods like Iterative Magnitude Pruning (IMP, Han et al., 2015) remove less important parameters and require a costly retraining procedure to recover performance after pruning. However, with the rise of Large Language Models (LLMs), full retraining has become infeasible due to memory and compute constraints. In this study, we challenge the practice of retraining all parameters by demonstrating that updating only a small subset of highly expressive parameters is often sufficient to recover or even improve performance compared to full retraining. Surprisingly, retraining as little as 0.27%-0.35% of the parameters of GPT-architectures achieves comparable performance to One Shot IMP across various sparsity levels. Our approach, Parameter-Efficient Retraining after Pruning (PERP), drastically reduces compute a
    
[^189]: 用$k$-means在图像分类中的对抗性鲁棒性

    Adversarial Robustness on Image Classification with $k$-means

    [https://arxiv.org/abs/2312.09533](https://arxiv.org/abs/2312.09533)

    本文研究了增强$k$-means聚类算法对抗性攻击鲁棒性的挑战和策略，并提出了一种对抗性训练方法来改善在对抗场景中的测试性能。

    

    本文探讨了增强$k$-means聚类算法对抗性攻击的挑战与策略。我们评估了聚类算法对抗性攻击的脆弱性，并强调了相关的安全风险。我们的研究调查了攻击强度对训练的影响，引入了监督模型和无监督模型之间的可迁移性概念，并强调了无监督模型对样本分布的敏感性。此外，我们还引入并评估了一种对抗性训练方法，在对抗场景中改善了测试性能，并强调了所提出的训练方法中各种参数的重要性，如连续学习、中心初始化和对抗步数。

    In this paper we explore the challenges and strategies for enhancing the robustness of $k$-means clustering algorithms against adversarial manipulations. We evaluate the vulnerability of clustering algorithms to adversarial attacks, emphasising the associated security risks. Our study investigates the impact of incremental attack strength on training, introduces the concept of transferability between supervised and unsupervised models, and highlights the sensitivity of unsupervised models to sample distributions. We additionally introduce and evaluate an adversarial training method that improves testing performance in adversarial scenarios, and we highlight the importance of various parameters in the proposed training method, such as continuous learning, centroid initialisation, and adversarial step-count.
    
[^190]: 在异质性和谱问题下重新审视基于图的欺诈检测

    Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum

    [https://arxiv.org/abs/2312.06441](https://arxiv.org/abs/2312.06441)

    本文提出了一种基于半监督GNN的欺诈检测器SEC-GFD，通过混合过滤模块和局部环境约束模块解决了异质性和标签利用问题。

    

    基于图的欺诈检测（GFD）可视为一项具有挑战性的半监督节点二分类任务。近年来，图神经网络（GNN）已广泛应用于GFD，通过聚合邻居信息来刻画节点的异常可能性。然而，欺诈图在本质上是异质的，因此大多数GNN由于假设同质性而表现不佳。此外，由于存在异质性和类别不平衡问题，现有模型未充分利用宝贵的节点标签信息。为了解决上述问题，本文提出了一种基于半监督GNN的欺诈检测器SEC-GFD。该检测器包括混合过滤模块和局部环境约束模块，这两个模块分别用于解决异质性和标签利用问题。第一个模块从谱域的角度出发，在一定程度上解决了异质性问题。具体而言，它将图分割称不同的谱成分，

    Graph-based fraud detection (GFD) can be regarded as a challenging semi-supervised node binary classification task. In recent years, Graph Neural Networks (GNN) have been widely applied to GFD, characterizing the anomalous possibility of a node by aggregating neighbor information. However, fraud graphs are inherently heterophilic, thus most of GNNs perform poorly due to their assumption of homophily. In addition, due to the existence of heterophily and class imbalance problem, the existing models do not fully utilize the precious node label information. To address the above issues, this paper proposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector includes a hybrid filtering module and a local environmental constraint module, the two modules are utilized to solve heterophily and label utilization problem respectively. The first module starts from the perspective of the spectral domain, and solves the heterophily problem to a certain extent. Specifically, it divides t
    
[^191]: 引起多义性的原因是什么？通过偶然因素的混合选择性的替代起源故事

    What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes

    [https://arxiv.org/abs/2312.03096](https://arxiv.org/abs/2312.03096)

    这项工作提出了多义性的替代起源故事，称为偶然多义性，即使有足够的神经元来表示所有特征，也可能产生多义性。

    

    多义性神经元——激活一组不相关特征的神经元——被视为解释任务优化深度网络的显著障碍，对AI安全性产生影响。传统的多义性起源故事是数据包含的“特征”多于神经元，因此学习执行任务迫使网络将多个不相关特征分配给同一个神经元，危及我们理解网络内部处理的能力。在这项工作中，我们提出了多义性的第二个且非互斥的替代起源故事。我们展示了即使有足够的神经元来表示数据中的所有特征，偶然多义性也可能产生，这是一种我们称之为“偶然多义性”的现象。通过理论和实验证明，偶然多义性可以由多种原因引起，包括正则化和神经噪音；这种偶然多义性发生是因为随机的因素。

    Polysemantic neurons -- neurons that activate for a set of unrelated features -- have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more ``features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term \textit{incidental polysemanticity}. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random in
    
[^192]: Bagged Regularized $k$-Distances用于异常检测

    Bagged Regularized $k$-Distances for Anomaly Detection

    [https://arxiv.org/abs/2312.01046](https://arxiv.org/abs/2312.01046)

    本文提出了一种称为Bagged Regularized $k$-Distances for Anomaly Detection (BRDAD)的基于距离的算法，通过将非监督异常检测问题转化为凸优化问题，成功解决了基于距离算法中超参数选择的敏感性挑战，并通过包集成方法解决了处理大规模数据集时的效率问题。

    

    本文考虑非监督异常检测的范式，即在没有标记的情况下识别数据集中的异常值。尽管基于距离的方法对于非监督异常检测具有较好的性能，但它们对最近邻数量的选择非常敏感。为此，我们提出了一种新的基于距离的算法，称为Bagged Regularized $k$-Distances for Anomaly Detection (BRDAD)，将非监督异常检测问题转化为凸优化问题。我们的BRDAD算法通过最小化替代风险（即经验风险的有限样本上界）来选择权重，以用于密度估计的带权重的$k$-distances。这种方法成功解决了基于距离算法中超参数选择的敏感性挑战。此外，在处理大规模数据集时，我们还可以通过包集成的方法来解决效率问题。

    We consider the paradigm of unsupervised anomaly detection, which involves the identification of anomalies within a dataset in the absence of labeled examples. Though distance-based methods are top-performing for unsupervised anomaly detection, they suffer heavily from the sensitivity to the choice of the number of the nearest neighbors. In this paper, we propose a new distance-based algorithm called bagged regularized $k$-distances for anomaly detection (BRDAD) converting the unsupervised anomaly detection problem into a convex optimization problem. Our BRDAD algorithm selects the weights by minimizing the surrogate risk, i.e., the finite sample bound of the empirical risk of the bagged weighted $k$-distances for density estimation (BWDDE). This approach enables us to successfully address the sensitivity challenge of the hyperparameter choice in distance-based algorithms. Moreover, when dealing with large-scale datasets, the efficiency issues can be addressed by the incorporated baggi
    
[^193]: 元共训练：两种视角优于一种

    Meta Co-Training: Two Views are Better than One

    [https://arxiv.org/abs/2311.18083](https://arxiv.org/abs/2311.18083)

    元共训练通过在数据上构建不同的视角，并利用未标记数据进行共同训练，提高了半监督学习的性能。

    

    在许多实际的计算机视觉场景中，未标记的数据很多，但标签却稀缺且难以获得。因此，半监督学习利用未标记的数据提升监督分类器的性能已经在最近的文献中引起了重要的关注。其中一种主要的半监督算法是共训练。在共训练中，两种不同的模型利用数据的不同独立和足够的“视角”来共同进行更好的预测。在共训练过程中，每个模型在未标记的数据点上创建伪标签，用于改进另一个模型的性能。我们展示了在常见情况下，当独立视角不可用时，我们可以使用预训练模型来廉价地构建这些视角。在构建的视角上进行共训练可以提高性能，优于我们构建的任何单个视角，并且与半监督学习中的最新方法性能相当，但具有一些不可取之处。

    In many practical computer vision scenarios unlabeled data is plentiful, but labels are scarce and difficult to obtain. As a result, semi-supervised learning which leverages unlabeled data to boost the performance of supervised classifiers have received significant attention in recent literature. One major class of semi-supervised algorithms is co-training. In co-training two different models leverage different independent and sufficient "views" of the data to jointly make better predictions. During co-training each model creates pseudo labels on unlabeled points which are used to improve the other model. We show that in the common case when independent views are not available we can construct such views inexpensively using pre-trained models. Co-training on the constructed views yields a performance improvement over any of the individual views we construct and performance comparable with recent approaches in semi-supervised learning, but has some undesirable properties. To alleviate t
    
[^194]: 大规模可学习向量存储压缩的实验分析

    Experimental Analysis of Large-scale Learnable Vector Storage Compression

    [https://arxiv.org/abs/2311.15578](https://arxiv.org/abs/2311.15578)

    本文对嵌入向量压缩进行了全面比较分析和实验评估，引入了一个新的分类法，根据特征和方法学对这些技术进行分类，并进一步发展了一个模块化的方法来实现压缩算法。

    

    可学习的嵌入向量是机器学习中最重要的应用之一，在各种与数据库相关的领域广泛应用。然而，在推荐任务中稀疏数据的高维度和检索任务中大规模语料库的巨大容量导致嵌入表的内存消耗很大，给模型的训练和部署带来了巨大挑战。近期的研究提出了各种方法来压缩嵌入向量，但这些方法在模型质量稍微下降或引入其他开销的同时，其相对性能仍不清楚。现有的实验比较只涵盖了这些方法的子集，并且关注有限的指标。本文对嵌入向量压缩进行了全面比较分析和实验评估。我们引入了一个新的分类法，根据特征和方法学对这些技术进行分类，并进一步发展了一个模块化的方法来实现压缩算法。

    Learnable embedding vector is one of the most important applications in machine learning, and is widely used in various database-related domains. However, the high dimensionality of sparse data in recommendation tasks and the huge volume of corpus in retrieval-related tasks lead to a large memory consumption of the embedding table, which poses a great challenge to the training and deployment of models. Recent research has proposed various methods to compress the embeddings at the cost of a slight decrease in model quality or the introduction of other overheads. Nevertheless, the relative performance of these methods remains unclear. Existing experimental comparisons only cover a subset of these methods and focus on limited metrics. In this paper, we perform a comprehensive comparative analysis and experimental evaluation of embedding compression. We introduce a new taxonomy that categorizes these techniques based on their characteristics and methodologies, and further develop a modular
    
[^195]: 学习计算格罗布纳基

    Learning to Compute Gr\"obner Bases

    [https://arxiv.org/abs/2311.12904](https://arxiv.org/abs/2311.12904)

    本文首次通过Transformer的训练实现了格罗布纳基的计算，通过解决随机生成格罗布纳基和将其转化为非格罗布纳多项式系统的问题，提出了解决计算任务中的关键挑战的方法。

    

    解决多项式系统，或计算相关的格罗布纳基，一直是计算代数学中的基本任务。然而，众所周知，它的计算成本非常昂贵，在最坏情况下的时间复杂性是指数级双倍。在本文中，我们首次通过Transformer的训练来实现格罗布纳基的计算。训练需要许多多项式系统和相关格罗布纳基的配对，引出了两个新颖的代数问题：格罗布纳基的随机生成和将其转化为非格罗布纳多项式系统的问题，称为格罗布纳反问题。我们通过零维根正理想解决了这些问题，这些理想在各种应用中出现。实验证明，所提出的数据集生成方法比朴素方法快三到六个数量级，克服了学习计算格罗布纳基的一个关键挑战。

    Solving a polynomial system, or computing an associated Gr\"obner basis, has been a fundamental task in computational algebra. However, it is also known for its notoriously expensive computational cost - doubly exponential time complexity in the number of variables in the worst case. In this paper, we achieve for the first time Gr\"obner basis computation through the training of a Transformer. The training requires many pairs of a polynomial system and the associated Gr\"obner basis, raising two novel algebraic problems: random generation of Gr\"obner bases and the transformation of them into non-Gr\"obner polynomial systems, termed as backward Gr\"obner problem. We resolve these problems with zero-dimensional radical ideals, the ideals appearing in various applications. The experiments show that the proposed dataset generation method is three to six orders of magnitude faster than a naive approach, overcoming a crucial challenge in learning to compute Gr\"obner bases.
    
[^196]: 在因果推断中的遗漏标签: 一项关于悖论的研究

    Omitted Labels in Causality: A Study of Paradoxes

    [https://arxiv.org/abs/2311.06840](https://arxiv.org/abs/2311.06840)

    本研究探讨了所谓的“遗漏标签上下文”，即训练数据仅限于可能标签的一个子集，并利用悖论展示了在这种上下文中因果推断面临的困难。研究发现在某些情况下，必须使用非可交换的处理组和对照组进行正确的校正。此外，研究还发现了结论网络与社会选择理论之间的有趣联系。

    

    我们探讨了我们所称之为“遗漏标签上下文”的概念，即训练数据仅限于可能标签的一个子集。这种设置在专业人士或特定的专注研究中非常普遍。我们利用已广泛研究的悖论（辛普森悖论和康多塞悖论）来说明在遗漏标签上下文中因果推断面临的更普遍困难。与因果推断基本原理相反，我们展示了“正确”的校正有时需要非可交换的处理组和对照组。这些陷阱引导我们研究不同上下文中得出的结论网络和其形成的结构，从而证明了这些网络与社会选择理论的有趣联系。

    We explore what we call ``omitted label contexts,'' in which training data is limited to a subset of the possible labels. This setting is common among specialized human experts or specific focused studies. We lean on well-studied paradoxes (Simpson's and Condorcet) to illustrate the more general difficulties of causal inference in omitted label contexts. Contrary to the fundamental principles on which much of causal inference is built, we show that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. These pitfalls lead us to the study networks of conclusions drawn from different contexts and the structures the form, proving an interesting connection between these networks and social choice theory.
    
[^197]: PowerFlowNet: 使用消息传递图神经网络进行功率流近似

    PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks

    [https://arxiv.org/abs/2311.03415](https://arxiv.org/abs/2311.03415)

    PowerFlowNet 是一种使用消息传递图神经网络进行功率流近似的新型架构，与传统的牛顿-拉夫逊方法相比，在简单的系统中速度提高了4倍，在实际的法国高电压网络中提高了145倍，同时在性能和执行时间方面明显优于其他传统方法。

    

    准确且高效的功率流分析对于现代电力网络的运行和规划至关重要。因此，需要能够为小型和大型电力网络提供准确和快速解的可扩展算法。由于电力网络可以被解释为一个图，图神经网络(GNNs)已经成为通过利用底层图结构的信息共享来改善功率流近似的准确性和速度的一种有前景的方法。在这项研究中，我们介绍了PowerFlowNet，一个新颖的GNN架构，用于功率流近似，在简单的IEEE 14总线系统中与传统的牛顿-拉夫逊方法展示了相似的性能，但在法国高电压网络(6470rte)的真实情况下实现了4倍的速度提升。同时，与其他传统的近似方法(如直流松弛法)相比，在性能和执行时间方面显著优于它们，从而实现了优越的表现。

    Accurate and efficient power flow (PF) analysis is crucial in modern electrical networks' operation and planning. Therefore, there is a need for scalable algorithms that can provide accurate and fast solutions for both small and large scale power networks. As the power network can be interpreted as a graph, Graph Neural Networks (GNNs) have emerged as a promising approach for improving the accuracy and speed of PF approximations by exploiting information sharing via the underlying graph structure. In this study, we introduce PowerFlowNet, a novel GNN architecture for PF approximation that showcases similar performance with the traditional Newton-Raphson method but achieves it 4 times faster in the simple IEEE 14-bus system and 145 times faster in the realistic case of the French high voltage network (6470rte). Meanwhile, it significantly outperforms other traditional approximation methods, such as the DC relaxation method, in terms of performance and execution time; therefore, making P
    
[^198]: LILO：通过压缩和文档化代码学习可解释库

    LILO: Learning Interpretable Libraries by Compressing and Documenting Code

    [https://arxiv.org/abs/2310.19791](https://arxiv.org/abs/2310.19791)

    LILO是一种神经符号框架，通过迭代地合成、压缩和文档化代码来构建可解释且适用于特定问题领域的程序库。在其中，LILO结合了大型语言模型引导的程序合成和程序自动重构的算法进展，并且通过自动文档过程使得代码抽象可解释并提升性能。

    

    尽管大型语言模型（LLMs）在代码生成方面表现出色，但软件开发的关键方面是重构的艺术：将代码整合到可重用和可读的程序库中。本文介绍了一种名为LILO的神经符号框架，它通过迭代地合成、压缩和文档化代码来构建适合特定问题领域的库。LILO将LLM引导的程序合成与Stitch自动重构的近期算法进展相结合：Stitch是一个符号压缩系统，可以高效地识别大型代码语料库中的最佳lambda抽象。为了使这些抽象可解释，我们引入了一种自动文档（AutoDoc）过程，它根据上下文中的使用示例推断出自然语言名称和文档字符串。除了提高人类可读性外，我们发现AutoDoc通过帮助LILO的合成器解释和部署学习到的抽象来提高性能。我们对LILO进行了三个归纳式程序综合的评估。

    While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
    
[^199]: 使用机器学习和经典技术的统计推断：基于积累的局部效应（ALE）

    Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)

    [https://arxiv.org/abs/2310.09877](https://arxiv.org/abs/2310.09877)

    本研究通过引入创新的工具和技术，使用积累的局部效应（ALE）进行统计推断，在解决小数据集、直观特征和健壮推断方面取得了突破。这项工作推动了ALE及其应用的研究。

    

    积累的局部效应（ALE）是一种对黑盒机器学习（ML）算法结果进行全局解释的模型无关方法。使用ALE进行统计推断面临至少三个挑战：确保ALE分析的可靠性，尤其在小数据集的情况下；直观地表征变量在ML中的整体效应；以及从ML数据分析中进行健壮的推断。为此，我们引入了创新的工具和技术，使用ALE进行统计推断，建立了适应数据集大小的自助法置信区间，并引入了直观指示对结果变量和标准化尺度上的效应的ALE效应大小度量。此外，我们演示了如何使用这些工具绘制可靠的统计推断，反映了ALE熟练突出的灵活模式，实现了R中“ale”包中的实现。这项工作推动了关于ALE及其应用的讨论。

    Accumulated Local Effects (ALE) is a model-agnostic approach for global explanations of the results of black-box machine learning (ML) algorithms. There are at least three challenges with conducting statistical inference based on ALE: ensuring the reliability of ALE analyses, especially in the context of small datasets; intuitively characterizing a variable's overall effect in ML; and making robust inferences from ML data analysis. In response, we introduce innovative tools and techniques for statistical inference using ALE, establishing bootstrapped confidence intervals tailored to dataset size and introducing ALE effect size measures that intuitively indicate effects on both the outcome variable scale and a normalized scale. Furthermore, we demonstrate how to use these tools to draw reliable statistical inferences, reflecting the flexible patterns ALE adeptly highlights, with implementations available in the 'ale' package in R. This work propels the discourse on ALE and its applicabi
    
[^200]: 一个基于均匀化方法的梯度主导随机优化方法

    A Homogenization Approach for Gradient-Dominated Stochastic Optimization

    [https://arxiv.org/abs/2308.10630](https://arxiv.org/abs/2308.10630)

    本文介绍了一种基于均匀化方法的梯度主导随机优化方法，通过满足梯度主导性质的随机函数，实现全局收敛。我们提供了样本复杂度分析，并通过方差减少技术提供了增强结果。实验结果表明，该方法在无需立方正则化的情况下达到了最佳样本复杂度。

    

    梯度主导性质是一种比强凸性条件更弱但足以确保全局收敛的条件，即使在非凸优化中也可以应用广泛。本文提出了一种基于最近提出的均匀化方法的梯度主导随机二阶下降方法（SHSODM），用于满足梯度主导性质的随机函数。从理论上讲，我们提供了其样本复杂度分析，并通过结合方差减少技术提供了进一步的增强结果。我们的发现表明，SHSODM与其他梯度主导随机优化的二阶方法相比，可以达到已知的最佳样本复杂度，而无需立方正则化。从经验上讲，由于均匀化方法仅依赖于每次迭代中解极值特征向量问题，而不是牛顿类型的系统，所以我们的方法具有更低的计算成本。

    Gradient dominance property is a condition weaker than strong convexity, yet sufficiently ensures global convergence even in non-convex optimization. This property finds wide applications in machine learning, reinforcement learning (RL), and operations management. In this paper, we propose the stochastic homogeneous second-order descent method (SHSODM) for stochastic functions enjoying gradient dominance property based on a recently proposed homogenization approach. Theoretically, we provide its sample complexity analysis, and further present an enhanced result by incorporating variance reduction techniques. Our findings show that SHSODM matches the best-known sample complexity achieved by other second-order methods for gradient-dominated stochastic optimization but without cubic regularization. Empirically, since the homogenization approach only relies on solving extremal eigenvector problem at each iteration instead of Newton-type system, our methods gain the advantage of cheaper com
    
[^201]: 优化的梯度跟踪用于分布式在线学习

    Optimized Gradient Tracking for Decentralized Online Learning

    [https://arxiv.org/abs/2306.06375](https://arxiv.org/abs/2306.06375)

    这项工作提出了一种新的通用梯度跟踪（GGT）框架，用于解决分布式在线学习中的优化问题。该方法在不需要梯度有界性假设的情况下，通过一种新颖的半定规划分析方法来获得理想的遗憾界，并可以适用于包括最先进算法和新动态版本的各种情况。

    

    本文考虑了分布式在线学习的问题，其中目标是跟踪分布在网络中的多个节点上的时间变化函数的最优值之和。函数和梯度的局部可用性需要节点之间的协调和共识。我们提出了通用梯度跟踪（GGT）框架，统一了一些现有的方法，包括最先进的方法。通过使用基于半定规划的新颖分析方法对所提出的GGT算法进行理论分析，得到了在非常一般的条件下且不需要梯度有界性假设的理想遗憾界。这些结果适用于GGT的特殊情况，包括各种最先进的算法以及各种古典分布式算法的新动态版本。为了进一步减小遗憾，我们考虑了只有四个自由参数的GGT的简化版本。

    This work considers the problem of decentralized online learning, where the goal is to track the optimum of the sum of time-varying functions, distributed across several nodes in a network. The local availability of the functions and their gradients necessitates coordination and consensus among the nodes. We put forth the Generalized Gradient Tracking (GGT) framework that unifies a number of existing approaches, including the state-of-the-art ones. The performance of the proposed GGT algorithm is theoretically analyzed using a novel semidefinite programming-based analysis that yields the desired regret bounds under very general conditions and without requiring the gradient boundedness assumption. The results are applicable to the special cases of GGT, which include various state-of-the-art algorithms as well as new dynamic versions of various classical decentralized algorithms. To further minimize the regret, we consider a condensed version of GGT with only four free parameters. A proc
    
[^202]: 快速高效的带有截止期限实例的匹配算法

    Fast and Efficient Matching Algorithm with Deadline Instances

    [https://arxiv.org/abs/2305.08353](https://arxiv.org/abs/2305.08353)

    本文介绍了一种带有截止期限实例的快速高效匹配算法，通过引入带有截止期限的市场模型，提出了两种优化算法（FastGreedy和FastPostponedGreedy）。该算法在处理机器学习中的在线加权匹配问题时具有较快的速度和准确性。

    

    在机器学习中，在线加权匹配问题由于其众多应用而成为一个基本问题。尽管在这个领域已经做了很多努力，但现有的算法要么速度太慢，要么没有考虑到截止期限（节点可以匹配的最长时间）。在本文中，我们首先引入了一个带有截止期限的市场模型。接下来，我们提出了两个优化算法（FastGreedy和FastPostponedGreedy），并给出了关于算法时间复杂度和正确性的理论证明。在FastGreedy算法中，我们已经知道一个节点是买家还是卖家。但在FastPostponedGreedy算法中，一开始我们不知道每个节点的状态。然后，我们推广了一个草图矩阵，以在真实数据集和合成数据集上运行原始算法和我们的算法。设 ε ∈（0,0.1）表示每条边的真实权重的相对误差。原始的Greedy和Po算法的竞争比率是多少。

    The online weighted matching problem is a fundamental problem in machine learning due to its numerous applications. Despite many efforts in this area, existing algorithms are either too slow or don't take $\mathrm{deadline}$ (the longest time a node can be matched) into account. In this paper, we introduce a market model with $\mathrm{deadline}$ first. Next, we present our two optimized algorithms (\textsc{FastGreedy} and \textsc{FastPostponedGreedy}) and offer theoretical proof of the time complexity and correctness of our algorithms. In \textsc{FastGreedy} algorithm, we have already known if a node is a buyer or a seller. But in \textsc{FastPostponedGreedy} algorithm, the status of each node is unknown at first. Then, we generalize a sketching matrix to run the original and our algorithms on both real data sets and synthetic data sets. Let $\epsilon \in (0,0.1)$ denote the relative error of the real weight of each edge. The competitive ratio of original \textsc{Greedy} and \textsc{Po
    
[^203]: 基于条件扩散模型的电力客户定制负荷曲线合成

    Customized Load Profiles Synthesis for Electricity Customers Based on Conditional Diffusion Models

    [https://arxiv.org/abs/2304.12076](https://arxiv.org/abs/2304.12076)

    本文提出了一种基于条件扩散模型的定制负荷曲线合成方法，用于解决电力客户的数据短缺问题，并实现高质量负荷曲线的合成。

    

    客户的负荷曲线是支持现代电力系统数据分析应用的关键资源。然而，由于采集成本和数据隐私问题，通常会缺乏足够的历史负荷曲线进行数据分析。为了解决数据短缺问题，负荷曲线合成是一种有效的技术，为客户提供合成的训练数据，以建立高性能的数据驱动模型。然而，由于客户负荷的高异质性，使用基于各自客户数据训练的生成模型合成高质量的负荷曲线仍然具有挑战性。在本文中，我们提出了一种新颖的基于条件扩散模型的定制负荷曲线合成方法，用于异构客户。具体而言，我们首先将定制合成转化为条件数据生成问题。然后，我们扩展传统的扩散模型为条件扩散模型，实现条件数据生成。

    Customers' load profiles are critical resources to support data analytics applications in modern power systems. However, there are usually insufficient historical load profiles for data analysis, due to the collection cost and data privacy issues. To address such data shortage problems, load profiles synthesis is an effective technique that provides synthetic training data for customers to build high-performance data-driven models. Nonetheless, it is still challenging to synthesize high-quality load profiles for each customer using generation models trained by the respective customer's data owing to the high heterogeneity of customer load. In this paper, we propose a novel customized load profiles synthesis method based on conditional diffusion models for heterogeneous customers. Specifically, we first convert the customized synthesis into a conditional data generation issue. We then extend traditional diffusion models to conditional diffusion models to realize conditional data generat
    
[^204]: MFAI:一种可扩展的贝叶斯矩阵分解方法来利用辅助信息

    MFAI: A Scalable Bayesian Matrix Factorization Approach to Leveraging Auxiliary Information

    [https://arxiv.org/abs/2303.02566](https://arxiv.org/abs/2303.02566)

    MFAI是一种可扩展的贝叶斯矩阵分解方法，通过利用辅助信息来克服由于数据质量差导致的挑战，具有灵活建模非线性关系和对辅助信息的鲁棒性。

    

    在各种实际情况下，矩阵分解方法在数据质量差的情况下往往表现不佳，例如数据稀疏性高和信噪比低。在这里，我们考虑利用辅助信息的矩阵分解问题，辅助信息在实际应用中是大量可用的，以克服由于数据质量差引起的挑战。与现有方法主要依赖于简单线性模型将辅助信息与主数据矩阵结合不同，我们提出将梯度增强树集成到概率矩阵分解框架中以有效地利用辅助信息(MFAI)。因此，MFAI自然地继承了梯度增强树的几个显著特点，如灵活建模非线性关系、对辅助信息中的不相关特征和缺失值具有鲁棒性。MFAI中的参数可以在经验贝叶斯框架下自动确定，使其适应于利用辅助信息。

    In various practical situations, matrix factorization methods suffer from poor data quality, such as high data sparsity and low signal-to-noise ratio (SNR). Here, we consider a matrix factorization problem by utilizing auxiliary information, which is massively available in real-world applications, to overcome the challenges caused by poor data quality. Unlike existing methods that mainly rely on simple linear models to combine auxiliary information with the main data matrix, we propose to integrate gradient boosted trees in the probabilistic matrix factorization framework to effectively leverage auxiliary information (MFAI). Thus, MFAI naturally inherits several salient features of gradient boosted trees, such as the capability of flexibly modeling nonlinear relationships and robustness to irrelevant features and missing values in auxiliary information. The parameters in MFAI can be automatically determined under the empirical Bayes framework, making it adaptive to the utilization of a
    
[^205]: nSimplex Zen:一种新颖的欧几里得空间与希尔伯特空间的降维方法

    nSimplex Zen: A Novel Dimensionality Reduction for Euclidean and Hilbert Spaces

    [https://arxiv.org/abs/2302.11508](https://arxiv.org/abs/2302.11508)

    nSimplex Zen是一种新颖的拓扑降维方法，它通过成对距离来降低高维空间的维度。它在物理内存使用和距离计算速度方面具有优势。

    

    维数约简技术将高维空间的值映射到低维空间中。结果是需要较少物理内存且具有更快距离计算的空间。这些技术被广泛应用于需要满足降维空间所具有的可接受精度与原始空间相比的场景。许多这样的转换方法已经被描述出来。它们被分为两大类：线性和拓扑。线性方法如主成分分析(PCA)和随机投影(RP)将转换成欧几里得空间的较低维度的基于矩阵的转换。拓扑方法如多维缩放(MDS)尝试保持更高层次的特性，如最近邻关系，有些方法可以应用于非欧几里得空间。在这里，我们介绍了一种新颖的拓扑降维方法nSimplex Zen。与MDS类似，它只依赖于原始空间中的成对距离。使用d

    Dimensionality reduction techniques map values from a high dimensional space to one with a lower dimension. The result is a space which requires less physical memory and has a faster distance calculation. These techniques are widely used where required properties of the reduced-dimension space give an acceptable accuracy with respect to the original space. Many such transforms have been described. They have been classified in two main groups: linear and topological. Linear methods such as Principal Component Analysis (PCA) and Random Projection (RP) define matrix-based transforms into a lower dimension of Euclidean space. Topological methods such as Multidimensional Scaling (MDS) attempt to preserve higher-level aspects such as the nearest-neighbour relation, and some may be applied to non-Euclidean spaces. Here, we introduce nSimplex Zen, a novel topological method of reducing dimensionality. Like MDS, it relies only upon pairwise distances measured in the original space. The use of d
    
[^206]: 选择性不确定性传播在离线强化学习中的应用

    Selective Uncertainty Propagation in Offline RL

    [https://arxiv.org/abs/2302.00284](https://arxiv.org/abs/2302.00284)

    本论文提出了一种名为选择性不确定性传播的方法，用于解决离线强化学习中的分布偏移问题。该方法通过自适应的方式建立置信区间，有效地处理了实际问题中策略学习的挑战。

    

    本研究考虑有限时间段离线强化学习的情景，目标在于应对动态规划算法中每一步策略学习的挑战。通过评估离开行为策略在第h步时的处理效果，就可以学习到这一步的策略。由于每一步策略都会影响下一状态的分布，相关的分布偏移问题使得这一问题在统计学上比随机情境挑战下的处理效果估计更加困难。然而，许多现实强化学习问题的难度介于这两种情境之间。我们开发了一种灵活且通用的方法，名为选择性不确定性传播，用于建立置信区间，并根据相关分布偏移问题的难度进行自适应。在玩具环境中展示了我们方法的优势，并证明了这些技术在离线策略学习中的好处。

    We consider the finite-horizon offline reinforcement learning (RL) setting, and are motivated by the challenge of learning the policy at any step h in dynamic programming (DP) algorithms. To learn this, it is sufficient to evaluate the treatment effect of deviating from the behavioral policy at step h after having optimized the policy for all future steps. Since the policy at any step can affect next-state distributions, the related distributional shift challenges can make this problem far more statistically hard than estimating such treatment effects in the stochastic contextual bandit setting. However, the hardness of many real-world RL instances lies between the two regimes. We develop a flexible and general method called selective uncertainty propagation for confidence interval construction that adapts to the hardness of the associated distribution shift challenges. We show benefits of our approach on toy environments and demonstrate the benefits of these techniques for offline pol
    
[^207]: 一种具有通用参数化和线性收敛的政策镜面下降新框架

    A Novel Framework for Policy Mirror Descent with General Parameterization and Linear Convergence

    [https://arxiv.org/abs/2301.13139](https://arxiv.org/abs/2301.13139)

    我们提出了一种新的政策优化框架，通过镜面下降自然地适应通用参数化，并获得了应用于通用参数化的基于政策梯度的方法的线性收敛保证。

    

    强化学习中现代政策优化方法（如TRPO和PPO）的成功归功于参数化政策的使用。然而，尽管已经为这类算法在标签设置中建立了理论保证，但对于通用参数化方案的使用仍然没有得到充分证明。在这项工作中，我们介绍了一种基于镜面下降的政策优化新框架，可以自然地适应通用参数化。我们方案所产生的政策类可以恢复已知的类，如softmax，并根据镜面映射的选择生成新类。使用我们的框架，我们获得了关于涉及通用参数化的基于政策梯度的方法的线性收敛的首个结果。为了展示我们的框架适应通用参数化方案的能力，我们提供了使用浅层神经网络时的样本复杂性，并展示它相对于先前方法的改进。

    Modern policy optimization methods in reinforcement learning, such as TRPO and PPO, owe their success to the use of parameterized policies. However, while theoretical guarantees have been established for this class of algorithms, especially in the tabular setting, the use of general parameterization schemes remains mostly unjustified. In this work, we introduce a novel framework for policy optimization based on mirror descent that naturally accommodates general parameterizations. The policy class induced by our scheme recovers known classes, e.g., softmax, and generates new ones depending on the choice of mirror map. Using our framework, we obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parameterization. To demonstrate the ability of our framework to accommodate general parameterization schemes, we provide its sample complexity when using shallow neural networks, show that it represents an improvement upon the previous be
    
[^208]: 因果DAG的子集验证和搜索算法

    Subset verification and search algorithms for causal DAGs

    [https://arxiv.org/abs/2301.03180](https://arxiv.org/abs/2301.03180)

    本文研究了在学习因果DAG的子集关系时，识别所需最小干预集的问题，提出了两种有效算法进行解决。在子集验证问题上，我们提供了一种计算最小干预集的高效算法。在子集搜索问题上，我们提出了两种解决方法。

    

    学习变量之间的因果关系是因果推断中的一项基本任务，有向无环图（DAGs）是表示因果关系的常见选择。由于我们只能从观测数据中恢复因果图的 Markov 等价类，因此通常需要使用干预来进行恢复任务。干预通常是昂贵的，因此设计能够最小化干预次数的算法非常重要。在这项工作中，我们研究了在学习一组边缘（目标边缘）之间的因果关系时，识别所需最小干预集的问题。在假设忠实性、因果充分性和理想干预的条件下，我们在两个设置下研究了这个问题：当底层真实因果图已知时（子集验证），以及当其未知时（子集搜索）。对于子集验证问题，我们提供了一个有效的算法来计算最小干预集；我们进一步解决了子集搜索问题，并提出了两种算法进行解决。

    Learning causal relationships between variables is a fundamental task in causal inference and directed acyclic graphs (DAGs) are a popular choice to represent the causal relationships. As one can recover a causal graph only up to its Markov equivalence class from observations, interventions are often used for the recovery task. Interventions are costly in general and it is important to design algorithms that minimize the number of interventions performed. In this work, we study the problem of identifying the smallest set of interventions required to learn the causal relationships between a subset of edges (target edges). Under the assumptions of faithfulness, causal sufficiency, and ideal interventions, we study this problem in two settings: when the underlying ground truth causal graph is known (subset verification) and when it is unknown (subset search). For the subset verification problem, we provide an efficient algorithm to compute a minimum sized interventional set; we further ex
    
[^209]: 通过深度符号检索在基于DCT的图像编码中压缩符号信息

    Compressing Sign Information in DCT-based Image Coding via Deep Sign Retrieval

    [https://arxiv.org/abs/2209.10712](https://arxiv.org/abs/2209.10712)

    该论文提出了一种名为“符号检索”的方法来高效压缩离散余弦变换（DCT）系数的符号信息。通过排除符号信息并在解码器中补充，该方法在符号位数和计算成本方面优于以前的方法。

    

    由于符号的等概率特性，压缩离散余弦变换（DCT）系数的符号信息是图像编码方案中一个棘手的问题。为了克服这个困难，我们提出了一种名为“符号检索”的符号信息高效压缩方法。该方法受到相位恢复的启发，相位恢复是从其幅度中找到离散傅里叶变换系数的相位信息的经典信号恢复问题。在编码器中，所有DCT系数的符号信息被排除在比特流之外，并且通过我们的符号检索方法在解码器中补充。通过实验证明，我们的方法在符号位数和计算成本方面优于先前的方法。我们使用Python语言实现的方法可以在https://github.com/ctsutake/dsr获得。

    Compressing the sign information of discrete cosine transform (DCT) coefficients is an intractable problem in image coding schemes due to the equiprobable characteristics of the signs. To overcome this difficulty, we propose an efficient compression method for the sign information called "sign retrieval." This method is inspired by phase retrieval, which is a classical signal restoration problem of finding the phase information of discrete Fourier transform coefficients from their magnitudes. The sign information of all DCT coefficients is excluded from a bitstream at the encoder and is complemented at the decoder through our sign retrieval method. We show through experiments that our method outperforms previous ones in terms of the bit amount for the signs and computation cost. Our method, implemented in Python language, is available from https://github.com/ctsutake/dsr.
    
[^210]: 通过合成到真实迁移学习进行解剖眼区隔离的多流注视估计

    Multistream Gaze Estimation with Anatomical Eye Region Isolation by Synthetic to Real Transfer Learning

    [https://arxiv.org/abs/2206.09256](https://arxiv.org/abs/2206.09256)

    该论文提出了一种多流注视估计的神经网络框架MSGazeNet，通过利用眼部解剖信息学习注视表示。研究首先使用合成数据集训练一个网络来隔离眼部区域，并通过域随机化实现将该网络转移到真实领域。通过合成到真实的迁移学习，实现了在真实世界眼部图像上生成掩模的目标。

    

    我们提出了一种新颖的神经网络框架MSGazeNet，通过多流注视估计从眼部解剖信息中学习注视表示。我们的解决方案由两个组件组成，第一个是用于隔离解剖眼区的网络，第二个是用于多流注视估计的网络。眼部区域隔离是使用U-Net风格的网络进行的，我们使用包含可见眼球和虹膜区域眼部区域掩模的合成数据集进行训练。在这个阶段使用的合成数据集是利用UnityEyes模拟器获得的，包含了80,000个眼部图像。在训练后，眼部区域隔离网络被转移到真实领域，用于生成真实世界眼部图像的掩模。为了成功进行迁移，我们在训练过程中利用领域随机化，通过类似于伪像的增强技术使合成图像受益于更大的变化。

    We propose a novel neural pipeline, MSGazeNet, that learns gaze representations by taking advantage of the eye anatomy information through a multistream framework. Our proposed solution comprises two components, first a network for isolating anatomical eye regions, and a second network for multistream gaze estimation. The eye region isolation is performed with a U-Net style network which we train using a synthetic dataset that contains eye region masks for the visible eyeball and the iris region. The synthetic dataset used in this stage is procured using the UnityEyes simulator, and consists of 80,000 eye images. Successive to training, the eye region isolation network is then transferred to the real domain for generating masks for the real-world eye images. In order to successfully make the transfer, we exploit domain randomization in the training process, which allows for the synthetic images to benefit from a larger variance with the help of augmentations that resemble artifacts. Th
    
[^211]: 深度主动学习的噪声稳定性

    Deep Active Learning with Noise Stability

    [https://arxiv.org/abs/2205.13340](https://arxiv.org/abs/2205.13340)

    本文提出了一种利用噪声稳定性来估计数据不确定性的深度主动学习算法，通过测量模型参数通过噪声进行随机扰动时，输出与原始观测值之间的差异。该算法适用于多个领域的任务。

    

    对于未标记的数据，不确定性估计对于主动学习至关重要。采用深度神经网络作为主干模型，由于模型推断的过度自信可能导致数据选择过程非常具有挑战性。现有方法借助特殊的学习方式（如对抗性学习）或辅助模型来应对这一挑战。然而，这往往导致复杂且低效的流程，使方法变得不切实际。在本文中，我们提出了一种利用噪声稳定性来估计数据不确定性的新算法。关键思想是通过在模型参数通过噪声进行随机扰动时，测量输出与原始观测值之间的差异。我们通过利用小高斯噪声理论进行理论分析，并证明我们的方法倾向于选择具有大而多样的梯度的子集。我们的方法通常适用于各种任务，包括计算机视觉、自然语言处理和结构数据分析。

    Uncertainty estimation for unlabeled data is crucial to active learning. With a deep neural network employed as the backbone model, the data selection process is highly challenging due to the potential over-confidence of the model inference. Existing methods resort to special learning fashions (e.g. adversarial) or auxiliary models to address this challenge. This tends to result in complex and inefficient pipelines, which would render the methods impractical. In this work, we propose a novel algorithm that leverages noise stability to estimate data uncertainty. The key idea is to measure the output derivation from the original observation when the model parameters are randomly perturbed by noise. We provide theoretical analyses by leveraging the small Gaussian noise theory and demonstrate that our method favors a subset with large and diverse gradients. Our method is generally applicable in various tasks, including computer vision, natural language processing, and structural data analy
    
[^212]: 通过打分规则最小化实现生成网络的概率预测

    Probabilistic Forecasting with Generative Networks via Scoring Rule Minimization

    [https://arxiv.org/abs/2112.08217](https://arxiv.org/abs/2112.08217)

    本论文提出了一种使用生成神经网络进行概率预测的方法，通过预测序列打分规则进行训练，避免了繁琐的超参数调整和不稳定的对抗训练，从而在概率预测中可靠地使用生成网络。

    

    概率预测依赖于过去的观察结果，以提供未来结果的概率分布，并通过打分规则与实际结果进行评估。在这里，我们使用生成神经网络进行概率预测，通过转换潜在变量的抽样来参数化高维空间上的分布。生成网络通常在对抗性框架中进行训练。相比之下，我们提出使用预测序列打分规则在记录的时间序列中训练生成网络，这种方法与常规评估预测系统的方式相一致。对于某些打分规则，可以实现无对抗的最小化；因此，我们的框架避免了繁琐的超参数调整和由于不稳定的对抗训练而导致的不确定性低估，从而在概率预测中可靠地使用生成网络。

    Probabilistic forecasting relies on past observations to provide a probability distribution for a future outcome, which is often evaluated against the realization using a scoring rule. Here, we perform probabilistic forecasting with generative neural networks, which parametrize distributions on high-dimensional spaces by transforming draws from a latent variable. Generative networks are typically trained in an adversarial framework. In contrast, we propose to train generative networks to minimize a predictive-sequential (or prequential) scoring rule on a recorded temporal sequence of the phenomenon of interest, which is appealing as it corresponds to the way forecasting systems are routinely evaluated. Adversarial-free minimization is possible for some scoring rules; hence, our framework avoids the cumbersome hyperparameter tuning and uncertainty underestimation due to unstable adversarial training, thus unlocking reliable use of generative networks in probabilistic forecasting. Furthe
    
[^213]: 学会公平：一种对公正决策的后果主义方法

    Learning to be Fair: A Consequentialist Approach to Equitable Decision-Making

    [https://arxiv.org/abs/2109.08792](https://arxiv.org/abs/2109.08792)

    本文提出了一种以后果为导向的设计公平算法的替代框架，这种方法主要考虑决策的后果，而非仅仅追求在种族或性别群体间的均衡。

    

    为了使算法公平，机器学习领域主要集中于在种族或性别群体间实现决策、结果或错误率的均衡。举个例子，考虑一个假设的政府共享出行项目，为即将上庭的低收入人群提供交通补助。遵循这个领域的研究，人们可能会根据每美元估计的治疗效果最高为标准分配乘车，同时限制各个种族群体的支出相等。然而，这种做法忽视了此类约束的下游后果，可能会导致意想不到的伤害。例如，如果某一人口群体距离法庭更远，实行平等支出必然意味着提供的总乘车次数减少，可能会导致更多人因缺席庭审而受到惩罚。本文提出了一种新的设计公平算法的框架，重点关注决策的后果。

    In an attempt to make algorithms fair, the machine learning literature has largely focused on equalizing decisions, outcomes, or error rates across race or gender groups. To illustrate, consider a hypothetical government rideshare program that provides transportation assistance to low-income people with upcoming court dates. Following this literature, one might allocate rides to those with the highest estimated treatment effect per dollar, while constraining spending to be equal across race groups. That approach, however, ignores the downstream consequences of such constraints, and, as a result, can induce unexpected harms. For instance, if one demographic group lives farther from court, enforcing equal spending would necessarily mean fewer total rides provided, and potentially more people penalized for missing court. Here we present an alternative framework for designing equitable algorithms that foregrounds the consequences of decisions. In our approach, one first elicits stakeholder
    
[^214]: 面向多维度在线决策的随机低秩张量赌博算法

    Stochastic Low-rank Tensor Bandits for Multi-dimensional Online Decision Making

    [https://arxiv.org/abs/2007.15788](https://arxiv.org/abs/2007.15788)

    这项研究提出了一种针对多维度在线决策的随机低秩张量赌博算法。通过考虑有上下文和没有上下文的情况，提出了两种学习算法，并推导了有限时间的遗憾界限。

    

    多维度在线决策在在线推荐和数字营销等实际应用中起着关键作用。在这些问题中，每个时间点的决策是来自不同类型实体的选择的组合。为了解决这个问题，我们引入了随机低秩张量赌博算法，一类其均值收益可表示为低秩张量的赌博算法。我们考虑了两种情况，即没有上下文的张量赌博和有上下文的张量赌博。在第一种情况中，平台旨在找到具有最高期望回报的最佳决策，即真实回报张量的最大条目。在第二种情况中，张量的某些模式是上下文，其余模式是决策，目标是在给定上下文信息的情况下找到最佳决策。我们提出了两种学习算法：张量消除和张量时代贪婪算法，用于没有上下文的张量赌博，并为它们推导了有限时间的遗憾界限。与现有的竞争算法相比，我们的算法表现更好。

    Multi-dimensional online decision making plays a crucial role in many real applications such as online recommendation and digital marketing. In these problems, a decision at each time is a combination of choices from different types of entities. To solve it, we introduce stochastic low-rank tensor bandits, a class of bandits whose mean rewards can be represented as a low-rank tensor. We consider two settings, tensor bandits without context and tensor bandits with context. In the first setting, the platform aims to find the optimal decision with the highest expected reward, a.k.a, the largest entry of true reward tensor. In the second setting, some modes of the tensor are contexts and the rest modes are decisions, and the goal is to find the optimal decision given the contextual information. We propose two learning algorithms tensor elimination and tensor epoch-greedy for tensor bandits without context, and derive finite-time regret bounds for them. Comparing with existing competitive m
    
[^215]: 通过运行时本地鲁棒性验证进行神经网络的输入验证

    Input Validation for Neural Networks via Runtime Local Robustness Verification

    [https://arxiv.org/abs/2002.03339](https://arxiv.org/abs/2002.03339)

    本文提出了通过运行时本地鲁棒性验证来验证神经网络输入的方法。实验证明，这种方法可以保护神经网络免受对抗性样本的影响，并提高准确性。

    

    本地鲁棒性验证可以验证神经网络对特定输入的扰动在一定距离内的鲁棒性。我们将这个距离称为鲁棒性半径。我们观察到，正确分类的输入的鲁棒性半径要比错误分类的输入（包括对抗性样本，特别是来自强对抗性攻击的样本）要大得多。另一个观察是，正确分类的输入的鲁棒性半径通常符合正态分布。基于这两个观察，我们提出通过运行时本地鲁棒性验证来验证神经网络的输入。实验证明，我们的方法可以保护神经网络免受对抗性样本的影响，并提高其准确性。

    Local robustness verification can verify that a neural network is robust wrt. any perturbation to a specific input within a certain distance. We call this distance Robustness Radius. We observe that the robustness radii of correctly classified inputs are much larger than that of misclassified inputs which include adversarial examples, especially those from strong adversarial attacks. Another observation is that the robustness radii of correctly classified inputs often follow a normal distribution. Based on these two observations, we propose to validate inputs for neural networks via runtime local robustness verification. Experiments show that our approach can protect neural networks from adversarial examples and improve their accuracies.
    
[^216]: 利用张量核减少深度聚类中的目标函数不匹配

    Leveraging tensor kernels to reduce objective function mismatch in deep clustering

    [https://arxiv.org/abs/2001.07026](https://arxiv.org/abs/2001.07026)

    本文研究了深度聚类中的目标函数不匹配（OFM）问题，并发现基于自编码器的方法容易导致降低聚类性能和重构与聚类目标之间的不匹配。为了解决这个问题，我们提出了一种新的辅助目标方法，称为无监督伴随对象（UCO），通过核函数在网络的中间表示上制定聚类目标。

    

    目标函数不匹配（OFM）指的是一个目标的优化对另一个目标的优化产生负面影响。在本研究中，我们研究了深度聚类中的OFM，并发现流行的基于自编码器的深度聚类方法既会降低聚类性能，又会导致重构目标和聚类目标之间存在显著的OFM。为了减少不匹配，同时保持辅助目标的结构保持特性，我们提出了一组新的用于深度聚类的辅助目标，称为无监督伴随对象（UCO）。UCOs依赖于核函数，在网络的中间表示上制定聚类目标。一般而言，中间表示可以包括除特征维度之外的其他维度，例如空间或时间。因此，我们认为简单地将其向量化并应用向量核对此类问题并不理想。

    Objective Function Mismatch (OFM) occurs when the optimization of one objective has a negative impact on the optimization of another objective. In this work we study OFM in deep clustering, and find that the popular autoencoder-based approach to deep clustering can lead to both reduced clustering performance, and a significant amount of OFM between the reconstruction and clustering objectives. To reduce the mismatch, while maintaining the structure-preserving property of an auxiliary objective, we propose a set of new auxiliary objectives for deep clustering, referred to as the Unsupervised Companion Objectives (UCOs). The UCOs rely on a kernel function to formulate a clustering objective on intermediate representations in the network. Generally, intermediate representations can include other dimensions, for instance spatial or temporal, in addition to the feature dimension. We therefore argue that the na\"ive approach of vectorizing and applying a vector kernel is suboptimal for such 
    
[^217]: 在任意线性变换下的自适应块稀疏正则化

    Adaptive Block sparse regularization under arbitrary linear transform. (arXiv:2401.15292v1 [cs.LG])

    [http://arxiv.org/abs/2401.15292](http://arxiv.org/abs/2401.15292)

    我们提出了一种在任意线性变换下重构具有块稀疏性的信号的方法，相比现有方法扩大了应用范围，并通过数值实验证明了其有效性。

    

    我们提出了一种用于在未知块结构下的任意线性变换下的块稀疏信号重构方法。该方法是现有方法LOP-$\ell_2$/$\ell_1$的推广，可以在非可逆变换下重构具有块稀疏性的信号，而LOP-$\ell_2$/$\ell_1$不能。我们的工作扩大了块稀疏正则化的范围，使其能够在各种信号处理领域中应用更加灵活和强大。我们推导了一个迭代算法来求解该方法，并给出了其收敛到最优解的条件。数值实验验证了该方法的有效性。

    We propose a convex signal reconstruction method for block sparsity under arbitrary linear transform with unknown block structure. The proposed method is a generalization of the existing method LOP-$\ell_2$/$\ell_1$ and can reconstruct signals with block sparsity under non-invertible transforms, unlike LOP-$\ell_2$/$\ell_1$. Our work broadens the scope of block sparse regularization, enabling more versatile and powerful applications across various signal processing domains. We derive an iterative algorithm for solving proposed method and provide conditions for its convergence to the optimal solution. Numerical experiments demonstrate the effectiveness of the proposed method.
    
[^218]: 强化学习代理中的新兴支配等级

    Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])

    [http://arxiv.org/abs/2401.12258](http://arxiv.org/abs/2401.12258)

    本研究在强化学习中探讨了一种新的支配等级现象，并证明了在没有明确编程和内在奖励的情况下，强化学习代理能够自主发明、学习、实施和传递支配等级给新的群体。

    

    现代强化学习算法在各种任务中能够胜过人类。多智能体强化学习(MARL)设置提出了额外的挑战，成功的混合动机代理协作取决于个体和群体目标之间的微妙平衡。社会习惯和规范，往往受到人类机构的启发，被用作实现这种平衡的工具。在本文中，我们研究了一种基本且经过深入研究的社会习惯，即支配等级，它在动物和人类社会中都存在。我们将支配等级的行为理论应用于人工智能代理，并尽可能少地修改现有的术语和定义。我们证明，在没有明确编程或内在奖励的情况下，强化学习代理的群体能够发明、学习、实施和传递支配等级给新的群体。所产生的支配等级有一个

    Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
    
[^219]: 通过随机森林机器学习非侵入性诊断急性间室综合征

    Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest Machine Learning. (arXiv:2401.10386v1 [cs.LG])

    [http://arxiv.org/abs/2401.10386](http://arxiv.org/abs/2401.10386)

    本研究提出了一种基于随机森林机器学习的非侵入性诊断急性间室综合征的方法。使用压力传感电阻器检测肌肉间室压力，并通过蓝牙传输结果到Web应用程序。该诊断方法在准确率、灵敏度和F1得分等关键性能指标方面表现出色。

    

    急性间室综合征（ACS）是一种骨科急症，由肌肉间室内的压力升高引起，导致永久组织损伤并最终致死。ACS的诊断主要依赖患者报告的症状，这种方法在临床上不可靠，通常需要通过侵入性肌肉间室压力测量进行补充。本研究提出了一种连续、客观、非侵入性的ACS诊断方法。该设备通过一个使用贴在皮肤上的压力传感电阻器（FSR）的随机森林机器学习模型检测ACS。最终诊断结果通过蓝牙以实时方式传输到Web应用程序。为了验证诊断结果，创建了一组包含FSR测量和相应的模拟肌肉间室压力的数据集。该诊断方法的准确率达到97%，与侵入性的金标准持平。该设备在关键性能指标包括准确度、灵敏度和F1得分方面表现优异。

    Acute compartment syndrome (ACS) is an orthopedic emergency, caused by elevated pressure within a muscle compartment, that leads to permanent tissue damage and eventually death. Diagnosis of ACS relies heavily on patient-reported symptoms, a method that is clinically unreliable and often supplemented with invasive intracompartmental pressure measurements. This study proposes a continuous, objective, noninvasive diagnostic for ACS. The device detects ACS through a random forest machine learning model that uses pressure readings from force-sensitive resistors (FSRs) placed on the skin. The final diagnosis is exported real-time to a web application via Bluetooth. To validate the diagnostic, a data set containing FSR measurements and the corresponding simulated intracompartmental pressure was created. The diagnostic achieved an accuracy, on par to the invasive gold standard, of 97%. The device excelled in key performance metrics including precision, sensitivity, and F1 score. Manufactured 
    
[^220]: 股票数据时间序列预测中的四元数神经网络

    Hypercomplex neural network in time series forecasting of stock data. (arXiv:2401.04632v1 [cs.NE])

    [http://arxiv.org/abs/2401.04632](http://arxiv.org/abs/2401.04632)

    本文测试了三种不同的神经网络架构，将其用于股票数据的时间序列预测。结果显示，具有超复数密集层的架构在准确性方面表现类似于其他架构，但可训练参数更少。此外，输入时间序列的顺序对有效性有影响。

    

    本文测试了三种用于时间序列预测的架构。它们的区别在于输入层包含卷积层、LSTM层或四元数4D代数的超复数层。输入是四个相关的股票市场时间序列，预测其中一个的结果。通过优化与架构类别相关的超参数，比较了最佳神经网络在类别内的表现。结果显示，在大多数情况下，具有超复数密集层的架构提供了与其他架构相似的MAE准确性，但可训练参数要少得多。由于这一点，超复数神经网络可以比其他测试的架构更快地学习和处理数据。此外，输入时间序列的顺序对有效性有影响。

    The three classes of architectures for time series prediction were tested. They differ by input layers which contain either convolutional, LSTM, or dense hypercomplex layers for 4D algebras. The input was four related Stock Market time series, and the prediction of one of them is expected. The optimization of hyperparameters related to the classes of architectures was performed in order to compare the best neural networks within the class. The results show that in most cases, the architecture with a hypercomplex dense layer provides similar MAE accuracy to other architectures, however, with considerably less trainable parameters. Thanks to it, hypercomplex neural networks can be learned and process data faster than the other tested architectures. Moreover, the order of the input time series has an impact on effectively.
    
[^221]: 推进深度主动学习和数据子集选择：用信息论直觉统一原则

    Advancing Deep Active Learning & Data Subset Selection: Unifying Principles with Information-Theory Intuitions. (arXiv:2401.04305v1 [cs.LG])

    [http://arxiv.org/abs/2401.04305](http://arxiv.org/abs/2401.04305)

    本论文探索了基于信息论原则的主动学习和主动采样方面的数据子集选择技术，以提高深度学习模型的标签和训练效率。

    

    本论文的核心目标是通过改进深度学习模型的标签和训练效率来提高深度学习的实用性。为此，我们探讨了基于信息论原则的数据子集选择技术，特别是主动学习和主动采样。主动学习提高了标签效率，而主动采样则提高了训练效率。监督式深度学习模型通常需要大量带标签的数据进行训练。标签获取可能既昂贵又耗时，并且训练大模型需要大量资源，这限制了其在学术研究和“大型科技公司”以外的应用。现有的深度学习数据子集选择方法通常依赖于启发式方法或缺乏基于信息论的原则基础。相比之下，本论文对深度学习中的数据子集选择目标及其应用进行了研究，力求通过信息论的启发，提出更具原则性的方法。

    At its core, this thesis aims to enhance the practicality of deep learning by improving the label and training efficiency of deep learning models. To this end, we investigate data subset selection techniques, specifically active learning and active sampling, grounded in information-theoretic principles. Active learning improves label efficiency, while active sampling enhances training efficiency. Supervised deep learning models often require extensive training with labeled data. Label acquisition can be expensive and time-consuming, and training large models is resource-intensive, hindering the adoption outside academic research and ``big tech.'' Existing methods for data subset selection in deep learning often rely on heuristics or lack a principled information-theoretic foundation. In contrast, this thesis examines several objectives for data subset selection and their applications within deep learning, striving for a more principled approach inspired by information theory. We begin 
    
[^222]: 《关于Transformer过度平滑的真相》

    Setting the Record Straight on Transformer Oversmoothing. (arXiv:2401.04301v1 [cs.LG])

    [http://arxiv.org/abs/2401.04301](http://arxiv.org/abs/2401.04301)

    Transformers are not inherently low-pass filters as previously thought and whether they oversmooth or not depends on the eigenspectrum of their update equations. Based on the analysis, a simple way to parameterize the weights of the Transformer update equations is derived, allowing for control over its spectrum and preventing oversmoothing.

    

    最近，基于Transformer的模型在各个领域取得了巨大成功。与此同时，最新的研究表明，Transformer本质上是一种低通滤波器，会逐渐过度平滑输入数据，降低其表示能力。一个自然的问题是：在存在这个缺陷的情况下，Transformer是如何取得这些成功的？在这项研究中，我们展示了事实上Transformer并不本质上是一种低通滤波器。相反，Transformer是否过度平滑取决于其更新方程的特征谱。我们的分析扩展了之前关于过度平滑和相关现象的研究。我们表明，许多成功的Transformer模型具有满足避免过度平滑条件的注意力和权重。基于这个分析，我们提出了一种简单的方法，对Transformer更新方程的权重进行参数化，使其可以控制其谱特性，确保不会发生过度平滑。与传统的方法相比，我们的方法可以更好地控制过度平滑问题。

    Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that gradually oversmooth the inputs, reducing the expressivity of their representations. A natural question is: How can Transformers achieve these successes given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Based on this analysis, we derive a simple way to parameterize the weights of the Transformer update equations that allows for control over its spectrum, ensuring that oversmoothing does not occur. Compared to a 
    
[^223]: 多语言指令调优中的多语言性

    Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])

    [http://arxiv.org/abs/2401.01854](http://arxiv.org/abs/2401.01854)

    本研究研究了多语言指令调优中的多语言性对跨语言指令遵循的影响。研究发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，只有40个多语言示例能够显著提高多语言指令遵循。总体来说，多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。

    

    随着大型语言模型（LLMs）的全球采纳，它们在多语言指令遵循能力变得越来越重要。一种有前途的方法是跨语言转移，通过在另一种语言上微调，模型可以在某种语言上获得特定的功能。本文研究了多语言LLM在指令调优过程中的多语言性对跨语言指令遵循的影响。首先我们发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，我们发现在英语调优集合中，只有40个多语言示例能够显著提高多语言指令遵循，在调优过程中不论是已见语言还是未见语言。总的来说，我们观察到在多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。

    As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
    
[^224]: 自我对弱语言模型进行细调可以将其转化为强语言模型

    Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])

    [http://arxiv.org/abs/2401.01335](http://arxiv.org/abs/2401.01335)

    本文提出了一种名为自我对弱语言模型进行细调（SPIN）的方法，通过模型自我对弈生成训练数据，并从中优化模型策略，从而将弱语言模型转化为强语言模型，无需额外的人类标注数据。

    

    通过监督细调（SFT）利用人类标注数据的力量对于推进大型语言模型（LLMs）至关重要。本文探讨了在不需要获取额外人类标注数据的情况下，将弱语言模型发展成为强语言模型的可能性。我们提出了一种名为自我对弱语言模型进行细调（SPIN）的新的细调方法，该方法从一个经过监督细调的模型开始。SPIN的核心是自我对弱语言模型的机制，其中弱语言模型通过与自身的实例对弈来提升自己的能力。具体而言，弱语言模型通过生成自己的训练数据来优化自身策略，通过区分自我生成的回应与来自人类标注数据的回应来改进。我们的方法逐步将弱语言模型提升为强大的模型，充分发掘人类标注示范数据在SFT中的潜力。在理论上，我们证明了该方法的训练目标函数的全局最优解是可以达到的。

    Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
    
[^225]: Local Discovery by Partitioning: 在有限先验知识下的多项式时间因果发现方法

    Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs. (arXiv:2310.17816v1 [stat.ML])

    [http://arxiv.org/abs/2310.17816](http://arxiv.org/abs/2310.17816)

    在有限先验知识下，通过局部分区发现算法（LDP），该研究解决了自动变量选择的问题。LDP根据与曝光-结果对{X,Y}相关的子集将变量集合Z进行分区，并区分混淆因素和其他变量类型。该算法具有理论保证，并在实践中观察到次二次的运行时间。

    

    该研究解决了在有限先验知识下自动变量选择的问题。给定一个{X,Y}的曝光-结果对和一个未知因果结构的变量集合Z，局部分区发现（LDP）算法将Z划分成与{X,Y}相关的子集。我们列举了任意Z的8个穷举且互不重复的分区，并利用这个分类法区分混淆因素和其他变量类型。LDP的动机是有效的调整集识别，但避免了自动变量选择方法中常见的预处理假设。我们提供了理论保证，LDP对于满足足够图形条件的任何Z都返回一个有效的调整集。在更强的条件下，我们证明了分区标签的渐近正确性。总独立性测试在|Z|的最坏情况下是二次的，经验上观察到次二次的运行时间。我们在合成数据上对理论保证进行了数值验证。

    This work addresses the problem of automated covariate selection under limited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable set Z of unknown causal structure, the Local Discovery by Partitioning (LDP) algorithm partitions Z into subsets defined by their relation to {X,Y}. We enumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z and leverage this taxonomy to differentiate confounders from other variable types. LDP is motivated by valid adjustment set identification, but avoids the pretreatment assumption commonly made by automated covariate selection methods. We provide theoretical guarantees that LDP returns a valid adjustment set for any Z that meets sufficient graphical conditions. Under stronger conditions, we prove that partition labels are asymptotically correct. Total independence tests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed empirically. We numerically validate our theoretical guarantees on synthetic 
    
[^226]: 受控解码来自语言模型

    Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])

    [http://arxiv.org/abs/2310.17022](http://arxiv.org/abs/2310.17022)

    本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。

    

    我们提出了一种新颖的离策略强化学习方法，称为受控解码（CD），用于控制自回归语言模型的生成，以获得高回报的结果。CD通过值函数来解决离策略强化学习问题，该值函数被称为前缀评分器。前缀评分器在推理时用于引导生成向更高回报的结果。我们展示了前缀评分器可以从（可能是）离策略数据中训练出来，用于预测从部分解码的响应继续解码时的预期回报。我们在Reddit对话语料库上经验证明，CD作为一种控制机制是有效的。我们还展示了CD设计的模块化使其能够有效解决多目标强化学习问题，而不会增加任何复杂性。最后，我们展示了CD可以以一种新颖的分块方式在推理时应用，同样无需任何额外的操作。

    We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
    
[^227]: 多标签学习中的神经坍缩问题研究

    Neural Collapse in Multi-label Learning with Pick-all-label Loss. (arXiv:2310.15903v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15903](http://arxiv.org/abs/2310.15903)

    这项论文研究了在多标签分类任务中的神经坍缩现象。他们推广了之前在多类别分类中发现的神经坍缩现象，证明了在“选择所有标签”公式下存在广义的神经坍缩现象。他们还发现了在广义的神经坍缩中的一个组合性质。

    

    我们通过神经坍缩（NC）的角度研究了深度神经网络在多标签分类（MLab）任务中的应用。之前的研究都局限于多类别分类，发现了一种普遍存在的NC现象，其中最后一层特征具有以下特点：（i）每个类别内的特征变异性为零，（ii）特征均值集合构成一个等角紧框架（ETF），（iii）最后一层分类器收缩到特征均值乘以某个缩放因子。我们将这个研究推广到多标签学习，并首次证明了“选择所有标签”公式存在广义NC现象。在自然的无约束特征模型（UFM）的情况下，我们证明了“选择所有标签”的交叉熵损失函数的全局分类器只显示出相同的ETF几何结构，进一步坍缩到多重性为1的特征类均值。

    We study deep neural networks for the multi-label classification (MLab) task through the lens of neural collapse (NC). Previous works have been restricted to the multi-class classification setting and discovered a prevalent NC phenomenon comprising of the following properties for the last-layer features: (i) the variability of features within every class collapses to zero, (ii) the set of feature means form an equi-angular tight frame (ETF), and (iii) the last layer classifiers collapse to the feature mean upon some scaling. We generalize the study to multi-label learning, and prove for the first time that a generalized NC phenomenon holds with the "pick-all-label" formulation. Under the natural analog of the unconstrained feature model (UFM), we establish that the only global classifier of the pick-all-label cross entropy loss display the same ETF geometry which further collapse to multiplicity-1 feature class means. Besides, we discover a combinatorial property in generalized NC whic
    
[^228]: 在DAG空间中使用基于模型的强化学习进行因果发现的树搜索

    Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery. (arXiv:2310.13576v1 [cs.LG])

    [http://arxiv.org/abs/2310.13576](http://arxiv.org/abs/2310.13576)

    本论文提出了一种在DAG空间中使用树搜索和模型驱动强化学习进行因果发现的方法，并通过两个实际任务的评估，证明了其在性能上显著优于目前最先进的无模型方法和贪婪搜索，具有很大的应用前景。

    

    识别因果结构对于许多领域至关重要，从战略决策到生物学和经济学。在这项工作中，我们提出了一种基于树搜索的因果发现的模型驱动强化学习方法，该方法逐步构建有向无环图。我们还形式化并证明了一种排除会引入循环的边的高效算法的正确性，这使得在DAG空间中进行更深入的离散搜索和采样成为可能。我们在两个实际任务中评估了我们的方法，在性能上比最先进的无模型方法和贪婪搜索取得了显著优势，这是组合方法的一个有希望的进展。

    Identifying causal structure is central to many fields ranging from strategic decision-making to biology and economics. In this work, we propose a model-based reinforcement learning method for causal discovery based on tree search, which builds directed acyclic graphs incrementally. We also formalize and prove the correctness of an efficient algorithm for excluding edges that would introduce cycles, which enables deeper discrete search and sampling in DAG space. We evaluate our approach on two real-world tasks, achieving substantially better performance than the state-of-the-art model-free method and greedy search, constituting a promising advancement for combinatorial methods.
    
[^229]: 自适应的对向编码用于链路预测

    Adaptive Pairwise Encodings for Link Prediction. (arXiv:2310.11009v1 [cs.LG])

    [http://arxiv.org/abs/2310.11009](http://arxiv.org/abs/2310.11009)

    提出了一种自适应的对向编码方法，用于解决链路预测中现有方法的归纳偏差问题。该方法将消息传递神经网络和启发式方法结合起来，能够更好地分类各种不同因素形成的链路。

    

    链路预测是一种常见的基于图结构数据的任务，在各个领域都有应用。经典方法通常使用手工设计的启发式策略来进行预测。启发式度量被选择为在与链路形成相关的基本因素上与之相关良好。近年来，出现了一类新的方法，将消息传递神经网络（MPNN）的优势与启发式方法结合起来。这些方法通过使用MPNN的输出以及捕捉候选链路中节点之间关系的“对向编码”来进行预测。它们已经在许多数据集上表现出强大的性能。然而，目前的对向编码往往具有强烈的归纳偏差，使用相同的基本因素来分类所有链路。这限制了现有方法学习如何正确分类可能由不同因素形成的各种不同链路的能力。为了解决这个问题，我们提出了一个自适应的对向编码方法。

    Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a 
    
[^230]: 人类课程指导下的指令调整

    Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])

    [http://arxiv.org/abs/2310.09518](http://arxiv.org/abs/2310.09518)

    本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。

    

    指令调整的主流范式是随机洗牌训练最大多样化指令-响应对。本文探讨了在当代大型语言模型如ChatGPT和GPT-4中应用结构化认知学习方法进行指令调整的潜在好处。与以往传统的随机化指令数据集不同，我们提出了一个高度结构化的合成数据集，模拟了人类教育的渐进性和有组织性。我们通过将数据集与教育框架对齐来策划我们的数据集，为每个样本包括主题和认知严谨程度等元信息。我们的数据集涵盖了从中学到研究生阶段的全面细粒度主题，每个主题都有各种问题，以利用布鲁姆的认知分级法提高概念深度，该分级法用于区分每个概念的不同人类认知水平。结果表明，这种认知学习方法优于传统的随机化方法，提高了指令调整的性能。

    The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
    
[^231]: 平均奖励马尔可夫决策过程的最优样本复杂度

    Optimal Sample Complexity for Average Reward Markov Decision Processes. (arXiv:2310.08833v1 [cs.LG])

    [http://arxiv.org/abs/2310.08833](http://arxiv.org/abs/2310.08833)

    本论文解决了对于均匀收敛的马尔可夫决策过程的长期平均奖励最大化策略学习的样本复杂度问题，并建立了一个样本复杂度为$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$的优化策略估计器。

    

    我们在假设有一个生成模型的情况下，解决了与均匀收敛的马尔可夫决策过程相关的长期平均奖励的策略学习的样本复杂性问题。在这个背景下，现有的文献提供了一个样本复杂度的上界，$ \widetilde O(|S||A|t_{\text{mix}}^2 \epsilon^{-2})$，和一个下界，$\Omega(|S||A|t_{\text{mix}} \epsilon^{-2})$。在这些表达式中，$|S|$和$|A|$分别表示状态空间和动作空间的势，$t_{\text{mix}}$作为总变异混合时间的统一上限，$\epsilon$表示误差容忍度。因此，$t_{\text{mix}}$仍然存在一个显着的差距需要填补。我们的主要贡献是建立一个优化策略的估计器，其样本复杂度为$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$，有效地达到了文献中的下界。这是通过结合算法思想实现的。

    We settle the sample complexity of policy learning for the maximization of the long run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. In this context, the existing literature provides a sample complexity upper bound of $\widetilde O(|S||A|t_{\text{mix}}^2 \epsilon^{-2})$ and a lower bound of $\Omega(|S||A|t_{\text{mix}} \epsilon^{-2})$. In these expressions, $|S|$ and $|A|$ denote the cardinalities of the state and action spaces respectively, $t_{\text{mix}}$ serves as a uniform upper limit for the total variation mixing times, and $\epsilon$ signifies the error tolerance. Therefore, a notable gap of $t_{\text{mix}}$ still remains to be bridged. Our primary contribution is to establish an estimator for the optimal policy of average reward MDPs with a sample complexity of $\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$, effectively reaching the lower bound in the literature. This is achieved by combining algorithmic idea
    
[^232]: FedMFS: 选择性模态通信的联邦多模态融合学习

    FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication. (arXiv:2310.07048v1 [cs.LG])

    [http://arxiv.org/abs/2310.07048](http://arxiv.org/abs/2310.07048)

    FedMFS是一种新的多模态融合联邦学习方法，通过选择性模态通信解决了缺乏特定模态的异构客户问题，并设计了最优的模态上传策略以提高学习性能。

    

    联邦学习是一种分布式机器学习范式，通过仅共享模型参数而不访问、侵犯或泄露原始用户数据，使客户能够合作。在物联网中，边缘设备越来越多地利用多模态数据组合和融合范式来提高模型性能。然而，在联邦学习应用中，仍然存在两个主要挑战：（一）解决由于缺乏特定模态的异构客户引起的问题；（二）设计一种最优的模态上传策略，以最小化通信开销同时最大化学习性能。在本文中，我们提出了一种新的多模态融合联邦学习方法，名为FedMFS，可以解决上述挑战。关键思想是利用Shapley值来量化每个模态的贡献和模态模型大小来衡量通信开销，以便每个客户端可以。

    Federated learning (FL) is a distributed machine learning (ML) paradigm that enables clients to collaborate without accessing, infringing upon, or leaking original user data by sharing only model parameters. In the Internet of Things (IoT), edge devices are increasingly leveraging multimodal data compositions and fusion paradigms to enhance model performance. However, in FL applications, two main challenges remain open: (i) addressing the issues caused by heterogeneous clients lacking specific modalities and (ii) devising an optimal modality upload strategy to minimize communication overhead while maximizing learning performance. In this paper, we propose Federated Multimodal Fusion learning with Selective modality communication (FedMFS), a new multimodal fusion FL methodology that can tackle the above mentioned challenges. The key idea is to utilize Shapley values to quantify each modality's contribution and modality model size to gauge communication overhead, so that each client can 
    
[^233]: 贝叶斯质量多样性方法用于混合连续、离散和分类变量的约束优化问题

    Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables. (arXiv:2310.05955v1 [math.OC])

    [http://arxiv.org/abs/2310.05955](http://arxiv.org/abs/2310.05955)

    本论文提出了一种基于贝叶斯优化的新型质量多样性方法，用于解决混合连续、离散和分类变量的约束优化问题，旨在提供具有多样性性质的最优解集。

    

    复杂的工程设计问题通常涉及到使用费时的模拟代码来预测待设计系统的行为和性能。为了进行系统设计，这些代码经常嵌入到优化过程中来提供最佳设计方案，同时满足设计约束条件。最近，提出了一种新的质量多样性方法，旨在增强设计空间的探索能力，并提供一组具有多样性性质的最优解集，以评估权衡。此外，复杂的工程设计问题通常涉及到混合连续、离散和分类的设计变量，以考虑在优化问题中的技术选择。本文提出了一种基于混合连续、离散和分类贝叶斯优化的新型质量多样性方法。

    Complex engineering design problems, such as those involved in aerospace, civil, or energy engineering, require the use of numerically costly simulation codes in order to predict the behavior and performance of the system to be designed. To perform the design of the systems, these codes are often embedded into an optimization process to provide the best design while satisfying the design constraints. Recently, new approaches, called Quality-Diversity, have been proposed in order to enhance the exploration of the design space and to provide a set of optimal diversified solutions with respect to some feature functions. These functions are interesting to assess trade-offs. Furthermore, complex engineering design problems often involve mixed continuous, discrete, and categorical design variables allowing to take into account technological choices in the optimization problem. In this paper, a new Quality-Diversity methodology based on mixed continuous, discrete and categorical Bayesian opti
    
[^234]: 具有平均光滑度的高效无偏学习

    Efficient Agnostic Learning with Average Smoothness. (arXiv:2309.17016v1 [cs.LG])

    [http://arxiv.org/abs/2309.17016](http://arxiv.org/abs/2309.17016)

    该论文研究了基于平均光滑度的无参回归问题，提出了无分布限制下的统一收敛界限和高效无偏学习算法。

    

    我们研究了在非参数回归中无分布限制的平均光滑度概念，该概念由Ashlagi等人（2021）提出，用于衡量函数相对于任意未知潜在分布的"有效"光滑度。最近的Hanneke等人（2023）的研究在可实现情况下建立了平均光滑函数的紧密一致收敛界限，并提供了具有高效可实现性的学习算法，但这些结果目前在普遍无偏（即有噪声）情况下尚缺乏类似结果。在这项工作中，我们完全填补了这些差距。首先，我们为无偏设置中的平均光滑类提供了一个无分布一致收敛界限。其次，我们将所得到的样本复杂度与一个具有高效无偏学习算法相匹配。我们的结果以数据的内在几何形状为基础，适用于任何全有界度量空间，并展示了最近在可实现情况下获得的保证。

    We study distribution-free nonparametric regression following a notion of average smoothness initiated by Ashlagi et al. (2021), which measures the "effective" smoothness of a function with respect to an arbitrary unknown underlying distribution. While the recent work of Hanneke et al. (2023) established tight uniform convergence bounds for average-smooth functions in the realizable case and provided a computationally efficient realizable learning algorithm, both of these results currently lack analogs in the general agnostic (i.e. noisy) case.  In this work, we fully close these gaps. First, we provide a distribution-free uniform convergence bound for average-smoothness classes in the agnostic setting. Second, we match the derived sample complexity with a computationally efficient agnostic learning algorithm. Our results, which are stated in terms of the intrinsic geometry of the data and hold over any totally bounded metric space, show that the guarantees recently obtained for realiz
    
[^235]: 基于多编码器自编码器的自监督盲源分离

    Self-Supervised Blind Source Separation via Multi-Encoder Autoencoders. (arXiv:2309.07138v1 [eess.SP])

    [http://arxiv.org/abs/2309.07138](http://arxiv.org/abs/2309.07138)

    本论文提出了一种基于多编码器自编码器和自监督学习的方法，用于解决盲源分离问题。通过训练网络进行输入解码和重构，然后利用编码掩蔽技术进行源推断，同时引入路径分离损失以促进稀疏性。

    

    盲源分离（BSS）的任务是在没有先验知识的情况下从混合信号中分离出源信号和混合系统。这是一个具有挑战性的问题，通常需要对混合系统和源信号做出限制性的假设。本文提出了一种新颖的方法来解决非线性混合的BSS问题，该方法利用多编码器自编码器的自然特征子空间专门化能力，并通过完全自监督学习进行训练，而不需要强先验知识。在训练阶段，我们的方法将输入解码成多编码器网络的单独编码空间，然后在解码器内重新混合这些表示以重构输入。然后，为了进行源推断，我们引入了一种新颖的编码掩蔽技术，即屏蔽除一个编码外的所有编码，使得解码器能够估计源信号。为此，我们还引入了一种称为路径分离损失的方法，以促进编码之间的稀疏性。

    The task of blind source separation (BSS) involves separating sources from a mixture without prior knowledge of the sources or the mixing system. This is a challenging problem that often requires making restrictive assumptions about both the mixing system and the sources. In this paper, we propose a novel method for addressing BSS of non-linear mixtures by leveraging the natural feature subspace specialization ability of multi-encoder autoencoders with fully self-supervised learning without strong priors. During the training phase, our method unmixes the input into the separate encoding spaces of the multi-encoder network and then remixes these representations within the decoder for a reconstruction of the input. Then to perform source inference, we introduce a novel encoding masking technique whereby masking out all but one of the encodings enables the decoder to estimate a source signal. To this end, we also introduce a so-called pathway separation loss that encourages sparsity betwe
    
[^236]: 大尺度和无穷宽度下的深度学习勒让演讲

    Les Houches Lectures on Deep Learning at Large & Infinite Width. (arXiv:2309.01592v1 [stat.ML])

    [http://arxiv.org/abs/2309.01592](http://arxiv.org/abs/2309.01592)

    本论文主要以无穷宽度和大宽度范围内的深度神经网络为研究对象，讨论了这些网络的各种统计和动力学特性，包括随机网络的性质、训练后的网络与线性模型、核函数和高斯过程之间的关系，以及对大但有限宽度网络在初始化和训练后的摄动和非摄动处理。

    

    这些演讲是在2022年勒让夏季学校统计物理和机器学习课程上展示的，着重探讨了深度神经网络在无限宽度和大宽度范围内的情况。涵盖的主题包括这些网络的各种统计和动力学特性。特别是，讲师们讨论了随机深度神经网络的特性；训练过的深度神经网络，线性模型，核函数和高斯过程之间的联系，这些联系在无穷宽度的极限下出现；以及在初始化和训练后对大但有限宽度网络的摄动和非摄动处理。

    These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
    
[^237]: SafeAR: 通过风险感知策略实现更安全的算法补偿

    SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v1 [cs.LG])

    [http://arxiv.org/abs/2308.12367](http://arxiv.org/abs/2308.12367)

    本文提出了一种更安全的算法补救方法（SafeAR），该方法通过考虑风险因素在计算和评估补救措施时，为那些受到机器学习模型决策不利影响的个体提供更可靠的建议。

    

    随着机器学习模型在金融和医疗等关键领域的广泛使用，为那些受到机器学习模型决策不利影响的个体提供补救措施的需求变得更加重要；个体应该获得改善自身情况和获得有利决策的建议。之前关于顺序算法补救的工作——推荐一系列变化——主要关注行动的可行性，并使用特征变化的接近程度确定行动成本。然而，未考虑特征变化的不确定性和补救中高于平均成本的风险。如果补救措施可能（以一定概率）导致更糟糕的情况，而恢复需要付出非常高的代价，那将是不可取的。在计算和评估补救措施时，必须考虑风险。我们将考虑了这种风险因素计算出的补救措施称为更安全的算法补救（SafeAR）。

    With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The ob
    
[^238]: MDB：互动查询数据集和模型

    MDB: Interactively Querying Datasets and Models. (arXiv:2308.06686v1 [cs.DB])

    [http://arxiv.org/abs/2308.06686](http://arxiv.org/abs/2308.06686)

    MDB是一个调试框架，用于互动查询数据集和模型。它通过集成函数式编程与关系代数，能够快速迭代和优化查询，发现和描述错误和模型行为。实验证明，MDB比其他工具能够实现更快的查询速度加快和查询长度缩短。

    

    随着模型的训练和部署，开发者需要能够系统地调试在机器学习流程中出现的错误。我们提出了MDB，一个用于互动查询数据集和模型的调试框架。MDB通过将函数式编程与关系代数结合起来，构建了一个对数据集和模型预测的数据库进行表达性查询的工具。查询可重用且易于修改，使得调试人员能够快速迭代和优化查询，以发现和描述错误和模型行为。我们在目标检测、偏差发现、图像分类和数据填充任务中评估了MDB在自动驾驶视频、大型语言模型和医疗记录上的性能。我们的实验证明，MDB比其他基准测试工具能够实现最高10倍的查询速度加快和40%的查询长度缩短。在用户研究中，我们发现开发者能够成功构建复杂查询来描述机器学习模型的错误。

    As models are trained and deployed, developers need to be able to systematically debug errors that emerge in the machine learning pipeline. We present MDB, a debugging framework for interactively querying datasets and models. MDB integrates functional programming with relational algebra to build expressive queries over a database of datasets and model predictions. Queries are reusable and easily modified, enabling debuggers to rapidly iterate and refine queries to discover and characterize errors and model behaviors. We evaluate MDB on object detection, bias discovery, image classification, and data imputation tasks across self-driving videos, large language models, and medical records. Our experiments show that MDB enables up to 10x faster and 40\% shorter queries than other baselines. In a user study, we find developers can successfully construct complex queries that describe errors of machine learning models.
    
[^239]: 利用替代特征选择找到最优的多样特征集

    Finding Optimal Diverse Feature Sets with Alternative Feature Selection. (arXiv:2307.11607v1 [cs.LG])

    [http://arxiv.org/abs/2307.11607](http://arxiv.org/abs/2307.11607)

    本文引入了替代特征选择的概念，将其形式化为优化问题，并通过约束定义了替代特征集，使用户可以控制替代的数量和差异性。我们证明了该问题的NP-hard性，并讨论了如何将传统特征选择方法作为目标集成。实验证明替代特征集确实可以具有高预测质量，同时分析了几个影响因素。

    

    特征选择是获取小型、可解释且高精度预测模型的一种常见方法。传统的特征选择方法通常只能得到一个特征集，这在某些场景下可能不足够。例如，用户可能对寻找具有相似预测质量但提供不同数据解释的替代特征集感兴趣。在本文中，我们引入了替代特征选择，并将其形式化为一个优化问题。特别地，我们通过约束定义了替代特征，并使用户可以控制替代的数量和差异性。接下来，我们分析了这个优化问题的复杂性并展示了其NP-hard性质。进一步地，我们讨论了如何将传统的特征选择方法作为目标集成。最后，我们使用30个分类数据集评估了替代特征选择的效果。我们观察到替代特征集确实可能具有较高的预测质量，并分析了几个影响这一结果的因素。

    Feature selection is popular for obtaining small, interpretable, yet highly accurate prediction models. Conventional feature-selection methods typically yield one feature set only, which might not suffice in some scenarios. For example, users might be interested in finding alternative feature sets with similar prediction quality, offering different explanations of the data. In this article, we introduce alternative feature selection and formalize it as an optimization problem. In particular, we define alternatives via constraints and enable users to control the number and dissimilarity of alternatives. Next, we analyze the complexity of this optimization problem and show NP-hardness. Further, we discuss how to integrate conventional feature-selection methods as objectives. Finally, we evaluate alternative feature selection with 30 classification datasets. We observe that alternative feature sets may indeed have high prediction quality, and we analyze several factors influencing this ou
    
[^240]: 分期变分推断：何时以及为什么使用？

    Amortized Variational Inference: When and Why?. (arXiv:2307.11018v1 [stat.ML])

    [http://arxiv.org/abs/2307.11018](http://arxiv.org/abs/2307.11018)

    本文研究了分期变分推断作为近似后验推断的一种通用替代方法，探讨了何时能够达到与传统的因子化变分推断相同的最优解。

    

    分期变分推断（A-VI）是一种近似处理概率模型中的难以计算的后验分布的方法。A-VI的定义特点是学习一个全局推断函数，将每个观察映射到其局部潜变量的近似后验分布。这与更传统的分解（或均场）变分推断（F-VI）形成对比，后者直接学习每个潜变量的近似分布的参数。在深度生成模型中，A-VI用作加速局部潜变量推断的计算技巧。本文研究A-VI作为近似后验推断的一种通用替代方法。由于分期家族是分解家族的子集，A-VI无法产生比F-VI最优解更低的Kullback-Leibler散度的近似值。因此，一个核心的理论问题是刻画A-VI何时仍然达到F-VI的最优解。

    Amortized variational inference (A-VI) is a method for approximating the intractable posterior distributions that arise in probabilistic models. The defining feature of A-VI is that it learns a global inference function that maps each observation to its local latent variable's approximate posterior. This stands in contrast to the more classical factorized (or mean-field) variational inference (F-VI), which directly learns the parameters of the approximating distribution for each latent variable. In deep generative models, A-VI is used as a computational trick to speed up inference for local latent variables. In this paper, we study A-VI as a general alternative to F-VI for approximate posterior inference. A-VI cannot produce an approximation with a lower Kullback-Leibler divergence than F-VI's optimal solution, because the amortized family is a subset of the factorized family. Thus a central theoretical problem is to characterize when A-VI still attains F-VI's optimal solution. We deri
    
[^241]: 非线性处理与线性光学

    Nonlinear Processing with Linear Optics. (arXiv:2307.08533v2 [physics.optics] UPDATED)

    [http://arxiv.org/abs/2307.08533](http://arxiv.org/abs/2307.08533)

    该论文提出了一种利用多次散射实现多层光学网络的新框架，可以以低光功率同时合成线性和非线性转换，实现能量高效和高速的光学实现神经网络。

    

    深度神经网络通过利用多层数据处理来提取隐藏的表征，取得了显着的突破，但却以大电子计算能力为代价。为了提高能量效率和速度，光学实现神经网络的目标是利用光学带宽的优势和光学互连的能量效率。在缺乏低功率光学非线性性的情况下，在实现多层光学网络中的挑战在于实现多个光学层，而不依赖电子元件。在本研究中，我们提出了一个新颖的框架，利用多次散射可以同时以低光功率合成可编程的线性和非线性转换，利用散射势能（由数据表示）与散射场之间的非线性关系。理论和实验研究表明，通过多次散射进行数据重复可以实现多个光学层。

    Deep neural networks have achieved remarkable breakthroughs by leveraging multiple layers of data processing to extract hidden representations, albeit at the cost of large electronic computing power. To enhance energy efficiency and speed, the optical implementation of neural networks aims to harness the advantages of optical bandwidth and the energy efficiency of optical interconnections. In the absence of low-power optical nonlinearities, the challenge in the implementation of multilayer optical networks lies in realizing multiple optical layers without resorting to electronic components. In this study, we present a novel framework that uses multiple scattering that is capable of synthesizing programmable linear and nonlinear transformations concurrently at low optical power by leveraging the nonlinear relationship between the scattering potential, represented by data, and the scattered field. Theoretical and experimental investigations show that repeating the data by multiple scatte
    
[^242]: 从随机游走到图形快跑：一种在连续时间动态图上具有低延迟的节点嵌入框架

    From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs. (arXiv:2307.08433v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08433](http://arxiv.org/abs/2307.08433)

    这篇论文介绍了一种在连续时间动态图上具有低延迟的节点嵌入框架，通过提出流式低延迟的近似随机游走特征，计算时间感知节点嵌入以总结多跳信息。

    

    许多真实世界的数据集具有基础的动态图结构，其中实体和它们的相互作用随时间演变。机器学习模型应考虑这些动态因素，以在下游任务中充分发挥其潜力。以前用于图表示学习的方法要么侧重于抽样k-跳邻域，类似于广度优先搜索，要么侧重于随机游走，类似于深度优先搜索。然而，这些方法在实时动态图上进行低延迟推断是计算上昂贵且不适用的。为了克服这些限制，我们提出了图形快跑，这是一个适用于连续时间动态图（CTDGs）的通用特征提取框架，具有低延迟，并且与高延迟模型相比具有竞争力。为了实现这一点，我们提出了一种流式、低延迟的近似随机游走特征。在我们的框架中，使用仅单跳操作计算总结多跳信息的时间感知节点嵌入。

    Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop ope
    
[^243]: 通过双层ReLU神经网络实现可证明的多任务表示学习

    Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks. (arXiv:2307.06887v1 [cs.LG])

    [http://arxiv.org/abs/2307.06887](http://arxiv.org/abs/2307.06887)

    通过双层ReLU神经网络，本论文提出了一种可证明的多任务表示学习方法，用于解决神经网络在实践中同时训练多个任务时遇到的问题。

    

    特征学习是神经网络实际成功的关键，然而如何以及为何发生特征学习仍然难以解释。最近的理论研究表明，在用梯度下降方法优化的浅层神经网络上可以学习有意义的特征，扩展了我们对于神经切向核或随机特征范例中微不足道的特征学习的了解。然而，在实践中，神经网络越来越经常地同时训练多个具有不同损失函数的任务，并且这些先前的分析并不适用于这种情况。在多任务学习设置中，各种研究已经表明简单线性模型可以有效地进行特征学习。然而，通过非线性模型进行多任务学习，这在实践中是最常见的学习范式，仍然存在许多未知。在这项工作中，我们首次提出了一种可证明的多任务表示学习方法，通过双层ReLU神经网络实现。

    Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first 
    
[^244]: 面向深度神经网络验证的可证明审查器

    Towards a Certified Proof Checker for Deep Neural Network Verification. (arXiv:2307.06299v1 [cs.LO])

    [http://arxiv.org/abs/2307.06299](http://arxiv.org/abs/2307.06299)

    面向DNN验证的一种新型证明检查器实现，通过提供数值稳定性和更大的可验证性改进现有搜索工具存在的问题。这一实现利用了Imandra的两个关键能力：无限精度实数算术和形式化验证基础设施。

    

    最近深度神经网络（DNNs）的发展导致了它们在安全关键系统中的应用，从而增加了对其安全性的保证的需求。可以使用由验证社区开发的工具来证明DNN的这些安全性质。然而，这些工具本身容易出现实现错误和数值稳定性问题，这使得它们的可靠性值得怀疑。为了克服这个问题，一些验证器会产生可以由可信检查器检查的结果证明。在这项工作中，我们提出了一种用于DNN验证的新型证明检查器的实现。它通过提供数值稳定性和更大的可验证性来改进现有的实现。为了实现这一点，我们利用了Imandra（一种工业级的定理证明器）的两个关键能力：对无限精度实数算术的支持和其形式化验证基础设施。迄今为止，我们在Imandra中实现了一个证明检查器，规定了其正确性属性和开始

    Recent developments in deep neural networks (DNNs) have led to their adoption in safety-critical systems, which in turn has heightened the need for guaranteeing their safety. These safety properties of DNNs can be proven using tools developed by the verification community. However, these tools are themselves prone to implementation bugs and numerical stability problems, which make their reliability questionable. To overcome this, some verifiers produce proofs of their results which can be checked by a trusted checker. In this work, we present a novel implementation of a proof checker for DNN verification. It improves on existing implementations by offering numerical stability and greater verifiability. To achieve this, we leverage two key capabilities of Imandra, an industrial theorem prover: its support of infinite precision real arithmetic and its formal verification infrastructure. So far, we have implemented a proof checker in Imandra, specified its correctness properties and start
    
[^245]: 使用预测编码和不确定性最小化的主动感知

    Active Sensing with Predictive Coding and Uncertainty Minimization. (arXiv:2307.00668v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00668](http://arxiv.org/abs/2307.00668)

    本论文提出了一种基于预测编码和不确定性最小化的主动感知方法，能够在各种任务中实现有效的探索和学习。

    

    我们提出了一种基于预测编码和不确定性最小化的全流程的主动探索方法。该方法可以在任何任务无关和内在驱动的情境下应用于探索。我们首先在迷宫导航任务中展示了我们的方法，并展示了我们的模型能够发现基础的转换分布并重建环境的空间特征。其次，我们将我们的模型应用于更复杂的主动视觉任务中，其中一个代理必须主动采样其视觉环境以获取信息。我们展示了我们的模型能够构建无监督表示，使其能够主动采样和高效分类感知场景。我们进一步展示，使用这些表示作为下游分类的输入相比其他基线方法具有更高的数据效率和学习速度，同时保持更低的参数复杂度。

    We present an end-to-end procedure for embodied exploration based on two biologically inspired computations: predictive coding and uncertainty minimization. The procedure can be applied to any exploration setting in a task-independent and intrinsically driven manner. We first demonstrate our approach in a maze navigation task and show that our model is capable of discovering the underlying transition distribution and reconstructing the spatial features of the environment. Second, we apply our model to the more complex task of active vision, where an agent must actively sample its visual environment to gather information. We show that our model is able to build unsupervised representations that allow it to actively sample and efficiently categorize sensory scenes. We further show that using these representations as input for downstream classification leads to superior data efficiency and learning speed compared to other baselines, while also maintaining lower parameter complexity. Final
    
[^246]: 基于U-Net和Segment Anything Model的乳腺肿瘤在超声和乳腺X线图像中的对比分析

    Comparative Analysis of Segment Anything Model and U-Net for Breast Tumor Detection in Ultrasound and Mammography Images. (arXiv:2306.12510v1 [eess.IV])

    [http://arxiv.org/abs/2306.12510](http://arxiv.org/abs/2306.12510)

    研究采用U-Net和pretrained SAM两种深度学习架构，针对乳腺超声和乳腺X线图像，进行肿瘤区域的识别和分割。结果表明，U-Net模型对于不同类型的良性和恶性肿瘤的识别和分割效果更好。

    

    本研究的主要目标是开发一种能够在乳腺超声（BUS）和乳腺X线图像中识别和描绘肿瘤区域的算法。该技术采用了两种先进的深度学习架构，即U-Net和pretrained SAM，用于肿瘤分割。U-Net模型专门设计用于医学图像分割，利用其深卷积神经网络框架从输入图像中提取有意义的特征。另一方面，pretrained SAM架构引入了一种机制来捕捉空间依赖关系并生成分割结果。评估在不同良性和恶性肿瘤的注释肿瘤区域的多样化数据集上进行。结果表明，U-Net模型能够在BUS和乳腺X线图像中准确地识别和分割乳腺肿瘤，且优于pretrained SAM架构。

    In this study, the main objective is to develop an algorithm capable of identifying and delineating tumor regions in breast ultrasound (BUS) and mammographic images. The technique employs two advanced deep learning architectures, namely U-Net and pretrained SAM, for tumor segmentation. The U-Net model is specifically designed for medical image segmentation and leverages its deep convolutional neural network framework to extract meaningful features from input images. On the other hand, the pretrained SAM architecture incorporates a mechanism to capture spatial dependencies and generate segmentation results. Evaluation is conducted on a diverse dataset containing annotated tumor regions in BUS and mammographic images, covering both benign and malignant tumors. This dataset enables a comprehensive assessment of the algorithm's performance across different tumor types. Results demonstrate that the U-Net model outperforms the pretrained SAM architecture in accurately identifying and segment
    
[^247]: 《可解释人工智能中的对抗性攻击和防御：调查报告》

    Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])

    [http://arxiv.org/abs/2306.06123](http://arxiv.org/abs/2306.06123)

    本文总结了对抗性攻击和防御在可解释人工智能中的研究。列出了现有的不安全因素，并表明了本领域的新兴研究方向。

    

    可解释人工智能（XAI）方法被描绘为调试和信任统计和深度学习模型的治疗方式，以及解释它们的预测。然而，对抗机器学习的最新进展突出了最新解释的局限性和漏洞，这些进展令人对其安全性和可信度产生质疑。操纵、欺骗或洗白模型推理证据的可能性在高风险决策和知识发现中产生不利后果。本文总结了50多篇论文的研究，概述了针对机器学习模型解释的对抗攻击以及公平度量的研究。我们讨论了如何防御攻击并设计鲁棒的解释方法。我们列出XAI中现有的不安全因素，并概述了对抗性XAI（AdvXAI）的新兴研究方向。

    Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning highlight the limitations and vulnerabilities of state-of-the-art explanations, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This concise survey of over 50 papers summarizes research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI).
    
[^248]: 用于组合优化的神经算法推理

    Neural Algorithmic Reasoning for Combinatorial Optimisation. (arXiv:2306.06064v1 [cs.NE])

    [http://arxiv.org/abs/2306.06064](http://arxiv.org/abs/2306.06064)

    本文提出了一种用于组合优化问题的神经算法推理方法，旨在解决旅行商问题。该方法是通过在TSP实例训练之前，将神经模型用相关算法进行预训练来实现的。实验结果表明，该方法可以显著提高TSP问题的解决效率。

    

    使用神经网络解决NP难/完全组合问题是一个挑战性的研究领域，旨在超越传统的近似算法。其长期目标是通过学习仅从训练数据生成更优解来超越手工设计的启发式算法，而旅行商问题(TSP)是经常被这些方法瞄准的一个重要的组合优化问题。然而，目前用于解决TSP的基于神经网络的方法常常忽略了问题固有的“算法”本质。与此相反，设计用于TSP的启发式方法常常利用诸如查找最小生成树之类的成熟算法。在本文中，我们提出利用神经算法推理的最新进展来改进TSP问题的学习。具体来说，我们建议在对TSP实例进行训练之前，在相关算法上对我们的神经模型进行预训练。我们的结果表明，使用这种学习方法可以显著提高TSP问题的解决效率。

    Solving NP-hard/complete combinatorial problems with neural networks is a challenging research area that aims to surpass classical approximate algorithms. The long-term objective is to outperform hand-designed heuristics for NP-hard/complete problems by learning to generate superior solutions solely from training data. The Travelling Salesman Problem (TSP) is a prominent combinatorial optimisation problem often targeted by such approaches. However, current neural-based methods for solving TSP often overlook the inherent "algorithmic" nature of the problem. In contrast, heuristics designed for TSP frequently leverage well-established algorithms, such as those for finding the minimum spanning tree. In this paper, we propose leveraging recent advancements in neural algorithmic reasoning to improve the learning of TSP problems. Specifically, we suggest pre-training our neural model on relevant algorithms before training it on TSP instances. Our results demonstrate that, using this learning
    
[^249]: 利用欧几里得距离函数解释和改进扩散模型

    Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v1 [cs.LG])

    [http://arxiv.org/abs/2306.04848](http://arxiv.org/abs/2306.04848)

    本文利用欧几里得距离函数解释去噪扩散模型，并提出了一种新的采样器。采样器表现出了最先进的FID得分，并能够生成高质量的样本。

    

    去噪直觉上与投影有关。事实上，在流形假设下，添加随机噪声近似等价于正交扰动。因此，学习去噪近似于学习投影。本文利用这一观察结果，将去噪扩散模型解释为应用于欧几里得距离函数的近似梯度下降。随后，我们基于对去噪器投影误差的简单假设，提供DDIM（Denoising Diffusion Implicit Models）采样器的简单收敛分析。最后，我们基于理论结果的洞见提出一种基于对DDIM的两个简单修改的新采样器。仅需要5-10个函数评估，我们的采样器就能在预训练的CIFAR-10和CelebA模型上达到最先进的FID得分，并且可以在潜在扩散模型上生成高质量的样本。

    Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to reinterpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection-error of the denoiser. Finally, we propose a new sampler based on two simple modifications to DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.
    
[^250]: 过度压缩如何影响GNN的能力？

    How does over-squashing affect the power of GNNs?. (arXiv:2306.03589v1 [cs.LG])

    [http://arxiv.org/abs/2306.03589](http://arxiv.org/abs/2306.03589)

    本文通过测量节点之间成对交互的水平，提供了严格的分析，以确定具有一定容量的MPNN可以学习哪些节点特征的函数类别。结果表明，为了保证节点对之间的充分通信，MPNN的容量必须是...

    

    图神经网络（GNN）是处理图结构数据的机器学习的最先进模型。最流行的GNN类别是通过相邻节点间的信息交换来操作的，称为消息传递神经网络（MPNN）。鉴于它们的广泛应用，了解MPNN的表达能力是一个关键问题。然而，现有结果通常考虑具有无信息节点特征的环境。在本文中，我们提供了一种严格的分析方法，以确定具有一定容量的MPNN可以学习哪些节点特征的函数类别。我们通过测量MPNN允许的节点之间的成对交互水平来实现此目的。该测量提供了一种新的量化特性，即所谓的过度压缩效应，该效应被观察到是当大量的信息聚合成固定大小的向量时发生的。使用我们的测量，我们证明，为了保证节点对之间的充分通信，MPNN的容量必须是...

    Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the expressive power of MPNNs is a key question. However, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be l
    
[^251]: 应对持续任务强化学习中的无界状态空间

    Tackling Unbounded State Spaces in Continuing Task Reinforcement Learning. (arXiv:2306.01896v1 [cs.LG])

    [http://arxiv.org/abs/2306.01896](http://arxiv.org/abs/2306.01896)

    本文提出了一种基于李雅普诺夫思想的奖励塑形方法，用于在持续任务的强化学习中应对无界状态空间，旨在鼓励代理器学习稳定性和最优策略。

    

    尽管深度强化学习（RL）算法已成功应用于许多任务，但它们无法外推且强烈依赖周期性重置，这限制了它们在许多现实世界设置中的适用性。针对这个问题，本文提出了一种基于李雅普诺夫思想的奖励塑形方法，以鼓励代理首先学习稳定性（即实现有界成本），然后再学习最优策略。理论上证明了奖励塑形技术减少了代理器的发散率，并通过实验进一步证实了这一点。

    While deep reinforcement learning (RL) algorithms have been successfully applied to many tasks, their inability to extrapolate and strong reliance on episodic resets inhibits their applicability to many real-world settings. For instance, in stochastic queueing problems, the state space can be unbounded and the agent may have to learn online without the system ever being reset to states the agent has seen before. In such settings, we show that deep RL agents can diverge into unseen states from which they can never recover due to the lack of resets, especially in highly stochastic environments. Towards overcoming this divergence, we introduce a Lyapunov-inspired reward shaping approach that encourages the agent to first learn to be stable (i.e. to achieve bounded cost) and then to learn to be optimal. We theoretically show that our reward shaping technique reduces the rate of divergence of the agent and empirically find that it prevents it. We further combine our reward shaping approach 
    
[^252]: 稳定性惩罚自适应跟随正则化领袖：稀疏性、游戏依赖性和最佳世界的并存

    Stability-penalty-adaptive Follow-the-regularized-leader: Sparsity, Game-dependency, and Best-of-both-worlds. (arXiv:2305.17301v1 [cs.LG])

    [http://arxiv.org/abs/2305.17301](http://arxiv.org/abs/2305.17301)

    本文开发了一种稳定性惩罚自适应（SPA）学习率，该学习率使FTRL具有稀疏性、游戏依赖性和最佳世界（BOBW）三种适应性类型，其中SPA-sparse算法可适应于未知的稀疏级别，SPA-game-dependency算法可根据所玩的游戏自适应地改变其行为，BOBW算法则是既具有稀疏性又具有游戏依赖性的适应性算法。

    

    在顺序决策问题中，适应问题的困难程度是扩展算法适用性的关键属性。跟随正则化领袖近年来成为获取淘汰法中各种类型适应性的最有前途的方法之一。为了进一步推广这种适应性，我们为FTRL开发了一个通用的自适应学习率，称为稳定性惩罚自适应（SPA）学习率。该学习率产生的遗憾界共同取决于算法的稳定性和惩罚，其中FTRL的遗憾通常被分解。凭借这个结果，我们建立了几个具有三种适应性类型的算法：稀疏性、游戏依赖性和最佳世界（BOBW）。稀疏性经常出现在真实世界的问题中，但是，现有的稀疏多臂赌博算法$k$-arms假定事先已知稀疏级别$s \leq k$，而这在真实世界的情况下通常不是情况。为了适应未知的稀疏级别，我们提出了一种新算法SPA-sparse，该算法显示比现有稀疏算法的性能提高了。游戏依赖性是另一种适应性类型，当用于生成数据的游戏发生变化时，即必需的。我们提出了一种新算法SPA-game-dependency，该算法根据所玩的游戏自适应地改变其行为，并表明它比非自适应算法的性能更好。最后，我们提出了一个既具有稀疏性又具有游戏依赖性适应性的BOBW算法，并显示它比仅集中于一种适应性类型的算法表现更好。

    Adaptivity to the difficulties of a problem is a key property in sequential decision-making problems to broaden the applicability of algorithms. Follow-the-Regularized-Leader (FTRL) has recently emerged as one of the most promising approaches for obtaining various types of adaptivity in bandit problems. Aiming to further generalize this adaptivity, we develop a generic adaptive learning rate, called Stability-Penalty-Adaptive (SPA) learning rate for FTRL. This learning rate yields a regret bound jointly depending on stability and penalty of the algorithm, into which the regret of FTRL is typically decomposed. With this result, we establish several algorithms with three types of adaptivity: sparsity, game-dependency, and Best-of-Both-Worlds (BOBW). Sparsity frequently appears in real-world problems. However, existing sparse multi-armed bandit algorithms with $k$-arms assume that the sparsity level $s \leq k$ is known in advance, which is often not the case in real-world scenarios. To ad
    
[^253]: 高维数据的差分隐私低维表示

    Differentially private low-dimensional representation of high-dimensional data. (arXiv:2305.17148v1 [cs.LG])

    [http://arxiv.org/abs/2305.17148](http://arxiv.org/abs/2305.17148)

    本文提出了一种在保护个人敏感信息的情况下，生成高效低维合成数据的算法，并在Wasserstein距离方面具有效用保证；与标准扰动分析不同，使用私有主成分分析过程避免了维度诅咒的影响。

    

    差分隐私合成数据提供了一种有效的机制，可以在保护个人敏感信息的同时进行数据分析。然而，当数据处于高维空间中时，合成数据的准确性会受到维度诅咒的影响。在本文中，我们提出了一种差分隐私算法，可以从高维数据集中高效地生成低维合成数据，并在Wasserstein距离方面具有效用保证。我们算法的一个关键步骤是使用具有近乎最优精度界限的私有主成分分析（PCA）过程，从而规避了维度诅咒的影响。与使用Davis-Kahan定理进行标准扰动分析不同，我们的私有PCA分析不需要假设样本协方差矩阵的谱间隙。

    Differentially private synthetic data provide a powerful mechanism to enable data analysis while protecting sensitive information about individuals. However, when the data lie in a high-dimensional space, the accuracy of the synthetic data suffers from the curse of dimensionality. In this paper, we propose a differentially private algorithm to generate low-dimensional synthetic data efficiently from a high-dimensional dataset with a utility guarantee with respect to the Wasserstein distance. A key step of our algorithm is a private principal component analysis (PCA) procedure with a near-optimal accuracy bound that circumvents the curse of dimensionality. Different from the standard perturbation analysis using the Davis-Kahan theorem, our analysis of private PCA works without assuming the spectral gap for the sample covariance matrix.
    
[^254]: 面向异质治疗效应估计的动态治疗信息共享

    Dynamic Inter-treatment Information Sharing for Heterogeneous Treatment Effects Estimation. (arXiv:2305.15984v1 [cs.LG])

    [http://arxiv.org/abs/2305.15984](http://arxiv.org/abs/2305.15984)

    本论文提出了一种基于深度学习的HyperCATE框架，通过软权重共享的方式实现端到端信息共享来解决现有CATE学习器中的有偏估计问题，并在IHDP、ACIC-2016和Twins基准测试中评估了该框架的表现。

    

    已有的异质治疗效应学习者缺乏端到端治疗信息共享的通用机制，必须将数据分割到潜在结果函数中训练CATE学习器，这可能导致具有有限观测数据的有偏估计。为了解决这个问题，我们提出了一种基于深度学习的框架，用于训练CATE学习器，促进治疗组之间的动态端到端信息共享。该框架基于“超网络”的“软权重共享”，具有参数效率、更快训练和改进结果等优点。所提出的框架补充了现有的CATE学习器，并引入了一类我们称之为“HyperCATE”的新型不确定性感知CATE学习器。我们开发了常用CATE学习器的HyperCATE版本，并在IHDP、ACIC-2016和Twins基准测试中进行了评估。

    Existing heterogeneous treatment effects learners, also known as conditional average treatment effects (CATE) learners, lack a general mechanism for end-to-end inter-treatment information sharing, and data have to be split among potential outcome functions to train CATE learners which can lead to biased estimates with limited observational datasets. To address this issue, we propose a novel deep learning-based framework to train CATE learners that facilitates dynamic end-to-end information sharing among treatment groups. The framework is based on \textit{soft weight sharing} of \textit{hypernetworks}, which offers advantages such as parameter efficiency, faster training, and improved results. The proposed framework complements existing CATE learners and introduces a new class of uncertainty-aware CATE learners that we refer to as \textit{HyperCATE}. We develop HyperCATE versions of commonly used CATE learners and evaluate them on IHDP, ACIC-2016, and Twins benchmarks. Our experimental 
    
[^255]: 基于代价感知的情境变量在贝叶斯优化中的学习

    Cost-aware learning of relevant contextual variables within Bayesian optimization. (arXiv:2305.14120v1 [cs.LG])

    [http://arxiv.org/abs/2305.14120](http://arxiv.org/abs/2305.14120)

    本文提出一种基于代价感知的模型选择BO方法SADCBO，通过对后验代理模型的敏感性分析来学习关于环境的相关情境信息，并通过平均模型预测来最小化优化代价，在实验中表现出卓越的性能。

    

    情境贝叶斯优化(CBO)是一种强大的框架，可针对设计变量优化黑盒昂贵的评估函数，并同时有效地整合关于环境的相关情境信息，如实验条件。然而，在许多实际场景中，情境变量的相关性不一定是预先已知的。此外，有时还可以最优化情境变量本身，这是当前CBO算法未考虑的设置。优化情境变量可能是昂贵的，这引出了确定一个最小相关子集的问题。在本文中，我们将这个问题作为一个代价感知的模型选择BO任务来构架，采用一种新方法，即基于敏感性分析的情境BO (SADCBO) 来解决这个问题。我们通过对特定输入点后验代理模型的敏感性分析来学习情境变量的相关性，同时通过平均模型预测来最小化优化的代价。SADCBO在多个合成和真实基准问题上进行了实证评估，显示出优于现有算法的性能。

    Contextual Bayesian Optimization (CBO) is a powerful framework for optimizing black-box, expensive-to-evaluate functions with respect to design variables, while simultaneously efficiently integrating relevant contextual information regarding the environment, such as experimental conditions. However, in many practical scenarios, the relevance of contextual variables is not necessarily known beforehand. Moreover, the contextual variables can sometimes be optimized themselves, a setting that current CBO algorithms do not take into account. Optimizing contextual variables may be costly, which raises the question of determining a minimal relevant subset. In this paper, we frame this problem as a cost-aware model selection BO task and address it using a novel method, Sensitivity-Analysis-Driven Contextual BO (SADCBO). We learn the relevance of context variables by sensitivity analysis of the posterior surrogate model at specific input points, whilst minimizing the cost of optimization by lev
    
[^256]: IVP-VAE: 利用初值问题求解器对电子病历时间序列进行建模

    IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers. (arXiv:2305.06741v1 [cs.LG])

    [http://arxiv.org/abs/2305.06741](http://arxiv.org/abs/2305.06741)

    本文提出了一种新的方法，在建模电子病历时间序列时，利用直接近似IVP的过程来消除递归计算，从而提高计算效率和训练速度。与目前基于IVP求解器和递归神经网络方法相比，本方法可以达到类似的分类和预测性能。

    

    连续时间模型（例如神经ODE和神经流量）在分析电子病历中常见的不规则采样时间序列方面显示出有希望的结果。 基于这些模型，时间序列通常在变分自动编码器架构中通过初值问题（IVP）求解器和递归神经网络的混合处理。 顺序求解IVP使得这样的模型在计算效率上不够高。 本文提出了一种纯粹使用连续过程对时间序列进行建模的方法，其状态演变可以通过IVP直接近似。 这消除了递归计算的需要，并允许多个状态并行演变。 我们进一步通过一种基于其可逆性的IVP求解器融合编码器和解码器，这导致参数更少，收敛更快。 在三个真实世界的数据集上进行的实验表明，所提出的方法在获得更快的训练速度的同时，仍然可以获得较高的分类性能和预测性能。

    Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver based on its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed approach achieves comparable extrapolation and classification performance while gaining more 
    
[^257]: 深层潜在位置主题模型用于带有文本边的网络聚类和表示

    The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges. (arXiv:2304.08242v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.08242](http://arxiv.org/abs/2304.08242)

    深层潜在位置主题模型用于网络聚类和表示，通过基于模型的聚类策略和概率模型对节点和边进行联合表示，并使用模型选择准则进行参数选择。

    

    数值交互导致用户共享其他人发布的文本内容，这些内容自然地由将个体与节点关联和交换的文本定义为边的网络来表示。为了理解这些异构和复杂的数据结构，将节点聚类为同类群组以及呈现可理解的数据可视化是必要的。为了解决这两个问题，我们引入了Deep-LPTM，这是一种基于模型的聚类策略，依赖于变分图自动编码器方法以及概率模型来描述讨论的主题。Deep-LPTM允许在两个嵌入空间中构建节点和边的联合表示。参数使用变分推断算法进行推断。我们还引入了IC2L，这是一种专门设计用于选择具有相关聚类和可视化属性的模型的模型选择准则。对合成数据进行了广泛的基准测试研究。特别是

    Numerical interactions leading to users sharing textual content published by others are naturally represented by a network where the individuals are associated with the nodes and the exchanged texts with the edges. To understand those heterogeneous and complex data structures, clustering nodes into homogeneous groups as well as rendering a comprehensible visualisation of the data is mandatory. To address both issues, we introduce Deep-LPTM, a model-based clustering strategy relying on a variational graph auto-encoder approach as well as a probabilistic model to characterise the topics of discussion. Deep-LPTM allows to build a joint representation of the nodes and of the edges in two embeddings spaces. The parameters are inferred using a variational inference algorithm. We also introduce IC2L, a model selection criterion specifically designed to choose models with relevant clustering and visualisation properties. An extensive benchmark study on synthetic data is provided. In particular
    
[^258]: 通过ERM实现对贴片攻击的可验证（多）鲁棒性

    Certifiable (Multi)Robustness Against Patch Attacks Using ERM. (arXiv:2303.08944v1 [cs.LG])

    [http://arxiv.org/abs/2303.08944](http://arxiv.org/abs/2303.08944)

    该文研究了针对贴片攻击的防御方法，并提出了一种使用ERM算法学习可证明具有多种蒙版下都具有鲁棒性的预测模型算法，实现了严格的证明多鲁棒性对贴片攻击。

    

    考虑贴片攻击，即在测试时对测试图像进行贴片植入，以诱导有针对性的错误分类。我们关注最近针对这种攻击的一种防御方法——Patch-Cleanser算法。该算法要求预测模型具有“两个蒙版正确性”属性，意味着预测模型在任何时候用任意两个空白蒙版替换图像部分时都应正确分类。我们提出了一种使用ERM（经验风险最小化）算法可以证明多种蒙版下都具有鲁棒性的预测模型学习算法。我们的算法基于多鲁棒性问题的凸松弛和鲁棒优化与基于边界的分类器之间的联系。我们证明了我们的算法在满足一定大小和位置约束的前提下实现了严格的证明多鲁棒性对贴片攻击。我们还通过实验证明了我们的算法对各种类型的贴片攻击都是有效的。

    Consider patch attacks, where at test-time an adversary manipulates a test image with a patch in order to induce a targeted misclassification. We consider a recent defense to patch attacks, Patch-Cleanser (Xiang et al. [2022]). The Patch-Cleanser algorithm requires a prediction model to have a ``two-mask correctness'' property, meaning that the prediction model should correctly classify any image when any two blank masks replace portions of the image. Xiang et al. learn a prediction model to be robust to two-mask operations by augmenting the training set with pairs of masks at random locations of training images and performing empirical risk minimization (ERM) on the augmented dataset.  However, in the non-realizable setting when no predictor is perfectly correct on all two-mask operations on all images, we exhibit an example where ERM fails. To overcome this challenge, we propose a different algorithm that provably learns a predictor robust to all two-mask operations using an ERM orac
    
[^259]: 在联邦深度学习中优化批标准化

    Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])

    [http://arxiv.org/abs/2303.06530](http://arxiv.org/abs/2303.06530)

    本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。

    This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.

    批标准化（BN）通常用于现代深度神经网络（DNN）中，以提高稳定性并加速集中式训练的收敛速度。在具有非IID分散数据的联邦学习（FL）中，先前的研究观察到使用BN进行训练可能会由于训练和测试之间的BN统计不匹配而阻碍性能。因此，群组归一化（GN）更常用于FL作为BN的替代方法。然而，通过我们在各种FL设置下的实证研究，我们发现BN和GN之间没有一致的优胜者。这促使我们重新审视FL中归一化层的使用。我们发现，在适当的处理下，BN可以在广泛的FL设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。我们希望我们的研究可以成为FL未来实际使用和理论分析的有价值参考。

    Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
    
[^260]: 使用主动学习方法的相关聚类

    Correlation Clustering with Active Learning of Pairwise Similarities. (arXiv:2302.10295v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10295](http://arxiv.org/abs/2302.10295)

    本文研究了相关聚类中成对相似性不事先给出的情况，并开发了一个通用的主动学习框架，适应各种相关聚类算法和查询策略，同时具有适应性灵活、噪声鲁棒性等优势。

    

    相关聚类是一个众所周知的无监督学习设置，处理正负相似性对。在本文中，我们研究了一种情况，即成对相似性不事先给出，必须以高效的方式查询。为此，我们开发了一个通用的主动学习框架，针对这个任务具有多种优势，例如，用户/注释者可以提供各种反馈类型、适应任何相关聚类算法和查询策略以及对噪声具有鲁棒性。此外，我们还提出和分析了一些适合这种设置的新的查询策略。通过几个实验研究，我们展示了我们框架和所提出的查询策略的有效性。

    Correlation clustering is a well-known unsupervised learning setting that deals with positive and negative pairwise similarities. In this paper, we study the case where the pairwise similarities are not given in advance and must be queried in a cost-efficient way. Thereby, we develop a generic active learning framework for this task that benefits from several advantages, e.g., flexibility in the type of feedback that a user/annotator can provide, adaptation to any correlation clustering algorithm and query strategy, and robustness to noise. In addition, we propose and analyze a number of novel query strategies suited to this setting. We demonstrate the effectiveness of our framework and the proposed query strategies via several experimental studies.
    
[^261]: 利用子采样实现实用的差分隐私超参数调整

    Practical Differentially Private Hyperparameter Tuning with Subsampling. (arXiv:2301.11989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11989](http://arxiv.org/abs/2301.11989)

    本文提出了一种利用子采样实现实用的差分隐私超参数调整的方法，相比基准算法，可以在不损失最终评估结果的情况下提高隐私-实用性权衡。

    

    调整差分隐私机器学习算法的超参数通常需要使用敏感数据，这可能通过超参数值泄漏私人信息。本文旨在通过仅使用敏感数据的随机子集并将最佳值外推到较大的数据集来降低这些方法的差分隐私界限和计算成本。我们提供了对该方法的 Renyi 差分隐私分析，并证明它在不损失所选超参数的最终评估准确性的情况下，可以始终实现更好的隐私-实用性权衡。

    Tuning the hyperparameters of differentially private (DP) machine learning (ML) algorithms often requires use of sensitive data and this may leak private information via hyperparameter values. Recently, Papernot and Steinke (2022) proposed a certain class of DP hyperparameter tuning algorithms, where the number of random search samples is randomized itself. Commonly, these algorithms still considerably increase the DP privacy parameter $\varepsilon$ over non-tuned DP ML model training and can be computationally heavy as evaluating each hyperparameter candidate requires a new training run. We focus on lowering both the DP bounds and the computational cost of these methods by using only a random subset of the sensitive data for the hyperparameter tuning and by extrapolating the optimal values to a larger dataset. We provide a R\'enyi differential privacy analysis for the proposed method and experimentally show that it consistently leads to better privacy-utility trade-off than the baseli
    
[^262]: 大型语言模型可被视为隐含的主题模型：解释和寻找好的示范以实现上下文学习

    Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11916](http://arxiv.org/abs/2301.11916)

    本研究发现，大型语言模型可以被视为隐式的主题模型，并提出了一种算法，从注释数据中选择最佳示范，大大提高了上下文学习的能力。

    

    近年来，预训练的大型语言模型表现出了在推理时实现少量样本学习能力的显著效率，被称为上下文学习。 然而，现有文献强调这种能力对少量样本示范的选择很敏感。本研究旨在通过贝叶斯视角研究上下文学习现象，将大型语言模型视为从示范中隐含地推断出相关信息的主题模型。在此前提下，我们提出了一种算法，用于从一组注释数据中选择最佳示范，并证明相对于随机选择基线的平均值，在八个不同的真实文本分类数据集上平均每个 GPT2 和 GPT3 模型有显着的 12.5% 的提升。我们的实证发现支持我们的假设，即大型语言模型可被视为隐含的主题模型。

    In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
    

