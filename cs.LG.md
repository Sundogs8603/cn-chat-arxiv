# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Energy-Aware Federated Traffic Prediction for Cellular Networks.](http://arxiv.org/abs/2309.10645) | 本文提出了一种能够考虑准确性和能源消耗之间权衡的可持续性指标，针对蜂窝网络中的交通预测问题提出了面向能源感知的联合交通预测技术，通过联合学习的方式实现了高精度的预测。 |
| [^2] | [Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers.](http://arxiv.org/abs/2309.10639) | 本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。 |
| [^3] | [Large language models can accurately predict searcher preferences.](http://arxiv.org/abs/2309.10621) | 大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。 |
| [^4] | [Source-free Active Domain Adaptation for Diabetic Retinopathy Grading Based on Ultra-wide-field Fundus Image.](http://arxiv.org/abs/2309.10619) | 本论文提出了一种基于超广角眼底图像的无源主动域自适应方法（SFADA）用于糖尿病视网膜病变分级，通过生成连续演变关系的特征、活动选择有价值的UWF眼底图像进行标注以及利用DR病变原型适应模型，实现了最先进的DR分级性能。 |
| [^5] | [A Dynamic Linear Bias Incorporation Scheme for Nonnegative Latent Factor Analysis.](http://arxiv.org/abs/2309.10618) | 本文提出了一种动态线性偏置引入方案，以提高非负潜在因子分析（NLFA）模型对高维不完整数据的表示学习能力。 |
| [^6] | [An Extendable Python Implementation of Robust Optimisation Monte Carlo.](http://arxiv.org/abs/2309.10612) | 这篇论文介绍了Python包ELFI中的Robust Optimisation Monte Carlo（ROMC）方法的实现。ROMC是一种新颖且高效的无似然函数推理（LFI）框架，可以提供准确的加权后验样本。实现可以作为即插即用的LFI算法使用，也可以支持可扩展性的研究。 |
| [^7] | [Asteroids co-orbital motion classification based on Machine Learning.](http://arxiv.org/abs/2309.10603) | 本研究基于机器学习，探索了如何对具有给定行星的共轨道运动的小行星进行分类，通过研究与共振相关的角度变量的时间序列，利用自定义的数据分析流程和机器学习算法实现了对小行星共轨道运动的有效分类。 |
| [^8] | [Neural Metamaterial Networks for Nonlinear Material Design.](http://arxiv.org/abs/2309.10600) | 本文提出了Neural Metamaterial Networks (NMN)方法，通过平滑神经表示编码整个超材料家族的非线性力学，实现了从结构参数到性能空间的平滑映射，适合进行梯度优化。这项研究对于非线性材料设计有着重要意义。 |
| [^9] | [Unsupervised Deep Cross-Language Entity Alignment.](http://arxiv.org/abs/2309.10598) | 本文提出了一种无监督的跨语言实体对齐方法，利用深度学习多语言编码器和机器翻译器对知识图谱文本进行编码，同时考虑了全局和局部对齐策略，生成了有排名的匹配结果，并提供了多种优化选择。 |
| [^10] | [Motif-Centric Representation Learning for Symbolic Music.](http://arxiv.org/abs/2309.10597) | 该论文提出了一种面向符号音乐的主题中心表示学习方法，通过Siamese网络架构和预训练与微调流程学习主题和其变化之间的隐含关系。实验证明，这两种方法相互补充，使精确率-召回率曲线下的面积提升了12.6%。最后，通过可视化获得的主题表示，提供了直观的理解。 |
| [^11] | [Decentralized Online Learning in Task Assignment Games for Mobile Crowdsensing.](http://arxiv.org/abs/2309.10594) | 本文研究了移动群智感知系统中的任务分配问题，提出了一种新颖的结合匹配理论和在线学习的去中心化方法。该方法通过考虑移动单元的个体目标，同时在线学习移动单元的努力，解决了感知平台和移动单元之间的目标冲突和不确定性问题。创新的"无感知"机制提高了学习过程并减少了冲突。 |
| [^12] | [Adversarial Attacks Against Uncertainty Quantification.](http://arxiv.org/abs/2309.10586) | 本研究着眼于对不确定性量化的对抗性攻击，提出了一种新的攻击场景，即攻击者通过操纵不确定度估计来削弱机器学习模型的输出效果，无论预测的正确性如何。本文设计了威胁模型并提出了多种攻击策略。 |
| [^13] | [PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring.](http://arxiv.org/abs/2309.10576) | 本文提出了一种基于多智能体强化学习的预测监控系统（PDRL），该系统可以在复杂环境中监测预测未来状态，并通过奖励策略来学习现有知识和最大化奖励。 |
| [^14] | [Task Graph offloading via Deep Reinforcement Learning in Mobile Edge Computing.](http://arxiv.org/abs/2309.10569) | 本文研究了在移动边缘计算环境下通过深度强化学习实现任务图离载的问题。现有的工作往往无法适应环境变化，导致用户体验下降。我们提出了一种将任务图调度建模为马尔可夫决策过程的方法，以适应计算能力随时间变化的情况。 |
| [^15] | [Multimodal Modeling For Spoken Language Identification.](http://arxiv.org/abs/2309.10567) | 该论文提出了一种多模态口语识别方法MuSeLI，利用视频标题、描述和地理位置等元数据来增强语言识别任务，并在两个YouTube视频数据集上获得了最新的结果。 |
| [^16] | [A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents.](http://arxiv.org/abs/2309.10563) | 本论文提出了一个名为MESc的分层神经框架，用于分类和解释大型非结构化法律文件。通过将文件分成多个部分并使用大型语言模型的嵌入和无监督聚类，该框架能够实现从长文档中预测判决并提取解释。 |
| [^17] | [Hybrid State Space-based Learning for Sequential Data Prediction with Joint Optimization.](http://arxiv.org/abs/2309.10553) | 本研究提出了一种基于混合状态空间的学习方法，通过联合优化循环神经网络和时间序列模型，实现了对序列数据的非线性预测和建模，克服了传统模型中的特征工程问题，并取得了显著的效果。 |
| [^18] | [A Neighbourhood-Aware Differential Privacy Mechanism for Static Word Embeddings.](http://arxiv.org/abs/2309.10551) | 该论文提出了一种邻域感知差分隐私机制，通过考虑预训练的词嵌入空间中单词的邻域来确定所需的最小噪声量，实验证明该机制在多个下游任务中优于其他机制，同时保证更高的隐私级别。 |
| [^19] | [Mean Absolute Directional Loss as a New Loss Function for Machine Learning Problems in Algorithmic Investment Strategies.](http://arxiv.org/abs/2309.10546) | 这项研究提出了一种新的损失函数(Mean Absolute Directional Loss，MADL)，用于解决在机器学习模型中应用于算法投资策略中的金融时间序列预测问题。通过在两种不同资产类别的数据上验证，MADL函数能够提供更好的超参数选择，并获得更有效的投资策略。 |
| [^20] | [Model Leeching: An Extraction Attack Targeting LLMs.](http://arxiv.org/abs/2309.10544) | 模型吸取是一种针对大型语言模型的提取攻击，能够将目标模型的任务特定知识提取到一个参数较少的模型中，并且具有较高的准确率和攻击成功率。 |
| [^21] | [Love or Hate? Share or Split? Privacy-Preserving Training Using Split Learning and Homomorphic Encryption.](http://arxiv.org/abs/2309.10517) | 本文提出了一种基于分割学习和同态加密的隐私保护训练方法，通过在用户激活图上应用同态加密，有效保护用户隐私，并改进了以往技术在准确性方面的表现。 |
| [^22] | [Single-Image based unsupervised joint segmentation and denoising.](http://arxiv.org/abs/2309.10511) | 本文提出一种基于单图像的无监督方法，实现了联合分割和去噪。该方法不需要大量标记样本，且能够处理高噪声和通用纹理，并在显微镜图像上表现出色。 |
| [^23] | [Learning End-to-End Channel Coding with Diffusion Models.](http://arxiv.org/abs/2309.10505) | 本文提出了使用扩散模型学习端到端信道编码的框架，并通过模拟实验证明了扩散模型能够准确学习信道分布从而实现接近最优的端到端符号误码率。 |
| [^24] | [A Configurable Library for Generating and Manipulating Maze Datasets.](http://arxiv.org/abs/2309.10498) | 这个论文介绍了一个可配置的库，用于生成和处理迷宫数据集，研究人员可以通过该库生成不同分布的迷宫数据集，并对生成参数和生成规则进行自定义控制。可以支持多种输出格式，适用于不同类型的模型。 |
| [^25] | [A comparative study of Grid and Natural sentences effects on Normal-to-Lombard conversion.](http://arxiv.org/abs/2309.10485) | 本文通过比较Grid句子和自然句子在Lombard效应和Normal-to-Lombard转换方面的表现，发现随着噪声水平的增加，Grid句子的alpha比例增加更大。在实验中，基于EMALG训练的StarGAN模型在主观可懂度评估中一致表现优于其他模型。 |
| [^26] | [Coreset selection can accelerate quantum machine learning models with provable generalization.](http://arxiv.org/abs/2309.10441) | 本论文提出了一种统一的方法，通过从原始训练数据集中选取一个合理的子集，加速量子神经网络和量子核的训练，并分析了它们在这些核心集上训练时的泛化误差界限，揭示了与在完整原始数据集上训练相比具有可比性的性能。 |
| [^27] | [Graph Neural Networks for Dynamic Modeling of Roller Bearing.](http://arxiv.org/abs/2309.10418) | 本文提出了将图神经网络应用于滚动轴承动力学建模的方法，通过将轴承的组件表示为图中的节点，GNN可以有效地模拟复杂的组件关系和相互作用。这种方法具有普适性和可解释性，在监测旋转机器的健康状态方面具有潜在的可扩展性。 |
| [^28] | [Unsupervised Learning via Network-Aware Embeddings.](http://arxiv.org/abs/2309.10408) | 本研究提出了一种通过估计数值节点属性之间的网络距离来创建网络感知嵌入的无监督学习方法，解决了传统聚类方法难以处理复杂网络结构的问题。 |
| [^29] | [Minimum width for universal approximation using ReLU networks on compact domain.](http://arxiv.org/abs/2309.10402) | 本研究通过使用ReLU-Like的激活函数，证明了在紧致域上将$L^p$函数从$[0,1]^{d_x}$逼近到$\mathbb R^{d_y}$所需的最小宽度为$\max\{d_x,d_y,2\}$，从而表明在紧致域上的逼近比在${\mathbb R^{d_x}}$上的逼近更容易。同时，利用包括ReLU在内的一般激活函数，我们还证明了一致逼近的最小宽度下界为$w_{\min}\ge d_y+1$（当$d_x<d_y\le2d_x$）。 |
| [^30] | [Differentiable Quantum Architecture Search for Quantum Reinforcement Learning.](http://arxiv.org/abs/2309.10392) | DQAS是一个基于梯度的框架，用于在NISQ时代自动设计量子电路。本研究的目标是探索DQAS在解决量子深度Q-learning问题上的能力。 |
| [^31] | [Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks.](http://arxiv.org/abs/2309.10376) | COLA是一种统一的方法，整合了图对比学习和元学习的优势，用于少样本节点分类任务。它利用图扩增来识别语义相似的节点，从而使得能够在少样本情况下进行快速泛化。 |
| [^32] | [Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization.](http://arxiv.org/abs/2309.10370) | 本文提供了浅层神经网络的几何结构解释，并通过基于${\mathcal L}^2$代价最小化的构造方法获得了一个具有优越性能的网络。 |
| [^33] | [Toward efficient resource utilization at edge nodes in federated learning.](http://arxiv.org/abs/2309.10367) | 本论文提出了一种受迁移学习启发的策略，旨在通过减少设备上的资源利用、以及减少服务器和网络的负载，提高联邦学习中边缘节点的效率。 |
| [^34] | [Testable Likelihoods for Beyond-the-Standard Model Fits.](http://arxiv.org/abs/2309.10365) | 使用归一化流构建似然函数，实现从低能量测量到高能量超出标准模型的准确信息传递，并提供额外样本生成和卡方检验统计量，以研究超出标准模型效应。 |
| [^35] | [Improving CLIP Robustness with Knowledge Distillation and Self-Training.](http://arxiv.org/abs/2309.10361) | 本文提出了一种名为LP-CLIP的方法，通过知识蒸馏和自训练来提高CLIP多模态计算机视觉模型的鲁棒性，这种方法不需要注释数据。 LP-CLIP通过在CLIP编码结构顶部添加线性探测层来蒸馏CLIP特征，并使用由CLIP生成的伪标签进行训练，具有增强模型在现实场景中应对各种挑战和不确定性的能力。 |
| [^36] | [Language Guided Adversarial Purification.](http://arxiv.org/abs/2309.10348) | 本文提出了一种语言引导的对抗净化（LGAP）框架，利用预训练的扩散模型和标题生成器来抵御对抗攻击。通过生成图像的标题并通过扩散网络进行引导，该方法可以有效地进行对抗净化，克服了现有方法的局限性。 |
| [^37] | [Explaining Agent Behavior with Large Language Models.](http://arxiv.org/abs/2309.10346) | 使用大规模语言模型解释智能体行为的方法，通过学习智能体行为的紧凑表示，并与用户进行交互，能够生成合理的解释，具备与人类专家相似的帮助性。 |
| [^38] | [Striking a Balance: An Optimal Mechanism Design for Heterogenous Differentially Private Data Acquisition for Logistic Regression.](http://arxiv.org/abs/2309.10340) | 本论文研究了在从隐私敏感卖方收集的数据上执行逻辑回归的问题，设计了一个优化测试损失、卖方隐私和支付的加权组合的最佳机制，通过结合博弈论、统计学习理论和差分隐私的思想，解决了买方的目标函数非凸的问题，并提供了当卖方数量变大时的渐近结果。 |
| [^39] | [FedWOA: A Federated Learning Model that uses the Whale Optimization Algorithm for Renewable Energy Prediction.](http://arxiv.org/abs/2309.10337) | 本论文提出了一种名为FedWOA的新型联邦学习模型，该模型利用鲸鱼优化算法从家庭能源数据训练的局部LTSM神经网络模型的权重中聚合出全局预测模型，解决了数据异质性和参数数量增加导致的预测精度降低的问题。 |
| [^40] | [Computational Approaches for App-to-App Retrieval and Design Consistency Check.](http://arxiv.org/abs/2309.10328) | 通过使用大规模网络图片训练的视觉模型，零-shot提取UI表示，并使用数学方法实现应用到应用的检索和设计一致性分析，我们提出了一种计算方法以解决现有方法的局限性。 |
| [^41] | [Investigating the Catastrophic Forgetting in Multimodal Large Language Models.](http://arxiv.org/abs/2309.10313) | 本论文针对多模态大规模语言模型中的灾难性遗忘问题进行研究，引入了EMT方法来评估灾难性遗忘，并发现在标准图像分类任务上，几乎所有评估的模型都无法保持与视觉编码器相同的性能水平。研究结果表明，早期微调阶段对性能至关重要。 |
| [^42] | [TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions.](http://arxiv.org/abs/2309.10310) | TensorCodec是一种紧凑的有损张量压缩算法，可以处理无强数据假设的一般张量。它采用神经张量列车分解、折叠输入张量和重新排序模式索引等关键点来提高压缩效果。 |
| [^43] | [Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning.](http://arxiv.org/abs/2309.10302) | 这篇论文提出了一种称为解耦训练（D-Train）的令人沮丧的、无超参数的多领域学习方法。该方法采用了一种三阶段的训练策略，首先进行预训练，然后在每个领域上进行后训练，最后进行头部微调，实现解耦训练以获得更好的性能。 |
| [^44] | [Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms.](http://arxiv.org/abs/2309.10301) | 该论文研究了领域自适应中条件不变组件的作用，提出了一种基于条件不变惩罚的新算法，该算法在目标风险保证方面具有优势。 |
| [^45] | [Using fine-tuning and min lookahead beam search to improve Whisper.](http://arxiv.org/abs/2309.10299) | 使用微调和最小先行搜索算法来改进Whisper，在低资源语言上提高了性能，并且证明了最小先行搜索优于标准束搜索算法。 |
| [^46] | [Learning Orbitally Stable Systems for Diagrammatically Teaching.](http://arxiv.org/abs/2309.10298) | 本文提出了一种学习轨道稳定系统用于图示教学的框架，通过将已知的轨道渐近稳定系统进行形变，实现机器人跟随用户指定草图进行周期运动的目标。 |
| [^47] | [Koopman Invertible Autoencoder: Leveraging Forward and Backward Dynamics for Temporal Modeling.](http://arxiv.org/abs/2309.10291) | Koopman可逆自编码器（KIA）是一种基于Koopman算子理论的机器学习模型，通过建模正向和反向动力学来提高长期预测的准确性。它能够有效地学习低维表示，并保证了正向和逆向操作的可逆性和一致性。 |
| [^48] | [Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity.](http://arxiv.org/abs/2309.10285) | Flash-LLM是一种能够低成本高效地进行大规模生成模型推断的方法，通过支持非结构化稀疏性，在高性能但具有高限制性的Tensor Cores上工作。它能够降低GPU内存消耗和计算量，并且保持良好的模型精度。 |
| [^49] | [FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning.](http://arxiv.org/abs/2309.10283) | FRAMU是一种基于注意力和联邦强化学习的机器遗忘框架，通过自适应学习机制、隐私保护技术和优化策略，处理各种数据源并保持准确性和隐私，适应波动的数据环境，支持持续模型演进。 |
| [^50] | [Crowdotic: Transformer-based Occupancy Estimation for Hospital Waiting Rooms with Non-speech Audio and Differential Privacy.](http://arxiv.org/abs/2309.10280) | 本研究提出了一种基于非语音音频的人群分析方法，利用Transformer模型实现医院候诊室的占用预测，并且在准确性方面表现出色。这是首次提出使用非语音音频信号进行占用预测的方法，超过了其他基线方法。 |
| [^51] | [Diffusion Methods for Generating Transition Paths.](http://arxiv.org/abs/2309.10276) | 本文提出了两种新方法来生成高质量的分子系统转变路径：一种是通过偏置原始动力学来促进转变，另一种是将原始转变分解为较小的转变。这些方法在数据丰富和数据稀缺的情况下都展现出了有效性。 |
| [^52] | [Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning.](http://arxiv.org/abs/2309.10275) | 该论文介绍了CRAMP，一种基于增强式课程强化学习的众包感知分散式路径规划方法，旨在解决拥挤环境下多智能体路径规划的困难。 |
| [^53] | [LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins.](http://arxiv.org/abs/2309.10254) | 本文提出了一个框架，用于分析和改进当前和未来与插件集成的LLM平台的安全性、隐私和安全性。在应用框架于OpenAI的插件生态系统时，我们发现了一些具体证明了潜在问题的插件。 |
| [^54] | [What is the Best Automated Metric for Text to Motion Generation?.](http://arxiv.org/abs/2309.10248) | 该论文研究了文本到动作生成任务中与人类评估最吻合的度量，并提出了新的更优度量。结果发现当前用于该任务的度量与人类判断相关性较低，但用于评估平均模型性能的常用度量显示出较强的相关性。 |
| [^55] | [On Explicit Curvature Regularization in Deep Generative Models.](http://arxiv.org/abs/2309.10237) | 该论文提出了一种基于曲率的正则化方法，用于深度生成模型的学习。在任意数据流形的情况下，推导出了明确的坐标不变公式来计算内在和外在曲率。实验证明，基于曲率的正则化方法优于现有的自动编码器方法，其中内在曲率度量略优于外在曲率度量。 |
| [^56] | [Multi-fidelity climate model parameterization for better generalization and extrapolation.](http://arxiv.org/abs/2309.10231) | 多层次气候模式参数化方法整合了不同准确性和丰富度的数据集，解决了机器学习参数化方法泛化和外推能力不足的问题，为气候变化的预测提供更准确的结果。 |
| [^57] | [Causal Theories and Structural Data Representations for Improving Out-of-Distribution Classification.](http://arxiv.org/abs/2309.10211) | 使用人生成的因果知识来改进数据表示可以提高神经网络在复杂分类任务中的性能，指示了改进机器学习系统开发实践的重要性。 |
| [^58] | [The Kernel Density Integral Transformation.](http://arxiv.org/abs/2309.10194) | 本文提出了一种使用核密度积分转换作为特征预处理步骤的方法，可以替代线性最小最大缩放和分位数转换，并通过调整超参数优化性能。这种方法在统计数据分析中具有应用价值。 |
| [^59] | [Stochastic Deep Koopman Model for Quality Propagation Analysis in Multistage Manufacturing Systems.](http://arxiv.org/abs/2309.10193) | 本研究引入了一种随机深度库普曼（SDK）框架来建模多阶段制造系统（MMS）的复杂行为，并通过传播关键质量信息，提高了数据驱动模型的可解释性和准确性。 |
| [^60] | [Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence.](http://arxiv.org/abs/2309.10186) | 本研究提出了一种使用图神经网络和强化学习监控的新方法，以更准确地预测时间序列数据。该方法能够克服深度学习的限制，并捕捉复杂的时序结构中的时间依赖关系，适用于医疗、交通和天气预测等领域。 |
| [^61] | [QoS-Aware Service Prediction and Orchestration in Cloud-Network Integrated Beyond 5G.](http://arxiv.org/abs/2309.10185) | 本论文提出了一个针对网络-云集成环境中的服务部署和资源分配的非线性规划模型，考虑了容量限制、动态用户和端到端延迟，并解决了超越5G中的服务连续性问题。 |
| [^62] | [Double Deep Q-Learning-based Path Selection and Service Placement for Latency-Sensitive Beyond 5G Applications.](http://arxiv.org/abs/2309.10180) | 通过研究通信和计算资源分配的联合问题，本文提出了两种方法，即B&B-CCRA和WF-CCRA，以最小化总成本。 |
| [^63] | [Self-Sustaining Multiple Access with Continual Deep Reinforcement Learning for Dynamic Metaverse Applications.](http://arxiv.org/abs/2309.10177) | 本文研究了动态元宇宙应用中的多重接入问题，在采用自适应人工智能和深度强化学习的基础上，通过连续训练来实现自我维持的策略。该研究填补了当前文献中对于适应非稳态环境的代理机制问题的研究空白。 |
| [^64] | [One ACT Play: Single Demonstration Behavior Cloning with Action Chunking Transformers.](http://arxiv.org/abs/2309.10175) | 本研究提出了一种通过行为克隆学习任务的方法，只需要一次人类演示，并使用线性变换生成多样化的轨迹，成功完成了三个块操作任务。此外，还提出了一种新的暂存方法，用于动作块代理的行动预测。 |
| [^65] | [Autoencoder-based Anomaly Detection System for Online Data Quality Monitoring of the CMS Electromagnetic Calorimeter.](http://arxiv.org/abs/2309.10157) | 本文介绍了一种基于自编码器的实时异常检测系统，通过半监督机器学习方法检测CMS电磁量能器数据中的异常，利用异常的时域演化和探测器响应的空间变化，最大化异常检测性能，并通过验证实验证明了系统的有效性。 |
| [^66] | [Preserving Tumor Volumes for Unsupervised Medical Image Registration.](http://arxiv.org/abs/2309.10153) | 该论文提出了一种保持肿瘤体积的无监督医学图像配准方法，通过两个阶段的过程来解决肿瘤区域体积变化失衡的问题。 |
| [^67] | [Primal-Dual $\ell_0$-Constrained Sparse Index Tracking.](http://arxiv.org/abs/2309.10152) | 本文提出了一种新的稀疏指数跟踪问题的形式化，使用了$\ell_0$-范数约束，可以轻松控制投资组合中资产数量的上限。 |
| [^68] | [Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions.](http://arxiv.org/abs/2309.10150) | Q-Transformer是一种可扩展的离线强化学习方法，通过使用Transformer来表示Q函数并利用离线数据集进行训练。它在大规模真实世界机器人操作任务中表现优越。 |
| [^69] | [Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?.](http://arxiv.org/abs/2309.10149) | 本文提出了一种新的连续学习框架，旨在在动态环境下实现鲁棒的泛化能力并保留过去的知识。通过使用有限容量的内存来保存先前观察到的环境信息，并从内存中采样数据点来获得对未知变化鲁棒的预测器。该分析展示了记忆和泛化之间的权衡，而实验证明了所提出的算法的优越性。 |
| [^70] | [Realistic Website Fingerprinting By Augmenting Network Trace.](http://arxiv.org/abs/2309.10147) | 通过增加网络跟踪，我们提出了一种适用于Tor跟踪的增强技术NetAugment，可以增强网站指纹识别器在未观察到的网络条件下的性能。 |
| [^71] | [A Geometric Framework for Neural Feature Learning.](http://arxiv.org/abs/2309.10140) | 本论文提出了一个基于神经特征学习的几何框架，在特征空间中利用几何结构解决学习问题。通过引入特征几何，将统计依赖和特征统一到同一空间中，并使用嵌套技术设计学习算法，展示了其在多变量学习问题中的应用。 |
| [^72] | [Efficient Low-Rank GNN Defense Against Structural Attacks.](http://arxiv.org/abs/2309.10136) | 提出了一种高效的低秩图神经网络（ELR-GNN）防御方法，通过学习低秩和稀疏的图结构抵御对抗攻击，并在更高的效率下实现有效的防御。 |
| [^73] | [GDM: Dual Mixup for Graph Classification with Limited Supervision.](http://arxiv.org/abs/2309.10134) | GDM是一种双重的mixup方法，利用图实例的功能和结构信息生成新的标记图样本。 |
| [^74] | [Deep Prompt Tuning for Graph Transformers.](http://arxiv.org/abs/2309.10131) | 提出了一种称为深度图形指导调优的替代fine-tuning的方法，通过引入可训练的特征节点和任务特定的令牌，来增强图形变换器模型在下游图形预测任务中的表达能力，同时减少了自由参数的数量和模型副本的需求，适用于小型数据集。 |
| [^75] | [Deep smoothness WENO scheme for two-dimensional hyperbolic conservation laws: A deep learning approach for learning smoothness indicators.](http://arxiv.org/abs/2309.10117) | 这篇论文提出了一种改进的二维双曲型守恒定律问题的深度平滑WENO格式，通过深度学习方法调整平滑性指标，在数值结果的准确性方面取得了优势，尤其是在陡峭激波附近。 |
| [^76] | [AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation.](http://arxiv.org/abs/2309.10109) | AR-TTA提出了一种简单的方法用于真实世界连续测试时间自适应。通过将内存缓冲区纳入自训练框架，并根据数据流的强度进行动态适应，提高了模型的稳定性。 |
| [^77] | [Understanding Catastrophic Forgetting in Language Models via Implicit Inference.](http://arxiv.org/abs/2309.10105) | 本研究通过在语言模型上进行实验，发现微调对模型在微调数据分布任务上的表现有正面影响，但会抑制模型在其他任务上的能力，特别是与微调分布最接近的任务。作者假设语言模型会隐式推理任务，并且微调过程偏向于微调数据分布中的任务。作者进一步提出了共轭提示方法，以尝试恢复模型在预训练阶段的能力。 |
| [^78] | [A Semi-Supervised Approach for Power System Event Identification.](http://arxiv.org/abs/2309.10095) | 提出了一种新颖的半监督框架，通过加入无标签的事件样本来评估提升现有事件识别方法的有效性。 |
| [^79] | [Unified Coarse-to-Fine Alignment for Video-Text Retrieval.](http://arxiv.org/abs/2309.10091) | 提出了一种统一粗到细对齐模型UCoFiA，用于视频-文本检索，该模型能够在不同粒度级别上捕捉跨模态相似性信息，并通过交互式相似性聚合模块有效考虑不同视觉特征的重要性，最终解决了视频-文本检索中的精确匹配问题。 |
| [^80] | [HTEC: Human Transcription Error Correction.](http://arxiv.org/abs/2309.10089) | HTEC是一种用于人类转录错误修正的方法，包括错误检测和填充两个阶段，提出了一种综合的修正操作列表，并针对删除错误提出了四种新操作。 |
| [^81] | [Invariant Probabilistic Prediction.](http://arxiv.org/abs/2309.10083) | 这篇论文研究了在分布变化下具有不变性和稳健性的概率预测方法。研究发现在适当评分规则下，任意分布偏移一般不具有不变和稳健的概率预测，通过限制分布偏移类别和选择评估指标，可以在特定模型中实现不变性和稳健性。 |
| [^82] | [GAME: Generalized deep learning model towards multimodal data integration for early screening of adolescent mental disorders.](http://arxiv.org/abs/2309.10077) | 本研究设计了一个广义深度学习模型GAME，通过整合多模态特征并采用新颖的关注机制，能够对青少年的心理状况进行高准确度的评估。研究发现每种模态都对心理障碍的筛查和共病状况有动态的贡献。 |
| [^83] | [A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes.](http://arxiv.org/abs/2309.10068) | 本论文提出了一个统一的视角来探讨非平稳核在深层高斯过程中的应用，以提高预测性能和不确定性估计。 |
| [^84] | [Bayesian longitudinal tensor response regression for modeling neuroplasticity.](http://arxiv.org/abs/2309.10065) | 提出了一种基于贝叶斯纵向张量响应回归的新方法，利用汇集空间分布的体素信息来推断显著的变化，并对协变量进行调整。该方法使用马尔科夫链蒙特卡洛采样，利用低秩分解减少维度并保持维度的空间配置，通过满足后验分布形状的联合可信区域实现特征选择，从而实现更准确的推断。 |
| [^85] | [Dual Student Networks for Data-Free Model Stealing.](http://arxiv.org/abs/2309.10058) | 该论文提出了一种无数据模型窃取的方法，通过训练两个对称学生来引导生成器生成样本，使得这两个学生对样本的分类意见不一致，从而在生成器中激励探索更多样化的输入空间，并利用学生模型的梯度间接估计目标模型的梯度。 |
| [^86] | [A Modular Spatial Clustering Algorithm with Noise Specification.](http://arxiv.org/abs/2309.10047) | 本文提出了一种模块化空间聚类算法"Bacteria-Farm"，基于实验室封闭饲养场中细菌生长的启发，通过平衡性能和寻找最优参数的难度来解决聚类算法中参数估计困难的问题。算法具有模块化设计，可以针对特定任务和数据分布创建不同的版本，并提供了指定噪声规范的功能。 |
| [^87] | [Parameter-Efficient Long-Tailed Recognition.](http://arxiv.org/abs/2309.10019) | 本文提出了一种名为PEL的参数高效微调方法，可以在不到20个训练轮数内有效地将预训练模型适应于长尾识别任务，而无需额外的数据。该方法通过引入少量的任务特定参数，解决了常用微调方法导致尾部类别性能下降的问题。 |
| [^88] | [Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction.](http://arxiv.org/abs/2309.10016) | 本研究评估了GPT-3在抗癌药物敏感性预测任务中的潜力，并发现药物的SMILES表示和细胞系的基因组突变特征对药物反应具有预测能力。这些结果有助于在精准肿瘤学中设计更有效的治疗方案。 |
| [^89] | [SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback.](http://arxiv.org/abs/2309.10015) | SYNDICOM是一种改进对话常识的方法，包含了一个常识对话数据集和一个基于自然语言反馈的模型，可用于训练对话应答生成模型。 |
| [^90] | [Prognosis of Multivariate Battery State of Performance and Health via Transformers.](http://arxiv.org/abs/2309.10014) | 本研究通过Transformer模型迈出了预测多变量电池性能和健康状态的第一步，为设计更好的电池和减少实验工作量提供了前所未有的见解。 |
| [^91] | [Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin.](http://arxiv.org/abs/2309.10013) | 这项研究探讨了超几何和欧几里得嵌入在少样本学习中的比较。研究表明，超几何嵌入在高维度中呈现出边界收敛的趋势，并在少样本分类任务中取得了最佳结果。与以往研究不同，研究者还发现，配备固定半径的欧几里得编码器也可以实现更好的性能。 |
| [^92] | [Looking through the past: better knowledge retention for generative replay in continual learning.](http://arxiv.org/abs/2309.10012) | 本文改进了不断学习环境中的生成回放方法，以在复杂场景下表现更好。通过蒸馏潜在空间、改善生成特征对齐和周期性生成以增强知识保留。 |
| [^93] | [Machine Learning Approaches to Predict and Detect Early-Onset of Digital Dermatitis in Dairy Cows using Sensor Data.](http://arxiv.org/abs/2309.10010) | 本研究提出了一种利用传感器数据的机器学习模型，可以在奶牛中早期预测和检测数字皮炎。其准确率分别为79%和64%，有助于开发实时自动化工具进行监测和诊断。 |
| [^94] | [DeepHEN: quantitative prediction essential lncRNA genes and rethinking essentialities of lncRNA genes.](http://arxiv.org/abs/2309.10008) | DeepHEN是一种能够预测长链非编码RNA基因必需性的模型，并能确定序列特征和网络空间特征对必需性的影响的方法。 |
| [^95] | [Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem.](http://arxiv.org/abs/2309.10007) | 本研究提出了一个模块化和并行化的多智能体深度强化学习框架，在AutoDRIVE生态系统中培养合作与竞争行为。我们利用该生态系统开发了准确物理和逼真图形的数字孪生体，并使用它来训练和部署多智能体强化学习策略，实现了在自主车辆中的合作和竞争行为。 |
| [^96] | [A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models.](http://arxiv.org/abs/2309.10003) | 本文提出了一种使用概率从语言模型中获得的自信息来测量专利权要求范围的新方法。该方法通过计算要求的发生概率和自信息来评估要求的信息量，进而反映出要求的范围。研究结果表明，不同类型的语言模型对范围测量的影响不同，最简单的模型可以将范围度量简化为单词或字符计数的倒数。此方法在九个系列的专利权要求上进行了验证，结果表明各系列的要求范围逐渐减小。 |
| [^97] | [Energy stable neural network for gradient flow equations.](http://arxiv.org/abs/2309.10002) | 本文提出了能量稳定网络(EStable-Net)用于解决梯度流方程，该网络能够降低离散能量并生成高准确性和稳定性的预测。 |
| [^98] | [Detecting covariate drift in text data using document embeddings and dimensionality reduction.](http://arxiv.org/abs/2309.10000) | 本研究通过比较不同的文档嵌入、降维技术和漂移检测方法，发现在检测文本数据中的协变漂移方面，特定的嵌入方法、降维技术和漂移检测方法的组合效果优于其他方法。 |
| [^99] | [Improving Speech Recognition for African American English With Audio Classification.](http://arxiv.org/abs/2309.09996) | 通过使用少量非洲裔美国英语的数据，结合音频分类器和地理信息，我们提出了一种改进美国英语语音识别的方法，相对词错误率减少了38.5%。 |
| [^100] | [Long-term Neurological Sequelae in Post-COVID-19 Patients: A Machine Learning Approach to Predict Outcomes.](http://arxiv.org/abs/2309.09993) | 该研究使用机器学习方法预测了新冠后长期神经并发症的结果，发现68%的患者报告有神经症状，其中疲劳、头痛和嗅觉丧失最常见。随机森林模型在识别患者发展神经病风险方面取得了有希望的准确性、敏感性和特异性。 |
| [^101] | [TCGF: A unified tensorized consensus graph framework for multi-view representation learning.](http://arxiv.org/abs/2309.09987) | 提出了一种名为TCGF的统一多视图表示学习框架，用于有效地融合多个视图中的关键信息。 |
| [^102] | [Exploration and Comparison of Deep Learning Architectures to Predict Brain Response to Realistic Pictures.](http://arxiv.org/abs/2309.09983) | 本研究探索和比较了多种深度学习架构，用于预测大脑对真实图片的反应。最终发现，使用多个简单模型，每个模型专注于受试者大脑每个半球的每个ROI，可以获得更好的预测结果。 |
| [^103] | [Introspective Deep Metric Learning.](http://arxiv.org/abs/2309.09982) | 本文提出了一种内省式深度度量学习 (IDML) 框架，通过考虑图像中的不确定性，以更好地处理模糊图像，实现更鲁棒的训练。 |
| [^104] | [Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification.](http://arxiv.org/abs/2309.09976) | Des-q是一种量子算法，用于在回归和二分类任务中构建和重新训练决策树。它显著减少了树重新训练所需的时间复杂度，并且能够处理新样本的加载时间。该算法通过 k 分段线性树分裂来构建决策树，将数据划分为不同的子空间。 |
| [^105] | [Graph topological property recovery with heat and wave dynamics-based features on graphsD.](http://arxiv.org/abs/2309.09924) | 本文提出了一种名为图微分方程网络（GDeNet）的方法，利用热和波动方程动力学特征来恢复图的拓扑属性，能够在各种下游任务中获得优秀的表现，同时在实际应用中也展现了较好的性能。 |
| [^106] | [Harnessing Collective Intelligence Under a Lack of Cultural Consensus.](http://arxiv.org/abs/2309.09787) | 在缺乏文化共识的情况下，通过无限深度潜在结构的文化共识理论（iDLC-CCT）模型，扩展了文化共识理论（CCT）的能力，提高了对共识信念多样性的建模能力。 |
| [^107] | [FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks.](http://arxiv.org/abs/2309.09517) | FedGKD是一种新颖的联邦图神经网络框架，通过利用客户端图数据集蒸馏方法提取更好的任务特征并引入感知全局协作结构的服务器端聚合机制，解决了联邦GNN系统中图异构性问题，提高了效率和准确性。 |
| [^108] | [Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles.](http://arxiv.org/abs/2309.09457) | 本文研究了在稀疏线性MDP中探索和学习的问题，通过特征选择提出了一个多项式时间算法，以在与环境的交互中学习出近似最优策略。 |
| [^109] | [CaT: Balanced Continual Graph Learning with Graph Condensation.](http://arxiv.org/abs/2309.09455) | 本文提出了一种名为CaT的方法，通过引入图压缩技术，解决持续图学习中存储预算紧张和数据不平衡的问题。 |
| [^110] | [Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits.](http://arxiv.org/abs/2309.08748) | 通过使用Wasserstein距离而不是KL散度，我们提出了一种新颖的分布保证优化方法，用于解决上下文乐队中实际环境不匹配和最坏情况下过度拟合的问题。 |
| [^111] | [Sparse Autoencoders Find Highly Interpretable Features in Language Models.](http://arxiv.org/abs/2309.08600) | 本研究通过稀疏自编码器在语言模型中发现了一组高度可解释和单一义的特征，从而解决了神经网络内部多义性的问题。 |
| [^112] | [A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification.](http://arxiv.org/abs/2309.08415) | 本研究提出了一种使用机器学习和不确定性量化建模的多阶段决策过程方法，用于预测心力衰竭患者对心脏再同步治疗的反应。该模型能够推荐收集额外的SPECT MPI变量，以提高预测准确性。 |
| [^113] | [Traveling Words: A Geometric Interpretation of Transformers.](http://arxiv.org/abs/2309.07315) | 本文提出了一种几何视角来解释变压器的内部机制，主要贡献在于阐明了层归一化如何限制潜在特征并在超球面上塑造注意力机制，通过探测预训练的GPT-2模型验证了该视角的有效性，并提供了对变压器的直观理解。 |
| [^114] | [Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach.](http://arxiv.org/abs/2309.07265) | 本文提出了一种混合迁移学习方法，用于解决在O-RAN网络中使用DRL算法进行闭环控制时遇到的收敛速度慢和性能不稳定的问题。 |
| [^115] | [Long-term drought prediction using deep neural networks based on geospatial weather data.](http://arxiv.org/abs/2309.06212) | 基于地理气象数据的深度神经网络用于长期干旱预测，提出了一种端到端解决方案来预测特定地区干旱概率。采用卷积LSTM和Transformer模型相比其他基线模型能够获得更高的准确性。 |
| [^116] | [Quantitative Analysis of Forecasting Models:In the Aspect of Online Political Bias.](http://arxiv.org/abs/2309.05589) | 本文通过定量分析在线社交媒体上的政治偏见预测模型，提出了一种启发式方法来分类政治倾向，并进行了对现有基准模型的深入分析，以确定最适合预测政治倾向的模型。 |
| [^117] | [Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis.](http://arxiv.org/abs/2309.05525) | 该研究提出了一种在6G环境中支持联邦学习的可信架构，利用分布式分类账技术和图神经网络解决了隐私和安全问题，以实现安全聚合和异常检测。 |
| [^118] | [Learning from Demonstration via Probabilistic Diagrammatic Teaching.](http://arxiv.org/abs/2309.03835) | 本文介绍了一种名为图示教学的示教学习的替代范式，通过要求用户在场景的二维图像上勾勒示范轨迹来教机器人新的技能，并将其合成为三维任务空间中的运动轨迹的生成模型。 |
| [^119] | [Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network.](http://arxiv.org/abs/2309.03694) | 这项研究提出了一种使用粒子群优化算法、多头注意力机制和计算效率优化框架相结合的方法，用于解决短期负荷预测中的超参数敏感性、解释性不透明和高计算开销等问题。结果表明，该方法在准确性、鲁棒性和计算效率方面具有优势。 |
| [^120] | [Characterizing Lipschitz Stability of GNN for Fairness.](http://arxiv.org/abs/2309.03648) | 论文通过研究GNN的Lipschitz界限表征了GNN对公平性稳定性的影响，尤其是在处理非欧几里得数据和固有偏倚的情况下。同时，该研究对于限制GNN输出的扰动以保障公平性训练提出了挑战。 |
| [^121] | [On the dynamics of multi agent nonlinear filtering and learning.](http://arxiv.org/abs/2309.03557) | 本文研究了具有非线性滤波/学习动力学的多智能体网络系统的行为，并介绍了在分布式和联邦学习场景中的应用。 |
| [^122] | [A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications.](http://arxiv.org/abs/2308.16375) | 这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。 |
| [^123] | [Evaluation and Analysis of Hallucination in Large Vision-Language Models.](http://arxiv.org/abs/2308.15126) | 本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。 |
| [^124] | [BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition.](http://arxiv.org/abs/2308.14906) | BayOTIDE是一种基于贝叶斯方法的在线多元时间序列插补与函数分解模型，通过将多元时间序列视为低秩时序因子组的加权组合来进行插补，同时解决了全局趋势和周期性模式的忽略以及不规则采样时间序列的处理问题。 |
| [^125] | [A multiobjective continuation method to compute the regularization path of deep neural networks.](http://arxiv.org/abs/2308.12044) | 本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。 |
| [^126] | [Attention Is Not All You Need Anymore.](http://arxiv.org/abs/2308.07661) | 本文提出了一种名为Extractor的插入替代器，用于取代Transformer中的自注意机制，实验证明使用Extractor可以提高Transformer的性能，并且具有更短的计算关键路径。 |
| [^127] | [Synthetic data generation method for hybrid image-tabular data using two generative adversarial networks.](http://arxiv.org/abs/2308.07573) | 本论文提出了一种使用两个生成对抗网络（GANs）生成合成医学记录的方法，可以生成包含胸部X射线图像和结构化表格数据的多样化合成记录，并保持了图像和数据之间的对应关系。 |
| [^128] | [VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs.](http://arxiv.org/abs/2308.02117) | VQGraph是一个框架，通过学习一个强大的图形表示空间，用于连接GNN和MLPs。它采用矢量量化变分自编码器（VQ-VAE）的编码器作为结构感知图标记器，有效地表示底层图的多样化局部结构。通过 VQGraph，可以实现从GNN到MLP的知识转移。 |
| [^129] | [Extended Graph Assessment Metrics for Graph Neural Networks.](http://arxiv.org/abs/2307.10112) | 本论文提出了扩展的图评估指标（GAMs），适用于回归任务和连续邻接矩阵。主要关注的两个GAMs是同质性和跨类邻域相似度（CCNS）。这些扩展的指标能够在图神经网络中评估图结构，提高模型性能。 |
| [^130] | [Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code.](http://arxiv.org/abs/2307.07686) | 本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。 |
| [^131] | [Weakly-supervised positional contrastive learning: application to cirrhosis classification.](http://arxiv.org/abs/2307.04617) | 本研究提出了一种弱监督定位对比学习策略，在医学应用中使用大量弱标签图像进行肝硬化预测。这种策略将每个2D切片的空间上下文和弱标签整合起来，通过使用通用基于核的损失函数实现高效学习。 |
| [^132] | [Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey.](http://arxiv.org/abs/2307.04370) | 本文调研了深度学习在端到端自动驾驶中的最新进展，并提供了一种基于神经网络的自动驾驶任务分类体系。研究分析了端到端自动驾驶的关键挑战，并列举了不同的研究方法和核心功能。 |
| [^133] | [Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression Values.](http://arxiv.org/abs/2306.06276) | 该论文提出应用对比学习方法，从有限的数据样本中学习基因表达数据的良好特征表示，并应用于Cox模型中，可以显著提高癌症预后的预测性能。 |
| [^134] | [ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks.](http://arxiv.org/abs/2306.06196) | 本研究提出了一种基于神经网络的小而高效模型，用于确定两个心电图(ECG)是否来自同一患者。同时，还提出了一种方法，利用该模型检测记录-分配错误，实现了在现实场景中的应用，并在新的ECG数据集上进行了评估。 |
| [^135] | [ChatGPT Informed Graph Neural Network for Stock Movement Prediction.](http://arxiv.org/abs/2306.03763) | 该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。 |
| [^136] | [Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion.](http://arxiv.org/abs/2305.15639) | 本文全面分析了基于图p-Laplacian的Framelet网络，证明了其具有良好的收敛性和能量动态，并能够通过广义非线性扩散过程进行多种训练。同时，该模型能够适应同质和异质数据，避免过度平滑问题。 |
| [^137] | [Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models.](http://arxiv.org/abs/2305.06704) | 该论文提出了一种基于聚类的鲁棒检测滞后多因子模型中的领先滞后关系方法，并使用各种聚类技术和相似度度量方法实现了对领先滞后估计的聚合，从而强化了对原始宇宙中的一致关系的识别。 |
| [^138] | [ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs.](http://arxiv.org/abs/2305.03513) | ChatGraph通过将ChatGPT的知识转换为图形，提高了文本分类的可解释性和性能 |
| [^139] | [Node Feature Augmentation Vitaminizes Network Alignment.](http://arxiv.org/abs/2304.12751) | 本研究提出了Grad-Align+方法，通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法，解决了NA方法缺乏额外信息的问题。 |
| [^140] | [Community Detection Using Revised Medoid-Shift Based on KNN.](http://arxiv.org/abs/2304.09512) | 在本文中，作者提出了一种名为修正Medoid-Shift（RMS）的新聚类算法，用于社区检测，该算法可以更好地解决社交网络中的问题。 |
| [^141] | [Conformal Off-Policy Evaluation in Markov Decision Processes.](http://arxiv.org/abs/2304.02574) | 本论文提出了一种基于合规预测的异策评估方法，能够以一定的确定性水平输出包含目标策略的真实奖励的区间，并提出了不同的处理分布偏移方法，其中一些方法在保证合规性的前提下实现了最先进的性能。 |
| [^142] | [Online Reinforcement Learning in Markov Decision Process Using Linear Programming.](http://arxiv.org/abs/2304.00155) | 本论文提出了一种在未知转移矩阵和固定但未知分布的情况下进行在线MDP学习的简单而高效的方法，可以实现更紧的遗憾界，并通过置信区间框架改进了现有算法。 |
| [^143] | [Neural networks trained on synthetically generated crystals can extract structural information from ICSD powder X-ray diffractograms.](http://arxiv.org/abs/2303.11699) | 本研究提出了一种使用合成晶体数据集，通过神经网络从ICSD粉末X射线衍射图中提取结构信息的方法，并且在空间群分类任务上，取得了比直接在ICSD晶体上进行训练更高的准确度。 |
| [^144] | [A unified scalable framework for causal sweeping strategies for Physics-Informed Neural Networks (PINNs) and their temporal decompositions.](http://arxiv.org/abs/2302.14227) | 这篇论文提出了一个统一可扩展框架，用于解决物理信息神经网络（PINNs）在优化过程中出现的挑战，并介绍了一种新的堆叠分解方法来加快计算速度。 |
| [^145] | [Elliptic PDE learning is provably data-efficient.](http://arxiv.org/abs/2302.12888) | 该论文提供了椭圆型偏微分方程学习中的数据效率理论保证，通过利用随机数值线性代数和PDE理论，实现了对于3D均匀椭圆型PDE解算符的数据高效恢复，并在训练数据集大小上以指数收敛率的误差。 |
| [^146] | [Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales.](http://arxiv.org/abs/2302.08720) | 本论文将新兴的图像超分辨率技术应用于统计降尺度任务，具体探索了基于生成对抗网络的算法在模拟北美地区地表风中的应用。通过使用非理想化的低分辨率和高分辨率输入数据，该方法克服了共享尺度不匹配的问题，并通过评估空间功率谱等指标来评估模型的技能。 |
| [^147] | [Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics.](http://arxiv.org/abs/2302.00600) | 本文通过训练扩散生成模型来学习粗粒化分子动力学的力场，无需使用任何力场输入，从而实现了对生物过程的精确模拟，并展示了在多个蛋白质模拟中具有优越性能。 |
| [^148] | [RouteNet-Fermi: Network Modeling with Graph Neural Networks.](http://arxiv.org/abs/2212.12070) | RouteNet-Fermi是一种基于图神经网络的自定义模型，与传统的排队理论相比，在存在真实流量模型的情况下具有更高的准确性。它能够准确预测网络的延迟、抖动和丢包情况。 |
| [^149] | [SoftCTC -- Semi-Supervised Learning for Text Recognition using Soft Pseudo-Labels.](http://arxiv.org/abs/2212.02135) | 本文研究了半监督学习在文本识别中的应用，提出了一种新的损失函数 SoftCTC，可以同时考虑多个转录变体，避免了基于置信度的过滤步骤，实验表明其在手写识别任务上与过滤流水线性能相当，并且计算效率更高。 |
| [^150] | [Unsupervised Unlearning of Concept Drift with Autoencoders.](http://arxiv.org/abs/2211.12989) | 本论文提出了一种基于自编码器的无监督的概念漂移取消学习方法，通过在全局级别引入自编码器，可以避免重新训练或调整学习模型，从而实现对概念漂移的适应。 |
| [^151] | [ZigZag: Universal Sampling-free Uncertainty Estimation Through Two-Step Inference.](http://arxiv.org/abs/2211.11435) | 本研究提出了一种通用的无采样不确定性估计方法，通过训练网络在有和没有额外信息的情况下产生相同的输出，实现了与最先进方法相媲美的可靠性估计，同时显著降低了计算成本。 |
| [^152] | [RCD-SGD: Resource-Constrained Distributed SGD in Heterogeneous Environment via Submodular Partitioning.](http://arxiv.org/abs/2211.00839) | RCD-SGD是一个在资源受限的异构环境中的分布式SGD算法，通过子模块划分实现了类别级特征分布的相似性和类别平衡。 |
| [^153] | [Differentially Private Optimization on Large Model at Small Cost.](http://arxiv.org/abs/2210.00038) | 本文提出了一种名为簿记（BK）的技术，实现了差分隐私优化器在大模型和高维数据上的快速训练，并在计算成本上取得了实质性的改进。 |
| [^154] | [Near-Optimal $\Phi$-Regret Learning in Extensive-Form Games.](http://arxiv.org/abs/2208.09747) | 本文介绍了一种有效和解耦的学习动力学，在多人博弈中能够使每个玩家的触发后悔按$O(\log T)$增长，达到接近最优的收敛速度。并且构建利用了一个更为一般的由具有多项式次数的有理函数导出的不动点结果，以及一个凸包的更细致的后悔电路，保留了RVU合适性的属性。 |
| [^155] | [Collaborative causal inference on distributed data.](http://arxiv.org/abs/2208.07898) | 提出了一种数据协作准实验（DC-QE）方法，可以在保护隐私的前提下对分布式数据进行因果推断。通过共享中间表示而不是私有数据，估计倾向分数和处理效应，能够减少随机误差和偏差，相比现有方法有更好的估计结果。 |
| [^156] | [A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs.](http://arxiv.org/abs/2206.04798) | 本论文提出了A*Net，一种基于路径的知识图谱推理方法，通过学习优先级函数，实现了对重要节点和边的选择，从而提高了训练和推理的效率。实验证明A*Net在性能和效率之间取得了平衡，在知识图谱推理中具有竞争力，并在大规模数据集上获得了新的最先进结果。 |
| [^157] | [Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning.](http://arxiv.org/abs/2205.14704) | 本论文提出了一种检索增强的提示学习方法，通过将知识从记忆中解耦，帮助模型在泛化和记忆之间取得平衡。 |
| [^158] | [Explainable Deep Learning Methods in Medical Image Classification: A Survey.](http://arxiv.org/abs/2205.04766) | 这项调查提供了关于可解释深度学习方法在医学图像分类中的应用的全面概述，包括各种解释方法的比较和性能评估。 |
| [^159] | [Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning.](http://arxiv.org/abs/2205.02355) | 提出了一种新的半参数学习范式，即检索增强的提示调优，用于关系抽取。通过构建开放式存储库，并使用线性插值的方式，模型能够在推断过程中根据存储库中的记忆信息推断关系。 |
| [^160] | [Learning High-Dimensional McKean-Vlasov Forward-Backward Stochastic Differential Equations with General Distribution Dependence.](http://arxiv.org/abs/2204.11924) | 本文提出了一种新型深度学习方法，用于计算具有一般形式均场相互作用的高维McKean-Vlasov正反向随机微分方程。通过将问题转化为求解具有显式系数函数的标准FBSDEs，并使用深度神经网络来近似MV-FBSDEs的模型系数，可以解决均场相互作用具有完全分布依赖性的问题。 |
| [^161] | [Contrastive Demonstration Tuning for Pre-trained Language Models.](http://arxiv.org/abs/2204.04392) | 本论文提出了一种名为对比演示调优的方法，可以在低数据场景下有效激发预训练语言模型的能力。实验结果表明，该方法与先前的提示调优方法相结合可以取得更好的性能。 |
| [^162] | [Theoretical Foundations for Pseudo-Inversion of Nonlinear Operators.](http://arxiv.org/abs/2111.10755) | 本文研究了非线性算子的伪逆，包括其存在性和唯一性条件以及性质分析，给出了一些众所周知的不可逆非线性算子PI的解析表达式，并讨论了其与小波阈值的关系。 |
| [^163] | [Optimal subgroup selection.](http://arxiv.org/abs/2109.01077) | 在回归设置中，我们提出了一个子群选择挑战，以确定回归函数超过预设阈值的特征空间区域。我们的主要贡献是确定了在样本规模和类型I错误概率上遗憾的最佳速率。 |
| [^164] | [Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning.](http://arxiv.org/abs/2108.08655) | 该论文研究了强化学习中在线演员-评论家算法的全局收敛性。通过数学分析证明，随着更新次数趋近无穷大，带有表格参数化的在线演员-评论家算法收敛于常微分方程。研究结果可以帮助我们理解演员-评论家算法在实践中的行为和性质。 |
| [^165] | [Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks.](http://arxiv.org/abs/2107.10234) | 该论文提出了一种统一框架，用于将基于空间和谱域的图神经网络进行整合，并紧密关联各自域内的方法。 |
| [^166] | [Calibrating multi-dimensional complex ODE from noisy data via deep neural networks.](http://arxiv.org/abs/2106.03591) | 该论文提出了一个两阶段非参数方法，通过深度神经网络校准多维复杂常微分方程的噪声数据。该方法能够恢复ODE系统，避免了维度灾难和复杂ODE结构的限制，并在模块化结构和适当选择网络架构的情况下被证明是一致的。 |
| [^167] | [The Lasso with general Gaussian designs with applications to hypothesis testing.](http://arxiv.org/abs/2007.13716) | 本论文推广了套索方法在高斯相关设计中的应用，通过一个更简单的“固定设计”模型来精确刻画套索估计器，解决了高维回归中的渐近正态性问题。 |
| [^168] | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.](http://arxiv.org/abs/1910.10683) | 本文通过引入一种统一的框架，将所有基于文本的语言问题转换为文本到文本格式，从而探索了NLP中的迁移学习技术的全貌，通过对多个任务进行系统研究，实现了许多基准测试上的最新结果。 |

# 详细

[^1]: 面向能源感知的联合交通预测技术在蜂窝网络中的应用

    Towards Energy-Aware Federated Traffic Prediction for Cellular Networks. (arXiv:2309.10645v1 [cs.LG])

    [http://arxiv.org/abs/2309.10645](http://arxiv.org/abs/2309.10645)

    本文提出了一种能够考虑准确性和能源消耗之间权衡的可持续性指标，针对蜂窝网络中的交通预测问题提出了面向能源感知的联合交通预测技术，通过联合学习的方式实现了高精度的预测。

    

    蜂窝网络中的交通预测对于优化第五代(5G)及更高版本的网络至关重要，准确的预测对于智能网络设计、资源分配和异常缓解是必不可少的。虽然机器学习(ML)是一种有效预测网络流量的方法，但大规模数据的集中存储在单个数据中心中，涉及到机密性、隐私和数据传输的问题。为了解决这些挑战，联合学习(FL)作为一种吸引人的ML训练框架应运而生，通过并行分布式计算提供高精度的预测。然而，这些方法的环境影响常常被忽视，这引发了对于其可持续性的质疑。本文通过提出一种新的可持续性指标，解决了联合学习中准确性和能源消耗之间的折衷问题，并对最先进的方法进行了全面评估。

    Cellular traffic prediction is a crucial activity for optimizing networks in fifth-generation (5G) networks and beyond, as accurate forecasting is essential for intelligent network design, resource allocation and anomaly mitigation. Although machine learning (ML) is a promising approach to effectively predict network traffic, the centralization of massive data in a single data center raises issues regarding confidentiality, privacy and data transfer demands. To address these challenges, federated learning (FL) emerges as an appealing ML training framework which offers high accurate predictions through parallel distributed computations. However, the environmental impact of these methods is often overlooked, which calls into question their sustainability. In this paper, we address the trade-off between accuracy and energy consumption in FL by proposing a novel sustainability indicator that allows assessing the feasibility of ML models. Then, we comprehensively evaluate state-of-the-art d
    
[^2]: 深度学习网络的几何结构和全局${\mathcal L}^2$最小化器的构建

    Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])

    [http://arxiv.org/abs/2309.10639](http://arxiv.org/abs/2309.10639)

    本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。

    

    本文提供了对深度学习（DL）网络结构的几何解释，该网络具有$L$个隐藏层，斜坡激活函数，${\mathcal L}^2$ Schatten类（或Hilbert-Schmidt）成本函数，以及相等维度$Q\geq1$的输入和输出空间${\mathbb R}^Q$。隐藏层也定义在${\mathbb R}^{Q}$的空间上。我们利用我们最新的关于浅层神经网络的结果，在$L\geq Q$的情况下构造了一个明确的最小化器族，该族能够全局最小化成本函数，并且我们证明这个族是退化的。在这里提到的上下文中，DL网络的隐藏层通过对训练输入的递归截断映射的应用来“整理”训练输入，以最小化噪声与信号的比率。此外，我们确定了$2^Q-1$个不同的退化局部最小值。

    In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
    
[^3]: 大型语言模型能够准确预测搜索者的偏好

    Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])

    [http://arxiv.org/abs/2309.10621](http://arxiv.org/abs/2309.10621)

    大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。

    

    相关性标签是评估和优化搜索系统的关键。获取大量相关性标签通常需要第三方标注人员，但存在低质量数据的风险。本论文介绍了一种改进标签质量的替代方法，通过从真实用户那里获得仔细反馈来获取高质量的第一方数据。

    Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
    
[^4]: 基于超广角眼底图像的无源主动域自适应糖尿病视网膜病变分级

    Source-free Active Domain Adaptation for Diabetic Retinopathy Grading Based on Ultra-wide-field Fundus Image. (arXiv:2309.10619v1 [cs.CV])

    [http://arxiv.org/abs/2309.10619](http://arxiv.org/abs/2309.10619)

    本论文提出了一种基于超广角眼底图像的无源主动域自适应方法（SFADA）用于糖尿病视网膜病变分级，通过生成连续演变关系的特征、活动选择有价值的UWF眼底图像进行标注以及利用DR病变原型适应模型，实现了最先进的DR分级性能。

    

    域自适应（DA）被广泛应用于未注释的超广角（UWF）眼底图像的糖尿病视网膜病变（DR）分级中，可以从已标注的彩色眼底图像中转化已注释的知识。然而，由于巨大的域差异和复杂的真实世界场景，大多数主流DA算法在DR分级性能上远远达不到临床诊断的水平。为了解决这个问题，我们在本文中提出了一种新颖的无源主动域自适应（SFADA）方法。具体而言，我们聚焦于DR分级问题本身，提出以DR连续演变关系的方式生成彩色眼底图像特征，通过局部表示匹配主动选择一些有价值的UWF眼底图像进行标注，并利用DR病变原型在UWF眼底图像上适应模型。值得注意的是，SFADA还考虑了数据隐私和计算效率。大量的实验结果表明，我们提出的SFADA方法取得了最先进的DR分级性能。

    Domain adaptation (DA) has been widely applied in the diabetic retinopathy (DR) grading of unannotated ultra-wide-field (UWF) fundus images, which can transfer annotated knowledge from labeled color fundus images. However, suffering from huge domain gaps and complex real-world scenarios, the DR grading performance of most mainstream DA is far from that of clinical diagnosis. To tackle this, we propose a novel source-free active domain adaptation (SFADA) in this paper. Specifically, we focus on DR grading problem itself and propose to generate features of color fundus images with continuously evolving relationships of DRs, actively select a few valuable UWF fundus images for labeling with local representation matching, and adapt model on UWF fundus images with DR lesion prototypes. Notably, the SFADA also takes data privacy and computational efficiency into consideration. Extensive experimental results demonstrate that our proposed SFADA achieves state-of-the-art DR grading performance,
    
[^5]: 非负潜在因子分析的动态线性偏置引入方案

    A Dynamic Linear Bias Incorporation Scheme for Nonnegative Latent Factor Analysis. (arXiv:2309.10618v1 [cs.AI])

    [http://arxiv.org/abs/2309.10618](http://arxiv.org/abs/2309.10618)

    本文提出了一种动态线性偏置引入方案，以提高非负潜在因子分析（NLFA）模型对高维不完整数据的表示学习能力。

    

    高维度和不完整（HDI）数据在大数据相关应用中很常见，比如社交网络服务系统，这些系统涉及与众多节点之间的有限交互。从HDI数据中获取知识是数据科学领域的一个重要问题，因为HDI数据中嵌入了丰富的模式，如节点行为。非负潜在因子分析（NLFA）模型已被证明在解决这个问题方面具有优势，在这个模型中，线性偏置引入（LBI）方案对于处理训练过度摆动、波动以及防止模型过早收敛非常重要。然而，现有的LBI方案都是统计型的，线性偏置是固定的，这严重限制了产生的NLFA模型的可扩展性，并导致了对HDI数据的表示学习能力的损失。在以上发现的基础上，本文创新地提出了一种动态线性偏置引入方案。

    High-Dimensional and Incomplete (HDI) data is commonly encountered in big data-related applications like social network services systems, which are concerning the limited interactions among numerous nodes. Knowledge acquisition from HDI data is a vital issue in the domain of data science due to their embedded rich patterns like node behaviors, where the fundamental task is to perform HDI data representation learning. Nonnegative Latent Factor Analysis (NLFA) models have proven to possess the superiority to address this issue, where a linear bias incorporation (LBI) scheme is important in present the training overshooting and fluctuation, as well as preventing the model from premature convergence. However, existing LBI schemes are all statistic ones where the linear biases are fixed, which significantly restricts the scalability of the resultant NLFA model and results in loss of representation learning ability to HDI data. Motivated by the above discoveries, this paper innovatively pres
    
[^6]: 可扩展的Robust Optimisation Monte Carlo的Python实现

    An Extendable Python Implementation of Robust Optimisation Monte Carlo. (arXiv:2309.10612v1 [cs.LG])

    [http://arxiv.org/abs/2309.10612](http://arxiv.org/abs/2309.10612)

    这篇论文介绍了Python包ELFI中的Robust Optimisation Monte Carlo（ROMC）方法的实现。ROMC是一种新颖且高效的无似然函数推理（LFI）框架，可以提供准确的加权后验样本。实现可以作为即插即用的LFI算法使用，也可以支持可扩展性的研究。

    

    在具有无法处理的似然函数的统计模型中进行推理是具有挑战性的，因此，大多数无似然函数推理（LFI）方法会遇到准确性和效率的限制。在本文中，我们介绍了Python包ELFI中LFI方法Robust Optimisation Monte Carlo（ROMC）的实现。ROMC是一种新颖而高效（高度可并行化）的LFI框架，可以提供准确的加权后验样本。我们的实现可以通过两种方式使用。首先，科学家可以将其作为即插即用的LFI算法使用；我们提供了一个易于使用的API，与ELFI的原则相协调，可以轻松与包中其他方法进行比较。此外，我们还将ROMC细分为孤立的组件，以支持可扩展性。研究人员可以实验新的方法来解决ROMC的一部分问题，而无需从头开始重新实现整个过程。在这两种情景下，ROMC部分可以完全并行运行。

    Performing inference in statistical models with an intractable likelihood is challenging, therefore, most likelihood-free inference (LFI) methods encounter accuracy and efficiency limitations. In this paper, we present the implementation of the LFI method Robust Optimisation Monte Carlo (ROMC) in the Python package ELFI. ROMC is a novel and efficient (highly-parallelizable) LFI framework that provides accurate weighted samples from the posterior. Our implementation can be used in two ways. First, a scientist may use it as an out-of-the-box LFI algorithm; we provide an easy-to-use API harmonized with the principles of ELFI, enabling effortless comparisons with the rest of the methods included in the package. Additionally, we have carefully split ROMC into isolated components for supporting extensibility. A researcher may experiment with novel method(s) for solving part(s) of ROMC without reimplementing everything from scratch. In both scenarios, the ROMC parts can run in a fully-paralle
    
[^7]: 基于机器学习的共轨道运动的小行星分类

    Asteroids co-orbital motion classification based on Machine Learning. (arXiv:2309.10603v1 [astro-ph.EP])

    [http://arxiv.org/abs/2309.10603](http://arxiv.org/abs/2309.10603)

    本研究基于机器学习，探索了如何对具有给定行星的共轨道运动的小行星进行分类，通过研究与共振相关的角度变量的时间序列，利用自定义的数据分析流程和机器学习算法实现了对小行星共轨道运动的有效分类。

    

    本研究探讨了如何使用机器学习对具有给定行星的共轨道运动的小行星进行分类。我们考虑了与行星的平均运动共振有四种不同类型的运动，分别是Tadpole（蝌蚪型），Horseshoe（马蹄型）和Quasi-satellite（准卫星型），并构建了三个数据集，分别为真实数据集（采用JPL Horizons系统中的真实小行星星历）、理想数据集和扰动数据集（均为通过传播考虑不同动力系统的初始条件得到的模拟数据），用于在不同条件下训练和测试机器学习算法。我们使用自定义的数据分析流程对与共振相关的角度变量theta的时间序列进行研究，该流程由数据创建和注释、基于tsfresh包的时间序列特征提取（可能包括选择和标准化）以及应用机器学习算法进行降维和分类组成。这种基于机器学习的方法可以有效分类小行星的共轨道运动。

    In this work, we explore how to classify asteroids in co-orbital motion with a given planet using Machine Learning. We consider four different kinds of motion in mean motion resonance with the planet, nominally Tadpole, Horseshoe and Quasi-satellite, building 3 datasets defined as Real (taking the ephemerides of real asteroids from the JPL Horizons system), Ideal and Perturbed (both simulated, obtained by propagating initial conditions considering two different dynamical systems) for training and testing the Machine Learning algorithms in different conditions.  The time series of the variable theta (angle related to the resonance) are studied with a data analysis pipeline defined ad hoc for the problem and composed by: data creation and annotation, time series features extraction thanks to the tsfresh package (potentially followed by selection and standardization) and the application of Machine Learning algorithms for Dimensionality Reduction and Classification. Such approach, based on
    
[^8]: 神经元超材料网络用于非线性材料设计

    Neural Metamaterial Networks for Nonlinear Material Design. (arXiv:2309.10600v1 [cs.GR])

    [http://arxiv.org/abs/2309.10600](http://arxiv.org/abs/2309.10600)

    本文提出了Neural Metamaterial Networks (NMN)方法，通过平滑神经表示编码整个超材料家族的非线性力学，实现了从结构参数到性能空间的平滑映射，适合进行梯度优化。这项研究对于非线性材料设计有着重要意义。

    

    在工程、医学、机器人学等领域中，具有定制化机械特性的非线性超材料具有很多应用。虽然建模它们的宏观力学行为本身就具有挑战性，但是找到能够实现高水平性能目标的结构参数是一项艰巨的任务。在这项工作中，我们提出了神经元超材料网络（NMN）- 一种能够编码整个超材料家族的非线性力学的平滑神经表示。给定结构参数作为输入，NMN返回连续可微的应变能密度函数，从而通过构造保证保守力。虽然在模拟数据上进行训练，NMN不会继承有限元网格中拓扑变化导致的不连续性。它们提供的是一个从参数空间到性能空间的平滑映射，完全可微，非常适合基于梯度的优化。基于此，我们将逆向材料设计形式化为一个非线性程ryn

    Nonlinear metamaterials with tailored mechanical properties have applications in engineering, medicine, robotics, and beyond. While modeling their macromechanical behavior is challenging in itself, finding structure parameters that lead to ideal approximation of high-level performance goals is a challenging task. In this work, we propose Neural Metamaterial Networks (NMN) -- smooth neural representations that encode the nonlinear mechanics of entire metamaterial families. Given structure parameters as input, NMN return continuously differentiable strain energy density functions, thus guaranteeing conservative forces by construction. Though trained on simulation data, NMN do not inherit the discontinuities resulting from topological changes in finite element meshes. They instead provide a smooth map from parameter to performance space that is fully differentiable and thus well-suited for gradient-based optimization. On this basis, we formulate inverse material design as a nonlinear prog
    
[^9]: 无监督深度跨语言实体对齐

    Unsupervised Deep Cross-Language Entity Alignment. (arXiv:2309.10598v1 [cs.CL])

    [http://arxiv.org/abs/2309.10598](http://arxiv.org/abs/2309.10598)

    本文提出了一种无监督的跨语言实体对齐方法，利用深度学习多语言编码器和机器翻译器对知识图谱文本进行编码，同时考虑了全局和局部对齐策略，生成了有排名的匹配结果，并提供了多种优化选择。

    

    跨语言实体对齐是在不同语言的知识图谱中找到相同语义实体的任务。在本文中，我们提出了一种简单而新颖的无监督跨语言实体对齐方法。我们利用深度学习多语言编码器结合机器翻译器来编码知识图谱文本，减少对标签数据的依赖。与仅强调全局或局部对齐的传统方法不同，我们的方法同时考虑了两种对齐策略。我们首先将对齐任务视为一个二分匹配问题，然后采用重新交换的思想来完成对齐。与仅给出一个最优解的传统二分匹配算法相比，我们的算法生成了有排名的匹配结果，这使得许多潜在的下游任务成为可能。此外，我们的方法可以在二分匹配过程中适应两种不同类型的优化（最小和最大），从而提供更多的灵活性。

    Cross-lingual entity alignment is the task of finding the same semantic entities from different language knowledge graphs. In this paper, we propose a simple and novel unsupervised method for cross-language entity alignment. We utilize the deep learning multi-language encoder combined with a machine translator to encode knowledge graph text, which reduces the reliance on label data. Unlike traditional methods that only emphasize global or local alignment, our method simultaneously considers both alignment strategies. We first view the alignment task as a bipartite matching problem and then adopt the re-exchanging idea to accomplish alignment. Compared with the traditional bipartite matching algorithm that only gives one optimal solution, our algorithm generates ranked matching results which enabled many potentials downstream tasks. Additionally, our method can adapt two different types of optimization (minimal and maximal) in the bipartite matching process, which provides more flexibil
    
[^10]: 面向符号音乐的主题中心表示学习

    Motif-Centric Representation Learning for Symbolic Music. (arXiv:2309.10597v1 [cs.SD])

    [http://arxiv.org/abs/2309.10597](http://arxiv.org/abs/2309.10597)

    该论文提出了一种面向符号音乐的主题中心表示学习方法，通过Siamese网络架构和预训练与微调流程学习主题和其变化之间的隐含关系。实验证明，这两种方法相互补充，使精确率-召回率曲线下的面积提升了12.6%。最后，通过可视化获得的主题表示，提供了直观的理解。

    

    音乐主题作为音乐组合的概念构建块，在音乐结构分析和自动作曲中至关重要。尽管人类听众可以轻松识别主题，但现有的计算模型在表示主题及其发展方面存在不足。这是因为主题的特性是隐含的，而主题变化的多样性超越了简单的重复和调制。在这项研究中，我们旨在通过表示学习学习主题与其变化之间的隐含关系，使用Siamese网络架构和预训练与微调流程。采用基于正则化的VICReg方法进行预训练，而对比学习用于微调。对基于检索的任务的实验结果表明，这两种方法互补，使在精确率-召回率曲线下的面积提升了12.6%。最后，我们可视化获取的主题表示，提供直观的理解。

    Music motif, as a conceptual building block of composition, is crucial for music structure analysis and automatic composition. While human listeners can identify motifs easily, existing computational models fall short in representing motifs and their developments. The reason is that the nature of motifs is implicit, and the diversity of motif variations extends beyond simple repetitions and modulations. In this study, we aim to learn the implicit relationship between motifs and their variations via representation learning, using the Siamese network architecture and a pretraining and fine-tuning pipeline. A regularization-based method, VICReg, is adopted for pretraining, while contrastive learning is used for fine-tuning. Experimental results on a retrieval-based task show that these two methods complement each other, yielding an improvement of 12.6% in the area under the precision-recall curve. Lastly, we visualize the acquired motif representations, offering an intuitive comprehension
    
[^11]: 移动群智感知中的任务分配游戏中的去中心化在线学习

    Decentralized Online Learning in Task Assignment Games for Mobile Crowdsensing. (arXiv:2309.10594v1 [cs.SI])

    [http://arxiv.org/abs/2309.10594](http://arxiv.org/abs/2309.10594)

    本文研究了移动群智感知系统中的任务分配问题，提出了一种新颖的结合匹配理论和在线学习的去中心化方法。该方法通过考虑移动单元的个体目标，同时在线学习移动单元的努力，解决了感知平台和移动单元之间的目标冲突和不确定性问题。创新的"无感知"机制提高了学习过程并减少了冲突。

    

    本文研究了移动群智感知系统中协调数据收集的问题。移动群智感知平台逐步发布感知任务给可用移动单元，并通过回传感知报价来表示它们参与任务的意愿。根据所收到的报价，感知平台决定任务分配。稳定的任务分配需要解决两个挑战：感知平台和移动单元之间的目标冲突，以及对移动单元所需努力和偏好的不确定性。为了克服这些挑战，提出了一种结合匹配理论和在线学习的新颖去中心化方法，称为"碰撞避免多臂老虎机-策略无感知"（CA-MAB-SFS）。将任务分配问题建模为考虑感知平台和移动单元的个体目标的匹配游戏，而移动单元在线学习其努力。我们的创新的"无感知"机制显著提高了移动单元的学习过程，同时减少了冲突。

    The problem of coordinated data collection is studied for a mobile crowdsensing (MCS) system. A mobile crowdsensing platform (MCSP) sequentially publishes sensing tasks to the available mobile units (MUs) that signal their willingness to participate in a task by sending sensing offers back to the MCSP. From the received offers, the MCSP decides the task assignment. A stable task assignment must address two challenges: the MCSP's and MUs' conflicting goals, and the uncertainty about the MUs' required efforts and preferences. To overcome these challenges a novel decentralized approach combining matching theory and online learning, called collision-avoidance multi-armed bandit with strategic free sensing (CA-MAB-SFS), is proposed. The task assignment problem is modeled as a matching game considering the MCSP's and MUs' individual goals while the MUs learn their efforts online. Our innovative "free-sensing" mechanism significantly improves the MU's learning process while reducing collision
    
[^12]: 对不确定性量化的对抗性攻击

    Adversarial Attacks Against Uncertainty Quantification. (arXiv:2309.10586v1 [cs.CV])

    [http://arxiv.org/abs/2309.10586](http://arxiv.org/abs/2309.10586)

    本研究着眼于对不确定性量化的对抗性攻击，提出了一种新的攻击场景，即攻击者通过操纵不确定度估计来削弱机器学习模型的输出效果，无论预测的正确性如何。本文设计了威胁模型并提出了多种攻击策略。

    

    机器学习模型可以被对抗性示例所欺骗，即通过精心设计的输入扰动来迫使模型输出错误的预测。尽管最近提出了使用不确定性量化来检测对抗性输入的方法，但在假设这些攻击显示比原始数据具有更高的预测不确定性的情况下，已经表明针对减少不确定度估计的自适应攻击可以轻易绕过这种防御机制。在这项工作中，我们关注的是另一种对抗性场景，其中攻击者仍然有兴趣操纵不确定度估计，但不考虑预测的正确性；具体而言，目标是当机器学习模型的输出被下游模块或人操作员使用时削弱其效果。在这个方向上，我们：\textit{(i)}为针对不确定性量化的攻击设计了一个威胁模型；\textit{(ii)}设计了不同的攻击策略

    Machine-learning models can be fooled by adversarial examples, i.e., carefully-crafted input perturbations that force models to output wrong predictions. While uncertainty quantification has been recently proposed to detect adversarial inputs, under the assumption that such attacks exhibit a higher prediction uncertainty than pristine data, it has been shown that adaptive attacks specifically aimed at reducing also the uncertainty estimate can easily bypass this defense mechanism. In this work, we focus on a different adversarial scenario in which the attacker is still interested in manipulating the uncertainty estimate, but regardless of the correctness of the prediction; in particular, the goal is to undermine the use of machine-learning models when their outputs are consumed by a downstream module or by a human operator. Following such direction, we: \textit{(i)} design a threat model for attacks targeting uncertainty quantification; \textit{(ii)} devise different attack strategies 
    
[^13]: PDRL：基于多智能体强化学习的预测监控

    PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring. (arXiv:2309.10576v1 [cs.LG])

    [http://arxiv.org/abs/2309.10576](http://arxiv.org/abs/2309.10576)

    本文提出了一种基于多智能体强化学习的预测监控系统（PDRL），该系统可以在复杂环境中监测预测未来状态，并通过奖励策略来学习现有知识和最大化奖励。

    

    强化学习因其能够从以往经验中学习并做出自适应决策的能力，在监控应用中越来越被应用。然而，现有的基于机器学习的健康监控应用大多是基于监督学习算法，训练标签数据，无法在不确定的复杂环境中做出自适应决策。本研究提出了一种新颖的通用系统，即具有多个强化学习智能体的预测深度强化学习（PDRL），应用于时间序列预测环境。该提出的通用框架可以容纳虚拟深度 Q 网络（DQN）智能体，以监测复杂环境的预测未来状态，并根据明确定义的奖励策略使智能体在最大化奖励的同时学习现有知识。在评估该框架的过程中，部署了三个强化学习智能体以监测通过 BiLSTM 模型预测的受试者未来的心率、呼吸率和体温。随着每次迭代，智能体根据实际反馈调整其行为策略。

    Reinforcement learning has been increasingly applied in monitoring applications because of its ability to learn from previous experiences and can make adaptive decisions. However, existing machine learning-based health monitoring applications are mostly supervised learning algorithms, trained on labels and they cannot make adaptive decisions in an uncertain complex environment. This study proposes a novel and generic system, predictive deep reinforcement learning (PDRL) with multiple RL agents in a time series forecasting environment. The proposed generic framework accommodates virtual Deep Q Network (DQN) agents to monitor predicted future states of a complex environment with a well-defined reward policy so that the agent learns existing knowledge while maximizing their rewards. In the evaluation process of the proposed framework, three DRL agents were deployed to monitor a subject's future heart rate, respiration, and temperature predicted using a BiLSTM model. With each iteration, t
    
[^14]: 移动边缘计算中通过深度强化学习进行任务图离载

    Task Graph offloading via Deep Reinforcement Learning in Mobile Edge Computing. (arXiv:2309.10569v1 [cs.DC])

    [http://arxiv.org/abs/2309.10569](http://arxiv.org/abs/2309.10569)

    本文研究了在移动边缘计算环境下通过深度强化学习实现任务图离载的问题。现有的工作往往无法适应环境变化，导致用户体验下降。我们提出了一种将任务图调度建模为马尔可夫决策过程的方法，以适应计算能力随时间变化的情况。

    

    随着移动应用程序越来越复杂，其中包含的依赖任务越来越流行，这些应用程序通常具有低延迟要求，从而导致对计算资源的需求急剧增加。随着移动边缘计算（MEC）的出现，将应用程序任务卸载到部署在移动网络边缘的小型设备上以获得高质量用户体验成为最重要的问题。然而，由于MEC环境是动态的，大多数依赖专家知识或准确的分析模型的现有工作在任务图离载方面无法完全适应这种环境变化，导致用户体验降低。本文研究了MEC中的任务图离载，考虑到边缘计算设备的计算能力随时间变化。为了适应环境变化，我们将计算离载的任务图调度建模为一个Markov决策过程（Markov Decision Process）。

    Various mobile applications that comprise dependent tasks are gaining widespread popularity and are increasingly complex. These applications often have low-latency requirements, resulting in a significant surge in demand for computing resources. With the emergence of mobile edge computing (MEC), it becomes the most significant issue to offload the application tasks onto small-scale devices deployed at the edge of the mobile network for obtaining a high-quality user experience. However, since the environment of MEC is dynamic, most existing works focusing on task graph offloading, which rely heavily on expert knowledge or accurate analytical models, fail to fully adapt to such environmental changes, resulting in the reduction of user experience. This paper investigates the task graph offloading in MEC, considering the time-varying computation capabilities of edge computing devices. To adapt to environmental changes, we model the task graph scheduling for computation offloading as a Mark
    
[^15]: 多模态建模用于口语识别

    Multimodal Modeling For Spoken Language Identification. (arXiv:2309.10567v1 [cs.CL])

    [http://arxiv.org/abs/2309.10567](http://arxiv.org/abs/2309.10567)

    该论文提出了一种多模态口语识别方法MuSeLI，利用视频标题、描述和地理位置等元数据来增强语言识别任务，并在两个YouTube视频数据集上获得了最新的结果。

    

    口语识别是指在给定的话语中自动预测口语的任务。传统上，它被建模为一种基于语音的语言识别任务。以往的技术都局限于单一模态；然而，在视频数据中，存在许多其他元数据，这些元数据对于这个任务可能会有益处。在这项工作中，我们提出了MuSeLI，一种多模态口语识别方法，它深入研究了使用各种元数据源来增强语言识别。我们的研究发现，诸如视频标题、描述和地理位置等元数据提供了大量信息，能够识别多媒体录制的口语。我们利用两个不同的YouTube视频的公共数据集进行实验，并在语言识别任务中获得了最新的结果。我们还进行了一项消融研究，描述了每种模态对语言识别的独特贡献。

    Spoken language identification refers to the task of automatically predicting the spoken language in a given utterance. Conventionally, it is modeled as a speech-based language identification task. Prior techniques have been constrained to a single modality; however in the case of video data there is a wealth of other metadata that may be beneficial for this task. In this work, we propose MuSeLI, a Multimodal Spoken Language Identification method, which delves into the use of various metadata sources to enhance language identification. Our study reveals that metadata such as video title, description and geographic location provide substantial information to identify the spoken language of the multimedia recording. We conduct experiments using two diverse public datasets of YouTube videos, and obtain state-of-the-art results on the language identification task. We additionally conduct an ablation study that describes the distinct contribution of each modality for language recognition.
    
[^16]: 一个用于分类和解释大型非结构化法律文件的分层神经框架

    A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents. (arXiv:2309.10563v1 [cs.IR])

    [http://arxiv.org/abs/2309.10563](http://arxiv.org/abs/2309.10563)

    本论文提出了一个名为MESc的分层神经框架，用于分类和解释大型非结构化法律文件。通过将文件分成多个部分并使用大型语言模型的嵌入和无监督聚类，该框架能够实现从长文档中预测判决并提取解释。

    

    自动法律判决预测及其解释常常面临长达数万字的案例文件和非统一结构的问题。在没有结构标注的文件上预测判决并提取解释变得更具挑战性。本论文将这一问题定义为“稀缺标注法律文件”，并通过一种称为MESc（基于多阶段编码器的带聚类的监督）的深度学习分类框架来探索缺乏结构信息和长文档的特点。具体来说，我们将文档分成多个部分，从自定义微调的大型语言模型的最后四个层中提取它们的嵌入，并试图通过无监督聚类来近似它们的结构。然后，我们利用另一组Transformer编码器层学习部分之间的表示。我们探索了多十亿参数的大型语言模型在这种情况下的适应性。

    Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We explore the adaptability of LLMs with multi-billion
    
[^17]: 基于混合状态空间的学习用于序列数据预测的联合优化

    Hybrid State Space-based Learning for Sequential Data Prediction with Joint Optimization. (arXiv:2309.10553v1 [stat.ML])

    [http://arxiv.org/abs/2309.10553](http://arxiv.org/abs/2309.10553)

    本研究提出了一种基于混合状态空间的学习方法，通过联合优化循环神经网络和时间序列模型，实现了对序列数据的非线性预测和建模，克服了传统模型中的特征工程问题，并取得了显著的效果。

    

    我们研究在线环境中的非线性预测/回归问题，并引入了一种混合模型，通过状态空间形式的联合机制有效地减轻了传统非线性预测模型中特定领域特征工程问题的需求，并实现了非线性和线性组分的有效混合。我们使用递归结构从原始顺序序列中提取特征，并使用传统线性时间序列模型处理序列数据的复杂性，如季节性、趋势等。与现有的集成或混合模型通常以分离的方式训练基础模型不同，这不仅耗时，而且由于建模的分隔或独立训练而非最优。相反，我们首次在文献中采用联合优化的方法，从原始数据中联合优化增强的循环神经网络（LSTM）自动提取特征和ARMA系列时间序列模型（SARIMAX）进行有效建模。

    We investigate nonlinear prediction/regression in an online setting and introduce a hybrid model that effectively mitigates, via a joint mechanism through a state space formulation, the need for domain-specific feature engineering issues of conventional nonlinear prediction models and achieves an efficient mix of nonlinear and linear components. In particular, we use recursive structures to extract features from raw sequential sequences and a traditional linear time series model to deal with the intricacies of the sequential data, e.g., seasonality, trends. The state-of-the-art ensemble or hybrid models typically train the base models in a disjoint manner, which is not only time consuming but also sub-optimal due to the separation of modeling or independent training. In contrast, as the first time in the literature, we jointly optimize an enhanced recurrent neural network (LSTM) for automatic feature extraction from raw data and an ARMA-family time series model (SARIMAX) for effectivel
    
[^18]: 用于静态词嵌入的邻域感知差分隐私机制

    A Neighbourhood-Aware Differential Privacy Mechanism for Static Word Embeddings. (arXiv:2309.10551v1 [cs.LG])

    [http://arxiv.org/abs/2309.10551](http://arxiv.org/abs/2309.10551)

    该论文提出了一种邻域感知差分隐私机制，通过考虑预训练的词嵌入空间中单词的邻域来确定所需的最小噪声量，实验证明该机制在多个下游任务中优于其他机制，同时保证更高的隐私级别。

    

    我们提出了一种考虑预训练的静态词嵌入空间中单词邻域的邻域感知差分隐私（NADP）机制，以确定保证指定隐私级别所需的最小噪声量。我们首先使用它们的嵌入构建单词的最近邻图，并将其分解为一组连通分量（即邻域）。然后，在每个邻域中根据该邻域中的单词集合分别对单词应用不同水平的高斯噪声。实验表明，我们提出的NADP机制在多个下游任务中始终优于多个先前提出的DP机制，如拉普拉斯、高斯和马氏距离，同时保证更高的隐私级别。

    We propose a Neighbourhood-Aware Differential Privacy (NADP) mechanism considering the neighbourhood of a word in a pretrained static word embedding space to determine the minimal amount of noise required to guarantee a specified privacy level. We first construct a nearest neighbour graph over the words using their embeddings, and factorise it into a set of connected components (i.e. neighbourhoods). We then separately apply different levels of Gaussian noise to the words in each neighbourhood, determined by the set of words in that neighbourhood. Experiments show that our proposed NADP mechanism consistently outperforms multiple previously proposed DP mechanisms such as Laplacian, Gaussian, and Mahalanobis in multiple downstream tasks, while guaranteeing higher levels of privacy.
    
[^19]: 机器学习问题中用于算法投资策略的新损失函数——平均绝对方向损失

    Mean Absolute Directional Loss as a New Loss Function for Machine Learning Problems in Algorithmic Investment Strategies. (arXiv:2309.10546v1 [q-fin.CP])

    [http://arxiv.org/abs/2309.10546](http://arxiv.org/abs/2309.10546)

    这项研究提出了一种新的损失函数(Mean Absolute Directional Loss，MADL)，用于解决在机器学习模型中应用于算法投资策略中的金融时间序列预测问题。通过在两种不同资产类别的数据上验证，MADL函数能够提供更好的超参数选择，并获得更有效的投资策略。

    

    本文研究了在算法投资策略（AIS）构建中，用于金融时间序列预测的机器学习模型的适当损失函数的问题。我们提出了平均绝对方向损失（MADL）函数，解决了传统预测误差函数在从预测中提取信息以创建有效的买卖信号方面的重要问题。最后，我们基于两种不同资产类别（加密货币：比特币和大宗商品：原油）的数据表明，新的损失函数使我们能够选择更好的LSTM模型超参数，并在样本外数据上获得更有效的投资策略，相对于风险调整回报指标。

    This paper investigates the issue of an adequate loss function in the optimization of machine learning models used in the forecasting of financial time series for the purpose of algorithmic investment strategies (AIS) construction. We propose the Mean Absolute Directional Loss (MADL) function, solving important problems of classical forecast error functions in extracting information from forecasts to create efficient buy/sell signals in algorithmic investment strategies. Finally, based on the data from two different asset classes (cryptocurrencies: Bitcoin and commodities: Crude Oil), we show that the new loss function enables us to select better hyperparameters for the LSTM model and obtain more efficient investment strategies, with regard to risk-adjusted return metrics on the out-of-sample data.
    
[^20]: 模型吸取: 针对LLMs的一种提取攻击

    Model Leeching: An Extraction Attack Targeting LLMs. (arXiv:2309.10544v1 [cs.LG])

    [http://arxiv.org/abs/2309.10544](http://arxiv.org/abs/2309.10544)

    模型吸取是一种针对大型语言模型的提取攻击，能够将目标模型的任务特定知识提取到一个参数较少的模型中，并且具有较高的准确率和攻击成功率。

    

    模型吸取是一种针对大型语言模型(LLMs)的新型提取攻击，能够将目标LLM的任务特定知识提炼到一个参数较少的模型中。我们通过从ChatGPT-3.5-Turbo中提取任务能力来演示我们攻击的有效性，实现了73%的准确匹配(EM)相似性以及75%的SQuAD EM准确率和87%的F1得分，仅需50美元的API费用。我们进一步展示了通过模型吸取提取的模型在对目标LLM进行机器学习攻击时的可行性，当应用于ChatGPT-3.5-Turbo时，攻击成功率提高了11%。

    Model Leeching is a novel extraction attack targeting Large Language Models (LLMs), capable of distilling task-specific knowledge from a target LLM into a reduced parameter model. We demonstrate the effectiveness of our attack by extracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match (EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%, respectively for only $50 in API cost. We further demonstrate the feasibility of adversarial attack transferability from an extracted model extracted via Model Leeching to perform ML attack staging against a target LLM, resulting in an 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.
    
[^21]: 爱还是恨？共享还是分割？使用分割学习和同态加密的隐私保护训练

    Love or Hate? Share or Split? Privacy-Preserving Training Using Split Learning and Homomorphic Encryption. (arXiv:2309.10517v1 [cs.CR])

    [http://arxiv.org/abs/2309.10517](http://arxiv.org/abs/2309.10517)

    本文提出了一种基于分割学习和同态加密的隐私保护训练方法，通过在用户激活图上应用同态加密，有效保护用户隐私，并改进了以往技术在准确性方面的表现。

    

    分割学习（SL）是一种新的协作学习技术，允许参与者，例如客户端和服务器，不共享原始数据进行机器学习模型的训练。在这种设置下，客户端首先在原始数据上应用其所属机器学习模型的部分，生成激活图，然后将它们发送到服务器以继续训练过程。以前的研究表明，重建激活图可能会导致客户数据的隐私泄漏。此外，现有的减轻SL隐私泄漏的技术在准确性方面证明显著较差。在本文中，我们改进了以往的工作，构建了一种基于U型SL的协议，可以在同态加密数据上操作。具体而言，在我们的方法中，客户端在将激活图发送到服务器之前对其进行同态加密，从而保护用户隐私。这是一个重要的改进，有助于提升隐私保护的能力。

    Split learning (SL) is a new collaborative learning technique that allows participants, e.g. a client and a server, to train machine learning models without the client sharing raw data. In this setting, the client initially applies its part of the machine learning model on the raw data to generate activation maps and then sends them to the server to continue the training process. Previous works in the field demonstrated that reconstructing activation maps could result in privacy leakage of client data. In addition to that, existing mitigation techniques that overcome the privacy leakage of SL prove to be significantly worse in terms of accuracy. In this paper, we improve upon previous works by constructing a protocol based on U-shaped SL that can operate on homomorphically encrypted data. More precisely, in our approach, the client applies homomorphic encryption on the activation maps before sending them to the server, thus protecting user privacy. This is an important improvement that
    
[^22]: 基于单图像的无监督联合分割和去噪方法

    Single-Image based unsupervised joint segmentation and denoising. (arXiv:2309.10511v1 [cs.CV])

    [http://arxiv.org/abs/2309.10511](http://arxiv.org/abs/2309.10511)

    本文提出一种基于单图像的无监督方法，实现了联合分割和去噪。该方法不需要大量标记样本，且能够处理高噪声和通用纹理，并在显微镜图像上表现出色。

    

    本文提出了一种基于单图像的无监督方法，用于联合分割和去噪。为此，我们将变分分割方法的优势与自监督、基于单图像的深度学习方法的能力结合起来。我们方法的一个主要优点在于，与需要大量标记样本的数据驱动方法不同，我们的模型可以在没有任何训练数据库的情况下将图像分割成多个有意义的区域。此外，我们引入了一种新颖的能量函数，其中去噪和分割以一种相互受益的方式耦合在一起。通过与自监督图像去噪的特定组合，我们解决了现有的单图像基于变分分割方法对高噪声或通用纹理的限制。我们提出了一个统一的优化策略，并展示了在显微镜中可用的非常嘈杂的图像上，我们的方法表现出色。

    In this work, we develop an unsupervised method for the joint segmentation and denoising of a single image. To this end, we combine the advantages of a variational segmentation method with the power of a self-supervised, single-image based deep learning approach. One major strength of our method lies in the fact, that in contrast to data-driven methods, where huge amounts of labeled samples are necessary, our model can segment an image into multiple meaningful regions without any training database. Further, we introduce a novel energy functional in which denoising and segmentation are coupled in a way that both tasks benefit from each other. The limitations of existing single-image based variational segmentation methods, which are not capable of dealing with high noise or generic texture, are tackled by this specific combination with self-supervised image denoising. We propose a unified optimisation strategy and show that, especially for very noisy images available in microscopy, our p
    
[^23]: 使用扩散模型学习端到端信道编码

    Learning End-to-End Channel Coding with Diffusion Models. (arXiv:2309.10505v1 [cs.IT])

    [http://arxiv.org/abs/2309.10505](http://arxiv.org/abs/2309.10505)

    本文提出了使用扩散模型学习端到端信道编码的框架，并通过模拟实验证明了扩散模型能够准确学习信道分布从而实现接近最优的端到端符号误码率。

    

    通过扩散模型近似信道分布，本文提出了一个基于扩散模型的端到端信道编码框架，并提出了一种高效的训练算法。通过与各种信道模型的模拟实验，验证了扩散模型精确学习信道分布的能力，从而实现了接近最优的端到端符号误码率（SER）。

    The training of neural encoders via deep learning necessitates a differentiable channel model due to the backpropagation algorithm. This requirement can be sidestepped by approximating either the channel distribution or its gradient through pilot signals in real-world scenarios. The initial approach draws upon the latest advancements in image generation, utilizing generative adversarial networks (GANs) or their enhanced variants to generate channel distributions. In this paper, we address this channel approximation challenge with diffusion models, which have demonstrated high sample quality in image generation. We offer an end-to-end channel coding framework underpinned by diffusion models and propose an efficient training algorithm. Our simulations with various channel models establish that our diffusion models learn the channel distribution accurately, thereby achieving near-optimal end-to-end symbol error rates (SERs). We also note a significant advantage of diffusion models: A robu
    
[^24]: 一个可配置的库用于生成和操作迷宫数据集

    A Configurable Library for Generating and Manipulating Maze Datasets. (arXiv:2309.10498v1 [cs.LG])

    [http://arxiv.org/abs/2309.10498](http://arxiv.org/abs/2309.10498)

    这个论文介绍了一个可配置的库，用于生成和处理迷宫数据集，研究人员可以通过该库生成不同分布的迷宫数据集，并对生成参数和生成规则进行自定义控制。可以支持多种输出格式，适用于不同类型的模型。

    

    理解机器学习模型对分布偏移的响应方式是一个重要的研究挑战。由于不同的生成算法提供了一个细致的平台来模拟微妙和显著的分布偏移，迷宫作为一个优秀的测试基准。为了支持对模型在分布偏离数据上行为的系统性研究，我们提出了“maze-dataset”，一个包含迷宫求解任务的生成、处理和可视化数据集的综合库。借助这个库，研究人员可以轻松创建数据集，可以对使用的生成算法、传递给选择算法的参数和生成的迷宫必须满足的筛选器进行广泛的控制。此外，它支持多种输出格式，包括栅格化和基于文本的格式，适用于卷积神经网络和自回归变换模型。这些格式以及用于可视化和转换的工具确保了灵活性和适应性。

    Understanding how machine learning models respond to distributional shifts is a key research challenge. Mazes serve as an excellent testbed due to varied generation algorithms offering a nuanced platform to simulate both subtle and pronounced distributional shifts. To enable systematic investigations of model behavior on out-of-distribution data, we present $\texttt{maze-dataset}$, a comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks. With this library, researchers can easily create datasets, having extensive control over the generation algorithm used, the parameters fed to the algorithm of choice, and the filters that generated mazes must satisfy. Furthermore, it supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models. These formats, along with tools for visualizing and converting between them, ensure versatility and adaptability in re
    
[^25]: Grid和自然语句对Normal-to-Lombard转换的比较研究

    A comparative study of Grid and Natural sentences effects on Normal-to-Lombard conversion. (arXiv:2309.10485v1 [cs.SD])

    [http://arxiv.org/abs/2309.10485](http://arxiv.org/abs/2309.10485)

    本文通过比较Grid句子和自然句子在Lombard效应和Normal-to-Lombard转换方面的表现，发现随着噪声水平的增加，Grid句子的alpha比例增加更大。在实验中，基于EMALG训练的StarGAN模型在主观可懂度评估中一致表现优于其他模型。

    

    Grid句子常用于研究Lombard效应和Normal-to-Lombard转换。然而，目前尚不清楚在真实应用中，基于Grid句子训练的Normal-to-Lombard模型是否足以提高自然语音可懂度。本文介绍了一个平行的Lombard语料库（称为Lombard Chinese TIMIT，LCT），并从中提取了中文TIMIT的自然句子。然后，我们使用LCT和Enhanced Mandarin Lombard Grid语料库（EMALG）比较了自然句子和Grid句子在Lombard效应和Normal-to-Lombard转换方面。通过对Lombard效应的参数分析，我们发现随着噪声水平的增加，自然句子和Grid句子的参数变化相似，但在alpha比例增加方面，Grid句子的增加更大。在跨性别和信噪比的主观可懂度评估中，基于EMALG训练的StarGAN模型始终表现优于其他模型。

    Grid sentence is commonly used for studying the Lombard effect and Normal-to-Lombard conversion. However, it's unclear if Normal-to-Lombard models trained on grid sentences are sufficient for improving natural speech intelligibility in real-world applications. This paper presents the recording of a parallel Lombard corpus (called Lombard Chinese TIMIT, LCT) extracting natural sentences from Chinese TIMIT. Then We compare natural and grid sentences in terms of Lombard effect and Normal-to-Lombard conversion using LCT and Enhanced MAndarin Lombard Grid corpus (EMALG). Through a parametric analysis of the Lombard effect, We find that as the noise level increases, both natural sentences and grid sentences exhibit similar changes in parameters, but in terms of the increase of the alpha ratio, grid sentences show a greater increase. Following a subjective intelligibility assessment across genders and Signal-to-Noise Ratios, the StarGAN model trained on EMALG consistently outperforms the mode
    
[^26]: 通过可证明的泛化加速量子机器学习模型的核心集选取

    Coreset selection can accelerate quantum machine learning models with provable generalization. (arXiv:2309.10441v1 [quant-ph])

    [http://arxiv.org/abs/2309.10441](http://arxiv.org/abs/2309.10441)

    本论文提出了一种统一的方法，通过从原始训练数据集中选取一个合理的子集，加速量子神经网络和量子核的训练，并分析了它们在这些核心集上训练时的泛化误差界限，揭示了与在完整原始数据集上训练相比具有可比性的性能。

    

    量子神经网络和量子核在量子机器学习领域中具有突出地位，利用即将到来的近期量子计算机的能力来克服经典机器学习的挑战。然而，训练效率的挑战限制了量子神经网络和量子核在应用于大规模数据集时的效果。为了解决这个问题，我们提出了一种统一的方法：核心集选择，旨在通过从原始训练数据集中提取出一个合理的子集来加速量子神经网络和量子核的训练。此外，当在这些核心集上训练时，我们分析了量子神经网络和量子核的泛化误差界限，并揭示了与在完整原始数据集上训练相比具有可比性的性能。通过系统的数值模拟，我们展示了核心集选择在加速涵盖合成数据分类、量子协议鉴定等任务中的潜力。

    Quantum neural networks (QNNs) and quantum kernels stand as prominent figures in the realm of quantum machine learning, poised to leverage the nascent capabilities of near-term quantum computers to surmount classical machine learning challenges. Nonetheless, the training efficiency challenge poses a limitation on both QNNs and quantum kernels, curbing their efficacy when applied to extensive datasets. To confront this concern, we present a unified approach: coreset selection, aimed at expediting the training of QNNs and quantum kernels by distilling a judicious subset from the original training dataset. Furthermore, we analyze the generalization error bounds of QNNs and quantum kernels when trained on such coresets, unveiling the comparable performance with those training on the complete original dataset. Through systematic numerical simulations, we illuminate the potential of coreset selection in expediting tasks encompassing synthetic data classification, identification of quantum co
    
[^27]: 图神经网络在滚动轴承动力学建模中的应用

    Graph Neural Networks for Dynamic Modeling of Roller Bearing. (arXiv:2309.10418v1 [cs.LG])

    [http://arxiv.org/abs/2309.10418](http://arxiv.org/abs/2309.10418)

    本文提出了将图神经网络应用于滚动轴承动力学建模的方法，通过将轴承的组件表示为图中的节点，GNN可以有效地模拟复杂的组件关系和相互作用。这种方法具有普适性和可解释性，在监测旋转机器的健康状态方面具有潜在的可扩展性。

    

    在本文中，我们提出将图神经网络（GNN）框架应用于预测滚动轴承的动力学。这种方法具有普适性和可解释性，具有潜在的可扩展性，可用于实时运行的数字孪生系统，用于监测旋转机器的健康状态。通过将轴承的组件表示为图中的节点，GNN可以有效地模拟它们之间的复杂关系和相互作用。我们利用轴承的动态弹簧阻尼模型生成GNN的训练数据。在该模型中，离散的质量代表轴承的组件，如滚动体、内圈和外圈，而赫兹接触模型用于计算这些组件之间的力。我们通过测试不同于训练配置的轴承配置来评估所提出的GNN框架的学习和推广能力。

    In the presented work, we propose to apply the framework of graph neural networks (GNNs) to predict the dynamics of a rolling element bearing. This approach offers generalizability and interpretability, having the potential for scalable use in real-time operational digital twin systems for monitoring the health state of rotating machines. By representing the bearing's components as nodes in a graph, the GNN can effectively model the complex relationships and interactions among them. We utilize a dynamic spring-mass-damper model of a bearing to generate the training data for the GNN. In this model, discrete masses represent bearing components such as rolling elements, inner raceways, and outer raceways, while a Hertzian contact model is employed to calculate the forces between these components.  We evaluate the learning and generalization capabilities of the proposed GNN framework by testing different bearing configurations that deviate from the training configurations. Through this app
    
[^28]: 通过网络感知嵌入进行无监督学习

    Unsupervised Learning via Network-Aware Embeddings. (arXiv:2309.10408v1 [cs.LG])

    [http://arxiv.org/abs/2309.10408](http://arxiv.org/abs/2309.10408)

    本研究提出了一种通过估计数值节点属性之间的网络距离来创建网络感知嵌入的无监督学习方法，解决了传统聚类方法难以处理复杂网络结构的问题。

    

    数据聚类是无监督学习的关键组成部分，其任务是根据相似性对观测结果进行分组，广泛应用于生物学、医学和社会科学等多个领域。然而，在这些领域中，数据的不同维度之间存在着复杂的相互依赖关系，例如人们在复杂的社交网络中可能具有的各种特征和观点。当前的聚类方法很难处理这种复杂性：深度学习可以近似这些依赖关系，但不能将其显式地作为分析的输入。本文旨在解决无监督学习文献中的这个盲点。我们通过估计数值节点属性之间的网络距离来创建网络感知嵌入，通过广义欧氏距离计算。与我们所知道的所有文献中的方法不同，我们不是对网络的节点进行聚类，而是对其节点属性进行聚类。在我们的实验中，还展示了我们方法的性能。

    Data clustering, the task of grouping observations according to their similarity, is a key component of unsupervised learning -- with real world applications in diverse fields such as biology, medicine, and social science. Often in these fields the data comes with complex interdependencies between the dimensions of analysis, for instance the various characteristics and opinions people can have live on a complex social network. Current clustering methods are ill-suited to tackle this complexity: deep learning can approximate these dependencies, but not take their explicit map as the input of the analysis. In this paper, we aim at fixing this blind spot in the unsupervised learning literature. We can create network-aware embeddings by estimating the network distance between numeric node attributes via the generalized Euclidean distance. Differently from all methods in the literature that we know of, we do not cluster the nodes of the network, but rather its node attributes. In our experi
    
[^29]: 使用ReLU网络在紧致域上进行通用逼近的最小宽度

    Minimum width for universal approximation using ReLU networks on compact domain. (arXiv:2309.10402v1 [cs.LG])

    [http://arxiv.org/abs/2309.10402](http://arxiv.org/abs/2309.10402)

    本研究通过使用ReLU-Like的激活函数，证明了在紧致域上将$L^p$函数从$[0,1]^{d_x}$逼近到$\mathbb R^{d_y}$所需的最小宽度为$\max\{d_x,d_y,2\}$，从而表明在紧致域上的逼近比在${\mathbb R^{d_x}}$上的逼近更容易。同时，利用包括ReLU在内的一般激活函数，我们还证明了一致逼近的最小宽度下界为$w_{\min}\ge d_y+1$（当$d_x<d_y\le2d_x$）。

    

    经过研究，限制宽度网络的通用逼近性质已经作为深度限制网络的经典通用逼近定理的对偶进行研究。已经有几次尝试来表征使得通用逼近性质成立的最小宽度$w_{\min}$，但只有很少几个找到了确切的值。在这项工作中，我们证明了对于从$[0,1]^{d_x}$到$\mathbb R^{d_y}$的$L^p$函数的通用逼近的最小宽度，如果激活函数是ReLU-Like（例如ReLU，GELU，Softplus），那么它的确切值是$\max\{d_x,d_y,2\}$。与已知的结果$w_{\min}=\max\{d_x+1,d_y\}$相比，当域为${\mathbb R^{d_x}}$时，我们的结果首次表明，在紧致域上的逼近要求比在${\mathbb R^{d_x}}$上的要求更小。我们接下来利用包括ReLU在内的一般激活函数进行一致逼近的最小宽度$w_{\min}$证明了一个下界：如果$d_x<d_y\le2d_x$，则$w_{\min}\ge d_y+1$。结合我们的第一个结果，这表明了一个二分法。

    The universal approximation property of width-bounded networks has been studied as a dual of the classical universal approximation theorem for depth-bounded ones. There were several attempts to characterize the minimum width $w_{\min}$ enabling the universal approximation property; however, only a few of them found the exact values. In this work, we show that the minimum width for the universal approximation of $L^p$ functions from $[0,1]^{d_x}$ to $\mathbb R^{d_y}$ is exactly $\max\{d_x,d_y,2\}$ if an activation function is ReLU-Like (e.g., ReLU, GELU, Softplus). Compared to the known result $w_{\min}=\max\{d_x+1,d_y\}$ when the domain is ${\mathbb R^{d_x}}$, our result first shows that approximation on a compact domain requires smaller width than on ${\mathbb R^{d_x}}$. We next prove a lower bound on $w_{\min}$ for uniform approximation using general activation functions including ReLU: $w_{\min}\ge d_y+1$ if $d_x<d_y\le2d_x$. Together with our first result, this shows a dichotomy be
    
[^30]: 可微分的量子架构搜索用于量子强化学习

    Differentiable Quantum Architecture Search for Quantum Reinforcement Learning. (arXiv:2309.10392v1 [quant-ph])

    [http://arxiv.org/abs/2309.10392](http://arxiv.org/abs/2309.10392)

    DQAS是一个基于梯度的框架，用于在NISQ时代自动设计量子电路。本研究的目标是探索DQAS在解决量子深度Q-learning问题上的能力。

    

    可微分量子架构搜索（DQAS）是一种基于梯度的框架，可以在NISQ时代自动设计量子电路。它的动机是量子硬件的低保真度，电路架构的低灵活性，电路设计成本高，平坦的荒原问题和权重的周期性。人们使用它来解决基于固定数据集的误差缓解、酉分解和量子逼近优化问题。量子强化学习（QRL）是量子机器学习的一部分，通常有各种数据。QRL通常使用手动设计的电路。然而，预定义的电路需要更多的灵活性来应对不同的任务，在不同数据集上基于电路设计可能变得棘手，特别是在电路规模较大的情况下。DQAS能否应用于具有不同数据集的量子深度Q-learning问题仍未解决。这项工作的主要目标是发现DQAS在解决量子深度Q-learning问题上的能力。

    Differentiable quantum architecture search (DQAS) is a gradient-based framework to design quantum circuits automatically in the NISQ era. It was motivated by such as low fidelity of quantum hardware, low flexibility of circuit architecture, high circuit design cost, barren plateau (BP) problem, and periodicity of weights. People used it to address error mitigation, unitary decomposition, and quantum approximation optimization problems based on fixed datasets. Quantum reinforcement learning (QRL) is a part of quantum machine learning and often has various data. QRL usually uses a manually designed circuit. However, the pre-defined circuit needs more flexibility for different tasks, and the circuit design based on various datasets could become intractable in the case of a large circuit. The problem of whether DQAS can be applied to quantum deep Q-learning with various datasets is still open. The main target of this work is to discover the capability of DQAS to solve quantum deep Q-learni
    
[^31]: 图对比学习与图元学习相遇：一种统一的用于少样本节点任务的方法

    Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks. (arXiv:2309.10376v1 [cs.LG])

    [http://arxiv.org/abs/2309.10376](http://arxiv.org/abs/2309.10376)

    COLA是一种统一的方法，整合了图对比学习和元学习的优势，用于少样本节点分类任务。它利用图扩增来识别语义相似的节点，从而使得能够在少样本情况下进行快速泛化。

    

    图神经网络（GNNs）已经在图表示学习（GRL）中变得流行起来。其中一个基本应用是少样本节点分类。大多数现有的方法都遵循元学习范式，展示了对少样本任务快速泛化的能力。然而，最近的研究表明，图对比学习结合微调可以明显优于元学习方法。尽管在经验上取得了成功，但对其背后原因的理解有限。在我们的研究中，我们首先确定了图对比学习相对于元学习的两个关键优势，包括（1）全面利用图节点和（2）图扩增的能力。为了将图对比学习和元学习的优势结合到少样本节点分类任务中，我们引入了一种新的范式：对比式少样本节点分类（COLA）。具体来说，COLA利用图扩增来识别语义相似的节点，从而使得

    Graph Neural Networks (GNNs) have become popular in Graph Representation Learning (GRL). One fundamental application is few-shot node classification. Most existing methods follow the meta learning paradigm, showing the ability of fast generalization to few-shot tasks. However, recent works indicate that graph contrastive learning combined with fine-tuning can significantly outperform meta learning methods. Despite the empirical success, there is limited understanding of the reasons behind it. In our study, we first identify two crucial advantages of contrastive learning compared to meta learning, including (1) the comprehensive utilization of graph nodes and (2) the power of graph augmentations. To integrate the strength of both contrastive learning and meta learning on the few-shot node classification tasks, we introduce a new paradigm: Contrastive Few-Shot Node Classification (COLA). Specifically, COLA employs graph augmentations to identify semantically similar nodes, which enables 
    
[^32]: 浅层神经网络的几何结构和基于${\mathcal L}^2$代价最小化的构造方法

    Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])

    [http://arxiv.org/abs/2309.10370](http://arxiv.org/abs/2309.10370)

    本文提供了浅层神经网络的几何结构解释，并通过基于${\mathcal L}^2$代价最小化的构造方法获得了一个具有优越性能的网络。

    

    本文给出了一个几何解释：浅层神经网络的结构由一个隐藏层、一个斜坡激活函数、一个${\mathcal L}^2$谱范类（或者Hilbert-Schmidt）的代价函数、输入空间${\mathbb R}^M$、输出空间${\mathbb R}^Q$（其中$Q\leq M$），以及训练输入样本数量$N>QM$所特征。我们证明了代价函数的最小值具有$O(\delta_P)$的上界，其中$\delta_P$衡量了训练输入的信噪比。我们使用适应于属于同一输出向量$y_j$的训练输入向量$\overline{x_{0,j}}$的投影来获得近似的优化器，其中$j=1,\dots,Q$。在特殊情况$M=Q$下，我们明确确定了代价函数的一个确切退化局部最小值；这个尖锐的值与对于$Q\leq M$所获得的上界之间有一个相对误差$O(\delta_P^2)$。上界证明的方法提供了一个构造性训练的网络；我们证明它测度了$Q$维空间中的给定输出。

    In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N>QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
    
[^33]: 边缘节点在联邦学习中的资源利用效率

    Toward efficient resource utilization at edge nodes in federated learning. (arXiv:2309.10367v1 [cs.LG])

    [http://arxiv.org/abs/2309.10367](http://arxiv.org/abs/2309.10367)

    本论文提出了一种受迁移学习启发的策略，旨在通过减少设备上的资源利用、以及减少服务器和网络的负载，提高联邦学习中边缘节点的效率。

    

    联邦学习（FL）使得边缘节点能够共同构建全局模型，而无需共享他们的数据。这是通过设备计算本地私有模型更新，然后由服务器进行聚合来实现的。然而，计算资源限制和网络通信对于大型深度学习应用中的较大模型大小可能成为严重瓶颈。边缘节点往往具有有限的硬件资源（RAM、CPU），而边缘的网络带宽和可靠性对于扩展联邦车队应用来说是一个问题。在本文中，我们提出并评估了一种受迁移学习启发的FL策略，以减少设备上的资源利用，以及每个全局训练轮次中服务器和网络的负载。对于每个本地模型更新，我们随机选择要训练的层，冻结模型的其余部分。通过这样做，我们可以通过排除所有未训练的部分来减少每轮的服务器负载和通信成本。

    Federated learning (FL) enables edge nodes to collaboratively contribute to constructing a global model without sharing their data. This is accomplished by devices computing local, private model updates that are then aggregated by a server. However, computational resource constraints and network communication can become a severe bottleneck for larger model sizes typical for deep learning applications. Edge nodes tend to have limited hardware resources (RAM, CPU), and the network bandwidth and reliability at the edge is a concern for scaling federated fleet applications. In this paper, we propose and evaluate a FL strategy inspired by transfer learning in order to reduce resource utilization on devices, as well as the load on the server and network in each global training round. For each local model update, we randomly select layers to train, freezing the remaining part of the model. In doing so, we can reduce both server load and communication costs per round by excluding all untrained
    
[^34]: 超出标准模型拟合的可测似然函数

    Testable Likelihoods for Beyond-the-Standard Model Fits. (arXiv:2309.10365v1 [hep-ph])

    [http://arxiv.org/abs/2309.10365](http://arxiv.org/abs/2309.10365)

    使用归一化流构建似然函数，实现从低能量测量到高能量超出标准模型的准确信息传递，并提供额外样本生成和卡方检验统计量，以研究超出标准模型效应。

    

    在精确测量边界上研究潜在的超出标准模型效应，需要将低能量测量的信息准确传递给高能量的超出标准模型。我们提出使用归一化流构建实现这种传递的似然函数。以这种方式构建的似然函数能够生成额外的样本，并允许使用"平凡"的拟合优度检验，即卡方检验统计量。在这里，我们研究了一种特定形式的归一化流，将其应用于多模态和非高斯的例子，并量化了似然函数及其拟合优度检验统计量的准确性。

    Studying potential BSM effects at the precision frontier requires accurate transfer of information from low-energy measurements to high-energy BSM models. We propose to use normalising flows to construct likelihood functions that achieve this transfer. Likelihood functions constructed in this way provide the means to generate additional samples and admit a ``trivial'' goodness-of-fit test in form of a $\chi^2$ test statistic. Here, we study a particular form of normalising flow, apply it to a multi-modal and non-Gaussian example, and quantify the accuracy of the likelihood function and its test statistic.
    
[^35]: 用知识蒸馏和自训练提高CLIP鲁棒性的研究

    Improving CLIP Robustness with Knowledge Distillation and Self-Training. (arXiv:2309.10361v1 [cs.CV])

    [http://arxiv.org/abs/2309.10361](http://arxiv.org/abs/2309.10361)

    本文提出了一种名为LP-CLIP的方法，通过知识蒸馏和自训练来提高CLIP多模态计算机视觉模型的鲁棒性，这种方法不需要注释数据。 LP-CLIP通过在CLIP编码结构顶部添加线性探测层来蒸馏CLIP特征，并使用由CLIP生成的伪标签进行训练，具有增强模型在现实场景中应对各种挑战和不确定性的能力。

    

    本文研究了一种多模态计算机视觉模型CLIP（对比语言-图像预训练）在无监督学习中的鲁棒性。主要目标是评估CLIP的鲁棒性，并探索增强其鲁棒性的策略。为此，我们引入了一种名为LP-CLIP的新方法。该技术通过在其编码结构顶部加入一个线性探测层，将CLIP特征蒸馏出来。这个新增的层使用CLIP生成的伪标签进行训练，并结合自训练策略。LP-CLIP技术提供了一种在不需要注释数据的情况下增强CLIP鲁棒性的有希望方法。通过利用简单的线性探测层，我们旨在提高该模型在现实场景中面临各种不确定性和挑战的能力。重要的是，我们的方法不依赖于注释数据。

    This paper examines the robustness of a multi-modal computer vision model, CLIP (Contrastive Language-Image Pretraining), in the context of unsupervised learning. The main objective is twofold: first, to evaluate the robustness of CLIP, and second, to explore strategies for augmenting its robustness. To achieve this, we introduce a novel approach named LP-CLIP. This technique involves the distillation of CLIP features through the incorporation of a linear probing layer positioned atop its encoding structure. This newly added layer is trained utilizing pseudo-labels produced by CLIP, coupled with a self-training strategy. The LP-CLIP technique offers a promising approach to enhance the robustness of CLIP without the need for annotations. By leveraging a simple linear probing layer, we aim to improve the model's ability to withstand various uncertainties and challenges commonly encountered in real-world scenarios. Importantly, our approach does not rely on annotated data, which makes it 
    
[^36]: 语言引导的对抗净化

    Language Guided Adversarial Purification. (arXiv:2309.10348v1 [cs.LG])

    [http://arxiv.org/abs/2309.10348](http://arxiv.org/abs/2309.10348)

    本文提出了一种语言引导的对抗净化（LGAP）框架，利用预训练的扩散模型和标题生成器来抵御对抗攻击。通过生成图像的标题并通过扩散网络进行引导，该方法可以有效地进行对抗净化，克服了现有方法的局限性。

    

    使用生成模型进行对抗净化展示出了强大的对抗防御性能。这些方法不依赖分类器和攻击手法，使其具有多功能性但通常计算密集。最近在扩散和评分网络方面取得的进展提高了图像生成和对抗净化的性能。另一类高效的对抗防御方法称为对抗训练，需要特定的攻击向量知识，迫使它们在对抗性样本上进行大量训练。为了克服这些局限性，我们提出了一种新的框架，即语言引导的对抗净化（LGAP），利用预训练的扩散模型和标题生成器来抵御对抗攻击。给定一个输入图像，我们的方法首先生成一个标题，然后通过扩散网络来指导对抗净化过程。我们的方法已经针对强大的对抗攻击进行了评估，证明了其有效性。

    Adversarial purification using generative models demonstrates strong adversarial defense performance. These methods are classifier and attack-agnostic, making them versatile but often computationally intensive. Recent strides in diffusion and score networks have improved image generation and, by extension, adversarial purification. Another highly efficient class of adversarial defense methods known as adversarial training requires specific knowledge of attack vectors, forcing them to be trained extensively on adversarial examples. To overcome these limitations, we introduce a new framework, namely Language Guided Adversarial Purification (LGAP), utilizing pre-trained diffusion models and caption generators to defend against adversarial attacks. Given an input image, our method first generates a caption, which is then used to guide the adversarial purification process through a diffusion network. Our approach has been evaluated against strong adversarial attacks, proving its effectivene
    
[^37]: 使用大规模语言模型解释智能体行为

    Explaining Agent Behavior with Large Language Models. (arXiv:2309.10346v1 [cs.LG])

    [http://arxiv.org/abs/2309.10346](http://arxiv.org/abs/2309.10346)

    使用大规模语言模型解释智能体行为的方法，通过学习智能体行为的紧凑表示，并与用户进行交互，能够生成合理的解释，具备与人类专家相似的帮助性。

    

    智能体如机器人越来越多地被部署在真实世界中的安全关键环境中。重要的是，这些智能体能够向人类对等体解释他们决策背后的推理，然而，他们的行为通常是由不可解释的模型（如深度神经网络）产生的。我们提出了一种方法，基于状态和动作的观察，不考虑底层模型表示，生成智能体行为的自然语言解释。我们展示了如何学习智能体行为的简洁表示，并用其生成合理的解释，同时保证了与预训练的大规模语言模型的用户交互。通过用户研究和实证实验，我们证明了我们的方法生成的解释与人类领域专家生成的解释一样有用，同时具备有益的交互，如澄清和反事实查询。

    Intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. It is vital that these agents are able to explain the reasoning behind their decisions to human counterparts, however, their behavior is often produced by uninterpretable models such as deep neural networks. We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, agnostic to the underlying model representation. We show how a compact representation of the agent's behavior can be learned and used to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. Through user studies and empirical experiments, we show that our approach generates explanations as helpful as those generated by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.
    
[^38]: 寻找平衡：用于异构差分隐私数据采集的逻辑回归的最佳机制设计

    Striking a Balance: An Optimal Mechanism Design for Heterogenous Differentially Private Data Acquisition for Logistic Regression. (arXiv:2309.10340v1 [cs.LG])

    [http://arxiv.org/abs/2309.10340](http://arxiv.org/abs/2309.10340)

    本论文研究了在从隐私敏感卖方收集的数据上执行逻辑回归的问题，设计了一个优化测试损失、卖方隐私和支付的加权组合的最佳机制，通过结合博弈论、统计学习理论和差分隐私的思想，解决了买方的目标函数非凸的问题，并提供了当卖方数量变大时的渐近结果。

    

    我们研究了在从隐私敏感卖方收集的数据上执行逻辑回归的问题。由于数据是私有的，卖方必须通过支付来激励他们提供数据。因此，目标是设计一个机制，优化测试损失、卖方隐私和支付的加权组合，即在多个感兴趣的目标之间寻找平衡。我们通过结合博弈论、统计学习理论和差分隐私的思想来解决这个问题。买方的目标函数可能非常非凸。然而，我们证明，在问题参数的某些条件下，可以通过变量的变换将问题凸化。我们还提供了当卖方数量变大时，买方的测试误差和支付的渐近结果。最后，我们通过将这些思想应用于一个真实的医疗数据集来展示我们的想法。

    We investigate the problem of performing logistic regression on data collected from privacy-sensitive sellers. Since the data is private, sellers must be incentivized through payments to provide their data. Thus, the goal is to design a mechanism that optimizes a weighted combination of test loss, seller privacy, and payment, i.e., strikes a balance between multiple objectives of interest. We solve the problem by combining ideas from game theory, statistical learning theory, and differential privacy. The buyer's objective function can be highly non-convex. However, we show that, under certain conditions on the problem parameters, the problem can be convexified by using a change of variables. We also provide asymptotic results characterizing the buyer's test error and payments when the number of sellers becomes large. Finally, we demonstrate our ideas by applying them to a real healthcare data set.
    
[^39]: FedWOA:一种利用鲸鱼优化算法进行可再生能源预测的联邦学习模型

    FedWOA: A Federated Learning Model that uses the Whale Optimization Algorithm for Renewable Energy Prediction. (arXiv:2309.10337v1 [cs.LG])

    [http://arxiv.org/abs/2309.10337](http://arxiv.org/abs/2309.10337)

    本论文提出了一种名为FedWOA的新型联邦学习模型，该模型利用鲸鱼优化算法从家庭能源数据训练的局部LTSM神经网络模型的权重中聚合出全局预测模型，解决了数据异质性和参数数量增加导致的预测精度降低的问题。

    

    隐私在处理敏感个人信息的机器学习模型中非常重要，这些模型需要大规模的数据集进行训练。在能源领域，访问家庭生产者和消费者的能源数据对于能源预测以支持能源网格管理和可再生能源的大规模采用至关重要，然而公民通常不愿意授予基于云的机器学习模型访问权限。联邦学习被提出作为解决隐私挑战的方法，但由于数据异质性、发电模式的变化和参数数量的增加导致全局预测模型的生成存在问题，进而降低了预测精度。本文通过引入FedWOA这种新颖的联邦学习模型来解决这些挑战，该模型利用鲸鱼优化算法从基于家庭能源数据训练的局部LTSM神经网络模型的权重中聚合出全局预测模型。所提出的解决方案能够识别出最优的权重向量。

    Privacy is important when dealing with sensitive personal information in machine learning models, which require large data sets for training. In the energy field, access to household prosumer energy data is crucial for energy predictions to support energy grid management and large-scale adoption of renewables however citizens are often hesitant to grant access to cloud-based machine learning models. Federated learning has been proposed as a solution to privacy challenges however report issues in generating the global prediction model due to data heterogeneity, variations in generation patterns, and the high number of parameters leading to even lower prediction accuracy. This paper addresses these challenges by introducing FedWOA a novel federated learning model that employs the Whale Optimization Algorithm to aggregate global prediction models from the weights of local LTSM neural network models trained on prosumer energy data. The proposed solution identifies the optimal vector of wei
    
[^40]: 应用于应用检索和设计一致性检查的计算方法

    Computational Approaches for App-to-App Retrieval and Design Consistency Check. (arXiv:2309.10328v1 [cs.HC])

    [http://arxiv.org/abs/2309.10328](http://arxiv.org/abs/2309.10328)

    通过使用大规模网络图片训练的视觉模型，零-shot提取UI表示，并使用数学方法实现应用到应用的检索和设计一致性分析，我们提出了一种计算方法以解决现有方法的局限性。

    

    从移动用户界面（UI）中提取语义表示，并将这些表示用于设计师的决策过程，已经显示出作为有效的计算设计支持工具的潜力。目前的方法依赖于在小规模移动UI数据集上训练的机器学习模型来提取语义向量，并使用屏幕截图进行对比来检索给定查询截图的相似UI。然而，这些方法的可用性有限，因为它们通常不是开源的，并且对从业人员来说，训练流程复杂，并且无法进行应用程序到应用程序的检索。为此，我们（1）使用大规模网络图片训练的视觉模型，并测试它们是否能够以零-shot方式提取UI表示，并超越现有的专用模型，以及（2）使用数学方法实现应用到应用的检索和设计一致性分析。我们的实验表明...

    Extracting semantic representations from mobile user interfaces (UI) and using the representations for designers' decision-making processes have shown the potential to be effective computational design support tools. Current approaches rely on machine learning models trained on small-sized mobile UI datasets to extract semantic vectors and use screenshot-to-screenshot comparison to retrieve similar-looking UIs given query screenshots. However, the usability of these methods is limited because they are often not open-sourced and have complex training pipelines for practitioners to follow, and are unable to perform screenshot set-to-set (i.e., app-to-app) retrieval. To this end, we (1) employ visual models trained with large web-scale images and test whether they could extract a UI representation in a zero-shot way and outperform existing specialized models, and (2) use mathematically founded methods to enable app-to-app retrieval and design consistency analysis. Our experiments show tha
    
[^41]: 对多模态大规模语言模型中的灾难性遗忘进行的研究

    Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])

    [http://arxiv.org/abs/2309.10313](http://arxiv.org/abs/2309.10313)

    本论文针对多模态大规模语言模型中的灾难性遗忘问题进行研究，引入了EMT方法来评估灾难性遗忘，并发现在标准图像分类任务上，几乎所有评估的模型都无法保持与视觉编码器相同的性能水平。研究结果表明，早期微调阶段对性能至关重要。

    

    在GPT4的成功之后，多模态大规模语言模型（MLLM）研究引起了广泛关注。这一研究方向侧重于通过微调预训练的LLM和视觉模型来开发通用的LLM。然而，灾难性遗忘，即微调模型无法保持与预训练模型相似的性能水平，仍然是多模态LLM（MLLM）中的一个固有问题。本文介绍了EMT：用于评估MLLM中灾难性遗忘的评估方法，将每个MLLM作为一个图像分类器进行评估。我们首先应用EMT来评估几个开源的微调MLLM，并发现几乎所有评估的MLLM在标准图像分类任务上无法保持与他们的视觉编码器相同的性能水平。此外，我们继续微调LLaVA，一种MLLM，并利用EMT来评估整个微调过程中的性能。有趣的是，我们的结果表明，早期的微调阶段是关键的，过早停止微调可能导致低性能的模型。

    Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
    
[^42]: TensorCodec: 无强数据假设的紧凑有损张量压缩

    TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions. (arXiv:2309.10310v1 [cs.LG])

    [http://arxiv.org/abs/2309.10310](http://arxiv.org/abs/2309.10310)

    TensorCodec是一种紧凑的有损张量压缩算法，可以处理无强数据假设的一般张量。它采用神经张量列车分解、折叠输入张量和重新排序模式索引等关键点来提高压缩效果。

    

    许多现实世界的数据集都是以张量的形式表示的，即多维数值数组。如果不进行压缩，存储这些数据集通常需要大量的空间，而且随着维度的增加呈指数级增长。尽管有许多张量压缩算法可用，但其中许多依赖于关于数据维度、稀疏性、秩和平滑性的强假设。在这项工作中，我们提出了TENSORCODEC，这是一种针对一般张量的有损压缩算法，不需要符合强假设的输入数据。TENSORCODEC包含了三个关键点。第一个关键点是神经张量列车分解（NTTD），我们将循环神经网络集成到张量列车分解中，以增强其表达能力并减轻由低秩假设所带来的限制。另一个关键点是将输入张量折叠成更高阶的张量，以减少NTTD所需的空间。最后，对输入张量进行模式索引的重新排序，以揭示可以被利用的模式。

    Many real-world datasets are represented as tensors, i.e., multi-dimensional arrays of numerical values. Storing them without compression often requires substantial space, which grows exponentially with the order. While many tensor compression algorithms are available, many of them rely on strong data assumptions regarding its order, sparsity, rank, and smoothness. In this work, we propose TENSORCODEC, a lossy compression algorithm for general tensors that do not necessarily adhere to strong input data assumptions. TENSORCODEC incorporates three key ideas. The first idea is Neural Tensor-Train Decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power and alleviate the limitations imposed by the low-rank assumption. Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD. Finally, the mode indices of the input tensor are reordered to reveal patterns that can be explo
    
[^43]: 解耦训练：令人沮丧的简单多领域学习的回归

    Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning. (arXiv:2309.10302v1 [cs.LG])

    [http://arxiv.org/abs/2309.10302](http://arxiv.org/abs/2309.10302)

    这篇论文提出了一种称为解耦训练（D-Train）的令人沮丧的、无超参数的多领域学习方法。该方法采用了一种三阶段的训练策略，首先进行预训练，然后在每个领域上进行后训练，最后进行头部微调，实现解耦训练以获得更好的性能。

    

    多领域学习（MDL）旨在训练一个模型，在多个重叠但非相同的领域中具有最小的平均风险。为了解决数据集偏差和领域优势的挑战，从对齐分布减少领域差距的角度或通过实施领域特定的塔、门甚至专家来保留差异，已经提出了许多MDL方法。MDL模型变得越来越复杂，具有复杂的网络架构或损失函数，引入额外的参数并增加计算成本。在本文中，我们提出了一种令人沮丧的、无超参数的多领域学习方法，命名为解耦训练（D-Train）。D-Train是一种三阶段的从一般到特殊的训练策略，首先在所有领域上进行预训练以热身一个根模型，然后通过将其拆分为多个头部在每个领域上进行后训练，最后通过固定骨干进行头部微调，实现解耦训练以获得更好的性能。

    Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training(D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to ac
    
[^44]: 领域自适应中条件不变组件的突出作用：理论和算法

    Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms. (arXiv:2309.10301v1 [stat.ML])

    [http://arxiv.org/abs/2309.10301](http://arxiv.org/abs/2309.10301)

    该论文研究了领域自适应中条件不变组件的作用，提出了一种基于条件不变惩罚的新算法，该算法在目标风险保证方面具有优势。

    

    领域自适应是一个统计学习问题，当用于训练模型的源数据分布与用于评估模型的目标数据分布不同时出现。虽然许多领域自适应算法已经证明了相当大的实证成功，但是盲目应用这些算法往往会导致在新的数据集上表现更差。为了解决这个问题，重要的是澄清领域自适应算法在具备良好目标性能的假设下。在这项工作中，我们关注在预测中具备条件不变的组件（CICs）的存在假设，这些组件在源数据和目标数据之间保持条件不变。我们证明了CICs，通过条件不变惩罚（CIP）可以估计，具备在领域自适应中提供目标风险保证的三个突出作用。首先，我们提出了一种基于CICs的新算法，即重要性加权的条件不变惩罚（IW-CIP），它在目标风险保证方面超越了简单的方法。

    Domain adaptation (DA) is a statistical learning problem that arises when the distribution of the source data used to train a model differs from that of the target data used to evaluate the model. While many DA algorithms have demonstrated considerable empirical success, blindly applying these algorithms can often lead to worse performance on new datasets. To address this, it is crucial to clarify the assumptions under which a DA algorithm has good target performance. In this work, we focus on the assumption of the presence of conditionally invariant components (CICs), which are relevant for prediction and remain conditionally invariant across the source and target data. We demonstrate that CICs, which can be estimated through conditional invariant penalty (CIP), play three prominent roles in providing target risk guarantees in DA. First, we propose a new algorithm based on CICs, importance-weighted conditional invariant penalty (IW-CIP), which has target risk guarantees beyond simple 
    
[^45]: 使用微调和最小先行搜索来提高Whisper

    Using fine-tuning and min lookahead beam search to improve Whisper. (arXiv:2309.10299v1 [eess.AS])

    [http://arxiv.org/abs/2309.10299](http://arxiv.org/abs/2309.10299)

    使用微调和最小先行搜索算法来改进Whisper，在低资源语言上提高了性能，并且证明了最小先行搜索优于标准束搜索算法。

    

    Whisper在低资源语言上的性能仍然远离完美。除了低资源语言上缺乏训练数据外，我们还发现Whisper中使用的束搜索算法存在一些限制。为了解决这些问题，我们在额外的数据上对Whisper进行微调，并提出了一种改进的解码算法。在越南语上，使用LoRA对Whisper-Tiny进行微调可以将WER的改进提高38.49，相比于全参数微调，进一步减少了1.45。此外，通过使用Filter-Ends和Min Lookahead解码算法，与标准束搜索相比，WER在一系列语言中平均降低了2.26。这些结果可以推广到更大的Whisper模型尺寸。我们还证明了Min Lookahead优于Whisper中使用的标准束搜索算法。

    The performance of Whisper in low-resource languages is still far from perfect. In addition to a lack of training data on low-resource languages, we identify some limitations in the beam search algorithm used in Whisper. To address these issues, we fine-tune Whisper on additional data and propose an improved decoding algorithm. On the Vietnamese language, fine-tuning Whisper-Tiny with LoRA leads to an improvement of 38.49 in WER over the zero-shot Whisper-Tiny setting which is a further reduction of 1.45 compared to full-parameter fine-tuning. Additionally, by using Filter-Ends and Min Lookahead decoding algorithms, the WER reduces by 2.26 on average over a range of languages compared to standard beam search. These results generalise to larger Whisper model sizes. We also prove a theorem that Min Lookahead outperforms the standard beam search algorithm used in Whisper.
    
[^46]: 学习轨道稳定系统以图示教学

    Learning Orbitally Stable Systems for Diagrammatically Teaching. (arXiv:2309.10298v1 [cs.RO])

    [http://arxiv.org/abs/2309.10298](http://arxiv.org/abs/2309.10298)

    本文提出了一种学习轨道稳定系统用于图示教学的框架，通过将已知的轨道渐近稳定系统进行形变，实现机器人跟随用户指定草图进行周期运动的目标。

    

    图示教学是一种机器人获取新技能的范式，用户在场景图像上提供2D草图来指导机器人的运动。本文解决了教导机器人接近表面并在其上进行周期运动的问题，运动的周期可以由用户提供的单个草图在机器人摄像头的图像上任意指定。因此，我们引入了“稳定迪歪形图示教学”（SDDT）框架，将机器人的运动建模为“轨道渐近稳定”（O.A.S.）的动力学系统，学习跟随用户指定的草图。通过应用可微分且可逆的函数来对已知的O.A.S.系统进行形变，从而实现这一目标。参数化的迪正形变在我们建模系统的极限周期和草图之间进行Hausdorff距离优化，产生所需的机器人运动。

    Diagrammatic Teaching is a paradigm for robots to acquire novel skills, whereby the user provides 2D sketches over images of the scene to shape the robot's motion. In this work, we tackle the problem of teaching a robot to approach a surface and then follow cyclic motion on it, where the cycle of the motion can be arbitrarily specified by a single user-provided sketch over an image from the robot's camera. Accordingly, we introduce the \emph{Stable Diffeomorphic Diagrammatic Teaching} (SDDT) framework. SDDT models the robot's motion as an \emph{Orbitally Asymptotically Stable} (O.A.S.) dynamical system that learns to follow the user-specified sketch. This is achieved by applying a \emph{diffeomorphism}, i.e. a differentiable and invertible function, to morph a known O.A.S. system. The parameterised diffeomorphism is then optimised with respect to the Hausdorff distance between the limit cycle of our modelled system and the sketch, to produce the desired robot motion. We provide theoret
    
[^47]: Koopman可逆自编码器：利用正向和反向动力学进行时间建模

    Koopman Invertible Autoencoder: Leveraging Forward and Backward Dynamics for Temporal Modeling. (arXiv:2309.10291v1 [cs.LG])

    [http://arxiv.org/abs/2309.10291](http://arxiv.org/abs/2309.10291)

    Koopman可逆自编码器（KIA）是一种基于Koopman算子理论的机器学习模型，通过建模正向和反向动力学来提高长期预测的准确性。它能够有效地学习低维表示，并保证了正向和逆向操作的可逆性和一致性。

    

    准确的长期预测是许多机器学习应用和决策过程的基础。然而，由于现有的时间模型（如循环神经网络）只能捕捉到训练数据中的统计关系，难以学习目标系统的潜在动力学，因此建立准确的长期预测模型仍然具有挑战性。为了解决这个问题，我们提出了一种基于Koopman算子理论的新型机器学习模型，称为Koopman可逆自编码器（KIA），它在无穷维希尔伯特空间中建模了正向和反向动力学，从而能够有效地学习低维表示，实现对长期系统行为的更准确预测。此外，我们方法的可逆设计保证了正向和逆向操作的可逆性和一致性。我们通过实验表明，KIA在多个时间序列预测任务中具有优异的性能。

    Accurate long-term predictions are the foundations for many machine learning applications and decision-making processes. However, building accurate long-term prediction models remains challenging due to the limitations of existing temporal models like recurrent neural networks (RNNs), as they capture only the statistical connections in the training data and may fail to learn the underlying dynamics of the target system. To tackle this challenge, we propose a novel machine learning model based on Koopman operator theory, which we call Koopman Invertible Autoencoders (KIA), that captures the inherent characteristic of the system by modeling both forward and backward dynamics in the infinite-dimensional Hilbert space. This enables us to efficiently learn low-dimensional representations, resulting in more accurate predictions of long-term system behavior. Moreover, our method's invertibility design guarantees reversibility and consistency in both forward and inverse operations. We illustra
    
[^48]: Flash-LLM: 通过非结构化稀疏性支持，实现经济高效大规模生成模型推断

    Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity. (arXiv:2309.10285v1 [cs.DC])

    [http://arxiv.org/abs/2309.10285](http://arxiv.org/abs/2309.10285)

    Flash-LLM是一种能够低成本高效地进行大规模生成模型推断的方法，通过支持非结构化稀疏性，在高性能但具有高限制性的Tensor Cores上工作。它能够降低GPU内存消耗和计算量，并且保持良好的模型精度。

    

    随着参数规模的快速增长，部署大规模生成模型变得越来越具有挑战性，因为它们通常需要大量的GPU内存消耗和大量的计算。非结构化模型修剪是一种常见的方法，可以降低GPU内存占用和整体计算量，同时保持良好的模型精度。然而，现有的解决方案在处理现代GPU上的非结构化稀疏性方面，并没有提供高效的支持，特别是在高度结构化的张量核心硬件上。因此，我们提出了Flash-LLM，以实现对高性能但具有高限制性的张量核心上的非结构化稀疏性进行低成本和高效的大规模生成模型推断。基于我们的关键观察，生成模型推断的主要瓶颈是若干个skinny矩阵乘法，其中由于计算强度低，Tensor Cores的利用率明显不足。我们提出了一种通用的负载...

    With the fast growth of parameter size, it becomes increasingly challenging to deploy large generative models as they typically require large GPU memory consumption and massive computation. Unstructured model pruning has been a common approach to reduce both GPU memory footprint and the overall computation while retaining good model accuracy. However, the existing solutions do not provide a highly-efficient support for handling unstructured sparsity on modern GPUs, especially on the highly-structured Tensor Core hardware. Therefore, we propose Flash-LLM for enabling low-cost and highly-efficient large generative model inference with the sophisticated support of unstructured sparsity on high-performance but highly restrictive Tensor Cores. Based on our key observation that the main bottleneck of generative model inference is the several skinny matrix multiplications for which Tensor Cores would be significantly under-utilized due to low computational intensity, we propose a general Load
    
[^49]: FRAMU: 基于注意力的联邦强化学习机器遗忘

    FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning. (arXiv:2309.10283v1 [cs.LG])

    [http://arxiv.org/abs/2309.10283](http://arxiv.org/abs/2309.10283)

    FRAMU是一种基于注意力和联邦强化学习的机器遗忘框架，通过自适应学习机制、隐私保护技术和优化策略，处理各种数据源并保持准确性和隐私，适应波动的数据环境，支持持续模型演进。

    

    机器遗忘是一个新兴领域，通过允许从机器学习过程中删除私有或无关数据，解决数据隐私问题。使用过时的、私有的和无关的数据会引发与隐私和模型效率相关的挑战。这些问题不仅影响模型在机器学习和遗忘中的准确性和计算效率，还会对数据隐私造成威胁。为了解决这些挑战，我们引入了一种新颖的框架，即基于注意力的联邦强化学习机器遗忘（FRAMU）。该框架融合了自适应学习机制、隐私保护技术和优化策略，是处理各种数据源（单模态或多模态）同时保持准确性和隐私性的综合解决方案。FRAMU的优势在于其适应波动的数据环境、遗忘过时、私有或无关数据的能力，以及支持模型持续演进的支持。

    Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution
    
[^50]: Crowdotic：基于Transformer的医院候诊室非语音音频与差分隐私的占用预测

    Crowdotic: Transformer-based Occupancy Estimation for Hospital Waiting Rooms with Non-speech Audio and Differential Privacy. (arXiv:2309.10280v1 [cs.SD])

    [http://arxiv.org/abs/2309.10280](http://arxiv.org/abs/2309.10280)

    本研究提出了一种基于非语音音频的人群分析方法，利用Transformer模型实现医院候诊室的占用预测，并且在准确性方面表现出色。这是首次提出使用非语音音频信号进行占用预测的方法，超过了其他基线方法。

    

    隐私保护的人群密度分析在各种场景中应用广泛，在提高智能建筑运营管理的同时，保持了不同空间隐私的期望。我们提出了一种基于非语音音频的人群分析方法，利用了Transformer模型。我们的结果表明，仅通过非语音音频就可以实现这种分析，而且准确性十分出色。据我们所知，这是第一次提出使用非语音音频信号来预测占用情况。截至目前，我们不知道有其他类似的方法。为了实现这一目标，我们在一家大型医院的候诊室中部署了基于传感器的平台，并获得了IRB批准，在数个月的时间里捕获了非语音音频和热像图以用于模型的训练和评估。我们提出的基于非语音的方法在性能上超过了基于热像摄像头的模型和其他所有基线方法。

    Privacy-preserving crowd density analysis finds application across a wide range of scenarios, substantially enhancing smart building operation and management while upholding privacy expectations in various spaces. We propose a non-speech audio-based approach for crowd analytics, leveraging a transformer-based model. Our results demonstrate that non-speech audio alone can be used to conduct such analysis with remarkable accuracy. To the best of our knowledge, this is the first time when non-speech audio signals are proposed for predicting occupancy. As far as we know, there has been no other similar approach of its kind prior to this. To accomplish this, we deployed our sensor-based platform in the waiting room of a large hospital with IRB approval over a period of several months to capture non-speech audio and thermal images for the training and evaluation of our models. The proposed non-speech-based approach outperformed the thermal camera-based model and all other baselines. In addit
    
[^51]: 生成过渡路径的扩散方法

    Diffusion Methods for Generating Transition Paths. (arXiv:2309.10276v1 [physics.comp-ph])

    [http://arxiv.org/abs/2309.10276](http://arxiv.org/abs/2309.10276)

    本文提出了两种新方法来生成高质量的分子系统转变路径：一种是通过偏置原始动力学来促进转变，另一种是将原始转变分解为较小的转变。这些方法在数据丰富和数据稀缺的情况下都展现出了有效性。

    

    在这项工作中，我们致力于使用基于评分的生成模型模拟稀有的亚稳态之间的转变。生成高质量的转变路径的高效方法对于研究分子系统非常有价值，因为数据往往难以获取。我们在本文中开发了两种新颖的路径生成方法：基于链的方法和基于中点的方法。第一种方法通过偏置原始动力学来促进转变，而第二种方法则采用分裂技术并将原始转变分解为较小的转变。在M\"uller势和二肽基丙氨酸的生成转变路径的数值结果中，这些方法在数据丰富和数据稀缺的情况下都展现出了有效性。

    In this work, we seek to simulate rare transitions between metastable states using score-based generative models. An efficient method for generating high-quality transition paths is valuable for the study of molecular systems since data is often difficult to obtain. We develop two novel methods for path generation in this paper: a chain-based approach and a midpoint-based approach. The first biases the original dynamics to facilitate transitions, while the second mirrors splitting techniques and breaks down the original transition into smaller transitions. Numerical results of generated transition paths for the M\"uller potential and for Alanine dipeptide demonstrate the effectiveness of these approaches in both the data-rich and data-scarce regimes.
    
[^52]: 具有增强课程强化学习的众包感知多智能体路径规划研究

    Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])

    [http://arxiv.org/abs/2309.10275](http://arxiv.org/abs/2309.10275)

    该论文介绍了CRAMP，一种基于增强式课程强化学习的众包感知分散式路径规划方法，旨在解决拥挤环境下多智能体路径规划的困难。

    

    在拥挤环境中进行的多智能体路径规划是一个具有挑战性的运动规划问题，旨在为系统中的所有智能体找到无碰撞路径。多智能体路径规划在各个领域中都有广泛的应用，包括空中群体、自动化仓储机器人和自动驾驶车辆。当前的多智能体路径规划方法可以大致分为两种主要类别：集中式规划和分散式规划。集中式规划受到维度灾难的困扰，因此在大型和复杂环境中不具备良好的可扩展性。另一方面，分散式规划使智能体能够在部分可观察环境中进行实时路径规划，展示了隐式的协调能力。然而，在密集环境中它们的收敛速度较慢且性能下降。本文介绍了一种名为CRAMP的众包感知分散式方法，通过增强式课程引导的强化学习来解决这个问题。

    Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
    
[^53]: LLM平台安全：将系统评估框架应用于OpenAI的ChatGPT插件

    LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins. (arXiv:2309.10254v1 [cs.CR])

    [http://arxiv.org/abs/2309.10254](http://arxiv.org/abs/2309.10254)

    本文提出了一个框架，用于分析和改进当前和未来与插件集成的LLM平台的安全性、隐私和安全性。在应用框架于OpenAI的插件生态系统时，我们发现了一些具体证明了潜在问题的插件。

    

    近期，如ChatGPT等大型语言模型（LLM）平台开始提供插件生态系统，以与互联网上的第三方服务进行交互。虽然这些插件扩展了LLM平台的功能，但它们是由任意的第三方开发的，因此不能隐式信任。插件还使用自然语言与LLM平台和用户进行交互，这可能导致模糊的解释。本文提出了一个框架，为LLM平台设计者分析和改进当前和未来与插件集成的LLM平台的安全性、隐私和安全性奠定了基础。我们的框架是一个攻击分类法的表述，通过迭代地探索LLM平台相关方如何利用他们的能力和责任对彼此进行攻击来开发的。作为我们迭代过程的一部分，我们将我们的框架应用于OpenAI的插件生态系统。我们揭示了一些具体证明了潜在问题的插件。

    Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the poten
    
[^54]: 文本到动作生成的最佳自动度量是什么？

    What is the Best Automated Metric for Text to Motion Generation?. (arXiv:2309.10248v1 [cs.CL])

    [http://arxiv.org/abs/2309.10248](http://arxiv.org/abs/2309.10248)

    该论文研究了文本到动作生成任务中与人类评估最吻合的度量，并提出了新的更优度量。结果发现当前用于该任务的度量与人类判断相关性较低，但用于评估平均模型性能的常用度量显示出较强的相关性。

    

    生成基于骨骼的人体动作并解释自然语言描述的兴趣日益增长。尽管大部分工作集中在为此任务开发更好的神经网络架构上，但在确定适当的评估度量方面没有进行重要工作。人类评估是该任务的最终准确性度量标准，自动度量应与人类质量判断相关性强。由于描述与许多动作相兼容，确定正确的度量对于评估和设计有效的生成模型至关重要。本文系统地研究了哪些度量与人类评估最吻合，并提出了与之更加吻合的新度量。我们的研究结果表明，目前用于此任务的度量中没有一个在样本级别上与人类判断有中等相关性。然而，对于评估平均模型性能，常用的度量如R-Precision和较少使用的坐标误差显示出较强的相关性。

    There is growing interest in generating skeleton-based human motions from natural language descriptions. While most efforts have focused on developing better neural architectures for this task, there has been no significant work on determining the proper evaluation metric. Human evaluation is the ultimate accuracy measure for this task, and automated metrics should correlate well with human quality judgments. Since descriptions are compatible with many motions, determining the right metric is critical for evaluating and designing effective generative models. This paper systematically studies which metrics best align with human evaluations and proposes new metrics that align even better. Our findings indicate that none of the metrics currently used for this task show even a moderate correlation with human judgments on a sample level. However, for assessing average model performance, commonly used metrics such as R-Precision and less-used coordinate errors show strong correlations. Addit
    
[^55]: 关于深度生成模型中显式曲率正则化的研究

    On Explicit Curvature Regularization in Deep Generative Models. (arXiv:2309.10237v1 [cs.AI])

    [http://arxiv.org/abs/2309.10237](http://arxiv.org/abs/2309.10237)

    该论文提出了一种基于曲率的正则化方法，用于深度生成模型的学习。在任意数据流形的情况下，推导出了明确的坐标不变公式来计算内在和外在曲率。实验证明，基于曲率的正则化方法优于现有的自动编码器方法，其中内在曲率度量略优于外在曲率度量。

    

    我们提出了一族基于曲率的正则化项，用于深度生成模型的学习。针对嵌入在高维欧几里得空间中的任意数据流形，推导出了在内在和外在曲率度量下的明确的坐标不变公式。由于计算曲率是一个涉及到二阶导数求值的计算密集型过程，我们导出了用于近似评估内在和外在曲率的有效公式。通过比较研究了内在曲率和外在曲率的正则化度量的相对有效性，以及与现有自动编码器训练方法的性能比较。实验证明，基于曲率的方法优于现有的自动编码器正则化方法，且内在曲率度量略优于外在曲率度量。

    We propose a family of curvature-based regularization terms for deep generative model learning. Explicit coordinate-invariant formulas for both intrinsic and extrinsic curvature measures are derived for the case of arbitrary data manifolds embedded in higher-dimensional Euclidean space. Because computing the curvature is a highly computation-intensive process involving the evaluation of second-order derivatives, efficient formulas are derived for approximately evaluating intrinsic and extrinsic curvatures. Comparative studies are conducted that compare the relative efficacy of intrinsic versus extrinsic curvature-based regularization measures, as well as performance comparisons against existing autoencoder training methods. Experiments involving noisy motion capture data confirm that curvature-based methods outperform existing autoencoder regularization methods, with intrinsic curvature measures slightly more effective than extrinsic curvature measures.
    
[^56]: 多层次气候模式参数化提高泛化和外推能力

    Multi-fidelity climate model parameterization for better generalization and extrapolation. (arXiv:2309.10231v1 [cs.LG])

    [http://arxiv.org/abs/2309.10231](http://arxiv.org/abs/2309.10231)

    多层次气候模式参数化方法整合了不同准确性和丰富度的数据集，解决了机器学习参数化方法泛化和外推能力不足的问题，为气候变化的预测提供更准确的结果。

    

    最近提出了基于机器学习的气候模型参数化方法，用于表示全球气候模型或湍流模拟中的子网格过程，这是一种强大的替代方案；相比于基于经验公式的传统方法，它具有更低的计算成本和更高的精度。然而，这些方法仍然存在泛化和外推能力不足的问题，而这对于气候变化的预测以及湍流的未观察到的态势是至关重要的。在这里，我们展示了一种多层次方法，该方法整合了不同准确性和丰富度的数据集，可以在两种方法之间提供最佳的结果：基于物理的参数化方法具备外推能力，并且基于机器学习的参数化方法具有更高的精度。在应用于气候建模的实验证明中，多层次方法能够提供更准确的气候预测，而无需大幅增加计算资源。

    Machine-learning-based parameterizations (i.e. representation of sub-grid processes) of global climate models or turbulent simulations have recently been proposed as a powerful alternative to physical, but empirical, representations, offering a lower computational cost and higher accuracy. Yet, those approaches still suffer from a lack of generalization and extrapolation beyond the training data, which is however critical to projecting climate change or unobserved regimes of turbulence. Here we show that a multi-fidelity approach, which integrates datasets of different accuracy and abundance, can provide the best of both worlds: the capacity to extrapolate leveraging the physically-based parameterization and a higher accuracy using the machine-learning-based parameterizations. In an application to climate modeling, the multi-fidelity framework yields more accurate climate projections without requiring major increase in computational resources. Our multi-fidelity randomized prior networ
    
[^57]: 因果理论和结构化数据表示的改进效果研究

    Causal Theories and Structural Data Representations for Improving Out-of-Distribution Classification. (arXiv:2309.10211v1 [cs.LG])

    [http://arxiv.org/abs/2309.10211](http://arxiv.org/abs/2309.10211)

    使用人生成的因果知识来改进数据表示可以提高神经网络在复杂分类任务中的性能，指示了改进机器学习系统开发实践的重要性。

    

    我们考虑如何使用以人为中心的因果理论和来自动力系统文献的工具，来引导神经网络在复杂分类任务中训练数据的表示。具体而言，我们使用模拟数据来展示，使用将数据生成过程的不变结构因果特征明确显示在数据表示中的神经网络训练，相比于更为天真的数据表示方法，能够提高在分类任务中的超出分布（OOD）泛化性能。我们认为这些结果表明，利用人生成的因果知识来减少机器学习开发者的认识不确定性，可以导致更加明确规范的机器学习流程。这进而指示了通过改进机器学习系统开发实践来提高机器学习系统的鲁棒性和安全性的更广泛努力中的动力系统方法的实用性。

    We consider how human-centered causal theories and tools from the dynamical systems literature can be deployed to guide the representation of data when training neural networks for complex classification tasks. Specifically, we use simulated data to show that training a neural network with a data representation that makes explicit the invariant structural causal features of the data generating process of an epidemic system improves out-of-distribution (OOD) generalization performance on a classification task as compared to a more naive approach to data representation. We take these results to demonstrate that using human-generated causal knowledge to reduce the epistemic uncertainty of ML developers can lead to more well-specified ML pipelines. This, in turn, points to the utility of a dynamical systems approach to the broader effort aimed at improving the robustness and safety of machine learning systems via improved ML system development practices.
    
[^58]: 核密度积分转换

    The Kernel Density Integral Transformation. (arXiv:2309.10194v1 [stat.ML])

    [http://arxiv.org/abs/2309.10194](http://arxiv.org/abs/2309.10194)

    本文提出了一种使用核密度积分转换作为特征预处理步骤的方法，可以替代线性最小最大缩放和分位数转换，并通过调整超参数优化性能。这种方法在统计数据分析中具有应用价值。

    

    在应用机器学习和统计方法于表格数据时，特征预处理继续发挥关键作用。在本文中，我们提出了使用核密度积分转换作为特征预处理步骤的方法。我们的方法综合了两种主要的特征预处理方法作为极限情况：线性最小最大缩放和分位数转换。我们证明了，在不调整超参数的情况下，核密度积分转换可以作为这两种方法的简单替代方法，对每种方法的弱点具有鲁棒性。另外，通过调整一个连续超参数，我们经常优于这两种方法。最后，我们表明核密度转换可以有益地应用于统计数据分析，特别是在相关性分析和单变量聚类上。

    Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.
    
[^59]: 随机深度库普曼模型用于多阶段制造系统中的质量传播分析

    Stochastic Deep Koopman Model for Quality Propagation Analysis in Multistage Manufacturing Systems. (arXiv:2309.10193v1 [cs.LG])

    [http://arxiv.org/abs/2309.10193](http://arxiv.org/abs/2309.10193)

    本研究引入了一种随机深度库普曼（SDK）框架来建模多阶段制造系统（MMS）的复杂行为，并通过传播关键质量信息，提高了数据驱动模型的可解释性和准确性。

    

    多阶段制造系统（MMS）的建模引起了学术界和工业界的更多关注。最近深度学习方法的进展为以较低成本和专业知识完成这个任务提供了机会。本研究引入了一种随机深度库普曼（SDK）框架来建模MMS的复杂行为。具体而言，我们通过库普曼算子在变分自编码器提取的关键质量信息的传播上，提出了一种新颖的应用。通过这个框架，我们能够使用转换的线性表示有效地捕捉产品质量的一般非线性演变，从而增强基于数据的模型的可解释性。为了评估SDK框架的性能，我们在一个开源数据集上进行了比较研究。本文的主要发现如下。我们的结果表明，在预测分阶段产品质量方面，SDK的准确性超过了其他流行的数据驱动模型。

    The modeling of multistage manufacturing systems (MMSs) has attracted increased attention from both academia and industry. Recent advancements in deep learning methods provide an opportunity to accomplish this task with reduced cost and expertise. This study introduces a stochastic deep Koopman (SDK) framework to model the complex behavior of MMSs. Specifically, we present a novel application of Koopman operators to propagate critical quality information extracted by variational autoencoders. Through this framework, we can effectively capture the general nonlinear evolution of product quality using a transferred linear representation, thus enhancing the interpretability of the data-driven model. To evaluate the performance of the SDK framework, we carried out a comparative study on an open-source dataset. The main findings of this paper are as follows. Our results indicate that SDK surpasses other popular data-driven models in accuracy when predicting stagewise product quality within t
    
[^60]: 基于图增强的自适应智能时间序列预测强化学习

    Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence. (arXiv:2309.10186v1 [cs.LG])

    [http://arxiv.org/abs/2309.10186](http://arxiv.org/abs/2309.10186)

    本研究提出了一种使用图神经网络和强化学习监控的新方法，以更准确地预测时间序列数据。该方法能够克服深度学习的限制，并捕捉复杂的时序结构中的时间依赖关系，适用于医疗、交通和天气预测等领域。

    

    强化学习以其能够自适应地建模序列任务和学习潜在数据模式的能力而闻名。深度学习模型在回归和分类任务中得到了广泛的探索和应用。然而，深度学习存在一些限制，例如假设数据等间隔有序以及无法充分融入图结构等。图神经网络(GNN)能够克服这些挑战并捕捉时间序列数据的时间依赖关系。 在本研究中，我们提出了一种使用GNN和强化学习监控预测时间序列数据的新方法。GNN能够显式地将数据的图结构纳入模型，使其能够以更自然的方式捕捉时间依赖关系。该方法能够在复杂的时序结构中进行更准确的预测，例如医疗、交通和天气预测中的时序数据。

    Reinforcement learning is well known for its ability to model sequential tasks and learn latent data patterns adaptively. Deep learning models have been widely explored and adopted in regression and classification tasks. However, deep learning has its limitations such as the assumption of equally spaced and ordered data, and the lack of ability to incorporate graph structure in terms of time-series prediction. Graphical neural network (GNN) has the ability to overcome these challenges and capture the temporal dependencies in time-series data. In this study, we propose a novel approach for predicting time-series data using GNN and monitoring with Reinforcement Learning (RL). GNNs are able to explicitly incorporate the graph structure of the data into the model, allowing them to capture temporal dependencies in a more natural way. This approach allows for more accurate predictions in complex temporal structures, such as those found in healthcare, traffic and weather forecasting. We also 
    
[^61]: 基于云-网络集成的超越5G中的QoS感知服务预测与编排

    QoS-Aware Service Prediction and Orchestration in Cloud-Network Integrated Beyond 5G. (arXiv:2309.10185v1 [cs.NI])

    [http://arxiv.org/abs/2309.10185](http://arxiv.org/abs/2309.10185)

    本论文提出了一个针对网络-云集成环境中的服务部署和资源分配的非线性规划模型，考虑了容量限制、动态用户和端到端延迟，并解决了超越5G中的服务连续性问题。

    

    诸如元宇宙等新型应用突显了超越5G网络的潜力，这需要超低延迟通信和大规模的宽带连接。此外，伴随着用户数量不断波动的这类服务的蓬勃需求，使得在B5G中需要更加关注服务连续性。为了实现这些服务，边缘云模式是利用云容量并实时有效地管理用户在网络中移动的潜在解决方案。然而，边缘云网络面临着诸多限制，包括必须集体管理网络和计算资源以释放其全部潜力。本文在考虑容量限制、动态用户和端到端延迟的情况下，解决了网络-云集成环境中服务部署和资源分配的联合问题。我们提出了一个非线性规划模型，该模型制定了优化问题的目标。

    Novel applications such as the Metaverse have highlighted the potential of beyond 5G networks, which necessitate ultra-low latency communications and massive broadband connections. Moreover, the burgeoning demand for such services with ever-fluctuating users has engendered a need for heightened service continuity consideration in B5G. To enable these services, the edge-cloud paradigm is a potential solution to harness cloud capacity and effectively manage users in real time as they move across the network. However, edge-cloud networks confront a multitude of limitations, including networking and computing resources that must be collectively managed to unlock their full potential. This paper addresses the joint problem of service placement and resource allocation in a network-cloud integrated environment while considering capacity constraints, dynamic users, and end-to-end delays. We present a non-linear programming model that formulates the optimization problem with the aiming objectiv
    
[^62]: 基于双深度Q学习的低延迟5G应用路径选择和服务部署

    Double Deep Q-Learning-based Path Selection and Service Placement for Latency-Sensitive Beyond 5G Applications. (arXiv:2309.10180v1 [cs.NI])

    [http://arxiv.org/abs/2309.10180](http://arxiv.org/abs/2309.10180)

    通过研究通信和计算资源分配的联合问题，本文提出了两种方法，即B&B-CCRA和WF-CCRA，以最小化总成本。

    

    如今，随着对容量的需求不断增长，全新的服务正在涌现。为了以实时响应和可扩展的方式提供这些服务，需要一个坚实的云网络集成基础设施。由于通信和计算资源的多样性和有限容量，必须协同管理这些资源以发挥它们的全部潜力。尽管已经提出了一些创新方法来编排这些资源，但大多数方法都忽略了网络资源或将网络简化为简单的图，只关注云资源。本文通过研究通信和计算资源分配的联合问题，称为CCRA，包括功能部署和分配、流量优先级和路径选择，考虑容量限制和质量要求，以最小化总成本。我们将问题形式化为非线性规划模型，并提出了两种方法，即B&B-CCRA和WF-CCRA，基于Br...

    Nowadays, as the need for capacity continues to grow, entirely novel services are emerging. A solid cloud-network integrated infrastructure is necessary to supply these services in a real-time responsive, and scalable way. Due to their diverse characteristics and limited capacity, communication and computing resources must be collaboratively managed to unleash their full potential. Although several innovative methods have been proposed to orchestrate the resources, most ignored network resources or relaxed the network as a simple graph, focusing only on cloud resources. This paper fills the gap by studying the joint problem of communication and computing resource allocation, dubbed CCRA, including function placement and assignment, traffic prioritization, and path selection considering capacity constraints and quality requirements, to minimize total cost. We formulate the problem as a non-linear programming model and propose two approaches, dubbed B\&B-CCRA and WF-CCRA, based on the Br
    
[^63]: 自我维持的连续深度强化学习在动态元宇宙应用中的多重接入

    Self-Sustaining Multiple Access with Continual Deep Reinforcement Learning for Dynamic Metaverse Applications. (arXiv:2309.10177v1 [cs.NI])

    [http://arxiv.org/abs/2309.10177](http://arxiv.org/abs/2309.10177)

    本文研究了动态元宇宙应用中的多重接入问题，在采用自适应人工智能和深度强化学习的基础上，通过连续训练来实现自我维持的策略。该研究填补了当前文献中对于适应非稳态环境的代理机制问题的研究空白。

    

    元宇宙是一个旨在创建由众多世界组成的虚拟环境的新范 Paradigm，其中每个世界将提供不同的服务。为了应对如此动态和复杂的场景，并考虑到针对第六代通信系统（6G）的严格服务质量要求，一种潜在的方法是采用自我维持策略，通过使用自适应人工智能（自适应AI）来实现，其中模型将不断地根据新的数据和条件进行重新训练。自我维持的一个方面是对频谱的多重接入进行管理。虽然已经提出了几种创新的方法来解决这个挑战，大多数方法使用深度强化学习（DRL），但是适应非稳态环境的代理机制问题尚未得到准确解决。本文通过研究多通道环境中的多重接入问题，填补了当前文献中的空白。

    The Metaverse is a new paradigm that aims to create a virtual environment consisting of numerous worlds, each of which will offer a different set of services. To deal with such a dynamic and complex scenario, considering the stringent quality of service requirements aimed at the 6th generation of communication systems (6G), one potential approach is to adopt self-sustaining strategies, which can be realized by employing Adaptive Artificial Intelligence (Adaptive AI) where models are continually re-trained with new data and conditions. One aspect of self-sustainability is the management of multiple access to the frequency spectrum. Although several innovative methods have been proposed to address this challenge, mostly using Deep Reinforcement Learning (DRL), the problem of adapting agents to a non-stationary environment has not yet been precisely addressed. This paper fills in the gap in the current literature by investigating the problem of multiple access in multi-channel environment
    
[^64]: 一次性演示行为克隆与动作分块转换的一人剧本

    One ACT Play: Single Demonstration Behavior Cloning with Action Chunking Transformers. (arXiv:2309.10175v1 [cs.RO])

    [http://arxiv.org/abs/2309.10175](http://arxiv.org/abs/2309.10175)

    本研究提出了一种通过行为克隆学习任务的方法，只需要一次人类演示，并使用线性变换生成多样化的轨迹，成功完成了三个块操作任务。此外，还提出了一种新的暂存方法，用于动作块代理的行动预测。

    

    从人类示范中学习（行为克隆）是机器人学习的基石。然而，大多数行为克隆算法需要大量示范来学习一个任务，尤其是对于具有多样化初始条件的一般任务。然而，人类只需看到一两个示范就能学会完成任务，即使是复杂的任务。我们的工作旨在模拟这种能力，仅凭一次人类演示就使用行为克隆学习一个任务。我们通过使用线性变换来增强单个演示，生成一系列适用于各种初始条件的轨迹，实现了这个目标。通过这些示范，我们能够训练一个行为克隆代理成功完成三个块操作任务。此外，在推理过程中，我们还开发了一种新的暂存方法，用于动作块代理的行动预测中综合标准差。

    Learning from human demonstrations (behavior cloning) is a cornerstone of robot learning. However, most behavior cloning algorithms require a large number of demonstrations to learn a task, especially for general tasks that have a large variety of initial conditions. Humans, however, can learn to complete tasks, even complex ones, after only seeing one or two demonstrations. Our work seeks to emulate this ability, using behavior cloning to learn a task given only a single human demonstration. We achieve this goal by using linear transforms to augment the single demonstration, generating a set of trajectories for a wide range of initial conditions. With these demonstrations, we are able to train a behavior cloning agent to successfully complete three block manipulation tasks. Additionally, we developed a novel addition to the temporal ensembling method used by action chunking agents during inference. By incorporating the standard deviation of the action predictions into the ensembling m
    
[^65]: 基于自编码器的CMS电磁量能器在线数据质量监测的异常检测系统

    Autoencoder-based Anomaly Detection System for Online Data Quality Monitoring of the CMS Electromagnetic Calorimeter. (arXiv:2309.10157v1 [physics.ins-det])

    [http://arxiv.org/abs/2309.10157](http://arxiv.org/abs/2309.10157)

    本文介绍了一种基于自编码器的实时异常检测系统，通过半监督机器学习方法检测CMS电磁量能器数据中的异常，利用异常的时域演化和探测器响应的空间变化，最大化异常检测性能，并通过验证实验证明了系统的有效性。

    

    CMS探测器是在LHC上探测高能碰撞产生的通用装置。CMS电磁量能器在线数据质量监测是一个重要的操作工具，能够让探测器专家快速识别、定位和诊断可能影响物理数据质量的各种探测器问题。本文提出了一种基于半监督机器学习的实时自编码器异常检测系统，能够检测CMS电磁量能器数据中的异常。介绍了一种新颖的方法，通过利用异常的时域演化和探测器响应的空间变化，最大化了异常检测性能。这个基于自编码器的系统能够高效地检测异常，同时保持非常低的误报率。该系统的性能通过在2018年和2022年LHC碰撞数据中发现的异常进行验证。此外，还首次报道了部署该系统后的初步结果。

    The CMS detector is a general-purpose apparatus that detects high-energy collisions produced at the LHC. Online Data Quality Monitoring of the CMS electromagnetic calorimeter is a vital operational tool that allows detector experts to quickly identify, localize, and diagnose a broad range of detector issues that could affect the quality of physics data. A real-time autoencoder-based anomaly detection system using semi-supervised machine learning is presented enabling the detection of anomalies in the CMS electromagnetic calorimeter data. A novel method is introduced which maximizes the anomaly detection performance by exploiting the time-dependent evolution of anomalies as well as spatial variations in the detector response. The autoencoder-based system is able to efficiently detect anomalies, while maintaining a very low false discovery rate. The performance of the system is validated with anomalies found in 2018 and 2022 LHC collision data. Additionally, the first results from deploy
    
[^66]: 保持肿瘤体积的无监督医学图像配准

    Preserving Tumor Volumes for Unsupervised Medical Image Registration. (arXiv:2309.10153v1 [eess.IV])

    [http://arxiv.org/abs/2309.10153](http://arxiv.org/abs/2309.10153)

    该论文提出了一种保持肿瘤体积的无监督医学图像配准方法，通过两个阶段的过程来解决肿瘤区域体积变化失衡的问题。

    

    医学图像配准是估计图像间空间对应关系的关键任务。然而，目前传统和基于深度学习的方法都依赖相似度度量来生成变形场，这往往导致不相似区域的体积变化失衡，特别是在肿瘤区域。这些改变会显著改变肿瘤大小和底层解剖结构，限制了图像配准在临床诊断中的实际应用。为了解决这个问题，我们将以肿瘤为约束条件的图像配准问题。我们提出的策略包括两个阶段的过程。在第一个阶段，我们使用基于相似度的配准来通过体积变化识别潜在的肿瘤区域，生成相应的软肿瘤掩膜。在第二阶段，我们提出一种保持体积的配准方法，采用一种新颖的自适应体积保持损失函数。

    Medical image registration is a critical task that estimates the spatial correspondence between pairs of images. However, current traditional and deep-learning-based methods rely on similarity measures to generate a deforming field, which often results in disproportionate volume changes in dissimilar regions, especially in tumor regions. These changes can significantly alter the tumor size and underlying anatomy, which limits the practical use of image registration in clinical diagnosis. To address this issue, we have formulated image registration with tumors as a constraint problem that preserves tumor volumes while maximizing image similarity in other normal regions. Our proposed strategy involves a two-stage process. In the first stage, we use similarity-based registration to identify potential tumor regions by their volume change, generating a soft tumor mask accordingly. In the second stage, we propose a volume-preserving registration with a novel adaptive volume-preserving loss t
    
[^67]: Primal-Dual $\ell_0$-约束稀疏指数跟踪

    Primal-Dual $\ell_0$-Constrained Sparse Index Tracking. (arXiv:2309.10152v1 [q-fin.PM])

    [http://arxiv.org/abs/2309.10152](http://arxiv.org/abs/2309.10152)

    本文提出了一种新的稀疏指数跟踪问题的形式化，使用了$\ell_0$-范数约束，可以轻松控制投资组合中资产数量的上限。

    

    稀疏指数跟踪是一种重要的被动投资组合管理策略，它通过构建稀疏投资组合来跟踪金融指数。相比于全仓投资组合，稀疏投资组合在降低交易成本和避免不流动资产方面更具优势。为了强制投资组合的稀疏性，传统研究提出了基于$\ell_p$-范数正则化的公式，作为$\ell_0$-范数正则化的连续替代。尽管这样的公式可以用来构建稀疏投资组合，但在实际投资中却不易使用，因为细致的参数调整来指定投资组合中资产数量的上限是艰难且耗时的。本文提出了一种新的稀疏指数跟踪问题的形式化，使用了$\ell_0$-范数约束，从而可以轻松控制投资组合中资产数量的上限。此外，我们的形式化允许在投资组合稀疏性和换手率之间进行选择。

    Sparse index tracking is one of the prominent passive portfolio management strategies that construct a sparse portfolio to track a financial index. A sparse portfolio is desirable over a full portfolio in terms of transaction cost reduction and avoiding illiquid assets. To enforce the sparsity of the portfolio, conventional studies have proposed formulations based on $\ell_p$-norm regularizations as a continuous surrogate of the $\ell_0$-norm regularization. Although such formulations can be used to construct sparse portfolios, they are not easy to use in actual investments because parameter tuning to specify the exact upper bound on the number of assets in the portfolio is delicate and time-consuming. In this paper, we propose a new problem formulation of sparse index tracking using an $\ell_0$-norm constraint that enables easy control of the upper bound on the number of assets in the portfolio. In addition, our formulation allows the choice between portfolio sparsity and turnover spa
    
[^68]: Q-Transformer：通过自回归Q-函数提供可扩展的离线强化学习

    Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions. (arXiv:2309.10150v1 [cs.RO])

    [http://arxiv.org/abs/2309.10150](http://arxiv.org/abs/2309.10150)

    Q-Transformer是一种可扩展的离线强化学习方法，通过使用Transformer来表示Q函数并利用离线数据集进行训练。它在大规模真实世界机器人操作任务中表现优越。

    

    在这项工作中，我们提出了一种可扩展的强化学习方法，用于训练可以利用人类演示和自主采集数据的大型离线数据集的多任务策略。我们的方法使用Transformer提供可扩展的Q函数表示，通过离线时差备份进行训练。因此，我们将该方法称为Q-Transformer。通过将每个动作维度进行离散化，并将每个动作维度的Q值表示为单独的标记，我们可以应用高容量序列建模技术进行Q学习。我们提出了几个设计决策，使其在离线RL训练中表现出良好性能，并展示了Q-Transformer在大规模多样化的真实世界机器人操作任务套件上优于以往的离线RL算法和模仿学习技术。该项目的网站和视频可以在https://q-transformer.github.io找到。

    In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://q-transformer.github.io
    
[^69]: AI代理的记忆和泛化能力分析：连续学习者是否具有鲁棒性？

    Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?. (arXiv:2309.10149v1 [cs.LG])

    [http://arxiv.org/abs/2309.10149](http://arxiv.org/abs/2309.10149)

    本文提出了一种新的连续学习框架，旨在在动态环境下实现鲁棒的泛化能力并保留过去的知识。通过使用有限容量的内存来保存先前观察到的环境信息，并从内存中采样数据点来获得对未知变化鲁棒的预测器。该分析展示了记忆和泛化之间的权衡，而实验证明了所提出的算法的优越性。

    

    在连续学习中，AI代理（例如自动驾驶车辆或机器人）在动态环境下从非稳态数据流中学习。对于这类应用的实际部署，保证对未知环境的鲁棒性以及保留过去的经验是非常重要的。本文提出了一种新的连续学习框架，旨在在动态环境下实现鲁棒的泛化能力并保留过去的知识。考虑到连续学习代理使用有限容量的内存来保存先前观察到的环境信息，以减轻遗忘问题。然后，从内存中采样数据点，以估计环境变化的风险分布，从而得到对未知变化具有鲁棒性的预测器。对所提出的框架的泛化和记忆性能进行了理论分析。该分析展示了随内存大小的记忆和泛化之间的权衡。实验证明了所提出的算法的优越性。

    In continual learning (CL), an AI agent (e.g., autonomous vehicles or robotics) learns from non-stationary data streams under dynamic environments. For the practical deployment of such applications, it is important to guarantee robustness to unseen environments while maintaining past experiences. In this paper, a novel CL framework is proposed to achieve robust generalization to dynamic environments while retaining past knowledge. The considered CL agent uses a capacity-limited memory to save previously observed environmental information to mitigate forgetting issues. Then, data points are sampled from the memory to estimate the distribution of risks over environmental change so as to obtain predictors that are robust with unseen changes. The generalization and memorization performance of the proposed framework are theoretically analyzed. This analysis showcases the tradeoff between memorization and generalization with the memory size. Experiments show that the proposed algorithm outpe
    
[^70]: 通过增加网络跟踪实现逼真的网站指纹识别

    Realistic Website Fingerprinting By Augmenting Network Trace. (arXiv:2309.10147v1 [cs.CR])

    [http://arxiv.org/abs/2309.10147](http://arxiv.org/abs/2309.10147)

    通过增加网络跟踪，我们提出了一种适用于Tor跟踪的增强技术NetAugment，可以增强网站指纹识别器在未观察到的网络条件下的性能。

    

    网站指纹识别（WF）被认为是对Tor用户（以及其他匿名系统）匿名性的主要威胁。尽管最先进的WF技术声称具有较高的攻击准确率，例如通过利用深度神经网络（DNN），但最近的几项研究对这些攻击的设计和评估中所做假设的实用性提出了质疑。在这项工作中，我们认为这些不切实际的问题主要是由于攻击者无法在全面的网络条件下收集训练数据引起的，例如，WF分类器可能只在特定高带宽网络链路上收集样本进行训练，但在具有不同网络条件的连接上部署。我们展示了通过增加网络跟踪可以提高在未观察到的网络条件下WF分类器的性能。具体而言，我们介绍了NetAugment，一种适用于Tor跟踪规范的增强技术。我们通过...

    Website Fingerprinting (WF) is considered a major threat to the anonymity of Tor users (and other anonymity systems). While state-of-the-art WF techniques have claimed high attack accuracies, e.g., by leveraging Deep Neural Networks (DNN), several recent works have questioned the practicality of such WF attacks in the real world due to the assumptions made in the design and evaluation of these attacks. In this work, we argue that such impracticality issues are mainly due to the attacker's inability in collecting training data in comprehensive network conditions, e.g., a WF classifier may be trained only on samples collected on specific high-bandwidth network links but deployed on connections with different network conditions. We show that augmenting network traces can enhance the performance of WF classifiers in unobserved network conditions. Specifically, we introduce NetAugment, an augmentation technique tailored to the specifications of Tor traces. We instantiate NetAugment through 
    
[^71]: 一个基于神经特征学习的几何框架

    A Geometric Framework for Neural Feature Learning. (arXiv:2309.10140v1 [cs.LG])

    [http://arxiv.org/abs/2309.10140](http://arxiv.org/abs/2309.10140)

    本论文提出了一个基于神经特征学习的几何框架，在特征空间中利用几何结构解决学习问题。通过引入特征几何，将统计依赖和特征统一到同一空间中，并使用嵌套技术设计学习算法，展示了其在多变量学习问题中的应用。

    

    我们提出了一个基于神经特征提取器的学习系统设计的新框架，通过利用特征空间中的几何结构。首先，我们引入了特征几何，它将统计依赖和特征统一到同一个具有几何结构的函数空间中。通过应用特征几何，我们将每个学习问题形式化为解决由学习设置指定的依赖组件的最佳特征近似解。我们提出了一种嵌套技术来设计学习算法，从数据样本中学习最佳特征，这可以应用于现有的网络架构和优化器。为了展示嵌套技术的应用，我们进一步讨论了多变量学习问题，包括条件推理和多模态学习，在这些问题中，我们提出了最佳特征并揭示了它们与经典方法的联系。

    We present a novel framework for learning system design based on neural feature extractors by exploiting geometric structures in feature spaces. First, we introduce the feature geometry, which unifies statistical dependence and features in the same functional space with geometric structures. By applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. We propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. To demonstrate the application of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.
    
[^72]: 高效的低秩图神经网络抵御结构攻击

    Efficient Low-Rank GNN Defense Against Structural Attacks. (arXiv:2309.10136v1 [cs.LG])

    [http://arxiv.org/abs/2309.10136](http://arxiv.org/abs/2309.10136)

    提出了一种高效的低秩图神经网络（ELR-GNN）防御方法，通过学习低秩和稀疏的图结构抵御对抗攻击，并在更高的效率下实现有效的防御。

    

    图神经网络（GNNs）在图数据上展现出强大的表示能力。然而，GNNs容易受到对抗攻击的影响，即使对图结构进行微小扰动也会严重影响它们的性能。现有方法要么对复杂攻击无效，要么需要优化稠密邻接矩阵，这非常耗时且容易陷入局部最优。为了解决这个问题，我们提出了一种高效的低秩图神经网络（ELR-GNN）防御方法，旨在学习低秩和稀疏的图结构来抵御对抗攻击，保证有效的防御与更高的效率。具体而言，ELR-GNN包括两个模块：粗糙低秩估计模块和细粒度估计模块。第一个模块采用截断奇异值分解（SVD）来初始化低秩邻接矩阵的估计，这作为优化低秩邻接矩阵的起点。

    Graph Neural Networks (GNNs) have been shown to possess strong representation abilities over graph data. However, GNNs are vulnerable to adversarial attacks, and even minor perturbations to the graph structure can significantly degrade their performance. Existing methods either are ineffective against sophisticated attacks or require the optimization of dense adjacency matrices, which is time-consuming and prone to local minima. To remedy this problem, we propose an Efficient Low-Rank Graph Neural Network (ELR-GNN) defense method, which aims to learn low-rank and sparse graph structures for defending against adversarial attacks, ensuring effective defense with greater efficiency. Specifically, ELR-GNN consists of two modules: a Coarse Low-Rank Estimation Module and a Fine-Grained Estimation Module. The first module adopts the truncated Singular Value Decomposition (SVD) to initialize the low-rank adjacency matrix estimation, which serves as a starting point for optimizing the low-rank 
    
[^73]: GDM: 双重Mixup用于有限标注的图分类

    GDM: Dual Mixup for Graph Classification with Limited Supervision. (arXiv:2309.10134v1 [cs.LG])

    [http://arxiv.org/abs/2309.10134](http://arxiv.org/abs/2309.10134)

    GDM是一种双重的mixup方法，利用图实例的功能和结构信息生成新的标记图样本。

    

    图神经网络(GNNs)对于图分类任务需要大量的标记图样本以获得良好的性能。随着标记图样本数量的减少，GNNs的性能显著下降。为了降低标注成本，开发能够生成新的图实例以增加可用标记图样本数量和多样性的图增强方法非常重要。在这项工作中，我们提出了一种新颖的基于mixup的图增强方法，称为Graph Dual Mixup (GDM)，它利用图实例的功能和结构信息来生成新的标记图样本。GDM使用图结构自动编码器来学习图样本的结构嵌入，然后在学到的结构嵌入空间中应用mixup到图的结构信息，并从mixup结构嵌入中生成新的图结构。至于功能信息，GDM应用...

    Graph Neural Networks (GNNs) require a large number of labeled graph samples to obtain good performance on the graph classification task. The performance of GNNs degrades significantly as the number of labeled graph samples decreases. To reduce the annotation cost, it is therefore important to develop graph augmentation methods that can generate new graph instances to increase the size and diversity of the limited set of available labeled graph samples. In this work, we propose a novel mixup-based graph augmentation method, Graph Dual Mixup (GDM), that leverages both functional and structural information of the graph instances to generate new labeled graph samples. GDM employs a graph structural auto-encoder to learn structural embeddings of the graph samples, and then applies mixup to the structural information of the graphs in the learned structural embedding space and generates new graph structures from the mixup structural embeddings. As for the functional information, GDM applies 
    
[^74]: 图形变换器的深度指导调优

    Deep Prompt Tuning for Graph Transformers. (arXiv:2309.10131v1 [cs.LG])

    [http://arxiv.org/abs/2309.10131](http://arxiv.org/abs/2309.10131)

    提出了一种称为深度图形指导调优的替代fine-tuning的方法，通过引入可训练的特征节点和任务特定的令牌，来增强图形变换器模型在下游图形预测任务中的表达能力，同时减少了自由参数的数量和模型副本的需求，适用于小型数据集。

    

    图形变换器通过解决传统图神经网络面临的挑战，在各种基于图形的任务中变得越来越受欢迎。然而，自我注意力操作的二次复杂性和图形变换器架构中的大规模层叠给将其应用于基于图形的预测任务带来了挑战。常见方法fine-tuning耗费资源且需要存储多个大型模型的副本。我们提出了一种新颖的方法，名为深度图形指导调优，作为在下游图形预测任务中利用大型图形变换器模型的替代方法。我们的方法引入了可训练的特征节点到图形中，并在图形变换器上预先添加任务特定的令牌，增强了模型的表达能力。通过冻结预训练参数并仅更新添加的令牌，我们的方法减少了自由参数的数量，消除了多个模型副本的需求，使其适用于小型数据集。

    Graph transformers have gained popularity in various graph-based tasks by addressing challenges faced by traditional Graph Neural Networks. However, the quadratic complexity of self-attention operations and the extensive layering in graph transformer architectures present challenges when applying them to graph based prediction tasks. Fine-tuning, a common approach, is resource-intensive and requires storing multiple copies of large models. We propose a novel approach called deep graph prompt tuning as an alternative to fine-tuning for leveraging large graph transformer models in downstream graph based prediction tasks. Our method introduces trainable feature nodes to the graph and pre-pends task-specific tokens to the graph transformer, enhancing the model's expressive power. By freezing the pre-trained parameters and only updating the added tokens, our approach reduces the number of free parameters and eliminates the need for multiple model copies, making it suitable for small dataset
    
[^75]: 二维双曲型守恒定律问题的深度平滑WENO格式：一种基于深度学习的平滑性指标学习方法

    Deep smoothness WENO scheme for two-dimensional hyperbolic conservation laws: A deep learning approach for learning smoothness indicators. (arXiv:2309.10117v1 [math.NA])

    [http://arxiv.org/abs/2309.10117](http://arxiv.org/abs/2309.10117)

    这篇论文提出了一种改进的二维双曲型守恒定律问题的深度平滑WENO格式，通过深度学习方法调整平滑性指标，在数值结果的准确性方面取得了优势，尤其是在陡峭激波附近。

    

    本文介绍了一种改进的五阶加权实质非振荡（WENO）激波捕获格式，采用深度学习技术进行改进。通过训练一个紧凑的神经网络来调整WENO格式中的平滑性指标，改进了现有的WENO算法。这种改进提高了数值结果的精确性，特别是在陡峭激波附近。与以前的基于深度学习的方法不同，不需要额外的后处理步骤来保持一致性。我们使用二维欧拉气体动力学方程的几个文献例子展示了我们新方法的优越性。通过对涉及各种激波和展开波的这些测试问题进行深入研究，新技术表现出在数值解在激波附近呈现出过度扩散或超调时的优势，特别是相对于传统的五阶WENO格式。

    In this paper, we introduce an improved version of the fifth-order weighted essentially non-oscillatory (WENO) shock-capturing scheme by incorporating deep learning techniques. The established WENO algorithm is improved by training a compact neural network to adjust the smoothness indicators within the WENO scheme. This modification enhances the accuracy of the numerical results, particularly near abrupt shocks. Unlike previous deep learning-based methods, no additional post-processing steps are necessary for maintaining consistency. We demonstrate the superiority of our new approach using several examples from the literature for the two-dimensional Euler equations of gas dynamics. Through intensive study of these test problems, which involve various shocks and rarefaction waves, the new technique is shown to outperform traditional fifth-order WENO schemes, especially in cases where the numerical solutions exhibit excessive diffusion or overshoot around shocks.
    
[^76]: AR-TTA: 一种用于真实世界连续测试时间自适应的简单方法

    AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation. (arXiv:2309.10109v1 [cs.CV])

    [http://arxiv.org/abs/2309.10109](http://arxiv.org/abs/2309.10109)

    AR-TTA提出了一种简单的方法用于真实世界连续测试时间自适应。通过将内存缓冲区纳入自训练框架，并根据数据流的强度进行动态适应，提高了模型的稳定性。

    

    测试时间自适应是一种有前景的研究方向，它允许源模型在没有任何监督的情况下适应数据分布的变化。然而，当前的方法通常在只是实际场景简化版本的基准测试中进行评估。因此，我们建议使用最近推出的自动驾驶数据集CLAD-C和SHIFT来验证测试时间自适应方法。我们观察到，当前的测试时间自适应方法往往难以有效处理不同程度的域偏移，常常导致性能下降，低于源模型。我们注意到问题的根源在于无法保留源模型的知识，并且无法适应动态变化、时间相关的数据流。因此，我们通过将一个小的内存缓冲区纳入到成熟的自训练框架中，增加模型的稳定性，并同时根据数据流的强度进行动态适应。

    Test-time adaptation is a promising research direction that allows the source model to adapt itself to changes in data distribution without any supervision. Yet, current methods are usually evaluated on benchmarks that are only a simplification of real-world scenarios. Hence, we propose to validate test-time adaptation methods using the recently introduced datasets for autonomous driving, namely CLAD-C and SHIFT. We observe that current test-time adaptation methods struggle to effectively handle varying degrees of domain shift, often resulting in degraded performance that falls below that of the source model. We noticed that the root of the problem lies in the inability to preserve the knowledge of the source model and adapt to dynamically changing, temporally correlated data streams. Therefore, we enhance well-established self-training framework by incorporating a small memory buffer to increase model stability and at the same time perform dynamic adaptation based on the intensity of 
    
[^77]: 通过隐式推理理解语言模型中的灾难性遗忘

    Understanding Catastrophic Forgetting in Language Models via Implicit Inference. (arXiv:2309.10105v1 [cs.CL])

    [http://arxiv.org/abs/2309.10105](http://arxiv.org/abs/2309.10105)

    本研究通过在语言模型上进行实验，发现微调对模型在微调数据分布任务上的表现有正面影响，但会抑制模型在其他任务上的能力，特别是与微调分布最接近的任务。作者假设语言模型会隐式推理任务，并且微调过程偏向于微调数据分布中的任务。作者进一步提出了共轭提示方法，以尝试恢复模型在预训练阶段的能力。

    

    微调（通过指令微调或从人类反馈进行强化学习等方法）是训练语言模型以鲁棒地执行所需任务的关键步骤。然而，我们缺乏对微调的影响的系统理解，特别是在狭窄的微调分布之外的任务上。在一个简化的场景中，我们证明，在微调数据分布内提高任务表现的同时，会抑制模型在其他任务上的能力。这种退化在与微调分布“最接近”的任务中尤为显著。我们假设语言模型会隐式推理出与提示相对应的任务，并且微调过程主要偏向于微调分布中的任务，以测试这个假设，我们提出了共轭提示以查看是否可以恢复预训练的能力。共轭提示会人为地使任务看起来与微调分布较远。

    Fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest. However, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks. This degradation is especially pronounced for tasks "closest" to the fine-tuning distribution. We hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution. To test this hypothesis, we propose Conjugate Prompting to see if we can recover pretrained capabilities. Conjugate prompting artificially makes the task look farther from the fine-tun
    
[^78]: 一种用于电力系统事件识别的半监督方法

    A Semi-Supervised Approach for Power System Event Identification. (arXiv:2309.10095v1 [cs.LG])

    [http://arxiv.org/abs/2309.10095](http://arxiv.org/abs/2309.10095)

    提出了一种新颖的半监督框架，通过加入无标签的事件样本来评估提升现有事件识别方法的有效性。

    

    事件识别被越来越认识到对于提高电力系统的可靠性、安全性和稳定性至关重要。随着相量测量装置的日益普及和数据科学的进步，通过机器学习分类技术探索基于数据驱动的事件识别具有很大的潜力。然而，由于工作量大和实时事件类型（类）的不确定性，获取精确标注的事件数据样本仍然具有挑战性。因此，使用半监督学习技术（同时使用有标签和无标签样本）是很自然的选择。

    Event identification is increasingly recognized as crucial for enhancing the reliability, security, and stability of the electric power system. With the growing deployment of Phasor Measurement Units (PMUs) and advancements in data science, there are promising opportunities to explore data-driven event identification via machine learning classification techniques. However, obtaining accurately-labeled eventful PMU data samples remains challenging due to its labor-intensive nature and uncertainty about the event type (class) in real-time. Thus, it is natural to use semi-supervised learning techniques, which make use of both labeled and unlabeled samples. %We propose a novel semi-supervised framework to assess the effectiveness of incorporating unlabeled eventful samples to enhance existing event identification methodologies. We evaluate three categories of classical semi-supervised approaches: (i) self-training, (ii) transductive support vector machines (TSVM), and (iii) graph-based lab
    
[^79]: 视频-文本检索的统一粗到细对齐方法

    Unified Coarse-to-Fine Alignment for Video-Text Retrieval. (arXiv:2309.10091v1 [cs.CV])

    [http://arxiv.org/abs/2309.10091](http://arxiv.org/abs/2309.10091)

    提出了一种统一粗到细对齐模型UCoFiA，用于视频-文本检索，该模型能够在不同粒度级别上捕捉跨模态相似性信息，并通过交互式相似性聚合模块有效考虑不同视觉特征的重要性，最终解决了视频-文本检索中的精确匹配问题。

    

    视频-文本检索通常利用视觉和文本信息之间的粗粒度或细粒度对齐。然而，根据文本查询检索正确的视频通常具有挑战性，因为它需要能够推理出高级（场景）和低级（对象）视觉线索及其与文本查询的关系。为此，我们提出了一种名为UCoFiA的统一粗到细对齐模型。具体而言，我们的模型在不同粒度级别上捕捉跨模态相似性信息。为减轻无关视觉线索的影响，我们还应用了交互式相似性聚合模块（ISA）来考虑不同视觉特征的重要性，同时聚合跨模态相似性以获得每个粒度的相似度得分。最后，我们应用Sinkhorn-Knopp算法对每个级别的相似性进行标准化，以减轻不同级别上的过度或不足表示问题。

    The canonical approach to video-text retrieval leverages a coarse-grained or fine-grained alignment between visual and textual information. However, retrieving the correct video according to the text query is often challenging as it requires the ability to reason about both high-level (scene) and low-level (object) visual clues and how they relate to the text query. To this end, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA. Specifically, our model captures the cross-modal similarity information at different granularity levels. To alleviate the effect of irrelevant visual clues, we also apply an Interactive Similarity Aggregation module (ISA) to consider the importance of different visual features while aggregating the cross-modal similarity to obtain a similarity score for each granularity. Finally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of each level before summing them, alleviating over- and under-representation issues at different l
    
[^80]: HTEC: 人类转录错误修正

    HTEC: Human Transcription Error Correction. (arXiv:2309.10089v1 [eess.AS])

    [http://arxiv.org/abs/2309.10089](http://arxiv.org/abs/2309.10089)

    HTEC是一种用于人类转录错误修正的方法，包括错误检测和填充两个阶段，提出了一种综合的修正操作列表，并针对删除错误提出了四种新操作。

    

    高质量的人类转录对于训练和改进自动语音识别（ASR）模型至关重要。最近的研究发现，每增加1%的转录错误率（WER），使用这些转录来训练ASR模型将增加约2%的ASR WER。即使是经过高度培训的注释员也难免出现转录错误。然而，很少有研究探讨人类转录错误的修正方法。其他问题的错误修正方法，如ASR错误修正和语法错误修正，对这个问题的表现不够好。因此，我们提出了HTEC用于人类转录错误修正。HTEC包括两个阶段：Trans-Checker，一种错误检测模型，用于预测和屏蔽错误单词，和Trans-Filler，一种序列到序列的生成模型，用于填充屏蔽位置。我们提出了一个综合的修正操作列表，其中包括四种处理删除错误的新操作。

    High-quality human transcription is essential for training and improving Automatic Speech Recognition (ASR) models. Recent study~\cite{libricrowd} has found that every 1% worse transcription Word Error Rate (WER) increases approximately 2% ASR WER by using the transcriptions to train ASR models. Transcription errors are inevitable for even highly-trained annotators. However, few studies have explored human transcription correction. Error correction methods for other problems, such as ASR error correction and grammatical error correction, do not perform sufficiently for this problem. Therefore, we propose HTEC for Human Transcription Error Correction. HTEC consists of two stages: Trans-Checker, an error detection model that predicts and masks erroneous words, and Trans-Filler, a sequence-to-sequence generative model that fills masked positions. We propose a holistic list of correction operations, including four novel operations handling deletion errors. We further propose a variant of e
    
[^81]: 不变的概率预测

    Invariant Probabilistic Prediction. (arXiv:2309.10083v1 [stat.ME])

    [http://arxiv.org/abs/2309.10083](http://arxiv.org/abs/2309.10083)

    这篇论文研究了在分布变化下具有不变性和稳健性的概率预测方法。研究发现在适当评分规则下，任意分布偏移一般不具有不变和稳健的概率预测，通过限制分布偏移类别和选择评估指标，可以在特定模型中实现不变性和稳健性。

    

    近年来，对于在训练和测试数据之间分布变化下表现稳健的统计方法越来越受关注。虽然大部分相关研究集中在使用平方误差损失的点预测上，但本文将焦点转向了概率预测，旨在全面量化给定协变量的结果变量的不确定性。在基于因果关系的框架下，我们研究了概率预测在适当评分规则下的不变性和稳健性。我们证明了任意分布偏移一般不具有不变和稳健的概率预测，与点预测的情况相反。我们展示了如何选择评估指标并限制分布偏移类别，以实现原型高斯异方差线性模型中的可识别性和不变性。在这些发现的基础上，我们提出了一种方法来产生不变的概率预测。

    In recent years, there has been a growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method to yield invariant pro
    
[^82]: GAME: 一种用于早期筛查青少年心理障碍的多模态数据整合的广义深度学习模型

    GAME: Generalized deep learning model towards multimodal data integration for early screening of adolescent mental disorders. (arXiv:2309.10077v1 [cs.LG])

    [http://arxiv.org/abs/2309.10077](http://arxiv.org/abs/2309.10077)

    本研究设计了一个广义深度学习模型GAME，通过整合多模态特征并采用新颖的关注机制，能够对青少年的心理状况进行高准确度的评估。研究发现每种模态都对心理障碍的筛查和共病状况有动态的贡献。

    

    及时发现青少年心理障碍是一个全球公共卫生挑战。由于其复杂而微妙的性质，单一因素很难检测到异常。此外，目前还没有广义的多模态图像与机器人相互作用的计算机辅助筛查系统用于检测青少年心理障碍。本研究设计了一个在便携机器人上部署的安卓应用程序，采用迷你游戏和聊天记录，对3,783名中学生进行筛查，并构建了包括面部图像、生理指标、语音记录和文本转录的多模态筛查数据集。我们开发了一个名为GAME（具有关注机制和多模态EmbraceNet的广义模型）的模型，该模型通过将跨模态特征整合到模型中，对青少年的心理状况进行高准确度（73.34%-92.77%）和F1-Score（71.32%-91.06%）的评估。我们发现每种模态动态地对心理障碍筛查和共病状况作出贡献。

    The timely identification of mental disorders in adolescents is a global public health challenge.Single factor is difficult to detect the abnormality due to its complex and subtle nature. Additionally, the generalized multimodal Computer-Aided Screening (CAS) systems with interactive robots for adolescent mental disorders are not available. Here, we design an android application with mini-games and chat recording deployed in a portable robot to screen 3,783 middle school students and construct the multimodal screening dataset, including facial images, physiological signs, voice recordings, and textual transcripts.We develop a model called GAME (Generalized Model with Attention and Multimodal EmbraceNet) with novel attention mechanism that integrates cross-modal features into the model. GAME evaluates adolescent mental conditions with high accuracy (73.34%-92.77%) and F1-Score (71.32%-91.06%).We find each modality contributes dynamically to the mental disorders screening and comorbiditi
    
[^83]: 非平稳核对深层高斯过程的统一视角

    A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes. (arXiv:2309.10068v1 [stat.ML])

    [http://arxiv.org/abs/2309.10068](http://arxiv.org/abs/2309.10068)

    本论文提出了一个统一的视角来探讨非平稳核在深层高斯过程中的应用，以提高预测性能和不确定性估计。

    

    高斯过程（GP）是一种流行的用于数据的随机函数近似和不确定性量化的统计技术。在过去的二十年中，由于其优越的预测能力，特别是在数据稀疏情况下，以及其固有的提供强健不确定性估计的能力，GP已被广泛应用于机器学习领域。然而，它们的性能高度依赖于核心方法的复杂定制，这往往在使用标准设置和现成软件工具时使从业者不满意。可以说，GP最重要的组成部分是核函数，它扮演协方差算子的角色。Mat\'ern类的平稳核在大多数应用研究中被使用；低效的预测性能和不现实的不确定性估计往往是其结果。非平稳核表现出更好的性能，但由于其更加复杂的属性，很少被使用。

    The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat\'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more
    
[^84]: 基于贝叶斯纵向张量响应回归的神经可塑性建模

    Bayesian longitudinal tensor response regression for modeling neuroplasticity. (arXiv:2309.10065v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.10065](http://arxiv.org/abs/2309.10065)

    提出了一种基于贝叶斯纵向张量响应回归的新方法，利用汇集空间分布的体素信息来推断显著的变化，并对协变量进行调整。该方法使用马尔科夫链蒙特卡洛采样，利用低秩分解减少维度并保持维度的空间配置，通过满足后验分布形状的联合可信区域实现特征选择，从而实现更准确的推断。

    

    纵向神经影像研究中对于由治疗和其他因素引起的体素级神经可塑性的调查是一个重要的研究兴趣。然而，传统的体素级方法存在一些缺陷，可能会影响这些方法的准确性。我们提出一种新颖的基于贝叶斯张量响应回归的纵向影像数据建模方法，通过汇集空间分布的体素信息来推断显著的变化，并对协变量进行调整。该方法采用马尔科夫链蒙特卡洛（MCMC）采样实现，利用低秩分解来减少维度并保持估计系数中体素的空间配置。它还通过满足后验分布形状的联合可信区域实现特征选择，以实现更准确的推断。除了群体水平的推断，该方法还能够推断个体水平的神经可塑性，从而可以进行个体水平的分析。

    A major interest in longitudinal neuroimaging studies involves investigating voxel-level neuroplasticity due to treatment and other factors across visits. However, traditional voxel-wise methods are beset with several pitfalls, which can compromise the accuracy of these approaches. We propose a novel Bayesian tensor response regression approach for longitudinal imaging data, which pools information across spatially-distributed voxels to infer significant changes while adjusting for covariates. The proposed method, which is implemented using Markov chain Monte Carlo (MCMC) sampling, utilizes low-rank decomposition to reduce dimensionality and preserve spatial configurations of voxels when estimating coefficients. It also enables feature selection via joint credible regions which respect the shape of the posterior distributions for more accurate inference. In addition to group level inferences, the method is able to infer individual-level neuroplasticity, allowing for examination of pers
    
[^85]: 无数据模型窃取的双学生网络

    Dual Student Networks for Data-Free Model Stealing. (arXiv:2309.10058v1 [cs.LG])

    [http://arxiv.org/abs/2309.10058](http://arxiv.org/abs/2309.10058)

    该论文提出了一种无数据模型窃取的方法，通过训练两个对称学生来引导生成器生成样本，使得这两个学生对样本的分类意见不一致，从而在生成器中激励探索更多样化的输入空间，并利用学生模型的梯度间接估计目标模型的梯度。

    

    现有的无数据模型窃取方法使用生成器产生样本来训练学生模型以匹配目标模型的输出。为此，主要挑战是在没有访问目标模型参数的情况下估计目标模型的梯度，并生成一组多样化的训练样本，充分探索输入空间。我们提出了一种双学生方法，通过对称训练两个学生，为生成器提供了一个标准，生成两个学生在其上意见不一致的样本。一方面，样本上的意见不一致意味着至少有一个学生将样本错误地分类为与目标模型相比。这种对不一致的激励隐含地促使生成器探索输入空间中更多样化的区域。另一方面，我们的方法利用学生模型的梯度间接估计目标模型的梯度。我们展示了这种用于生成器网络的新型训练目标与现有方法等价。

    Existing data-free model stealing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent 
    
[^86]: 一种带有噪声规范的模块化空间聚类算法

    A Modular Spatial Clustering Algorithm with Noise Specification. (arXiv:2309.10047v1 [cs.LG])

    [http://arxiv.org/abs/2309.10047](http://arxiv.org/abs/2309.10047)

    本文提出了一种模块化空间聚类算法"Bacteria-Farm"，基于实验室封闭饲养场中细菌生长的启发，通过平衡性能和寻找最优参数的难度来解决聚类算法中参数估计困难的问题。算法具有模块化设计，可以针对特定任务和数据分布创建不同的版本，并提供了指定噪声规范的功能。

    

    聚类技术是数据挖掘、机器学习和模式识别的关键驱动力。其中最流行的聚类算法之一是DBSCAN，由于其高准确性和噪声容忍度。许多优秀的算法，如DBSCAN，其输入参数难以估计。因此，找到这些参数是一个耗时的过程。在本文中，我们提出了一种新颖的聚类算法"Bacteria-Farm"，它在性能和寻找聚类的最优参数之间取得了平衡。Bacteria-Farm算法受到实验室封闭饲养场中细菌生长的启发 - 它们的能力消耗食物并生长 - 这与聚类算法中期望的理想聚类生长方式非常相似。此外，该算法具有模块化设计，允许针对特定任务/数据分布的版本创建算法。与其他聚类算法相比，我们的算法还提供了指定噪声规范的功能。

    Clustering techniques have been the key drivers of data mining, machine learning and pattern recognition for decades. One of the most popular clustering algorithms is DBSCAN due to its high accuracy and noise tolerance. Many superior algorithms such as DBSCAN have input parameters that are hard to estimate. Therefore, finding those parameters is a time consuming process. In this paper, we propose a novel clustering algorithm Bacteria-Farm, which balances the performance and ease of finding the optimal parameters for clustering. Bacteria- Farm algorithm is inspired by the growth of bacteria in closed experimental farms - their ability to consume food and grow - which closely represents the ideal cluster growth desired in clustering algorithms. In addition, the algorithm features a modular design to allow the creation of versions of the algorithm for specific tasks / distributions of data. In contrast with other clustering algorithms, our algorithm also has a provision to specify the amo
    
[^87]: 参数高效的长尾识别

    Parameter-Efficient Long-Tailed Recognition. (arXiv:2309.10019v1 [cs.CV])

    [http://arxiv.org/abs/2309.10019](http://arxiv.org/abs/2309.10019)

    本文提出了一种名为PEL的参数高效微调方法，可以在不到20个训练轮数内有效地将预训练模型适应于长尾识别任务，而无需额外的数据。该方法通过引入少量的任务特定参数，解决了常用微调方法导致尾部类别性能下降的问题。

    

    自从出现大规模视觉语言模型（例如对比语言-图像预训练模型CLIP），"预训练和微调"范例在解决长尾识别任务中引起了极大的兴趣。虽然先前研究在适应预训练模型用于这些任务方面表现出了希望，但它们常常需要大量的训练轮数或额外的训练数据来保持良好的性能，这是不可取的。在本文中，我们提出了一种名为PEL的微调方法，可以在不到20个训练轮数内有效地将预训练模型适应于长尾识别任务，而无需额外的数据。我们首先经验性地发现，常用的微调方法（例如全面微调和分类器微调）容易过拟合，导致尾部类别的性能下降。为了解决这个问题，PEL采用了现有的参数高效微调方法的设计，引入了少量的任务特定参数。

    The "pre-training and fine-tuning" paradigm in addressing long-tailed recognition tasks has sparked significant interest since the emergence of large vision-language models like the contrastive language-image pre-training (CLIP). While previous studies have shown promise in adapting pre-trained models for these tasks, they often undesirably require extensive training epochs or additional training data to maintain good performance. In this paper, we propose PEL, a fine-tuning method that can effectively adapt pre-trained models to long-tailed recognition tasks in fewer than 20 epochs without the need for extra data. We first empirically find that commonly used fine-tuning methods, such as full fine-tuning and classifier fine-tuning, suffer from overfitting, resulting in performance deterioration on tail classes. To mitigate this issue, PEL introduces a small number of task-specific parameters by adopting the design of any existing parameter-efficient fine-tuning method. Additionally, to
    
[^88]: GPT-3用于抗癌药物敏感性预测的评估

    Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction. (arXiv:2309.10016v1 [cs.LG])

    [http://arxiv.org/abs/2309.10016](http://arxiv.org/abs/2309.10016)

    本研究评估了GPT-3在抗癌药物敏感性预测任务中的潜力，并发现药物的SMILES表示和细胞系的基因组突变特征对药物反应具有预测能力。这些结果有助于在精准肿瘤学中设计更有效的治疗方案。

    

    本研究使用结构化的药物基因组数据，在五种组织类型中探究了GPT-3在抗癌药物敏感性预测任务中的潜力，并分别采用零样本提示和微调范式对其性能进行了评估。药物的SMILES表示和细胞系的基因组突变特征对药物反应具有预测能力。本研究的结果有望为精准肿瘤学中设计更有效的治疗方案铺平道路。

    In this study, we investigated the potential of GPT-3 for the anti-cancer drug sensitivity prediction task using structured pharmacogenomics data across five tissue types and evaluated its performance with zero-shot prompting and fine-tuning paradigms. The drug's smile representation and cell line's genomic mutation features were predictive of the drug response. The results from this study have the potential to pave the way for designing more efficient treatment protocols in precision oncology.
    
[^89]: SYNDICOM: 错误注入和自然语言反馈改进对话常识研究

    SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback. (arXiv:2309.10015v1 [cs.CL])

    [http://arxiv.org/abs/2309.10015](http://arxiv.org/abs/2309.10015)

    SYNDICOM是一种改进对话常识的方法，包含了一个常识对话数据集和一个基于自然语言反馈的模型，可用于训练对话应答生成模型。

    

    常识推理是人类交流的关键方面。尽管近年来由大型语言模型驱动的对话人工智能取得了进展，但常识推理仍然是一个具有挑战性的任务。在这项工作中，我们介绍了SYNDICOM - 一种改进对话应答生成中常识的方法。SYNDICOM由两个部分组成。第一个组件是一个由知识图创建的常识对话数据集，并以自然语言形式合成。该数据集包括对话环境中的有效和无效回答，以及对无效回答的自然语言反馈（NLF）。第二个贡献是一个两步的过程：训练一个模型来预测无效回答的自然语言反馈（NLF），然后根据预测的NLF、无效回答和对话条件训练一个应答生成模型。SYNDICOM具有可伸缩性，不需要强化学习。通过对三个任务的经验结果进行评估。

    Commonsense reasoning is a critical aspect of human communication. Despite recent advances in conversational AI driven by large language models, commonsense reasoning remains a challenging task. In this work, we introduce SYNDICOM - a method for improving commonsense in dialogue response generation. SYNDICOM consists of two components. The first component is a dataset composed of commonsense dialogues created from a knowledge graph and synthesized into natural language. This dataset includes both valid and invalid responses to dialogue contexts, along with natural language feedback (NLF) for the invalid responses. The second contribution is a two-step procedure: training a model to predict natural language feedback (NLF) for invalid responses, and then training a response generation model conditioned on the predicted NLF, the invalid response, and the dialogue. SYNDICOM is scalable and does not require reinforcement learning. Empirical results on three tasks are evaluated using a broad
    
[^90]: 通过Transformer模型预测多变量电池性能和健康状态

    Prognosis of Multivariate Battery State of Performance and Health via Transformers. (arXiv:2309.10014v1 [cs.LG])

    [http://arxiv.org/abs/2309.10014](http://arxiv.org/abs/2309.10014)

    本研究通过Transformer模型迈出了预测多变量电池性能和健康状态的第一步，为设计更好的电池和减少实验工作量提供了前所未有的见解。

    

    电池在深度去碳化的未来中是至关重要的组成部分。理解电池性能和“寿命”与设计和使用的关系对于加快采用电池技术至关重要。在历史上，电池健康状态（SOH）被总结为一个参数，即电池容量相对于初始状态的比例。然而，更实用的方法是综合描述其状态和复杂性，使用一组相关的描述符，包括容量、能量、离子和电子阻抗、开路电压和微观结构度量。事实上，准确预测电池使用过程中一系列性能是电池科学的“圣杯”；它可以为更好地设计电池提供前所未有的见解，并降低满足CO2减排目标所必需的能源储存投资的风险。在这项工作中，我们通过Transformer模型迈出了这个方向的第一步。

    Batteries are an essential component in a deeply decarbonized future. Understanding battery performance and "useful life" as a function of design and use is of paramount importance to accelerating adoption. Historically, battery state of health (SOH) was summarized by a single parameter, the fraction of a battery's capacity relative to its initial state. A more useful approach, however, is a comprehensive characterization of its state and complexities, using an interrelated set of descriptors including capacity, energy, ionic and electronic impedances, open circuit voltages, and microstructure metrics. Indeed, predicting across an extensive suite of properties as a function of battery use is a "holy grail" of battery science; it can provide unprecedented insights toward the design of better batteries with reduced experimental effort, and de-risking energy storage investments that are necessary to meet CO2 reduction targets. In this work, we present a first step in that direction via de
    
[^91]: 超几何与欧几里得嵌入在少样本学习中的比较：同一个硬币的两面。(arXiv:2309.10013v1 [cs.CV])

    Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin. (arXiv:2309.10013v1 [cs.CV])

    [http://arxiv.org/abs/2309.10013](http://arxiv.org/abs/2309.10013)

    这项研究探讨了超几何和欧几里得嵌入在少样本学习中的比较。研究表明，超几何嵌入在高维度中呈现出边界收敛的趋势，并在少样本分类任务中取得了最佳结果。与以往研究不同，研究者还发现，配备固定半径的欧几里得编码器也可以实现更好的性能。

    

    最近的表示学习研究表明，层次化数据在超几何空间中具有低维且高度信息丰富的表示。然而，即使超几何嵌入在图像识别中引起了关注，它们的优化容易遇到数值问题。此外，与传统的欧几里得特征相比，超几何性对哪些应用程序最有益仍不清楚。在本文中，我们重点关注原型超几何神经网络。特别是超几何嵌入在高维度中收敛于Poincar\'e球边界的趋势以及这对少样本分类的影响。我们证明了最好的少样本结果是在共同的超几何半径下获得的超几何嵌入。与之前的基准结果相反，我们证明了不论Euclidean度量，配备固定半径的编码器都可以实现更好的性能。

    Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincar\'e ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the
    
[^92]: 窥探过去：改进不断学习中生成回放的知识保留

    Looking through the past: better knowledge retention for generative replay in continual learning. (arXiv:2309.10012v1 [cs.LG])

    [http://arxiv.org/abs/2309.10012](http://arxiv.org/abs/2309.10012)

    本文改进了不断学习环境中的生成回放方法，以在复杂场景下表现更好。通过蒸馏潜在空间、改善生成特征对齐和周期性生成以增强知识保留。

    

    在这项工作中，我们改进了不断学习环境中的生成回放，以在具有挑战性的场景中表现良好。当前的生成回放方法通常在小型和简单的数据集上进行基准测试，因为它们不足以生成更复杂的数据和更多的类别。我们注意到，在基于VAE的生成回放中，这可能归因于当生成的特征映射到潜在空间时与原始特征之间存在较大差异。因此，我们提出了三种修改方法，使模型能够学习和生成复杂的数据。具体而言，我们将当前模型与先前模型之间的潜在空间进行蒸馏，以减少特征漂移。此外，我们提出了一种用于改善生成特征对齐的重建和原始数据的潜在匹配。进一步地，基于对保存知识的重建效果更好的观察，我们通过先前训练过的模型周期性生成的方式进一步增强了知识保留。

    In this work, we improve the generative replay in a continual learning setting to perform well on challenging scenarios. Current generative rehearsal methods are usually benchmarked on small and simple datasets as they are not powerful enough to generate more complex data with a greater number of classes. We notice that in VAE-based generative replay, this could be attributed to the fact that the generated features are far from the original ones when mapped to the latent space. Therefore, we propose three modifications that allow the model to learn and generate complex data. More specifically, we incorporate the distillation in latent space between the current and previous models to reduce feature drift. Additionally, a latent matching for the reconstruction and original data is proposed to improve generated features alignment. Further, based on the observation that the reconstructions are better for preserving knowledge, we add the cycling of generations through the previously trained
    
[^93]: 使用传感器数据的机器学习方法预测和检测奶牛早发性数字皮炎

    Machine Learning Approaches to Predict and Detect Early-Onset of Digital Dermatitis in Dairy Cows using Sensor Data. (arXiv:2309.10010v1 [cs.LG])

    [http://arxiv.org/abs/2309.10010](http://arxiv.org/abs/2309.10010)

    本研究提出了一种利用传感器数据的机器学习模型，可以在奶牛中早期预测和检测数字皮炎。其准确率分别为79%和64%，有助于开发实时自动化工具进行监测和诊断。

    

    本研究旨在利用基于传感器行为数据的机器学习算法，实现早期检测奶牛数字皮炎（DD）和预测DD。通过建立早期预警工具，更好地监测和管理商业环境下的DD，降低DD的发病率和严重程度，同时改善动物福利。在本探索性研究中，提出并测试了一种能够基于行为传感器数据预测和检测散养条件下奶牛数字皮炎的机器学习模型。在首次出现临床症状的第0天，DD检测模型的准确率达到了79%，在首次临床症状出现前2天，DD预测模型的准确率达到了64%。所提出的机器学习模型可以帮助开发实时自动化工具进行监测和诊断。

    The aim of this study was to employ machine learning algorithms based on sensor behavior data for (1) early-onset detection of digital dermatitis (DD); and (2) DD prediction in dairy cows. With the ultimate goal to set-up early warning tools for DD prediction, which would than allow a better monitoring and management of DD under commercial settings, resulting in a decrease of DD prevalence and severity, while improving animal welfare. A machine learning model that is capable of predicting and detecting digital dermatitis in cows housed under free-stall conditions based on behavior sensor data has been purposed and tested in this exploratory study. The model for DD detection on day 0 of the appearance of the clinical signs has reached an accuracy of 79%, while the model for prediction of DD 2 days prior to the appearance of the first clinical signs has reached an accuracy of 64%. The proposed machine learning models could help to develop a real-time automated tool for monitoring and dia
    
[^94]: DeepHEN: 预测基因的必需性长链非编码RNA基因并重新思考长链非编码RNA基因的必需性

    DeepHEN: quantitative prediction essential lncRNA genes and rethinking essentialities of lncRNA genes. (arXiv:2309.10008v1 [q-bio.MN])

    [http://arxiv.org/abs/2309.10008](http://arxiv.org/abs/2309.10008)

    DeepHEN是一种能够预测长链非编码RNA基因必需性的模型，并能确定序列特征和网络空间特征对必需性的影响的方法。

    

    基因的必需性是指基因对生物体的生存和繁殖效果的必要程度。尽管非编码基因的必需性已经有所记录，但我们对非编码基因的必需性仍然存在一些未知的方面。例如，我们不知道序列特征和网络空间特征对必需性的贡献。因此，在这项工作中，我们提出了DeepHEN，它可以回答上述问题。通过建立一个新的长链非编码RNA-蛋白质-蛋白质网络，并利用表示学习和图神经网络，我们成功构建了DeepHEN模型，可以预测长链非编码RNA基因的必需性。与其他预测长链非编码RNA基因必需性的方法相比，我们的DeepHEN模型不仅可以确定序列特征或网络空间特征对必需性的影响更大，还可以解决这些方法由于必需性较少而导致的过度拟合问题。

    Gene essentiality refers to the degree to which a gene is necessary for the survival and reproductive efficacy of a living organism. Although the essentiality of non-coding genes has been documented, there are still aspects of non-coding genes' essentiality that are unknown to us. For example, We do not know the contribution of sequence features and network spatial features to essentiality. As a consequence, in this work, we propose DeepHEN that could answer the above question. By buidling a new lncRNA-proteion-protein network and utilizing both representation learning and graph neural network, we successfully build our DeepHEN models that could predict the essentiality of lncRNA genes. Compared to other methods for predicting the essentiality of lncRNA genes, our DeepHEN model not only tells whether sequence features or network spatial features have a greater influence on essentiality but also addresses the overfitting issue of those methods caused by the low number of essential lncRN
    
[^95]: 自主车辆间的多智能体深度强化学习在AutoDRIVE生态系统中的合作与竞争

    Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem. (arXiv:2309.10007v1 [cs.RO])

    [http://arxiv.org/abs/2309.10007](http://arxiv.org/abs/2309.10007)

    本研究提出了一个模块化和并行化的多智能体深度强化学习框架，在AutoDRIVE生态系统中培养合作与竞争行为。我们利用该生态系统开发了准确物理和逼真图形的数字孪生体，并使用它来训练和部署多智能体强化学习策略，实现了在自主车辆中的合作和竞争行为。

    

    本研究提出了一个模块化和可并行化的多智能体深度强化学习框架，用于在自主车辆中培养合作和竞争行为。我们引入了AutoDRIVE生态系统作为一个工具，开发出与真实的Nigel和F1TENTH两种比例自主车辆平台具有独特特性和能力的准确物理和逼真图形的数字孪生体，并利用这个生态系统来训练和部署多智能体强化学习策略。我们首先研究了一个交叉路口穿越问题，使用一组合作车辆（Nigel）在单个或多个智能体学习环境中共享有限状态信息，采用一种公共策略方法。然后我们研究了一个对抗性的头对头自主赛车问题，使用另一组车辆（F1TENTH）在多个智能体学习环境中采用个体策略方法。在任何一组实验中，都采用了分散学习架构。

    This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles. We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies. We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted
    
[^96]: 基于语言模型的概率测量专利权要求范围的新方法

    A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])

    [http://arxiv.org/abs/2309.10003](http://arxiv.org/abs/2309.10003)

    本文提出了一种使用概率从语言模型中获得的自信息来测量专利权要求范围的新方法。该方法通过计算要求的发生概率和自信息来评估要求的信息量，进而反映出要求的范围。研究结果表明，不同类型的语言模型对范围测量的影响不同，最简单的模型可以将范围度量简化为单词或字符计数的倒数。此方法在九个系列的专利权要求上进行了验证，结果表明各系列的要求范围逐渐减小。

    

    本文提出了一种将专利权要求的范围测量为该要求所包含的自信息的倒数的方法。这种方法基于信息论，基于一个假设，即罕见的概念比平常的概念更具信息量，因为它更令人惊讶。自信息是从该要求的发生概率计算得出的，其中概率是根据语言模型计算的。本文考虑了五个语言模型，从最简单的模型（每个单词或字符均从均匀分布中抽取）到中等模型（使用平均词或字符频率），再到一个大型语言模型（GPT2）。有趣的是，最简单的语言模型将范围度量减少为单词或字符计数的倒数，这是先前作品中已经使用的度量标准。该方法应用于九个系列的针对不同发明的专利权要求，其中每个系列的要求范围逐渐减小。

    This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
    
[^97]: 梯度流方程的能量稳定神经网络

    Energy stable neural network for gradient flow equations. (arXiv:2309.10002v1 [cs.LG])

    [http://arxiv.org/abs/2309.10002](http://arxiv.org/abs/2309.10002)

    本文提出了能量稳定网络(EStable-Net)用于解决梯度流方程，该网络能够降低离散能量并生成高准确性和稳定性的预测。

    

    本文提出了一种用于求解梯度流方程的能量稳定网络（EStable-Net）。我们的神经网络EStable-Net的解更新方案受到了梯度流方程基于辅助变量的等价形式的启发。EStable-Net能够在神经网络中降低离散能量，与梯度流方程的演化过程的性质保持一致。神经网络EStable-Net的架构包括几个能量衰减模块，每个模块的输出可以解释为梯度流方程演化过程的中间状态。这种设计提供了一个稳定、高效且可解释的网络结构。数值实验结果表明，我们的网络能够生成高准确性和稳定性的预测。

    In this paper, we propose an energy stable network (EStable-Net) for solving gradient flow equations. The solution update scheme in our neural network EStable-Net is inspired by a proposed auxiliary variable based equivalent form of the gradient flow equation. EStable-Net enables decreasing of a discrete energy along the neural network, which is consistent with the property in the evolution process of the gradient flow equation. The architecture of the neural network EStable-Net consists of a few energy decay blocks, and the output of each block can be interpreted as an intermediate state of the evolution process of the gradient flow equation. This design provides a stable, efficient and interpretable network structure. Numerical experimental results demonstrate that our network is able to generate high accuracy and stable predictions.
    
[^98]: 使用文档嵌入和降维方法检测文本数据中的协变漂移

    Detecting covariate drift in text data using document embeddings and dimensionality reduction. (arXiv:2309.10000v1 [cs.LG])

    [http://arxiv.org/abs/2309.10000](http://arxiv.org/abs/2309.10000)

    本研究通过比较不同的文档嵌入、降维技术和漂移检测方法，发现在检测文本数据中的协变漂移方面，特定的嵌入方法、降维技术和漂移检测方法的组合效果优于其他方法。

    

    检测文本数据中的协变漂移对于保持文本分析模型的可靠性和性能至关重要。在这项研究中，我们调查了不同的文档嵌入、降维技术和漂移检测方法对于识别文本数据中的协变漂移的有效性。我们探索了三种流行的文档嵌入方法：使用潜在语义分析（LSA）的词频-逆文档频率（TF-IDF）进行降维，以及使用主成分分析（PCA）进行降维的Doc2Vec和BERT嵌入。为了量化训练数据和测试数据分布之间的差异，我们采用了Kolmogorov-Smirnov（KS）统计量和最大均值差异（MMD）检验作为漂移检测方法。实验结果表明，在检测协变漂移方面，某些嵌入方法、降维技术和漂移检测方法的组合表现优于其他方法。

    Detecting covariate drift in text data is essential for maintaining the reliability and performance of text analysis models. In this research, we investigate the effectiveness of different document embeddings, dimensionality reduction techniques, and drift detection methods for identifying covariate drift in text data. We explore three popular document embeddings: term frequency-inverse document frequency (TF-IDF) using Latent semantic analysis(LSA) for dimentionality reduction and Doc2Vec, and BERT embeddings, with and without using principal component analysis (PCA) for dimensionality reduction. To quantify the divergence between training and test data distributions, we employ the Kolmogorov-Smirnov (KS) statistic and the Maximum Mean Discrepancy (MMD) test as drift detection methods. Experimental results demonstrate that certain combinations of embeddings, dimensionality reduction techniques, and drift detection methods outperform others in detecting covariate drift. Our findings co
    
[^99]: 用音频分类改进非洲裔美国英语的语音识别

    Improving Speech Recognition for African American English With Audio Classification. (arXiv:2309.09996v1 [eess.AS])

    [http://arxiv.org/abs/2309.09996](http://arxiv.org/abs/2309.09996)

    通过使用少量非洲裔美国英语的数据，结合音频分类器和地理信息，我们提出了一种改进美国英语语音识别的方法，相对词错误率减少了38.5%。

    

    自动语音识别(ASR)系统在识别不同语言变种时存在较大的质量差异。为了缓解这个问题，一种方法是使用更具代表性的数据集来训练或微调模型。但是有时候在领域内数据的数量有限，这会使该方法受到限制。我们提出了一种新的方法，利用少量领域外数据(长篇形式的非洲裔美国英语)来提高美国英语短篇语音识别器的鲁棒性。我们使用CORAAL、YouTube和Mozilla Common Voice来训练一个音频分类器，该分类器可以大致判断一句话是非洲裔美国英语还是其他变种，包括主流美国英语。通过将分类器输出与粗略的地理信息结合起来，我们可以从大量未翻译的短篇查询语料库中选择一部分语句进行半监督学习。在此数据上进行微调结果显示相对词错误率减少了38.5%。

    Automatic speech recognition (ASR) systems have been shown to have large quality disparities between the language varieties they are intended or expected to recognize. One way to mitigate this is to train or fine-tune models with more representative datasets. But this approach can be hindered by limited in-domain data for training and evaluation. We propose a new way to improve the robustness of a US English short-form speech recognizer using a small amount of out-of-domain (long-form) African American English (AAE) data. We use CORAAL, YouTube and Mozilla Common Voice to train an audio classifier to approximately output whether an utterance is AAE or some other variety including Mainstream American English (MAE). By combining the classifier output with coarse geographic information, we can select a subset of utterances from a large corpus of untranscribed short-form queries for semi-supervised learning at scale. Fine-tuning on this data results in a 38.5% relative word error rate disp
    
[^100]: 新冠后长期神经后遗症：一种基于机器学习的预测结果方法

    Long-term Neurological Sequelae in Post-COVID-19 Patients: A Machine Learning Approach to Predict Outcomes. (arXiv:2309.09993v1 [cs.LG])

    [http://arxiv.org/abs/2309.09993](http://arxiv.org/abs/2309.09993)

    该研究使用机器学习方法预测了新冠后长期神经并发症的结果，发现68%的患者报告有神经症状，其中疲劳、头痛和嗅觉丧失最常见。随机森林模型在识别患者发展神经病风险方面取得了有希望的准确性、敏感性和特异性。

    

    新冠疫情揭示了康复后患者长期神经并发症的一个令人担忧的方面。本研究对500例新冠后患者进行了神经后遗症的调查，包括疾病严重程度不同的个体。主要目标是基于多样化的临床数据和神经影像参数，使用机器学习方法预测结果。结果表明，68%的新冠后患者报告出现神经症状，疲劳、头痛和嗅觉丧失是最常见的表现。此外，22%的患者出现更严重的神经并发症，包括脑病和中风。机器学习模型的应用在预测长期神经结果方面取得了有希望的结果。值得注意的是，随机森林模型在辨别发展神经病风险的患者上达到了85%的准确性、80%的敏感性和90%的特异性。

    The COVID-19 pandemic has brought to light a concerning aspect of long-term neurological complications in post-recovery patients. This study delved into the investigation of such neurological sequelae in a cohort of 500 post-COVID-19 patients, encompassing individuals with varying illness severity. The primary aim was to predict outcomes using a machine learning approach based on diverse clinical data and neuroimaging parameters. The results revealed that 68% of the post-COVID-19 patients reported experiencing neurological symptoms, with fatigue, headache, and anosmia being the most common manifestations. Moreover, 22% of the patients exhibited more severe neurological complications, including encephalopathy and stroke. The application of machine learning models showed promising results in predicting long-term neurological outcomes. Notably, the Random Forest model achieved an accuracy of 85%, sensitivity of 80%, and specificity of 90% in identifying patients at risk of developing neur
    
[^101]: TCGF: 一种统一的张量一致性图框架用于多视图表示学习

    TCGF: A unified tensorized consensus graph framework for multi-view representation learning. (arXiv:2309.09987v1 [cs.LG])

    [http://arxiv.org/abs/2309.09987](http://arxiv.org/abs/2309.09987)

    提出了一种名为TCGF的统一多视图表示学习框架，用于有效地融合多个视图中的关键信息。

    

    多视图学习技术近年来在机器学习领域引起了重要关注，因为它们能够利用多个视图之间的一致性和互补信息。然而，目前对于将现有工作统一到可扩展和鲁棒的学习框架中的泛化多视图框架的研究还不足，因为大部分当前的工作都专注于特定类型的多视图模型。此外，大多数多视图学习方法在特定尺度场景下依赖较重，并且无法整体有效地理解多种尺度。这些限制妨碍了从多视图中有效融合关键信息，导致泛化能力不佳。为了解决这些问题，本文提出了一种名为Tensorized Consensus Graph Framework (TCGF)的通用多视图表示学习框架。具体而言，它首先提供了一个统一的框架来利用各个视图的表示，

    Multi-view learning techniques have recently gained significant attention in the machine learning domain for their ability to leverage consistency and complementary information across multiple views. However, there remains a lack of sufficient research on generalized multi-view frameworks that unify existing works into a scalable and robust learning framework, as most current works focus on specific styles of multi-view models. Additionally, most multi-view learning works rely heavily on specific-scale scenarios and fail to effectively comprehend multiple scales holistically. These limitations hinder the effective fusion of essential information from multiple views, resulting in poor generalization. To address these limitations, this paper proposes a universal multi-view representation learning framework named Tensorized Consensus Graph Framework (TCGF). Specifically, it first provides a unified framework for existing multi-view works to exploit the representations for individual view,
    
[^102]: 探索和比较用于预测大脑对真实图片的深度学习架构

    Exploration and Comparison of Deep Learning Architectures to Predict Brain Response to Realistic Pictures. (arXiv:2309.09983v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.09983](http://arxiv.org/abs/2309.09983)

    本研究探索和比较了多种深度学习架构，用于预测大脑对真实图片的反应。最终发现，使用多个简单模型，每个模型专注于受试者大脑每个半球的每个ROI，可以获得更好的预测结果。

    

    我们展示了一项关于预测大脑对真实图像反应的机器学习架构的研究，以应对Algonauts Challenge 2023。我们的研究涉及对各种预训练模型进行了广泛实验。最初，我们采用了较简单的模型来预测大脑活动，但逐渐引入了更复杂的架构，利用可用的数据和大规模预训练模型生成的嵌入。我们遇到了与机器学习问题相关的典型困难，比如正则化和过拟合，以及与挑战特定的问题，如难以结合多个输入编码，以及输出的高维度、不明确的结构和嘈杂性。为了克服这些问题，我们测试了基于单边三维位置、多感兴趣区域(ROI)和半球的预测模型，但我们发现使用多个简单的模型，每个模型专注于受试者大脑每个半球的每个ROI，可以获得更好的结果。

    We present an exploration of machine learning architectures for predicting brain responses to realistic images on occasion of the Algonauts Challenge 2023. Our research involved extensive experimentation with various pretrained models. Initially, we employed simpler models to predict brain activity but gradually introduced more complex architectures utilizing available data and embeddings generated by large-scale pre-trained models. We encountered typical difficulties related to machine learning problems, e.g. regularization and overfitting, as well as issues specific to the challenge, such as difficulty in combining multiple input encodings, as well as the high dimensionality, unclear structure, and noisy nature of the output. To overcome these issues we tested single edge 3D position-based, multi-region of interest (ROI) and hemisphere predictor models, but we found that employing multiple simple models, each dedicated to a ROI in each hemisphere of the brain of each subject, yielded
    
[^103]: 内省式深度度量学习

    Introspective Deep Metric Learning. (arXiv:2309.09982v1 [cs.CV])

    [http://arxiv.org/abs/2309.09982](http://arxiv.org/abs/2309.09982)

    本文提出了一种内省式深度度量学习 (IDML) 框架，通过考虑图像中的不确定性，以更好地处理模糊图像，实现更鲁棒的训练。

    

    本文提出了一种内省式深度度量学习 (IDML) 框架，用于对图像进行不确定性感知的比较。传统的深度度量学习方法着重于学习一个具有区分度的嵌入来描述图像的语义特征，忽略了由于噪声或语义模糊性而导致的每个图像的不确定性存在。在没有意识到这些不确定性的情况下进行训练会导致模型在训练期间过度拟合注释标签，并在推理期间产生不令人满意的判断。受此启发，我们认为一个好的相似度模型应考虑到不确定性的语义差异，以更好地处理模糊图像，从而实现更鲁棒的训练。为了实现这一点，我们提出了一种表示图像的方法，不仅使用语义嵌入，还使用伴随的不确定性嵌入，分别描述图像的语义特征和模糊性。我们进一步提出了一种内省式相似度度量方法，用于进行内省式比较，以更好地处理图像的不确定性。

    This paper proposes an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. Conventional deep metric learning methods focus on learning a discriminative embedding to describe the semantic features of images, which ignore the existence of uncertainty in each image resulting from noise or semantic ambiguity. Training without awareness of these uncertainties causes the model to overfit the annotated labels during training and produce unsatisfactory judgments during inference. Motivated by this, we argue that a good similarity model should consider the semantic discrepancies with awareness of the uncertainty to better deal with ambiguous images for more robust training. To achieve this, we propose to represent an image using not only a semantic embedding but also an accompanying uncertainty embedding, which describes the semantic characteristics and ambiguity of an image, respectively. We further propose an introspective similarity metric to make
    
[^104]: Des-q: 一种用于回归和二分类的构建和高效重新训练决策树的量子算法

    Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification. (arXiv:2309.09976v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2309.09976](http://arxiv.org/abs/2309.09976)

    Des-q是一种量子算法，用于在回归和二分类任务中构建和重新训练决策树。它显著减少了树重新训练所需的时间复杂度，并且能够处理新样本的加载时间。该算法通过 k 分段线性树分裂来构建决策树，将数据划分为不同的子空间。

    

    决策树由于其简单构造和可解释性而广泛应用于机器学习。然而，随着数据规模的增长，传统的决策树构建和重新训练方法变得越来越慢，与训练样本数量呈多项式规模。在本研究中，我们介绍了一种新颖的量子算法Des-q，用于在回归和二分类任务中构建和重新训练决策树。假设数据流产生较小的新训练样本增量，我们证明了我们的Des-q算法显著减少了树重新训练所需的时间，即使考虑将新样本加载到量子可访问内存所需的时间，其时间复杂度也达到了多对数级别。我们的方法涉及构建一个决策树算法，在每个内部节点执行k分段线性树分裂。这些分裂同时生成多个超平面，将数据划分为不同的子空间。

    Decision trees are widely used in machine learning due to their simplicity in construction and interpretability. However, as data sizes grow, traditional methods for constructing and retraining decision trees become increasingly slow, scaling polynomially with the number of training examples. In this work, we introduce a novel quantum algorithm, named Des-q, for constructing and retraining decision trees in regression and binary classification tasks. Assuming the data stream produces small increments of new training examples, we demonstrate that our Des-q algorithm significantly reduces the time required for tree retraining, achieving a poly-logarithmic time complexity in the number of training examples, even accounting for the time needed to load the new examples into quantum-accessible memory. Our approach involves building a decision tree algorithm to perform k-piecewise linear tree splits at each internal node. These splits simultaneously generate multiple hyperplanes, dividing the
    
[^105]: 基于热和波动动力学特征的图拓扑属性恢复

    Graph topological property recovery with heat and wave dynamics-based features on graphsD. (arXiv:2309.09924v1 [cs.LG])

    [http://arxiv.org/abs/2309.09924](http://arxiv.org/abs/2309.09924)

    本文提出了一种名为图微分方程网络（GDeNet）的方法，利用热和波动方程动力学特征来恢复图的拓扑属性，能够在各种下游任务中获得优秀的表现，同时在实际应用中也展现了较好的性能。

    

    本文提出了一种名为图微分方程网络（GDeNet）的方法，利用图上的PDE解的表达能力，为各种下游任务获得连续的节点和图级表示。我们推导出了热和波动方程动力学与图的谱特性以及连续时间随机游走在图上行为之间的理论结果。我们通过恢复随机图生成参数、Ricci曲率和持久同调等方式实验证明了这些动力学能够捕捉到图形几何和拓扑的显著方面。此外，我们还展示了GDeNet在包括引用图、药物分子和蛋白质在内的真实世界数据集上的优越性能。

    In this paper, we propose Graph Differential Equation Network (GDeNet), an approach that harnesses the expressive power of solutions to PDEs on a graph to obtain continuous node- and graph-level representations for various downstream tasks. We derive theoretical results connecting the dynamics of heat and wave equations to the spectral properties of the graph and to the behavior of continuous-time random walks on graphs. We demonstrate experimentally that these dynamics are able to capture salient aspects of graph geometry and topology by recovering generating parameters of random graphs, Ricci curvature, and persistent homology. Furthermore, we demonstrate the superior performance of GDeNet on real-world datasets including citation graphs, drug-like molecules, and proteins.
    
[^106]: 在缺乏文化共识的情况下利用集体智慧

    Harnessing Collective Intelligence Under a Lack of Cultural Consensus. (arXiv:2309.09787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09787](http://arxiv.org/abs/2309.09787)

    在缺乏文化共识的情况下，通过无限深度潜在结构的文化共识理论（iDLC-CCT）模型，扩展了文化共识理论（CCT）的能力，提高了对共识信念多样性的建模能力。

    

    利用集体智慧来推动有效的决策和合作受益于能够检测和描述共识信念的多样性。文化共识理论（CCT）提供了一种统计框架，用于检测和描述这些不同的共识信念。然而，它在现代应用中不可行，因为它缺乏对相似信念的概括能力，对稀疏数据无效，并且无法利用外部知识库或学习到的机器表示。在这里，我们通过无限深度潜在结构的文化共识理论（iDLC-CCT）来克服这些限制。该模型是一个非参数贝叶斯模型，通过一个潜在结构来扩展CCT。该结构允许我们将文化共识看作是一系列无限深度的概念构建块，从而提高了对信念多样性的建模能力。

    Harnessing collective intelligence to drive effective decision-making and collaboration benefits from the ability to detect and characterize heterogeneity in consensus beliefs. This is particularly true in domains such as technology acceptance or leadership perception, where a consensus defines an intersubjective truth, leading to the possibility of multiple "ground truths" when subsets of respondents sustain mutually incompatible consensuses. Cultural Consensus Theory (CCT) provides a statistical framework for detecting and characterizing these divergent consensus beliefs. However, it is unworkable in modern applications because it lacks the ability to generalize across even highly similar beliefs, is ineffective with sparse data, and can leverage neither external knowledge bases nor learned machine representations. Here, we overcome these limitations through Infinite Deep Latent Construct Cultural Consensus Theory (iDLC-CCT), a nonparametric Bayesian model that extends CCT with a lat
    
[^107]: FedGKD:在联邦图神经网络中释放协作的力量

    FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks. (arXiv:2309.09517v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09517](http://arxiv.org/abs/2309.09517)

    FedGKD是一种新颖的联邦图神经网络框架，通过利用客户端图数据集蒸馏方法提取更好的任务特征并引入感知全局协作结构的服务器端聚合机制，解决了联邦GNN系统中图异构性问题，提高了效率和准确性。

    

    最近几年来，由于联邦图神经网络（GNN）能够在数据隔离场景下执行与图相关的任务并保护数据隐私，联邦训练已经变得流行起来。然而，联邦GNN系统中的图异构性问题仍然存在挑战。现有的框架通过使用不同的统计量来表示局部任务，并通过简单的聚合机制将它们联系起来来解决这个问题。然而，这些方法在两个方面都效率有限：任务相关性量化的质量低和利用协作结构的无效性。为了解决这些问题，我们提出了FedGKD，一种新颖的联邦GNN框架，它利用一种新颖的客户端图数据集蒸馏方法提取更好地描述任务相关性的任务特征，并引入一个新颖的服务器端聚合机制，该机制能够感知到全局的协作结构。我们在六个真实世界的数据集上进行了大量实验证明了FedGKD框架的有效性。

    Federated training of Graph Neural Networks (GNN) has become popular in recent years due to its ability to perform graph-related tasks under data isolation scenarios while preserving data privacy. However, graph heterogeneity issues in federated GNN systems continue to pose challenges. Existing frameworks address the problem by representing local tasks using different statistics and relating them through a simple aggregation mechanism. However, these approaches suffer from limited efficiency from two aspects: low quality of task-relatedness quantification and inefficacy of exploiting the collaboration structure. To address these issues, we propose FedGKD, a novel federated GNN framework that utilizes a novel client-side graph dataset distillation method to extract task features that better describe task-relatedness, and introduces a novel server-side aggregation mechanism that is aware of the global collaboration structure. We conduct extensive experiments on six real-world datasets of
    
[^108]: 不需要计算复杂性无法解决的预言机，在稀疏线性MDP中探索和学习。

    Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles. (arXiv:2309.09457v1 [cs.LG])

    [http://arxiv.org/abs/2309.09457](http://arxiv.org/abs/2309.09457)

    本文研究了在稀疏线性MDP中探索和学习的问题，通过特征选择提出了一个多项式时间算法，以在与环境的交互中学习出近似最优策略。

    

    线性马尔可夫决策过程（MDPs）的基本假设是学习者可以访问已知的特征映射$ \phi（x，a）$，该映射将状态-动作对映射到$d$维向量，并且奖励和转换是此表示中的线性函数。但是这些特征从哪里来？在没有专家领域知识的情况下，一种诱人的策略是使用“厨房水槽”方法，并希望真实特征包含在一个更大的潜在特征集中。在本文中，我们从特征选择的角度重新审视线性MDP。在$k$-稀疏线性MDP中，存在一个未知的大小为$k$的子集$S \subset [d]$，其中包含所有相关特征，目标是在与环境的交互中仅经过poly$(k,\log d)$次学习，学习出近似最优策略。我们的主要结果是这个问题的第一个多项式时间算法。与此相反，早期的研究要么做出了明显的假设，使得探索无关紧要，要么提供了指数复杂度的算法。

    The key assumption underlying linear Markov Decision Processes (MDPs) is that the learner has access to a known feature map $\phi(x, a)$ that maps state-action pairs to $d$-dimensional vectors, and that the rewards and transitions are linear functions in this representation. But where do these features come from? In the absence of expert domain knowledge, a tempting strategy is to use the ``kitchen sink" approach and hope that the true features are included in a much larger set of potential features. In this paper we revisit linear MDPs from the perspective of feature selection. In a $k$-sparse linear MDP, there is an unknown subset $S \subset [d]$ of size $k$ containing all the relevant features, and the goal is to learn a near-optimal policy in only poly$(k,\log d)$ interactions with the environment. Our main result is the first polynomial-time algorithm for this problem. In contrast, earlier works either made prohibitively strong assumptions that obviated the need for exploration, o
    
[^109]: CaT: 带有图压缩的平衡持续图学习

    CaT: Balanced Continual Graph Learning with Graph Condensation. (arXiv:2309.09455v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09455](http://arxiv.org/abs/2309.09455)

    本文提出了一种名为CaT的方法，通过引入图压缩技术，解决持续图学习中存储预算紧张和数据不平衡的问题。

    

    持续图学习（CGL）的目标是以流式方式不断更新图模型。由于模型在训练新数据时容易忘记之前学到的知识，灾难性遗忘问题已成为CGL的主要关注点。最近的基于回放的方法通过使用（1）整个新数据和（2）存储回放图以近似历史数据分布的基于采样的记忆库来更新模型，试图解决这个问题。在更新模型后，从输入图中采样的新回放图将添加到现有的记忆库中。尽管这些方法对CGL来说直观且有效，但本文发现其中存在两个问题。首先，绝大多数基于采样的方法在存储预算紧张时难以完全捕捉历史分布。其次，在复杂新数据的规模和现有数据之间存在显著的数据不平衡。

    Continual graph learning (CGL) is purposed to continuously update a graph model with graph data being fed in a streaming manner. Since the model easily forgets previously learned knowledge when training with new-coming data, the catastrophic forgetting problem has been the major focus in CGL. Recent replay-based methods intend to solve this problem by updating the model using both (1) the entire new-coming data and (2) a sampling-based memory bank that stores replayed graphs to approximate the distribution of historical data. After updating the model, a new replayed graph sampled from the incoming graph will be added to the existing memory bank. Despite these methods are intuitive and effective for the CGL, two issues are identified in this paper. Firstly, most sampling-based methods struggle to fully capture the historical distribution when the storage budget is tight. Secondly, a significant data imbalance exists in terms of the scales of the complex new-coming graph data and the lig
    
[^110]: Wasserstein分布保证的策略评估和学习在上下文乐队中

    Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits. (arXiv:2309.08748v1 [cs.LG])

    [http://arxiv.org/abs/2309.08748](http://arxiv.org/abs/2309.08748)

    通过使用Wasserstein距离而不是KL散度，我们提出了一种新颖的分布保证优化方法，用于解决上下文乐队中实际环境不匹配和最坏情况下过度拟合的问题。

    

    在没有与环境直接互动的情况下，数据收集的环境通常与学习的策略应用的环境不同。为了在学习和执行过程中考虑不同环境的影响，我们提出了一种使用Wasserstein距离的新型分布保证优化(DRO)方法，该方法在假设新环境的分布位于不确定集合内时，计算策略值的最坏情况下界。典型地，这个不确定集合是基于从日志数据集中计算的经验分布的KL散度定义的。然而，KL不确定集合无法包含具有不同支持的分布，也缺乏对分布支持的几何感知。结果，KL方法在解决实际环境不匹配和导致过度拟合最坏情况方面存在不足。为了克服这些限制，我们提出了一种使用Wasserstein距离的新型DRO方法。

    Without direct interaction with the environment. Often, the environment in which the data are collected differs from the environment in which the learned policy is applied. To account for the effect of different environments during learning and execution, distributionally robust optimization (DRO) methods have been developed that compute worst-case bounds on the policy values assuming that the distribution of the new environment lies within an uncertainty set. Typically, this uncertainty set is defined based on the KL divergence around the empirical distribution computed from the logging dataset. However, the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support. As a result, KL approaches fall short in addressing practical environment mismatches and lead to over-fitting to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach that employs the Wasserstein distance instead. 
    
[^111]: 稀疏自编码器在语言模型中发现高度可解释的特征

    Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v1 [cs.LG])

    [http://arxiv.org/abs/2309.08600](http://arxiv.org/abs/2309.08600)

    本研究通过稀疏自编码器在语言模型中发现了一组高度可解释和单一义的特征，从而解决了神经网络内部多义性的问题。

    

    神经网络内部理解的一个障碍是多义性，其中神经元在多个语义不同的上下文中激活。多义性使我们无法找到简洁的、人类可理解的解释来解释神经网络内部的工作。多义性的一个猜测原因是叠加效应，即神经网络通过将特征分配给激活空间中的一个过完备方向集合，而不是个别神经元，表示更多的特征。在这里，我们尝试使用稀疏自编码器来确定这些方向，以重构语言模型的内部激活。这些自编码器学习到的一组稀疏激活特征比其他方法鉴定出的方向更可解释和单一义，解释性是通过自动化方法衡量的。删除这些特征可以实现精确的模型编辑，例如通过删除这些特征可以改变模型输出。

    One of the roadblocks to a better understanding of neural networks' internals is \textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Ablating these features enables precise model editing, for example, by remo
    
[^112]: 使用机器学习和不确定性量化对CRT的多阶段决策过程进行建模的新方法

    A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification. (arXiv:2309.08415v1 [cs.LG])

    [http://arxiv.org/abs/2309.08415](http://arxiv.org/abs/2309.08415)

    本研究提出了一种使用机器学习和不确定性量化建模的多阶段决策过程方法，用于预测心力衰竭患者对心脏再同步治疗的反应。该模型能够推荐收集额外的SPECT MPI变量，以提高预测准确性。

    

    目的。本研究旨在创建一个多阶段的机器学习模型，用于预测心力衰竭（HF）患者心脏再同步治疗（CRT）的反应。该模型利用不确定性量化来推荐在基线临床变量和心电图（ECG）的特征不足时收集额外的单光子发射计算机体层摄影心肌灌注显像（SPECT MPI）变量。方法。本研究纳入了218名接受静息门控SPECT MPI的患者。CRT反应被定义为6个月随访时左室射血分数（LVEF）增加> 5%。通过组合两个集成模型创建了一个多阶段的机器学习模型。结果。CRT的反应率为55.5%（n = 121），整体男性占61.0%（n = 133），平均年龄62.0岁，LVEF为27.7。该多阶段模型的性能与集成模型2（利用了额外的SPECT数据）相似，AUC分别为0.75和0.77，准确性分别为0.71和...

    Aims. The purpose of this study is to create a multi-stage machine learning model to predict cardiac resynchronization therapy (CRT) response for heart failure (HF) patients. This model exploits uncertainty quantification to recommend additional collection of single-photon emission computed tomography myocardial perfusion imaging (SPECT MPI) variables if baseline clinical variables and features from electrocardiogram (ECG) are not sufficient. Methods. 218 patients who underwent rest-gated SPECT MPI were enrolled in this study. CRT response was defined as an increase in left ventricular ejection fraction (LVEF) > 5% at a 6 month follow-up. A multi-stage ML model was created by combining two ensemble models. Results. The response rate for CRT was 55.5% (n = 121) with overall male gender 61.0% (n = 133), an average age of 62.0, and LVEF of 27.7. The multi-stage model performed similarly to Ensemble 2 (which utilized the additional SPECT data) with AUC of 0.75 vs. 0.77, accuracy of 0.71 vs
    
[^113]: 旅行词：一种变压器的几何解释。

    Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])

    [http://arxiv.org/abs/2309.07315](http://arxiv.org/abs/2309.07315)

    本文提出了一种几何视角来解释变压器的内部机制，主要贡献在于阐明了层归一化如何限制潜在特征并在超球面上塑造注意力机制，通过探测预训练的GPT-2模型验证了该视角的有效性，并提供了对变压器的直观理解。

    

    变压器在自然语言处理领域取得了显著的进展，但理解其内部机制仍然是一个挑战。本文介绍了一种新颖的几何视角，阐明了变压器操作的内部机制。我们的主要贡献是说明了层归一化如何将潜在特征限制在一个超球面上，从而使注意力能够在该表面上塑造单词的语义表示。这种几何视点无缝地连接了迭代改进和上下文嵌入等已知属性。我们通过探测一个预训练的124M参数的GPT-2模型验证了我们的见解。我们的发现揭示了早期层中清晰的查询-键注意力模式，并在更深的层次上建立在先前关于注意头的专门性的观察基础上。利用这些几何见解，我们提出了对变压器的直观理解，将其描绘为塑造轨迹的过程。

    Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajec
    
[^114]: 安全且加速的基于深度强化学习的O-RAN切片: 一种混合迁移学习方法

    Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach. (arXiv:2309.07265v1 [cs.NI])

    [http://arxiv.org/abs/2309.07265](http://arxiv.org/abs/2309.07265)

    本文提出了一种混合迁移学习方法，用于解决在O-RAN网络中使用DRL算法进行闭环控制时遇到的收敛速度慢和性能不稳定的问题。

    

    开放无线接入网络（O-RAN）架构支持智能网络控制算法作为其核心能力之一。数据驱动应用程序利用这些算法通过无线接入网络智能控制器（RIC）来优化无线接入网络（RAN）功能。在O-RAN文献中，深度强化学习（DRL）算法是解决动态无线资源管理问题的主要方法之一。然而，尽管O-RAN RIC引入了诸多好处，但在真实网络部署中，DRL算法的实际采用却落后。这主要是因为DRL代理在部署和面对之前未见过的网络条件时收敛速度慢、性能不稳定。在本文中，我们通过将迁移学习（TL）作为O-RAN功能的DRL基于闭环控制的训练和部署流程的核心组成部分来解决这些挑战。为此，我们提出并设计了一个混合TL辅助的方法

    The open radio access network (O-RAN) architecture supports intelligent network control algorithms as one of its core capabilities. Data-driven applications incorporate such algorithms to optimize radio access network (RAN) functions via RAN intelligent controllers (RICs). Deep reinforcement learning (DRL) algorithms are among the main approaches adopted in the O-RAN literature to solve dynamic radio resource management problems. However, despite the benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms in real network deployments falls behind. This is primarily due to the slow convergence and unstable performance exhibited by DRL agents upon deployment and when facing previously unseen network conditions. In this paper, we address these challenges by proposing transfer learning (TL) as a core component of the training and deployment workflows for the DRL-based closed-loop control of O-RAN functionalities. To this end, we propose and design a hybrid TL-aided a
    
[^115]: 基于地理气象数据的深度神经网络用于长期干旱预测

    Long-term drought prediction using deep neural networks based on geospatial weather data. (arXiv:2309.06212v1 [cs.LG])

    [http://arxiv.org/abs/2309.06212](http://arxiv.org/abs/2309.06212)

    基于地理气象数据的深度神经网络用于长期干旱预测，提出了一种端到端解决方案来预测特定地区干旱概率。采用卷积LSTM和Transformer模型相比其他基线模型能够获得更高的准确性。

    

    在农业实践中，准确预测特定地区干旱概率对于决策具有重要性。尤其对于长期决策，提前一年进行预测至关重要。然而，由于感兴趣区域及其相邻区域内各种因素的复杂相互作用，预测这一概率存在挑战。在本研究中，我们提出了一种基于各种时空神经网络的端到端解决方案来解决这个问题。所考虑的模型主要是根据Palmer干旱严重指数（PDSI）预测感兴趣亚区的干旱强度，利用气候模型的内在因素和见解来提高干旱预测的准确性。比较评估结果表明，与基准梯度提升和逻辑回归解决方案相比，卷积LSTM（ConvLSTM）和Transformer模型的准确性更高。前两种模型取得了令人印象深刻的ROC AUC分数，高达0.90

    The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.  Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 
    
[^116]: 在在线政治偏见方面对预测模型进行定量分析

    Quantitative Analysis of Forecasting Models:In the Aspect of Online Political Bias. (arXiv:2309.05589v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2309.05589](http://arxiv.org/abs/2309.05589)

    本文通过定量分析在线社交媒体上的政治偏见预测模型，提出了一种启发式方法来分类政治倾向，并进行了对现有基准模型的深入分析，以确定最适合预测政治倾向的模型。

    

    理解和减少在线社交媒体平台中的政治偏见是对抗错误信息和回音室效应至关重要的任务。然而，使用计算方法对政治偏见进行时间化描述会面临社交媒体数据中噪声频繁的挑战。虽然现有研究已经探索了各种方法来描述政治偏见，但对于预测政治偏见和预测政治对话在不远的将来如何发展的能力还没有得到广泛研究。在本文中，我们提出了一种启发式方法将社交媒体帖子分类为五种不同的政治倾向类别。由于缺乏关于预测政治偏见的先前工作，我们对现有基准模型进行深入分析，以确定哪种模型最适合预测政治倾向时间序列。我们的方法是在两个具有不同政治观点的社交媒体数据集上利用现有的时间序列预测模型。

    Understanding and mitigating political bias in online social media platforms are crucial tasks to combat misinformation and echo chamber effects. However, characterizing political bias temporally using computational methods presents challenges due to the high frequency of noise in social media datasets. While existing research has explored various approaches to political bias characterization, the ability to forecast political bias and anticipate how political conversations might evolve in the near future has not been extensively studied. In this paper, we propose a heuristic approach to classify social media posts into five distinct political leaning categories. Since there is a lack of prior work on forecasting political bias, we conduct an in-depth analysis of existing baseline models to identify which model best fits to forecast political leaning time series. Our approach involves utilizing existing time series forecasting models on two social media datasets with different politica
    
[^117]: 6G中的联邦学习发展：基于图分析的可信架构

    Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis. (arXiv:2309.05525v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2309.05525](http://arxiv.org/abs/2309.05525)

    该研究提出了一种在6G环境中支持联邦学习的可信架构，利用分布式分类账技术和图神经网络解决了隐私和安全问题，以实现安全聚合和异常检测。

    

    将原生AI支持集成到网络架构中是6G的一个重要目标。联邦学习（FL）作为一种潜在的范式出现，可以在中央服务器的协调下，促进分散的AI模型训练跨越多种设备。然而，在6G环境下，有几个挑战阻碍了其广泛应用，例如恶意攻击和对本地模型更新的隐私监视，以及集中化的缺点。本研究提出了一种支持FL的可信架构，该架构利用分布式分类账技术（DLT）和图神经网络（GNN），包括三个关键特性。首先，引入同态加密的预处理层用于安全地聚合本地模型，保护个体模型的隐私。其次，考虑到预处理层中客户端和节点之间的分布式性质和图结构，利用GNN来识别异常本地模型，增强系统安全性。第三，利用DLT来维护网络中节点之间的信任关系，并提供安全的共识机制。

    Integrating native AI support into the network architecture is an essential objective of 6G. Federated Learning (FL) emerges as a potential paradigm, facilitating decentralized AI model training across a diverse range of devices under the coordination of a central server. However, several challenges hinder its wide application in the 6G context, such as malicious attacks and privacy snooping on local model updates, and centralization pitfalls. This work proposes a trusted architecture for supporting FL, which utilizes Distributed Ledger Technology (DLT) and Graph Neural Network (GNN), including three key features. First, a pre-processing layer employing homomorphic encryption is incorporated to securely aggregate local models, preserving the privacy of individual models. Second, given the distributed nature and graph structure between clients and nodes in the pre-processing layer, GNN is leveraged to identify abnormal local models, enhancing system security. Third, DLT is utilized to d
    
[^118]: 通过概率性图示教学进行示教学习

    Learning from Demonstration via Probabilistic Diagrammatic Teaching. (arXiv:2309.03835v1 [cs.RO])

    [http://arxiv.org/abs/2309.03835](http://arxiv.org/abs/2309.03835)

    本文介绍了一种名为图示教学的示教学习的替代范式，通过要求用户在场景的二维图像上勾勒示范轨迹来教机器人新的技能，并将其合成为三维任务空间中的运动轨迹的生成模型。

    

    示教学习（Learning for Demonstration，LfD）使得机器人可以通过模仿专家示范来获得新技能，允许用户以直观的方式传达他们的指示。最近在LfD领域的进展往往依赖于动作示范教学或远程操作作为用户指定示范的手段。动作示范教学需要对机器人进行物理操纵，而远程操作则需要熟练掌握额外的硬件。本文介绍了一种名为图示教学的LfD的替代范式。图示教学旨在通过要求用户在场景的二维图像上勾勒示范轨迹来教机器人新的技能，然后这些轨迹将被合成为三维任务空间中的运动轨迹的生成模型。此外，我们还提出了用于图示教学的射线追踪概率轨迹学习（RPTL）框架。RPTL从二维图示中提取时间变化的概率密度，并应用射线追踪来寻找相应的区域。

    Learning for Demonstration (LfD) enables robots to acquire new skills by imitating expert demonstrations, allowing users to communicate their instructions in an intuitive manner. Recent progress in LfD often relies on kinesthetic teaching or teleoperation as the medium for users to specify the demonstrations. Kinesthetic teaching requires physical handling of the robot, while teleoperation demands proficiency with additional hardware. This paper introduces an alternative paradigm for LfD called Diagrammatic Teaching. Diagrammatic Teaching aims to teach robots novel skills by prompting the user to sketch out demonstration trajectories on 2D images of the scene, these are then synthesised as a generative model of motion trajectories in 3D task space. Additionally, we present the Ray-tracing Probabilistic Trajectory Learning (RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying probability densities from the 2D sketches, applies ray-tracing to find corresponding regions i
    
[^119]: 使用粒子群优化的多头注意力增强的CNN-LSTM网络进行短期负荷预测

    Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network. (arXiv:2309.03694v1 [cs.LG])

    [http://arxiv.org/abs/2309.03694](http://arxiv.org/abs/2309.03694)

    这项研究提出了一种使用粒子群优化算法、多头注意力机制和计算效率优化框架相结合的方法，用于解决短期负荷预测中的超参数敏感性、解释性不透明和高计算开销等问题。结果表明，该方法在准确性、鲁棒性和计算效率方面具有优势。

    

    短期负荷预测对于电力系统的高效运行和规划至关重要，给定其固有的非线性和动态特性。深度学习在解决这一挑战方面取得了一些进展。然而，这些方法通常面临超参数敏感性、解释性不透明和实时部署的高计算开销等问题。在本文中，我提出了一种克服这些障碍的新方法。我们的方法利用粒子群优化算法自主地探索和优化超参数，利用多头注意力机制识别对准确预测至关重要的显著特征，并采用简化的框架实现计算效率。我们的方法使用真实电力需求数据集进行了严格的评估。结果突显了其在准确性、鲁棒性和计算效率方面的优势。值得注意的是，我们的平均绝对百分比误差...

    Short-term load forecasting is of paramount importance in the efficient operation and planning of power systems, given its inherent non-linear and dynamic nature. Recent strides in deep learning have shown promise in addressing this challenge. However, these methods often grapple with hyperparameter sensitivity, opaqueness in interpretability, and high computational overhead for real-time deployment. In this paper, I propose a novel solution that surmounts these obstacles. Our approach harnesses the power of the Particle-Swarm Optimization algorithm to autonomously explore and optimize hyperparameters, a Multi-Head Attention mechanism to discern the salient features crucial for accurate forecasting, and a streamlined framework for computational efficiency. Our method undergoes rigorous evaluation using a genuine electricity demand dataset. The results underscore its superiority in terms of accuracy, robustness, and computational efficiency. Notably, our Mean Absolute Percentage Error o
    
[^120]: GNN对公平性稳定性的Lipschitz特性表征

    Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])

    [http://arxiv.org/abs/2309.03648](http://arxiv.org/abs/2309.03648)

    论文通过研究GNN的Lipschitz界限表征了GNN对公平性稳定性的影响，尤其是在处理非欧几里得数据和固有偏倚的情况下。同时，该研究对于限制GNN输出的扰动以保障公平性训练提出了挑战。

    

    Lipschitz界限是从鲁棒统计学中借鉴的一种技术，可以限制输出相对于输入的最大变化，考虑到相关的非关键偏倚因素。这是一种高效且可证明的方法，可以检查机器学习模型的输出稳定性，而不会增加额外的计算成本。最近，对于在非欧几里得数据上操作的图神经网络（GNN）引起了广泛的关注。然而，之前没有研究调查GNN的Lipschitz界限以揭示模型输出的稳定性，特别是在处理具有固有偏倚的非欧几里得数据时。由于常见图形数据在GNN训练中存在固有偏差，这给限制由输入偏差引起的GNN输出扰动，从而在训练期间保障公平性，带来了严峻的挑战。最近，尽管Lipschitz常数在控制欧几里得神经网络的稳定性方面有所应用，但精确Lipschitz常数的计算十分困难。

    The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant rem
    
[^121]: 论多智能体非线性滤波和学习的动力学

    On the dynamics of multi agent nonlinear filtering and learning. (arXiv:2309.03557v1 [stat.ML])

    [http://arxiv.org/abs/2309.03557](http://arxiv.org/abs/2309.03557)

    本文研究了具有非线性滤波/学习动力学的多智能体网络系统的行为，并介绍了在分布式和联邦学习场景中的应用。

    

    多智能体系统通过分散一致性寻求动力学来完成高度复杂的学习任务，其在信号处理和计算智能社区引起了极大关注。本文研究了具有非线性滤波/学习动力学的多智能体网络系统的行为。为此，提出了多智能体网络系统中一个智能体的行动的一般表述，并给出了实现协同学习行为的条件。重要的是，还介绍了该推导框架在分布式和联邦学习场景中的应用。

    Multiagent systems aim to accomplish highly complex learning tasks through decentralised consensus seeking dynamics and their use has garnered a great deal of attention in the signal processing and computational intelligence societies. This article examines the behaviour of multiagent networked systems with nonlinear filtering/learning dynamics. To this end, a general formulation for the actions of an agent in multiagent networked systems is presented and conditions for achieving a cohesive learning behaviour is given. Importantly, application of the so derived framework in distributed and federated learning scenarios are presented.
    
[^122]: 图神经网络中的隐私调查：攻击、保护和应用

    A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])

    [http://arxiv.org/abs/2308.16375](http://arxiv.org/abs/2308.16375)

    这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。

    

    随着处理图结构数据的能力和实际应用的改善，图神经网络（GNNs）引起了人们的极大关注。然而，许多这些模型优先考虑高效能表现，如准确性，而缺乏隐私考虑，这是现代社会隐私攻击盛行的重要问题。为了解决这个问题，研究人员开始开发保护隐私的GNNs。尽管取得了进展，但在图领域缺乏对攻击和隐私保护技术的综合概述。在本调查中，我们旨在通过总结针对图数据的攻击、对GNNs中的隐私保护技术进行分类以及审查可用于分析/解决GNNs中隐私问题的数据集和应用程序，填补这一空白。我们还概述了未来研究的潜在方向，以建立更好的隐私保护GNNs。

    Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
    
[^123]: 大型视觉语言模型中幻觉的评估与分析

    Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])

    [http://arxiv.org/abs/2308.15126](http://arxiv.org/abs/2308.15126)

    本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。

    

    最近，大型视觉语言模型（LVLMs）取得了显著的成功。然而，LVLMs仍然存在幻觉问题，这限制了在许多场景中的实用性。幻觉指的是LVLMs响应中不存在于视觉输入中的信息，这可能导致重大后果的潜在风险。目前对LVLMs中的幻觉评估的研究工作有限。在本文中，我们提出了基于大型语言模型（LLM）的幻觉评估框架HaELM。HaELM的性能近似于ChatGPT的95%，并具有低成本、可复现、保护隐私和本地部署等额外优势。利用HaELM，我们评估了当前LVLMs中的幻觉。此外，我们分析了导致LVLMs中幻觉的因素，并提出了缓解幻觉问题的有用建议。

    Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
    
[^124]: BayOTIDE: 基于贝叶斯方法的在线多元时间序列插补与函数分解

    BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. (arXiv:2308.14906v1 [cs.LG])

    [http://arxiv.org/abs/2308.14906](http://arxiv.org/abs/2308.14906)

    BayOTIDE是一种基于贝叶斯方法的在线多元时间序列插补与函数分解模型，通过将多元时间序列视为低秩时序因子组的加权组合来进行插补，同时解决了全局趋势和周期性模式的忽略以及不规则采样时间序列的处理问题。

    

    在真实世界的场景中，如交通和能源，经常观察到具有缺失值和噪声的大规模时间序列数据，甚至是不规则采样。尽管已经提出了许多插补方法，但它们大多数只适用于局部视角，即将长序列拆分为适当大小的批次进行训练。这种局部视角可能使模型忽略全局趋势或周期性模式。更重要的是，几乎所有方法都假设观测值在规则的时间间隔进行采样，并且无法处理来自不同应用的复杂不规则采样时间序列。此外，大多数现有方法都是在离线状态下进行学习的。因此，对于那些有快速到达的流数据的应用来说，它们并不合适。为了克服这些局限性，我们提出了BayOTIDE：基于贝叶斯方法的在线多元时间序列插补与函数分解。

    In real-world scenarios like traffic and energy, massive time-series data with missing values and noises are widely observed, even sampled irregularly. While many imputation methods have been proposed, most of them work with a local horizon, which means models are trained by splitting the long sequence into batches of fit-sized patches. This local horizon can make models ignore global trends or periodic patterns. More importantly, almost all methods assume the observations are sampled at regular time stamps, and fail to handle complex irregular sampled time series arising from different applications. Thirdly, most existing methods are learned in an offline manner. Thus, it is not suitable for many applications with fast-arriving streaming data. To overcome these limitations, we propose \ours: Bayesian Online Multivariate Time series Imputation with functional decomposition. We treat the multivariate time series as the weighted combination of groups of low-rank temporal factors with dif
    
[^125]: 用于计算深度神经网络正则化路径的多目标延续方法

    A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])

    [http://arxiv.org/abs/2308.12044](http://arxiv.org/abs/2308.12044)

    本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。

    

    稀疏性是深度神经网络(DNNs)中非常理想的特征，因为它确保了数值效率，提高了模型的可解释性(由于相关特征的数量较少)和鲁棒性。在基于线性模型的机器学习方法中，众所周知在$\ell^1$范数(即零权重)的最稀疏解和非正则化解之间存在一条连接路径，这条路径被称为正则化路径。最近，通过将经验损失和稀疏性($\ell^1$范数)作为两个冲突的标准，并解决由此产生的多目标优化问题，首次尝试将正则化路径的概念扩展到DNNs。然而，由于$\ell^1$范数的不光滑性和参数数量的高度，从计算的角度来看，这种方法并不是很有效。为了克服这个限制，我们提出了一种算法，可以近似计算整个帕累托曲线

    Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
    
[^126]: 注意力不再是唯一需要的东西了。

    Attention Is Not All You Need Anymore. (arXiv:2308.07661v1 [cs.LG])

    [http://arxiv.org/abs/2308.07661](http://arxiv.org/abs/2308.07661)

    本文提出了一种名为Extractor的插入替代器，用于取代Transformer中的自注意机制，实验证明使用Extractor可以提高Transformer的性能，并且具有更短的计算关键路径。

    

    在最近几年中，流行的Transformer架构在自然语言处理和计算机视觉等许多应用领域取得了巨大成功。许多现有的工作旨在通过性能平衡来减少Transformer中自注意机制的计算和存储复杂度。然而，性能对于Transformer的持续成功至关重要。本文提出了一种用于取代Transformer中自注意机制的插入替代器（Extractor）。实验结果表明，使用Extractor替换自注意机制可以提高Transformer的性能。此外，Extractor具有更短的计算关键路径，因此有潜力比自注意更快。此外，本文还使用可变长离散时间马尔可夫链对文本生成中的序列预测问题进行了建模，并针对我们的插入替代器对Transformer进行了评估。

    In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a drop-in replacement for the self-attention mechanism in the Transformer, called the Extractor, is proposed. Experimental results show that replacing the self-attention mechanism with the Extractor improves the performance of the Transformer. Furthermore, the proposed Extractor has the potential to run faster than the self-attention since it has a much shorter critical path of computation. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our unders
    
[^127]: 使用两个生成对抗网络对混合图像-表格数据进行合成数据生成的方法

    Synthetic data generation method for hybrid image-tabular data using two generative adversarial networks. (arXiv:2308.07573v1 [eess.IV])

    [http://arxiv.org/abs/2308.07573](http://arxiv.org/abs/2308.07573)

    本论文提出了一种使用两个生成对抗网络（GANs）生成合成医学记录的方法，可以生成包含胸部X射线图像和结构化表格数据的多样化合成记录，并保持了图像和数据之间的对应关系。

    

    使用生成对抗网络（GANs）生成合成医学记录已经变得越来越重要，以应对医学领域的隐私问题并促进数据共享。在本文中，我们提出了一种新的方法，使用自动编码GAN（αGAN）和条件表格GAN（CTGAN）生成合成的混合医学记录，其中包括胸部X射线图像（CXRs）和结构化表格数据（包括人体测量数据和实验室测试）。我们的方法涉及在一个大的公共数据库（pDB）上训练一个αGAN模型，以降低CXRs的维度。然后，我们将训练好的GAN模型的编码器应用于原始数据库（oDB）中的图像，以获得潜在向量。这些潜在向量与oDB中的表格数据相结合，并使用这些联合数据来训练CTGAN模型。我们成功地生成了多样化的混合CXRs和表格数据合成记录，并保持了它们之间的对应关系。我们对这个方法进行了评估。

    The generation of synthetic medical records using generative adversarial networks (GANs) has become increasingly important for addressing privacy concerns and promoting data sharing in the medical field. In this paper, we propose a novel method for generating synthetic hybrid medical records consisting of chest X-ray images (CXRs) and structured tabular data (including anthropometric data and laboratory tests) using an auto-encoding GAN ({\alpha}GAN) and a conditional tabular GAN (CTGAN). Our approach involves training a {\alpha}GAN model on a large public database (pDB) to reduce the dimensionality of CXRs. We then applied the trained encoder of the GAN model to the images in original database (oDB) to obtain the latent vectors. These latent vectors were combined with tabular data in oDB, and these joint data were used to train the CTGAN model. We successfully generated diverse synthetic records of hybrid CXR and tabular data, maintaining correspondence between them. We evaluated this
    
[^128]: VQGraph: 图形向量量化用于连接GNN和MLPs

    VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. (arXiv:2308.02117v1 [cs.LG])

    [http://arxiv.org/abs/2308.02117](http://arxiv.org/abs/2308.02117)

    VQGraph是一个框架，通过学习一个强大的图形表示空间，用于连接GNN和MLPs。它采用矢量量化变分自编码器（VQ-VAE）的编码器作为结构感知图标记器，有效地表示底层图的多样化局部结构。通过 VQGraph，可以实现从GNN到MLP的知识转移。

    

    图神经网络（GNNs）进行信息传递，聚合局部邻居以更新节点表示。这种信息传递导致在实际的延迟约束应用程序中存在可扩展性问题。为了解决这个问题，最近的方法采用知识蒸馏（KD）通过模仿GNN的输出来学习计算效率高的多层感知机（MLP）。然而，现有的GNN表示空间可能不足以表示底层图的多样化局部结构，这限制了从GNN到MLP的知识转移。在这里，我们提出了一个新颖的框架VQGraph，用于学习一个强大的图形表示空间，用于连接GNN和MLPs。我们采用一种变体的矢量量化变分自编码器（VQ-VAE）的编码器作为结构感知图标记器，它将多样化的局部结构节点明确表示为大量离散令牌，并构成一个有意义的代码书。配备了学习的代码书，我们提出

    Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propos
    
[^129]: 扩展图神经网络的图评估指标

    Extended Graph Assessment Metrics for Graph Neural Networks. (arXiv:2307.10112v1 [cs.SI])

    [http://arxiv.org/abs/2307.10112](http://arxiv.org/abs/2307.10112)

    本论文提出了扩展的图评估指标（GAMs），适用于回归任务和连续邻接矩阵。主要关注的两个GAMs是同质性和跨类邻域相似度（CCNS）。这些扩展的指标能够在图神经网络中评估图结构，提高模型性能。

    

    当将患者队列重组为所谓的人口图时，最初独立的数据点可以合并成一个相互连接的图结构。利用图神经网络（GNNs），可以使用这种人口图进行医学的下游任务。适合的图结构的构建是学习过程中的一个具有挑战性的步骤，它对模型的性能有着重要的影响。为此，已经引入了不同的图评估指标来评估图结构。然而，这些指标仅适用于分类任务和离散的邻接矩阵，只覆盖了一小部分实际应用。在这项工作中，我们引入了针对回归任务和连续邻接矩阵的扩展图评估指标（GAMs）。我们重点关注两个具体的GAMs：同质性和跨类邻域相似度（CCNS）。我们将GAMs的概念扩展到多个跳跃，并为回归任务定义了同质性。

    When re-structuring patient cohorts into so-called population graphs, initially independent data points can be incorporated into one interconnected graph structure. This population graph can then be used for medical downstream tasks using graph neural networks (GNNs). The construction of a suitable graph structure is a challenging step in the learning pipeline that can have severe impact on model performance. To this end, different graph assessment metrics have been introduced to evaluate graph structures. However, these metrics are limited to classification tasks and discrete adjacency matrices, only covering a small subset of real-world applications. In this work, we introduce extended graph assessment metrics (GAMs) for regression tasks and continuous adjacency matrices. We focus on two GAMs in specific: \textit{homophily} and \textit{cross-class neighbourhood similarity} (CCNS). We extend the notion of GAMs to more than one hop, define homophily for regression tasks, as well as con
    
[^130]: 创建一个支持OpenMP Fortran和C++代码相互翻译的数据集

    Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])

    [http://arxiv.org/abs/2307.07686](http://arxiv.org/abs/2307.07686)

    本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。

    

    在本研究中，我们提出了一个新颖的数据集，用于训练在OpenMP Fortran和C++代码之间进行翻译的机器学习模型。通过精细的代码相似性测试，我们确保了数据集的可靠性和适用性。我们使用定量（CodeBLEU）和定性（人工评估）方法评估了我们数据集的有效性。我们展示了这个数据集如何显著提高大规模语言模型的翻译能力，对于没有先前编码知识的模型，提高了5.1倍，对于具有一定编码熟悉度的模型，提高了9.9倍。我们的工作突显了这个数据集在高性能计算的代码翻译领域的潜力。

    In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
    
[^131]: 弱监督定位对比学习：肝硬化分类的应用

    Weakly-supervised positional contrastive learning: application to cirrhosis classification. (arXiv:2307.04617v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.04617](http://arxiv.org/abs/2307.04617)

    本研究提出了一种弱监督定位对比学习策略，在医学应用中使用大量弱标签图像进行肝硬化预测。这种策略将每个2D切片的空间上下文和弱标签整合起来，通过使用通用基于核的损失函数实现高效学习。

    

    大型医学影像数据集可以通过低置信度的弱标签（例如放射学评分）进行廉价快速的注释。而高置信度的标签（如基于组织学的诊断）很少且昂贵。预训练策略，如对比学习方法，可以利用未标记或弱标注的数据集。然而，这些方法通常需要较大的批处理大小，这在大型3D图像的全分辨率情况下存在困难，因为GPU内存有限。尽管如此，关于每个2D切片的空间上下文的体积信息对于某些医学应用非常重要。在这项研究中，我们提出了一种高效的弱监督定位对比学习策略，通过使用通用基于核的损失函数将每个2D切片的空间上下文和弱标签进行整合。我们通过使用大量弱标签图像（即放射学低置信度标注）来说明我们的方法在肝硬化预测中的应用。

    Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations,
    
[^132]: 深度学习在端到端自动驾驶中的最新进展：一项调研

    Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey. (arXiv:2307.04370v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2307.04370](http://arxiv.org/abs/2307.04370)

    本文调研了深度学习在端到端自动驾驶中的最新进展，并提供了一种基于神经网络的自动驾驶任务分类体系。研究分析了端到端自动驾驶的关键挑战，并列举了不同的研究方法和核心功能。

    

    端到端驾驶是一种有前景的范例，它避免了模块化系统的缺点，如复杂性过高和误差传播的倾向。自动驾驶通过预先主动识别关键事件来超越传统交通模式，确保乘客的安全并为他们提供舒适的交通，特别是在高度随机和多变的交通环境中。本文对端到端自动驾驶技术进行了全面的回顾，提供了一种在端到端方式中使用神经网络的自动驾驶任务分类体系，包括从感知到控制的整个驾驶过程，并解决了实际应用中遇到的关键挑战。本文对端到端自动驾驶的最新发展进行了分析，并根据基本原理、方法论和核心功能对研究进行了分类。

    End-to-End driving is a promising paradigm as it circumvents the drawbacks associated with modular systems, such as their overwhelming complexity and propensity for error propagation. Autonomous driving transcends conventional traffic patterns by proactively recognizing critical events in advance, ensuring passengers' safety and providing them with comfortable transportation, particularly in highly stochastic and variable traffic settings. This paper presents a comprehensive review of the End-to-End autonomous driving stack. It provides a taxonomy of automated driving tasks wherein neural networks have been employed in an End-to-End manner, encompassing the entire driving process from perception to control, while addressing key challenges encountered in real-world applications. Recent developments in End-to-End autonomous driving are analyzed, and research is categorized based on underlying principles, methodologies, and core functionality. These categories encompass sensorial input, m
    
[^133]: 利用对比学习预测癌症预后的基因表达值

    Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression Values. (arXiv:2306.06276v1 [cs.LG])

    [http://arxiv.org/abs/2306.06276](http://arxiv.org/abs/2306.06276)

    该论文提出应用对比学习方法，从有限的数据样本中学习基因表达数据的良好特征表示，并应用于Cox模型中，可以显著提高癌症预后的预测性能。

    

    最近开发了多种人工神经网络（ANNs）作为Cox比例风险模型，以基于肿瘤转录组预测癌症预后。然而，它们并没有表现出比传统有规则化的Cox回归显着更好的性能。在有限数量的数据样本和高维特征空间存在的情况下，训练具有高预测能力的ANN是具有挑战性的。图像分类的最新进展表明，对比学习可以通过从有限数量的数据样本中学习良好的特征表示来促进进一步的学习任务。在本文中，我们将监督对比学习应用于肿瘤基因表达和临床数据，以在低维空间中学习特征表示。然后，我们使用这些学习到的特征来训练Cox模型，以预测癌症预后。使用来自The Cancer Genome Atlas（TCGA）的数据，我们证明了我们基于对比学习的Cox模型能够通过学习基因表达数据的良好特征表征，显著提高癌症预后预测的性能。

    Several artificial neural networks (ANNs) have recently been developed as the Cox proportional hazard model for predicting cancer prognosis based on tumor transcriptome. However, they have not demonstrated significantly better performance than the traditional Cox regression with regularization. Training an ANN with high prediction power is challenging in the presence of a limited number of data samples and a high-dimensional feature space. Recent advancements in image classification have shown that contrastive learning can facilitate further learning tasks by learning good feature representation from a limited number of data samples. In this paper, we applied supervised contrastive learning to tumor gene expression and clinical data to learn feature representations in a low-dimensional space. We then used these learned features to train the Cox model for predicting cancer prognosis. Using data from The Cancer Genome Atlas (TCGA), we demonstrated that our contrastive learning-based Cox 
    
[^134]: ElectroCardioGuard：通过神经网络防止心电图数据库中患者误识别

    ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks. (arXiv:2306.06196v1 [cs.LG])

    [http://arxiv.org/abs/2306.06196](http://arxiv.org/abs/2306.06196)

    本研究提出了一种基于神经网络的小而高效模型，用于确定两个心电图(ECG)是否来自同一患者。同时，还提出了一种方法，利用该模型检测记录-分配错误，实现了在现实场景中的应用，并在新的ECG数据集上进行了评估。

    

    心电图(ECG)通常被心脏病专家用于检测与心脏相关的病理情况，而可靠的ECG集合对于确诊非常重要。然而，在临床实践中，将记录的ECG分配给错误的患者可能会不经意地发生。本文与一家临床和研究机构合作，该机构认识到这一挑战并联系我们，我们提出了一项研究来解决这个问题。我们提出了一种小巧高效的基于神经网络的模型，用于确定两个ECG是否来自同一患者。我们的模型展现了很强的泛化能力，并在利用760倍更少的参数的情况下，在PTB-XL上实现了最新的画廊探针患者识别表现。此外，我们提出了一种技术，利用我们的模型来检测记录-分配错误，展示了它在一个现实场景中的适用性。最后，我们对新收集的ECG数据集进行了评估。

    Electrocardiograms (ECGs) are commonly used by cardiologists to detect heart-related pathological conditions. Reliable collections of ECGs are crucial for precise diagnosis. However, in clinical practice, the assignment of captured ECG recordings to incorrect patients can occur inadvertently. In collaboration with a clinical and research facility which recognized this challenge and reached out to us, we present a study that addresses this issue. In this work, we propose a small and efficient neural-network based model for determining whether two ECGs originate from the same patient. Our model demonstrates great generalization capabilities and achieves state-of-the-art performance in gallery-probe patient identification on PTB-XL while utilizing 760x fewer parameters. Furthermore, we present a technique leveraging our model for detection of recording-assignment mistakes, showcasing its applicability in a realistic scenario. Finally, we evaluate our model on a newly collected ECG dataset
    
[^135]: ChatGPT信息的图神经网络用于股票价格预测

    ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.03763](http://arxiv.org/abs/2306.03763)

    该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。

    

    ChatGPT已在各种自然语言处理（NLP）任务中展示了出色的能力。然而，它从时间文本数据（尤其是财经新闻）推断动态网络结构的潜力仍是一个未开发的领域。在这项研究中，我们介绍了一个新的框架，利用ChatGPT的图推断能力来增强图神经网络（GNN）。我们的框架巧妙地从文本数据中提取出不断变化的网络结构，并将这些网络结构融合到图神经网络中，进行后续的预测任务。股票价格预测的实验结果表明，我们的模型始终优于基于深度学习的最新基准。此外，基于我们模型的产出构建的组合展示出更高的年化累计回报、更低的波动性和最大回撤。这种卓越表现突显了ChatGPT用于基于文本的网络推断和金融预测应用的潜力。

    ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
    
[^136]: 重新审视广义p-Laplacian正则化框架图卷积网络: 收敛性、能量动态和非线性扩散训练的研究

    Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion. (arXiv:2305.15639v1 [cs.LG])

    [http://arxiv.org/abs/2305.15639](http://arxiv.org/abs/2305.15639)

    本文全面分析了基于图p-Laplacian的Framelet网络，证明了其具有良好的收敛性和能量动态，并能够通过广义非线性扩散过程进行多种训练。同时，该模型能够适应同质和异质数据，避免过度平滑问题。

    

    本文对基于图p-Laplacian的Framelet网络（pL-UFG）进行了全面的理论分析，以建立对其性质的深入理解。我们首先对Framelet卷积后集成p-Laplacian的隐式层进行了收敛性分析，提供了关于pL-UFG渐近行为的洞察力。通过探索pL-UFG的广义Dirichlet能量，我们证明了Dirichlet能量保持非零，确保在pL-UFG接近收敛时避免过度平滑问题。此外，我们通过动态能量视角阐明了pL-UFG中的隐式层与图Framelets协同工作，增强了该模型对同质和异质数据的适应性。值得注意的是，我们证明了这个隐式层可以被解释成广义的非线性扩散过程，使得可以使用多种不同的训练方案。这些多方面的分析导致了统一的结论，提供了新的洞见。

    This work presents a comprehensive theoretical analysis of graph p-Laplacian based framelet network (pL-UFG) to establish a solid understanding of its properties. We begin by conducting a convergence analysis of the p-Laplacian based implicit layer integrated after the framelet convolution, providing insights into the asymptotic behavior of pL-UFG. By exploring the generalized Dirichlet energy of pL-UFG, we demonstrate that the Dirichlet energy remains non-zero, ensuring the avoidance of over-smoothing issues in pL-UFG as it approaches convergence. Furthermore, we elucidate the dynamic energy perspective through which the implicit layer in pL-UFG synergizes with graph framelets, enhancing the model's adaptability to both homophilic and heterophilic data. Remarkably, we establish that the implicit layer can be interpreted as a generalized non-linear diffusion process, enabling training using diverse schemes. These multifaceted analyses lead to unified conclusions that provide novel insi
    
[^137]: 滞后多因子模型中领先滞后关系的鲁棒检测

    Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models. (arXiv:2305.06704v1 [stat.ML])

    [http://arxiv.org/abs/2305.06704](http://arxiv.org/abs/2305.06704)

    该论文提出了一种基于聚类的鲁棒检测滞后多因子模型中的领先滞后关系方法，并使用各种聚类技术和相似度度量方法实现了对领先滞后估计的聚合，从而强化了对原始宇宙中的一致关系的识别。

    

    在多元时间序列系统中，通过发现数据中固有的领先滞后关系，可以获得关键信息，这指的是两个相对时间互移的时间序列之间的依赖关系，可以用于控制、预测或聚类。我们开发了一种基于聚类的方法，用于鲁棒检测滞后多因子模型中的领先滞后关系。在我们的框架中，所设想的管道接收一组时间序列作为输入，并使用滑动窗口方法从每个输入时间序列中提取一组子序列时间序列。然后，我们应用各种聚类技术（例如K-means++和谱聚类），采用各种成对相似性度量，包括非线性的相似性度量。一旦聚类被提取出来，跨聚类的领先滞后估计被聚合起来，以增强对原始宇宙中一致关系的识别。由于多

    In multivariate time series systems, key insights can be obtained by discovering lead-lag relationships inherent in the data, which refer to the dependence between two time series shifted in time relative to one another, and which can be leveraged for the purposes of control, forecasting or clustering. We develop a clustering-driven methodology for the robust detection of lead-lag relationships in lagged multi-factor models. Within our framework, the envisioned pipeline takes as input a set of time series, and creates an enlarged universe of extracted subsequence time series from each input time series, by using a sliding window approach. We then apply various clustering techniques (e.g, K-means++ and spectral clustering), employing a variety of pairwise similarity measures, including nonlinear ones. Once the clusters have been extracted, lead-lag estimates across clusters are aggregated to enhance the identification of the consistent relationships in the original universe. Since multi
    
[^138]: ChatGraph: 通过将ChatGPT的知识转换为图形来实现可解释的文本分类

    ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v1 [cs.CL])

    [http://arxiv.org/abs/2305.03513](http://arxiv.org/abs/2305.03513)

    ChatGraph通过将ChatGPT的知识转换为图形，提高了文本分类的可解释性和性能

    

    ChatGPT作为最近推出的大型语言模型（LLM），在各种自然语言处理（NLP）任务中展现出卓越的性能。然而，存在两个主要限制阻碍了它的潜在应用：（1）在下游任务上微调的不灵活性和（2）在决策过程中缺乏可解释性。为了解决这些限制，我们提出了一种新颖的框架，利用ChatGPT的能力来进行特定任务（如文本分类），同时提高其可解释性。

    ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: (1) the inflexibility of finetuning on downstream tasks and (2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four datasets. The result shows that our method can significantly improve the performance compared to directly utilizing Cha
    
[^139]: 节点特征增强改进网络对齐

    Node Feature Augmentation Vitaminizes Network Alignment. (arXiv:2304.12751v1 [cs.SI])

    [http://arxiv.org/abs/2304.12751](http://arxiv.org/abs/2304.12751)

    本研究提出了Grad-Align+方法，通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法，解决了NA方法缺乏额外信息的问题。

    

    网络对齐（NA）是通过给定网络的拓扑和/或特征信息来发现多个网络之间的节点对应关系的任务。虽然NA方法在各种场景下取得了显著的成功，但其有效性并不总是有额外信息，如先前的锚点链接和/或节点特征。为了解决这个实际的挑战，我们提出了Grad-Align+，这是一种新颖的NA方法，建立在最近一种最先进的NA方法Grad-Align之上，Grad-Align+仅逐步发现部分节点对，直到找到所有节点对。在设计Grad-Align+时，我们考虑如何通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法。为了实现这个目标，我们开发了由三个关键组成部分组成的Grad-Align+：基于中心性的节点特征增强（CNFA）、图切片生成和优化节点嵌入特征（ONIFE）。

    Network alignment (NA) is the task of discovering node correspondences across multiple networks using topological and/or feature information of given networks. Although NA methods have achieved remarkable success in a myriad of scenarios, their effectiveness is not without additional information such as prior anchor links and/or node features, which may not always be available due to privacy concerns or access restrictions. To tackle this practical challenge, we propose Grad-Align+, a novel NA method built upon a recent state-of-the-art NA method, the so-called Grad-Align, that gradually discovers only a part of node pairs until all node pairs are found. In designing Grad-Align+, we account for how to augment node features in the sense of performing the NA task and how to design our NA method by maximally exploiting the augmented node features. To achieve this goal, we develop Grad-Align+ consisting of three key components: 1) centrality-based node feature augmentation (CNFA), 2) graph
    
[^140]: 基于KNN的修正Medoid-Shift的社区检测

    Community Detection Using Revised Medoid-Shift Based on KNN. (arXiv:2304.09512v1 [cs.SI])

    [http://arxiv.org/abs/2304.09512](http://arxiv.org/abs/2304.09512)

    在本文中，作者提出了一种名为修正Medoid-Shift（RMS）的新聚类算法，用于社区检测，该算法可以更好地解决社交网络中的问题。

    

    随着社交网络的兴起，社区检测成为了一个重要的问题。虽然均值漂移是一种出色的聚类算法，但由于均值漂移算法只能处理具有坐标的数据，而社区检测问题中的数据大多以图的形式表示，可以视为具有距离矩阵（或相似度矩阵）的数据，因此均值漂移算法不能直接用于社区检测。幸运的是，提出了一种新的聚类算法 Medoid-Shift，该算法保留了均值漂移的优点，并可以应用于基于距离矩阵的问题，如社区检测。Medoid-Shift算法的一个缺点是可能在由距离参数定义的邻域区域内没有数据点。 为了更好地处理社区检测问题，因此在本文中提出了一种名为修正Medoid-Shift（RMS）的新算法。 在寻找下一个中心点的过程中，RMS算法基于邻域的定义。

    Community detection becomes an important problem with the booming of social networks. As an excellent clustering algorithm, Mean-Shift can not be applied directly to community detection, since Mean-Shift can only handle data with coordinates, while the data in the community detection problem is mostly represented by a graph that can be treated as data with a distance matrix (or similarity matrix). Fortunately, a new clustering algorithm called Medoid-Shift is proposed. The Medoid-Shift algorithm preserves the benefits of Mean-Shift and can be applied to problems based on distance matrix, such as community detection. One drawback of the Medoid-Shift algorithm is that there may be no data points within the neighborhood region defined by a distance parameter. To deal with the community detection problem better, a new algorithm called Revised Medoid-Shift (RMS) in this work is thus proposed. During the process of finding the next medoid, the RMS algorithm is based on a neighborhood defined
    
[^141]: 马尔可夫决策过程中的合规异策评估

    Conformal Off-Policy Evaluation in Markov Decision Processes. (arXiv:2304.02574v1 [cs.LG])

    [http://arxiv.org/abs/2304.02574](http://arxiv.org/abs/2304.02574)

    本论文提出了一种基于合规预测的异策评估方法，能够以一定的确定性水平输出包含目标策略的真实奖励的区间，并提出了不同的处理分布偏移方法，其中一些方法在保证合规性的前提下实现了最先进的性能。

    

    强化学习旨在从数据中识别和评估有效的控制策略。在许多实际应用中，学习者不能进行实验，也不能以在线方式获取数据（在实验费用高昂、风险高或不道德的情况下，就会出现这种情况）。针对这种应用，必须使用在不同策略下收集的历史数据（行为策略）来估计给定策略（目标策略）的奖励。大多数针对这种学习任务的方法，即异策评估（OPE），都没有准确性和确定性保证。我们提出了一种基于合规预测的新型OPE方法，该方法输出一个包含目标策略的真实奖励的区间，同时具有一定的确定性水平。OPE中的主要挑战来自于目标策略和行为策略之间的差异引起的分布偏移。我们提出并经验性地评估了不同处理这种偏移的方法。其中一些方法在保证估计的奖励区间的合规性的同时，在基准环境中实现了最先进的性能。

    Reinforcement Learning aims at identifying and evaluating efficient control policies from data. In many real-world applications, the learner is not allowed to experiment and cannot gather data in an online manner (this is the case when experimenting is expensive, risky or unethical). For such applications, the reward of a given policy (the target policy) must be estimated using historical data gathered under a different policy (the behavior policy). Most methods for this learning task, referred to as Off-Policy Evaluation (OPE), do not come with accuracy and certainty guarantees. We present a novel OPE method based on Conformal Prediction that outputs an interval containing the true reward of the target policy with a prescribed level of certainty. The main challenge in OPE stems from the distribution shift due to the discrepancies between the target and the behavior policies. We propose and empirically evaluate different ways to deal with this shift. Some of these methods yield conform
    
[^142]: 使用线性规划在马尔可夫决策过程上进行在线强化学习

    Online Reinforcement Learning in Markov Decision Process Using Linear Programming. (arXiv:2304.00155v1 [cs.LG])

    [http://arxiv.org/abs/2304.00155](http://arxiv.org/abs/2304.00155)

    本论文提出了一种在未知转移矩阵和固定但未知分布的情况下进行在线MDP学习的简单而高效的方法，可以实现更紧的遗憾界，并通过置信区间框架改进了现有算法。

    

    本文考虑了具有未知转移矩阵和固定但未知分布的随机奖励的情况下，马尔可夫决策过程中的在线强化学习。学习者旨在通过与环境交互来学习最优策略并在有限的时间内最小化他们的遗憾。我们设计了一种简单而高效的模型算法，通过保持过渡和奖励函数的置信区间并使用占用度量将在线MDP与线性规划相连接，实现了$\tilde{O}(LX\sqrt{TA})$的高概率遗憾界。它比现有的使用类似置信区间框架的算法实现了更紧的遗憾界并改善了计算效率。

    We consider online reinforcement learning in episodic Markov decision process (MDP) with an unknown transition matrix and stochastic rewards drawn from a fixed but unknown distribution. The learner aims to learn the optimal policy and minimize their regret over a finite time horizon through interacting with the environment. We devise a simple and efficient model-based algorithm that achieves $\tilde{O}(LX\sqrt{TA})$ regret with high probability, where $L$ is the episode length, $T$ is the number of episodes, and $X$ and $A$ are the cardinalities of the state space and the action space, respectively. The proposed algorithm, which is based on the concept of "optimism in the face of uncertainty", maintains confidence sets of transition and reward functions and uses occupancy measures to connect the online MDP with linear programming. It achieves a tighter regret bound compared to the existing works that use a similar confidence sets framework and improves the computational effort compared
    
[^143]: 在合成晶体数据集上训练的神经网络可以从ICSD粉末X射线衍射图中提取结构信息

    Neural networks trained on synthetically generated crystals can extract structural information from ICSD powder X-ray diffractograms. (arXiv:2303.11699v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2303.11699](http://arxiv.org/abs/2303.11699)

    本研究提出了一种使用合成晶体数据集，通过神经网络从ICSD粉末X射线衍射图中提取结构信息的方法，并且在空间群分类任务上，取得了比直接在ICSD晶体上进行训练更高的准确度。

    

    机器学习技术已成功地应用于从粉末X射线衍射图中提取结构信息，如晶体空间群。然而，直接在ICSD等数据库的模拟衍射图上进行训练存在挑战，原因是其规模有限、类别不均匀并且偏向某些结构类型。我们提出了一种替代方法，即通过利用每个空间群的对称操作，在随机坐标下生成合成晶体。基于这种方法，我们演示了使用深度ResNet类模型进行在线训练，每小时最多可生成少量百万个唯一的合成衍射图。针对我们选择的空间群分类任务，在大多数空间群的未见ICSD结构类型上，我们实现了79.9%的测试准确性。这超过了直接在ICSD晶体上进行训练的当前最先进方法的56.1%准确性。我们的结果证明合成晶体数据集可用于通过神经网络提取ICSD粉末X射线衍射图中的结构信息。

    Machine learning techniques have successfully been used to extract structural information such as the crystal space group from powder X-ray diffractograms. However, training directly on simulated diffractograms from databases such as the ICSD is challenging due to its limited size, class-inhomogeneity, and bias toward certain structure types. We propose an alternative approach of generating synthetic crystals with random coordinates by using the symmetry operations of each space group. Based on this approach, we demonstrate online training of deep ResNet-like models on up to a few million unique on-the-fly generated synthetic diffractograms per hour. For our chosen task of space group classification, we achieved a test accuracy of 79.9% on unseen ICSD structure types from most space groups. This surpasses the 56.1% accuracy of the current state-of-the-art approach of training on ICSD crystals directly. Our results demonstrate that synthetically generated crystals can be used to extract
    
[^144]: 物理信息神经网络（PINNs）和它们的时间分解的因果扫描策略的统一可扩展框架

    A unified scalable framework for causal sweeping strategies for Physics-Informed Neural Networks (PINNs) and their temporal decompositions. (arXiv:2302.14227v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2302.14227](http://arxiv.org/abs/2302.14227)

    这篇论文提出了一个统一可扩展框架，用于解决物理信息神经网络（PINNs）在优化过程中出现的挑战，并介绍了一种新的堆叠分解方法来加快计算速度。

    

    物理信息神经网络（PINNs）作为求解偏微分方程（PDE）的一种方法，在计算科学和工程（CS&E）领域引起了广泛的关注。然而，近期一个有趣的话题是探索各种训练（即优化）挑战 - 特别是在优化景观中陷入了差的局部最小点时，导致了一个PDE近似给出了一个较差甚至是微不足道的解，而且不需要数据以前进的时间相关PDE的求解时，这个问题还存在于一些领域分解策略中，如使用XPINNs的时间分解。我们提供了不同训练挑战的示例和解释，以及它们的原因，以及它们如何与信息传播和时间分解有关。然后，我们提出了一种新的堆叠分解方法，以弥合时间步进PINNs和XPINNs之间的差距。我们还通过使用迁移学习引入了显著的计算速度提升。

    Physics-informed neural networks (PINNs) as a means of solving partial differential equations (PDE) have garnered much attention in the Computational Science and Engineering (CS&E) world. However, a recent topic of interest is exploring various training (i.e., optimization) challenges - in particular, arriving at poor local minima in the optimization landscape results in a PINN approximation giving an inferior, and sometimes trivial, solution when solving forward time-dependent PDEs with no data. This problem is also found in, and in some sense more difficult, with domain decomposition strategies such as temporal decomposition using XPINNs. We furnish examples and explanations for different training challenges, their cause, and how they relate to information propagation and temporal decomposition. We then propose a new stacked-decomposition method that bridges the gap between time-marching PINNs and XPINNs. We also introduce significant computational speed-ups by using transfer learnin
    
[^145]: 椭圆型偏微分方程学习在数据效率上是可靠的

    Elliptic PDE learning is provably data-efficient. (arXiv:2302.12888v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12888](http://arxiv.org/abs/2302.12888)

    该论文提供了椭圆型偏微分方程学习中的数据效率理论保证，通过利用随机数值线性代数和PDE理论，实现了对于3D均匀椭圆型PDE解算符的数据高效恢复，并在训练数据集大小上以指数收敛率的误差。

    

    PDE学习是一个将物理学和机器学习相结合的新兴领域，旨在从实验数据中恢复未知的物理系统。虽然深度学习模型通常需要大量的训练数据，但最近的PDE学习技术在数据有限的情况下取得了令人瞩目的结果。然而，这些结果仍然是经验性的。我们的工作在PDE学习中提供了对所需的输入-输出训练对数量的理论保证。具体而言，我们利用随机数值线性代数和PDE理论推导出了一种具有可靠数据效率的算法，该算法从输入-输出数据中恢复3D均匀椭圆型PDE的解算符，并以极高的成功概率实现了与训练数据集大小指数收敛率的误差。

    PDE learning is an emerging field that combines physics and machine learning to recover unknown physical systems from experimental data. While deep learning models traditionally require copious amounts of training data, recent PDE learning techniques achieve spectacular results with limited data availability. Still, these results are empirical. Our work provides theoretical guarantees on the number of input-output training pairs required in PDE learning. Specifically, we exploit randomized numerical linear algebra and PDE theory to derive a provably data-efficient algorithm that recovers solution operators of 3D uniformly elliptic PDEs from input-output data and achieves an exponential convergence rate of the error with respect to the size of the training dataset with an exceptionally high probability of success.
    
[^146]: 近地表风的算法幻觉：使用生成对抗网络进行统计降尺度的研究

    Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales. (arXiv:2302.08720v2 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2302.08720](http://arxiv.org/abs/2302.08720)

    本论文将新兴的图像超分辨率技术应用于统计降尺度任务，具体探索了基于生成对抗网络的算法在模拟北美地区地表风中的应用。通过使用非理想化的低分辨率和高分辨率输入数据，该方法克服了共享尺度不匹配的问题，并通过评估空间功率谱等指标来评估模型的技能。

    

    本论文探讨了将图像超分辨率（SR）中新兴的机器学习方法应用于统计降尺度任务。我们特别关注卷积神经网络的生成对抗网络（GANs）。我们的GANs是通过对低分辨率（LR）输入进行条件训练来生成模拟北美地区 Weather Research and Forecasting（WRF）模型的高分辨率（HR）地表风。与传统的SR模型不同，LR输入在WRF模拟中使用了非理想化的LR和HR配对，导致由于内部变异引起的共享尺度不匹配。我们的研究基于当前基于SR的统计降尺度，并尝试了计算机视觉领域的新颖频率分离（FS）方法。为了评估SR模型的技能，我们精选评估指标，并关注基于空间功率谱的性能度量。我们的分析揭示了GAN配置如何影响模型的性能。

    This paper explores the application of emerging machine learning methods from image super-resolution (SR) to the task of statistical downscaling. We specifically focus on convolutional neural network-based Generative Adversarial Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to generate high-resolution (HR) surface winds emulating Weather Research and Forecasting (WRF) model simulations over North America. Unlike traditional SR models, where LR inputs are idealized coarsened versions of the HR images, WRF emulation involves using non-idealized LR and HR pairs resulting in shared-scale mismatches due to internal variability. Our study builds upon current SR-based statistical downscaling by experimenting with a novel frequency-separation (FS) approach from the computer vision field. To assess the skill of SR models, we carefully select evaluation metrics, and focus on performance measures based on spatial power spectra. Our analyses reveal how GAN configurations 
    
[^147]: 一招两得：粗粒化分子动力学的扩散模型和力场

    Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics. (arXiv:2302.00600v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00600](http://arxiv.org/abs/2302.00600)

    本文通过训练扩散生成模型来学习粗粒化分子动力学的力场，无需使用任何力场输入，从而实现了对生物过程的精确模拟，并展示了在多个蛋白质模拟中具有优越性能。

    

    粗粒化（CG）分子动力学使得在原子分辨率下无法解决的生物过程可以得以研究。然而，准确学习CG力场仍然是一个挑战。在本文中，我们利用基于评分的生成模型、力场和分子动力学之间的联系，学习了一个CG力场，而在训练过程中不需要任何力场输入。具体地，我们对来自分子动力学模拟的蛋白质结构进行了扩散生成模型的训练，我们展示了它的评分函数近似一个可以直接用于模拟CG分子动力学的力场。尽管相比以前的工作，我们的方法具有大大简化的训练设置，但我们证明了我们的方法在几个小型到中型蛋白质模拟中具有改进的性能，能够重现CG平衡分布，并保持蛋白质折叠等全原子模拟的动力学。

    Coarse-grained (CG) molecular dynamics enables the study of biological processes at temporal and spatial scales that would be intractable at an atomistic resolution. However, accurately learning a CG force field remains a challenge. In this work, we leverage connections between score-based generative models, force fields and molecular dynamics to learn a CG force field without requiring any force inputs during training. Specifically, we train a diffusion generative model on protein structures from molecular dynamics simulations, and we show that its score function approximates a force field that can directly be used to simulate CG molecular dynamics. While having a vastly simplified training setup compared to previous work, we demonstrate that our approach leads to improved performance across several small- to medium-sized protein simulations, reproducing the CG equilibrium distribution, and preserving dynamics of all-atom simulations such as protein folding events.
    
[^148]: RouteNet-Fermi: 使用图神经网络进行网络建模

    RouteNet-Fermi: Network Modeling with Graph Neural Networks. (arXiv:2212.12070v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2212.12070](http://arxiv.org/abs/2212.12070)

    RouteNet-Fermi是一种基于图神经网络的自定义模型，与传统的排队理论相比，在存在真实流量模型的情况下具有更高的准确性。它能够准确预测网络的延迟、抖动和丢包情况。

    

    网络模型是现代网络的重要组成部分，广泛用于网络规划和优化。然而，随着网络规模和复杂性的增加，一些模型存在限制，如排队理论模型中对马尔可夫流量的假设，以及网络模拟器的高计算成本。机器学习的最新进展，如图神经网络（GNN），正在推动一代新的数据驱动网络模型，能够学习复杂的非线性行为。在本文中，我们提出了一种名为RouteNet-Fermi的自定义GNN模型，它与排队理论具有相同的目标，并且在存在真实流量模型的情况下准确性更高。该模型可以准确预测网络的延迟、抖动和丢包情况。我们在不断增长的网络规模（最大达到300个节点）和包括具有混合流量特性的样本（如复杂的非马尔可夫模型）中测试了RouteNet-Fermi模型。

    Network models are an essential block of modern networks. For example, they are widely used in network planning and optimization. However, as networks increase in scale and complexity, some models present limitations, such as the assumption of Markovian traffic in queuing theory models, or the high computational cost of network simulators. Recent advances in machine learning, such as Graph Neural Networks (GNN), are enabling a new generation of network models that are data-driven and can learn complex non-linear behaviors. In this paper, we present RouteNet-Fermi, a custom GNN model that shares the same goals as Queuing Theory, while being considerably more accurate in the presence of realistic traffic models. The proposed model predicts accurately the delay, jitter, and packet loss of a network. We have tested RouteNet-Fermi in networks of increasing size (up to 300 nodes), including samples with mixed traffic profiles -- e.g., with complex non-Markovian models -- and arbitrary routin
    
[^149]: SoftCTC -- 利用软伪标签进行半监督学习的文本识别

    SoftCTC -- Semi-Supervised Learning for Text Recognition using Soft Pseudo-Labels. (arXiv:2212.02135v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02135](http://arxiv.org/abs/2212.02135)

    本文研究了半监督学习在文本识别中的应用，提出了一种新的损失函数 SoftCTC，可以同时考虑多个转录变体，避免了基于置信度的过滤步骤，实验表明其在手写识别任务上与过滤流水线性能相当，并且计算效率更高。

    

    本文探索了序列任务的半监督训练，如光学字符识别或自动语音识别。我们提出了一种新的损失函数 SoftCTC，它是CTC的扩展，可以同时考虑多个转录变体。这使得可以省略基于置信度的过滤步骤，该步骤对于伪标签方法的半监督学习是一个重要组成部分。我们在具有挑战性的手写识别任务上展示了我们方法的有效性，并得出结论，SoftCTC可以与精调的基于过滤的流水线的性能相匹配。我们还评估了SoftCTC的计算效率，得出结论它在训练多个转录变体方面比朴素的CTC方法更高效，并且我们公开了我们的GPU实现。

    This paper explores semi-supervised training for sequence tasks, such as Optical Character Recognition or Automatic Speech Recognition. We propose a novel loss function $\unicode{x2013}$ SoftCTC $\unicode{x2013}$ which is an extension of CTC allowing to consider multiple transcription variants at the same time. This allows to omit the confidence based filtering step which is otherwise a crucial component of pseudo-labeling approaches to semi-supervised learning. We demonstrate the effectiveness of our method on a challenging handwriting recognition task and conclude that SoftCTC matches the performance of a finely-tuned filtering based pipeline. We also evaluated SoftCTC in terms of computational efficiency, concluding that it is significantly more efficient than a na\"ive CTC-based approach for training on multiple transcription variants, and we make our GPU implementation public.
    
[^150]: 使用自编码器进行无监督的概念漂移取消学习

    Unsupervised Unlearning of Concept Drift with Autoencoders. (arXiv:2211.12989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12989](http://arxiv.org/abs/2211.12989)

    本论文提出了一种基于自编码器的无监督的概念漂移取消学习方法，通过在全局级别引入自编码器，可以避免重新训练或调整学习模型，从而实现对概念漂移的适应。

    

    概念漂移是指影响未来样本数据流的数据分布变化。因此，对数据流进行操作的学习模型可能会变得过时，需要昂贵且困难的调整，如重新训练或适应。现有方法通常实施局部概念漂移适应方案，其中要么使用增量学习模型，要么在漂移检测机制触发警报时完全重新训练模型。本文提出了一种替代方法，介绍了一种基于自编码器的无监督和模型无关的全局级别概念漂移适应方法。具体而言，所提出的方法旨在“取消学习”概念漂移，而无需重新训练或调整任何在数据上操作的学习模型。在两个应用领域进行了广泛的实验评估。我们考虑了一个具有30个以上模型的真实水配送网络，其中我们...

    Concept drift refers to a change in the data distribution affecting the data stream of future samples. Consequently, learning models operating on the data stream might become obsolete, and need costly and difficult adjustments such as retraining or adaptation. Existing methods usually implement a local concept drift adaptation scheme, where either incremental learning of the models is used, or the models are completely retrained when a drift detection mechanism triggers an alarm. This paper proposes an alternative approach in which an unsupervised and model-agnostic concept drift adaptation method at the global level is introduced, based on autoencoders. Specifically, the proposed method aims to ``unlearn'' the concept drift without having to retrain or adapt any of the learning models operating on the data. An extensive experimental evaluation is conducted in two application domains. We consider a realistic water distribution network with more than 30 models in-place, from which we cr
    
[^151]: ZigZag: 通过两步推理实现的通用无采样不确定性估计

    ZigZag: Universal Sampling-free Uncertainty Estimation Through Two-Step Inference. (arXiv:2211.11435v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11435](http://arxiv.org/abs/2211.11435)

    本研究提出了一种通用的无采样不确定性估计方法，通过训练网络在有和没有额外信息的情况下产生相同的输出，实现了与最先进方法相媲美的可靠性估计，同时显著降低了计算成本。

    

    尽管深度网络产生有用的预测的能力已经被充分证明，但是估计这些预测的可靠性仍然具有挑战性。诸如MC-Dropout和Deep Ensembles之类的采样方法已经成为最流行的用于此目的的方法。不幸的是，它们在推理时需要进行许多前向传递，这会减慢速度。无采样方法可能更快，但存在其他缺点，例如不确定性估计的可信度较低、使用困难以及适用于不同类型的任务和数据的能力有限。在这项工作中，我们介绍了一种通用且易于部署的无采样方法，它以明显较低的计算成本获得与最先进方法相媲美的可靠性估计。其基本原理是训练网络在有和没有额外信息的情况下产生相同的输出。在推理时，当没有提供先验信息时，我们使用网络的自身预测。

    Whereas the ability of deep networks to produce useful predictions has been amply demonstrated, estimating the reliability of these predictions remains challenging. Sampling approaches such as MC-Dropout and Deep Ensembles have emerged as the most popular ones for this purpose. Unfortunately, they require many forward passes at inference time, which slows them down. Sampling-free approaches can be faster but suffer from other drawbacks, such as lower reliability of uncertainty estimates, difficulty of use, and limited applicability to different types of tasks and data.  In this work, we introduce a sampling-free approach that is generic and easy to deploy, while producing reliable uncertainty estimates on par with state-of-the-art methods at a significantly lower computational cost. It is predicated on training the network to produce the same output with and without additional information about it. At inference time, when no prior information is given, we use the network's own predicti
    
[^152]: RCD-SGD: 资源受限的异构环境中基于子模块划分的分布式 SGD

    RCD-SGD: Resource-Constrained Distributed SGD in Heterogeneous Environment via Submodular Partitioning. (arXiv:2211.00839v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00839](http://arxiv.org/abs/2211.00839)

    RCD-SGD是一个在资源受限的异构环境中的分布式SGD算法，通过子模块划分实现了类别级特征分布的相似性和类别平衡。

    

    基于SGD的分布式训练算法的收敛性与工作者之间的数据分布相关。标准的分区技术试图实现等大小分区，其中每个类别的数据分布与总数据集成比例。即使具有相同总体人口大小或相同类别每个样本的分区，特征空间中仍可能存在非独立分布。在异构计算环境中，当设备具有不同的计算能力时，跨设备的均匀分区可能导致分布式SGD中的拖尾问题。我们开发了一个基于新型数据划分算法的异构环境下的分布式SGD框架，该算法显式考虑了工作者之间的资源异质性，同时实现类别级特征分布的相似性和类别平衡。

    The convergence of SGD based distributed training algorithms is tied to the data distribution across workers. Standard partitioning techniques try to achieve equal-sized partitions with per-class population distribution in proportion to the total dataset. Partitions having the same overall population size or even the same number of samples per class may still have Non-IID distribution in the feature space. In heterogeneous computing environments, when devices have different computing capabilities, even-sized partitions across devices can lead to the straggler problem in distributed SGD. We develop a framework for distributed SGD in heterogeneous environments based on a novel data partitioning algorithm involving submodular optimization. Our data partitioning algorithm explicitly accounts for resource heterogeneity across workers while achieving similar class-level feature distribution and maintaining class balance. Based on this algorithm, we develop a distributed SGD framework that ca
    
[^153]: 在小成本上对大模型进行差分隐私优化

    Differentially Private Optimization on Large Model at Small Cost. (arXiv:2210.00038v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00038](http://arxiv.org/abs/2210.00038)

    本文提出了一种名为簿记（BK）的技术，实现了差分隐私优化器在大模型和高维数据上的快速训练，并在计算成本上取得了实质性的改进。

    

    差分隐私（DP）优化是学习准确且保护隐私的大型神经网络的标准范式。然而，由于逐样本梯度修剪，DP深度学习的计算成本非常高昂。现有的DP实现比标准（非私有）训练的时间和空间复杂度高2-1000倍。在这项工作中，我们开发了一种新颖的簿记（BK）技术，它实现了现有的DP优化器（从而实现相同的准确性），并在计算成本上有实质性的改进。具体而言，BK使得对大型模型和高维数据进行DP训练的速度和节省内存与标准训练相当，而以前的DP算法可能因内存错误而低效或无法训练。通过复杂度分析和对视觉和语言任务的广泛实验，验证了BK的计算优势。我们的实现达到了最先进的水平（SOTA）。

    Differentially private (DP) optimization is the standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are 2-1000X more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as fast and memory-saving as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error. The computational advantage of BK is supported by the complexity analysis as well as extensive experiments on vision and language tasks. Our implementation achieves state-of-the-art (SOTA
    
[^154]: 博弈树中的NEAR-OPTIMAL PHI-REGRET学习

    Near-Optimal $\Phi$-Regret Learning in Extensive-Form Games. (arXiv:2208.09747v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2208.09747](http://arxiv.org/abs/2208.09747)

    本文介绍了一种有效和解耦的学习动力学，在多人博弈中能够使每个玩家的触发后悔按$O(\log T)$增长，达到接近最优的收敛速度。并且构建利用了一个更为一般的由具有多项式次数的有理函数导出的不动点结果，以及一个凸包的更细致的后悔电路，保留了RVU合适性的属性。

    

    本文在多人完美回忆不完美信息博弈中建立了有效和解耦的学习动力学，以便每个玩家的触发后悔在T次游戏重复后按$O(\log T)$增长。这相对于先前已知的触发后悔边界$O(T^{1/4})$有指数级的改进，并解决了Bai等人（2022）提出的一个最近的开放问题。作为直接的结果，我们保证以接近最优的速度$\frac{\log T}{T}$收敛到广泛形式的相关均衡和粗略的相关均衡。在现有工作的基础上，我们的构建的核心是一个更为一般的由具有多项式次数的有理函数导出的不动点结果，这是我们为触发偏差函数（粗略的）固定点所建立的属性。此外，我们的构建利用了凸包的更细致的后悔电路，与先前的保证不同，它保留了RVU合适性的属性。

    In this paper, we establish efficient and uncoupled learning dynamics so that, when employed by all players in multiplayer perfect-recall imperfect-information extensive-form games, the trigger regret of each player grows as $O(\log T)$ after $T$ repetitions of play. This improves exponentially over the prior best known trigger-regret bound of $O(T^{1/4})$, and settles a recent open question by Bai et al. (2022). As an immediate consequence, we guarantee convergence to the set of extensive-form correlated equilibria and coarse correlated equilibria at a near-optimal rate of $\frac{\log T}{T}$.  Building on prior work, at the heart of our construction lies a more general result regarding fixed points deriving from rational functions with polynomial degree, a property that we establish for the fixed points of (coarse) trigger deviation functions. Moreover, our construction leverages a refined regret circuit for the convex hull, which -- unlike prior guarantees -- preserves the RVU proper
    
[^155]: 分布式数据上的协同因果推断

    Collaborative causal inference on distributed data. (arXiv:2208.07898v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.07898](http://arxiv.org/abs/2208.07898)

    提出了一种数据协作准实验（DC-QE）方法，可以在保护隐私的前提下对分布式数据进行因果推断。通过共享中间表示而不是私有数据，估计倾向分数和处理效应，能够减少随机误差和偏差，相比现有方法有更好的估计结果。

    

    近年来，基于隐私保护的分布式数据因果推断技术的发展引起了广泛关注。为了解决这个问题，我们提出了一种数据协作准实验（DC-QE）方法，可以在保护隐私的前提下对分布式数据进行因果推断。在我们的方法中，首先，本地各方从私有数据中构建降维的中间表示。其次，他们共享中间表示，而不是私有数据，以保护隐私。然后，从共享的中间表示中估计倾向分数。最后，从倾向分数中估计处理效应。我们的方法能够减少随机误差和偏差，而现有方法只能减少处理效应估计中的随机误差。通过在人工数据和实际数据上进行数值实验，我们确认我们的方法可以得到比单独分析更好的估计结果。

    The development of technologies for causal inference with the privacy preservation of distributed data has attracted considerable attention in recent years. To address this issue, we propose a data collaboration quasi-experiment (DC-QE) that enables causal inference from distributed data with privacy preservation. In our method, first, local parties construct dimensionality-reduced intermediate representations from the private data. Second, they share intermediate representations, instead of private data for privacy preservation. Third, propensity scores were estimated from the shared intermediate representations. Finally, the treatment effects were estimated from propensity scores. Our method can reduce both random errors and biases, whereas existing methods can only reduce random errors in the estimation of treatment effects. Through numerical experiments on both artificial and real-world data, we confirmed that our method can lead to better estimation results than individual analyse
    
[^156]: A*Net：基于路径的知识图谱推理方法的可扩展性研究

    A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs. (arXiv:2206.04798v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.04798](http://arxiv.org/abs/2206.04798)

    本论文提出了A*Net，一种基于路径的知识图谱推理方法，通过学习优先级函数，实现了对重要节点和边的选择，从而提高了训练和推理的效率。实验证明A*Net在性能和效率之间取得了平衡，在知识图谱推理中具有竞争力，并在大规模数据集上获得了新的最先进结果。

    

    长期以来，对大规模知识图谱的推理一直由嵌入方法主导。虽然基于路径的方法具有嵌入方法所缺乏的归纳能力，但其可扩展性受到指数级路径数量的限制。在这里，我们提出了A*Net，一种适用于知识图谱推理的可扩展路径方法。受到A*算法最短路径问题的启发，我们的A*Net学习了一个优先级函数，在每次迭代中选择重要的节点和边，以减少训练和推理的时间和内存占用。选择的节点和边的比例可以指定，以在性能和效率之间进行权衡。在传导性和归纳性知识图推理基准测试中的实验证明，A*Net在仅访问每次迭代中的10%节点和10%边的情况下，实现了与现有最先进基于路径方法竞争的性能。在一个百万级数据集ogbl-wikikg2上，A*Net不仅取得了新的最先进结果，还实现了收敛。

    Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A* algorithm for shortest path problems, our A*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A*Net not only achieves a new state-of-the-art result, but also converges 
    
[^157]: 将知识从记忆中解耦：检索增强的提示学习

    Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.14704](http://arxiv.org/abs/2205.14704)

    本论文提出了一种检索增强的提示学习方法，通过将知识从记忆中解耦，帮助模型在泛化和记忆之间取得平衡。

    

    提示学习方法在自然语言处理领域取得了显著的突破，提高了少样本学习的性能，但仍然遵循参数化学习范式；在学习过程中，遗忘和机械记忆问题可能导致不稳定的泛化问题。为了缓解这些限制，我们开发了RetroPrompt，旨在从记忆中将知识解耦，帮助模型在泛化和记忆之间取得平衡。与传统的提示学习方法相比，RetroPrompt从训练实例构建了一个开放式知识库，并在输入、训练和推断过程中实施检索机制，使模型具备了从训练语料库中检索相关上下文用于增强的能力。大量实验证明了RetroPrompt的效果。

    Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstra
    
[^158]: 医学图像分类中的可解释深度学习方法：一项调查

    Explainable Deep Learning Methods in Medical Image Classification: A Survey. (arXiv:2205.04766v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2205.04766](http://arxiv.org/abs/2205.04766)

    这项调查提供了关于可解释深度学习方法在医学图像分类中的应用的全面概述，包括各种解释方法的比较和性能评估。

    

    深度学习在医学影像诊断中取得了显著的成功，但由于其缺乏可解释性，这些模型很难在临床工作流程中得到采用。这引发了对解释深度学习模型决策过程的需求，进而形成了可解释人工智能（XAI）的研究领域。本文全面调查了XAI在医学影像诊断中的应用，包括视觉、文本、基于示例和基于概念的解释方法。此外，本文还回顾了现有的医学影像数据集和用于评估解释质量的指标。此外，我们还对一组基于报告生成的方法进行了性能比较。

    The remarkable success of deep learning has prompted interest in its application to medical imaging diagnosis. Even though state-of-the-art deep learning models have achieved human-level accuracy on the classification of different types of medical data, these models are hardly adopted in clinical workflows, mainly due to their lack of interpretability. The black-box-ness of deep learning models has raised the need for devising strategies to explain the decision process of these models, leading to the creation of the topic of eXplainable Artificial Intelligence (XAI). In this context, we provide a thorough survey of XAI applied to medical imaging diagnosis, including visual, textual, example-based and concept-based explanation methods. Moreover, this work reviews the existing medical imaging datasets and the existing metrics for evaluating the quality of the explanations. In addition, we include a performance comparison among a set of report generation-based methods. Finally, the major 
    
[^159]: 关系抽取作为开书考试：检索增强的提示调优

    Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.02355](http://arxiv.org/abs/2205.02355)

    提出了一种新的半参数学习范式，即检索增强的提示调优，用于关系抽取。通过构建开放式存储库，并使用线性插值的方式，模型能够在推断过程中根据存储库中的记忆信息推断关系。

    

    预训练语言模型通过展示出卓越的少样本学习能力，在关系抽取方面做出了重要贡献。然而，关系抽取的提示调优方法可能仍然无法推广到那些罕见或困难的模式中。我们将关系抽取视为一种开放式考试，并提出了一种新的检索增强的提示调优的半参数学习范式。我们构建了一个开放式存储库，用于检索基于提示的实例表示和相应的关系标签作为记忆的键值对。在推断过程中，模型可以通过线性插值基于PLM的基本输出与存储库上的非参数最近邻分布来推断关系。

    Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In
    
[^160]: 学习具有一般分布依赖性的高维McKean-Vlasov正反向随机微分方程

    Learning High-Dimensional McKean-Vlasov Forward-Backward Stochastic Differential Equations with General Distribution Dependence. (arXiv:2204.11924v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2204.11924](http://arxiv.org/abs/2204.11924)

    本文提出了一种新型深度学习方法，用于计算具有一般形式均场相互作用的高维McKean-Vlasov正反向随机微分方程。通过将问题转化为求解具有显式系数函数的标准FBSDEs，并使用深度神经网络来近似MV-FBSDEs的模型系数，可以解决均场相互作用具有完全分布依赖性的问题。

    

    在均场控制和均场博弈中，解决相应的McKean-Vlasov正反向随机微分方程(MV-FBSDEs)是一个核心问题。大多数现有的方法只适用于均场相互作用仅依赖于期望或其他矩的特殊情况，因此无法解决均场相互作用具有完全分布依赖性的问题。本文提出了一种用于计算具有一般形式均场相互作用的MV-FBSDEs的新型深度学习方法。具体而言，基于虚拟博弈，我们将问题重新表述为反复求解具有显式系数函数的标准FBSDEs。这些系数函数用于近似具有完全分布依赖性的MV-FBSDEs的模型系数，并通过使用从上一次迭代的FBSDE解模拟的训练数据来解决另一个监督学习问题来更新。我们使用深度神经网络来求解标准BSDEs并进行近似

    One of the core problems in mean-field control and mean-field games is to solve the corresponding McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs). Most existing methods are tailored to special cases in which the mean-field interaction only depends on expectation or other moments and thus inadequate to solve problems when the mean-field interaction has full distribution dependence.  In this paper, we propose a novel deep learning method for computing MV-FBSDEs with a general form of mean-field interactions. Specifically, built on fictitious play, we recast the problem into repeatedly solving standard FBSDEs with explicit coefficient functions. These coefficient functions are used to approximate the MV-FBSDEs' model coefficients with full distribution dependence, and are updated by solving another supervising learning problem using training data simulated from the last iteration's FBSDE solutions. We use deep neural networks to solve standard BSDEs and approx
    
[^161]: 面向预训练语言模型的对比演示调优

    Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.04392](http://arxiv.org/abs/2204.04392)

    本论文提出了一种名为对比演示调优的方法，可以在低数据场景下有效激发预训练语言模型的能力。实验结果表明，该方法与先前的提示调优方法相结合可以取得更好的性能。

    

    在低数据场景中，使用文本提示或演示可以有效地激发预训练语言模型的能力。最近的研究主要集中在自动搜索离散或连续提示或优化语言表达者，但对于演示的研究仍然有限。具体来说，演示示例对于最终的提示调优性能至关重要。本文提出了一种新颖的可插拔、可扩展和高效的方法，称为对比演示调优，它不需要进行演示采样。此外，该方法能够：（i）嵌入到任何先前的提示调优方法中；（ii）扩展到具有大量类别的广泛分类任务中。在16个数据集上的实验结果表明，我们的方法与先前的LM-BFF和P-tuning方法相结合可以得到更好的性能。代码可在https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning中获得。

    Pretrained language models can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Recent works have focused on automatically searching discrete or continuous prompts or optimized verbalizers, yet studies for the demonstration are still limited. Concretely, the demonstration examples are crucial for an excellent final performance of prompt-tuning. In this paper, we propose a novel pluggable, extensible, and efficient approach named contrastive demonstration tuning, which is free of demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged into any previous prompt-tuning approaches; (ii) Extended to widespread classification tasks with a large number of categories. Experimental results on 16 datasets illustrate that our method integrated with previous approaches LM-BFF and P-tuning can yield better performance. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.
    
[^162]: 非线性算子的伪逆的理论基础

    Theoretical Foundations for Pseudo-Inversion of Nonlinear Operators. (arXiv:2111.10755v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2111.10755](http://arxiv.org/abs/2111.10755)

    本文研究了非线性算子的伪逆，包括其存在性和唯一性条件以及性质分析，给出了一些众所周知的不可逆非线性算子PI的解析表达式，并讨论了其与小波阈值的关系。

    

    Moore-Penrose伪逆在物理学、统计学和各个工程领域被广泛使用。在数据科学中，非线性算子被广泛使用。本文研究了非线性算子的伪逆，广义地定义了这个概念，首先对于一般集合，然后对于赋范空间进行了细化。当算子是矩阵时，赋范空间的PI产生了Moore-Penrose伪逆。我们给出了PI存在和唯一性的条件，并建立了关于其性质的理论结果，如连续性、算子组合和投影算子的价值等。我们对一些众所周知的不可逆非线性算子的PI给出了解析表达式，例如硬/软阈值和ReLU。最后，我们分析了一个神经层，并讨论了与小波阈值有关的关系。

    The Moore-Penrose inverse is widely used in physics, statistics, and various fields of engineering. It captures well the notion of inversion of linear operators in the case of overcomplete data. In data science, nonlinear operators are extensively used. In this paper we characterize the fundamental properties of a pseudo-inverse (PI) for nonlinear operators.  The concept is defined broadly. First for general sets, and then a refinement for normed spaces. The PI for normed spaces yields the Moore-Penrose inverse when the operator is a matrix. We present conditions for existence and uniqueness of a PI and establish theoretical results investigating its properties, such as continuity, its value for operator compositions and projection operators, and others. Analytic expressions are given for the PI of some well-known, non-invertible, nonlinear operators, such as hard- or soft-thresholding and ReLU. Finally, we analyze a neural layer and discuss relations to wavelet thresholding.
    
[^163]: 最佳子群选择

    Optimal subgroup selection. (arXiv:2109.01077v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2109.01077](http://arxiv.org/abs/2109.01077)

    在回归设置中，我们提出了一个子群选择挑战，以确定回归函数超过预设阈值的特征空间区域。我们的主要贡献是确定了在样本规模和类型I错误概率上遗憾的最佳速率。

    

    在临床试验和其他应用中，我们经常看到特征空间中出现了有趣的行为区域，但不清楚这些观察到的现象是否在总体水平上有所反映。针对回归设置，我们考虑子群选择挑战，即识别一个特征空间的区域，在该区域上，回归函数超过了预设的阈值。我们将这个问题形式化为一种约束优化问题，通过寻找一个低复杂度、数据相关的选择集，在这个选择集上，回归函数有至少与阈值一样大的概率，同时要求该区域在边缘特征分布下的质量尽可能大。这导致了一种自然的遗憾概念，我们的主要贡献是确定了遗憾在样本规模和第一类错误概率上的最优值。这个最优值涉及到样本大小和类型I错误概率的微妙相互影响。

    In clinical trials and other applications, we often see regions of the feature space that appear to exhibit interesting behaviour, but it is unclear whether these observed phenomena are reflected at the population level. Focusing on a regression setting, we consider the subgroup selection challenge of identifying a region of the feature space on which the regression function exceeds a pre-determined threshold. We formulate the problem as one of constrained optimisation, where we seek a low-complexity, data-dependent selection set on which, with a guaranteed probability, the regression function is uniformly at least as large as the threshold; subject to this constraint, we would like the region to contain as much mass under the marginal feature distribution as possible. This leads to a natural notion of regret, and our main contribution is to determine the minimax optimal rate for this regret in both the sample size and the Type I error probability. The rate involves a delicate interpla
    
[^164]: 在强化学习中，在线演员-评论家算法的ODE极限全局收敛性

    Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning. (arXiv:2108.08655v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.08655](http://arxiv.org/abs/2108.08655)

    该论文研究了强化学习中在线演员-评论家算法的全局收敛性。通过数学分析证明，随着更新次数趋近无穷大，带有表格参数化的在线演员-评论家算法收敛于常微分方程。研究结果可以帮助我们理解演员-评论家算法在实践中的行为和性质。

    

    演员-评论家算法在强化学习中被广泛使用，但由于非独立同分布的在线数据样本的到来，其在数学上分析具有挑战性。数据样本的分布随着模型的更新而动态变化，引入了数据分布和强化学习算法之间复杂的反馈循环。我们证明，在时间重缩放下，带有表格参数化的在线演员-评论家算法在更新次数趋近于无穷大时收敛于常微分方程（ODE）。证明首先在固定的演员策略下建立数据样本的几何遍历性。然后，使用泊松方程，我们证明随着更新次数趋近于无穷大，数据样本关于一种动态概率测度的波动在演变的演员模型的函数下消失。一旦得到ODE极限，我们使用双时间尺度分析研究其收敛特性。

    Actor-critic algorithms are widely used in reinforcement learning, but are challenging to mathematically analyse due to the online arrival of non-i.i.d. data samples. The distribution of the data samples dynamically changes as the model is updated, introducing a complex feedback loop between the data distribution and the reinforcement learning algorithm. We prove that, under a time rescaling, the online actor-critic algorithm with tabular parametrization converges to an ordinary differential equation (ODE) as the number of updates becomes large. The proof first establishes the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the data samples around a dynamic probability measure, which is a function of the evolving actor model, vanish as the number of updates become large. Once the ODE limit has been derived, we study its convergence properties using a two time-scale analysis which asymptotically de-co
    
[^165]: 跨越空间和光谱域的鸿沟：一种用于图神经网络的统一框架。

    Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks. (arXiv:2107.10234v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.10234](http://arxiv.org/abs/2107.10234)

    该论文提出了一种统一框架，用于将基于空间和谱域的图神经网络进行整合，并紧密关联各自域内的方法。

    

    近年来，深度学习的性能得到了广泛的认可。图神经网络（GNN）旨在处理经典深度学习难以处理的图结构数据。由于大多数GNN是使用不同的理论创建的，因此无法直接进行比较。先前的研究主要集中在对现有模型进行分类，对它们的内在连接关系关注甚少。本研究的目的是建立一个基于谱图和近似论的统一框架，集成基于空间和谱域的GNN，并紧密关联各自域内的方法。

    Deep learning's performance has been extensively recognized recently. Graph neural networks (GNNs) are designed to deal with graph-structural data that classical deep learning does not easily manage. Since most GNNs were created using distinct theories, direct comparisons are impossible. Prior research has primarily concentrated on categorizing existing models, with little attention paid to their intrinsic connections. The purpose of this study is to establish a unified framework that integrates GNNs based on spectral graph and approximation theory. The framework incorporates a strong integration between spatial- and spectral-based GNNs while tightly associating approaches that exist within each respective domain.
    
[^166]: 通过深度神经网络校准多维复杂常微分方程的噪声数据

    Calibrating multi-dimensional complex ODE from noisy data via deep neural networks. (arXiv:2106.03591v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.03591](http://arxiv.org/abs/2106.03591)

    该论文提出了一个两阶段非参数方法，通过深度神经网络校准多维复杂常微分方程的噪声数据。该方法能够恢复ODE系统，避免了维度灾难和复杂ODE结构的限制，并在模块化结构和适当选择网络架构的情况下被证明是一致的。

    

    常微分方程（ODE）被广泛用于建模生物学、化学、工程、金融、物理等领域的复杂动态。使用噪声数据校准复杂ODE系统通常非常困难。在这项工作中，我们提出了一个两阶段非参数方法来解决这个问题。我们首先使用边界核方法提取去噪数据及其高阶导数，然后将它们输入具有ReLU激活函数的稀疏连接深度神经网络中。我们的方法能够恢复ODE系统，而不受维度灾难和复杂ODE结构的限制。当ODE具有一般的模块化结构，每个模块组件仅涉及少量输入变量，并且网络架构被适当选择时，我们的方法被证明是一致的。理论性质通过广泛的模拟研究得到验证，证明了所提出方法的有效性和有效性。

    Ordinary differential equations (ODEs) are widely used to model complex dynamics that arises in biology, chemistry, engineering, finance, physics, etc. Calibration of a complicated ODE system using noisy data is generally very difficult. In this work, we propose a two-stage nonparametric approach to address this problem. We first extract the de-noised data and their higher order derivatives using boundary kernel method, and then feed them into a sparsely connected deep neural network with ReLU activation function. Our method is able to recover the ODE system without being subject to the curse of dimensionality and complicated ODE structure. When the ODE possesses a general modular structure, with each modular component involving only a few input variables, and the network architecture is properly chosen, our method is proven to be consistent. Theoretical properties are corroborated by an extensive simulation study that demonstrates the validity and effectiveness of the proposed method.
    
[^167]: 带有一般高斯设计的套索方法及其在假设检验中的应用

    The Lasso with general Gaussian designs with applications to hypothesis testing. (arXiv:2007.13716v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2007.13716](http://arxiv.org/abs/2007.13716)

    本论文推广了套索方法在高斯相关设计中的应用，通过一个更简单的“固定设计”模型来精确刻画套索估计器，解决了高维回归中的渐近正态性问题。

    

    套索方法是一种高维回归方法，当自变量的数量$p$与观测数量$n$相同或更大时，现在通常使用。由于两个基本原因，经典的渐近正态理论不适用于该模型：(1) 正则化风险是非光滑的；(2) 估计器$\widehat{\boldsymbol{\theta}}$与真实参数向量$\boldsymbol{\theta}^*$之间的距离不能忽略。因此，传统的渐近正态理论的基础——标准的摄动论证失败。另一方面，在$n$和$p$都很大且$n/p$为1阶的情况下，可以精确地描述套索估计器。这个描述首先是在具有独立同分布自变量的高斯设计情况下得到的：我们将其推广到具有非奇异协方差结构的高斯相关设计。这可以通过一个更简单的“固定设计”模型来表达。

    The Lasso is a method for high-dimensional regression, which is now commonly used when the number of covariates $p$ is of the same order or larger than the number of observations $n$. Classical asymptotic normality theory does not apply to this model due to two fundamental reasons: $(1)$ The regularized risk is non-smooth; $(2)$ The distance between the estimator $\widehat{\boldsymbol{\theta}}$ and the true parameters vector $\boldsymbol{\theta}^*$ cannot be neglected. As a consequence, standard perturbative arguments that are the traditional basis for asymptotic normality fail.  On the other hand, the Lasso estimator can be precisely characterized in the regime in which both $n$ and $p$ are large and $n/p$ is of order one. This characterization was first obtained in the case of Gaussian designs with i.i.d. covariates: here we generalize it to Gaussian correlated designs with non-singular covariance structure. This is expressed in terms of a simpler ``fixed-design'' model. We establish
    
[^168]: 探索使用统一的文本到文本转换器进行迁移学习的极限

    Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. (arXiv:1910.10683v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1910.10683](http://arxiv.org/abs/1910.10683)

    本文通过引入一种统一的框架，将所有基于文本的语言问题转换为文本到文本格式，从而探索了NLP中的迁移学习技术的全貌，通过对多个任务进行系统研究，实现了许多基准测试上的最新结果。

    

    迁移学习已经成为自然语言处理(NLP)中一种强大的技术，其中模型在进行下游任务的微调之前首先在数据丰富的任务上进行预训练。迁移学习的有效性催生了多种方法、方法论和实践。本文通过引入一种将所有基于文本的语言问题转换为文本到文本格式的统一框架，探索了NLP中的迁移学习技术的全貌。我们对许多语言理解任务进行了系统性的研究，比较了预训练目标、架构、无标签数据集、迁移方法和其他因素。通过将我们探索的见解与规模和我们的新的“庞大干净抓取语料库”相结合，在许多涉及摘要、问答、文本分类等基准测试上取得了最新的成果。为了促进NLP领域的未来迁移学习研究，我们发布了我们的数据集、预训练模型等。

    Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained mode
    

