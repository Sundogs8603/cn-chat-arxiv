# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DreamLLM: Synergistic Multimodal Comprehension and Creation.](http://arxiv.org/abs/2309.11499) | DreamLLM是一种学习框架，实现了多模态理解与创作的协同效应。通过直接采样生成语言和图像的生成模型，避免了信息损失，并获得了更全面的多模态理解。此外，DreamLLM能够生成自由形式交织内容，展现了其在零样本多模态学习任务中的卓越性能。 |
| [^2] | [Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning.](http://arxiv.org/abs/2309.11489) | Text2Reward是一个无需数据的自动化框架，可以根据大型语言模型自动生成可解释、自由形式的密集奖励函数，广泛适用于各种任务，并允许人类反馈进行迭代改进。 |
| [^3] | [Multiplying poles to avoid unwanted points in root finding and optimization.](http://arxiv.org/abs/2309.11475) | 通过增加极点来避免根查找和优化中不需要的点，方法是将代价函数除以到目标点的距离函数的适当幂。 |
| [^4] | [Model-free tracking control of complex dynamical trajectories with machine learning.](http://arxiv.org/abs/2309.11470) | 本论文提出了一个无模型的机器学习框架，通过仅使用部分观测状态来控制具有两臂的机器人操作器。该框架利用储层计算和随机输入进行训练，并展示了对多种周期性和混沌信号的跟踪控制鲁棒性。 |
| [^5] | [AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition.](http://arxiv.org/abs/2309.11462) | AudioFool是一种高速、通用和无需同步的跨领域语音识别攻击，能够通过反向傅里叶变换构造对同步的不变性和对滤波的鲁棒性，实现对ASR系统的拒绝服务攻击。 |
| [^6] | [Digital twins of nonlinear dynamical systems: A perspective.](http://arxiv.org/abs/2309.11461) | 数字孪生在各个领域引起了广泛关注。非线性动力系统的数字孪生可以生成系统演化、预测灾难性行为，并提供早期警告。数字孪生可以用于实时系统监测和预测性问题解决。两种方法构建数字孪生：稀疏优化和机器学习。 |
| [^7] | [Multi-Step Model Predictive Safety Filters: Reducing Chattering by Increasing the Prediction Horizon.](http://arxiv.org/abs/2309.11453) | 本文提出了一种通过增加预测视野来减少震荡的多步骤模型预测安全滤波器方法，通过将长期输入修正考虑在内，对有界模型不确定性下的递归可行性进行了证明，并在仿真和四旋翼实验中进行了验证。 |
| [^8] | [Distribution and volume based scoring for Isolation Forests.](http://arxiv.org/abs/2309.11450) | 本文在Isolation Forest方法中提出了两个贡献，分别是对评分函数的信息理论推广和在个体树估计器层面上使用基于超体积的评分函数。这两种方法均对一些数据集相对于标准Isolation Forest表现出显著改进，并且其中一种方法在所有数据集上平均表现出改进。 |
| [^9] | [Weight Averaging Improves Knowledge Distillation under Domain Shift.](http://arxiv.org/abs/2309.11446) | 加权平均技术在领域转移下的知识蒸馏中也能提高性能，可以应用于没有验证数据的简单加权平均策略。 |
| [^10] | [Signature Activation: A Sparse Signal View for Holistic Saliency.](http://arxiv.org/abs/2309.11443) | 该研究介绍了一种称为签名激活的显著性方法，利用稀疏信号观点生成对卷积神经网络输出的整体和类别不可知的解释。该方法在医学图像分析和临床病变检测中具有潜在应用价值。 |
| [^11] | [Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing.](http://arxiv.org/abs/2309.11427) | 本研究提出了TRACE-GPT模型，用于半导体制造业中无监督故障检测。通过使用时间卷积嵌入和生成式预训练Transformer来预训练时间序列数据，并使用交叉熵损失函数进行异常序列和正常序列的分类，实验结果表明模型具有更好的性能。 |
| [^12] | [Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models.](http://arxiv.org/abs/2309.11420) | 深度神经网络可以作为降噪算法在高维图形模型中学习扩散模型，为生成建模提供了高效的逼近方法。 |
| [^13] | [EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning.](http://arxiv.org/abs/2309.11414) | EDMP是一种结合了传统和基于深度学习运动规划优势的基于成本引导的扩散运动规划方法，通过对多样化的运动学有效轨迹进行训练，来提高解决方案的成功率。 |
| [^14] | [Transformers versus LSTMs for electronic trading.](http://arxiv.org/abs/2309.11400) | 本研究比较了基于LSTM和基于Transformer的模型在金融时间序列预测任务上的表现，并介绍了一种新的LSTM-based模型DLSTM。 |
| [^15] | [Preconditioned Federated Learning.](http://arxiv.org/abs/2309.11378) | 本文提出了基于两个自适应框架的新的通信高效联邦学习算法：PreFed和PreFedOp，通过使用新颖的协方差矩阵预处理器来实现适应性。实验证明这些方法在i.i.d.和非i.i.d.设置下均取得了最先进的性能。 |
| [^16] | [Learning Patient Static Information from Time-series EHR and an Approach for Safeguarding Privacy and Fairness.](http://arxiv.org/abs/2309.11373) | 本文研究了从时间序列电子健康记录数据预测患者静态信息的能力，并发现不仅原始数据，机器学习模型学习到的表示也可以用于预测各种静态信息，包括生物学性别、二值化年龄和自报种族，具有高预测性能。 |
| [^17] | [3D Face Reconstruction: the Road to Forensics.](http://arxiv.org/abs/2309.11357) | 本综述研究了3D人脸重建算法的应用，从整形手术到娱乐行业，但在法医学领域的应用仍然有一些限制和障碍。对于将3D人脸重建作为法医学证据的有效工具，我们仍需进行进一步的研究和探索。 |
| [^18] | [Self-supervised learning unveils change in urban housing from street-level images.](http://arxiv.org/abs/2309.11354) | 本论文使用自监督学习方法分析伦敦的城市变化，通过应用于1500万张街景图像，成功地识别出了住房供应的变化，并区分了主要和次要变化。 |
| [^19] | [C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters.](http://arxiv.org/abs/2309.11351) | C⋅ASE是一个学习基于物理的角色的条件对抗技能嵌入的框架，通过将异构的技能动作划分为不同子集，以实现多样性和可控性。训练过程中采用了焦点技能采样、骨骼残余力和逐元素特征屏蔽，以平衡不同复杂性的技能，并捕捉更一般的行为特征。 |
| [^20] | [GECTurk: Grammatical Error Correction and Detection Dataset for Turkish.](http://arxiv.org/abs/2309.11346) | GECTurk是一个用于土耳其语的语法错误纠正和检测数据集。它采用灵活且可扩展的合成数据生成流水线，覆盖了20多个专家策划的语法和拼写规则，并且通过手动注释电影评论创造了更真实的测试集。 |
| [^21] | [Using Property Elicitation to Understand the Impacts of Fairness Constraints.](http://arxiv.org/abs/2309.11343) | 这项研究使用属性引导方法来探索损失函数和正则化函数与最优决策之间的关系，特别是在公平机器学习中的应用。它提供了损失函数和正则化函数成对时属性改变的必要和充分条件，并通过实验证明了算法决策与数据分布变化和约束难度的相关性。 |
| [^22] | [Improving Article Classification with Edge-Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2309.11341) | 本论文提出了一种使用边异构图神经网络改进文章分类的方法，通过加入高阶语义的节点特征生成，能够显著提高分类性能。 |
| [^23] | [Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition.](http://arxiv.org/abs/2309.11327) | 本研究通过收集和标注数据以及探索切换方法，提出了一种有效的突尼斯方言自动语音识别解决方案，并且通过人工评估来消除拼写不合适的干扰。 |
| [^24] | [WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting.](http://arxiv.org/abs/2309.11319) | 本文提出了一种利用全局和局部周期性的Wavelet-Fourier变换网络（WFTNet）用于长期时间序列预测。在利用傅里叶和小波变换提取时频信息的基础上，引入了周期重要性加权系数（PWC）来平衡全局和局部频率模式的重要性。大量实验证明，WFTNet在各种时间序列数据集上优于其他基线模型。 |
| [^25] | [Create and Find Flatness: Building Flat Training Spaces in Advance for Continual Learning.](http://arxiv.org/abs/2309.11305) | 该论文提出了一个名为C&F的框架，通过提前为每个任务构建一个平坦的训练空间，在连续学习中解决了灾难性遗忘的问题。该框架在学习当前任务时自适应地创建平坦区域，并根据参数的平坦程度找到其对当前任务的重要性。在适应新任务时，利用平坦度应用约束并为新任务准备平坦空间。 |
| [^26] | [CPLLM: Clinical Prediction with Large Language Models.](http://arxiv.org/abs/2309.11295) | CPLLM是一种使用大规模语言模型进行临床疾病预测的方法。通过量化和提示来微调语言模型，利用患者的历史诊断记录来预测目标疾病的诊断结果。实验证明，CPLLM在各项指标上均超越了其他基线模型，显示出显著的改进。 |
| [^27] | [Beyond Accuracy: Measuring Representation Capacity of Embeddings to Preserve Structural and Contextual Information.](http://arxiv.org/abs/2309.11294) | 本文提出了一种衡量嵌入的表示能力的方法，通过结合外部评估方法和基于t-SNE的邻域分析方法，对表示能力进行了全面评估。 |
| [^28] | [Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains.](http://arxiv.org/abs/2309.11285) | AuTexTification是IberLEF 2023研讨会的共享任务，旨在检测和归属多领域机器生成的文本。数据集包含160,000多条文本，涵盖了英语和西班牙语以及推文、评论、新闻、法律和操作指南等五个领域。共有114个团队参与，提交了175次运行结果。 |
| [^29] | [From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring.](http://arxiv.org/abs/2309.11267) | 本研究提出了一种将可解释人工智能应用于裂缝分割和监测的方法，通过从分类器的解释中导出分割结果，无需像素级注释。实验结果表明，该方法能有效进行裂缝分割和生长监测。 |
| [^30] | [Sequence-to-Sequence Spanish Pre-trained Language Models.](http://arxiv.org/abs/2309.11259) | 该论文介绍了一种新的序列到序列的西班牙预训练语言模型，该模型在各种序列到序列任务中表现出了竞争性能，并提供了BART、T5和BERT2BERT-style模型的西班牙版本。 |
| [^31] | [Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering.](http://arxiv.org/abs/2309.11247) | 提出了一个针对空中战斗机动的分层多智能体强化学习框架，通过将决策过程分为两个层次的抽象来处理高维状态和动作空间的挑战，通过训练低层策略实现准确的单位战斗控制。 |
| [^32] | [Grassroots Operator Search for Model Edge Adaptation.](http://arxiv.org/abs/2309.11246) | 本论文介绍了一种基于草根操作搜索的硬件感知神经架构搜索方法，旨在通过搜索高效的操作替换来适应边缘设备上的给定模型。该方法通过使用数学指令作为基础，选择高效替换操作以降低计算复杂度，同时保持原始模型的准确性。 |
| [^33] | [Towards a Prediction of Machine Learning Training Time to Support Continuous Learning Systems Development.](http://arxiv.org/abs/2309.11226) | 本文介绍了我们对机器学习模型训练时间进行预测的研究。我们通过对全参数时间复杂度方法的实证研究，探讨了该方法的优势和弱点，发现训练时间的预测与数据集和模型参数密切相关。 |
| [^34] | [A Model-Based Machine Learning Approach for Assessing the Performance of Blockchain Applications.](http://arxiv.org/abs/2309.11205) | 本研究提出了一种基于模型的机器学习方法，用于评估区块链应用性能。通过使用预定配置参数预测性能并探索最佳配置，我们的研究为区块链应用的开发和评估提供了可靠的建模方法。 |
| [^35] | [The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute.](http://arxiv.org/abs/2309.11197) | Languini Kitchen是一个研究集体和代码库，旨在通过等效计算来进行语言模型比较，并提供新的大规模、多样化且高质量的数据集。要点：实验协议、模型比较、等效计算、大规模数据集。 |
| [^36] | [When to Trust AI: Advances and Challenges for Certification of Neural Networks.](http://arxiv.org/abs/2309.11196) | 本文主要关注人工智能（AI）的认证和可解释性，综述了已经开发的用于确保AI决策安全的技术，并探讨了未来的挑战。 |
| [^37] | [RHALE: Robust and Heterogeneity-aware Accumulated Local Effects.](http://arxiv.org/abs/2309.11193) | RHALE方法是一种强健且考虑异质性的累积局部效应方法，通过量化异质性和自动确定最佳的区间划分，解决了累积局部效应方法的限制。 |
| [^38] | [Investigating Personalization Methods in Text to Music Generation.](http://arxiv.org/abs/2309.11140) | 本研究探索了在文本到音乐生成中的个性化方法，使用了预训练的文本到音频扩散器和两种已有的个性化方法。研究发现，当前的个性化方法更容易学习节奏音乐结构而非旋律。 |
| [^39] | [Ano-SuPs: Multi-size anomaly detection for manufactured products by identifying suspected patches.](http://arxiv.org/abs/2309.11120) | Ano-SuPs是一种通过识别可疑区块来进行制造产品的多尺度异常检测的两阶段策略方法。它可以解决图像背景复杂性和异常模式的挑战，并具有较高的准确性和鲁棒性。 |
| [^40] | [Bold but Cautious: Unlocking the Potential of Personalized Federated Learning through Cautiously Aggressive Collaboration.](http://arxiv.org/abs/2309.11103) | 个性化联邦学习中的一个关键问题是决定哪些参数应该本地化或共享。传统方法中通常个性化与非IID数据敏感的所有层的参数，但这种方法过于保守。本论文提出在合作中要考虑与其他客户端的潜在好处，即使参数容易受非IID数据影响的情况下，也仍可以通过共享这些参数来获益。 |
| [^41] | [A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making.](http://arxiv.org/abs/2309.11101) | 本研究介绍了一个新的神经网络模型，称为TT-rules，在医疗决策中具有可解释性，并通过将神经网络转换为基于规则的模型实现了高性能。该模型支持二分类、多标签分类和回归任务，且在医疗应用中表现出色。 |
| [^42] | [Delays in Reinforcement Learning.](http://arxiv.org/abs/2309.11096) | 这篇论文研究了延迟对强化学习中马尔可夫决策过程的影响，通过对代理观测延迟和执行行动延迟的研究，揭示了延迟对系统性能的重要性。 |
| [^43] | [K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling.](http://arxiv.org/abs/2309.11093) | 研究者介绍了一种新颖的K-pop歌词翻译数据集，该数据集揭示了K-pop歌词翻译的独特特征，并构建了一个神经歌词翻译模型，强调了专用数据集的重要性。 |
| [^44] | [Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling.](http://arxiv.org/abs/2309.11089) | 本论文提出了一种实用的概率模型基于深度强化学习方法，通过结合dropout不确定性和轨迹采样，稳定地预测系统不确定性，纠正神经网络的拟合误差，过滤aleatoric不确定性，从而实现了优越的控制能力。在实验评估中，该方法在多个控制任务和实际机械臂操作任务中表现出优于其他方法和模型的性能。 |
| [^45] | [Weak Supervision for Label Efficient Visual Bug Detection.](http://arxiv.org/abs/2309.11077) | 我们提出了一种利用未标注数据和领域特定增强技术的弱监督方法，在视觉错误检测中实现了数据集扩大和自主/交互弱监督，展示了在广阔的游戏世界中的有效性。 |
| [^46] | [GPSINDy: Data-Driven Discovery of Equations of Motion.](http://arxiv.org/abs/2309.11076) | GPSINDy是一种数据驱动的方法，通过将高斯过程回归与SINDy相结合，能够从噪声数据中发现非线性动力学系统模型，并在实验证明了其在系统动态和预测未来轨迹方面的改进性能。 |
| [^47] | [InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update.](http://arxiv.org/abs/2309.11071) | InkStream是一种在流式图上进行实时推理的方法，通过增量更新节点嵌入来解决传统图神经网络在流式图上的挑战。 |
| [^48] | [Extreme Scenario Selection in Day-Ahead Power Grid Operational Planning.](http://arxiv.org/abs/2309.11067) | 本文提出并分析了一种在天然电网规划中选择极端情景的方法，通过统计功能深度度量来筛选出最重要的情景以减轻运营风险。研究结果表明该方法在真实的Texas-7k电网上具有良好的效果。 |
| [^49] | [Design of Chain-of-Thought in Math Problem Solving.](http://arxiv.org/abs/2309.11054) | 本论文研究了数学问题解决中思路链的设计方法，对比了自然语言思路链和程序思路链的效果，并发现程序思路链通常在数学问题解决中更加有效，特别是自我描述程序具有更大多样性且性能更高。此外，研究还发现Python是程序思路链的较好选择。实验结果为未来思路链设计提供了宝贵指导。 |
| [^50] | [Fake News BR: A Fake News Detection Platform for Brazilian Portuguese.](http://arxiv.org/abs/2309.11052) | 本研究提出了一个用于巴西葡萄牙语的假新闻检测平台，采用机器学习和自然语言处理技术，能够高效准确地识别假新闻，同时提供实时分析和验证新闻文章真实性的用户友好平台。 |
| [^51] | [Containing Analog Data Deluge at Edge through Frequency-Domain Compression in Collaborative Compute-in-Memory Networks.](http://arxiv.org/abs/2309.11048) | 本文提出了一种在边缘进行高效深度学习推理的解决方案，通过频域压缩和内存相互融合机制，实现了更高的面积效率和能量效率。 |
| [^52] | [Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion.](http://arxiv.org/abs/2309.11044) | 提出了一种基于贝叶斯信息准则的中间全局模型Clustered FedStack。通过将本地客户端的模型预测和输出层权重发送到服务器，构建一个强大的全局模型，并使用聚类机制对本地客户进行聚类。 |
| [^53] | [Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems.](http://arxiv.org/abs/2309.11039) | 联邦学习在智能交通系统中提供了一种具有隐私保护和易扩展性的新范式，为解决动态车辆环境中的各种应用挑战提供了解决方案。 |
| [^54] | [A Region-Shrinking-Based Acceleration for Classification-Based Derivative-Free Optimization.](http://arxiv.org/abs/2309.11036) | 本文研究了基于分类的无导数优化算法的加速方法，通过引入区域收缩步骤，提出了一种名为“RACE-CARS”的算法，并证明了区域收缩的加速性质。实验结果验证了"RACE-CARS"的高效性，并提出了经验的超参数调优策略。 |
| [^55] | [The Topology and Geometry of Neural Representations.](http://arxiv.org/abs/2309.11028) | 本文探索了从几何结构到拓扑结构的抽象步骤，并提出了一种拓扑表征相似分析方法（tRSA），通过一系列地理拓扑摘要统计量对大脑表征进行表征。 |
| [^56] | [Information Leakage from Data Updates in Machine Learning Models.](http://arxiv.org/abs/2309.11022) | 本文研究了在机器学习模型中进行数据更新时可能发生的信息泄露问题，并提出了基于预测置信度差异的攻击方法。实验证实了两个模型快照相对于仅访问更新数据会导致更高的信息泄露。 |
| [^57] | [Conformalized Multimodal Uncertainty Regression and Reasoning.](http://arxiv.org/abs/2309.11018) | 本文提出了一种共形化的轻量级不确定性估计器，通过将共形预测与深度学习回归器结合，在多模态情况下预测不确定性边界，并基于光流推理提高预测准确性。 |
| [^58] | [3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images.](http://arxiv.org/abs/2309.11015) | 本文提出了一种新颖的3D-U-SAM网络，用于少样本CBCT图像的牙齿分割。通过使用预训练的SAM和卷积逼近方法，以及跳跃连接融合特征，本方法在解决小样本问题上表现出很好的效果。 |
| [^59] | [ModelGiF: Gradient Fields for Model Functional Distance.](http://arxiv.org/abs/2309.11013) | 本文介绍了一种用于测量模型功能距离的方法，称为ModelGiF。通过在异构的预训练模型中提取同质的表示，ModelGiF可通过模型之间的相似性来衡量它们之间的距离。实验证实了该方法在任务相关性估计、知识产权保护和模型遗忘验证等方面的有效性。 |
| [^60] | [It's Simplex! Disaggregating Measures to Improve Certified Robustness.](http://arxiv.org/abs/2309.11005) | 本文提出了通过解构度量来提高认证的鲁棒性的方法，该方法可以更准确地评估认证机制的性能，对于数据集无关和数据集相关的度量都有效。经验证实，这种方法能够将可实现的认证半径提高一倍以上。 |
| [^61] | [AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning.](http://arxiv.org/abs/2309.10980) | 本研究提出了一种基于多智能体深度强化学习的AI驱动患者监测框架，通过部署多个学习智能体，针对不同的生理特征进行监测，并根据紧急程度预警医疗紧急团队。 |
| [^62] | [Towards Data-centric Graph Machine Learning: Review and Outlook.](http://arxiv.org/abs/2309.10979) | 本文回顾并展望了以数据为中心的图机器学习在图数据领域的当前努力，提出了一个系统框架：数据中心化图机器学习（DC-GML），并介绍了解决图数据可用性、质量和构建图MLOps系统的关键问题。 |
| [^63] | [PAGER: A Framework for Failure Analysis of Deep Regression Models.](http://arxiv.org/abs/2309.10977) | PAGER提出了一种用于深度回归模型故障分析的框架，通过综合利用认识不确定性和不一致分数，对样本进行分组并提供全面的分析。 |
| [^64] | [Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks.](http://arxiv.org/abs/2309.10976) | 本文针对图神经网络（GNNs）提出了一种准确且可扩展的认知不确定性估计方法，该方法通过扩展随机居中框架，并支持结构化数据和部分随机性，能够获得更好的校准的信心指标（CI）性能。 |
| [^65] | [SPFQ: A Stochastic Algorithm and Its Error Analysis for Neural Network Quantization.](http://arxiv.org/abs/2309.10975) | SPFQ是一种用于神经网络量化的快速随机算法，通过贪婪的路径跟踪和随机量化器有效地减少网络中的冗余并提高量化效率。在本文中，我们首次建立了全网络的误差界，并通过应用于具有高斯权重的多层网络的量化证明了结果的有效性。 |
| [^66] | [SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics.](http://arxiv.org/abs/2309.10972) | 本文提出了一种名为SEMPART的自监督多分辨率图像语义分割方法，通过联合推断图像的粗细双分割，并利用基于图形的正则化方法进行边界细节保留，成功将粗糙掩码语义提取为细粒度掩码，在显著对象检测和单个对象定位方面表现出色。 |
| [^67] | [DPpack: An R Package for Differentially Private Statistical Analysis and Machine Learning.](http://arxiv.org/abs/2309.10965) | DPpack是一个开源的R包，提供了差分隐私统计分析和机器学习的工具集合，包括不同的保护机制和描述性统计函数。它还提供了隐私保护版本的逻辑回归、支持向量机和线性回归的实现，以及对每个模型的差分隐私超参数调整。 |
| [^68] | [In-Context Learning for Text Classification with Many Labels.](http://arxiv.org/abs/2309.10954) | 本文通过使用预训练的密集检索模型，解决了上下文学习中的标签限制问题，并在多个意图分类数据集的少样本设置中取得了新的最佳性能，同时在某些情况下超越了微调模型的表现。研究还发现，更大规模的模型对于有效利用更长的上下文长度进行上下文学习是必要的。 |
| [^69] | [Deep Reinforcement Learning for Infinite Horizon Mean Field Problems in Continuous Spaces.](http://arxiv.org/abs/2309.10953) | 这项研究提出了一种基于深度强化学习的算法，通过将演员-评论家范式与均场分布表示配对，来解决连续空间中的均场博弈和均场控制问题，并使用朗之万动力学从分布中获取样本。该算法在渐近无限时域框架下使用线性二次基准进行评估。 |
| [^70] | [LMDX: Language Model-based Document Information Extraction and Localization.](http://arxiv.org/abs/2309.10952) | LMDX是一种基于语言模型的文档信息提取与定位方法，克服了布局编码和答案虚构的困难，能够在半结构化文档中提取关键实体。 |
| [^71] | [A Novel Deep Neural Network for Trajectory Prediction in Automated Vehicles Using Velocity Vector Field.](http://arxiv.org/abs/2309.10948) | 本论文提出一种新型的深度神经网络方法，通过结合速度向量场和基于数据驱动的学习方法，预测自动驾驶车辆的轨迹。实验结果表明，速度向量场的加入提高了预测的准确性。 |
| [^72] | [Test-Time Training for Speech.](http://arxiv.org/abs/2309.10930) | 本文研究了测试时间训练在语音应用中处理分布偏移的应用，提出了一种解决方案BitFit，该方案通过只考虑偏置参数进行微调，解决了测试时间训练中的关键挑战。 |
| [^73] | [Semi-automatic staging area for high-quality structured data extraction from scientific literature.](http://arxiv.org/abs/2309.10923) | 这篇论文介绍了一种半自动化分区平台，用于从科学文献中提取超导体实验数据。该平台通过自动和手动过程的结合，提高了数据更新效率，同时保持或提高了数据质量。评估实验表明该分区平台显著提高了数据的管理质量。 |
| [^74] | [Posterior Contraction Rates for Mat\'ern Gaussian Processes on Riemannian Manifolds.](http://arxiv.org/abs/2309.10918) | 该论文研究了定义在紧致Riemannian流形上的内在Matern高斯过程和外在过程之间的收缩速率，并发现它们的速率在适当匹配平滑参数的情况下是相等的。 |
| [^75] | [End-to-End Speech Recognition Contextualization with Large Language Models.](http://arxiv.org/abs/2309.10917) | 本论文介绍了一种在语音识别中使用大型语言模型进行上下文化的新方法。通过整合预训练的语言模型，我们的方法在训练过程中通过提供音频特征和文本上下文，使系统隐含地学习如何利用上下文信息，并达到了显著的性能改善。 |
| [^76] | [What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples.](http://arxiv.org/abs/2309.10916) | 本文将图像处理领域中的对抗子空间技术应用于自然语言处理，提出了基于最近邻和影响函数的检测器，并通过使用影响函数揭示了自然语言处理中的对抗样本子空间与图像处理中的子空间的关系和任务差异。 |
| [^77] | [Amplifying Pathological Detection in EEG Signaling Pathways through Cross-Dataset Transfer Learning.](http://arxiv.org/abs/2309.10910) | 该研究通过跨数据集迁移学习来放大脑电信号通路中的病理检测，以提高病理诊断的准确性和可行性，解决了标注数据稀缺性和低成本招募真实患者队列的问题。 |
| [^78] | [Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer.](http://arxiv.org/abs/2309.10891) | 本文提出了一种名为SALT的方法，通过结合代码切换和嵌入混合与自我增强，有效地提高了多语言预训练模型的零样本跨语言传递能力。 |
| [^79] | [Crypto'Graph: Leveraging Privacy-Preserving Distributed Link Prediction for Robust Graph Learning.](http://arxiv.org/abs/2309.10890) | Crypto'Graph是一种用于隐私保护的分布式图形链路预测协议，在不透露各方私有图形结构的情况下，允许参与方推断未来新链路的形成可能性。 |
| [^80] | [DeepliteRT: Computer Vision at the Edge.](http://arxiv.org/abs/2309.10878) | DeepliteRT是一个在边缘设备上部署深度学习模型的解决方案，通过超低位量化技术实现了高效的模型运算，并通过编译器转换工作简化了量化模型的部署过程。 |
| [^81] | [Dynamical Tests of a Deep-Learning Weather Prediction Model.](http://arxiv.org/abs/2309.10867) | 最近的研究发现，全球深度学习天气预测模型能够产生与基于物理的模型相媲美的预测。这项研究对一种名为Pangu-weather的模型进行了一系列动力学实验，结果表明该模型在添加恒定的热源和局部扰动时产生了经典的大气动力学响应和信号传播行为。 |
| [^82] | [Improving Opioid Use Disorder Risk Modelling through Behavioral and Genetic Feature Integration.](http://arxiv.org/abs/2309.10837) | 通过整合与阿片类药物使用障碍有关的遗传变异和行为特征，该论文开发了一种实验设计和计算方法，用于评估阿片类药物使用障碍的风险。结果显示整合遗传和迁移模式可以改善风险估计的能力。 |
| [^83] | [Analysing race and sex bias in brain age prediction.](http://arxiv.org/abs/2309.10835) | 通过对种族和生物性别亚组进行全面分析和特征检查，我们发现大脑年龄预测模型存在偏见。我们使用Kruskal-Wallis和Conover-Iman检验发现了种族和性别之间的偏见，使用PCA和Kolmogorov-Smirnov检验检测到了生成特征的分布变化。 |
| [^84] | [Sparser Random Networks Exist: Enforcing Communication-Efficient Federated Learning via Regularization.](http://arxiv.org/abs/2309.10834) | 本论文提出了一种通过正则化方法增强通信效率的联合学习方法，该方法训练过参数化的随机网络，通过优化二进制掩码来减少通信开销，同时在本地目标中添加正则化项来促进稀疏网络的解决方案。实验证明，在通信和内存效率方面取得了显著的改进。 |
| [^85] | [Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach.](http://arxiv.org/abs/2309.10831) | 本文提供了一个框架来解决强化学习中的模型不确定性和计算成本高的问题，通过使用强化学习解决随机动态规划方程，生成的控制器能够主动学习模型不确定性，并确保安全性和实时学习。 |
| [^86] | [Latent Disentanglement in Mesh Variational Autoencoders Improves the Diagnosis of Craniofacial Syndromes and Aids Surgical Planning.](http://arxiv.org/abs/2309.10825) | 本研究改进了网格变分自编码器中的潜在解缠构架，可以提高脑颅综合症的诊断准确性，同时辅助手术规划，并允许客观评估手术结果。 |
| [^87] | [AI Foundation Models for Weather and Climate: Applications, Design, and Implementation.](http://arxiv.org/abs/2309.10808) | 该论文介绍了AI基础模型在天气和气候中的应用，主要讨论了使用transformers、物理知识启发的机器学习和图神经网络的最新方法。尽管取得了进展，但全球地球系统模型的可推广AI模型仍处于初级阶段。 |
| [^88] | [PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring.](http://arxiv.org/abs/2309.10576) | 本文提出了一种基于多智能体强化学习的预测监控系统（PDRL），该系统可以在复杂环境中监测预测未来状态，并通过奖励策略来学习现有知识和最大化奖励。 |
| [^89] | [PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training.](http://arxiv.org/abs/2309.10400) | 本文介绍了一种名为PoSE的训练方法，通过在训练过程中使用固定的上下文窗口和操纵位置索引来适应极长的上下文窗口，实验证明这种方法大大减小了内存和时间开销，对性能影响较小，成功将LLaMA模型扩展到了128k个标记。 |
| [^90] | [Differentiable Quantum Architecture Search for Quantum Reinforcement Learning.](http://arxiv.org/abs/2309.10392) | DQAS是一个基于梯度的框架，用于在NISQ时代自动设计量子电路。本研究的目标是探索DQAS在解决量子深度Q-learning问题上的能力。 |
| [^91] | [TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions.](http://arxiv.org/abs/2309.10310) | TensorCodec是一种紧凑的有损张量压缩算法，可以处理无强数据假设的一般张量。它采用神经张量列车分解、折叠输入张量和重新排序模式索引等关键点来提高压缩效果。 |
| [^92] | [Primal-Dual $\ell_0$-Constrained Sparse Index Tracking.](http://arxiv.org/abs/2309.10152) | 本文提出了一种新的稀疏指数跟踪问题的形式化，使用了$\ell_0$-范数约束，可以轻松控制投资组合中资产数量的上限。 |
| [^93] | [Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions.](http://arxiv.org/abs/2309.10150) | Q-Transformer是一种可扩展的离线强化学习方法，通过使用Transformer来表示Q函数并利用离线数据集进行训练。它在大规模真实世界机器人操作任务中表现优越。 |
| [^94] | [Context $\approx$ Environment.](http://arxiv.org/abs/2309.09888) | 在这篇论文中，作者通过理论与实验证明了将注意力放在上下文-未标记样本上，可以实现更好的领域泛化。 |
| [^95] | [Contrastive Initial State Buffer for Reinforcement Learning.](http://arxiv.org/abs/2309.09752) | 本论文提出了对比初始状态缓冲区的概念，它通过选择过去的经验中的状态来初始化环境中的代理，以引导其进入更有信息量的状态。实验证明，该方法在两个复杂机器人任务上取得了更高的任务性能并加速了训练过程。 |
| [^96] | [Intelligent machines work in unstructured environments by differential neural computing.](http://arxiv.org/abs/2309.08835) | 本研究提出了一种基于差分神经计算的智能机器方法，通过提取环境信息的主要特征并应用相应的编码刺激到记忆阻性器件，成功地实现了处理无结构环境信息的类人能力，并展现了良好的可扩展性和泛化性。该方法在物体抓取和自动驾驶等应用方面得到了验证。 |
| [^97] | [Tackling the dimensions in imaging genetics with CLUB-PLS.](http://arxiv.org/abs/2309.07352) | 本文介绍了一种名为Cluster-Bootstrap PLS（CLUB-PLS）的基于偏最小二乘的框架，用于解决影像遗传学中的维度问题。该框架可以处理两个领域的大量输入维度和大样本量，并使用聚类自助法提供稳健的统计数据。 |
| [^98] | [Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization.](http://arxiv.org/abs/2309.04810) | 本论文提出了神经潜在几何搜索(NLGS)的概念，旨在通过格罗莫夫-豪斯多夫距离来自动识别下游任务的最佳潜在几何结构，以提高机器学习模型的性能。 |
| [^99] | [Inferring effective couplings with Restricted Boltzmann Machines.](http://arxiv.org/abs/2309.02292) | 本研究通过实现受限玻尔兹曼机的能量函数与有效伊辛自旋哈密顿量之间的直接映射，提供了一种用于推断复杂数据中高阶相互作用的方法。 |
| [^100] | [Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation.](http://arxiv.org/abs/2308.15363) | 本文提出了一个大规模语言模型(LLMs)赋能的文本到SQL任务的基准评估，并基于实验结果提出了一种新的集成解决方案DAIL-SQL，刷新了Spider榜单并实现了86.6%的执行准确率。同时，强调了在提示工程中的词汇效率以实现高效经济的LLM-based文本到SQL解决方案，此外还对在上下文学习中应用开源LLMs进行了研究，并进行了任务特定的性能优化。 |
| [^101] | [Bayesian Exploration Networks.](http://arxiv.org/abs/2308.13049) | 这篇论文提出了一种贝叶斯探索网络的方法，通过在一维Bellman算子中建模不确定性，解决了贝叶斯强化学习中学习贝叶斯最优策略的计算复杂性的挑战。 |
| [^102] | [Dyadic Reinforcement Learning.](http://arxiv.org/abs/2308.07843) | 该论文介绍了一个称为二元强化学习的在线算法，用于根据上下文因素和目标人与其照顾伴侣的过去反馈，个性化地提供干预措施。该算法是贝叶斯和层次的，并通过模拟展示了良好的实证效果。 |
| [^103] | [REFORMS: Reporting Standards for Machine Learning Based Science.](http://arxiv.org/abs/2308.07832) | REFORMS是一个基于机器学习的科学报告标准，旨在解决使用机器学习方法在科学研究中出现的有效性、可重复性和可推广性失败问题。这个标准由32个问题和一套指导方针组成，可作为研究人员设计和实施科研时的参考资源。 |
| [^104] | [Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models.](http://arxiv.org/abs/2308.06534) | 本研究评估了在医学影像领域使用自监督预训练方法的可行性，比较了共同对比学习和掩码自编码器方法在CT扫描卷积模型中的性能。 |
| [^105] | [Multiclass Online Learnability under Bandit Feedback.](http://arxiv.org/abs/2308.04620) | Bandit反馈下的在线多类学习的关键在于Bandit Littlestone维度的有限性，无论标签空间是否无界。 |
| [^106] | [LOB-Based Deep Learning Models for Stock Price Trend Prediction: A Benchmark Study.](http://arxiv.org/abs/2308.01915) | 本研究通过基准研究探讨了基于LOB数据的股票价格趋势预测的15种最新DL模型的鲁棒性和泛化能力。实验证明这些模型在面对新数据时性能明显下降，对其在实际市场中的应用性提出了问题。 |
| [^107] | [A Novel Convolutional Neural Network Architecture with a Continuous Symmetry.](http://arxiv.org/abs/2308.01621) | 本文介绍了一种新的卷积神经网络架构，通过连续的对称性修改权重，具有与传统模型不同的特点，希望将对称性作为神经网络的新期望特性，并推广将偏微分方程的视角应用于ConvNet的分析和解释。 |
| [^108] | [Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes.](http://arxiv.org/abs/2307.07604) | 本论文提出了一种通过填充和置换指纹编码的方法来产生困难实例，从而在各种情景下提供平滑下界。这方法适用于差分隐私平均问题和近似k. |
| [^109] | [Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks.](http://arxiv.org/abs/2307.05318) | 这项工作提出了一种使用深度集成神经网络在端点设备上预测小分子溶解度的方法，通过静态网站运行，同时具备预测不确定性，并实现了令人满意的结果。 |
| [^110] | [Concept-Oriented Deep Learning with Large Language Models.](http://arxiv.org/abs/2306.17089) | 本文讨论了大型语言模型在概念导向深度学习中的应用，包括从文本和图像中提取概念和概念图。同时也探讨了多模态语言模型在表达人类知识方面的优势。 |
| [^111] | [Principles and Guidelines for Evaluating Social Robot Navigation Algorithms.](http://arxiv.org/abs/2306.16740) | 本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。 |
| [^112] | [PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning.](http://arxiv.org/abs/2306.06394) | PEAR是一种基于原始操作的自适应重标记方法，用于Boosting层次强化学习。它通过对专家演示进行自适应重标记来生成高效的子目标监督，并通过联合优化强化学习和模仿学习来训练分层代理。实验结果显示，PEAR能够在具有挑战性的机器人环境中取得良好的性能。 |
| [^113] | [Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal.](http://arxiv.org/abs/2306.04502) | 本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。 |
| [^114] | [Adaptive PD Control using Deep Reinforcement Learning for Local-Remote Teleoperation with Stochastic Time Delays.](http://arxiv.org/abs/2305.16979) | 本论文引入了一种使用深度强化学习来解决局部-远程遥操作中的时延问题的自适应PD控制方法，通过实时调整控制器参数，改善同步性和稳定性，并克服了随机延迟带来的挑战。 |
| [^115] | [Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN Acceleration and 30%-Boost Adaptive Body Biasing.](http://arxiv.org/abs/2305.08415) | Marsellus是一款具有2至8位DNN加速和30%自适应体偏化的异构RISC-V AI-IoT末端SoC，适用于计算密集型的深度神经网络推理以及高精度浮点运算的信号处理和控制。 |
| [^116] | [ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion.](http://arxiv.org/abs/2305.06395) | 本文提出了一种名为ACTC的方法，在冷启动知识图谱补全时进行主动阈值校准，可以有效地利用有限的标记元组来找到每个关系的最佳阈值，同时也结合未标记元组进行了实验。 |
| [^117] | [Uncertainty Quantification in Machine Learning for Engineering Design and Health Prognostics: A Tutorial.](http://arxiv.org/abs/2305.04933) | 本文教程介绍机器学习中的不确定性量化方法，帮助提升其安全性和可靠性，进而促进其在高风险决策背景下的广泛应用，特别关注神经网络及其在工程设计和预测健康管理中的应用。 |
| [^118] | [Multi-Head Graph Convolutional Network for Structural Connectome Classification.](http://arxiv.org/abs/2305.02199) | 本文提出了一种基于多头图卷积网络的机器学习模型，用于对大脑连接组进行分类。该模型通过不同的图卷积头部，包括边缘和节点，全面捕捉输入数据的表示。实验结果表明，该模型在性别分类任务上表现最佳，并能从连接组数据中提取互补和代表性特征。 |
| [^119] | [DeepAqua: Self-Supervised Semantic Segmentation of Wetlands from SAR Images using Knowledge Distillation.](http://arxiv.org/abs/2305.01698) | 本文提出了DeepAqua，一种利用自我监督深度学习模型，使用知识蒸馏技术来从合成孔径雷达图像中分割水域的方法。该方法不需要手动注释，可用于大规模监测湿地变化。 |
| [^120] | [A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction.](http://arxiv.org/abs/2304.13032) | 提出了一个针对软件性能预测的主动学习框架，通过将源代码解析成流增强抽象语法树图形式，构造各种无监督和有监督的图嵌入，进行主动学习，实现对未标注数据的高效利用，并比现有方法更加优越。 |
| [^121] | [In-Context Operator Learning for Differential Equation Problems.](http://arxiv.org/abs/2304.07993) | 本文提出了一种新的神经网络方法INDEED，它可以同时学习不同微分方程问题的操作符，而无需重新训练，且只需要极少的演示。 |
| [^122] | [BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised Semantic Segmentation of Medical Images.](http://arxiv.org/abs/2303.07853) | BoundaryCAM提出了一种基于边界的弱监督的优化框架，能够预测对象位置，实现精细的高精度分割掩模。 |
| [^123] | [Deep Active Learning in the Presence of Label Noise: A Survey.](http://arxiv.org/abs/2302.11075) | 深度主动学习在处理带有噪音标签的分类数据集时面临着困难。本文综述了在这种情况下深度主动学习的现状和方法，并提出了使用视觉Transformer和对比学习方法来提高标记样本选择的可能性。 |
| [^124] | [A Unifying Perspective on Multi-Calibration: Game Dynamics for Multi-Objective Learning.](http://arxiv.org/abs/2302.10863) | 本论文提出了一个统一的框架，通过将多校准问题放置在多目标学习的背景下，并利用游戏动力学的连接，实现了一系列多校准学习问题的最新保证，同时也改进了现有保证并简化了分析。 |
| [^125] | [Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches.](http://arxiv.org/abs/2301.13506) | 本文通过不同的分析管道对DNN失效的根本原因进行了实证评估，结果显示最佳管道结合了迁移学习，DBSCAN和UMAP，并几乎只产生了一个簇群。 |
| [^126] | [RouteNet-Fermi: Network Modeling with Graph Neural Networks.](http://arxiv.org/abs/2212.12070) | RouteNet-Fermi是一种基于图神经网络的自定义模型，与传统的排队理论相比，在存在真实流量模型的情况下具有更高的准确性。它能够准确预测网络的延迟、抖动和丢包情况。 |
| [^127] | [Statistical and Computational Guarantees for Influence Diagnostics.](http://arxiv.org/abs/2212.04014) | 本论文提出了统计和计算边界来保证影响诊断的有效性，通过使用高效的逆-Hessian-向量乘积实现的方法，我们可以在机器学习和人工智能领域的应用中准确识别有影响力的数据点或子集。 |
| [^128] | [Edge Video Analytics: A Survey on Applications, Systems and Enabling Techniques.](http://arxiv.org/abs/2211.15751) | 这项研究综述了边缘视频分析（EVA）的应用、系统和支持技术。随着深度学习的发展和互联设备的普及，EVA作为一种在网络边缘处理视频数据的解决方案受到越来越多的关注。 |
| [^129] | [Toward Unlimited Self-Learning MCMC with Parallel Adaptive Annealing.](http://arxiv.org/abs/2211.14024) | 提出了一种并行自适应退火的方法，将自学习Monte Carlo（SLMC）方法应用于多峰分布，实现了高效的Monte Carlo更新，并解决了建议模型的模式崩溃问题。 |
| [^130] | [Boosting Object Representation Learning via Motion and Object Continuity.](http://arxiv.org/abs/2211.09771) | 通过集成运动和连续性信息，我们提出了一种提升物体表示学习的方法，可以在无监督多对象检测模型中产生更优的物体编码，并在物体发现、收敛速度和总体潜在物体表示等方面取得明显的改进。 |
| [^131] | [DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision.](http://arxiv.org/abs/2210.16906) | DyG2Vec是一个具有自监督学习能力的动态图表征学习模型，采用了高效而有效的注意力编码器和非对比自监督学习方法，能够提取丰富的时间嵌入表示。在基准数据集上的实验结果表明，该模型在未来链接预测任务中表现出色。 |
| [^132] | [Graph Fuzzy System: Concepts, Models and Algorithms.](http://arxiv.org/abs/2210.16730) | 本文提出了一种适用于图数据建模的新型模糊系统，名为图模糊系统（GFS），通过定义相关概念、构建模型框架和算法，实现了在处理具有非欧几里得结构的图数据时保留模糊系统优势的目标。 |
| [^133] | [Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees.](http://arxiv.org/abs/2210.07893) | 本文针对高斯过程模型的数值稳定性进行了研究，通过感兴趣点的选择和计算，提供了稳定可靠的稀疏逼近方法。 |
| [^134] | [Conformal Prediction is Robust to Dispersive Label Noise.](http://arxiv.org/abs/2209.14295) | 本研究研究了Conformal Prediction方法对于标签噪声具有鲁棒性。我们找出了构建可以正确覆盖无噪声真实标签的不确定性集合的条件，并提出了对具有噪声标签的一般损失函数进行正确控制的要求。实验证明，在对抗性案例之外，使用Conformal Prediction和风险控制技术可以实现对干净真实标签的保守风险。我们还提出了一种有界尺寸噪声修正的方法，以确保实现正确的真实标签风险。 |
| [^135] | [A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated Databases.](http://arxiv.org/abs/2208.05845) | 本研究通过提供包含47种不同属性注释的大规模数据集，并对三种最先进的Deepfake检测模型进行全面分析，旨在研究公共Deepfake数据集可能带来的AI偏差问题 |
| [^136] | [Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec.](http://arxiv.org/abs/2208.03680) | 通过NeurVec，我们提出了一种基于深度学习的修正器，它能够补偿集成误差并在模拟中实现更大的时间步长。NeurVec在复杂动态系统中表现出了显著的泛化能力，加速了传统求解器，同时保持了高水平的准确性和稳定性。 |
| [^137] | [Black-box Generalization of Machine Teaching.](http://arxiv.org/abs/2206.15205) | 本文提出了一种黑盒教学假设$h^\mathcal{T}$，通过引入更紧密的松弛项，可以改进主动学习中的假设修剪方法。理论上证明，在这个教学假设的指导下，学习者可以达到比无指导的学习者更好的泛化误差和标记复杂度界限。 |
| [^138] | [Toward Dynamic Stability Assessment of Power Grid Topologies using Graph Neural Networks.](http://arxiv.org/abs/2206.06369) | 本研究通过使用图神经网络（GNN）分析电网的动态稳定性，生成了新的大型数据集，并证明了GNN在仅利用拓扑信息的情况下能够有效预测非线性目标，同时能够准确识别电网中的脆弱节点。 |
| [^139] | [Towards Disentangling Information Paths with Coded ResNeXt.](http://arxiv.org/abs/2202.05343) | 本文提出了一种新颖的方法，通过使用编码理论设计特定路径，实现了神经网络的透明度增强和分类模型的轻量级构建。 |
| [^140] | [XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks.](http://arxiv.org/abs/2111.10854) | 本文提出了两种精确快速的全连接层，即XnODR和XnIDR，通过在动态路由机制中采用XNOR处理线性投影，解决了胶囊网络在计算复杂度和准确性方面的问题。 |
| [^141] | [Optimal subgroup selection.](http://arxiv.org/abs/2109.01077) | 在回归设置中，我们提出了一个子群选择挑战，以确定回归函数超过预设阈值的特征空间区域。我们的主要贡献是确定了在样本规模和类型I错误概率上遗憾的最佳速率。 |
| [^142] | [A Pragmatic Look at Deep Imitation Learning.](http://arxiv.org/abs/2108.01867) | 本文在深度模仿学习领域以实际的视角进行了研究，重新实现了6种不同的模仿学习算法，并使用共同的离线策略算法进行了评估。通过对专家轨迹数据进行测试，比较了这些算法的性能。 |
| [^143] | [Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning.](http://arxiv.org/abs/2106.12059) | 研究了一种简单的随机策略，用于将单点获取函数适应于批量主动学习，与计算密集型的批量采集函数相比，性能相当，但使用的计算资源更少。 |
| [^144] | [Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability.](http://arxiv.org/abs/2006.04787) | 本文解决了半空间在Massart噪声下的错配学习问题，提出了一个简化的算法并回答了一些开放问题。通过黑盒知识蒸馏过程，将复杂分类器转换为同样好的合适分类器。此外，我们还提出了一个小样本的合适学习算法，并将其与矩感知技术相结合，得到了一个具有多项式时间复杂性的学习算法。 |
| [^145] | [On the Ambiguity of Rank-Based Evaluation of Entity Alignment or Link Prediction Methods.](http://arxiv.org/abs/2002.06914) | 本研究研究了实体对齐或链接预测方法的排名评估的歧义性。分析了当前评估指标的不足之处，提出了评估方法的调整，以实现对模型性能的公平、可比和可解释的评估。 |

# 详细

[^1]: DreamLLM：协同的多模态理解与创作

    DreamLLM: Synergistic Multimodal Comprehension and Creation. (arXiv:2309.11499v1 [cs.CV])

    [http://arxiv.org/abs/2309.11499](http://arxiv.org/abs/2309.11499)

    DreamLLM是一种学习框架，实现了多模态理解与创作的协同效应。通过直接采样生成语言和图像的生成模型，避免了信息损失，并获得了更全面的多模态理解。此外，DreamLLM能够生成自由形式交织内容，展现了其在零样本多模态学习任务中的卓越性能。

    

    本文介绍了DreamLLM，一种学习框架，它首次实现了多模态大型语言模型（MLLMs），利用了多模态理解与创作之间经常被忽视的协同效应。DreamLLM遵循两个基本原则。第一个原则专注于通过在原始多模态空间中进行直接采样来生成语言和图像后验的生成建模。这种方法避免了像CLIP这样的外部特征提取器所固有的限制和信息损失，并获得了更全面的多模态理解。其次，DreamLLM促进了原始的、交织的文件生成，对文本和图像内容以及非结构化布局进行建模。这使得DreamLLM能够有效地学习所有条件、边缘和联合多模态分布。作为结果，DreamLLM是第一个能够生成自由形式交织内容的MLLM。综合实验突显了DreamLLM作为零样本多模态学习任务的卓越性能。

    This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal
    
[^2]: Text2Reward：针对强化学习的自动生成密集奖励函数的自动化框架

    Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])

    [http://arxiv.org/abs/2309.11489](http://arxiv.org/abs/2309.11489)

    Text2Reward是一个无需数据的自动化框架，可以根据大型语言模型自动生成可解释、自由形式的密集奖励函数，广泛适用于各种任务，并允许人类反馈进行迭代改进。

    

    设计奖励函数是强化学习中长期以来的挑战；它需要专业知识或领域数据，导致开发成本高。为了解决这个问题，我们引入了Text2Reward，一个无需数据的框架，可基于大型语言模型（LLM）自动生成密集奖励函数。给定自然语言描述的目标，Text2Reward生成作为环境紧凑表示的可执行程序的密集奖励函数。与逆强化学习和最近使用LLM编写稀疏奖励代码的工作不同，Text2Reward生成可解释的、自由形式的密集奖励代码，可涵盖各种任务，利用现有软件包，并允许通过人类反馈进行迭代改进。我们在两个机器人操作基准（ManiSkill2，MetaWorld）和两个MuJoCo的运动环境上评估了Text2Reward。在17个操作任务中的13个任务中，使用生成的奖励代码训练的政策实现了类似或更好的性能。

    Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
    
[^3]: 避免在根查找和优化中出现不需要的点的方法：通过增加极点

    Multiplying poles to avoid unwanted points in root finding and optimization. (arXiv:2309.11475v1 [math.OC])

    [http://arxiv.org/abs/2309.11475](http://arxiv.org/abs/2309.11475)

    通过增加极点来避免根查找和优化中不需要的点，方法是将代价函数除以到目标点的距离函数的适当幂。

    

    在根查找和优化中，存在许多情况下，我们可能无法保证自己选择的方法构造的序列收敛于一个闭集合A（在这里，我们并不假设A有其他附加属性，如凸性或连通性）。在这种情况下，我们希望有一个机制来避免在算法的下一次运行中再次遇到这个点z*。在本文中，我们提出了一种新的方法来实现这一目标：我们将代价函数除以到A的距离函数的适当幂。这个想法受到了在一维函数中尝试找到所有根的启发。我们首先解释了在代价函数的最小值恰好为0的情况下这种方法的启发式方法，然后解释了如果最小值不为零该如何进行（同时允许正的最小值）。

    In root finding and optimization, there are many cases where there is a closed set $A$ one does not the sequence constructed by one's favourite method will converge to A (here, we do not assume extra properties on $A$ such as being convex or connected). For example, if one wants to find roots, and one chooses initial points in the basin of attraction for 1 root $x^*$ (a fact which one may not know before hand), then one will always end up in that root. In this case, one would like to have a mechanism to avoid this point $z^*$ in the next runs of one's algorithm.  In this paper, we propose a new method aiming to achieve this: we divide the cost function by an appropriate power of the distance function to $A$. This idea is inspired by how one would try to find all roots of a function in 1 variable. We first explain the heuristic for this method in the case where the minimum of the cost function is exactly 0, and then explain how to proceed if the minimum is non-zero (allowing both positi
    
[^4]: 无模型跟踪控制与机器学习在复杂动态轨迹中的应用

    Model-free tracking control of complex dynamical trajectories with machine learning. (arXiv:2309.11470v1 [cs.RO])

    [http://arxiv.org/abs/2309.11470](http://arxiv.org/abs/2309.11470)

    本论文提出了一个无模型的机器学习框架，通过仅使用部分观测状态来控制具有两臂的机器人操作器。该框架利用储层计算和随机输入进行训练，并展示了对多种周期性和混沌信号的跟踪控制鲁棒性。

    

    非线性跟踪控制是机器人技术中的基础问题，可以应用于民用和国防领域。传统的跟踪控制方法需要完全了解系统模型和方程。本文提出了一种无模型的机器学习框架，通过仅使用部分观测状态来控制具有两臂的机器人操作器。控制器采用了储层计算的方法，并利用随机输入进行训练。在训练中，输入包括观测到的部分状态向量和该向量的立即未来状态向量，使得神经网络将后者视为前者的未来状态。在测试和实际应用阶段，立即未来状态向量被替换为参考轨迹中的期望观测向量。通过使用各种周期性和混沌信号来验证该控制框架的有效性，并证明其对多种干扰的鲁棒性。

    Nonlinear tracking control enabling a dynamical system to track a desired trajectory is fundamental to robotics, serving a wide range of civil and defense applications. In control engineering, designing tracking control requires complete knowledge of the system model and equations. We develop a model-free, machine-learning framework to control a two-arm robotic manipulator using only partially observed states, where the controller is realized by reservoir computing. Stochastic input is exploited for training, which consists of the observed partial state vector as the first and its immediate future as the second component so that the neural machine regards the latter as the future state of the former. In the testing (deployment) phase, the immediate-future component is replaced by the desired observational vector from the reference trajectory. We demonstrate the effectiveness of the control framework using a variety of periodic and chaotic signals, and establish its robustness against m
    
[^5]: AudioFool: 高速、通用和无需同步的跨领域语音识别攻击

    AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition. (arXiv:2309.11462v1 [cs.CR])

    [http://arxiv.org/abs/2309.11462](http://arxiv.org/abs/2309.11462)

    AudioFool是一种高速、通用和无需同步的跨领域语音识别攻击，能够通过反向傅里叶变换构造对同步的不变性和对滤波的鲁棒性，实现对ASR系统的拒绝服务攻击。

    

    自动语音识别系统已经被证明容易受到对设备上命令的恶意攻击。最近的研究主要探讨如何创建这种攻击，然而一些与空中接口 (OTA) 攻击相关的问题尚未得到妥善解决。在我们的工作中，我们研究了与OTA模型兼容的强攻击所需的属性，并设计了一种可以生成具有任意所需属性的攻击的方法，即对同步的不变性和对滤波的鲁棒性：这允许对ASR系统进行拒绝服务（DoS）攻击。我们通过构建一个改进的频域攻击，并通过逆傅里叶变换实现了这些特性。我们在标准的关键词分类任务上评估了我们的方法，并在OTA环境中进行了分析，并分析了跨领域攻击的属性，以解释该方法的高效性。

    Automatic Speech Recognition systems have been shown to be vulnerable to adversarial attacks that manipulate the command executed on the device. Recent research has focused on exploring methods to create such attacks, however, some issues relating to Over-The-Air (OTA) attacks have not been properly addressed. In our work, we examine the needed properties of robust attacks compatible with the OTA model, and we design a method of generating attacks with arbitrary such desired properties, namely the invariance to synchronization, and the robustness to filtering: this allows a Denial-of-Service (DoS) attack against ASR systems. We achieve these characteristics by constructing attacks in a modified frequency domain through an inverse Fourier transform. We evaluate our method on standard keyword classification tasks and analyze it in OTA, and we analyze the properties of the cross-domain attacks to explain the efficiency of the approach.
    
[^6]: 非线性动力系统的数字孪生：一种视角。（arXiv：2309.11461v1 [cs.LG]）

    Digital twins of nonlinear dynamical systems: A perspective. (arXiv:2309.11461v1 [cs.LG])

    [http://arxiv.org/abs/2309.11461](http://arxiv.org/abs/2309.11461)

    数字孪生在各个领域引起了广泛关注。非线性动力系统的数字孪生可以生成系统演化、预测灾难性行为，并提供早期警告。数字孪生可以用于实时系统监测和预测性问题解决。两种方法构建数字孪生：稀疏优化和机器学习。

    

    数字孪生近年来在各个领域引起了广泛关注。非线性动力系统的数字孪生的基本要求是能够生成系统演化并预测潜在的灾难性突发行为，以提供及早的警告。数字孪生可以用于实时的系统“健康”监测和预测性问题解决。特别是，如果数字孪生预测未来可能发生系统崩溃，这是由环境变化或扰动引起的参数漂移导致的，可以制定并执行最优控制策略作为早期干预，以防止系统崩溃。构建非线性动力系统的数字孪生存在两种方法：稀疏优化和机器学习。本文描述了这两种方法的基本原理，并讨论了它们的优点和注意事项。

    Digital twins have attracted a great deal of recent attention from a wide range of fields. A basic requirement for digital twins of nonlinear dynamical systems is the ability to generate the system evolution and predict potentially catastrophic emergent behaviors so as to providing early warnings. The digital twin can then be used for system "health" monitoring in real time and for predictive problem solving. In particular, if the digital twin forecasts a possible system collapse in the future due to parameter drifting as caused by environmental changes or perturbations, an optimal control strategy can be devised and executed as early intervention to prevent the collapse. Two approaches exist for constructing digital twins of nonlinear dynamical systems: sparse optimization and machine learning. The basics of these two approaches are described and their advantages and caveats are discussed.
    
[^7]: 多步骤模型预测安全滤波器：通过增加预测视野减少震荡

    Multi-Step Model Predictive Safety Filters: Reducing Chattering by Increasing the Prediction Horizon. (arXiv:2309.11453v1 [cs.RO])

    [http://arxiv.org/abs/2309.11453](http://arxiv.org/abs/2309.11453)

    本文提出了一种通过增加预测视野来减少震荡的多步骤模型预测安全滤波器方法，通过将长期输入修正考虑在内，对有界模型不确定性下的递归可行性进行了证明，并在仿真和四旋翼实验中进行了验证。

    

    在各种任务中，基于学习的控制器表现出比经典控制器更好的性能。然而，提供安全保证并不容易。安全，即满足状态和输入约束，可以通过将学习到的控制策略与安全滤波器相结合来保证。模型预测安全滤波器（MPSFs）是一种基于模型预测控制（MPC）的常见安全滤波方法。MPSFs旨在在立即的下一个时间步骤中保证安全，同时最小化提出的输入与应用的输入之间的差异。这种有限的预见性可能导致在约束边界附近出现颤动运动和不希望的振荡，即所谓的震荡现象。在本文中，我们通过考虑更长的前瞻性输入修正来减少震荡。在有界模型不确定性的假设下，我们使用鲁棒MPC的技术证明了递归可行性。我们在广泛的仿真和四旋翼实验中验证了所提出的方法。

    Learning-based controllers have demonstrated superior performance compared to classical controllers in various tasks. However, providing safety guarantees is not trivial. Safety, the satisfaction of state and input constraints, can be guaranteed by augmenting the learned control policy with a safety filter. Model predictive safety filters (MPSFs) are a common safety filtering approach based on model predictive control (MPC). MPSFs seek to guarantee safety while minimizing the difference between the proposed and applied inputs in the immediate next time step. This limited foresight can lead to jerky motions and undesired oscillations close to constraint boundaries, known as chattering. In this paper, we reduce chattering by considering input corrections over a longer horizon. Under the assumption of bounded model uncertainties, we prove recursive feasibility using techniques from robust MPC. We verified the proposed approach in both extensive simulation and quadrotor experiments. In exp
    
[^8]: Isolation Forest的分布和容量基准评分方法

    Distribution and volume based scoring for Isolation Forests. (arXiv:2309.11450v1 [stat.ML])

    [http://arxiv.org/abs/2309.11450](http://arxiv.org/abs/2309.11450)

    本文在Isolation Forest方法中提出了两个贡献，分别是对评分函数的信息理论推广和在个体树估计器层面上使用基于超体积的评分函数。这两种方法均对一些数据集相对于标准Isolation Forest表现出显著改进，并且其中一种方法在所有数据集上平均表现出改进。

    

    我们对异常和离群值检测方法Isolation Forest做出了两项贡献。第一个贡献是对用于聚合随机树估计器的得分函数进行了信息理论上的推广。这个推广允许考虑整个分布，而不仅仅是树的集成平均值。第二个贡献是在单个树估计器层面上替换了Isolation Forest基于深度的得分方法，改为基于隔离树叶节点的超体积的得分方法。我们通过生成的数据对这两种方法进行了验证，并在来自最近和详尽的“ADBench”基准的34个数据集上进行了评估，在某些数据集上两种变体相对于标准的Isolation Forest表现出显著的改进，并在所有数据集上对其中一种变体平均显示出改进。代码可复现。

    We make two contributions to the Isolation Forest method for anomaly and outlier detection. The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators. This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution. The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree's leaf nodes.  We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive ``ADBench'' benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants. The code to reproduce
    
[^9]: 加权平均改善了领域转移下的知识蒸馏

    Weight Averaging Improves Knowledge Distillation under Domain Shift. (arXiv:2309.11446v1 [cs.LG])

    [http://arxiv.org/abs/2309.11446](http://arxiv.org/abs/2309.11446)

    加权平均技术在领域转移下的知识蒸馏中也能提高性能，可以应用于没有验证数据的简单加权平均策略。

    

    知识蒸馏是一种广泛应用于实际深度学习应用中的强大模型压缩技术。它专注于训练一个小型的学生网络来模仿一个较大的教师网络。尽管人们普遍认为知识蒸馏可以在i.i.d的情况下改善学生的泛化能力，但在领域转移下学生网络的性能，即在训练期间未见过的领域数据上的性能，在文献中得到了很少的关注。在本文中，我们在知识蒸馏和领域泛化的研究领域之间迈出了一步。我们展示了在领域转移下，领域泛化文献中提出的加权平均技术，如SWAD和SMA，也可以提高知识蒸馏的性能。此外，我们提出了一种简单的加权平均策略，不需要在训练过程中对验证数据进行评估，并且展示了当应用于知识蒸馏时，它与SWAD和SMA的性能相当。我们将其命名为f

    Knowledge distillation (KD) is a powerful model compression technique broadly used in practical deep learning applications. It is focused on training a small student network to mimic a larger teacher network. While it is widely known that KD can offer an improvement to student generalization in i.i.d setting, its performance under domain shift, i.e. the performance of student networks on data from domains unseen during training, has received little attention in the literature. In this paper we make a step towards bridging the research fields of knowledge distillation and domain generalization. We show that weight averaging techniques proposed in domain generalization literature, such as SWAD and SMA, also improve the performance of knowledge distillation under domain shift. In addition, we propose a simplistic weight averaging strategy that does not require evaluation on validation data during training and show that it performs on par with SWAD and SMA when applied to KD. We name our f
    
[^10]: 签名激活: 一种用于整体显著性的稀疏信号观点

    Signature Activation: A Sparse Signal View for Holistic Saliency. (arXiv:2309.11443v1 [cs.CV])

    [http://arxiv.org/abs/2309.11443](http://arxiv.org/abs/2309.11443)

    该研究介绍了一种称为签名激活的显著性方法，利用稀疏信号观点生成对卷积神经网络输出的整体和类别不可知的解释。该方法在医学图像分析和临床病变检测中具有潜在应用价值。

    

    在医疗领域中，机器学习的应用需要模型的透明度和可解释性。在本研究中，我们引入了一种称为签名激活的显著性方法，用于生成对卷积神经网络（CNN）输出的整体和类别不可知的解释。我们的方法利用了某些类型的医学图像（如血管造影图像）具有清晰的前景和背景对象的事实。我们给出了理论解释来证明我们的方法的有效性。通过评估我们的方法在冠状动脉造影图像中辅助病变检测的效果，我们展示了我们方法在临床设置中的潜在应用。

    The adoption of machine learning in healthcare calls for model transparency and explainability. In this work, we introduce Signature Activation, a saliency method that generates holistic and class-agnostic explanations for Convolutional Neural Network (CNN) outputs. Our method exploits the fact that certain kinds of medical images, such as angiograms, have clear foreground and background objects. We give theoretical explanation to justify our methods. We show the potential use of our method in clinical settings through evaluating its efficacy for aiding the detection of lesions in coronary angiograms.
    
[^11]: 半导体制造业中无监督故障检测的时间序列数据生成预训练

    Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing. (arXiv:2309.11427v1 [cs.LG])

    [http://arxiv.org/abs/2309.11427](http://arxiv.org/abs/2309.11427)

    本研究提出了TRACE-GPT模型，用于半导体制造业中无监督故障检测。通过使用时间卷积嵌入和生成式预训练Transformer来预训练时间序列数据，并使用交叉熵损失函数进行异常序列和正常序列的分类，实验结果表明模型具有更好的性能。

    

    本文介绍了TRACE-GPT（Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers），它是用于半导体制造中的未标记数据集上预训练单变量时间序列传感器数据并检测故障的模型。研究表明，通过使用时间卷积嵌入和生成式预训练Transformer（GPT）来提取时间序列数据的特征，并使用交叉熵损失函数对异常序列和正常序列进行分类，我们的模型表现出更好的性能。

    This paper introduces TRACE-GPT, which stands for Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor data and detect faults on unlabeled datasets in semiconductor manufacturing. In semiconductor industry, classifying abnormal time-series sensor data from normal data is important because it is directly related to wafer defect. However, small, unlabeled, and even mixed training data without enough anomalies make classification tasks difficult. In this research, we capture features of time-series data with temporal convolutional embedding and Generative Pre-trained Transformer (GPT) to classify abnormal sequences from normal sequences using cross entropy loss. We prove that our model shows better performance than previous unsupervised models with both an open dataset, the University of California Riverside (UCR) time-series classification archive, and the process log of our Ch
    
[^12]: 深度神经网络作为降噪算法：高维图形模型中扩散模型的高效学习

    Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models. (arXiv:2309.11420v1 [cs.LG])

    [http://arxiv.org/abs/2309.11420](http://arxiv.org/abs/2309.11420)

    深度神经网络可以作为降噪算法在高维图形模型中学习扩散模型，为生成建模提供了高效的逼近方法。

    

    我们研究了深度神经网络在基于扩散的生成建模中通过评分函数的逼近效率。尽管现有的近似理论利用了评分函数的平滑性，但它们在本质上高维数据中受到维度灾难的困扰。这种限制在图形模型（如马尔可夫随机场）中尤为明显，这是图像分布常见的类型，评分函数的近似效率尚未确立。为了解决这个问题，我们观察到评分函数在图形模型中通常可以通过变分推断降噪算法进行较好的逼近。此外，这些算法适用于高效的神经网络表示。我们在图形模型的例子中进行了演示，包括伊辛模型、条件伊辛模型、受限玻尔兹曼机和稀疏编码模型。结合基于扩散采样的现成离散化误差界限，我们提供了一种高效的样本方法。

    We investigate the approximation efficiency of score functions by deep neural networks in diffusion-based generative modeling. While existing approximation theories utilize the smoothness of score functions, they suffer from the curse of dimensionality for intrinsically high-dimensional data. This limitation is pronounced in graphical models such as Markov random fields, common for image distributions, where the approximation efficiency of score functions remains unestablished.  To address this, we observe score functions can often be well-approximated in graphical models through variational inference denoising algorithms. Furthermore, these algorithms are amenable to efficient neural network representation. We demonstrate this in examples of graphical models, including Ising models, conditional Ising models, restricted Boltzmann machines, and sparse encoding models. Combined with off-the-shelf discretization error bounds for diffusion-based sampling, we provide an efficient sample com
    
[^13]: EDMP: 基于成本引导的扩散运动规划

    EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning. (arXiv:2309.11414v1 [cs.RO])

    [http://arxiv.org/abs/2309.11414](http://arxiv.org/abs/2309.11414)

    EDMP是一种结合了传统和基于深度学习运动规划优势的基于成本引导的扩散运动规划方法，通过对多样化的运动学有效轨迹进行训练，来提高解决方案的成功率。

    

    传统的机器人操纵运动规划包括一组通用算法，旨在最小化执行给定计划的特定于场景的成本。这种方法具有显著的适应性，因为它们可以直接使用于任何新场景，无需特定的训练数据集。然而，如果没有对多样有效轨迹有先验了解，并且没有针对给定场景设计的成本函数，解决方案的成功率往往较低。虽然基于深度学习的算法极大地提高了成功率，但没有专门的训练数据集很难应用。我们提出了EDMP，一种基于成本引导的扩散运动规划集成，旨在结合传统和基于深度学习的运动规划的优势。我们的扩散网络在一组多样化的运动学有效轨迹上进行训练。与传统规划一样，在推断时对于任何新场景，我们计算特定于场景的成本函数。

    Classical motion planning for robotic manipulation includes a set of general algorithms that aim to minimize a scene-specific cost of executing a given plan. This approach offers remarkable adaptability, as they can be directly used off-the-shelf for any new scene without needing specific training datasets. However, without a prior understanding of what diverse valid trajectories are and without specially designed cost functions for a given scene, the overall solutions tend to have low success rates. While deep-learning-based algorithms tremendously improve success rates, they are much harder to adopt without specialized training datasets. We propose EDMP, an Ensemble-of-costs-guided Diffusion for Motion Planning that aims to combine the strengths of classical and deep-learning-based motion planning. Our diffusion-based network is trained on a set of diverse kinematically valid trajectories. Like classical planning, for any new scene at the time of inference, we compute scene-specific 
    
[^14]: Transformers对电子交易的比较测试

    Transformers versus LSTMs for electronic trading. (arXiv:2309.11400v1 [q-fin.TR])

    [http://arxiv.org/abs/2309.11400](http://arxiv.org/abs/2309.11400)

    本研究比较了基于LSTM和基于Transformer的模型在金融时间序列预测任务上的表现，并介绍了一种新的LSTM-based模型DLSTM。

    

    随着人工智能的快速发展，长短期记忆(LSTM)作为一种循环神经网络(RNN)被广泛应用于时间序列预测。与RNN相似，Transformer被设计用来处理序列数据。由于Transformer在自然语言处理(NLP)领域取得了巨大成功，研究者开始关注Transformer在时间序列预测中的表现，并最近提出了许多基于Transformer的长时间序列预测解决方案。然而，当涉及金融时间序列预测时，LSTM仍然是主流的架构。因此，本研究想要回答的问题是：Transformer基于模型是否可以应用在金融时间序列预测中，并击败LSTM。为了回答这个问题，本研究在基于高频限价委托簿数据的多个金融预测任务上比较了多种基于LSTM和Transformer的模型。创建了一个名为DLSTM的新型LSTM-based模型，并提出了新的架构。

    With the rapid development of artificial intelligence, long short term memory (LSTM), one kind of recurrent neural network (RNN), has been widely applied in time series prediction.  Like RNN, Transformer is designed to handle the sequential data. As Transformer achieved great success in Natural Language Processing (NLP), researchers got interested in Transformer's performance on time series prediction, and plenty of Transformer-based solutions on long time series forecasting have come out recently. However, when it comes to financial time series prediction, LSTM is still a dominant architecture. Therefore, the question this study wants to answer is: whether the Transformer-based model can be applied in financial time series prediction and beat LSTM.  To answer this question, various LSTM-based and Transformer-based models are compared on multiple financial prediction tasks based on high-frequency limit order book data. A new LSTM-based model called DLSTM is built and new architecture f
    
[^15]: 预处理联邦学习

    Preconditioned Federated Learning. (arXiv:2309.11378v1 [cs.LG])

    [http://arxiv.org/abs/2309.11378](http://arxiv.org/abs/2309.11378)

    本文提出了基于两个自适应框架的新的通信高效联邦学习算法：PreFed和PreFedOp，通过使用新颖的协方差矩阵预处理器来实现适应性。实验证明这些方法在i.i.d.和非i.i.d.设置下均取得了最先进的性能。

    

    联邦学习（FL）是一种分布式机器学习方法，可以以高效的通信和保护隐私的方式进行模型训练。FL中的标准优化方法是联邦平均（FedAvg），它在通信轮之间执行多个本地SGD步骤。与现代的一阶自适应优化相比，FedAvg被认为缺乏算法适应性。本文提出了基于两个自适应框架（本地适应性和服务器端适应性）的新的高效通信FL算法：PreFed和PreFedOp。提出的方法通过使用新颖的协方差矩阵预处理器来实现适应性。理论上，我们为我们的算法提供了收敛性保证。实证实验表明，我们的方法在i.i.d.和非i.i.d.设置上都达到了最先进的性能。

    Federated Learning (FL) is a distributed machine learning approach that enables model training in communication efficient and privacy-preserving manner. The standard optimization method in FL is Federated Averaging (FedAvg), which performs multiple local SGD steps between communication rounds. FedAvg has been considered to lack algorithm adaptivity compared to modern first-order adaptive optimizations. In this paper, we propose new communication-efficient FL algortithms based on two adaptive frameworks: local adaptivity (PreFed) and server-side adaptivity (PreFedOp). Proposed methods adopt adaptivity by using a novel covariance matrix preconditioner. Theoretically, we provide convergence guarantees for our algorithms. The empirical experiments show our methods achieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings.
    
[^16]: 从时间序列电子健康记录学习患者静态信息及保护隐私和公平性的方法

    Learning Patient Static Information from Time-series EHR and an Approach for Safeguarding Privacy and Fairness. (arXiv:2309.11373v1 [cs.LG])

    [http://arxiv.org/abs/2309.11373](http://arxiv.org/abs/2309.11373)

    本文研究了从时间序列电子健康记录数据预测患者静态信息的能力，并发现不仅原始数据，机器学习模型学习到的表示也可以用于预测各种静态信息，包括生物学性别、二值化年龄和自报种族，具有高预测性能。

    

    最近的医疗机器学习研究引起了人们对患者隐私和算法公平性的关注。例如，之前的研究表明，即使医疗数据中没有明确包含种族信息，也可以通过医疗数据预测患者自报的种族信息。然而，我们对数据识别的程度没有了解，也缺乏开发模型以最小程度受到这些信息影响的方法。在这里，我们系统地研究了时间序列电子健康记录数据预测患者静态信息的能力。我们发现，不仅原始的时间序列数据，还有从机器学习模型学习到的表示，都可以被训练用来预测各种静态信息，其接收者操作特征曲线下面积可达到0.851（对生物学性别）、0.869（对二值化年龄）和0.810（对自报种族）。这种高预测性能可以扩展到广泛的共病因素，并且即使在模型被解释时也存在。

    Recent work in machine learning for healthcare has raised concerns about patient privacy and algorithmic fairness. For example, previous work has shown that patient self-reported race can be predicted from medical data that does not explicitly contain racial information. However, the extent of data identification is unknown, and we lack ways to develop models whose outcomes are minimally affected by such information. Here we systematically investigated the ability of time-series electronic health record data to predict patient static information. We found that not only the raw time-series data, but also learned representations from machine learning models, can be trained to predict a variety of static information with area under the receiver operating characteristic curve as high as 0.851 for biological sex, 0.869 for binarized age and 0.810 for self-reported race. Such high predictive performance can be extended to a wide range of comorbidity factors and exists even when the model was
    
[^17]: 3D人脸重建：通往法医学的道路

    3D Face Reconstruction: the Road to Forensics. (arXiv:2309.11357v1 [cs.CV])

    [http://arxiv.org/abs/2309.11357](http://arxiv.org/abs/2309.11357)

    本综述研究了3D人脸重建算法的应用，从整形手术到娱乐行业，但在法医学领域的应用仍然有一些限制和障碍。对于将3D人脸重建作为法医学证据的有效工具，我们仍需进行进一步的研究和探索。

    

    3D人脸重建算法通过图像和视频在许多领域得到应用，从整形手术到娱乐行业，得益于其优势特点。然而，当涉及法医应用时，3D人脸重建必须遵守严格的要求，这仍然使其在提供诉讼证据方面的潜在作用不明确。对其在法医学应用中的约束、潜力和限制进行全面的研究仍然缺失。本综述旨在阐明法医应用和生物特征识别之间的关系，重点关注人脸识别。因此，它对来自监控视频和嫌疑犯照片的3D人脸重建算法的成果进行了分析，并讨论了目前阻碍3D人脸重建在法医应用中发挥积极作用的障碍。最后，它还对底层数据集进行了检查，包括其优点和限制。

    3D face reconstruction algorithms from images and videos are applied to many fields, from plastic surgery to the entertainment sector, thanks to their advantageous features. However, when looking at forensic applications, 3D face reconstruction must observe strict requirements that still make its possible role in bringing evidence to a lawsuit unclear. An extensive investigation of the constraints, potential, and limits of its application in forensics is still missing. Shedding some light on this matter is the goal of the present survey, which starts by clarifying the relation between forensic applications and biometrics, with a focus on face recognition. Therefore, it provides an analysis of the achievements of 3D face reconstruction algorithms from surveillance videos and mugshot images and discusses the current obstacles that separate 3D face reconstruction from an active role in forensic applications. Finally, it examines the underlying data sets, with their advantages and limitati
    
[^18]: 自监督学习揭示了街景图像中城市住房的变化

    Self-supervised learning unveils change in urban housing from street-level images. (arXiv:2309.11354v1 [cs.CV])

    [http://arxiv.org/abs/2309.11354](http://arxiv.org/abs/2309.11354)

    本论文使用自监督学习方法分析伦敦的城市变化，通过应用于1500万张街景图像，成功地识别出了住房供应的变化，并区分了主要和次要变化。

    

    全球各地的城市都面临着可负担和体面住房严重短缺的问题。尽管这对政策至关重要，但我们在有效监测和追踪城市住房进展方面的能力有限。应用于街景图像的基于深度学习的计算机视觉方法在测量社会经济和环境不平等方面取得了成功，但由于时变标签通常不可用，它们并没有充分利用时间图像来跟踪城市变化。我们使用自监督方法在2008年至2021年之间使用1500万张伦敦街景图像来测量变化。我们对Barlow Twins的新颖改进Street2Vec，在不需要手动注释的情况下，嵌入了城市结构，并对季节性和日常变化具有不变性。它优于通用嵌入，成功地从街景图像中识别了伦敦住房供应的点级变化，并区分了主要和次要变化。这种能力可以为城市规划提供及时信息。

    Cities around the world face a critical shortage of affordable and decent housing. Despite its critical importance for policy, our ability to effectively monitor and track progress in urban housing is limited. Deep learning-based computer vision methods applied to street-level images have been successful in the measurement of socioeconomic and environmental inequalities but did not fully utilize temporal images to track urban change as time-varying labels are often unavailable. We used self-supervised methods to measure change in London using 15 million street images taken between 2008 and 2021. Our novel adaptation of Barlow Twins, Street2Vec, embeds urban structure while being invariant to seasonal and daily changes without manual annotations. It outperformed generic embeddings, successfully identified point-level change in London's housing supply from street-level images, and distinguished between major and minor change. This capability can provide timely information for urban plann
    
[^19]: C⋅ASE：学习基于物理的角色的条件对抗技能嵌入

    C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters. (arXiv:2309.11351v1 [cs.GR])

    [http://arxiv.org/abs/2309.11351](http://arxiv.org/abs/2309.11351)

    C⋅ASE是一个学习基于物理的角色的条件对抗技能嵌入的框架，通过将异构的技能动作划分为不同子集，以实现多样性和可控性。训练过程中采用了焦点技能采样、骨骼残余力和逐元素特征屏蔽，以平衡不同复杂性的技能，并捕捉更一般的行为特征。

    

    我们提出了C⋅ASE，一个高效而有效的框架，用于学习基于物理的角色的条件对抗技能嵌入。我们的物理模拟角色可以学习各种技能，同时提供以直接操纵执行的技能的可控性。C⋅ASE将异构的技能动作划分为包含同质样本的不同子集，用于训练低级条件模型来学习条件行为分布。训练过程中采用了焦点技能采样、骨骼残余力和逐元素特征屏蔽，以平衡不同复杂性的各种技能，减轻动力学不匹配以掌握敏捷动作，并捕捉更多的一般行为特征。一旦训练完成，条件模型就可以生成高度多样且逼真的技能。

    We present C$\cdot$ASE, an efficient and effective framework that learns conditional Adversarial Skill Embeddings for physics-based characters. Our physically simulated character can learn a diverse repertoire of skills while providing controllability in the form of direct manipulation of the skills to be performed. C$\cdot$ASE divides the heterogeneous skill motions into distinct subsets containing homogeneous samples for training a low-level conditional model to learn conditional behavior distribution. The skill-conditioned imitation learning naturally offers explicit control over the character's skills after training. The training course incorporates the focal skill sampling, skeletal residual forces, and element-wise feature masking to balance diverse skills of varying complexities, mitigate dynamics mismatch to master agile motions and capture more general behavior characteristics, respectively. Once trained, the conditional model can produce highly diverse and realistic skills, o
    
[^20]: GECTurk：用于土耳其语的语法错误纠正和检测数据集

    GECTurk: Grammatical Error Correction and Detection Dataset for Turkish. (arXiv:2309.11346v1 [cs.CL])

    [http://arxiv.org/abs/2309.11346](http://arxiv.org/abs/2309.11346)

    GECTurk是一个用于土耳其语的语法错误纠正和检测数据集。它采用灵活且可扩展的合成数据生成流水线，覆盖了20多个专家策划的语法和拼写规则，并且通过手动注释电影评论创造了更真实的测试集。

    

    语法错误检测和纠正（GEC）工具已经被证明对于母语使用者和第二语言学习者非常有用。开发这样的工具需要大量平行的、注释的数据，但是对于大多数语言来说，这种数据是不可得到的。合成数据生成是克服这种数据稀缺的常见做法。然而，对于土耳其语这样形态丰富的语言来说，并不直接，因为复杂的写作规则需要音韵、形态和句法信息。在这项工作中，我们提出了一个灵活且可扩展的土耳其语合成数据生成流水线，涵盖了20多个专家策划的语法和拼写规则（即写作规则），通过复杂的转换函数实现。使用这个流水线，我们从专业编辑的文章中派生出了13万条高质量的平行句子。此外，我们通过手动注释一组电影评论来创建一个更真实的测试集。我们实现了三个基准线，制定了任务的

    Grammatical Error Detection and Correction (GEC) tools have proven useful for native speakers and second language learners. Developing such tools requires a large amount of parallel, annotated data, which is unavailable for most languages. Synthetic data generation is a common practice to overcome the scarcity of such data. However, it is not straightforward for morphologically rich languages like Turkish due to complex writing rules that require phonological, morphological, and syntactic information. In this work, we present a flexible and extensible synthetic data generation pipeline for Turkish covering more than 20 expert-curated grammar and spelling rules (a.k.a., writing rules) implemented through complex transformation functions. Using this pipeline, we derive 130,000 high-quality parallel sentences from professionally edited articles. Additionally, we create a more realistic test set by manually annotating a set of movie reviews. We implement three baselines formulating the tas
    
[^21]: 使用属性引导方法来理解公平性约束的影响

    Using Property Elicitation to Understand the Impacts of Fairness Constraints. (arXiv:2309.11343v1 [cs.LG])

    [http://arxiv.org/abs/2309.11343](http://arxiv.org/abs/2309.11343)

    这项研究使用属性引导方法来探索损失函数和正则化函数与最优决策之间的关系，特别是在公平机器学习中的应用。它提供了损失函数和正则化函数成对时属性改变的必要和充分条件，并通过实验证明了算法决策与数据分布变化和约束难度的相关性。

    

    预测算法通常通过优化某个损失函数进行训练，并添加正则化函数来施加违反约束的惩罚。预期地，添加这样的正则化函数可以改变目标函数的最小化值。目前还不清楚哪些正则化函数会改变损失函数的最小化值，以及当最小化值发生变化时，它会如何变化。我们使用属性引导方法来初步了解损失函数和正则化函数与给定问题实例的最优决策之间的联合关系。具体而言，我们给出了损失函数和正则化函数成对时，属性改变的必要和充分条件，并研究了一些满足这个条件的正则化函数在公平机器学习文献中的标准。我们通过实验证明了算法决策如何随着数据分布的变化和约束的难度而改变。

    Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraint
    
[^22]: 用边异构图神经网络改进文章分类

    Improving Article Classification with Edge-Heterogeneous Graph Neural Networks. (arXiv:2309.11341v1 [cs.LG])

    [http://arxiv.org/abs/2309.11341](http://arxiv.org/abs/2309.11341)

    本论文提出了一种使用边异构图神经网络改进文章分类的方法，通过加入高阶语义的节点特征生成，能够显著提高分类性能。

    

    鉴于现有和新发布的文章数量庞大，将研究成果分类到特定上下文标签体系是一项具有挑战性和相关性的下游任务。我们提出了一种方法，通过使用边异构图表示来丰富简单的图神经网络（GNN）流水线，以提高文章分类的性能。我们使用SciBERT来生成节点特征，以捕捉文章的文本元数据中的高阶语义。我们在Open Graph Benchmark（OGB）ogbn-arxiv数据集和PubMed糖尿病数据集上进行了完全监督的传导式节点分类实验，分别通过Microsoft Academic Graph（MAG）和PubMed Central添加了附加元数据。结果表明，边异构图相对于边同构图，能够始终提高所有GNN模型的性能。转换后的数据使简单且浅层的GNN流水线能够与更复杂的架构相媲美的结果。

    Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architect
    
[^23]: 利用数据收集和无监督学习进行切换突尼斯阿拉伯语的自动语音识别

    Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition. (arXiv:2309.11327v1 [eess.AS])

    [http://arxiv.org/abs/2309.11327](http://arxiv.org/abs/2309.11327)

    本研究通过收集和标注数据以及探索切换方法，提出了一种有效的突尼斯方言自动语音识别解决方案，并且通过人工评估来消除拼写不合适的干扰。

    

    开发能够有效识别方言的自动语音识别（ASR）解决方案需要创新的方法，不仅要解决数据稀缺问题，还要处理语言多样性的复杂性。本文针对突尼斯方言，解决了上述ASR挑战。首先，收集了文本和音频数据，并且在某些情况下进行了标注。其次，我们探索自我监督、半监督和少样本切换方法，以在不同突尼斯测试集上推动最先进的技术；涵盖不同的声学、语言和韵律条件。最后，鉴于常规拼写的缺失，我们对转录进行人工评估，以避免测试参考中的拼写不合适所带来的噪声。我们的模型可以转录突尼斯阿拉伯语、英语和法语混合语言的音频样本，并公开发布了所有训练和测试所使用的数据，供公众使用和进一步改进。

    Crafting an effective Automatic Speech Recognition (ASR) solution for dialects demands innovative approaches that not only address the data scarcity issue but also navigate the intricacies of linguistic diversity. In this paper, we address the aforementioned ASR challenge, focusing on the Tunisian dialect. First, textual and audio data is collected and in some cases annotated. Second, we explore self-supervision, semi-supervision and few-shot code-switching approaches to push the state-of-the-art on different Tunisian test sets; covering different acoustic, linguistic and prosodic conditions. Finally, and given the absence of conventional spelling, we produce a human evaluation of our transcripts to avoid the noise coming from spelling inadequacies in our testing references. Our models, allowing to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English and French, and all the data used during training and testing are released for public use and further improvem
    
[^24]: WFTNet：利用全局和局部周期性在长期时间序列预测中的应用

    WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting. (arXiv:2309.11319v1 [cs.LG])

    [http://arxiv.org/abs/2309.11319](http://arxiv.org/abs/2309.11319)

    本文提出了一种利用全局和局部周期性的Wavelet-Fourier变换网络（WFTNet）用于长期时间序列预测。在利用傅里叶和小波变换提取时频信息的基础上，引入了周期重要性加权系数（PWC）来平衡全局和局部频率模式的重要性。大量实验证明，WFTNet在各种时间序列数据集上优于其他基线模型。

    

    最近的卷积神经网络和Transformer模型尝试利用频率和周期性信息进行长期时间序列预测。然而，大多数现有的工作都基于傅里叶变换，无法捕捉到细粒度和局部频率结构。在本文中，我们提出了一种Wavelet-Fourier Transform Network（WFTNet）用于长期时间序列预测。WFTNet利用傅里叶变换和小波变换从信号中提取全面的时频信息，其中傅里叶变换捕捉全局周期模式，而小波变换捕捉局部周期模式。此外，我们引入了一种周期重要性加权系数（PWC），以自适应地平衡全局和局部频率模式的重要性。在各种时间序列数据集上进行的大量实验证明，WFTNet始终优于其他最先进的基线模型。

    Recent CNN and Transformer-based models tried to utilize frequency and periodicity information for long-term time series forecasting. However, most existing work is based on Fourier transform, which cannot capture fine-grained and local frequency structure. In this paper, we propose a Wavelet-Fourier Transform Network (WFTNet) for long-term time series forecasting. WFTNet utilizes both Fourier and wavelet transforms to extract comprehensive temporal-frequency information from the signal, where Fourier transform captures the global periodic patterns and wavelet transform captures the local ones. Furthermore, we introduce a Periodicity-Weighted Coefficient (PWC) to adaptively balance the importance of global and local frequency patterns. Extensive experiments on various time series datasets show that WFTNet consistently outperforms other state-of-the-art baseline.
    
[^25]: 创建和发现平面：为连续学习构建平坦训练空间

    Create and Find Flatness: Building Flat Training Spaces in Advance for Continual Learning. (arXiv:2309.11305v1 [cs.LG])

    [http://arxiv.org/abs/2309.11305](http://arxiv.org/abs/2309.11305)

    该论文提出了一个名为C&F的框架，通过提前为每个任务构建一个平坦的训练空间，在连续学习中解决了灾难性遗忘的问题。该框架在学习当前任务时自适应地创建平坦区域，并根据参数的平坦程度找到其对当前任务的重要性。在适应新任务时，利用平坦度应用约束并为新任务准备平坦空间。

    

    在连续学习领域中，灾难性遗忘仍然是一个严重的挑战，神经网络在吸收新信息时很难保留先前的知识。大多数现有研究只注重在遇到新任务时缓解这个问题，忽视了任务前阶段的重要性。因此，我们将注意力转移到当前任务学习阶段，提出了一个新的框架C&F（创建和发现平面），该框架为每个任务提前建立一个平坦的训练空间。具体来说，在学习当前任务时，我们的框架自适应地创建了一个在损失空间中最小值周围的平坦区域。随后，它根据参数的平坦度找到它们对当前任务的重要性。在适应模型到新任务时，根据平坦度应用约束，并同时为即将到来的任务准备一个平坦空间。我们在理论上证明了创建和发现平坦度之间的一致性。

    Catastrophic forgetting remains a critical challenge in the field of continual learning, where neural networks struggle to retain prior knowledge while assimilating new information. Most existing studies emphasize mitigating this issue only when encountering new tasks, overlooking the significance of the pre-task phase. Therefore, we shift the attention to the current task learning stage, presenting a novel framework, C&F (Create and Find Flatness), which builds a flat training space for each task in advance. Specifically, during the learning of the current task, our framework adaptively creates a flat region around the minimum in the loss landscape. Subsequently, it finds the parameters' importance to the current task based on their flatness degrees. When adapting the model to a new task, constraints are applied according to the flatness and a flat space is simultaneously prepared for the impending task. We theoretically demonstrate the consistency between the created and found flatne
    
[^26]: CPLLM: 基于大规模语言模型的临床预测

    CPLLM: Clinical Prediction with Large Language Models. (arXiv:2309.11295v1 [cs.CL])

    [http://arxiv.org/abs/2309.11295](http://arxiv.org/abs/2309.11295)

    CPLLM是一种使用大规模语言模型进行临床疾病预测的方法。通过量化和提示来微调语言模型，利用患者的历史诊断记录来预测目标疾病的诊断结果。实验证明，CPLLM在各项指标上均超越了其他基线模型，显示出显著的改进。

    

    我们提出了一种使用大规模语言模型 (LLM) 进行临床疾病预测的方法，该方法包括对预训练的语言模型进行微调。我们利用量化和提示来微调LLM，任务是预测患者在下一次就诊或随后的诊断中是否会被诊断为目标疾病，并利用他们的历史诊断记录。我们将结果与多个基线模型进行了比较，包括逻辑回归、RETAIN和Med-BERT，后者是使用结构化电子病历数据进行疾病预测的当前最先进模型。实验结果显示，CPLLM在PR-AUC和ROC-AUC指标上均超过了所有测试模型，相比基线模型显示出显著的改进。

    We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
    
[^27]: 超越准确性：衡量嵌入的表示能力以保留结构和上下文信息

    Beyond Accuracy: Measuring Representation Capacity of Embeddings to Preserve Structural and Contextual Information. (arXiv:2309.11294v1 [cs.LG])

    [http://arxiv.org/abs/2309.11294](http://arxiv.org/abs/2309.11294)

    本文提出了一种衡量嵌入的表示能力的方法，通过结合外部评估方法和基于t-SNE的邻域分析方法，对表示能力进行了全面评估。

    

    在各种机器学习任务中，有效地表示数据至关重要，因为它捕捉到了数据的底层结构和上下文信息。嵌入已经成为一种强大的数据表示技术，但评估其质量和保留结构和上下文信息的能力仍然是一个挑战。本文通过提出一种衡量嵌入的“表示能力”的方法来解决这个问题。这项工作的动机源于理解嵌入的优势和局限性的重要性，使研究人员和实践者能够在选择适合其特定应用的嵌入模型时做出明智的决策。通过将分类和聚类等外部评估方法与基于t-SNE的邻域分析方法（如邻域一致性和可信度）结合起来，我们对表示能力进行了全面评估。此外，使用优化技术的方法能增强表示能力。

    Effective representation of data is crucial in various machine learning tasks, as it captures the underlying structure and context of the data. Embeddings have emerged as a powerful technique for data representation, but evaluating their quality and capacity to preserve structural and contextual information remains a challenge. In this paper, we address this need by proposing a method to measure the \textit{representation capacity} of embeddings. The motivation behind this work stems from the importance of understanding the strengths and limitations of embeddings, enabling researchers and practitioners to make informed decisions in selecting appropriate embedding models for their specific applications. By combining extrinsic evaluation methods, such as classification and clustering, with t-SNE-based neighborhood analysis, such as neighborhood agreement and trustworthiness, we provide a comprehensive assessment of the representation capacity. Additionally, the use of optimization techni
    
[^28]: 《IberLEF 2023的AuTexTification概述：多领域机器生成文本的检测和归属》

    Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains. (arXiv:2309.11285v1 [cs.CL])

    [http://arxiv.org/abs/2309.11285](http://arxiv.org/abs/2309.11285)

    AuTexTification是IberLEF 2023研讨会的共享任务，旨在检测和归属多领域机器生成的文本。数据集包含160,000多条文本，涵盖了英语和西班牙语以及推文、评论、新闻、法律和操作指南等五个领域。共有114个团队参与，提交了175次运行结果。

    

    本文介绍了作为IberLEF 2023研讨会一部分的AuTexTification共享任务的概述，该研讨会是在SEPLN 2023会议框架内的伊比利亚语言评估论坛中进行的。AuTexTification包括两个子任务：在子任务1中，参与者需要确定一段文本是人工撰写还是由大型语言模型生成的。在子任务2中，参与者需要将机器生成的文本归属于六种不同的文本生成模型之一。我们的AuTexTification 2023数据集涵盖了两种语言（英语和西班牙语）和五个领域（推文、评论、新闻、法律和操作指南），共包含超过160,000条文本。共有114个团队报名参与，其中36个团队提交了175次运行结果，其中20个团队还提交了工作笔记。在这个概述中，我们介绍了AuTexTification数据集和任务，以及参与系统的结果。

    This paper presents the overview of the AuTexTification shared task as part of the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the framework of the SEPLN 2023 conference. AuTexTification consists of two subtasks: for Subtask 1, participants had to determine whether a text is human-authored or has been generated by a large language model. For Subtask 2, participants had to attribute a machine-generated text to one of six different text generation models. Our AuTexTification 2023 dataset contains more than 160.000 texts across two languages (English and Spanish) and five domains (tweets, reviews, news, legal, and how-to articles). A total of 114 teams signed up to participate, of which 36 sent 175 runs, and 20 of them sent their working notes. In this overview, we present the AuTexTification dataset and task, the submitted participating systems, and the results.
    
[^29]: 从分类到分割与可解释人工智能：关于裂纹检测和生长监测的研究

    From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring. (arXiv:2309.11267v1 [cs.CV])

    [http://arxiv.org/abs/2309.11267](http://arxiv.org/abs/2309.11267)

    本研究提出了一种将可解释人工智能应用于裂缝分割和监测的方法，通过从分类器的解释中导出分割结果，无需像素级注释。实验结果表明，该方法能有效进行裂缝分割和生长监测。

    

    监测基础设施的表面裂缝对于结构健康监测至关重要。自动视觉检测提供了一种有效的解决方案，特别是在难以到达的区域。机器学习方法已经证明了它们的有效性，但通常需要大量的注释数据集进行监督训练。一旦检测到裂缝，监测其严重程度通常需要对损害进行精确的分割。然而，对于分割而言，图像的像素级注释是一项劳动密集型的工作。为了减少这种成本，可以利用可解释人工智能（XAI）从分类器的解释中导出分割结果，仅需要弱图像级别的监督。本文提出将这种方法应用于分割和监测表面裂缝。我们评估了各种XAI方法的性能，并研究了这种方法如何促进严重程度量化和生长监测。结果表明，虽然得到的分割掩模可能较低质量，但采用这种方法仍能有效地进行裂纹分割。

    Monitoring surface cracks in infrastructure is crucial for structural health monitoring. Automatic visual inspection offers an effective solution, especially in hard-to-reach areas. Machine learning approaches have proven their effectiveness but typically require large annotated datasets for supervised training. Once a crack is detected, monitoring its severity often demands precise segmentation of the damage. However, pixel-level annotation of images for segmentation is labor-intensive. To mitigate this cost, one can leverage explainable artificial intelligence (XAI) to derive segmentations from the explanations of a classifier, requiring only weak image-level supervision. This paper proposes applying this methodology to segment and monitor surface cracks. We evaluate the performance of various XAI methods and examine how this approach facilitates severity quantification and growth monitoring. Results reveal that while the resulting segmentation masks may exhibit lower quality than th
    
[^30]: 序列到序列的西班牙预训练语言模型

    Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])

    [http://arxiv.org/abs/2309.11259](http://arxiv.org/abs/2309.11259)

    该论文介绍了一种新的序列到序列的西班牙预训练语言模型，该模型在各种序列到序列任务中表现出了竞争性能，并提供了BART、T5和BERT2BERT-style模型的西班牙版本。

    

    近年来，预训练语言模型的重大进展为许多非英语语言版本的开发铺平了道路，其中特别关注了仅编码器和仅解码器的架构。虽然西班牙语语言模型包括BERT、RoBERTa和GPT在自然语言理解和生成方面展现出了优势，但在涉及输入输出对的序列到序列任务中，缺乏编码器-解码器模型。本文通过引入实施和评估著名的仅在西班牙语语料库上进行预训练的编码器-解码器架构，开创了新的领域。具体而言，我们提出了BART、T5和BERT2BERT风格模型的西班牙语版本，并对它们在各种序列到序列任务上进行了全面评估，包括摘要、重述和生成式问答。我们的研究结果强调了所有模型的竞争性能，其中BART和T5表现出色。

    In recent years, substantial advancements in pre-trained language models have paved the way for the development of numerous non-English language versions, with a particular focus on encoder-only and decoder-only architectures. While Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited prowess in natural language understanding and generation, there remains a scarcity of encoder-decoder models designed for sequence-to-sequence tasks involving input-output pairs. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures, exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across a diverse range of sequence-to-sequence tasks, spanning summarization, rephrasing, and generative question answering. Our findings underscore the competitive performance of all models, with BART and T5 emerging a
    
[^31]: 针对空中战斗机动的分层多智能体强化学习

    Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering. (arXiv:2309.11247v1 [cs.LG])

    [http://arxiv.org/abs/2309.11247](http://arxiv.org/abs/2309.11247)

    提出了一个针对空中战斗机动的分层多智能体强化学习框架，通过将决策过程分为两个层次的抽象来处理高维状态和动作空间的挑战，通过训练低层策略实现准确的单位战斗控制。

    

    将人工智能应用于模拟空中对空格斗场景正引起越来越多的关注。迄今为止，高维状态和行动空间、情况信息的高复杂性（如不完美和筛选信息、随机性、关于任务目标的不完全知识）以及非线性飞行动力学给准确的空战决策带来了重大挑战。当涉及多个异构智能体时，这些挑战变得更加严峻。我们提出了一个针对多个异构智能体的空对空格斗的分层多智能体强化学习框架。在我们的框架中，决策过程分为两个层次的抽象，异构的低层策略控制单个单位的行动，并且在整体任务目标下，高层指挥策略发出宏观命令。对于准确的单位战斗控制，我们训练了低层策略。他们的训练是以一种学习方式组织起来的。

    The application of artificial intelligence to simulate air-to-air combat scenarios is attracting increasing attention. To date the high-dimensional state and action spaces, the high complexity of situation information (such as imperfect and filtered information, stochasticity, incomplete knowledge about mission targets) and the nonlinear flight dynamics pose significant challenges for accurate air combat decision-making. These challenges are exacerbated when multiple heterogeneous agents are involved. We propose a hierarchical multi-agent reinforcement learning framework for air-to-air combat with multiple heterogeneous agents. In our framework, the decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets. Low-level policies are trained for accurate unit combat control. Their training is organized in a learnin
    
[^32]: 基于草根操作搜索的模型边缘适应

    Grassroots Operator Search for Model Edge Adaptation. (arXiv:2309.11246v1 [cs.LG])

    [http://arxiv.org/abs/2309.11246](http://arxiv.org/abs/2309.11246)

    本论文介绍了一种基于草根操作搜索的硬件感知神经架构搜索方法，旨在通过搜索高效的操作替换来适应边缘设备上的给定模型。该方法通过使用数学指令作为基础，选择高效替换操作以降低计算复杂度，同时保持原始模型的准确性。

    

    硬件感知神经架构搜索（HW-NAS）在设计高效的深度学习架构上越来越普遍。一个高效且灵活的搜索空间对于HW-NAS的成功至关重要。当前的方法主要侧重于设计一个宏架构，并基于一组可能值来搜索架构的超参数。这种方法受深度学习工程师和标准建模方法的专业知识的影响。本文提出了一种基于草根操作搜索（GOS）的方法。我们的HW-NAS通过搜索高效的操作替换，来适应边缘设备上的给定模型。我们将每个操作表示为一组能捕捉其行为的数学指令。然后，这些数学指令被用作搜索和选择高效替换操作的基础，以保持原始模型的准确性同时降低计算复杂度。我们的方法是以草根为基础，因为它依赖数学基础。

    Hardware-aware Neural Architecture Search (HW-NAS) is increasingly being used to design efficient deep learning architectures. An efficient and flexible search space is crucial to the success of HW-NAS. Current approaches focus on designing a macro-architecture and searching for the architecture's hyperparameters based on a set of possible values. This approach is biased by the expertise of deep learning (DL) engineers and standard modeling approaches. In this paper, we present a Grassroots Operator Search (GOS) methodology. Our HW-NAS adapts a given model for edge devices by searching for efficient operator replacement. We express each operator as a set of mathematical instructions that capture its behavior. The mathematical instructions are then used as the basis for searching and selecting efficient replacement operators that maintain the accuracy of the original model while reducing computational complexity. Our approach is grassroots since it relies on the mathematical foundations
    
[^33]: 为支持持续学习系统开发而预测机器学习训练时间的研究

    Towards a Prediction of Machine Learning Training Time to Support Continuous Learning Systems Development. (arXiv:2309.11226v1 [cs.LG])

    [http://arxiv.org/abs/2309.11226](http://arxiv.org/abs/2309.11226)

    本文介绍了我们对机器学习模型训练时间进行预测的研究。我们通过对全参数时间复杂度方法的实证研究，探讨了该方法的优势和弱点，发现训练时间的预测与数据集和模型参数密切相关。

    

    在科学界中，预测机器学习模型的训练时间的问题变得非常重要。能够预先预测机器学习模型的训练时间将使得能够自动选择最佳模型，包括在能源效率和性能方面，例如在MLOps架构的背景下。本文介绍了我们正在进行的研究工作。具体来说，我们对郑等人提出的全参数时间复杂度（FPTC）方法进行了广泛的实证研究，这是我们目前所知唯一将机器学习模型的训练时间形式化为数据集和模型参数的函数的方法。我们研究了逻辑回归和随机森林分类器的公式，并突出了该方法的主要优势和弱点。最后，我们观察到，从所进行的研究中，训练时间的预测与...

    The problem of predicting the training time of machine learning (ML) models has become extremely relevant in the scientific community. Being able to predict a priori the training time of an ML model would enable the automatic selection of the best model both in terms of energy efficiency and in terms of performance in the context of, for instance, MLOps architectures. In this paper, we present the work we are conducting towards this direction. In particular, we present an extensive empirical study of the Full Parameter Time Complexity (FPTC) approach by Zheng et al., which is, to the best of our knowledge, the only approach formalizing the training time of ML models as a function of both dataset's and model's parameters. We study the formulations proposed for the Logistic Regression and Random Forest classifiers, and we highlight the main strengths and weaknesses of the approach. Finally, we observe how, from the conducted study, the prediction of training time is strictly related to t
    
[^34]: 基于模型的机器学习方法用于评估区块链应用性能的研究

    A Model-Based Machine Learning Approach for Assessing the Performance of Blockchain Applications. (arXiv:2309.11205v1 [cs.DC])

    [http://arxiv.org/abs/2309.11205](http://arxiv.org/abs/2309.11205)

    本研究提出了一种基于模型的机器学习方法，用于评估区块链应用性能。通过使用预定配置参数预测性能并探索最佳配置，我们的研究为区块链应用的开发和评估提供了可靠的建模方法。

    

    区块链技术的最新进展巩固了其作为各个领域可行替代方案的地位。然而，由于基础设施的复杂性和分布式特性，评估区块链应用的性能可能具有挑战性。因此，需要一种可靠的建模方法来提高基于区块链的应用的开发和评估。虽然已经进行了基于模拟的解决方案的研究，但很少讨论机器学习（ML）模型与评估区块链应用性能的结合。我们的研究采用了两种基于ML模型的方法。首先，我们通过训练k最近邻（kNN）和支持向量机（SVM）来预测使用预定配置参数的区块链性能。其次，我们采用了salp swarm optimization（SO）ML模型，可以探索实现所需性能水平的最佳区块链配置。我们使用粗糙集方法进行了数据分析，以评估和验证所提出方法的有效性。

    The recent advancement of Blockchain technology consolidates its status as a viable alternative for various domains. However, evaluating the performance of blockchain applications can be challenging due to the underlying infrastructure's complexity and distributed nature. Therefore, a reliable modelling approach is needed to boost Blockchain-based applications' development and evaluation. While simulation-based solutions have been researched, machine learning (ML) model-based techniques are rarely discussed in conjunction with evaluating blockchain application performance. Our novel research makes use of two ML model-based methods. Firstly, we train a $k$ nearest neighbour ($k$NN) and support vector machine (SVM) to predict blockchain performance using predetermined configuration parameters. Secondly, we employ the salp swarm optimization (SO) ML model which enables the investigation of optimal blockchain configurations for achieving the required performance level. We use rough set the
    
[^35]: Languini Kitchen: 在不同计算规模上实现语言模型研究

    The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute. (arXiv:2309.11197v1 [cs.LG])

    [http://arxiv.org/abs/2309.11197](http://arxiv.org/abs/2309.11197)

    Languini Kitchen是一个研究集体和代码库，旨在通过等效计算来进行语言模型比较，并提供新的大规模、多样化且高质量的数据集。要点：实验协议、模型比较、等效计算、大规模数据集。

    

    Languini Kitchen既是一个研究集体，又是一个代码库，旨在赋予计算资源有限的研究人员对语言模型领域做出有意义贡献的能力。我们引入了一个实验协议，使得可以基于等效计算（以加速器小时计量）来进行模型比较。模型训练的令牌数量由模型的吞吐量和选择的计算类别来定义。值得注意的是，这种方法避免了对影响总参数或浮点操作的关键超参数的限制。为了评估，我们预处理了一个现有的大规模、多样化且高质量的图书数据集，该数据集在质量、多样性和文档长度方面超过了现有的学术基准。在此基础上，我们通过不同计算水平上的实验来估计方法的经验性扩展趋势。这项工作还提供了两个基准模型：从GPT-2架构推导出的前馈模型及...

    The Languini Kitchen serves as both a research collective and codebase designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modelling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and
    
[^36]: 何时信任人工智能：神经网络认证的进展与挑战

    When to Trust AI: Advances and Challenges for Certification of Neural Networks. (arXiv:2309.11196v1 [cs.LG])

    [http://arxiv.org/abs/2309.11196](http://arxiv.org/abs/2309.11196)

    本文主要关注人工智能（AI）的认证和可解释性，综述了已经开发的用于确保AI决策安全的技术，并探讨了未来的挑战。

    

    人工智能（AI）正在快速发展，并且正准备在自主系统、医学诊断和自然语言处理等各种应用中进行部署。对于真实世界应用来说，早期采用AI技术并不是没有问题的，特别是对于神经网络来说，其可能是不稳定的，并容易受到对抗性示例的影响。从长远来看，需要开发适当的安全保证技术，以减少可避免的系统故障带来的潜在危害，并确保其可信性。本文以认证和可解释性为重点，综述了已经开发的用于确保AI决策安全的技术，并讨论了未来的挑战。

    Artificial intelligence (AI) has been advancing at a fast pace and it is now poised for deployment in a wide range of applications, such as autonomous systems, medical diagnosis and natural language processing. Early adoption of AI technology for real-world applications has not been without problems, particularly for neural networks, which may be unstable and susceptible to adversarial examples. In the longer term, appropriate safety assurance techniques need to be developed to reduce potential harm due to avoidable system failures and ensure trustworthiness. Focusing on certification and explainability, this paper provides an overview of techniques that have been developed to ensure safety of AI decisions and discusses future challenges.
    
[^37]: Robust and Heterogeneity-aware Accumulated Local Effects (RHALE): 强健且考虑异质性的累积局部效应方法

    RHALE: Robust and Heterogeneity-aware Accumulated Local Effects. (arXiv:2309.11193v1 [cs.LG])

    [http://arxiv.org/abs/2309.11193](http://arxiv.org/abs/2309.11193)

    RHALE方法是一种强健且考虑异质性的累积局部效应方法，通过量化异质性和自动确定最佳的区间划分，解决了累积局部效应方法的限制。

    

    累积局部效应（ALE）是一种广泛使用的解释性方法，用于分离特征对输出的平均影响，因为它能很好地处理相关特征的情况。然而，它有两个限制。首先，它不能量化实例级（局部）效应与平均（全局）效应之间的偏差，即异质性。其次，为了估计平均效应，它将特征域划分为用户定义的固定大小的区间，不同的区间大小会导致不一致的ALE估计。为了解决这些限制，我们提出了强健且考虑异质性的ALE（RHALE）方法。RHALE通过考虑局部效应的标准差来量化异质性，并自动确定最佳的可变大小区间划分。在本文中，我们证明了为了在每个区间内获得局部效应的无偏估计的标准差，区间划分必须遵循一组充分条件。

    Accumulated Local Effects (ALE) is a widely-used explainability method for isolating the average effect of a feature on the output, because it handles cases with correlated features well. However, it has two limitations. First, it does not quantify the deviation of instance-level (local) effects from the average (global) effect, known as heterogeneity. Second, for estimating the average effect, it partitions the feature domain into user-defined, fixed-sized bins, where different bin sizes may lead to inconsistent ALE estimations. To address these limitations, we propose Robust and Heterogeneity-aware ALE (RHALE). RHALE quantifies the heterogeneity by considering the standard deviation of the local effects and automatically determines an optimal variable-size bin-splitting. In this paper, we prove that to achieve an unbiased approximation of the standard deviation of local effects within each bin, bin splitting must follow a set of sufficient conditions. Based on these conditions, we pr
    
[^38]: 探究个性化方法在文本到音乐生成中的应用

    Investigating Personalization Methods in Text to Music Generation. (arXiv:2309.11140v1 [cs.SD])

    [http://arxiv.org/abs/2309.11140](http://arxiv.org/abs/2309.11140)

    本研究探索了在文本到音乐生成中的个性化方法，使用了预训练的文本到音频扩散器和两种已有的个性化方法。研究发现，当前的个性化方法更容易学习节奏音乐结构而非旋律。

    

    在这项工作中，我们在少样本场景下研究了文本到音乐扩散模型的个性化问题。受计算机视觉领域的最新进展的启发，我们首次探索了预训练的文本到音频扩散器与两种已建立的个性化方法的结合。我们尝试了音频特定数据增强对整个系统性能的影响，并评估了不同的训练策略。为了进行评估，我们构建了一个新颖的数据集，包括提示和音乐片段。我们考虑了基于嵌入和音乐特定的指标进行定量评估，以及用户研究进行定性评估。我们的分析表明，相似度指标与用户偏好一致，并且当前的个性化方法更容易学习到节奏音乐结构而非旋律。本研究的代码、数据集和示例资料对研究社区开放。

    In this work, we investigate the personalization of text-to-music diffusion models in a few-shot setting. Motivated by recent advances in the computer vision domain, we are the first to explore the combination of pre-trained text-to-audio diffusers with two established personalization methods. We experiment with the effect of audio-specific data augmentation on the overall system performance and assess different training strategies. For evaluation, we construct a novel dataset with prompts and music clips. We consider both embedding-based and music-specific metrics for quantitative evaluation, as well as a user study for qualitative evaluation. Our analysis shows that similarity metrics are in accordance with user preferences and that current personalization approaches tend to learn rhythmic music constructs more easily than melody. The code, dataset, and example material of this study are open to the research community.
    
[^39]: Ano-SuPs: 通过识别可疑的区块进行制造产品的多尺度异常检测

    Ano-SuPs: Multi-size anomaly detection for manufactured products by identifying suspected patches. (arXiv:2309.11120v1 [stat.ML])

    [http://arxiv.org/abs/2309.11120](http://arxiv.org/abs/2309.11120)

    Ano-SuPs是一种通过识别可疑区块来进行制造产品的多尺度异常检测的两阶段策略方法。它可以解决图像背景复杂性和异常模式的挑战，并具有较高的准确性和鲁棒性。

    

    基于图像的系统因其提供丰富的制造状态信息、低实施成本和高采集速度而受到欢迎。然而，图像背景的复杂性和各种异常模式给现有的矩阵分解方法带来了新的挑战，这些方法不足以满足建模需求。此外，异常的不确定性可能导致异常的污染问题，使得设计的模型和方法对外部干扰非常敏感。为了解决这些挑战，我们提出了一种通过识别可疑区块（Ano-SuPs）来检测异常的两阶段策略异常检测方法。具体来说，我们提出了通过两次重建输入图像来检测带有异常的区块的方法：第一步是通过去除那些可疑区块来获得一组正常区块，第二步是使用这些正常区块来优化对带有异常区块的识别。我们通过实验证明了这种方法的效果。

    Image-based systems have gained popularity owing to their capacity to provide rich manufacturing status information, low implementation costs and high acquisition rates. However, the complexity of the image background and various anomaly patterns pose new challenges to existing matrix decomposition methods, which are inadequate for modeling requirements. Moreover, the uncertainty of the anomaly can cause anomaly contamination problems, making the designed model and method highly susceptible to external disturbances. To address these challenges, we propose a two-stage strategy anomaly detection method that detects anomalies by identifying suspected patches (Ano-SuPs). Specifically, we propose to detect the patches with anomalies by reconstructing the input image twice: the first step is to obtain a set of normal patches by removing those suspected patches, and the second step is to use those normal patches to refine the identification of the patches with anomalies. To demonstrate its ef
    
[^40]: 大胆而谨慎：通过谨慎而积极的合作释放个性化联邦学习的潜力

    Bold but Cautious: Unlocking the Potential of Personalized Federated Learning through Cautiously Aggressive Collaboration. (arXiv:2309.11103v1 [cs.LG])

    [http://arxiv.org/abs/2309.11103](http://arxiv.org/abs/2309.11103)

    个性化联邦学习中的一个关键问题是决定哪些参数应该本地化或共享。传统方法中通常个性化与非IID数据敏感的所有层的参数，但这种方法过于保守。本论文提出在合作中要考虑与其他客户端的潜在好处，即使参数容易受非IID数据影响的情况下，也仍可以通过共享这些参数来获益。

    

    个性化联邦学习（PFL）通过允许每个客户端在与其他人合作时训练个性化模型，以减少非独立同分布（non-IID）数据对客户端的影响。PFL中的一个关键问题是决定客户端的哪些参数应该本地化或与其他人共享。在当前主流方法中，通常会个性化与非IID数据敏感的所有层（如分类器层）的参数。这种方法的理由是可以理解的，因为本地化易受非IID数据影响的参数可以防止合作的潜在负面影响。然而，我们认为这种方法对于合作来说过于保守。例如，对于某个客户端，即使其参数容易受非IID数据影响，与具有相似数据分布的客户端共享这些参数仍然可以带来收益。这一观察强调了不仅要考虑对非IID数据的敏感性，而且要考虑与其他客户端进行合作所带来的潜在好处的重要性。

    Personalized federated learning (PFL) reduces the impact of non-independent and identically distributed (non-IID) data among clients by allowing each client to train a personalized model when collaborating with others. A key question in PFL is to decide which parameters of a client should be localized or shared with others. In current mainstream approaches, all layers that are sensitive to non-IID data (such as classifier layers) are generally personalized. The reasoning behind this approach is understandable, as localizing parameters that are easily influenced by non-IID data can prevent the potential negative effect of collaboration. However, we believe that this approach is too conservative for collaboration. For example, for a certain client, even if its parameters are easily influenced by non-IID data, it can still benefit by sharing these parameters with clients having similar data distribution. This observation emphasizes the importance of considering not only the sensitivity to
    
[^41]: 一个新的可解释的基于神经网络的规则模型用于医疗决策

    A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making. (arXiv:2309.11101v1 [cs.LG])

    [http://arxiv.org/abs/2309.11101](http://arxiv.org/abs/2309.11101)

    本研究介绍了一个新的神经网络模型，称为TT-rules，在医疗决策中具有可解释性，并通过将神经网络转换为基于规则的模型实现了高性能。该模型支持二分类、多标签分类和回归任务，且在医疗应用中表现出色。

    

    在医疗应用中，理解机器/深度学习模型如何做出决策至关重要。在这项研究中，我们介绍了一个神经网络框架，称为“真值表规则”（TT-rules），它将基于规则的模型的全局和精确可解释性性质与深度神经网络的高性能相结合。TT-rules基于“真值表网络”（TTnet）构建，这是一族最初用于形式验证的深度神经网络。通过从训练好的TTnet模型中提取必要且充分的规则$\mathcal{R}$来产生与TTnet相同输出的规则（全局可解释性），TT-rules有效地将神经网络转换为基于规则的模型。这种基于规则的模型支持小到大的表格数据集的二分类、多标签分类和回归任务。在概述了框架后，我们评估了TT-rules在医疗应用中的性能，并将其与最先进的规则模型进行了比较。

    In healthcare applications, understanding how machine/deep learning models make decisions is crucial. In this study, we introduce a neural network framework, $\textit{Truth Table rules}$ (TT-rules), that combines the global and exact interpretability properties of rule-based models with the high performance of deep neural networks. TT-rules is built upon $\textit{Truth Table nets}$ (TTnet), a family of deep neural networks initially developed for formal verification. By extracting the necessary and sufficient rules $\mathcal{R}$ from the trained TTnet model (global interpretability) to yield the same output as the TTnet (exact interpretability), TT-rules effectively transforms the neural network into a rule-based model. This rule-based model supports binary classification, multi-label classification, and regression tasks for small to large tabular datasets. After outlining the framework, we evaluate TT-rules' performance on healthcare applications and compare it to state-of-the-art rul
    
[^42]: 延迟在强化学习中的影响

    Delays in Reinforcement Learning. (arXiv:2309.11096v1 [cs.LG])

    [http://arxiv.org/abs/2309.11096](http://arxiv.org/abs/2309.11096)

    这篇论文研究了延迟对强化学习中马尔可夫决策过程的影响，通过对代理观测延迟和执行行动延迟的研究，揭示了延迟对系统性能的重要性。

    

    延迟是大多数动态系统固有的特性。除了将过程推后一段时间外，延迟还会显著影响系统的性能。因此，研究延迟并加以考虑通常是很有价值的。由于它们是动态系统，所以延迟也会影响到马尔可夫决策过程（MDP）等顺序决策问题。MDP是强化学习（RL）的基本框架，RL的目标是通过与环境互动学习以最大化效用的人工智能代理的创建。尽管RL取得了强大的实证结果，对于延迟的显式考虑却很少见。对于MDP的延迟影响的理解还很有限。在这篇论文中，我们提出研究代理对环境状态的观测或代理执行行动的延迟。我们将不断改变对问题的观点以揭示延迟的影响。

    Delays are inherent to most dynamical systems. Besides shifting the process in time, they can significantly affect their performance. For this reason, it is usually valuable to study the delay and account for it. Because they are dynamical systems, it is of no surprise that sequential decision-making problems such as Markov decision processes (MDP) can also be affected by delays. These processes are the foundational framework of reinforcement learning (RL), a paradigm whose goal is to create artificial agents capable of learning to maximise their utility by interacting with their environment.  RL has achieved strong, sometimes astonishing, empirical results, but delays are seldom explicitly accounted for. The understanding of the impact of delay on the MDP is limited. In this dissertation, we propose to study the delay in the agent's observation of the state of the environment or in the execution of the agent's actions. We will repeatedly change our point of view on the problem to reve
    
[^43]: K-pop歌词翻译：数据集、分析与神经建模

    K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])

    [http://arxiv.org/abs/2309.11093](http://arxiv.org/abs/2309.11093)

    研究者介绍了一种新颖的K-pop歌词翻译数据集，该数据集揭示了K-pop歌词翻译的独特特征，并构建了一个神经歌词翻译模型，强调了专用数据集的重要性。

    

    歌词翻译作为一个研究了一个世纪的领域，如今吸引着计算语言学研究者的注意。我们在以往研究中发现了两个限制。首先，在歌词翻译研究中，尽管K-pop非常受欢迎，但主要关注的是西方流派和语言，没有研究集中在K-pop上。其次，歌词翻译领域缺乏可公开获得的数据集；据我们所知，目前尚无此类数据集。为了拓宽歌词翻译研究的流派和语言范围，我们引入了一种新颖的可唱歌词翻译数据集，其中约89%为K-pop歌词。该数据集通过逐行和逐节对齐了韩语和英语歌词。我们利用该数据集揭示了K-pop歌词翻译的独特特征，与其他广泛研究的流派区分开，并构建了一个神经歌词翻译模型，从而强调了专用数据集的重要性。

    Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
    
[^44]: 结合Dropout不确定性与轨迹采样的实用概率模型基于深度强化学习

    Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling. (arXiv:2309.11089v1 [eess.SY])

    [http://arxiv.org/abs/2309.11089](http://arxiv.org/abs/2309.11089)

    本论文提出了一种实用的概率模型基于深度强化学习方法，通过结合dropout不确定性和轨迹采样，稳定地预测系统不确定性，纠正神经网络的拟合误差，过滤aleatoric不确定性，从而实现了优越的控制能力。在实验评估中，该方法在多个控制任务和实际机械臂操作任务中表现出优于其他方法和模型的性能。

    

    本文针对当前建立在神经网络上的概率模型基于强化学习（MBRL）的预测稳定性、预测准确性和控制能力进行了研究。提出了一种新颖的方法，即基于dropout的概率集成和轨迹采样（DPETS），在一个框架中通过Monte-Carlo dropout和轨迹采样来稳定地预测系统不确定性。其损失函数设计用于纠正神经网络的拟合误差，以更准确地预测概率模型。其策略中的状态传播被扩展用于滤除aleatoric不确定性，从而实现了更优越的控制能力。通过在几个Mujoco基准控制任务和一个实际机械臂操作任务下的评估，DPETS在平均回报和收敛速度上优于相关MBRL方法，同时在显著的样本效率方面表现出优异性能，胜过知名的无模型基线模型。

    This paper addresses the prediction stability, prediction accuracy and control capability of the current probabilistic model-based reinforcement learning (MBRL) built on neural networks. A novel approach dropout-based probabilistic ensembles with trajectory sampling (DPETS) is proposed where the system uncertainty is stably predicted by combining the Monte-Carlo dropout and trajectory sampling in one framework. Its loss function is designed to correct the fitting error of neural networks for more accurate prediction of probabilistic models. The state propagation in its policy is extended to filter the aleatoric uncertainty for superior control capability. Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency. The ope
    
[^45]: 弱监督在标注高效可视化错误检测中的应用

    Weak Supervision for Label Efficient Visual Bug Detection. (arXiv:2309.11077v1 [cs.CV])

    [http://arxiv.org/abs/2309.11077](http://arxiv.org/abs/2309.11077)

    我们提出了一种利用未标注数据和领域特定增强技术的弱监督方法，在视觉错误检测中实现了数据集扩大和自主/交互弱监督，展示了在广阔的游戏世界中的有效性。

    

    随着视频游戏进化为广阔、细致的世界，视觉质量变得至关重要，但也日益具有挑战性。传统的测试方法由于资源有限，难以应对大量潜在的错误。机器学习提供了可扩展的解决方案，然而，对大型标注数据集的严重依赖仍然是一个制约因素。针对这一挑战，我们提出了一种新的方法，利用未标注的游戏玩法和领域特定的增强技术生成用于预训练或多任务设置的数据集和自监督目标，用于后续的视觉错误检测。我们的方法使用弱监督技术扩大量表和促进自主和交互式弱监督，包括基于无监督聚类和/或基于文本和几何提示的交互式方法。我们在广阔的Giantmap游戏世界中展示了对一人称玩家剪辑/碰撞错误（FPPC）的检测，证明了我们的方法的有效性

    As video games evolve into expansive, detailed worlds, visual quality becomes essential, yet increasingly challenging. Traditional testing methods, limited by resources, face difficulties in addressing the plethora of potential bugs. Machine learning offers scalable solutions; however, heavy reliance on large labeled datasets remains a constraint. Addressing this challenge, we propose a novel method, utilizing unlabeled gameplay and domain-specific augmentations to generate datasets & self-supervised objectives used during pre-training or multi-task settings for downstream visual bug detection. Our methodology uses weak-supervision to scale datasets for the crafted objectives and facilitates both autonomous and interactive weak-supervision, incorporating unsupervised clustering and/or an interactive approach based on text and geometric prompts. We demonstrate on first-person player clipping/collision bugs (FPPC) within the expansive Giantmap game world, that our approach is very effect
    
[^46]: GPSINDy: 数据驱动的动力学系统方程发现

    GPSINDy: Data-Driven Discovery of Equations of Motion. (arXiv:2309.11076v1 [cs.LG])

    [http://arxiv.org/abs/2309.11076](http://arxiv.org/abs/2309.11076)

    GPSINDy是一种数据驱动的方法，通过将高斯过程回归与SINDy相结合，能够从噪声数据中发现非线性动力学系统模型，并在实验证明了其在系统动态和预测未来轨迹方面的改进性能。

    

    本论文考虑从有噪声数据中发现动力学系统模型的问题。已知噪声存在对符号回归算法来说是一个重要问题。我们将高斯过程回归（一种非参数学习方法）与SINDy（一种参数学习方法）相结合，从数据中识别非线性动力学系统。我们的方法具有简单性和与SINDy相比在有噪声数据上表现出更好的鲁棒性的优点。我们在Lotka-Volterra模型和仿真中的单轮车动态模型上以及在使用硬件数据的NVIDIA JetRacer系统上展示了我们的方法。我们展示了相较于SINDy，我们的方法在发现系统动态和预测未来轨迹方面的改进性能。

    In this paper, we consider the problem of discovering dynamical system models from noisy data. The presence of noise is known to be a significant problem for symbolic regression algorithms. We combine Gaussian process regression, a nonparametric learning method, with SINDy, a parametric learning approach, to identify nonlinear dynamical systems from data. The key advantages of our proposed approach are its simplicity coupled with the fact that it demonstrates improved robustness properties with noisy data over SINDy. We demonstrate our proposed approach on a Lotka-Volterra model and a unicycle dynamic model in simulation and on an NVIDIA JetRacer system using hardware data. We demonstrate improved performance over SINDy for discovering the system dynamics and predicting future trajectories.
    
[^47]: InkStream: 通过增量更新在流式图上进行实时的图神经网络推理

    InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update. (arXiv:2309.11071v1 [cs.LG])

    [http://arxiv.org/abs/2309.11071](http://arxiv.org/abs/2309.11071)

    InkStream是一种在流式图上进行实时推理的方法，通过增量更新节点嵌入来解决传统图神经网络在流式图上的挑战。

    

    传统的图神经网络推理方法适用于静态图，而对于随时间演变的流式图则不合适。流式图的动态性需要进行持续的更新，对GPU加速提出了独特的挑战。我们基于两个关键观点来解决这些挑战：（1）在k-hop邻域内，当模型使用最小或最大聚合函数时，只有一小部分节点受到修改边的影响；（2）当模型权重保持静态而图结构发生变化时，节点嵌入可以通过仅计算邻域的受影响部分来逐步演化。基于这些观点，我们提出了一种新颖的方法InkStream，旨在实现实时推理，最小化内存访问和计算，并确保与传统方法相同的输出。InkStream的操作原则是仅在必要时传播和获取数据。它使用基于事件的系统来控制。

    Classic Graph Neural Network (GNN) inference approaches, designed for static graphs, are ill-suited for streaming graphs that evolve with time. The dynamism intrinsic to streaming graphs necessitates constant updates, posing unique challenges to acceleration on GPU. We address these challenges based on two key insights: (1) Inside the $k$-hop neighborhood, a significant fraction of the nodes is not impacted by the modified edges when the model uses min or max as aggregation function; (2) When the model weights remain static while the graph structure changes, node embeddings can incrementally evolve over time by computing only the impacted part of the neighborhood. With these insights, we propose a novel method, InkStream, designed for real-time inference with minimal memory access and computation, while ensuring an identical output to conventional methods. InkStream operates on the principle of propagating and fetching data only when necessary. It uses an event-based system to control 
    
[^48]: 天然电网运营计划中的极端情景选择

    Extreme Scenario Selection in Day-Ahead Power Grid Operational Planning. (arXiv:2309.11067v1 [stat.ML])

    [http://arxiv.org/abs/2309.11067](http://arxiv.org/abs/2309.11067)

    本文提出并分析了一种在天然电网规划中选择极端情景的方法，通过统计功能深度度量来筛选出最重要的情景以减轻运营风险。研究结果表明该方法在真实的Texas-7k电网上具有良好的效果。

    

    我们提出并分析了在提前一天的电网规划中应用统计功能深度度量来选择极端情景的方法。我们的主要目的是筛选针对实际负载和可再生能源生成的概率情景，以识别对运营风险缓解最重要的情景。为了处理资产类别和日内时段的场景高维度情况，我们使用功能深度度量来子选择最有可能对电网运营风险最高的异常情景。我们研究了一系列功能深度度量以及一系列运营风险，包括负荷削减、运营成本、备用不足和可变可再生能源削减。通过对现实的Texas-7k电网进行案例研究，证明了所提出的筛选方法的有效性。

    We propose and analyze the application of statistical functional depth metrics for the selection of extreme scenarios in day-ahead grid planning. Our primary motivation is screening of probabilistic scenarios for realized load and renewable generation, in order to identify scenarios most relevant for operational risk mitigation. To handle the high-dimensionality of the scenarios across asset classes and intra-day periods, we employ functional measures of depth to sub-select outlying scenarios that are most likely to be the riskiest for the grid operation. We investigate a range of functional depth measures, as well as a range of operational risks, including load shedding, operational costs, reserves shortfall and variable renewable energy curtailment. The effectiveness of the proposed screening approach is demonstrated through a case study on the realistic Texas-7k grid.
    
[^49]: 数学问题解决中的思路链设计

    Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v1 [cs.CL])

    [http://arxiv.org/abs/2309.11054](http://arxiv.org/abs/2309.11054)

    本论文研究了数学问题解决中思路链的设计方法，对比了自然语言思路链和程序思路链的效果，并发现程序思路链通常在数学问题解决中更加有效，特别是自我描述程序具有更大多样性且性能更高。此外，研究还发现Python是程序思路链的较好选择。实验结果为未来思路链设计提供了宝贵指导。

    

    思路链在数学问题解决中扮演着至关重要的角色。我们对设计思路链的方法进行了全面的考察，比较了传统自然语言思路链和各种程序思路链，包括自我描述程序、注释描述程序和非描述程序。此外，我们还研究了编程语言对程序思路链的影响，比较了Python和Wolfram语言。通过对GSM8K、MATHQA和SVAMP进行广泛实验，我们发现程序思路链在数学问题解决中通常具有更好的效果。值得注意的是，具有30B参数的最佳组合明显超过了GPT-3.5-turbo。结果表明，自我描述程序提供了更大的多样性，因此通常可以实现更高的性能。我们还发现，Python是程序思路链的更好选择比Wolfram语言。实验结果为未来考虑因素提供了宝贵的指导。

    Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into acco
    
[^50]: Fake News BR: 一种用于巴西葡萄牙语的假新闻检测平台

    Fake News BR: A Fake News Detection Platform for Brazilian Portuguese. (arXiv:2309.11052v1 [cs.CL])

    [http://arxiv.org/abs/2309.11052](http://arxiv.org/abs/2309.11052)

    本研究提出了一个用于巴西葡萄牙语的假新闻检测平台，采用机器学习和自然语言处理技术，能够高效准确地识别假新闻，同时提供实时分析和验证新闻文章真实性的用户友好平台。

    

    由于假新闻传播误导公众舆论的潜力，其传播已成为近期关注的一个重要问题。本文对巴西葡萄牙语中的假新闻检测进行了全面的研究，重点关注新闻类型。我们提出了一种基于机器学习的方法，利用自然语言处理技术，包括TF-IDF和Word2Vec，从文本数据中提取特征。我们评估了各种分类算法的性能，如逻辑回归、支持向量机、随机森林、AdaBoost和LightGBM，使用包含真实和假新闻文章的数据集。所提出的方法在准确率和F1得分上都取得了高水平，证明了其识别假新闻的有效性。此外，我们开发了一个用户友好的网站平台FAKENEWSBR.COM，以便验证新闻文章的真实性。我们的平台提供实时分析，允许用户检查新闻文章的真实性。

    The proliferation of fake news has become a significant concern in recent times due to its potential to spread misinformation and manipulate public opinion. In this paper, we present a comprehensive study on the detection of fake news in Brazilian Portuguese, focusing on journalistic-type news. We propose a machine learning-based approach that leverages natural language processing techniques, including TF-IDF and Word2Vec, to extract features from textual data. We evaluate the performance of various classification algorithms, such as logistic regression, support vector machine, random forest, AdaBoost, and LightGBM, on a dataset containing both true and fake news articles. The proposed approach achieves a high level of accuracy and F1-Score, demonstrating its effectiveness in identifying fake news. Additionally, we develop a user-friendly web platform, FAKENEWSBR.COM, to facilitate the verification of news articles' veracity. Our platform provides real-time analysis, allowing users to 
    
[^51]: 在协作的内存计算网络中，通过频域压缩来控制边缘的模拟数据洪水

    Containing Analog Data Deluge at Edge through Frequency-Domain Compression in Collaborative Compute-in-Memory Networks. (arXiv:2309.11048v1 [cs.LG])

    [http://arxiv.org/abs/2309.11048](http://arxiv.org/abs/2309.11048)

    本文提出了一种在边缘进行高效深度学习推理的解决方案，通过频域压缩和内存相互融合机制，实现了更高的面积效率和能量效率。

    

    边缘计算是处理来自传感器和物联网设备的高维、多光谱模拟数据的一种有希望的解决方案，用于自主无人机等应用。然而，边缘设备的有限存储和计算资源使得在边缘进行复杂的预测建模成为一项挑战。内存计算已经成为一种主要范式，用于在边缘的基于深度学习的推理中最小化能量消耗。然而，集成存储和处理在存储单元和/或存储外设上变得复杂，从根本上在面积效率和能量效率之间进行权衡。本文提出了一种改进深度学习推理任务中面积效率的新方法。所提出的方法采用两种关键策略。首先，频域学习方法使用二值化的Walsh-Hadamard变换，减少了DNN的必要参数（在MobileNetV2中减少了87%），并实现了在SRAM上进行计算，从而更好地利用推理过程中的并行性。其次，一种内存相互融合机制。

    Edge computing is a promising solution for handling high-dimensional, multispectral analog data from sensors and IoT devices for applications such as autonomous drones. However, edge devices' limited storage and computing resources make it challenging to perform complex predictive modeling at the edge. Compute-in-memory (CiM) has emerged as a principal paradigm to minimize energy for deep learning-based inference at the edge. Nevertheless, integrating storage and processing complicates memory cells and/or memory peripherals, essentially trading off area efficiency for energy efficiency. This paper proposes a novel solution to improve area efficiency in deep learning inference tasks. The proposed method employs two key strategies. Firstly, a Frequency domain learning approach uses binarized Walsh-Hadamard Transforms, reducing the necessary parameters for DNN (by 87% in MobileNetV2) and enabling compute-in-SRAM, which better utilizes parallelism during inference. Secondly, a memory-immer
    
[^52]: Clustered FedStack：基于贝叶斯信息准则的中间全局模型。

    Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion. (arXiv:2309.11044v1 [cs.LG])

    [http://arxiv.org/abs/2309.11044](http://arxiv.org/abs/2309.11044)

    提出了一种基于贝叶斯信息准则的中间全局模型Clustered FedStack。通过将本地客户端的模型预测和输出层权重发送到服务器，构建一个强大的全局模型，并使用聚类机制对本地客户进行聚类。

    

    联邦学习（FL）是目前人工智能领域最受欢迎的技术之一，因其协作学习和保护客户隐私的能力而受到青睐。然而，它面临非独立和非独立分布（非IID）以及本地客户之间标签不平衡等挑战。为了解决这些限制，研究团队探索了各种方法，如使用本地模型参数、联邦生成对抗学习和联邦表示学习。在我们的研究中，我们提出了一种基于已发表的Stacked Federated Learning（FedStack）框架的新颖Clustered FedStack框架。本地客户端将其模型预测和输出层权重发送到服务器，然后构建一个强大的全局模型。这个全局模型使用聚类机制基于其输出层权重对本地客户进行聚类。我们采用了三种聚类机制，分别是K-Means、Agglomerative、DBSCAN。

    Federated Learning (FL) is currently one of the most popular technologies in the field of Artificial Intelligence (AI) due to its collaborative learning and ability to preserve client privacy. However, it faces challenges such as non-identically and non-independently distributed (non-IID) and data with imbalanced labels among local clients. To address these limitations, the research community has explored various approaches such as using local model parameters, federated generative adversarial learning, and federated representation learning. In our study, we propose a novel Clustered FedStack framework based on the previously published Stacked Federated Learning (FedStack) framework. The local clients send their model predictions and output layer weights to a server, which then builds a robust global model. This global model clusters the local clients based on their output layer weights using a clustering mechanism. We adopt three clustering mechanisms, namely K-Means, Agglomerative, a
    
[^53]: 智能交通系统中的联邦学习：最近的应用和开放问题

    Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems. (arXiv:2309.11039v1 [cs.LG])

    [http://arxiv.org/abs/2309.11039](http://arxiv.org/abs/2309.11039)

    联邦学习在智能交通系统中提供了一种具有隐私保护和易扩展性的新范式，为解决动态车辆环境中的各种应用挑战提供了解决方案。

    

    智能交通系统（ITS）受到通信技术、传感器技术和物联网（IoT）的迅猛发展的推动。然而，由于车辆网络的动态特性，及时准确地决策车辆行为非常具有挑战性。此外，在移动无线通信的情况下，车辆信息的隐私和安全面临着持续的风险。在这种背景下，急需一种新的范式来应对动态车辆环境中的各种应用。作为一种分布式机器学习技术，联邦学习（FL）因其出色的隐私保护性能和易扩展性而受到广泛关注。我们对ITS中FL最新发展进行了全面调研。具体而言，我们首先研究ITS中普遍存在的挑战，并从各个角度阐明应用FL的动机。随后，我们回顾了现有的FL部署

    Intelligent transportation systems (ITSs) have been fueled by the rapid development of communication technologies, sensor technologies, and the Internet of Things (IoT). Nonetheless, due to the dynamic characteristics of the vehicle networks, it is rather challenging to make timely and accurate decisions of vehicle behaviors. Moreover, in the presence of mobile wireless communications, the privacy and security of vehicle information are at constant risk. In this context, a new paradigm is urgently needed for various applications in dynamic vehicle environments. As a distributed machine learning technology, federated learning (FL) has received extensive attention due to its outstanding privacy protection properties and easy scalability. We conduct a comprehensive survey of the latest developments in FL for ITS. Specifically, we initially research the prevalent challenges in ITS and elucidate the motivations for applying FL from various perspectives. Subsequently, we review existing depl
    
[^54]: 基于区域收缩的分类优化算法的加速

    A Region-Shrinking-Based Acceleration for Classification-Based Derivative-Free Optimization. (arXiv:2309.11036v1 [cs.LG])

    [http://arxiv.org/abs/2309.11036](http://arxiv.org/abs/2309.11036)

    本文研究了基于分类的无导数优化算法的加速方法，通过引入区域收缩步骤，提出了一种名为“RACE-CARS”的算法，并证明了区域收缩的加速性质。实验结果验证了"RACE-CARS"的高效性，并提出了经验的超参数调优策略。

    

    无导数优化算法在科学和工程设计优化问题中起着重要作用，特别是当无法获取导数信息时。本文研究了基于分类的无导数优化算法的框架。通过引入一种称为假设-目标破裂率的概念，我们重新审视了该类型算法的计算复杂性上界。受重新审视的上界的启发，我们提出了一种名为“RACE-CARS”的算法，与“SRACOS”（Hu et al., 2017）相比，该算法添加了一个随机区域收缩步骤。我们进一步证明了区域收缩的加速性质。针对合成函数以及语言模型服务的黑盒调优的实验在经验证明了“RACE-CARS”的效率。我们还进行了关于引入超参数的消融实验，揭示了“RACE-CARS”的工作机制，并提出了经验的超参数调优策略。

    Derivative-free optimization algorithms play an important role in scientific and engineering design optimization problems, especially when derivative information is not accessible. In this paper, we study the framework of classification-based derivative-free optimization algorithms. By introducing a concept called hypothesis-target shattering rate, we revisit the computational complexity upper bound of this type of algorithms. Inspired by the revisited upper bound, we propose an algorithm named "RACE-CARS", which adds a random region-shrinking step compared with "SRACOS" (Hu et al., 2017).. We further establish a theorem showing the acceleration of region-shrinking. Experiments on the synthetic functions as well as black-box tuning for language-model-as-a-service demonstrate empirically the efficiency of "RACE-CARS". An ablation experiment on the introduced hyperparameters is also conducted, revealing the mechanism of "RACE-CARS" and putting forward an empirical hyperparameter-tuning g
    
[^55]: 神经表征的拓扑和几何结构

    The Topology and Geometry of Neural Representations. (arXiv:2309.11028v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.11028](http://arxiv.org/abs/2309.11028)

    本文探索了从几何结构到拓扑结构的抽象步骤，并提出了一种拓扑表征相似分析方法（tRSA），通过一系列地理拓扑摘要统计量对大脑表征进行表征。

    

    神经科学所关心的一个核心问题是如何表征感知和认知内容的大脑表征。一个理想的表征应该能够区分不同的功能区域，并且对噪声和个体大脑的特异性具有稳健性，而不会与计算差异相对应。以前的研究通过表征几何结构来表征大脑表征，几何结构由表征不相似矩阵（RDM）定义，RDM是一个摘要统计量，摘要了个体神经元（或响应通道）的作用，并表征了刺激的可辨别性。在这里，我们进一步探索了从几何结构到大脑表征拓扑的抽象步骤。我们提出了拓扑表征相似分析（tRSA），它是表征相似分析（RSA）的一种扩展，使用了一系列地理拓扑摘要统计量，将RDM进行泛化以表征拓扑结构并减弱几何结构的作用。

    A central question for neuroscience is how to characterize brain representations of perceptual and cognitive content. An ideal characterization should distinguish different functional regions with robustness to noise and idiosyncrasies of individual brains that do not correspond to computational differences. Previous studies have characterized brain representations by their representational geometry, which is defined by the representational dissimilarity matrix (RDM), a summary statistic that abstracts from the roles of individual neurons (or responses channels) and characterizes the discriminability of stimuli. Here we explore a further step of abstraction: from the geometry to the topology of brain representations. We propose topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics that generalizes the RDM to characterize the topology while de-emphasizing the geometry. 
    
[^56]: 机器学习模型中数据更新的信息泄露

    Information Leakage from Data Updates in Machine Learning Models. (arXiv:2309.11022v1 [cs.LG])

    [http://arxiv.org/abs/2309.11022](http://arxiv.org/abs/2309.11022)

    本文研究了在机器学习模型中进行数据更新时可能发生的信息泄露问题，并提出了基于预测置信度差异的攻击方法。实验证实了两个模型快照相对于仅访问更新数据会导致更高的信息泄露。

    

    本文考虑了在更新数据集时重新训练机器学习模型的情景，以便将最新信息纳入模型或反映分布变化。我们研究了是否可以推断出关于这些更新的信息（例如，记录属性值的更改）。在这里，攻击者可以访问数据集变化前后的机器学习模型的快照。与现有文献相反，我们假设一个或多个训练数据点的属性发生改变，而不是整个数据记录被删除或添加。我们提出了基于原始模型和更新模型预测置信度的攻击方法。我们在两个公共数据集上评估了我们的攻击方法，采用了多层感知器和逻辑回归模型。我们验证了模型的两个快照与仅访问更新数据相比会导致更高的信息泄露。

    In this paper we consider the setting where machine learning models are retrained on updated datasets in order to incorporate the most up-to-date information or reflect distribution shifts. We investigate whether one can infer information about these updates in the training data (e.g., changes to attribute values of records). Here, the adversary has access to snapshots of the machine learning model before and after the change in the dataset occurs. Contrary to the existing literature, we assume that an attribute of a single or multiple training data points are changed rather than entire data records are removed or added. We propose attacks based on the difference in the prediction confidence of the original model and the updated model. We evaluate our attack methods on two public datasets along with multi-layer perceptron and logistic regression models. We validate that two snapshots of the model can result in higher information leakage in comparison to having access to only the update
    
[^57]: 多模态不确定性回归与推理的共形化方法

    Conformalized Multimodal Uncertainty Regression and Reasoning. (arXiv:2309.11018v1 [cs.LG])

    [http://arxiv.org/abs/2309.11018](http://arxiv.org/abs/2309.11018)

    本文提出了一种共形化的轻量级不确定性估计器，通过将共形预测与深度学习回归器结合，在多模态情况下预测不确定性边界，并基于光流推理提高预测准确性。

    

    本文介绍了一种轻量级的不确定性估计器，通过将共形预测与深度学习回归器相结合，能够预测多模态（不重叠）的不确定性边界。我们特别讨论了在视觉里程计（VO）中的应用，其中环境特征（例如飞行域对称性）和传感器测量在模糊和遮挡下会导致多模态的不确定性。我们的仿真结果表明，在我们的框架中，不确定性估计可以根据具有挑战性的操作条件（如噪声、有限的训练数据和预测模型参数的有限大小）进行样本适应。我们还开发了一个推理框架，利用这些稳健的不确定性估计，并结合基于光流的推理来提高预测准确性。因此，通过适当考虑数据驱动学习的预测不确定性，并通过基于规则的推理来闭环估计，我们的方法始终能够提供稳定的结果。

    This paper introduces a lightweight uncertainty estimator capable of predicting multimodal (disjoint) uncertainty bounds by integrating conformal prediction with a deep-learning regressor. We specifically discuss its application for visual odometry (VO), where environmental features such as flying domain symmetries and sensor measurements under ambiguities and occlusion can result in multimodal uncertainties. Our simulation results show that uncertainty estimates in our framework adapt sample-wise against challenging operating conditions such as pronounced noise, limited training data, and limited parametric size of the prediction model. We also develop a reasoning framework that leverages these robust uncertainty estimates and incorporates optical flow-based reasoning to improve prediction prediction accuracy. Thus, by appropriately accounting for predictive uncertainties of data-driven learning and closing their estimation loop via rule-based reasoning, our methodology consistently s
    
[^58]: 通过3D-U-SAM网络进行少样本CBCT图像的牙齿分割

    3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images. (arXiv:2309.11015v1 [eess.IV])

    [http://arxiv.org/abs/2309.11015](http://arxiv.org/abs/2309.11015)

    本文提出了一种新颖的3D-U-SAM网络，用于少样本CBCT图像的牙齿分割。通过使用预训练的SAM和卷积逼近方法，以及跳跃连接融合特征，本方法在解决小样本问题上表现出很好的效果。

    

    牙齿位置的准确表示在治疗中非常重要。3D牙齿图像分割是一种广泛使用的方法，然而标注的3D牙齿数据集是稀缺的资源，这导致了这个任务在许多情况下面临小样本问题。为了解决这个问题，本文采用了预训练的SAM，并提出了一种新颖的3D-U-SAM网络用于3D牙齿图像分割。具体而言，为了解决在3D数据集上使用2D预训练权重的问题，我们采用了卷积逼近方法；为了保留更多细节，我们设计了跳跃连接，以参考U-Net在所有层级上融合特征。通过消融实验、对比实验和样本大小实验证明了所提出方法的有效性。

    Accurate representation of tooth position is extremely important in treatment. 3D dental image segmentation is a widely used method, however labelled 3D dental datasets are a scarce resource, leading to the problem of small samples that this task faces in many cases. To this end, we address this problem with a pretrained SAM and propose a novel 3D-U-SAM network for 3D dental image segmentation. Specifically, in order to solve the problem of using 2D pre-trained weights on 3D datasets, we adopted a convolution approximation method; in order to retain more details, we designed skip connections to fuse features at all levels with reference to U-Net. The effectiveness of the proposed method is demonstrated in ablation experiments, comparison experiments, and sample size experiments.
    
[^59]: ModelGiF: 模型功能距离的梯度场

    ModelGiF: Gradient Fields for Model Functional Distance. (arXiv:2309.11013v1 [cs.LG])

    [http://arxiv.org/abs/2309.11013](http://arxiv.org/abs/2309.11013)

    本文介绍了一种用于测量模型功能距离的方法，称为ModelGiF。通过在异构的预训练模型中提取同质的表示，ModelGiF可通过模型之间的相似性来衡量它们之间的距离。实验证实了该方法在任务相关性估计、知识产权保护和模型遗忘验证等方面的有效性。

    

    过去十年见证了深度学习的成功和公开发布的训练模型的激增，这就需要对各种目的的模型功能距离进行量化。然而，由于内部工作的不透明性和体系结构或任务的异质性，量化模型功能距离始终具有挑战性。受物理学中“场”概念的启发，本文引入了模型梯度场（简称ModelGiF），从异构的预训练模型中提取同质的表示。我们的主要假设是，每个预训练深度模型在输入空间上唯一确定一个ModelGiF。因此，模型之间的距离可以通过它们的ModelGiF的相似性来衡量。我们使用一系列实验验证了所提出的ModelGiF的有效性，包括任务相关性估计、知识产权保护和模型遗忘验证。实验结果证明了其有效性。

    The last decade has witnessed the success of deep learning and the surge of publicly released trained models, which necessitates the quantification of the model functional distance for various purposes. However, quantifying the model functional distance is always challenging due to the opacity in inner workings and the heterogeneity in architectures or tasks. Inspired by the concept of "field" in physics, in this work we introduce Model Gradient Field (abbr. ModelGiF) to extract homogeneous representations from the heterogeneous pre-trained models. Our main assumption underlying ModelGiF is that each pre-trained deep model uniquely determines a ModelGiF over the input space. The distance between models can thus be measured by the similarity between their ModelGiFs. We validate the effectiveness of the proposed ModelGiF with a suite of testbeds, including task relatedness estimation, intellectual property protection, and model unlearning verification. Experimental results demonstrate th
    
[^60]: Simplex！通过解构度量来提高认证的鲁棒性

    It's Simplex! Disaggregating Measures to Improve Certified Robustness. (arXiv:2309.11005v1 [cs.LG])

    [http://arxiv.org/abs/2309.11005](http://arxiv.org/abs/2309.11005)

    本文提出了通过解构度量来提高认证的鲁棒性的方法，该方法可以更准确地评估认证机制的性能，对于数据集无关和数据集相关的度量都有效。经验证实，这种方法能够将可实现的认证半径提高一倍以上。

    

    通过给模型的预测结果提供攻击大小上不变的保证，认证的鲁棒性绕过了对抗攻击中防御的脆弱性。尽管这些认证具有价值，但我们评估它们性能的技术没有对其优点和缺点进行适当的考虑，因为它们的分析在考虑聚合度量而不是单个样本的性能方面进行了规避。通过考虑认证模型的潜在输出空间，本文提出了两种不同的方法来改进认证机制的分析，这两种方法允许进行数据集无关和数据集相关的认证性能度量。采纳这种视角揭示了新的认证方法，相对于当前的最新技术，这些方法有潜力将可实现的认证半径提高一倍以上。经验证实，我们的新方法可以认证

    Certified robustness circumvents the fragility of defences against adversarial attacks, by endowing model predictions with guarantees of class invariance for attacks up to a calculated size. While there is value in these certifications, the techniques through which we assess their performance do not present a proper accounting of their strengths and weaknesses, as their analysis has eschewed consideration of performance over individual samples in favour of aggregated measures. By considering the potential output space of certified models, this work presents two distinct approaches to improve the analysis of certification mechanisms, that allow for both dataset-independent and dataset-dependent measures of certification performance. Embracing such a perspective uncovers new certification approaches, which have the potential to more than double the achievable radius of certification, relative to current state-of-the-art. Empirical evaluation verifies that our new approach can certify $9\
    
[^61]: 基于多智能体深度强化学习的AI驱动患者监测

    AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])

    [http://arxiv.org/abs/2309.10980](http://arxiv.org/abs/2309.10980)

    本研究提出了一种基于多智能体深度强化学习的AI驱动患者监测框架，通过部署多个学习智能体，针对不同的生理特征进行监测，并根据紧急程度预警医疗紧急团队。

    

    有效的患者监测对及时干预和改善医疗结果至关重要。传统的监测系统往往难以处理复杂、动态的环境和波动的生命体征，导致延迟发现危急情况。为了应对这一挑战，我们提出了一种新颖的基于多智能体深度强化学习（DRL）的AI驱动患者监测框架。我们的方法部署了多个学习智能体，每个智能体专门负责监测特定的生理特征，如心率、呼吸和体温。这些智能体与通用的医疗监测环境进行交互，学习患者的行为模式，并根据估计的紧急程度做出通知相应医疗紧急团队（MET）的决策。在本研究中，我们使用来自两个数据集（PPG-DaLiA和WESAD）的真实生理和运动数据评估了提出的多智能体DRL框架的性能。

    Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
    
[^62]: 走向以数据为中心的图机器学习：回顾与展望

    Towards Data-centric Graph Machine Learning: Review and Outlook. (arXiv:2309.10979v1 [cs.LG])

    [http://arxiv.org/abs/2309.10979](http://arxiv.org/abs/2309.10979)

    本文回顾并展望了以数据为中心的图机器学习在图数据领域的当前努力，提出了一个系统框架：数据中心化图机器学习（DC-GML），并介绍了解决图数据可用性、质量和构建图MLOps系统的关键问题。

    

    近年来，以数据为中心的人工智能（AI）以其对数据的收集、管理和利用为主要焦点，引起了越来越多的关注。在本文中，我们进行了深入全面的回顾，提供了对与图数据相关的以数据为中心的AI当前工作的前瞻性展望，这是表示和捕捉庞大且多样的现实实体之间复杂依赖关系的基本数据结构。我们引入了一个系统框架，数据中心化图机器学习（DC-GML），涵盖了图数据生命周期的所有阶段，包括图数据收集、探索、改进、利用和维护。通过详细的分类法回答了三个关键的图中心化问题：（1）如何提高图数据的可用性和质量；（2）如何从可用性有限和质量较低的图数据中学习；（3）如何从图数据构建图MLOps系统。

    Data-centric AI, with its primary focus on the collection, management, and utilization of data to drive AI models and applications, has attracted increasing attention in recent years. In this article, we conduct an in-depth and comprehensive review, offering a forward-looking outlook on the current efforts in data-centric AI pertaining to graph data-the fundamental data structure for representing and capturing intricate dependencies among massive and diverse real-life entities. We introduce a systematic framework, Data-centric Graph Machine Learning (DC-GML), that encompasses all stages of the graph data lifecycle, including graph data collection, exploration, improvement, exploitation, and maintenance. A thorough taxonomy of each stage is presented to answer three critical graph-centric questions: (1) how to enhance graph data availability and quality; (2) how to learn from graph data with limited-availability and low-quality; (3) how to build graph MLOps systems from the graph data-c
    
[^63]: PAGER: 一种用于深度回归模型故障分析的框架

    PAGER: A Framework for Failure Analysis of Deep Regression Models. (arXiv:2309.10977v1 [cs.LG])

    [http://arxiv.org/abs/2309.10977](http://arxiv.org/abs/2309.10977)

    PAGER提出了一种用于深度回归模型故障分析的框架，通过综合利用认识不确定性和不一致分数，对样本进行分组并提供全面的分析。

    

    安全部署AI模型需要主动检测潜在的预测故障，以防止昂贵的错误。尽管分类问题的故障检测已经引起了广泛关注，但在回归任务中表征故障模式更加复杂且较少研究。现有方法依赖于认识不确定性或与训练分布的特征不一致来表征模型风险。然而，我们表明，仅靠不确定性无法准确表征故障，这是由于各种误差源的存在。在本文中，我们提出了PAGER（回归器的原则性泛化错误分析），这是一个系统检测和表征深度回归模型故障的框架。基于最近提出的深度模型锚定思想，PAGER将认识不确定性和新颖的、互补的不一致分数统一起来，将样本组织成不同的风险区域，从而提供全面的分析。

    Safe deployment of AI models requires proactive detection of potential prediction failures to prevent costly errors. While failure detection in classification problems has received significant attention, characterizing failure modes in regression tasks is more complicated and less explored. Existing approaches rely on epistemic uncertainties or feature inconsistency with the training distribution to characterize model risk. However, we show that uncertainties are necessary but insufficient to accurately characterize failure, owing to the various sources of error. In this paper, we propose PAGER (Principled Analysis of Generalization Errors in Regressors), a framework to systematically detect and characterize failures in deep regression models. Built upon the recently proposed idea of anchoring in deep models, PAGER unifies both epistemic uncertainties and novel, complementary non-conformity scores to organize samples into different risk regimes, thereby providing a comprehensive analys
    
[^64]: 准确且可扩展的图神经网络的认知不确定性估计

    Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks. (arXiv:2309.10976v1 [cs.LG])

    [http://arxiv.org/abs/2309.10976](http://arxiv.org/abs/2309.10976)

    本文针对图神经网络（GNNs）提出了一种准确且可扩展的认知不确定性估计方法，该方法通过扩展随机居中框架，并支持结构化数据和部分随机性，能够获得更好的校准的信心指标（CI）性能。

    

    在分布转移下安全部署图神经网络（GNNs）需要模型提供准确的信心指标（CI）。然而，尽管在计算机视觉领域已经知道在分布转移下CI的质量会降低，但对于GNNs来说这个行为仍未得到充分研究。因此，我们通过对受控的结构和特征分布转移下的CI校准进行案例研究，证明了表达能力或模型大小的增加不总是能提高CI的性能。因此，我们提倡使用认知不确定性量化（UQ）方法来调节CI。为此，我们提出了G-$\Delta$UQ，一种新的单模型UQ方法，它扩展了最近提出的随机居中框架，以支持结构化数据和部分随机性。在协变量、概念和图大小转移方面的评估中，G-$\Delta$UQ不仅在获得校准的CI方面表现优于几种流行的UQ方法，而且在CI需要设计为稀缺资源时表现优于其他替代方法。

    Safe deployment of graph neural networks (GNNs) under distribution shift requires models to provide accurate confidence indicators (CI). However, while it is well-known in computer vision that CI quality diminishes under distribution shift, this behavior remains understudied for GNNs. Hence, we begin with a case study on CI calibration under controlled structural and feature distribution shifts and demonstrate that increased expressivity or model size do not always lead to improved CI performance. Consequently, we instead advocate for the use of epistemic uncertainty quantification (UQ) methods to modulate CIs. To this end, we propose G-$\Delta$UQ, a new single model UQ method that extends the recently proposed stochastic centering framework to support structured data and partial stochasticity. Evaluated across covariate, concept, and graph size shifts, G-$\Delta$UQ not only outperforms several popular UQ methods in obtaining calibrated CIs, but also outperforms alternatives when CIs a
    
[^65]: SPFQ:一种用于神经网络量化的随机算法及其误差分析

    SPFQ: A Stochastic Algorithm and Its Error Analysis for Neural Network Quantization. (arXiv:2309.10975v1 [cs.LG])

    [http://arxiv.org/abs/2309.10975](http://arxiv.org/abs/2309.10975)

    SPFQ是一种用于神经网络量化的快速随机算法，通过贪婪的路径跟踪和随机量化器有效地减少网络中的冗余并提高量化效率。在本文中，我们首次建立了全网络的误差界，并通过应用于具有高斯权重的多层网络的量化证明了结果的有效性。

    

    量化是一种广泛使用的压缩方法，可以有效减少过参数化神经网络中的冗余。然而，由于存在非凸损失函数和非线性激活函数，现有的深度神经网络量化技术往往缺乏全面的误差分析。在本文中，我们提出了一种快速随机算法，用于量化完全训练好的神经网络的权重。我们的方法利用贪婪的路径跟踪机制结合随机量化器。其计算复杂度只与网络中的权重数量呈线性关系，从而实现了大型网络的高效量化。重要的是，我们首次在无穷字母条件和最小的权重和输入数据假设下建立了全网络的误差界。作为这一结果的一个应用，我们证明了当量化具有高斯权重的多层网络时，相对平方量化误差e的要点

    Quantization is a widely used compression method that effectively reduces redundancies in over-parameterized neural networks. However, existing quantization techniques for deep neural networks often lack a comprehensive error analysis due to the presence of non-convex loss functions and nonlinear activations. In this paper, we propose a fast stochastic algorithm for quantizing the weights of fully trained neural networks. Our approach leverages a greedy path-following mechanism in combination with a stochastic quantizer. Its computational complexity scales only linearly with the number of weights in the network, thereby enabling the efficient quantization of large networks. Importantly, we establish, for the first time, full-network error bounds, under an infinite alphabet condition and minimal assumptions on the weights and input data. As an application of this result, we prove that when quantizing a multi-layer network having Gaussian weights, the relative square quantization error e
    
[^66]: SEMPART: 自监督多分辨率图像语义分割

    SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics. (arXiv:2309.10972v1 [cs.CV])

    [http://arxiv.org/abs/2309.10972](http://arxiv.org/abs/2309.10972)

    本文提出了一种名为SEMPART的自监督多分辨率图像语义分割方法，通过联合推断图像的粗细双分割，并利用基于图形的正则化方法进行边界细节保留，成功将粗糙掩码语义提取为细粒度掩码，在显著对象检测和单个对象定位方面表现出色。

    

    当标记数据稀缺时，准确确定图像的显著区域是具有挑战性的。基于DINO的自监督方法最近利用了通过基于补丁的特征捕捉的有意义的图像语义来定位前景对象。最近的方法还融合了直观的先验知识，并展示了在无监督对象分割方法中具有价值。在本文中，我们提出了SEMPART，它在基于DINO的语义图中联合推断图像的粗细双分割。此外，SEMPART利用基于图形的正则化方法保留细节边界，并成功将粗糙掩码语义提取为细粒度掩码。我们的显著对象检测和单个对象定位结果表明，SEMPART能够快速产生高质量的掩码，并且通过协同优化粗分支和细分支而受益。

    Accurately determining salient regions of an image is challenging when labeled data is scarce. DINO-based self-supervised approaches have recently leveraged meaningful image semantics captured by patch-wise features for locating foreground objects. Recent methods have also incorporated intuitive priors and demonstrated value in unsupervised methods for object partitioning. In this paper, we propose SEMPART, which jointly infers coarse and fine bi-partitions over an image's DINO-based semantic graph. Furthermore, SEMPART preserves fine boundary details using graph-driven regularization and successfully distills the coarse mask semantics into the fine mask. Our salient object detection and single object localization findings suggest that SEMPART produces high-quality masks rapidly without additional post-processing and benefits from co-optimizing the coarse and fine branches.
    
[^67]: DPpack：一个用于差分隐私统计分析和机器学习的R包

    DPpack: An R Package for Differentially Private Statistical Analysis and Machine Learning. (arXiv:2309.10965v1 [stat.ML])

    [http://arxiv.org/abs/2309.10965](http://arxiv.org/abs/2309.10965)

    DPpack是一个开源的R包，提供了差分隐私统计分析和机器学习的工具集合，包括不同的保护机制和描述性统计函数。它还提供了隐私保护版本的逻辑回归、支持向量机和线性回归的实现，以及对每个模型的差分隐私超参数调整。

    

    差分隐私（DP）是目前用于保护个人隐私的最先进的框架，可用于发布聚合统计数据或从数据中构建统计/机器学习模型。我们开发了开源的R包DPpack，提供了大量的差分隐私分析工具。当前版本的DPpack实现了三种常用的DP保护机制：拉普拉斯，高斯和指数。此外，DPpack还提供了一个易于访问的隐私保护描述性统计函数工具包。其中包括均值、方差、协方差和分位数，以及直方图和列联表。最后，DPpack提供了基于隐私保护的逻辑回归、支持向量机和线性回归的用户友好实现，以及针对每个模型的差分隐私超参数调整。这个大量实现的差分隐私统计和模型集合允许无麻烦地利用差异化。

    Differential privacy (DP) is the state-of-the-art framework for guaranteeing privacy for individuals when releasing aggregated statistics or building statistical/machine learning models from data. We develop the open-source R package DPpack that provides a large toolkit of differentially private analysis. The current version of DPpack implements three popular mechanisms for ensuring DP: Laplace, Gaussian, and exponential. Beyond that, DPpack provides a large toolkit of easily accessible privacy-preserving descriptive statistics functions. These include mean, variance, covariance, and quantiles, as well as histograms and contingency tables. Finally, DPpack provides user-friendly implementation of privacy-preserving versions of logistic regression, SVM, and linear regression, as well as differentially private hyperparameter tuning for each of these models. This extensive collection of implemented differentially private statistics and models permits hassle-free utilization of differential
    
[^68]: 带有多个标签的文本分类中的上下文学习

    In-Context Learning for Text Classification with Many Labels. (arXiv:2309.10954v1 [cs.CL])

    [http://arxiv.org/abs/2309.10954](http://arxiv.org/abs/2309.10954)

    本文通过使用预训练的密集检索模型，解决了上下文学习中的标签限制问题，并在多个意图分类数据集的少样本设置中取得了新的最佳性能，同时在某些情况下超越了微调模型的表现。研究还发现，更大规模的模型对于有效利用更长的上下文长度进行上下文学习是必要的。

    

    使用大型语言模型进行具有许多标签的任务的上下文学习是具有挑战性的，因为有限的上下文窗口使得在提示中难以适应足够数量的示例。在本文中，我们使用预训练的密集检索模型绕过了这个限制，每次推理调用只给模型提供了对完整标签空间的部分视图。在最近的开源语言模型(OPT, LLaMA)上进行测试，我们在三个常见的意图分类数据集的少样本设置中，无需微调即取得了最新的最佳性能。在某些情况下，我们还超越了微调性能在细粒度情感分类上的表现。我们分析了不同数量的上下文示例以及不同模型规模下的性能，表明更大规模的模型对于有效而一致地利用更长的上下文长度进行上下文学习是必要的。通过运行几个消融实验，我们分析了模型对以下内容的使用：a)上下文示例与当前输入的相似度, b) 即时查询语句的相似度。

    In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt. In this paper, we use a pre-trained dense retrieval model to bypass this limitation, giving the model only a partial view of the full label space for each inference call. Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art performance in few-shot settings for three common intent classification datasets, with no finetuning. We also surpass fine-tuned performance on fine-grained sentiment classification in certain cases. We analyze the performance across number of in-context examples and different model scales, showing that larger models are necessary to effectively and consistently make use of larger context lengths for ICL. By running several ablations, we analyze the model's use of: a) the similarity of the in-context examples to the current input, b) 
    
[^69]: 基于深度强化学习的连续空间无限时域均场问题解决方法

    Deep Reinforcement Learning for Infinite Horizon Mean Field Problems in Continuous Spaces. (arXiv:2309.10953v1 [math.OC])

    [http://arxiv.org/abs/2309.10953](http://arxiv.org/abs/2309.10953)

    这项研究提出了一种基于深度强化学习的算法，通过将演员-评论家范式与均场分布表示配对，来解决连续空间中的均场博弈和均场控制问题，并使用朗之万动力学从分布中获取样本。该算法在渐近无限时域框架下使用线性二次基准进行评估。

    

    我们提出了一种强化学习算法，用于统一解决连续空间均场博弈（MFG）和均场控制（MFC）问题，并对其进行了分析和发展。所提出的方法将演员-评论家（AC）范式与通过参数化评分函数表示的均场分布配对，可以以在线方式有效地更新，并使用朗之万动力学从得到的分布中获得样本。AC代理和评分函数按迭代方式进行更新，以收敛到给定均场问题的MFG平衡或MFC最优解，具体取决于学习率的选择。算法的简单修改使我们能够解决混合均场控制博弈（MFCG）。我们使用渐近无限时域框架中的线性二次基准评估我们的算法性能。

    We present the development and analysis of a reinforcement learning (RL) algorithm designed to solve continuous-space mean field game (MFG) and mean field control (MFC) problems in a unified manner. The proposed approach pairs the actor-critic (AC) paradigm with a representation of the mean field distribution via a parameterized score function, which can be efficiently updated in an online fashion, and uses Langevin dynamics to obtain samples from the resulting distribution. The AC agent and the score function are updated iteratively to converge, either to the MFG equilibrium or the MFC optimum for a given mean field problem, depending on the choice of learning rates. A straightforward modification of the algorithm allows us to solve mixed mean field control games (MFCGs). The performance of our algorithm is evaluated using linear-quadratic benchmarks in the asymptotic infinite horizon framework.
    
[^70]: LMDX：基于语言模型的文档信息提取与定位

    LMDX: Language Model-based Document Information Extraction and Localization. (arXiv:2309.10952v1 [cs.CL])

    [http://arxiv.org/abs/2309.10952](http://arxiv.org/abs/2309.10952)

    LMDX是一种基于语言模型的文档信息提取与定位方法，克服了布局编码和答案虚构的困难，能够在半结构化文档中提取关键实体。

    

    大规模语言模型（LLM）在自然语言处理（NLP）中取得了革命性的进展，改进了许多现有任务的最新技术，并展示了新兴的能力。然而，LLM尚未成功应用于半结构化文档信息提取，这是许多文档处理工作流的核心，包括从视觉丰富的文档（VRD）中提取关键实体，给定预定义的目标模式。LLM在这个任务中的主要障碍是LLM中缺乏布局编码，这对于高质量的提取至关重要，以及缺乏一个基于理论的机制，确保答案不是虚构的。在本文中，我们介绍了一种基于语言模型的文档信息提取与定位（LMDX）的方法，用于将任意LLM适应文档信息提取。LMDX可以提取单一、重复和层次结构实体，无论是否有训练数据，并提供基于理论的保证。

    Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art on many existing tasks and exhibiting emergent capabilities. However, LLMs have not yet been successfully applied on semi-structured document information extraction, which is at the core of many document processing workflows and consists of extracting key entities from a visually rich document (VRD) given a predefined target schema. The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated. In this paper, we introduce Language Model-based Document Information Extraction and Localization (LMDX), a methodology to adapt arbitrary LLMs for document information extraction. LMDX can do extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and lo
    
[^71]: 使用速度向量场的自动驾驶车辆轨迹预测的新型深度神经网络

    A Novel Deep Neural Network for Trajectory Prediction in Automated Vehicles Using Velocity Vector Field. (arXiv:2309.10948v1 [cs.LG])

    [http://arxiv.org/abs/2309.10948](http://arxiv.org/abs/2309.10948)

    本论文提出一种新型的深度神经网络方法，通过结合速度向量场和基于数据驱动的学习方法，预测自动驾驶车辆的轨迹。实验结果表明，速度向量场的加入提高了预测的准确性。

    

    对其他道路用户的运动进行预测对于自动驾驶系统至关重要，因为它能够实现安全和明智的下游决策和运动规划。然而，当前基于学习的运动预测方法在预测范围增加或观测窗口减小时存在显著的性能下降。本文提出了一种新颖的轨迹预测技术，它将以流体流动动力学为灵感的速度向量场与基于数据驱动的学习方法相结合。在这项工作中，将向量场作为卷积-循环深度神经网络的额外输入，帮助预测给定一系列鸟瞰场景表示的最可能的未来轨迹。该模型在HighD数据集上与最先进的方法进行了性能比较，结果表明速度向量场的加入提高了预测的准确性，无论是短还是长的预测范围。

    Anticipating the motion of other road users is crucial for automated driving systems (ADS), as it enables safe and informed downstream decision-making and motion planning. Unfortunately, contemporary learning-based approaches for motion prediction exhibit significant performance degradation as the prediction horizon increases or the observation window decreases. This paper proposes a novel technique for trajectory prediction that combines a data-driven learning-based method with a velocity vector field (VVF) generated from a nature-inspired concept, i.e., fluid flow dynamics. In this work, the vector field is incorporated as an additional input to a convolutional-recurrent deep neural network to help predict the most likely future trajectories given a sequence of bird's eye view scene representations. The performance of the proposed model is compared with state-of-the-art methods on the HighD dataset demonstrating that the VVF inclusion improves the prediction accuracy for both short a
    
[^72]: 测试时间训练用于语音的应用

    Test-Time Training for Speech. (arXiv:2309.10930v1 [cs.SD])

    [http://arxiv.org/abs/2309.10930](http://arxiv.org/abs/2309.10930)

    本文研究了测试时间训练在语音应用中处理分布偏移的应用，提出了一种解决方案BitFit，该方案通过只考虑偏置参数进行微调，解决了测试时间训练中的关键挑战。

    

    本文研究了测试时间训练（TTT）在处理语音应用中的分布偏移问题上的应用。特别是，我们将分布偏移引入到标准语音分类任务的测试数据集中，例如说话人识别和情绪检测，并探讨测试时间训练如何帮助调整到这些分布偏移。在我们的实验中，我们包括了由背景噪声和语音的自然变化（如性别和年龄）引起的分布偏移，我们发现了测试时间训练的一些关键挑战，包括对优化超参数（例如优化步骤的数量和选择用于测试时间训练的参数子集）的敏感性以及可扩展性（例如，由于每个样本都有自己的一组参数，测试时间训练不可扩展）。最后，我们提出使用BitFit——一种仅考虑偏置参数进行微调的参数高效的文本应用中提出的微调算法——作为解决上述问题的方案。

    In this paper, we study the application of Test-Time Training (TTT) as a solution to handling distribution shifts in speech applications. In particular, we introduce distribution-shifts to the test datasets of standard speech-classification tasks -- for example, speaker-identification and emotion-detection -- and explore how Test-Time Training (TTT) can help adjust to the distribution-shift. In our experiments that include distribution shifts due to background noise and natural variations in speech such as gender and age, we identify some key-challenges with TTT including sensitivity to optimization hyperparameters (e.g., number of optimization steps and subset of parameters chosen for TTT) and scalability (e.g., as each example gets its own set of parameters, TTT is not scalable). Finally, we propose using BitFit -- a parameter-efficient fine-tuning algorithm proposed for text applications that only considers the bias parameters for fine-tuning -- as a solution to the aforementioned c
    
[^73]: 半自动化分区: 用于从科学文献中提取高质量结构化数据的平台

    Semi-automatic staging area for high-quality structured data extraction from scientific literature. (arXiv:2309.10923v1 [cs.CL])

    [http://arxiv.org/abs/2309.10923](http://arxiv.org/abs/2309.10923)

    这篇论文介绍了一种半自动化分区平台，用于从科学文献中提取超导体实验数据。该平台通过自动和手动过程的结合，提高了数据更新效率，同时保持或提高了数据质量。评估实验表明该分区平台显著提高了数据的管理质量。

    

    在本研究中，我们提出了一个用于从科学文章中采集超导体实验数据的 SuperCon 数据库的分区平台。我们的目标是提高更新 SuperCon 的效率，同时保持或提高数据质量。我们介绍了一个由自动和手动过程组成的工作流驱动的半自动化分区平台，用于从提取的数据库中对数据进行校验和纠错。异常检测自动过程用于预先筛选采集到的数据。用户可以通过定制的用户界面在原始 PDF 文档上进行数据验证和纠错。此外，当记录被纠错时，其原始数据被收集并用于改进机器学习模型的训练数据。评估实验表明我们的分区平台显著提高了数据的管理质量。我们将界面与传统的手动阅读 PDF 文档并在 Excel 文档中记录信息的方法进行了比较。

    In this study, we propose a staging area for ingesting new superconductors' experimental data in SuperCon that is machine-collected from scientific articles. Our objective is to enhance the efficiency of updating SuperCon while maintaining or enhancing the data quality. We present a semi-automatic staging area driven by a workflow combining automatic and manual processes on the extracted database. An anomaly detection automatic process aims to pre-screen the collected data. Users can then manually correct any errors through a user interface tailored to simplify the data verification on the original PDF documents. Additionally, when a record is corrected, its raw data is collected and utilised to improve machine learning models as training data. Evaluation experiments demonstrate that our staging area significantly improves curation quality. We compare the interface with the traditional manual approach of reading PDF documents and recording information in an Excel document. Using the in
    
[^74]: Riemannian流形上Matern高斯过程的后验收缩速率

    Posterior Contraction Rates for Mat\'ern Gaussian Processes on Riemannian Manifolds. (arXiv:2309.10918v1 [stat.ML])

    [http://arxiv.org/abs/2309.10918](http://arxiv.org/abs/2309.10918)

    该论文研究了定义在紧致Riemannian流形上的内在Matern高斯过程和外在过程之间的收缩速率，并发现它们的速率在适当匹配平滑参数的情况下是相等的。

    

    高斯过程在许多依赖于不确定性量化的机器学习应用中被使用。最近，已经开发了在几何设置下处理这些模型的计算工具，例如，当输入位于Riemannian流形上时。这引出了一个问题：这些内在模型在理论上是否可以证明相比于将所有相关量嵌入到$\mathbb{R}^d$并使用普通欧几里德高斯过程的限制，可以带来更好的性能？为了研究这个问题，我们证明了定义在紧致Riemannian流形上的内在Matern高斯过程的最优收缩速率。我们还通过流形和环境Sobolev空间之间的迹和扩展定理证明了外在过程的类似速率：令人惊讶的是，所得到的速率与内在过程的速率相符，前提是它们的平滑参数适当匹配。我们在一些实证数据上进行了对这些速率的演示。

    Gaussian processes are used in many machine learning applications that rely on uncertainty quantification. Recently, computational tools for working with these models in geometric settings, such as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all relevant quantities into $\mathbb{R}^d$ and using the restriction of an ordinary Euclidean Gaussian process? To study this, we prove optimal contraction rates for intrinsic Mat\'ern Gaussian processes defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the rates obtained turn out to coincide with those of the intrinsic processes, provided that their smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of
    
[^75]: 使用大型语言模型的端到端语音识别上下文化

    End-to-End Speech Recognition Contextualization with Large Language Models. (arXiv:2309.10917v1 [eess.AS])

    [http://arxiv.org/abs/2309.10917](http://arxiv.org/abs/2309.10917)

    本论文介绍了一种在语音识别中使用大型语言模型进行上下文化的新方法。通过整合预训练的语言模型，我们的方法在训练过程中通过提供音频特征和文本上下文，使系统隐含地学习如何利用上下文信息，并达到了显著的性能改善。

    

    近年来，大型语言模型（LLMs）由于其出色的性能和泛化能力而受到研究界的广泛关注。本文介绍了一种新的方法，用于在语音识别模型中融入LLMs进行上下文化。我们的方法将语音识别视为基于预训练LLM的混合模态语言建模任务。我们提供音频特征以及可选的文本标记来训练系统以解码方式完成转录。因此，在训练过程中，系统会隐含地学习如何利用非结构化的上下文信息。我们的实证结果表明性能显著提高，当提供额外的文本上下文时，词错误率（WER）降低了6%。此外，我们发现我们的方法在竞争中表现良好，并在整体上将WER提高了7.5%，对于罕见词语的WER提高了17%，相较于基准上下文化RNN-T系统的训练结果。

    In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We provide audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system is implicitly incentivized to learn how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively and improve by 7.5% WER overall and 17% WER on rare words against a baseline contextualized RNN-T system that has been trained on
    
[^76]: 通过学习表示和影响函数，我们能从对抗样本中获得什么信息

    What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v1 [cs.LG])

    [http://arxiv.org/abs/2309.10916](http://arxiv.org/abs/2309.10916)

    本文将图像处理领域中的对抗子空间技术应用于自然语言处理，提出了基于最近邻和影响函数的检测器，并通过使用影响函数揭示了自然语言处理中的对抗样本子空间与图像处理中的子空间的关系和任务差异。

    

    对抗样本是通过微小扰动来欺骗深度神经网络的，起初在图像处理领域进行研究，最近在自然语言处理领域也开始关注。尽管在自然语言处理中检测对抗样本的方法主要依赖于输入扰动的搜索，但图像处理领域已经发展出一系列技术来表征学习表示中的对抗子空间。本文将这两种方法应用于自然语言处理，一种基于最近邻和影响函数，一种基于马氏距离。特别是前者相比几个强基准产生了最先进的检测器；此外，对影响函数的新颖使用揭示了自然语言处理中的对抗样本子空间与图像处理中的子空间的关系，并展示了它们根据不同自然语言处理任务的差异。

    Adversarial examples, deliberately crafted using small perturbations to fool deep neural networks, were first studied in image processing and more recently in NLP. While approaches to detecting adversarial examples in NLP have largely relied on search over input perturbations, image processing has seen a range of techniques that aim to characterise adversarial subspaces over the learned representations.  In this paper, we adapt two such approaches to NLP, one based on nearest neighbors and influence functions and one on Mahalanobis distances. The former in particular produces a state-of-the-art detector when compared against several strong baselines; moreover, the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing, and also how they differ depending on the kind of NLP task.
    
[^77]: 通过跨数据集迁移学习来放大脑电信号通路中的病理检测

    Amplifying Pathological Detection in EEG Signaling Pathways through Cross-Dataset Transfer Learning. (arXiv:2309.10910v1 [cs.LG])

    [http://arxiv.org/abs/2309.10910](http://arxiv.org/abs/2309.10910)

    该研究通过跨数据集迁移学习来放大脑电信号通路中的病理检测，以提高病理诊断的准确性和可行性，解决了标注数据稀缺性和低成本招募真实患者队列的问题。

    

    基于脑电信号和脑活动解码的病理诊断在理解神经系统疾病方面具有重要意义。随着人工智能方法和机器学习技术的进步，准确的数据驱动诊断和有效的治疗的潜力显著增长。然而，将机器学习算法应用于真实世界的数据集在多个层面上都存在各种挑战。标注数据的稀缺性，特别是在低范围情景下，由于高昂的招募成本导致真实患者队列有限，凸显了扩展和迁移学习技术的重要性。在这项研究中，我们探讨了一个真实的病理分类任务，以凸显数据和模型扩展以及跨数据集知识迁移的有效性。因此，我们观察到通过数据扩展可以获得不同程度的性能改进，表明需要进行仔细评估和标注。

    Pathology diagnosis based on EEG signals and decoding brain activity holds immense importance in understanding neurological disorders. With the advancement of artificial intelligence methods and machine learning techniques, the potential for accurate data-driven diagnoses and effective treatments has grown significantly. However, applying machine learning algorithms to real-world datasets presents diverse challenges at multiple levels. The scarcity of labelled data, especially in low regime scenarios with limited availability of real patient cohorts due to high costs of recruitment, underscores the vital deployment of scaling and transfer learning techniques. In this study, we explore a real-world pathology classification task to highlight the effectiveness of data and model scaling and cross-dataset knowledge transfer. As such, we observe varying performance improvements through data scaling, indicating the need for careful evaluation and labelling. Additionally, we identify the chall
    
[^78]: 自我增强改进了零样本跨语言传递

    Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer. (arXiv:2309.10891v1 [cs.CL])

    [http://arxiv.org/abs/2309.10891](http://arxiv.org/abs/2309.10891)

    本文提出了一种名为SALT的方法，通过结合代码切换和嵌入混合与自我增强，有效地提高了多语言预训练模型的零样本跨语言传递能力。

    

    零样本跨语言传递是多语言自然语言处理的核心任务，允许在具有更充足训练资源的语言中训练的模型推广到其他资源匮乏的语言。先前在这个任务上的努力使用平行语料库、双语词典或其他标注对齐数据来提高跨语言传递能力，这些通常是昂贵的获取方式。在本文中，我们提出了一种简单而有效的方法SALT，用于改进多语言预训练语言模型的零样本跨语言传递，而不需要这些外部数据的帮助。通过结合代码切换和嵌入混合与自我增强，SALT有效地蒸馏了多语言PLM的跨语言知识，并增强了其在下游任务中的传递能力。在XNLI和PAWS-X上的实验结果表明，我们的方法可以在没有外部数据的情况下提高零样本跨语言传递能力。我们的代码可以在https://github.com/luka-group/SALT找到。

    Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. Our code is available at https://github.com/luka-group/SALT.
    
[^79]: Crypto'Graph：利用隐私保护的分布式链路预测实现强大的图形学习

    Crypto'Graph: Leveraging Privacy-Preserving Distributed Link Prediction for Robust Graph Learning. (arXiv:2309.10890v1 [cs.CR])

    [http://arxiv.org/abs/2309.10890](http://arxiv.org/abs/2309.10890)

    Crypto'Graph是一种用于隐私保护的分布式图形链路预测协议，在不透露各方私有图形结构的情况下，允许参与方推断未来新链路的形成可能性。

    

    图形是一种广泛使用的数据结构，用于收集和分析关系数据。然而，当图形结构分布在多个参与方之间时，其分析尤为具有挑战性。特别是由于数据的敏感性，每个参与方可能希望保持其对图形的部分知识私有，同时仍愿意与其他参与方合作进行相互利益的任务，如数据整理或毒数据删除。为了解决这一挑战，我们提出了Crypto'Graph，这是一种用于隐私保护的分布式图形链路预测的高效协议。更具体地说，它允许部分共享具有分布式链路的图形的参与方推断未来新链路的形成可能性。通过使用加密原语，Crypto'Graph能够在不透露每个参与方的私有个人图形结构的情况下，在联合网络上计算这些新链路的可能性，即使他们知道参与方的数量。

    Graphs are a widely used data structure for collecting and analyzing relational data. However, when the graph structure is distributed across several parties, its analysis is particularly challenging. In particular, due to the sensitivity of the data each party might want to keep their partial knowledge of the graph private, while still willing to collaborate with the other parties for tasks of mutual benefit, such as data curation or the removal of poisoned data. To address this challenge, we propose Crypto'Graph, an efficient protocol for privacy-preserving link prediction on distributed graphs. More precisely, it allows parties partially sharing a graph with distributed links to infer the likelihood of formation of new links in the future. Through the use of cryptographic primitives, Crypto'Graph is able to compute the likelihood of these new links on the joint network without revealing the structure of the private individual graph of each party, even though they know the number of 
    
[^80]: DeepliteRT：边缘计算机视觉中的深度学习

    DeepliteRT: Computer Vision at the Edge. (arXiv:2309.10878v1 [cs.LG])

    [http://arxiv.org/abs/2309.10878](http://arxiv.org/abs/2309.10878)

    DeepliteRT是一个在边缘设备上部署深度学习模型的解决方案，通过超低位量化技术实现了高效的模型运算，并通过编译器转换工作简化了量化模型的部署过程。

    

    边缘设备的普及为计算机视觉应用中深度学习模型的部署开辟了前所未有的机会。然而，这些复杂模型需要大量的电力、内存和计算资源，而这些资源通常在边缘平台上不可用。超低位量化提供了一个解决方案，通过将模型权重和激活从32位缩减到小于8位来降低模型的复杂度。我们在基于ARM架构的目标上实现了高度优化的超低位卷积运算符，性能优于现有方法的最多4.34倍。我们的运算符是在Deeplite Runtime（DeepliteRT）中实现的，这是一个端到端的解决方案，用于在ARM设备上编译、调优和推断超低位模型。在DeepliteRT中的编译器转换工作会将一个基于伪量化的全精度模型自动转换为紧凑的超低位表示，从而简化了在商用硬件上部署量化模型的过程。我们分析了DeepliteRT在某些计算机视觉任务上的性能。

    The proliferation of edge devices has unlocked unprecedented opportunities for deep learning model deployment in computer vision applications. However, these complex models require considerable power, memory and compute resources that are typically not available on edge platforms. Ultra low-bit quantization presents an attractive solution to this problem by scaling down the model weights and activations from 32-bit to less than 8-bit. We implement highly optimized ultra low-bit convolution operators for ARM-based targets that outperform existing methods by up to 4.34x. Our operator is implemented within Deeplite Runtime (DeepliteRT), an end-to-end solution for the compilation, tuning, and inference of ultra low-bit models on ARM devices. Compiler passes in DeepliteRT automatically convert a fake-quantized model in full precision to a compact ultra low-bit representation, easing the process of quantized model deployment on commodity hardware. We analyze the performance of DeepliteRT on 
    
[^81]: 一种基于深度学习的天气预测模型的动力学测试

    Dynamical Tests of a Deep-Learning Weather Prediction Model. (arXiv:2309.10867v1 [physics.ao-ph])

    [http://arxiv.org/abs/2309.10867](http://arxiv.org/abs/2309.10867)

    最近的研究发现，全球深度学习天气预测模型能够产生与基于物理的模型相媲美的预测。这项研究对一种名为Pangu-weather的模型进行了一系列动力学实验，结果表明该模型在添加恒定的热源和局部扰动时产生了经典的大气动力学响应和信号传播行为。

    

    最近已经证明全球深度学习天气预测模型能够产生与操作中心使用基于物理的模型相媲美的预测。目前不清楚这些模型是否编码了大气动力学，还是只是进行了模式匹配以获得最小的预测误差。回答这个问题对于确定这些模型作为基础科学工具的实用性至关重要。在这里，我们对一个名为Pangu-weather的模型进行了一系列四个经典的动力学实验，这些实验与模型训练数据不相似。我们对模型输出和初始条件施加局部扰动，并加入恒定的时间平均条件，以评估信号自局部源点传播的速度和结构演化。通过添加恒定的热源来扰动模型物理，结果显示在加热区域附近有经典的松野-吉尔响应，并有向外层热带放射的行星波。在冬季均值上施加局部扰动将产生球对称波和地转波等静止波，-简称MJO。

    Global deep-learning weather prediction models have recently been shown to produce forecasts that rival those from physics-based models run at operational centers. It is unclear whether these models have encoded atmospheric dynamics, or simply pattern matching that produces the smallest forecast error. Answering this question is crucial to establishing the utility of these models as tools for basic science. Here we subject one such model, Pangu-weather, to a set of four classical dynamical experiments that do not resemble the model training data. Localized perturbations to the model output and the initial conditions are added to steady time-averaged conditions, to assess the propagation speed and structural evolution of signals away from the local source. Perturbing the model physics by adding a steady tropical heat source results in a classical Matsuno--Gill response near the heating, and planetary waves that radiate into the extratropics. A localized disturbance on the winter-average
    
[^82]: 通过行为和遗传特征整合改进阿片类药物使用障碍风险建模

    Improving Opioid Use Disorder Risk Modelling through Behavioral and Genetic Feature Integration. (arXiv:2309.10837v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.10837](http://arxiv.org/abs/2309.10837)

    通过整合与阿片类药物使用障碍有关的遗传变异和行为特征，该论文开发了一种实验设计和计算方法，用于评估阿片类药物使用障碍的风险。结果显示整合遗传和迁移模式可以改善风险估计的能力。

    

    阿片类药物是一种有效的急性和慢性疼痛止痛药，但也带来了相当大的成瘾风险，导致美国每年数百万阿片类药物使用障碍（OUD）病例和数万人的过早死亡。在处方之前估计OUD风险可以改进治疗方案的效果、监测计划和干预策略，但风险估计通常基于自我报告的数据或调查问卷。我们开发了实验设计和计算方法，将与OUD相关的遗传变异与从GPS和Wi-Fi时空坐标中提取的行为特征结合起来评估OUD风险。由于OUD迁移和遗传数据不存在于同一个群体中，我们开发了算法来(1)根据经验分布生成迁移特征和(2)合成迁移和遗传样本，假设存在某种共病和相对风险水平。我们展示了整合遗传和迁移模式可以改善风险估计的能力。

    Opioids are an effective analgesic for acute and chronic pain, but also carry a considerable risk of addiction leading to millions of opioid use disorder (OUD) cases and tens of thousands of premature deaths in the United States yearly. Estimating OUD risk prior to prescription could improve the efficacy of treatment regimens, monitoring programs, and intervention strategies, but risk estimation is typically based on self-reported data or questionnaires. We develop an experimental design and computational methods that combines genetic variants associated with OUD with behavioral features extracted from GPS and Wi-Fi spatiotemporal coordinates to assess OUD risk. Since both OUD mobility and genetic data do not exist for the same cohort, we develop algorithms to (1) generate mobility features from empirical distributions and (2) synthesize mobility and genetic samples assuming a level of comorbidity and relative risks. We show that integrating genetic and mobility modalities improves ris
    
[^83]: 分析大脑年龄预测中的种族和性别偏见

    Analysing race and sex bias in brain age prediction. (arXiv:2309.10835v1 [eess.IV])

    [http://arxiv.org/abs/2309.10835](http://arxiv.org/abs/2309.10835)

    通过对种族和生物性别亚组进行全面分析和特征检查，我们发现大脑年龄预测模型存在偏见。我们使用Kruskal-Wallis和Conover-Iman检验发现了种族和性别之间的偏见，使用PCA和Kolmogorov-Smirnov检验检测到了生成特征的分布变化。

    

    从MRI中预测大脑年龄已成为与多种神经病理学相关的流行成像生物标志物。然而，用于训练的数据集往往在人口统计上偏斜和不平衡，这可能使大脑年龄预测模型容易受到偏见的影响。我们通过进行全面的亚组性能分析和特征检查来分析常用的ResNet-34模型。该模型在来自Cam-CAN和IXI的1,215个T1加权MRI扫描上进行训练，并在UK Biobank（n=42,786）上进行测试，分为六个种族和生物性别亚组。为了比较亚组之间的性能，通过绝对预测误差来衡量，我们使用Kruskal-Wallis检验，后跟两个后续的Conover-Iman检验来检查种族和生物性别之间的偏见。为了检查生成的特征中的偏见，我们使用PCA进行维度约简，并使用两样本Kolmogorov-Smirnov检验来识别亚组之间的分布变化。

    Brain age prediction from MRI has become a popular imaging biomarker associated with a wide range of neuropathologies. The datasets used for training, however, are often skewed and imbalanced regarding demographics, potentially making brain age prediction models susceptible to bias. We analyse the commonly used ResNet-34 model by conducting a comprehensive subgroup performance analysis and feature inspection. The model is trained on 1,215 T1-weighted MRI scans from Cam-CAN and IXI, and tested on UK Biobank (n=42,786), split into six racial and biological sex subgroups. With the objective of comparing the performance between subgroups, measured by the absolute prediction error, we use a Kruskal-Wallis test followed by two post-hoc Conover-Iman tests to inspect bias across race and biological sex. To examine biases in the generated features, we use PCA for dimensionality reduction and employ two-sample Kolmogorov-Smirnov tests to identify distribution shifts among subgroups. Our results 
    
[^84]: 存在稀疏的随机网络：通过正则化增强通信效率的联合学习方法

    Sparser Random Networks Exist: Enforcing Communication-Efficient Federated Learning via Regularization. (arXiv:2309.10834v1 [cs.LG])

    [http://arxiv.org/abs/2309.10834](http://arxiv.org/abs/2309.10834)

    本论文提出了一种通过正则化方法增强通信效率的联合学习方法，该方法训练过参数化的随机网络，通过优化二进制掩码来减少通信开销，同时在本地目标中添加正则化项来促进稀疏网络的解决方案。实验证明，在通信和内存效率方面取得了显著的改进。

    

    本研究提出了一种新的方法，用于增强随机联合学习中的通信效率，该方法训练过参数化的随机网络。在这种设置下，优化的是二进制掩码，而不是固定的模型权重。该掩码表征了一个能够和较小的目标网络一样好地泛化的稀疏子网络。重要的是，在传统联合学习中，交换的是稀疏的二进制掩码，而不是浮点权重，这样可以将通信成本降低到每个参数最多1个比特。我们发现，之前的最先进的随机方法无法找到能够通过一致的损失目标减少通信和存储开销的稀疏网络。为了解决这个问题，我们建议在本地目标中添加正则化项，通过消除子网络之间的冗余特征来促进更稀疏的解决方案。大量实验表明，在通信和内存效率方面取得了显著的改进，高达五个数量级。

    This work presents a new method for enhancing communication efficiency in stochastic Federated Learning that trains over-parameterized random networks. In this setting, a binary mask is optimized instead of the model weights, which are kept fixed. The mask characterizes a sparse sub-network that is able to generalize as good as a smaller target network. Importantly, sparse binary masks are exchanged rather than the floating point weights in traditional federated learning, reducing communication cost to at most 1 bit per parameter. We show that previous state of the art stochastic methods fail to find the sparse networks that can reduce the communication and storage overhead using consistent loss objectives. To address this, we propose adding a regularization term to local objectives that encourages sparser solutions by eliminating redundant features across sub-networks. Extensive experiments demonstrate significant improvements in communication and memory efficiency of up to five magni
    
[^85]: 活动学习强化学习：一种随机最优控制方法的应用

    Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach. (arXiv:2309.10831v1 [cs.LG])

    [http://arxiv.org/abs/2309.10831](http://arxiv.org/abs/2309.10831)

    本文提供了一个框架来解决强化学习中的模型不确定性和计算成本高的问题，通过使用强化学习解决随机动态规划方程，生成的控制器能够主动学习模型不确定性，并确保安全性和实时学习。

    

    本文提供了一个框架来应对两个问题：（i）强化学习在模型不确定性方面的脆弱性，因为受控实验室/仿真和实际条件之间的不匹配，以及（ii）随机最优控制的计算成本过高。我们通过使用强化学习来解决随机动态规划方程来解决这两个问题。由此产生的强化学习控制器对于几种类型的约束条件是安全的，并且它可以主动学习模型不确定性。与探索和利用不同，探测和安全性由控制器自身自动实现，实现了实时学习。一个仿真示例证明了所提方法的有效性。

    In this paper we provide framework to cope with two problems: (i) the fragility of reinforcement learning due to modeling uncertainties because of the mismatch between controlled laboratory/simulation and real-world conditions and (ii) the prohibitive computational cost of stochastic optimal control. We approach both problems by using reinforcement learning to solve the stochastic dynamic programming equation. The resulting reinforcement learning controller is safe with respect to several types of constraints constraints and it can actively learn about the modeling uncertainties. Unlike exploration and exploitation, probing and safety are employed automatically by the controller itself, resulting real-time learning. A simulation example demonstrates the efficacy of the proposed approach.
    
[^86]: 网格变分自编码器中的潜在解缠构架在脑颅综合症的诊断和手术规划中有所改进

    Latent Disentanglement in Mesh Variational Autoencoders Improves the Diagnosis of Craniofacial Syndromes and Aids Surgical Planning. (arXiv:2309.10825v1 [eess.IV])

    [http://arxiv.org/abs/2309.10825](http://arxiv.org/abs/2309.10825)

    本研究改进了网格变分自编码器中的潜在解缠构架，可以提高脑颅综合症的诊断准确性，同时辅助手术规划，并允许客观评估手术结果。

    

    使用深度学习对人类头部的形状进行分析具有很大的潜力。然而，在全局和局部水平上准确建模一直存在一些障碍。在本研究中，我们将讨论交换解缠变分自编码器（SD-VAE）在Crouzon、Apert和Muenke综合征方面的应用。虽然综合症分类是在整个网格上进行的，但首次可以分析头部每个区域对综合症表型的影响。通过操纵生成模型的特定参数，并产生特定手术的新形状，还可以模拟一系列颅颜面手术的结果。这打开了推动诊断进展、帮助手术规划和实施手术结果客观评估的新途径。

    The use of deep learning to undertake shape analysis of the complexities of the human head holds great promise. However, there have traditionally been a number of barriers to accurate modelling, especially when operating on both a global and local level. In this work, we will discuss the application of the Swap Disentangled Variational Autoencoder (SD-VAE) with relevance to Crouzon, Apert and Muenke syndromes. Although syndrome classification is performed on the entire mesh, it is also possible, for the first time, to analyse the influence of each region of the head on the syndromic phenotype. By manipulating specific parameters of the generative model, and producing procedure-specific new shapes, it is also possible to simulate the outcome of a range of craniofacial surgical procedures. This opens new avenues to advance diagnosis, aids surgical planning and allows for the objective evaluation of surgical outcomes.
    
[^87]: 天气和气候的AI基础模型：应用、设计和实施

    AI Foundation Models for Weather and Climate: Applications, Design, and Implementation. (arXiv:2309.10808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.10808](http://arxiv.org/abs/2309.10808)

    该论文介绍了AI基础模型在天气和气候中的应用，主要讨论了使用transformers、物理知识启发的机器学习和图神经网络的最新方法。尽管取得了进展，但全球地球系统模型的可推广AI模型仍处于初级阶段。

    

    机器学习和深度学习方法在理解大气的混沌行为和推进天气预报方面得到了广泛探索。科技公司、政府机构和气象机构对建立地球的数字孪生体表现出越来越大的兴趣。最近使用transformers、受物理知识启发的机器学习和图神经网络的方法在相对狭窄的时空范围和特定任务上展示了最先进的性能。随着使用预训练的transformers进行语言建模和视觉生成人工智能的最近成功，我们正在朝着可推广的人工智能模型迈进。特别是，我们正在见证能够在多个领域特定下游任务上有竞争力的AI基础模型的崛起。尽管取得了进展，但全球地球系统模型的可推广AI模型仍处于初级阶段。

    Machine learning and deep learning methods have been widely explored in understanding the chaotic behavior of the atmosphere and furthering weather forecasting. There has been increasing interest from technology companies, government institutions, and meteorological agencies in building digital twins of the Earth. Recent approaches using transformers, physics-informed machine learning, and graph neural networks have demonstrated state-of-the-art performance on relatively narrow spatiotemporal scales and specific tasks. With the recent success of generative artificial intelligence (AI) using pre-trained transformers for language modeling and vision with prompt engineering and fine-tuning, we are now moving towards generalizable AI. In particular, we are witnessing the rise of AI foundation models that can perform competitively on multiple domain-specific downstream tasks. Despite this progress, we are still in the nascent stages of a generalizable AI model for global Earth system models
    
[^88]: PDRL：基于多智能体强化学习的预测监控

    PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring. (arXiv:2309.10576v1 [cs.LG])

    [http://arxiv.org/abs/2309.10576](http://arxiv.org/abs/2309.10576)

    本文提出了一种基于多智能体强化学习的预测监控系统（PDRL），该系统可以在复杂环境中监测预测未来状态，并通过奖励策略来学习现有知识和最大化奖励。

    

    强化学习因其能够从以往经验中学习并做出自适应决策的能力，在监控应用中越来越被应用。然而，现有的基于机器学习的健康监控应用大多是基于监督学习算法，训练标签数据，无法在不确定的复杂环境中做出自适应决策。本研究提出了一种新颖的通用系统，即具有多个强化学习智能体的预测深度强化学习（PDRL），应用于时间序列预测环境。该提出的通用框架可以容纳虚拟深度 Q 网络（DQN）智能体，以监测复杂环境的预测未来状态，并根据明确定义的奖励策略使智能体在最大化奖励的同时学习现有知识。在评估该框架的过程中，部署了三个强化学习智能体以监测通过 BiLSTM 模型预测的受试者未来的心率、呼吸率和体温。随着每次迭代，智能体根据实际反馈调整其行为策略。

    Reinforcement learning has been increasingly applied in monitoring applications because of its ability to learn from previous experiences and can make adaptive decisions. However, existing machine learning-based health monitoring applications are mostly supervised learning algorithms, trained on labels and they cannot make adaptive decisions in an uncertain complex environment. This study proposes a novel and generic system, predictive deep reinforcement learning (PDRL) with multiple RL agents in a time series forecasting environment. The proposed generic framework accommodates virtual Deep Q Network (DQN) agents to monitor predicted future states of a complex environment with a well-defined reward policy so that the agent learns existing knowledge while maximizing their rewards. In the evaluation process of the proposed framework, three DRL agents were deployed to monitor a subject's future heart rate, respiration, and temperature predicted using a BiLSTM model. With each iteration, t
    
[^89]: PoSE: 通过位置跳跃式训练提高LLMs对于上下文窗口的有效拓展

    PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])

    [http://arxiv.org/abs/2309.10400](http://arxiv.org/abs/2309.10400)

    本文介绍了一种名为PoSE的训练方法，通过在训练过程中使用固定的上下文窗口和操纵位置索引来适应极长的上下文窗口，实验证明这种方法大大减小了内存和时间开销，对性能影响较小，成功将LLaMA模型扩展到了128k个标记。

    

    本文介绍了一种名为Positional Skip-wise (PoSE)训练的方法，用于将大型语言模型（LLMs）适应于极长的上下文窗口。PoSE通过在训练过程中使用固定的上下文窗口和操纵位置索引来模拟长输入，将训练长度与目标上下文窗口大小分离。具体而言，我们从长输入序列中选择若干短块，并引入不同的跳跃偏置项来修改每个块的位置索引。这些跳跃偏置项以及每个块的长度在每个训练样本中都会变化，使得模型能够适应目标上下文窗口中的所有位置，而无需对完整长度的输入进行训练。实验证明，与对完整长度进行微调相比，PoSE大大减小了内存和时间开销，对性能影响较小。利用这一优势，我们成功将LLaMA模型扩展到了128k个标记。此外，我们经验证实，PoSE与

    In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with 
    
[^90]: 可微分的量子架构搜索用于量子强化学习

    Differentiable Quantum Architecture Search for Quantum Reinforcement Learning. (arXiv:2309.10392v1 [quant-ph])

    [http://arxiv.org/abs/2309.10392](http://arxiv.org/abs/2309.10392)

    DQAS是一个基于梯度的框架，用于在NISQ时代自动设计量子电路。本研究的目标是探索DQAS在解决量子深度Q-learning问题上的能力。

    

    可微分量子架构搜索（DQAS）是一种基于梯度的框架，可以在NISQ时代自动设计量子电路。它的动机是量子硬件的低保真度，电路架构的低灵活性，电路设计成本高，平坦的荒原问题和权重的周期性。人们使用它来解决基于固定数据集的误差缓解、酉分解和量子逼近优化问题。量子强化学习（QRL）是量子机器学习的一部分，通常有各种数据。QRL通常使用手动设计的电路。然而，预定义的电路需要更多的灵活性来应对不同的任务，在不同数据集上基于电路设计可能变得棘手，特别是在电路规模较大的情况下。DQAS能否应用于具有不同数据集的量子深度Q-learning问题仍未解决。这项工作的主要目标是发现DQAS在解决量子深度Q-learning问题上的能力。

    Differentiable quantum architecture search (DQAS) is a gradient-based framework to design quantum circuits automatically in the NISQ era. It was motivated by such as low fidelity of quantum hardware, low flexibility of circuit architecture, high circuit design cost, barren plateau (BP) problem, and periodicity of weights. People used it to address error mitigation, unitary decomposition, and quantum approximation optimization problems based on fixed datasets. Quantum reinforcement learning (QRL) is a part of quantum machine learning and often has various data. QRL usually uses a manually designed circuit. However, the pre-defined circuit needs more flexibility for different tasks, and the circuit design based on various datasets could become intractable in the case of a large circuit. The problem of whether DQAS can be applied to quantum deep Q-learning with various datasets is still open. The main target of this work is to discover the capability of DQAS to solve quantum deep Q-learni
    
[^91]: TensorCodec: 无强数据假设的紧凑有损张量压缩

    TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions. (arXiv:2309.10310v1 [cs.LG])

    [http://arxiv.org/abs/2309.10310](http://arxiv.org/abs/2309.10310)

    TensorCodec是一种紧凑的有损张量压缩算法，可以处理无强数据假设的一般张量。它采用神经张量列车分解、折叠输入张量和重新排序模式索引等关键点来提高压缩效果。

    

    许多现实世界的数据集都是以张量的形式表示的，即多维数值数组。如果不进行压缩，存储这些数据集通常需要大量的空间，而且随着维度的增加呈指数级增长。尽管有许多张量压缩算法可用，但其中许多依赖于关于数据维度、稀疏性、秩和平滑性的强假设。在这项工作中，我们提出了TENSORCODEC，这是一种针对一般张量的有损压缩算法，不需要符合强假设的输入数据。TENSORCODEC包含了三个关键点。第一个关键点是神经张量列车分解（NTTD），我们将循环神经网络集成到张量列车分解中，以增强其表达能力并减轻由低秩假设所带来的限制。另一个关键点是将输入张量折叠成更高阶的张量，以减少NTTD所需的空间。最后，对输入张量进行模式索引的重新排序，以揭示可以被利用的模式。

    Many real-world datasets are represented as tensors, i.e., multi-dimensional arrays of numerical values. Storing them without compression often requires substantial space, which grows exponentially with the order. While many tensor compression algorithms are available, many of them rely on strong data assumptions regarding its order, sparsity, rank, and smoothness. In this work, we propose TENSORCODEC, a lossy compression algorithm for general tensors that do not necessarily adhere to strong input data assumptions. TENSORCODEC incorporates three key ideas. The first idea is Neural Tensor-Train Decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power and alleviate the limitations imposed by the low-rank assumption. Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD. Finally, the mode indices of the input tensor are reordered to reveal patterns that can be explo
    
[^92]: Primal-Dual $\ell_0$-约束稀疏指数跟踪

    Primal-Dual $\ell_0$-Constrained Sparse Index Tracking. (arXiv:2309.10152v1 [q-fin.PM])

    [http://arxiv.org/abs/2309.10152](http://arxiv.org/abs/2309.10152)

    本文提出了一种新的稀疏指数跟踪问题的形式化，使用了$\ell_0$-范数约束，可以轻松控制投资组合中资产数量的上限。

    

    稀疏指数跟踪是一种重要的被动投资组合管理策略，它通过构建稀疏投资组合来跟踪金融指数。相比于全仓投资组合，稀疏投资组合在降低交易成本和避免不流动资产方面更具优势。为了强制投资组合的稀疏性，传统研究提出了基于$\ell_p$-范数正则化的公式，作为$\ell_0$-范数正则化的连续替代。尽管这样的公式可以用来构建稀疏投资组合，但在实际投资中却不易使用，因为细致的参数调整来指定投资组合中资产数量的上限是艰难且耗时的。本文提出了一种新的稀疏指数跟踪问题的形式化，使用了$\ell_0$-范数约束，从而可以轻松控制投资组合中资产数量的上限。此外，我们的形式化允许在投资组合稀疏性和换手率之间进行选择。

    Sparse index tracking is one of the prominent passive portfolio management strategies that construct a sparse portfolio to track a financial index. A sparse portfolio is desirable over a full portfolio in terms of transaction cost reduction and avoiding illiquid assets. To enforce the sparsity of the portfolio, conventional studies have proposed formulations based on $\ell_p$-norm regularizations as a continuous surrogate of the $\ell_0$-norm regularization. Although such formulations can be used to construct sparse portfolios, they are not easy to use in actual investments because parameter tuning to specify the exact upper bound on the number of assets in the portfolio is delicate and time-consuming. In this paper, we propose a new problem formulation of sparse index tracking using an $\ell_0$-norm constraint that enables easy control of the upper bound on the number of assets in the portfolio. In addition, our formulation allows the choice between portfolio sparsity and turnover spa
    
[^93]: Q-Transformer：通过自回归Q-函数提供可扩展的离线强化学习

    Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions. (arXiv:2309.10150v1 [cs.RO])

    [http://arxiv.org/abs/2309.10150](http://arxiv.org/abs/2309.10150)

    Q-Transformer是一种可扩展的离线强化学习方法，通过使用Transformer来表示Q函数并利用离线数据集进行训练。它在大规模真实世界机器人操作任务中表现优越。

    

    在这项工作中，我们提出了一种可扩展的强化学习方法，用于训练可以利用人类演示和自主采集数据的大型离线数据集的多任务策略。我们的方法使用Transformer提供可扩展的Q函数表示，通过离线时差备份进行训练。因此，我们将该方法称为Q-Transformer。通过将每个动作维度进行离散化，并将每个动作维度的Q值表示为单独的标记，我们可以应用高容量序列建模技术进行Q学习。我们提出了几个设计决策，使其在离线RL训练中表现出良好性能，并展示了Q-Transformer在大规模多样化的真实世界机器人操作任务套件上优于以往的离线RL算法和模仿学习技术。该项目的网站和视频可以在https://q-transformer.github.io找到。

    In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://q-transformer.github.io
    
[^94]: 上下文≈环境。

    Context $\approx$ Environment. (arXiv:2309.09888v1 [cs.LG])

    [http://arxiv.org/abs/2309.09888](http://arxiv.org/abs/2309.09888)

    在这篇论文中，作者通过理论与实验证明了将注意力放在上下文-未标记样本上，可以实现更好的领域泛化。

    

    AI研究的中心在于两个方面。一方面，社区正在努力构建能够丢弃虚假相关性并在新颖的测试环境中更好地进行泛化的模型。不幸的是，到目前为止，没有任何提案能够令人信服地超越简单的经验风险最小化基线。另一方面，大型语言模型(LLMs)已经成为能够在上下文中学习、根据用户通过提示施加的多种上下文背景灵活泛化的算法。本文认为上下文≈环境，并假设在上下文学习中隐藏着更好的领域泛化之钥。通过广泛的理论与实验，我们展示了注意上下文-未标记的样本的重要性，这种注意可以使我们提出的In-Context Risk Minimization (ICRM)算法聚焦于测试环境风险最小化。

    Two lines of work are taking the central stage in AI research. On the one hand, the community is making increasing efforts to build models that discard spurious correlations and generalize better in novel test environments. Unfortunately, the bitter lesson so far is that no proposal convincingly outperforms a simple empirical risk minimization baseline. On the other hand, large language models (LLMs) have erupted as algorithms able to learn in-context, generalizing on-the-fly to the eclectic contextual circumstances that users enforce by means of prompting. In this paper, we argue that context $\approx$ environment, and posit that in-context learning holds the key to better domain generalization. Via extensive theory and experiments, we show that paying attention to context$\unicode{x2013}\unicode{x2013}$unlabeled examples as they arrive$\unicode{x2013}\unicode{x2013}$allows our proposed In-Context Risk Minimization (ICRM) algorithm to zoom-in on the test environment risk minimizer, le
    
[^95]: 对比初始状态缓冲区在强化学习中的应用

    Contrastive Initial State Buffer for Reinforcement Learning. (arXiv:2309.09752v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09752](http://arxiv.org/abs/2309.09752)

    本论文提出了对比初始状态缓冲区的概念，它通过选择过去的经验中的状态来初始化环境中的代理，以引导其进入更有信息量的状态。实验证明，该方法在两个复杂机器人任务上取得了更高的任务性能并加速了训练过程。

    

    在强化学习中，勘探与利用之间的平衡给从有限的样本中实现高效学习带来了复杂的挑战。虽然最近的工作在利用过去的经验进行策略更新方面是有效的，但它们常常忽视了重新利用过去经验进行数据收集的潜力。独立于基本强化学习算法，我们引入了对比初始状态缓冲区的概念，它从过去的经验中选择状态，并用这些状态初始化环境中的代理，以引导它走向更具信息量的状态。我们在两个复杂的机器人任务上验证了我们的方法，而不依赖任何关于环境的先验信息：（i）四足机器人穿越具有挑战性的地形和（ii）四旋翼无人机在赛道上飞行。实验结果表明，我们的初始状态缓冲区在任务性能上优于基准线，同时还加速了训练过程。

    In Reinforcement Learning, the trade-off between exploration and exploitation poses a complex challenge for achieving efficient learning from limited samples. While recent works have been effective in leveraging past experiences for policy updates, they often overlook the potential of reusing past experiences for data collection. Independent of the underlying RL algorithm, we introduce the concept of a Contrastive Initial State Buffer, which strategically selects states from past experiences and uses them to initialize the agent in the environment in order to guide it toward more informative states. We validate our approach on two complex robotic tasks without relying on any prior information about the environment: (i) locomotion of a quadruped robot traversing challenging terrains and (ii) a quadcopter drone racing through a track. The experimental results show that our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training conv
    
[^96]: 通过差分神经计算，智能机器在无结构环境中工作

    Intelligent machines work in unstructured environments by differential neural computing. (arXiv:2309.08835v1 [eess.SP])

    [http://arxiv.org/abs/2309.08835](http://arxiv.org/abs/2309.08835)

    本研究提出了一种基于差分神经计算的智能机器方法，通过提取环境信息的主要特征并应用相应的编码刺激到记忆阻性器件，成功地实现了处理无结构环境信息的类人能力，并展现了良好的可扩展性和泛化性。该方法在物体抓取和自动驾驶等应用方面得到了验证。

    

    希望智能机器能够在现实世界中高效地工作，需要一种新的方法来准确地理解未知环境中的无结构信息，具有良好的准确性、可扩展性和泛化性，就像人类一样。本文介绍了一种基于记忆阻性神经计算的感知信号差分处理和学习方法，通过提取环境信息的主要特征并应用相关编码刺激到记忆阻性器件，我们成功地获得了处理无结构环境信息的类人能力，如机械刺激的放大（>720%）和适应（<50%）。该方法还展现了良好的可扩展性和泛化性，在智能机器的两个典型应用中得到了验证：物体抓取和自动驾驶。在物体抓取方面，通过在1毫秒内使用单个记忆阻性器件学习未知物体特征（例如尖锐的角和光滑的表面），一个机器手实现了安全稳定的抓取。

    Expecting intelligent machines to efficiently work in real world requires a new method to understand unstructured information in unknown environments with good accuracy, scalability and generalization, like human. Here, a memristive neural computing based perceptual signal differential processing and learning method for intelligent machines is presented, via extracting main features of environmental information and applying associated encoded stimuli to memristors, we successfully obtain human-like ability in processing unstructured environmental information, such as amplification (>720%) and adaptation (<50%) of mechanical stimuli. The method also exhibits good scalability and generalization, validated in two typical applications of intelligent machines: object grasping and autonomous driving. In the former, a robot hand experimentally realizes safe and stable grasping, through learning unknown object features (e.g., sharp corner and smooth surface) with a single memristor in 1 ms. In
    
[^97]: 使用CLUB-PLS解决影像遗传学中的维度问题

    Tackling the dimensions in imaging genetics with CLUB-PLS. (arXiv:2309.07352v1 [q-bio.GN])

    [http://arxiv.org/abs/2309.07352](http://arxiv.org/abs/2309.07352)

    本文介绍了一种名为Cluster-Bootstrap PLS（CLUB-PLS）的基于偏最小二乘的框架，用于解决影像遗传学中的维度问题。该框架可以处理两个领域的大量输入维度和大样本量，并使用聚类自助法提供稳健的统计数据。

    

    影像遗传学和类似领域的一个主要挑战是将一个领域（如遗传数据）中的高维数据与另一个领域（如脑成像数据）中的高维数据进行关联。该领域的标准方法是通过遗传因素和成像表型进行大规模单变量分析。这意味着对每个预定义的成像测量执行一次全基因组关联研究（GWAS）。尽管这种方法非常成功，但它的一个缺点是表型必须事先定义好。因此，不被限制在预选感兴趣区域内或反映更大的整个大脑模式的效应很容易被忽视。在这项工作中，我们引入了一种基于偏最小二乘（PLS）的框架，称为Cluster-Bootstrap PLS（CLUB-PLS），它可以同时处理两个领域的大量输入维度以及大样本量。该框架的一个关键因素是使用聚类自助法为单个输入特征提供稳健的统计数据。

    A major challenge in imaging genetics and similar fields is to link high-dimensional data in one domain, e.g., genetic data, to high dimensional data in a second domain, e.g., brain imaging data. The standard approach in the area are mass univariate analyses across genetic factors and imaging phenotypes. That entails executing one genome-wide association study (GWAS) for each pre-defined imaging measure. Although this approach has been tremendously successful, one shortcoming is that phenotypes must be pre-defined. Consequently, effects that are not confined to pre-selected regions of interest or that reflect larger brain-wide patterns can easily be missed. In this work we introduce a Partial Least Squares (PLS)-based framework, which we term Cluster-Bootstrap PLS (CLUB-PLS), that can work with large input dimensions in both domains as well as with large sample sizes. One key factor of the framework is to use cluster bootstrap to provide robust statistics for single input features in b
    
[^98]: 神经潜在几何搜索：通过格罗莫夫-豪斯多夫信息驱动的贝叶斯优化来进行乘积流形推断

    Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization. (arXiv:2309.04810v1 [cs.LG])

    [http://arxiv.org/abs/2309.04810](http://arxiv.org/abs/2309.04810)

    本论文提出了神经潜在几何搜索(NLGS)的概念，旨在通过格罗莫夫-豪斯多夫距离来自动识别下游任务的最佳潜在几何结构，以提高机器学习模型的性能。

    

    最近的研究表明，通过将潜在空间的几何结构与底层数据结构对齐，可以提高机器学习模型的性能。研究人员提出使用具有恒定曲率的双曲和球形空间，或者它们的组合，来更好地建模潜在空间并增强模型性能，而不仅仅依赖于欧几里得空间。然而，目前对自动识别下游任务的最佳潜在几何结构问题还没有给予足够关注。我们在数学上定义了这个新颖的问题，并将其称为神经潜在几何搜索(NLGS)。具体而言，我们引入了一种基于格罗莫夫-豪斯多夫距离的候选潜在几何结构之间的新概念距离，以实现这一目标。为了计算格罗莫夫-豪斯多夫距离，我们提出了一种通过最小查询评估搜索由恒定曲率模型空间乘积组成的潜在几何结构的原则方法。

    Recent research indicates that the performance of machine learning models can be improved by aligning the geometry of the latent space with the underlying data structure. Rather than relying solely on Euclidean space, researchers have proposed using hyperbolic and spherical spaces with constant curvature, or combinations thereof, to better model the latent space and enhance model performance. However, little attention has been given to the problem of automatically identifying the optimal latent geometry for the downstream task. We mathematically define this novel formulation and coin it as neural latent geometry search (NLGS). More specifically, we introduce a principled method that searches for a latent geometry composed of a product of constant curvature model spaces with minimal query evaluations. To accomplish this, we propose a novel notion of distance between candidate latent geometries based on the Gromov-Hausdorff distance from metric geometry. In order to compute the Gromov-Ha
    
[^99]: 使用受限玻尔兹曼机推断有效耦合

    Inferring effective couplings with Restricted Boltzmann Machines. (arXiv:2309.02292v2 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2309.02292](http://arxiv.org/abs/2309.02292)

    本研究通过实现受限玻尔兹曼机的能量函数与有效伊辛自旋哈密顿量之间的直接映射，提供了一种用于推断复杂数据中高阶相互作用的方法。

    

    生成模型为我们提供了一种直接建模复杂数据的方法。在这些模型中，基于能量的模型为我们提供了一个神经网络模型，旨在以模型的玻尔兹曼权重的水平准确重现数据中观察到的所有统计相关性。然而，一个挑战是理解这些模型的物理解释。在本研究中，我们通过实现受限玻尔兹曼机的能量函数与包含自旋之间高阶相互作用的有效伊辛自旋哈密顿量之间的直接映射，提出了一个简单的解决方案。这种映射包括所有可能阶的相互作用，超越了逆伊辛方法中通常考虑的传统二次相互作用，使其能够描述复杂的数据集。早期的研究试图实现这个目标，但所提出的映射没有正确处理问题的复杂性，或者没有具体的实际应用指导。

    Generative models offer a direct way to model complex data. Among them, energy-based models provide us with a neural network model that aims to accurately reproduce all statistical correlations observed in the data at the level of the Boltzmann weight of the model. However, one challenge is to understand the physical interpretation of such models. In this study, we propose a simple solution by implementing a direct mapping between the energy function of the Restricted Boltzmann Machine and an effective Ising spin Hamiltonian that includes high-order interactions between spins. This mapping includes interactions of all possible orders, going beyond the conventional pairwise interactions typically considered in the inverse Ising approach, and allowing the description of complex datasets. Earlier works attempted to achieve this goal, but the proposed mappings did not do properly treat the complexity of the problem or did not contain direct prescriptions for practical application. To valid
    
[^100]: 大语言模型赋能文本到SQL的研究：一个基准评估

    Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v1 [cs.DB])

    [http://arxiv.org/abs/2308.15363](http://arxiv.org/abs/2308.15363)

    本文提出了一个大规模语言模型(LLMs)赋能的文本到SQL任务的基准评估，并基于实验结果提出了一种新的集成解决方案DAIL-SQL，刷新了Spider榜单并实现了86.6%的执行准确率。同时，强调了在提示工程中的词汇效率以实现高效经济的LLM-based文本到SQL解决方案，此外还对在上下文学习中应用开源LLMs进行了研究，并进行了任务特定的性能优化。

    

    大语言模型(LLMs)已经成为文本到SQL任务的一种新范式。然而，缺乏一个系统性的基准阻碍了设计有效、高效和经济的LLM-based文本到SQL解决方案的发展。为了解决这一挑战，本文首先对现有的提示工程方法进行了系统性和广泛的比较，包括问题表示、示例选择和示例组织，并根据实验结果详细阐述了它们的优缺点。基于这些发现，我们提出了一种新的集成解决方案，名为DAIL-SQL，刷新了Spider榜单，达到了86.6%的执行准确率，建立了一个新的标杆。为了实现高效经济的LLM-based文本到SQL解决方案，我们强调提示工程中的词汇效率，并在此度量下比较了之前的研究。此外，我们还研究了上下文学习中的开源LLMs，并用任务特定的监督进行了进一步的性能优化。

    Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborates their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. Additionally, we investigate open-source LLMs in in-context learning, and further enhance their performance with task-specific superv
    
[^101]: 贝叶斯探索网络

    Bayesian Exploration Networks. (arXiv:2308.13049v1 [cs.LG])

    [http://arxiv.org/abs/2308.13049](http://arxiv.org/abs/2308.13049)

    这篇论文提出了一种贝叶斯探索网络的方法，通过在一维Bellman算子中建模不确定性，解决了贝叶斯强化学习中学习贝叶斯最优策略的计算复杂性的挑战。

    

    贝叶斯强化学习为不确定性下的顺序决策提供了一种原则性和优雅的方法。最显著的是，贝叶斯代理不会面临频率方法的探索/开发困境，这是一个重大的问题。贝叶斯强化学习的一个关键挑战是学习贝叶斯最优策略的计算复杂性，这在玩具领域中是可计算的。在本文中，我们提出了一种新颖的无模型方法来解决这一挑战。与基于模型的方法不同，我们在一维Bellman算子中建模不确定性而不是在高维状态转移分布中建模。我们的理论分析揭示了现有的无模型方法要么不通过MDP传播认知不确定性，要么在一组语境策略中优化而不是所有历史条件策略。这两个近似得到的策略可能是任意贝叶斯次优的。为了克服这些问题，我们引入了贝叶斯探索网络（Bayesian exploration network）。

    Bayesian reinforcement learning (RL) offers a principled and elegant approach for sequential decision making under uncertainty. Most notably, Bayesian agents do not face an exploration/exploitation dilemma, a major pathology of frequentist methods. A key challenge for Bayesian RL is the computational complexity of learning Bayes-optimal policies, which is only tractable in toy domains. In this paper we propose a novel model-free approach to address this challenge. Rather than modelling uncertainty in high-dimensional state transition distributions as model-based approaches do, we model uncertainty in a one-dimensional Bellman operator. Our theoretical analysis reveals that existing model-free approaches either do not propagate epistemic uncertainty through the MDP or optimise over a set of contextual policies instead of all history-conditioned policies. Both approximations yield policies that can be arbitrarily Bayes-suboptimal. To overcome these issues, we introduce the Bayesian explo
    
[^102]: Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG]) 该论文标题已翻译：二元强化学习。

    Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG])

    [http://arxiv.org/abs/2308.07843](http://arxiv.org/abs/2308.07843)

    该论文介绍了一个称为二元强化学习的在线算法，用于根据上下文因素和目标人与其照顾伴侣的过去反馈，个性化地提供干预措施。该算法是贝叶斯和层次的，并通过模拟展示了良好的实证效果。

    

    移动医疗旨在通过在个人日常生活中提供干预来提高健康结果。照顾伴侣和社会支持网络的参与经常在帮助个人管理繁重的医疗条件方面起着关键作用。这为移动医疗提供了机会，设计针对二元关系——目标人和其照顾伴侣之间关系——以提高社会支持的干预措施。在本文中，我们开发了二元强化学习（Dyadic RL），这是一种基于环境因素和目标人及其照顾伴侣的过去反馈个性化干预措施的在线强化学习算法。在这里，多组干预措施影响着二元关系在多个时间间隔内。开发的二元强化学习是贝叶斯和层次的。我们正式介绍了问题设定，开发了二元强化学习并确定了遗憾边界。通过模拟，我们展示了二元强化学习的实证效果。

    Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation st
    
[^103]: REFORMS: 基于机器学习的科学报告标准

    REFORMS: Reporting Standards for Machine Learning Based Science. (arXiv:2308.07832v1 [cs.LG])

    [http://arxiv.org/abs/2308.07832](http://arxiv.org/abs/2308.07832)

    REFORMS是一个基于机器学习的科学报告标准，旨在解决使用机器学习方法在科学研究中出现的有效性、可重复性和可推广性失败问题。这个标准由32个问题和一套指导方针组成，可作为研究人员设计和实施科研时的参考资源。

    

    机器学习方法在科学研究中得到了广泛应用。然而，这些方法的采用也伴随着有效性、可重复性和可推广性的失败。这些失败可能会阻碍科学进展，导致对无效结论的错误共识，并削弱基于机器学习的科学的可信度。机器学习方法在不同学科中常常以相似的方式应用且失败。出于这个观察，我们的目标是为基于机器学习的科学提供清晰的报告标准。基于对过去文献的广泛评论，我们提出了REFORMS检查表（$\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience）。它由32个问题和一套配套的指导方针组成。REFORMS是基于19位研究人员的共识开发的，这些人来自计算机科学、数据科学、数学、社会科学和生物医学科学领域。REFORMS可以为研究人员在设计和实施科研时提供参考。

    Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing 
    
[^104]: 解决医学影像深度学习中的小型注释数据集问题：对比共同对比学习和掩码自编码器方法在CT扫描卷积模型中的自监督预训练的评估

    Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])

    [http://arxiv.org/abs/2308.06534](http://arxiv.org/abs/2308.06534)

    本研究评估了在医学影像领域使用自监督预训练方法的可行性，比较了共同对比学习和掩码自编码器方法在CT扫描卷积模型中的性能。

    

    医学影像中的深度学习有潜力减少诊断错误的风险、减轻放射科医生的工作负担并加速确诊。训练这样的深度学习模型需要大型且准确的数据集，并且需要为所有训练样本提供注释。然而，在医学影像领域，由于注释的高复杂性、受限的获取方式或疾病的罕见性，特定任务的注释数据集通常很小。为了应对这一挑战，深度学习模型可以使用自监督学习领域的方法，在没有注释的大型图像数据集上进行预训练。在预训练之后，小型的已注释数据集就足以对模型进行特定任务的微调，即所谓的“下游任务”。医学影像中最流行的自监督预训练方法基于共同对比学习。然而，最近的自然图像处理研究表明掩码自编码器方法具有很大的潜力。本研究比较了二者在CT扫描卷积模型中的性能。

    Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
    
[^105]: 多类在线学习在Bandit反馈下的研究

    Multiclass Online Learnability under Bandit Feedback. (arXiv:2308.04620v1 [cs.LG])

    [http://arxiv.org/abs/2308.04620](http://arxiv.org/abs/2308.04620)

    Bandit反馈下的在线多类学习的关键在于Bandit Littlestone维度的有限性，无论标签空间是否无界。

    

    我们研究了在Bandit反馈下的多类在线分类问题。我们扩展了(daniely2013price)的结果，通过展示Bandit Littlestone维度的有限性是多类在线学习的必要且充分条件，即使标签空间是无界的。我们的结果补充了(hanneke2023multiclass)的最近工作，他们在标签空间无界的全信息设置中，展示了Littlestone维度刻画了在线多类学习的能力。

    We study online multiclass classification under bandit feedback. We extend the results of (daniely2013price) by showing that the finiteness of the Bandit Littlestone dimension is necessary and sufficient for bandit online multiclass learnability even when the label space is unbounded. Our result complements the recent work by (hanneke2023multiclass) who show that the Littlestone dimension characterizes online multiclass learnability in the full-information setting when the label space is unbounded.
    
[^106]: 基于LOB的深度学习模型用于股票价格趋势预测：一项基准研究

    LOB-Based Deep Learning Models for Stock Price Trend Prediction: A Benchmark Study. (arXiv:2308.01915v1 [q-fin.TR])

    [http://arxiv.org/abs/2308.01915](http://arxiv.org/abs/2308.01915)

    本研究通过基准研究探讨了基于LOB数据的股票价格趋势预测的15种最新DL模型的鲁棒性和泛化能力。实验证明这些模型在面对新数据时性能明显下降，对其在实际市场中的应用性提出了问题。

    

    深度学习（DL）研究在金融行业中产生了显著影响。我们研究了基于限价订单簿（LOB）数据的股票价格趋势预测（SPTP）的十五种最新 DL模型的鲁棒性和泛化能力。为了进行这项研究，我们开发了LOBCAST，一个开源框架，包括数据预处理、DL 模型训练、评估和利润分析。我们的大量实验表明，所有模型在面对新数据时性能显著下降，从而对它们在实际市场中的适用性提出了疑问。我们的工作作为一个基准，揭示了当前方法的潜力和局限性，并为创新解决方案提供了见解。

    The recent advancements in Deep Learning (DL) research have notably influenced the finance sector. We examine the robustness and generalizability of fifteen state-of-the-art DL models focusing on Stock Price Trend Prediction (SPTP) based on Limit Order Book (LOB) data. To carry out this study, we developed LOBCAST, an open-source framework that incorporates data preprocessing, DL model training, evaluation and profit analysis. Our extensive experiments reveal that all models exhibit a significant performance drop when exposed to new data, thereby raising questions about their real-world market applicability. Our work serves as a benchmark, illuminating the potential and the limitations of current approaches and providing insight for innovative solutions.
    
[^107]: 一种具有连续对称性的新型卷积神经网络架构

    A Novel Convolutional Neural Network Architecture with a Continuous Symmetry. (arXiv:2308.01621v1 [cs.CV])

    [http://arxiv.org/abs/2308.01621](http://arxiv.org/abs/2308.01621)

    本文介绍了一种新的卷积神经网络架构，通过连续的对称性修改权重，具有与传统模型不同的特点，希望将对称性作为神经网络的新期望特性，并推广将偏微分方程的视角应用于ConvNet的分析和解释。

    

    本文介绍了一种新的卷积神经网络(ConvNet)架构，其灵感来自于一类称为拟线性双曲系统的偏微分方程(PDEs)。在图像分类任务上具有可比较的性能，它允许通过连续的对称性修改权重。这是与传统模型中基本固定的架构和权重相比的重大转变。我们希望将(内部)对称性作为一种新的神经网络期望特性，并在更广泛的深度学习社区中吸引对PDE视角分析和解释ConvNet的关注。

    This paper introduces a new Convolutional Neural Network (ConvNet) architecture inspired by a class of partial differential equations (PDEs) called quasi-linear hyperbolic systems. With comparable performance on image classification task, it allows for the modification of the weights via a continuous group of symmetry. This is a significant shift from traditional models where the architecture and weights are essentially fixed. We wish to promote the (internal) symmetry as a new desirable property for a neural network, and to draw attention to the PDE perspective in analyzing and interpreting ConvNets in the broader Deep Learning community.
    
[^108]: 通过填充和置换指纹编码的方法，对差分隐私算法提供平滑下界

    Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes. (arXiv:2307.07604v1 [cs.CR])

    [http://arxiv.org/abs/2307.07604](http://arxiv.org/abs/2307.07604)

    本论文提出了一种通过填充和置换指纹编码的方法来产生困难实例，从而在各种情景下提供平滑下界。这方法适用于差分隐私平均问题和近似k.

    

    指纹编码方法是最广泛用于确定约束差分隐私算法的样本复杂度或错误率的方法。然而，对于许多差分隐私问题，我们并不知道适当的下界，并且即使对于我们知道的问题，下界也不平滑，并且通常在误差大于某个阈值时变得无意义。在这项工作中，我们通过将填充和置换转换应用于指纹编码，提出了一种生成困难实例的简单方法。我们通过在不同情景下提供新的下界来说明这种方法的适用性：1. 低准确度情景下差分隐私平均问题的紧密下界，这尤其意味着新的私有1簇问题的下界 2. 近似k

    Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.  In this work, we present a simple method to generate hard instances by applying a padding-and-permuting transformation to a fingerprinting code. We illustrate the applicability of this method by providing new lower bounds in various settings:  1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a new lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).  2. A lower bound on the additive error of DP algorithms for approximate k
    
[^109]: 使用深度集成神经网络在端点设备上预测小分子的溶解度

    Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks. (arXiv:2307.05318v1 [physics.chem-ph])

    [http://arxiv.org/abs/2307.05318](http://arxiv.org/abs/2307.05318)

    这项工作提出了一种使用深度集成神经网络在端点设备上预测小分子溶解度的方法，通过静态网站运行，同时具备预测不确定性，并实现了令人满意的结果。

    

    水溶解度是一种有价值但难以预测的性质。使用一级原理方法计算溶解度需要考虑熵和焓的竞争效应，导致计算时间较长且准确性相对较差。基于数据驱动的方法，如深度学习，提供了更高的准确性和计算效率，但通常缺乏不确定性量化。此外，任何计算技术的易用性仍然是一个问题，导致群体贡献方法的持续流行。在这项工作中，我们使用一种具有预测不确定性的深度学习模型来解决这些问题，该模型在静态网站上运行（无需服务器）。这种方法将计算需求转移到网站访问者身上，而不需要安装，消除了支付和维护服务器的需求。我们的模型在溶解度预测上取得了令人满意的结果。此外，我们展示了如何创建平衡溶解度预测模型。

    Aqueous solubility is a valuable yet challenging property to predict. Computing solubility using first-principles methods requires accounting for the competing effects of entropy and enthalpy, resulting in long computations for relatively poor accuracy. Data-driven approaches, such as deep learning, offer improved accuracy and computational efficiency but typically lack uncertainty quantification. Additionally, ease of use remains a concern for any computational technique, resulting in the sustained popularity of group-based contribution methods. In this work, we addressed these problems with a deep learning model with predictive uncertainty that runs on a static website (without a server). This approach moves computing needs onto the website visitor without requiring installation, removing the need to pay for and maintain servers. Our model achieves satisfactory results in solubility prediction. Furthermore, we demonstrate how to create molecular property prediction models that balanc
    
[^110]: 基于大型语言模型的概念导向深度学习

    Concept-Oriented Deep Learning with Large Language Models. (arXiv:2306.17089v1 [cs.LG])

    [http://arxiv.org/abs/2306.17089](http://arxiv.org/abs/2306.17089)

    本文讨论了大型语言模型在概念导向深度学习中的应用，包括从文本和图像中提取概念和概念图。同时也探讨了多模态语言模型在表达人类知识方面的优势。

    

    大型语言模型（LLMs）已成功应用于许多自然语言任务和应用，包括文本生成和人工智能聊天机器人。它们也是概念导向深度学习（CODL）的一种有前景的新技术。然而，前提是LLMs要理解概念并确保概念一致性。本文讨论了这些问题，以及LLMs在CODL中的主要用途，包括从文本中提取概念、从文本中提取概念图和概念学习。人类知识包括符号（概念性）知识和具体（感性）知识。而仅文本的LLMs只能表示符号（概念性）知识。另一方面，多模态LLMs能够表示人类知识的完整范围（概念性和感性）。我们讨论了视觉-语言LLMs中的概念理解，这是最重要的多模态LLMs，并介绍了它们在CODL中的主要用途，包括从图像中提取概念、从图像中提取概念图。

    Large Language Models (LLMs) have been successfully used in many natural-language tasks and applications including text generation and AI chatbots. They also are a promising new technology for concept-oriented deep learning (CODL). However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency. We discuss these in this paper, as well as major uses of LLMs for CODL including concept extraction from text, concept graph extraction from text, and concept learning. Human knowledge consists of both symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal LLMs, on the other hand, are capable of representing the full range (conceptual and sensory) of human knowledge. We discuss conceptual understanding in visual-language LLMs, the most important multimodal LLMs, and major uses of them for CODL including concept extraction from image, concept graph extraction from image
    
[^111]: 评估社交机器人导航算法的原则与指南

    Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])

    [http://arxiv.org/abs/2306.16740](http://arxiv.org/abs/2306.16740)

    本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。

    

    在人类居住环境中导航是部署机器人广泛应用的主要挑战，通常被称为社交机器人导航。虽然社交导航领域近年来取得了很大进展，但评估解决社交导航的算法仍然困难，因为它不仅涉及机器人在静态环境中移动，还涉及到动态的人类参与者及其对机器人行为的感知适应性。相比之下，清晰、可重复、易于获得的基准在计算机视觉、自然语言处理和传统机器人导航等领域加速了进展，使研究人员能够公平比较算法，揭示现有解决方案的局限性，并呈现有前途的新方向。我们相信相同的方法可以有助于社交导航。在本文中，我们为评估社交机器人导航建立了共同、广泛可用且可重复的基准标准，并提出了自己的创新点。

    A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
    
[^112]: PEAR: 基于原始操作的自适应重标记用于Boosting层次强化学习

    PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning. (arXiv:2306.06394v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06394](http://arxiv.org/abs/2306.06394)

    PEAR是一种基于原始操作的自适应重标记方法，用于Boosting层次强化学习。它通过对专家演示进行自适应重标记来生成高效的子目标监督，并通过联合优化强化学习和模仿学习来训练分层代理。实验结果显示，PEAR能够在具有挑战性的机器人环境中取得良好的性能。

    

    层次强化学习（HRL）利用时间抽象和增加的探索性能解决复杂的长期任务。然而，由于固有的非静态性，分层代理难以训练。我们提出了基于原始操作的自适应重标记（PEAR），这是一个两阶段方法，我们首先对少量专家演示进行自适应重标记，产生高效的子目标监督，然后通过使用强化学习（RL）和模仿学习（IL）联合优化HRL代理。我们进行理论分析来$(i)$限制我们方法的次优性，和$(ii)$推导出使用RL和IL的广义即插即用的框架进行联合优化。PEAR使用一些专家演示，并对任务结构进行最小的限制假设。此外，它可以轻松与典型的模型自由RL算法集成，产生一个实用的HRL算法。我们在具有挑战性的机器人环境上进行了实验。

    Hierarchical reinforcement learning (HRL) has the potential to solve complex long horizon tasks using temporal abstraction and increased exploration. However, hierarchical agents are difficult to train due to inherent non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a two-phase approach where we first perform adaptive relabeling on a few expert demonstrations to generate efficient subgoal supervision, and then jointly optimize HRL agents by employing reinforcement learning (RL) and imitation learning (IL). We perform theoretical analysis to $(i)$ bound the sub-optimality of our approach, and $(ii)$ derive a generalized plug-and-play framework for joint optimization using RL and IL. PEAR uses a handful of expert demonstrations and makes minimal limiting assumptions on the task structure. Additionally, it can be easily integrated with typical model free RL algorithms to produce a practical HRL algorithm. We perform experiments on challenging robotic environments
    
[^113]: 自适应基于梯度的异常值去除的嘈杂标签学习方法

    Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])

    [http://arxiv.org/abs/2306.04502](http://arxiv.org/abs/2306.04502)

    本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。

    

    训练可靠和高性能模型需要准确和丰富的数据集，但即便是人工标注的数据集也会包含错误，更不用说自动标注的数据集了。现有的一些数据去噪方法主要集中于检测异常值并进行永久性去除，但这种方法很容易过度或者欠度过滤数据集。在本论文中，我们提出了一种新的自适应梯度异常值去除方法（AGRA），不同于在模型训练之前清洗数据集，我们的方法在训练过程中动态调整数据集。通过比较一组样本的累积梯度和单个样本的梯度，我们的方法可以决定是否在当前更新时保留对应的样本，以此来确定它是否有助于模型的学习效果。在多个数据集上进行的广泛评估表明，AGRA方法的有效性，并且全面的结果分析证实了我们方法的理论和实践收益。

    An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
    
[^114]: 使用深度强化学习的自适应PD控制在具有随机时延的局部-远程遥操作中

    Adaptive PD Control using Deep Reinforcement Learning for Local-Remote Teleoperation with Stochastic Time Delays. (arXiv:2305.16979v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.16979](http://arxiv.org/abs/2305.16979)

    本论文引入了一种使用深度强化学习来解决局部-远程遥操作中的时延问题的自适应PD控制方法，通过实时调整控制器参数，改善同步性和稳定性，并克服了随机延迟带来的挑战。

    

    局部-远程系统允许机器人在危险环境中执行复杂任务，如太空和核电站。然而，由于可能威胁到系统性能和稳定性的时延，确立局部和远程设备之间的准确位置映射可能很困难。增强局部-远程系统的同步性和稳定性对于使机器人能够在更远距离和高度挑战的网络环境中与环境交互至关重要，包括时延。我们引入了一种采用强化学习来解决时延控制问题的自适应控制方法。通过实时调整控制器参数，这种自适应控制器可以补偿随机延迟并改善局部和远程机器人操作器之间的同步性。为了提高自适应PD控制器的性能，我们设计了一种基于模型的强化学习方法，有效地将多步延迟纳入到学习框架中。

    Local-remote systems allow robots to execute complex tasks in hazardous environments such as space and nuclear power stations. However, establishing accurate positional mapping between local and remote devices can be difficult due to time delays that can compromise system performance and stability. Enhancing the synchronicity and stability of local-remote systems is vital for enabling robots to interact with environments at greater distances and under highly challenging network conditions, including time delays. We introduce an adaptive control method employing reinforcement learning to tackle the time-delayed control problem. By adjusting controller parameters in real-time, this adaptive controller compensates for stochastic delays and improves synchronicity between local and remote robotic manipulators. To improve the adaptive PD controller's performance, we devise a model-based reinforcement learning approach that effectively incorporates multi-step delays into the learning framewor
    
[^115]: Marsellus: 一款具有2至8位DNN加速和30%自适应体偏化的异构RISC-V AI-IoT末端SoC

    Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN Acceleration and 30%-Boost Adaptive Body Biasing. (arXiv:2305.08415v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2305.08415](http://arxiv.org/abs/2305.08415)

    Marsellus是一款具有2至8位DNN加速和30%自适应体偏化的异构RISC-V AI-IoT末端SoC，适用于计算密集型的深度神经网络推理以及高精度浮点运算的信号处理和控制。

    

    新兴的人工智能互联物联网（AI-IoT）系统级芯片（SoC）需要在范围广泛的工作条件下，在几十毫瓦的功耗限制下运行许多不同的任务，包括计算密集型但强量化的深度神经网络（DNN）推理以及需要高精度浮点运算的信号处理和控制。我们提出了Marsellus，一个在GlobalFoundries 22nm FDX上制造的全数字异构SoC，用于AI-IoT末端节点，它结合了：1）一个16个RISC-V数字信号处理（DSP）核心的通用集群，用于执行各种支持4位和2位算术扩展（XpulpNN）的工作负载，同时结合了融合的MAC和LOAD操作和浮点支持；2）一个2-8位可重构二进制引擎（RBE），用于加速DNN中的3x3和1x1（逐点）卷积；3）一组连接到自适应体偏化的片上监视（OCM）模块。

    Emerging Artificial Intelligence-enabled Internet-of-Things (AI-IoT) System-on-a-Chip (SoC) for augmented reality, personalized healthcare, and nano-robotics need to run many diverse tasks within a power envelope of a few tens of mW over a wide range of operating conditions: compute-intensive but strongly quantized Deep Neural Network (DNN) inference, as well as signal processing and control requiring high-precision floating-point. We present Marsellus, an all-digital heterogeneous SoC for AI-IoT end-nodes fabricated in GlobalFoundries 22nm FDX that combines 1) a general-purpose cluster of 16 RISC-V Digital Signal Processing (DSP) cores attuned for the execution of a diverse range of workloads exploiting 4-bit and 2-bit arithmetic extensions (XpulpNN), combined with fused MAC&LOAD operations and floating-point support; 2) a 2-8bit Reconfigurable Binary Engine (RBE) to accelerate 3x3 and 1x1 (pointwise) convolutions in DNNs; 3) a set of On-Chip Monitoring (OCM) blocks connected to an Ad
    
[^116]: ACTC: 冷启动知识图谱补全的主动阈值校准

    ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion. (arXiv:2305.06395v1 [cs.LG])

    [http://arxiv.org/abs/2305.06395](http://arxiv.org/abs/2305.06395)

    本文提出了一种名为ACTC的方法，在冷启动知识图谱补全时进行主动阈值校准，可以有效地利用有限的标记元组来找到每个关系的最佳阈值，同时也结合未标记元组进行了实验。

    

    自监督的知识图谱补全(KGC)依赖于估计得分模型(实体，关系，实体)-元组，例如，通过嵌入初始知识图。通过调整预测阈值(使用手动注释的示例)，可以改善预测质量。本文尝试首次针对KGC进行冷启动校准，在此过程中初始没有注释的示例，并且只能选择有限数量的元组进行注释。我们的新方法ACTC基于有限的注释元组有效地找到好的每个关系的阈值。除了一些注释的元组外，ACTC还利用Logistic回归或高斯过程分类器估计的未标记元组的正确性。我们还通过密度和随机选择等不同方法选择候选元组进行注释。我们使用五个评分模型和一个oracle注释进行实验。

    Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotat
    
[^117]: 机器学习中的不确定性量化在工程设计与健康预测中的应用：一篇教程

    Uncertainty Quantification in Machine Learning for Engineering Design and Health Prognostics: A Tutorial. (arXiv:2305.04933v1 [cs.LG])

    [http://arxiv.org/abs/2305.04933](http://arxiv.org/abs/2305.04933)

    本文教程介绍机器学习中的不确定性量化方法，帮助提升其安全性和可靠性，进而促进其在高风险决策背景下的广泛应用，特别关注神经网络及其在工程设计和预测健康管理中的应用。

    

    在机器学习模型中，不确定性量化（UQ）作为安全保障的基本层，可以通过启用合理的风险评估和管理来促进更加原则性的决策制定。UQ使得ML模型的安全性和可靠性得到改善，有潜力显著促进高风险决策背景下的ML解决方案的广泛采用，例如医疗保健，制造业和航空等。在本教程中，我们旨在全方位介绍机器学习模型中新兴UQ方法，特别关注神经网络，以及这些UQ方法在解决工程设计和预测健康管理问题方面的应用。为此，我们首先对涉及ML模型UQ的不确定性类型，来源和原因进行了全面分类。接下来，我们提供了几种最先进的UQ方法的教程式描述：高斯过程回归，贝叶斯神经网络。

    On top of machine learning models, uncertainty quantification (UQ) functions as an essential layer of safety assurance that could lead to more principled decision making by enabling sound risk assessment and management. The safety and reliability improvement of ML models empowered by UQ has the potential to significantly facilitate the broad adoption of ML solutions in high-stakes decision settings, such as healthcare, manufacturing, and aviation, to name a few. In this tutorial, we aim to provide a holistic lens on emerging UQ methods for ML models with a particular focus on neural networks and the applications of these UQ methods in tackling engineering design as well as prognostics and health management problems. Toward this goal, we start with a comprehensive classification of uncertainty types, sources, and causes pertaining to UQ of ML models. Next, we provide a tutorial-style description of several state-of-the-art UQ methods: Gaussian process regression, Bayesian neural network
    
[^118]: 多头图卷积网络在结构连接组分类中的应用

    Multi-Head Graph Convolutional Network for Structural Connectome Classification. (arXiv:2305.02199v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.02199](http://arxiv.org/abs/2305.02199)

    本文提出了一种基于多头图卷积网络的机器学习模型，用于对大脑连接组进行分类。该模型通过不同的图卷积头部，包括边缘和节点，全面捕捉输入数据的表示。实验结果表明，该模型在性别分类任务上表现最佳，并能从连接组数据中提取互补和代表性特征。

    

    本文致力于基于扩散磁共振图像提取的大脑连接数据的分类。我们提出了一种基于图卷积网络（GCN）的机器学习模型，该模型采用多头并行GCN机制，分别对大脑连接输入图进行处理。所提出的网络是一种简单的设计，采用不同的头部，涉及边缘和节点的图卷积，充分捕捉输入数据的表示。为了测试我们的模型从大脑连接数据中提取互补和代表性特征的能力，我们选择了性别分类任务。这表征了连接组在性别方面的变化程度，这对于提高我们对两性健康和疾病理解至关重要。本文在两个公开数据集PREVENT-AD（347个受试者）和OASIS3（771个受试者）上进行实验。结果表明，所提出的模型表现最佳。

    We tackle classification based on brain connectivity derived from diffusion magnetic resonance images. We propose a machine-learning model inspired by graph convolutional networks (GCNs), which takes a brain connectivity input graph and processes the data separately through a parallel GCN mechanism with multiple heads. The proposed network is a simple design that employs different heads involving graph convolutions focused on edges and nodes, capturing representations from the input data thoroughly. To test the ability of our model to extract complementary and representative features from brain connectivity data, we chose the task of sex classification. This quantifies the degree to which the connectome varies depending on the sex, which is important for improving our understanding of health and disease in both sexes. We show experiments on two publicly available datasets: PREVENT-AD (347 subjects) and OASIS3 (771 subjects). The proposed model demonstrates the highest performance compa
    
[^119]: DeepAqua:使用知识蒸馏方法从SAR图像自我监督分割湿地的语义信息

    DeepAqua: Self-Supervised Semantic Segmentation of Wetlands from SAR Images using Knowledge Distillation. (arXiv:2305.01698v1 [cs.CV])

    [http://arxiv.org/abs/2305.01698](http://arxiv.org/abs/2305.01698)

    本文提出了DeepAqua，一种利用自我监督深度学习模型，使用知识蒸馏技术来从合成孔径雷达图像中分割水域的方法。该方法不需要手动注释，可用于大规模监测湿地变化。

    

    遥感技术通过将语义分割技术应用于卫星图像已经显著提高了水的检测能力。然而，由于需要大量的注释数据，语义分割仍然是具有挑战性的。这在湿地检测方面尤其困难，因为水的范围随时间和空间的变化而变化，需要对同一区域进行多次注释。本文提出了DeepAqua，这是一种自我监督的深度学习模型，利用知识蒸馏方法在训练阶段消除了手动注释的需求。DeepAqua利用归一化差异水指数（NDWI）作为教师模型，训练卷积神经网络（CNN）以分割合成孔径雷达（SAR）图像中的水。为训练学生模型，我们利用光学和雷达水质掩蔽相重叠的情况，实现了开放及有植被水面的检测。我们的模型在湿地检测领域具有显著的计算机视觉技术进步，可用于大规模监测湿地变化。

    Remote sensing has significantly advanced water detection by applying semantic segmentation techniques to satellite imagery. However, semantic segmentation remains challenging due to the substantial amount of annotated data required. This is particularly problematic in wetland detection, where water extent varies over time and space, necessitating multiple annotations for the same area. In this paper, we present DeepAqua, a self-supervised deep learning model that leverages knowledge distillation to eliminate the need for manual annotations during the training phase. DeepAqua utilizes the Normalized Difference Water Index (NDWI) as a teacher model to train a Convolutional Neural Network (CNN) for segmenting water from Synthetic Aperture Radar (SAR) images. To train the student model, we exploit cases where optical- and radar-based water masks coincide, enabling the detection of both open and vegetated water surfaces. Our model represents a significant advancement in computer vision tec
    
[^120]: 一个统一的主动学习框架，用于注释图形数据并应用于软件代码性能预测

    A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction. (arXiv:2304.13032v1 [cs.SE])

    [http://arxiv.org/abs/2304.13032](http://arxiv.org/abs/2304.13032)

    提出了一个针对软件性能预测的主动学习框架，通过将源代码解析成流增强抽象语法树图形式，构造各种无监督和有监督的图嵌入，进行主动学习，实现对未标注数据的高效利用，并比现有方法更加优越。

    

    大多数机器学习和数据分析应用程序，包括软件系统中的性能工程，需要大量注释和标记数据，这些可能事先并不可用。获取注释通常需要显着的时间、精力和计算资源，这使得任务变得具有挑战性。我们开发了一个统一的主动学习框架，专门针对软件性能预测来解决这个任务。我们从将源代码解析成抽象语法树（AST）开始，然后用数据和控制流边来增强它。然后，我们将源代码的树形表示转换为流增强抽象语法树图（FA-AST）表示法。基于图形表示，我们将各种图嵌入（无监督和有监督）构造成一个潜在空间。鉴于这样的嵌入，该框架变得任务不可知，因为可以使用任何回归方法和适用于回归的查询策略来执行主动学习。在该框架内，我们引入并评估了几种用于软件性能预测的主动学习查询策略和回归算法，证明了我们的方法优于现有方法。

    Most machine learning and data analytics applications, including performance engineering in software systems, require a large number of annotations and labelled data, which might not be available in advance. Acquiring annotations often requires significant time, effort, and computational resources, making it challenging. We develop a unified active learning framework, specializing in software performance prediction, to address this task. We begin by parsing the source code to an Abstract Syntax Tree (AST) and augmenting it with data and control flow edges. Then, we convert the tree representation of the source code to a Flow Augmented-AST graph (FA-AST) representation. Based on the graph representation, we construct various graph embeddings (unsupervised and supervised) into a latent space. Given such an embedding, the framework becomes task agnostic since active learning can be performed using any regression method and query strategy suited for regression. Within this framework, we in
    
[^121]: 内在上下文算子学习用于微分方程问题

    In-Context Operator Learning for Differential Equation Problems. (arXiv:2304.07993v1 [cs.LG])

    [http://arxiv.org/abs/2304.07993](http://arxiv.org/abs/2304.07993)

    本文提出了一种新的神经网络方法INDEED，它可以同时学习不同微分方程问题的操作符，而无需重新训练，且只需要极少的演示。

    

    本文介绍了一种新的基于神经网络的方法——IN-context Differential Equation Encoder-Decoder（INDEED），用于从数据中同时学习操作符并在推理阶段将其应用于新问题，而无需进行任何权重更新。现有方法局限于使用神经网络来逼近特定的方程解或特定的操作符，需要重新训练来处理具有不同方程的新问题。通过训练单个神经网络作为操作符学习器，我们不仅可以摆脱为新问题重新训练（甚至微调）神经网络的困扰，还可以利用操作符之间共享的共同点，这样在学习新的操作符时只需要极少的演示即可。我们的数值结果显示了神经网络作为少样本学习器的能力，用于各种不同类型的微分方程问题，包括ODE和PDE的正向和反向问题，同时显示它可以推广学习能力。

    This paper introduces a new neural-network-based approach, namely IN-context Differential Equation Encoder-Decoder (INDEED), to simultaneously learn operators from data and apply it to new questions during the inference stage, without any weight update. Existing methods are limited to using a neural network to approximate a specific equation solution or a specific operator, requiring retraining when switching to a new problem with different equations. By training a single neural network as an operator learner, we can not only get rid of retraining (even fine-tuning) the neural network for new problems, but also leverage the commonalities shared across operators so that only a few demos are needed when learning a new operator. Our numerical results show the neural network's capability as a few-shot operator learner for a diversified type of differential equation problems, including forward and inverse problems of ODEs and PDEs, and also show that it can generalize its learning capabilit
    
[^122]: BoundaryCAM：一种基于边界的弱监督医学图像语义分割优化框架

    BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised Semantic Segmentation of Medical Images. (arXiv:2303.07853v1 [cs.CV])

    [http://arxiv.org/abs/2303.07853](http://arxiv.org/abs/2303.07853)

    BoundaryCAM提出了一种基于边界的弱监督的优化框架，能够预测对象位置，实现精细的高精度分割掩模。

    

    仅利用图像级别监督的弱监督语义分割（WSSS）是解决分割网络需求的一种有前途的方法，尤其是对于在给定数据集中生成大量像素级掩模。然而，大多数最先进的图像级WSSS技术缺乏对图像中包含的几何特征的理解，因为网络无法从仅图像级别标签中导出任何对象边界信息。为了解决这个缺陷，我们提出了我们的新型BoundaryCAM框架，该框架采用最先进的类激活图结合各种后处理技术，以实现精细的高精度分割掩模。为了实现这一目标，我们调查了一种最先进的无监督语义分割网络，该网络可用于构建边界图，以使BoundaryCAM能够高精度预测对象位置。

    Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision is a promising approach to deal with the need for Segmentation networks, especially for generating a large number of pixel-wise masks in a given dataset. However, most state-of-the-art image-level WSSS techniques lack an understanding of the geometric features embedded in the images since the network cannot derive any object boundary information from just image-level labels. We define a boundary here as the line separating an object and its background, or two different objects. To address this drawback, we propose our novel BoundaryCAM framework, which deploys state-of-the-art class activation maps combined with various post-processing techniques in order to achieve fine-grained higher-accuracy segmentation masks. To achieve this, we investigate a state-of-the-art unsupervised semantic segmentation network that can be used to construct a boundary map, which enables BoundaryCAM to predict object locations w
    
[^123]: 标题：标签噪音存在下的深度主动学习综述

    Deep Active Learning in the Presence of Label Noise: A Survey. (arXiv:2302.11075v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11075](http://arxiv.org/abs/2302.11075)

    深度主动学习在处理带有噪音标签的分类数据集时面临着困难。本文综述了在这种情况下深度主动学习的现状和方法，并提出了使用视觉Transformer和对比学习方法来提高标记样本选择的可能性。

    

    深度主动学习已经成为在预定义标签预算内训练深度学习模型的强大工具。这些模型的性能与在离线设置下进行训练的模型相当。然而，当处理包含噪音标签的分类数据集时，深度主动学习面临重大问题。在这篇文献综述中，我们讨论了标签噪音存在下的深度主动学习的现状，重点介绍了独特的方法以及它们的优势和弱点。随着视觉Transformer在图像分类任务中的最近成功，我们提供了一个简要的概述，并考虑如何使用Transformer层和注意力机制来增强多样性、重要性和基于不确定性的查询选择，以发送给标签预测师。我们进一步提出探索对比学习方法，以得到能够在主动学习环境中选择高价值样本的良好图像表示。我们还强调了解了该领域的最新发展，并提出了未来的研究方向。

    Deep active learning has emerged as a powerful tool for training deep learning models within a predefined labeling budget. These models have achieved performances comparable to those trained in an offline setting. However, deep active learning faces substantial issues when dealing with classification datasets containing noisy labels. In this literature review, we discuss the current state of deep active learning in the presence of label noise, highlighting unique approaches, their strengths, and weaknesses. With the recent success of vision transformers in image classification tasks, we provide a brief overview and consider how the transformer layers and attention mechanisms can be used to enhance diversity, importance, and uncertainty-based selection in queries sent to an oracle for labeling. We further propose exploring contrastive learning methods to derive good image representations that can aid in selecting high-value samples for labeling in an active learning setting. We also hig
    
[^124]: 多校准的统一视角: 多目标学习的游戏动力学

    A Unifying Perspective on Multi-Calibration: Game Dynamics for Multi-Objective Learning. (arXiv:2302.10863v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10863](http://arxiv.org/abs/2302.10863)

    本论文提出了一个统一的框架，通过将多校准问题放置在多目标学习的背景下，并利用游戏动力学的连接，实现了一系列多校准学习问题的最新保证，同时也改进了现有保证并简化了分析。

    

    我们提供了一个统一的框架，用于设计和分析多校准的预测器。将多校准问题放置在多目标学习的一般背景下，我们利用与游戏动力学的联系，在多样的多校准学习问题中实现了最先进的保证。除了阐明现有多校准保证并极大简化它们的分析，我们的方法还产生了改进的保证，例如获得随着群体规模的平方根缩放的强多校准条件，并将$ k $-类多校准的复杂性提高了$ k $的指数因子。除了多校准，我们还利用这些游戏动力学来解决在群体公平和多分布学习研究中出现的考虑问题。

    We provide a unifying framework for the design and analysis of multicalibrated predictors. By placing the multicalibration problem in the general setting of multi-objective learning -- where learning guarantees must hold simultaneously over a set of distributions and loss functions -- we exploit connections to game dynamics to achieve state-of-the-art guarantees for a diverse set of multicalibration learning problems. In addition to shedding light on existing multicalibration guarantees and greatly simplifying their analysis, our approach also yields improved guarantees, such as obtaining stronger multicalibration conditions that scale with the square-root of group size and improving the complexity of $k$-class multicalibration by an exponential factor of $k$. Beyond multicalibration, we use these game dynamics to address emerging considerations in the study of group fairness and multi-distribution learning.
    
[^125]: 通过基于聚类的方法支持图像处理DNN的安全分析

    Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches. (arXiv:2301.13506v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2301.13506](http://arxiv.org/abs/2301.13506)

    本文通过不同的分析管道对DNN失效的根本原因进行了实证评估，结果显示最佳管道结合了迁移学习，DBSCAN和UMAP，并几乎只产生了一个簇群。

    

    在安全关键的环境中，由于缺乏有效的手段解释其结果（尤其是当结果错误时），深度神经网络（DNN）往往无法应用。在我们之前的工作中，我们提出了一种白盒方法（HUDD）和一种黑盒方法（SAFE）来自动化表征DNN的失效情况。它们都可以从一组潜在庞大的导致DNN失效的图像中识别相似的图像集群。然而，HUDD和SAFE的分析管道是按照常规实践实例化的，将其他管道的分析推迟到将来的工作中。在本文中，我们报告了对99种不同的用于根本原因分析DNN失效的管道进行的实证评估结果。它们结合了迁移学习，自动编码器，神经元相关性的热图，降维技术和不同的聚类算法。我们的结果表明，最佳的管道是结合迁移学习，DBSCAN和UMAP，几乎只产生了簇群。

    The adoption of deep neural networks (DNNs) in safety-critical contexts is often prevented by the lack of effective means to explain their results, especially when they are erroneous. In our previous work, we proposed a white-box approach (HUDD) and a black-box approach (SAFE) to automatically characterize DNN failures. They both identify clusters of similar images from a potentially large set of images leading to DNN failures. However, the analysis pipelines for HUDD and SAFE were instantiated in specific ways according to common practices, deferring the analysis of other pipelines to future work. In this paper, we report on an empirical evaluation of 99 different pipelines for root cause analysis of DNN failures. They combine transfer learning, autoencoders, heatmaps of neuron relevance, dimensionality reduction techniques, and different clustering algorithms. Our results show that the best pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters almost exclusively
    
[^126]: RouteNet-Fermi: 使用图神经网络进行网络建模

    RouteNet-Fermi: Network Modeling with Graph Neural Networks. (arXiv:2212.12070v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2212.12070](http://arxiv.org/abs/2212.12070)

    RouteNet-Fermi是一种基于图神经网络的自定义模型，与传统的排队理论相比，在存在真实流量模型的情况下具有更高的准确性。它能够准确预测网络的延迟、抖动和丢包情况。

    

    网络模型是现代网络的重要组成部分，广泛用于网络规划和优化。然而，随着网络规模和复杂性的增加，一些模型存在限制，如排队理论模型中对马尔可夫流量的假设，以及网络模拟器的高计算成本。机器学习的最新进展，如图神经网络（GNN），正在推动一代新的数据驱动网络模型，能够学习复杂的非线性行为。在本文中，我们提出了一种名为RouteNet-Fermi的自定义GNN模型，它与排队理论具有相同的目标，并且在存在真实流量模型的情况下准确性更高。该模型可以准确预测网络的延迟、抖动和丢包情况。我们在不断增长的网络规模（最大达到300个节点）和包括具有混合流量特性的样本（如复杂的非马尔可夫模型）中测试了RouteNet-Fermi模型。

    Network models are an essential block of modern networks. For example, they are widely used in network planning and optimization. However, as networks increase in scale and complexity, some models present limitations, such as the assumption of Markovian traffic in queuing theory models, or the high computational cost of network simulators. Recent advances in machine learning, such as Graph Neural Networks (GNN), are enabling a new generation of network models that are data-driven and can learn complex non-linear behaviors. In this paper, we present RouteNet-Fermi, a custom GNN model that shares the same goals as Queuing Theory, while being considerably more accurate in the presence of realistic traffic models. The proposed model predicts accurately the delay, jitter, and packet loss of a network. We have tested RouteNet-Fermi in networks of increasing size (up to 300 nodes), including samples with mixed traffic profiles -- e.g., with complex non-Markovian models -- and arbitrary routin
    
[^127]: 统计和计算保证了影响诊断的有效性

    Statistical and Computational Guarantees for Influence Diagnostics. (arXiv:2212.04014v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.04014](http://arxiv.org/abs/2212.04014)

    本论文提出了统计和计算边界来保证影响诊断的有效性，通过使用高效的逆-Hessian-向量乘积实现的方法，我们可以在机器学习和人工智能领域的应用中准确识别有影响力的数据点或子集。

    

    影响诊断，如影响函数和近似最大影响扰动，在机器学习和人工智能领域的应用中很受欢迎。影响诊断是用于识别有影响力的数据点或数据子集的强大统计工具。我们使用高效的逆-Hessian-向量乘积实现建立了影响函数和近似最大影响扰动的有限样本统计界限和计算复杂度界限。我们使用广义线性模型和基于大规模注意力机制的模型在合成和真实数据上进行了实验验证。

    Influence diagnostics such as influence functions and approximate maximum influence perturbations are popular in machine learning and in AI domain applications. Influence diagnostics are powerful statistical tools to identify influential datapoints or subsets of datapoints. We establish finite-sample statistical bounds, as well as computational complexity bounds, for influence functions and approximate maximum influence perturbations using efficient inverse-Hessian-vector product implementations. We illustrate our results with generalized linear models and large attention based models on synthetic and real data.
    
[^128]: 边缘视频分析：应用、系统和支持技术综述

    Edge Video Analytics: A Survey on Applications, Systems and Enabling Techniques. (arXiv:2211.15751v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2211.15751](http://arxiv.org/abs/2211.15751)

    这项研究综述了边缘视频分析（EVA）的应用、系统和支持技术。随着深度学习的发展和互联设备的普及，EVA作为一种在网络边缘处理视频数据的解决方案受到越来越多的关注。

    

    视频作为全球数字信息爆炸的关键驱动力，可以为人类社会带来巨大的好处。政府和企业正在部署无数摄像头用于各种应用，例如执法、应急管理、交通控制和安全监控，这些都是由视频分析（VA）所支持。深度学习的快速发展使得对象分类、检测和跟踪的模型更加精确。同时，随着互联设备的广泛使用，每天都会产生海量数据，超越了云的处理能力。边缘计算是一种新兴的范式，它将工作负载和服务从网络核心移动到网络边缘，被广泛认为是一个有希望的解决方案。由此产生的新交叉领域，即边缘视频分析（EVA），开始引起广泛关注。然而，在这个领域中只有少数松散相关的综述存在。EVA的基本概念（例如

    Video, as a key driver in the global explosion of digital information, can create tremendous benefits for human society. Governments and enterprises are deploying innumerable cameras for a variety of applications, e.g., law enforcement, emergency management, traffic control, and security surveillance, all facilitated by video analytics (VA). This trend is spurred by the rapid advancement of deep learning (DL), which enables more precise models for object classification, detection, and tracking. Meanwhile, with the proliferation of Internet-connected devices, massive amounts of data are generated daily, overwhelming the cloud. Edge computing, an emerging paradigm that moves workloads and services from the network core to the network edge, has been widely recognized as a promising solution. The resulting new intersection, edge video analytics (EVA), begins to attract widespread attention. Nevertheless, only a few loosely-related surveys exist on this topic. The basic concepts of EVA (e.g
    
[^129]: 迈向无限自学习的并行自适应退火的MCMC方法

    Toward Unlimited Self-Learning MCMC with Parallel Adaptive Annealing. (arXiv:2211.14024v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14024](http://arxiv.org/abs/2211.14024)

    提出了一种并行自适应退火的方法，将自学习Monte Carlo（SLMC）方法应用于多峰分布，实现了高效的Monte Carlo更新，并解决了建议模型的模式崩溃问题。

    

    自学习Monte Carlo（SLMC）方法是最近提出的使用机器学习模型加速Markov链Monte Carlo（MCMC）方法的方法。通过潜在生成模型，SLMC方法实现了具有较少自相关性的高效Monte Carlo更新。然而，SLMC方法难以直接应用于多峰分布，其中难以获得训练数据。为了解决这个限制，我们提出了并行自适应退火方法，它使SLMC方法直接应用于具有逐渐训练的建议分布的多峰分布。并行自适应退火基于（i）带有退火的顺序学习，以继承和更新模型参数，（ii）自适应退火，以自动检测欠学习，（iii）并行退火，以减轻建议模型的模式崩溃。我们还提出了使用变分自动编码器（VAE）作为SLMC建议的VAE-SLMC方法，以实现高效的并行建议。

    Self-learning Monte Carlo (SLMC) methods are recently proposed to accelerate Markov chain Monte Carlo (MCMC) methods using a machine learning model. With latent generative models, SLMC methods realize efficient Monte Carlo updates with less autocorrelation. However, SLMC methods are difficult to directly apply to multimodal distributions for which training data are difficult to obtain. To solve the limitation, we propose parallel adaptive annealing, which makes SLMC methods directly apply to multimodal distributions with a gradually trained proposal while annealing target distribution. Parallel adaptive annealing is based on (i) sequential learning with annealing to inherit and update the model parameters, (ii) adaptive annealing to automatically detect under-learning, and (iii) parallel annealing to mitigate mode collapse of proposal models. We also propose VAE-SLMC method which utilizes a variational autoencoder (VAE) as a proposal of SLMC to make efficient parallel proposals indepen
    
[^130]: 通过运动和物体连续性提升物体表示学习

    Boosting Object Representation Learning via Motion and Object Continuity. (arXiv:2211.09771v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.09771](http://arxiv.org/abs/2211.09771)

    通过集成运动和连续性信息，我们提出了一种提升物体表示学习的方法，可以在无监督多对象检测模型中产生更优的物体编码，并在物体发现、收敛速度和总体潜在物体表示等方面取得明显的改进。

    

    最近的无监督多对象检测模型在性能方面取得了令人印象深刻的改进，这主要归因于新颖的架构偏见。不幸的是，它们可能为下游任务产生次优的物体编码。为了克服这一问题，我们提出利用物体运动和连续性，即物体不能突然出现和消失。这通过两个机制实现：（i）通过集成光流提供物体位置的先验知识，（ii）对连续图像帧之间的对比物体连续性损失进行建模。与开发显式深度架构不同，得到的运动和物体连续性（MOC）方案可以使用任何基线对象检测模型来实现。我们的结果表明，相对于玩Atari游戏而言，SOTA模型在物体发现、收敛速度和总体潜在物体表示方面都有很大的改进。总的来说，我们展示了整合运动和连续性的明显好处。

    Recent unsupervised multi-object detection models have shown impressive performance improvements, largely attributed to novel architectural inductive biases. Unfortunately, they may produce suboptimal object encodings for downstream tasks. To overcome this, we propose to exploit object motion and continuity, i.e., objects do not pop in and out of existence. This is accomplished through two mechanisms: (i) providing priors on the location of objects through integration of optical flow, and (ii) a contrastive object continuity loss across consecutive image frames. Rather than developing an explicit deep architecture, the resulting Motion and Object Continuity (MOC) scheme can be instantiated using any baseline object detection model. Our results show large improvements in the performances of a SOTA model in terms of object discovery, convergence speed and overall latent object representations, particularly for playing Atari games. Overall, we show clear benefits of integrating motion and
    
[^131]: DyG2Vec: 带有自监督的动态图表征学习

    DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision. (arXiv:2210.16906v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16906](http://arxiv.org/abs/2210.16906)

    DyG2Vec是一个具有自监督学习能力的动态图表征学习模型，采用了高效而有效的注意力编码器和非对比自监督学习方法，能够提取丰富的时间嵌入表示。在基准数据集上的实验结果表明，该模型在未来链接预测任务中表现出色。

    

    时间图神经网络已经展示出在通过自动提取时间模式来学习归纳表示方面的有希望结果。然而，以往的工作常常依赖于复杂的记忆模块或低效的随机游走方法来构建时间表示。此外，现有的动态图编码器不容易适应自监督范式，这阻碍了它们利用无标签数据。为了解决这些限制，我们提出了一种高效而有效的基于注意力的编码器，利用时间边编码和基于窗口的子图采样来生成任务无关的嵌入。此外，我们提出了一种使用非对比SSL的联合嵌入架构，以学习丰富的时间嵌入而不需要标签。在7个基准数据集上的实验结果表明，我们的模型在传导设置和归纳设置的未来链接预测任务中，平均优于现有的SoTA基线4.23％和3.30％。

    Temporal graph neural networks have shown promising results in learning inductive representations by automatically extracting temporal patterns. However, previous works often rely on complex memory modules or inefficient random walk methods to construct temporal representations. In addition, the existing dynamic graph encoders are non-trivial to adapt to self-supervised paradigms, which prevents them from utilizing unlabeled data. To address these limitations, we present an efficient yet effective attention-based encoder that leverages temporal edge encodings and window-based subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-embedding architecture using non-contrastive SSL to learn rich temporal embeddings without labels. Experimental results on 7 benchmark datasets indicate that on average, our model outperforms SoTA baselines on the future link prediction task by 4.23% for the transductive setting and 3.30% for the inductive setting while only requi
    
[^132]: 图模糊系统: 概念、模型和算法

    Graph Fuzzy System: Concepts, Models and Algorithms. (arXiv:2210.16730v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.16730](http://arxiv.org/abs/2210.16730)

    本文提出了一种适用于图数据建模的新型模糊系统，名为图模糊系统（GFS），通过定义相关概念、构建模型框架和算法，实现了在处理具有非欧几里得结构的图数据时保留模糊系统优势的目标。

    

    模糊系统在各个领域都有广泛的应用，包括模式识别、智能控制、数据挖掘和生物信息学，这归功于其强大的解释和学习能力。在传统的应用场景中，模糊系统主要用于建模欧几里得空间数据，无法处理非欧几里得结构的图数据，比如社交网络和交通路线图。因此，开发适用于图数据并能保留传统模糊系统优势的建模方法是一个重要的研究课题。为了应对这个挑战，本文提出了一种适用于图数据建模的新型模糊系统，称为图模糊系统（GFS），并系统地开发了相关概念、建模框架和构建算法。首先，定义了GFS相关概念，包括图模糊规则库、图模糊集和图后件处理单元（GCPU）。然后构建了一个GFS建模框架。

    Fuzzy systems (FSs) have enjoyed wide applications in various fields, including pattern recognition, intelligent control, data mining and bioinformatics, which is attributed to the strong interpretation and learning ability. In traditional application scenarios, FSs are mainly applied to model Euclidean space data and cannot be used to handle graph data of non-Euclidean structure in nature, such as social networks and traffic route maps. Therefore, development of FS modeling method that is suitable for graph data and can retain the advantages of traditional FSs is an important research. To meet this challenge, a new type of FS for graph data modeling called Graph Fuzzy System (GFS) is proposed in this paper, where the concepts, modeling framework and construction algorithms are systematically developed. First, GFS related concepts, including graph fuzzy rule base, graph fuzzy sets and graph consequent processing unit (GCPU), are defined. A GFS modeling framework is then constructed and
    
[^133]: 通过使用Cover Trees的最小间隔实现数值稳定的稀疏高斯过程

    Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees. (arXiv:2210.07893v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.07893](http://arxiv.org/abs/2210.07893)

    本文针对高斯过程模型的数值稳定性进行了研究，通过感兴趣点的选择和计算，提供了稳定可靠的稀疏逼近方法。

    

    高斯过程常用于较大的机器学习和决策系统中，例如地理空间建模、贝叶斯优化或潜在高斯模型中。在一个系统中，高斯过程模型需要以稳定可靠的方式运行，以确保与系统的其他部分正确交互。本文研究了基于感兴趣点的可扩展稀疏逼近的数值稳定性。为此，我们首先回顾了数值稳定性，并阐述了高斯过程模型可能不稳定的典型情况。在插值文献中原始开发的稳定性理论的基础上，我们导出了对感兴趣点进行计算的数值稳定性的充分条件和某些情况下的必要条件。对于地理空间建模等低维任务，我们提出了一种自动计算满足这些条件的感兴趣点的方法。

    Gaussian processes are frequently deployed as part of larger machine learning and decision-making systems, for instance in geospatial modeling, Bayesian optimization, or in latent Gaussian models. Within a system, the Gaussian process model needs to perform in a stable and reliable manner to ensure it interacts correctly with other parts of the system. In this work, we study the numerical stability of scalable sparse approximations based on inducing points. To do so, we first review numerical stability, and illustrate typical situations in which Gaussian process models can be unstable. Building on stability theory originally developed in the interpolation literature, we derive sufficient and in certain cases necessary conditions on the inducing points for the computations performed to be numerically stable. For low-dimensional tasks such as geospatial modeling, we propose an automated method for computing inducing points satisfying these conditions. This is done via a modification of t
    
[^134]: Conformal Prediction对分散标签噪声具有稳健性

    Conformal Prediction is Robust to Dispersive Label Noise. (arXiv:2209.14295v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14295](http://arxiv.org/abs/2209.14295)

    本研究研究了Conformal Prediction方法对于标签噪声具有鲁棒性。我们找出了构建可以正确覆盖无噪声真实标签的不确定性集合的条件，并提出了对具有噪声标签的一般损失函数进行正确控制的要求。实验证明，在对抗性案例之外，使用Conformal Prediction和风险控制技术可以实现对干净真实标签的保守风险。我们还提出了一种有界尺寸噪声修正的方法，以确保实现正确的真实标签风险。

    

    我们研究了对标签噪声具有鲁棒性的Conformal Prediction方法，该方法是一种用于不确定性量化的强大工具。我们的分析涵盖了回归和分类问题，对于如何构建能够正确覆盖未观察到的无噪声真实标签的不确定性集合进行了界定。我们进一步扩展了我们的理论，并提出了对于带有噪声标签正确控制一般损失函数（如假阴性比例）的要求。我们的理论和实验表明，在带有噪声标签的情况下，Conformal Prediction和风险控制技术能够实现对干净真实标签的保守风险，除了在对抗性案例中。在这种情况下，我们还可以通过对Conformal Prediction算法进行有界尺寸的噪声修正，以确保实现正确的真实标签风险，而无需考虑分数或数据的规则性。

    We study the robustness of conformal prediction, a powerful tool for uncertainty quantification, to label noise. Our analysis tackles both regression and classification problems, characterizing when and how it is possible to construct uncertainty sets that correctly cover the unobserved noiseless ground truth labels. We further extend our theory and formulate the requirements for correctly controlling a general loss function, such as the false negative proportion, with noisy labels. Our theory and experiments suggest that conformal prediction and risk-controlling techniques with noisy labels attain conservative risk over the clean ground truth labels except in adversarial cases. In such cases, we can also correct for noise of bounded size in the conformal prediction algorithm in order to ensure achieving the correct risk of the ground truth labels without score or data regularity.
    
[^135]: 基于大规模注释数据库的深度伪造检测 AI 偏差的全面分析

    A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated Databases. (arXiv:2208.05845v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.05845](http://arxiv.org/abs/2208.05845)

    本研究通过提供包含47种不同属性注释的大规模数据集，并对三种最先进的Deepfake检测模型进行全面分析，旨在研究公共Deepfake数据集可能带来的AI偏差问题

    

    近年来，Deepfake 对图像和视频的篡改已经成为安全和社会的严重关注点。许多检测模型和数据集已经被提出，可靠地检测 Deepfake 数据。然而，人们越来越担心这些模型和训练数据集可能存在偏差，从而导致 Deepfake 检测器失效。本研究通过提供五个流行的 Deepfake 数据集中 47 种不同属性的大规模人口统计和非人口统计属性注释，并全面分析三种最先进的 Deepfake 检测模型对这些数据集的 AI 偏差问题，调查研究了超过 6500 万个标签的许多不同属性（包括人口统计学（年龄、性别、种族）和非人口统计学（头发、皮肤、配饰等）信息对检测性能的影响。结果表明，调查的数据库缺乏多样性，可能导致 AI 偏差。

    In recent years, image and video manipulations with Deepfake have become a severe concern for security and society. Many detection models and datasets have been proposed to detect Deepfake data reliably. However, there is an increased concern that these models and training databases might be biased and, thus, cause Deepfake detectors to fail. In this work, we investigate the bias issue caused by public Deepfake datasets by (a) providing large-scale demographic and non-demographic attribute annotations of 47 different attributes for five popular Deepfake datasets and (b) comprehensively analysing AI-bias of three state-of-the-art Deepfake detection backbone models on these datasets. The investigation analyses the influence of a large variety of distinctive attributes (from over 65M labels) on the detection performance, including demographic (age, gender, ethnicity) and non-demographic (hair, skin, accessories, etc.) information. The results indicate that investigated databases lack dive
    
[^136]: 通过NeurVec加速大规模动态系统数值求解器的模拟

    Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec. (arXiv:2208.03680v2 [cs.CE] UPDATED)

    [http://arxiv.org/abs/2208.03680](http://arxiv.org/abs/2208.03680)

    通过NeurVec，我们提出了一种基于深度学习的修正器，它能够补偿集成误差并在模拟中实现更大的时间步长。NeurVec在复杂动态系统中表现出了显著的泛化能力，加速了传统求解器，同时保持了高水平的准确性和稳定性。

    

    在众多科学和工程领域中，大规模动态系统的模拟至关重要。然而，传统的数值求解器在估计积分时由于步长选择的限制，存在着精度和计算效率之间的权衡。为了解决这个挑战，我们引入了一种基于深度学习的修正器，称为NeurVec，它可以补偿集成误差并在模拟中实现更大的时间步长。我们在各种复杂动态系统基准上进行了大量实验证明，即使在使用有限和离散数据进行训练时，NeurVec在连续相空间上表现出了显著的泛化能力。NeurVec显著加速了传统求解器，实现了几十到几百倍的速度提升，同时保持了高水平的准确性和稳定性。此外，NeurVec的简单而有效的设计结合了易于实现的特点，有潜力建立起一个新的求解器范式。

    The large-scale simulation of dynamical systems is critical in numerous scientific and engineering disciplines. However, traditional numerical solvers are limited by the choice of step sizes when estimating integration, resulting in a trade-off between accuracy and computational efficiency. To address this challenge, we introduce a deep learning-based corrector called Neural Vector (NeurVec), which can compensate for integration errors and enable larger time step sizes in simulations. Our extensive experiments on a variety of complex dynamical system benchmarks demonstrate that NeurVec exhibits remarkable generalization capability on a continuous phase space, even when trained using limited and discrete data. NeurVec significantly accelerates traditional solvers, achieving speeds tens to hundreds of times faster while maintaining high levels of accuracy and stability. Moreover, NeurVec's simple-yet-effective design, combined with its ease of implementation, has the potential to establi
    
[^137]: 机器教学的黑盒泛化性能

    Black-box Generalization of Machine Teaching. (arXiv:2206.15205v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.15205](http://arxiv.org/abs/2206.15205)

    本文提出了一种黑盒教学假设$h^\mathcal{T}$，通过引入更紧密的松弛项，可以改进主动学习中的假设修剪方法。理论上证明，在这个教学假设的指导下，学习者可以达到比无指导的学习者更好的泛化误差和标记复杂度界限。

    

    假设修剪最大化用于主动学习的假设更新，以找到期望的未标记数据。其固有的假设是，这种学习方式可以将这些更新导出为最优假设。然而，如果这些递增的更新是负的和无序的，它的收敛性可能无法很好地保证。在本文中，我们引入了一个采用更紧密的松弛项$\left(1+\mathcal{F}^{\mathcal{T}}(\widehat{h}_t)\right)\Delta_t$来替代修剪的典型$2\Delta_t$的黑盒教学假设$h^\mathcal{T}$。从理论上讲，我们证明，在这个教学假设的指导下，学习者可以收敛到比那些没有从教师那里得到任何指导的非教育学习者更紧密的泛化误差和标记复杂度界限：1）泛化误差的上界可以从$R(h^*)+4\Delta_{T-1}$减少到大约$R(h^{\mathcal{T}})+2\Delta_{T-1}$，2）标记复杂度的上界可以从$4 \theta

    Hypothesis-pruning maximizes the hypothesis updates for active learning to find those desired unlabeled data. An inherent assumption is that this learning manner can derive those updates into the optimal hypothesis. However, its convergence may not be guaranteed well if those incremental updates are negative and disordered. In this paper, we introduce a black-box teaching hypothesis $h^\mathcal{T}$ employing a tighter slack term $\left(1+\mathcal{F}^{\mathcal{T}}(\widehat{h}_t)\right)\Delta_t$ to replace the typical $2\Delta_t$ for pruning. Theoretically, we prove that, under the guidance of this teaching hypothesis, the learner can converge into a tighter generalization error and label complexity bound than those non-educated learners who do not receive any guidance from a teacher:1) the generalization error upper bound can be reduced from $R(h^*)+4\Delta_{T-1}$ to approximately $R(h^{\mathcal{T}})+2\Delta_{T-1}$, and 2) the label complexity upper bound can be decreased from $4 \theta
    
[^138]: 用图神经网络评估电网拓扑的动态稳定性

    Toward Dynamic Stability Assessment of Power Grid Topologies using Graph Neural Networks. (arXiv:2206.06369v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06369](http://arxiv.org/abs/2206.06369)

    本研究通过使用图神经网络（GNN）分析电网的动态稳定性，生成了新的大型数据集，并证明了GNN在仅利用拓扑信息的情况下能够有效预测非线性目标，同时能够准确识别电网中的脆弱节点。

    

    为了应对气候变化，可再生能源在电力生产中的份额需要增加。可再生能源引入了新的挑战，涉及到电网的动态稳定性，包括分散化、降低的惯性和生产的波动性。由于对于大型电网而言，动态稳定性模拟是棘手且极其昂贵的，图神经网络（GNN）是一个有希望的方法，可以减少分析电网动态稳定性的计算工作量。作为GNN模型的测试平台，我们生成了新的、大型的合成电网动态稳定性数据集，并将其作为开源资源提供给研究界。我们发现，仅凭拓扑信息，GNN能够出奇地有效地预测高度非线性的目标。首次实现了适用于实际应用的性能。此外，我们展示了这些模型准确识别电网中特定脆弱节点的能力。

    To mitigate climate change, the share of renewable energies in power production needs to be increased. Renewables introduce new challenges to power grids regarding the dynamic stability due to decentralization, reduced inertia, and volatility in production. Since dynamic stability simulations are intractable and exceedingly expensive for large grids, graph neural networks (GNNs) are a promising method to reduce the computational effort of analyzing the dynamic stability of power grids. As a testbed for GNN models, we generate new, large datasets of dynamic stability of synthetic power grids, and provide them as an open-source resource to the research community. We find that GNNs are surprisingly effective at predicting the highly non-linear targets from topological information only. For the first time, performance that is suitable for practical use cases is achieved. Furthermore, we demonstrate the ability of these models to accurately identify particular vulnerable nodes in power grid
    
[^139]: 实现通过编码ResNeXt来分离信息路径

    Towards Disentangling Information Paths with Coded ResNeXt. (arXiv:2202.05343v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.05343](http://arxiv.org/abs/2202.05343)

    本文提出了一种新颖的方法，通过使用编码理论设计特定路径，实现了神经网络的透明度增强和分类模型的轻量级构建。

    

    传统的深度学习模型被视为黑箱，对于指导神经网络决策的机制提供了有限或没有洞察力。为了解决这个问题，已经投入了大量的研究努力来构建可解释的模型。大多数努力要么专注于与最后几层相关的高级特征，要么尝试解释单个层的输出。在本文中，我们采用了一种新颖的方法来增强整个网络功能的透明度。我们提出了一种用于分类的神经网络架构，其中与每个类相关的信息通过特定的路径流动。这些路径在训练之前利用编码理论设计，并且不依赖于类之间的语义相似性。一个关键的特性是每个路径都可以用作自主单用途模型。这使得我们可以在任何类别上获得一个轻量级的二分类器，而无需进行任何额外的训练。

    The conventional, widely used treatment of deep learning models as black boxes provides limited or no insights into the mechanisms that guide neural network decisions. Significant research effort has been dedicated to building interpretable models to address this issue. Most efforts either focus on the high-level features associated with the last layers, or attempt to interpret the output of a single layer. In this paper, we take a novel approach to enhance the transparency of the function of the whole network. We propose a neural network architecture for classification, in which the information that is relevant to each class flows through specific paths. These paths are designed in advance before training leveraging coding theory and without depending on the semantic similarities between classes. A key property is that each path can be used as an autonomous single-purpose model. This enables us to obtain, without any additional training and for any class, a lightweight binary classifi
    
[^140]: XnODR和XnIDR：用于卷积神经网络的两种精确快速的全连接层

    XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks. (arXiv:2111.10854v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.10854](http://arxiv.org/abs/2111.10854)

    本文提出了两种精确快速的全连接层，即XnODR和XnIDR，通过在动态路由机制中采用XNOR处理线性投影，解决了胶囊网络在计算复杂度和准确性方面的问题。

    

    胶囊网络在深度神经网络中定义特征之间的位置关系方面非常强大，但计算复杂且不适合在移动设备上运行。计算复杂度集中在胶囊之间采用的动态路由机制中。另一方面，XNOR-Net虽然快速且计算效率高，但由于二值化过程中的信息损失而导致准确性不高。为了解决动态路由机制的计算负担，本文提出了通过对CapsFC层内或外的线性投影进行XNOR处理的新的全连接（FC）层。具体而言，我们提出的FC层有两个版本，即XnODR（在动态路由之外对线性投影进行XNOR处理）和XnIDR（在动态路由之内对线性投影进行XNOR处理）。为了测试XnODR和XnIDR的泛化能力，我们将它们插入到两个不同的网络中，MobileNetV...

    Capsule Network is powerful at defining the positional relationship between features in deep neural networks for visual recognition tasks, but it is computationally expensive and not suitable for running on mobile devices. The bottleneck is in the computational complexity of the Dynamic Routing mechanism used between the capsules. On the other hand, XNOR-Net is fast and computationally efficient, though it suffers from low accuracy due to information loss in the binarization process. To address the computational burdens of the Dynamic Routing mechanism, this paper proposes new Fully Connected (FC) layers by xnorizing the linear projection outside or inside the Dynamic Routing within the CapsFC layer. Specifically, our proposed FC layers have two versions, XnODR (Xnorize the Linear Projection Outside Dynamic Routing) and XnIDR (Xnorize the Linear Projection Inside Dynamic Routing). To test the generalization of both XnODR and XnIDR, we insert them into two different networks, MobileNetV
    
[^141]: 最佳子群选择

    Optimal subgroup selection. (arXiv:2109.01077v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2109.01077](http://arxiv.org/abs/2109.01077)

    在回归设置中，我们提出了一个子群选择挑战，以确定回归函数超过预设阈值的特征空间区域。我们的主要贡献是确定了在样本规模和类型I错误概率上遗憾的最佳速率。

    

    在临床试验和其他应用中，我们经常看到特征空间中出现了有趣的行为区域，但不清楚这些观察到的现象是否在总体水平上有所反映。针对回归设置，我们考虑子群选择挑战，即识别一个特征空间的区域，在该区域上，回归函数超过了预设的阈值。我们将这个问题形式化为一种约束优化问题，通过寻找一个低复杂度、数据相关的选择集，在这个选择集上，回归函数有至少与阈值一样大的概率，同时要求该区域在边缘特征分布下的质量尽可能大。这导致了一种自然的遗憾概念，我们的主要贡献是确定了遗憾在样本规模和第一类错误概率上的最优值。这个最优值涉及到样本大小和类型I错误概率的微妙相互影响。

    In clinical trials and other applications, we often see regions of the feature space that appear to exhibit interesting behaviour, but it is unclear whether these observed phenomena are reflected at the population level. Focusing on a regression setting, we consider the subgroup selection challenge of identifying a region of the feature space on which the regression function exceeds a pre-determined threshold. We formulate the problem as one of constrained optimisation, where we seek a low-complexity, data-dependent selection set on which, with a guaranteed probability, the regression function is uniformly at least as large as the threshold; subject to this constraint, we would like the region to contain as much mass under the marginal feature distribution as possible. This leads to a natural notion of regret, and our main contribution is to determine the minimax optimal rate for this regret in both the sample size and the Type I error probability. The rate involves a delicate interpla
    
[^142]: 深度模仿学习的实用视角

    A Pragmatic Look at Deep Imitation Learning. (arXiv:2108.01867v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.01867](http://arxiv.org/abs/2108.01867)

    本文在深度模仿学习领域以实际的视角进行了研究，重新实现了6种不同的模仿学习算法，并使用共同的离线策略算法进行了评估。通过对专家轨迹数据进行测试，比较了这些算法的性能。

    

    生成对抗模仿学习（GAIL）算法的引入推动了使用深度神经网络进行可扩展模仿学习方法的发展。许多后续算法使用了类似的过程，将在线策略演员-评论家算法与逆向强化学习相结合。最近出现了更多种类的方法，大多数使用离线策略算法。然而，由于算法的广泛性，从数据集到基础强化学习算法再到评估设置都可能有所变化，这使得公正比较它们变得困难。在这项工作中，我们重新实现了6种不同的模仿学习算法，将其中3种更新为离线策略，将它们基于一个常用的离线策略算法（SAC），并在一个广泛使用的专家轨迹数据集（D4RL）上对它们进行评估，以进行最常见的基准测试（MuJoCo）。在给所有算法相同的超参数优化预算之后，我们比较了它们在一系列专家轨迹测试上的结果。

    The introduction of the generative adversarial imitation learning (GAIL) algorithm has spurred the development of scalable imitation learning approaches using deep neural networks. Many of the algorithms that followed used a similar procedure, combining on-policy actor-critic algorithms with inverse reinforcement learning. More recently there have been an even larger breadth of approaches, most of which use off-policy algorithms. However, with the breadth of algorithms, everything from datasets to base reinforcement learning algorithms to evaluation settings can vary, making it difficult to fairly compare them. In this work we re-implement 6 different IL algorithms, updating 3 of them to be off-policy, base them on a common off-policy algorithm (SAC), and evaluate them on a widely-used expert trajectory dataset (D4RL) for the most common benchmark (MuJoCo). After giving all algorithms the same hyperparameter optimisation budget, we compare their results for a range of expert trajectori
    
[^143]: 随机批量获取：深度主动学习的简单基准方法

    Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning. (arXiv:2106.12059v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.12059](http://arxiv.org/abs/2106.12059)

    研究了一种简单的随机策略，用于将单点获取函数适应于批量主动学习，与计算密集型的批量采集函数相比，性能相当，但使用的计算资源更少。

    

    本文研究了一种简单的随机策略，用于将众所周知的单点获取函数适应于批量主动学习。与从池集合中获取前K个点不同，基于分数或排名的采样考虑到获取数据后采集分数的变化。这种简单的策略可以与计算密集型的最新批量采集函数（如BatchBALD或BADGE）一样表现出色，并且使用的计算资源数量级较少。除了为机器学习从业者提供了一个实用选项外，在各种实验环境中提出的方法的意外成功还为这个领域提出了一个困难的问题：这些昂贵的批量采集方法何时才能发挥作用？

    We examine a simple stochastic strategy for adapting well-known single-point acquisition functions to allow batch active learning. Unlike acquiring the top-K points from the pool set, score- or rank-based sampling takes into account that acquisition scores change as new data are acquired. This simple strategy for adapting standard single-sample acquisition strategies can even perform just as well as compute-intensive state-of-the-art batch acquisition functions, like BatchBALD or BADGE, while using orders of magnitude less compute. In addition to providing a practical option for machine learning practitioners, the surprising success of the proposed method in a wide range of experimental settings raises a difficult question for the field: when are these expensive batch acquisition methods pulling their weight?
    
[^144]: 在错配情况下的分类：半空间、广义线性模型和与可进化性的联系

    Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability. (arXiv:2006.04787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.04787](http://arxiv.org/abs/2006.04787)

    本文解决了半空间在Massart噪声下的错配学习问题，提出了一个简化的算法并回答了一些开放问题。通过黑盒知识蒸馏过程，将复杂分类器转换为同样好的合适分类器。此外，我们还提出了一个小样本的合适学习算法，并将其与矩感知技术相结合，得到了一个具有多项式时间复杂性的学习算法。

    

    本文重新审视了一些关于错配情况下的分类的经典问题。特别是，我们研究了在Massart噪声下以速率$\eta$学习半空间的问题。在最近的一项工作中，Diakonikolas、Goulekakis和Tzamos通过提供第一个有效的算法来解决了一个长期存在的问题，该算法可以学习到准确度$\eta + \epsilon$，其中$\epsilon > 0$。然而，他们的算法输出了一个复杂的假设，将空间分割为$\text{poly}(d,1/\epsilon)$个区域。这里我们给出了一个更简单的算法，并在此过程中解决了一些悬而未决的开放问题：(1)我们提供了第一个可以实现$\eta + \epsilon$的Massart半空间合适学习器。我们还给出了多项式时间算法可以实现的样本复杂性的改进界限。(2)基于(1)，我们开发了一个黑盒知识蒸馏过程，将任意复杂的分类器转换为同样好的合适的分类器。(3)通过利用一个简单但被忽视的机制，我们在没有任何额外假设的情况下，构造了一个小样本的合适学习算法，并将其与基于矩感知的技术相结合，得到一个具有多项式时间复杂性的学习算法。

    In this paper we revisit some classic problems on classification under misspecification. In particular, we study the problem of learning halfspaces under Massart noise with rate $\eta$. In a recent work, Diakonikolas, Goulekakis, and Tzamos resolved a long-standing problem by giving the first efficient algorithm for learning to accuracy $\eta + \epsilon$ for any $\epsilon > 0$. However, their algorithm outputs a complicated hypothesis, which partitions space into $\text{poly}(d,1/\epsilon)$ regions. Here we give a much simpler algorithm and in the process resolve a number of outstanding open questions:  (1) We give the first proper learner for Massart halfspaces that achieves $\eta + \epsilon$. We also give improved bounds on the sample complexity achievable by polynomial time algorithms.  (2) Based on (1), we develop a blackbox knowledge distillation procedure to convert an arbitrarily complex classifier to an equally good proper classifier.  (3) By leveraging a simple but overlooked 
    
[^145]: 关于实体对齐或链接预测方法排名评估的歧义性研究

    On the Ambiguity of Rank-Based Evaluation of Entity Alignment or Link Prediction Methods. (arXiv:2002.06914v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2002.06914](http://arxiv.org/abs/2002.06914)

    本研究研究了实体对齐或链接预测方法的排名评估的歧义性。分析了当前评估指标的不足之处，提出了评估方法的调整，以实现对模型性能的公平、可比和可解释的评估。

    

    本研究针对两种从知识图谱中增强信息的方法：链接预测和实体对齐，对其评估方法进行了深入研究。当前实验设置中，采用多个不同的评估指标来评估模型性能的不同方面。我们分析了这些评估指标的信息性，并发现了一些问题。特别地，我们证明了所有现有的评估指标几乎不能用于在不同数据集之间比较结果。此外，我们还证明了测试集大小的变化会对相同模型的性能产生影响，这是基于实体对齐任务常用度量标准的结果。我们展示了这导致了结果解释上的各种问题，可能支持误导性的结论。因此，我们提出了对评估方法的调整，并通过实验证明了如何实现对模型性能的公平、可比和可解释的评估。我们的代码可供使用。

    In this work, we take a closer look at the evaluation of two families of methods for enriching information from knowledge graphs: Link Prediction and Entity Alignment. In the current experimental setting, multiple different scores are employed to assess different aspects of model performance. We analyze the informativeness of these evaluation measures and identify several shortcomings. In particular, we demonstrate that all existing scores can hardly be used to compare results across different datasets. Moreover, we demonstrate that varying size of the test size automatically has impact on the performance of the same model based on commonly used metrics for the Entity Alignment task. We show that this leads to various problems in the interpretation of results, which may support misleading conclusions. Therefore, we propose adjustments to the evaluation and demonstrate empirically how this supports a fair, comparable, and interpretable assessment of model performance. Our code is availa
    

