# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A statistical approach to detect sensitive features in a group fairness setting.](http://arxiv.org/abs/2305.06994) | 本文提出了一种使用希尔伯特-施密特独立性准则的预处理步骤来自动识别敏感特征的方法，以应对高社会影响决策支持系统中的不公平结果问题。 |
| [^2] | [Neural Wave Functions for Superfluids.](http://arxiv.org/abs/2305.06989) | 本论文利用费米神经网络波函数方法研究了均匀费米气体超流，提出一种针对FermiNet模型的改进方法，获得了极其准确的结果。 |
| [^3] | [Self-Chained Image-Language Model for Video Localization and Question Answering.](http://arxiv.org/abs/2305.06988) | SeViLA是一个利用单个图像语言模型的框架，在视频定位和问答方面表现出色，通过自我链接策略训练局部化器和回答器模块以定位最具信息的关键帧以回答问题。 |
| [^4] | [Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks.](http://arxiv.org/abs/2305.06986) | 本文研究了三层神经网络的特征学习能力，相比之下，它具有比两层网络更丰富的可证的特征学习能力，并提出了一个通用定理，限制了目标结构的样本复杂度和宽度，以实现低测试误差。 |
| [^5] | [Active Retrieval Augmented Generation.](http://arxiv.org/abs/2305.06983) | 本论文提出了一种主动检索增强生成的方法，与以往的方法相比，它在生成过程中更紧密地集成了主动检索和生成，并展示了在一组句子生成任务中的性能优势。 |
| [^6] | [A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges.](http://arxiv.org/abs/2305.06969) | 本篇论文调查了机器学习中交叉公平性的最新进展，提出了分类法和缓解方法，并探讨了未来研究的挑战与方向。 |
| [^7] | [Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image Classification Using Transformers.](http://arxiv.org/abs/2305.06963) | 本论文提出了一种新的级联交叉关注网络（CCAN），以线性复杂度处理大量提取出来的图像块，解决了传统转换器计算复杂度呈平方级增长的问题，该方法用于整张图像分类效果至少与其他方法相当，甚至更好。 |
| [^8] | [Deep Multi-View Subspace Clustering with Anchor Graph.](http://arxiv.org/abs/2305.06939) | 本文提出了锚图辅助的深度多视角子空间聚类方法（DMCAG），该方法可以解决现有DMVSC方法存在的自编码器嵌入子优问题，并通过构建小尺寸锚图来显著减少复杂度。 |
| [^9] | [An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-Markov Decision Processes.](http://arxiv.org/abs/2305.06936) | 本文研究了在有限时间半马尔科夫决策进程中，分层强化学习方法的优化问题，提供了一种通过降低时间分辨率来减少计划时间的选项依赖上界算法，并验证了通过要素框架实现计算成本减少和鲁棒性提高的可行性。 |
| [^10] | [Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition.](http://arxiv.org/abs/2305.06934) | 本文展示了一个对传统认识的颠覆性观点：在计算机编程领域的典型ChatGPT任务中，人类程序员仍然比ChatGPT更优秀。 |
| [^11] | [Convergence of Alternating Gradient Descent for Matrix Factorization.](http://arxiv.org/abs/2305.06927) | 本文提出一种交替梯度下降算法，能够高概率地从非典型随机初始化达到一个$\epsilon$-最优矩阵分解，实验表明该初始化不仅在理论上有益，而且在实践中显著提高了梯度下降的收敛性。 |
| [^12] | [Accurate Surface and Finite Temperature Bulk Properties of Lithium Metal at Large Scales using Machine Learning Interaction Potentials.](http://arxiv.org/abs/2305.06925) | 使用机器学习相互作用势的方法，在大尺度上准确预测锂金属表面和有限温度下的批量性质，克服了传统计算的缺陷，有助于研究锂金属在电池中的应用。 |
| [^13] | [An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation.](http://arxiv.org/abs/2305.06924) | 本论文提出了一种基于模仿学习的算法，利用先验知识和与变化环境的交互实现了GENCO的投标策略优化和贝叶斯纳什均衡估计，针对现代电力市场中先验知识未被充分利用导致现有方法不准确和低效的问题进行了改进。 |
| [^14] | [How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications.](http://arxiv.org/abs/2305.06921) | 本文开发了一种基于强化学习的模拟方法来联合设计电力市场，详细阐述了设计电力现货市场、辅助服务市场中的保留能力产品和金融市场中的虚拟竞标产品的方法，并通过案例研究演示了如何选择最佳市场设计选项。 |
| [^15] | [Pseudo-Hamiltonian system identification.](http://arxiv.org/abs/2305.06920) | 该论文提出了一种能够在受未知干扰和阻尼影响时学习到内部动态解析项的伪哈密顿系统辨识模型，通过混合模型，即使难以找到扰动解析项，也能够准确地识别出动态，对于其他系统辨识模型无法处理的情况具有重要的应用价值。此外，该论文提出了一种使用四阶对称积分方案的方法，能够提高在噪声数据上的性能表现。 |
| [^16] | [Meta-Learners for Few-Shot Weakly-Supervised Medical Image Segmentation.](http://arxiv.org/abs/2305.06912) | 该论文提出了一个用于医学成像少样本弱监督分割的通用元学习框架，并通过比较实验发现基于度量的元学习在小数据场景下表现更佳。 |
| [^17] | [Reinterpreting causal discovery as the task of predicting unobserved joint statistics.](http://arxiv.org/abs/2305.06894) | 研究者提出将因果发现视为预测未观察到联合统计量的任务，这样可以更好地推断未观察到集合的属性。 |
| [^18] | [A Category-theoretical Meta-analysis of Definitions of Disentanglement.](http://arxiv.org/abs/2305.06886) | 本文提出了一个存在的去卷积定义的范畴论元分析，将笛卡儿积和幺模积的概念应该构成去卷积的核心，并展现了处理函数、等变映射、关系和随机映射的相似性和关键区别。 |
| [^19] | [Risk-limiting Financial Audits via Weighted Sampling without Replacement.](http://arxiv.org/abs/2305.06884) | 本文介绍了一种通过构建新的加权抽样信心序列，对N个未知值的加权平均值进行估计的风险限制财务审计（RLFA）。该方法可以通过合并未知值的附加信息提高生成序列的质量，从而提高估计的准确性和置信度。 |
| [^20] | [Convex Quaternion Optimization for Signal Processing: Theory and Applications.](http://arxiv.org/abs/2305.06879) | 建立了基于广义哈密尔顿实数计算的凸四元数优化基本理论，并提出了一些判别定理和判别标准，同时通过三个应用验证了理论的实用性。 |
| [^21] | [Multi-Tier Client Selection for Mobile Federated Learning Networks.](http://arxiv.org/abs/2305.06865) | 本文提出了一种名为SocFedCS的方法，基于社交网络进行联合学习客户端选择，在移动联合学习网络中最小化成本并训练高质量的模型。 |
| [^22] | [A General Framework for Visualizing Embedding Spaces of Neural Survival Analysis Models Based on Angular Information.](http://arxiv.org/abs/2305.06862) | 本文提出了一种通用框架，可以可视化任何神经生存分析模型所使用的中间嵌入表示。该框架基于嵌入空间中的锚定方向，可用于表格数据和原始输入（例如图像）。本文说明了在一个基于角度信息的嵌入空间中存在的信息丢失问题，并提供了减少信息丢失的实用方法。 |
| [^23] | [Policy Gradient Algorithms Implicitly Optimize by Continuation.](http://arxiv.org/abs/2305.06851) | 本文提供了政策梯度算法的新理论解释和证明，即政策梯度算法可以通过连续方式隐式优化确定性策略，并指出政策梯度算法探索的实质是计算当前策略收益的连续函数，策略的方差应该是历史依赖性函数。 |
| [^24] | [A Generic Approach to Integrating Time into Spatial-Temporal Forecasting via Conditional Neural Fields.](http://arxiv.org/abs/2305.06827) | 本文提出了一种将时间组件融入预测模型的通用方法，通过使用条件神经场来表示辅助特征，解决了在利用时间序列进行预测时存在的一个未解决问题。 |
| [^25] | [Information Design in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2305.06807) | 本文探究了多智能体强化学习中的信息设计问题及其挑战，提出了“马尔科夫信令博弈”的概念。 |
| [^26] | [Towards Theoretical Understanding of Data-Driven Policy Refinement.](http://arxiv.org/abs/2305.06796) | 本文介绍了一种数据驱动的强化学习政策细化方法，用于改进安全关键应用的策略，并提出了一系列定理验证其收敛性、鲁棒性界限、泛化误差和对模型错误的弹性。 |
| [^27] | [Integrating nearest neighbors on neural network models for treatment effect estimation.](http://arxiv.org/abs/2305.06789) | 本论文提出了一种新的方法NNCI，用于将最近邻居信息集成到神经网络模型中，以更准确地估计治疗效果。 |
| [^28] | [Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning.](http://arxiv.org/abs/2305.06784) | 本文提出了一种首个效用最大化竞标策略（Fed-Bidder），使多个FL数据消费者可以通过AFL有效而高效地竞争数据拥有者。 |
| [^29] | [Generating high-quality 3DMPCs by adaptive data acquisition and NeREF-based reflectance correction to facilitate efficient plant phenotyping.](http://arxiv.org/abs/2305.06777) | 本研究提出了自适应数据采集和反射校正方法生成高质量的3DMPCs，用于高效植物表型分析。 |
| [^30] | [Comparison of Clustering Algorithms for Statistical Features of Vibration Data Sets.](http://arxiv.org/abs/2305.06753) | 本文通过比较K均值聚类、OPTICS和高斯混合模型聚类算法对振动数据集的统计特征进行聚类，发现平均值和方差类特征特别重要，PCA特征选择可以提高聚类性能，GMM是最优算法，但需要更多的计算资源。 |
| [^31] | [Investigating the generative dynamics of energy-based neural networks.](http://arxiv.org/abs/2305.06745) | 本文系统地研究了受限玻尔兹曼机(RBM)的生成动力学，发现从嵌合态开始自上而下采样可以增加生成多样的数据原型的能力。 |
| [^32] | [Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits.](http://arxiv.org/abs/2305.06743) | 本文提出了一种针对奖励分布重尾的MAB问题的隐式规范化预测器，证明该方法在线性和非线性重尾随机MAB问题上是最优的。 |
| [^33] | [IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers.](http://arxiv.org/abs/2305.06741) | 本文提出了一种新的方法，在建模电子病历时间序列时，利用直接近似IVP的过程来消除递归计算，从而提高计算效率和训练速度。与目前基于IVP求解器和递归神经网络方法相比，本方法可以达到类似的分类和预测性能。 |
| [^34] | [Deep Learning for Retrospective Motion Correction in MRI: A Comprehensive Review.](http://arxiv.org/abs/2305.06739) | 该综述针对MRI中的运动问题，综述了基于深度学习对运动校正的方法，发现了不同应用之间的差异和共同点，在未来的方向上提出了建议。 |
| [^35] | [NUBO: A Transparent Python Package for Bayesian Optimisation.](http://arxiv.org/abs/2305.06709) | NUBO是一个透明的Python包，用于优化昂贵的黑盒函数，它利用高斯过程做代理模型以及获取函数来指导选择候选点，专注于透明度和用户体验。 |
| [^36] | [A data-driven rutting depth short-time prediction model with metaheuristic optimization for asphalt pavements based on RIOHTrack.](http://arxiv.org/abs/2305.06707) | 本研究利用人工智能模型预测不同沥青路面的车辙深度，采用复杂网络方法进行结构元素选择，并使用带有RCO修正的ELM算法进行短时预测。 |
| [^37] | [Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models.](http://arxiv.org/abs/2305.06704) | 该论文提出了一种基于聚类的鲁棒检测滞后多因子模型中的领先滞后关系方法，并使用各种聚类技术和相似度度量方法实现了对领先滞后估计的聚合，从而强化了对原始宇宙中的一致关系的识别。 |
| [^38] | [Neural Fine-Gray: Monotonic neural networks for competing risks.](http://arxiv.org/abs/2305.06703) | 本文提出了一种使用单调约束神经网络模拟每种竞争生存率分布的方法，从而确保可以在计算成本下实现精确的最大似然值最优化，该方法可以用于生存分析领域。 |
| [^39] | [Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species.](http://arxiv.org/abs/2305.06695) | 本文提出了一种利用对齐的视觉-遗传推理空间来提高少量图像数据珍稀物种分类的方法，该方法通过深度嵌入模型实现对齐，适用于提高稀有物种的长尾识别，并且可以显著有益于仅基于视觉的稀有物种识别。 |
| [^40] | [INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models.](http://arxiv.org/abs/2305.06677) | 本文提出了一种使用信息丰富的数据子集来高效预训练大型语言模型的方法，减少了训练时间和计算成本，同时保持了模型的泛化能力。 |
| [^41] | [On the convergence of the MLE as an estimator of the learning rate in the Exp3 algorithm.](http://arxiv.org/abs/2305.06660) | 本文旨在证明：在Exp3算法中，如果学习率是恒定的，则MLE不能有效估计学习率；而如果学习率随样本量的增加呈多项式下降，则MLE的预测误差在概率上满足随样本量多项式下降的边界。 |
| [^42] | [On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm.](http://arxiv.org/abs/2305.06657) | 本文提出了一个名为ARQ-Learning的鲁棒性强化学习算法，采用了一个更加实际的不确定性集，并提出了一种称为“悲观代理”的方法。该算法在表格化的情况下获得了与现有算法相同的快速收敛速度，并为实际应用提供了更好的鲁棒性。而且，本文还首次提出了用于深度Q网络和深度确定策略梯度的鲁棒RL算法PR-DQN和PR-DDPG。 |
| [^43] | [Generalization bounds for neural ordinary differential equations and deep residual networks.](http://arxiv.org/abs/2305.06648) | 本研究提出了神经常微分方程及其变体的泛化界限，涵盖了深度残差网络，其泛化界限与连续权重差异大小有关。 |
| [^44] | [Predictive change point detection for heterogeneous data.](http://arxiv.org/abs/2305.06630) | 该论文提出了一种基于“预测与比较”机器学习模型的变点监测框架，它能够比现有的在线监测方法更好地控制误报率和失控平均运行长度。该方法使用ARIMA模型和LSTM递归神经网络模型进行预测，具有很强的推广性能。 |
| [^45] | [Dropout Regularization in Extended Generalized Linear Models based on Double Exponential Families.](http://arxiv.org/abs/2305.06625) | 本论文研究了基于双指数族的扩展广义线性模型中的dropout正则化，dropout正则化偏好罕见但重要的特征，在均值和离散度方面都具有普适性。 |
| [^46] | [Matrix tri-factorization over the tropical semiring.](http://arxiv.org/abs/2305.06624) | 本研究提出了一种热带半环上的矩阵三因式分解算法 triFastSTMF，用于分析多部分网络结构，并在四部分网络结构分析中恢复网络的边长度。 |
| [^47] | [V2Meow: Meowing to the Visual Beat via Music Generation.](http://arxiv.org/abs/2305.06594) | V2Meow是一种新方法，通过与O(100K)音频片段配对的视频帧进行训练，生成与各种类型的视频输入的视觉语义相匹配的高质量音频，无需符号音乐数据。 |
| [^48] | [HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level.](http://arxiv.org/abs/2305.06588) | 提出了HAHE模型，使用全局和局部水平的注意力学习了超关系知识图谱的图形结构和顺序结构，并在多个基准数据集上实现了最先进的表现。 |
| [^49] | [How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?.](http://arxiv.org/abs/2305.06587) | 该论文研究了谱时图神经网络的表达能力，并揭示了其具有线性谱时GNN是普适的、表现力受到离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。同时，论文提出了一个简单实例TGC，其在时间序列预测方面具有显著的性能优势。 |
| [^50] | [Clustering of Time-Varying Graphs Based on Temporal Label Smoothness.](http://arxiv.org/abs/2305.06576) | 本文提出了一种基于谱聚类的时变图节点聚类方法，并添加了簇标签平滑性的约束条件。实验证明该方法是有效的。 |
| [^51] | [How to Index Item IDs for Recommendation Foundation Models.](http://arxiv.org/abs/2305.06569) | 本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。 |
| [^52] | [Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation.](http://arxiv.org/abs/2305.06563) | 本文提出了一种基于流形正则化Tucker分解的时空交通数据填充方法，该方法利用稀疏正则化项改善了Tucker核的稀疏性，并引入流形正则化和时间约束项来优化张量的填充性能。 |
| [^53] | [Long-Tailed Question Answering in an Open World.](http://arxiv.org/abs/2305.06557) | 本研究提出了一种支持长尾分布数据的Open Long-Tailed QA (OLTQA)模型，鼓励头部、尾部和未知任务间的知识共享，并从大型预训练语言模型中明确挖掘知识，解决了QA方法中瓶颈问题。 |
| [^54] | [Domain Incremental Lifelong Learning in an Open World.](http://arxiv.org/abs/2305.06555) | 本文提出了Diana模型，一种基于动态架构的生命周期学习模型，它使用四种层次化组织的提示来学习一系列任务。其中，任务级提示用于捕获任务特定的知识，实例级提示用于学习跨输入样本共享的知识，从而提高模型的泛化性能。 |
| [^55] | [Neural Lyapunov Control for Discrete-Time Systems.](http://arxiv.org/abs/2305.06547) | 该论文介绍了一种用于离散时间系统的神经李雅普诺夫控制方法，该方法利用混合整数线性规划来验证稳定性条件，计算子水平集刻画吸引域，有效地学习控制策略。 |
| [^56] | [Spectral Clustering on Large Datasets: When Does it Work? Theory from Continuous Clustering and Density Cheeger-Buser.](http://arxiv.org/abs/2305.06541) | 本文研究了大数据集上的谱聚类问题，提供了关于在大数据集上进行谱聚类的理论直觉，从概率密度函数中提取大量点时的谱聚类问题得到了一定的解决。 |
| [^57] | [Semantic Random Walk for Graph Representation Learning in Attributed Graphs.](http://arxiv.org/abs/2305.06531) | 本文提出基于语义随机游走的属性图图表示学习方法，通过联合优化两种异构源，构建辅助加权图进行高阶距离学习，从而表征节点和属性，并捕获图结构和语义内部和之间的非线性高阶内在相关性。 |
| [^58] | [How Good are Commercial Large Language Models on African Languages?.](http://arxiv.org/abs/2305.06530) | 本文对商用大型语言模型在跨越不同语言系和地理区域的八种非洲语言上进行了初步分析，结果显示它们在非洲语言上的表现略低。呼吁确保非洲语言在商业大型语言模型中得到充分的重视。 |
| [^59] | [A fast topological approach for predicting anomalies in time-varying graphs.](http://arxiv.org/abs/2305.06523) | 本文介绍了一种快速的拓扑方法来预测时变图中的异常点。该方法避免了特征的组合爆炸，并在合成和真实的时变图的异常检测上实现了最先进的表现。 |
| [^60] | [ST-GIN: An Uncertainty Quantification Approach in Traffic Data Imputation with Spatio-temporal Graph Attention and Bidirectional Recurrent United Neural Networks.](http://arxiv.org/abs/2305.06480) | 本研究提出了一种创新的交通数据插值方法，利用图注意力和双向神经网络捕捉时空相关性，实验结果表明在处理缺失值方面优于其他基准技术。 |
| [^61] | [Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction.](http://arxiv.org/abs/2305.06474) | 本文研究了大型语言模型（LLMs）在用户评分预测任务中的表现，与传统的协同过滤方法进行对比。结果发现LLMs能够在较少数据的情况下保持优秀的性能，并且在零样本和少样本情况下表现很好。 |
| [^62] | [Securing Distributed SGD against Gradient Leakage Threats.](http://arxiv.org/abs/2305.06473) | 本文提出了一种用于防止梯度泄漏的综合方法，使得联邦学习中的分布式SGD更具隐私、准确性和攻击韧性。 |
| [^63] | [ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps.](http://arxiv.org/abs/2305.06472) | 该论文综述了基于大规模基础模型（LSF-Models）如ChatGPT和DALLE-E的人工智能（AI）技术在预测与健康管理（PHM）中的广泛应用。这种技术可以实现多模态、多任务、大量数据和超大模型范式，成为AI-2.0的新时代的标志之一。 |
| [^64] | [Continual Facial Expression Recognition: A Benchmark.](http://arxiv.org/abs/2305.06448) | 本研究提出了“持续面部表情识别（ConFER）”基准测试，以评估在真实世界交互中，基于连续学习（CL）能够不断适应不同任务、场景等多变环境的面部表情识别（FER）模型。 |
| [^65] | [Dynamic Graph Representation Learning for Depression Screening with Transformer.](http://arxiv.org/abs/2305.06447) | 利用Transformer进行抑郁症筛查，克服了传统方法的特征工程依赖和忽略时变因素的缺点。 |
| [^66] | [Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation.](http://arxiv.org/abs/2305.06446) | 该论文探讨了多智能体强化学习在情节式马尔可夫决策过程中的协作问题，提出了一种基于值迭代的算法，可以实现异步通信，在保证合作优势的同时降低通信开销。通过提供和证明的算法和复杂度界限，为多智能体强化学习在实际应用中提供理论依据。 |
| [^67] | [Data, Trees, and Forests -- Decision Tree Learning in K-12 Education.](http://arxiv.org/abs/2305.06442) | 针对K-12教育中学生对于机器学习的学习难度大的问题，提出了一种教学概念，同时注重概念理解与实际应用，基于决策树学习，旨在赋予学生积极应用机器学习方法和反思对社会的影响的能力。 |
| [^68] | [Phase transitions in the mini-batch size for sparse and dense neural networks.](http://arxiv.org/abs/2305.06435) | 本文系统地研究了小批量大小对稀疏和密集神经网络训练的影响，发现在临界值处会出现尖锐的相变，阐明了神经网络优化的基本机制。 |
| [^69] | [Word Grounded Graph Convolutional Network.](http://arxiv.org/abs/2305.06434) | 该论文提出了一种基于词语的图卷积网络模型，可以在处理图外文档时进行归纳推理；该模型在多个基准数据集上表现出较好的性能，同时表明了基于图的方法联合建模词级和文档级信息的有效性。 |
| [^70] | [A Generalizable Physics-informed Learning Framework for Risk Probability Estimation.](http://arxiv.org/abs/2305.06432) | 本文提出了一种基于物理学的学习框架，通过将MC方法与基于物理学的神经网络相结合，有效评估长期风险概率及其梯度。数值结果表明，该方法具有更好的样本效率，能够适应系统变化。 |
| [^71] | [A Method to Automate the Discharge Summary Hospital Course for Neurology Patients.](http://arxiv.org/abs/2305.06416) | 开发了一种使用编码器-解码器序列到序列变换模型进行医院过程小结的自动化方法以缓解医生过劳。该方法在实际性上进行了优化，盲评估表明62%的自动化摘要符合标准，具有临床应用的潜力。 |
| [^72] | [Accelerating Batch Active Learning Using Continual Learning Techniques.](http://arxiv.org/abs/2305.06408) | 本文介绍了一种新的技术，即永续性主动学习（CAL），通过偏向先前标记集来加速训练，通过使用一系列回放方案，包括模型蒸馏和从历史中选择多样化的和不确定的点。实验结果表明，CAL可以大幅提升训练速度。 |
| [^73] | [ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion.](http://arxiv.org/abs/2305.06395) | 本文提出了一种名为ACTC的方法，在冷启动知识图谱补全时进行主动阈值校准，可以有效地利用有限的标记元组来找到每个关系的最佳阈值，同时也结合未标记元组进行了实验。 |
| [^74] | [Text-To-Concept (and Back) via Cross-Model Alignment.](http://arxiv.org/abs/2305.06386) | 本文介绍了"Text-To-Concept"的方法，通过特征对齐，将来自预训练模型的特征转换为可与文本编码器比较的标准化形式，并免费将视觉编码器转换为零样本分类器。 |
| [^75] | [Discovery of Optimal Quantum Error Correcting Codes via Reinforcement Learning.](http://arxiv.org/abs/2305.06378) | 该论文介绍了一种使用强化学习策略来发掘最佳量子纠错码的方法，该方法可以训练代理以最大化编码距离或最小化在偏置Pauli噪声下的逻辑错误概率。作者证明，与其他CSS代码相比，他们可以限制量子比特数量并获得优越的结果。 |
| [^76] | [Efficient Training of Multi-task Neural Solver with Multi-armed Bandits.](http://arxiv.org/abs/2305.06361) | 本文提出了一种基于多臂赌博机的通用高效训练范式，用于多任务神经求解器的训练，通过任务影响矩阵进行更高效的训练，相比于标准计划，在有限的训练预算或相同的训练时长内实现了更高的整体性能。 |
| [^77] | [Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy.](http://arxiv.org/abs/2305.06360) | 本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。 |
| [^78] | [HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion.](http://arxiv.org/abs/2305.06356) | 具有4D动态场景表示的HumanRF能够从多视角视频输入中捕捉全身外貌，以高压缩率捕捉精细细节并支持高分辨率。ActorsHQ提供了12MP的镜头，为长序列获得时间上连贯的人物重建。 |
| [^79] | [Enhancing Quantum Support Vector Machines through Variational Kernel Training.](http://arxiv.org/abs/2305.06063) | 本文研究了量子支持向量机，并提出了一种新方法：量子变分核支持向量机（QVK-SVM），能够在准确性、损失和混淆矩阵指标方面优于现有模型。它应该成为未来QML研究中的可靠工具。 |
| [^80] | [Domain Agnostic Image-to-image Translation using Low-Resolution Conditioning.](http://arxiv.org/abs/2305.05023) | 本文提出了一种低分辨率条件下的领域无关的图像翻译方法，实现了源图像的可视特征与低分辨率目标图像的信息相结合，解决了领域相关的细粒度问题。 |
| [^81] | [Causal Policy Gradient for Whole-Body Mobile Manipulation.](http://arxiv.org/abs/2305.04866) | 本文提出了一种新框架——因果MoMa，可以训练适用于典型MoMa任务的策略，在此框架下，机动和交互自由度可以同时组合，并且不需要人类领域知识来划分动作空间或将动作部分与子目标匹配。 |
| [^82] | [High-Dimensional Smoothed Entropy Estimation via Dimensionality Reduction.](http://arxiv.org/abs/2305.04712) | 本文研究如何通过PCA将数据投影到低维空间来降低微分熵估计的指数样本复杂性问题，并表明该方法对于嵌入高维空间中的低维结构具有近乎最优性能。 |
| [^83] | [MO-DEHB: Evolutionary-based Hyperband for Multi-Objective Optimization.](http://arxiv.org/abs/2305.04502) | MO-DEHB是一种扩展自DEHB的多目标优化器，可应用于各种多目标问题，并在多个性能指标上表现出最优性能。 |
| [^84] | [Spiking neural networks with Hebbian plasticity for unsupervised representation learning.](http://arxiv.org/abs/2305.03866) | 本文介绍了一种新的脉冲神经网络模型，实现了从数据中无监督地学习分布式内部表示。该模型采用在线Hebbian-Bayesian学习和重连机制，并表现出和传统的非脉冲神经网络相近的性能。 |
| [^85] | [Exploring Softly Masked Language Modelling for Controllable Symbolic Music Generation.](http://arxiv.org/abs/2305.03530) | 本文探索了软掩模语言建模在符号音乐生成中的应用。使用变压器编码器架构，成功将SMLM应用于受限符号音乐生成。 |
| [^86] | [SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data.](http://arxiv.org/abs/2305.02993) | 本论文介绍SemEval 2023的任务七，旨在进行临床试验数据的多证据自然语言推理，该任务难度较大，证据选择任务相对于蕴含任务表现更佳。 |
| [^87] | [On the Expressivity Role of LayerNorm in Transformers' Attention.](http://arxiv.org/abs/2305.02582) | 本文揭示了LayerNorm在Transformers的Attention层中有着至关重要的作用，通过对输入向量进行投影并对所有向量进行缩放，LayerNorm可以帮助注意力机制更好地处理输入。 |
| [^88] | [A Chain Rule for the Expected Suprema of Bernoulli Processes.](http://arxiv.org/abs/2304.14474) | 该论文得出了一个关于伯努利过程期望最大值的上限界限，该界限由指数集在均匀利普希茨函数类下的属性和函数类得出。 |
| [^89] | [On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective.](http://arxiv.org/abs/2304.13836) | 本论文评估了RemOve-And-Retrain（ROAR）协议的可靠性。研究结果表明，ROAR基准测试中的属性可能有更少的有关决策的重要信息，这种偏差称为毛糙度偏差，并提醒人们不要在ROAR指标上进行盲目的依赖。 |
| [^90] | [Surrogate Assisted Generation of Human-Robot Interaction Scenarios.](http://arxiv.org/abs/2304.13787) | 本文提出了基于替代模型的人机交互场景生成方法，可以高效合成多样化的挑战性数据集，以便评估和理解人机交互系统的优劣，可以在实际交互中重现这些场景。 |
| [^91] | [Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization.](http://arxiv.org/abs/2304.13761) | 通过独热编码和正则化提高梯度提升决策树的鲁棒性，研究表明对带有$L_1$或$L_2$正则化的线性回归形式进行拟合可提高GBDT模型的鲁棒性。 |
| [^92] | [Organizational Governance of Emerging Technologies: AI Adoption in Healthcare.](http://arxiv.org/abs/2304.13081) | 该研究通过与美国主要医疗保健系统的领导人和相关领域的主要知情人合作，制定了AI在医疗保健中的组织治理框架，包括关键控制点和决策标准，为卫生系统领导人做出更加明智的决策提供了支持。 |
| [^93] | [More Communication Does Not Result in Smaller Generalization Error in Federated Learning.](http://arxiv.org/abs/2304.12216) | 我们研究了联邦学习环境下的统计学习模型泛化误差，表明更频繁地与参数服务器通信会负面影响此类学习算法的泛化性能。 |
| [^94] | [Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT.](http://arxiv.org/abs/2304.11116) | 本文旨在通过Graph-ToolFormer框架赋予LLMs图形推理能力，并解决现有LLMs在执行图形学习任务中存在的固有弱点。 |
| [^95] | [MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos.](http://arxiv.org/abs/2304.05292) | 本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。 |
| [^96] | [Solving Regularized Exp, Cosh and Sinh Regression Problems.](http://arxiv.org/abs/2303.15725) | 该文研究和解决了正则化指数回归问题，根据大型语言模型注意力计算的灵感，使用近似牛顿方法在输入稀疏时间内求解。 |
| [^97] | [Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities.](http://arxiv.org/abs/2303.12706) | 本文提出了一种多模态规范建模框架，能够更好地检测出多种成像和生物变量中的异常性，特别适用于研究带有异质性的疾病。 |
| [^98] | [Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review.](http://arxiv.org/abs/2303.07647) | 本文综述了近年来机器学习在实验固体力学领域中的最新进展和应用，提供了物理感知和基于物理学的机器学习方法，并涵盖了断裂力学、生物力学、纳米和微观力学、构建材料和二维材料等传统和新兴领域。 |
| [^99] | [Using VAEs to Learn Latent Variables: Observations on Applications in cryo-EM.](http://arxiv.org/abs/2303.07487) | 本研究通过定性分析，发现VAE在生物应用中摊销潜在变量的特性与传统显式表示方法相似。 |
| [^100] | [Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference.](http://arxiv.org/abs/2302.11944) | 我们提出了一种反事实场景测试框架，通过比较数据集中类似的保护和非保护实例来检测分类器中的歧视，通过比较组间决策结果差异，来发现个人歧视。该框架可以更好地对「给定差异的公平原则」进行操作，以揭示在公平原则下的歧视差异。 |
| [^101] | [Reinforcement Learning for Combining Search Methods in the Calibration of Economic ABMs.](http://arxiv.org/abs/2302.11835) | 本文提出了一种通过强化学习自动选择和组合搜索方法的方案，用于经济代理模型的参数校准，实验结果表明该方案在提高效率的同时不需要专业领域知识或手动调整参数。 |
| [^102] | [Imprecise Bayesian Neural Networks.](http://arxiv.org/abs/2302.09656) | 在机器学习和人工智能领域，该论文提出了一种新的算法——不精确的贝叶斯神经网络(IBNNs)。这种算法使用可信区间先验分布集合和似然分布集合进行训练，相比标准的BNNs，可以区分先验和后验的不确定性并量化。此外，IBNNs在贝叶斯灵敏度分析方面具有更强的鲁棒性，并且对分布变化也更加鲁棒。 |
| [^103] | [Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most.](http://arxiv.org/abs/2302.09195) | 该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。 |
| [^104] | [Run-Off Election: Improved Provable Defense against Data Poisoning Attacks.](http://arxiv.org/abs/2302.02300) | 本文提出了一种名为ROE的新型数据污染防御方法，通过在基本模型之间进行运行式选举，有效利用logits层的信息，并在MNIST数据集和CIFAR-10上得到了比最先进的集成方法更好的结果，提供了针对数据污染攻击的可证明保证 |
| [^105] | [Reverse Ordering Techniques for Attention-Based Channel Prediction.](http://arxiv.org/abs/2302.00341) | 本文提出了基于Seq2Seq-attn和Transformer的信道预测模型，并引入了反向技术以提高模型鲁棒性，仿真结果表明比现有方法更好。 |
| [^106] | [NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks.](http://arxiv.org/abs/2301.12667) | NeSyFOLD是一种神经符号框架，可以从CNN中提取逻辑规则并生成可解释的分类模型。它使用基于规则的FOLD-SE-M机器学习算法和自动映射算法来将CNN核映射到语义概念，并产生可解释的规则集。 |
| [^107] | [Speech Driven Video Editing via an Audio-Conditioned Diffusion Model.](http://arxiv.org/abs/2301.04474) | 本文提出了一种使用音频条件下的扩散模型进行端到端语音驱动视频编辑的方法，通过条件生成器实现对唇部和下巴运动的同步，避免了对中间结构表示的依赖。 |
| [^108] | [Kernel Subspace and Feature Extraction.](http://arxiv.org/abs/2301.01410) | 本文研究了机器学习中的核方法，建立了特征子空间和核之间的一一对应关系，并提出了一个信息熵度量方法用于核的比较。特别地，构建了一个极大相关核，并证明其在信息熵度量方面的最优性。最后，把Fisher核解释为一种特殊的极大相关核，并建立了它的最优性。 |
| [^109] | [Continual Learning of Natural Language Processing Tasks: A Survey.](http://arxiv.org/abs/2211.12701) | 本文综述了NLP中持续学习的最新进展，其中CF预防、知识迁移和跨任务类分离等方面对NLP任务至关重要，并讨论了未来研究方向。 |
| [^110] | [Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations.](http://arxiv.org/abs/2211.08794) | 本文提出了一种利用多视角压缩表示降低预训练语言模型微调过程中过拟合问题的方法，经过测试在低资源NLP任务中表现良好。 |
| [^111] | [From Denoising Diffusions to Denoising Markov Models.](http://arxiv.org/abs/2211.03595) | 本论文提出了一个统一的框架，将去噪扩散模型推广到广泛的空间中，并导致分数匹配的原始扩展，适用于各种应用程序。 |
| [^112] | [$2 \times 2$ Zero-Sum Games with Commitments and Noisy Observations.](http://arxiv.org/abs/2211.01703) | 本文研究了带承诺和噪声观测的$2\times 2$零和博弈，发现平衡点总是存在的；领导者的动作观测结果对于追随者来说要么是有益的，要么是无关紧要的；该博弈的收益在均衡点上被上界限制为纯策略下的SE的收益，下界为混合策略下的纳什均衡的收益。 |
| [^113] | [Inflexible Multi-Asset Hedging of incomplete market.](http://arxiv.org/abs/2211.00948) | 本文提出了一种在不完全市场下对冲的方法，使用新的跳跃扩散模型和三种神经网络获得了较好的结果。 |
| [^114] | [Improving Hyperspectral Adversarial Robustness Under Multiple Attacks.](http://arxiv.org/abs/2210.16346) | 该论文提出了一种Adversarial Discriminator Ensemble Network（ADE-Net）来增强高光谱图像模型的鲁棒性，针对不同攻击类型使用相应的攻击专家集合网络。 |
| [^115] | [Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes.](http://arxiv.org/abs/2210.14410) | 本文提出一种联合分类和多个明确检测类的方法来提高对抗鲁棒性，在保证可验证的防御机制的基础上，实现了对具有多个明确弃权类别的网络的保障，并通过正则化方法和训练方法对抗了模型的退化。所提出的方法在实现有利的标准和鲁棒性验证准确性平衡点方面表现出色，比现有算法更出色。 |
| [^116] | [On Many-Actions Policy Gradient.](http://arxiv.org/abs/2210.13011) | 本论文研究了具有多个动作样本的随机策略梯度的方差问题，并提出了一种基于动态模型的多动作采样方法，实现了更高的样本效率和更高的回报。 |
| [^117] | [Deep Linear Networks for Matrix Completion -- An Infinite Depth Limit.](http://arxiv.org/abs/2210.12497) | 本文研究了深度线性网络用于矩阵完成的训练过程，得到了黎曼几何和训练渐近性之间的关系，证明了隐式正则化是高状态空间体积偏见的结果。 |
| [^118] | [Continuous-in-time Limit for Bayesian Bandits.](http://arxiv.org/abs/2210.07513) | 本文提出了一种适用于解决大时间长度下的贝叶斯赌博机问题的近似贝叶斯最优策略，并且其计算成本不包括依赖于时间长度的项。 |
| [^119] | [Using Full-Text Content to Characterize and Identify Best Seller Books.](http://arxiv.org/abs/2210.02334) | 该研究通过对书籍的全文内容进行可视化和分类任务，研究预测书籍是否会成为畅销书。使用了 SemAxis 和线性判别分析进行数据初步探索，采用多种分类器获得定量和更加客观的结果。 |
| [^120] | [Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law.](http://arxiv.org/abs/2209.06049) | 本研究针对印度法律文本，重新训练和从零开始训练了两个PLMs，即LegalBERT和CaseLawBERT，并采用基于印度法律文本的词汇表训练了一个模型。我们在几项基准法律NLP任务中，对印度和非印度的法律文本进行了应用。 |
| [^121] | [Making Intelligence: Ethical Values in IQ and ML Benchmarks.](http://arxiv.org/abs/2209.00692) | 本文探讨了人类智慧基准和机器学习（ML）基准之间的相似之处，并强调了在创建ML基准时应考虑和记录价值观。 |
| [^122] | [Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning.](http://arxiv.org/abs/2208.08831) | 本研究利用现成的图像生成和字幕生成技术，自动发现视觉模型中的错误。通过生成大量合成但逼真的输入，聚类和描述，评估和发现分类器的失败和虚假相关性。这为未来构建实用的框架提供了一个有前途的思路。 |
| [^123] | [FedOBD: Opportunistic Block Dropout for Efficiently Training Large-scale Neural Networks through Federated Learning.](http://arxiv.org/abs/2208.05174) | FedOBD是一个新的联邦学习方法，它通过将大型模型分解为语义块，允许参与者机会主义地上传量化的块来进行聚合，以此解决了在大规模联邦学习模型训练过程中操作模型参数所产生的通信开销和性能问题。 |
| [^124] | [PointConvFormer: Revenge of the Point-based Convolution.](http://arxiv.org/abs/2208.02879) | PointConvFormer 是一种新颖的点云深度网络架构构建模块，它利用特征差异计算出的注意力来修改卷积权重，保留了点卷积的不变性，同时能够选择具有相关性的点进行卷积操作，适用于点级细节的多个任务，既提高了精度，又提高了速度。 |
| [^125] | [Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift.](http://arxiv.org/abs/2206.13089) | 本研究发现神经网络内部分布的准确性与在分布转换下的准确性具有强线性相关性。这种现象也适用于一对神经网络分类器之间的一致性。我们提出了“在线一致性”这一理论，并展示其在估算没有标记的OOD验证集时的效果优于现有基线检测模型。 |
| [^126] | [Provably Efficient Risk-Sensitive Reinforcement Learning: Iterated CVaR and Worst Path.](http://arxiv.org/abs/2206.02678) | 本文提出了迭代CVaR和最坏路径的风险敏感强化学习问题，并设计了有效算法解决该问题，适用于实际应用中需要避免风险的任务。 |
| [^127] | [Spreading Factor assisted LoRa Localization with Deep Reinforcement Learning.](http://arxiv.org/abs/2205.11428) | 该论文提出了一种利用SF的LoRa RSSI指纹方法，通过深度强化学习模型以应对LoRa网络复杂性和可扩展性问题，定位精度比最先进方法提高6.67％到48.10％不等。 |
| [^128] | [Minority Stress Experienced by LGBTQ Online Communities during the COVID-19 Pandemic.](http://arxiv.org/abs/2205.09511) | 研究探究了LGBTQ在线社群在COVID-19疫情期间经历的少数群体压力。利用机器学习分类器检测Twitter上表现出的少数群体压力，比较疫情前后的语言差异。 |
| [^129] | [Analysing similarities between legal court documents using natural language processing approaches based on Transformers.](http://arxiv.org/abs/2204.07182) | 本文尝试通过利用六种基于Transformer的自然语言处理技术，基于巴西葡萄牙语的通用语料库预训练，利用210,000份法律诉讼文档进行微调和专业化训练，解决法律文件相似度问题，从而协助快速解决司法程序。 |
| [^130] | [Language modeling via stochastic processes.](http://arxiv.org/abs/2203.11370) | 本篇论文探究了对比表示在生成任务中的应用，提出了Time Control(TC)方法，可以保留长文本的结构，并在学习句子表示以获得话语连贯性方面表现竞争力。 |
| [^131] | [Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification.](http://arxiv.org/abs/2203.11155) | 该论文将量子密度矩阵应用于经典问答和图像分类中，证明了其可以提高任务的效率，尤其在图像分类中取得了优秀的性能表现。 |
| [^132] | [Cooperation for Scalable Supervision of Autonomy in Mixed Traffic.](http://arxiv.org/abs/2112.07569) | 这项工作研究了在自动驾驶车辆与人类驾驶员混合交通中，如何通过自主代理的合作，实现可扩展且安全的监管，分析结果表明，AV合作可以将监管可靠性提高数个数量级，并且需要更少的监管员。 |
| [^133] | [Stochastic differential equations for limiting description of UCB rule for Gaussian multi-armed bandits.](http://arxiv.org/abs/2112.06423) | 本文介绍了对于高斯多臂老虎机中已知控制时域大小$N$的上限置信度策略的极限描述，并使用随机微分方程进行了验证。 |
| [^134] | [Rigorous data-driven computation of spectral properties of Koopman operators for dynamical systems.](http://arxiv.org/abs/2111.14889) | 本文提出了一种数据驱动算法，可以严格计算从轨迹数据中获得的Koopman算子的谱信息。该算法达到了高阶收敛，适用于混沌系统。 |
| [^135] | [Auctions and Peer Prediction for Academic Peer Review.](http://arxiv.org/abs/2109.00923) | 本论文提出了一种机制设计方法，通过拍卖和同伴预测机制，同时激励高质量的提交和评审，来改善同行评审流程。 |
| [^136] | [Continuous Mean-Covariance Bandits.](http://arxiv.org/abs/2102.12090) | 本文提出了一种新的连续均值协方差赌博机（CMCB）模型，以考虑选项相关性。算法具有最优遗憾，并在实验中表现良好。 |
| [^137] | [Towards Adversarial-Resilient Deep Neural Networks for False Data Injection Attack Detection in Power Grids.](http://arxiv.org/abs/2102.09057) | 本文提出了一种对抗性深度神经网络方法，用于电力系统中虚假数据注入攻击检测，并在训练和推理阶段采用随机输入填充技术，从而显著减少对抗攻击的有效性。 |
| [^138] | [Learning to Rank under Multinomial Logit Choice.](http://arxiv.org/abs/2009.03207) | 该论文提出了一个基于多项Logit选择模型的学习排序框架，能够更准确地捕捉用户在整个项目列表中的选择行为，为网站设计提供了更好的排序方案。 |

# 详细

[^1]: 在群体公平设置中检测敏感特征的统计方法

    A statistical approach to detect sensitive features in a group fairness setting. (arXiv:2305.06994v1 [cs.LG])

    [http://arxiv.org/abs/2305.06994](http://arxiv.org/abs/2305.06994)

    本文提出了一种使用希尔伯特-施密特独立性准则的预处理步骤来自动识别敏感特征的方法，以应对高社会影响决策支持系统中的不公平结果问题。

    

    机器学习模型在决策支持系统中的高社会影响应用引起了对于不同人群之间不公平（不一致）结果的担忧。当评估这样的不公平决策时，通常会依赖于由一组特征确定的预定义群体，这些特征被视为敏感。然而，这种方法是主观的，不能保证这些特征是唯一需要考虑的敏感特征，也不能保证它们会导致不公平（不一致）的结果。在本文中，我们提出了一种预处理步骤来自动识别敏感特征，而不需要训练模型来验证不公平的结果。我们的提议基于希尔伯特-施密特独立性准则，该准则衡量变量分布的统计依赖性。我们假设，如果敏感特征与标签向量之间的依赖性很高，那么这个特征提供的信息将导致不一致的结果。

    The use of machine learning models in decision support systems with high societal impact raised concerns about unfair (disparate) results for different groups of people. When evaluating such unfair decisions, one generally relies on predefined groups that are determined by a set of features that are considered sensitive. However, such an approach is subjective and does not guarantee that these features are the only ones to be considered as sensitive nor that they entail unfair (disparate) outcomes.  In this paper, we propose a preprocessing step to address the task of automatically recognizing sensitive features that does not require a trained model to verify unfair results. Our proposal is based on the Hilber-Schmidt independence criterion, which measures the statistical dependence of variable distributions. We hypothesize that if the dependence between the label vector and a candidate is high for a sensitive feature, then the information provided by this feature will entail disparate
    
[^2]: 超流体的神经波函数研究

    Neural Wave Functions for Superfluids. (arXiv:2305.06989v1 [cond-mat.quant-gas])

    [http://arxiv.org/abs/2305.06989](http://arxiv.org/abs/2305.06989)

    本论文利用费米神经网络波函数方法研究了均匀费米气体超流，提出一种针对FermiNet模型的改进方法，获得了极其准确的结果。

    

    理解超流性仍然是凝聚态物理的一个主要目标。在这里，我们利用最近开发的费米神经网络（FermiNet）波函数Ansatz进行变分蒙特卡洛计算来解决这一挑战。我们研究了一个具有强烈短程双体相互作用的系统-- 均匀费米气体，该系统已知存在超流基态，但难以定量描述。我们展示了在研究均匀费米气体时FermiNet Ansatz的关键局限性，并提出了一种简单的修改，其表现显著优于原始FermiNet，可以给出高度准确的结果。我们数学证明了新的Ansatz是原始FermiNet体系结构的严格概括，尽管使用的参数更少。我们的方法与FermiNet共享几个优势:使用神经网络消除了底层基组的需求;网络的灵活性在变分量子Monte Carlo中产生了极其准确的结果。

    Understanding superfluidity remains a major goal of condensed matter physics. Here we tackle this challenge utilizing the recently developed Fermionic neural network (FermiNet) wave function Ansatz for variational Monte Carlo calculations. We study the unitary Fermi gas, a system with strong, short-range, two-body interactions known to possess a superfluid ground state but difficult to describe quantitively. We demonstrate key limitations of the FermiNet Ansatz in studying the unitary Fermi gas and propose a simple modification that outperforms the original FermiNet significantly, giving highly accurate results. We prove mathematically that the new Ansatz is a strict generalization of the original FermiNet architecture, despite the use of fewer parameters. Our approach shares several advantanges with the FermiNet: the use of a neural network removes the need for an underlying basis set; and the flexiblity of the network yields extremely accurate results within a variational quantum Mon
    
[^3]: 自我链式图像语言模型用于视频定位与问答

    Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v1 [cs.CV])

    [http://arxiv.org/abs/2305.06988](http://arxiv.org/abs/2305.06988)

    SeViLA是一个利用单个图像语言模型的框架，在视频定位和问答方面表现出色，通过自我链接策略训练局部化器和回答器模块以定位最具信息的关键帧以回答问题。

    

    最近的研究显示，利用预训练的图像语言模型进行视频问答能够取得良好的结果。虽然这些图像语言模型可以有效启动视频语言模型的表示学习，但它们通常将均匀采样的视频帧作为视觉输入进行串接，而未进行显式的语言感知和时间建模。当视频输入中只有一部分与语言查询相关时，这种均匀帧采样通常会导致重要的视觉线索丢失。尽管人类通常会找到视频中要关注的片段并倒带片刻来回答问题，但训练一个明确的视频片段局部化器通常需要昂贵的注释和高计算成本。为了解决这个问题，我们提出了SeViLA框架，利用单个图像语言模型（BLIP-2）来处理视频的时间关键帧定位和问答。SeViLA框架包括两个模块：局部化器和回答器，两者共享相同的图像语言模型，并通过自我链接策略进行训练，以定位最具信息量的帧以回答给定的问题。在TVQA、TVR和How2QA数据集上的实验结果表明，SeViLA显著优于最先进的方法，且使用更少的参数和注释就能达到竞争性的性能。

    Recent studies have shown promising results on utilizing pre-trained image-language models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos. SeViLA framework consists of two modules: Localizer and A
    
[^4]: 三层神经网络中非线性特征学习的可证保证

    Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks. (arXiv:2305.06986v1 [cs.LG])

    [http://arxiv.org/abs/2305.06986](http://arxiv.org/abs/2305.06986)

    本文研究了三层神经网络的特征学习能力，相比之下，它具有比两层网络更丰富的可证的特征学习能力，并提出了一个通用定理，限制了目标结构的样本复杂度和宽度，以实现低测试误差。

    

    深度学习理论中的一个核心问题是理解神经网络如何学习分层特征。深度网络提取显著特征的能力对其卓越的泛化能力和现代深度学习范式的预训练和微调至关重要。然而，从理论角度来看，这种特征学习过程仍然不够清晰，现有的分析主要局限于两层网络。在本文中，我们展示了三层神经网络具有证明的比两层网络更丰富的特征学习能力。我们分析了通过逐层梯度下降训练的三层网络学习的特征，并提出了一个通用定理，它上界了目标具有特定层次结构时实现低测试错误所需的样本复杂度和宽度。我们将我们的框架实例化到特定的统计学学习设置中——单指数模型和二次函数。

    One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings -- single-index models and functions of quadratic 
    
[^5]: 主动检索增强生成

    Active Retrieval Augmented Generation. (arXiv:2305.06983v1 [cs.CL])

    [http://arxiv.org/abs/2305.06983](http://arxiv.org/abs/2305.06983)

    本论文提出了一种主动检索增强生成的方法，与以往的方法相比，它在生成过程中更紧密地集成了主动检索和生成，并展示了在一组句子生成任务中的性能优势。

    

    尽管大型语言模型（LM）具有理解和生成语言的卓越能力，它们往往会产生虚假的和错误的输出。从外部知识资源中检索信息来增强LM是一种有前途的解决方案。大多数现有的检索增强的LM采用一种检索和生成的设置，仅基于输入一次检索信息。然而，在涉及生成长文本的更普遍的场景中，通过在生成过程中不断地收集信息是至关重要的。过去有一些检索信息，同时生成输出的努力，大多数都是使用前一个上下文作为查询，在固定的时间间隔内检索文档。在这项工作中，我们提供了一个主动检索增强生成的广义视图，即在整个生成过程中主动决定何时以及何时从哪里检索信息的方法。我们提出了前瞻性主动检索增强生成（FLARE），它通过允许生成器在每个步骤中主动查询检索组件来更紧密地集成主动检索和生成。FLARE在一组句子生成任务中优于以前的方法，展示了主动检索在整个生成过程中的好处。

    Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval-augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout the generation process is essential. There have been some past efforts to retrieve information multiple times while generating outputs, which mostly retrieve documents at fixed intervals using the previous context as queries. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval aug
    
[^6]: 机器学习中交叉公平性调查：概念、缓解和挑战

    A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges. (arXiv:2305.06969v1 [cs.LG])

    [http://arxiv.org/abs/2305.06969](http://arxiv.org/abs/2305.06969)

    本篇论文调查了机器学习中交叉公平性的最新进展，提出了分类法和缓解方法，并探讨了未来研究的挑战与方向。

    

    机器学习系统的广泛应用，特别是在更为决策至关重要的应用中，如刑事判决和银行贷款，引发了对公平性的更多关注。已经开发了算法和指标来缓解和衡量这些歧视。最近，一些研究发现了一种更具挑战性的偏见形式，称为交叉偏见，涵盖了多个敏感属性，例如种族和性别。在本调查中，我们回顾了交叉公平性的最新进展，提出了一个交叉公平性和缓解的分类法。最后，我们确定了关键挑战，并为未来的研究指导提供了指导方针。

    The widespread adoption of Machine Learning systems, especially in more decision-critical applications such as criminal sentencing and bank loans, has led to increased concerns about fairness implications. Algorithms and metrics have been developed to mitigate and measure these discriminations. More recently, works have identified a more challenging form of bias called intersectional bias, which encompasses multiple sensitive attributes, such as race and gender, together. In this survey, we review the state-of-the-art in intersectional fairness. We present a taxonomy for intersectional notions of fairness and mitigation. Finally, we identify the key challenges and provide researchers with guidelines for future directions.
    
[^7]: 基于级联交叉关注网络的转换器用于数据有效的整张图像分类

    Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image Classification Using Transformers. (arXiv:2305.06963v1 [cs.CV])

    [http://arxiv.org/abs/2305.06963](http://arxiv.org/abs/2305.06963)

    本论文提出了一种新的级联交叉关注网络（CCAN），以线性复杂度处理大量提取出来的图像块，解决了传统转换器计算复杂度呈平方级增长的问题，该方法用于整张图像分类效果至少与其他方法相当，甚至更好。

    

    整张图像成像技术允许对组织学标本进行高分辨率成像和数字化。深度学习模型对这些图像的自动分析因此需求极高。转换器架构被提出作为有效利用高分辨率信息的可能候选者，把整张图像划分为多个图像块，从这些图像块中提取特征标记。然而，传统转换器允许同时处理大量输入标记，但计算量随着输入标记数和图像块数的增加呈平方级增长。为了解决这个问题，我们提出了一种基于交叉注意力机制的新型级联交叉关注网络（CCAN），其计算复杂度随着提取的图像块数呈线性增长。我们的实验表明，这种架构至少与其他方法相当甚至更好。

    Whole-Slide Imaging allows for the capturing and digitization of high-resolution images of histological specimen. An automated analysis of such images using deep learning models is therefore of high demand. The transformer architecture has been proposed as a possible candidate for effectively leveraging the high-resolution information. Here, the whole-slide image is partitioned into smaller image patches and feature tokens are extracted from these image patches. However, while the conventional transformer allows for a simultaneous processing of a large set of input tokens, the computational demand scales quadratically with the number of input tokens and thus quadratically with the number of image patches. To address this problem we propose a novel cascaded cross-attention network (CCAN) based on the cross-attention mechanism that scales linearly with the number of extracted patches. Our experiments demonstrate that this architecture is at least on-par with and even outperforms other at
    
[^8]: 锚图辅助的深度多视角子空间聚类

    Deep Multi-View Subspace Clustering with Anchor Graph. (arXiv:2305.06939v1 [cs.LG])

    [http://arxiv.org/abs/2305.06939](http://arxiv.org/abs/2305.06939)

    本文提出了锚图辅助的深度多视角子空间聚类方法（DMCAG），该方法可以解决现有DMVSC方法存在的自编码器嵌入子优问题，并通过构建小尺寸锚图来显著减少复杂度。

    

    最近，由于其良好的性能，深度多视角子空间聚类（DMVSC）引起了越来越多的关注。然而，现有的DMVSC方法仍存在两个问题：(1)它们主要依靠自编码器来非线性嵌入数据，而嵌入可能对于聚类来说是次优的，因为聚类目标很少考虑在自编码器中;(2)现有的方法通常具有二次或甚至三次复杂度，这使得处理大规模数据变得困难。为了解决这些问题，本文提出了一种新颖的深度多视角子空间聚类方法，即锚图辅助的深度多视角子空间聚类（DMCAG）。具体而言，DMCAG首先独立地为每个视图学习嵌入特征，这些特征用于获取子空间表示。为了显著降低复杂性，我们为每个视图构建了一个具有小尺寸的锚图。然后，在一个综合锚图上执行谱聚类以获取伪标签。为了克服负面影响......

    Deep multi-view subspace clustering (DMVSC) has recently attracted increasing attention due to its promising performance. However, existing DMVSC methods still have two issues: (1) they mainly focus on using autoencoders to nonlinearly embed the data, while the embedding may be suboptimal for clustering because the clustering objective is rarely considered in autoencoders, and (2) existing methods typically have a quadratic or even cubic complexity, which makes it challenging to deal with large-scale data. To address these issues, in this paper we propose a novel deep multi-view subspace clustering method with anchor graph (DMCAG). To be specific, DMCAG firstly learns the embedded features for each view independently, which are used to obtain the subspace representations. To significantly reduce the complexity, we construct an anchor graph with small size for each view. Then, spectral clustering is performed on an integrated anchor graph to obtain pseudo-labels. To overcome the negativ
    
[^9]: 有限时间半马尔科夫决策进程中遗憾最小化算法的依赖于选项分析。

    An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-Markov Decision Processes. (arXiv:2305.06936v1 [cs.LG])

    [http://arxiv.org/abs/2305.06936](http://arxiv.org/abs/2305.06936)

    本文研究了在有限时间半马尔科夫决策进程中，分层强化学习方法的优化问题，提供了一种通过降低时间分辨率来减少计划时间的选项依赖上界算法，并验证了通过要素框架实现计算成本减少和鲁棒性提高的可行性。

    

    许多真实世界中的强化学习任务都具有复杂和异构的结构，导致端到端学习方式难以应用或甚至不可行。分层强化学习（HRL）通过方便的任务多级分解提供了解决这些问题的通用解决方案，使得它们的解决变得易于访问。虽然实际应用中经常使用，但很少有作品提供理论保证有效地证明这一结果。因此，与标准平面方法相比何时更愿意使用这些方法尚不清楚。在这项工作中，我们为有限时间问题中遗憾最小化算法提供了一个依赖于选项的上界。我们展示了由分层结构引起的时间抽象所带来的计划时间减少，推动了性能的提升。然后，专注于HRL方法的一个子集要素框架，我们强调了要素如何可以有效地减少学习的计算成本和提高其鲁棒性。

    A large variety of real-world Reinforcement Learning (RL) tasks is characterized by a complex and heterogeneous structure that makes end-to-end (or flat) approaches hardly applicable or even infeasible. Hierarchical Reinforcement Learning (HRL) provides general solutions to address these problems thanks to a convenient multi-level decomposition of the tasks, making their solution accessible. Although often used in practice, few works provide theoretical guarantees to justify this outcome effectively. Thus, it is not yet clear when to prefer such approaches compared to standard flat ones. In this work, we provide an option-dependent upper bound to the regret suffered by regret minimization algorithms in finite-horizon problems. We illustrate that the performance improvement derives from the planning horizon reduction induced by the temporal abstraction enforced by the hierarchical structure. Then, focusing on a sub-setting of HRL approaches, the options framework, we highlight how the a
    
[^10]: 人类仍然比ChatGPT更优秀：以IEEEXtreme编程竞赛为例

    Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition. (arXiv:2305.06934v1 [cs.SE])

    [http://arxiv.org/abs/2305.06934](http://arxiv.org/abs/2305.06934)

    本文展示了一个对传统认识的颠覆性观点：在计算机编程领域的典型ChatGPT任务中，人类程序员仍然比ChatGPT更优秀。

    

    自从ChatGPT发布以来，许多研究已经突出了ChatGPT的卓越性能，它在各种任务和领域中往往可以与甚至超越人类能力。然而，本文通过展示人类表现优异的实例，提出了一个对立的观点，特别是在计算机编程领域的典型ChatGPT任务中。我们将IEEExtreme编程挑战赛作为基准进行全面评估，这是一个包含各种复杂度问题的知名国际编程竞赛。为了进行彻底的评估，我们选择了102个来自五个不同IEEExtreme版本的挑战，使用三种主要编程语言Python、Java和C++进行执行。我们的经验分析提供了证据，证明与普遍认为的相反，人类程序员在编程环境中的问题解决的某些方面上仍然具有竞争优势。

    Since the release of ChatGPT, numerous studies have highlighted the remarkable performance of ChatGPT, which often rivals or even surpasses human capabilities in various tasks and domains. However, this paper presents a contrasting perspective by demonstrating an instance where human performance excels in typical tasks suited for ChatGPT, specifically in the domain of computer programming. We utilize the IEEExtreme Challenge competition as a benchmark, a prestigious, annual international programming contest encompassing a wide range of problems with different complexities. To conduct a thorough evaluation, we selected and executed a diverse set of 102 challenges, drawn from five distinct IEEExtreme editions, using three major programming languages: Python, Java, and C++. Our empirical analysis provides evidence that contrary to popular belief, human programmers maintain a competitive edge over ChatGPT in certain aspects of problem-solving within the programming context. In fact, we fou
    
[^11]: 矩阵分解中交替梯度下降的收敛性分析

    Convergence of Alternating Gradient Descent for Matrix Factorization. (arXiv:2305.06927v1 [cs.LG])

    [http://arxiv.org/abs/2305.06927](http://arxiv.org/abs/2305.06927)

    本文提出一种交替梯度下降算法，能够高概率地从非典型随机初始化达到一个$\epsilon$-最优矩阵分解，实验表明该初始化不仅在理论上有益，而且在实践中显著提高了梯度下降的收敛性。

    

    本文考虑了应用于不对称矩阵分解目标的具有固定步长$\eta>0$的交替梯度下降（AGD）。我们证明了，对于秩为$r$的矩阵$\mathbf {A}\in \mathbb {R} ^ {m \times n}$，$T=\left(\left(\frac{\sigma_1(\mathbf{A})}{\sigma_r(\mathbf{A})}\right)^2\log(1/\epsilon)\right)$次交替梯度下降即可从非典型随机初始化高概率地达到$\epsilon$-最优分解$\|\mathbf {A}\mathbf {X}_T^{\vphantom{\intercal}}\mathbf {Y}_T^{\intercal}\|_{\rm F}^2\le\epsilon\|\mathbf {A}\|_{\rm F}^2$。分解中因子的秩为$d>r$，因此$\mathbf{X}_T\in\mathbb{R}^{m \times d}$且$\mathbf{Y}_T\in\mathbb{R}^{n \times d}$。实验表明，我们提出的初始化不仅在理论上有益，而且在实践中显著提高了梯度下降的收敛性。我们的证明概念上很简单：一致的PL不等式和一致的Lipschitz平滑性。

    We consider alternating gradient descent (AGD) with fixed step size $\eta > 0$, applied to the asymmetric matrix factorization objective. We show that, for a rank-$r$ matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, $T = \left( \left(\frac{\sigma_1(\mathbf{A})}{\sigma_r(\mathbf{A})}\right)^2 \log(1/\epsilon)\right)$ iterations of alternating gradient descent suffice to reach an $\epsilon$-optimal factorization $\| \mathbf{A} \mathbf{X}_T^{\vphantom{\intercal}} \mathbf{Y}_T^{\intercal} \|_{\rm F}^2 \leq \epsilon \| \mathbf{A} \|_{\rm F}^2$ with high probability starting from an atypical random initialization. The factors have rank $d>r$ so that $\mathbf{X}_T\in\mathbb{R}^{m \times d}$ and $\mathbf{Y}_T \in\mathbb{R}^{n \times d}$. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves convergence of gradient descent in practice. Our proof is conceptually simple: a uniform PL-inequality and uniform Lipschitz smoothne
    
[^12]: 使用机器学习相互作用势的方法，在大尺度上准确预测锂金属表面和有限温度下的批量性质

    Accurate Surface and Finite Temperature Bulk Properties of Lithium Metal at Large Scales using Machine Learning Interaction Potentials. (arXiv:2305.06925v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2305.06925](http://arxiv.org/abs/2305.06925)

    使用机器学习相互作用势的方法，在大尺度上准确预测锂金属表面和有限温度下的批量性质，克服了传统计算的缺陷，有助于研究锂金属在电池中的应用。

    

    锂金属的性质是设计锂离子和锂金属电池的关键参数。由于锂的高反应性和低熔点以及锂在电池中存在于微观尺度下，很难在实验中进行探测。计算上，缺少能够在所有性质上一致且定量准确的经验势，而从头计算的成本又太高。本工作中，我们使用机器学习相互作用势 (MLIPs) 对密度泛函理论 (DFT) 数据进行训练，以在大长度和时间尺度下以状况-of-the-art 精度复现实验和从头计算的结果。我们准确预测热力学性质、声子光谱、弹性常数的温度依赖性以及各种表面性质，这些性质在 DFT 中无法获得。我们认为不同的 DFT 泛函存在微妙但显着的定量差异，影响了关键性质如表面能。通过 MLIPs 我们克服了这些缺点，实现了对锂金属的大尺度多尺度模拟，这是研究电池中锂金属必需的。

    The properties of lithium metal are key parameters in the design of lithium ion and lithium metal batteries. They are difficult to probe experimentally due to the high reactivity and low melting point of lithium as well as the microscopic scales at which lithium exists in batteries where it is found to have enhanced strength, with implications for dendrite suppression strategies. Computationally, there is a lack of empirical potentials that are consistently quantitatively accurate across all properties and ab-initio calculations are too costly. In this work, we train Machine Learning Interaction Potentials (MLIPs) on Density Functional Theory (DFT) data to state-of-the-art accuracy in reproducing experimental and ab-initio results across a wide range of simulations at large length and time scales. We accurately predict thermodynamic properties, phonon spectra, temperature dependence of elastic constants and various surface properties inaccessible using DFT. We establish that there exis
    
[^13]: 基于模仿学习的算法在现代电力市场中实现先验知识传递以进行贝叶斯纳什均衡估计

    An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation. (arXiv:2305.06924v1 [cs.GT])

    [http://arxiv.org/abs/2305.06924](http://arxiv.org/abs/2305.06924)

    本论文提出了一种基于模仿学习的算法，利用先验知识和与变化环境的交互实现了GENCO的投标策略优化和贝叶斯纳什均衡估计，针对现代电力市场中先验知识未被充分利用导致现有方法不准确和低效的问题进行了改进。

    

    在电力市场的投标游戏中，纳什均衡（NE）估计是发电公司（GENCO）进行投标策略优化和独立系统运营商（ISO）进行市场监视的关键问题。然而，现有的NE估计方法在新兴现代电力市场（FEM）中是不准确和低效的，因为在任何环境变化之前，如负载需求变化、网络拥堵和市场设计的修改，投标策略的先验知识没有充分利用。为此，本文针对FEM开发了Bayes自适应马尔科夫决策过程（BAMDP-FEM），以考虑先验知识来建模GENCO的投标策略优化。随后提出了一种新颖的多智能体生成对抗模仿学习算法（MAGAIL-FEM），使GENCO能够同时从先验知识和与变化环境的交互中进行学习。得到的NE是一种贝叶斯纳什均衡（BNE）。

    The Nash Equilibrium (NE) estimation in bidding games of electricity markets is the key concern of both generation companies (GENCOs) for bidding strategy optimization and the Independent System Operator (ISO) for market surveillance. However, existing methods for NE estimation in emerging modern electricity markets (FEM) are inaccurate and inefficient because the priori knowledge of bidding strategies before any environment changes, such as load demand variations, network congestion, and modifications of market design, is not fully utilized. In this paper, a Bayes-adaptive Markov Decision Process in FEM (BAMDP-FEM) is therefore developed to model the GENCOs' bidding strategy optimization considering the priori knowledge. A novel Multi-Agent Generative Adversarial Imitation Learning algorithm (MAGAIL-FEM) is then proposed to enable GENCOs to learn simultaneously from priori knowledge and interactions with changing environments. The obtained NE is a Bayesian Nash Equilibrium (BNE) with 
    
[^14]: 如何使用强化学习促进未来的电力市场设计？第二部分：方法和应用

    How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications. (arXiv:2305.06921v1 [cs.GT])

    [http://arxiv.org/abs/2305.06921](http://arxiv.org/abs/2305.06921)

    本文开发了一种基于强化学习的模拟方法来联合设计电力市场，详细阐述了设计电力现货市场、辅助服务市场中的保留能力产品和金融市场中的虚拟竞标产品的方法，并通过案例研究演示了如何选择最佳市场设计选项。

    

    本为两部分的论文发展了一种范式理论和详细的方法，利用基于强化学习（RL）的模拟来联合电力市场设计。在第二部分中，通过阐述详细的方法设计电力现货市场（ESM）、辅助服务市场（ASM）中的保留能力产品（RC）和金融市场（FM）中的虚拟竞标（VB）产品来进一步演示这一理论。根据第一部分提出的理论，首先确定联合市场中的市场设计选项。接着，开发了马尔科夫博弈模型，展示了如何将市场设计选项和不确定风险纳入模型公式中。详细阐述了一种多智能体策略近端优化（MAPPO）算法，作为第一部分开发的广义市场模拟方法的实际实现。最后，通过使用一些市场运行绩效指标，案例研究演示如何选择最佳市场设计选项。

    This two-part paper develops a paradigmatic theory and detailed methods of the joint electricity market design using reinforcement-learning (RL)-based simulation. In Part 2, this theory is further demonstrated by elaborating detailed methods of designing an electricity spot market (ESM), together with a reserved capacity product (RC) in the ancillary service market (ASM) and a virtual bidding (VB) product in the financial market (FM). Following the theory proposed in Part 1, firstly, market design options in the joint market are specified. Then, the Markov game model is developed, in which we show how to incorporate market design options and uncertain risks in model formulation. A multi-agent policy proximal optimization (MAPPO) algorithm is elaborated, as a practical implementation of the generalized market simulation method developed in Part 1. Finally, the case study demonstrates how to pick the best market design options by using some of the market operation performance indicators 
    
[^15]: 伪哈密顿系统辨识

    Pseudo-Hamiltonian system identification. (arXiv:2305.06920v1 [eess.SY])

    [http://arxiv.org/abs/2305.06920](http://arxiv.org/abs/2305.06920)

    该论文提出了一种能够在受未知干扰和阻尼影响时学习到内部动态解析项的伪哈密顿系统辨识模型，通过混合模型，即使难以找到扰动解析项，也能够准确地识别出动态，对于其他系统辨识模型无法处理的情况具有重要的应用价值。此外，该论文提出了一种使用四阶对称积分方案的方法，能够提高在噪声数据上的性能表现。

    

    当只有观测数据时，确定物理系统的基本动态可能具有挑战性。本文考虑可以建模为一阶常微分方程的系统。通过假设一定的伪哈密顿形式，即使模型在系统受到未知阻尼和外扰的数据上进行训练，我们也能够学习到内部动态的解析项。在难以找到扰动解析项的情况下，使用神经网络学习这些项的混合模型仍能够准确地识别出系统的动态，就像在理想情况下一样。这使得该模型适用于其他系统辨识模型无法处理的情况。此外，我们提出在损失函数中使用四阶对称积分方案，避免训练中的实际积分，并展示了在噪声数据上如何提高性能的各种示例。

    Identifying the underlying dynamics of physical systems can be challenging when only provided with observational data. In this work, we consider systems that can be modelled as first-order ordinary differential equations. By assuming a certain pseudo-Hamiltonian formulation, we are able to learn the analytic terms of internal dynamics even if the model is trained on data where the system is affected by unknown damping and external disturbances. In cases where it is difficult to find analytic terms for the disturbances, a hybrid model that uses a neural network to learn these can still accurately identify the dynamics of the system as if under ideal conditions. This makes the models applicable in situations where other system identification models fail. Furthermore, we propose to use a fourth-order symmetric integration scheme in the loss function and avoid actual integration in the training, and demonstrate on varied examples how this leads to increased performance on noisy data.
    
[^16]: 元学习用于少样本弱监督医学图像分割

    Meta-Learners for Few-Shot Weakly-Supervised Medical Image Segmentation. (arXiv:2305.06912v1 [cs.CV])

    [http://arxiv.org/abs/2305.06912](http://arxiv.org/abs/2305.06912)

    该论文提出了一个用于医学成像少样本弱监督分割的通用元学习框架，并通过比较实验发现基于度量的元学习在小数据场景下表现更佳。

    

    大多数元学习在视觉识别中的应用往往局限于图像分类，其他任务，如分割和检测的工作相对较少。我们针对医学成像领域提出了一个通用的用于少样本弱监督分割的元学习框架，并比较分析了不同范式的元学习器在不同稀疏标注放射性任务中适用于少样本图像分割的效果。成像模态包括2D胸部、乳腺和牙科X光以及彩超和磁共振成像的2D滑动切片。我们的实验考虑了9种元学习器、4种主干网络和多个目标器官分割任务，探索了具有不同弱标注样式和密度的医学小数据场景。我们的分析表明，在较小的领域变化下，基于度量的元学习方法在分割结果方面优于在元训练数据集中进行培训，而有些方法则可能在面对较大的领域变化时存在困难。

    Most uses of Meta-Learning in visual recognition are very often applied to image classification, with a relative lack of works in other tasks {such} as segmentation and detection. We propose a generic Meta-Learning framework for few-shot weakly-supervised segmentation in medical imaging domains. We conduct a comparative analysis of meta-learners from distinct paradigms adapted to few-shot image segmentation in different sparsely annotated radiological tasks. The imaging modalities include 2D chest, mammographic and dental X-rays, as well as 2D slices of volumetric tomography and resonance images. Our experiments consider a total of 9 meta-learners, 4 backbones and multiple target organ segmentation tasks. We explore small-data scenarios in radiology with varying weak annotation styles and densities. Our analysis shows that metric-based meta-learning approaches achieve better segmentation results in tasks with smaller domain shifts in comparison to the meta-training datasets, while some
    
[^17]: 将因果发现重新解释为预测未观察到的联合统计量的任务

    Reinterpreting causal discovery as the task of predicting unobserved joint statistics. (arXiv:2305.06894v1 [stat.ML])

    [http://arxiv.org/abs/2305.06894](http://arxiv.org/abs/2305.06894)

    研究者提出将因果发现视为预测未观察到联合统计量的任务，这样可以更好地推断未观察到集合的属性。

    

    如果$X,Y,Z$表示随机变量集，不同的数据源可以包含$P_{X,Y}$和$P_{Y,Z}$的样本。我们认为因果发现可以帮助推断“未观察到的联合分布”$P_{X,Y,Z}$或$P_{X,Z}$的性质。这些性质可以是条件独立性（如“整合因果推理”中那样），也可以是关于依赖性的定量说明。更一般地，我们定义了一个学习场景，其中输入是变量的子集，标签是该子集的某些统计属性。共同观测变量集定义了训练点，而未观察到的集合是可能的测试点。为了解决这个学习任务，我们从观察结果中推断出一个因果模型，这个因果模型可以得到未观察到集合的属性。因此，我们可以定义一个因果模型类的VC维，并为预测推导出泛化界限。因此，因果发现变得更加谦逊和易于访问。

    If $X,Y,Z$ denote sets of random variables, two different data sources may contain samples from $P_{X,Y}$ and $P_{Y,Z}$, respectively. We argue that causal discovery can help inferring properties of the `unobserved joint distributions' $P_{X,Y,Z}$ or $P_{X,Z}$. The properties may be conditional independences (as in `integrative causal inference') or also quantitative statements about dependences.  More generally, we define a learning scenario where the input is a subset of variables and the label is some statistical property of that subset. Sets of jointly observed variables define the training points, while unobserved sets are possible test points. To solve this learning task, we infer, as an intermediate step, a causal model from the observations that then entails properties of unobserved sets. Accordingly, we can define the VC dimension of a class of causal models and derive generalization bounds for the predictions.  Here, causal discovery becomes more modest and better accessible 
    
[^18]: 对“去卷积”定义的范畴论元分析

    A Category-theoretical Meta-analysis of Definitions of Disentanglement. (arXiv:2305.06886v1 [cs.LG])

    [http://arxiv.org/abs/2305.06886](http://arxiv.org/abs/2305.06886)

    本文提出了一个存在的去卷积定义的范畴论元分析，将笛卡儿积和幺模积的概念应该构成去卷积的核心，并展现了处理函数、等变映射、关系和随机映射的相似性和关键区别。

    

    在机器学习中，将数据的变化因素分离是一个基本概念，并且不同的研究人员以各种方式研究它，导致了众多的定义。尽管有许多经验研究，但我们仍需要更多的理论研究来充分理解去卷积的定义属性，以及不同的定义之间的关系。本文提出了一个存在的去卷积定义的范畴论元分析，将范畴论作为一个统一而严谨的框架。我们提出笛卡儿积和幺模积的概念应该构成去卷积的核心。有了这些核心概念，我们展示了处理（i）函数，（ii）等变映射，（iii）关系和（iv）随机映射的相似性和关键区别。总的来说，我们的范畴论元分析深化了我们对去卷积及其不同制定的理解，可以帮助研究人员在不同的定义之间进行导航，并选择最合适的定义。

    Disentangling the factors of variation in data is a fundamental concept in machine learning and has been studied in various ways by different researchers, leading to a multitude of definitions. Despite the numerous empirical studies, more theoretical research is needed to fully understand the defining properties of disentanglement and how different definitions relate to each other. This paper presents a meta-analysis of existing definitions of disentanglement, using category theory as a unifying and rigorous framework. We propose that the concepts of the cartesian and monoidal products should serve as the core of disentanglement. With these core concepts, we show the similarities and crucial differences in dealing with (i) functions, (ii) equivariant maps, (iii) relations, and (iv) stochastic maps. Overall, our meta-analysis deepens our understanding of disentanglement and its various formulations and can help researchers navigate different definitions and choose the most appropriate o
    
[^19]: 通过无替换加权抽样进行风险限制财务审计

    Risk-limiting Financial Audits via Weighted Sampling without Replacement. (arXiv:2305.06884v1 [stat.ME])

    [http://arxiv.org/abs/2305.06884](http://arxiv.org/abs/2305.06884)

    本文介绍了一种通过构建新的加权抽样信心序列，对N个未知值的加权平均值进行估计的风险限制财务审计（RLFA）。该方法可以通过合并未知值的附加信息提高生成序列的质量，从而提高估计的准确性和置信度。

    

    我们介绍了风险限制财务审计（RLFA）的概念：在给定误差$\epsilon$和置信度$1-\delta$的情况下，通过构建新的加权抽样信心序列（CSs），对$N$个未知值的加权平均值进行估计。我们利用重要权重的想法构建测试鞅，首先开发了一个框架，以构建任意抽样策略的CSs。接下来，我们开发了一些方法来通过合并与每个项目关联的未知值的附加信息来提高CSs的质量。当附加信息足够具有预测性时，我们表明它可以直接驱动抽样。对于精度未知的情况，我们引入一种通过控制变量使用附加信息的方法。关键的是，我们的构建是自适应的。

    We introduce the notion of a risk-limiting financial auditing (RLFA): given $N$ transactions, the goal is to estimate the total misstated monetary fraction~($m^*$) to a given accuracy $\epsilon$, with confidence $1-\delta$. We do this by constructing new confidence sequences (CSs) for the weighted average of $N$ unknown values, based on samples drawn without replacement according to a (randomized) weighted sampling scheme. Using the idea of importance weighting to construct test martingales, we first develop a framework to construct CSs for arbitrary sampling strategies. Next, we develop methods to improve the quality of CSs by incorporating side information about the unknown values associated with each item. We show that when the side information is sufficiently predictive, it can directly drive the sampling. Addressing the case where the accuracy is unknown a priori, we introduce a method that incorporates side information via control variates. Crucially, our construction is adaptive
    
[^20]: 用于信号处理的凸四元数优化：理论和应用

    Convex Quaternion Optimization for Signal Processing: Theory and Applications. (arXiv:2305.06879v1 [math.OC])

    [http://arxiv.org/abs/2305.06879](http://arxiv.org/abs/2305.06879)

    建立了基于广义哈密尔顿实数计算的凸四元数优化基本理论，并提出了一些判别定理和判别标准，同时通过三个应用验证了理论的实用性。

    

    凸优化方法已被广泛应用于通信和信号处理领域。然而，四元数优化理论目前没有像复数和实数优化理论那样完全发展和系统化。为此，我们基于广义哈密尔顿实数(GHR)计算，建立了凸四元数优化的基本理论，这是与传统的复数和实数优化理论符合的方式。我们提出了五个判别凸四元数函数的定理，和四个强凸四元数函数的判别标准。此外，我们提供了一个基本定理，用于说明凸四元数优化问题的最优性，并通过三个四元数信号处理应用来演示其实用性。这些结果为凸四元数优化提供了坚实的理论基础，并为信号处理领域的进一步发展开启了途径。

    Convex optimization methods have been extensively used in the fields of communications and signal processing. However, the theory of quaternion optimization is currently not as fully developed and systematic as that of complex and real optimization. To this end, we establish an essential theory of convex quaternion optimization for signal processing based on the generalized Hamilton-real (GHR) calculus. This is achieved in a way which conforms with traditional complex and real optimization theory. For rigorous, We present five discriminant theorems for convex quaternion functions, and four discriminant criteria for strongly convex quaternion functions. Furthermore, we provide a fundamental theorem for the optimality of convex quaternion optimization problems, and demonstrate its utility through three applications in quaternion signal processing. These results provide a solid theoretical foundation for convex quaternion optimization and open avenues for further developments in signal pr
    
[^21]: 移动联合学习网络的多层客户端选择

    Multi-Tier Client Selection for Mobile Federated Learning Networks. (arXiv:2305.06865v1 [cs.LG])

    [http://arxiv.org/abs/2305.06865](http://arxiv.org/abs/2305.06865)

    本文提出了一种名为SocFedCS的方法，基于社交网络进行联合学习客户端选择，在移动联合学习网络中最小化成本并训练高质量的模型。

    

    联合学习（FL）通过以分布方式在资源受限的移动设备上训练模型来解决数据隐私问题，已引起广泛关注。然而，在移动联合学习网络（MFLNs）中优化FL客户端选择的问题仍然存在。为了弥补这一差距，我们提出了一种首创的“SocFedCS”方法，即基于社交网络进行联合学习客户端的选择，以最小化成本并训练高质量的FL模型。SocFedCS通过使数据所有者在他们的信任本地网络中传播FL任务信息来丰富候选的FL客户端池，即使设备相互移动，也能够保持其有效性。

    Federated learning (FL), which addresses data privacy issues by training models on resource-constrained mobile devices in a distributed manner, has attracted significant research attention. However, the problem of optimizing FL client selection in mobile federated learning networks (MFLNs), where devices move in and out of each others' coverage and no FL server knows all the data owners, remains open. To bridge this gap, we propose a first-of-its-kind \underline{Soc}ially-aware \underline{Fed}erated \underline{C}lient \underline{S}election (SocFedCS) approach to minimize costs and train high-quality FL models. SocFedCS enriches the candidate FL client pool by enabling data owners to propagate FL task information through their local networks of trust, even as devices are moving into and out of each others' coverage. Based on Lyapunov optimization, we first transform this time-coupled problem into a step-by-step optimization problem. Then, we design a method based on alternating minimiza
    
[^22]: 基于角度信息的神经生存分析模型嵌入空间可视化的通用框架

    A General Framework for Visualizing Embedding Spaces of Neural Survival Analysis Models Based on Angular Information. (arXiv:2305.06862v1 [stat.ML])

    [http://arxiv.org/abs/2305.06862](http://arxiv.org/abs/2305.06862)

    本文提出了一种通用框架，可以可视化任何神经生存分析模型所使用的中间嵌入表示。该框架基于嵌入空间中的锚定方向，可用于表格数据和原始输入（例如图像）。本文说明了在一个基于角度信息的嵌入空间中存在的信息丢失问题，并提供了减少信息丢失的实用方法。

    

    我们提出了一个通用框架，用于可视化神经生存分析模型所使用的任何中间嵌入表示。我们的框架基于嵌入空间中的所谓锚定方向。我们展示了如何使用聚类或用户提供的“概念”来定义原始输入（例如，来自女性患者的特征向量可以编码“女性”概念）来估计这些锚定方向。对于表格数据，我们提供了可视化策略，以显示锚定方向与原始临床特征和生存时间分布之间的关系。然后，我们展示了这些可视化思想如何扩展到处理图像等原始输入。我们的框架是基于查看嵌入空间中向量之间的角度建立的，由于忽略了幅度信息而可能存在“信息丢失”，我们展示了在实践中如何减少这种信息丢失导致的“聚集”伪影。

    We propose a general framework for visualizing any intermediate embedding representation used by any neural survival analysis model. Our framework is based on so-called anchor directions in an embedding space. We show how to estimate these anchor directions using clustering or, alternatively, using user-supplied "concepts" defined by collections of raw inputs (e.g., feature vectors all from female patients could encode the concept "female"). For tabular data, we present visualization strategies that reveal how anchor directions relate to raw clinical features and to survival time distributions. We then show how these visualization ideas extend to handling raw inputs that are images. Our framework is built on looking at angles between vectors in an embedding space, where there could be "information loss" by ignoring magnitude information. We show how this loss results in a "clumping" artifact that appears in our visualizations, and how to reduce this information loss in practice.
    
[^23]: 通过连续方式隐式优化的政策梯度算法

    Policy Gradient Algorithms Implicitly Optimize by Continuation. (arXiv:2305.06851v1 [cs.LG])

    [http://arxiv.org/abs/2305.06851](http://arxiv.org/abs/2305.06851)

    本文提供了政策梯度算法的新理论解释和证明，即政策梯度算法可以通过连续方式隐式优化确定性策略，并指出政策梯度算法探索的实质是计算当前策略收益的连续函数，策略的方差应该是历史依赖性函数。

    

    强化学习中的直接策略优化通常通过政策梯度算法解决，该算法通过随机梯度上升优化策略参数。本文提供了一种新的理论解释和证明这些算法的方法。首先，我们将直接策略优化问题建立在优化连续框架下。后者是一种用于优化非凸函数的框架，其中以连续的替代目标函数序列为基础。其次，我们证明了优化仿射高斯策略并执行熵正则化可以解释为通过连续隐式地优化确定性策略。基于这些理论结果，我们认为政策梯度算法中的探索包括计算当前的策略收益的连续函数，策略的方差应该是历史依赖性函数，以避免局部最值而不是仅仅最大化政策的收益。

    Direct policy optimization in reinforcement learning is usually solved with policy-gradient algorithms, which optimize policy parameters via stochastic gradient ascent. This paper provides a new theoretical interpretation and justification of these algorithms. First, we formulate direct policy optimization in the optimization by continuation framework. The latter is a framework for optimizing nonconvex functions where a sequence of surrogate objective functions, called continuations, are locally optimized. Second, we show that optimizing affine Gaussian policies and performing entropy regularization can be interpreted as implicitly optimizing deterministic policies by continuation. Based on these theoretical results, we argue that exploration in policy-gradient algorithms consists in computing a continuation of the return of the policy at hand, and that the variance of policies should be history-dependent functions adapted to avoid local extrema rather than to maximize the return of th
    
[^24]: 通过条件神经场在时空预测中将时间融入到通用方法中

    A Generic Approach to Integrating Time into Spatial-Temporal Forecasting via Conditional Neural Fields. (arXiv:2305.06827v1 [cs.LG])

    [http://arxiv.org/abs/2305.06827](http://arxiv.org/abs/2305.06827)

    本文提出了一种将时间组件融入预测模型的通用方法，通过使用条件神经场来表示辅助特征，解决了在利用时间序列进行预测时存在的一个未解决问题。

    

    自我意识是自主系统的关键能力，例如自主驾驶网络，需要高效的时间序列预测算法，使系统能够推断环境的未来状态以及其随时间推移对系统行为的影响。最近，大量利用卷积神经网络和图神经网络的预测算法已被开发出来，以利用时间序列中存在的复杂时空依赖关系。虽然这些解决方案在统计方法上显示出了显著的优势，但一个未解决的问题是如何有效地将表示季节性模式的全局信息通过时间序列的时间组件整合到预测模型中以提高其精度。本文提出了一种将时间组件融入预测模型的通用方法。其主要思想是使用条件神经场来表示辅助特征。

    Self-awareness is the key capability of autonomous systems, e.g., autonomous driving network, which relies on highly efficient time series forecasting algorithm to enable the system to reason about the future state of the environment, as well as its effect on the system behavior as time progresses. Recently, a large number of forecasting algorithms using either convolutional neural networks or graph neural networks have been developed to exploit the complex temporal and spatial dependencies present in the time series. While these solutions have shown significant advantages over statistical approaches, one open question is to effectively incorporate the global information which represents the seasonality patterns via the time component of time series into the forecasting models to improve their accuracy. This paper presents a general approach to integrating the time component into forecasting models. The main idea is to employ conditional neural fields to represent the auxiliary feature
    
[^25]: 多智能体强化学习中的信息设计

    Information Design in Multi-Agent Reinforcement Learning. (arXiv:2305.06807v1 [cs.GT])

    [http://arxiv.org/abs/2305.06807](http://arxiv.org/abs/2305.06807)

    本文探究了多智能体强化学习中的信息设计问题及其挑战，提出了“马尔科夫信令博弈”的概念。

    

    强化学习（RL）模仿人类和动物与环境交互的方式。然而实际环境中存在其他有自己目标的智能体，它们会适应地与自己相互作用。因此，为了在这些环境中成功，自主智能体需要影响其他智能体以使它们的行为更有益。信息设计是影响其他智能体行为的一种方法。本文探讨了针对一组RL代理的信息设计问题，并提出了“马尔科夫信令博弈”的概念。

    Reinforcement learning (RL) mimics how humans and animals interact with the environment. The setting is somewhat idealized because, in actual tasks, other agents in the environment have their own goals and behave adaptively to the ego agent. To thrive in those environments, the agent needs to influence other agents so their actions become more helpful and less harmful. Research in computational economics distills two ways to influence others directly: by providing tangible goods (mechanism design) and by providing information (information design). This work investigates information design problems for a group of RL agents. The main challenges are two-fold. One is the information provided will immediately affect the transition of the agent trajectories, which introduces additional non-stationarity. The other is the information can be ignored, so the sender must provide information that the receivers are willing to respect. We formulate the Markov signaling game, and develop the notions 
    
[^26]: 数据驱动政策细化的理论研究

    Towards Theoretical Understanding of Data-Driven Policy Refinement. (arXiv:2305.06796v1 [cs.LG])

    [http://arxiv.org/abs/2305.06796](http://arxiv.org/abs/2305.06796)

    本文介绍了一种数据驱动的强化学习政策细化方法，用于改进安全关键应用的策略，并提出了一系列定理验证其收敛性、鲁棒性界限、泛化误差和对模型错误的弹性。

    

    本文介绍了一种为安全关键应用量身定制的强化学习数据驱动政策细化方法。我们的方法利用数据驱动优化和强化学习的优势，通过迭代改进来增强策略的安全性和优化性。我们的主要贡献在于这个数据驱动政策细化概念的数学表述。该方法通过从数据驱动验证中浮现的反例学习，系统地改进强化学习策略。此外，我们提出了一系列定理，阐明了我们方法的关键理论性质，包括收敛性、鲁棒性界限、泛化误差和对模型错误的弹性。这些结果不仅验证了我们方法的有效性，而且有助于更深入地理解这个方法在不同环境和情况下的行为。

    This paper presents an approach for data-driven policy refinement in reinforcement learning, specifically designed for safety-critical applications. Our methodology leverages the strengths of data-driven optimization and reinforcement learning to enhance policy safety and optimality through iterative refinement. Our principal contribution lies in the mathematical formulation of this data-driven policy refinement concept. This framework systematically improves reinforcement learning policies by learning from counterexamples surfaced during data-driven verification. Furthermore, we present a series of theorems elucidating key theoretical properties of our approach, including convergence, robustness bounds, generalization error, and resilience to model mismatch. These results not only validate the effectiveness of our methodology but also contribute to a deeper understanding of its behavior in different environments and scenarios.
    
[^27]: 在神经网络模型中集成最近邻居以估计治疗效果

    Integrating nearest neighbors on neural network models for treatment effect estimation. (arXiv:2305.06789v1 [stat.ML])

    [http://arxiv.org/abs/2305.06789](http://arxiv.org/abs/2305.06789)

    本论文提出了一种新的方法NNCI，用于将最近邻居信息集成到神经网络模型中，以更准确地估计治疗效果。

    

    治疗效果估计对于许多科学和工业领域的研究人员和从业者来说具有高度重要性。观察数据的丰富性使它们越来越受到研究人员用于因果效应的估计。然而，这些数据存在偏差和其他弱点，导致如果不正确处理，估计因果效应会不准确。因此，提出了几种机器学习技术，其中大部分都专注于利用神经网络模型的预测能力，以达到更精确的因果效应估计。在本文中，我们提出了一种名为最近邻居信息用于因果推断（NNCI）的新方法，用于将有价值的最近邻居信息集成到基于神经网络的模型中，以估计治疗效果。提出的NNCI方法被应用于一些最广泛使用的基于神经网络的治疗效果估计模型，其使用观察数据。

    Treatment effect estimation is of high-importance for both researchers and practitioners across many scientific and industrial domains. The abundance of observational data makes them increasingly used by researchers for the estimation of causal effects. However, these data suffer from biases, from several weaknesses, leading to inaccurate causal effect estimations, if not handled properly. Therefore, several machine learning techniques have been proposed, most of them focusing on leveraging the predictive power of neural network models to attain more precise estimation of causal effects. In this work, we propose a new methodology, named Nearest Neighboring Information for Causal Inference (NNCI), for integrating valuable nearest neighboring information on neural network-based models for estimating treatment effects. The proposed NNCI methodology is applied to some of the most well established neural network-based models for treatment effect estimation with the use of observational data
    
[^28]: 基于拍卖的联邦学习中数据消费者的效用最大化竞标策略

    Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning. (arXiv:2305.06784v1 [cs.LG])

    [http://arxiv.org/abs/2305.06784](http://arxiv.org/abs/2305.06784)

    本文提出了一种首个效用最大化竞标策略（Fed-Bidder），使多个FL数据消费者可以通过AFL有效而高效地竞争数据拥有者。

    

    基于拍卖的联邦学习（AFL）因通过经济手段激励数据拥有者加入FL而受到广泛的研究兴趣。现有工作假设在AFL市场上仅存在一个数据消费者和多个数据拥有者（即垄断市场）。因此，数据拥有者竞标加入数据消费者进行FL。但是，在实际的AFL市场中，多个数据消费者可能会竞争以吸引数据拥有者加入他们各自的FL任务，这种假设是不现实的。本文提出了一种首个效用最大化竞标策略（Fed-Bidder），使多个FL数据消费者可以通过AFL有效而高效地竞争数据拥有者，并提供了能够容纳不同市场动态的各种获胜函数的效用估计能力。基于六个常用基准数据集的广泛实验表明了策略的有效性和可扩展性。

    Auction-based Federated Learning (AFL) has attracted extensive research interest due to its ability to motivate data owners to join FL through economic means. Existing works assume that only one data consumer and multiple data owners exist in an AFL marketplace (i.e., a monopoly market). Therefore, data owners bid to join the data consumer for FL. However, this assumption is not realistic in practical AFL marketplaces in which multiple data consumers can compete to attract data owners to join their respective FL tasks. In this paper, we bridge this gap by proposing a first-of-its-kind utility-maximizing bidding strategy for data consumers in federated learning (Fed-Bidder). It enables multiple FL data consumers to compete for data owners via AFL effectively and efficiently by providing with utility estimation capabilities which can accommodate diverse forms of winning functions, each reflecting different market dynamics. Extensive experiments based on six commonly adopted benchmark dat
    
[^29]: 采用自适应数据采集和基于NeREF的反射校正生成高质量的3DMPCs，促进高效植物表型分析

    Generating high-quality 3DMPCs by adaptive data acquisition and NeREF-based reflectance correction to facilitate efficient plant phenotyping. (arXiv:2305.06777v1 [eess.IV])

    [http://arxiv.org/abs/2305.06777](http://arxiv.org/abs/2305.06777)

    本研究提出了自适应数据采集和反射校正方法生成高质量的3DMPCs，用于高效植物表型分析。

    

    使用高质量的三维（3D）和多光谱数据对植物表型特征进行非破坏性评估可以加深育种者对植物生长的理解，并使他们能够做出知情管理决策。然而，在自然光照条件下，主观视角选择和复杂的光照效应会降低数据质量，增加解决表型参数的难度。我们提出了自适应数据采集和反射校正的方法，以分别生成植物的高质量3D多光谱点云（3DMPCs）。在第一阶段，我们提出了一种基于新型UGV平台和多传感器装备的机器人臂的高效的下一个最佳视角（NBV）规划方法。在第二阶段，我们通过使用神经参考场（NeREF）来预测参考的数字（DN）来消除光照效应。我们在6个紫苏和6个番茄植株上进行了测试，并选择了2片可见叶和4个感兴趣的区域。

    Non-destructive assessments of plant phenotypic traits using high-quality three-dimensional (3D) and multispectral data can deepen breeders' understanding of plant growth and allow them to make informed managerial decisions. However, subjective viewpoint selection and complex illumination effects under natural light conditions decrease the data quality and increase the difficulty of resolving phenotypic parameters. We proposed methods for adaptive data acquisition and reflectance correction respectively, to generate high-quality 3D multispectral point clouds (3DMPCs) of plants. In the first stage, we proposed an efficient next-best-view (NBV) planning method based on a novel UGV platform with a multi-sensor-equipped robotic arm. In the second stage, we eliminated the illumination effects by using the neural reference field (NeREF) to predict the digital number (DN) of the reference. We tested them on 6 perilla and 6 tomato plants, and selected 2 visible leaves and 4 regions of interest
    
[^30]: 振动数据集统计特征的聚类算法比较

    Comparison of Clustering Algorithms for Statistical Features of Vibration Data Sets. (arXiv:2305.06753v1 [cs.LG])

    [http://arxiv.org/abs/2305.06753](http://arxiv.org/abs/2305.06753)

    本文通过比较K均值聚类、OPTICS和高斯混合模型聚类算法对振动数据集的统计特征进行聚类，发现平均值和方差类特征特别重要，PCA特征选择可以提高聚类性能，GMM是最优算法，但需要更多的计算资源。

    

    基于振动的状态监测系统由于捕获宽频范围内的动态特征而受到越来越多的关注，能够准确地识别不同的状态。然而，在振动数据中聚类方法的研究还很少，所得到的解决方案通常只针对单个数据集进行优化。在本研究中，我们对来自于振动数据集时间和频域的统计特征采用了K均值聚类、OPTICS和高斯混合模型聚类（GMM）进行广泛的比较。此外，我们研究了特征组合、使用主成分分析（PCA）进行的特征选择以及指定数量的聚类对聚类算法性能的影响。我们使用三种不同的基准数据集通过网格搜索进行了比较。我们的研究表明，均值（平均值，中位数）和基于方差（标准差）的特征对于聚类振动数据非常关键，而PCA特征选择可以提高聚类性能。此外，我们发现GMM在准确性和稳定性方面优于K均值和OPTICS，但需要更多的计算资源。

    Vibration-based condition monitoring systems are receiving increasing attention due to their ability to accurately identify different conditions by capturing dynamic features over a broad frequency range. However, there is little research on clustering approaches in vibration data and the resulting solutions are often optimized for a single data set. In this work, we present an extensive comparison of the clustering algorithms K-means clustering, OPTICS, and Gaussian mixture model clustering (GMM) applied to statistical features extracted from the time and frequency domains of vibration data sets. Furthermore, we investigate the influence of feature combinations, feature selection using principal component analysis (PCA), and the specified number of clusters on the performance of the clustering algorithms. We conducted this comparison in terms of a grid search using three different benchmark data sets. Our work showed that averaging (Mean, Median) and variance-based features (Standard 
    
[^31]: 研究基于能量的神经网络的生成动力学

    Investigating the generative dynamics of energy-based neural networks. (arXiv:2305.06745v1 [cs.NE])

    [http://arxiv.org/abs/2305.06745](http://arxiv.org/abs/2305.06745)

    本文系统地研究了受限玻尔兹曼机(RBM)的生成动力学，发现从嵌合态开始自上而下采样可以增加生成多样的数据原型的能力。

    

    生成神经网络可以根据其所训练的分布的统计属性生成数据样本。这个特性可以用来测试现代计算神经科学假说，即自发的脑活动在一定程度上得到了自上而下的生成处理的支持。对于一类广泛研究的生成模型——受限玻尔兹曼机(RBMs)，我们系统地探索了它的生成动力学，表征了自上而下的抽样过程中访问的状态数量，并探究了从有偏向隐层状态开始产生生成过程是否能增加访问吸引子的异质性。通过考虑在经典手写数字数据集上训练的RBMs，我们展示了通过从嵌合态开始自上而下采样可以增加生成多样的数据原型的能力。

    Generative neural networks can produce data samples according to the statistical properties of their training distribution. This feature can be used to test modern computational neuroscience hypotheses suggesting that spontaneous brain activity is partially supported by top-down generative processing. A widely studied class of generative models is that of Restricted Boltzmann Machines (RBMs), which can be used as building blocks for unsupervised deep learning architectures. In this work, we systematically explore the generative dynamics of RBMs, characterizing the number of states visited during top-down sampling and investigating whether the heterogeneity of visited attractors could be increased by starting the generation process from biased hidden states. By considering an RBM trained on a classic dataset of handwritten digits, we show that the capacity to produce diverse data prototypes can be increased by initiating top-down sampling from chimera states, which encode high-level vis
    
[^32]: 针对线性和非线性重尾多臂老虎机的隐式范数预测器的修剪

    Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits. (arXiv:2305.06743v1 [cs.LG])

    [http://arxiv.org/abs/2305.06743](http://arxiv.org/abs/2305.06743)

    本文提出了一种针对奖励分布重尾的MAB问题的隐式规范化预测器，证明该方法在线性和非线性重尾随机MAB问题上是最优的。

    

    已知隐式范数预测器（在线镜像下降，以Tsallis熵作为prox函数）是对抗性多臂老虎机问题（MAB）的最佳算法。但是，大多数复杂性结果都依赖于有界奖励或其他限制性假设。最近有关最佳二者结合算法的研究已经针对对手性和随机重尾MAB设置进行了探讨。这个算法在这两种情况下都是最优的，但不能充分利用数据。在本文中，我们针对奖励分布重尾的MAB问题提出了带剪辑的隐式规范化预测器。我们在奖励分布上提出渐进收敛性结果，并证明所提出的方法对于线性和非线性重尾随机MAB问题是最优的。我们还证明了与最好的二者结合算法相比，该算法通常表现更好。

    Implicitly Normalized Forecaster (online mirror descent with Tsallis entropy as prox-function) is known to be an optimal algorithm for adversarial multi-armed problems (MAB). However, most of the complexity results rely on bounded rewards or other restrictive assumptions. Recently closely related best-of-both-worlds algorithm were proposed for both adversarial and stochastic heavy-tailed MAB settings. This algorithm is known to be optimal in both settings, but fails to exploit data fully. In this paper, we propose Implicitly Normalized Forecaster with clipping for MAB problems with heavy-tailed distribution on rewards. We derive convergence results under mild assumptions on rewards distribution and show that the proposed method is optimal for both linear and non-linear heavy-tailed stochastic MAB problems. Also we show that algorithm usually performs better compared to best-of-two-worlds algorithm.
    
[^33]: IVP-VAE: 利用初值问题求解器对电子病历时间序列进行建模

    IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers. (arXiv:2305.06741v1 [cs.LG])

    [http://arxiv.org/abs/2305.06741](http://arxiv.org/abs/2305.06741)

    本文提出了一种新的方法，在建模电子病历时间序列时，利用直接近似IVP的过程来消除递归计算，从而提高计算效率和训练速度。与目前基于IVP求解器和递归神经网络方法相比，本方法可以达到类似的分类和预测性能。

    

    连续时间模型（例如神经ODE和神经流量）在分析电子病历中常见的不规则采样时间序列方面显示出有希望的结果。 基于这些模型，时间序列通常在变分自动编码器架构中通过初值问题（IVP）求解器和递归神经网络的混合处理。 顺序求解IVP使得这样的模型在计算效率上不够高。 本文提出了一种纯粹使用连续过程对时间序列进行建模的方法，其状态演变可以通过IVP直接近似。 这消除了递归计算的需要，并允许多个状态并行演变。 我们进一步通过一种基于其可逆性的IVP求解器融合编码器和解码器，这导致参数更少，收敛更快。 在三个真实世界的数据集上进行的实验表明，所提出的方法在获得更快的训练速度的同时，仍然可以获得较高的分类性能和预测性能。

    Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver based on its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed approach achieves comparable extrapolation and classification performance while gaining more 
    
[^34]: MRI中深度学习用于回顾性运动校正的综述

    Deep Learning for Retrospective Motion Correction in MRI: A Comprehensive Review. (arXiv:2305.06739v1 [eess.IV])

    [http://arxiv.org/abs/2305.06739](http://arxiv.org/abs/2305.06739)

    该综述针对MRI中的运动问题，综述了基于深度学习对运动校正的方法，发现了不同应用之间的差异和共同点，在未来的方向上提出了建议。

    

    运动是磁共振成像(MRI)中的主要挑战之一。由于MR信号在频率空间中获取，任何成像物体的运动都会导致复杂的伪影，此外还会产生其他MR成像伪影。深度学习已经经常被提出用于重建过程的多个阶段中的运动校正。MRI采集序列的广泛应用，感兴趣的解剖学和病理学以及运动模式（刚性vs可变形和随机vs规律性）使得全面性解决方案不太可能。为了促进不同应用之间的思想传递，该综述提供了一份详细的基于学习的MRI运动校正方法概述及其常见挑战和潜力。该综述识别了不同应用之间的数据使用、体系结构和评估策略的差异和协同作用。我们批判性地讨论了大趋势并概述了未来的方向，旨在增强MRI中深度学习的回顾性运动校正能力。

    Motion represents one of the major challenges in magnetic resonance imaging (MRI). Since the MR signal is acquired in frequency space, any motion of the imaged object leads to complex artefacts in the reconstructed image in addition to other MR imaging artefacts. Deep learning has been frequently proposed for motion correction at several stages of the reconstruction process. The wide range of MR acquisition sequences, anatomies and pathologies of interest, and motion patterns (rigid vs. deformable and random vs. regular) makes a comprehensive solution unlikely. To facilitate the transfer of ideas between different applications, this review provides a detailed overview of proposed methods for learning-based motion correction in MRI together with their common challenges and potentials. This review identifies differences and synergies in underlying data usage, architectures and evaluation strategies. We critically discuss general trends and outline future directions, with the aim to enhan
    
[^35]: NUBO：一个透明的 Python 包用于贝叶斯优化

    NUBO: A Transparent Python Package for Bayesian Optimisation. (arXiv:2305.06709v1 [cs.LG])

    [http://arxiv.org/abs/2305.06709](http://arxiv.org/abs/2305.06709)

    NUBO是一个透明的Python包，用于优化昂贵的黑盒函数，它利用高斯过程做代理模型以及获取函数来指导选择候选点，专注于透明度和用户体验。

    

    NUBO（Newcastle University Bayesian Optimisation）是一个贝叶斯优化框架，用于优化昂贵的黑盒函数，比如物理实验和计算机模拟器。它利用高斯过程做代理模型、并通过获取函数来选择用于全局最优化的候选点。NUBO专注于透明度和用户体验，以便让不同领域的研究人员更容易使用贝叶斯优化。

    NUBO, short for Newcastle University Bayesian Optimisation, is a Bayesian optimisation framework for the optimisation of expensive-to-evaluate black-box functions, such as physical experiments and computer simulators. Bayesian optimisation is a cost-efficient optimisation strategy that uses surrogate modelling via Gaussian processes to represent an objective function and acquisition functions to guide the selection of candidate points to approximate the global optimum of the objective function. NUBO itself focuses on transparency and user experience to make Bayesian optimisation easily accessible to researchers from all disciplines. Clean and understandable code, precise references, and thorough documentation ensure transparency, while user experience is ensured by a modular and flexible design, easy-to-write syntax, and careful selection of Bayesian optimisation algorithms. NUBO allows users to tailor Bayesian optimisation to their specific problem by writing the optimisation loop the
    
[^36]: 基于RIOHTrack的人工智能模型实现沥青路面车辙深度的短时预测

    A data-driven rutting depth short-time prediction model with metaheuristic optimization for asphalt pavements based on RIOHTrack. (arXiv:2305.06707v1 [cs.AI])

    [http://arxiv.org/abs/2305.06707](http://arxiv.org/abs/2305.06707)

    本研究利用人工智能模型预测不同沥青路面的车辙深度，采用复杂网络方法进行结构元素选择，并使用带有RCO修正的ELM算法进行短时预测。

    

    对于各种道路设计指南来说，沥青路面的车辙深度是一个关键的设计标准。本文试图使用人工智能模型来估计不同沥青路面的车辙深度剪辑，温度以及负载轴作为主要特征。实验数据来自施加不同原油来源的19个沥青路面在通州市的2.038公里长的全尺寸加速路面试验线路(RIOHTrack, Road Track Institute)中所获取。此外，本文还通过复杂网络方法和Louvain算法对不同的路面车辙深度构建复杂网络，可以从中选择最重要的结构元素，并找到相似的结构元素。采用带残差修正优化(Residual Corrective Optimization, RCO)的极限学习机算法(ELM)用于短时间内预测沥青路面的车辙深度。

    Rutting of asphalt pavements is a crucial design criterion in various pavement design guides. A good road transportation base can provide security for the transportation of oil and gas in road transportation. This study attempts to develop a robust artificial intelligence model to estimate different asphalt pavements' rutting depth clips, temperature, and load axes as primary characteristics. The experiment data were obtained from 19 asphalt pavements with different crude oil sources on a 2.038 km long full-scale field accelerated pavement test track (RIOHTrack, Road Track Institute) in Tongzhou, Beijing. In addition, this paper also proposes to build complex networks with different pavement rutting depths through complex network methods and the Louvain algorithm for community detection. The most critical structural elements can be selected from different asphalt pavement rutting data, and similar structural elements can be found. An extreme learning machine algorithm with residual cor
    
[^37]: 滞后多因子模型中领先滞后关系的鲁棒检测

    Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models. (arXiv:2305.06704v1 [stat.ML])

    [http://arxiv.org/abs/2305.06704](http://arxiv.org/abs/2305.06704)

    该论文提出了一种基于聚类的鲁棒检测滞后多因子模型中的领先滞后关系方法，并使用各种聚类技术和相似度度量方法实现了对领先滞后估计的聚合，从而强化了对原始宇宙中的一致关系的识别。

    

    在多元时间序列系统中，通过发现数据中固有的领先滞后关系，可以获得关键信息，这指的是两个相对时间互移的时间序列之间的依赖关系，可以用于控制、预测或聚类。我们开发了一种基于聚类的方法，用于鲁棒检测滞后多因子模型中的领先滞后关系。在我们的框架中，所设想的管道接收一组时间序列作为输入，并使用滑动窗口方法从每个输入时间序列中提取一组子序列时间序列。然后，我们应用各种聚类技术（例如K-means++和谱聚类），采用各种成对相似性度量，包括非线性的相似性度量。一旦聚类被提取出来，跨聚类的领先滞后估计被聚合起来，以增强对原始宇宙中一致关系的识别。由于多

    In multivariate time series systems, key insights can be obtained by discovering lead-lag relationships inherent in the data, which refer to the dependence between two time series shifted in time relative to one another, and which can be leveraged for the purposes of control, forecasting or clustering. We develop a clustering-driven methodology for the robust detection of lead-lag relationships in lagged multi-factor models. Within our framework, the envisioned pipeline takes as input a set of time series, and creates an enlarged universe of extracted subsequence time series from each input time series, by using a sliding window approach. We then apply various clustering techniques (e.g, K-means++ and spectral clustering), employing a variety of pairwise similarity measures, including nonlinear ones. Once the clusters have been extracted, lead-lag estimates across clusters are aggregated to enhance the identification of the consistent relationships in the original universe. Since multi
    
[^38]: 竞争风险的单调神经网络：用于生存分析的模型

    Neural Fine-Gray: Monotonic neural networks for competing risks. (arXiv:2305.06703v1 [cs.LG])

    [http://arxiv.org/abs/2305.06703](http://arxiv.org/abs/2305.06703)

    本文提出了一种使用单调约束神经网络模拟每种竞争生存率分布的方法，从而确保可以在计算成本下实现精确的最大似然值最优化，该方法可以用于生存分析领域。

    

    生存分析是一种处理患者因未经历感兴趣事件而出现的“censoring”的时间至事件模型。机器学习算法在此类问题中表现突出，但往往忽略了竞争风险对感兴趣事件的影响，从而导致生存率估计存在偏差。本文提出了一种使用单调约束神经网络模拟每种竞争生存率分布的方法，确保采用自动微分方法能够在计算成本下实现精确的最大似然值最优化。通过效果实验对比完成了一个合成数据和三个医学数据集的生存数据分析。最后讨论了在开发医疗实践风险评估指标时考虑竞争风险的意义。

    Time-to-event modelling, known as survival analysis, differs from standard regression as it addresses censoring in patients who do not experience the event of interest. Despite competitive performances in tackling this problem, machine learning methods often ignore other competing risks that preclude the event of interest. This practice biases the survival estimation. Extensions to address this challenge often rely on parametric assumptions or numerical estimations leading to sub-optimal survival approximations. This paper leverages constrained monotonic neural networks to model each competing survival distribution. This modelling choice ensures the exact likelihood maximisation at a reduced computational cost by using automatic differentiation. The effectiveness of the solution is demonstrated on one synthetic and three medical datasets. Finally, we discuss the implications of considering competing risks when developing risk scores for medical practice.
    
[^39]: 深度视觉和遗传生物测定用于少量图像数据珍稀物种分类

    Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species. (arXiv:2305.06695v1 [cs.CV])

    [http://arxiv.org/abs/2305.06695](http://arxiv.org/abs/2305.06695)

    本文提出了一种利用对齐的视觉-遗传推理空间来提高少量图像数据珍稀物种分类的方法，该方法通过深度嵌入模型实现对齐，适用于提高稀有物种的长尾识别，并且可以显著有益于仅基于视觉的稀有物种识别。

    

    在生物应用中，视觉和遗传生物测定通常用于识别物种和个体。然而，在计算上增强少量图像数据稀有类别的视觉分类方面，该领域尚未进行尝试。因此，本文提出了对齐的视觉-遗传推理空间，旨在隐式编码跨域关联以提高性能。我们首次证明了这种对齐可以通过深度嵌入模型实现，并且该方法直接适用于提高稀有物种的长尾识别（LTR）。我们通过应用于32个物种、超过30,000个浮游有孔虫壳的显微图像并与独立的遗传数据样本一起使用来实验室展现了该概念的效力。最重要的是，对从业者而言，我们展示了视觉-遗传对齐可以显著有益于仅基于视觉的稀有物种识别。

    Visual as well as genetic biometrics are routinely employed to identify species and individuals in biological applications. However, no attempts have been made in this domain to computationally enhance visual classification of rare classes with little image data via genetics. In this paper, we thus propose aligned visual-genetic inference spaces with the aim to implicitly encode cross-domain associations for improved performance. We demonstrate for the first time that such alignment can be achieved via deep embedding models and that the approach is directly applicable to boosting long-tailed recognition (LTR) particularly for rare species. We experimentally demonstrate the efficacy of the concept via application to microscopic imagery of 30k+ planktic foraminifer shells across 32 species when used together with independent genetic data samples. Most importantly for practitioners, we show that visual-genetic alignment can significantly benefit visual-only recognition of the rarest speci
    
[^40]: INGENIOUS：使用信息丰富的数据子集对大型语言模型进行高效预训练

    INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])

    [http://arxiv.org/abs/2305.06677](http://arxiv.org/abs/2305.06677)

    本文提出了一种使用信息丰富的数据子集来高效预训练大型语言模型的方法，减少了训练时间和计算成本，同时保持了模型的泛化能力。

    

    大型预训练语言模型的显着特点是在其泛化能力和新能力方面随着模型容量和预训练数据集大小的增加而 achieved. 然而，必须认识到这不可避免地导致了过长的训练时间、过高的计算成本和有害的环境影响。本文提出了一种方法，即是否可能仅使用高度信息丰富的训练数据子集来训练 PTLM，并同时保持其下游性能？

    A salient characteristic of large pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we 
    
[^41]: 关于MLE作为Exp3算法学习率估计器的收敛性

    On the convergence of the MLE as an estimator of the learning rate in the Exp3 algorithm. (arXiv:2305.06660v1 [cs.LG])

    [http://arxiv.org/abs/2305.06660](http://arxiv.org/abs/2305.06660)

    本文旨在证明：在Exp3算法中，如果学习率是恒定的，则MLE不能有效估计学习率；而如果学习率随样本量的增加呈多项式下降，则MLE的预测误差在概率上满足随样本量多项式下降的边界。

    

    当把个体的学习数据拟合成类似算法的学习模型时，观测值非常相互依赖且非平稳，以至于人们可能会想知道即使是实验性认知的常规工具——经典最大似然估计器（MLE）能够做些什么。本文的目的是展示如果Exp3算法中的学习率是恒定的，则学习率的估计不可能是有效的，另外，如果学习率随着样本量的增加呈多项式下降，则MLE的预测误差在概率上满足随样本量多项式下降的边界。

    When fitting the learning data of an individual to algorithm-like learning models, the observations are so dependent and non-stationary that one may wonder what the classical Maximum Likelihood Estimator (MLE) could do, even if it is the usual tool applied to experimental cognition. Our objective in this work is to show that the estimation of the learning rate cannot be efficient if the learning rate is constant in the classical Exp3 (Exponential weights for Exploration and Exploitation) algorithm. Secondly, we show that if the learning rate decreases polynomially with the sample size, then the prediction error and in some cases the estimation error of the MLE satisfy bounds in probability that decrease at a polynomial rate.
    
[^42]: 实用的鲁棒性强化学习：相邻不确定性集和双代理算法

    On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm. (arXiv:2305.06657v1 [cs.LG])

    [http://arxiv.org/abs/2305.06657](http://arxiv.org/abs/2305.06657)

    本文提出了一个名为ARQ-Learning的鲁棒性强化学习算法，采用了一个更加实际的不确定性集，并提出了一种称为“悲观代理”的方法。该算法在表格化的情况下获得了与现有算法相同的快速收敛速度，并为实际应用提供了更好的鲁棒性。而且，本文还首次提出了用于深度Q网络和深度确定策略梯度的鲁棒RL算法PR-DQN和PR-DDPG。

    

    鲁棒性强化学习（RL）旨在学习一个策略，该策略在一个不确定性集上优化最差性能。给定一个产生训练样本的标准马尔可夫决策过程（N-MDP），该集合包含通过对N-MDP进行某些扰动而获得的MDP。本文引入了一个新的不确定性集，其中包含比现有集合更实际的MDP。使用这个不确定性集，我们提出了一个鲁棒RL算法，名为ARQ-Learning，用于表格化的情况。此外，我们表征了有限时间的误差界并证明它与Q-Learning和鲁棒Q-Learning（即现有的鲁棒RL方法）一样快地收敛，同时为实际应用提供更好的鲁棒性。我们提出了一种称为“悲观代理”的方法，有效地解决了将ARQ-Learning扩展到大型或连续状态空间的关键瓶颈。利用这一技术，我们首先提出了PRQ-Learning。接着，将其与DQN和DDPG相结合，我们分别开发了PR-DQN和PR-DDPG，这是首个用于深度Q网络和深度确定策略梯度的鲁棒RL算法。我们在基准领域上的实验验证了我们所提出算法的有效性。

    Robust reinforcement learning (RL) aims at learning a policy that optimizes the worst-case performance over an uncertainty set. Given nominal Markov decision process (N-MDP) that generates samples for training, the set contains MDPs obtained by some perturbations from N-MDP. In this paper, we introduce a new uncertainty set containing more realistic MDPs in practice than the existing sets. Using this uncertainty set, we present a robust RL, named ARQ-Learning, for tabular cases. Also, we characterize the finite-time error bounds and prove that it converges as fast as Q-Learning and robust Q-Learning (i.e., the state-of-the-art robust RL method) while providing better robustness for real applications. We propose {\em pessimistic agent} that efficiently tackles the key bottleneck for the extension of ARQ-Learning into large or continuous state spaces. Using this technique, we first propose PRQ-Learning. To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG, respect
    
[^43]: 神经常微分方程与深度残差网络的泛化界限

    Generalization bounds for neural ordinary differential equations and deep residual networks. (arXiv:2305.06648v1 [stat.ML])

    [http://arxiv.org/abs/2305.06648](http://arxiv.org/abs/2305.06648)

    本研究提出了神经常微分方程及其变体的泛化界限，涵盖了深度残差网络，其泛化界限与连续权重差异大小有关。

    

    神经常微分方程（Neural ODEs）是一类流行的连续深度深度学习模型。本文考虑了一个由连续时间参数化的ODE及时变的神经ODE组成的大类。我们通过Lipschitz方法推导了这个类别的泛化界限。通过利用神经ODE和深度残差网络之间的类比，我们的方法得到了一个深度残差网络的泛化界限。这个界限与连续权重之间的差异的大小有关。我们通过数值结果演示了这个量是如何影响神经网络的泛化能力的。

    Neural ordinary differential equations (neural ODEs) are a popular family of continuous-depth deep learning models. In this work, we consider a large family of parameterized ODEs with continuous-in-time parameters, which include time-dependent neural ODEs. We derive a generalization bound for this class by a Lipschitz-based argument. By leveraging the analogy between neural ODEs and deep residual networks, our approach yields in particular a generalization bound for a class of deep residual networks. The bound involves the magnitude of the difference between successive weight matrices. We illustrate numerically how this quantity affects the generalization capability of neural networks.
    
[^44]: 异质数据的预测性变点检测

    Predictive change point detection for heterogeneous data. (arXiv:2305.06630v1 [cs.LG])

    [http://arxiv.org/abs/2305.06630](http://arxiv.org/abs/2305.06630)

    该论文提出了一种基于“预测与比较”机器学习模型的变点监测框架，它能够比现有的在线监测方法更好地控制误报率和失控平均运行长度。该方法使用ARIMA模型和LSTM递归神经网络模型进行预测，具有很强的推广性能。

    

    引入一个名为“预测与比较”的机器学习模型辅助的变点检测（CPD）框架，并与其他在线CPD例程进行比较，结果表明该方法在误报率和失控平均运行长度方面表现更优。该方法的重点是通过使用更复杂的预测模型（预测步骤）代替通常使用的趋势估计函数（如滑动平均），并将其预测与实际数据进行比较（比较步骤），从而改善顺序分析中的标准方法，例如CUSUM规则，以提高这些质量指标。

    A change point detection (CPD) framework assisted by a predictive machine learning model called ''Predict and Compare'' is introduced and characterised in relation to other state-of-the-art online CPD routines which it outperforms in terms of false positive rate and out-of-control average run length. The method's focus is on improving standard methods from sequential analysis such as the CUSUM rule in terms of these quality measures.  This is achieved by replacing typically used trend estimation functionals such as the running mean with more sophisticated predictive models (Predict step), and comparing their prognosis with actual data (Compare step). The two models used in the Predict step are the ARIMA model and the LSTM recursive neural network. However, the framework is formulated in general terms, so as to allow the use of other prediction or comparison methods than those tested here. The power of the method is demonstrated in a tribological case study in which change points separa
    
[^45]: 基于双指数族的扩展广义线性模型中的Dropout正则化

    Dropout Regularization in Extended Generalized Linear Models based on Double Exponential Families. (arXiv:2305.06625v1 [stat.ML])

    [http://arxiv.org/abs/2305.06625](http://arxiv.org/abs/2305.06625)

    本论文研究了基于双指数族的扩展广义线性模型中的dropout正则化，dropout正则化偏好罕见但重要的特征，在均值和离散度方面都具有普适性。

    

    尽管dropout是一种流行的正则化技术，但其理论性质尚未被充分理解。本文研究了基于双指数族的扩展广义线性模型中的dropout正则化，其中离散参数可以随特征变化。理论分析表明，dropout正则化偏好罕见但重要的特征，在均值和离散度方面都具有普适性，这扩展了之前针对传统广义线性模型的结果 。采用自适应学习率的随机梯度下降进行训练。为了说明这一点，我们将dropout应用于自适应B样条平滑，其中均值和离散度参数都被灵活地建模。重要的B样条基础函数可以被认为是罕见的特征，我们在实验中证实，dropout是一种改善了罚最大似然方法的显式平滑性的均值和离散度参数的有效正则化形式。

    Even though dropout is a popular regularization technique, its theoretical properties are not fully understood. In this paper we study dropout regularization in extended generalized linear models based on double exponential families, for which the dispersion parameter can vary with the features. A theoretical analysis shows that dropout regularization prefers rare but important features in both the mean and dispersion, generalizing an earlier result for conventional generalized linear models. Training is performed using stochastic gradient descent with adaptive learning rate. To illustrate, we apply dropout to adaptive smoothing with B-splines, where both the mean and dispersion parameters are modelled flexibly. The important B-spline basis functions can be thought of as rare features, and we confirm in experiments that dropout is an effective form of regularization for mean and dispersion parameters that improves on a penalized maximum likelihood approach with an explicit smoothness p
    
[^46]: 热带半环上的矩阵三因式分解算法

    Matrix tri-factorization over the tropical semiring. (arXiv:2305.06624v1 [cs.LG])

    [http://arxiv.org/abs/2305.06624](http://arxiv.org/abs/2305.06624)

    本研究提出了一种热带半环上的矩阵三因式分解算法 triFastSTMF，用于分析多部分网络结构，并在四部分网络结构分析中恢复网络的边长度。

    

    热带半环在优化控制、生物信息学、离散事件系统或决策问题等几个研究领域都取得了成功。本研究提出了一种热带半环上的矩阵三因式分解算法 triFastSTMF，用于分析在热带半环上多部分网络的结构，我们将其应用于四部分网络结构分析，以恢复网络的边长度。

    Tropical semiring has proven successful in several research areas, including optimal control, bioinformatics, discrete event systems, or solving a decision problem. In previous studies, a matrix two-factorization algorithm based on the tropical semiring has been applied to investigate bipartite and tripartite networks. Tri-factorization algorithms based on standard linear algebra are used for solving tasks such as data fusion, co-clustering, matrix completion, community detection, and more. However, there is currently no tropical matrix tri-factorization approach, which would allow for the analysis of multipartite networks with a high number of parts. To address this, we propose the triFastSTMF algorithm, which performs tri-factorization over the tropical semiring. We apply it to analyze a four-partition network structure and recover the edge lengths of the network. We show that triFastSTMF performs similarly to Fast-NMTF in terms of approximation and prediction performance when fitted
    
[^47]: V2Meow: 通过音乐生成器跟随视觉节拍进行“喵叫”(arXiv:2305.06594v1 [cs.SD])

    V2Meow: Meowing to the Visual Beat via Music Generation. (arXiv:2305.06594v1 [cs.SD])

    [http://arxiv.org/abs/2305.06594](http://arxiv.org/abs/2305.06594)

    V2Meow是一种新方法，通过与O(100K)音频片段配对的视频帧进行训练，生成与各种类型的视频输入的视觉语义相匹配的高质量音频，无需符号音乐数据。

    

    生成与视频视觉内容相匹配的高质量音乐是一项具有挑战性的任务。大多数现有的视觉条件音乐生成系统生成符号音乐数据，如MIDI文件，而不是原始音频波形。考虑到符号音乐数据有限的情况下，这些方法只能为少数乐器或特定类型的视觉输入生成音乐。本文提出了一种名为V2Meow的新方法，它可以生成与各种类型的视频输入的视觉语义相匹配的高质量音频。具体而言，所提出的音乐生成系统是一个多阶段自回归模型，它是通过与从野生音乐视频中挖掘的O(100K)音乐音频片段配对的视频帧进行训练的，而没有涉及任何并行符号音乐数据。V2Meow能够仅在先前训练的从任意静态视频提取的视觉特征的条件下合成高保真度的音频波形。

    Generating high quality music that complements the visual content of a video is a challenging task. Most existing visual conditioned music generation systems generate symbolic music data, such as MIDI files, instead of raw audio waveform. Given the limited availability of symbolic music data, such methods can only generate music for a few instruments or for specific types of visual input. In this paper, we propose a novel approach called V2Meow that can generate high-quality music audio that aligns well with the visual semantics of a diverse range of video input types. Specifically, the proposed music generation system is a multi-stage autoregressive model which is trained with a number of O(100K) music audio clips paired with video frames, which are mined from in-the-wild music videos, and no parallel symbolic music data is involved. V2Meow is able to synthesize high-fidelity music audio waveform solely conditioned on pre-trained visual features extracted from an arbitrary silent vide
    
[^48]: HAHE: 基于全局和局部水平的分层注意力模型用于超关系知识图谱

    HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level. (arXiv:2305.06588v1 [cs.AI])

    [http://arxiv.org/abs/2305.06588](http://arxiv.org/abs/2305.06588)

    提出了HAHE模型，使用全局和局部水平的注意力学习了超关系知识图谱的图形结构和顺序结构，并在多个基准数据集上实现了最先进的表现。

    

    在超关系知识图谱上进行链接预测是值得尝试的。该论文提出了一种新颖的基于分层注意力的模型——HAHE，包括全局和局部水平的注意力机制来表示超关系知识图谱中的结构。通过采用超图双重注意力层，全局级别的注意力可以建模超关系知识图谱的图形结构；而采用异质性自注意层，局部级别的注意力则可以学习H-Facts内部的顺序结构。实验结果表明，HAHE在多个基准数据集上的链接预测方面取得了最先进的性能。

    Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs' representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state
    
[^49]: 面向时间序列预测的谱时图神经网络的表达能力研究

    How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?. (arXiv:2305.06587v1 [cs.LG])

    [http://arxiv.org/abs/2305.06587](http://arxiv.org/abs/2305.06587)

    该论文研究了谱时图神经网络的表达能力，并揭示了其具有线性谱时GNN是普适的、表现力受到离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。同时，论文提出了一个简单实例TGC，其在时间序列预测方面具有显著的性能优势。

    

    谱时图神经网络是大多数基于图神经网络(GNN)的时间序列预测模型的一个有前途的抽象。然而，我们需要更多关于这种方法的基础知识。本文建立了一个理论框架，揭示了谱时GNN的表现力。我们的结果表明，具有线性谱时GNN是普适的，在温和的假设下，它们的表现力受到我们的离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。为了使我们的发现在实践中有用，我们详细讨论了相关限制，并概述了在谱域中设计空间和时间模块的理论蓝图。基于这些见解，并为了展示基于我们的框架，谱时GNN有多么强大，我们提出了一个名为 Temporal Graph GegenConv (TGC) 的简单实例，显著优于大多数已有的模型。

    Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph GegenConv (TGC), which significantly outperforms most e
    
[^50]: 基于时间标签平滑性的时变图聚类方法

    Clustering of Time-Varying Graphs Based on Temporal Label Smoothness. (arXiv:2305.06576v1 [cs.LG])

    [http://arxiv.org/abs/2305.06576](http://arxiv.org/abs/2305.06576)

    本文提出了一种基于谱聚类的时变图节点聚类方法，并添加了簇标签平滑性的约束条件。实验证明该方法是有效的。

    

    本文提出了一种基于簇标签随时间平滑变化的节点聚类方法。聚类是许多科学和工程领域中基础的任务，包括信号处理、机器学习和数据挖掘。虽然大多数已有的研究侧重于对静态图中节点的聚类，但我们经常遇到时变图，如社交网络、脑功能连接和点云等时序数据。本文将时变图的节点聚类问题建模为一个基于谱聚类的优化问题，并添加了簇标签平滑性的约束条件。我们使用原始-对偶拆分算法来解决该问题。实验结果表明了该方法的有效性。

    We propose a node clustering method for time-varying graphs based on the assumption that the cluster labels are changed smoothly over time. Clustering is one of the fundamental tasks in many science and engineering fields including signal processing, machine learning, and data mining. Although most existing studies focus on the clustering of nodes in static graphs, we often encounter time-varying graphs for time-series data, e.g., social networks, brain functional connectivity, and point clouds. In this paper, we formulate a node clustering of time-varying graphs as an optimization problem based on spectral clustering, with a smoothness constraint of the node labels. We solve the problem with a primal-dual splitting algorithm. Experiments on synthetic and real-world time-varying graphs are performed to validate the effectiveness of the proposed approach.
    
[^51]: 如何为推荐基础模型索引项目ID

    How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])

    [http://arxiv.org/abs/2305.06569](http://arxiv.org/abs/2305.06569)

    本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。

    

    推荐基础模型将推荐任务转换为自然语言任务，利用大型语言模型（LLM）进行推荐。它通过直接生成建议的项目而不是计算传统推荐模型中每个候选项目的排名得分，简化了推荐管道，避免了多段过滤的问题。为了避免在决定要推荐哪些项目时生成过长的文本，为推荐基础模型创建LLM兼容的项目ID是必要的。本研究系统地研究了推荐基础模型的项目索引问题，以P5为代表的主干模型，并使用各种索引方法复制其结果。我们首先讨论了几种微不足道的项目索引方法（如独立索引、标题索引和随机索引）的问题，并表明它们不适用于推荐基础模型，然后提出了一种新的索引方法，称为上下文感知索引。我们表明，这种索引方法在项目推荐准确性和文本生成质量方面优于其他索引方法。

    Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
    
[^52]: 基于流形正则化 Tucker 分解的时空交通数据填充方法

    Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation. (arXiv:2305.06563v1 [stat.ML])

    [http://arxiv.org/abs/2305.06563](http://arxiv.org/abs/2305.06563)

    本文提出了一种基于流形正则化Tucker分解的时空交通数据填充方法，该方法利用稀疏正则化项改善了Tucker核的稀疏性，并引入流形正则化和时间约束项来优化张量的填充性能。

    

    时空交通数据填充(STDI)是数据驱动智能交通系统中不可避免和具有挑战性的任务，在部分观测到的交通数据中估计丢失数据。由于交通数据具有多维和时空性质，我们将丢失数据填充视为张量完成问题。过去十年中，许多关于基于张量分解的 STDI 的研究已经展开。然而，如何利用时空相关性和核张量稀疏性来改善填充性能仍然需要解决。本文重新构造了3/4阶汉克尔张量，并提出了一种创新的流形正则化 Tucker 分解(maniRTD)模型用于STDI。明确地，我们通过引入多维延迟嵌入变换将传感交通状态数据表示为3/4阶张量。然后，ManiRTD使用稀疏正则化项改善了Tucker核的稀疏性，并使用流形正则化和时间约束项来优化张量的填充性能。

    Spatiotemporal traffic data imputation (STDI), estimating the missing data from partially observed traffic data, is an inevitable and challenging task in data-driven intelligent transportation systems (ITS). Due to traffic data's multidimensional and spatiotemporal properties, we treat the missing data imputation as a tensor completion problem. Many studies have been on STDI based on tensor decomposition in the past decade. However, how to use spatiotemporal correlations and core tensor sparsity to improve the imputation performance still needs to be solved. This paper reshapes a 3rd/4th order Hankel tensor and proposes an innovative manifold regularized Tucker decomposition (ManiRTD) model for STDI. Expressly, we represent the sensory traffic state data as the 3rd/4th tensors by introducing Multiway Delay Embedding Transforms. Then, ManiRTD improves the sparsity of the Tucker core using a sparse regularization term and employs manifold regularization and temporal constraint terms of f
    
[^53]: 开放世界中的长尾问题回答

    Long-Tailed Question Answering in an Open World. (arXiv:2305.06557v1 [cs.CL])

    [http://arxiv.org/abs/2305.06557](http://arxiv.org/abs/2305.06557)

    本研究提出了一种支持长尾分布数据的Open Long-Tailed QA (OLTQA)模型，鼓励头部、尾部和未知任务间的知识共享，并从大型预训练语言模型中明确挖掘知识，解决了QA方法中瓶颈问题。

    

    现实世界的数据通常具有开放长尾分布，并构建一个统一的QA模型以支持各种任务对于实际的QA应用至关重要。然而，扩展以前的QA方法并不容易，因为它们要么需要访问足够样本的已知任务，要么不明确地对未知任务进行建模。在本文中，我们将Open Long-Tailed QA (OLTQA)定义为学习长尾分布数据并在已知和未知QA任务上优化性能。我们提出了一个OLTQA模型，该模型鼓励头部、尾部和未知任务之间的知识共享，并从大型预训练语言模型中明确挖掘知识。具体而言，我们通过一组细粒度的组件来组织我们的模型，并动态组合这些组件以方便知识共享。进一步引入了一个检索-重排框架来选择上下文例子，这些例子指导LM生成表达QA任务知识的文本。此外，我们还提出了一种新颖的动态池化机制。

    Real-world data often have an open long-tailed distribution, and building a unified QA model supporting various tasks is vital for practical QA applications. However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks. In this paper, we define Open Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and optimizing performance over seen and unseen QA tasks. We propose an OLTQA model that encourages knowledge sharing between head, tail and unseen tasks, and explicitly mines knowledge from a large pre-trained language model (LM). Specifically, we organize our model through a pool of fine-grained components and dynamically combine these components for an input to facilitate knowledge sharing. A retrieve-then-rerank frame is further introduced to select in-context examples, which guild the LM to generate text that express knowledge for QA tasks. Moreover, 
    
[^54]: 开放世界下的领域增量生机学习

    Domain Incremental Lifelong Learning in an Open World. (arXiv:2305.06555v1 [cs.CL])

    [http://arxiv.org/abs/2305.06555](http://arxiv.org/abs/2305.06555)

    本文提出了Diana模型，一种基于动态架构的生命周期学习模型，它使用四种层次化组织的提示来学习一系列任务。其中，任务级提示用于捕获任务特定的知识，实例级提示用于学习跨输入样本共享的知识，从而提高模型的泛化性能。

    

    终身学习是NLP模型不断学习新任务的重要能力。基于架构的方法被报道为LL模型的有效实现。然而，将先前的方法扩展到领域增量LL场景并非易事，因为它们要么需要在测试阶段访问任务身份，要么无法处理来自未见任务的样本。在本文中，我们提出Diana：一种基于动态架构的生命周期学习模型，试图通过增强语言模型来学习一系列任务。 Diana使用四种层次化组织的提示来捕获不同粒度的知识。

    Lifelong learning (LL) is an important ability for NLP models to learn new tasks continuously. Architecture-based approaches are reported to be effective implementations for LL models. However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks. In this paper, we propose \textbf{Diana}: a \underline{d}ynam\underline{i}c \underline{a}rchitecture-based lifelo\underline{n}g le\underline{a}rning model that tries to learn a sequence of tasks with a prompt-enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across input samples to improve the model's generalization performance. Moreover, w
    
[^55]: 离散系统的神经李雅普诺夫控制

    Neural Lyapunov Control for Discrete-Time Systems. (arXiv:2305.06547v1 [cs.LG])

    [http://arxiv.org/abs/2305.06547](http://arxiv.org/abs/2305.06547)

    该论文介绍了一种用于离散时间系统的神经李雅普诺夫控制方法，该方法利用混合整数线性规划来验证稳定性条件，计算子水平集刻画吸引域，有效地学习控制策略。

    

    尽管线性系统的稳定性已经被充分了解，但对于具有非线性动力学的系统仍然是一个主要的挑战。在这种情况下的一般方法是利用李雅普诺夫稳定性理论来计算李雅普诺夫控制函数和相关控制策略的组合。然而，对于一般的非线性系统寻找李雅普诺夫函数是一个具有挑战性的任务。为了解决这个挑战，最近提出了几种使用神经网络表示李雅普诺夫函数的方法。然而，这些方法仅针对连续时间系统设计。我们提出了第一种适用于离散时间系统学习神经李雅普诺夫控制的方法。三个关键要素使我们能够有效地学习可证明稳定的控制策略。第一个是通过混合整数线性规划来验证离散时间系统中的稳定性条件，这是一种新的方法。第二个是计算子水平集的一种新方法，该方法刻画了吸引域的区域

    While ensuring stability for linear systems is well understood, it remains a major challenge for systems with nonlinear dynamics. A general approach in such cases is to leverage Lyapunov stability theory to compute a combination of a Lyapunov control function and an associated control policy. However, finding Lyapunov functions for general nonlinear systems is a challenging task. To address this challenge, several methods have been recently proposed that represent Lyapunov functions using neural networks. However, such approaches have been designed exclusively for continuous-time systems. We propose the first approach for learning neural Lyapunov control in discrete-time systems. Three key ingredients enable us to effectively learn provably stable control policies. The first is a novel mixed-integer linear programming approach for verifying the stability conditions in discrete-time systems. The second is a novel approach for computing sub-level sets which characterize the region of att
    
[^56]: 大数据集上的谱聚类：在何时有效？来自连续聚类和密度Cheeger-Buser理论的证明。

    Spectral Clustering on Large Datasets: When Does it Work? Theory from Continuous Clustering and Density Cheeger-Buser. (arXiv:2305.06541v1 [cs.LG])

    [http://arxiv.org/abs/2305.06541](http://arxiv.org/abs/2305.06541)

    本文研究了大数据集上的谱聚类问题，提供了关于在大数据集上进行谱聚类的理论直觉，从概率密度函数中提取大量点时的谱聚类问题得到了一定的解决。

    

    谱聚类是最受欢迎的聚类算法之一，已经经受了时间的考验。它易于描述，可以使用标准的线性代数实现，并且通常比传统聚类算法如K-means和K-centers找到更好的聚类。由Shi和Malik开发的两向谱聚类基础算法从数据中创建几何图形，并找到图形的谱切割。在现代机器学习中，许多数据集被建模为从概率密度函数中提取的大量点。对于这种方式如何进行谱聚类我们知之甚少，过去的研究者通过引用图Cheeger不等式证明了谱聚类的正确性（即表示图谱切割逼近“归一化切割”），但这种证明在大数据集上崩溃了。我们提供了关于在大数据集上进行谱聚类的理论直觉，这些数据集是从概率密度函数中提取的。

    Spectral clustering is one of the most popular clustering algorithms that has stood the test of time. It is simple to describe, can be implemented using standard linear algebra, and often finds better clusters than traditional clustering algorithms like $k$-means and $k$-centers. The foundational algorithm for two-way spectral clustering, by Shi and Malik, creates a geometric graph from data and finds a spectral cut of the graph.  In modern machine learning, many data sets are modeled as a large number of points drawn from a probability density function. Little is known about when spectral clustering works in this setting -- and when it doesn't. Past researchers justified spectral clustering by appealing to the graph Cheeger inequality (which states that the spectral cut of a graph approximates the ``Normalized Cut''), but this justification is known to break down on large data sets.  We provide theoretically-informed intuition about spectral clustering on large data sets drawn from pr
    
[^57]: 基于语义随机游走的属性图图表示学习

    Semantic Random Walk for Graph Representation Learning in Attributed Graphs. (arXiv:2305.06531v1 [cs.SI])

    [http://arxiv.org/abs/2305.06531](http://arxiv.org/abs/2305.06531)

    本文提出基于语义随机游走的属性图图表示学习方法，通过联合优化两种异构源，构建辅助加权图进行高阶距离学习，从而表征节点和属性，并捕获图结构和语义内部和之间的非线性高阶内在相关性。

    

    本文关注于属性图中的图表示学习，提出一种新的"语义图表示"方法，将两个异构源的联合优化转化为常见的高阶距离学习框架。通过构建一个辅助加权图，该图全面编码原始图中节点和属性之间的复杂同构和异构关系，然后对新构建的图应用传统的高阶拓扑距离学习方法，从而学习到节点和属性的表示，并捕获图结构和语义内部和之间的非线性高阶内在相关性。同时学习到的属性嵌入也可以有效地捕获每个节点的语义信息，可以用于下游的图分析任务，如聚类和分类。

    In this study, we focus on the graph representation learning (a.k.a. network embedding) in attributed graphs. Different from existing embedding methods that treat the incorporation of graph structure and semantic as the simple combination of two optimization objectives, we propose a novel semantic graph representation (SGR) method to formulate the joint optimization of the two heterogeneous sources into a common high-order proximity based framework. Concretely, we first construct an auxiliary weighted graph, where the complex homogeneous and heterogeneous relations among nodes and attributes in the original graph are comprehensively encoded. Conventional embedding methods that consider high-order topology proximities can then be easily applied to the newly constructed graph to learn the representations of both node and attribute while capturing the nonlinear high-order intrinsic correlation inside or among graph structure and semantic. The learned attribute embeddings can also effectiv
    
[^58]: 商用大型语言模型在非洲语种上表现如何？

    How Good are Commercial Large Language Models on African Languages?. (arXiv:2305.06530v1 [cs.CL])

    [http://arxiv.org/abs/2305.06530](http://arxiv.org/abs/2305.06530)

    本文对商用大型语言模型在跨越不同语言系和地理区域的八种非洲语言上进行了初步分析，结果显示它们在非洲语言上的表现略低。呼吁确保非洲语言在商业大型语言模型中得到充分的重视。

    

    自然语言处理领域最近的进展，促使了大型预训练语言模型的普及。这些模型通过上下文学习表现出良好的性能，即使在未知任务和语言上也能表现出良好的性能。它们也被作为一种语言模型服务的商业API所采用。然而，它们在非洲语种上的表现尚不明确。本文对跨越不同语言系和地理区域的八种非洲语言上的两项任务（机器翻译和文本分类）上商业大型语言模型进行了初步分析。我们的结果表明，商业语言模型在非洲语种上的性能略低。我们还发现它们在文本分类方面的表现更好。总体上，我们的研究呼吁确保非洲语言在商业大型语言模型中得到充分的重视，这也是非洲语种逐渐流行的原因。

    Recent advancements in Natural Language Processing (NLP) has led to the proliferation of large pretrained language models. These models have been shown to yield good performance, using in-context learning, even on unseen tasks and languages. They have also been exposed as commercial APIs as a form of language-model-as-a-service, with great adoption. However, their performance on African languages is largely unknown. We present a preliminary analysis of commercial large language models on two tasks (machine translation and text classification) across eight African languages, spanning different language families and geographical areas. Our results suggest that commercial language models produce below-par performance on African languages. We also find that they perform better on text classification than machine translation. In general, our findings present a call-to-action to ensure African languages are well represented in commercial large language models, given their growing popularity.
    
[^59]: 一种快速的拓扑方法预测时变图的异常点

    A fast topological approach for predicting anomalies in time-varying graphs. (arXiv:2305.06523v1 [cs.LG])

    [http://arxiv.org/abs/2305.06523](http://arxiv.org/abs/2305.06523)

    本文介绍了一种快速的拓扑方法来预测时变图中的异常点。该方法避免了特征的组合爆炸，并在合成和真实的时变图的异常检测上实现了最先进的表现。

    

    大规模时变图在金融、社会和生物学等领域越来越普遍。有效提取编码稀疏、多层、动态图复杂结构的特征存在计算和方法上的挑战。拓扑数据分析（TDA）中的持久化图（PD）已成为具有明确定义距离的数据形状描述符的热门选择。然而，TDA在图上的应用仍然很少探讨。本文通过引入一种计算效率高的框架来从图数据中提取形状信息，填补了文献中的这一空白。我们的框架有两个主要步骤：首先，我们使用所谓的下星过滤器（lower-star filtration）使用定量节点属性计算PD，然后在一维网格上通过平均相关的Betti函数来矢量化它。我们的方法避免了传统基于图的方法中特征的组合爆炸，并在合成和真实的时变图的异常检测上实现了最先进的表现。

    Large time-varying graphs are increasingly common in financial, social and biological settings. Feature extraction that efficiently encodes the complex structure of sparse, multi-layered, dynamic graphs presents computational and methodological challenges. In the past decade, a persistence diagram (PD) from topological data analysis (TDA) has become a popular descriptor of shape of data with a well-defined distance between points. However, applications of TDA to graphs, where there is no intrinsic concept of distance between the nodes, remain largely unexplored. This paper addresses this gap in the literature by introducing a computationally efficient framework to extract shape information from graph data. Our framework has two main steps: first, we compute a PD using the so-called lower-star filtration which utilizes quantitative node attributes, and then vectorize it by averaging the associated Betti function over successive scale values on a one-dimensional grid. Our approach avoids
    
[^60]: ST-GIN:一种具有时空图注意力和双向循环联合神经网络的交通数据插值不确定性量化方法

    ST-GIN: An Uncertainty Quantification Approach in Traffic Data Imputation with Spatio-temporal Graph Attention and Bidirectional Recurrent United Neural Networks. (arXiv:2305.06480v1 [cs.LG])

    [http://arxiv.org/abs/2305.06480](http://arxiv.org/abs/2305.06480)

    本研究提出了一种创新的交通数据插值方法，利用图注意力和双向神经网络捕捉时空相关性，实验结果表明在处理缺失值方面优于其他基准技术。

    

    交通数据是智能交通系统中的基本组成部分。然而，从环路检测器或类似来源收集的现实世界交通数据通常包含缺失值(MVs)，这可能会对相关应用和研究产生不利影响。为了恢复这些缺失值，研究人员利用数字统计、张量分解和深度学习技术等方法实现了数据插值。本文提出了一种创新的深度学习方法用于插值缺失数据。该方法采用图注意架构来捕捉交通数据中存在的空间相关性，同时利用双向神经网络学习时间信息。实验结果表明，我们提出的方法优于所有其他基准技术，证明其有效性。

    Traffic data serves as a fundamental component in both research and applications within intelligent transportation systems. However, real-world transportation data, collected from loop detectors or similar sources, often contain missing values (MVs), which can adversely impact associated applications and research. Instead of discarding this incomplete data, researchers have sought to recover these missing values through numerical statistics, tensor decomposition, and deep learning techniques. In this paper, we propose an innovative deep-learning approach for imputing missing data. A graph attention architecture is employed to capture the spatial correlations present in traffic data, while a bidirectional neural network is utilized to learn temporal information. Experimental results indicate that our proposed method outperforms all other benchmark techniques, thus demonstrating its effectiveness.
    
[^61]: LLM是否能理解用户偏好？在用户评分预测任务中对LLM进行评估。

    Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction. (arXiv:2305.06474v1 [cs.IR])

    [http://arxiv.org/abs/2305.06474](http://arxiv.org/abs/2305.06474)

    本文研究了大型语言模型（LLMs）在用户评分预测任务中的表现，与传统的协同过滤方法进行对比。结果发现LLMs能够在较少数据的情况下保持优秀的性能，并且在零样本和少样本情况下表现很好。

    

    大型语言模型(LLMs)在零样本或少样本情况下展现出了杰出的泛化能力。然而，LLMs在基于用户以前的行为推断用户偏好方面能力的程度还是一个尚不清楚的问题。传统上，协同过滤(CF)是这些任务中最有效的方法，主要依赖于大量的评分数据。相比之下，LLMs通常需要更少的数据，同时又保持了每个项目(如电影或产品)的详尽的世界知识。在本文中，我们对用户评分预测这一经典任务中的CF和LLMs进行了全面的比较。这一任务涉及基于用户过去的评分预测候选项目的评分。我们研究了不同大小的LLMs，从250M到540B个参数，并评估了它们在零样本、少样本和微调场景下的性能。

    Large Language Models (LLMs) have demonstrated exceptional capabilities in generalizing to new tasks in a zero-shot or few-shot manner. However, the extent to which LLMs can comprehend user preferences based on their previous behavior remains an emerging and still unclear research question. Traditionally, Collaborative Filtering (CF) has been the most effective method for these tasks, predominantly relying on the extensive volume of rating data. In contrast, LLMs typically demand considerably less data while maintaining an exhaustive world knowledge about each item, such as movies or products. In this paper, we conduct a thorough examination of both CF and LLMs within the classic task of user rating prediction, which involves predicting a user's rating for a candidate item based on their past ratings. We investigate various LLMs in different sizes, ranging from 250M to 540B parameters and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We conduct comprehen
    
[^62]: 针对梯度泄漏威胁的分布式SGD安全方法

    Securing Distributed SGD against Gradient Leakage Threats. (arXiv:2305.06473v1 [cs.LG])

    [http://arxiv.org/abs/2305.06473](http://arxiv.org/abs/2305.06473)

    本文提出了一种用于防止梯度泄漏的综合方法，使得联邦学习中的分布式SGD更具隐私、准确性和攻击韧性。

    

    本文介绍了一种用于防止梯度泄漏的综合方法，以确保分布式随机梯度下降算法的安全。首先，我们分析了两种隐私增强联邦学习策略：（i）使用随机选择或低秩滤波的梯度剪枝，以及（ii）使用加性随机噪声或差分隐私噪声的梯度扰动。我们分析了这些方法的固有限制以及它们对隐私保证、模型准确度和攻击韧性的影响。接下来，我们提出了一种基于差分隐私控制噪声的梯度泄漏保护方法-保护分布式SGD的联邦学习。与传统方法不同，我们的方法跟踪每个示例的梯度更新趋势，使得自适应噪声注入在联邦模型训练中保持紧密对齐。最后，我们对隐私保护进行了实证分析。

    This paper presents a holistic approach to gradient leakage resilient distributed Stochastic Gradient Descent (SGD). First, we analyze two types of strategies for privacy-enhanced federated learning: (i) gradient pruning with random selection or low-rank filtering and (ii) gradient perturbation with additive random noise or differential privacy noise. We analyze the inherent limitations of these approaches and their underlying impact on privacy guarantee, model accuracy, and attack resilience. Next, we present a gradient leakage resilient approach to securing distributed SGD in federated learning, with differential privacy controlled noise as the tool. Unlike conventional methods with the per-client federated noise injection and fixed noise parameter strategy, our approach keeps track of the trend of per-example gradient updates. It makes adaptive noise injection closely aligned throughout the federated model training. Finally, we provide an empirical privacy analysis on the privacy gu
    
[^63]: ChatGPT式的大规模基础模型在预测与健康管理中的应用：综述与路线图

    ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v1 [cs.LG])

    [http://arxiv.org/abs/2305.06472](http://arxiv.org/abs/2305.06472)

    该论文综述了基于大规模基础模型（LSF-Models）如ChatGPT和DALLE-E的人工智能（AI）技术在预测与健康管理（PHM）中的广泛应用。这种技术可以实现多模态、多任务、大量数据和超大模型范式，成为AI-2.0的新时代的标志之一。

    

    预测与健康管理技术在工业生产和设备维护中扮演着至关重要的角色，通过基于人工智能的PHM技术识别和预测设备故障和损坏。现在，基于大规模基础模型（LSF-Models）如ChatGPT和DALLE-E的AI技术，可以实现多模态、多任务、大规模数据和超大模型范式，成为AI-2.0的新时代的标志之一。这种技术广泛应用于各种工业领域，如铁路、能源和航空等，以提高设备的服务寿命和可靠性，同时降低生产成本和停机时间。

    Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT r
    
[^64]: 持续面部表情识别：基准测试。

    Continual Facial Expression Recognition: A Benchmark. (arXiv:2305.06448v1 [cs.CV])

    [http://arxiv.org/abs/2305.06448](http://arxiv.org/abs/2305.06448)

    本研究提出了“持续面部表情识别（ConFER）”基准测试，以评估在真实世界交互中，基于连续学习（CL）能够不断适应不同任务、场景等多变环境的面部表情识别（FER）模型。

    

    理解人类情感行为（尤其在现实世界动态中），需要面部表情识别（FER）模型能够不断适应个体表情、环境及背景，但当前基于机器学习（ML）的FER方法在基准数据集上的孤立预训练无法捕捉到真实世界交互的细微差别。而一些生命学习，如连续学习（CL），可以使智能体适应不断变化的数据分布，集成新信息而不会干扰先前的知识。因为生命学习在FER方面的高效性，本研究提出了“持续面部表情识别（ConFER）”基准测试，评估流行的CL技术。

    Understanding human affective behaviour, especially in the dynamics of real-world settings, requires Facial Expression Recognition (FER) models to continuously adapt to individual differences in user expression, contextual attributions, and the environment. Current (deep) Machine Learning (ML)-based FER approaches pre-trained in isolation on benchmark datasets fail to capture the nuances of real-world interactions where data is available only incrementally, acquired by the agent or robot during interactions. New learning comes at the cost of previous knowledge, resulting in catastrophic forgetting. Lifelong or Continual Learning (CL), on the other hand, enables adaptability in agents by being sensitive to changing data distributions, integrating new information without interfering with previously learnt knowledge. Positing CL as an effective learning paradigm for FER, this work presents the Continual Facial Expression Recognition (ConFER) benchmark that evaluates popular CL techniques 
    
[^65]: 利用Transformer进行抑郁症筛查的动态图表示学习

    Dynamic Graph Representation Learning for Depression Screening with Transformer. (arXiv:2305.06447v1 [cs.LG])

    [http://arxiv.org/abs/2305.06447](http://arxiv.org/abs/2305.06447)

    利用Transformer进行抑郁症筛查，克服了传统方法的特征工程依赖和忽略时变因素的缺点。

    

    快速发现心理障碍至关重要，因为这样可以及时干预和治疗，从而大大改善患有严重心理疾病的个体的预后。社交媒体平台上最近出现的心理健康讨论的激增为研究心理健康提供了机会，并有可能检测到心理疾病的发生。然而，现有的抑郁症检测方法由于两个主要限制而受到限制：(1)依赖于特征工程，(2)没有考虑时变因素。具体而言，这些方法需要大量的特征工程和领域知识，其中严重依赖于用户生成内容的数量、质量和类型。此外，这些方法忽视了时变因素对抑郁症检测的重要影响，例如社交媒体上随时间推移而产生的语言模式和人际互动行为的动态变化(例如回复、提及和引用推文)。

    Early detection of mental disorder is crucial as it enables prompt intervention and treatment, which can greatly improve outcomes for individuals suffering from debilitating mental affliction. The recent proliferation of mental health discussions on social media platforms presents research opportunities to investigate mental health and potentially detect instances of mental illness. However, existing depression detection methods are constrained due to two major limitations: (1) the reliance on feature engineering and (2) the lack of consideration for time-varying factors. Specifically, these methods require extensive feature engineering and domain knowledge, which heavily rely on the amount, quality, and type of user-generated content. Moreover, these methods ignore the important impact of time-varying factors on depression detection, such as the dynamics of linguistic patterns and interpersonal interactive behaviors over time on social media (e.g., replies, mentions, and quote-tweets)
    
[^66]: 多智能体强化学习: 异步通信和线性函数逼近

    Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])

    [http://arxiv.org/abs/2305.06446](http://arxiv.org/abs/2305.06446)

    该论文探讨了多智能体强化学习在情节式马尔可夫决策过程中的协作问题，提出了一种基于值迭代的算法，可以实现异步通信，在保证合作优势的同时降低通信开销。通过提供和证明的算法和复杂度界限，为多智能体强化学习在实际应用中提供理论依据。

    

    我们研究了多智能体强化学习在情节式马尔科夫决策过程中的设置，多个智能体通过中央服务器进行通信以合作。我们提出了一种基于值迭代的可证明有效的算法，可以实现异步通信，同时确保合作优势且通信开销低。我们证明了在使用线性函数逼近的情况下，我们的算法具有 $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ 的遗憾值和 $\tilde{\mathcal{O}}(dHM^2)$ 的通信复杂度，其中 $d$ 是特征维数，$H$ 是时间跨度，$M$ 是智能体总数，$K$ 是总情节数。我们还提供了一个下限证明，表明通过协作至少需要 $\Omega(dM)$ 的通信复杂度才能改善性能。

    We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\Omega(dM)$ communication complexity is required to improve the performance through collaboration.
    
[^67]: 数据、树和森林——K-12教育中的决策树学习

    Data, Trees, and Forests -- Decision Tree Learning in K-12 Education. (arXiv:2305.06442v1 [cs.CY])

    [http://arxiv.org/abs/2305.06442](http://arxiv.org/abs/2305.06442)

    针对K-12教育中学生对于机器学习的学习难度大的问题，提出了一种教学概念，同时注重概念理解与实际应用，基于决策树学习，旨在赋予学生积极应用机器学习方法和反思对社会的影响的能力。

    

    随着机器学习对我们生活的影响越来越大，每个人都需要理解相应现象的能力，同时也需要参与塑造我们的世界并做出关于对社会影响的知情决策。因此，在K-12教育中，学生需要学习机器学习的核心思想和原则。然而，对于这个目标群体来说，实现所有上述目标都是一个巨大的挑战。为此，我们提出了一种教学概念，将注重概念理解的有趣易懂的非插件方法与赋予学生积极应用机器学习方法和反思其对社会影响的能力相结合，基于决策树学习。

    As a consequence of the increasing influence of machine learning on our lives, everyone needs competencies to understand corresponding phenomena, but also to get involved in shaping our world and making informed decisions regarding the influences on our society. Therefore, in K-12 education, students need to learn about core ideas and principles of machine learning. However, for this target group, achieving all of the aforementioned goals presents an enormous challenge. To this end, we present a teaching concept that combines a playful and accessible unplugged approach focusing on conceptual understanding with empowering students to actively apply machine learning methods and reflect their influence on society, building upon decision tree learning.
    
[^68]: 稀疏和密集神经网络中的小批量大小的相变

    Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2305.06435](http://arxiv.org/abs/2305.06435)

    本文系统地研究了小批量大小对稀疏和密集神经网络训练的影响，发现在临界值处会出现尖锐的相变，阐明了神经网络优化的基本机制。

    

    在训练人工神经网络时，使用小批量数据现在非常普遍。尽管已经广泛使用，但缺少定量解释最佳小批量大小应该是多大的理论。本文尝试系统地理解小批量大小在训练两层神经网络中的作用。在教师-学生情境下，使用稀疏教师，并聚焦于不同复杂度的任务，我们量化了改变小批量大小m的影响。我们发现，通常情况下，学生的泛化性能强烈依赖于m，并且可能在临界值mc处经历尖锐的相变，这样当m< mc时，训练过程失败，而当m> mc时，学生可以完美地学习或很好地泛化教师。相变是由统计力学首次发现的集体现象，并在许多科学领域观察到。找到在深度学习中改变小批量大小的相变，可以阐明神经网络优化的基本机制。

    The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m<m_c$ the training process fails, while for $m>m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the 
    
[^69]: 基于词语的图卷积网络

    Word Grounded Graph Convolutional Network. (arXiv:2305.06434v1 [cs.CL])

    [http://arxiv.org/abs/2305.06434](http://arxiv.org/abs/2305.06434)

    该论文提出了一种基于词语的图卷积网络模型，可以在处理图外文档时进行归纳推理；该模型在多个基准数据集上表现出较好的性能，同时表明了基于图的方法联合建模词级和文档级信息的有效性。

    

    图卷积网络（GCNs）在学习文本表示方面表现出较强的性能，特别是在建模图结构数据（如文献引用网络）方面，对于各种任务如文本分类等都有良好的表现。大多数现有的GCNs仅限于处理预定义图中的文档，即不能推广到图外文档。为了解决这个问题，我们提出将文档图转化为词图，通过使用独立于文档的图来解耦数据样本（即训练和测试集中的文档）和GCN模型。这种基于词级的GCN因此可以自然地归纳地推理出图外文档。提出了基于WGraph的归纳词语图卷积网络（WGCN），它可以对单词级文本实例（如不在语料库中的文档）进行归纳推理。在四个基准数据集上的实验表明，WGCN优于现有方法，通过图形方法联合建模词级和文档级信息的有效性。

    Graph Convolutional Networks (GCNs) have shown strong performance in learning text representations for various tasks such as text classification, due to its expressive power in modeling graph structure data (e.g., a literature citation network). Most existing GCNs are limited to deal with documents included in a pre-defined graph, i.e., it cannot be generalized to out-of-graph documents. To address this issue, we propose to transform the document graph into a word graph, to decouple data samples (i.e., documents in training and test sets) and a GCN model by using a document-independent graph. Such word-level GCN could therefore naturally inference out-of-graph documents in an inductive way. The proposed Word-level Graph (WGraph) can not only implicitly learning word presentation with commonly-used word co-occurrences in corpora, but also incorporate extra global semantic dependency derived from inter-document relationships (e.g., literature citations). An inductive Word-grounded Graph 
    
[^70]: 一种适用于风险概率估计的可推广、物理学基础学习框架

    A Generalizable Physics-informed Learning Framework for Risk Probability Estimation. (arXiv:2305.06432v1 [eess.SY])

    [http://arxiv.org/abs/2305.06432](http://arxiv.org/abs/2305.06432)

    本文提出了一种基于物理学的学习框架，通过将MC方法与基于物理学的神经网络相结合，有效评估长期风险概率及其梯度。数值结果表明，该方法具有更好的样本效率，能够适应系统变化。

    

    准确评估长期风险概率及其梯度对于许多随机安全控制方法至关重要。然而，在实时和未知或变化的环境中计算这些风险概率是具有挑战性的。在本文中，我们开发了一种有效的方法来评估长期风险概率及其梯度。所提出的方法利用了长期风险概率满足某些偏微分方程(PDEs)的事实，该方程表征了概率之间的邻近关系，以将MC方法和基于物理学的神经网络相结合。我们提供了在特定训练配置下给出估计误差的理论保证。数值结果表明，所提出的方法具有更好的样本效率，能够很好地推广到未知区域，并能够适应系统变化，相比MC方法和现有的数据驱动方法，它表现出更好的性能。

    Accurate estimates of long-term risk probabilities and their gradients are critical for many stochastic safe control methods. However, computing such risk probabilities in real-time and in unseen or changing environments is challenging. Monte Carlo (MC) methods cannot accurately evaluate the probabilities and their gradients as an infinitesimal devisor can amplify the sampling noise. In this paper, we develop an efficient method to evaluate the probabilities of long-term risk and their gradients. The proposed method exploits the fact that long-term risk probability satisfies certain partial differential equations (PDEs), which characterize the neighboring relations between the probabilities, to integrate MC methods and physics-informed neural networks. We provide theoretical guarantees of the estimation error given certain choices of training configurations. Numerical results show the proposed method has better sample efficiency, generalizes well to unseen regions, and can adapt to sys
    
[^71]: 自动化神经科患者出院小结医院过程的方法

    A Method to Automate the Discharge Summary Hospital Course for Neurology Patients. (arXiv:2305.06416v1 [cs.CL])

    [http://arxiv.org/abs/2305.06416](http://arxiv.org/abs/2305.06416)

    开发了一种使用编码器-解码器序列到序列变换模型进行医院过程小结的自动化方法以缓解医生过劳。该方法在实际性上进行了优化，盲评估表明62%的自动化摘要符合标准，具有临床应用的潜力。

    

    自动化临床笔记的生成被提出作为缓解医生过劳的策略。具体来说，病人住院期间自动化叙述性摘要可作为住院医生在电子病历系统中记录的出院小结中的医院过程部分的补充。在本研究中，我们开发并评估了一种使用编码器-解码器序列到序列变换模型进行医院过程小结的自动化方法。我们对BERT和BART模型进行了微调并通过限制波束搜索进行了实际性优化，采用了从学术医疗中心神经科病人的电子病历数据进行训练和测试。该方法表现出良好的ROUGE分数和13.76的R-2。在盲评估中，两位董事会认证的医生评价62%的自动化摘要符合标准，这表明该方法可能在临床上有用。据我们所知，这项研究是其中之一。

    Generation of automated clinical notes have been posited as a strategy to mitigate physician burnout. In particular, an automated narrative summary of a patient's hospital stay could supplement the hospital course section of the discharge summary that inpatient physicians document in electronic health record (EHR) systems. In the current study, we developed and evaluated an automated method for summarizing the hospital course section using encoder-decoder sequence-to-sequence transformer models. We fine tuned BERT and BART models and optimized for factuality through constraining beam search, which we trained and tested using EHR data from patients admitted to the neurology unit of an academic medical center. The approach demonstrated good ROUGE scores with an R-2 of 13.76. In a blind evaluation, two board-certified physicians rated 62% of the automated summaries as meeting the standard of care, which suggests the method may be useful clinically. To our knowledge, this study is among th
    
[^72]: 使用永续学习技术加速批次主动学习

    Accelerating Batch Active Learning Using Continual Learning Techniques. (arXiv:2305.06408v1 [cs.LG])

    [http://arxiv.org/abs/2305.06408](http://arxiv.org/abs/2305.06408)

    本文介绍了一种新的技术，即永续性主动学习（CAL），通过偏向先前标记集来加速训练，通过使用一系列回放方案，包括模型蒸馏和从历史中选择多样化的和不确定的点。实验结果表明，CAL可以大幅提升训练速度。

    

    主动学习（AL）的一个主要问题是训练成本很高，因为模型通常在每个查询轮之后都要从头开始重新训练。本文首先演示了使用预热启动的标准神经网络进行AL的失败，既不能加速训练，又不能避免在AL查询轮上使用微调时发生灾难性遗忘。然后，我们开发了一类新的技术，通过偏向先前标记集来加速训练。我们通过使用现有的和发展新的基于回放的永续学习（CL）算法实现这一点，这些算法在快速学习新知识而不遗忘老知识的情况下，在数据来自不断变化的分布时特别有效。我们将这一范例称为“永续性主动学习”（CAL）。我们展示了CAL通过使用大量的回放方案来实现显著的加速效果，这些方案使用模型蒸馏并从历史中选择多样化的和不确定的点。我们在许多数据领域进行了实验，包括自然

    A major problem with Active Learning (AL) is high training costs since models are typically retrained from scratch after every query round. We start by demonstrating that standard AL on neural networks with warm starting fails, both to accelerate training and to avoid catastrophic forgetting when using fine-tuning over AL query rounds. We then develop a new class of techniques, circumventing this problem, by biasing further training towards previously labeled sets. We accomplish this by employing existing, and developing novel, replay-based Continual Learning (CL) algorithms that are effective at quickly learning the new without forgetting the old, especially when data comes from an evolving distribution. We call this paradigm Continual Active Learning (CAL). We show CAL achieves significant speedups using a plethora of replay schemes that use model distillation and that select diverse, uncertain points from the history. We conduct experiments across many data domains, including natura
    
[^73]: ACTC: 冷启动知识图谱补全的主动阈值校准

    ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion. (arXiv:2305.06395v1 [cs.LG])

    [http://arxiv.org/abs/2305.06395](http://arxiv.org/abs/2305.06395)

    本文提出了一种名为ACTC的方法，在冷启动知识图谱补全时进行主动阈值校准，可以有效地利用有限的标记元组来找到每个关系的最佳阈值，同时也结合未标记元组进行了实验。

    

    自监督的知识图谱补全(KGC)依赖于估计得分模型(实体，关系，实体)-元组，例如，通过嵌入初始知识图。通过调整预测阈值(使用手动注释的示例)，可以改善预测质量。本文尝试首次针对KGC进行冷启动校准，在此过程中初始没有注释的示例，并且只能选择有限数量的元组进行注释。我们的新方法ACTC基于有限的注释元组有效地找到好的每个关系的阈值。除了一些注释的元组外，ACTC还利用Logistic回归或高斯过程分类器估计的未标记元组的正确性。我们还通过密度和随机选择等不同方法选择候选元组进行注释。我们使用五个评分模型和一个oracle注释进行实验。

    Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotat
    
[^74]: 跨模型对齐实现文本到概念的转换

    Text-To-Concept (and Back) via Cross-Model Alignment. (arXiv:2305.06386v1 [cs.CV])

    [http://arxiv.org/abs/2305.06386](http://arxiv.org/abs/2305.06386)

    本文介绍了"Text-To-Concept"的方法，通过特征对齐，将来自预训练模型的特征转换为可与文本编码器比较的标准化形式，并免费将视觉编码器转换为零样本分类器。

    

    本文观察到即使在不同模型之间，图像表示的映射也可以通过仅使用线性层进行学习。在此基础上，本文提出了"Text-To-Concept"，其中来自预训练模型的特征与CLIP空间进行线性对齐，使得来自CLIP文本编码器的文本嵌入可直接与对齐特征进行比较。通过Text-To-Concept转换，可免费将固定的现成视觉编码器转换为强大的零样本分类器，有时甚至可以超过CLIP的精度，即使这些编码器比CLIP小得多，并且相对于CLIP，训练数据仅占很小一部分。本文还展示了Text-To-Concept的其他直接应用：如构建不需要概念监督的概念瓶颈模型，通过人类概念诊断分布移位，并检索满足一组基于文本的约束条件的图像。最后，本文证明了特征对准确实现界限。

    We observe that the mapping between an image's representation in one model to its representation in another can be learned surprisingly well with just a linear layer, even across diverse models. Building on this observation, we propose $\textit{text-to-concept}$, where features from a fixed pretrained model are aligned linearly to the CLIP space, so that text embeddings from CLIP's text encoder become directly comparable to the aligned features. With text-to-concept, we convert fixed off-the-shelf vision encoders to surprisingly strong zero-shot classifiers for free, with accuracy at times even surpassing that of CLIP, despite being much smaller models and trained on a small fraction of the data compared to CLIP. We show other immediate use-cases of text-to-concept, like building concept bottleneck models with no concept supervision, diagnosing distribution shifts in terms of human concepts, and retrieving images satisfying a set of text-based constraints. Lastly, we demonstrate the fe
    
[^75]: 通过强化学习发掘最佳量子纠错码的方法发现

    Discovery of Optimal Quantum Error Correcting Codes via Reinforcement Learning. (arXiv:2305.06378v1 [quant-ph])

    [http://arxiv.org/abs/2305.06378](http://arxiv.org/abs/2305.06378)

    该论文介绍了一种使用强化学习策略来发掘最佳量子纠错码的方法，该方法可以训练代理以最大化编码距离或最小化在偏置Pauli噪声下的逻辑错误概率。作者证明，与其他CSS代码相比，他们可以限制量子比特数量并获得优越的结果。

    

    最近提出的量子乐高框架为利用简单的方法生成复杂的量子纠错码（QECC）提供了一种强大的方法。我们将这个过程变成了一个游戏，并使用强化学习（RL）解锁了一种设计和发掘编码的新途径。 RL的一个好处是我们可以指定待优化的编码的\textit{任意}属性。我们针对两种这样的属性进行训练，最大化编码距离和在偏置Pauli噪声下最小化逻辑错误的概率。针对第一个属性，我们发现通过训练的代理能够识别增加编码距离的方法，超过原始的级联方法，在13个量子比特上饱和线性编程界限的CSS编码。对于学习目标是在偏置Pauli噪声下最小化逻辑错误概率，我们发现了在这个任务中最好的已知CSS代码，针对$\lesssim 20$个量子比特。与其他（局部变形的）CSS代码，包括Surface，XZZX和2D Color codes相比，我们的$[[17,1,3]]$编码构造实际上具有优势。

    The recently introduced Quantum Lego framework provides a powerful method for generating complex quantum error correcting codes (QECCs) out of simple ones. We gamify this process and unlock a new avenue for code design and discovery using reinforcement learning (RL). One benefit of RL is that we can specify \textit{arbitrary} properties of the code to be optimized. We train on two such properties, maximizing the code distance, and minimizing the probability of logical error under biased Pauli noise. For the first, we show that the trained agent identifies ways to increase code distance beyond naive concatenation, saturating the linear programming bound for CSS codes on 13 qubits. With a learning objective to minimize the logical error probability under biased Pauli noise, we find the best known CSS code at this task for $\lesssim 20$ qubits. Compared to other (locally deformed) CSS codes, including Surface, XZZX, and 2D Color codes, our $[[17,1,3]]$ code construction actually has \text
    
[^76]: 多臂赌博机用于多任务神经求解器的高效训练

    Efficient Training of Multi-task Neural Solver with Multi-armed Bandits. (arXiv:2305.06361v1 [cs.LG])

    [http://arxiv.org/abs/2305.06361](http://arxiv.org/abs/2305.06361)

    本文提出了一种基于多臂赌博机的通用高效训练范式，用于多任务神经求解器的训练，通过任务影响矩阵进行更高效的训练，相比于标准计划，在有限的训练预算或相同的训练时长内实现了更高的整体性能。

    

    针对如何高效地为各种组合优化问题 (COP) 训练多任务神经求解器，目前的研究相对较少。在本文中，我们提出了一种基于多臂赌博机的通用高效训练范式，以提供一个统一的多任务神经求解器。为此，我们利用编码器-解码器框架下的多任务理论损失分解，通过一个任务影响矩阵通过正确的赌博算法实现更高效的训练。相比标准的训练计划，我们的方法在有限的训练预算或相同的训练时段内实现了更高的整体性能，这可以为其他多任务大模型的高效训练提供指导，此外，影响矩阵可以提供学习优化领域中常见实践的经验证据，从而支持我们方法的可行性。

    Efficiently training a multi-task neural solver for various combinatorial optimization problems (COPs) has been less studied so far. In this paper, we propose a general and efficient training paradigm based on multi-armed bandits to deliver a unified multi-task neural solver. To this end, we resort to the theoretical loss decomposition for multiple tasks under an encoder-decoder framework, which enables more efficient training via proper bandit task-sampling algorithms through an intra-task influence matrix. Our method achieves much higher overall performance with either limited training budgets or the same training epochs, compared to standard training schedules, which can be promising for advising efficient training of other multi-task large models. Additionally, the influence matrix can provide empirical evidence of some common practices in the area of learning to optimize, which in turn supports the validity of our approach.
    
[^77]: 探索机器遗忘的领域：一篇综述与分类

    Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])

    [http://arxiv.org/abs/2305.06360](http://arxiv.org/abs/2305.06360)

    本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。

    

    机器遗忘是一个越来越受关注的领域，因为需要删除或修改机器学习模型所做出的预测。虽然训练模型变得更加有效和准确，但在某些领域（如隐私、安全和公正性），遗忘先前学到的信息的重要性变得越来越显著。本文介绍了机器遗忘的综述，涵盖了当前最先进的技术和方法，包括数据删除、扰动和模型更新。此外，文中还介绍了常用的度量标准和数据集。文章还强调了需要解决的挑战，包括攻击复杂性、标准化、可转移性、可解释性、训练数据和资源限制。本文的贡献包括讨论MU的潜在益处以及它在自然语言处理、计算机视觉和推荐系统中的未来方向。

    Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
    
[^78]: HumanRF：用于运动中人的高保真神经辐射场

    HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion. (arXiv:2305.06356v1 [cs.CV])

    [http://arxiv.org/abs/2305.06356](http://arxiv.org/abs/2305.06356)

    具有4D动态场景表示的HumanRF能够从多视角视频输入中捕捉全身外貌，以高压缩率捕捉精细细节并支持高分辨率。ActorsHQ提供了12MP的镜头，为长序列获得时间上连贯的人物重建。

    

    在各种应用程序中，如电影制作、电脑游戏或视频会议中，高保真地表现人类表现是一个重要的构建块。为了接近生产级的质量，我们介绍了HumanRF，这是一个4D动态神经场景表示，从多视角视频输入中捕捉运动中的全身外貌，并使其可以在新的、看不见的视角下播放。我们的新型表示作为一个动态视频编码，通过将时空分解为一个时间矩阵向量分解，以高压缩率捕捉精细细节。这使我们可以为长序列获得时间上连贯的人物重建，并在具有挑战性的动作情况下表示高分辨率的细节。虽然大多数研究集中在合成4MP或更低分辨率，但我们解决了在12MP上操作的挑战。为此，我们介绍了ActorsHQ，这是一个新的多视角数据集，为160个摄像机提供了12MP的镜头。

    Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 
    
[^79]: 通过误差函数核训练增强量子支持向量机

    Enhancing Quantum Support Vector Machines through Variational Kernel Training. (arXiv:2305.06063v1 [quant-ph])

    [http://arxiv.org/abs/2305.06063](http://arxiv.org/abs/2305.06063)

    本文研究了量子支持向量机，并提出了一种新方法：量子变分核支持向量机（QVK-SVM），能够在准确性、损失和混淆矩阵指标方面优于现有模型。它应该成为未来QML研究中的可靠工具。

    

    最近，量子机器学习（QML）取得了巨大的进展，其中量子支持向量机（QSVM）成为了一种有前途的模型。本文重点关注两种现有的QSVM方法：量子核支持向量机（QK-SVM）和量子变分支持向量机（QV-SVM）。虽然两种方法均取得了令人印象深刻的结果，但我们提出了一种新的方法，即量子变分核支持向量机（QVK-SVM），以增强准确性，融合了QK-SVM和QV-SVM的优点。我们在鸢尾花数据集上进行了广泛的实验，发现QVK-SVM在准确性、损失和混淆矩阵指标方面均优于现有模型。我们的结果表明，QVK-SVM作为一种可靠的、具有变革性的QML应用工具具有巨大的潜力。因此，我们建议在未来的QML研究中采用它。

    Quantum machine learning (QML) has witnessed immense progress recently, with quantum support vector machines (QSVMs) emerging as a promising model. This paper focuses on the two existing QSVM methods: quantum kernel SVM (QK-SVM) and quantum variational SVM (QV-SVM). While both have yielded impressive results, we present a novel approach that synergizes the strengths of QK-SVM and QV-SVM to enhance accuracy. Our proposed model, quantum variational kernel SVM (QVK-SVM), leverages the quantum kernel and quantum variational algorithm. We conducted extensive experiments on the Iris dataset and observed that QVK-SVM outperforms both existing models in terms of accuracy, loss, and confusion matrix indicators. Our results demonstrate that QVK-SVM holds tremendous potential as a reliable and transformative tool for QML applications. Hence, we recommend its adoption in future QML research endeavors.
    
[^80]: 低分辨率条件下的领域无关的图像翻译

    Domain Agnostic Image-to-image Translation using Low-Resolution Conditioning. (arXiv:2305.05023v1 [eess.IV])

    [http://arxiv.org/abs/2305.05023](http://arxiv.org/abs/2305.05023)

    本文提出了一种低分辨率条件下的领域无关的图像翻译方法，实现了源图像的可视特征与低分辨率目标图像的信息相结合，解决了领域相关的细粒度问题。

    

    图像翻译方法旨在学习跨领域的映射，假定用于翻译的图像共享内容，但具有自己的领域特定信息（即风格）。本文提出了一种领域无关的图像翻译方法，旨在解决细粒度问题，其中领域相关。具体而言，我们的领域无关方法旨在生成一幅图像，将源图像的可视特征与低频信息（例如姿势、颜色）相结合。我们提出了一种新的方法，通过训练生成模型来产生同时具有源图像的独特信息和低分辨率目标图像信息的图像。

    Generally, image-to-image translation (i2i) methods aim at learning mappings across domains with the assumption that the images used for translation share content (e.g., pose) but have their own domain-specific information (a.k.a. style). Conditioned on a target image, such methods extract the target style and combine it with the source image content, keeping coherence between the domains. In our proposal, we depart from this traditional view and instead consider the scenario where the target domain is represented by a very low-resolution (LR) image, proposing a domain-agnostic i2i method for fine-grained problems, where the domains are related. More specifically, our domain-agnostic approach aims at generating an image that combines visual features from the source image with low-frequency information (e.g. pose, color) of the LR target image. To do so, we present a novel approach that relies on training the generative model to produce images that both share distinctive information of 
    
[^81]: 移动机器人全身操作的因果策略梯度

    Causal Policy Gradient for Whole-Body Mobile Manipulation. (arXiv:2305.04866v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.04866](http://arxiv.org/abs/2305.04866)

    本文提出了一种新框架——因果MoMa，可以训练适用于典型MoMa任务的策略，在此框架下，机动和交互自由度可以同时组合，并且不需要人类领域知识来划分动作空间或将动作部分与子目标匹配。

    

    开发下一代家庭机器人助手需要结合机动和交互能力，即通常所说的移动操作。由于机器人的大动作空间和任务常见的多目标性质，例如能够有效地达到目标且避免障碍，移动操作任务很难。目前的方法通常根据人工匹配动作空间的部分到移动操作子目标（例如用于移动目标的基础动作和用于操作的手臂动作）将任务分为不带操作的导航和不带机动的固定操作。此解决方案防止了机动和交互自由度的同时组合，并且需要人类领域知识来划分动作空间并将动作部分与子目标匹配。在本文中，我们介绍了一种新的框架——因果MoMa，该框架用于训练典型MoMa任务的策略。

    Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion by manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subsp
    
[^82]: 通过降维实现高维平滑熵估计

    High-Dimensional Smoothed Entropy Estimation via Dimensionality Reduction. (arXiv:2305.04712v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2305.04712](http://arxiv.org/abs/2305.04712)

    本文研究如何通过PCA将数据投影到低维空间来降低微分熵估计的指数样本复杂性问题，并表明该方法对于嵌入高维空间中的低维结构具有近乎最优性能。

    

    本文研究在高斯卷积下克服微分熵估计的指数样本复杂性问题。具体而言，我们考虑通过$X$的$n$个独立同分布样本来估计微分熵$h(X+Z)$，其中$X$和$Z$是独立的$D$维随机变量，其中$X$是次高斯的，二阶矩有界，而$Z\sim\mathcal{N}(0,\sigma^2I_D)$。在绝对误差损失下，上述问题具有$\frac{c^D}{\sqrt{n}}$的参数估计速率，其在数据维度$D$中呈指数级增长，常常在应用中引发问题。我们通过在熵估计之前通过主成分分析（PCA）将$X$投影到低维空间来克服这种指数样本复杂性，证明了当PCA的未解释方差消失时渐近误差的开销也将消失。这意味着在高维空间中嵌入本质上低维结构的近乎最优性能。

    We study the problem of overcoming exponential sample complexity in differential entropy estimation under Gaussian convolutions. Specifically, we consider the estimation of the differential entropy $h(X+Z)$ via $n$ independently and identically distributed samples of $X$, where $X$ and $Z$ are independent $D$-dimensional random variables with $X$ sub-Gaussian with bounded second moment and $Z\sim\mathcal{N}(0,\sigma^2I_D)$. Under the absolute-error loss, the above problem has a parametric estimation rate of $\frac{c^D}{\sqrt{n}}$, which is exponential in data dimension $D$ and often problematic for applications. We overcome this exponential sample complexity by projecting $X$ to a low-dimensional space via principal component analysis (PCA) before the entropy estimation, and show that the asymptotic error overhead vanishes as the unexplained variance of the PCA vanishes. This implies near-optimal performance for inherently low-dimensional structures embedded in high-dimensional spaces,
    
[^83]: MO-DEHB:多目标优化的进化超带方法

    MO-DEHB: Evolutionary-based Hyperband for Multi-Objective Optimization. (arXiv:2305.04502v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04502](http://arxiv.org/abs/2305.04502)

    MO-DEHB是一种扩展自DEHB的多目标优化器，可应用于各种多目标问题，并在多个性能指标上表现出最优性能。

    

    超参数优化是自动调整机器学习模型的强大技术。然而，在许多现实应用中，准确性只是必须考虑的多个性能标准之一。在复杂多元化的搜索空间中同时优化这些目标仍然是一项具有挑战性的任务。在本文中，我们提出了MO-DEHB，一种有效灵活的多目标优化器，它扩展了最近的进化Hyperband方法DEHB。我们使用一套包括15个不同且具有挑战性的多目标问题的全面基准套件来验证MO-DEHB的性能，其中包括超参数优化、神经架构搜索(NAS)和联合NAS和HPO，目标包括准确性、延迟和算法公平性。与最先进的多目标优化器进行比较研究表明，MO-DEHB在这15个基准测试中表现最好。

    Hyperparameter optimization (HPO) is a powerful technique for automating the tuning of machine learning (ML) models. However, in many real-world applications, accuracy is only one of multiple performance criteria that must be considered. Optimizing these objectives simultaneously on a complex and diverse search space remains a challenging task. In this paper, we propose MO-DEHB, an effective and flexible multi-objective (MO) optimizer that extends the recent evolutionary Hyperband method DEHB. We validate the performance of MO-DEHB using a comprehensive suite of 15 benchmarks consisting of diverse and challenging MO problems, including HPO, neural architecture search (NAS), and joint NAS and HPO, with objectives including accuracy, latency and algorithmic fairness. A comparative study against state-of-the-art MO optimizers demonstrates that MO-DEHB clearly achieves the best performance across our 15 benchmarks.
    
[^84]: 带有Hebbian可塑性的脉冲神经网络用于无监督表示学习。

    Spiking neural networks with Hebbian plasticity for unsupervised representation learning. (arXiv:2305.03866v1 [cs.NE])

    [http://arxiv.org/abs/2305.03866](http://arxiv.org/abs/2305.03866)

    本文介绍了一种新的脉冲神经网络模型，实现了从数据中无监督地学习分布式内部表示。该模型采用在线Hebbian-Bayesian学习和重连机制，并表现出和传统的非脉冲神经网络相近的性能。

    

    我们引入了一种新的脉冲神经网络模型，用于在无监督过程中从数据中学习分布式内部表示。我们通过将非脉冲前馈贝叶斯置信传播神经网络（BCPNN）模型转化为脉冲神经网络，并采用在线基于相关性的Hebbian-Bayesian学习和重连机制，该机制先前已表现出执行表示学习的能力。我们的脉冲模型采用泊松统计和低发火率与在体星形神经元相媲美。我们使用线性分类器评估了我们的脉冲模型学习的表示，并展示了与非脉冲BCPNN接近的性能，以及与其他基于Hebbian的脉冲网络在MNIST和F-MNIST机器学习基准测试中的竞争性能。

    We introduce a novel spiking neural network model for learning distributed internal representations from data in an unsupervised procedure. We achieved this by transforming the non-spiking feedforward Bayesian Confidence Propagation Neural Network (BCPNN) model, employing an online correlation-based Hebbian-Bayesian learning and rewiring mechanism, shown previously to perform representation learning, into a spiking neural network with Poisson statistics and low firing rate comparable to in vivo cortical pyramidal neurons. We evaluated the representations learned by our spiking model using a linear classifier and show performance close to the non-spiking BCPNN, and competitive with other Hebbian-based spiking networks when trained on MNIST and F-MNIST machine learning benchmarks.
    
[^85]: 探索软掩模语言建模在可控符号音乐生成中的应用

    Exploring Softly Masked Language Modelling for Controllable Symbolic Music Generation. (arXiv:2305.03530v1 [cs.SD])

    [http://arxiv.org/abs/2305.03530](http://arxiv.org/abs/2305.03530)

    本文探索了软掩模语言建模在符号音乐生成中的应用。使用变压器编码器架构，成功将SMLM应用于受限符号音乐生成。

    

    本文介绍了将软掩模语言建模（SMLM）应用于符号音乐生成的早期探索。SMLM可视为掩模语言建模（MLM）的一种推广，其中输入集合的每个元素可以是部分已知的，而不是已知或未知的。我们使用变压器编码器架构展示了将SMLM应用于受限符号音乐生成的结果。可在https://erl-j.github.io/smlm-web-supplement/上找到若干音频示例。

    This document presents some early explorations of applying Softly Masked Language Modelling (SMLM) to symbolic music generation. SMLM can be seen as a generalisation of masked language modelling (MLM), where instead of each element of the input set being either known or unknown, elements can be partly known. We demonstrate some results of applying SMLM to constrained symbolic music generation using a transformer encoder architecture. Several audio examples are available at https://erl-j.github.io/smlm-web-supplement/
    
[^86]: SemEval-2023任务7: 临床试验数据的多证据自然语言推理

    SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data. (arXiv:2305.02993v1 [cs.CL])

    [http://arxiv.org/abs/2305.02993](http://arxiv.org/abs/2305.02993)

    本论文介绍SemEval 2023的任务七，旨在进行临床试验数据的多证据自然语言推理，该任务难度较大，证据选择任务相对于蕴含任务表现更佳。

    

    本篇论文介绍SemEval 2023任务7的结果，该任务主要涉及临床试验数据中的多证据自然语言推理（NLI4CT），由两个子任务组成：一个是自然语言推理（NLI）任务，另一个是证据选择任务。这两个任务需要进行医学和数字推理，这对于开发能够进行大规模医疗证据解释和检索、提供个性化基于证据的保健具有重要意义。第1个子任务“蕴含任务”收到了来自40位参赛者的643份提交，第2个子任务“证据选择任务”收到了来自23位参赛者的364份提交。这两个任务具有挑战性，大部分提交的系统在蕴含任务上未能明显优于大多数类基线，而我们观察到证据选择任务的表现明显优于蕴含任务。增加模型参数会导致模型在测试集上表现更差。

    This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2 tasks, a Natural Language Inference (NLI) task, and an evidence selection task on clinical trial data. The proposed challenges require multi-hop biomedical and numerical reasoning, which are of significant importance to the development of systems capable of large-scale interpretation and retrieval of medical evidence, to provide personalized evidence-based care.  Task 1, the entailment task, received 643 submissions from 40 participants, and Task 2, the evidence selection task, received 364 submissions from 23 participants. The tasks are challenging, with the majority of submitted systems failing to significantly outperform the majority class baseline on the entailment task, and we observe significantly better performance on the evidence selection task than on the entailment task. Increasing the number of model parameters leads to a di
    
[^87]: 关于LayerNorm在Transformers的Attention中表达能力的作用

    On the Expressivity Role of LayerNorm in Transformers' Attention. (arXiv:2305.02582v1 [cs.LG])

    [http://arxiv.org/abs/2305.02582](http://arxiv.org/abs/2305.02582)

    本文揭示了LayerNorm在Transformers的Attention层中有着至关重要的作用，通过对输入向量进行投影并对所有向量进行缩放，LayerNorm可以帮助注意力机制更好地处理输入。

    

    Layer Normalization（LayerNorm）是所有基于Transformer的模型中都具有的组件。在本文中，我们展示LayerNorm对随后的多头注意力层的表达能力至关重要，这与通常认为LayerNorm仅在前向传播期间归一化激活值和在反向传播期间归一化梯度的共识不同。我们考虑了LayerNorm的几何解释，并认为它由两个组成部分组成：（a）将输入向量投影到正交于$\left[1,1,...,1\right]$向量的$d-1$空间，并且（b）将所有向量缩放到相同的$\sqrt{d}$范数。我们展示了每个组件对随后的Transformers中的注意力层都很重要：（a）投影允许注意机制创建一个关注所有键等量的注意查询，从而减轻了注意机制学习此操作的需要；（b）缩放使每个键都有可能接收到最高的注意力分配权重。

    Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models. In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm's only role is to normalize the activations during the forward pass, and their gradients during the backward pass. We consider a geometric interpretation of LayerNorm and show that it consists of two components: (a) projection of the input vectors to a $d-1$ space that is orthogonal to the $\left[1,1,...,1\right]$ vector, and (b) scaling of all vectors to the same norm of $\sqrt{d}$. We show that each of these components is important for the attention layer that follows it in Transformers: (a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation by the attention; and (b) scaling allows each key to potentially receive the highest a
    
[^88]: 一项针对伯努利过程期望最大值的链式规则

    A Chain Rule for the Expected Suprema of Bernoulli Processes. (arXiv:2304.14474v1 [math.PR])

    [http://arxiv.org/abs/2304.14474](http://arxiv.org/abs/2304.14474)

    该论文得出了一个关于伯努利过程期望最大值的上限界限，该界限由指数集在均匀利普希茨函数类下的属性和函数类得出。

    

    我们得出了一个关于伯努利过程期望最大值的上限界限，该界限由指数集在均匀利普希茨函数类下的属性和函数类得出，拓展了Maurer针对高斯过程的早期结果。证明必须基于Bednorz和Latala针对伯努利过程有界性的最近结果。

    We obtain an upper bound on the expected supremum of a Bernoulli process indexed by the image of an index set under a uniformly Lipschitz function class in terms of properties of the index set and the function class, extending an earlier result of Maurer for Gaussian processes. The proof makes essential use of recent results of Bednorz and Latala on the boundedness of Bernoulli processes.
    
[^89]: 论RemOve-And-Retrain的陷阱：数据处理不等式的视角

    On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])

    [http://arxiv.org/abs/2304.13836](http://arxiv.org/abs/2304.13836)

    本论文评估了RemOve-And-Retrain（ROAR）协议的可靠性。研究结果表明，ROAR基准测试中的属性可能有更少的有关决策的重要信息，这种偏差称为毛糙度偏差，并提醒人们不要在ROAR指标上进行盲目的依赖。

    

    本文评估了RemOve-And-Retrain（ROAR）协议的可靠性，该协议用于测量特征重要性估计的性能。我们从理论背景和实证实验中发现，具有较少有关决策功能的信息的属性在ROAR基准测试中表现更好，与ROAR的原始目的相矛盾。这种现象也出现在最近提出的变体RemOve-And-Debias（ROAD）中，我们提出了ROAR归因度量中毛糙度偏差的一致趋势。我们的结果提醒人们不要盲目依赖ROAR的性能评估指标。

    This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
    
[^90]: 基于替代模型的人机交互场景生成

    Surrogate Assisted Generation of Human-Robot Interaction Scenarios. (arXiv:2304.13787v1 [cs.RO])

    [http://arxiv.org/abs/2304.13787](http://arxiv.org/abs/2304.13787)

    本文提出了基于替代模型的人机交互场景生成方法，可以高效合成多样化的挑战性数据集，以便评估和理解人机交互系统的优劣，可以在实际交互中重现这些场景。

    

    随着人机交互系统的发展，不同环境和用户下评估和理解这些系统的优缺点变得越来越困难。为此，以往的方法通过算法生成了多样的场景，揭示了共享控制遥操作任务的系统失效情况。然而，这些方法需要通过模拟机器人策略和人类行为来直接评估生成的场景。这些评估所需的计算成本限制了它们在更复杂的领域的适用性。因此，我们提出了通过替代模型来预测人类和机器人行为来增强场景生成系统的建议。在共享控制遥操作域和更复杂的共享工作空间协作任务中，我们展示了替代模型辅助的场景生成可以高效地合成具有挑战性的多样数据集。我们展示了这些故障在真实世界中的交互中是可重现的。

    As human-robot interaction (HRI) systems advance, so does the difficulty of evaluating and understanding the strengths and limitations of these systems in different environments and with different users. To this end, previous methods have algorithmically generated diverse scenarios that reveal system failures in a shared control teleoperation task. However, these methods require directly evaluating generated scenarios by simulating robot policies and human actions. The computational cost of these evaluations limits their applicability in more complex domains. Thus, we propose augmenting scenario generation systems with surrogate models that predict both human and robot behaviors. In the shared control teleoperation domain and a more complex shared workspace collaboration task, we show that surrogate assisted scenario generation efficiently synthesizes diverse datasets of challenging scenarios. We demonstrate that these failures are reproducible in real-world interactions.
    
[^91]: 通过独热编码和正则化提高梯度提升决策树的鲁棒性

    Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization. (arXiv:2304.13761v1 [stat.ML])

    [http://arxiv.org/abs/2304.13761](http://arxiv.org/abs/2304.13761)

    通过独热编码和正则化提高梯度提升决策树的鲁棒性，研究表明对带有$L_1$或$L_2$正则化的线性回归形式进行拟合可提高GBDT模型的鲁棒性。

    

    梯度提升决策树(GBDT)是一种广泛应用的高效机器学习方法，用于表格数据建模。然而，它们复杂的结构可能导致模型对未见数据中的小协变量扰动的鲁棒性较低。本研究应用独热编码将GBDT模型转换为线性框架，通过将每个树叶编码为一个虚拟变量。这允许使用线性回归技术，以及一种新颖的风险分解方法来评估GBDT模型对协变量扰动的鲁棒性。我们建议通过重新拟合其带有$L_1$或$L_2$正则化的线性回归形式，提高GBDT模型的鲁棒性。理论结果表明了正则化对模型性能和鲁棒性的影响。在数值实验中，证明了所提出的正则化方法可以提高独热编码GBDT模型的鲁棒性。

    Gradient-boosted decision trees (GBDT) are widely used and highly effective machine learning approach for tabular data modeling. However, their complex structure may lead to low robustness against small covariate perturbation in unseen data. In this study, we apply one-hot encoding to convert a GBDT model into a linear framework, through encoding of each tree leaf to one dummy variable. This allows for the use of linear regression techniques, plus a novel risk decomposition for assessing the robustness of a GBDT model against covariate perturbations. We propose to enhance the robustness of GBDT models by refitting their linear regression forms with $L_1$ or $L_2$ regularization. Theoretical results are obtained about the effect of regularization on the model performance and robustness. It is demonstrated through numerical experiments that the proposed regularization approach can enhance the robustness of the one-hot-encoded GBDT models.
    
[^92]: 新兴技术的组织治理：AI在医疗保健中的应用

    Organizational Governance of Emerging Technologies: AI Adoption in Healthcare. (arXiv:2304.13081v1 [cs.AI])

    [http://arxiv.org/abs/2304.13081](http://arxiv.org/abs/2304.13081)

    该研究通过与美国主要医疗保健系统的领导人和相关领域的主要知情人合作，制定了AI在医疗保健中的组织治理框架，包括关键控制点和决策标准，为卫生系统领导人做出更加明智的决策提供了支持。

    

    私营和公共部门的结构和规范精细化了新兴技术的实际应用。在医疗保健中，尽管出现了大量的AI采用方式，但是其使用和整合周围的组织治理往往被认为不可行。健康AI合作伙伴关系（HAIP）旨在通过此研究更好地定义医疗保健中AI系统的充分组织治理要求，并支持卫生系统领导人做出更加明智的决策。要达到这个目标，我们首先确定了AI在医疗保健中采用的标准如何易于使用和高效运作。然后，我们在特定的卫生系统中，绘制出实际机构采用AI技术的具体决策点。在实践中，我们通过与美国主要医疗保健系统的领导人和相关领域的主要知情人合作，实现了这一目标。通过这种合作，我们制定了AI在医疗保健中的组织治理框架，其中包括关键控制点和决策标准。

    Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working w
    
[^93]: 更多通信不会使联邦学习中的泛化误差变小。

    More Communication Does Not Result in Smaller Generalization Error in Federated Learning. (arXiv:2304.12216v1 [stat.ML])

    [http://arxiv.org/abs/2304.12216](http://arxiv.org/abs/2304.12216)

    我们研究了联邦学习环境下的统计学习模型泛化误差，表明更频繁地与参数服务器通信会负面影响此类学习算法的泛化性能。

    

    我们研究了联邦学习（FL）环境下统计学习模型的泛化误差。具体而言，有$K$个设备或客户端，每个设备持有一个大小为$n$的独立数据集。通过随机梯度下降本地学习的个体模型通过一个中央服务器进行聚合（平均），然后发送回设备。我们考虑多次（比如说$R\in \mathbb{N}^*$）模型聚合并研究$R$对最终聚合模型的泛化误差的影响。我们建立了一个上界，明确考虑了$R$（除了参与设备的数量$K$和数据集大小$n$）的影响。观察到对于固定的$(n,K)$，上界随$R$的增加而增加，这表明更频繁地与参数服务器通信会负面影响此类学习算法的泛化性能。与此同时，由于经验风险通常只随着$n$的增加而减少，因此我们的理论证明了为了在FL中实现良好的泛化性能，需要权衡本地学习和全局聚合之间的权衡。

    We study the generalization error of statistical learning models in a Federated Learning (FL) setting. Specifically, there are $K$ devices or clients, each holding an independent own dataset of size $n$. Individual models, learned locally via Stochastic Gradient Descent, are aggregated (averaged) by a central server into a global model and then sent back to the devices. We consider multiple (say $R \in \mathbb N^*$) rounds of model aggregation and study the effect of $R$ on the generalization error of the final aggregated model. We establish an upper bound on the generalization error that accounts explicitly for the effect of $R$ (in addition to the number of participating devices $K$ and dataset size $n$). It is observed that, for fixed $(n, K)$, the bound increases with $R$, suggesting that the generalization of such learning algorithms is negatively affected by more frequent communication with the parameter server. Combined with the fact that the empirical risk, however, generally d
    
[^94]: Graph-ToolFormer: 通过ChatGPT增强的提示，赋予LLMs图形推理能力

    Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. (arXiv:2304.11116v1 [cs.AI])

    [http://arxiv.org/abs/2304.11116](http://arxiv.org/abs/2304.11116)

    本文旨在通过Graph-ToolFormer框架赋予LLMs图形推理能力，并解决现有LLMs在执行图形学习任务中存在的固有弱点。

    

    本文旨在开发一个能够对复杂图形数据进行推理的大语言模型（LLM）。当前，LLMs在各种自然语言学习任务上取得了非常出色的表现，这些扩展也已被应用于研究具有多模态数据的视觉任务。然而，在图形学习任务中，现有的LLMs由于在执行多步逻辑推理、精确的数学计算以及对空间和时间因素的感知方面存在一些固有弱点，因此呈现出非常严重的缺陷。为了解决这些挑战，本文将调查探索赋予现有LLMs图形推理能力的原理、方法和算法，这将对LLMs和图形学习的当前研究产生巨大影响。受最新的ChatGPT和Toolformer模型的启发，我们提出了Graph-ToolFormer（面向图形推理的Toolformer）框架，通过ChatGPT增强的提示来教导LLMs自身，旨在培养他们的图形推理能力。

    In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.  To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with pro
    
[^95]: MC-ViViT: 多分支分类器-ViViT用于使用面部视频检测老年人轻度认知障碍

    MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])

    [http://arxiv.org/abs/2304.05292](http://arxiv.org/abs/2304.05292)

    本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。

    

    深度机器学习模型包括卷积神经网络(CNN)已成功地应用于使用医学图像、问卷和视频检测轻度认知障碍(MCI)。本文提出了一种新的多分支分类器-视频视觉变换器(MC-ViViT)模型，通过分析面部特征区分MCI和正常认知。数据来自I-CONECT，一个旨在通过提供频繁视频聊天来改善认知功能的行为干预试验。MC-ViViT在一个分支中提取视频的时空特征，并通过MC模块增强表示。由于I-CONECT数据集中的样本不平衡问题（包含难易和正负样本），这使MC-ViViT的性能受到影响。我们提出了一种Hard-Easy和Positive-Negative样本的损失函数（HP Loss）来结合对比度调节损失Focal loss和AD-CORRE loss来解决不平衡问题。我们在I-CONECT数据集上的实验结果显示出该算法的有效性。

    Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
    
[^96]: 求解正则化的exp、cosh和sinh回归问题

    Solving Regularized Exp, Cosh and Sinh Regression Problems. (arXiv:2303.15725v1 [cs.LG])

    [http://arxiv.org/abs/2303.15725](http://arxiv.org/abs/2303.15725)

    该文研究和解决了正则化指数回归问题，根据大型语言模型注意力计算的灵感，使用近似牛顿方法在输入稀疏时间内求解。

    

    在现代机器学习中，注意力计算是训练大型语言模型（如Transformer、GPT-4和ChatGPT）的基本任务。该文研究了受softmax/exp单元启发的指数回归问题。标准指数回归是非凸的。我们研究了指数回归问题的正则化版本，这是一个凸问题。我们使用近似牛顿方法以输入稀疏时间解决问题。形式上，给定矩阵$A\in \mathbb{R}^{n\times d}$，$b\in \mathbb{R}^n$，$w\in\mathbb{R}^n$和任何函数$\exp,\cosh$和$\sinh$，记作$f$。目标是找到最优$x$，使得$0.5 \| f(Ax) - b \|_2^2 + 0.5 \| \mathrm{diag}(w) A x \|_2^2$最小化。研究并解决了正则化指数回归问题，其中函数为$\exp, \cosh$和$\sinh$，并使用近似牛顿法在输入稀疏时间内求解，灵感来自大型语言模型中的注意力计算。

    In modern machine learning, attention computation is a fundamental task for training large language models such as Transformer, GPT-4 and ChatGPT. In this work, we study exponential regression problem which is inspired by the softmax/exp unit in the attention mechanism in large language models. The standard exponential regression is non-convex. We study the regularization version of exponential regression problem which is a convex problem. We use approximate newton method to solve in input sparsity time.  Formally, in this problem, one is given matrix $A \in \mathbb{R}^{n \times d}$, $b \in \mathbb{R}^n$, $w \in \mathbb{R}^n$ and any of functions $\exp, \cosh$ and $\sinh$ denoted as $f$. The goal is to find the optimal $x$ that minimize $ 0.5 \| f(Ax) - b \|_2^2 + 0.5 \| \mathrm{diag}(w) A x \|_2^2$. The straightforward method is to use the naive Newton's method. Let $\mathrm{nnz}(A)$ denote the number of non-zeros entries in matrix $A$. Let $\omega$ denote the exponent of matrix multi
    
[^97]: 多模态变分自编码器用于跨多种成像模态进行规范建模

    Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities. (arXiv:2303.12706v1 [cs.CV])

    [http://arxiv.org/abs/2303.12706](http://arxiv.org/abs/2303.12706)

    本文提出了一种多模态规范建模框架，能够更好地检测出多种成像和生物变量中的异常性，特别适用于研究带有异质性的疾病。

    

    研究常见神经疾病的挑战之一是疾病异质性，包括病因、神经成像特征、合并症或基因变异的差异。规范建模已成为研究这种人群的流行方法，其中对生理系统的“正常”行为进行建模，并可以用于个体层面上检测与疾病病理相关的偏差。对于许多异质性疾病，我们预计会观察到多种神经成像和生物变量的异常。然而，到目前为止，规范模型主要是为了研究单一成像模态而开发的。我们旨在开发一种多模态规范建模框架，在多个模态的变量中聚合异常性，并且比单模式基线更能检测到偏差。我们提出了两种用于检测T1和DTI数据中的个体层面偏差的多模态VAE规范模型。与单模式基线相比，我们提出的模型能够更好地检测到轻度认知受损的受试者中的偏差，证明了多模态规范建模用于疾病异质性的潜力。

    One of the challenges of studying common neurological disorders is disease heterogeneity including differences in causes, neuroimaging characteristics, comorbidities, or genetic variation. Normative modelling has become a popular method for studying such cohorts where the 'normal' behaviour of a physiological system is modelled and can be used at subject level to detect deviations relating to disease pathology. For many heterogeneous diseases, we expect to observe abnormalities across a range of neuroimaging and biological variables. However, thus far, normative models have largely been developed for studying a single imaging modality. We aim to develop a multi-modal normative modelling framework where abnormality is aggregated across variables of multiple modalities and is better able to detect deviations than uni-modal baselines. We propose two multi-modal VAE normative models to detect subject level deviations across T1 and DTI data. Our proposed models were better able to detect di
    
[^98]: 实验固体力学中机器学习的最新进展与应用：一篇综述

    Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review. (arXiv:2303.07647v1 [cs.LG])

    [http://arxiv.org/abs/2303.07647](http://arxiv.org/abs/2303.07647)

    本文综述了近年来机器学习在实验固体力学领域中的最新进展和应用，提供了物理感知和基于物理学的机器学习方法，并涵盖了断裂力学、生物力学、纳米和微观力学、构建材料和二维材料等传统和新兴领域。

    

    多年来，实验固体力学在表征和理解天然和新材料的力学性质方面发挥了至关重要的作用。机器学习的最新进展为该领域提供了新的机遇，包括实验设计、数据分析、不确定性量化和反问题。由于近年来该新兴领域发表的论文数量迅速增加，因此及时进行全面和更新的综述，对于最近机器学习在实验固体力学中的应用具有重要意义。在本文中，我们首先概述了与该综述相关的常见机器学习算法和术语，重点介绍了基于物理学和物理感知的机器学习方法。然后，我们全面涵盖了实验力学传统和新兴领域中机器学习的最新应用，包括断裂力学、生物力学、纳米和微观力学、构建材料和二维材料。最后，我们强调了当前活跃的研究方向和领域面临的主要挑战，最后讨论了机器学习在实验固体力学未来的潜在机会。

    For many decades, experimental solid mechanics has played a crucial role in characterizing and understanding the mechanical properties of natural and novel materials. Recent advances in machine learning (ML) provide new opportunities for the field, including experimental design, data analysis, uncertainty quantification, and inverse problems. As the number of papers published in recent years in this emerging field is exploding, it is timely to conduct a comprehensive and up-to-date review of recent ML applications in experimental solid mechanics. Here, we first provide an overview of common ML algorithms and terminologies that are pertinent to this review, with emphasis placed on physics-informed and physics-based ML methods. Then, we provide thorough coverage of recent ML applications in traditional and emerging areas of experimental mechanics, including fracture mechanics, biomechanics, nano- and micro-mechanics, architected materials, and 2D material. Finally, we highlight some curr
    
[^99]: 使用VAE学习潜在变量：在cryo-EM中的应用观察（arXiv:2303.07487v1 [stat.ML]）

    Using VAEs to Learn Latent Variables: Observations on Applications in cryo-EM. (arXiv:2303.07487v1 [stat.ML])

    [http://arxiv.org/abs/2303.07487](http://arxiv.org/abs/2303.07487)

    本研究通过定性分析，发现VAE在生物应用中摊销潜在变量的特性与传统显式表示方法相似。

    

    变分自编码器（VAEs）是一种流行的生成模型，用于近似分布。VAE的编码器部分用于认证学习潜在变量，为数据样本生成潜在表示。最近，VAEs已用于表征物理和生物系统。在这个案例研究中，我们定性地研究了VAE在生物应用中的摊销特性。我们发现，在这种应用中，编码器与更传统的显式潜在变量表示具有定性相似性。

    Variational autoencoders (VAEs) are a popular generative model used to approximate distributions. The encoder part of the VAE is used in amortized learning of latent variables, producing a latent representation for data samples. Recently, VAEs have been used to characterize physical and biological systems. In this case study, we qualitatively examine the amortization properties of a VAE used in biological applications. We find that in this application the encoder bears a qualitative resemblance to more traditional explicit representation of latent variables.
    
[^100]: 测试反事实场景：揭示在公平原则下的歧视差异 (arXiv:2302.11944v2 [stat.ML] UPDATED)

    Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference. (arXiv:2302.11944v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.11944](http://arxiv.org/abs/2302.11944)

    我们提出了一种反事实场景测试框架，通过比较数据集中类似的保护和非保护实例来检测分类器中的歧视，通过比较组间决策结果差异，来发现个人歧视。该框架可以更好地对「给定差异的公平原则」进行操作，以揭示在公平原则下的歧视差异。

    

    我们提出了一种被称为反事实场景测试(CST)的因果数据挖掘框架来检测分类器中的歧视情况。CST旨在以可操作且有意义的方式回答一种直观问题：“如果个人或投诉人所属的受保护身份不同，模型的结果将会是什么？”它通过反事实推理来对法律基础的情景测试进行扩展，以操作“给定差异的公平原则”的概念。对于任何投诉人，我们在分类器使用的数据集中找到并比较相似的受保护和非受保护实例，构造控制组和测试组，两组的决策结果差异意味着潜在的个人歧视。与情境测试不同，情境测试是围绕投诉人构建两组，我们根据因果知识在投诉人的反事实生成测试组。反事实旨在反映受保护属性对结果的影响。

    We present counterfactual situation testing (CST), a causal data mining framework for detecting discrimination in classifiers. CST aims to answer in an actionable and meaningful way the intuitive question "what would have been the model outcome had the individual, or complainant, been of a different protected status?" It extends the legally-grounded situation testing of Thanh et al. (2011) by operationalizing the notion of fairness given the difference using counterfactual reasoning. For any complainant, we find and compare similar protected and non-protected instances in the dataset used by the classifier to construct a control and test group, where a difference between the decision outcomes of the two groups implies potential individual discrimination. Unlike situation testing, which builds both groups around the complainant, we build the test group on the complainant's counterfactual generated using causal knowledge. The counterfactual is intended to reflect how the protected attrib
    
[^101]: 强化学习用于经济代理模型参数校准的搜索方法组合

    Reinforcement Learning for Combining Search Methods in the Calibration of Economic ABMs. (arXiv:2302.11835v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11835](http://arxiv.org/abs/2302.11835)

    本文提出了一种通过强化学习自动选择和组合搜索方法的方案，用于经济代理模型的参数校准，实验结果表明该方案在提高效率的同时不需要专业领域知识或手动调整参数。

    

    经济学和金融中的代理模型参数校准通常涉及到对非常大的参数空间进行无导数搜索。本文在真实数据上对众所周知的宏观经济代理模型的若干搜索方法进行了基准测试，进一步评估了通过组合不同方法所做出的“混合策略”的表现。研究发现，基于随机森林替代模型的方法特别高效，并且组合搜索方法通常会增加性能，因为任何单一方法的偏差都会被缓解。通过这些观察，我们提出了一种强化学习方案，在校准运行过程中自动选择和组合搜索方法。强化学习代理人只有在该方法的性能表现良好时才继续利用特定方法，但在该方法达到性能平台时探索新策略。得到的强化学习搜索方案在任何其他测试的方法或方法组合上都表现更好，并且不需要专业的领域知识或手动参数调整。

    Calibrating agent-based models (ABMs) in economics and finance typically involves a derivative-free search in a very large parameter space. In this work, we benchmark a number of search methods in the calibration of a well-known macroeconomic ABM on real data, and further assess the performance of "mixed strategies" made by combining different methods. We find that methods based on random-forest surrogates are particularly efficient, and that combining search methods generally increases performance since the biases of any single method are mitigated. Moving from these observations, we propose a reinforcement learning (RL) scheme to automatically select and combine search methods on-the-fly during a calibration run. The RL agent keeps exploiting a specific method only as long as this keeps performing well, but explores new strategies when the specific method reaches a performance plateau. The resulting RL search scheme outperforms any other method or method combination tested, and does 
    
[^102]: 不精确的贝叶斯神经网络

    Imprecise Bayesian Neural Networks. (arXiv:2302.09656v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09656](http://arxiv.org/abs/2302.09656)

    在机器学习和人工智能领域，该论文提出了一种新的算法——不精确的贝叶斯神经网络(IBNNs)。这种算法使用可信区间先验分布集合和似然分布集合进行训练，相比标准的BNNs，可以区分先验和后验的不确定性并量化。此外，IBNNs在贝叶斯灵敏度分析方面具有更强的鲁棒性，并且对分布变化也更加鲁棒。

    

    在机器学习和人工智能中, 确定不确定性和鲁棒性是重要的目标。虽然贝叶斯神经网络使得预测中的不确定性能够被评估，不同来源的不确定性是无法区分的。我们提出了不精确的贝叶斯神经网络（IBNNs），它们可以概括和克服标准BNNs的某些缺点。标准BNNs使用单一的先验分布和似然分布进行训练，而IBNNs使用可信区间先验分布和似然分布进行训练。它们允许区分先验和后验不确定性，并对其进行量化。此外，IBNNs在贝叶斯灵敏度分析方面具有鲁棒性，并且对分布变化比标准BNNs更加鲁棒。它们还可以用于计算具有PAC样本复杂性的结果集。我们将IBNNs应用于两个案例研究：一个是为了人工胰腺控制模拟血糖和胰岛素动力学，另一个是运动规划。

    Uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. Although Bayesian neural networks (BNNs) allow for uncertainty in the predictions to be assessed, different sources of uncertainty are indistinguishable. We present imprecise Bayesian neural networks (IBNNs); they generalize and overcome some of the drawbacks of standard BNNs. These latter are trained using a single prior and likelihood distributions, whereas IBNNs are trained using credal prior and likelihood sets. They allow to distinguish between aleatoric and epistemic uncertainties, and to quantify them. In addition, IBNNs are robust in the sense of Bayesian sensitivity analysis, and are more robust than BNNs to distribution shift. They can also be used to compute sets of outcomes that enjoy PAC-like properties. We apply IBNNs to two case studies. One, to model blood glucose and insulin dynamics for artificial pancreas control, and two, for motion p
    
[^103]: 数据高效的对比自监督学习：易于学习的样本起到最大的作用。

    Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09195](http://arxiv.org/abs/2302.09195)

    该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。

    

    自监督学习（SSL）从大量的无标签训练数据中学习高质量的表示。随着数据集变得越来越大，识别对学习此类表示最有用的示例变得至关重要。这可以通过减少学习高质量表示所需的数据量来实现有效的SSL。然而，对于SSL的价值如何量化一直是一个悬而未决的问题。在本文中，我们首次解决了这个问题，证明在期望意义下，对比SSL中对学习做出最大贡献的示例是具有最相似数据增强的示例。我们对这些子集的SSL的广义性能提供了严格的保证。实验证明，令人惊讶的是，对SSL做出最大贡献的子集是对监督学习做出最小贡献的子集。通过广泛的实验，我们证明了我们的子集在CIFAR100、CIFAR中的表现优于随机子集3%以上。

    Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
    
[^104]: 运行式选举：针对数据污染攻击的改进可证明防御方法

    Run-Off Election: Improved Provable Defense against Data Poisoning Attacks. (arXiv:2302.02300v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02300](http://arxiv.org/abs/2302.02300)

    本文提出了一种名为ROE的新型数据污染防御方法，通过在基本模型之间进行运行式选举，有效利用logits层的信息，并在MNIST数据集和CIFAR-10上得到了比最先进的集成方法更好的结果，提供了针对数据污染攻击的可证明保证

    

    在数据污染攻击中，攻击者试图通过在训练数据中添加、修改或删除样本来改变模型的预测结果。最近，提出了基于集成方法的可证明数据污染防御方法，其中预测是通过对多个基本模型进行多数表决来完成的。本文中，我们表明仅考虑集成防御中的大多数表决是浪费的，因为它没有有效地利用基本模型中的logits层中可用的信息。相反地，我们提出了运行式选举（ROE），这是一种基于基本模型之间的两轮选举的新型聚合方法：在第一轮中，模型为它们首选的类别投票，然后在第一轮中排名前两的类别之间进行第二轮“Run-Off”选举。基于这种方法，我们提出了基于先前工作中的Deep Partition Aggregation（DPA）和Finite Aggregation（FA）方法的DPA+ROE和FA+ROE防御方法。我们在MNIST数据集和CIFAR-10上评估了我们的方法，实验结果表明，我们提出的基于ROE的防御方法优于最先进的集成方法，并提供针对数据污染攻击的可证明保证

    In data poisoning attacks, an adversary tries to change a model's prediction by adding, modifying, or removing samples in the training data. Recently, ensemble-based approaches for obtaining provable defenses against data poisoning have been proposed where predictions are done by taking a majority vote across multiple base models. In this work, we show that merely considering the majority vote in ensemble defenses is wasteful as it does not effectively utilize available information in the logits layers of the base models. Instead, we propose Run-Off Election (ROE), a novel aggregation method based on a two-round election across the base models: In the first round, models vote for their preferred class and then a second, Run-Off election is held between the top two classes in the first round. Based on this approach, we propose DPA+ROE and FA+ROE defense methods based on Deep Partition Aggregation (DPA) and Finite Aggregation (FA) approaches from prior work. We evaluate our methods on MN
    
[^105]: 基于注意力及反向技术的信道预测方法

    Reverse Ordering Techniques for Attention-Based Channel Prediction. (arXiv:2302.00341v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00341](http://arxiv.org/abs/2302.00341)

    本文提出了基于Seq2Seq-attn和Transformer的信道预测模型，并引入了反向技术以提高模型鲁棒性，仿真结果表明比现有方法更好。

    

    本文旨在利用序列到序列模型（Seq2Seq-attn）和Transformer模型，基于噪声观测来预测无线通信系统中的信道。两种模型都是从自然语言处理中改编而来，以应对信道预测的复杂挑战。此外，还引入了一种称为“反向位置编码”的新技术以提高Transformer模型在不同序列长度下的鲁棒性。类似地，在应用注意力之前，Seq2Seq-attn模型的编码器输出也会被翻转。仿真结果表明，所提出的反向技术使模型能够更好地捕捉序列中信道瞬间之间的关系，无论序列长度如何，与现有方法相比有更好的效果。

    This work aims to predict channels in wireless communication systems based on noisy observations, utilizing sequence-to-sequence models with attention (Seq2Seq-attn) and transformer models. Both models are adapted from natural language processing to tackle the complex challenge of channel prediction. Additionally, a new technique called reverse positional encoding is introduced in the transformer model to improve the robustness of the model against varying sequence lengths. Similarly, the encoder outputs of the Seq2Seq-attn model are reversed before applying attention. Simulation results demonstrate that the proposed ordering techniques allow the models to better capture the relationships between the channel snapshots within the sequence, irrespective of the sequence length, as opposed to existing methods.
    
[^106]: NeSyFOLD: 从卷积神经网络中提取逻辑程序

    NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks. (arXiv:2301.12667v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12667](http://arxiv.org/abs/2301.12667)

    NeSyFOLD是一种神经符号框架，可以从CNN中提取逻辑规则并生成可解释的分类模型。它使用基于规则的FOLD-SE-M机器学习算法和自动映射算法来将CNN核映射到语义概念，并产生可解释的规则集。

    

    我们提出了一种新的神经符号框架NeSyFOLD，从CNN中提取逻辑规则并创建一个NeSyFOLD模型来对图像进行分类。NeSyFOLD的学习流程如下：（i）我们首先在输入图像数据集上预训练CNN，并提取最后一层核的激活作为二进制值；（ii）接下来，我们使用基于规则的FOLD-SE-M机器学习算法生成能够分类图像的逻辑程序——表示为每个核对应的二进制激活向量，同时产生逻辑解释。由FOLD-SE-M算法生成的规则具有核编号作为谓词。我们设计了一种新的算法，用于自动将CNN核映射到图像中的语义概念。这个映射被用来将规则集中的谓词名（核编号）替换为对应的语义概念标签。结果产生了可解释的规则集，可以被人类直观地理解。我们将我们的NeSyFOLD框架与最先进的方法进行了比较，并表明它可以实现竞争性的分类性能，同时提供可解释的和解释性的知识。

    We present a novel neurosymbolic framework called NeSyFOLD to extract logic rules from a CNN and create a NeSyFOLD model to classify images. NeSyFOLD's learning pipeline is as follows: (i) We first pre-train a CNN on the input image dataset and extract activations of the last layer kernels as binary values; (ii) Next, we use the FOLD-SE-M rule-based machine learning algorithm to generate a logic program that can classify an image -- represented as a vector of binary activations corresponding to each kernel -- while producing a logical explanation. The rules generated by the FOLD-SE-M algorithm have kernel numbers as predicates. We have devised a novel algorithm for automatically mapping the CNN kernels to semantic concepts in the images. This mapping is used to replace predicate names (kernel numbers) in the rule-set with corresponding semantic concept labels. The resulting rule-set is interpretable, and can be intuitively understood by humans. We compare our NeSyFOLD framework with th
    
[^107]: 基于音频条件扩散模型的语音驱动视频编辑

    Speech Driven Video Editing via an Audio-Conditioned Diffusion Model. (arXiv:2301.04474v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.04474](http://arxiv.org/abs/2301.04474)

    本文提出了一种使用音频条件下的扩散模型进行端到端语音驱动视频编辑的方法，通过条件生成器实现对唇部和下巴运动的同步，避免了对中间结构表示的依赖。

    

    本文受到最近扩散模型在视觉生成任务中的应用启发，提出了一种使用去噪扩散模型实现端到端语音驱动视频编辑的方法。给定一个说话者的视频和单独的音频录音，通过将音频梅尔频谱特征作为条件，重新同步唇部和下巴运动，而不依赖于中间的结构表示，如面部标志或3D面部模型。我们在CREMA-D音频视觉数据集上展示了单说话人和多说话人视频编辑的结果，提供了一个基准模型。据我们所知，这是第一个证明和验证应用端到端去噪扩散模型到语音驱动视频编辑任务中的可行性的工作。

    Taking inspiration from recent developments in visual generative tasks using diffusion models, we propose a method for end-to-end speech-driven video editing using a denoising diffusion model. Given a video of a talking person, and a separate auditory speech recording, the lip and jaw motions are re-synchronized without relying on intermediate structural representations such as facial landmarks or a 3D face model. We show this is possible by conditioning a denoising diffusion model on audio mel spectral features to generate synchronised facial motion. Proof of concept results are demonstrated on both single-speaker and multi-speaker video editing, providing a baseline model on the CREMA-D audiovisual data set. To the best of our knowledge, this is the first work to demonstrate and validate the feasibility of applying end-to-end denoising diffusion models to the task of audio-driven video editing.
    
[^108]: 核子空间和特征提取

    Kernel Subspace and Feature Extraction. (arXiv:2301.01410v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01410](http://arxiv.org/abs/2301.01410)

    本文研究了机器学习中的核方法，建立了特征子空间和核之间的一一对应关系，并提出了一个信息熵度量方法用于核的比较。特别地，构建了一个极大相关核，并证明其在信息熵度量方面的最优性。最后，把Fisher核解释为一种特殊的极大相关核，并建立了它的最优性。

    

    本文从特征子空间的角度研究了机器学习中的核方法，建立了特征子空间和核之间的一一对应关系，并提出了一个信息熵度量方法用于核的比较。特别地，我们构建了一个由Hirschfeld-Gebelein-R\'{e}nyi极大相关函数构成的核，称之为极大相关核，并展示了它在信息熵度量方面的最优性。我们以支持向量机(SVM)为例，将核方法与特征提取方法联系起来，并表明在极大相关核上的核SVM具有最小预测误差。最后，我们把Fisher核解释为一种特殊的极大相关核，并建立了它的最优性。

    We study kernel methods in machine learning from the perspective of feature subspace. We establish a one-to-one correspondence between feature subspaces and kernels and propose an information-theoretic measure for kernels. In particular, we construct a kernel from Hirschfeld--Gebelein--R\'{e}nyi maximal correlation functions, coined the maximal correlation kernel, and demonstrate its information-theoretic optimality. We use the support vector machine (SVM) as an example to illustrate a connection between kernel methods and feature extraction approaches. We show that the kernel SVM on maximal correlation kernel achieves minimum prediction error. Finally, we interpret the Fisher kernel as a special maximal correlation kernel and establish its optimality.
    
[^109]: 自然语言处理任务的持续学习：一项调查

    Continual Learning of Natural Language Processing Tasks: A Survey. (arXiv:2211.12701v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.12701](http://arxiv.org/abs/2211.12701)

    本文综述了NLP中持续学习的最新进展，其中CF预防、知识迁移和跨任务类分离等方面对NLP任务至关重要，并讨论了未来研究方向。

    

    持续学习是一种学习范式，模拟人类不断学习和积累知识的能力，不会忘记之前学过的知识，并将学到的知识传递给新任务更好地学习。本文调查了NLP中CL的最新进展，它与计算机视觉和机器学习中的CL有显着区别。它涵盖了（1）所有CL设置及现有技术分类；（2）防止灾难性遗忘（CF）；（3）知识迁移（KT），对NLP任务尤其重要；以及（4）一些理论和交任务类分离（ICS）的隐含挑战。本文还讨论了未来方向的一些列表。

    Continual learning (CL) is a learning paradigm that emulates the human capability of learning and accumulating knowledge continually without forgetting the previously learned knowledge and also transferring the learned knowledge to help learn new tasks better. This survey presents a comprehensive review and analysis of the recent progress of CL in NLP, which has significant differences from CL in computer vision and machine learning. It covers (1) all CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting (CF) prevention, (3) knowledge transfer (KT), which is particularly important for NLP tasks; and (4) some theory and the hidden challenge of inter-task class separation (ICS). (1), (3) and (4) have not been included in the existing survey. Finally, a list of future directions is discussed.
    
[^110]: 多视角压缩表示的鲁棒性低资源微调研究

    Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08794](http://arxiv.org/abs/2211.08794)

    本文提出了一种利用多视角压缩表示降低预训练语言模型微调过程中过拟合问题的方法，经过测试在低资源NLP任务中表现良好。

    

    由于参数的巨大数量，预训练语言模型（PLMs）的微调容易在低资源场景中出现过度拟合的问题。本文提出了一种新方法，该方法在PLM的隐藏表示上操作，以减少过拟合。在微调过程中，我们的方法在PLM的隐藏层之间插入随机自编码器，将来自前一层的激活转换为多视角压缩表示，然后将其馈送到上层。微调结束后，自编码器会被移除掉，因此我们的方法在推理过程中不会增加额外的参数或计算成本。我们的方法在一系列序列和标记级别的低资源NLP任务中展现了出色的性能提升。

    Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
    
[^111]: 从去噪扩散到去噪马尔科夫模型

    From Denoising Diffusions to Denoising Markov Models. (arXiv:2211.03595v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.03595](http://arxiv.org/abs/2211.03595)

    本论文提出了一个统一的框架，将去噪扩散模型推广到广泛的空间中，并导致分数匹配的原始扩展，适用于各种应用程序。

    

    去噪扩散是展现出卓越实验性能的最先进的生成模型。他们通过将数据分布扩散到高斯分布，然后学习逆转这个噪声过程以获取合成数据点。去噪扩散依赖于使用分数匹配对噪声数据密度的对数导数的逼近。当只能从先验分布和似然函数中进行抽样时，这种模型也可用于执行近似后验模拟。我们提出了一个统一框架，将此方法推广到一类广泛的空间，并导致分数匹配的原始扩展。我们通过各种应用程序说明了所得模型。

    Denoising diffusions are state-of-the-art generative models exhibiting remarkable empirical performance. They work by diffusing the data distribution into a Gaussian distribution and then learning to reverse this noising process to obtain synthetic datapoints. The denoising diffusion relies on approximations of the logarithmic derivatives of the noised data densities using score matching. Such models can also be used to perform approximate posterior simulation when one can only sample from the prior and likelihood. We propose a unifying framework generalising this approach to a wide class of spaces and leading to an original extension of score matching. We illustrate the resulting models on various applications.
    
[^112]: 带承诺和噪声观测的$2\times 2$零和博弈研究

    $2 \times 2$ Zero-Sum Games with Commitments and Noisy Observations. (arXiv:2211.01703v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2211.01703](http://arxiv.org/abs/2211.01703)

    本文研究了带承诺和噪声观测的$2\times 2$零和博弈，发现平衡点总是存在的；领导者的动作观测结果对于追随者来说要么是有益的，要么是无关紧要的；该博弈的收益在均衡点上被上界限制为纯策略下的SE的收益，下界为混合策略下的纳什均衡的收益。

    

    本文研究了在以下假设下的$2\times 2$零和博弈：$(1)$其中一位玩家（领导者）承诺通过采样给定的概率分布（策略）来选择他的动作;$(2)$领导者宣布他的动作，这个动作通过二进制信道被对手（追随者）观察到;$(3)$追随者基于领导者的策略和领导者动作的噪声观测来选择她的策略。在这些条件下，平衡点被证明总是存在的。有趣的是，即使受到噪声的影响，观察领导者的行动对追随者来说实质上要么是有益的，要么是无关紧要的。具体而言，在这个博弈的均衡点上，收益被上界限制为纯策略下SE的收益；并且下界为纳什均衡的收益，这等价于混合策略下的SE。最后，我们提供了必要和充分的条件来观察均衡点的收益。

    In this paper, $2\times2$ zero-sum games are studied under the following assumptions: $(1)$ One of the players (the leader) commits to choose its actions by sampling a given probability measure (strategy); $(2)$ The leader announces its action, which is observed by its opponent (the follower) through a binary channel; and $(3)$ the follower chooses its strategy based on the knowledge of the leader's strategy and the noisy observation of the leader's action. Under these conditions, the equilibrium is shown to always exist. Interestingly, even subject to noise, observing the actions of the leader is shown to be either beneficial or immaterial for the follower. More specifically, the payoff at the equilibrium of this game is upper bounded by the payoff at the Stackelberg equilibrium (SE) in pure strategies; and lower bounded by the payoff at the Nash equilibrium, which is equivalent to the SE in mixed strategies.Finally, necessary and sufficient conditions for observing the payoff at equi
    
[^113]: 不灵活的多资产对冲不完全市场

    Inflexible Multi-Asset Hedging of incomplete market. (arXiv:2211.00948v2 [q-fin.ST] UPDATED)

    [http://arxiv.org/abs/2211.00948](http://arxiv.org/abs/2211.00948)

    本文提出了一种在不完全市场下对冲的方法，使用新的跳跃扩散模型和三种神经网络获得了较好的结果。

    

    通常情况下，基于完全市场的假设训练出的模型在不完全市场中不起作用。本文解决了存在风险因素、无流动性和离散交易日期三种不完整性的不完全市场对冲问题。提出了一种新的跳跃扩散模型描述了随机资产价格。采用了包括RNN、LSTM和Mogrifier-LSTM在内的三种神经网络来实现对冲策略，并实施和比较了MSE Loss和Huber Loss。结果表明，Mogrifier-LSTM是速度最快、在MSE和Huber Loss下结果最好的模型。

    Models trained under assumptions in the complete market usually don't take effect in the incomplete market. This paper solves the hedging problem in incomplete market with three sources of incompleteness: risk factor, illiquidity, and discrete transaction dates. A new jump-diffusion model is proposed to describe stochastic asset prices. Three neutral networks, including RNN, LSTM, Mogrifier-LSTM are used to attain hedging strategies with MSE Loss and Huber Loss implemented and compared.As a result, Mogrifier-LSTM is the fastest model with the best results under MSE and Huber Loss.
    
[^114]: 在多次攻击下提高高光谱对抗鲁棒性

    Improving Hyperspectral Adversarial Robustness Under Multiple Attacks. (arXiv:2210.16346v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16346](http://arxiv.org/abs/2210.16346)

    该论文提出了一种Adversarial Discriminator Ensemble Network（ADE-Net）来增强高光谱图像模型的鲁棒性，针对不同攻击类型使用相应的攻击专家集合网络。

    

    对高光谱图像进行语义分割的模型容易受到对抗样本的攻击。传统的对抗鲁棒性方法主要集中在对单个网络进行训练或重新训练，但是在遇到多种攻击时，这些方法的性能会下降，比单独训练每个网络都要差。为了解决这个问题，我们提出了一种Adversarial Discriminator Ensemble Network（ADE-Net），该网络专注于攻击类型检测和鲁棒性，在一个统一的模型下保留最佳的每个数据类型权重，同时增强整个网络的鲁棒性。在所提出的方法中，使用鉴别器网络将数据按攻击类型进行分离，进而针对特定攻击类型使用相应的攻击专家集合网络。

    Semantic segmentation models classifying hyperspectral images (HSI) are vulnerable to adversarial examples. Traditional approaches to adversarial robustness focus on training or retraining a single network on attacked data, however, in the presence of multiple attacks these approaches decrease in performance compared to networks trained individually on each attack. To combat this issue we propose an Adversarial Discriminator Ensemble Network (ADE-Net) which focuses on attack type detection and adversarial robustness under a unified model to preserve per data-type weight optimally while robustifiying the overall network. In the proposed method, a discriminator network is used to separate data by attack type into their specific attack-expert ensemble network.
    
[^115]: 通过联合分类和多个明确检测类来提高对抗鲁棒性

    Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes. (arXiv:2210.14410v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.14410](http://arxiv.org/abs/2210.14410)

    本文提出一种联合分类和多个明确检测类的方法来提高对抗鲁棒性，在保证可验证的防御机制的基础上，实现了对具有多个明确弃权类别的网络的保障，并通过正则化方法和训练方法对抗了模型的退化。所提出的方法在实现有利的标准和鲁棒性验证准确性平衡点方面表现出色，比现有算法更出色。

    

    本文关注于开发能够在对抗性攻击下有保障的深度神经网络。联合鲁棒性分类和检测被最近引入作为一种可验证的防御机制，其中对抗性示例被正确分类或分配到“弃权”类别。在本文中，我们表明这样的可证明框架可以通过扩展到具有多个明确弃权类别的网络中而获益，其中对抗性示例被适应地分配到那些类别。我们表明，简单地添加多个弃权类别可能会导致“模型退化”，然后我们提出了一种正则化方法和训练方法来对抗这种退化，通过促进充分使用多个弃权类别。我们的实验表明，所提出的方法一致地实现了有利的标准和鲁棒性验证准确性平衡点，并在各种选择弃权类别数量的情况下优于最先进的算法。

    This work concerns the development of deep networks that are certifiably robust to adversarial attacks. Joint robust classification-detection was recently introduced as a certified defense mechanism, where adversarial examples are either correctly classified or assigned to the "abstain" class. In this work, we show that such a provable framework can benefit by extension to networks with multiple explicit abstain classes, where the adversarial examples are adaptively assigned to those. We show that naively adding multiple abstain classes can lead to "model degeneracy", then we propose a regularization approach and a training method to counter this degeneracy by promoting full use of the multiple abstain classes. Our experiments demonstrate that the proposed approach consistently achieves favorable standard vs. robust verified accuracy tradeoffs, outperforming state-of-the-art algorithms for various choices of number of abstain classes.
    
[^116]: 论多动作策略梯度

    On Many-Actions Policy Gradient. (arXiv:2210.13011v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13011](http://arxiv.org/abs/2210.13011)

    本论文研究了具有多个动作样本的随机策略梯度的方差问题，并提出了一种基于动态模型的多动作采样方法，实现了更高的样本效率和更高的回报。

    

    我们研究了具有多个动作样本的随机策略梯度（SPG）的方差。我们得出了一个多动作最优性条件，它决定了当与比例扩展轨迹的单动作代理相比，多动作SPG产生比较低的方差。我们提出了一种称为基于模型的多动作（MBMA）的方法，在SPG背景下利用动态模型进行多动作采样，以解决现有多动作SPG实现所涉及的问题，并在模型模拟的回合中提供与SPG相当的偏差和方差。我们发现，MBMA的偏差和方差结构与理论预测的相匹配。因此，在一系列连续动作环境中，MBMA与无模型，多动作和基于模型的策略梯度基线相比，实现了更高的样本效率和更高的回报。

    We study the variance of stochastic policy gradients (SPGs) with many action samples per state. We derive a many-actions optimality condition, which determines when many-actions SPG yields lower variance as compared to a single-action agent with proportionally extended trajectory. We propose Model-Based Many-Actions (MBMA), an approach leveraging dynamics models for many-actions sampling in the context of SPG. MBMA addresses issues associated with existing implementations of many-actions SPG and yields lower bias and comparable variance to SPG estimated from states in model-simulated rollouts. We find that MBMA bias and variance structure matches that predicted by theory. As a result, MBMA achieves improved sample efficiency and higher returns on a range of continuous action environments as compared to model-free, many-actions, and model-based on-policy SPG baselines.
    
[^117]: 深度线性网络用于矩阵完成——无穷深度极限。

    Deep Linear Networks for Matrix Completion -- An Infinite Depth Limit. (arXiv:2210.12497v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2210.12497](http://arxiv.org/abs/2210.12497)

    本文研究了深度线性网络用于矩阵完成的训练过程，得到了黎曼几何和训练渐近性之间的关系，证明了隐式正则化是高状态空间体积偏见的结果。

    

    深度线性网络（DLN）是一种用于超参数学习结构的梯度下降隐式正则化模型。训练DLN对应于黎曼梯度流，其中黎曼度量由网络结构定义，损失函数由学习任务定义。我们扩展了这一几何框架，得到了卷积体积形式的显式表达式，包括网络具有无穷深度的情况。我们通过严格的分析和数值研究研究了黎曼几何和矩阵完成训练的渐近性之间的关系。我们建议，隐式正则化是高状态空间体积偏见的结果。

    The deep linear network (DLN) is a model for implicit regularization in gradient based optimization of overparametrized learning architectures. Training the DLN corresponds to a Riemannian gradient flow, where the Riemannian metric is defined by the architecture of the network and the loss function is defined by the learning task. We extend this geometric framework, obtaining explicit expressions for the volume form, including the case when the network has infinite depth. We investigate the link between the Riemannian geometry and the training asymptotics for matrix completion with rigorous analysis and numerics. We propose that implicit regularization is a result of bias towards high state space volume.
    
[^118]: 贝叶斯赌博机问题的连续时间极限

    Continuous-in-time Limit for Bayesian Bandits. (arXiv:2210.07513v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.07513](http://arxiv.org/abs/2210.07513)

    本文提出了一种适用于解决大时间长度下的贝叶斯赌博机问题的近似贝叶斯最优策略，并且其计算成本不包括依赖于时间长度的项。

    

    本文重新审视了贝叶斯设置下的赌博机问题。贝叶斯方法将赌博机问题制定为一个优化问题，旨在寻找最优策略以最小化贝叶斯遗憾。面对的主要挑战之一是，当问题的时间长度或臂数较大时，计算最优策略通常是不可行的。我们首先展示了在适当的重缩放下，贝叶斯赌博机问题收敛于一个连续的哈密尔顿 - 雅各比 - 贝尔曼（HJB）方程。对于常见的一些赌博机问题，可以明确获得极限HJB方程的最优策略，并且在无法明确解决方案的情况下，我们提供了解决HJB方程的数字方法。基于这些结果，我们提出了一种适用于解决大时间长度下的贝叶斯赌博机问题的近似贝叶斯最优策略。我们的方法的计算成本不包括依赖于时间长度的项，这与现有方法不同。数值模拟表明了我们方法的有效性。

    This paper revisits the bandit problem in the Bayesian setting. The Bayesian approach formulates the bandit problem as an optimization problem, and the goal is to find the optimal policy which minimizes the Bayesian regret. One of the main challenges facing the Bayesian approach is that computation of the optimal policy is often intractable, especially when the length of the problem horizon or the number of arms is large. In this paper, we first show that under a suitable rescaling, the Bayesian bandit problem converges toward a continuous Hamilton-Jacobi-Bellman (HJB) equation. The optimal policy for the limiting HJB equation can be explicitly obtained for several common bandit problems, and we give numerical methods to solve the HJB equation when an explicit solution is not available. Based on these results, we propose an approximate Bayes-optimal policy for solving Bayesian bandit problems with large horizons. Our method has the added benefit that its computational cost does not inc
    
[^119]: 使用全文内容表征和识别畅销书

    Using Full-Text Content to Characterize and Identify Best Seller Books. (arXiv:2210.02334v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.02334](http://arxiv.org/abs/2210.02334)

    该研究通过对书籍的全文内容进行可视化和分类任务，研究预测书籍是否会成为畅销书。使用了 SemAxis 和线性判别分析进行数据初步探索，采用多种分类器获得定量和更加客观的结果。

    

    艺术作品可以从多个角度进行研究，其中一个例子是它们在读者中的接受情况。在本文中，我们从文艺作品的角度出发，特别是评估预测书籍是否会成为畅销书的任务。与以往的方法不同，我们专注于书籍的全文内容，并考虑了可视化和分类任务。我们使用 SemAxis 和线性判别分析进行数据结构和属性的初步探索。为了获得定量和更加客观的结果，我们采用了各种分类器。这些方法与数据集一起使用，该数据集包含从1895年到1924年出版的书籍，并被《出版周刊畅销书榜》确定为畅销书和在同一时期出版但未被提及的文学作品。我们方法比较的结果表明，最好的成绩是...

    Artistic pieces can be studied from several perspectives, one example being their reception among readers over time. In the present work, we approach this interesting topic from the standpoint of literary works, particularly assessing the task of predicting whether a book will become a best seller. Dissimilarly from previous approaches, we focused on the full content of books and considered visualization and classification tasks. We employed visualization for the preliminary exploration of the data structure and properties, involving SemAxis and linear discriminant analyses. Then, to obtain quantitative and more objective results, we employed various classifiers. Such approaches were used along with a dataset containing (i) books published from 1895 to 1924 and consecrated as best sellers by the Publishers Weekly Bestseller Lists and (ii) literary works published in the same period but not being mentioned in that list. Our comparison of methods revealed that the best-achieved result 
    
[^120]: 面向法律领域的预训练语言模型研究：以印度法律为例

    Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.06049](http://arxiv.org/abs/2209.06049)

    本研究针对印度法律文本，重新训练和从零开始训练了两个PLMs，即LegalBERT和CaseLawBERT，并采用基于印度法律文本的词汇表训练了一个模型。我们在几项基准法律NLP任务中，对印度和非印度的法律文本进行了应用。

    

    随着基于Transformer预训练语言模型（PLMs）在法律领域中应用的增多，特别是在欧美法律文本方面，PLMs获得了显著的成功。然而，印度等其他国家的法律文本具有很多特殊特征，因此也需要在这些方面进行预训练。本文尝试在印度法律领域进行预训练。我们在印度法律数据上重新训练（继续预训练）了两个流行的法律PLMs, LegalBERT和CaseLawBERT，以及使用基于印度法律文本的词汇表从零开始训练了一个模型。我们将这些PLMs应用于三个基准法律NLP任务——从事实中识别法律法规、对法院判决文件进行语义分割，以及预测法院上诉判决--在印度和非印度的文本上。

    NLP in the legal domain has seen increasing success with the emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text. PLMs trained over European and US legal text are available publicly; however, legal text from other domains (countries), such as India, have a lot of distinguishing characteristics. With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well. In this work, we attempt to investigate pre-training in the Indian legal domain. We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text. We apply these PLMs over three benchmark legal NLP tasks -Legal Statute Identification from facts, Semantic Segmentation of Court Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian and non-
    
[^121]: 创造智能：智商和机器学习基准的伦理价值

    Making Intelligence: Ethical Values in IQ and ML Benchmarks. (arXiv:2209.00692v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.00692](http://arxiv.org/abs/2209.00692)

    本文探讨了人类智慧基准和机器学习（ML）基准之间的相似之处，并强调了在创建ML基准时应考虑和记录价值观。

    

    近年来，机器学习（ML）研究人员一直在定义和改进机器学习基准和数据集。与此同时，一些人在关注数据集创建和ML研究的伦理道德。在本文中，我们强调了伦理和看似“技术”或“科学”决策在ML基准设计中的纠缠。我们的出发点是人类智力基准和ML基准之间存在多重被忽视的结构相似性。这两种基准都设定了用于描述、评估和比较智力相关任务表现的标准，许多人类智力学者长期以来已经认识到这些标准是价值观负载的。我们利用女性主义科学哲学和社会科学中的厚概念视角来 论证，在创建ML基准时需要考虑和记录价值观。通过创建关于数据集本身的规范，而不是隐含的伦理，可以避免不必要的选择。

    In recent years, ML researchers have wrestled with defining and improving machine learning (ML) benchmarks and datasets. In parallel, some have trained a critical lens on the ethics of dataset creation and ML research. In this position paper, we highlight the entanglement of ethics with seemingly ``technical'' or ``scientific'' decisions about the design of ML benchmarks. Our starting point is the existence of multiple overlooked structural similarities between human intelligence benchmarks and ML benchmarks. Both types of benchmarks set standards for describing, evaluating, and comparing performance on tasks relevant to intelligence -- standards that many scholars of human intelligence have long recognized as value-laden. We use perspectives from feminist philosophy of science on IQ benchmarks and thick concepts in social science to argue that values need to be considered and documented when creating ML benchmarks. It is neither possible nor desirable to avoid this choice by creating 
    
[^122]: 利用现成的图像生成和字幕生成技术发现视觉模型中的错误

    Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning. (arXiv:2208.08831v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.08831](http://arxiv.org/abs/2208.08831)

    本研究利用现成的图像生成和字幕生成技术，自动发现视觉模型中的错误。通过生成大量合成但逼真的输入，聚类和描述，评估和发现分类器的失败和虚假相关性。这为未来构建实用的框架提供了一个有前途的思路。

    

    在真实世界的情境下自动发现视觉模型的失败仍然是一个开放的挑战。本研究展示了如何利用大量数据训练的现成大型图像到文本和文本到图像模型来自动发现这些失败。本研究将条件文本到图像生成模型用于生成大量合成但逼真的输入。分类错误的输入将被聚类并使用字幕生成模型来描述每个聚类。每个聚类的描述依次用于生成更多输入并评估是否特定聚类引起的失败超出了预期。我们利用此流程演示了我们可以有效地调查在ImageNet上训练的分类器，找到特定失败案例并发现虚假相关性。我们还展示了我们可以扩展此方法，生成针对特定分类器架构的对抗数据集。本研究作为一个实用的框架的概念验证，用于发现视觉模型中的失误。

    Automatically discovering failures in vision models under real-world settings remains an open challenge. This work demonstrates how off-the-shelf, large-scale, image-to-text and text-to-image models, trained on vast amounts of data, can be leveraged to automatically find such failures. In essence, a conditional text-to-image generative model is used to generate large amounts of synthetic, yet realistic, inputs given a ground-truth label. Misclassified inputs are clustered and a captioning model is used to describe each cluster. Each cluster's description is used in turn to generate more inputs and assess whether specific clusters induce more failures than expected. We use this pipeline to demonstrate that we can effectively interrogate classifiers trained on ImageNet to find specific failure cases and discover spurious correlations. We also show that we can scale the approach to generate adversarial datasets targeting specific classifier architectures. This work serves as a proof-of-co
    
[^123]: FedOBD：机会主义块丢弃在联邦学习中高效训练大型神经网络

    FedOBD: Opportunistic Block Dropout for Efficiently Training Large-scale Neural Networks through Federated Learning. (arXiv:2208.05174v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.05174](http://arxiv.org/abs/2208.05174)

    FedOBD是一个新的联邦学习方法，它通过将大型模型分解为语义块，允许参与者机会主义地上传量化的块来进行聚合，以此解决了在大规模联邦学习模型训练过程中操作模型参数所产生的通信开销和性能问题。

    

    大型神经网络具有相当的表达能力，非常适合行业应用中的复杂学习任务。然而，大型模型在当前的联邦学习范式下训练存在显著的挑战。现有的高效联邦学习训练方法通常利用模型参数dropout，但单独操作模型参数不仅在大规模联邦学习模型训练中无法有效减少通信开销，而且还可能对缩放和模型性能产生不利影响。为了解决这些问题，我们提出了联邦机会主义块丢弃（FedOBD）方法。其关键创新点在于将大型模型分解为语义块，以便联邦学习参与者可以机会主义地上传量化的块，这些块被认为对于训练模型非常重要，以进行聚合。通过大量的实验证明了FedOBD的有效性。

    Large-scale neural networks possess considerable expressive power. They are well-suited for complex learning tasks in industrial applications. However, large-scale models pose significant challenges for training under the current Federated Learning (FL) paradigm. Existing approaches for efficient FL training often leverage model parameter dropout. However, manipulating individual model parameters is not only inefficient in meaningfully reducing the communication overhead when training large-scale FL models, but may also be detrimental to the scaling efforts and model performance as shown by recent research. To address these issues, we propose the Federated Opportunistic Block Dropout (FedOBD) approach. The key novelty is that it decomposes large-scale models into semantic blocks so that FL participants can opportunistically upload quantized blocks, which are deemed to be significant towards training the model, to the FL server for aggregation. Extensive experiments evaluating FedOBD ag
    
[^124]: PointConvFormer：基于注意力机制的点云卷积神经网络构建模块

    PointConvFormer: Revenge of the Point-based Convolution. (arXiv:2208.02879v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.02879](http://arxiv.org/abs/2208.02879)

    PointConvFormer 是一种新颖的点云深度网络架构构建模块，它利用特征差异计算出的注意力来修改卷积权重，保留了点卷积的不变性，同时能够选择具有相关性的点进行卷积操作，适用于点级细节的多个任务，既提高了精度，又提高了速度。

    

    我们介绍了PointConvFormer，这是一种用于点云深度网络架构的新颖构建模块。PointConvFormer结合了点卷积和Transformer思想，利用特征差异计算出的注意力来修改每个点的卷积权重。它适用于需要点级细节的多个任务，如分割和场景流估计任务。我们在多个数据集上进行了实验，包括ScanNet、SemanticKitti、FlyingThings3D和KITTI，结果表明PointConvFormer比经典卷积、常规Transformer和体素卷积提供更好的精度-速度平衡。

    We introduce PointConvFormer, a novel building block for point cloud based deep network architectures. Inspired by generalization theory, PointConvFormer combines ideas from point convolution, where filter weights are only based on relative position, and Transformers which utilize feature-based attention. In PointConvFormer, attention computed from feature difference between points in the neighborhood is used to modify the convolutional weights at each point. Hence, we preserved the invariances from point convolution, whereas attention helps to select relevant points in the neighborhood for convolution. PointConvFormer is suitable for multiple tasks that require details at the point level, such as segmentation and scene flow estimation tasks. We experiment on both tasks with multiple datasets including ScanNet, SemanticKitti, FlyingThings3D and KITTI. Our results show that PointConvFormer offers a better accuracy-speed tradeoff than classic convolutions, regular transformers, and voxel
    
[^125]: “在线一致性”：预测在分布转换下神经网络的性能表现

    Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift. (arXiv:2206.13089v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.13089](http://arxiv.org/abs/2206.13089)

    本研究发现神经网络内部分布的准确性与在分布转换下的准确性具有强线性相关性。这种现象也适用于一对神经网络分类器之间的一致性。我们提出了“在线一致性”这一理论，并展示其在估算没有标记的OOD验证集时的效果优于现有基线检测模型。

    

    最近，Miller等人展示了模型在内部分布（ID）上的准确性与其在几个OOD基准上的准确性具有强烈的线性相关性，他们称之为“准确性在线”。 虽然这对于模型选择（即，ID准确度最高的模型最有可能表现最好OOD）是有用的工具，但这一事实无法帮助估计没有标记的OOD验证集的模型实际OOD表现。在本文中，我们展示了类似但令人惊讶的现象也适用于一对神经网络分类器之间的一致性：每当准确性在线成立时，我们观察到任意两对神经网络（具有潜在不同体系结构）的预测在OOD上的协议也与其ID协议之间具有强烈的线性相关性。此外，我们观察到OOD与ID协议的斜率和偏差与OOD与ID准确度非常接近。这种现象被我们称为“在线一致性”，它提供了一种实际的方法来估计OOD性能，而不需要访问标记的OOD验证集。 我们通过在几个公共基准测试（ImageNet、CIFAR-10-C和-10-P）上部署我们的方法来展示这一点，并展示它优于现有的OOD检测基线。

    Recently, Miller et al. showed that a model's in-distribution (ID) accuracy has a strong linear correlation with its out-of-distribution (OOD) accuracy on several OOD benchmarks -- a phenomenon they dubbed ''accuracy-on-the-line''. While a useful tool for model selection (i.e., the model most likely to perform the best OOD is the one with highest ID accuracy), this fact does not help estimate the actual OOD performance of models without access to a labeled OOD validation set. In this paper, we show a similar but surprising phenomenon also holds for the agreement between pairs of neural network classifiers: whenever accuracy-on-the-line holds, we observe that the OOD agreement between the predictions of any two pairs of neural networks (with potentially different architectures) also observes a strong linear correlation with their ID agreement. Furthermore, we observe that the slope and bias of OOD vs ID agreement closely matches that of OOD vs ID accuracy. This phenomenon, which we call
    
[^126]: 证明有效的风险敏感强化学习：迭代CVaR和最坏路径

    Provably Efficient Risk-Sensitive Reinforcement Learning: Iterated CVaR and Worst Path. (arXiv:2206.02678v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02678](http://arxiv.org/abs/2206.02678)

    本文提出了迭代CVaR和最坏路径的风险敏感强化学习问题，并设计了有效算法解决该问题，适用于实际应用中需要避免风险的任务。

    

    本文研究了一种名为迭代CVaR强化学习的新型风险敏感强化学习问题，旨在最大化每步回报的尾部，并专注于紧密控制每个阶段进入灾难性情况的风险。这种制定适用于需要在决策过程中强烈避免风险的现实任务，如自动驾驶、临床治疗规划和机器人。我们研究了迭代CVaR强化学习中的两个性能指标，即遗憾最小化和最佳策略识别。对于这两个指标，我们分别设计了有效的算法ICVaR-RM和ICVaR-BPI，并提供了与剧集数$K$相匹配的上限和下限。我们还研究了迭代CVaR强化学习的一个有趣的极限情况，称为最坏路径强化学习，其中目标是最大化可能的最小累积回报。对于最坏路径强化学习，我们提出了一种具有严格性能保证的高效算法。我们的理论分析和数值实验展示了我们所提出的算法在解决风险敏感强化学习问题方面的有效性和效率。

    In this paper, we study a novel episodic risk-sensitive Reinforcement Learning (RL) problem, named Iterated CVaR RL, which aims to maximize the tail of the reward-to-go at each step, and focuses on tightly controlling the risk of getting into catastrophic situations at each stage. This formulation is applicable to real-world tasks that demand strong risk avoidance throughout the decision process, such as autonomous driving, clinical treatment planning and robotics. We investigate two performance metrics under Iterated CVaR RL, i.e., Regret Minimization and Best Policy Identification. For both metrics, we design efficient algorithms ICVaR-RM and ICVaR-BPI, respectively, and provide nearly matching upper and lower bounds with respect to the number of episodes $K$. We also investigate an interesting limiting case of Iterated CVaR RL, called Worst Path RL, where the objective becomes to maximize the minimum possible cumulative reward. For Worst Path RL, we propose an efficient algorithm wi
    
[^127]: 利用扩频因子的LoRa定位与深度强化学习

    Spreading Factor assisted LoRa Localization with Deep Reinforcement Learning. (arXiv:2205.11428v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2205.11428](http://arxiv.org/abs/2205.11428)

    该论文提出了一种利用SF的LoRa RSSI指纹方法，通过深度强化学习模型以应对LoRa网络复杂性和可扩展性问题，定位精度比最先进方法提高6.67％到48.10％不等。

    

    大多数实现定位的解决方案依赖于RSSI指纹。然而，在LoRa网络中，由于网络设置中的扩频因子（SF），传统指纹可能缺乏无线电地图的代表性，导致位置估计不准确。因此，在本研究中，我们提出了一种考虑SF的新型LoRa RSSI指纹方法。性能评估显示了我们提出的方法的卓越性，因为与最先进的方法相比，我们实现了最多6.67％的定位精度提高。评估是使用基线全连接深度神经网络（DNN）进行的。为了进一步提高定位精度，我们提出了一种深度强化学习模型，以捕捉LoRa网络的不断增长的复杂性，并应对其可扩展性。所获得的结果显示，与基线DNN模型相比，在定位精度方面提高了48.10％。

    Most of the developed localization solutions rely on RSSI fingerprinting. However, in the LoRa networks, due to the spreading factor (SF) in the network setting, traditional fingerprinting may lack representativeness of the radio map, leading to inaccurate position estimates. As such, in this work, we propose a novel LoRa RSSI fingerprinting approach that takes into account the SF. The performance evaluation shows the prominence of our proposed approach since we achieved an improvement in localization accuracy by up to 6.67% compared to the state-of-the-art methods. The evaluation has been done using a fully connected deep neural network (DNN) set as the baseline. To further improve the localization accuracy, we propose a deep reinforcement learning model that captures the ever-growing complexity of LoRa networks and copes with their scalability. The obtained results show an improvement of 48.10% in the localization accuracy compared to the baseline DNN model.
    
[^128]: LGBTQ在线社群在COVID-19疫情期间经历的少数群体压力

    Minority Stress Experienced by LGBTQ Online Communities during the COVID-19 Pandemic. (arXiv:2205.09511v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2205.09511](http://arxiv.org/abs/2205.09511)

    研究探究了LGBTQ在线社群在COVID-19疫情期间经历的少数群体压力。利用机器学习分类器检测Twitter上表现出的少数群体压力，比较疫情前后的语言差异。

    

    COVID-19疫情的影响在少数族群中产生了不成比例的影响，如LGBTQ社群（女同性恋、男同性恋、双性恋、跨性别和酷儿）的成员，因为他们本身存在社会不利和健康差异。虽然对于COVID-19疫情对大众生活各个方面的影响进行了广泛研究，但很少有研究关注LGBTQ族群。本文利用疫情前和疫情期间的数据集开发和评估了两组机器学习分类器，以识别在Twitter上表现出少数群体压力的帖子。少数群体压力是LGBTQ族群成员由于其性别和性别认同而面临的独特压力。我们证明了我们最佳的疫情前和疫情期间模型表现出强大和稳定的性能，可以检测包含少数群体压力的帖子。我们研究了疫情前和疫情期间少数群体压力帖子的语言差异。我们发现愤怒是在疫情期间最显著的情绪表达。

    The COVID-19 pandemic has disproportionately impacted the lives of minorities, such as members of the LGBTQ community (lesbian, gay, bisexual, transgender, and queer) due to pre-existing social disadvantages and health disparities. Although extensive research has been carried out on the impact of the COVID-19 pandemic on different aspects of the general population's lives, few studies are focused on the LGBTQ population. In this paper, we develop and evaluate two sets of machine learning classifiers using a pre-pandemic and a during-pandemic dataset to identify Twitter posts exhibiting minority stress, which is a unique pressure faced by the members of the LGBTQ population due to their sexual and gender identities. We demonstrate that our best pre- and during-pandemic models show strong and stable performance for detecting posts that contain minority stress. We investigate the linguistic differences in minority stress posts across pre- and during-pandemic periods. We find that anger wo
    
[^129]: 基于Transformer的自然语言处理方法用于法院文件的相似性分析

    Analysing similarities between legal court documents using natural language processing approaches based on Transformers. (arXiv:2204.07182v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.07182](http://arxiv.org/abs/2204.07182)

    本文尝试通过利用六种基于Transformer的自然语言处理技术，基于巴西葡萄牙语的通用语料库预训练，利用210,000份法律诉讼文档进行微调和专业化训练，解决法律文件相似度问题，从而协助快速解决司法程序。

    

    最近人工智能领域的发展在自然语言处理方面取得了重要进展，成为法律领域中协助快速解决司法程序的重要工具。本文以巴西司法系统的案例为研究对象，运用六种基于Transformer结构的自然语言处理技术，解决法律文件相似度问题。包括BERT、GPT-2、RoBERTa等NLP基于Transformer的模型，用巴西葡萄牙语的通用语料库进行预训练，并利用210,000份法律诉讼文档进行微调和专业化训练。通过计算每个文档的嵌入向量表征，运用聚类方法对诉讼案件进行分析，并计算每个模型的品质。

    Recent advances in Artificial Intelligence (AI) have leveraged promising results in solving complex problems in the area of Natural Language Processing (NLP), being an important tool to help in the expeditious resolution of judicial proceedings in the legal area. In this context, this work targets the problem of detecting the degree of similarity between judicial documents that can be achieved in the inference group, by applying six NLP techniques based on the transformers architecture to a case study of legal proceedings in the Brazilian judicial system. The NLP transformer-based models, namely BERT, GPT-2 and RoBERTa, were pre-trained using a general purpose corpora of the Brazilian Portuguese language, and then were fine-tuned and specialised for the legal sector using 210,000 legal proceedings. Vector representations of each legal document were calculated based on their embeddings, which were used to cluster the lawsuits, calculating the quality of each model based on the cosine of
    
[^130]: 基于随机过程的语言建模

    Language modeling via stochastic processes. (arXiv:2203.11370v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.11370](http://arxiv.org/abs/2203.11370)

    本篇论文探究了对比表示在生成任务中的应用，提出了Time Control(TC)方法，可以保留长文本的结构，并在学习句子表示以获得话语连贯性方面表现竞争力。

    

    现代语言模型能够生成高质量的短文本，但是当它们生成较长文本时，往往显得冗杂或者不连贯。这些问题源自next-token-only的语言建模目标。最近的自监督学习工作表明，模型可以通过对比学习学习到好的潜在表征，这对于区分性任务是有效的。我们的工作分析了对比表示应用于生成任务（如长文本生成）的情况。我们提出了一种利用对比表示的方法，称为Time Control (TC)。TC首先学习目标文本领域的对比表示，然后通过这些表示解码生成文本。与特定于领域的方法和跨越各种文本领域的fine-tuning GPT2相比，TC在学习句子表示以获得话语连贯性方面表现竞争力。在长文本生成设置中，TC保留了文本结构。

    Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. Recent work in self-supervised learning suggests that models can learn good latent representations via contrastive learning, which can be effective for discriminative tasks. Our work analyzes the application of contrastive representations for generative tasks, like long text generation. We propose one approach for leveraging constrastive representations, which we call Time Control (TC). TC first learns a contrastive representation of the target text domain, then generates text by decoding from these representations. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC performs competitively to methods specific for learning sentence representations on discourse coherence. On long text generation settings, TC preserves the text structure bo
    
[^131]: 量子密度矩阵在经典问答和图像分类中的应用

    Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.11155](http://arxiv.org/abs/2203.11155)

    该论文将量子密度矩阵应用于经典问答和图像分类中，证明了其可以提高任务的效率，尤其在图像分类中取得了优秀的性能表现。

    

    量子密度矩阵可表示整个量子系统的全部信息，将密度矩阵用于经典问答任务可以更加有效地实现问题回答。本论文设计了一种基于LSTM的新机制，以应对输入为矩阵的情况，并将该机制应用于卷积神经网络进行QA问题的求解，同时也证明了量子密度矩阵可以增强经典图像分类中的特征信息和特征之间的关系。实验结果表明，该新框架在CIFAR-10数据集上的性能优于传统的基于CNN的分类方法。

    Quantum density matrix represents all the information of the entire quantum system, and novel models of meaning employing density matrices naturally model linguistic phenomena such as hyponymy and linguistic ambiguity, among others in quantum question answering tasks. Naturally, we argue that applying the quantum density matrix into classical Question Answering (QA) tasks can show more effective performance. Specifically, we (i) design a new mechanism based on Long Short-Term Memory (LSTM) to accommodate the case when the inputs are matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural Network (CNN) and gain the LSTM-based QA model with the quantum density matrix. Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging results. Similarly, we argue that the quantum density matrix can also enhance the image feature information and the relationship between the features for the classical image classification. Thus, we (i) combine density mat
    
[^132]: 混合交通中可扩展监管自主性的合作

    Cooperation for Scalable Supervision of Autonomy in Mixed Traffic. (arXiv:2112.07569v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.07569](http://arxiv.org/abs/2112.07569)

    这项工作研究了在自动驾驶车辆与人类驾驶员混合交通中，如何通过自主代理的合作，实现可扩展且安全的监管，分析结果表明，AV合作可以将监管可靠性提高数个数量级，并且需要更少的监管员。

    

    自主性的进步为许多领域带来了巨大的积极成果，但实现其安全部署仍然是一个未解决的问题。本文以如下动机为出发点：在安全关键的情况下，我们能否避免需要一个人始终监督一个机器？通过考虑远程人类监管员的情况，本文规范化了这个可扩展监管问题，并研究了自主代理如何合作以实现安全。本文着重探讨了自动驾驶车辆（AVs）在由AVs和人类驾驶员混合组成的交通中合并的安全关键上下文。分析结果确定了人类监管要求的高可靠性上限。此外，它进一步表明AV合作可以将监管可靠性提高数个数量级，并且令人意想不到的需要更少的监管员（每个AV）随着越来越多的AV采用。这些分析结果利用了排队理论分析、顺序统计学以及保守的反应控制策略——时间到碰撞避难。

    Advances in autonomy offer the potential for dramatic positive outcomes in a number of domains, yet enabling their safe deployment remains an open problem. This work's motivating question is: In safety-critical settings, can we avoid the need to have one human supervise one machine at all times? The work formalizes this scalable supervision problem by considering remotely located human supervisors and investigating how autonomous agents can cooperate to achieve safety. This article focuses on the safety-critical context of autonomous vehicles (AVs) merging into traffic consisting of a mixture of AVs and human drivers. The analysis establishes high reliability upper bounds on human supervision requirements. It further shows that AV cooperation can improve supervision reliability by orders of magnitude and counterintuitively requires fewer supervisors (per AV) as more AVs are adopted. These analytical results leverage queuing-theoretic analysis, order statistics, and a conservative, reac
    
[^133]: 高斯多臂老虎机中UCB策略的极限描述的随机微分方程

    Stochastic differential equations for limiting description of UCB rule for Gaussian multi-armed bandits. (arXiv:2112.06423v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.06423](http://arxiv.org/abs/2112.06423)

    本文介绍了对于高斯多臂老虎机中已知控制时域大小$N$的上限置信度策略的极限描述，并使用随机微分方程进行了验证。

    

    本文考虑了已知控制时域大小$N$的高斯多臂老虎机中的上限置信度策略，并使用随机微分方程和常微分方程系统建立了其极限描述。假设臂的奖励具有未知的期望值和已知的方差。进行了一组蒙特卡罗模拟，针对奖励分布接近的情况（即均值奖励相差$N^{-1/2}$数量级，因为这会产生最大的标准化遗憾），以验证所得到的描述的有效性。估计了当标准化遗憾不明显大于最大值时的控制时域的最小大小。

    We consider the upper confidence bound strategy for Gaussian multi-armed bandits with known control horizon sizes $N$ and build its limiting description with a system of stochastic differential equations and ordinary differential equations. Rewards for the arms are assumed to have unknown expected values and known variances. A set of Monte-Carlo simulations was performed for the case of close distributions of rewards, when mean rewards differ by the magnitude of order $N^{-1/2}$, as it yields the highest normalized regret, to verify the validity of the obtained description. The minimal size of the control horizon when the normalized regret is not noticeably larger than maximum possible was estimated.
    
[^134]: 动力系统的 Koopman 算子谱特性的严谨数据驱动计算

    Rigorous data-driven computation of spectral properties of Koopman operators for dynamical systems. (arXiv:2111.14889v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2111.14889](http://arxiv.org/abs/2111.14889)

    本文提出了一种数据驱动算法，可以严格计算从轨迹数据中获得的Koopman算子的谱信息。该算法达到了高阶收敛，适用于混沌系统。

    

    Koopman 算子是无限维算子，全局线性化非线性动力系统，因此它们的谱信息对于理解动力学非常有价值。然而，Koopman 算子可能具有连续的谱和无限维不变子空间，使得计算它们的谱信息成为一个巨大的挑战。本文介绍了一种数据驱动算法，它具有严格的收敛性保证，可以计算从轨迹数据中获得的 Koopman 算子的谱信息。我们介绍了剩余动态模态分解（ResDMD），这是第一个从快照数据中计算普通 Koopman 算子的谱和伪谱的方案，而不会产生谱污染。利用共轭算子和 ResDMD，我们计算与普通的保测度动力系统相关的谱测量的平滑近似值。我们证明了算法的显式收敛定理，即使在混沌系统中，我们的算法也可以达到高阶收敛。

    Koopman operators are infinite-dimensional operators that globally linearize nonlinear dynamical systems, making their spectral information valuable for understanding dynamics. However, Koopman operators can have continuous spectra and infinite-dimensional invariant subspaces, making computing their spectral information a considerable challenge. This paper describes data-driven algorithms with rigorous convergence guarantees for computing spectral information of Koopman operators from trajectory data. We introduce residual dynamic mode decomposition (ResDMD), which provides the first scheme for computing the spectra and pseudospectra of general Koopman operators from snapshot data without spectral pollution. Using the resolvent operator and ResDMD, we compute smoothed approximations of spectral measures associated with general measure-preserving dynamical systems. We prove explicit convergence theorems for our algorithms, which can achieve high-order convergence even for chaotic system
    
[^135]: 学术同行评审中的拍卖和同伴预测

    Auctions and Peer Prediction for Academic Peer Review. (arXiv:2109.00923v2 [econ.GN] UPDATED)

    [http://arxiv.org/abs/2109.00923](http://arxiv.org/abs/2109.00923)

    本论文提出了一种机制设计方法，通过拍卖和同伴预测机制，同时激励高质量的提交和评审，来改善同行评审流程。

    

    同行评议的发表被认为是认证和传播研究界认为有价值的思想的黄金标准。然而，当前系统存在两个主要缺点：（1）由于提交数量庞大，对审稿人的需求压倒性，（2）缺乏激励机制促使审稿人参与并付出必要的努力提供高质量的审稿意见。在这项工作中，我们采用机制设计方法提出了改进同行评审流程的方案，将文献提交和审稿流程联系在一起，并同时激励高质量的提交和评审。在提交阶段，作者通过提交其论文和代表他们期望得到审稿意见的出价参加VCG拍卖，竞价争取审核名额。对于审稿阶段，我们提出了一种新颖的同伴预测机制（H-DIPP），建立在近期信息挖掘研究成果基础上，这种机制激励审稿人提出准确的审核意见。

    Peer reviewed publications are considered the gold standard in certifying and disseminating ideas that a research community considers valuable. However, we identify two major drawbacks of the current system: (1) the overwhelming demand for reviewers due to a large volume of submissions, and (2) the lack of incentives for reviewers to participate and expend the necessary effort to provide high-quality reviews. In this work, we adopt a mechanism-design approach to propose improvements to the peer review process, tying together the paper submission and review processes and simultaneously incentivizing high-quality submissions and reviews. In the submission stage, authors participate in a VCG auction for review slots by submitting their papers along with a bid that represents their expected value for having their paper reviewed. For the reviewing stage, we propose a novel peer prediction mechanism (H-DIPP) building on recent work in the information elicitation literature, which incentivize
    
[^136]: 连续均值协方差赌博机

    Continuous Mean-Covariance Bandits. (arXiv:2102.12090v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.12090](http://arxiv.org/abs/2102.12090)

    本文提出了一种新的连续均值协方差赌博机（CMCB）模型，以考虑选项相关性。算法具有最优遗憾，并在实验中表现良好。

    

    现有的风险感知多臂赌博模型通常关注于单个选项的风险度量，例如方差。因此，它们无法直接应用于具有相关选项的重要的实际在线决策问题。在本文中，我们提出了一种新颖的连续均值协方差赌博机（CMCB）模型，以显式考虑选项相关性。具体而言，在CMCB中，有一个学习者顺序选择给定选项上的权重向量，并根据决策观察随机反馈。代理的目标是在衡量选项协方差的奖励和风险之间实现最佳权衡。为了捕捉实践中的不同奖励观察场景，我们考虑了三种反馈设置，即完全信息、半赌徒和全赌徒反馈。我们提出了具有最优遗憾（在对数因子内），并提供相应的下界来验证它们的最优性的新算法。实验结果也证明了该算法的有效性。

    Existing risk-aware multi-armed bandit models typically focus on risk measures of individual options such as variance. As a result, they cannot be directly applied to important real-world online decision making problems with correlated options. In this paper, we propose a novel Continuous Mean-Covariance Bandit (CMCB) model to explicitly take into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent's objective is to achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, we consider three feedback settings, i.e., full-information, semi-bandit and full-bandit feedback. We propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the 
    
[^137]: 针对电力系统中虚假数据攻击检测的对抗性深度神经网络研究

    Towards Adversarial-Resilient Deep Neural Networks for False Data Injection Attack Detection in Power Grids. (arXiv:2102.09057v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2102.09057](http://arxiv.org/abs/2102.09057)

    本文提出了一种对抗性深度神经网络方法，用于电力系统中虚假数据注入攻击检测，并在训练和推理阶段采用随机输入填充技术，从而显著减少对抗攻击的有效性。

    

    虚假数据注入攻击（FDIA）对电力系统状态估计造成了重大安全威胁。为了检测这种攻击，近期的研究提出了机器学习（ML）技术，特别是深度神经网络（DNNs）。然而，这些方法中大多数未能考虑对抗性测量带来的风险，这可能危及各种ML应用中DNNs的可靠性。在本文中，我们提出了一种对抗性网络，用于FDIA检测。我们首先分析了计算机视觉中使用的几种对抗防御机制，并展示了它们在FDIA检测中的固有限制。接着，我们提出了一种对抗攻击弹性的DNN检测框架来检测FDIA，在训练和推理阶段都采用随机输入填充技术。我们基于IEEE标准电力系统进行的仿真实验表明，这种框架显著降低了对抗攻击的有效性，同时对DNN在FDIA检测中的检测性能的影响非常小。

    False data injection attacks (FDIAs) pose a significant security threat to power system state estimation. To detect such attacks, recent studies have proposed machine learning (ML) techniques, particularly deep neural networks (DNNs). However, most of these methods fail to account for the risk posed by adversarial measurements, which can compromise the reliability of DNNs in various ML applications. In this paper, we present a DNN-based FDIA detection approach that is resilient to adversarial attacks. We first analyze several adversarial defense mechanisms used in computer vision and show their inherent limitations in FDIA detection. We then propose an adversarial-resilient DNN detection framework for FDIA that incorporates random input padding in both the training and inference phases. Our simulations, based on an IEEE standard power system, demonstrate that this framework significantly reduces the effectiveness of adversarial attacks while having a negligible impact on the DNNs' dete
    
[^138]: 学习在多项Logit选择下进行排序

    Learning to Rank under Multinomial Logit Choice. (arXiv:2009.03207v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.03207](http://arxiv.org/abs/2009.03207)

    该论文提出了一个基于多项Logit选择模型的学习排序框架，能够更准确地捕捉用户在整个项目列表中的选择行为，为网站设计提供了更好的排序方案。

    

    在网站设计中，学习最佳内容排序是一个重要的挑战。学习排序（LTR）框架将这个问题建模为选择内容列表并观察用户决定点击的顺序问题。大多数以前的LTR工作假设用户在列表中独立考虑每个项目，并对每个项目进行二选一的选择。我们引入了多项式Logit（MNL）选择模型到LTR框架中，它捕捉到用户将有序的项目列表作为一个整体，从所有项目和没有点击选项中做出一个选择的行为。在MNL模型下，用户更喜欢本质上更有吸引力的项目，或者处于列表中更可取的位置的项目。我们提出了上置信界（UCB）算法，以在已知和未知的位置依赖参数的两种设置中最小化遗憾。我们提出了理论分析，导致了对问题的$\Omega（\sqrt{JT}）$下限。

    Learning the optimal ordering of content is an important challenge in website design. The learning to rank (LTR) framework models this problem as a sequential problem of selecting lists of content and observing where users decide to click. Most previous work on LTR assumes that the user considers each item in the list in isolation, and makes binary choices to click or not on each. We introduce a multinomial logit (MNL) choice model to the LTR framework, which captures the behaviour of users who consider the ordered list of items as a whole and make a single choice among all the items and a no-click option. Under the MNL model, the user favours items which are either inherently more attractive, or placed in a preferable position within the list. We propose upper confidence bound (UCB) algorithms to minimise regret in two settings where the position dependent parameters are known, and unknown. We present theoretical analysis leading to an $\Omega(\sqrt{JT})$ lower bound for the problem
    

