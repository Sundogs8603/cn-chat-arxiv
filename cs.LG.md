# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Is Channel Independent strategy optimal for Time Series Forecasting?.](http://arxiv.org/abs/2310.17658) | 本文重新考虑了当前通道独立策略在时间序列预测中是否是最佳解决方案，并提出了一种称为CSC的通道自聚类策略来增强性能并减小参数大小。 |
| [^2] | [A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP.](http://arxiv.org/abs/2310.16485) | 本文介绍了一个新的深度学习监督方法，用于检测多元时间序列数据中的事件。与现有方法相比，该方法在回归、标记数据集需求和鲁棒性方面具有创新。 |
| [^3] | [Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels.](http://arxiv.org/abs/2310.16407) | 这项工作提出了一种基于信息论的拓扑感知联邦边缘学习的泛化分析方法，并提出了一种名为FedGMIR的正则化方法来增强模型性能。 |
| [^4] | [HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2310.15318) | HetGPT是一种预训练异构图神经网络的方法，通过利用提示调整来解决预训练与下游任务之间的不匹配问题。 |
| [^5] | [ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models.](http://arxiv.org/abs/2310.10505) | ReMax是一种用于对齐大型语言模型的简单、有效和高效的强化学习方法，相比于PPO，ReMax简化了实现，减少了内存使用，并解决了fine-tuning时的内存溢出问题。 |
| [^6] | [A predict-and-optimize approach to profit-driven churn prevention.](http://arxiv.org/abs/2310.07047) | 本文提出了一种利润驱动流失防止的预测和优化方法。该方法利用个体顾客生命周期价值（CLV）来定位最有价值的顾客，并采用预测和优化框架进行解决。研究结果表明，该方法在12个流失预测数据集上取得了最佳的平均利润绩效。 |
| [^7] | [Recurrent Neural Language Models as Probabilistic Finite-state Automata.](http://arxiv.org/abs/2310.05161) | 本文研究了循环神经网络语言模型（RNN LMs）作为概率有限状态自动机的能力，并发现它们只能表示有限状态模型所能表达的概率分布的一个严格子集。 |
| [^8] | [Latent Graph Inference with Limited Supervision.](http://arxiv.org/abs/2310.04314) | 本文研究了有限监督下的潜在图推理问题并提出了一种恢复关键连接和补充缺失监督的方法。 |
| [^9] | [Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models.](http://arxiv.org/abs/2310.03059) | Point-PEFT是一种用于3D预训练模型的参数高效微调框架，它通过冻结大部分参数，只微调新增的PEFT模块，包括Point-prior Prompt和Geometry-aware Adapter，以最小化学习参数，并利用内存库和准确的聚合方法来提高模型性能。 |
| [^10] | [PharmacoNet: Accelerating Large-Scale Virtual Screening by Deep Pharmacophore Modeling.](http://arxiv.org/abs/2310.00681) | PharmacoNet是一个利用深度学习框架的虚拟筛选方法，通过识别最佳的三维药物谱排列来加速大规模虚拟筛选过程，相比于现有方法更快且准确性合理。 |
| [^11] | [Maximum Diffusion Reinforcement Learning.](http://arxiv.org/abs/2309.15293) | 最大扩散强化学习是一种克服强化学习中数据相关性问题的方法，通过解耦代理的经验实现持续学习，并在各种测试中表现出色。 |
| [^12] | [A Theory of Multimodal Learning.](http://arxiv.org/abs/2309.12458) | 这篇论文提供了一个理论框架来解释多模态学习中的一个有趣发现，即在单模态任务上，训练在多个模态上的模型可以胜过经过精细调节的单模态模型。 |
| [^13] | [Using Property Elicitation to Understand the Impacts of Fairness Constraints.](http://arxiv.org/abs/2309.11343) | 这项研究使用属性引导方法来探索损失函数和正则化函数与最优决策之间的关系，特别是在公平机器学习中的应用。它提供了损失函数和正则化函数成对时属性改变的必要和充分条件，并通过实验证明了算法决策与数据分布变化和约束难度的相关性。 |
| [^14] | [Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers.](http://arxiv.org/abs/2309.10639) | 本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。 |
| [^15] | [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning.](http://arxiv.org/abs/2309.05173) | DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。 |
| [^16] | [Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape.](http://arxiv.org/abs/2309.04788) | 本研究利用动力学均场理论研究了随机梯度下降（SGD）在高维非凸成本函数优化中的表现。实验结果表明，SGD在恢复高维非线性加密信号问题上明显优于梯度下降（GD）。 |
| [^17] | [When Do Program-of-Thoughts Work for Reasoning?.](http://arxiv.org/abs/2308.15452) | 提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。 |
| [^18] | [Price-Discrimination Game for Distributed Resource Management in Federated Learning.](http://arxiv.org/abs/2308.13838) | 本论文提出了一种价格差异化游戏（PDG），通过对不同客户提供的服务进行定价差异化，改善了联合学习的性能并降低了激励客户参与联合学习的成本。 |
| [^19] | [Channel Estimation in RIS-Enabled mmWave Wireless Systems: A Variational Inference Approach.](http://arxiv.org/abs/2308.13616) | 本论文提出了一种变分推断方法来估计RIS-启用的毫米波无线系统中的信道状态信息，通过联合估计用户设备到RIS和RIS到基站的通道，减少了信号复杂性和瞬时信道的高信令开销。 |
| [^20] | [Will More Expressive Graph Neural Networks do Better on Generative Tasks?.](http://arxiv.org/abs/2308.11978) | 本论文调查了更具表现力的图神经网络在分子图生成任务中的表现能力，并通过替换图生成模型的基础GNN来进行实验。研究发现，使用更具表现力的GNN可以改善生成任务的性能。 |
| [^21] | [Audio Generation with Multiple Conditional Diffusion Model.](http://arxiv.org/abs/2308.11940) | 本论文提出了一种使用多条件扩散模型进行音频生成的方法。通过引入内容和风格等额外条件，增强了现有模型的可控性。这种方法可以精确控制生成音频的时间顺序、音高和能量。由于缺乏合适的数据集和评估指标，作者整合了现有数据集并进行了实验验证。 |
| [^22] | [Knowledge Graph Prompting for Multi-Document Question Answering.](http://arxiv.org/abs/2308.11730) | 这篇论文提出了一种知识图谱引导的方法，用于在多文档问答任务中为大型语言模型（LLMs）提示正确的上下文。通过构建多个文档上的知识图谱，并设计基于语言模型的图遍历器，该方法能够帮助LLMs在MD-QA中进行答案预测。 |
| [^23] | [SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models.](http://arxiv.org/abs/2308.10997) | 本文提出了一种使用马尔可夫随机场（MRF）模型的轻量级方法，用于实现图像不同区域的相容性，以降低生成文本到图像模型的计算成本。 |
| [^24] | [BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions.](http://arxiv.org/abs/2308.09936) | BLIVA是一个简单的多模态LLM模型，用于更好地处理嵌入了文本的图像上下文场景。它通过引入InstructBLIP的查询嵌入和LLaVA的编码补丁嵌入技术来改进视觉语言模型，从而有效解决了文本丰富的视觉问题。 |
| [^25] | [On the Unexpected Abilities of Large Language Models.](http://arxiv.org/abs/2308.09720) | 大型语言模型展示了与其训练任务不直接相关的广泛能力，通过间接获取过程使其拥有综合能力的发展，这些能力在一定程度上可预测，并且与人类认知有关。 |
| [^26] | [From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space.](http://arxiv.org/abs/2308.09437) | 该论文提出了一种通过梯度减小模型对偏见的敏感性的方法，从而在概念级别上确保正确原因，有效减轻深度神经网络中的偏见。该方法在多个数据集和环境中被验证有效。 |
| [^27] | [Neural oscillators for generalization of physics-informed machine learning.](http://arxiv.org/abs/2308.08989) | 本文提出了一种称为神经振荡器的方法，通过与物理信息机器学习模型的融合，利用偏微分方程解的因果关系和时间特性来提高其泛化能力。实验证明了这种方法在处理复杂物理问题时的有效性。 |
| [^28] | [Can Transformers Learn Optimal Filtering for Unknown Systems?.](http://arxiv.org/abs/2308.08536) | 本文研究了使用transformers进行最优输出估计问题，通过训练一个transformer来在未知系统上进行预测，并命名为元输出预测器（MOP）。我们观察到，尽管MOP没有访问模型的权限，但在大多数线性动态系统中，它的性能与基于卡尔曼滤波器的最优输出估计器相当，在具有非独立同分布噪声和时变动态的挑战性场景中也表现优秀。 |
| [^29] | [Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability.](http://arxiv.org/abs/2308.07728) | 本文提出了领域感知微调（DAFT）方法，通过批归一化转换和线性探测与微调的集成，有效减轻微调过程中的特征畸变问题。 |
| [^30] | [Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items.](http://arxiv.org/abs/2307.13709) | 本论文提出了深度布拉德利-特里评分（DBTR）方法，用于评估不一定存在于数据集中的未知物品的属性。该方法通过将传统的布拉德利-特里模型与神经网络结构无缝结合，成功地学习了这些属性的预期量化。 |
| [^31] | [NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services.](http://arxiv.org/abs/2307.06148) | NetGPT是一个能够在边缘和云端部署适当的大型语言模型的本地AI网络架构，实现了个性化生成服务，并通过协作云边方法论来优化资源协调和互动效果。 |
| [^32] | [PIGEON: Predicting Image Geolocations.](http://arxiv.org/abs/2307.05845) | PIGEON是一个用于全球规模图像地理定位的多任务端到端系统，通过语义地理单元的创建和精化，以及无监督聚类和ProtoNets的应用，实现了最先进的性能，并提供了预训练的CLIP转换器模型StreetCLIP。 |
| [^33] | [STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting.](http://arxiv.org/abs/2307.02507) | 本研究通过引入先进的对比学习方法，提出了一种新颖的时空同步上下文对比学习（STS-CCL）模型，用于高效地捕捉大规模无标签交通数据的复杂时空表示。该模型通过使用动态图视图生成器和语义上下文对比方法，实现了节点级和图级的对比学习。 |
| [^34] | [All in One: Multi-task Prompting for Graph Neural Networks.](http://arxiv.org/abs/2307.01504) | 本文提出了一种新颖的图模型的多任务提示方法，通过统一图提示和语言提示的格式，填补了预训练模型与各种图任务之间的差距。 |
| [^35] | [AVSegFormer: Audio-Visual Segmentation with Transformer.](http://arxiv.org/abs/2307.01146) | AVSegFormer是一种基于Transformer的音视频分割框架，通过引入音频查询和可学习查询来选择性地关注视觉特征，还使用音频-视觉混合器动态调整视觉特征，并通过中间掩模损失增强解码器的监督。实验证明该方法的有效性。 |
| [^36] | [Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring.](http://arxiv.org/abs/2306.13803) | 本文探讨了AI和ML在保护非洲保护区中至关重要的大象方面的作用。新的AI和ML技术提供了解决方案以处理并提取重要信息。在AI专家和生态研究人员之间的协作下，这些创新技术可以帮助增强野生动物保护，为其他物种设定先例。 |
| [^37] | [MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates.](http://arxiv.org/abs/2306.12212) | 本文提出的 MimiC 算法解决了联邦学习中客户端退出问题，通过模仿缺失的客户端更新解决了聚合更新和期望中心更新之间的分歧，实现了更高的测试准确率和更低的通信成本。 |
| [^38] | [LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning.](http://arxiv.org/abs/2306.09910) | 本论文介绍了一个新的综合性标签高效学习基准评估框架LabelBench，并通过引入一种新的与半监督学习相结合的主动学习方法的基准测试，证明了在相对较少的标记示例下实现更好的标签效率。 |
| [^39] | [A Survey on Blood Pressure Measurement Technologies: Addressing Potential Sources of Bias.](http://arxiv.org/abs/2306.08451) | 该综述聚焦于带式血压监测技术，强调了由于测量和设备误差、人口统计学数据和体型差异等因素导致的血压测量偏差和方差。研发使用人工智能来纠正误差的新一代带式血压设备是重点发展方向。 |
| [^40] | [On the Expected Size of Conformal Prediction Sets.](http://arxiv.org/abs/2306.07254) | 该论文研究了适应性预测集的期望大小问题，提出了一种理论量化方法以及点估计和高概率区间，并在真实数据集上验证了其实用性。 |
| [^41] | [Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean.](http://arxiv.org/abs/2306.05144) | Mesogeos是一个地中海地区的大规模多用途数据集，用于数据驱动的野火建模。它集成了历史野火记录和野火驱动因素，具有机器学习的潜力，提供了短期野火危险预测和最终烧毁区域估计两个可用于演示潜力的数据集。 |
| [^42] | [Multi-modal Latent Diffusion.](http://arxiv.org/abs/2306.04445) | 该论文提出一个新的方法来处理多模态数据，该方法使用了一组独立训练的单模态确定性自编码器，将单个潜在变量连接到公共潜在空间中，并通过掩蔽扩散模型实现了良好的生成质量和模态间连贯性。 |
| [^43] | [Learning Linear Causal Representations from Interventions under General Nonlinear Mixing.](http://arxiv.org/abs/2306.02235) | 本文针对先前工作弱一类问题进行推广，提出了一种用于在非线性混合下的干预中学习线性因果表示的强可识别性算法，证明了其有效性，并提出了在实践中识别潜在变量的对比算法。 |
| [^44] | [Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts.](http://arxiv.org/abs/2305.19951) | 本研究通过将NeSy模型中出现的推理快捷方式定义为学习目标的意外最优解，并确定其发生的四个关键条件，提出了几种可行的缓解策略并对其进行了分析，显示推理快捷方式难以处理。 |
| [^45] | [Is Learning in Games Good for the Learners?.](http://arxiv.org/abs/2305.19496) | 我们提出了“广义均衡”的概念，通过学习可以在某些游戏中获得更好的结果。如果没有纯纳什均衡，则一名玩家可以从不同策略中受益，结果捕获了Stackelberg均衡的扩展。 |
| [^46] | [Learning Two-Layer Neural Networks, One (Giant) Step at a Time.](http://arxiv.org/abs/2305.18270) | 本文研究了浅层神经网络的训练动态及其条件，证明了动态下梯度下降可以通过有限数量的大批量梯度下降步骤来促进特征学习，并找到了多个和单一方向的最佳批量大小，有助于促进特征学习和方向的专业化。 |
| [^47] | [GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations.](http://arxiv.org/abs/2305.17021) | GLOBE-CE是一种用于全球因果解释的机器学习方法，它可以超越局部解释，提供更有效和交互式的解释工具。 |
| [^48] | [Union Subgraph Neural Networks.](http://arxiv.org/abs/2305.15747) | 本文提出了一种新型图神经网络UnionSNN，注入了邻居连接信息，通过联合子图来编码高阶连接性，实验证明其在节点分类任务中优于1-WL和当前最先进的GNN模型。 |
| [^49] | [Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators.](http://arxiv.org/abs/2305.14561) | 本文介绍了一种新的训练方法，使用负反馈机制来增强DNN模型的鲁棒性，特别是在存在设备变异的情况下。 |
| [^50] | [ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding.](http://arxiv.org/abs/2305.14196) | ZeroSCROLLS是一个用于长文本自然语言理解的零Shot基准测试，包括六个任务和四个数据集，能够评估大型语言模型的性能。当前，GPT-4的平均得分最高，但在聚合任务等多个挑战上，仍有改进的空间。 |
| [^51] | [Optimality of Message-Passing Architectures for Sparse Graphs.](http://arxiv.org/abs/2305.10391) | 本研究证明了将消息传递神经网络应用于稀疏图的节点分类任务是渐近本地贝叶斯最优的，提出了一种实现最优分类器的算法，并将最优分类器的性能理论上与现有学习方法进行了比较。 |
| [^52] | [Low-complexity subspace-descent over symmetric positive definite manifold.](http://arxiv.org/abs/2305.02041) | 本文提出了一种基于黎曼子空间下降算法的对称正定流形上的函数最小化方法，其具有低复杂度和避免昂贵矩阵操作和计算黎曼梯度的优点。 |
| [^53] | [Mixing predictions for online metric algorithms.](http://arxiv.org/abs/2304.01781) | 本文提出了一种在线算法的混合预测方法，针对度量任务系统，我们获得了$O(\ell^2)$的竞争比，可以使算法跟随不同的预测器，对限制切换次数的情况可以获得$(1+\epsilon)$-竞争算法。 |
| [^54] | [DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving.](http://arxiv.org/abs/2304.01168) | 本文提出了一个大规模的 DeepAccident 数据集，其中包含各种真实世界驾驶中发生的事故场景，并提出了一个端到端的运动和事故预测任务，该任务可用于直接评估自动驾驶算法的事故预测能力。 |
| [^55] | [FairGen: Towards Fair Graph Generation.](http://arxiv.org/abs/2303.17743) | 本文提出了一种公平感知图生成模型FairGen，通过标签损失和公平损失来对模型进行训练，并使用雅可比优化方法来实现公平性和逼真性。 |
| [^56] | [Continuous-Time Functional Diffusion Processes.](http://arxiv.org/abs/2303.00800) | 连续时间功能扩散过程引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。通过使用新的数学框架和扩展，FDPs可以在函数空间中构建新型生成模型，在处理连续数据时能够实现高质量的图像生成，所需参数数量比现有模型低几个数量级。 |
| [^57] | [CrystalBox: Future-Based Explanations for DRL Network Controllers.](http://arxiv.org/abs/2302.13483) | CrystalBox是一种解释DRL网络控制器行为的框架，它能够使用未来关键网络性能指标的影响来生成简明而富有表现力的解释。 |
| [^58] | [Simplifying Momentum-based Riemannian Submanifold Optimization.](http://arxiv.org/abs/2302.09738) | 本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。 |
| [^59] | [Algorithm Selection for Deep Active Learning with Imbalanced Datasets.](http://arxiv.org/abs/2302.07317) | 本论文提出了适用于不平衡数据集下的深度主动学习的自适应算法选择策略TAILOR，它在多类和多标签应用的实验中取得了与候选算法相当或更高的准确性。 |
| [^60] | [Direct Uncertainty Quantification.](http://arxiv.org/abs/2302.02420) | 本文提出一种新的直接不确定量化（DirectUQ）方法，它能在神经网络中直接输出均值和方差，同时结合了传统神经网络和贝叶斯神经网络的优点，有助于改进模型的正则化器和风险边界等方面。 |
| [^61] | [Editing Language Model-based Knowledge Graph Embeddings.](http://arxiv.org/abs/2301.10405) | 本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。 |
| [^62] | [ClimaX: A foundation model for weather and climate.](http://arxiv.org/abs/2301.10343) | ClimaX是一种灵活且可推广的深度学习模型，用于天气和气候科学，可以使用不同数据集进行训练。 |
| [^63] | [Fake detection in imbalance dataset by Semi-supervised learning with GAN.](http://arxiv.org/abs/2212.01071) | 本文提出了一种在不平衡数据集中使用半监督学习和生成对抗网络进行虚假检测的方法，实验证明仅使用100个标记样本的情况下，准确率达到了91\%。 |
| [^64] | [Meta-Referential Games to Learn Compositional Learning Behaviours.](http://arxiv.org/abs/2207.08012) | 本论文提出了一种元元反游戏学习的方法来解决组合学习行为的问题，通过解决绑定问题来支持人工智能代理展示组合学习行为的能力。 |
| [^65] | [Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models.](http://arxiv.org/abs/2207.06950) | 本文提出了一种新算法GAMI-Tree，使用基于模型的树以及新的交互过滤方法，可以更好地拟合底层交互，具有更好的预测性能和更高的效率。 |
| [^66] | [Removing the fat from your posterior samples with margarine.](http://arxiv.org/abs/2205.12841) | 本文总结了一种使用掩蔽自回归流和核密度估计器的方法，可以学习对应于核心科学参数的边际后验密度。该方法在计算边际库尔巴克-勒布勒散度、边际贝叶斯模型维度、似然函数模拟和先验模拟等方面具有广泛应用。 |
| [^67] | [Impartial Games: A Challenge for Reinforcement Learning.](http://arxiv.org/abs/2205.12787) | AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations. |
| [^68] | [Deep Feature Screening: Feature Selection for Ultra High-Dimensional Data via Deep Neural Networks.](http://arxiv.org/abs/2204.01682) | 本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服高维、低样本数据上的困难和挑战，并对超高维、低样本数据进行高精度的特征筛选。 |
| [^69] | [FedGCN: Convergence and Communication Tradeoffs in Federated Training of Graph Convolutional Networks.](http://arxiv.org/abs/2201.12433) | 介绍了一个新算法 FedGCN，使用联邦学习训练 GCN 模型进行半监督节点分类，实现收敛快，通信量小，同时还能够保护本地数据隐私。 |
| [^70] | [Persistent Homological State-Space Estimation of Functional Human Brain Networks at Rest.](http://arxiv.org/abs/2201.00087) | 该论文提出了一种新的数据驱动的拓扑数据分析方法，用于估计静息状态下人类脑网络的状态空间。该方法通过惩罚网络之间的拓扑距离将动态变化的脑网络聚类为不同的状态，并通过考虑时间维度来提高准确性。研究还发现脑网络的整体拓扑具有遗传特征。 |
| [^71] | [High-order Tensor Pooling with Attention for Action Recognition.](http://arxiv.org/abs/2110.05216) | 本论文提出了一种利用注意机制进行高阶张量池化的方法，通过引入特征值幂归一化（EPN）来防止爆发现象并提高动作识别的准确性。 |

# 详细

[^1]: 通道独立策略是否是时间序列预测的最佳解？

    Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v1 [cs.LG])

    [http://arxiv.org/abs/2310.17658](http://arxiv.org/abs/2310.17658)

    本文重新考虑了当前通道独立策略在时间序列预测中是否是最佳解决方案，并提出了一种称为CSC的通道自聚类策略来增强性能并减小参数大小。

    

    近年来出现了许多用于长期时间序列预测的模型。最近的研究表明，使用单一线性层的通道相关(CD)或通道独立(CI)建模，甚至可以超过许多复杂模型的性能。然而，当前的研究主要将CD和CI视为两种互补但互斥的方法，无法同时利用这两个极端。而且，CD和CI都是静态策略，无法在没有大量实验的情况下确定是特定数据集的最佳策略。在本文中，我们重新考虑了当前CI策略是否是时间序列预测的最佳解决方案。首先，我们提出了一种简单而有效的策略，称为CSC（通道自聚类策略），用于线性模型。我们的通道自聚类策略增强了CI策略的性能改进，并减小了参数大小。

    There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, fo
    
[^2]: 一种用于多元时间序列数据的基于深度学习的事件检测和自然语言处理中的信息检索的全面Python库

    A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP. (arXiv:2310.16485v1 [cs.LG])

    [http://arxiv.org/abs/2310.16485](http://arxiv.org/abs/2310.16485)

    本文介绍了一个新的深度学习监督方法，用于检测多元时间序列数据中的事件。与现有方法相比，该方法在回归、标记数据集需求和鲁棒性方面具有创新。

    

    时间序列数据中的事件检测在金融、医疗保健、网络安全和科学等各个领域都至关重要。准确地识别时间序列数据中的事件对于做出明智决策、检测异常和预测未来趋势至关重要。尽管针对时间序列的事件检测已经进行了广泛的研究，其中深度学习方法是最先进的方法之一，但在这个领域仍有改进和创新的空间。本文提出了一种新的深度学习监督方法，用于检测多元时间序列数据中的事件。与现有的深度学习监督方法相比，我们的方法结合了四个独特的创新。首先，它基于回归而不是二元分类。其次，它不需要标记的数据集，每个数据点都有标签；相反，它只需要定义为时间点或时间间隔的参考事件。第三，它通过使用堆叠集成模型设计得到了鲁棒性。

    Event detection in time series data is crucial in various domains, including finance, healthcare, cybersecurity, and science. Accurately identifying events in time series data is vital for making informed decisions, detecting anomalies, and predicting future trends. Despite extensive research exploring diverse methods for event detection in time series, with deep learning approaches being among the most advanced, there is still room for improvement and innovation in this field. In this paper, we present a new deep learning supervised method for detecting events in multivariate time series data. Our method combines four distinct novelties compared to existing deep-learning supervised methods. Firstly, it is based on regression instead of binary classification. Secondly, it does not require labeled datasets where each point is labeled; instead, it only requires reference events defined as time points or intervals of time. Thirdly, it is designed to be robust by using a stacked ensemble l
    
[^3]: 基于信息论的拓扑感知异构联邦边缘学习在噪声通道上的泛化分析

    Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels. (arXiv:2310.16407v1 [cs.IT])

    [http://arxiv.org/abs/2310.16407](http://arxiv.org/abs/2310.16407)

    这项工作提出了一种基于信息论的拓扑感知联邦边缘学习的泛化分析方法，并提出了一种名为FedGMIR的正则化方法来增强模型性能。

    

    随着边缘智能的快速发展，无线网络上的联邦学习部署越来越受关注，被称为联邦边缘学习（FEEL）。在FEEL中，移动设备通过噪声通道传输模型参数和在各种环境中收集数据，这给训练模型的泛化带来了挑战。此外，设备可以通过设备间通信进行分散式联邦学习，而连接设备的通信拓扑也影响模型的泛化能力。然而，大多数最近的理论研究在开展泛化分析时忽视了所有这些效应的纳入。与之相反，我们的工作提出了一种基于信息论的拓扑感知FEEL的泛化分析方法，考虑到了数据异构性和噪声通道的影响。此外，我们还提出了一种名为联邦全局互信息减少（FedGMIR）的新型正则化方法，以提高模型的性能。

    With the rapid growth of edge intelligence, the deployment of federated learning (FL) over wireless networks has garnered increasing attention, which is called Federated Edge Learning (FEEL). In FEEL, both mobile devices transmitting model parameters over noisy channels and collecting data in diverse environments pose challenges to the generalization of trained models. Moreover, devices can engage in decentralized FL via Device-to-Device communication while the communication topology of connected devices also impacts the generalization of models. Most recent theoretical studies overlook the incorporation of all these effects into FEEL when developing generalization analyses. In contrast, our work presents an information-theoretic generalization analysis for topology-aware FEEL in the presence of data heterogeneity and noisy channels. Additionally, we propose a novel regularization method called Federated Global Mutual Information Reduction (FedGMIR) to enhance the performance of models
    
[^4]: HetGPT: 利用预训练异构图神经网络中的提示调整的能力

    HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v1 [cs.LG])

    [http://arxiv.org/abs/2310.15318](http://arxiv.org/abs/2310.15318)

    HetGPT是一种预训练异构图神经网络的方法，通过利用提示调整来解决预训练与下游任务之间的不匹配问题。

    

    图表现为表示和分析Web中的复杂模式和丰富信息的自然选择，使得在线页面分类和社交推荐等应用成为可能。然而，当前的“预训练，微调”范式在图机器学习任务中广泛应用，特别是在有限标记节点的情况下，往往存在预训练目标任务与下游任务之间的不匹配问题。这种差距可能导致“负转移”问题，即预训练所获得的知识对下游任务的性能产生不利影响。自然语言处理领域中基于提示的学习的兴起表明了将“预训练，提示”范式应用于图形的潜力，作为一种替代方案。然而，现有的图形提示技术针对的是同质图，忽视了Web图的内在异构性。为了填补这一差距，我们提出了HetGPT，

    Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a 
    
[^5]: ReMax:一种用于对齐大型语言模型的简单、有效和高效的强化学习方法

    ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models. (arXiv:2310.10505v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10505](http://arxiv.org/abs/2310.10505)

    ReMax是一种用于对齐大型语言模型的简单、有效和高效的强化学习方法，相比于PPO，ReMax简化了实现，减少了内存使用，并解决了fine-tuning时的内存溢出问题。

    

    对齐对于训练大型语言模型（LLMs）非常重要。目前解决这个问题的主要策略是通过从人类反馈中进行强化学习（RLHF），其中PPO是事实上的算法。然而，众所周知，PPO在计算效率上存在问题，这是本论文试图解决的挑战。我们在RLHF任务中确定了三个重要特性：快速模拟、确定性转换和轨迹级奖励，这些特性在PPO中没有得到充分利用。基于这些观察，我们开发了一种针对RLHF的新算法，称为ReMax。ReMax的算法设计是基于一种广为使用的算法REINFORCE，但配备了一种新的方差减少技术。我们的方法相对于PPO具有三重优势：首先，ReMax实现简单，消除了PPO中的许多与规模相关且繁琐的超参数。其次，ReMax原则上可以节约约50%的内存使用。结果导致PPO在进行fine-tuning时出现内存溢出的问题。

    Alignment is of critical importance for training large language models (LLMs). The predominant strategy to address this is through Reinforcement Learning from Human Feedback (RLHF), where PPO serves as the de-facto algorithm. Yet, PPO is known to suffer from computational inefficiency, which is a challenge that this paper aims to address. We identify three important properties in RLHF tasks: fast simulation, deterministic transitions, and trajectory-level rewards, which are not leveraged in PPO. Based on such observations, we develop a new algorithm tailored for RLHF, called ReMax. The algorithm design of ReMax is built on a celebrated algorithm REINFORCE but is equipped with a new variance-reduction technique.  Our method has three-fold advantages over PPO: first, ReMax is simple to implement and removes many hyper-parameters in PPO, which are scale-sensitive and laborious to tune. Second, ReMax saves about 50% memory usage in principle. As a result, PPO runs out-of-memory when fine-t
    
[^6]: 利润驱动流失防止的预测和优化方法

    A predict-and-optimize approach to profit-driven churn prevention. (arXiv:2310.07047v1 [cs.LG])

    [http://arxiv.org/abs/2310.07047](http://arxiv.org/abs/2310.07047)

    本文提出了一种利润驱动流失防止的预测和优化方法。该方法利用个体顾客生命周期价值（CLV）来定位最有价值的顾客，并采用预测和优化框架进行解决。研究结果表明，该方法在12个流失预测数据集上取得了最佳的平均利润绩效。

    

    本文介绍一种新颖的利润驱动流失防止的预测和优化方法。我们将定位顾客用于留存活动的任务构建为一个后悔最小化问题。主要目标是利用个体顾客生命周期价值（CLV）确保只有最有价值的顾客被定位。与之相反，许多利润驱动策略关注流失概率同时考虑平均CLV，这往往导致数据聚合带来的信息损失。我们提出的模型符合预测和优化（PnO）框架的指导方针，并可以使用随机梯度下降方法高效解决。来自12个流失预测数据集的结果突出了我们方法的有效性，在平均利润方面实现了最佳的绩效。

    In this paper, we introduce a novel predict-and-optimize method for profit-driven churn prevention. We frame the task of targeting customers for a retention campaign as a regret minimization problem. The main objective is to leverage individual customer lifetime values (CLVs) to ensure that only the most valuable customers are targeted. In contrast, many profit-driven strategies focus on churn probabilities while considering average CLVs. This often results in significant information loss due to data aggregation. Our proposed model aligns with the guidelines of Predict-and-Optimize (PnO) frameworks and can be efficiently solved using stochastic gradient descent methods. Results from 12 churn prediction datasets underscore the effectiveness of our approach, which achieves the best average performance compared to other well-established strategies in terms of average profit.
    
[^7]: 循环神经语言模型作为概率有限状态自动机

    Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05161](http://arxiv.org/abs/2310.05161)

    本文研究了循环神经网络语言模型（RNN LMs）作为概率有限状态自动机的能力，并发现它们只能表示有限状态模型所能表达的概率分布的一个严格子集。

    

    通过以容易理解的形式来研究语言模型（LMs）可以使我们精确地描述它们的能力和局限性。先前的研究已经考察了循环神经网络（RNN）语言模型在识别无权重形式语言的能力。然而，LMs并不描述无权重形式语言，而是定义了对字符串的概率分布。在本研究中，我们研究了RNN LMs可以表示哪些类的概率分布，这使得我们可以更直接地陈述它们的能力。我们证明了简单的RNN等价于概率有限状态自动机的一个子类，因此只能模拟有限状态模型所能表达的概率分布的一个严格子集。此外，我们研究了用RNNs表示有限状态LMs的空间复杂度。我们证明了，为了表示一个任意确定的有限状态LMs，其中有$N$个状态且字符集为$\Sigma$的RNN requir

    Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the representational capacity of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages -- rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requir
    
[^8]: 有限监督下的潜在图推理

    Latent Graph Inference with Limited Supervision. (arXiv:2310.04314v1 [cs.LG])

    [http://arxiv.org/abs/2310.04314](http://arxiv.org/abs/2310.04314)

    本文研究了有限监督下的潜在图推理问题并提出了一种恢复关键连接和补充缺失监督的方法。

    

    潜在图推理 (LGI) 的目标是从数据特征中同时学习潜在的图结构和节点表示。然而，现有的LGI方法通常遭受督导匮乏问题，导致大量的边权重在没有语义监督的情况下学习，不能对训练损失做出贡献。因此，这些缺乏监督的权重可能决定测试样本的预测，但并不一定是语义上最优的，导致泛化能力差。本文观察到这个问题实际上是由于图稀疏化操作导致的，严重破坏了关键节点和标记节点之间建立的重要连接。为了解决这个问题，我们提出恢复受损的相似性，并为更好的LGI补充缺失的监督。关键挑战在于识别关键节点并恢复损坏的相似性。我们首先将关键节点定义为$k$-hop饥饿节点，可以通过饥饿状况计算获得。

    Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. However, existing LGI methods commonly suffer from the issue of supervision starvation, where massive edge weights are learned without semantic supervision and do not contribute to the training loss. Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization. In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. To address this, we propose to restore the corrupted affinities and replenish the missed supervision for better LGI. The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities. We begin by defining the pivotal nodes as $k$-hop starved nodes, which can be id
    
[^9]: Point-PEFT: 用于3D预训练模型的参数高效微调

    Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])

    [http://arxiv.org/abs/2310.03059](http://arxiv.org/abs/2310.03059)

    Point-PEFT是一种用于3D预训练模型的参数高效微调框架，它通过冻结大部分参数，只微调新增的PEFT模块，包括Point-prior Prompt和Geometry-aware Adapter，以最小化学习参数，并利用内存库和准确的聚合方法来提高模型性能。

    

    大型预训练模型的流行已经彻底改变了语言、视觉和多模态等领域的下游任务。为了降低下游任务的适应成本，许多参数高效微调（PEFT）技术被提出用于语言和2D图像预训练模型。然而，对于3D预训练模型的专门PEFT方法仍未得到充分探索。为此，我们引入了Point-PEFT，一种用于适应点云预训练模型的新型框架，其具有最少的可学习参数。具体而言，对于预训练的3D模型，我们冻结大部分参数，只微调新增的PEFT模块。这些模块包括Point-prior Prompt和Geometry-aware Adapter。Point-prior Prompt采用一组可学习的提示标记，并提出使用具有领域特定知识的内存库来增强提示标记的参数无关的注意力机制。Geometry-aware Adapter旨在对不同任务或数据进行准确地聚合。

    The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
    
[^10]: PharmacoNet: 利用深度药物谱建模加速大规模虚拟筛选

    PharmacoNet: Accelerating Large-Scale Virtual Screening by Deep Pharmacophore Modeling. (arXiv:2310.00681v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2310.00681](http://arxiv.org/abs/2310.00681)

    PharmacoNet是一个利用深度学习框架的虚拟筛选方法，通过识别最佳的三维药物谱排列来加速大规模虚拟筛选过程，相比于现有方法更快且准确性合理。

    

    随着可访问化合物库的规模扩大到超过100亿个，对更高效的基于结构的虚拟筛选方法的需求正在出现。尽管已经开发了不同的预筛选方法来快速筛选库，但适用于通用蛋白质的基于结构的方法仍然缺乏：挑战是在极短的时间内预测蛋白质和配体之间的结合位姿并进行评分。我们引入PharmacoNet，这是一个深度学习框架，它通过从结合位点生成的最佳三维药物谱排列来识别配体应该具有的稳定结合要求。通过粗粒化图匹配，我们在一步中解决了现有方法中昂贵的结合位姿采样和评分过程。PharmacoNet比最先进的基于结构的方法要快得多，但仍具有合理的准确性和简单的评分函数。此外，我们展示了有希望的再利用效果。

    As the size of accessible compound libraries expands to over 10 billion, the need for more efficient structure-based virtual screening methods is emerging. Different pre-screening methods have been developed to rapidly screen the library, but the structure-based methods applicable to general proteins are still lacking: the challenge is to predict the binding pose between proteins and ligands and perform scoring in an extremely short time. We introduce PharmacoNet, a deep learning framework that identifies the optimal 3D pharmacophore arrangement which a ligand should have for stable binding from the binding site. By coarse-grained graph matching between ligands and the generated pharmacophore arrangement, we solve the expensive binding pose sampling and scoring procedures of existing methods in a single step. PharmacoNet is significantly faster than state-of-the-art structure-based approaches, yet reasonably accurate with a simple scoring function. Furthermore, we show the promising re
    
[^11]: 最大扩散强化学习

    Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])

    [http://arxiv.org/abs/2309.15293](http://arxiv.org/abs/2309.15293)

    最大扩散强化学习是一种克服强化学习中数据相关性问题的方法，通过解耦代理的经验实现持续学习，并在各种测试中表现出色。

    

    所有机器学习都建立在数据独立且同分布的假设上。然而，在强化学习中，当数据是依次从代理经验中收集而来时，这一假设通常不成立。因此，我们提出了一种名为最大扩散强化学习的方法，利用统计力学中的遍历过程来克服这些限制。我们的方法通过解耦代理的经验，可证明地使代理在单次部署中能够持续学习，而不受初始化方式的影响。此外，我们证明了我们的方法推广了众所周知的最大熵技术，并且通过在流行的基准测试中稳定超过了最先进的性能水平。我们的研究成果极大地促进了物理学、学习和控制的交叉领域，为强化学习代理（如行走机器人和自动驾驶汽车）的透明可靠决策提供了一条道路。

    The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
    
[^12]: 多模态学习的理论

    A Theory of Multimodal Learning. (arXiv:2309.12458v1 [cs.LG])

    [http://arxiv.org/abs/2309.12458](http://arxiv.org/abs/2309.12458)

    这篇论文提供了一个理论框架来解释多模态学习中的一个有趣发现，即在单模态任务上，训练在多个模态上的模型可以胜过经过精细调节的单模态模型。

    

    人类对经验世界的感知涉及到识别基础物体的各种外观或“模态”。尽管哲学和认知科学领域长期以来一直考虑这一观点，但是在机器学习领域中，对多模态的研究相对较少。然而，目前关于多模态机器学习的研究仅限于经验实践，缺乏理论基础，只有启发式论证。多模态学习实践中的一个有趣发现是，在单模态任务上，训练在多个模态上的模型可以胜过经过精细调节的单模态模型。本文提供了一个理论框架来解释这一现象，通过研究多模态学习算法的泛化性质。我们证明了多模态学习相比于单模态学习具有更优的泛化界限，高达$O(\sqrt{n})$的因子，其中$n$表示样本大小。

    Human perception of the empirical world involves recognizing the diverse appearances, or 'modalities', of underlying objects. Despite the longstanding consideration of this perspective in philosophy and cognitive science, the study of multimodality remains relatively under-explored within the field of machine learning. Nevertheless, current studies of multimodal machine learning are limited to empirical practices, lacking theoretical foundations beyond heuristic arguments. An intriguing finding from the practice of multimodal learning is that a model trained on multiple modalities can outperform a finely-tuned unimodal model, even on unimodal tasks. This paper provides a theoretical framework that explains this phenomenon, by studying generalization properties of multimodal learning algorithms. We demonstrate that multimodal learning allows for a superior generalization bound compared to unimodal learning, up to a factor of $O(\sqrt{n})$, where $n$ represents the sample size. Such adva
    
[^13]: 使用属性引导方法来理解公平性约束的影响

    Using Property Elicitation to Understand the Impacts of Fairness Constraints. (arXiv:2309.11343v1 [cs.LG])

    [http://arxiv.org/abs/2309.11343](http://arxiv.org/abs/2309.11343)

    这项研究使用属性引导方法来探索损失函数和正则化函数与最优决策之间的关系，特别是在公平机器学习中的应用。它提供了损失函数和正则化函数成对时属性改变的必要和充分条件，并通过实验证明了算法决策与数据分布变化和约束难度的相关性。

    

    预测算法通常通过优化某个损失函数进行训练，并添加正则化函数来施加违反约束的惩罚。预期地，添加这样的正则化函数可以改变目标函数的最小化值。目前还不清楚哪些正则化函数会改变损失函数的最小化值，以及当最小化值发生变化时，它会如何变化。我们使用属性引导方法来初步了解损失函数和正则化函数与给定问题实例的最优决策之间的联合关系。具体而言，我们给出了损失函数和正则化函数成对时，属性改变的必要和充分条件，并研究了一些满足这个条件的正则化函数在公平机器学习文献中的标准。我们通过实验证明了算法决策如何随着数据分布的变化和约束的难度而改变。

    Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraint
    
[^14]: 深度学习网络的几何结构和全局${\mathcal L}^2$最小化器的构建

    Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])

    [http://arxiv.org/abs/2309.10639](http://arxiv.org/abs/2309.10639)

    本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。

    

    本文提供了对深度学习（DL）网络结构的几何解释，该网络具有$L$个隐藏层，斜坡激活函数，${\mathcal L}^2$ Schatten类（或Hilbert-Schmidt）成本函数，以及相等维度$Q\geq1$的输入和输出空间${\mathbb R}^Q$。隐藏层也定义在${\mathbb R}^{Q}$的空间上。我们利用我们最新的关于浅层神经网络的结果，在$L\geq Q$的情况下构造了一个明确的最小化器族，该族能够全局最小化成本函数，并且我们证明这个族是退化的。在这里提到的上下文中，DL网络的隐藏层通过对训练输入的递归截断映射的应用来“整理”训练输入，以最小化噪声与信号的比率。此外，我们确定了$2^Q-1$个不同的退化局部最小值。

    In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
    
[^15]: DePT:分解提示调整以实现参数高效微调

    DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05173](http://arxiv.org/abs/2309.05173)

    DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。

    

    提示调整（PT）是一种将可训练的少量软提示向量附加到语言模型（LM）输入中的参数高效微调（PEFT）方法，已在各种任务和模型中显示出了有希望的结果。 与其他PEFT方法相比，PT的竞争性能可以在可训练参数更少的情况下保持，并且随着模型规模的扩大，其参数并不会显著增加。 但是，PT引入了额外的软提示标记，导致输入序列变长，这对于Transformer的二次复杂度而言，在训练和推理时间以及内存使用方面会产生显著影响。 这对于面临大量每日查询的大型语言模型（LLMs）尤其令人担忧。

    Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
    
[^16]: 随机梯度下降在高维信号恢复的玻璃能量景观中表现优于梯度下降

    Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape. (arXiv:2309.04788v1 [cs.LG])

    [http://arxiv.org/abs/2309.04788](http://arxiv.org/abs/2309.04788)

    本研究利用动力学均场理论研究了随机梯度下降（SGD）在高维非凸成本函数优化中的表现。实验结果表明，SGD在恢复高维非线性加密信号问题上明显优于梯度下降（GD）。

    

    随机梯度下降（SGD）是一种非平衡算法，广泛用于训练人工神经网络。然而，我们对SGD在这项技术的成功中至关重要的程度以及相对于其他优化算法（如梯度下降）在优化高维非凸成本函数方面的效果知之甚少。在这项工作中，我们利用动力学均场理论在高维极限中准确分析了其性能。我们考虑恢复隐藏的高维非线性加密信号问题，即一个典型的高维非凸的难优化问题。我们比较了SGD和GD的性能，并表明SGD大大优于GD。特别地，对这些算法的弛豫时间进行幂律拟合表明，SGD在小批量大小的情况下的恢复阈值小于GD的对应阈值。

    Stochastic Gradient Descent (SGD) is an out-of-equilibrium algorithm used extensively to train artificial neural networks. However very little is known on to what extent SGD is crucial for to the success of this technology and, in particular, how much it is effective in optimizing high-dimensional non-convex cost functions as compared to other optimization algorithms such as Gradient Descent (GD). In this work we leverage dynamical mean field theory to analyze exactly its performances in the high-dimensional limit. We consider the problem of recovering a hidden high-dimensional non-linearly encrypted signal, a prototype high-dimensional non-convex hard optimization problem. We compare the performances of SGD to GD and we show that SGD largely outperforms GD. In particular, a power law fit of the relaxation time of these algorithms shows that the recovery threshold for SGD with small batch size is smaller than the corresponding one of GD.
    
[^17]: 什么时候编程思维对推理起作用?

    When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])

    [http://arxiv.org/abs/2308.15452](http://arxiv.org/abs/2308.15452)

    提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。

    

    大型语言模型（LLM）的推理能力在体现出人工智能领域中起着关键作用。尽管像编程思维提示这样的方法对于使用编程语言来解决复杂推理任务的LLM非常有效，但代码数据对推理能力的具体影响仍未充分探索。为了填补这一空白，我们提出了复杂性影响推理分数（CIRS），它结合了结构和逻辑属性，以衡量代码和推理能力之间的相关性。具体而言，我们使用抽象语法树来编码结构信息，并通过考虑难度和圈复杂度来计算逻辑复杂性。通过实证分析，我们发现并非所有复杂性的代码数据都可以被LLM学习或理解。最佳复杂性水平对于通过编程辅助提示改善推理能力至关重要。然后我们设计了一个自动合成的方法...

    The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
    
[^18]: 分布式资源管理中的价格差异化游戏对联合学习的影响

    Price-Discrimination Game for Distributed Resource Management in Federated Learning. (arXiv:2308.13838v1 [cs.LG])

    [http://arxiv.org/abs/2308.13838](http://arxiv.org/abs/2308.13838)

    本论文提出了一种价格差异化游戏（PDG），通过对不同客户提供的服务进行定价差异化，改善了联合学习的性能并降低了激励客户参与联合学习的成本。

    

    在传统的联合学习中，参数服务器和多个分布式客户端可以形成典型的买方市场，其中PS/买家数量远远少于客户端/卖家数量。为了改善联合学习的性能并减少激励客户参与联合学习的成本，本文提出了对不同客户提供的服务进行定价差异化，而不是简单地为不同客户提供相同的服务定价。价格差异化基于对联合学习带来的性能改进和计算通信能力的异质性。为此，本文提出了一个价格差异化游戏（PDG），全面解决了联合学习中的分布式资源管理问题，包括多目标权衡、客户端选择和激励机制。由于PDG是一个混合整数非线性规划（MINLP）问题，本文提出了一个具有低计算成本的分布式半启发式算法来解决该问题。

    In vanilla federated learning (FL) such as FedAvg, the parameter server (PS) and multiple distributed clients can form a typical buyer's market, where the number of PS/buyers of FL services is far less than the number of clients/sellers. In order to improve the performance of FL and reduce the cost of motivating clients to participate in FL, this paper proposes to differentiate the pricing for services provided by different clients rather than simply providing the same service pricing for different clients. The price is differentiated based on the performance improvements brought to FL and their heterogeneity in computing and communication capabilities. To this end, a price-discrimination game (PDG) is formulated to comprehensively address the distributed resource management problems in FL, including multi-objective trade-off, client selection, and incentive mechanism. As the PDG is a mixed-integer nonlinear programming (MINLP) problem, a distributed semi-heuristic algorithm with low c
    
[^19]: RIS-启用的毫米波无线系统中的信道估计：一种变分推断方法

    Channel Estimation in RIS-Enabled mmWave Wireless Systems: A Variational Inference Approach. (arXiv:2308.13616v1 [eess.SP])

    [http://arxiv.org/abs/2308.13616](http://arxiv.org/abs/2308.13616)

    本论文提出了一种变分推断方法来估计RIS-启用的毫米波无线系统中的信道状态信息，通过联合估计用户设备到RIS和RIS到基站的通道，减少了信号复杂性和瞬时信道的高信令开销。

    

    我们在完全被动的可重构智能表面(RIS)辅助的毫米波单用户单输入多输出(SIMO)通信系统中提出了一种基于变分推断(VI)的信道状态信息(CSI)估计方法。具体而言，我们首先提出了一种基于VI的联合信道估计方法，使用上行训练信号来估计用户设备(UE)到RIS(UE-RIS)和RIS到基站(RIS-BS)信道在被动RIS设置中。然而，基于瞬时CSI(I-CSI)更新相移会导致高信令开销，特别是由于UE-RIS信道的短相干块。因此，为了减少信令复杂性，我们提出了一种基于VI的方法，用于估计RIS-BS信道以及对于比瞬时UE-RIS信道更长时间保持准静态的UE-RIS信道的协方差矩阵。在VI框架下，我们使用方便的分布函数来近似信道增益/协方差矩阵的后验分布。

    We propose a variational inference (VI)-based channel state information (CSI) estimation approach in a fully-passive reconfigurable intelligent surface (RIS)-aided mmWave single-user single-input multiple-output (SIMO) communication system. Specifically, we first propose a VI-based joint channel estimation method to estimate the user-equipment (UE) to RIS (UE-RIS) and RIS to base station (RIS-BS) channels using uplink training signals in a passive RIS setup. However, updating the phase-shifts based on the instantaneous CSI (I-CSI) leads to a high signaling overhead especially due to the short coherence block of the UE-RIS channel. Therefore, to reduce the signaling complexity, we propose a VI-based method to estimate the RIS-BS channel along with the covariance matrix of the UE-RIS channel that remains quasi-static for a longer period than the instantaneous UE-RIS channel. In the VI framework, we approximate the posterior of the channel gains/covariance matrix with convenient distribut
    
[^20]: 更具表现力的图神经网络在生成任务中是否更好？

    Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])

    [http://arxiv.org/abs/2308.11978](http://arxiv.org/abs/2308.11978)

    本论文调查了更具表现力的图神经网络在分子图生成任务中的表现能力，并通过替换图生成模型的基础GNN来进行实验。研究发现，使用更具表现力的GNN可以改善生成任务的性能。

    

    图生成是一个重要的挑战，它涉及根据给定的标签预测一个完整的具有多个节点和边的图。这个任务对许多实际应用非常重要，包括药物和分子设计。近年来，在图生成领域出现了几种成功的方法。然而，这些方法存在两个重大问题：(1) 这些方法中使用的基础图神经网络（GNN）架构往往未经深入探索；(2) 这些方法往往只在有限的指标上进行评估。为填补这个空白，我们通过将图生成模型的基础GNN替换为更具表现力的GNN，研究了GNN在分子图生成任务中的表现能力。具体而言，我们分析了两种不同生成框架（GCPN和GraphAF）中六种GNN在六个不同的分子生成目标上的性能。

    Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
    
[^21]: 使用多条件扩散模型进行音频生成

    Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])

    [http://arxiv.org/abs/2308.11940](http://arxiv.org/abs/2308.11940)

    本论文提出了一种使用多条件扩散模型进行音频生成的方法。通过引入内容和风格等额外条件，增强了现有模型的可控性。这种方法可以精确控制生成音频的时间顺序、音高和能量。由于缺乏合适的数据集和评估指标，作者整合了现有数据集并进行了实验验证。

    

    基于文本的音频生成模型有其局限性，因为它们无法包含音频中的所有信息，仅依靠文本会导致受控性受限。为了解决这个问题，我们提出了一种新颖的模型，通过引入额外的条件（包括内容（时间戳）和风格（音高曲线和能量曲线））作为文本的补充，增强了现有预训练文本到音频模型的可控性。这种方法实现了对生成音频的时间顺序、音高和能量的精细控制。为了保持生成的多样性，我们使用一个可训练的控制条件编码器，该编码器由一个大型语言模型增强，并使用一个可训练的融合网络来编码和融合额外的条件，同时保持预训练文本到音频模型的权重不变。由于缺乏合适的数据集和评估指标，我们将现有数据集整合为一个新的数据集，包括音频和相应的条件，并使用一个可训练的控制条件编码器，该编码器由一个大型语言模型增强，并使用一个可训练的融合网络来编码和融合额外的条件，同时保持预训练文本到音频模型的权重不变。

    Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
    
[^22]: 多文档问答中的知识图谱引导

    Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])

    [http://arxiv.org/abs/2308.11730](http://arxiv.org/abs/2308.11730)

    这篇论文提出了一种知识图谱引导的方法，用于在多文档问答任务中为大型语言模型（LLMs）提示正确的上下文。通过构建多个文档上的知识图谱，并设计基于语言模型的图遍历器，该方法能够帮助LLMs在MD-QA中进行答案预测。

    

    大型语言模型（LLMs）的“预训练、提示、预测”范式在开放域问答（OD-QA）中取得了显著的成功。然而，很少有工作在多文档问答（MD-QA）场景下探索这个范式，这是一个要求对不同文档的内容和结构之间的逻辑关联有深入理解的任务。为了填补这一重要的空白，我们提出了一种知识图谱引导（KGP）方法，用于在MD-QA中为LLMs提示正确的上下文，该方法包括图构建模块和图遍历模块。对于图构建，我们使用节点来表示文段或文档结构（例如，页面/表格），而使用边来表示文段之间的语义/词汇相似性或者文档内的结构关系。对于图遍历，我们设计了一个基于LM的图遍历器，它在节点之间导航并收集支持性的文段，以帮助LLMs在MD-QA中进行答案预测。

    The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
    
[^23]: SPEGTI: 结构预测用于高效生成文本到图像模型

    SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models. (arXiv:2308.10997v1 [cs.CV])

    [http://arxiv.org/abs/2308.10997](http://arxiv.org/abs/2308.10997)

    本文提出了一种使用马尔可夫随机场（MRF）模型的轻量级方法，用于实现图像不同区域的相容性，以降低生成文本到图像模型的计算成本。

    

    现代文本到图像生成模型能够生成高质量的图像，既逼真又与文本提示相符。然而，这种质量需要付出巨大的计算成本：几乎所有这些模型都是迭代式的，需要多次运行推断，并使用大模型。这种迭代过程是为了确保图像的不同区域不仅与文本提示对齐，还与其他区域相容。本文中，我们提出了一种轻量级的方法来实现图像不同区域的相容性，使用了马尔可夫随机场（MRF）模型。这种方法能够与最近提出的Muse模型配合使用。MRF编码了不同空间位置的图像标记之间的相容性，并且使我们能够显著减少所需的Muse预测步骤。使用MRF的推断成本大大降低，并且可以通过反向传播快速学习其参数，通过对MRF进行建模。

    Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MR
    
[^24]: BLIVA: 一个简单的多模态LLM用于更好地处理文本丰富的视觉问题

    BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])

    [http://arxiv.org/abs/2308.09936](http://arxiv.org/abs/2308.09936)

    BLIVA是一个简单的多模态LLM模型，用于更好地处理嵌入了文本的图像上下文场景。它通过引入InstructBLIP的查询嵌入和LLaVA的编码补丁嵌入技术来改进视觉语言模型，从而有效解决了文本丰富的视觉问题。

    

    视觉语言模型（VLM）通过整合视觉理解能力扩展了大规模语言模型（LLM），在解决开放式视觉问答（VQA）任务方面取得了显著进展。然而，这些模型无法准确解释嵌入文本的图像，这在现实场景中经常发生。从图像中提取信息的标准流程通常涉及学习一组固定的查询嵌入。这些嵌入被设计为封装图像上下文，并随后用作LLM中的软提示输入。然而，这个过程受令牌数量的限制，可能限制对文本丰富的上下文场景的识别。为了改进这一点，本研究引入了BLIVA：InstructBLIP with Visual Assistant的增强版本。BLIVA集成了来自InstructBLIP的查询嵌入，并将编码的补丁嵌入直接投影到LLM中，这是受到LLaVA的启发的一种技术。这种方法有助于处理文本丰富的视觉问题。

    Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
    
[^25]: 关于大型语言模型的意想不到能力

    On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v1 [cs.AI])

    [http://arxiv.org/abs/2308.09720](http://arxiv.org/abs/2308.09720)

    大型语言模型展示了与其训练任务不直接相关的广泛能力，通过间接获取过程使其拥有综合能力的发展，这些能力在一定程度上可预测，并且与人类认知有关。

    

    大型语言模型能展示出与其训练任务（预测人类书写文本的下一个单词）不直接相关的广泛能力。本文讨论了这种间接获取过程的性质及其与其他已知间接过程的关系。文章主张这种间接获取的一个重要副作用是综合能力的发展。本文还讨论了大型语言模型所开发的能力在多大程度上是可预测的。最后，文章简要讨论了这些系统所获得的认知技能与人类认知之间的关系。

    Large language models are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I discuss the nature of this indirect acquisition process and its relation to other known indirect processes. I argue that an important side effect of such indirect acquisition is the development of integrated abilities. I discuss the extent to which the abilities developed by large language models are predictable. Finally, I briefly discuss the relation between the cognitive skills acquired by these systems and human cognition.
    
[^26]: 从期望到安全：通过在潜在空间中强制正确的原因来消除深度模型的偏见

    From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space. (arXiv:2308.09437v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.09437](http://arxiv.org/abs/2308.09437)

    该论文提出了一种通过梯度减小模型对偏见的敏感性的方法，从而在概念级别上确保正确原因，有效减轻深度神经网络中的偏见。该方法在多个数据集和环境中被验证有效。

    

    深度神经网络容易学习训练数据中潜藏的错误相关性，从而导致可能有偏见的预测。这在将这些模型部署于高风险决策场景（如医学应用）时存在风险。目前的后处理模型校正方法要么需要输入级别的注释，这只适用于局部化偏见，要么通过扩充潜在特征空间，希望能实现正确的原因。我们提出了一种新的方法，通过梯度减小模型对偏见的敏感性，从而在概念级别上确保正确的原因。当通过概念激活向量来建模偏见时，我们强调选择稳健的方向的重要性，因为传统的基于回归的方法（如支持向量机）往往会导致发散的方向。我们使用VGG、ResNet和EfficientN在ISIC、骨龄、ImageNet和CelebA数据集上在受控和真实环境中有效减轻偏见。

    Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientN
    
[^27]: 神经振荡器用于物理信息机器学习的泛化能力提升

    Neural oscillators for generalization of physics-informed machine learning. (arXiv:2308.08989v1 [cs.LG])

    [http://arxiv.org/abs/2308.08989](http://arxiv.org/abs/2308.08989)

    本文提出了一种称为神经振荡器的方法，通过与物理信息机器学习模型的融合，利用偏微分方程解的因果关系和时间特性来提高其泛化能力。实验证明了这种方法在处理复杂物理问题时的有效性。

    

    物理信息机器学习（PIML）的一个主要挑战是其在训练域之外的泛化能力，特别是在处理由偏微分方程（PDEs）表示的复杂物理问题时。本文旨在增强PIML的泛化能力，促进在未探索区域进行准确预测的实际应用。我们利用PDE解的固有因果关系和时间顺序特性，将基于常微分方程组的循环神经架构与PIML模型融合，称之为神经振荡器。通过有效地捕捉长时间依赖关系和缓解梯度爆炸和梯度消失问题，神经振荡器促进了PIML任务中的改进泛化能力。通过涉及时间依赖的非线性PDE和双调和梁方程的广泛实验，证明了所提出方法的有效性。

    A primary challenge of physics-informed machine learning (PIML) is its generalization beyond the training domain, especially when dealing with complex physical problems represented by partial differential equations (PDEs). This paper aims to enhance the generalization capabilities of PIML, facilitating practical, real-world applications where accurate predictions in unexplored regions are crucial. We leverage the inherent causality and temporal sequential characteristics of PDE solutions to fuse PIML models with recurrent neural architectures based on systems of ordinary differential equations, referred to as neural oscillators. Through effectively capturing long-time dependencies and mitigating the exploding and vanishing gradient problem, neural oscillators foster improved generalization in PIML tasks. Extensive experimentation involving time-dependent nonlinear PDEs and biharmonic beam equations demonstrates the efficacy of the proposed approach. Incorporating neural oscillators out
    
[^28]: Transformers能否学习用于未知系统的最优滤波？

    Can Transformers Learn Optimal Filtering for Unknown Systems?. (arXiv:2308.08536v1 [eess.SY])

    [http://arxiv.org/abs/2308.08536](http://arxiv.org/abs/2308.08536)

    本文研究了使用transformers进行最优输出估计问题，通过训练一个transformer来在未知系统上进行预测，并命名为元输出预测器（MOP）。我们观察到，尽管MOP没有访问模型的权限，但在大多数线性动态系统中，它的性能与基于卡尔曼滤波器的最优输出估计器相当，在具有非独立同分布噪声和时变动态的挑战性场景中也表现优秀。

    

    Transformers在自然语言处理中取得了显著的成功，然而它们在动态系统中的潜力仍然大部分未被探索。本文研究了使用transformers进行最优输出估计问题，它使用过去的所有输出来生成预测。我们使用来自先验分布的各种系统来训练transformer，然后在先前未见过的相同分布的系统上评估其性能。结果表明，获得的transformer就像一个预测算法，它可以在上下文中学习并快速适应和预测不同的系统，因此我们称之为元输出预测器（MOP）。尽管MOP没有访问模型的权限，但在大多数线性动态系统中，它的性能与基于卡尔曼滤波器的最优输出估计器相当。通过大量的数值实验，我们观察到MOP在具有非独立同分布噪声和时变动态的挑战性场景中也表现优秀。

    Transformers have demonstrated remarkable success in natural language processing; however, their potential remains mostly unexplored for problems arising in dynamical systems. In this work, we investigate the optimal output estimation problem using transformers, which generate output predictions using all the past ones. We train the transformer using various systems drawn from a prior distribution and then evaluate its performance on previously unseen systems from the same distribution. As a result, the obtained transformer acts like a prediction algorithm that learns in-context and quickly adapts to and predicts well for different systems - thus we call it meta-output-predictor (MOP). MOP matches the performance of the optimal output estimator, based on Kalman filter, for most linear dynamical systems even though it does not have access to a model. We observe via extensive numerical experiments that MOP also performs well in challenging scenarios with non-i.i.d. noise, time-varying dy
    
[^29]: 领域感知微调：增强神经网络的适应性能力

    Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v1 [cs.LG])

    [http://arxiv.org/abs/2308.07728](http://arxiv.org/abs/2308.07728)

    本文提出了领域感知微调（DAFT）方法，通过批归一化转换和线性探测与微调的集成，有效减轻微调过程中的特征畸变问题。

    

    微调预训练神经网络模型已成为各个领域广泛采用的方法。然而，微调可能导致已具备强大泛化能力的预训练特征提取器发生畸变。在适应新目标领域时减轻特征畸变至关重要。最近的研究表明，在进行微调之前，在分布数据集上对头层进行对齐处理可以处理特征畸变问题取得有希望的结果。然而，在微调过程中，批归一化层的处理存在显著局限性，导致性能不佳。在本文中，我们提出了领域感知微调（DAFT），一种新的方法，它结合了批归一化转换、线性探测和微调的特性。我们的批归一化转换方法通过减少对神经网络的修改来有效减轻特征畸变。此外，我们还引入了线性探测和微调的集成方法。

    Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integrati
    
[^30]: 深度布拉德利-特里评分：在没有具体评价标准的情况下估计物品的属性

    Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])

    [http://arxiv.org/abs/2307.13709](http://arxiv.org/abs/2307.13709)

    本论文提出了深度布拉德利-特里评分（DBTR）方法，用于评估不一定存在于数据集中的未知物品的属性。该方法通过将传统的布拉德利-特里模型与神经网络结构无缝结合，成功地学习了这些属性的预期量化。

    

    在现实世界中，许多属性，如竞争环境中的可取性或强度，无法直接观测，这使得它们难以评估。为了解决这个具有挑战性的问题，先前的研究主要集中在估计已知物品的这些属性，特别是出现在配对比较数据集中的运动员的实力。在本文中，我们介绍了深度布拉德利-特里评分（DBTR），这是一个新颖的机器学习框架，用于评估不一定存在于数据集中的未知物品的任何属性。我们的方法无缝地将传统的布拉德利-特里模型与神经网络结构相结合。我们还进一步推广了这个架构，用于具有不公平性的非对称环境，这在现实世界中更为常见。在我们的实验分析中，DBTR成功地学习了这些属性的预期量化。

    Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
    
[^31]: NetGPT: 超越提供个性化生成服务的本地AI网络架构

    NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services. (arXiv:2307.06148v1 [cs.LG])

    [http://arxiv.org/abs/2307.06148](http://arxiv.org/abs/2307.06148)

    NetGPT是一个能够在边缘和云端部署适当的大型语言模型的本地AI网络架构，实现了个性化生成服务，并通过协作云边方法论来优化资源协调和互动效果。

    

    大型语言模型（LLMs）通过生成信息在日常生活中取得了巨大成功，LLMs的个性化可能进一步促进它们在应用中的作用，因为它们能更好地与人类意图对齐。针对个性化生成服务，协作云边方法论听起来很有前景，因为它有助于有效协调异构分布式通信和计算资源。在本文中，我们讨论了几种候选的云边协作技术的利弊，提出了NetGPT，根据其计算能力在边缘和云端部署适当的LLMs。此外，边缘LLMs可以高效利用基于位置的信息进行个性化提示完成，从而有益于与云端LLMs的互动。在边缘和云端部署代表性的开源LLMs（例如GPT-2-base和LLaMA模型）之后，我们展示了NetGPT的可行性。

    Large language models (LLMs) have triggered tremendous success to empower daily life by generative information, and the personalization of LLMs could further contribute to their applications due to better alignment with human intents. Towards personalized generative services, a collaborative cloud-edge methodology sounds promising, as it facilitates the effective orchestration of heterogeneous distributed communication and computing resources. In this article, after discussing the pros and cons of several candidate cloud-edge collaboration techniques, we put forward NetGPT to capably deploy appropriate LLMs at the edge and the cloud in accordance with their computing capacity. In addition, edge LLMs could efficiently leverage location-based information for personalized prompt completion, thus benefiting the interaction with cloud LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on th
    
[^32]: PIGEON: 预测图像地理位置

    PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])

    [http://arxiv.org/abs/2307.05845](http://arxiv.org/abs/2307.05845)

    PIGEON是一个用于全球规模图像地理定位的多任务端到端系统，通过语义地理单元的创建和精化，以及无监督聚类和ProtoNets的应用，实现了最先进的性能，并提供了预训练的CLIP转换器模型StreetCLIP。

    

    我们引入PIGEON，一个用于全球规模图像地理定位的多任务端到端系统，在外部基准测试和人类评估中均实现了最先进的性能。我们的工作结合语义地理单元的创建和标签平滑，对具有地理信息的图像进行视觉转换器的预训练，并通过ProtoNets在候选地理单元集合中改进位置预测。PIGEON的贡献有三个方面：首先，我们设计了一种基于开源数据的语义地理单元创建和分割算法，可以适用于任何地理空间数据集。第二，我们展示了地理单元内部精化的有效性，并展示了无监督聚类和ProtoNets在该任务中的适用性。最后，我们将我们预训练的CLIP转换器模型，StreetCLIP，公开提供，可用于与应对气候变化和城市乡村场景理解相关的领域。

    We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation. Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells. The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset. Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task. Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding.
    
[^33]: STS-CCL：用于城市交通预测的时空同步上下文对比学习

    STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting. (arXiv:2307.02507v1 [cs.LG])

    [http://arxiv.org/abs/2307.02507](http://arxiv.org/abs/2307.02507)

    本研究通过引入先进的对比学习方法，提出了一种新颖的时空同步上下文对比学习（STS-CCL）模型，用于高效地捕捉大规模无标签交通数据的复杂时空表示。该模型通过使用动态图视图生成器和语义上下文对比方法，实现了节点级和图级的对比学习。

    

    高效地捕捉大规模无标签交通数据中复杂的时空表示仍然是一个具有挑战性的任务。鉴于这个困境，本文采用先进的对比学习方法，并提出了一种新颖的时空同步上下文对比学习（STS-CCL）模型。首先，我们详细阐述了用于时空图数据的基本和强大的增强方法，这些方法不仅扰动了图结构和时间特征的数据，而且还利用了基于学习的动态图视图生成器进行自适应增强。其次，我们引入了一种时空同步对比模块（STS-CM），以同时捕捉良好的时空依赖关系并实现图级对比。为了进一步区分负筛选中的节点个体，设计了一种基于语义特征和空间异质性的语义上下文对比方法，实现了节点级的对比学习以及…

    Efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains to be a challenging task. In considering of the dilemma, this work employs the advanced contrastive learning and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation methods for spatiotemporal graph data, which not only perturb the data in terms of graph structure and temporal characteristics, but also employ a learning-based dynamic graph view generator for adaptive augmentation. Second, we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to simultaneously capture the decent spatial-temporal dependencies and realize graph-level contrasting. To further discriminate node individuals in negative filtering, a Semantic Contextual Contrastive method is designed based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with
    
[^34]: 一体化：图神经网络的多任务提示

    All in One: Multi-task Prompting for Graph Neural Networks. (arXiv:2307.01504v1 [cs.SI])

    [http://arxiv.org/abs/2307.01504](http://arxiv.org/abs/2307.01504)

    本文提出了一种新颖的图模型的多任务提示方法，通过统一图提示和语言提示的格式，填补了预训练模型与各种图任务之间的差距。

    

    最近，“预训练和微调”已成为许多图任务的标准工作流程，因为它可以利用通用的图知识来缓解每个应用中缺乏图注释的问题。然而，节点级、边级和图级的图任务差异很大，导致预训练预文本通常与这些多任务不兼容。这种差距甚至可能导致对特定应用的“负迁移”，从而导致结果不佳。受自然语言处理（NLP）中提示学习的启发，该方法在各种NLP任务中利用先前知识已经显示出较大的有效性，我们研究了填补预训练模型和各种图任务之间差距的提示主题。在本文中，我们提出了一种新颖的用于图模型的多任务提示方法。具体来说，我们首先通过提示令牌、令牌结构和插入模式统一图提示和语言提示的格式。

    Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In
    
[^35]: AVSegFormer: 基于Transformer的音视频分割

    AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.01146](http://arxiv.org/abs/2307.01146)

    AVSegFormer是一种基于Transformer的音视频分割框架，通过引入音频查询和可学习查询来选择性地关注视觉特征，还使用音频-视觉混合器动态调整视觉特征，并通过中间掩模损失增强解码器的监督。实验证明该方法的有效性。

    

    音频与视觉的结合长期以来一直是多模态领域的一个研究课题。最近，引入了一项新的音频-视觉分割（AVS）任务，旨在定位和分割给定视频中的有声对象。这个任务首次要求在像素级别对音频驱动的场景进行理解，存在着重大挑战。在本文中，我们提出了AVSegFormer，这是一种利用Transformer架构进行AVS任务的新框架。具体来说，我们在变压器解码器中引入了音频查询和可学习查询，使网络能够有选择地关注感兴趣的视觉特征。此外，我们还设计了一个音频-视觉混合器，通过增强相关的空间通道和抑制无关的空间通道来动态调整视觉特征。此外，我们设计了一个中间掩模损失，以增强解码器的监督，鼓励网络产生更准确的中间预测。大量实验证明了我们方法的有效性。

    The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate tha
    
[^36]: 大象与算法：人工智能在大象监测中的当前和未来作用综述

    Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring. (arXiv:2306.13803v1 [cs.AI])

    [http://arxiv.org/abs/2306.13803](http://arxiv.org/abs/2306.13803)

    本文探讨了AI和ML在保护非洲保护区中至关重要的大象方面的作用。新的AI和ML技术提供了解决方案以处理并提取重要信息。在AI专家和生态研究人员之间的协作下，这些创新技术可以帮助增强野生动物保护，为其他物种设定先例。

    

    人工智能（AI）和机器学习（ML）为增进对动物行为和保护策略的理解提供了革命性机会。以非洲保护区中至关重要的大象为焦点，本文探讨了AI和ML在它们保护中的作用。给定从各种传感器（如摄像头、麦克风、地震仪、无人机和卫星）收集到的越来越多的数据，挑战在于管理和解读这些庞大的数据。新的AI和ML技术提供了简化这一过程的解决方案，帮助我们提取重要信息，否则可能会被忽视。本文重点介绍了不同的AI驱动监测方法及其在改善大象保护方面的潜力。AI专家和生态研究人员之间的协作是利用这些创新技术以增强野生动物保护的关键所在，为许多其他物种设定了先例。

    Artificial intelligence (AI) and machine learning (ML) present revolutionary opportunities to enhance our understanding of animal behavior and conservation strategies. Using elephants, a crucial species in Africa's protected areas, as our focal point, we delve into the role of AI and ML in their conservation. Given the increasing amounts of data gathered from a variety of sensors like cameras, microphones, geophones, drones, and satellites, the challenge lies in managing and interpreting this vast data. New AI and ML techniques offer solutions to streamline this process, helping us extract vital information that might otherwise be overlooked. This paper focuses on the different AI-driven monitoring methods and their potential for improving elephant conservation. Collaborative efforts between AI experts and ecological researchers are essential in leveraging these innovative technologies for enhanced wildlife conservation, setting a precedent for numerous other species.
    
[^37]: MimiC：模仿中心更新解决联邦学习中的客户端退出问题

    MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v1 [cs.LG])

    [http://arxiv.org/abs/2306.12212](http://arxiv.org/abs/2306.12212)

    本文提出的 MimiC 算法解决了联邦学习中客户端退出问题，通过模仿缺失的客户端更新解决了聚合更新和期望中心更新之间的分歧，实现了更高的测试准确率和更低的通信成本。

    

    联邦学习是一种有前途的隐私保护协作学习框架。在联邦学习中，模型训练任务分发给客户端，只需要在中央服务器收集模型更新。然而，在移动边缘网络中部署时，客户端（如智能手机和可穿戴设备）可能会无预警地退出任何一次训练迭代，这会阻碍联邦学习达到收敛。本文解决了联邦学习中这一关键挑战，设计出一种名为 MimiC 的新型训练算法，该算法在中心服务器修改其更新以模仿缺失客户端更新，通过实验结果显示，MimiC 相对现有方法在多个基准数据集上均取得了更高的测试准确率和更低的通信成本。

    Federated learning (FL) is a promising framework for privacy-preserving collaborative learning. In FL, the model training tasks are distributed to clients and only the model updates need to be collected at a central server. However, when being deployed at the mobile edge network, clients (e.g., smartphones and wearables) may have unpredictable availability and randomly drop out of any training iteration, which hinders FL from achieving the convergence. This paper tackles such a critical challenge of FL. In particular, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg can only oscillate within the neighborhood of a stationary point of the global loss function, which is caused by the divergence between the aggregated update and the desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modi
    
[^38]: LabelBench：基于综合框架的标签高效学习基准评估

    LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])

    [http://arxiv.org/abs/2306.09910](http://arxiv.org/abs/2306.09910)

    本论文介绍了一个新的综合性标签高效学习基准评估框架LabelBench，并通过引入一种新的与半监督学习相结合的主动学习方法的基准测试，证明了在相对较少的标记示例下实现更好的标签效率。

    

    标记数据是现代机器学习应用程序的关键，但获取标记可能很昂贵。为了减缓这一成本，机器学习方法（如迁移学习、半监督学习和主动学习）旨在实现标签高效性：从相对较少的标记示例中实现高预测性能。虽然在实践中获得最佳的标签效率通常需要这些技术的组合，但现有的基准评估框架并没有捕捉到所有这些技术的协同组合。本文通过引入LabelBench解决了这个缺陷，这是一个新的计算效率高的综合性框架，用于联合评估多个标签高效学习技术。作为LabelBench的一个应用，我们引入了一种与半监督学习一起使用的最新主动学习方法的新基准，用于微调预训练的视觉转换器。我们的基准证明了比先前报告的更好的标签效率。

    Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
    
[^39]: 血压测量技术综述：解决潜在的偏差来源

    A Survey on Blood Pressure Measurement Technologies: Addressing Potential Sources of Bias. (arXiv:2306.08451v2 [physics.med-ph] UPDATED)

    [http://arxiv.org/abs/2306.08451](http://arxiv.org/abs/2306.08451)

    该综述聚焦于带式血压监测技术，强调了由于测量和设备误差、人口统计学数据和体型差异等因素导致的血压测量偏差和方差。研发使用人工智能来纠正误差的新一代带式血压设备是重点发展方向。

    

    临床和流动场景中的定期血压（BP）监测在预防、诊断、治疗和管理心血管疾病中起着至关重要的作用。最近，流动式BP测量设备的广泛采用主要是由于高血压的普遍增加以及其相关风险和临床状况。最近的指南建议将定期BP监测作为常规临床访视甚至在家里进行。 这种增加的BP测量技术利用带式测压方法也带来了重要的关注点，涉及各种设置下报告的BP值的准确性。在这项综述中，我们重点介绍了基于带式测压技术的BP测量方法如何由于测量和设备误差、人口统计学数据和体型差异等因素导致显著偏差和方差。在这些固有的偏差情况下，发展一种新一代的基于带式测压设备，其使用人工智能技术来纠正误差是非常重要的。

    Regular blood pressure (BP) monitoring in clinical and ambulatory settings plays a crucial role in the prevention, diagnosis, treatment, and management of cardiovascular diseases. Recently, the widespread adoption of ambulatory BP measurement devices has been driven predominantly by the increased prevalence of hypertension and its associated risks and clinical conditions. Recent guidelines advocate for regular BP monitoring as part of regular clinical visits or even at home. This increased utilization of BP measurement technologies has brought up significant concerns, regarding the accuracy of reported BP values across settings.  In this survey, focusing mainly on cuff-based BP monitoring technologies, we highlight how BP measurements can demonstrate substantial biases and variances due to factors such as measurement and device errors, demographics, and body habitus. With these inherent biases, the development of a new generation of cuff-based BP devices which use artificial-intelligen
    
[^40]: 关于适应性预测集期望大小的研究

    On the Expected Size of Conformal Prediction Sets. (arXiv:2306.07254v1 [stat.ML])

    [http://arxiv.org/abs/2306.07254](http://arxiv.org/abs/2306.07254)

    该论文研究了适应性预测集的期望大小问题，提出了一种理论量化方法以及点估计和高概率区间，并在真实数据集上验证了其实用性。

    

    虽然适应性预测器在误差频率方面具有严格的统计保证，但其预测集大小对其实际效用至关重要。不幸的是，目前缺乏有限样本分析和预测集大小的保证。为了解决这个问题，我们在分裂适应性预测框架下理论量化预测集的期望大小。因为这种精确的计算通常无法直接计算，我们进一步推导出可轻松计算的点估计和高概率区间，提供了一种描述测试和校准数据不同可能实现的期望预测集大小的实用方法。此外，我们通过对回归和分类问题的真实世界数据集进行实验证实了我们结果的实用性。

    While conformal predictors reap the benefits of rigorous statistical guarantees for their error frequency, the size of their corresponding prediction sets is critical to their practical utility. Unfortunately, there is currently a lack of finite-sample analysis and guarantees for their prediction set sizes. To address this shortfall, we theoretically quantify the expected size of the prediction set under the split conformal prediction framework. As this precise formulation cannot usually be calculated directly, we further derive point estimates and high probability intervals that can be easily computed, providing a practical method for characterizing the expected prediction set size across different possible realizations of the test and calibration data. Additionally, we corroborate the efficacy of our results with experiments on real-world datasets, for both regression and classification problems.
    
[^41]: Mesogeos: 地中海区域数据驱动野火建模的多用途数据集

    Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean. (arXiv:2306.05144v1 [cs.CV])

    [http://arxiv.org/abs/2306.05144](http://arxiv.org/abs/2306.05144)

    Mesogeos是一个地中海地区的大规模多用途数据集，用于数据驱动的野火建模。它集成了历史野火记录和野火驱动因素，具有机器学习的潜力，提供了短期野火危险预测和最终烧毁区域估计两个可用于演示潜力的数据集。

    

    我们介绍了Mesogeos，这是一个大规模的多用途数据集，用于地中海地区的野火建模。Mesogeos集成了代表野火驱动因素（气象、植被、人类活动）和17年（2006-2022年）野火点燃和烧毁区域的历史记录的变量。它被设计为云友好型时空数据集（即数据立方体），在1km x 1km x 1天的分辨率下对所有变量进行了协调。数据立方体的结构为各种野火建模任务的机器学习(ML)使用提供了机会。我们提取出两个ML可用的数据集，以展示这个潜力：(1)短期野火危险预测和(2)在点火位置的情况下估计最终烧毁区域。我们定义了适当的指标和基线来评估每个跟踪中模型的性能。通过发布数据立方体，以及用于创建ML数据集和模型的代码，我们鼓励社区促进实施。

    We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire modeling in the Mediterranean. Mesogeos integrates variables representing wildfire drivers (meteorology, vegetation, human activity) and historical records of wildfire ignitions and burned areas for 17 years (2006-2022). It is designed as a cloud-friendly spatio-temporal dataset, namely a datacube, harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The datacube structure offers opportunities to assess machine learning (ML) usage in various wildfire modeling tasks. We extract two ML-ready datasets that establish distinct tracks to demonstrate this potential: (1) short-term wildfire danger forecasting and (2) final burned area estimation given the point of ignition. We define appropriate metrics and baselines to evaluate the performance of models in each track. By publishing the datacube, along with the code to create the ML datasets and models, we encourage the community to foster the implementatio
    
[^42]: 多模态潜在扩散

    Multi-modal Latent Diffusion. (arXiv:2306.04445v1 [cs.LG])

    [http://arxiv.org/abs/2306.04445](http://arxiv.org/abs/2306.04445)

    该论文提出一个新的方法来处理多模态数据，该方法使用了一组独立训练的单模态确定性自编码器，将单个潜在变量连接到公共潜在空间中，并通过掩蔽扩散模型实现了良好的生成质量和模态间连贯性。

    

    在现代应用中，多模态数据集无处不在，多模态变分自编码器是一类流行的模型，旨在学习不同模态的联合表示。然而，现有方法存在连贯性 - 质量折衷的问题，具有良好生成质量的模型缺乏模态间的生成连贯性，反之亦然。我们讨论现有方法的局限性，从而说明需要不同的方法。我们提出了一种新方法，该方法使用一组独立训练的单模态确定性自编码器，将单个潜在变量连接到公共潜在空间中，然后将其输入到掩蔽扩散模型中以实现生成建模。我们还引入了一种新的多时间训练方法，用于学习多模态扩散的条件得分网络。通过对几个多模态数据集的评估，我们的方法在生成质量和连贯性方面都明显优于竞争对手。

    Multi-modal data-sets are ubiquitous in modern applications, and multi-modal Variational Autoencoders are a popular family of models that aim to learn a joint representation of the different modalities. However, existing approaches suffer from a coherence-quality tradeoff, where models with good generation quality lack generative coherence across modalities, and vice versa. We discuss the limitations underlying the unsatisfactory performance of existing methods, to motivate the need for a different approach. We propose a novel method that uses a set of independently trained, uni-modal, deterministic autoencoders. Individual latent variables are concatenated into a common latent space, which is fed to a masked diffusion model to enable generative modeling. We also introduce a new multi-time training method to learn the conditional score network for multi-modal diffusion. Our methodology substantially outperforms competitors in both generation quality and coherence, as shown through an e
    
[^43]: 从非线性混合下的干预中学习线性因果表示

    Learning Linear Causal Representations from Interventions under General Nonlinear Mixing. (arXiv:2306.02235v1 [cs.LG])

    [http://arxiv.org/abs/2306.02235](http://arxiv.org/abs/2306.02235)

    本文针对先前工作弱一类问题进行推广，提出了一种用于在非线性混合下的干预中学习线性因果表示的强可识别性算法，证明了其有效性，并提出了在实践中识别潜在变量的对比算法。

    

    我们研究了在混合函数完全通用的一般设置下，从未知的潜在干预中学习因果表示的问题，其中潜在分布是高斯分布。 我们证明了在单节点未知干预（即没有干预目标的情况下）给出强可识别性结果。这推广了先前的工作，先前的工作着重于更弱的类别，例如线性映射或成对的反事实数据。这也是首次从非配对干预的深度神经网络嵌入中获得因果可识别性。我们的证明依赖于仔细揭示经过非线性密度转换后数据分布中存在的高维几何结构，我们通过分析潜在分布的精度矩阵的二次形式来捕捉这种结构。最后，我们提出了一种对比算法来实际识别潜在变量，并评估其在各种任务上的性能。

    We study the problem of learning causal representations from unknown, latent interventions in a general setting, where the latent distribution is Gaussian but the mixing function is completely general. We prove strong identifiability results given unknown single-node interventions, i.e., without having access to the intervention targets. This generalizes prior works which have focused on weaker classes, such as linear maps or paired counterfactual data. This is also the first instance of causal identifiability from non-paired interventions for deep neural network embeddings. Our proof relies on carefully uncovering the high-dimensional geometric structure present in the data distribution after a non-linear density transformation, which we capture by analyzing quadratic forms of precision matrices of the latent distributions. Finally, we propose a contrastive algorithm to identify the latent variables in practice and evaluate its performance on various tasks.
    
[^44]: 不是所有神经符号概念都是平等的： 推理快捷方式的分析和缓解

    Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts. (arXiv:2305.19951v1 [cs.LG])

    [http://arxiv.org/abs/2305.19951](http://arxiv.org/abs/2305.19951)

    本研究通过将NeSy模型中出现的推理快捷方式定义为学习目标的意外最优解，并确定其发生的四个关键条件，提出了几种可行的缓解策略并对其进行了分析，显示推理快捷方式难以处理。

    

    神经符号（NeSy）预测模型承诺具有改进的约束遵从性，系统化泛化和可解释性，因为它们允许通过对从子符号输入中提取出的高级概念进行推理来推断与某些先验知识一致的标签。最近显示NeSy预测器受到推理快捷方式的影响：它们可以通过利用具有意外语义的概念达到高精度，从而短于其承诺的优势。但是，缺少对推理快捷方式及其潜在缓解策略的系统描述。本文通过将其表征为学习目标的意外最优解，并确定其发生背后的四个关键条件来填补这一空白。基于此，我们推导出几种自然的缓解策略，并从理论和实证角度分析它们的功效。我们的分析显示，推理快捷方式很难处理，这对于信任它们的合理性产生了疑问。

    Neuro-Symbolic (NeSy) predictive models hold the promise of improved compliance with given constraints, systematic generalization, and interpretability, as they allow to infer labels that are consistent with some prior knowledge by reasoning over high-level concepts extracted from sub-symbolic inputs. It was recently shown that NeSy predictors are affected by reasoning shortcuts: they can attain high accuracy but by leveraging concepts with unintended semantics, thus coming short of their promised advantages. Yet, a systematic characterization of reasoning shortcuts and of potential mitigation strategies is missing. This work fills this gap by characterizing them as unintended optima of the learning objective and identifying four key conditions behind their occurrence. Based on this, we derive several natural mitigation strategies, and analyze their efficacy both theoretically and empirically. Our analysis shows reasoning shortcuts are difficult to deal with, casting doubts on the trus
    
[^45]: 游戏中的学习是否对学习者有益？

    Is Learning in Games Good for the Learners?. (arXiv:2305.19496v1 [cs.GT])

    [http://arxiv.org/abs/2305.19496](http://arxiv.org/abs/2305.19496)

    我们提出了“广义均衡”的概念，通过学习可以在某些游戏中获得更好的结果。如果没有纯纳什均衡，则一名玩家可以从不同策略中受益，结果捕获了Stackelberg均衡的扩展。

    

    我们考虑与奖励和后悔在两个代理之间重复玩游戏相关的一些问题。为了实现这一点，我们引入了广义均衡的概念，该概念允许不对称的后悔约束，并为每个代理和一对后悔约束派生可行值的多面体。作为核心案例，我们强调了一方是禁止交换的，另一方的后悔没有限制。我们证明了这一点，它捕获了与Stackelberg均衡的一种扩展，可匹配最优值，并且存在一大类游戏，在这些游戏中，一名玩家可以通过从禁止交换的后悔算法中偏离，显著增加自己的效用（实际上，几乎所有没有纯纳什均衡的游戏都是这种形式）。

    We consider a number of questions related to tradeoffs between reward and regret in repeated gameplay between two agents. To facilitate this, we introduce a notion of {\it generalized equilibrium} which allows for asymmetric regret constraints, and yields polytopes of feasible values for each agent and pair of regret constraints, where we show that any such equilibrium is reachable by a pair of algorithms which maintain their regret guarantees against arbitrary opponents. As a central example, we highlight the case one agent is no-swap and the other's regret is unconstrained. We show that this captures an extension of {\it Stackelberg} equilibria with a matching optimal value, and that there exists a wide class of games where a player can significantly increase their utility by deviating from a no-swap-regret algorithm against a no-swap learner (in fact, almost any game without pure Nash equilibria is of this form). Additionally, we make use of generalized equilibria to consider tradeo
    
[^46]: 学习两层神经网络：一次(巨大)的步骤。

    Learning Two-Layer Neural Networks, One (Giant) Step at a Time. (arXiv:2305.18270v1 [stat.ML])

    [http://arxiv.org/abs/2305.18270](http://arxiv.org/abs/2305.18270)

    本文研究了浅层神经网络的训练动态及其条件，证明了动态下梯度下降可以通过有限数量的大批量梯度下降步骤来促进特征学习，并找到了多个和单一方向的最佳批量大小，有助于促进特征学习和方向的专业化。

    

    我们研究了浅层神经网络的训练动态，研究了有限数量的大批量梯度下降步骤有助于在核心范围之外促进特征学习的条件。我们比较了批量大小和多个(但有限的)步骤的影响。我们分析了单步骤过程，发现批量大小为$n=O(d)$可以促进特征学习，但只适合学习单一方向或单索引模型。相比之下，$n=O(d^2)$对于学习多个方向和专业化至关重要。此外，我们证明“硬”方向缺乏前$\ell$个Hermite系数，仍未被发现，并且需要批量大小为$n=O(d^\ell)$才能被梯度下降捕获。经过几次迭代，情况发生变化：批量大小为$n=O(d)$足以学习新的目标方向，这些方向在Hermite基础上线性连接到之前学习的方向所涵盖的子空间。

    We study the training dynamics of shallow neural networks, investigating the conditions under which a limited number of large batch gradient descent steps can facilitate feature learning beyond the kernel regime. We compare the influence of batch size and that of multiple (but finitely many) steps. Our analysis of a single-step process reveals that while a batch size of $n = O(d)$ enables feature learning, it is only adequate for learning a single direction, or a single-index model. In contrast, $n = O(d^2)$ is essential for learning multiple directions and specialization. Moreover, we demonstrate that ``hard'' directions, which lack the first $\ell$ Hermite coefficients, remain unobserved and require a batch size of $n = O(d^\ell)$ for being captured by gradient descent. Upon iterating a few steps, the scenario changes: a batch-size of $n = O(d)$ is enough to learn new target directions spanning the subspace linearly connected in the Hermite basis to the previously learned directions,
    
[^47]: GLOBE-CE：一种用于全球因果解释的基于翻译的方法

    GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. (arXiv:2305.17021v1 [cs.LG])

    [http://arxiv.org/abs/2305.17021](http://arxiv.org/abs/2305.17021)

    GLOBE-CE是一种用于全球因果解释的机器学习方法，它可以超越局部解释，提供更有效和交互式的解释工具。

    

    因果解释在可解释性方面得到了广泛研究，公平性、追索权和模型理解等领域的应用依赖于一系列方法。然而，这些方法最大的缺点是无法提供超越局部或实例级别的解释。尽管许多作品涉及全局解释的概念，通常建议聚合大量局部解释以确定全局属性，但很少提供可靠且计算可行的框架。同时，实践者需要更有效和交互式的可解释性工具。我们借此机会提出了全局且有效的反事实解释框架(GLOBE-CE)，这是一个灵活的框架，解决了当前最先进框架在高维数据集和连续特征存在的可靠性和可扩展性问题。此外，我们提供了一个独特的数学模型。

    Counterfactual explanations have been widely studied in explainability, with a range of application dependent methods prominent in fairness, recourse and model understanding. The major shortcoming associated with these methods, however, is their inability to provide explanations beyond the local or instance-level. While many works touch upon the notion of a global explanation, typically suggesting to aggregate masses of local explanations in the hope of ascertaining global properties, few provide frameworks that are both reliable and computationally tractable. Meanwhile, practitioners are requesting more efficient and interactive explainability tools. We take this opportunity to propose Global & Efficient Counterfactual Explanations (GLOBE-CE), a flexible framework that tackles the reliability and scalability issues associated with current state-of-the-art, particularly on higher dimensional datasets and in the presence of continuous features. Furthermore, we provide a unique mathemati
    
[^48]: Union Subgraph神经网络

    Union Subgraph Neural Networks. (arXiv:2305.15747v1 [cs.LG])

    [http://arxiv.org/abs/2305.15747](http://arxiv.org/abs/2305.15747)

    本文提出了一种新型图神经网络UnionSNN，注入了邻居连接信息，通过联合子图来编码高阶连接性，实验证明其在节点分类任务中优于1-WL和当前最先进的GNN模型。

    

    图神经网络(GNNs)被广泛用于许多应用领域的图表示学习。由于它们通过迭代传递消息来处理有根子树，因此普通的GNNs的表达能力上限为1维Weisfeiler-Leman(1-WL)测试。在本文中，我们通过注入从新类型的子结构中提取的邻居连接信息来增强GNNs。我们首先研究了局部邻域中存在的不同连接性，并确定了一个称为联合子图的子结构，它能够捕捉到一条边的1-跳邻居的完整图像。然后，我们设计了一种基于最短路径的子结构描述符，具有三个良好的性质，并且可以有效地编码联合子图中的高阶连接性。通过注入编码邻居连接性，我们提出了一种新的模型，即Union Subgraph神经网络(UnionSNN)，它被证明在区分非同构图方面比1-WL严格更强。在基准数据集上的实验表明，UnionSNN始终优于最先进的GNN模型，并在多个节点分类任务上取得了显著的改进。

    Graph Neural Networks (GNNs) are widely used for graph representation learning in many application domains. The expressiveness of vanilla GNNs is upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) test as they operate on rooted subtrees through iterative message passing. In this paper, we empower GNNs by injecting neighbor-connectivity information extracted from a new type of substructure. We first investigate different kinds of connectivities existing in a local neighborhood and identify a substructure called union subgraph, which is able to capture the complete picture of the 1-hop neighborhood of an edge. We then design a shortest-path-based substructure descriptor that possesses three nice properties and can effectively encode the high-order connectivities in union subgraphs. By infusing the encoded neighbor connectivities, we propose a novel model, namely Union Subgraph Neural Network (UnionSNN), which is proven to be strictly more powerful than 1-WL in distinguishing non-isom
    
[^49]: 负反馈训练：提高NVCiM DNN加速器鲁棒性的新概念

    Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])

    [http://arxiv.org/abs/2305.14561](http://arxiv.org/abs/2305.14561)

    本文介绍了一种新的训练方法，使用负反馈机制来增强DNN模型的鲁棒性，特别是在存在设备变异的情况下。

    

    利用非挥发性存储器(NVM)实现的内存计算(CiM)为加速深度神经网络(DNNs)提供了一种高效的方法。 CiM加速器通过在同一电路板结构中存储网络权重和执行矩阵操作，以最小的面积需求和异常的能效，提供DNN推理加速。然而，NVM设备的随机性和内在变化往往导致性能降低，如与预期结果相比减少分类精度。尽管提出了几种方法来减轻设备变异并增强鲁棒性，但大多数方法都依赖于整体调节并缺乏对训练过程的限制。受到负反馈机制的启发，我们引入了一种新的训练方法，使用多出口机制作为负反馈，在设备变异的情况下增强DNN模型的性能。

    Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents a highly promising and efficient approach for accelerating deep neural networks (DNNs). By concurrently storing network weights and performing matrix operations within the same crossbar structure, CiM accelerators offer DNN inference acceleration with minimal area requirements and exceptional energy efficiency. However, the stochasticity and intrinsic variations of NVM devices often lead to performance degradation, such as reduced classification accuracy, compared to expected outcomes. Although several methods have been proposed to mitigate device variation and enhance robustness, most of them rely on overall modulation and lack constraints on the training process. Drawing inspiration from the negative feedback mechanism, we introduce a novel training approach that uses a multi-exit mechanism as negative feedback to enhance the performance of DNN models in the presence of device variation. Our negative feedbac
    
[^50]: ZeroSCROLLS：一个用于长文本理解的零Shot基准测试

    ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding. (arXiv:2305.14196v1 [cs.CL])

    [http://arxiv.org/abs/2305.14196](http://arxiv.org/abs/2305.14196)

    ZeroSCROLLS是一个用于长文本自然语言理解的零Shot基准测试，包括六个任务和四个数据集，能够评估大型语言模型的性能。当前，GPT-4的平均得分最高，但在聚合任务等多个挑战上，仍有改进的空间。

    

    我们介绍了 ZeroSCROLLS，这是一个用于长文本自然语言理解的零Shot基准测试，仅包含测试集而没有训练或开发数据。我们从SCROLLS基准测试中适应了六个任务，并添加了四个新数据集，包括两个新的信息融合任务，例如聚合正面评价的百分比。使用ZeroSCROLLS，我们对开源和闭源大型语言模型进行了全面评估，发现Claude优于ChatGPT，并且GPT-4获得了最高的平均分数。然而，在ZeroSCROLLS的多个开放挑战方面（例如，聚合任务），还有改进的空间，因为模型很难通过朴素的基准测试。由于最先进的技术还在不断更新，我们邀请研究人员在实时的ZeroSCROLLS排行榜上评估他们的想法。

    We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test sets, without training or development data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard
    
[^51]: 稀疏图的消息传递架构的最优性

    Optimality of Message-Passing Architectures for Sparse Graphs. (arXiv:2305.10391v1 [cs.LG])

    [http://arxiv.org/abs/2305.10391](http://arxiv.org/abs/2305.10391)

    本研究证明了将消息传递神经网络应用于稀疏图的节点分类任务是渐近本地贝叶斯最优的，提出了一种实现最优分类器的算法，并将最优分类器的性能理论上与现有学习方法进行了比较。

    

    我们研究了特征装饰图上的节点分类问题，在稀疏设置下，即节点的预期度数为节点数的O(1)时。这样的图通常被称为本地树状图。我们引入了一种叫做渐近本地贝叶斯最优性的节点分类任务的贝叶斯最优性概念，并根据这个标准计算了具有任意节点特征和边连接分布的相当一般的统计数据模型的最优分类器。该最优分类器可以使用消息传递图神经网络架构实现。然后我们计算了该分类器的泛化误差，并在一个已经研究充分的统计模型上从理论上与现有的学习方法进行比较。我们发现，在低图信号的情况下，最佳消息传递架构插值于标准MLP和一种典型的c架构之间。

    We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is $O(1)$ in the number of nodes. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical c
    
[^52]: 对称正定流形上低复杂度的子空间下降算法

    Low-complexity subspace-descent over symmetric positive definite manifold. (arXiv:2305.02041v1 [stat.ML])

    [http://arxiv.org/abs/2305.02041](http://arxiv.org/abs/2305.02041)

    本文提出了一种基于黎曼子空间下降算法的对称正定流形上的函数最小化方法，其具有低复杂度和避免昂贵矩阵操作和计算黎曼梯度的优点。

    

    本文提出了一种低复杂度的黎曼子空间下降算法，用于在对称正定（SPD）流形上对函数进行最小化。与现有的黎曼梯度下降变体不同的是，所提出的方法利用 carefully chosen 的子空间，使得更新可以写成迭代的 Cholesky 因子和一个稀疏矩阵的乘积形式。由此产生的更新避免了昂贵的矩阵操作，如矩阵指数和密集矩阵乘法，这些操作通常在几乎所有其他 Riemannian 优化算法中都是必需的。

    This work puts forth low-complexity Riemannian subspace descent algorithms for the minimization of functions over the symmetric positive definite (SPD) manifold. Different from the existing Riemannian gradient descent variants, the proposed approach utilizes carefully chosen subspaces that allow the update to be written as a product of the Cholesky factor of the iterate and a sparse matrix. The resulting updates avoid the costly matrix operations like matrix exponentiation and dense matrix multiplication, which are generally required in almost all other Riemannian optimization algorithms on SPD manifold. We further identify a broad class of functions, arising in diverse applications, such as kernel matrix learning, covariance estimation of Gaussian distributions, maximum likelihood parameter estimation of elliptically contoured distributions, and parameter estimation in Gaussian mixture model problems, over which the Riemannian gradients can be calculated efficiently. The proposed uni-
    
[^53]: 在线指标算法的混合预测

    Mixing predictions for online metric algorithms. (arXiv:2304.01781v1 [cs.LG])

    [http://arxiv.org/abs/2304.01781](http://arxiv.org/abs/2304.01781)

    本文提出了一种在线算法的混合预测方法，针对度量任务系统，我们获得了$O(\ell^2)$的竞争比，可以使算法跟随不同的预测器，对限制切换次数的情况可以获得$(1+\epsilon)$-竞争算法。

    

    在学习-增强的在线算法中，主要技术之一是组合多个算法或预测器。由于每个预测器的性能可能随时间变化，因此希望使用动态组合，根据不同的时间跟随不同的预测器，而不是使用单个最佳预测器作为基准。我们设计了一些组合预测并针对广泛的在线问题类别（即度量任务系统）与这样的动态组合进行竞争。针对最佳（事后）无约束组合的$\ell$个预测器，我们获得了$O(\ell^2)$的竞争比，并证明这是最优的。然而，对于一个约束在不同预测器之间切换次数的基准，我们可以获得$(1+\epsilon)$-竞争算法。此外，我们的算法可以适应类似于赌博机式的访问预测器的方式，每次查询一个预测器。我们其中一条下界的一个意外推论是，出现了新的结构。

    A major technique in learning-augmented online algorithms is combining multiple algorithms or predictors. Since the performance of each predictor may vary over time, it is desirable to use not the single best predictor as a benchmark, but rather a dynamic combination which follows different predictors at different times. We design algorithms that combine predictions and are competitive against such dynamic combinations for a wide class of online problems, namely, metrical task systems. Against the best (in hindsight) unconstrained combination of $\ell$ predictors, we obtain a competitive ratio of $O(\ell^2)$, and show that this is best possible. However, for a benchmark with slightly constrained number of switches between different predictors, we can get a $(1+\epsilon)$-competitive algorithm. Moreover, our algorithms can be adapted to access predictors in a bandit-like fashion, querying only one predictor at a time. An unexpected implication of one of our lower bounds is a new structu
    
[^54]: DeepAccident：V2X自动驾驶运动和事故预测基准数据集

    DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving. (arXiv:2304.01168v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.01168](http://arxiv.org/abs/2304.01168)

    本文提出了一个大规模的 DeepAccident 数据集，其中包含各种真实世界驾驶中发生的事故场景，并提出了一个端到端的运动和事故预测任务，该任务可用于直接评估自动驾驶算法的事故预测能力。

    

    安全是自动驾驶的首要任务。但是，目前没有已发布的数据集可以支持自动驾驶的直接和可解释的安全评估。在本文中，我们提出了 DeepAccident，这是一个通过现实模拟器生成的大规模数据集，包含经常在现实驾驶中发生的各种事故场景。DeepAccident 数据集包含 57k 个带注释帧和 285k 个带注释的样本，这几乎是大规模 nuScenes 数据集的 7 倍，其样本数为 40k。此外，我们基于所提出的数据集提出了一个新任务，即端到端的运动和事故预测，可用于直接评估不同自动驾驶算法的事故预测能力。此外，对于每种场景，我们设置了四辆车和一个基础设施来记录数据，从而为事故场景提供了多种视角，并使 V2X（车辆对一切）感知和预测研究成为可能。

    Safety is the primary priority of autonomous driving. Nevertheless, no published dataset currently supports the direct and explainable safety evaluation for autonomous driving. In this work, we propose DeepAccident, a large-scale dataset generated via a realistic simulator containing diverse accident scenarios that frequently occur in real-world driving. The proposed DeepAccident dataset contains 57K annotated frames and 285K annotated samples, approximately 7 times more than the large-scale nuScenes dataset with 40k annotated samples. In addition, we propose a new task, end-to-end motion and accident prediction, based on the proposed dataset, which can be used to directly evaluate the accident prediction ability for different autonomous driving algorithms. Furthermore, for each scenario, we set four vehicles along with one infrastructure to record data, thus providing diverse viewpoints for accident scenarios and enabling V2X (vehicle-to-everything) research on perception and predicti
    
[^55]: FairGen: 迈向公平图生成

    FairGen: Towards Fair Graph Generation. (arXiv:2303.17743v1 [cs.LG])

    [http://arxiv.org/abs/2303.17743](http://arxiv.org/abs/2303.17743)

    本文提出了一种公平感知图生成模型FairGen，通过标签损失和公平损失来对模型进行训练，并使用雅可比优化方法来实现公平性和逼真性。

    

    在过去的几十年中，人们致力于在各种领域中生成逼真的图形，从社交网络到计算机网络，从基因调控网络到在线交易网络。尽管这些工作取得了显著的成功，但绝大多数工作都没有监督性质，通常是在训练时最小化预期的图形重构损失，这可能导致在生成的图形中出现表示偏差问题，即受保护的群体(通常是少数群体)对目标贡献更少，因此会遭受系统性更高的误差。本文旨在通过利用标签信息和用户首选的平等约束，将图生成调整为下游挖掘任务。特别地，在探究图生成模型中的表示偏差的背景下，我们提出了一个名为FairGen的公平感知图生成模型，以减轻这种偏差。我们的模型联合了标签损失和公平损失，并通过雅可比优化方法进行训练，以得到公平性，同时保持逼真性。

    There have been tremendous efforts over the past decades dedicated to the generation of realistic graphs in a variety of domains, ranging from social networks to computer networks, from gene regulatory networks to online transaction networks. Despite the remarkable success, the vast majority of these works are unsupervised in nature and are typically trained to minimize the expected graph reconstruction loss, which would result in the representation disparity issue in the generated graphs, i.e., the protected groups (often minorities) contribute less to the objective and thus suffer from systematically higher errors. In this paper, we aim to tailor graph generation to downstream mining tasks by leveraging label information and user-preferred parity constraint. In particular, we start from the investigation of representation disparity in the context of graph generative models. To mitigate the disparity, we propose a fairness-aware graph generative model named FairGen. Our model jointly 
    
[^56]: 连续时间功能扩散过程

    Continuous-Time Functional Diffusion Processes. (arXiv:2303.00800v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00800](http://arxiv.org/abs/2303.00800)

    连续时间功能扩散过程引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。通过使用新的数学框架和扩展，FDPs可以在函数空间中构建新型生成模型，在处理连续数据时能够实现高质量的图像生成，所需参数数量比现有模型低几个数量级。

    

    我们引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。 FDPs需要一种新的数学框架来描述前向和反向动力学，并进行多个扩展以得出实际的训练目标。这些扩展包括Girsanov定理的无限维版本，以便能够计算ELBO，以及采样定理的无限维版本，以确保可数个点上的函数评估等价于无限维函数。我们使用FDPs在函数空间中构建了一种新型生成模型，不需要专门的网络架构，并且可以处理任何类型的连续数据。我们在真实数据上的结果显示，使用简单的多层感知机（MLP）结构，FDPs实现了高质量的图像生成，所需的参数数量比现有的扩散模型低几个数量级。

    We introduce Functional Diffusion Processes (FDPs), which generalize score-based diffusion models to infinite-dimensional function spaces. FDPs require a new mathematical framework to describe the forward and backward dynamics, and several extensions to derive practical training objectives. These include infinite-dimensional versions of Girsanov theorem, in order to be able to compute an ELBO, and of the sampling theorem, in order to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. We use FDPs to build a new breed of generative models in function spaces, which do not require specialized network architectures, and that can work with any kind of continuous data. Our results on real data show that FDPs achieve high-quality image generation, using a simple MLP architecture with orders of magnitude fewer parameters than existing diffusion models.
    
[^57]: CrystalBox：基于未来的DRL网络控制器的解释器

    CrystalBox: Future-Based Explanations for DRL Network Controllers. (arXiv:2302.13483v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13483](http://arxiv.org/abs/2302.13483)

    CrystalBox是一种解释DRL网络控制器行为的框架，它能够使用未来关键网络性能指标的影响来生成简明而富有表现力的解释。

    

    解释性不足是限制高效深度强化学习(DRL)控制器实际采用的关键因素。网络解释性强化学习迄今使用引人注目的输入特征来解释控制器的行为。然而，这些基于特征的解决方案不能完全解释控制器的决策过程。通常，运营商有兴趣了解控制器对未来性能的影响，而基于特征的解决方案无法捕捉。在本文中，我们提出了CrystalBox，这是一个框架，通过关键网络性能指标的未来影响来解释控制器的行为。CrystalBox采用一种新颖的基于学习的方法，生成简洁而富有表现力的解释。我们使用DRL网络控制器的奖励组件作为解释的基础，这是运营商有意义的关键绩效指标。CrystalBox是通用的，可以适用于离散和连续控制环境。

    Lack of explainability is a key factor limiting the practical adoption of high-performant Deep Reinforcement Learning (DRL) controllers. Explainable RL for networking hitherto used salient input features to interpret a controller's behavior. However, these feature-based solutions do not completely explain the controller's decision-making process. Often, operators are interested in understanding the impact of a controller's actions on performance in the future, which feature-based solutions cannot capture.  In this paper, we present CrystalBox, a framework that explains a controller's behavior in terms of the future impact on key network performance metrics. CrystalBox employs a novel learning-based approach to generate succinct and expressive explanations. We use reward components of the DRL network controller, which are key performance metrics meaningful to operators, as the basis for explanations. CrystalBox is generalizable and can work across both discrete and continuous control en
    
[^58]: 简化基于动量的黎曼子流形优化

    Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09738](http://arxiv.org/abs/2302.09738)

    本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。

    

    带有动量的黎曼子流形优化在计算上是具有挑战性的，因为确保迭代保持在子流形上通常需要解决困难的微分方程。本文针对具有仿射不变度量的对称正定矩阵的子流形优化算法进行了简化。我们提出了黎曼正常坐标的广义版本，可以将问题动态地简化为欧几里得无约束问题。我们使用我们的方法来解释和简化现有的结构化协方差方法，并为深度学习开发了高效的二阶优化器，而无需显式矩阵求逆。

    Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
    
[^59]: 不平衡数据集下深度主动学习算法选择研究

    Algorithm Selection for Deep Active Learning with Imbalanced Datasets. (arXiv:2302.07317v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07317](http://arxiv.org/abs/2302.07317)

    本论文提出了适用于不平衡数据集下的深度主动学习的自适应算法选择策略TAILOR，它在多类和多标签应用的实验中取得了与候选算法相当或更高的准确性。

    

    标签效率已成为深度学习应用中越来越重要的目标。主动学习旨在减少训练深度网络所需的标记示例数量，但是，在不同的数据集和应用中，主动学习算法的实证性能可能会大幅度变化。事先很难知道哪种主动学习策略在特定应用中表现良好或最佳。为解决这个问题，我们提出了第一个用于深度主动学习的自适应算法选择策略。对于任何未标记的数据集，我们的(meta)算法TAILOR (Thompson ActIve Learning algORithm selection)迭代地并自适应地选择一组候选主动学习算法。TAILOR使用旨在收集类平衡示例的新奖励函数。在多类和多标签应用的大量实验中，TAILOR在实现与候选算法中最佳算法相当或更高的准确性方面表现出良好的效果。

    Label efficiency has become an increasingly important objective in deep learning applications. Active learning aims to reduce the number of labeled examples needed to train deep networks, but the empirical performance of active learning algorithms can vary dramatically across datasets and applications. It is difficult to know in advance which active learning strategy will perform well or best in a given application. To address this, we propose the first adaptive algorithm selection strategy for deep active learning. For any unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning algORithm selection) iteratively and adaptively chooses among a set of candidate active learning algorithms. TAILOR uses novel reward functions aimed at gathering class-balanced examples. Extensive experiments in multi-class and multi-label applications demonstrate TAILOR's effectiveness in achieving accuracy comparable or better than that of the best of the candidate algorithms.
    
[^60]: 直接不确定量化

    Direct Uncertainty Quantification. (arXiv:2302.02420v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02420](http://arxiv.org/abs/2302.02420)

    本文提出一种新的直接不确定量化（DirectUQ）方法，它能在神经网络中直接输出均值和方差，同时结合了传统神经网络和贝叶斯神经网络的优点，有助于改进模型的正则化器和风险边界等方面。

    

    传统神经网络易于训练，但会产生过于自信的预测；而贝叶斯神经网络提供了良好的不确定量化，但优化它们需要耗费时间。本文介绍了一种新的方法——“直接不确定量化”（DirectUQ），它结合了它们的优点，其中神经网络直接输出最后一层的均值和方差。DirectUQ可以导出为一个替代的变分下界，因此从落单变分推理中获益，提供了改进的正则化器。另一方面，像非概率模型一样，DirectUQ具有简单的训练方法，可以使用Rademacher复杂性为模型提供风险边界。实验表明，DirectUQ和DirectUQ集成提供了时间和不确定性量化方面的良好平衡，特别是对于分布之外的数据。

    Traditional neural networks are simple to train but they produce overconfident predictions, while Bayesian neural networks provide good uncertainty quantification but optimizing them is time consuming. This paper introduces a new approach, direct uncertainty quantification (DirectUQ), that combines their advantages where the neural network directly outputs the mean and variance of the last layer. DirectUQ can be derived as an alternative variational lower bound, and hence benefits from collapsed variational inference that provides improved regularizers. On the other hand, like non-probabilistic models, DirectUQ enjoys simple training and one can use Rademacher complexity to provide risk bounds for the model. Experiments show that DirectUQ and ensembles of DirectUQ provide a good tradeoff in terms of run time and uncertainty quantification, especially for out of distribution data.
    
[^61]: 基于语言模型的知识图谱嵌入编辑

    Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10405](http://arxiv.org/abs/2301.10405)

    本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。

    

    近几十年来，使用语言模型进行知识图谱（KG）嵌入已经取得了实证成功。但是，基于语言模型的KG嵌入通常作为静态工件部署，修改起来具有挑战性，需要重新训练。为了解决这个问题，本文提出了一种新的任务，即编辑基于语言模型的KG嵌入。该任务旨在实现对KG嵌入的数据高效和快速更新，而不影响其余部分的性能。我们构建了四个新数据集：E-FB15k237、A-FB15k237、E-WN18RR 和 A-WN18RR，并评估了几种知识编辑基线，证明了之前的模型处理该任务的能力有限。我们进一步提出了一个简单但强大的基线——KGEditor，它利用超网络的附加参数层来编辑/添加事实。全面的实验结果表明，当更新特定事实而不影响其余部分的性能时，KGEditor 的表现更好。

    Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
    
[^62]: ClimaX:一种用于天气和气候的基础模型

    ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10343](http://arxiv.org/abs/2301.10343)

    ClimaX是一种灵活且可推广的深度学习模型，用于天气和气候科学，可以使用不同数据集进行训练。

    

    目前大多数先进的天气和气候模型都是基于物理信息的数值模型。这些方法旨在模拟非线性动力学和多变量之间的复杂相互作用，这些相互作用很难近似。此外，许多这样的数值模型在模拟细粒度空间和时间分辨率的大气现象时计算量很大。最近的基于机器学习的数据驱动方法通过使用深度神经网络学习数据驱动的功能映射来直接解决下游预测或投射任务。然而，这些网络是使用为特定时空任务策划和同质化的气候数据集进行训练的，因此缺乏数值模型的普遍性。我们开发并演示了ClimaX，这是一个灵活且可推广的深度学习模型，可用于天气和气候科学，并可以使用跨越不同数据集的异构数据进行训练。

    Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning dif
    
[^63]: 通过半监督学习和生成对抗网络在不平衡数据集中进行虚假检测

    Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01071](http://arxiv.org/abs/2212.01071)

    本文提出了一种在不平衡数据集中使用半监督学习和生成对抗网络进行虚假检测的方法，实验证明仅使用100个标记样本的情况下，准确率达到了91\%。

    

    随着社交媒体的快速发展，骚扰行为变得更加普遍，这导致了虚假检测成为研究人员中引人注目的领域。数据的图形特性以及大量节点导致了许多障碍，包括矩阵中大量无关特征的高离散度和不平衡类别。为了解决这些问题，本文采用了自编码器和半监督学习与生成对抗网络算法的组合，即SGAN。本文将少量标签应用于SGAN作为分类器。实验结果表明，仅使用100个标记样本，该方法在检测虚假账户方面的准确率达到了91\%。

    As social media grows faster, harassment becomes more prevalent which leads to considered fake detection a fascinating field among researchers. The graph nature of data with the large number of nodes caused different obstacles including a considerable amount of unrelated features in matrices as high dispersion and imbalance classes in the dataset. To deal with these issues Auto-encoders and a combination of semi-supervised learning and the GAN algorithm which is called SGAN were used. This paper is deploying a smaller number of labels and applying SGAN as a classifier. The result of this test showed that the accuracy had reached 91\% in detecting fake accounts using only 100 labeled samples.
    
[^64]: 元元反游戏学习组合学习行为

    Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.08012](http://arxiv.org/abs/2207.08012)

    本论文提出了一种元元反游戏学习的方法来解决组合学习行为的问题，通过解决绑定问题来支持人工智能代理展示组合学习行为的能力。

    

    人类利用组合性从过去的经验中推广到新颖的经验。我们假设我们的经验可以分解为基本的原子组件，这些组件可以以新颖的方式重新组合，以支持我们参与新颖经验的能力。我们将这视为学习以组合方式泛化的能力，并将利用这种能力的行为称为组合学习行为（CLBs）。学习CLBs的一个核心问题是解决绑定问题（BP）。尽管这是人类轻松完成的智能壮举，但对于现有技术的人工智能代理来说并非如此。因此，为了构建能够与人类合作的人工智能代理，我们建议开发一个新的基准来研究代理商通过解决BP的领域无关版本来展示CLBs的能力。我们受到指代游戏的语言涌现和基础架构框架的启发，提出了一个元学习扩展方案

    Human beings use compositionality to generalise from past experiences to novel experiences. We assume a separation of our experiences into fundamental atomic components that can be recombined in novel ways to support our ability to engage with novel experiences. We frame this as the ability to learn to generalise compositionally, and we will refer to behaviours making use of this ability as compositional learning behaviours (CLBs). A central problem to learning CLBs is the resolution of a binding problem (BP). While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents. Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extensio
    
[^65]: 使用基于模型的树和提升方法拟合低阶函数ANOVA模型

    Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models. (arXiv:2207.06950v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.06950](http://arxiv.org/abs/2207.06950)

    本文提出了一种新算法GAMI-Tree，使用基于模型的树以及新的交互过滤方法，可以更好地拟合底层交互，具有更好的预测性能和更高的效率。

    

    低阶函数ANOVA模型已经被机器学习社区重新发现，并称之为内在可解释的机器学习。本文提出了一种新算法GAMI-Tree，类似于EBM，但具有一些趋向更好性能的特性。我们采用模型为基础的树，并融入一种新的交互过滤方法，提高了对底层交互的捕捉。此外，我们的迭代训练方法收敛于具有更好预测性能的模型，并确保相互作用在分层意义上正交于主效应。该算法不需要广泛的调整，并且实现快速高效。我们使用模拟和真实数据集进行比较。

    Low-order functional ANOVA (fANOVA) models have been rediscovered in the machine learning (ML) community under the guise of inherently interpretable machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting functional main effects and second-order interactions. We propose a new algorithm, called GAMI-Tree, that is similar to EBM, but has a number of features that lead to better performance. It uses model-based trees as base learners and incorporates a new interaction filtering method that is better at capturing the underlying interactions. In addition, our iterative training method converges to a model with better predictive performance, and the embedded purification ensures that interactions are hierarchically orthogonal to main effects. The algorithm does not need extensive tuning, and our implementation is fast and efficient. We use simulated and real datasets to compare the performanc
    
[^66]: 用人造黄油从后验样本中去除脂肪

    Removing the fat from your posterior samples with margarine. (arXiv:2205.12841v3 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2205.12841](http://arxiv.org/abs/2205.12841)

    本文总结了一种使用掩蔽自回归流和核密度估计器的方法，可以学习对应于核心科学参数的边际后验密度。该方法在计算边际库尔巴克-勒布勒散度、边际贝叶斯模型维度、似然函数模拟和先验模拟等方面具有广泛应用。

    

    贝叶斯分析已经成为许多宇宙学领域的不可或缺工具，包括引力波研究、宇宙微波背景和宇宙黎明时期的21厘米信号等现象。该方法提供了一种将复杂模型与描述关键宇宙学和天体物理信号以及各种污染信号和仪器效应的'干扰参数'拟合到数据的方式。在本文中，我们总结了一种使用掩蔽自回归流和核密度估计器来学习对应于核心科学参数的边际后验密度的方法。我们发现，边际或“无干扰”的后验分布及其相关的似然函数具有许多应用，包括计算以前难以处理的边际库尔巴克-勒布勒散度和边际贝叶斯模型维度，似然函数模拟和先验模拟。我们使用玩具例子和实际案例分别展示了每个应用。

    Bayesian analysis has become an indispensable tool across many different cosmological fields including the study of gravitational waves, the Cosmic Microwave Background and the 21-cm signal from the Cosmic Dawn among other phenomena. The method provides a way to fit complex models to data describing key cosmological and astrophysical signals and a whole host of contaminating signals and instrumental effects modelled with 'nuisance parameters'. In this paper, we summarise a method that uses Masked Autoregressive Flows and Kernel Density Estimators to learn marginal posterior densities corresponding to core science parameters. We find that the marginal or 'nuisance-free' posteriors and the associated likelihoods have an abundance of applications including; the calculation of previously intractable marginal Kullback-Leibler divergences and marginal Bayesian Model Dimensionalities, likelihood emulation and prior emulation. We demonstrate each application using toy examples, examples from t
    
[^67]: 公正游戏：对强化学习的挑战

    Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.12787](http://arxiv.org/abs/2205.12787)

    AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.

    

    类似AlphaZero的强化学习算法在各种棋盘游戏中表现出色，但在公正游戏中却面临挑战，这些游戏中玩家共享棋子。我们提供了一个具体的游戏例子，即小孩们玩的尼姆游戏，以及其他一些公正游戏，这些游戏似乎成为AlphaZero和类似的强化学习算法的绊脚石。我们的发现与最近的研究一致，表明AlphaZero-style算法容易受到敌对攻击和敌对扰动的影响，显示了在所有合法状态下学习掌握这些游戏的困难。我们发现尼姆游戏在小型棋盘上可以学习，但当棋盘尺寸增大时，AlphaZero-style算法的学习速度显著减慢。直观上，尼姆等公正游戏与象棋和围棋等党派游戏之间的区别在于，如果系统中添加了微小的噪音（例如，棋盘的一小部分被覆盖），对于公正游戏来说，这是一种典型的情况。

    AlphaZero-style reinforcement learning (RL) algorithms excel in various board games but face challenges with impartial games, where players share pieces. We present a concrete example of a game - namely the children's game of nim - and other impartial games that seem to be a stumbling block for AlphaZero-style and similar reinforcement learning algorithms.  Our findings are consistent with recent studies showing that AlphaZero-style algorithms are vulnerable to adversarial attacks and adversarial perturbations, showing the difficulty of learning to master the games in all legal states.  We show that nim can be learned on small boards, but AlphaZero-style algorithms learning dramatically slows down when the board size increases. Intuitively, the difference between impartial games like nim and partisan games like Chess and Go can be explained by the fact that if a tiny amount of noise is added to the system (e.g. if a small part of the board is covered), for impartial games, it is typica
    
[^68]: 深度特征筛选：通过深度神经网络进行超高维数据的特征选择

    Deep Feature Screening: Feature Selection for Ultra High-Dimensional Data via Deep Neural Networks. (arXiv:2204.01682v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.01682](http://arxiv.org/abs/2204.01682)

    本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服高维、低样本数据上的困难和挑战，并对超高维、低样本数据进行高精度的特征筛选。

    This paper proposes a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome the challenges of high-dimensional, low-sample-size data and identify significant features with high precision for ultra high-dimensional, low-sample-size data.

    传统的统计特征选择方法在高维、低样本数据上的应用经常遇到困难和挑战，如过拟合、维数灾难、计算不可行和强模型假设。本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服这些问题，并对超高维、低样本数据进行高精度的特征筛选。该方法首先提取输入数据的低维表示，然后应用基于Deb和Sen（2021）最近开发的多元秩距相关性的特征筛选。该方法结合了深度神经网络和特征筛选的优点，除了处理具有少量样本的超高维数据的能力外，还具有以下吸引人的特点：（1）它是模型自由和分布自由的；（2）它可以

    The applications of traditional statistical feature selection methods to high-dimension, low sample-size data often struggle and encounter challenging problems, such as overfitting, curse of dimensionality, computational infeasibility, and strong model assumption. In this paper, we propose a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome these problems and identify significant features with high precision for ultra high-dimensional, low-sample-size data. This approach first extracts a low-dimensional representation of input data and then applies feature screening based on multivariate rank distance correlation recently developed by Deb and Sen (2021). This approach combines the strengths of both deep neural networks and feature screening, and thereby has the following appealing features in addition to its ability of handling ultra high-dimensional data with small number of samples: (1) it is model free and distribution free; (2) it can be
    
[^69]: FedGCN：联邦训练中图卷积网络的收敛与通信的权衡

    FedGCN: Convergence and Communication Tradeoffs in Federated Training of Graph Convolutional Networks. (arXiv:2201.12433v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12433](http://arxiv.org/abs/2201.12433)

    介绍了一个新算法 FedGCN，使用联邦学习训练 GCN 模型进行半监督节点分类，实现收敛快，通信量小，同时还能够保护本地数据隐私。

    

    近年来，在分布于多个客户端的图上训练模型的方法因其图的规模和数据保留规定的原因而越来越受欢迎。然而，由于连接图节点的跨客户端边缘，单个连接图不能被分别分隔到多个客户端。因此，在单个图上分布式训练模型会导致客户端之间的通信开销巨大或训练中信息的丢失。我们介绍了FedGCN算法，它使用联邦学习来训练用于半监督节点分类的GCN模型，而且能够快速收敛而且通信量较小。与之前需要在每个训练轮次中客户端之间进行通信的方法相比，FedGCN客户端仅在一个预训练步骤中与中央服务器通信，从而极大地减少了通信成本，并允许使用同态加密来保护本地数据的隐私。在四个基准数据集上的实验结果表明，FedGCN相比于最先进的集中式训练方法，能够取得竞争性的表现，同时使用的通信量明显更少。

    Methods for training models on graphs distributed across multiple clients have recently grown in popularity, due to the size of these graphs as well as regulations on keeping data where it is generated. However, a single connected graph cannot be disjointly partitioned onto multiple clients due to the cross-client edges connecting graph nodes. Thus, distributed methods for training a model on a single graph incur either significant communication overhead between clients or a loss of available information to the training. We introduce the Federated Graph Convolutional Network (FedGCN) algorithm, which uses federated learning to train GCN models for semi-supervised node classification with fast convergence and little communication. Compared to prior methods that require communication among clients at each training round, FedGCN clients only communicate with the central server in one pre-training step, greatly reducing communication costs and allowing the use of homomorphic encryption to 
    
[^70]: 静息时人类功能性脑网络的持续同调状态空间估计

    Persistent Homological State-Space Estimation of Functional Human Brain Networks at Rest. (arXiv:2201.00087v3 [math.AT] UPDATED)

    [http://arxiv.org/abs/2201.00087](http://arxiv.org/abs/2201.00087)

    该论文提出了一种新的数据驱动的拓扑数据分析方法，用于估计静息状态下人类脑网络的状态空间。该方法通过惩罚网络之间的拓扑距离将动态变化的脑网络聚类为不同的状态，并通过考虑时间维度来提高准确性。研究还发现脑网络的整体拓扑具有遗传特征。

    

    我们提出了一种新的数据驱动的拓扑数据分析（TDA）方法，用于估计动态变化的人类功能性脑网络的状态空间。我们的方法通过对网络之间的拓扑距离进行惩罚，将动态变化的脑网络聚类为拓扑上不同的状态。我们的方法通过计算网络之间的Wasserstein距离考虑了数据的时间维度。我们的方法在估计脑网络的状态空间方面表现优于广泛使用的k-means聚类方法。该方法被应用于更准确地确定动态变化的功能性脑网络的状态空间。随后，我们采用双生子研究设计探讨了脑网络的整体拓扑是否具有遗传特征。

    We present a new data driven topological data analysis (TDA) approach for estimating state spaces in dynamically changing human functional brain networks of human. Our approach penalizes the topological distance between networks and clusters dynamically changing brain networks into topologically distinct states. Our method takes into account the temporal dimension of the data through the Wasserstein distance between networks. Our method is shown to outperform the widely used k-means clustering often used in estimating the state space in brain networks. The method is applied to more accurately determine the state spaces of dynamically changing functional brain networks. Subsequently, we address the question of whether the overall topology of brain networks is a heritable feature using the twin study design.
    
[^71]: 利用注意机制的高阶张量池化在动作识别中的应用

    High-order Tensor Pooling with Attention for Action Recognition. (arXiv:2110.05216v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2110.05216](http://arxiv.org/abs/2110.05216)

    本论文提出了一种利用注意机制进行高阶张量池化的方法，通过引入特征值幂归一化（EPN）来防止爆发现象并提高动作识别的准确性。

    

    本论文提出了一种利用注意机制进行高阶张量池化的方法，旨在捕捉神经网络生成的特征向量的高阶统计信息，形成张量描述符。张量描述符要求具备鲁棒的相似性度量，以应对聚合向量数量较少和爆发现象，即某些特征出现的频率高于或低于统计预期的情况。我们将图拉普拉斯矩阵上的热扩散过程与协方差/自相关矩阵的特征值幂归一化（EPN）密切相关，其逆形成了一个环状图拉普拉斯矩阵。我们证明了热扩散过程与EPN具有相同的作用，即增强或减弱特征值谱的幅度，从而防止爆发现象。我们将高阶张量配备了EPN，它可以作为高阶出现的谱检测器，以防止爆发现象。我们还证明，对于一个由d维特征描述符构建的阶数为r的张量，这样的检测器可以给出至少存在一个高阶出现的可能性。

    We aim at capturing high-order statistics of feature vectors formed by a neural network, and propose end-to-end second- and higher-order pooling to form a tensor descriptor. Tensor descriptors require a robust similarity measure due to low numbers of aggregated vectors and the burstiness phenomenon, when a given feature appears more/less frequently than statistically expected. The Heat Diffusion Process (HDP) on a graph Laplacian is closely related to the Eigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix, whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN play the same role, i.e., to boost or dampen the magnitude of the eigenspectrum thus preventing the burstiness. We equip higher-order tensors with EPN which acts as a spectral detector of higher-order occurrences to prevent burstiness. We also prove that for a tensor of order r built from d dimensional feature descriptors, such a detector gives the likelihood if at least one high
    

