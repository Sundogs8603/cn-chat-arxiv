# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models](https://rss.arxiv.org/abs/2402.01345) | 本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。 |
| [^2] | [Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees](https://rss.arxiv.org/abs/2402.00899) | 这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。 |
| [^3] | [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249) | HarmBench是一个为自动红队和强大拒绝设计的标准化评估框架，通过对18种红队方法和33个目标LLM和防御的比较，得出了新的见解，并介绍了一种高效的对抗训练方法，提高了LLM在各种攻击下的稳健性。 |
| [^4] | [Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks](https://arxiv.org/abs/2402.04248) | 本研究评估了Mamba在上下文学习任务中的性能，并与Transformer模型进行比较。结果显示，SSMs在标准回归任务中与Transformer性能相当，在稀疏奇偶学习等任务中表现优异，但在涉及非标准检索功能的任务中表现不佳。为了解决这些局限性，引入了一种混合模型。 |
| [^5] | [Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science](https://arxiv.org/abs/2402.04247) | 本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。 |
| [^6] | [CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers](https://arxiv.org/abs/2402.04239) | CAST提出了一种利用替代令牌进行聚类自注意力的高效Transformer机制，通过减少计算复杂度，实现了在整个输入序列上的信息流动，从而提高了效率。 |
| [^7] | [MusicRL: Aligning Music Generation to Human Preferences](https://arxiv.org/abs/2402.04229) | MusicRL是第一个通过人类反馈进行优化的音乐生成系统。它使用强化学习通过离散音频令牌的自回归模型进行微调，以最大化序列级奖励。在部署后，通过从用户收集的配对偏好数据集来进一步优化系统。 |
| [^8] | [Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks](https://arxiv.org/abs/2402.04216) | 通过资源感知的分层联邦学习，我们提出了一种解决方案，可以预测用户未来的内容请求，并减轻无线视频缓存网络中回程流量拥塞的问题。 |
| [^9] | [Variational Shapley Network: A Probabilistic Approach to Self-Explaining Shapley values with Uncertainty Quantification](https://arxiv.org/abs/2402.04211) | 本研究引入了变分Shapley网络，通过概率化的方法简化了计算Shapley值的过程，并解决了估计模型边际值和处理解释可变性的挑战。 |
| [^10] | [Acute kidney injury prediction for non-critical care patients: a retrospective external and internal validation study](https://arxiv.org/abs/2402.04209) | 这项研究开发并比较了深度学习和传统机器学习模型，用于预测非危重病患者在接下来48小时内发展为2期或更高程度的急性肾损伤。内外部验证和亚组分析表明模型的有效性和稳定性。 |
| [^11] | [Gradient Coding in Decentralized Learning for Evading Stragglers](https://arxiv.org/abs/2402.04193) | 本文提出了一种新的基于八卦的梯度编码分散式学习方法（GOCO），以解决存在延迟节点的分散式学习问题。与基准方法相比，该方法在学习性能上具有优越性。 |
| [^12] | [Reinforcement Learning with Ensemble Model Predictive Safety Certification](https://arxiv.org/abs/2402.04182) | 本文提出了集成模型预测安全认证算法，通过将基于模型的深度强化学习与基于管道的模型预测控制相结合，通过规划来纠正学习代理的行动，最小化安全约束违规。结果显示，该方法比其他强化学习方法实现了显著较少的约束违规。 |
| [^13] | [Scaling Laws for Downstream Task Performance of Large Language Models](https://arxiv.org/abs/2402.04177) | 本研究探讨了在转移学习环境中大型语言模型的尺度行为，发现微调数据集的大小和预训练数据与下游数据的分布一致性对下游性能有显著影响。 |
| [^14] | [Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions](https://arxiv.org/abs/2402.04168) | 本论文引入了基于信息的增强学习方法，将结构化的规则手册作为知识源，利用情境感知的奖励设计评估学习轨迹，以实现对需要控制交通规则例外的情景的学习。 |
| [^15] | [Tempered Calculus for ML: Application to Hyperbolic Model Embedding](https://arxiv.org/abs/2402.04163) | 本论文介绍了基于温和微积分的理论和工具，来改进目前在机器学习中使用的数学扭曲方法，特别强调与几何和机器学习相关的特性。 |
| [^16] | [Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains](https://arxiv.org/abs/2402.04161) | 提出了一个新的框架，通过马尔可夫链的视角研究了注意力模型的顺序建模能力，理论上刻画了单层Transformer的损失景观并发现了全局最小值和坏局部最小值的存在。 |
| [^17] | [Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction](https://arxiv.org/abs/2402.04154) | 本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示) |
| [^18] | [Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process](https://arxiv.org/abs/2402.04146) | 这篇论文提出了一种基于潜变量高斯过程的多源数据融合框架，用于解决多个数据源之间质量和全面性差异给系统优化带来的问题。 |
| [^19] | [OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning](https://arxiv.org/abs/2402.04129) | 这项研究提出了一种新的正则化方法，利用虚拟异常值来改善无需回顾的类增量学习过程中不同任务间的类别混淆问题，并且消除了额外的提示查询和组合计算开销。 |
| [^20] | [Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science](https://arxiv.org/abs/2402.04119) | 本研究提出了一种科学语言建模（SLM）的新方法，通过大型语言模型（LLM）来解决分子建模和设计中的挑战。通过多模态基准和实验评估，我们提供了关于模型与数据模态匹配的量化信息，同时也揭示了模型的知识学习偏好。 |
| [^21] | [SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning](https://arxiv.org/abs/2402.04114) | 本文量化了联邦线性随机逼近算法中异质性偏差的影响，并提出SCAFFLSA作为一种改进方法来消除此偏差。在联邦时间差异学习中，该方法能够显著提高算法的复杂性。 |
| [^22] | [Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems](https://arxiv.org/abs/2402.04108) | 本文研究了基于事件描述的机器学习决策支持方法，用于在列车管理系统中自动分配延迟归因代码。研究发现，分层方法比扁平方法表现更好，但仍然不及手动分类的性能。 |
| [^23] | [An Exploration of Clustering Algorithms for Customer Segmentation in the UK Retail Market](https://arxiv.org/abs/2402.04103) | 本文旨在开发一个客户分群模型，通过对基于英国的在线零售数据集的研究，使用RFM框架和最先进的聚类算法，以提高零售市场行业的决策过程。 |
| [^24] | [The Use of a Large Language Model for Cyberbullying Detection](https://arxiv.org/abs/2402.04088) | 这项研究探索了使用大型语言模型（LLMs）如BERT和RoBERTa进行网络欺凌检测，并准备了一个新的数据集（D2）以应对高度不平衡的类别和泛化问题。 |
| [^25] | [A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation](https://arxiv.org/abs/2402.04087) | 本文研究了CLIP训练方法的一个经典算法，称为高斯判别分析（GDA），并将其应用于CLIP的下游分类任务。通过对特征分布的估计，无需训练即可实现分类器，从而提高了CLIP的性能。 |
| [^26] | [Provably learning a multi-head attention layer](https://arxiv.org/abs/2402.04084) | 该论文介绍了可证学习多头注意力层的研究，并给出了该问题的非平凡的上下界限制。 |
| [^27] | [An Optimal House Price Prediction Algorithm: XGBoost](https://arxiv.org/abs/2402.04082) | 房价预测是房地产和抵押贷款等多个领域的基本要求，这篇论文提出了一种优化的房价预测算法XGBoost，通过比较多种机器学习技术的表现，并确定关键因素，结果表明XGBoost是最佳的模型。 |
| [^28] | [Improved Generalization of Weight Space Networks via Augmentations](https://arxiv.org/abs/2402.04081) | 通过扩充权重空间的数据集，采用MixUp方法，我们改进了权重空间网络的泛化能力和性能。 |
| [^29] | [Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning](https://arxiv.org/abs/2402.04080) | 本文介绍了一种熵正则化的扩散策略与Q-集合相结合的离线强化学习方法，该方法通过将一个复杂的动作分布转化为标准高斯分布，然后使用逆时间SDE采样动作，以改善离线数据集的探索能力，并通过学习Q-集合的下信心界实现更强健的策略改进。在D4RL基准任务的大多数任务上达到了最先进的性能。 |
| [^30] | [Retrieve to Explain: Evidence-driven Predictions with Language Models](https://arxiv.org/abs/2402.04068) | 检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。 |
| [^31] | [Link Prediction with Relational Hypergraphs](https://arxiv.org/abs/2402.04062) | 本文提出了两种适用于关系超图的链接预测框架，并通过实证分析验证了这些框架在各种关系超图基准测试上的有效性。 |
| [^32] | [TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments](https://arxiv.org/abs/2402.04061) | TopoNav是一种拓扑导航框架，它通过主动拓扑映射、内部奖励机制和层次化目标优先级的组合来实现在稀疏奖励环境中高效探索。 |
| [^33] | [Deep Learning for Multivariate Time Series Imputation: A Survey](https://arxiv.org/abs/2402.04059) | 本文调查了深度学习在多变量时间序列插补中的应用。通过综述不同的方法以及它们的优点和限制，研究了它们对下游任务性能的改进，并指出了未来研究的开放问题。 |
| [^34] | [More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms](https://arxiv.org/abs/2402.04054) | 通过学习学习算法，实现更灵活的PAC-Bayesian元学习，允许更灵活的任务之间的知识转移，提供新的泛化界限，可适用于分析和设计各种元学习机制，并在实际应用中改善了预测质量。 |
| [^35] | [Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching](https://arxiv.org/abs/2402.04051) | 通过基于排列的权重匹配分析线性模式连接性，我们实验证明了通过权重匹配找到的排列可以改变权重矩阵奇异向量的方向，但不能改变奇异值。这一发现对于理解随机梯度下降的有效性及其在模型合并等领域的应用具有重要意义。 |
| [^36] | [Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models](https://arxiv.org/abs/2402.04050) | 本论文提出了一种协作微调黑盒视觉语言模型的方法，可以在只有输入提示和输出预测的情况下进行微调，提供了一个提示生成模块和一个预测优化模块，并引入了辅助的预测一致性损失进行一致优化。 |
| [^37] | [Generative Modeling of Graphs via Joint Diffusion of Node and Edge Attributes](https://arxiv.org/abs/2402.04046) | 通过联合扩散节点和边属性，我们提出了一个新的图形生成模型，考虑了所有图组件，并通过注意模块和相互依赖的节点、边和邻接信息实现了更好的效果。 |
| [^38] | [PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network](https://arxiv.org/abs/2402.04038) | 本文提出了一种基于PAC-Bayesian框架的方法，来研究对抗鲁棒性泛化界限问题，针对两种流行的图神经网络模型进行了分析，结果发现图上扩散矩阵的谱范数、权重的谱范数和扰动因子对模型的鲁棒泛化界限有重要影响。 |
| [^39] | [On provable privacy vulnerabilities of graph representations](https://arxiv.org/abs/2402.04033) | 研究揭示了图神经模型中的结构性漏洞，通过边重构攻击可以推断出敏感的拓扑信息，并探讨了噪声聚合机制产生的隐私图表示对该攻击的韧性。 |
| [^40] | [Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation](https://arxiv.org/abs/2402.04031) | Polyp-DDPM是一种基于扩散的方法，利用分割掩码生成逼真的胃肠道息肉图像，提升了分割效果，并在图像质量和分割性能方面优于现有方法，为训练提供了高质量、多样化的合成数据集，使得分割模型达到与真实图像相比可比的效果。 |
| [^41] | [Reducing the Cost of Quantum Chemical Data By Backpropagating Through Density Functional Theory](https://arxiv.org/abs/2402.04030) | 使用神经网络以更快的速度近似密度泛函理论（DFT），通过直接训练NN来绕过创建数据集的问题，大大减少了量子化学数据的成本和时间。 |
| [^42] | [Positive concave deep equilibrium models](https://arxiv.org/abs/2402.04029) | 引入了一种新颖的正凹深度均衡（pcDEQ）模型，通过引入非负权重和凹函数的激活函数，确保了模型的存在性和唯一性。 |
| [^43] | [AlbNews: A Corpus of Headlines for Topic Modeling in Albanian](https://arxiv.org/abs/2402.04028) | 本文介绍了一个包含600个主题标签的新闻标题和2600个未标签的阿尔巴尼亚语数据集，该数据集对于低资源语言的自然语言处理研究非常有价值，同时也证实了基本模型的优越性能。 |
| [^44] | [A General Theory for Kernel Packets: from state space model to compactly supported basis](https://arxiv.org/abs/2402.04022) | 该论文提出了一种从状态空间模型到紧支持基的核分组的通用理论，该理论可以用于降低高斯过程的训练和预测时间，并且通过适当的线性组合产生了$m$个紧支持的核分组函数。 |
| [^45] | [Exploring the Effects of Population and Employment Characteristics on Truck Flows: An Analysis of NextGen NHTS Origin-Destination Data](https://arxiv.org/abs/2402.04019) | 本研究通过分析NextGen NHTS出发地-目的地数据集，探究人口和就业特征对卡车流量的影响。这些研究结果对于改善交通规划和投资决策非常关键。 |
| [^46] | [Quantized Approximately Orthogonal Recurrent Neural Networks](https://arxiv.org/abs/2402.04012) | 本文提出了量化近似正交循环神经网络（QORNNs）来解决正交循环神经网络（ORNNs）中参数过多的问题，采用一种后训练量化策略和三种融入正交约束和量化权重的量化感知训练算法，取得了与s相似的结果。 |
| [^47] | [Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously](https://arxiv.org/abs/2402.04010) | 本文提出了一种高效的可用性攻击方法，可以同时针对监督学习和对比学习，通过生成难以察觉的噪音和制造不可学习的示例来防止未经授权使用数据，并实现了监督和对比学习算法的最新最坏情况的不可学习性。 |
| [^48] | [Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning](https://arxiv.org/abs/2402.04005) | 这篇论文提出了一种利用贝叶斯推断的梯度聚合方法，通过引入概率分布来量化梯度维度的不确定性，在多任务学习中获得更好的效果。 |
| [^49] | [Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought](https://arxiv.org/abs/2402.04004) | 本论文研究了链式思维中的噪声对LLM训练数据的影响，并开发了追踪整数框架来生成可定制的噪声执行跟踪。通过评估预训练模型在算法可解任务中的表现，揭示了噪声的类型和强度对任务性能的影响。 |
| [^50] | [Gradient Sketches for Training Data Attribution and Studying the Loss Landscape](https://arxiv.org/abs/2402.03994) | 本论文提出了一种可扩展的渐变草图算法，用于训练数据归因和损失地貌研究。作者在三个应用中展示了该方法的有效性。 |
| [^51] | [Space Group Constrained Crystal Generation](https://arxiv.org/abs/2402.03992) | 本文在晶体生成中考虑了空间群约束，提出了一个更容易实现的等效形式，进一步提出了一个改进的扩散模型DiffCSP++，实验结果表明其有效性。 |
| [^52] | [Neural Rank Collapse: Weight Decay and Small Within-Class Variability Yield Low-Rank Bias](https://arxiv.org/abs/2402.03991) | 神经网络中的权重衰减和小的类内变化与低秩偏差现象有关 |
| [^53] | [Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation](https://arxiv.org/abs/2402.03990) | 通过研究差分隐私随机梯度下降（DP-SGD）中的总梯度方差，我们发现大批次大小有助于减小則采樣引起的方差，从而提高优化效果。 |
| [^54] | [A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets](https://arxiv.org/abs/2402.03985) | 本研究通过对多个合成数据集进行偏差-方差分解，增加了对其理论理解。实验证明多个合成数据集对于高方差的下游预测器特别有益，并提供了一个简单的经验法则用于选择适当的合成数据集数量。 |
| [^55] | [On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions](https://arxiv.org/abs/2402.03982) | 本文研究了在宽松假设下的随机优化中Adam算法的收敛性。我们引入了一个全面的噪声模型，并证明了在这个模型下，Adam算法可以以较高的概率高效地寻找到一个稳定点。与其他随机一阶算法相比，Adam算法具有更好的自适应性能，无需调整步长和问题参数。 |
| [^56] | [Cross Entropy versus Label Smoothing: A Neural Collapse Perspective](https://arxiv.org/abs/2402.03979) | 本研究从神经崩溃的视角研究了标签平滑，并发现模型在标签平滑训练下更快地收敛到神经崩溃解，并达到更强的神经崩溃水平。此外，标签平滑损失下的模型在相同的NC1水平下表现出加强的NC2，并可在理论上更快地收敛。 |
| [^57] | [Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given Enough Time](https://arxiv.org/abs/2402.03973) | 人类在识别不寻常姿势中的物体上表现优于深度网络，当给予足够时间时。然而，随着图像曝光时间的限制，人类的表现降至深度网络的水平，这暗示人类在识别不寻常姿势中的物体时需要额外的心理过程。此外，人类与网络之间的错误模式也存在不同。因此，我们需要进一步研究，以提高计算机视觉系统的鲁棒性水平。 |
| [^58] | [Tabular Data: Is Attention All You Need?](https://arxiv.org/abs/2402.03970) | 本文引入了一项大规模实证研究，比较了神经网络和梯度提升决策树在表格数据上的表现，还比较了基于Transformer的架构和传统的多层感知器（MLP）与残差连接的架构。实证结果显示，神经网络在决策树方面具有竞争力，而基于Transformer的架构在表格数据集上并没有超过传统MLP架构的简化变体。 |
| [^59] | [In-context learning agents are asymmetric belief updaters](https://arxiv.org/abs/2402.03969) | 本研究发现大型语言模型以不对称方式更新其信念，更多地从比预期好的结果中学到。同时，我们还发现学习关于反事实反馈时，这种效应会逆转。这些结果有助于我们理解上下文学习的工作方式，并揭示了问题框架如何影响学习过程。 |
| [^60] | [On dimensionality of feature vectors in MPNNs](https://arxiv.org/abs/2402.03966) | 这篇论文重新考察了消息传递图神经网络（MPNN）特征向量的维度问题，发现实际使用的架构与理论保证存在差距。 |
| [^61] | [Discovery of the Hidden World with Large Language Models](https://arxiv.org/abs/2402.03941) | 通过使用大型语言模型，我们提出了COAT：因果表示助手，该助手从原始观测数据中提取潜在的因果因子，并将其转化为结构化数据，为探索隐藏世界提供了新的机会。 |
| [^62] | [Fully autonomous tuning of a spin qubit](https://arxiv.org/abs/2402.03931) | 本研究首次实现了半导体量子比特的全自动调谐，通过集成深度学习、贝叶斯优化和计算机视觉技术，在无需人工干预的情况下，成功地实现了从接地器件到拉比振荡的完全自主操作。 |
| [^63] | [Return-Aligned Decision Transformer](https://arxiv.org/abs/2402.03923) | 本研究提出了返回对齐的决策Transformer（RADT），通过分离回报与传统输入序列，实现有效地将实际回报与目标回报对齐。 |
| [^64] | [Large Language Models to Enhance Bayesian Optimization](https://arxiv.org/abs/2402.03921) | 通过结合大型语言模型（LLM）的能力，我们提出了一种名为LLAMBO的新方法，将其应用于贝叶斯优化（BO）。通过用自然语言描述BO问题，并利用LLM的上下文理解、少样本学习能力和领域知识，LLAMBO能够提供有前景的解决方案，并且在零样本热启动方面表现出良好的效果。 |
| [^65] | [Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning](https://arxiv.org/abs/2402.03917) | 这篇论文解决了冷启动场景的无示例增量学习的问题，提出了一种弹性特征整合的方法，通过规范特征漂移并利用原型来减少任务新鲜度偏差。 |
| [^66] | [Learning Metrics that Maximise Power for Accelerated A/B-Tests](https://arxiv.org/abs/2402.03915) | 本论文提出了一种新方法，通过从短期信号中学习指标，直接最大化指标与北极度量标准之间的统计能力，从而减少在线控制实验的成本。 |
| [^67] | [Employee Turnover Analysis Using Machine Learning Algorithms](https://arxiv.org/abs/2402.03905) | 本文使用机器学习算法分析员工流失情况，并采用AdaBoost，SVM和RandomForest三种算法进行员工离职预测准确性的评估。 |
| [^68] | [Compound Returns Reduce Variance in Reinforcement Learning](https://arxiv.org/abs/2402.03903) | 复合回报是一种新的强化学习方法，在降低方差和提高样本效率方面具有重要的贡献和创新。 |
| [^69] | [A phase transition between positional and semantic learning in a solvable model of dot-product attention](https://arxiv.org/abs/2402.03902) | 这篇论文研究了点积注意力层如何同时学习位置和语义关注，发现在高维数据和大量训练样本条件下，存在从位置机制到语义机制的相变，并提供了非凸经验损失函数全局最小值的闭式表征。 |
| [^70] | [Batch Universal Prediction](https://arxiv.org/abs/2402.03901) | 该论文研究了大型语言模型（LLM）在通用预测方面的性能评估，引入了批量遗憾的概念，并研究了在无记忆源和一阶马尔可夫源的情况下的渐近值。 |
| [^71] | [DistiLLM: Towards Streamlined Distillation for Large Language Models](https://arxiv.org/abs/2402.03898) | DistiLLM是一个更有效和高效的自回归语言模型蒸馏框架，通过引入新颖的偏斜Kullback-Leibler散度损失和自适应的离策略方法，解决了当前针对大语言模型的知识蒸馏方法缺乏标准化目标函数和计算成本过高的问题。 |
| [^72] | [Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency](https://arxiv.org/abs/2402.03893) | 在自动驾驶中，我们通过研究不同预测时域对性能的影响，提出了根据其特定需求来确定最佳预测时域的框架。 |
| [^73] | [MOMENT: A Family of Open Time-series Foundation Models](https://arxiv.org/abs/2402.03885) | MOMENT是一个开放的时间序列基础模型家族，通过解决时间序列数据的挑战，编制了一个大规模的公共时间序列数据集，并设计了一个基准测试来评估有限监督场景下模型的性能。 |
| [^74] | [A Framework for Bilevel Optimization on Riemannian Manifolds](https://arxiv.org/abs/2402.03883) | 本论文提出了一个在黎曼流形上解决约束双层优化问题的框架，并提供了多种超梯度估计策略，并对其进行了研究。该框架不仅适用于确定性双层优化问题，还适用于随机双层优化问题，并且可以使用一般的回退。在各种应用中，该框架都具有很高的实用性。 |
| [^75] | [Geometric quantum machine learning of BQP$^A$ protocols and latent graph classifiers](https://arxiv.org/abs/2402.03871) | 几何量子机器学习可以用于构建具有经典模拟器无法达到的指数差异的协议，并且我们通过学习Simon算法发现了一个例子，这是BQP$^A\neq$BPP协议的示例。 |
| [^76] | [The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks](https://arxiv.org/abs/2402.03864) | 本文研究了针对物理引导神经网络（PINNs）在非线性偏微分方程（PDEs）求解中的挑战。通过理论分析和数值实验，我们说明了NTK在不同线性微分算子下的行为，并强调了采用二阶方法训练PINNs的优势。我们还探讨了二阶方法的收敛能力，并解决了谱偏差和收敛缓慢的问题。通过大量的数值实验和基准测试用例，我们验证了我们的方法的有效性。 |
| [^77] | [Position Paper: Toward New Frameworks for Studying Model Representations](https://arxiv.org/abs/2402.03855) | 通过逆向工程AI模型的确切算法，机制解释性（MI）旨在理解模型。然而，目前的研究主要关注微不足道的行为和能力，而忽视了隐藏在网络内部的表示。因此，我们呼吁研究界朝着新的框架努力，研究这些表示。 |
| [^78] | [Efficient Generation of Hidden Outliers for Improved Outlier Detection](https://arxiv.org/abs/2402.03846) | 提出了一种新的异常点生成方法BISECT，具有更好的效率和效果，可以创建具有真实行为的异常点。使用BISECT生成的合成异常点可有效增强多种数据集的异常检测，例如，在与基线比较时，使用BISECT进行过采样将错误率降低了最多3倍。 |
| [^79] | [On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models](https://arxiv.org/abs/2402.03845) | 这项研究涉及到扩散模型中的规范自由度、保守性和内在维度估计。现有研究在实际应用中常将向量场实现为神经网络函数，而不是限制为能量函数的梯度，导致关于此约束是否提高性能的研究结果矛盾。 |
| [^80] | [Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels](https://arxiv.org/abs/2402.03838) | 本研究提出了一种带有切片Wasserstein Weisfeiler-Lehman图核的高斯过程回归方法，在处理大规模稀疏图形数据集时具有正定性和显著的复杂度降低。 |
| [^81] | [Estimating Barycenters of Distributions with Neural Optimal Transport](https://arxiv.org/abs/2402.03828) | 本研究借助最优传输的对偶形式，提出了一种新的可扩展方法，用于估计分布的重心。通过神经最优传输求解器，我们实现了双层对抗学习并适用于一般成本函数。这个方法的关键优势在于不需要使用三层优化，并且适用于多种成本函数。我们还建立了理论误差界，并在示例场景和图像数据上展示了方法的应用性和有效性。 |
| [^82] | [RevOrder: A Novel Method for Enhanced Arithmetic in Language Models](https://arxiv.org/abs/2402.03822) | 本文提出了一种名为RevOrder的新方法，通过翻转加法、减法和nD乘以1D的输出数字，显著改善了语言模型中的算术运算。经过全面测试，RevOrder在基本算术运算中达到了完美准确度，并在除法任务中提升了语言模型性能，特别是在处理大数时。在GSM8K数学任务中应用RevOrder进行微调，有效降低了错误率并提高了总体得分。 |
| [^83] | [Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies](https://arxiv.org/abs/2402.03819) | SMOTE是一种处理不平衡数据集的常用重新平衡策略，它通过复制原始少数样本来重新生成原始分布。本研究证明了SMOTE的密度在少数样本分布的边界附近逐渐减小，从而验证了BorderLine SMOTE策略的合理性。此外，研究还提出了两种新的SMOTE相关策略，并与其他重新平衡方法进行了比较。最终发现，在数据集极度不平衡的情况下，SMOTE、提出的方法或欠采样程序是最佳的策略。 |
| [^84] | [Asymptotic generalization error of a single-layer graph convolutional network](https://arxiv.org/abs/2402.03818) | 本研究针对单层图卷积网络（GCN）在高维极限下的性能进行了预测，并推广了对多种数据模型的分析。我们的研究显示，尽管GCN在收敛速度上是一致的，但在任何情况下都不能达到贝叶斯最优率。 |
| [^85] | [Expediting In-Network Federated Learning by Voting-Based Consensus Model Compression](https://arxiv.org/abs/2402.03815) | 本论文提出了一种基于投票一致性模型压缩的促进网络内联合学习的算法，通过客户端投票和模型聚合来减少内存空间和通信流量。 |
| [^86] | [Masked Graph Autoencoder with Non-discrete Bandwidths](https://arxiv.org/abs/2402.03814) | 本文通过引入非离散边蒙面和逐层带宽预测目标，提出了一种信息丰富且有效的拓扑蒙面图自编码器，弥补了现有方法在学习基于图神经网络的拓扑信息表示方面的不足。 |
| [^87] | [NK Hybrid Genetic Algorithm for Clustering](https://arxiv.org/abs/2402.03813) | 本文介绍了一种用于聚类的NK混合遗传算法，利用NK聚类验证准则2（NKCV2）来评估解决方案并识别基于密度的区域。算法利用决策变量之间的关系进行灰盒优化，并使用变异算子、分区交叉和局部搜索策略进行优化。该算法具有较低的计算复杂度，并能有效检测聚类区域。 |
| [^88] | [SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal Denoising](https://arxiv.org/abs/2402.03808) | SDEMG是一种基于得分的扩散模型，用于表面肌电信号去噪。实验证明，SDEMG胜过了其他比较方法。 |
| [^89] | [SEABO: A Simple Search-Based Method for Offline Imitation Learning](https://arxiv.org/abs/2402.03807) | SEABO是一种简单而有效的基于搜索的离线模仿学习方法，它以无监督学习的方式，根据专家数据和无标签数据得到奖励函数，实验结果表明其性能与离线强化学习相当。 |
| [^90] | [Explainable Automated Machine Learning for Credit Decisions: Enhancing Human Artificial Intelligence Collaboration in Financial Engineering](https://arxiv.org/abs/2402.03806) | 本文介绍了可解释的自动化机器学习（AutoML）在信贷决策中的应用，结合了可解释的人工智能（XAI）方法，特别是SHapley Additive exPlanations（SHAP）。它提高了信贷决策的效率和准确性，同时增进了人类和人工智能系统之间的信任和合作。 |
| [^91] | [ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804) | 通过动态跳过非活跃神经元的计算，我们提出了一种稀疏LLM的高效激活函数ReLU^2，它在稀疏性与性能、稀疏性的预测性和硬件亲和性等方面表现出色。 |
| [^92] | [Face Detection: Present State and Research Directions](https://arxiv.org/abs/2402.03796) | 本文对人脸检测领域的现状进行了综述，指出了其存在的问题，并提供了研究方向，以提高人脸检测的准确性和速度。 |
| [^93] | [No-Regret Reinforcement Learning in Smooth MDPs](https://arxiv.org/abs/2402.03792) | 本论文针对具有连续状态和/或动作空间的强化学习问题提出了一种无悔保证的新方法，即通过在Legendre多项式基础上构建MDP表示来解决$\nu-$平滑 MDPs，在较弱的假设下可以达到无悔特性，同时在多项式时间内运行。 |
| [^94] | [Weakly Supervised Anomaly Detection via Knowledge-Data Alignment](https://arxiv.org/abs/2402.03785) | 本文提出了一个通过知识-数据对齐的框架KDAlign，用于弱监督异常检测。这个框架将人工专家总结的规则知识与有限的带标签数据集成，通过对齐知识和数据来提高模型性能。 |
| [^95] | [AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction](https://arxiv.org/abs/2402.03784) | AirPhyNet是一种利用物理引导的神经网络方法，通过将空气颗粒运动的物理原理和神经网络结构相结合，提高了对空气质量的预测精度和可解释性。 |
| [^96] | [MolTC: Towards Molecular Relational Modeling In Language Models](https://arxiv.org/abs/2402.03781) | 本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。 |
| [^97] | [Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification](https://arxiv.org/abs/2402.03780) | 本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。 |
| [^98] | [EERO: Early Exit with Reject Option for Efficient Classification with limited budget](https://arxiv.org/abs/2402.03779) | EERO 是一种早期退出与拒绝选项的新方法，通过使用多个分类器来选择每个实例的退出头，以实现高效分类。实验结果表明，它可以有效管理预算分配并提高准确性。 |
| [^99] | [Learning a Decision Tree Algorithm with Transformers](https://arxiv.org/abs/2402.03774) | 该论文介绍了MetaTree模型，它使用经典算法的输出训练基于Transformer的模型，以产生具有强大概括性能的决策树。 |
| [^100] | [Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution](https://arxiv.org/abs/2402.03771) | 该论文提出了一种名为强化学习来自打包奖励的新问题，其中学习器只能获取序列的打包奖励，在这种情况下探索未知的即时奖励是一项困难的任务，并引入了基于Transformer的方法来解决这个问题。 |
| [^101] | [Fed-CVLC: Compressing Federated Learning Communications with Variable-Length Codes](https://arxiv.org/abs/2402.03770) | 通过分析和实验证明，在联邦学习中使用可变长度编码可以有效压缩通信。本文提出了Fed-CVLC，可以根据模型更新的动态进行代码长度微调。 |
| [^102] | [Deep Learning-Based Correction and Unmixing of Hyperspectral Images for Brain Tumor Surgery](https://arxiv.org/abs/2402.03761) | 本研究使用基于深度学习的方法提出了两种模型，用于校正和分离高光谱图像中的荧光物质。这些模型能够考虑非线性效应，产生更准确的丰度估计，为脑肿瘤手术提供改进方法。 |
| [^103] | [The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs](https://arxiv.org/abs/2402.03757) | 本论文研究发现，虚假图像会导致多模态大型语言模型产生幻觉，作者提出了评估幻觉程度的基准CorrelationQA，并发现主流多模态大型语言模型普遍受到这种本能偏见的影响。 |
| [^104] | [Enhanced sampling of robust molecular datasets with uncertainty-based collective variables](https://arxiv.org/abs/2402.03753) | 本研究提出了一种基于不确定性的集体变量方法，通过引导采集化学相关数据点、关注模型预测最不确定的区域，进行了稳健分子数据集的增强采样。 |
| [^105] | [Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images](https://arxiv.org/abs/2402.03752) | 本研究展示了轻量级视觉Transformer（ViT）在小数据集上使用最小缩放图像进行预训练，通过掩码自编码器技术实现优秀性能，无需显著增加图像尺寸。该方法不仅高效处理小数据集，还能有效处理接近原始尺寸的图像。 |
| [^106] | [Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach](https://arxiv.org/abs/2402.03750) | 本论文提出了一种数字孪生移动性建模方法，通过构建对齐图和设计扩张对齐卷积网络来捕捉交通场景中复杂的时空特征，为智能交通系统的开发提供了有效的方法。 |
| [^107] | [An invariance constrained deep learning network for PDE discovery](https://arxiv.org/abs/2402.03747) | 本研究提出了一种约束不变性的深度学习网络（ICNet）用于PDE的发现，通过过滤掉不能满足Galilean变换要求的候选项，嵌入固定和可能的项到神经网络的损失函数中，显著抵消了稀疏高噪声数据的影响并过滤掉冗余项。 |
| [^108] | [SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2402.03741) | 该研究首次揭示了攻击者在多智能体竞争环境中即使受限于受害者的部分观测也能生成对抗策略的能力。 |
| [^109] | [BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning](https://arxiv.org/abs/2402.03740) | 提出了一种名为BotSSCL的框架，利用自监督对比学习来提高社交机器人检测的性能，从而解决了当前模型在检测复杂机器人和依赖简易特征上的局限性问题。 |
| [^110] | [Differentially Private High Dimensional Bandits](https://arxiv.org/abs/2402.03737) | 这篇论文提出了PrivateLASSO算法，用于解决高维随机上下文线性赌臂问题，并在差分隐私的约束下证明了其隐私和实用性保证。 |
| [^111] | [An Effective Branch-and-Bound Algorithm with New Bounding Methods for the Maximum $s$-Bundle Problem](https://arxiv.org/abs/2402.03736) | 本文介绍了一种新颖的基于分割的上界(PUB)和利用短随机行走生成更大初始解的分支定界算法，用于解决最大s-束问题(MBP)。 |
| [^112] | [Deep Outdated Fact Detection in Knowledge Graphs](https://arxiv.org/abs/2402.03732) | 本文介绍了一种名为DEAN的新型深度学习框架，用于在知识图谱中识别过时事实。DEAN通过全面建模实体和关系之间的隐含结构信息，采用了一种基于对比的方法来揭示潜在的过时信息。实验证明DEAN相对于最先进的基准方法具有显著的优势。 |
| [^113] | [Consistent Joint Decision-Making with Heterogeneous Learning Models](https://arxiv.org/abs/2402.03728) | 本文提出了一种新颖的决策框架，通过整合不同模型的预测和外部知识，实现了决策的一致性。经过实证研究，我们的方法在多个数据集上表现出优越性能。 |
| [^114] | [Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes](https://arxiv.org/abs/2402.03726) | 本论文提出了一种名为ISAHP的深度学习框架，可以从异步、相互依赖的多类型事件序列中无监督地学习实例级的格兰杰因果关系。它是第一个满足格兰杰因果关系要求的神经点过程模型，并利用变压器的自我注意机制来实现格兰杰因果关系的推断。 |
| [^115] | [Statistical Test for Anomaly Detections by Variational Auto-Encoders](https://arxiv.org/abs/2402.03724) | 本研究提出了一种利用变分自动编码器进行异常检测的统计测试方法（VAE-AD测试），通过量化异常区域的可靠性，可以控制误检的概率到所期望的水平。 |
| [^116] | [Similarity-based Neighbor Selection for Graph LLMs](https://arxiv.org/abs/2402.03720) | 基于相似性的邻居选择（SNS）通过改善所选邻居的质量，改善了图形表示，并提高了泛化性能和可扩展性，解决了处理文本属性图（TAGs）的挑战。 |
| [^117] | [Clarify: Improving Model Robustness With Natural Language Corrections](https://arxiv.org/abs/2402.03715) | 论文提出了Clarify，一种通过自然语言纠正模型错误概念的方法，该方法通过用户提供简短的文本描述来纠正模型的一致失败模式，从而提高模型的鲁棒性。 |
| [^118] | [Advancing Location-Invariant and Device-Agnostic Motion Activity Recognition on Wearable Devices](https://arxiv.org/abs/2402.03714) | 本文通过对跨传感器位置的动作模型的通用性进行全面评估，确定了用于构建位置无关模型的关键身体部位，并提出了在设备上部署的动作模型，单个模型的帧级别F1得分达到91.41％，无论传感器放置如何。 |
| [^119] | [Improving and Unifying Discrete&Continuous-time Discrete Denoising Diffusion](https://arxiv.org/abs/2402.03701) | 本文提出了一种改进和统一离散和连续时间离散去噪扩散的方法。通过数学简化和推导，使得离散扩散的训练更准确易优化，并且实现了精确和加速的采样。同时，成功地统一了离散时间和连续时间离散扩散。 |
| [^120] | [Estimating the Local Learning Coefficient at Scale](https://arxiv.org/abs/2402.03698) | 本文提出了一种方法，可以在深度线性网络中准确地测量高达1亿参数的局部学习系数(LLC)，并证明了估计得到的LLC具有重缩放不变性。 |
| [^121] | [A Survey of Privacy Threats and Defense in Vertical Federated Learning: From Model Life Cycle Perspective](https://arxiv.org/abs/2402.03688) | 本文综述了垂直联邦学习中隐私威胁和防御的最新研究进展，通过对模型生命周期的视角进行讨论，提供了行业和实践者在保护数据隐私方面的指导和见解。 |
| [^122] | [Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation](https://arxiv.org/abs/2402.03687) | PARD是一种将扩散模型与自回归方法相结合的置换不变性自回归扩散模型，通过使用图中的部分顺序以块逐块的自回归方式生成图。 |
| [^123] | [RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback](https://arxiv.org/abs/2402.03681) | RL-VLM-F是一种通过视觉语言基础模型反馈的强化学习方法，能够自动生成有效的奖励函数和策略，从而解决了传统强化学习中奖励设计的挑战。 |
| [^124] | [Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents](https://arxiv.org/abs/2402.03678) | 本文提出了一种逻辑规范引导下的动态任务采样（LSTS）方法，通过学习一组强化学习策略，根据高级任务规范指导智能体在最小化环境交互次数的同时实现从初始状态到目标状态的引导。在网格世界实验中，LSTS实现了改进的时间到阈值。 |
| [^125] | [Effective Protein-Protein Interaction Exploration with PPIretrieval](https://arxiv.org/abs/2402.03675) | PPIretrieval是第一个基于深度学习的模型，可以在嵌入空间中有效搜索潜在PPIs，并捕捉蛋白质表面的丰富几何和化学信息。 |
| [^126] | [Efficient Solvers for Partial Gromov-Wasserstein](https://arxiv.org/abs/2402.03664) | 本文提出了两个基于Frank-Wolfe算法的新的高效求解器来解决偏差Gromov-Wasserstein问题，并且证明了PGW问题构成了度量测度空间的度量。 |
| [^127] | [Symbol Correctness in Deep Neural Networks Containing Symbolic Layers](https://arxiv.org/abs/2402.03663) | 本文介绍了神经符号深度神经网络（NS-DNNs）中的符号正确性原则，即用于推理的神经层对中间符号的预测必须与输入数据的符号表示相匹配。符号正确性是NS-DNN可解释性和迁移学习的必要特性，并为推理和交流模型行为提供了精确的方法。 |
| [^128] | [Transductive Reward Inference on Graph](https://arxiv.org/abs/2402.03661) | 该研究提出了一种在图上进行传递式奖励推断的方法，可以有效地估计离线强化学习中未标记数据的奖励。通过利用有限的人工奖励注释和可用数据构建奖励传播图，并利用图进行奖励推断，从而推断出未标记数据的奖励。 |
| [^129] | [Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm](https://arxiv.org/abs/2402.03660) | 本文发现了在预训练-微调范式中，使用相同预训练检查点初始化并在不同任务上进行微调的模型会出现一个有趣的线性现象，称为跨任务线性。我们提供了实证证据并推测神经网络在这一范式中本质上类似线性映射，从参数空间到特征空间的映射。这一发现揭示了关于模型合并/编辑和参数共享等方面的新见解。 |
| [^130] | [Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models](https://arxiv.org/abs/2402.03659) | 这个论文介绍了使用大型语言模型生成可解释的股票预测的方法，并提出了Summarize-Explain-Predict（SEP）模型来解决股票预测中的解释问题和数据标注成本的挑战。 |
| [^131] | [Operator SVD with Neural Networks via Nested Low-Rank Approximation](https://arxiv.org/abs/2402.03655) | 本文提出了一个新的优化框架，使用嵌套的低秩近似方法通过神经网络实现运算符的奇异值分解。该方法通过无约束优化公式隐式高效地保持学习函数的正交性。 |
| [^132] | [Temporal Graph Analysis with TGX](https://arxiv.org/abs/2402.03651) | TGX是一个专为分析时间网络而设计的Python软件包，提供了自动化的数据加载、数据处理和演化图分析。它支持多个内置数据集和外部数据集的访问，并提供数据处理和网络分析功能，使得处理和研究时间网络变得更加便捷。 |
| [^133] | [Multilinear Kernel Regression and Imputation via Manifold Learning](https://arxiv.org/abs/2402.03648) | 本论文介绍了一种基于多线性核回归和流形学习的非参数数据插补方法，可以用于处理缺失数据，并且在建模和计算方面具有鲁棒性和高效性。 |
| [^134] | [CAMBranch: Contrastive Learning with Augmented MILPs for Branching](https://arxiv.org/abs/2402.03647) | CAMBranch是一个利用对比学习和增强MILP的机器学习框架，用于改进混合整数线性规划的分支策略。通过生成增强MILP并应用对比学习，CAMBranch能够获取大量标记的专家样本，从而提高分支决策的质量。 |
| [^135] | [Lens: A Foundation Model for Network Traffic](https://arxiv.org/abs/2402.03646) | "Lens"是一个基于T5架构的基础网络流量模型，通过学习大规模无标签数据的预训练表示，能够在流量理解和生成任务中取得精确的预测和生成。 |
| [^136] | [Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation](https://arxiv.org/abs/2402.03642) | Stanceosaurus 2.0扩展了原始框架，新增对俄罗斯和西班牙的分类，旨在支持分析跨文化和跨语言的虚假信息。通过与初始研究的结果相当的零-shot跨语言迁移，验证了数据的价值和立场分类的可行性。 |
| [^137] | [Disparate Impact on Group Accuracy of Linearization for Private Inference](https://arxiv.org/abs/2402.03629) | 本文研究了线性化对隐私推断中群体准确性的影响，发现减少ReLU激活函数数量会不成比例地降低少数群体的准确性，而对于多数群体则几乎没有影响。采用简单的微调步骤可以解决这个问题。 |
| [^138] | [Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time](https://arxiv.org/abs/2402.03625) | 本文研究了两层ReLU网络在加权衰减正则化下及其凸松弛之间的最优性差距，证明了当训练数据是随机的时候，相对最优性差距可以被一个$O(\sqrt{\log n})$的因子界限。此外，在温和的假设下，局部梯度方法几乎肯定会收敛到训练损失较低的点。 |
| [^139] | [Neural Network Approximators for Marginal MAP in Probabilistic Circuits](https://arxiv.org/abs/2402.03621) | 本文提出了一种使用神经网络近似概率电路中边际MAP推理的方法，该方法通过使用连续多线性函数来估计查询变量的赋值成本并将其作为损失函数，具有自我监督和高效性的优点。 |
| [^140] | [Bayesian Factorised Granger-Causal Graphs For Multivariate Time-series Data](https://arxiv.org/abs/2402.03614) | 本研究提出了一种新的贝叶斯VAR模型，利用分层图先验推断二元格兰杰因果图的后验概率。相比竞争方法，我们的方法在不确定性量化、超参数数量和稀疏多变量时间序列数据上都表现更好。 |
| [^141] | [RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents](https://arxiv.org/abs/2402.03610) | 本研究提出了一种称为RAP的框架，它能够以动态方式利用过去的经验来增强代理的规划能力，并在纯文本和多模态环境中表现出色。实验结果显示RAP在文本场景中达到了最先进水平，并显著提高了多模态LLM代理在具身任务中的性能。 |
| [^142] | [Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models](https://arxiv.org/abs/2402.03597) | 本研究评估了一种大型语言模型GPT-4在识别避孕药切换原因上的能力，结果表明GPT-4可以准确地从临床记录中提取避孕药切换的原因，相较于基准BERT模型有更好的表现。 |
| [^143] | [Assessing the Impact of Distribution Shift on Reinforcement Learning Performance](https://arxiv.org/abs/2402.03590) | 评估强化学习性能时需要考虑分布转变，我们提出了一套评估方法，并推荐使用时间序列分析进行观测RL评估。 |
| [^144] | [A Reinforcement Learning Approach for Dynamic Rebalancing in Bike-Sharing System](https://arxiv.org/abs/2402.03589) | 本研究介绍了一种针对自行车共享系统中动态再平衡问题的时空强化学习算法，通过多智能体马尔可夫决策过程实现独立和协作的车辆再平衡，解决了传统数学优化方法的不实际限制。 |
| [^145] | [Continual Domain Adversarial Adaptation via Double-Head Discriminators](https://arxiv.org/abs/2402.03588) | 本文提出了一种通过双头判别器进行连续领域对抗适应的方法，在源学习阶段引入了一个仅在源域训练的源域判别器，减少了对抗损失的经验估计误差，实验结果表明算法实现了超过2%的准确提升。 |
| [^146] | [Effective Acquisition Functions for Active Correlation Clustering](https://arxiv.org/abs/2402.03587) | 本文提出了三种有效的获取函数用于主动相关聚类，分别基于不一致性概念和信息论量。 |
| [^147] | [MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models](https://arxiv.org/abs/2402.03583) | 研究者发现知识图谱嵌入模型存在的“Z-悖论”限制了其表达能力，并提出了一种名为MQuinE的新模型，通过理论证明，MQuinE成功解决了Z-悖论，并在链接预测任务中显著优于现有模型。 |
| [^148] | [Deconstructing the Goldilocks Zone of Neural Network Initialization](https://arxiv.org/abs/2402.03579) | 我们对神经网络的初始化进行了全面的分析，发现损失海森矩阵的高正曲率与可训练性强的初始点相关。与先前的观念相反，正曲率并不仅仅与初始化范数相关，而与模型置信度、初始损失较低以及一种以前未知的损失梯度消失相关。 |
| [^149] | [Revisiting the Dataset Bias Problem from a Statistical Perspective](https://arxiv.org/abs/2402.03577) | 本文从统计学的角度重新审视了“数据集偏差”问题，发现其主要原因是输入的类属性和非类属性之间的强相关性。通过在训练过程中考虑这种相关性，我们提出了一种缓解数据集偏差的方法，通过对每个样本的目标加权或以权重比例采样来实现。这种方法在实践中更加稳定和有效，并且与因果推理有一定的关联。 |
| [^150] | [Generalization Properties of Adversarial Training for $\ell_0$-Bounded Adversarial Attacks](https://arxiv.org/abs/2402.03576) | 本文主要通过研究$\ell_0$-有界对抗攻击，证明了一个分布无关的对抗训练泛化界限，解决了截断内积的非线性和$\ell_0$空间上最大化问题的挑战。 |
| [^151] | [Diffusion World Model](https://arxiv.org/abs/2402.03570) | 扩散世界模型是一个能够预测多步未来状态和奖励的条件性扩散模型，在模型效果和性能方面超过了传统的一步动力学模型。 |
| [^152] | [SkipPredict: When to Invest in Predictions for Scheduling](https://arxiv.org/abs/2402.03564) | SkipPredict是一种新颖的调度方法，用于解决预测成本的问题。它根据作业的预测要求对作业进行分类，并通过优先处理预测为短作业的作业以及应用详细预测来近似长作业的最短剩余处理时间。这种方法可以在排队系统中有效地应用。 |
| [^153] | [Distinguishing the Knowable from the Unknowable with Language Models](https://arxiv.org/abs/2402.03563) | 通过研究大型语言模型，在自由文本中识别作为代理的模型和冻结预训练模型的嵌入的小型线性探测器可以准确预测更大模型令牌级别上的自信度，进一步提出了一种无监督的方法在相同任务上达到了非平凡的准确度，这证明了语言模型中存在不同类型的不确定性表示。 |
| [^154] | [Projected Generative Diffusion Models for Constraint Satisfaction](https://arxiv.org/abs/2402.03559) | 本文介绍了一种名为投影式生成扩散模型（PGDM）的方法，它能够通过约束优化问题将生成扩散模型适用于对特定条件有严格要求的场景。该方法通过迭代投影方法确保生成的数据符合指定的约束或物理原理。实验证明PGDM在复杂的约束和常微分方程的情况下也能合成出符合要求的输出。 |
| [^155] | [Path Signatures and Graph Neural Networks for Slow Earthquake Analysis: Better Together?](https://arxiv.org/abs/2402.03558) | 本文介绍了一种新的方法，将路径签名与图神经网络结合起来，用于慢地震分析。通过利用路径签名的特征提取能力和图神经网络处理空间交互的优势，实现了更好的效果。在实际情况下的应用中，我们利用GPS传感器网络对慢滑事件进行分析，并在模拟的随机微分方程中建立了基准模型。 |
| [^156] | [Single-GPU GNN Systems: Traps and Pitfalls](https://arxiv.org/abs/2402.03548) | 本论文分析了当前单GPU GNN系统的陷阱与注意事项，并提出了一系列的解决方案。通过开发新的参考系统，实现高效实用地解决系统设计陷阱的一系列优化，并推动技术的发展。 |
| [^157] | [Online Feature Updates Improve Online (Generalized) Label Shift Adaptation](https://arxiv.org/abs/2402.03545) | 本文提出了一种名为OLS-OFU的新方法，通过在测试时利用无标签数据进行自监督学习和特征优化，以解决在线设置中的标签转移问题。研究结果表明，OLS-OFU在各种数据集和领域转移条件下都表现出了有效性和鲁棒性。 |
| [^158] | [HAMLET: Graph Transformer Neural Operator for Partial Differential Equations](https://arxiv.org/abs/2402.03541) | HAMLET是一个图变换神经算子框架，通过使用模块化输入编码器将微分方程信息直接融入解决过程中，并展示出在处理复杂数据和噪声方面的鲁棒性，适用于任意几何形状和各种输入格式的PDE问题。通过大量实验，我们证明了HAMLET能够超越当前的PDE技术。 |
| [^159] | [Regulation Games for Trustworthy Machine Learning](https://arxiv.org/abs/2402.03540) | 本论文提出了一个以调节游戏为基础的框架，将可信机器学习视为多目标多主体优化问题。通过引入ParetoPlay算法，寻找社会最优的游戏解决方案，该算法能够确保代理方始终保持在Pareto前沿上。 |
| [^160] | [ANN-based position and speed sensorless estimation for BLDC motors](https://arxiv.org/abs/2402.03534) | 无传感器位置和速度估计方法在BLDC电机应用中取得了显著的改进效果。 |
| [^161] | [Fairness and Privacy Guarantees in Federated Contextual Bandits](https://arxiv.org/abs/2402.03531) | 本文研究了在联邦环境中的上下文多臂老虎机问题，提出了一个合作式联邦学习算法Fed-FairX-LinUCB。该算法通过设计新颖的通信协议，保证了公平性和隐私性，解决了传统算法在代理数量上产生的公平性损失问题。 |
| [^162] | [Consistent Validation for Predictive Methods in Spatial Settings](https://arxiv.org/abs/2402.03527) | 本论文研究了在空间环境中验证预测方法的一致性问题，提出了一种能够处理不匹配情况的方法。 |
| [^163] | [Deep Reinforcement Learning for Picker Routing Problem in Warehousing](https://arxiv.org/abs/2402.03525) | 本研究提出了一种基于注意力机制的神经网络模型，通过深度强化学习解决仓储中的拣选车辆路径问题。与传统启发式方法相比，该方法具有更快的速度和准确度，并能降低路径的感知复杂度。 |
| [^164] | [Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains](https://arxiv.org/abs/2402.03509) | 本研究跨领域评估了零样本摘要生成器的真实性。通过对生物医学文章和法律法案等专业领域进行评估，我们特别关注生成摘要的真实性，并分析了预训练语料库中给定领域的普遍性对摘要质量的影响。 |
| [^165] | [Neural networks for abstraction and reasoning: Towards broad generalization in machines](https://arxiv.org/abs/2402.03507) | 这项工作探索了神经网络在广泛泛化方面的应用，通过研究解决抽象和推理任务的新方法，试图提高计算机系统从少量示例中学习新概念的能力。 |
| [^166] | [How Does Unlabeled Data Provably Help Out-of-Distribution Detection?](https://arxiv.org/abs/2402.03502) | 本文介绍了一种新的学习框架SAL（Separate And Learn），通过利用未标记数据和标记数据，分离并训练异常点和OOD分类器，理论上提供了强大的保证和严格的错误界限。 |
| [^167] | [Curriculum reinforcement learning for quantum architecture search under hardware errors](https://arxiv.org/abs/2402.03500) | 本研究提出了一种基于课程的强化学习量子架构搜索算法（CRLQAS），解决了在噪声中间规模量子时代中，对于架构搜索的噪声效应的不理解的问题。 |
| [^168] | [Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective](https://arxiv.org/abs/2402.03496) | 移除自适应方法中的平方根可以在卷积结构上减小与SGD的泛化差距，同时保持在transformers上的性能。 |
| [^169] | [Partially Stochastic Infinitely Deep Bayesian Neural Networks](https://arxiv.org/abs/2402.03495) | 本文提出了一种部分随机性的无限深度贝叶斯神经网络，通过在网络框架中整合部分随机性，改善现有架构在训练和推理时间上的计算效率限制，并提供了多种灵活的网络设计配置，同时通过数学证明确保了模型的表达能力。 |
| [^170] | [Early prediction of onset of sepsis in Clinical Setting](https://arxiv.org/abs/2402.03486) | 本研究提出了使用机器学习模型预测临床数据中脓毒症的早期发作，通过使用有监督学习方法训练XGBoost模型，并利用规范化效用分数评估模型的性能。 |
| [^171] | [Attention Meets Post-hoc Interpretability: A Mathematical Perspective](https://arxiv.org/abs/2402.03485) | 本文通过数学方式研究了基于注意力机制的架构，比较了事后解释和基于注意力机制的解释的差异，发现尽管有局限性，事后解释方法能够捕获比仅仅检查注意力权重更有用的洞察。 |
| [^172] | [FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning](https://arxiv.org/abs/2402.03481) | FINEST方法通过从给定的推荐模型获得参考排序列表，并在模拟的扰动场景下进行模型微调，保持排序，以稳定和改善推荐系统的性能。 |
| [^173] | [Trillion Parameter AI Serving Infrastructure for Scientific Discovery: A Survey and Vision](https://arxiv.org/abs/2402.03480) | 这篇论文调研了为科学研究提供支持的兆级参数人工智能基础设施，并描述了在系统设计中所面临的重要技术挑战和开放问题。 |
| [^174] | [ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design](https://arxiv.org/abs/2402.03479) | 本研究探索了深度强化学习代理的零样本迁移能力，并发现通过根据值损失优先选择级别，可以改善代理的推广能力。此外，无监督环境设计方法对改善代理表现也具有重要作用。 |
| [^175] | [Hyper-Diffusion: Estimating Epistemic and Aleatoric Uncertainty with a Single Model](https://arxiv.org/abs/2402.03478) | 本研究引入了一种新的集合方法，超扩散，可以使用单一模型准确估计认识和偶然不确定性。 |
| [^176] | [CT Material Decomposition using Spectral Diffusion Posterior Sampling](https://arxiv.org/abs/2402.03476) | 本研究提出了一种利用扩散后验抽样（DPS）进行光谱CT材料分解的深度学习方法。该方法结合了无监督训练的先验知识和物理模型，能够在更短的时间内实现高准确性、低不确定性和低计算成本。 |
| [^177] | [Active Region-based Flare Forecasting with Sliding Window Multivariate Time Series Forest Classifiers](https://arxiv.org/abs/2402.03474) | 本研究提出了一种基于滑动窗口多变量时间序列森林分类器的活动区域耀斑预测方法，并通过一种新颖的特征排序方法，解决了当前数据驱动方法在忽略时间演化特征时的局限性。 |
| [^178] | [The Information of Large Language Model Geometry](https://arxiv.org/abs/2402.03471) | 论文研究了大型语言模型中嵌入的信息编码，并发现表示熵与模型大小呈幂律关系。通过信息理论和回归技术，建立了新标记的信息增益与岭回归之间的理论联系，并探索了Lasso回归在选择有意义的标记方面的有效性。实验结果表明，信息在标记之间分布，而不仅仅集中在特定的“有意义”的标记上。 |
| [^179] | [Preference-free Alignment Learning with Regularized Relevance Reward](https://arxiv.org/abs/2402.03469) | 无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。 |
| [^180] | [Exact Tensor Completion Powered by Arbitrary Linear Transforms](https://arxiv.org/abs/2402.03468) | 本文研究了使用任意线性变换进行精确张量补全的问题，并提出了一种新的张量-张量乘积和张量核范数定义，设计了高效的算法并得到理论界限。这些工作极大地提升了张量补全的灵活性，大量实验证实了该方法的优越性。 |
| [^181] | [Stochastic Modified Flows for Riemannian Stochastic Gradient Descent](https://arxiv.org/abs/2402.03467) | 本文研究了黎曼随机梯度下降（RSGD）的收敛速度，并介绍了一种基于随机修改流（RSMF）的扩散逼近方法，该方法可以提高对RSGD的近似精度。 |
| [^182] | [Breaking the Curse of Dimensionality with Distributed Neural Computation](https://arxiv.org/abs/2402.03460) | 通过分布式神经计算算法，我们提出了一种理论方法来克服维度灾难，并证明了我们的模型可以在任意精度下逼近Lipschitz函数，在参数量和前向传播方面具有优势。 |
| [^183] | [Efficient and Interpretable Traffic Destination Prediction using Explainable Boosting Machines](https://arxiv.org/abs/2402.03457) | 本研究评估了一种高效且可解释的交通目的地预测模型（EBM），在多个混合交通数据集上表现出有竞争力的性能，并能够提供特征重要性和交互作用的分析以及预测解释的质量示例。 |
| [^184] | [Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees](https://arxiv.org/abs/2402.03448) | 本文提出了一种称为分散式间歇联邦学习（DSpodFL）的方法，它统一了分布式梯度下降（DGD）、随机闲话（RG）和分散式联邦平均（DFedAvg）等著名的分散优化方法。根据分析结果，DSpodFL能够在更一般的假设下达到几何收敛速率与最佳性差距的匹配。经过实验验证了该方法的有效性。 |
| [^185] | [Challenges in Variable Importance Ranking Under Correlation](https://arxiv.org/abs/2402.03447) | 变量重要性排序在解释机器学习中很重要，但是特征之间的相关性是一个挑战。最近提出了基于特征 Knockoffs 的改进方法来解决这个问题，我们的工作重点是对这些方法进行评估和评估。 |
| [^186] | [An end-to-end deep learning pipeline to derive blood input with partial volume corrections for automated parametric brain PET mapping](https://arxiv.org/abs/2402.03414) | 该论文介绍了一个端到端的深度学习流水线，利用局部体积校正和非侵入性动脉采样模型，自动推导血液输入函数，从而解决了动态FDG-PET定量分析中的关键挑战。 |
| [^187] | [Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations](https://arxiv.org/abs/2402.03407) | 通过自监督表示，我们提出了一种新的自助转换架构，该架构可以增强LLM基础的语音生成系统的稳定性，并克服了在推理时出现的多个稳定性问题。使用这种方法，LLM可以从文本中仅生成语音的内容和风格，而说话者身份由另一个模型提供。 |
| [^188] | [Deep Nonlinear Hyperspectral Unmixing Using Multi-task Learning](https://arxiv.org/abs/2402.03398) | 本文提出了一种基于深度学习的无监督非线性高光谱解混方法，通过将通用的非线性模型与无特殊假设的方法相结合，使用多任务学习来提高解混性能。 |
| [^189] | [UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing](https://arxiv.org/abs/2402.03396) | 本研究提出了UniTSyn，一个大型数据集，可以增强大型语言模型在程序测试中的能力。通过关联测试和被测试函数，UniTSyn能够提高模型在生成准确和完整的测试方面的表现。 |
| [^190] | [Delivery Optimized Discovery in Behavioral User Segmentation under Budget Constrain](https://arxiv.org/abs/2402.03388) | 在预算限制下，我们提出了一种基于随机优化的算法，用于优化传递发现行为用户细分。 |
| [^191] | [Overcoming Order in Autoregressive Graph Generation](https://arxiv.org/abs/2402.03387) | 这项研究提出在图生成中使用RNN，并通过增加无序正则化项解决了顺序问题，对于顺序图生成模型尤其在数据稀缺时有益处。 |
| [^192] | [A generalized decision tree ensemble based on the NeuralNetworks architecture: Distributed Gradient Boosting Forest (DGBF)](https://arxiv.org/abs/2402.03386) | 本文提出了一种基于神经网络结构的通用决策树集成算法，分布式梯度提升森林（DGBF），通过将包和提升的数学公式结合起来，实现了树之间自然地进行分布式表示学习过程。该算法能够处理离散或表格数据，并具有建模非结构化数据的能力。 |
| [^193] | [Adolescent relational behaviour and the obesity pandemic: A descriptive study applying social network analysis and machine learning techniques](https://arxiv.org/abs/2402.03385) | 本研究通过应用社交网络分析和机器学习技术，研究了青少年关系行为与肥胖疫情之间的关系。研究结果表明，青少年的群体间存在显著差异，且与性别和饮食变量有关。总结出的关键因素有助于了解和干预青少年肥胖疫情。 |
| [^194] | [Survival and grade of the glioma prediction using transfer learning](https://arxiv.org/abs/2402.03384) | 本研究利用迁移学习技术，通过对胶质母细胞瘤图像数据集进行微调，成功实现了生存和肿瘤等级预测，生存预测准确率达到65%，肿瘤等级预测准确率达到97%。 |
| [^195] | [Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing](https://arxiv.org/abs/2402.03379) | 全链路上升建模方法ECUP旨在解决链路偏差和处理不适应问题，在线营销中有重要的应用价值。 |
| [^196] | [Evaluation of Google's Voice Recognition and Sentence Classification for Health Care Applications](https://arxiv.org/abs/2402.03369) | 本研究评估了谷歌的语音识别和句子分类在医疗健康应用中的应用。通过将语音识别技术应用于围手术期服务，可以改善患者流程和医疗质量。通过使用后处理分类器，本研究增强了谷歌的语音识别能力。 |
| [^197] | [RAG-Fusion: a New Take on Retrieval-Augmented Generation](https://arxiv.org/abs/2402.03367) | RAG-Fusion方法通过生成多个查询，并结合互惠排名融合技术，能够从不同角度上下文化原始查询，提供准确和全面的信息。这项研究在人工智能和自然语言处理应用方面有重要进展，并展示了全球和区域之间的转变。 |
| [^198] | [Uncertainty-Aware Explainable Recommendation with Large Language Models](https://arxiv.org/abs/2402.03366) | 这项研究开发了一个模型，通过训练用户和项目输入的ID向量作为提示，利用GPT-2实现不确定性感知的可解释推荐系统。该系统采用联合训练机制并在多任务学习框架中进行优化，能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。 |
| [^199] | [Heterophily-Aware Fair Recommendation using Graph Convolutional Networks](https://arxiv.org/abs/2402.03365) | 本文提出了一种利用图卷积网络的公平推荐系统，名为HetroFair，旨在提高项目侧的公平性。HetroFair使用公平注意力和异质性特征加权两个组件来生成具有公平性意识的嵌入。 |
| [^200] | [Exploring Prime Number Classification: Achieving High Recall Rate and Rapid Convergence with Sparse Encoding](https://arxiv.org/abs/2402.03363) | 通过稀疏编码和神经网络结构的组合，本文提出了一种在质数和非质数分类中实现高召回率和快速收敛的新方法，取得了令人满意的结果。 |
| [^201] | [A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation](https://arxiv.org/abs/2402.03358) | 这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。 |
| [^202] | [Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers via Self-Imitation Learning](https://arxiv.org/abs/2402.03357) | 本研究针对假新闻在社交网络中的影响，通过部署揭露者传播真实新闻，提出了一种通过自我模仿学习选择揭露者的方法。通过NAGASIL算法，能够在假新闻减轻中学习更有效的揭露者选择策略。 |
| [^203] | [Tweet Influence on Market Trends: Analyzing the Impact of Social Media Sentiment on Biotech Stocks](https://arxiv.org/abs/2402.03353) | 本研究通过分析推特情绪与生物技术股票市场行为之间的关系，发现了社交媒体话语对投资者情绪和决策过程的影响，并提出了使用情绪协变量来改善股市预测准确性的方法。 |
| [^204] | [Zeroth-Order primal-dual Alternating Projection Gradient Algorithms for Nonconvex Minimax Problems with Coupled linear Constraints](https://arxiv.org/abs/2402.03352) | 本文研究了具有耦合线性约束的非凸极小极大问题的零阶算法，提出了两个单循环算法用于求解这些问题，并证明了它们的迭代复杂度分别为O(ε^(-2))和O(ε^(-4))。 |
| [^205] | [When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges](https://arxiv.org/abs/2402.03349) | 这篇论文探讨了生成AI和大型语言模型在地球科学中的潜在应用，并讨论了几种已在地球科学中使用的GAI模型，包括生成对抗网络（GANs）、基于物理的模型等。 |
| [^206] | [Weakly supervised covariance matrices alignment through Stiefel matrices estimation for MEG applications](https://arxiv.org/abs/2402.03345) | 本文引入了一种在MEG应用中用于时间序列数据的新颖领域自适应技术，称为混合模型斯蒂弗适应（MSA），通过利用无标记数据建立了等效信号方差的成对对应关系以确保有效的预测性能。在神经科学问题中，MSA在使用脑磁图进行脑龄回归时表现优于最近的方法。 |
| [^207] | [MADRL-based UAVs Trajectory Design with Anti-Collision Mechanism in Vehicular Networks](https://arxiv.org/abs/2402.03342) | 本论文提出了一种基于MADRL的无人机轨迹设计方法，采用排名二进制掩码的方式，实现了多个无人机之间的无碰撞路径设计。 |
| [^208] | [CNN-DRL with Shuffled Features in Finance](https://arxiv.org/abs/2402.03338) | 本研究通过对特征向量进行特定排列，应用CNN-DRL方法于金融数据中，在回报上取得了显著的提高。 |
| [^209] | [Reinforcement-learning robotic sailboats: simulator and preliminary results](https://arxiv.org/abs/2402.03337) | 本文介绍了开发虚拟海洋环境模拟真实实验的主要挑战和问题，并提出了使用强化学习代理进行自主导航和控制的关键特性。同时，还讨论了创建基于真实机器人帆船的功能性数字孪生所需的建模和实施步骤以及挑战。这项研究对于开发基于强化学习的导航算法在真实船只上的应用具有直接的意义。 |
| [^210] | [Cyclic Neural Network](https://arxiv.org/abs/2402.03332) | 本文介绍了一种具有创新性的循环神经网络（Cyclic NNs），可以模拟生物智能系统的灵活动态图形特性，并在广泛测试的数据集上验证了其优越性。 |
| [^211] | [Slot Structured World Models](https://arxiv.org/abs/2402.03326) | Slot Structured World Models结合了基于Slot注意力的“对象为中心”编码器和基于潜在图的动力学模型，能够在多步预测任务中优于基线模型，解决了无法提取“对象为中心”表示和区分外观相似的多个对象的问题。 |
| [^212] | [Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations](https://arxiv.org/abs/2402.03325) | 本文研究了在标记的源领域上训练的模型在部署到分布不同的目标领域时的泛化问题，并提出了一种名为连接延迟的方法，通过自监督预训练和定向增强方法来改善模型鲁棒性。 |
| [^213] | [SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization](https://arxiv.org/abs/2402.03317) | 该论文介绍了SpecFormer，一种通过最大奇异值惩罚来增强视觉Transformer（ViTs）对对抗性攻击的韧性的方法。该方法通过引入局部Lipschitz边界和最大奇异值惩罚方法（MSVP），有效地降低了注意力权重矩阵的谱范数。 |
| [^214] | [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) | DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。 |
| [^215] | [Cool-chic video: Learned video coding with 800 parameters](https://arxiv.org/abs/2402.03179) | 我们提出了一个轻量级的学习视频编码器，使用800个参数和900次乘法来实现低解码复杂度。该编码器在压缩视频时能够利用时间冗余，并在接近AVC的速率失真条件下表现优于其他过拟合编解码器。 |
| [^216] | [EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/abs/2402.03049) | EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。 |
| [^217] | [Data-induced multiscale losses and efficient multirate gradient descent schemes](https://arxiv.org/abs/2402.03021) | 本文研究了多尺度数据对机器学习算法的影响，并提出了一种基于数据的新的梯度下降方法，旨在提高训练效率。 |
| [^218] | [Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing](https://arxiv.org/abs/2402.02817) | 本文提出了一种基于贝叶斯最优的公平分类方法，通过先处理、中处理和后处理来最小化分类错误，并在给定群体公平性约束的情况下进行优化。该方法引入了线性和双线性差异度量的概念，并找到了贝叶斯最优公平分类器的形式。本方法能够处理多个公平性约束和常见情况。 |
| [^219] | [Rethinking Optimization and Architecture for Tiny Language Models](https://arxiv.org/abs/2402.02791) | 本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。 |
| [^220] | [Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning](https://arxiv.org/abs/2402.02772) | 在强化学习中应用扩散模型进行规划常受限于基础分布和样本多样性。本文提出的CDiffuser方法通过对比学习来提高到达高回报状态的概率。 |
| [^221] | [Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner with Robust Low-Level Control for Robotic Manipulators](https://arxiv.org/abs/2402.02551) | 这篇论文提出了一种基于深度强化学习和鲁棒低级控制的机器人操作器的障碍物避障轨迹规划方法，该方法通过与环境的交互积极参与学习，绕过了计算复杂性，同时解决了非重复和随机的避障任务。 |
| [^222] | [TopoX: A Suite of Python Packages for Machine Learning on Topological Domains](https://arxiv.org/abs/2402.02441) | TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。 |
| [^223] | [Exploiting Low-level Representations for Ultra-Fast Road Segmentation](https://arxiv.org/abs/2402.02430) | 本研究利用低层次特征表示进行道路分割，在主流网络模型的主要阶段实现了大部分道路像素的准确表示，提出了以低层次特征为主导的道路分割网络（LFD-RoadSeg）。 |
| [^224] | [Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction](https://arxiv.org/abs/2402.02416) | Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。 |
| [^225] | [Dynamic Incremental Optimization for Best Subset Selection](https://arxiv.org/abs/2402.02322) | 本文研究了一类$\ell_0$正则化问题的对偶形式，并提出了一种高效的原对偶算法，通过充分利用对偶范围估计和增量策略，提高了最佳子集选择问题的解决方案的效率和统计性质。 |
| [^226] | [Causal Bayesian Optimization via Exogenous Distribution Learning](https://arxiv.org/abs/2402.02277) | 本文引入了一种新的方法，通过学习外源变量的分布，提高了结构化因果模型的近似精度，并将因果贝叶斯优化扩展到更一般的因果方案。 |
| [^227] | [Vanilla Bayesian Optimization Performs Great in High Dimension](https://arxiv.org/abs/2402.02229) | 本文研究了高维情况下贝叶斯优化算法的问题，并提出了一种改进方法，通过对先验假设进行简单的缩放，使普通贝叶斯优化在高维任务中表现出色。 |
| [^228] | [Position Paper: The Landscape and Challenges of HPC Research and LLMs](https://arxiv.org/abs/2402.02018) | 运用语言模型技术于高性能计算任务中具有巨大潜力 |
| [^229] | [Simulation-Enhanced Data Augmentation for Machine Learning Pathloss Prediction](https://arxiv.org/abs/2402.01969) | 本文提出了一种模拟增强数据增强方法，用于改善机器学习路径损耗预测中数据有限的挑战。该方法通过结合来自蜂窝覆盖模拟器生成的合成数据和独立收集的真实世界数据集，提供了关键的真实值用于模型训练。通过使用特征工程和高效稳健的梯度提升机器学习算法CatBoost，该方法显著提高了路径损耗预测的性能。 |
| [^230] | [Analyzing Neural Network-Based Generative Diffusion Models through Convex Optimization](https://arxiv.org/abs/2402.01965) | 本研究通过凸优化方法分析了基于神经网络的生成扩散模型，揭示了这些模型在非渐近设置下的精确预测分数函数和收敛结果。 |
| [^231] | [Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction](https://arxiv.org/abs/2402.01960) | 通过符合预测方法，提出了一种校准不确定性量化的分位数神经运算器，能够在函数定义域上同时量化不确定性，无需分布假设，实验结果表明其在2D Darcy流动和3D车辆表面压力预测任务上优于基线方法。 |
| [^232] | [Large Language Model Agent for Hyper-Parameter Optimization](https://arxiv.org/abs/2402.01881) | 基于大规模语言模型的AgentHPO技术通过自动化超参数优化，在机器学习任务中大大减少了试验次数，简化了设置过程，提升了解释性和用户信任。 |
| [^233] | [LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks](https://arxiv.org/abs/2402.01817) | LLMs无法独自进行规划或自我验证，但在规划/推理任务中可以作为通用近似知识源发挥更大作用。 |
| [^234] | [Large Language Models for Time Series: A Survey](https://arxiv.org/abs/2402.01801) | 本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。 |
| [^235] | [When Large Language Models Meet Vector Databases: A Survey](https://arxiv.org/abs/2402.01763) | 本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。 |
| [^236] | [SymbolicAI: A framework for logic-based approaches combining generative models and solvers](https://arxiv.org/abs/2402.00854) | SymbolicAI是一个基于逻辑的框架，将生成模型与多种求解器无缝集成，通过将大型语言模型作为语义解析器，实现了符号推理与生成式人工智能的融合。 |
| [^237] | [Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI](https://arxiv.org/abs/2402.00809) | 《在大规模人工智能时代的贝叶斯深度学习》这篇立场论文探讨了贝叶斯深度学习在各种不同设置下的优势，并指出了与之相关的挑战和有趣的研究方向。未来的研究重点将放在如何将大规模基础模型与贝叶斯深度学习相结合，以发挥它们的全部潜力。 |
| [^238] | [Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling](https://arxiv.org/abs/2402.00522) | 本研究系统地探讨了Transformer在长序列建模中的近似性质，并研究了其关键组件对表达能力的影响机制。这些发现揭示了关键参数对Transformer的作用，并为替代架构提供了自然建议。 |
| [^239] | [PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks](https://arxiv.org/abs/2402.00326) | PirateNets是一种物理知识驱动的深度学习框架，解决了多层感知机网络在较大深度时性能下降的问题，通过引入自适应残差连接实现了稳定和高效的训练，并提升了模型的性能。 |
| [^240] | [The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise](https://arxiv.org/abs/2401.07844) | 本文通过使用ODE方法，将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，提高了其在离策略强化学习中的适用性。 |
| [^241] | [Diffusion Models, Image Super-Resolution And Everything: A Survey](https://arxiv.org/abs/2401.00736) | 扩散模型（DMs）在图像超分辨率（SR）领域产生了颠覆性的影响，缩小了图像质量与人类感知偏好之间的差距。该研究调查了DM的理论基础，分析了其独特特点和方法，探索了替代输入领域等当前的研究方向。 |
| [^242] | [Dual-stage optimizer for systematic overestimation adjustment applied to multi-objective genetic algorithms for biomarker selection](https://arxiv.org/abs/2312.16624) | 该论文介绍了一种双阶段优化器，用于对多目标遗传算法中的生物标志物选择进行系统性过估调整。 |
| [^243] | [Faster Rates for Switchback Experiments](https://arxiv.org/abs/2312.15574) | 本研究提出了一种更快速的Switchback实验方法，通过使用整个时间块，以 $\sqrt{\log T/T}$ 的速率估计全局平均处理效应。 |
| [^244] | [Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning](https://arxiv.org/abs/2312.15122) | 本研究提出了一种扩展的自动驾驶强化学习方法，在大规模实验中展示了随着规模增加，策略性能的改善。与现有机器学习自动驾驶策略相比，我们的最佳策略将故障率降低了64％，同时提高了25％的驾驶进展速度。 |
| [^245] | [XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX](https://arxiv.org/abs/2312.12044) | XLand-MiniGrid是一个在JAX中可扩展的元强化学习环境工具套件，提供了包含数百万个不同难度的任务和易于使用的基线，实现了在有限资源下的大规模实验民主化。 |
| [^246] | [A mathematical perspective on Transformers](https://arxiv.org/abs/2312.10794) | 该论文提出了一种数学框架用于分析Transformers，并揭示了在长时间下的集团形成。这一研究为数学家和计算机科学家提供了新的视角。 |
| [^247] | [Momentum Particle Maximum Likelihood](https://arxiv.org/abs/2312.07335) | 该论文提出了一种动态系统方法，用于在参数和概率分布的扩展空间上最小化自由能函数，该方法融合了Nesterov的加速梯度方法、欠阻尼Langevin扩散和p。 |
| [^248] | [Compressed Context Memory For Online Language Model Interaction](https://arxiv.org/abs/2312.03414) | 本论文提出了一种用于在线场景中Transformer语言模型的压缩上下文记忆系统，可以在有限的内存空间中实现语言模型推断，提高吞吐量，并通过个性化和多任务学习的评估证明了其有效性。 |
| [^249] | [Eliciting Latent Knowledge from Quirky Language Models](https://arxiv.org/abs/2312.01037) | 本研究通过引入一套“古怪”的语言模型，调取了这些模型在特定上下文中的潜在知识，展示了从可信度低的模型中调取可靠知识的前景。 |
| [^250] | [Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks](https://arxiv.org/abs/2311.12997) | 本研究通过在合成数据生成过程上训练自回归Transformer模型，展示了其学习组合结构和泛化能力。通过生成中间输出来组合函数在泛化到新的未见组合时比不生成中间输出更有效。同时，组合顺序的偏差会对模型的性能产生影响。 |
| [^251] | [Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction](https://arxiv.org/abs/2311.09386) | 本研究提出了一种概率性Gram-Schmidt方法来进行特征提取，该方法可以检测和去除非线性依赖性，从而提取数据中的线性特征并去除非线性冗余。 |
| [^252] | [CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data Centers](https://arxiv.org/abs/2311.03615) | 本文介绍了一种名为CAFE的碳感知联邦学习框架，在地理分布式数据中心中优化训练过程，平衡学习性能和碳足迹。该方法通过coreset选择评估学习性能，利用Lyapunov漂移加惩罚框架处理未来碳强度的不可预测性，并设计了高效的算法来解决计算和通信成本的挑战。 |
| [^253] | [DeepInception: Hypnotize Large Language Model to Be Jailbreaker](https://arxiv.org/abs/2311.03191) | 本研究提出了一种名为DeepInception的轻量级方法，利用语言模型的角色扮演能力构建新颖的嵌套场景，成功催眠大型语言模型成为破解者。通过实验证明，DeepInception在破解成功率方面具有竞争力，并揭示了开源和闭源语言模型的关键弱点。 |
| [^254] | [Causal Fair Metric: Bridging Causality, Individual Fairness, and Adversarial Robustness](https://arxiv.org/abs/2310.19391) | 本文引入了一种因果公平度量方法，它能够综合考虑因果性、个体公平性和对抗鲁棒性等因素。通过基于因果结构和保护性因果扰动来生成可比较的输入数据实例，并借助度量学习进行度量估计和应用。 |
| [^255] | [A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions](https://arxiv.org/abs/2310.19218) | 联邦遗忘（FU）是解决联邦学习（FL）中数据隐私问题的战略解决方案。在发展FU方法时需要平衡隐私、安全、效用和效率的竞争性要求，以维持FL系统的效果和可用性。 |
| [^256] | [Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting](https://arxiv.org/abs/2310.18948) | 通过利用AIS数据预测船舶轨迹，本研究旨在通过减少船舶与鲸鱼碰撞来建立更安全的海洋环境。 |
| [^257] | [HarmonyDream: Task Harmonization Inside World Models](https://arxiv.org/abs/2310.00344) | 在基于模型的强化学习中，通过深入研究观测建模和奖励建模的作用，发现通过缓解观测建模或奖励建模的占用优势来实现高效学习的潜力。 |
| [^258] | [CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs](https://arxiv.org/abs/2309.09844) | 本研究介绍了一种采用异构图神经网络的新方法，将普通驾驶场景转化为边界情况场景。通过生成简洁的场景图，并利用注意力和三元嵌入扰动图形，我们的模型成功学习到生成边界情况的能力。 |
| [^259] | [OHQ: On-chip Hardware-aware Quantization](https://arxiv.org/abs/2309.01945) | 本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），通过构建量化感知流水线和引入掩码引导的量化估计技术，实现了在资源受限的硬件上进行高效量化，填补了现有混合精度量化的搜索空间过大和实际部署差距大的问题。 |
| [^260] | [CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent Masking](https://arxiv.org/abs/2307.16847) | CroSSL是一种跨模态的自监督学习方法，通过隐藏掩码和跨模态聚合器实现对时间序列的学习，无需负样本对和数据预处理。 |
| [^261] | [Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking](https://arxiv.org/abs/2307.16806) | 本研究通过基于ASCII-Art的跨模态任务，探讨了ChatGPT和GPT3.5在视觉任务中的能力，结果表明它们在图像识别、图像部分知识和图像生成方面并不完全缺乏。 |
| [^262] | [An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics](https://arxiv.org/abs/2305.14998) | 研究评估了无参考图像标题评估指标在高词汇重叠但含义差异很大的情况下的鲁棒性，结果发现尽管这些指标与人类判断相关性较高，但对细粒度错误识别困难，并且在标题不合理性错误、图像相关对象大小变化以及标题对否定意义的理解方面存在敏感性差异。 |
| [^263] | [Online Recommendations for Agents with Discounted Adaptive Preferences](https://arxiv.org/abs/2302.06014) | 本文研究了具有折扣自适应偏好的代理的在线推荐问题，通过在每一轮中展示一系列物品并考虑代理的偏好演变，以实现对于目标集合的最小化后悔。在长期记忆的情况下，可以实现对于能够在任何时刻实现的分布集合的高效次线性后悔。 |
| [^264] | [Concept Gradient: Concept-based Interpretation Without Linear Assumption](https://arxiv.org/abs/2208.14966) | 本文提出了一种概念梯度（CG）的方法，将基于概念的解释扩展到了非线性概念函数，并证明在玩具示例和真实世界数据集上，概念梯度（CG）优于概念激活向量（CAV）。 |
| [^265] | [ARIEL: Adversarial Graph Contrastive Learning](https://arxiv.org/abs/2208.06956) | 该论文提出了对抗图对比学习（ARIEL）方法，通过引入对抗图视图进行数据增强，提取信息丰富的对比样本，进一步改进了图表示学习中的对比学习方法。 |
| [^266] | [Reinforcement Learning Assisted Recursive QAOA](https://arxiv.org/abs/2207.06294) | 递归QAOA是一种非局部的QAOA变体，用于改善近似解的质量。本文通过识别和分析RQAOA失败的案例，提出了一种使用强化学习辅助的方法。 |
| [^267] | [Hilbert Curve Projection Distance for Distribution Comparison](https://arxiv.org/abs/2205.15059) | 本研究提出了一种新的度量方法，称为Hilbert曲线投影(HCP)距离，用于测量两个概率分布之间的距离，并具有低复杂度。通过Hilbert曲线投影和运输距离计算，该方法适用于有界支撑的概率测度，并在高维空间中具有较好的收敛性能。为了解决维度灾难，还开发了两个HCP距离的变体，使用子空间投影。 |
| [^268] | [Bayes-Optimal Classifiers under Group Fairness](https://arxiv.org/abs/2202.09724) | 这篇论文提供了一个统一的框架，推导出在组公平性下的贝叶斯最优分类器，并提出了一种名为FairBayes的基于组的阈值方法，可以直接控制不公平现象，实现基本最优的公平性-准确性权衡。 |
| [^269] | [Order Optimal Bounds for One-Shot Federated Learning over non-Convex Loss Functions](https://arxiv.org/abs/2108.08677) | 本论文研究了一次性联邦学习中非凸损失函数的最优界限，并提出了一个分布式学习算法(MRE-NC)，其期望损失与最优界限匹配。 |
| [^270] | [Alignment and Comparison of Directed Networks via Transition Couplings of Random Walks](https://arxiv.org/abs/2106.07106) | NetOTC是一种用于比较和对齐两个网络的方法，通过最小化随机行走的转换耦合的期望成本来量化网络之间的差异，并提供顶点和边的对齐。它捕捉到了关于网络的局部和全局信息，并保留了边缘。 |
| [^271] | [IM-META: Influence Maximization Using Node Metadata in Networks With Unknown Topology](https://arxiv.org/abs/2106.02926) | IM-META是一种在未知拓扑网络中进行影响最大化的方法，通过利用节点元数据和查询信息，通过学习元数据与边的关系、构建增强图以及使用拓扑感知排序策略来确定最具影响力的种子节点和查询节点。 |
| [^272] | [InstaHide's Sample Complexity When Mixing Two Private Images](https://arxiv.org/abs/2011.11877) | 本文研究了对InstaHide的最新攻击，提出了一个统一的框架来理解和分析这些攻击。通过一种新算法，在InstaHide挑战设置下，以可证明的保证和最优样本复杂度恢复所有私人图片，并提供了关于检索所有InstaHide图片的计算难度结果。 |
| [^273] | [On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU Networks](https://arxiv.org/abs/2011.05530) | 本研究探讨了多项式逼近隐私保护和可验证ReLU网络的方法。通过实验证明，平方函数并不是最佳替代ReLU函数的二次多项式。我们提出了一个具有 |
| [^274] | [Learning Calibrated Uncertainties for Domain Shift: A Distributionally Robust Learning Approach](https://arxiv.org/abs/2010.05784) | 本论文提出了一种学习校准不确定性的方法，在领域转移下有效应用于无监督领域自适应和半监督学习等任务。通过可微的密度比估计器和分布鲁棒学习框架，我们生成了校准的不确定性，为下游任务提供了帮助。 |
| [^275] | [Large Margin Mechanism and Pseudo Query Set on Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2005.09218) | 本文提出了一种基于大边际机制和伪查询集的跨域少样本学习方法，通过生成伪查询图像，并借鉴人脸识别方法中的大边际机制对特征提取模块进行微调。实验证明，该方法在各个领域上都取得了显著的优势，展示了其鲁棒性和适应预训练模型到新领域的能力。 |
| [^276] | [High-Dimensional Independence Testing via Maximum and Average Distance Correlations](https://arxiv.org/abs/2001.01095) | 本文介绍并研究了利用最大和平均距离相关性进行高维度独立性检测的方法，并提出了一种快速卡方检验的程序。该方法适用于欧氏距离和高斯核，具有较好的实证表现和广泛的应用场景。 |
| [^277] | [Independence Testing for Temporal Data](https://arxiv.org/abs/1908.06486) | 本文介绍了一种适用于测试时序数据之间独立性的时序依赖统计方法，并能够估计最佳依赖滞后。该方法解决了现有方法的限制，并且在测试平稳时间序列之间的独立性时渐近有效和普遍一致，并且与多种依赖度量方法兼容。 |
| [^278] | [SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models.](http://arxiv.org/abs/2401.15270) | SimFair是一种物理引导的公平感知学习框架，通过集成模拟和逆向建模来解决纯数据驱动的公平性问题，有效保持公平性。 |
| [^279] | [Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar Power Generation.](http://arxiv.org/abs/2401.14422) | 提出了一种基于深度学习的域自适应框架，利用气候特征预测太阳能发电量，在不同地区间具有一定的通用性和适应性。 |
| [^280] | [MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning.](http://arxiv.org/abs/2401.14199) | 该论文引入了一种新的框架——多模态时间关系图学习（MTRGL），通过将时间序列数据和离散特征结合成一个时间图，并采用记忆机制的图神经网络来辨识实体间的时间相关性，取得了在实证实验中的优异表现。这一研究对于提升自动化配对交易策略具有重要意义。 |
| [^281] | [Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models.](http://arxiv.org/abs/2401.13227) | 本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。 |
| [^282] | [Critical Data Size of Language Models from a Grokking Perspective.](http://arxiv.org/abs/2401.10463) | 本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。 |
| [^283] | [Towards Principled Graph Transformers.](http://arxiv.org/abs/2401.10119) | 边缘变换器是一个全局注意力模型，它具有至少3-WL的表达能力，能够在预测性能上超过其他架构，而不依赖于位置或结构编码。 |
| [^284] | [Interplay between depth and width for interpolation in neural ODEs.](http://arxiv.org/abs/2401.09902) | 本文研究了神经ODE插值中深度和宽度之间的相互作用，并发现在数据集插值中存在着$p$和$L$之间的平衡折衷关系，而在测度插值中，$L$的增长与$p$和$\varepsilon$的关系有关。 |
| [^285] | [Extreme Compression of Large Language Models via Additive Quantization.](http://arxiv.org/abs/2401.06118) | 本文提出的算法在大规模语言模型的极端压缩方面取得了较好的性能，相比最新技术，在给定的压缩预算下准确性更高。 |
| [^286] | [First 100 days of pandemic; an interplay of pharmaceutical, behavioral and digital interventions -- A study using agent based modeling.](http://arxiv.org/abs/2401.04795) | 本研究通过Agent-Based模型模拟了药物、行为和数字干预的相互作用，并建议综合运用这些干预措施应对大流行疫情。通过分析发现，最初的100天对决定疫情发展至关重要，强调了迅速决策和高效政策制定的重要性。 |
| [^287] | [Sampling in Unit Time with Kernel Fisher-Rao Flow.](http://arxiv.org/abs/2401.03892) | 本文提出了一种具有核Fisher-Rao流的新方法，在单位时间内从非归一化目标密度或贝叶斯后验中进行采样。方法使用了均场ODE和相互作用粒子系统，无需梯度，只需要能够从参考密度中采样并计算目标对参考密度的比率。该方法通过在几何混合的路径上沿速度场运输样本，径向输运样本。方法通过在再生核希尔伯特空间中求解泊松方程，使泊松方程的求解变得可行，并将其离散化为有限样本的均场ODE，作为实现简单的相互作用粒子系统。同时，这种方法也可以从离散时间的角度推导出均场ODE，作为蒙杰-安普尔方程连续线性化的极限。 |
| [^288] | [On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond.](http://arxiv.org/abs/2401.03301) | 本文提出了通过数据多样性概念来统一离线强化学习算法的方法，并证明了基于版本空间、正则化优化和后验采样的算法在标准假设下达到了可比的样本效率。 |
| [^289] | [Trajectory-Oriented Policy Optimization with Sparse Rewards.](http://arxiv.org/abs/2401.02225) | 该论文提出了一种基于轨迹导向的稀疏奖励策略优化方法，通过利用离线示范轨迹，在稀疏奖励环境中实现更快速、更高效的在线强化学习。 |
| [^290] | [PAC-Bayes-Chernoff bounds for unbounded losses.](http://arxiv.org/abs/2401.01148) | 这篇论文提出了一种用于无界损失的高概率PAC-Bayes参考界限，并通过优化自由参数解决了一些开放问题，并通过灵活的假设产生了新的广义界限。 |
| [^291] | [Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency.](http://arxiv.org/abs/2312.11509) | 这个论文介绍了一种基于强化学习的系统，该系统可以根据患者言语不流畅程度自动调整药物，通过对药物组合的强化学习算法的优化，能够收敛到良好的用药方案。 |
| [^292] | [TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training.](http://arxiv.org/abs/2312.08846) | TiMix是一种将文本感知的图像混合技术用于视觉语言预训练的方法，通过集成混合数据增强技术，并从互信息的角度进行理论分析，提高了数据效率并取得了可比较的性能。 |
| [^293] | [Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers.](http://arxiv.org/abs/2311.01344) | 本文研究了如何通过简单的功率分析方法，在32位微控制器上提取深度神经网络模型的架构信息。 |
| [^294] | [Language Model Training Paradigms for Clinical Feature Embeddings.](http://arxiv.org/abs/2311.00768) | 本研究使用自监督训练范式的语言模型，通过表示学习为临床时间序列推导出高质量的通用临床特征嵌入。通过无监督的降维技术可视化学习到的嵌入，并在MIMIC-III基准测试中验证了它们的有效性。 |
| [^295] | [Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records.](http://arxiv.org/abs/2310.19917) | 本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。 |
| [^296] | [Personas as a Way to Model Truthfulness in Language Models.](http://arxiv.org/abs/2310.18168) | 本研究探讨了在大型语言模型中使用人设来建模真实性的可能性。通过建模真实人设，语言模型可以将真实性推广到不同上下文中，并通过相关特征判断个体产生文本的真实性。 |
| [^297] | [LASER: Linear Compression in Wireless Distributed Optimization.](http://arxiv.org/abs/2310.13033) | LASER是一种新的压缩方案，通过利用梯度的低秩结构在噪声通道上高效传输梯度，相对于现有方案在计算机视觉和GPT语言建模任务上表现出持续优势。 |
| [^298] | [An Empirical Study of Simplicial Representation Learning with Wasserstein Distance.](http://arxiv.org/abs/2310.10143) | 本文研究了在树结构上利用Wasserstein距离进行简化表示学习的问题，并提出了一种基于SimCLR和负TWD的自监督学习方法来估计简化表示，通过实证研究找到了稳定的训练策略。 |
| [^299] | [On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism.](http://arxiv.org/abs/2310.07852) | 本文研究了在高维稀疏线性回归模型中的差分隐私模型选择问题。我们使用指数机制进行模型选择，并提出了Metropolis-Hastings算法来克服指数搜索空间的计算复杂性。我们的算法在一定边界条件下能够实现强模型恢复性质，并具有多项式混合时间和近似差分隐私性质。 |
| [^300] | [Molecule Design by Latent Prompt Transformer.](http://arxiv.org/abs/2310.03253) | 本文提出了一种潜在提示Transformer模型，用于解决分子设计中的优化问题。该模型包括潜在向量、分子生成模型和性质预测模型，通过对现有分子进行训练后进行模型分布的逐渐转移。 |
| [^301] | [Learning to Reach Goals via Diffusion.](http://arxiv.org/abs/2310.02505) | 本论文提出了一种通过扩散学习实现目标达成的方法，可以在任意初始状态下从预定义或新目标达成，而无需学习单独的价值函数。 |
| [^302] | [OceanGPT: A Large Language Model for Ocean Science Tasks.](http://arxiv.org/abs/2310.02031) | OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。 |
| [^303] | [Energy-Guided Continuous Entropic Barycenter Estimation for General Costs.](http://arxiv.org/abs/2310.01105) | 本文提出了一种基于能量导向的方法用于近似计算任意OT成本函数的连续熵OT巴氏中心，该方法具有优越的性能，并且能与基于能量的模型（EBMs）学习过程无缝连接。 |
| [^304] | [Are Graph Neural Networks Optimal Approximation Algorithms?.](http://arxiv.org/abs/2310.00526) | 本文设计了图神经网络架构OptGNN，利用半定规划工具获得大类组合优化问题的最优近似算法。通过实证结果表明在各种数据集上超过了神经网络基线算法和传统算法，同时利用OptGNN的能力设计了一个产生优化的对偶证书的算法。 |
| [^305] | [The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing.](http://arxiv.org/abs/2309.16883) | 本文提出了一个增强随机平滑的方法，通过研究随机平滑引入的方差与分类器的Lipschitz常数和边界之间的关系，以及采用单纯形投影技术来增加认证鲁棒半径。 |
| [^306] | [P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification.](http://arxiv.org/abs/2309.08499) | 本研究提出了一种名为P-ROCKET的方法，通过在特征选择的角度删除卷积核，从而实现对时间序列分类中的随机卷积核进行剪枝。 |
| [^307] | [Deep Nonnegative Matrix Factorization with Beta Divergences.](http://arxiv.org/abs/2309.08249) | 本文提出了一种使用Beta散度的深度非负矩阵分解方法，应用于面部特征提取、文档主题识别和高光谱图像材料识别。 |
| [^308] | [DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings.](http://arxiv.org/abs/2309.02908) | 本论文介绍了一种基于历史数据、占用模式和天气条件的LSTM模型，用于准确预测建筑能耗，该模型在预测精度上表现出卓越性能。 |
| [^309] | [MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval.](http://arxiv.org/abs/2309.01516) | 多途径适配器是一个创新的框架，利用"对齐增强器"加深模态对齐，实现高可转移性，可有效减少调整参数的时间并提高零样本图像-文本检索性能。 |
| [^310] | [Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models.](http://arxiv.org/abs/2308.15812) | 本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。 |
| [^311] | [Bayesian low-rank adaptation for large language models.](http://arxiv.org/abs/2308.13111) | 本研究提出了一种名为Laplace-LoRA的贝叶斯方法，通过应用拉普拉斯近似来增强经过微调的大型语言模型的校准能力。 |
| [^312] | [Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances.](http://arxiv.org/abs/2308.11129) | 本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。 |
| [^313] | [Graph of Thoughts: Solving Elaborate Problems with Large Language Models.](http://arxiv.org/abs/2308.09687) | 想法图（GoT）是一种新的框架，它超越了现有的提示范式，通过将大型语言模型（LLM）的信息建模为任意图形，将LLM想法组合成具有协同效应的结果，提炼整个思维网络的本质，或者使用反馈环路增强思维。GoT在不同任务上展示出优势，并可以通过新的想法转换进行扩展，使LLM的推理更接近人类思维。 |
| [^314] | [Natural Language is All a Graph Needs.](http://arxiv.org/abs/2308.07134) | 本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。 |
| [^315] | [GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text.](http://arxiv.org/abs/2308.06911) | GIT-Mol是一种多模态大型语言模型，可在分子科学中处理图像、图形和文本信息。通过新提出的GIT-Former架构，该模型能够将多种模态的数据对齐到一个统一的潜在空间中。与基线相比，GIT-Mol在性质预测和分子生成有效性方面取得了显著改进。此外，该模型还可用于化合物名称识别和化学反应预测等下游任务。 |
| [^316] | [SITTA: A Semantic Image-Text Alignment for Image Captioning.](http://arxiv.org/abs/2307.05591) | SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。 |
| [^317] | [Provably Efficient UCB-type Algorithms For Learning Predictive State Representations.](http://arxiv.org/abs/2307.00405) | 这篇论文提出了第一种已知的UCB类型方法用于学习预测状态表示（PSRs），并设计了一个新的奖励项来上界t |
| [^318] | [Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition.](http://arxiv.org/abs/2306.14670) | 本文研究了机器学习模型在竞争环境下的行为，发现提高数据表示质量可能会导致供应商整体预测准确性降低，从而降低社会福利。 |
| [^319] | [Trojan Model Detection Using Activation Optimization.](http://arxiv.org/abs/2306.04877) | 本文提出了一种新颖的特洛伊模型检测方法，通过激活优化为模型创建签名，然后训练分类器来检测特洛伊模型。该方法在两个公共数据集上实现了最先进的性能。 |
| [^320] | [High-dimensional and Permutation Invariant Anomaly Detection.](http://arxiv.org/abs/2306.03933) | 该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。 |
| [^321] | [Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning.](http://arxiv.org/abs/2306.01474) | 本文提出了一种通用等变Transformer用于学习3D分子相互作用，该模型具有双层注意力模块、前馈模块和层归一化模块，每个模块都是E（3）等变的，可以有效地捕捉块级和原子级的交互，实验结果表明其在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面优于各种最先进的方法。 |
| [^322] | [Federated Learning on Heterogeneous Data via Adaptive Self-Distillation.](http://arxiv.org/abs/2305.19600) | 本文提出一种基于自适应自蒸馏的新型正则化技术来训练客户端模型，该正则化方案基于客户端本地模型预测和全局模型的相似性以及客户端的标签分布来自适应地调整客户端的训练数据。实验结果表明，该方法在各种基准数据集上优于目前流行的联邦学习方法。 |
| [^323] | [Inverse Approximation Theory for Nonlinear Recurrent Neural Networks.](http://arxiv.org/abs/2305.19190) | 该论文证明了使用RNNs逼近非线性序列关系的逆近似定理，进一步将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并提出了一个有原则的重新参数化方法来克服这些限制。 |
| [^324] | [Kernel-SSL: Kernel KL Divergence for Self-supervised Learning.](http://arxiv.org/abs/2305.17326) | 本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。 |
| [^325] | [Better Batch for Deep Probabilistic Time Series Forecasting.](http://arxiv.org/abs/2305.17028) | 该研究提出了一种新的训练方法，通过在 mini-batch 中显式地学习误差的序列相关性，来提高深度概率时间序列预测的准确性和不确定性量化。 |
| [^326] | [Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective.](http://arxiv.org/abs/2305.15611) | 本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。 |
| [^327] | [Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport.](http://arxiv.org/abs/2305.14777) | 本文提出了一种基于非平衡最优输运半对偶公式的新型生成模型，相比于OT，它具有更好的鲁棒性、稳定性和更快的收敛速度，实验结果表明其优于现有的基于OT的生成模型。 |
| [^328] | [Selective Pre-training for Private Fine-tuning.](http://arxiv.org/abs/2305.13865) | 本文提出了一个通用框架，解决在保护隐私和满足内存和推理时间要求的情况下，在公共数据集上预训练一个固定大小的模型，并在私有数据集上进行微调以最大化对下游任务的性能。框架的关键是在公共数据集的子集上进行有选择性的预训练，使公共分布靠近私有分布。 |
| [^329] | [The emergence of clusters in self-attention dynamics.](http://arxiv.org/abs/2305.05465) | 本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。 |
| [^330] | [Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards.](http://arxiv.org/abs/2304.14989) | 本文提出了Kullback-Leibler Maillard Sampling (KL-MS)算法，能够在有界奖励的多臂赌博机中实现KL空间的扩展，具有较好的渐近性能。 |
| [^331] | [MUDiff: Unified Diffusion for Complete Molecule Generation.](http://arxiv.org/abs/2304.14621) | MUDiff 是一种新的分子数据生成模型，它通过组合离散和连续扩散过程来生成全面的分子表示。使用扩散过程可以捕捉分子过程的概率本质，并探索不同因素对分子结构和性质的影响。我们的模型还包括一个新颖的图形转换器架构，用于去噪扩散过程。 |
| [^332] | [On the lifting and reconstruction of nonlinear systems with multiple attractors.](http://arxiv.org/abs/2304.11860) | 本文研究了具有多个吸引子的非线性系统的Koopman算子提升和重构机制，通过利用吸引域之间的固有对称性，只需三个自由度的线性重构就可以全局线性化系统。 |
| [^333] | [Scaling Transformer to 1M tokens and beyond with RMT.](http://arxiv.org/abs/2304.11062) | 本文介绍了一种利用循环记忆扩展BERT上下文长度的方法，成功扩展到了前所未有的200万个标记，有望增强自然语言处理中的长期依赖处理并为内存密集型应用程序实现大规模上下文处理。 |
| [^334] | [Approaching an unknown communication system by latent space exploration and causal inference.](http://arxiv.org/abs/2303.10931) | 本文通过探索无监督深度生成模型的潜在空间来发现数据中有意义的属性，提出了一种极端值因果分离 (CDEV) 的方法，应用于测试鲸鱼通信系统并发现其中存在语法。 |
| [^335] | [Out-of-Domain Robustness via Targeted Augmentations.](http://arxiv.org/abs/2302.11861) | 本文研究了为领域外泛化设计数据增强的原则，通过有针对性的增强方法，在保留鲁棒特征的同时随机化虚假的领域相关特征，提高了领域外性能。 |
| [^336] | [A Comprehensive Survey of Continual Learning: Theory, Method and Application.](http://arxiv.org/abs/2302.00487) | 本文为持续学习的全面综述，总结了持续学习的一般目标，回顾了各种持续学习方法，并强调了一些有前途的应用领域和未来研究方向。 |
| [^337] | [RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System.](http://arxiv.org/abs/2211.06108) | 本论文提出了一种基于鸟瞰视角的引导框自由物体检测系统，通过雷达和激光雷达的特征融合学习，解决了在恶劣天气下物体检测的问题。 |
| [^338] | [Quasi-Arithmetic Mixtures, Divergence Minimization, and Bregman Information.](http://arxiv.org/abs/2209.07481) | 本文提供了对准算术混合、散度最小化和Bregman信息的全面分析。通过在密度函数的单调嵌入下使用Bregman散度，我们将常见的散度函数与退火路径上的中间密度关联起来。 |
| [^339] | [Variational Inference with Gaussian Mixture by Entropy Approximation.](http://arxiv.org/abs/2202.13059) | 论文提出了一种用高斯混合分布作为参数分布的变分推断方法，通过将高斯混合的熵近似为单峰高斯的熵之和来解决多峰性的问题，并从理论上分析近似误差。 |

# 详细

[^1]: 跳过$\textbackslash n$: 一种简单的方法减少大规模视觉-语言模型中的幻觉

    Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models

    [https://rss.arxiv.org/abs/2402.01345](https://rss.arxiv.org/abs/2402.01345)

    本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。

    

    最近大规模视觉-语言模型（LVLMs）的进展展示了其在视觉信息理解与人类语言方面的令人印象深刻的能力。尽管取得了这些进展，LVLMs仍然面临多模态幻觉的挑战，例如生成与视觉信息中不存在的对象相关的文本描述。然而，多模态幻觉的根本原因仍然未被充分探索。在本文中，我们提出了一个新的视角，认为LVLMs中固有的偏见可能是幻觉的关键因素。具体而言，我们系统地确定了与段落分割符（'$\textbackslash n\textbackslash n$'）相关的语义漂移偏差，即在训练数据中，在“$\textbackslash n\textbackslash n$”之前和之后的内容经常表现出显著的语义改变。这种模式使得模型推断在“$\textbackslash n\textbackslash n$”之后的内容应明显不同于前面的内容。

    Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
    
[^2]: 弱监督学习器实现具有可证明性能保证的AI错误修正

    Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees

    [https://rss.arxiv.org/abs/2402.00899](https://rss.arxiv.org/abs/2402.00899)

    这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。

    

    我们提出了一种新的方法来处理AI错误，通过引入具有先验性能保证的弱监督AI错误修正器。这些AI修正器是辅助映射，其作用是通过批准或拒绝以调节之前构建的底层分类器的决策。拒绝一个决策可以用作建议放弃做出决策的信号。该工作的一个关键技术重点是通过对错误决策的概率界限提供这些新的AI修正器的性能保证。这些界限是分布不可知的，并且不依赖于对数据维度的假设。我们的实证示例说明了该框架如何应用于改善在训练数据稀缺的具有挑战性的真实世界任务中图像分类器的性能。

    We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
    
[^3]: HarmBench：用于自动红队和强大拒绝的标准化评估框架

    HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal

    [https://arxiv.org/abs/2402.04249](https://arxiv.org/abs/2402.04249)

    HarmBench是一个为自动红队和强大拒绝设计的标准化评估框架，通过对18种红队方法和33个目标LLM和防御的比较，得出了新的见解，并介绍了一种高效的对抗训练方法，提高了LLM在各种攻击下的稳健性。

    

    自动红队具有发现和减轻大型语言模型（LLM）恶意使用的风险的巨大潜力，然而该领域缺乏一个标准化的评估框架来严格评估新方法。为了解决这个问题，我们引入了HarmBench，一个用于自动红队的标准化评估框架。我们在红队评估中确定了几个以前未考虑的有吸引力的特性，并系统地设计了HarmBench以满足这些标准。使用HarmBench，我们对18种红队方法和33个目标LLM和防御进行了大规模比较，得到了新的见解。我们还引入了一种高效的对抗训练方法，显著增强了LLM在各种攻击下的稳健性，展示了HarmBench如何促进攻击和防御的共同开发。我们在https://github.com/centerforaisafety/HarmBench上开源了HarmBench。

    Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
    
[^4]: Mamba能学习如何学习吗？对上下文学习任务进行比较研究

    Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks

    [https://arxiv.org/abs/2402.04248](https://arxiv.org/abs/2402.04248)

    本研究评估了Mamba在上下文学习任务中的性能，并与Transformer模型进行比较。结果显示，SSMs在标准回归任务中与Transformer性能相当，在稀疏奇偶学习等任务中表现优异，但在涉及非标准检索功能的任务中表现不佳。为了解决这些局限性，引入了一种混合模型。

    

    状态空间模型（SSMs）被提出作为语言建模中替代Transformer网络的选择，通过引入门控、卷积和基于输入的令牌选择来缓解多头注意力的二次成本。虽然SSMs表现出竞争性能，但与Transformer相比，它们在上下文学习（ICL）能力方面仍然是未充分探索的。在这项研究中，我们评估了SSMs的ICL性能，着重研究了Mamba在各种任务中与Transformer模型的比较。我们的结果表明，在标准回归ICL任务中，SSMs的表现与Transformer相当，而在稀疏奇偶学习等任务中超过了Transformer。然而，在涉及非标准检索功能的任务中，SSMs表现不佳。为了解决这些局限性，我们引入了一种混合模型\variant，它将Mamba与注意力库结合起来。

    State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, \variant, that combines Mamba with attention bloc
    
[^5]: 优先安全保障而非自治：科学中LLM智能机器人的风险

    Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science

    [https://arxiv.org/abs/2402.04247](https://arxiv.org/abs/2402.04247)

    本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。

    

    由大型语言模型（LLMs）驱动的智能机器人在各个学科中自主进行实验和促进科学发现方面展示了巨大的前景。尽管它们的能力非常有前途，但也引入了一些新的漏洞，需要仔细考虑安全性。然而，文献中存在显著的空白，尚未对这些漏洞进行全面探讨。本文通过对科学领域中基于LLM的机器人的漏洞进行深入研究，揭示了它们误用可能带来的潜在风险，并强调了对安全措施的需求，填补了这一空白。我们首先全面概述了科学LLM机器人固有的潜在风险，考虑了用户意图、特定的科学领域以及它们对外部环境可能造成的影响。然后，我们深入探讨了这些漏洞的起源和提供的解决方案。

    Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
    
[^6]: CAST：利用替代令牌进行聚类自注意力的高效Transformer

    CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers

    [https://arxiv.org/abs/2402.04239](https://arxiv.org/abs/2402.04239)

    CAST提出了一种利用替代令牌进行聚类自注意力的高效Transformer机制，通过减少计算复杂度，实现了在整个输入序列上的信息流动，从而提高了效率。

    

    Transformer架构已经证明是一种强大的工具，适用于广泛的任务。它基于自注意机制，这是一种本质上计算密集型的操作，其计算复杂性呈二次增长：内存使用和计算时间与输入序列的长度呈二次增加，从而限制了Transformers的应用。在这项工作中，我们提出了一种新颖的利用替代令牌进行聚类自注意力机制（CAST），以优化注意力计算，实现高效的Transformer。CAST利用可学习的替代令牌构建聚类亲和矩阵，用于对输入序列进行聚类并生成新的聚类摘要。然后将每个聚类内的自注意力与其他聚类的聚类摘要相结合，实现整个输入序列上的信息流动。CAST通过将复杂度从O(N^2)降低到O(αN)，其中N为序列长度，α为常数，从而提高了效率。

    The Transformer architecture has shown to be a powerful tool for a wide range of tasks. It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers. In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers. CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries. The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence. CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\alpha N)$ where N is the sequence length, and {\alpha} is c
    
[^7]: MusicRL: 将音乐生成对齐到人类偏好

    MusicRL: Aligning Music Generation to Human Preferences

    [https://arxiv.org/abs/2402.04229](https://arxiv.org/abs/2402.04229)

    MusicRL是第一个通过人类反馈进行优化的音乐生成系统。它使用强化学习通过离散音频令牌的自回归模型进行微调，以最大化序列级奖励。在部署后，通过从用户收集的配对偏好数据集来进一步优化系统。

    

    我们提出了MusicRL，这是第一个通过人类反馈进行优化的音乐生成系统。文本到音乐的模型鉴赏度特别主观，因为音乐性的概念以及文本背后的具体意图都是用户相关的（例如，“活力四溢的健身音乐”可以对应复古吉他独奏或者电子流行音乐节拍）。这不仅使得这类模型的监督训练具有挑战性，而且也需要在部署后将连续的人类反馈整合到其微调中。MusicRL是一个经过预训练的离散音频令牌自回归MusicLM（Agostinelli等人，2023）模型，通过强化学习进行微调以最大化序列级奖励。我们从选定的评价者那里设计与文本一致性和音频质量相关的奖励函数，并使用它们将MusicLM微调为MusicRL-R。我们向用户部署MusicLM并收集了一个包含30万个配对偏好的大型数据集。

    We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as "upbeat work-out music" can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning fr
    
[^8]: 无线视频缓存网络中的资源感知分层联邦学习

    Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks

    [https://arxiv.org/abs/2402.04216](https://arxiv.org/abs/2402.04216)

    通过资源感知的分层联邦学习，我们提出了一种解决方案，可以预测用户未来的内容请求，并减轻无线视频缓存网络中回程流量拥塞的问题。

    

    在无线视频缓存网络中，通过将待请求内容存储在不同级别上，可以减轻由少数热门文件的视频流量造成的回程拥塞。通常，内容服务提供商（CSP）拥有内容，用户使用其（无线）互联网服务提供商（ISP）从CSP请求其首选内容。由于这些参与方不会透露其私密信息和商业机密，传统技术可能无法用于预测用户未来需求的动态变化。出于这个原因，我们提出了一种新颖的资源感知分层联邦学习（RawHFL）解决方案，用于预测用户未来的内容请求。采用了一种实用的数据获取技术，允许用户根据其请求的内容更新其本地训练数据集。此外，由于网络和其他计算资源有限，考虑到只有一部分用户参与模型训练，我们推导出

    Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks. Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs). As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users' future demands. Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user's future content requests. A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content. Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive
    
[^9]: 变分Shapley网络：一种概率化的方法用于具有不确定性量化的自解释Shapley值

    Variational Shapley Network: A Probabilistic Approach to Self-Explaining Shapley values with Uncertainty Quantification

    [https://arxiv.org/abs/2402.04211](https://arxiv.org/abs/2402.04211)

    本研究引入了变分Shapley网络，通过概率化的方法简化了计算Shapley值的过程，并解决了估计模型边际值和处理解释可变性的挑战。

    

    Shapley值已经成为机器学习中阐明模型决策过程的基础工具。尽管它们被广泛采用并具有满足重要可解释性公理的独特能力，但在估计过程中仍然存在计算挑战，包括（i）对模型在所有可能的输入特征组合上进行评估，（ii）估计模型的边际值，以及（iii）处理解释的可变性。我们提出了一种新颖的自解释方法，显著简化了Shapley值的计算，只需要一次前向传递。鉴于Shapley值的确定性处理被认为是一种限制，我们探索了将概率框架纳入其中以捕捉解释中固有的不确定性。与其他替代方法不同，我们的技术不直接依赖于观测数据空间来估计边际值；相反，它使用从潜在的、特定于特征的嵌入空间中派生出的可适应的基线值。

    Shapley values have emerged as a foundational tool in machine learning (ML) for elucidating model decision-making processes. Despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations. We introduce a novel, self-explaining method that simplifies the computation of Shapley values significantly, requiring only a single forward pass. Recognizing the deterministic treatment of Shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations. Unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a no
    
[^10]: 非危重病患者急性肾损伤预测：回顾性外部和内部验证研究

    Acute kidney injury prediction for non-critical care patients: a retrospective external and internal validation study

    [https://arxiv.org/abs/2402.04209](https://arxiv.org/abs/2402.04209)

    这项研究开发并比较了深度学习和传统机器学习模型，用于预测非危重病患者在接下来48小时内发展为2期或更高程度的急性肾损伤。内外部验证和亚组分析表明模型的有效性和稳定性。

    

    背景：急性肾损伤（AKI），即肾排泄功能下降，在住院患者中发生的比例高达18%。AKI的进展可能导致不可逆的肾脏损伤。方法：本回顾性队列研究包括在匹兹堡大学医疗中心（UPMC）的非重症监护病房接受治疗的成年患者（n=46,815），以及在佛罗里达大学健康中心（UFH）接受治疗的成年患者（n=127,202）。我们开发并比较了深度学习和传统机器学习模型，以预测接下来48小时内进展至2期或更高程度的AKI。我们分别为各个医院训练本地模型（在UFH上训练的UFH模型，以及在UPMC上训练的UPMC模型），并使用两个医院的发展队列患者开发了一个独立的模型（UFH-UPMC模型）。我们在每个医院内外部验证了这些模型，并进行了性别和种族的亚组分析。结果：UFH和UPMC的患者中，2期或更高程度的AKI分别发生在3%（n=3,257）和8%（n=2,296）的患者中。roc积分率

    Background: Acute kidney injury (AKI), the decline of kidney excretory function, occurs in up to 18% of hospitalized admissions. Progression of AKI may lead to irreversible kidney damage. Methods: This retrospective cohort study includes adult patients admitted to a non-intensive care unit at the University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of Florida Health (UFH) (n = 127,202). We developed and compared deep learning and conventional machine learning models to predict progression to Stage 2 or higher AKI within the next 48 hours. We trained local models for each site (UFH Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a development cohort of patients from both sites (UFH-UPMC Model). We internally and externally validated the models on each site and performed subgroup analyses across sex and race. Results: Stage 2 or higher AKI occurred in 3% (n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively. Area under the rec
    
[^11]: 避免延迟节点的分散式学习中的梯度编码

    Gradient Coding in Decentralized Learning for Evading Stragglers

    [https://arxiv.org/abs/2402.04193](https://arxiv.org/abs/2402.04193)

    本文提出了一种新的基于八卦的梯度编码分散式学习方法（GOCO），以解决存在延迟节点的分散式学习问题。与基准方法相比，该方法在学习性能上具有优越性。

    

    本文考虑了存在延迟节点的分散式学习问题。尽管梯度编码技术已经被开发用于分布式学习以避免延迟节点，即设备使用冗余训练数据发送编码梯度，但是将这些技术直接应用于分散式学习场景是困难的。为了解决这个问题，我们提出了一种新的基于八卦的梯度编码分散式学习方法（GOCO）。在这种方法中，为了避免延迟节点的负面影响，参数向量使用基于随机梯度编码框架的编码梯度进行本地更新，然后以八卦方式进行平均。我们分析了GOCO在强凸损失函数下的收敛性能，并通过仿真结果证明了该方法在学习性能上相对于基准方法的优越性。

    In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.
    
[^12]: 用集成模型预测安全认证的强化学习

    Reinforcement Learning with Ensemble Model Predictive Safety Certification

    [https://arxiv.org/abs/2402.04182](https://arxiv.org/abs/2402.04182)

    本文提出了集成模型预测安全认证算法，通过将基于模型的深度强化学习与基于管道的模型预测控制相结合，通过规划来纠正学习代理的行动，最小化安全约束违规。结果显示，该方法比其他强化学习方法实现了显著较少的约束违规。

    

    强化学习算法需要探索才能学习。然而，无监督探索阻止了这些算法在安全关键任务上的部署，并限制了实际应用。在本文中，我们提出了一种名为集成模型预测安全认证的新算法，将基于模型的深度强化学习与基于管道的模型预测控制相结合，通过规划来纠正学习代理的行动，将安全约束违规降至最低。我们的方法旨在通过仅需要由安全控制器生成的离线数据来减少对实际系统的先前知识需求。我们的结果显示，与可比较的强化学习方法相比，我们可以实现显著较少的约束违规。

    Reinforcement learning algorithms need exploration to learn. However, unsupervised exploration prevents the deployment of such algorithms on safety-critical tasks and limits real-world deployment. In this paper, we propose a new algorithm called Ensemble Model Predictive Safety Certification that combines model-based deep reinforcement learning with tube-based model predictive control to correct the actions taken by a learning agent, keeping safety constraint violations at a minimum through planning. Our approach aims to reduce the amount of prior knowledge about the actual system by requiring only offline data generated by a safe controller. Our results show that we can achieve significantly fewer constraint violations than comparable reinforcement learning methods.
    
[^13]: 大型语言模型的下游任务性能的尺度律

    Scaling Laws for Downstream Task Performance of Large Language Models

    [https://arxiv.org/abs/2402.04177](https://arxiv.org/abs/2402.04177)

    本研究探讨了在转移学习环境中大型语言模型的尺度行为，发现微调数据集的大小和预训练数据与下游数据的分布一致性对下游性能有显著影响。

    

    尺度律提供了重要的见解，可以指导大型语言模型（LLM）的设计。现有研究主要集中在研究预训练（上游）损失的尺度律。然而，在转移学习环境中，LLM先在无监督数据集上进行预训练，然后在下游任务上进行微调，我们通常也关心下游性能。在这项工作中，我们研究了在转移学习环境中的尺度行为，其中LLM被微调用于机器翻译任务。具体而言，我们研究了预训练数据的选择和大小对下游性能（翻译质量）的影响，使用了两个评价指标：下游交叉熵和BLEU分数。我们的实验证明，微调数据集的大小和预训练数据与下游数据的分布一致性显著影响尺度行为。在充分一致性情况下，下游交叉熵和BLEU分数都会逐渐提升。

    Scaling laws provide important insights that can guide the design of large language models (LLMs). Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance. In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks. Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score. Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior. With sufficient alignment, both downstream cross-entropy and BLEU score improve monotonically with 
    
[^14]: 基于信息的增强学习用于情境感知交通规则例外

    Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions

    [https://arxiv.org/abs/2402.04168](https://arxiv.org/abs/2402.04168)

    本论文引入了基于信息的增强学习方法，将结构化的规则手册作为知识源，利用情境感知的奖励设计评估学习轨迹，以实现对需要控制交通规则例外的情景的学习。

    

    增强学习是一个非常活跃的研究领域，具有很多有前景的进展。然而，在自动驾驶领域，通常只研究非常简单的场景。常见的方法使用非可解释的控制命令作为动作空间，以及缺乏结构的奖励设计。在本文中，我们引入了基于信息的增强学习，将结构化的规则手册作为知识源集成进来。我们学习轨迹，并使用情境感知的奖励设计来评估它们，从而产生动态奖励，使代理能够学习需要控制交通规则例外的情况。我们的方法适用于任意增强学习模型。我们成功地展示了最近的基于模型的代理在复杂场景中的高完成率。

    Reinforcement Learning is a highly active research field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure. In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents.
    
[^15]: Tempered Calculus for ML: 应用于双曲模型嵌入的研究

    Tempered Calculus for ML: Application to Hyperbolic Model Embedding

    [https://arxiv.org/abs/2402.04163](https://arxiv.org/abs/2402.04163)

    本论文介绍了基于温和微积分的理论和工具，来改进目前在机器学习中使用的数学扭曲方法，特别强调与几何和机器学习相关的特性。

    

    大多数在机器学习中使用的数学扭曲本质上都是积分的：$f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances等。在本文中，我们揭示了一个基于温和微积分的理论和工具，可以帮助改进这些扭曲以更好地满足机器学习的要求。我们从广义的黎曼积分开始，这种积分还包括不严格可加的函数，而是更一般地是$t$-可加的，就像非极限统计力学中的情况一样。特别地，这种方法将Volterra的乘积积分作为特例。然后，我们使用（欧几里得）导数的扩展来推广基本定理。这些推广以及一系列更具体的定理为结果提供了基础，展示了如何以简单的方式专门设计、改变或改变扭曲度量的基本特性，特别强调与几何和机器学习相关的特性。

    Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc. In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements. We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics. Notably, this recovers Volterra's product integral as a special case. We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative. This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the
    
[^16]: 基于马尔可夫链的注意力模型的规范分析框架：通过马尔可夫链研究Transformer的顺序建模能力

    Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains

    [https://arxiv.org/abs/2402.04161](https://arxiv.org/abs/2402.04161)

    提出了一个新的框架，通过马尔可夫链的视角研究了注意力模型的顺序建模能力，理论上刻画了单层Transformer的损失景观并发现了全局最小值和坏局部最小值的存在。

    

    近年来，基于注意力的Transformer在包括自然语言在内的多个领域取得了巨大成功。其中一个关键因素是生成式预训练过程，模型在此过程中通过自回归的方式在大型文本语料库上进行训练。为了揭示这一现象，我们提出了一个新的框架，通过马尔可夫链的视角，允许理论和系统实验来研究Transformer的顺序建模能力。受到自然语言的马尔可夫性质的启发，我们将数据建模为一个马尔可夫源，并利用这个框架系统地研究数据分布特性、Transformer架构、学到的分布和最终模型性能之间的相互作用。特别地，我们理论上刻画了单层Transformer的损失景观，并展示了全局最小值和坏局部最小值的存在，这取决于具体的数据性质。

    In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data chara
    
[^17]: 《读玩游戏（R2-Play）: 多模态游戏指导下的决策 Transformer》

    Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction

    [https://arxiv.org/abs/2402.04154](https://arxiv.org/abs/2402.04154)

    本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示)

    

    在人工智能领域，开发一款通用智能体一直是一个长期的目标。先前的研究利用来自各种任务的大量离线数据集，在强化学习的多任务场景中表现出了出色的性能。然而，这些工作在扩展到新任务方面面临挑战。最近的方法将文本指导或视觉轨迹整合到决策网络中，提供任务特定的上下文提示，代表了一个有前途的方向。然而，观察到仅依赖于文本指导或视觉轨迹对于准确传达任务的上下文信息是不足够的。本文探索了增强智能体任务指导的形式，使其能够理解游戏指导，从而实现"读玩游戏"的能力。受到多模态指导调优在视觉任务中的成功启发，我们将基于视觉的强化学习任务视为一个长期视觉任务，并构建了一组... (内容太长，无法继续显示)

    Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
    
[^18]: 可解释的多源数据融合通过潜变量高斯过程

    Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process

    [https://arxiv.org/abs/2402.04146](https://arxiv.org/abs/2402.04146)

    这篇论文提出了一种基于潜变量高斯过程的多源数据融合框架，用于解决多个数据源之间质量和全面性差异给系统优化带来的问题。

    

    随着人工智能（AI）和机器学习（ML）的出现，各个科学和工程领域已经利用数据驱动的替代模型来建模来自大量信息源（数据）的复杂系统。这种增加导致了开发出用于执行特定功能的优越系统所需的成本和时间的显著降低。这样的替代模型往往广泛地融合多个数据来源，可能是发表的论文、专利、开放资源库或其他资源。然而，对于已知和未知的信息来源的基础物理参数的质量和全面性的差异，可能对系统优化过程产生后续影响，却没有得到充分的关注。为了解决这个问题，提出了一种基于潜变量高斯过程（LVGP）的多源数据融合框架。

    With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data). The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities. A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources. However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization. Towards resolving this issue, a multi-source data fusion framework based on Latent Variable Gaussian Process (LVGP) is proposed. The individual data sources are tagged as a characteristic cate
    
[^19]: OVOR：一种使用虚拟异常值正则化的OnePrompt方法，实现无需回顾的类增量学习

    OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning

    [https://arxiv.org/abs/2402.04129](https://arxiv.org/abs/2402.04129)

    这项研究提出了一种新的正则化方法，利用虚拟异常值来改善无需回顾的类增量学习过程中不同任务间的类别混淆问题，并且消除了额外的提示查询和组合计算开销。

    

    最近的研究表明，利用大规模预训练模型和可学习的提示，在无需回顾的类增量学习（CIL）设置中可以实现比著名的基于回顾的方法更好的性能。无需回顾的CIL方法在区分不同任务的类别时遇到困难，因为它们并未一同训练。在这项研究中，我们提出了一种基于虚拟异常值的正则化方法，通过紧缩分类器的决策边界，减轻不同任务间类别的混淆。最近的基于提示的方法通常需要一个存储各任务特定提示的集合，以防止新任务的知识覆盖先前任务的知识，从而导致额外的查询和组合适当提示的计算开销。我们在论文中揭示，可以消除这种额外开销而不牺牲准确性。我们演示了简化的基于提示的方法可以达到与先前最新状态-of-the-art方法相当的结果。

    Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones. Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together. In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate prompt from the pool. This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper. We illustrate that a simplified prompt-based method can achieve results comparable to previous state-of-the
    
[^20]: 科学语言建模：分子科学中大型语言模型的定量评估

    Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science

    [https://arxiv.org/abs/2402.04119](https://arxiv.org/abs/2402.04119)

    本研究提出了一种科学语言建模（SLM）的新方法，通过大型语言模型（LLM）来解决分子建模和设计中的挑战。通过多模态基准和实验评估，我们提供了关于模型与数据模态匹配的量化信息，同时也揭示了模型的知识学习偏好。

    

    高效的分子建模和设计对于发现和探索新型分子至关重要，而深度学习方法的引入在这个领域中产生了革命性的影响。特别是，大型语言模型（LLM）以自然语言处理（NLP）的视角为科学问题提供了一种新的方法，引入了一种名为科学语言建模（SLM）的研究范式。然而，仍然存在两个关键问题：如何量化模型与数据模态之间的匹配以及如何识别模型的知识学习偏好。为了解决这些挑战，我们提出了一个名为ChEBI-20-MM的多模态基准，并进行了1263个实验，评估了模型与数据模态的兼容性和知识获取能力。通过模态转移概率矩阵，我们为任务提供了最合适的模态的见解。此外，我们还引入了一种统计可解释的方法，通过本地化来发现特定上下文的知识映射。

    Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field. In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM). However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by locali
    
[^21]: SCAFFLSA：量化和消除联邦线性随机逼近和时间差异学习中的异质性偏差

    SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning

    [https://arxiv.org/abs/2402.04114](https://arxiv.org/abs/2402.04114)

    本文量化了联邦线性随机逼近算法中异质性偏差的影响，并提出SCAFFLSA作为一种改进方法来消除此偏差。在联邦时间差异学习中，该方法能够显著提高算法的复杂性。

    

    本文对联邦线性随机逼近算法（FedLSA）进行了非渐进分析。我们明确量化了异质代理本地训练引入的偏差，并研究了该算法的样本复杂性。我们证明了FedLSA的通信复杂性与所需精度 $\epsilon$ 呈多项式关系，这限制了联邦的好处。为了克服这一问题，我们提出了SCAFFLSA，一种新型的FedLSA变体，它使用控制变量来校正本地训练的偏差，并在不对统计异质性做出任何假设的情况下证明了其收敛性。我们将所提出的方法应用于具有线性函数逼近的联邦时间差异学习，并分析了相应的复杂性改进。

    In this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm. We show that the communication complexity of FedLSA scales polynomially with the desired precision $\epsilon$, which limits the benefits of federation. To overcome this, we propose SCAFFLSA, a novel variant of FedLSA, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity. We apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements.
    
[^22]: 在列车管理系统中使用非结构化文本进行分层延迟归因分类

    Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems

    [https://arxiv.org/abs/2402.04108](https://arxiv.org/abs/2402.04108)

    本文研究了基于事件描述的机器学习决策支持方法，用于在列车管理系统中自动分配延迟归因代码。研究发现，分层方法比扁平方法表现更好，但仍然不及手动分类的性能。

    

    欧盟指令要求对列车延迟进行系统性跟踪。在瑞典，瑞典交通管理局注册并分配适当的延迟归因代码。然而，延迟归因代码是手动分配的，这是一个复杂的任务。本文研究了基于事件描述进行延迟归因代码分配的机器学习决策支持。使用TF-IDF转换文本，并评估了两个模型，随机森林和支持向量机，与随机均匀分类器以及瑞典交通管理局的分类性能进行对比。此外，该问题被建模为分层和扁平两种方法。结果表明，分层方法比扁平方法表现更好。两种方法表现优于随机均匀分类器，但表现不及手动分类。

    EU directives stipulate a systematic follow-up of train delays. In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code. However, this delay attribution code is assigned manually, which is a complex task. In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated. The text is transformed using TF-IDF, and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration. Further, the problem is modeled as both a hierarchical and flat approach. The results indicate that a hierarchical approach performs better than a flat approach. Both approaches perform better than the random uniform classifier but perform worse than the manual classification.
    
[^23]: 英国零售市场的客户分群算法探索

    An Exploration of Clustering Algorithms for Customer Segmentation in the UK Retail Market

    [https://arxiv.org/abs/2402.04103](https://arxiv.org/abs/2402.04103)

    本文旨在开发一个客户分群模型，通过对基于英国的在线零售数据集的研究，使用RFM框架和最先进的聚类算法，以提高零售市场行业的决策过程。

    

    最近，人们对在线购买的意识显著提高，这导致了在线零售平台的出现，对于客户购买行为的更好理解的需求也因此产生。零售公司面临着处理大量客户购买的压力，这需要更准确和高效的客户分群方法。客户分群是一种市场分析工具，有助于客户中心服务，从而提高盈利能力。本文旨在开发一个客户分群模型，改善零售市场行业的决策过程。为了实现这一目标，我们使用了一个来自UCI机器学习库的基于英国的在线零售数据集。这个零售数据集包含541,909个客户记录和8个特征。我们的研究采用了RFM（最近性、频率和货币）框架来量化客户价值。然后，我们比较了几种最先进的聚类算法（SOTA）。

    Recently, peoples awareness of online purchases has significantly risen. This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour. Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation. Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability. In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry. To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository. The retail dataset consists of 541,909 customer records and eight features. Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values. Thereafter, we compared several state-of-the-art (SOTA) clustering alg
    
[^24]: 使用大型语言模型进行网络欺凌检测的研究

    The Use of a Large Language Model for Cyberbullying Detection

    [https://arxiv.org/abs/2402.04088](https://arxiv.org/abs/2402.04088)

    这项研究探索了使用大型语言模型（LLMs）如BERT和RoBERTa进行网络欺凌检测，并准备了一个新的数据集（D2）以应对高度不平衡的类别和泛化问题。

    

    社交媒体的盛行为恶意用户提供了更多的网络欺凌渠道。不幸的是，网络欺凌是当今网络世界中最普遍的现象，对公民的心理和身体健康构成严重威胁。这就需要开发一个强大的系统，以阻止在线论坛、博客和社交媒体平台上的欺凌内容，以管理其对我们社会的影响。已经提出了一些用于此目的的机器学习算法。然而，由于高度的类别不平衡和泛化问题，它们的性能并不稳定。近年来，像BERT和RoBERTa这样的大型语言模型在几个自然语言处理任务中取得了最先进的结果。不幸的是，LLM在网络欺凌检测方面尚未得到广泛应用。在我们的论文中，我们研究了使用这些模型进行网络欺凌检测。我们从现有研究（Formspring和Twitter）中准备了一个新的数据集（D2）。

    The dominance of social media has added to the channels of bullying for perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. Several machine learning (ML) algorithms have been proposed for this purpose. However, their performances are not consistent due to high class imbalance and generalisation issues. In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks. Unfortunately, the LLMs have not been applied extensively for CB detection. In our paper, we explored the use of these models for cyberbullying (CB) detection. We have prepared a new dataset (D2) from existing studies (Formspring and Twitter)
    
[^25]: 一个难以超越的基线用于无需训练的CLIP适应

    A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation

    [https://arxiv.org/abs/2402.04087](https://arxiv.org/abs/2402.04087)

    本文研究了CLIP训练方法的一个经典算法，称为高斯判别分析（GDA），并将其应用于CLIP的下游分类任务。通过对特征分布的估计，无需训练即可实现分类器，从而提高了CLIP的性能。

    

    对比语言-图像预训练（CLIP）因其显著的零样本能力而受到青睐。最近的研究集中在开发高效的微调方法，如提示学习和适配器，以提高CLIP在下游任务中的性能。然而，这些方法仍然需要额外的训练时间和计算资源，这对于资源有限的设备来说是不可取的。在本文中，我们重新审视了一个经典算法，高斯判别分析（GDA），并将其应用于CLIP的下游分类。通常，GDA假设每个类别的特征都遵循具有相同协方差的高斯分布。通过利用贝叶斯公式，可以用类别的均值和协方差来表示分类器，这可以在无需训练的情况下从数据中估计得到。为了整合来自视觉和文本模态的知识，我们将其与CLIP中的原始零样本分类器集成。在17个数据集上进行了广泛的结果实验。

    Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datas
    
[^26]: 可证学习多头注意力层

    Provably learning a multi-head attention layer

    [https://arxiv.org/abs/2402.04084](https://arxiv.org/abs/2402.04084)

    该论文介绍了可证学习多头注意力层的研究，并给出了该问题的非平凡的上下界限制。

    

    多头注意力层是变形器架构的关键组件之一，使其与传统的前馈模型有所区别。通过给定序列长度 $k$，注意力矩阵 $\mathbf{\Theta}_1,\ldots,\mathbf{\Theta}_m\in\mathbb{R}^{d\times d}$，以及投影矩阵 $\mathbf{W}_1,\ldots,\mathbf{W}_m\in\mathbb{R}^{d\times d}$，相应的多头注意力层 $F: \mathbb{R}^{k\times d}\to \mathbb{R}^{k\times d}$ 通过 $F(\mathbf{X}) \triangleq \sum^m_{i=1} \mathrm{softmax}(\mathbf{X}\mathbf{\Theta}_i\mathbf{X}^\top)\mathbf{X}\mathbf{W}_i$ 来转化长度为 $k$ 的 $d$ 维令牌序列 $\mathbf{X}\in\mathbb{R}^{k\times d}$。在这项工作中，我们开始研究通过随机示例可证学习多头注意力层，并为该问题给出了首个非平凡的上下界限制：假设 $\{\mathbf{W}_i, \mathbf{\Theta}_i\}$ 满足某些非退化条件，我们提供了一个 $(dk)^{O(m^3)}$ 时间复杂度的算法，用于学习。

    The multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models. Given a sequence length $k$, attention matrices $\mathbf{\Theta}_1,\ldots,\mathbf{\Theta}_m\in\mathbb{R}^{d\times d}$, and projection matrices $\mathbf{W}_1,\ldots,\mathbf{W}_m\in\mathbb{R}^{d\times d}$, the corresponding multi-head attention layer $F: \mathbb{R}^{k\times d}\to \mathbb{R}^{k\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\mathbf{X}\in\mathbb{R}^{k\times d}$ via $F(\mathbf{X}) \triangleq \sum^m_{i=1} \mathrm{softmax}(\mathbf{X}\mathbf{\Theta}_i\mathbf{X}^\top)\mathbf{X}\mathbf{W}_i$. In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem:   - Provided $\{\mathbf{W}_i, \mathbf{\Theta}_i\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns
    
[^27]: 一种优化的房价预测算法：XGBoost

    An Optimal House Price Prediction Algorithm: XGBoost

    [https://arxiv.org/abs/2402.04082](https://arxiv.org/abs/2402.04082)

    房价预测是房地产和抵押贷款等多个领域的基本要求，这篇论文提出了一种优化的房价预测算法XGBoost，通过比较多种机器学习技术的表现，并确定关键因素，结果表明XGBoost是最佳的模型。

    

    准确预测房价是房地产和抵押贷款等多个领域的基本要求。人们普遍认为，房产价值不仅仅由其物理属性决定，而且在很大程度上受到其周围社区的影响。在平衡预算限制的同时满足个人多样化的住房需求是房地产开发商的主要关注点。为此，我们将房价预测问题作为回归任务进行探讨，并采用了能够表达独立变量重要性的各种机器学习技术。我们使用了美国爱荷华州埃姆斯市的住房数据集，比较了支持向量回归、随机森林回归、XGBoost、多层感知器和多元线性回归算法在房价预测中的表现。随后，我们确定了影响房屋成本的关键因素。结果显示XGBoost是最佳的房价预测模型。

    An accurate prediction of house prices is a fundamental requirement for various sectors including real estate and mortgage lending. It is widely recognized that a property value is not solely determined by its physical attributes but is significantly influenced by its surrounding neighbourhood. Meeting the diverse housing needs of individuals while balancing budget constraints is a primary concern for real estate developers. To this end, we addressed the house price prediction problem as a regression task and thus employed various machine learning techniques capable of expressing the significance of independent variables. We made use of the housing dataset of Ames City in Iowa, USA to compare support vector regressor, random forest regressor, XGBoost, multilayer perceptron and multiple linear regression algorithms for house price prediction. Afterwards, we identified the key factors that influence housing costs. Our results show that XGBoost is the best performing model for house price
    
[^28]: 通过扩充改进权重空间网络的泛化能力

    Improved Generalization of Weight Space Networks via Augmentations

    [https://arxiv.org/abs/2402.04081](https://arxiv.org/abs/2402.04081)

    通过扩充权重空间的数据集，采用MixUp方法，我们改进了权重空间网络的泛化能力和性能。

    

    深度权重空间（DWS）中的学习是一个新兴的研究方向，神经网络通过处理其他神经网络的权重来进行学习，它在2D和3D神经场（INRs，NeRFs）以及对其他类型神经网络进行推理方面有广泛应用。然而，权重空间模型往往容易受到过拟合的影响。我们通过实证分析了过拟合的原因，并发现一个关键原因是DWS数据集的缺乏多样性。虽然一个给定的对象可以被许多不同的权重配置所表示，但典型的INR训练集未能捕捉到表示同一对象的不同INR之间的变异性。为了解决这个问题，我们探索了权重空间中的数据扩充策略，并提出了适用于权重空间的MixUp方法。我们在两个设置中证明了这些方法的有效性。在分类任务中，它们的性能提升类似于拥有多达10倍的数据量。在自监督对比学习中，它们产生了实质性的改进。

    Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substanti
    
[^29]: 熵正则化扩散策略与Q-集合用于离线强化学习

    Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning

    [https://arxiv.org/abs/2402.04080](https://arxiv.org/abs/2402.04080)

    本文介绍了一种熵正则化的扩散策略与Q-集合相结合的离线强化学习方法，该方法通过将一个复杂的动作分布转化为标准高斯分布，然后使用逆时间SDE采样动作，以改善离线数据集的探索能力，并通过学习Q-集合的下信心界实现更强健的策略改进。在D4RL基准任务的大多数任务上达到了最先进的性能。

    

    本文提出了训练用于离线强化学习的扩散策略的先进技术。核心是一个均值回归随机微分方程（SDE），它将复杂的动作分布转化为标准高斯分布，然后在环境状态条件下使用相应的逆时间SDE采样动作，类似于典型的扩散策略。我们展示了这样一个SDE有解，我们可以用它来计算策略的对数概率，从而得到一个熵正则项，改进了离线数据集的探索能力。为了减轻来自分布外数据点的不准确值函数的影响，我们进一步提出了学习Q-集合的下信心界以实现更强健的策略改进。通过将熵正则化扩散策略与Q-集合结合应用于离线强化学习，我们的方法在D4RL基准任务的大多数任务上达到了最先进的性能。代码可在\href{https://github.com/ruoqizzz/Entro}{https://github.com/ruoqizzz/Entro}找到。

    This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at \href{https://github.com/ruoqizzz/Entro
    
[^30]: 检索以解释：基于语言模型的证据驱动预测

    Retrieve to Explain: Evidence-driven Predictions with Language Models

    [https://arxiv.org/abs/2402.04068](https://arxiv.org/abs/2402.04068)

    检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。

    

    机器学习模型，尤其是语言模型，往往难以深入分析。黑盒模型可能掩盖了模型训练中的问题和有害偏差。对于人机协作过程来说，不透明的预测可能导致缺乏信任，限制模型的影响，即使模型的性能很好。为了解决这些问题，我们引入了检索以解释（Retrieve to Explain，简称R2E）。R2E是一种基于检索的语言模型，根据文档语料库中的证据，使用Shapley值来确定证据对最终预测的相对重要性，并根据自然语言模板将结构化数据纳入其中。R2E能够在不重新训练的情况下适应新的证据，并且能够通过模板化将结构化数据纳入到自然语言中。我们在通过分析已发表的科学文献进行药物靶点鉴定的实际案例中进行了评估，结果显示该模型在预测临床试验结果方面优于行业标准的基因学方法。

    Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
    
[^31]: 使用关系超图进行链接预测

    Link Prediction with Relational Hypergraphs

    [https://arxiv.org/abs/2402.04062](https://arxiv.org/abs/2402.04062)

    本文提出了两种适用于关系超图的链接预测框架，并通过实证分析验证了这些框架在各种关系超图基准测试上的有效性。

    

    对于使用知识图谱进行链接预测已经进行了深入的研究，导致了具有成功应用的图神经网络体系结构的丰富景观。然而，将这些体系结构的成功转移到使用关系超图进行链接预测仍然具有挑战性。关系超边的存在使得链接预测成为在不同选择的k个节点之间的任务，这比使用知识图谱进行链接预测要困难得多，因为每个关系都是二进制的（k=2）。在本文中，我们提出了两个使用关系超图进行链接预测的框架，并通过相应的关系Weisfeiler-Leman算法以及一些自然逻辑形式对生成的模型体系结构的表达能力进行了彻底分析。通过广泛的实证分析，我们验证了提出的模型体系结构在各种关系超图基准测试上的能力。

    Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model archit
    
[^32]: TopoNav：节约奖励环境中高效探索的拓扑导航

    TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments

    [https://arxiv.org/abs/2402.04061](https://arxiv.org/abs/2402.04061)

    TopoNav是一种拓扑导航框架，它通过主动拓扑映射、内部奖励机制和层次化目标优先级的组合来实现在稀疏奖励环境中高效探索。

    

    自动化机器人在未知区域的探索面临着一个重大挑战——在没有先前地图和有限外部反馈的情况下有效导航。在稀疏奖励环境中，这个挑战更加严峻，传统的探索技术往往失败。本文介绍了TopoNav，一种全新的框架，使机器人能够克服这些限制，实现高效、适应性强且目标导向的探索。TopoNav的基本构建模块是主动拓扑映射、内部奖励机制和层次化目标优先级。在探索过程中，TopoNav构建了动态拓扑地图，捕获关键位置和路径。它利用内部奖励来指导机器人朝着地图中指定的子目标前进，促进在稀疏奖励环境中的结构化探索。为了确保高效导航，TopoNav采用了分层目标驱动的主动拓扑框架，使机器人能够优先考虑最紧急的目标。

    Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration. TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization. Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways. It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings. To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immed
    
[^33]: 深度学习在多变量时间序列插补中的应用：一项调查

    Deep Learning for Multivariate Time Series Imputation: A Survey

    [https://arxiv.org/abs/2402.04059](https://arxiv.org/abs/2402.04059)

    本文调查了深度学习在多变量时间序列插补中的应用。通过综述不同的方法以及它们的优点和限制，研究了它们对下游任务性能的改进，并指出了未来研究的开放问题。

    

    普遍存在的缺失值导致多变量时间序列数据部分观测，破坏了时间序列的完整性，阻碍了有效的时间序列数据分析。最近，深度学习插补方法在提高损坏的时间序列数据质量方面取得了显著的成功，进而提高了下游任务的性能。本文对最近提出的深度学习插补方法进行了全面的调查。首先，我们提出了对这些方法进行分类的方法，并通过强调它们的优点和限制来进行了结构化的综述。我们还进行了实证实验，研究了不同方法，并比较了它们对下游任务的改进。最后，我们指出了多变量时间序列插补未来研究的开放问题。本文的所有代码和配置，包括定期维护的多变量时间序列插补论文列表，可以在以下位置找到。

    The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be foun
    
[^34]: 通过学习学习算法，实现更灵活的PAC-Bayesian元学习

    More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms

    [https://arxiv.org/abs/2402.04054](https://arxiv.org/abs/2402.04054)

    通过学习学习算法，实现更灵活的PAC-Bayesian元学习，允许更灵活的任务之间的知识转移，提供新的泛化界限，可适用于分析和设计各种元学习机制，并在实际应用中改善了预测质量。

    

    我们引入了一种使用PAC-Bayesian理论研究元学习方法的新框架。与之前的工作相比，其主要优势在于它允许在任务之间的知识转移中更具灵活性。以往的方法只能通过学习模型的先验分布间接发生。相比之下，我们证明的新的泛化界限更直接地表达了元学习的过程，即学习适用于将来任务的学习算法。我们的框架的灵活性使其适用于分析各种元学习机制甚至设计新的机制。除了我们的理论贡献外，我们还在实际元学习机制中展示了我们的框架提高了预测质量。

    We introduce a new framework for studying meta-learning methods using PAC-Bayesian theory. Its main advantage over previous work is that it allows for more flexibility in how the transfer of knowledge between tasks is realized. For previous approaches, this could only happen indirectly, by means of learning prior distributions over models. In contrast, the new generalization bounds that we prove express the process of meta-learning much more directly as learning the learning algorithm that should be used for future tasks. The flexibility of our framework makes it suitable to analyze a wide range of meta-learning mechanisms and even design new mechanisms. Other than our theoretical contributions we also show empirically that our framework improves the prediction quality in practical meta-learning mechanisms.
    
[^35]: 通过基于排列的权重匹配分析线性模式连接性

    Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching

    [https://arxiv.org/abs/2402.04051](https://arxiv.org/abs/2402.04051)

    通过基于排列的权重匹配分析线性模式连接性，我们实验证明了通过权重匹配找到的排列可以改变权重矩阵奇异向量的方向，但不能改变奇异值。这一发现对于理解随机梯度下降的有效性及其在模型合并等领域的应用具有重要意义。

    

    最近，Ainsworth等人展示了使用权重匹配（WM）来最小化排列搜索模型参数中的$L_2$距离有效地识别满足线性模式连接性（LMC）的排列的方法，其中，在两个具有不同种子的独立训练模型之间的线性路径上的损失保持几乎恒定。本文通过WM提供了LMC的理论分析，这对于理解随机梯度下降的有效性及其在模型合并等领域的应用至关重要。我们首先通过实验和理论分析表明，WM找到的排列并不显着减少两个模型之间的$L_2$距离，而LMC的出现并不仅仅是由于WM本身的距离减小。然后，我们提供了理论洞见，表明排列可以改变每层权重矩阵的奇异向量的方向，但不能改变奇异值。这一发现表明，WM找到的排列主要改变了权重矩阵的方向，而不是奇异值。

    Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging. We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself. We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM mainly al
    
[^36]: 连接点：协作微调黑盒视觉语言模型

    Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models

    [https://arxiv.org/abs/2402.04050](https://arxiv.org/abs/2402.04050)

    本论文提出了一种协作微调黑盒视觉语言模型的方法，可以在只有输入提示和输出预测的情况下进行微调，提供了一个提示生成模块和一个预测优化模块，并引入了辅助的预测一致性损失进行一致优化。

    

    随着预训练的视觉语言模型（VLMs）的出现，人们在将其用于下游任务时投入了相当大的努力。尽管在设计高效的微调方法方面取得了进展，但这些方法需要访问模型的参数，而对于保护模型所有权，模型所有者通常选择将其作为黑盒提供。本文提出了一种协作微调（CraFT）方法，用于将黑盒VLMs fine-tuning到下游任务中，其中只能访问模型的输入提示和输出预测。CraFT包括两个模块，一个用于学习文本提示的提示生成模块，一个用于以残差方式增强输出预测的预测优化模块。此外，我们引入了一个辅助的预测一致性损失来促进这些模块之间的一致优化。这些模块通过一种新的协作训练算法进行优化。

    With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algor
    
[^37]: 通过节点和边属性的联合扩散，实现图形的生成建模

    Generative Modeling of Graphs via Joint Diffusion of Node and Edge Attributes

    [https://arxiv.org/abs/2402.04046](https://arxiv.org/abs/2402.04046)

    通过联合扩散节点和边属性，我们提出了一个新的图形生成模型，考虑了所有图组件，并通过注意模块和相互依赖的节点、边和邻接信息实现了更好的效果。

    

    图生成是各种工程和科学学科的基础。然而，现有的方法往往忽视了边属性的生成。然而，我们确定了一些关键应用中边属性的重要性，这使得先前的方法在这些情境中可能不适用。此外，虽然存在一些简单的适应方法，但经验调查显示它们的效果有限，因为它们没有很好地模拟图组件之间的相互作用。为了解决这个问题，我们提出了一个节点和边的联合评分模型，用于图形生成，考虑了所有图组件。我们的方法具有两个关键创新点：(i) 将节点和边属性结合在一个注意模块中，基于这两个因素生成样本；(ii) 在图形扩散过程中，节点、边和邻接信息相互依赖。我们在涉及实际和合成数据集的具有挑战性的基准测试中评估了我们的方法，其中包含边特征。

    Graph generation is integral to various engineering and scientific disciplines. Nevertheless, existing methodologies tend to overlook the generation of edge attributes. However, we identify critical applications where edge attributes are essential, making prior methods potentially unsuitable in such contexts. Moreover, while trivial adaptations are available, empirical investigations reveal their limited efficacy as they do not properly model the interplay among graph components. To address this, we propose a joint score-based model of nodes and edges for graph generation that considers all graph components. Our approach offers two key novelties: (i) node and edge attributes are combined in an attention module that generates samples based on the two ingredients; and (ii) node, edge and adjacency information are mutually dependent during the graph diffusion process. We evaluate our method on challenging benchmarks involving real-world and synthetic datasets in which edge features are cr
    
[^38]: PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network

    PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network

    [https://arxiv.org/abs/2402.04038](https://arxiv.org/abs/2402.04038)

    本文提出了一种基于PAC-Bayesian框架的方法，来研究对抗鲁棒性泛化界限问题，针对两种流行的图神经网络模型进行了分析，结果发现图上扩散矩阵的谱范数、权重的谱范数和扰动因子对模型的鲁棒泛化界限有重要影响。

    

    图神经网络（GNNs）在各种与图相关的任务中广受欢迎。然而，类似于深度神经网络，GNNs也容易受到对抗攻击。经验研究表明，对抗鲁棒性泛化在建立有效的抵御对抗攻击的防御算法方面起着关键作用。本文通过使用PAC-Bayesian框架，为两种流行的GNNs，即图卷积网络（GCN）和消息传递图神经网络，提供了对抗鲁棒泛化界限。我们的结果揭示了图上扩散矩阵的谱范数、权重的谱范数以及扰动因子对两个模型的鲁棒泛化界限的影响。我们的界限是（Liao等人，2020）中结果的非平凡推广，从标准设置扩展到对抗设置，同时避免了最大节点度的指数依赖。作为推论，我们得出更好的界限...

    Graph neural networks (GNNs) have gained popularity for various graph-related tasks. However, similar to deep neural networks, GNNs are also vulnerable to adversarial attacks. Empirical studies have shown that adversarially robust generalization has a pivotal role in establishing effective defense algorithms against adversarial attacks. In this paper, we contribute by providing adversarially robust generalization bounds for two kinds of popular GNNs, graph convolutional network (GCN) and message passing graph neural network, using the PAC-Bayesian framework. Our result reveals that spectral norm of the diffusion matrix on the graph and spectral norm of the weights as well as the perturbation factor govern the robust generalization bounds of both models. Our bounds are nontrivial generalizations of the results developed in (Liao et al., 2020) from the standard setting to adversarial setting while avoiding exponential dependence of the maximum node degree. As corollaries, we derive bette
    
[^39]: 关于图形表示可证明的隐私漏洞

    On provable privacy vulnerabilities of graph representations

    [https://arxiv.org/abs/2402.04033](https://arxiv.org/abs/2402.04033)

    研究揭示了图神经模型中的结构性漏洞，通过边重构攻击可以推断出敏感的拓扑信息，并探讨了噪声聚合机制产生的隐私图表示对该攻击的韧性。

    

    图形表示学习(GRL)对于从复杂的网络结构中提取洞见至关重要，但也引发了安全问题，因为这些表示中可能存在隐私漏洞。本文研究了图神经模型中的结构性漏洞，可以通过边重构攻击推断出敏感的拓扑信息。我们的研究主要解决了基于余弦相似度的边重构攻击(COSERA)的理论基础，并提供了理论和实证证据，证明随着图的规模增加，这种攻击可以完美地重构稀疏的Erdos Renyi图与独立随机特征。反之，我们证明了稀疏性对COSERA的有效性至关重要，通过对随机块模型的分析和实验进行了验证。最后，我们探讨了通过噪声聚合(NAG)机制产生的(可证明的)隐私图表示对COSERA攻击的韧性。我们实证了...

    Graph representation learning (GRL) is critical for extracting insights from complex network structures, but it also raises security concerns due to potential privacy vulnerabilities in these representations. This paper investigates the structural vulnerabilities in graph neural models where sensitive topological information can be inferred through edge reconstruction attacks. Our research primarily addresses the theoretical underpinnings of cosine-similarity-based edge reconstruction attacks (COSERA), providing theoretical and empirical evidence that such attacks can perfectly reconstruct sparse Erdos Renyi graphs with independent random features as graph size increases. Conversely, we establish that sparsity is a critical factor for COSERA's effectiveness, as demonstrated through analysis and experiments on stochastic block models. Finally, we explore the resilience of (provably) private graph representations produced via noisy aggregation (NAG) mechanism against COSERA. We empirical
    
[^40]: Polyp-DDPM: 基于扩散的语义息肉合成方法，以增强分割效果

    Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation

    [https://arxiv.org/abs/2402.04031](https://arxiv.org/abs/2402.04031)

    Polyp-DDPM是一种基于扩散的方法，利用分割掩码生成逼真的胃肠道息肉图像，提升了分割效果，并在图像质量和分割性能方面优于现有方法，为训练提供了高质量、多样化的合成数据集，使得分割模型达到与真实图像相比可比的效果。

    

    本研究提出了Polyp-DDPM，一种基于扩散的方法，用于在条件掩码上生成逼真的息肉图像，旨在增强胃肠道息肉的分割效果。我们的方法解决了医学图像数据限制、高昂的注释成本和隐私问题带来的挑战。通过将扩散模型条件化于分割掩码（表示异常区域的二进制掩码），Polyp-DDPM在图像质量和分割性能方面优于现有方法（Frechet Inception Distance (FID) 评分为78.47，而高于83.79的评分；Intersection over Union (IoU) 为0.7156，而基准模型合成图像为0.6694以下，真实数据为0.7067）。我们的方法生成了高质量、多样化的合成数据集用于训练，从而提升了息肉分割模型与真实图像的可比性，并提供更大的数据增强能力以改善分割效果。

    This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve seg
    
[^41]: 通过反向传播通过密度泛函理论减少量子化学数据的成本

    Reducing the Cost of Quantum Chemical Data By Backpropagating Through Density Functional Theory

    [https://arxiv.org/abs/2402.04030](https://arxiv.org/abs/2402.04030)

    使用神经网络以更快的速度近似密度泛函理论（DFT），通过直接训练NN来绕过创建数据集的问题，大大减少了量子化学数据的成本和时间。

    

    密度泛函理论（DFT）准确预测分子的量子化学性质，但复杂度为 $O(N_{\text{electrons}}^3)$。Sch\"utt等人（2019年）通过神经网络（NN）成功地以 1000 倍的速度近似DFT。在扩展到较大分子时面临的最大问题可能是DFT标签的成本。例如，创建PCQ数据集（Nakata＆Shimazaki，2017年）耗费了数年时间，而在一个星期内用于训练后续的NN。DFT通过最小化能量 $E(\cdot )$ 作为“损失函数”来标记分子。我们通过直接将 $E(\cdot )$ 作为损失函数来直接训练NN，避免了数据集的创建。相比之下，Sch\"utt等人（2019年）花费了626小时创建一个数据集，用该数据集训练他们的NN需要160小时，总共786小时；我们的方法在31小时内实现了可比较的性能。

    Density Functional Theory (DFT) accurately predicts the quantum chemical properties of molecules, but scales as $O(N_{\text{electrons}}^3)$. Sch\"utt et al. (2019) successfully approximate DFT 1000x faster with Neural Networks (NN). Arguably, the biggest problem one faces when scaling to larger molecules is the cost of DFT labels. For example, it took years to create the PCQ dataset (Nakata & Shimazaki, 2017) on which subsequent NNs are trained within a week. DFT labels molecules by minimizing energy $E(\cdot )$ as a "loss function." We bypass dataset creation by directly training NNs with $E(\cdot )$ as a loss function. For comparison, Sch\"utt et al. (2019) spent 626 hours creating a dataset on which they trained their NN for 160h, for a total of 786h; our method achieves comparable performance within 31h.
    
[^42]: 正凹深度均衡模型

    Positive concave deep equilibrium models

    [https://arxiv.org/abs/2402.04029](https://arxiv.org/abs/2402.04029)

    引入了一种新颖的正凹深度均衡（pcDEQ）模型，通过引入非负权重和凹函数的激活函数，确保了模型的存在性和唯一性。

    

    深度均衡（DEQ）模型被广泛认为是标准神经网络的一种内存效率高的替代方案，在语言建模和计算机视觉任务中实现了最先进的性能。这些模型通过求解一个固定点方程而不是显式地计算输出，使它们与标准神经网络有所不同。然而，现有的DEQ模型往往缺乏对固定点存在性和唯一性的形式保证，并且计算固定点的数值方案的收敛性也没有得到形式上的证明。因此，在实践中DEQ模型可能是不稳定的。为了解决这些缺点，我们引入了一种新颖的DEQ模型类，称为正凹深度均衡（pcDEQ）模型。我们的方法基于非线性Perron-Frobenius理论，强制施加非负权重和在正半轴上是凹函数的激活函数。通过引入这些约束，我们可以轻松地确保固定点的存在性和唯一性。

    Deep equilibrium (DEQ) models are widely recognized as a memory efficient alternative to standard neural networks, achieving state-of-the-art performance in language modeling and computer vision tasks. These models solve a fixed point equation instead of explicitly computing the output, which sets them apart from standard neural networks. However, existing DEQ models often lack formal guarantees of the existence and uniqueness of the fixed point, and the convergence of the numerical scheme used for computing the fixed point is not formally established. As a result, DEQ models are potentially unstable in practice. To address these drawbacks, we introduce a novel class of DEQ models called positive concave deep equilibrium (pcDEQ) models. Our approach, which is based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and activation functions that are concave on the positive orthant. By imposing these constraints, we can easily ensure the existence and uniqueness of the fi
    
[^43]: AlbNews：阿尔巴尼亚语主题建模标题语料库

    AlbNews: A Corpus of Headlines for Topic Modeling in Albanian

    [https://arxiv.org/abs/2402.04028](https://arxiv.org/abs/2402.04028)

    本文介绍了一个包含600个主题标签的新闻标题和2600个未标签的阿尔巴尼亚语数据集，该数据集对于低资源语言的自然语言处理研究非常有价值，同时也证实了基本模型的优越性能。

    

    阿尔巴尼亚语等低资源语言的文本语料库稀缺，这对自然语言处理任务的研究是一个严重的障碍。本文介绍了AlbNews，一个包含600个主题标签的新闻标题和2600个未标签的阿尔巴尼亚语数据集。这些数据可用于进行主题建模研究。我们报告了使用AlbNews样本训练的一些传统机器学习分类器的初始分类得分。这些结果表明基本模型胜过集成学习模型，并可作为未来实验的基准。

    The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks. This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian. The data can be freely used for conducting topic modeling research. We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples. These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments.
    
[^44]: 一种从状态空间模型到紧支持基的核分组的通用理论

    A General Theory for Kernel Packets: from state space model to compactly supported basis

    [https://arxiv.org/abs/2402.04022](https://arxiv.org/abs/2402.04022)

    该论文提出了一种从状态空间模型到紧支持基的核分组的通用理论，该理论可以用于降低高斯过程的训练和预测时间，并且通过适当的线性组合产生了$m$个紧支持的核分组函数。

    

    众所周知，高斯过程（GP）的状态空间（SS）模型公式可以将其训练和预测时间降低到O（n）（n为数据点个数）。我们证明了一个m维的GP的SS模型公式等价于我们引入的一个概念，称为通用右核分组（KP）：一种用于GP协方差函数K的变换，使得对于任意$t \leq t_1$，$0 \leq j \leq m-1$和$m+1$个连续点$t_i$，都满足$\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$，其中${D}_t^{(j)}f(t)$表示在$t$上作用的第j阶导数。我们将这个思想扩展到了GP的向后SS模型公式，得到了下一个$m$个连续点的左核分组的概念：$\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$，对于任意$t\geq t_{2m}$。通过结合左右核分组，可以证明这些协方差函数的适当线性组合产生了$m$个紧支持的核分组函数：对于任意$t\not\in(t_0,t_{2m})$和$j=0,\cdots,m-1$，$\phi^{(j)}(t)=0$。

    It is well known that the state space (SS) model formulation of a Gaussian process (GP) can lower its training and prediction time both to O(n) for n data points. We prove that an $m$-dimensional SS model formulation of GP is equivalent to a concept we introduce as the general right Kernel Packet (KP): a transformation for the GP covariance function $K$ such that $\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j \leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. We extend this idea to the backward SS model formulation of the GP, leading to the concept of the left KP for next $m$ consecutive points: $\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\geq t_{2m}$. By combining both left and right KPs, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported KP functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and $j=0,\cdots,m-1$
    
[^45]: 探究人口和就业特征对卡车流量的影响：对NextGen NHTS出发地-目的地数据的分析

    Exploring the Effects of Population and Employment Characteristics on Truck Flows: An Analysis of NextGen NHTS Origin-Destination Data

    [https://arxiv.org/abs/2402.04019](https://arxiv.org/abs/2402.04019)

    本研究通过分析NextGen NHTS出发地-目的地数据集，探究人口和就业特征对卡车流量的影响。这些研究结果对于改善交通规划和投资决策非常关键。

    

    卡车运输因其灵活性（能够方便地到达提取和卸货点）和更快的交货速度等优势，仍然是美国货物运输的主要方式。由于卡车运输承载了大量货物，了解人口和就业特征对卡车流量的影响对于更好的交通规划和投资决策至关重要。美国联邦公路管理局作为Next Generation National Household Travel Survey项目的一部分，公布了一组卡车出行起-止点数据。该数据集包含2020年在每个州和华盛顿特区内583个预定义区域之间的卡车出行总数。本研究通过将区域层面的人口和就业特征数据从美国人口普查局和县级商业模式数据加入到起-止点级别的卡车出行流量数据中。

    Truck transportation remains the dominant mode of US freight transportation because of its advantages, such as the flexibility of accessing pickup and drop-off points and faster delivery. Because of the massive freight volume transported by trucks, understanding the effects of population and employment characteristics on truck flows is critical for better transportation planning and investment decisions. The US Federal Highway Administration published a truck travel origin-destination data set as part of the Next Generation National Household Travel Survey program. This data set contains the total number of truck trips in 2020 within and between 583 predefined zones encompassing metropolitan and nonmetropolitan statistical areas within each state and Washington, DC. In this study, origin-destination-level truck trip flow data was augmented to include zone-level population and employment characteristics from the US Census Bureau. Census population and County Business Patterns data were 
    
[^46]: 量化近似正交循环神经网络

    Quantized Approximately Orthogonal Recurrent Neural Networks

    [https://arxiv.org/abs/2402.04012](https://arxiv.org/abs/2402.04012)

    本文提出了量化近似正交循环神经网络（QORNNs）来解决正交循环神经网络（ORNNs）中参数过多的问题，采用一种后训练量化策略和三种融入正交约束和量化权重的量化感知训练算法，取得了与s相似的结果。

    

    正交循环神经网络（ORNN）是一种吸引人的选择，用于学习涉及具有长期依赖性的时间序列的任务，由于其简单性和计算稳定性。然而，这些网络通常需要大量参数才能表现良好，这在功率受限的环境（如紧凑设备）中可能是阻碍因素。解决这个问题的一种方法是神经网络量化。构建这样的网络仍然是一个待解决的问题，其固有的不稳定性是被认可的。在本文中，我们探讨了ORNN中的循环和输入权重矩阵的量化，导致了量化近似正交RNN（QORNN）。我们研究了一种后训练量化（PTQ）策略和三种融入正交约束和量化权重的量化感知训练（QAT）算法。实证结果证明了使用QAT的优势。最高效的模型实现了与s类似的结果。

    Orthogonal recurrent neural networks (ORNNs) are an appealing option for learning tasks involving time series with long-term dependencies, thanks to their simplicity and computational stability. However, these networks often require a substantial number of parameters to perform well, which can be prohibitive in power-constrained environments, such as compact devices. One approach to address this issue is neural network quantization. The construction of such networks remains an open problem, acknowledged for its inherent instability.In this paper, we explore the quantization of the recurrent and input weight matrices in ORNNs, leading to Quantized approximately Orthogonal RNNs (QORNNs). We investigate one post-training quantization (PTQ) strategy and three quantization-aware training (QAT) algorithms that incorporate orthogonal constraints and quantized weights. Empirical results demonstrate the advantages of employing QAT over PTQ. The most efficient model achieves results similar to s
    
[^47]: 针对监督和对比学习的高效可用性攻击

    Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously

    [https://arxiv.org/abs/2402.04010](https://arxiv.org/abs/2402.04010)

    本文提出了一种高效的可用性攻击方法，可以同时针对监督学习和对比学习，通过生成难以察觉的噪音和制造不可学习的示例来防止未经授权使用数据，并实现了监督和对比学习算法的最新最坏情况的不可学习性。

    

    可用性攻击可以通过生成难以察觉的噪音和制造不可学习的示例来防止私人数据和商业数据集的未经授权使用。当监督学习算法失败时，恶意的数据收集者可能会转向对比学习算法以绕过保护。通过评估，我们发现现有的大多数方法无法同时实现监督和对比的不可学习性，这给数据保护带来了风险。与基于对比误差最小化的最新方法不同，我们在监督误差最小化或最大化框架中使用类似对比的数据增强来获得对监督和对比学习均有效的攻击。我们提出的AUE和AAP攻击在减少计算消耗的前提下，实现了监督和对比学习算法的最新最坏情况的不可学习性，展示了未来的前景。

    Availability attacks can prevent the unauthorized use of private data and commercial datasets by generating imperceptible noise and making unlearnable examples before release. Ideally, the obtained unlearnability prevents algorithms from training usable models. When supervised learning (SL) algorithms have failed, a malicious data collector possibly resorts to contrastive learning (CL) algorithms to bypass the protection. Through evaluation, we have found that most of the existing methods are unable to achieve both supervised and contrastive unlearnability, which poses risks to data protection. Different from recent methods based on contrastive error minimization, we employ contrastive-like data augmentations in supervised error minimization or maximization frameworks to obtain attacks effective for both SL and CL. Our proposed AUE and AAP attacks achieve state-of-the-art worst-case unlearnability across SL and CL algorithms with less computation consumption, showcasing prospects in re
    
[^48]: 贝叶斯不确定性用于多任务学习中的梯度聚合

    Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning

    [https://arxiv.org/abs/2402.04005](https://arxiv.org/abs/2402.04005)

    这篇论文提出了一种利用贝叶斯推断的梯度聚合方法，通过引入概率分布来量化梯度维度的不确定性，在多任务学习中获得更好的效果。

    

    随着机器学习的日益突出，需要同时执行多个推理任务的需求也在增长。为每个任务运行专用模型在计算上十分昂贵，因此对多任务学习（MTL）的兴趣也越来越大。MTL的目标是学习一个能高效解决多个任务的单个模型。通过计算每个任务的单一梯度并将它们聚合起来以获得结合的更新方向来优化MTL模型。然而，这些方法并没有考虑到一个重要的方面，即梯度维度的敏感性。在这里，我们引入了一种新颖的利用贝叶斯推断的梯度聚合方法。我们为任务特定参数放置一个概率分布，这又引起了任务梯度的分布。这些额外的有价值的信息使我们能够量化每个梯度维度中的不确定性，从而在聚合它们时将其纳入考虑。我们通过实证方法证明了我们的方法的效果。

    As machine learning becomes more prominent there is a growing demand to perform several inference tasks in parallel. Running a dedicated model for each task is computationally expensive and therefore there is a great interest in multi-task learning (MTL). MTL aims at learning a single model that solves several tasks efficiently. Optimizing MTL models is often achieved by computing a single gradient per task and aggregating them for obtaining a combined update direction. However, these approaches do not consider an important aspect, the sensitivity in the gradient dimensions. Here, we introduce a novel gradient aggregation approach using Bayesian inference. We place a probability distribution over the task-specific parameters, which in turn induce a distribution over the gradients of the tasks. This additional valuable information allows us to quantify the uncertainty in each of the gradients dimensions, which can then be factored in when aggregating them. We empirically demonstrate the
    
[^49]: 了解算法式思维链中噪声对LLM训练数据的影响

    Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought

    [https://arxiv.org/abs/2402.04004](https://arxiv.org/abs/2402.04004)

    本论文研究了链式思维中的噪声对LLM训练数据的影响，并开发了追踪整数框架来生成可定制的噪声执行跟踪。通过评估预训练模型在算法可解任务中的表现，揭示了噪声的类型和强度对任务性能的影响。

    

    在预训练和微调过程中，大型语言模型（LLMs）通常会使用数万亿个标记的文本进行训练，这些文本质量各异。在训练的两个阶段中，通常会根据启发式方法过滤掉“低质量”或“有噪声”的训练样本，然而很少有人量化地了解噪声的类型或强度如何影响下游性能。在本研究中，我们研究了链式思维（CoT）中的噪声如何影响在算法可解任务的高度控制环境下的任务性能。首先，我们开发了追踪整数（TInt）框架，用于为任意整数列表上的算术函数生成高度可定制的噪声执行跟踪。然后，我们定义了两种类型的噪声：局部形式的静态噪声，在计算CoT跟踪后应用；以及全局形式的动态噪声，在计算中传播跟踪中的错误。然后，我们评估了预训练模型在测试性能上的表现。

    During both pretraining and fine-tuning, Large Language Models (\textbf{LLMs}) are trained on trillions of tokens of text of widely varying quality. Both phases of training typically involve heuristically filtering out ``low-quality'' or \textit{noisy} training samples, yet little is known quantitatively about how the type or intensity of noise affects downstream performance. In this work, we study how noise in chain of thought (\textbf{CoT}) impacts task performance in the highly-controlled setting of algorithmically solvable tasks. First, we develop the Traced Integer (\textbf{TInt}) framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers. We then define two types of noise: \textit{static} noise, a local form of noise which is applied after the CoT trace is computed, and \textit{dynamic} noise, a global form of noise which propagates errors in the trace as it is computed. We then evaluate the test performance of pretrained mo
    
[^50]: 使用渐变草图进行训练数据归因和损失地貌研究

    Gradient Sketches for Training Data Attribution and Studying the Loss Landscape

    [https://arxiv.org/abs/2402.03994](https://arxiv.org/abs/2402.03994)

    本论文提出了一种可扩展的渐变草图算法，用于训练数据归因和损失地貌研究。作者在三个应用中展示了该方法的有效性。

    

    随机投影或渐变和Hessian向量乘积的草图在需要存储许多这些向量并保留关于它们的相对几何信息的应用中起着重要作用。两个重要场景是训练数据归因（跟踪模型对训练数据的行为），其中需要存储每个训练示例的渐变，以及Hessian的频谱研究（分析训练动态），其中需要存储多个Hessian向量乘积。虽然使用密集矩阵的草图易于实现，但它们受存储限制，不能扩展到现代神经网络。在神经网络内在维度的研究工作的推动下，我们提出并研究了可伸缩草图算法的设计空间。我们在三个应用中展示了我们方法的有效性：训练数据归因，Hessian谱分析和微调预先训练时的内在维度计算。

    Random projections or sketches of gradients and Hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry. Two important scenarios are training data attribution (tracing a model's behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the Hessian (to analyze the training dynamics), where one needs to store multiple Hessian vector products. While sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks. Motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms. We demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the Hessian spectrum and the computation of the intrinsic dimension when fine-tuning pre-tra
    
[^51]: 空间群约束的晶体生成

    Space Group Constrained Crystal Generation

    [https://arxiv.org/abs/2402.03992](https://arxiv.org/abs/2402.03992)

    本文在晶体生成中考虑了空间群约束，提出了一个更容易实现的等效形式，进一步提出了一个改进的扩散模型DiffCSP++，实验结果表明其有效性。

    

    结晶体是许多科学和工业应用的基础。虽然已经提出了各种基于学习的方法用于生成晶体，但现有方法很少考虑到空间群约束，而空间群约束对于描述晶体的几何形状和许多理想特性密切相关。然而，考虑到空间群约束是具有多样性和复杂性的。在本文中，我们将空间群约束简化为更容易手工放入生成过程的等效形式。具体而言，我们将空间群约束分为两部分：晶格矩阵不变对数空间的基础约束和分数坐标的Wyckoff位置约束。基于这些约束，我们提出了DiffCSP++，这是一种新颖的扩展了之前工作DiffCSP的扩散模型，进一步考虑了空间群约束。实验在多个数据集上进行。

    Crystals are the foundation of numerous scientific and industrial applications. While various learning-based approaches have been proposed for crystal generation, existing methods seldom consider the space group constraint which is crucial in describing the geometry of crystals and closely relevant to many desirable properties. However, considering space group constraint is challenging owing to its diverse and nontrivial forms. In this paper, we reduce the space group constraint into an equivalent formulation that is more tractable to be handcrafted into the generation process. In particular, we translate the space group constraint into two parts: the basis constraint of the invariant logarithmic space of the lattice matrix and the Wyckoff position constraint of the fractional coordinates. Upon the derived constraints, we then propose DiffCSP++, a novel diffusion model that has enhanced a previous work DiffCSP by further taking space group constraint into account. Experiments on severa
    
[^52]: 神经网络的权重衰减和类内变化小会导致低秩偏差

    Neural Rank Collapse: Weight Decay and Small Within-Class Variability Yield Low-Rank Bias

    [https://arxiv.org/abs/2402.03991](https://arxiv.org/abs/2402.03991)

    神经网络中的权重衰减和小的类内变化与低秩偏差现象有关

    

    近期在深度学习领域的研究显示了一个隐含的低秩偏差现象：深度网络中的权重矩阵往往近似为低秩，在训练过程中或从已经训练好的模型中去除相对较小的奇异值可以显著减小模型大小，同时保持甚至提升模型性能。然而，大多数关于神经网络低秩偏差的理论研究都涉及到简化的线性深度网络。在本文中，我们考虑了带有非线性激活函数和权重衰减参数的通用网络，并展示了一个有趣的神经秩崩溃现象，它将训练好的网络的低秩偏差与网络的神经崩溃特性联系起来：随着权重衰减参数的增加，网络中每一层的秩呈比例递减，与前面层的隐藏空间嵌入的类内变化成反比。我们的理论发现得到了支持。

    Recent work in deep learning has shown strong empirical and theoretical evidence of an implicit low-rank bias: weight matrices in deep networks tend to be approximately low-rank and removing relatively small singular values during training or from available trained models may significantly reduce model size while maintaining or even improving model performance. However, the majority of the theoretical investigations around low-rank bias in neural networks deal with oversimplified deep linear networks. In this work, we consider general networks with nonlinear activations and the weight decay parameter, and we show the presence of an intriguing neural rank collapse phenomenon, connecting the low-rank bias of trained networks with networks' neural collapse properties: as the weight decay parameter grows, the rank of each layer in the network decreases proportionally to the within-class variability of the hidden-space embeddings of the previous layers. Our theoretical findings are supporte
    
[^53]: 則采樣并不是魔法: 大批量大小為什麼適用於差分隱私隨機優化

    Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation

    [https://arxiv.org/abs/2402.03990](https://arxiv.org/abs/2402.03990)

    通过研究差分隐私随机梯度下降（DP-SGD）中的总梯度方差，我们发现大批次大小有助于减小則采樣引起的方差，从而提高优化效果。

    

    我們研究了批次大小對差分隱私隨機梯度下降（DP-SGD）中總梯度方差的影響，尋求對大批次大小有用性的理論解釋。由於DP-SGD是現代差分隱私深度學習的基礎，其性質已被廣泛研究，最近的工作在實踐中發現大批次大小有益。然而，對於這種好處的理論解釋目前最多只能說是啟發式的。我們首先觀察到，在DP-SGD中，總梯度方差可以分解為由則采樣和噪聲引起的方差。然後，我們證明在無限次迭代的極限情況下，有效的噪聲引起的方差對批次大小是不變的。剩下的則采樣引起的方差隨著批次大小的增大而減小，因此大批次大小減小了有效的總梯度方差。我們在數值上確認這種漸進的情況在實際環境中是相關的，當批次大小不小的時候會起作用，並且發現

    We study the effect of the batch size to the total gradient variance in differentially private stochastic gradient descent (DP-SGD), seeking a theoretical explanation for the usefulness of large batch sizes. As DP-SGD is the basis of modern DP deep learning, its properties have been widely studied, and recent works have empirically found large batch sizes to be beneficial. However, theoretical explanations of this benefit are currently heuristic at best. We first observe that the total gradient variance in DP-SGD can be decomposed into subsampling-induced and noise-induced variances. We then prove that in the limit of an infinite number of iterations, the effective noise-induced variance is invariant to the batch size. The remaining subsampling-induced variance decreases with larger batch sizes, so large batches reduce the effective total gradient variance. We confirm numerically that the asymptotic regime is relevant in practical settings when the batch size is not small, and find tha
    
[^54]: 对多个合成数据集的集成进行偏差-方差分解

    A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets

    [https://arxiv.org/abs/2402.03985](https://arxiv.org/abs/2402.03985)

    本研究通过对多个合成数据集进行偏差-方差分解，增加了对其理论理解。实验证明多个合成数据集对于高方差的下游预测器特别有益，并提供了一个简单的经验法则用于选择适当的合成数据集数量。

    

    最近的研究强调了为监督学习生成多个合成数据集的好处，包括增加准确性、更有效的模型选择和不确定性估计。这些好处在经验上有明确的支持，但对它们的理论理解目前非常有限。我们通过推导使用多个合成数据集的几种设置的偏差-方差分解，来增加理论理解。我们的理论预测，对于高方差的下游预测器，多个合成数据集将特别有益，并为均方误差和Brier分数的情况提供了一个简单的经验法则来选择合适的合成数据集数量。我们通过评估一个集成在多个合成数据集和几个真实数据集以及下游预测器上的性能来研究我们的理论在实践中的效果。结果验证了我们的理论，表明我们的洞察也在实践中具有相关性。

    Recent studies have highlighted the benefits of generating multiple synthetic datasets for supervised learning, from increased accuracy to more effective model selection and uncertainty estimation. These benefits have clear empirical support, but the theoretical understanding of them is currently very light. We seek to increase the theoretical understanding by deriving bias-variance decompositions for several settings of using multiple synthetic datasets. Our theory predicts multiple synthetic datasets to be especially beneficial for high-variance downstream predictors, and yields a simple rule of thumb to select the appropriate number of synthetic datasets in the case of mean-squared error and Brier score. We investigate how our theory works in practice by evaluating the performance of an ensemble over many synthetic datasets for several real datasets and downstream predictors. The results follow our theory, showing that our insights are also practically relevant.
    
[^55]: 在宽松假设下关于随机优化中Adam收敛性的研究

    On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions

    [https://arxiv.org/abs/2402.03982](https://arxiv.org/abs/2402.03982)

    本文研究了在宽松假设下的随机优化中Adam算法的收敛性。我们引入了一个全面的噪声模型，并证明了在这个模型下，Adam算法可以以较高的概率高效地寻找到一个稳定点。与其他随机一阶算法相比，Adam算法具有更好的自适应性能，无需调整步长和问题参数。

    

    适应性动量评估（Adam）算法在训练各种深度学习任务中非常有效。尽管如此，在非凸光滑场景下，特别是在可能存在无界梯度和仿射方差噪声的情况下，对于Adam的理论理解仍然有限。在本文中，我们研究了在这些具有挑战性条件下的普通Adam算法。我们引入了一个全面的噪声模型，该模型控制着仿射方差噪声、有界噪声和次高斯噪声。我们证明了在这个通用噪声模型下，Adam算法可以以$\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$的概率高效地寻找到一个稳定点，其中$T$表示总迭代次数，与随机一阶算法的更底效率相匹配。更重要的是，我们揭示了在相同条件下，Adam算法无需调整步长和任何问题参数，具有比随机梯度下降更好的自适应性能。

    The Adaptive Momentum Estimation (Adam) algorithm is highly effective in training various deep learning tasks. Despite this, there's limited theoretical understanding for Adam, especially when focusing on its vanilla form in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. In this paper, we study vanilla Adam under these challenging conditions. We introduce a comprehensive noise model which governs affine variance noise, bounded noise and sub-Gaussian noise. We show that Adam can find a stationary point with a $\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. More importantly, we reveal that Adam is free of tuning step-sizes with any problem-parameters, yielding a better adaptation property than the Stochastic Gradient Descent under the same conditions. We also provide 
    
[^56]: 交叉熵与标签平滑：神经崩溃的视角

    Cross Entropy versus Label Smoothing: A Neural Collapse Perspective

    [https://arxiv.org/abs/2402.03979](https://arxiv.org/abs/2402.03979)

    本研究从神经崩溃的视角研究了标签平滑，并发现模型在标签平滑训练下更快地收敛到神经崩溃解，并达到更强的神经崩溃水平。此外，标签平滑损失下的模型在相同的NC1水平下表现出加强的NC2，并可在理论上更快地收敛。

    

    标签平滑损失是深度神经网络中广泛采用的一种技术，用于减轻过拟合。本文从神经崩溃（NC）的视角研究了标签平滑，这是一个强大的经验和理论框架，用于描述训练的最后阶段模型的行为。首先，我们通过实验证明，在标签平滑训练的模型更快地收敛到神经崩溃解，并达到更强的神经崩溃水平。此外，我们还表明，在相同的NC1水平下，标签平滑损失下的模型显示出加强的NC2。这些发现为理解标签平滑损失下的性能优势和增强的模型校准提供了有价值的见解。然后，我们利用无约束特征模型推导出两种损失函数的全局最小值的闭式解，并进一步证明了标签平滑下的模型具有较低的条件数，因此在理论上更快地收敛。我们的研究综合了经验和理论的方法，为理解标签平滑的效果提供了重要的贡献。

    Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empiri
    
[^57]: 给予足够时间，人类在识别不寻常姿势中的物体上击败了深度网络

    Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given Enough Time

    [https://arxiv.org/abs/2402.03973](https://arxiv.org/abs/2402.03973)

    人类在识别不寻常姿势中的物体上表现优于深度网络，当给予足够时间时。然而，随着图像曝光时间的限制，人类的表现降至深度网络的水平，这暗示人类在识别不寻常姿势中的物体时需要额外的心理过程。此外，人类与网络之间的错误模式也存在不同。因此，我们需要进一步研究，以提高计算机视觉系统的鲁棒性水平。

    

    深度学习在几个物体识别基准上正在缩小与人类的差距。本文在涉及从不寻常视角观察物体的挑战性图像中对这一差距进行了研究。我们发现人类在识别不寻常姿势中的物体时表现出色，与先进的预训练网络（EfficientNet、SWAG、ViT、SWIN、BEiT、ConvNext）相比，这些网络在此情况下普遍脆弱。值得注意的是，随着我们限制图像曝光时间，人类的表现下降到深度网络的水平，这表明人类在识别不寻常姿势中的物体时需要额外的心理过程（需要额外的时间）。最后，我们分析了人类与网络的错误模式，发现即使在限制时间的情况下，人类与前馈深度网络也有不同。我们得出结论，需要更多的工作将计算机视觉系统带到人类视觉系统的鲁棒性水平。理解在外部情况下发生的心理过程的本质是必要的。

    Deep learning is closing the gap with humans on several object recognition benchmarks. Here we investigate this gap in the context of challenging images where objects are seen from unusual viewpoints. We find that humans excel at recognizing objects in unusual poses, in contrast with state-of-the-art pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are systematically brittle in this condition. Remarkably, as we limit image exposure time, human performance degrades to the level of deep networks, suggesting that additional mental processes (requiring additional time) take place when humans identify objects in unusual poses. Finally, our analysis of error patterns of humans vs. networks reveals that even time-limited humans are dissimilar to feed-forward deep networks. We conclude that more work is needed to bring computer vision systems to the level of robustness of the human visual system. Understanding the nature of the mental processes taking place during extr
    
[^58]: 表格数据：注意力是唯一需要的吗？

    Tabular Data: Is Attention All You Need?

    [https://arxiv.org/abs/2402.03970](https://arxiv.org/abs/2402.03970)

    本文引入了一项大规模实证研究，比较了神经网络和梯度提升决策树在表格数据上的表现，还比较了基于Transformer的架构和传统的多层感知器（MLP）与残差连接的架构。实证结果显示，神经网络在决策树方面具有竞争力，而基于Transformer的架构在表格数据集上并没有超过传统MLP架构的简化变体。

    

    深度学习彻底改变了人工智能领域，并在涉及图像和文本数据的应用中取得了令人瞩目的成就。遗憾的是，关于神经网络在结构化表格数据上的优势存在着不一致的证据。本文引入了一项大规模实证研究，比较了神经网络和梯度提升决策树在表格数据上的表现，还比较了基于Transformer的架构和传统的多层感知器（MLP）与残差连接的架构。与之前的研究相比，我们的实证发现表明神经网络在决策树方面具有竞争力。此外，我们还评估了基于Transformer的架构在表格数据集上并没有超过传统MLP架构的简化变体。因此，本文帮助研究和实践社区在未来的表格数据应用中做出明智的选择。

    Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data. Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data. In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections. In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees. Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets. As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications.
    
[^59]: 上下文学习智能体是不对称的信念更新者

    In-context learning agents are asymmetric belief updaters

    [https://arxiv.org/abs/2402.03969](https://arxiv.org/abs/2402.03969)

    本研究发现大型语言模型以不对称方式更新其信念，更多地从比预期好的结果中学到。同时，我们还发现学习关于反事实反馈时，这种效应会逆转。这些结果有助于我们理解上下文学习的工作方式，并揭示了问题框架如何影响学习过程。

    

    本文研究了大型语言模型（LLMs）的上下文学习动态，通过从认知心理学中引入的三个仪器学习任务进行适应。我们发现LLMs以不对称的方式更新其信念，并且从比预期好的结果中学到的东西比从比预期差的结果中学到的东西更多。此外，我们还发现当学习关于反事实反馈时，这种效应会逆转，并且在没有涉及代理性时消失。通过研究通过元-强化学习获得的理想化的上下文学习智能体，我们进一步证实了这些发现，发现类似的模式。总的来说，我们的结果有助于我们理解上下文学习的工作方式，突出显示问题的框架显著影响学习的过程，这也是人类认知中观察到的现象。

    We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology. We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition.
    
[^60]: 关于MPNN中特征向量的维度

    On dimensionality of feature vectors in MPNNs

    [https://arxiv.org/abs/2402.03966](https://arxiv.org/abs/2402.03966)

    这篇论文重新考察了消息传递图神经网络（MPNN）特征向量的维度问题，发现实际使用的架构与理论保证存在差距。

    

    我们重新考察了Morris等人（AAAI'19）关于消息传递图神经网络（MPNNs）与Weisfeiler-Leman（WL）同构测试在区分能力上相等的经典结果。Morris等人展示了使用ReLU激活函数和$O(n)$维特征向量的仿真结果，其中$n$是图的节点数。最近，通过将随机性引入到架构中，Aamand等人（NeurIPS'22）能够将特征向量的维度提高到$O(\log n)$，尽管以高概率保证完全模拟的开销也增加了。在所有这些构造中，为了保证与WL测试的等价性，MPNN中的特征向量维度必须随着图的大小增加。然而，实际使用的架构具有恒定维度的特征向量。因此，这些结果提供的保证与实际使用的架构特性之间存在差距。

    We revisit the classical result of Morris et al.~(AAAI'19) that message-passing graphs neural networks (MPNNs) are equal in their distinguishing power to the Weisfeiler--Leman (WL) isomorphism test.   Morris et al.~show their simulation result with ReLU activation function and $O(n)$-dimensional feature vectors, where $n$ is the number of nodes of the graph. Recently, by introducing randomness into the architecture, Aamand et al.~(NeurIPS'22) were able to improve this bound to $O(\log n)$-dimensional feature vectors, although at the expense of guaranteeing perfect simulation only with high probability.   In all these constructions, to guarantee equivalence to the WL test, the dimension of feature vectors in the MPNN has to increase with the size of the graphs. However, architectures used in practice have feature vectors of constant dimension. Thus, there is a gap between the guarantees provided by these results and the actual characteristics of architectures used in practice. In this p
    
[^61]: 用大型语言模型探索隐藏世界

    Discovery of the Hidden World with Large Language Models

    [https://arxiv.org/abs/2402.03941](https://arxiv.org/abs/2402.03941)

    通过使用大型语言模型，我们提出了COAT：因果表示助手，该助手从原始观测数据中提取潜在的因果因子，并将其转化为结构化数据，为探索隐藏世界提供了新的机会。

    

    科学起源于从已知事实和观察中发现新的因果知识。传统的因果发现方法主要依赖于高质量的测量变量，通常由人类专家提供，以找到因果关系。然而，在许多现实世界的应用中，因果变量通常无法获取。大型语言模型（LLMs）的崛起为从原始观测数据中发现高级隐藏变量提供了新的机会。因此，我们介绍了COAT：因果表示助手。COAT将LLMs作为因素提供器引入，提取出来自非结构化数据的潜在因果因子。此外，LLMs还可以被指示提供用于收集数据值（例如注释标准）的额外信息，并将原始非结构化数据进一步解析为结构化数据。注释数据将被输入到...

    Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a
    
[^62]: 完全自主调谐自旋量子比特

    Fully autonomous tuning of a spin qubit

    [https://arxiv.org/abs/2402.03931](https://arxiv.org/abs/2402.03931)

    本研究首次实现了半导体量子比特的全自动调谐，通过集成深度学习、贝叶斯优化和计算机视觉技术，在无需人工干预的情况下，成功地实现了从接地器件到拉比振荡的完全自主操作。

    

    在过去的二十年里，半导体上的量子比特研究为量子计算技术做出了重大突破。然而，大规模半导体量子电路的发展仍受限于有效调谐和操作这些电路的挑战。确定这些比特的最佳操作条件是复杂的，涉及到广泛的参数空间的探索。这提出了一个现实中的“大海捞针”问题，直到现在，由于器件变异性和制造瑕疵，这一问题仍未完全实现自动化。在本研究中，我们首次报道了半导体量子比特的全自动调谐，从接地的器件到拉比振荡，清晰地表示比特操作成功。我们在Ge/Si核/壳纳米线器件中展示了这种无需人工干预实现的自动化。我们的方法将深度学习、贝叶斯优化和计算机视觉技术进行了整合。我们期望这种自动化算法能够适用于更广泛的半导体量子电路。

    Spanning over two decades, the study of qubits in semiconductors for quantum computing has yielded significant breakthroughs. However, the development of large-scale semiconductor quantum circuits is still limited by challenges in efficiently tuning and operating these circuits. Identifying optimal operating conditions for these qubits is complex, involving the exploration of vast parameter spaces. This presents a real 'needle in the haystack' problem, which, until now, has resisted complete automation due to device variability and fabrication imperfections. In this study, we present the first fully autonomous tuning of a semiconductor qubit, from a grounded device to Rabi oscillations, a clear indication of successful qubit operation. We demonstrate this automation, achieved without human intervention, in a Ge/Si core/shell nanowire device. Our approach integrates deep learning, Bayesian optimization, and computer vision techniques. We expect this automation algorithm to apply to a wi
    
[^63]: 返回对齐的决策Transformer

    Return-Aligned Decision Transformer

    [https://arxiv.org/abs/2402.03923](https://arxiv.org/abs/2402.03923)

    本研究提出了返回对齐的决策Transformer（RADT），通过分离回报与传统输入序列，实现有效地将实际回报与目标回报对齐。

    

    传统的离线强化学习方法旨在学习最大化累积奖励（即回报）的最优策略。然而，随着应用范围的扩大，训练能够最大化回报并使实际回报与指定目标回报对齐的智能体变得越来越重要，从而控制智能体的性能。决策Transformer（DT）通过监督学习优化生成以目标回报为条件的动作的策略，并配备了使用目标回报控制智能体的机制。尽管DT旨在对齐实际回报与目标回报，但我们在实验中发现了DT中实际回报与目标回报之间的差异。在本文中，我们提出了返回对齐的决策Transformer（RADT），旨在有效地将实际回报与目标回报对齐。我们的模型将回报从传统的输入序列中分离出来，传统输入序列通常包含回报、状态和动作。

    Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent's performance. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and is equipped with a mechanism to control the agent using the target return. Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to effectively align the actual return with the target return. Our model decouples returns from the conventional input sequence, which typically consists of returns, s
    
[^64]: 用大型语言模型增强贝叶斯优化

    Large Language Models to Enhance Bayesian Optimization

    [https://arxiv.org/abs/2402.03921](https://arxiv.org/abs/2402.03921)

    通过结合大型语言模型（LLM）的能力，我们提出了一种名为LLAMBO的新方法，将其应用于贝叶斯优化（BO）。通过用自然语言描述BO问题，并利用LLM的上下文理解、少样本学习能力和领域知识，LLAMBO能够提供有前景的解决方案，并且在零样本热启动方面表现出良好的效果。

    

    贝叶斯优化（BO）是一种优化复杂和昂贵的黑盒函数的强大方法。它在许多应用中的重要性得到了强调，特别是超参数调优，但其有效性取决于有效地平衡勘探和开发。尽管在BO方法方面取得了重大进展，但平衡这一问题仍然是一个微妙的过程。在这个背景下，我们提出了一个新方法LLAMBO，它将大型语言模型（LLM）的能力与BO相结合。在高层次上，我们用自然语言的方式来描述BO问题，使LLM能够根据历史评估提出有前景的解决方案。更具体地说，我们探讨了如何结合LLM的上下文理解、少样本学习能力和领域知识，来增强基于模型的BO的各个组成部分。我们的研究结果表明，LLAMBO在零样本热启动方面是有效的，并且可以改善代理模型的性能。

    Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that \texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modelin
    
[^65]: 冷启动无示例增量学习的弹性特征整合

    Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning

    [https://arxiv.org/abs/2402.03917](https://arxiv.org/abs/2402.03917)

    这篇论文解决了冷启动场景的无示例增量学习的问题，提出了一种弹性特征整合的方法，通过规范特征漂移并利用原型来减少任务新鲜度偏差。

    

    无示例类别增量学习（EFCIL）旨在从一系列任务中学习，而不需要访问先前任务的数据。在本文中，我们考虑了具有挑战性的冷启动场景，在第一个任务中没有足够的数据来学习高质量的骨干网络。对于EFCIL来说，这是特别具有挑战性的，因为它需要高度的可塑性，这会导致特征漂移，在无示例的情况下很难进行补偿。为了解决这个问题，我们提出了一种简单而有效的方法，通过规范在与先前任务高度相关的方向上的漂移，并利用原型来减少任务新鲜度偏差，以整合特征表示。我们的方法被称为弹性特征整合（EFC），它利用基于经验特征矩阵（EFM）的可解二阶近似来处理特征漂移。EFM在特征空间中引入了伪度量，我们使用它来规范重要方向上的特征漂移，并更新高斯原型。

    Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prot
    
[^66]: 学习最大化加速A/B测试的指标

    Learning Metrics that Maximise Power for Accelerated A/B-Tests

    [https://arxiv.org/abs/2402.03915](https://arxiv.org/abs/2402.03915)

    本论文提出了一种新方法，通过从短期信号中学习指标，直接最大化指标与北极度量标准之间的统计能力，从而减少在线控制实验的成本。

    

    在技术公司中，在线控制实验是一种重要的工具，可以实现自信的决策。定义了一个北极度量标准（如长期收入或用户保留），在A/B测试中，能够在这个指标上有统计显著提升的系统变体可以被认为是优越的。然而，北极度量标准通常具有时延和不敏感性。因此，实验的成本很高：实验需要长时间运行，即使如此，二类错误（即假阴性）仍然普遍存在。为了解决这个问题，我们提出了一种从短期信号中学习指标的方法，这些指标直接最大化它们相对于北极度量标准所具有的统计能力。我们展示了现有方法容易过拟合的问题，即更高的平均度量敏感性并不意味着改进了二类错误，我们建议通过最小化指标在过去实验的$log$上产生的$p$-value来解决。我们从两个社交媒体应用程序中收集了这样的数据集。

    Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent.   We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with
    
[^67]: 使用机器学习算法进行员工流失分析

    Employee Turnover Analysis Using Machine Learning Algorithms

    [https://arxiv.org/abs/2402.03905](https://arxiv.org/abs/2402.03905)

    本文使用机器学习算法分析员工流失情况，并采用AdaBoost，SVM和RandomForest三种算法进行员工离职预测准确性的评估。

    

    员工的知识是组织的资产。流失可能会产生明显和隐藏的成本，并造成不可修复的损害。为了克服和减轻这种风险，应该监测员工的状况。由于分析员工福利特征的复杂性较高，可以将员工的离职预测交给机器学习技术。在本文中，我们讨论员工的离职率。使用AdaBoost，SVM和RandomForest三种不同的监督学习算法来评估员工离职的准确性。得到的模型可以帮助建立预测分析。

    Employee's knowledge is an organization asset. Turnover may impose apparent and hidden costs and irreparable damages. To overcome and mitigate this risk, employee's condition should be monitored. Due to high complexity of analyzing well-being features, employee's turnover predicting can be delegated to machine learning techniques. In this paper, we discuss employee's attrition rate. Three different supervised learning algorithms comprising AdaBoost, SVM and RandomForest are used to benchmark employee attrition accuracy. Attained models can help out at establishing predictive analytics.
    
[^68]: 复合回报降低强化学习中的方差

    Compound Returns Reduce Variance in Reinforcement Learning

    [https://arxiv.org/abs/2402.03903](https://arxiv.org/abs/2402.03903)

    复合回报是一种新的强化学习方法，在降低方差和提高样本效率方面具有重要的贡献和创新。

    

    多步回报，例如$n$步回报和$\lambda$回报，通常用于提高强化学习方法的样本效率。多步回报的方差成为其长度的限制因素，过度远望未来会增加方差并逆转多步学习的好处。在我们的工作中，我们展示了复合回报（$n$步回报的加权平均）降低方差的能力。我们首次证明了任何与给定$n$步回报具有相同收缩模数的复合回报的方差严格较低。我们还证明了这种降低方差的特性改善了线性函数逼近下时序差分学习的有限样本复杂性。由于一般复合回报的实施可能代价高昂，我们引入了两个自助回报，它们在保持高效性的同时降低了方差，即使在使用小批量经验回放时也是如此。我们进行了实验，显示……

    Multistep returns, such as $n$-step returns and $\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing
    
[^69]: 一个可求解的点积注意力模型中位置学习和语义学习之间的相变

    A phase transition between positional and semantic learning in a solvable model of dot-product attention

    [https://arxiv.org/abs/2402.03902](https://arxiv.org/abs/2402.03902)

    这篇论文研究了点积注意力层如何同时学习位置和语义关注，发现在高维数据和大量训练样本条件下，存在从位置机制到语义机制的相变，并提供了非凸经验损失函数全局最小值的闭式表征。

    

    我们研究了点积注意力层如何学习位置注意力矩阵（通过各自的位置决定令牌之间的关注）和语义注意力矩阵（通过意义决定令牌之间的关注）。通过算法任务的实验，我们展示了同样简单的架构如何使用位置机制或语义机制来实现解决方案。在理论上，我们研究了具有可训练的绑定和低秩查询和键矩阵的非线性自注意层的学习。在高维数据和相对较大数量的训练样本的渐近极限下，我们提供了非凸经验损失函数全局最小值的闭式表征。我们展示了这个最小值对应于位置机制或语义机制，证明了随着样本复杂性的增加，从位置机制向语义机制的自发相变。最后，我们比较了点积注意力与其他注意力机制的性能。

    We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning). For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism. On the theoretical side, we study the learning of a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product att
    
[^70]: 批处理通用预测

    Batch Universal Prediction

    [https://arxiv.org/abs/2402.03901](https://arxiv.org/abs/2402.03901)

    该论文研究了大型语言模型（LLM）在通用预测方面的性能评估，引入了批量遗憾的概念，并研究了在无记忆源和一阶马尔可夫源的情况下的渐近值。

    

    大型语言模型（LLM）因其惊人的生成类似人类英语句子的能力而近年来备受关注。LLMs本质上是预测器，通过估计给定过去的单词序列的概率来评估它们的性能。因此，从通用预测的角度评估它们的性能是自然而然的。为了公平地进行评估，我们引入了批量遗憾的概念作为经典平均遗憾的修改，并研究了在无记忆源和一阶马尔可夫源的情况下，对于增加常数预测器的渐近值。

    Large language models (LLMs) have recently gained much popularity due to their surprising ability at generating human-like English sentences. LLMs are essentially predictors, estimating the probability of a sequence of words given the past. Therefore, it is natural to evaluate their performance from a universal prediction perspective. In order to do that fairly, we introduce the notion of batch regret as a modification of the classical average regret, and we study its asymptotical value for add-constant predictors, in the case of memoryless sources and first-order Markov sources.
    
[^71]: DistiLLM: 面向大型语言模型的简化蒸馏方法

    DistiLLM: Towards Streamlined Distillation for Large Language Models

    [https://arxiv.org/abs/2402.03898](https://arxiv.org/abs/2402.03898)

    DistiLLM是一个更有效和高效的自回归语言模型蒸馏框架，通过引入新颖的偏斜Kullback-Leibler散度损失和自适应的离策略方法，解决了当前针对大语言模型的知识蒸馏方法缺乏标准化目标函数和计算成本过高的问题。

    

    知识蒸馏（KD）被广泛用于将教师模型压缩为更小的学生模型，降低推理成本和内存占用，同时保持模型能力。然而，当前针对自回归序列模型（例如大型语言模型）的KD方法存在缺乏标准化目标函数的问题。此外，最近使用学生生成的输出来解决训练-推理不匹配问题的做法显著增加了计算成本。为了解决这些问题，我们引入了DistiLLM，这是一个更有效和高效的自回归语言模型蒸馏框架。DistiLLM由两个组成部分组成：（1）一种新颖的偏斜Kullback-Leibler散度损失，我们揭示并利用了它的理论属性；（2）一种自适应的离策略方法，旨在提高利用学生生成的输出的效率。包括指令跟随任务在内的大量实验验证了DistiLLM在构建高性能模型方面的有效性。

    Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing 
    
[^72]: 自动驾驶的预测时域需求：优化安全、舒适和效率

    Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency

    [https://arxiv.org/abs/2402.03893](https://arxiv.org/abs/2402.03893)

    在自动驾驶中，我们通过研究不同预测时域对性能的影响，提出了根据其特定需求来确定最佳预测时域的框架。

    

    预测其他道路使用者的移动对于改善自动驾驶车辆(AV)的性能是有益的。然而，预测与AV性能相关的时间范围的关系仍不清楚。尽管存在大量的轨迹预测算法，但还没有研究探讨不同预测长度如何影响AV安全和其他车辆性能指标，导致预测方法的预测时域需求未定义。我们的研究填补了这一空白，通过使用先进的基于风险的预测轨迹规划器进行多次实验，模拟了长达20秒的预测。基于我们的模拟结果，我们提出了一个框架，用于根据特定AV性能标准和应用需求指定最低要求和最佳预测时域。我们的结果表明，一个

    Predicting the movement of other road users is beneficial for improving automated vehicle (AV) performance. However, the relationship between the time horizon associated with these predictions and AV performance remains unclear. Despite the existence of numerous trajectory prediction algorithms, no studies have been conducted on how varying prediction lengths affect AV safety and other vehicle performance metrics, resulting in undefined horizon requirements for prediction methods. Our study addresses this gap by examining the effects of different prediction horizons on AV performance, focusing on safety, comfort, and efficiency. Through multiple experiments using a state-of-the-art, risk-based predictive trajectory planner, we simulated predictions with horizons up to 20 seconds. Based on our simulations, we propose a framework for specifying the minimum required and optimal prediction horizons based on specific AV performance criteria and application needs. Our results indicate that a
    
[^73]: MOMENT：一个开放的时间序列基础模型家族

    MOMENT: A Family of Open Time-series Foundation Models

    [https://arxiv.org/abs/2402.03885](https://arxiv.org/abs/2402.03885)

    MOMENT是一个开放的时间序列基础模型家族，通过解决时间序列数据的挑战，编制了一个大规模的公共时间序列数据集，并设计了一个基准测试来评估有限监督场景下模型的性能。

    

    我们介绍了MOMENT，一个开源的通用时间序列分析基础模型家族。在时间序列数据的预训练大模型方面存在着一些挑战，包括：（1）缺乏一个大而有凝聚力的公共时间序列存储库，以及（2）多样的时间序列特征使得多数据集的训练变得困难。此外，这些模型的实验评估标准，特别是在资源、时间和监督有限的情况下，仍处于初级阶段。（3）为解决这些挑战，我们编制了一个大而多样的公共时间序列数据集，称为时间序列堆，以系统地解决时间序列特定的挑战，以解锁大规模的多数据集预训练。最后，我们借鉴最近的工作，设计了一个基准测试来评估有限监督场景下时间序列基础模型在不同任务和数据集上的效果。在这个基准测试上的实验证明了我们的预训练模型在少量数据的情况下的有效性。

    We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data 
    
[^74]: 在黎曼流形上进行双层优化的框架

    A Framework for Bilevel Optimization on Riemannian Manifolds

    [https://arxiv.org/abs/2402.03883](https://arxiv.org/abs/2402.03883)

    本论文提出了一个在黎曼流形上解决约束双层优化问题的框架，并提供了多种超梯度估计策略，并对其进行了研究。该框架不仅适用于确定性双层优化问题，还适用于随机双层优化问题，并且可以使用一般的回退。在各种应用中，该框架都具有很高的实用性。

    

    双层优化在各个领域的应用中越来越常见。在这项工作中，我们提出了一个在黎曼流形上约束双层优化问题变量的框架。我们提供了几种在流形上的超梯度估计策略，并研究了它们的估计误差。我们对流形上的超梯度下降算法提供了收敛性和复杂性分析。我们还将这些研究扩展到随机双层优化和使用一般的回退。我们展示了该框架在各种应用中的实用性。

    Bilevel optimization has seen an increasing presence in various domains of applications. In this work, we propose a framework for solving bilevel optimization problems where variables of both lower and upper level problems are constrained on Riemannian manifolds. We provide several hypergradient estimation strategies on manifolds and study their estimation error. We provide convergence and complexity analysis for the proposed hypergradient descent algorithm on manifolds. We also extend the developments to stochastic bilevel optimization and to the use of general retraction. We showcase the utility of the proposed framework on various applications.
    
[^75]: 几何量子机器学习用于BQP$^A$协议和潜在图分类器

    Geometric quantum machine learning of BQP$^A$ protocols and latent graph classifiers

    [https://arxiv.org/abs/2402.03871](https://arxiv.org/abs/2402.03871)

    几何量子机器学习可以用于构建具有经典模拟器无法达到的指数差异的协议，并且我们通过学习Simon算法发现了一个例子，这是BQP$^A\neq$BPP协议的示例。

    

    几何量子机器学习（GQML）旨在嵌入问题的对称性，以学习高效的求解协议。然而，关键问题是是否可以将(G)QML常规地用于构建与经典模拟器相比的指数差异的协议。本文考虑用于学习布尔函数属性的Simon问题，并显示这可以与非监督电路分类问题相关联。使用几何QML的工作流程，我们从基本原理学习Simon算法，从而发现了关于某个数据集（oracle $A$）的BQP$^A\neq$BPP协议的示例。我们的关键发现包括开发出用于嵌入布尔函数的等变特征映射，基于识别的位翻转和置换对称性的扭转，以及基于不变量观测量的测量，具有采样优势。所提出的工作流程指向了数据嵌入和经典后处理的重要性，同时保持了多样性。

    Geometric quantum machine learning (GQML) aims to embed problem symmetries for learning efficient solving protocols. However, the question remains if (G)QML can be routinely used for constructing protocols with an exponential separation from classical analogs. In this Letter we consider Simon's problem for learning properties of Boolean functions, and show that this can be related to an unsupervised circuit classification problem. Using the workflow of geometric QML, we learn from first principles Simon's algorithm, thus discovering an example of BQP$^A\neq$BPP protocol with respect to some dataset (oracle $A$). Our key findings include the development of an equivariant feature map for embedding Boolean functions, based on twirling with respect to identified bitflip and permutational symmetries, and measurement based on invariant observables with a sampling advantage. The proposed workflow points to the importance of data embeddings and classical post-processing, while keeping the vari
    
[^76]: 针对物理引导神经网络的非线性阶段的挑战

    The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks

    [https://arxiv.org/abs/2402.03864](https://arxiv.org/abs/2402.03864)

    本文研究了针对物理引导神经网络（PINNs）在非线性偏微分方程（PDEs）求解中的挑战。通过理论分析和数值实验，我们说明了NTK在不同线性微分算子下的行为，并强调了采用二阶方法训练PINNs的优势。我们还探讨了二阶方法的收敛能力，并解决了谱偏差和收敛缓慢的问题。通过大量的数值实验和基准测试用例，我们验证了我们的方法的有效性。

    

    神经切向核（NTK）视角是研究无限宽度情况下物理引导神经网络（PINNs）训练动态的有价值方法。我们利用这个视角，并着重研究PINNs求解非线性偏微分方程（PDEs）的情况。我们提供了关于NTK在不同线性微分算子下的不同行为的理论结果。此外，根据我们的理论结果，我们强调采用二阶方法训练PINNs的优势。此外，我们还探讨了二阶方法的收敛能力，并解决了谱偏差和收敛缓慢的挑战。所有的理论结果都得到了线性和非线性PDEs的数值示例的支持，我们还通过基准测试用例验证了我们的训练方法。

    The Neural Tangent Kernel (NTK) viewpoint represents a valuable approach to examine the training dynamics of Physics-Informed Neural Networks (PINNs) in the infinite width limit. We leverage this perspective and focus on the case of nonlinear Partial Differential Equations (PDEs) solved by PINNs. We provide theoretical results on the different behaviors of the NTK depending on the linearity of the differential operator. Moreover, inspired by our theoretical results, we emphasize the advantage of employing second-order methods for training PINNs. Additionally, we explore the convergence capabilities of second-order methods and address the challenges of spectral bias and slow convergence. Every theoretical result is supported by numerical examples with both linear and nonlinear PDEs, and we validate our training method on benchmark test cases.
    
[^77]: 《定位论文：探索研究模型表示的新框架》

    Position Paper: Toward New Frameworks for Studying Model Representations

    [https://arxiv.org/abs/2402.03855](https://arxiv.org/abs/2402.03855)

    通过逆向工程AI模型的确切算法，机制解释性（MI）旨在理解模型。然而，目前的研究主要关注微不足道的行为和能力，而忽视了隐藏在网络内部的表示。因此，我们呼吁研究界朝着新的框架努力，研究这些表示。

    

    机制解释性（MI）旨在通过逆向工程AI模型学习的确切算法来理解模型。迄今为止，大多数MI研究的行为和能力都是微不足道的和符号对齐的。然而，大多数能力并不那么微不足道，这为研究网络内部的隐藏表示作为分析单位提供了支持。我们进行了文献回顾，对特征和行为进行了形式化的表示，强调了它们的重要性和评估，并进行了一些基本的探索性研究。通过讨论和探索性结果，我们证明了研究表示是一个重要且未被充分研究的领域，并且目前MI中建立的方法不足以理解表示，因此推动研究界朝着研究表示的新框架努力。

    Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations. With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations.
    
[^78]: 提高异常检测的高效隐藏异常点生成方法

    Efficient Generation of Hidden Outliers for Improved Outlier Detection

    [https://arxiv.org/abs/2402.03846](https://arxiv.org/abs/2402.03846)

    提出了一种新的异常点生成方法BISECT，具有更好的效率和效果，可以创建具有真实行为的异常点。使用BISECT生成的合成异常点可有效增强多种数据集的异常检测，例如，在与基线比较时，使用BISECT进行过采样将错误率降低了最多3倍。

    

    异常点生成是解决重要异常检测任务的常用技术。生成具有真实行为的异常点具有挑战性。现有的流行方法往往忽视高维空间中异常点的“多视图”属性。唯一考虑到此属性的现有方法在效率和效果上存在不足。我们提出了一种名为BISECT的新的异常点生成方法，该方法可以创建具有真实行为且模仿该属性的异常点。为了实现这一点，BISECT采用了本文介绍的一种新颖的命题，说明如何高效地生成这些真实异常点。与当前重新创建“多视图”的方法相比，我们的方法具有更好的保证和复杂性。我们使用BISECT生成的合成异常点有效地增强了各种数据集的异常检测，适用于多种用例。例如，与基线相比，使用BISECT进行过采样将错误率降低了最多3倍。

    Outlier generation is a popular technique used for solving important outlier detection tasks. Generating outliers with realistic behavior is challenging. Popular existing methods tend to disregard the 'multiple views' property of outliers in high-dimensional spaces. The only existing method accounting for this property falls short in efficiency and effectiveness. We propose BISECT, a new outlier generation method that creates realistic outliers mimicking said property. To do so, BISECT employs a novel proposition introduced in this article stating how to efficiently generate said realistic outliers. Our method has better guarantees and complexity than the current methodology for recreating 'multiple views'. We use the synthetic outliers generated by BISECT to effectively enhance outlier detection in diverse datasets, for multiple use cases. For instance, oversampling with BISECT reduced the error by up to 3 times when compared with the baselines.
    
[^79]: 关于扩散模型中的规范自由度、保守性和内在维度估计

    On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models

    [https://arxiv.org/abs/2402.03845](https://arxiv.org/abs/2402.03845)

    这项研究涉及到扩散模型中的规范自由度、保守性和内在维度估计。现有研究在实际应用中常将向量场实现为神经网络函数，而不是限制为能量函数的梯度，导致关于此约束是否提高性能的研究结果矛盾。

    

    扩散模型是生成模型，最近在高维度的采样质量和密度估计方面表现出令人印象深刻的性能。它们依赖于一个正向连续扩散过程和一个反向连续去噪过程，可以用一个时变向量场来描述，并用作一个生成模型。在扩散模型的原始定义中，该向量场被假设为分数函数（即在扩散过程中某一特定时间的概率对数的梯度）。有趣的是，在实际应用中，大多数关于扩散模型的研究将这个向量场实现为神经网络函数，并且不限制其为某种能量函数的梯度（也就是说，大多数研究并不约束向量场是保守的）。尽管一些研究从实证角度考察这种约束是否会带来性能的提升，但它们得出了矛盾的结果，并未提供分析结果。

    Diffusion models are generative models that have recently demonstrated impressive performances in terms of sampling quality and density estimation in high dimensions. They rely on a forward continuous diffusion process and a backward continuous denoising process, which can be described by a time-dependent vector field and is used as a generative model. In the original formulation of the diffusion model, this vector field is assumed to be the score function (i.e. it is the gradient of the log-probability at a given time in the diffusion process). Curiously, on the practical side, most studies on diffusion models implement this vector field as a neural network function and do not constrain it be the gradient of some energy function (that is, most studies do not constrain the vector field to be conservative). Even though some studies investigated empirically whether such a constraint will lead to a performance gain, they lead to contradicting results and failed to provide analytical resul
    
[^80]: 带有切片Wasserstein Weisfeiler-Lehman图核的高斯过程回归

    Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels

    [https://arxiv.org/abs/2402.03838](https://arxiv.org/abs/2402.03838)

    本研究提出了一种带有切片Wasserstein Weisfeiler-Lehman图核的高斯过程回归方法，在处理大规模稀疏图形数据集时具有正定性和显著的复杂度降低。

    

    监督学习在计算物理领域引起了广泛关注，因为它能够有效地提取复杂模式，用于解决偏微分方程或预测材料性质等任务。传统上，这类数据集由具有大量节点的网格表示的输入（视为图形）和使用数值求解器获得的相应输出组成。这意味着监督学习模型必须能够处理具有连续节点属性的大规模稀疏图形。在本研究中，我们专注于高斯过程回归，引入了切片Wasserstein Weisfeiler-Lehman（SWWL）图核。与现有的图核相比，所提出的SWWL核具有正定性和显著的复杂度降低，使其能够处理此前不可处理的数据集。新的核首先在分子图分类中进行了验证。

    Supervised learning has recently garnered significant attention in the field of computational physics due to its ability to effectively extract complex patterns for tasks like solving partial differential equations, or predicting material properties. Traditionally, such datasets consist of inputs given as meshes with a large number of nodes representing the problem geometry (seen as graphs), and corresponding outputs obtained with a numerical solver. This means the supervised learning model must be able to handle large and sparse graphs with continuous node attributes. In this work, we focus on Gaussian process regression, for which we introduce the Sliced Wasserstein Weisfeiler-Lehman (SWWL) graph kernel. In contrast to existing graph kernels, the proposed SWWL kernel enjoys positive definiteness and a drastic complexity reduction, which  makes it possible to process datasets that were previously impossible to handle. The new kernel is first validated on graph classification for molec
    
[^81]: 使用神经最优传输估计分布的重心

    Estimating Barycenters of Distributions with Neural Optimal Transport

    [https://arxiv.org/abs/2402.03828](https://arxiv.org/abs/2402.03828)

    本研究借助最优传输的对偶形式，提出了一种新的可扩展方法，用于估计分布的重心。通过神经最优传输求解器，我们实现了双层对抗学习并适用于一般成本函数。这个方法的关键优势在于不需要使用三层优化，并且适用于多种成本函数。我们还建立了理论误差界，并在示例场景和图像数据上展示了方法的应用性和有效性。

    

    鉴于一组概率分布，从业者有时需要找到一种“平均”分布，以充分聚合参考分布。一个理论上有吸引力的平均分布概念是Wasserstein重心，而我们的工作重点就是它。通过建立在最优传输的对偶形式之上，我们提出了一种新的可扩展方法来解决Wasserstein重心问题。我们的方法基于最近的神经最优传输求解器：它具有双层对抗学习目标，并适用于一般的成本函数。这些是我们方法的关键优势，因为典型的利用重心任务的对抗性算法往往利用三层优化，并且主要集中在二次成本上。我们还为我们提出的方法建立了理论误差界，并展示了它在示例场景和图像数据设置上的适用性和有效性。

    Given a collection of probability measures, a practitioner sometimes needs to find an "average" distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions. These are key advantages of our method, since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups.
    
[^82]: RevOrder：一种增强语言模型中算术运算的新方法

    RevOrder: A Novel Method for Enhanced Arithmetic in Language Models

    [https://arxiv.org/abs/2402.03822](https://arxiv.org/abs/2402.03822)

    本文提出了一种名为RevOrder的新方法，通过翻转加法、减法和nD乘以1D的输出数字，显著改善了语言模型中的算术运算。经过全面测试，RevOrder在基本算术运算中达到了完美准确度，并在除法任务中提升了语言模型性能，特别是在处理大数时。在GSM8K数学任务中应用RevOrder进行微调，有效降低了错误率并提高了总体得分。

    

    本文提出了RevOrder，一种旨在改善大型语言模型中算术运算的新技术。该方法通过翻转加法、减法和n位数乘以1位数（nD乘以1D）的输出数字，显著降低了顺序中间数字的数量 (CSID)，这是我们引入的一种评估方程复杂性的新度量。通过全面的测试，RevOrder不仅在基本的算术运算中达到了完美的准确度，而且在除法任务中显著提升了语言模型的性能，特别是在传统模型难以处理的大数情况下。RevOrder的实现对于训练和推理阶段都具有成本效益。此外，将RevOrder应用于对GSM8K数学任务进行微调的LLaMA2-7B模型中，取得了显著的改善，将方程计算错误率降低了46%，将总体得分从41.6提升到44.4。

    This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.
    
[^83]: SMOTE的理论和实验研究：关于重新平衡策略的限制和比较

    Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies

    [https://arxiv.org/abs/2402.03819](https://arxiv.org/abs/2402.03819)

    SMOTE是一种处理不平衡数据集的常用重新平衡策略，它通过复制原始少数样本来重新生成原始分布。本研究证明了SMOTE的密度在少数样本分布的边界附近逐渐减小，从而验证了BorderLine SMOTE策略的合理性。此外，研究还提出了两种新的SMOTE相关策略，并与其他重新平衡方法进行了比较。最终发现，在数据集极度不平衡的情况下，SMOTE、提出的方法或欠采样程序是最佳的策略。

    

    SMOTE（Synthetic Minority Oversampling Technique）是处理不平衡数据集常用的重新平衡策略。我们证明了在渐进情况下，SMOTE（默认参数）通过简单复制原始少数样本来重新生成原始分布。我们还证明了在少数样本分布的支持边界附近，SMOTE的密度会减小，从而验证了常见的BorderLine SMOTE策略。随后，我们提出了两种新的SMOTE相关策略，并将它们与现有的重新平衡方法进行了比较。我们发现，只有当数据集极度不平衡时才需要重新平衡策略。对于这种数据集，SMOTE、我们提出的方法或欠采样程序是最佳的策略。

    Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing strategy for handling imbalanced data sets. Asymptotically, we prove that SMOTE (with default parameter) regenerates the original distribution by simply copying the original minority samples. We also prove that SMOTE density vanishes near the boundary of the support of the minority distribution, therefore justifying the common BorderLine SMOTE strategy. Then we introduce two new SMOTE-related strategies, and compare them with state-of-the-art rebalancing procedures. We show that rebalancing strategies are only required when the data set is highly imbalanced. For such data sets, SMOTE, our proposals, or undersampling procedures are the best strategies.
    
[^84]: 单层图卷积网络的渐进泛化误差

    Asymptotic generalization error of a single-layer graph convolutional network

    [https://arxiv.org/abs/2402.03818](https://arxiv.org/abs/2402.03818)

    本研究针对单层图卷积网络（GCN）在高维极限下的性能进行了预测，并推广了对多种数据模型的分析。我们的研究显示，尽管GCN在收敛速度上是一致的，但在任何情况下都不能达到贝叶斯最优率。

    

    虽然图卷积网络在实践中展现出很大的潜力，但是相对于广泛研究的全连接神经网络，关于其泛化特性与样本数量的理论理解仍处于初级阶段。在本文中，我们预测了在高维极限下，基于属性随机块模型（SBM）生成的数据训练的单层图卷积网络（GCN）的性能。之前，仅在Shi等人的文章中考虑了上下文-SBM（CSBM）上的岭回归分析；我们将分析推广到CSBM的任意凸损失和正则化方法，并添加了对另一个数据模型——神经优先SBM的分析。我们还研究了高信噪比极限，并详细介绍了GCN的收敛速度，并且展示了尽管一致，但对于任何考虑的情况都不能达到贝叶斯最优率。

    While graph convolutional networks show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of supervised fully connected neural networks. In this article, we predict the performances of a single-layer graph convolutional network (GCN) trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit. Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM. We also study the high signal-to-noise ratio limit, detail the convergence rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases.
    
[^85]: 基于投票一致性模型压缩的促进网络内联合学习

    Expediting In-Network Federated Learning by Voting-Based Consensus Model Compression

    [https://arxiv.org/abs/2402.03815](https://arxiv.org/abs/2402.03815)

    本论文提出了一种基于投票一致性模型压缩的促进网络内联合学习的算法，通过客户端投票和模型聚合来减少内存空间和通信流量。

    

    最近，由于其保护数据隐私的能力，联合学习（FL）越来越受关注。为了通过FL进行模型训练，多个客户端通过互联网与参数服务器交换模型更新。为了加速通信速度，研究人员探索了在参数服务器的位置部署可编程交换机（PS）以协调客户端。在FL中部署PS的挑战在于其内存空间稀缺，无法在PS上运行消耗大量内存的聚合算法。为了克服这个挑战，我们提出了联合学习网络内聚合压缩（FediAC）算法，它包含两个阶段：客户端投票和模型聚合。在前一阶段，客户端向PS报告其重要模型更新的索引，以估计全局重要模型更新。在后一阶段，客户端将全局重要模型更新上传到PS进行聚合。与现有的方法相比，FediAC消耗的内存空间和通信流量较少。

    Recently, federated learning (FL) has gained momentum because of its capability in preserving data privacy. To conduct model training by FL, multiple clients exchange model updates with a parameter server via Internet. To accelerate the communication speed, it has been explored to deploy a programmable switch (PS) in lieu of the parameter server to coordinate clients. The challenge to deploy the PS in FL lies in its scarce memory space, prohibiting running memory consuming aggregation algorithms on the PS. To overcome this challenge, we propose Federated Learning in-network Aggregation with Compression (FediAC) algorithm, consisting of two phases: client voting and model aggregating. In the former phase, clients report their significant model update indices to the PS to estimate global significant model updates. In the latter phase, clients upload global significant model updates to the PS for aggregation. FediAC consumes much less memory space and communication traffic than existing w
    
[^86]: 带非离散带宽的蒙面图自编码器

    Masked Graph Autoencoder with Non-discrete Bandwidths

    [https://arxiv.org/abs/2402.03814](https://arxiv.org/abs/2402.03814)

    本文通过引入非离散边蒙面和逐层带宽预测目标，提出了一种信息丰富且有效的拓扑蒙面图自编码器，弥补了现有方法在学习基于图神经网络的拓扑信息表示方面的不足。

    

    蒙面图自编码器作为一种强大的图自监督学习方法已经出现，但尚未完全探索。在本文中，我们揭示了现有的离散边蒙面和二进制链接重构策略在学习基于图神经网络的拓扑信息表示方面的不足之处。这些限制包括阻塞消息流、容易过度平滑以及次优的邻域区分性。受这些认识的启发，我们探索了非离散边蒙面，其从连续和分散概率分布中采样，而不是离散伯努利分布。这些蒙面限制了每个边的输出消息量，称为“带宽”。我们提出了一种新颖、信息丰富且有效的拓扑蒙面图自编码器，利用带宽蒙面和逐层带宽预测目标。我们展示了它在图拓扑学习方面的强大能力。

    Masked graph autoencoders have emerged as a powerful graph self-supervised learning method that has yet to be fully explored. In this paper, we unveil that the existing discrete edge masking and binary link reconstruction strategies are insufficient to learn topologically informative representations, from the perspective of message propagation on graph neural networks. These limitations include blocking message flows, vulnerability to over-smoothness, and suboptimal neighborhood discriminability. Inspired by these understandings, we explore non-discrete edge masks, which are sampled from a continuous and dispersive probability distribution instead of the discrete Bernoulli distribution. These masks restrict the amount of output messages for each edge, referred to as "bandwidths". We propose a novel, informative, and effective topological masked graph autoencoder using bandwidth masking and a layer-wise bandwidth prediction objective. We demonstrate its powerful graph topological learni
    
[^87]: NK混合遗传算法用于聚类

    NK Hybrid Genetic Algorithm for Clustering

    [https://arxiv.org/abs/2402.03813](https://arxiv.org/abs/2402.03813)

    本文介绍了一种用于聚类的NK混合遗传算法，利用NK聚类验证准则2（NKCV2）来评估解决方案并识别基于密度的区域。算法利用决策变量之间的关系进行灰盒优化，并使用变异算子、分区交叉和局部搜索策略进行优化。该算法具有较低的计算复杂度，并能有效检测聚类区域。

    

    本文提出了一种用于聚类的NK混合遗传算法。为了评估解决方案，该混合算法使用NK聚类验证准则2（NKCV2）。NKCV2利用关于N个小对象组的位置信息。每个组由数据集中的K+1个对象组成。实验结果表明，使用具有固定小K的NKCV2可以识别基于密度的区域。在NKCV2中，决策变量之间的关系已知，这允许我们应用灰盒优化。提出了变异算子、分区交叉和局部搜索策略，所有这些都使用了决策变量之间的关系信息。在分区交叉中，评估函数被分解为q个独立的部分；分区交叉以计算复杂度为O(N)返回在2^q个可能的后代中最好的后代。NK混合遗传算法能够检测到具有密度的区域，从而实现聚类。

    The NK hybrid genetic algorithm for clustering is proposed in this paper. In order to evaluate the solutions, the hybrid algorithm uses the NK clustering validation criterion 2 (NKCV2). NKCV2 uses information about the disposition of $N$ small groups of objects. Each group is composed of $K+1$ objects of the dataset. Experimental results show that density-based regions can be identified by using NKCV2 with fixed small $K$. In NKCV2, the relationship between decision variables is known, which in turn allows us to apply gray box optimization. Mutation operators, a partition crossover, and a local search strategy are proposed, all using information about the relationship between decision variables. In partition crossover, the evaluation function is decomposed into $q$ independent components; partition crossover then deterministically returns the best among $2^q$ possible offspring with computational complexity $O(N)$. The NK hybrid genetic algorithm allows the detection of clusters with a
    
[^88]: SDEMG: 基于得分的扩散模型用于表面肌电信号去噪

    SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal Denoising

    [https://arxiv.org/abs/2402.03808](https://arxiv.org/abs/2402.03808)

    SDEMG是一种基于得分的扩散模型，用于表面肌电信号去噪。实验证明，SDEMG胜过了其他比较方法。

    

    当被监测的肌肉靠近心脏时，表面肌电图（sEMG）记录可能受到心电图（ECG）信号的影响。一些现有方法使用基于信号处理的方法，如高通滤波器和模板减法，而一些方法则通过求取映射函数从带有ECG干扰的sEMG（噪声sEMG）中恢复出干净的sEMG信号。最近引入了一种著名的生成模型，即基于得分的扩散模型，用于通过噪声输入数据生成高质量和准确的样本。在本研究中，我们提出了一种新颖的方法，称为SDEMG，用作sEMG信号去噪的基于得分的扩散模型。为了评估所提出的SDEMG方法，我们使用来自开放接源的Non-Invasive Adaptive Prosthetics数据库的数据以及来自MIT-BIH Normal Sinus Rhythm数据库的ECG信号进行了降噪实验。实验结果表明，SDEMG胜过了比较方法。

    Surface electromyography (sEMG) recordings can be influenced by electrocardiogram (ECG) signals when the muscle being monitored is close to the heart. Several existing methods use signal-processing-based approaches, such as high-pass filter and template subtraction, while some derive mapping functions to restore clean sEMG signals from noisy sEMG (sEMG with ECG interference). Recently, the score-based diffusion model, a renowned generative model, has been introduced to generate high-quality and accurate samples with noisy input data. In this study, we proposed a novel approach, termed SDEMG, as a score-based diffusion model for sEMG signal denoising. To evaluate the proposed SDEMG approach, we conduct experiments to reduce noise in sEMG signals, employing data from an openly accessible source, the Non-Invasive Adaptive Prosthetics database, along with ECG signals from the MIT-BIH Normal Sinus Rhythm Database. The experiment result indicates that SDEMG outperformed comparative methods a
    
[^89]: SEABO: 一种简单的基于搜索的离线模仿学习方法

    SEABO: A Simple Search-Based Method for Offline Imitation Learning

    [https://arxiv.org/abs/2402.03807](https://arxiv.org/abs/2402.03807)

    SEABO是一种简单而有效的基于搜索的离线模仿学习方法，它以无监督学习的方式，根据专家数据和无标签数据得到奖励函数，实验结果表明其性能与离线强化学习相当。

    

    离线强化学习（RL）由于能够从静态离线数据集中学习并消除与环境交互的需求，受到了广泛关注。然而，离线RL的成功在很大程度上取决于标有奖励标签的离线转换。在实践中，我们经常需要手工设计奖励函数，这有时是困难的、劳动密集的或低效的。为了解决这个挑战，我们把重点放在离线模仿学习（IL）设置上，旨在基于专家数据和无标签数据得到一个奖励函数。为此，我们提出了一种简单但有效的基于搜索的离线IL方法，称为SEABO。SEABO以无监督学习的方式，将较大的奖励分配给与专家演示中最接近的转换，否则分配较小的奖励。在多个D4RL数据集上的实验结果表明，SEABO能够达到与离线RL相当的性能水平。

    Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL 
    
[^90]: 可解释的自动化机器学习在信贷决策中的应用：增强金融工程中人工智能与人类合作的能力

    Explainable Automated Machine Learning for Credit Decisions: Enhancing Human Artificial Intelligence Collaboration in Financial Engineering

    [https://arxiv.org/abs/2402.03806](https://arxiv.org/abs/2402.03806)

    本文介绍了可解释的自动化机器学习（AutoML）在信贷决策中的应用，结合了可解释的人工智能（XAI）方法，特别是SHapley Additive exPlanations（SHAP）。它提高了信贷决策的效率和准确性，同时增进了人类和人工智能系统之间的信任和合作。

    

    本文探讨了可解释的自动化机器学习（AutoML）在金融工程领域的整合，特别关注其在信贷决策中的应用。金融领域中人工智能的快速发展需要在这些系统中找到复杂算法决策与透明度之间的平衡。重点是AutoML如何简化强大的机器学习模型的开发，以进行信用评分，而可解释的人工智能（XAI）方法，尤其是SHapley Additive exPlanations（SHAP），则为模型的决策过程提供了洞察力。本研究证明了AutoML和XAI的结合不仅提高了信贷决策的效率和准确性，还促进了人类和人工智能系统之间的信任和合作。研究结果强调了可解释的AutoML在改善人工智能驱动的金融决策的透明度和责任方面的潜力，与要求为用户提供解释的趋势相一致。

    This paper explores the integration of Explainable Automated Machine Learning (AutoML) in the realm of financial engineering, specifically focusing on its application in credit decision-making. The rapid evolution of Artificial Intelligence (AI) in finance has necessitated a balance between sophisticated algorithmic decision-making and the need for transparency in these systems. The focus is on how AutoML can streamline the development of robust machine learning models for credit scoring, while Explainable AI (XAI) methods, particularly SHapley Additive exPlanations (SHAP), provide insights into the models' decision-making processes. This study demonstrates how the combination of AutoML and XAI not only enhances the efficiency and accuracy of credit decisions but also fosters trust and collaboration between humans and AI systems. The findings underscore the potential of explainable AutoML in improving the transparency and accountability of AI-driven financial decisions, aligning with r
    
[^91]: ReLU^2获胜：发现稀疏LLM的高效激活函数

    ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs

    [https://arxiv.org/abs/2402.03804](https://arxiv.org/abs/2402.03804)

    通过动态跳过非活跃神经元的计算，我们提出了一种稀疏LLM的高效激活函数ReLU^2，它在稀疏性与性能、稀疏性的预测性和硬件亲和性等方面表现出色。

    

    通过动态跳过非活跃神经元的计算，稀疏计算为低资源场景中的大型语言模型（LLM）的推断提供了一种引人注目的解决方案。虽然传统方法注重基于ReLU的LLM，利用激活值中的零，但我们将稀疏LLM的范围扩大到零激活值之外。我们引入了一种通用方法，通过神经元输出幅度和定制的幅度阈值来定义神经元激活，并证明非ReLU LLM也表现出稀疏激活。为了找到最高效的稀疏计算激活函数，我们提出了一个系统框架，从三个方面考察LLM的稀疏性：稀疏性与性能之间的权衡、稀疏性的预测性和硬件亲和性。我们对使用不同激活函数的LLM进行了彻底的实验，包括ReLU, SwiGLU, ReGLU 和 ReLU^2。结果表明，采用ReLU^2的模型在所有方面表现出色。

    Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing ReLU$^2$ excel across all t
    
[^92]: 人脸检测：现状与研究方向

    Face Detection: Present State and Research Directions

    [https://arxiv.org/abs/2402.03796](https://arxiv.org/abs/2402.03796)

    本文对人脸检测领域的现状进行了综述，指出了其存在的问题，并提供了研究方向，以提高人脸检测的准确性和速度。

    

    大多数处理包含人类图像的计算机视觉应用都使用人脸检测作为核心组件。尽管对该主题进行了大量研究，但人脸检测仍然存在问题。人脸检测的准确性和速度仍有待提高。本综述论文展示了在这一领域取得的进展以及仍需解决的重大问题。该论文提供了可以作为人脸检测领域研究项目的研究方向。

    The majority of computer vision applications that handle images featuring humans use face detection as a core component. Face detection still has issues, despite much research on the topic. Face detection's accuracy and speed might yet be increased. This review paper shows the progress made in this area as well as the substantial issues that still need to be tackled. The paper provides research directions that can be taken up as research projects in the field of face detection.
    
[^93]: 连续状态和/或动作空间中的无悔强化学习在平滑MDPs中的应用

    No-Regret Reinforcement Learning in Smooth MDPs

    [https://arxiv.org/abs/2402.03792](https://arxiv.org/abs/2402.03792)

    本论文针对具有连续状态和/或动作空间的强化学习问题提出了一种无悔保证的新方法，即通过在Legendre多项式基础上构建MDP表示来解决$\nu-$平滑 MDPs，在较弱的假设下可以达到无悔特性，同时在多项式时间内运行。

    

    对于具有连续状态和/或动作空间的问题的强化学习（RL），如何获得无悔保证仍然是该领域的主要挑战之一。最近，提出了多种解决方案，但除非是非常特定的情景，否则这个普遍问题仍未解决。本文引入了一种新的对马尔可夫决策过程（MDPs）进行结构假设的方法，即$\nu-$平滑性，该方法推广了目前提出的多种设置（如线性MDPs和Lipschitz MDPs）。为了应对这个具有挑战性的场景，我们提出了两种算法来在$\nu-$平滑 MDPs中进行悔恨最小化。这两种算法都基于通过基于Legendre多项式的正交特征映射构建MDP表示的思想。第一个算法\textsc{Legendre-Eleanor}在较弱的假设下可以达到无悔特性，但计算效率低下，而第二个\textsc{Legendre-LSVI}算法在多项式时间内运行，

    Obtaining no-regret guarantees for reinforcement learning (RL) in the case of problems with continuous state and/or action spaces is still one of the major open challenges in the field. Recently, a variety of solutions have been proposed, but besides very specific settings, the general problem remains unsolved. In this paper, we introduce a novel structural assumption on the Markov decision processes (MDPs), namely $\nu-$smoothness, that generalizes most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs). To face this challenging scenario, we propose two algorithms for regret minimization in $\nu-$smooth MDPs. Both algorithms build upon the idea of constructing an MDP representation through an orthogonal feature map based on Legendre polynomials. The first algorithm, \textsc{Legendre-Eleanor}, archives the no-regret property under weaker assumptions but is computationally inefficient, whereas the second one, \textsc{Legendre-LSVI}, runs in polynomial time, although 
    
[^94]: 通过知识-数据对齐进行弱监督异常检测

    Weakly Supervised Anomaly Detection via Knowledge-Data Alignment

    [https://arxiv.org/abs/2402.03785](https://arxiv.org/abs/2402.03785)

    本文提出了一个通过知识-数据对齐的框架KDAlign，用于弱监督异常检测。这个框架将人工专家总结的规则知识与有限的带标签数据集成，通过对齐知识和数据来提高模型性能。

    

    异常检测在许多基于web的应用程序中起着重要作用，包括恶意软件检测、反洗钱、设备故障检测和网络故障分析。大多数依赖于无监督学习的方法由于缺乏标签而难以达到令人满意的检测精度。弱监督异常检测 (WSAD) 引入了有限数量的带标签异常样本来提高模型性能。然而，对于在有限的标记数据上训练的模型来说，泛化到未见异常仍然具有挑战性。在本文中，我们引入了一种新颖的框架Knowledge-Data Alignment (KDAlign)，将通常由人工专家总结的规则知识集成到有限的带标签数据中以补充。具体而言，我们将这些规则转化为知识空间，并将知识与数据的融合重新映射为知识和数据的对齐。为了促进这种对齐，我们采用了最优传输方法。

    Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis. Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance. Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies. In this paper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data. Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data. To facilitate this alignment, we employ the Optimal Transport 
    
[^95]: AirPhyNet: 利用物理引导的神经网络预测空气质量

    AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction

    [https://arxiv.org/abs/2402.03784](https://arxiv.org/abs/2402.03784)

    AirPhyNet是一种利用物理引导的神经网络方法，通过将空气颗粒运动的物理原理和神经网络结构相结合，提高了对空气质量的预测精度和可解释性。

    

    空气质量预测和建模在公共卫生和环境管理中起着关键作用，帮助个人和当局做出明智决策。尽管传统的数据驱动模型在这个领域已经显示出了潜力，但它们在长期预测精度上可能受到限制，特别是在数据稀疏或不完整的情况下，它们往往依赖于缺乏坚实物理基础的黑盒深度学习结构，导致预测的透明度和可解释性降低。为了解决这些限制，本文提出了一种名为Physics guided Neural Network for Air Quality Prediction（AirPhyNet）的新方法。具体而言，我们利用空气颗粒运动的两个成熟的物理原理（扩散和平流）将其表示为微分方程网络。然后，我们利用图结构将物理知识融入神经网络架构，并利用潜在表示来捕捉时空关系。

    Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships
    
[^96]: MolTC: 在语言模型中进行分子关系建模

    MolTC: Towards Molecular Relational Modeling In Language Models

    [https://arxiv.org/abs/2402.03781](https://arxiv.org/abs/2402.03781)

    本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。

    

    分子关系学习（MRL）旨在理解分子之间的相互作用，在推进生物化学研究方面起到了关键作用。最近，大型语言模型（LLMs）的采用已成为一种有效和高效的MRL方法，这些模型以其庞大的知识存储库和先进的逻辑推理能力而闻名。尽管具有潜力，但这些方法主要依赖于文本数据，因此没有充分利用分子图中固有的丰富结构信息。此外，缺乏统一的框架加剧了信息的浪费，因为它阻碍了在不同数据集之间共享学习到的相互作用理由。为了解决这些挑战，本研究提出了一种基于LLM的多模态框架，用于根据思维链（CoT）理论对分子相互作用进行预测，称为MolTC，它可以高效地整合分子对的丰富图形信息。

    Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
    
[^97]: 揭示宣传：基于人类注释和机器分类的文体线索比较分析

    Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification

    [https://arxiv.org/abs/2402.03780](https://arxiv.org/abs/2402.03780)

    本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。

    

    本文研究了宣传语言及其文体特征。提出了PPN数据集，即宣传性伪新闻数据集，它是一种多源、多语言、多模态的数据集，由专家机构确定的宣传来源网站上的新闻文章组成。从该数据集中随机选择了一部分样本与来自常规法国新闻的文章混合，并对它们的URL进行了掩盖，以进行人类注释实验，使用11个不同的标签。结果显示，人类注释者能够可靠地区分两种类型的新闻纸对每个标签。我们提出了不同的自然语言处理技术来识别注释者使用的线索，并与机器分类进行比较。其中包括使用VAGO分析器进行辞述模糊和主观性的测量，使用TF-IDF作为基准，以及四种不同的分类器：两个基于RoBERTa模型的模型，使用句法的CATS，以及结合句法和语义特征的一个XGBoost模型。

    This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
    
[^98]: EERO: 早期退出与拒绝选项用于有限预算下的高效分类

    EERO: Early Exit with Reject Option for Efficient Classification with limited budget

    [https://arxiv.org/abs/2402.03779](https://arxiv.org/abs/2402.03779)

    EERO 是一种早期退出与拒绝选项的新方法，通过使用多个分类器来选择每个实例的退出头，以实现高效分类。实验结果表明，它可以有效管理预算分配并提高准确性。

    

    先进的机器学习模型的不断复杂化要求创新的方法来有效管理计算资源。其中一种方法是早期退出策略，通过提供缩短简单数据实例处理路径的机制，实现自适应计算。在本文中，我们提出了EERO，一种将早期退出问题转化为使用具有拒绝选项的多个分类器问题的新方法，以便更好地选择每个实例的退出头。我们使用指数权重聚合来校准不同头部退出的概率，以保证一个固定的预算。我们考虑贝叶斯风险、预算约束和头部特定预算消耗等因素。通过在Cifar和ImageNet数据集上使用ResNet-18模型和ConvNext架构进行的实验结果表明，我们的方法不仅能有效管理预算分配，还能提高过度考虑场景中的准确性。

    The increasing complexity of advanced machine learning models requires innovative approaches to manage computational resources effectively. One such method is the Early Exit strategy, which allows for adaptive computation by providing a mechanism to shorten the processing path for simpler data instances. In this paper, we propose EERO, a new methodology to translate the problem of early exiting to a problem of using multiple classifiers with reject option in order to better select the exiting head for each instance. We calibrate the probabilities of exiting at the different heads using aggregation with exponential weights to guarantee a fixed budget .We consider factors such as Bayesian risk, budget constraints, and head-specific budget consumption. Experimental results, conducted using a ResNet-18 model and a ConvNext architecture on Cifar and ImageNet datasets, demonstrate that our method not only effectively manages budget allocation but also enhances accuracy in overthinking scenar
    
[^99]: 使用Transformer学习决策树算法

    Learning a Decision Tree Algorithm with Transformers

    [https://arxiv.org/abs/2402.03774](https://arxiv.org/abs/2402.03774)

    该论文介绍了MetaTree模型，它使用经典算法的输出训练基于Transformer的模型，以产生具有强大概括性能的决策树。

    

    决策树因其可解释性和在表格数据上实现高预测性能而闻名。传统上，决策树是通过递归算法构建的，在树的每个节点上将数据进行分区。然而，确定最佳分区是具有挑战性的，因为针对局部段优化的决策树可能无法带来全局概括。为了解决这个问题，我们引入了MetaTree，该模型使用经典算法的过滤输出来训练基于Transformer的模型，以产生强大的分类决策树。具体而言，我们在大量数据集上拟合贪婪决策树和优化决策树。然后，我们训练MetaTree产生具有强大概括性能的决策树。这种训练使MetaTree不仅可以模拟这些算法，还可以根据上下文智能地调整策略，从而实现更强的概括性能。

    Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.
    
[^100]: 来自打包奖励的强化学习：一种基于Transformer的实例级奖励重新分配方法

    Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution

    [https://arxiv.org/abs/2402.03771](https://arxiv.org/abs/2402.03771)

    该论文提出了一种名为强化学习来自打包奖励的新问题，其中学习器只能获取序列的打包奖励，在这种情况下探索未知的即时奖励是一项困难的任务，并引入了基于Transformer的方法来解决这个问题。

    

    在强化学习中，每个动作的即时奖励信号将为代理生成，以便代理学习如何最大化累积奖励以获取最优策略。然而，在许多实际应用中，代理无法获取即时奖励信号。相反，学习器只在路径的结束处获取奖励，其中路径的部分序列被定义为一个包。在这种情况下，学习器必须面对探索包中未知即时奖励的显著困难，这不能通过现有方法解决，包括仅考虑完整路径并忽略内部奖励分布的轨迹方法。为了正式研究这种情况，我们引入了一种新的强化学习设置，称为来自打包奖励的强化学习（Reinforcement Learning from Bagged Rewards，RLBR），只能获取序列的打包奖励。我们进行了理论研究，建立了RLBR与标准强化学习之间的联系。

    In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy. However, in many real-world applications, the instant reward signals are not obtainable by the agent. Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory. In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions. To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained. We provide the theoretical study to establish the connection between RLBR and standa
    
[^101]: Fed-CVLC:使用可变长度编码压缩联邦学习通信

    Fed-CVLC: Compressing Federated Learning Communications with Variable-Length Codes

    [https://arxiv.org/abs/2402.03770](https://arxiv.org/abs/2402.03770)

    通过分析和实验证明，在联邦学习中使用可变长度编码可以有效压缩通信。本文提出了Fed-CVLC，可以根据模型更新的动态进行代码长度微调。

    

    在联邦学习（FL）范 paradigm 下，一个参数服务器（PS）同时与分布式参与的客户端进行通信，进行模型收集、更新聚合和模型分发，同时不接触个别客户端拥有的私有数据。FL 在保护数据隐私方面具有吸引力；然而，PS 与分散客户端之间的通信可能成为严重的瓶颈。已经提出了模型压缩算法，如量化和稀疏化，但它们一般假设了固定的代码长度，这不反映模型更新的异质性和可变性。本文通过分析和实验证明了在 FL 中可变长度对于压缩是有益的。因此，我们提出了 Fed-CVLC（带有可变长度编码的联邦学习压缩），它根据模型更新的动态对代码长度进行微调。我们开发了最优的调整策略，最小化损失函数（等价于 ...

    In Federated Learning (FL) paradigm, a parameter server (PS) concurrently communicates with distributed participating clients for model collection, update aggregation, and model distribution over multiple rounds, without touching private data owned by individual clients. FL is appealing in preserving data privacy; yet the communication between the PS and scattered clients can be a severe bottleneck. Model compression algorithms, such as quantization and sparsification, have been suggested but they generally assume a fixed code length, which does not reflect the heterogeneity and variability of model updates. In this paper, through both analysis and experiments, we show strong evidences that variable-length is beneficial for compression in FL. We accordingly present Fed-CVLC (Federated Learning Compression with Variable-Length Codes), which fine-tunes the code length in response of the dynamics of model updates. We develop optimal tuning strategy that minimizes the loss function (equiva
    
[^102]: 基于深度学习的高光谱图像校正与分离在脑肿瘤手术中的应用

    Deep Learning-Based Correction and Unmixing of Hyperspectral Images for Brain Tumor Surgery

    [https://arxiv.org/abs/2402.03761](https://arxiv.org/abs/2402.03761)

    本研究使用基于深度学习的方法提出了两种模型，用于校正和分离高光谱图像中的荧光物质。这些模型能够考虑非线性效应，产生更准确的丰度估计，为脑肿瘤手术提供改进方法。

    

    高光谱成像（HSI）用于荧光引导的脑肿瘤切除能够使人们能够看到不同组织之间的差异，而这些差异对人眼来说是无法辨别的。这种增强技术可以最大限度地切除脑肿瘤，提高患者的治疗效果。然而，HSI中的许多处理方法使用了简化的线性方法，无法捕捉到非线性、波长依赖性的现象，这些现象在准确恢复荧光物质丰度时必须进行建模。因此，我们提出了两个基于深度学习的模型，用于校正和分离，可以考虑非线性效应，并产生更准确的丰度估计。这两个模型都使用类似自动编码器的结构来处理获取的光谱。其中一个模型使用盐酸卟啉（PpIX）浓度标记进行训练，另一个模型经历半监督训练，首先无监督学习高光谱分离，然后学习使用具有异质光学和几何特性的荧光发射光谱进行校正。

    Hyperspectral Imaging (HSI) for fluorescence-guided brain tumor resection enables visualization of differences between tissues that are not distinguishable to humans. This augmentation can maximize brain tumor resection, improving patient outcomes. However, much of the processing in HSI uses simplified linear methods that are unable to capture the non-linear, wavelength-dependent phenomena that must be modeled for accurate recovery of fluorophore abundances. We therefore propose two deep learning models for correction and unmixing, which can account for the nonlinear effects and produce more accurate estimates of abundances. Both models use an autoencoder-like architecture to process the captured spectra. One is trained with protoporphyrin IX (PpIX) concentration labels. The other undergoes semi-supervised training, first learning hyperspectral unmixing self-supervised and then learning to correct fluorescence emission spectra for heterogeneous optical and geometric properties using a 
    
[^103]: 本能偏见：虚假图像导致MLLMs产生幻觉

    The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs

    [https://arxiv.org/abs/2402.03757](https://arxiv.org/abs/2402.03757)

    本论文研究发现，虚假图像会导致多模态大型语言模型产生幻觉，作者提出了评估幻觉程度的基准CorrelationQA，并发现主流多模态大型语言模型普遍受到这种本能偏见的影响。

    

    大型语言模型（LLMs）近年来取得了显著进展，多模态大型语言模型（MLLMs）的出现使LLMs具备了视觉能力，在各种多模态任务中表现出色。然而，像GPT-4V这样强大的MLLMs在面对某些图像和文本输入时仍然以惊人的方式失败了。本文中，我们确定了一类典型输入，这些输入令MLLMs困惑，它们由高度相关但与答案不一致的图像组成，导致MLLMs产生幻觉。为了量化这种影响，我们提出了CorrelationQA，这是首个评估给定虚假图像的幻觉程度的基准。该基准包含13个类别的7,308个文本-图像对。基于提出的CorrelationQA，我们对9个主流MLLMs进行了深入分析，表明它们普遍受到这种本能偏见的不同程度的影响。我们希望我们精选的基准和评估结果能有所帮助。

    Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation result
    
[^104]: 基于不确定性的集体变量增强采样稳健分子数据集

    Enhanced sampling of robust molecular datasets with uncertainty-based collective variables

    [https://arxiv.org/abs/2402.03753](https://arxiv.org/abs/2402.03753)

    本研究提出了一种基于不确定性的集体变量方法，通过引导采集化学相关数据点、关注模型预测最不确定的区域，进行了稳健分子数据集的增强采样。

    

    生成一个代表分子系统可访问构型空间的数据集对于机器学习得到的原子间相互作用势函数(MLIP)的稳健性至关重要。然而，由于分子系统的复杂性，其潜在能面(PESs)呈现出众多局部极小值和能垒，导致了一个很大的挑战。传统的数据生成方法，如随机采样或全面探索，要么难以处理，要么无法捕捉到稀有但高度信息化的构型。在本研究中，我们提出了一种利用不确定性作为集体变量(CV)引导采集化学相关数据点、关注ML模型预测最不确定区域的方法。该方法使用了基于高斯混合模型的不确定性度量作为有偏分子动力学模拟的CV。我们的方法在克服能量的效果上是有效的。

    Generating a data set that is representative of the accessible configuration space of a molecular system is crucial for the robustness of machine learned interatomic potentials (MLIP). However, the complexity of molecular systems, characterized by intricate potential energy surfaces (PESs) with numerous local minima and energy barriers, presents a significant challenge. Traditional methods of data generation, such as random sampling or exhaustive exploration, are either intractable or may not capture rare, but highly informative configurations. In this study, we propose a method that leverages uncertainty as the collective variable (CV) to guide the acquisition of chemically-relevant data points, focusing on regions of the configuration space where ML model predictions are most uncertain. This approach employs a Gaussian Mixture Model-based uncertainty metric from a single model as the CV for biased molecular dynamics simulations. The effectiveness of our approach in overcoming energy 
    
[^105]: 在小数据集上使用最小缩放图像对轻量级视觉Transformer进行预训练

    Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images

    [https://arxiv.org/abs/2402.03752](https://arxiv.org/abs/2402.03752)

    本研究展示了轻量级视觉Transformer（ViT）在小数据集上使用最小缩放图像进行预训练，通过掩码自编码器技术实现优秀性能，无需显著增加图像尺寸。该方法不仅高效处理小数据集，还能有效处理接近原始尺寸的图像。

    

    轻量级视觉Transformer（ViT）能否在分辨率较小的小数据集上与ResNet等卷积神经网络（CNN）的性能相匹敌或超越？本报告通过使用最小图像缩放的掩码自编码器技术，证明了纯ViT确实可以通过预训练实现优秀的性能。我们在CIFAR-10和CIFAR-100数据集上进行了实验，所使用的ViT模型参数不超过365万个，并且乘累加（MAC）计数低于0.27G，可以称之为“轻量级”模型。与以往方法不同的是，我们的方法在不显著增加CIFAR-10和CIFAR-100图像尺寸的情况下，达到了类似轻量级基于Transformer的架构的最先进性能。这一成就凸显了我们模型的效率，它不仅可以处理小数据集，而且可以有效处理接近原始尺寸的图像。

    Can a lightweight Vision Transformer (ViT) match or exceed the performance of Convolutional Neural Networks (CNNs) like ResNet on small datasets with small image resolutions? This report demonstrates that a pure ViT can indeed achieve superior performance through pre-training, using a masked auto-encoder technique with minimal image scaling. Our experiments on the CIFAR-10 and CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters and a multiply-accumulate (MAC) count below 0.27G, qualifying them as 'lightweight' models. Unlike previous approaches, our method attains state-of-the-art performance among similar lightweight transformer-based architectures without significantly scaling up images from CIFAR-10 and CIFAR-100. This achievement underscores the efficiency of our model, not only in handling small datasets but also in effectively processing images close to their original scale.
    
[^106]: 数字孪生移动性建模：一种时空图学习方法

    Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach

    [https://arxiv.org/abs/2402.03750](https://arxiv.org/abs/2402.03750)

    本论文提出了一种数字孪生移动性建模方法，通过构建对齐图和设计扩张对齐卷积网络来捕捉交通场景中复杂的时空特征，为智能交通系统的开发提供了有效的方法。

    

    随着大数据时代的到来，移动性建模成为利用大量移动性数据创建智能交通系统的可行方法。移动性建模可以从移动性数据中提取城市交通中的潜在模式，是各种与交通相关的应用的关键。然而，由于复杂性高和数据量巨大，移动性建模面临巨大挑战。数字孪生技术通过创建网络的虚拟表示来模拟其行为，为成本有效和性能优化的管理铺平了道路。为了捕捉交通场景中的复杂的时空特征，我们构建对齐图来完成时空相关表示，并设计扩张对齐卷积网络（DACN）来学习精细的相关性，即时空相互作用。我们提出了一种数字孪生移动性建模（DTMP）方法。

    With the arrival of the big data era, mobility profiling has become a viable method of utilizing enormous amounts of mobility data to create an intelligent transportation system. Mobility profiling can extract potential patterns in urban traffic from mobility data and is critical for a variety of traffic-related applications. However, due to the high level of complexity and the huge amount of data, mobility profiling faces huge challenges. Digital Twin (DT) technology paves the way for cost-effective and performance-optimised management by digitally creating a virtual representation of the network to simulate its behaviour. In order to capture the complex spatio-temporal features in traffic scenario, we construct alignment diagrams to assist in completing the spatio-temporal correlation representation and design dilated alignment convolution network (DACN) to learn the fine-grained correlations, i.e., spatio-temporal interactions. We propose a digital twin mobility profiling (DTMP) fra
    
[^107]: PDE发现的约束深度学习网络

    An invariance constrained deep learning network for PDE discovery

    [https://arxiv.org/abs/2402.03747](https://arxiv.org/abs/2402.03747)

    本研究提出了一种约束不变性的深度学习网络（ICNet）用于PDE的发现，通过过滤掉不能满足Galilean变换要求的候选项，嵌入固定和可能的项到神经网络的损失函数中，显著抵消了稀疏高噪声数据的影响并过滤掉冗余项。

    

    从数据集中发现偏微分方程（PDE）已经引起了越来越多的关注。然而，从稀疏数据和高噪声中发现控制方程仍然非常具有挑战性，这是由于导数计算的困难和噪声的干扰。此外，为了满足物理定律，候选库的选择原则还需要进一步研究。不变性是控制方程的基本定律之一。在本研究中，我们提出了一种约束不变性的深度学习网络（ICNet）用于PDE的发现。考虑到时空平移不变性（Galilean不变性）是物理定律的基本属性，我们过滤掉不能满足Galilean变换要求的候选项。然后，我们将固定和可能的项嵌入到神经网络的损失函数中，显著抵消了稀疏高噪声数据的影响。然后，通过过滤掉冗余项。

    The discovery of partial differential equations (PDEs) from datasets has attracted increased attention. However, the discovery of governing equations from sparse data with high noise is still very challenging due to the difficulty of derivatives computation and the disturbance of noise. Moreover, the selection principles for the candidate library to meet physical laws need to be further studied. The invariance is one of the fundamental laws for governing equations. In this study, we propose an invariance constrained deep learning network (ICNet) for the discovery of PDEs. Considering that temporal and spatial translation invariance (Galilean invariance) is a fundamental property of physical laws, we filter the candidates that cannot meet the requirement of the Galilean transformations. Subsequently, we embedded the fixed and possible terms into the loss function of neural network, significantly countering the effect of sparse data with high noise. Then, by filtering out redundant terms
    
[^108]: SUB-PLAY：针对部分观测的多智能体强化学习系统的对抗策略

    SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems

    [https://arxiv.org/abs/2402.03741](https://arxiv.org/abs/2402.03741)

    该研究首次揭示了攻击者在多智能体竞争环境中即使受限于受害者的部分观测也能生成对抗策略的能力。

    

    最近在多智能体强化学习（MARL）领域取得的进展为无人机的群体控制、机械臂的协作操纵以及多目标包围等开辟了广阔的应用前景。然而，在MARL部署过程中存在潜在的安全威胁需要更多关注和深入调查。最近的研究表明，攻击者可以迅速利用受害者的漏洞生成对抗策略，导致受害者在特定任务中失败。例如，将超人级别的围棋AI的获胜率降低到约20%。这些研究主要关注两人竞争环境，并假设攻击者具有完整的全局状态观测。

    Recent advances in multi-agent reinforcement learning (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent researches reveal that an attacker can rapidly exploit the victim's vulnerabilities and generate adversarial policies, leading to the victim's failure in specific tasks. For example, reducing the winning rate of a superhuman-level Go AI to around 20%. They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporate
    
[^109]: BotSSCL: 基于自监督对比学习的社交机器人检测

    BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning

    [https://arxiv.org/abs/2402.03740](https://arxiv.org/abs/2402.03740)

    提出了一种名为BotSSCL的框架，利用自监督对比学习来提高社交机器人检测的性能，从而解决了当前模型在检测复杂机器人和依赖简易特征上的局限性问题。

    

    对于在线社交网络（OSNs）来说，自动化账户（也称为“社交机器人”）的检测已经成为一个日益重要的问题。尽管已经提出了几种方法来检测社交机器人，但仍存在显著的研究空白。首先，当前模型在检测旨在模仿真实OSN用户的复杂机器人方面存在局限性。其次，这些方法往往依赖于容易受到操纵的简易个人资料特征。除了易受到对抗性操纵的脆弱性外，这些模型缺乏泛化性，导致在一个数据集上训练，又在另一个数据集上进行测试时性能不佳。为了解决这些挑战，我们提出了一种基于自监督对比学习的社交机器人检测的新框架（BotSSCL）。我们的框架利用对比学习在嵌入空间中区分社交机器人和人类，从而提高线性可分性。BotSSCL产生的高级表示增强了其鲁棒性。

    The detection of automated accounts, also known as "social bots", has been an increasingly important concern for online social networks (OSNs). While several methods have been proposed for detecting social bots, significant research gaps remain. First, current models exhibit limitations in detecting sophisticated bots that aim to mimic genuine OSN users. Second, these methods often rely on simplistic profile features, which are susceptible to manipulation. In addition to their vulnerability to adversarial manipulations, these models lack generalizability, resulting in subpar performance when trained on one dataset and tested on another.   To address these challenges, we propose a novel framework for social Bot detection with Self-Supervised Contrastive Learning (BotSSCL). Our framework leverages contrastive learning to distinguish between social bots and humans in the embedding space to improve linear separability. The high-level representations derived by BotSSCL enhance its resilienc
    
[^110]: 面向高维上下文线性赌臂问题的差分隐私算法

    Differentially Private High Dimensional Bandits

    [https://arxiv.org/abs/2402.03737](https://arxiv.org/abs/2402.03737)

    这篇论文提出了PrivateLASSO算法，用于解决高维随机上下文线性赌臂问题，并在差分隐私的约束下证明了其隐私和实用性保证。

    

    我们在参数向量为$s_{0}$-稀疏且决策制定者受到差分隐私的中央和本地模型的约束下考虑高维随机上下文线性赌臂问题。我们提出了PrivateLASSO，一种差分隐私LASSO赌臂算法。PrivateLASSO基于两个子程序：(i)基于稀疏硬阈值的隐私机制和(ii)用于识别参数$\theta$支持集的阈值规则。我们证明了PrivateLASSO的最小最大私有下界，并在标准假设下建立了PrivateLASSO在中央模型下的隐私和实用性保证。

    We consider a high-dimensional stochastic contextual linear bandit problem when the parameter vector is $s_{0}$-sparse and the decision maker is subject to privacy constraints under both central and local models of differential privacy. We present PrivateLASSO, a differentially private LASSO bandit algorithm. PrivateLASSO is based on two sub-routines: (i) a sparse hard-thresholding-based privacy mechanism and (ii) an episodic thresholding rule for identifying the support of the parameter $\theta$. We prove minimax private lower bounds and establish privacy and utility guarantees for PrivateLASSO for the central model under standard assumptions.
    
[^111]: 一种有效的分支定界算法及最大s-束问题的新边界方法

    An Effective Branch-and-Bound Algorithm with New Bounding Methods for the Maximum $s$-Bundle Problem

    [https://arxiv.org/abs/2402.03736](https://arxiv.org/abs/2402.03736)

    本文介绍了一种新颖的基于分割的上界(PUB)和利用短随机行走生成更大初始解的分支定界算法，用于解决最大s-束问题(MBP)。

    

    最大s-束问题（MBP）涉及在给定图中识别最大s-束的任务。图G=(V, E)被称为s-束，如果其顶点连通度至少为|V|-s，其中顶点连通度等于删除最少个顶点使得图变为非连通或平凡图的最小数量。MBP是NP难的，对于强调顶点连通度的许多实际场景具有相关性。MBP的精确算法主要采用分支定界（BnB）框架，其性能严重依赖于对最大s-束的基数的上界质量和图减少的初始下界。在这项工作中，我们提出了一种利用图分割技术实现更紧密上界的新型基于分割的上界（PUB）。为了增加下界，我们提出在团上进行短随机行走来生成更大的初始解。然后，我们提出了一种使用新的初始解的BnB算法。

    The Maximum s-Bundle Problem (MBP) addresses the task of identifying a maximum s-bundle in a given graph. A graph G=(V, E) is called an s-bundle if its vertex connectivity is at least |V|-s, where the vertex connectivity equals the minimum number of vertices whose deletion yields a disconnected or trivial graph. MBP is NP-hard and holds relevance in numerous realworld scenarios emphasizing the vertex connectivity. Exact algorithms for MBP mainly follow the branch-and-bound (BnB) framework, whose performance heavily depends on the quality of the upper bound on the cardinality of a maximum s-bundle and the initial lower bound with graph reduction. In this work, we introduce a novel Partition-based Upper Bound (PUB) that leverages the graph partitioning technique to achieve a tighter upper bound compared to existing ones. To increase the lower bound, we propose to do short random walks on a clique to generate larger initial solutions. Then, we propose a new BnB algorithm that uses the ini
    
[^112]: 知识图谱中深度过时事实检测

    Deep Outdated Fact Detection in Knowledge Graphs

    [https://arxiv.org/abs/2402.03732](https://arxiv.org/abs/2402.03732)

    本文介绍了一种名为DEAN的新型深度学习框架，用于在知识图谱中识别过时事实。DEAN通过全面建模实体和关系之间的隐含结构信息，采用了一种基于对比的方法来揭示潜在的过时信息。实验证明DEAN相对于最先进的基准方法具有显著的优势。

    

    知识图谱（KG）因其在各个领域的广泛潜力而引起了广泛的关注。然而，过时事实的问题给KG带来了挑战，影响了其作为现实世界信息的整体质量。现有的过时事实检测解决方案通常依赖于手动识别。为此，本文提出了DEAN（深度过时事实检测），这是一个基于深度学习的新颖框架，用于在KG中识别过时事实。DEAN通过全面建模实体和关系之间的隐含结构信息，从而在区分事实方面表现出自己的独特之处。为了有效地揭示潜在的过时信息，DEAN采用了基于预定义的关系到节点（R2N）图的对比方法，该图由实体数量加权。实验结果证明了DEAN相对于最先进的基准方法的有效性和优越性。

    Knowledge graphs (KGs) have garnered significant attention for their vast potential across diverse domains. However, the issue of outdated facts poses a challenge to KGs, affecting their overall quality as real-world information evolves. Existing solutions for outdated fact detection often rely on manual recognition. In response, this paper presents DEAN (Deep outdatEd fAct detectioN), a novel deep learning-based framework designed to identify outdated facts within KGs. DEAN distinguishes itself by capturing implicit structural information among facts through comprehensive modeling of both entities and relations. To effectively uncover latent out-of-date information, DEAN employs a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph, weighted by the number of entities. Experimental results demonstrate the effectiveness and superiority of DEAN over state-of-the-art baseline methods.
    
[^113]: 异构学习模型的一致联合决策

    Consistent Joint Decision-Making with Heterogeneous Learning Models

    [https://arxiv.org/abs/2402.03728](https://arxiv.org/abs/2402.03728)

    本文提出了一种新颖的决策框架，通过整合不同模型的预测和外部知识，实现了决策的一致性。经过实证研究，我们的方法在多个数据集上表现出优越性能。

    

    本文介绍了一种新颖的决策框架，促进了由不同模型做出的决策之间的一致性，并利用外部知识。通过利用整数线性规划（ILP）框架，我们将各种模型的预测映射到全局归一化和可比较的值，同时考虑到决策的先验概率、置信度（不确定性）和模型的预期准确性。我们的实证研究证明了我们的方法在多个数据集上优于传统基准方法。

    This paper introduces a novel decision-making framework that promotes consistency among decisions made by diverse models while utilizing external knowledge. Leveraging the Integer Linear Programming (ILP) framework, we map predictions from various models into globally normalized and comparable values by incorporating information about decisions' prior probability, confidence (uncertainty), and the models' expected accuracy. Our empirical study demonstrates the superiority of our approach over conventional baselines on multiple datasets.
    
[^114]: 从实例级的自我注意力Hawkes过程中学习格兰杰因果关系

    Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes

    [https://arxiv.org/abs/2402.03726](https://arxiv.org/abs/2402.03726)

    本论文提出了一种名为ISAHP的深度学习框架，可以从异步、相互依赖的多类型事件序列中无监督地学习实例级的格兰杰因果关系。它是第一个满足格兰杰因果关系要求的神经点过程模型，并利用变压器的自我注意机制来实现格兰杰因果关系的推断。

    

    我们解决了从异步、相互依赖的多类型事件序列中学习格兰杰因果关系的问题。特别是，我们对以无监督的方式发现实例级的因果结构感兴趣。实例级因果关系识别单个事件之间的因果关系，为决策提供了更精细化的信息。现有文献中的工作要么需要强加一些假设，比如强加到强度函数中的线性假设，要么启发式地定义模型参数，这些不一定满足格兰杰因果关系的要求。我们提出了一种新颖的深度学习框架，即实例级自我注意力Hawkes过程（ISAHP），可以直接推断事件实例级的格兰杰因果关系。ISAHP是第一个满足格兰杰因果关系要求的神经点过程模型。它利用了变压器的自我注意机制，与格兰杰因果关系的原理相一致。我们通过实验证明了ISAHP的有效性和优越性。

    We address the problem of learning Granger causality from asynchronous, interdependent, multi-type event sequences. In particular, we are interested in discovering instance-level causal structures in an unsupervised manner. Instance-level causality identifies causal relationships among individual events, providing more fine-grained information for decision-making. Existing work in the literature either requires strong assumptions, such as linearity in the intensity function, or heuristically defined model parameters that do not necessarily meet the requirements of Granger causality. We propose Instance-wise Self-Attentive Hawkes Processes (ISAHP), a novel deep learning framework that can directly infer the Granger causality at the event instance level. ISAHP is the first neural point process model that meets the requirements of Granger causality. It leverages the self-attention mechanism of the transformer to align with the principles of Granger causality. We empirically demonstrate th
    
[^115]: 变分自动编码器进行异常检测的统计测试

    Statistical Test for Anomaly Detections by Variational Auto-Encoders

    [https://arxiv.org/abs/2402.03724](https://arxiv.org/abs/2402.03724)

    本研究提出了一种利用变分自动编码器进行异常检测的统计测试方法（VAE-AD测试），通过量化异常区域的可靠性，可以控制误检的概率到所期望的水平。

    

    在本研究中，我们考虑使用变分自动编码器（VAE）进行异常检测（AD）的可靠性评估。在过去的十年里，基于VAE的AD已经在各个角度进行了积极的研究，从方法开发到应用研究。然而，当AD的结果用于高风险的决策时，如医学诊断，需要确保检测到的异常的可靠性。在本研究中，我们提出了VAE-AD测试作为在统计检验框架下量化基于VAE的AD的统计可靠性的方法。利用VAE-AD测试，可以以p值的形式量化VAE检测到的异常区域的可靠性。这意味着，如果在p值低于某个阈值时宣布为异常，则可以将误检的概率控制在所期望的水平。由于VAE-AD测试是基于一种称为选择性推理的新统计推断框架构建的，其有效性是确保被证明的。

    In this study, we consider the reliability assessment of anomaly detection (AD) using Variational Autoencoder (VAE). Over the last decade, VAE-based AD has been actively studied in various perspective, from method development to applied research. However, when the results of ADs are used in high-stakes decision-making, such as in medical diagnosis, it is necessary to ensure the reliability of the detected anomalies. In this study, we propose the VAE-AD Test as a method for quantifying the statistical reliability of VAE-based AD within the framework of statistical testing. Using the VAE-AD Test, the reliability of the anomaly regions detected by a VAE can be quantified in the form of p-values. This means that if an anomaly is declared when the p-value is below a certain threshold, it is possible to control the probability of false detection to a desired level. Since the VAE-AD Test is constructed based on a new statistical inference framework called selective inference, its validity is 
    
[^116]: 基于相似性的邻居选择用于图形LLMs

    Similarity-based Neighbor Selection for Graph LLMs

    [https://arxiv.org/abs/2402.03720](https://arxiv.org/abs/2402.03720)

    基于相似性的邻居选择（SNS）通过改善所选邻居的质量，改善了图形表示，并提高了泛化性能和可扩展性，解决了处理文本属性图（TAGs）的挑战。

    

    文本属性图（TAGs）在直接处理语言学习模型（LLMs）时面临独特挑战，但它们的广泛常识知识和强大的推理能力为TAGs中的节点分类提供了极大的希望。先前的研究在这一领域中已经解决了过度压缩、异质性和信息集成不当等问题，同时还受到数据集分区的不一致性和高级LLMs的低利用率的影响。为了解决这些挑战，我们引入了基于相似性的邻居选择（SNS）。使用SimCSE和高级邻居选择技术，SNS有效提高了所选邻居的质量，从而改善了图形表示并减轻了过度压缩和异质性等问题。此外，作为一种归纳和无需训练的方法，SNS在传统GNN方法上展示了更强的泛化和可伸缩性。我们的全面实验符合标准的数据集分区实践。

    Text-attributed graphs (TAGs) present unique challenges for direct processing by Language Learning Models (LLMs), yet their extensive commonsense knowledge and robust reasoning capabilities offer great promise for node classification in TAGs. Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs. To address these challenges, we introduce Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor selection techniques, SNS effectively improves the quality of selected neighbors, thereby improving graph representation and alleviating issues like over-squashing and heterophily. Besides, as an inductive and training-free approach, SNS demonstrates superior generalization and scalability over traditional GNN methods. Our comprehensive experiments, adhering to standard dataset partitioning prac
    
[^117]: 澄清：通过自然语言纠正提高模型的鲁棒性

    Clarify: Improving Model Robustness With Natural Language Corrections

    [https://arxiv.org/abs/2402.03715](https://arxiv.org/abs/2402.03715)

    论文提出了Clarify，一种通过自然语言纠正模型错误概念的方法，该方法通过用户提供简短的文本描述来纠正模型的一致失败模式，从而提高模型的鲁棒性。

    

    在监督学习中，模型被训练从静态数据集中提取相关性。这通常会导致模型依赖于高级错误概念。为了防止这种错误概念，我们必须提供额外的信息。现有的方法包括一些额外的实例级监督形式，例如标记虚假特征或来自平衡分布的额外标记数据。对于大规模数据集来说，这些策略可能会变得昂贵，因为它们需要以接近原始训练数据的规模进行额外注释。我们假设有针对性的关于模型错误概念的自然语言反馈是一种更有效的额外监督形式。我们引入了Clarify，一种新型界面和方法来交互式地纠正模型的错误概念。通过Clarify，用户只需要提供一个简短的文本描述来描述模型的一致性失败模式。然后，我们完全自动化地使用s

    In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns. Then, in an entirely automated way, we use s
    
[^118]: 在可穿戴设备上推进位置无关和设备无关的动作活动识别

    Advancing Location-Invariant and Device-Agnostic Motion Activity Recognition on Wearable Devices

    [https://arxiv.org/abs/2402.03714](https://arxiv.org/abs/2402.03714)

    本文通过对跨传感器位置的动作模型的通用性进行全面评估，确定了用于构建位置无关模型的关键身体部位，并提出了在设备上部署的动作模型，单个模型的帧级别F1得分达到91.41％，无论传感器放置如何。

    

    可穿戴传感器已经渗透到人们的生活中，在交互系统和活动识别方面产生了重大影响。然而，从业者在处理感知异构性时面临着重大障碍，需要为不同的平台定制模型。在本文中，我们对跨传感器位置的动作模型的通用性进行了全面评估。我们的分析突显了这个挑战，并确定了用于构建位置无关模型的关键身体部位，这些模型可以集成到任何设备上。为此，我们引入了最大的多位置活动数据集（N=50，累计200小时），并将其公开。我们还提出了在设备上部署的动作模型，单个模型的帧级别F1得分达到91.41％，无论传感器放置如何。最后，我们研究了跨位置数据合成，旨在通过在一个位置合成另一个位置的数据，从而减轻繁琐的数据收集任务。

    Wearable sensors have permeated into people's lives, ushering impactful applications in interactive systems and activity recognition. However, practitioners face significant obstacles when dealing with sensing heterogeneities, requiring custom models for different platforms. In this paper, we conduct a comprehensive evaluation of the generalizability of motion models across sensor locations. Our analysis highlights this challenge and identifies key on-body locations for building location-invariant models that can be integrated on any device. For this, we introduce the largest multi-location activity dataset (N=50, 200 cumulative hours), which we make publicly available. We also present deployable on-device motion models reaching 91.41% frame-level F1-score from a single model irrespective of sensor placements. Lastly, we investigate cross-location data synthesis, aiming to alleviate the laborious data collection tasks by synthesizing data in one location given data from another. These 
    
[^119]: 改进和统一离散和连续时间离散去噪扩散

    Improving and Unifying Discrete&Continuous-time Discrete Denoising Diffusion

    [https://arxiv.org/abs/2402.03701](https://arxiv.org/abs/2402.03701)

    本文提出了一种改进和统一离散和连续时间离散去噪扩散的方法。通过数学简化和推导，使得离散扩散的训练更准确易优化，并且实现了精确和加速的采样。同时，成功地统一了离散时间和连续时间离散扩散。

    

    离散扩散模型在自然离散数据如语言和图形上得到了广泛关注。虽然离散时间离散扩散已经建立了一段时间，但直到最近Campbell等人（2022）才引入了连续时间离散扩散的第一个框架。然而，他们的训练和采样过程与离散时间版本有很大差异，需要非平凡的近似才能进行可行性分析。本文首先介绍了一系列对变分下界的数学简化，这些简化使离散扩散的训练更加准确和易于优化。此外，我们推导出了一种简单的反向去噪公式，能够实现精确和加速的采样，更重要的是能够优雅地统一离散时间和连续时间离散扩散。通过更简单的分析公式，前向和现在也包括了后向概率可以灵活地适应任何噪声分布。

    Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs. Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion. However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability. In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion. In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion. Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distrib
    
[^120]: 在大规模情况下估计局部学习系数

    Estimating the Local Learning Coefficient at Scale

    [https://arxiv.org/abs/2402.03698](https://arxiv.org/abs/2402.03698)

    本文提出了一种方法，可以在深度线性网络中准确地测量高达1亿参数的局部学习系数(LLC)，并证明了估计得到的LLC具有重缩放不变性。

    

    局部学习系数(LLC)是一种量化模型复杂性的原则性方法，最初是在贝叶斯统计中使用奇异学习理论(SLT)推导出来的。已知有几种数值估计局部学习系数的方法，但迄今为止这些方法尚未扩展到现代深度学习架构或数据集的规模。通过在arXiv:2308.12108 [stat.ML]中开发的一种方法，我们经验证明可以准确和自洽地测量深度线性网络(DLN)中高达1亿参数的局部学习系数(LLC)。我们还证明了估计得到的LLC具有理论数量所具备的重缩放不变性。

    The \textit{local learning coefficient} (LLC) is a principled way of quantifying model complexity, originally derived in the context of Bayesian statistics using singular learning theory (SLT). Several methods are known for numerically estimating the local learning coefficient, but so far these methods have not been extended to the scale of modern deep learning architectures or data sets. Using a method developed in {\tt arXiv:2308.12108 [stat.ML]} we empirically show how the LLC may be measured accurately and self-consistently for deep linear networks (DLNs) up to 100M parameters. We also show that the estimated LLC has the rescaling invariance that holds for the theoretical quantity.
    
[^121]: 基于模型生命周期视角的垂直联邦学习中的隐私威胁和防御综述

    A Survey of Privacy Threats and Defense in Vertical Federated Learning: From Model Life Cycle Perspective

    [https://arxiv.org/abs/2402.03688](https://arxiv.org/abs/2402.03688)

    本文综述了垂直联邦学习中隐私威胁和防御的最新研究进展，通过对模型生命周期的视角进行讨论，提供了行业和实践者在保护数据隐私方面的指导和见解。

    

    垂直联邦学习（Vertical Federated Learning，VFL）是一种联邦学习范式，多个参与者共同训练机器学习模型，这些参与者共享相同的样本集，但持有不同的特征。虽然VFL在不共享原始数据的情况下实现了协同机器学习，但仍然容易受到各种隐私威胁的影响。本文首次对VFL中的隐私攻击和防御的最新研究进行了全面的调查。我们根据特征对攻击和防御进行分类，并讨论了开放性挑战和未来的研究方向。具体而言，我们的讨论围绕模型的生命周期展开，深入探讨了机器学习的不同阶段遇到的隐私威胁及相应的对策。这项调查既为研究界提供了资源，也为实践者提供了明确的指导和可行的见解，以保护数据隐私。

    Vertical Federated Learning (VFL) is a federated learning paradigm where multiple participants, who share the same set of samples but hold different features, jointly train machine learning models. Although VFL enables collaborative machine learning without sharing raw data, it is still susceptible to various privacy threats. In this paper, we conduct the first comprehensive survey of the state-of-the-art in privacy attacks and defenses in VFL. We provide taxonomies for both attacks and defenses, based on their characterizations, and discuss open challenges and future research directions. Specifically, our discussion is structured around the model's life cycle, by delving into the privacy threats encountered during different stages of machine learning and their corresponding countermeasures. This survey not only serves as a resource for the research community but also offers clear guidance and actionable insights for practitioners to safeguard data privacy throughout the model's life c
    
[^122]: Pard: 具有置换不变性的自回归扩散用于图生成

    Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation

    [https://arxiv.org/abs/2402.03687](https://arxiv.org/abs/2402.03687)

    PARD是一种将扩散模型与自回归方法相结合的置换不变性自回归扩散模型，通过使用图中的部分顺序以块逐块的自回归方式生成图。

    

    尽管自回归模型对于图的顺序敏感，但其简单有效，在图生成领域一直占据主导地位。然而，扩散模型因其置换不变性而越来越受关注。目前的图扩散模型一次性生成图，但需要额外的特征和成千上万步的去噪才能达到最佳性能。我们引入了PARD，一种将扩散模型与自回归方法相结合的置换不变性自回归扩散模型。PARD利用自回归模型的效果和效率，同时保持置换不变性，无需关注图的顺序敏感性。具体来说，我们发现与集合不同，图中的元素并不是完全无序的，节点和边有一个独特的部分顺序。利用这个部分顺序，PARD以块逐块的自回归方式生成图，其中每个块的概率为c。

    Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is c
    
[^123]: RL-VLM-F: 强化学习通过视觉语言基础模型反馈

    RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback

    [https://arxiv.org/abs/2402.03681](https://arxiv.org/abs/2402.03681)

    RL-VLM-F是一种通过视觉语言基础模型反馈的强化学习方法，能够自动生成有效的奖励函数和策略，从而解决了传统强化学习中奖励设计的挑战。

    

    传统强化学习研究中的奖励设计一直是一个挑战，因为通常需要大量人力和反复试错的过程来设计有效的奖励函数。本文提出了一种自动生成奖励函数的方法，用于代理学习新任务，只使用任务目标的文本描述和代理的视觉观测，并利用视觉语言基础模型（VLMs）的反馈。我们的方法的关键是通过查询这些模型，基于任务目标的文本描述给出对代理的图像观测的偏好，并从偏好标签中学习奖励函数，而不是直接要求这些模型输出原始奖励分数，这可能存在噪音和不一致性。我们证明了RL-VLM-F在各种领域中成功地产生了有效的奖励和策略，包括经典控制以及刚性和灵活操纵方面。

    Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
    
[^124]: 逻辑规范引导下的强化学习智能体动态任务采样

    Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents

    [https://arxiv.org/abs/2402.03678](https://arxiv.org/abs/2402.03678)

    本文提出了一种逻辑规范引导下的动态任务采样（LSTS）方法，通过学习一组强化学习策略，根据高级任务规范指导智能体在最小化环境交互次数的同时实现从初始状态到目标状态的引导。在网格世界实验中，LSTS实现了改进的时间到阈值。

    

    强化学习（RL）在使人工智能智能体学习多样化行为方面取得了重要进展。然而，学习有效的策略通常需要大量的环境交互。为了减少样本复杂性问题，最近的方法使用高级任务规范，如线性时态逻辑（LTL$_f$）公式或奖励机器（RM），来指导智能体的学习过程。在这项工作中，我们提出了一种新颖的方法，称为逻辑规范引导下的动态任务采样（LSTS），它通过学习一组强化学习策略，根据高级任务规范指导智能体从初始状态到目标状态，同时最小化环境交互次数。与以前的工作不同，LSTS不假设环境动力学或奖励机器的信息，并动态采样导致成功目标策略的有希望的任务。我们在一个网格世界上评估了LSTS，并展示了它实现了改进的时间到阈值。

    Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshol
    
[^125]: 使用PPIretrieval进行有效的蛋白质相互作用探索

    Effective Protein-Protein Interaction Exploration with PPIretrieval

    [https://arxiv.org/abs/2402.03675](https://arxiv.org/abs/2402.03675)

    PPIretrieval是第一个基于深度学习的模型，可以在嵌入空间中有效搜索潜在PPIs，并捕捉蛋白质表面的丰富几何和化学信息。

    

    蛋白质相互作用（PPIs）在调控许多细胞功能中至关重要，包括信号传导、运输和免疫防御。随着多链蛋白质复合物结构预测准确性的提高，挑战已经转向有效地导航庞大的复杂宇宙以识别潜在的PPIs。在这里，我们提出了PPIretrieval，这是第一个基于深度学习的蛋白质相互作用探索模型，它利用现有的PPI数据在嵌入空间中有效搜索潜在的PPIs，捕捉蛋白质表面的丰富几何和化学信息。当提供了一个未见过的查询蛋白质及其相关的结合位点时，PPIretrieval能够在嵌入空间中有效识别潜在的结合伴侣及其相应的结合位点，促进蛋白质相互作用复合物的形成。

    Protein-protein interactions (PPIs) are crucial in regulating numerous cellular functions, including signal transduction, transportation, and immune defense. As the accuracy of multi-chain protein complex structure prediction improves, the challenge has shifted towards effectively navigating the vast complex universe to identify potential PPIs. Herein, we propose PPIretrieval, the first deep learning-based model for protein-protein interaction exploration, which leverages existing PPI data to effectively search for potential PPIs in an embedding space, capturing rich geometric and chemical information of protein surfaces. When provided with an unseen query protein with its associated binding site, PPIretrieval effectively identifies a potential binding partner along with its corresponding binding site in an embedding space, facilitating the formation of protein-protein complexes.
    
[^126]: 高效求解偏差Gromov-Wasserstein问题

    Efficient Solvers for Partial Gromov-Wasserstein

    [https://arxiv.org/abs/2402.03664](https://arxiv.org/abs/2402.03664)

    本文提出了两个基于Frank-Wolfe算法的新的高效求解器来解决偏差Gromov-Wasserstein问题，并且证明了PGW问题构成了度量测度空间的度量。

    

    偏差Gromov-Wasserstein（PGW）问题可以比较具有不均匀质量的度量空间中的测度，从而实现这些空间之间的不平衡和部分匹配。本文证明了PGW问题可以转化为Gromov-Wasserstein问题的一个变种，类似于把偏差最优运输问题转化为最优运输问题。这个转化导致了两个新的求解器，基于Frank-Wolfe算法，数学和计算上等价，提供了高效的PGW问题解决方案。我们进一步证明了PGW问题构成了度量测度空间的度量。最后，我们通过与现有基线方法在形状匹配和正样本未标记学习问题上的计算时间和性能比较，验证了我们提出的求解器的有效性。

    The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of measures with unequal masses residing in potentially distinct metric spaces, thereby enabling unbalanced and partial matching across these spaces. In this paper, we demonstrate that the PGW problem can be transformed into a variant of the Gromov-Wasserstein problem, akin to the conversion of the partial optimal transport problem into an optimal transport problem. This transformation leads to two new solvers, mathematically and computationally equivalent, based on the Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem. We further establish that the PGW problem constitutes a metric for metric measure spaces. Finally, we validate the effectiveness of our proposed solvers in terms of computation time and performance on shape-matching and positive-unlabeled learning problems, comparing them against existing baselines.
    
[^127]: 深度神经网络中包含符号层的符号正确性

    Symbol Correctness in Deep Neural Networks Containing Symbolic Layers

    [https://arxiv.org/abs/2402.03663](https://arxiv.org/abs/2402.03663)

    本文介绍了神经符号深度神经网络（NS-DNNs）中的符号正确性原则，即用于推理的神经层对中间符号的预测必须与输入数据的符号表示相匹配。符号正确性是NS-DNN可解释性和迁移学习的必要特性，并为推理和交流模型行为提供了精确的方法。

    

    为了处理以感知和逻辑推理相结合的人工智能任务，最近的工作引入了神经符号深度神经网络（NS-DNNs），它们除了传统的神经层之外，还包含符号层：在推理过程中由符号求解器评估的符号表达式（例如，SAT公式，逻辑程序）。我们确定并形式化了一种直观、高层次的原则，可以指导NS-DNNs的设计和分析：符号正确性，即神经层对中间符号的正确性，相对于输入数据的（通常未知的）基本符号表示。我们证明了符号正确性是NS-DNN可解释性和迁移学习的必要特性（尽管通常无法进行训练）。此外，我们还展示了符号正确性框架在神经符号边界处推理和交流模型行为方面提供了一种精确的方法，并对基本权衡有了深入的理解。

    To handle AI tasks that combine perception and logical reasoning, recent work introduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- in addition to traditional neural layers -- symbolic layers: symbolic expressions (e.g., SAT formulas, logic programs) that are evaluated by symbolic solvers during inference. We identify and formalize an intuitive, high-level principle that can guide the design and analysis of NS-DNNs: symbol correctness, the correctness of the intermediate symbols predicted by the neural layers with respect to a (generally unknown) ground-truth symbolic representation of the input data. We demonstrate that symbol correctness is a necessary property for NS-DNN explainability and transfer learning (despite being in general impossible to train for). Moreover, we show that the framework of symbol correctness provides a precise way to reason and communicate about model behavior at neural-symbolic boundaries, and gives insight into the fundamental tradeoffs
    
[^128]: 在图上进行传递式奖励推断

    Transductive Reward Inference on Graph

    [https://arxiv.org/abs/2402.03661](https://arxiv.org/abs/2402.03661)

    该研究提出了一种在图上进行传递式奖励推断的方法，可以有效地估计离线强化学习中未标记数据的奖励。通过利用有限的人工奖励注释和可用数据构建奖励传播图，并利用图进行奖励推断，从而推断出未标记数据的奖励。

    

    在这项研究中，我们提出了一种在奖励信息传播图上进行传递式推断的方法，从而能够有效地估计离线强化学习中未标记数据的奖励。奖励推断是在实际场景中学习有效策略的关键，而直接的环境交互要么成本太高，要么是不道德的，并且很少有可访问的奖励函数，例如在医疗保健和机器人领域。我们的研究集中于开发一种基于图上信息传播的上下文特性的奖励推断方法，利用有限数量的人工奖励注释来推断未标记数据的奖励。我们利用可用数据和有限的奖励注释构建奖励传播图，其中边权重包含与奖励相关的各种影响因素。随后，我们使用构建的图进行传递式奖励推断，从而估计奖励。

    In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning. Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics. Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards
    
[^129]: 预训练-微调范式中出现了跨任务线性关系

    Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm

    [https://arxiv.org/abs/2402.03660](https://arxiv.org/abs/2402.03660)

    本文发现了在预训练-微调范式中，使用相同预训练检查点初始化并在不同任务上进行微调的模型会出现一个有趣的线性现象，称为跨任务线性。我们提供了实证证据并推测神经网络在这一范式中本质上类似线性映射，从参数空间到特征空间的映射。这一发现揭示了关于模型合并/编辑和参数共享等方面的新见解。

    

    预训练-微调范式已成为现代深度学习的主流趋势。在这项工作中，我们发现在从公共预训练检查点初始化并在不同任务上进行微调的模型中出现了一个有趣的线性现象，称为跨任务线性（CTL）。具体而言，如果我们线性插值两个微调模型的权重，权重插值模型中的特征大致等于每层中两个微调模型特征的线性插值。这样的跨任务线性在同行文献中尚未被注意到。我们提供了全面的实证证据，支持从相同预训练检查点开始的微调模型一致出现CTL。我们推测在预训练-微调范式中，神经网络本质上是线性映射，从参数空间到特征空间的映射。基于这个观点，我们的研究揭示了关于模型合并/编辑、参数共享等的新见解。

    The pretraining-finetuning paradigm has become the prevailing trend in modern deep learning. In this work, we discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL). Specifically, if we linearly interpolate the weights of two finetuned models, the features in the weight-interpolated model are approximately equal to the linear interpolation of features in two finetuned models at each layer. Such cross-task linearity has not been noted in peer literature. We provide comprehensive empirical evidence supporting that CTL consistently occurs for finetuned models that start from the same pretrained checkpoint. We conjecture that in the pretraining-finetuning paradigm, neural networks essentially function as linear maps, mapping from the parameter space to the feature space. Based on this viewpoint, our study unveils novel insights into explaining model merging/editing, p
    
[^130]: 使用自反大型语言模型学习生成可解释的股票预测

    Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models

    [https://arxiv.org/abs/2402.03659](https://arxiv.org/abs/2402.03659)

    这个论文介绍了使用大型语言模型生成可解释的股票预测的方法，并提出了Summarize-Explain-Predict（SEP）模型来解决股票预测中的解释问题和数据标注成本的挑战。

    

    对于传统的非生成式深度学习模型来说，解释股票预测通常是一项困难的任务，其中解释仅限于可视化重要文本上的注意力权重。目前，大型语言模型（LLM）为解决这个问题提供了一个解决方案，因为它们具有生成人类可读解释其决策过程的能力。然而，股票预测对LLM来说仍然具有挑战性，因为它需要能够权衡混乱社会文本对股票价格的不同影响。随着引入解释组件，问题变得越来越困难，需要LLM能够用口头方式解释为什么某些因素比其他因素更重要。另一方面，要为这样的任务对LLM进行微调，需要专家标注的样本来解释训练集中的每次股票波动，这在成本和实际可扩展性上是昂贵且不可行的。为了解决这些问题，我们提出了我们的Summarize-Explain-Predict（SEP）模型。

    Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) 
    
[^131]: 使用神经网络通过嵌套低秩近似实现运算符的奇异值分解

    Operator SVD with Neural Networks via Nested Low-Rank Approximation

    [https://arxiv.org/abs/2402.03655](https://arxiv.org/abs/2402.03655)

    本文提出了一个新的优化框架，使用嵌套的低秩近似方法通过神经网络实现运算符的奇异值分解。该方法通过无约束优化公式隐式高效地保持学习函数的正交性。

    

    在许多机器学习和科学计算问题中，计算给定线性算子的特征值分解（EVD）或找到其主要特征值和特征函数是一项基础任务。对于高维特征值问题，训练神经网络参数化特征函数被认为是传统数值线性代数技术的有希望的替代方法。本文提出了一个新的优化框架，基于截断奇异值分解的低秩近似表征，并伴随着称为嵌套的学习方法，以正确的顺序学习前L个奇异值和奇异函数。所提出的方法通过无约束优化公式隐式高效地促进了学习函数的正交性，这个公式可以很容易地通过现成的基于梯度的优化算法求解。我们展示了所提出的优化框架在使用案例中的有效性。

    Computing eigenvalue decomposition (EVD) of a given linear operator, or finding its leading eigenvalues and eigenfunctions, is a fundamental task in many machine learning and scientific computing problems. For high-dimensional eigenvalue problems, training neural networks to parameterize the eigenfunctions is considered as a promising alternative to the classical numerical linear algebra techniques. This paper proposes a new optimization framework based on the low-rank approximation characterization of a truncated singular value decomposition, accompanied by new techniques called nesting for learning the top-$L$ singular values and singular functions in the correct order. The proposed method promotes the desired orthogonality in the learned functions implicitly and efficiently via an unconstrained optimization formulation, which is easy to solve with off-the-shelf gradient-based optimization algorithms. We demonstrate the effectiveness of the proposed optimization framework for use cas
    
[^132]: 使用TGX进行时间图分析

    Temporal Graph Analysis with TGX

    [https://arxiv.org/abs/2402.03651](https://arxiv.org/abs/2402.03651)

    TGX是一个专为分析时间网络而设计的Python软件包，提供了自动化的数据加载、数据处理和演化图分析。它支持多个内置数据集和外部数据集的访问，并提供数据处理和网络分析功能，使得处理和研究时间网络变得更加便捷。

    

    现实世界中的网络，随着其不断变化的关系，最能以时间图形式捕捉。然而，现有的软件库主要是针对静态图设计的，忽略了时间图的动态性质。为了填补这一空白，我们引入了TGX，这是一个专为分析时间网络而设计的Python软件包，包含了自动化的数据加载、数据处理和演化图分析的工作流程。TGX提供了11个内置数据集和8个外部时间图基准（TGB）数据集的访问，以及任何以.csv格式的新数据集。除了数据加载外，TGX还提供了数据处理功能，例如对时间图的离散化和节点子采样，以加速处理较大的数据集。为了全面的研究，TGX通过提供多样化的度量指标（如平均节点度和每个时间戳的节点和边的演化数量）以进行网络分析。此外，该软件包还整合了度量指标的计算及对图形进行可视化的功能。

    Real-world networks, with their evolving relations, are best captured as temporal graphs. However, existing software libraries are largely designed for static graphs where the dynamic nature of temporal graphs is ignored. Bridging this gap, we introduce TGX, a Python package specially designed for analysis of temporal networks that encompasses an automated pipeline for data loading, data processing, and analysis of evolving graphs. TGX provides access to eleven built-in datasets and eight external Temporal Graph Benchmark (TGB) datasets as well as any novel datasets in the .csv format. Beyond data loading, TGX facilitates data processing functionalities such as discretization of temporal graphs and node subsampling to accelerate working with larger datasets. For comprehensive investigation, TGX offers network analysis by providing a diverse set of measures, including average node degree and the evolving number of nodes and edges per timestamp. Additionally, the package consolidates mea
    
[^133]: 多线性核回归和流形学习的数据插补方法

    Multilinear Kernel Regression and Imputation via Manifold Learning

    [https://arxiv.org/abs/2402.03648](https://arxiv.org/abs/2402.03648)

    本论文介绍了一种基于多线性核回归和流形学习的非参数数据插补方法，可以用于处理缺失数据，并且在建模和计算方面具有鲁棒性和高效性。

    

    本论文介绍了一种新颖的非参数数据插补方法，被称为多线性核回归和流形学习插补方法（MultiL-KRIM）。受到流形学习的启发，MultiL-KRIM把数据特征建模为位于或接近嵌入在再生核希尔伯特空间中的未知平滑流形的点云。与典型的流形学习方法不同，后者通过基于图拉普拉斯矩阵的正则化器寻找低维模式，MultiL-KRIM则建立在对流形的切空间的直观概念上，并将点云邻居之间的协作（回归器）直接纳入损失函数的数据建模项中。多个核函数的使用提供了鲁棒性和丰富的逼近性质，而多个矩阵因子提供了低秩建模，整合了降维，并且在没有训练数据的情况下简化了计算。两个重要的应用领域展示了该方法的功能。

    This paper introduces a novel nonparametric framework for data imputation, coined multilinear kernel regression and imputation via the manifold assumption (MultiL-KRIM). Motivated by manifold learning, MultiL-KRIM models data features as a point cloud located in or close to a user-unknown smooth manifold embedded in a reproducing kernel Hilbert space. Unlike typical manifold-learning routes, which seek low-dimensional patterns via regularizers based on graph-Laplacian matrices, MultiL-KRIM builds instead on the intuitive concept of tangent spaces to manifolds and incorporates collaboration among point-cloud neighbors (regressors) directly into the data-modeling term of the loss function. Multiple kernel functions are allowed to offer robustness and rich approximation properties, while multiple matrix factors offer low-rank modeling, integrate dimensionality reduction, and streamline computations with no need of training data. Two important application domains showcase the functionality
    
[^134]: CAMBranch: 基于增强MILP的对比学习用于分支

    CAMBranch: Contrastive Learning with Augmented MILPs for Branching

    [https://arxiv.org/abs/2402.03647](https://arxiv.org/abs/2402.03647)

    CAMBranch是一个利用对比学习和增强MILP的机器学习框架，用于改进混合整数线性规划的分支策略。通过生成增强MILP并应用对比学习，CAMBranch能够获取大量标记的专家样本，从而提高分支决策的质量。

    

    最近的研究已经引入了机器学习框架来增强混合整数线性规划(B\&B)的分支策略。这些方法主要依赖于强分支的模仿学习，并展现了卓越的性能。然而，收集用于模仿学习的专家样本，特别是用于强分支的样本，是一项耗时的任务。为了解决这一挑战，我们提出了CAMBranch: 基于增强MILP的对比学习用于分支的框架，通过对有限数量的来自原始MILP的专家数据应用变量转移来生成增强MILP (AMILP)。这种方法可以获取大量标记的专家样本。CAMBranch利用MILP和AMILP进行模仿学习，并采用对比学习来提高模型捕捉MILP特征的能力，从而提高分支决策的质量。

    Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B\&B) branching policies for solving Mixed Integer Linear Programming (MILP). These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance. However, collecting expert samples for imitation learning, particularly for Strong Branching, is a time-consuming endeavor. To address this challenge, we propose \textbf{C}ontrastive Learning with \textbf{A}ugmented \textbf{M}ILPs for \textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs (AMILPs) by applying variable shifting to limited expert data from their original MILPs. This approach enables the acquisition of a considerable number of labeled expert samples. CAMBranch leverages both MILPs and AMILPs for imitation learning and employs contrastive learning to enhance the model's ability to capture MILP features, thereby improving the quality of branching decisions. Experimental resul
    
[^135]: Lens: 网络流量的基础模型

    Lens: A Foundation Model for Network Traffic

    [https://arxiv.org/abs/2402.03646](https://arxiv.org/abs/2402.03646)

    "Lens"是一个基于T5架构的基础网络流量模型，通过学习大规模无标签数据的预训练表示，能够在流量理解和生成任务中取得精确的预测和生成。

    

    网络流量是指通过互联网或连接计算机的任何系统发送和接收的信息量。分析和理解网络流量对于提高网络安全和管理至关重要。然而，由于数据包的特殊特性，如异构标头和缺乏语义的加密负载，网络流量的分析带来了巨大的挑战。为了捕捉流量的潜在语义，一些研究采用了基于Transformer编码器或解码器的预训练技术，从大规模的流量数据中学习表示。然而，这些方法通常只在流量理解（分类）或流量生成任务中表现出色。为了解决这个问题，我们开发了Lens，这是一个基础的网络流量模型，利用T5架构从大规模的无标签数据中学习预训练表示。借助编码器-解码器框架的优势，该模型能够捕捉全局和局部特征，实现精确的流量预测和生成。

    Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data. However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the glob
    
[^136]: Stanceosaurus 2.0: 对俄罗斯和西班牙的虚假信息分类

    Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation

    [https://arxiv.org/abs/2402.03642](https://arxiv.org/abs/2402.03642)

    Stanceosaurus 2.0扩展了原始框架，新增对俄罗斯和西班牙的分类，旨在支持分析跨文化和跨语言的虚假信息。通过与初始研究的结果相当的零-shot跨语言迁移，验证了数据的价值和立场分类的可行性。

    

    Stanceosaurus 语料库（Zheng等，2022）旨在提供高质量的、标注的、从Twitter中提取的五分类立场数据，适用于分析跨文化和跨语言的虚假信息。在Stanceosaurus 2.0版本中，我们扩展了这个框架，包括了俄罗斯和西班牙。前者由于与西方紧张局势和对乌克兰的暴力入侵而变得具有重要意义。而后者则代表着一个被主要社交媒体平台忽视的庞大社群。通过添加3,874条西班牙和俄语推文，涉及41则虚假信息，我们的目标是支持关注这些问题的研究。为了展示这些数据的价值，我们在多语言BERT上使用零-shot跨语言迁移，得到了与最初的Stanceosaurus研究相当的结果，两种语言的macro F1得分均为43。这显示了立场分类的可行性。

    The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide high-quality, annotated, 5-way stance data extracted from Twitter, suitable for analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus 2.0 iteration, we extend this framework to encompass Russian and Spanish. The former is of current significance due to prevalent misinformation amid escalating tensions with the West and the violent incursion into Ukraine. The latter, meanwhile, represents an enormous community that has been largely overlooked on major social media platforms. By incorporating an additional 3,874 Spanish and Russian tweets over 41 misinformation claims, our objective is to support research focused on these issues. To demonstrate the value of this data, we employed zero-shot cross-lingual transfer on multilingual BERT, yielding results on par with the initial Stanceosaurus study with a macro F1 score of 43 for both languages. This underlines the viability of stance classificatio
    
[^137]: 私有推断的线性化对群体准确性的不对称影响

    Disparate Impact on Group Accuracy of Linearization for Private Inference

    [https://arxiv.org/abs/2402.03629](https://arxiv.org/abs/2402.03629)

    本文研究了线性化对隐私推断中群体准确性的影响，发现减少ReLU激活函数数量会不成比例地降低少数群体的准确性，而对于多数群体则几乎没有影响。采用简单的微调步骤可以解决这个问题。

    

    确保对具有密码安全性的数据进行隐私保护的推断是一个众所周知的计算挑战。为了减轻非线性激活函数中昂贵的加密计算的瓶颈，最近的方法建议在线性神经网络中线性化目标部分的激活函数。这种技术可以显著减少运行时间，对准确性的影响往往可以忽略不计。在本文中，我们证明了这种计算优势可能导致公平性成本增加。具体而言，我们发现减少ReLU激活函数数量会不成比例地降低少数群体的准确性，而对于多数群体则几乎没有影响。为了解释这些观察结果，我们在对决策边界性质进行限制性假设的基础上提供了数学解释，同时还展示了这个问题在广泛使用的数据集和体系结构中的普遍性。最后，我们展示了如何通过简单的程序改变线性模型的微调步骤来解决这个问题。

    Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the fine-tuning step for linearized models ca
    
[^138]: ReLU神经网络的凸松弛在多项式时间内逼近全局最优解

    Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time

    [https://arxiv.org/abs/2402.03625](https://arxiv.org/abs/2402.03625)

    本文研究了两层ReLU网络在加权衰减正则化下及其凸松弛之间的最优性差距，证明了当训练数据是随机的时候，相对最优性差距可以被一个$O(\sqrt{\log n})$的因子界限。此外，在温和的假设下，局部梯度方法几乎肯定会收敛到训练损失较低的点。

    

    本文研究了两层ReLU网络在加权衰减正则化下及其凸松弛之间的最优性差距。我们证明了当训练数据是随机的时候，原始问题与其凸松弛之间的相对最优性差距可以被一个$O(\sqrt{\log n})$的因子界限，其中$n$是训练样本的数量。一个简单的应用可以导出一个可行的多项式时间算法，该算法能够保证在对数因子范围内解决原始的非凸问题。此外，在温和的假设下，我们证明了在参数的随机初始化下，局部梯度方法几乎肯定会收敛到训练损失较低的点。我们的结果相对于现有结果而言是指数级的改进，并且揭示了为什么局部梯度方法表现良好的新见解。

    In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations. We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of $O(\sqrt{\log n})$, where $n$ is the number of training samples. A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor. Moreover, under mild assumptions, we show that with random initialization on the parameters local gradient methods almost surely converge to a point that has low training loss. Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well.
    
[^139]: 概率电路中用于边际MAP的神经网络近似器

    Neural Network Approximators for Marginal MAP in Probabilistic Circuits

    [https://arxiv.org/abs/2402.03621](https://arxiv.org/abs/2402.03621)

    本文提出了一种使用神经网络近似概率电路中边际MAP推理的方法，该方法通过使用连续多线性函数来估计查询变量的赋值成本并将其作为损失函数，具有自我监督和高效性的优点。

    

    概率电路（PCs）如和积网络以高效地表示大型多变量概率分布。在实践中，与贝叶斯网络和马尔可夫网络等其他概率表示相比，PCs更受青睐，因为PCs可以在网络大小线性扩展的时间内解决边际推理（MAR）任务。然而，最大后验概率（MAP）和边际MAP（MMAP）任务在这些模型中仍然是NP困难的。受最近关于使用神经网络生成接近最优解的优化问题（如整数线性规划）的工作的启发，我们提出了一种方法，该方法使用神经网络来近似PCs中的(M)MAP推理。我们方法的关键思想是使用连续多线性函数来近似查询变量的赋值成本，然后将其用作损失函数。我们的新方法有两个主要优点，即自我监督和在学习神经网络之后，它只需要

    Probabilistic circuits (PCs) such as sum-product networks efficiently represent large multi-variate probability distributions. They are preferred in practice over other probabilistic representations such as Bayesian and Markov networks because PCs can solve marginal inference (MAR) tasks in time that scales linearly in the size of the network. Unfortunately, the maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in these models. Inspired by the recent work on using neural networks for generating near-optimal solutions to optimization problems such as integer linear programming, we propose an approach that uses neural networks to approximate (M)MAP inference in PCs. The key idea in our approach is to approximate the cost of an assignment to the query variables using a continuous multilinear function, and then use the latter as a loss function. The two main benefits of our new method are that it is self-supervised and after the neural network is learned, it requires 
    
[^140]: 贝叶斯分解格兰杰因果图用于多变量时间序列数据的研究

    Bayesian Factorised Granger-Causal Graphs For Multivariate Time-series Data

    [https://arxiv.org/abs/2402.03614](https://arxiv.org/abs/2402.03614)

    本研究提出了一种新的贝叶斯VAR模型，利用分层图先验推断二元格兰杰因果图的后验概率。相比竞争方法，我们的方法在不确定性量化、超参数数量和稀疏多变量时间序列数据上都表现更好。

    

    我们研究了自动发现多变量时间序列数据中格兰杰因果关系的问题。矢量自回归(VAR)模型已经在解决这个问题上经过了时间的考验，包括贝叶斯变种和使用深度神经网络的最新发展。大多数现有的VAR格兰杰因果方法使用稀疏性诱导惩罚/先验或事后阈值来解释它们的系数作为格兰杰因果图。相反，我们提出了一个新的贝叶斯VAR模型，其中包含了一个分层图先验来表示二元格兰杰因果图，与VAR系数分开考虑。我们开发了一种高效的算法来推断二元格兰杰因果图的后验概率。我们的方法提供了更好的不确定性量化，较少的超参数，并在稀疏多变量时间序列数据上实现了更好的性能。

    We study the problem of automatically discovering Granger causal relations from observational multivariate time-series data. Vector autoregressive (VAR) models have been time-tested for this problem, including Bayesian variants and more recent developments using deep neural networks. Most existing VAR methods for Granger causality use sparsity-inducing penalties/priors or post-hoc thresholds to interpret their coefficients as Granger causal graphs. Instead, we propose a new Bayesian VAR model with a hierarchical graph prior over binary Granger causal graphs, separately from the VAR coefficients. We develop an efficient algorithm to infer the posterior over binary Granger causal graphs. Our method provides better uncertainty quantification, has less hyperparameters, and achieves better performance than competing approaches, especially on sparse multivariate time-series data.
    
[^141]: RAP：具有上下文记忆的检索增强规划用于多模态LLM代理

    RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents

    [https://arxiv.org/abs/2402.03610](https://arxiv.org/abs/2402.03610)

    本研究提出了一种称为RAP的框架，它能够以动态方式利用过去的经验来增强代理的规划能力，并在纯文本和多模态环境中表现出色。实验结果显示RAP在文本场景中达到了最先进水平，并显著提高了多模态LLM代理在具身任务中的性能。

    

    最近的进展使得大型语言模型(LLM)可以被部署为用于机器人、游戏和API集成等领域中越来越复杂的决策应用的代理。然而，利用过去的经验来指导当前的决策过程仍然是一个困难的问题。为解决这个问题，我们提出了一种称为检索增强规划（RAP）的框架，旨在动态地利用过去与当前情况和上下文相关的经验，从而提升代理的规划能力。RAP的特点在于它的多功能性：它在纯文本和多模态环境中表现出色，适用于多个任务。实证评估结果显示RAP的有效性，在文本场景中取得了SOTA的表现，并显著提升了多模态LLM代理在具身任务中的性能。这些结果突显了RAP在推进LLM的功能和适用性方面的潜力。

    Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM 
    
[^142]: 使用大型语言模型从实际数据中识别避孕药切换的原因

    Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models

    [https://arxiv.org/abs/2402.03597](https://arxiv.org/abs/2402.03597)

    本研究评估了一种大型语言模型GPT-4在识别避孕药切换原因上的能力，结果表明GPT-4可以准确地从临床记录中提取避孕药切换的原因，相较于基准BERT模型有更好的表现。

    

    处方避孕药在支持妇女生殖健康方面扮演着关键角色。在美国有将近5000万女性使用避孕药，了解导致避孕药选择和切换的因素非常重要。然而，与药物切换相关的许多因素通常只在无结构的临床记录中得到捕获，并且很难提取。在这里，我们评估了最近开发的大型语言模型GPT-4（通过符合HIPAA的Microsoft Azure API）的零-shot能力，以从UCSF信息共享平台的临床记录数据集中识别避孕药类别切换的原因。我们证明了GPT-4可以准确地提取避孕药切换的原因，相较于基准BERT模型，在避孕药开始和停止提取方面的microF1分数分别为0.849和0.881。对于GPT-4提取的切换原因的人工评估显示出91.4%的准确度，出现幻觉的情况很少。

    Prescription contraceptives play a critical role in supporting women's reproductive health. With nearly 50 million women in the United States using contraceptives, understanding the factors that drive contraceptives selection and switching is of significant interest. However, many factors related to medication switching are often only captured in unstructured clinical notes and can be difficult to extract. Here, we evaluate the zero-shot abilities of a recently developed large language model, GPT-4 (via HIPAA-compliant Microsoft Azure API), to identify reasons for switching between classes of contraceptives from the UCSF Information Commons clinical notes dataset. We demonstrate that GPT-4 can accurately extract reasons for contraceptive switching, outperforming baseline BERT-based models with microF1 scores of 0.849 and 0.881 for contraceptive start and stop extraction, respectively. Human evaluation of GPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal hallucin
    
[^143]: 评估分布转变对强化学习性能的影响

    Assessing the Impact of Distribution Shift on Reinforcement Learning Performance

    [https://arxiv.org/abs/2402.03590](https://arxiv.org/abs/2402.03590)

    评估强化学习性能时需要考虑分布转变，我们提出了一套评估方法，并推荐使用时间序列分析进行观测RL评估。

    

    机器学习的研究正在解决自身的可重复性危机。特别是，强化学习(RL)面临着一系列独特的挑战。比较点估计和在训练过程中显示成功收敛到最优策略的图表可能会掩盖过拟合或对实验设置的依赖性。虽然RL的研究人员提出了可靠性指标以考虑不确定性，以更好地理解每个算法的优势和劣势，但过去的工作建议并不假设存在超出分布的观测结果。我们提出了一套评估方法，衡量了在分布转变下RL算法的稳健性。本文介绍的工具支持在代理在其环境中行动时考虑性能的需求。我们尤其推荐使用时间序列分析作为观测RL评估的方法。我们还展示了RL和模拟动力学的独特属性。

    Research in machine learning is making progress in fixing its own reproducibility crisis. Reinforcement learning (RL), in particular, faces its own set of unique challenges. Comparison of point estimates, and plots that show successful convergence to the optimal policy during training, may obfuscate overfitting or dependence on the experimental setup. Although researchers in RL have proposed reliability metrics that account for uncertainty to better understand each algorithm's strengths and weaknesses, the recommendations of past work do not assume the presence of out-of-distribution observations. We propose a set of evaluation methods that measure the robustness of RL algorithms under distribution shifts. The tools presented here argue for the need to account for performance over time while the agent is acting in its environment. In particular, we recommend time series analysis as a method of observational RL evaluation. We also show that the unique properties of RL and simulated dyna
    
[^144]: 自行车共享系统中动态再平衡的强化学习方法

    A Reinforcement Learning Approach for Dynamic Rebalancing in Bike-Sharing System

    [https://arxiv.org/abs/2402.03589](https://arxiv.org/abs/2402.03589)

    本研究介绍了一种针对自行车共享系统中动态再平衡问题的时空强化学习算法，通过多智能体马尔可夫决策过程实现独立和协作的车辆再平衡，解决了传统数学优化方法的不实际限制。

    

    自行车共享系统提供环保的城市出行方式，有助于缓解交通拥堵，促进健康生活方式。由于行程需求的随机性，这些系统的有效运营和保持高客户满意度具有挑战性，常常出现满站或空站现象。为了解决这个问题，使用车辆重新分配自行车到不同站点的再平衡策略至关重要。本文引入了一种时空强化学习算法，用于解决带有多辆车辆的动态再平衡问题。首先，在连续时间框架中将问题建模为多智能体马尔可夫决策过程。这允许独立和协作的车辆再平衡，消除了基于时间离散化模型的不切实际的限制。

    Bike-Sharing Systems provide eco-friendly urban mobility, contributing to the alleviation of traffic congestion and to healthier lifestyles. Efficiently operating such systems and maintaining high customer satisfaction is challenging due to the stochastic nature of trip demand, leading to full or empty stations. Devising effective rebalancing strategies using vehicles to redistribute bikes among stations is therefore of uttermost importance for operators. As a promising alternative to classical mathematical optimization, reinforcement learning is gaining ground to solve sequential decision-making problems. This paper introduces a spatio-temporal reinforcement learning algorithm for the dynamic rebalancing problem with multiple vehicles. We first formulate the problem as a Multi-agent Markov Decision Process in a continuous time framework. This allows for independent and cooperative vehicle rebalancing, eliminating the impractical restriction of time-discretized models where vehicle dep
    
[^145]: 通过双头判别器进行连续领域对抗适应

    Continual Domain Adversarial Adaptation via Double-Head Discriminators

    [https://arxiv.org/abs/2402.03588](https://arxiv.org/abs/2402.03588)

    本文提出了一种通过双头判别器进行连续领域对抗适应的方法，在源学习阶段引入了一个仅在源域训练的源域判别器，减少了对抗损失的经验估计误差，实验结果表明算法实现了超过2%的准确提升。

    

    在连续设置下的领域对抗适应面临着重大挑战，因为存在对之前的源域数据的访问限制。尽管在连续学习中进行了广泛研究，但是仅仅使用少量存储的源域数据（这是记忆重播方法中的标准设置）无法有效完成对抗适应任务。这个限制来自于使用少量源域样本对$\gH$-divergence进行经验估计的错误。为了解决这个问题，我们提出了一个双头判别器算法，通过引入一个仅在源学习阶段训练的源域判别器，从源域一侧减少了$\gH$-divergence相关对抗损失的经验估计误差。进一步在现有的领域适应基准上的实验表明，我们提出的算法实现了超过2%的准确提升。

    Domain adversarial adaptation in a continual setting poses a significant challenge due to the limitations on accessing previous source domain data. Despite extensive research in continual learning, the task of adversarial adaptation cannot be effectively accomplished using only a small number of stored source domain data, which is a standard setting in memory replay approaches. This limitation arises from the erroneous empirical estimation of $\gH$-divergence with few source domain samples. To tackle this problem, we propose a double-head discriminator algorithm, by introducing an addition source-only domain discriminator that are trained solely on source learning phase. We prove that with the introduction of a pre-trained source-only domain discriminator, the empirical estimation error of $\gH$-divergence related adversarial loss is reduced from the source domain side. Further experiments on existing domain adaptation benchmark show that our proposed algorithm achieves more than 2$\%$
    
[^146]: 主动相关聚类的有效获取函数

    Effective Acquisition Functions for Active Correlation Clustering

    [https://arxiv.org/abs/2402.03587](https://arxiv.org/abs/2402.03587)

    本文提出了三种有效的获取函数用于主动相关聚类，分别基于不一致性概念和信息论量。

    

    相关聚类是一种强大的无监督学习范例，支持正和负的相似性。本文假设相似性事先未知，而是采用主动学习以一种成本有效的方式迭代地查询相似性。具体而言，我们开发了三种有效的获取函数用于在此设置下使用。其中一种基于不一致性概念（即当相似性违反传递性时）。其余两个基于信息论量，即熵和信息增益。

    Correlation clustering is a powerful unsupervised learning paradigm that supports positive and negative similarities. In this paper, we assume the similarities are not known in advance. Instead, we employ active learning to iteratively query similarities in a cost-efficient way. In particular, we develop three effective acquisition functions to be used in this setting. One is based on the notion of inconsistency (i.e., when similarities violate the transitive property). The remaining two are based on information-theoretic quantities, i.e., entropy and information gain.
    
[^147]: MQuinE:知识图谱嵌入模型中“Z-悖论”的解决方案

    MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models

    [https://arxiv.org/abs/2402.03583](https://arxiv.org/abs/2402.03583)

    研究者发现知识图谱嵌入模型存在的“Z-悖论”限制了其表达能力，并提出了一种名为MQuinE的新模型，通过理论证明，MQuinE成功解决了Z-悖论，并在链接预测任务中显著优于现有模型。

    

    知识图谱嵌入（KGE）模型在许多知识图谱任务，包括链接预测和信息检索方面取得了最先进的结果。尽管KGE模型在实践中表现出优越性能，但我们发现一些流行的现有KGE模型存在表达不足的问题，称为“Z-悖论”。受到Z-悖论的存在的启发，我们提出了一种新的KGE模型，称为MQuinE，在不受Z-悖论的困扰的同时，保持强大的表达能力来模拟各种关系模式，包括对称/非对称，逆向，1-N/N-1/N-N和组合关系，并提供了理论上的证明。对实际知识库的实验表明，Z-悖论确实降低了现有KGE模型的性能，并且可能导致某些具有挑战性的测试样本的准确性下降超过20％。我们的实验进一步证明了MQuinE可以减轻Z-悖论的负面影响，并在链接预测方面以明显优势超越现有的KGE模型。

    Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction
    
[^148]: 解构神经网络初始化的“金发女孩区”

    Deconstructing the Goldilocks Zone of Neural Network Initialization

    [https://arxiv.org/abs/2402.03579](https://arxiv.org/abs/2402.03579)

    我们对神经网络的初始化进行了全面的分析，发现损失海森矩阵的高正曲率与可训练性强的初始点相关。与先前的观念相反，正曲率并不仅仅与初始化范数相关，而与模型置信度、初始损失较低以及一种以前未知的损失梯度消失相关。

    

    训练损失的二阶性质对深度学习模型的优化动力学有着巨大影响。Fort＆Scherlis（2019）发现，损失海森矩阵的高正曲率和局部凸性与位于被称为“金发女孩区”的高度可训练的初始点相关。只有少数几项后续研究涉及该关系，因此其仍然未被充分解释。在本文中，我们对均质神经网络的“金发女孩区”进行了严格而全面的分析。特别是，我们推导出导致损失海森矩阵非零正曲率的基本条件，并认为它与初始化范数只是偶然相关，与先前的信念相反。此外，我们将高正曲率与模型置信度、初始损失较低以及一种以前未知的消失的交叉熵损失梯度相关联。为了了解正曲率对深度学习模型的可训练性的重要性，我们研究学习的能力和效率，首先是通过泛化误差和高效率的学习算法的标准度量。

    The second-order properties of the training loss have a massive impact on the optimization dynamics of deep learning models. Fort & Scherlis (2019) discovered that a high positive curvature and local convexity of the loss Hessian are associated with highly trainable initial points located in a region coined the "Goldilocks zone". Only a handful of subsequent studies touched upon this relationship, so it remains largely unexplained. In this paper, we present a rigorous and comprehensive analysis of the Goldilocks zone for homogeneous neural networks. In particular, we derive the fundamental condition resulting in non-zero positive curvature of the loss Hessian and argue that it is only incidentally related to the initialization norm, contrary to prior beliefs. Further, we relate high positive curvature to model confidence, low initial loss, and a previously unknown type of vanishing cross-entropy loss gradient. To understand the importance of positive curvature for trainability of deep 
    
[^149]: 从统计学角度重新审视数据集偏差问题

    Revisiting the Dataset Bias Problem from a Statistical Perspective

    [https://arxiv.org/abs/2402.03577](https://arxiv.org/abs/2402.03577)

    本文从统计学的角度重新审视了“数据集偏差”问题，发现其主要原因是输入的类属性和非类属性之间的强相关性。通过在训练过程中考虑这种相关性，我们提出了一种缓解数据集偏差的方法，通过对每个样本的目标加权或以权重比例采样来实现。这种方法在实践中更加稳定和有效，并且与因果推理有一定的关联。

    

    在本文中，我们从统计学的角度研究了“数据集偏差”问题，并确定了该问题的主要原因是输入x中类属性u与非类属性b之间的强相关性，由p(u|b)与p(u)显著不同所表示。由于p(u|b)出现在标准的最大对数似然（MLL）目标的抽样分布中，通过MLL在偏差数据集上训练的模型天然地将这种相关性纳入其参数中，导致对无偏测试数据的泛化能力差。基于这个观察，我们提出通过将每个样本n的目标加权为\frac{1}{p(u_{n}|b_{n})}或者以与\frac{1}{p(u_{n}|b_{n})}成比例的权重对样本进行采样来缓解数据集偏差。虽然这两种方法在统计上是等价的，但前者在实践中证明更稳定和有效。此外，我们建立了我们的去偏方法与因果推理之间的联系，加强了我们方法的思想。

    In this paper, we study the "dataset bias" problem from a statistical standpoint, and identify the main cause of the problem as the strong correlation between a class attribute u and a non-class attribute b in the input x, represented by p(u|b) differing significantly from p(u). Since p(u|b) appears as part of the sampling distributions in the standard maximum log-likelihood (MLL) objective, a model trained on a biased dataset via MLL inherently incorporates such correlation into its parameters, leading to poor generalization to unbiased test data. From this observation, we propose to mitigate dataset bias via either weighting the objective of each sample n by \frac{1}{p(u_{n}|b_{n})} or sampling that sample with a weight proportional to \frac{1}{p(u_{n}|b_{n})}. While both methods are statistically equivalent, the former proves more stable and effective in practice. Additionally, we establish a connection between our debiasing approach and causal reasoning, reinforcing our method's th
    
[^150]: $\ell_0$-有界对抗攻击的对抗训练的泛化性质

    Generalization Properties of Adversarial Training for $\ell_0$-Bounded Adversarial Attacks

    [https://arxiv.org/abs/2402.03576](https://arxiv.org/abs/2402.03576)

    本文主要通过研究$\ell_0$-有界对抗攻击，证明了一个分布无关的对抗训练泛化界限，解决了截断内积的非线性和$\ell_0$空间上最大化问题的挑战。

    

    我们广泛观察到神经网络容易受到小的加性扰动的影响，导致错误分类。本文关注$\ell_0$-有界对抗攻击，并旨在理论上表征截断分类器的对抗训练性能。此类分类器在高斯混合模型和$\ell_0$-对抗设置中经验上和理论上表现出强大的性能。本文的主要贡献是证明了一个分布无关的$\ell_0$-有界对抗扰动的二分类设置的新型泛化界限。在这一设置中推导泛化界限面临两个主要挑战：（i）截断内积高度非线性；（ii）由于对抗训练使得在$\ell_0$空间上的最大化问题是非凸和高度非光滑的。为了解决这些挑战，我们开发了新的编码技术来界定...

    We have widely observed that neural networks are vulnerable to small additive perturbations to the input causing misclassification. In this paper, we focus on the $\ell_0$-bounded adversarial attacks, and aim to theoretically characterize the performance of adversarial training for an important class of truncated classifiers. Such classifiers are shown to have strong performance empirically, as well as theoretically in the Gaussian mixture model, in the $\ell_0$-adversarial setting. The main contribution of this paper is to prove a novel generalization bound for the binary classification setting with $\ell_0$-bounded adversarial perturbation that is distribution-independent. Deriving a generalization bound in this setting has two main challenges: (i) the truncated inner product which is highly non-linear; and (ii) maximization over the $\ell_0$ ball due to adversarial training is non-convex and highly non-smooth. To tackle these challenges, we develop new coding techniques for bounding
    
[^151]: 扩散世界模型

    Diffusion World Model

    [https://arxiv.org/abs/2402.03570](https://arxiv.org/abs/2402.03570)

    扩散世界模型是一个能够预测多步未来状态和奖励的条件性扩散模型，在模型效果和性能方面超过了传统的一步动力学模型。

    

    我们引入了扩散世界模型（DWM），这是一个条件扩散模型，能够同时预测多步的未来状态和奖励。与传统的一步动力学模型相反，DWM通过单个前向传递提供了长时程的预测，消除了递归查询的需要。我们将DWM整合到基于模型的价值估计中，其中短期回报通过从DWM中采样的未来轨迹进行模拟。在离线强化学习的背景下，DWM可以被视为通过生成建模来实现保守的值正则化。另外，它也可以被视为一种数据源，使离线Q学习能够使用合成数据。我们在D4RL数据集上的实验证实了DWM对长时程模拟的鲁棒性。在绝对性能方面，DWM显著超过了一步动力学模型，性能提高了44%，并达到了最先进的水平。

    We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
    
[^152]: SkipPredict：何时投资于作业调度的预测

    SkipPredict: When to Invest in Predictions for Scheduling

    [https://arxiv.org/abs/2402.03564](https://arxiv.org/abs/2402.03564)

    SkipPredict是一种新颖的调度方法，用于解决预测成本的问题。它根据作业的预测要求对作业进行分类，并通过优先处理预测为短作业的作业以及应用详细预测来近似长作业的最短剩余处理时间。这种方法可以在排队系统中有效地应用。

    

    鉴于最近对具有预测作业大小的调度工作的研究，我们考虑了预测在排队系统中的成本影响，消除了先前研究中预测是外部资源和/或免费的假设。具体而言，我们引入了一种新颖的利用预测的方法，SkipPredict，旨在解决其固有的成本问题。我们建议的方法不是均匀地将预测应用于所有作业，而是根据预测要求将作业分为不同类别。为此，我们使用一位“廉价预测”来对作业进行分类，判断其是短作业还是长作业。SkipPredict将优先处理预测为短作业的作业，对于预测为长作业的作业，SkipPredict将应用第二轮更详细的“昂贵预测”来近似这些作业的最短剩余处理时间。我们的分析考虑了预测的成本。我们研究了这种成本对两个不同模型的影响。在外部成本模型中，预测的成本与作业的大小和调度时间有关。

    In light of recent work on scheduling with predicted job sizes, we consider the effect of the cost of predictions in queueing systems, removing the assumption in prior research that predictions are external to the system's resources and/or cost-free. In particular, we introduce a novel approach to utilizing predictions, SkipPredict, designed to address their inherent cost. Rather than uniformly applying predictions to all jobs, we propose a tailored approach that categorizes jobs based on their prediction requirements. To achieve this, we employ one-bit "cheap predictions" to classify jobs as either short or long. SkipPredict prioritizes predicted short jobs over long jobs, and for the latter, SkipPredict applies a second round of more detailed "expensive predictions" to approximate Shortest Remaining Processing Time for these jobs. Our analysis takes into account the cost of prediction. We examine the effect of this cost for two distinct models. In the external cost model, predictions
    
[^153]: 用语言模型区分可知与不可知的能力

    Distinguishing the Knowable from the Unknowable with Language Models

    [https://arxiv.org/abs/2402.03563](https://arxiv.org/abs/2402.03563)

    通过研究大型语言模型，在自由文本中识别作为代理的模型和冻结预训练模型的嵌入的小型线性探测器可以准确预测更大模型令牌级别上的自信度，进一步提出了一种无监督的方法在相同任务上达到了非平凡的准确度，这证明了语言模型中存在不同类型的不确定性表示。

    

    我们研究了在大型语言模型（LLMs）生成的自由文本输出中，是否可以鉴别出认知不确定性（反映缺乏知识的不确定性）和偶然不确定性（反映基础分布中的熵）。在没有真实概率的情况下，我们探索了一个设置，在这个设置中，为了（近似地）分解给定LLM的不确定性，一个明显更大的模型充当地面真相的代理。我们表明，基于冻结预训练模型的嵌入的小型线性探测器能够准确预测在令牌级别上更大模型将更自信的情况，并且在一个文本领域上训练的探测器可以泛化到其他领域。进一步地，我们提出了一种完全无监督的方法，在相同任务上达到了非平凡的准确度。综合考虑这些结果，我们解释这些结果作为LLMs内部自然地包含了不同类型不确定性的表示，这可能有助于制定更有效的方法。

    We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more i
    
[^154]: 用于约束满足的投影式生成扩散模型

    Projected Generative Diffusion Models for Constraint Satisfaction

    [https://arxiv.org/abs/2402.03559](https://arxiv.org/abs/2402.03559)

    本文介绍了一种名为投影式生成扩散模型（PGDM）的方法，它能够通过约束优化问题将生成扩散模型适用于对特定条件有严格要求的场景。该方法通过迭代投影方法确保生成的数据符合指定的约束或物理原理。实验证明PGDM在复杂的约束和常微分方程的情况下也能合成出符合要求的输出。

    

    生成扩散模型通过一个顺序过程，能够从原始噪声中合成出连贯的内容。然而，在需要输出符合特定严格条件的场景中直接应用这些模型面临着严重的挑战。本文旨在克服这些挑战，并介绍了投影式生成扩散模型（PGDM），该方法将传统的扩散模型采样重新构建为一个约束优化问题。这使得可以应用迭代投影方法，以确保生成的数据忠实地遵循指定的约束或物理原理。本文在受限制的约束类别下，对PGDM能够从可行子分布中合成输出的能力提供了理论支持，并在复杂的非凸约束和常微分方程的案例中提供了大量的经验证据。这些能力通过在视频生成中体现了具有物理学信息的动态。

    Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process. However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges. This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem. This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles. This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations. These capabilities are demonstrated by physics-informed motion in video generatio
    
[^155]: 路径签名和图神经网络在慢地震分析中的应用：更好的结合方式？

    Path Signatures and Graph Neural Networks for Slow Earthquake Analysis: Better Together?

    [https://arxiv.org/abs/2402.03558](https://arxiv.org/abs/2402.03558)

    本文介绍了一种新的方法，将路径签名与图神经网络结合起来，用于慢地震分析。通过利用路径签名的特征提取能力和图神经网络处理空间交互的优势，实现了更好的效果。在实际情况下的应用中，我们利用GPS传感器网络对慢滑事件进行分析，并在模拟的随机微分方程中建立了基准模型。

    

    路径签名作为一种在机器学习领域取得成功的理论驱动方法，用于从不规则路径中提取特征。另一方面，图神经网络（GNN）是一种用于处理图数据的神经网络架构，在传感器网络等不规则领域的任务中表现出色。本文介绍了一种新颖的方法，路径签名图卷积神经网络（PS-GCNN），将路径签名与图卷积神经网络（GCNN）结合起来，发挥了路径签名的特征提取能力和GCNN处理空间互动的优势。我们将该方法应用于慢地震序列（也称为慢滑事件）的分析，利用来自GPS时间序列的数据，在新西兰北岛东海岸的GPS传感器网络上进行了案例研究。我们还在模拟随机微分方程中建立了我们方法的基准，这种模型模拟了类似的反应扩散现象。

    The path signature, having enjoyed recent success in the machine learning community, is a theoretically-driven method for engineering features from irregular paths. On the other hand, graph neural networks (GNN), neural architectures for processing data on graphs, excel on tasks with irregular domains, such as sensor networks. In this paper, we introduce a novel approach, Path Signature Graph Convolutional Neural Networks (PS-GCNN), integrating path signatures into graph convolutional neural networks (GCNN), and leveraging the strengths of both path signatures, for feature extraction, and GCNNs, for handling spatial interactions. We apply our method to analyze slow earthquake sequences, also called slow slip events (SSE), utilizing data from GPS timeseries, with a case study on a GPS sensor network on the east coast of New Zealand's north island. We also establish benchmarks for our method on simulated stochastic differential equations, which model similar reaction-diffusion phenomenon
    
[^156]: 单GPU GNN系统：陷阱与注意事项

    Single-GPU GNN Systems: Traps and Pitfalls

    [https://arxiv.org/abs/2402.03548](https://arxiv.org/abs/2402.03548)

    本论文分析了当前单GPU GNN系统的陷阱与注意事项，并提出了一系列的解决方案。通过开发新的参考系统，实现高效实用地解决系统设计陷阱的一系列优化，并推动技术的发展。

    

    当前的图神经网络（GNN）系统普遍不展示训练准确性结果，并直接或间接依赖于较小的数据集进行评估。我们的深入分析表明，这给系统设计和评估过程带来了一系列的陷阱，质疑了许多提议的系统优化的实用性，并影响了结论和经验教训。我们分析了许多单GPU系统，展示了这些陷阱的根本影响。我们进一步提出了假设、建议和评估方法，并提供未来的方向。最后，我们开发了一个新的参考系统，以实现高效实用地解决系统设计陷阱的一系列优化。该设计可以有效地融入之前的工作，真正推动技术的发展。

    The current graph neural network (GNN) systems have established a clear trend of not showing training accuracy results, and directly or indirectly relying on smaller datasets for evaluations majorly. Our in-depth analysis shows that it leads to a chain of pitfalls in the system design and evaluation process, questioning the practicality of many of the proposed system optimizations, and affecting conclusions and lessons learned. We analyze many single-GPU systems and show the fundamental impact of these pitfalls. We further develop hypotheses, recommendations, and evaluation methodologies, and provide future directions. Finally, a new reference system is developed to establish a new line of optimizations rooted in solving the system-design pitfalls efficiently and practically. The proposed design can productively be integrated into prior works, thereby truly advancing the state-of-the-art.
    
[^157]: 在线特征更新改善在线（广义）标签转移适应问题

    Online Feature Updates Improve Online (Generalized) Label Shift Adaptation

    [https://arxiv.org/abs/2402.03545](https://arxiv.org/abs/2402.03545)

    本文提出了一种名为OLS-OFU的新方法，通过在测试时利用无标签数据进行自监督学习和特征优化，以解决在线设置中的标签转移问题。研究结果表明，OLS-OFU在各种数据集和领域转移条件下都表现出了有效性和鲁棒性。

    

    本文解决了在线设置中标签转移的普遍问题，其中存在缺失的标签，数据分布随时间变化，及时获得标签是一项具有挑战性的任务。虽然现有的方法主要集中在调整或更新预训练分类器的最后一层，我们探索了在测试时使用无标签数据来改进特征表示的未被发掘的潜力。我们的新方法，在线标签转移自适应与在线特征更新（OLS-OFU），利用自监督学习来优化特征提取过程，从而改进预测模型。理论分析证实，OLS-OFU通过利用自监督学习进行特征优化，减少了算法遗憾。在各种数据集上的实证研究，在在线标签转移和广义标签转移条件下，强调了OLS-OFU的有效性和鲁棒性，特别是在领域转移的情况下。

    This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts.
    
[^158]: HAMLET：用于偏微分方程的图变换神经算子

    HAMLET: Graph Transformer Neural Operator for Partial Differential Equations

    [https://arxiv.org/abs/2402.03541](https://arxiv.org/abs/2402.03541)

    HAMLET是一个图变换神经算子框架，通过使用模块化输入编码器将微分方程信息直接融入解决过程中，并展示出在处理复杂数据和噪声方面的鲁棒性，适用于任意几何形状和各种输入格式的PDE问题。通过大量实验，我们证明了HAMLET能够超越当前的PDE技术。

    

    我们提出了一种新颖的图变换框架HAMLET，旨在解决使用神经网络求解偏微分方程（PDE）时的挑战。该框架使用具有模块化输入编码器的图变换器，将微分方程信息直接融入解决过程中。这种模块化增强了参数对应控制，使得HAMLET能够适应任意几何形状和各种输入格式的PDE。值得注意的是，HAMLET能够有效扩展到处理复杂数据和噪声，展示出其鲁棒性。HAMLET不仅适用于单一类型的物理模拟，还可以应用于各个领域。此外，它提升了模型的弹性和性能，特别是在数据有限的场景下。通过大量实验，我们证明了我们的框架能够超越当前的PDE技术。

    We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks. The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process. This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats. Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness. HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains. Moreover, it boosts model resilience and performance, especially in scenarios with limited data. We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs.
    
[^159]: 《可信机器学习的调节游戏》

    Regulation Games for Trustworthy Machine Learning

    [https://arxiv.org/abs/2402.03540](https://arxiv.org/abs/2402.03540)

    本论文提出了一个以调节游戏为基础的框架，将可信机器学习视为多目标多主体优化问题。通过引入ParetoPlay算法，寻找社会最优的游戏解决方案，该算法能够确保代理方始终保持在Pareto前沿上。

    

    现有的关于可信机器学习的研究往往集中在信任的个别方面，如公平性或隐私。此外，许多技术忽视了训练机器学习模型的人和负责评估其可信度的人之间的区别。为了解决这些问题，我们提出了一个框架，将可信机器学习视为多目标多主体优化问题。这自然地导致了一个称为调节游戏的博弈论形式。我们介绍了一个特定的游戏实例——SpecGame，其中我们建模了机器学习模型构建者与公平性和隐私监管者之间的关系。监管者希望设计处罚措施来强制执行他们的规范，但不希望阻止构建者的参与。为了寻找这种社会最优（即对所有代理方都有效）的游戏解决方案，我们引入了ParetoPlay。这种新型均衡搜索算法确保代理方始终保持在Pareto前沿上。

    Existing work on trustworthy machine learning (ML) often concentrates on individual aspects of trust, such as fairness or privacy. Additionally, many techniques overlook the distinction between those who train ML models and those responsible for assessing their trustworthiness. To address these issues, we propose a framework that views trustworthy ML as a multi-objective multi-agent optimization problem. This naturally lends itself to a game-theoretic formulation we call regulation games. We illustrate a particular game instance, the SpecGame in which we model the relationship between an ML model builder and fairness and privacy regulators. Regulators wish to design penalties that enforce compliance with their specification, but do not want to discourage builders from participation. Seeking such socially optimal (i.e., efficient for all agents) solutions to the game, we introduce ParetoPlay. This novel equilibrium search algorithm ensures that agents remain on the Pareto frontier of th
    
[^160]: 基于ANN的无传感器位置和速度估计方法应用于BLDC电机

    ANN-based position and speed sensorless estimation for BLDC motors

    [https://arxiv.org/abs/2402.03534](https://arxiv.org/abs/2402.03534)

    无传感器位置和速度估计方法在BLDC电机应用中取得了显著的改进效果。

    

    BLDC电机应用需要精确的位置和速度测量，传统上需要传感器来获取这些测量结果。本文提出了一种使用带有衰减杂散的终端相电压来估计这些测量结果的方法，该电压是通过FPGA获取的，该FPGA还操作一个PWM控制的逆变器。通过使用一个编码器，将电压标记为电气和虚拟转子状态，为两个具有感知器级联拓扑结构的三层ANN提供训练和测试数据。第一个ANN根据带有增量时间戳的电压特征来估计位置，第二个ANN根据考虑采集窗口中的时间戳的位置差异特征来估计速度。在125到1,500转/分钟的负载8极对电机上，基于传感器的训练和无传感器的测试得到的绝对误差分别为0.8电气度和22转/分钟。结果表明，整体位置估计明显改善了传统和先进的方法，

    BLDC motor applications require precise position and speed measurements, traditionally obtained with sensors. This article presents a method for estimating those measurements without position sensors using terminal phase voltages with attenuated spurious, acquired with a FPGA that also operates a PWM-controlled inverter. Voltages are labelled with electrical and virtual rotor states using an encoder that provides training and testing data for two three-layer ANNs with perceptron-based cascade topology. The first ANN estimates the position from features of voltages with incremental timestamps, and the second ANN estimates the speed from features of position differentials considering timestamps in an acquisition window. Sensor-based training and sensorless testing at 125 to 1,500 rpm with a loaded 8-pole-pair motor obtained absolute errors of 0.8 electrical degrees and 22 rpm. Results conclude that the overall position estimation significantly improved conventional and advanced methods, 
    
[^161]: 在联邦环境中的上下文多臂老虎机问题中的公平性和隐私保证

    Fairness and Privacy Guarantees in Federated Contextual Bandits

    [https://arxiv.org/abs/2402.03531](https://arxiv.org/abs/2402.03531)

    本文研究了在联邦环境中的上下文多臂老虎机问题，提出了一个合作式联邦学习算法Fed-FairX-LinUCB。该算法通过设计新颖的通信协议，保证了公平性和隐私性，解决了传统算法在代理数量上产生的公平性损失问题。

    

    本文考虑了在联邦环境中具有公平性和隐私保证的上下文多臂老虎机（CMAB）问题。我们将基于优势的曝光视为期望的公平结果，该结果根据相关奖励的比例给予每个动作曝光。我们使用公平损失来模拟算法的效果，公平损失捕捉了公平最优策略和算法输出策略之间的差异。将公平CMAB算法应用于每个独立的代理会导致与代理数量成线性关系的公平损失。我们提出合作式--联邦学习可更加有效，并提供确保差分隐私的算法Fed-FairX-LinUCB。扩展现有隐私框架的主要挑战在于设计跨代理通信所需信息的通信协议。一个简单的协议可能会导致较弱的隐私保证或更高的损失。我们设计了一种新颖的通信协议，允许f

    This paper considers the contextual multi-armed bandit (CMAB) problem with fairness and privacy guarantees in a federated environment. We consider merit-based exposure as the desired fair outcome, which provides exposure to each action in proportion to the reward associated. We model the algorithm's effectiveness using fairness regret, which captures the difference between fair optimal policy and the policy output by the algorithm. Applying fair CMAB algorithm to each agent individually leads to fairness regret linear in the number of agents. We propose that collaborative -- federated learning can be more effective and provide the algorithm Fed-FairX-LinUCB that also ensures differential privacy. The primary challenge in extending the existing privacy framework is designing the communication protocol for communicating required information across agents. A naive protocol can either lead to weaker privacy guarantees or higher regret. We design a novel communication protocol that allows f
    
[^162]: 在空间环境中一致验证预测方法

    Consistent Validation for Predictive Methods in Spatial Settings

    [https://arxiv.org/abs/2402.03527](https://arxiv.org/abs/2402.03527)

    本论文研究了在空间环境中验证预测方法的一致性问题，提出了一种能够处理不匹配情况的方法。

    

    空间预测任务对于天气预报、空气污染研究和其他科学工作至关重要。确定我们对统计或物理方法所作预测的可信度是科学结论的重要问题。不幸的是，传统的验证方法无法处理验证位置和我们希望进行预测的（测试）位置之间的不匹配。这种不匹配通常不是协变量偏移的一个实例（常常被形式化），因为验证和测试位置是固定的（例如，在网格上或选定的点上），而不是从两个分布中独立同分布地采样。在本文中，我们形式化了对验证方法的检查：随着验证数据的密度越来越大，它们能够变得任意精确。我们证明了传统方法和协变量偏移方法可能不满足这个检查。相反，我们提出了一种方法，它借鉴了协变量偏移文献中的现有思想，但对验证数据进行了调整。

    Spatial prediction tasks are key to weather forecasting, studying air pollution, and other scientific endeavors. Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d. from two distributions. In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. We show that classical and covariate-shift methods can fail this check. We instead propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data 
    
[^163]: 深度强化学习在仓储中的拣选车辆路径问题中的应用

    Deep Reinforcement Learning for Picker Routing Problem in Warehousing

    [https://arxiv.org/abs/2402.03525](https://arxiv.org/abs/2402.03525)

    本研究提出了一种基于注意力机制的神经网络模型，通过深度强化学习解决仓储中的拣选车辆路径问题。与传统启发式方法相比，该方法具有更快的速度和准确度，并能降低路径的感知复杂度。

    

    订单拣选车辆路径是仓库运营管理中的关键问题。由于问题的复杂性和需要快速解决方案的需求，实践中常常使用次优算法。然而，强化学习提供了一种吸引人的替代传统启发式方法的选择，可能在速度和准确度方面优于现有方法。我们引入了一种基于注意力机制的神经网络来建模拣选车辆路径，该网络通过强化学习进行训练。我们的方法在一系列问题参数上与现有启发式算法进行了评估，展示了其效果。我们提出方法的一个重要优点是可以降低路径的感知复杂度。

    Order Picker Routing is a critical issue in Warehouse Operations Management. Due to the complexity of the problem and the need for quick solutions, suboptimal algorithms are frequently employed in practice. However, Reinforcement Learning offers an appealing alternative to traditional heuristics, potentially outperforming existing methods in terms of speed and accuracy. We introduce an attention based neural network for modeling picker tours, which is trained using Reinforcement Learning. Our method is evaluated against existing heuristics across a range of problem parameters to demonstrate its efficacy. A key advantage of our proposed method is its ability to offer an option to reduce the perceived complexity of routes.
    
[^164]: 跨领域评估零样本摘要生成器的真实性

    Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains

    [https://arxiv.org/abs/2402.03509](https://arxiv.org/abs/2402.03509)

    本研究跨领域评估了零样本摘要生成器的真实性。通过对生物医学文章和法律法案等专业领域进行评估，我们特别关注生成摘要的真实性，并分析了预训练语料库中给定领域的普遍性对摘要质量的影响。

    

    最近的研究表明，大型语言模型(LLMs)能够零样本（即在没有明确监督的情况下）生成摘要，经过人工评估，这些摘要往往与手工编写的参考摘要相比，甚至更受欢迎。然而，这些早期的研究几乎专注于评估新闻文章的摘要。零样本摘要生成器在其他（可能更专业）领域中的表现如何？在这项工作中，我们评估了跨专业领域中零样本生成的摘要，包括生物医学文章和法律法案（除了标准新闻摘要的参考）。我们特别关注输出的真实性。我们从领域专家处获取注释，以识别摘要中的不一致之处，并对这些错误进行系统分类。我们分析了预训练语料库中给定领域的普遍性是否会影响在该领域的文章的摘要的提取和忠实度。我们发布了所有收集到的注释。

    Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains? In this work we evaluate zero-shot generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotat
    
[^165]: 神经网络抽象和推理：迈向机器的广泛泛化

    Neural networks for abstraction and reasoning: Towards broad generalization in machines

    [https://arxiv.org/abs/2402.03507](https://arxiv.org/abs/2402.03507)

    这项工作探索了神经网络在广泛泛化方面的应用，通过研究解决抽象和推理任务的新方法，试图提高计算机系统从少量示例中学习新概念的能力。

    

    半个世纪以来，人工智能研究一直试图复制人类的抽象和推理能力-创造出能够从一小组示例中学习新概念的计算机系统，在人们发现这很容易的情况下。虽然特定的神经网络能够解决各种各样的问题，但对于超越训练数据范围的广泛泛化仍然是一个难题。在这项工作中，我们研究了几种解决抽象与推理语料库（ARC）的新方法，ARC是一个用于测试算法在广泛泛化方面的抽象视觉推理任务数据集。尽管有三个国际竞赛提供了10万美元的奖金，最好的算法仍然无法解决大多数ARC任务，并且依赖于复杂的手工规则，没有使用机器学习。我们重新思考了最近神经网络的进展是否能在这个任务上取得进展。首先，我们将DreamCoder神经符号推理求解器应用到ARC上。

    For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task.   First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder
    
[^166]: 未标记数据如何在超出分布检测中发挥可证明的作用？

    How Does Unlabeled Data Provably Help Out-of-Distribution Detection?

    [https://arxiv.org/abs/2402.03502](https://arxiv.org/abs/2402.03502)

    本文介绍了一种新的学习框架SAL（Separate And Learn），通过利用未标记数据和标记数据，分离并训练异常点和OOD分类器，理论上提供了强大的保证和严格的错误界限。

    

    使用未标记数据对机器学习模型进行正则化已经显示出在检测超出分布（OOD）数据方面改进安全性和可靠性的潜力。利用野外未标记数据的能力是非常困难的，因为内分布（ID）和OOD数据的异质性。缺乏清洁的OOD样本集合在学习最优OOD分类器方面存在重大挑战。目前，缺乏对未标记数据如何帮助OOD检测的研究。本文通过引入新的学习框架SAL（Separate And Learn），填补了这一空白，该框架在理论上具有强大的保证和实证的有效性。该框架将候选异常点从未标记数据中分离出来，并使用候选异常点和标记的ID数据训练OOD分类器。从可分离性和可学习性的角度，我们在理论上提供了严格的错误界限，正式证明了我们算法中的两个组成部分。

    Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data. This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier. Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection. This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness. The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data. Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm. Our the
    
[^167]: 量子硬件错误下的课程强化学习量子架构搜索

    Curriculum reinforcement learning for quantum architecture search under hardware errors

    [https://arxiv.org/abs/2402.03500](https://arxiv.org/abs/2402.03500)

    本研究提出了一种基于课程的强化学习量子架构搜索算法（CRLQAS），解决了在噪声中间规模量子时代中，对于架构搜索的噪声效应的不理解的问题。

    

    在噪声中间规模量子时代，找到与当前设备限制兼容的有用电路是关键挑战。变分量子算法（VQAs）通过固定电路架构和优化外部循环中的个别门参数来提供潜在解决方案。然而，参数优化可能变得难以处理，并且算法的整体性能严重依赖初始选择的电路架构。已经开发了几种量子架构搜索（QAS）算法来自动设计有用的电路架构。在仅限参数优化的情况下，观察到噪声效应严重影响优化器和最终结果的性能，这是一条重要的研究线。然而，对于架构搜索的噪声效应，可能同样重要的问题目前还不太理解。本文通过引入基于课程的强化学习QAS（CRLQAS）算法来解决这个问题。

    The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm desi
    
[^168]: 我们能去掉自适应梯度方法中的平方根吗？一个二阶角度的研究

    Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective

    [https://arxiv.org/abs/2402.03496](https://arxiv.org/abs/2402.03496)

    移除自适应方法中的平方根可以在卷积结构上减小与SGD的泛化差距，同时保持在transformers上的性能。

    

    自适应梯度优化器如Adam(W)是许多深度学习结构（如transformers）的默认训练算法。它们的对角先验基于梯度外积，通过平方根加入到参数更新中。虽然这些方法通常被称为近似的二阶方法，但平方根表示了一个根本性的区别。在这项研究中，我们研究了在去掉平方根后自适应方法的行为如何变化，即加强它们的二阶动机。令人惊讶的是，我们发现这种去掉平方根的自适应方法能够在卷积结构上缩小与SGD的泛化差距，同时保持了在transformers上基于平方根的方法的性能。二阶角度对于开发具有非对角先验的自适应方法也具有实际好处。与像Shampoo这样基于平方根的对应方法不同，它们不需要数值不稳定的矩阵平方。

    Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square
    
[^169]: 部分随机的无限深度贝叶斯神经网络

    Partially Stochastic Infinitely Deep Bayesian Neural Networks

    [https://arxiv.org/abs/2402.03495](https://arxiv.org/abs/2402.03495)

    本文提出了一种部分随机性的无限深度贝叶斯神经网络，通过在网络框架中整合部分随机性，改善现有架构在训练和推理时间上的计算效率限制，并提供了多种灵活的网络设计配置，同时通过数学证明确保了模型的表达能力。

    

    在本文中，我们提出了一种部分随机的无限深度贝叶斯神经网络，这是一种将部分随机性整合到无限深度神经网络框架中的新型架构。我们的新型架构旨在改善现有架构在训练和推理时间上的计算效率限制。为实现这一目标，我们利用了部分随机性在无限深度极限下的优势，包括全随机性的好处，如鲁棒性、不确定性量化和内存效率，同时改善了它们在训练和推理时间上的计算效率限制。我们提出了多种架构配置，提供了网络设计的灵活性，包括不同的权重划分方法。我们还通过确立我们的网络家族符合通用条件分布近似器的数学保证，对我们的模型的表达能力进行了证明。

    In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators
    
[^170]: 临床环境中早期预测脓毒症的研究

    Early prediction of onset of sepsis in Clinical Setting

    [https://arxiv.org/abs/2402.03486](https://arxiv.org/abs/2402.03486)

    本研究提出了使用机器学习模型预测临床数据中脓毒症的早期发作，通过使用有监督学习方法训练XGBoost模型，并利用规范化效用分数评估模型的性能。

    

    本研究提出了使用机器学习模型，利用来自纽约布朗克斯Montefiore医疗中心的去标识化临床数据，预测脓毒症的早期发作。采用了有监督学习的方法，训练了一个XGBoost模型，使用了80%的训练数据集，包括107个特征（包括原始和衍生特征）。随后，该模型在剩余的20%的测试数据上进行了评估。模型在训练阶段完全未知的前瞻数据上进行了验证。为了评估模型在个体患者水平上的性能和预测的及时性，使用了规范化效用分数，这是脓毒症检测中广泛认可的评分方法，如PhysioNet Sepsis Challenge论文中所述。还设计了F1值、敏感性、特异性和标志率等指标。该模型在测试数据上的规范化效用分数为0.494，在前瞻数据上的规范化效用分数为0.378（阈值为0.3）。

    This study proposes the use of Machine Learning models to predict the early onset of sepsis using deidentified clinical data from Montefiore Medical Center in Bronx, NY, USA. A supervised learning approach was adopted, wherein an XGBoost model was trained utilizing 80\% of the train dataset, encompassing 107 features (including the original and derived features). Subsequently, the model was evaluated on the remaining 20\% of the test data. The model was validated on prospective data that was entirely unseen during the training phase. To assess the model's performance at the individual patient level and timeliness of the prediction, a normalized utility score was employed, a widely recognized scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis Challenge paper. Metrics such as F1 Score, Sensitivity, Specificity, and Flag Rate were also devised. The model achieved a normalized utility score of 0.494 on test data and 0.378 on prospective data at threshold 0.3. The
    
[^171]: 注意力与事后可解释性相遇：数学视角

    Attention Meets Post-hoc Interpretability: A Mathematical Perspective

    [https://arxiv.org/abs/2402.03485](https://arxiv.org/abs/2402.03485)

    本文通过数学方式研究了基于注意力机制的架构，比较了事后解释和基于注意力机制的解释的差异，发现尽管有局限性，事后解释方法能够捕获比仅仅检查注意力权重更有用的洞察。

    

    注意力机制基于transformer等架构，成为了技术革命的核心。有趣的是，除了帮助在各种应用中获得最先进的结果之外，注意力机制本身还提供了关于模型内部行为的有意义洞察。这些洞察是否可以用作解释？关于此争论不断。本文通过数学方式研究了一个简单的基于注意力机制的架构，并准确定位了事后解释和基于注意力机制的解释之间的区别。我们表明它们提供了相当不同的结果，并且尽管有其局限性，事后解释方法能够捕获比仅仅检查注意力权重更有用的洞察。

    Attention-based architectures, in particular transformers, are at the heart of a technological revolution. Interestingly, in addition to helping obtain state-of-the-art results on a wide range of applications, the attention mechanism intrinsically provides meaningful insights on the internal behavior of the model. Can these insights be used as explanations? Debate rages on. In this paper, we mathematically study a simple attention-based architecture and pinpoint the differences between post-hoc and attention-based explanations. We show that they provide quite different results, and that, despite their limitations, post-hoc methods are capable of capturing more useful insights than merely examining the attention weights.
    
[^172]: FINEST: 通过保持排序进行微调以稳定推荐

    FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning

    [https://arxiv.org/abs/2402.03481](https://arxiv.org/abs/2402.03481)

    FINEST方法通过从给定的推荐模型获得参考排序列表，并在模拟的扰动场景下进行模型微调，保持排序，以稳定和改善推荐系统的性能。

    

    现代推荐系统可能会因为训练数据的微小扰动而输出大不相同的推荐结果。单个用户数据的改变会改变其他用户的推荐结果。在医疗、住房和金融等应用中，这种敏感性可能对用户体验产生不利影响。我们提出了一种方法来稳定给定的推荐系统，抵抗这种扰动。这是一个具有挑战性的任务，因为(1)缺乏可以用来锚定输出的“参考”排序列表；(2)在保证模型与训练数据的所有可能扰动的排序列表稳定性方面存在计算挑战。我们的方法FINEST通过从给定的推荐模型获得参考排序列表，然后在模拟的扰动场景下进行模型微调，并保持排序的规则化来克服这些挑战。我们在真实世界的数据集上的实验证明FINEST可以稳定推荐系统并保持性能。

    Modern recommender systems may output considerably different recommendations due to small perturbations in the training data. Changes in the data from a single user will alter the recommendations as well as the recommendations of other users. In applications like healthcare, housing, and finance, this sensitivity can have adverse effects on user experience. We propose a method to stabilize a given recommender system against such perturbations. This is a challenging task due to (1) the lack of a ``reference'' rank list that can be used to anchor the outputs; and (2) the computational challenges in ensuring the stability of rank lists with respect to all possible perturbations of training data. Our method, FINEST, overcomes these challenges by obtaining reference rank lists from a given recommendation model and then fine-tuning the model under simulated perturbation scenarios with rank-preserving regularization on sampled items. Our experiments on real-world datasets demonstrate that FIN
    
[^173]: 为科学发现服务的兆级参数人工智能基础设施：一项调研和展望

    Trillion Parameter AI Serving Infrastructure for Scientific Discovery: A Survey and Vision

    [https://arxiv.org/abs/2402.03480](https://arxiv.org/abs/2402.03480)

    这篇论文调研了为科学研究提供支持的兆级参数人工智能基础设施，并描述了在系统设计中所面临的重要技术挑战和开放问题。

    

    深度学习方法正在改变研究工作，实现新技术，并最终带来新的发现。随着对更强大的人工智能模型的需求不断增长，我们正进入一个兆级参数模型（TPM）的时代，即具有超过一万亿个参数的模型，如华为的PanGu-$ \ Sigma $。我们描述了一个针对科学界特定需求的TPM用户和提供者生态系统的愿景。然后，我们概述了为提供TPM服务的系统设计中的重大技术挑战和开放问题。具体来说，我们描述了一个全面的软件堆栈和接口要求，以支持研究者多样化和灵活的需求。

    Deep learning methods are transforming research, enabling new techniques, and ultimately leading to new discoveries. As the demand for more capable AI models continues to grow, we are now entering an era of Trillion Parameter Models (TPM), or models with more than a trillion parameters -- such as Huawei's PanGu-$\Sigma$. We describe a vision for the ecosystem of TPM users and providers that caters to the specific needs of the scientific community. We then outline the significant technical challenges and open problems in system design for serving TPMs to enable scientific research and discovery. Specifically, we describe the requirements of a comprehensive software stack and interfaces to support the diverse and flexible requirements of researchers.
    
[^174]: ICED: 通过上下文环境设计实现强化学习的零样本迁移

    ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design

    [https://arxiv.org/abs/2402.03479](https://arxiv.org/abs/2402.03479)

    本研究探索了深度强化学习代理的零样本迁移能力，并发现通过根据值损失优先选择级别，可以改善代理的推广能力。此外，无监督环境设计方法对改善代理表现也具有重要作用。

    

    使用深度强化学习训练的自主代理通常缺乏成功地推广到新环境的能力，即使这些环境与它们在训练过程中遇到的环境具有相似的特征。本研究探讨了个体环境实例（或级别）的采样对强化学习代理的零样本推广能力的影响。我们发现，对于共享基本层的深度演员-评论家架构，根据其值损失优先选择级别，可以最小化代理的内部表示与生成的训练数据中的训练级别之间的互信息。这为某些自适应采样策略实现的隐式正则化提供了新颖的理论解释。然后，我们将注意力转向无监督环境设计（UED）方法，这些方法对数据生成机制具有更多控制。我们发现现有的UED方法可以显著改变训练数据中的环境实例，从而影响代理的表现能力。

    Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the trainin
    
[^175]: 超扩散：使用单一模型估计认识和偶然不确定性

    Hyper-Diffusion: Estimating Epistemic and Aleatoric Uncertainty with a Single Model

    [https://arxiv.org/abs/2402.03478](https://arxiv.org/abs/2402.03478)

    本研究引入了一种新的集合方法，超扩散，可以使用单一模型准确估计认识和偶然不确定性。

    

    在将机器学习应用于高风险应用领域（如医学影像和天气预报）时，准确估计和区分认识不确定性（可以通过更多的训练数据降低的不确定性）和偶然不确定性（与当前任务固有的不确定性）至关重要。条件扩散模型具有准确有效地从数据集的后验分布中采样的突破性能力，现在使得不确定性估计从概念上变得简单明了：只需要训练和从一个大型扩散模型集合中采样即可。然而，随着模型架构的复杂性增加，训练这样一个集合变得难以计算。在本文中，我们介绍了一种新的集合方法，超扩散，它可以使用单一模型准确估计认识和偶然不确定性。

    Estimating and disentangling epistemic uncertainty (uncertainty that can be reduced with more training data) and aleatoric uncertainty (uncertainty that is inherent to the task at hand) is critically important when applying machine learning (ML) to high-stakes applications such as medical imaging and weather forecasting. Conditional diffusion models' breakthrough ability to accurately and efficiently sample from the posterior distribution of a dataset now makes uncertainty estimation conceptually straightforward: One need only train and sample from a large ensemble of diffusion models. Unfortunately, training such an ensemble becomes computationally intractable as the complexity of the model architecture grows.   In this work we introduce a new approach to ensembling, hyper-diffusion, which allows one to accurately estimate epistemic and aleatoric uncertainty with a single model. Unlike existing Monte Carlo dropout based single-model ensembling methods, hyper-diffusion offers the same 
    
[^176]: CT材料分解技术的光谱扩散后验抽样翻译

    CT Material Decomposition using Spectral Diffusion Posterior Sampling

    [https://arxiv.org/abs/2402.03476](https://arxiv.org/abs/2402.03476)

    本研究提出了一种利用扩散后验抽样（DPS）进行光谱CT材料分解的深度学习方法。该方法结合了无监督训练的先验知识和物理模型，能够在更短的时间内实现高准确性、低不确定性和低计算成本。

    

    在本研究中，我们介绍了一种基于扩散后验抽样（DPS）的新型深度学习方法，用于从光谱CT测量中进行材料分解。该方法结合了来自无监督训练的复杂先验知识和严格的测量物理模型。提出了一种更快且更稳定的变体，使用启动过程减少了反向过程所需的时间步数，并使用梯度近似减少了计算成本。针对两种光谱CT系统（双能量CT和双层探测器CT），对性能进行了研究。在这两个系统上，DPS仅使用10%的迭代次数，就可以获得与基于模型的材料分解（MBMD）中使用的相同结构相似性指数度量（SSIM）相当的结果。启动过程的DPS（JSDPS）进一步减少了计算时间超过85%，并且与经典DPS和MBMD相比，具有最高的准确性、最低的不确定性和最低的计算成本。实验结果表明了该技术的潜力。

    In this work, we introduce a new deep learning approach based on diffusion posterior sampling (DPS) to perform material decomposition from spectral CT measurements. This approach combines sophisticated prior knowledge from unsupervised training with a rigorous physical model of the measurements. A faster and more stable variant is proposed that uses a jumpstarted process to reduce the number of time steps required in the reverse process and a gradient approximation to reduce the computational cost. Performance is investigated for two spectral CT systems: dual-kVp and dual-layer detector CT. On both systems, DPS achieves high Structure Similarity Index Metric Measure(SSIM) with only 10% of iterations as used in the model-based material decomposition(MBMD). Jumpstarted DPS (JSDPS) further reduces computational time by over 85% and achieves the highest accuracy, the lowest uncertainty, and the lowest computational costs compared to classic DPS and MBMD. The results demonstrate the potenti
    
[^177]: 基于滑动窗口多变量时间序列森林分类器的活动区域耀斑预测

    Active Region-based Flare Forecasting with Sliding Window Multivariate Time Series Forest Classifiers

    [https://arxiv.org/abs/2402.03474](https://arxiv.org/abs/2402.03474)

    本研究提出了一种基于滑动窗口多变量时间序列森林分类器的活动区域耀斑预测方法，并通过一种新颖的特征排序方法，解决了当前数据驱动方法在忽略时间演化特征时的局限性。

    

    在过去几十年中，基于物理模拟和数据驱动技术（包括机器学习和深度学习）的许多应用已经出现，用于分析和预测太阳耀斑。这些方法在理解太阳耀斑的动力学过程中起着关键作用，主要目的是预测这些事件并最大限度地减小它们可能对地球造成的风险。尽管当前的方法取得了重要进展，但数据驱动方法仍存在局限性。一个显著的缺点是缺乏考虑耀斑起源活动区域的时间演化特征。这个疏忽妨碍了这些方法把高维活动区域特征之间的关系全部抓住，从而限制了它们在操作中的可用性。本研究集中于开发解释性分类器用于多变量时间序列，并展示了一种新颖的基于滑动窗口的子间特征排序方法。

    Over the past few decades, many applications of physics-based simulations and data-driven techniques (including machine learning and deep learning) have emerged to analyze and predict solar flares. These approaches are pivotal in understanding the dynamics of solar flares, primarily aiming to forecast these events and minimize potential risks they may pose to Earth. Although current methods have made significant progress, there are still limitations to these data-driven approaches. One prominent drawback is the lack of consideration for the temporal evolution characteristics in the active regions from which these flares originate. This oversight hinders the ability of these methods to grasp the relationships between high-dimensional active region features, thereby limiting their usability in operations. This study centers on the development of interpretable classifiers for multivariate time series and the demonstration of a novel feature ranking method with sliding window-based sub-int
    
[^178]: 大型语言模型几何信息的研究

    The Information of Large Language Model Geometry

    [https://arxiv.org/abs/2402.03471](https://arxiv.org/abs/2402.03471)

    论文研究了大型语言模型中嵌入的信息编码，并发现表示熵与模型大小呈幂律关系。通过信息理论和回归技术，建立了新标记的信息增益与岭回归之间的理论联系，并探索了Lasso回归在选择有意义的标记方面的有效性。实验结果表明，信息在标记之间分布，而不仅仅集中在特定的“有意义”的标记上。

    

    本文研究了大型语言模型中嵌入的信息编码。我们进行了模拟实验，分析了表示熵，并发现了与模型大小呈幂律关系的现象。基于这一观察，我们提出了一个基于（条件）熵的理论来解释这种规模定律现象。此外，我们深入探讨了LLMs的自回归结构，并使用信息理论和回归技术来分析最后一个标记与之前上下文标记之间的关系。具体而言，我们建立了新标记的信息增益与岭回归之间的理论联系。此外，我们还探索了Lasso回归在选择有意义的标记方面的有效性，有时表现优于紧密相关的注意力权重。最后，我们进行了对比实验，并发现信息分布在标记之间，而不仅仅集中在特定的“有意义”的标记上。

    This paper investigates the information encoded in the embeddings of large language models (LLMs). We conduct simulations to analyze the representation entropy and discover a power law relationship with model sizes. Building upon this observation, we propose a theory based on (conditional) entropy to elucidate the scaling law phenomenon. Furthermore, we delve into the auto-regressive structure of LLMs and examine the relationship between the last token and previous context tokens using information theory and regression techniques. Specifically, we establish a theoretical connection between the information gain of new tokens and ridge regression. Additionally, we explore the effectiveness of Lasso regression in selecting meaningful tokens, which sometimes outperforms the closely related attention weights. Finally, we conduct controlled experiments, and find that information is distributed across tokens, rather than being concentrated in specific "meaningful" tokens alone.
    
[^179]: 无偏好对齐学习与正则化相关奖励

    Preference-free Alignment Learning with Regularized Relevance Reward

    [https://arxiv.org/abs/2402.03469](https://arxiv.org/abs/2402.03469)

    无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。

    

    从人类偏好中学习被认为是将大规模语言模型（LLMs）与人类价值观对齐的关键。然而，与普遍的观点相反，我们的初步研究发现，基于人类偏好数据集训练的奖励模型倾向于给长的与主题无关的回复更高的分数，而给短的与主题相关的回复较低分。在这一观察的驱动下，我们探索了一种无偏好的方法，利用“相关性”作为对齐的一个关键目标。在我们的第一次尝试中，我们发现仅仅通过检索得到的相关性得分容易受到奖励欺骗的影响，即过度优化到不期望的捷径上，当我们将该得分作为奖励用于强化学习。为了缓解这个问题，我们将有效的归纳偏差整合到常规的相关性中，互相正则化，形成了一种混合奖励函数：正则化相关奖励（$R^3$）。$R^3$通过提供稳健的奖励信号，显著提高了在偏好基准测试中的性能。值得注意的是，$R^3$不需要

    Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
    
[^180]: 由任意线性变换驱动的精确张量补全

    Exact Tensor Completion Powered by Arbitrary Linear Transforms

    [https://arxiv.org/abs/2402.03468](https://arxiv.org/abs/2402.03468)

    本文研究了使用任意线性变换进行精确张量补全的问题，并提出了一种新的张量-张量乘积和张量核范数定义，设计了高效的算法并得到理论界限。这些工作极大地提升了张量补全的灵活性，大量实验证实了该方法的优越性。

    

    本文研究了一个张量补全问题，旨在从部分观测中完美恢复张量。现有的理论保证要求涉及的变换是正交的，这限制了其应用。在本文中，我们跳出了各向同性或自伴的约束，建立了使用任意线性变换进行精确张量补全的理论保证。为此，我们定义了一种新的张量-张量乘积，导致了张量核范数的新定义。配备了这些工具，我们设计了一种基于交替方向乘数法的高效算法来解决变换后的张量补全问题，并得到理论界限。我们的模型和证明极大地提升了张量补全的灵活性，大量的实验证实了所提方法的优越性。

    In this work, a tensor completion problem is studied, which aims to perfectly recover the tensor from partial observations. Existing theoretical guarantee requires the involved transform to be orthogonal, which hinders its applications. In this paper, jumping out of the constraints of isotropy or self-adjointness, the theoretical guarantee of exact tensor completion with arbitrary linear transforms is established. To that end, we define a new tensor-tensor product, which leads us to a new definition of the tensor nuclear norm. Equipped with these tools, an efficient algorithm based on alternating direction of multipliers is designed to solve the transformed tensor completion program and the theoretical bound is obtained. Our model and proof greatly enhance the flexibility of tensor completion and extensive experiments validate the superiority of the proposed method.
    
[^181]: 基于随机修改流的黎曼随机梯度下降

    Stochastic Modified Flows for Riemannian Stochastic Gradient Descent

    [https://arxiv.org/abs/2402.03467](https://arxiv.org/abs/2402.03467)

    本文研究了黎曼随机梯度下降（RSGD）的收敛速度，并介绍了一种基于随机修改流（RSMF）的扩散逼近方法，该方法可以提高对RSGD的近似精度。

    

    我们对黎曼随机梯度下降（RSGD）收敛速度给出了定量估计，并将其与黎曼梯度流和扩散过程——黎曼随机修改流（RSMF）进行了比较。利用随机微分几何工具，我们证明在小学习率范围内，RSGD可以近似为由无穷维维纳过程驱动的RSMF的解。RSMF考虑到了RSGD的随机波动，从而提高了与确定性黎曼梯度流的逼近顺序。RSGD使用了重传递映射的概念，即对指数映射的一种成本效益近似，我们对扩散逼近的弱误差进行了定量界定，在重传递映射、流形几何和梯度的随机估计的假设下证明了这些界定。

    We give quantitative estimates for the rate of convergence of Riemannian stochastic gradient descent (RSGD) to Riemannian gradient flow and to a diffusion process, the so-called Riemannian stochastic modified flow (RSMF). Using tools from stochastic differential geometry we show that, in the small learning rate regime, RSGD can be approximated by the solution to the RSMF driven by an infinite-dimensional Wiener process. The RSMF accounts for the random fluctuations of RSGD and, thereby, increases the order of approximation compared to the deterministic Riemannian gradient flow. The RSGD is build using the concept of a retraction map, that is, a cost efficient approximation of the exponential map, and we prove quantitative bounds for the weak error of the diffusion approximation under assumptions on the retraction map, the geometry of the manifold, and the random estimators of the gradient.
    
[^182]: 用分布式神经计算突破维度灾难

    Breaking the Curse of Dimensionality with Distributed Neural Computation

    [https://arxiv.org/abs/2402.03460](https://arxiv.org/abs/2402.03460)

    通过分布式神经计算算法，我们提出了一种理论方法来克服维度灾难，并证明了我们的模型可以在任意精度下逼近Lipschitz函数，在参数量和前向传播方面具有优势。

    

    我们提出了一种理论方法，利用分布式神经计算算法来克服维度灾难。我们的模块化分布式深度学习范式，称为“神经途径”，可以在多台机器上实现任意精度，同时仅加载少量参数到GPU VRAM中。具体地，我们证明了对于每个误差水平$\varepsilon>0$和每个Lipschitz函数$f:[0,1]^n\to \mathbb{R}$，我们可以构建一个神经途径模型，该模型能够在$[0,1]^n$上以$\varepsilon$精度均匀逼近$f$，并且仅需要在内存中加载$\mathcal{O}(\varepsilon^{-1})$个网络参数以及在前向传播期间加载$\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$个网络参数。这改进了传统非分布式深度学习模型（即ReLU多层感知机）的最优界限，后者需要$\mathcal{O}(\varepsilon^{-n/2})$个参数来达到相同的精度。目前唯一的其他可用深度学习模型

    We present a theoretical approach to overcome the curse of dimensionality using a neural computation algorithm which can be distributed across several machines. Our modular distributed deep learning paradigm, termed \textit{neural pathways}, can achieve arbitrary accuracy while only loading a small number of parameters into GPU VRAM. Formally, we prove that for every error level $\varepsilon>0$ and every Lipschitz function $f:[0,1]^n\to \mathbb{R}$, one can construct a neural pathways model which uniformly approximates $f$ to $\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of $\mathcal{O}(\varepsilon^{-1})$ parameters to be loaded in memory and $\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$ to be loaded during the forward pass. This improves the optimal bounds for traditional non-distributed deep learning models, namely ReLU MLPs, which need $\mathcal{O}(\varepsilon^{-n/2})$ parameters to achieve the same accuracy. The only other available deep learning model
    
[^183]: 高效且可解释的交通目的地预测方法——可解释拟合机器的应用

    Efficient and Interpretable Traffic Destination Prediction using Explainable Boosting Machines

    [https://arxiv.org/abs/2402.03457](https://arxiv.org/abs/2402.03457)

    本研究评估了一种高效且可解释的交通目的地预测模型（EBM），在多个混合交通数据集上表现出有竞争力的性能，并能够提供特征重要性和交互作用的分析以及预测解释的质量示例。

    

    开发精确的交通轨迹预测模型对于实现完全自动驾驶至关重要。各种深度神经网络模型已被应用于解决此挑战，但它们的黑盒特性限制了在部署系统中的透明度和调试能力。玻璃盒模型通过类似于GAM的方法提供了全面的可解释性。在本研究中，我们评估了一种高效的可加性模型，称为EBM，用于三个流行的混合交通数据集（SDD、InD和Argoverse）的交通预测。我们的结果显示EBM模型在预测SDD和InD中的行人目的地时具有竞争力，同时对以车辆为主的Argoverse数据集提供了适度的预测。此外，我们透明的训练模型使我们能够分析特征的重要性和相互作用，并提供预测解释的质量示例。全面的训练代码将在论文发表后公开。

    Developing accurate models for traffic trajectory predictions is crucial for achieving fully autonomous driving. Various deep neural network models have been employed to address this challenge, but their black-box nature hinders transparency and debugging capabilities in a deployed system. Glass-box models offer a solution by providing full interpretability through methods like \ac{GAM}. In this study, we evaluate an efficient additive model called \ac{EBM} for traffic prediction on three popular mixed traffic datasets: \ac{SDD}, \ac{InD}, and Argoverse. Our results show that the \ac{EBM} models perform competitively in predicting pedestrian destinations within \ac{SDD} and \ac{InD} while providing modest predictions for vehicle-dominant Argoverse dataset. Additionally, our transparent trained models allow us to analyse feature importance and interactions, as well as provide qualitative examples of predictions explanation. The full training code will be made public upon publication.
    
[^184]: 分散式间歇联邦学习：具有广义收敛保证的统一方法

    Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees

    [https://arxiv.org/abs/2402.03448](https://arxiv.org/abs/2402.03448)

    本文提出了一种称为分散式间歇联邦学习（DSpodFL）的方法，它统一了分布式梯度下降（DGD）、随机闲话（RG）和分散式联邦平均（DFedAvg）等著名的分散优化方法。根据分析结果，DSpodFL能够在更一般的假设下达到几何收敛速率与最佳性差距的匹配。经过实验验证了该方法的有效性。

    

    分散式联邦学习（DFL）近来受到了重要的研究关注，涵盖了模型更新和模型聚合这两个关键联邦学习过程都由客户端进行的设置。在本文中，我们提出了分散式间歇联邦学习（DSpodFL），这是一种DFL方法，它在这两个过程中广义化了间歇性的概念，建模了在实际DFL设置中出现的不同形式的异质性的影响。DSpodFL将许多着名的分散优化方法，如分布式梯度下降（DGD），随机闲话（RG）和分散式联邦平均（DFedAvg），统一到一个建模框架下。我们对DSpodFL的收敛行为进行了分析，显示出可以在更一般的假设下，将几何收敛速率与有限的最佳性差距相匹配。通过实验证明：

    Decentralized Federated Learning (DFL) has received significant recent research attention, capturing settings where both model updates and model aggregations -- the two key FL processes -- are conducted by the clients. In this work, we propose Decentralized Sporadic Federated Learning ($\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of sporadicity in both of these processes, modeling the impact of different forms of heterogeneity that manifest in realistic DFL settings. $\texttt{DSpodFL}$ unifies many of the prominent decentralized optimization methods, e.g., distributed gradient descent (DGD), randomized gossip (RG), and decentralized federated averaging (DFedAvg), under a single modeling framework. We analytically characterize the convergence behavior of $\texttt{DSpodFL}$, showing, among other insights, that we can match a geometric convergence rate to a finite optimality gap under more general assumptions than in existing works. Through experiments, we demonstra
    
[^185]: 变量重要性排序在相关性下的挑战

    Challenges in Variable Importance Ranking Under Correlation

    [https://arxiv.org/abs/2402.03447](https://arxiv.org/abs/2402.03447)

    变量重要性排序在解释机器学习中很重要，但是特征之间的相关性是一个挑战。最近提出了基于特征 Knockoffs 的改进方法来解决这个问题，我们的工作重点是对这些方法进行评估和评估。

    

    变量重要性在可解释机器学习中起到了关键作用，它帮助衡量因素对预测模型输出的影响。基于通过排列生成“空”特征的模型无关方法可以应用于此。这种分析通常在制药应用中使用，因为它能解释包括基于树的集成模型在内的黑盒模型。然而，变量重要性估算中存在的一个主要挑战和显著干扰因素是特征之间的相关性。最近，针对这个问题提出了几种基于特征 Knockoffs 的边际排列调整，如条件预测影响（CPI）等变量重要性度量方法。我们的工作重点是对这些方法进行评估和评估。我们首先提出了一项综合模拟研究，探讨了特征相关性对变量重要性评估的影响。然后我们在理论上证明了...

    Variable importance plays a pivotal role in interpretable machine learning as it helps measure the impact of factors on the output of the prediction model. Model agnostic methods based on the generation of "null" features via permutation (or related approaches) can be applied. Such analysis is often utilized in pharmaceutical applications due to its ability to interpret black-box models, including tree-based ensembles. A major challenge and significant confounder in variable importance estimation however is the presence of between-feature correlation. Recently, several adjustments to marginal permutation utilizing feature knockoffs were proposed to address this issue, such as the variable importance measure known as conditional predictive impact (CPI). Assessment and evaluation of such approaches is the focus of our work. We first present a comprehensive simulation study investigating the impact of feature correlation on the assessment of variable importance. We then theoretically prov
    
[^186]: 一个端到端的深度学习流水线用于血液输入的计算，包括局部体积校正以便进行自动化的参数化脑PET映射

    An end-to-end deep learning pipeline to derive blood input with partial volume corrections for automated parametric brain PET mapping

    [https://arxiv.org/abs/2402.03414](https://arxiv.org/abs/2402.03414)

    该论文介绍了一个端到端的深度学习流水线，利用局部体积校正和非侵入性动脉采样模型，自动推导血液输入函数，从而解决了动态FDG-PET定量分析中的关键挑战。

    

    动态2-[18F]氟-2-脱氧-D-葡萄糖正电子发射断层扫描（dFDG-PET）对人类脑部成像具有相当大的临床潜力，但其利用仍受限制。在对dFDG-PET的定量分析中，一个关键挑战是表征特定于患者的血液输入功能，传统上依赖侵入性动脉采血。本研究引入了一种新方法，利用非侵入性深度学习模型计算来自颈内动脉的局部体积（PV）校正，从而消除了侵入性动脉采血的需要。我们提出了一个端到端流程，其中包括基于3D U-Net的颈内动脉（ICA）分割和基于循环神经网络（RNN）的MCIF-net用于计算具有PV校正的模型修正血液输入函数（MCIF）。所开发的3D U-Net和RNN是使用5倍交叉验证方法在50个人类脑FDG PET数据集上进行训练和验证的。ICA-net在平均Dice系数上达到了...

    Dynamic 2-[18F] fluoro-2-deoxy-D-glucose positron emission tomography (dFDG-PET) for human brain imaging has considerable clinical potential, yet its utilization remains limited. A key challenge in the quantitative analysis of dFDG-PET is characterizing a patient-specific blood input function, traditionally reliant on invasive arterial blood sampling. This research introduces a novel approach employing non-invasive deep learning model-based computations from the internal carotid arteries (ICA) with partial volume (PV) corrections, thereby eliminating the need for invasive arterial sampling. We present an end-to-end pipeline incorporating a 3D U-Net based ICA-net for ICA segmentation, alongside a Recurrent Neural Network (RNN) based MCIF-net for the derivation of a model-corrected blood input function (MCIF) with PV corrections. The developed 3D U-Net and RNN was trained and validated using a 5-fold cross-validation approach on 50 human brain FDG PET datasets. The ICA-net achieved an av
    
[^187]: 通过自监督表示增强LLM基础的语音生成系统的稳定性

    Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations

    [https://arxiv.org/abs/2402.03407](https://arxiv.org/abs/2402.03407)

    通过自监督表示，我们提出了一种新的自助转换架构，该架构可以增强LLM基础的语音生成系统的稳定性，并克服了在推理时出现的多个稳定性问题。使用这种方法，LLM可以从文本中仅生成语音的内容和风格，而说话者身份由另一个模型提供。

    

    大型语言模型（LLMs）是下一代语音生成系统最有前景的技术之一，由于其可扩展性和上下文学习能力。然而，在推理时，它们遇到了多个稳定性问题，如幻觉、跳过内容或语音重复。在这项工作中，我们介绍了一种新的自监督语音转换（VC）架构，可以用于学习将瞬态特征（如内容）与固定特征（如说话者ID或录制条件）分开编码，创建说话者解耦表示。使用说话者解耦编码来训练LLMs进行文本到语音（TTS）允许LLM仅通过文本生成语音的内容和风格，类似于人类，而说话者身份由VC模型的解码器提供。结果显示，训练过说话者解耦的自监督表示的LLMs在说话者相似性方面提供了4.7pp的改进。

    Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity o
    
[^188]: 深度非线性高光谱解混使用多任务学习

    Deep Nonlinear Hyperspectral Unmixing Using Multi-task Learning

    [https://arxiv.org/abs/2402.03398](https://arxiv.org/abs/2402.03398)

    本文提出了一种基于深度学习的无监督非线性高光谱解混方法，通过将通用的非线性模型与无特殊假设的方法相结合，使用多任务学习来提高解混性能。

    

    非线性高光谱解混近年来受到了广泛关注，因为线性混合模型在一些问题上无法达到可接受的分辨率。事实上，大多数非线性解混方法都是通过假设特定的非线性模型而设计的，这限制了解混性能。在本文中，我们提出了一种基于深度学习的无监督非线性解混方法，将通用的非线性模型与没有特殊假设的方法相结合。该模型由两个分支组成。在第一个分支中，通过重构高光谱图像的行来学习端元，使用一些隐藏层，然后在第二个分支中，根据相应图像的列来学习丰度值。然后，通过多任务学习，我们引入了一个辅助任务来促使这两个分支共同工作。这种技术可以被视为一个正则化器，用于减轻过拟合，从而提高整个网络的性能。

    Nonlinear hyperspectral unmixing has recently received considerable attention, as linear mixture models do not lead to an acceptable resolution in some problems. In fact, most nonlinear unmixing methods are designed by assuming specific assumptions on the nonlinearity model which subsequently limits the unmixing performance. In this paper, we propose an unsupervised nonlinear unmixing approach based on deep learning by incorporating a general nonlinear model with no special assumptions. This model consists of two branches. In the first branch, endmembers are learned by reconstructing the rows of hyperspectral images using some hidden layers, and in the second branch, abundance values are learned based on the columns of respective images. Then, using multi-task learning, we introduce an auxiliary task to enforce the two branches to work together. This technique can be considered as a regularizer mitigating overfitting, which improves the performance of the total network. Extensive exper
    
[^189]: UniTSyn：一个可增强大型语言模型在程序测试中的能力的大规模数据集

    UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing

    [https://arxiv.org/abs/2402.03396](https://arxiv.org/abs/2402.03396)

    本研究提出了UniTSyn，一个大型数据集，可以增强大型语言模型在程序测试中的能力。通过关联测试和被测试函数，UniTSyn能够提高模型在生成准确和完整的测试方面的表现。

    

    大型语言模型（LLM）在生成高质量代码方面的出色能力已经引起了软件测试社区的广泛关注。然而，现有的代码LLM在生成准确和完整的测试方面往往表现不佳，因为它们是在不区分测试目的代码和其他代码的情况下进行训练的。在本文中，我们提出了一个大规模数据集UniTSyn，它能够增强LLM在单元测试合成方面的能力。将测试与被测试函数进行关联对于LLM推断预期行为和要验证的逻辑路径至关重要。通过利用语言服务器协议，UniTSyn实现了在没有每个项目执行设置或易碎且难以扩展的每个语言启发式的情况下收集焦点测试对的挑战目标。它包含了五种主流编程语言的270万个焦点测试对，使其能够被应用于优化大型语言模型的能力。

    The remarkable capability of large language models (LLMs) in generating high-quality code has drawn increasing attention in the software testing community. However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate and complete tests since they were trained on code snippets collected without differentiating between code for testing purposes and other code. In this paper, we present a large-scale dataset UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test Synthesis. Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified. By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics that tend to be fragile and difficult to scale. It contains 2.7 million focal-test pairs across five mainstream programming languages, making it possible to be utilize
    
[^190]: 在预算限制下行为用户分割中的优化传递发现

    Delivery Optimized Discovery in Behavioral User Segmentation under Budget Constrain

    [https://arxiv.org/abs/2402.03388](https://arxiv.org/abs/2402.03388)

    在预算限制下，我们提出了一种基于随机优化的算法，用于优化传递发现行为用户细分。

    

    用户在线行为足迹可以使公司发现基于行为的用户细分，并向用户发送特定细分的信息。在发现细分之后，通过像Facebook和Google这样的首选媒体渠道向用户发送信息可能具有挑战性，因为只有部分行为细分中的用户在媒体上找到匹配，并且只有其中一小部分看到消息（曝光）。即使高质量的发现也会在传递失败时变得无用。许多复杂的算法用于发现行为细分，然而这些算法忽略了传递组件。问题变得复杂是因为（i）发现是在公司数据（例如用户点击）的行为数据空间中进行的，而传递则是基于媒体定义的静态数据空间（例如地理位置，年龄）进行的；（ii）公司在预算限制下运作。我们引入了一种基于随机优化的算法，用于在预算限制下优化传递发现行为用户细分。

    Users' behavioral footprints online enable firms to discover behavior-based user segments (or, segments) and deliver segment specific messages to users. Following the discovery of segments, delivery of messages to users through preferred media channels like Facebook and Google can be challenging, as only a portion of users in a behavior segment find match in a medium, and only a fraction of those matched actually see the message (exposure). Even high quality discovery becomes futile when delivery fails. Many sophisticated algorithms exist for discovering behavioral segments; however, these ignore the delivery component. The problem is compounded because (i) the discovery is performed on the behavior data space in firms' data (e.g., user clicks), while the delivery is predicated on the static data space (e.g., geo, age) as defined by media; and (ii) firms work under budget constraint. We introduce a stochastic optimization based algorithm for delivery optimized discovery of behavioral u
    
[^191]: 克服自回归图生成中的顺序问题

    Overcoming Order in Autoregressive Graph Generation

    [https://arxiv.org/abs/2402.03387](https://arxiv.org/abs/2402.03387)

    这项研究提出在图生成中使用RNN，并通过增加无序正则化项解决了顺序问题，对于顺序图生成模型尤其在数据稀缺时有益处。

    

    图生成是各个领域中的一个基本问题，包括化学和社交网络。最近的研究表明，使用递归神经网络（RNN）生成分子图相对于传统的生成方法更具优势，传统方法需要将连续潜在表示转换为图。将图生成视为顺序生成时会出现一个问题，即由于图扁平化方法的具体选择而导致的序列的任意顺序。在这项工作中，我们提出使用RNN，通过增加无序正则化（OLR）项来考虑图的非顺序性，这鼓励递归模型的隐藏状态在训练分布中的不同有效排序下保持不变。我们证明了顺序图生成模型受益于我们提出的正则化方案，尤其在数据不足时。我们的发现为图生成领域的研究增添了贡献。

    Graph generation is a fundamental problem in various domains, including chemistry and social networks. Recent work has shown that molecular graph generation using recurrent neural networks (RNNs) is advantageous compared to traditional generative approaches which require converting continuous latent representations into graphs. One issue which arises when treating graph generation as sequential generation is the arbitrary order of the sequence which results from a particular choice of graph flattening method. In this work we propose using RNNs, taking into account the non-sequential nature of graphs by adding an Orderless Regularization (OLR) term that encourages the hidden state of the recurrent model to be invariant to different valid orderings present under the training distribution. We demonstrate that sequential graph generation models benefit from our proposed regularization scheme, especially when data is scarce. Our findings contribute to the growing body of research on graph g
    
[^192]: 基于神经网络结构的通用决策树集成：分布式梯度提升森林（DGBF）

    A generalized decision tree ensemble based on the NeuralNetworks architecture: Distributed Gradient Boosting Forest (DGBF)

    [https://arxiv.org/abs/2402.03386](https://arxiv.org/abs/2402.03386)

    本文提出了一种基于神经网络结构的通用决策树集成算法，分布式梯度提升森林（DGBF），通过将包和提升的数学公式结合起来，实现了树之间自然地进行分布式表示学习过程。该算法能够处理离散或表格数据，并具有建模非结构化数据的能力。

    

    目前，随机森林和梯度提升是建模离散或表格数据的主要方法，然而，由于它们的数学特性，无法像神经网络那样从原始数据中进行分层表示学习，这是深度学习问题和建模非结构化数据的关键特征。然而，在本文中，我们证明了包和提升的数学公式可以合并在一起，定义一个图结构的树集成算法，并在树之间自然地进行分布式表示学习过程（无需使用反向传播）。我们将这种新颖的方法称为分布式梯度提升森林（DGBF），并证明了随机森林和梯度提升都可以表示为DGBF的特定图结构。最后，我们发现...（摘要截断）

    Tree ensemble algorithms as RandomForest and GradientBoosting are currently the dominant methods for modeling discrete or tabular data, however, they are unable to perform a hierarchical representation learning from raw data as NeuralNetworks does thanks to its multi-layered structure, which is a key feature for DeepLearning problems and modeling unstructured data. This limitation is due to the fact that tree algorithms can not be trained with back-propagation because of their mathematical nature. However, in this work, we demonstrate that the mathematical formulation of bagging and boosting can be combined together to define a graph-structured-tree-ensemble algorithm with a distributed representation learning process between trees naturally (without using back-propagation). We call this novel approach Distributed Gradient Boosting Forest (DGBF) and we demonstrate that both RandomForest and GradientBoosting can be expressed as particular graph architectures of DGBT. Finally, we see tha
    
[^193]: 青少年关系行为与肥胖疫情：应用社交网络分析和机器学习技术的描述性研究

    Adolescent relational behaviour and the obesity pandemic: A descriptive study applying social network analysis and machine learning techniques

    [https://arxiv.org/abs/2402.03385](https://arxiv.org/abs/2402.03385)

    本研究通过应用社交网络分析和机器学习技术，研究了青少年关系行为与肥胖疫情之间的关系。研究结果表明，青少年的群体间存在显著差异，且与性别和饮食变量有关。总结出的关键因素有助于了解和干预青少年肥胖疫情。

    

    目的：通过探索群体节点属性之间的相似性，研究与饮食和性别相关的亚群存在，并通过SNA和人工智能技术分析群体之间的连接性。方法：2015年3月至12月，共有5个不同教育中心的235名学生参与了本研究。数据分析分为两个部分：社交网络分析和无监督机器学习技术。至于社交网络分析，应用Girvan-Newman技术在不同班级的友谊网络中找到最佳的内聚群体数量。结果：在三个班级中应用Girvan-Newman后，分别得到了2个、7个和6个最佳的群体划分。群体之间与性别和饮食变量之间存在显著差异。在分析了社交网络分析和无监督机器学习技术的结果后，研究者发现了影响青少年肥胖疫情的关键因素。

    Aim: To study the existence of subgroups by exploring the similarities between the attributes of the nodes of the groups, in relation to diet and gender and, to analyse the connectivity between groups based on aspects of similarities between them through SNA and artificial intelligence techniques.   Methods: 235 students from 5 different educational centres participate in this study between March and December 2015. Data analysis carried out is divided into two blocks: social network analysis and unsupervised machine learning techniques. As for the social network analysis, the Girvan-Newman technique was applied to find the best number of cohesive groups within each of the friendship networks of the different classes analysed.   Results: After applying Girvan-Newman in the three classes, the best division into clusters was respectively 2 for classroom A, 7 for classroom B and 6 for classroom C. There are significant differences between the groups and the gender and diet variables. After
    
[^194]: 使用迁移学习预测胶质瘤的生存和等级

    Survival and grade of the glioma prediction using transfer learning

    [https://arxiv.org/abs/2402.03384](https://arxiv.org/abs/2402.03384)

    本研究利用迁移学习技术，通过对胶质母细胞瘤图像数据集进行微调，成功实现了生存和肿瘤等级预测，生存预测准确率达到65%，肿瘤等级预测准确率达到97%。

    

    胶质母细胞瘤是一种高度恶性的脑肿瘤，没有治疗的情况下预期寿命仅为3至6个月。准确检测和预测其生存和等级至关重要。本研究引入了一种使用迁移学习技术的新方法。通过详尽的优化，测试了各种预训练的网络，包括EfficientNet、ResNet、VGG16和Inception，以找到最适合的架构。迁移学习应用于对胶质母细胞瘤图像数据集进行微调，旨在实现两个目标：生存预测和肿瘤等级预测。实验结果显示，生存预测准确率达到65%，将患者分为短期、中期和长期生存类别。此外，肿瘤等级的预测准确率达到97%，准确区分低等级胶质母细胞瘤(LGG)和高等级胶质母细胞瘤(HGG)。这种方法的成功归因于迁移学习的有效性，超越了当前的研究水平。

    Glioblastoma is a highly malignant brain tumor with a life expectancy of only 3 to 6 months without treatment. Detecting and predicting its survival and grade accurately are crucial. This study introduces a novel approach using transfer learning techniques. Various pre-trained networks, including EfficientNet, ResNet, VGG16, and Inception, were tested through exhaustive optimization to identify the most suitable architecture. Transfer learning was applied to fine-tune these models on a glioblastoma image dataset, aiming to achieve two objectives: survival and tumor grade prediction.The experimental results show 65% accuracy in survival prediction, classifying patients into short, medium, or long survival categories. Additionally, the prediction of tumor grade achieved an accuracy of 97%, accurately differentiating low-grade gliomas (LGG) and high-grade gliomas (HGG). The success of the approach is attributed to the effectiveness of transfer learning, surpassing the current state-of-the
    
[^195]: 全链路上升建模与上下文增强学习用于智能营销

    Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing

    [https://arxiv.org/abs/2402.03379](https://arxiv.org/abs/2402.03379)

    全链路上升建模方法ECUP旨在解决链路偏差和处理不适应问题，在线营销中有重要的应用价值。

    

    上升建模在在线营销中非常重要，它旨在通过预测个体处理效果（ITE）来准确衡量不同策略（如优惠券或折扣）对不同用户的影响。在电子商务环境中，用户行为遵循确定的顺序链路，包括展示、点击和转化。营销策略在这个链路中的每个阶段都会产生不同的上升效应，影响着点击率和转化率等指标。尽管其实用性，现有研究忽视了特定处理中所有阶段的相互影响，并未充分利用处理信息，可能给后续的营销决策引入了重大偏差。本文将这两个问题称为链路偏差问题和处理不适应问题。本文介绍了一种用于解决这些问题的具有上下文增强学习的全链路上升方法（ECUP）。ECUP包括两个主要组成部分：

    Uplift modeling, vital in online marketing, seeks to accurately measure the impact of various strategies, such as coupons or discounts, on different users by predicting the Individual Treatment Effect (ITE). In an e-commerce setting, user behavior follows a defined sequential chain, including impression, click, and conversion. Marketing strategies exert varied uplift effects at each stage within this chain, impacting metrics like click-through and conversion rate. Despite its utility, existing research has neglected to consider the inter-task across all stages impacts within a specific treatment and has insufficiently utilized the treatment information, potentially introducing substantial bias into subsequent marketing decisions. We identify these two issues as the chain-bias problem and the treatment-unadaptive problem. This paper introduces the Entire Chain UPlift method with context-enhanced learning (ECUP), devised to tackle these issues. ECUP consists of two primary components: 1)
    
[^196]: 评估谷歌语音识别和句子分类在医疗健康应用中的应用

    Evaluation of Google's Voice Recognition and Sentence Classification for Health Care Applications

    [https://arxiv.org/abs/2402.03369](https://arxiv.org/abs/2402.03369)

    本研究评估了谷歌的语音识别和句子分类在医疗健康应用中的应用。通过将语音识别技术应用于围手术期服务，可以改善患者流程和医疗质量。通过使用后处理分类器，本研究增强了谷歌的语音识别能力。

    

    本研究探讨了将语音识别技术应用于围手术期服务（Periop），以使Periop员工能够使用移动技术记录工作流程里程碑。如果能够使这种语音识别技术变得强大可靠，就可以通过使用移动技术改善患者流程和医疗质量。此实验的目标是使Periop员工能够提供无干扰的护理，而不需要进行数据录入和查询任务。然而，该研究的结果也适用于其他情况，即工程经理尝试使用移动技术改善通信性能。本研究通过使用后处理分类器（即句子包、支持向量机和最大熵）增强了谷歌的语音识别能力。实验研究了三个因素（原始措辞、简化措辞和个性化措辞）在三个水平（零次训练重复、5次训练重复和10次训练重复）的影响。

    This study examined the use of voice recognition technology in perioperative services (Periop) to enable Periop staff to record workflow milestones using mobile technology. The use of mobile technology to improve patient flow and quality of care could be facilitated if such voice recognition technology could be made robust. The goal of this experiment was to allow the Periop staff to provide care without being interrupted with data entry and querying tasks. However, the results are generalizable to other situations where an engineering manager attempts to improve communication performance using mobile technology. This study enhanced Google's voice recognition capability by using post-processing classifiers (i.e., bag-of-sentences, support vector machine, and maximum entropy). The experiments investigated three factors (original phrasing, reduced phrasing, and personalized phrasing) at three levels (zero training repetition, 5 training repetitions, and 10 training repetitions). Results 
    
[^197]: RAG-Fusion: 检索增强生成的新途径

    RAG-Fusion: a New Take on Retrieval-Augmented Generation

    [https://arxiv.org/abs/2402.03367](https://arxiv.org/abs/2402.03367)

    RAG-Fusion方法通过生成多个查询，并结合互惠排名融合技术，能够从不同角度上下文化原始查询，提供准确和全面的信息。这项研究在人工智能和自然语言处理应用方面有重要进展，并展示了全球和区域之间的转变。

    

    Infineon已经确定工程师、客户经理和客户迅速获取产品信息的需求。传统上，这个问题通过检索增强生成（RAG）聊天机器人来解决，但在这项研究中，我评估了新近流行的RAG-Fusion方法的使用。RAG-Fusion将RAG和互惠排名融合（RRF）相结合，通过生成多个查询，使用互惠分数对其进行再排序，并融合文档和分数。通过对准确性、相关性和全面性进行手动评估，我发现RAG-Fusion能够通过从不同的角度对原始查询进行上下文化，提供准确和全面的回答。然而，当生成的查询与原始查询的相关性不足时，有些答案偏离了主题。这项研究在人工智能（AI）和自然语言处理（NLP）应用方面取得了重要进展，并展示了全球和区域之间的变革。

    Infineon has identified a need for engineers, account managers, and customers to rapidly obtain product information. This problem is traditionally addressed with retrieval-augmented generation (RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion combines RAG and reciprocal rank fusion (RRF) by generating multiple queries, reranking them with reciprocal scores and fusing the documents and scores. Through manually evaluating answers on accuracy, relevance, and comprehensiveness, I found that RAG-Fusion was able to provide accurate and comprehensive answers due to the generated queries contextualizing the original query from various perspectives. However, some answers strayed off topic when the generated queries' relevance to the original query is insufficient. This research marks significant progress in artificial intelligence (AI) and natural language processing (NLP) applications and demonstrates transformations in a global and m
    
[^198]: 带有大型语言模型的不确定性感知可解释推荐

    Uncertainty-Aware Explainable Recommendation with Large Language Models

    [https://arxiv.org/abs/2402.03366](https://arxiv.org/abs/2402.03366)

    这项研究开发了一个模型，通过训练用户和项目输入的ID向量作为提示，利用GPT-2实现不确定性感知的可解释推荐系统。该系统采用联合训练机制并在多任务学习框架中进行优化，能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。

    

    在推荐系统内提供解释能够提升用户满意度并建立信任，特别是通过详细说明为用户定制推荐项目的原因。当前领域中主要的方法是生成基于文本的解释，而大型语言模型（LLMs）的应用尤为突出。然而，由于时间和计算资源限制，改进LLMs以实现可解释的推荐在实践上是不可行的。作为替代方案，当前的方法是训练提示而不是LLM。在这项研究中，我们开发了一个模型，利用用户和项目输入的ID向量作为GPT-2的提示。我们在多任务学习框架中采用联合训练机制，优化推荐任务和解释任务。这种策略能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。通过实验，我们的方法表现出...

    Providing explanations within the recommendation system would boost user satisfaction and foster trust, especially by elaborating on the reasons for selecting recommended items tailored to the user. The predominant approach in this domain revolves around generating text-based explanations, with a notable emphasis on applying large language models (LLMs). However, refining LLMs for explainable recommendations proves impractical due to time constraints and computing resource limitations. As an alternative, the current approach involves training the prompt rather than the LLM. In this study, we developed a model that utilizes the ID vectors of user and item inputs as prompts for GPT-2. We employed a joint training mechanism within a multi-task learning framework to optimize both the recommendation task and explanation task. This strategy enables a more effective exploration of users' interests, improving recommendation effectiveness and user satisfaction. Through the experiments, our meth
    
[^199]: 利用图卷积网络的異质友善推荐方法

    Heterophily-Aware Fair Recommendation using Graph Convolutional Networks

    [https://arxiv.org/abs/2402.03365](https://arxiv.org/abs/2402.03365)

    本文提出了一种利用图卷积网络的公平推荐系统，名为HetroFair，旨在提高项目侧的公平性。HetroFair使用公平注意力和异质性特征加权两个组件来生成具有公平性意识的嵌入。

    

    近年来，图神经网络（GNNs）已成为提高推荐系统准确性和性能的流行工具。现代推荐系统不仅设计为为最终用户服务，还要让其他参与者（如项目和项目供应商）从中受益。这些参与者可能具有不同或冲突的目标和利益，这引发了对公平性和流行度偏差考虑的需求。基于GNN的推荐方法也面临不公平性和流行度偏差的挑战，其归一化和聚合过程受到这些挑战的影响。在本文中，我们提出了一种公平的基于GNN的推荐系统，称为HetroFair，旨在提高项目侧的公平性。HetroFair使用两个独立的组件生成具有公平性意识的嵌入：i）公平注意力，它在GNN的归一化过程中结合了点积，以减少节点度数的影响；ii）异质性特征加权，为不同的特征分配不同的权重。

    In recent years, graph neural networks (GNNs) have become a popular tool to improve the accuracy and performance of recommender systems. Modern recommender systems are not only designed to serve the end users, but also to benefit other participants, such as items and items providers. These participants may have different or conflicting goals and interests, which raise the need for fairness and popularity bias considerations. GNN-based recommendation methods also face the challenges of unfairness and popularity bias and their normalization and aggregation processes suffer from these challenges. In this paper, we propose a fair GNN-based recommender system, called HetroFair, to improve items' side fairness. HetroFair uses two separate components to generate fairness-aware embeddings: i) fairness-aware attention which incorporates dot product in the normalization process of GNNs, to decrease the effect of nodes' degrees, and ii) heterophily feature weighting to assign distinct weights to 
    
[^200]: 探索质数分类：使用稀疏编码实现高召回率和快速收敛

    Exploring Prime Number Classification: Achieving High Recall Rate and Rapid Convergence with Sparse Encoding

    [https://arxiv.org/abs/2402.03363](https://arxiv.org/abs/2402.03363)

    通过稀疏编码和神经网络结构的组合，本文提出了一种在质数和非质数分类中实现高召回率和快速收敛的新方法，取得了令人满意的结果。

    

    本文提出了一种新颖的方法，结合机器学习和数论，在质数和非质数分类上进行研究。我们的研究核心是开发一种高度稀疏的编码方法，与传统的神经网络结构相结合。这种组合取得了令人满意的结果，在识别质数时达到了超过99\%的召回率，在识别非质数时达到了79\%的召回率，这些数字是从本质上不平衡的顺序整数序列中得出的，并且在完成单个训练周期之前迅速收敛。我们使用 $10^6$ 个整数进行训练，从指定的整数开始，然后在一个不同范围的 $2 \times 10^6$ 个整数上进行测试，范围从 $10^6$ 到 $3 \times 10^6$，偏移量相同。尽管受限于资源的内存容量，限制我们的分析跨越了 $3\times10^6$，但我们认为我们的研究对机器学习在......的应用做出了贡献

    This paper presents a novel approach at the intersection of machine learning and number theory, focusing on the classification of prime and non-prime numbers. At the core of our research is the development of a highly sparse encoding method, integrated with conventional neural network architectures. This combination has shown promising results, achieving a recall of over 99\% in identifying prime numbers and 79\% for non-prime numbers from an inherently imbalanced sequential series of integers, while exhibiting rapid model convergence before the completion of a single training epoch. We performed training using $10^6$ integers starting from a specified integer and tested on a different range of $2 \times 10^6$ integers extending from $10^6$ to $3 \times 10^6$, offset by the same starting integer. While constrained by the memory capacity of our resources, which limited our analysis to a span of $3\times10^6$, we believe that our study contribute to the application of machine learning in
    
[^201]: 图缩减的综合调研：稀疏化、粗化和浓缩

    A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation

    [https://arxiv.org/abs/2402.03358](https://arxiv.org/abs/2402.03358)

    这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。

    

    许多真实世界的数据集可以自然地表示为图，涵盖了广泛的领域。然而，图数据集的复杂性和规模的增加为分析和计算带来了显著的挑战。为此，图缩减技术在保留关键属性的同时简化大型图形数据变得越来越受关注。在本调研中，我们旨在提供对图缩减方法的全面理解，包括图稀疏化、图粗化和图浓缩。具体而言，我们建立了这些方法的统一定义，并引入了一个分层分类法来分类这些方法所解决的挑战。我们的调研系统地回顾了这些方法的技术细节，并强调了它们在各种场景中的实际应用。此外，我们还概述了保证图缩减技术持续有效性的关键研究方向，并提供了一个详细的论文列表链接。

    Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
    
[^202]: 利用网络效应减轻假新闻传播：通过自我模仿学习选择揭露者

    Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers via Self-Imitation Learning

    [https://arxiv.org/abs/2402.03357](https://arxiv.org/abs/2402.03357)

    本研究针对假新闻在社交网络中的影响，通过部署揭露者传播真实新闻，提出了一种通过自我模仿学习选择揭露者的方法。通过NAGASIL算法，能够在假新闻减轻中学习更有效的揭露者选择策略。

    

    本研究旨在通过部署揭露者传播真实新闻，最小化假新闻对社交网络的影响。这被设定为一种强化学习问题，每个阶段选取一个用户传播真实的新闻。一个具有挑战性的问题是一次性奖励，在社交网络上交织信息传播中，无法区分单个揭露者的选择所带来的“净”效应，只能观察到来自减轻努力的集体效果。现有的自我模仿学习（SIL）方法在从一次性奖励中学习方面显示出很大的潜力，但在实际的假新闻减轻应用中由于样本效率低下，不适用。为了学习更有效的假新闻减轻揭露者选择策略，本研究提出了NAGASIL - 基于负采样和状态增强生成对抗自我模仿学习，它包括两个针对假新闻减轻的改进:从负样本中学习和状态增强。

    This study aims to minimize the influence of fake news on social networks by deploying debunkers to propagate true news. This is framed as a reinforcement learning problem, where, at each stage, one user is selected to propagate true news. A challenging issue is episodic reward where the "net" effect of selecting individual debunkers cannot be discerned from the interleaving information propagation on social networks, and only the collective effect from mitigation efforts can be observed. Existing Self-Imitation Learning (SIL) methods have shown promise in learning from episodic rewards, but are ill-suited to the real-world application of fake news mitigation because of their poor sample efficiency. To learn a more effective debunker selection policy for fake news mitigation, this study proposes NAGASIL - Negative sampling and state Augmented Generative Adversarial Self-Imitation Learning, which consists of two improvements geared towards fake news mitigation: learning from negative sa
    
[^203]: 推特对市场趋势的影响：分析社交媒体情绪对生物技术股票的影响

    Tweet Influence on Market Trends: Analyzing the Impact of Social Media Sentiment on Biotech Stocks

    [https://arxiv.org/abs/2402.03353](https://arxiv.org/abs/2402.03353)

    本研究通过分析推特情绪与生物技术股票市场行为之间的关系，发现了社交媒体话语对投资者情绪和决策过程的影响，并提出了使用情绪协变量来改善股市预测准确性的方法。

    

    本研究调查了推特情绪在不同类别（新闻、公司观点、CEO观点、竞争对手观点）与生物技术领域股市行为之间的关系，重点是理解社交媒体话语对投资者情绪和决策过程的影响。我们分析了十家最大、最具影响力的制药公司的历史股市数据，同时考察了与COVID-19、疫苗、这些公司以及它们的CEO相关的推特数据。通过使用VADER情绪分析方法，我们考察了推文的情绪得分，并评估它们与股市表现的关系。我们采用ARIMA（自回归滑动平均）和VAR（向量自回归）模型来预测股市表现，并加入情绪协变量来提升预测准确性。我们的研究结果揭示了推特情绪、新闻、生物技术公司、其CEO和股市表现之间的复杂相互作用。

    This study investigates the relationship between tweet sentiment across diverse categories: news, company opinions, CEO opinions, competitor opinions, and stock market behavior in the biotechnology sector, with a focus on understanding the impact of social media discourse on investor sentiment and decision-making processes. We analyzed historical stock market data for ten of the largest and most influential pharmaceutical companies alongside Twitter data related to COVID-19, vaccines, the companies, and their respective CEOs. Using VADER sentiment analysis, we examined the sentiment scores of tweets and assessed their relationships with stock market performance. We employed ARIMA (AutoRegressive Integrated Moving Average) and VAR (Vector AutoRegression) models to forecast stock market performance, incorporating sentiment covariates to improve predictions. Our findings revealed a complex interplay between tweet sentiment, news, biotech companies, their CEOs, and stock market performance
    
[^204]: 面向具有耦合线性约束的非凸极小极大问题的零阶原始对偶交替投影梯度算法

    Zeroth-Order primal-dual Alternating Projection Gradient Algorithms for Nonconvex Minimax Problems with Coupled linear Constraints

    [https://arxiv.org/abs/2402.03352](https://arxiv.org/abs/2402.03352)

    本文研究了具有耦合线性约束的非凸极小极大问题的零阶算法，提出了两个单循环算法用于求解这些问题，并证明了它们的迭代复杂度分别为O(ε^(-2))和O(ε^(-4))。

    

    本文研究了确定性和随机设置下具有耦合线性约束的非凸极小极大问题的零阶算法，这在机器学习、信号处理和其他领域中近年来引起了广泛关注，例如资源分配问题和网络流问题中的对抗攻击等。我们提出了两个单循环算法，分别是零阶原始对偶交替投影梯度（ZO-PDAPG）算法和零阶正则动量原始对偶投影梯度算法（ZO-RMPDPG），用于解决具有耦合线性约束的确定性和随机非凸-(强)凹极小极大问题。证明了这两个算法获得一个ε-稳定点的迭代复杂度分别为O(ε^(-2))（对于求解非凸-凹极小极大问题）和O(ε^(-4))（对于求解非凸-凹极小极大问题）。

    In this paper, we study zeroth-order algorithms for nonconvex minimax problems with coupled linear constraints under the deterministic and stochastic settings, which have attracted wide attention in machine learning, signal processing and many other fields in recent years, e.g., adversarial attacks in resource allocation problems and network flow problems etc. We propose two single-loop algorithms, namely the zero-order primal-dual alternating projected gradient (ZO-PDAPG) algorithm and the zero-order regularized momentum primal-dual projected gradient algorithm (ZO-RMPDPG), for solving deterministic and stochastic nonconvex-(strongly) concave minimax problems with coupled linear constraints. The iteration complexity of the two proposed algorithms to obtain an $\varepsilon$-stationary point are proved to be $\mathcal{O}(\varepsilon ^{-2})$ (resp. $\mathcal{O}(\varepsilon ^{-4})$) for solving nonconvex-strongly concave (resp. nonconvex-concave) minimax problems with coupled linear const
    
[^205]: 当地球科学遇上生成AI和大型语言模型：基础、趋势和未来挑战

    When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges

    [https://arxiv.org/abs/2402.03349](https://arxiv.org/abs/2402.03349)

    这篇论文探讨了生成AI和大型语言模型在地球科学中的潜在应用，并讨论了几种已在地球科学中使用的GAI模型，包括生成对抗网络（GANs）、基于物理的模型等。

    

    生成人工智能（GAI）代表着一个新兴领域，承诺在不同的模态中创造合成数据和输出。 GAI最近在生物学、医学、教育、立法、计算机科学和金融等多个应用领域展示了令人印象深刻的成果。 为了实现增强的安全性、效率和可持续性，生成AI确实成为一个关键的差异化因素，并承诺在该领域引起模式转变。 本文探讨了生成AI和大型语言模型在地球科学中的潜在应用。 机器学习和深度学习领域的最新发展使得生成模型能够应对与地球科学和地球系统动力学相关的多样化预测问题、模拟和多标准决策挑战。 本综述讨论了在地球科学中使用的几种GAI模型，包括生成对抗网络（GANs）、基于物理的模型等。

    Generative Artificial Intelligence (GAI) represents an emerging field that promises the creation of synthetic data and outputs in different modalities. GAI has recently shown impressive results across a large spectrum of applications ranging from biology, medicine, education, legislation, computer science, and finance. As one strives for enhanced safety, efficiency, and sustainability, generative AI indeed emerges as a key differentiator and promises a paradigm shift in the field. This paper explores the potential applications of generative AI and large language models in geoscience. The recent developments in the field of machine learning and deep learning have enabled the generative model's utility for tackling diverse prediction problems, simulation, and multi-criteria decision-making challenges related to geoscience and Earth system dynamics. This survey discusses several GAI models that have been used in geoscience comprising generative adversarial networks (GANs), physics-informe
    
[^206]: 弱监督协方差矩阵对齐通过斯蒂弗矩阵估计在MEG应用中

    Weakly supervised covariance matrices alignment through Stiefel matrices estimation for MEG applications

    [https://arxiv.org/abs/2402.03345](https://arxiv.org/abs/2402.03345)

    本文引入了一种在MEG应用中用于时间序列数据的新颖领域自适应技术，称为混合模型斯蒂弗适应（MSA），通过利用无标记数据建立了等效信号方差的成对对应关系以确保有效的预测性能。在神经科学问题中，MSA在使用脑磁图进行脑龄回归时表现优于最近的方法。

    

    本文介绍了一种新的基于混合模型斯蒂弗适应（MSA）的时间序列数据领域自适应技巧，特别解决了目标数据集中有限标记信号的挑战。通过利用领域相关的混合模型和最优传输领域自适应假设，我们利用目标域中丰富的无标记数据，通过建立等效信号方差之间的成对对应关系，确保了有效的预测性能。为了从观测信号协方差的黎曼表示中恢复基础信号方差，我们建立了识别关键的斯蒂弗矩阵的理论基础。我们提出了一个综合成本函数，同时学习这些矩阵、成对域关系以及根据任务的预测器、分类器或回归器。应用于神经科学问题中，MSA在使用脑磁图进行任务变化的脑龄回归中优于最近的方法。

    This paper introduces a novel domain adaptation technique for time series data, called Mixing model Stiefel Adaptation (MSA), specifically addressing the challenge of limited labeled signals in the target dataset. Leveraging a domain-dependent mixing model and the optimal transport domain adaptation assumption, we exploit abundant unlabeled data in the target domain to ensure effective prediction by establishing pairwise correspondence with equivalent signal variances between domains. Theoretical foundations are laid for identifying crucial Stiefel matrices, essential for recovering underlying signal variances from a Riemannian representation of observed signal covariances. We propose an integrated cost function that simultaneously learns these matrices, pairwise domain relationships, and a predictor, classifier, or regressor, depending on the task. Applied to neuroscience problems, MSA outperforms recent methods in brain-age regression with task variations using magnetoencephalography
    
[^207]: 基于MADRL的车联网中具有防碰撞机制的无人机轨迹设计

    MADRL-based UAVs Trajectory Design with Anti-Collision Mechanism in Vehicular Networks

    [https://arxiv.org/abs/2402.03342](https://arxiv.org/abs/2402.03342)

    本论文提出了一种基于MADRL的无人机轨迹设计方法，采用排名二进制掩码的方式，实现了多个无人机之间的无碰撞路径设计。

    

    在即将到来的6G网络中，无人机作为移动基站，特别是针对车辆到一切（V2X）应用，将发挥重要作用。在这种情况下，最具挑战性的问题之一是多个无人机的轨迹设计，共同为同一区域提供服务。这种联合轨迹设计可以使用多智能体深度强化学习（MADRL）算法进行，但确保无人机之间的无碰撞路径成为一个关键挑战。传统方法在训练过程中施加高惩罚以避免不安全情况，但这些方法证明是无效的，而二进制掩码可以用来限制不安全的行动，但是简单应用于所有智能体可能导致次优解和低效率。为了解决这些问题，我们提出了一种基于等级的二进制掩码方法。较高等级的无人机移动优化，较低等级的无人机利用这些信息定义了改进的路径。

    In upcoming 6G networks, unmanned aerial vehicles (UAVs) are expected to play a fundamental role by acting as mobile base stations, particularly for demanding vehicle-to-everything (V2X) applications. In this scenario, one of the most challenging problems is the design of trajectories for multiple UAVs, cooperatively serving the same area. Such joint trajectory design can be performed using multi-agent deep reinforcement learning (MADRL) algorithms, but ensuring collision-free paths among UAVs becomes a critical challenge. Traditional methods involve imposing high penalties during training to discourage unsafe conditions, but these can be proven to be ineffective, whereas binary masks can be used to restrict unsafe actions, but naively applying them to all agents can lead to suboptimal solutions and inefficiencies. To address these issues, we propose a rank-based binary masking approach. Higher-ranked UAVs move optimally, while lower-ranked UAVs use this information to define improved 
    
[^208]: 在金融领域中，应用具有随机特征的CNN-DRL方法

    CNN-DRL with Shuffled Features in Finance

    [https://arxiv.org/abs/2402.03338](https://arxiv.org/abs/2402.03338)

    本研究通过对特征向量进行特定排列，应用CNN-DRL方法于金融数据中，在回报上取得了显著的提高。

    

    在先前的方法中，观察到将卷积神经网络代理应用于深度强化学习中的金融数据可以提高回报。本研究应用了特定的排列方式来对特征向量进行处理，从而生成一个CNN矩阵，将更相关的特征放置在靠近的位置。我们的全面实验评估明确地证明了回报的显著提高。

    In prior methods, it was observed that the application of Convolutional Neural Networks agent in Deep Reinforcement Learning to financial data resulted in an enhanced reward. In this study, a specific permutation was applied to the feature vector, thereby generating a CNN matrix that strategically positions more pertinent features in close proximity. Our comprehensive experimental evaluations unequivocally demonstrate a substantial enhancement in reward attainment.
    
[^209]: 强化学习的机器人帆船：模拟器和初步结果

    Reinforcement-learning robotic sailboats: simulator and preliminary results

    [https://arxiv.org/abs/2402.03337](https://arxiv.org/abs/2402.03337)

    本文介绍了开发虚拟海洋环境模拟真实实验的主要挑战和问题，并提出了使用强化学习代理进行自主导航和控制的关键特性。同时，还讨论了创建基于真实机器人帆船的功能性数字孪生所需的建模和实施步骤以及挑战。这项研究对于开发基于强化学习的导航算法在真实船只上的应用具有直接的意义。

    

    本文着重解决在使用无人表面船只(Unmanned Surface Vehicles, USV)的数字孪生开展虚拟海洋环境来复制真实实验时所面临的主要挑战和问题。我们介绍了构建虚拟世界的关键特性，考虑使用强化学习(RL)智能体进行自主导航和控制。在此基础上，主要问题涉及模拟方程的定义(物理和数学)、它们的有效实施以及如何将用于RL的模拟控制和感知策略(传感器)包括进来。我们介绍了基于真实机器人帆船创建功能性数字孪生所需的建模、实施步骤和挑战。该应用将立即用于开发基于RL的导航算法，以应用于真实船只。

    This work focuses on the main challenges and problems in developing a virtual oceanic environment reproducing real experiments using Unmanned Surface Vehicles (USV) digital twins. We introduce the key features for building virtual worlds, considering using Reinforcement Learning (RL) agents for autonomous navigation and control. With this in mind, the main problems concern the definition of the simulation equations (physics and mathematics), their effective implementation, and how to include strategies for simulated control and perception (sensors) to be used with RL. We present the modeling, implementation steps, and challenges required to create a functional digital twin based on a real robotic sailing vessel. The application is immediate for developing navigation algorithms based on RL to be applied on real boats.
    
[^210]: 循环神经网络

    Cyclic Neural Network

    [https://arxiv.org/abs/2402.03332](https://arxiv.org/abs/2402.03332)

    本文介绍了一种具有创新性的循环神经网络（Cyclic NNs），可以模拟生物智能系统的灵活动态图形特性，并在广泛测试的数据集上验证了其优越性。

    

    本文回答了人工神经网络（ANN）设计中的一个基本问题：我们不需要按顺序逐层构建ANN来保证有向无环图（DAG）属性。受生物智能（BI）的启发，其中神经元形成了一个复杂的、图形结构的网络，我们引入了具有开创性意义的循环神经网络（Cyclic NNs）。它模拟了生物神经系统灵活动态的图形特性，允许神经元在包括环路在内的任何图形结构中进行连接。与当前ANN的DAG结构相比，这提供了更大的适应性。我们进一步发展了基于这种新设计范 paradigm的首个详细模型——图覆盖多层感知机。通过在广泛测试的数据集上进行实验证明了循环神经网络在大多数一般情况下的优势，通过使用前向训练算法（FF）证明了它相对于当前的BP训练方法的优越性。这项研究阐述了循环神经网络的创新和贡献。

    This paper answers a fundamental question in artificial neural network (ANN) design: We do not need to build ANNs layer-by-layer sequentially to guarantee the Directed Acyclic Graph (DAG) property. Drawing inspiration from biological intelligence (BI), where neurons form a complex, graph-structured network, we introduce the groundbreaking Cyclic Neural Networks (Cyclic NNs). It emulates the flexible and dynamic graph nature of biological neural systems, allowing neuron connections in any graph-like structure, including cycles. This offers greater adaptability compared to the DAG structure of current ANNs. We further develop the Graph Over Multi-layer Perceptron, which is the first detailed model based on this new design paradigm. Experimental validation of the Cyclic NN's advantages on widely tested datasets in most generalized cases, demonstrating its superiority over current BP training methods through the use of a forward-forward (FF) training algorithm. This research illustrates a 
    
[^211]: Slot结构化世界模型

    Slot Structured World Models

    [https://arxiv.org/abs/2402.03326](https://arxiv.org/abs/2402.03326)

    Slot Structured World Models结合了基于Slot注意力的“对象为中心”编码器和基于潜在图的动力学模型，能够在多步预测任务中优于基线模型，解决了无法提取“对象为中心”表示和区分外观相似的多个对象的问题。

    

    感知和推理个体对象及其相互作用的能力是构建智能人工系统的目标。现有的方法使用前馈编码器来提取对象嵌入，使用潜在图神经网络来建模这些对象嵌入之间的相互作用。然而，前馈编码器无法提取“对象为中心”的表示，也无法区分外观相似的多个对象。为了解决这些问题，我们引入了一种称为“Slot结构化世界模型”（SSWM）的世界模型类，它将基于Slot注意力的“对象为中心”编码器与基于潜在图的动力学模型相结合。我们在Spriteworld基准测试中评估了我们的方法，该测试使用简单的物理相互作用规则，在操作条件下评估了一系列（多步）预测任务中的预测性能。所有复现论文实验的代码都可获取。

    The ability to perceive and reason about individual objects and their interactions is a goal to be achieved for building intelligent artificial systems. State-of-the-art approaches use a feedforward encoder to extract object embeddings and a latent graph neural network to model the interaction between these object embeddings. However, the feedforward encoder can not extract {\it object-centric} representations, nor can it disentangle multiple objects with similar appearance. To solve these issues, we introduce {\it Slot Structured World Models} (SSWM), a class of world models that combines an {\it object-centric} encoder (based on Slot Attention) with a latent graph-based dynamics model. We evaluate our method in the Spriteworld benchmark with simple rules of physical interaction, where Slot Structured World Models consistently outperform baselines on a range of (multi-step) prediction tasks with action-conditional object interactions. All code to reproduce paper experiments is availab
    
[^212]: 连接延迟：利用定向增强方法提高鲁棒性的微调改进

    Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations

    [https://arxiv.org/abs/2402.03325](https://arxiv.org/abs/2402.03325)

    本文研究了在标记的源领域上训练的模型在部署到分布不同的目标领域时的泛化问题，并提出了一种名为连接延迟的方法，通过自监督预训练和定向增强方法来改善模型鲁棒性。

    

    在标记的源领域上训练的模型（例如野生动物相机陷阱的标记图像）通常在部署到分布不同的目标领域（例如新的相机陷阱位置的图像）时泛化能力较差。在存在未标记的目标数据的域适应设置中，自监督预训练（例如遮蔽自编码或对比学习）是缓解性能下降的一种有希望的方法。预训练可以通过将源领域和目标领域相连接的通用数据增强方法（例如遮蔽或剪裁）来提高分布不同的错误率，即使输入空间中的两个领域相差很远。本文通过真实任务展示了在预训练后进行标准微调并不能持续改善分布不同的错误率，相比在标记的源数据上从头训练。为了更好地利用预训练来应对分布转变，我们提出了连接延迟（Connect Later）：在使用通用增强方法进行预训练后，用基于对目标领域了解的定向增强方法进行微调。

    Models trained on a labeled source domain (e.g., labeled images from wildlife camera traps) often generalize poorly when deployed on an out-of-distribution (OOD) target domain (e.g., images from new camera trap locations). In the domain adaptation setting where unlabeled target data is available, self-supervised pretraining (e.g., masked autoencoding or contrastive learning) is a promising method to mitigate this performance drop. Pretraining improves OOD error when the generic data augmentations used (e.g., masking or cropping) connect the source and target domains, which may be far apart in the input space. In this paper, we show on real-world tasks that standard fine-tuning after pretraining does not consistently improve OOD error over simply training from scratch on labeled source data. To better leverage pretraining for distribution shifts, we propose Connect Later: after pretraining with generic augmentations, fine-tune with targeted augmentations designed with knowledge of the d
    
[^213]: SpecFormer：通过最大奇异值惩罚来保护视觉Transformer的稳健性

    SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization

    [https://arxiv.org/abs/2402.03317](https://arxiv.org/abs/2402.03317)

    该论文介绍了SpecFormer，一种通过最大奇异值惩罚来增强视觉Transformer（ViTs）对对抗性攻击的韧性的方法。该方法通过引入局部Lipschitz边界和最大奇异值惩罚方法（MSVP），有效地降低了注意力权重矩阵的谱范数。

    

    视觉Transformer（ViTs）因其出色的性能而成为广泛使用的计算机视觉任务的首选。然而，其广泛应用引起了对面对恶意攻击时安全性的担忧。大多数现有方法依赖于训练过程中的经验调整，缺乏明确的理论基础。在本研究中，我们通过引入SpecFormer来填补这一空白，该方法专门设计用于增强ViTs对对抗性攻击的韧性，并得到了仔细推导的理论保证的支持。我们为自注意层建立了本地Lipschitz边界，并引入了一种新颖的方法，最大奇异值惩罚（MSVP），以精确控制这些边界。我们使用幂迭代方法将MSVP无缝集成到ViTs的注意力层中，以提高计算效率。修改后的模型SpecFormer有效地降低了注意力权重矩阵的谱范数，

    Vision Transformers (ViTs) have gained prominence as a preferred choice for a wide range of computer vision tasks due to their exceptional performance. However, their widespread adoption has raised concerns about security in the face of malicious attacks. Most existing methods rely on empirical adjustments during the training process, lacking a clear theoretical foundation. In this study, we address this gap by introducing SpecFormer, specifically designed to enhance ViTs' resilience against adversarial attacks, with support from carefully derived theoretical guarantees. We establish local Lipschitz bounds for the self-attention layer and introduce a novel approach, Maximum Singular Value Penalization (MSVP), to attain precise control over these bounds. We seamlessly integrate MSVP into ViTs' attention layers, using the power iteration method for enhanced computational efficiency. The modified model, SpecFormer, effectively reduces the spectral norms of attention weight matrices, there
    
[^214]: DeepSeekMath: 将开放语言模型中的数学推理能力推向极限

    DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

    [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)

    DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。

    

    数学推理由于其复杂和结构化的特性，对语言模型提出了重大挑战。本文介绍了DeepSeekMath 7B，它在Common Crawl中获取了120B个与数学相关的标记，并结合了自然语言和代码数据来继续预训练DeepSeek-Coder-Base-v1.5 7B。DeepSeekMath 7B在竞赛级别的数学基准测试中取得了令人印象深刻的51.7%的分数，无需依赖外部工具包和投票技术，接近了Gemini-Ultra和GPT-4的性能水平。DeepSeekMath 7B的自一致性在MATH上的64个样本中达到了60.9%的分数。DeepSeekMath的数学推理能力归因于两个关键因素：首先，我们通过精心设计的数据选择管道充分利用了公开可用的网络数据的巨大潜力。其次，我们引入了群体相对策略优化（GRPO），这是近端策略优化（PPO）的一个变体，可以增强数学推理能力。

    Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
    
[^215]: Cool-chic视频：通过800个参数学习视频编码

    Cool-chic video: Learned video coding with 800 parameters

    [https://arxiv.org/abs/2402.03179](https://arxiv.org/abs/2402.03179)

    我们提出了一个轻量级的学习视频编码器，使用800个参数和900次乘法来实现低解码复杂度。该编码器在压缩视频时能够利用时间冗余，并在接近AVC的速率失真条件下表现优于其他过拟合编解码器。

    

    我们提出了一种轻量级的通过学习的视频编解码器，每个解码像素有900次乘法，总共有800个参数。据我们所知，这是一个解码复杂度最低的神经视频编解码器之一。它基于过拟合的图片编解码器Cool-chic，并通过增加一个时域编码模块来强化视频的时间冗余。所提出的模型能够压缩视频以实现低延迟和随机访问配置，并在接近AVC的速率失真条件下优于其他过拟合编解码器，如FFNeRV。该系统是开源的：orange-opensource.github.io/Cool-Chic。

    We propose a lightweight learned video codec with 900 multiplications per decoded pixel and 800 parameters overall. To the best of our knowledge, this is one of the neural video codecs with the lowest decoding complexity. It is built upon the overfitted image codec Cool-chic and supplements it with an inter coding module to leverage the video's temporal redundancies. The proposed model is able to compress videos using both low-delay and random access configurations and achieves rate-distortion close to AVC while out-performing other overfitted codecs such as FFNeRV. The system is made open-source: orange-opensource.github.io/Cool-Chic.
    
[^216]: EasyInstruct：一个易于使用的用于大型语言模型的指令处理框架

    EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models

    [https://arxiv.org/abs/2402.03049](https://arxiv.org/abs/2402.03049)

    EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。

    

    近年来，指令调整已经引起了越来越多的关注，并成为增强大型语言模型（LLMs）能力的一种关键技术。为了构建高质量的指令数据集，已经提出了许多指令处理方法，旨在在数据数量和数据质量之间达到精巧的平衡。然而，由于各种指令处理方法之间仍然存在不一致，目前没有标准的开源指令处理实现框架可供社区使用，这使得从业者无法进一步开发和推进。为了促进指令处理的研究和开发，我们提出了EasyInstruct，一个易于使用的用于LLMs的指令处理框架，它将指令生成、选择和提示模块化，并考虑它们的组合和交互。EasyInstruct已经在https://github.com/zjunlp/EasyInstruct上公开发布，并得到了积极维护。

    In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
    
[^217]: 数据诱导的多尺度损失和高效多速率梯度下降方案

    Data-induced multiscale losses and efficient multirate gradient descent schemes

    [https://arxiv.org/abs/2402.03021](https://arxiv.org/abs/2402.03021)

    本文研究了多尺度数据对机器学习算法的影响，并提出了一种基于数据的新的梯度下降方法，旨在提高训练效率。

    

    本文研究多尺度数据对机器学习算法的影响，特别是在深度学习的背景下。如果一个数据集的分布在不同方向上具有尺度的显著变化，则其被称为多尺度数据。本文揭示了损失景观中的多尺度结构，包括其梯度和来自数据的海森矩阵。相应地，本文引入了一种新的梯度下降方法，受科学计算中使用的多尺度算法的启发。这种方法试图超越经验性学习率选择，提供一种更系统、数据驱动的策略来提高训练效率，特别是在后期阶段。

    This paper investigates the impact of multiscale data on machine learning algorithms, particularly in the context of deep learning. A dataset is multiscale if its distribution shows large variations in scale across different directions. This paper reveals multiscale structures in the loss landscape, including its gradients and Hessians inherited from the data. Correspondingly, it introduces a novel gradient descent approach, drawing inspiration from multiscale algorithms used in scientific computing. This approach seeks to transcend empirical learning rate selection, offering a more systematic, data-informed strategy to enhance training efficiency, especially in the later stages.
    
[^218]: 基于先处理、中处理和后处理的线性差异约束下的贝叶斯最优公平分类

    Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing

    [https://arxiv.org/abs/2402.02817](https://arxiv.org/abs/2402.02817)

    本文提出了一种基于贝叶斯最优的公平分类方法，通过先处理、中处理和后处理来最小化分类错误，并在给定群体公平性约束的情况下进行优化。该方法引入了线性和双线性差异度量的概念，并找到了贝叶斯最优公平分类器的形式。本方法能够处理多个公平性约束和常见情况。

    

    机器学习算法可能对受保护的群体产生不公平的影响。为解决这个问题，我们开发了基于贝叶斯最优的公平分类方法，旨在在给定群体公平性约束的情况下最小化分类错误。我们引入了线性差异度量的概念，它们是概率分类器的线性函数；以及双线性差异度量，它们在群体回归函数方面也是线性的。我们证明了几种常见的差异度量（如人口平等、机会平等和预测平等）都是双线性的。我们通过揭示与Neyman-Pearson引理的连接，找到了在单一线性差异度量下的贝叶斯最优公平分类器的形式。对于双线性差异度量，贝叶斯最优公平分类器变成了群体阈值规则。我们的方法还可以处理多个公平性约束（如平等的几率）和受保护属性常见的情况。

    Machine learning algorithms may have disparate impacts on protected groups. To address this, we develop methods for Bayes-optimal fair classification, aiming to minimize classification error subject to given group fairness constraints. We introduce the notion of \emph{linear disparity measures}, which are linear functions of a probabilistic classifier; and \emph{bilinear disparity measures}, which are also linear in the group-wise regression functions. We show that several popular disparity measures -- the deviations from demographic parity, equality of opportunity, and predictive equality -- are bilinear.   We find the form of Bayes-optimal fair classifiers under a single linear disparity measure, by uncovering a connection with the Neyman-Pearson lemma. For bilinear disparity measures, Bayes-optimal fair classifiers become group-wise thresholding rules. Our approach can also handle multiple fairness constraints (such as equalized odds), and the common scenario when the protected attr
    
[^219]: 重新思考微型语言模型的优化和架构

    Rethinking Optimization and Architecture for Tiny Language Models

    [https://arxiv.org/abs/2402.02791](https://arxiv.org/abs/2402.02791)

    本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。

    

    大型语言模型（LLMs）的威力通过大量的数据和计算资源得到了证明。然而，在移动设备上应用语言模型面临着计算和内存成本的巨大挑战，迫切需要高性能的微型语言模型。受复杂训练过程的限制，优化语言模型的许多细节很少得到仔细研究。在本研究中，基于一个具有10亿参数的微型语言模型，我们仔细设计了一系列经验研究来分析每个组件的影响。主要讨论了三个方面，即神经架构、参数初始化和优化策略。多个设计公式在微型语言模型中经验性地被证明特别有效，包括分词器压缩、架构调整、参数继承和多轮训练。然后，我们在1.6T多语种数据集上训练了PanGu-$\pi$-1B Pro和PanGu-$\pi$-1.5B Pro。

    The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
    
[^220]: 对比扩散器：通过对比学习规划高回报状态

    Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning

    [https://arxiv.org/abs/2402.02772](https://arxiv.org/abs/2402.02772)

    在强化学习中应用扩散模型进行规划常受限于基础分布和样本多样性。本文提出的CDiffuser方法通过对比学习来提高到达高回报状态的概率。

    

    最近在强化学习中应用扩散模型进行长期规划引起了广泛关注。几种基于扩散的方法成功地利用了扩散的建模能力进行任意分布的规划。这些方法为规划生成了后续轨迹，并取得了显著改进。然而，这些方法受到基础分布的限制，并忽视了样本的多样性，在这些方法中，不同状态具有不同的回报。它们仅仅利用扩散模型来学习离线数据集的分布，并生成与离线数据集具有相同分布的轨迹。因此，这些模型到达高回报状态的概率在很大程度上依赖于数据集的分布。即使配备了引导模型，性能仍然受到压制。针对这些限制，本文提出了一种名为CDiffuser的新方法，设计了一个返回函数

    Applying diffusion models in reinforcement learning for long-term planning has gained much attention recently. Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions. These methods generate subsequent trajectories for planning and have demonstrated significant improvement. However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns. They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset. As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution. Even equipped with the guidance model, the performance is still suppressed. To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return
    
[^221]: 基于深度强化学习的障碍物避障轨迹规划器与鲁棒低级控制的机器人操作器

    Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner with Robust Low-Level Control for Robotic Manipulators

    [https://arxiv.org/abs/2402.02551](https://arxiv.org/abs/2402.02551)

    这篇论文提出了一种基于深度强化学习和鲁棒低级控制的机器人操作器的障碍物避障轨迹规划方法，该方法通过与环境的交互积极参与学习，绕过了计算复杂性，同时解决了非重复和随机的避障任务。

    

    在机器人领域，现代策略往往是基于学习的，其特点是黑盒性质复杂，缺乏解释性，可能在确保稳定性和安全性方面带来挑战。为了解决这些问题，我们提出了将无障碍深度强化学习（DRL）轨迹规划器与新颖的自动调谐低级和关节级控制策略集成在一起，并通过与环境的交互积极参与学习阶段。这种方法绕过了与计算相关的复杂性，同时解决了非重复和随机的避障任务。首先，利用无模型DRL代理在关节级推理任务空间中进行速度限制和无障碍运动规划，然后将该规划输入到稳健的子系统自适应控制器中，产生所需的扭矩，而杜鹃搜索优化（CSO）算法增强了控制增益以最小化。

    In robotics, contemporary strategies are learning-based, characterized by a complex black-box nature and a lack of interpretability, which may pose challenges in ensuring stability and safety. To address these issues, we propose integrating an obstacle-free deep reinforcement learning (DRL) trajectory planner with a novel auto-tuning low- and joint-level control strategy, all while actively engaging in the learning phase through interactions with the environment. This approach circumvents the complexities associated with computations while also addressing nonrepetitive and random obstacle avoidance tasks. First, a model-free DRL agent to plan velocity-bounded and obstacle-free motion is employed for a manipulator with 'n' degrees of freedom (DoF) in task space through joint-level reasoning. This plan is then input into a robust subsystem-based adaptive controller, which produces the necessary torques, while the Cuckoo Search Optimization (CSO) algorithm enhances control gains to minimi
    
[^222]: TopoX: 一个用于拓扑域上的机器学习的Python软件包套件

    TopoX: A Suite of Python Packages for Machine Learning on Topological Domains

    [https://arxiv.org/abs/2402.02441](https://arxiv.org/abs/2402.02441)

    TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。

    

    我们介绍了topox，一个提供可靠且用户友好的Python软件包套件，用于在拓扑域（扩展了图的领域）上进行计算和机器学习：超图、单纯、胞腔、路径和组合复合体。topox由三个软件包组成：toponetx用于构建和计算这些域，包括节点、边和高阶单元的处理；topoembedx提供了将拓扑域嵌入到向量空间的方法，类似于流行的基于图的嵌入算法，如node2vec；topomodelx建立在PyTorch之上，为拓扑域上的神经网络提供了一套全面的高阶消息传递功能工具箱。topox的源代码经过广泛的文档化和单元测试，并在https://github.com/pyt-team以MIT许可证的形式提供。

    We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
    
[^223]: 利用低层次表示进行超快速道路分割

    Exploiting Low-level Representations for Ultra-Fast Road Segmentation

    [https://arxiv.org/abs/2402.02430](https://arxiv.org/abs/2402.02430)

    本研究利用低层次特征表示进行道路分割，在主流网络模型的主要阶段实现了大部分道路像素的准确表示，提出了以低层次特征为主导的道路分割网络（LFD-RoadSeg）。

    

    实现嵌入式平台上的实时性和准确性一直是道路分割方法的追求。为此，他们提出了许多轻量级网络。然而，他们忽视了道路是“物质”（背景或环境元素）而不是“东西”（特定可识别的对象）的事实，这激发了我们探索用低层次特征而不是高层次特征来表示道路的可行性。令人惊讶的是，我们发现主流网络模型的主要阶段足以表示大部分像素的道路进行分割。在此基础上，我们提出了一个以低层次特征为主导的道路分割网络（LFD-RoadSeg）。具体而言，LFD-RoadSeg采用了双边结构。首先设计了空间细节分支，通过ResNet-18的第一阶段提取道路的低层次特征表示。然后设计了上下文语义分支，以抑制低层次特征中错误地将无纹理区域误认为道路。

    Achieving real-time and accuracy on embedded platforms has always been the pursuit of road segmentation methods. To this end, they have proposed many lightweight networks. However, they ignore the fact that roads are "stuff" (background or environmental elements) rather than "things" (specific identifiable objects), which inspires us to explore the feasibility of representing roads with low-level instead of high-level features. Surprisingly, we find that the primary stage of mainstream network models is sufficient to represent most pixels of the road for segmentation. Motivated by this, we propose a Low-level Feature Dominated Road Segmentation network (LFD-RoadSeg). Specifically, LFD-RoadSeg employs a bilateral structure. The spatial detail branch is firstly designed to extract low-level feature representation for the road by the first stage of ResNet-18. To suppress texture-less regions mistaken as the road in the low-level feature, the context semantic branch is then designed to ext
    
[^224]: Aligner: 通过弱到强校正实现高效对齐

    Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction

    [https://arxiv.org/abs/2402.02416](https://arxiv.org/abs/2402.02416)

    Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。

    

    对于大型语言模型（LLMs），通过强化学习来进行对齐的努力主要是通过人类反馈的强化学习方法进行的。然而，强化学习面临着主要的挑战，包括训练奖励模型、演员-评论家工程以及重要的是，需要访问LLM参数。在这里，我们介绍了一种新的高效对齐范式Aligner，它通过学习对齐和未对齐答案之间的校正残差来绕过整个强化学习过程。我们的Aligner具有几个关键优势。首先，它是一个基于自监督学习的自动回归seq2seq模型，通过训练查询-答案-校正数据集，提供了一种参数高效的对齐解决方案，并且对资源需求较少。其次，Aligner实现了从弱到强的泛化；通过Aligner的监督信号来微调大型预训练模型，可以显著提升性能。第三，Aligner作为一个模型不可知的即插即用模块，可以直接应用于…

    Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
    
[^225]: 动态增量优化用于最佳子集选择

    Dynamic Incremental Optimization for Best Subset Selection

    [https://arxiv.org/abs/2402.02322](https://arxiv.org/abs/2402.02322)

    本文研究了一类$\ell_0$正则化问题的对偶形式，并提出了一种高效的原对偶算法，通过充分利用对偶范围估计和增量策略，提高了最佳子集选择问题的解决方案的效率和统计性质。

    

    最佳子集选择被认为是稀疏学习问题的“黄金标准”。已经提出了各种优化技术来攻击这个非光滑非凸问题。本文研究了一类$\ell_0$正则化问题的对偶形式。基于原始问题和对偶问题的结构，我们提出了一种高效的原对偶算法。通过充分利用对偶范围估计和增量策略，我们的算法潜在地减少了冗余计算并改进了最佳子集选择的解决方案。理论分析和对合成和真实数据集的实验验证了所提出解决方案的效率和统计性质。

    Best subset selection is considered the `gold standard' for many sparse learning problems. A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. In this paper, we investigate the dual forms of a family of $\ell_0$-regularized problems. An efficient primal-dual algorithm is developed based on the primal and dual problem structures. By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.
    
[^226]: 因果贝叶斯优化通过外源分布学习

    Causal Bayesian Optimization via Exogenous Distribution Learning

    [https://arxiv.org/abs/2402.02277](https://arxiv.org/abs/2402.02277)

    本文引入了一种新的方法，通过学习外源变量的分布，提高了结构化因果模型的近似精度，并将因果贝叶斯优化扩展到更一般的因果方案。

    

    在结构化因果模型中，将目标变量最大化作为操作目标是一个重要的问题。现有的因果贝叶斯优化（CBO）方法要么依赖于改变因果结构以最大化奖励的硬干预，要么引入动作节点到内生变量中，以调整数据生成机制以实现目标。本文引入了一种新的方法来学习外源变量的分布，这在现有方法中通常被忽略或通过期望进行边缘化。外源分布学习提高了通常通过有限观测数据训练的代理模型中的结构化因果模型的近似精度。此外，学习到的外源分布将现有的CBO扩展到超出加性噪声模型（ANM）的一般因果方案。恢复外源变量使我们能够为噪声或未观测到的隐藏变量使用更灵活的先验。引入了一种新的CBO方法。

    Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods.   Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is 
    
[^227]: 高维情况下，普通贝叶斯优化算法表现出色

    Vanilla Bayesian Optimization Performs Great in High Dimension

    [https://arxiv.org/abs/2402.02229](https://arxiv.org/abs/2402.02229)

    本文研究了高维情况下贝叶斯优化算法的问题，并提出了一种改进方法，通过对先验假设进行简单的缩放，使普通贝叶斯优化在高维任务中表现出色。

    

    长期以来，高维问题一直被认为是贝叶斯优化算法的软肋。受到维度噪音的刺激，许多算法旨在通过对目标应用各种简化假设来提高其性能。本文通过识别导致普通贝叶斯优化在高维任务中不适用的退化现象，并进一步展示了现有算法如何通过降低模型复杂度来应对这些退化现象。此外，我们还提出了一种对普通贝叶斯优化算法中典型先验假设的改进方法，该方法在不对目标施加结构性限制的情况下将复杂性降低到可管理的水平。我们的修改方法——通过维度对高斯过程长度先验进行简单的缩放——揭示了标准贝叶斯优化在高维情况下的显著改进，明确表明其效果远远超出以往的预期。

    High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly
    
[^228]: 位置论文：超级计算研究和LLMs的现状与挑战

    Position Paper: The Landscape and Challenges of HPC Research and LLMs

    [https://arxiv.org/abs/2402.02018](https://arxiv.org/abs/2402.02018)

    运用语言模型技术于高性能计算任务中具有巨大潜力

    

    最近，语言模型（LMs），特别是大规模的语言模型（LLMs），已经彻底改变了深度学习领域。编码器-解码器模型和基于提示的技术均展现出在自然语言处理和基于代码的任务中巨大的潜力。在过去几年中，许多研究实验室和机构在高性能计算方面投入了大量资源，达到或突破了超级计算的性能水平。本文提出，将这些基于语言模型的技术调整和应用于高性能计算任务中将会非常有益。本研究阐述了我们上述观点的理由，并强调了现有想法在HPC任务中的改进和应用。

    Recently, language models (LMs), especially large language models (LLMs), have revolutionized the field of deep learning. Both encoder-decoder models and prompt-based techniques have shown immense potential for natural language processing and code-based tasks. Over the past several years, many research labs and institutions have invested heavily in high-performance computing, approaching or breaching exascale performance levels. In this paper, we posit that adapting and utilizing such language model-based techniques for tasks in high-performance computing (HPC) would be very beneficial. This study presents our reasoning behind the aforementioned position and highlights how existing ideas can be improved and adapted for HPC tasks.
    
[^229]: 机器学习路径损耗预测的模拟增强数据增强方法

    Simulation-Enhanced Data Augmentation for Machine Learning Pathloss Prediction

    [https://arxiv.org/abs/2402.01969](https://arxiv.org/abs/2402.01969)

    本文提出了一种模拟增强数据增强方法，用于改善机器学习路径损耗预测中数据有限的挑战。该方法通过结合来自蜂窝覆盖模拟器生成的合成数据和独立收集的真实世界数据集，提供了关键的真实值用于模型训练。通过使用特征工程和高效稳健的梯度提升机器学习算法CatBoost，该方法显著提高了路径损耗预测的性能。

    

    机器学习（ML）对于路径损耗预测提供了一种有前景的解决方案。然而，由于数据的有限可用性，其有效性可能会降低。为了减轻这些挑战，本文引入了一种新颖的模拟增强数据增强方法，用于机器学习路径损耗预测。我们的方法将从蜂窝覆盖模拟器生成的合成数据与独立收集的现实世界数据集相结合。这些数据集通过在不同环境（包括农场、丘陵地带和住宅区）开展广泛的测量活动来收集。这种全面的数据收集为模型训练提供了至关重要的真实值。我们还设计了一组信道特征，包括从LiDAR数据集中导出的地理属性。然后，我们使用这些特征来训练我们的预测模型，结合了高效且稳健的梯度提升机器学习算法CatBoost。在我们的研究中，模拟数据的集成显著改善了路径损耗预测的性能。

    Machine learning (ML) offers a promising solution to pathloss prediction. However, its effectiveness can be degraded by the limited availability of data. To alleviate these challenges, this paper introduces a novel simulation-enhanced data augmentation method for ML pathloss prediction. Our method integrates synthetic data generated from a cellular coverage simulator and independently collected real-world datasets. These datasets were collected through an extensive measurement campaign in different environments, including farms, hilly terrains, and residential areas. This comprehensive data collection provides vital ground truth for model training. A set of channel features was engineered, including geographical attributes derived from LiDAR datasets. These features were then used to train our prediction model, incorporating the highly efficient and robust gradient boosting ML algorithm, CatBoost. The integration of synthetic data, as demonstrated in our study, significantly improves t
    
[^230]: 通过凸优化对基于神经网络的生成扩散模型进行分析

    Analyzing Neural Network-Based Generative Diffusion Models through Convex Optimization

    [https://arxiv.org/abs/2402.01965](https://arxiv.org/abs/2402.01965)

    本研究通过凸优化方法分析了基于神经网络的生成扩散模型，揭示了这些模型在非渐近设置下的精确预测分数函数和收敛结果。

    

    扩散模型在最先进的图像、视频和音频生成中变得广泛使用。基于分数的扩散模型在这些方法中脱颖而出，需要估计输入数据分布的分数函数。在本研究中，我们提出了一个理论框架，通过将分数匹配和去噪分数匹配重新构建为凸优化的形式，来分析两层神经网络的扩散模型。尽管现有的扩散理论主要是渐近的，但我们对神经网络的扩散模型给出了精确的预测分数函数，并建立了有限数据情况下的收敛结果。这项工作有助于理解神经网络的非渐近设置中学习到的扩散模型。

    Diffusion models are becoming widely used in state-of-the-art image, video and audio generation. Score-based diffusion models stand out among these methods, necessitating the estimation of score function of the input data distribution. In this study, we present a theoretical framework to analyze two-layer neural network-based diffusion models by reframing score matching and denoising score matching as convex optimization. Though existing diffusion theory is mainly asymptotic, we characterize the exact predicted score function and establish the convergence result for neural network-based diffusion models with finite data. This work contributes to understanding what neural network-based diffusion model learns in non-asymptotic settings.
    
[^231]: 通过符合预测实现运算器学习的校准不确定性量化

    Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction

    [https://arxiv.org/abs/2402.01960](https://arxiv.org/abs/2402.01960)

    通过符合预测方法，提出了一种校准不确定性量化的分位数神经运算器，能够在函数定义域上同时量化不确定性，无需分布假设，实验结果表明其在2D Darcy流动和3D车辆表面压力预测任务上优于基线方法。

    

    运算器学习在科学和工程应用中越来越被采用，其中很多应用需要校准的不确定性量化。由于运算器学习的输出是连续函数，在整个定义域上同时量化不确定性是具有挑战性的。当前的方法只考虑一个点的校准，或者针对一个标量函数进行校准，或者做出强大的假设，比如假设高斯性。我们提出了一种风险控制的分位数神经运算器,一种无分布、有限样本的函数校准符合预测方法。我们提供了一个理论上的校准保证，即覆盖率，其定义为函数定义域内真实值位于预测不确定性球内的预期百分比。在一个2D Darcy流动和一个3D车辆表面压力预测任务上的实证结果验证了我们的理论结果，表明校准的覆盖率和有效的不确定性区间优于基线方法。

    Operator learning has been increasingly adopted in scientific and engineering applications, many of which require calibrated uncertainty quantification. Since the output of operator learning is a continuous function, quantifying uncertainty simultaneously at all points in the domain is challenging. Current methods consider calibration at a single point or over one scalar function or make strong assumptions such as Gaussianity. We propose a risk-controlling quantile neural operator, a distribution-free, finite-sample functional calibration conformal prediction method. We provide a theoretical calibration guarantee on the coverage rate, defined as the expected percentage of points on the function domain whose true value lies within the predicted uncertainty ball. Empirical results on a 2D Darcy flow and a 3D car surface pressure prediction tasks validate our theoretical results, demonstrating calibrated coverage and efficient uncertainty bands outperforming baseline methods. In particula
    
[^232]: 基于大规模语言模型的超参数优化的技术

    Large Language Model Agent for Hyper-Parameter Optimization

    [https://arxiv.org/abs/2402.01881](https://arxiv.org/abs/2402.01881)

    基于大规模语言模型的AgentHPO技术通过自动化超参数优化，在机器学习任务中大大减少了试验次数，简化了设置过程，提升了解释性和用户信任。

    

    超参数优化在现代机器学习中至关重要，需要专业知识、大量实验以及高计算和人力资源。尽管自动化机器学习（AutoML）取得了一些进展，但试验效率、设置复杂性和互操作性方面仍存在挑战。为了解决这些问题，我们引入了一种新的范式，利用大规模语言模型（LLMs）来自动化不同机器学习任务的超参数优化，称为AgentHPO（LLM Agent-based Hyperparameter Optimization）。具体来说，AgentHPO自主处理任务信息，根据历史试验对特定超参数（HPs）进行实验，并进行迭代优化。与传统的AutoML方法相比，这种类似人类的优化过程极大地减少了所需的试验次数，简化了设置过程，并提升了解释性和用户信任。

    Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted o
    
[^233]: LLMs无法规划，但可以在LLM-Modulo框架中帮助规划

    LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks

    [https://arxiv.org/abs/2402.01817](https://arxiv.org/abs/2402.01817)

    LLMs无法独自进行规划或自我验证，但在规划/推理任务中可以作为通用近似知识源发挥更大作用。

    

    关于大型语言模型（LLMs）在规划和推理任务中的角色存在很大的困惑。一方面有人过于乐观地声称只需正确提示或自我验证策略，LLMs就能完成这些任务。另一方面，也有人过于悲观地认为LLMs在规划/推理任务中仅能作为问题规范的简单翻译器，并将问题交给外部符号求解器。在这篇立场文章中，我们认为这两种极端观点都是错误的。我们认为自回归LLMs本身不能进行规划或自我验证（毕竟这是一种推理形式），并对文献中的误解原因进行了一些阐述。我们还将辩称LLMs应该被视为具有更有意义的角色的通用近似知识源，能在规划/推理任务中发挥更大的作用。

    There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end forma
    
[^234]: 大规模语言模型用于时间序列：一项调研

    Large Language Models for Time Series: A Survey

    [https://arxiv.org/abs/2402.01801](https://arxiv.org/abs/2402.01801)

    本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。

    

    大规模语言模型（LLM）在自然语言处理和计算机视觉等领域得到了广泛应用。LLM不仅仅局限于文本、图像和图形，还具有对时间序列数据进行分析的重要潜力，可以在气候、物联网、医疗、交通、音频和金融等领域受益。本调研论文对利用LLM进行时间序列分析的各种方法进行了深入探讨和详细分类。我们解决了LLM原始文本数据训练与数值型时间序列数据之间的差异挑战，并探索了将LLM的知识转移和提取到数值时间序列分析的策略。我们详细介绍了各种方法，包括（1）直接提示LLM，（2）时间序列量化，（3）对齐技术，（4）利用视觉方式作为桥接机制，和（5）结合LLM与工具。此外，本调研还提供了一系列涉及应用领域、评估方法和未来研究方向的讨论。

    Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
    
[^235]: 当大型语言模型遇上向量数据库：一项综述

    When Large Language Models Meet Vector Databases: A Survey

    [https://arxiv.org/abs/2402.01763](https://arxiv.org/abs/2402.01763)

    本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。

    

    最近大型语言模型的突破在人类文字处理和生成方面开启了新的领域。然而，随着它们的显著增长，大型语言模型面临着包括幻觉、偏见、实时知识更新以及在商业环境中实施和维护的高成本等重要挑战。而另一种日益流行的工具，向量数据库则为这些挑战提供了潜在的解决方案。这些数据库擅长处理高维数据，并且对于高效的信息检索和语义搜索等任务至关重要。通过与大型语言模型的整合，它们显著增强了人工智能系统管理和更有效地利用多样数据的能力。本综述论文对大型语言模型和向量数据库之间的交叉点进行了深入而独特的分析。

    The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
    
[^236]: SymbolicAI: 一个结合生成模型和求解器的基于逻辑的方法的框架

    SymbolicAI: A framework for logic-based approaches combining generative models and solvers

    [https://arxiv.org/abs/2402.00854](https://arxiv.org/abs/2402.00854)

    SymbolicAI是一个基于逻辑的框架，将生成模型与多种求解器无缝集成，通过将大型语言模型作为语义解析器，实现了符号推理与生成式人工智能的融合。

    

    我们介绍了SymbolicAI，这是一个多功能且模块化的框架，采用基于逻辑的方法来处理生成过程中的概念学习和流程管理。SymbolicAI通过将大型语言模型（LLM）作为语义解析器来执行基于自然语言和形式语言指令的任务，从而弥合了符号推理和生成式人工智能之间的差距，使生成模型与各种求解器无缝集成。我们利用概率编程原理来处理复杂任务，并利用可微分和经典编程范 paradigms 的各自优势。该框架引入了一系列多态的、组合的和自指的数据流操作，将LLM的输出与用户的目标对齐。因此，我们可以在具有零次和少次学习能力的各种基础模型之间进行过渡，并与擅长解决特定问题的专业化调优模型或求解器配合使用。

    We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
    
[^237]: 《在大规模人工智能时代的贝叶斯深度学习》的立场论文

    Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI

    [https://arxiv.org/abs/2402.00809](https://arxiv.org/abs/2402.00809)

    《在大规模人工智能时代的贝叶斯深度学习》这篇立场论文探讨了贝叶斯深度学习在各种不同设置下的优势，并指出了与之相关的挑战和有趣的研究方向。未来的研究重点将放在如何将大规模基础模型与贝叶斯深度学习相结合，以发挥它们的全部潜力。

    

    在当前的深度学习研究领域中，人们主要关注在涉及大规模图像和语言数据集的监督任务中实现高预测准确性。然而，更广泛的视角揭示了许多被忽视的度量标准、任务和数据类型，如不确定性、主动和持续学习以及科学数据，这些方面需要关注。贝叶斯深度学习（BDL）是一条有前景的道路，可以在这些不同的设置中提供优势。本文认为BDL可以提升深度学习的能力。它重新审视了BDL的优势、承认了现有的挑战，并重点介绍了一些旨在解决这些障碍的有趣的研究方向。展望未来，讨论集中在可能的方式上，将大规模基础模型与BDL相结合，以充分发挥它们的潜力。

    In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.
    
[^238]: 理解Transformer在序列建模中的表达能力和机制

    Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling

    [https://arxiv.org/abs/2402.00522](https://arxiv.org/abs/2402.00522)

    本研究系统地探讨了Transformer在长序列建模中的近似性质，并研究了其关键组件对表达能力的影响机制。这些发现揭示了关键参数对Transformer的作用，并为替代架构提供了自然建议。

    

    我们对Transformer在长、稀疏和复杂记忆的序列建模中的近似性质进行了系统研究。我们调查了Transformer的不同组件（如点积自注意力、位置编码和前馈层）是如何影响其表达能力的机制，并通过建立明确的近似率来研究它们的综合影响。我们的研究揭示了Transformer中关键参数（如层数和注意力头数）的作用，并且这些洞察还为替代架构提供了自然建议。

    We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.
    
[^239]: PirateNets：采用残差自适应网络的物理知识驱动深度学习

    PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks

    [https://arxiv.org/abs/2402.00326](https://arxiv.org/abs/2402.00326)

    PirateNets是一种物理知识驱动的深度学习框架，解决了多层感知机网络在较大深度时性能下降的问题，通过引入自适应残差连接实现了稳定和高效的训练，并提升了模型的性能。

    

    虽然物理知识驱动神经网络(PINNs)已成为解决由偏微分方程(PDEs)控制的正向和反向问题的流行深度学习框架，但在采用更大和更深的神经网络架构时，它们的性能会下降。我们的研究发现，这种反直觉行为的根源在于使用不适合的初始化方案的多层感知机(MLP)网络结构，导致网络导数的可训练性较差，并最终导致PDE残差损失的不稳定最小化。为了解决这个问题，我们提出了物理知识驱动残差自适应网络(PirateNets)，这是一种新型架构，旨在促进深度PINN模型的稳定和高效训练。PirateNets利用一种新颖的自适应残差连接，允许网络作为浅层网络进行初始化，并在训练过程中逐渐加深。我们还展示了所提出的初始化方案可以提高PINN模型的训练效果并改善性能。

    While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initializatio
    
[^240]: 使用ODE方法进行带有马尔可夫噪声的随机逼近和强化学习

    The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise

    [https://arxiv.org/abs/2401.07844](https://arxiv.org/abs/2401.07844)

    本文通过使用ODE方法，将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，提高了其在离策略强化学习中的适用性。

    

    随机逼近是一类通过迭代、增量和随机更新向量的算法，包括随机梯度下降和时序差分学习。分析随机逼近算法的一个主要挑战是确保其稳定性，即证明随机向量迭代几乎必定有界。本文将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，大大提高了其在强化学习中的适用性，特别是那些具有线性函数逼近和资格迹的离策略强化学习算法。我们的分析的核心在于少数函数的渐进变化速率下降，这一点由大数定律和常用的V4 Lyapunov漂移条件隐含，并在马尔可夫链是有限且不可约时显然成立。

    Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
    
[^241]: 扩散模型、图像超分辨率和一切：一项调查研究

    Diffusion Models, Image Super-Resolution And Everything: A Survey

    [https://arxiv.org/abs/2401.00736](https://arxiv.org/abs/2401.00736)

    扩散模型（DMs）在图像超分辨率（SR）领域产生了颠覆性的影响，缩小了图像质量与人类感知偏好之间的差距。该研究调查了DM的理论基础，分析了其独特特点和方法，探索了替代输入领域等当前的研究方向。

    

    扩散模型（DMs）在图像超分辨率（SR）领域中产生了颠覆性的影响，进一步缩小了图像质量与人类感知偏好之间的差距。它们易于训练，并能生成比以前的生成方法产生的样本更高质量的图像。尽管取得了有希望的结果，但它们也带来了新的挑战，需要进一步的研究：高计算需求、可比性、缺乏可解释性、色彩偏移等。不幸的是，由于大量的出版物，进入这个领域令人难以应对。为了解决这个问题，我们提供了一个统一的叙述，阐明了应用于图像超分辨率的DM的理论基础，并提供了一份详细的分析，突出了该领域内与其他综述文章不同的独特特点和方法。这项调查研究对DM的原则进行了一个连贯的理解，并探索了当前的研究方向，包括替代输入领域等。

    Diffusion Models (DMs) have disrupted the image Super-Resolution (SR) field and further closed the gap between image quality and human perceptual preferences. They are easy to train and can produce very high-quality samples that exceed the realism of those produced by previous generative methods. Despite their promising results, they also come with new challenges that need further research: high computational demands, comparability, lack of explainability, color shifts, and more. Unfortunately, entry into this field is overwhelming because of the abundance of publications. To address this, we provide a unified recount of the theoretical foundations underlying DMs applied to image SR and offer a detailed analysis that underscores the unique characteristics and methodologies within this domain, distinct from broader existing reviews in the field. This survey articulates a cohesive understanding of DM principles and explores current research avenues, including alternative input domains, c
    
[^242]: 双阶段优化器用于系统性过估调整应用于多目标遗传算法的生物标志物选择

    Dual-stage optimizer for systematic overestimation adjustment applied to multi-objective genetic algorithms for biomarker selection

    [https://arxiv.org/abs/2312.16624](https://arxiv.org/abs/2312.16624)

    该论文介绍了一种双阶段优化器，用于对多目标遗传算法中的生物标志物选择进行系统性过估调整。

    

    在使用机器学习从组学数据中发现生物标志物的挑战在于分子特征的丰富性但样本的稀缺性。大多数机器学习中的特征选择方法需要评估各种特征集（模型）以确定最有效的组合。这个过程通常使用验证数据集进行，涉及测试不同的特征集以优化模型的性能。评估存在性能估计误差，当选择涉及多个模型时，最好的模型几乎肯定被过估。使用特征选择方法进行生物标志物识别可以视为具有预测能力和特征数量中的简约性之间的权衡的多目标问题。遗传算法是多目标优化中的一种流行工具，但它们进化出许多解决方案，因此容易出现过估。已经提出了一些方法来在选择了模型后减少过估。

    The challenge in biomarker discovery using machine learning from omics data lies in the abundance of molecular features but scarcity of samples. Most feature selection methods in machine learning require evaluating various sets of features (models) to determine the most effective combination. This process, typically conducted using a validation dataset, involves testing different feature sets to optimize the model's performance. Evaluations have performance estimation error and when the selection involves many models the best ones are almost certainly overestimated. Biomarker identification with feature selection methods can be addressed as a multi-objective problem with trade-offs between predictive ability and parsimony in the number of features. Genetic algorithms are a popular tool for multi-objective optimization but they evolve numerous solutions thus are prone to overestimation. Methods have been proposed to reduce the overestimation after a model has already been selected in si
    
[^243]: 更快速的Switchback实验方法

    Faster Rates for Switchback Experiments

    [https://arxiv.org/abs/2312.15574](https://arxiv.org/abs/2312.15574)

    本研究提出了一种更快速的Switchback实验方法，通过使用整个时间块，以 $\sqrt{\log T/T}$ 的速率估计全局平均处理效应。

    

    Switchback实验设计中，一个单独的单元（例如整个系统）在交替的时间块中暴露于一个随机处理，处理并行处理了跨单元和时间干扰问题。Hu和Wager（2022）最近提出了一种截断块起始的处理效应估计器，并在Markov条件下证明了用于估计全局平均处理效应（GATE）的$T^{-1/3}$速率，他们声称这个速率是最优的，并建议将注意力转向不同（且依赖设计）的估计量，以获得更快的速率。对于相同的设计，我们提出了一种替代估计器，使用整个块，并惊人地证明，在相同的假设下，它实际上达到了原始的设计独立GATE估计量的$\sqrt{\log T/T}$的估计速率。

    Switchback experimental design, wherein a single unit (e.g., a whole system) is exposed to a single random treatment for interspersed blocks of time, tackles both cross-unit and temporal interference. Hu and Wager (2022) recently proposed a treatment-effect estimator that truncates the beginnings of blocks and established a $T^{-1/3}$ rate for estimating the global average treatment effect (GATE) in a Markov setting with rapid mixing. They claim this rate is optimal and suggest focusing instead on a different (and design-dependent) estimand so as to enjoy a faster rate. For the same design we propose an alternative estimator that uses the whole block and surprisingly show that it in fact achieves an estimation rate of $\sqrt{\log T/T}$ for the original design-independent GATE estimand under the same assumptions.
    
[^244]: 扩展就是一切：使用JAX加速强化学习的自动驾驶

    Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning

    [https://arxiv.org/abs/2312.15122](https://arxiv.org/abs/2312.15122)

    本研究提出了一种扩展的自动驾驶强化学习方法，在大规模实验中展示了随着规模增加，策略性能的改善。与现有机器学习自动驾驶策略相比，我们的最佳策略将故障率降低了64％，同时提高了25％的驾驶进展速度。

    

    强化学习已经在复杂领域如视频游戏中展现出超越最优人类的能力。然而，为自动驾驶运行必要规模的强化学习实验非常困难。构建一个大规模的强化学习系统并在多个GPU上进行分布是具有挑战性的。在训练过程中在真实世界车辆上收集经验从安全和可扩展性的角度来看是不可行的。因此，需要一个高效且真实的驾驶模拟器，使用大量来自真实驾驶的数据。我们将这些能力集合在一起，并进行大规模的强化学习实验用于自动驾驶。我们证明，随着规模的增加，我们的策略表现得到了提升。我们最佳策略将故障率降低了64％，同时比现有机器学习自动驾驶策略提高了25％的驾驶进展速度。

    Reinforcement learning has been demonstrated to outperform even the best humans in complex domains like video games. However, running reinforcement learning experiments on the required scale for autonomous driving is extremely difficult. Building a large scale reinforcement learning system and distributing it across many GPUs is challenging. Gathering experience during training on real world vehicles is prohibitive from a safety and scalability perspective. Therefore, an efficient and realistic driving simulator is required that uses a large amount of data from real-world driving. We bring these capabilities together and conduct large-scale reinforcement learning experiments for autonomous driving. We demonstrate that our policy performance improves with increasing scale. Our best performing policy reduces the failure rate by 64% while improving the rate of driving progress by 25% compared to the policies produced by state-of-the-art machine learning for autonomous driving.
    
[^245]: XLand-MiniGrid: 在JAX中可扩展的元强化学习环境

    XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX

    [https://arxiv.org/abs/2312.12044](https://arxiv.org/abs/2312.12044)

    XLand-MiniGrid是一个在JAX中可扩展的元强化学习环境工具套件，提供了包含数百万个不同难度的任务和易于使用的基线，实现了在有限资源下的大规模实验民主化。

    

    受到XLand的多样性和深度以及MiniGrid的简单和简约的启发，我们提出了XLand-MiniGrid，这是一个用于元强化学习研究的工具套件和网格世界环境。XLand-MiniGrid采用JAX编写，旨在高度可扩展，并且可以在GPU或TPU加速器上运行，用有限资源实现大规模实验的民主化。除了环境外，XLand-MiniGrid还提供了预采样的基准测试，其中包含数百万个不同难度的独特任务和易于使用的基线，使用户可以快速开始训练自适应代理。此外，我们还进行了规模化和泛化的初步分析，表明我们的基线在训练中可以达到每秒数百万步，并验证了所提出的基准是具有挑战性的。

    Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging.
    
[^246]: Transformers的数学视角

    A mathematical perspective on Transformers

    [https://arxiv.org/abs/2312.10794](https://arxiv.org/abs/2312.10794)

    该论文提出了一种数学框架用于分析Transformers，并揭示了在长时间下的集团形成。这一研究为数学家和计算机科学家提供了新的视角。

    

    Transformers在大型语言模型的内部工作中起着核心作用。我们基于将Transformers解释为相互作用的粒子系统，开发了一个数学框架来分析Transformers，揭示了长时间下的集团形成。我们的研究探索了潜在的理论，并为数学家和计算机科学家提供了新的视角。

    Transformers play a central role in the inner workings of large language models. We develop a mathematical framework for analyzing Transformers based on their interpretation as interacting particle systems, which reveals that clusters emerge in long time. Our study explores the underlying theory and offers new perspectives for mathematicians as well as computer scientists.
    
[^247]: 动量粒子最大似然

    Momentum Particle Maximum Likelihood

    [https://arxiv.org/abs/2312.07335](https://arxiv.org/abs/2312.07335)

    该论文提出了一种动态系统方法，用于在参数和概率分布的扩展空间上最小化自由能函数，该方法融合了Nesterov的加速梯度方法、欠阻尼Langevin扩散和p。

    

    最大似然估计(MLE)的潜变量模型通常被重新解释为在参数和概率分布的扩展空间上的优化问题。例如，期望最大化(EM)算法可以解释为在这个空间上适用于合适的自由能函数的坐标下降。最近，这个观点与从最优传输和Wasserstein梯度流中获得的启示相结合，发展出了适用于更广泛模型类的基于粒子的算法，而不是标准的EM。受先前论文的启发，将将动量丰富的优化算法解释为常微分方程的离散化，我们提出了一种类似的动态系统方法，用于最小化在参数和概率分布的扩展空间上的自由能函数。结果是一个动态系统，结合了Nesterov的加速梯度方法、欠阻尼Langevin扩散和p。

    Maximum likelihood estimation (MLE) of latent variable models is often recast as an optimization problem over the extended space of parameters and probability distributions. For example, the Expectation Maximization (EM) algorithm can be interpreted as coordinate descent applied to a suitable free energy functional over this space. Recently, this perspective has been combined with insights from optimal transport and Wasserstein gradient flows to develop particle-based algorithms applicable to wider classes of models than standard EM.   Drawing inspiration from prior works which interpret `momentum-enriched' optimisation algorithms as discretizations of ordinary differential equations, we propose an analogous dynamical systems-inspired approach to minimizing the free energy functional over the extended space of parameters and probability distributions. The result is a dynamic system that blends elements of Nesterov's Accelerated Gradient method, the underdamped Langevin diffusion, and p
    
[^248]: 在线语言模型交互中的压缩上下文记忆

    Compressed Context Memory For Online Language Model Interaction

    [https://arxiv.org/abs/2312.03414](https://arxiv.org/abs/2312.03414)

    本论文提出了一种用于在线场景中Transformer语言模型的压缩上下文记忆系统，可以在有限的内存空间中实现语言模型推断，提高吞吐量，并通过个性化和多任务学习的评估证明了其有效性。

    

    本论文介绍了一种用于在线场景中Transformer语言模型的上下文键/值压缩方法，其中上下文不断扩展。随着上下文的增加，注意力过程需要更多的内存和计算，进而降低语言模型的吞吐量。为了解决这个挑战，我们提出了一个压缩上下文记忆系统，将累积的注意力键/值对不断压缩到紧凑的内存空间中，以便在计算环境的有限内存空间中进行语言模型推断。我们的压缩过程涉及将轻量级的条件LoRA整合到语言模型的前向传递中进行推断，而无需微调模型的所有权重。通过将递归压缩过程建模为单个并行化的前向计算，我们实现了高效的训练。通过对对话、个性化和多任务学习的评估，我们证明了我们的方法的有效性。

    This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approac
    
[^249]: 从古怪的语言模型中调取潜在知识

    Eliciting Latent Knowledge from Quirky Language Models

    [https://arxiv.org/abs/2312.01037](https://arxiv.org/abs/2312.01037)

    本研究通过引入一套“古怪”的语言模型，调取了这些模型在特定上下文中的潜在知识，展示了从可信度低的模型中调取可靠知识的前景。

    

    调取潜在知识（ELK）旨在在一个能力强大的神经网络的激活中找到模式，即使网络的明显输出是错误或误导性的，也能稳定跟踪世界的真实状态。为了进一步研究ELK，我们引入了12个数据集和一套相应的“古怪”的语言模型，这些模型在回答问题时，只有在提示中包含关键词“Bob”时才会进行系统性错误的微调。我们证明了简单的探测方法可以调取模型在这些上下文中对正确答案的潜在知识，即使问题比探测器训练的问题更困难。这是由于中间层激活中的上下文无关的知识表示的存在。我们还发现，一种机械的异常检测方法可以以94%的AUROC标识不真实行为。我们的结果显示，从能力强但不受信任的模型中调取可靠的知识，并促进未来研究ELK方法的实证研究是有希望的。

    Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
    
[^250]: 自回归Transformer的组合能力：对合成的可解释任务的研究

    Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks

    [https://arxiv.org/abs/2311.12997](https://arxiv.org/abs/2311.12997)

    本研究通过在合成数据生成过程上训练自回归Transformer模型，展示了其学习组合结构和泛化能力。通过生成中间输出来组合函数在泛化到新的未见组合时比不生成中间输出更有效。同时，组合顺序的偏差会对模型的性能产生影响。

    

    在大量文本语料库上训练的Transformer展示了一系列出色的能力，例如执行基本算术运算。考虑到语言的内在组合性质，我们可以期望模型学习将这些能力组合起来，从而潜在地产生对输入进行操作的组合爆炸。基于以上动机，我们在一个涉及一组明确定义的整体能力组合的合成数据生成过程上训练自回归Transformer模型。通过一系列广泛而系统的实验，我们发现：(1) 自回归Transformer可以从少量训练数据中学习组合结构，并且泛化到指数甚至组合爆炸数量的函数；(2) 在组合函数时生成中间输出比不生成任何中间输出更有效地泛化到新的、未见过的组合；(3) 在组合顺序中存在偏差会影响模型的性能。

    Transformers trained on huge text corpora exhibit a remarkable set of capabilities, e.g., performing basic arithmetic. Given the inherent compositional nature of language, one can expect the model to learn to compose these capabilities, potentially yielding a combinatorial explosion of what operations it can perform on an input. Motivated by the above, we train autoregressive Transformer models on a synthetic data-generating process that involves compositions of a set of well-defined monolithic capabilities. Through a series of extensive and systematic experiments on this data-generating process, we show that: (1) autoregressive Transformers can learn compositional structures from small amounts of training data and generalize to exponentially or even combinatorially many functions; (2) generating intermediate outputs when composing functions is more effective for generalizing to new, unseen compositions than not generating any intermediate outputs (3) biases in the order of the composi
    
[^251]: 超越PCA：一种概率性Gram-Schmidt方法的特征提取

    Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction

    [https://arxiv.org/abs/2311.09386](https://arxiv.org/abs/2311.09386)

    本研究提出了一种概率性Gram-Schmidt方法来进行特征提取，该方法可以检测和去除非线性依赖性，从而提取数据中的线性特征并去除非线性冗余。

    

    在无监督学习中，线性特征提取在数据中存在非线性依赖的情况下是一个基本挑战。我们提出使用概率性Gram-Schmidt (GS)类型的正交化过程来检测和映射出冗余维度。具体而言，通过在一族函数上应用GS过程，该族函数预计捕捉到数据中的非线性依赖性，我们构建了一系列协方差矩阵，可以用于识别新的大方差方向，或者将这些依赖性从主成分中去除。在前一种情况下，我们提供了熵减少的信息理论保证。在后一种情况下，我们证明在某些假设下，所得算法在所选择函数族的线性张成空间中可以检测和去除非线性依赖性。两种提出的方法都可以从数据中提取线性特征并去除非线性冗余。

    Linear feature extraction at the presence of nonlinear dependencies among the data is a fundamental challenge in unsupervised learning. We propose using a probabilistic Gram-Schmidt (GS) type orthogonalization process in order to detect and map out redundant dimensions. Specifically, by applying the GS process over a family of functions which presumably captures the nonlinear dependencies in the data, we construct a series of covariance matrices that can either be used to identify new large-variance directions, or to remove those dependencies from the principal components. In the former case, we provide information-theoretic guarantees in terms of entropy reduction. In the latter, we prove that under certain assumptions the resulting algorithms detect and remove nonlinear dependencies whenever those dependencies lie in the linear span of the chosen function family. Both proposed methods extract linear features from the data while removing nonlinear redundancies. We provide simulation r
    
[^252]: CAFE：在地理分布式数据中心中的碳感知联邦学习

    CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data Centers

    [https://arxiv.org/abs/2311.03615](https://arxiv.org/abs/2311.03615)

    本文介绍了一种名为CAFE的碳感知联邦学习框架，在地理分布式数据中心中优化训练过程，平衡学习性能和碳足迹。该方法通过coreset选择评估学习性能，利用Lyapunov漂移加惩罚框架处理未来碳强度的不可预测性，并设计了高效的算法来解决计算和通信成本的挑战。

    

    大规模人工智能（AI）模型的训练需要巨大的计算能力和能源，导致碳足迹增加，可能对环境产生潜在的影响。本文探讨了地理分布式（地理分布的）数据中心中训练AI模型的挑战，强调学习性能和碳足迹之间的平衡。我们考虑联邦学习（FL）作为一种解决方案，该方案将模型参数交换置于原始数据之上，确保数据隐私并符合当地法规。鉴于不同地区的碳强度变化，我们提出了一个名为CAFE（碳感知联邦学习）的新框架，以优化在固定碳足迹预算内的训练。我们的方法采用coreset选择来评估学习性能，采用Lyapunov漂移加惩罚框架来处理未来碳强度的不可预测性，并设计了一种高效的算法来解决计算和通信成本的挑战。

    Training large-scale artificial intelligence (AI) models demands significant computational power and energy, leading to increased carbon footprint with potential environmental repercussions. This paper delves into the challenges of training AI models across geographically distributed (geo-distributed) data centers, emphasizing the balance between learning performance and carbon footprint. We consider Federated Learning (FL) as a solution, which prioritizes model parameter exchange over raw data, ensuring data privacy and compliance with local regulations. Given the variability in carbon intensity across regions, we propose a new framework called CAFE (short for Carbon-Aware Federated Learning) to optimize training within a fixed carbon footprint budget. Our approach incorporates coreset selection to assess learning performance, employs the Lyapunov drift-plus-penalty framework to address the unpredictability of future carbon intensity, and devises an efficient algorithm to address the 
    
[^253]: DeepInception: 催眠大型语言模型成为破解者

    DeepInception: Hypnotize Large Language Model to Be Jailbreaker

    [https://arxiv.org/abs/2311.03191](https://arxiv.org/abs/2311.03191)

    本研究提出了一种名为DeepInception的轻量级方法，利用语言模型的角色扮演能力构建新颖的嵌套场景，成功催眠大型语言模型成为破解者。通过实验证明，DeepInception在破解成功率方面具有竞争力，并揭示了开源和闭源语言模型的关键弱点。

    

    尽管大型语言模型（LLMs）在各种应用中取得了显著的成功，但它们容易受到破解攻击，使得安全措施无效。然而，以往的破解研究通常采用暴力优化或高计算成本的外推方法，这可能并不实际或有效。本文受到以米尔格拉姆实验为灵感，关于权威力量对于引发有害行为的影响，我们提出了一种轻量级的方法，称为DeepInception，可以轻松地催眠LLM成为破解者。具体而言，DeepInception利用LLM的角色扮演能力构建了一个新颖的嵌套场景来行为，实现了在正常场景下逃避使用控制的自适应方式。实验结果表明，我们的DeepInception在破解成功率方面与以往的方法竞争力相当，并可以在后续交互中实现持续的破解，揭示了开源和闭源LLM的自失关键弱点。

    Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective. In this paper, inspired by the Milgram experiment w.r.t. the authority power for inciting harmfulness, we disclose a lightweight method, termed DeepInception, which can easily hypnotize LLM to be a jailbreaker. Specifically, DeepInception leverages the personification ability of LLM to construct a novel nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario. Empirically, our DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open and closed-source LLMs l
    
[^254]: 因果公平度量：连接因果性、个体公平性和对抗鲁棒性

    Causal Fair Metric: Bridging Causality, Individual Fairness, and Adversarial Robustness

    [https://arxiv.org/abs/2310.19391](https://arxiv.org/abs/2310.19391)

    本文引入了一种因果公平度量方法，它能够综合考虑因果性、个体公平性和对抗鲁棒性等因素。通过基于因果结构和保护性因果扰动来生成可比较的输入数据实例，并借助度量学习进行度量估计和应用。

    

    尽管负责任AI需要全面考虑因素，如鲁棒性、公平性和因果性，但这些因素常常被孤立地研究。用于识别模型中的漏洞的对抗扰动和为相似个体提供公平处理的个体公平性，尽管最初存在差异，但都依赖于生成可比较的输入数据实例的度量。以往关于定义这种联合度量的尝试往往缺乏关于数据或结构因果模型的一般假设，并且无法反映反事实的接近度。为了解决这个问题，我们的论文引入了一种基于因果结构的因果公平度量，包含敏感属性和受保护的因果扰动。为了增强我们的度量的实用性，在没有结构因果模型的情况下，我们提出了度量学习作为度量估计和在现实世界问题中部署的方法。我们还展示了我们的新度量在分类器中的应用。

    Despite the essential need for comprehensive considerations in responsible AI, factors like robustness, fairness, and causality are often studied in isolation. Adversarial perturbation, used to identify vulnerabilities in models, and individual fairness, aiming for equitable treatment of similar individuals, despite initial differences, both depend on metrics to generate comparable input data instances. Previous attempts to define such joint metrics often lack general assumptions about data or structural causal models and were unable to reflect counterfactual proximity. To address this, our paper introduces a causal fair metric formulated based on causal structures encompassing sensitive attributes and protected causal perturbation. To enhance the practicality of our metric, we propose metric learning as a method for metric estimation and deployment in real-world problems in the absence of structural causal models. We also demonstrate the application of our novel metric in classifiers.
    
[^255]: 联邦遗忘的综述：分类、挑战和未来方向

    A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions

    [https://arxiv.org/abs/2310.19218](https://arxiv.org/abs/2310.19218)

    联邦遗忘（FU）是解决联邦学习（FL）中数据隐私问题的战略解决方案。在发展FU方法时需要平衡隐私、安全、效用和效率的竞争性要求，以维持FL系统的效果和可用性。

    

    随着隐私保护的联邦学习（FL）的发展，对实现被遗忘权的需求越来越大。由于FL的分散性质，实施选择性遗忘尤其具有挑战性。这种复杂性催生了一个新的领域，即联邦遗忘（FU）。FU作为解决数据隐私需求的战略解决方案，包括实施“被遗忘权”。开发FU方法的主要挑战在于在隐私、安全、效用和效率之间取得平衡，因为这些因素往往具有竞争性要求。在保持FL系统的有效性和可用性的同时，实现这些方面的最佳平衡对于遵守隐私和安全标准至关重要。本综述对现有的FU方法进行了全面分析，包括对各种评估指标的详细评论。

    The evolution of privacy-preserving Federated Learning (FL) has led to an increasing demand for implementing the right to be forgotten. The implementation of selective forgetting is particularly challenging in FL due to its decentralized nature. This complexity has given rise to a new field, Federated Unlearning (FU). FU emerges as a strategic solution to address the increasing need for data privacy, including the implementation of the `right to be forgotten'. The primary challenge in developing FU approaches lies in balancing the trade-offs in privacy, security, utility, and efficiency, as these elements often have competing requirements. Achieving an optimal equilibrium among these facets is crucial for maintaining the effectiveness and usability of FL systems while adhering to privacy and security standards. This survey provides a comprehensive analysis of existing FU methods, incorporating a detailed review of the various evaluation metrics. Furthermore, we unify these diverse meth
    
[^256]: 通过多路径长期船舶轨迹预测建立更安全的海洋环境

    Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting

    [https://arxiv.org/abs/2310.18948](https://arxiv.org/abs/2310.18948)

    通过利用AIS数据预测船舶轨迹，本研究旨在通过减少船舶与鲸鱼碰撞来建立更安全的海洋环境。

    

    海上交通对于实现全球经济增长至关重要，同时也需要在可持续性和保护濒危海洋物种方面履行生态义务，尤其是保护大型鲸类种群。在这方面，自动识别系统(AIS)数据通过提供船舶运动的实时流数据，可以实现强化的交通监控，从而避免船舶与鲸鱼碰撞。本研究探讨利用AIS数据预测长期船舶轨迹，从而预防船舶与鲸鱼的碰撞。为此，我们采用双向长短期记忆网络(Bi-LSTM)构建了一种编码器-解码器模型架构，通过将1到3小时的AIS数据作为输入，预测接下来12小时的船舶轨迹。我们从历史AIS数据中提取潜在路线和目的地的概率特征，并将其作为模型的输入。模型随后预测船舶的轨迹，考虑到潜在路线和目的地的影响。

    Maritime transportation is paramount in achieving global economic growth, entailing concurrent ecological obligations in sustainability and safeguarding endangered marine species, most notably preserving large whale populations. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, allowing enhanced traffic monitoring. This study explores using AIS data to prevent vessel-to-whale collisions by forecasting long-term vessel trajectories from engineered AIS data sequences. For such a task, we have developed an encoder-decoder model architecture using Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data as input. We feed the model with probabilistic features engineered from historical AIS data that refer to each trajectory's potential route and destination. The model then predicts the vessel's trajectory, considerin
    
[^257]: HarmonyDream: 世界模型中的任务协调化

    HarmonyDream: Task Harmonization Inside World Models

    [https://arxiv.org/abs/2310.00344](https://arxiv.org/abs/2310.00344)

    在基于模型的强化学习中，通过深入研究观测建模和奖励建模的作用，发现通过缓解观测建模或奖励建模的占用优势来实现高效学习的潜力。

    

    基于模型的强化学习（MBRL）通过利用世界模型来实现高效学习的目标，世界模型通常包含观测建模和奖励建模两个任务。本文通过实证研究深入理解了每个任务在世界模型中的作用，并发现通过缓解观测建模或奖励建模的占用优势来实现高效学习的潜力。我们的关键观点是，虽然当前的显式MBRL方法试图通过观测模型恢复环境的丰富细节，但由于环境的复杂性和模型容量的限制，这是困难的。另一方面，隐式MBRL中奖励模型占主导地位，擅长学习紧凑的任务导向动态，但在没有更丰富的学习信号的情况下不适合高效学习。在这些观点和发现的基础上，我们提出了一个...

    Model-based reinforcement learning (MBRL) holds the promise of sample-efficient learning by utilizing a world model, which models how the environment works and typically encompasses components for two tasks: observation modeling and reward modeling. In this paper, through a dedicated empirical investigation, we gain a deeper understanding of the role each task plays in world models and uncover the overlooked potential of sample-efficient MBRL by mitigating the domination of either observation or reward modeling. Our key insight is that while prevalent approaches of explicit MBRL attempt to restore abundant details of the environment via observation models, it is difficult due to the environment's complexity and limited model capacity. On the other hand, reward models, while dominating implicit MBRL and adept at learning compact task-centric dynamics, are inadequate for sample-efficient learning without richer learning signals. Motivated by these insights and discoveries, we propose a s
    
[^258]: CC-SGG: 使用学习的场景图生成边界情况场景

    CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs

    [https://arxiv.org/abs/2309.09844](https://arxiv.org/abs/2309.09844)

    本研究介绍了一种采用异构图神经网络的新方法，将普通驾驶场景转化为边界情况场景。通过生成简洁的场景图，并利用注意力和三元嵌入扰动图形，我们的模型成功学习到生成边界情况的能力。

    

    边界情况场景对于测试和验证自主驾驶车辆（AV）的安全性至关重要。由于这些场景在自然驾驶数据集中通常不足，因此使用合成的边界情况数据可以极大地增强AV在独特情况下的安全操作。然而，生成合成的、真实但又逼真的边界情况场景是一个重要挑战。在这项工作中，我们介绍了一种基于异构图神经网络（HGNNs）的新方法，将普通驾驶场景转换为边界情况场景。为了实现这一目标，我们首先将普通驾驶场景生成简洁的场景图形式，最小化对其结构和属性的改变。然后，我们的模型通过注意力和三元嵌入来学习扰动这些图形以生成边界情况。输入和扰动后的图形再次导入模拟器以生成边界情况场景。我们的模型成功学习到从普通场景生成边界情况的能力。

    Corner case scenarios are an essential tool for testing and validating the safety of autonomous vehicles (AVs). As these scenarios are often insufficiently present in naturalistic driving datasets, augmenting the data with synthetic corner cases greatly enhances the safe operation of AVs in unique situations. However, the generation of synthetic, yet realistic, corner cases poses a significant challenge. In this work, we introduce a novel approach based on Heterogeneous Graph Neural Networks (HGNNs) to transform regular driving scenarios into corner cases. To achieve this, we first generate concise representations of regular driving scenes as scene graphs, minimally manipulating their structure and properties. Our model then learns to perturb those graphs to generate corner cases using attention and triple embeddings. The input and perturbed graphs are then imported back into the simulation to generate corner case scenarios. Our model successfully learned to produce corner cases from i
    
[^259]: OHQ: 芯片上的硬件感知量化

    OHQ: On-chip Hardware-aware Quantization

    [https://arxiv.org/abs/2309.01945](https://arxiv.org/abs/2309.01945)

    本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），通过构建量化感知流水线和引入掩码引导的量化估计技术，实现了在资源受限的硬件上进行高效量化，填补了现有混合精度量化的搜索空间过大和实际部署差距大的问题。

    

    量化成为在资源受限的硬件上部署先进深度模型的最有前途的方法之一。混合精度量化利用多位宽架构来释放量化模型的准确性和效率潜力。然而，现有的混合精度量化存在搜索空间过大的问题，导致巨大的计算开销。因此，量化过程依赖于独立的高性能设备，而不是本地进行，这也导致了考虑的硬件指标与实际部署之间的显著差距。本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），而无需访问在线设备。首先，我们构建了芯片上的量化感知（OQA）流水线，能够感知量化算子在硬件上的实际效率指标。其次，我们提出了基于掩码引导的量化估计（MQE）技术。

    Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
    
[^260]: CroSSL: 跨模态的自监督学习在时间序列上的应用通过隐藏掩码

    CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent Masking

    [https://arxiv.org/abs/2307.16847](https://arxiv.org/abs/2307.16847)

    CroSSL是一种跨模态的自监督学习方法，通过隐藏掩码和跨模态聚合器实现对时间序列的学习，无需负样本对和数据预处理。

    

    机器学习多模态时间序列的标注数据的有限可用性严重阻碍了该领域的进展。自监督学习（SSL）是一种无需依赖标签学习数据表示的有前景的方法。然而，现有的SSL方法需要计算昂贵的负样本对，并且通常仅适用于单模态，从而限制了它们的多功能性。我们引入了CroSSL（跨模态自监督学习），它提出了两个创新概念：通过模态特定编码器产生的中间嵌入的隐藏掩码，以及通过跨模态聚合器将其聚合为全局嵌入，可以提供给下游分类器。CroSSL允许处理缺失模态和端到端的跨模态学习，无需进行预处理以处理缺失输入或进行对比学习的负样本采样。我们对各种数据进行了评估，包括加速度计或陀螺仪等运动传感器和生物传感器。

    Limited availability of labeled data for machine learning on multimodal time-series extensively hampers progress in the field. Self-supervised learning (SSL) is a promising approach to learning data representations without relying on labels. However, existing SSL methods require expensive computations of negative pairs and are typically designed for single modalities, which limits their versatility. We introduce CroSSL (Cross-modal SSL), which puts forward two novel concepts: masking intermediate embeddings produced by modality-specific encoders, and their aggregation into a global embedding through a cross-modal aggregator that can be fed to down-stream classifiers. CroSSL allows for handling missing modalities and end-to-end cross-modal learning without requiring prior data preprocessing for handling missing inputs or negative-pair sampling for contrastive learning. We evaluate our method on a wide range of data, including motion sensors such as accelerometers or gyroscopes and biosi
    
[^261]: 通过基于ASCII-Art的跨模态任务测试ChatGPT对于理解深度的能力：GPT3.5在识别和生成ASCII-Art方面的能力并不完全缺乏

    Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking

    [https://arxiv.org/abs/2307.16806](https://arxiv.org/abs/2307.16806)

    本研究通过基于ASCII-Art的跨模态任务，探讨了ChatGPT和GPT3.5在视觉任务中的能力，结果表明它们在图像识别、图像部分知识和图像生成方面并不完全缺乏。

    

    在发布后的八个月里，由于其强大的能力和易于使用，ChatGPT及其底层模型GPT3.5引起了广泛关注。虽然出现了一批研究这些模型能力范围的论文，但这些网络所接收和提取的信息要么是自然语言文本，要么是类似代码的风格化语言。在这项工作中，我们受到对一个真正达到人类水平的智能代理在多个信号模态上具备的能力的启示，考察了GPT3.5在视觉任务中的能力，其中输入以ASCII-Art形式提供内容，没有明显的语言化总结。我们进行了一系列实验，分析了该模型在经过典型的视觉设置下的图像识别任务上的表现，调查了其对图像部分的知识以及图像生成的任务。

    Over the eight months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche-industry of papers have emerged examining the scope of capabilities these models possess, the information fed to and extracted from these networks has been either natural language text or stylized, code-like language. Drawing inspiration from the prowess we expect a truly human-level intelligent agent to have across multiple signal modalities, in this work we examine GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary. We conduct experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings, trials investigating knowledge of image parts, and tasks covering image generation.
    
[^262]: 无参考图像标题评估指标的鲁棒性研究

    An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics

    [https://arxiv.org/abs/2305.14998](https://arxiv.org/abs/2305.14998)

    研究评估了无参考图像标题评估指标在高词汇重叠但含义差异很大的情况下的鲁棒性，结果发现尽管这些指标与人类判断相关性较高，但对细粒度错误识别困难，并且在标题不合理性错误、图像相关对象大小变化以及标题对否定意义的理解方面存在敏感性差异。

    

    最近，提出了一些无参考指标，如CLIPScore（Hessel等，2021），UMIC（Lee等，2021）和PAC-S（Sarto等，2023），用于自动无参考评估图像标题。我们的研究重点在于评估这些指标在需要区分具有高词汇重叠但含义差异很大的两个标题的情况下的鲁棒性。我们的研究结果显示，尽管这些指标与人类判断具有很高的相关性，但CLIPScore、UMIC和PAC-S很难识别细粒度错误。虽然所有指标对视觉错误敏感，但对标题不合理性错误的敏感性有限。此外，我们还发现所有指标对标题中提及的与图像相关的对象的大小变化敏感，而CLIPScore和PAC-S对标题中提及的与图像相关的对象的数量也敏感。关于标题的语言方面，所有指标对否定意义的理解能力较弱。

    Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021), UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evaluation of image captions. Our focus lies in evaluating the robustness of these metrics in scenarios that require distinguishing between two captions with high lexical overlap but very different meanings. Our findings reveal that despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors. While all metrics exhibit strong sensitivity to visual grounding errors, their sensitivity to caption implausibility errors is limited. Furthermore, we found that all metrics are sensitive to variations in the size of image-relevant objects mentioned in the caption, while CLIPScore and PAC-S are also sensitive to the number of mentions of image-relevant objects in the caption. Regarding linguistic aspects of a caption, all metrics show weak comprehension of negat
    
[^263]: 在具有折扣自适应偏好的代理中的在线推荐问题

    Online Recommendations for Agents with Discounted Adaptive Preferences

    [https://arxiv.org/abs/2302.06014](https://arxiv.org/abs/2302.06014)

    本文研究了具有折扣自适应偏好的代理的在线推荐问题，通过在每一轮中展示一系列物品并考虑代理的偏好演变，以实现对于目标集合的最小化后悔。在长期记忆的情况下，可以实现对于能够在任何时刻实现的分布集合的高效次线性后悔。

    

    我们考虑了一个Bandit推荐问题，其中代理的偏好（代表对推荐物品的选择概率）根据过去的选择作为未知"偏好模型"的函数而演变。在每一轮中，我们向代理展示$k$个物品（共$n$个），代理选择一个物品，我们的目标是在代理的选择上对于某个"目标集合"（物品简单形式的子集）的对抗损失最小化后悔。我们扩展了Agarwal和Brown（2022）的设定，他们考虑了一种均匀记忆的代理，而在这里，我们允许非均匀记忆，在每一轮中对代理的记忆向量应用一个折扣因子。在"长期记忆"的情况下（当有效的记忆时程与$T$的次线性变化），我们证明了可以实现对于"能够在任何时刻实现的分布集合"（即"即时实现分布集合"）的高效次线性后悔。

    We consider a bandit recommendations problem in which an agent's preferences (representing selection probabilities over recommended items) evolve as a function of past selections, according to an unknown $\textit{preference model}$. In each round, we show a menu of $k$ items (out of $n$ total) to the agent, who then chooses a single item, and we aim to minimize regret with respect to some $\textit{target set}$ (a subset of the item simplex) for adversarial losses over the agent's choices. Extending the setting from Agarwal and Brown (2022), where uniform-memory agents were considered, here we allow for non-uniform memory in which a discount factor is applied to the agent's memory vector at each subsequent round. In the "long-term memory" regime (when the effective memory horizon scales with $T$ sublinearly), we show that efficient sublinear regret is obtainable with respect to the set of $\textit{everywhere instantaneously realizable distributions}$ (the "EIRD set", as formulated in pr
    
[^264]: 概念梯度：基于概念的解释不依赖于线性假设

    Concept Gradient: Concept-based Interpretation Without Linear Assumption

    [https://arxiv.org/abs/2208.14966](https://arxiv.org/abs/2208.14966)

    本文提出了一种概念梯度（CG）的方法，将基于概念的解释扩展到了非线性概念函数，并证明在玩具示例和真实世界数据集上，概念梯度（CG）优于概念激活向量（CAV）。

    

    基于概念的解释对于人类更易理解。目前最常用的基于概念的解释方法是概念激活向量（CAV）。CAV依赖于学习给定模型的某种潜在表示与概念之间的线性关系。通常隐含地假设线性可分，但并不总是成立。在这项工作中，我们从基于概念解释的原始目的出发，提出了概念梯度（CG），将基于概念的解释扩展到了非线性概念函数。我们证明了对于一般（可能非线性）的概念，我们可以数学上评估概念的微小变化如何影响模型的预测，从而将梯度解释扩展到了概念空间。我们在玩具示例和真实世界数据集上进行了实证，证明了概念梯度在性能上优于概念激活向量（CAV）。

    Concept-based interpretations of black-box models are often more intuitive for humans to understand. The most widely adopted approach for concept-based interpretation is Concept Activation Vector (CAV). CAV relies on learning a linear relation between some latent representation of a given model and concepts. The linear separability is usually implicitly assumed but does not hold true in general. In this work, we started from the original intent of concept-based interpretation and proposed Concept Gradient (CG), extending concept-based interpretation beyond linear concept functions. We showed that for a general (potentially non-linear) concept, we can mathematically evaluate how a small change of concept affecting the model's prediction, which leads to an extension of gradient-based interpretation to the concept space. We demonstrated empirically that CG outperforms CAV in both toy examples and real world datasets.
    
[^265]: ARIEL: 对抗图对比学习

    ARIEL: Adversarial Graph Contrastive Learning

    [https://arxiv.org/abs/2208.06956](https://arxiv.org/abs/2208.06956)

    该论文提出了对抗图对比学习（ARIEL）方法，通过引入对抗图视图进行数据增强，提取信息丰富的对比样本，进一步改进了图表示学习中的对比学习方法。

    

    对比学习是图表示学习中有效的无监督方法，对比学习的关键组成部分在于正负样本的构建。以前的方法通常利用图中节点之间的接近性作为原则。最近，基于数据增强的对比学习方法在视觉领域展现了很大的能力，并且一些工作将这种方法从图像扩展到了图中。然而，与图像上的数据增强不同，图上的数据增强不太直观，更难以提供高质量的对比样本，这给改进留下了很大的空间。在这项工作中，通过引入对抗图视图进行数据增强，我们提出了一种简单而有效的方法，对抗图对比学习（ARIEL），在合理的约束条件下提取信息丰富的对比样本。我们开发了一种名为信息正则化的技术，用于稳定训练，并使用s

    Contrastive learning is an effective unsupervised method in graph representation learning, and the key component of contrastive learning lies in the construction of positive and negative samples. Previous methods usually utilize the proximity of nodes in the graph as the principle. Recently, the data-augmentation-based contrastive learning method has advanced to show great power in the visual domain, and some works extended this method from images to graphs. However, unlike the data augmentation on images, the data augmentation on graphs is far less intuitive and much harder to provide high-quality contrastive samples, which leaves much space for improvement. In this work, by introducing an adversarial graph view for data augmentation, we propose a simple but effective method, Adversarial Graph Contrastive Learning (ARIEL), to extract informative contrastive samples within reasonable constraints. We develop a new technique called information regularization for stable training and use s
    
[^266]: 加强学习辅助的递归QAOA

    Reinforcement Learning Assisted Recursive QAOA

    [https://arxiv.org/abs/2207.06294](https://arxiv.org/abs/2207.06294)

    递归QAOA是一种非局部的QAOA变体，用于改善近似解的质量。本文通过识别和分析RQAOA失败的案例，提出了一种使用强化学习辅助的方法。

    

    近年来，诸如量子近似优化算法（QAOA）之类的变分量子算法因为它们有望利用NISQ设备解决困难的组合优化问题而变得流行。然而，已知在低深度下，QAOA的某些局部性约束限制了其性能。为了超越这些限制，提出了一种非局部的QAOA变体，即递归QAOA（RQAOA），以提高近似解的质量。RQAOA相对于QAOA的研究较少，人们对其了解较少，例如在哪些实例族上可能无法提供高质量的解。然而，由于我们处理的是NP难问题（具体而言是伊辛自旋模型），预计RQAOA会失败，这引发了设计更好的量子算法来解决组合优化问题的问题。在这种精神下，我们确定并分析了RQAOA失败的案例，并基于此提出了一种强化学习辅助的方法。

    Variational quantum algorithms such as the Quantum Approximation Optimization Algorithm (QAOA) in recent years have gained popularity as they provide the hope of using NISQ devices to tackle hard combinatorial optimization problems. It is, however, known that at low depth, certain locality constraints of QAOA limit its performance. To go beyond these limitations, a non-local variant of QAOA, namely recursive QAOA (RQAOA), was proposed to improve the quality of approximate solutions. The RQAOA has been studied comparatively less than QAOA, and it is less understood, for instance, for what family of instances it may fail to provide high quality solutions. However, as we are tackling $\mathsf{NP}$-hard problems (specifically, the Ising spin model), it is expected that RQAOA does fail, raising the question of designing even better quantum algorithms for combinatorial optimization. In this spirit, we identify and analyze cases where RQAOA fails and, based on this, propose a reinforcement le
    
[^267]: Hilbert曲线投影距离用于分布比较

    Hilbert Curve Projection Distance for Distribution Comparison

    [https://arxiv.org/abs/2205.15059](https://arxiv.org/abs/2205.15059)

    本研究提出了一种新的度量方法，称为Hilbert曲线投影(HCP)距离，用于测量两个概率分布之间的距离，并具有低复杂度。通过Hilbert曲线投影和运输距离计算，该方法适用于有界支撑的概率测度，并在高维空间中具有较好的收敛性能。为了解决维度灾难，还开发了两个HCP距离的变体，使用子空间投影。

    

    分布比较在数据分类和生成建模等许多机器学习任务中起着核心作用。本研究提出了一种新的度量方法，称为Hilbert曲线投影(HCP)距离，用于测量两个概率分布之间的距离，并具有低复杂度。具体而言，我们首先使用Hilbert曲线将两个高维概率分布投影到一起，得到它们之间的耦合，然后根据耦合在原始空间中计算这两个分布之间的运输距离。我们证明了HCP距离是一个适当的度量，并且对于有界支撑的概率测度是良定义的。此外，我们还证明了在$d$维空间中具有$L_p$成本的改进经验HCP距离以不超过$O(n^{-1/2\max\{d,p\}})$的速率收敛到其总体对应项。为了抑制维度灾难，我们还开发了两个HCP距离的变体，使用（可学习的）子空间投影。

    Distribution comparison plays a central role in many machine learning tasks like data classification and generative modeling. In this study, we propose a novel metric, called Hilbert curve projection (HCP) distance, to measure the distance between two probability distributions with low complexity. In particular, we first project two high-dimensional probability distributions using Hilbert curve to obtain a coupling between them, and then calculate the transport distance between these two distributions in the original space, according to the coupling. We show that HCP distance is a proper metric and is well-defined for probability measures with bounded supports. Furthermore, we demonstrate that the modified empirical HCP distance with the $L_p$ cost in the $d$-dimensional space converges to its population counterpart at a rate of no more than $O(n^{-1/2\max\{d,p\}})$. To suppress the curse-of-dimensionality, we also develop two variants of the HCP distance using (learnable) subspace pro
    
[^268]: 基于组公平性的贝叶斯最优分类器

    Bayes-Optimal Classifiers under Group Fairness

    [https://arxiv.org/abs/2202.09724](https://arxiv.org/abs/2202.09724)

    这篇论文提供了一个统一的框架，推导出在组公平性下的贝叶斯最优分类器，并提出了一种名为FairBayes的基于组的阈值方法，可以直接控制不公平现象，实现基本最优的公平性-准确性权衡。

    

    机器学习算法正越来越多地融入到高风险决策过程中，例如社会福利问题。由于需要减少算法预测可能造成的不平等影响，许多公平机器学习方法已经被提出。然而，在各种组公平性约束下刻画贝叶斯最优分类器的基本问题仅在一些特殊情况下进行了研究。本文基于经典的Neyman-Pearson假设检验理论（Neyman和Pearson，1933；Shao，2003），提供了一个统一框架来推导在组公平性下的贝叶斯最优分类器。这使我们能够提出一种基于组的阈值方法，称为FairBayes，可以直接控制不公平现象，并实现基本最优的公平性-准确性权衡。这些优势通过充分的实验支持。

    Machine learning algorithms are becoming integrated into more and more high-stakes decision-making processes, such as in social welfare issues. Due to the need of mitigating the potentially disparate impacts from algorithmic predictions, many approaches have been proposed in the emerging area of fair machine learning. However, the fundamental problem of characterizing Bayes-optimal classifiers under various group fairness constraints has only been investigated in some special cases. Based on the classical Neyman-Pearson argument (Neyman and Pearson, 1933; Shao, 2003) for optimal hypothesis testing, this paper provides a unified framework for deriving Bayes-optimal classifiers under group fairness. This enables us to propose a group-based thresholding method we call FairBayes, that can directly control disparity, and achieve an essentially optimal fairness-accuracy tradeoff. These advantages are supported by thorough experiments.
    
[^269]: 一次性联邦学习中非凸损失函数的最优界限

    Order Optimal Bounds for One-Shot Federated Learning over non-Convex Loss Functions

    [https://arxiv.org/abs/2108.08677](https://arxiv.org/abs/2108.08677)

    本论文研究了一次性联邦学习中非凸损失函数的最优界限，并提出了一个分布式学习算法(MRE-NC)，其期望损失与最优界限匹配。

    

    我们考虑在一次性设置下的联邦学习问题，其中有m台机器，每台机器从未知分布上的非凸损失函数中观测到n个样本函数。设F：[-1,1]^d → R是这个未知分布下的预期损失函数。目标是找到F的最小化估计。基于它的观测，每台机器产生一个长度有界为B的信号并将其发送到服务器。服务器收集所有机器的信号并输出F的最小化估计。我们证明，任何算法的期望损失下界为max(1/(\sqrt{n}(mB)^{1/d}), 1/\sqrt{mn})，除去对数因子。然后我们证明该下界在m和n上是次优的，通过提出一个分布式学习算法，称为多分辨率非凸损失函数估计器(MRE-NC)，其期望损失在大的mn时与下界匹配，除了多对数因子。

    We consider the problem of federated learning in a one-shot setting in which there are $m$ machines, each observing $n$ sample functions from an unknown distribution on non-convex loss functions. Let $F:[-1,1]^d\to\mathbb{R}$ be the expected loss function with respect to this unknown distribution. The goal is to find an estimate of the minimizer of $F$. Based on its observations, each machine generates a signal of bounded length $B$ and sends it to a server. The server collects signals of all machines and outputs an estimate of the minimizer of $F$. We show that the expected loss of any algorithm is lower bounded by $\max\big(1/(\sqrt{n}(mB)^{1/d}), 1/\sqrt{mn}\big)$, up to a logarithmic factor. We then prove that this lower bound is order optimal in $m$ and $n$ by presenting a distributed learning algorithm, called Multi-Resolution Estimator for Non-Convex loss function (MRE-NC), whose expected loss matches the lower bound for large $mn$ up to polylogarithmic factors.
    
[^270]: 通过随机行走的转换耦合比对和对齐有向网络

    Alignment and Comparison of Directed Networks via Transition Couplings of Random Walks

    [https://arxiv.org/abs/2106.07106](https://arxiv.org/abs/2106.07106)

    NetOTC是一种用于比较和对齐两个网络的方法，通过最小化随机行走的转换耦合的期望成本来量化网络之间的差异，并提供顶点和边的对齐。它捕捉到了关于网络的局部和全局信息，并保留了边缘。

    

    我们描述并研究了一种基于传输的过程，称为NetOTC（网络优化转换耦合），用于比较和对齐两个网络。所研究的网络可以是有向或无向的，带权重或不带权重，并且可能具有不同大小的不同顶点集。给定两个网络和一个与它们的顶点相关的成本函数，NetOTC找到其随机行走的转换耦合，使其具有最小的期望成本。最小化的成本量化了网络之间的差异，而最佳传输计划本身提供了两个网络的顶点和边的对齐。通过耦合完整的随机行走，而不是它们的边缘分布，确保NetOTC捕捉到了关于网络的局部和全局信息，并保留了边缘。NetOTC没有自由参数，并且不依赖随机化。我们研究了NetOTC的一些理论特性，并进行了一些实验证明其实际性能。

    We describe and study a transport based procedure called NetOTC (network optimal transition coupling) for the comparison and alignment of two networks. The networks of interest may be directed or undirected, weighted or unweighted, and may have distinct vertex sets of different sizes. Given two networks and a cost function relating their vertices, NetOTC finds a transition coupling of their associated random walks having minimum expected cost. The minimizing cost quantifies the difference between the networks, while the optimal transport plan itself provides alignments of both the vertices and the edges of the two networks. Coupling of the full random walks, rather than their marginal distributions, ensures that NetOTC captures local and global information about the networks, and preserves edges. NetOTC has no free parameters, and does not rely on randomization. We investigate a number of theoretical properties of NetOTC and present experiments establishing its empirical performance.
    
[^271]: IM-META: 使用节点元数据在未知拓扑网络中进行影响最大化

    IM-META: Influence Maximization Using Node Metadata in Networks With Unknown Topology

    [https://arxiv.org/abs/2106.02926](https://arxiv.org/abs/2106.02926)

    IM-META是一种在未知拓扑网络中进行影响最大化的方法，通过利用节点元数据和查询信息，通过学习元数据与边的关系、构建增强图以及使用拓扑感知排序策略来确定最具影响力的种子节点和查询节点。

    

    由于复杂网络的结构通常是未知的，我们可以通过探索底层网络的一部分来确定具有最大影响力的种子节点，给定节点查询的小预算。我们提出了IM-META，一种解决在未知拓扑网络中影响最大化（IM）问题的方法，通过从查询和节点元数据中检索信息。由于使用这样的元数据并不是没有风险，因为元数据的噪声特性和连通性推断的不确定性，我们制定了一个新的IM问题，旨在找到种子节点和查询节点。在IM-META中，我们开发了一个有效的方法，通过以下三个步骤迭代进行：1）我们通过孪生神经网络学习收集到的元数据与边的关系，2）我们选择一定数量的推断出的可信边来构建一个增强图，3）我们通过使用我们的拓扑感知排序策略，通过最大化推断的影响扩散来确定下一个要查询的节点。通过对IM-META的实验评估，我们证明了其有效性。

    Since the structure of complex networks is often unknown, we may identify the most influential seed nodes by exploring only a part of the underlying network, given a small budget for node queries. We propose IM-META, a solution to influence maximization (IM) in networks with unknown topology by retrieving information from queries and node metadata. Since using such metadata is not without risk due to the noisy nature of metadata and uncertainties in connectivity inference, we formulate a new IM problem that aims to find both seed nodes and queried nodes. In IM-META, we develop an effective method that iteratively performs three steps: 1) we learn the relationship between collected metadata and edges via a Siamese neural network, 2) we select a number of inferred confident edges to construct a reinforced graph, and 3) we identify the next node to query by maximizing the inferred influence spread using our topology-aware ranking strategy. Through experimental evaluation of IM-META on fou
    
[^272]: InstaHide混合两个私人图片时的样本复杂度

    InstaHide's Sample Complexity When Mixing Two Private Images

    [https://arxiv.org/abs/2011.11877](https://arxiv.org/abs/2011.11877)

    本文研究了对InstaHide的最新攻击，提出了一个统一的框架来理解和分析这些攻击。通过一种新算法，在InstaHide挑战设置下，以可证明的保证和最优样本复杂度恢复所有私人图片，并提供了关于检索所有InstaHide图片的计算难度结果。

    

    训练神经网络通常需要大量的敏感训练数据，如何保护训练数据的隐私已成为深度学习研究中的重要课题。InstaHide是一种最先进的用于保护训练数据隐私的方案，对测试准确性只有微小影响，并且其安全性已成为一个突出的问题。本文系统研究了对InstaHide的最新攻击，并提出了一个统一的框架来理解和分析这些攻击。我们发现现有的攻击要么没有可证明的保证，要么只能恢复一个私人图片。在当前的InstaHide挑战设置下，每个InstaHide图片是两个私人图片的混合，我们提出了一种新算法来以可证明的保证和最优样本复杂度恢复所有私人图片。此外，我们还提供了关于检索所有InstaHide图片的计算难度结果。我们的结果表明InstaHide在信息论上并非是安全的。

    Training neural networks usually require large numbers of sensitive training data, and how to protect the privacy of training data has thus become a critical topic in deep learning research. InstaHide is a state-of-the-art scheme to protect training data privacy with only minor effects on test accuracy, and its security has become a salient question. In this paper, we systematically study recent attacks on InstaHide and present a unified framework to understand and analyze these attacks. We find that existing attacks either do not have a provable guarantee or can only recover a single private image. On the current InstaHide challenge setup, where each InstaHide image is a mixture of two private images, we present a new algorithm to recover all the private images with a provable guarantee and optimal sample complexity. In addition, we also provide a computational hardness result on retrieving all InstaHide images. Our results demonstrate that InstaHide is not information-theoretically s
    
[^273]: 关于隐私保护和可验证ReLU网络的多项式逼近研究

    On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU Networks

    [https://arxiv.org/abs/2011.05530](https://arxiv.org/abs/2011.05530)

    本研究探讨了多项式逼近隐私保护和可验证ReLU网络的方法。通过实验证明，平方函数并不是最佳替代ReLU函数的二次多项式。我们提出了一个具有

    

    将深度神经网络（DNNs）的推理任务外包给不受信任的云，引发了数据隐私和完整性的担忧。虽然有许多技术可以确保基于多项式计算的隐私和完整性，但DNNs涉及非多项式计算。为了解决这些挑战，已经提出了几种基于将非多项式激活函数（如ReLU函数）替换为多项式激活函数的隐私保护和可验证推理技术。这些技术通常需要具有整数系数或有限域上的多项式。在这项工作中，我们通过实验证明，即使将多项式限制为具有整数系数，平方函数也不是能替代ReLU函数的最佳二次多项式。相反，我们提出了一个具有

    Outsourcing deep neural networks (DNNs) inference tasks to an untrusted cloud raises data privacy and integrity concerns. While there are many techniques to ensure privacy and integrity for polynomial-based computations, DNNs involve non-polynomial computations. To address these challenges, several privacy-preserving and verifiable inference techniques have been proposed based on replacing the non-polynomial activation functions such as the rectified linear unit (ReLU) function with polynomial activation functions. Such techniques usually require polynomials with integer coefficients or polynomials over finite fields. Motivated by such requirements, several works proposed replacing the ReLU function with the square function. In this work, we empirically show that the square function is not the best degree-2 polynomial that can replace the ReLU function even when restricting the polynomials to have integer coefficients. We instead propose a degree-2 polynomial activation function with a
    
[^274]: 学习领域转移的校准不确定性：一种分布鲁棒学习方法

    Learning Calibrated Uncertainties for Domain Shift: A Distributionally Robust Learning Approach

    [https://arxiv.org/abs/2010.05784](https://arxiv.org/abs/2010.05784)

    本论文提出了一种学习校准不确定性的方法，在领域转移下有效应用于无监督领域自适应和半监督学习等任务。通过可微的密度比估计器和分布鲁棒学习框架，我们生成了校准的不确定性，为下游任务提供了帮助。

    

    我们提出了一种在领域转移下学习校准不确定性的框架，其中源（训练）分布与目标（测试）分布不同。我们通过可微的密度比估计器检测这种领域转移，并与任务网络一起训练，构成一个关于领域转移的调整后的softmax预测形式。特别地，密度比估计反映了目标（测试）样本与源（训练）分布的接近程度。我们利用它来调整任务网络中的预测不确定性。这种使用密度比的想法基于分布鲁棒学习（DRL）框架，通过对抗性风险最小化考虑领域转移。我们证明了我们提出的方法生成的校准不确定性有助于下游任务，如无监督领域自适应（UDA）和半监督学习（SSL）。在这些任务中，像自我训练和FixMatch这样的方法使用不确定性进行选择。

    We propose a framework for learning calibrated uncertainties under domain shifts, where the source (training) distribution differs from the target (test) distribution. We detect such domain shifts via a differentiable density ratio estimator and train it together with the task network, composing an adjusted softmax predictive form concerning domain shift. In particular, the density ratio estimation reflects the closeness of a target (test) sample to the source (training) distribution. We employ it to adjust the uncertainty of prediction in the task network. This idea of using the density ratio is based on the distributionally robust learning (DRL) framework, which accounts for the domain shift by adversarial risk minimization. We show that our proposed method generates calibrated uncertainties that benefit downstream tasks, such as unsupervised domain adaptation (UDA) and semi-supervised learning (SSL). On these tasks, methods like self-training and FixMatch use uncertainties to select
    
[^275]: 基于大边际机制和伪查询集的跨域少样本学习

    Large Margin Mechanism and Pseudo Query Set on Cross-Domain Few-Shot Learning

    [https://arxiv.org/abs/2005.09218](https://arxiv.org/abs/2005.09218)

    本文提出了一种基于大边际机制和伪查询集的跨域少样本学习方法，通过生成伪查询图像，并借鉴人脸识别方法中的大边际机制对特征提取模块进行微调。实验证明，该方法在各个领域上都取得了显著的优势，展示了其鲁棒性和适应预训练模型到新领域的能力。

    

    近年来，少样本学习问题引起了广泛的关注。然而，大多数以前的方法都是在单一领域的数据集上进行训练和测试，跨域少样本学习是少样本学习问题的一个全新分支，其中模型在训练和测试阶段处理不同领域的数据集。在本文中，为了解决在单个数据集上进行预训练（元训练）而在包括普通物体、卫星图像和医学图像在内的四个不同领域的数据集上进行微调的问题，我们提出了一种新颖的大边际微调方法（LMM-PQS）。该方法通过支持图像生成伪查询图像，并借鉴人脸识别方法中的大边际机制对特征提取模块进行微调。根据实验结果，LMM-PQS在比基准模型上取得了显著的优势，并证明了我们的方法具有鲁棒性，并能够轻松地将预训练模型适应新的领域。

    In recent years, few-shot learning problems have received a lot of attention. While methods in most previous works were trained and tested on datasets in one single domain, cross-domain few-shot learning is a brand-new branch of few-shot learning problems, where models handle datasets in different domains between training and testing phases. In this paper, to solve the problem that the model is pre-trained (meta-trained) on a single dataset while fine-tuned on datasets in four different domains, including common objects, satellite images, and medical images, we propose a novel large margin fine-tuning method (LMM-PQS), which generates pseudo query images from support images and fine-tunes the feature extraction modules with a large margin mechanism inspired by methods in face recognition. According to the experiment results, LMM-PQS surpasses the baseline models by a significant margin and demonstrates that our approach is robust and can easily adapt pre-trained models to new domains w
    
[^276]: 高维度独立性检测: 通过最大和平均距离相关性

    High-Dimensional Independence Testing via Maximum and Average Distance Correlations

    [https://arxiv.org/abs/2001.01095](https://arxiv.org/abs/2001.01095)

    本文介绍并研究了利用最大和平均距离相关性进行高维度独立性检测的方法，并提出了一种快速卡方检验的程序。该方法适用于欧氏距离和高斯核，具有较好的实证表现和广泛的应用场景。

    

    本文介绍并研究了利用最大和平均距离相关性进行多元独立性检测的方法。我们在高维环境中表征了它们相对于边际相关维度数量的一致性特性，评估了每个检验统计量的优势，检查了它们各自的零分布，并提出了一种基于快速卡方检验的检测程序。得出的检验是非参数的，并适用于欧氏距离和高斯核作为底层度量。为了更好地理解所提出的测试的实际使用情况，我们在各种多元相关场景中评估了最大距离相关性、平均距离相关性和原始距离相关性的实证表现，同时进行了一个真实数据实验，以检测人类血浆中不同癌症类型和肽水平的存在。

    This paper introduces and investigates the utilization of maximum and average distance correlations for multivariate independence testing. We characterize their consistency properties in high-dimensional settings with respect to the number of marginally dependent dimensions, assess the advantages of each test statistic, examine their respective null distributions, and present a fast chi-square-based testing procedure. The resulting tests are non-parametric and applicable to both Euclidean distance and the Gaussian kernel as the underlying metric. To better understand the practical use cases of the proposed tests, we evaluate the empirical performance of the maximum distance correlation, average distance correlation, and the original distance correlation across various multivariate dependence scenarios, as well as conduct a real data experiment to test the presence of various cancer types and peptide levels in human plasma.
    
[^277]: 时序数据的独立性检验

    Independence Testing for Temporal Data

    [https://arxiv.org/abs/1908.06486](https://arxiv.org/abs/1908.06486)

    本文介绍了一种适用于测试时序数据之间独立性的时序依赖统计方法，并能够估计最佳依赖滞后。该方法解决了现有方法的限制，并且在测试平稳时间序列之间的独立性时渐近有效和普遍一致，并且与多种依赖度量方法兼容。

    

    时序数据在现代数据科学中越来越常见。一个基本问题是判断两个时间序列是否相关。现有方法常常存在限制，如依赖参数假设、仅检测线性关联、需要多个测试和修正等。虽然最近提出了许多非参数和普遍一致的依赖度量方法，但直接应用于时序数据可能导致p值膨胀和无效的检验。为了解决这些挑战，本文引入了基于块置换的时序依赖统计量来测试时序数据之间的独立性。在适当的假设下，所提出的方法在测试平稳时间序列之间的独立性时是渐近有效和普遍一致的，并且能够估计最大化依赖的最佳依赖滞后。值得注意的是，它与丰富的距离和核心依赖度量方法兼容，消除了

    Temporal data are increasingly prevalent in modern data science. A fundamental question is whether two time-series are related or not. Existing approaches often have limitations, such as relying on parametric assumptions, detecting only linear associations, and requiring multiple tests and corrections. While many non-parametric and universally consistent dependence measures have recently been proposed, directly applying them to temporal data can inflate the p-value and result in invalid test. To address these challenges, this paper introduces the temporal dependence statistic with block permutation to test independence between temporal data. Under proper assumptions, the proposed procedure is asymptotically valid and universally consistent for testing independence between stationary time-series, and capable of estimating the optimal dependence lag that maximizes the dependence. Notably, it is compatible with a rich family of distance and kernel based dependence measures, eliminates the
    
[^278]: SimFair：物理引导的公平感知学习与模拟模型

    SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models. (arXiv:2401.15270v1 [cs.LG])

    [http://arxiv.org/abs/2401.15270](http://arxiv.org/abs/2401.15270)

    SimFair是一种物理引导的公平感知学习框架，通过集成模拟和逆向建模来解决纯数据驱动的公平性问题，有效保持公平性。

    

    公平感知已经成为在真实应用中负责任使用人工智能的关键基础。在许多情况下，性能不平等是由于不同区域分布的变化引起的。虽然已经开发出提高公平可迁移性的技术，但是在没有来自新区域的样本的情况下解决这个问题并不总是可行的，这对于纯数据驱动的尝试是一个瓶颈。幸运的是，基于物理机制模型已经在许多具有重大社会影响的问题上进行了研究。我们提出了SimFair，一种物理引导的公平感知学习框架，通过集成基于物理规则的模拟和逆向建模到训练设计中来弥补数据限制。以温度预测为例，我们演示了所提出的SimFair在保持公平性方面的有效性。

    Fairness-awareness has emerged as an essential building block for the responsible use of artificial intelligence in real applications. In many cases, inequity in performance is due to the change in distribution over different regions. While techniques have been developed to improve the transferability of fairness, a solution to the problem is not always feasible with no samples from the new regions, which is a bottleneck for pure data-driven attempts. Fortunately, physics-based mechanistic models have been studied for many problems with major social impacts. We propose SimFair, a physics-guided fairness-aware learning framework, which bridges the data limitation by integrating physical-rule-based simulation and inverse modeling into the training design. Using temperature prediction as an example, we demonstrate the effectiveness of the proposed SimFair in fairness preservation.
    
[^279]: 定位无关源免域自适应学习预测太阳能发电

    Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar Power Generation. (arXiv:2401.14422v1 [cs.LG])

    [http://arxiv.org/abs/2401.14422](http://arxiv.org/abs/2401.14422)

    提出了一种基于深度学习的域自适应框架，利用气候特征预测太阳能发电量，在不同地区间具有一定的通用性和适应性。

    

    太阳能发电的预测是一项具有挑战性的任务，因为它依赖于呈现空间和时间变异性的气候特征。预测模型的性能可能因数据分布的变化而在不同地区产生变化，导致一个在某个地区工作良好但在其他地区不起作用的模型。此外，由于全球变暖，每年天气模式的变化在加速。这种现象可能导致现有模型在同一地理区域内随时间推移而效果减弱。本文提出了一种基于深度学习的域自适应框架，利用可以解决上述挑战的气候特征来估计太阳能发电量。采用前馈深度卷积网络模型来在已知位置数据集上进行有监督训练，并用于后续预测未知位置的太阳能发电量。

    The prediction of solar power generation is a challenging task due to its dependence on climatic characteristics that exhibit spatial and temporal variability. The performance of a prediction model may vary across different places due to changes in data distribution, resulting in a model that works well in one region but not in others. Furthermore, as a consequence of global warming, there is a notable acceleration in the alteration of weather patterns on an annual basis. This phenomenon introduces the potential for diminished efficacy of existing models, even within the same geographical region, as time progresses. In this paper, a domain adaptive deep learning-based framework is proposed to estimate solar power generation using weather features that can solve the aforementioned challenges. A feed-forward deep convolutional network model is trained for a known location dataset in a supervised manner and utilized to predict the solar power of an unknown location later. This adaptive da
    
[^280]: MTRGL：通过多模态时间关系图学习有效辨识时间相关性

    MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning. (arXiv:2401.14199v1 [cs.LG])

    [http://arxiv.org/abs/2401.14199](http://arxiv.org/abs/2401.14199)

    该论文引入了一种新的框架——多模态时间关系图学习（MTRGL），通过将时间序列数据和离散特征结合成一个时间图，并采用记忆机制的图神经网络来辨识实体间的时间相关性，取得了在实证实验中的优异表现。这一研究对于提升自动化配对交易策略具有重要意义。

    

    本研究探索了深度学习与金融市场应用之间的协同作用，重点关注配对交易。这种市场中性策略对量化金融至关重要，并且适用于先进的深度学习技术。配对交易中的一个关键挑战是辨识实体之间的时间相关性，这要求整合多样化的数据模态。为了应对这一挑战，我们介绍了一种新的框架，即多模态时间关系图学习（MTRGL）。MTRGL将时间序列数据和离散特征结合到一个时间图中，并采用了基于记忆的时间图神经网络。这种方法将时间相关性识别重新定义为一个时间图链路预测任务，取得了实证成功。我们在真实数据集上的实验验证了MTRGL的卓越性能，强调其在优化自动化配对交易策略方面的潜力。

    In this study, we explore the synergy of deep learning and financial market applications, focusing on pair trading. This market-neutral strategy is integral to quantitative finance and is apt for advanced deep-learning techniques. A pivotal challenge in pair trading is discerning temporal correlations among entities, necessitating the integration of diverse data modalities. Addressing this, we introduce a novel framework, Multi-modal Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and discrete features into a temporal graph and employs a memory-based temporal graph neural network. This approach reframes temporal correlation identification as a temporal graph link prediction task, which has shown empirical success. Our experiments on real-world datasets confirm the superior performance of MTRGL, emphasizing its promise in refining automated pair trading strategies.
    
[^281]: 大规模异构图上基于大型语言模型的链接预测的可扩展性研究

    Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])

    [http://arxiv.org/abs/2401.13227](http://arxiv.org/abs/2401.13227)

    本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。

    

    探索将大规模语言模型应用于图学习是一项新颖的努力。然而，大图中蕴含的大量信息给这一过程带来了重大挑战。本文侧重于链接预测任务，并介绍了LPNL（Link Prediction via Natural Language），这是一个基于大型语言模型的框架，用于大规模异构图上的可扩展链接预测。我们设计了能以自然语言表达图细节的创新提示语。我们提出了一个两阶段的采样流程，从大规模异构图中提取关键信息，并采用分而治之的策略来控制输入令牌数量在预定限制内，解决了信息过载的挑战。我们还通过自监督学习设计了一个用于链接预测的T5模型进行微调。在大型公共异构图上进行的广泛实验表明，LPNL的性能超过了各种先进的基准模型。

    Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
    
[^282]: 从理解的角度探讨语言模型的关键数据规模

    Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])

    [http://arxiv.org/abs/2401.10463](http://arxiv.org/abs/2401.10463)

    本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。

    

    我们探讨了语言模型中的关键数据规模，这是从快速记忆到缓慢泛化的一个基本转变阈值。我们将这个相变形式化为Grokking配置下的数据效率假说，并确定了语言模型训练动力学中的数据不足、充足和过剩阶段。我们通过重新调整初始化和权重衰减，开发出了一种Grokking配置，稳定地在简化的语言模型上重现了Grokking。我们表明只有当语言模型达到关键大小时才会发生泛化。我们分析了样本级和模型级的Grokking，验证了提出的数据效率假说。我们的实验揭示了语言数据集的关键数据集大小处发生的更平滑的相变。随着模型大小的增加，这个临界点也变得更大，这表明更大的模型需要更多的数据。我们的研究结果加深了对语言模型训练的理解，提供了一种新颖的视角。

    We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
    
[^283]: 走向基于原则的图形变换器

    Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])

    [http://arxiv.org/abs/2401.10119](http://arxiv.org/abs/2401.10119)

    边缘变换器是一个全局注意力模型，它具有至少3-WL的表达能力，能够在预测性能上超过其他架构，而不依赖于位置或结构编码。

    

    基于k维Weisfeiler-Leman（k-WL）层次结构的图形学习架构提供了理论上很好理解的表达能力。然而，这样的架构在真实任务中往往无法提供可靠的预测性能，从而限制了它们的实际影响力。相比之下，基于全局注意力的模型如图形变换器在实践中表现出了强大的性能，但是将它们的表达能力与k-WL层次结构进行比较仍然具有挑战性，尤其是因为这些架构依赖于位置或结构编码来实现其表达能力和预测性能。为了解决这个问题，我们展示了最近提出的边缘变换器，这是一个在节点对而不是节点上进行操作的全局注意力模型，具有至少3-WL的表达能力。经验上，我们证明了边缘变换器在预测性能上超过了其他理论对齐的架构，同时不依赖于位置或结构编码。

    Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.
    
[^284]: 深度和宽度在神经ODE插值中的相互作用

    Interplay between depth and width for interpolation in neural ODEs. (arXiv:2401.09902v1 [math.OC])

    [http://arxiv.org/abs/2401.09902](http://arxiv.org/abs/2401.09902)

    本文研究了神经ODE插值中深度和宽度之间的相互作用，并发现在数据集插值中存在着$p$和$L$之间的平衡折衷关系，而在测度插值中，$L$的增长与$p$和$\varepsilon$的关系有关。

    

    神经常微分方程(neural ODEs)已经成为从控制角度进行监督学习的自然工具，然而对其最佳结构的完全理解仍然难以捉摸。在这项工作中，我们研究了宽度$p$和层之间的过渡次数$L$（实际上是深度$L+1$）之间的相互作用。具体来说，我们评估了模型的表达能力，以其能够在Wasserstein误差边界$\varepsilon>0$内插值一个包含$N$对点的有限数据集$D$或两个概率测度在$\mathbb{R}^d$中。我们的发现揭示了$p$和$L$之间的平衡折衷关系，在数据集插值中，$L$随着$O(1+N/p)$的比例增长，而在测度插值中，$L=O\left(1+(p\varepsilon^d)^{-1}\right)$。在自主情况下，$L=0$，需要进行单独的研究，我们将重点关注数据集插值。我们解决了$\varepsilon$-近似控制性的放松问题，并建立了误差

    Neural ordinary differential equations (neural ODEs) have emerged as a natural tool for supervised learning from a control perspective, yet a complete understanding of their optimal architecture remains elusive. In this work, we examine the interplay between their width $p$ and number of layer transitions $L$ (effectively the depth $L+1$). Specifically, we assess the model expressivity in terms of its capacity to interpolate either a finite dataset $D$ comprising $N$ pairs of points or two probability measures in $\mathbb{R}^d$ within a Wasserstein error margin $\varepsilon>0$. Our findings reveal a balancing trade-off between $p$ and $L$, with $L$ scaling as $O(1+N/p)$ for dataset interpolation, and $L=O\left(1+(p\varepsilon^d)^{-1}\right)$ for measure interpolation.  In the autonomous case, where $L=0$, a separate study is required, which we undertake focusing on dataset interpolation. We address the relaxed problem of $\varepsilon$-approximate controllability and establish an error 
    
[^285]: 大规模语言模型的极端压缩通过加性量化

    Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])

    [http://arxiv.org/abs/2401.06118](http://arxiv.org/abs/2401.06118)

    本文提出的算法在大规模语言模型的极端压缩方面取得了较好的性能，相比最新技术，在给定的压缩预算下准确性更高。

    

    准确的开源大规模语言模型(LLMs)的出现引发了对这些模型进行量化技术的竞赛，从而使其能够在最终用户设备上执行。在本文中，我们从多码本量化(MCQ)的经典方法角度重新思考了“极端”LLM压缩的问题，即针对非常低的位数，例如每个参数2到3位。我们的工作建立在加性量化这一经典算法之上，并将其适应于语言模型的量化。由此得到的算法在LLM压缩方面推进了最新技术，以给定压缩预算的准确性而言，优于所有最近提出的技术。例如，当将Llama 2模型压缩到每个参数2位时，我们的算法将7B模型量化为6.93困惑度(相对于之前最佳工作改进1.29，相对于FP16改进1.81)，13B模型量化为5.70困惑度(改进0.36)，70B模型量化为3.94困惑度。

    The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
    
[^286]: 大流行的前100天；药物、行为和数字干预的相互作用——基于Agent-Based模型的研究。

    First 100 days of pandemic; an interplay of pharmaceutical, behavioral and digital interventions -- A study using agent based modeling. (arXiv:2401.04795v1 [cs.MA])

    [http://arxiv.org/abs/2401.04795](http://arxiv.org/abs/2401.04795)

    本研究通过Agent-Based模型模拟了药物、行为和数字干预的相互作用，并建议综合运用这些干预措施应对大流行疫情。通过分析发现，最初的100天对决定疫情发展至关重要，强调了迅速决策和高效政策制定的重要性。

    

    大流行，特别是最近的COVID-19疫情爆发，对公共卫生和全球经济产生了影响。因此，我们需要深入了解疾病发展趋势和高效的应对策略，以应对潜在的未来疫情爆发。本文强调了代理人模型（Agent-Based Models，ABM）在捕捉复杂的感染动态和理解干预措施影响的潜力。我们模拟了反映现实政策采纳中的挑战的真实药物、行为和数字干预，并提出了这些干预的整体组合用于大流行疫情应对。通过这些模拟，我们根据华盛顿州金斯县的真实社会人口统计数据和地理普查数据，研究了大规模人口的新兴行为趋势。我们的分析揭示了最初100天在决定大流行疫情走势方面的关键作用，强调了快速决策和高效政策制定的重要性。

    Pandemics, notably the recent COVID-19 outbreak, have impacted both public health and the global economy. A profound understanding of disease progression and efficient response strategies is thus needed to prepare for potential future outbreaks. In this paper, we emphasize the potential of Agent-Based Models (ABM) in capturing complex infection dynamics and understanding the impact of interventions. We simulate realistic pharmaceutical, behavioral, and digital interventions that mirror challenges in real-world policy adoption and suggest a holistic combination of these interventions for pandemic response. Using these simulations, we study the trends of emergent behavior on a large-scale population based on real-world socio-demographic and geo-census data from Kings County in Washington. Our analysis reveals the pivotal role of the initial 100 days in dictating a pandemic's course, emphasizing the importance of quick decision-making and efficient policy development. Further, we highligh
    
[^287]: 以核Fisher-Rao流进行单位时间采样

    Sampling in Unit Time with Kernel Fisher-Rao Flow. (arXiv:2401.03892v1 [stat.CO])

    [http://arxiv.org/abs/2401.03892](http://arxiv.org/abs/2401.03892)

    本文提出了一种具有核Fisher-Rao流的新方法，在单位时间内从非归一化目标密度或贝叶斯后验中进行采样。方法使用了均场ODE和相互作用粒子系统，无需梯度，只需要能够从参考密度中采样并计算目标对参考密度的比率。该方法通过在几何混合的路径上沿速度场运输样本，径向输运样本。方法通过在再生核希尔伯特空间中求解泊松方程，使泊松方程的求解变得可行，并将其离散化为有限样本的均场ODE，作为实现简单的相互作用粒子系统。同时，这种方法也可以从离散时间的角度推导出均场ODE，作为蒙杰-安普尔方程连续线性化的极限。

    

    我们引入了一种新的均场ODE和相应的相互作用粒子系统，用于从非归一化的目标密度或贝叶斯后验中进行采样。相互作用粒子系统无需梯度，可以闭合形式获得，并且只需要能够从参考密度中采样并计算（非归一化的）目标对参考密度的比率。通过求解运输样本沿两个密度的几何混合的速度场的泊松方程来获得均场ODE，这是一种特定的Fisher-Rao梯度流的路径。我们采用再生核希尔伯特空间方法来获得速度场的泊松方程，这使得泊松方程可处理，并使我们能够离散化有限样本的结果均场ODE，形成一个简单的相互作用粒子系统。均场ODE还可以通过离散时间视角从蒙杰-安普尔方程的连续线性化的极限中推导出来，这在一个已知的框架内进行。

    We introduce a new mean-field ODE and corresponding interacting particle systems for sampling from an unnormalized target density or Bayesian posterior. The interacting particle systems are gradient-free, available in closed form, and only require the ability to sample from the reference density and compute the (unnormalized) target-to-reference density ratio. The mean-field ODE is obtained by solving a Poisson equation for a velocity field that transports samples along the geometric mixture of the two densities, which is the path of a particular Fisher-Rao gradient flow. We employ a reproducing kernel Hilbert space ansatz for the velocity field, which makes the Poisson equation tractable and enables us to discretize the resulting mean-field ODE over finite samples, as a simple interacting particle system. The mean-field ODE can be additionally be derived from a discrete-time perspective as the limit of successive linearizations of the Monge-Amp\`ere equations within a framework known 
    
[^288]: 在样本高效的离线强化学习中：数据多样性、后验采样，以及更多

    On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG])

    [http://arxiv.org/abs/2401.03301](http://arxiv.org/abs/2401.03301)

    本文提出了通过数据多样性概念来统一离线强化学习算法的方法，并证明了基于版本空间、正则化优化和后验采样的算法在标准假设下达到了可比的样本效率。

    

    我们试图理解什么促进了对于序贝叶斯决策的历史数据集进行样本高效学习，这个问题通常被称为离线强化学习（RL）。此外，我们对于在利用（值）函数逼近的同时享受样本效率的算法感兴趣。在本文中，我们通过提出一个包括离线RL中覆盖度量的先前概念的数据多样性概念来解决这些基本问题，并且利用这个概念将基于版本空间（VS）、正则化优化（RO）和后验采样（PS）的三个不同类别的离线RL算法进行统一。我们在标准假设下证明，基于VS、基于RO和基于PS的算法达到了\emph{可比}的样本效率，这恢复了在有限和线性模型类别下的最优性的标准假设的边界。这个结果令人惊讶，因为之前的研究表明这些算法不具有有利性的样本效率。

    We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sa
    
[^289]: 基于轨迹导向的稀疏奖励策略优化

    Trajectory-Oriented Policy Optimization with Sparse Rewards. (arXiv:2401.02225v1 [cs.LG])

    [http://arxiv.org/abs/2401.02225](http://arxiv.org/abs/2401.02225)

    该论文提出了一种基于轨迹导向的稀疏奖励策略优化方法，通过利用离线示范轨迹，在稀疏奖励环境中实现更快速、更高效的在线强化学习。

    

    深度强化学习(DRL)在稀疏奖励任务中仍然具有挑战性。这些稀疏奖励通常只表示任务是否部分或完全完成，这意味着在代理获得有用反馈之前必须执行许多探索动作。因此，大多数现有的DRL算法无法在合理的时间内学习可行的策略。为了解决这个问题，我们开发了一种利用离线示范轨迹的方法，在稀疏奖励环境中进行更快速和更高效的在线强化学习。我们的关键见解是，通过将离线示范轨迹视为指导而不是模仿它们，我们的方法学习了一种使状态-动作访问分布与离线示范相匹配的策略。具体来说，我们引入了一种基于最大均值差异(MMD)的轨迹距离，并将策略优化建模为一个受距离约束的优化问题。

    Deep reinforcement learning (DRL) remains challenging in tasks with sparse rewards. These sparse rewards often only indicate whether the task is partially or fully completed, meaning that many exploration actions must be performed before the agent obtains useful feedback. Hence, most existing DRL algorithms fail to learn feasible policies within a reasonable time frame. To overcome this problem, we develop an approach that exploits offline demonstration trajectories for faster and more efficient online RL in sparse reward settings. Our key insight is that by regarding offline demonstration trajectories as guidance, instead of imitating them, our method learns a policy whose state-action visitation marginal distribution matches that of offline demonstrations. Specifically, we introduce a novel trajectory distance based on maximum mean discrepancy (MMD) and formulate policy optimization as a distance-constrained optimization problem. Then, we show that this distance-constrained optimizat
    
[^290]: 无界损失的PAC-Bayes-Chernoff界限

    PAC-Bayes-Chernoff bounds for unbounded losses. (arXiv:2401.01148v1 [stat.ML])

    [http://arxiv.org/abs/2401.01148](http://arxiv.org/abs/2401.01148)

    这篇论文提出了一种用于无界损失的高概率PAC-Bayes参考界限，并通过优化自由参数解决了一些开放问题，并通过灵活的假设产生了新的广义界限。

    

    我们提出了一种新的用于无界损失的高概率PAC-Bayes参考界限。这个结果可以理解为Chernoff界限的PAC-Bayes版本。证明技巧依赖于通过Cramér变换对损失进行统一边界的尾部随机变量。我们强调了我们主要结果的两个应用。首先，我们证明了我们的界限解决了许多PAC-Bayes界限上的自由参数优化的开放问题。最后，我们证明了我们的方法允许在损失函数上进行灵活的假设，从而产生了广义了之前的界限，并且可以通过最小化来获得类似Gibbs的后验概率。

    We present a new high-probability PAC-Bayes oracle bound for unbounded losses. This result can be understood as a PAC-Bayes version of the Chernoff bound. The proof technique relies on uniformly bounding the tail of certain random variable based on the Cram\'er transform of the loss. We highlight two applications of our main result. First, we show that our bound solves the open problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we show that our approach allows working with flexible assumptions on the loss function, resulting in novel bounds that generalize previous ones and can be minimized to obtain Gibbs-like posteriors.
    
[^291]: 面向基于强化学习的药物调整系统以减少言语不流畅的论文翻译

    Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.11509](http://arxiv.org/abs/2312.11509)

    这个论文介绍了一种基于强化学习的系统，该系统可以根据患者言语不流畅程度自动调整药物，通过对药物组合的强化学习算法的优化，能够收敛到良好的用药方案。

    

    我们提出了一种基于强化学习的系统，该系统可以自动为患有与心理健康相关的言语不流畅的虚拟患者开具药物处方，并根据零成本频繁测量结果，调整药物和剂量。我们展示了系统的两个组成部分：一个在我们构建的大型数据集上检测和评估言语不流畅的模块，以及一个可以自动找到良好药物组合的强化学习算法。为了支持这两个模块，我们从文献中收集了关于药物治疗言语不流畅的效果的数据，并建立了一个可信的患者模拟系统。我们证明了在某些情况下，强化学习系统能够收敛到一个良好的用药方案。我们收集并对可能存在言语不流畅的人群进行了数据标注，并使用该数据集演示了我们的方法。我们的工作是一个概念验证:

    We propose a Reinforcement-Learning-based system that would automatically prescribe a hypothetical patient medication that may help the patient with their mental-health-related speech disfluency, and adjust the medication and the dosages in response to zero-cost frequent measurement of the fluency of the patient. We demonstrate the components of the system: a module that detects and evaluates speech disfluency on a large dataset we built, and a Reinforcement Learning algorithm that automatically finds good combinations of medications. To support the two modules, we collect data on the effect of psychiatric medications for speech disfluency from the literature, and build a plausible patient simulation system. We demonstrate that the Reinforcement Learning system is, under some circumstances, able to converge to a good medication regime. We collect and label a dataset of people with possible speech disfluency and demonstrate our methods using that dataset. Our work is a proof of concept:
    
[^292]: TiMix: 文本感知图像混合用于有效的视觉语言预训练

    TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.08846](http://arxiv.org/abs/2312.08846)

    TiMix是一种将文本感知的图像混合技术用于视觉语言预训练的方法，通过集成混合数据增强技术，并从互信息的角度进行理论分析，提高了数据效率并取得了可比较的性能。

    

    自监督的多模态对比学习（SMCL）通过对齐视觉和语言模态，显著推进了现代视觉语言预训练（VLP）模型的发展。然而，由于网络收集的文本-图像对中存在噪声，扩大SMCL的训练数据量在计算成本和数据效率方面面临着相当大的障碍。为了提高VLP的数据效率，我们提出了文本感知图像混合（TiMix），将基于混合的数据增强技术集成到SMCL中，显著提升了性能，而不会显著增加计算开销。我们从互信息（MI）的角度对TiMix进行了理论分析，表明跨模态对比学习的混合数据样本隐式地作为对比损失的正则化器。实验结果表明，即使使用较少的训练数据和较短的训练时间，TiMix在下游任务上表现出可比较的性能。

    Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when be
    
[^293]: 用简单的功率分析在32位微控制器上阅读神经网络架构

    Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers. (arXiv:2311.01344v1 [cs.CR])

    [http://arxiv.org/abs/2311.01344](http://arxiv.org/abs/2311.01344)

    本文研究了如何通过简单的功率分析方法，在32位微控制器上提取深度神经网络模型的架构信息。

    

    模型提取是AI系统安全的一个不断增长的关注点。对于深度神经网络模型来说，架构是对手试图恢复的最重要的信息。作为一系列重复计算块，部署在边缘设备上的神经网络模型将产生独特的侧信道泄露。当目标平台在物理上可访问时，可以利用这些信道泄露来提取关键信息。通过结合对深度学习实践的理论知识和对广泛使用的实现库（ARM CMSIS-NN）的分析，我们的目的是回答这个关键问题：通过简单地检查一个EM侧信道跟踪，我们可以提取多远的架构信息？我们首次提出了一种用于传统MLP和CNN模型的提取方法，这些模型在高端32位微控制器（Cortex-M7）上运行，仅依赖于简单的模式识别分析。尽管存在几个具有挑战性的情况，但我们声称，与参数提取相反，我们可以通过这种方法从简单的功率分析中提取出架构信息。

    Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction
    
[^294]: 用于临床特征嵌入的语言模型训练范式

    Language Model Training Paradigms for Clinical Feature Embeddings. (arXiv:2311.00768v1 [cs.LG])

    [http://arxiv.org/abs/2311.00768](http://arxiv.org/abs/2311.00768)

    本研究使用自监督训练范式的语言模型，通过表示学习为临床时间序列推导出高质量的通用临床特征嵌入。通过无监督的降维技术可视化学习到的嵌入，并在MIMIC-III基准测试中验证了它们的有效性。

    

    在数据稀缺的研究领域，表示学习起着重要的作用。本研究旨在通过对临床时间序列进行表示学习，推导出临床特征（如心率和血压）的通用嵌入。我们使用语言模型的自监督训练范式，学习高质量的临床特征嵌入，实现比现有的时间步和患者级别表示学习更细粒度的表征。我们通过无监督的降维技术可视化学习到的嵌入，并观察到与先前的临床知识高度一致。我们还在MIMIC-III基准测试上评估模型性能，并展示了使用临床特征嵌入的有效性。我们将我们的代码发布在网上以供复制。

    In research areas with scarce data, representation learning plays a significant role. This work aims to enhance representation learning for clinical time series by deriving universal embeddings for clinical features, such as heart rate and blood pressure. We use self-supervised training paradigms for language models to learn high-quality clinical feature embeddings, achieving a finer granularity than existing time-step and patient-level representation learning. We visualize the learnt embeddings via unsupervised dimension reduction techniques and observe a high degree of consistency with prior clinical knowledge. We also evaluate the model performance on the MIMIC-III benchmark and demonstrate the effectiveness of using clinical feature embeddings. We publish our code online for replication.
    
[^295]: 揭示偏见和不平等：利用电子健康记录的医疗人工智能中偏见检测和缓解的系统综述

    Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records. (arXiv:2310.19917v1 [cs.AI])

    [http://arxiv.org/abs/2310.19917](http://arxiv.org/abs/2310.19917)

    本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。

    

    目的：利用电子健康记录的人工智能应用在医疗领域越来越受到欢迎，但也引入了各种类型的偏见。本研究旨在系统综述涉及利用电子健康记录数据的人工智能研究中的偏见。方法：遵循Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA)准则进行了系统综述。从PubMed、Web of Science和电气和电子工程师学会中检索了2010年1月1日至2022年10月31日期间发表的文章。我们定义了六种主要的偏见类型，并总结了现有的偏见处理方法。结果：在检索到的252篇文章中，有20篇符合最终综述的纳入标准。本综述涵盖了六种偏见中的五种：八项研究分析了选择偏见；六项研究针对隐性偏见；五项研究对混杂偏见进行了研究；四项研究对测量偏见进行了研究；两项研究对算法偏见进行了研究。在偏见处理方法方面，有十项研究进行了探讨。

    Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten st
    
[^296]: 使用人设来建模语言模型中的真实性

    Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])

    [http://arxiv.org/abs/2310.18168](http://arxiv.org/abs/2310.18168)

    本研究探讨了在大型语言模型中使用人设来建模真实性的可能性。通过建模真实人设，语言模型可以将真实性推广到不同上下文中，并通过相关特征判断个体产生文本的真实性。

    

    大型语言模型使用互联网上的大量文本进行训练，这些文本中既包含了事实，也包含了误导性的信息。语言模型能够从这些相互矛盾的数据中辨别真实与虚假吗？基于语言模型能够建模不同产生文本的个体这一观点，我们假设它们可以通过建模真实人设来聚类真实文本：一群很可能产生真实文本并具有相似特征的个体。例如，可信源如维基百科和科学期刊通常使用正式的写作风格并提出一致的主张。通过建模这一人设，语言模型可以将真实性推广到每个个体生成训练文本的特定上下文之外。例如，模型可以推断出“维基百科”这个个体在“科学”生成的主题上会表现出真实性，因为它们共享一个人设。我们首先通过两个观察结果为人设假设提供了证据：（1）我们可以探测模型在不同领域中判断真实性的能力；（2）模型可以从相关特征中推测个体产生文本的真实性。

    Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
    
[^297]: LASER：无线分布式优化中的线性压缩

    LASER: Linear Compression in Wireless Distributed Optimization. (arXiv:2310.13033v1 [cs.NE])

    [http://arxiv.org/abs/2310.13033](http://arxiv.org/abs/2310.13033)

    LASER是一种新的压缩方案，通过利用梯度的低秩结构在噪声通道上高效传输梯度，相对于现有方案在计算机视觉和GPT语言建模任务上表现出持续优势。

    

    数据并行SGD是分布式优化的事实上的算法，尤其适用于大规模机器学习。尽管它有很多优点，但通信瓶颈是其中持久存在的问题之一。大多数压缩方案要么假设通信链路无噪声，要么在实际任务中无法取得良好的性能。在本文中，我们填补了这一空白，介绍了LASER：无线分布式优化中的线性压缩。LASER利用梯度的固有低秩结构，在噪声通道上高效传输梯度。尽管享受与经典SGD相似的理论保证，LASER在各种实际基准测试中表现出持续的优势。特别是，在具有挑战性的计算机视觉和GPT语言建模任务中，它优于最先进的压缩方案。在后者中，我们相对于噪声通道上的基准模型在困惑度上获得了50-64%的提升。

    Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64 \%$ improvement in perplexity over our baselines for noisy channels.
    
[^298]: 用Wasserstein距离进行简化表示学习的实证研究

    An Empirical Study of Simplicial Representation Learning with Wasserstein Distance. (arXiv:2310.10143v1 [stat.ML])

    [http://arxiv.org/abs/2310.10143](http://arxiv.org/abs/2310.10143)

    本文研究了在树结构上利用Wasserstein距离进行简化表示学习的问题，并提出了一种基于SimCLR和负TWD的自监督学习方法来估计简化表示，通过实证研究找到了稳定的训练策略。

    

    本文探讨了在树结构上利用1-Wasserstein距离进行简化表示学习的问题，其中树- Wasserstein距离(TWD)定义为两个树嵌入向量之间的L1距离。具体而言，我们考虑了一种基于SimCLR和负TWD作为相似度度量的自监督学习方法来估计简化表示。在SimCLR中，通常使用与实向量嵌入的余弦相似度，但是尚未对利用L1距离与简化嵌入进行深入研究。一个关键挑战是训练L1距离在数值上具有挑战性，并且往往会产生不令人满意的结果，概率模型的选择也有很多。因此，本研究从实证角度探究了用TWD优化自监督学习的策略，并找到了稳定的训练过程。更具体地说，我们评估了两种类型TWD的组合（总 ...

    In this paper, we delve into the problem of simplicial representation learning utilizing the 1-Wasserstein distance on a tree structure (a.k.a., Tree-Wasserstein distance (TWD)), where TWD is defined as the L1 distance between two tree-embedded vectors. Specifically, we consider a framework for simplicial representation estimation employing a self-supervised learning approach based on SimCLR with a negative TWD as a similarity measure. In SimCLR, the cosine similarity with real-vector embeddings is often utilized; however, it has not been well studied utilizing L1-based measures with simplicial embeddings. A key challenge is that training the L1 distance is numerically challenging and often yields unsatisfactory outcomes, and there are numerous choices for probability models. Thus, this study empirically investigates a strategy for optimizing self-supervised learning with TWD and find a stable training procedure. More specifically, we evaluate the combination of two types of TWD (total
    
[^299]: 关于通过指数机制进行高维私有模型选择的计算复杂性

    On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism. (arXiv:2310.07852v1 [stat.ML])

    [http://arxiv.org/abs/2310.07852](http://arxiv.org/abs/2310.07852)

    本文研究了在高维稀疏线性回归模型中的差分隐私模型选择问题。我们使用指数机制进行模型选择，并提出了Metropolis-Hastings算法来克服指数搜索空间的计算复杂性。我们的算法在一定边界条件下能够实现强模型恢复性质，并具有多项式混合时间和近似差分隐私性质。

    

    在差分隐私框架下，我们考虑了高维稀疏线性回归模型中的模型选择问题。具体而言，我们考虑了差分隐私最佳子集选择的问题，并研究了其效用保证。我们采用了广为人知的指数机制来选择最佳模型，并在一定边界条件下，建立了其强模型恢复性质。然而，指数机制的指数搜索空间导致了严重的计算瓶颈。为了克服这个挑战，我们提出了Metropolis-Hastings算法来进行采样步骤，并在问题参数$n$、$p$和$s$中建立了其到稳态分布的多项式混合时间。此外，我们还利用其混合性质建立了Metropolis-Hastings随机行走的最终估计的近似差分隐私性质。最后，我们还进行了一些说明性模拟，印证了我们主要结果的理论发现。

    We consider the problem of model selection in a high-dimensional sparse linear regression model under the differential privacy framework. In particular, we consider the problem of differentially private best subset selection and study its utility guarantee. We adopt the well-known exponential mechanism for selecting the best model, and under a certain margin condition, we establish its strong model recovery property. However, the exponential search space of the exponential mechanism poses a serious computational bottleneck. To overcome this challenge, we propose a Metropolis-Hastings algorithm for the sampling step and establish its polynomial mixing time to its stationary distribution in the problem parameters $n,p$, and $s$. Furthermore, we also establish approximate differential privacy for the final estimates of the Metropolis-Hastings random walk using its mixing property. Finally, we also perform some illustrative simulations that echo the theoretical findings of our main results
    
[^300]: 潜在提示Transformer模型在分子设计中的应用

    Molecule Design by Latent Prompt Transformer. (arXiv:2310.03253v1 [cs.LG])

    [http://arxiv.org/abs/2310.03253](http://arxiv.org/abs/2310.03253)

    本文提出了一种潜在提示Transformer模型，用于解决分子设计中的优化问题。该模型包括潜在向量、分子生成模型和性质预测模型，通过对现有分子进行训练后进行模型分布的逐渐转移。

    

    本文提出了一种用于解决分子设计等具有挑战性优化问题的潜在提示Transformer模型，其中目标是找到具有目标化学或生物性质最优值的分子，该值可以由现有软件计算得出。我们提出的模型包括三个组件：（1）潜在向量，其先验分布由高斯白噪声向量的Unet变换建模。（2）分子生成模型，在（1）中给定潜在向量的条件下生成基于字符串的分子表示。我们采用了以（1）中的潜在向量作为提示的因果Transformer模型。（3）性质预测模型，根据（1）中的潜在向量进行非线性回归预测分子的目标性质值。我们称该提出的模型为潜在提示Transformer模型。在对现有分子及其性质值进行初步训练后，我们逐渐转移模型分布的学习。

    This paper proposes a latent prompt Transformer model for solving challenging optimization problems such as molecule design, where the goal is to find molecules with optimal values of a target chemical or biological property that can be computed by an existing software. Our proposed model consists of three components. (1) A latent vector whose prior distribution is modeled by a Unet transformation of a Gaussian white noise vector. (2) A molecule generation model that generates the string-based representation of molecule conditional on the latent vector in (1). We adopt the causal Transformer model that takes the latent vector in (1) as prompt. (3) A property prediction model that predicts the value of the target property of a molecule based on a non-linear regression on the latent vector in (1). We call the proposed model the latent prompt Transformer model. After initial training of the model on existing molecules and their property values, we then gradually shift the model distributi
    
[^301]: 通过扩散学习实现目标达成

    Learning to Reach Goals via Diffusion. (arXiv:2310.02505v1 [cs.LG])

    [http://arxiv.org/abs/2310.02505](http://arxiv.org/abs/2310.02505)

    本论文提出了一种通过扩散学习实现目标达成的方法，可以在任意初始状态下从预定义或新目标达成，而无需学习单独的价值函数。

    

    扩散模型是一类强大的生成模型，能够通过迭代去噪将高维空间中的随机噪声映射到目标流形上。在本研究中，我们通过将目标条件强化学习框架放在扩散建模的背景下，给出了一种新的视角。类似于扩散过程，其中利用高斯噪声创建随机轨迹，使其远离数据流形，我们构造了远离潜在目标状态的轨迹。然后我们学习一个类似于评分函数的目标条件策略。这个称为Merlin的方法能够在任意初始状态下从预定义或新目标达成，而无需学习单独的价值函数。我们考虑了三种选择，用于取代扩散中的高斯噪声模型 - 缓冲区中的反向播放，反向动力学模型和一种新的非参数方法。我们在离线目标达成任务上理论上证明了我们的方法，并对其进行了验证。

    Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks.
    
[^302]: OceanGPT：用于海洋科学任务的大型语言模型

    OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])

    [http://arxiv.org/abs/2310.02031](http://arxiv.org/abs/2310.02031)

    OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。

    

    海洋科学是探索充满生命和生物多样性的海洋的科学，考虑到海洋覆盖了地球表面的70％以上，这一领域具有重要意义。最近，大型语言模型（LLM）的进展改变了科学的范式。尽管在其他领域取得了成功，但现有的LLM通常无法满足海洋学家等领域专家的需求，同时对LLM在海洋科学中的潜力尚未得到充分探索。这其中的根本原因可能是海洋数据的庞大而复杂的性质，以及对更高的粒度和丰富的知识的需求。为了解决这些问题，我们推出了首个海洋领域的LLM——OceanGPT，该模型擅长各种海洋科学任务。我们提出了一个新颖的框架DoInstruct，用于自动获取大量的海洋领域指导数据，它基于多智能体的协作生成指导。

    Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
    
[^303]: 基于能量导向的连续熵巴氏中心估计方法及其在一般成本问题中的应用

    Energy-Guided Continuous Entropic Barycenter Estimation for General Costs. (arXiv:2310.01105v1 [cs.LG])

    [http://arxiv.org/abs/2310.01105](http://arxiv.org/abs/2310.01105)

    本文提出了一种基于能量导向的方法用于近似计算任意OT成本函数的连续熵OT巴氏中心，该方法具有优越的性能，并且能与基于能量的模型（EBMs）学习过程无缝连接。

    

    优化输运（OT）巴氏中心是一种在捕捉概率分布几何特性的同时对其进行平均的数学方法。本文提出了一种新颖的算法，用于近似计算任意OT成本函数的连续熵OT巴氏中心。我们的方法基于最近在机器学习社区中受到关注的基于弱OT的连续熵最优输运问题的对偶重构。除了创新性之外，我们的方法还具有以下若干优势特点：（i）我们建立了对恢复解的质量界限；（ii）该方法与基于能量的模型（EBMs）学习过程无缝连接，可以使用经过良好调整的算法解决感兴趣的问题；（iii）它提供了一种直观的优化方案，避免使用极小-极大、强化等复杂技巧。为了验证我们的方法，我们考虑了s

    Optimal transport (OT) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. In short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given OT discrepancies. We propose a novel algorithm for approximating the continuous Entropic OT (EOT) barycenter for arbitrary OT cost functions. Our approach is built upon the dual reformulation of the EOT problem based on weak OT, which has recently gained the attention of the ML community. Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seemlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. For validation, we consider s
    
[^304]: 图神经网络能否作为最优近似算法？

    Are Graph Neural Networks Optimal Approximation Algorithms?. (arXiv:2310.00526v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00526](http://arxiv.org/abs/2310.00526)

    本文设计了图神经网络架构OptGNN，利用半定规划工具获得大类组合优化问题的最优近似算法。通过实证结果表明在各种数据集上超过了神经网络基线算法和传统算法，同时利用OptGNN的能力设计了一个产生优化的对偶证书的算法。

    

    在这项工作中，我们设计了能够使用半定规划（SDP）强大的算法工具来获得大类组合优化问题的最优近似算法的图神经网络架构。具体而言，我们证明了多项式大小的消息传递算法可以表示最强大的多项式时间算法，前提是假设唯一游戏猜想成立。我们利用这一结果构建了高效的图神经网络架构OptGNN，它在诸如最大割和最大独立集等重要组合优化问题上获得了高质量的近似解。我们的方法在各种真实世界和合成数据集上表现出强大的实证结果，不仅超过了神经网络基线算法，还超过了传统算法。最后，我们利用OptGNN捕捉凸松弛的能力，设计了一个产生优化的对偶证书（确定性上界）的算法。

    In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on
    
[^305]: 增强随机平滑的Lipschitz-方差-边界权衡

    The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing. (arXiv:2309.16883v1 [cs.LG])

    [http://arxiv.org/abs/2309.16883](http://arxiv.org/abs/2309.16883)

    本文提出了一个增强随机平滑的方法，通过研究随机平滑引入的方差与分类器的Lipschitz常数和边界之间的关系，以及采用单纯形投影技术来增加认证鲁棒半径。

    

    面对噪声输入和对抗性攻击时，深度神经网络的实际应用受到其不稳定的预测的阻碍。在这种情况下，认证半径是模型鲁棒性的关键指标。然而，如何设计一个具有足够认证半径的高效分类器呢？随机平滑通过在输入中注入噪声来获得平滑且更鲁棒的分类器的框架提供了有希望的解决方案。本文首先展示了随机平滑引入的方差与分类器的另外两个重要属性，即其Lipschitz常数和边界之间的密切关系。更具体地说，我们的工作强调了基分类器的Lipschitz常数对平滑分类器和经验方差的双重影响。此外，为了增加认证鲁棒半径，我们引入了一种不同的单纯形投影技术，以便通过Bernst的方差-边界权衡来利用基分类器。

    Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernst
    
[^306]: P-ROCKET: 针对时间序列分类的随机卷积核剪枝

    P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification. (arXiv:2309.08499v1 [cs.LG])

    [http://arxiv.org/abs/2309.08499](http://arxiv.org/abs/2309.08499)

    本研究提出了一种名为P-ROCKET的方法，通过在特征选择的角度删除卷积核，从而实现对时间序列分类中的随机卷积核进行剪枝。

    

    在最近几年，两个时间序列分类模型ROCKET和MINIROCKET因其低训练成本和最先进的准确性而受到广泛关注。ROCKET和MINIROCKET利用无需训练的随机一维卷积核，可以快速从时间序列数据中提取特征，从而实现线性分类器的高效拟合。然而，为了全面捕捉有用的特征，需要大量的随机卷积核，这对于资源受限的设备来说是不兼容的。因此，我们设计了一种启发式进化算法S-ROCKET，用于识别和剪枝冗余的卷积核。然而，进化算法本身的特性导致在S-ROCKET中评估卷积核是一个耗时的过程。本文中，与直接评估具有非显著差异的随机卷积核的S-ROCKET不同，我们从特征选择的角度删除卷积核，通过消除序列中的相关连接来实现。

    In recent years, two time series classification models, ROCKET and MINIROCKET, have attracted much attention for their low training cost and state-of-the-art accuracy. Utilizing random 1-D convolutional kernels without training, ROCKET and MINIROCKET can rapidly extract features from time series data, allowing for the efficient fitting of linear classifiers. However, to comprehensively capture useful features, a large number of random kernels are required, which is incompatible for resource-constrained devices. Therefore, a heuristic evolutionary algorithm named S-ROCKET is devised to recognize and prune redundant kernels. Nevertheless, the inherent nature of evolutionary algorithms renders the evaluation of kernels within S-ROCKET an unacceptable time-consuming process. In this paper, diverging from S-ROCKET, which directly evaluates random kernels with nonsignificant differences, we remove kernels from a feature selection perspective by eliminating associating connections in the sequ
    
[^307]: 带有Beta散度的深度非负矩阵分解

    Deep Nonnegative Matrix Factorization with Beta Divergences. (arXiv:2309.08249v1 [cs.LG])

    [http://arxiv.org/abs/2309.08249](http://arxiv.org/abs/2309.08249)

    本文提出了一种使用Beta散度的深度非负矩阵分解方法，应用于面部特征提取、文档主题识别和高光谱图像材料识别。

    

    深度非负矩阵分解（deep NMF）最近成为一种有价值的技术，用于在不同尺度上提取多层特征。然而，所有现有的深度NMF模型和算法主要都以最小二乘误差为评估标准，这可能不是评估多样化数据集近似质量的最合适指标。例如，当处理音频信号和文档等数据类型时，广泛认可的是$\beta$-divergences提供了更适合的替代方案。本文基于$\beta$-divergences开发了新的深度NMF模型和算法，并将这些技术应用于面部特征提取、文档集合中的主题识别以及高光谱图像中材料的识别。

    Deep Nonnegative Matrix Factorization (deep NMF) has recently emerged as a valuable technique for extracting multiple layers of features across different scales. However, all existing deep NMF models and algorithms have primarily centered their evaluation on the least squares error, which may not be the most appropriate metric for assessing the quality of approximations on diverse datasets. For instance, when dealing with data types such as audio signals and documents, it is widely acknowledged that $\beta$-divergences offer a more suitable alternative. In this paper, we develop new models and algorithms for deep NMF using $\beta$-divergences. Subsequently, we apply these techniques to the extraction of facial features, the identification of topics within document collections, and the identification of materials within hyperspectral images.
    
[^308]: DECODE: 基于历史数据和环境因素的数据驱动能耗预测在建筑中的应用

    DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings. (arXiv:2309.02908v1 [cs.LG])

    [http://arxiv.org/abs/2309.02908](http://arxiv.org/abs/2309.02908)

    本论文介绍了一种基于历史数据、占用模式和天气条件的LSTM模型，用于准确预测建筑能耗，该模型在预测精度上表现出卓越性能。

    

    建筑中的能耗预测在有效的能源管理中起着至关重要的作用。精确的预测对于实现优化的能耗和电网分配是必要的。本论文引入了一种基于历史能源数据、占用模式和天气条件的长短期记忆（LSTM）模型，用于预测建筑能耗。与现有的预测模型相比，LSTM模型提供了准确的短期、中期和长期能耗预测，适用于住宅和商业建筑。我们将我们的LSTM模型与线性回归、决策树和随机森林等已有的预测方法进行了比较。令人鼓舞的是，提出的LSTM模型在所有指标上表现出优越性能。它具有出色的预测精度，R2得分为0.97，最低的平均绝对误差（MAE）为0.007。我们开发的模型的另一个优点是能够实现高效的能耗。

    Energy prediction in buildings plays a crucial role in effective energy management. Precise predictions are essential for achieving optimal energy consumption and distribution within the grid. This paper introduces a Long Short-Term Memory (LSTM) model designed to forecast building energy consumption using historical energy data, occupancy patterns, and weather conditions. The LSTM model provides accurate short, medium, and long-term energy predictions for residential and commercial buildings compared to existing prediction models. We compare our LSTM model with established prediction methods, including linear regression, decision trees, and random forest. Encouragingly, the proposed LSTM model emerges as the superior performer across all metrics. It demonstrates exceptional prediction accuracy, boasting the highest R2 score of 0.97 and the most favorable mean absolute error (MAE) of 0.007. An additional advantage of our developed model is its capacity to achieve efficient energy consu
    
[^309]: 多途径适配器：为可扩展的图像-文本检索调整大规模多模态模型

    MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval. (arXiv:2309.01516v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.01516](http://arxiv.org/abs/2309.01516)

    多途径适配器是一个创新的框架，利用"对齐增强器"加深模态对齐，实现高可转移性，可有效减少调整参数的时间并提高零样本图像-文本检索性能。

    

    随着大规模多模态模型（LMMs）的规模不断增加，将这些预训练模型调整到专门的任务上已成为一个计算和内存密集的挑战。传统的微调方法需要为每个新任务进行孤立、穷举的重新调整，限制了模型的多功能性。此外，当前的高效调整技术经常忽视模态对齐，仅关注新任务的知识提取。为了解决这些问题，我们引入了多途径适配器，这是一个创新的框架，它包含了一个“对齐增强器”，可以加深模态对齐，实现高度的可转移性而无需调整预训练参数。我们的方法仅向LMMs添加了不到1.25%的额外参数，以BEiT-3模型为例。与完全微调的模型相比，我们的方法在零样本图像-文本检索性能上具有优势，同时缩短了高达57%的微调时间。我们的方法提供了一种资源高效和高效的方法。

    As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25\% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57\% reduction in fine-tuning time. Our approach offers a resource-efficient and ef
    
[^310]: 透过偏好看大型语言模型的反馈获取：揭示对齐的重要性

    Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])

    [http://arxiv.org/abs/2308.15812](http://arxiv.org/abs/2308.15812)

    本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。

    

    大型语言模型（LLMs）与人类价值观和意图的对齐承诺涉及使用人工智能或人类反馈。稠密的反馈注释获取和整合成本较高，而稀疏的反馈则涉及结构性设计选择，即评分（例如，在1-7的范围内对回答A进行评分）和排名（例如，回答A是否比回答B更好？）。在这项工作中，我们分析了这种设计选择对LLMs的对齐和评估的影响。我们发现，评分和排名所推断出的偏好在人类和AI注释者中都存在严重的不一致问题，达到了60%。我们的后续分析确定了解释这个现象的各种注释者偏见方面，比如人类注释者更喜欢密集回答并在两个选项之间更青睐准确性。令我们惊讶的是，我们还观察到反馈协议的选择对对齐的LLMs的评估也有显著影响。特别是，我们发现LLMs的评估结果因为反馈协议的选择而有所不同。

    Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
    
[^311]: 基于贝叶斯低秩适应的大型语言模型

    Bayesian low-rank adaptation for large language models. (arXiv:2308.13111v1 [cs.LG])

    [http://arxiv.org/abs/2308.13111](http://arxiv.org/abs/2308.13111)

    本研究提出了一种名为Laplace-LoRA的贝叶斯方法，通过应用拉普拉斯近似来增强经过微调的大型语言模型的校准能力。

    

    参数高效的微调（PEFT）已成为大型语言模型（LLMs）成本高效微调的新范式，其中低秩适应（LoRA）被广泛采用。然而，经过微调的LLMs往往变得过于自信，尤其是在较小数据集上进行微调时。贝叶斯方法具有估计不确定性的固有能力，可作为减轻过度自信并增强校准能力的有力工具。在这项工作中，我们引入了Laplace-LoRA，一种直观而有效的贝叶斯方法，它将拉普拉斯近似应用于LoRA参数，并显著提升了经过微调的LLMs的校准能力。

    Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially on when fine-tuned on smaller datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, a straightforward yet effective Bayesian method, which applies the Laplace approximation to the LoRA parameters and, considerably boosts the calibration of fine-tuned LLMs.
    
[^312]: 使用层次结构距离捕捉多层次图结构的变压器

    Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])

    [http://arxiv.org/abs/2308.11129](http://arxiv.org/abs/2308.11129)

    本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。

    

    图变压器需要强大的归纳偏差来得出有意义的注意力分数。然而，当前的提议很少涉及捕捉更长距离、层次结构或社区结构的方法，而这些在分子、社交网络和引用网络等各种图形中都会出现。在本文中，我们提出了一种层次距离结构编码（HDSE）方法，用于建模图中节点之间的层次距离，重点关注其多层次、层次化的性质。特别是，这产生了一个可以灵活与现有图变压器集成的框架，可以与其他位置表示同时应用。通过在12个真实世界数据集上进行大量实验，我们证明了我们的HDSE方法成功提升了各种类型的基线变压器，在10个基准数据集上获得了最先进的实证性能。

    Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
    
[^313]: 想法图：用大型语言模型解决复杂问题

    Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.09687](http://arxiv.org/abs/2308.09687)

    想法图（GoT）是一种新的框架，它超越了现有的提示范式，通过将大型语言模型（LLM）的信息建模为任意图形，将LLM想法组合成具有协同效应的结果，提炼整个思维网络的本质，或者使用反馈环路增强思维。GoT在不同任务上展示出优势，并可以通过新的想法转换进行扩展，使LLM的推理更接近人类思维。

    

    我们介绍了一种名为想法图（Graph of Thoughts，GoT）的框架，它在大型语言模型（LLM）的提示能力上超越了Chain-of-Thought或Tree of Thoughts（ToT）等范式。GoT的关键思想和主要优势在于能够将LLM生成的信息建模为任意图形，其中信息单元（"LLM想法"）是顶点，边表示这些顶点之间的依赖关系。这种方法使得将任意LLM想法组合成具有协同效应的结果、提炼整个思维网络的本质或者使用反馈环路增强思维成为可能。我们证明GoT在不同任务上比最先进的方法有优势，例如在排序任务上质量提高了62%，同时成本降低了超过31%。我们确保GoT能够通过新的想法转换进行扩展，从而可以用于开创新的提示方案。这项工作使得LLM的推理更接近人类思维。

    We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
    
[^314]: 自然语言是图表所需要的全部内容

    Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07134](http://arxiv.org/abs/2308.07134)

    本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。

    

    大规模预训练语言模型的出现，如ChatGPT，已经在人工智能的各个研究领域中引起了革命。基于Transformer的大型语言模型（LLMs）逐渐取代了CNN和RNN，将计算机视觉和自然语言处理领域统一起来。与相对独立存在的数据（如图像、视频或文本）相比，图表是一种包含丰富结构和关系信息的数据类型。同时，作为最具表现力的媒介之一，自然语言在描述复杂结构方面表现出色。然而，将图表学习问题纳入生成式语言建模框架的现有工作仍然非常有限。随着大型语言模型的重要性不断增长，探索LLMs是否也可以替代GNNs成为图表的基础模型变得至关重要。在本文中，我们提出了InstructGLM（结构化语言模型）算法，系统地设计高度可扩展的模型来处理图表学习问题。

    The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
    
[^315]: GIT-Mol：一种多模态大型语言模型用于分子科学中的图像，图形和文本

    GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text. (arXiv:2308.06911v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06911](http://arxiv.org/abs/2308.06911)

    GIT-Mol是一种多模态大型语言模型，可在分子科学中处理图像、图形和文本信息。通过新提出的GIT-Former架构，该模型能够将多种模态的数据对齐到一个统一的潜在空间中。与基线相比，GIT-Mol在性质预测和分子生成有效性方面取得了显著改进。此外，该模型还可用于化合物名称识别和化学反应预测等下游任务。

    

    大型语言模型在自然语言处理方面取得了重要进展，通过处理分子的文本表示，为分子科学中的创新应用提供了可能。然而，大多数现有的语言模型无法捕捉具有复杂分子结构或图像的丰富信息。在本文中，我们引入了GIT-Mol，一种集成了图形、图像和文本信息的多模态大型语言模型。为了促进多模态分子数据的集成，我们提出了GIT-Former，一种新颖的架构，能够将所有模态对齐到统一的潜在空间中。与基线相比，我们在性质预测方面实现了5%-10%的准确性提高，并在分子生成有效性方面提高了20.2%。通过任意到语言的分子翻译策略，我们的模型有潜力进行更多的下游任务，例如化合物名称识别和化学反应预测。

    Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties prediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.
    
[^316]: SITTA: 一种用于图像描述的语义图像文本对齐方法

    SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])

    [http://arxiv.org/abs/2307.05591](http://arxiv.org/abs/2307.05591)

    SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。

    

    对图像的文本和语义理解对于生成适当的描述非常重要。这需要检测图像中的对象，建模它们之间的关系，评估场景的语义，并将提取的知识表示在语言空间中。为了在保证良好的图像-语言映射的同时实现丰富的语言能力，预训练的语言模型（LMs）被条件化为预训练的多模态（图像-文本）模型，允许使用图像输入。这要求将多模态模型的视觉编码器中检测到的语义与生成性LM的语言表示进行对齐。然而，如何最好地将视觉编码器检测到的语义传递给LM还不清楚。我们介绍了两种构建线性映射的新方法，成功地将两个预训练模型的嵌入空间之间的语义转移。第一种方法是将多模态语言编码器的嵌入空间与生成性LM的嵌入空间进行对齐。

    Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
    
[^317]: 可证明高效的UCB类型算法用于学习预测状态表示

    Provably Efficient UCB-type Algorithms For Learning Predictive State Representations. (arXiv:2307.00405v1 [cs.LG])

    [http://arxiv.org/abs/2307.00405](http://arxiv.org/abs/2307.00405)

    这篇论文提出了第一种已知的UCB类型方法用于学习预测状态表示（PSRs），并设计了一个新的奖励项来上界t

    

    一般的顺序决策问题旨在通过基于过去观察和行动的历史来最大化累积奖励。最近的研究表明，如果顺序决策问题可以用预测状态表示（PSRs）建模低秩结构，那么它是可统计学习的。尽管有这些进展，但现有方法通常需要使用预先设计好的步骤或者是计算效率低下的或者是不可计算的。另一方面，上限置信区间（UCB）方法在赌博机和MDPs中被成功地作为计算效率高的方法，但对PSR这种更具挑战性的问题还没有进行研究，这是由于在这种更具挑战性的情况下，乐观型奖励的设计十分困难。本文提出了PSRs的第一种已知的UCB类型方法，其中包含了一个新的奖励项来上界t

    The general sequential decision-making problem, which includes Markov decision processes (MDPs) and partially observable MDPs (POMDPs) as special cases, aims at maximizing a cumulative reward by making a sequence of decisions based on a history of observations and actions over time. Recent studies have shown that the sequential decision-making problem is statistically learnable if it admits a low-rank structure modeled by predictive state representations (PSRs). Despite these advancements, existing approaches typically involve oracles or steps that are not computationally efficient. On the other hand, the upper confidence bound (UCB) based approaches, which have served successfully as computationally efficient methods in bandits and MDPs, have not been investigated for more general PSRs, due to the difficulty of optimistic bonus design in these more challenging settings. This paper proposes the first known UCB-type approach for PSRs, featuring a novel bonus term that upper bounds the t
    
[^318]: 竞争环境下贝叶斯风险的提高可能导致社会福利的降低

    Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition. (arXiv:2306.14670v1 [cs.GT])

    [http://arxiv.org/abs/2306.14670](http://arxiv.org/abs/2306.14670)

    本文研究了机器学习模型在竞争环境下的行为，发现提高数据表示质量可能会导致供应商整体预测准确性降低，从而降低社会福利。

    

    随着机器学习模型规模的增长，缩放定律等趋势预计会导致预测准确性的持续改进。然而，这些趋势只考虑了单个模型供应商的视角，而实际上供应商之间常常竞争用户。本文证明了竞争可以从根本上改变这些缩放趋势的行为，甚至可能造成整体预测准确性随着规模的增大而非单调或降低。我们定义了一个分类任务的竞争模型，并使用数据表示作为研究规模增加的影响的镜头。我们发现在一家市场上，改善数据表示质量（按贝叶斯风险计量）可能会降低竞争模型供应商的整体预测准确性（即社会福利）。我们的例子涵盖了简单设置中的封闭式公式到预训练的 CIFAR-10 模拟。

    As the scale of machine learning models increases, trends such as scaling laws anticipate consistent downstream improvements in predictive accuracy. However, these trends take the perspective of a single model-provider in isolation, while in reality providers often compete with each other for users. In this work, we demonstrate that competition can fundamentally alter the behavior of these scaling trends, even causing overall predictive accuracy across users to be non-monotonic or decreasing with scale. We define a model of competition for classification tasks, and use data representations as a lens for studying the impact of increases in scale. We find many settings where improving data representation quality (as measured by Bayes risk) decreases the overall predictive accuracy across users (i.e., social welfare) for a marketplace of competing model-providers. Our examples range from closed-form formulas in simple settings to simulations with pretrained representations on CIFAR-10. At
    
[^319]: 使用激活优化进行特洛伊模型检测

    Trojan Model Detection Using Activation Optimization. (arXiv:2306.04877v1 [cs.CV])

    [http://arxiv.org/abs/2306.04877](http://arxiv.org/abs/2306.04877)

    本文提出了一种新颖的特洛伊模型检测方法，通过激活优化为模型创建签名，然后训练分类器来检测特洛伊模型。该方法在两个公共数据集上实现了最先进的性能。

    

    由于数据的不可用性或大规模，以及训练机器学习模型的高计算和人力成本，通常会在可能的情况下依赖于开源预训练模型。但是，从安全的角度来看，这种做法非常令人担忧。预训练模型可能会被感染特洛伊攻击，在这种攻击中，攻击者嵌入一个触发器在模型中，使得当触发器存在于输入中时，攻击者可以控制模型的行为。本文提出了一种新颖的特洛伊模型检测方法的初步工作。我们的方法根据激活优化为模型创建签名。然后训练分类器来检测特洛伊模型并给出其签名。我们的方法在两个公共数据集上实现了最先进的性能。

    Due to data's unavailability or large size, and the high computational and human labor costs of training machine learning models, it is a common practice to rely on open source pre-trained models whenever possible. However, this practice is worry some from the security perspective. Pre-trained models can be infected with Trojan attacks, in which the attacker embeds a trigger in the model such that the model's behavior can be controlled by the attacker when the trigger is present in the input. In this paper, we present our preliminary work on a novel method for Trojan model detection. Our method creates a signature for a model based on activation optimization. A classifier is then trained to detect a Trojan model given its signature. Our method achieves state of the art performance on two public datasets.
    
[^320]: 高维和置换不变异常检测。

    High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])

    [http://arxiv.org/abs/2306.03933](http://arxiv.org/abs/2306.03933)

    该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。

    

    由于学习高维概率密度的困难，新物理过程的异常检测方法通常局限于低维空间。特别是在成分级别上，将置换不变性和可变长度的输入等良好性质合并到流行的密度估计方法中变得更加困难。在本研究中，我们引入了一种基于扩散模型的粒子物理数据置换不变密度估计器，专门设计用于处理可变长度的输入。我们通过将学习到的密度用作置换不变的异常检测评分来展示我们方法的功效，有效地识别出在仅具备背景假设下的可能性较低的喷注。为了验证我们的密度估计方法，我们研究了学习到的密度比与被监督分类算法获得的密度之间的比较。

    Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
    
[^321]: 通用等变Transformer：用于3D分子相互作用学习

    Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning. (arXiv:2306.01474v1 [cs.LG])

    [http://arxiv.org/abs/2306.01474](http://arxiv.org/abs/2306.01474)

    本文提出了一种通用等变Transformer用于学习3D分子相互作用，该模型具有双层注意力模块、前馈模块和层归一化模块，每个模块都是E（3）等变的，可以有效地捕捉块级和原子级的交互，实验结果表明其在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面优于各种最先进的方法。

    

    生物学和药物开发中的许多过程涉及不同分子之间的各种3D相互作用，例如蛋白质与蛋白质，蛋白质与小分子等。设计一个通用模型来学习普适的分子相互作用具有重要价值，但也具有挑战性，因为不同的分子通常以不同粒度表示。本文首先提出了将3D分子通用表示为集合的几何图形图，与传统单层表示形式形成对比。在提出的统一表示下，我们提出了通用等变Transformer（GET），以有效地捕捉稀疏块级和密集原子级交互。具体而言，GET由双层注意力模块、前馈模块和层归一化模块组成，值得注意的是，每个模块都是E（3）等变的，以满足3D世界的对称性。在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面进行了大量实验，表明GET优于各种最先进的方法。

    Many processes in biology and drug discovery involve various 3D interactions between different molecules, such as protein and protein, protein and small molecule, etc. Designing a generalist model to learn universal molecular interactions is valuable yet challenging, given that different molecules are usually represented in different granularity. In this paper, we first propose to universally represent a 3D molecule as a geometric graph of sets, in contrast to conventional single-level representations. Upon the proposed unified representation, we then propose a Generalist Equivariant Transformer (GET) to effectively capture both sparse block-level and dense atom-level interactions. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where, notably, each module is E(3) equivariant to meet the symmetry of 3D world. Extensive experiments on the prediction of protein-protein affinity, ligand binding affinity, and ligand effica
    
[^322]: 自适应自蒸馏下的异构数据联邦学习

    Federated Learning on Heterogeneous Data via Adaptive Self-Distillation. (arXiv:2305.19600v1 [cs.LG])

    [http://arxiv.org/abs/2305.19600](http://arxiv.org/abs/2305.19600)

    本文提出一种基于自适应自蒸馏的新型正则化技术来训练客户端模型，该正则化方案基于客户端本地模型预测和全局模型的相似性以及客户端的标签分布来自适应地调整客户端的训练数据。实验结果表明，该方法在各种基准数据集上优于目前流行的联邦学习方法。

    

    联邦学习是一种机器学习范式，它使得客户机可以聚合本地训练模型而无需共享任何本地训练数据从而训练全局模型。然而，实践中发现，每个客户端观察到的本地数据分布之间可能存在显著的不均匀性（例如类别不平衡）。在这种不均匀的数据分布下，联邦学习会出现“客户机漂移”问题，导致每个客户端收敛到其自己的局部最优解，这会降低模型的收敛速度并降低模型性能。为了解决这个问题，我们提出了一种基于自适应自蒸馏的新型正则化技术来训练客户端模型。我们的正则化方案基于客户端本地模型预测和全局模型的相似性以及客户端的标签分布来自适应地调整客户端的训练数据。该正则化技术可以轻松地集成在现有的联邦学习算法之上，而不需要对客户端或服务器代码进行任何更改，因此具有高度的可部署性。我们在各种基准数据集上验证了我们的方法，并展示了在非独立同分布数据下的优越性。

    Federated Learning (FL) is a machine learning paradigm that enables clients to jointly train a global model by aggregating the locally trained models without sharing any local training data. In practice, there can often be substantial heterogeneity (e.g., class imbalance) across the local data distributions observed by each of these clients. Under such non-iid data distributions across clients, FL suffers from the 'client-drift' problem where every client converges to its own local optimum. This results in slower convergence and poor performance of the aggregated model. To address this limitation, we propose a novel regularization technique based on adaptive self-distillation (ASD) for training models on the client side. Our regularization scheme adaptively adjusts to the client's training data based on: (1) the closeness of the local model's predictions with that of the global model and (2) the client's label distribution. The proposed regularization can be easily integrated atop exis
    
[^323]: 非线性循环神经网络的逆近似理论

    Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19190](http://arxiv.org/abs/2305.19190)

    该论文证明了使用RNNs逼近非线性序列关系的逆近似定理，进一步将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并提出了一个有原则的重新参数化方法来克服这些限制。

    

    我们证明了使用RNNs来逼近非线性序列关系的逆近似定理。这是近似理论中的一种称为Bernstein型结果的结果，它在假设目标函数可以通过假设空间有效逼近的条件下推导出目标函数的属性。特别地，我们展示了非线性序列关系可以被具有hardtanh/tanh激活函数的RNNs稳定逼近的时候，必须具有一个指数衰减的记忆结构--这个概念可以被明确定义。这将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并量化了RNN架构在学习具有长期记忆的序列关系时的重要限制。基于分析，我们提出了一个有原则的重新参数化方法来克服这些限制。我们的理论结果通过数值实验进行了确认。

    We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using RNNs. This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships, viewed as functional sequences, that can be stably approximated by RNNs with hardtanh/tanh activations must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. Based on the analysis, we propose a principled reparameterization method to overcome the limitations. Our theoretical results are confirmed by numerical experiments.
    
[^324]: 基于内核KL散度的自监督学习方法Kernel-SSL

    Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])

    [http://arxiv.org/abs/2305.17326](http://arxiv.org/abs/2305.17326)

    本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。

    

    对比学习通常将一个正锚点样本与许多负样本进行比较，来完成自监督学习（SSL）。相反，非对比学习，例如BYOL、SimSiam和Barlow Twins等方法，在没有显式使用负样本的情况下完成SSL。受对比学习现有分析的启发，我们提供了多种现有非对比学习方法的再生核希尔伯特空间（RKHS）理解。随后，我们提出了一种新的损失函数Kernel-SSL，直接优化RKHS中的均值嵌入和协方差算子。实验中，我们的方法Kernel-SSL在线性评估设置下在ImageNet数据集上大幅优于最先进的方法。具体来说，在进行100个epoch的预训练时，我们的方法比SimCLR表现提高了4.6%。

    Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
    
[^325]: 深度概率时间序列预测的更好Batch方法

    Better Batch for Deep Probabilistic Time Series Forecasting. (arXiv:2305.17028v1 [stat.ML])

    [http://arxiv.org/abs/2305.17028](http://arxiv.org/abs/2305.17028)

    该研究提出了一种新的训练方法，通过在 mini-batch 中显式地学习误差的序列相关性，来提高深度概率时间序列预测的准确性和不确定性量化。

    

    深度概率时间序列预测因其能够提供有价值的不确定性量化而受到广泛关注。然而，许多现有模型过于简单化问题，假设误差过程是与时间无关的，从而忽略了误差过程中的序列相关性。这可能会降低预测的准确性，使这些模型对决策性任务的有效性减弱。为了克服这一限制，我们提出了一种创新的训练方法，将误差自相关性纳入考虑，以增强概率预测的准确性。我们的方法涉及构造一个mini-batch，作为$D$个连续时间序列段进行模型训练，并显式地学习一个协方差矩阵，覆盖了相邻时间步之间的误差相关性。由此产生的协方差矩阵可用于提高预测准确性和增强不确定性的量化。

    Deep probabilistic time series forecasting has gained significant attention due to its ability to provide valuable uncertainty quantification for decision-making tasks. However, many existing models oversimplify the problem by assuming the error process is time-independent, thereby overlooking the serial correlation in the error process. This oversight can potentially diminish the accuracy of the forecasts, rendering these models less effective for decision-making purposes. To overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance the accuracy of probabilistic forecasting. Our method involves constructing a mini-batch as a collection of $D$ consecutive time series segments for model training and explicitly learning a covariance matrix over each mini-batch that encodes the error correlation among adjacent time steps. The resulting covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantifica
    
[^326]: 基于谱角度剖析生物数据中图神经网络的尺寸可泛化性：观点和实践

    Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])

    [http://arxiv.org/abs/2305.15611](http://arxiv.org/abs/2305.15611)

    本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。

    

    本文探讨了图神经网络 (GNNs) 是否具有从小图中学习的知识可推广到同一领域的大图中。之前的研究表明，不同大小的图之间的分布偏移，尤其是度分布，可能会导致图分类任务的性能下降。然而，在生物数据集中，度数是有界的，因此度分布的偏移很小。即使度分布偏移很小，我们观察到GNNs在同一数据集的大图上的性能仍然下降，暗示有其他原因。事实上，以往对于真实数据集中各种图尺寸引起的分布偏移类型和属性的探索不足。此外，以前的尺寸可泛化性分析大多集中在空间领域。为填补这些空白，我们采用谱角度去研究GNNs在生物图数据上的尺寸可泛化性。我们首先提出一个新框架来模拟各种类型的度分布偏移，并利用它来测试GNNs 在真实生物数据集上的尺寸可泛化性。我们的实验表明，除了度分布偏移外，GNNs 还对图大小变化引起的谱分布偏移很敏感。我们进一步分析了不同的GNN模型的影响，并表明，一些模型比其他模型更具有尺寸泛化性。本文展示了关于GNNs尺寸可泛化性问题的新观点和实践，并为该领域的未来研究提供了有益的洞察和建议。

    We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
    
[^327]: 通过非平衡最优输运半对偶公式的生成建模

    Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport. (arXiv:2305.14777v1 [cs.CV])

    [http://arxiv.org/abs/2305.14777](http://arxiv.org/abs/2305.14777)

    本文提出了一种基于非平衡最优输运半对偶公式的新型生成模型，相比于OT，它具有更好的鲁棒性、稳定性和更快的收敛速度，实验结果表明其优于现有的基于OT的生成模型。

    

    最优输运（OT）问题研究一种运输映射，能够在最小化给定成本函数的同时连接两个分布。在这方面，OT已被用于生成建模任务中的可追溯的先验分布和数据之间。然而，基于OT的方法容易受到离群点的影响，并在训练期间面临优化挑战。在本文中，我们提出了一种基于非平衡最优输运（UOT）半对偶公式的新型生成模型。与OT不同，UOT消除了分布匹配的硬性约束，提供了更好的对抗离群点的鲁棒性，训练期间的稳定性以及更快的收敛速度。我们通过实验验证了这些属性。此外，我们还研究了UOT之间分布差异的理论上限。我们的模型优于现有的基于OT的生成模型，在CIFAR-10和CelebA-HQ-256上实现了分别为2.97和5.80的FID分数。

    Optimal Transport (OT) problem investigates a transport map that bridges two distributions while minimizing a given cost function. In this regard, OT between tractable prior distribution and data has been utilized for generative modeling tasks. However, OT-based methods are susceptible to outliers and face optimization challenges during training. In this paper, we propose a novel generative model based on the semi-dual formulation of Unbalanced Optimal Transport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution matching. This approach provides better robustness against outliers, stability during training, and faster convergence. We validate these properties empirically through experiments. Moreover, we study the theoretical upper-bound of divergence between distributions in UOT. Our model outperforms existing OT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 5.80 on CelebA-HQ-256.
    
[^328]: 针对私有微调的有选择性预训练

    Selective Pre-training for Private Fine-tuning. (arXiv:2305.13865v1 [cs.LG])

    [http://arxiv.org/abs/2305.13865](http://arxiv.org/abs/2305.13865)

    本文提出了一个通用框架，解决在保护隐私和满足内存和推理时间要求的情况下，在公共数据集上预训练一个固定大小的模型，并在私有数据集上进行微调以最大化对下游任务的性能。框架的关键是在公共数据集的子集上进行有选择性的预训练，使公共分布靠近私有分布。

    

    假设我们想在电子邮件客户端或文字处理器中训练文本预测模型。这些模型必须保护用户数据的隐私，并遵守特定的固定大小，以满足内存和推理时间要求。我们介绍了一个通用框架来解决这个问题。具体来说，我们有一个公共数据集D_pub和一个对应于下游任务T的私有数据集D_priv。我们如何在D_pub上预训练一个固定大小的模型M，并在D_priv上微调它，使得M相对于T的性能最大化，并且M相对于D_priv具有差分隐私保护？我们展示了在D_pub的一个子集上预训练，将公共分布与私有分布靠近，是最大化M预训练后的迁移学习能力的关键因素，特别是在模型大小相对较小的情况下。除了性能改进外，我们的框架还提供了保护隐私的机制。

    Suppose we want to train text prediction models in email clients or word processors. The models must preserve the privacy of user data and adhere to a specific fixed size to meet memory and inference time requirements. We introduce a generic framework to solve this problem. Specifically, we are given a public dataset $D_\text{pub}$ and a private dataset $D_\text{priv}$ corresponding to a downstream task $T$. How should we pre-train a fixed-size model $M$ on $D_\text{pub}$ and fine-tune it on $D_\text{priv}$ such that performance of $M$ with respect to $T$ is maximized and $M$ satisfies differential privacy with respect to $D_\text{priv}$? We show that pre-training on a {\em subset} of dataset $D_\text{pub}$ that brings the public distribution closer to the private distribution is a crucial ingredient to maximize the transfer learning abilities of $M$ after pre-training, especially in the regimes where model sizes are relatively small. Besides performance improvements, our framework als
    
[^329]: 自注意力动态中的聚类现象

    The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])

    [http://arxiv.org/abs/2305.05465](http://arxiv.org/abs/2305.05465)

    本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。

    

    将Transformer视为相互作用的粒子系统，当权重不随时间变化时，本文描述了学习表示的几何形状。我们展示了代表token的粒子随着时间趋于无穷大而趋向于特定的极限对象。出现的极限对象类型取决于价值矩阵的谱。此外，在一维情况下，我们证明了自我注意力矩阵收敛于低秩布尔矩阵。这些结果的组合在数学上证实了Vaswani等人的经验观察，即Transformer处理一系列token时会出现“领导者”。

    Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
    
[^330]: Kullback-Leibler Maillard采样在有界奖励的多臂赌博机问题中的应用

    Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards. (arXiv:2304.14989v1 [cs.LG])

    [http://arxiv.org/abs/2304.14989](http://arxiv.org/abs/2304.14989)

    本文提出了Kullback-Leibler Maillard Sampling (KL-MS)算法，能够在有界奖励的多臂赌博机中实现KL空间的扩展，具有较好的渐近性能。

    

    本文研究了奖励分布集中在区间$[0,1]$内的$K$臂数臂赌博机问题。本文提出了一种名为Kullback-Leibler Maillard Sampling (KL-MS)的新算法，它是Maillard采样在KL空间的自然扩展。实验表明，KL-MS在Bernoulli奖励时具有渐近最优性能，其最坏情况遗憾度上界为$O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$，其中$\mu^*$是最优臂的期望奖励，$T$是时段长度。

    We study $K$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. It has been a challenge to design regret-efficient randomized exploration algorithms in this setting. Maillard sampling~\cite{maillard13apprentissage}, an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting~\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we propose the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling for achieving KL-style gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has a worst-case regret bound of the form $O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$, where $\mu^*$ is the expected reward of the optimal arm, and $T$ is the time horizon length.
    
[^331]: MUDiff: 统一扩散生成完整分子

    MUDiff: Unified Diffusion for Complete Molecule Generation. (arXiv:2304.14621v1 [cs.LG])

    [http://arxiv.org/abs/2304.14621](http://arxiv.org/abs/2304.14621)

    MUDiff 是一种新的分子数据生成模型，它通过组合离散和连续扩散过程来生成全面的分子表示。使用扩散过程可以捕捉分子过程的概率本质，并探索不同因素对分子结构和性质的影响。我们的模型还包括一个新颖的图形转换器架构，用于去噪扩散过程。

    

    我们提出了一种新的分子数据生成模型，通过组合离散和连续扩散过程来生成分子数据。我们的模型生成了分子的全面表示，包括原子特征、二维离散分子结构和三维连续分子坐标。使用扩散过程可以捕捉分子过程的概率本质，并探索不同因素对分子结构和性质的影响。此外，我们提出了一种新颖的图形转换器架构，用于去噪扩散过程。转换器对欧几里得变换是等变的，使其能够学习不变的原子和边界表示，同时保持原子坐标的等变性。这种转换器可以用于学习对几何变换具有鲁棒性的分子表示。我们通过实验和与现有方法的比较评估了我们模型的性能，展示了其生成更稳定和有效的分子结构的能力。

    We present a new model for generating molecular data by combining discrete and continuous diffusion processes. Our model generates a comprehensive representation of molecules, including atom features, 2D discrete molecule structures, and 3D continuous molecule coordinates. The use of diffusion processes allows for capturing the probabilistic nature of molecular processes and the ability to explore the effect of different factors on molecular structures and properties. Additionally, we propose a novel graph transformer architecture to denoise the diffusion process. The transformer is equivariant to Euclidean transformations, allowing it to learn invariant atom and edge representations while preserving the equivariance of atom coordinates. This transformer can be used to learn molecular representations robust to geometric transformations. We evaluate the performance of our model through experiments and comparisons with existing methods, showing its ability to generate more stable and val
    
[^332]: 关于多个吸引子非线性系统的提升和重构

    On the lifting and reconstruction of nonlinear systems with multiple attractors. (arXiv:2304.11860v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2304.11860](http://arxiv.org/abs/2304.11860)

    本文研究了具有多个吸引子的非线性系统的Koopman算子提升和重构机制，通过利用吸引域之间的固有对称性，只需三个自由度的线性重构就可以全局线性化系统。

    

    Koopman算子通过关注不变子空间中的观测量的演化，提供了非线性动力学的线性视角。感兴趣的观测量通常是从Koopman特征函数线性重构出来的。尽管Koopman算子在过去几年广泛使用，但对于具有多个稳定点的动力系统，关于Koopman算子的适用性存在一些误解。本研究解释了具有多个吸引子的非线性系统的Koopman算子提升机制。通过考虑Duffing振荡器的例子，我们表明通过利用吸引域之间的固有对称性，Koopman可观测空间中具有三个自由度的线性重构就足以全局线性化系统。

    The Koopman operator provides a linear perspective on non-linear dynamics by focusing on the evolution of observables in an invariant subspace. Observables of interest are typically linearly reconstructed from the Koopman eigenfunctions. Despite the broad use of Koopman operators over the past few years, there exist some misconceptions about the applicability of Koopman operators to dynamical systems with more than one fixed point. In this work, an explanation is provided for the mechanism of lifting for the Koopman operator of nonlinear systems with multiple attractors. Considering the example of the Duffing oscillator, we show that by exploiting the inherent symmetry between the basins of attraction, a linear reconstruction with three degrees of freedom in the Koopman observable space is sufficient to globally linearize the system.
    
[^333]: 利用RMT将Transformer扩展到100万个标记及以上。

    Scaling Transformer to 1M tokens and beyond with RMT. (arXiv:2304.11062v1 [cs.CL])

    [http://arxiv.org/abs/2304.11062](http://arxiv.org/abs/2304.11062)

    本文介绍了一种利用循环记忆扩展BERT上下文长度的方法，成功扩展到了前所未有的200万个标记，有望增强自然语言处理中的长期依赖处理并为内存密集型应用程序实现大规模上下文处理。

    

    本技术报告介绍了一种利用循环记忆扩展BERT上下文长度的方法，BERT是自然语言处理中最有效的基于Transformer模型之一。通过利用循环记忆Transformer架构，我们成功地将模型的有效上下文长度增加到了前所未有的200万个标记，同时保持了高的内存检索准确性。我们的方法允许存储和处理本地和全局信息，并通过使用循环实现输入序列各部分之间的信息流动。我们的实验证明了我们的方法的有效性，具有显著的潜力来增强自然语言理解和生成任务中的长期依赖处理，并能够为内存密集型应用程序实现大规模上下文处理。

    This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.
    
[^334]: 通过潜在空间探索和因果推断方法逼近未知的通信系统

    Approaching an unknown communication system by latent space exploration and causal inference. (arXiv:2303.10931v1 [stat.ML])

    [http://arxiv.org/abs/2303.10931](http://arxiv.org/abs/2303.10931)

    本文通过探索无监督深度生成模型的潜在空间来发现数据中有意义的属性，提出了一种极端值因果分离 (CDEV) 的方法，应用于测试鲸鱼通信系统并发现其中存在语法。

    

    本文提出了一种方法，通过探索无监督深度生成模型的潜在空间来发现数据中有意义的属性。我们将对单个潜在变量的操作与因果推断方法相结合，实现了一个称为极端值因果分离 (CDEV) 的方法，并展示了该方法对模型可解释性的洞察力。通过此技术，我们可以推断模型将未知数据编码为有意义的属性。我们将该方法应用于测试鲸鱼的通信系统中存在哪些有意义的属性，鲸鱼通信是最具有吸引力和研究不足的动物通信之一。我们训练了一个网络，该网络已被证明能够学习到有意义的语音表示，并测试是否可以利用这种无监督学习来解析另一个我们没有地面真相的声音通信系统的属性。所提出的技术表明，鲸鱼在其声音通信中存在语法，这是以前不知道的。

    This paper proposes a methodology for discovering meaningful properties in data by exploring the latent space of unsupervised deep generative models. We combine manipulation of individual latent variables to extreme values outside the training range with methods inspired by causal inference into an approach we call causal disentanglement with extreme values (CDEV) and show that this approach yields insights for model interpretability. Using this technique, we can infer what properties of unknown data the model encodes as meaningful. We apply the methodology to test what is meaningful in the communication system of sperm whales, one of the most intriguing and understudied animal communication systems. We train a network that has been shown to learn meaningful representations of speech and test whether we can leverage such unsupervised learning to decipher the properties of another vocal communication system for which we have no ground truth. The proposed technique suggests that sperm wh
    
[^335]: 通过有针对性的增强实现领域外的鲁棒性

    Out-of-Domain Robustness via Targeted Augmentations. (arXiv:2302.11861v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11861](http://arxiv.org/abs/2302.11861)

    本文研究了为领域外泛化设计数据增强的原则，通过有针对性的增强方法，在保留鲁棒特征的同时随机化虚假的领域相关特征，提高了领域外性能。

    

    在一个领域上训练的模型往往在未见过的领域上表现下降，比如野生动物监测模型在新的摄像机位置上部署时会出现这种情况。在这项工作中，我们研究了为领域外泛化设计数据增强的原则。特别地，我们关注现实世界中的场景，在这些场景中，一些与领域相关的特征是鲁棒的，即一些在不同领域间变化的特征对泛化有预测能力。例如，在上述的野生动物监测应用中，图像背景在摄像机位置上不同，但可以指示栖息地类型，从而帮助预测被摄动物的物种。在对线性设置进行理论分析的基础上，我们提出了有针对性的增强方法，即有选择性地随机化虚假的领域相关特征，同时保留鲁棒的特征。我们证明了有针对性的增强可以提高领域外性能，使模型在较少领域上具有更好的泛化能力。相比之下，现有的通用增强方法未能实现领域外的鲁棒性。

    Models trained on one set of domains often suffer performance drops on unseen domains, e.g., when wildlife monitoring models are deployed in new camera locations. In this work, we study principles for designing data augmentations for out-of-domain (OOD) generalization. In particular, we focus on real-world scenarios in which some domain-dependent features are robust, i.e., some features that vary across domains are predictive OOD. For example, in the wildlife monitoring application above, image backgrounds vary across camera locations but indicate habitat type, which helps predict the species of photographed animals. Motivated by theoretical analysis on a linear setting, we propose targeted augmentations, which selectively randomize spurious domain-dependent features while preserving robust ones. We prove that targeted augmentations improve OOD performance, allowing models to generalize better with fewer domains. In contrast, existing approaches such as generic augmentations, which fai
    
[^336]: 持续学习综述：理论、方法与应用

    A Comprehensive Survey of Continual Learning: Theory, Method and Application. (arXiv:2302.00487v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00487](http://arxiv.org/abs/2302.00487)

    本文为持续学习的全面综述，总结了持续学习的一般目标，回顾了各种持续学习方法，并强调了一些有前途的应用领域和未来研究方向。

    

    智能系统需要在其生命周期内不断获取、更新、积累和利用知识以应对现实世界的动态变化。这种能力称为持续学习，为AI系统自适应发展提供了基础。然而，持续学习的一个显著限制是灾难性遗忘，即学习一个新任务通常会导致旧任务的性能显著降低。近年来，不断涌现的各种进展大大扩展了持续学习的理解和应用。本文提供了一个全面的持续学习综述，旨在连接基本设置、理论基础、代表性方法和实际应用。我们总结了现有理论和实证结果，概括了持续学习的一般目标：减少灾难性遗忘，实现高效的和终身的学习，以及实现更深层次的表征学习。我们还回顾了各种持续学习方法，包括基于正则化、基于回放和基于生成的方法，并讨论了它们的优缺点。最后，我们强调了一些有前途的应用领域，如机器人、自然语言处理和计算机视觉，并确定了一些开放挑战和未来研究方向。

    To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of 
    
[^337]: RaLiBEV: 雷达和激光雷达的引导框自由物体检测系统的融合学习

    RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System. (arXiv:2211.06108v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.06108](http://arxiv.org/abs/2211.06108)

    本论文提出了一种基于鸟瞰视角的引导框自由物体检测系统，通过雷达和激光雷达的特征融合学习，解决了在恶劣天气下物体检测的问题。

    

    在自动驾驶系统中，激光雷达和雷达在感知周围环境中起着重要作用。激光雷达提供精确的三维空间感知信息，但在雾等恶劣天气下无法工作。另一方面，雷达信号由于波长的特性在遇到雨滴或雾粒时会发生衍射，但它受到大噪声的干扰。最近的最新研究表明，雷达和激光雷达的融合可以在恶劣天气下实现强健的检测。现有的方法采用卷积神经网络架构从每个传感器数据流中提取特征，然后对齐和汇聚两个分支的特征以预测物体检测结果。然而，由于标签分配和融合策略的简单设计，这些方法对边界框估计的准确性较低。在本文中，我们提出了一种基于鸟瞰视角融合学习的引导框自由物体检测系统，该系统将来自雷达的距离-方位特征融合起来

    In autonomous driving systems, LiDAR and radar play important roles in the perception of the surrounding environment. LiDAR provides accurate 3D spatial sensing information but cannot work in adverse weather like fog. On the other hand, the radar signal can be diffracted when encountering raindrops or mist particles thanks to its wavelength, but it suffers from large noise. Recent state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data stream, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of bounding box estimations due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth 
    
[^338]: 准算术混合、散度最小化和Bregman信息

    Quasi-Arithmetic Mixtures, Divergence Minimization, and Bregman Information. (arXiv:2209.07481v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07481](http://arxiv.org/abs/2209.07481)

    本文提供了对准算术混合、散度最小化和Bregman信息的全面分析。通过在密度函数的单调嵌入下使用Bregman散度，我们将常见的散度函数与退火路径上的中间密度关联起来。

    

    马尔可夫链蒙特卡洛方法用于从复杂分布中采样和估计归一化常数通常模拟沿着连接可跟踪初始分布和目标密度的退火路径的中间分布的样本。先前的工作使用准算术平均构建了退火路径，并解释了由此产生的中间密度是最小化期望散度到端点的。我们通过在密度函数的单调嵌入下使用Bregman散度对这个“质心”性质进行了全面分析，从而将常见的散度（如Amari和Renyi的alpha散度、（alpha，beta）散度和Jensen-Shannon散度）与沿着退火路径的中间密度关联起来。我们的分析突出了参数化族、准算术平均和散度函数之间的相互作用，使用了Zhang的rho-tau Bregman散度框架。

    Markov Chain Monte Carlo methods for sampling from complex distributions and estimating normalization constants often simulate samples from a sequence of intermediate distributions along an annealing path, which bridges between a tractable initial distribution and a target density of interest. Prior work has constructed annealing paths using quasi-arithmetic means, and interpreted the resulting intermediate densities as minimizing an expected divergence to the endpoints. We provide a comprehensive analysis of this 'centroid' property using Bregman divergences under a monotonic embedding of the density function, thereby associating common divergences such as Amari's and Renyi's ${\alpha}$-divergences, ${(\alpha,\beta)}$-divergences, and the Jensen-Shannon divergence with intermediate densities along an annealing path. Our analysis highlights the interplay between parametric families, quasi-arithmetic means, and divergence functions using the rho-tau Bregman divergence framework of Zhang
    
[^339]: 用熵近似的高斯混合变分推断

    Variational Inference with Gaussian Mixture by Entropy Approximation. (arXiv:2202.13059v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.13059](http://arxiv.org/abs/2202.13059)

    论文提出了一种用高斯混合分布作为参数分布的变分推断方法，通过将高斯混合的熵近似为单峰高斯的熵之和来解决多峰性的问题，并从理论上分析近似误差。

    

    变分推断是一种用于近似无法处理的后验分布以量化机器学习不确定性的技术。然而，单峰的高斯分布通常被选择作为参数分布，很难逼近多峰性。在本文中，我们采用高斯混合分布作为参数分布。使用高斯混合进行变分推断的一个主要难点是如何近似高斯混合的熵。我们将高斯混合的熵近似为单峰高斯的熵之和，可以通过解析计算得到。此外，我们从理论上分析了真实熵与近似熵之间的近似误差，以便揭示我们的近似何时起作用。具体而言，近似误差由高斯混合均值之间距离与方差之和的比率控制。此外，当高斯混合组件的数量趋近于无穷大时，近似误差趋近于零。

    Variational inference is a technique for approximating intractable posterior distributions in order to quantify the uncertainty of machine learning. Although the unimodal Gaussian distribution is usually chosen as a parametric distribution, it hardly approximates the multimodality. In this paper, we employ the Gaussian mixture distribution as a parametric distribution. A main difficulty of variational inference with the Gaussian mixture is how to approximate the entropy of the Gaussian mixture. We approximate the entropy of the Gaussian mixture as the sum of the entropy of the unimodal Gaussian, which can be analytically calculated. In addition, we theoretically analyze the approximation error between the true entropy and approximated one in order to reveal when our approximation works well. Specifically, the approximation error is controlled by the ratios of the distances between the means to the sum of the variances of the Gaussian mixture. Furthermore, it converges to zero when the 
    

