# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DACR: Distribution-Augmented Contrastive Reconstruction for Time-Series Anomaly Detection.](http://arxiv.org/abs/2401.11271) | DACR提出了分布增强对比重建（DACR）方法，通过生成额外的数据来压缩正常数据的表示空间，并通过对比学习来增强特征提取器，同时利用注意力机制建模多变量时间序列特征之间的语义依赖关系，从而实现更强大的时间序列异常检测重建。 |
| [^2] | [Diffusion Model Conditioning on Gaussian Mixture Model and Negative Gaussian Mixture Gradient.](http://arxiv.org/abs/2401.11261) | 本研究提出了一种利用高斯混合模型作为特征条件来引导扩散模型的去噪过程的条件机制，并通过实验证实了基于特征的条件潜在分布能够产生更少的缺陷生成。 |
| [^3] | [Automated Fusion of Multimodal Electronic Health Records for Better Medical Predictions.](http://arxiv.org/abs/2401.11252) | 该论文提出了一种自动融合多模态电子健康记录的神经架构搜索框架，用于改进医疗预测准确性。该框架可以自动搜索用于编码不同输入模态和融合策略的最佳模型架构，并在实验中取得了超越传统方法的效果。 |
| [^4] | [AFS-BM: Enhancing Model Performance through Adaptive Feature Selection with Binary Masking.](http://arxiv.org/abs/2401.11250) | AFS-BM通过联合优化实现了自适应特征选择和模型训练，提高了模型准确性并减少了计算需求。 |
| [^5] | [Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View.](http://arxiv.org/abs/2401.11237) | 这篇论文研究了强化学习方法在训练过程中将不同经验片段组合起来解决未见过的任务的属性，并通过分析发现这种组合属性与组合泛化有关。 |
| [^6] | [TreeMIL: A Multi-instance Learning Framework for Time Series Anomaly Detection with Inexact Supervision.](http://arxiv.org/abs/2401.11235) | 本文提出了一个基于树的多实例学习框架(TreeMIL)，用于带有不精确监督的时间序列异常检测。该框架通过将整个序列分解成多个节点，并提取子序列特征，旨在解决集体异常的挑战。 |
| [^7] | [A Hybrid Approach of Transfer Learning and Physics-Informed Modeling: Improving Dissolved Oxygen Concentration Prediction in an Industrial Wastewater Treatment Plant.](http://arxiv.org/abs/2401.11217) | 本论文提出了一种传输学习和物理信息建模的混合方法，通过将其他任务中的知识转移到目标任务中，以提高工业废水处理厂溶解氧浓度预测的准确性。这种方法结合了开源模型的物理知识和另一个工业厂的数据，并使训练问题的目标函数具备物理信息建模的特点。 |
| [^8] | [Selecting Walk Schemes for Database Embedding.](http://arxiv.org/abs/2401.11215) | 本文研究了数据库嵌入中的行走方案选择问题，通过关注几个信息丰富的行走方案，能够更快地获得元组嵌入，并保持其质量。 |
| [^9] | [PartIR: Composing SPMD Partitioning Strategies for Machine Learning.](http://arxiv.org/abs/2401.11202) | PartIR是一种用于机器学习的分区系统，具备表达力强和可预测性强的特点。它通过高级程序员发出的分区策略驱动，并采用增量重写方法，能够组合不同的分片策略，评估结果表明其可预测性、表达能力和达到峰值性能能力强。 |
| [^10] | [Projected Belief Networks With Discriminative Alignment for Acoustic Event Classification: Rivaling State of the Art CNNs.](http://arxiv.org/abs/2401.11199) | 基于鉴别对齐的投影信念网络在声学事件分类中达到了与最先进CNN相媲美的效果。 |
| [^11] | [Machine learning based state observer for discrete time systems evolving on Lie groups.](http://arxiv.org/abs/2401.11196) | 本文提出了一种基于深度学习的机器学习方法，用于在李群上演化的系统的状态观测。通过设计一个限制在李群上的观测器，仅使用一个训练的算法来预测状态，并利用从李代数到群的映射以及群作用和当前状态来估计时刻t的状态。 |
| [^12] | [Fast and Exact Enumeration of Deep Networks Partitions Regions.](http://arxiv.org/abs/2401.11188) | 本文提出了第一个能够精确枚举DN分区的并行算法，可以评估常用的基于随机抽样的近似方法，发现如果只对具有“大”体积的区域感兴趣，那么对空间进行均匀抽样非常高效。 |
| [^13] | [Data-Driven Target Localization: Benchmarking Gradient Descent Using the Cram\'er-Rao Bound.](http://arxiv.org/abs/2401.11176) | 本研究提出了一种数据驱动的神经网络方法，通过降低均方误差（MSE）实现了改进的目标方位和速度估计。这一发现强调了在雷达系统中采用深度学习方法的潜力，为在杂乱和动态环境中更准确的定位铺平了道路。 |
| [^14] | [Pixel-Wise Recognition for Holistic Surgical Scene Understanding.](http://arxiv.org/abs/2401.11174) | 本文提出了一个整体和多粒度外科场景理解数据集，以及一个基于变形器的模型，该模型有效地结合了全局视频特征提取和局部器械分割，可用于多层次理解外科活动。 |
| [^15] | [Document Set Expansion with Positive-Unlabeled Learning: A Density Estimation-based Approach.](http://arxiv.org/abs/2401.11145) | 本文提出了一种基于密度估计的正-负学习框架，可以处理文档集扩展任务中存在的问题，并证明其在真实数据集上的有效性。 |
| [^16] | [Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities.](http://arxiv.org/abs/2401.11143) | 该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。 |
| [^17] | [Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable.](http://arxiv.org/abs/2401.11130) | 本文提出了一种通过工具变量法识别和估计连续处理的因果效应异质性的方法，并开发了三类相应的估计器，并对其进行了统计性质分析。 |
| [^18] | [CARE: Ensemble Adversarial Robustness Evaluation Against Adaptive Attackers for Security Applications.](http://arxiv.org/abs/2401.11126) | 本研究提出了一个名为CARE的通用网络安全对抗鲁棒性评估平台，旨在为解决集成对抗攻击和防御中的关键问题提供一个综合评估平台。 |
| [^19] | [EMA-Net: Efficient Multitask Affinity Learning for Dense Scene Predictions.](http://arxiv.org/abs/2401.11124) | EMA-Net 是一个高效的多任务关联学习网络，通过引入跨任务关联学习模块(CTAL)，能够同时捕捉局部、全局和跨任务的相互作用。 |
| [^20] | [Meta Reinforcement Learning for Strategic IoT Deployments Coverage in Disaster-Response UAV Swarms.](http://arxiv.org/abs/2401.11118) | 本研究提出了一种战略物联网部署覆盖灾难响应无人机群的元强化学习方法，通过优化模型来最小化能源消耗，并提供满足时间和功率约束的最佳路径规划。 |
| [^21] | [SPAND: Sleep Prediction Architecture using Network Dynamics.](http://arxiv.org/abs/2401.11113) | SPAND是一个利用网络动态的睡眠预测架构，可以通过图网络和移动设备数据来预测下一天的睡眠持续时间标签。 |
| [^22] | [VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE.](http://arxiv.org/abs/2401.11110) | VONet是一种无监督视频对象学习方法，通过并行U-Net注意力和逐个对象的序列VAE实现，具有高效且有效的推理过程和提升时间一致性的特点，成为五个MOVI数据集上领先的方法。 |
| [^23] | [Are Latent Vulnerabilities Hidden Gems for Software Vulnerability Prediction? An Empirical Study.](http://arxiv.org/abs/2401.11105) | 该研究通过对潜在漏洞的研究，发现它们可以有效增加漏洞数量并纠正错误标记的函数，尽管存在噪声，但最先进的漏洞预测模型仍可以从中受益。 |
| [^24] | [Efficient Data Shapley for Weighted Nearest Neighbor Algorithms.](http://arxiv.org/abs/2401.11103) | 本研究提出了一种解决加权K最近邻算法高效计算Data Shapley值的方法，并通过实验证明其在数据质量判别方面优于未加权版本。 |
| [^25] | [Neural auto-designer for enhanced quantum kernels.](http://arxiv.org/abs/2401.11098) | 本研究提出了一种自动化设计问题特定量子特征映射的数据驱动方法，通过利用特征选择技术和深度神经预测器，解决了在限制量子比特的近期量子机器上处理高维数据的挑战，并证明了该方法相对于先前方法的优越性。 |
| [^26] | [Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions.](http://arxiv.org/abs/2401.11081) | 本文研究了从聚合响应中学习的两种损失函数：包级别损失和实例级别损失，并发现实例级别损失可以被视为包级别损失的正则化形式。 |
| [^27] | [On The Temporal Domain of Differential Equation Inspired Graph Neural Networks.](http://arxiv.org/abs/2401.11074) | 本文提出了一种名为TDE-GNN的模型，解决了现有微分方程激发的图神经网络中一阶或二阶时域相关性的限制。通过学习时域依赖性，我们的模型能够捕捉到更广泛的时域动态，并取得了在几个图基准测试上的优越表现。 |
| [^28] | [The Significance of Data Abstraction Methods in Machine Learning Classification Processes for Critical Decision-Making.](http://arxiv.org/abs/2401.11044) | 本文研究了机器学习分类过程中数据抽象方法的重要性，提出了Small and Incomplete Dataset Analyser (SaNDA)采用ROC曲线方法开发的数据抽象协议，该方法在缺少值很少的情况下可以成为随机森林的可行替代品，始终保持高准确性。 |
| [^29] | [Equivariant Graph Neural Operator for Modeling 3D Dynamics.](http://arxiv.org/abs/2401.11037) | 本文提出了一种新的等变图神经操作器（EGNO）方法，能够直接将动力学建模为轨迹而不仅仅是下一步预测，以准确捕捉时间相关性，并利用等变时间卷积来保持其内在的等变性。 |
| [^30] | [Exploring Highly Quantised Neural Networks for Intrusion Detection in Automotive CAN.](http://arxiv.org/abs/2401.11030) | 本文介绍了在汽车CAN中使用高度量化的神经网络来进行入侵检测的案例，以应对不断增长的连接性可能带来的安全威胁。该方法通过使用自定义量化的多层感知机（CQMLP）模型，可以同时检测多个攻击向量，降低了资源成本和能量消耗。 |
| [^31] | [Communication Efficient and Provable Federated Unlearning.](http://arxiv.org/abs/2401.11018) | 这篇论文解决了联邦学习中的一个新问题：通过联邦取消学习，消除特定客户端或数据点对全局模型的影响。作者提出了一个通信高效且可证明的确切联邦取消学习框架，并通过开发一个TV稳定FL算法来实现快速确切联邦取消学习。 |
| [^32] | [Revealing Emotional Clusters in Speaker Embeddings: A Contrastive Learning Strategy for Speech Emotion Recognition.](http://arxiv.org/abs/2401.11017) | 该论文研究了一种使用对比学习策略来揭示说话者嵌入中的情感聚类，并将其应用于语音情感识别中。研究发现情感信息可以直接从说话者嵌入中提取出来，并提出了一种利用情感未标记数据的对抗预训练方法，显著提高了识别效果。 |
| [^33] | [Bounding Consideration Probabilities in Consider-Then-Choose Ranking Models.](http://arxiv.org/abs/2401.11016) | 在考虑-然后-选择的排名模型中，我们提出了一种方法来确定考虑概率的界限。尽管不能准确确定考虑概率，但在已知备选方案效用的情况下，我们可以推断出备选概率的相对大小的界限。 |
| [^34] | [Provably Scalable Black-Box Variational Inference with Structured Variational Families.](http://arxiv.org/abs/2401.10989) | 本文研究了均值场变分族和满秩变分族之间的理论中间地带：结构化变分族，并通过理论证明结构化变分族可以在迭代复杂性上表现更好，缩放效果更好。 |
| [^35] | [T2MAC: Targeted and Trusted Multi-Agent Communication through Selective Engagement and Evidence-Driven Integration.](http://arxiv.org/abs/2401.10973) | T2MAC提出了一种名为目标和可信的多智能体通信方法，该方法使智能体能够学习选择性参与和基于证据的整合，从而提高通信效率。 |
| [^36] | [Clustering Molecular Energy Landscapes by Adaptive Network Embedding.](http://arxiv.org/abs/2401.10972) | 本文提出了一种自适应网络嵌入方法，用于聚类分子能量景观。通过该方法，可以在降维空间中解释动态节点之间的关系，并可应用于化学空间探索和机器学习任务。 |
| [^37] | [HOSC: A Periodic Activation Function for Preserving Sharp Features in Implicit Neural Representations.](http://arxiv.org/abs/2401.10967) | HOSC是一种新型的激活函数，专门设计用于捕捉输入信号中的锐利特征和低频过渡。它可以在隐式神经网络表示中提供改善质量的插入式功能。 |
| [^38] | [One Step Learning, One Step Review.](http://arxiv.org/abs/2401.10962) | 本文提出了一种基于权重回滚的微调方法OLOR，通过结合优化器和权重回滚项，解决了完全微调方法中的知识遗忘问题，并在各种任务上提高了微调性能。 |
| [^39] | [The Synergy Between Optimal Transport Theory and Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2401.10949) | 本文研究了最优输运理论与多智能体强化学习之间的协同作用。通过利用最优输运来处理分布和运输问题，增强了多智能体强化学习的效率、协调性和适应性。通过在政策对齐、分布式资源管理、应对非平稳性、可扩展的多智能体学习和提高能源效率五个方面应用最优输运理论，为解决可扩展性问题、优化资源分配和在合作环境中对齐智能体策略提供了新的方法。 |
| [^40] | [Automatic dimensionality reduction of Twin-in-the-Loop Observers.](http://arxiv.org/abs/2401.10945) | 本论文提出了一种自动降维的方法来解决车辆动力学估计中的各个变量独立计算和校准的问题，通过将经典控制取向车辆模型替换为车辆模拟器或数字双胞胎(DT)来实现，然后使用贝叶斯优化来调节滤波器。 |
| [^41] | [Machine Unlearning for Recommendation Systems: An Insight.](http://arxiv.org/abs/2401.10942) | 本文探讨了机器反学习在推荐系统中的应用，解决了适应性、个性化、隐私和偏见等挑战。与传统模型不同，MUL根据用户偏好的变化和伦理考虑动态调整系统知识。通过批判性检验和文献梳理，本文提供了MUL如何改变推荐、用户信任以及未来研究路径的见解。强调个性化和隐私之间的权衡挑战，并鼓励以满足实际需求为目标的贡献，推动MUL在安全和适应性机器学习中的发展。 |
| [^42] | [Crowd-PrefRL: Preference-Based Reward Learning from Crowds.](http://arxiv.org/abs/2401.10941) | Crowd-PrefRL是一种基于众包的偏好反馈学习框架，能够从来自群体的反馈中学习奖励函数，并且能够强大地聚合群体偏好反馈并估计用户的可靠性。 |
| [^43] | [RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation.](http://arxiv.org/abs/2401.10940) | RELIANCE是一个可靠的集成学习系统，用于评估信息和新闻的可信度。它通过整合多个基本模型的优势，提供了对可信和不可信信息源的准确区分，并在信息和新闻可信度评估方面优于基准模型。 |
| [^44] | [Even-if Explanations: Formal Foundations, Priorities and Complexity.](http://arxiv.org/abs/2401.10938) | 本论文研究了解释性人工智能中的局部事后解释性查询，特别关注半事实的解释，并对线性模型和基于树的模型与神经网络的解释能力进行了比较。此外，提出了一种基于偏好的框架，允许用户根据自己的首选项个性化解释。最后，探讨了模型复杂度的问题。 |
| [^45] | [Forecasting Cryptocurrency Staking Rewards.](http://arxiv.org/abs/2401.10931) | 本研究研究了预测加密货币质押奖励的方法，发现使用滑动窗口平均法可以在很大程度上准确预测ETH的奖励，并且不同加密货币的预测准确度存在差异。线性回归方法在XTZ和ATOM的短期预测中表现出优势，结果突显了大多数资产质押奖励的稳定可预测性，MATIC是一个值得注意的异常情况。 |
| [^46] | [Debiasing and a local analysis for population clustering using semidefinite programming.](http://arxiv.org/abs/2401.10927) | 本文研究了使用半正定规划进行人群聚类的问题，并提出了计算高效的算法。这些算法可以根据小样本数据的原始种群将数据分为两组，适用于种群之间差异较小的情况。 |
| [^47] | [Push- and Pull-based Effective Communication in Cyber-Physical Systems.](http://arxiv.org/abs/2401.10921) | 本论文研究了在网络物理系统中基于推和拉的通信的优化问题，发现最优策略与信息价值最大化相一致，结果表明在特定情况下，基于拉的通信可能比基于推的通信更有效。 |
| [^48] | [Application of Machine Learning in Stock Market Forecasting: A Case Study of Disney Stock.](http://arxiv.org/abs/2401.10903) | 本文研究了机器学习在股票市场预测中的应用，以迪士尼股票为案例进行了分析。通过探索性数据分析、特征工程、数据准备和模型选择，发现线性回归模型表现最佳。 |
| [^49] | [AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis.](http://arxiv.org/abs/2401.10895) | 本文通过系统文献综述和文献计量分析，填补了供应链风险评估中新兴人工智能/机器学习技术的研究空白，为了解这些技术在实践中的实际影响提供了关键见解。 |
| [^50] | [Starlit: Privacy-Preserving Federated Learning to Enhance Financial Fraud Detection.](http://arxiv.org/abs/2401.10765) | Starlit是一个新的可扩展隐私保护的联邦学习机制，解决了对于金融欺诈检测中的几个限制，包括缺乏正式的安全定义和证明、假定冻结账户、规模扩大、身份对齐阶段和难以抵抗客户端退出。 |
| [^51] | [PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks.](http://arxiv.org/abs/2401.10586) | PuriDefense是一种高效的防御机制，通过使用轻量级净化模型进行随机路径净化，减缓基于查询的攻击的收敛速度，并有效防御黑盒基于查询的攻击。 |
| [^52] | [Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian Optimization Approach.](http://arxiv.org/abs/2401.10451) | 本研究提出了一种学习辅助的贝叶斯优化方法，用于解决大规模容量扩展问题。通过构建和求解可行的时间聚合代理问题，识别出低成本的规划决策。通过在验证集和测试预测上评估解决的规划结果，实现了随机容量扩展问题的可行解决。 |
| [^53] | [Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments.](http://arxiv.org/abs/2401.10393) | 本研究在自然学习环境中通过回忆方法减轻了灾难性干扰，该方法受到功率法则的启发。 |
| [^54] | [Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning.](http://arxiv.org/abs/2401.10371) | Langevin遗忘是一种基于噪声梯度下降的遗忘框架，能够在近似遗忘问题中提供隐私保证，并且具有算法上的优势。 |
| [^55] | [Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition.](http://arxiv.org/abs/2401.10337) | 该论文提出了一种基于噪声对比估计的低资源安全攻击模式识别匹配框架，通过直接语义相似度决定文本与攻击模式之间的关联，以降低大量类别、标签分布不均和标签空间复杂性带来的学习难度。 |
| [^56] | [Personality Trait Inference Via Mobile Phone Sensors: A Machine Learning Approach.](http://arxiv.org/abs/2401.10305) | 该研究通过手机传感器收集的活动数据可靠地预测了个性特征，这些研究成果为社会科学研究提供了新的途径。通过使用智能手机传感和机器学习技术，可以以成本效益高、无问卷调查的方式对个性相关问题进行研究。这些发现有助于推动个性研究，并且可以为从业者和研究人员提供信息。 |
| [^57] | [Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction.](http://arxiv.org/abs/2401.10189) | 这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。 |
| [^58] | [A novel hybrid time-varying graph neural network for traffic flow forecasting.](http://arxiv.org/abs/2401.10155) | 本文提出了一种新型的混合时变图神经网络（HTVGNN）用于交通流量预测，解决了现有方法中预定义图和自适应图的学习能力受限的问题。 |
| [^59] | [Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study.](http://arxiv.org/abs/2401.10107) | 本研究通过比较标准多导睡眠图（PSG）和耳内脑电信号的相似性，旨在探索一种更少侵入、成本效益高和便携的替代方法。研究确定了评估方法，并通过提取特征进行分析。 |
| [^60] | [Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification.](http://arxiv.org/abs/2401.09953) | 通过光谱视角，研究了图数据增强方法中的图属性和结构变化的关系，发现保持低频特征值不变可以保留关键属性，提出了双棱镜（DP）增强方法，该方法灵活地保留关键的图属性同时增加图的多样性。 |
| [^61] | [PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection.](http://arxiv.org/abs/2401.09793) | PatchAD是一种新颖的基于块的MLP-Mixer体系结构，利用对比学习进行时间序列异常检测。它具有高效和轻量级的架构，并采用创新的双项目约束模块来提高表示能力。 |
| [^62] | [Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach.](http://arxiv.org/abs/2401.09671) | 本研究旨在解决无监督领域转换中的可识别性问题，引入了一个MPA消除理论，解决了CycleGAN及其变体产生内容不对齐的限制。 |
| [^63] | [Diffusion-Driven Generative Framework for Molecular Conformation Prediction.](http://arxiv.org/abs/2401.09451) | 本文介绍了一种基于扩散驱动的生成框架\method{}，用于预测分子的三维构象，具有较高的预测精度并改进了传统方法的不足。 |
| [^64] | [Machines Do See Color: A Guideline to Classify Different Forms of Racist Discourse in Large Corpora.](http://arxiv.org/abs/2401.09333) | 本文提供了一个逐步可推广的准则，用于在大规模语料库中识别和分类不同形式的种族主义言论。通过对种族主义的概念化和上下文化，以及使用XLM-R和XLM-R-Racismo模型，我们展示了在大规模语料库中进行种族主义分类的优势。 |
| [^65] | [Code Simulation Challenges for Large Language Models.](http://arxiv.org/abs/2401.09074) | 大型语言模型在模拟计算机代码和算法执行方面遇到挑战，性能随着代码长度的增加而迅速下降。在处理短程序或标准过程时，它们能以低错误率按顺序执行指令，但对于复杂的程序，特别是包含关键路径和冗余指令的程序，模拟效果较差。我们提出了一种逐行模拟代码执行的方法来解决这个问题。 |
| [^66] | [Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation.](http://arxiv.org/abs/2401.09031) | 本文研究了数据归因方法对扩散模型的影响，发现对于在引发大范数时间步骤上训练的样本，其损失梯度范数高度依赖于时间步骤，导致在影响估计中存在显著的偏差。为了解决这个问题，提出了Diffusion-ReTr方法。 |
| [^67] | [Augmenting Math Word Problems via Iterative Question Composing.](http://arxiv.org/abs/2401.09003) | 本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。 |
| [^68] | [AntiPhishStack: LSTM-based Stacked Generalization Model for Optimized Phishing URLs Detection.](http://arxiv.org/abs/2401.08947) | 本文介绍了一种名为AntiPhishStack的LSTM-based堆叠泛化模型，用于优化网络钓鱼URL的检测。该模型通过对URL和字符级TF-IDF特征进行对称学习，提高了对新型网络钓鱼威胁的应对能力，并采用对抗性训练策略增加鲁棒性和对抗刚性网络钓鱼攻击。 |
| [^69] | [The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images.](http://arxiv.org/abs/2401.08865) | 本文研究了神经网络在自然图像和医学图像领域学习时的差异，提出了一个与训练集维度有关的泛化缩放定律，并认为医学图像数据集更高的固有“标签锐度”可能是两个领域之间显著差异的部分原因。 |
| [^70] | [Machine Learning-Based Analysis of Ebola Virus' Impact on Gene Expression in Nonhuman Primates.](http://arxiv.org/abs/2401.08738) | 本研究引入了一种基于机器学习的方法用于分析感染伊波拉病毒的非人类灵长类动物的基因表达数据，发现IFI6和IFI27等基因作为关键生物标志物能有效分类不同阶段的伊波拉感染。 |
| [^71] | [Benchmarking the Robustness of Image Watermarks.](http://arxiv.org/abs/2401.08573) | 本研究提出了一种新颖的基准测试方法WAVES，用于评估图像水印的鲁棒性。通过整合检测和识别任务，并进行多样化压力测试，我们揭示了以前未被发现的水印脆弱性。 |
| [^72] | [Robustness Against Adversarial Attacks via Learning Confined Adversarial Polytopes.](http://arxiv.org/abs/2401.07991) | 本文提出了一种通过学习受限对抗多面体来提高深度神经网络对抗攻击的鲁棒性的方法，并通过实验证明了该方法相对于现有对抗鲁棒性方法的有效性。 |
| [^73] | [Learning Explainable and Better Performing Representations of POMDP Strategies.](http://arxiv.org/abs/2401.07656) | 本研究提出了一种学习部分可观测的马尔可夫决策过程（POMDP）策略自动机表示的方法。与传统的表格表示相比，该方法得到的自动机更小更易理解，且在学习过程中可改善策略性能。与其他方法相比，本方法在可扩展性上具有显著优势。 |
| [^74] | [PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar Nonlinear Conservation Laws.](http://arxiv.org/abs/2401.07364) | 本文以一维标量非线性守恒定律为例，详细介绍了使用上下文操作符网络（ICON）解决PDE问题的方法，并展示了ICON模型在没有微调的情况下可以很好地泛化到具有新形式的PDEs。 |
| [^75] | [Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning.](http://arxiv.org/abs/2401.06344) | 本论文提出了Hyper-STTN，一种基于超图的时空转换网络，用于人群轨迹预测。通过构建多尺度超图来捕捉拥挤场景中的群体间相互作用，并利用空间-时间转换器来捕捉行人的成对潜在相互作用。这些异构的群体间和成对间相互作用通过一个多模态转换网络进行融合和对准。 |
| [^76] | [DFU: scale-robust diffusion model for zero-shot super-resolution image generation.](http://arxiv.org/abs/2401.06144) | DFU是一种尺度鲁棒的扩散模型，可以实现零样本超分辨率图像生成，并通过在多个分辨率上训练来提高模型的可扩展性。 |
| [^77] | [DiarizationLM: Speaker Diarization Post-Processing with Large Language Models.](http://arxiv.org/abs/2401.03506) | 本文介绍了DiarizationLM框架，利用大语言模型对说话人分离系统的输出进行后处理。实验证明，使用finetuned的PaLM 2-S模型可以显著减少分离错误率，对多种目标都有优化效果。 |
| [^78] | [Decision Making in Non-Stationary Environments with Policy-Augmented Search.](http://arxiv.org/abs/2401.03197) | 在非稳定环境下的决策制定是一个具有挑战性的问题，本文介绍了一种新的算法--策略增强蒙特卡洛树搜索（PA-MCTS），它将在线搜索与策略学习相结合。 |
| [^79] | [Improving Diffusion-Based Image Synthesis with Context Prediction.](http://arxiv.org/abs/2401.02015) | 本研究提出了一种名为ConPreDiff的方法，通过上下文预测来改善基于扩散的图像合成。在训练阶段，我们使用上下文解码器鼓励每个点预测其邻域上下文，并在推理阶段去除解码器。这种方法能够更好地重建图像。 |
| [^80] | [Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes.](http://arxiv.org/abs/2401.01841) | 本文提出了一种自适应蒙特卡洛树搜索算法来应对非稳态环境下的决策问题，解决了传统方法中对环境动态假设的限制和规划过程的悲观性问题。 |
| [^81] | [Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction.](http://arxiv.org/abs/2401.01084) | 本文开发了一种新的自然策略梯度变体NPG-HM，采用Hessian辅助动量技术进行方差减小，通过随机梯度下降解决子问题。实验证明NPG-HM在通用Fisher非退化策略参数化下可以实现全局最后迭代的$\epsilon$-最优性，并且在Mujoco环境中表现出卓越的性能。 |
| [^82] | [Explainability-Driven Leaf Disease Classification using Adversarial Training and Knowledge Distillation.](http://arxiv.org/abs/2401.00334) | 本研究通过对抗训练来提高植物叶片疾病分类模型对对抗攻击的鲁棒性，并通过可解释性技术获得模型的决策过程，同时通过模型压缩技术提高计算效率。实验证明，鲁棒性可能以分类准确性为代价，而学生模型可以以较低的性能损失蒸馏复杂模型的知识，从而提高计算效率。 |
| [^83] | [Matching of Users and Creators in Two-Sided Markets with Departures.](http://arxiv.org/abs/2401.00313) | 本论文提出了一个双边市场中匹配用户和创作者的模型，并展示了一个以用户为中心的贪心算法可能导致整体参与度下降的问题。 |
| [^84] | [Unraveling the Key Components of OOD Generalization via Diversification.](http://arxiv.org/abs/2312.16313) | 本文研究了通过多样化方法来解决OOD广义化问题，并确定了其关键组件。研究发现，多样化方法对无标签数据的分布非常敏感，且仅仅进行多样化是不足以实现OOD广义化的，学习算法的选择也很重要。 |
| [^85] | [Task-Driven Causal Feature Distillation: Towards Trustworthy Risk Prediction.](http://arxiv.org/abs/2312.16113) | 该论文提出了一种任务驱动的因果特征提取模型（TDCFD），通过将原始特征值转化为因果特征归因来实现可信的风险预测。实验证实了该方法在精确度、召回率和可解释性方面的优势。 |
| [^86] | [Neural Stochastic Differential Equations with Change Points: A Generative Adversarial Approach.](http://arxiv.org/abs/2312.13152) | 本文提出了一种使用神经随机微分方程来建模时间序列的变点检测算法，通过生成对抗网络学习每个变点对应的神经随机微分方程的参数，并通过GAN判别器的输出检测变点。验证结果表明，该算法在合成和真实数据集上的性能优于传统方法。 |
| [^87] | [Augment on Manifold: Mixup Regularization with UMAP.](http://arxiv.org/abs/2312.13141) | 本文提出了一种称为UMAP Mixup的Mixup正则化方案，通过利用统一流形逼近与投影技术，实现了“在流形上”的自动数据增强，提高了深度学习模型的泛化性能。 |
| [^88] | [LRS: Enhancing Adversarial Transferability through Lipschitz Regularized Surrogate.](http://arxiv.org/abs/2312.13118) | 本文提出了一种名为LRS的转移黑盒攻击方法，通过Lipschitz正则化替代模型，使得现有的基于转移的黑盒攻击运行更好，生成更可转移的对抗性样本。 |
| [^89] | [When Model Meets New Normals: Test-time Adaptation for Unsupervised Time-series Anomaly Detection.](http://arxiv.org/abs/2312.11976) | 本论文针对无监督时间序列异常检测中的新常态问题，提出了一种基于趋势估计和自监督学习的测试时适应策略，实验证明该策略能够提高模型性能，增加对分布变化的鲁棒性。 |
| [^90] | [Provably Convergent Federated Trilevel Learning.](http://arxiv.org/abs/2312.11835) | 该论文提出了一种异步联邦三层优化方法来解决层次决策过程中的TLO问题，通过利用μ剖分构建超多面体近似并以异步方式求解，解决了现有TLO工作中的隐私泄露和收敛速度缺乏分析的问题。 |
| [^91] | [Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation.](http://arxiv.org/abs/2312.11532) | 本文介绍了一种利用隐变量码本实现灵活的主题导向文档生成的新方法，通过名为TVQ-VAE的生成式主题模型，可以有效捕捉主题上下文，并支持灵活形式的文档生成。 |
| [^92] | [DeRDaVa: Deletion-Robust Data Valuation for Machine Learning.](http://arxiv.org/abs/2312.11413) | 提出了DeRDaVa：一种适用于机器学习的删除鲁棒数据估值框架。通过在预测删除后保持模型性能的前提下对每个数据源的贡献进行估值，避免了昂贵的重新计算。推广到Risk-DeRDaVa以满足对最坏/最好情况感到风险厌恶/寻求的模型所有者的需求。 |
| [^93] | [Self-Supervised Disentangled Representation Learning for Robust Target Speech Extraction.](http://arxiv.org/abs/2312.10305) | 该论文提出了一种自监督分解表示学习方法，通过逐步分离说话人身份信息和其他无关因素，解决了目标语音提取任务中存在的说话人混叠问题，并使用分解的说话人身份信息来指导语音提取网络。 |
| [^94] | [Towards Optimal Statistical Watermarking.](http://arxiv.org/abs/2312.07930) | 追求最优统计水印技术。通过将统计水印技术视为假设检验问题并引入伪随机生成器，我们实现了输出令牌和拒绝区域的耦合，实现了第一类错误和第二类错误之间的非平凡权衡，同时提出了最统一最有力的水印和最小化第二类错误的解决方案。我们还提供了独立同分布令牌数量的上下界，突显了改进的潜力。此外，我们还探讨了鲁棒性水印问题。 |
| [^95] | [Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating Reinforcement Learning Algorithms.](http://arxiv.org/abs/2312.07178) | 本文提出了一种超越预期回报的评估方法，在强化学习中考虑策略的可复制性。现有的评估方法仅使用预期回报，无法充分考虑分布的扩散，这限制了其在比较策略时的有效性。 |
| [^96] | [Optimal Multi-Distribution Learning.](http://arxiv.org/abs/2312.05134) | 本论文提出了一种最优化多分布学习的方法，通过自适应采样来实现数据高效的学习。针对Vapnik-Chervonenkis (VC)维数为d的假设类，算法可以生成一个ε-最优随机假设，并且样本复杂度与最佳下界保持一致。同时，该算法的思想和理论还被进一步扩展以适应Rademacher类。最终提出的算法是奥拉克尔高效的，仅访问假设类。 |
| [^97] | [On the Nystrom Approximation for Preconditioning in Kernel Machines.](http://arxiv.org/abs/2312.03311) | 本文分析了核机器预处理中使用Nystrom逼近的权衡。研究表明，使用对数大小的样本能够让Nystrom逼近的预处理器几乎与梯度下降同样有效地加速。 |
| [^98] | [Congestion-aware Distributed Task Offloading in Wireless Multi-hop Networks Using Graph Neural Networks.](http://arxiv.org/abs/2312.02471) | 本文提出了一种基于图神经网络的拥塞感知分布式任务卸载方案，通过改进分布式贪心框架，有效减少了无线多跳网络中的拥塞并提升了执行延迟。 |
| [^99] | [ALEXR: An Optimal Single-Loop Algorithm for Convex Finite-Sum Coupled Compositional Stochastic Optimization.](http://arxiv.org/abs/2312.02277) | 本文提出了一种名为ALEXR的高效算法，用于解决凸有限和耦合组成随机优化问题。此算法在解决平滑和非平滑问题时具有优越的收敛速度，并且可应用于多个领域，包括组分布鲁棒优化、不平衡数据学习、强化学习和排序学习。 |
| [^100] | [The GPU Phase Folding and Deep Learning Method for Detecting Exoplanet Transits.](http://arxiv.org/abs/2312.02063) | 本文提出了GPU相位折叠和深度学习方法GPFC，用于探测系外行星凌变。GPFC利用并行化的快速折叠算法放大低信噪比的凌日信号，实现高精度和高速搜索。与主要的BLS方法相比，GPFC的速度提高了三个数量级，准确率为97%。 |
| [^101] | [Universal Backdoor Attacks.](http://arxiv.org/abs/2312.00157) | 通过通用数据污染攻击，可以控制深度图像分类器对任何源类别到任何目标类别的错误分类，而只需增加少量的污染样本。 |
| [^102] | [Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis.](http://arxiv.org/abs/2311.16141) | 本研究受到神经科学中的关键大脑假设的启发，提出了一种基于神经元关键性的高效SNN修剪方法，以加强特征提取和加速修剪过程，并取得了比当前最先进方法更好的性能。 |
| [^103] | [Annotation Sensitivity: Training Data Collection Methods Affect Model Performance.](http://arxiv.org/abs/2311.14212) | 该研究发现训练数据收集方法对注释本身和下游模型性能产生影响。在对仇恨言论和冒犯性语言进行注释收集的实验中，发现注释工具的设计选择会对模型的性能产生明显差异。 |
| [^104] | [Machine-Learned Atomic Cluster Expansion Potentials for Fast and Quantum-Accurate Thermal Simulations of Wurtzite AlN.](http://arxiv.org/abs/2311.11990) | 本论文开发了一种机器学习的原子间势，用于快速而准确地模拟Wurtzite氮化铝的声子传输性质。采用原子团簇展开（ACE）框架，该势的预测能力在多个性质上得到了验证，并通过与密度泛函理论（DFT）计算和实验结果进行比较，展示了其描述非谐振声子相互作用的能力。作者还将该势应用于晶格动力学分析，揭示了正交应变对Wurtzite氮化铝的热导率和声子性质的影响。 |
| [^105] | [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts.](http://arxiv.org/abs/2311.09127) | 通过自对抗攻击和系统提示漏洞，我们发现了GPT-4V中存在的安全风险，并提出了一种名为SASP的新型攻击方法，以搜索潜在的破解提示。我们通过添加人工修改，成功率提高到98.7%。我们还评估了修改系统提示对解锁GPT-4V的影响。 |
| [^106] | [On the Foundation of Distributionally Robust Reinforcement Learning.](http://arxiv.org/abs/2311.09018) | 该论文为分布鲁棒强化学习的理论基础做出了贡献，通过一个综合的建模框架，决策者在最坏情况下的分布转变下选择最优策略，并考虑了各种建模属性和对手引起的转变的灵活性。 |
| [^107] | [Convolve and Conquer: Data Comparison with Wiener Filters.](http://arxiv.org/abs/2311.06558) | 本文介绍了一种基于Wiener滤波器理论的新方法，用于比较数据样本之间的(不)相似性。通过全局相关的卷积方式，我们的方法在多个机器学习应用中得到了验证，并展示了在数据压缩、医学图像填充、翻译分类和非参数生成模型等方面的优越性能。 |
| [^108] | [In-Context Learning for MIMO Equalization Using Transformer-Based Sequence Models.](http://arxiv.org/abs/2311.06101) | 本研究利用上下文学习技术解决了多输入多输出（MIMO）均衡的逆问题，基于任务的上下文中的导频符号和未知的衰落信道以及信噪比（SNR）水平。这种方法展示了在实践中的潜力。 |
| [^109] | [Bayesian Methods for Media Mix Modelling with shape and funnel effects.](http://arxiv.org/abs/2311.05587) | 本研究探索了在媒体组合建模中利用基于物理原理的方程的潜在用途，提出了将马克思-玻尔兹曼方程和米氏方程纳入分层贝叶斯模型的方法，用于分析广告背景下的消费者行为。 |
| [^110] | [Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures.](http://arxiv.org/abs/2311.03242) | 本论文提出了一种使用类似ResNet的神经网络架构来近似Langevin Monte Carlo算法，通过将来自简单参考分布的样本映射到目标分布的样本中来进行采样，具有较好的逼近速度和表达性。 |
| [^111] | [Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects.](http://arxiv.org/abs/2311.02332) | 这项调查研究了多模式机器学习在医学图像分析和临床决策支持系统中的影响，强调了在多模态表示、融合、翻译、对齐和联合学习方面的挑战和创新，以及如何解决数据偏差和“大数据”稀缺性等问题。 |
| [^112] | [Learning Defect Prediction from Unrealistic Data.](http://arxiv.org/abs/2311.00931) | 该论文研究了从不真实的数据集中学习缺陷预测的问题，并提出了一种基于学习表示的方法来识别与真实数据集最相似的子集。 |
| [^113] | [Generator Identification for Linear SDEs with Additive and Multiplicative Noise.](http://arxiv.org/abs/2310.19491) | 本文介绍了从具有给定初始状态的解过程的分布中识别线性随机微分方程（SDE）的发生器的条件，并且提供了对于具有加性和乘性噪声的SDE的识别条件。 |
| [^114] | [Hierarchical Ensemble-Based Feature Selection for Time Series Forecasting.](http://arxiv.org/abs/2310.17544) | 这项研究提出了一种基于层次集成的特征选择方法，能够克服传统方法和最先进方法在非平稳和特征数目庞大且样本有限的情况下的局限性，并在合成和实际数据集上展示了更好的性能。 |
| [^115] | [Learning an Inventory Control Policy with General Inventory Arrival Dynamics.](http://arxiv.org/abs/2310.17168) | 本文解决了学习具有一般库存到货动态下的库存控制策略的问题，同时允许修改订购数量以满足供应商的限制，并将周期性审核库存控制问题定义为外部决策过程。 |
| [^116] | [2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision.](http://arxiv.org/abs/2310.12817) | 本文提出了一种基于场景级监督的2D-3D交错Transformer模型，用于弱监督点云分割。该模型通过两个编码器计算2D和3D数据的自注意特征，并通过交替切换查询和键值对的角色，实现了2D和3D特征的融合。 |
| [^117] | [Physics-guided Noise Neural Proxy for Low-light Raw Image Denoising.](http://arxiv.org/abs/2310.09126) | 本文提出了一种新的物理引导噪声神经代理（PNNP）用于准确噪声建模和低光原始图像去噪，集成了物理引导噪声解耦、物理引导代理模型和可微分分布导向损失等高效技术。 |
| [^118] | [Learning bounded-degree polytrees with known skeleton.](http://arxiv.org/abs/2310.06333) | 本论文提出了一种高效学习已知骨架的有界度多树的算法，并给出了在多项式时间和样本复杂度内的有限样本保证。这对于复杂概率分布的学习具有重要意义。 |
| [^119] | [A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling.](http://arxiv.org/abs/2310.03298) | 提出了一种基于潜变量的方法，用于非层次化多保真度自适应采样。该方法能够利用不同保真度模型之间的相关性以更高效地探索和利用设计空间。 |
| [^120] | [MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models.](http://arxiv.org/abs/2310.02255) | 本论文提出了MathVista，这是一个评估视觉场景中数学推理能力的基准测试。通过对12个著名的基础模型进行全面的定量评估，发现最好的GPT-4V模型相对于第二名的Bard模型在准确率上提升了15.1%。 |
| [^121] | [GenSim: Generating Robotic Simulation Tasks via Large Language Models.](http://arxiv.org/abs/2310.01361) | GenSim通过利用大型语言模型自动生成丰富的模拟环境和专家示范，解决了目前模拟数据中缺乏任务级别多样性的问题，提高了机器人策略在任务级别上的泛化能力。 |
| [^122] | [Leveraging Optimization for Adaptive Attacks on Image Watermarks.](http://arxiv.org/abs/2309.16952) | 该论文提出了一种利用优化技术进行对图像水印的自适应攻击的方法，通过自适应地生成替代密钥来复制秘密水印密钥，并通过优化问题的解决方法进行攻击优化。 |
| [^123] | [Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale Localization.](http://arxiv.org/abs/2309.16034) | 本论文研究了基于原始数据的体内纳米尺度定位的分析建模，分析了纳米设备的通信和能源约束对定位性能的影响。 |
| [^124] | [DTC: Deep Tracking Control -- A Unifying Approach to Model-Based Planning and Reinforcement-Learning for Versatile and Robust Locomotion.](http://arxiv.org/abs/2309.15462) | 本文提出了一种混合控制架构，同时结合了基于模型的规划和基于强化学习的方法，用于解决腿式运动的复杂控制问题。这种方法具有精确性、鲁棒性以及对稀疏奖励环境的适应能力。 |
| [^125] | [Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs.](http://arxiv.org/abs/2309.13365) | 该论文研究了在IBMDP中使用Actor-Critic算法学习决策树策略的局限性。结果表明，即使是在简单的玩具任务上，深度RL也可能失败。 |
| [^126] | [Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming.](http://arxiv.org/abs/2309.12701) | 本文提出了一种使用动态规划找到最优决策树的方法，可以得到多个可解释性-性能权衡的最优决策树，使用户可以根据自己的需求选择最适合的树。 |
| [^127] | [Federated Learning with Neural Graphical Models.](http://arxiv.org/abs/2309.11680) | 本研究提出了一种名为FedNGMs的联邦学习框架，利用概率神经图模型来处理多个客户端的数据，并在保持训练数据私密性的同时提升模型准确性。 |
| [^128] | [On the different regimes of Stochastic Gradient Descent.](http://arxiv.org/abs/2309.10688) | 这项研究解决了对于随机梯度下降（SGD）中不同模式的追踪和理解的问题，提供了一个相位图来区分噪声主导的SGD和大步骤主导的SGD。 |
| [^129] | [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning.](http://arxiv.org/abs/2309.05173) | DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。 |
| [^130] | [Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio.](http://arxiv.org/abs/2309.03202) | 本研究评估了在S&P 500指数上使用强化学习技术进行多样化投资组合的可行性。研究发现，包含COVID-19时期的市场数据在训练数据集中可以提供更好的性能，并且基于策略的方法（VI和SARSA）在测试中表现优于Q学习。 |
| [^131] | [Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions in Sichuan Province.](http://arxiv.org/abs/2309.01115) | 本研究使用机器学习方法解决了多重共线性问题，针对四川省的碳排放情况进行了案例研究。研究结果确定了行业分组，评估了排放驱动因素，并提出了科学的减排策略，以改善情况。 |
| [^132] | [Learned Visual Features to Textual Explanations.](http://arxiv.org/abs/2309.00733) | 本研究提出了一种名为TExplain的方法，将大型语言模型与预训练图像分类器的特征空间连接起来，通过生成解释性句子来理解分类器学习到的特征。该方法首次利用这些频繁单词揭示出分类器的决策过程，实现了检测虚假特征的能力。 |
| [^133] | [Federated Fine-tuning of Billion-Sized Language Models across Mobile Devices.](http://arxiv.org/abs/2308.13894) | 这项工作引入了一种创新的FL协议FwdLLM，旨在提高在移动设备上进行十亿规模语言模型的联邦微调（FedLLM）的效率。FwdLLM通过使用无反向传播（BP）训练方法以及“扰动推断”来提高内存效率和时间效率。 |
| [^134] | [Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models.](http://arxiv.org/abs/2308.13507) | 通过在生成代码之前提问澄清问题，大型语言模型的代码生成能力可以得到提升，增加了对生成代码的信心。 |
| [^135] | [Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape.](http://arxiv.org/abs/2308.11737) | Animal3D是一个全面的哺乳动物3D姿态和形状数据集，通过手工标注和检查确保了高质量的结果。通过Animal3D数据集，我们对形状和姿态估计模型进行了基准测试，包括使用只有Animal3D数据的监督学习、合成到真实的转移和微调人体姿势和形状估计模型。 |
| [^136] | [Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction.](http://arxiv.org/abs/2308.09647) | 这篇论文介绍了一种名为MC-CP的新型混合不确定性量化方法，通过将自适应蒙特卡洛dropout方法与合规预测相结合，实现了节省资源和产生鲁棒预测集/区间的目标。实验证明MC-CP在分类任务中相比其他先进方法具有显著提升 |
| [^137] | [Latent State Models of Training Dynamics.](http://arxiv.org/abs/2308.09543) | 这项研究使用隐马尔可夫模型解释了神经网络训练过程中随机性的影响，并提供了对训练动态的直观概述。 |
| [^138] | [Multiclass Online Learnability under Bandit Feedback.](http://arxiv.org/abs/2308.04620) | Bandit反馈下的在线多类学习的关键在于Bandit Littlestone维度的有限性，无论标签空间是否无界。 |
| [^139] | [FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning.](http://arxiv.org/abs/2307.13716) | FedDRL是一种分阶段强化学习的联邦学习模型融合方法，解决了传统方法中无法解决的客户端模型质量和恶意模型问题。 |
| [^140] | [Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching.](http://arxiv.org/abs/2307.13158) | 本文提出了一种深度强化学习方法，用于优化多无人机在三维空中高速公路上的小区关联决策和移动速度，以提升交通和通信性能。仿真结果显示，性能提高了18.32%。 |
| [^141] | [A DPLL(T) Framework for Verifying Deep Neural Networks.](http://arxiv.org/abs/2307.10266) | 这项工作介绍了一个新的约束求解方法NeuralSAT，用于验证深度神经网络。Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art. |
| [^142] | [EasyTPP: Towards Open Benchmarking Temporal Point Processes.](http://arxiv.org/abs/2307.08097) | EasyTPP是第一个关于事件序列建模领域的中心资源库，提供统一的数据集使用界面和广泛的评估程序，解决了该领域缺乏标准化的问题，推动了研究和应用的进展。 |
| [^143] | [Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach.](http://arxiv.org/abs/2307.06564) | 本论文提出了一种在资源限制下进行处方过程监控的强化学习方法。通过考虑对干预需求、及时性或效果预测的不确定性和资源利用水平，来触发干预，从而优化业务过程的性能。 |
| [^144] | [Fairness-aware Federated Minimax Optimization with Convergence Guarantee.](http://arxiv.org/abs/2307.04417) | 本文提出了一种名为FFALM的算法，通过施加公平约束和解决极小化极大回归问题，在联邦学习中解决了群体公平性问题。实验证明FFALM在处理严重统计异质性问题时具有良好的效果。 |
| [^145] | [Look, Remember and Reason: Visual Reasoning with Grounded Rationales.](http://arxiv.org/abs/2306.17778) | 在这项研究中，我们借鉴了人类视觉问题解决的方法，通过三个步骤（看、记住、推理）逐步提取视觉信息来解决复杂的视觉推理问题，从而使大型语言模型能够有效解决这些问题。 |
| [^146] | [Moreau Envelope Based Difference-of-weakly-Convex Reformulation and Algorithm for Bilevel Programs.](http://arxiv.org/abs/2306.16761) | 本文提出了一种基于Moreau包络的弱凸差分重构与双层规划算法，可以适用于更多与机器学习和统计相关的应用。 |
| [^147] | [Logarithmic Bayes Regret Bounds.](http://arxiv.org/abs/2306.09136) | 该论文提出了对于贝叶斯赌博机的首个有限时间对数遗憾边界，并用于高斯和线性赌博机，从而阐明了贝叶斯设置中先验价值以及对$\tilde{O}(\sqrt{n})$界限的改善。 |
| [^148] | [Adversarial Attack On Yolov5 For Traffic And Road Sign Detection.](http://arxiv.org/abs/2306.06071) | 本文研究了YOLOv5在交通和道路标志检测中的对抗攻击脆弱性，发现YOLOv5易受多种攻击的影响，有 important implications for the safety and reliability of object detection algorithms used in traffic. |
| [^149] | [Neural Algorithmic Reasoning for Combinatorial Optimisation.](http://arxiv.org/abs/2306.06064) | 本文提出了一种用于组合优化问题的神经算法推理方法，旨在解决旅行商问题。该方法是通过在TSP实例训练之前，将神经模型用相关算法进行预训练来实现的。实验结果表明，该方法可以显著提高TSP问题的解决效率。 |
| [^150] | [Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders.](http://arxiv.org/abs/2306.05023) | 本文研究了高度相似的变分后验分布和先验分布之间的后验崩溃现象，特别地，通过对线性条件VAE和分层VAE进行分析，证明了这种现象是由于潜在变量层次关系不清晰而引起的。 |
| [^151] | [Data-Driven Regret Balancing for Online Model Selection in Bandits.](http://arxiv.org/abs/2306.02869) | 论文讨论在具有赌博反馈的随机环境中进行选择，提出了两种基于数据的模型选择算法，并证明了其保证。通过利用实际遗憾，这些算法在实际中取得了好效果。 |
| [^152] | [Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network.](http://arxiv.org/abs/2306.01631) | 本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。 |
| [^153] | [Transfer learning for atomistic simulations using GNNs and kernel mean embeddings.](http://arxiv.org/abs/2306.01589) | 本论文提出了一种传递学习算法，利用图神经网络和核均值嵌入在原子模拟中学习了势能表面。该方法在现实数据集上表现良好，展现出较好的可概括性和可转移性能。 |
| [^154] | [Medication Recommendation via Domain Knowledge Informed Deep Learning.](http://arxiv.org/abs/2305.19604) | 提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。 |
| [^155] | [On Optimal Regularization Parameters via Bilevel Learning.](http://arxiv.org/abs/2305.18394) | 本文提出了一个利用双层学习确定正则化参数的方法，并提出了一个新的表征正确参数的条件。 |
| [^156] | [Better Batch for Deep Probabilistic Time Series Forecasting.](http://arxiv.org/abs/2305.17028) | 该研究提出了一种新的训练方法，通过在 mini-batch 中显式地学习误差的序列相关性，来提高深度概率时间序列预测的准确性和不确定性量化。 |
| [^157] | [DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models.](http://arxiv.org/abs/2305.16943) | DiffusionNAG是一种基于扩散模型的神经结构生成方法，通过考虑神经结构的有向图特性，并结合参数化的预测器的指导，可以更高效地生成具有期望特性的任务最优结构。 |
| [^158] | [Modulate Your Spectrum in Self-Supervised Learning.](http://arxiv.org/abs/2305.16789) | 本文提出了谱变换（ST）框架，可以调节自监督学习的频谱，并避免特征崩溃。其中，INTL是ST的一个实例，能够在优化过程中将嵌入的频谱调节到一个等特征值分布，实现较高的准确率。 |
| [^159] | [Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations.](http://arxiv.org/abs/2305.16326) | 本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。 |
| [^160] | [Manifold Diffusion Fields.](http://arxiv.org/abs/2305.15586) | 流形扩散场是一种在黎曼流形上生成连续函数的方法，可以使用特征函数定义流形上的内在坐标系，并且使用多个输入输出对表示函数。相比以往的方法，其能够更好地捕捉这些函数的分布，具有更好的多样性和保真度。 |
| [^161] | [Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation.](http://arxiv.org/abs/2305.14189) | 本文提出了一种超越共享词汇的方法，通过定义词级信息传输路径和使用图网络来融合跨语言的词嵌入，实现了在多语言机器翻译中提高相似含义词的对齐性和BLEU分数的一致提升。此方法只需要少量额外参数且计算成本增加有限，并且推理时间与基线相同。 |
| [^162] | [EXACT: Extensive Attack for Split Learning.](http://arxiv.org/abs/2305.12997) | 本文提出了一种名为EXACT的方法，可以安全地在分布式学习中进行梯度交换，同时保护隐私、保持准确性和效率性。 |
| [^163] | [Theoretical Analysis of Inductive Biases in Deep Convolutional Networks.](http://arxiv.org/abs/2305.08404) | 本文研究深卷积神经网络（CNN）中的归纳偏置，证明了$\mathcal{O}(\log d)$的深度就足以实现普适性，用CNN学习稀疏函数只需要$\tilde{\mathcal{O}}(\log^2d)$个样本。同时，通过局部连接网络（LCN）分析了权重共享和局部性的归纳偏置的区别，得出了它们在表示需要有限平移等变和高方向选择性的函数方面的优越性。 |
| [^164] | [Explaining RL Decisions with Trajectories.](http://arxiv.org/abs/2305.04073) | 本文提出一种用训练过程中遇到的轨迹解释强化学习决策的方法，并在离散和连续状态及行动空间的多样化环境中证明了其有效性。 |
| [^165] | [Exploring the Effectiveness of Large Language Models in Generating Unit Tests.](http://arxiv.org/abs/2305.00418) | 本文研究了三种生成模型在单元测试生成方面的效果，并发现在不经过微调的情况下，它们的覆盖率较低且存在测试味道问题。 |
| [^166] | [What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement.](http://arxiv.org/abs/2303.11249) | 本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。 |
| [^167] | [Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm.](http://arxiv.org/abs/2303.07287) | 本文提出了一种通过最大化一系列归一化矩来使用子高斯内在矩范实现紧凑的非渐进推断的方法，该方法可以导致更紧的Hoeffding子高斯浓度不等式，并且可以通过子高斯图检查具有有限样本大小的子高斯数据。 |
| [^168] | [Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning.](http://arxiv.org/abs/2303.05479) | 本文介绍了一种计算机辅助脱机强化学习方法Cal-QL，该方法能够从脱机数据中学习一个保守的值函数初始化，使得在在线微调时同时保障了快速和性能。 |
| [^169] | [Bayesian Matrix Decomposition and Applications.](http://arxiv.org/abs/2302.11337) | 本书旨在介绍贝叶斯矩阵分解的概念和工具，并总结了贝叶斯矩阵分解方法在不同领域的应用。 |
| [^170] | [Swap Agnostic Learning, or Characterizing Omniprediction via Multicalibration.](http://arxiv.org/abs/2302.06726) | 本文介绍了无关交换学习的概念，并展示了其在任何凸损失函数下的可行性。此外，研究发现全预测和多校准之间存在联系，全预测是一种新的最优性概念，可以加强经典的无关学习。 |
| [^171] | [Exploring Randomly Wired Neural Networks for Climate Model Emulation.](http://arxiv.org/abs/2212.03369) | 本研究探索了随机连接的神经网络在气候模型仿真中的应用。通过使用ClimateBench数据集，我们发现随机连接的神经网络在拥有100万和1000万参数的模型上具有与传统网络相当的性能，且能耗更低。 |
| [^172] | [Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning.](http://arxiv.org/abs/2212.01168) | 通过元学习方法，在哈密顿流形中识别出普遍的神经表示，实现了对不同物理系统的快速适应能力。 |
| [^173] | [HashVFL: Defending Against Data Reconstruction Attacks in Vertical Federated Learning.](http://arxiv.org/abs/2212.00325) | 【论文标题翻译】HashVFL: 在垂直联合学习中引入哈希算法以防御数据重构攻击。我们的工作表明哈希算法是对抗数据重构攻击的一种有希望的解决方案。 |
| [^174] | [On the Sample Complexity of Two-Layer Networks: Lipschitz vs. Element-Wise Lipschitz Activation.](http://arxiv.org/abs/2211.09634) | 本文研究了使用不同激活函数的有界两层神经网络的样本复杂度，证明了当激活函数为逐元素Lipschitz时，样本复杂度在宽度上仅有对数依赖，并且这种依赖是紧致的。 |
| [^175] | [Orthogonal Polynomials Approximation Algorithm (OPAA):a functional analytic approach to estimating probability densities.](http://arxiv.org/abs/2211.08594) | OPAA是一种功能分析方法的算法，通过找到平滑的概率分布函数估计值、计算归一化权重的估计值，并使用特殊的函数空间转换来估计证据，实现了并行计算的一次通过。它适用于估计概率密度函数，尤其在贝叶斯问题中估计归一化权重。 |
| [^176] | [Transfer learning with affine model transformation.](http://arxiv.org/abs/2210.09745) | 本文提出了一种叫做仿射模型转移的迁移学习方法，通过最小化期望平方损失，可以适应各种不同的方法，包括基于神经特征提取器的方法。对于这个方法也给出了理论上的解释。 |
| [^177] | [ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks.](http://arxiv.org/abs/2210.00108) | ImpNet是一种在编译神经网络中添加的不可察觉和黑盒不可检测的后门攻击。这些后门可以绕过数据准备和模型训练阶段的保护措施，并且只能在插入阶段可靠地检测到，移除它们很具有挑战性。 |
| [^178] | [Task Formulation Matters When Learning Continually: A Case Study in Visual Question Answering.](http://arxiv.org/abs/2210.00044) | 这项研究在连续学习领域中深入研究了任务的不同表述方式对视觉问答任务的性能的影响，发现输出分布的变化是性能和任务顺序敏感度的关键，同时探讨了预训练模型和不同视觉嵌入的Transformer模型的稳健性及模型表示对遗忘的影响。 |
| [^179] | [High-Frequency Space Diffusion Models for Accelerated MRI.](http://arxiv.org/abs/2208.05481) | 本文提出了一种针对高频空间扩散过程的MR重建的新型SDE模型（HFS-SDE），该模型确保了低频区域的确定性并加速了逆扩散采样过程，实验结果表明该方法在加速MRI重建中表现出良好的性能。 |
| [^180] | [Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution.](http://arxiv.org/abs/2208.04957) | 本研究首次研究了异构零样本协同问题，并提出了一种基于协同进化的通用方法，通过配对、更新和选择的过程，实现了多智能体零样本协同。实验结果表明，考虑异构情况的必要性，并证明了该方法对于异构零样本协同任务的有前景的解决方案。 |
| [^181] | [Deep Reinforcement Learning with Swin Transformers.](http://arxiv.org/abs/2206.15269) | 本文介绍了基于 Swin Transformer 的在线强化学习方案 Swin DQN，通过将组合的图像像素分成小的补丁并在局部应用自我注意力操作，实现了在 Atari 基准测试上超越现有基于 CNN 的强化学习方法的最先进性能。 |
| [^182] | [Using Twitter Data to Understand Public Perceptions of Approved versus Off-label Use for COVID-19-related Medications.](http://arxiv.org/abs/2206.14358) | 通过Twitter数据分析了解公众对COVID-19相关药物的批准和离标使用的看法。Hydroxychloroquine和Ivermectin比Molnupiravir和Remdesivir的讨论更多，时间趋势分析和内容分析揭示了人们对每种药物立场的可能理由。 |
| [^183] | [The Manifold Scattering Transform for High-Dimensional Point Cloud Data.](http://arxiv.org/abs/2206.10078) | 多样性散射变换是一种用于高维点云数据的深度特征提取器，在实现上采用了扩散映射理论，有效用于信号分类和流形分类任务。 |
| [^184] | [SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners.](http://arxiv.org/abs/2205.14540) | SupMAE是一种监督式掩膜自编码器，通过添加监督分类分支扩展了MAE，并从黄金标签中有效学习全局特征。通过实验证明，SupMAE在训练效率、特征的鲁棒性和迁移学习性能等方面都表现出了优势。 |
| [^185] | [Towards Size-Independent Generalization Bounds for Deep Operator Nets.](http://arxiv.org/abs/2205.11359) | 本论文研究了深度操作器网络的泛化界限问题，在一类DeepONets中证明了它们的Rademacher复杂度的界限不会随网络宽度扩展而明确变化，并利用这个结果展示了如何选择Huber损失来获得不明确依赖于网络大小的泛化误差界限。 |
| [^186] | [Self-Supervised Anomaly Detection: A Survey and Outlook.](http://arxiv.org/abs/2205.05173) | 自监督学习的出现引发了新型异常检测算法的发展，其表现优于现有的最先进方法。本文全面综述了当前自监督异常检测方法的技术细节，并讨论了它们的优势和缺点，同时比较了这些模型与其他自监督异常检测模型以及最先进的异常检测模型的性能。 |
| [^187] | [Statistical-Computational Trade-offs in Tensor PCA and Related Problems via Communication Complexity.](http://arxiv.org/abs/2204.07526) | 本文通过通信复杂度推导出了对于内存受限算法在张量主成分分析中的计算下界，并且指定了解决该问题的算法必须在数据样本经过次数、样本大小和所需内存之间进行权衡。这些下界暗示了许多常用算法在样本大小不够大时需要更多的迭代次数。 |
| [^188] | [The Concordance Index decomposition: A measure for a deeper understanding of survival prediction models.](http://arxiv.org/abs/2203.00144) | 本文提出了一种将Concordance指数分解成两个部分的方法，用于评估生存预测模型的性能。该分解方法可以进行更细粒度的分析，揭示不同预测方法之间的优劣。实验证明，深度学习模型更好地利用了观测事件。 |
| [^189] | [High-dimensional Inference and FDR Control for Simulated Markov Random Fields.](http://arxiv.org/abs/2202.05612) | 本文提出了一种在高维背景下进行模拟马尔可夫随机场统计推断的方法，实现了一致性，并构建了两种误发现率控制程序。 |
| [^190] | [Thundernna: a white box adversarial attack.](http://arxiv.org/abs/2111.12305) | 本研究提出了一种名为Thundernna的白盒对抗攻击方法，通过开发一种一阶优化方法，该方法在攻击神经网络时具有更高的成功率和更快的速度。 |
| [^191] | [New Versions of Gradient Temporal Difference Learning.](http://arxiv.org/abs/2109.04033) | 本文提出了基于梯度时差学习(GTD)的新版本，通过凸凹鞍点解释统一了所有GTD算法，并提供了简单的稳定性分析。还通过数值比较分析对这些方法进行了评估。 |
| [^192] | [Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding.](http://arxiv.org/abs/2109.01636) | 研究开发了一种分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息，实验表明将词的特异性融入NER方法可提高NER的性能。 |
| [^193] | [DASVDD: Deep Autoencoding Support Vector Data Descriptor for Anomaly Detection.](http://arxiv.org/abs/2106.05410) | DASVDD是一种深度自编码支持向量数据描述符，用于半监督异常检测。它联合学习自编码器参数，并最小化潜在表示中超球体的体积，通过组合自编码器的重构误差和超球体中心的距离来计算异常分数，并解决了超球体坍塌问题。 |
| [^194] | [MNL-Bandit with Knapsacks: a near-optimal algorithm.](http://arxiv.org/abs/2106.01135) | 这篇论文介绍了一种解决动态商品选择问题的算法，通过使用近似最优策略，可在未知需求情况下最大化总体预期收入。在大库存环境下，该算法能够接近最优解。 |
| [^195] | [Wavelet Networks: Scale-Translation Equivariant Learning From Raw Time-Series.](http://arxiv.org/abs/2006.05259) | 本文介绍了一种利用时间序列固有对称性构建的小波网络，其表现出嵌套的非线性小波样的时频变换，实验证明其在原始波形上优于传统的CNN。 |

# 详细

[^1]: DACR：分布增强对比重建用于时间序列异常检测

    DACR: Distribution-Augmented Contrastive Reconstruction for Time-Series Anomaly Detection. (arXiv:2401.11271v1 [cs.LG])

    [http://arxiv.org/abs/2401.11271](http://arxiv.org/abs/2401.11271)

    DACR提出了分布增强对比重建（DACR）方法，通过生成额外的数据来压缩正常数据的表示空间，并通过对比学习来增强特征提取器，同时利用注意力机制建模多变量时间序列特征之间的语义依赖关系，从而实现更强大的时间序列异常检测重建。

    

    时间序列数据的异常检测对于识别故障、失败、威胁和异常值在各种应用中至关重要。最近，深度学习技术已被应用于此主题，但它们在复杂和高度动态的实际场景中往往面临困难，例如正常数据可能由多个分布组成，不同程度的异常可能与正常数据不同。在这项工作中，为了解决这些挑战，我们提出了分布增强对比重建（DACR）。DACR生成与正常数据分布不相交的额外数据来压缩正常数据的表示空间，并通过对比学习增强特征提取器，以更好地捕获时间序列数据的内在语义。此外，DACR采用注意力机制来建模多变量时间序列特征之间的语义依赖关系，从而实现更强大的异常检测重建。大量实验证明

    Anomaly detection in time-series data is crucial for identifying faults, failures, threats, and outliers across a range of applications. Recently, deep learning techniques have been applied to this topic, but they often struggle in real-world scenarios that are complex and highly dynamic, e.g., the normal data may consist of multiple distributions, and various types of anomalies may differ from the normal data to different degrees. In this work, to tackle these challenges, we propose Distribution-Augmented Contrastive Reconstruction (DACR). DACR generates extra data disjoint from the normal data distribution to compress the normal data's representation space, and enhances the feature extractor through contrastive learning to better capture the intrinsic semantics from time-series data. Furthermore, DACR employs an attention mechanism to model the semantic dependencies among multivariate time-series features, thereby achieving more robust reconstruction for anomaly detection. Extensive 
    
[^2]: 基于高斯混合模型和负高斯混合梯度的扩散模型条件设计

    Diffusion Model Conditioning on Gaussian Mixture Model and Negative Gaussian Mixture Gradient. (arXiv:2401.11261v1 [cs.LG])

    [http://arxiv.org/abs/2401.11261](http://arxiv.org/abs/2401.11261)

    本研究提出了一种利用高斯混合模型作为特征条件来引导扩散模型的去噪过程的条件机制，并通过实验证实了基于特征的条件潜在分布能够产生更少的缺陷生成。

    

    扩散模型（DMs）是一种对图像合成和其他领域有巨大影响的生成模型。它们在各种生成任务中取得了最先进的生成结果。可以使用各种多样的条件输入，如文本或边界框，来控制生成过程。本研究中，我们提出了一种使用高斯混合模型（GMM）作为特征条件来引导去噪过程的条件机制。基于集合论，我们提供了一种全面的理论分析，表明基于特征和类别的条件潜在分布显著不同，因此基于特征的条件潜在分布比基于类别的条件潜在分布产生更少的缺陷生成。分别训练了两个基于高斯混合模型的扩散模型进行比较。实验证实了我们的发现。我们提出了一种新的梯度函数，称为负高斯混合梯度（NGMG），并应用于扩散模型训练中。

    Diffusion models (DMs) are a type of generative model that has a huge impact on image synthesis and beyond. They achieve state-of-the-art generation results in various generative tasks. A great diversity of conditioning inputs, such as text or bounding boxes, are accessible to control the generation. In this work, we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as feature conditioning to guide the denoising process. Based on set theory, we provide a comprehensive theoretical analysis that shows that conditional latent distribution based on features and classes is significantly different, so that conditional latent distribution on features produces fewer defect generations than conditioning on classes. Two diffusion models conditioned on the Gaussian mixture model are trained separately for comparison. Experiments support our findings. A novel gradient function called the negative Gaussian mixture gradient (NGMG) is proposed and applied in diffusion model tr
    
[^3]: 自动融合多模态电子健康记录以提升医疗预测准确性

    Automated Fusion of Multimodal Electronic Health Records for Better Medical Predictions. (arXiv:2401.11252v1 [cs.LG])

    [http://arxiv.org/abs/2401.11252](http://arxiv.org/abs/2401.11252)

    该论文提出了一种自动融合多模态电子健康记录的神经架构搜索框架，用于改进医疗预测准确性。该框架可以自动搜索用于编码不同输入模态和融合策略的最佳模型架构，并在实验中取得了超越传统方法的效果。

    

    在医疗机构广泛采用电子健康记录（EHR）系统产生了大量医疗数据，为通过深度学习技术改进医疗服务提供了重要机会。然而，真实世界的EHR数据中复杂多样的模态和特征结构给深度学习模型设计带来了巨大挑战。为了解决EHR数据中的多模态问题，当前的方法主要依靠基于直觉和经验的手工模型架构，导致子优模型架构和有限性能。因此，为了自动化EHR数据挖掘模型设计的过程，我们提出了一种名为AutoFM的新颖神经架构搜索（NAS）框架，该框架能够自动搜索用于编码不同输入模态和融合策略的最佳模型架构。我们在真实的多模态EHR数据和预测任务上进行了全面的实验，实验证明了我们的方法的有效性和性能超越了传统方法。

    The widespread adoption of Electronic Health Record (EHR) systems in healthcare institutes has generated vast amounts of medical data, offering significant opportunities for improving healthcare services through deep learning techniques. However, the complex and diverse modalities and feature structures in real-world EHR data pose great challenges for deep learning model design. To address the multi-modality challenge in EHR data, current approaches primarily rely on hand-crafted model architectures based on intuition and empirical experiences, leading to sub-optimal model architectures and limited performance. Therefore, to automate the process of model design for mining EHR data, we propose a novel neural architecture search (NAS) framework named AutoFM, which can automatically search for the optimal model architectures for encoding diverse input modalities and fusion strategies. We conduct thorough experiments on real-world multi-modal EHR data and prediction tasks, and the results 
    
[^4]: AFS-BM:通过自适应特征选择和二值屏蔽增强模型性能

    AFS-BM: Enhancing Model Performance through Adaptive Feature Selection with Binary Masking. (arXiv:2401.11250v1 [cs.LG])

    [http://arxiv.org/abs/2401.11250](http://arxiv.org/abs/2401.11250)

    AFS-BM通过联合优化实现了自适应特征选择和模型训练，提高了模型准确性并减少了计算需求。

    

    我们研究了机器学习领域中特征选择的问题，这是该领域中最关键的主题之一。尽管存在许多特征选择方法，但是这些方法面临可扩展性、处理高维数据、处理相关特征、适应可变特征重要性和整合领域知识等挑战。为了解决这些问题，我们引入了“自适应特征选择和二值屏蔽”(AFS-BM)。AFS-BM通过联合优化来同时进行特征选择和模型训练。具体而言，我们通过联合优化和二值屏蔽，在训练过程中持续调整特征集和模型参数。这种方法显著提高了模型的准确性，并减少了计算需求。我们进行了一系列的实验证明，将AFS-BM与已有的特征选择方法进行了比较。

    We study the problem of feature selection in general machine learning (ML) context, which is one of the most critical subjects in the field. Although, there exist many feature selection methods, however, these methods face challenges such as scalability, managing high-dimensional data, dealing with correlated features, adapting to variable feature importance, and integrating domain knowledge. To this end, we introduce the ``Adaptive Feature Selection with Binary Masking" (AFS-BM) which remedies these problems. AFS-BM achieves this by joint optimization for simultaneous feature selection and model training. In particular, we do the joint optimization and binary masking to continuously adapt the set of features and model parameters during the training process. This approach leads to significant improvements in model accuracy and a reduction in computational requirements. We provide an extensive set of experiments where we compare AFS-BM with the established feature selection methods usin
    
[^5]: 缩小TD学习和监督学习之间的差距--从一般化的角度来看

    Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View. (arXiv:2401.11237v1 [cs.LG])

    [http://arxiv.org/abs/2401.11237](http://arxiv.org/abs/2401.11237)

    这篇论文研究了强化学习方法在训练过程中将不同经验片段组合起来解决未见过的任务的属性，并通过分析发现这种组合属性与组合泛化有关。

    

    一些强化学习算法可以将经验片段组合起来解决训练过程中从未见过的任务。这种经常被追求的特性是基于动态规划的强化学习方法与基于监督学习的强化学习方法之间的几种区别之一。然而，某些基于现成监督学习算法的强化学习方法在没有明确的组合机制的情况下，也能取得出色的结果；目前尚不清楚这些方法是否放弃了这种重要的组合特性。本文研究了即将达到目标状态和达到目标回报值的问题，我们的主要结果是展示了组合特性对应了一种组合泛化：在(state, goal)对的分布上进行训练后，希望在训练数据中没有同时出现的(state, goal)对上进行评估。我们的分析表明，这种组合泛化与i.i.d.泛化是不同的。这种连接将组合特性与泛化联系在一起。

    Some reinforcement learning (RL) algorithms can stitch pieces of experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which RL methods based on dynamic-programming differ from RL methods based on supervised-learning (SL). Yet, certain RL methods based on off-the-shelf SL algorithms achieve excellent results without an explicit mechanism for stitching; it remains unclear whether those methods forgo this important stitching property. This paper studies this question for the problems of achieving a target goal state and achieving a target return value. Our main result is to show that the stitching property corresponds to a form of combinatorial generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data. Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching 
    
[^6]: TreeMIL：一个用于带有不精确监督的时间序列异常检测的多实例学习框架

    TreeMIL: A Multi-instance Learning Framework for Time Series Anomaly Detection with Inexact Supervision. (arXiv:2401.11235v1 [cs.LG])

    [http://arxiv.org/abs/2401.11235](http://arxiv.org/abs/2401.11235)

    本文提出了一个基于树的多实例学习框架(TreeMIL)，用于带有不精确监督的时间序列异常检测。该框架通过将整个序列分解成多个节点，并提取子序列特征，旨在解决集体异常的挑战。

    

    时间序列异常检测在医疗、网络和工业等各个领域扮演着重要的角色。考虑到标签对于检测来说是至关重要但很难获得，我们转而研究带有不精确监督的时间序列异常检测：在训练阶段只提供序列级别的标签，在测试阶段预测出点级别的异常。之前的研究采用传统的多实例学习(MIL)方法，重点是鼓励在各个时间步骤中得到高的异常分数。然而，时间序列的异常不仅限于个别点异常，它们还可以是集体异常，通常在子序列中展示出异常模式。为了应对集体异常的挑战，本文提出了一种基于树的MIL框架(TreeMIL)。我们首先采用一个N叉树结构将整个序列分成多个节点，不同层级的节点表示具有不同长度的子序列。然后，子序列特征被提取来预测节点的异常得分。

    Time series anomaly detection (TSAD) plays a vital role in various domains such as healthcare, networks, and industry. Considering labels are crucial for detection but difficult to obtain, we turn to TSAD with inexact supervision: only series-level labels are provided during the training phase, while point-level anomalies are predicted during the testing phase. Previous works follow a traditional multi-instance learning (MIL) approach, which focuses on encouraging high anomaly scores at individual time steps. However, time series anomalies are not only limited to individual point anomalies, they can also be collective anomalies, typically exhibiting abnormal patterns over subsequences. To address the challenge of collective anomalies, in this paper, we propose a tree-based MIL framework (TreeMIL). We first adopt an N-ary tree structure to divide the entire series into multiple nodes, where nodes at different levels represent subsequences with different lengths. Then, the subsequence fe
    
[^7]: 一种传输学习和物理信息建模的混合方法：提高工业废水处理厂溶解氧浓度预测的准确性

    A Hybrid Approach of Transfer Learning and Physics-Informed Modeling: Improving Dissolved Oxygen Concentration Prediction in an Industrial Wastewater Treatment Plant. (arXiv:2401.11217v1 [cs.LG])

    [http://arxiv.org/abs/2401.11217](http://arxiv.org/abs/2401.11217)

    本论文提出了一种传输学习和物理信息建模的混合方法，通过将其他任务中的知识转移到目标任务中，以提高工业废水处理厂溶解氧浓度预测的准确性。这种方法结合了开源模型的物理知识和另一个工业厂的数据，并使训练问题的目标函数具备物理信息建模的特点。

    

    构建非线性和复杂系统（如废水处理单元）的第一原理模型是一项具有挑战性的任务。近年来，数据驱动模型被广泛用于解决复杂性问题。然而，它们常常面临着缺失、低质量或噪声数据等问题。传输学习是解决这个问题的一种方法，它将来自另一个任务的知识传输到目标任务中，以提高预测性能。本文旨在通过传输（i）捕获过程底层物理特性的开源仿真模型的知识，尽管与目标工厂有所不同，（ii）另一个特点是噪声和有限数据的同一炼油厂的工业厂的知识，以及（iii）来自（ii）模型，并将训练问题的目标函数建模为物理信息，从开源模型的导出物理信息。

    Constructing first principles models is a challenging task for nonlinear and complex systems such as a wastewater treatment unit. In recent years, data-driven models are widely used to overcome the complexity. However, they often suffer from issues such as missing, low quality or noisy data. Transfer learning is a solution for this issue where knowledge from another task is transferred to target one to increase the prediction performance. In this work, the objective is increasing the prediction performance of an industrial wastewater treatment plant by transferring the knowledge of (i) an open-source simulation model that captures the underlying physics of the process, albeit with dissimilarities to the target plant, (ii) another industrial plant characterized by noisy and limited data but located in the same refinery, and (iii) the model in (ii) and making the objective function of the training problem physics informed where the physics information derived from the open-source model i
    
[^8]: 选择用于数据库嵌入的行走方案

    Selecting Walk Schemes for Database Embedding. (arXiv:2401.11215v1 [cs.LG])

    [http://arxiv.org/abs/2401.11215](http://arxiv.org/abs/2401.11215)

    本文研究了数据库嵌入中的行走方案选择问题，通过关注几个信息丰富的行走方案，能够更快地获得元组嵌入，并保持其质量。

    

    数据分析的机械设备通常需要对输入进行数值表示。为此，一种常见做法是将结构化数据的组成部分嵌入到高维向量空间中。我们研究关系数据库中元组的嵌入，现有技术通常基于对数据库中的一组随机行走的优化任务。本文的重点是最近提出的适用于动态数据库的FoRWaRD算法，其中行走是通过遵循元组间的外键来采样的。重要的是，不同的行走有不同的模式，或者“行走方案”，通过列出行走中的关系和属性来派生。重要的是，不同的行走方案描述了数据库中不同性质的关系。我们展示了通过集中关注几个信息丰富的行走方案，我们可以以更快的速度获得元组嵌入，同时保持其质量。我们定义了用于元组嵌入的方案选择问题，并设计了几种实验策略来解决该问题。

    Machinery for data analysis often requires a numeric representation of the input. Towards that, a common practice is to embed components of structured data into a high-dimensional vector space. We study the embedding of the tuples of a relational database, where existing techniques are often based on optimization tasks over a collection of random walks from the database. The focus of this paper is on the recent FoRWaRD algorithm that is designed for dynamic databases, where walks are sampled by following foreign keys between tuples. Importantly, different walks have different schemas, or "walk schemes", that are derived by listing the relations and attributes along the walk. Also importantly, different walk schemes describe relationships of different natures in the database. We show that by focusing on a few informative walk schemes, we can obtain tuple embedding significantly faster, while retaining the quality. We define the problem of scheme selection for tuple embedding, devise sev
    
[^9]: PartIR: 为机器学习组合SPMD分区策略

    PartIR: Composing SPMD Partitioning Strategies for Machine Learning. (arXiv:2401.11202v1 [cs.LG])

    [http://arxiv.org/abs/2401.11202](http://arxiv.org/abs/2401.11202)

    PartIR是一种用于机器学习的分区系统，具备表达力强和可预测性强的特点。它通过高级程序员发出的分区策略驱动，并采用增量重写方法，能够组合不同的分片策略，评估结果表明其可预测性、表达能力和达到峰值性能能力强。

    

    现代大规模神经网络（NN）的训练需要结合数据、模型或优化器分片的并行化策略。当策略变得复杂时，分区工具需要具备以下特点：1）表达力强，允许组合简单策略；2）可预测性强，可以通过分析估算性能。我们提出了PartIR，一种用于NN分区的设计。PartIR采用增量重写方法，与硬件和运行时无关。我们提供了一个简单而强大的API用于组合分片策略，并提供了一个模拟器进行验证。整个过程由高级程序员发出的分区策略驱动，既可以手动也可以自动。重要的是，这些策略与模型代码分开指定，易于更改。我们通过对几种不同模型的评估来展示PartIR的可预测性、表达能力和达到峰值性能的能力。

    Training of modern large neural networks (NN) requires a combination of parallelization strategies encompassing data, model, or optimizer sharding. When strategies increase in complexity, it becomes necessary for partitioning tools to be 1) expressive, allowing the composition of simpler strategies, and 2) predictable to estimate performance analytically. We present PartIR, our design for a NN partitioning system. PartIR is focused on an incremental approach to rewriting and is hardware-and-runtime agnostic. We present a simple but powerful API for composing sharding strategies and a simulator to validate them. The process is driven by high-level programmer-issued partitioning tactics, which can be both manual and automatic. Importantly, the tactics are specified separately from the model code, making them easy to change. We evaluate PartIR on several different models to demonstrate its predictability, expressibility, and ability to reach peak performance..
    
[^10]: 基于鉴别对齐的投影信念网络在声学事件分类方面达到了与最先进CNN相媲美的效果

    Projected Belief Networks With Discriminative Alignment for Acoustic Event Classification: Rivaling State of the Art CNNs. (arXiv:2401.11199v1 [cs.LG])

    [http://arxiv.org/abs/2401.11199](http://arxiv.org/abs/2401.11199)

    基于鉴别对齐的投影信念网络在声学事件分类中达到了与最先进CNN相媲美的效果。

    

    投影信念网络（PBN）是一种具有可计算似然函数的生成性随机网络，它基于前向神经网络（FFNN）。生成函数通过在FFNN中进行“备份”操作来运作。 PBN是两个网络，一个在正向方向上运作的FFNN和一个在反向方向上运作的生成网络。两个网络共同存在于相同的参数集上，具有自己的成本函数，可以分别或联合训练。因此，PBN有可能拥有鉴别性和生成性分类器的最佳特点。为了实现这个潜力，对每个类别分别训练一个独立的PBN，对给定类别最大化生成似然函数，同时最小化FFNN相对于“所有其他类别”的鉴别成本。这个技术被称为鉴别对齐（PBN-DA），它将似然函数的轮廓与决策边界对齐，并实现了大大改善的分类效果。

    The projected belief network (PBN) is a generative stochastic network with tractable likelihood function based on a feed-forward neural network (FFNN). The generative function operates by "backing up" through the FFNN. The PBN is two networks in one, a FFNN that operates in the forward direction, and a generative network that operates in the backward direction. Both networks co-exist based on the same parameter set, have their own cost functions, and can be separately or jointly trained. The PBN therefore has the potential to possess the best qualities of both discriminative and generative classifiers. To realize this potential, a separate PBN is trained on each class, maximizing the generative likelihood function for the given class, while minimizing the discriminative cost for the FFNN against "all other classes". This technique, called discriminative alignment (PBN-DA), aligns the contours of the likelihood function to the decision boundaries and attains vastly improved classificati
    
[^11]: 基于机器学习的离散时间系统状态观测器在李群上的应用

    Machine learning based state observer for discrete time systems evolving on Lie groups. (arXiv:2401.11196v1 [eess.SY])

    [http://arxiv.org/abs/2401.11196](http://arxiv.org/abs/2401.11196)

    本文提出了一种基于深度学习的机器学习方法，用于在李群上演化的系统的状态观测。通过设计一个限制在李群上的观测器，仅使用一个训练的算法来预测状态，并利用从李代数到群的映射以及群作用和当前状态来估计时刻t的状态。

    

    本文设计了一种基于机器学习的观测器，用于在流形上演化的系统，以使观测器的状态限制在系统演化的李群上。传统的在李群上演化的机器学习观测器技术涉及设计李群的图表，为每个图表训练一个基于机器学习的观测器，并根据系统状态切换训练好的模型。我们提出了一种新颖的基于深度学习的技术，其预测结果仅限于欧几里得空间上的0测度子集，而无需使用图表。利用这个网络，我们设计了一个观测器，确保观测器的状态限制在李群上，并且仅使用一个训练的算法来预测状态。深度学习网络在李群的李代数上预测了一个“误差项”，利用了从李代数到群的映射，并使用群作用和当前状态来估计时刻t的状态。

    In this paper, a machine learning based observer for systems evolving on manifolds is designed such that the state of the observer is restricted to the Lie group on which the system evolves. Conventional techniques involving machine learning based observers on systems evolving on Lie groups involve designing charts for the Lie group, training a machine learning based observer for each chart, and switching between the trained models based on the state of the system. We propose a novel deep learning based technique whose predictions are restricted to a measure 0 subset of Euclidean space without using charts. Using this network, we design an observer ensuring that the state of the observer is restricted to the Lie group, and predicting the state using only one trained algorithm. The deep learning network predicts an ``error term'' on the Lie algebra of the Lie group, uses the map from the Lie algebra to the group, and uses the group action and the present state to estimate the state at t
    
[^12]: 快速准确枚举深度网络分区区域

    Fast and Exact Enumeration of Deep Networks Partitions Regions. (arXiv:2401.11188v1 [cs.LG])

    [http://arxiv.org/abs/2401.11188](http://arxiv.org/abs/2401.11188)

    本文提出了第一个能够精确枚举DN分区的并行算法，可以评估常用的基于随机抽样的近似方法，发现如果只对具有“大”体积的区域感兴趣，那么对空间进行均匀抽样非常高效。

    

    深度网络(DNs)的一个富有成果的表述方式是通过分段仿射样条来实现，它既能够进行理论研究，又能为从业者提供实践指导。在这个领域中，一个DN的输入映射被表达为按区域的仿射映射，其中这些区域由模型的架构隐式确定，并形成它们的输入空间的一个分区。迄今为止，这个分区——这个研究线中产生的所有结果都涉及到它——只在DN的输入空间的$2/3$维切片上计算过，或者通过随机抽样进行估计。在本文中，我们提出了第一个能够精确枚举DN分区区域的并行算法。所提出的算法最终能够对常用的近似方法进行评估，例如基于DN输入空间的随机抽样。我们的一个重要发现是，如果只对具有“大”体积的区域感兴趣，那么对空间进行均匀抽样非常高效。

    One fruitful formulation of Deep Networks (DNs) enabling their theoretical study and providing practical guidelines to practitioners relies on Piecewise Affine Splines. In that realm, a DN's input-mapping is expressed as per-region affine mapping where those regions are implicitly determined by the model's architecture and form a partition of their input space. That partition -- which is involved in all the results spanned from this line of research -- has so far only been computed on $2/3$-dimensional slices of the DN's input space or estimated by random sampling. In this paper, we provide the first parallel algorithm that does exact enumeration of the DN's partition regions. The proposed algorithm enables one to finally assess the closeness of the commonly employed approximations methods, e.g. based on random sampling of the DN input space. One of our key finding is that if one is only interested in regions with ``large'' volume, then uniform sampling of the space is highly efficient
    
[^13]: 数据驱动的目标定位: 使用Cramér-Rao界限对梯度下降进行基准测试

    Data-Driven Target Localization: Benchmarking Gradient Descent Using the Cram\'er-Rao Bound. (arXiv:2401.11176v1 [eess.SP])

    [http://arxiv.org/abs/2401.11176](http://arxiv.org/abs/2401.11176)

    本研究提出了一种数据驱动的神经网络方法，通过降低均方误差（MSE）实现了改进的目标方位和速度估计。这一发现强调了在雷达系统中采用深度学习方法的潜力，为在杂乱和动态环境中更准确的定位铺平了道路。

    

    在现代雷达系统中，使用方位和速度估计进行精确的目标定位至关重要。传统的无偏估计方法利用梯度下降算法达到Cramér-Rao界限（CRB）的理论极限，用于参数估计的误差。在本研究中，我们提出了一种数据驱动的神经网络方法，优于这些传统技术，在目标方位和速度估计方面表现出更高的准确性。使用代表性的模拟场景，我们展示了我们提出的神经网络模型始终实现了改进的参数估计，这是由于其固有的偏见性，从而得到了减小的均方误差（MSE）。我们的发现强调了在雷达系统中采用深度学习方法的潜力，为在杂乱和动态环境中更准确的定位铺平了道路。

    In modern radar systems, precise target localization using azimuth and velocity estimation is paramount. Traditional unbiased estimation methods have leveraged gradient descent algorithms to reach the theoretical limits of the Cram\'er Rao Bound (CRB) for the error of the parameter estimates. In this study, we present a data-driven neural network approach that outperforms these traditional techniques, demonstrating improved accuracies in target azimuth and velocity estimation. Using a representative simulated scenario, we show that our proposed neural network model consistently achieves improved parameter estimates due to its inherently biased nature, yielding a diminished mean squared error (MSE). Our findings underscore the potential of employing deep learning methods in radar systems, paving the way for more accurate localization in cluttered and dynamic environments.
    
[^14]: 像素级别识别用于整体外科场景理解

    Pixel-Wise Recognition for Holistic Surgical Scene Understanding. (arXiv:2401.11174v1 [cs.CV])

    [http://arxiv.org/abs/2401.11174](http://arxiv.org/abs/2401.11174)

    本文提出了一个整体和多粒度外科场景理解数据集，以及一个基于变形器的模型，该模型有效地结合了全局视频特征提取和局部器械分割，可用于多层次理解外科活动。

    

    本文提出了Prostatectomies的整体和多粒度外科场景理解（GraSP）数据集，该数据集对外科场景理解进行了层次化建模，包括不同粒度的互补任务。我们的方法实现了对外科活动的多层次理解，包括外科阶段和步骤的识别以及包括外科器械分割和原子可视动作检测在内的短期任务。为了利用我们提出的数据集，我们引入了基于变形器（Transformers）的行动、阶段、步骤和器械分割（TAPIS）模型，该模型将全局视频特征提取器与来自器械分割模型的局部区域建议相结合，以应对我们数据集的多粒度问题。通过广泛的实验，我们展示了在短期识别任务中包括分割注释的影响，并突显了不同的粒度要求。

    This paper presents the Holistic and Multi-Granular Surgical Scene Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that models surgical scene understanding as a hierarchy of complementary tasks with varying levels of granularity. Our approach enables a multi-level comprehension of surgical activities, encompassing long-term tasks such as surgical phases and steps recognition and short-term tasks including surgical instrument segmentation and atomic visual actions detection. To exploit our proposed benchmark, we introduce the Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS) model, a general architecture that combines a global video feature extractor with localized region proposals from an instrument segmentation model to tackle the multi-granularity of our benchmark. Through extensive experimentation, we demonstrate the impact of including segmentation annotations in short-term recognition tasks, highlight the varying granularity require
    
[^15]: 使用基于密度估计的正-负学习方法的文档集扩展

    Document Set Expansion with Positive-Unlabeled Learning: A Density Estimation-based Approach. (arXiv:2401.11145v1 [cs.LG])

    [http://arxiv.org/abs/2401.11145](http://arxiv.org/abs/2401.11145)

    本文提出了一种基于密度估计的正-负学习框架，可以处理文档集扩展任务中存在的问题，并证明其在真实数据集上的有效性。

    

    文档集扩展旨在基于一组精细主题的小型文档集合，从大型集合中识别相关文档。先前的研究表明，正-负学习是这一任务的一种有希望的方法。然而，一些严重问题仍未解决，例如正-负学习方法所面临的典型挑战，如未知类别先验和数据不平衡，以及需要转导型实验设置。在本文中，我们提出了一种基于密度估计的全新正-负学习框架，称为puDE，它可以处理上述问题。puDE的优势在于它既不受限于SCAR假设，也不需要任何类别先验知识。我们使用一系列真实数据集验证了所提出方法的有效性，并得出结论：我们的方法是DSE任务的一种更好的选择。

    Document set expansion aims to identify relevant documents from a large collection based on a small set of documents that are on a fine-grained topic. Previous work shows that PU learning is a promising method for this task. However, some serious issues remain unresolved, i.e. typical challenges that PU methods suffer such as unknown class prior and imbalanced data, and the need for transductive experimental settings. In this paper, we propose a novel PU learning framework based on density estimation, called puDE, that can handle the above issues. The advantage of puDE is that it neither constrained to the SCAR assumption and nor require any class prior knowledge. We demonstrate the effectiveness of the proposed method using a series of real-world datasets and conclude that our method is a better alternative for the DSE task.
    
[^16]: 高斯自适应注意力是唯一所需的：跨多个模态的健壮上下文表示

    Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])

    [http://arxiv.org/abs/2401.11143](http://arxiv.org/abs/2401.11143)

    该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。

    

    我们提出了多头高斯自适应注意力机制（GAAM），一种新颖的概率注意力框架，并设计了高斯自适应变压器（GAT），旨在增强跨多个模态（包括语音、文本和视觉）的信息聚合。GAAM将可学习的均值和方差融入其注意力机制中，采用多头框架实现，使其能够集体建模任何概率分布，以动态重新调整特征重要性。该方法在处理高度非平稳数据时表现出显著改进，通过识别特征空间中的关键元素，超越了现有的注意力技术在模型性能上的状态（精度增加约20%）。GAAM与基于点积的注意力模型兼容，并具有相对较低的参数数量，展示了其适应性和提升现有注意力框架的潜力。在实证方面，GAAM表现出卓越的适应性和功效。

    We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
    
[^17]: 通过工具变量法识别和估计条件平均偏因果效应

    Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable. (arXiv:2401.11130v1 [cs.LG])

    [http://arxiv.org/abs/2401.11130](http://arxiv.org/abs/2401.11130)

    本文提出了一种通过工具变量法识别和估计连续处理的因果效应异质性的方法，并开发了三类相应的估计器，并对其进行了统计性质分析。

    

    近年来，对于估计异质因果效应的兴趣日益增加。本文引入了条件平均偏因果效应（CAPCE），以揭示连续处理的因果效应的异质性。我们给出了在工具变量设置下识别CAPCE的条件。我们开发了三类CAPCE估计器：筛选、参数化和再生核希尔伯特空间（RKHS）-基础，分析了它们的统计性质。我们通过合成和真实数据对提出的CAPCE估计器进行了说明。

    There has been considerable recent interest in estimating heterogeneous causal effects. In this paper, we introduce conditional average partial causal effects (CAPCE) to reveal the heterogeneity of causal effects with continuous treatment. We provide conditions for identifying CAPCE in an instrumental variable setting. We develop three families of CAPCE estimators: sieve, parametric, and reproducing kernel Hilbert space (RKHS)-based, and analyze their statistical properties. We illustrate the proposed CAPCE estimators on synthetic and real-world data.
    
[^18]: CARE: 针对安全应用的自适应攻击者的集成对抗鲁棒性评估

    CARE: Ensemble Adversarial Robustness Evaluation Against Adaptive Attackers for Security Applications. (arXiv:2401.11126v1 [cs.CR])

    [http://arxiv.org/abs/2401.11126](http://arxiv.org/abs/2401.11126)

    本研究提出了一个名为CARE的通用网络安全对抗鲁棒性评估平台，旨在为解决集成对抗攻击和防御中的关键问题提供一个综合评估平台。

    

    集成防御在各种安全相关应用中被广泛采用，以提高模型的性能和鲁棒性。然而，这些技术的广泛应用也引发了许多问题：一般的集成防御是否确保比个体更具鲁棒性？当网络安全领域的攻击与防御不断升级时，更强大的自适应攻击是否会击败现有的集成防御策略？集成防御是否能够同时对不同类型的攻击达到对抗性鲁棒性并抵御不断调整的自适应攻击？不幸的是，由于网络安全领域没有全面评估集成对抗攻击和防御的平台，这些重要问题仍然没有得到解决。在本文中，我们提出了一个名为CARE的通用网络安全对抗鲁棒性评估平台，旨在填补这一空白。

    Ensemble defenses, are widely employed in various security-related applications to enhance model performance and robustness. The widespread adoption of these techniques also raises many questions: Are general ensembles defenses guaranteed to be more robust than individuals? Will stronger adaptive attacks defeat existing ensemble defense strategies as the cybersecurity arms race progresses? Can ensemble defenses achieve adversarial robustness to different types of attacks simultaneously and resist the continually adjusted adaptive attacks? Unfortunately, these critical questions remain unresolved as there are no platforms for comprehensive evaluation of ensemble adversarial attacks and defenses in the cybersecurity domain. In this paper, we propose a general Cybersecurity Adversarial Robustness Evaluation (CARE) platform aiming to bridge this gap.
    
[^19]: EMA-Net: 高效的多任务关联学习用于稠密场景预测

    EMA-Net: Efficient Multitask Affinity Learning for Dense Scene Predictions. (arXiv:2401.11124v1 [cs.CV])

    [http://arxiv.org/abs/2401.11124](http://arxiv.org/abs/2401.11124)

    EMA-Net 是一个高效的多任务关联学习网络，通过引入跨任务关联学习模块(CTAL)，能够同时捕捉局部、全局和跨任务的相互作用。

    

    多任务学习（MTL）因其能够联合预测多个任务，在使用比单任务学习更少的模型参数的情况下实现更好的每个任务性能而备受关注。最近，以解码器为重点的架构通过使用其他相关任务的特征来改进多任务性能。然而，大多数这些改进方法在以参数高效的方式同时捕捉局部和全局任务特定表示以及跨任务模式方面存在问题。在本文中，我们引入了高效多任务关联学习网络（EMA-Net），它是一个轻量级框架，增强了多任务网络的任务改进能力。EMA-Net通过我们的新颖的跨任务关联学习（CTAL）模块巧妙地捕捉局部、全局和跨任务的相互作用。CTAL的关键创新在于其能够以最适合任务亲和矩阵的方式操纵任务亲和矩阵。

    Multitask learning (MTL) has gained prominence for its ability to jointly predict multiple tasks, achieving better per-task performance while using fewer per-task model parameters than single-task learning. More recently, decoder-focused architectures have considerably improved multitask performance by refining task predictions using the features of other related tasks. However, most of these refinement methods fail to simultaneously capture local and global task-specific representations, as well as cross-task patterns in a parameter-efficient manner. In this paper, we introduce the Efficient Multitask Affinity Learning Network (EMA-Net), which is a lightweight framework that enhances the task refinement capabilities of multitask networks. EMA-Net adeptly captures local, global, and cross-task interactions using our novel Cross-Task Affinity Learning (CTAL) module. The key innovation of CTAL lies in its ability to manipulate task affinity matrices in a manner that is optimally suited t
    
[^20]: 战略物联网部署覆盖灾难响应无人机群的元强化学习(arXiv:2401.11118v1 [cs.LG])

    Meta Reinforcement Learning for Strategic IoT Deployments Coverage in Disaster-Response UAV Swarms. (arXiv:2401.11118v1 [cs.LG])

    [http://arxiv.org/abs/2401.11118](http://arxiv.org/abs/2401.11118)

    本研究提出了一种战略物联网部署覆盖灾难响应无人机群的元强化学习方法，通过优化模型来最小化能源消耗，并提供满足时间和功率约束的最佳路径规划。

    

    在过去的十年中，无人机在学术界和工业界引起了研究人员的广泛关注，因为它们在提供无线服务和收集灾害区域数据等关键紧急应用中具有机动性和运动灵活性的优势。然而，无人机的资源有限、能源预算有限以及严格的任务完成时间对于这些应用的采用造成了挑战。本论文考虑一个无人机群体在一个区域中收集地面物联网设备的数据，其重点是为战略位置提供更好的服务，并允许无人机以动态方式加入和离开群体（例如充电）。我们引入了一个优化模型，旨在最小化总能源消耗，并在最短完成时间和最低传输功率约束下提供无人机的最佳路径规划。

    In the past decade, Unmanned Aerial Vehicles (UAVs) have grabbed the attention of researchers in academia and industry for their potential use in critical emergency applications, such as providing wireless services to ground users and collecting data from areas affected by disasters, due to their advantages in terms of maneuverability and movement flexibility. The UAVs' limited resources, energy budget, and strict mission completion time have posed challenges in adopting UAVs for these applications. Our system model considers a UAV swarm that navigates an area collecting data from ground IoT devices focusing on providing better service for strategic locations and allowing UAVs to join and leave the swarm (e.g., for recharging) in a dynamic way. In this work, we introduce an optimization model with the aim of minimizing the total energy consumption and provide the optimal path planning of UAVs under the constraints of minimum completion time and transmit power. The formulated optimizati
    
[^21]: SPAND: 使用网络动态的睡眠预测架构

    SPAND: Sleep Prediction Architecture using Network Dynamics. (arXiv:2401.11113v1 [cs.LG])

    [http://arxiv.org/abs/2401.11113](http://arxiv.org/abs/2401.11113)

    SPAND是一个利用网络动态的睡眠预测架构，可以通过图网络和移动设备数据来预测下一天的睡眠持续时间标签。

    

    睡眠行为对健康有重大影响，对身心健康的指示也非常重要。利用普遍存在的传感器监测和预测睡眠行为，可以帮助管理睡眠并追踪相关健康状况。虽然睡眠行为取决于个体的生理状况，但也受到数字媒体使用、社交网络传染以及周围天气等外部因素的影响。在本研究中，我们提出了SPAND（Sleep Prediction Architecture using Network Dynamics），这是一个利用图网络中的社交传染来预测睡眠行为的系统，并将其与从普遍存在的移动设备和可穿戴设备中提取的生理和手机数据集成，以预测下一天的睡眠持续时间标签。我们的架构通过设计一种注意机制，克服了包含与睡眠行为无关的连接的大规模图形的局限性。广泛的实验评估突显出该系统的性能。

    Sleep behavior significantly impacts health and acts as an indicator of physical and mental well-being. Monitoring and predicting sleep behavior with ubiquitous sensors may therefore assist in both sleep management and tracking of related health conditions. While sleep behavior depends on, and is reflected in the physiology of a person, it is also impacted by external factors such as digital media usage, social network contagion, and the surrounding weather. In this work, we propose SPAND (Sleep Prediction Architecture using Network Dynamics), a system that exploits social contagion in sleep behavior through graph networks and integrates it with physiological and phone data extracted from ubiquitous mobile and wearable devices for predicting next-day sleep labels about sleep duration. Our architecture overcomes the limitations of large-scale graphs containing connections irrelevant to sleep behavior by devising an attention mechanism. The extensive experimental evaluation highlights th
    
[^22]: VONet: 无监督视频对象学习通过并行U-Net注意力和逐个对象的序列VAE

    VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE. (arXiv:2401.11110v1 [cs.CV])

    [http://arxiv.org/abs/2401.11110](http://arxiv.org/abs/2401.11110)

    VONet是一种无监督视频对象学习方法，通过并行U-Net注意力和逐个对象的序列VAE实现，具有高效且有效的推理过程和提升时间一致性的特点，成为五个MOVI数据集上领先的方法。

    

    无监督的视频对象学习旨在将视频场景分解为结构化的对象表示，无需深度、光流或分割的监督。我们提出了VONet，这是一种受MONet启发的创新方法。VONet利用U-Net架构，采用高效且有效的并行注意力推理过程，同时为所有槽生成注意力掩码。此外，为了增强每个掩码在连续视频帧中的时间一致性，VONet开发了一种逐个对象的序列VAE框架。这些创新的编码器端技术与表达性强的基于Transformer的解码器的整合，使VONet成为涵盖了各种复杂性视频的五个MOVI数据集上领先的无监督对象学习方法。代码可在https://github.com/hnyu/vonet找到。

    Unsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present VONet, an innovative approach that is inspired by MONet. While utilizing a U-Net architecture, VONet employs an efficient and effective parallel attention inference process, generating attention masks for all slots simultaneously. Additionally, to enhance the temporal consistency of each mask across consecutive video frames, VONet develops an object-wise sequential VAE framework. The integration of these innovative encoder-side techniques, in conjunction with an expressive transformer-based decoder, establishes VONet as the leading unsupervised method for object learning across five MOVI datasets, encompassing videos of diverse complexities. Code is available at https://github.com/hnyu/vonet.
    
[^23]: 潜在漏洞是软件漏洞预测中的隐藏宝石吗？一项实证研究。

    Are Latent Vulnerabilities Hidden Gems for Software Vulnerability Prediction? An Empirical Study. (arXiv:2401.11105v1 [cs.SE])

    [http://arxiv.org/abs/2401.11105](http://arxiv.org/abs/2401.11105)

    该研究通过对潜在漏洞的研究，发现它们可以有效增加漏洞数量并纠正错误标记的函数，尽管存在噪声，但最先进的漏洞预测模型仍可以从中受益。

    

    收集相关高质量的数据对于开发有效的软件漏洞预测模型至关重要。目前的大部分软件漏洞数据集都依赖于漏洞修复提交来提取漏洞函数和代码行。然而，这些数据集没有考虑到收集的漏洞之间存在的潜在漏洞。关于这些潜在漏洞在漏洞预测中的实用性也知之甚少。为了填补这些空白，我们对两个常用的软件漏洞数据集中的潜在漏洞函数进行了大规模研究，并对其在函数级别和代码行级别上的漏洞预测进行了利用。通过最先进的SZZ算法，我们在研究的数据集中发现了超过10万个潜在漏洞函数。我们发现这些潜在函数平均能增加4倍的漏洞数量，并纠正多达5,000个错误标记的函数，但噪声水平约为6%。尽管有噪声，我们证明了最先进的漏洞预测模型可以从这些潜在漏洞中受益。

    Collecting relevant and high-quality data is integral to the development of effective Software Vulnerability (SV) prediction models. Most of the current SV datasets rely on SV-fixing commits to extract vulnerable functions and lines. However, none of these datasets have considered latent SVs existing between the introduction and fix of the collected SVs. There is also little known about the usefulness of these latent SVs for SV prediction. To bridge these gaps, we conduct a large-scale study on the latent vulnerable functions in two commonly used SV datasets and their utilization for function-level and line-level SV predictions. Leveraging the state-of-the-art SZZ algorithm, we identify more than 100k latent vulnerable functions in the studied datasets. We find that these latent functions can increase the number of SVs by 4x on average and correct up to 5k mislabeled functions, yet they have a noise level of around 6%. Despite the noise, we show that the state-of-the-art SV prediction 
    
[^24]: 加权最近邻算法的高效数据Shapley值计算方法

    Efficient Data Shapley for Weighted Nearest Neighbor Algorithms. (arXiv:2401.11103v1 [cs.DS])

    [http://arxiv.org/abs/2401.11103](http://arxiv.org/abs/2401.11103)

    本研究提出了一种解决加权K最近邻算法高效计算Data Shapley值的方法，并通过实验证明其在数据质量判别方面优于未加权版本。

    

    本文旨在解决数据评估文献中关于加权K最近邻算法的数据Shapley值（WKNN-Shapley）高效计算的问题。通过将硬标签KNN的准确度与离散权重视为效用函数，我们将WKNN-Shapley值的计算转化为一个计数问题，并引入了一个二次时间复杂度的算法，从而实现了对现有文献中O(N^K)的最佳结果的显著改进。我们开发了一个确定性近似算法，进一步提高了计算效率，同时保持了Shapley值的关键公平性质。通过大量实验，我们展示了WKNN-Shapley值的计算效率以及与未加权版本相比在判断数据质量方面的优势。

    This work aims to address an open problem in data valuation literature concerning the efficient computation of Data Shapley for weighted $K$ nearest neighbor algorithm (WKNN-Shapley). By considering the accuracy of hard-label KNN with discretized weights as the utility function, we reframe the computation of WKNN-Shapley into a counting problem and introduce a quadratic-time algorithm, presenting a notable improvement from $O(N^K)$, the best result from existing literature. We develop a deterministic approximation algorithm that further improves computational efficiency while maintaining the key fairness properties of the Shapley value. Through extensive experiments, we demonstrate WKNN-Shapley's computational efficiency and its superior performance in discerning data quality compared to its unweighted counterpart.
    
[^25]: 提升量子核函数性能的神经自动设计者

    Neural auto-designer for enhanced quantum kernels. (arXiv:2401.11098v1 [quant-ph])

    [http://arxiv.org/abs/2401.11098](http://arxiv.org/abs/2401.11098)

    本研究提出了一种自动化设计问题特定量子特征映射的数据驱动方法，通过利用特征选择技术和深度神经预测器，解决了在限制量子比特的近期量子机器上处理高维数据的挑战，并证明了该方法相对于先前方法的优越性。

    

    量子核函数在提供计算优势方面具有巨大潜力，而这些核函数的有效性与量子特征映射的设计密切相关。然而，在缺乏充分先验信息的情况下，针对现实世界数据集设计有效的量子特征映射仍然是一个重大障碍。在本研究中，我们提出了一种数据驱动方法，自动化地设计问题特定的量子特征映射。我们的方法利用特征选择技术来处理限制量子比特的近期量子机器上的高维数据，并结合深度神经预测器来高效评估各种候选量子核函数的性能。通过对不同数据集的广泛数值模拟，我们证明了我们的提议相对于先前方法的优越性，尤其是在消除核密度问题和确定具有预测能力的特征映射方面。

    Quantum kernels hold great promise for offering computational advantages over classical learners, with the effectiveness of these kernels closely tied to the design of the quantum feature map. However, the challenge of designing effective quantum feature maps for real-world datasets, particularly in the absence of sufficient prior information, remains a significant obstacle. In this study, we present a data-driven approach that automates the design of problem-specific quantum feature maps. Our approach leverages feature-selection techniques to handle high-dimensional data on near-term quantum machines with limited qubits, and incorporates a deep neural predictor to efficiently evaluate the performance of various candidate quantum kernels. Through extensive numerical simulations on different datasets, we demonstrate the superiority of our proposal over prior methods, especially for the capability of eliminating the kernel concentration issue and identifying the feature map with predicti
    
[^26]: 从聚合响应中学习：实例级别与包级别的损失函数比较

    Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions. (arXiv:2401.11081v1 [cs.LG])

    [http://arxiv.org/abs/2401.11081](http://arxiv.org/abs/2401.11081)

    本文研究了从聚合响应中学习的两种损失函数：包级别损失和实例级别损失，并发现实例级别损失可以被视为包级别损失的正则化形式。

    

    由于隐私问题的增加，在许多实际应用中，训练数据在与学习者共享之前会被聚合起来，以保护用户敏感响应的隐私。在聚合学习框架中，数据集被分组成样本的包，每个包只提供一个聚合响应，提供了该包中个体响应的摘要。本文研究了从聚合响应中学习的两种自然损失函数：包级别损失和实例级别损失。在前者中，模型通过最小化聚合响应与聚合模型预测之间的损失来学习，而在后者中，模型旨在将个体预测与聚合响应拟合。在这项工作中，我们表明实例级别损失可以被视为包级别损失的正则化形式。这个观察让我们能够比较这两种方法关于所得估计值的偏差和方差，并引入了一种新的插值。

    Due to the rise of privacy concerns, in many practical applications the training data is aggregated before being shared with the learner, in order to protect privacy of users' sensitive responses. In an aggregate learning framework, the dataset is grouped into bags of samples, where each bag is available only with an aggregate response, providing a summary of individuals' responses in that bag. In this paper, we study two natural loss functions for learning from aggregate responses: bag-level loss and the instance-level loss. In the former, the model is learnt by minimizing a loss between aggregate responses and aggregate model predictions, while in the latter the model aims to fit individual predictions to the aggregate responses. In this work, we show that the instance-level loss can be perceived as a regularized form of the bag-level loss. This observation lets us compare the two approaches with respect to bias and variance of the resulting estimators, and introduce a novel interpol
    
[^27]: 关于微分方程激发的图神经网络的时域问题研究

    On The Temporal Domain of Differential Equation Inspired Graph Neural Networks. (arXiv:2401.11074v1 [cs.LG])

    [http://arxiv.org/abs/2401.11074](http://arxiv.org/abs/2401.11074)

    本文提出了一种名为TDE-GNN的模型，解决了现有微分方程激发的图神经网络中一阶或二阶时域相关性的限制。通过学习时域依赖性，我们的模型能够捕捉到更广泛的时域动态，并取得了在几个图基准测试上的优越表现。

    

    图神经网络（GNNs）在建模图结构数据中的复杂关系方面取得了显著的成功。最近在这个领域的一个创新是微分方程激发的图神经网络（DE-GNNs），它利用连续动态系统的原理来模拟具有内置属性（如特征平滑或保留）的图上的信息流。然而，现有的DE-GNNs依赖于一阶或二阶时域相关性。本文提出了一个神经扩展来解决这些预定义的时域依赖问题。我们展示了我们的模型TDE-GNN能够捕捉到超过一阶或二阶方法的各种时域动态，并提供了现有时域模型无法解决的用例。我们通过在几个图基准测试上学习时域依赖性的益处来证明我们的方法相比于使用预定义时域动态的优势。

    Graph Neural Networks (GNNs) have demonstrated remarkable success in modeling complex relationships in graph-structured data. A recent innovation in this field is the family of Differential Equation-Inspired Graph Neural Networks (DE-GNNs), which leverage principles from continuous dynamical systems to model information flow on graphs with built-in properties such as feature smoothing or preservation. However, existing DE-GNNs rely on first or second-order temporal dependencies. In this paper, we propose a neural extension to those pre-defined temporal dependencies. We show that our model, called TDE-GNN, can capture a wide range of temporal dynamics that go beyond typical first or second-order methods, and provide use cases where existing temporal models are challenged. We demonstrate the benefit of learning the temporal dependencies using our method rather than using pre-defined temporal dynamics on several graph benchmarks.
    
[^28]: 机器学习分类过程中数据抽象方法在关键决策中的重要性

    The Significance of Data Abstraction Methods in Machine Learning Classification Processes for Critical Decision-Making. (arXiv:2401.11044v1 [cs.LG])

    [http://arxiv.org/abs/2401.11044](http://arxiv.org/abs/2401.11044)

    本文研究了机器学习分类过程中数据抽象方法的重要性，提出了Small and Incomplete Dataset Analyser (SaNDA)采用ROC曲线方法开发的数据抽象协议，该方法在缺少值很少的情况下可以成为随机森林的可行替代品，始终保持高准确性。

    

    广泛采用的机器学习(ML)方法在分类方面的适用性受到了解释能力和不确定性的限制，特别是在医疗保健、行为科学和金融等领域，其中责任问题至关重要。最近，提出了Small and Incomplete Dataset Analyser (SaNDA)，通过使用基于ROC曲线的方法开发数据抽象协议，以增强在这些领域中执行分类的能力。本文关注于列间数据转换，即抽象，这对SaNDA的分类过程非常关键，并探讨了替代的抽象协议，如常量分箱和分位数。将最佳的方法与可解释方法的基准模型随机森林进行了比较。结果表明，即使数据不完整，SaNDA在缺少值很少的情况下仍然可以成为随机森林的可行替代品，并且始终保持高准确性。

    The applicability of widely adopted machine learning (ML) methods to classification is circumscribed by the imperatives of explicability and uncertainty, particularly evident in domains such as healthcare, behavioural sciences, and finances, wherein accountability assumes priority. Recently, Small and Incomplete Dataset Analyser (SaNDA) has been proposed to enhance the ability to perform classification in such domains, by developing a data abstraction protocol using a ROC curve-based method. This paper focuses on column-wise data transformations called abstractions, which are crucial for SaNDA's classification process and explores alternative abstractions protocols, such as constant binning and quantiles. The best-performing methods have been compared against Random Forest as a baseline for explainable methods. The results suggests that SaNDA can be a viable substitute for Random Forest when data is incomplete, even with minimal missing values. It consistently maintains high accuracy e
    
[^29]: 建模三维动力学的等变图神经操作器

    Equivariant Graph Neural Operator for Modeling 3D Dynamics. (arXiv:2401.11037v1 [cs.LG])

    [http://arxiv.org/abs/2401.11037](http://arxiv.org/abs/2401.11037)

    本文提出了一种新的等变图神经操作器（EGNO）方法，能够直接将动力学建模为轨迹而不仅仅是下一步预测，以准确捕捉时间相关性，并利用等变时间卷积来保持其内在的等变性。

    

    在自然科学中，建模复杂的三维关系系统动力学是一个重要问题，涉及领域从分子模拟到粒子力学。机器学习方法通过学习图神经网络来建模空间相互作用已取得良好成功。然而，这些方法只能进行下一步预测，不能准确捕捉时间相关性。本文提出了一种新颖而有原则的方法，即等变图神经操作器（EGNO），直接将动力学建模为轨迹而不仅仅是下一步预测。与现有方法不同，EGNO显式地学习三维动力学的时间演化，通过时间函数来建模动力学并学习神经操作器来近似。为了捕捉时间相关性，同时保持内在的SE(3)等变性，我们在傅里叶空间中参数化等变时间卷积，并通过堆叠卷积层构建EGNO。

    Modeling the complex three-dimensional (3D) dynamics of relational systems is an important problem in the natural sciences, with applications ranging from molecular simulations to particle mechanics. Machine learning methods have achieved good success by learning graph neural networks to model spatial interactions. However, these approaches do not faithfully capture temporal correlations since they only model next-step predictions. In this work, we propose Equivariant Graph Neural Operator (EGNO), a novel and principled method that directly models dynamics as trajectories instead of just next-step prediction. Different from existing methods, EGNO explicitly learns the temporal evolution of 3D dynamics where we formulate the dynamics as a function over time and learn neural operators to approximate it. To capture the temporal correlations while keeping the intrinsic SE(3)-equivariance, we develop equivariant temporal convolutions parameterized in the Fourier space and build EGNO by stac
    
[^30]: 在汽车CAN中探索高度量化的神经网络用于入侵检测

    Exploring Highly Quantised Neural Networks for Intrusion Detection in Automotive CAN. (arXiv:2401.11030v1 [cs.CR])

    [http://arxiv.org/abs/2401.11030](http://arxiv.org/abs/2401.11030)

    本文介绍了在汽车CAN中使用高度量化的神经网络来进行入侵检测的案例，以应对不断增长的连接性可能带来的安全威胁。该方法通过使用自定义量化的多层感知机（CQMLP）模型，可以同时检测多个攻击向量，降低了资源成本和能量消耗。

    

    当今的车辆包括智能系统，如连接的自主驾驶和先进驾驶辅助系统（ADAS），以增强驾驶体验，这是通过增加与基础设施的连接性和来自不同感知模式的信息融合实现的。然而，不断增长的连接性与车辆内的传统网络架构相结合，可以被利用来发动对关键车辆系统的主动和被动攻击，直接影响乘客的安全性。机器学习基于入侵检测模型已在最近的文献中成功检测到多个目标攻击向量，其部署通过针对低功耗平台的量化神经网络是可能的。通常需要多个模型同时检测多个攻击向量，增加了面积（资源）成本和能量消耗。在本文中，我们提出使用自定义量化的多层感知机（CQMLP）作为多类分类模型的案例。

    Vehicles today comprise intelligent systems like connected autonomous driving and advanced driving assistance systems (ADAS) to enhance the driving experience, which is enabled through increased connectivity to infrastructure and fusion of information from different sensing modes. However, the rising connectivity coupled with the legacy network architecture within vehicles can be exploited for launching active and passive attacks on critical vehicle systems and directly affecting the safety of passengers. Machine learning-based intrusion detection models have been shown to successfully detect multiple targeted attack vectors in recent literature, whose deployments are enabled through quantised neural networks targeting low-power platforms. Multiple models are often required to simultaneously detect multiple attack vectors, increasing the area, (resource) cost, and energy consumption. In this paper, we present a case for utilising custom-quantised MLP's (CQMLP) as a multi-class classifi
    
[^31]: 通信高效、可证明的联邦取消学习

    Communication Efficient and Provable Federated Unlearning. (arXiv:2401.11018v1 [cs.LG])

    [http://arxiv.org/abs/2401.11018](http://arxiv.org/abs/2401.11018)

    这篇论文解决了联邦学习中的一个新问题：通过联邦取消学习，消除特定客户端或数据点对全局模型的影响。作者提出了一个通信高效且可证明的确切联邦取消学习框架，并通过开发一个TV稳定FL算法来实现快速确切联邦取消学习。

    

    我们研究了联邦取消学习，这是一个通过联邦学习（FL）消除特定客户端或数据点对全局模型影响的新问题。该问题源于被遗忘的权利和FL中的隐私挑战。我们引入了一个新的框架，用于确切的联邦取消学习，满足两个基本标准：通信效率和确切取消学习的可证明性。据我们所知，这是第一个一致解决这两个方面的工作。我们首先给出了"确切"联邦取消学习的严格定义，确保取消学习后的模型在统计上不可区分于在没有被删除数据的情况下训练的模型。然后我们确定了实现快速确切联邦取消学习的关键属性：总变差（TV）稳定性，该属性衡量了模型参数对数据集细微变化的敏感性。基于这一见解，我们开发了一种名为FATS的TV稳定FL算法，对原有的FL算法进行了修改。

    We study federated unlearning, a novel problem to eliminate the impact of specific clients or data points on the global model learned via federated learning (FL). This problem is driven by the right to be forgotten and the privacy challenges in FL. We introduce a new framework for exact federated unlearning that meets two essential criteria: \textit{communication efficiency} and \textit{exact unlearning provability}. To our knowledge, this is the first work to tackle both aspects coherently. We start by giving a rigorous definition of \textit{exact} federated unlearning, which guarantees that the unlearned model is statistically indistinguishable from the one trained without the deleted data. We then pinpoint the key property that enables fast exact federated unlearning: total variation (TV) stability, which measures the sensitivity of the model parameters to slight changes in the dataset. Leveraging this insight, we develop a TV-stable FL algorithm called \texttt{FATS}, which modifies
    
[^32]: 揭示说话者嵌入中的情感聚类：一种对话者表达式识别的对抗学习策略

    Revealing Emotional Clusters in Speaker Embeddings: A Contrastive Learning Strategy for Speech Emotion Recognition. (arXiv:2401.11017v1 [eess.AS])

    [http://arxiv.org/abs/2401.11017](http://arxiv.org/abs/2401.11017)

    该论文研究了一种使用对比学习策略来揭示说话者嵌入中的情感聚类，并将其应用于语音情感识别中。研究发现情感信息可以直接从说话者嵌入中提取出来，并提出了一种利用情感未标记数据的对抗预训练方法，显著提高了识别效果。

    

    说话者嵌入携带有有价值的与情感相关的信息，使其成为增强语音情感识别（SER）的有前景的资源，特别是在有限标注数据情况下。传统上，人们认为情感信息是间接嵌入在说话者嵌入中的，导致它们的利用不足。我们的研究揭示了情感与最先进的说话者嵌入之间的直接而有用的联系，即内部说话者聚类的形式。通过进行详尽的聚类分析，我们证明情感信息可以很容易地从说话者嵌入中提取出来。为了利用这些信息，我们引入了一种新颖的对抗预训练方法，应用于情感未标记数据的语音情感识别。所提出的方法涉及根据说话者嵌入的内部聚类来采样正例和负例。该策略利用大量的情感未标记数据，导致明显的效果提升。

    Speaker embeddings carry valuable emotion-related information, which makes them a promising resource for enhancing speech emotion recognition (SER), especially with limited labeled data. Traditionally, it has been assumed that emotion information is indirectly embedded within speaker embeddings, leading to their under-utilization. Our study reveals a direct and useful link between emotion and state-of-the-art speaker embeddings in the form of intra-speaker clusters. By conducting a thorough clustering analysis, we demonstrate that emotion information can be readily extracted from speaker embeddings. In order to leverage this information, we introduce a novel contrastive pretraining approach applied to emotion-unlabeled data for speech emotion recognition. The proposed approach involves the sampling of positive and the negative examples based on the intra-speaker clusters of speaker embeddings. The proposed strategy, which leverages extensive emotion-unlabeled data, leads to a significa
    
[^33]: 在考虑-然后-选择排名模型中确定考虑概率的界限

    Bounding Consideration Probabilities in Consider-Then-Choose Ranking Models. (arXiv:2401.11016v1 [cs.LG])

    [http://arxiv.org/abs/2401.11016](http://arxiv.org/abs/2401.11016)

    在考虑-然后-选择的排名模型中，我们提出了一种方法来确定考虑概率的界限。尽管不能准确确定考虑概率，但在已知备选方案效用的情况下，我们可以推断出备选概率的相对大小的界限。

    

    选择理论中一种常见的观点认为，个体在做出选择之前，会先进行两步的过程，首先选择一些备选方案进行考虑，然后从所得的考虑集合中进行选择。然而，在这种“考虑然后选择”的情景下推断未观察到的考虑集合（或者备选方案的考虑概率）面临着重大挑战，因为即使是对于具有强独立性假设的简单考虑模型，在已知备选方案效用的情况下也无法确定身份。我们考虑将考虑-然后-选择模型自然地扩展到top-k排名的情景，假设排名是根据Plackett-Luce模型在采样了考虑集合后构建的。尽管在这种情景下备选方案的考虑概率仍旧不能确定，但我们证明了在获得备选方案效用的知识的情况下，我们可以推断出备选概率相对大小的界限。此外，通过对期望考虑集合大小的条件进行推导，我们得到绝对界限。

    A common theory of choice posits that individuals make choices in a two-step process, first selecting some subset of the alternatives to consider before making a selection from the resulting consideration set. However, inferring unobserved consideration sets (or item consideration probabilities) in this "consider then choose" setting poses significant challenges, because even simple models of consideration with strong independence assumptions are not identifiable, even if item utilities are known. We consider a natural extension of consider-then-choose models to a top-$k$ ranking setting, where we assume rankings are constructed according to a Plackett-Luce model after sampling a consideration set. While item consideration probabilities remain non-identified in this setting, we prove that knowledge of item utilities allows us to infer bounds on the relative sizes of consideration probabilities. Additionally, given a condition on the expected consideration set size, we derive absolute u
    
[^34]: 具有结构化变分族的可证伸缩性黑盒变分推断

    Provably Scalable Black-Box Variational Inference with Structured Variational Families. (arXiv:2401.10989v1 [stat.ML])

    [http://arxiv.org/abs/2401.10989](http://arxiv.org/abs/2401.10989)

    本文研究了均值场变分族和满秩变分族之间的理论中间地带：结构化变分族，并通过理论证明结构化变分族可以在迭代复杂性上表现更好，缩放效果更好。

    

    已知具有满秩协方差逼近的变分族在黑盒变分推断中表现不佳，无论是从实证上还是理论上。事实上，最近对黑盒变分推断的计算复杂性结果表明，与均值场变分族相比，满秩变分族在问题的维度上扩展得很差。这对具有本地变量的分层贝叶斯模型尤为关键，它们的维度随着数据集的大小而增加。因此，迭代复杂性对数据集大小N存在明确的O(N^2)依赖。在本文中，我们探索了均值场变分族和满秩变分族之间的理论中间地带：结构化变分族。我们严格证明了某些尺度矩阵结构可以实现更好的迭代复杂性O(N)，从而与N的缩放更好地匹配。我们在现实中验证了我们的理论结果

    Variational families with full-rank covariance approximations are known not to work well in black-box variational inference (BBVI), both empirically and theoretically. In fact, recent computational complexity results for BBVI have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean field families. This is particularly critical to hierarchical Bayesian models with local variables; their dimensionality increases with the size of the datasets. Consequently, one gets an iteration complexity with an explicit $\mathcal{O}(N^2)$ dependence on the dataset size $N$. In this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. We rigorously prove that certain scale matrix structures can achieve a better iteration complexity of $\mathcal{O}(N)$, implying better scaling with respect to $N$. We empirically verify our theoretical results on l
    
[^35]: T2MAC: 通过选择性参与和基于证据的整合实现目标和可信的多智能体通信

    T2MAC: Targeted and Trusted Multi-Agent Communication through Selective Engagement and Evidence-Driven Integration. (arXiv:2401.10973v1 [cs.MA])

    [http://arxiv.org/abs/2401.10973](http://arxiv.org/abs/2401.10973)

    T2MAC提出了一种名为目标和可信的多智能体通信方法，该方法使智能体能够学习选择性参与和基于证据的整合，从而提高通信效率。

    

    通信是协调多个智能体行为的一种有效机制。然而，现有的研究主要集中在广播通信上，这不仅缺乏实用性，而且还导致信息冗余。这种一刀切的信息可能对通信效率产生不利影响。此外，现有的研究常常采用基础机制来整合观察到的和接收到的信息，从而影响学习过程。为了解决这些困难，我们提出了一种名为T2MAC的目标和可信的多智能体通信方法，这是一种简单而有效的方法，使智能体能够学习选择性参与和基于证据的整合。通过T2MAC，智能体能够制定个性化消息，确定理想的通信窗口，并与可靠的伙伴进行互动，从而提高通信效率。在接收消息后，智能体可以从不同来源观察和接收的信息进行整合。

    Communication stands as a potent mechanism to harmonize the behaviors of multiple agents. However, existing works primarily concentrate on broadcast communication, which not only lacks practicality, but also leads to information redundancy. This surplus, one-fits-all information could adversely impact the communication efficiency. Furthermore, existing works often resort to basic mechanisms to integrate observed and received information, impairing the learning process. To tackle these difficulties, we propose Targeted and Trusted Multi-Agent Communication (T2MAC), a straightforward yet effective method that enables agents to learn selective engagement and evidence-driven integration. With T2MAC, agents have the capability to craft individualized messages, pinpoint ideal communication windows, and engage with reliable partners, thereby refining communication efficiency. Following the reception of messages, the agents integrate information observed and received from different sources at 
    
[^36]: 自适应网络嵌入方法对分子能量景观进行聚类

    Clustering Molecular Energy Landscapes by Adaptive Network Embedding. (arXiv:2401.10972v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.10972](http://arxiv.org/abs/2401.10972)

    本文提出了一种自适应网络嵌入方法，用于聚类分子能量景观。通过该方法，可以在降维空间中解释动态节点之间的关系，并可应用于化学空间探索和机器学习任务。

    

    为了有效探索所有可能的小分子的化学空间，一种常见方法是通过压缩系统的维度来促进下游的机器学习任务。为此，我们提出了一种数据驱动的方法，通过应用最近开发的网络嵌入技术，对分子结构的潜在能量景观进行聚类，从而得到通过嵌入函数定义的潜在变量。为了扩展该方法，我们还结合熵敏感的自适应方案，基于元动力学和转变路径理论，对能量景观进行层次采样。通过考虑系统能量景观所隐含的动力学信息，我们能够解释在降维空间中的动态节点之间的关系。我们通过Lennard-Jones (LJ)簇和人类DNA序列来演示这个框架。

    In order to efficiently explore the chemical space of all possible small molecules, a common approach is to compress the dimension of the system to facilitate downstream machine learning tasks. Towards this end, we present a data driven approach for clustering potential energy landscapes of molecular structures by applying recently developed Network Embedding techniques, to obtain latent variables defined through the embedding function. To scale up the method, we also incorporate an entropy sensitive adaptive scheme for hierarchical sampling of the energy landscape, based on Metadynamics and Transition Path Theory. By taking into account the kinetic information implied by a system's energy landscape, we are able to interpret dynamical node-node relationships in reduced dimensions. We demonstrate the framework through Lennard-Jones (LJ) clusters and a human DNA sequence.
    
[^37]: HOSC:一种用于保留隐式神经表示中锐利特征的周期性激活函数

    HOSC: A Periodic Activation Function for Preserving Sharp Features in Implicit Neural Representations. (arXiv:2401.10967v1 [cs.NE])

    [http://arxiv.org/abs/2401.10967](http://arxiv.org/abs/2401.10967)

    HOSC是一种新型的激活函数，专门设计用于捕捉输入信号中的锐利特征和低频过渡。它可以在隐式神经网络表示中提供改善质量的插入式功能。

    

    最近提出的用于用基于坐标的神经网络架构隐式表示信号（例如图像，场景或几何体）的方法通常不利用选择激活函数，或者仅在一定程度上利用。在本文中，我们引入了一种名为Hyperbolic Oscillation function (HOSC) 的新型激活函数，该函数具有可控的锐利度参数。与以往的激活函数不同，HOSC专门设计用于更好地捕捉输入信号中的突变，从而捕捉底层数据的锐利或急剧特征，以及平滑的低频过渡。由于其简单性和模块化性，HOSC提供了一种插入式功能，可以轻松地将其纳入任何使用神经网络作为隐式表示信号的方法中。我们在一系列通用任务中将HOSC与其他常用的激活函数进行了基准测试，实证显示了所得到表示的质量的改善，并提供了数学证明。

    Recently proposed methods for implicitly representing signals such as images, scenes, or geometries using coordinate-based neural network architectures often do not leverage the choice of activation functions, or do so only to a limited extent. In this paper, we introduce the Hyperbolic Oscillation function (HOSC), a novel activation function with a controllable sharpness parameter. Unlike any previous activations, HOSC has been specifically designed to better capture sudden changes in the input signal, and hence sharp or acute features of the underlying data, as well as smooth low-frequency transitions. Due to its simplicity and modularity, HOSC offers a plug-and-play functionality that can be easily incorporated into any existing method employing a neural network as a way of implicitly representing a signal. We benchmark HOSC against other popular activations in an array of general tasks, empirically showing an improvement in the quality of obtained representations, provide the mathe
    
[^38]: 一步学习，一步评审

    One Step Learning, One Step Review. (arXiv:2401.10962v1 [cs.CV])

    [http://arxiv.org/abs/2401.10962](http://arxiv.org/abs/2401.10962)

    本文提出了一种基于权重回滚的微调方法OLOR，通过结合优化器和权重回滚项，解决了完全微调方法中的知识遗忘问题，并在各种任务上提高了微调性能。

    

    随着预训练视觉模型的兴起，视觉微调已经引起了广泛关注。当前主流的方法——完全微调，存在知识遗忘的问题，因为它只专注于拟合下游训练集。在本文中，我们提出了一种新颖的基于权重回滚的微调方法，称为OLOR（一步学习，一步评审）。OLOR将微调与优化器相结合，将权重回滚项加入到每个步骤的权重更新项中。这确保了上游和下游模型的权重范围的一致性，有效地减轻了知识遗忘问题，并增强了微调性能。此外，我们提出了一种逐层惩罚方法，通过 penalty decay 和不同的衰减率来调整层的权重回滚程度，以适应不同的下游任务。通过在图像分类、目标检测、语义分割和实例分割等各种任务上进行大量实验证明，我们的方法提高了微调的性能。

    Visual fine-tuning has garnered significant attention with the rise of pre-trained vision models. The current prevailing method, full fine-tuning, suffers from the issue of knowledge forgetting as it focuses solely on fitting the downstream training set. In this paper, we propose a novel weight rollback-based fine-tuning method called OLOR (One step Learning, One step Review). OLOR combines fine-tuning with optimizers, incorporating a weight rollback term into the weight update term at each step. This ensures consistency in the weight range of upstream and downstream models, effectively mitigating knowledge forgetting and enhancing fine-tuning performance. In addition, a layer-wise penalty is presented to employ penalty decay and the diversified decay rate to adjust the weight rollback levels of layers for adapting varying downstream tasks. Through extensive experiments on various tasks such as image classification, object detection, semantic segmentation, and instance segmentation, we
    
[^39]: 《最优输运理论与多智能体强化学习之间的协同作用》

    The Synergy Between Optimal Transport Theory and Multi-Agent Reinforcement Learning. (arXiv:2401.10949v1 [cs.MA])

    [http://arxiv.org/abs/2401.10949](http://arxiv.org/abs/2401.10949)

    本文研究了最优输运理论与多智能体强化学习之间的协同作用。通过利用最优输运来处理分布和运输问题，增强了多智能体强化学习的效率、协调性和适应性。通过在政策对齐、分布式资源管理、应对非平稳性、可扩展的多智能体学习和提高能源效率五个方面应用最优输运理论，为解决可扩展性问题、优化资源分配和在合作环境中对齐智能体策略提供了新的方法。

    

    本文探讨了最优输运（OT）理论与多智能体强化学习（MARL）的整合。该整合使用OT处理分布和运输问题，以提高MARL的效率、协调性和适应性。OT在以下五个关键领域可以影响MARL：（1）政策对齐，利用OT的Wasserstein度量来将不同的智能体策略对齐到统一的目标上；（2）分布式资源管理，利用OT来优化智能体之间的资源分配；（3）应对非平稳性，利用OT适应动态环境变化；（4）可扩展的多智能体学习，利用OT将大规模学习目标分解为可管理的任务；（5）提高能源效率，应用OT原则来开发可持续的MARL系统。本文阐述了OT与MARL之间的协同作用如何解决可扩展性问题、优化资源分配、在合作环境中对齐智能体策略。

    This paper explores the integration of optimal transport (OT) theory with multi-agent reinforcement learning (MARL). This integration uses OT to handle distributions and transportation problems to enhance the efficiency, coordination, and adaptability of MARL. There are five key areas where OT can impact MARL: (1) policy alignment, where OT's Wasserstein metric is used to align divergent agent strategies towards unified goals; (2) distributed resource management, employing OT to optimize resource allocation among agents; (3) addressing non-stationarity, using OT to adapt to dynamic environmental shifts; (4) scalable multi-agent learning, harnessing OT for decomposing large-scale learning objectives into manageable tasks; and (5) enhancing energy efficiency, applying OT principles to develop sustainable MARL systems. This paper articulates how the synergy between OT and MARL can address scalability issues, optimize resource distribution, align agent policies in cooperative environments,
    
[^40]: Twin-in-the-Loop Observers的自动降维

    Automatic dimensionality reduction of Twin-in-the-Loop Observers. (arXiv:2401.10945v1 [cs.SY])

    [http://arxiv.org/abs/2401.10945](http://arxiv.org/abs/2401.10945)

    本论文提出了一种自动降维的方法来解决车辆动力学估计中的各个变量独立计算和校准的问题，通过将经典控制取向车辆模型替换为车辆模拟器或数字双胞胎(DT)来实现，然后使用贝叶斯优化来调节滤波器。

    

    目前车辆动力学估计技术通常存在一个共同的缺点：每个要估计的变量都是用独立的简化滤波模块计算的。这些模块并行运行并需要单独校准。为了解决这个问题，最近提出了一种统一的Twin-in-the-Loop(TiL)观测器架构：估计器中的经典简化控制取向车辆模型被一个完整的车辆模拟器或数字双胞胎(DT)替代。DT的状态通过线性时不变的输出误差定律实时校正。由于模拟器是一个黑盒子，没有明确的分析公式可用，因此无法使用经典的滤波器调节技术。出于这个原因，贝叶斯优化将用于解决一个数据驱动的优化问题来调节滤波器。由于DT的复杂性，优化问题是高维的。本文旨在找到一种调节高复杂度观测器的流程。

    State-of-the-art vehicle dynamics estimation techniques usually share one common drawback: each variable to estimate is computed with an independent, simplified filtering module. These modules run in parallel and need to be calibrated separately. To solve this issue, a unified Twin-in-the-Loop (TiL) Observer architecture has recently been proposed: the classical simplified control-oriented vehicle model in the estimators is replaced by a full-fledged vehicle simulator, or digital twin (DT). The states of the DT are corrected in real time with a linear time invariant output error law. Since the simulator is a black-box, no explicit analytical formulation is available, hence classical filter tuning techniques cannot be used. Due to this reason, Bayesian Optimization will be used to solve a data-driven optimization problem to tune the filter. Due to the complexity of the DT, the optimization problem is high-dimensional. This paper aims to find a procedure to tune the high-complexity obser
    
[^41]: 机器反学习在推荐系统中的应用：一种洞察力

    Machine Unlearning for Recommendation Systems: An Insight. (arXiv:2401.10942v1 [cs.IR])

    [http://arxiv.org/abs/2401.10942](http://arxiv.org/abs/2401.10942)

    本文探讨了机器反学习在推荐系统中的应用，解决了适应性、个性化、隐私和偏见等挑战。与传统模型不同，MUL根据用户偏好的变化和伦理考虑动态调整系统知识。通过批判性检验和文献梳理，本文提供了MUL如何改变推荐、用户信任以及未来研究路径的见解。强调个性化和隐私之间的权衡挑战，并鼓励以满足实际需求为目标的贡献，推动MUL在安全和适应性机器学习中的发展。

    

    本综述探讨了推荐系统中的机器反学习（MUL），解决了适应性、个性化、隐私和偏见等挑战。与传统模型不同，MUL根据用户偏好的变化和伦理考虑动态调整系统知识。本文对MUL的基本原理、现实世界应用和算法透明性等挑战进行了批判性的检验。它梳理了相关文献，提供了MUL如何改变推荐的见解，探讨了用户信任，并提出了未来研究在负责任和用户关注的人工智能领域的路径。本文引导研究人员面对个性化和隐私之间的权衡挑战，鼓励以满足有针对性的数据删除实际需求为目标的贡献。强调MUL在安全和适应性机器学习中的作用，提出了推动其边界的方法。本文的创新之处在于探讨了这些方法的局限性。

    This review explores machine unlearning (MUL) in recommendation systems, addressing adaptability, personalization, privacy, and bias challenges. Unlike traditional models, MUL dynamically adjusts system knowledge based on shifts in user preferences and ethical considerations. The paper critically examines MUL's basics, real-world applications, and challenges like algorithmic transparency. It sifts through literature, offering insights into how MUL could transform recommendations, discussing user trust, and suggesting paths for future research in responsible and user-focused artificial intelligence (AI). The document guides researchers through challenges involving the trade-off between personalization and privacy, encouraging contributions to meet practical demands for targeted data removal. Emphasizing MUL's role in secure and adaptive machine learning, the paper proposes ways to push its boundaries. The novelty of this paper lies in its exploration of the limitations of the methods, w
    
[^42]: Crowd-PrefRL: 基于众包的偏好反馈学习

    Crowd-PrefRL: Preference-Based Reward Learning from Crowds. (arXiv:2401.10941v1 [cs.HC])

    [http://arxiv.org/abs/2401.10941](http://arxiv.org/abs/2401.10941)

    Crowd-PrefRL是一种基于众包的偏好反馈学习框架，能够从来自群体的反馈中学习奖励函数，并且能够强大地聚合群体偏好反馈并估计用户的可靠性。

    

    基于偏好的强化学习提供了一个框架，通过对行为对的偏好进行人类反馈来训练智能体，使其能够在难以指定数值奖励函数的情况下学习期望的行为。尽管这个范式利用了人类的反馈，但目前将反馈视为单个人类用户所给出的。与此同时，以强大的方式合并来自群体（即用户集合）的偏好反馈仍然是一个挑战，而使用来自多个用户的反馈来训练强化学习智能体的问题仍然被研究不足。在这项工作中，我们引入了Crowd-PrefRL，一个利用来自群体的反馈进行基于偏好的强化学习的框架。这项工作展示了利用未知专业水平和可靠性的群体偏好反馈来学习奖励函数的可行性。Crowd-PrefRL不仅能够强大地聚合群体偏好反馈，还能够估计每个用户的可靠性。

    Preference-based reinforcement learning (RL) provides a framework to train agents using human feedback through pairwise preferences over pairs of behaviors, enabling agents to learn desired behaviors when it is difficult to specify a numerical reward function. While this paradigm leverages human feedback, it currently treats the feedback as given by a single human user. Meanwhile, incorporating preference feedback from crowds (i.e. ensembles of users) in a robust manner remains a challenge, and the problem of training RL agents using feedback from multiple human users remains understudied. In this work, we introduce Crowd-PrefRL, a framework for performing preference-based RL leveraging feedback from crowds. This work demonstrates the viability of learning reward functions from preference feedback provided by crowds of unknown expertise and reliability. Crowd-PrefRL not only robustly aggregates the crowd preference feedback, but also estimates the reliability of each user within the cr
    
[^43]: RELIANCE: 可靠的集成学习用于信息和新闻可信度评估

    RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation. (arXiv:2401.10940v1 [cs.IR])

    [http://arxiv.org/abs/2401.10940](http://arxiv.org/abs/2401.10940)

    RELIANCE是一个可靠的集成学习系统，用于评估信息和新闻的可信度。它通过整合多个基本模型的优势，提供了对可信和不可信信息源的准确区分，并在信息和新闻可信度评估方面优于基准模型。

    

    在信息泛滥的时代，辨别新闻内容的可信度越来越具有挑战性。本文介绍了RELIANCE，这是一个专为鲁棒信息和虚假新闻可信度评估而设计的先进的集成学习系统。RELIANCE由五个不同的基本模型组成，包括支持向量机（SVM）、朴素贝叶斯、逻辑回归、随机森林和双向长短期记忆网络（BiLSTMs）。RELIANCE采用了创新的方法来整合它们的优势，利用集成的智能提高准确性。实验证明了RELIANCE在区分可信和不可信信息源方面的优越性，表明其在信息和新闻可信度评估方面超过了单个模型，并成为评估信息源可靠性的有效解决方案。

    In the era of information proliferation, discerning the credibility of news content poses an ever-growing challenge. This paper introduces RELIANCE, a pioneering ensemble learning system designed for robust information and fake news credibility evaluation. Comprising five diverse base models, including Support Vector Machine (SVM), naive Bayes, logistic regression, random forest, and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs an innovative approach to integrate their strengths, harnessing the collective intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the superiority of RELIANCE over individual models, indicating its efficacy in distinguishing between credible and non-credible information sources. RELIANCE, also surpasses baseline models in information and news credibility assessment, establishing itself as an effective solution for evaluating the reliability of information sources.
    
[^44]: 即使解释：形式基础，优先级和复杂性

    Even-if Explanations: Formal Foundations, Priorities and Complexity. (arXiv:2401.10938v1 [cs.AI])

    [http://arxiv.org/abs/2401.10938](http://arxiv.org/abs/2401.10938)

    本论文研究了解释性人工智能中的局部事后解释性查询，特别关注半事实的解释，并对线性模型和基于树的模型与神经网络的解释能力进行了比较。此外，提出了一种基于偏好的框架，允许用户根据自己的首选项个性化解释。最后，探讨了模型复杂度的问题。

    

    解释性人工智能近年来受到了重要关注。机器学习模型通常作为黑盒子运行，缺乏解释和透明性，而又支持决策过程。局部事后解释性查询试图回答为什么给定模型如何对个体输入进行分类的问题。虽然关于反事实解释已经进行了重要工作，但对半事实解释的关注较少。本文关注于半事实的局部事后解释性查询以及不同模型类别中其计算复杂性，并表明线性和基于树的模型比神经网络更易于解释。接着，我们介绍了一种基于偏好的框架，使用户能够根据自己的偏好个性化解释，无论是在半事实还是反事实的情况下，提高解释能力和用户中心性。最后，我们探讨了模型复杂度。

    EXplainable AI has received significant attention in recent years. Machine learning models often operate as black boxes, lacking explainability and transparency while supporting decision-making processes. Local post-hoc explainability queries attempt to answer why individual inputs are classified in a certain way by a given model. While there has been important work on counterfactual explanations, less attention has been devoted to semifactual ones. In this paper, we focus on local post-hoc explainability queries within the semifactual `even-if' thinking and their computational complexity among different classes of models, and show that both linear and tree-based models are strictly more interpretable than neural networks. After this, we introduce a preference-based framework that enables users to personalize explanations based on their preferences, both in the case of semifactuals and counterfactuals, enhancing interpretability and user-centricity. Finally, we explore the complexity o
    
[^45]: 预测加密货币质押奖励

    Forecasting Cryptocurrency Staking Rewards. (arXiv:2401.10931v1 [q-fin.ST])

    [http://arxiv.org/abs/2401.10931](http://arxiv.org/abs/2401.10931)

    本研究研究了预测加密货币质押奖励的方法，发现使用滑动窗口平均法可以在很大程度上准确预测ETH的奖励，并且不同加密货币的预测准确度存在差异。线性回归方法在XTZ和ATOM的短期预测中表现出优势，结果突显了大多数资产质押奖励的稳定可预测性，MATIC是一个值得注意的异常情况。

    

    本研究探索了一个相对未被探索的领域，即预测加密货币质押奖励，为研究人员和投资者提供潜在的洞察。我们研究了两种预测方法：a）简单的滑动窗口平均法，和b）基于历史数据的线性回归模型。研究结果表明，使用7天滑动窗口平均法，ETH质押奖励可以在1天和7天的预测中，预测误差均在均值的0.7%和1.1%之间。此外，我们还发现不同加密货币（包括SOL、XTZ、ATOM和MATIC）的预测准确度存在差异。对于XTZ和ATOM，线性回归方法在短期预测中被认为优于滑动窗口平均法。结果强调了大多数资产的质押奖励通常是稳定可预测的，而MATIC则是一个值得注意的异常情况。

    This research explores a relatively unexplored area of predicting cryptocurrency staking rewards, offering potential insights to researchers and investors. We investigate two predictive methodologies: a) a straightforward sliding-window average, and b) linear regression models predicated on historical data. The findings reveal that ETH staking rewards can be forecasted with an RMSE within 0.7% and 1.1% of the mean value for 1-day and 7-day look-aheads respectively, using a 7-day sliding-window average approach. Additionally, we discern diverse prediction accuracies across various cryptocurrencies, including SOL, XTZ, ATOM, and MATIC. Linear regression is identified as superior to the moving-window average for perdicting in the short term for XTZ and ATOM. The results underscore the generally stable and predictable nature of staking rewards for most assets, with MATIC presenting a noteworthy exception.
    
[^46]: 使用半正定规划的去偏和局部分析进行人群聚类

    Debiasing and a local analysis for population clustering using semidefinite programming. (arXiv:2401.10927v1 [stat.ML])

    [http://arxiv.org/abs/2401.10927](http://arxiv.org/abs/2401.10927)

    本文研究了使用半正定规划进行人群聚类的问题，并提出了计算高效的算法。这些算法可以根据小样本数据的原始种群将数据分为两组，适用于种群之间差异较小的情况。

    

    本文考虑了从混合的2个次高斯分布中抽取的小数据样本的分区问题。我们分析了同一作者提出的计算高效的算法，将数据根据其原始种群大致分为两组，给定一个小样本。本文的研究动机是将个体根据其原始种群使用p个标记进行聚类，当任意两个种群之间的差异很小时。我们基于整数二次规划的半正定松弛形式构建，该规划问题本质上是在一个图上找到最大割，其中割中的边权重表示基于它们的p个特征的两个节点之间的不相似度得分。我们用Δ^2:=pγ来表示两个中心（均值向量）之间的ℓ_2^2距离，即μ^(1), μ^(2)∈ℝ^p。目标是在交换精度和计算效率之间提供全面的权衡。

    In this paper, we consider the problem of partitioning a small data sample of size $n$ drawn from a mixture of $2$ sub-gaussian distributions. In particular, we analyze computational efficient algorithms proposed by the same author, to partition data into two groups approximately according to their population of origin given a small sample. This work is motivated by the application of clustering individuals according to their population of origin using $p$ markers, when the divergence between any two of the populations is small. We build upon the semidefinite relaxation of an integer quadratic program that is formulated essentially as finding the maximum cut on a graph, where edge weights in the cut represent dissimilarity scores between two nodes based on their $p$ features. Here we use $\Delta^2 :=p \gamma$ to denote the $\ell_2^2$ distance between two centers (mean vectors), namely, $\mu^{(1)}$, $\mu^{(2)}$ $\in$ $\mathbb{R}^p$. The goal is to allow a full range of tradeoffs between
    
[^47]: 基于推和拉的有效通信在网络物理系统中的应用

    Push- and Pull-based Effective Communication in Cyber-Physical Systems. (arXiv:2401.10921v1 [eess.SY])

    [http://arxiv.org/abs/2401.10921](http://arxiv.org/abs/2401.10921)

    本论文研究了在网络物理系统中基于推和拉的通信的优化问题，发现最优策略与信息价值最大化相一致，结果表明在特定情况下，基于拉的通信可能比基于推的通信更有效。

    

    在网络物理系统（CPS）中，两组参与者通过互动来最大化系统性能：观测和传播系统状态的传感器和基于接收到的信息进行物理决策的执行器。虽然通常假设传感器定期传输更新，仅在必要时返回反馈信号，并因此根据通信策略调整物理决策，但实际上通过选择基于推的通信（传感器自主启动更新）和基于拉的通信（执行器请求更新）可以显著提高系统的效率。在这项工作中，我们提出了一个优化CPS中基于推和拉的通信的分析模型，观察到策略的最优性与信息价值（VoI）最大化相一致。我们的结果还表明，尽管提供了更好的最优解决方案，但在特定情况下，基于拉的通信可能会比基于推的通信更有效。

    In Cyber Physical Systems (CPSs), two groups of actors interact toward the maximization of system performance: the sensors, observing and disseminating the system state, and the actuators, performing physical decisions based on the received information. While it is generally assumed that sensors periodically transmit updates, returning the feedback signal only when necessary, and consequently adapting the physical decisions to the communication policy, can significantly improve the efficiency of the system. In particular, the choice between push-based communication, in which updates are initiated autonomously by the sensors, and pull-based communication, in which they are requested by the actuators, is a key design step. In this work, we propose an analytical model for optimizing push- and pull-based communication in CPSs, observing that the policy optimality coincides with Value of Information (VoI) maximization. Our results also highlight that, despite providing a better optimal solu
    
[^48]: 机器学习在股票市场预测中的应用：迪士尼股票的案例研究

    Application of Machine Learning in Stock Market Forecasting: A Case Study of Disney Stock. (arXiv:2401.10903v1 [q-fin.ST])

    [http://arxiv.org/abs/2401.10903](http://arxiv.org/abs/2401.10903)

    本文研究了机器学习在股票市场预测中的应用，以迪士尼股票为案例进行了分析。通过探索性数据分析、特征工程、数据准备和模型选择，发现线性回归模型表现最佳。

    

    本文介绍了对一份包含750个实例和16个属性的数据集进行的股票市场分析。分析包括探索性数据分析 (EDA) 部分、特征工程、数据准备、模型选择以及分析的见解。在分析中还运用了Fama French 3因子模型。结果显示，线性回归是表现最佳的模型。

    This document presents a stock market analysis conducted on a dataset consisting of 750 instances and 16 attributes donated in 2014-10-23. The analysis includes an exploratory data analysis (EDA) section, feature engineering, data preparation, model selection, and insights from the analysis. The Fama French 3-factor model is also utilized in the analysis. The results of the analysis are presented, with linear regression being the best-performing model.
    
[^49]: 供应链风险评估中的人工智能：一项系统文献综述和文献计量分析

    AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis. (arXiv:2401.10895v1 [cs.LG])

    [http://arxiv.org/abs/2401.10895](http://arxiv.org/abs/2401.10895)

    本文通过系统文献综述和文献计量分析，填补了供应链风险评估中新兴人工智能/机器学习技术的研究空白，为了解这些技术在实践中的实际影响提供了关键见解。

    

    通过整合人工智能和机器学习技术，供应链风险评估(SCRA)经历了深刻的演变，革新了预测能力和风险缓解策略。这种演变的重要性在于在现代供应链中确保运营的韧性和连续性，需要稳健的风险管理策略。以往的综述已经概述了已建立的方法，但忽视了新兴的人工智能/机器学习技术，在理解其在SCRA中的实际影响方面存在明显的研究空白。本文进行了系统的文献综述，并结合了全面的文献计量分析。我们仔细研究了1717篇论文，并从2014年至2023年之间发表的48篇文章中获得了关键见解。该综述填补了这一研究空白，通过回答关键研究问题，探究了现有的人工智能/机器学习技术、方法论、研究结果和未来发展方向。

    Supply chain risk assessment (SCRA) has witnessed a profound evolution through the integration of artificial intelligence (AI) and machine learning (ML) techniques, revolutionizing predictive capabilities and risk mitigation strategies. The significance of this evolution stems from the critical role of robust risk management strategies in ensuring operational resilience and continuity within modern supply chains. Previous reviews have outlined established methodologies but have overlooked emerging AI/ML techniques, leaving a notable research gap in understanding their practical implications within SCRA. This paper conducts a systematic literature review combined with a comprehensive bibliometric analysis. We meticulously examined 1,717 papers and derived key insights from a select group of 48 articles published between 2014 and 2023. The review fills this research gap by addressing pivotal research questions, and exploring existing AI/ML techniques, methodologies, findings, and future 
    
[^50]: Starlit: 隐私保护的联邦学习以增强金融欺诈检测

    Starlit: Privacy-Preserving Federated Learning to Enhance Financial Fraud Detection. (arXiv:2401.10765v1 [cs.LG])

    [http://arxiv.org/abs/2401.10765](http://arxiv.org/abs/2401.10765)

    Starlit是一个新的可扩展隐私保护的联邦学习机制，解决了对于金融欺诈检测中的几个限制，包括缺乏正式的安全定义和证明、假定冻结账户、规模扩大、身份对齐阶段和难以抵抗客户端退出。

    

    联邦学习（FL）是一种数据最小化方法，能够在具有本地数据的各个客户端之间进行协作模型训练，避免直接数据交换。然而，用于识别欺诈金融交易的最新FL解决方案存在以下一些限制：缺乏正式的安全定义和证明，假定金融机构事先冻结可疑客户的账户（限制了解决方案的采用），规模不断扩大，涉及$O(n^2)$的计算昂贵的模块指数运算（其中$n$是金融机构的总数）或者高效率低的完全同态加密，假设各方已经完成了身份对齐阶段，因此将其排除在实施、性能评估和安全分析之外，并且难以抵抗客户端的退出。本文引入了一种新颖的可扩展隐私保护FL机制——Starlit，克服了这些限制。

    Federated Learning (FL) is a data-minimization approach enabling collaborative model training across diverse clients with local data, avoiding direct data exchange. However, state-of-the-art FL solutions to identify fraudulent financial transactions exhibit a subset of the following limitations. They (1) lack a formal security definition and proof, (2) assume prior freezing of suspicious customers' accounts by financial institutions (limiting the solutions' adoption), (3) scale poorly, involving either $O(n^2)$ computationally expensive modular exponentiation (where $n$ is the total number of financial institutions) or highly inefficient fully homomorphic encryption, (4) assume the parties have already completed the identity alignment phase, hence excluding it from the implementation, performance evaluation, and security analysis, and (5) struggle to resist clients' dropouts. This work introduces Starlit, a novel scalable privacy-preserving FL mechanism that overcomes these limitations
    
[^51]: PuriDefense：用于防御黑盒基于查询的攻击的随机局部隐式对抗净化

    PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR])

    [http://arxiv.org/abs/2401.10586](http://arxiv.org/abs/2401.10586)

    PuriDefense是一种高效的防御机制，通过使用轻量级净化模型进行随机路径净化，减缓基于查询的攻击的收敛速度，并有效防御黑盒基于查询的攻击。

    

    黑盒基于查询的攻击对机器学习作为服务系统构成重大威胁，因为它们可以生成对抗样本而不需要访问目标模型的架构和参数。传统的防御机制，如对抗训练、梯度掩盖和输入转换，要么带来巨大的计算成本，要么损害非对抗输入的测试准确性。为了应对这些挑战，我们提出了一种高效的防御机制PuriDefense，在低推理成本的级别上使用轻量级净化模型的随机路径净化。这些模型利用局部隐式函数并重建自然图像流形。我们的理论分析表明，这种方法通过将随机性纳入净化过程来减缓基于查询的攻击的收敛速度。对CIFAR-10和ImageNet的大量实验验证了我们提出的净化器防御的有效性。

    Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost. These models leverage the local implicit function and rebuild the natural image manifold. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defen
    
[^52]: 学习辅助的随机容量扩展规划：一种贝叶斯优化方法

    Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian Optimization Approach. (arXiv:2401.10451v1 [eess.SY])

    [http://arxiv.org/abs/2401.10451](http://arxiv.org/abs/2401.10451)

    本研究提出了一种学习辅助的贝叶斯优化方法，用于解决大规模容量扩展问题。通过构建和求解可行的时间聚合代理问题，识别出低成本的规划决策。通过在验证集和测试预测上评估解决的规划结果，实现了随机容量扩展问题的可行解决。

    

    解决大规模的容量扩展问题对于区域能源系统的成本效益低碳化至关重要。为了确保容量扩展问题的预期结果，建模考虑到天气相关的可再生能源供应和能源需求的不确定性变得至关重要。然而，由此产生的随机优化模型通常比确定性模型难以计算。在这里，我们提出了一种学习辅助的近似解法来可行地解决两阶段随机容量扩展问题。我们的方法通过构建和求解一系列可行的时间聚合代理问题，识别出低成本的规划决策。我们采用贝叶斯优化方法搜索时间序列聚合超参数的空间，并计算在供需预测的验证集上最小化成本的近似解。重要的是，我们在一组保留的测试预测上评估解决的规划结果。

    Solving large-scale capacity expansion problems (CEPs) is central to cost-effective decarbonization of regional-scale energy systems. To ensure the intended outcomes of CEPs, modeling uncertainty due to weather-dependent variable renewable energy (VRE) supply and energy demand becomes crucially important. However, the resulting stochastic optimization models are often less computationally tractable than their deterministic counterparts. Here, we propose a learning-assisted approximate solution method to tractably solve two-stage stochastic CEPs. Our method identifies low-cost planning decisions by constructing and solving a sequence of tractable temporally aggregated surrogate problems. We adopt a Bayesian optimization approach to searching the space of time series aggregation hyperparameters and compute approximate solutions that minimize costs on a validation set of supply-demand projections. Importantly, we evaluate solved planning outcomes on a held-out set of test projections. We 
    
[^53]: 自然的功率法则学习环境中能够减轻灾难性干扰

    Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v1 [cs.LG])

    [http://arxiv.org/abs/2401.10393](http://arxiv.org/abs/2401.10393)

    本研究在自然学习环境中通过回忆方法减轻了灾难性干扰，该方法受到功率法则的启发。

    

    神经网络通常遭受灾难性干扰（CI）：在学习新任务时，先前学习任务的表现显著下降。这与人类形成鲜明对比，人类可以连续学习新任务而不会明显忘记先前的任务。先前的工作已经探索了各种减轻CI的技术，例如正则化、回忆、生成性回放和浓缩方法。本研究采用了一种不同的方法，该方法受到认知科学研究的指导，该研究表明在自然环境中，遇到任务的概率与最后一次执行任务的时间成功率法则递减。我们认为，在模拟自然学习环境中进行减轻CI技术的真实评估是必要的。因此，我们评估了在类似人类面临的功率法则环境中训练简单的回忆方法时，CI的减轻程度。我们的工作探索了这种基于回忆的新方法。

    Neural networks often suffer from catastrophic interference (CI): performance on previously learned tasks drops off significantly when learning a new task. This contrasts strongly with humans, who can sequentially learn new tasks without appreciably forgetting previous tasks. Prior work has explored various techniques for mitigating CI such as regularization, rehearsal, generative replay, and distillation methods. The current work takes a different approach, one guided by cognitive science research showing that in naturalistic environments, the probability of encountering a task decreases as a power-law of the time since it was last performed. We argue that a realistic evaluation of techniques for the mitigation of CI should be performed in simulated naturalistic learning environments. Thus, we evaluate the extent of mitigation of CI when training simple rehearsal-based methods in power-law environments similar to the ones humans face. Our work explores this novel rehearsal-based appro
    
[^54]: Langevin遗忘：噪声梯度下降的机器遗忘新视角

    Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning. (arXiv:2401.10371v1 [cs.LG])

    [http://arxiv.org/abs/2401.10371](http://arxiv.org/abs/2401.10371)

    Langevin遗忘是一种基于噪声梯度下降的遗忘框架，能够在近似遗忘问题中提供隐私保证，并且具有算法上的优势。

    

    随着采用确保“被遗忘权”的法律，机器遗忘引起了极大的兴趣。研究人员提供了一个概率性的近似遗忘定义，类似于差分隐私（DP）的定义，其中隐私被定义为对重新训练的统计不可区分性。我们提出了Langevin遗忘，这是一个基于噪声梯度下降的近似遗忘问题的隐私保证的遗忘框架。Langevin遗忘在算法上统一了DP学习过程和隐私认证的遗忘过程。其中包括非凸问题的近似认证遗忘，相对于重新训练的复杂度节省，以及用于多个遗忘请求的顺序和批量遗忘。我们通过在基准数据集上的实验验证了Langevin遗忘的实用性，并展示了它对梯度下降的优势。

    Machine unlearning has raised significant interest with the adoption of laws ensuring the ``right to be forgotten''. Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests. We verify the practicality of Langevin unlearning by studying its privacy-utility-complexity trade-off via experiments on benchmark datasets, and also demonstrate its superiority against gradient-decent-p
    
[^55]: 基于噪声对比估计的低资源安全攻击模式识别匹配框架

    Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])

    [http://arxiv.org/abs/2401.10337](http://arxiv.org/abs/2401.10337)

    该论文提出了一种基于噪声对比估计的低资源安全攻击模式识别匹配框架，通过直接语义相似度决定文本与攻击模式之间的关联，以降低大量类别、标签分布不均和标签空间复杂性带来的学习难度。

    

    战术、技术和程序（TTPs）是网络安全领域中复杂的攻击模式，在文本知识库中有详细的描述。在网络安全写作中识别TTPs，通常称为TTP映射，是一个重要而具有挑战性的任务。传统的学习方法通常以经典的多类或多标签分类设置为目标。由于存在大量的类别（即TTPs），标签分布的不均衡和标签空间的复杂层次结构，这种设置限制了模型的学习能力。我们采用了一种不同的学习范式来解决这个问题，其中将文本与TTP标签之间的直接语义相似度决定为文本分配给TTP标签，从而减少了仅仅在大型标签空间上竞争的复杂性。为此，我们提出了一种具有有效的基于采样的学习比较机制的神经匹配架构，促进学习过程。

    Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
    
[^56]: 通过手机传感器推断个性特征：一种机器学习方法

    Personality Trait Inference Via Mobile Phone Sensors: A Machine Learning Approach. (arXiv:2401.10305v1 [eess.SP])

    [http://arxiv.org/abs/2401.10305](http://arxiv.org/abs/2401.10305)

    该研究通过手机传感器收集的活动数据可靠地预测了个性特征，这些研究成果为社会科学研究提供了新的途径。通过使用智能手机传感和机器学习技术，可以以成本效益高、无问卷调查的方式对个性相关问题进行研究。这些发现有助于推动个性研究，并且可以为从业者和研究人员提供信息。

    

    该研究提供了证据表明，可以通过手机传感器收集的活动数据可靠地预测个性特征。通过加速度计记录和运动模式计算得出一组知情指标，我们能够在两类问题上预测用户的个性特征，达到了0.78的F1分数。鉴于手机收集数据的快速增长，我们的新颖个性特征指标为社会科学未来研究开辟了令人兴奋的途径。我们的研究结果揭示了不同行为模式，它们被证明有差异性地预测了五大人格特征。它们有潜力以成本效益高、无问卷调查的方式，在前所未有的规模上研究与个性相关的问题。总体而言，该论文展示了使用智能手机传感和机器学习技术获得丰富行为数据的组合如何帮助推动个性研究，并且可以向从业者和研究人员提供信息。

    This study provides evidence that personality can be reliably predicted from activity data collected through mobile phone sensors. Employing a set of well informed indicators calculable from accelerometer records and movement patterns, we were able to predict users' personality up to a 0.78 F1 score on a two class problem. Given the fast growing number of data collected from mobile phones, our novel personality indicators open the door to exciting avenues for future research in social sciences. Our results reveal distinct behavioral patterns that proved to be differentially predictive of big five personality traits. They potentially enable cost effective, questionnaire free investigation of personality related questions at an unprecedented scale. Overall, this paper shows how a combination of rich behavioral data obtained with smartphone sensing and the use of machine learning techniques can help to advance personality research and can inform both practitioners and researchers about th
    
[^57]: Chem-FINESE: 通过文本重构验证细粒度少样本实体提取

    Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])

    [http://arxiv.org/abs/2401.10189](http://arxiv.org/abs/2401.10189)

    这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。

    

    在化学领域中，细粒度少样本实体提取面临两个独特的挑战。首先，与一般领域的实体提取任务相比，化学论文中的句子通常包含更多的实体。此外，实体提取模型通常难以提取长尾类型的实体。在本文中，我们提出了一种新颖的基于序列到序列的少样本实体提取方法Chem-FINESE来解决这两个挑战。我们的Chem-FINESE包含两个组件：一个序列到序列的实体提取器用于从输入句子中提取命名实体，以及一个序列到序列的自我验证模块用于从提取的实体中重构原始输入句子。受到一个好的实体提取系统需要忠实提取实体的事实启发，我们的新自我验证模块利用实体提取结果来重构原始输入句子。此外，我们设计了一种新的对比损失来减少在提取过程中的过度复制。

    Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
    
[^58]: 一种新型的混合时变图神经网络用于交通流量预测

    A novel hybrid time-varying graph neural network for traffic flow forecasting. (arXiv:2401.10155v1 [cs.LG])

    [http://arxiv.org/abs/2401.10155](http://arxiv.org/abs/2401.10155)

    本文提出了一种新型的混合时变图神经网络（HTVGNN）用于交通流量预测，解决了现有方法中预定义图和自适应图的学习能力受限的问题。

    

    实时准确的交通流量预测是确保智能交通系统高效运行的基础。在现有基于图神经网络（GNN）的交通流量预测方法中，通常使用预定义的图来描述城市道路网络中不同交通节点的空间相关性。然而，预定义图描述空间相关性的能力受限于先前的知识和图生成方法。尽管基于数据驱动学习的时变图能部分克服预定义图的缺点，但现有自适应图的学习能力有限。例如，时变图不能充分捕捉交通流量数据中固有的空间相关性。为了解决这些问题，我们提出了一种用于交通流量预测的混合时变图神经网络（HTVGNN）。

    Real-time and accurate traffic flow prediction is the foundation for ensuring the efficient operation of intelligent transportation systems.In existing traffic flow prediction methods based on graph neural networks (GNNs), pre-defined graphs were usually used to describe the spatial correlations of different traffic nodes in urban road networks. However, the ability of pre-defined graphs used to describe spatial correlation was limited by prior knowledge and graph generation methods. Although time-varying graphs based on data-driven learning can partially overcome the drawbacks of pre-defined graphs, the learning ability of existing adaptive graphs was limited. For example, time-varying graphs cannot adequately capture the inherent spatial correlations in traffic flow data.In order to solve these problems, we have proposed a hybrid time-varying graph neural network (HTVGNN) for traffic flow prediction.
    
[^59]: 标准多导睡眠图与耳内脑电信号的比较分析：初步研究

    Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study. (arXiv:2401.10107v1 [eess.SP])

    [http://arxiv.org/abs/2401.10107](http://arxiv.org/abs/2401.10107)

    本研究通过比较标准多导睡眠图（PSG）和耳内脑电信号的相似性，旨在探索一种更少侵入、成本效益高和便携的替代方法。研究确定了评估方法，并通过提取特征进行分析。

    

    研究目的：多导睡眠图（PSG）目前被用作评估睡眠障碍的基准。其不舒适、不适合家庭使用以及在睡眠质量评估中引入偏差的问题需要探索更少侵入性、成本效益高和便携性的替代方法。一种有前景的候选方法是耳内脑电传感器，它在舒适性、固定电极位置、抗电磁干扰性和易于使用性方面均具有优势。本研究旨在建立一种评估耳内脑电信号与标准PSG之间相似性的方法。方法：我们评估PSG和耳内脑电推导的睡眠图之间的一致性。我们从PSG和耳内脑电信号的30秒时域和频域提取特征。我们只考虑在PSG评分员和耳内脑电评分员达成一致时的时段。我们引入一种方法来量化PSG推导和单通道耳内脑电之间的相似性。该方法包括...

    Study Objectives: Polysomnography (PSG) currently serves as the benchmark for evaluating sleep disorders. Its discomfort, impracticality for home-use, and introduction of bias in sleep quality assessment necessitate the exploration of less invasive, cost-effective, and portable alternatives. One promising contender is the in-ear-EEG sensor, which offers advantages in terms of comfort, fixed electrode positions, resistance to electromagnetic interference, and user-friendliness. This study aims to establish a methodology to assess the similarity between the in-ear-EEG signal and standard PSG.  Methods: We assess the agreement between the PSG and in-ear-EEG derived hypnograms. We extract features in the time- and frequency- domain from PSG and in-ear-EEG 30-second epochs. We only consider the epochs where the PSG-scorers and the in-ear-EEG-scorers were in agreement. We introduce a methodology to quantify the similarity between PSG derivations and the single-channel in-ear-EEG. The approac
    
[^60]: 通过双棱镜: 光谱视角下的图数据增强用于图分类

    Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification. (arXiv:2401.09953v1 [cs.LG])

    [http://arxiv.org/abs/2401.09953](http://arxiv.org/abs/2401.09953)

    通过光谱视角，研究了图数据增强方法中的图属性和结构变化的关系，发现保持低频特征值不变可以保留关键属性，提出了双棱镜（DP）增强方法，该方法灵活地保留关键的图属性同时增加图的多样性。

    

    图神经网络（GNNs）已成为处理图数据的首选工具，其通过图数据增强技术的提高加强了其有效性。尽管增强方法的发展，但图属性扭曲和受限结构变化等问题仍然存在。这引发了一个问题：是否可能开发更加保留属性并具有结构敏感性的增强方法？通过光谱镜头，我们研究了图属性、它们的增强和它们的光谱行为之间的相互作用，并发现保持低频特征值不变可以保持生成的增强图的关键属性。这些观察结果启发我们引入了双棱镜（DP）增强方法，包括DP-Noise和DP-Mask，它们灵活地保留了关键的图属性并丰富了增强图。大量实验证实了我们方法的有效性，为一种新的、有前景的直接方法提供了支持。

    Graph Neural Networks (GNNs) have become the preferred tool to process graph data, with their efficacy being boosted through graph data augmentation techniques. Despite the evolution of augmentation methods, issues like graph property distortions and restricted structural changes persist. This leads to the question: Is it possible to develop more property-conserving and structure-sensitive augmentation methods? Through a spectral lens, we investigate the interplay between graph properties, their augmentation, and their spectral behavior, and found that keeping the low-frequency eigenvalues unchanged can preserve the critical properties at a large scale when generating augmented graphs. These observations inform our introduction of the Dual-Prism (DP) augmentation method, comprising DP-Noise and DP-Mask, which adeptly retains essential graph properties while diversifying augmented graphs. Extensive experiments validate the efficiency of our approach, providing a new and promising direct
    
[^61]: PatchAD: 基于块的MLP-Mixer的时间序列异常检测

    PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection. (arXiv:2401.09793v1 [cs.LG])

    [http://arxiv.org/abs/2401.09793](http://arxiv.org/abs/2401.09793)

    PatchAD是一种新颖的基于块的MLP-Mixer体系结构，利用对比学习进行时间序列异常检测。它具有高效和轻量级的架构，并采用创新的双项目约束模块来提高表示能力。

    

    异常检测是时间序列分析的关键方面，旨在识别时间序列样本中的异常事件。这一任务的核心挑战在于在缺乏标签的情况下有效地学习正常和异常模式的表示。先前的研究大多依赖于基于重构的方法，限制了模型的表征能力。此外，大多数当前的深度学习方法不够轻量级，这促使我们设计一个更高效的异常检测框架。本研究中，我们介绍了PatchAD，一种新颖的多尺度基于块的MLP-Mixer体系结构，利用对比学习进行表征提取和异常检测。具体而言，PatchAD由四个独特的MLP Mixer组成，专门利用MLP架构实现高效和轻量级的架构。此外，我们还创新地设计了一个双项目约束模块来缓解潜在的问题。

    Anomaly detection stands as a crucial aspect of time series analysis, aiming to identify abnormal events in time series samples. The central challenge of this task lies in effectively learning the representations of normal and abnormal patterns in a label-lacking scenario. Previous research mostly relied on reconstruction-based approaches, restricting the representational abilities of the models. In addition, most of the current deep learning-based methods are not lightweight enough, which prompts us to design a more efficient framework for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale patch-based MLP-Mixer architecture that leverages contrastive learning for representational extraction and anomaly detection. Specifically, PatchAD is composed of four distinct MLP Mixers, exclusively utilizing the MLP architecture for high efficiency and lightweight architecture. Additionally, we also innovatively crafted a dual project constraint module to mitigate potenti
    
[^62]: 迈向可识别的无监督领域转换：一种多样化分布匹配的方法

    Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach. (arXiv:2401.09671v1 [cs.LG])

    [http://arxiv.org/abs/2401.09671](http://arxiv.org/abs/2401.09671)

    本研究旨在解决无监督领域转换中的可识别性问题，引入了一个MPA消除理论，解决了CycleGAN及其变体产生内容不对齐的限制。

    

    无监督领域转换（UDT）旨在找到将一个领域的样本（例如素描）转换为另一个领域（例如照片）的函数，同时不改变高层语义意义（也称为“内容”）。这些转换函数通常通过转换源领域和目标领域的概率分布来寻找。CycleGAN可以说是这一领域中最具代表性的方法。然而，文献中指出CycleGAN及其变体可能无法识别所需的转换函数，并产生内容不对齐的转换。这种局限性源于学习准则解空间中存在多个转换函数，称为“保度自同构（MPA）”。尽管意识到了这种可识别性问题，但解决方案仍然难以找到。本研究深入探究了核心的可识别性问题，并引入了MPA消除理论。我们的分析表明...

    Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysi
    
[^63]: 扩散驱动的分子构象预测生成框架

    Diffusion-Driven Generative Framework for Molecular Conformation Prediction. (arXiv:2401.09451v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.09451](http://arxiv.org/abs/2401.09451)

    本文介绍了一种基于扩散驱动的生成框架\method{}，用于预测分子的三维构象，具有较高的预测精度并改进了传统方法的不足。

    

    从二维图形表示中推断出三维分子构型的任务在计算化学和药物开发领域具有重要意义。它对我们理解分子机制和相互作用起着基本作用。机器学习，特别是深度生成网络的快速发展，推动了这种预测建模精度的突破。传统方法通常采用分叉策略：首先估计原子间距，然后通过解决距离几何问题来塑造分子的空间结构。然而，这种顺序方法有时无法准确捕捉到局部原子排列的复杂性，从而损害结果结构模型的完整性。为了解决这些不足，本文引入了一个前卫的生成框架：\method{}，它基于扩散驱动的方法进行预测，并取得了重要的改进。

    The task of inferring three-dimensional molecular configurations from their two-dimensional graph representations is of critical significance in the domains of computational chemistry and the development of pharmaceuticals. It contributes fundamentally to our grasp of molecular mechanisms and interactions. The rapid evolution of machine learning, especially in the realm of deep generative networks, has catalyzed breakthroughs in the precision of such predictive modeling. Traditional methodologies typically employ a bifurcated strategy: initially estimating interatomic distances followed by sculpting the spatial molecular structure via solving a distance geometry problem. This sequential approach, however, occasionally fails to capture the intricacies of local atomic arrangements accurately, thus compromising the integrity of the resultant structural models. Addressing these deficiencies, this work introduces an avant-garde generative framework: \method{}, which is predicated on the dif
    
[^64]: 机器能够看到颜色：大规模语料库中分类不同形式种族主义言论的准则

    Machines Do See Color: A Guideline to Classify Different Forms of Racist Discourse in Large Corpora. (arXiv:2401.09333v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.09333](http://arxiv.org/abs/2401.09333)

    本文提供了一个逐步可推广的准则，用于在大规模语料库中识别和分类不同形式的种族主义言论。通过对种族主义的概念化和上下文化，以及使用XLM-R和XLM-R-Racismo模型，我们展示了在大规模语料库中进行种族主义分类的优势。

    

    目前识别和分类文本中的种族主义语言的方法主要依赖小规模的质性方法或大规模的方法，专注于明显的种族主义言论。本文提供了一个逐步可推广的准则，用于在大规模语料库中识别和分类不同形式的种族主义言论。在我们的方法中，我们首先将种族主义及其不同表现形式进行概念化。然后，我们将这些种族主义表现形式置于感兴趣的时间和地点背景下，以便研究人员能够识别它们的话语形式。最后，我们应用了XLM-RoBERTa（XLM-R），这是一个具有先进上下文理解能力的跨语言监督文本分类模型。我们展示了XLM-R和XLM-R-Racismo（我们的预训练模型）在大规模语料库中对种族主义进行分类的性能优于其他最先进的方法。我们通过使用涉及2018年至2021年厄瓜多尔本土群体的推文语料库来说明我们的方法。

    Current methods to identify and classify racist language in text rely on small-n qualitative approaches or large-n approaches focusing exclusively on overt forms of racist discourse. This article provides a step-by-step generalizable guideline to identify and classify different forms of racist discourse in large corpora. In our approach, we start by conceptualizing racism and its different manifestations. We then contextualize these racist manifestations to the time and place of interest, which allows researchers to identify their discursive form. Finally, we apply XLM-RoBERTa (XLM-R), a cross-lingual model for supervised text classification with a cutting-edge contextual understanding of text. We show that XLM-R and XLM-R-Racismo, our pretrained model, outperform other state-of-the-art approaches in classifying racism in large corpora. We illustrate our approach using a corpus of tweets relating to the Ecuadorian ind\'igena community between 2018 and 2021.
    
[^65]: 大型语言模型中的代码模拟挑战

    Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])

    [http://arxiv.org/abs/2401.09074](http://arxiv.org/abs/2401.09074)

    大型语言模型在模拟计算机代码和算法执行方面遇到挑战，性能随着代码长度的增加而迅速下降。在处理短程序或标准过程时，它们能以低错误率按顺序执行指令，但对于复杂的程序，特别是包含关键路径和冗余指令的程序，模拟效果较差。我们提出了一种逐行模拟代码执行的方法来解决这个问题。

    

    我们调查了大型语言模型（LLMs）在模拟计算机代码和算法执行方面的能力。我们首先研究了直线程序，并展示了当前LLMs在处理这样简单的程序时表现出的性能较差——性能随着代码长度的增加而迅速下降。接着，我们研究了LLMs在模拟包含关键路径和冗余指令的程序方面的能力。我们还通过排序算法和嵌套循环超越了直线程序的模拟，并展示了程序的计算复杂性直接影响LLMs模拟其执行的能力。我们观察到LLMs只有在处理短程序或标准过程时才能以低错误率按顺序执行指令。LLMs的代码模拟与它们的模式识别和记忆能力存在矛盾：在记忆对任务有害的情况下，我们提出了一种新的提示方法，逐行模拟代码的执行。

    We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
    
[^66]: 数据归因对扩散模型的影响：时间步引起的对影响估计的偏差

    Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation. (arXiv:2401.09031v1 [cs.LG])

    [http://arxiv.org/abs/2401.09031](http://arxiv.org/abs/2401.09031)

    本文研究了数据归因方法对扩散模型的影响，发现对于在引发大范数时间步骤上训练的样本，其损失梯度范数高度依赖于时间步骤，导致在影响估计中存在显著的偏差。为了解决这个问题，提出了Diffusion-ReTr方法。

    

    数据归因方法可以将模型行为追溯到其训练数据集，为理解“黑箱”神经网络提供了一种有效的方法。虽然先前的研究已经在各种情况下建立了模型输出与训练数据之间的可量化联系，但在与训练样本相关的扩散模型输出的解释方面仍然未被充分探索。特别是，扩散模型通过一系列时间步骤而不是之前的瞬时输入输出关系操作，对直接将现有框架扩展到扩散模型构成了重大挑战。值得注意的是，我们提出了Diffusion-TracIn，它包含了这种时间动力学，并观察到样本的损失梯度范数高度依赖于时间步骤。这种趋势导致影响估计中存在显著的偏差，对于在引发大范数时间步骤上训练的样本尤为明显，导致它们通常具有影响力。为了减轻这种影响，我们引入了Diffusion-ReTr方法。

    Data attribution methods trace model behavior back to its training dataset, offering an effective approach to better understand ``black-box'' neural networks. While prior research has established quantifiable links between model output and training data in diverse settings, interpreting diffusion model outputs in relation to training samples remains underexplored. In particular, diffusion models operate over a sequence of timesteps instead of instantaneous input-output relationships in previous contexts, posing a significant challenge to extend existing frameworks to diffusion models directly. Notably, we present Diffusion-TracIn that incorporates this temporal dynamics and observe that samples' loss gradient norms are highly dependent on timestep. This trend leads to a prominent bias in influence estimation, and is particularly noticeable for samples trained on large-norm-inducing timesteps, causing them to be generally influential. To mitigate this effect, we introduce Diffusion-ReTr
    
[^67]: 通过迭代组合问题来增强数学问题求解

    Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])

    [http://arxiv.org/abs/2401.09003](http://arxiv.org/abs/2401.09003)

    本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。

    

    尽管在改善大型语言模型(LLMs)的数学推理能力方面取得了一定进展，但在不使用外部工具的情况下解决竞赛级数学问题仍然对开源LLMs具有挑战性。在这项工作中，我们介绍了MMIQC数据集，这是一个混合处理的网络数据和合成问题-响应对的混合数据集，以提供基础模型更好的数学推理能力。通过在MMIQC上对Mistral-7B(arXiv:2310.06825)进行微调获得的模型Mistral-7B-MMIQC，在MATH(arXiv:2103.03874)上达到了36.0%的准确率，比之前(model size $\sim$7B)的最佳结果高出5.8%。我们的实验还表明，改进的一个重要部分归功于我们的新颖增强方法IQC(迭代组合问题)，其中我们迭代地要求LLM从给定的种子问题中组合新问题，并从另一个LLM中进行拒绝抽样。MMIQC现已在https://huggingface.co/datasets/Vivacem/MMIQC上发布。

    Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
    
[^68]: AntiPhishStack：基于LSTM的堆叠泛化模型用于优化网络钓鱼URL的检测

    AntiPhishStack: LSTM-based Stacked Generalization Model for Optimized Phishing URLs Detection. (arXiv:2401.08947v1 [cs.CR])

    [http://arxiv.org/abs/2401.08947](http://arxiv.org/abs/2401.08947)

    本文介绍了一种名为AntiPhishStack的LSTM-based堆叠泛化模型，用于优化网络钓鱼URL的检测。该模型通过对URL和字符级TF-IDF特征进行对称学习，提高了对新型网络钓鱼威胁的应对能力，并采用对抗性训练策略增加鲁棒性和对抗刚性网络钓鱼攻击。

    

    革命性的在线网络服务的不断依赖引入了更高的安全风险，尽管有广泛的安全措施，网络钓鱼仍然带来持续的挑战。传统的基于机器学习和手动特征的网络钓鱼系统在应对不断变化的策略上很困难。深度学习的最新进展为解决新型网络钓鱼和恶意URL挑战提供了有希望的途径。本文介绍了一种名为AntiPhishStack的两阶段堆叠泛化模型，旨在检测网络钓鱼网站。该模型对URL和字符级TF-IDF特征进行对称学习，增强了对新型网络钓鱼威胁的应对能力。第一阶段在基本的机器学习分类器上训练特征，采用K折交叉验证进行均值预测以提高鲁棒性。第二阶段采用两层堆叠LSTM网络，配合五个自适应优化器进行动态编译，确保在这些特征上获得出色的预测性能。此外，该模型使用对抗性训练策略来增加鲁棒性和对抗刚性网络钓鱼攻击。

    The escalating reliance on revolutionary online web services has introduced heightened security risks, with persistent challenges posed by phishing despite extensive security measures. Traditional phishing systems, reliant on machine learning and manual features, struggle with evolving tactics. Recent advances in deep learning offer promising avenues for tackling novel phishing challenges and malicious URLs. This paper introduces a two-phase stack generalized model named AntiPhishStack, designed to detect phishing sites. The model leverages the learning of URLs and character-level TF-IDF features symmetrically, enhancing its ability to combat emerging phishing threats. In Phase I, features are trained on a base machine learning classifier, employing K-fold cross-validation for robust mean prediction. Phase II employs a two-layered stacked-based LSTM network with five adaptive optimizers for dynamic compilation, ensuring premier prediction on these features. Additionally, the symmetrica
    
[^69]: Intrinsic Dataset Properties对泛化能力的影响：揭示自然图像和医学图像之间的学习差异

    The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images. (arXiv:2401.08865v1 [cs.CV])

    [http://arxiv.org/abs/2401.08865](http://arxiv.org/abs/2401.08865)

    本文研究了神经网络在自然图像和医学图像领域学习时的差异，提出了一个与训练集维度有关的泛化缩放定律，并认为医学图像数据集更高的固有“标签锐度”可能是两个领域之间显著差异的部分原因。

    

    本文研究了神经网络在不同图像领域学习时的差异，这在从自然图像到其他专门领域（如医学图像）采用计算机视觉技术时通常被忽视。最近的研究发现，训练集的固有维度($d_{data}$)与网络的泛化错误一般会增加。然而，医学（放射学）和自然图像领域之间的这种关系的陡峭程度存在显著差异，且无现有的理论解释。我们通过建立并经验证一个与$d_{data}$相关的泛化缩放定律来解决这个知识空白，并提出考虑到医学图像数据集更高的固有“标签锐度”($K_F$)这一度量指标可以部分解释这两个领域之间的显著缩放差异。接下来，我们展示了利用测量这一指标可以提供的额外好处。

    This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic "label sharpness" ($K_F$) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring th
    
[^70]: 基于机器学习的非人类灵长类动物中伊波拉病毒对基因表达影响的分析

    Machine Learning-Based Analysis of Ebola Virus' Impact on Gene Expression in Nonhuman Primates. (arXiv:2401.08738v1 [q-bio.GN])

    [http://arxiv.org/abs/2401.08738](http://arxiv.org/abs/2401.08738)

    本研究引入了一种基于机器学习的方法用于分析感染伊波拉病毒的非人类灵长类动物的基因表达数据，发现IFI6和IFI27等基因作为关键生物标志物能有效分类不同阶段的伊波拉感染。

    

    本研究引入了一种基于机器学习的方法，即监督型幅度-高度评分 (SMAS) 方法，用于分析感染伊波拉病毒 (EBOV) 的非人类灵长类动物 (NHPs) 的基因表达数据。我们利用了来自感染伊波拉病毒的 NHPs 的NanoString基因表达数据集，采用SMAS系统进行微妙的宿主-病原体相互作用分析。SMAS有效地结合了基于统计学显著性和表达变化的基因选择，采用线性分类器如逻辑回归以准确区分RT-qPCR阳性和阴性NHP样本。我们研究的一项重要发现是鉴定了IFI6和IFI27作为关键生物标志物，表现出100%的准确性和曲线下面积 (AUC) 指标在分类不同阶段的伊波拉感染中。除IFI6和IFI27外，基因如MX1，OAS1和ISG15显著上调，突出显示了...

    This study introduces the Supervised Magnitude-Altitude Scoring (SMAS) methodology, a machine learning-based approach, for analyzing gene expression data obtained from nonhuman primates (NHPs) infected with Ebola virus (EBOV). We utilize a comprehensive dataset of NanoString gene expression profiles from Ebola-infected NHPs, deploying the SMAS system for nuanced host-pathogen interaction analysis. SMAS effectively combines gene selection based on statistical significance and expression changes, employing linear classifiers such as logistic regression to accurately differentiate between RT-qPCR positive and negative NHP samples. A key finding of our research is the identification of IFI6 and IFI27 as critical biomarkers, demonstrating exceptional predictive performance with 100% accuracy and Area Under the Curve (AUC) metrics in classifying various stages of Ebola infection. Alongside IFI6 and IFI27, genes, including MX1, OAS1, and ISG15, were significantly upregulated, highlighting the
    
[^71]: 图像水印鲁棒性的基准测试

    Benchmarking the Robustness of Image Watermarks. (arXiv:2401.08573v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.08573](http://arxiv.org/abs/2401.08573)

    本研究提出了一种新颖的基准测试方法WAVES，用于评估图像水印的鲁棒性。通过整合检测和识别任务，并进行多样化压力测试，我们揭示了以前未被发现的水印脆弱性。

    

    本文研究了图像水印技术的弱点。我们提出了一种新颖的基准测试方法WAVES（通过增强的压力测试进行水印分析），用于评估水印的鲁棒性，克服了现有评估方法的局限性。WAVES整合了检测和识别任务，并建立了一个由多样化压力测试组成的标准化评估协议。WAVES中的攻击范围从传统的图像失真到扩散和对抗攻击的高级和新颖变体。我们的评估考察了两个关键维度：图像质量降低程度和攻击后水印检测的有效性。我们开发了一系列性能与质量2D图，变化基于几种突出的图像相似度度量，然后用一种启发式的新颖方法将它们聚合，从而为水印的鲁棒性和攻击能力提供一个全面的画面。我们的综合评估揭示了以前未被发现的脆弱性。

    This paper investigates the weaknesses of image watermarking techniques. We present WAVES (Watermark Analysis Via Enhanced Stress-testing), a novel benchmark for assessing watermark robustness, overcoming the limitations of current evaluation methods.WAVES integrates detection and identification tasks, and establishes a standardized evaluation protocol comprised of a diverse range of stress tests. The attacks in WAVES range from traditional image distortions to advanced and novel variations of diffusive, and adversarial attacks. Our evaluation examines two pivotal dimensions: the degree of image quality degradation and the efficacy of watermark detection after attacks. We develop a series of Performance vs. Quality 2D plots, varying over several prominent image similarity metrics, which are then aggregated in a heuristically novel manner to paint an overall picture of watermark robustness and attack potency. Our comprehensive evaluation reveals previously undetected vulnerabilities of 
    
[^72]: 通过学习受限对抗多面体提高对抗攻击的鲁棒性

    Robustness Against Adversarial Attacks via Learning Confined Adversarial Polytopes. (arXiv:2401.07991v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.07991](http://arxiv.org/abs/2401.07991)

    本文提出了一种通过学习受限对抗多面体来提高深度神经网络对抗攻击的鲁棒性的方法，并通过实验证明了该方法相对于现有对抗鲁棒性方法的有效性。

    

    深度神经网络(DNNs)可能会被生成的对干净样本的人类无法察觉的扰动欺骗。因此，提高DNNs对抗攻击的鲁棒性是一项关键任务。本文旨在通过限制添加到干净样本的范数有界扰动的输出集来训练鲁棒的DNNs。我们将这个集合称为对抗多面体，每个干净样本都有相应的对抗多面体。实际上，如果所有样本的相应多面体都是紧凑的，即它们不与DNN的决策边界相交，那么DNN对抗样本具有鲁棒性。因此，我们的算法的内部工作是基于学习受限的对抗多面体(CAP)。通过进行一系列的实验，我们证明了CAP在改善模型对抗最先进攻击(包括AutoAttack)的鲁棒性方面相对于现有的对抗鲁棒性方法的有效性。

    Deep neural networks (DNNs) could be deceived by generating human-imperceptible perturbations of clean samples. Therefore, enhancing the robustness of DNNs against adversarial attacks is a crucial task. In this paper, we aim to train robust DNNs by limiting the set of outputs reachable via a norm-bounded perturbation added to a clean sample. We refer to this set as adversarial polytope, and each clean sample has a respective adversarial polytope. Indeed, if the respective polytopes for all the samples are compact such that they do not intersect the decision boundaries of the DNN, then the DNN is robust against adversarial samples. Hence, the inner-working of our algorithm is based on learning \textbf{c}onfined \textbf{a}dversarial \textbf{p}olytopes (CAP). By conducting a thorough set of experiments, we demonstrate the effectiveness of CAP over existing adversarial robustness methods in improving the robustness of models against state-of-the-art attacks including AutoAttack.
    
[^73]: 学习可解释且性能更好的POMDP策略表示

    Learning Explainable and Better Performing Representations of POMDP Strategies. (arXiv:2401.07656v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2401.07656](http://arxiv.org/abs/2401.07656)

    本研究提出了一种学习部分可观测的马尔可夫决策过程（POMDP）策略自动机表示的方法。与传统的表格表示相比，该方法得到的自动机更小更易理解，且在学习过程中可改善策略性能。与其他方法相比，本方法在可扩展性上具有显著优势。

    

    部分可观测的马尔可夫决策过程（POMDP）的策略通常需要记忆。一种表示这种记忆的方法是使用自动机。我们提出了一种使用改进的L*算法学习策略的自动机表示的方法。与策略的表格表示相比，得到的自动机体积显著更小，因此更易于理解。此外，在学习过程中，我们的启发式方法甚至可以改善策略的性能。与直接从POMDP合成自动机以解决问题的方法相比，我们的方法具有不可比拟的可扩展性。

    Strategies for partially observable Markov decision processes (POMDP) typically require memory. One way to represent this memory is via automata. We present a method to learn an automaton representation of a strategy using a modification of the L*-algorithm. Compared to the tabular representation of a strategy, the resulting automaton is dramatically smaller and thus also more explainable. Moreover, in the learning process, our heuristics may even improve the strategy's performance. In contrast to approaches that synthesize an automaton directly from the POMDP thereby solving it, our approach is incomparably more scalable.
    
[^74]: PDE广义化的上下文操作符网络：对一维标量非线性守恒定律的研究

    PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar Nonlinear Conservation Laws. (arXiv:2401.07364v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.07364](http://arxiv.org/abs/2401.07364)

    本文以一维标量非线性守恒定律为例，详细介绍了使用上下文操作符网络（ICON）解决PDE问题的方法，并展示了ICON模型在没有微调的情况下可以很好地泛化到具有新形式的PDEs。

    

    我们能否构建一个针对各种PDE相关科学学习任务的单一大模型？这个模型能否在没有任何微调的情况下泛化到新的PDE，甚至是新形式的PDE？上下文操作符学习及其对应模型In-Context Operator Networks（ICON）代表了对这些问题的初步探索。之前已经证明了ICON对第一个问题的能力。在本文中，我们提出了一种用ICON解决PDE问题的详细方法，并展示了一个单一的ICON模型如何通过适当设计的数据提示来进行不同方程的正向和反向预测。我们展示了对第二个问题的积极证据，即ICON可以在没有任何微调的情况下很好地泛化到一些具有新形式的PDE。这通过对一维标量非线性守恒定律的研究加以说明，这是一类具有时间演化的PDE族群。我们还展示了如何扩展ICON模型能够解决的问题范围。

    Can we build a single large model for a wide range of PDE-related scientific learning tasks? Can this model generalize to new PDEs, even of new forms, without any fine-tuning? In-context operator learning and the corresponding model In-Context Operator Networks (ICON) represent an initial exploration of these questions. The capability of ICON regarding the first question has been demonstrated previously. In this paper, we present a detailed methodology for solving PDE problems with ICON, and show how a single ICON model can make forward and reverse predictions for different equations with different strides, provided with appropriately designed data prompts. We show the positive evidence to the second question, i.e., ICON can generalize well to some PDEs with new forms without any fine-tuning. This is exemplified through a study on 1D scalar nonlinear conservation laws, a family of PDEs with temporal evolution. We also show how to broaden the range of problems that an ICON model can add
    
[^75]: 超级-STTN：社交群体感知的时空转换网络用于人体轨迹预测与超图推理

    Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning. (arXiv:2401.06344v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2401.06344](http://arxiv.org/abs/2401.06344)

    本论文提出了Hyper-STTN，一种基于超图的时空转换网络，用于人群轨迹预测。通过构建多尺度超图来捕捉拥挤场景中的群体间相互作用，并利用空间-时间转换器来捕捉行人的成对潜在相互作用。这些异构的群体间和成对间相互作用通过一个多模态转换网络进行融合和对准。

    

    在各种现实世界的应用中，包括服务机器人和自动驾驶汽车，预测拥挤的意图和轨迹是至关重要的。理解环境动态是具有挑战性的，不仅因为对建模成对的空间和时间相互作用的复杂性，还因为群体间相互作用的多样性。为了解码拥挤场景中全面的成对和群体间相互作用，我们引入了Hyper-STTN，这是一种基于超图的时空转换网络，用于人群轨迹预测。在Hyper-STTN中，通过一组多尺度超图构建了拥挤的群体间相关性，这些超图具有不同的群体大小，通过基于随机游走概率的超图谱卷积进行捕捉。此外，还采用了空间-时间转换器来捕捉行人在空间-时间维度上的对照相互作用。然后，这些异构的群体间和成对间相互作用通过一个多模态转换网络进行融合和对准。

    Predicting crowded intents and trajectories is crucial in varouls real-world applications, including service robots and autonomous vehicles. Understanding environmental dynamics is challenging, not only due to the complexities of modeling pair-wise spatial and temporal interactions but also the diverse influence of group-wise interactions. To decode the comprehensive pair-wise and group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory prediction. In Hyper-STTN, crowded group-wise correlations are constructed using a set of multi-scale hypergraphs with varying group sizes, captured through random-walk robability-based hypergraph spectral convolution. Additionally, a spatial-temporal transformer is adapted to capture pedestrians' pair-wise latent interactions in spatial-temporal dimensions. These heterogeneous group-wise and pair-wise are then fused and aligned though a multimodal transformer net
    
[^76]: DFU: 零样本超分辨率图像生成的尺度鲁棒扩散模型

    DFU: scale-robust diffusion model for zero-shot super-resolution image generation. (arXiv:2401.06144v1 [cs.CV])

    [http://arxiv.org/abs/2401.06144](http://arxiv.org/abs/2401.06144)

    DFU是一种尺度鲁棒的扩散模型，可以实现零样本超分辨率图像生成，并通过在多个分辨率上训练来提高模型的可扩展性。

    

    扩散生成模型在生成固定分辨率的图像方面取得了显著的成功。然而，现有模型在没有相应分辨率的训练数据时，很难推广到不同的分辨率。借鉴操作符学习技术，我们提出了一种新的深度学习架构，Dual-FNO UNet (DFU)，通过在多个分辨率上同时组合空间和光谱信息来近似评分操作符。将DFU与基准模型进行比较，我们证明其可扩展性：1）在多个分辨率上同时训练可以改善FID，而单一固定分辨率的训练则不能实现；2）DFU可以推广到其训练分辨率之外，实现高分辨率的协调、高保真度的图像生成，即零样本超分辨率图像生成；3）我们提出了一种微调策略来进一步提高我们模型的零样本超分辨率图像生成能力，使FID为11.3。

    Diffusion generative models have achieved remarkable success in generating images with a fixed resolution. However, existing models have limited ability to generalize to different resolutions when training data at those resolutions are not available. Leveraging techniques from operator learning, we present a novel deep-learning architecture, Dual-FNO UNet (DFU), which approximates the score operator by combining both spatial and spectral information at multiple resolutions. Comparisons of DFU to baselines demonstrate its scalability: 1) simultaneously training on multiple resolutions improves FID over training at any single fixed resolution; 2) DFU generalizes beyond its training resolutions, allowing for coherent, high-fidelity generation at higher-resolutions with the same model, i.e. zero-shot super-resolution image-generation; 3) we propose a fine-tuning strategy to further enhance the zero-shot super-resolution image-generation capability of our model, leading to a FID of 11.3 at 
    
[^77]: DiarizationLM: 基于大语言模型的说话人分离后处理

    DiarizationLM: Speaker Diarization Post-Processing with Large Language Models. (arXiv:2401.03506v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2401.03506](http://arxiv.org/abs/2401.03506)

    本文介绍了DiarizationLM框架，利用大语言模型对说话人分离系统的输出进行后处理。实验证明，使用finetuned的PaLM 2-S模型可以显著减少分离错误率，对多种目标都有优化效果。

    

    本文介绍了DiarizationLM，一个利用大语言模型（LLM）对说话人分离系统输出进行后处理的框架。这个框架可以实现多种目标，如改善分离对话转录的可读性，或减少词级分离错误率（WDER）。在这个框架中，自动语音识别（ASR）和说话人分离系统的输出被表示为一种紧凑的文本格式，其包含在一个可选择调整的LLM的提示中。LLM的输出可以作为所需改进的精细化分离结果。作为后处理步骤，该框架可以轻松应用于任何现有的ASR和说话人分离系统，无需重新训练现有的组件。我们的实验证明，finetuned的PaLM 2-S模型可以在Fisher电话对话数据集上将WDER降低55.5%，在Callhome英语数据集上降低44.9%。

    In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. Various goals can be achieved with the proposed framework, such as improving the readability of the diarized transcript, or reducing the word diarization error rate (WDER). In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset.
    
[^78]: 在非稳定环境下的决策制定与策略增强搜索

    Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v1 [cs.AI])

    [http://arxiv.org/abs/2401.03197](http://arxiv.org/abs/2401.03197)

    在非稳定环境下的决策制定是一个具有挑战性的问题，本文介绍了一种新的算法--策略增强蒙特卡洛树搜索（PA-MCTS），它将在线搜索与策略学习相结合。

    

    在许多重要问题中，存在着不确定性下的连续决策制定。针对这类问题，传统的方法包括强化学习和在线搜索（如蒙特卡洛树搜索）。前者通过与环境的交互来学习策略（通常在执行之前完成），而后者在决策时使用环境的生成模型来采样有前景的行动轨迹。在非稳定环境下的决策制定尤为具有挑战性，因为代理操作的环境可能随时间变化。两种方法在这种情况下都存在缺陷--一方面，执行之前学习的策略在环境改变时变得陈旧，重新学习需要时间和计算资源。另一方面，在线搜索在允许的运行时间有限时可能会返回次优行动。本文介绍了一种新的算法--策略增强蒙特卡洛树搜索（PA-MCTS），它将在线搜索与策略学习相结合。

    Sequential decision-making under uncertainty is present in many important problems. Two popular approaches for tackling such problems are reinforcement learning and online search (e.g., Monte Carlo tree search). While the former learns a policy by interacting with the environment (typically done before execution), the latter uses a generative model of the environment to sample promising action trajectories at decision time. Decision-making is particularly challenging in non-stationary environments, where the environment in which an agent operates can change over time. Both approaches have shortcomings in such settings -- on the one hand, policies learned before execution become stale when the environment changes and relearning takes both time and computational effort. Online search, on the other hand, can return sub-optimal actions when there are limitations on allowed runtime. In this paper, we introduce \textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines actio
    
[^79]: 改进基于扩散的图像合成与上下文预测

    Improving Diffusion-Based Image Synthesis with Context Prediction. (arXiv:2401.02015v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2401.02015](http://arxiv.org/abs/2401.02015)

    本研究提出了一种名为ConPreDiff的方法，通过上下文预测来改善基于扩散的图像合成。在训练阶段，我们使用上下文解码器鼓励每个点预测其邻域上下文，并在推理阶段去除解码器。这种方法能够更好地重建图像。

    

    扩散模型是一种新的生成模型类别，极大提升了图像生成的质量和多样性。现有的扩散模型主要通过像素或特征约束在空间轴上对损坏图像进行重建。然而，这种点对点的重建可能无法完全保留每个预测像素/特征的邻域上下文，影响了基于扩散的图像合成。为了解决这个问题，我们首次提出了ConPreDiff，通过上下文预测改进基于扩散的图像合成。训练阶段，在扩散去噪块的末端增加了一个上下文解码器，明确地鼓励每个点预测其邻域上下文（即多步长特征/令牌/像素），并在推理阶段去除解码器。通过这种方式，每个点可以更好地重建自身。

    Diffusion models are a new class of generative models, and have dramatically promoted image generation with unprecedented quality and diversity. Existing diffusion models mainly try to reconstruct input image from a corrupted one with a pixel-wise or feature-wise constraint along spatial axes. However, such point-based reconstruction may fail to make each predicted pixel/feature fully preserve its neighborhood context, impairing diffusion-based image synthesis. As a powerful source of automatic supervisory signal, context has been well studied for learning representations. Inspired by this, we for the first time propose ConPreDiff to improve diffusion-based image synthesis with context prediction. We explicitly reinforce each point to predict its neighborhood context (i.e., multi-stride features/tokens/pixels) with a context decoder at the end of diffusion denoising blocks in training stage, and remove the decoder for inference. In this way, each point can better reconstruct itself by 
    
[^80]: 按照你的学习行动：非稳态马尔可夫决策过程中的自适应决策

    Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v1 [cs.AI])

    [http://arxiv.org/abs/2401.01841](http://arxiv.org/abs/2401.01841)

    本文提出了一种自适应蒙特卡洛树搜索算法来应对非稳态环境下的决策问题，解决了传统方法中对环境动态假设的限制和规划过程的悲观性问题。

    

    在顺序决策中，处理非稳态环境是一个基本（且在很大程度上是未解决的）挑战，其中外部环境条件随时间变化。这类问题通常被建模为非稳态马尔可夫决策过程（NSMDP）。然而，现有的NSMDP决策方法存在两个主要缺点：首先，它们假设当前时刻更新的环境动态是已知的（尽管未来动态可能会改变）；其次，规划过程主要是悲观的，即代理人会“安全行动”以考虑环境的非稳态演变。我们认为这两个假设在实践中是无效的-更新的环境条件很少是已知的，并且当代理人与环境交互时，它可以学习更新的动态并避免悲观，至少在其对动态有信心的状态下。我们提出了一种启发式搜索算法，称为自适应蒙特卡洛树搜索。

    A fundamental (and largely open) challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time. Such problems are traditionally modeled as non-stationary Markov decision processes (NSMDP). However, existing approaches for decision-making in NSMDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts ``safely'' to account for the non-stationary evolution of the environment. We argue that both these assumptions are invalid in practice -updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about. We present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree Se
    
[^81]: 具有Hessian辅助动量方差减小的自然策略梯度全局收敛

    Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction. (arXiv:2401.01084v1 [cs.LG])

    [http://arxiv.org/abs/2401.01084](http://arxiv.org/abs/2401.01084)

    本文开发了一种新的自然策略梯度变体NPG-HM，采用Hessian辅助动量技术进行方差减小，通过随机梯度下降解决子问题。实验证明NPG-HM在通用Fisher非退化策略参数化下可以实现全局最后迭代的$\epsilon$-最优性，并且在Mujoco环境中表现出卓越的性能。

    

    自然策略梯度（NPG）及其变体是强化学习中广泛使用的策略搜索方法。本文在之前的工作的基础上，开发了一种新的NPG变体，命名为NPG-HM，该方法利用了Hessian辅助动量技术进行方差减小，并通过随机梯度下降方法解决子问题。结果表明，NPG-HM可以在样本复杂度为$\mathcal{O}(\epsilon^{-2})$的情况下实现全局最后迭代的$\epsilon$-最优性，这是在通用的Fisher非退化策略参数化下自然策略梯度方法中已知的最佳结果。收敛分析建立在针对NPG的松弛弱梯度优势性质以及处理子问题时的错误分解的兼容函数逼近框架下。此外，基于Mujoco环境的数值实验表明NPG-HM相对于其他最先进的策略方法展现出卓越的性能。

    Natural policy gradient (NPG) and its variants are widely-used policy search methods in reinforcement learning. Inspired by prior work, a new NPG variant coined NPG-HM is developed in this paper, which utilizes the Hessian-aided momentum technique for variance reduction, while the sub-problem is solved via the stochastic gradient descent method. It is shown that NPG-HM can achieve the global last iterate $\epsilon$-optimality with a sample complexity of $\mathcal{O}(\epsilon^{-2})$, which is the best known result for natural policy gradient type methods under the generic Fisher non-degenerate policy parameterizations. The convergence analysis is built upon a relaxed weak gradient dominance property tailored for NPG under the compatible function approximation framework, as well as a neat way to decompose the error when handling the sub-problem. Moreover, numerical experiments on Mujoco-based environments demonstrate the superior performance of NPG-HM over other state-of-the-art policy g
    
[^82]: 使用对抗训练和知识蒸馏的可解释性导向叶片疾病分类

    Explainability-Driven Leaf Disease Classification using Adversarial Training and Knowledge Distillation. (arXiv:2401.00334v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.00334](http://arxiv.org/abs/2401.00334)

    本研究通过对抗训练来提高植物叶片疾病分类模型对对抗攻击的鲁棒性，并通过可解释性技术获得模型的决策过程，同时通过模型压缩技术提高计算效率。实验证明，鲁棒性可能以分类准确性为代价，而学生模型可以以较低的性能损失蒸馏复杂模型的知识，从而提高计算效率。

    

    本研究关注植物叶片疾病分类，并探索了对抗训练、模型可解释性和模型压缩三个关键方面。通过对抗训练增强模型对对抗攻击的鲁棒性，在威胁存在的情况下确保准确分类。借助可解释性技术，我们深入了解模型的决策过程，提高了信任和透明度。此外，我们还探索了模型压缩技术，以优化计算效率同时保持分类性能。通过实验证明，在基准数据集上，鲁棒性可能以分类准确性为代价，对常规测试的性能损失为3%-20%，对对抗攻击测试的性能提高为50%-70%。我们还证明，一个学生模型在性能稍有降低的情况下可以比复杂模型高15-25倍的计算效率，蒸馏了更复杂模型的知识。

    This work focuses on plant leaf disease classification and explores three crucial aspects: adversarial training, model explainability, and model compression. The models' robustness against adversarial attacks is enhanced through adversarial training, ensuring accurate classification even in the presence of threats. Leveraging explainability techniques, we gain insights into the model's decision-making process, improving trust and transparency. Additionally, we explore model compression techniques to optimize computational efficiency while maintaining classification performance. Through our experiments, we determine that on a benchmark dataset, the robustness can be the price of the classification accuracy with performance reductions of 3%-20% for regular tests and gains of 50%-70% for adversarial attack tests. We also demonstrate that a student model can be 15-25 times more computationally efficient for a slight performance reduction, distilling the knowledge of more complex models.
    
[^83]: 在双边市场中匹配用户和创作者的研究

    Matching of Users and Creators in Two-Sided Markets with Departures. (arXiv:2401.00313v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2401.00313](http://arxiv.org/abs/2401.00313)

    本论文提出了一个双边市场中匹配用户和创作者的模型，并展示了一个以用户为中心的贪心算法可能导致整体参与度下降的问题。

    

    现今许多在线平台，包括社交媒体网站，都是桥接内容创作者与用户的双边市场。现有的关于平台推荐算法的文献主要集中在用户偏好和决策上，并没有同时考虑到创作者的动机。我们提出了一个内容推荐模型，明确关注用户-内容匹配的动态过程，其新颖之处在于如果用户和创作者没有足够的参与感，他们都可能永久离开平台。在我们的模型中，每个参与者根据当前匹配的实用性决定是否在每个时间步参与：用户基于推荐内容与其偏好的一致性，而创作者则基于其受众规模。我们证明了一种以用户为中心的贪心算法，如果不考虑创作者的离开，可能导致整体参与度任意下降，相对于考虑到双方利益最大化整体参与度的算法。

    Many online platforms of today, including social media sites, are two-sided markets bridging content creators and users. Most of the existing literature on platform recommendation algorithms largely focuses on user preferences and decisions, and does not simultaneously address creator incentives. We propose a model of content recommendation that explicitly focuses on the dynamics of user-content matching, with the novel property that both users and creators may leave the platform permanently if they do not experience sufficient engagement. In our model, each player decides to participate at each time step based on utilities derived from the current match: users based on alignment of the recommended content with their preferences, and creators based on their audience size. We show that a user-centric greedy algorithm that does not consider creator departures can result in arbitrarily poor total engagement, relative to an algorithm that maximizes total engagement while accounting for two
    
[^84]: 通过多样化解析OOD广义化的关键组件

    Unraveling the Key Components of OOD Generalization via Diversification. (arXiv:2312.16313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16313](http://arxiv.org/abs/2312.16313)

    本文研究了通过多样化方法来解决OOD广义化问题，并确定了其关键组件。研究发现，多样化方法对无标签数据的分布非常敏感，且仅仅进行多样化是不足以实现OOD广义化的，学习算法的选择也很重要。

    

    监督学习数据集可能包含多个解释训练集同样良好的线索，即学习其中任何一个都会导致对训练数据的正确预测。然而，其中许多线索可能是虚假的，在分布偏移下失去了预测能力，并因此无法推广到超出分布的数据。最近开发的“多样化”方法通过找到依赖不同特征的多个不同的假设来解决这个问题。本文旨在研究这类方法并确定对其OOD广义化能力的贡献的关键组件。我们发现(1) 多样化方法对于用于多样化的无标签数据的分布非常敏感，当远离方法特定的最佳点时性能会显著下降。(2) 仅仅进行多样化是不足以实现OOD广义化的。所使用的学习算法的选择，例如

    Supervised learning datasets may contain multiple cues that explain the training set equally well, i.e., learning any of them would lead to the correct predictions on the training data. However, many of them can be spurious, i.e., lose their predictive power under a distribution shift and consequently fail to generalize to out-of-distribution (OOD) data. Recently developed "diversification" methods (Lee et al., 2023; Pagliardini et al., 2023) approach this problem by finding multiple diverse hypotheses that rely on different features. This paper aims to study this class of methods and identify the key components contributing to their OOD generalization abilities.  We show that (1) diversification methods are highly sensitive to the distribution of the unlabeled data used for diversification and can underperform significantly when away from a method-specific sweet spot. (2) Diversification alone is insufficient for OOD generalization. The choice of the used learning algorithm, e.g., the
    
[^85]: 任务驱动的因果特征提取：朝着可信的风险预测迈进

    Task-Driven Causal Feature Distillation: Towards Trustworthy Risk Prediction. (arXiv:2312.16113v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16113](http://arxiv.org/abs/2312.16113)

    该论文提出了一种任务驱动的因果特征提取模型（TDCFD），通过将原始特征值转化为因果特征归因来实现可信的风险预测。实验证实了该方法在精确度、召回率和可解释性方面的优势。

    

    由于人工智能在许多领域取得了巨大的成功，对其在可信和可解释的风险预测方面的潜力引起了极大的兴趣。然而，大多数模型缺乏因果推理，并且在类别不平衡的情况下难以应对，导致精确度和召回率较低。为了解决这个问题，我们提出了一种任务驱动的因果特征提取模型（TDCFD），将原始特征值转化为特定风险预测任务的因果特征归因。因果特征归因有助于描述该特征的值对风险预测结果的贡献程度。在因果特征提取之后，我们应用深度神经网络生成具有因果可解释性和高精确度/召回率的可信预测结果。我们在几个合成和真实数据集上评估了TDCFD方法的性能，结果表明其在精确度、召回率和可解释性方面优于现有方法。

    Since artificial intelligence has seen tremendous recent successes in many areas, it has sparked great interest in its potential for trustworthy and interpretable risk prediction. However, most models lack causal reasoning and struggle with class imbalance, leading to poor precision and recall. To address this, we propose a Task-Driven Causal Feature Distillation model (TDCFD) to transform original feature values into causal feature attributions for the specific risk prediction task. The causal feature attribution helps describe how much contribution the value of this feature can make to the risk prediction result. After the causal feature distillation, a deep neural network is applied to produce trustworthy prediction results with causal interpretability and high precision/recall. We evaluate the performance of our TDCFD method on several synthetic and real datasets, and the results demonstrate its superiority over the state-of-the-art methods regarding precision, recall, interpretabi
    
[^86]: 带有变点的神经随机微分方程：一种生成对抗方法

    Neural Stochastic Differential Equations with Change Points: A Generative Adversarial Approach. (arXiv:2312.13152v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.13152](http://arxiv.org/abs/2312.13152)

    本文提出了一种使用神经随机微分方程来建模时间序列的变点检测算法，通过生成对抗网络学习每个变点对应的神经随机微分方程的参数，并通过GAN判别器的输出检测变点。验证结果表明，该算法在合成和真实数据集上的性能优于传统方法。

    

    随机微分方程广泛用于建模真实世界的随机现象。现有的工作主要集中在时间序列由单个随机微分方程建模的情况，这对于具有分布偏移的时间序列建模可能是局限的。在这项工作中，我们提出了一种将时间序列建模为神经随机微分方程的变点检测算法。给定一个时间序列数据集，所提出的方法联合学习未知的变点和与每个变点对应的不同神经随机微分方程模型的参数。具体而言，神经随机微分方程在生成对抗网络（GAN）的框架下进行学习，并且变点是根据GAN判别器的输出在前向传递中被检测到。在所提出的算法的每一步中，变点和SDE模型参数以交替的方式进行更新。通过对合成和真实数据集的数值结果进行验证，以验证我们的算法与传统方法的性能比较。

    Stochastic differential equations (SDEs) have been widely used to model real world random phenomena. Existing works mainly focus on the case where the time series is modeled by a single SDE, which might be restrictive for modeling time series with distributional shift. In this work, we propose a change point detection algorithm for time series modeled as neural SDEs. Given a time series dataset, the proposed method jointly learns the unknown change points and the parameters of distinct neural SDE models corresponding to each change point. Specifically, the SDEs are learned under the framework of generative adversarial networks (GANs) and the change points are detected based on the output of the GAN discriminator in a forward pass. At each step of the proposed algorithm, the change points and the SDE model parameters are updated in an alternating fashion. Numerical results on both synthetic and real datasets are provided to validate the performance of our algorithm in comparison to clas
    
[^87]: 增加流形：将Mixup正则化与UMAP结合

    Augment on Manifold: Mixup Regularization with UMAP. (arXiv:2312.13141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.13141](http://arxiv.org/abs/2312.13141)

    本文提出了一种称为UMAP Mixup的Mixup正则化方案，通过利用统一流形逼近与投影技术，实现了“在流形上”的自动数据增强，提高了深度学习模型的泛化性能。

    

    数据增强技术在增强深度学习模型性能方面起着重要作用。尽管它们在计算机视觉任务中的优势已被证明，但在其他领域的应用仍然有限。本文提出了一种称为UMAP Mixup的Mixup正则化方案，旨在为深度学习预测模型提供“在流形上”的自动数据增强。所提出的方法利用一种称为统一流形逼近与投影的降维技术，确保Mixup操作产生的合成样本位于特征和标签的数据流形上。在各种回归任务中的评估表明，UMAP Mixup与其他Mixup变体相比具有竞争力甚至表现更好，显示出其作为增强深度学习模型泛化性能的有效工具的潜力。

    Data augmentation techniques play an important role in enhancing the performance of deep learning models. Despite their proven benefits in computer vision tasks, their application in the other domains remains limited. This paper proposes a Mixup regularization scheme, referred to as UMAP Mixup, designed for ``on-manifold" automated data augmentation for deep learning predictive models. The proposed approach ensures that the Mixup operations result in synthesized samples that lie on the data manifold of the features and labels by utilizing a dimensionality reduction technique known as uniform manifold approximation and projection. Evaluations across diverse regression tasks show that UMAP Mixup is competitive with or outperforms other Mixup variants, show promise for its potential as an effective tool for enhancing the generalization performance of deep learning models.
    
[^88]: LRS: 通过Lipschitz正则化替代模型增强对抗性可转移性

    LRS: Enhancing Adversarial Transferability through Lipschitz Regularized Surrogate. (arXiv:2312.13118v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.13118](http://arxiv.org/abs/2312.13118)

    本文提出了一种名为LRS的转移黑盒攻击方法，通过Lipschitz正则化替代模型，使得现有的基于转移的黑盒攻击运行更好，生成更可转移的对抗性样本。

    

    对抗性样本的可转移性对于基于转移的黑盒对抗攻击非常重要。之前生成可转移对抗性样本的工作集中在攻击给定的预训练替代模型上，而替代模型和对抗性可转移性之间的联系被忽视了。本文提出一种名为"Lipschitz正则化替代模型"（LRS）的转移黑盒攻击方法，该方法将替代模型转化为有利于对抗性可转移性的形式，使用这样转化后的替代模型，任何现有的基于转移的黑盒攻击都能够正常运行，且性能更好。具体来说，我们在替代模型的损失函数中引入Lipschitz正则化，以实现更平滑和可控的优化过程，生成更可转移的对抗性样本。此外，本文还阐明了替代模型内部属性与对抗性可转移性之间的关系。

    The transferability of adversarial examples is of central importance to transfer-based black-box adversarial attacks. Previous works for generating transferable adversarial examples focus on attacking \emph{given} pretrained surrogate models while the connections between surrogate models and adversarial trasferability have been overlooked. In this paper, we propose {\em Lipschitz Regularized Surrogate} (LRS) for transfer-based black-box attacks, a novel approach that transforms surrogate models towards favorable adversarial transferability. Using such transformed surrogate models, any existing transfer-based black-box attack can run without any change, yet achieving much better performance. Specifically, we impose Lipschitz regularization on the loss landscape of surrogate models to enable a smoother and more controlled optimization process for generating more transferable adversarial examples. In addition, this paper also sheds light on the connection between the inner properties of s
    
[^89]: 当模型遇见新常态：针对无监督时间序列异常检测的测试时适应

    When Model Meets New Normals: Test-time Adaptation for Unsupervised Time-series Anomaly Detection. (arXiv:2312.11976v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11976](http://arxiv.org/abs/2312.11976)

    本论文针对无监督时间序列异常检测中的新常态问题，提出了一种基于趋势估计和自监督学习的测试时适应策略，实验证明该策略能够提高模型性能，增加对分布变化的鲁棒性。

    

    时间序列异常检测处理通过从观察序列中学习正常性来检测异常时间步骤的问题。然而，正常性的概念随时间演变，导致出现了“新常态问题”，即由于训练和测试数据之间的分布变化，正常性的分布可能发生改变。本文重点研究了无监督时间序列异常检测研究中新常态问题的普遍存在。为解决这个问题，我们提出了一种简单而有效的测试时适应策略，该策略基于趋势估计和自监督学习方法，在推断过程中学习新的正常性。对真实世界基准的广泛实验表明，将所提策略纳入异常检测器中，与基准相比，能够持续改善模型的性能，使其对分布变化具有鲁棒性。

    Time-series anomaly detection deals with the problem of detecting anomalous timesteps by learning normality from the sequence of observations. However, the concept of normality evolves over time, leading to a "new normal problem", where the distribution of normality can be changed due to the distribution shifts between training and test data. This paper highlights the prevalence of the new normal problem in unsupervised time-series anomaly detection studies. To tackle this issue, we propose a simple yet effective test-time adaptation strategy based on trend estimation and a self-supervised approach to learning new normalities during inference. Extensive experiments on real-world benchmarks demonstrate that incorporating the proposed strategy into the anomaly detector consistently improves the model's performance compared to the baselines, leading to robustness to the distribution shifts.
    
[^90]: 可证明收敛的联邦三层学习

    Provably Convergent Federated Trilevel Learning. (arXiv:2312.11835v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11835](http://arxiv.org/abs/2312.11835)

    该论文提出了一种异步联邦三层优化方法来解决层次决策过程中的TLO问题，通过利用μ剖分构建超多面体近似并以异步方式求解，解决了现有TLO工作中的隐私泄露和收敛速度缺乏分析的问题。

    

    三层学习，也称为三层优化（TLO），被认为是层次决策过程的强大建模工具，并广泛应用于许多机器学习应用中，如鲁棒的神经网络架构搜索、超参数优化和领域自适应。解决TLO问题在于其嵌套的决策结构而面临巨大挑战。此外，现有的TLO工作面临以下关键挑战：1）它们都专注于非分布式设置，这可能导致隐私泄露；2）它们没有提供任何非渐进收敛分析，即刻画算法收敛速度的特征。为了解决上述挑战，本文提出了一种异步联邦三层优化方法来解决TLO问题。所提出的方法利用μ剖分构建TLO问题的超多面体近似，并以异步方式解决。我们证明了所提出的μ剖分是对TLO问题的一个有效近似。

    Trilevel learning, also called trilevel optimization (TLO), has been recognized as a powerful modelling tool for hierarchical decision process and widely applied in many machine learning applications, such as robust neural architecture search, hyperparameter optimization, and domain adaptation. Tackling TLO problems has presented a great challenge due to their nested decision-making structure. In addition, existing works on TLO face the following key challenges: 1) they all focus on the non-distributed setting, which may lead to privacy breach; 2) they do not offer any non-asymptotic convergence analysis which characterizes how fast an algorithm converges. To address the aforementioned challenges, this paper proposes an asynchronous federated trilevel optimization method to solve TLO problems. The proposed method utilizes $\mu$-cuts to construct a hyper-polyhedral approximation for the TLO problem and solve it in an asynchronous manner. We demonstrate that the proposed $\mu$-cuts are a
    
[^91]: Topic-VQ-VAE: 利用隐变量码本实现灵活的主题导向文档生成

    Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation. (arXiv:2312.11532v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.11532](http://arxiv.org/abs/2312.11532)

    本文介绍了一种利用隐变量码本实现灵活的主题导向文档生成的新方法，通过名为TVQ-VAE的生成式主题模型，可以有效捕捉主题上下文，并支持灵活形式的文档生成。

    

    本文介绍了一种利用Vector-Quantized Variational Auto-Encoder（VQ-VAE）中的隐变量码本进行主题建模的新方法，离散地封装了预训练嵌入（例如预训练语言模型）的丰富信息。根据对隐变量码本和嵌入的新解释，我们提出了一种新的生成式主题模型，称为Topic-VQ-VAE（TVQ-VAE），它可以反向生成与相应隐变量码本相关的原始文档。TVQ-VAE可以通过包括传统的词袋（BoW）分布和自回归图像生成在内的各种生成分布来可视化主题。我们在文档分析和图像生成上的实验结果表明，TVQ-VAE可以有效捕捉主题上下文，揭示数据集的潜在结构，并支持灵活形式的文档生成。所提出的TVQ-VAE的官方实现可在https://github.com/clo找到。

    This paper introduces a novel approach for topic modeling utilizing latent codebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely encapsulating the rich information of the pre-trained embeddings such as the pre-trained language model. From the novel interpretation of the latent codebooks and embeddings as conceptual bag-of-words, we propose a new generative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates the original documents related to the respective latent codebook. The TVQ-VAE can visualize the topics with various generative distributions including the traditional BoW distribution and the autoregressive image generation. Our experimental results on document analysis and image generation demonstrate that TVQ-VAE effectively captures the topic context which reveals the underlying structures of the dataset and supports flexible forms of document generation. Official implementation of the proposed TVQ-VAE is available at https://github.com/clo
    
[^92]: DeRDaVa: 机器学习中的删除鲁棒数据估值

    DeRDaVa: Deletion-Robust Data Valuation for Machine Learning. (arXiv:2312.11413v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11413](http://arxiv.org/abs/2312.11413)

    提出了DeRDaVa：一种适用于机器学习的删除鲁棒数据估值框架。通过在预测删除后保持模型性能的前提下对每个数据源的贡献进行估值，避免了昂贵的重新计算。推广到Risk-DeRDaVa以满足对最坏/最好情况感到风险厌恶/寻求的模型所有者的需求。

    

    数据估值涉及确定在预测中对数据源进行公平估价以补偿它们，或确定最有用或最无用的训练样本。随着个人数据所有权和数据保护法规的增加，模型所有者可能需要满足更多的数据删除请求。这引发了现有工作尚未解决的问题：删除后数据估值分数是否仍然公平？是否必须昂贵地重新计算分数？答案是否定的。为了避免重新计算，我们提出在预测删除后保持模型性能的前提下，使用我们的数据估值框架DeRDaVa来对每个数据源的贡献进行估值。DeRDaVa可以高效地近似，并给更有用或更不容易被删除的数据分配更高的值。我们进一步推广了DeRDaVa到Risk-DeRDaVa，以满足对最坏/最好情况感到风险厌恶/寻求的模型所有者的需求。

    Data valuation is concerned with determining a fair valuation of data from data sources to compensate them or to identify training examples that are the most or least useful for predictions. With the rising interest in personal data ownership and data protection regulations, model owners will likely have to fulfil more data deletion requests. This raises issues that have not been addressed by existing works: Are the data valuation scores still fair with deletions? Must the scores be expensively recomputed? The answer is no. To avoid recomputations, we propose using our data valuation framework DeRDaVa upfront for valuing each data source's contribution to preserving robust model performance after anticipated data deletions. DeRDaVa can be efficiently approximated and will assign higher values to data that are more useful or less likely to be deleted. We further generalize DeRDaVa to Risk-DeRDaVa to cater to risk-averse/seeking model owners who are concerned with the worst/best-cases mo
    
[^93]: 自监督分解表示学习用于鲁棒目标语音提取

    Self-Supervised Disentangled Representation Learning for Robust Target Speech Extraction. (arXiv:2312.10305v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2312.10305](http://arxiv.org/abs/2312.10305)

    该论文提出了一种自监督分解表示学习方法，通过逐步分离说话人身份信息和其他无关因素，解决了目标语音提取任务中存在的说话人混叠问题，并使用分解的说话人身份信息来指导语音提取网络。

    

    语音信号本质上是复杂的，因为它包含全局声学特征和局部语义信息。然而，在目标语音提取任务中，参考语音中与说话人身份无关的全局和局部语义信息可能导致在语音提取网络中出现说话人混叠问题。为了克服这个挑战，我们提出了一种自监督分解表示学习方法。我们的方法通过一个两阶段过程来解决这个问题，利用参考语音编码网络和全局信息分解网络逐渐分解说话人身份信息和其他不相关因素。我们专门使用分解的说话人身份信息来指导语音提取网络。此外，我们引入自适应调制Transformer来确保混合信号的声学表示不受说话人嵌入的影响。

    Speech signals are inherently complex as they encompass both global acoustic characteristics and local semantic information. However, in the task of target speech extraction, certain elements of global and local semantic information in the reference speech, which are irrelevant to speaker identity, can lead to speaker confusion within the speech extraction network. To overcome this challenge, we propose a self-supervised disentangled representation learning method. Our approach tackles this issue through a two-phase process, utilizing a reference speech encoding network and a global information disentanglement network to gradually disentangle the speaker identity information from other irrelevant factors. We exclusively employ the disentangled speaker identity information to guide the speech extraction network. Moreover, we introduce the adaptive modulation Transformer to ensure that the acoustic representation of the mixed signal remains undisturbed by the speaker embeddings. This com
    
[^94]: 追求最优统计水印技术

    Towards Optimal Statistical Watermarking. (arXiv:2312.07930v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.07930](http://arxiv.org/abs/2312.07930)

    追求最优统计水印技术。通过将统计水印技术视为假设检验问题并引入伪随机生成器，我们实现了输出令牌和拒绝区域的耦合，实现了第一类错误和第二类错误之间的非平凡权衡，同时提出了最统一最有力的水印和最小化第二类错误的解决方案。我们还提供了独立同分布令牌数量的上下界，突显了改进的潜力。此外，我们还探讨了鲁棒性水印问题。

    

    我们将统计水印技术作为一个假设检验问题进行研究，这是一个泛化了所有之前统计水印方法的通用框架。我们的关键是通过实践中的伪随机生成器实现输出令牌和拒绝区域的耦合，从而允许在第一类错误和第二类错误之间进行非平凡的权衡。我们在一般的假设检验环境下表征了最统一最有力的水印以及在模型无关的环境中最小化第二类错误。在输出是$n$个令牌的常见情况下，我们对需要保证小的第一类和第二类错误的独立同分布令牌数量建立了近乎匹配的上下界。与之前的工作中的$ h ^ {-2} $速率相比，我们相对于每个令牌的平均熵$h$的速率为$ \Theta(h ^ {-1} \log (1/h)) $，突显了改进的潜力。此外，我们提出了鲁棒性水印问题，其中用户都是...

    We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-off between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in the general hypothesis testing setting and the minimax Type II error in the model-agnostic setting. In the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate of $\Theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. Moreover, we formulate the robust watermarking problem where users are all
    
[^95]: 超越预期回报：评估强化学习算法时考虑政策可复制性

    Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating Reinforcement Learning Algorithms. (arXiv:2312.07178v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.07178](http://arxiv.org/abs/2312.07178)

    本文提出了一种超越预期回报的评估方法，在强化学习中考虑策略的可复制性。现有的评估方法仅使用预期回报，无法充分考虑分布的扩散，这限制了其在比较策略时的有效性。

    

    在强化学习中，许多应用通常在环境中存在噪声或随机性。除了对学习的影响之外，这些不确定性导致相同的策略在不同的试验中表现不同，即产生不同的回报。强化学习中常用的评估程序仅使用预期回报来总结结果分布，而不考虑分布的扩散。我们的工作将这种扩散定义为政策的可复制性：当多次试验时，政策获得类似性能的能力，在一些实际应用中这是至关重要的属性。我们指出，现有的仅使用预期回报的程序在两个方面存在局限性：第一，存在无数个回报分布，具有广泛的性能和可复制性的权衡，但是它们可以有相同的预期回报，限制了它在比较策略时的有效性；第二，预期回报度量没有留下足够的空间，以描述分布的其他关键性质。

    Many applications in Reinforcement Learning (RL) usually have noise or stochasticity present in the environment. Beyond their impact on learning, these uncertainties lead the exact same policy to perform differently, i.e. yield different return, from one roll-out to another. Common evaluation procedures in RL summarise the consequent return distributions using solely the expected return, which does not account for the spread of the distribution. Our work defines this spread as the policy reproducibility: the ability of a policy to obtain similar performance when rolled out many times, a crucial property in some real-world applications. We highlight that existing procedures that only use the expected return are limited on two fronts: first an infinite number of return distributions with a wide range of performance-reproducibility trade-offs can have the same expected return, limiting its effectiveness when used for comparing policies; second, the expected return metric does not leave an
    
[^96]: 最优化多分布学习

    Optimal Multi-Distribution Learning. (arXiv:2312.05134v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05134](http://arxiv.org/abs/2312.05134)

    本论文提出了一种最优化多分布学习的方法，通过自适应采样来实现数据高效的学习。针对Vapnik-Chervonenkis (VC)维数为d的假设类，算法可以生成一个ε-最优随机假设，并且样本复杂度与最佳下界保持一致。同时，该算法的思想和理论还被进一步扩展以适应Rademacher类。最终提出的算法是奥拉克尔高效的，仅访问假设类。

    

    多分布学习（MDL）旨在学习一个共享模型，使得在k个不同的数据分布下，最小化最坏情况风险，已成为适应健壮性、公平性、多组合作等需求的统一框架。实现数据高效的MDL需要在学习过程中进行自适应采样，也称为按需采样。然而，最优样本复杂度的上下界之间存在较大差距。针对Vapnik-Chervonenkis（VC）维数为d的假设类，我们提出了一种新颖的算法，可生成一个ε-最优随机假设，其样本复杂度接近于（d+k）/ε^2（在某些对数因子中），与已知的最佳下界匹配。我们的算法思想和理论被进一步扩展，以适应Rademacher类。提出的算法是奥拉克尔高效的，仅仅访问假设类

    Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we propose a novel algorithm that yields an $varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$ (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory have been further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely
    
[^97]: 对于核机器在预处理中的Nystrom逼近

    On the Nystrom Approximation for Preconditioning in Kernel Machines. (arXiv:2312.03311v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2312.03311](http://arxiv.org/abs/2312.03311)

    本文分析了核机器预处理中使用Nystrom逼近的权衡。研究表明，使用对数大小的样本能够让Nystrom逼近的预处理器几乎与梯度下降同样有效地加速。

    

    核方法是机器学习中一类流行的非线性预测模型。学习核模型的可扩展算法需要具有迭代性质，但由于糟糕的条件，收敛可能很慢。谱预处理是加快训练核模型迭代算法收敛速度的重要工具。然而，计算和存储谱预处理器可能代价高昂，会导致大量的计算和存储开销，限制了核方法在大型数据集问题上的应用。Nystrom逼近的谱预处理器通常更便宜和更容易计算和存储，并在实际应用中取得了成功。本文分析了使用这种逼近预处理器的权衡。具体来说，我们表明与数据集大小相关的对数样本数量能够让基于Nystrom逼近的预处理器几乎与梯度下降同样有效地加速。

    Kernel methods are a popular class of nonlinear predictive models in machine learning. Scalable algorithms for learning kernel models need to be iterative in nature, but convergence can be slow due to poor conditioning. Spectral preconditioning is an important tool to speed-up the convergence of such iterative algorithms for training kernel models. However computing and storing a spectral preconditioner can be expensive which can lead to large computational and storage overheads, precluding the application of kernel methods to problems with large datasets. A Nystrom approximation of the spectral preconditioner is often cheaper to compute and store, and has demonstrated success in practical applications. In this paper we analyze the trade-offs of using such an approximated preconditioner. Specifically, we show that a sample of logarithmic size (as a function of the size of the dataset) enables the Nystrom-based approximated preconditioner to accelerate gradient descent nearly as well as
    
[^98]: 无线多跳网络中基于图神经网络的拥塞感知分布式任务卸载

    Congestion-aware Distributed Task Offloading in Wireless Multi-hop Networks Using Graph Neural Networks. (arXiv:2312.02471v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2312.02471](http://arxiv.org/abs/2312.02471)

    本文提出了一种基于图神经网络的拥塞感知分布式任务卸载方案，通过改进分布式贪心框架，有效减少了无线多跳网络中的拥塞并提升了执行延迟。

    

    计算卸载已成为移动和智能设备边缘智能的基础组件。现有的卸载方案主要集中在移动设备和服务器上，忽视了多个移动设备引起的潜在网络拥塞问题，特别是在无线多跳网络中。为了填补这一空白，我们提出了一种低开销的、拥塞感知的分布式任务卸载方案，通过将基于图的机器学习与分布式贪心框架相结合。在模拟的无线多跳网络中，节点数为20-110个，并采用基于最短路径路由和基于竞争的链路调度的资源分配方案下，我们的方法证明在减少拥塞或不稳定队列的同时，提高了本地计算的执行延迟。

    Computational offloading has become an enabling component for edge intelligence in mobile and smart devices. Existing offloading schemes mainly focus on mobile devices and servers, while ignoring the potential network congestion caused by tasks from multiple mobile devices, especially in wireless multi-hop networks. To fill this gap, we propose a low-overhead, congestion-aware distributed task offloading scheme by augmenting a distributed greedy framework with graph-based machine learning. In simulated wireless multi-hop networks with 20-110 nodes and a resource allocation scheme based on shortest path routing and contention-based link scheduling, our approach is demonstrated to be effective in reducing congestion or unstable queues under the context-agnostic baseline, while improving the execution latency over local computing.
    
[^99]: ALEXR:一种用于凸有限和耦合组成随机优化的最优单循环算法

    ALEXR: An Optimal Single-Loop Algorithm for Convex Finite-Sum Coupled Compositional Stochastic Optimization. (arXiv:2312.02277v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2312.02277](http://arxiv.org/abs/2312.02277)

    本文提出了一种名为ALEXR的高效算法，用于解决凸有限和耦合组成随机优化问题。此算法在解决平滑和非平滑问题时具有优越的收敛速度，并且可应用于多个领域，包括组分布鲁棒优化、不平衡数据学习、强化学习和排序学习。

    

    本文重新审视了一类具有多个应用的凸有限和耦合组成随机优化（cFCCO）问题，包括组分布鲁棒优化（GDRO），不平衡数据学习，强化学习和排序学习。为了更好地解决这些问题，我们引入了一个高效的单循环原始-对偶块坐标近端算法，称为ALEXR。该算法利用块坐标随机镜像上升更新对偶变量和随机近端梯度下降更新原始变量。我们在平滑和非平滑函数条件下建立了ALEXR在凸和强凸情况下的收敛速度，这不仅改进了以前在平滑cFCCO问题上的最佳速度，还扩展了cFCCO的范围，用于解决更具挑战性的非平滑问题，如GDRO的对偶形式。最后，我们提供了较低的复杂性下界，以证明算法具有很强的效率。

    This paper revisits a class of convex Finite-Sum Coupled Compositional Stochastic Optimization (cFCCO) problems with many applications, including group distributionally robust optimization (GDRO), learning with imbalanced data, reinforcement learning, and learning to rank. To better solve these problems, we introduce an efficient single-loop primal-dual block-coordinate proximal algorithm, dubbed ALEXR. This algorithm leverages block-coordinate stochastic mirror ascent updates for the dual variable and stochastic proximal gradient descent updates for the primal variable. We establish the convergence rates of ALEXR in both convex and strongly convex cases under smoothness and non-smoothness conditions of involved functions, which not only improve the best rates in previous works on smooth cFCCO problems but also expand the realm of cFCCO for solving more challenging non-smooth problems such as the dual form of GDRO. Finally, we present lower complexity bounds to demonstrate that the con
    
[^100]: GPU相位折叠和深度学习方法用于探测系外行星凌变

    The GPU Phase Folding and Deep Learning Method for Detecting Exoplanet Transits. (arXiv:2312.02063v2 [astro-ph.EP] UPDATED)

    [http://arxiv.org/abs/2312.02063](http://arxiv.org/abs/2312.02063)

    本文提出了GPU相位折叠和深度学习方法GPFC，用于探测系外行星凌变。GPFC利用并行化的快速折叠算法放大低信噪比的凌日信号，实现高精度和高速搜索。与主要的BLS方法相比，GPFC的速度提高了三个数量级，准确率为97%。

    

    本文提出了一个新颖的GPU相位折叠和卷积神经网络（CNN）系统GPFC，用于利用凌日方法探测系外行星。我们设计了一个在GPU上并行化的快速折叠算法，用于放大低信噪比的凌日信号，从而实现高精度和高速搜索。一个在两百万个合成光变曲线上训练的CNN会在每个周期上报告一个表明行星信号可能性的分数。虽然GPFC方法在周期范围上适用广泛，但此研究专注于探测轨道周期小于一天的超短周期行星。GPFC在速度上比主要的盒式最小二乘（BLS）方法提高了三个数量级。我们的模拟结果表明，与BLS相比，GPFC在训练准确率为97%、在相同假阳性率下真正阳性率更高、在相同召回率下精确度更高。

    This paper presents GPFC, a novel Graphics Processing Unit (GPU) Phase Folding and Convolutional Neural Network (CNN) system to detect exoplanets using the transit method. We devise a fast folding algorithm parallelized on a GPU to amplify low signal-to-noise ratio transit signals, allowing a search at high precision and speed. A CNN trained on two million synthetic light curves reports a score indicating the likelihood of a planetary signal at each period. While the GPFC method has broad applicability across period ranges, this research specifically focuses on detecting ultra-short-period planets with orbital periods less than one day. GPFC improves on speed by three orders of magnitude over the predominant Box-fitting Least Squares (BLS) method. Our simulation results show GPFC achieves $97%$ training accuracy, higher true positive rate at the same false positive rate of detection, and higher precision at the same recall rate when compared to BLS. GPFC recovers $100\%$ of known ultra
    
[^101]: 通用后门攻击

    Universal Backdoor Attacks. (arXiv:2312.00157v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.00157](http://arxiv.org/abs/2312.00157)

    通过通用数据污染攻击，可以控制深度图像分类器对任何源类别到任何目标类别的错误分类，而只需增加少量的污染样本。

    

    通过数据污染，网络抓取的数据集容易受到后门攻击，在训练过程中可以用于篡改深度图像分类器。由于在大型数据集上进行训练是昂贵的，因此模型只需要训练一次，然后多次重复使用。与对抗性样本不同，后门攻击通常针对特定的类别，而不是模型学习到的任何类别。我们展示了这并不一定是真实的情况，并且存在更有效的通用数据污染攻击，允许通过增加少量的污染样本来控制从任何源类别到任何目标类别的错误分类。我们的想法是生成具有显著特征的触发器，使模型可以学习。我们制作的触发器利用了我们称之为跨类别污染的现象，即学习一个类别的触发器使得模型更容易学习其他类别的触发器。

    Web-scraped datasets are vulnerable to data poisoning, which can be used for backdooring deep image classifiers during training. Since training on large datasets is expensive, a model is trained once and re-used many times. Unlike adversarial examples, backdoor attacks often target specific classes rather than any class learned by the model. One might expect that targeting many classes through a naive composition of attacks vastly increases the number of poison samples. We show this is not necessarily true and more efficient, universal data poisoning attacks exist that allow controlling misclassifications from any source class into any target class with a small increase in poison samples. Our idea is to generate triggers with salient characteristics that the model can learn. The triggers we craft exploit a phenomenon we call inter-class poison transferability, where learning a trigger from one class makes the model more vulnerable to learning triggers for other classes. We demonstrate 
    
[^102]: SNNs中基于关键性的高效修剪方法，受到关键性大脑假设的启发

    Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis. (arXiv:2311.16141v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2311.16141](http://arxiv.org/abs/2311.16141)

    本研究受到神经科学中的关键大脑假设的启发，提出了一种基于神经元关键性的高效SNN修剪方法，以加强特征提取和加速修剪过程，并取得了比当前最先进方法更好的性能。

    

    由于其节能和无乘法特性，SNNs已经引起了相当大的关注。深度SNNs规模的不断增长给模型部署带来了挑战。网络修剪通过压缩网络规模来减少模型部署的硬件资源需求。然而，现有的SNN修剪方法由于修剪迭代增加了SNNs的训练难度，导致修剪成本高昂且性能损失严重。本文受到神经科学中的关键大脑假设的启发，提出了一种基于神经元关键性的用于SNN修剪的再生机制，以增强特征提取并加速修剪过程。首先，我们提出了一种SNN中用于关键性的低成本度量方式。然后，在修剪后对所修剪结构进行重新排序，并再生那些具有较高关键性的结构，以获取关键网络。我们的方法表现优于当前的最先进方法。

    Spiking Neural Networks (SNNs) have gained considerable attention due to the energy-efficient and multiplication-free characteristics. The continuous growth in scale of deep SNNs poses challenges for model deployment. Network pruning reduces hardware resource requirements of model deployment by compressing the network scale. However, existing SNN pruning methods cause high pruning costs and performance loss because the pruning iterations amplify the training difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in neuroscience, we propose a regeneration mechanism based on the neuron criticality for SNN pruning to enhance feature extraction and accelerate the pruning process. Firstly, we propose a low-cost metric for the criticality in SNNs. Then, we re-rank the pruned structures after pruning and regenerate those with higher criticality to obtain the critical network. Our method achieves higher performance than the current state-of-the-art (SOTA) method with up t
    
[^103]: 注释敏感性：训练数据收集方法影响模型性能

    Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2311.14212](http://arxiv.org/abs/2311.14212)

    该研究发现训练数据收集方法对注释本身和下游模型性能产生影响。在对仇恨言论和冒犯性语言进行注释收集的实验中，发现注释工具的设计选择会对模型的性能产生明显差异。

    

    当训练数据由人工注释者收集时，注释工具的设计、给予注释者的指示、注释者的特征以及他们之间的互动都可能对训练数据产生影响。这项研究证明了创建注释工具时的设计选择也会影响基于得到的注释训练的模型。我们引入了"注释敏感性"这个术语，用来指代注释数据收集方法对注释本身以及下游模型性能和预测的影响。我们在五种实验条件下对仇恨言论和冒犯性语言进行注释收集，随机将注释者分配到不同条件下。然后，在每个得到的五个数据集上对BERT模型进行微调，并在每个条件的保留部分上评估模型性能。我们发现在以下方面条件之间存在明显差异：1）仇恨言论/冒犯性语言注释的比例，2）模型性能。

    When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. We collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. We then fine-tune BERT models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. We find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model per
    
[^104]: 机器学习的原子团簇展开势用于快速和量子精确的Wurtzite AlN热模拟

    Machine-Learned Atomic Cluster Expansion Potentials for Fast and Quantum-Accurate Thermal Simulations of Wurtzite AlN. (arXiv:2311.11990v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2311.11990](http://arxiv.org/abs/2311.11990)

    本论文开发了一种机器学习的原子间势，用于快速而准确地模拟Wurtzite氮化铝的声子传输性质。采用原子团簇展开（ACE）框架，该势的预测能力在多个性质上得到了验证，并通过与密度泛函理论（DFT）计算和实验结果进行比较，展示了其描述非谐振声子相互作用的能力。作者还将该势应用于晶格动力学分析，揭示了正交应变对Wurtzite氮化铝的热导率和声子性质的影响。

    

    使用原子团簇展开（ACE）框架，我们开发了一种机器学习的原子间势，可快速而准确地模拟Wurtzite氮化铝的声子传输性质。 ACE势对密度泛函理论（DFT）的预测能力在w-AlN的一系列性质中得到了证明，包括基态晶格参数，比热容，热膨胀系数，体模量和谐振声子分散。通过将ACE预测值与DFT计算和实验进行比较，进一步验证了晶格热导率，展示了我们的ACE势在描述非谐振声子相互作用方面的整体能力。作为一个实际应用，我们使用该势进行晶格动力学分析，以揭示正交应变对w-AlN的热导率和声子性质的影响，并确定其为近结节区的重要调节因素。

    Using the atomic cluster expansion (ACE) framework, we develop a machine learning interatomic potential for fast and accurately modelling the phonon transport properties of wurtzite aluminum nitride. The predictive power of the ACE potential against density functional theory (DFT) is demonstrated across a broad range of properties of w-AlN, including ground-state lattice parameters, specific heat capacity, coefficients of thermal expansion, bulk modulus, and harmonic phonon dispersions. Validation of lattice thermal conductivity is further carried out by comparing the ACE-predicted values to the DFT calculations and experiments, exhibiting the overall capability of our ACE potential in sufficiently describing anharmonic phonon interactions. As a practical application, we perform a lattice dynamics analysis using the potential to unravel the effects of biaxial strains on thermal conductivity and phonon properties of w-AlN, which is identified as a significant tuning factor for near-junc
    
[^105]: 通过自对抗攻击和系统提示的破解GPT-4V

    Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts. (arXiv:2311.09127v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2311.09127](http://arxiv.org/abs/2311.09127)

    通过自对抗攻击和系统提示漏洞，我们发现了GPT-4V中存在的安全风险，并提出了一种名为SASP的新型攻击方法，以搜索潜在的破解提示。我们通过添加人工修改，成功率提高到98.7%。我们还评估了修改系统提示对解锁GPT-4V的影响。

    

    现有关于破解多模态大型语言模型（MLLMs）的工作主要集中在对模型输入的对抗性样本上，较少关注模型API的漏洞。为填补这一研究空白，我们进行了以下工作：1）我们发现了GPT-4V中的系统提示泄漏漏洞。通过精心设计的对话，我们成功提取了GPT-4V的内部系统提示。这一发现表明MLLMs存在潜在的可利用的安全风险；2）基于获取的系统提示，我们提出了一种称为SASP（通过系统提示的自对抗攻击）的新型MLLM破解攻击方法。通过将GPT-4作为红队工具来针对自身进行攻击，我们旨在利用窃取的系统提示搜索潜在的破解提示。此外，为了提高攻击成功率，我们还根据GPT-4的分析添加了人工修改，将攻击成功率进一步提高到98.7％；3）我们评估了修改系统提示对解锁GPT-4V的影响。

    Existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities, especially in model API. To fill the research gap, we carry out the following work: 1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully extract the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs; 2) Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7\%; 3) We evaluated the effect of modifying system prompts to d
    
[^106]: 关于分布鲁棒强化学习的基础

    On the Foundation of Distributionally Robust Reinforcement Learning. (arXiv:2311.09018v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.09018](http://arxiv.org/abs/2311.09018)

    该论文为分布鲁棒强化学习的理论基础做出了贡献，通过一个综合的建模框架，决策者在最坏情况下的分布转变下选择最优策略，并考虑了各种建模属性和对手引起的转变的灵活性。

    

    出于对在训练和部署之间环境变化时鲁棒策略的需求，我们为分布鲁棒强化学习的理论基础做出了贡献。通过一个以分布鲁棒马尔科夫决策过程（DRMDPs）为中心的综合建模框架，我们使决策者在一个由对手操纵的最坏情况分布转变下选择最优策略。通过统一和扩展现有的表述，我们严格构建了适用于决策者和对手的各种建模属性的DRMDPs，包括适应性粒度、探索历史依赖性、马尔科夫和马尔科夫时间齐次的决策者和对手动态。此外，我们深入研究了对手引起的转变的灵活性，研究了SA和S-矩形性。在这个DRMDP框架下，我们研究了实现鲁棒性所需的条件。

    Motivated by the need for a robust policy in the face of environment shifts between training and the deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embraces various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the e
    
[^107]: Convolve and Conquer: 使用Wiener滤波器进行数据比较

    Convolve and Conquer: Data Comparison with Wiener Filters. (arXiv:2311.06558v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.06558](http://arxiv.org/abs/2311.06558)

    本文介绍了一种基于Wiener滤波器理论的新方法，用于比较数据样本之间的(不)相似性。通过全局相关的卷积方式，我们的方法在多个机器学习应用中得到了验证，并展示了在数据压缩、医学图像填充、翻译分类和非参数生成模型等方面的优越性能。

    

    量化评估数据样本之间的差异和/或相似性定义和塑造了与学习数据分布相关的优化问题。目前的比较数据方法常常在捕捉这种分布方面存在局限性，或者缺乏优化所需的良好数学属性（如平滑性、可微性或凸性）。在本文中，我们引入了一种受Wiener滤波器理论启示的新方法，用于度量成对样本之间的(不)相似性。Wiener滤波器的卷积性质使我们能够全面地以全局相关的方式比较数据样本。我们在四个机器学习应用中验证了我们的方法：数据压缩、医学图像填充、翻译分类和非参数生成模型。我们的结果表明，与传统的均方误差相比，在重构图像方面具有更高的感知质量和数据保真度，并且对平移具有更好的鲁棒性。

    Quantitative evaluations of differences and/or similarities between data samples define and shape optimisation problems associated with learning data distributions. Current methods to compare data often suffer from limitations in capturing such distributions or lack desirable mathematical properties for optimisation (e.g. smoothness, differentiability, or convexity). In this paper, we introduce a new method to measure (dis)similarities between paired samples inspired by Wiener-filter theory. The convolutional nature of Wiener filters allows us to comprehensively compare data samples in a globally correlated way. We validate our approach in four machine learning applications: data compression, medical imaging imputation, translated classification, and non-parametric generative modelling. Our results demonstrate increased resolution in reconstructed images with better perceptual quality and higher data fidelity, as well as robustness against translations, compared to conventional mean-sq
    
[^108]: 使用基于Transformer的序列模型进行MIMO均衡的上下文学习

    In-Context Learning for MIMO Equalization Using Transformer-Based Sequence Models. (arXiv:2311.06101v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2311.06101](http://arxiv.org/abs/2311.06101)

    本研究利用上下文学习技术解决了多输入多输出（MIMO）均衡的逆问题，基于任务的上下文中的导频符号和未知的衰落信道以及信噪比（SNR）水平。这种方法展示了在实践中的潜力。

    

    最近的研究表明，大型预训练的序列模型（例如基于Transformer的架构）具有进行上下文学习（ICL）的能力。在ICL中，通过将输入和任务的上下文中的几个示例直接映射到输出变量，对新输入进行决策。无需显式更新模型参数即可调整决策以适应新任务。预训练是一种元学习形式，可以观察几个相关任务的示例。先前的研究已经展示了线性回归的ICL能力。在本研究中，我们利用ICL来解决基于导频符号上下文的多输入多输出（MIMO）均衡的逆问题。一个任务由未知的衰落信道和信噪比（SNR）水平定义，可能是已知的。为了突显该方法的实际潜力，我们允许接收到的信号存在量化。

    Large pre-trained sequence models, such as transformer-based architectures, have been recently shown to have the capacity to carry out in-context learning (ICL). In ICL, a decision on a new input is made via a direct mapping of the input and of a few examples from the given task, serving as the task's context, to the output variable. No explicit updates of the model parameters are needed to tailor the decision to a new task. Pre-training, which amounts to a form of meta-learning, is based on the observation of examples from several related tasks. Prior work has shown ICL capabilities for linear regression. In this study, we leverage ICL to address the inverse problem of multiple-input and multiple-output (MIMO) equalization based on a context given by pilot symbols. A task is defined by the unknown fading channel and by the signal-to-noise ratio (SNR) level, which may be known. To highlight the practical potential of the approach, we allow the presence of quantization of the received s
    
[^109]: 基于形状和漏斗效应的媒体组合建模的贝叶斯方法

    Bayesian Methods for Media Mix Modelling with shape and funnel effects. (arXiv:2311.05587v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.05587](http://arxiv.org/abs/2311.05587)

    本研究探索了在媒体组合建模中利用基于物理原理的方程的潜在用途，提出了将马克思-玻尔兹曼方程和米氏方程纳入分层贝叶斯模型的方法，用于分析广告背景下的消费者行为。

    

    在最近几年，生成式人工智能的重大进展突显了基于物理原理的模型在增强人工智能能力方面的重要作用。其中，基于扩散方程的模型极大地提高了图像质量。本研究旨在探索马克思-玻尔兹曼方程和米氏方程在营销组合建模（MMM）应用中的潜在用途。我们提出将这些方程纳入分层贝叶斯模型中，以分析广告背景下的消费者行为。这些方程组在准确描述社交互动和消费者广告互动等复杂系统的随机动力学方面表现出色。

    In recent years, significant progress in generative AI has highlighted the important role of physics-inspired models that utilize advanced mathematical concepts based on fundamental physics principles to enhance artificial intelligence capabilities. Among these models, those based on diffusion equations have greatly improved image quality. This study aims to explore the potential uses of Maxwell-Boltzmann equation, which forms the basis of the kinetic theory of gases, and the Michaelis-Menten model in Marketing Mix Modelling (MMM) applications. We propose incorporating these equations into Hierarchical Bayesian models to analyse consumer behaviour in the context of advertising. These equation sets excel in accurately describing the random dynamics in complex systems like social interactions and consumer-advertising interactions.
    
[^110]: 使用类似ResNet的神经网络架构近似Langevin Monte Carlo

    Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures. (arXiv:2311.03242v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.03242](http://arxiv.org/abs/2311.03242)

    本论文提出了一种使用类似ResNet的神经网络架构来近似Langevin Monte Carlo算法，通过将来自简单参考分布的样本映射到目标分布的样本中来进行采样，具有较好的逼近速度和表达性。

    

    我们通过构建一个神经网络，将来自简单参考分布（如标准正态分布）的样本映射到目标分布的样本中，从而从给定的目标分布中进行采样。为此，我们提出使用受Langevin Monte Carlo (LMC)算法启发的神经网络架构。基于LMC扰动结果，在Wasserstein-2距离上，我们展示了该架构对于平滑的对数凹目标分布的逼近速度。分析严重依赖于扰动LMC过程的中间度量的亚高斯性概念。特别地，我们根据不同扰动假设推导出了中间方差代理的增长界限。此外，我们提出了一种类似于深度残差神经网络的架构，并推导出了近似样本与目标分布映射的表达性结果。

    We sample from a given target distribution by constructing a neural network which maps samples from a simple reference, e.g. the standard normal distribution, to samples from the target. To that end, we propose using a neural network architecture inspired by the Langevin Monte Carlo (LMC) algorithm. Based on LMC perturbation results, we show approximation rates of the proposed architecture for smooth, log-concave target distributions measured in the Wasserstein-$2$ distance. The analysis heavily relies on the notion of sub-Gaussianity of the intermediate measures of the perturbed LMC process. In particular, we derive bounds on the growth of the intermediate variance proxies under different assumptions on the perturbations. Moreover, we propose an architecture similar to deep residual neural networks and derive expressivity results for approximating the sample to target distribution map.
    
[^111]: 图像和临床生物医学中的多模式机器学习：调查和前景

    Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.02332](http://arxiv.org/abs/2311.02332)

    这项调查研究了多模式机器学习在医学图像分析和临床决策支持系统中的影响，强调了在多模态表示、融合、翻译、对齐和联合学习方面的挑战和创新，以及如何解决数据偏差和“大数据”稀缺性等问题。

    

    医疗人工智能系统中的机器学习应用已经从传统和统计方法转向越来越多地应用深度学习模型。本调查研究了当前多模式机器学习领域的现状，重点关注其对医学图像分析和临床决策支持系统的深远影响。本文强调了在多模态表示、融合、翻译、对齐和联合学习方面面临的挑战和创新，并探讨了多模态模型对临床预测的转变潜力。同时，本文还对这些模型的实际应用提出了疑问，关注决策支持系统与医疗服务提供者之间的相互影响。尽管取得了进展，但在许多生物医学领域中，数据偏差和“大数据”的稀缺性等挑战仍然存在。最后，本文讨论了有效创新和合作努力以进一步解决这些问题的方法。

    Machine learning (ML) applications in medical artificial intelligence (AI) systems have shifted from traditional and statistical methods to increasing application of deep learning models. This survey navigates the current landscape of multimodal ML, focusing on its profound impact on medical image analysis and clinical decision support systems. Emphasizing challenges and innovations in addressing multimodal representation, fusion, translation, alignment, and co-learning, the paper explores the transformative potential of multimodal models for clinical predictions. It also questions practical implementation of such models, bringing attention to the dynamics between decision support systems and healthcare providers. Despite advancements, challenges such as data biases and the scarcity of "big data" in many biomedical domains persist. We conclude with a discussion on effective innovation and collaborative efforts to further the miss
    
[^112]: 从不真实数据中学习缺陷预测

    Learning Defect Prediction from Unrealistic Data. (arXiv:2311.00931v1 [cs.LG])

    [http://arxiv.org/abs/2311.00931](http://arxiv.org/abs/2311.00931)

    该论文研究了从不真实的数据集中学习缺陷预测的问题，并提出了一种基于学习表示的方法来识别与真实数据集最相似的子集。

    

    预训练的代码模型，如CodeBERT和CodeT5，成为代码理解和生成任务的流行选择。这些模型往往庞大且需要相应数量的训练数据，但在下游任务中很少提供。相反，使用远比真实数据集更大但不真实的数据集（如人为注入缺陷的函数）来训练模型已经变得流行。然而，使用此类数据训练的模型往往只在类似数据上表现良好，在真实世界程序上表现不佳。本文假设这种差异是由于存在干扰样本，这些样本使模型偏离了真实世界任务分布。为了验证这个假设，我们提出了一种基于学习表示的方法，用于识别这些大而不真实的数据集中与真实数据集中的示例最相似的子集。我们的方法提取了真实世界和人工程序的高维嵌入表示。

    Pretrained models of code, such as CodeBERT and CodeT5, have become popular choices for code understanding and generation tasks. Such models tend to be large and require commensurate volumes of training data, which are rarely available for downstream tasks. Instead, it has become popular to train models with far larger but less realistic datasets, such as functions with artificially injected bugs. Models trained on such data, however, tend to only perform well on similar data, while underperforming on real world programs. In this paper, we conjecture that this discrepancy stems from the presence of distracting samples that steer the model away from the real-world task distribution. To investigate this conjecture, we propose an approach for identifying the subsets of these large yet unrealistic datasets that are most similar to examples in real-world datasets based on their learned representations. Our approach extracts high-dimensional embeddings of both real-world and artificial progr
    
[^113]: 具有加性和乘性噪声的线性随机微分方程的发生器识别

    Generator Identification for Linear SDEs with Additive and Multiplicative Noise. (arXiv:2310.19491v1 [math.ST])

    [http://arxiv.org/abs/2310.19491](http://arxiv.org/abs/2310.19491)

    本文介绍了从具有给定初始状态的解过程的分布中识别线性随机微分方程（SDE）的发生器的条件，并且提供了对于具有加性和乘性噪声的SDE的识别条件。

    

    本文提出了一种从具有给定固定初始状态的解过程的分布中识别线性随机微分方程（SDE）的发生器的条件。这些可识别性条件在使用线性SDE进行因果推断时至关重要，因为它们使得可以从其观测分布中识别出干预后的分布。我们具体推导出了识别具有加性噪声的线性SDE的发生器的充分必要条件，以及识别具有乘性噪声的线性SDE的发生器的充分条件。我们证明了对于两种类型的SDE，得到的条件是一般性的。此外，我们提供了对得到的可识别性条件的几何解释，以增强对其的理解。为了验证我们的理论结果，我们进行了一系列的模拟实验，这些实验支持并证实了我们所得到的结果。

    In this paper, we present conditions for identifying the generator of a linear stochastic differential equation (SDE) from the distribution of its solution process with a given fixed initial state. These identifiability conditions are crucial in causal inference using linear SDEs as they enable the identification of the post-intervention distributions from its observational distribution. Specifically, we derive a sufficient and necessary condition for identifying the generator of linear SDEs with additive noise, as well as a sufficient condition for identifying the generator of linear SDEs with multiplicative noise. We show that the conditions derived for both types of SDEs are generic. Moreover, we offer geometric interpretations of the derived identifiability conditions to enhance their understanding. To validate our theoretical results, we perform a series of simulations, which support and substantiate the established findings.
    
[^114]: 基于层次集成的时间序列预测特征选择研究

    Hierarchical Ensemble-Based Feature Selection for Time Series Forecasting. (arXiv:2310.17544v1 [cs.LG])

    [http://arxiv.org/abs/2310.17544](http://arxiv.org/abs/2310.17544)

    这项研究提出了一种基于层次集成的特征选择方法，能够克服传统方法和最先进方法在非平稳和特征数目庞大且样本有限的情况下的局限性，并在合成和实际数据集上展示了更好的性能。

    

    我们研究了一种针对非平稳和样本有限的大量特征情况下的特征选择新的集成方法。我们的方法利用层次结构来利用特征之间的相互依赖关系。首先，使用特征子集训练机器学习模型，然后使用另一种算法更新模型的输出，以最小化目标损失。这种层次结构允许灵活的深度和特征选择。通过层次地利用特征之间的依赖关系，我们的方法克服了传统特征选择方法和特征重要性评分的局限性。该方法在合成和现实数据集上展示了其有效性，与传统方法和最先进的方法相比，性能具有可扩展性和稳定性。

    We study a novel ensemble approach for feature selection based on hierarchical stacking in cases of non-stationarity and limited number of samples with large number of features. Our approach exploits the co-dependency between features using a hierarchical structure. Initially, a machine learning model is trained using a subset of features, and then the model's output is updated using another algorithm with the remaining features to minimize the target loss. This hierarchical structure allows for flexible depth and feature selection. By exploiting feature co-dependency hierarchically, our proposed approach overcomes the limitations of traditional feature selection methods and feature importance scores. The effectiveness of the approach is demonstrated on synthetic and real-life datasets, indicating improved performance with scalability and stability compared to the traditional methods and state-of-the-art approaches.
    
[^115]: 学习处理具有一般库存到货动态的库存控制策略

    Learning an Inventory Control Policy with General Inventory Arrival Dynamics. (arXiv:2310.17168v1 [cs.LG])

    [http://arxiv.org/abs/2310.17168](http://arxiv.org/abs/2310.17168)

    本文解决了学习具有一般库存到货动态下的库存控制策略的问题，同时允许修改订购数量以满足供应商的限制，并将周期性审核库存控制问题定义为外部决策过程。

    

    本文在面对一般到货动态的情况下解决了学习和回测库存控制策略的问题，我们将其称为数量随时间到货模型（QOT）。在实际供应链中，我们还允许修改订购数量以满足供应商的限制，例如订购最低数量和批次大小约束。据我们所知，这是第一篇处理任意到货动态或任意后续处理的订购数量的研究。在最近的工作（Madeka等，2022）的基础上，我们同样将周期性审核库存控制问题定义为外部决策过程，其中大部分状态不受代理的控制。Madeka等人（2022）展示了如何构建一个模拟器来回放历史数据以解决这类问题。在我们的例子中，我们将一个深度生成模型纳入到货过程的历史回放中。

    In this paper we address the problem of learning and backtesting inventory control policies in the presence of general arrival dynamics -- which we term as a quantity-over-time arrivals model (QOT). We also allow for order quantities to be modified as a post-processing step to meet vendor constraints such as order minimum and batch size constraints -- a common practice in real supply chains. To the best of our knowledge this is the first work to handle either arbitrary arrival dynamics or an arbitrary downstream post-processing of order quantities. Building upon recent work (Madeka et al., 2022) we similarly formulate the periodic review inventory control problem as an exogenous decision process, where most of the state is outside the control of the agent. Madeka et al. (2022) show how to construct a simulator that replays historic data to solve this class of problem. In our case, we incorporate a deep generative model for the arrivals process as part of the history replay. By formulat
    
[^116]: 基于场景级监督的点云分割的2D-3D交错Transformer模型

    2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision. (arXiv:2310.12817v1 [cs.CV])

    [http://arxiv.org/abs/2310.12817](http://arxiv.org/abs/2310.12817)

    本文提出了一种基于场景级监督的2D-3D交错Transformer模型，用于弱监督点云分割。该模型通过两个编码器计算2D和3D数据的自注意特征，并通过交替切换查询和键值对的角色，实现了2D和3D特征的融合。

    

    本文提出了一种多模态交错Transformer模型（MIT），用于考虑2D和3D数据进行弱监督点云分割。研究表明，2D和3D特征在点云分割中互补。然而，现有方法需要额外的2D注释来实现2D-3D信息融合。鉴于点云的高注释成本，基于弱监督学习的有效2D和3D特征融合需求非常迫切。为此，我们提出了一个具有两个编码器和一个解码器的Transformer模型，仅使用场景级类标签进行弱监督点云分割。具体而言，两个编码器分别计算3D点云和2D多视图图像的自注意特征。解码器实现交错的2D-3D交叉注意力，并进行隐式2D和3D特征融合。我们在解码器层中交替切换查询和键值对的角色。实验证明，2D和3D特征是互补的。

    We present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud segmentation. However, existing methods require extra 2D annotations to achieve 2D-3D information fusion. Considering the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly supervised point cloud segmentation using only scene-level class tags. Specifically, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queries and key-value pairs in the decoder layers. It turns out that the 2D and 3D features are 
    
[^117]: 物理引导的噪声神经代理用于低光原始图像去噪

    Physics-guided Noise Neural Proxy for Low-light Raw Image Denoising. (arXiv:2310.09126v1 [eess.IV])

    [http://arxiv.org/abs/2310.09126](http://arxiv.org/abs/2310.09126)

    本文提出了一种新的物理引导噪声神经代理（PNNP）用于准确噪声建模和低光原始图像去噪，集成了物理引导噪声解耦、物理引导代理模型和可微分分布导向损失等高效技术。

    

    低光原始图像去噪在移动摄影中起着至关重要的作用，学习方法已成为主流方法。使用合成数据训练学习方法成为替代对应真实数据的高效实用方法。然而，合成数据的质量受噪声模型精度的限制，降低了低光原始图像去噪的性能。本文提出了一个新颖的准确噪声建模框架，学习一个从暗场中获得的物理引导噪声神经代理（PNNP）。PNNP集成了三种高效技术：物理引导噪声解耦（PND），物理引导代理模型（PPM）和可微分分布导向损失（DDL）。PND将暗场解耦为不同的组分，并以灵活的方式处理不同水平的噪声，降低了噪声神经代理的复杂度。PPM通过引入物理先验有效地约束生成的噪声。

    Low-light raw image denoising plays a crucial role in mobile photography, and learning-based methods have become the mainstream approach. Training the learning-based methods with synthetic data emerges as an efficient and practical alternative to paired real data. However, the quality of synthetic data is inherently limited by the low accuracy of the noise model, which decreases the performance of low-light raw image denoising. In this paper, we develop a novel framework for accurate noise modeling that learns a physics-guided noise neural proxy (PNNP) from dark frames. PNNP integrates three efficient techniques: physics-guided noise decoupling (PND), physics-guided proxy model (PPM), and differentiable distribution-oriented loss (DDL). The PND decouples the dark frame into different components and handles different levels of noise in a flexible manner, which reduces the complexity of the noise neural proxy. The PPM incorporates physical priors to effectively constrain the generated no
    
[^118]: 学习具有已知骨架的有界度多树

    Learning bounded-degree polytrees with known skeleton. (arXiv:2310.06333v1 [cs.LG])

    [http://arxiv.org/abs/2310.06333](http://arxiv.org/abs/2310.06333)

    本论文提出了一种高效学习已知骨架的有界度多树的算法，并给出了在多项式时间和样本复杂度内的有限样本保证。这对于复杂概率分布的学习具有重要意义。

    

    我们为高维概率分布的一类丰富的多树（polytrees）——有界度多树，建立了高效适当学习的有限样本保证。有界度多树是贝叶斯网络的子类，贝叶斯网络是一种广泛研究的图模型类型。最近，Bhattacharyya等人（2021）通过提供一种高效算法，在已知无向图（骨架）的情况下，为1-多树恢复了有限样本保证。我们通过扩展他们的结果，提供了一种高效算法，可以在多项式时间和样本复杂度内学习任何有界度的$d$-多树。我们将算法与信息论样本复杂度的下界结合起来，表明对维度和目标精度参数的依赖几乎是紧致的。

    We establish finite-sample guarantees for efficient proper learning of bounded-degree polytrees, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We extend their results by providing an efficient algorithm which learns $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight.
    
[^119]: 一种用于非层次化多保真度自适应采样的潜变量方法

    A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling. (arXiv:2310.03298v1 [stat.ML])

    [http://arxiv.org/abs/2310.03298](http://arxiv.org/abs/2310.03298)

    提出了一种基于潜变量的方法，用于非层次化多保真度自适应采样。该方法能够利用不同保真度模型之间的相关性以更高效地探索和利用设计空间。

    

    多保真度（MF）方法在提高替代模型和设计优化方面越来越受欢迎，通过整合来自不同低保真度（LF）模型的数据。尽管大多数现有的MF方法假定了一个固定的数据集，但是动态分配资源在不同保真度模型之间可以实现更高的探索和利用设计空间的效率。然而，大多数现有的MF方法依赖于保真度级别的层次假设，或者无法捕捉多个保真度级别之间的相互关系并利用其来量化未来样本的价值和导航自适应采样。为了解决这个障碍，我们提出了一个基于不同保真度模型的潜变量嵌入和相关的先验-后验分析的框架，以显式地利用它们的相关性进行自适应采样。在这个框架中，每个填充采样迭代包括两个步骤：首先我们确定具有最大潜力影响的位置。

    Multi-fidelity (MF) methods are gaining popularity for enhancing surrogate modeling and design optimization by incorporating data from various low-fidelity (LF) models. While most existing MF methods assume a fixed dataset, adaptive sampling methods that dynamically allocate resources among fidelity models can achieve higher efficiency in the exploring and exploiting the design space. However, most existing MF methods rely on the hierarchical assumption of fidelity levels or fail to capture the intercorrelation between multiple fidelity levels and utilize it to quantify the value of the future samples and navigate the adaptive sampling. To address this hurdle, we propose a framework hinged on a latent embedding for different fidelity models and the associated pre-posterior analysis to explicitly utilize their correlation for adaptive sampling. In this framework, each infill sampling iteration includes two steps: We first identify the location of interest with the greatest potential imp
    
[^120]: MathVista: 用GPT-4V、Bard和其他大型多模态模型评估视觉场景中的数学推理能力

    MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.02255](http://arxiv.org/abs/2310.02255)

    本论文提出了MathVista，这是一个评估视觉场景中数学推理能力的基准测试。通过对12个著名的基础模型进行全面的定量评估，发现最好的GPT-4V模型相对于第二名的Bard模型在准确率上提升了15.1%。

    

    大型语言模型（LLMs）和大型多模态模型（LMMs）在许多任务和领域中展示出令人印象深刻的问题解决能力，但它们在视觉环境中的数学推理能力尚未得到系统研究。为了弥补这一差距，我们提出了MathVista，这是一个综合了不同数学和视觉任务的挑战的基准测试。它包含了6141个例子，其中有28个现有的多模态数据集和3个新创建的数据集（即IQTest、FunctionQA和PaperQA）。完成这些任务需要精细的、深入的视觉理解和组合推理，这些都是当前最先进的基础模型所面临的困难。通过MathVista，我们对12个著名的基础模型进行了全面的定量评估。表现最好的GPT-4V模型的整体准确率为49.9%，明显优于第二名的Bard模型，相差15.1%。我们的深入分析揭示了

    Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of
    
[^121]: 通过大型语言模型生成机器人模拟任务的GenSim

    GenSim: Generating Robotic Simulation Tasks via Large Language Models. (arXiv:2310.01361v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.01361](http://arxiv.org/abs/2310.01361)

    GenSim通过利用大型语言模型自动生成丰富的模拟环境和专家示范，解决了目前模拟数据中缺乏任务级别多样性的问题，提高了机器人策略在任务级别上的泛化能力。

    

    由于收集大量真实交互数据来训练通用机器人策略往往代价高昂，因此在数据生成方面通常采用模拟数据。然而，现有的数据生成方法通常专注于场景级别的多样性（例如对象实例和姿势），而忽视了任务级别的多样性，这是因为需要人工努力提出和验证新的任务。这导致在模拟数据上训练的策略很难展示显著的任务级别泛化能力。本文提出了一种通过利用大型语言模型（LLM）的基于场景和编码能力自动生成丰富的模拟环境和专家示范的方法，称为GenSim。我们的方法有两种模式：目标导向生成，其中将目标任务提供给LLM，LLM提出解决目标任务的任务课程；探索性生成，其中LLM从先前的任务中启动，并迭代地提出新的任务。

    Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scene-level diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models' (LLM) grounding and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks th
    
[^122]: 利用优化技术进行对图像水印的自适应攻击

    Leveraging Optimization for Adaptive Attacks on Image Watermarks. (arXiv:2309.16952v1 [cs.CR])

    [http://arxiv.org/abs/2309.16952](http://arxiv.org/abs/2309.16952)

    该论文提出了一种利用优化技术进行对图像水印的自适应攻击的方法，通过自适应地生成替代密钥来复制秘密水印密钥，并通过优化问题的解决方法进行攻击优化。

    

    不可靠的用户可以滥用图像生成器来合成高质量的深度伪造品并参与在线的垃圾信息或虚假宣传活动。水印技术通过在生成的内容中标记隐藏信息来防止滥用，并使用秘密水印密钥进行检测。水印技术的核心安全属性是鲁棒性，即攻击者只能通过大幅降低图像质量来逃避检测。评估鲁棒性需要为特定的水印算法设计自适应攻击。评估水印算法及其（自适应）攻击时的一个挑战是确定自适应攻击是否是最优的，即它是最佳的攻击方法。我们通过定义目标函数并将自适应攻击视为优化问题来解决这个问题。我们自适应攻击的核心思想是通过创建可微分的替代密钥来本地复制秘密水印密钥，以优化攻击过程。

    Untrustworthy users can misuse image generators to synthesize high-quality deepfakes and engage in online spam or disinformation campaigns. Watermarking deters misuse by marking generated content with a hidden message, enabling its detection using a secret watermarking key. A core security property of watermarking is robustness, which states that an attacker can only evade detection by substantially degrading image quality. Assessing robustness requires designing an adaptive attack for the specific watermarking algorithm. A challenge when evaluating watermarking algorithms and their (adaptive) attacks is to determine whether an adaptive attack is optimal, i.e., it is the best possible attack. We solve this problem by defining an objective function and then approach adaptive attacks as an optimization problem. The core idea of our adaptive attacks is to replicate secret watermarking keys locally by creating surrogate keys that are differentiable and can be used to optimize the attack's 
    
[^123]: 基于原始数据的体内纳米尺度定位的分析建模

    Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale Localization. (arXiv:2309.16034v1 [cs.ET])

    [http://arxiv.org/abs/2309.16034](http://arxiv.org/abs/2309.16034)

    本论文研究了基于原始数据的体内纳米尺度定位的分析建模，分析了纳米设备的通信和能源约束对定位性能的影响。

    

    纳米技术和材料科学的进展为纳米尺度设备的发展铺平了道路，这些设备结合了传感、计算、数据和能源储存以及无线通信。在精密医学中，这些纳米设备对于疾病诊断、治疗和监测呈现出巨大的潜力，而体内纳米尺度定位的流引导定位，即将所感知的生物事件与事件本身的位置关联起来，从精密医学的角度看将具有极大的益处。纳米设备的纳米尺度特性以及血液流动环境的挑战性导致目前的流引导定位方法在通信和能源相关能力方面受到限制。纳米设备的通信和能源约束导致流引导定位的原始数据具有不同的特征，从而影响其性能。本研究通过分析建模研究了这些不完美的影响。

    Advancements in nanotechnology and material science are paving the way toward nanoscale devices that combine sensing, computing, data and energy storage, and wireless communication. In precision medicine, these nanodevices show promise for disease diagnostics, treatment, and monitoring from within the patients' bloodstreams. Assigning the location of a sensed biological event with the event itself, which is the main proposition of flow-guided in-body nanoscale localization, would be immensely beneficial from the perspective of precision medicine. The nanoscale nature of the nanodevices and the challenging environment that the bloodstream represents, result in current flow-guided localization approaches being constrained in their communication and energy-related capabilities. The communication and energy constraints of the nanodevices result in different features of raw data for flow-guided localization, in turn affecting its performance. An analytical modeling of the effects of imperfe
    
[^124]: DTC: 深度跟踪控制--一种统一的基于模型的规划和强化学习方法，用于多功能和鲁棒的运动

    DTC: Deep Tracking Control -- A Unifying Approach to Model-Based Planning and Reinforcement-Learning for Versatile and Robust Locomotion. (arXiv:2309.15462v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2309.15462](http://arxiv.org/abs/2309.15462)

    本文提出了一种混合控制架构，同时结合了基于模型的规划和基于强化学习的方法，用于解决腿式运动的复杂控制问题。这种方法具有精确性、鲁棒性以及对稀疏奖励环境的适应能力。

    

    腿式运动是一个复杂的控制问题，需要精确性和鲁棒性来应对现实世界的挑战。传统上，腿式系统使用逆动力学的轨迹优化控制。这种层次化的基于模型的方法因其直观的成本函数调整、准确的规划以及超过十年的广泛研究所带来的深刻理解而具有吸引力。然而，模型不匹配和假设的违反是故障操作的常见原因，并可能妨碍成功的模拟到真实环境的转换。另一方面，基于仿真的强化学习产生了具有前所未有的鲁棒性和恢复能力的运动策略。然而，所有学习算法在稀疏奖励的环境中都面临着困难，这种环境中合法的落脚点很少，比如缺口或跳石。在这项工作中，我们提出了一种混合控制架构，结合了两个世界的优点，同时实现目标。

    Legged locomotion is a complex control problem that requires both accuracy and robustness to cope with real-world challenges. Legged systems have traditionally been controlled using trajectory optimization with inverse dynamics. Such hierarchical model-based methods are appealing due to intuitive cost function tuning, accurate planning, and most importantly, the insightful understanding gained from more than one decade of extensive research. However, model mismatch and violation of assumptions are common sources of faulty operation and may hinder successful sim-to-real transfer. Simulation-based reinforcement learning, on the other hand, results in locomotion policies with unprecedented robustness and recovery skills. Yet, all learning algorithms struggle with sparse rewards emerging from environments where valid footholds are rare, such as gaps or stepping stones. In this work, we propose a hybrid control architecture that combines the advantages of both worlds to simultaneously achie
    
[^125]: 决策树策略在IBMDP中的Actor-Critic算法的局限性（arXiv:2309.13365v2 [cs.LG] UPDATED）

    Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs. (arXiv:2309.13365v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13365](http://arxiv.org/abs/2309.13365)

    该论文研究了在IBMDP中使用Actor-Critic算法学习决策树策略的局限性。结果表明，即使是在简单的玩具任务上，深度RL也可能失败。

    

    AI模型的可解释性可以通过用户安全检查来建立对这些AI的信任。特别是，决策树（DT）提供了对学习模型的整体视角，并透明地揭示了哪些输入特征对于做出决策至关重要。然而，如果决策树过大，可解释性就会受到影响。为了学习紧凑的决策树，最近提出了一种强化学习（RL）框架，用于使用深度RL探索DT的空间。该框架通过增加动作来收集关于隐藏输入特征的信息，通过适当地对这些动作进行惩罚，代理学习如何在树的大小和性能之间进行最优权衡。在实践中，仍然存在一个开放问题，即需要学习部分可观察马尔可夫决策过程（MDP）的反应性策略。本文表明，即使在这一类简单的玩具任务上，深度RL也可能失败。

    Interpretability of AI models allows for user safety checks to build trust in such AIs. In particular, Decision Trees (DTs) provide a global look at the learned model and transparently reveal which features of the input are critical for making a decision. However, interpretability is hindered if the DT is too large. To learn compact trees, a recent Reinforcement Learning (RL) framework has been proposed to explore the space of DTs using deep RL. This framework augments a decision problem (e.g. a supervised classification task) with additional actions that gather information about the features of an otherwise hidden input. By appropriately penalizing these actions, the agent learns to optimally trade-off size and performance of DTs. In practice, a reactive policy for a partially observable Markov decision process (MDP) needs to be learned, which is still an open problem. We show in this paper that deep RL can fail even on simple toy tasks of this class. However, when the underlying deci
    
[^126]: 使用动态规划发现决策树的可解释性-性能帕累托前沿

    Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming. (arXiv:2309.12701v1 [cs.LG])

    [http://arxiv.org/abs/2309.12701](http://arxiv.org/abs/2309.12701)

    本文提出了一种使用动态规划找到最优决策树的方法，可以得到多个可解释性-性能权衡的最优决策树，使用户可以根据自己的需求选择最适合的树。

    

    众所周知，决策树由于可以被人类检查和解释而具有固有的可解释性。此外，最近硬件的进步重新引起了对最优决策树算法的关注，这些算法比通常的贪婪方法产生更准确的树。然而，这些最优算法返回的是一个优化手动定义的可解释性-性能权衡的单个树，通过指定最大决策节点数量来获得，对于这个权衡的质量没有进一步的洞察。在本文中，我们提出了一种新的马尔可夫决策问题（MDP）形式来找到最优决策树。这种形式的主要优点是，我们可以通过解决一个单一的动态规划问题计算出多个可解释性-性能权衡的最优决策树，让用户事后选择最适合他们需求的树。在实证方面，我们证明我们的方法在准确性和运行时间方面与最先进的算法竞争力相当。

    Decision trees are known to be intrinsically interpretable as they can be inspected and interpreted by humans. Furthermore, recent hardware advances have rekindled an interest for optimal decision tree algorithms, that produce more accurate trees than the usual greedy approaches. However, these optimal algorithms return a single tree optimizing a hand defined interpretability-performance trade-off, obtained by specifying a maximum number of decision nodes, giving no further insights about the quality of this trade-off. In this paper, we propose a new Markov Decision Problem (MDP) formulation for finding optimal decision trees. The main interest of this formulation is that we can compute the optimal decision trees for several interpretability-performance trade-offs by solving a single dynamic program, letting the user choose a posteriori the tree that best suits their needs. Empirically, we show that our method is competitive with state-of-the-art algorithms in terms of accuracy and run
    
[^127]: 具有神经图模型的联邦学习

    Federated Learning with Neural Graphical Models. (arXiv:2309.11680v1 [cs.LG])

    [http://arxiv.org/abs/2309.11680](http://arxiv.org/abs/2309.11680)

    本研究提出了一种名为FedNGMs的联邦学习框架，利用概率神经图模型来处理多个客户端的数据，并在保持训练数据私密性的同时提升模型准确性。

    

    联邦学习（FL）解决了在多个客户端保留对数据的独占控制的同时，基于专有数据创建模型的需求。近期提出的神经图模型（NGMs）是概率图模型，利用神经网络的表达能力学习输入特征之间的复杂非线性依赖关系。它们学会捕捉底层的数据分布，并具有高效的推理和采样算法。我们开发了一个FL框架，它维护一个全局的NGM模型，从本地NGM模型中学习到平均信息，同时保持训练数据在客户端的环境中。我们的设计FedNGMs避免了神经元匹配框架（如联邦匹配平均）中模型参数爆炸的缺点和不足。我们的全局模型大小在整个过程中保持不变。

    Federated Learning (FL) addresses the need to create models based on proprietary data in such a way that multiple clients retain exclusive control over their data, while all benefit from improved model accuracy due to pooled resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic Graphical models that utilize the expressive power of neural networks to learn complex non-linear dependencies between the input features. They learn to capture the underlying data distribution and have efficient algorithms for inference and sampling. We develop a FL framework which maintains a global NGM model that learns the averaged information from the local NGM models while keeping the training data within the client's environment. Our design, FedNGMs, avoids the pitfalls and shortcomings of neuron matching frameworks like Federated Matched Averaging that suffers from model parameter explosion. Our global model size remains constant throughout the process. In the cases where clients 
    
[^128]: 关于随机梯度下降的不同模式

    On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v1 [cs.LG])

    [http://arxiv.org/abs/2309.10688](http://arxiv.org/abs/2309.10688)

    这项研究解决了对于随机梯度下降（SGD）中不同模式的追踪和理解的问题，提供了一个相位图来区分噪声主导的SGD和大步骤主导的SGD。

    

    现代深度网络通过随机梯度下降（SGD）进行训练，其关键参数是每个步骤考虑的数据量或批量大小B以及步长或学习率η。对于小的B和大的η，SGD对应于参数的随机演化，其噪声幅度由“温度”T=η/B控制。然而当批量大小B≥B^*足够大时，这种描述被观察到失效，或者在温度足够小时简化为梯度下降（GD）。理解这些交叉发生的位置仍然是一个中心挑战。在这里，我们解决了这些问题，在一个教师-学生感知器分类模型中，我们展示了我们的关键预测仍适用于深度网络。具体来说，我们在B-η平面上获得了一个相位图，将三个动态阶段分开：（i）受温度控制的噪声主导的SGD，（ii）大步骤主导的SGD和

    Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this description is observed to break down for sufficiently large batches $B\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a noise-dominated SGD governed by temperature, $\textit{(ii)}$ a large-first-step-dominated SGD and
    
[^129]: DePT:分解提示调整以实现参数高效微调

    DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05173](http://arxiv.org/abs/2309.05173)

    DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。

    

    提示调整（PT）是一种将可训练的少量软提示向量附加到语言模型（LM）输入中的参数高效微调（PEFT）方法，已在各种任务和模型中显示出了有希望的结果。 与其他PEFT方法相比，PT的竞争性能可以在可训练参数更少的情况下保持，并且随着模型规模的扩大，其参数并不会显著增加。 但是，PT引入了额外的软提示标记，导致输入序列变长，这对于Transformer的二次复杂度而言，在训练和推理时间以及内存使用方面会产生显著影响。 这对于面临大量每日查询的大型语言模型（LLMs）尤其令人担忧。

    Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
    
[^130]: 对多样化投资组合进行强化学习技术的评估

    Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio. (arXiv:2309.03202v1 [q-fin.TR])

    [http://arxiv.org/abs/2309.03202](http://arxiv.org/abs/2309.03202)

    本研究评估了在S&P 500指数上使用强化学习技术进行多样化投资组合的可行性。研究发现，包含COVID-19时期的市场数据在训练数据集中可以提供更好的性能，并且基于策略的方法（VI和SARSA）在测试中表现优于Q学习。

    

    本研究旨在回答关于在S&P 500指数上使用强化学习的可行性的关键研究问题。采用了基于策略的价值迭代（VI）和状态-动作-奖励-状态-动作（SARSA）的技术，以及基于策略外的Q学习。该模型在包含2000-2023年多年股市数据的数据集上进行训练和测试。分析展示了在两个不同时间段上训练和测试模型的结果和发现：一个包括COVID-19大流行期间的年份，一个不包括。结果表明，在训练数据集中包含COVID-19时期的市场数据比基准策略表现更好。在测试中，基于策略的方法（VI和SARSA）优于Q学习，凸显了简单策略的偏差-方差权衡和泛化能力的影响。然而，需要注意的是，Q学习的性能可能因条件的变化而有所不同。

    This work seeks to answer key research questions regarding the viability of reinforcement learning over the S&P 500 index. The on-policy techniques of Value Iteration (VI) and State-action-reward-state-action (SARSA) are implemented along with the off-policy technique of Q-Learning. The models are trained and tested on a dataset comprising multiple years of stock market data from 2000-2023. The analysis presents the results and findings from training and testing the models using two different time periods: one including the COVID-19 pandemic years and one excluding them. The results indicate that including market data from the COVID-19 period in the training dataset leads to superior performance compared to the baseline strategies. During testing, the on-policy approaches (VI and SARSA) outperform Q-learning, highlighting the influence of bias-variance tradeoff and the generalization capabilities of simpler policies. However, it is noted that the performance of Q-learning may vary depe
    
[^131]: 基于机器学习的多重共线性解决方案：四川省碳排放案例研究

    Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions in Sichuan Province. (arXiv:2309.01115v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01115](http://arxiv.org/abs/2309.01115)

    本研究使用机器学习方法解决了多重共线性问题，针对四川省的碳排放情况进行了案例研究。研究结果确定了行业分组，评估了排放驱动因素，并提出了科学的减排策略，以改善情况。

    

    本研究使用矩阵归一化对四川省46个关键产业2000-2019年的能源消耗数据进行预处理。DBSCAN聚类识别了16个特征类别以客观地分组行业。接下来，采用罚函数回归模型，以应对过拟合控制、高维数据处理和特征选择等复杂能源数据处理的优势。结果表明，煤炭周围的第二个聚类因生产需求而产生的排放量最高。以汽油和焦炭为中心的聚类的排放量也很显著。基于此，减排建议包括清洁煤技术、交通管理、钢铁中的煤电替代和行业标准化。该研究引入了无监督学习的方法来客观选择因素，并旨在探索新的减排途径。总而言之，本研究确定了行业分组，评估了排放驱动因素，并提出了科学的减排策略以进一步改善情况。

    This study preprocessed 2000-2019 energy consumption data for 46 key Sichuan industries using matrix normalization. DBSCAN clustering identified 16 feature classes to objectively group industries. Penalized regression models were then applied for their advantages in overfitting control, high-dimensional data processing, and feature selection - well-suited for the complex energy data. Results showed the second cluster around coal had highest emissions due to production needs. Emissions from gasoline-focused and coke-focused clusters were also significant. Based on this, emission reduction suggestions included clean coal technologies, transportation management, coal-electricity replacement in steel, and industry standardization. The research introduced unsupervised learning to objectively select factors and aimed to explore new emission reduction avenues. In summary, the study identified industry groupings, assessed emissions drivers, and proposed scientific reduction strategies to bette
    
[^132]: 学习的视觉特征到文本解释

    Learned Visual Features to Textual Explanations. (arXiv:2309.00733v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.00733](http://arxiv.org/abs/2309.00733)

    本研究提出了一种名为TExplain的方法，将大型语言模型与预训练图像分类器的特征空间连接起来，通过生成解释性句子来理解分类器学习到的特征。该方法首次利用这些频繁单词揭示出分类器的决策过程，实现了检测虚假特征的能力。

    

    在机器学习领域，解释视觉模型学习到的特征一直是一个长期存在的挑战。为了解决这个问题，我们提出了一种新颖的方法，利用大型语言模型（LLMs）的能力来解释预训练图像分类器学习到的特征。我们的方法名为TExplain，通过训练一个神经网络在图像分类器的特征空间和LLMs之间建立连接来解决这个任务。然后，在推断过程中，我们的方法生成大量的句子来解释分类器对给定图像学习到的特征。这些句子然后用于提取最频繁的单词，从而全面理解分类器中学习到的特征和模式。我们的方法首次利用与视觉表示对应的这些频繁单词来揭示独立训练的分类器的决策过程，从而实现检测虚假特征的能力。

    Interpreting the learned features of vision models has posed a longstanding challenge in the field of machine learning. To address this issue, we propose a novel method that leverages the capabilities of large language models (LLMs) to interpret the learned features of pre-trained image classifiers. Our method, called TExplain, tackles this task by training a neural network to establish a connection between the feature space of image classifiers and LLMs. Then, during inference, our approach generates a vast number of sentences to explain the features learned by the classifier for a given image. These sentences are then used to extract the most frequent words, providing a comprehensive understanding of the learned features and patterns within the classifier. Our method, for the first time, utilizes these frequent words corresponding to a visual representation to provide insights into the decision-making process of the independently trained classifier, enabling the detection of spurious
    
[^133]: 在移动设备上进行十亿规模语言模型的联邦微调

    Federated Fine-tuning of Billion-Sized Language Models across Mobile Devices. (arXiv:2308.13894v1 [cs.AI])

    [http://arxiv.org/abs/2308.13894](http://arxiv.org/abs/2308.13894)

    这项工作引入了一种创新的FL协议FwdLLM，旨在提高在移动设备上进行十亿规模语言模型的联邦微调（FedLLM）的效率。FwdLLM通过使用无反向传播（BP）训练方法以及“扰动推断”来提高内存效率和时间效率。

    

    大规模语言模型（LLM）正在改变移动智能的格局。联邦学习（FL）是一种保护用户数据隐私的方法，通常用于对下游移动任务进行LLM的微调，这被称为FedLLM。尽管最近的研究已经解决了由庞大模型大小引起的网络问题，但它们在与移动设备的整合方面并没有实际缓解诸多挑战，比如显著的内存消耗和缓慢的模型收敛。为了应对这些挑战，本研究引入了一种创新的FL协议FwdLLM，旨在提高FedLLM的效率。FwdLLM的关键思想是采用无反向传播（BP）训练方法，只需要设备执行“扰动推断”。因此，FwdLLM具有更好的内存效率和时间效率（通过移动NPUs和扩大的参与设备数组）。FwdLLM围绕三个关键设计展开：（1）将无反向传播训练与p

    Large Language Models (LLMs) are transforming the landscape of mobile intelligence. Federated Learning (FL), a method to preserve user data privacy, is often employed in fine-tuning LLMs to downstream mobile tasks, an approach known as FedLLM. Though recent efforts have addressed the network issue induced by the vast model size, they have not practically mitigated vital challenges concerning integration with mobile devices, such as significant memory consumption and sluggish model convergence.  In response to these challenges, this work introduces FwdLLM, an innovative FL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLM to employ backpropagation (BP)-free training methods, requiring devices only to execute ``perturbed inferences''. Consequently, FwdLLM delivers way better memory efficiency and time efficiency (expedited by mobile NPUs and an expanded array of participant devices). FwdLLM centers around three key designs: (1) it combines BP-free training with p
    
[^134]: 提问澄清问题是否增加了生成代码的信心？关于大型语言模型的沟通能力的研究

    Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models. (arXiv:2308.13507v1 [cs.SE])

    [http://arxiv.org/abs/2308.13507](http://arxiv.org/abs/2308.13507)

    通过在生成代码之前提问澄清问题，大型语言模型的代码生成能力可以得到提升，增加了对生成代码的信心。

    

    大型语言模型(LLMs)显著提高了代码生成任务的能力。然而，LLMs在成为顶级软件工程师方面仍存在差距。基于观察到顶级软件工程师通常会提出澄清问题以减少需求和编码解决方案的不确定性，我们认为在代码生成任务中，LLMs也应该采用同样的方法。通过在生成最终代码之前提出深入的问题，可以减轻使用LLMs进行编程所面临的挑战，如意图规范不明确、计算思维不足和代码质量不理想。这反过来增加了对生成代码的自信。在这项工作中，我们探讨如何利用更好的沟通技巧来增加对生成代码的信心。我们提出了一个以沟通为中心的过程，利用LLM生成的沟通器来识别高度不确定或信心低的问题。

    Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, we explore how to leverage better communication skills to achieve greater confidence in generated code. We propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low conf
    
[^135]: Animal3D:一份全面的3D动物姿态和形状数据集

    Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape. (arXiv:2308.11737v1 [cs.CV])

    [http://arxiv.org/abs/2308.11737](http://arxiv.org/abs/2308.11737)

    Animal3D是一个全面的哺乳动物3D姿态和形状数据集，通过手工标注和检查确保了高质量的结果。通过Animal3D数据集，我们对形状和姿态估计模型进行了基准测试，包括使用只有Animal3D数据的监督学习、合成到真实的转移和微调人体姿势和形状估计模型。

    

    准确估计3D姿态和形状是理解动物行为的重要一步，对于野生动物保护等许多下游应用有潜在好处。然而，这个领域的研究受到缺乏包含高质量3D姿态和形状注释的全面多样数据集的限制。在本文中，我们提出了Animal3D，这是一个用于哺乳动物动物3D姿态和形状估计的首个全面数据集。Animal3D由来自40个哺乳动物物种的3379个图像组成，包含26个关键点的高质量注释，以及SMAL模型的姿态和形状参数。所有注释都经过多阶段手工标注和检查，以确保最高质量的结果。基于Animal3D数据集，我们对代表性的形状和姿态估计模型进行了以下基准测试：（1）只使用Animal3D数据的监督学习，（2）从合成生成的图像进行合成到真实的转移，（3）微调人体姿势和形状估计模型。

    Accurately estimating the 3D pose and shape is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. However, research in this area is held back by the lack of a comprehensive and diverse dataset with high-quality 3D pose and shape annotations. In this paper, we propose Animal3D, the first comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and importantly the pose and shape parameters of the SMAL model. All annotations were labeled and checked manually in a multi-stage process to ensure highest quality results. Based on the Animal3D dataset, we benchmark representative shape and pose estimation models at: (1) supervised learning from only the Animal3D data, (2) synthetic to real transfer from synthetically generated images, and (3) fine-tuning human pose and shape estim
    
[^136]: 使用合规的蒙特卡洛预测实现鲁棒的不确定性量化

    Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction. (arXiv:2308.09647v1 [cs.LG])

    [http://arxiv.org/abs/2308.09647](http://arxiv.org/abs/2308.09647)

    这篇论文介绍了一种名为MC-CP的新型混合不确定性量化方法，通过将自适应蒙特卡洛dropout方法与合规预测相结合，实现了节省资源和产生鲁棒预测集/区间的目标。实验证明MC-CP在分类任务中相比其他先进方法具有显著提升

    

    在安全关键应用中部署深度学习模型仍然是一项非常具有挑战性的任务，需要对这些模型的可靠运行提供保证。不确定性量化（UQ）方法估计每个预测的模型置信度，通过考虑随机性和模型错误规范化的影响来指导决策。尽管最先进的UQ方法取得了一些进展，但它们在计算上要么非常昂贵，要么产生保守的预测集/区间。我们引入了一种新的混合UQ方法MC-CP，它将一种新的自适应蒙特卡洛（MC）dropout方法与合规预测（CP）相结合。MC-CP在运行时自适应调节传统的MC dropout以节省内存和计算资源，使得预测可以被CP使用，得到鲁棒的预测集/区间。通过全面的实验，我们展示了MC-CP相比MC dropout、RAPS和CQR等先进的UQ方法能够显著改善分类性能

    Deploying deep learning models in safety-critical applications remains a very challenging task, mandating the provision of assurances for the dependable operation of these models. Uncertainty quantification (UQ) methods estimate the model's confidence per prediction, informing decision-making by considering the effect of randomness and model misspecification. Despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ method that combines a new adaptive Monte Carlo (MC) dropout method with conformal prediction (CP). MC-CP adaptively modulates the traditional MC dropout at runtime to save memory and computation resources, enabling predictions to be consumed by CP, yielding robust prediction sets/intervals. Throughout comprehensive experiments, we show that MC-CP delivers significant improvements over advanced UQ methods, like MC dropout, RAPS and CQR, both in classificati
    
[^137]: 潜在状态模型的训练动态

    Latent State Models of Training Dynamics. (arXiv:2308.09543v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.09543](http://arxiv.org/abs/2308.09543)

    这项研究使用隐马尔可夫模型解释了神经网络训练过程中随机性的影响，并提供了对训练动态的直观概述。

    

    随机性对模型训练的影响尚未得到很好的理解。数据顺序和初始化的差异如何实际体现在模型中，以至于一些训练运行表现出色或收敛更快？此外，我们如何解释产生的训练动态以及表征不同轨迹的相变？为了理解随机性对神经网络训练的动态和结果的影响，我们使用不同的随机种子多次训练模型，并在训练过程中计算各种指标，如神经网络权重的$L_2$范数、均值和方差。然后，我们在这些指标的结果序列上拟合了一个隐马尔可夫模型(HMM)。HMM表示训练过程是一个在潜在状态之间转换的随机过程，提供了对训练过程中显著变化的直观概述。使用我们的方法，我们对grokking任务、图像分类等训练动态进行了低维、离散表示。

    The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image clas
    
[^138]: 多类在线学习在Bandit反馈下的研究

    Multiclass Online Learnability under Bandit Feedback. (arXiv:2308.04620v1 [cs.LG])

    [http://arxiv.org/abs/2308.04620](http://arxiv.org/abs/2308.04620)

    Bandit反馈下的在线多类学习的关键在于Bandit Littlestone维度的有限性，无论标签空间是否无界。

    

    我们研究了在Bandit反馈下的多类在线分类问题。我们扩展了(daniely2013price)的结果，通过展示Bandit Littlestone维度的有限性是多类在线学习的必要且充分条件，即使标签空间是无界的。我们的结果补充了(hanneke2023multiclass)的最近工作，他们在标签空间无界的全信息设置中，展示了Littlestone维度刻画了在线多类学习的能力。

    We study online multiclass classification under bandit feedback. We extend the results of (daniely2013price) by showing that the finiteness of the Bandit Littlestone dimension is necessary and sufficient for bandit online multiclass learnability even when the label space is unbounded. Our result complements the recent work by (hanneke2023multiclass) who show that the Littlestone dimension characterizes online multiclass learnability in the full-information setting when the label space is unbounded.
    
[^139]: FedDRL: 一种基于分阶段强化学习的可信联邦学习模型融合方法

    FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])

    [http://arxiv.org/abs/2307.13716](http://arxiv.org/abs/2307.13716)

    FedDRL是一种分阶段强化学习的联邦学习模型融合方法，解决了传统方法中无法解决的客户端模型质量和恶意模型问题。

    

    传统的联邦学习使用样本数量计算每个客户端模型的权重，并使用这个固定权重值来融合全局模型。然而，在实际场景中，每个客户端设备和数据的异质性导致每个客户端模型的质量存在差异。因此，对全局模型的贡献不仅仅取决于样本量。此外，如果客户端故意上传低质量或恶意模型，使用这些模型进行聚合将严重降低全局模型的准确性。传统的联邦学习算法没有解决这些问题。为了解决这个问题，我们提出了一种名为FedDRL的模型融合方法，它使用两个阶段的强化学习。在第一个阶段，我们的方法可以过滤掉恶意模型，并选择可信的客户端模型参与模型融合。在第二个阶段，FedDRL算法自适应地调整可信客户端模型的权重并聚合。

    Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
    
[^140]: 多无人机速度控制兼顾避障和交接感知的小区关联：带有动作分支的深度强化学习研究

    Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching. (arXiv:2307.13158v1 [cs.LG])

    [http://arxiv.org/abs/2307.13158](http://arxiv.org/abs/2307.13158)

    本文提出了一种深度强化学习方法，用于优化多无人机在三维空中高速公路上的小区关联决策和移动速度，以提升交通和通信性能。仿真结果显示，性能提高了18.32%。

    

    本文提出了一种深度强化学习方法，用于优化三维空中高速公路上多无人机的小区关联决策和移动速度，以提升交通和通信性能，包括避障、连接性和交接效果。问题被建模为一个马尔可夫决策过程（MDP），其中无人机的状态由速度和通信数据速率定义。我们提出了一种神经结构，其中包含一个共享的决策模块和多个网络分支，每个分支专门处理二维交通-通信空间中的特定动作维度。这种设计有效地处理了多维动作空间，使得各个动作维度可以独立决策。我们介绍了两个模型，分支型对抗性Q网络（BDQ）和分支型对抗性双重深度Q网络（Dueling DDQN），来证明这种方法。仿真结果显示，与现有基准相比，性能提高了18.32%。

    This paper presents a deep reinforcement learning solution for optimizing multi-UAV cell-association decisions and their moving velocity on a 3D aerial highway. The objective is to enhance transportation and communication performance, including collision avoidance, connectivity, and handovers. The problem is formulated as a Markov decision process (MDP) with UAVs' states defined by velocities and communication data rates. We propose a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design efficiently handles the multi-dimensional action space, allowing independence for individual action dimensions. We introduce two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.
    
[^141]: 用于验证深度神经网络的DPLL(T)框架

    A DPLL(T) Framework for Verifying Deep Neural Networks. (arXiv:2307.10266v1 [cs.LG])

    [http://arxiv.org/abs/2307.10266](http://arxiv.org/abs/2307.10266)

    这项工作介绍了一个新的约束求解方法NeuralSAT，用于验证深度神经网络。Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art.

    

    深度神经网络(DNNs)已经成为解决现实世界问题的有效方法。然而，与人工编写的软件一样，自动生成的DNNs可能存在错误并受到攻击。因此，近年来在开发有效和可扩展的DNN验证技术和工具方面引起了广泛关注。在这项工作中，我们介绍了NeuralSAT，一种用于DNN验证的新的约束求解方法。NeuralSAT的设计遵循了用于现代SMT求解的DPLL(T)算法，包括（冲突）子句学习、抽象和理论求解，因此NeuralSAT可以被视为DNNs的一个SMT框架。初步结果显示，NeuralSAT原型与最先进的方法相竞争。我们希望通过适当的优化和工程化，NeuralSAT能够将现代SAT/SMT求解器的能力和成功带到DNN验证中。NeuralSAT可从以下链接获取：https://github.com/dynaroars/neuralsat-solver

    Deep Neural Networks (DNNs) have emerged as an effective approach to tackling real-world problems. However, like human-written software, automatically-generated DNNs can have bugs and be attacked. This thus attracts many recent interests in developing effective and scalable DNN verification techniques and tools. In this work, we introduce a NeuralSAT, a new constraint solving approach to DNN verification. The design of NeuralSAT follows the DPLL(T) algorithm used modern SMT solving, which includes (conflict) clause learning, abstraction, and theory solving, and thus NeuralSAT can be considered as an SMT framework for DNNs. Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art. We hope, with proper optimization and engineering, NeuralSAT will carry the power and success of modern SAT/SMT solvers to DNN verification. NeuralSAT is avaliable from: https://github.com/dynaroars/neuralsat-solver
    
[^142]: EasyTPP: 迈向开放基准测试时间点过程模型

    EasyTPP: Towards Open Benchmarking Temporal Point Processes. (arXiv:2307.08097v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08097](http://arxiv.org/abs/2307.08097)

    EasyTPP是第一个关于事件序列建模领域的中心资源库，提供统一的数据集使用界面和广泛的评估程序，解决了该领域缺乏标准化的问题，推动了研究和应用的进展。

    

    连续时间事件序列在诸如医疗保健、金融、在线购物、社交网络等现实世界领域中发挥着重要作用。为了对这类数据进行建模，时间点过程模型（TPPs）已成为最自然和有竞争力的模型，在学术界和应用界都产生了重要影响。尽管近年来出现了许多强大的模型，但这些模型和未来的研究尝试之间缺乏一个中心基准。这种缺乏标准化的情况阻碍了研究人员和从业者对方法进行比较和结果的再现，可能会减慢该领域的进展。在本文中，我们提出了EasyTPP，这是第一个关于事件序列建模领域的研究资产（例如数据、模型、评估程序、文档）的集中存储库。我们的EasyTPP对此领域有几个独特的贡献：统一的使用现有数据集和添加新数据集的界面；广泛的评估程序，包括...

    Continuous-time event sequences play a vital role in real-world domains such as healthcare, finance, online shopping, social networks, and so on. To model such data, temporal point processes (TPPs) have emerged as the most natural and competitive models, making a significant impact in both academic and application communities. Despite the emergence of many powerful models in recent years, there hasn't been a central benchmark for these models and future research endeavors. This lack of standardization impedes researchers and practitioners from comparing methods and reproducing results, potentially slowing down progress in this field. In this paper, we present EasyTPP, the first central repository of research assets (e.g., data, models, evaluation programs, documentations) in the area of event sequence modeling. Our EasyTPP makes several unique contributions to this area: a unified interface of using existing datasets and adding new datasets; a wide range of evaluation programs that are
    
[^143]: 在资源限制下的处方过程监控：一种强化学习方法

    Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach. (arXiv:2307.06564v1 [cs.AI])

    [http://arxiv.org/abs/2307.06564](http://arxiv.org/abs/2307.06564)

    本论文提出了一种在资源限制下进行处方过程监控的强化学习方法。通过考虑对干预需求、及时性或效果预测的不确定性和资源利用水平，来触发干预，从而优化业务过程的性能。

    

    处方过程监控方法旨在通过在运行时触发干预来优化业务过程的性能，从而增加正面案例结果的概率。这些干预是根据干预策略触发的。强化学习被提出作为通过试错学习干预策略的一种方法。然而，现有方法在这一领域假设可用于执行干预的资源数量是无限的，这在实践中是不现实的。本文认为，在资源限制的情况下，处方过程监控领域面临的一个关键困境是基于对干预需求、及时性或效果的预测的不确定性和资源利用水平来触发干预。实际上，当对干预的必要性或效果存在高度不确定性时，将有限的资源用于干预是一项挑战。

    Prescriptive process monitoring methods seek to optimize the performance of business processes by triggering interventions at runtime, thereby increasing the probability of positive case outcomes. These interventions are triggered according to an intervention policy. Reinforcement learning has been put forward as an approach to learning intervention policies through trial and error. Existing approaches in this space assume that the number of resources available to perform interventions in a process is unlimited, an unrealistic assumption in practice. This paper argues that, in the presence of resource constraints, a key dilemma in the field of prescriptive process monitoring is to trigger interventions based not only on predictions of their necessity, timeliness, or effect but also on the uncertainty of these predictions and the level of resource utilization. Indeed, committing scarce resources to an intervention when the necessity or effects of this intervention are highly uncertain m
    
[^144]: 具有收敛保证的公正感知联邦极小化优化

    Fairness-aware Federated Minimax Optimization with Convergence Guarantee. (arXiv:2307.04417v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04417](http://arxiv.org/abs/2307.04417)

    本文提出了一种名为FFALM的算法，通过施加公平约束和解决极小化极大回归问题，在联邦学习中解决了群体公平性问题。实验证明FFALM在处理严重统计异质性问题时具有良好的效果。

    

    由于其保护隐私的特性，联邦学习 (FL) 吸引了相当多的关注。然而，管理用户数据的自由度不足可能导致群体公平性问题，即模型偏向于敏感因素诸如种族或性别。为了解决这个问题，本文提出了一种新颖的算法，名为带有增广拉格朗日方法的公平联邦平均法 (FFALM)，专门用于解决FL中的群体公平问题。具体来说，我们对训练目标施加了公平约束，并解决了受约束优化问题的极小化极大回归。然后，我们推导了FFALM的收敛速率的理论上界。通过在CelebA和UTKFace数据集中充分考虑严重统计异质性，实证结果表明了FFALM 在提高公平性方面的有效性。

    Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature. Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models are biased towards sensitive factors such as race or gender. To tackle this issue, this paper proposes a novel algorithm, fair federated averaging with augmented Lagrangian method (FFALM), designed explicitly to address group fairness issues in FL. Specifically, we impose a fairness constraint on the training objective and solve the minimax reformulation of the constrained optimization problem. Then, we derive the theoretical upper bound for the convergence rate of FFALM. The effectiveness of FFALM in improving fairness is shown empirically on CelebA and UTKFace datasets in the presence of severe statistical heterogeneity.
    
[^145]: 看看、记住和推理：基于机理的视觉推理

    Look, Remember and Reason: Visual Reasoning with Grounded Rationales. (arXiv:2306.17778v1 [cs.CV])

    [http://arxiv.org/abs/2306.17778](http://arxiv.org/abs/2306.17778)

    在这项研究中，我们借鉴了人类视觉问题解决的方法，通过三个步骤（看、记住、推理）逐步提取视觉信息来解决复杂的视觉推理问题，从而使大型语言模型能够有效解决这些问题。

    

    近期，大型语言模型在各种推理任务上展示了与人类水平的表现。然而，这些模型在进行复杂的视觉推理方面的能力尚未得到详细研究。在许多视觉推理任务中，一个关键挑战是需要将视觉信息紧密融合到推理过程中。我们提出通过借鉴人类视觉问题解决的方法来解决这个挑战，这个方法依赖于多种低级视觉能力。它通常可以被看作是“看，记住，推理”的三个步骤过程：通过逐步进行低级视觉过程提取视觉信息，直到得出最终答案。我们遵循相同的范例，通过最小的架构更改，使现有的大型语言模型能够解决视觉推理问题。为此，我们引入了基于视觉输入的原理，允许我们集成低级视觉能力，如对象识别。

    Large language models have recently shown human level performance on a variety of reasoning tasks. However, the ability of these models to perform complex visual reasoning has not been studied in detail yet. A key challenge in many visual reasoning tasks is that the visual information needs to be tightly integrated in the reasoning process. We propose to address this challenge by drawing inspiration from human visual problem solving which depends on a variety of low-level visual capabilities. It can often be cast as the three step-process of ``Look, Remember, Reason'': visual information is incrementally extracted using low-level visual routines in a step-by-step fashion until a final answer is reached. We follow the same paradigm to enable existing large language models, with minimal changes to the architecture, to solve visual reasoning problems. To this end, we introduce rationales over the visual input that allow us to integrate low-level visual capabilities, such as object recogni
    
[^146]: 基于Moreau包络的弱凸差分重构与双层规划算法

    Moreau Envelope Based Difference-of-weakly-Convex Reformulation and Algorithm for Bilevel Programs. (arXiv:2306.16761v1 [math.OC])

    [http://arxiv.org/abs/2306.16761](http://arxiv.org/abs/2306.16761)

    本文提出了一种基于Moreau包络的弱凸差分重构与双层规划算法，可以适用于更多与机器学习和统计相关的应用。

    

    最近，Ye等人设计了一个解决特定类别双层规划的算法，重点是与超参数选择相关的应用，利用基于值函数方法改写的差分凸算法。在下层问题完全凸性的情况下，该算法特别强大，如支持向量机模型或最小绝对收缩和选择算子模型。在本文中，为了适应更多与机器学习和统计相关的应用，我们将下层完全凸性的基本假设大大削弱为弱凸性。因此，我们提出了一种使用下层问题的Moreau包络进行重构，并证明了该重构是一个弱凸差分规划。随后，我们开发了一个逐步收敛的算法来解决这个弱凸差分规划。

    Recently, Ye et al. (Mathematical Programming 2023) designed an algorithm for solving a specific class of bilevel programs with an emphasis on applications related to hyperparameter selection, utilizing the difference of convex algorithm based on the value function approach reformulation. The proposed algorithm is particularly powerful when the lower level problem is fully convex , such as a support vector machine model or a least absolute shrinkage and selection operator model. In this paper, to suit more applications related to machine learning and statistics, we substantially weaken the underlying assumption from lower level full convexity to weak convexity. Accordingly, we propose a new reformulation using Moreau envelope of the lower level problem and demonstrate that this reformulation is a difference of weakly convex program. Subsequently, we develop a sequentially convergent algorithm for solving this difference of weakly convex program. To evaluate the effectiveness of our app
    
[^147]: 对数贝叶斯遗憾边界

    Logarithmic Bayes Regret Bounds. (arXiv:2306.09136v1 [cs.LG])

    [http://arxiv.org/abs/2306.09136](http://arxiv.org/abs/2306.09136)

    该论文提出了对于贝叶斯赌博机的首个有限时间对数遗憾边界，并用于高斯和线性赌博机，从而阐明了贝叶斯设置中先验价值以及对$\tilde{O}(\sqrt{n})$界限的改善。

    

    我们为贝叶斯赌博机导出了首个有限时间对数遗憾边界。对于高斯赌博机，我们获得了一个$O(c_h \log^2 n)$的边界，其中$c_h$是与先验相关的常量。这与Lai（1987）的渐近下限相匹配。我们的证明与先前的工作有所不同，且简单且普遍。为了显示一般性，我们将我们的技术应用于线性赌博机。我们的界限阐明了贝叶斯设置中先验的价值，既可以作为目标，也可以作为传递给学习者的附加信息。它们显着改善了现有的$\tilde{O}(\sqrt{n})$界限，尽管存在下限，但已成为文献中的标准。

    We derive the first finite-time logarithmic regret bounds for Bayesian bandits. For Gaussian bandits, we obtain a $O(c_h \log^2 n)$ bound, where $c_h$ is a prior-dependent constant. This matches the asymptotic lower bound of Lai (1987). Our proofs mark a technical departure from prior works, and are simple and general. To show generality, we apply our technique to linear bandits. Our bounds shed light on the value of the prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve the $\tilde{O}(\sqrt{n})$ bounds, that despite the existing lower bounds, have become standard in the literature.
    
[^148]: YOLOv5在交通和道路标志检测中的对抗攻击

    Adversarial Attack On Yolov5 For Traffic And Road Sign Detection. (arXiv:2306.06071v1 [cs.CV])

    [http://arxiv.org/abs/2306.06071](http://arxiv.org/abs/2306.06071)

    本文研究了YOLOv5在交通和道路标志检测中的对抗攻击脆弱性，发现YOLOv5易受多种攻击的影响，有 important implications for the safety and reliability of object detection algorithms used in traffic.

    

    本文针对YOLOv5目标检测算法实现并研究了流行的对抗攻击。研究探讨了YOLOv5在交通和道路标志检测中的对抗攻击脆弱性。并研究了不同类型的攻击（包括L-BFGS、FGSM、C&W、BIM、PGD、One Pixel Attack和UAP）对YOLOv5在道路标志检测中的影响。结果显示，YOLOv5易受这些攻击的影响，随着扰动大小的增加，分类错误率也会增加。我们还使用显著性图解释了这些结果。本文的发现对于交通目标检测算法的安全性和可靠性具有重要的意义。

    This paper implements and investigates popular adversarial attacks on the YOLOv5 Object Detection algorithm. The paper explores the vulnerability of the YOLOv5 to adversarial attacks in the context of traffic and road sign detection. The paper investigates the impact of different types of attacks, including the Limited memory Broyden Fletcher Goldfarb Shanno (L-BFGS), the Fast Gradient Sign Method (FGSM) attack, the Carlini and Wagner (C&W) attack, the Basic Iterative Method (BIM) attack, the Projected Gradient Descent (PGD) attack, One Pixel Attack, and the Universal Adversarial Perturbations attack on the accuracy of YOLOv5 in detecting traffic and road signs. The results show that YOLOv5 is susceptible to these attacks, with misclassification rates increasing as the magnitude of the perturbations increases. We also explain the results using saliency maps. The findings of this paper have important implications for the safety and reliability of object detection algorithms used in traf
    
[^149]: 用于组合优化的神经算法推理

    Neural Algorithmic Reasoning for Combinatorial Optimisation. (arXiv:2306.06064v1 [cs.NE])

    [http://arxiv.org/abs/2306.06064](http://arxiv.org/abs/2306.06064)

    本文提出了一种用于组合优化问题的神经算法推理方法，旨在解决旅行商问题。该方法是通过在TSP实例训练之前，将神经模型用相关算法进行预训练来实现的。实验结果表明，该方法可以显著提高TSP问题的解决效率。

    

    使用神经网络解决NP难/完全组合问题是一个挑战性的研究领域，旨在超越传统的近似算法。其长期目标是通过学习仅从训练数据生成更优解来超越手工设计的启发式算法，而旅行商问题(TSP)是经常被这些方法瞄准的一个重要的组合优化问题。然而，目前用于解决TSP的基于神经网络的方法常常忽略了问题固有的“算法”本质。与此相反，设计用于TSP的启发式方法常常利用诸如查找最小生成树之类的成熟算法。在本文中，我们提出利用神经算法推理的最新进展来改进TSP问题的学习。具体来说，我们建议在对TSP实例进行训练之前，在相关算法上对我们的神经模型进行预训练。我们的结果表明，使用这种学习方法可以显著提高TSP问题的解决效率。

    Solving NP-hard/complete combinatorial problems with neural networks is a challenging research area that aims to surpass classical approximate algorithms. The long-term objective is to outperform hand-designed heuristics for NP-hard/complete problems by learning to generate superior solutions solely from training data. The Travelling Salesman Problem (TSP) is a prominent combinatorial optimisation problem often targeted by such approaches. However, current neural-based methods for solving TSP often overlook the inherent "algorithmic" nature of the problem. In contrast, heuristics designed for TSP frequently leverage well-established algorithms, such as those for finding the minimum spanning tree. In this paper, we propose leveraging recent advancements in neural algorithmic reasoning to improve the learning of TSP problems. Specifically, we suggest pre-training our neural model on relevant algorithms before training it on TSP instances. Our results demonstrate that, using this learning
    
[^150]: 线性条件VAE和分层VAE中的后验崩溃现象

    Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders. (arXiv:2306.05023v1 [stat.ML])

    [http://arxiv.org/abs/2306.05023](http://arxiv.org/abs/2306.05023)

    本文研究了高度相似的变分后验分布和先验分布之间的后验崩溃现象，特别地，通过对线性条件VAE和分层VAE进行分析，证明了这种现象是由于潜在变量层次关系不清晰而引起的。

    

    在变分自编码器（VAE）中，后验崩溃现象指的是变分后验分布与先验分布的相似度过高，导致编码器提取的潜在变量保存的输入数据信息较少，无法为解码器的数据重建过程产生有意义的表示。尽管该现象一直是VAEs性能的研究热点，但是对于后验崩溃的理论却相对薄弱，特别是在非标准的VAEs中。本文通过对两类重要而常见又较少研究的VAEs进行非平凡的理论分析，即具有两个潜在变量层次的线性条件VAE和分层VAE，提升了对后验崩溃的理论认识，证明了其成因。

    The posterior collapse phenomenon in variational autoencoders (VAEs), where the variational posterior distribution closely matches the prior distribution, can hinder the quality of the learned latent variables. As a consequence of posterior collapse, the latent variables extracted by the encoder in VAEs preserve less information from the input data and thus fail to produce meaningful representations as input to the reconstruction process in the decoder. While this phenomenon has been an actively addressed topic related to VAEs performance, the theory for posterior collapse remains underdeveloped, especially beyond the standard VAEs. In this work, we advance the theoretical understanding of posterior collapse to two important and prevalent yet less studied classes of VAEs: conditional VAEs and hierarchical VAEs. Specifically, via a non-trivial theoretical analysis of linear conditional VAEs and hierarchical VAEs with two levels of latent, we prove that the cause of posterior collapses i
    
[^151]: 基于数据驱动的遗憾平衡在线模型选择的研究

    Data-Driven Regret Balancing for Online Model Selection in Bandits. (arXiv:2306.02869v1 [cs.LG])

    [http://arxiv.org/abs/2306.02869](http://arxiv.org/abs/2306.02869)

    论文讨论在具有赌博反馈的随机环境中进行选择，提出了两种基于数据的模型选择算法，并证明了其保证。通过利用实际遗憾，这些算法在实际中取得了好效果。

    

    我们考虑在具有赌博反馈的随机环境中进行顺序决策模型选择，其中元学习器可以使用一组基本学习器，并根据每个基本学习器推荐的策略动态决策。我们通过遗憾平衡来执行模型选择，但与此相关的最近文献不同的是，我们没有假设任何关于基本学习器的先验知识，如候选遗憾保证；相反，我们以数据驱动的方式揭示这些数量。因此，元学习器能够利用每个基本学习器在给定的学习环境中产生的实际遗憾（而不是期望遗憾），并挑选出最佳的遗憾。我们设计了两个模型选择算法，操作更为雄心勃勃的遗憾概念，并且除了通过遗憾平衡证明模型选择保证外，我们还在实验中展示了处理实际遗憾的令人信服的实际优势。

    We consider model selection for sequential decision making in stochastic environments with bandit feedback, where a meta-learner has at its disposal a pool of base learners, and decides on the fly which action to take based on the policies recommended by each base learner. Model selection is performed by regret balancing but, unlike the recent literature on this subject, we do not assume any prior knowledge about the base learners like candidate regret guarantees; instead, we uncover these quantities in a data-driven manner. The meta-learner is therefore able to leverage the realized regret incurred by each base learner for the learning environment at hand (as opposed to the expected regret), and single out the best such regret. We design two model selection algorithms operating with this more ambitious notion of regret and, besides proving model selection guarantees via regret balancing, we experimentally demonstrate the compelling practical benefits of dealing with actual regrets ins
    
[^152]: Gode -- 将生物化学知识图谱集成到分子图神经网络的预训练中

    Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])

    [http://arxiv.org/abs/2306.01631](http://arxiv.org/abs/2306.01631)

    本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。

    

    分子属性的准确预测对于促进创新治疗方法的发展和理解化学物质和生物系统之间复杂的相互作用至关重要。本研究提出了一种新的方法，将单个分子结构的图表示与生物医学知识图谱 (KG) 的多个领域信息进行集成。通过集成两个级别的信息，我们可以使用自我监督策略预先训练更广泛和更强大的表示，用于分子级和 KG 级预测任务。在性能评估方面，我们在 11 个具有挑战性的化学属性预测任务上微调我们预先训练的模型。我们的框架的结果表明，我们微调的模型优于现有的最先进的模型。

    The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
    
[^153]: 基于GNN和核均值嵌入的原子模拟传递学习

    Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v1 [cs.LG])

    [http://arxiv.org/abs/2306.01589](http://arxiv.org/abs/2306.01589)

    本论文提出了一种传递学习算法，利用图神经网络和核均值嵌入在原子模拟中学习了势能表面。该方法在现实数据集上表现良好，展现出较好的可概括性和可转移性能。

    

    使用机器学习方法学习的原子相互作用势在原子模拟中得到了成功的应用。然而，深度学习管道需要大量数据，而生成参考计算是计算上要求很高的。为了克服这一困难，我们提出了一种传递学习算法，利用了图神经网络（GNNs）在描述化学环境方面的能力，以及核均值嵌入。我们从预先在OC20数据集上进行过训练的GNN中提取特征映射，并使用它来从催化过程的系统特定数据集中学习势能表面。我们的方法进一步通过灵活的核函数来增强，该核函数包括化学物种信息，从而提高了性能和可解释性。我们在一系列逐渐复杂的现实数据集上测试了我们的方法，展示了出色的概括能力和可转移性能，改进了依赖GNNs或岭回归方法的方法。

    Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, deep learning pipelines are notoriously data-hungry, while generating reference calculations is computationally demanding. To overcome this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) in describing chemical environments, together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by a flexible kernel function that incorporates chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression 
    
[^154]: 通过领域知识启示的深度学习进行药物推荐

    Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])

    [http://arxiv.org/abs/2305.19604](http://arxiv.org/abs/2305.19604)

    提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。

    

    药物推荐是医疗保健的基本但至关重要的分支，提供机会为复杂健康状况的患者支持临床医生更精确的药物处方。从电子健康记录（EHR）中学习推荐药物是先前研究中最常见的方法。然而，大多数研究忽视了根据患者的EHR中的临床表现纳入领域知识的问题。为了解决这些问题，我们提出了一种新颖的基于动态领域知识的药物推荐框架，即领域知识启示网络（DKINet），用于将领域知识与可观察的患者临床表现相结合。特别是，我们首先设计了一个基于领域知识的编码器来捕捉领域信息，然后开发了一个数据驱动的编码器将领域知识整合到可观察的EHR中。

    Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
    
[^155]: 基于双层学习的最优正则化参数研究

    On Optimal Regularization Parameters via Bilevel Learning. (arXiv:2305.18394v1 [math.OC])

    [http://arxiv.org/abs/2305.18394](http://arxiv.org/abs/2305.18394)

    本文提出了一个利用双层学习确定正则化参数的方法，并提出了一个新的表征正确参数的条件。

    

    变分正则化常用于解线性反问题，它通过添加正则化项来提高先验信息质量，并通过正则化参数加以权衡，而合适的正则化参数的选择至关重要。现有的策略例如差异原则和L-曲线可以用于确定合适的参数值，但是近年来，一种叫做双层学习的监督机器学习方法被用于确定最优参数。虽然以前的策略有各种理论结果，但在这种情况下，双层学习的良好性质仍然是一个发展中的领域。本文提出了一个更好的条件来表征确定正则化参数的正值性。

    Variational regularization is commonly used to solve linear inverse problems, and involves augmenting a data fidelity by a regularizer. The regularizer is used to promote a priori information, and is weighted by a regularization parameter. Selection of an appropriate regularization parameter is critical, with various choices leading to very different reconstructions. Existing strategies such as the discrepancy principle and L-curve can be used to determine a suitable parameter value, but in recent years a supervised machine learning approach called bilevel learning has been employed. Bilevel learning is a powerful framework to determine optimal parameters, and involves solving a nested optimisation problem. While previous strategies enjoy various theoretical results, the well-posedness of bilevel learning in this setting is still a developing field. One necessary property is positivity of the determined regularization parameter. In this work, we provide a new condition that better char
    
[^156]: 深度概率时间序列预测的更好Batch方法

    Better Batch for Deep Probabilistic Time Series Forecasting. (arXiv:2305.17028v1 [stat.ML])

    [http://arxiv.org/abs/2305.17028](http://arxiv.org/abs/2305.17028)

    该研究提出了一种新的训练方法，通过在 mini-batch 中显式地学习误差的序列相关性，来提高深度概率时间序列预测的准确性和不确定性量化。

    

    深度概率时间序列预测因其能够提供有价值的不确定性量化而受到广泛关注。然而，许多现有模型过于简单化问题，假设误差过程是与时间无关的，从而忽略了误差过程中的序列相关性。这可能会降低预测的准确性，使这些模型对决策性任务的有效性减弱。为了克服这一限制，我们提出了一种创新的训练方法，将误差自相关性纳入考虑，以增强概率预测的准确性。我们的方法涉及构造一个mini-batch，作为$D$个连续时间序列段进行模型训练，并显式地学习一个协方差矩阵，覆盖了相邻时间步之间的误差相关性。由此产生的协方差矩阵可用于提高预测准确性和增强不确定性的量化。

    Deep probabilistic time series forecasting has gained significant attention due to its ability to provide valuable uncertainty quantification for decision-making tasks. However, many existing models oversimplify the problem by assuming the error process is time-independent, thereby overlooking the serial correlation in the error process. This oversight can potentially diminish the accuracy of the forecasts, rendering these models less effective for decision-making purposes. To overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance the accuracy of probabilistic forecasting. Our method involves constructing a mini-batch as a collection of $D$ consecutive time series segments for model training and explicitly learning a covariance matrix over each mini-batch that encodes the error correlation among adjacent time steps. The resulting covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantifica
    
[^157]: DiffusionNAG: 基于扩散模型的预测引导神经结构生成

    DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models. (arXiv:2305.16943v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16943](http://arxiv.org/abs/2305.16943)

    DiffusionNAG是一种基于扩散模型的神经结构生成方法，通过考虑神经结构的有向图特性，并结合参数化的预测器的指导，可以更高效地生成具有期望特性的任务最优结构。

    

    现有的神经架构搜索(NAS)方法存在着对许多与任务无关的架构进行重复采样和训练所需的过长时间的问题。为了解决这些问题，我们提出了一种从NAS转向基于扩散模型的新型条件神经结构生成(NAG)框架，命名为DiffusionNAG。具体地，我们将神经结构视为有向图，并提出了一种用于生成神经结构的图扩散模型。此外，在参数化的预测器的指导下，DiffusionNAG可以通过从更有可能满足所需特性的区域中进行采样，灵活生成具有期望特性的任务最优结构。与使用属性预测器对架构进行采样和过滤的先前NAS方案相比，这种条件NAG方案显著更高效。我们通过在两个基于预测器的NAS场景下进行大量实验验证了DiffusionNAG的有效性。

    Existing NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG) framework based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the neural architectures as directed graphs and propose a graph diffusion model for generating them. Moreover, with the guidance of parameterized predictors, DiffusionNAG can flexibly generate task-optimal architectures with the desired properties for diverse tasks, by sampling from a region that is more likely to satisfy the properties. This conditional NAG scheme is significantly more efficient than previous NAS schemes which sample the architectures and filter them using the property predictors. We validate the effectiveness of DiffusionNAG through extensive experiments in two predictor-based NAS scenarios: Trans
    
[^158]: 在自监督学习中调节频谱

    Modulate Your Spectrum in Self-Supervised Learning. (arXiv:2305.16789v1 [cs.LG])

    [http://arxiv.org/abs/2305.16789](http://arxiv.org/abs/2305.16789)

    本文提出了谱变换（ST）框架，可以调节自监督学习的频谱，并避免特征崩溃。其中，INTL是ST的一个实例，能够在优化过程中将嵌入的频谱调节到一个等特征值分布，实现较高的准确率。

    

    白化损失为使用联合嵌入架构进行自监督学习提供了理论保证，避免了特征崩溃。本文提出了谱变换（ST）框架，在前向传递过程中将嵌入的频谱映射到所需的分布，并在反向传递过程中通过隐式梯度更新来调制嵌入的频谱。我们证明了白化变换是ST的一个特例，还有其他实例可以避免崩溃。此外，本文提出了INTL（IterNorm with trace loss）的新实例。我们理论上证明了INTL可以避免崩溃，并在优化过程中将嵌入的频谱调节到一个等特征值分布。此外，INTL实现了76.6％的最高精度。

    Whitening loss provides theoretical guarantee in avoiding feature collapse for self-supervised learning (SSL) using joint embedding architectures. One typical implementation of whitening loss is hard whitening that designs whitening transformation over embedding and imposes the loss on the whitened output. In this paper, we propose spectral transformation (ST) framework to map the spectrum of embedding to a desired distribution during forward pass, and to modulate the spectrum of embedding by implicit gradient update during backward pass. We show that whitening transformation is a special instance of ST by definition, and there exist other instances that can avoid collapse by our empirical investigation. Furthermore, we propose a new instance of ST, called IterNorm with trace loss (INTL). We theoretically prove that INTL can avoid collapse and modulate the spectrum of embedding towards an equal-eigenvalue distribution during the course of optimization. Moreover, INTL achieves 76.6% top
    
[^159]: 生物医学自然语言处理中的大型语言模型: 基准、基线和建议

    Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])

    [http://arxiv.org/abs/2305.16326](http://arxiv.org/abs/2305.16326)

    本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。

    

    生物医学文献呈指数级增长，手动筛选和提取知识变得困难。自动从生物医学文献中提取信息的生物医学自然语言处理（BioNLP）技术有助于减轻这种负担。近年来，如GPT-3和GPT-4等大型语言模型（LLMs）因其卓越的性能而受到重视。但是，它们在BioNLP任务中的有效性以及对方法开发和下游用户的影响仍未得到研究。本研究（1）在四个应用程序中在八个BioNLP数据集中建立了GPT-3和GPT-4在零-shot和一-shot设置下的基准表现，包括命名实体识别，关系提取，多标签文档分类和语义相似性和推理；（2）审查了LLMs产生的错误，并将错误分为三种类型：缺失，不一致和不需要的人工内容；（3）提出了使用LLMs的建议。

    Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
    
[^160]: 流形扩散场

    Manifold Diffusion Fields. (arXiv:2305.15586v1 [cs.LG])

    [http://arxiv.org/abs/2305.15586](http://arxiv.org/abs/2305.15586)

    流形扩散场是一种在黎曼流形上生成连续函数的方法，可以使用特征函数定义流形上的内在坐标系，并且使用多个输入输出对表示函数。相比以往的方法，其能够更好地捕捉这些函数的分布，具有更好的多样性和保真度。

    

    我们提出了流形扩散场（MDF），这是一种在黎曼流形上定义连续函数的生成模型的方法。利用谱几何分析的见解，我们通过Laplace-Beltrami算子的特征函数定义流形上的内在坐标系。MDF使用多个输入输出对构成的显式参数化来表示函数。我们的方法允许在流形上对连续函数进行采样，并且对流形的刚性和等距变换具有不变性。在多个数据集和流形上的实证结果表明，与以往的方法相比，MDF能够更好地捕捉这些函数的分布，具有更好的多样性和保真度。

    We present Manifold Diffusion Fields (MDF), an approach to learn generative models of continuous functions defined over Riemannian manifolds. Leveraging insights from spectral geometry analysis, we define an intrinsic coordinate system on the manifold via the eigen-functions of the Laplace-Beltrami Operator. MDF represents functions using an explicit parametrization formed by a set of multiple input-output pairs. Our approach allows to sample continuous functions on manifolds and is invariant with respect to rigid and isometric transformations of the manifold. Empirical results on several datasets and manifolds show that MDF can capture distributions of such functions with better diversity and fidelity than previous approaches.
    
[^161]: 超越共享词汇：增加多语言机器翻译中的表示词语相似性

    Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation. (arXiv:2305.14189v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14189](http://arxiv.org/abs/2305.14189)

    本文提出了一种超越共享词汇的方法，通过定义词级信息传输路径和使用图网络来融合跨语言的词嵌入，实现了在多语言机器翻译中提高相似含义词的对齐性和BLEU分数的一致提升。此方法只需要少量额外参数且计算成本增加有限，并且推理时间与基线相同。

    

    在多语言神经机器翻译(MNMT)中，使用共享的词汇是常见的做法。除了简单的设计外，共享标记在积极的知识转移中起着重要的作用，假设共享标记在不同语言中指的是相似的含义。然而，当词汇的重叠较小时，尤其是由于不同的书写系统，转移被限制。在本文中，我们通过词等价类定义了词级信息传输路径，并依赖图网络来融合跨语言的词嵌入。我们的实验证明了我们方法的优势：1) 具有相似含义的词的嵌入在不同语言中更好地对齐，2) 我们的方法在高和低资源MNMT方面实现了一致的BLEU提升达2.3个点，3) 需要少于1.0%的额外可训练参数，并且计算成本的增加有限，而推理时间与基线相同。

    Using a vocabulary that is shared across languages is common practice in Multilingual Neural Machine Translation (MNMT). In addition to its simple design, shared tokens play an important role in positive knowledge transfer, assuming that shared tokens refer to similar meanings across languages. However, when word overlap is small, especially due to different writing systems, transfer is inhibited. In this paper, we define word-level information transfer pathways via word equivalence classes and rely on graph networks to fuse word embeddings across languages. Our experiments demonstrate the advantages of our approach: 1) embeddings of words with similar meanings are better aligned across languages, 2) our method achieves consistent BLEU improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less than 1.0\% additional trainable parameters are required with a limited increase in computational costs, while inference time remains identical to the baseline. We release the c
    
[^162]: EXACT：用于分布式学习的全面攻击方法

    EXACT: Extensive Attack for Split Learning. (arXiv:2305.12997v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12997](http://arxiv.org/abs/2305.12997)

    本文提出了一种名为EXACT的方法，可以安全地在分布式学习中进行梯度交换，同时保护隐私、保持准确性和效率性。

    

    隐私保护机器学习可以帮助我们训练和部署利用私人信息的模型。在设备上进行机器学习可以使我们在推断期间完全避免与第三方服务器共享信息。然而，与服务器端相比，设备上的模型通常较不准确，因为它们通常只依赖于一小组设备特征且需要足够小才能在终端用户设备上高效运行。分布式学习是一种有前途的方法，可以克服这些限制。在分布式学习中，将一个大型的机器学习模型分成两部分，大部分位于服务器端，小部分在设备上执行，旨在整合私有特征。然而，这种模型的端到端训练需要在分界处交换梯度，这可能编码私有特征或标签。在本文中，我们提出了一种名为 EXACT（Extensive Attack for Split Learning）的新颖隐私保护方法，通过引入广泛的噪声实现安全的梯度交换，同时保持模型的准确性和效率。我们在三个基准数据集上评估了我们的方法，结果显示它在准确性和隐私保护方面优于现有隐私保护方法。

    Privacy-Preserving machine learning (PPML) can help us train and deploy models that utilize private information. In particular, on-device Machine Learning allows us to completely avoid sharing information with a third-party server during inference. However, on-device models are typically less accurate when compared to the server counterparts due to the fact that (1) they typically only rely on a small set of on-device features and (2) they need to be small enough to run efficiently on end-user devices. Split Learning (SL) is a promising approach that can overcome these limitations. In SL, a large machine learning model is divided into two parts, with the bigger part residing on the server-side and a smaller part executing on-device, aiming to incorporate the private features. However, end-to-end training of such models requires exchanging gradients at the cut layer, which might encode private features or labels. In this paper, we provide insights into potential privacy risks associated
    
[^163]: 深卷积神经网络中归纳偏置的理论分析

    Theoretical Analysis of Inductive Biases in Deep Convolutional Networks. (arXiv:2305.08404v1 [cs.LG])

    [http://arxiv.org/abs/2305.08404](http://arxiv.org/abs/2305.08404)

    本文研究深卷积神经网络（CNN）中的归纳偏置，证明了$\mathcal{O}(\log d)$的深度就足以实现普适性，用CNN学习稀疏函数只需要$\tilde{\mathcal{O}}(\log^2d)$个样本。同时，通过局部连接网络（LCN）分析了权重共享和局部性的归纳偏置的区别，得出了它们在表示需要有限平移等变和高方向选择性的函数方面的优越性。

    

    本文研究卷积神经网络（CNN）中的归纳偏置，这被认为是CNN在视觉任务上表现异常出色的重要驱动因素。我们首先分析了CNN的普适性，即逼近连续函数的能力。我们证明了$\mathcal{O}(\log d)$的深度就足以实现普适性，其中$d$是输入维度。这相比于现有结果需要$\Omega(d)$的深度是一项重大改进。我们还证明了用CNN学习稀疏函数只需要$\tilde{\mathcal{O}}(\log^2d)$个样本，表明深度CNN可以有效地捕捉长程稀疏相关性。我们的研究还分析了共享权重和局部性的归纳偏置，通过对称性得出结论。为了区分这两种偏见，我们引入了局部连接网络（LCN）并证明了它们在表示需要有限平移等变和高方向选择性的函数方面的优越性。我们的结果为深CNN的成功提供了理论洞察力，同时更好地理解了它们的局限性。

    In this paper, we study the inductive biases in convolutional neural networks (CNNs), which are believed to be vital drivers behind CNNs' exceptional performance on vision-like tasks. We first analyze the universality of CNNs, i.e., the ability to approximate continuous functions. We prove that a depth of $\mathcal{O}(\log d)$ is sufficient for achieving universality, where $d$ is the input dimension. This is a significant improvement over existing results that required a depth of $\Omega(d)$. We also prove that learning sparse functions with CNNs needs only $\tilde{\mathcal{O}}(\log^2d)$ samples, indicating that deep CNNs can efficiently capture long-range sparse correlations. Note that all these are achieved through a novel combination of increased network depth and the utilization of multichanneling and downsampling.  Lastly, we study the inductive biases of weight sharing and locality through the lens of symmetry. To separate two biases, we introduce locally-connected networks (LCN
    
[^164]: 用轨迹解释强化学习的决策

    Explaining RL Decisions with Trajectories. (arXiv:2305.04073v1 [cs.AI])

    [http://arxiv.org/abs/2305.04073](http://arxiv.org/abs/2305.04073)

    本文提出一种用训练过程中遇到的轨迹解释强化学习决策的方法，并在离散和连续状态及行动空间的多样化环境中证明了其有效性。

    

    解释是强化学习在许多实际决策问题中应用的关键组成部分。本文提出了一种补充这些解释的方法，特别是针对离线强化学习，即我们将训练过程中遇到的轨迹用编码的方式进行解释。

    Explanation is a key component for the adoption of reinforcement learning (RL) in many real-world decision-making problems. In the literature, the explanation is often provided by saliency attribution to the features of the RL agent's state. In this work, we propose a complementary approach to these explanations, particularly for offline RL, where we attribute the policy decisions of a trained RL agent to the trajectories encountered by it during training. To do so, we encode trajectories in offline training data individually as well as collectively (encoding a set of trajectories). We then attribute policy decisions to a set of trajectories in this encoded space by estimating the sensitivity of the decision with respect to that set. Further, we demonstrate the effectiveness of the proposed approach in terms of quality of attributions as well as practical scalability in diverse environments that involve both discrete and continuous state and action spaces such as grid-worlds, video gam
    
[^165]: 探究大型语言模型在生成单元测试方面的有效性

    Exploring the Effectiveness of Large Language Models in Generating Unit Tests. (arXiv:2305.00418v1 [cs.SE])

    [http://arxiv.org/abs/2305.00418](http://arxiv.org/abs/2305.00418)

    本文研究了三种生成模型在单元测试生成方面的效果，并发现在不经过微调的情况下，它们的覆盖率较低且存在测试味道问题。

    

    代码生成模型可以通过使用代码注释、现有代码或两者的组合来生成代码。本文调查了三个生成模型（CodeGen、Codex和GPT-3.5）在没有微调的情况下是否能够成功用于生成单元测试的效果。研究中使用了两个基准（HumanEval和Evosuite SF110）来调查环境生成对单元测试生成过程的影响。我们根据编译率、测试正确性、覆盖率和测试味道来评估模型。我们发现，Codex模型在HumanEval数据集上取得了超过80%的覆盖率，但在EvoSuite SF110基准中没有一个模型超过2%的覆盖率。生成的测试还存在测试味道问题，比如重复的断言和空测试。

    A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning. To fill this gap, we investigated how well three generative models (CodeGen, Codex, and GPT-3.5) can generate test cases. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the context generation's effect in the unit test generation process. We evaluated the models based on compilation rates, test correctness, coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.
    
[^166]: 什么让数据适合于局部连接神经网络？一种基于量子纠缠的必要且充分条件

    What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11249](http://arxiv.org/abs/2303.11249)

    本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。

    

    关于数据分布适用于深度学习的问题是一个基本的开放性问题。本文采用来自量子物理学的理论工具，针对包括卷积神经网络、循环神经网络和局部自注意力模型在内的广泛的局部连接神经网络，解决了这个问题。我们的主要理论结果是，在某些特征的规范划分下，当数据分布接受低量子纠缠时，特定的局部连接神经网络才能够准确地预测该数据分布。作为本结果的实际应用，我们导出了一种预处理方法，以增强数据分布适合局部连接神经网络的性能。在各种数据集上对广泛的模型进行实验，证明了我们的发现。我们希望我们使用量子纠缠将鼓励形式推理的物理工具来进一步采用。

    The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
    
[^167]: 通过子高斯内在矩范实现紧凑的非渐进推断

    Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm. (arXiv:2303.07287v1 [stat.ML])

    [http://arxiv.org/abs/2303.07287](http://arxiv.org/abs/2303.07287)

    本文提出了一种通过最大化一系列归一化矩来使用子高斯内在矩范实现紧凑的非渐进推断的方法，该方法可以导致更紧的Hoeffding子高斯浓度不等式，并且可以通过子高斯图检查具有有限样本大小的子高斯数据。

    This paper proposes a method of achieving tight non-asymptotic inference by using sub-Gaussian intrinsic moment norm through maximizing a series of normalized moments, which can lead to tighter Hoeffding's sub-Gaussian concentration inequalities and can be checked with sub-Gaussian plot for sub-Gaussian data with a finite sample size.

    在非渐进统计推断中，子高斯分布的方差类型参数起着至关重要的作用。然而，基于经验矩生成函数（MGF）的直接估计这些参数是不可行的。为此，我们建议通过最大化一系列归一化矩来使用子高斯内在矩范[Buldygin和Kozachenko（2000），定理1.3]。重要的是，推荐的范数不仅可以恢复相应MGF的指数矩界限，而且还可以导致更紧的Hoeffding子高斯浓度不等式。在实践中，我们提出了一种直观的方法，通过子高斯图检查具有有限样本大小的子高斯数据。可以通过简单的插入方法鲁棒地估计内在矩范数。我们的理论结果应用于非渐进分析，包括多臂赌博机。

    In non-asymptotic statistical inferences, variance-type parameters of sub-Gaussian distributions play a crucial role. However, direct estimation of these parameters based on the empirical moment generating function (MGF) is infeasible. To this end, we recommend using a sub-Gaussian intrinsic moment norm [Buldygin and Kozachenko (2000), Theorem 1.3] through maximizing a series of normalized moments. Importantly, the recommended norm can not only recover the exponential moment bounds for the corresponding MGFs, but also lead to tighter Hoeffding's sub-Gaussian concentration inequalities. In practice, {\color{black} we propose an intuitive way of checking sub-Gaussian data with a finite sample size by the sub-Gaussian plot}. Intrinsic moment norm can be robustly estimated via a simple plug-in approach. Our theoretical results are applied to non-asymptotic analysis, including the multi-armed bandit.
    
[^168]: Cal-QL: 计算机辅助脱机强化学习预先训练用于高效在线微调

    Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05479](http://arxiv.org/abs/2303.05479)

    本文介绍了一种计算机辅助脱机强化学习方法Cal-QL，该方法能够从脱机数据中学习一个保守的值函数初始化，使得在在线微调时同时保障了快速和性能。

    

    脱机强化学习方法可以用来从现有数据集中获取策略初始化并通过有限互动进行快速在线微调。然而，现有的脱机强化学习方法在在线微调中表现较差。本文研究了保守脱机强化学习方法中的微调问题，并设计了一种方法，可以从脱机数据中学习到有效的初始化，并使其具备快速的在线微调功能。我们的方法，即Cal-QL，通过学习一个保守的值函数初始化，低估从脱机数据中学到的策略的价值，同时确保学习到的Q值在合理的范围内。实验结果表明，我们的方法在多个基准环境中具有显著的性能优势，并且也能在真实机器人问题中进行有效的在线微调。

    A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply
    
[^169]: 贝叶斯矩阵分解及应用

    Bayesian Matrix Decomposition and Applications. (arXiv:2302.11337v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2302.11337](http://arxiv.org/abs/2302.11337)

    本书旨在介绍贝叶斯矩阵分解的概念和工具，并总结了贝叶斯矩阵分解方法在不同领域的应用。

    

    本书的唯一目的是为了给出贝叶斯矩阵分解概念和数学工具的自包含介绍，以便在后续章节中无缝引入矩阵分解技术及其应用。然而，我们清楚地意识到我们无法覆盖关于贝叶斯矩阵分解的所有有用和有趣的结果，并且由于讨论的范围有限，例如分析变分推理以进行优化的分离分析。我们将读者引导到贝叶斯分析领域的文献中，以便更详细地介绍相关领域。本书主要总结了重要的贝叶斯矩阵分解方法（例如实值分解、非负矩阵分解、贝叶斯插值分解）的目的和意义，以及这些方法的起源和复杂性对其应用提供的启示。数学先决条件是第一门课程。

    The sole aim of this book is to give a self-contained introduction to concepts and mathematical tools in Bayesian matrix decomposition in order to seamlessly introduce matrix decomposition techniques and their applications in subsequent sections. However, we clearly realize our inability to cover all the useful and interesting results concerning Bayesian matrix decomposition and given the paucity of scope to present this discussion, e.g., the separated analysis of variational inference for conducting the optimization. We refer the reader to literature in the field of Bayesian analysis for a more detailed introduction to the related fields.  This book is primarily a summary of purpose, significance of important Bayesian matrix decomposition methods, e.g., real-valued decomposition, nonnegative matrix factorization, Bayesian interpolative decomposition, and the origin and complexity of the methods which shed light on their applications. The mathematical prerequisite is a first course in 
    
[^170]: 无关交换学习，或通过多校准对全预测进行表征

    Swap Agnostic Learning, or Characterizing Omniprediction via Multicalibration. (arXiv:2302.06726v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06726](http://arxiv.org/abs/2302.06726)

    本文介绍了无关交换学习的概念，并展示了其在任何凸损失函数下的可行性。此外，研究发现全预测和多校准之间存在联系，全预测是一种新的最优性概念，可以加强经典的无关学习。

    

    我们介绍并研究了无关交换学习。这个问题可以看作是预测者和对手之间的一种博弈：首先，预测者选择一个假设$h$；然后，对手以响应方式进行游戏，并且对于预测者的每个水平集$\{x \in \mathcal{X} : h(x) = v\}$选择一个（不同的）使损失最小化的假设$c_v \in \mathcal{C}$；如果$h$与对手的损失竞争，预测者获胜。尽管对手的强大，我们证明了无关交换学习在任何凸损失函数下是可行的。令人惊讶的是，这一结果通过研究全预测和多校准之间的联系得出。全预测是预测者的一种新的最优性概念，它加强了像无关学习这样的经典概念。它要求在适用于一个丰富的损失函数族的任何损失函数上，相对于一个假设类别的损失最小化保证。最近的一系列研究探索了这一领域。

    We introduce and study Swap Agnostic Learning. The problem can be phrased as a game between a predictor and an adversary: first, the predictor selects a hypothesis $h$; then, the adversary plays in response, and for each level set of the predictor $\{x \in \mathcal{X} : h(x) = v\}$ selects a (different) loss-minimizing hypothesis $c_v \in \mathcal{C}$; the predictor wins if $h$ competes with the adaptive adversary's loss. Despite the strength of the adversary, we demonstrate the feasibility Swap Agnostic Learning for any convex loss.  Somewhat surprisingly, the result follows through an investigation into the connections between Omniprediction and Multicalibration. Omniprediction is a new notion of optimality for predictors that strengthtens classical notions such as agnostic learning. It asks for loss minimization guarantees (relative to a hypothesis class) that apply not just for a specific loss function, but for any loss belonging to a rich family of losses. A recent line of work sh
    
[^171]: 探索随机连接的神经网络在气候模型仿真中的应用

    Exploring Randomly Wired Neural Networks for Climate Model Emulation. (arXiv:2212.03369v3 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2212.03369](http://arxiv.org/abs/2212.03369)

    本研究探索了随机连接的神经网络在气候模型仿真中的应用。通过使用ClimateBench数据集，我们发现随机连接的神经网络在拥有100万和1000万参数的模型上具有与传统网络相当的性能，且能耗更低。

    

    探索不同人类活动排放情景的气候影响对于制定气候变化减缓和适应的决策至关重要。先进的地球系统模型可以提供对这些影响的详细洞见，但每个情景的计算成本很高。这种巨大的计算负担引发了对开发廉价的机器学习模型进行气候模型仿真任务的兴趣。在本文中，我们探讨了随机连接的神经网络在这一任务中的有效性。我们描述了如何构建这些网络，并将它们与传统的前馈神经网络进行比较，使用ClimateBench数据集进行评估。具体而言，我们将多层感知机、卷积神经网络和卷积长短期记忆网络中串行连接的稠密层替换为随机连接的稠密层，并评估在拥有100万和1000万参数的模型上的性能影响。我们发现，随机连接的神经网络在能耗较低的情况下可以获得与传统网络相当的模型性能。

    Exploring the climate impacts of various anthropogenic emissions scenarios is key to making informed decisions for climate change mitigation and adaptation. State-of-the-art Earth system models can provide detailed insight into these impacts, but have a large associated computational cost on a per-scenario basis. This large computational burden has driven recent interest in developing cheap machine learning models for the task of climate model emulation. In this manuscript, we explore the efficacy of randomly wired neural networks for this task. We describe how they can be constructed and compare them to their standard feedforward counterparts using the ClimateBench dataset. Specifically, we replace the serially connected dense layers in multilayer perceptrons, convolutional neural networks, and convolutional long short-term memory networks with randomly wired dense layers and assess the impact on model performance for models with 1 million and 10 million parameters. We find that model
    
[^172]: 通过元学习在哈密顿流形中识别普遍的神经表示

    Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning. (arXiv:2212.01168v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01168](http://arxiv.org/abs/2212.01168)

    通过元学习方法，在哈密顿流形中识别出普遍的神经表示，实现了对不同物理系统的快速适应能力。

    

    最近物理学中深度学习的进展集中在通过将物理先验或归纳偏见引入神经网络来发现目标系统的共享表示。然而，这些方法特定于系统，不允许轻松适应由不同物理法则驱动的新物理系统。例如，训练于质点弹簧系统的神经网络无法准确预测双体系统或任何具有不同物理法则的系统的行为。在本研究中，我们使用图神经网络模拟我们的系统，并采用元学习算法使模型在一系列任务中积累经验，并使其适应新的物理系统。我们的方法旨在学习跨各种哈密顿流形的通用表示，这是哈密顿系统数据分布的共同特征。我们使用由不同系统组成的数据集训练模型，每个系统都有其自身固有的动力学，并评估其性能。

    Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. However, these approaches are system-specific and do not allow for easy adaptation to new physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot accurately predict the behavior of a two-body system or any other system with different governing physics. In this work, we model our system with a graph neural network and employ a meta-learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a general representation across the various Hamiltonian manifolds, which is a common feature of the data distribution of Hamiltonian systems. We train our model using a dataset of different physical systems, each governed by its own inherent dynamics, and evaluate its 
    
[^173]: HashVFL: 在垂直联合学习中防御数据重构攻击

    HashVFL: Defending Against Data Reconstruction Attacks in Vertical Federated Learning. (arXiv:2212.00325v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2212.00325](http://arxiv.org/abs/2212.00325)

    【论文标题翻译】HashVFL: 在垂直联合学习中引入哈希算法以防御数据重构攻击。我们的工作表明哈希算法是对抗数据重构攻击的一种有希望的解决方案。

    

    垂直联合学习（VFL）是一种流行的协作机器学习模型训练方案。现有的工业框架使用同态加密等安全多方计算技术来确保数据的安全性和隐私性。尽管有这些努力，研究发现由于中间表示与原始数据之间的相关性，数据泄露仍然是VFL中的风险。神经网络能够准确捕捉这些相关性，使得对手可以重构数据。这强调了对于保护VFL系统的持续研究的需要。我们的工作表明，哈希算法是对抗数据重构攻击的一种有希望的解决方案。哈希的单向性使得对手难以从哈希码中恢复数据。然而，在VFL中实施哈希算法也带来了新的挑战，包括梯度消失和信息丢失。为了解决这些问题，我们提出了HashVFL，它将哈希算法与垂直联合学习相结合，同时实现了最佳的结果。

    Vertical Federated Learning (VFL) is a trending collaborative machine learning model training solution. Existing industrial frameworks employ secure multi-party computation techniques such as homomorphic encryption to ensure data security and privacy. Despite these efforts, studies have revealed that data leakage remains a risk in VFL due to the correlations between intermediate representations and raw data. Neural networks can accurately capture these correlations, allowing an adversary to reconstruct the data. This emphasizes the need for continued research into securing VFL systems.  Our work shows that hashing is a promising solution to counter data reconstruction attacks. The one-way nature of hashing makes it difficult for an adversary to recover data from hash codes. However, implementing hashing in VFL presents new challenges, including vanishing gradients and information loss. To address these issues, we propose HashVFL, which integrates hashing and simultaneously achieves lea
    
[^174]: 关于两层网络样本复杂度的研究：Lipschitz与逐元素Lipschitz激活的比较

    On the Sample Complexity of Two-Layer Networks: Lipschitz vs. Element-Wise Lipschitz Activation. (arXiv:2211.09634v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09634](http://arxiv.org/abs/2211.09634)

    本文研究了使用不同激活函数的有界两层神经网络的样本复杂度，证明了当激活函数为逐元素Lipschitz时，样本复杂度在宽度上仅有对数依赖，并且这种依赖是紧致的。

    

    我们研究了使用不同激活函数的有界两层神经网络的样本复杂度。特别地，我们考虑了类$$\mathcal{H} = \{\textbf{x}\mapsto \langle \textbf{v}, \sigma \circ W\textbf{b} + \textbf{b} \rangle : \textbf{b}\in\mathbb{R}^d, W \in \mathbb{R}^{\mathcal{T}\times d}, \textbf{v} \in \mathbb{R}^{\mathcal{T}}\}$$其中$W$和$\textbf{v}$的谱范数被$O(1)$限制，$W$的Frobenius范数从初始化开始被$R>0$限制，$\sigma$是一个Lipschitz激活函数。我们证明了如果$\sigma$是逐元素的，则$\mathcal{H}$的样本复杂度仅仅在宽度上有对数依赖，并且这种复杂度是紧致的，仅有对数因子的差异。我们进一步展示了$\sigma$的逐元素性质对于宽度上的对数依赖界限是至关重要的，也就是说存在不是逐元素激活函数的样本复杂度在宽度上是线性的，针对宽度，样本复杂度介于对数和线性之间。

    We investigate the sample complexity of bounded two-layer neural networks using different activation functions.  In particular, we consider the class  $$ \mathcal{H} = \left\{\textbf{x}\mapsto \langle \textbf{v}, \sigma \circ W\textbf{b} + \textbf{b} \rangle : \textbf{b}\in\mathbb{R}^d, W \in \mathbb{R}^{\mathcal{T}\times d}, \textbf{v} \in \mathbb{R}^{\mathcal{T}}\right\} $$  where the spectral norm of $W$ and $\textbf{v}$ is bounded by $O(1)$, the Frobenius norm of $W$ is bounded from its initialization by $R > 0$, and $\sigma$ is a Lipschitz activation function.  We prove that if $\sigma$ is element-wise, then the sample complexity of $\mathcal{H}$ has only logarithmic dependency in width and that this complexity is tight, up to logarithmic factors.  We further show that the element-wise property of $\sigma$ is essential for a logarithmic dependency bound in width, in the sense that there exist non-element-wise activation functions whose sample complexity is linear in width, for wid
    
[^175]: 正交多项式逼近算法（OPAA）：一种用于估计概率密度的功能分析方法

    Orthogonal Polynomials Approximation Algorithm (OPAA):a functional analytic approach to estimating probability densities. (arXiv:2211.08594v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08594](http://arxiv.org/abs/2211.08594)

    OPAA是一种功能分析方法的算法，通过找到平滑的概率分布函数估计值、计算归一化权重的估计值，并使用特殊的函数空间转换来估计证据，实现了并行计算的一次通过。它适用于估计概率密度函数，尤其在贝叶斯问题中估计归一化权重。

    

    我们提出了一种新的正交多项式逼近算法（OPAA），这是一种可并行化的算法，使用功能分析方法估计概率分布：首先，它找到了概率分布的平滑函数估计，无论它是否归一化；其次，算法提供了归一化权重的估计；第三，算法提出了一种新的计算方案来计算这些估计值。OPAA的核心组成部分是将联合分布的平方根转化为我们构造的特殊函数空间的特殊变换。通过这个变换，证据等于转换函数的$L^2$范数的平方。因此，可以通过转换系数的平方和来估计证据。计算可以并行化并在一次通过中完成。OPAA可以广泛应用于概率密度函数的估计。在贝叶斯问题中，它可以用于估计归一化权重。

    We present the new Orthogonal Polynomials Approximation Algorithm (OPAA), a parallelizable algorithm that estimates probability distributions using functional analytic approach: first, it finds a smooth functional estimate of the probability distribution, whether it is normalized or not; second, the algorithm provides an estimate of the normalizing weight; and third, the algorithm proposes a new computation scheme to compute such estimates.  A core component of OPAA is a special transform of the square root of the joint distribution into a special functional space of our construct. Through this transform, the evidence is equated with the $L^2$ norm of the transformed function, squared. Hence, the evidence can be estimated by the sum of squares of the transform coefficients. Computations can be parallelized and completed in one pass.  OPAA can be applied broadly to the estimation of probability density functions. In Bayesian problems, it can be applied to estimating the normalizing weig
    
[^176]: 利用仿射模型变换的迁移学习

    Transfer learning with affine model transformation. (arXiv:2210.09745v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.09745](http://arxiv.org/abs/2210.09745)

    本文提出了一种叫做仿射模型转移的迁移学习方法，通过最小化期望平方损失，可以适应各种不同的方法，包括基于神经特征提取器的方法。对于这个方法也给出了理论上的解释。

    

    由于在数据稀缺的情况下能够提高机器学习的预测能力，受到了广泛关注。传统上，使用给定的源模型集和来自目标领域的数据集，通过统计学习来适应预训练模型到目标领域，学习领域转移和领域特定因素。虽然这种方法在广泛的实际应用中取得了巨大的成功，但缺乏理论基础阻碍了进一步的方法发展。本文提出了一种称为仿射模型转移的通用类别的迁移学习回归方法，遵循期望平方损失最小化的原则。结果表明，仿射模型转移广泛包括各种现有方法，包括基于神经特征提取器的最常见过程。此外，本文阐明了仿射模型转移的理论特性。

    Supervised transfer learning has received considerable attention due to its potential to boost the predictive power of machine learning in scenarios where data are scarce. Generally, a given set of source models and a dataset from a target domain are used to adapt the pre-trained models to a target domain by statistically learning domain shift and domain-specific factors. While such procedurally and intuitively plausible methods have achieved great success in a wide range of real-world applications, the lack of a theoretical basis hinders further methodological development. This paper presents a general class of transfer learning regression called affine model transfer, following the principle of expected-square loss minimization. It is shown that the affine model transfer broadly encompasses various existing methods, including the most common procedure based on neural feature extractors. Furthermore, the current paper clarifies theoretical properties of the affine model transfer such 
    
[^177]: ImpNet: 编译神经网络中不可察觉和黑盒不可检测的后门攻击

    ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks. (arXiv:2210.00108v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00108](http://arxiv.org/abs/2210.00108)

    ImpNet是一种在编译神经网络中添加的不可察觉和黑盒不可检测的后门攻击。这些后门可以绕过数据准备和模型训练阶段的保护措施，并且只能在插入阶段可靠地检测到，移除它们很具有挑战性。

    

    早期的后门攻击引起了攻防开发中的一场竞争。防御手段已经出现，并展示了在模型中检测和移除后门的能力。这些防御手段通过检查训练数据、模型或训练过程的完整性来工作。在这项工作中，我们展示了后门可以在编译过程中添加，绕过了数据准备和模型训练阶段的任何保护措施。攻击者不仅可以在编译过程中插入已有的基于权重的后门，还可以插入一种新的与权重无关的后门，例如ImpNet。这些后门在训练或数据准备过程中是不可检测的，因为它们尚不存在。接下来，我们展示了一些后门，包括ImpNet，只能在插入它们的阶段可靠地检测到，而在其他任何地方移除它们都是一个重大挑战。我们得出结论，机器学习模型的安全性需要保证。

    Early backdoor attacks against machine learning set off an arms race in attack and defence development. Defences have since appeared demonstrating some ability to detect backdoors in models or even remove them. These defences work by inspecting the training data, the model, or the integrity of the training procedure. In this work, we show that backdoors can be added during compilation, circumventing any safeguards in the data preparation and model training stages. The attacker can not only insert existing weight-based backdoors during compilation, but also a new class of weight-independent backdoors, such as ImpNet. These backdoors are impossible to detect during the training or data preparation processes, because they are not yet present. Next, we demonstrate that some backdoors, including ImpNet, can only be reliably detected at the stage where they are inserted and removing them anywhere else presents a significant challenge. We conclude that ML model security requires assurance of 
    
[^178]: 连续学习中任务的表述方式很重要：视觉问答案例研究

    Task Formulation Matters When Learning Continually: A Case Study in Visual Question Answering. (arXiv:2210.00044v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00044](http://arxiv.org/abs/2210.00044)

    这项研究在连续学习领域中深入研究了任务的不同表述方式对视觉问答任务的性能的影响，发现输出分布的变化是性能和任务顺序敏感度的关键，同时探讨了预训练模型和不同视觉嵌入的Transformer模型的稳健性及模型表示对遗忘的影响。

    

    连续学习旨在在一系列任务上增量训练模型，而不会遗忘之前的知识。尽管连续学习在计算机视觉中已被广泛研究，但在视觉+语言任务中的应用并不那么直观，因为根据输入模态可以有多种方式进行参数化。在本文中，我们对不同设置如何影响视觉问答的性能进行了详细研究。首先，我们提出了三种合理的任务表述，并展示了它们对连续学习算法性能的影响。我们对任务相似性的几个因素进行了细分，表明性能和对任务顺序的敏感度高度依赖于输出分布的变化。我们还调查了预训练模型的潜力，并比较了使用不同视觉嵌入的Transformer模型的稳健性。最后，我们提供了对模型表示和其对遗忘的影响的分析。

    Continual learning aims to train a model incrementally on a sequence of tasks without forgetting previous knowledge. Although continual learning has been widely studied in computer vision, its application to Vision+Language tasks is not that straightforward, as settings can be parameterized in multiple ways according to their input modalities. In this paper, we present a detailed study of how different settings affect performance for Visual Question Answering. We first propose three plausible task formulations and demonstrate their impact on the performance of continual learning algorithms. We break down several factors of task similarity, showing that performance and sensitivity to task order highly depend on the shift of the output distribution. We also investigate the potential of pretrained models and compare the robustness of transformer models with different visual embeddings. Finally, we provide an analysis interpreting model representations and their impact on forgetting. Our r
    
[^179]: 高频空间扩散模型用于加速MRI

    High-Frequency Space Diffusion Models for Accelerated MRI. (arXiv:2208.05481v5 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2208.05481](http://arxiv.org/abs/2208.05481)

    本文提出了一种针对高频空间扩散过程的MR重建的新型SDE模型（HFS-SDE），该模型确保了低频区域的确定性并加速了逆扩散采样过程，实验结果表明该方法在加速MRI重建中表现出良好的性能。

    

    连续随机微分方程（SDE）的扩散模型在图像生成方面表现出卓越性能。它可以作为解决磁共振（MR）重建中逆问题的深度生成先验。然而，在快速MR成像中，$k$-空间数据的低频区域通常是完全采样的，而现有的扩散模型是在整个图像或$k$-空间中进行的，这必然会引入低频区域重建的不确定性。此外，现有的扩散模型通常需要大量的迭代才能收敛，导致重建过程耗时。为了解决这些问题，我们提出了一种新颖的SDE，专门用于带有高频空间扩散过程的MR重建（称为HFS-SDE）。该方法确保了完全采样的低频区域的确定性，并加速了逆扩散过程中的采样过程。在公开可用的实验数据集上进行实验，结果表明我们的方法在加速MRI重建中取得了良好的性能。

    Diffusion models with continuous stochastic differential equations (SDEs) have shown superior performances in image generation. It can serve as a deep generative prior to solving the inverse problem in magnetic resonance (MR) reconstruction. However, low-frequency regions of $k$-space data are typically fully sampled in fast MR imaging, while existing diffusion models are performed throughout the entire image or $k$-space, inevitably introducing uncertainty in the reconstruction of low-frequency regions. Additionally, existing diffusion models often demand substantial iterations to converge, resulting in time-consuming reconstructions. To address these challenges, we propose a novel SDE tailored specifically for MR reconstruction with the diffusion process in high-frequency space (referred to as HFS-SDE). This approach ensures determinism in the fully sampled low-frequency regions and accelerates the sampling procedure of reverse diffusion. Experiments conducted on the publicly availab
    
[^180]: 异构多智能体零样本协同进化研究

    Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution. (arXiv:2208.04957v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2208.04957](http://arxiv.org/abs/2208.04957)

    本研究首次研究了异构零样本协同问题，并提出了一种基于协同进化的通用方法，通过配对、更新和选择的过程，实现了多智能体零样本协同。实验结果表明，考虑异构情况的必要性，并证明了该方法对于异构零样本协同任务的有前景的解决方案。

    

    在合作多智能体强化学习领域，生成能够与未知合作伙伴零样本协同的智能体是一个新的挑战。最近的一些研究在零样本协同方面取得了进展，通过训练过程中向智能体暴露多样化的合作伙伴。然而，这些方法通常在训练伙伴时涉及自我对弈，隐式地假设任务是同质的。然而，许多真实世界的任务是异构的，因此先前的方法可能效率低下。本文首次研究了异构零样本协同的问题，并提出了一种基于协同进化的通用方法，通过三个子过程：配对、更新和选择，对两个智能体和合作伙伴进行协同进化。对不同异构任务的实验结果突出了考虑异构情况的必要性，并证明我们提出的方法是解决异构零样本协同任务的一种有前景的解决方案。

    Generating agents that can achieve zero-shot coordination (ZSC) with unseen partners is a new challenge in cooperative multi-agent reinforcement learning (MARL). Recently, some studies have made progress in ZSC by exposing the agents to diverse partners during the training process. They usually involve self-play when training the partners, implicitly assuming that the tasks are homogeneous. However, many real-world tasks are heterogeneous, and hence previous methods may be inefficient. In this paper, we study the heterogeneous ZSC problem for the first time and propose a general method based on coevolution, which coevolves two populations of agents and partners through three sub-processes: pairing, updating and selection. Experimental results on various heterogeneous tasks highlight the necessity of considering the heterogeneous setting and demonstrate that our proposed method is a promising solution for heterogeneous ZSC tasks.
    
[^181]: Swin Transformer 深度强化学习

    Deep Reinforcement Learning with Swin Transformers. (arXiv:2206.15269v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.15269](http://arxiv.org/abs/2206.15269)

    本文介绍了基于 Swin Transformer 的在线强化学习方案 Swin DQN，通过将组合的图像像素分成小的补丁并在局部应用自我注意力操作，实现了在 Atari 基准测试上超越现有基于 CNN 的强化学习方法的最先进性能。

    

    Transformer 是神经网络模型，利用多层自我注意力头，并在自然语言处理任务中展示了巨大的潜力。同时，有人努力将 Transformer 适应于机器学习的视觉任务，包括 Vision Transformer 和 Swin Transformer。虽然一些研究人员将 Vision Transformer 用于强化学习任务，但由于高计算成本，实验仍停留在小规模。另一方面，大规模的实验必须依赖于技术来减少 Vision Transformer 的成本，这也会产生较差的结果。为了解决这一挑战，本文提出了基于 Swin Transformer 的第一个在线强化学习方案：Swin DQN。Swin Transformer 可作为神经网络中的主干骨干，将图像像素的组合分成小的补丁，并在固定大小的窗口内应用局部自我注意力操作。它们在 ImageNet 分类任务中表现卓越。Swin DQN 在 Atari 基准测试中实现了最先进的性能，优于现有的基于 CNN 的强化学习方法。

    Transformers are neural network models that utilize multiple layers of self-attention heads and have exhibited enormous potential in natural language processing tasks. Meanwhile, there have been efforts to adapt transformers to visual tasks of machine learning, including Vision Transformers and Swin Transformers. Although some researchers use Vision Transformers for reinforcement learning tasks, their experiments remain at a small scale due to the high computational cost. Experiments conducted at a large scale, on the other hand, have to rely on techniques to cut the costs of Vision Transformers, which also yield inferior results.  To address this challenge, this article presents the first online reinforcement learning scheme that is based on Swin Transformers: Swin DQN. Swin Transformers are promising as a backbone in neural networks by splitting groups of image pixels into small patches and applying local self-attention operations inside the (shifted) windows of fixed sizes. They hav
    
[^182]: 使用Twitter数据了解公众对COVID-19相关药物批准和离标使用的看法

    Using Twitter Data to Understand Public Perceptions of Approved versus Off-label Use for COVID-19-related Medications. (arXiv:2206.14358v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2206.14358](http://arxiv.org/abs/2206.14358)

    通过Twitter数据分析了解公众对COVID-19相关药物的批准和离标使用的看法。Hydroxychloroquine和Ivermectin比Molnupiravir和Remdesivir的讨论更多，时间趋势分析和内容分析揭示了人们对每种药物立场的可能理由。

    

    理解公众关于未经证实治疗方法的紧急使用的讨论对于监测安全使用和打击错误信息至关重要。我们开发了一种自然语言处理的流程，以理解Twitter上关于冠状病毒病2019（COVID-19）相关药物的公众看法和立场。这项回顾性研究包括了在COVID-19大流行期间，从2020年1月29日到2021年11月30日，关于四种药物的609,189条美国推文，这四种药物在公众中引起了重大关注：（1）羟氯喹和伊维菌素，具有案例证据的治疗方法；（2）莫米匹雷韦和瑞德西韦，FDA批准用于合格患者的治疗方法。利用时间趋势分析了解其受欢迎度趋势和相关事件。进行了内容和人口统计学分析，以探索人们对每种药物立场的潜在理由。

    Understanding public discourse on emergency use of unproven therapeutics is crucial for monitoring safe use and combating misinformation. We developed a natural language processing-based pipeline to comprehend public perceptions of and stances on coronavirus disease 2019 (COVID-19)-related drugs on Twitter over time. This retrospective study included 609,189 US-based tweets from January 29, 2020, to November 30, 2021, about four drugs that garnered significant public attention during the COVID-19 pandemic: (1) Hydroxychloroquine and Ivermectin, therapies with anecdotal evidence; and (2) Molnupiravir and Remdesivir, FDA-approved treatments for eligible patients. Time-trend analysis was employed to understand popularity trends and related events. Content and demographic analyses were conducted to explore potential rationales behind people's stances on each drug. Time-trend analysis indicated that Hydroxychloroquine and Ivermectin were discussed more than Molnupiravir and Remdesivir, part
    
[^183]: 高维点云数据的多样性散射变换

    The Manifold Scattering Transform for High-Dimensional Point Cloud Data. (arXiv:2206.10078v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10078](http://arxiv.org/abs/2206.10078)

    多样性散射变换是一种用于高维点云数据的深度特征提取器，在实现上采用了扩散映射理论，有效用于信号分类和流形分类任务。

    

    多样性散射变换是一种用于定义在黎曼流形上的数据的深度特征提取器。它是将类似于卷积神经网络的操作扩展到一般流形的最早的例子之一。这个模型的最初工作主要关注其理论稳定性和不变性特性，但除了在具有预定义网格的二维曲面的情况下提供其数值实现的方法外，还没有提供其数值实现的方法。在这项工作中，我们提出了一种基于扩散映射理论的实用方案，用于将多样性散射变换应用于自然系统中产生的数据集，例如单细胞遗传学，其中数据是作为低维流形上的高维点云建模的。我们展示了我们的方法对于信号分类和流形分类任务的有效性。

    The manifold scattering transform is a deep feature extractor for data defined on a Riemannian manifold. It is one of the first examples of extending convolutional neural network-like operators to general manifolds. The initial work on this model focused primarily on its theoretical stability and invariance properties but did not provide methods for its numerical implementation except in the case of two-dimensional surfaces with predefined meshes. In this work, we present practical schemes, based on the theory of diffusion maps, for implementing the manifold scattering transform to datasets arising in naturalistic systems, such as single cell genetics, where the data is a high-dimensional point cloud modeled as lying on a low-dimensional manifold. We show that our methods are effective for signal classification and manifold classification tasks.
    
[^184]: SupMAE: 监督式掩膜自编码器是高效的视觉学习器

    SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners. (arXiv:2205.14540v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.14540](http://arxiv.org/abs/2205.14540)

    SupMAE是一种监督式掩膜自编码器，通过添加监督分类分支扩展了MAE，并从黄金标签中有效学习全局特征。通过实验证明，SupMAE在训练效率、特征的鲁棒性和迁移学习性能等方面都表现出了优势。

    

    最近，自监督的掩膜自编码器（MAE）因其出色的表示学习能力而受到了前所未有的关注。但是，预处理任务掩膜图像建模（MIM）只能重构缺失的局部图像块，缺乏对图像的整体理解。本文通过添加一个监督分类分支，将MAE扩展到完全监督的设置中，从而使MAE能够有效地从黄金标签中学习全局特征。所提出的监督式MAE（SupMAE）仅利用图像中可见的图像块子集进行分类，而不是使用所有图像块的标准监督预训练。通过实验证明，SupMAE在训练效率上更高，并且学习到更强大和可迁移的特征。具体而言，SupMAE在使用ViT-B / 16模型在ImageNet上评估时，只使用30％的计算资源即可达到与MAE相当的性能。SupMAE在ImageNet变体上的健壮性和迁移学习性能也非常好。

    Recently, self-supervised Masked Autoencoders (MAE) have attracted unprecedented attention for their impressive representation learning ability. However, the pretext task, Masked Image Modeling (MIM), reconstructs the missing local patches, lacking the global understanding of the image. This paper extends MAE to a fully supervised setting by adding a supervised classification branch, thereby enabling MAE to learn global features from golden labels effectively. The proposed Supervised MAE (SupMAE) only exploits a visible subset of image patches for classification, unlike the standard supervised pre-training where all image patches are used. Through experiments, we demonstrate that SupMAE is not only more training efficient but it also learns more robust and transferable features. Specifically, SupMAE achieves comparable performance with MAE using only 30% of compute when evaluated on ImageNet with the ViT-B/16 model. SupMAE's robustness on ImageNet variants and transfer learning perform
    
[^185]: 面向尺度无关的深度操作器网络的泛化界限

    Towards Size-Independent Generalization Bounds for Deep Operator Nets. (arXiv:2205.11359v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11359](http://arxiv.org/abs/2205.11359)

    本论文研究了深度操作器网络的泛化界限问题，在一类DeepONets中证明了它们的Rademacher复杂度的界限不会随网络宽度扩展而明确变化，并利用这个结果展示了如何选择Huber损失来获得不明确依赖于网络大小的泛化误差界限。

    

    在最近的时期，机器学习方法在分析物理系统方面取得了重要进展。在这个主题中特别活跃的领域是"物理信息机器学习"，它专注于使用神经网络来数值求解微分方程。在这项工作中，我们旨在推进在训练DeepONets时测量样本外误差的理论 - 这是解决PDE系统最通用的方法之一。首先，针对一类DeepONets，我们证明了它们的Rademacher复杂度有一个界限，该界限不会明确地随着涉及的网络宽度扩展。其次，我们利用这一结果来展示如何选择Huber损失，使得对于这些DeepONet类，能够获得不明确依赖于网络大小的泛化误差界限。我们指出，我们的理论结果适用于任何目标是由DeepONets求解的PDE。

    In recent times machine learning methods have made significant advances in becoming a useful tool for analyzing physical systems. A particularly active area in this theme has been "physics-informed machine learning" which focuses on using neural nets for numerically solving differential equations. In this work, we aim to advance the theory of measuring out-of-sample error while training DeepONets -- which is among the most versatile ways to solve PDE systems in one-shot.  Firstly, for a class of DeepONets, we prove a bound on their Rademacher complexity which does not explicitly scale with the width of the nets involved. Secondly, we use this to show how the Huber loss can be chosen so that for these DeepONet classes generalization error bounds can be obtained that have no explicit dependence on the size of the nets. We note that our theoretical results apply to any PDE being targeted to be solved by DeepONets.
    
[^186]: 自监督异常检测：综述与展望

    Self-Supervised Anomaly Detection: A Survey and Outlook. (arXiv:2205.05173v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.05173](http://arxiv.org/abs/2205.05173)

    自监督学习的出现引发了新型异常检测算法的发展，其表现优于现有的最先进方法。本文全面综述了当前自监督异常检测方法的技术细节，并讨论了它们的优势和缺点，同时比较了这些模型与其他自监督异常检测模型以及最先进的异常检测模型的性能。

    

    异常检测在网络安全、金融和医疗等各个领域中起到了至关重要的作用，通过识别偏离正常行为的模式或事件。近年来，深度学习模型的显著增长使得在异常检测领域取得了重大进展。值得注意的是，自监督学习的出现引发了新型异常检测算法的发展，其表现优于现有的最先进方法。本文旨在全面综述当前自监督异常检测方法的技术细节，并讨论它们的优势和缺点。我们还比较了这些模型与其他自监督异常检测模型以及最先进的异常检测模型的性能。最后，本文对自监督异常检测的未来方向进行了讨论，包括开发更加有效和高效的算法等等。

    Anomaly detection (AD) plays a crucial role in various domains, including cybersecurity, finance, and healthcare, by identifying patterns or events that deviate from normal behaviour. In recent years, significant progress has been made in this field due to the remarkable growth of deep learning models. Notably, the advent of self-supervised learning has sparked the development of novel AD algorithms that outperform the existing state-of-the-art approaches by a considerable margin. This paper aims to provide a comprehensive review of the current methodologies in self-supervised anomaly detection. We present technical details of the standard methods and discuss their strengths and drawbacks. We also compare the performance of these models against each other and other state-of-the-art anomaly detection models. Finally, the paper concludes with a discussion of future directions for self-supervised anomaly detection, including the development of more effective and efficient algorithms and t
    
[^187]: 在张量主成分分析和相关问题中的统计计算权衡：通过通信复杂度

    Statistical-Computational Trade-offs in Tensor PCA and Related Problems via Communication Complexity. (arXiv:2204.07526v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2204.07526](http://arxiv.org/abs/2204.07526)

    本文通过通信复杂度推导出了对于内存受限算法在张量主成分分析中的计算下界，并且指定了解决该问题的算法必须在数据样本经过次数、样本大小和所需内存之间进行权衡。这些下界暗示了许多常用算法在样本大小不够大时需要更多的迭代次数。

    

    张量主成分分析是Montanari和Richard引入的一种风格化统计推断问题，用于研究从高阶矩张量中估计未知参数的计算难度。与其矩阵对应问题不同，张量主成分分析展现了一种统计计算差距，即在样本大小范围内，问题在信息理论上可解，但被认为在计算上较困难。本文利用通信复杂度推导出了内存受限算法在张量主成分分析中的计算下界。这些下界指定了成功解决张量主成分分析的任何算法在数据样本经过次数、样本大小和所需内存之间的权衡。尽管下界不能排除多项式时间算法，但它们意味着许多常用的算法，如梯度下降和幂迭代方法，在样本大小不够大时必须有更高的迭代次数。类似的下界还可以使用通信复杂度获得。

    Tensor PCA is a stylized statistical inference problem introduced by Montanari and Richard to study the computational difficulty of estimating an unknown parameter from higher-order moment tensors. Unlike its matrix counterpart, Tensor PCA exhibits a statistical-computational gap, i.e., a sample size regime where the problem is information-theoretically solvable but conjectured to be computationally hard. This paper derives computational lower bounds on the run-time of memory bounded algorithms for Tensor PCA using communication complexity. These lower bounds specify a trade-off among the number of passes through the data sample, the sample size, and the memory required by any algorithm that successfully solves Tensor PCA. While the lower bounds do not rule out polynomial-time algorithms, they do imply that many commonly-used algorithms, such as gradient descent and power method, must have a higher iteration count when the sample size is not large enough. Similar lower bounds are obtai
    
[^188]: Concordance指数的分解: 对生存预测模型深入理解的度量方法

    The Concordance Index decomposition: A measure for a deeper understanding of survival prediction models. (arXiv:2203.00144v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.00144](http://arxiv.org/abs/2203.00144)

    本文提出了一种将Concordance指数分解成两个部分的方法，用于评估生存预测模型的性能。该分解方法可以进行更细粒度的分析，揭示不同预测方法之间的优劣。实验证明，深度学习模型更好地利用了观测事件。

    

    Concordance指数（C-index）是生存分析中常用的评估预测模型性能的指标。本文提出了一种将C-index分解为两个数量的加权调和平均的方法：一个用于比较观测事件与其他观测事件的排序，另一个用于比较观测事件与被剪辑的情况的排序。这种分解方法可以对不同生存预测方法之间的优劣进行更细粒度的分析。通过与经典模型和最先进方法的基准比较，以及本文提出的新的变分生成神经网络方法（SurVED），展示了该分解方法的实用性。使用四个公开可用的具有不同剪辑水平的数据集评估模型的性能。通过C-index分解和合成剪辑，分析结果显示深度学习模型更好地利用了观测事件。

    The Concordance Index (C-index) is a commonly used metric in Survival Analysis for evaluating the performance of a prediction model. In this paper, we propose a decomposition of the C-index into a weighted harmonic mean of two quantities: one for ranking observed events versus other observed events, and the other for ranking observed events versus censored cases. This decomposition enables a finer-grained analysis of the relative strengths and weaknesses between different survival prediction methods. The usefulness of this decomposition is demonstrated through benchmark comparisons against classical models and state-of-the-art methods, together with the new variational generative neural-network-based method (SurVED) proposed in this paper. The performance of the models is assessed using four publicly available datasets with varying levels of censoring. Using the C-index decomposition and synthetic censoring, the analysis shows that deep learning models utilize the observed events more 
    
[^189]: 高维推断与模拟马尔可夫随机场的FDR控制

    High-dimensional Inference and FDR Control for Simulated Markov Random Fields. (arXiv:2202.05612v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.05612](http://arxiv.org/abs/2202.05612)

    本文提出了一种在高维背景下进行模拟马尔可夫随机场统计推断的方法，实现了一致性，并构建了两种误发现率控制程序。

    

    在各种科学领域中，确定与响应变量相关的重要特征是一项基本任务。本文探讨高维背景下模拟马尔可夫随机场的统计推断。我们提出了一种基于马尔可夫链蒙特卡罗极大似然估计（MCMC-MLE）与弹性网正则化的方法。在MCMC方法的温和条件下，我们的罚款MCMC-MLE方法实现了$\ell_{1}$一致性。我们提出了一个去相关的得分检验，确定其渐近正态性和一步估计量的渐近正态性，以及相应的置信区间。此外，我们基于p值和e值的渐近行为构建了两种误发现率控制程序。全面的数值模拟验证了所提方法的理论有效性。

    Identifying important features linked to a response variable is a fundamental task in various scientific domains. This article explores statistical inference for simulated Markov random fields in high-dimensional settings. We introduce a methodology based on Markov Chain Monte Carlo Maximum Likelihood Estimation (MCMC-MLE) with Elastic-net regularization. Under mild conditions on the MCMC method, our penalized MCMC-MLE method achieves $\ell_{1}$-consistency. We propose a decorrelated score test, establishing both its asymptotic normality and that of a one-step estimator, along with the associated confidence interval. Furthermore, we construct two false discovery rate control procedures via the asymptotic behaviors for both p-values and e-values. Comprehensive numerical simulations confirm the theoretical validity of the proposed methods.
    
[^190]: Thundernna: 一种白盒对抗攻击

    Thundernna: a white box adversarial attack. (arXiv:2111.12305v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.12305](http://arxiv.org/abs/2111.12305)

    本研究提出了一种名为Thundernna的白盒对抗攻击方法，通过开发一种一阶优化方法，该方法在攻击神经网络时具有更高的成功率和更快的速度。

    

    存在的研究证明，使用朴素的基于梯度的优化方法训练的神经网络容易受到对抗攻击，只要在普通输入上加入少量恶意信息就足以使神经网络出错。与此同时，对神经网络进行攻击是提高其鲁棒性的关键。针对对抗性样本的训练可以使神经网络抵抗某些类型的对抗攻击。同时，对神经网络进行对抗攻击也可以揭示一些特征，如之前的工作所讨论的复杂的高维非线性函数。在这个项目中，我们开发了一种一阶方法来攻击神经网络。与其他一阶攻击相比，我们的方法具有更高的成功率。此外，它比二阶攻击和多步一阶攻击快得多。

    The existing work shows that the neural network trained by naive gradient-based optimization method is prone to adversarial attacks, adds small malicious on the ordinary input is enough to make the neural network wrong. At the same time, the attack against a neural network is the key to improving its robustness. The training against adversarial examples can make neural networks resist some kinds of adversarial attacks. At the same time, the adversarial attack against a neural network can also reveal some characteristics of the neural network, a complex high-dimensional non-linear function, as discussed in previous work.  In This project, we develop a first-order method to attack the neural network. Compare with other first-order attacks, our method has a much higher success rate. Furthermore, it is much faster than second-order attacks and multi-steps first-order attacks.
    
[^191]: 基于梯度时差学习（GTD）的新版本研究

    New Versions of Gradient Temporal Difference Learning. (arXiv:2109.04033v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.04033](http://arxiv.org/abs/2109.04033)

    本文提出了基于梯度时差学习(GTD)的新版本，通过凸凹鞍点解释统一了所有GTD算法，并提供了简单的稳定性分析。还通过数值比较分析对这些方法进行了评估。

    

    Sutton、Szepesvári和Maei提出了与线性函数逼近和离策略训练兼容的第一个梯度时差（GTD）学习算法。本文的目标是（a）提出一些GTD的变种，并进行广泛的比较分析，以及（b）建立GTD的新理论分析框架。这些变种基于GTD的凸凹鞍点解释，将所有GTD统一到一个框架中，并基于最近对原始-对偶梯度动力学的结果提供简单的稳定性分析。最后，给出了数值比较分析以评估这些方法。

    Sutton, Szepesv\'{a}ri and Maei introduced the first gradient temporal-difference (GTD) learning algorithms compatible with both linear function approximation and off-policy training. The goal of this paper is (a) to propose some variants of GTDs with extensive comparative analysis and (b) to establish new theoretical analysis frameworks for the GTDs. These variants are based on convex-concave saddle-point interpretations of GTDs, which effectively unify all the GTDs into a single framework, and provide simple stability analysis based on recent results on primal-dual gradient dynamics. Finally, numerical comparative analysis is given to evaluate these approaches.
    
[^192]: 使用分布感知词嵌入的命名实体识别性能的实证研究。

    Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2109.01636](http://arxiv.org/abs/2109.01636)

    研究开发了一种分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息，实验表明将词的特异性融入NER方法可提高NER的性能。

    

    随着深度学习技术的快速发展，命名实体识别（NER）在信息提取任务中变得越来越重要。NER任务面临的最大困难是即使在NE类型和文档不熟悉的情况下仍然需要保持可检测性。意识到特定性信息可能包含单词的潜在含义并生成词嵌入的语义相关特征，我们开发了一个分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息。结果表明，如果将词的特异性融入现有的NER方法中，NER的性能将得到提高。

    With the fast development of Deep Learning techniques, Named Entity Recognition (NER) is becoming more and more important in the information extraction task. The greatest difficulty that the NER task faces is to keep the detectability even when types of NE and documents are unfamiliar. Realizing that the specificity information may contain potential meanings of a word and generate semantic-related features for word embedding, we develop a distribution-aware word embedding and implement three different methods to make use of the distribution information in a NER framework. And the result shows that the performance of NER will be improved if the word specificity is incorporated into existing NER methods.
    
[^193]: DASVDD: 深度自编码支持向量数据描述符用于异常检测

    DASVDD: Deep Autoencoding Support Vector Data Descriptor for Anomaly Detection. (arXiv:2106.05410v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.05410](http://arxiv.org/abs/2106.05410)

    DASVDD是一种深度自编码支持向量数据描述符，用于半监督异常检测。它联合学习自编码器参数，并最小化潜在表示中超球体的体积，通过组合自编码器的重构误差和超球体中心的距离来计算异常分数，并解决了超球体坍塌问题。

    

    半监督异常检测旨在使用在正常数据上训练的模型来检测异常样本。随着深度学习的最新进展，研究人员设计了高效的深度异常检测方法。现有的方法通常使用神经网络将数据映射到更具信息的表示，并应用一个异常检测算法。在本文中，我们提出了一种方法DASVDD，它在最小化潜在表示中的超球体的体积的同时，联合学习自编码器的参数。我们提出了一个异常分数，它是自编码器的重构误差和潜在表示中到超球体中心的距离的组合。最小化这个异常分数有助于我们在训练过程中学习正常类的潜在分布。将重构误差包含在异常分数中确保DASVDD不会遭受常见的超球体坍塌问题。

    Semi-supervised anomaly detection aims to detect anomalies from normal samples using a model that is trained on normal data. With recent advancements in deep learning, researchers have designed efficient deep anomaly detection methods. Existing works commonly use neural networks to map the data into a more informative representation and then apply an anomaly detection algorithm. In this paper, we propose a method, DASVDD, that jointly learns the parameters of an autoencoder while minimizing the volume of an enclosing hyper-sphere on its latent representation. We propose an anomaly score which is a combination of autoencoder's reconstruction error and the distance from the center of the enclosing hypersphere in the latent representation. Minimizing this anomaly score aids us in learning the underlying distribution of the normal class during training. Including the reconstruction error in the anomaly score ensures that DASVDD does not suffer from the common hypersphere collapse issue sin
    
[^194]: MNL-带有背包的剧集挑选问题：一种近似最优算法

    MNL-Bandit with Knapsacks: a near-optimal algorithm. (arXiv:2106.01135v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.01135](http://arxiv.org/abs/2106.01135)

    这篇论文介绍了一种解决动态商品选择问题的算法，通过使用近似最优策略，可在未知需求情况下最大化总体预期收入。在大库存环境下，该算法能够接近最优解。

    

    我们考虑一种动态的商品选择问题，其中卖方拥有固定库存的N种可替代产品，并面临在T个时期内顺序到达的未知需求。在每个时期，卖方需要决定要向客户提供的产品组合（基数最多为K）。顾客的反应遵循具有参数v的未知多项式对数模型（MNL）。卖方的目标是在给定固定初始库存的情况下最大化总体预期收入。我们给出了一种策略，达到了$\tilde O\Big(K \sqrt{KN T}\Big(\sqrt{v_{\text{max}}} + \frac{1}{q_{\text{min}}}\text{OPT}\Big)\Big)$的遗憾值，在模型参数的一种温和假设下。特别地，我们的策略在高库存环境下达到了接近最优的$\tilde O(\sqrt{T})$遗憾值。我们的策略基于基于UCB的方法。

    We consider a dynamic assortment selection problem where a seller has a fixed inventory of $N$ substitutable products and faces an unknown demand that arrives sequentially over $T$ periods. In each period, the seller needs to decide on the assortment of products (of cardinality at most $K$) to offer to the customers. The customer's response follows an unknown multinomial logit model (MNL) with parameters $v$. The goal of the seller is to maximize the total expected revenue given the fixed initial inventory of $N$ products. We give a policy that achieves a regret of $\tilde O\Big(K \sqrt{KN T}\Big(\sqrt{v_{\text{max}}} + \frac{1}{q_{\text{min}}}\text{OPT}\Big)\Big)$, where $v_{\text{max}}\leq 1$ is the maximum utility for any product and $q_{\text{min}}$ the minimum inventory level, under a mild assumption on the model parameters. In particular, our policy achieves a near-optimal $\tilde O(\sqrt{T})$ regret in a large-inventory setting.  Our policy builds upon the UCB-based approach for
    
[^195]: Wavelet Networks: 从原始时间序列学习尺度平移等变性的学习网络

    Wavelet Networks: Scale-Translation Equivariant Learning From Raw Time-Series. (arXiv:2006.05259v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.05259](http://arxiv.org/abs/2006.05259)

    本文介绍了一种利用时间序列固有对称性构建的小波网络，其表现出嵌套的非线性小波样的时频变换，实验证明其在原始波形上优于传统的CNN。

    

    利用特定数据领域中固有的对称性构建等变性神经网络，可显著提高数据效率和泛化能力。然而，大多数现有研究集中在平面和体积数据中产生的对称性上，而把一个关键的数据源基本未开发：时间序列。本文通过利用时间序列的固有对称性来构建等变性神经网络，填补了这一空白。我们确认了两个核心对称性：尺度和平移，并构建了适用于时间序列学习的尺度平移等变性神经网络。有趣的是，我们发现尺度平移等变性映射与小波变换具有很强的相似性。受到这种相似性的启发，我们将我们的网络称为小波网络，并展示它们执行嵌套的非线性小波样的时频变换。实证结果表明，小波网络在原始波形上优于传统的CNN。

    Leveraging the symmetries inherent to specific data domains for the construction of equivariant neural networks has lead to remarkable improvements in terms of data efficiency and generalization. However, most existing research focuses on symmetries arising from planar and volumetric data, leaving a crucial data source largely underexplored: time-series. In this work, we fill this gap by leveraging the symmetries inherent to time-series for the construction of equivariant neural network. We identify two core symmetries: *scale and translation*, and construct scale-translation equivariant neural networks for time-series learning. Intriguingly, we find that scale-translation equivariant mappings share strong resemblance with the wavelet transform. Inspired by this resemblance, we term our networks Wavelet Networks, and show that they perform nested non-linear wavelet-like time-frequency transforms. Empirical results show that Wavelet Networks outperform conventional CNNs on raw waveforms
    

