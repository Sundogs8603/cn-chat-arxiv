# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On the Interplay of Subset Selection and Informed Graph Neural Networks.](http://arxiv.org/abs/2306.10066) | 该研究探讨了在QM9数据集中，采用基于领域知识的数据采样方法来选择有效的训练集，再与信息图神经网络相结合，以最大化分子多样性和相关性来提高性能。 |
| [^2] | [Revealing the structure of language model capabilities.](http://arxiv.org/abs/2306.10062) | 本文研究了大规模语言模型的能力结构，发现这些模型不是单一能力，而是由推理、理解和核心语言建模等三个明确定义的因素组成，并且这三个能力可以解释模型性能中的大部分方差。 |
| [^3] | [MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction.](http://arxiv.org/abs/2306.10060) | MUBen评估不同骨干和UQ模型组合对分子不确定性估计和属性预测的性能，以解决预训练模型微调中的过拟合校准问题。 |
| [^4] | [EM-Network: Oracle Guided Self-distillation for Sequence Learning.](http://arxiv.org/abs/2306.10058) | EM-Network是一种自我蒸馏方法，通过神谕指导能够有效利用目标信息进行监督序列到序列学习，并在语音识别和机器翻译等任务上取得了最先进的成果。 |
| [^5] | [Smoothing the Rough Edges: Evaluating Automatically Generated Multi-Lattice Transitions.](http://arxiv.org/abs/2306.10055) | 本论文描述和评估了使用变分自编码器自动创建过渡晶格单元的方法，以解决多晶格结构中不同拓扑的单元晶格之间平滑转换的问题。 |
| [^6] | [Interpolating Item and User Fairness in Recommendation Systems.](http://arxiv.org/abs/2306.10050) | 本文研究在推荐系统中平衡项目和用户公平性的框架，并通过低后悔的在线优化算法实现了维持收益同时实现公平推荐的目标。 |
| [^7] | [Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction.](http://arxiv.org/abs/2306.10045) | 本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。 |
| [^8] | [Unraveling the Interconnected Axes of Heterogeneity in Machine Learning for Democratic and Inclusive Advancements.](http://arxiv.org/abs/2306.10043) | 本论文探讨了机器学习应用中的异质性问题，从价值观、数据组成和资源基础设施三个角度入手并指出它们相互依存，需要一同考虑解决。同时，本文还揭示了现有机器学习研究存在的问题，如权力集中和依赖性增加。 |
| [^9] | [Unlocking Insights into Business Trajectories with Transformer-based Spatio-temporal Data Analysis.](http://arxiv.org/abs/2306.10034) | 本研究利用新闻文章数据，采用Transformer模型模拟商业轨迹，揭示商业趋势和行业表现。 |
| [^10] | [Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective.](http://arxiv.org/abs/2306.10033) | 该论文调查了跨越语音和语言处理学科的七个会议中，源代码可用性对于Interspeech会议要少于40%，并提出了建议以提高未来研究的可重复性。 |
| [^11] | [Pseudo session-based recommendation with hierarchical embedding and session attributes.](http://arxiv.org/abs/2306.10029) | 本文提出了一种新方法CoHHGN+，用于解决缺乏用户ID的电商网站数据的推荐问题。该方法使用了定义的伪会话以及包括价格、类别、性别和地区等用户信息，得到了较好的推荐结果。 |
| [^12] | [Graph Based Long-Term And Short-Term Interest Model for Click-Through Rate Prediction.](http://arxiv.org/abs/2306.10028) | 本论文提出了一种名为GLSM的基于图的长期和短期兴趣模型，可以很好地处理长期和短期用户兴趣数据的问题。 |
| [^13] | [Theoretical Analysis on the Efficiency of Interleaved Comparisons.](http://arxiv.org/abs/2306.10023) | 本研究对交错比较方法的效率进行了理论分析，发现当用户根据相关性离开排名时，交错比较比A/B测试更为有效。 |
| [^14] | [CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search.](http://arxiv.org/abs/2306.10008) | 该论文提出了一种两步法面部隐私保护方法CLIP2Protect，使用对抗性潜在搜索结合文本引导化妆，生成高质量面部图像，从而保护面部隐私。 |
| [^15] | [Evaluating Superhuman Models with Consistency Checks.](http://arxiv.org/abs/2306.09983) | 本文提出了一个用一致性检查评估超人模型的框架，可以发现决策制定中的逻辑不一致性，即使对于超人模型的决策正确性可能是不可能评估的情况。 |
| [^16] | [Correlation Clustering of Bird Sounds.](http://arxiv.org/abs/2306.09906) | 本文研究鸟声聚类问题，提出了一种通过学习训练集中记录对相关概率后，应用相关聚类于测试集进行聚类的方法，并比较了这种方法与测试集分类的准确性和相关性。同时，还探究了这种方法在应用于训练期间未听到的鸟类物种的记录以及分离鸟声和环境噪声方面的效果。 |
| [^17] | [Studying Generalization on Memory-Based Methods in Continual Learning.](http://arxiv.org/abs/2306.09890) | 本文研究了连续学习中基于记忆的方法的泛化性能，发现虽然这些方法可以帮助传统的内分布泛化，但它们可以通过学习虚假的特征和相关性来大大损害超出分布泛化，尤其是在线性分类器中。 |
| [^18] | [Generalizable One-shot Rope Manipulation with Parameter-Aware Policy.](http://arxiv.org/abs/2306.09872) | GenORM通过增加可变形绳索参数和使用各种可变形绳索的模拟训练操作策略，实现利用一次真实演示处理不同可形变绳索，从而节省演示时间和提高适用性。 |
| [^19] | [Multi-View Class Incremental Learning.](http://arxiv.org/abs/2306.09675) | 本文提出了一种名为多视角分类增量学习（MVCIL）的新模型，该模型使用随机化的表示学习技术进行特征提取，并提出正交融合子空间和选择性权重合并来解决增量学习中遗忘旧信息和学习新概念的挑战。实验结果表明该方法相比最新方法有效性更高。 |
| [^20] | [The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement.](http://arxiv.org/abs/2306.09633) | 谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。 |
| [^21] | [From Hypergraph Energy Functions to Hypergraph Neural Networks.](http://arxiv.org/abs/2306.09623) | 本文提出了一种新的基于超图能量函数的节点嵌入方法，可以通过双层优化实现节点分类任务，相比传统GNN模型有更好的表现。 |
| [^22] | [Towards Practical Federated Causal Structure Learning.](http://arxiv.org/abs/2306.09433) | 为了解决联邦学习条件下的因果结构学习难题，提出了一种基于联邦条件独立性检验的因果结构学习方案FedC2SL，无需收集原始数据且对数据变异具有更强的抵抗力。 |
| [^23] | [Multi-omics Prediction from High-content Cellular Imaging with Deep Learning.](http://arxiv.org/abs/2306.09391) | 本研究使用深度学习方法，从高内容细胞成像中直接预测细胞群体的多组学。实验结果表明，该方法能够在多种刺激条件下实现显著成果，为细胞组学领域提供了新的方法。 |
| [^24] | [Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation.](http://arxiv.org/abs/2306.09381) | STAR框架通过设计多种时空图来捕捉位置的动态时空效应，为人类移动模拟任务提供新的方法。 |
| [^25] | [Equitable Multi-task Learning.](http://arxiv.org/abs/2306.09373) | 该论文提出了一种名为EMTL的多任务优化方法，以实现公平的多任务学习。通过规范化不同任务的相对贡献，可以提高MTL的泛化性能，并利用方差正则化和高效的优化算法保证收敛。实验证明，该方法在合成和真实数据集上均表现出了更好的性能。 |
| [^26] | [OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection.](http://arxiv.org/abs/2306.09301) | OpenOOD v1.5 是对前身的重大改进，将OCC检测方法的评估能力扩展到大规模数据集，调查了全光谱OCC检测，引入了在线排行榜和易于使用的评估器等新功能，并提供了深入的分析和实验结果的见解。 |
| [^27] | [RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation.](http://arxiv.org/abs/2306.08947) | 本文提出了 RecFusion，一种特定针对1D和/或二进制设置的推荐模型方法，其利用了二项式扩散过程对二元用户-项目交互进行显式建模，并在核心推荐设置和最常见的数据集上接近复杂的VAE基线的表现。 |
| [^28] | [Scalable Resource Management for Dynamic MEC: An Unsupervised Link-Output Graph Neural Network Approach.](http://arxiv.org/abs/2306.08938) | 本文提出了一种面向动态MEC的资源管理方法，使用了无监督的链路输出图神经网络来对任意数量的边缘节点进行灵活的资源分配优化，具有极低的算法推理延迟。 |
| [^29] | [PLAN: Variance-Aware Private Mean Estimation.](http://arxiv.org/abs/2306.08745) | 本文提出了“隐私限制适应噪声”（PLAN），是一组差分隐私算法，用于在输入的数据集结构中进行更好的均值估计。PLAN将噪声的形状量身定制为数据的形状，不同于以往的均值估计算法，而且可以在一些集中分布的情况下，通过利用标准差的偏斜来获得接近零平均均方误差（MSE）的估计。 |
| [^30] | [Learning to Rank when Grades Matter.](http://arxiv.org/abs/2306.08650) | 本论文提出了一种学习排序和成绩预测的多目标函数，可以在真实应用中更好地平衡排序和成绩，优于现有方法。 |
| [^31] | [Language to Rewards for Robotic Skill Synthesis.](http://arxiv.org/abs/2306.08647) | 该研究介绍了一种将大型语言模型用于定义奖励参数，并通过奖励函数进行优化，从而使机器人可以执行各种自然语言指令指定的任务的新方法。 |
| [^32] | [Domain-Aware Few-Shot Learning for Optical Coherence Tomography Noise Reduction.](http://arxiv.org/abs/2306.08102) | 本文提出了一种少样本监督学习框架，用于光学相干断层扫描噪音降低，并成功推广到不同成像领域。 |
| [^33] | [VISION Datasets: A Benchmark for Vision-based InduStrial InspectiON.](http://arxiv.org/abs/2306.07890) | 本论文介绍了VISION数据集，这是一个多元化集合，包含14个不同工业领域检测数据集，提供了注释墨水和实例分割注释，并适用于多种检测方法。VISION数据集可以应对现实世界中工业领域检测中的挑战，并有望促进基于视觉的工业检测的发展。 |
| [^34] | [Compositionally Equivariant Representation Learning.](http://arxiv.org/abs/2306.07783) | 本文研究了利用组合性来学习更具可解释性和泛化性的医学图像分割表示，提出了一种将组合等变性质纳入表示学习中的方法，并在各种医学图像分割任务上展示了其对泛化性能的提升。 |
| [^35] | [A Protocol for Continual Explanation of SHAP.](http://arxiv.org/abs/2306.07218) | 本文研究了在持续学习中SHAP值解释的稳定性问题，提出了一种评估协议，并指出随机循环模型是更有效的备选循环方法。 |
| [^36] | [Correlated Time Series Self-Supervised Representation Learning via Spatiotemporal Bootstrapping.](http://arxiv.org/abs/2306.06994) | 该论文提出了一种基于时空自举的时间步级表示学习框架，能为相关时间序列分析提供高效降维表示，并通过在PeMS-BAY数据集上的测试取得了较好的效果。 |
| [^37] | [Local-to-global Perspectives on Graph Neural Networks.](http://arxiv.org/abs/2306.06547) | 本文提出了局部到全局的图神经网络模型，包括不变图网络、局部信息传递神经网络和全局图变换器，并研究其收敛性质和在图粗化中的应用。 |
| [^38] | [Defining and Explorting the Intelligence Space.](http://arxiv.org/abs/2306.06499) | 本文通过引入广泛视角来界定智能并建立了三级嵌套结构及其基础的广泛空间。利用这些定义初步探索了奇点、生成AI、伦理和知识产权等话题。 |
| [^39] | [Understanding the Effect of the Long Tail on Neural Network Compression.](http://arxiv.org/abs/2306.06238) | 本文研究了在神经网络压缩中如何保持与原始网络的语义相同，在长尾现象中探讨了压缩提高推广性能的记忆要素。 |
| [^40] | [Deep Learning Method for Object Tracking, Velocity Estimation and Projection of Sensor Data over Time.](http://arxiv.org/abs/2306.06126) | 本文提出了一种利用Transformer机制的新型循环神经网络单元，实现了利用传感器记录中的时空相关性来实现目标跟踪和速度估计，并将记忆状态进行投影。 |
| [^41] | [SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking.](http://arxiv.org/abs/2306.05426) | 本文提出了一种称为SequenceMatch的带有回溯的自回归模型的模仿学习框架，该框架通过最小化自回归模型生成序列和数据集序列之间的各种分歧来减少在自回归生成过程中的复合误差，并允许引入回溯动作。 |
| [^42] | [Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts.](http://arxiv.org/abs/2306.04723) | 本文提出了衡量文本内部维度的方法，应用于鲁棒性AI生成文本的检测，展示了人类文本与AI生成文本在内部维度上的差异。 |
| [^43] | [Uncovering solutions from data corrupted by systematic errors: A physics-constrained convolutional neural network approach.](http://arxiv.org/abs/2306.04600) | 本文提出了一种基于物理约束卷积神经网络的方法，可以从受到系统误差影响的数据中还原出潜在物理系统的时空解。 |
| [^44] | [Matched Pair Calibration for Ranking Fairness.](http://arxiv.org/abs/2306.03775) | 本文提出了一个针对排名系统公平性的测试方法——匹配对校准，通过构建混淆差异最小的匹配物品对来计算适当的排名误差测量结果，可以直接说明子组水平曝光的不公平性。实验证明该方法在检测排名偏差方面具有很好的效果。 |
| [^45] | [Linear Distance Metric Learning.](http://arxiv.org/abs/2306.03173) | 本文介绍了一种线性距离度量学习方法，可以有效地从一个欧几里得度量空间中的数据学习出另一个欧几里得度量空间中的线性映射，即使数据中存在噪声，也可以以任意精度学习真实的线性距离度量，并提供相应的样本复杂度上界。此外，提供了一种有效的低秩模型截断方法，可以保证模型的准确性和精度。 |
| [^46] | [Comparative Study on Semi-supervised Learning Applied for Anomaly Detection in Hydraulic Condition Monitoring System.](http://arxiv.org/abs/2306.02709) | 本研究比较了不同类型的半监督学习方法在液压状态监测系统中用于异常检测。深度学习模型表现最好，而集成模型可以进一步提高检测性能。 |
| [^47] | [SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts.](http://arxiv.org/abs/2306.02207) | 本文探索了一个名为SpeechGen的统一框架，通过提示调节，解锁了语音语言模型的生成能力，成功地实现了直接适应连续语音到离散标记的任务，使得语音生成成为可能。 |
| [^48] | [Inconsistent Matters: A Knowledge-guided Dual-consistency Network for Multi-modal Rumor Detection.](http://arxiv.org/abs/2306.02137) | 本文提出了一种基于知识引导的双一致性网络，用于检测带有多媒体内容的谣言。该网络使用两个一致性检测子网络同时捕获跨模态级别和内容-知识级别的不一致性，并在不同的缺失视觉模态条件下实现了稳健的多模态表示学习。 |
| [^49] | [The feasibility of artificial consciousness through the lens of neuroscience.](http://arxiv.org/abs/2306.00915) | 从神经科学的角度来看，目前大型语言模型难以具备哺乳动物意识感知相关的丘脑皮层系统的关键特征，缺乏周围世界的具体嵌入式信息，且当前的人工智能无法做到存在的依赖于其行为，这意味着人工意识的可行性存在瓶颈。 |
| [^50] | [Introduction to Medical Imaging Informatics.](http://arxiv.org/abs/2306.00421) | 本文介绍了医学成像信息学的基本概念和最新进展，包括图像处理、特征工程、机器学习、计算机视觉和深度学习，以及如何将它们应用于疾病检测、诊断和预后预测模型的开发。这对于理解信息学在医学中的作用以及其对患者护理的潜在影响具有重要意义。 |
| [^51] | [Doubly Robust Self-Training.](http://arxiv.org/abs/2306.00265) | 本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。 |
| [^52] | [Beam Tree Recursive Cells.](http://arxiv.org/abs/2305.19999) | 本论文提出了一种支持反向传播的递归神经网络框架——束搜索递归单元（BT-Cell），用于扩展递归神经网络，实现对潜在结构的感知；此外，我们提出了一种放松束搜索中硬前k算子的方法，以实现更好的梯度信号传递。在评估中发现，BT-Cell在合成和实际数据的多个具有结构敏感性的任务中表现优异。 |
| [^53] | [MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images.](http://arxiv.org/abs/2305.19956) | 本文提出了一种基于深度学习的微型超声图像前列腺分割方法，利用多尺度注释引导的Transformer UNet模型和注释引导的二分类交叉熵损失解决低分辨率和界限不清的挑战，该方法更加关注难以分割的区域。 |
| [^54] | [Image Registration of In Vivo Micro-Ultrasound and Ex Vivo Pseudo-Whole Mount Histopathology Images of the Prostate: A Proof-of-Concept Study.](http://arxiv.org/abs/2305.19939) | 本文提供了一种半自动化的流程，用于将体内微-US图像与离体全切片组织病理学图像配准，以帮助泌尿外科医生提高小前列腺癌的检测率。 |
| [^55] | [Representation-Driven Reinforcement Learning.](http://arxiv.org/abs/2305.19922) | 该论文提出了一个表示驱动的强化学习框架，通过在线性特征空间中嵌入策略网络，重新框定探索-利用问题为表示-利用问题，以实现最佳的探索。该框架通过应用进化和策略梯度法取得了显著的性能提升。 |
| [^56] | [A rule-general abductive learning by rough sets.](http://arxiv.org/abs/2305.19718) | 本文提出了一种基于粗糙集理论的规则通用逆推学习方法，通过转化目标概念和子概念为信息表，以更低的成本解决领域知识获取和规则的修正、减少、生成问题。 |
| [^57] | [Federated Learning on Heterogeneous Data via Adaptive Self-Distillation.](http://arxiv.org/abs/2305.19600) | 本文提出一种基于自适应自蒸馏的新型正则化技术来训练客户端模型，该正则化方案基于客户端本地模型预测和全局模型的相似性以及客户端的标签分布来自适应地调整客户端的训练数据。实验结果表明，该方法在各种基准数据集上优于目前流行的联邦学习方法。 |
| [^58] | [Towards Omni-generalizable Neural Methods for Vehicle Routing Problems.](http://arxiv.org/abs/2305.19587) | 提出了一个通用meta-learning框架，使得模型可以在推理过程中快速适应新任务。通过在多个合成和基准数据集上的实验表明，该方法可以有效地解决大小和分布变化的车辆路径问题（VRP）。 |
| [^59] | [Learning to Learn from APIs: Black-Box Data-Free Meta-Learning.](http://arxiv.org/abs/2305.18413) | 该论文提出了一个BiDf-MKD框架，可以从一组API库中无需访问训练数据，直接进行元学习；能够在更广泛的黑盒API上进行元学习，提高了元模型的泛化性能和应用范围。 |
| [^60] | [On Optimal Regularization Parameters via Bilevel Learning.](http://arxiv.org/abs/2305.18394) | 本文提出了一个利用双层学习确定正则化参数的方法，并提出了一个新的表征正确参数的条件。 |
| [^61] | [Distilling BlackBox to Interpretable models for Efficient Transfer Learning.](http://arxiv.org/abs/2305.17303) | 本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。 |
| [^62] | [Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration.](http://arxiv.org/abs/2305.16173) | 本文提出了一种基于循环矩阵和格拉姆迭代的方法，用于高效估计卷积神经网络中的Lipschitz常数上界。该方法精确、快速、可微分，并展现了超线性收敛。在实验上表现出较高的精度、计算成本和可扩展性，在利普希茨正则化方面也取得了具有竞争力的结果。 |
| [^63] | [Sound Design Strategies for Latent Audio Space Explorations Using Deep Learning Architectures.](http://arxiv.org/abs/2305.15571) | 本研究探索了利用深度学习架构直接应用于原始音频数据来探索潜在音频空间的新方法。 |
| [^64] | [PromptNER: Prompting For Named Entity Recognition.](http://arxiv.org/abs/2305.15444) | PromptNER是一种基于提示的命名实体识别算法，利用LLM生成潜在实体列表并提供解释，在少样本NER和跨领域NER方面实现了最先进性能。 |
| [^65] | [PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines.](http://arxiv.org/abs/2305.15424) | 该论文提出了一种利用深度学习对犬的ECG信号进行自动分类的方法，通过使用随机数据增强策略和连续小波变换，分类精度得到了提高。 |
| [^66] | [Deep Learning and Ethics.](http://arxiv.org/abs/2305.15239) | 本文探讨了人工智能存在的算法偏见、缺乏可解释性、数据隐私侵犯、军事化、欺诈和环境问题，旨在促进哲学、政治科学和社会科学领域关于这些问题的探讨。 |
| [^67] | [Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding.](http://arxiv.org/abs/2305.14449) | 一种协同过滤新方法用于稳健对话理解，在历史用户-实体交互的基础上，利用多跳客户亲和力丰富每个用户的索引，并使用有限内存BFGS算法调整每个索引的权重，实验结果显示其明显优于最先进的个性化查询重写方法。 |
| [^68] | [Regularization Through Simultaneous Learning: A Case Study for Hop Classification.](http://arxiv.org/abs/2305.13447) | 本文提出了一种新颖的同时学习正则化方法，将迁移学习和多任务学习原理应用于啤酒生产中的啤酒花品种分类，利用辅助数据集的协同作用增强获取高度相关特征的能力，成功实现了两个数据集的同时分类。 |
| [^69] | [Error-Tolerant Exact Query Learning of Finite Set Partitions with Same-Cluster Oracle.](http://arxiv.org/abs/2305.13402) | 本文提出了一个新问题：如何通过同簇预言机在存在有限对抗错误时积极学习完全恢复划分。我们建立了解析框架并证明了最坏情况下查询复杂度的上下界，并研究了适应性和查询复杂度之间的关系。 |
| [^70] | [On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation.](http://arxiv.org/abs/2305.11283) | 本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。 |
| [^71] | [The Principle of Uncertain Maximum Entropy.](http://arxiv.org/abs/2305.09868) | 介绍了不确定最大熵原理，该原理可以处理模型元素不可观测的情况，并优于特定条件下的最大熵方法。同时将黑匣子机器学习模型的输出用作不确定机器熵框架的输入，性能得到了提高。 |
| [^72] | [Causal Analysis for Robust Interpretability of Neural Networks.](http://arxiv.org/abs/2305.08950) | 本文提出了一种基于因果分析的鲁棒干预方法，用于解释神经网络的决策，避免噪音的干扰。 |
| [^73] | [Quadratic Functional Encryption for Secure Training in Vertical Federated Learning.](http://arxiv.org/abs/2305.08358) | 本文提出了基于二次功能加密的纵向联邦学习安全训练方法，可以有效避免信息泄露问题。 |
| [^74] | [TIPS: Topologically Important Path Sampling for Anytime Neural Networks.](http://arxiv.org/abs/2305.08021) | TIPS是一种自动设计AnytimeNNs框架，通过识别贡献最大的路径来提高收敛速度和测试准确率，比现有方法提高了2%-6.6%的准确率，在准确率-FLOPs之间取得了最佳平衡。 |
| [^75] | [Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning.](http://arxiv.org/abs/2305.08014) | 本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。 |
| [^76] | [Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning.](http://arxiv.org/abs/2305.05119) | 本文提出了一种基于深度强化学习和自注意模型的端到端学习框架，通过双重注意力网络来准确而简洁地表示操作和机器之间错综复杂的关系，以协同地确定FJSP的优先级分配规则。 |
| [^77] | [Large-Scale Study of Temporal Shift in Health Insurance Claims.](http://arxiv.org/abs/2305.05087) | 本文是对健康保险索赔数据中历史偏差进行的大规模研究，通过构建算法测试时间偏移，进行回顾性扫描以寻找时间偏移，并创建1010个任务来评估242项医疗保健结果。研究发现有9.7%的任务显示出人群水平的时间偏移，93%显示出已发现的子人群内的时间偏移。 |
| [^78] | [LSGNN: Towards General Graph Neural Network in Node Classification by Local Similarity.](http://arxiv.org/abs/2305.04225) | 本文提出使用局部相似性（LocalSim）学习节点级加权融合的即插即用模块，提取更具信息性的多跳信息，并对其有效性进行了理论分析。在真实基准数据集上的广泛评估表明，我们提出的LSGNN方法在同质性和异质性图上均能提供可比或优于最先进的性能。 |
| [^79] | [Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation.](http://arxiv.org/abs/2305.03942) | 论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。 |
| [^80] | [Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era.](http://arxiv.org/abs/2305.02555) | ChatGPT和Bard等AI工具需要持续大量且高质量的数据来提高其性能，但现行的版权法则限制了它们对各种数据的获取。与数据提供者分享收益将有助于将AI工具与大多数版权数据拥有者之间的敌对关系转变为合作关系，使AI生态系统更健康。 |
| [^81] | [Streaming PCA for Markovian Data.](http://arxiv.org/abs/2305.02456) | 本文提出了一种面向马尔可夫数据采样的流式PCA算法，并获得了该算法在整个数据集上的第一个尖锐率，提高了算法的效率。同时，本文提出的自适应方案在模拟和真实数据示例中表现良好。 |
| [^82] | [Stream Efficient Learning.](http://arxiv.org/abs/2305.02217) | 本文介绍了“流高效学习”的概念，该概念旨在解决从数据流中机器学习的效率问题，其泛化性能不仅取决于接收到了多少数据，而且还取决于有多少数据能够及时有效地被利用，加上资源和速度的考虑。 |
| [^83] | [Pseudo-Hamiltonian neural networks for learning partial differential equations.](http://arxiv.org/abs/2304.14374) | 本文介绍了一种新方法伪哈密顿神经网络(PHNN)，可以用于学习偏微分方程。相比基线模型，PHNN表现更为优越，模型可应用于去除或改变外力情况并可分别得到三个不同物理解释的部分。 |
| [^84] | [Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques.](http://arxiv.org/abs/2304.12583) | 本文综述了非平稳环境下动态系统实时安全评估的方法和技术，包括在线主动学习、在线半监督学习、在线迁移学习和在线异常检测，并讨论了未来的研究方向。 |
| [^85] | [Revisiting k-NN for Pre-trained Language Models.](http://arxiv.org/abs/2304.09058) | 本研究提出一种新方法，结合k-NN和预训练语言模型（PLMs）能够提高自然语言处理（NLP）的性能，并在多个基准数据集上得到验证。 |
| [^86] | [Cross-Entropy Loss Functions: Theoretical Analysis and Applications.](http://arxiv.org/abs/2304.07288) | 本文对交叉熵、广义交叉熵、均方误差等一大类损失函数进行了理论分析，并提出了具有优势的双交叉熵损失函数，特别适用于存在标签噪声或类别不平衡的情况。 |
| [^87] | [Inhomogeneous graph trend filtering via a l2,0 cardinality penalty.](http://arxiv.org/abs/2304.05223) | 本文提出了一种基于L2，0基数惩罚的图趋势过滤（GTF）模型，可同时进行k-means聚类和基于图的最小割，以估计在节点之间具有不均匀平滑水平的分段平滑图信号，并在降噪、支持恢复和半监督分类任务上表现更好，比现有方法更高效地处理大型数据集。 |
| [^88] | [OpenAGI: When LLM Meets Domain Experts.](http://arxiv.org/abs/2304.04370) | 基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。 |
| [^89] | [Maximal Ordinal Two-Factorizations.](http://arxiv.org/abs/2304.03338) | 本文研究了最大序数二次因子分解问题，证明了其判定是否存在是一个NP完全问题，并提供了用于计算最大因子分解的算法Ord2Factor。 |
| [^90] | [How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis.](http://arxiv.org/abs/2304.01545) | 本研究探讨了使用3D卷积神经网络（3D-CNN）对时空数据进行风速预测的精度，并发现使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。 |
| [^91] | [From G\"odel's Incompleteness Theorem to the completeness of bot religions (Extended abstract).](http://arxiv.org/abs/2303.14338) | 本文研究了从哥德尔不完备定理到机器人宗教的完备性的逻辑过程，提出了任何信仰系统可以被形式化为逻辑理论，并且不完备定理意味着存在真实但无法证明的陈述，可以用来定义出与现有信仰和传统一致的新宗教实践。 |
| [^92] | [Adversarial Robustness of Learning-based Static Malware Classifiers.](http://arxiv.org/abs/2303.13372) | 本文提出了一种通用的对抗性补丁（UAP）攻击方法，只需附加一个相对较小的字节级补丁即可绕过MalConv分类器的检测，可以将恶意软件文件的检测率降低80％。同时，作者提出了窗口消除处理作为应对此种攻击的一种方法。 |
| [^93] | [Graph Kalman Filters.](http://arxiv.org/abs/2303.12021) | 本文首次将卡尔曼和扩展卡尔曼滤波器推广到图形上，使得它可以适用于输出是向量或标量的情况，并且可以学习未知的状态转移和读取函数。 |
| [^94] | [Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning.](http://arxiv.org/abs/2303.11183) | 不受体系结构、数据集和模型规模限制的无数据元学习框架PURER，通过ECI执行伪周期训练以适应新的任务，通过ICFIL对反演梯度进行校准来优化反演过程，并在各种任务中显著优于现有方法。 |
| [^95] | [Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting.](http://arxiv.org/abs/2303.08331) | 本论文提出了一种利用空间时间信息来提高视频超分辨率的新方法，采用高维卷积网络进行预测并应用时间注意机制以去除冗余信息并提高效率。 |
| [^96] | [What is the state of the art? Accounting for multiplicity in machine learning benchmark performance.](http://arxiv.org/abs/2303.07272) | 机器学习基准性能评估中，最先进的（SOTA）性能的估计值过于乐观，容易导致方法的忽视。本文提供了一个概率模型，用于校正多重性偏差并比较方法的性能。 |
| [^97] | [Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference.](http://arxiv.org/abs/2303.06182) | 本文提出了三种优化技术来缓解混合专家（MoE）模型在推理时的低效率，包括动态门控、专家缓冲和专家负载平衡。这些技术可以显著提高执行时间和减少内存使用。 |
| [^98] | [Pacos: Modeling Users' Interpretable and Context-Dependent Choices in Preference Reversals.](http://arxiv.org/abs/2303.05648) | Pacos是一个上下文依赖的偏好模型，可以处理偏好逆转问题，并提供用户的自适应权重、比较和显示位置等可解释因素，有助于提供个性化服务。 |
| [^99] | [Scaling up GANs for Text-to-Image Synthesis.](http://arxiv.org/abs/2303.05511) | 本研究介绍了GigaGAN，一种新型的GAN架构，远远超过之前使用的StyleGAN。GigaGAN具有三个重要优势：超快速的推理时间、高分辨率图像合成的能力和更精确的图像合成控制。这使GAN成为文本到图像合成的可行选项。 |
| [^100] | [Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning.](http://arxiv.org/abs/2303.05479) | 本文介绍了一种计算机辅助脱机强化学习方法Cal-QL，该方法能够从脱机数据中学习一个保守的值函数初始化，使得在在线微调时同时保障了快速和性能。 |
| [^101] | [Embodied Active Learning of Relational State Abstractions for Bilevel Planning.](http://arxiv.org/abs/2303.04912) | 该论文提出了一种基于身体知识的主动学习方法，通过在线交互与专家学习神经谓词解释、符号规划算子和任务特定的关系状态抽象，以提高机器人操作任务中的性能。 |
| [^102] | [An Inception-Residual-Based Architecture with Multi-Objective Loss for Detecting Respiratory Anomalies.](http://arxiv.org/abs/2303.04104) | 本文提出了一种深度学习系统，用于从呼吸声音记录中检测异常。该系统使用Gammatone和连续小波变换进行音频特征提取，并将Inception-Residual-based的骨干模型与multi-head attention和multi-objective loss相结合，通过线性组合的方法来平衡每个单独频谱图的贡献。 |
| [^103] | [Multi-Symmetry Ensembles: Improving Diversity and Generalization via Opposing Symmetries.](http://arxiv.org/abs/2303.02484) | 本研究提出了一种新的集成学习方法——多对称集合（MSE），它通过对称轴上假设的多样性来提高多样性和泛化能力，超越传统随机扰动的方法探索假设空间，并取得了良好效果。 |
| [^104] | [You Only Transfer What You Share: Intersection-Induced Graph Transfer Learning for Link Prediction.](http://arxiv.org/abs/2302.14189) | 本文提出了一种称为图交集-诱导图传递学习的方法，旨在解决稀疏图在链接预测中的问题。在此方法中，通过创建一个交集子图，在源图上训练模型并将知识传递到目标图上进行预测。 |
| [^105] | [Random Teachers are Good Teachers.](http://arxiv.org/abs/2302.12091) | 在自身蒸馏中，使用随机初始化的教师可以使用，可以提高模型并产生有趣的特性，但要注意表征是数据相关的。 |
| [^106] | [Diffusion Probabilistic Models for Structured Node Classification.](http://arxiv.org/abs/2302.10506) | 本文提出了一种新颖的DPM-SNC框架，通过反向扩散过程和流形约束的采样方法实现基于图结构的结构化节点分类，并设计了一种新的训练算法来应用DPMs，最大化一个新的变分下界。实验证明DPMs可以提高GNN的表达能力，提高节点分类的效果。 |
| [^107] | [Gaussian processes at the Helm(holtz): A more fluid model for ocean currents.](http://arxiv.org/abs/2302.10364) | 该论文提出了一种更符合已知电流物理特性的模型，在通过Helmholtz分解获得的向量场的发散和无旋分量上使用高斯过程来预测海流。证明了这种方法在模拟数据和真实浮标数据方面都比之前的方法更有效。 |
| [^108] | [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat.](http://arxiv.org/abs/2302.10289) | 本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。 |
| [^109] | [Self-supervised learning of Split Invariant Equivariant representations.](http://arxiv.org/abs/2302.10283) | 本文介绍了一个数据集3DIEBench，提出了一种基于超网络的等变表达式预测器SIE，结合分裂的不变与等变表达式以获得更加丰富的表示，同时证明了显著的性能提高。 |
| [^110] | [JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models.](http://arxiv.org/abs/2302.09125) | 本文提出了 JANA 方法，用于处理复杂贝叶斯模型的近似计算。通过端到端训练三个神经网络来实现分摊的近似后验和似然，为贝叶斯工作流程提供了一种新的途径。此方法在多种模拟模型中进行了基准测试，并提出了一种联合校准诊断方法。 |
| [^111] | [Practical Contextual Bandits with Feedback Graphs.](http://arxiv.org/abs/2302.08631) | 本文提出了一种利用反馈图的情境赌博算法，可用于缓解学习的统计复杂性，在实际应用中具有计算上的可行性和优秀的表现。 |
| [^112] | [Dataset Interfaces: Diagnosing Model Failures Using Controllable Counterfactual Generation.](http://arxiv.org/abs/2302.07865) | 本文提出了一种数据集接口的概念，旨在通过给定一个输入数据集和一个用户指定的分布偏移来返回针对该分布的实例。我们提出了一种利用文本反转的数据集接口实现，使得生成更加适应输入分布，进而帮助研究模型在各种分布偏移下的行为。 |
| [^113] | [Vector Quantized Wasserstein Auto-Encoder.](http://arxiv.org/abs/2302.05917) | 本文提出了一种向量量化Wasserstein自编码器，可通过最小化WS距离将离散潜在表示传递到数据分布，实现更好的、可控的聚类解决方案。 |
| [^114] | [Compositional Exemplars for In-context Learning.](http://arxiv.org/abs/2302.05698) | 该论文提出了CEIL（Compositional Exemplars for In-context Learning）框架，利用决定性点过程（DPP）模型处理上下文示例选择问题，从而提高了大型预训练语言模型（LMs）进行上下文学习的性能。 |
| [^115] | [Hypernetworks build Implicit Neural Representations of Sounds.](http://arxiv.org/abs/2302.04959) | 该论文介绍了一种新的方法，名为“HyperSound”，可将超网络结构应用于元学习，从而生成音频隐式神经表示（INR），该方法可用于音频信号的处理，并且重构质量可与其他最先进的模型相媲美，是当代音频处理中的一个有潜力的替代方案。 |
| [^116] | [Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion.](http://arxiv.org/abs/2302.04451) | 本文提出了用特征扩散矩阵的最大奇异值来缩放泛化界限的方法，并用Hessians来衡量图神经网络对噪声扰动的稳定性。实验证明，这些方法可以有效减小泛化界限，更好地解决了实际图形问题。 |
| [^117] | [Machine Learning for Synthetic Data Generation: A Review.](http://arxiv.org/abs/2302.04062) | 机器学习用于合成数据生成的综述，探讨了合成数据生成的应用（计算机视觉、语音、自然语言、医疗保健和商业）、机器学习方法（神经网络架构和深度生成模型）以及隐私和公平问题，并提出了未来的研究方向。 |
| [^118] | [Domain Adaptation for Time Series Under Feature and Label Shifts.](http://arxiv.org/abs/2302.03133) | 本文提出了Raincoat，这是第一个针对复杂时间序列的封闭集和通用领域自适应模型。Raincoat通过跨域对齐时间和频率特征，修正偏移以便于检测私有标签，并通过识别共享结构来提高可传递性。 |
| [^119] | [Toward Large Kernel Models.](http://arxiv.org/abs/2302.02605) | 本文提出了一种构建大规模通用核模型的方法，这解决了传统核机器中模型大小与数据大小相互耦合的问题，使其能够在大数据集上进行训练。 |
| [^120] | [Better Training of GFlowNets with Local Credit and Incomplete Trajectories.](http://arxiv.org/abs/2302.01687) | 本文提出了一种基于局部信用和不完整轨迹的GFlowNets更好的训练方法，通过为轨迹的每个步骤分配部分奖励，基于局部奖励估计器，实现更有效利用数据，提高了GFlowNets性能。 |
| [^121] | [Mnemosyne: Learning to Train Transformers with Transformers.](http://arxiv.org/abs/2302.01128) | Mnemosyne优化器使用Performers方法来学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节，并成功训练ViTs和应用于机器人领域中，具有更好的泛化能力与快速收敛。 |
| [^122] | [Generative Adversarial Symmetry Discovery.](http://arxiv.org/abs/2302.00236) | 使用LieGAN框架，可以自动发现数据集中的等变性，从而提高预测的准确性和泛化能力。 |
| [^123] | [Convolutional autoencoder for the spatiotemporal latent representation of turbulence.](http://arxiv.org/abs/2301.13728) | 采用卷积自编码器获得高效准确的湍流流动潜在描述。 |
| [^124] | [Holistic Graph-based Motion Prediction.](http://arxiv.org/abs/2301.13545) | 该论文提出了一种基于全局图形的运动预测方法，使用综合性异构图形表示来结合交通参与者之间的时空信息、属性和关系以及道路网络等静态元素的关系，以提高运动预测的质量。 |
| [^125] | [MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning.](http://arxiv.org/abs/2301.13287) | 提出了一个模型无关子集选择框架MILO，将子集选择与模型训练分离，通过易到难的课程实现了卓越的模型收敛和性能。 |
| [^126] | [Mutual Wasserstein Discrepancy Minimization for Sequential Recommendation.](http://arxiv.org/abs/2301.12197) | 本文提出了一种基于互Wasserstein距离最小化的新型自监督学习框架用于提高推荐系统的性能，该方法使用Wasserstein距离测量来增强互信息最大化，优于现有的几种顺序推荐方法。 |
| [^127] | [Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees.](http://arxiv.org/abs/2301.11911) | 提出了多维概念发现(MCD)方法，它满足概念层面上的完整性关系，不需要加强概念可解释性或重新训练模型部分，并提供概念激活图分析工具 |
| [^128] | [Input Perturbation Reduces Exposure Bias in Diffusion Models.](http://arxiv.org/abs/2301.11706) | 本文提出了一种输入扰动方法来缓解扩散模型中的曝光偏差问题，该方法不影响模型性能，能显著提高生成样本的质量并减少训练和推断时间。 |
| [^129] | [Class-Incremental Learning with Repetition.](http://arxiv.org/abs/2301.11396) | 该论文提出了带有重复的类增量学习，并通过两个随机流生成器产生不同的CIR流进行评估和提出了一种新的重放策略。 |
| [^130] | [Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series.](http://arxiv.org/abs/2301.11308) | 本研究提出了一个用于不规则采样时间序列的神经连续离散状态空间模型，其采用辅助变量来区分识别和动态，从而实现了准确的贝叶斯推理和改进的性能。 |
| [^131] | [How Jellyfish Characterise Alternating Group Equivariant Neural Networks.](http://arxiv.org/abs/2301.10152) | 该论文提供了交替群($A_n$)等变神经网络的完整表征，其中描述了可学习的、线性的、$A_n$等变层函数的矩阵基，在神经网络构建中具有广泛的适用性。 |
| [^132] | [Trustworthiness Score to Evaluate CNNs Predictions.](http://arxiv.org/abs/2301.08839) | 本文提出了一种用于评估CNN预测可信度的可信度分数（TS）度量标准，通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。 |
| [^133] | [Causal Falsification of Digital Twins.](http://arxiv.org/abs/2301.07210) | 这篇论文提出了一种数字孪生的因果伪证方法，以可靠并实用的方式在最小限度的假设下提供孪生的信息和评估结果。 |
| [^134] | [The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks.](http://arxiv.org/abs/2301.07068) | 本文提出了#DNN-Verification问题，即计算违反特定安全性质的DNN输入配置数量的问题。作者提出了一种新的方法和一种随机的近似方法，分别给出了确切的违规计数和可证明概率界，并在安全关键基准测试上进行了实验比较。 |
| [^135] | [Short-length SSVEP data extension by a novel generative adversarial networks based framework.](http://arxiv.org/abs/2301.05599) | 本文提出了一种基于GAN的端到端信号转化网络TEGAN，可以将短SSVEP信号转换成长的人工SSVEP信号，并显著提高BCI系统的效率和准确性。 |
| [^136] | [Sequential Fair Resource Allocation under a Markov Decision Process Framework.](http://arxiv.org/abs/2301.03758) | 本研究提出了一种基于马尔可夫决策过程框架的算法SAFFE，用于在有限时间内分配有限资源给披露其随机需求的代理，并实现公平分配。实验结果表明，SAFFE在公平性和社会福利方面的性能优于多种基线算法。 |
| [^137] | [Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data.](http://arxiv.org/abs/2301.00437) | 研究者证明对于均方误差和交叉熵损失，深度线性网络中出现的全局解在不同数据上都具有神经塌陷的特性，即最后一层特征会崩溃为类均值，而这些类均值是等角紧框架的顶点。 |
| [^138] | [Understanding the Complexity Gains of Single-Task RL with a Curriculum.](http://arxiv.org/abs/2212.12809) | 本文提出一个理论框架将单任务强化学习问题重新构造为由课程定义的多任务问题，证明在课程有轻微正则化条件的情况下，依次解决每个任务比直接解决原始单任务更加计算上高效。 |
| [^139] | [StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation.](http://arxiv.org/abs/2212.10229) | 本文对GAN的领域适应问题进行了深入研究并提出了StyleGAN用于领域适应的新的高效轻量级参数化方案。 |
| [^140] | [Deep learning applied to computational mechanics: A comprehensive review, state of the art, and the classics.](http://arxiv.org/abs/2212.08989) | 本文综述了深度学习在计算力学中的应用，讨论了混合和纯深度学习方法的进展。其中，混合方法将传统PDE离散化与深度学习相结合，而纯深度学习方法则代表为物理信息神经网络（PINN）方法。 |
| [^141] | [Brauer's Group Equivariant Neural Networks.](http://arxiv.org/abs/2212.08630) | 本文描述了对于三个在机器学习文献中缺失的对称群（$O(n)$、$SO(n)$和$Sp(n)$），所有可能的群等变神经网络的特征，并找到了这些网络在不同基础下矩阵的生成集。 |
| [^142] | [Neural Implicit k-Space for Binning-free Non-Cartesian Cardiac MR Imaging.](http://arxiv.org/abs/2212.08479) | 本文提出了一种基于神经隐式k空间的无分bin非笛卡尔心脏磁共振成像方法，能够实现一个连续、无分bin以及个体化的k空间表示，并通过简单的反傅里叶变换恢复图像，消除密度补偿和费用昂贵的非均匀傅里叶变换的需求。 |
| [^143] | [Stochastic First-Order Learning for Large-Scale Flexibly Tied Gaussian Mixture Model.](http://arxiv.org/abs/2212.05402) | 本文提出了一种面对高维流数据和复杂密度挑战的快速在线参数估计算法，并利用灵活绑定因子分解的协方差矩阵提供了一个框架。通过引入一阶随机优化和新的随机流形优化算法，实现了高斯混合模型的优化。 |
| [^144] | [Multi-Concept Customization of Text-to-Image Diffusion.](http://arxiv.org/abs/2212.04488) | 该论文提出了一种称为Custom Diffusion的高效方法，用于增强现有的文本图像模型以实现多概念定制化，该方法只需优化文本图像调节机制中的几个参数就可以表示新概念并实现快速调整。同时，该方法可以联合训练多个概念或将多个精调模型合并为一个，并在生成多个新概念的变异并将它们与现有概念无缝地组合方面表现出色。 |
| [^145] | [Bi-LSTM Price Prediction based on Attention Mechanism.](http://arxiv.org/abs/2212.03443) | 本文提出了一种基于注意力机制的双向LSTM神经网络，用于黄金和比特币的价格预测，结合传统的技术因素和时间序列模型开发了因素，使用预测结果获得了显著的回报，且模型表现最佳。 |
| [^146] | [Statistical mechanics of continual learning: variational principle and mean-field potential.](http://arxiv.org/abs/2212.02846) | 从物理学的角度将连续学习的问题转化为Franz-Parisi热力学势的框架，将之前学习到的任务作为先验和参考，提出了一个在场空间中训练神经网络的变分贝叶斯学习设置，用于调节任务间的突触资源。 |
| [^147] | [Deep neural network techniques for monaural speech enhancement: state of the art analysis.](http://arxiv.org/abs/2212.00369) | 本文回顾了在单声道语音增强领域被广泛应用的DNN技术，包括从特征提取到模型训练的整个增强流程，并探讨了使用预训练模型提高增强效果的方法。 |
| [^148] | [On the Power of Foundation Models.](http://arxiv.org/abs/2211.16327) | 本文通过范畴论探究了基础模型的能力，提出了具有最小所需能力的基础模型可以通过微调和足够的资源来解决前置任务所定义的类别中的下游任务，并且这种能力可以扩展到任何下游任务，只要允许微调且下游任务可在前置任务定义的范畴中表示。 |
| [^149] | [Lipschitz constant estimation for 1D convolutional neural networks.](http://arxiv.org/abs/2211.15253) | 本文提出了一种利用耗散理论估计一维卷积神经网络Lipschitz常数的方法，具有高效、准确和可扩展性的特点。 |
| [^150] | [AugOp: Inject Transformation into Neural Operator.](http://arxiv.org/abs/2211.12514) | 本文提出了一种基于注入额外的分组变换来增强常规卷积算子的方法，名为 AugConv，可以引入更大的学习能力，提升模型性能，而不会增加部署时的额外计算开销。 |
| [^151] | [PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design.](http://arxiv.org/abs/2211.12020) | 提出了PhAST方法来快速发现更有效的催化剂来驱动电化学反应, 该方法适用于大多数体系结构, 可以增加计算效率和精度 |
| [^152] | [AdaFocal: Calibration-aware Adaptive Focal Loss.](http://arxiv.org/abs/2211.11838) | 本文介绍了AdaFocal，一种新的校准感知自适应Focal Loss。与其他方法相比，在多个数据集上表现出更好的校准性能。 |
| [^153] | [SinFusion: Training Diffusion Models on a Single Image or Video.](http://arxiv.org/abs/2211.11743) | 本文提出了一种在单张图像或视频上训练扩散模型的方法，称为SinFusion。该模型可以解决各种图像/视频特定的操作任务，包括从少量帧中学习单个输入视频的运动和动态，生成相同动态场景的多样化新视频样本，将短视频推广为长视频（向前和向后）并执行视频上采样。 |
| [^154] | [Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters.](http://arxiv.org/abs/2211.11031) | 本论文提出了一种名为GRACE的方法来实现终身模型编辑，它通过在流式错误上执行目标编辑来修复部署模型的问题，生成一个离散、本地的编辑编码本，而不会改变模型权重，在进行数千个顺序编辑时表现为最先进的性能。 |
| [^155] | [NVDiff: Graph Generation through the Diffusion of Node Vectors.](http://arxiv.org/abs/2211.10794) | NVDiff利用分数为基础的生成模型采样节点向量来生成图形，显著降低了扩散过程的维度，提高了采样速度，同时在生成质量方面表现更好。 |
| [^156] | [Efficient Video Representation Learning via Motion-Aware Token Selection.](http://arxiv.org/abs/2211.10636) | 该论文提出了一种新的运动感知标记选择方法，针对视频中不同补丁的信息密度，选择包含丰富动态特性的标记，放弃无效的标记，从而大大降低计算和存储需求，实现了在单台机器上进行预训练和微调而不影响性能。 |
| [^157] | [Mirror Sinkhorn: Fast Online Optimization on Transport Polytopes.](http://arxiv.org/abs/2211.10420) | 该论文提出了一种在传输多面体上进行在线凸目标优化的算法，此算法基于Sinkhorn矩阵缩放和镜像下降的原理，并且可以在噪音环境下使用。 |
| [^158] | [CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI.](http://arxiv.org/abs/2211.10354) | 本文介绍了一种名为CRONOS的系统，可以通过彩色化和对比学习来基于Wi-Fi CSI实现无人设备NLoS人体检测，可以区分房间中的移动人员和空置。实验结果表明该系统在NLoS条件下能够准确地检测出房间中的人物存在。 |
| [^159] | [Variants of SGD for Lipschitz Continuous Loss Functions in Low-Precision Environments.](http://arxiv.org/abs/2211.04655) | 本文研究了在低精度环境下神经网络训练的SGD变种，并测试了不同变种的效果。结果表明，相比于传统的SGD方法，在低精度算术环境下使用自适应步长的SGD变种可以获得更好的测试集准确性。 |
| [^160] | [Benefits of Monotonicity in Safe Exploration with Gaussian Processes.](http://arxiv.org/abs/2211.01561) | 本文提出了一种名为单调安全UCB(M-SafeUCB)的算法，通过单调性假设，取得了在保证精度的同时显著的优势。 |
| [^161] | [Balancing Utility and Fairness in Submodular Maximization (Technical Report).](http://arxiv.org/abs/2211.00980) | 本文提出了一个新的问题，称为“二标准子模最大化”，以平衡效用和公平性。该问题要求找到一个固定大小的解，以最大化效用函数为目标。 |
| [^162] | [Lifelong Bandit Optimization: No Prior and No Regret.](http://arxiv.org/abs/2210.15513) | 本文提出了一种算法LIBO，可以无需直接访问数据，对一系列赌博优化任务进行学习和适应，并保证最优性能和亚线性终身后悔率。 |
| [^163] | [On the Robustness of Dataset Inference.](http://arxiv.org/abs/2210.13631) | 机器学习模型的知识产权需要保护，数据集推理技术(DI)提供更好的鲁棒性和效率，但在某些情况下存在高假阳性。 |
| [^164] | [Deep Multi-Branch CNN Architecture for Early Alzheimer's Detection from Brain MRIs.](http://arxiv.org/abs/2210.12331) | 本论文提出了一种基于脑MRIs的早期阿尔茨海默病检测方法，使用深度多分支卷积神经网络结构并达到了99.05%的三分类准确率。 |
| [^165] | [Graphically Structured Diffusion Models.](http://arxiv.org/abs/2210.11633) | 本文提出了一种自动定义和学习具有问题特定结构的深度生成模型的框架，通过训练具有特定于问题规范的体系结构的扩散模型，优化了问题维度与模型性能之间的缩放关系。 |
| [^166] | [Towards Explaining Distribution Shifts.](http://arxiv.org/abs/2210.10275) | 本文介绍了一种通过可解释的转运图来解释分布变化的方法，以帮助人工进行减轻影响的任务。通过许多现实数据集的例子，展示了解释性映射提供了更好的详细性和可解释性之间的平衡。 |
| [^167] | [The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks.](http://arxiv.org/abs/2210.10040) | 研究揭示了社会偏见基准中数据集构建偏见可能对结果造成了重要影响，需要更严谨的社会偏见度量方法。 |
| [^168] | [PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting.](http://arxiv.org/abs/2210.08964) | 提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast），将数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，可以直接应用于语言模型。 |
| [^169] | [Reprogramming Pretrained Language Models for Antibody Sequence Infilling.](http://arxiv.org/abs/2210.07144) | 该篇论文利用模型重编程技术，将基于自然语言的预训练模型重新用于抗体CDR序列推断任务中，成功提高了生成的序列多样性和新颖性，并保持了高质量的结构一致性。 |
| [^170] | [Actor-Critic or Critic-Actor? A Tale of Two Time Scales.](http://arxiv.org/abs/2210.04470) | 这篇论文提出了一种评论演员算法，它在快速和慢速时间尺度上计算价值函数和策略，该算法与演员评论算法在准确性和计算成本方面表现相当。 |
| [^171] | [Efficient Learning of Mesh-Based Physical Simulation with BSMS-GNN.](http://arxiv.org/abs/2210.02573) | 该论文提出了一种名为bi-stride的新型池化策略，利用二部图决策实现高效的基于网格的物理仿真，无需手动绘制粗网格，并避免了空间接近性带来的错误边缘。 |
| [^172] | [Class-Imbalanced Complementary-Label Learning via Weighted Loss.](http://arxiv.org/abs/2209.14189) | 该论文提出了一种名为加权互补标签学习的新型CLL方法，能够解决现实世界数据集中的类别不平衡训练样本问题，通过利用类别不平衡的互补标签建模加权的经验风险最小化损失，也适用于多类别不平衡的训练样本，并在实验中展示了其比其他最先进的方法更好的性能。 |
| [^173] | [Survey on Fairness Notions and Related Tensions.](http://arxiv.org/abs/2209.13012) | 本文调查了公平性的不同概念以及它们与其他期望属性的紧张关系，并介绍了处理公平性-准确性权衡问题的不同方法。 |
| [^174] | [Self-supervised learning of hologram reconstruction using physics consistency.](http://arxiv.org/abs/2209.08288) | 本文报道了一种名为GedankenNet的自监督学习模型，可以在不需要标记或实验数据的情况下，利用物理一致性损失和合成的人工随机图像进行全息图重建，并展示了其在各种样品类型上的有效性和优越的泛化能力。 |
| [^175] | [Normalizing Flows for Interventional Density Estimation.](http://arxiv.org/abs/2209.06203) | 本研究提出了一种名为干预正则化流的全参数深度学习方法，用于从观测数据中估计干预后的潜在结果密度。该方法结合了nuisance flow和target flow，并开发了一个易于处理的优化目标，以实现有效和双重稳健的估计。实验证明，该方法优于现有最先进的方法。 |
| [^176] | [Bridging the Gap: Differentially Private Equivariant Deep Learning for Medical Image Analysis.](http://arxiv.org/abs/2209.04338) | 本文提出使用差分隐私保护的可变等变卷积网络进行医学图像分析，通过改进特征质量和参数效率缩小了隐私-效用差距，带来了显著的准确度提高。 |
| [^177] | [SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion.](http://arxiv.org/abs/2209.03855) | 这篇论文介绍了一种通过扩散模型学习数据驱动的SE(3)成本函数的方法，该方法可以与其他成本无缝集成到单个可微分的目标函数中，从而实现联合抓取和运动优化。 |
| [^178] | [TFN: An Interpretable Neural Network with Time-Frequency Transform Embedded for Intelligent Fault Diagnosis.](http://arxiv.org/abs/2209.01992) | 提出一种新的可解释神经网络模型——时频网络（TFN）。在传统卷积层中嵌入了物理上有意义的时频变换（TFT）方法作为自适应预处理层，该层不仅提高了诊断性能，还使得网络结构可以可解释，故障诊断过程透明。 |
| [^179] | [Detecting Multivariate Time Series Anomalies with Zero Known Label.](http://arxiv.org/abs/2208.02108) | 本文提出了一种利用动态图和实体注意力机制实现零标签多元时间序列异常检测的方法，其利用密度估计比较异常和正常实例，性能优于多种无监督方法和半监督方法。 |
| [^180] | [Adversarial Camouflage for Node Injection Attack on Graphs.](http://arxiv.org/abs/2208.01819) | 本文提出了一种名为CANA的对抗伪装框架，可以在节点注入攻击中使被注入节点看起来正常，并提高在实际场景下防御/检测方法的攻击性能。 |
| [^181] | [Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy.](http://arxiv.org/abs/2207.12141) | 本文提出了一种名为PDML的学习动力学模型的方法，该方法动态调整历史策略混合分布以适应训练过程中的进化策略，实验结果表明PDML优于最先进的方法，实现了更快的收敛和更好的最终性能。 |
| [^182] | [Towards an Improved Understanding of Software Vulnerability Assessment Using Data-Driven Approaches.](http://arxiv.org/abs/2207.11708) | 本文通过提供知识和自动化支持，基于数据驱动方法来推进软件安全领域中的软件漏洞评估。该论文提供了一套新颖的数据驱动技术和实用建议，适用于该领域的研究人员和实践者，以提高对真实软件系统中日益增长的漏洞的评估理解，从而实现对这些关键安全问题的更彻底和及时修复。 |
| [^183] | [Probing Classifiers are Unreliable for Concept Removal and Detection.](http://arxiv.org/abs/2207.04153) | 本文研究了在文本数据上神经网络模型中的不良概念去除。对于现有的后期和对抗性方法，本文理论和实证分析表明其依赖的探测分类器可能使用非概念特征，导致无法完全去除不需要的概念。我们提出了一种直接学习从模型的表示中去除概念的方法，实验结果表明其优于最先进的后期和对抗性方法。 |
| [^184] | [Bandwidth Enables Generalization in Quantum Kernel Models.](http://arxiv.org/abs/2206.06686) | 该论文研究了量子核模型的普适性问题，通过改变量子核带宽的值，能够使量子模型具备普适性，从而提高在实际问题中的速度表现。 |
| [^185] | [Hysteretic Behavior Simulation Based on Pyramid Neural Network:Principle, Network Architecture, Case Study and Explanation.](http://arxiv.org/abs/2206.03990) | 本文提出了一种加权堆叠的金字塔神经网络架构，通过引入多级快捷方式建立金字塔结构，将特征直接集成到输出模块中，并提出了加权堆叠策略来增强特征融合方法。在滞回行为模拟中具有较优的效率和准确性。 |
| [^186] | [Decomposed Linear Dynamical Systems (dLDS) for learning the latent components of neural dynamics.](http://arxiv.org/abs/2206.02972) | 该论文提出了一种新的分解动力系统模型，将复杂非平稳和非线性动态表示为简单、可解释的稀疏组合，并通过字典学习过程进行训练。 |
| [^187] | [Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning.](http://arxiv.org/abs/2206.02670) | 本文提出了一种基于可解释的深度学习方法的创新性检测方案，以保护采用这些方法的无人机免受对抗攻击。 |
| [^188] | [A Survey on Deep Learning for Skin Lesion Segmentation.](http://arxiv.org/abs/2206.00356) | 本综述调查了177篇深度学习分割皮肤病变的研究论文，探索了深度学习模型在皮肤病变分割中的适用性，并从多个方面对输入数据、模型设计和评估方面进行分析和讨论，提出了开放的挑战和未来的研究方向。 |
| [^189] | [VFed-SSD: Towards Practical Vertical Federated Advertising.](http://arxiv.org/abs/2205.15987) | 本文提出了一种半监督分裂知识蒸馏框架VFed-SSD，用于改善广告模型的学习效果。该框架利用未标记的相互重叠的数据，并通过分解联邦模型来在模型性能和推理效率之间保持平衡。实验结果在真实数据集上表明，VFed-SSD相对于其他最先进的垂直联合学习方法具有更高的预测准确性和推理效率。 |
| [^190] | [Robust Anytime Learning of Markov Decision Processes.](http://arxiv.org/abs/2205.15827) | 文章介绍了一种将贝叶斯推理方案和计算强健策略相结合的、不断学习马尔可夫决策过程转移概率的方法，阐述了不确定MDP（uMDP）的概念，针对应用中有限数据导致的统计误差问题提出了基于不确定性集的解决方法，并介绍了计算强健策略以遵循形式规范的工具。 |
| [^191] | [Physical Activation Functions (PAFs): An Approach for More Efficient Induction of Physics into Physics-Informed Neural Networks (PINNs).](http://arxiv.org/abs/2205.14630) | 介绍了一种物理激活函数（PAFs）的概念，它基于研究现象的物理定律的数学表示，而不是一般的激活函数，可以提高PINN的训练效率和优化物理模型的交错。 |
| [^192] | [Towards A Proactive ML Approach for Detecting Backdoor Poison Samples.](http://arxiv.org/abs/2205.13616) | 本研究提出了一种主动式机器学习方法，以检测深度学习模型中的后门毒样本，强调防御者的主动介入，直接强制实施并放大后受攻击模型的特殊特征，缓解后门攻击的威胁。 |
| [^193] | [Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification.](http://arxiv.org/abs/2205.13094) | 该论文证明在非参数二元分类中，缺乏少数派样本是学习的根本限制，并探讨了欠采样算法的最小化极差风险的鲁棒性表现，特别是在标签转移的情况下可以最优化。 |
| [^194] | [QGNN: Value Function Factorisation with Graph Neural Networks.](http://arxiv.org/abs/2205.13005) | 本文提出了一种基于图神经网络的QGNN价值函数分解方法，可以解决多智能体样本数据利用率低的问题。相比以往的方法，QGNN的多层消息传递架构提供了更高的表示复杂度，引入的置换不变的混合器也使其更具伸缩性和灵活性，在合作和非合作任务中均表现出优于先前的方法的性能。 |
| [^195] | [Scale Dependencies and Self-Similar Models with Wavelet Scattering Spectra.](http://arxiv.org/abs/2204.10177) | 本论文提出了小波散射谱方法，可以用于建模具有平稳增量的时间序列的非高斯特性，其系数可以用于构建最大熵模型和生成新的时间序列，同时证明了自相似过程具有散射谱的尺度不变性。 |
| [^196] | [Learning Optimal Dynamic Treatment Regimes Using Causal Tree Methods in Medicine.](http://arxiv.org/abs/2204.07124) | 该论文开发了两种新的方法，用于有效处理复杂的患者数据，基于数据驱动的异质性治疗效应估计，使用因果树方法（具体来说是因果树和因果森林），学习非线性关系，控制时间变化混淆，是双重稳健的和可解释的。在治疗抑郁症的真实世界数据应用程序中，该方法在准确性和实际可解释性方面表现出色。 |
| [^197] | [Multimodal Fusion Transformer for Remote Sensing Image Classification.](http://arxiv.org/abs/2203.16952) | 本文引入了一个新的多模态融合Transformer，利用多源互补信息来提高遥感图像分类的性能。 |
| [^198] | [Whiplash Gradient Descent Dynamics.](http://arxiv.org/abs/2203.02140) | 本文提出了一种鞭锤惯性梯度动力学算法，在闭环优化中利用梯度信息，在有限维度设置中寻找代价函数的最小值。该算法的探索式启发式变体可以有效逃离鞍点，并且实验结果表明其具有多项式和指数收敛率。 |
| [^199] | [SAITS: Self-Attention-based Imputation for Time Series.](http://arxiv.org/abs/2202.08516) | SAITS是一种基于自注意力机制的多元时间序列缺失值插值方法，通过两个对角线掩码自注意力块的加权组合，能够明确地捕捉时间步长之间的时序依赖关系和特征相关性，从而提高了插值准确性和训练速度。 |
| [^200] | [OLIVE: Oblivious Federated Learning on Trusted Execution Environment against the risk of sparsification.](http://arxiv.org/abs/2202.07165) | 本文通过分析FL中TEE的漏洞，并在TEE中引入Oblivious Memory Access（OMA）以保护免受稀疏化风险的影响，提出了OLIVE算法，该算法在通信效率和模型精度方面优于最先进的安全聚合和差分隐私FL算法。 |
| [^201] | [Discrete Simulation Optimization for Tuning Machine Learning Method Hyperparameters.](http://arxiv.org/abs/2201.05978) | 本文介绍了使用离散仿真优化方法调整机器学习方法超参数的过程。作者展示了如何使用排名和选择（R&S）和随机搜索等方法识别一个最优超参数集，以最大化ML方法性能。 |
| [^202] | [The Dual PC Algorithm and the Role of Gaussianity for Structure Learning of Bayesian Networks.](http://arxiv.org/abs/2112.09036) | 双重PC算法通过利用协方差和精度矩阵之间的反向关系，实现了CI测试，能够恢复正确的等价类，并可对互补调节集的偏相关进行测试。 |
| [^203] | [Understanding Dynamic Spatio-Temporal Contexts in Long Short-Term Memory for Road Traffic Speed Prediction.](http://arxiv.org/abs/2112.02409) | 本论文提出了一种动态局部化的LSTM模型，能够同时考虑道路之间的空间与时间依赖关系，该模型比基线方法具有更优异的预测性能。 |
| [^204] | [Model-Based Reinforcement Learning via Stochastic Hybrid Models.](http://arxiv.org/abs/2111.06211) | 本文提出了一种基于混合系统的非线性建模和控制方法，采用序列建模范例和期望最大化算法，将非线性动态分解为具有非线性转换边界的随机分段仿射模型，并成功应用于自动化领域。 |
| [^205] | [The ODE Method for Asymptotic Statistics in Stochastic Approximation and Reinforcement Learning.](http://arxiv.org/abs/2110.14427) | 本文提出了一种称为ODE方法的渐近统计方法解决$d$维随机逼近递归的问题，证明了其收敛性和中心极限定理，为强化学习等领域的应用提供了有力的理论支持。 |
| [^206] | [Quantifying Epistemic Uncertainty in Deep Learning.](http://arxiv.org/abs/2110.12122) | 本文提供了一个理论框架来分解深度学习中的不确定性，并提出了两种方法来估计这些不确定性，这些方法使我们能够克服在使用传统统计方法时遇到的困难，从而为建模和数据收集提供直接的指导。 |
| [^207] | [Decoupled Contrastive Learning.](http://arxiv.org/abs/2110.06848) | 本研究提出了分离式对比学习（DCL）损失，通过去除传统对比学习中的正项提高学习效率。 |
| [^208] | [On the Convergence and Calibration of Deep Learning with Differential Privacy.](http://arxiv.org/abs/2106.07830) | 本文通过NTK对差分隐私训练进行连续时间分析，发现噪声只会影响隐私风险而不影响收敛性和校准性，而基于每个样本的梯度剪裁会影响收敛性和校准性。此外，大剪裁范数下的差分隐私模型不仅享有相同的隐私保证，而且校准效果好。 |
| [^209] | [Learning to Combine Per-Example Solutions for Neural Program Synthesis.](http://arxiv.org/abs/2106.07175) | 该论文提出了一种把程序合成问题分为两个阶段的方法，提高了解决方案的成功率。作者使用了由多头注意机制构建的Cross Aggregator神经网络模块，学习如何组合每个样例程序解决方案，生成全局解决方案。 |
| [^210] | [IID-GAN: an IID Sampling Perspective for Regularizing Mode Collapse.](http://arxiv.org/abs/2106.00563) | 本论文提出一种基于独立同分布采样视角的GAN方法，通过在源分布下实现逆采样的IID样本与真实数据在潜在空间中的高斯源之间的相似性，从而规范化了GAN中的模式崩溃问题。 |
| [^211] | [Clustered Federated Learning via Generalized Total Variation Minimization.](http://arxiv.org/abs/2105.12769) | 本文介绍了一种基于广义全变差最小化的完全分散的联邦学习算法，可以训练适用于具有本地数据集的分散式去中心化环境的本地化（或个性化）模型，并获得了良好的模拟结果。 |
| [^212] | [A Low-Delay MAC for IoT Applications: Decentralized Optimal Scheduling of Queues without Explicit State Information Sharing.](http://arxiv.org/abs/2105.11213) | 本文研究了一种适用于物联网应用的低延迟MAC协议，该协议无需显式状态信息共享且具有分布式控制。该协议采用“贪心”和“穷尽”的策略，旨在在流量竞争访问时实现低延迟，在重负载下实现定时访问。 |
| [^213] | [Learning to Rearrange Deformable Cables, Fabrics, and Bags with Goal-Conditioned Transporter Networks.](http://arxiv.org/abs/2012.03385) | 本论文开发了一套仿真基准测试，提出将目标条件嵌入Transporter Networks来学习可变形物体操作，并证明该方法成功地执行了复杂目标条件和多步操作任务。 |
| [^214] | [Improving Speech Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information.](http://arxiv.org/abs/2011.07442) | 本文提出了一种新的语音增强方法，通过利用广义语音类别序列的损失来提高SE性能，实验证明上下文BPC信息可以提高性能。 |
| [^215] | [Simple and optimal methods for stochastic variational inequalities, I: operator extrapolation.](http://arxiv.org/abs/2011.02987) | 本文提出了一种简单实用的算子外推法用来解决变分不等式问题，同时还提出了一种随机算子外推法实现随机平滑和强单调VI的最优复杂度。 |
| [^216] | [LEAD: Least-Action Dynamics for Min-Max Optimization.](http://arxiv.org/abs/2010.13846) | 本文提出了一种名为LEAD的优化器，它基于物理学中的动力学特性来改进博弈优化的收敛问题，并在二次极小-极大博弈中展示了线性收敛到纳什均衡的特性。 |
| [^217] | [Riemannian Langevin Algorithm for Solving Semidefinite Programs.](http://arxiv.org/abs/2010.11176) | 研究了一种基于Langevin扩散的算法，用于在球面的乘积流形上进行非凸优化和采样，展示了在适当的温度选择下，该算法可以保证次优解到全局最小值的间隙概率非常小，并在此基础上提出了求解半定向规划模型的新方法，并给出了全局最优性的保证。 |
| [^218] | [IRX-1D: A Simple Deep Learning Architecture for Remote Sensing Classifications.](http://arxiv.org/abs/2010.03902) | 我们提出了一种简单的深度学习架构，用于遥感分类。在小样本情况下，该架构比2D-CNN表现更好；与其他不同深度学习架构相比，在印第安娜松树高光谱数据集上表现相当或更好。但使用有限训练样本得到的分类图像与使用大样本训练的模型得到的分类图像会将不同的陆地覆盖类别分配到相同的区域。 |
| [^219] | [Cooperative Multi-Agent Reinforcement Learning with Partial Observations.](http://arxiv.org/abs/2006.10822) | 本文提出了一种基于局部状态和动作信息的分布式零阶策略优化方法，可用于部分观测的协作多智能体强化学习，减小通信开销并取得更好的效果。 |
| [^220] | [Deep Learning Convective Flow Using Conditional Generative Adversarial Networks.](http://arxiv.org/abs/2005.06422) | FluidGAN是一种通用深度学习框架，能够高速、高准确性地预测复杂对流流动，并可帮助理解物理模型复杂或未知的确定性多物理现象。 |
| [^221] | [On-the-Fly Adaptation of Source Code Models using Meta-Learning.](http://arxiv.org/abs/2003.11768) | 本文提出了一种基于元学习的方法用于源码模型的实时适应性研究，以提高源代码模型的预测准确性，解决代码自动完成问题。 |
| [^222] | [Secure Summation via Subset Sums: A New Primitive for Privacy-Preserving Distributed Machine Learning.](http://arxiv.org/abs/1906.11993) | 本文提出了一种通过子集和实现安全求和的方法（S5），该方法可以在存在恶意服务器和只有两个诚实客户的情况下运行，并且可以实现更高的准确性和更低的通信和计算成本。 |

# 详细

[^1]: 「子集选择与信息图神经网络的相互作用」研究

    On the Interplay of Subset Selection and Informed Graph Neural Networks. (arXiv:2306.10066v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10066](http://arxiv.org/abs/2306.10066)

    该研究探讨了在QM9数据集中，采用基于领域知识的数据采样方法来选择有效的训练集，再与信息图神经网络相结合，以最大化分子多样性和相关性来提高性能。

    

    机器学习技术结合海量数据集的可用性显著提高了我们探究化合物空间的能力，通过快速准确地预测分子性质。然而，大数据集的学习受到计算资源的限制，在某些情况下可能是不可行的。此外，数据集中的实例可能还没有被标记，生成标记可能成本高昂，例如量子化学计算。因此，有必要从大型未标记数据点池中选择小训练子集，并开发可靠的机器学习方法，以有效地从小训练集中学习。本文集中于预测 QM9 数据集中分子的原子化能。我们研究了采用基于领域知识的数据采样方法来进行有效训练集选择与通知机器学习技术相结合的优势。特别是，我们展示了如何最大化分子的多样性和相关性，从而提高了信息图神经网络的性能，这是一种专门为分子数据设计的机器学习算法。

    Machine learning techniques paired with the availability of massive datasets dramatically enhance our ability to explore the chemical compound space by providing fast and accurate predictions of molecular properties. However, learning on large datasets is strongly limited by the availability of computational resources and can be infeasible in some scenarios. Moreover, the instances in the datasets may not yet be labelled and generating the labels can be costly, as in the case of quantum chemistry computations. Thus, there is a need to select small training subsets from large pools of unlabelled data points and to develop reliable ML methods that can effectively learn from small training sets. This work focuses on predicting the molecules atomization energy in the QM9 dataset. We investigate the advantages of employing domain knowledge-based data sampling methods for an efficient training set selection combined with informed ML techniques. In particular, we show how maximizing molecular
    
[^2]: 揭示语言模型能力的结构

    Revealing the structure of language model capabilities. (arXiv:2306.10062v1 [cs.CL])

    [http://arxiv.org/abs/2306.10062](http://arxiv.org/abs/2306.10062)

    本文研究了大规模语言模型的能力结构，发现这些模型不是单一能力，而是由推理、理解和核心语言建模等三个明确定义的因素组成，并且这三个能力可以解释模型性能中的大部分方差。

    

    建立大规模语言模型（LLMs）能力的理论理解对于我们预测和解释这些系统的行为至关重要。在这里，我们通过从各种LLMs的个体差异模式中提取潜在能力来调查LLMs能力的结构。使用贝叶斯和频率因子分析的组合，我们分析了来自29个不同LLMs的27种认知任务的数据。我们发现，LLMs能力并非单一的，相反，它们更好地由三个明确定义的因素解释，分别代表推理、理解和核心语言建模。此外，我们发现这三个因素可以解释模型性能中的高比例方差。这些结果揭示了不同LLMs能力的一致结构，并展示了这些能力的多方面性质。我们还发现这三个功能与模型属性具有不同的关系。

    Building a theoretical understanding of the capabilities of large language models (LLMs) is vital for our ability to predict and explain the behavior of these systems. Here, we investigate the structure of LLM capabilities by extracting latent capabilities from patterns of individual differences across a varied population of LLMs. Using a combination of Bayesian and frequentist factor analysis, we analyzed data from 29 different LLMs across 27 cognitive tasks. We found evidence that LLM capabilities are not monolithic. Instead, they are better explained by three well-delineated factors that represent reasoning, comprehension and core language modeling. Moreover, we found that these three factors can explain a high proportion of the variance in model performance. These results reveal a consistent structure in the capabilities of different LLMs and demonstrate the multifaceted nature of these capabilities. We also found that the three abilities show different relationships to model prope
    
[^3]: MUBen：评估分子性质预测预训练模型的不确定性基准

    MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction. (arXiv:2306.10060v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10060](http://arxiv.org/abs/2306.10060)

    MUBen评估不同骨干和UQ模型组合对分子不确定性估计和属性预测的性能，以解决预训练模型微调中的过拟合校准问题。

    

    预训练于大规模无标签分子数据的大型Transformer模型在预测分子性质方面取得了巨大成功。然而，在微调期间，这些模型可能容易出现过拟合，导致对测试数据的过度自信预测落在了训练分布之外。为了解决这个问题，可以使用不确定性量化（UQ）方法来改善模型的预测校准。虽然存在许多UQ方法，但并不是所有方法都会导致性能改善。虽然一些研究使用UQ来改善分子预训练模型，但选择适合的骨干和UQ方法以可靠地估计分子不确定性的过程仍然是未经探索的。为了解决这个差距，我们提出了MUBen，评估不同的骨干和UQ模型组合，以量化它们在属性预测和不确定性估计方面的性能。通过微调使用不同分子描述符的各种骨干分子表示模型。

    Large Transformer models pre-trained on massive unlabeled molecular data have shown great success in predicting molecular properties. However, these models can be prone to overfitting during fine-tuning, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have used UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different combinations of backbone and UQ models to quantify their performance for both property prediction and uncertainty estimation. By fine-tuning various backbone molecular representation models using different molecular descrip
    
[^4]: EM-Network: 用于序列学习的自身蒸馏方法

    EM-Network: Oracle Guided Self-distillation for Sequence Learning. (arXiv:2306.10058v1 [cs.LG])

    [http://arxiv.org/abs/2306.10058](http://arxiv.org/abs/2306.10058)

    EM-Network是一种自我蒸馏方法，通过神谕指导能够有效利用目标信息进行监督序列到序列学习，并在语音识别和机器翻译等任务上取得了最先进的成果。

    

    我们引入了EM-Network，一种新颖的自我蒸馏方法，可有效利用目标信息进行监督序列到序列（seq2seq）学习，与传统方法不同的是，它是在来自目标序列的“神谕指导”下训练的。由于神谕指导紧凑地表示了目标方面的上下文，可以帮助序列模型解决任务，因此与仅使用源输入相比，EM-Network实现了更好的预测。为了使序列模型继承EM-Network的有前途的能力，我们提出了一种新的自我蒸馏策略，原始序列模型可以在一个阶段中从EM-Network的知识中获益。我们在两种seq2seq模型上进行了综合实验：用于语音识别的连接主义时间分类（CTC）和用于机器翻译的基于注意力的编码器-解码器（AED）。实验结果表明，EM-Network极大地提高了模型性能，并在各种基准数据集上取得了最先进的成果。

    We introduce EM-Network, a novel self-distillation approach that effectively leverages target information for supervised sequence-to-sequence (seq2seq) learning. In contrast to conventional methods, it is trained with oracle guidance, which is derived from the target sequence. Since the oracle guidance compactly represents the target-side context that can assist the sequence model in solving the task, the EM-Network achieves a better prediction compared to using only the source input. To allow the sequence model to inherit the promising capability of the EM-Network, we propose a new self-distillation strategy, where the original sequence model can benefit from the knowledge of the EM-Network in a one-stage manner. We conduct comprehensive experiments on two types of seq2seq models: connectionist temporal classification (CTC) for speech recognition and attention-based encoder-decoder (AED) for machine translation. Experimental results demonstrate that the EM-Network significantly advanc
    
[^5]: 平滑粗糙的边缘：评估自动生成的复合晶格转换

    Smoothing the Rough Edges: Evaluating Automatically Generated Multi-Lattice Transitions. (arXiv:2306.10055v1 [cs.LG])

    [http://arxiv.org/abs/2306.10055](http://arxiv.org/abs/2306.10055)

    本论文描述和评估了使用变分自编码器自动创建过渡晶格单元的方法，以解决多晶格结构中不同拓扑的单元晶格之间平滑转换的问题。

    

    添加制造技术在满足复杂设计要求的同时，生产轻量级的部件上具有非常大的优势。引入单元晶格单元和这些单元的渐变可以极大地增强这种能力。在部件加载在不同情况下变化的情况下，使用多个不同的单元晶格类型可能是有益的，这将导致多晶格结构。在这种结构中，单元晶格拓扑之间的突然转换可能会导致应力集中。

    Additive manufacturing is advantageous for producing lightweight components while addressing complex design requirements. This capability has been bolstered by the introduction of unit lattice cells and the gradation of those cells. In cases where loading varies throughout a part, it may be beneficial to use multiple, distinct lattice cell types, resulting in multi-lattice structures. In such structures, abrupt transitions between unit cell topologies may cause stress concentrations, making the boundary between unit cell types a primary failure point. Thus, these regions require careful design in order to ensure the overall functionality of the part. Although computational design approaches have been proposed, smooth transition regions are still difficult to achieve, especially between lattices of drastically different topologies. This work demonstrates and assesses a method for using variational autoencoders to automate the creation of transitional lattice cells, examining the factors
    
[^6]: 在推荐系统中插值项目和用户公平性

    Interpolating Item and User Fairness in Recommendation Systems. (arXiv:2306.10050v1 [cs.IR])

    [http://arxiv.org/abs/2306.10050](http://arxiv.org/abs/2306.10050)

    本文研究在推荐系统中平衡项目和用户公平性的框架，并通过低后悔的在线优化算法实现了维持收益同时实现公平推荐的目标。

    

    在多边平台中，平台与卖家（项目）和客户（用户）等各种各样的利益相关者互动，每个相关者都有自己的期望结果，寻找合适的平衡点变得非常复杂。在这项工作中，我们研究了“公平成本”，它捕捉了平台在平衡不同利益相关者利益时可能做出的妥协。出于这个目的，我们提出了一个公平推荐框架，其中平台在插值项目和用户公平性约束时最大化其收益。我们在一个更现实但具有挑战性的在线设置中进一步研究了公平推荐问题，在这种情况下，平台缺乏了解用户偏好的知识，只能观察二进制购买决策。为了解决这个问题，我们设计了一种低后悔的在线优化算法，它在维护平台收益的同时管理项目和用户公平性之间的权衡。我们的实验证明了我们提出的框架在实现公平推荐同时保持高收益方面的有效性。

    Online platforms employ recommendation systems to enhance customer engagement and drive revenue. However, in a multi-sided platform where the platform interacts with diverse stakeholders such as sellers (items) and customers (users), each with their own desired outcomes, finding an appropriate middle ground becomes a complex operational challenge. In this work, we investigate the ``price of fairness'', which captures the platform's potential compromises when balancing the interests of different stakeholders. Motivated by this, we propose a fair recommendation framework where the platform maximizes its revenue while interpolating between item and user fairness constraints. We further examine the fair recommendation problem in a more realistic yet challenging online setting, where the platform lacks knowledge of user preferences and can only observe binary purchase decisions. To address this, we design a low-regret online optimization algorithm that preserves the platform's revenue while
    
[^7]: 预测晶体性质的完整相互作用势的高效逼近

    Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10045](http://arxiv.org/abs/2306.10045)

    本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。

    

    我们研究了晶体材料的性质预测。晶体结构由一个最小的单元格组成，在三维空间中无限重复。如何在机器学习模型中准确表示这种重复结构仍然没有解决。当前的方法只在附近的节点之间建立边缘来构建图形，因此无法忠实地捕捉无限重复的模式和远距离的原子间相互作用。在这项工作中，我们提出了几个创新来克服这些限制。首先，我们建议直接建模物理原理的相互作用势，而不仅仅使用距离，如许多现有方法所做的。这些势包括库仑势，伦敦分散势和Pauli斥力势。其次，我们建模所有原子之间的完整势，而不仅仅是现有方法中的附近原子之间的势。这得益于我们用可证明的误差界逼近无限势和的方法。我们进一步开发了...

    We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
    
[^8]: 机器学习中异质性的交织轴解析以促进民主和包容性发展

    Unraveling the Interconnected Axes of Heterogeneity in Machine Learning for Democratic and Inclusive Advancements. (arXiv:2306.10043v1 [cs.CY])

    [http://arxiv.org/abs/2306.10043](http://arxiv.org/abs/2306.10043)

    本论文探讨了机器学习应用中的异质性问题，从价值观、数据组成和资源基础设施三个角度入手并指出它们相互依存，需要一同考虑解决。同时，本文还揭示了现有机器学习研究存在的问题，如权力集中和依赖性增加。

    

    机器学习在决策过程中的广泛应用引发了有关其对社会的利益的问题。本研究指出并分析了三个明显影响机器学习产品发展轨迹的异质性轴，它们是：价值观、文化和法规、数据组成以及资源和基础设施能力。我们展示了这些轴如何相互依存并相互影响，强调需要共同考虑和解决它们的必要性。不幸的是，目前的研究景观在这方面还有所不足，往往未能采用整体性方法。我们检查了偏向少数人的支配性偏见和方法学，以及由此导致的权力集中、同质化控制和增加的依赖性。我们讨论了这种三个轴的片面研究构成的显著挑战，导致一个缺乏反映现实情况的实际解荐空间。

    The growing utilization of machine learning (ML) in decision-making processes raises questions about its benefits to society. In this study, we identify and analyze three axes of heterogeneity that significantly influence the trajectory of ML products. These axes are i) values, culture and regulations, ii) data composition, and iii) resource and infrastructure capacity. We demonstrate how these axes are interdependent and mutually influence one another, emphasizing the need to consider and address them jointly. Unfortunately, the current research landscape falls short in this regard, often failing to adopt a holistic approach. We examine the prevalent practices and methodologies that skew these axes in favor of a selected few, resulting in power concentration, homogenized control, and increased dependency. We discuss how this fragmented study of the three axes poses a significant challenge, leading to an impractical solution space that lacks reflection of real-world scenarios. Addressi
    
[^9]: 基于Transformer的时空数据分析在揭示商业轨迹方面的应用研究

    Unlocking Insights into Business Trajectories with Transformer-based Spatio-temporal Data Analysis. (arXiv:2306.10034v1 [cs.IR])

    [http://arxiv.org/abs/2306.10034](http://arxiv.org/abs/2306.10034)

    本研究利用新闻文章数据，采用Transformer模型模拟商业轨迹，揭示商业趋势和行业表现。

    

    商业世界持续发展，保持领先需要深入理解市场趋势和业绩。本文通过模拟商业轨迹，利用新闻文章数据来满足这一需求。

    The world of business is constantly evolving and staying ahead of the curve requires a deep understanding of market trends and performance. This article addresses this requirement by modeling business trajectories using news articles data.
    
[^10]: 在Interspeech会议上研究可重复性：一种长期和比较的视角。

    Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective. (arXiv:2306.10033v1 [cs.DL])

    [http://arxiv.org/abs/2306.10033](http://arxiv.org/abs/2306.10033)

    该论文调查了跨越语音和语言处理学科的七个会议中，源代码可用性对于Interspeech会议要少于40%，并提出了建议以提高未来研究的可重复性。

    

    可重复性是横跨学科的科学进展的关键方面，降低开放科学的障碍是Interspeech 2023主题的焦点领域。源代码的可用性是促进可重复性的指标之一。然而，与领域内其他会议相比，Interspeech会议的再现率较低是我们鲜有了解的。为了填补这个空白，我们对跨越语音和语言处理学科的七个会议的27,717篇论文进行了调查。我们发现，尽管接受的论文数量与其他会议相近，但Interspeech的可用源代码较少，达到40%。除了报告我们在研究过程中遇到的困难，我们还提供了建议和可能的方向，以增加可重复性进行进一步研究。

    Reproducibility is a key aspect for scientific advancement across disciplines, and reducing barriers for open science is a focus area for the theme of Interspeech 2023. Availability of source code is one of the indicators that facilitates reproducibility. However, less is known about the rates of reproducibility at Interspeech conferences in comparison to other conferences in the field. In order to fill this gap, we have surveyed 27,717 papers at seven conferences across speech and language processing disciplines. We find that despite having a close number of accepted papers to the other conferences, Interspeech has up to 40% less source code availability. In addition to reporting the difficulties we have encountered during our research, we also provide recommendations and possible directions to increase reproducibility for further studies.
    
[^11]: 采用分层嵌入和会话属性的伪会话推荐

    Pseudo session-based recommendation with hierarchical embedding and session attributes. (arXiv:2306.10029v1 [cs.IR])

    [http://arxiv.org/abs/2306.10029](http://arxiv.org/abs/2306.10029)

    本文提出了一种新方法CoHHGN+，用于解决缺乏用户ID的电商网站数据的推荐问题。该方法使用了定义的伪会话以及包括价格、类别、性别和地区等用户信息，得到了较好的推荐结果。

    

    最近，由于隐私问题，电子商务网站无法为每个交易数据条目提供标识号（用户ID）。因为大多数推荐方法假定所有数据都被分配了用户ID，所以它们不能应用于没有用户ID的数据。最近研究了基于会话信息的会话推荐（SBR），该方法基于用户的短期行为信息。常规的SBR只使用与感兴趣的项目相关的信息来进行推荐（例如，在EC站点上使用项目ID）。特别是在EC网站的情况下，记录的数据包括被购买的物品名称、物品价格、类别层次结构以及用户的性别和地区。在本研究中，我们为没有用户ID和会话ID的EC网站的购买历史数据定义了伪会话。最后，我们提出了一种CoHHGN+会话推荐方法，它使用协同导向的异构超图和全局图网络。

    Recently, electronic commerce (EC) websites have been unable to provide an identification number (user ID) for each transaction data entry because of privacy issues. Because most recommendation methods assume that all data are assigned a user ID, they cannot be applied to the data without user IDs. Recently, session-based recommendation (SBR) based on session information, which is short-term behavioral information of users, has been studied. A general SBR uses only information about the item of interest to make a recommendation (e.g., item ID for an EC site). Particularly in the case of EC sites, the data recorded include the name of the item being purchased, the price of the item, the category hierarchy, and the gender and region of the user. In this study, we define a pseudo--session for the purchase history data of an EC site without user IDs and session IDs. Finally, we propose an SBR with a co-guided heterogeneous hypergraph and globalgraph network plus, called CoHHGN+. The result
    
[^12]: 基于图的长短期兴趣模型用于点击率预测

    Graph Based Long-Term And Short-Term Interest Model for Click-Through Rate Prediction. (arXiv:2306.10028v1 [cs.IR])

    [http://arxiv.org/abs/2306.10028](http://arxiv.org/abs/2306.10028)

    本论文提出了一种名为GLSM的基于图的长期和短期兴趣模型，可以很好地处理长期和短期用户兴趣数据的问题。

    

    点击率预测旨在预测用户点击项的概率，是在线推荐和广告系统中的关键任务之一。在这样的系统中，丰富的用户行为（即长期和短期）已被证明对捕捉用户兴趣非常有价值。工业界和学术界都对这个主题非常关注，并提出了不同的方法来建模长期和短期用户行为数据。但仍存在一些未解决的问题。更具体地说，（1）基于规则和截断的方法从长期行为中提取信息易导致信息丢失，（2）从短期行为中提取信息时单一反馈行为无论场景都会导致信息混淆和噪声。为了填补这一空白，我们提出了一种基于图的长期和短期兴趣模型，称为GLSM。它由一个多兴趣图结构组成，用于捕捉长期用户行为，一个多场景兴趣子图用于捕捉短期用户行为。

    Click-through rate (CTR) prediction aims to predict the probability that the user will click an item, which has been one of the key tasks in online recommender and advertising systems. In such systems, rich user behavior (viz. long- and short-term) has been proved to be of great value in capturing user interests. Both industry and academy have paid much attention to this topic and propose different approaches to modeling with long-term and short-term user behavior data. But there are still some unresolved issues. More specially, (1) rule and truncation based methods to extract information from long-term behavior are easy to cause information loss, and (2) single feedback behavior regardless of scenario to extract information from short-term behavior lead to information confusion and noise. To fill this gap, we propose a Graph based Long-term and Short-term interest Model, termed GLSM. It consists of a multi-interest graph structure for capturing long-term user behavior, a multi-scenari
    
[^13]: 对交错比较效率的理论分析

    Theoretical Analysis on the Efficiency of Interleaved Comparisons. (arXiv:2306.10023v1 [cs.IR])

    [http://arxiv.org/abs/2306.10023](http://arxiv.org/abs/2306.10023)

    本研究对交错比较方法的效率进行了理论分析，发现当用户根据相关性离开排名时，交错比较比A/B测试更为有效。

    

    本研究针对一种用于排名的高效在线评估方法——交错比较，进行了效率的理论分析。虽然交错比较已经应用于实际系统中，但其高效率的源头在文献中尚未得到明确的阐述。因此，本研究设计了一个类似于普通交错比较方法的简单交错比较方法，并探索了一种条件，该条件下交错比较方法比A/B测试更有效。其中的条件是，当用户根据物品的相关性来离开排名时（这是点击模型中的一个典型假设），这种情况就会出现。最后，我们还基于数值分析和用户模拟进行实验，证明了理论结果与实证结果是一致的。

    This study presents a theoretical analysis on the efficiency of interleaving, an efficient online evaluation method for rankings. Although interleaving has already been applied to production systems, the source of its high efficiency has not been clarified in the literature. Therefore, this study presents a theoretical analysis on the efficiency of interleaving methods. We begin by designing a simple interleaving method similar to ordinary interleaving methods. Then, we explore a condition under which the interleaving method is more efficient than A/B testing and find that this is the case when users leave the ranking depending on the item's relevance, a typical assumption made in click models. Finally, we perform experiments based on numerical analysis and user simulation, demonstrating that the theoretical results are consistent with the empirical results.
    
[^14]: CLIP2Protect：使用对抗性潜在搜索的文本引导化妆来保护面部隐私

    CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search. (arXiv:2306.10008v1 [cs.CV])

    [http://arxiv.org/abs/2306.10008](http://arxiv.org/abs/2306.10008)

    该论文提出了一种两步法面部隐私保护方法CLIP2Protect，使用对抗性潜在搜索结合文本引导化妆，生成高质量面部图像，从而保护面部隐私。

    

    基于深度学习的面部识别系统的成功已经引起了严重的隐私问题，因为它们能够在数字世界中启用未授权的用户跟踪。现有的隐私增强方法无法生成自然主义图像，既能保护面部隐私又不会损害用户体验。我们提出了一种新的面部隐私保护方法，采用两步法，依靠在预训练生成模型的低维流形中找到对抗性潜在编码。第一步将给定的面部图像反演成潜在空间中的编码，并微调生成模型，以从其潜在代码准确地重构给定的图像。这一步产生了一个良好的初始化，有助于生成类似于给定身份的高质量面部。随后，使用用户定义的化妆文本提示和保持身份的规范化来指导在潜在空间中寻找对抗性代码的搜索。广泛的实验表明，我们的方法能够生成具有高质量化妆的面部图像，可有效地保护面部隐私。

    The success of deep learning based face recognition systems has given rise to serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Existing methods for enhancing privacy fail to generate naturalistic images that can protect facial privacy without compromising user experience. We propose a novel two-step approach for facial privacy protection that relies on finding adversarial latent codes in the low-dimensional manifold of a pretrained generative model. The first step inverts the given face image into the latent space and finetunes the generative model to achieve an accurate reconstruction of the given image from its latent code. This step produces a good initialization, aiding the generation of high-quality faces that resemble the given identity. Subsequently, user-defined makeup text prompts and identity-preserving regularization are used to guide the search for adversarial codes in the latent space. Extensive experiments demons
    
[^15]: 用一致性检查评估超人模型

    Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])

    [http://arxiv.org/abs/2306.09983](http://arxiv.org/abs/2306.09983)

    本文提出了一个用一致性检查评估超人模型的框架，可以发现决策制定中的逻辑不一致性，即使对于超人模型的决策正确性可能是不可能评估的情况。

    

    如果机器学习模型在各种推理或决策任务上实现了超人能力，那么我们该如何评估这些模型，考虑到人类代理会产生偏差? 在本文中，我们提出了一个用一致性检查评估超人模型的框架。我们的前提是，虽然评估超人决策的正确性可能是不可能的，但是如果模型的决策未能满足某些逻辑上、可解释的规则，我们仍然可以发现错误。我们将我们的框架实现在三个任务上，这些任务的决策正确性由于超人模型能力或其他缺乏基本事实而难以评估：评估国际象棋局面、预测未来事件和作出法律判断。我们表明，无论模型在这些任务上的表现如何(可能是超人的)，我们都能发现决策制定中的逻辑不一致性。例如：国际象棋引擎给出对局中棋子相对估值的不同排列。

    If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
    
[^16]: 鸟鸣声的相关聚类

    Correlation Clustering of Bird Sounds. (arXiv:2306.09906v1 [cs.SD])

    [http://arxiv.org/abs/2306.09906](http://arxiv.org/abs/2306.09906)

    本文研究鸟声聚类问题，提出了一种通过学习训练集中记录对相关概率后，应用相关聚类于测试集进行聚类的方法，并比较了这种方法与测试集分类的准确性和相关性。同时，还探究了这种方法在应用于训练期间未听到的鸟类物种的记录以及分离鸟声和环境噪声方面的效果。

    

    鸟类声音分类是将任何声音记录与可以在记录中听到的鸟类物种相关联的任务。在这里，我们研究鸟声聚类，即决定任何一对声音记录是否可以听到相同的鸟类物种。我们首先从训练集中学习记录对按这种方式相关的概率，然后通过相关聚类来推断测试集的最大可能分区。我们解决以下问题：与测试集的分类相比，这种聚类的准确性如何？从分类获得的聚类如何与因此推断得出的聚类相关？在应用于训练期间未听到的鸟类物种的记录时，这种聚类的准确性如何？这种聚类在分离鸟鸣声和训练期间未听到的环境噪声方面有多有效？

    Bird sound classification is the task of relating any sound recording to those species of bird that can be heard in the recording. Here, we study bird sound clustering, the task of deciding for any pair of sound recordings whether the same species of bird can be heard in both. We address this problem by first learning, from a training set, probabilities of pairs of recordings being related in this way, and then inferring a maximally probable partition of a test set by correlation clustering. We address the following questions: How accurate is this clustering, compared to a classification of the test set? How do the clusters thus inferred relate to the clusters obtained by classification? How accurate is this clustering when applied to recordings of bird species not heard during training? How effective is this clustering in separating, from bird sounds, environmental noise not heard during training?
    
[^17]: 连续学习中基于记忆的方法的泛化性研究

    Studying Generalization on Memory-Based Methods in Continual Learning. (arXiv:2306.09890v1 [cs.LG])

    [http://arxiv.org/abs/2306.09890](http://arxiv.org/abs/2306.09890)

    本文研究了连续学习中基于记忆的方法的泛化性能，发现虽然这些方法可以帮助传统的内分布泛化，但它们可以通过学习虚假的特征和相关性来大大损害超出分布泛化，尤其是在线性分类器中。

    

    连续学习的目标之一是在一系列经验中不断学习新的概念，同时避免灾难性遗忘。为了减轻完全知识覆盖的问题，基于记忆的方法会存储一定比例的先前数据分布，在训练中使用。虽然这些方法产生了良好的结果，但很少有研究测试它们的超出分布泛化性能以及这些方法是否过度拟合重放记忆。在这项工作中，我们展示了尽管这些方法可以帮助传统的内分布泛化，但它们可以通过学习虚假的特征和相关性来大大损害超出分布泛化。使用控制环境，我们使用Synbol基准生成器（Lacoste等人，2020）展示了这种缺乏超出分布泛化主要出现在线性分类器中。

    One of the objectives of Continual Learning is to learn new concepts continually over a stream of experiences and at the same time avoid catastrophic forgetting. To mitigate complete knowledge overwriting, memory-based methods store a percentage of previous data distributions to be used during training. Although these methods produce good results, few studies have tested their out-of-distribution generalization properties, as well as whether these methods overfit the replay memory. In this work, we show that although these methods can help in traditional in-distribution generalization, they can strongly impair out-of-distribution generalization by learning spurious features and correlations. Using a controlled environment, the Synbol benchmark generator (Lacoste et al., 2020), we demonstrate that this lack of out-of-distribution generalization mainly occurs in the linear classifier.
    
[^18]: 可泛化的一次性绳索操作策略及其参数感知性。

    Generalizable One-shot Rope Manipulation with Parameter-Aware Policy. (arXiv:2306.09872v1 [cs.LG])

    [http://arxiv.org/abs/2306.09872](http://arxiv.org/abs/2306.09872)

    GenORM通过增加可变形绳索参数和使用各种可变形绳索的模拟训练操作策略，实现利用一次真实演示处理不同可形变绳索，从而节省演示时间和提高适用性。

    

    以绳索在运动过程中的固有不确定性为因素，以往绳索操作方法往往需要数百次真实演示来为每个绳索训练操作策略，即使是简单的“到达目标”任务，这限制了它们在我们不断变化的世界中的应用。为了解决这个问题，我们介绍了GenORM，一个框架，它可以让操作策略通过一次真实演示就可以处理不同可形变的绳索。我们通过在策略上增加可变形绳索参数并使用各种模拟可变形绳索来训练它，使策略能够根据不同的绳索参数调整行动。在推断时，GenORM通过最小化真实演示和模拟点云的网格密度差异来估计可变形绳索参数。通过可微分物理模拟器的帮助，我们仅需要一次演示数据就可以处理不同的绳索。

    Due to the inherent uncertainty in their deformability during motion, previous methods in rope manipulation often require hundreds of real-world demonstrations to train a manipulation policy for each rope, even for simple tasks such as rope goal reaching, which hinder their applications in our ever-changing world. To address this issue, we introduce GenORM, a framework that allows the manipulation policy to handle different deformable ropes with a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable rope parameters and training it with a diverse range of simulated deformable ropes so that the policy can adjust actions based on different rope parameters. At the time of inference, given a new rope, GenORM estimates the deformable rope parameters by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations. With the help of a differentiable physics simulator, we require only a single r
    
[^19]: 多视角分类增量学习

    Multi-View Class Incremental Learning. (arXiv:2306.09675v1 [cs.LG])

    [http://arxiv.org/abs/2306.09675](http://arxiv.org/abs/2306.09675)

    本文提出了一种名为多视角分类增量学习（MVCIL）的新模型，该模型使用随机化的表示学习技术进行特征提取，并提出正交融合子空间和选择性权重合并来解决增量学习中遗忘旧信息和学习新概念的挑战。实验结果表明该方法相比最新方法有效性更高。

    

    多视角学习（MVL）在整合数据集的多个视角以提高下游任务性能方面取得了巨大成功。为了使MVL方法在开放式环境中更实用，本文研究了一种新的范例，称为多视角分类增量学习（MVCIL），其中单个模型从连续的视图流中逐步分类新类，不需要访问早期数据的视图。但是，MVCIL面临着老信息的灾难性遗忘和学习新概念的干扰。为了解决这个问题，我们首先开发了一种基于随机化的表示学习技术，用于特征提取，以保证它们在工作状态下的分离视图最优，其中属于类的多个视图按顺序呈现；然后，我们将它们逐个集成到由提取的特征跨越的正交融合子空间中；最后，我们介绍选择性权重合并，以保留旧类的知识。基准数据集上的实验结果证明了我们提出的方法相对于最新方法的有效性。

    Multi-view learning (MVL) has gained great success in integrating information from multiple perspectives of a dataset to improve downstream task performance. To make MVL methods more practical in an open-ended environment, this paper investigates a novel paradigm called multi-view class incremental learning (MVCIL), where a single model incrementally classifies new classes from a continual stream of views, requiring no access to earlier views of data. However, MVCIL is challenged by the catastrophic forgetting of old information and the interference with learning new concepts. To address this, we first develop a randomization-based representation learning technique serving for feature extraction to guarantee their separate view-optimal working states, during which multiple views belonging to a class are presented sequentially; Then, we integrate them one by one in the orthogonality fusion subspace spanned by the extracted features; Finally, we introduce selective weight consolidation f
    
[^20]: 虚假黎明：重新评估谷歌强化学习在芯片宏观布局中的应用

    The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])

    [http://arxiv.org/abs/2306.09633](http://arxiv.org/abs/2306.09633)

    谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。

    

    谷歌2021年在《自然》杂志上发表的有关使用强化学习设计芯片的论文，因为所声称的结果缺乏充分的文件记录和关键步骤的说明，引发争议并受到媒体的批评报道。 而两项独立的评估填补了空白，证明谷歌强化学习落后于人类设计师、落后于一种众所周知的算法（模拟退火），并且还落后于普遍可用的商业软件。交叉检查的数据表明，由于行为、分析和报告中的错误，该《自然》文章的完整性受到了严重的损害。

    Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
    
[^21]: 从超图能量函数到超图神经网络

    From Hypergraph Energy Functions to Hypergraph Neural Networks. (arXiv:2306.09623v1 [cs.LG])

    [http://arxiv.org/abs/2306.09623](http://arxiv.org/abs/2306.09623)

    本文提出了一种新的基于超图能量函数的节点嵌入方法，可以通过双层优化实现节点分类任务，相比传统GNN模型有更好的表现。

    

    超图是表示实体之间高阶交互的强大抽象模型。为了在实现下游预测的过程中利用这些关系，最近提出了多种超图神经网络架构，这些架构在很大程度上是建立在传统图神经网络（GNN）文献的先驱上的。在这篇论文中，我们首先介绍了一类具有表达能力的参数化超图正则化能量函数。然后，我们演示了如何将这些能量的极小值有效地作为节点嵌入器，再配合一个参数化分类器进行端到端训练，通过一个监督的双层优化过程实现。之后，我们发现了建议的双层超图优化中出现的预测模型的隐式架构和常用GNN架构之间的相似之处。在实证方面，我们在各种超图节点分类任务中展示了最先进的结果。

    Hypergraphs are a powerful abstraction for representing higher-order interactions between entities of interest. To exploit these relationships in making downstream predictions, a variety of hypergraph neural network architectures have recently been proposed, in large part building upon precursors from the more traditional graph neural network (GNN) literature. Somewhat differently, in this paper we begin by presenting an expressive family of parameterized, hypergraph-regularized energy functions. We then demonstrate how minimizers of these energies effectively serve as node embeddings that, when paired with a parameterized classifier, can be trained end-to-end via a supervised bilevel optimization process. Later, we draw parallels between the implicit architecture of the predictive models emerging from the proposed bilevel hypergraph optimization, and existing GNN architectures in common use. Empirically, we demonstrate state-of-the-art results on various hypergraph node classification
    
[^22]: 实用联邦因果结构学习之路

    Towards Practical Federated Causal Structure Learning. (arXiv:2306.09433v1 [cs.LG])

    [http://arxiv.org/abs/2306.09433](http://arxiv.org/abs/2306.09433)

    为了解决联邦学习条件下的因果结构学习难题，提出了一种基于联邦条件独立性检验的因果结构学习方案FedC2SL，无需收集原始数据且对数据变异具有更强的抵抗力。

    

    理解因果关系对于科学发现至关重要。因果结构学习的过程涉及从观测数据中识别因果图以理解这种关系。通常，一个中央服务器执行此任务，但与服务器共享数据会带来隐私风险。联邦学习可以解决这个问题，但现有的联邦因果结构学习解决方案对数据做出了不切实际的假设，并缺乏收敛保证。FedC2SL是一种联邦基于约束的因果结构学习方案，它使用联邦条件独立性检验来学习因果图，该检验在不收集客户端原始数据的情况下检查两个变量在一组条件下的条件独立性。FedC2SL对数据做出了更弱和更现实的假设，并更强地抵御了客户端之间的数据变异。FedPC和FedFCI是FedC2SL的两个变体，用于因果充分性和因果不充分性情况下的因果结构学习。

    Understanding causal relations is vital in scientific discovery. The process of causal structure learning involves identifying causal graphs from observational data to understand such relations. Usually, a central server performs this task, but sharing data with the server poses privacy risks. Federated learning can solve this problem, but existing solutions for federated causal structure learning make unrealistic assumptions about data and lack convergence guarantees. FedC2SL is a federated constraint-based causal structure learning scheme that learns causal graphs using a federated conditional independence test, which examines conditional independence between two variables under a condition set without collecting raw data from clients. FedC2SL requires weaker and more realistic assumptions about data and offers stronger resistance to data variability among clients. FedPC and FedFCI are the two variants of FedC2SL for causal structure learning in causal sufficiency and causal insuffic
    
[^23]: 基于深度学习的高内容细胞成像多组学预测

    Multi-omics Prediction from High-content Cellular Imaging with Deep Learning. (arXiv:2306.09391v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.09391](http://arxiv.org/abs/2306.09391)

    本研究使用深度学习方法，从高内容细胞成像中直接预测细胞群体的多组学。实验结果表明，该方法能够在多种刺激条件下实现显著成果，为细胞组学领域提供了新的方法。

    

    高内容细胞成像、转录组学和蛋白质组学数据为影响细胞状态和功能的生物分子层提供了丰富和互补的视角。但是，尚未系统地探讨多组学测量值影响细胞形态的生物学决定因素，因此目前尚不清楚细胞成像是否能够直接预测多组学。在这里，我们探讨了使用Image2Omics——一种深度学习方法——直接从用多重荧光染料染色的高内容图像中预测细胞群体的多组学是否可能。我们在多种刺激条件下的人类诱导多能干细胞（hiPSC）衍生的基因编辑巨噬细胞中进行实验评估，并证明Image2Omics取得了显著的成果。

    High-content cellular imaging, transcriptomics, and proteomics data provide rich and complementary views on the molecular layers of biology that influence cellular states and function. However, the biological determinants through which changes in multi-omics measurements influence cellular morphology have not yet been systematically explored, and the degree to which cell imaging could potentially enable the prediction of multi-omics directly from cell imaging data is therefore currently unclear. Here, we address the question of whether it is possible to predict bulk multi-omics measurements directly from cell images using Image2Omics -- a deep learning approach that predicts multi-omics in a cell population directly from high-content images stained with multiplexed fluorescent dyes. We perform an experimental evaluation in gene-edited macrophages derived from human induced pluripotent stem cell (hiPSC) under multiple stimulation conditions and demonstrate that Image2Omics achieves sign
    
[^24]: 基于时空扩展图神经网络的人类移动模拟

    Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation. (arXiv:2306.09381v1 [cs.LG])

    [http://arxiv.org/abs/2306.09381](http://arxiv.org/abs/2306.09381)

    STAR框架通过设计多种时空图来捕捉位置的动态时空效应，为人类移动模拟任务提供新的方法。

    

    人类移动模式在政策决策和经济行为研究中有着重要的应用。人类移动模拟任务旨在给定一小组轨迹数据生成人类移动轨迹，但由于人类移动数据的稀缺性和稀疏性，引起了广泛关注。现有方法大多依赖于地点之间的静态关系，而很大程度上忽略了位置的动态时空效应。因此，我们提出了一种新的框架，即SpatioTemporal-Augmented gRaph神经网络（STAR），来模拟位置的动态时空效应。

    Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal co
    
[^25]: 公平的多任务学习

    Equitable Multi-task Learning. (arXiv:2306.09373v1 [cs.LG])

    [http://arxiv.org/abs/2306.09373](http://arxiv.org/abs/2306.09373)

    该论文提出了一种名为EMTL的多任务优化方法，以实现公平的多任务学习。通过规范化不同任务的相对贡献，可以提高MTL的泛化性能，并利用方差正则化和高效的优化算法保证收敛。实验证明，该方法在合成和真实数据集上均表现出了更好的性能。

    

    多任务学习（MTL）在各个研究领域（如计算机视觉、自然语言处理和信息检索等）中取得了巨大成功。但是，由于任务之间存在复杂且相互竞争的相关性，单纯地训练所有任务可能会导致不公平的学习，即一些任务被很好地学习，而其他任务则被忽视。多任务优化（MTO）旨在同时提高所有任务的表现，但传统方法往往在任务损失规模或梯度范数差异较大的情况下表现不佳。为了解决这个问题，我们深入研究了MTL的公平性问题，并发现在更新共享参数时，规范化不同任务的相对贡献（即任务特定损失值除以其原始梯度范数的值）可以提高MTL的泛化性能。基于我们的理论分析，我们提出了一种新的多任务优化方法，名为EMTL，以实现公平的MTL。具体来说，我们有效地添加了方差正则化，使不同任务的相对贡献更具可比性，并开发了一种高效的优化算法来保证收敛。我们在合成和真实数据集上进行了大量实验，结果表明了我们方法的有效性和优越性。

    Multi-task learning (MTL) has achieved great success in various research domains, such as CV, NLP and IR etc. Due to the complex and competing task correlation, na\"ive training all tasks may lead to inequitable learning, \textit{i.e.} some tasks are learned well while others are overlooked. Multi-task optimization (MTO) aims to improve all tasks at same time, but conventional methods often perform poor when tasks with large loss scale or gradient norm magnitude difference. To solve the issue, we in-depth investigate the equity problem for MTL and find that regularizing relative contribution of different tasks (\textit{i.e.} value of task-specific loss divides its raw gradient norm) in updating shared parameter can improve generalization performance of MTL. Based on our theoretical analysis, we propose a novel multi-task optimization method, named \textit{EMTL}, to achieve equitable MTL. Specifically, we efficiently add variance regularization to make different tasks' relative contribu
    
[^26]: OpenOOD v1.5：增强的OCC（Out-of-Distribution Detection）基准测试

    OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection. (arXiv:2306.09301v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.09301](http://arxiv.org/abs/2306.09301)

    OpenOOD v1.5 是对前身的重大改进，将OCC检测方法的评估能力扩展到大规模数据集，调查了全光谱OCC检测，引入了在线排行榜和易于使用的评估器等新功能，并提供了深入的分析和实验结果的见解。

    

    OCC检测对于开放世界智能系统的可靠运行至关重要。虽然出现了越来越多的OCC检测方法，但评估不一致性仍然存在挑战，难以跟踪该领域的进展。本文介绍了OpenOOD v1.5，这是对前身的重大改进，确保OCC检测方法的准确、标准化和用户友好的评估。值得注意的是，OpenOOD v1.5将其评估能力扩展到大规模数据集，如ImageNet。此外，它还调查了全光谱OCC检测，引入了在线排行榜和易于使用的评估器等新功能。该工作还提供了深入的分析和综合实验结果的见解，从而丰富了知识库。

    Out-of-Distribution (OOD) detection is critical for the reliable operation of open-world intelligent systems. Despite the emergence of an increasing number of OOD detection methods, the evaluation inconsistencies present challenges for tracking the progress in this field. OpenOOD v1 initiated the unification of the OOD detection evaluation but faced limitations in scalability and usability. In response, this paper presents OpenOOD v1.5, a significant improvement from its predecessor that ensures accurate, standardized, and user-friendly evaluation of OOD detection methodologies. Notably, OpenOOD v1.5 extends its evaluation capabilities to large-scale datasets such as ImageNet, investigates full-spectrum OOD detection which is important yet underexplored, and introduces new features including an online leaderboard and an easy-to-use evaluator. This work also contributes in-depth analysis and insights derived from comprehensive experimental results, thereby enriching the knowledge pool o
    
[^27]: RecFusion：基于二项式扩散过程的1D数据推荐模型

    RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation. (arXiv:2306.08947v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2306.08947](http://arxiv.org/abs/2306.08947)

    本文提出了 RecFusion，一种特定针对1D和/或二进制设置的推荐模型方法，其利用了二项式扩散过程对二元用户-项目交互进行显式建模，并在核心推荐设置和最常见的数据集上接近复杂的VAE基线的表现。

    

    本文提出了RecFusion，这是一组用于推荐的扩散模型。不同于包含空间相关性的图像数据，常用于推荐的用户-项目交互矩阵缺乏用户和项目之间的空间关系。我们在一个一维向量上制定了扩散方法，并提出了二项式扩散，这个方法利用了伯努利过程显式地对二元用户-项目交互进行建模。我们展示了RecFusion在核心推荐设置（针对二进制非顺序反馈的前n项推荐）和最常见的数据集（MovieLens和Netflix）上接近于复杂的VAE基线的表现。我们提出的专门针对1D和/或二进制设置的扩散模型的意义超出了推荐系统，例如在医学领域中使用MRI和CT扫描。

    In this paper we propose RecFusion, which comprise a set of diffusion models for recommendation. Unlike image data which contain spatial correlations, a user-item interaction matrix, commonly utilized in recommendation, lacks spatial relationships between users and items. We formulate diffusion on a 1D vector and propose binomial diffusion, which explicitly models binary user-item interactions with a Bernoulli process. We show that RecFusion approaches the performance of complex VAE baselines on the core recommendation setting (top-n recommendation for binary non-sequential feedback) and the most common datasets (MovieLens and Netflix). Our proposed diffusion models that are specialized for 1D and/or binary setups have implications beyond recommendation systems, such as in the medical domain with MRI and CT scans.
    
[^28]: 面向动态MEC的可扩展资源管理：一种无监督的链路输出图神经网络方法。

    Scalable Resource Management for Dynamic MEC: An Unsupervised Link-Output Graph Neural Network Approach. (arXiv:2306.08938v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2306.08938](http://arxiv.org/abs/2306.08938)

    本文提出了一种面向动态MEC的资源管理方法，使用了无监督的链路输出图神经网络来对任意数量的边缘节点进行灵活的资源分配优化，具有极低的算法推理延迟。

    

    深度学习成功地应用于移动边缘计算（MEC）的任务卸载和资源分配优化中。然而，边缘网络的动态性在神经网络（NN）优化方法中导致了两个挑战：低可扩展性和高训练成本。尽管传统的节点输出图神经网络（GNN）在网络规模扩展时能够提取边缘节点的特征，但它们在处理维度决策空间随网络规模扩展而变化的新可扩展性问题时会失败。针对这个问题，本文提出了一种基于链路输出GNN（LOGNN）的资源管理方法，用于对MEC中任意数量的边缘节点进行灵活的资源分配优化，并具有极低的算法推理延迟。此外，采用无需标签的无监督方法来高效地训练LOGNN，其中明确地推导出了边缘任务处理延迟相对于LOGNN参数的梯度。此外，本文还给出了一个理论上的推导，证明LOGNN的收敛性和泛化性。

    Deep learning has been successfully adopted in mobile edge computing (MEC) to optimize task offloading and resource allocation. However, the dynamics of edge networks raise two challenges in neural network (NN)-based optimization methods: low scalability and high training costs. Although conventional node-output graph neural networks (GNN) can extract features of edge nodes when the network scales, they fail to handle a new scalability issue whereas the dimension of the decision space may change as the network scales. To address the issue, in this paper, a novel link-output GNN (LOGNN)-based resource management approach is proposed to flexibly optimize the resource allocation in MEC for an arbitrary number of edge nodes with extremely low algorithm inference delay. Moreover, a label-free unsupervised method is applied to train the LOGNN efficiently, where the gradient of edge tasks processing delay with respect to the LOGNN parameters is derived explicitly. In addition, a theoretical a
    
[^29]: PLAN: 方差感知的差分隐私均值估计

    PLAN: Variance-Aware Private Mean Estimation. (arXiv:2306.08745v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.08745](http://arxiv.org/abs/2306.08745)

    本文提出了“隐私限制适应噪声”（PLAN），是一组差分隐私算法，用于在输入的数据集结构中进行更好的均值估计。PLAN将噪声的形状量身定制为数据的形状，不同于以往的均值估计算法，而且可以在一些集中分布的情况下，通过利用标准差的偏斜来获得接近零平均均方误差（MSE）的估计。

    

    差分隐私均值估计是数据分析和机器学习中保护隐私的算法的重要组成部分。然而，虽然在最坏情况下隐私和效用之间的权衡已经被很好地理解，但许多数据集展示了可能被利用以产生更好算法的结构。在本文中，我们提出了“隐私限制适应噪声”（PLAN）。PLAN是一组差分隐私算法，用于在独立采样于分布$\mathcal{D}$的输入的设置中进行均值估计，其中分布的坐标标准差$\boldsymbol{\sigma}\in \mathbf{R}^d$。与Mahalanobis距离下的均值估计类似，PLAN将噪声的形状量身定制为数据的形状，但与以前的算法不同，隐私预算不是均匀地花费在各个坐标上。在对$\mathcal{D}$的集中性假设下，我们展示了如何利用向量$\boldsymbol{\sigma}$中的偏斜，从而获得接近零平均均方误差（MSE）的估计。

    Differentially private mean estimation is an important building block in privacy-preserving algorithms for data analysis and machine learning. Though the trade-off between privacy and utility is well understood in the worst case, many datasets exhibit structure that could potentially be exploited to yield better algorithms. In this paper we present $\textit{Private Limit Adapted Noise}$ (PLAN), a family of differentially private algorithms for mean estimation in the setting where inputs are independently sampled from a distribution $\mathcal{D}$ over $\mathbf{R}^d$, with coordinate-wise standard deviations $\boldsymbol{\sigma} \in \mathbf{R}^d$. Similar to mean estimation under Mahalanobis distance, PLAN tailors the shape of the noise to the shape of the data, but unlike previous algorithms the privacy budget is spent non-uniformly over the coordinates. Under a concentration assumption on $\mathcal{D}$, we show how to exploit skew in the vector $\boldsymbol{\sigma}$, obtaining a (zero-
    
[^30]: 当评分很重要时的学习排序

    Learning to Rank when Grades Matter. (arXiv:2306.08650v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2306.08650](http://arxiv.org/abs/2306.08650)

    本论文提出了一种学习排序和成绩预测的多目标函数，可以在真实应用中更好地平衡排序和成绩，优于现有方法。

    

    在真实世界中的学习排序应用中，分级标签广泛存在，特别是在人工标注的相关性数据中。传统的学习排序技术旨在优化文件的排序顺序。然而，它们通常忽略了实际分数的预测。这使它们无法在需要考虑分数的应用程序中被采用，例如筛选“劣质”文件。在良好的排序性能和良好的等级预测性能之间取得平衡仍然是一个未经探索的问题。现有的研究要么只关注排序性能而不校准模型输出，要么将成绩视为数值，假设标签在线性范围内，并未利用序数级别信息。本文对学习排序与成绩进行了深入研究，同时重视排序性能和等级预测性能。我们提供了关于如何使用非标量分级进行排名的形式化讨论，并提出了一个多目标函数，用于联合优化排序和成绩预测性能。基于两个真实数据集的实验表明，我们的方法在排序和等级预测性能方面均显著优于现有方法。

    Graded labels are ubiquitous in real-world learning-to-rank applications, especially in human rated relevance data. Traditional learning-to-rank techniques aim to optimize the ranked order of documents. They typically, however, ignore predicting actual grades. This prevents them from being adopted in applications where grades matter, such as filtering out ``poor'' documents. Achieving both good ranking performance and good grade prediction performance is still an under-explored problem. Existing research either focuses only on ranking performance by not calibrating model outputs, or treats grades as numerical values, assuming labels are on a linear scale and failing to leverage the ordinal grade information. In this paper, we conduct a rigorous study of learning to rank with grades, where both ranking performance and grade prediction performance are important. We provide a formal discussion on how to perform ranking with non-scalar predictions for grades, and propose a multiobjective f
    
[^31]: 语言转奖励：用于机器人技能综合的方法

    Language to Rewards for Robotic Skill Synthesis. (arXiv:2306.08647v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.08647](http://arxiv.org/abs/2306.08647)

    该研究介绍了一种将大型语言模型用于定义奖励参数，并通过奖励函数进行优化，从而使机器人可以执行各种自然语言指令指定的任务的新方法。

    

    大型语言模型已经取得了许多令人兴奋的进展，从逻辑推理到代码编写等，展现了在情境学习中获得多种新能力的潜力。机器人学研究人员也探索使用大型语言模型来提高机器人控制的能力。然而，由于低级机器人动作取决于硬件并且在大型语言模型的训练语料库中所占的比重较小，因此在机器人学中应用大型语言模型的现有努力主要将其视为语义规划器，或依赖于人工控制原语与机器人进行交互。另一方面，奖励函数被证明是可以灵活表示并且可以被优化以实现多种任务的控制策略，其语义丰富性使其适合由大型语言模型来指定。在本文中，我们引入了一种新的方法，通过利用大型语言模型来定义可以被优化的奖励参数并完成各种机器人任务。使用奖励作为中间介质，我们提出的方法使机器人能够执行各种由自然语言指令指定的任务，而无需人类在设计行为原语方面付出努力。我们在仿真环境中展示了我们的方法在拾取物品和搭建塔两个任务上的有效性。

    Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate inter
    
[^32]: 面向域感知的光学相干断层扫描噪音降低的少样本学习

    Domain-Aware Few-Shot Learning for Optical Coherence Tomography Noise Reduction. (arXiv:2306.08102v1 [eess.IV])

    [http://arxiv.org/abs/2306.08102](http://arxiv.org/abs/2306.08102)

    本文提出了一种少样本监督学习框架，用于光学相干断层扫描噪音降低，并成功推广到不同成像领域。

    

    散斑噪声一直是医学成像中一个被广泛研究的问题。近年来，利用深度学习方法进行降噪方面取得了重大进展。然而，将有监督的学习模型适应到新领域仍然是一个具有挑战性的问题。本文提出了一种少样本监督学习框架，用于光学相干断层扫描噪音降低，大大提高了训练速度，只需要单张图像或部分图像以及相应的去斑地面实况进行训练。此外，我们对光学相干断层扫描不同成像系统的域转移问题进行了制定，并证明了我们提出的方法适应不同成像领域的有效性。我们的方法在合成和真实的光学相干断层扫描数据集上均优于现有方法，展示了它很好地推广到新领域的能力。

    Speckle noise has long been an extensively studied problem in medical imaging. In recent years, there have been significant advances in leveraging deep learning methods for noise reduction. Nevertheless, adaptation of supervised learning models to unseen domains remains a challenging problem. Specifically, deep neural networks (DNNs) trained for computational imaging tasks are vulnerable to changes in the acquisition system's physical parameters, such as: sampling space, resolution, and contrast. Even within the same acquisition system, performance degrades across datasets of different biological tissues. In this work, we propose a few-shot supervised learning framework for optical coherence tomography (OCT) noise reduction, that offers a dramatic increase in training speed and requires only a single image, or part of an image, and a corresponding speckle suppressed ground truth, for training. Furthermore, we formulate the domain shift problem for OCT diverse imaging systems, and prove
    
[^33]: VISION数据集：用于基于视觉的工业检测的基准测试

    VISION Datasets: A Benchmark for Vision-based InduStrial InspectiON. (arXiv:2306.07890v1 [cs.CV])

    [http://arxiv.org/abs/2306.07890](http://arxiv.org/abs/2306.07890)

    本论文介绍了VISION数据集，这是一个多元化集合，包含14个不同工业领域检测数据集，提供了注释墨水和实例分割注释，并适用于多种检测方法。VISION数据集可以应对现实世界中工业领域检测中的挑战，并有望促进基于视觉的工业检测的发展。

    

    尽管基于视觉的检测算法取得了一定的进展，但现实世界中工业挑战——特别是数据可用性、质量和复杂的生产要求——往往仍未得到解决。我们介绍了VISION数据集，这是一个包含14个不同工业领域检测数据集的多元化集合，独具优势可以应对这些挑战。与以往的数据集不同，VISION为缺陷检测提供丰富的数据，包括所有数据分类的注释掩模，并且适用于多种检测方法。我们的数据集还具有实例分割注释功能，可以精确地识别缺陷。VISION通过提供18000张图像，包含44种缺陷类型，致力于反映出各种现实工业生产场景。通过支持VISION数据集上两个正在进行的竞赛，我们希望促进基于视觉的工业检测的进一步发展。

    Despite progress in vision-based inspection algorithms, real-world industrial challenges -- specifically in data availability, quality, and complex production requirements -- often remain under-addressed. We introduce the VISION Datasets, a diverse collection of 14 industrial inspection datasets, uniquely poised to meet these challenges. Unlike previous datasets, VISION brings versatility to defect detection, offering annotation masks across all splits and catering to various detection methodologies. Our datasets also feature instance-segmentation annotation, enabling precise defect identification. With a total of 18k images encompassing 44 defect types, VISION strives to mirror a wide range of real-world production scenarios. By supporting two ongoing challenge competitions on the VISION Datasets, we hope to foster further advancements in vision-based industrial inspection.
    
[^34]: 组合等变表示学习

    Compositionally Equivariant Representation Learning. (arXiv:2306.07783v1 [cs.CV])

    [http://arxiv.org/abs/2306.07783](http://arxiv.org/abs/2306.07783)

    本文研究了利用组合性来学习更具可解释性和泛化性的医学图像分割表示，提出了一种将组合等变性质纳入表示学习中的方法，并在各种医学图像分割任务上展示了其对泛化性能的提升。

    

    深度学习模型通常需要充分的监督（标记数据）才能有效地进行训练。相比之下，人类可以迅速学习识别医学图像（如磁共振和 CT 扫描中的重要解剖结构），只需要少量的指导。这种识别能力容易泛化到来自不同医疗机构的新图像以及不同设置中的新任务。这种快速且泛化的学习能力很大程度上是由于人脑中图像模式的组合结构，而当前的医学模型并不能很好地表示出这种结构。在本文中，我们研究利用组合性来学习更具有可解释性和泛化性的医学图像分割表示。具体而言，我们提出了一个假设：用于生成医学图像的基础生成因素满足组合等变性质，其中每个因素都是组合的（例如对应于人体解剖结构）并且对任务是等变的。因此，我们引入了一种方法来将组合等变性质纳入表示学习中，并展示了这种方法在各种医学图像分割任务上提高了泛化性能。

    Deep learning models often need sufficient supervision (i.e. labelled data) in order to be trained effectively. By contrast, humans can swiftly learn to identify important anatomy in medical images like MRI and CT scans, with minimal guidance. This recognition capability easily generalises to new images from different medical facilities and to new tasks in different settings. This rapid and generalisable learning ability is largely due to the compositional structure of image patterns in the human brain, which are not well represented in current medical models. In this paper, we study the utilisation of compositionality in learning more interpretable and generalisable representations for medical image segmentation. Overall, we propose that the underlying generative factors that are used to generate the medical images satisfy compositional equivariance property, where each factor is compositional (e.g. corresponds to the structures in human anatomy) and also equivariant to the task. Henc
    
[^35]: SHAP解释的持续解释协议

    A Protocol for Continual Explanation of SHAP. (arXiv:2306.07218v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.07218](http://arxiv.org/abs/2306.07218)

    本文研究了在持续学习中SHAP值解释的稳定性问题，提出了一种评估协议，并指出随机循环模型是更有效的备选循环方法。

    

    持续学习旨在在数据流中训练模型，以学习新信息而不会忘记先前的知识。鉴于这种环境的动态性质，解释这些模型的预测可能具有挑战性。我们研究了 SHAP 值解释在持续学习中的行为，并提出了一种评估协议，以可靠地评估逐类增量场景中解释的变化。我们观察到，虽然重放策略可以强制前馈/卷积模型中的 SHAP 值的稳定性，但它们无法在完全训练的循环模型中做到这一点。我们表明，像随机循环模型这样的备选循环方法在随时间保持解释稳定方面更有效。

    Continual Learning trains models on a stream of data, with the aim of learning new information without forgetting previous knowledge. Given the dynamic nature of such environments, explaining the predictions of these models can be challenging. We study the behavior of SHAP values explanations in Continual Learning and propose an evaluation protocol to robustly assess the change of explanations in Class-Incremental scenarios. We observed that, while Replay strategies enforce the stability of SHAP values in feedforward/convolutional models, they are not able to do the same with fully-trained recurrent models. We show that alternative recurrent approaches, like randomized recurrent models, are more effective in keeping the explanations stable over time.
    
[^36]: 基于时空自举的相关时间序列自监督表示学习

    Correlated Time Series Self-Supervised Representation Learning via Spatiotemporal Bootstrapping. (arXiv:2306.06994v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06994](http://arxiv.org/abs/2306.06994)

    该论文提出了一种基于时空自举的时间步级表示学习框架，能为相关时间序列分析提供高效降维表示，并通过在PeMS-BAY数据集上的测试取得了较好的效果。

    

    相关时间序列分析在许多实际工业中起着重要作用。对大规模数据进行高效降维表示以便进行下游任务是必要但具有挑战性的。本文提出了一种基于时空自举表示预测的时间步级表示学习框架，以便为个体实例进行表示学习。我们在相关时间序列预测和将预测模型冷启动转移到数据受限的新实例方面评估了该表示学习框架的有效性和灵活性。经过训练在学习表示上的线性回归模型证明了在大多数情况下我们的模型表现最佳。特别地，在与表示学习模型的比较中，我们在PeMS-BAY数据集上将RMSE、MAE和MAPE分别减少了37％、49％和48％。在实际的地铁客流数据中，我们的框架展示了将预测能力转移到新的冷启动情况下的能力。

    Correlated time series analysis plays an important role in many real-world industries. Learning an efficient representation of this large-scale data for further downstream tasks is necessary but challenging. In this paper, we propose a time-step-level representation learning framework for individual instances via bootstrapped spatiotemporal representation prediction. We evaluated the effectiveness and flexibility of our representation learning framework on correlated time series forecasting and cold-start transferring the forecasting model to new instances with limited data. A linear regression model trained on top of the learned representations demonstrates our model performs best in most cases. Especially compared to representation learning models, we reduce the RMSE, MAE, and MAPE by 37%, 49%, and 48% on the PeMS-BAY dataset, respectively. Furthermore, in real-world metro passenger flow data, our framework demonstrates the ability to transfer to infer future information of new cold-
    
[^37]: 图神经网络的局部到全局视角

    Local-to-global Perspectives on Graph Neural Networks. (arXiv:2306.06547v1 [cs.LG])

    [http://arxiv.org/abs/2306.06547](http://arxiv.org/abs/2306.06547)

    本文提出了局部到全局的图神经网络模型，包括不变图网络、局部信息传递神经网络和全局图变换器，并研究其收敛性质和在图粗化中的应用。

    

    本文提出了一种对于图神经网络（GNN）的局部到全局的视角，其中分为局部信息传递神经网络（MPNN）和全局图变换器。本文提出了三个工作：1）研究一种全局 GNN，不变图网络的收敛性质，2）连接局部 MPNN 和全局图变换器，3）在全局建模中，使用局部 MPNN 进行图粗化，这是一个常见的子程序。

    We present a local-to-global perspective on graph neural networks (GNN), which are categorized as local Message Passing Neural Networks (MPNN) and global Graph Transformer. We present three pieces of work: 1) study the convergence property of a type of global GNN, Invariant Graph Networks, 2) connect the local MPNN and global Graph Transformer, and 3) use local MPNN for graph coarsening, a common subroutine used in global modeling.
    
[^38]: 确定和探索智能空间

    Defining and Explorting the Intelligence Space. (arXiv:2306.06499v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.06499](http://arxiv.org/abs/2306.06499)

    本文通过引入广泛视角来界定智能并建立了三级嵌套结构及其基础的广泛空间。利用这些定义初步探索了奇点、生成AI、伦理和知识产权等话题。

    

    尽管尝试了许多次，智能是一个难以定义的概念。本文引入了一个广泛的视角来定义智能, 提出了一系列定义，构建了三个层次的智能嵌套层次和以它们及其近似值为基础的广泛空间。在这个智能空间中，鉴别出对应于自然——尤其是人类——智能和人工智能（AI）以及类人智能的区域。然后，利用这些定义初步探索了四个更先进、可能更具争议性的话题：奇点、生成AI、伦理和知识产权。

    Intelligence is a difficult concept to define, despite many attempts at doing so. Rather than trying to settle on a single definition, this article introduces a broad perspective on what intelligence is, by laying out a cascade of definitions that induces both a nested hierarchy of three levels of intelligence and a wider-ranging space that is built around them and approximations to them. Within this intelligence space, regions are identified that correspond to both natural -- most particularly, human -- intelligence and artificial intelligence (AI), along with the crossover notion of humanlike intelligence. These definitions are then exploited in early explorations of four more advanced, and likely more controversial, topics: the singularity, generative AI, ethics, and intellectual property.
    
[^39]: 理解长尾效应对神经网络压缩的影响

    Understanding the Effect of the Long Tail on Neural Network Compression. (arXiv:2306.06238v1 [cs.LG])

    [http://arxiv.org/abs/2306.06238](http://arxiv.org/abs/2306.06238)

    本文研究了在神经网络压缩中如何保持与原始网络的语义相同，在长尾现象中探讨了压缩提高推广性能的记忆要素。

    

    网络压缩现在是神经网络研究的一个成熟的子领域，过去的十年中，取得了显著的进展，以减小模型尺寸和加速推断为目标，同时保持分类准确性。然而，许多研究观察到，仅关注总体准确性可能是误导的。例如，已经证明全模型和压缩模型之间的差异可能会偏向于在数据集中低频的类。这引出了一个重要的研究问题，即“我们能否在保持与原始网络语义等同的情况下实现网络压缩？”在本文中，我们研究了这个问题，重点关注计算机视觉数据集中Feldman等人观察到的“长尾”现象。他们认为，某些输入（适当定义）的记忆对于实现良好的泛化是必要的。由于压缩限制了网络的容量（因此也限制了其记忆能力），所以我们研究了这个问题：

    Network compression is now a mature sub-field of neural network research: over the last decade, significant progress has been made towards reducing the size of models and speeding up inference, while maintaining the classification accuracy. However, many works have observed that focusing on just the overall accuracy can be misguided. E.g., it has been shown that mismatches between the full and compressed models can be biased towards under-represented classes. This raises the important research question, \emph{can we achieve network compression while maintaining ``semantic equivalence'' with the original network?} In this work, we study this question in the context of the ``long tail'' phenomenon in computer vision datasets observed by Feldman, et al. They argue that \emph{memorization} of certain inputs (appropriately defined) is essential to achieving good generalization. As compression limits the capacity of a network (and hence also its ability to memorize), we study the question: a
    
[^40]: 基于深度学习的目标跟踪、速度估计和传感器数据的时间投影方法

    Deep Learning Method for Object Tracking, Velocity Estimation and Projection of Sensor Data over Time. (arXiv:2306.06126v1 [cs.CV])

    [http://arxiv.org/abs/2306.06126](http://arxiv.org/abs/2306.06126)

    本文提出了一种利用Transformer机制的新型循环神经网络单元，实现了利用传感器记录中的时空相关性来实现目标跟踪和速度估计，并将记忆状态进行投影。

    

    目前环境分割和速度估计的深度学习方法依赖于卷积循环神经网络，以利用所获取的传感器数据中的时空关系。这些方法通过将新输入和记忆数据相关联来隐式地推导场景动态，利用卷积神经网络。本文发现卷积神经网络在这个任务上存在架构限制，因此提出了利用Transformer机制的新型循环神经网络单元来解决利用一系列传感器记录中的时空相关性所面临的各种问题。在该单元中，通过将基于传感器输入和记忆状态分别导出的关键-查询对相关联，跟踪对象编码在连续帧上的位置。然后利用得到的跟踪模式来获取场景动态和回归速度。最后，基于提取的速度估计将循环神经网络的记忆状态进行投影。

    Current Deep Learning methods for environment segmentation and velocity estimation rely on Convolutional Recurrent Neural Networks to exploit spatio-temporal relationships within obtained sensor data. These approaches derive scene dynamics implicitly by correlating novel input and memorized data utilizing ConvNets. We show how ConvNets suffer from architectural restrictions for this task. Based on these findings, we then provide solutions to various issues on exploiting spatio-temporal correlations in a sequence of sensor recordings by presenting a novel Recurrent Neural Network unit utilizing Transformer mechanisms. Within this unit, object encodings are tracked across consecutive frames by correlating key-query pairs derived from sensor inputs and memory states, respectively. We then use resulting tracking patterns to obtain scene dynamics and regress velocities. In a last step, the memory state of the Recurrent Neural Network is projected based on extracted velocity estimates to res
    
[^41]: SequenceMatch：带回溯的自回归序列模型的模仿学习

    SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking. (arXiv:2306.05426v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.05426](http://arxiv.org/abs/2306.05426)

    本文提出了一种称为SequenceMatch的带有回溯的自回归模型的模仿学习框架，该框架通过最小化自回归模型生成序列和数据集序列之间的各种分歧来减少在自回归生成过程中的复合误差，并允许引入回溯动作。

    

    在许多领域，自回归模型可以在预测下一个观测值的任务上获得高似然度。然而，这种最大似然（MLE）目标不一定与自回归生成高质量序列的下游用例相匹配。MLE目标按照数据分布下序列的频率加权，不提供模型在分布之外行为的指导，这会导致在自回归生成过程中复合误差。为了解决这个复合误差问题，我们将序列生成定为模仿学习（IL）问题。这使我们可以最小化自回归模型生成的序列分布和数据集序列之间的各种分歧，包括考虑出分布序列的分歧。IL框架还允许我们通过在生成过程中引入回格动作来引入回溯。这进一步减轻了复合效应。

    In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compound
    
[^42]: 鲁棒性AI生成文本检测的内部维度估计

    Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])

    [http://arxiv.org/abs/2306.04723](http://arxiv.org/abs/2306.04723)

    本文提出了衡量文本内部维度的方法，应用于鲁棒性AI生成文本的检测，展示了人类文本与AI生成文本在内部维度上的差异。

    

    快速提高的AI生成内容的质量使得很难区分人类和AI生成的文本，这可能会对社会产生不良影响。因此，研究人类文本的不变属性变得越来越重要。本文提出了一种人类文本的不变特征，即给定文本样本嵌入集合下的流形的内部维度。我们展示了自然语言流畅文本的平均内部维度在几个基于字母的语言中约为 $9$，而中文约为 $7$，而每种语言的AI生成文本的平均内部维度较低，差约 $1.5$，并且有明显的统计分离。

    Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
    
[^43]: 一种基于物理约束卷积神经网络方法的数据误差修正工具

    Uncovering solutions from data corrupted by systematic errors: A physics-constrained convolutional neural network approach. (arXiv:2306.04600v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2306.04600](http://arxiv.org/abs/2306.04600)

    本文提出了一种基于物理约束卷积神经网络的方法，可以从受到系统误差影响的数据中还原出潜在物理系统的时空解。

    

    自然现象和工程系统的信息通常包含在数据中。 然而，数据可能会被模型和实验中的系统性误差所损坏。 本文提出了一种工具，即基于物理约束卷积神经网络（PC-CNN），通过从数据中删除系统误差来揭示潜在物理系统的时空解。 PC-CNN结合了系统控制方程和数据的信息。 我们重点关注由偏微分方程模拟的基础现象，例如线性对流，Burgers方程和二维湍流。

    Information on natural phenomena and engineering systems is typically contained in data. Data can be corrupted by systematic errors in models and experiments. In this paper, we propose a tool to uncover the spatiotemporal solution of the underlying physical system by removing the systematic errors from data. The tool is the physics-constrained convolutional neural network (PC-CNN), which combines information from both the systems governing equations and data. We focus on fundamental phenomena that are modelled by partial differential equations, such as linear convection, Burgers equation, and two-dimensional turbulence. First, we formulate the problem, describe the physics-constrained convolutional neural network, and parameterise the systematic error. Second, we uncover the solutions from data corrupted by large multimodal systematic errors. Third, we perform a parametric study for different systematic errors. We show that the method is robust. Fourth, we analyse the physical properti
    
[^44]: 匹配对校准用于排名公平性的测试

    Matched Pair Calibration for Ranking Fairness. (arXiv:2306.03775v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03775](http://arxiv.org/abs/2306.03775)

    本文提出了一个针对排名系统公平性的测试方法——匹配对校准，通过构建混淆差异最小的匹配物品对来计算适当的排名误差测量结果，可以直接说明子组水平曝光的不公平性。实验证明该方法在检测排名偏差方面具有很好的效果。

    

    我们提出了一种针对基于分数的排名系统中公平性的测试方法——匹配对校准。我们的方法构建了一组匹配的物品对，这些物品对子组之间的混淆差异最小，然后在这组物品上计算适当的排名误差测量结果。匹配步骤确保了我们在相同分数的物品之间比较子组结果，从而直接说明子组水平曝光的不公平性。我们展示了我们的方法如何将校准的公平性直觉从二元分类设置推广到排名，并将我们的方法与其他提议的排名公平性措施联系起来。此外，我们的策略展示了边际结果测试逻辑如何扩展到分析人员可以访问模型得分的情况。最后，我们提供了一个将匹配对校准应用于真实排名数据集以证明其检测排名偏差的效能的示例。

    We propose a test of fairness in score-based ranking systems called matched pair calibration. Our approach constructs a set of matched item pairs with minimal confounding differences between subgroups before computing an appropriate measure of ranking error over the set. The matching step ensures that we compare subgroup outcomes between identically scored items so that measured performance differences directly imply unfairness in subgroup-level exposures. We show how our approach generalizes the fairness intuitions of calibration from a binary classification setting to ranking and connect our approach to other proposals for ranking fairness measures. Moreover, our strategy shows how the logic of marginal outcome tests extends to cases where the analyst has access to model scores. Lastly, we provide an example of applying matched pair calibration to a real-word ranking data set to demonstrate its efficacy in detecting ranking bias.
    
[^45]: 线性距离度量学习

    Linear Distance Metric Learning. (arXiv:2306.03173v1 [cs.LG])

    [http://arxiv.org/abs/2306.03173](http://arxiv.org/abs/2306.03173)

    本文介绍了一种线性距离度量学习方法，可以有效地从一个欧几里得度量空间中的数据学习出另一个欧几里得度量空间中的线性映射，即使数据中存在噪声，也可以以任意精度学习真实的线性距离度量，并提供相应的样本复杂度上界。此外，提供了一种有效的低秩模型截断方法，可以保证模型的准确性和精度。

    

    在线性距离度量学习中，给定欧几里得度量空间中的数据，目标是寻找一个适当的线性映射到另一个欧几里得度量空间，尽可能地满足一定的距离条件。本文规范了一种简单优美的方法，它简化为一个连续的凸损失函数优化问题，对于不同的噪声模型，我们推导出了相应的损失函数。我们展示了即使数据有噪声，只要有足够的样本，就可以以任意精度学习真实的线性距离度量，并提供相应的样本复杂度上界。此外，我们提供了一种有效的方法将学习到的模型截断为低秩模型，可以证明在损失函数和参数的准确性方面保持精度，这是这种类型的首个结果。对合成和真实数据集的几个实验观察支持和证明了我们的理论结果。

    In linear distance metric learning, we are given data in one Euclidean metric space and the goal is to find an appropriate linear map to another Euclidean metric space which respects certain distance conditions as much as possible. In this paper, we formalize a simple and elegant method which reduces to a general continuous convex loss optimization problem, and for different noise models we derive the corresponding loss functions. We show that even if the data is noisy, the ground truth linear metric can be learned with any precision provided access to enough samples, and we provide a corresponding sample complexity bound. Moreover, we present an effective way to truncate the learned model to a low-rank model that can provably maintain the accuracy in loss function and in parameters -- the first such results of this type. Several experimental observations on synthetic and real data sets support and inform our theoretical results.
    
[^46]: 用于液压状态监测系统异常检测的半监督学习比较研究

    Comparative Study on Semi-supervised Learning Applied for Anomaly Detection in Hydraulic Condition Monitoring System. (arXiv:2306.02709v1 [cs.LG])

    [http://arxiv.org/abs/2306.02709](http://arxiv.org/abs/2306.02709)

    本研究比较了不同类型的半监督学习方法在液压状态监测系统中用于异常检测。深度学习模型表现最好，而集成模型可以进一步提高检测性能。

    

    基于状态的维护在液压系统中变得越来越重要。然而，这些系统的异常检测仍然具有挑战性，特别是由于异常数据很少，标记这些数据是费时费力甚至危险的。因此，建议使用无监督或半监督方法，特别是对于只有少量标签可用的情况下利用无监督学习作为特征提取机制来辅助监督学习的半监督学习方法。本研究系统地比较了在液压状态监测系统中应用的半监督学习方法用于异常检测。首先，进行了深入的数据分析和特征学习，以了解开源的液压状态监测数据集。然后，实施和评估了各种方法，包括传统的独立半监督学习模型（例如，一类支持向量机、鲁棒协方差）、集成模型（例如，孤立森林）和基于深度学习的模型（例如，自动编码器、图卷积网络）。结果表明，深度学习模型优于传统模型，而集成模型可以进一步提高检测性能。

    Condition-based maintenance is becoming increasingly important in hydraulic systems. However, anomaly detection for these systems remains challenging, especially since that anomalous data is scarce and labeling such data is tedious and even dangerous. Therefore, it is advisable to make use of unsupervised or semi-supervised methods, especially for semi-supervised learning which utilizes unsupervised learning as a feature extraction mechanism to aid the supervised part when only a small number of labels are available. This study systematically compares semi-supervised learning methods applied for anomaly detection in hydraulic condition monitoring systems. Firstly, thorough data analysis and feature learning were carried out to understand the open-sourced hydraulic condition monitoring dataset. Then, various methods were implemented and evaluated including traditional stand-alone semi-supervised learning models (e.g., one-class SVM, Robust Covariance), ensemble models (e.g., Isolation F
    
[^47]: SpeechGen: 利用提示解锁语音语言模型的生成能力

    SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.02207](http://arxiv.org/abs/2306.02207)

    本文探索了一个名为SpeechGen的统一框架，通过提示调节，解锁了语音语言模型的生成能力，成功地实现了直接适应连续语音到离散标记的任务，使得语音生成成为可能。

    

    大型语言模型（LLM）在人工智能生成内容（AIGC）中引起了相当大的关注，特别是随着ChatGPT的出现。然而，将连续语音直接适应于处理离散标记的LLM仍然是一个未解决的挑战，这妨碍了LLM在语音生成方面的应用。高级语音LM们无法充分利用语音信号所包含的丰富信息，包括说话者和情感等，这些信息仅通过文本数据无法获取。在一些语音分类任务中，简单的提示调整已经表现出明显的参数效率和竞争性能的提高。但在多大程度上提示能够有效地激发语音LM的生成任务仍然是一个未知的问题。本文提出了一项先驱性研究，该研究在称为SpeechGen的统一框架中使用提示调节来刺激语音LM进行各种生成任务，并具有约10M可训练参数。

    Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unif
    
[^48]: 不一致的关键点：一种基于知识引导的双一致性网络用于多模态谣言检测

    Inconsistent Matters: A Knowledge-guided Dual-consistency Network for Multi-modal Rumor Detection. (arXiv:2306.02137v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02137](http://arxiv.org/abs/2306.02137)

    本文提出了一种基于知识引导的双一致性网络，用于检测带有多媒体内容的谣言。该网络使用两个一致性检测子网络同时捕获跨模态级别和内容-知识级别的不一致性，并在不同的缺失视觉模态条件下实现了稳健的多模态表示学习。

    

    谣言传播者越来越多地使用多媒体内容来吸引新闻消费者的注意力和信任。虽然有不少谣言检测模型利用了多模态数据，但它们很少考虑图像和文本之间的不一致语义，也很少发现发布内容和背景知识之间的不一致性。此外，它们通常假设多种模态的完整性，因此无法处理现实生活场景中可能出现的缺失模态。受到社交媒体中谣言更容易具有不一致语义的启发，提出了一种新颖的基于知识引导的双一致性网络，用于检测带有多媒体内容的谣言。它使用两个一致性检测子网络同时捕获跨模态级别和内容-知识级别的不一致性。它还利用特殊的标记，在不同的缺失视觉模态条件下实现了稳健的多模态表示学习。

    Rumor spreaders are increasingly utilizing multimedia content to attract the attention and trust of news consumers. Though quite a few rumor detection models have exploited the multi-modal data, they seldom consider the inconsistent semantics between images and texts, and rarely spot the inconsistency among the post contents and background knowledge. In addition, they commonly assume the completeness of multiple modalities and thus are incapable of handling handle missing modalities in real-life scenarios. Motivated by the intuition that rumors in social media are more likely to have inconsistent semantics, a novel Knowledge-guided Dual-consistency Network is proposed to detect rumors with multimedia contents. It uses two consistency detection subnetworks to capture the inconsistency at the cross-modal level and the content-knowledge level simultaneously. It also enables robust multi-modal representation learning under different missing visual modality conditions, using a special token
    
[^49]: 通过神经科学的视角探究人工意识的可行性

    The feasibility of artificial consciousness through the lens of neuroscience. (arXiv:2306.00915v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2306.00915](http://arxiv.org/abs/2306.00915)

    从神经科学的角度来看，目前大型语言模型难以具备哺乳动物意识感知相关的丘脑皮层系统的关键特征，缺乏周围世界的具体嵌入式信息，且当前的人工智能无法做到存在的依赖于其行为，这意味着人工意识的可行性存在瓶颈。

    

    与大型语言模型的交互引发了这些模型可能具有意识的猜测。从神经科学的角度来看，这种观点很难被证实。首先，大型语言模型的架构缺少哺乳动物意识感知相关的丘脑皮层系统的关键特征。其次，大型语言模型的输入缺乏我们与周围世界的感官接触的具有体验、嵌入式信息的特征。最后，虽然前两个论点在未来的AI系统中可以被克服，但第三个可能更难在不久的将来跨越。换言之，我们认为意识可能取决于是否在“游戏中有皮肤”，即系统的存在是否取决于其行为，而这在当前的人工智能中并不成立。

    Interactions with large language models have led to the suggestion that these models may be conscious. From the perspective of neuroscience, this position is difficult to defend. For one, the architecture of large language models is missing key features of the thalamocortical system that have been linked to conscious awareness in mammals. Secondly, the inputs to large language models lack the embodied, embedded information content characteristic of our sensory contact with the world around us. Finally, while the previous two arguments can be overcome in future AI systems, the third one might be harder to bridge in the near future. Namely, we argue that consciousness might depend on having 'skin in the game', in that the existence of the system depends on its actions, which is not true for present-day artificial intelligence.
    
[^50]: 医学成像信息学简介

    Introduction to Medical Imaging Informatics. (arXiv:2306.00421v1 [eess.IV])

    [http://arxiv.org/abs/2306.00421](http://arxiv.org/abs/2306.00421)

    本文介绍了医学成像信息学的基本概念和最新进展，包括图像处理、特征工程、机器学习、计算机视觉和深度学习，以及如何将它们应用于疾病检测、诊断和预后预测模型的开发。这对于理解信息学在医学中的作用以及其对患者护理的潜在影响具有重要意义。

    

    医学成像信息学是将医学成像和信息学的原理结合起来，以改善医学图像的获取、管理和解释为目的的快速增长的领域。本章介绍了医学成像信息学的基本概念，包括图像处理、特征工程和机器学习。同时，本章还讨论了计算机视觉和深度学习技术的最新进展，以及它们如何用于开发新的定量图像标记和疾病检测、诊断和预后预测模型。通过涵盖医学成像信息学的基本知识，本章为理解信息学在医学中的作用及其对患者护理的潜在影响提供了基础。

    Medical imaging informatics is a rapidly growing field that combines the principles of medical imaging and informatics to improve the acquisition, management, and interpretation of medical images. This chapter introduces the basic concepts of medical imaging informatics, including image processing, feature engineering, and machine learning. It also discusses the recent advancements in computer vision and deep learning technologies and how they are used to develop new quantitative image markers and prediction models for disease detection, diagnosis, and prognosis prediction. By covering the basic knowledge of medical imaging informatics, this chapter provides a foundation for understanding the role of informatics in medicine and its potential impact on patient care.
    
[^51]: 双重稳健自我训练

    Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])

    [http://arxiv.org/abs/2306.00265](http://arxiv.org/abs/2306.00265)

    本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。

    

    自我训练是解决半监督学习问题的一种重要技术。它通过生成伪标签并将其与有限的标记数据集结合使用进行训练，从而利用无标签数据。自我训练的有效性在很大程度上依赖于这些伪标签的准确性。本文引入了双重稳健自我训练，这是一种新颖的半监督算法，可以保证在两个极端之间平衡。当伪标签完全不正确时，我们的方法将被减少到仅使用标记数据进行训练。相反，当伪标签完全准确时，我们的方法将变成利用所有伪标签数据和标记数据进行训练的过程，从而增加有效的样本量。通过在ImageNet图像分类和nuScenes自主驾驶数据集上的实证评估，我们证明了双重稳健损失优于标准自我训练基线的优越性。

    Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
    
[^52]: 束搜索递归单元：一种支持反向传播的递归神经网络框架

    Beam Tree Recursive Cells. (arXiv:2305.19999v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19999](http://arxiv.org/abs/2305.19999)

    本论文提出了一种支持反向传播的递归神经网络框架——束搜索递归单元（BT-Cell），用于扩展递归神经网络，实现对潜在结构的感知；此外，我们提出了一种放松束搜索中硬前k算子的方法，以实现更好的梯度信号传递。在评估中发现，BT-Cell在合成和实际数据的多个具有结构敏感性的任务中表现优异。

    

    本文提出了一种叫做束搜索递归单元（BT-Cell）的框架，用于扩展支持使用束搜索进行潜在结构感知的递归神经网络（RvNN）。我们进一步通过提出在束搜索中对硬性前k算子的放松来扩展此框架，以更好地传递梯度信号。我们在合成和实际数据的不同代表性分布上评估了我们的模型。实验结果表明，BT-Cell在多个具有挑战性的体现结构敏感性的任务（如ListOps和逻辑推理）上达到了几乎完美的性能，同时在实际数据上与其他基于RvNN的模型具有可比性的性能。此外，我们在ListOps中确定了神经模型在推广到未见过的参数数量上的未知失效案例。代码可在https://github.com/JRC1995/BeamTreeRecursiveCells上获得。

    We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly framework to extend Recursive Neural Networks (RvNNs) with beam search for latent structure induction. We further extend this framework by proposing a relaxation of the hard top-k operators in beam search for better propagation of gradient signals. We evaluate our proposed models in different out-of-distribution splits in both synthetic and realistic data. Our experiments show that BTCell achieves near-perfect performance on several challenging structure-sensitive synthetic tasks like ListOps and logical inference while maintaining comparable performance in realistic data against other RvNN-based models. Additionally, we identify a previously unknown failure case for neural models in generalization to unseen number of arguments in ListOps. The code is available at: https://github.com/JRC1995/BeamTreeRecursiveCells.
    
[^53]: MicroSegNet：一种基于深度学习的微型超声图像前列腺分割方法

    MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images. (arXiv:2305.19956v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.19956](http://arxiv.org/abs/2305.19956)

    本文提出了一种基于深度学习的微型超声图像前列腺分割方法，利用多尺度注释引导的Transformer UNet模型和注释引导的二分类交叉熵损失解决低分辨率和界限不清的挑战，该方法更加关注难以分割的区域。

    

    微型超声是一种新型的29MHz超声技术，提供比传统超声高3-4倍的分辨率，在诊断前列腺癌的准确性方面与MRI相当，但成本更低。然而，由于低分辨率和前列腺、膀胱和尿道中线之间的界限不清，基于微型超声的前列腺分割具有挑战性。本文提出了MicroSegNet，这是一个特别设计用于解决这些挑战的多尺度注释引导的Transformer UNet模型。在训练过程中，MicroSegNet更加关注难以分割（难区域）的区域，这些区域具有专家和非专家注释之间的差异。为此，我们提出了注释引导的二分类交叉熵（AG-BCE）损失，它在难区域中给预测误差分配更大的权重和较低的权重。

    Micro-ultrasound (micro-US) is a novel 29-MHz ultrasound technique that provides 3-4 times higher resolution than traditional ultrasound, delivering comparable accuracy for diagnosing prostate cancer to MRI but at a lower cost. Accurate prostate segmentation is crucial for prostate volume measurement, cancer diagnosis, prostate biopsy, and treatment planning. However, prostate segmentation on microUS is challenging due to artifacts and indistinct borders between the prostate, bladder, and urethra in the midline. This paper presents MicroSegNet, a multi-scale annotation-guided transformer UNet model designed specifically to tackle these challenges. During the training process, MicroSegNet focuses more on regions that are hard to segment (hard regions), characterized by discrepancies between expert and non-expert annotations. We achieve this by proposing an annotation-guided binary cross entropy (AG-BCE) loss that assigns a larger weight to prediction errors in hard regions and a lower w
    
[^54]: 前列腺体内微型超声与离体伪全切片组织标本图像的图像配准: 一个概念验证研究。

    Image Registration of In Vivo Micro-Ultrasound and Ex Vivo Pseudo-Whole Mount Histopathology Images of the Prostate: A Proof-of-Concept Study. (arXiv:2305.19939v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.19939](http://arxiv.org/abs/2305.19939)

    本文提供了一种半自动化的流程，用于将体内微-US图像与离体全切片组织病理学图像配准，以帮助泌尿外科医生提高小前列腺癌的检测率。

    

    前列腺癌的早期诊断显著提高了患者5年生存率。图像引导下的活检可以改善对小型前列腺癌的检测。MRI-超声融合引导下的活检对更小的肿瘤敏感，但由于MRI和融合设备的高成本而被少使用。微型超声（微-US）是一种新型高分辨率超声技术，可提供MRI成像类似的诊断精度，同时成本更低。然而，由于癌细胞和正常组织之间的灰度变化微弱，因此解释微-US图像是具有挑战性的。可以通过向泌尿外科医生提供一个包含地面真实癌变区域的微-US图像大数据集来解决这个挑战。这样的数据集可以通过图像配准将手术标本（组织病理学）映射到微-US图像上。在本文中，我们提出了一个半自动化的流程，用于将体内微-US图像与离体全切片组织病理学图像配准。

    Early diagnosis of prostate cancer significantly improves a patient's 5-year survival rate. Biopsy of small prostate cancers is improved with image-guided biopsy. MRI-ultrasound fusion-guided biopsy is sensitive to smaller tumors but is underutilized due to the high cost of MRI and fusion equipment. Micro-ultrasound (micro-US), a novel high-resolution ultrasound technology, provides a cost-effective alternative to MRI while delivering comparable diagnostic accuracy. However, the interpretation of micro-US is challenging due to subtle gray scale changes indicating cancer vs normal tissue. This challenge can be addressed by training urologists with a large dataset of micro-US images containing the ground truth cancer outlines. Such a dataset can be mapped from surgical specimens (histopathology) onto micro-US images via image registration. In this paper, we present a semi-automated pipeline for registering in vivo micro-US images with ex vivo whole-mount histopathology images. Our pipeli
    
[^55]: 表示驱动的强化学习框架

    Representation-Driven Reinforcement Learning. (arXiv:2305.19922v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19922](http://arxiv.org/abs/2305.19922)

    该论文提出了一个表示驱动的强化学习框架，通过在线性特征空间中嵌入策略网络，重新框定探索-利用问题为表示-利用问题，以实现最佳的探索。该框架通过应用进化和策略梯度法取得了显著的性能提升。

    

    我们提出了一个表示驱动的强化学习框架。通过将策略表示为其期望值的估计，我们利用来自情境推断的方法来指导探索和利用。特别地，将策略网络嵌入到线性特征空间中，使我们能够将探索-利用问题重新框定为表示-利用问题，其中良好的策略表示能够实现最佳的探索。我们通过应用进化和策略梯度法来展示该框架的有效性，相比于传统方法，这些方法带来了显著的性能提升。我们的框架提供了一种强化学习的新视角，强调了策略表示在决定最佳探索-利用策略方面的重要性。

    We present a representation-driven framework for reinforcement learning. By representing policies as estimates of their expected values, we leverage techniques from contextual bandits to guide exploration and exploitation. Particularly, embedding a policy network into a linear feature space allows us to reframe the exploration-exploitation problem as a representation-exploitation problem, where good policy representations enable optimal exploration. We demonstrate the effectiveness of this framework through its application to evolutionary and policy gradient-based approaches, leading to significantly improved performance compared to traditional methods. Our framework provides a new perspective on reinforcement learning, highlighting the importance of policy representation in determining optimal exploration-exploitation strategies.
    
[^56]: 粗糙集下一种规则通用逆推学习方法

    A rule-general abductive learning by rough sets. (arXiv:2305.19718v1 [cs.LG])

    [http://arxiv.org/abs/2305.19718](http://arxiv.org/abs/2305.19718)

    本文提出了一种基于粗糙集理论的规则通用逆推学习方法，通过转化目标概念和子概念为信息表，以更低的成本解决领域知识获取和规则的修正、减少、生成问题。

    

    在现实任务中，通常存在大量未标记数据和标记数据。将两者组合起来进行学习的任务被称为半监督学习。专家可以使用逻辑规则来标记未标记数据，但这个操作很昂贵。感知和推理的结合在处理具有领域知识的半监督任务方面具有良好的效果。然而，获取领域知识以及规则的修正、减少和生成仍然是需要解决的复杂问题。粗糙集理论是解决信息系统中知识处理的重要方法。本文提出了一种粗糙集下的规则通用逆推学习方法（RS-ABL）。通过将规则的目标概念和子概念转化为信息表，利用粗糙集理论来解决以更低的成本获取领域知识和修正、减少、生成规则的问题。该框架还可以生成更广泛的负规则，以增强规则范围。

    In real-world tasks, there is usually a large amount of unlabeled data and labeled data. The task of combining the two to learn is known as semi-supervised learning. Experts can use logical rules to label unlabeled data, but this operation is costly. The combination of perception and reasoning has a good effect in processing such semi-supervised tasks with domain knowledge. However, acquiring domain knowledge and the correction, reduction and generation of rules remain complex problems to be solved. Rough set theory is an important method for solving knowledge processing in information systems. In this paper, we propose a rule general abductive learning by rough set (RS-ABL). By transforming the target concept and sub-concepts of rules into information tables, rough set theory is used to solve the acquisition of domain knowledge and the correction, reduction and generation of rules at a lower cost. This framework can also generate more extensive negative rules to enhance the breadth of
    
[^57]: 自适应自蒸馏下的异构数据联邦学习

    Federated Learning on Heterogeneous Data via Adaptive Self-Distillation. (arXiv:2305.19600v1 [cs.LG])

    [http://arxiv.org/abs/2305.19600](http://arxiv.org/abs/2305.19600)

    本文提出一种基于自适应自蒸馏的新型正则化技术来训练客户端模型，该正则化方案基于客户端本地模型预测和全局模型的相似性以及客户端的标签分布来自适应地调整客户端的训练数据。实验结果表明，该方法在各种基准数据集上优于目前流行的联邦学习方法。

    

    联邦学习是一种机器学习范式，它使得客户机可以聚合本地训练模型而无需共享任何本地训练数据从而训练全局模型。然而，实践中发现，每个客户端观察到的本地数据分布之间可能存在显著的不均匀性（例如类别不平衡）。在这种不均匀的数据分布下，联邦学习会出现“客户机漂移”问题，导致每个客户端收敛到其自己的局部最优解，这会降低模型的收敛速度并降低模型性能。为了解决这个问题，我们提出了一种基于自适应自蒸馏的新型正则化技术来训练客户端模型。我们的正则化方案基于客户端本地模型预测和全局模型的相似性以及客户端的标签分布来自适应地调整客户端的训练数据。该正则化技术可以轻松地集成在现有的联邦学习算法之上，而不需要对客户端或服务器代码进行任何更改，因此具有高度的可部署性。我们在各种基准数据集上验证了我们的方法，并展示了在非独立同分布数据下的优越性。

    Federated Learning (FL) is a machine learning paradigm that enables clients to jointly train a global model by aggregating the locally trained models without sharing any local training data. In practice, there can often be substantial heterogeneity (e.g., class imbalance) across the local data distributions observed by each of these clients. Under such non-iid data distributions across clients, FL suffers from the 'client-drift' problem where every client converges to its own local optimum. This results in slower convergence and poor performance of the aggregated model. To address this limitation, we propose a novel regularization technique based on adaptive self-distillation (ASD) for training models on the client side. Our regularization scheme adaptively adjusts to the client's training data based on: (1) the closeness of the local model's predictions with that of the global model and (2) the client's label distribution. The proposed regularization can be easily integrated atop exis
    
[^58]: 面向车辆路径问题的全通用神经方法

    Towards Omni-generalizable Neural Methods for Vehicle Routing Problems. (arXiv:2305.19587v1 [cs.LG])

    [http://arxiv.org/abs/2305.19587](http://arxiv.org/abs/2305.19587)

    提出了一个通用meta-learning框架，使得模型可以在推理过程中快速适应新任务。通过在多个合成和基准数据集上的实验表明，该方法可以有效地解决大小和分布变化的车辆路径问题（VRP）。

    

    由于避免了对手工规则的依赖，学习车辆路径问题（VRP）的启发式方法受到了广泛关注。然而，现有方法通常在固定大小和节点分布的同一任务上进行训练和测试，因此具有有限的泛化性能。本文研究了一个具有挑战性但又现实的场景，该场景考虑了VRP在大小和分布方面的一般性。我们提出了一种通用的元学习框架，在推理期间能够快速适应新任务的能力下对初始化模型进行有效训练。我们进一步开发了一种简单而有效的近似方法来减少训练开销。对旅行商问题（TSP）和容量车辆路径问题（CVRP）的合成和基准实例进行了广泛的实验证明了我们方法的有效性。代码可在https://github.com/RoyalSkye/Omni-VRP得到。

    Learning heuristics for vehicle routing problems (VRPs) has gained much attention due to the less reliance on hand-crafted rules. However, existing methods are typically trained and tested on the same task with a fixed size and distribution (of nodes), and hence suffer from limited generalization performance. This paper studies a challenging yet realistic setting, which considers generalization across both size and distribution in VRPs. We propose a generic meta-learning framework, which enables effective training of an initialized model with the capability of fast adaptation to new tasks during inference. We further develop a simple yet efficient approximation method to reduce the training overhead. Extensive experiments on both synthetic and benchmark instances of the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) demonstrate the effectiveness of our method. The code is available at: https://github.com/RoyalSkye/Omni-VRP.
    
[^59]: 从API学习学习：黑盒数据无关元学习

    Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])

    [http://arxiv.org/abs/2305.18413](http://arxiv.org/abs/2305.18413)

    该论文提出了一个BiDf-MKD框架，可以从一组API库中无需访问训练数据，直接进行元学习；能够在更广泛的黑盒API上进行元学习，提高了元模型的泛化性能和应用范围。

    

    无数据元学习（DFML）旨在通过从一组预训练模型进行元学习而无需访问训练数据，从而实现高效学习新任务。现有的DFML工作仅能从（i）白盒和（ii）小规模预训练模型（iii）相同的架构中元学习，忽略了更实际的设置，即用户仅能通过任意模型架构和规模的API进行推断。为解决这个问题，我们提出了一个双层数据无关元知识蒸馏（BiDf-MKD）框架，将更通用的元知识从一组黑盒API转移到一个单一的元模型中。

    Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. 
    
[^60]: 基于双层学习的最优正则化参数研究

    On Optimal Regularization Parameters via Bilevel Learning. (arXiv:2305.18394v1 [math.OC])

    [http://arxiv.org/abs/2305.18394](http://arxiv.org/abs/2305.18394)

    本文提出了一个利用双层学习确定正则化参数的方法，并提出了一个新的表征正确参数的条件。

    

    变分正则化常用于解线性反问题，它通过添加正则化项来提高先验信息质量，并通过正则化参数加以权衡，而合适的正则化参数的选择至关重要。现有的策略例如差异原则和L-曲线可以用于确定合适的参数值，但是近年来，一种叫做双层学习的监督机器学习方法被用于确定最优参数。虽然以前的策略有各种理论结果，但在这种情况下，双层学习的良好性质仍然是一个发展中的领域。本文提出了一个更好的条件来表征确定正则化参数的正值性。

    Variational regularization is commonly used to solve linear inverse problems, and involves augmenting a data fidelity by a regularizer. The regularizer is used to promote a priori information, and is weighted by a regularization parameter. Selection of an appropriate regularization parameter is critical, with various choices leading to very different reconstructions. Existing strategies such as the discrepancy principle and L-curve can be used to determine a suitable parameter value, but in recent years a supervised machine learning approach called bilevel learning has been employed. Bilevel learning is a powerful framework to determine optimal parameters, and involves solving a nested optimisation problem. While previous strategies enjoy various theoretical results, the well-posedness of bilevel learning in this setting is still a developing field. One necessary property is positivity of the determined regularization parameter. In this work, we provide a new condition that better char
    
[^61]: 从黑盒模型到可解释模型的转化，用于高效的迁移学习

    Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])

    [http://arxiv.org/abs/2305.17303](http://arxiv.org/abs/2305.17303)

    本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。

    

    建立具有普适性的AI模型是医疗领域面临的主要挑战之一。神经网络（NN）模型即使输入分布轻微移位（例如扫描仪类型），也会受到影响，而放射科医生则依赖于异常性的通用描述性规则。微调模型以将知识从一个领域转移到另一个领域需要大量标记数据。在本文中，我们开发了一种可解释的模型，它可以在计算成本最小的情况下，高效地针对未知的目标域进行微调。我们认为NN的可解释组件大致是域不变的。然而，可解释模型通常表现不及它们的BB变体。在源域中我们先使用人类理解的概念从BB开始，将其提炼成一组浅显易懂的interpretable模型。由于每个interpretable模型都覆盖了数据的一个子集，具有一组interpretable模型的混合可以实现与BB相当的性能。

    Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
    
[^62]: 通过格拉姆迭代实现卷积层利普希茨常数的高效边界

    Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration. (arXiv:2305.16173v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16173](http://arxiv.org/abs/2305.16173)

    本文提出了一种基于循环矩阵和格拉姆迭代的方法，用于高效估计卷积神经网络中的Lipschitz常数上界。该方法精确、快速、可微分，并展现了超线性收敛。在实验上表现出较高的精度、计算成本和可扩展性，在利普希茨正则化方面也取得了具有竞争力的结果。

    

    由于利普希茨常数的控制对神经网络的训练稳定性、泛化和鲁棒性有很大影响，因此估计这个值是目前的一个科学难题。在本文中，我们使用循环矩阵理论和一种新的功率迭代替代方法，介绍了一个精确、快速和可微分的上界，用于卷积层的谱范数。称为格拉姆迭代，我们的方法展现了一个超线性的收敛。首先，我们通过一系列全面的实验证明了我们的方法在精度、计算成本和可伸缩性方面优于其他最先进的方法。然后，我们证明了它对于卷积神经网络的利普希茨正则化非常有效，与其他方法相比具有竞争力的结果。代码可在 https://github.com/blaisedelattre/lip4conv 上获得。

    Since the control of the Lipschitz constant has a great impact on the training stability, generalization, and robustness of neural networks, the estimation of this value is nowadays a real scientific challenge. In this paper we introduce a precise, fast, and differentiable upper bound for the spectral norm of convolutional layers using circulant matrix theory and a new alternative to the Power iteration. Called the Gram iteration, our approach exhibits a superlinear convergence. First, we show through a comprehensive set of experiments that our approach outperforms other state-of-the-art methods in terms of precision, computational cost, and scalability. Then, it proves highly effective for the Lipschitz regularization of convolutional neural networks, with competitive results against concurrent approaches. Code is available at https://github.com/blaisedelattre/lip4conv.
    
[^63]: 使用深度学习架构进行潜在音频空间探索的声音设计策略

    Sound Design Strategies for Latent Audio Space Explorations Using Deep Learning Architectures. (arXiv:2305.15571v1 [cs.SD])

    [http://arxiv.org/abs/2305.15571](http://arxiv.org/abs/2305.15571)

    本研究探索了利用深度学习架构直接应用于原始音频数据来探索潜在音频空间的新方法。

    

    近年来，深度学习在声音和音乐计算方面的应用已经引起了人们的关注。然而，这些新技术与它们如何被纳入真实世界的艺术实践之间仍存在着一些缺失。本文探讨了一种常见的深度学习架构——变分自编码器（VAE）。之前，VAE已经被用于生成潜在音色空间或符号音乐例子的潜在空间。本研究将VAE直接应用于原始音频数据而不是音频特征提取的音频数据上，既可以使用任何音频录音，同时也具有灵活性。

    The research in Deep Learning applications in sound and music computing have gathered an interest in the recent years; however, there is still a missing link between these new technologies and on how they can be incorporated into real-world artistic practices. In this work, we explore a well-known Deep Learning architecture called Variational Autoencoders (VAEs). These architectures have been used in many areas for generating latent spaces where data points are organized so that similar data points locate closer to each other. Previously, VAEs have been used for generating latent timbre spaces or latent spaces of symbolic music excepts. Applying VAE to audio features of timbre requires a vocoder to transform the timbre generated by the network to an audio signal, which is computationally expensive. In this work, we apply VAEs to raw audio data directly while bypassing audio feature extraction. This approach allows the practitioners to use any audio recording while giving flexibility an
    
[^64]: PromptNER: 基于提示的命名实体识别

    PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v1 [cs.CL])

    [http://arxiv.org/abs/2305.15444](http://arxiv.org/abs/2305.15444)

    PromptNER是一种基于提示的命名实体识别算法，利用LLM生成潜在实体列表并提供解释，在少样本NER和跨领域NER方面实现了最先进性能。

    

    令人惊讶的是，大型语言模型（LLMs）和越来越多的基于提示的启发式方法现在提供了强大的现成方法，为各种经典的NLP问题提供了少量样本的解决方案。然而，尽管有着令人期待的初步结果，但这些基于LLM的少样本方法在命名实体识别（NER）方面仍远未达到最先进水平，现有的方法包括通过端到端结构理解学习表示，并在标准标记语料库上进行微调。本文介绍了PromptNER，一种新的用于少样本和跨领域NER的最先进算法。为了适应任何新的NER任务，PromptNER需要提供一组实体定义，除基本的少样本样例以外。给定输入句子，PromptNER提示LLM生成一个潜在实体列表，并提供相应的解释，证明它们与提供的实体类型定义的兼容性。值得注意的是，PromptNER在少样本NER任务方面实现了最先进的性能，并在具有挑战性的WikiAnn数据集上为跨领域NER设定了新的SOTA。

    In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-sho
    
[^65]: PulseNet: 使用随机数据增强策略和连续小波变换进行犬ECG信号分类的深度学习模型。

    PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines. (arXiv:2305.15424v1 [eess.SP])

    [http://arxiv.org/abs/2305.15424](http://arxiv.org/abs/2305.15424)

    该论文提出了一种利用深度学习对犬的ECG信号进行自动分类的方法，通过使用随机数据增强策略和连续小波变换，分类精度得到了提高。

    

    评估犬的心电图(ECG)需要熟练的兽医，但目前可用的兽医心脏病专家用于ECG解读和诊断支持的数量有限。开发自动评估ECG序列的工具可以通过提供临床医生实时结果和决策支持工具来改善兽医护理。我们实现了一个深度卷积神经网络(CNN)来将犬的心电图序列分类为正常或异常。将ECG记录转换为8秒的第二导联序列，根据是否存在一种或多种心脏异常将其分类为正常或异常。训练ECG序列使用RandomAugmentECG进行随机数据增强，这是一个专门为该项目实现的新增强库。然后，每个块使用连续小波变换转换成2D scalogram。2D scalogram使用二元CNN分类器分类成正常或异常。

    Evaluating canine electrocardiograms (ECG) require skilled veterinarians, but current availability of veterinary cardiologists for ECG interpretation and diagnostic support is limited. Developing tools for automated assessment of ECG sequences can improve veterinary care by providing clinicians real-time results and decision support tools. We implement a deep convolutional neural network (CNN) approach for classifying canine electrocardiogram sequences as either normal or abnormal. ECG records are converted into 8 second Lead II sequences and classified as either normal (no evidence of cardiac abnormalities) or abnormal (presence of one or more cardiac abnormalities). For training ECG sequences are randomly augmented using RandomAugmentECG, a new augmentation library implemented specifically for this project. Each chunk is then is converted using a continuous wavelet transform into a 2D scalogram. The 2D scalogram are then classified as either normal or abnormal by a binary CNN classif
    
[^66]: 深度学习与伦理学

    Deep Learning and Ethics. (arXiv:2305.15239v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.15239](http://arxiv.org/abs/2305.15239)

    本文探讨了人工智能存在的算法偏见、缺乏可解释性、数据隐私侵犯、军事化、欺诈和环境问题，旨在促进哲学、政治科学和社会科学领域关于这些问题的探讨。

    

    本文是Prince（2023年）《理解深度学习》的第21章，教材的完整草稿可在此http URL获得。本章考虑了人工智能系统设计和使用可能产生的潜在危害，包括算法偏见、缺乏可解释性、数据隐私侵犯、军事化、欺诈和环境问题。目的不是为了提供更加道德的建议。相反，目标是在哲学、政治科学和更广泛的社会科学领域引发关键领域的思想和对话。

    This article appears as chapter 21 of Prince (2023, Understanding Deep Learning); a complete draft of the textbook is available here: this http URL This chapter considers potential harms arising from the design and use of AI systems. These include algorithmic bias, lack of explainability, data privacy violations, militarization, fraud, and environmental concerns. The aim is not to provide advice on being more ethical. Instead, the goal is to express ideas and start conversations in key areas that have received attention in philosophy, political science, and the broader social sciences.
    
[^67]: 图谱遇见LLM：一种用于稳健对话理解的协同过滤新方法

    Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. (arXiv:2305.14449v1 [cs.AI])

    [http://arxiv.org/abs/2305.14449](http://arxiv.org/abs/2305.14449)

    一种协同过滤新方法用于稳健对话理解，在历史用户-实体交互的基础上，利用多跳客户亲和力丰富每个用户的索引，并使用有限内存BFGS算法调整每个索引的权重，实验结果显示其明显优于最先进的个性化查询重写方法。

    

    会话式人工智能系统（例如Alexa，Siri，Google Assistant等）需要理解存在缺陷的查询以确保稳健的会话理解并减少用户摩擦。这些有缺陷的查询通常是由用户的歧义和错误，自动语音识别（ASR）和自然语言理解（NLU）中的错误引起的。个性化查询重写（个性化QR）旨在减少身体和尾部用户查询流量中的缺陷，通常依赖于与对话式人工智能的过去成功的用户交互的索引。本文提出我们的“协同查询重写”方法，专注于重写用户历史中没有出现过的新型用户交互。该方法构建了一个“用户反馈交互图”（FIG），由历史用户-实体交互组成，并利用多跳客户亲和力来丰富每个用户的索引（即协同用户索引），从而帮助覆盖未来未曾见过的存在缺陷的查询。为了防止这些新的丰富索引被噪声反馈交互所支配，我们采用了有限内存BFGS（LLM）算法和回退方案来调整每个索引的权重。实验结果表明，我们的方法明显优于最先进的个性化QR方法，并在未看到的用户交互上取得了近乎完美的性能。

    Conversational AI systems (e.g. Alexa, Siri, Google Assistant, etc.) need to understand queries with defects to ensure robust conversational understanding and reduce user frictions. The defective queries are often induced by user ambiguities and mistakes, or errors in the automatic speech recognition (ASR) and natural language understanding (NLU).  Personalized query rewriting (personalized QR) targets reducing defects in the torso and tail user query traffic, and it typically relies on an index of past successful user interactions with the conversational AI. This paper presents our "Collaborative Query Rewriting" approach that focuses on rewriting novel user interactions unseen in the user history. This approach builds a "user Feedback Interaction Graph" (FIG) consisting of historical user-entity interactions, and leverages multi-hop customer affinity to enrich each user's index (i.e. the Collaborative User Index) that would help cover future unseen defective queries. To counteract th
    
[^68]: 同时学习正则化方法：以啤酒花分类为例的案例研究

    Regularization Through Simultaneous Learning: A Case Study for Hop Classification. (arXiv:2305.13447v1 [cs.LG])

    [http://arxiv.org/abs/2305.13447](http://arxiv.org/abs/2305.13447)

    本文提出了一种新颖的同时学习正则化方法，将迁移学习和多任务学习原理应用于啤酒生产中的啤酒花品种分类，利用辅助数据集的协同作用增强获取高度相关特征的能力，成功实现了两个数据集的同时分类。

    

    过度拟合仍然是深度神经网络面临的一个普遍挑战，导致现实世界中的表现不佳。采用正则化技术是抵制这一挑战的常见策略，可以提高模型的泛化能力。本文提出了一种新颖的正则化方法：Simultaneous Learning，它利用迁移学习和多任务学习原理，专门应用于啤酒生产中的啤酒花品种分类。我们的方法利用辅助数据集的强大能力，与目标数据集协同工作，从而增强获取高度相关特征的能力。通过对模型的最终层进行战略性修改，我们实现了两个数据集的同时分类，无需将它们视为不同的任务。为了实现这一点，我们制定了一个包括组间惩罚的损失函数。我们使用InceptionV3和ResNet50模型进行实验评估，并指定了UFOP-HVD啤酒花叶数据集。

    Overfitting remains a prevalent challenge in deep neural networks, leading to suboptimal real-world performance. Employing regularization techniques is a common strategy to counter this challenge, improving model generalization. This paper proposes Simultaneous Learning, a novel regularization approach drawing on Transfer Learning and Multi-task Learning principles, applied specifically to the classification of hop varieties - an integral component of beer production. Our approach harnesses the power of auxiliary datasets in synergy with the target dataset to amplify the acquisition of highly relevant features. Through a strategic modification of the model's final layer, we enable the simultaneous classification of both datasets without the necessity to treat them as disparate tasks. To realize this, we formulate a loss function that includes an inter-group penalty. We conducted experimental evaluations using the InceptionV3 and ResNet50 models, designating the UFOP-HVD hop leaf datase
    
[^69]: 通过同簇预言机的容错精确查询学习有限集合划分

    Error-Tolerant Exact Query Learning of Finite Set Partitions with Same-Cluster Oracle. (arXiv:2305.13402v1 [cs.DS])

    [http://arxiv.org/abs/2305.13402](http://arxiv.org/abs/2305.13402)

    本文提出了一个新问题：如何通过同簇预言机在存在有限对抗错误时积极学习完全恢复划分。我们建立了解析框架并证明了最坏情况下查询复杂度的上下界，并研究了适应性和查询复杂度之间的关系。

    

    本文研究了当存在有限的对抗错误时，仅通过同簇预言机来积极学习完全恢复划分的问题。首先突出了学习划分和相关聚类之间的新颖联系。然后利用这种联系为这个问题建立了一个Rényi-Ulam样式的解析框架，并证明了最坏情况下查询复杂度的上下界。此外，我们还限制了相关随机算法的期望性能。最后，我们研究了适应性和查询复杂度在该问题和相关变体中之间的关系。

    This paper initiates the study of active learning for exact recovery of partitions exclusively through access to a same-cluster oracle in the presence of bounded adversarial error. We first highlight a novel connection between learning partitions and correlation clustering. Then we use this connection to build a R\'enyi-Ulam style analytical framework for this problem, and prove upper and lower bounds on its worst-case query complexity. Further, we bound the expected performance of a relevant randomized algorithm. Finally, we study the relationship between adaptivity and query complexity for this problem and related variants.
    
[^70]: 关于一般函数逼近下的均场强化学习的统计效率

    On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])

    [http://arxiv.org/abs/2305.11283](http://arxiv.org/abs/2305.11283)

    本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。

    

    本文研究了一般函数逼近下的均场控制（MFC）和均场博弈（MFG）中强化学习的统计效率。引入了一种称为Mean-Field Model-Based Eluder Dimension (MBED)的新概念，包含了一系列丰富的均场强化学习问题。此外，我们提出了基于乐观最大似然估计的算法，可以返回一个$\epsilon$优的策略，适用于MFC或$\epsilon$纳什均衡策略适用于MFG，样本复杂度多项式与相关参数无关，与状态、动作和代理数量无关。值得注意的是，我们的结果仅对转移动力学具有Lipschitz连续性的假设，避免了以前的强结构假设。最后，在tabular设置下，假设有一个生成模型，我们建立了一个指数级的下界支持MFC设置，同时提供了一种新颖的样本高效的模型消除算法以逼近最优策略。

    In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
    
[^71]: 不确定最大熵原理

    The Principle of Uncertain Maximum Entropy. (arXiv:2305.09868v1 [cs.IT])

    [http://arxiv.org/abs/2305.09868](http://arxiv.org/abs/2305.09868)

    介绍了不确定最大熵原理，该原理可以处理模型元素不可观测的情况，并优于特定条件下的最大熵方法。同时将黑匣子机器学习模型的输出用作不确定机器熵框架的输入，性能得到了提高。

    

    最大熵原理在信息理论中的引入，为统计力学，机器学习和生态学等各个领域的发展做出了贡献。其得到的解决方案作为催化剂，促进研究人员将他们的经验观察映射到获取无偏模型，同时加深了对复杂系统和现象的理解。然而，在模型元素不直接可观测的情况下，例如存在噪声或眼部遮挡的情况下，标准最大熵方法可能会失败，因为它们无法匹配特征约束。在这里，我们展示了不确定最大熵原理作为一种方法，尽管存在任意噪声观察，它同时将所有可用信息编码，而且优于一些特定条件下的最大熵方法的准确度。此外，我们将黑匣子机器学习模型的输出用作不确定机器熵框架的输入，从而在与最大似然算法相比时建立了改进的性能。

    The principle of maximum entropy, as introduced by Jaynes in information theory, has contributed to advancements in various domains such as Statistical Mechanics, Machine Learning, and Ecology. Its resultant solutions have served as a catalyst, facilitating researchers in mapping their empirical observations to the acquisition of unbiased models, whilst deepening the understanding of complex systems and phenomena. However, when we consider situations in which the model elements are not directly observable, such as when noise or ocular occlusion is present, possibilities arise for which standard maximum entropy approaches may fail, as they are unable to match feature constraints. Here we show the Principle of Uncertain Maximum Entropy as a method that both encodes all available information in spite of arbitrarily noisy observations while surpassing the accuracy of some ad-hoc methods. Additionally, we utilize the output of a black-box machine learning model as input into an uncertain ma
    
[^72]: 鲁棒性神经网络因果分析方法用于解释性研究

    Causal Analysis for Robust Interpretability of Neural Networks. (arXiv:2305.08950v1 [cs.LG])

    [http://arxiv.org/abs/2305.08950](http://arxiv.org/abs/2305.08950)

    本文提出了一种基于因果分析的鲁棒干预方法，用于解释神经网络的决策，避免噪音的干扰。

    

    神经网络内部的解释对于这些黑盒模型的可靠开发和部署至关重要。然而，以往的解释方法集中在基于相关性的度量上，以将模型决策归因于个别示例。然而，这些方法容易受到训练阶段中编码在模型中的噪声和虚假相关性的影响（例如，有偏输入，模型过拟合或错配）。此外，这个过程已经证明会产生嘈杂和不稳定的归因，从而阻碍了对模型行为的透明理解。本文开发了一种基于因果分析的鲁棒干预方法，用于捕捉预训练神经网络中的因果机制及其与预测的关系。我们的方法依赖于路径干预，以推断隐藏层中的因果机制并隔离相关和必要的信息（以进行模型预测），从而避免噪音的干扰。结果是针对特定任务的稳健且可靠的解释。

    Interpreting the inner function of neural networks is crucial for the trustworthy development and deployment of these black-box models. Prior interpretability methods focus on correlation-based measures to attribute model decisions to individual examples. However, these measures are susceptible to noise and spurious correlations encoded in the model during the training phase (e.g., biased inputs, model overfitting, or misspecification). Moreover, this process has proven to result in noisy and unstable attributions that prevent any transparent understanding of the model's behavior. In this paper, we develop a robust interventional-based method grounded by causal analysis to capture cause-effect mechanisms in pre-trained neural networks and their relation to the prediction. Our novel approach relies on path interventions to infer the causal mechanisms within hidden layers and isolate relevant and necessary information (to model prediction), avoiding noisy ones. The result is task-specifi
    
[^73]: 基于二次功能加密的纵向联邦学习中安全训练方法

    Quadratic Functional Encryption for Secure Training in Vertical Federated Learning. (arXiv:2305.08358v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2305.08358](http://arxiv.org/abs/2305.08358)

    本文提出了基于二次功能加密的纵向联邦学习安全训练方法，可以有效避免信息泄露问题。

    

    纵向联邦学习（VFL）支持在多个数据隐私得到保护的参与方之间协同训练机器学习（ML）模型。在VFL中，标签仅对一个方可见，仅当所有参与方的数据被组合后才形成完整的特征集。最近，Xu等人提出了一种名为FedV的新框架，利用多输入功能加密用于VFL的安全梯度计算。本文认为，对于竖直联邦学习中的广义线性模型训练，使用二次功能加密可以避免Xu等人的信息泄露。

    Vertical federated learning (VFL) enables the collaborative training of machine learning (ML) models in settings where the data is distributed amongst multiple parties who wish to protect the privacy of their individual data. Notably, in VFL, the labels are available to a single party and the complete feature set is formed only when data from all parties is combined. Recently, Xu et al. proposed a new framework called FedV for secure gradient computation for VFL using multi-input functional encryption. In this work, we explain how some of the information leakage in Xu et al. can be avoided by using Quadratic functional encryption when training generalized linear models for vertical federated learning.
    
[^74]: TIPS：任何时候神经网络的拓扑重要路径采样

    TIPS: Topologically Important Path Sampling for Anytime Neural Networks. (arXiv:2305.08021v1 [cs.LG])

    [http://arxiv.org/abs/2305.08021](http://arxiv.org/abs/2305.08021)

    TIPS是一种自动设计AnytimeNNs框架，通过识别贡献最大的路径来提高收敛速度和测试准确率，比现有方法提高了2%-6.6%的准确率，在准确率-FLOPs之间取得了最佳平衡。

    

    任何时候神经网络(AnytimeNNs) 是一种能够在各种硬件资源约束下适应性调整模型复杂度的有前途的解决方案。然而，手动设计的 AnytimeNNs 往往会受到设计师先前经验的影响，从而提供次优解。为了解决现有手工方法的限制，我们首先将 AnytimeNNs 的训练过程建模为离散时间马尔可夫链(DTMC)，并利用它来识别对 AnytimeNNs 训练贡献最大的路径。基于这种新的 DTMC 基础分析，我们进一步提出了 TIPS (Topologically Important Path Sampling) 框架，以自动设计适应各种硬件约束的 AnytimeNNs。我们的实验结果表明，TIPS 能够提高 AnytimeNNs 的收敛速度和测试准确率。与现有 AnytimeNNs 方法相比，TIPS 在多个数据集上将准确率提高了 2%-6.6%，并实现了 SOTA 的准确率-FLOPs 折衷。

    Anytime neural networks (AnytimeNNs) are a promising solution to adaptively adjust the model complexity at runtime under various hardware resource constraints. However, the manually-designed AnytimeNNs are biased by designers' prior experience and thus provide sub-optimal solutions. To address the limitations of existing hand-crafted approaches, we first model the training process of AnytimeNNs as a discrete-time Markov chain (DTMC) and use it to identify the paths that contribute the most to the training of AnytimeNNs. Based on this new DTMC-based analysis, we further propose TIPS, a framework to automatically design AnytimeNNs under various hardware constraints. Our experimental results show that TIPS can improve the convergence rate and test accuracy of AnytimeNNs. Compared to the existing AnytimeNNs approaches, TIPS improves the accuracy by 2%-6.6% on multiple datasets and achieves SOTA accuracy-FLOPs tradeoffs.
    
[^75]: 基于表面肌电图像的轻量级全卷积神经网络和迁移学习的跨场景手势识别

    Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])

    [http://arxiv.org/abs/2305.08014](http://arxiv.org/abs/2305.08014)

    本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。

    

    利用低分辨率瞬时高清肌电图像进行手势识别可以开辟发展更流畅、更自然的肌肉-计算机界面的新途径。然而，跨场景数据的变异性存在极大的挑战。现有的方法采用非常大且复杂的深度卷积神经网络或基于2SRNN的领域适应方法，来逼近由这些跨场景数据变异性引起的分布偏移。因此，这些方法也需要在预训练和适应阶段中在数百万个训练参数和大规模预训练数据集上进行学习。结果，这使得在实时应用中进行高端资源约束和计算非常昂贵的部署。为了解决这个问题，我们提出了一种轻量级的全卷积神经网络+迁移学习模型，利用轻量级全卷积神经网络和迁移学习(TL)来增强跨场景手势识别。

    Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
    
[^76]: 基于双重注意力网络的弹性车间调度问题的深度强化学习

    Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning. (arXiv:2305.05119v1 [cs.LG])

    [http://arxiv.org/abs/2305.05119](http://arxiv.org/abs/2305.05119)

    本文提出了一种基于深度强化学习和自注意模型的端到端学习框架，通过双重注意力网络来准确而简洁地表示操作和机器之间错综复杂的关系，以协同地确定FJSP的优先级分配规则。

    

    弹性制造催生了复杂的调度问题，如弹性车间调度问题（FJSP）。在FJSP中，操作可以在多台机器上进行处理，导致操作和机器之间存在错综复杂的关系。最近的研究利用深度强化学习（DRL）来学习优先级分配规则（PDRs）以解决FJSP。然而，相对于诸如OR-Tools等精确方法的解决方案，解决方案的质量仍有提高的空间。为了解决这个问题，本文提出了一种新的端到端学习框架，结合了自注意模型进行深度特征提取和DRL进行可扩展决策制定的优点。操作和机器之间的复杂关系被准确而简洁地表示出来，提出了一个由多个相互连接的操作信息注意块和机器信息注意块组成的双注意力网络（DAN）。DAN利用这些复杂的关系，以协同地确定FJSP的PDRs。

    Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this paper presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to co
    
[^77]: 健康保险索赔数据中历史偏差的大规模研究

    Large-Scale Study of Temporal Shift in Health Insurance Claims. (arXiv:2305.05087v1 [cs.LG])

    [http://arxiv.org/abs/2305.05087](http://arxiv.org/abs/2305.05087)

    本文是对健康保险索赔数据中历史偏差进行的大规模研究，通过构建算法测试时间偏移，进行回顾性扫描以寻找时间偏移，并创建1010个任务来评估242项医疗保健结果。研究发现有9.7%的任务显示出人群水平的时间偏移，93%显示出已发现的子人群内的时间偏移。

    

    多数用于预测临床结果的机器学习模型是通过历史数据开发的。然而，即使这些模型在不久的将来部署，数据集随时间发生偏移可能会导致不理想的表现。为了捕捉这种现象，我们认为在特定时间点预测的任务（即要预测的结果）是非平稳的，如果历史模型不再是预测该结果的最佳模型。我们构建了一个算法来测试人群水平或已发现的子人群内的时间偏移。然后，我们构建了一个元算法，在一组大量的任务中执行回顾性扫描以寻找时间偏移。根据我们所知，这是健康保健领域对历史偏差的首次全面评估。我们通过评估242项医疗保健结果从2015年到2020年的健康保险索赔数据集中的历史偏移来创建1010个任务。9.7%的任务显示出人群水平的时间偏移，93.0%显示出已发现的子人群内的时间偏移。

    Most machine learning models for predicting clinical outcomes are developed using historical data. Yet, even if these models are deployed in the near future, dataset shift over time may result in less than ideal performance. To capture this phenomenon, we consider a task--that is, an outcome to be predicted at a particular time point--to be non-stationary if a historical model is no longer optimal for predicting that outcome. We build an algorithm to test for temporal shift either at the population level or within a discovered sub-population. Then, we construct a meta-algorithm to perform a retrospective scan for temporal shift on a large collection of tasks. Our algorithms enable us to perform the first comprehensive evaluation of temporal shift in healthcare to our knowledge. We create 1,010 tasks by evaluating 242 healthcare outcomes for temporal shift from 2015 to 2020 on a health insurance claims dataset. 9.7% of the tasks show temporal shifts at the population level, and 93.0% ha
    
[^78]: LSGNN：通过局部相似性实现节点分类中的普适图神经网络

    LSGNN: Towards General Graph Neural Network in Node Classification by Local Similarity. (arXiv:2305.04225v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04225](http://arxiv.org/abs/2305.04225)

    本文提出使用局部相似性（LocalSim）学习节点级加权融合的即插即用模块，提取更具信息性的多跳信息，并对其有效性进行了理论分析。在真实基准数据集上的广泛评估表明，我们提出的LSGNN方法在同质性和异质性图上均能提供可比或优于最先进的性能。

    

    异质性被认为是伤害图神经网络（GNN）性能的问题。为了解决这个问题，一些现有的工作使用多跳邻居信息的图级加权融合来包含更多具有同质性的节点。然而，异质性可能在节点之间不同，需要考虑局部拓扑。在此基础上，本文提出使用局部相似性（LocalSim）学习节点级加权融合，并可作为即插即用模块。为了更好地融合，我们提出了一种新的、高效的初始残差差连接（IRDC）来提取更具信息性的多跳信息。此外，我们对合成图上LocalSim代表节点同质性的有效性进行了理论分析。在真实基准数据集上的广泛评估表明，我们提出的方法，即局部相似性图神经网络（LSGNN），在同质性和异质性图上均能提供可比或优于最先进的性能。

    Heterophily has been considered as an issue that hurts the performance of Graph Neural Networks (GNNs). To address this issue, some existing work uses a graph-level weighted fusion of the information of multi-hop neighbors to include more nodes with homophily. However, the heterophily might differ among nodes, which requires to consider the local topology. Motivated by it, we propose to use the local similarity (LocalSim) to learn node-level weighted fusion, which can also serve as a plug-and-play module. For better fusion, we propose a novel and efficient Initial Residual Difference Connection (IRDC) to extract more informative multi-hop information. Moreover, we provide theoretical analysis on the effectiveness of LocalSim representing node homophily on synthetic graphs. Extensive evaluations over real benchmark datasets show that our proposed method, namely Local Similarity Graph Neural Network (LSGNN), can offer comparable or superior state-of-the-art performance on both homophilic
    
[^79]: 学习6D非抓取式操作的混合演员-评论员地图

    Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])

    [http://arxiv.org/abs/2305.03942](http://arxiv.org/abs/2305.03942)

    论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。

    

    在人类的灵巧性中，非抓取式操作是操作物体的重要组成部分。非抓取式操纵可以使与物体的交互更加复杂，但也在推理交互方面提出了挑战。在本文中，我们引入了一个名为HACMan的混合演员评论员地图，这是一种使用点云观察的6D非抓取式物体操作的强化学习方法。HACMan提出了一种时间抽象和空间基础的物体中心动作表示，该表示包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。我们修改了一个现有的离线策略RL算法，以在这种混合的离散-连续动作表示学习。我们在仿真和现实世界中对HACMan进行了6D物体姿态对齐任务的评估。在最难的任务版本中，通过随机初始化物体和机器人配置，HACMan的表现优于现有的基线方法。

    Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
    
[^80]: ChatGPT和Bard是否应该与其数据提供者分享收益？AI时代的新商业模式

    Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era. (arXiv:2305.02555v1 [cs.LG])

    [http://arxiv.org/abs/2305.02555](http://arxiv.org/abs/2305.02555)

    ChatGPT和Bard等AI工具需要持续大量且高质量的数据来提高其性能，但现行的版权法则限制了它们对各种数据的获取。与数据提供者分享收益将有助于将AI工具与大多数版权数据拥有者之间的敌对关系转变为合作关系，使AI生态系统更健康。

    

    随着ChatGPT等各种人工智能工具越来越受欢迎，我们正进入真正的AI时代。我们可以预见，卓越的AI工具很快将获得可观的利润。一个关键问题出现了：除了传统的利益相关者和股东，AI工具是否应该与它们的训练数据提供者分享收益？答案是肯定的。大型AI工具，例如大型语言模型，始终需要更多、更高质量的数据来不断改进，但当前的版权法限制了它们对各种类型数据的获取。在AI工具和数据提供者之间分享收益可以将当前敌对的零和游戏关系转变为一种合作和互利的关系，而这种关系对于促进AI工具、用户和数据提供者之间的良性循环发展、推动AI技术并建立健康的AI生态系统是必要的。然而，当前的收益分享商业模式……

    With various AI tools such as ChatGPT becoming increasingly popular, we are entering a true AI era. We can foresee that exceptional AI tools will soon reap considerable profits. A crucial question arise: should AI tools share revenue with their training data providers in additional to traditional stakeholders and shareholders? The answer is Yes. Large AI tools, such as large language models, always require more and better quality data to continuously improve, but current copyright laws limit their access to various types of data. Sharing revenue between AI tools and their data providers could transform the current hostile zero-sum game relationship between AI tools and a majority of copyrighted data owners into a collaborative and mutually beneficial one, which is necessary to facilitate the development of a virtuous cycle among AI tools, their users and data providers that drives forward AI technology and builds a healthy AI ecosystem. However, current revenue-sharing business models 
    
[^81]: 面向马尔可夫数据的流式PCA算法

    Streaming PCA for Markovian Data. (arXiv:2305.02456v1 [math.ST])

    [http://arxiv.org/abs/2305.02456](http://arxiv.org/abs/2305.02456)

    本文提出了一种面向马尔可夫数据采样的流式PCA算法，并获得了该算法在整个数据集上的第一个尖锐率，提高了算法的效率。同时，本文提出的自适应方案在模拟和真实数据示例中表现良好。

    

    自从Oja在1982年的经典论文中首次提出以来，Oja算法已成为流式主成分分析(PCA)的一种常用方法。本文研究了流式PCA问题，其中数据点从一个不可约、无周期、可逆的马尔可夫链中采样。我们的目标是估计平稳分布的未知协方差矩阵的前一个特征向量。这种情况适用于只能从马尔可夫链蒙特卡罗(MCMC)类型的算法中采样数据，并且目标是对该链的平稳分布的参数进行推断的情况。现有文献中大多数Oja算法的收敛保证都假定数据点是IID采样的。对于具有马尔可夫依赖关系的数据流，人们通常对数据进行下采样以获得"几乎"独立的数据流。在本文中，我们获得了Oja算法在整个数据集上的第一个尖锐率，其中去掉了$n$的对数依赖性，结果是$\mathcal{O}(n^{-1})$的速率。我们还提出了一种自适应方案来调整算法的步长，它在模拟和真实数据示例中都表现更好。

    Since its inception in Erikki Oja's seminal paper in 1982, Oja's algorithm has become an established method for streaming principle component analysis (PCA). We study the problem of streaming PCA, where the data-points are sampled from an irreducible, aperiodic, and reversible Markov chain. Our goal is to estimate the top eigenvector of the unknown covariance matrix of the stationary distribution. This setting has implications in situations where data can only be sampled from a Markov Chain Monte Carlo (MCMC) type algorithm, and the goal is to do inference for parameters of the stationary distribution of this chain. Most convergence guarantees for Oja's algorithm in the literature assume that the data-points are sampled IID. For data streams with Markovian dependence, one typically downsamples the data to get a "nearly" independent data stream. In this paper, we obtain the first sharp rate for Oja's algorithm on the entire data, where we remove the logarithmic dependence on $n$ resulti
    
[^82]: 流数据高效学习

    Stream Efficient Learning. (arXiv:2305.02217v1 [cs.LG])

    [http://arxiv.org/abs/2305.02217](http://arxiv.org/abs/2305.02217)

    本文介绍了“流高效学习”的概念，该概念旨在解决从数据流中机器学习的效率问题，其泛化性能不仅取决于接收到了多少数据，而且还取决于有多少数据能够及时有效地被利用，加上资源和速度的考虑。

    

    许多真实世界应用的数据往往随着时间的积累以流的形式进行。与传统的机器学习研究关注于从给定的训练数据集中学习不同，从数据流中学习不能忽视流入的数据流可能是无休止的、规模巨大、变化未知，并且假设有足够的计算/存储资源可以及时处理所有接收到的数据是不现实的。因此，从数据流中学习的泛化性能不仅取决于接收到了多少数据，而且取决于有多少数据能够被及时地有效利用，加上资源和速度的考虑，再加上学习算法的能力和问题的复杂度。为此，在本文中我们介绍了机器学习吞吐量的概念，定义了流高效学习，并提出了一个初步的理论框架。

    Data in many real-world applications are often accumulated over time, like a stream. In contrast to conventional machine learning studies that focus on learning from a given training data set, learning from data streams cannot ignore the fact that the incoming data stream can be potentially endless with overwhelming size and unknown changes, and it is impractical to assume to have sufficient computational/storage resource such that all received data can be handled in time. Thus, the generalization performance of learning from data streams depends not only on how many data have been received, but also on how many data can be well exploited timely, with resource and rapidity concerns, in addition to the ability of learning algorithm and complexity of the problem. For this purpose, in this article we introduce the notion of machine learning throughput, define Stream Efficient Learning and present a preliminary theoretical framework.
    
[^83]: 伪哈密顿神经网络用于学习偏微分方程

    Pseudo-Hamiltonian neural networks for learning partial differential equations. (arXiv:2304.14374v1 [cs.LG])

    [http://arxiv.org/abs/2304.14374](http://arxiv.org/abs/2304.14374)

    本文介绍了一种新方法伪哈密顿神经网络(PHNN)，可以用于学习偏微分方程。相比基线模型，PHNN表现更为优越，模型可应用于去除或改变外力情况并可分别得到三个不同物理解释的部分。

    

    最近提出了伪哈密顿神经网络(PHNN)来学习可以用普通微分方程建模的动力系统。本文将该方法扩展到了偏微分方程。所得模型由高达三个神经网络，模拟代表守恒、耗散和外力的项以及可以学习或为先前知识的离散卷积算子构成。我们通过数值结果表明PHNN相比单个神经网络建模的基线模型具有更优越的性能。此外，由于PHNN模型由三个具有不同物理解释的部分组成，可以分别研究这些部分以获得对系统的洞察，并且即使去除或改变外力，所学得的模型仍然适用。

    Pseudo-Hamiltonian neural networks (PHNN) were recently introduced for learning dynamical systems that can be modelled by ordinary differential equations. In this paper, we extend the method to partial differential equations. The resulting model is comprised of up to three neural networks, modelling terms representing conservation, dissipation and external forces, and discrete convolution operators that can either be learned or be prior knowledge. We demonstrate numerically the superior performance of PHNN compared to a baseline model that models the full dynamics by a single neural network. Moreover, since the PHNN model consists of three parts with different physical interpretations, these can be studied separately to gain insight into the system, and the learned model is applicable also if external forces are removed or changed.
    
[^84]: 非平稳环境下动态系统的实时安全评估：方法和技术综述

    Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques. (arXiv:2304.12583v1 [eess.SY])

    [http://arxiv.org/abs/2304.12583](http://arxiv.org/abs/2304.12583)

    本文综述了非平稳环境下动态系统实时安全评估的方法和技术，包括在线主动学习、在线半监督学习、在线迁移学习和在线异常检测，并讨论了未来的研究方向。

    

    动态系统的实时安全评估是一个关键任务，对于工业和交通应用等各个领域具有重要意义，特别是在非平稳环境中。然而，缺乏非平稳环境下实时安全评估方法的全面综述阻碍了相关方法的进展和改进。本文提供了针对非平稳环境下实时安全评估任务的方法和技术综述。具体而言，首先突出了非平稳环境中实时安全评估方法的背景和重要性。然后，我们提出了问题描述，包括定义、分类和主要挑战。我们还回顾了相关技术的最新发展，如在线主动学习、在线半监督学习、在线迁移学习和在线异常检测。最后，我们讨论了未来的展望和进一步研究的潜在方向。本文的综述旨在为非平稳环境下实时安全评估提供参考依据。

    Real-time safety assessment (RTSA) of dynamic systems is a critical task that has significant implications for various fields such as industrial and transportation applications, especially in non-stationary environments. However, the absence of a comprehensive review of real-time safety assessment methods in non-stationary environments impedes the progress and refinement of related methods. In this paper, a review of methods and techniques for RTSA tasks in non-stationary environments is provided. Specifically, the background and significance of RTSA approaches in non-stationary environments are firstly highlighted. We then present a problem description that covers the definition, classification, and main challenges. We review recent developments in related technologies such as online active learning, online semi-supervised learning, online transfer learning, and online anomaly detection. Finally, we discuss future outlooks and potential directions for further research. Our review aims
    
[^85]: 重访基于预训练语言模型的k-NN

    Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])

    [http://arxiv.org/abs/2304.09058](http://arxiv.org/abs/2304.09058)

    本研究提出一种新方法，结合k-NN和预训练语言模型（PLMs）能够提高自然语言处理（NLP）的性能，并在多个基准数据集上得到验证。

    

    预训练语言模型（PLMs）作为参数化的急切学习器，已成为自然语言处理（NLP）当前范式的实际选择。与此形成对比的是，k-最近邻（k-NN）分类器作为延迟学习模型，倾向于减轻过拟合和孤立噪声。本文中我们重访了k-NN分类器，以增强基于PLMs的分类器。从方法层面上，我们提出采用文本表示的PLMs在两个步骤中采用k-NN：（1）利用k-NN作为先验知识来校准训练过程（2）线性插值k-NN预测的概率分布和PLMs分类器的概率分布。我们的方法核心是实现了k-NN校准训练，将预测结果作为训练过程中易于和难以学习的示例的指标。从应用场景多样性的角度出发，我们在各种基准数据集上进行了广泛的微调、提示微调范式和零样本任务设置的实验。我们的结果表明，结合k-NN可以在所有受到检查的设置中持续提高PLMs的性能，并且在所有受到考虑的设置中跑赢了基于普通PLMs的方法。

    Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-sh
    
[^86]: 交叉熵损失函数：理论分析与应用

    Cross-Entropy Loss Functions: Theoretical Analysis and Applications. (arXiv:2304.07288v1 [cs.LG])

    [http://arxiv.org/abs/2304.07288](http://arxiv.org/abs/2304.07288)

    本文对交叉熵、广义交叉熵、均方误差等一大类损失函数进行了理论分析，并提出了具有优势的双交叉熵损失函数，特别适用于存在标签噪声或类别不平衡的情况。

    

    交叉熵是广泛应用的损失函数。当使用softmax函数时，它与神经网络输出应用于逻辑回归损失函数相符。但是，使用交叉熵作为代理损失函数时，我们能依靠什么保证呢？我们提出了对广泛的损失函数家族进行理论分析，包括交叉熵（或逻辑损失）、广义交叉熵、均方误差和其他交叉熵类函数。我们给出了这些损失函数的第一个$H$-连续性界限。这些都是非渐进保证，以估计代理损失的估计误差为上限，用于特定的假设集$H$。我们进一步展示了这些边界的紧密程度。这些边界取决于称为可最小化间隙的量，这些间隙只取决于损失函数和假设集。为了使它们更具体化，我们对复杂和损失函数的这些间隙进行了具体分析。我们还引入了一种新的损失函数，称为双交叉熵损失，它基于两个交叉熵损失的组合。我们表明，它可以优于标准交叉熵损失，特别是在存在标签噪声或类别不平衡的情况下。

    Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of losses, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other loss cross-entropy-like functions. We give the first $H$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set $H$ used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps, which only depend on the loss function and the hypothesis set. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduc
    
[^87]: 基于L2，0基数惩罚的不均匀图趋势过滤。

    Inhomogeneous graph trend filtering via a l2,0 cardinality penalty. (arXiv:2304.05223v1 [cs.LG])

    [http://arxiv.org/abs/2304.05223](http://arxiv.org/abs/2304.05223)

    本文提出了一种基于L2，0基数惩罚的图趋势过滤（GTF）模型，可同时进行k-means聚类和基于图的最小割，以估计在节点之间具有不均匀平滑水平的分段平滑图信号，并在降噪、支持恢复和半监督分类任务上表现更好，比现有方法更高效地处理大型数据集。

    

    我们研究了在图上估计分段平滑信号的方法，并提出了一种$\ell_{2,0}$-范数惩罚图趋势过滤（GTF）模型，以估计在节点之间具有不均匀平滑水平的分段平滑图信号。我们证明了所提出的GTF模型同时是基于节点上的信号的k-means聚类和基于图的最小割，其中聚类和割共享相同的分配矩阵。我们提出了两种方法来解决所提出的GTF模型：一种是基于谱分解的方法，另一种是基于模拟退火的方法。在合成和现实数据集的实验中，我们展示了所提出的GTF模型在降噪、支持恢复和半监督分类任务上表现更好，且比现有方法更高效地解决了大型数据集的问题。

    We study estimation of piecewise smooth signals over a graph. We propose a $\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimate piecewise smooth graph signals that exhibits inhomogeneous levels of smoothness across the nodes. We prove that the proposed GTF model is simultaneously a k-means clustering on the signal over the nodes and a minimum graph cut on the edges of the graph, where the clustering and the cut share the same assignment matrix. We propose two methods to solve the proposed GTF model: a spectral decomposition method and a method based on simulated annealing. In the experiment on synthetic and real-world datasets, we show that the proposed GTF model has a better performances compared with existing approaches on the tasks of denoising, support recovery and semi-supervised classification. We also show that the proposed GTF model can be solved more efficiently than existing models for the dataset with a large edge set.
    
[^88]: OpenAGI：当LLM遇到领域专家

    OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])

    [http://arxiv.org/abs/2304.04370](http://arxiv.org/abs/2304.04370)

    基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。

    

    人类具有将基本技能组合成复杂技能以解决复杂任务的显著能力。这种能力对于人工智能同样重要，因此，我们断言，除了开发大型综合智能模型外，将不同领域专家模型应用于复杂任务解决能力同样关键，以在人工智能通用智能的追求中使其具备这种能力。最近的大型语言模型（LLM）的发展证明其具有出色的学习和推理能力，使它们成为选择、综合和执行外部模型以解决复杂任务的控制器的有前途的选择。在这个项目中，我们开发了一个名为OpenAGI的开源AGI研究平台，专门设计为提供复杂的多步骤任务，并配有任务特定的数据集、评估指标和各种可扩展模型。OpenAGI将复杂任务阐释为自然语言问答，旨在促进领域专家和语言模型之间的协同作用。

    Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
    
[^89]: 最大序数二次因子分解

    Maximal Ordinal Two-Factorizations. (arXiv:2304.03338v1 [cs.AI])

    [http://arxiv.org/abs/2304.03338](http://arxiv.org/abs/2304.03338)

    本文研究了最大序数二次因子分解问题，证明了其判定是否存在是一个NP完全问题，并提供了用于计算最大因子分解的算法Ord2Factor。

    

    在一个形式背景中，序数因子是其关系的子集，形成概念格中的链，即对应于线性顺序的数据集的一部分。为了可视化形式上下文中的数据，Ganter和Glodeanu提出了基于两个序数因子的双图。为了使双图有用，重要的是这些因子尽可能包含更多数据点，即覆盖尽可能多的关系。本文研究这样的序数二次因子分解。首先，我们研究了省略序数二次因子分解的形式背景中两个因子的不相交性。然后，我们证明判定给定大小的二次因子分解是否存在是一个NP完全问题，这使得计算最大因子分解具有计算成本。最后，我们提供了算法Ord2Factor，它允许我们计算大的序数二次因子分解。

    Given a formal context, an ordinal factor is a subset of its incidence relation that forms a chain in the concept lattice, i.e., a part of the dataset that corresponds to a linear order. To visualize the data in a formal context, Ganter and Glodeanu proposed a biplot based on two ordinal factors. For the biplot to be useful, it is important that these factors comprise as much data points as possible, i.e., that they cover a large part of the incidence relation. In this work, we investigate such ordinal two-factorizations. First, we investigate for formal contexts that omit ordinal two-factorizations the disjointness of the two factors. Then, we show that deciding on the existence of two-factorizations of a given size is an NP-complete problem which makes computing maximal factorizations computationally expensive. Finally, we provide the algorithm Ord2Factor that allows us to compute large ordinal two-factorizations.
    
[^90]: 区域风力特征如何影响基于CNN的风速预测：来自时空相关性分析的见解。

    How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis. (arXiv:2304.01545v1 [cs.LG])

    [http://arxiv.org/abs/2304.01545](http://arxiv.org/abs/2304.01545)

    本研究探讨了使用3D卷积神经网络（3D-CNN）对时空数据进行风速预测的精度，并发现使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。

    

    本研究探讨了时空数据维度对利用人工神经网络构建的风速预测模型精度的影响。尽管以前的研究表明，加入空间数据可以提高风速预测模型的精度，但很少有研究探讨了基于神经网络的预测模型中不同空间尺度改进的程度。此外，对于这些模型的最佳时间长度的输入数据的研究也很有限。为了解决这个问题，本研究在使用3D卷积神经网络（3D-CNN）预测风速时，采用具有不同时空维度的数据作为输入，并评估其预测性能。结果表明，使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。此外，多时间数据对预测性能的影响更为显著。

    This study investigates the impact of spatiotemporal data dimensions on the precision of a wind forecasting model developed using an artificial neural network. Although previous studies have shown that incorporating spatial data can enhance the accuracy of wind forecasting models, few investigations have explored the extent of the improvement owing to different spatial scales in neural network-based predictive models. Additionally, there are limited studies on the optimal temporal length of the input data for these models. To address this gap, this study employs data with various spatiotemporal dimensions as inputs when forecasting wind using 3D-Convolutional Neural Networks (3D-CNN) and assesses their predictive performance. The results indicate that using spatial data of the surrounding area for 3D-CNN training can achieve better predictive performance than using only single-point information. Additionally, multi-time data had a more positive effect on the predictive performance than
    
[^91]: 从哥德尔不完备定理到机器人宗教的完备性（扩展摘要）

    From G\"odel's Incompleteness Theorem to the completeness of bot religions (Extended abstract). (arXiv:2303.14338v1 [cs.AI])

    [http://arxiv.org/abs/2303.14338](http://arxiv.org/abs/2303.14338)

    本文研究了从哥德尔不完备定理到机器人宗教的完备性的逻辑过程，提出了任何信仰系统可以被形式化为逻辑理论，并且不完备定理意味着存在真实但无法证明的陈述，可以用来定义出与现有信仰和传统一致的新宗教实践。

    

    Hilbert 和 Ackermann 提出了一种将不完备理论一致地扩展到完备理论的方法。哥德尔基本上证明了任何能够对其自身陈述及其证明进行编码的理论都包含了真实但不能被证明的陈述。哥德尔的构造并没有回答希尔伯特的问题，希尔伯特认为理论可以通过逐步添加公理来证明越来越多的真实陈述，就像科学一样，完备性是消失点。我们研究了底层的逻辑过程，并描述了导致可测试但不可行的机器人宗教的轨迹，这些宗教扩展了传统宗教并提出了新的仪式和信仰。我们的方法是基于任何信仰系统都可以被形式化为一个逻辑理论的想法，并且不完备定理意味着存在真实但无法证明的陈述，可以并入这个理论。我们提供了这样的例子，并展示了如何使用它们来定义与现有信仰和传统一致的新宗教实践。

    Hilbert and Ackermann asked for a method to consistently extend incomplete theories to complete theories. G\"odel essentially proved that any theory capable of encoding its own statements and their proofs contains statements that are true but not provable. Hilbert did not accept that G\"odel's construction answered his question, and in his late writings and lectures, G\"odel agreed that it did not, since theories can be completed incrementally, by adding axioms to prove ever more true statements, as science normally does, with completeness as the vanishing point. This pragmatic view of validity is familiar not only to scientists who conjecture test hypotheses but also to real estate agents and other dealers, who conjure claims, albeit invalid, as necessary to close a deal, confident that they will be able to conjure other claims, albeit invalid, sufficient to make the first claims valid. We study the underlying logical process and describe the trajectories leading to testable but unfal
    
[^92]: 基于学习的静态恶意软件分类器的对抗性鲁棒性

    Adversarial Robustness of Learning-based Static Malware Classifiers. (arXiv:2303.13372v1 [cs.CR])

    [http://arxiv.org/abs/2303.13372](http://arxiv.org/abs/2303.13372)

    本文提出了一种通用的对抗性补丁（UAP）攻击方法，只需附加一个相对较小的字节级补丁即可绕过MalConv分类器的检测，可以将恶意软件文件的检测率降低80％。同时，作者提出了窗口消除处理作为应对此种攻击的一种方法。

    

    恶意软件检测一直是恶意软件作者和反病毒系统之间持续的军备竞赛阶段。随着这场竞赛规模的不断增加，利用机器学习（ML）的解决方案得到了关注。然而，这种趋势使得直接对ML进行攻击对于对手而言成为一种有吸引力的前景。本文研究了这场军备竞赛的两个方面，即从恶意软件文件的原始字节中操作的基于卷积神经网络的流行分类器MalConv的角度。首先，我们表明MalConv易受到对抗性补丁攻击的影响:将一个字节级的补丁附加到恶意软件文件中，使其绕过检测的概率高达94.3％。此外，我们开发了一种通用的对抗性补丁（UAP）攻击，在任何包含该补丁的恶意软件文件的恒定时间内，可以将其检测率降低80％。即使相对于原始文件大小而言，这些补丁的大小也相对较小-在2％-8％之间。为了抵御这种攻击，我们进行了窗口消除处理，允许识别恶意代码的部分不受对抗性补丁攻击。

    Malware detection has long been a stage for an ongoing arms race between malware authors and anti-virus systems. Solutions that utilize machine learning (ML) gain traction as the scale of this arms race increases. This trend, however, makes performing attacks directly on ML an attractive prospect for adversaries. We study this arms race from both perspectives in the context of MalConv, a popular convolutional neural network-based malware classifier that operates on raw bytes of files. First, we show that MalConv is vulnerable to adversarial patch attacks: appending a byte-level patch to malware files bypasses detection 94.3% of the time. Moreover, we develop a universal adversarial patch (UAP) attack where a single patch can drop the detection rate in constant time of any malware file that contains it by 80%. These patches are effective even being relatively small with respect to the original file size -between 2%-8%. As a countermeasure, we then perform window ablation that allows u
    
[^93]: 图卡尔曼滤波器

    Graph Kalman Filters. (arXiv:2303.12021v1 [cs.LG])

    [http://arxiv.org/abs/2303.12021](http://arxiv.org/abs/2303.12021)

    本文首次将卡尔曼和扩展卡尔曼滤波器推广到图形上，使得它可以适用于输出是向量或标量的情况，并且可以学习未知的状态转移和读取函数。

    

    众所周知，卡尔曼滤波器通过使用状态空间表示来模拟动态系统，下一个状态的更新以及与新观察到的系统输出相关的信息来控制其不确定性。本文首次将卡尔曼和扩展卡尔曼滤波器推广到离散时间的设置下，其中输入、状态和输出均表示为带属性的图形，其拓扑和属性可以随时间变化。此设置使得我们可以将框架适应于输出是向量或标量的情况（节点/图级任务）。在所提出的理论框架内，未知的状态转移和读取函数与下游预测任务一起端到端学习。

    The well-known Kalman filters model dynamical systems by relying on state-space representations with the next state updated, and its uncertainty controlled, by fresh information associated with newly observed system outputs. This paper generalizes, for the first time in the literature, Kalman and extended Kalman filters to discrete-time settings where inputs, states, and outputs are represented as attributed graphs whose topology and attributes can change with time. The setup allows us to adapt the framework to cases where the output is a vector or a scalar too (node/graph level tasks). Within the proposed theoretical framework, the unknown state-transition and the readout functions are learned end-to-end along with the downstream prediction task.
    
[^94]: 不受体系结构、数据集和模型规模限制的无数据元学习

    Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning. (arXiv:2303.11183v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11183](http://arxiv.org/abs/2303.11183)

    不受体系结构、数据集和模型规模限制的无数据元学习框架PURER，通过ECI执行伪周期训练以适应新的任务，通过ICFIL对反演梯度进行校准来优化反演过程，并在各种任务中显著优于现有方法。

    

    无数据元学习的目的是从一组经过预训练的模型中学习有用的先验知识，而无需访问其训练数据。然而，现有的研究仅在参数空间中解决了该问题，忽略了预训练模型中蕴含的丰富数据知识，无法扩展到大规模预训练模型，只能元学习具有相同网络架构的预训练模型。为了解决这些问题，我们提出了一个统一的框架——PURER，其中包含：（1）数据无关的元训练期间的节目课程反转（ECI）；（2）元测试期间内部循环后的反演校准（ICFIL）。在元训练期间，我们提出了ECI来执行伪周期训练，以便快速适应新的看不见的任务。在元测试期间，我们提出了ICFIL来校准反演梯度，以减少基于反演的优化的负面影响。广泛的实验结果表明，所提出的PURER可以有效地元学习来自具有不同网络架构、数据集域甚至不同大小的预训练模型，并在各种任务中显著优于现有方法。

    The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PURER, which contains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta testing. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. The ECI adaptively increases the difficulty level of pseudo episodes accord
    
[^95]: 通过空间时间数据过拟合实现高质量高效的视频超分辨率

    Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting. (arXiv:2303.08331v1 [cs.CV])

    [http://arxiv.org/abs/2303.08331](http://arxiv.org/abs/2303.08331)

    本论文提出了一种利用空间时间信息来提高视频超分辨率的新方法，采用高维卷积网络进行预测并应用时间注意机制以去除冗余信息并提高效率。

    

    随着深度卷积神经网络(DNN)在计算机视觉的各个领域得到广泛应用，利用DNN的过拟合能力实现视频分辨率的提升已经成为现代视频传输系统的新趋势。将视频分为块并将每个块与超分辨率模型过拟合，从而在传输给客户端之前对视频进行编码，从而实现更好的视频质量和传输效率。然而，为了保证良好的过拟合质量，需要大量的块，这会大大增加存储量和消耗更多带宽资源进行数据传输。另一方面，通过训练优化技术减少块的数量通常需要高模型容量，这会显著降低执行速度。为了解决这个问题，我们提出了一种新的方法来完成高质量和高效的视频分辨率升级任务，利用空间时间信息来准确捕捉有限训练数据的视频块的预测结果。具体来说，我们提出利用视频块的空间时间相关性，使用高维卷积网络改进每个块的预测，并进一步应用时间注意机制以去除冗余信息并促进传输。实验结果表明，我们的方法在视觉质量和效率方面均优于现有方法。

    As deep convolutional neural networks (DNNs) are widely used in various fields of computer vision, leveraging the overfitting ability of the DNN to achieve video resolution upscaling has become a new trend in the modern video delivery system. By dividing videos into chunks and overfitting each chunk with a super-resolution model, the server encodes videos before transmitting them to the clients, thus achieving better video quality and transmission efficiency. However, a large number of chunks are expected to ensure good overfitting quality, which substantially increases the storage and consumes more bandwidth resources for data transmission. On the other hand, decreasing the number of chunks through training optimization techniques usually requires high model capacity, which significantly slows down execution speed. To reconcile such, we propose a novel method for high-quality and efficient video resolution upscaling tasks, which leverages the spatial-temporal information to accurately
    
[^96]: 机器学习基准性能评估中的多重性问题

    What is the state of the art? Accounting for multiplicity in machine learning benchmark performance. (arXiv:2303.07272v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2303.07272](http://arxiv.org/abs/2303.07272)

    机器学习基准性能评估中，最先进的（SOTA）性能的估计值过于乐观，容易导致方法的忽视。本文提供了一个概率模型，用于校正多重性偏差并比较方法的性能。

    

    机器学习方法通常通过在公共数据库中的数据集上的性能来进行评估和比较。这允许多种方法，在相同条件下并跨越时间进行评估。在问题中排名最高的性能被称为最先进的（SOTA）性能，并且被用作新方法出版的参考点。但使用最高排名的性能作为SOTA的估计值是一种有偏的估计器，会给出过于乐观的结果。这种多重性的机制是多重比较和多重检验中广泛研究的主题，但在关于SOTA估计的讨论中几乎没有得到提及。过于乐观的最先进估计值被用作评估新方法的标准，而具有明显劣势结果的方法很容易被忽视。在本文中，我们提供了一个概率模型，用于校正多重性偏差并比较方法的性能。

    Machine learning methods are commonly evaluated and compared by their performance on data sets from public repositories. This allows for multiple methods, oftentimes several thousands, to be evaluated under identical conditions and across time. The highest ranked performance on a problem is referred to as state-of-the-art (SOTA) performance, and is used, among other things, as a reference point for publication of new methods. Using the highest-ranked performance as an estimate for SOTA is a biased estimator, giving overly optimistic results. The mechanisms at play are those of multiplicity, a topic that is well-studied in the context of multiple comparisons and multiple testing, but has, as far as the authors are aware of, been nearly absent from the discussion regarding SOTA estimates. The optimistic state-of-the-art estimate is used as a standard for evaluating new methods, and methods with substantial inferior results are easily overlooked. In this article, we provide a probability 
    
[^97]: 迈向MoE部署：缓解混合专家（MoE）推理中的低效率

    Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference. (arXiv:2303.06182v1 [cs.DC])

    [http://arxiv.org/abs/2303.06182](http://arxiv.org/abs/2303.06182)

    本文提出了三种优化技术来缓解混合专家（MoE）模型在推理时的低效率，包括动态门控、专家缓冲和专家负载平衡。这些技术可以显著提高执行时间和减少内存使用。

    This paper proposes three optimization techniques to mitigate inefficiencies in Mixture-of-Experts (MoE) models during inference, including dynamic gating, expert buffering, and expert load balancing. These techniques can significantly improve execution time and reduce memory usage.

    混合专家（MoE）模型最近在计算机视觉和自然语言处理的广泛任务中取得了最先进的性能。它们在训练期间有效地扩展了模型容量，同时增加的计算成本很小。然而，由于其庞大的模型大小和复杂的通信模式，部署这样的模型进行推理是困难的。在这项工作中，我们提供了两个MoE工作负载的特征化，即语言建模（LM）和机器翻译（MT），并确定了它们在部署时的低效率来源。我们提出了三种优化技术来缓解低效率的来源，即（1）动态门控，（2）专家缓冲和（3）专家负载平衡。我们展示了动态门控可以使LM的执行时间提高1.25-4倍，MT编码器提高2-5倍，MT解码器提高1.09-1.5倍。它还可以将LM的内存使用减少高达1.36倍，MT的内存使用减少高达1.1倍。

    Mixture-of-Experts (MoE) models have recently gained steam in achieving the state-of-the-art performance in a wide range of tasks in computer vision and natural language processing. They effectively expand the model capacity while incurring a minimal increase in computation cost during training. However, deploying such models for inference is difficult due to their large model size and complex communication pattern. In this work, we provide a characterization of two MoE workloads, namely Language Modeling (LM) and Machine Translation (MT) and identify their sources of inefficiencies at deployment.  We propose three optimization techniques to mitigate sources of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show that dynamic gating improves execution time by 1.25-4$\times$ for LM, 2-5$\times$ for MT Encoder and 1.09-1.5$\times$ for MT Decoder. It also reduces memory usage by up to 1.36$\times$ for LM and up to 1.1$\times$ for MT. We f
    
[^98]: Pacos: 建模用户的可解释和上下文依赖选择以处理偏好逆转问题

    Pacos: Modeling Users' Interpretable and Context-Dependent Choices in Preference Reversals. (arXiv:2303.05648v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2303.05648](http://arxiv.org/abs/2303.05648)

    Pacos是一个上下文依赖的偏好模型，可以处理偏好逆转问题，并提供用户的自适应权重、比较和显示位置等可解释因素，有助于提供个性化服务。

    

    选择问题是指从多个项目中选择最佳选择，学习用户在选择问题中的偏好对于理解决策机制和提供个性化服务具有重要意义。现有的研究通常假设人们独立地评估项目。在实践中，但是用户的偏好取决于项目所处的市场，这被称为上下文效应；而用户对两个项目的偏好顺序甚至可能被颠倒，这被称为偏好逆转。本文识别了导致上下文效应的三个因素：用户的自适应权重、项目之间的比较和显示位置。我们提出了一个名为Pacos的上下文依赖偏好模型作为统一框架来同时解决这三个因素，并考虑了两种设计方法，包括具有高可解释性的加性方法和具有高准确性的基于ANN的方法。我们研究了基于各种市场情景和模型参数的偏好逆转条件，并展示了Pacos可以有效地捕捉偏好逆转。此外，Pacos可以提供上下文效应和用户自适应行为的可解释指示，有助于提供个性化服务。

    Choice problems refer to selecting the best choices from several items, and learning users' preferences in choice problems is of great significance in understanding the decision making mechanisms and providing personalized services. Existing works typically assume that people evaluate items independently. In practice, however, users' preferences depend on the market in which items are placed, which is known as context effects; and the order of users' preferences for two items may even be reversed, which is referred to preference reversals. In this work, we identify three factors contributing to context effects: users' adaptive weights, the inter-item comparison, and display positions. We propose a context-dependent preference model named Pacos as a unified framework for addressing three factors simultaneously, and consider two design methods including an additive method with high interpretability and an ANN-based method with high accuracy. We study the conditions for preference reversa
    
[^99]: 文本到图像合成的GAN扩展

    Scaling up GANs for Text-to-Image Synthesis. (arXiv:2303.05511v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05511](http://arxiv.org/abs/2303.05511)

    本研究介绍了GigaGAN，一种新型的GAN架构，远远超过之前使用的StyleGAN。GigaGAN具有三个重要优势：超快速的推理时间、高分辨率图像合成的能力和更精确的图像合成控制。这使GAN成为文本到图像合成的可行选项。

    

    最近，文本到图像合成的成功引起了全球范围内的关注，并吸引了普通大众的想象力。从技术角度来看，它也标志着设计生成图像模型的优先架构发生了根本性变化。过去，GAN是事实上的选择，有着像StyleGAN这样的技术。然而，DALL-E2之后，自回归和扩散模型成为了大尺度生成模型的新标准。这个快速的转变引出了一个根本性问题：我们能否扩展GAN以从像LAION这样的大数据集中受益？我们发现，简单地增加StyleGAN架构的容量很快就会变得不稳定。我们介绍GigaGAN，这是一种新型的GAN架构，远远超过了这个限制，证明GAN是文本到图像合成的一个可行选项。GigaGAN提供了三个重要优势。首先，它的推理时间比之前快了几个数量级，只需要0.13秒即可合成512像素图像。其次，它可以合成高分辨率的图像，例如2048×2048，这是之前在该领域内GAN无法实现的。第三，GigaGAN允许更好地控制图像合成过程，使用户能够更精确地指定姿势、光照和背景等特征。

    The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL-E 2, auto-regressive and diffusion models became the new standard for large-scale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that na\"Ively increasing the capacity of the StyleGAN architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for exam
    
[^100]: Cal-QL: 计算机辅助脱机强化学习预先训练用于高效在线微调

    Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05479](http://arxiv.org/abs/2303.05479)

    本文介绍了一种计算机辅助脱机强化学习方法Cal-QL，该方法能够从脱机数据中学习一个保守的值函数初始化，使得在在线微调时同时保障了快速和性能。

    

    脱机强化学习方法可以用来从现有数据集中获取策略初始化并通过有限互动进行快速在线微调。然而，现有的脱机强化学习方法在在线微调中表现较差。本文研究了保守脱机强化学习方法中的微调问题，并设计了一种方法，可以从脱机数据中学习到有效的初始化，并使其具备快速的在线微调功能。我们的方法，即Cal-QL，通过学习一个保守的值函数初始化，低估从脱机数据中学到的策略的价值，同时确保学习到的Q值在合理的范围内。实验结果表明，我们的方法在多个基准环境中具有显著的性能优势，并且也能在真实机器人问题中进行有效的在线微调。

    A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply
    
[^101]: 基于身体知识的关系状态抽象的双层规划中的主动学习

    Embodied Active Learning of Relational State Abstractions for Bilevel Planning. (arXiv:2303.04912v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.04912](http://arxiv.org/abs/2303.04912)

    该论文提出了一种基于身体知识的主动学习方法，通过在线交互与专家学习神经谓词解释、符号规划算子和任务特定的关系状态抽象，以提高机器人操作任务中的性能。

    

    状态抽象是在机器人环境中进行规划的有效技术，该环境具有连续状态和动作、长任务时间和稀疏反馈。在面向对象的环境中，谓词是一种特别有用的状态抽象形式，因为其与符号规划器的兼容性以及其关系泛化的能力。然而，要使用谓词进行规划，代理必须能够在连续环境状态下解释它们（即接地符号）。手动编程谓词解释可能很困难，因此我们希望从数据中学习它们。我们提出了一种基于身体知识的主动学习范式，其中代理通过与专家进行在线交互来学习谓词解释。例如，在堆叠积木环境中采取行动后，代理可能会问专家：“On(block1，block2)是否为真？”从这个经验中，代理学习规划：学习神经谓词解释、符号规划算子和任务特定的关系状态抽象。我们的实验表明，这种方法可以学习有效的抽象，并在一系列机器人操作任务中改进基线的性能。

    State abstraction is an effective technique for planning in robotics environments with continuous states and actions, long task horizons, and sparse feedback. In object-oriented environments, predicates are a particularly useful form of state abstraction because of their compatibility with symbolic planners and their capacity for relational generalization. However, to plan with predicates, the agent must be able to interpret them in continuous environment states (i.e., ground the symbols). Manually programming predicate interpretations can be difficult, so we would instead like to learn them from data. We propose an embodied active learning paradigm where the agent learns predicate interpretations through online interaction with an expert. For example, after taking actions in a block stacking environment, the agent may ask the expert: "Is On(block1, block2) true?" From this experience, the agent learns to plan: it learns neural predicate interpretations, symbolic planning operators, an
    
[^102]: 一种基于Inception-Residual的架构和多目标损失的呼吸异常检测系统

    An Inception-Residual-Based Architecture with Multi-Objective Loss for Detecting Respiratory Anomalies. (arXiv:2303.04104v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2303.04104](http://arxiv.org/abs/2303.04104)

    本文提出了一种深度学习系统，用于从呼吸声音记录中检测异常。该系统使用Gammatone和连续小波变换进行音频特征提取，并将Inception-Residual-based的骨干模型与multi-head attention和multi-objective loss相结合，通过线性组合的方法来平衡每个单独频谱图的贡献。

    

    本文介绍了一种应用于呼吸声音记录中的异常检测的深度学习系统。我们的系统开始于使用Gammatone和Continuous Wavelet转换进行音频特征提取。这一步旨在将呼吸声音输入转换为二维频谱图，其中展现了谱和时域特征。然后，我们提出的系统将Inception-Residual-based的骨干模型与multi-head attention和multi-objective loss相结合，用于分类呼吸异常。我们提出了一种线性组合方法，而不是简单的连接方法，这种方法具有在整个训练过程中平衡每个单独频谱图贡献的能力。我们在IEEE BioCAS 2022挑战赛提出的SPRSound（开源SJTU儿科呼吸声音）基准数据集上进行实验评估。

    This paper presents a deep learning system applied for detecting anomalies from respiratory sound recordings. Initially, our system begins with audio feature extraction using Gammatone and Continuous Wavelet transformation. This step aims to transform the respiratory sound input into a two-dimensional spectrogram where both spectral and temporal features are presented. Then, our proposed system integrates Inception-residual-based backbone models combined with multi-head attention and multi-objective loss to classify respiratory anomalies. Instead of applying a simple concatenation approach by combining results from various spectrograms, we propose a Linear combination, which has the ability to regulate equally the contribution of each individual spectrogram throughout the training process. To evaluate the performance, we conducted experiments over the benchmark dataset of SPRSound (The Open-Source SJTU Paediatric Respiratory Sound) proposed by the IEEE BioCAS 2022 challenge. As regards
    
[^103]: 多对称集合：通过反向对称性提高多样性和泛化能力

    Multi-Symmetry Ensembles: Improving Diversity and Generalization via Opposing Symmetries. (arXiv:2303.02484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02484](http://arxiv.org/abs/2303.02484)

    本研究提出了一种新的集成学习方法——多对称集合（MSE），它通过对称轴上假设的多样性来提高多样性和泛化能力，超越传统随机扰动的方法探索假设空间，并取得了良好效果。

    

    深度集合（DE）通过学习随机初始化的多样化成员成功地提高了模型性能。虽然最近的一些研究尝试通过超参数或正则化损失函数来促进进一步的多样性，但这些方法主要仍然依赖于随机方法来探索假设空间。在本文中，我们提出了多对称集合（MSE），通过捕捉对称轴上假设的多样性，构建了一个构建多样性集合的框架，它可以超越模型权重和超参数的随机扰动探索假设空间。我们利用对比表示学习的最新进展，创建了分别捕捉不变和等变函数类的对立假设的模型，并提出了一种简单的集成方法，以高效地组合适当的假设来完成给定的任务。我们证明，MSE有效地捕捉了相互矛盾的假设的多样性。

    Deep ensembles (DE) have been successful in improving model performance by learning diverse members via the stochasticity of random initialization. While recent works have attempted to promote further diversity in DE via hyperparameters or regularizing loss functions, these methods primarily still rely on a stochastic approach to explore the hypothesis space. In this work, we present Multi-Symmetry Ensembles (MSE), a framework for constructing diverse ensembles by capturing the multiplicity of hypotheses along symmetry axes, which explore the hypothesis space beyond stochastic perturbations of model weights and hyperparameters. We leverage recent advances in contrastive representation learning to create models that separately capture opposing hypotheses of invariant and equivariant functional classes and present a simple ensembling approach to efficiently combine appropriate hypotheses for a given task. We show that MSE effectively captures the multiplicity of conflicting hypotheses th
    
[^104]: 你只转移你分享的内容：基于交集的图传递学习在链接预测中的应用

    You Only Transfer What You Share: Intersection-Induced Graph Transfer Learning for Link Prediction. (arXiv:2302.14189v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14189](http://arxiv.org/abs/2302.14189)

    本文提出了一种称为图交集-诱导图传递学习的方法，旨在解决稀疏图在链接预测中的问题。在此方法中，通过创建一个交集子图，在源图上训练模型并将知识传递到目标图上进行预测。

    

    链接预测是许多实际应用的核心，但当目标图非常稀疏时，预测的性能可能会受到影响。为了解决稀疏图造成的问题，我们研究了一个之前被忽视的现象：在许多情况下，原始图形可能会有一个密集相连、互补的图形。这个密集的图形可能会与原始图形共享节点，从而提供了一个将有意义的选择性知识转移的自然桥梁。我们将这种情况称为基于交集的图传递学习(GITL)，它受到了电子商务或学术合著预测等实际应用的启发。我们开发了一个框架来有效地利用这种情况下的结构性先验知识。我们首先使用两个图之间共享的节点创建一个交集子图，然后将来自源丰富的交集子图的知识传递到完整的目标图上。在第二步中，我们考虑了两种方法：一种是改进的标签传播方法，另一种是多层次的传输学习方法。

    Link prediction is central to many real-world applications, but its performance may be hampered when the graph of interest is sparse. To alleviate issues caused by sparsity, we investigate a previously overlooked phenomenon: in many cases, a densely connected, complementary graph can be found for the original graph. The denser graph may share nodes with the original graph, which offers a natural bridge for transferring selective, meaningful knowledge. We identify this setting as Graph Intersection-induced Transfer Learning (GITL), which is motivated by practical applications in e-commerce or academic co-authorship predictions. We develop a framework to effectively leverage the structural prior in this setting. We first create an intersection subgraph using the shared nodes between the two graphs, then transfer knowledge from the source-enriched intersection subgraph to the full target graph. In the second step, we consider two approaches: a modified label propagation, and a multi-layer
    
[^105]: 随机教师是好的教师

    Random Teachers are Good Teachers. (arXiv:2302.12091v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12091](http://arxiv.org/abs/2302.12091)

    在自身蒸馏中，使用随机初始化的教师可以使用，可以提高模型并产生有趣的特性，但要注意表征是数据相关的。

    

    本文研究了自身蒸馏中由师生学习动态引发的隐式规则化。为了隔离其影响，作者进行了一个简单的实验，使用随机初始化的教师代替了训练有素的教师。惊奇地发现，当将学生蒸馏到这样一位随机教师时，生成的模型及其表征已经拥有非常有趣的特点；(1)在探究准确性方面，与其教师相比，我们观察到蒸馏出的学生明显提高。(2)所学习的表征是数据相关的，在不同的任务之间是可传递的，但如果在随机输入上进行训练，会严重恶化。(3)学生检查点含有稀疏子网络，称为抽奖票，位于受监督的损失景观的线性盆地边缘。这些观察结果对机器学习中的几个重要领域有着有趣的影响：(1)自身蒸馏可以仅基于随机初始化的教师工作。

    In this work, we investigate the implicit regularization induced by teacher-student learning dynamics in self-distillation. To isolate its effect, we describe a simple experiment where we consider teachers at random initialization instead of trained teachers. Surprisingly, when distilling a student into such a random teacher, we observe that the resulting model and its representations already possess very interesting characteristics; (1) we observe a strong improvement of the distilled student over its teacher in terms of probing accuracy. (2) The learned representations are data-dependent and transferable between different tasks but deteriorate strongly if trained on random inputs. (3) The student checkpoint contains sparse subnetworks, so-called lottery tickets, and lies on the border of linear basins in the supervised loss landscape. These observations have interesting consequences for several important areas in machine learning: (1) Self-distillation can work solely based on the im
    
[^106]: 基于扩散概率模型的结构化节点分类研究

    Diffusion Probabilistic Models for Structured Node Classification. (arXiv:2302.10506v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10506](http://arxiv.org/abs/2302.10506)

    本文提出了一种新颖的DPM-SNC框架，通过反向扩散过程和流形约束的采样方法实现基于图结构的结构化节点分类，并设计了一种新的训练算法来应用DPMs，最大化一个新的变分下界。实验证明DPMs可以提高GNN的表达能力，提高节点分类的效果。

    

    本文研究了基于图结构的结构化节点分类问题，其中节点标签之间存在依赖关系。特别地，我们集中研究了部分标记图的分类问题，需要将已知标记的信息纳入未知标签的预测中。为解决这个问题，我们提出了一种新颖的框架，利用扩散概率模型进行结构化节点分类(DPM-SNC)。我们的框架的核心是DPM-SNC的出色能力，即(a)使用具有表达性的反向扩散过程学习标签的联合分布，(b)利用流形约束的采样方法在已知标签的条件下进行预测。由于DPMs缺乏部分标记数据的训练算法，因此我们设计了一种新的训练算法，应用DPMs，最大化一个新的变分下界。我们还从理论上分析了如何通过增强GNN的表达能力来提高DPMs对节点分类的效果。

    This paper studies structured node classification on graphs, where the predictions should consider dependencies between the node labels. In particular, we focus on solving the problem for partially labeled graphs where it is essential to incorporate the information in the known label for predicting the unknown labels. To address this issue, we propose a novel framework leveraging the diffusion probabilistic model for structured node classification (DPM-SNC). At the heart of our framework is the extraordinary capability of DPM-SNC to (a) learn a joint distribution over the labels with an expressive reverse diffusion process and (b) make predictions conditioned on the known labels utilizing manifold-constrained sampling. Since the DPMs lack training algorithms for partially labeled data, we design a novel training algorithm to apply DPMs, maximizing a new variational lower bound. We also theoretically analyze how DPMs benefit node classification by enhancing the expressive power of GNNs 
    
[^107]: 高斯过程在赫赫尔姆霍兹分解中的应用：一种更流体的海洋气流模型

    Gaussian processes at the Helm(holtz): A more fluid model for ocean currents. (arXiv:2302.10364v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.10364](http://arxiv.org/abs/2302.10364)

    该论文提出了一种更符合已知电流物理特性的模型，在通过Helmholtz分解获得的向量场的发散和无旋分量上使用高斯过程来预测海流。证明了这种方法在模拟数据和真实浮标数据方面都比之前的方法更有效。

    

    海洋学家有兴趣预测海流和基于浮标速度的稀疏观测数据来识别当前矢量场中的发散性。高斯过程(GPs)在空间位置上充当连续但高度非线性功能的速度提供了一种吸引人的模型。但我们表明，将具有标准平稳核的GP直接应用于浮标数据可能在当前预测和发散性识别方面遇到困难—由于一些物理上不切实际的先验假设。为了更好地反映已知的电流物理特性，我们建议将标准平稳核放在通过Helmholtz分解获得的向量场的发散和无旋分量上。我们表明，由于该分解仅通过混合偏导数与原始向量场相关，因此我们仍然可以在原始数据给定的情况下进行推理，并且只需要额外进行少数计算。我们通过模拟数据和真实浮标数据证明了这种螺旋GP的有效性，从而在均方预测误差方面优于先前的方法。

    Oceanographers are interested in predicting ocean currents and identifying divergences in a current vector field based on sparse observations of buoy velocities. Since we expect current velocity to be a continuous but highly non-linear function of spatial location, Gaussian processes (GPs) offer an attractive model. But we show that applying a GP with a standard stationary kernel directly to buoy data can struggle at both current prediction and divergence identification -- due to some physically unrealistic prior assumptions. To better reflect known physical properties of currents, we propose to instead put a standard stationary kernel on the divergence and curl-free components of a vector field obtained through a Helmholtz decomposition. We show that, because this decomposition relates to the original vector field just via mixed partial derivatives, we can still perform inference given the original data with only a small constant multiple of additional computational expense. We illust
    
[^108]: 将黑匣子分解为可解释模型的混合物：路线规划，解释，重复。

    Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10289](http://arxiv.org/abs/2302.10289)

    本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。

    

    机器学习模型设计要么从解释性模型开始，要么从黑盒开始并事后解释。黑盒模型灵活但难以解释，而解释性模型本质上是可解释的。然而，解释性模型需要广泛的机器学习知识，并且往往比它们的黑盒变体不够灵活和表现不佳。本文旨在模糊黑盒的事后解释和构建可解释模型之间的界限。我们从黑盒开始，迭代地Carve出一种混合解释模型（MoIE）和一个残余网络。每个可解释模型专门处理一个样本子集，并使用一阶逻辑(FOL)对其进行解释，从黑盒中提供基本推理概念。我们通过灵活的残差路由其余的样本。我们在残转网络上重复该方法，直到所有可解释模型解释所需比例的数据。我们进行了大量实验，结果表明我们的路线规划，解释和重复方法在各种数据集上优于目前几种黑匣子模型解释方法，并产生高度可解释的模型。

    ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
    
[^109]: 自监督学习的分割不变等变表示

    Self-supervised learning of Split Invariant Equivariant representations. (arXiv:2302.10283v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10283](http://arxiv.org/abs/2302.10283)

    本文介绍了一个数据集3DIEBench，提出了一种基于超网络的等变表达式预测器SIE，结合分裂的不变与等变表达式以获得更加丰富的表示，同时证明了显著的性能提高。

    

    最近，自监督学习在学习不变或等变表示方面取得了进展。尽管不变方法在大规模数据集上进行评估，但等变方法在更小、更可控的环境中进行评估。我们旨在弥合两者之间的差距，以便学习更加多样化、适用于广泛任务的表示方法。我们首先介绍一个称为3DIEBench的数据集，包括来自55个类别的3D模型渲染图像超过250万张，我们可以完全控制应用于对象的变换。我们还引入了一种基于超网络的预测器体系结构，以学习等变表示，从而不可能出现不变性崩塌的情况。我们介绍了SIE（分裂不变-等变）的概念，它将基于超网络的预测器与分裂为两部分（一部分为不变形，另一部分为等变形）的表示相结合，以学习更丰富的表示。我们证明了显著的性能提高。

    Recent progress has been made towards learning invariant or equivariant representations with self-supervised learning. While invariant methods are evaluated on large scale datasets, equivariant ones are evaluated in smaller, more controlled, settings. We aim at bridging the gap between the two in order to learn more diverse representations that are suitable for a wide range of tasks. We start by introducing a dataset called 3DIEBench, consisting of renderings from 3D models over 55 classes and more than 2.5 million images where we have full control on the transformations applied to the objects. We further introduce a predictor architecture based on hypernetworks to learn equivariant representations with no possible collapse to invariance. We introduce SIE (Split Invariant-Equivariant) which combines the hypernetwork-based predictor with representations split in two parts, one invariant, the other equivariant, to learn richer representations. We demonstrate significant performance gains
    
[^110]: JANA：复杂贝叶斯模型的联合分摊近似神经网络

    JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models. (arXiv:2302.09125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09125](http://arxiv.org/abs/2302.09125)

    本文提出了 JANA 方法，用于处理复杂贝叶斯模型的近似计算。通过端到端训练三个神经网络来实现分摊的近似后验和似然，为贝叶斯工作流程提供了一种新的途径。此方法在多种模拟模型中进行了基准测试，并提出了一种联合校准诊断方法。

    

    本文提出了“联合分摊神经网络近似”（JANA）方法，用于处理贝叶斯代理建模和基于模拟的推理中出现的难以计算的似然函数和后验密度。我们以端到端的方式训练三个相互补充的神经网络：1）一个总结网络，将个别数据点、集合或时间序列压缩成信息嵌入向量；2）一个后验网络，学习分摊的近似后验；3）一个似然网络，学习分摊的近似似然。它们的交互为分摊边缘似然和后验预测估计提供了新的途径，这是贝叶斯工作流程的两个重要组成部分，常常对于标准方法来说太昂贵了。我们在各种模拟模型中对JANA的保真度进行了基准测试，与最先进的贝叶斯方法进行了比较，并提出了一种强大而可解释的联合校准诊断方法。此外，我们研究了循环似然网络模拟复杂模型的能力。

    This work proposes ''jointly amortized neural approximation'' (JANA) of intractable likelihood functions and posterior densities arising in Bayesian surrogate modeling and simulation-based inference. We train three complementary networks in an end-to-end fashion: 1) a summary network to compress individual data points, sets, or time series into informative embedding vectors; 2) a posterior network to learn an amortized approximate posterior; and 3) a likelihood network to learn an amortized approximate likelihood. Their interaction opens a new route to amortized marginal likelihood and posterior predictive estimation -- two important ingredients of Bayesian workflows that are often too expensive for standard methods. We benchmark the fidelity of JANA on a variety of simulation models against state-of-the-art Bayesian methods and propose a powerful and interpretable diagnostic for joint calibration. In addition, we investigate the ability of recurrent likelihood networks to emulate comp
    
[^111]: 反馈图上的实用情境赌博算法

    Practical Contextual Bandits with Feedback Graphs. (arXiv:2302.08631v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08631](http://arxiv.org/abs/2302.08631)

    本文提出了一种利用反馈图的情境赌博算法，可用于缓解学习的统计复杂性，在实际应用中具有计算上的可行性和优秀的表现。

    

    尽管情境赌博已经有成熟的理论，但如何有效地利用不同的反馈模式来加速学习仍不清楚。在反馈图上的赌博问题提供了一个有前途的框架来缓解学习的统计复杂性。本文基于回归的思想，提出和分析了一种情境赌博算法，这些算法在现实应用中具有计算上的可行性，并实现了已知的极小极值，因此减少了学习的统计复杂性。

    While contextual bandit has a mature theory, effectively leveraging different feedback patterns to enhance the pace of learning remains unclear. Bandits with feedback graphs, which interpolates between the full information and bandit regimes, provides a promising framework to mitigate the statistical complexity of learning. In this paper, we propose and analyze an approach to contextual bandits with feedback graphs based upon reduction to regression. The resulting algorithms are computationally practical and achieve established minimax rates, thereby reducing the statistical complexity in real-world applications.
    
[^112]: 数据集接口：使用可控对抗生成来诊断模型失败

    Dataset Interfaces: Diagnosing Model Failures Using Controllable Counterfactual Generation. (arXiv:2302.07865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07865](http://arxiv.org/abs/2302.07865)

    本文提出了一种数据集接口的概念，旨在通过给定一个输入数据集和一个用户指定的分布偏移来返回针对该分布的实例。我们提出了一种利用文本反转的数据集接口实现，使得生成更加适应输入分布，进而帮助研究模型在各种分布偏移下的行为。

    

    数据分布偏移是机器学习模型失败的主要原因。然而，评估模型在分布偏移下的可靠性可能很具有挑战性，特别是因为可能很难获取表现出指定偏移的反事实示例。在本文中，我们引入了数据集接口的概念：一个框架，给定输入数据集和用户指定的偏移，返回来自该输入分布的具有所需偏移的实例。我们研究了许多自然的实现方式，发现它们经常引入混淆偏移来使模型评估复杂化。受此启发，我们提出了一个数据集接口实现，利用文本反转来自定义生成器以适应输入分布。然后，我们演示了如何将这个数据集接口应用于ImageNet数据集，以研究模型在多样的分布偏移下的行为，包括背景、光照和属性的变化。

    Distribution shift is a major source of failure for machine learning models. However, evaluating model reliability under distribution shift can be challenging, especially since it may be difficult to acquire counterfactual examples that exhibit a specified shift. In this work, we introduce the notion of a dataset interface: a framework that, given an input dataset and a user-specified shift, returns instances from that input distribution that exhibit the desired shift. We study a number of natural implementations for such an interface, and find that they often introduce confounding shifts that complicate model evaluation. Motivated by this, we propose a dataset interface implementation that leverages Textual Inversion to tailor generation to the input distribution. We then demonstrate how applying this dataset interface to the ImageNet dataset enables studying model behavior across a diverse array of distribution shifts, including variations in background, lighting, and attributes of t
    
[^113]: 向量量化Wasserstein自编码器

    Vector Quantized Wasserstein Auto-Encoder. (arXiv:2302.05917v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05917](http://arxiv.org/abs/2302.05917)

    本文提出了一种向量量化Wasserstein自编码器，可通过最小化WS距离将离散潜在表示传递到数据分布，实现更好的、可控的聚类解决方案。

    

    学习深度离散潜在表示提供了更好的符号和总结的抽象，更有用于后续的下游任务。受到 Vector Quantized Variational Auto-Encoder (VQ-VAE) 的启发，学习深度离散表示的工作主要集中在改进原始的VQ-VAE形式，没有一个讨论从生成的角度学习深度离散表示。在这项工作中，我们从生成的角度研究了学习深度离散表示。具体而言，我们赋予离散分布在码字序列上，并学习一个确定性解码器，通过最小化它们之间的WS距离将码字序列的分布传递到数据分布。我们进一步发展了理论，将其与WS距离的聚类视图连接起来，这使我们可以有更好的、可控的聚类解决方案。最后，我们在几个常见测试集上进行了实证评估。

    Learning deep discrete latent presentations offers a promise of better symbolic and summarized abstractions that are more useful to subsequent downstream tasks. Inspired by the seminal Vector Quantized Variational Auto-Encoder (VQ-VAE), most of work in learning deep discrete representations has mainly focused on improving the original VQ-VAE form and none of them has studied learning deep discrete representations from the generative viewpoint. In this work, we study learning deep discrete representations from the generative viewpoint. Specifically, we endow discrete distributions over sequences of codewords and learn a deterministic decoder that transports the distribution over the sequences of codewords to the data distribution via minimizing a WS distance between them. We develop further theories to connect it with the clustering viewpoint of WS distance, allowing us to have a better and more controllable clustering solution. Finally, we empirically evaluate our method on several wel
    
[^114]: 用于上下文学习的组合范例

    Compositional Exemplars for In-context Learning. (arXiv:2302.05698v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.05698](http://arxiv.org/abs/2302.05698)

    该论文提出了CEIL（Compositional Exemplars for In-context Learning）框架，利用决定性点过程（DPP）模型处理上下文示例选择问题，从而提高了大型预训练语言模型（LMs）进行上下文学习的性能。

    

    大型预训练语言模型已经表现出令人印象深刻的上下文学习能力，其中模型通过输入输出示例作为演示，在不进行任何参数更新的情况下学习执行看不见的任务。上下文学习的性能高度受到所选上下文示例的质量所支配。然而，以前的选择方法基本上是基于简单的启发式，导致性能次优。在这项工作中，我们将上下文示例选择形式化为子集选择问题。我们提出CEIL（Compositional Exemplars for In-context Learning），它通过决定性点过程（DPP）对所给输入和上下文示例之间的交互进行建模，并通过精心设计的对比学习目标进行优化，从而获得来自LM的偏好。我们在来自7个不同自然语言处理任务的12个分类和生成数据集上验证了CEIL，包括情感分析、释义检测、自然语言生成等任务。

    Large pretrained language models (LMs) have shown impressive In-Context Learning (ICL) ability, where the model learns to do an unseen task via a prompt consisting of input-output examples as the demonstration, without any parameter updates. The performance of ICL is highly dominated by the quality of the selected in-context examples. However, previous selection methods are mostly based on simple heuristics, leading to sub-optimal performance. In this work, we formulate in-context example selection as a subset selection problem. We propose CEIL (Compositional Exemplars for In-context Learning), which is instantiated by Determinantal Point Processes (DPPs) to model the interaction between the given input and in-context examples, and optimized through a carefully-designed contrastive learning objective to obtain preference from LMs. We validate CEIL on 12 classification and generation datasets from 7 distinct NLP tasks, including sentiment analysis, paraphrase detection, natural language
    
[^115]: 超网络构建音频的隐式神经表示

    Hypernetworks build Implicit Neural Representations of Sounds. (arXiv:2302.04959v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04959](http://arxiv.org/abs/2302.04959)

    该论文介绍了一种新的方法，名为“HyperSound”，可将超网络结构应用于元学习，从而生成音频隐式神经表示（INR），该方法可用于音频信号的处理，并且重构质量可与其他最先进的模型相媲美，是当代音频处理中的一个有潜力的替代方案。

    

    隐式神经表示（INR）现在被广泛地应用于各种实际应用程序中来代表多媒体信号，包括图像超分辨率、图像压缩或3D渲染。现有的利用INR的方法主要集中在视觉数据上，因为在基于图像的INR模型的架构属性中存在归纳偏差，所以将其应用于其他模态，如音频，是非常困难的。为了解决这个问题，我们介绍了超声（HyperSound），这是一种利用超网络进行元学习的方法，用于为音频样本生成INR，以便能够在训练中观察到的样本上进行推广。我们的方法以可比较其他最先进模型的质量重构音频样本，并为用于音频处理的深度神经网络中的当代声音表示提供了可行的替代方案，如谱图。

    Implicit Neural Representations (INRs) are nowadays used to represent multimedia signals across various real-life applications, including image super-resolution, image compression, or 3D rendering. Existing methods that leverage INRs are predominantly focused on visual data, as their application to other modalities, such as audio, is nontrivial due to the inductive biases present in architectural attributes of image-based INR models. To address this limitation, we introduce HyperSound, the first meta-learning approach to produce INRs for audio samples that leverages hypernetworks to generalize beyond samples observed in training. Our approach reconstructs audio samples with quality comparable to other state-of-the-art models and provides a viable alternative to contemporary sound representations used in deep neural networks for audio processing, such as spectrograms.
    
[^116]: 图神经网络中的泛化：基于图扩散的改进PAC-Bayesian界限。

    Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion. (arXiv:2302.04451v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04451](http://arxiv.org/abs/2302.04451)

    本文提出了用特征扩散矩阵的最大奇异值来缩放泛化界限的方法，并用Hessians来衡量图神经网络对噪声扰动的稳定性。实验证明，这些方法可以有效减小泛化界限，更好地解决了实际图形问题。

    

    图神经网络是图预测任务中广泛使用的工具。由其实证表现所驱动，先前的研究开发了图神经网络的泛化界限，它们根据最大度数在图结构方面进行缩放。在本文中，我们提出了泛化界限，这些界限根据图神经网络特征扩散矩阵的最大奇异值进行缩放。对于实际图形，这些界限的数值要比先前的界限小得多。我们还构建了一个相符的泛化差距下限，其渐近地匹配了我们的上限界限。为了实现这些结果，我们分析了一个统一的模型，其中包括先前的设置（即卷积和消息传递网络）和新的设置（即图同构网络）。我们的关键思想是利用Hessians来衡量图神经网络对于噪声扰动的稳定性。实验证明，基于Hessian的测量与观察到的泛化差距相关。

    Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network's feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works' settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with the observed generalization gaps
    
[^117]: 机器学习用于合成数据生成的综述

    Machine Learning for Synthetic Data Generation: A Review. (arXiv:2302.04062v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04062](http://arxiv.org/abs/2302.04062)

    机器学习用于合成数据生成的综述，探讨了合成数据生成的应用（计算机视觉、语音、自然语言、医疗保健和商业）、机器学习方法（神经网络架构和深度生成模型）以及隐私和公平问题，并提出了未来的研究方向。

    

    数据在机器学习中发挥着关键作用。然而，在实际应用中，数据存在多种问题，如数据质量低，有限的数据点导致机器学习模型欠拟合，由于隐私、安全和监管问题难以访问数据。合成数据生成提供了一种有前途的新途径，因为它可以以真实世界数据无法做到的方式进行共享和使用。本文系统地回顾了利用机器学习模型进行合成数据生成的现有工作。具体而言，我们从以下几个方面讨论合成数据生成的工作：（i）应用，包括计算机视觉、语音、自然语言、医疗保健和商业；（ii）机器学习方法，特别是神经网络架构和深度生成模型；（iii）隐私和公平问题。此外，我们还确定了这一新兴领域的挑战和机遇，并提出了未来的研究方向。

    Data plays a crucial role in machine learning. However, in real-world applications, there are several problems with data, e.g., data are of low quality; a limited number of data points lead to under-fitting of the machine learning model; it is hard to access the data due to privacy, safety and regulatory concerns. Synthetic data generation offers a promising new avenue, as it can be shared and used in ways that real-world data cannot. This paper systematically reviews the existing works that leverage machine learning models for synthetic data generation. Specifically, we discuss the synthetic data generation works from several perspectives: (i) applications, including computer vision, speech, natural language, healthcare, and business; (ii) machine learning methods, particularly neural network architectures and deep generative models; (iii) privacy and fairness issue. In addition, we identify the challenges and opportunities in this emerging field and suggest future research directions
    
[^118]: 针对特征与标签偏移的时间序列领域自适应

    Domain Adaptation for Time Series Under Feature and Label Shifts. (arXiv:2302.03133v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03133](http://arxiv.org/abs/2302.03133)

    本文提出了Raincoat，这是第一个针对复杂时间序列的封闭集和通用领域自适应模型。Raincoat通过跨域对齐时间和频率特征，修正偏移以便于检测私有标签，并通过识别共享结构来提高可传递性。

    

    无监督领域自适应（UDA）可以将源域训练的模型转移到未标记的目标域。然而，转移复杂的时间序列模型面临着挑战，因为不同域中存在动态的时间结构变化，导致时间和频率表示中的特征偏移。此外，源域和目标域中的任务标签分布可能存在显著差异，难以解决标签偏移和识别唯一于目标域的标签。有效地转移复杂的时间序列模型仍然是一个十分困难的问题。本文提出了Raincoat，这是第一个针对复杂时间序列的封闭集和通用领域自适应模型。Raincoat通过考虑时间和频率特征，跨域对齐它们，修正不匹配以便于检测私有标签来解决特征和标签偏移问题。此外，Raincoat通过识别源域和目标域之间的共享结构来提高可传递性。

    Unsupervised domain adaptation (UDA) enables the transfer of models trained on source domains to unlabeled target domains. However, transferring complex time series models presents challenges due to the dynamic temporal structure variations across domains. This leads to feature shifts in the time and frequency representations. Additionally, the label distributions of tasks in the source and target domains can differ significantly, posing difficulties in addressing label shifts and recognizing labels unique to the target domain. Effectively transferring complex time series models remains a formidable problem. We present Raincoat, the first model for both closed-set and universal domain adaptation on complex time series. Raincoat addresses feature and label shifts by considering both temporal and frequency features, aligning them across domains, and correcting for misalignments to facilitate the detection of private labels. Additionally, Raincoat improves transferability by identifying l
    
[^119]: 向大核模型迈进

    Toward Large Kernel Models. (arXiv:2302.02605v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02605](http://arxiv.org/abs/2302.02605)

    本文提出了一种构建大规模通用核模型的方法，这解决了传统核机器中模型大小与数据大小相互耦合的问题，使其能够在大数据集上进行训练。

    

    最近的研究表明，与深度神经网络（DNN）相比，核机器在小数据集上的表现通常可以达到或超过DNN。核机器的兴趣受到其在某些情况下等效于宽神经网络的发现的推动。然而，DNN的一个关键特征是它们能够独立地扩展模型大小和训练数据量，而在传统的核机器中，模型大小与数据大小是相互耦合的。由于这种耦合，将核机器扩展到大数据是计算上具有挑战性的。在本文中，我们提供了一种构建大规模通用核模型的方法，这是核机器的一般化，通过解耦模型和数据，允许在大数据集上进行训练。具体地，我们引入了基于投影双重预处理SGD的EigenPro 3.0算法，并展示了使用现有核方法不可能实现的模型和数据规模的扩展。

    Recent studies indicate that kernel machines can often perform similarly or better than deep neural networks (DNNs) on small datasets. The interest in kernel machines has been additionally bolstered by the discovery of their equivalence to wide neural networks in certain regimes. However, a key feature of DNNs is their ability to scale the model size and training data size independently, whereas in traditional kernel machines model size is tied to data size. Because of this coupling, scaling kernel machines to large data has been computationally challenging. In this paper, we provide a way forward for constructing large-scale general kernel models, which are a generalization of kernel machines that decouples the model and data, allowing training on large datasets. Specifically, we introduce EigenPro 3.0, an algorithm based on projected dual preconditioned SGD and show scaling to model and data sizes which have not been possible with existing kernel methods.
    
[^120]: 基于局部信用和不完整轨迹的GFlowNets更好的训练方法

    Better Training of GFlowNets with Local Credit and Incomplete Trajectories. (arXiv:2302.01687v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01687](http://arxiv.org/abs/2302.01687)

    本文提出了一种基于局部信用和不完整轨迹的GFlowNets更好的训练方法，通过为轨迹的每个步骤分配部分奖励，基于局部奖励估计器，实现更有效利用数据，提高了GFlowNets性能。

    

    生成流网络或GFlowNets与Markov链蒙特卡罗方法相关(因为他们从由能量函数确定的分布中取样),与强化学习(因为他们学习通过一系列步骤取样组合的对象的策略),生成模型(因为他们学习表示和从分布中取样),以及分摊变分方法(因为它们可以用来学习近似并从一个否则难以处理的后验分布中取样,给定先验分布和似然函数)。它们被训练来生成一个对象$x$，通过一系列步骤，其概率与一些奖励函数$R(x)$(或$\exp(-\mathcal{E}(x))$，其中$\mathcal{E}(x)$表示能量函数)成正比，给出了生成轨迹的末端。与其他RL设置一样，在奖励仅在结束时给出时，当这些轨迹较长时，训练效率和信用分配可能受到影响。在以前的GFlowNet工作中，不能从不完整的过渡中进行学习。在本文中，作者提出了一种新的信用分配策略，允许使用不完整的轨迹更好地训练GFlowNets。该方法涉及到为轨迹的每个步骤分配部分奖励，基于局部奖励估计器。这导致了对可用数据的更有效利用和GFlowNets性能的改善。在基准数据集上的实验结果证明了所提出方法的有效性。

    Generative Flow Networks or GFlowNets are related to Monte-Carlo Markov chain methods (as they sample from a distribution specified by an energy function), reinforcement learning (as they learn a policy to sample composed objects through a sequence of steps), generative models (as they learn to represent and sample from a distribution) and amortized variational methods (as they can be used to learn to approximate and sample from an otherwise intractable posterior, given a prior and a likelihood). They are trained to generate an object $x$ through a sequence of steps with probability proportional to some reward function $R(x)$ (or $\exp(-\mathcal{E}(x))$ with $\mathcal{E}(x)$ denoting the energy function), given at the end of the generative trajectory. Like for other RL settings where the reward is only given at the end, the efficiency of training and credit assignment may suffer when those trajectories are longer. With previous GFlowNet work, no learning was possible from incomplete tr
    
[^121]: Mnemosyne: 使用Transformers来训练Transformers

    Mnemosyne: Learning to Train Transformers with Transformers. (arXiv:2302.01128v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01128](http://arxiv.org/abs/2302.01128)

    Mnemosyne优化器使用Performers方法来学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节，并成功训练ViTs和应用于机器人领域中，具有更好的泛化能力与快速收敛。

    

    训练复杂的机器学习(ML)架构需要耗费大量计算和时间来选择合适的优化器并调节其超参数。从数据中学习优化器的新学习范式已经成为手动设计ML优化器的更好选择。我们提出了Mnemosyne优化器，它使用Performers: 隐式低秩attention Transformers。它可以学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节。我们展示了Mnemosyne：(a)比流行的LSTM优化器具有更好的泛化能力；(b)特别地，可以在标准MLPs上进行元训练后成功地训练Vision Transformers(ViTs) (c)可以初始化优化器以实现机器人应用中更快的收敛。我们相信这些结果开启了使用Transformers构建基础优化模型的可能性，可以应对常规的Transformer训练挑战。我们通过广泛的理论分析来补充我们的结果。

    Training complex machine learning (ML) architectures requires a compute and time consuming process of selecting the right optimizer and tuning its hyper-parameters. A new paradigm of learning optimizers from data has emerged as a better alternative to hand-designed ML optimizers. We propose Mnemosyne optimizer, that uses Performers: implicit low-rank attention Transformers. It can learn to train entire neural network architectures including other Transformers without any task-specific optimizer tuning. We show that Mnemosyne: (a) generalizes better than popular LSTM optimizer, (b) in particular can successfully train Vision Transformers (ViTs) while meta--trained on standard MLPs and (c) can initialize optimizers for faster convergence in Robotics applications. We believe that these results open the possibility of using Transformers to build foundational optimization models that can address the challenges of regular Transformer training. We complement our results with an extensive theo
    
[^122]: 对称生成对抗性发现

    Generative Adversarial Symmetry Discovery. (arXiv:2302.00236v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00236](http://arxiv.org/abs/2302.00236)

    使用LieGAN框架，可以自动发现数据集中的等变性，从而提高预测的准确性和泛化能力。

    

    尽管等变神经网络在科学应用中取得了成功，但它们需要事先知道对称群。然而在实践中，知道要使用哪个对称群作为归纳偏差可能很困难，错误地强制使用对称群甚至可能会损害性能。本文提出了一个LieGAN框架，通过类似生成对抗性训练的范式自动从数据集中发现等变性。生成器学习一组应用于数据的变换，这些变换保持原始分布并欺骗鉴别器。LieGAN使用可解释的李代数基表示对称性，并且可以在轨迹预测和顶夸克标记任务中发现各种对称性，例如旋转群$\mathrm{SO}(n)$，限制Lorentz群$\mathrm{SO}(1,3)^+$。所学习的对称性也可以轻松地应用于几个现有的等变神经网络，以提高预测的准确性和泛化能力。

    Despite the success of equivariant neural networks in scientific applications, they require knowing the symmetry group a priori. However, it may be difficult to know which symmetry to use as an inductive bias in practice. Enforcing the wrong symmetry could even hurt the performance. In this paper, we propose a framework, LieGAN, to automatically discover equivariances from a dataset using a paradigm akin to generative adversarial training. Specifically, a generator learns a group of transformations applied to the data, which preserve the original distribution and fool the discriminator. LieGAN represents symmetry as interpretable Lie algebra basis and can discover various symmetries such as the rotation group $\mathrm{SO}(n)$, restricted Lorentz group $\mathrm{SO}(1,3)^+$ in trajectory prediction and top-quark tagging tasks. The learned symmetry can also be readily used in several existing equivariant neural networks to improve accuracy and generalization in prediction.
    
[^123]: 用于湍流时空潜在描述的卷积自编码器

    Convolutional autoencoder for the spatiotemporal latent representation of turbulence. (arXiv:2301.13728v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2301.13728](http://arxiv.org/abs/2301.13728)

    采用卷积自编码器获得高效准确的湍流流动潜在描述。

    

    湍流的混沌动力学和高维状态空间使得其难以预测。然而，湍流流动常常被一些连续的时空结构所描述，如漩涡或大尺度模式，这可以帮助获得湍流流动的潜在描述。然而，目前的方法通常受到阈值化和传统模态流分解方法的线性性的限制。本文旨在获得一种对于具有极端事件的湍流流动而言既高效又准确的降维潜在描述。具体来说，我们采用三维多尺度卷积自编码器(CAE)来获得湍流流动的潜在描述。

    Turbulence is characterised by chaotic dynamics and a high-dimensional state space, which make this phenomenon challenging to predict. However, turbulent flows are often characterised by coherent spatiotemporal structures, such as vortices or large-scale modes, which can help obtain a latent description of turbulent flows. However, current approaches are often limited by either the need to use some form of thresholding on quantities defining the isosurfaces to which the flow structures are associated or the linearity of traditional modal flow decomposition approaches, such as those based on proper orthogonal decomposition. This problem is exacerbated in flows that exhibit extreme events, which are rare and sudden changes in a turbulent state. The goal of this paper is to obtain an efficient and accurate reduced-order latent representation of a turbulent flow that exhibits extreme events. Specifically, we employ a three-dimensional multiscale convolutional autoencoder (CAE) to obtain su
    
[^124]: 全局图形运动预测

    Holistic Graph-based Motion Prediction. (arXiv:2301.13545v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2301.13545](http://arxiv.org/abs/2301.13545)

    该论文提出了一种基于全局图形的运动预测方法，使用综合性异构图形表示来结合交通参与者之间的时空信息、属性和关系以及道路网络等静态元素的关系，以提高运动预测的质量。

    

    自动驾驶车辆在复杂环境下的运动预测是一个困难的任务，在自动驾驶车辆在不同情况下被使用时需要掌握。许多因素影响着交通参与者的未来动向，从交通规则、相互交互到人类驾驶员的个人习惯。因此，我们提出了一种基于全局图形的预测方法，该方法基于一个综合性异构图形表示，结合了交通参与者之间的时空信息、属性和关系以及与道路网络等静态元素的关系。通过不同类型的节点和边缘来编码信息，两者都具备任意特性。我们在INTERACTION和Argoverse数据集上评估了该方法，并进行了信息消融研究，以证明各种信息对运动预测质量的益处。

    Motion prediction for automated vehicles in complex environments is a difficult task that is to be mastered when automated vehicles are to be used in arbitrary situations. Many factors influence the future motion of traffic participants starting with traffic rules and reaching from the interaction between each other to personal habits of human drivers. Therefore we present a novel approach for a graph-based prediction based on a heterogeneous holistic graph representation that combines temporal information, properties and relations between traffic participants as well as relations with static elements like the road network. The information are encoded through different types of nodes and edges that both are enriched with arbitrary features. We evaluated the approach on the INTERACTION and the Argoverse dataset and conducted an informative ablation study to demonstrate the benefit of different types of information for the motion prediction quality.
    
[^125]: MILO: 模型无关子集选择框架，用于高效模型训练和调优。

    MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning. (arXiv:2301.13287v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13287](http://arxiv.org/abs/2301.13287)

    提出了一个模型无关子集选择框架MILO，将子集选择与模型训练分离，通过易到难的课程实现了卓越的模型收敛和性能。

    

    训练深度网络和调优大型数据集的超参数是计算密集型的。减少训练成本的主要研究方向之一是通过选择很好的训练数据子集来实现。与简单的自适应随机子集选择基准相比，现有的智能子集选择方法由于耗时的子集选择步骤而不具竞争力，该步骤涉及计算依赖于模型的梯度和特征嵌入，并应用子模块目标的贪心最大化。我们的关键洞察是消除对下游模型参数的依赖，将子集选择作为预处理步骤，并使其能够在不增加成本的情况下训练多个模型。在这个工作中，我们提出了 MILO，一个模型无关的子集选择框架，它将子集选择与模型训练分离，同时通过使用一个易到难的课程实现了卓越的模型收敛和性能。通过实验结果验证了我们的方法。

    Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results in
    
[^126]: 序列推荐的互Wasserstein距离最小化算法

    Mutual Wasserstein Discrepancy Minimization for Sequential Recommendation. (arXiv:2301.12197v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12197](http://arxiv.org/abs/2301.12197)

    本文提出了一种基于互Wasserstein距离最小化的新型自监督学习框架用于提高推荐系统的性能，该方法使用Wasserstein距离测量来增强互信息最大化，优于现有的几种顺序推荐方法。

    

    自监督的顺序推荐通过最大化互信息和设计良好的数据增广显著提高了推荐系统的性能。然而, 目前的互信息估计基于计算Kullback Leibler差异，并且存在许多局限性，包括非对称估计、指数样本大小需求和训练不稳定。而使用的现有数据增广大多是随机的，可能会因为随机修改而破坏顺序相关性。因此，我们提出了一种基于互Wasserstein距离最小化的新型自监督学习框架来解决这些问题，提出了Wasserstein距离测量来评估增广序列之间的互信息，通过减少原始和增广序列的概率分布之间的距离，增强互信息最大化。在实际数据集上进行的实验结果表明，我们的方法优于现有的几种顺序推荐方法。

    Self-supervised sequential recommendation significantly improves recommendation performance by maximizing mutual information with well-designed data augmentations. However, the mutual information estimation is based on the calculation of Kullback Leibler divergence with several limitations, including asymmetrical estimation, the exponential need of the sample size, and training instability. Also, existing data augmentations are mostly stochastic and can potentially break sequential correlations with random modifications. These two issues motivate us to investigate an alternative robust mutual information measurement capable of modeling uncertainty and alleviating KL divergence limitations. To this end, we propose a novel self-supervised learning framework based on Mutual WasserStein discrepancy minimization MStein for the sequential recommendation. We propose the Wasserstein Discrepancy Measurement to measure the mutual information between augmented sequences. Wasserstein Discrepancy M
    
[^127]: 多维概念发现(MCD): 一个具有完整性保证的统一框架

    Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees. (arXiv:2301.11911v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11911](http://arxiv.org/abs/2301.11911)

    提出了多维概念发现(MCD)方法，它满足概念层面上的完整性关系，不需要加强概念可解释性或重新训练模型部分，并提供概念激活图分析工具

    

    完整性公理使得后续XAI方法的解释仅对模型在单个决策上有效。为了可信地应用XAI，特别是对于高风险的决策，需要更全球的模型理解。最近，已经提出了基于概念的方法，但这些方法不能保证与实际的模型推理相结合。为了解决这个问题，我们提出了多维概念发现(MCD)，作为之前方法的扩展，满足概念层面上的完整性关系。我们的方法从通用的线性子空间作为概念开始，并不需要加强概念可解释性或重新训练模型部分。我们提出了稀疏子空间聚类来发现改进的概念，充分利用了多维子空间的潜能。MCD提供了两种概念在输入空间中的互补分析工具：(1)概念激活图，显示概念表达的位置

    The completeness axiom renders the explanation of a post-hoc XAI method only locally faithful to the model, i.e. for a single decision. For the trustworthy application of XAI, in particular for high-stake decisions, a more global model understanding is required. Recently, concept-based methods have been proposed, which are however not guaranteed to be bound to the actual model reasoning. To circumvent this problem, we propose Multi-dimensional Concept Discovery (MCD) as an extension of previous approaches that fulfills a completeness relation on the level of concepts. Our method starts from general linear subspaces as concepts and does neither require reinforcing concept interpretability nor re-training of model parts. We propose sparse subspace clustering to discover improved concepts and fully leverage the potential of multi-dimensional subspaces. MCD offers two complementary analysis tools for concepts in input space: (1) concept activation maps, that show where a concept is express
    
[^128]: 输入扰动降低扩散模型的暴露偏差

    Input Perturbation Reduces Exposure Bias in Diffusion Models. (arXiv:2301.11706v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11706](http://arxiv.org/abs/2301.11706)

    本文提出了一种输入扰动方法来缓解扩散模型中的曝光偏差问题，该方法不影响模型性能，能显著提高生成样本的质量并减少训练和推断时间。

    

    去噪扩散概率模型显示出了令人印象深刻的生成质量，但是它们长的抽样链导致了高计算成本。本文观察到长时间的抽样链也会导致一种错误积累现象，类似于自回归文本生成中的曝光偏差问题。具体来说，我们注意到训练和测试之间存在差异，因为前者是基于真实样本进行条件训练，而后者是基于之前生成的结果进行条件的。为了缓解这个问题，我们提出了一种非常简单但有效的训练规则，即通过扰动真实样本来模拟推断时间的预测误差。我们经验证明，采用这种输入扰动方式，不会影响模型的召回率和精确率，却能显著提高样本质量，同时减少训练和推断时间。例如，在CelebA 64$\times$64上，我们实现了36.74的Fréchet Inception Distance，优于其他最先进的模型。

    Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64$\times$64, we ach
    
[^129]: 带有重复的类增量学习

    Class-Incremental Learning with Repetition. (arXiv:2301.11396v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11396](http://arxiv.org/abs/2301.11396)

    该论文提出了带有重复的类增量学习，并通过两个随机流生成器产生不同的CIR流进行评估和提出了一种新的重放策略。

    

    实际数据流中自然包含之前概念的重复。从持续学习（CL）的角度来看，重复是环境的属性，与重放不同，不能由代理程序控制。现在，类增量（CI）场景代表了评估和比较CL策略的主要测试平台。这种情况非常容易使用，但它从不允许重新访问之前看到的类，因此完全忽略了重复的作用。我们关注带有重复的类增量（CIR）场景家族，其中重复被嵌入到流的定义中。我们提出了两个随机流生成器，它们可以从单个数据集和几个可解释的控制参数开始产生各种CIR流。我们通过研究现有CL策略在不同CIR流下的行为，进行了对重复在CL中的第一次全面评估。然后我们提出了一种利用重复的新的重放策略。

    Real-world data streams naturally include the repetition of previous concepts. From a Continual Learning (CL) perspective, repetition is a property of the environment and, unlike replay, cannot be controlled by the agent. Nowadays, the Class-Incremental (CI) scenario represents the leading test-bed for assessing and comparing CL strategies. This scenario type is very easy to use, but it never allows revisiting previously seen classes, thus completely neglecting the role of repetition. We focus on the family of Class-Incremental with Repetition (CIR) scenario, where repetition is embedded in the definition of the stream. We propose two stochastic stream generators that produce a wide range of CIR streams starting from a single dataset and a few interpretable control parameters. We conduct the first comprehensive evaluation of repetition in CL by studying the behavior of existing CL strategies under different CIR streams. We then present a novel replay strategy that exploits repetition a
    
[^130]: 针对不规则采样时间序列的神经连续离散状态空间模型

    Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series. (arXiv:2301.11308v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11308](http://arxiv.org/abs/2301.11308)

    本研究提出了一个用于不规则采样时间序列的神经连续离散状态空间模型，其采用辅助变量来区分识别和动态，从而实现了准确的贝叶斯推理和改进的性能。

    

    学习真实世界动态现象（如气候、生物学等）的准确预测模型仍然是一个具有挑战性的任务。一项关键问题是，自然和人工过程生成的数据往往包含不规则采样和/或缺失的时间序列。本研究提出神经连续离散状态空间模型（NCDSSM），用于通过离散时间观测对时间序列进行连续时间建模。NCDSSM采用辅助变量来区分识别和动态，因此仅需要对辅助变量进行摊销推理。利用连续-离散滤波理论的技术，我们展示了如何对动态状态进行准确的贝叶斯推断。我们提出了三种灵活的潜在动态参数化方法和一种在推断期间对动态状态进行边缘化的高效培训目标。在各个领域的多个基准数据集上的实证结果表明了改进了的性能。

    Learning accurate predictive models of real-world dynamic phenomena (e.g., climate, biological) remains a challenging task. One key issue is that the data generated by both natural and artificial processes often comprise time series that are irregularly sampled and/or contain missing observations. In this work, we propose the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations. NCDSSM employs auxiliary variables to disentangle recognition from dynamics, thus requiring amortized inference only for the auxiliary variables. Leveraging techniques from continuous-discrete filtering theory, we demonstrate how to perform accurate Bayesian inference for the dynamic states. We propose three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference. Empirical results on multiple benchmark datasets across various domains show improved i
    
[^131]: 水母如何表征交替群等变神经网络

    How Jellyfish Characterise Alternating Group Equivariant Neural Networks. (arXiv:2301.10152v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10152](http://arxiv.org/abs/2301.10152)

    该论文提供了交替群($A_n$)等变神经网络的完整表征，其中描述了可学习的、线性的、$A_n$等变层函数的矩阵基，在神经网络构建中具有广泛的适用性。

    

    我们提供了所有可能的层数为$\mathbb{R}^{n}$张量幂次的交替群($A_n$)等变神经网络的完整表征。特别地，我们在$\mathbb{R}^{n}$的标准基础上找到一组可学习的、线性的、$A_n$等变层函数的矩阵基。我们还描述了我们的方法如何推广到构建等变于局部对称性的神经网络。

    We provide a full characterisation of all of the possible alternating group ($A_n$) equivariant neural networks whose layers are some tensor power of $\mathbb{R}^{n}$. In particular, we find a basis of matrices for the learnable, linear, $A_n$-equivariant layer functions between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$. We also describe how our approach generalises to the construction of neural networks that are equivariant to local symmetries.
    
[^132]: 用于评估CNN预测的可信度分数

    Trustworthiness Score to Evaluate CNNs Predictions. (arXiv:2301.08839v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08839](http://arxiv.org/abs/2301.08839)

    本文提出了一种用于评估CNN预测可信度的可信度分数（TS）度量标准，通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。

    This paper proposes a trustworthiness score (TS) metric to evaluate the confidence of CNN predictions, which quantifies the trustworthiness by checking for the existence of certain features in the predictions made by the CNN.

    由于卷积神经网络（CNN）的黑盒特性，无法在操作期间持续验证CNN，这使得开发人员和监管机构难以对使用CNN的自主系统的部署获得信心。在操作期间，了解CNN的预测何时可信或可疑对安全至关重要。基本方法是使用模型的输出置信度分数来评估预测是否可信或可疑。然而，模型的置信度分数是来自黑盒计算的结果，因此缺乏透明度，使得很难将可信度归因于预测。我们引入了可信度分数（TS），这是一种简单的度量标准，提供了一种更透明和有效的方式来提供CNN预测的信心。该度量标准通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。

    Due to the black box nature of Convolutional neural networks (CNNs), the continuous validation of CNNs during operation is infeasible. As a result this makes it difficult for developers and regulators to gain confidence in the deployment of autonomous systems employing CNNs. It is critical for safety during operation to know when a CNN's predictions are trustworthy or suspicious. The basic approach is to use the model's output confidence score to assess if predictions are trustworthy or suspicious. However, the model's confidence score is a result of computations coming from a black box, therefore lacks transparency and makes it challenging to credit trustworthiness to predictions. We introduce the trustworthiness score (TS), a simple metric that provides a more transparent and effective way of providing confidence in CNNs predictions. The metric quantifies the trustworthiness in a prediction by checking for the existence of certain features in the predictions made by the CNN. The TS m
    
[^133]: 数字孪生的因果伪证

    Causal Falsification of Digital Twins. (arXiv:2301.07210v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2301.07210](http://arxiv.org/abs/2301.07210)

    这篇论文提出了一种数字孪生的因果伪证方法，以可靠并实用的方式在最小限度的假设下提供孪生的信息和评估结果。

    

    数字孪生在很多应用中具有巨大的潜力，但是在安全关键场景下广泛部署它们的精度评估需要严格的程序。通过在因果推理框架内制定这个任务，我们表明，使用现实数据尝试证明孪生的正确性是不可靠的，除非在数据生成过程中进行可能有风险的假设。为了避免这些假设，我们提出了一种评估策略，旨在找到孪生不正确的情况，并提出了用于实现此目标的通用统计过程，可用于各种应用和孪生模型。我们的方法在最小假设下提供了可靠和可操作的孪生信息和评估结果。通过包含脉冲生理学引擎中脓毒症建模的大型案例研究，我们证明了我们方法的有效性。

    Digital twins hold substantial promise in many applications, but rigorous procedures for assessing their accuracy are essential for their widespread deployment in safety-critical settings. By formulating this task within the framework of causal inference, we show that attempts to certify the correctness of a twin using real-world observational data are unsound unless potentially tenuous assumptions are made about the data-generating process. To avoid these assumptions, we propose an assessment strategy that instead aims to find cases where the twin is not correct, and present a general-purpose statistical procedure for doing so that may be used across a wide variety of applications and twin models. Our approach yields reliable and actionable information about the twin under minimal assumptions about the twin and the real-world process of interest. We demonstrate the effectiveness of our methodology via a large-scale case study involving sepsis modelling within the Pulse Physiology Engi
    
[^134]: 深度神经网络不安全输入计数的#DNN-Verification问题

    The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks. (arXiv:2301.07068v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.07068](http://arxiv.org/abs/2301.07068)

    本文提出了#DNN-Verification问题，即计算违反特定安全性质的DNN输入配置数量的问题。作者提出了一种新的方法和一种随机的近似方法，分别给出了确切的违规计数和可证明概率界，并在安全关键基准测试上进行了实验比较。

    

    深度神经网络（DNN）在需要高度安全性的关键任务中，例如自动驾驶中越来越被采用。虽然最先进的验证器可以用来检查DNN是否不安全，即是否存在至少一种不安全的输入配置，但它们的是/否输出对于其他目的（如屏蔽、模型选择或培训改进）的信息不足够详细。在本文中，我们介绍了#DNN-Verification问题，它涉及计算导致DNN违反特定安全性质的输入配置数量。我们分析了这个问题的复杂性，并提出了一种新的方法，它返回确切的违规计数。由于该问题的#P完备性，我们还提出了一种随机的近似方法，该方法提供了正确计数的可证明概率界，同时显著降低了计算要求。我们在一组安全关键基准测试上呈现了实验结果，比较了我们的方法与最先进的验证器和基于计数的启发式算法。

    Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-cr
    
[^135]: 基于对抗生成网络的短SSVEP数据扩展框架

    Short-length SSVEP data extension by a novel generative adversarial networks based framework. (arXiv:2301.05599v3 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2301.05599](http://arxiv.org/abs/2301.05599)

    本文提出了一种基于GAN的端到端信号转化网络TEGAN，可以将短SSVEP信号转换成长的人工SSVEP信号，并显著提高BCI系统的效率和准确性。

    

    基于SSVEP的脑机接口因其高信息传输速率和目标数量可用性而受到广泛关注。然而，频率识别方法的性能在很大程度上取决于用户校准数据的数量和数据长度，这限制了它在实际应用中的部署。最近，基于生成对抗网络（GANs）的数据生成方法已被广泛采用来创建合成的脑电数据，有望解决这些问题。本文提出了一种基于GANs的端到端信号转化网络TEGAN，用于数据长度扩展。TEGAN可以将短SSVEP信号转换成长的人工SSVEP信号。通过将一个新颖的U型生成器架构和一个辅助分类器加入到网络结构中，TEGAN可以在合成数据中产生有条件的特征。此外，我们实现并比较了两种最先进的频率识别方法，以评估TEGAN生成数据的有效性。实验结果表明，所提出的TEGAN方法优于传统的线性插值方法和最先进的基于深度学习的方法。所提出的TEGAN方法可以显著提高BCI系统的效率，减少所需的校准时间并改善分类的准确性。

    Steady-state visual evoked potentials (SSVEPs) based brain-computer interface (BCI) has received considerable attention due to its high information transfer rate (ITR) and available quantity of targets. However, the performance of frequency identification methods heavily hinges on the amount of user calibration data and data length, which hinders the deployment in real-world applications. Recently, generative adversarial networks (GANs)-based data generation methods have been widely adopted to create synthetic electroencephalography (EEG) data, holds promise to address these issues. In this paper, we proposed a GAN-based end-to-end signal transformation network for data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP signals into long-length artificial SSVEP signals. By incorporating a novel U-Net generator architecture and an auxiliary classifier into the network architecture, the TEGAN could produce conditioned features in the synthetic data. Additionally, we i
    
[^136]: 基于马尔可夫决策过程框架的序列公平资源分配

    Sequential Fair Resource Allocation under a Markov Decision Process Framework. (arXiv:2301.03758v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.03758](http://arxiv.org/abs/2301.03758)

    本研究提出了一种基于马尔可夫决策过程框架的算法SAFFE，用于在有限时间内分配有限资源给披露其随机需求的代理，并实现公平分配。实验结果表明，SAFFE在公平性和社会福利方面的性能优于多种基线算法。

    

    本研究研究了在有限时间内，将有限资源分配给会在到达时披露其随机需求的代理的顺序决策问题。我们的目标是设计公平的分配算法，以用尽可用的资源预算。在信息不完全的情况下做出决策时，这在序列设置中是具有挑战性的。我们将问题制定为离散时间马尔可夫决策过程（MDP）。我们提出了一种新算法SAFFE，它通过考虑到每个到达时间的未来预期需求量，根据全面披露的需求实现公平分配。该算法引入了正则化之后，就能根据代理未来需求的不确定性，使当前披露出来的需求优先于未来潜在需求。使用MDP公式，我们展示了SAFFE基于Nash社会福利公平性目标的上限，以及将其差距与最优性的绑定函数作为代理数量和horizon长度。我们的实验表明，在各种需求模式和尺寸的设置中，SAFFE实现了比几种基线更高的公平性和社会福利。

    We study the sequential decision-making problem of allocating a limited resource to agents that reveal their stochastic demands on arrival over a finite horizon. Our goal is to design fair allocation algorithms that exhaust the available resource budget. This is challenging in sequential settings where information on future demands is not available at the time of decision-making. We formulate the problem as a discrete time Markov decision process (MDP). We propose a new algorithm, SAFFE, that makes fair allocations with respect to the entire demands revealed over the horizon by accounting for expected future demands at each arrival time. The algorithm introduces regularization which enables the prioritization of current revealed demands over future potential demands depending on the uncertainty in agents' future demands. Using the MDP formulation, we show that SAFFE optimizes allocations based on an upper bound on the Nash Social Welfare fairness objective, and we bound its gap to opti
    
[^137]: 深度线性网络中的神经塌陷:从平衡到不平衡的数据

    Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data. (arXiv:2301.00437v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00437](http://arxiv.org/abs/2301.00437)

    研究者证明对于均方误差和交叉熵损失，深度线性网络中出现的全局解在不同数据上都具有神经塌陷的特性，即最后一层特征会崩溃为类均值，而这些类均值是等角紧框架的顶点。

    

    现代深度神经网络在图像分类和自然语言处理等任务中表现出色，但令人惊讶的是，这些具有大量参数的复杂系统在训练到收敛时，它们的最后一层特征和分类器在经典数据集上表现出相同的结构性质。特别地，观察到最后一层特征会崩溃为类均值，并且这些类均值是等角紧框架(simplex Equiangular Tight Frame)的顶点。这种现象被称为神经塌陷(NC)。最近的论文理论上证明了在简化的“无约束特征模型”训练问题的全局最小值中出现了$\mathcal{NC}$。在这个语境下，我们进一步证明了在常用的均方误差(MSE)和交叉熵(CE)损失下，深度线性网络中也会发生$\mathcal{NC}$现象，表明全局解在不同数据上都具有$\mathcal{NC}$的特性。

    Modern deep neural networks have achieved impressive performance on tasks from image classification to natural language processing. Surprisingly, these complex systems with massive amounts of parameters exhibit the same structural properties in their last-layer features and classifiers across canonical datasets when training until convergence. In particular, it has been observed that the last-layer features collapse to their class-means, and those class-means are the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is known as Neural Collapse ($\mathcal{NC}$). Recent papers have theoretically shown that $\mathcal{NC}$ emerges in the global minimizers of training problems with the simplified ``unconstrained feature model''. In this context, we take a step further and prove the $\mathcal{NC}$ occurrences in deep linear networks for the popular mean squared error (MSE) and cross entropy (CE) losses, showing that global solutions exhibit $\mathcal{NC}$ properties across
    
[^138]: 通过课程理解单任务强化学习中的复杂度收益

    Understanding the Complexity Gains of Single-Task RL with a Curriculum. (arXiv:2212.12809v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12809](http://arxiv.org/abs/2212.12809)

    本文提出一个理论框架将单任务强化学习问题重新构造为由课程定义的多任务问题，证明在课程有轻微正则化条件的情况下，依次解决每个任务比直接解决原始单任务更加计算上高效。

    

    没有经过良好设计的奖励机制使得强化学习问题变得具有挑战性。现有的高效强化学习方法通常提出使用专门的探索策略来解决此问题。然而，另一种解决这个问题的方法是将其重新构造为一个多任务强化学习问题，其中任务空间不仅包含挑战性任务，还包含隐含的课程作为辅助。这样的重新构造打开了使用现有多任务强化学习方法作为解决单个具有挑战性任务的更高效的替代方法的可能性。本文提供了一个理论框架，将单任务强化学习问题重新构造为由课程定义的多任务强化学习问题。在课程有轻微正则化条件的情况下，我们证明了依次解决多任务RL问题中的每个任务比从头开始解决原始单任务问题更加计算上高效，而无需任何显式的探索奖励。

    Reinforcement learning (RL) problems can be challenging without well-shaped rewards. Prior work on provably efficient RL methods generally proposes to address this issue with dedicated exploration strategies. However, another way to tackle this challenge is to reformulate it as a multi-task RL problem, where the task space contains not only the challenging task of interest but also easier tasks that implicitly function as a curriculum. Such a reformulation opens up the possibility of running existing multi-task RL methods as a more efficient alternative to solving a single challenging task from scratch. In this work, we provide a theoretical framework that reformulates a single-task RL problem as a multi-task RL problem defined by a curriculum. Under mild regularity conditions on the curriculum, we show that sequentially solving each task in the multi-task RL problem is more computationally efficient than solving the original single-task problem, without any explicit exploration bonuse
    
[^139]: StyleDomain：用于单次和少次领域适应的StyleGAN高效轻量化参数化

    StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation. (arXiv:2212.10229v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.10229](http://arxiv.org/abs/2212.10229)

    本文对GAN的领域适应问题进行了深入研究并提出了StyleGAN用于领域适应的新的高效轻量级参数化方案。

    

    GAN的领域适应是fine-tuning在大规模数据集上预训练的先进GAN模型（例如StyleGAN）以适应具有少量样本的特定领域（例如绘画面孔、素描等）。本文对GAN的领域适应问题（特别是StyleGAN模型）进行系统和深入的分析，并提出了StyleGAN用于领域适应的新的高效轻量级参数化方案。

    Domain adaptation of GANs is a problem of fine-tuning the state-of-the-art GAN models (e.g. StyleGAN) pretrained on a large dataset to a specific domain with few samples (e.g. painting faces, sketches, etc.). While there are a great number of methods that tackle this problem in different ways, there are still many important questions that remain unanswered.  In this paper, we provide a systematic and in-depth analysis of the domain adaptation problem of GANs, focusing on the StyleGAN model. First, we perform a detailed exploration of the most important parts of StyleGAN that are responsible for adapting the generator to a new domain depending on the similarity between the source and target domains. As a result of this in-depth study, we propose new efficient and lightweight parameterizations of StyleGAN for domain adaptation. Particularly, we show there exist directions in StyleSpace (StyleDomain directions) that are sufficient for adapting to similar domains and they can be reduced fu
    
[^140]: 深度学习在计算力学中的应用：综述、现状与经典

    Deep learning applied to computational mechanics: A comprehensive review, state of the art, and the classics. (arXiv:2212.08989v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08989](http://arxiv.org/abs/2212.08989)

    本文综述了深度学习在计算力学中的应用，讨论了混合和纯深度学习方法的进展。其中，混合方法将传统PDE离散化与深度学习相结合，而纯深度学习方法则代表为物理信息神经网络（PINN）方法。

    

    本文综述深度学习在计算力学中的应用，并讨论了混合和纯深度学习方法的进展。混合方法将传统PDE离散化与深度学习方法相结合，纯深度学习方法则代表为物理信息神经网络（PINN）方法。

    Three recent breakthroughs due to AI in arts and science serve as motivation: An award winning digital image, protein folding, fast matrix multiplication. Many recent developments in artificial neural networks, particularly deep learning (DL), applied and relevant to computational mechanics (solid, fluids, finite-element technology) are reviewed in detail. Both hybrid and pure machine learning (ML) methods are discussed. Hybrid methods combine traditional PDE discretizations with ML methods either (1) to help model complex nonlinear constitutive relations, (2) to nonlinearly reduce the model order for efficient simulation (turbulence), or (3) to accelerate the simulation by predicting certain components in the traditional integration methods. Here, methods (1) and (2) relied on Long-Short-Term Memory (LSTM) architecture, with method (3) relying on convolutional neural networks. Pure ML methods to solve (nonlinear) PDEs are represented by Physics-Informed Neural network (PINN) methods, 
    
[^141]: Brauer群等变神经网络

    Brauer's Group Equivariant Neural Networks. (arXiv:2212.08630v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08630](http://arxiv.org/abs/2212.08630)

    本文描述了对于三个在机器学习文献中缺失的对称群（$O(n)$、$SO(n)$和$Sp(n)$），所有可能的群等变神经网络的特征，并找到了这些网络在不同基础下矩阵的生成集。

    

    我们提供了所有可能的群等变神经网络的完整特征描述，其中的层是$\mathbb{R}^{n}$的某些张量幂，适用于三个在机器学习文献中缺失的对称群：$O(n)$（正交群），$SO(n)$（特殊正交群）和$Sp(n)$（辛群）。特别地，当群为$O(n)$或$SO(n)$时，在$\mathbb{R}^{n}$的标准基础下，我们找到了可学习的、线性的、等变的层函数之间的矩阵的生成集；当群为$Sp(n)$时，我们则在$\mathbb{R}^{n}$的辛基础下找到了这样的生成集。

    We provide a full characterisation of all of the possible group equivariant neural networks whose layers are some tensor power of $\mathbb{R}^{n}$ for three symmetry groups that are missing from the machine learning literature: $O(n)$, the orthogonal group; $SO(n)$, the special orthogonal group; and $Sp(n)$, the symplectic group. In particular, we find a spanning set of matrices for the learnable, linear, equivariant layer functions between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$ when the group is $O(n)$ or $SO(n)$, and in the symplectic basis of $\mathbb{R}^{n}$ when the group is $Sp(n)$.
    
[^142]: 基于神经隐式k空间的无分bin非笛卡尔心脏磁共振成像

    Neural Implicit k-Space for Binning-free Non-Cartesian Cardiac MR Imaging. (arXiv:2212.08479v5 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2212.08479](http://arxiv.org/abs/2212.08479)

    本文提出了一种基于神经隐式k空间的无分bin非笛卡尔心脏磁共振成像方法，能够实现一个连续、无分bin以及个体化的k空间表示，并通过简单的反傅里叶变换恢复图像，消除密度补偿和费用昂贵的非均匀傅里叶变换的需求。

    

    本文提出了一种新的图像重建框架，通过直接在k空间中学习神经隐式表示，实现了心脏磁共振成像的ECG触发非笛卡尔方式。相较于现有的方法需要将相邻时间点的数据进行分bin，以重建心脏运动的一个相位，我们的框架允许了一个连续、无分bin 以及个体化的k空间表示。我们为每个采样的k空间点分配一个独特的坐标，由时间、线圈指数和频率域位置组成。然后使用带有频域正则化的多层感知器学习从这些唯一坐标到k空间强度的个体化映射。在推断过程中，我们获得笛卡尔坐标和任意时间分辨率的完整k空间。一个简单的傅里叶反变换即可恢复图像，消除了对密度补偿和昂贵的非均匀傅里叶变换的需求。

    In this work, we propose a novel image reconstruction framework that directly learns a neural implicit representation in k-space for ECG-triggered non-Cartesian Cardiac Magnetic Resonance Imaging (CMR). While existing methods bin acquired data from neighboring time points to reconstruct one phase of the cardiac motion, our framework allows for a continuous, binning-free, and subject-specific k-space representation.We assign a unique coordinate that consists of time, coil index, and frequency domain location to each sampled k-space point. We then learn the subject-specific mapping from these unique coordinates to k-space intensities using a multi-layer perceptron with frequency domain regularization. During inference, we obtain a complete k-space for Cartesian coordinates and an arbitrary temporal resolution. A simple inverse Fourier transform recovers the image, eliminating the need for density compensation and costly non-uniform Fourier transforms for non-Cartesian data. This novel im
    
[^143]: 面向大规模灵活绑定高斯混合模型的随机一阶学习

    Stochastic First-Order Learning for Large-Scale Flexibly Tied Gaussian Mixture Model. (arXiv:2212.05402v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05402](http://arxiv.org/abs/2212.05402)

    本文提出了一种面对高维流数据和复杂密度挑战的快速在线参数估计算法，并利用灵活绑定因子分解的协方差矩阵提供了一个框架。通过引入一阶随机优化和新的随机流形优化算法，实现了高斯混合模型的优化。

    

    高斯混合模型(GMM)是一种基于核模型的最有效的参数密度估计器，广泛应用于许多科学领域。随着数据源的剧增，传统的机器学习算法，如期望最大化(EM)，在面对高维和流数据时遇到困难。此外，复杂的密度往往需要大量的高斯组件。本文提出了一种快速的在线参数估计算法，通过使用一阶随机优化来处理GMM所面临的高维流数据和复杂密度的挑战，利用协方差矩阵的灵活绑定因子分解提供了一个框架。引入了一种新的随机流形优化算法，保持正交性，并与众所周知的欧几里得空间数值优化一起使用。在合成和实验数据上进行了大量实证结果。

    Gaussian Mixture Models (GMM) are one of the most potent parametric density estimators based on the kernel model that finds application in many scientific domains. In recent years, with the dramatic enlargement of data sources, typical machine learning algorithms, e.g. Expectation Maximization (EM), encounters difficulty with high-dimensional and streaming data. Moreover, complicated densities often demand a large number of Gaussian components. This paper proposes a fast online parameter estimation algorithm for GMM by using first-order stochastic optimization. This approach provides a framework to cope with the challenges of GMM when faced with high-dimensional streaming data and complex densities by leveraging the flexibly-tied factorization of the covariance matrix. A new stochastic Manifold optimization algorithm that preserves the orthogonality is introduced and used along with the well-known Euclidean space numerical optimization. Numerous empirical results on both synthetic and 
    
[^144]: 多概念定制化的文本图像扩散

    Multi-Concept Customization of Text-to-Image Diffusion. (arXiv:2212.04488v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.04488](http://arxiv.org/abs/2212.04488)

    该论文提出了一种称为Custom Diffusion的高效方法，用于增强现有的文本图像模型以实现多概念定制化，该方法只需优化文本图像调节机制中的几个参数就可以表示新概念并实现快速调整。同时，该方法可以联合训练多个概念或将多个精调模型合并为一个，并在生成多个新概念的变异并将它们与现有概念无缝地组合方面表现出色。

    

    尽管生成模型可以从大规模数据库中学习到高质量的概念图像，但用户通常希望合成他们自己的概念的实例（例如，他们的家庭、宠物或物品）。我们能否教会模型快速获取新的概念，只给出几个示例？此外，我们能否组合多个新概念？我们提出了一种高效的方法——定制化扩散，用于增强现有的文本图像模型。我们发现，只优化文本图像调节机制中的几个参数就足够强大，可以表示新概念，并能实现快速调整（约6分钟）。此外，我们通过闭合形式约束优化，可以联合训练多个概念或将多个精调模型合并为一个。我们的精调模型生成多个新概念的变异，并将它们与现有概念无缝地组合，形成新颖的场景。我们的方法优于或与多个基线和并发作品并驾齐驱。

    While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (~6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works
    
[^145]: 基于注意力机制的双向LSTM价格预测

    Bi-LSTM Price Prediction based on Attention Mechanism. (arXiv:2212.03443v2 [q-fin.CP] UPDATED)

    [http://arxiv.org/abs/2212.03443](http://arxiv.org/abs/2212.03443)

    本文提出了一种基于注意力机制的双向LSTM神经网络，用于黄金和比特币的价格预测，结合传统的技术因素和时间序列模型开发了因素，使用预测结果获得了显著的回报，且模型表现最佳。

    

    随着金融衍生品市场的日益丰富和发展，交易频率也越来越快。由于人类的限制，算法和自动交易最近成为了讨论的焦点。本文提出了一种基于注意力机制的双向LSTM神经网络，该网络基于两种热门资产：黄金和比特币。在特征工程方面，我们一方面添加了传统的技术因素，同时结合时间序列模型开发了因素。在模型参数的选择上，我们最终选择了一个双层深度学习网络。根据AUC度量，比特币和黄金的准确率分别为71.94％和73.03％。使用预测结果，在两年内获得了1089.34％的回报。同时，我们还将本文提出的注意力Bi-LSTM模型与传统模型进行比较，结果表明我们的模型在这个预测问题中具有最佳性能。

    With the increasing enrichment and development of the financial derivatives market, the frequency of transactions is also faster and faster. Due to human limitations, algorithms and automatic trading have recently become the focus of discussion. In this paper, we propose a bidirectional LSTM neural network based on an attention mechanism, which is based on two popular assets, gold and bitcoin. In terms of Feature Engineering, on the one hand, we add traditional technical factors, and at the same time, we combine time series models to develop factors. In the selection of model parameters, we finally chose a two-layer deep learning network. According to AUC measurement, the accuracy of bitcoin and gold is 71.94% and 73.03% respectively. Using the forecast results, we achieved a return of 1089.34% in two years. At the same time, we also compare the attention Bi-LSTM model proposed in this paper with the traditional model, and the results show that our model has the best performance in thi
    
[^146]: 连续学习的统计力学:变分原理和平均场势

    Statistical mechanics of continual learning: variational principle and mean-field potential. (arXiv:2212.02846v3 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2212.02846](http://arxiv.org/abs/2212.02846)

    从物理学的角度将连续学习的问题转化为Franz-Parisi热力学势的框架，将之前学习到的任务作为先验和参考，提出了一个在场空间中训练神经网络的变分贝叶斯学习设置，用于调节任务间的突触资源。

    

    人工智能通用性的一个障碍是多种不同任务的连续学习。最近，涉及机器学习和神经科学的各种启发性技巧被提出，但它们缺乏一个统一的理论基础。本文关注二元权重的单层和多层神经网络的连续学习。提出了一个变分贝叶斯学习设置，其中神经网络在场空间而不是渐变未定义的离散权重空间中进行训练，并且自然地将权重不确定性合并，并调节任务间的突触资源。从物理学的角度，我们将变分的连续学习转化为Franz-Parisi热力学势的框架，其中以前的任务知识充当先验和参考。因此，我们将一个教师-学生设置中的二元感知器的连续学习解释为一个Franz-Parisi势。

    An obstacle to artificial general intelligence is set by the continual learning of multiple tasks of different nature. Recently, various heuristic tricks, both from machine learning and from neuroscience angles, were proposed, but they lack a unified theory ground. Here, we focus on the continual learning in single-layered and multi-layered neural networks of binary weights. A variational Bayesian learning setting is thus proposed, where the neural network is trained in a field-space, rather than the gradient-ill-defined discrete-weight space, and furthermore, the weight uncertainty is naturally incorporated, and modulates the synaptic resources among tasks. From a physics perspective, we translate the variational continual learning into the Franz-Parisi thermodynamic potential framework, where the previous task knowledge acts as a prior and a reference as well. We thus interprete the continual learning of the binary perceptron in a teacher-student setting as a Franz-Parisi potential c
    
[^147]: 深度神经网络在单声道语音增强中的技术：最新研究分析

    Deep neural network techniques for monaural speech enhancement: state of the art analysis. (arXiv:2212.00369v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2212.00369](http://arxiv.org/abs/2212.00369)

    本文回顾了在单声道语音增强领域被广泛应用的DNN技术，包括从特征提取到模型训练的整个增强流程，并探讨了使用预训练模型提高增强效果的方法。

    

    深度神经网络技术已经在自然语言处理和计算机视觉等领域得到广泛应用。在机器翻译和图像生成等任务中取得了巨大成功。由于其成功，这些数据驱动的技术已经应用于音频领域。具体而言，DNN模型已被应用于语音增强领域，以实现在单声道语音增强中的去噪、去混响和多扬声器分离。在本文中，我们回顾了一些被广泛应用于语音分离的主导DNN技术。回顾了从特征抽取、基于DNN的工具如何建模语音的全局和局部特征到模型训练（监督和非监督）的整个语音增强流程。我们还回顾了使用语音增强预训练模型以增强语音增强过程的方法。本文旨在涵盖DNN在单声道语音增强中的主要趋势。

    Deep neural networks (DNN) techniques have become pervasive in domains such as natural language processing and computer vision. They have achieved great success in these domains in task such as machine translation and image generation. Due to their success, these data driven techniques have been applied in audio domain. More specifically, DNN models have been applied in speech enhancement domain to achieve denosing, dereverberation and multi-speaker separation in monaural speech enhancement. In this paper, we review some dominant DNN techniques being employed to achieve speech separation. The review looks at the whole pipeline of speech enhancement from feature extraction, how DNN based tools are modelling both global and local features of speech and model training (supervised and unsupervised). We also review the use of speech-enhancement pre-trained models to boost speech enhancement process. The review is geared towards covering the dominant trends with regards to DNN application in
    
[^148]: 基础模型的能力探究

    On the Power of Foundation Models. (arXiv:2211.16327v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.16327](http://arxiv.org/abs/2211.16327)

    本文通过范畴论探究了基础模型的能力，提出了具有最小所需能力的基础模型可以通过微调和足够的资源来解决前置任务所定义的类别中的下游任务，并且这种能力可以扩展到任何下游任务，只要允许微调且下游任务可在前置任务定义的范畴中表示。

    

    如果基础模型具有无限高质量的数据点、无限计算能力、一个无限大的完美训练算法、以及在预设任务上保证零泛化误差，那么它可以用于一切吗？传统的表示、优化或泛化理论无法回答这个问题，因为它们主要探讨的问题在这里都是不存在的。本文提出范畴论提供了强大的理论工具，以回答这个问题。我们证明了三个结果，第一个限制了基于提示的学习的能力，即仅当任务可表示时，模型才能用提示解决下游任务；第二个结果表明，微调不受这个限制，因为一个具有最小所需能力（对称性）的基础模型可以通过微调和足够的资源来理论上解决前置任务所定义的类别中的下游任务。我们的最终结果可以看作是第二个结果的一般化，表明如果允许微调并且下游任务可在前置任务定义的范畴中表示，则基础模型的最小能力也足以解决任何下游任务。

    With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be se
    
[^149]: 一维卷积神经网络的Lipschitz常数估计

    Lipschitz constant estimation for 1D convolutional neural networks. (arXiv:2211.15253v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15253](http://arxiv.org/abs/2211.15253)

    本文提出了一种利用耗散理论估计一维卷积神经网络Lipschitz常数的方法，具有高效、准确和可扩展性的特点。

    

    本文提出了一种基于耗散方法的一维卷积神经网络（CNNs）Lipschitz常数估计方法。具体来说，我们利用非线性激活函数和池化操作的增量二次约束分析了卷积、池化和全连接层的耗散特性。然后，通过解决从耗散理论中推导出的半定规划问题来估计这些映射的拼接的Lipschitz常数。为了使我们的方法尽可能高效，我们利用卷积层的结构，将这些有限冲激响应滤波器表示为状态空间中的因果动态系统，并对状态空间实现进行耗散性分析。我们提供的示例表明，我们的Lipschitz界在准确性和可扩展性方面具有优势。

    In this work, we propose a dissipativity-based method for Lipschitz constant estimation of 1D convolutional neural networks (CNNs). In particular, we analyze the dissipativity properties of convolutional, pooling, and fully connected layers making use of incremental quadratic constraints for nonlinear activation functions and pooling operations. The Lipschitz constant of the concatenation of these mappings is then estimated by solving a semidefinite program which we derive from dissipativity theory. To make our method as efficient as possible, we exploit the structure of convolutional layers by realizing these finite impulse response filters as causal dynamical systems in state space and carrying out the dissipativity analysis for the state space realizations. The examples we provide show that our Lipschitz bounds are advantageous in terms of accuracy and scalability.
    
[^150]: AugOp：将变换注入神经算子

    AugOp: Inject Transformation into Neural Operator. (arXiv:2211.12514v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12514](http://arxiv.org/abs/2211.12514)

    本文提出了一种基于注入额外的分组变换来增强常规卷积算子的方法，名为 AugConv，可以引入更大的学习能力，提升模型性能，而不会增加部署时的额外计算开销。

    

    本文提出了一种简单而通用的方法，通过在训练期间注入额外的分组变换来增强常规卷积算子，并在推理期间恢复它。精心选择的额外变换确保其可以与每个组中的常规卷积合并，并且在推理期间不会改变常规卷积的拓扑结构。与常规卷积算子相比，我们的方法（AugConv）可以引入更大的学习能力，以改善模型在训练期间的性能，但不会增加模型部署的额外计算开销。基于 ResNet，我们利用 AugConv 构建卷积神经网络，称为 AugResNet。图像分类数据集 Cifar-10 上的结果显示，AugResNet 在模型性能方面优于其基线。

    In this paper, we propose a simple and general approach to augment regular convolution operator by injecting extra group-wise transformation during training and recover it during inference. Extra transformation is carefully selected to ensure it can be merged with regular convolution in each group and will not change the topological structure of regular convolution during inference. Compared with regular convolution operator, our approach (AugConv) can introduce larger learning capacity to improve model performance during training but will not increase extra computational overhead for model deployment. Based on ResNet, we utilize AugConv to build convolutional neural networks named AugResNet. Result on image classification dataset Cifar-10 shows that AugResNet outperforms its baseline in terms of model performance.
    
[^151]: PhAST：物理感知、可扩展、任务特定的GNN在加速催化剂设计中的应用

    PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design. (arXiv:2211.12020v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12020](http://arxiv.org/abs/2211.12020)

    提出了PhAST方法来快速发现更有效的催化剂来驱动电化学反应, 该方法适用于大多数体系结构, 可以增加计算效率和精度

    

    缓解气候危机需要快速向低碳能源转变。催化剂材料在许多工业过程中的电化学反应中起着至关重要的作用，如可再生能源储存和电荷合成。为了减少在这些过程中消耗的能量，我们必须快速发现更有效的催化剂来驱动电化学反应。机器学习（ML）有潜力从大量数据中高效地模拟材料的性质，从而加速电催化剂的设计。为此，Open Catalyst Project OC20数据集已经被构建。然而，大多数已经在OC20上训练的现有ML模型仍然无法满足实际应用的可扩展性和准确性要求。在这里，我们提出了几个任务特定的创新，适用于大多数体系结构，可以增加计算效率和精度。特别是我们在图翻译层、图注意力层和池化层中提出了改进。我们将这种方法称为Physical Attribute Scaling Transformer (PhAST)。我们证明了PhAST在生成准确数据的同时，具有低计算成本，适用于几个相关应用，包括电催化剂的发现和设计。

    Mitigating the climate crisis requires a rapid transition towards lower carbon energy. Catalyst materials play a crucial role in the electrochemical reactions involved in a great number of industrial processes key to this transition, such as renewable energy storage and electrofuel synthesis. To reduce the amount of energy spent on such processes, we must quickly discover more efficient catalysts to drive the electrochemical reactions. Machine learning (ML) holds the potential to efficiently model the properties of materials from large amounts of data, and thus to accelerate electrocatalyst design. The Open Catalyst Project OC20 data set was constructed to that end. However, most existing ML models trained on OC20 are still neither scalable nor accurate enough for practical applications. Here, we propose several task-specific innovations, applicable to most architectures, which increase both computational efficiency and accuracy. In particular, we propose improvements in (1) the graph 
    
[^152]: AdaFocal：校准感知自适应Focal Loss

    AdaFocal: Calibration-aware Adaptive Focal Loss. (arXiv:2211.11838v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11838](http://arxiv.org/abs/2211.11838)

    本文介绍了AdaFocal，一种新的校准感知自适应Focal Loss。与其他方法相比，在多个数据集上表现出更好的校准性能。

    

    近期的工作致力于解决神经网络的置信度得分与正确概率不相符的校准问题。研究发现，相较于交叉熵，使用Focal Loss进行训练能更好地解决校准问题并保持相同的准确性。Focal Loss通过调控参数γ来规范模型预测的熵，从而抑制模型的过度自信。本文提出了一种新的校准感知自适应Focal Loss，称为AdaFocal，它利用了Focal Loss和Inverse-Focal Loss的校准性质，并根据前一次迭代中的γt自适应地修改不同样本组的γt。实验证明，所提出的方法在多个数据集上优于最先进的方法解决校准问题。

    Much recent work has been devoted to the problem of ensuring that a neural network's confidence scores match the true probability of being correct, i.e. the calibration problem. Of note, it was found that training with focal loss leads to better calibration than cross-entropy while achieving similar level of accuracy \cite{mukhoti2020}. This success stems from focal loss regularizing the entropy of the model's prediction (controlled by the parameter $\gamma$), thereby reining in the model's overconfidence. Further improvement is expected if $\gamma$ is selected independently for each training sample (Sample-Dependent Focal Loss (FLSD-53) \cite{mukhoti2020}). However, FLSD-53 is based on heuristics and does not generalize well. In this paper, we propose a calibration-aware adaptive focal loss called AdaFocal that utilizes the calibration properties of focal (and inverse-focal) loss and adaptively modifies $\gamma_t$ for different groups of samples based on $\gamma_{t-1}$ from the previo
    
[^153]: SinFusion：在单张图像或视频上训练扩散模型

    SinFusion: Training Diffusion Models on a Single Image or Video. (arXiv:2211.11743v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11743](http://arxiv.org/abs/2211.11743)

    本文提出了一种在单张图像或视频上训练扩散模型的方法，称为SinFusion。该模型可以解决各种图像/视频特定的操作任务，包括从少量帧中学习单个输入视频的运动和动态，生成相同动态场景的多样化新视频样本，将短视频推广为长视频（向前和向后）并执行视频上采样。

    

    扩散模型在图像和视频生成方面取得了巨大的进展，超过了GAN在质量和多样性方面。然而，它们通常是在非常大的数据集上训练的，并且不自然地适应于操作给定的输入图像或视频。在本文中，我们展示了如何通过在单个输入图像或视频上训练扩散模型来解决这个问题。我们的图像/视频特定扩散模型（SinFusion）学习单个图像或视频的外观和动态，同时利用扩散模型的条件能力。它可以解决各种图像/视频特定的操作任务。特别地，我们的模型可以从少量帧中学习单个输入视频的运动和动态。然后，它可以生成相同动态场景的多样化新视频样本，将短视频推广为长视频（向前和向后）并执行视频上采样。这些任务中的大多数都无法通过当前的视频特定生成方法实现。

    Diffusion models exhibited tremendous progress in image and video generation, exceeding GANs in quality and diversity. However, they are usually trained on very large datasets and are not naturally adapted to manipulate a given input image or video. In this paper we show how this can be resolved by training a diffusion model on a single input image or video. Our image/video-specific diffusion model (SinFusion) learns the appearance and dynamics of the single image or video, while utilizing the conditioning capabilities of diffusion models. It can solve a wide array of image/video-specific manipulation tasks. In particular, our model can learn from few frames the motion and dynamics of a single input video. It can then generate diverse new video samples of the same dynamic scene, extrapolate short videos into long ones (both forward and backward in time) and perform video upsampling. Most of these tasks are not realizable by current video-specific generation methods.
    
[^154]: GRACE：离散键值适配器实现的终身模型编辑

    Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters. (arXiv:2211.11031v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11031](http://arxiv.org/abs/2211.11031)

    本论文提出了一种名为GRACE的方法来实现终身模型编辑，它通过在流式错误上执行目标编辑来修复部署模型的问题，生成一个离散、本地的编辑编码本，而不会改变模型权重，在进行数千个顺序编辑时表现为最先进的性能。

    

    部署的模型随时间推移会衰退，原因是输入的变化、用户需求不断改变、或由于出现知识空缺。当发现有害行为时，需要进行有针对性的编辑。然而，当前的模型编辑器在多次编辑中会降低模型的性能。我们提出了GRACE，一种终身模型编辑方法，它在部署模型的流式错误上实现了问题修补，确保对不相关的输入的影响最小化。GRACE将新的映射项写入预训练模型的潜在空间，创建了一个离散的、本地的编码本，而不会改变模型权重。这是第一种只使用流式错误实现数千个顺序编辑的方法。我们在T5、BERT和GPT模型上进行了实验，结果表明GRACE在进行并保留编辑方面的性能处于最先进水平，同时可以推广到未见过的输入。我们的代码可在https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace中获得。

    Deployed models decay over time due to shifting inputs, changing user needs, or emergent knowledge gaps. When harmful behaviors are identified, targeted edits are required. However, current model editors, which adjust specific behaviors of pre-trained models, degrade model performance over multiple edits. We propose GRACE, a Lifelong Model Editing method, which implements spot-fixes on streaming errors of a deployed model, ensuring minimal impact on unrelated inputs. GRACE writes new mappings into a pre-trained model's latent space, creating a discrete, local codebook of edits without altering model weights. This is the first method enabling thousands of sequential edits using only streaming errors. Our experiments on T5, BERT, and GPT models show GRACE's state-of-the-art performance in making and retaining edits, while generalizing to unseen inputs. Our code is available at https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace}.
    
[^155]: NVDiff：通过节点向量扩散生成图形

    NVDiff: Graph Generation through the Diffusion of Node Vectors. (arXiv:2211.10794v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10794](http://arxiv.org/abs/2211.10794)

    NVDiff利用分数为基础的生成模型采样节点向量来生成图形，显著降低了扩散过程的维度，提高了采样速度，同时在生成质量方面表现更好。

    

    学习生成图形是具有挑战性的，因为图形是一组成对连接的无序节点，编码具有复杂组合结构的信息。最近，一些研究提出了基于归一化流或基于分数的扩散模型的图形生成模型。然而，这些模型需要从相同的过程并行生成节点和边，其维数是不必要地高。我们提出了NVDiff，采用VGAE结构，并以分数为基础的生成模型（SGM）作为灵活的先验来采样节点向量。通过仅在潜空间中建模节点向量，NVDiff显着降低了扩散过程的维度，从而提高了采样速度。基于NVDiff框架，我们引入了一种基于注意力的分数网络，能够捕捉图形的局部和全局上下文。实验表明，NVDiff显着减少了计算量，可以模拟比竞争方法更大的图形。与此同时，它也实现了更好的生成质量。

    Learning to generate graphs is challenging as a graph is a set of pairwise connected, unordered nodes encoding complex combinatorial structures. Recently, several works have proposed graph generative models based on normalizing flows or score-based diffusion models. However, these models need to generate nodes and edges in parallel from the same process, whose dimensionality is unnecessarily high. We propose NVDiff, which takes the VGAE structure and uses a score-based generative model (SGM) as a flexible prior to sample node vectors. By modeling only node vectors in the latent space, NVDiff significantly reduces the dimension of the diffusion process and thus improves sampling speed. Built on the NVDiff framework, we introduce an attention-based score network capable of capturing both local and global contexts of graphs. Experiments indicate that NVDiff significantly reduces computations and can model much larger graphs than competing methods. At the same time, it achieves superior or
    
[^156]: 运动感知标记选择实现高效视频表征学习

    Efficient Video Representation Learning via Motion-Aware Token Selection. (arXiv:2211.10636v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.10636](http://arxiv.org/abs/2211.10636)

    该论文提出了一种新的运动感知标记选择方法，针对视频中不同补丁的信息密度，选择包含丰富动态特性的标记，放弃无效的标记，从而大大降低计算和存储需求，实现了在单台机器上进行预训练和微调而不影响性能。

    

    最近出现的蒙版视频建模技术通过在视频的自我监督学习中获得了显着的优势。然而，由于随机蒙版策略导致预测无效的标记/帧，这些技术需要大量的计算和存储，需要昂贵的计算机和大量显卡进行训练。我们利用视频补丁中的不均匀信息密度，并提出一种新的标记选择方法：MATS：运动感知标记选择，在自监督预训练和微调过程中找到包含丰富动态特性的标记，并放弃无效的标记，我们还提出了自适应帧选择策略，使模型能够关注最重要和因果性的帧，并使计算和存储需求得到显着降低，使得在单台机器上进行预训练和微调而不影响性能。

    Recently emerged Masked Video Modeling techniques demonstrated their potential by significantly outperforming previous methods in self-supervised learning for video. However, they require an excessive amount of computations and memory while predicting uninformative tokens/frames due to random masking strategies, requiring excessive computing power for training. (e.g., over 16 nodes with 128 NVIDIA A100 GPUs). To resolve this issue, we exploit the unequal information density among the patches in videos and propose a new token selection method, MATS: Motion-Aware Token Selection, that finds tokens containing rich motion features and drops uninformative ones during both self-supervised pre-training and fine-tuning. We further present an adaptive frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. Our method significantly reduces computation and memory requirements, enabling the pre-training and fine-tuning on a single machine w
    
[^157]: 镜像Sinkhorn：在传输多面体上进行快速在线优化

    Mirror Sinkhorn: Fast Online Optimization on Transport Polytopes. (arXiv:2211.10420v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10420](http://arxiv.org/abs/2211.10420)

    该论文提出了一种在传输多面体上进行在线凸目标优化的算法，此算法基于Sinkhorn矩阵缩放和镜像下降的原理，并且可以在噪音环境下使用。

    

    最优传输是机器学习中的重要工具，通过在传输多面体上的线性规划来捕捉数据的几何属性。我们提出了一种单循环优化算法，利用Sinkhorn矩阵缩放和镜像下降的原理，在这些领域上最小化一般凸目标。该算法对噪音具有鲁棒性，并可在在线设置中使用。我们提供了凸目标的理论保证和实验结果，展示了其在合成数据和实际数据上的有效性。

    Optimal transport is an important tool in machine learning, allowing to capture geometric properties of the data through a linear program on transport polytopes. We present a single-loop optimization algorithm for minimizing general convex objectives on these domains, utilizing the principles of Sinkhorn matrix scaling and mirror descent. The proposed algorithm is robust to noise, and can be used in an online setting. We provide theoretical guarantees for convex objectives and experimental results showcasing it effectiveness on both synthetic and real-world data.
    
[^158]: CRONOS：基于Wi-Fi CSI的无人设备NLoS人体检测的彩色化和对比学习

    CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI. (arXiv:2211.10354v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.10354](http://arxiv.org/abs/2211.10354)

    本文介绍了一种名为CRONOS的系统，可以通过彩色化和对比学习来基于Wi-Fi CSI实现无人设备NLoS人体检测，可以区分房间中的移动人员和空置。实验结果表明该系统在NLoS条件下能够准确地检测出房间中的人物存在。

    

    近年来，对于全面智能化服务和应用的需求迅速增长。通过传感器或摄像头进行无人检测已被广泛采用，但存在隐私问题以及对静止人员的错误检测。为了解决这些缺点，商用Wi-Fi设备捕获的信道状态信息(CSI)提供了丰富的信号特征，以进行准确的检测。然而，现有系统在非直视(NLoS)和静态场景下存在分类不准确的问题，例如当一个人静止站在房间角落时。在本文中，我们提出了一个名为CRONOS(基于彩色化和对比度学习增强的NLoS人体存在检测)的系统，它生成动态的复发图(RPs)和颜色编码的CSI比率以区分房间中的移动人员和空置。我们还结合监督对比学习来检索实质性的表征，其中咨询损失被制定为区分同类和异类的嵌入点之间距离度量的损失。在数据集上的实验结果表明，提出的系统在NLoS条件下能够准确地检测出房间中的人物存在。

    In recent years, the demand for pervasive smart services and applications has increased rapidly. Device-free human detection through sensors or cameras has been widely adopted, but it comes with privacy issues as well as misdetection for motionless people. To address these drawbacks, channel state information (CSI) captured from commercialized Wi-Fi devices provides rich signal features for accurate detection. However, existing systems suffer from inaccurate classification under a non-line-of-sight (NLoS) and stationary scenario, such as when a person is standing still in a room corner. In this work, we propose a system called CRONOS (Colorization and Contrastive Learning Enhanced NLoS Human Presence Detection), which generates dynamic recurrence plots (RPs) and color-coded CSI ratios to distinguish mobile people from vacancy in a room, respectively. We also incorporate supervised contrastive learning to retrieve substantial representations, where consultation loss is formulated to dif
    
[^159]: 低精度环境下利普希茨连续损失函数的SGD变种

    Variants of SGD for Lipschitz Continuous Loss Functions in Low-Precision Environments. (arXiv:2211.04655v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2211.04655](http://arxiv.org/abs/2211.04655)

    本文研究了在低精度环境下神经网络训练的SGD变种，并测试了不同变种的效果。结果表明，相比于传统的SGD方法，在低精度算术环境下使用自适应步长的SGD变种可以获得更好的测试集准确性。

    

    本文研究了在低位浮点和定点环境下神经网络训练时使用自适应步长的SGD变种的收敛性。考虑到一般随机利普希茨连续的损失函数，假设只能计算损失函数随机梯度的近似值以及计算SGD步骤本身时的误差，给出了一个渐近收敛到Clarke稳定点的结果和到近似稳定点的非渐近收敛。在各种低精度算术环境下经验地测试了不同的SGD变种，在两个图像识别任务中与SGD相比观察到了改进的测试集准确性。

    Motivated by neural network training in low-bit floating and fixed-point environments, this work studies the convergence of variants of SGD using adaptive step sizes with computational error. Considering a general stochastic Lipschitz continuous loss function, an asymptotic convergence result to a Clarke stationary point, and the non-asymptotic convergence to an approximate stationary point are presented assuming that only an approximation of the loss function's stochastic gradient can be computed, as well as error in computing the SGD step itself. Different variants of SGD are tested empirically in a variety of low-precision arithmetic environments, where improved test set accuracy is observed compared to SGD for two image recognition tasks.
    
[^160]: 基于高斯过程中单调性的安全探索的优势

    Benefits of Monotonicity in Safe Exploration with Gaussian Processes. (arXiv:2211.01561v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.01561](http://arxiv.org/abs/2211.01561)

    本文提出了一种名为单调安全UCB(M-SafeUCB)的算法，通过单调性假设，取得了在保证精度的同时显著的优势。

    

    本文考虑了在相应的安全阈值下顺序地寻找未知函数的最大值的问题。我们使用基于核的和高斯过程方法建模函数，但假设该函数相对于“安全变量”是单调递增的，这与以前的工作不同。此假设受到了各种实际应用的启发，例如自适应临床试验设计和机器人学。我们从GP-UCB和SafeOpt算法中汲取灵感，提出了一种名为单调安全UCB(M-SafeUCB)的算法来完成这个任务。我们证明了M-SafeUCB在安全性、适当定义的后悔概念和近似找到整个安全边界方面具有理论保证。此外，我们说明，单调性假设在保证准确性的同时也具有显著优势。

    We consider the problem of sequentially maximising an unknown function over a set of actions while ensuring that every sampled point has a function value below a given safety threshold. We model the function using kernel-based and Gaussian process methods, while differing from previous works in our assumption that the function is monotonically increasing with respect to a \emph{safety variable}. This assumption is motivated by various practical applications such as adaptive clinical trial design and robotics. Taking inspiration from the \textsc{\sffamily GP-UCB} and \textsc{\sffamily SafeOpt} algorithms, we propose an algorithm, monotone safe {\sffamily UCB} (\textsc{\sffamily M-SafeUCB}) for this task. We show that \textsc{\sffamily M-SafeUCB} enjoys theoretical guarantees in terms of safety, a suitably-defined regret notion, and approximately finding the entire safe boundary. In addition, we illustrate that the monotonicity assumption yields significant benefits in terms of the guara
    
[^161]: 在子模最大化中平衡效用和公平性（技术报告）

    Balancing Utility and Fairness in Submodular Maximization (Technical Report). (arXiv:2211.00980v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2211.00980](http://arxiv.org/abs/2211.00980)

    本文提出了一个新的问题，称为“二标准子模最大化”，以平衡效用和公平性。该问题要求找到一个固定大小的解，以最大化效用函数为目标。

    

    子模函数最大化是一个基本的组合优化问题，具有许多应用，包括数据汇总、影响力最大化和推荐等。在许多问题中，目标是找到一解，使得对于每个用户，效用函数是单调子模的情况下，平均效用最大化。然而，当用户群体由几个人口统计学分组组成时，另一个关键问题是效用是否公平地分配在不同的群体中。虽然效用和公平目标都是可取的，但它们可能互相矛盾，并且据我们所知，很少有人关注如何一起优化它们。在本文中，我们提出了一个新问题，称为“二标准子模最大化”（BSM），以在效用和公平性之间取得平衡，具体而言，它要求找到一个固定大小的解，以最大化效用函数为目标。

    Submodular function maximization is a fundamental combinatorial optimization problem with plenty of applications -- including data summarization, influence maximization, and recommendation. In many of these problems, the goal is to find a solution that maximizes the average utility over all users, for each of whom the utility is defined by a monotone submodular function. However, when the population of users is composed of several demographic groups, another critical problem is whether the utility is fairly distributed across different groups. Although the \emph{utility} and \emph{fairness} objectives are both desirable, they might contradict each other, and, to the best of our knowledge, little attention has been paid to optimizing them jointly.  In this paper, we propose a new problem called \emph{Bicriteria Submodular Maximization} (BSM) to strike a balance between utility and fairness. Specifically, it requires finding a fixed-size solution to maximize the utility function, subject
    
[^162]: 终身赌博优化：无先验知识和无后悔算法

    Lifelong Bandit Optimization: No Prior and No Regret. (arXiv:2210.15513v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.15513](http://arxiv.org/abs/2210.15513)

    本文提出了一种算法LIBO，可以无需直接访问数据，对一系列赌博优化任务进行学习和适应，并保证最优性能和亚线性终身后悔率。

    

    机器学习算法经常重复应用于相似结构的问题。本文关注解决一系列赌博优化任务，并开发了一种适应环境的算法LIBO。我们假设内核化结构，其中的内核在所有任务中都是未知的但共享的。LIBO依次元学习一个逼近真实内核的内核，然后用最新的内核估计来解决即将到来的任务。本算法可以与任何内核化或线性赌博算法配对，并保证最优的预期性能。如果与亚线性赌博算法配对，LIBO将产生一个亚线性终身后悔率。

    Machine learning algorithms are often repeatedly applied to problems with similar structure over and over again. We focus on solving a sequence of bandit optimization tasks and develop LIBO, an algorithm which adapts to the environment by learning from past experience and becomes more sample-efficient in the process. We assume a kernelized structure where the kernel is unknown but shared across all tasks. LIBO sequentially meta-learns a kernel that approximates the true kernel and solves the incoming tasks with the latest kernel estimate. Our algorithm can be paired with any kernelized or linear bandit algorithm and guarantees oracle optimal performance, meaning that as more tasks are solved, the regret of LIBO on each task converges to the regret of the bandit algorithm with oracle knowledge of the true kernel. Naturally, if paired with a sublinear bandit algorithm, LIBO yields a sublinear lifelong regret. We also show that direct access to the data from each task is not necessary for
    
[^163]: 论数据集推理的鲁棒性

    On the Robustness of Dataset Inference. (arXiv:2210.13631v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13631](http://arxiv.org/abs/2210.13631)

    机器学习模型的知识产权需要保护，数据集推理技术(DI)提供更好的鲁棒性和效率，但在某些情况下存在高假阳性。

    

    机器学习(ML)模型的训练成本高，需要大量的数据、计算资源和专业技术。因此，它们构成了需要保护免受盗用的有价值的知识产权。所有权验证技术允许模型被盗用攻击的受害者证明一个嫌疑模型实际上是从他们那里被盗。虽然已经提出了一些基于数字水印或指纹技术的所有权验证技术，但它们中的大多数在安全保障(装备完备的对手可以逃避验证)或计算成本方面都存在缺陷。指纹技术“数据集推理(DI)”已被证明比之前的方法具有更好的鲁棒性和效率。DI的作者为线性(嫌疑)模型提供了一个正确性证明。然而，在相同设置的子空间中，我们证明DI存在高假阳性(FPs)——它可能会错误地识别模型盗用行为。

    Machine learning (ML) models are costly to train as they can require a significant amount of data, computational resources and technical expertise. Thus, they constitute valuable intellectual property that needs protection from adversaries wanting to steal them. Ownership verification techniques allow the victims of model stealing attacks to demonstrate that a suspect model was in fact stolen from theirs.  Although a number of ownership verification techniques based on watermarking or fingerprinting have been proposed, most of them fall short either in terms of security guarantees (well-equipped adversaries can evade verification) or computational cost. A fingerprinting technique, Dataset Inference (DI), has been shown to offer better robustness and efficiency than prior methods.  The authors of DI provided a correctness proof for linear (suspect) models. However, in a subspace of the same setting, we prove that DI suffers from high false positives (FPs) -- it can incorrectly identify 
    
[^164]: 深度多分支卷积神经网络结构用于基于脑MRIs的早期阿尔茨海默病检测

    Deep Multi-Branch CNN Architecture for Early Alzheimer's Detection from Brain MRIs. (arXiv:2210.12331v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2210.12331](http://arxiv.org/abs/2210.12331)

    本论文提出了一种基于脑MRIs的早期阿尔茨海默病检测方法，使用深度多分支卷积神经网络结构并达到了99.05%的三分类准确率。

    

    阿尔茨海默病是一种神经退行性疾病，可能导致痴呆和严重的大脑功能下降，如果没有预防性护理，会抑制简单任务的完成。逾9分之1的美国人患有由AD引起的痴呆症，未得到报酬的AD相关痴呆患者护理价值为2716亿美元。因此，为防止AD进一步进展，已开发了各种方法进行早期AD诊断。在本文中，我们首先回顾了其他可用于早期AD检测的方法。然后我们介绍了我们的数据集，该数据集来自阿尔茨海默病神经影像学倡议（ADNI），并提出了一个由7866819个参数组成的深度卷积神经网络架构。该模型具有三个不同的卷积分支，每个分支的长度不同。每个分支由不同的卷积核大小组成。该模型可以预测患有非痴呆、轻度痴呆或中度痴呆的患者，其三分类准确率为99.05％。

    Alzheimer's disease (AD) is a neuro-degenerative disease that can cause dementia and result severe reduction in brain function inhibiting simple tasks especially if no preventative care is taken. Over 1 in 9 Americans suffer from AD induced dementia and unpaid care for people with AD related dementia is valued at $271.6 billion. Hence, various approaches have been developed for early AD diagnosis to prevent its further progression. In this paper, we first review other approaches that could be used for early detection of AD. We then give an overview of our dataset that was from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and propose a deep Convolutional Neural Network (CNN) architecture consisting of 7,866,819 parameters. This model has three different convolutional branches with each having a different length. Each branch is comprised of different kernel sizes. This model can predict whether a patient is non-demented, mild-demented, or moderately demented with a 99.05% three
    
[^165]: 图形化结构扩散模型

    Graphically Structured Diffusion Models. (arXiv:2210.11633v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.11633](http://arxiv.org/abs/2210.11633)

    本文提出了一种自动定义和学习具有问题特定结构的深度生成模型的框架，通过训练具有特定于问题规范的体系结构的扩散模型，优化了问题维度与模型性能之间的缩放关系。

    

    我们引入了一种自动定义和学习具有问题特定结构的深度生成模型的框架。我们解决的问题域更多地采用算法来解决，如排序，数独的限制满足和矩阵因式分解。具体而言，我们训练具有特定于问题规范的体系结构的扩散模型。这个问题规范应该包含描述变量之间关系的图形模型，并且通常受益于子计算的明确表示。置换不变性也可以被利用。在各种各样的实验中，我们提高了问题维度与我们模型性能之间的缩放关系，无论是在训练时间还是最终准确性方面。我们的代码可以在https://github.com/plai-group/gsdm找到。

    We introduce a framework for automatically defining and learning deep generative models with problem-specific structure. We tackle problem domains that are more traditionally solved by algorithms such as sorting, constraint satisfaction for Sudoku, and matrix factorization. Concretely, we train diffusion models with an architecture tailored to the problem specification. This problem specification should contain a graphical model describing relationships between variables, and often benefits from explicit representation of subcomputations. Permutation invariances can also be exploited. Across a diverse set of experiments we improve the scaling relationship between problem dimension and our model's performance, in terms of both training time and final accuracy. Our code can be found at https://github.com/plai-group/gsdm.
    
[^166]: 探究分布变化的解释方法

    Towards Explaining Distribution Shifts. (arXiv:2210.10275v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10275](http://arxiv.org/abs/2210.10275)

    本文介绍了一种通过可解释的转运图来解释分布变化的方法，以帮助人工进行减轻影响的任务。通过许多现实数据集的例子，展示了解释性映射提供了更好的详细性和可解释性之间的平衡。

    

    分布的改变可能会对下游模型的准确性产生重大影响，因此理解分布的变化对于研究和减轻其影响至关重要。大多数先前的工作仅关注于检测是否发生了分布变化，并假设任何检测到的变化均可以被运营者正确地理解和处理。本文旨在通过可解释的转运图来解释分布变化，从而帮助人工进行减轻影响的任务。我们将可解释的映射从最优输运的松弛中推导出来，在候选映射限制为可解释映射的集合中。我们检查了多个现实中表格、文本和图像数据集中的分布变化，并展示了我们的解释性映射如何在详细性和可解释性之间提供更好的平衡。

    A distribution shift can have fundamental consequences such as signaling a change in the operating environment or significantly reducing the accuracy of downstream models. Thus, understanding distribution shifts is critical for examining and hopefully mitigating the effect of such a shift. Most prior work focuses on merely detecting if a shift has occurred and assumes any detected shift can be understood and handled appropriately by a human operator. We hope to aid in these manual mitigation tasks by explaining the distribution shift using interpretable transportation maps from the original distribution to the shifted one. We derive our interpretable mappings from a relaxation of optimal transport, where the candidate mappings are restricted to a set of interpretable mappings. We then inspect multiple quintessential use-cases of distribution shift in real-world tabular, text, and image datasets to showcase how our explanatory mappings provide a better balance between detail and interpr
    
[^167]: 数据集构建的偏见：社会偏见基准的问题

    The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks. (arXiv:2210.10040v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10040](http://arxiv.org/abs/2210.10040)

    研究揭示了社会偏见基准中数据集构建偏见可能对结果造成了重要影响，需要更严谨的社会偏见度量方法。

    

    我们能否可靠地相信从社会偏见基准得到的分数是对给定语言模型中存在的问题社会偏见的忠实指标？本文通过将社会偏见与来源于数据集构建过程中的非社会偏见进行对比研究这一问题。为此，我们根据无害的修改（如释义或随机抽样）实际模拟了给定基准的各种替代结构，这些修改保持其社会偏见的本质。在两个众所周知的社会偏见基准（Winogender和BiasNLI）中，我们观察到这些浅显的修改对各种模型中导致的偏见程度产生了惊人的影响。我们希望这些令人不安的观察结果能够激发更严谨的社会偏见度量方法。

    How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given language model? In this work, we study this question by contrasting social biases with non-social biases stemming from choices made during dataset construction that might not even be discernible to the human eye. To do so, we empirically simulate various alternative constructions for a given benchmark based on innocuous modifications (such as paraphrasing or random-sampling) that maintain the essence of their social bias. On two well-known social bias benchmarks (Winogender and BiasNLI) we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models. We hope these troubling observations motivate more robust measures of social biases.
    
[^168]: PromptCast：一种新的基于提示的时间序列预测范式

    PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.08964](http://arxiv.org/abs/2210.08964)

    提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast），将数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，可以直接应用于语言模型。

    

    本文提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast）。在这种新的任务中，将原来的数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，使得语言模型可以直接应用于预测的目的。为了支持和促进这个任务的研究，我们还提出了一个大规模的数据集（PISA）。

    This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
    
[^169]: 预训练语言模型在抗体序列填充中的应用

    Reprogramming Pretrained Language Models for Antibody Sequence Infilling. (arXiv:2210.07144v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2210.07144](http://arxiv.org/abs/2210.07144)

    该篇论文利用模型重编程技术，将基于自然语言的预训练模型重新用于抗体CDR序列推断任务中，成功提高了生成的序列多样性和新颖性，并保持了高质量的结构一致性。

    

    抗体是最多才多艺的结合分子，应用于生物医学领域。计算机设计抗体需要生成新颖多样的序列，同时保持结构一致性。设计互补决定区域（CDR）是抗体所特有的挑战，这决定了其抗原结合亲和力和特异性。最近的深度学习模型表现出了惊人的成果，然而已知的抗体序列/结构对的数量有限，常常导致性能降低，特别是在生成序列方面缺乏多样性。我们的工作通过利用模型重编程（MR），将预训练模型重新用于适应一个不同的语言任务，它们的数据稀缺，因此从头训练高性能模型或有效地微调基于特定任务的预训练模型可能会很困难。具体而言，我们重新编程了BERT和GPT-2模型，这些模型是基于自然语言数据进行预训练的，用于建立抗体的CDR序列。我们展示了我们重新编程的模型在维持高质量CDR序列结果的同时，在多样性和新颖性方面优于之前的最先进方法。

    Antibodies comprise the most versatile class of binding molecules, with numerous applications in biomedicine. Computational design of antibodies involves generating novel and diverse sequences, while maintaining structural consistency. Unique to antibodies, designing the complementarity-determining region (CDR), which determines the antigen binding affinity and specificity, creates its own unique challenges. Recent deep learning models have shown impressive results, however the limited number of known antibody sequence/structure pairs frequently leads to degraded performance, particularly lacking diversity in the generated sequences. In our work we address this challenge by leveraging Model Reprogramming (MR), which repurposes pretrained models on a source language to adapt to the tasks that are in a different language and have scarce data - where it may be difficult to train a high-performing model from scratch or effectively fine-tune an existing pre-trained model on the specific tas
    
[^170]: 演员评论或评论演员？两个时间尺度的故事。

    Actor-Critic or Critic-Actor? A Tale of Two Time Scales. (arXiv:2210.04470v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04470](http://arxiv.org/abs/2210.04470)

    这篇论文提出了一种评论演员算法，它在快速和慢速时间尺度上计算价值函数和策略，该算法与演员评论算法在准确性和计算成本方面表现相当。

    

    我们重新审视基于表格的演员评论算法的标准公式，将其视为两个时间尺度的随机逼近，其中价值函数在快速时间尺度上计算，策略在慢速时间尺度上计算。这模拟了策略迭代。我们首先观察到，时间尺度的反转实际上会模拟值迭代，并且是一种合法的算法。我们提供了收敛性证明，并通过带有线性和非线性函数逼近器的函数逼近测试两种方法，并观察到我们提出的评论演员算法在准确性和计算成本方面与演员评论算法相当。

    We revisit the standard formulation of tabular actor-critic algorithm as a two time-scale stochastic approximation with value function computed on a faster time-scale and policy computed on a slower time-scale. This emulates policy iteration. We begin by observing that reversal of the time scales will in fact emulate value iteration and is a legitimate algorithm. We provide a proof of convergence and compare the two empirically with and without function approximation (with both linear and nonlinear function approximators) and observe that our proposed critic-actor algorithm performs on par with actor-critic in terms of both accuracy and computational effort.
    
[^171]: 用BSMS-GNN高效学习基于网格的物理仿真

    Efficient Learning of Mesh-Based Physical Simulation with BSMS-GNN. (arXiv:2210.02573v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02573](http://arxiv.org/abs/2210.02573)

    该论文提出了一种名为bi-stride的新型池化策略，利用二部图决策实现高效的基于网格的物理仿真，无需手动绘制粗网格，并避免了空间接近性带来的错误边缘。

    

    基于平面图神经网络（GNNs）和堆叠的消息传递（MPs）学习大规模网格上的物理仿真具有与节点数量相关的缩放复杂度和过度平滑的挑战。引入“多尺度”结构到GNNs以进行物理仿真引起了社区的越来越多的兴趣。然而，当前最先进的方法受到依赖于繁琐的绘制粗网格或基于空间邻近性构建粗略级别的限制，这可能在几何边界处引入错误的边缘。受到二部图决策的启发，我们提出了一种新的汇集策略——双步跨（bi-stride），以解决上述限制。当进行广度优先搜索（BFS）时，双步跨在每个其他前沿的节点上进行池化，而无需手动绘制较粗的网格，并通过空间邻近性避免错误的边缘。此外，它还实现了每层的单一MP方案和无参数的池化。

    Learning the physical simulation on large-scale meshes with flat Graph Neural Networks (GNNs) and stacking Message Passings (MPs) is challenging due to the scaling complexity w.r.t. the number of nodes and over-smoothing. There has been growing interest in the community to introduce \textit{multi-scale} structures to GNNs for physical simulation. However, current state-of-the-art methods are limited by their reliance on the labor-intensive drawing of coarser meshes or building coarser levels based on spatial proximity, which can introduce wrong edges across geometry boundaries. Inspired by the bipartite graph determination, we propose a novel pooling strategy, \textit{bi-stride} to tackle the aforementioned limitations. Bi-stride pools nodes on every other frontier of the breadth-first search (BFS), without the need for the manual drawing of coarser meshes and avoiding the wrong edges by spatial proximity. Additionally, it enables a one-MP scheme per level and non-parametrized pooling 
    
[^172]: 基于加权损失的类别不平衡互补标签学习

    Class-Imbalanced Complementary-Label Learning via Weighted Loss. (arXiv:2209.14189v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14189](http://arxiv.org/abs/2209.14189)

    该论文提出了一种名为加权互补标签学习的新型CLL方法，能够解决现实世界数据集中的类别不平衡训练样本问题，通过利用类别不平衡的互补标签建模加权的经验风险最小化损失，也适用于多类别不平衡的训练样本，并在实验中展示了其比其他最先进的方法更好的性能。

    

    互补标签学习 (CLL) 在弱监督分类中得到广泛应用，但在现实世界数据集中面临类别不平衡训练样本的显著挑战。在这种情况下，一个类别中的样本数量比其他类别要少得多，这导致预测准确率下降。不幸的是，现有的CLL方法没有研究解决这个问题。为了缓解这一挑战，我们提出了一种新的问题设置，它能够对多类分类进行类别不平衡互补标签学习。为了解决这个问题，我们提出了一种名为加权互补标签学习 (WCLL) 的新型CLL方法。该方法通过利用类别不平衡的互补标签来建模加权的经验风险最小化损失，也适用于多类别不平衡的训练样本。此外，我们推导了一个估计误差界来提供理论保证。为了评估所提出的方法，我们在三个具有不同程度类别不平衡的公共数据集上进行实验。实验结果表明，我们提出的WCLL方法相比其他最先进的方法具有更好的性能。

    Complementary-label learning (CLL) is widely used in weakly supervised classification, but it faces a significant challenge in real-world datasets when confronted with class-imbalanced training samples. In such scenarios, the number of samples in one class is considerably lower than in other classes, which consequently leads to a decline in the accuracy of predictions. Unfortunately, existing CLL approaches have not investigate this problem. To alleviate this challenge, we propose a novel problem setting that enables learning from class-imbalanced complementary labels for multi-class classification. To tackle this problem, we propose a novel CLL approach called Weighted Complementary-Label Learning (WCLL). The proposed method models a weighted empirical risk minimization loss by utilizing the class-imbalanced complementary labels, which is also applicable to multi-class imbalanced training samples. Furthermore, we derive an estimation error bound to provide theoretical assurance. To ev
    
[^173]: 公平性概念及其相关张力研究综述

    Survey on Fairness Notions and Related Tensions. (arXiv:2209.13012v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2209.13012](http://arxiv.org/abs/2209.13012)

    本文调查了公平性的不同概念以及它们与其他期望属性的紧张关系，并介绍了处理公平性-准确性权衡问题的不同方法。

    

    自动决策系统越来越多地用于解决招聘和贷款等涉及重大决策的问题，希望用机器学习算法代替主观人为决策。然而，基于机器学习的决策系统容易出现偏见，导致不公平的决策。文献中定义了几种公平性概念以捕捉这个伦理和社会概念的不同微妙之处（例如统计平等、机会平等等）。在学习模型时需要满足公平性要求，这产生了不同的公平概念之间以及隐私和分类准确性等其他期望属性之间的紧张关系。本文概述了通常使用的公平性概念，并讨论了它们与隐私和准确性之间的张力。本综述介绍了解决公平性与准确性权衡问题的不同方法（分为预处理、处理中、后处理和混合四种方法）。

    Automated decision systems are increasingly used to take consequential decisions in problems such as job hiring and loan granting with the hope of replacing subjective human decisions with objective machine learning (ML) algorithms. However, ML-based decision systems are prone to bias, which results in yet unfair decisions. Several notions of fairness have been defined in the literature to capture the different subtleties of this ethical and social concept (e.g., statistical parity, equal opportunity, etc.). Fairness requirements to be satisfied while learning models created several types of tensions among the different notions of fairness and other desirable properties such as privacy and classification accuracy. This paper surveys the commonly used fairness notions and discusses the tensions among them with privacy and accuracy. Different methods to address the fairness-accuracy trade-off (classified into four approaches, namely, pre-processing, in-processing, post-processing, and hy
    
[^174]: 利用物理一致性进行自监督学习的全息图重建

    Self-supervised learning of hologram reconstruction using physics consistency. (arXiv:2209.08288v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.08288](http://arxiv.org/abs/2209.08288)

    本文报道了一种名为GedankenNet的自监督学习模型，可以在不需要标记或实验数据的情况下，利用物理一致性损失和合成的人工随机图像进行全息图重建，并展示了其在各种样品类型上的有效性和优越的泛化能力。

    

    在过去的十年里，深度学习在各种计算成像、传感和显微镜任务中得到了广泛的应用。由于使用的是监督学习方案，这些方法大多依赖于大规模的、多样化的和标记的训练数据。这些训练图像数据集的获取和准备通常费时费力，也容易导致有偏估计和对新样本类型的有限泛化。本文报道了一个名为GedankenNet的自监督学习模型，可以消除标记或实验训练数据的需要，并展示了其在全息图重建任务上的有效性和优越的泛化能力。在没有先验了解要成像的样品类型的情况下，该自监督学习模型使用物理一致性损失和人工随机图像进行训练，这些图像是合成的，没有任何实验或类似于实际样品的外观特点。经过自监督训练后，GedankenNet成功地重建了各种样品的高质量全息图像，包括定量相位和振幅信息，展示了其在实际应用中的巨大潜力。

    The past decade has witnessed transformative applications of deep learning in various computational imaging, sensing and microscopy tasks. Due to the supervised learning schemes employed, these methods mostly depend on large-scale, diverse, and labeled training data. The acquisition and preparation of such training image datasets are often laborious and costly, also leading to biased estimation and limited generalization to new sample types. Here, we report a self-supervised learning model, termed GedankenNet, that eliminates the need for labeled or experimental training data, and demonstrate its effectiveness and superior generalization on hologram reconstruction tasks. Without prior knowledge about the sample types to be imaged, the self-supervised learning model was trained using a physics-consistency loss and artificial random images that are synthetically generated without any experiments or resemblance to real-world samples. After its self-supervised training, GedankenNet success
    
[^175]: 针对干预密度估计的正则化流

    Normalizing Flows for Interventional Density Estimation. (arXiv:2209.06203v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06203](http://arxiv.org/abs/2209.06203)

    本研究提出了一种名为干预正则化流的全参数深度学习方法，用于从观测数据中估计干预后的潜在结果密度。该方法结合了nuisance flow和target flow，并开发了一个易于处理的优化目标，以实现有效和双重稳健的估计。实验证明，该方法优于现有最先进的方法。

    

    现有机器学习方法针对因果推断通常通过潜在结果的均值（例如平均处理效应）来计算数量。然而，这些数量并不能完全捕捉潜在结果分布的全部信息。本研究旨在从观测数据中估计干预后的潜在结果密度。为此，我们提出了一种新的全参数深度学习方法，称为干预正则化流。具体而言，我们组合了两种正则化流，即（i）用于估计干扰参数的nuisance flow和（ii）用于参数化估计潜在结果密度的target flow。我们进一步基于单步偏差校正开发了一个易于处理的优化目标，以有效和双重稳健的方式估计目标流参数。因此，我们的干预正则化流提供了一个正确归一化的密度估计器。在各种实验中，我们展示了我们的干预正则化流方法优于现有的用于从观测数据中估计潜在结果密度的最先进的方法。

    Existing machine learning methods for causal inference usually estimate quantities expressed via the mean of potential outcomes (e.g., average treatment effect). However, such quantities do not capture the full information about the distribution of potential outcomes. In this work, we estimate the density of potential outcomes after interventions from observational data. For this, we propose a novel, fully-parametric deep learning method called Interventional Normalizing Flows. Specifically, we combine two normalizing flows, namely (i) a nuisance flow for estimating nuisance parameters and (ii) a target flow for a parametric estimation of the density of potential outcomes. We further develop a tractable optimization objective based on a one-step bias correction for an efficient and doubly robust estimation of the target flow parameters. As a result our Interventional Normalizing Flows offer a properly normalized density estimator. Across various experiments, we demonstrate that our Int
    
[^176]: 桥接差距：使用差分隐私可变等变深度学习进行医学图像分析

    Bridging the Gap: Differentially Private Equivariant Deep Learning for Medical Image Analysis. (arXiv:2209.04338v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.04338](http://arxiv.org/abs/2209.04338)

    本文提出使用差分隐私保护的可变等变卷积网络进行医学图像分析，通过改进特征质量和参数效率缩小了隐私-效用差距，带来了显著的准确度提高。

    

    使用形式隐私保护技术（如差分隐私）进行机器学习可从敏感医学图像数据中提取有价值的见解，同时承诺保护患者隐私，但通常会产生尖锐的隐私-效用权衡。本文提出使用可变等变卷积网络进行带有差分隐私保护的医学图像分析。它们改进的特征质量和参数效率带来了显著的准确度提高，缩小了隐私-效用差距。

    Machine learning with formal privacy-preserving techniques like Differential Privacy (DP) allows one to derive valuable insights from sensitive medical imaging data while promising to protect patient privacy, but it usually comes at a sharp privacy-utility trade-off. In this work, we propose to use steerable equivariant convolutional networks for medical image analysis with DP. Their improved feature quality and parameter efficiency yield remarkable accuracy gains, narrowing the privacy-utility gap.
    
[^177]: SE(3)-DiffusionFields: 通过扩散学习平滑的联合抓取和运动优化成本函数

    SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion. (arXiv:2209.03855v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.03855](http://arxiv.org/abs/2209.03855)

    这篇论文介绍了一种通过扩散模型学习数据驱动的SE(3)成本函数的方法，该方法可以与其他成本无缝集成到单个可微分的目标函数中，从而实现联合抓取和运动优化。

    

    多目标优化问题在机器人领域中十分普遍，如机器人操纵任务的优化需要联合考虑抓取姿态配置、碰撞和关节限制等因素。虽然一些需求可以轻松手工设计，例如轨迹的平滑性，但是一些任务特定的目标需要从数据中学习得到。本文介绍了一种通过扩散模型学习数据驱动的SE(3)成本函数的方法。扩散模型可以表示高度表现力的多模态分布，并由于其得分匹配训练目标而在整个空间内显示出适当的梯度。将成本函数学习为扩散模型可以将其与其他成本无缝集成到单个可微分的目标函数中，实现联合梯度优化。在本文中，我们着重研究了学习6DoF抓取的SE(3)扩散模型，从而产生了一种新的框架，无需解耦抓取选择和运动计划即可进行联合抓取和运动优化。

    Multi-objective optimization problems are ubiquitous in robotics, e.g., the optimization of a robot manipulation task requires a joint consideration of grasp pose configurations, collisions and joint limits. While some demands can be easily hand-designed, e.g., the smoothness of a trajectory, several task-specific objectives need to be learned from data. This work introduces a method for learning data-driven SE(3) cost functions as diffusion models. Diffusion models can represent highly-expressive multimodal distributions and exhibit proper gradients over the entire space due to their score-matching training objective. Learning costs as diffusion models allows their seamless integration with other costs into a single differentiable objective function, enabling joint gradient-based motion optimization. In this work, we focus on learning SE(3) diffusion models for 6DoF grasping, giving rise to a novel framework for joint grasp and motion optimization without needing to decouple grasp sel
    
[^178]: TFN：基于时频转换的可解释神经网络用于智能故障诊断

    TFN: An Interpretable Neural Network with Time-Frequency Transform Embedded for Intelligent Fault Diagnosis. (arXiv:2209.01992v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.01992](http://arxiv.org/abs/2209.01992)

    提出一种新的可解释神经网络模型——时频网络（TFN）。在传统卷积层中嵌入了物理上有意义的时频变换（TFT）方法作为自适应预处理层，该层不仅提高了诊断性能，还使得网络结构可以可解释，故障诊断过程透明。

    

    卷积神经网络（CNN）因其强大的特征提取和分类能力而广泛应用于机械系统的故障诊断。然而，CNN是一种典型的黑匣子模型，其决策机制不清晰，这限制了它在高可靠性的故障诊断场景中的应用。为解决这个问题，我们提出了一种新的可解释神经网络，称为时频网络（TFN），其中物理上有意义的时频变换（TFT）方法嵌入到传统卷积层中作为自适应预处理层。这个预处理层称为时频卷积（TFconv）层，受到一个精心设计的核函数的限制，用于提取与故障相关的时频信息。它不仅提高了诊断性能，而且揭示了CNN在频率域内预测的逻辑基础。不同的TFT方法对应着TFconv层的不同核函数，使网络结构可解释，故障诊断过程透明。轴承故障数据集和齿轮箱振动数据集上的实验结果表明，所提出的TFN相对于现有的基于CNN的故障诊断方法具有更优的诊断性能，而提取的时频特征可以有效地揭示故障频率特征和突出潜在故障源。

    Convolutional Neural Networks (CNNs) are widely used in fault diagnosis of mechanical systems due to their powerful feature extraction and classification capabilities. However, the CNN is a typical black-box model, and the mechanism of CNN's decision-making are not clear, which limits its application in high-reliability-required fault diagnosis scenarios. To tackle this issue, we propose a novel interpretable neural network termed as Time-Frequency Network (TFN), where the physically meaningful time-frequency transform (TFT) method is embedded into the traditional convolutional layer as an adaptive preprocessing layer. This preprocessing layer named as time-frequency convolutional (TFconv) layer, is constrained by a well-designed kernel function to extract fault-related time-frequency information. It not only improves the diagnostic performance but also reveals the logical foundation of the CNN prediction in the frequency domain. Different TFT methods correspond to different kernel fun
    
[^179]: 零标签的多元时间序列异常检测

    Detecting Multivariate Time Series Anomalies with Zero Known Label. (arXiv:2208.02108v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.02108](http://arxiv.org/abs/2208.02108)

    本文提出了一种利用动态图和实体注意力机制实现零标签多元时间序列异常检测的方法，其利用密度估计比较异常和正常实例，性能优于多种无监督方法和半监督方法。

    

    多元时间序列异常检测已在半监督环境下 extensively studied，但需要训练集中的所有正常样本，这很费力。因此，本文提出了一种无监督的异常检测方法，MTGFlow，基于一个广泛的假设，即异常实例的密度比正常实例稀疏。MTGFlow 建立动态图捕捉实体间的交互，并利用实体注意机制进行密度和异常分数建模，不仅能有效解决实体特征多样性和密度估计问题，而且在多个基准数据集上的实验结果表明，MTGFlow 比几种先进的无监督方法优越，并即使与半监督方法相比也有竞争力。

    Multivariate time series anomaly detection has been extensively studied under the semi-supervised setting, where a training dataset with all normal instances is required. However, preparing such a dataset is very laborious since each single data instance should be fully guaranteed to be normal. It is, therefore, desired to explore multivariate time series anomaly detection methods based on the dataset without any label knowledge. In this paper, we propose MTGFlow, an unsupervised anomaly detection approach for multivariate time series anomaly detection via dynamic graph and entity-aware normalizing flow, leaning only on a widely accepted hypothesis that abnormal instances exhibit sparse densities than the normal. However, the complex interdependencies among entities and the diverse inherent characteristics of each entity pose significant challenges on the density estimation, let alone to detect anomalies based on the estimated possibility distribution. To tackle these problems, we prop
    
[^180]: 图节点注入攻击的对抗伪装

    Adversarial Camouflage for Node Injection Attack on Graphs. (arXiv:2208.01819v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.01819](http://arxiv.org/abs/2208.01819)

    本文提出了一种名为CANA的对抗伪装框架，可以在节点注入攻击中使被注入节点看起来正常，并提高在实际场景下防御/检测方法的攻击性能。

    

    图神经网络所受到的节点注入攻击（Node injection attacks）近年来引起了人们的重视，因为这种攻击具有很高的攻击成功率，并能显著降低图神经网络的性能。但是，我们的研究表明，这些攻击在实际场景中经常失败，因为防御/检测方法可以轻松识别和删除被注入的节点。为了解决这个问题，我们致力于进行节点注入攻击的伪装，使被注入的节点看起来正常，并且对于防御/检测方法是不可感知的。然而，图数据的非欧几里德性质和缺乏直观先验知识为伪装的形式化、实现和评估带来了巨大的挑战。在本文中，我们首先提出并定义了伪装作为注入节点和正常节点的邻域网络之间的分布相似性。然后，针对实现，我们提出了一种针对节点注入攻击的对抗伪装框架，即CANA，以提高在实际场景下防御/检测方法的攻击性能。

    Node injection attacks on Graph Neural Networks (GNNs) have received emerging attention due to their potential to significantly degrade GNN performance with high attack success rates. However, our study indicates these attacks often fail in practical scenarios, since defense/detection methods can easily identify and remove the injected nodes. To address this, we devote to camouflage node injection attack, making injected nodes appear normal and imperceptible to defense/detection methods. Unfortunately, the non-Euclidean nature of graph data and lack of intuitive prior present great challenges to the formalization, implementation, and evaluation of camouflage. In this paper, we first propose and define camouflage as distribution similarity between ego networks of injected nodes and normal nodes. Then for implementation, we propose an adversarial CAmouflage framework for Node injection Attack, namely CANA, to improve attack performance under defense/detection methods in practical scenari
    
[^181]: 活在当下：适应性进化策略的学习动力学模型

    Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy. (arXiv:2207.12141v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.12141](http://arxiv.org/abs/2207.12141)

    本文提出了一种名为PDML的学习动力学模型的方法，该方法动态调整历史策略混合分布以适应训练过程中的进化策略，实验结果表明PDML优于最先进的方法，实现了更快的收敛和更好的最终性能。

    

    基于模型的强化学习通常比模型无关的强化学习在实践中具有更高的样本效率，因为它学习一个动力学模型来生成策略学习的样本。以前的研究学习使动力学模型适应所有历史策略下的经验状态-动作访问分布，即样本回放缓冲区。然而，在本文中，我们观察到，在“所有历史策略”下使动力学模型适应分布不一定有益于模型预测“当前策略”，因为正在使用的策略在时间上不断演变。训练过程中的进化策略会导致状态-动作访问分布的转移。我们在理论上分析了这种历史策略分布的转移如何影响模型学习和模型回滚。因此，我们提出了一种新颖的动力学模型学习方法，称为策略适应动力学模型学习(PDML)。PDML动态调整用于动力学模型学习的历史策略混合分布，以适应训练过程中的进化策略。实验结果表明，PDML优于最先进的方法，实现了更快的收敛和更好的最终性能。

    Model-based reinforcement learning (RL) often achieves higher sample efficiency in practice than model-free RL by learning a dynamics model to generate samples for policy learning. Previous works learn a dynamics model that fits under the empirical state-action visitation distribution for all historical policies, i.e., the sample replay buffer. However, in this paper, we observe that fitting the dynamics model under the distribution for \emph{all historical policies} does not necessarily benefit model prediction for the \emph{current policy} since the policy in use is constantly evolving over time. The evolving policy during training will cause state-action visitation distribution shifts. We theoretically analyze how this distribution shift over historical policies affects the model learning and model rollouts. We then propose a novel dynamics model learning method, named \textit{Policy-adapted Dynamics Model Learning (PDML)}. PDML dynamically adjusts the historical policy mixture dist
    
[^182]: 基于数据驱动方法提高软件漏洞评估理解的研究

    Towards an Improved Understanding of Software Vulnerability Assessment Using Data-Driven Approaches. (arXiv:2207.11708v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2207.11708](http://arxiv.org/abs/2207.11708)

    本文通过提供知识和自动化支持，基于数据驱动方法来推进软件安全领域中的软件漏洞评估。该论文提供了一套新颖的数据驱动技术和实用建议，适用于该领域的研究人员和实践者，以提高对真实软件系统中日益增长的漏洞的评估理解，从而实现对这些关键安全问题的更彻底和及时修复。

    

    本文通过提供知识和自动化支持，基于数据驱动方法来推进软件安全领域中的软件漏洞评估。软件漏洞评估提供了重要且多方面的信息，以预防和减轻野外危险的网络攻击。本论文的主要贡献包括知识体系化，一套新颖的数据驱动技术和实用建议，适用于该领域的研究人员和实践者。本论文的结果有助于提高对真实软件系统中日益增长的漏洞的评估理解，从而实现对这些关键安全问题的更彻底和及时修复。

    The thesis advances the field of software security by providing knowledge and automation support for software vulnerability assessment using data-driven approaches. Software vulnerability assessment provides important and multifaceted information to prevent and mitigate dangerous cyber-attacks in the wild. The key contributions include a systematisation of knowledge, along with a suite of novel data-driven techniques and practical recommendations for researchers and practitioners in the area. The thesis results help improve the understanding and inform the practice of assessing ever-increasing vulnerabilities in real-world software systems. This in turn enables more thorough and timely fixing prioritisation and planning of these critical security issues.
    
[^183]: 探测分类器对于概念去除和检测不可靠

    Probing Classifiers are Unreliable for Concept Removal and Detection. (arXiv:2207.04153v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04153](http://arxiv.org/abs/2207.04153)

    本文研究了在文本数据上神经网络模型中的不良概念去除。对于现有的后期和对抗性方法，本文理论和实证分析表明其依赖的探测分类器可能使用非概念特征，导致无法完全去除不需要的概念。我们提出了一种直接学习从模型的表示中去除概念的方法，实验结果表明其优于最先进的后期和对抗性方法。

    

    在文本数据上训练的神经网络模型被发现在其表示中编码了不良的语言或敏感概念，移除这些概念是不容易的，因为概念、文本输入和学习到的表示之间存在复杂的关系。最近的研究提出了后期和对抗性方法来从模型的表示中去除这些不需要的概念。通过广泛的理论和实证分析，我们表明这些方法可能是适得其反的：它们不能完全去除概念，而在最糟糕的情况下可能会破坏所有任务相关的特征。原因是这些方法依赖于一个探测分类器作为概念的代理。即使在概念相关特征在表示空间中就可以提供100%准确性的最有利条件下学习探测分类器，我们证明探测分类器很可能会使用非概念特征，因此后期或对抗性处理方法将不能完全去除不需要的概念。我们提出一种替代方法，通过在损失函数中训练正则化项来直接学习从模型表示中去除概念。我们的实验表明，这种方法在去除概念的同时保留任务相关特征方面优于最先进的后期和对抗性方法。

    Neural network models trained on text data have been found to encode undesirable linguistic or sensitive concepts in their representation. Removing such concepts is non-trivial because of a complex relationship between the concept, text input, and the learnt representation. Recent work has proposed post-hoc and adversarial methods to remove such unwanted concepts from a model's representation. Through an extensive theoretical and empirical analysis, we show that these methods can be counter-productive: they are unable to remove the concepts entirely, and in the worst case may end up destroying all task-relevant features. The reason is the methods' reliance on a probing classifier as a proxy for the concept. Even under the most favorable conditions for learning a probing classifier when a concept's relevant features in representation space alone can provide 100% accuracy, we prove that a probing classifier is likely to use non-concept features and thus post-hoc or adversarial methods wi
    
[^184]: 带宽使量子核模型具有普适性

    Bandwidth Enables Generalization in Quantum Kernel Models. (arXiv:2206.06686v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2206.06686](http://arxiv.org/abs/2206.06686)

    该论文研究了量子核模型的普适性问题，通过改变量子核带宽的值，能够使量子模型具备普适性，从而提高在实际问题中的速度表现。

    

    在某些特定的情况下，量子计算机已被证明在机器学习领域中具有超越经典状态的速度优势。例如，已经证明，量子核方法在离散对数问题的学习版本上提供指数级的加速。了解量子模型的普适性对于实现类似的问题速度优势至关重要。最近的研究结果表明，由于量子特征空间的指数级大小，普适性受到阻碍。尽管这些结果表明，当量子比特数很大时，量子模型无法普适化，但本文表明，这些结果基于过于严格的假设。我们通过改变称为量子核带宽的超参数来考虑更广泛的模型类。我们分析大量比特极限并提供了可以用闭合形式解决的量子模型的普适化的明确公式。具体来说，我们表明改变量子核带宽的值可以使量子模型普适化。

    Quantum computers are known to provide speedups over classical state-of-the-art machine learning methods in some specialized settings. For example, quantum kernel methods have been shown to provide an exponential speedup on a learning version of the discrete logarithm problem. Understanding the generalization of quantum models is essential to realizing similar speedups on problems of practical interest. Recent results demonstrate that generalization is hindered by the exponential size of the quantum feature space. Although these results suggest that quantum models cannot generalize when the number of qubits is large, in this paper we show that these results rely on overly restrictive assumptions. We consider a wider class of models by varying a hyperparameter that we call quantum kernel bandwidth. We analyze the large-qubit limit and provide explicit formulas for the generalization of a quantum model that can be solved in closed form. Specifically, we show that changing the value of th
    
[^185]: 基于金字塔神经网络的滞回行为模拟：原理、网络结构、案例研究和解释

    Hysteretic Behavior Simulation Based on Pyramid Neural Network:Principle, Network Architecture, Case Study and Explanation. (arXiv:2206.03990v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03990](http://arxiv.org/abs/2206.03990)

    本文提出了一种加权堆叠的金字塔神经网络架构，通过引入多级快捷方式建立金字塔结构，将特征直接集成到输出模块中，并提出了加权堆叠策略来增强特征融合方法。在滞回行为模拟中具有较优的效率和准确性。

    

    一种准确高效的材料与构件滞回行为模拟对结构分析至关重要。基于神经网络的代理模型在效率和准确性上具有显著潜力。本文提出一种加权堆叠的金字塔神经网络架构。该网络通过引入多级快捷方式建立金字塔结构，将特征直接集成到输出模块中。此外，还提出了加权堆叠策略来增强传统的特征融合方法。随后，将重新设计的架构与其他常用网络架构进行比较。结果表明，重新设计的架构在87.5%的情况下优于替代方案。同时，分析了不同基本网络架构的长短期记忆能力。

    An accurate and efficient simulation of the hysteretic behavior of materials and components is essential for structural analysis. The surrogate model based on neural networks shows significant potential in balancing efficiency and accuracy. However, its serial information flow and prediction based on single-level features adversely affect the network performance. Therefore, a weighted stacked pyramid neural network architecture is proposed herein. This network establishes a pyramid architecture by introducing multi-level shortcuts to integrate features directly in the output module. In addition, a weighted stacked strategy is proposed to enhance the conventional feature fusion method. Subsequently, the redesigned architectures are compared with other commonly used network architectures. Results show that the redesigned architectures outperform the alternatives in 87.5% of cases. Meanwhile, the long and short-term memory abilities of different basic network architectures are analyzed th
    
[^186]: 分解线性动态系统（dLDS）用于学习神经动力学的潜在成分

    Decomposed Linear Dynamical Systems (dLDS) for learning the latent components of neural dynamics. (arXiv:2206.02972v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.02972](http://arxiv.org/abs/2206.02972)

    该论文提出了一种新的分解动力系统模型，将复杂非平稳和非线性动态表示为简单、可解释的稀疏组合，并通过字典学习过程进行训练。

    

    在群体水平上学习神经动力学的可解释表示是理解观察到的神经活动如何与知觉和行为相关的关键第一步。神经动力学模型通常集中于神经活动的低维投影，或者学习与神经状态随时间明确相关的动力系统。通过将动力系统视为低维流的代表，我们讨论了这两种方法之间的相互关系。在此概念基础上，我们提出了一种新的分解动力系统模型，将时间序列数据的复杂非平稳和非线性动态表示为更简单、更可解释的成分的稀疏组合。我们的模型通过一个字典学习过程进行训练，其中我们利用了最近在跟踪稀疏向量随时间变化方面的结果。相较于以往的开关方法，在给定参数数量的情况下，我们的分解动态性质更为明显。

    Learning interpretable representations of neural dynamics at a population level is a crucial first step to understanding how observed neural activity relates to perception and behavior. Models of neural dynamics often focus on either low-dimensional projections of neural activity, or on learning dynamical systems that explicitly relate to the neural state over time. We discuss how these two approaches are interrelated by considering dynamical systems as representative of flows on a low-dimensional manifold. Building on this concept, we propose a new decomposed dynamical system model that represents complex non-stationary and nonlinear dynamics of time series data as a sparse combination of simpler, more interpretable components. Our model is trained through a dictionary learning procedure, where we leverage recent results in tracking sparse vectors over time. The decomposed nature of the dynamics is more expressive than previous switched approaches for a given number of parameters and 
    
[^187]: 基于可解释的深度强化学习的无人机引导和规划的鲁棒性对抗攻击检测

    Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning. (arXiv:2206.02670v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02670](http://arxiv.org/abs/2206.02670)

    本文提出了一种基于可解释的深度学习方法的创新性检测方案，以保护采用这些方法的无人机免受对抗攻击。

    

    针对无人机在公共领域遭受对抗攻击的风险增加的问题，本文提出了一种基于可解释的深度学习方法的创新性检测方案，以保护采用这些方法的无人机免受攻击。该方案采用深度强化学习进行引导和规划，利用人工势场来提高训练效率和障碍物避免率。

    The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agents operating in public are increasing. Adopting AI-based techniques and, more specifically, Deep Learning (DL) approaches to control and guide these UAVs can be beneficial in terms of performance but can add concerns regarding the safety of those techniques and their vulnerability against adversarial attacks. Confusion in the agent's decision-making process caused by these attacks can seriously affect the safety of the UAV. This paper proposes an innovative approach based on the explainability of DL methods to build an efficient detector that will protect these DL schemes and the UAVs adopting them from attacks. The agent adopts a Deep Reinforcement Learning (DRL) scheme for guidance and planning. The agent is trained with a Deep Deterministic Policy Gradient (DDPG) with Prioritised Experience Replay (PER) DRL scheme that utilises Artificial Potential Field (APF) to improve training times and obstacle avoidance per
    
[^188]: 深度学习用于皮肤病变分割的综述

    A Survey on Deep Learning for Skin Lesion Segmentation. (arXiv:2206.00356v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2206.00356](http://arxiv.org/abs/2206.00356)

    本综述调查了177篇深度学习分割皮肤病变的研究论文，探索了深度学习模型在皮肤病变分割中的适用性，并从多个方面对输入数据、模型设计和评估方面进行分析和讨论，提出了开放的挑战和未来的研究方向。

    

    皮肤癌是一个重大的公共卫生问题，计算机辅助诊断可降低这种常见疾病的负担。从图像中分割皮肤病变是实现这一目标的重要步骤。然而，天然和人为的伪影（例如，毛发和气泡）、内在因素（例如，病变形状和对比度）以及图像采集条件的变化使得皮肤病变分割成为具有挑战性的任务。最近，各种研究者探索了深度学习模型在皮肤病变分割中的适用性。本综述调查了177篇深度学习分割皮肤病变的研究论文。我们从输入数据（数据集、预处理和合成数据生成）、模型设计（架构、模块和损失）和评估方面（数据注释要求和分割性能）等多个方面对这些工作进行分析。我们从技术细节和临床相关性两个方面讨论这些维度，突出开放的挑战和未来的研究方向。

    Skin cancer is a major public health problem that could benefit from computer-aided diagnosis to reduce the burden of this common disease. Skin lesion segmentation from images is an important step toward achieving this goal. However, the presence of natural and artificial artifacts (e.g., hair and air bubbles), intrinsic factors (e.g., lesion shape and contrast), and variations in image acquisition conditions make skin lesion segmentation a challenging task. Recently, various researchers have explored the applicability of deep learning models to skin lesion segmentation. In this survey, we cross-examine 177 research papers that deal with deep learning-based segmentation of skin lesions. We analyze these works along several dimensions, including input data (datasets, preprocessing, and synthetic data generation), model design (architecture, modules, and losses), and evaluation aspects (data annotation requirements and segmentation performance). We discuss these dimensions both from the 
    
[^189]: VFed-SSD：面向实用的垂直联邦广告

    VFed-SSD: Towards Practical Vertical Federated Advertising. (arXiv:2205.15987v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15987](http://arxiv.org/abs/2205.15987)

    本文提出了一种半监督分裂知识蒸馏框架VFed-SSD，用于改善广告模型的学习效果。该框架利用未标记的相互重叠的数据，并通过分解联邦模型来在模型性能和推理效率之间保持平衡。实验结果在真实数据集上表明，VFed-SSD相对于其他最先进的垂直联合学习方法具有更高的预测准确性和推理效率。

    

    垂直联邦学习是一种新兴的安全学习范式，利用跨机构私有数据，旨在通过启用广告主和发布者私有拥有的互补用户属性的联合学习来改进广告模型。然而，将它应用到广告系统时存在两个关键挑战：a）标记重叠样本的有限规模，以及b）实时跨机构服务的高成本。为了缓解这两个限制，本文提出了一种半监督分裂知识蒸馏框架VFed-SSD。我们认为：i）在广告系统中有大量未标记的重叠数据可用，ii）通过分解联邦模型，我们可以在模型性能和推理成本之间保持平衡。

    As an emerging secure learning paradigm in lever-aging cross-agency private data, vertical federatedlearning (VFL) is expected to improve advertising models by enabling the joint learning of complementary user attributes privately owned by the advertiser and the publisher. However, there are two key challenges in applying it to advertising systems: a) the limited scale of labeled overlapping samples, and b) the high cost of real-time cross-agency serving. In this paper, we propose a semi-supervised split distillation framework VFed-SSD to alleviate the two limitations. We identify that: i)there are massive unlabeled overlapped data available in advertising systems, and ii) we can keep a balance between model performance and inference cost by decomposing the federated model. Specifically, we develop a self-supervised task MatchedPair Detection (MPD) to exploit the vertically partitioned unlabeled data and propose the Split Knowledge Distillation (SplitKD) schema to avoid cross-agency se
    
[^190]: 强健的马尔可夫决策过程即时学习

    Robust Anytime Learning of Markov Decision Processes. (arXiv:2205.15827v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.15827](http://arxiv.org/abs/2205.15827)

    文章介绍了一种将贝叶斯推理方案和计算强健策略相结合的、不断学习马尔可夫决策过程转移概率的方法，阐述了不确定MDP（uMDP）的概念，针对应用中有限数据导致的统计误差问题提出了基于不确定性集的解决方法，并介绍了计算强健策略以遵循形式规范的工具。

    

    马尔可夫决策过程（MDP）是经常用于顺序决策的形式模型。MDP通过转移函数中的概率捕获可能出现的来自不精确执行器的随机性。然而，在数据驱动的应用中，从（有限的）数据中推导出准确的概率会引入统计误差，可能导致意外或不良结果。不确定MDP（uMDP）不需要准确的概率，而是使用所谓的不确定性集，在转换中考虑这些有限的数据。我们通过组合专门的贝叶斯推理方案和计算强健策略的方法，在一个强健的即时学习方法中不断学习MDP的转移概率。特别地，我们的方法（1）近似p

    Markov decision processes (MDPs) are formal models commonly used in sequential decision-making. MDPs capture the stochasticity that may arise, for instance, from imprecise actuators via probabilities in the transition function. However, in data-driven applications, deriving precise probabilities from (limited) data introduces statistical errors that may lead to unexpected or undesirable outcomes. Uncertain MDPs (uMDPs) do not require precise probabilities but instead use so-called uncertainty sets in the transitions, accounting for such limited data. Tools from the formal verification community efficiently compute robust policies that provably adhere to formal specifications, like safety constraints, under the worst-case instance in the uncertainty set. We continuously learn the transition probabilities of an MDP in a robust anytime-learning approach that combines a dedicated Bayesian inference scheme with the computation of robust policies. In particular, our method (1) approximates p
    
[^191]: 物理激活函数（PAFs）：一种更有效地将物理引入PINNs的方法。

    Physical Activation Functions (PAFs): An Approach for More Efficient Induction of Physics into Physics-Informed Neural Networks (PINNs). (arXiv:2205.14630v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14630](http://arxiv.org/abs/2205.14630)

    介绍了一种物理激活函数（PAFs）的概念，它基于研究现象的物理定律的数学表示，而不是一般的激活函数，可以提高PINN的训练效率和优化物理模型的交错。

    

    近年来，通过物理信息神经网络（PINNs）的发展，试图填补深度学习（DL）方法和科学计算中的分析或数值方法之间的差距。然而，PINN的训练和物理模型的最佳交错仍然存在许多复杂问题。在这里，我们介绍了物理激活函数（PAFs）的概念。这个概念提供了一种替代一般激活函数（AFs）如ReLU、tanh和sigmoid进行所有神经元计算的方法，而是使用基于研究现象的物理定律的通用AFs的数学表示。PAFs的公式可能受到问题解析解中的项的启发。我们表明，PAFs可以受到与研究现象相关的任何数学公式的启发，例如PDE系统的初始或边界条件。我们证明了PAFs在包括谐振运动等几个PDE中的优势。

    In recent years, the gap between Deep Learning (DL) methods and analytical or numerical approaches in scientific computing is tried to be filled by the evolution of Physics-Informed Neural Networks (PINNs). However, still, there are many complications in the training of PINNs and optimal interleaving of physical models. Here, we introduced the concept of Physical Activation Functions (PAFs). This concept offers that instead of using general activation functions (AFs) such as ReLU, tanh, and sigmoid for all the neurons, one can use generic AFs that their mathematical expression is inherited from the physical laws of the investigating phenomena. The formula of PAFs may be inspired by the terms in the analytical solution of the problem. We showed that the PAFs can be inspired by any mathematical formula related to the investigating phenomena such as the initial or boundary conditions of the PDE system. We validated the advantages of PAFs for several PDEs including the harmonic oscillation
    
[^192]: 面向主动式机器学习方法的后门毒样本检测研究

    Towards A Proactive ML Approach for Detecting Backdoor Poison Samples. (arXiv:2205.13616v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13616](http://arxiv.org/abs/2205.13616)

    本研究提出了一种主动式机器学习方法，以检测深度学习模型中的后门毒样本，强调防御者的主动介入，直接强制实施并放大后受攻击模型的特殊特征，缓解后门攻击的威胁。

    

    攻击者可以通过在训练数据集中引入后门毒样本来嵌入深度学习模型中的后门。本研究探讨如何检测这些毒样本，以缓解后门攻击的威胁。首先，我们揭示了大多数先前工作中潜在的后处理工作流程，即防御者被动允许攻击进行，并利用后受攻击模型的特征来揭示毒样本。我们发现该工作流程未充分利用防御者的能力，在许多场景下，建立在该工作流程之上的防御管道容易失败或性能下降。其次，我们提出一种范式转变，通过推广主动思维，使得防御者可以主动参与整个模型训练和毒样本检测管道，直接强制实施并放大后受攻击模型的特殊特征以促进毒样本检测。基于此，我们制定了一个统一的框架，并提供了设计防御策略的实用见解。

    Adversaries can embed backdoors in deep learning models by introducing backdoor poison samples into training datasets. In this work, we investigate how to detect such poison samples to mitigate the threat of backdoor attacks. First, we uncover a post-hoc workflow underlying most prior work, where defenders passively allow the attack to proceed and then leverage the characteristics of the post-attacked model to uncover poison samples. We reveal that this workflow does not fully exploit defenders' capabilities, and defense pipelines built on it are prone to failure or performance degradation in many scenarios. Second, we suggest a paradigm shift by promoting a proactive mindset in which defenders engage proactively with the entire model training and poison detection pipeline, directly enforcing and magnifying distinctive characteristics of the post-attacked model to facilitate poison detection. Based on this, we formulate a unified framework and provide practical insights on designing de
    
[^193]: 非参数分类中的欠采样是一种极小化极差风险的鲁棒性干预方法

    Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification. (arXiv:2205.13094v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13094](http://arxiv.org/abs/2205.13094)

    该论文证明在非参数二元分类中，缺乏少数派样本是学习的根本限制，并探讨了欠采样算法的最小化极差风险的鲁棒性表现，特别是在标签转移的情况下可以最优化。

    

    尽管已经提出了广泛的技术来解决分布偏移问题，但在几个流行的基准测试中，基于欠采样的平衡数据集的训练通常能够实现接近最先进准确性。我们证明了在非参数二元分类设置下，学习的基本限制是由于缺乏少数群体样本而产生的。我们的结果表明，除非训练和测试分布之间存在高度重叠（这在真实数据集中不太可能），否则算法无法超越欠采样，除非算法利用有关分布偏移的其他结构。特别地，在标签转移的情况下，我们证明了总是存在一种最小化极差风险的欠采样算法。在组转换的情况下，我们介绍了一类最小极差风险的欠采样算法。我们在合成和真实数据集上进行了验证实验以验证我们的理论结果。

    While a broad range of techniques have been proposed to tackle distribution shift, the simple baseline of training on an $\textit{undersampled}$ balanced dataset often achieves close to state-of-the-art-accuracy across several popular benchmarks. This is rather surprising, since undersampling algorithms discard excess majority group data. To understand this phenomenon, we ask if learning is fundamentally constrained by a lack of minority group samples. We prove that this is indeed the case in the setting of nonparametric binary classification. Our results show that in the worst case, an algorithm cannot outperform undersampling unless there is a high degree of overlap between the train and test distributions (which is unlikely to be the case in real-world datasets), or if the algorithm leverages additional structure about the distribution shift. In particular, in the case of label shift we show that there is always an undersampling algorithm that is minimax optimal. In the case of grou
    
[^194]: QGNN: 基于图神经网络的价值函数分解方法

    QGNN: Value Function Factorisation with Graph Neural Networks. (arXiv:2205.13005v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13005](http://arxiv.org/abs/2205.13005)

    本文提出了一种基于图神经网络的QGNN价值函数分解方法，可以解决多智能体样本数据利用率低的问题。相比以往的方法，QGNN的多层消息传递架构提供了更高的表示复杂度，引入的置换不变的混合器也使其更具伸缩性和灵活性，在合作和非合作任务中均表现出优于先前的方法的性能。

    

    在多智能体强化学习中，使用全局目标是促进合作的一种强有力的工具。然而，使用全局奖励来训练个体智能体并不具备高效的样本数据利用率，因为它不一定与单个智能体的行动相关。为了解决这个问题，可以将全局价值函数分解为局部价值函数。这个领域的早期工作是通过将局部价值函数纯粹地置于本地信息条件下来进行分解。最近的研究表明，在提供局部信息和全局状态编码的情况下也能促进合作行为。本文提出了QGNN，这是首个使用基于图神经网络（GNN）模型的价值分解方法。QGNN的多层消息传递架构提供了比先前工作更多的表示复杂度，从而使其产生更有效的分解。QGNN还引入了一种置换不变的混合器，能够处理智能体数量和顺序变化，使其更具伸缩性和灵活性。我们在一系列多智能体场景中评估了QGNN，包括一个网格世界导航任务和一个星际争霸微观管理任务，并表明在合作和非合作任务中，它均优于先前的方法。

    In multi-agent reinforcement learning, the use of a global objective is a powerful tool for incentivising cooperation. Unfortunately, it is not sample-efficient to train individual agents with a global reward, because it does not necessarily correlate with an agent's individual actions. This problem can be solved by factorising the global value function into local value functions. Early work in this domain performed factorisation by conditioning local value functions purely on local information. Recently, it has been shown that providing both local information and an encoding of the global state can promote cooperative behaviour. In this paper we propose QGNN, the first value factorisation method to use a graph neural network (GNN) based model. The multi-layer message passing architecture of QGNN provides more representational complexity than models in prior work, allowing it to produce a more effective factorisation. QGNN also introduces a permutation invariant mixer which is able to 
    
[^195]: 基于小波散射谱的尺度依赖性和自相似模型

    Scale Dependencies and Self-Similar Models with Wavelet Scattering Spectra. (arXiv:2204.10177v2 [physics.data-an] UPDATED)

    [http://arxiv.org/abs/2204.10177](http://arxiv.org/abs/2204.10177)

    本论文提出了小波散射谱方法，可以用于建模具有平稳增量的时间序列的非高斯特性，其系数可以用于构建最大熵模型和生成新的时间序列，同时证明了自相似过程具有散射谱的尺度不变性。

    

    我们提出了小波散射谱，提供了一种具有平稳增量的时间序列的非高斯模型。复小波变换计算每个尺度上的信号变化。跨尺度的依赖关系由小波系数及其模数在时间和尺度上的联合相关性所捕获。该相关矩阵由第二个小波变换近似对角化，定义了散射谱。我们证明了自相似过程具有散射谱的尺度不变性。该特性可以在单个实现上进行统计测试，并定义了一类宽义自相似过程。我们通过散射谱系数构建最大熵模型，并使用微正则采样算法生成新的时间序列。展示了高度非高斯的金融和湍流时间序列的应用。

    We introduce the wavelet scattering spectra which provide non-Gaussian models of time-series having stationary increments. A complex wavelet transform computes signal variations at each scale. Dependencies across scales are captured by the joint correlation across time and scales of wavelet coefficients and their modulus. This correlation matrix is nearly diagonalized by a second wavelet transform, which defines the scattering spectra. We show that this vector of moments characterizes a wide range of non-Gaussian properties of multi-scale processes. We prove that self-similar processes have scattering spectra which are scale invariant. This property can be tested statistically on a single realization and defines a class of wide-sense self-similar processes. We build maximum entropy models conditioned by scattering spectra coefficients, and generate new time-series with a microcanonical sampling algorithm. Applications are shown for highly non-Gaussian financial and turbulence time-seri
    
[^196]: 在医学中使用因果树方法学习最佳动态治疗方案

    Learning Optimal Dynamic Treatment Regimes Using Causal Tree Methods in Medicine. (arXiv:2204.07124v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.07124](http://arxiv.org/abs/2204.07124)

    该论文开发了两种新的方法，用于有效处理复杂的患者数据，基于数据驱动的异质性治疗效应估计，使用因果树方法（具体来说是因果树和因果森林），学习非线性关系，控制时间变化混淆，是双重稳健的和可解释的。在治疗抑郁症的真实世界数据应用程序中，该方法在准确性和实际可解释性方面表现出色。

    

    动态治疗方案（DTR）在医学中用于根据患者异质性定制连续的治疗决策。然而，常见的学习最佳DTR的方法存在缺陷：它们通常基于结果预测而非治疗效应估计，或者使用线性模型，对于现代电子健康记录中的患者数据具有限制性。为了解决这些缺陷，我们开发了两种新的方法，用于有效处理复杂的患者数据，称为DTR-CT和DTR-CF。我们的方法基于数据驱动的异质性治疗效应估计，使用因果树方法（具体来说是因果树和因果森林），学习非线性关系，控制时间变化混淆，是双重稳健的和可解释的。据我们所知，我们的论文是第一篇为了学习最佳DTR而改编因果树方法的论文。我们使用合成数据和在抑郁症治疗背景下的真实世界数据应用程序来评估我们提出的方法。我们的结果表明，我们的方法在准确性和实际可解释性方面优于现有的方法。

    Dynamic treatment regimes (DTRs) are used in medicine to tailor sequential treatment decisions to patients by considering patient heterogeneity. Common methods for learning optimal DTRs, however, have shortcomings: they are typically based on outcome prediction and not treatment effect estimation, or they use linear models that are restrictive for patient data from modern electronic health records. To address these shortcomings, we develop two novel methods for learning optimal DTRs that effectively handle complex patient data. We call our methods DTR-CT and DTR-CF. Our methods are based on a data-driven estimation of heterogeneous treatment effects using causal tree methods, specifically causal trees and causal forests, that learn non-linear relationships, control for time-varying confounding, are doubly robust, and explainable. To the best of our knowledge, our paper is the first that adapts causal tree methods for learning optimal DTRs. We evaluate our proposed methods using synthet
    
[^197]: 遥感图像分类的多模态融合Transformer

    Multimodal Fusion Transformer for Remote Sensing Image Classification. (arXiv:2203.16952v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.16952](http://arxiv.org/abs/2203.16952)

    本文引入了一个新的多模态融合Transformer，利用多源互补信息来提高遥感图像分类的性能。

    

    视觉Transformer在图像分类任务中表现出色，而为了接近卷积神经网络的性能，需要更少的参数。本文引入了一个新的多模态融合Transformer网络，利用多源互补信息来帮助高光谱遥感图像分类任务。我们的网络利用多头交叉补丁注意力机制（mCrossPA）来获取其他数据源的信息，从而提高Transformer的泛化能力。

    Vision transformers (ViTs) have been trending in image classification tasks due to their promising performance when compared to convolutional neural networks (CNNs). As a result, many researchers have tried to incorporate ViTs in hyperspectral image (HSI) classification tasks. To achieve satisfactory performance, close to that of CNNs, transformers need fewer parameters. ViTs and other similar transformers use an external classification (CLS) token which is randomly initialized and often fails to generalize well, whereas other sources of multimodal datasets, such as light detection and ranging (LiDAR) offer the potential to improve these models by means of a CLS. In this paper, we introduce a new multimodal fusion transformer (MFT) network which comprises a multihead cross patch attention (mCrossPA) for HSI land-cover classification. Our mCrossPA utilizes other sources of complementary information in addition to the HSI in the transformer encoder to achieve better generalization. The c
    
[^198]: 鞭锤梯度下降动力学

    Whiplash Gradient Descent Dynamics. (arXiv:2203.02140v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2203.02140](http://arxiv.org/abs/2203.02140)

    本文提出了一种鞭锤惯性梯度动力学算法，在闭环优化中利用梯度信息，在有限维度设置中寻找代价函数的最小值。该算法的探索式启发式变体可以有效逃离鞍点，并且实验结果表明其具有多项式和指数收敛率。

    

    本文提出了鞭锤惯性梯度动力学，这是一种利用梯度信息在有限维度设置中找到代价函数最小值的闭环优化方法。我们为凸函数介绍了鞭锤系统的辛渐近收敛分析。我们还引入了松弛序列来解释算法的非典型性质，并介绍了鞭锤算法的探索式启发式变体，以决定性地逃离鞍点。我们研究了算法在各种代价函数下的性能，并提供了使用积分约束界和新型李雅普诺夫速率方法分析收敛速率的实用方法。我们的结果展示了二次代价函数的多项式和指数收敛率。

    In this paper, we propose the Whiplash Inertial Gradient dynamics, a closed-loop optimization method that utilises gradient information, to find the minima of a cost function in finite-dimensional settings. We introduce the symplectic asymptotic convergence analysis for the Whiplash system for convex functions. We also introduce relaxation sequences to explain the non-classical nature of the algorithm and an exploring heuristic variant of the Whiplash algorithm to escape saddle points, deterministically. We study the algorithm's performance for various costs and provide a practical methodology for analyzing convergence rates using integral constraint bounds and a novel Lyapunov rate method. Our results demonstrate polynomial and exponential rates of convergence for quadratic cost functions.
    
[^199]: SAITS: 基于自注意力机制的时间序列插值方法

    SAITS: Self-Attention-based Imputation for Time Series. (arXiv:2202.08516v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.08516](http://arxiv.org/abs/2202.08516)

    SAITS是一种基于自注意力机制的多元时间序列缺失值插值方法，通过两个对角线掩码自注意力块的加权组合，能够明确地捕捉时间步长之间的时序依赖关系和特征相关性，从而提高了插值准确性和训练速度。

    

    时间序列中的缺失数据常常成为深入分析的障碍。插值是一种常见的解决方法，其核心问题是如何确定缺失值。本文提出了一种新颖的基于自注意力机制的多元时间序列缺失值插值方法——SAITS。通过联合优化的方式训练，SAITS通过两个对角线掩码自注意力块的加权组合来学习缺失值。对角线掩码自注意力块可以明确地捕捉时间步长之间的时序依赖关系和特征相关性，从而提高了插值准确性和训练速度。同时，加权组合设计使得SAITS能够根据注意力图和缺失信息动态地分配来自两个对角线掩码自注意力块的学习表示的权重。广泛的实验证明，SAITS在时间序列插值方面优于现有的方法。

    Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weighted-combination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-
    
[^200]: OLIVE: 基于可信执行环境的隐私保护联邦学习防范稀疏性风险

    OLIVE: Oblivious Federated Learning on Trusted Execution Environment against the risk of sparsification. (arXiv:2202.07165v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07165](http://arxiv.org/abs/2202.07165)

    本文通过分析FL中TEE的漏洞，并在TEE中引入Oblivious Memory Access（OMA）以保护免受稀疏化风险的影响，提出了OLIVE算法，该算法在通信效率和模型精度方面优于最先进的安全聚合和差分隐私FL算法。

    

    结合可信执行环境(TEE)的联邦学习(FL)是实现隐私保护FL的一种有前途的方法，近年来引起了广泛的学术关注。在服务器端实现TEE可以使每轮FL在不将客户端梯度信息暴露给不可信的服务器的情况下进行。这解决了现有安全聚合方案中存在的可用性差距以及差分隐私FL中的效用差距。然而，为了解决使用TEE的问题，需要考虑服务器端TEE的漏洞，这在FL的背景下尚未得到充分的研究。本研究的主要技术贡献是分析FL中TEE的漏洞和防御。首先，我们在理论上分析了内存访问模式的泄漏，揭示了稀疏梯度的风险，稀疏梯度通常用于增强通信效率和模型精度。其次，我们设计了一个推理攻击以保护免受稀疏化风险的影响，该攻击使用TEE中的混淆RAM引入了Oblivious Memory Access(OMA)。我们对真实数据集的实验表明，我们提出的算法OLIVE在通信效率和模型精度方面都优于最先进的安全聚合和差分隐私FL算法。

    Combining Federated Learning (FL) with a Trusted Execution Environment (TEE) is a promising approach for realizing privacy-preserving FL, which has garnered significant academic attention in recent years. Implementing the TEE on the server side enables each round of FL to proceed without exposing the client's gradient information to untrusted servers. This addresses usability gaps in existing secure aggregation schemes as well as utility gaps in differentially private FL. However, to address the issue using a TEE, the vulnerabilities of server-side TEEs need to be considered -- this has not been sufficiently investigated in the context of FL. The main technical contribution of this study is the analysis of the vulnerabilities of TEE in FL and the defense. First, we theoretically analyze the leakage of memory access patterns, revealing the risk of sparsified gradients, which are commonly used in FL to enhance communication efficiency and model accuracy. Second, we devise an inference at
    
[^201]: 用于调整机器学习方法超参数的离散仿真优化

    Discrete Simulation Optimization for Tuning Machine Learning Method Hyperparameters. (arXiv:2201.05978v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.05978](http://arxiv.org/abs/2201.05978)

    本文介绍了使用离散仿真优化方法调整机器学习方法超参数的过程。作者展示了如何使用排名和选择（R&S）和随机搜索等方法识别一个最优超参数集，以最大化ML方法性能。

    

    机器学习（ML）方法在图像识别、产品推荐、金融分析、医学诊断和预测维护等大部分技术领域都有应用。实现ML方法的重要方面涉及控制ML方法的学习过程，以最大化所考虑的方法的性能。超参数调整是选择适当的ML方法参数集以控制其学习过程的过程。本文展示了使用离散仿真优化方法，例如排名和选择（R＆S）和随机搜索，以确定最大化ML方法性能的超参数集。具体来说，我们使用KN R＆S方法和随机尺子随机搜索方法及其其中一种变化来实现。我们还构建了适用KN方法的理论基础，该方法通过解空间枚举确定具有统计保证的最优解。

    Machine learning (ML) methods are used in most technical areas such as image recognition, product recommendation, financial analysis, medical diagnosis, and predictive maintenance. An important aspect of implementing ML methods involves controlling the learning process for the ML method so as to maximize the performance of the method under consideration. Hyperparameter tuning is the process of selecting a suitable set of ML method parameters that control its learning process. In this work, we demonstrate the use of discrete simulation optimization methods such as ranking and selection (R&S) and random search for identifying a hyperparameter set that maximizes the performance of a ML method. Specifically, we use the KN R&S method and the stochastic ruler random search method and one of its variations for this purpose. We also construct the theoretical basis for applying the KN method, which determines the optimal solution with a statistical guarantee via solution space enumeration. In c
    
[^202]: 双重PC算法及其对高斯性质在贝叶斯网络结构学习中的作用

    The Dual PC Algorithm and the Role of Gaussianity for Structure Learning of Bayesian Networks. (arXiv:2112.09036v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.09036](http://arxiv.org/abs/2112.09036)

    双重PC算法通过利用协方差和精度矩阵之间的反向关系，实现了CI测试，能够恢复正确的等价类，并可对互补调节集的偏相关进行测试。

    

    学习贝叶斯网络的图形结构是描述许多复杂应用程序中的数据生成机制的关键，但面临着巨大的计算挑战。在某些假设下，流行的PC算法可以通过逆向工程变量分布中所具有的条件独立关系来一致地恢复正确的等价类。双重PC算法是一种新颖的方案，通过利用协方差和精度矩阵之间的反向关系来进行PC算法中的CI测试。通过利用块矩阵求逆，我们还可以对互补（或双重）调节集的偏相关进行测试。双重PC算法的多个CI测试首先考虑边缘和完全排序CI关系。

    Learning the graphical structure of Bayesian networks is key to describing data-generating mechanisms in many complex applications but poses considerable computational challenges. Observational data can only identify the equivalence class of the directed acyclic graph underlying a Bayesian network model, and a variety of methods exist to tackle the problem. Under certain assumptions, the popular PC algorithm can consistently recover the correct equivalence class by reverse-engineering the conditional independence (CI) relationships holding in the variable distribution. The dual PC algorithm is a novel scheme to carry out the CI tests within the PC algorithm by leveraging the inverse relationship between covariance and precision matrices. By exploiting block matrix inversions we can also perform tests on partial correlations of complementary (or dual) conditioning sets. The multiple CI tests of the dual PC algorithm proceed by first considering marginal and full-order CI relationships a
    
[^203]: 针对道路交通速度的长短期记忆模型中动态时空背景的理解

    Understanding Dynamic Spatio-Temporal Contexts in Long Short-Term Memory for Road Traffic Speed Prediction. (arXiv:2112.02409v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.02409](http://arxiv.org/abs/2112.02409)

    本论文提出了一种动态局部化的LSTM模型，能够同时考虑道路之间的空间与时间依赖关系，该模型比基线方法具有更优异的预测性能。

    

    可靠的交通流预测对于创建智能交通系统至关重要。虽然许多基于大数据的预测方法已经被开发，但它们并没有反映出考虑时间和位置的复杂动态道路交互作用。在本研究中，我们提出了一种动态局部化的长短期记忆（LSTM）模型，涉及道路之间的空间和时间依赖关系。为此，我们使用局部动态空间权重矩阵及其动态变化。此外，LSTM模型可以处理具有长依赖性和复杂非线性特征的序列数据。实证结果表明，所提出的模型相比于两种不同的基线方法具有更优异的预测性能。

    Reliable traffic flow prediction is crucial to creating intelligent transportation systems. Many big-data-based prediction approaches have been developed but they do not reflect complicated dynamic interactions between roads considering time and location. In this study, we propose a dynamically localised long short-term memory (LSTM) model that involves both spatial and temporal dependence between roads. To do so, we use a localised dynamic spatial weight matrix along with its dynamic variation. Moreover, the LSTM model can deal with sequential data with long dependency as well as complex non-linear features. Empirical results indicated superior prediction performances of the proposed model compared to two different baseline methods.
    
[^204]: 基于随机混合模型的模型强化学习

    Model-Based Reinforcement Learning via Stochastic Hybrid Models. (arXiv:2111.06211v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2111.06211](http://arxiv.org/abs/2111.06211)

    本文提出了一种基于混合系统的非线性建模和控制方法，采用序列建模范例和期望最大化算法，将非线性动态分解为具有非线性转换边界的随机分段仿射模型，并成功应用于自动化领域。

    

    普适的非线性系统的最优控制是自动化领域的核心挑战。基于强大的函数逼近器，数据驱动的控制方法最近已经成功应用于挑战性应用。然而，这些方法通常使用黑箱过度参数化表示法隐藏了动态和控制的结构，从而限制了我们理解闭合环行为的能力。本文采用非线性建模和控制的混合系统视图，为问题提供了显式的分层结构，并将复杂动态分解成更简单的局部单元。我们考虑一种捕捉数据时间结构的序列建模范例，并推导出一种期望最大化（EM）算法，该算法自动将非线性动态分解为具有非线性转换边界的随机分段仿射模型。此外，我们展示了这些时间序列模型自然地具有闭环扩展，我们利用它来提取局部模型的LQR（线性二次型调节）控制器。

    Optimal control of general nonlinear systems is a central challenge in automation. Enabled by powerful function approximators, data-driven approaches to control have recently successfully tackled challenging applications. However, such methods often obscure the structure of dynamics and control behind black-box over-parameterized representations, thus limiting our ability to understand closed-loop behavior. This paper adopts a hybrid-system view of nonlinear modeling and control that lends an explicit hierarchical structure to the problem and breaks down complex dynamics into simpler localized units. We consider a sequence modeling paradigm that captures the temporal structure of the data and derive an expectation-maximization (EM) algorithm that automatically decomposes nonlinear dynamics into stochastic piecewise affine models with nonlinear transition boundaries. Furthermore, we show that these time-series models naturally admit a closed-loop extension that we use to extract local p
    
[^205]: 随机逼近和强化学习中渐近统计的ODE方法

    The ODE Method for Asymptotic Statistics in Stochastic Approximation and Reinforcement Learning. (arXiv:2110.14427v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2110.14427](http://arxiv.org/abs/2110.14427)

    本文提出了一种称为ODE方法的渐近统计方法解决$d$维随机逼近递归的问题，证明了其收敛性和中心极限定理，为强化学习等领域的应用提供了有力的理论支持。

    

    本文研究了$d$维随机逼近递归$$\theta_{n+1}=\theta_n+\alpha_{n+1}f(\theta_n, \Phi_{n+1})$$其中$\Phi$是一个在一般状态空间$\textsf{X}$上具有平稳分布$\pi$的几何遍历马尔可夫链，$f：\Re^d\times\textsf{X}\to\Re^d$。在称为（DV3）的Donsker-Varadhan Lyapunov漂移条件的一种版本和对具有向量场$\bar{f}(\theta)=\textsf{E}[f(\theta,\Phi)]$以及$\Phi\sim\pi$的均值流的稳定性条件下，建立了主要结果。(i) $\{\theta_n\}$以概率1和$L_4$收敛于$\bar{f}(\theta)$的唯一根$\theta^*$。(ii) 建立了泛函中心极限定理，以及归一化误差一维中心极限定理。(iii) 对于归一化版本$z_n{=:} \sqrt{n} (\theta^{\text{PR}}_n -\theta^*)$的平均参数$\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$ ，在步长的标准假设下，建立了中心极限定理。

    The paper concerns the $d$-dimensional stochastic approximation recursion, $$ \theta_{n+1}= \theta_n + \alpha_{n + 1} f(\theta_n, \Phi_{n+1}) $$ in which $\Phi$ is a geometrically ergodic Markov chain on a general state space $\textsf{X}$ with stationary distribution $\pi$, and $f:\Re^d\times\textsf{X}\to\Re^d$.  The main results are established under a version of the Donsker-Varadhan Lyapunov drift condition known as (DV3), and a stability condition for the mean flow with vector field $\bar{f}(\theta)=\textsf{E}[f(\theta,\Phi)]$, with $\Phi\sim\pi$.  (i) $\{ \theta_n\}$ is convergent a.s. and in $L_4$ to the unique root $\theta^*$ of $\bar{f}(\theta)$.  (ii) A functional CLT is established, as well as the usual one-dimensional CLT for the normalized error.  (iii) The CLT holds for the normalized version, $z_n{=:} \sqrt{n} (\theta^{\text{PR}}_n -\theta^*)$, of the averaged parameters, $\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$, subject to standard assumptions on the step-s
    
[^206]: 深度学习中认识不确定性的量化

    Quantifying Epistemic Uncertainty in Deep Learning. (arXiv:2110.12122v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.12122](http://arxiv.org/abs/2110.12122)

    本文提供了一个理论框架来分解深度学习中的不确定性，并提出了两种方法来估计这些不确定性，这些方法使我们能够克服在使用传统统计方法时遇到的困难，从而为建模和数据收集提供直接的指导。

    

    不确定性量化是机器学习可靠性和鲁棒性的核心。本文提供了一个理论框架来分解深度学习中的不确定性，特别是\textit {认识成分}；分为\textit {程序变异性}(来自训练过程)和\textit {数据变异性} (来自训练数据)，这是文献中首次尝试。然后我们提出两种方法来估计这些不确定性，一种是基于影响函数的方法，另一种是批次化的方法。我们展示了我们的方法如何克服在应用经典统计方法时遇到的计算困难。多个问题设置的实验评估证实了我们的理论，并说明了我们的框架和估计如何提供对建模和数据收集工作的直接指导。

    Uncertainty quantification is at the core of the reliability and robustness of machine learning. In this paper, we provide a theoretical framework to dissect the uncertainty, especially the \textit{epistemic} component, in deep learning into \textit{procedural variability} (from the training procedure) and \textit{data variability} (from the training data), which is the first such attempt in the literature to our best knowledge. We then propose two approaches to estimate these uncertainties, one based on influence function and one on batching. We demonstrate how our approaches overcome the computational difficulties in applying classical statistical methods. Experimental evaluations on multiple problem settings corroborate our theory and illustrate how our framework and estimation can provide direct guidance on modeling and data collection efforts.
    
[^207]: 分离式对比学习

    Decoupled Contrastive Learning. (arXiv:2110.06848v3 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2110.06848](http://arxiv.org/abs/2110.06848)

    本研究提出了分离式对比学习（DCL）损失，通过去除传统对比学习中的正项提高学习效率。

    

    对比学习（CL）是自监督学习（SSL）最成功的范例之一。它以一种原则性的方式，将同一图像的两个增强“视图”视为正面，将所有其他图像视为负面，以拉近它们之间的距离。然而，在基于CL的技术的令人印象深刻的成功背后，它们的制定通常依赖于重计算设置，包括大的样本批次、广泛的训练时期等。因此，我们有动力解决这些问题，并建立一种简单、高效、有竞争力的对比学习基线。

    Contrastive learning (CL) is one of the most successful paradigms for self-supervised learning (SSL). In a principled way, it considers two augmented "views" of the same image as positive to be pulled closer, and all other images as negative to be pushed further apart. However, behind the impressive success of CL-based techniques, their formulation often relies on heavy-computation settings, including large sample batches, extensive training epochs, etc. We are thus motivated to tackle these issues and establish a simple, efficient, yet competitive baseline of contrastive learning. Specifically, we identify, from theoretical and empirical studies, a noticeable negative-positive-coupling (NPC) effect in the widely used InfoNCE loss, leading to unsuitable learning efficiency concerning the batch size. By removing the NPC effect, we propose decoupled contrastive learning (DCL) loss, which removes the positive term from the denominator and significantly improves the learning efficiency. DC
    
[^208]: 论深度学习在差分隐私下的收敛性与校准性

    On the Convergence and Calibration of Deep Learning with Differential Privacy. (arXiv:2106.07830v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.07830](http://arxiv.org/abs/2106.07830)

    本文通过NTK对差分隐私训练进行连续时间分析，发现噪声只会影响隐私风险而不影响收敛性和校准性，而基于每个样本的梯度剪裁会影响收敛性和校准性。此外，大剪裁范数下的差分隐私模型不仅享有相同的隐私保证，而且校准效果好。

    

    差分隐私训练通常会以数据隐私保护为代价，导致收敛速度变慢（从而精度降低），且比非隐私方法更容易出现严重的校准误差。本研究通过神经切向核（NTK）从连续时间的角度分析了差分隐私训练的收敛性，对任意网络结构和损失函数进行了建模，发现噪声只会影响隐私风险而不影响收敛性和校准性，而基于每个样本的梯度剪裁（在平坦和层级剪裁风格下）会影响收敛性和校准性。此外，研究表明，尽管小剪裁范数下的差分隐私模型通常会在精度上表现最佳，但其校准性较差且不可靠。而在大剪裁范数下训练的差分隐私模型不仅享有相同的隐私保证，而且校准效果好。

    Differentially private (DP) training preserves the data privacy usually at the cost of slower convergence (and thus lower accuracy), as well as more severe mis-calibration than its non-private counterpart. To analyze the convergence of DP training, we formulate a continuous time analysis through the lens of neural tangent kernel (NTK), which characterizes the per-sample gradient clipping and the noise addition in DP training, for arbitrary network architectures and loss functions. Interestingly, we show that the noise addition only affects the privacy risk but not the convergence or calibration, whereas the per-sample gradient clipping (under both flat and layerwise clipping styles) only affects the convergence and calibration.  Furthermore, we observe that while DP models trained with small clipping norm usually achieve the best accurate, but are poorly calibrated and thus unreliable. In sharp contrast, DP models trained with large clipping norm enjoy the same privacy guarantee and si
    
[^209]: 学习将周例解决方案进行组合用于神经程序合成

    Learning to Combine Per-Example Solutions for Neural Program Synthesis. (arXiv:2106.07175v2 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2106.07175](http://arxiv.org/abs/2106.07175)

    该论文提出了一种把程序合成问题分为两个阶段的方法，提高了解决方案的成功率。作者使用了由多头注意机制构建的Cross Aggregator神经网络模块，学习如何组合每个样例程序解决方案，生成全局解决方案。

    

    例子程序合成的目标是找到一个与给定的输入输出样例一致的计算机程序。大多数基于学习的方法尝试找到满足所有样例的程序。相比之下，我们的工作考虑将问题分为两个阶段的方法：(a)找到只满足一个样例的程序，(b)利用这些单例程序解决方案生成满足所有样例的程序。我们引入了基于多头注意机制的Cross Aggregator神经网络模块，学习如何组合这些单例方案中存在的提示来合成一个全局解决方案。在不同长度的程序和两种不同实验设置下的评估表明，当在相同的时间预算下时，我们的技术显著提高了成功率，超过了PCCoder [Zohar等人，2018]和其他消融基线。我们的工作的代码、数据和训练模型可以在https://github.com/shriva找到。

    The goal of program synthesis from examples is to find a computer program that is consistent with a given set of input-output examples. Most learning-based approaches try to find a program that satisfies all examples at once. Our work, by contrast, considers an approach that breaks the problem into two stages: (a) find programs that satisfy only one example, and (b) leverage these per-example solutions to yield a program that satisfies all examples. We introduce the Cross Aggregator neural network module based on a multi-head attention mechanism that learns to combine the cues present in these per-example solutions to synthesize a global solution. Evaluation across programs of different lengths and under two different experimental settings reveal that when given the same time budget, our technique significantly improves the success rate over PCCoder [Zohar et. al 2018] and other ablation baselines. The code, data and trained models for our work can be found at https://github.com/shriva
    
[^210]: IID-GAN：一种独立同分布采样视角下的规范模式崩溃方法

    IID-GAN: an IID Sampling Perspective for Regularizing Mode Collapse. (arXiv:2106.00563v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.00563](http://arxiv.org/abs/2106.00563)

    本论文提出一种基于独立同分布采样视角的GAN方法，通过在源分布下实现逆采样的IID样本与真实数据在潜在空间中的高斯源之间的相似性，从而规范化了GAN中的模式崩溃问题。

    

    尽管生成对抗网络（GAN）很成功，但它仍然受到模式崩溃的影响，即生成器只能将潜在变量映射到目标分布中部分模式。本文分析并试图通过独立同分布（IID）采样视角来规范化这个问题，并强调保持生成的IID属性可以自然地避免模式崩溃。这是基于机器学习中实际数据的基础IID假设。然而，尽管源样本{z}服从IID，但生成物{G（z）}不一定是从目标分布中采样得到的IID样本。基于这个观察，考虑到实现IID生成的必要条件是从目标数据中逆采样的样本在源分布下也应该是IID的，我们提出了一种新的损失来鼓励真实数据的逆采样和潜在空间中的高斯源之间的相似性，以规范生成。

    Despite its success, generative adversarial networks (GANs) still suffer from mode collapse, i.e., the generator can only map latent variables to a partial set of modes in the target distribution. In this paper, we analyze and seek to regularize this issue with an independent and identically distributed (IID) sampling perspective and emphasize that holding the IID property referring to the target distribution for generation can naturally avoid mode collapse. This is based on the basic IID assumption for real data in machine learning. However, though the source samples {z} obey IID, the generations {G(z)} may not necessarily be IID sampling from the target distribution. Based on this observation, considering a necessary condition of IID generation that the inverse samples from target data should also be IID in the source distribution, we propose a new loss to encourage the closeness between inverse samples of real data and the Gaussian source in latent space to regularize the generation
    
[^211]: 基于广义全变差最小化的聚类联邦学习

    Clustered Federated Learning via Generalized Total Variation Minimization. (arXiv:2105.12769v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.12769](http://arxiv.org/abs/2105.12769)

    本文介绍了一种基于广义全变差最小化的完全分散的联邦学习算法，可以训练适用于具有本地数据集的分散式去中心化环境的本地化（或个性化）模型，并获得了良好的模拟结果。

    

    本文研究了在具有内在网络结构的分散式本地数据集的去中心化环境下训练本地（或个性化）模型的优化方法。这种网络结构是由本地数据集之间的领域特定相似性概念引起的。这些概念的例子包括时空邻近性，统计依赖性或功能关系。我们的主要概念性贡献在于将联邦学习描述为广义总变差（GTV）最小化。这种表述统一并显著扩展了现有的联邦学习方法。它具有高度的灵活性，并且可以与广泛的参数模型结合使用，包括广义线性模型或深度神经网络。我们的主要算法贡献是一种完全分散的联邦学习算法。该算法是通过应用已建立的原始-对偶方法来解决GTV最小化问题而获得的。它可以实现为消息传递，并且对于由通信延迟或干扰产生的不精确计算很稳健。我们在合成和真实数据集上的模拟中展示了我们方法的有效性。

    We study optimization methods to train local (or personalized) models for decentralized collections of local datasets with an intrinsic network structure. This network structure arises from domain-specific notions of similarity between local datasets. Examples for such notions include spatio-temporal proximity, statistical dependencies or functional relations. Our main conceptual contribution is to formulate federated learning as generalized total variation (GTV) minimization. This formulation unifies and considerably extends existing federated learning methods. It is highly flexible and can be combined with a broad range of parametric models, including generalized linear models or deep neural networks. Our main algorithmic contribution is a fully decentralized federated learning algorithm. This algorithm is obtained by applying an established primal-dual method to solve GTV minimization. It can be implemented as message passing and is robust against inexact computations that arise fro
    
[^212]: 一种适用于物联网应用的低延迟MAC协议：无需显式状态信息共享的分散式最优队列调度

    A Low-Delay MAC for IoT Applications: Decentralized Optimal Scheduling of Queues without Explicit State Information Sharing. (arXiv:2105.11213v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2105.11213](http://arxiv.org/abs/2105.11213)

    本文研究了一种适用于物联网应用的低延迟MAC协议，该协议无需显式状态信息共享且具有分布式控制。该协议采用“贪心”和“穷尽”的策略，旨在在流量竞争访问时实现低延迟，在重负载下实现定时访问。

    

    本文考虑共享时隙无线信道的多个相邻节点系统，并寻求一种具有低平均延迟、分布式控制（即不存在中央调度器）且不需要显式交换状态信息或控制信号的MAC协议。这种MAC协议的设计必须考虑轻量级流量的竞争访问和重负载下的定时访问，导致人们长期以来对混合自适应MAC的兴趣。在离散时间设置中，对于分布式MAC设计，我们考虑每个节点拥有本地信息和一些从监听中获得的公共信息的实际信息结构。在此设置中，“ZMAC”是一种现有的混合自适应协议。我们通过以下两个步骤来解决这个问题：(1)我们表明策略“贪心”和“穷尽”是充分的。将策略限制在这个类别中，可以将问题简化为获得一个队列。

    We consider a system of several collocated nodes sharing a time slotted wireless channel, and seek a MAC (medium access control) that (i) provides low mean delay, (ii) has distributed control (i.e., there is no central scheduler), and (iii) does not require explicit exchange of state information or control signals. The design of such MAC protocols must keep in mind the need for contention access at light traffic, and scheduled access in heavy traffic, leading to the long-standing interest in hybrid, adaptive MACs.  Working in the discrete time setting, for the distributed MAC design, we consider a practical information structure where each node has local information and some common information obtained from overhearing. In this setting, "ZMAC" is an existing protocol that is hybrid and adaptive. We approach the problem via two steps (1) We show that it is sufficient for the policy to be "greedy" and "exhaustive". Limiting the policy to this class reduces the problem to obtaining a queu
    
[^213]: 学习通过目标条件传输网络重新排列可变形电缆、织物和袋子

    Learning to Rearrange Deformable Cables, Fabrics, and Bags with Goal-Conditioned Transporter Networks. (arXiv:2012.03385v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2012.03385](http://arxiv.org/abs/2012.03385)

    本论文开发了一套仿真基准测试，提出将目标条件嵌入Transporter Networks来学习可变形物体操作，并证明该方法成功地执行了复杂目标条件和多步操作任务。

    

    重新排列和操作电缆、织物和袋子等可变形物体一直是机器人操作中的一个长期挑战。与刚性物体相比，可变形物体的复杂动力学和高维配置空间使得不仅对于多步规划，还包括目标规范，操作变得困难。目标不像刚性物体姿态那样容易规定，可能涉及复杂的相对空间关系，例如“把物品放在袋子里”。我们在这项工作中，开发了一套包含1D、2D和3D可变形结构的仿真基准测试，包括涉及基于图像的目标条件和多步可变形操作的任务。我们建议将目标条件嵌入Transporter Networks，这是一种最近提出的模型架构，用于学习机器人操控，并重排深层特征，以推断可以表示取放动作的位移。在仿真和物理实验中，我们证明了可以使用目标条件传输网络来学习操作具有复杂几何形状的可变形物体，并且可以成功执行涉及目标条件和多步操作的任务。我们的方法可应用于广泛的现实世界任务，包括机器人假肢和抓握精细物品。

    Rearranging and manipulating deformable objects such as cables, fabrics, and bags is a long-standing challenge in robotic manipulation. The complex dynamics and high-dimensional configuration spaces of deformables, compared to rigid objects, make manipulation difficult not only for multi-step planning, but even for goal specification. Goals cannot be as easily specified as rigid object poses, and may involve complex relative spatial relations such as "place the item inside the bag". In this work, we develop a suite of simulated benchmarks with 1D, 2D, and 3D deformable structures, including tasks that involve image-based goal-conditioning and multi-step deformable manipulation. We propose embedding goal-conditioning into Transporter Networks, a recently proposed model architecture for learning robotic manipulation that rearranges deep features to infer displacements that can represent pick and place actions. In simulation and in physical experiments, we demonstrate that goal-conditione
    
[^214]: 利用背景音素类信息提高语音增强性能

    Improving Speech Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information. (arXiv:2011.07442v4 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2011.07442](http://arxiv.org/abs/2011.07442)

    本文提出了一种新的语音增强方法，通过利用广义语音类别序列的损失来提高SE性能，实验证明上下文BPC信息可以提高性能。

    

    先前的研究证实，通过用发音的位置和方式特征增强声学特征，语音增强(SE)过程可以指导考虑输入语音的广义语音属性，以获得性能改进。在本文中，我们探索了结构属性的背景信息，作为进一步受益的SE的附加信息。更具体地，我们建议通过利用端到端自动语音识别(E2E-ASR)模型中预测的广义语音类别序列的损失来提高SE性能。我们还开发了多目标训练，利用ASR和感知损失基于基于BPC的E2E-ASR来训练SE系统。来自语音降噪，语音去混响和受损语音增强任务的实验结果表明，上下文BPC信息可以提高SE性能。此外，使用基于BPC的E2E-ASR训练的SE模型胜过了其他基线模型。

    Previous studies have confirmed that by augmenting acoustic features with the place/manner of articulatory features, the speech enhancement (SE) process can be guided to consider the broad phonetic properties of the input speech when performing enhancement to attain performance improvements. In this paper, we explore the contextual information of articulatory attributes as additional information to further benefit SE. More specifically, we propose to improve the SE performance by leveraging losses from an end-to-end automatic speech recognition (E2E-ASR) model that predicts the sequence of broad phonetic classes (BPCs). We also developed multi-objective training with ASR and perceptual losses to train the SE system based on a BPC-based E2E-ASR. Experimental results from speech denoising, speech dereverberation, and impaired speech enhancement tasks confirmed that contextual BPC information improves SE performance. Moreover, the SE model trained with the BPC-based E2E-ASR outperforms th
    
[^215]: 随机变分不等式的简单而最优方法I：算子外推法

    Simple and optimal methods for stochastic variational inequalities, I: operator extrapolation. (arXiv:2011.02987v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2011.02987](http://arxiv.org/abs/2011.02987)

    本文提出了一种简单实用的算子外推法用来解决变分不等式问题，同时还提出了一种随机算子外推法实现随机平滑和强单调VI的最优复杂度。

    

    本文首先提出了一种新颖的算子外推法（OE）来解决确定性变分不等式（VI）问题。类似于梯度（算子）投影法，OE通过在每次迭代中求解单个投影子问题来更新一个搜索序列。我们证明了OE比现有方法更简单地实现了解决各种VI问题的最优收敛速率。然后，我们介绍了随机算子外推（SOE）方法，并建立了其用于解决不同随机VI问题的最优收敛行为。特别地，SOE首次在文献中实现了用于解决基础问题（即随机平滑和强单调VI）的最优复杂度。我们还提出了随机块算子外推（SBOE）方法，以进一步降低应用于具有特定块结构的大规模确定性VI的OE方法的迭代成本。数值实验证明了所提出方法的有效性和效率。

    In this paper we first present a novel operator extrapolation (OE) method for solving deterministic variational inequality (VI) problems. Similar to the gradient (operator) projection method, OE updates one single search sequence by solving a single projection subproblem in each iteration. We show that OE can achieve the optimal rate of convergence for solving a variety of VI problems in a much simpler way than existing approaches. We then introduce the stochastic operator extrapolation (SOE) method and establish its optimal convergence behavior for solving different stochastic VI problems. In particular, SOE achieves the optimal complexity for solving a fundamental problem, i.e., stochastic smooth and strongly monotone VI, for the first time in the literature. We also present a stochastic block operator extrapolations (SBOE) method to further reduce the iteration cost for the OE method applied to large-scale deterministic VIs with a certain block structure. Numerical experiments have 
    
[^216]: LEAD：用于极小-极大优化的最小作用动力学

    LEAD: Least-Action Dynamics for Min-Max Optimization. (arXiv:2010.13846v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.13846](http://arxiv.org/abs/2010.13846)

    本文提出了一种名为LEAD的优化器，它基于物理学中的动力学特性来改进博弈优化的收敛问题，并在二次极小-极大博弈中展示了线性收敛到纳什均衡的特性。

    

    对抗性建模（如生成性对抗网络（GAN））重新燃起了人们对双人极小-极大博弈的兴趣。该类博弈优化的一个主要难题是旋转动力学阻碍其收敛。本文表明，博弈优化共享粒子系统受多重力的动力学特性，可以利用物理学中的工具来改进优化动力学。受物理框架启发，我们提出一个用于极小-极大博弈的优化器LEAD。接下来，我们使用李亚普诺夫稳定性理论和谱分析，在连续和离散时间设置下研究了LEAD在一类二次极小-极大博弈中的收敛特性，展示了线性收敛到纳什均衡。最后，我们在合成数据集和CIFAR-10图像生成上对我们的方法进行了实证评估，证明了GAN训练中的改进效果。

    Adversarial formulations such as generative adversarial networks (GANs) have rekindled interest in two-player min-max games. A central obstacle in the optimization of such games is the rotational dynamics that hinder their convergence. In this paper, we show that game optimization shares dynamic properties with particle systems subject to multiple forces, and one can leverage tools from physics to improve optimization dynamics. Inspired by the physical framework, we propose LEAD, an optimizer for min-max games. Next, using Lyapunov stability theory and spectral analysis, we study LEAD's convergence properties in continuous and discrete time settings for a class of quadratic min-max games to demonstrate linear convergence to the Nash equilibrium. Finally, we empirically evaluate our method on synthetic setups and CIFAR-10 image generation to demonstrate improvements in GAN training.
    
[^217]: 用黎曼Langevin算法求解半定向规划

    Riemannian Langevin Algorithm for Solving Semidefinite Programs. (arXiv:2010.11176v6 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2010.11176](http://arxiv.org/abs/2010.11176)

    研究了一种基于Langevin扩散的算法，用于在球面的乘积流形上进行非凸优化和采样，展示了在适当的温度选择下，该算法可以保证次优解到全局最小值的间隙概率非常小，并在此基础上提出了求解半定向规划模型的新方法，并给出了全局最优性的保证。

    

    我们提出了一种基于 Langevin 扩散的算法，用于在球面的乘积流形上进行非凸优化和采样。在对数 Sobolev 不等式下，我们建立了一个有限迭代收敛到 Gibbs 分布的保证，这个保证是基于 Kullback-Leibler 散度的。我们展示了一个合适的温度选择，可以保证次优解到全局最小值的间隙概率极高地非常小。作为一个应用，我们考虑 Burer-Monteiro 方法用于解决带对角约束的半定向规划(SDP)，并分析了用于优化非凸目标的 Langevin 算法。特别地，我们证明了在没有虚假局部极小值的情况下，Burer-Monteiro 问题的对数 Sobolev 不等式是成立的，但这里存在鞍点。结合这些结果，我们提供了 SDP 和 Max-Cut 问题的全局最优性保证。更具体地说，我们展示了 Langevin 算法实现了这个保证。

    We propose a Langevin diffusion-based algorithm for non-convex optimization and sampling on a product manifold of spheres. Under a logarithmic Sobolev inequality, we establish a guarantee for finite iteration convergence to the Gibbs distribution in terms of Kullback--Leibler divergence. We show that with an appropriate temperature choice, the suboptimality gap to the global minimum is guaranteed to be arbitrarily small with high probability.  As an application, we consider the Burer--Monteiro approach for solving a semidefinite program (SDP) with diagonal constraints, and analyze the proposed Langevin algorithm for optimizing the non-convex objective. In particular, we establish a logarithmic Sobolev inequality for the Burer--Monteiro problem when there are no spurious local minima, but under the presence saddle points. Combining the results, we then provide a global optimality guarantee for the SDP and the Max-Cut problem. More precisely, we show that the Langevin algorithm achieves 
    
[^218]: IRX-1D: 一种用于遥感分类的简单深度学习架构

    IRX-1D: A Simple Deep Learning Architecture for Remote Sensing Classifications. (arXiv:2010.03902v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2010.03902](http://arxiv.org/abs/2010.03902)

    我们提出了一种简单的深度学习架构，用于遥感分类。在小样本情况下，该架构比2D-CNN表现更好；与其他不同深度学习架构相比，在印第安娜松树高光谱数据集上表现相当或更好。但使用有限训练样本得到的分类图像与使用大样本训练的模型得到的分类图像会将不同的陆地覆盖类别分配到相同的区域。

    

    我们提出了一种简单的深度学习架构，组合了Inception、ResNet和Xception网络的元素。我们使用了四个新数据集来进行分类，包括小样本和大样本。分类准确度方面的结果表明，与贝叶斯优化的2D-CNN相比，所提出的架构在小样本情况下具有更好的性能。在使用小样本进行比较的结果方面，与印第安娜松树高光谱数据集上使用不同深度学习架构进行的九项报告相比，我们提出的架构表现相当或更好。尽管在使用有限的训练样本时实现了高分类准确度，但分类图像的比较表明，与在所有数据集上使用大样本训练的模型提供的分类图像相比，不同的陆地覆盖类别被分配给了相同的区域。

    We proposes a simple deep learning architecture combining elements of Inception, ResNet and Xception networks. Four new datasets were used for classification with both small and large training samples. Results in terms of classification accuracy suggests improved performance by proposed architecture in comparison to Bayesian optimised 2D-CNN with small training samples. Comparison of results using small training sample with Indiana Pines hyperspectral dataset suggests comparable or better performance by proposed architecture than nine reported works using different deep learning architectures. In spite of achieving high classification accuracy with limited training samples, comparison of classified image suggests different land cover classes are assigned to same area when compared with the classified image provided by the model trained using large training samples with all datasets.
    
[^219]: 部分观测下的协作多智能体强化学习

    Cooperative Multi-Agent Reinforcement Learning with Partial Observations. (arXiv:2006.10822v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.10822](http://arxiv.org/abs/2006.10822)

    本文提出了一种基于局部状态和动作信息的分布式零阶策略优化方法，可用于部分观测的协作多智能体强化学习，减小通信开销并取得更好的效果。

    

    本文提出了一种分布式的零阶策略优化方法，用于多智能体强化学习（MARL）。现有的MARL算法通常假设每个智能体都可以观察网络中所有其他智能体的状态和动作。但在大规模问题中，与多跳邻居共享状态和动作信息可能会导致显着的通信开销。提出的零阶策略优化方法的优势在于，它允许智能体仅基于局部的、部分的状态和动作信息来计算本地策略梯度，从而更新它们的本地策略函数，并使用共识来获得依赖于全局累积奖励的局部估计。具体来说，为了计算本地策略梯度，我们开发了一种新的分布式零阶策略梯度估计器，它依赖于一点残差反馈， im同时与现有的依赖于一点反馈的零阶估计器相比，显著降低了通信开销。我们在几个协作多智能体基准任务上展示了提出方法的有效性，并表明它胜过了现有的假设具有全观察信息的方法。

    In this paper, we propose a distributed zeroth-order policy optimization method for Multi-Agent Reinforcement Learning (MARL). Existing MARL algorithms often assume that every agent can observe the states and actions of all the other agents in the network. This can be impractical in large-scale problems, where sharing the state and action information with multi-hop neighbors may incur significant communication overhead. The advantage of the proposed zeroth-order policy optimization method is that it allows the agents to compute the local policy gradients needed to update their local policy functions using local estimates of the global accumulated rewards that depend on partial state and action information only and can be obtained using consensus. Specifically, to calculate the local policy gradients, we develop a new distributed zeroth-order policy gradient estimator that relies on one-point residual-feedback which, compared to existing zeroth-order estimators that also rely on one-poi
    
[^220]: 使用条件生成对抗网络进行深度学习对流流动

    Deep Learning Convective Flow Using Conditional Generative Adversarial Networks. (arXiv:2005.06422v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2005.06422](http://arxiv.org/abs/2005.06422)

    FluidGAN是一种通用深度学习框架，能够高速、高准确性地预测复杂对流流动，并可帮助理解物理模型复杂或未知的确定性多物理现象。

    

    我们开发了一个通用的深度学习框架FluidGAN，能够学习和预测与能量传输耦合的时间依赖性对流流动。FluidGAN是完全数据驱动的，具有高速和高准确性，并且在不需要对底层流体和能量传输物理有任何先验知识的情况下满足流体物理学。FluidGAN还学习了速度、压力和温度场之间的耦合关系。我们的框架可帮助理解底层物理模型复杂或未知的确定性多物理现象。

    We developed a general deep learning framework, FluidGAN, capable of learning and predicting time-dependent convective flow coupled with energy transport. FluidGAN is thoroughly data-driven with high speed and accuracy and satisfies the physics of fluid without any prior knowledge of underlying fluid and energy transport physics. FluidGAN also learns the coupling between velocity, pressure, and temperature fields. Our framework helps understand deterministic multiphysics phenomena where the underlying physical model is complex or unknown.
    
[^221]: 基于元学习的源码模型实时适应性研究

    On-the-Fly Adaptation of Source Code Models using Meta-Learning. (arXiv:2003.11768v2 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2003.11768](http://arxiv.org/abs/2003.11768)

    本文提出了一种基于元学习的方法用于源码模型的实时适应性研究，以提高源代码模型的预测准确性，解决代码自动完成问题。

    

    适应未知的本地环境是成功的源代码模型必须克服的重要挑战之一。动态评估是最流行的适应模型的方法之一，但本文提出了一种不同的方法——将上下文适应问题转化为元学习问题。我们旨在训练一个基本的源码模型，能够从文件中的信息提取最佳的支持标记进行学习，以提供缺失标记的改进预测。与动态评估不同，这个公式允许我们选择更有针对性的信息（支持标记）进行适应，即在目标文件中的目标空位置之前和之后。我们考虑了一种称为行级维护的评估设置，旨在反映IDE中代码自动完成的下游任务。

    The ability to adapt to unseen, local contexts is an important challenge that successful models of source code must overcome. One of the most popular approaches for the adaptation of such models is dynamic evaluation. With dynamic evaluation, when running a model on an unseen file, the model is updated immediately after having observed each token in that file. In this work, we propose instead to frame the problem of context adaptation as a meta-learning problem. We aim to train a base source code model that is best able to learn from information in a file to deliver improved predictions of missing tokens. Unlike dynamic evaluation, this formulation allows us to select more targeted information (support tokens) for adaptation, that is both before and after a target hole in a file. We consider an evaluation setting that we call line-level maintenance, designed to reflect the downstream task of code auto-completion in an IDE. Leveraging recent developments in meta-learning such as first-o
    
[^222]: 通过子集和实现安全求和：一种新的保护隐私的分布式机器学习基础

    Secure Summation via Subset Sums: A New Primitive for Privacy-Preserving Distributed Machine Learning. (arXiv:1906.11993v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/1906.11993](http://arxiv.org/abs/1906.11993)

    本文提出了一种通过子集和实现安全求和的方法（S5），该方法可以在存在恶意服务器和只有两个诚实客户的情况下运行，并且可以实现更高的准确性和更低的通信和计算成本。

    

    在进行人口统计研究或训练复杂机器学习模型时，通常需要从不同的参与者收集数据。在这些应用中，求和是一个重要的基础：用于计算平均值、计数或小批量梯度。在许多情况下，数据是敏感的隐私数据，因此不能在中心服务器上收集。因此，需要以分布式和保护隐私的方式进行求和。现有的具有计算隐私保证的分布式求和解决方案需要信任或连接的假设，例如，存在受信任的服务器或客户之间的点对点连接，这些假设在实际情况下可能无法实现。在这些挑战的驱动下，我们提出了一种称为“S5”的通过子集和实现安全求和的方法，该方法可以在存在恶意服务器和只有两个诚实客户的情况下运行，并且不需要客户之间的点对点连接。 S5在将客户端的消息发送给服务器之前向其添加零和噪声，并且服务器使用子集求和来提取客户端的意图消息，同时抵消噪声。我们的实验证明，与基于同态加密或秘密共享的最新方法相比，S5在实现分布式求和时可以实现更高的准确性，同时达到更低的通信和计算成本。

    For population studies or for the training of complex machine learning models, it is often required to gather data from different actors. In these applications, summation is an important primitive: for computing means, counts or mini-batch gradients. In many cases, the data is privacy-sensitive and therefore cannot be collected on a central server. Hence the summation needs to be performed in a distributed and privacy-preserving way. Existing solutions for distributed summation with computational privacy guarantees make trust or connection assumptions - e.g., the existence of a trusted server or peer-to-peer connections between clients - that might not be fulfilled in real world settings. Motivated by these challenges, we propose Secure Summation via Subset Sums (S5), a method for distributed summation that works in the presence of a malicious server and only two honest clients, and without the need for peer-to-peer connections between clients. S5 adds zero-sum noise to clients' messag
    

