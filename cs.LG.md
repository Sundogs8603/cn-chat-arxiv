# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Trigger-Level Event Reconstruction for Neutrino Telescopes Using Sparse Submanifold Convolutional Neural Networks.](http://arxiv.org/abs/2303.08812) | SSCNN是一种用于解决中微子望远镜数据处理中稀疏、高维度和非规则几何形状等问题的网络，其事件重构性能优于传统和机器学习算法，运行速度大大提高，可用于改善中微子能量和方向的第一估计以播种更先进的重建。 |
| [^2] | [Relax, it doesn't matter how you get there: A new self-supervised approach for multi-timescale behavior analysis.](http://arxiv.org/abs/2303.08811) | 本文提出了一种自监督的多任务表示学习模型，结合行动预测和多尺度结构以建立局部和全局动态的潜在空间，并在多智能体行为挑战赛中取得最先进的结果。 |
| [^3] | [Understanding Post-hoc Explainers: The Case of Anchors.](http://arxiv.org/abs/2303.08806) | 本文对Anchors进行了理论分析，这是一种基于规则的可解释性方法，用于解释文本分类器的决策。 |
| [^4] | [Stochastic Interpolants: A Unifying Framework for Flows and Diffusions.](http://arxiv.org/abs/2303.08797) | 本文提出了一种统一的生成模型，该模型基于随机插值框架，可以实现流和扩散方法的统一。作者构建了一类广泛的连续时间随机过程，用于将两个任意的密度在有限时间内精确地连接。这种方法可以用于基于概率微分方程的确定性和随机生成模型的构建。 |
| [^5] | [PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining.](http://arxiv.org/abs/2303.08789) | PLEX提出了一种新的机器人操纵预训练方法，利用任务不可知的视觉运动轨迹和大量的任务条件下的物体操作视频，在学习通用的操纵例程的同时，通过视频演示学习如何在这个特征空间中规划各种任务。 |
| [^6] | [Fully neuromorphic vision and control for autonomous drone flight.](http://arxiv.org/abs/2303.08778) | 本文介绍了第一个全神经形态视觉到控制的流程，使无人机具备了执行自主基于视觉的飞行所需的能力，为机器人感知和行动提供了低延迟和能量有效的解决方案。 |
| [^7] | [Distribution-free Deviation Bounds of Learning via Model Selection with Cross-validation Risk Estimation.](http://arxiv.org/abs/2303.08777) | 本文提出通过模型选择和交叉验证风险估计来学习的一般方法，并建立了无分布偏差界，比经验风险最小化方法更紧密，在一些情况下表现更优。 |
| [^8] | [Deep Calibration With Artificial Neural Network: A Performance Comparison on Option Pricing Models.](http://arxiv.org/abs/2303.08760) | 论文通过构建人工神经网络的方式，以蒙特卡罗模拟数据集作为训练数据，探讨了使用ANN进行期权定价模型参数校准的方法，结果表明ANN方法的性能优于MCS方法，并且具有更短的计算时间。 |
| [^9] | [Exploiting 4D CT Perfusion for segmenting infarcted areas in patients with suspected acute ischemic stroke.](http://arxiv.org/abs/2303.08757) | 该研究提出了一种新颖的方法，通过利用四维CTP全面利用时空信息，以分割疑似急性缺血性卒中患者的梗死区。实验证明，该方法明显优于现有的最先进方法，有潜力为改善AIS患者的早期诊断和治疗规划的准确性和效率做出贡献。 |
| [^10] | [Evaluating gesture-generation in a large-scale open challenge: The GENEA Challenge 2022.](http://arxiv.org/abs/2303.08737) | 本文介绍了GENEA Challenge 2022的研究结果，该比赛旨在基准测试基于数据驱动的自动共同语言手势生成。使用具有相同语音和动作的数据集，众多参赛团队的手势生成系统在几个大型用户研究中得到了评估，因此能够进行直接比较。 |
| [^11] | [A machine-learning approach to thunderstorm forecasting through post-processing of simulation data.](http://arxiv.org/abs/2303.08736) | 介绍了一种机器学习方法SALAMA用于预测雷暴发生情况，可以在长达11小时的时间内进行预测，预测技能优于传统方案，且预测时间尺度随着预报的空间尺度呈线性增加。 |
| [^12] | [DACOS-A Manually Annotated Dataset of Code Smells.](http://arxiv.org/abs/2303.08729) | DACOS是一个手动注释的包含三种不同粒度的代码异味（多面抽象、复杂方法和长参数列表）的数据集，用于作为机器学习检测代码异味的训练和基准测试。 |
| [^13] | [Artificial Influence: An Analysis Of AI-Driven Persuasion.](http://arxiv.org/abs/2303.08721) | 本文探讨了AI驱动的说服未来的不确定性，包括移动说服力平衡的方式、定制化的说服、虚假信息的带动以及改变人类自身言论的方式。我们警告存在负面影响，并呼吁加强对其开发和使用的监管。 |
| [^14] | [Practicality of generalization guarantees for unsupervised domain adaptation with neural networks.](http://arxiv.org/abs/2303.08720) | 研究评估了现有文献中有潜力满足我们要求的域适应图像分类任务的界限，发现所有界限都是空泛的，样本泛化术语占据了观察到的松弛程度的很大部分，特别是当这些术语与域的度量互动时。 |
| [^15] | [Learning to Reconstruct Signals From Binary Measurements.](http://arxiv.org/abs/2303.08691) | 该论文提出了一种新的自监督学习方法SSBM，它只需要二进制数据进行训练，并探索了从不完整的二进制观察中学习的极端情况。这为从二进制测量中恢复信号提供了必要和充分条件，并在一系列真实数据集上展示了SSBM的卓越表现。 |
| [^16] | [Replay Buffer With Local Forgetting for Adaptive Deep Model-Based Reinforcement Learning.](http://arxiv.org/abs/2303.08690) | 提出了一种带有局部遗忘的新型重放缓冲区，可以在状态空间的相关部分快速遗忘过时的经验。实验证明该方法提高了自适应深度模型强化学习代理对环境变化的适应能力，加速了学习速度并改善了策略。 |
| [^17] | [Making Vision Transformers Efficient from A Token Sparsification View.](http://arxiv.org/abs/2303.08685) | 本文提出了一种新的Semantic Token ViT (STViT)方法，实现了全局和本地视觉Transformer的高效性能，同时可用作下游任务的主干骨干。其通过聚类中心的语义令牌代表来代替图像令牌，实现较少的语义令牌即可达到同样的效果。 |
| [^18] | [Muti-Agent Proximal Policy Optimization For Data Freshness in UAV-assisted Networks.](http://arxiv.org/abs/2303.08680) | 本文研究了无人机辅助网络中多智能体近端策略优化的数据实时性问题，通过协作强化学习优化无人机的轨迹设计和对物联网设备的访问，并成功地最小化了全局更新年龄（AoU）。 |
| [^19] | [Visual Prompt Based Personalized Federated Learning.](http://arxiv.org/abs/2303.08678) | 本文提出了一种基于视觉提示的PFL框架pFedPT，通过利用个性化的视觉提示隐含地表示客户端的本地数据分布信息，并将该信息提供给聚合模型来帮助分类任务，相对于其他算法取得了更好的效果。 |
| [^20] | [Fashion-model pose recommendation and generation using Machine Learning.](http://arxiv.org/abs/2303.08660) | 本研究使用机器学习算法将图像分割，并为时尚人员建议与输入图像相似的摆姿，同时扩展了工作生成合成图像。 |
| [^21] | [Pixel-Level Explanation of Multiple Instance Learning Models in Biomedical Single Cell Images.](http://arxiv.org/abs/2303.08632) | 本文研究了四种属性方法来解释医学单细胞图像多实例学习模型，并在血癌患者的血液涂片中推导出像素级别的血癌诊断解释。 |
| [^22] | [Smoothed Q-learning.](http://arxiv.org/abs/2303.08631) | 本文提出了一种能够缓解Q学习算法高估问题的平滑Q学习算法。 |
| [^23] | [From Images to Features: Unbiased Morphology Classification via Variational Auto-Encoders and Domain Adaptation.](http://arxiv.org/abs/2303.08627) | 本研究提出了一种无偏态的星系形态分类方法，利用变分自编码器和域适应实现低维度表示，40维潜在变量能够有效再现星系图像中的大多数形态特征，并通过经典随机森林分类器实现了详细的形态特征分类。此外，该方法可以无偏地应用于两个不同的星系调查中。 |
| [^24] | [Interpretable Ensembles of Hyper-Rectangles as Base Models.](http://arxiv.org/abs/2303.08625) | 本文提出了一种基于超矩形的可解释的集成模型，它将均匀生成的轴对齐超矩形作为基模型，并成功地避免了过拟合。 |
| [^25] | [Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer.](http://arxiv.org/abs/2303.08622) | 本文提出了一种适用于文本引导图像风格迁移中的零样本对比损失方法，可以在不需要额外训练的情况下生成具有相同语义内容的图像。 |
| [^26] | [Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model.](http://arxiv.org/abs/2303.08613) | 本文设计了一种样本高效算法，将 UCB 算法（Auer等人，2002）应用于委托代理模型的在线设置，该算法能够通过与策略代理多次互动来设计最优的计分规则，并实现良好的效果。 |
| [^27] | [Automatic Attention Pruning: Improving and Automating Model Pruning using Attentions.](http://arxiv.org/abs/2303.08595) | 本文提出一种自适应、基于注意力的结构化剪枝方法，以自动产生小、准确和硬件有效的模型以满足用户的目标。 |
| [^28] | [Delay-SDE-net: A deep learning approach for time series modelling with memory and uncertainty estimates.](http://arxiv.org/abs/2303.08587) | 本研究提出了一种基于随机延迟微分方程的神经网络模型Delay-SDE-net，可以准确地建模具有记忆效应的时间序列，并且能够对模型的不确定性进行实时估计。 |
| [^29] | [High-dimensional multi-view clustering methods.](http://arxiv.org/abs/2303.08582) | 本论文比较了两类高维多视角聚类方法（基于图和基于子空间），重点关注了如何处理高阶相关性，并在基准数据集上进行了实验研究。 |
| [^30] | [Investigating GANsformer: A Replication Study of a State-of-the-Art Image Generation Model.](http://arxiv.org/abs/2303.08577) | 本文通过重新创建一种新的GAN变体GANformer，并对作者的声明进行评论，以探究最先进的图像生成模型。 |
| [^31] | [WikiCoder: Learning to Write Knowledge-Powered Code.](http://arxiv.org/abs/2303.08574) | WikiCoder是一种利用知识图谱进行程序合成的系统，可以自动从输入输出例子中学习代码，用于解决需要使用外部知识的问题，能够解决以前无法解决的任务。 |
| [^32] | [Distinguishing Cause from Effect on Categorical Data: The Uniform Channel Model.](http://arxiv.org/abs/2303.08572) | 本文提出了一种名为“均匀通道模型”的方法，用于区分分类数据中的因果关系，该方法将条件概率质量函数视为离散无记忆通道，并选择最可能的因果方向使得条件概率质量函数更接近于均匀分布。 |
| [^33] | [Sensitivity-Aware Visual Parameter-Efficient Tuning.](http://arxiv.org/abs/2303.08566) | 本文提出了敏感度感知的视觉参数低效调整（SPT）方案，可以自适应地将可训练参数分配到任务特定的重要位置，以提高表示能力，适应预训练视觉模型到下游任务。 |
| [^34] | [Joint Graph and Vertex Importance Learning.](http://arxiv.org/abs/2303.08552) | 本研究提出了一种新颖的方法学习图形，并产生了比其他方法更为稀疏和可解释的模型。 |
| [^35] | [Adapting U-Net for linear elastic stress estimation in polycrystal Zr microstructures.](http://arxiv.org/abs/2303.08541) | 该论文提出了一种适用于多晶Zr微结构中线性弹性应力估计的U-Net神经网络架构。网络的性能不受晶粒结构规则性或纹理影响，可以推广到任意Zr晶体结构，与有限元分析相比，具有更快的速度（大约200x至6000x）和较小的内存开销，但最高精度可能会降低10％。 |
| [^36] | [Health Monitoring of Movement Disorder Subject based on Diamond Stacked Sparse Autoencoder Ensemble Model.](http://arxiv.org/abs/2303.08538) | 本文提出了一种基于Diamond Stacked稀疏自编码器集成模型的健康监测算法，通过利用特征嵌入堆叠稀疏自编码器（FSSAE）进行特征扩展和使用Diamond层结构去除冗余和非信息特征，有效地提高了运动障碍患者的诊断精度。 |
| [^37] | [Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring.](http://arxiv.org/abs/2303.08536) | 本论文研究了音视频语音识别在多模态输入损坏情况下的问题，并设计了音视频可靠性评分模块来提高模型的韧性。 |
| [^38] | [Singular relaxation of a random walk in a box with a Metropolis Monte Carlo dynamics.](http://arxiv.org/abs/2303.08535) | 本文分析了一个在盒子中移动的粒子的蒙特卡罗算法的弛豫本征模式，发现当跳跃长度与盒子大小相当时，弛豫本征模式数量可能非常少，初始条件的对称性的合适选择可以使得向平衡状态的局部化衰减。 |
| [^39] | [Fair Off-Policy Learning from Observational Data.](http://arxiv.org/abs/2303.08516) | 本文针对非自助学习的算法公平性问题，提出了一种新的公平非自助学习框架，可以从偏见可能存在的观测数据中学习决策规则。 |
| [^40] | [The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models.](http://arxiv.org/abs/2303.08500) | 保护个人隐私信息是很重要的，但规避扩散模型中添加的噪声对数据进行保护存在挑战。AVATAR算法借助扩散模型的威力，提供了一种精心设计的去噪过程来消除数据保护扰动的影响，并获得了在多个数据集上的最先进的性能。 |
| [^41] | [Unsupervised Traffic Scene Generation with Synthetic 3D Scene Graphs.](http://arxiv.org/abs/2303.08473) | 本文提出了一种基于领域不变场景表示的方法，使用合成场景图直接合成逼真的交通场景，提高了合成场景图的空间信息，并通过场景操作证明了我们的方法的有效性。 |
| [^42] | [Hybrid-Physical Probabilistic Forecasting for a Set of Photovoltaic Systems using Recurrent Neural Networks.](http://arxiv.org/abs/2303.08459) | 本文提出了一种基于递归神经网络的光伏电池组混合物理概率预测模型，通过使用数值天气预测结果作为协变量，改善了光伏系统功率输出的准确性，最终可以达到7.54％的技能评分。 |
| [^43] | [On the uncertainty analysis of the data-enabled physics-informed neural network for solving neutron diffusion eigenvalue problem.](http://arxiv.org/abs/2303.08455) | 本文针对数据不可避免带有噪声的情况，研究了DEPINN在求解中子扩散本征值问题方面的可行性，并提出了创新的区间损失函数用于减少噪声影响和提高先验数据利用率，此方法在两个基准问题上得到了验证。 |
| [^44] | [Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection.](http://arxiv.org/abs/2303.08452) | 提出了一种无监督的异常检测方法 PHANES，它可以将健康组织保留并用伪健康重构替换异常区域，有效检测中风损伤。 |
| [^45] | [A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records.](http://arxiv.org/abs/2303.08448) | 本研究通过对乳腺癌表型提取任务的评估，展示了基于BERT的临床NLP模型在不同临床环境中具有良好的泛化能力，并强调了使用转移学习开发广义临床NLP模型的潜力。 |
| [^46] | [MAHTM: A Multi-Agent Framework for Hierarchical Transactive Microgrids.](http://arxiv.org/abs/2303.08447) | 该论文提出了一个多智能体强化学习框架，用于管理微电网中的能源交易。该框架通过最小化碳足迹，同时平衡可再生能源和传统能源的消费和生产，并考虑能源变化。 |
| [^47] | [Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models.](http://arxiv.org/abs/2303.08440) | 本论文提出了一种使用两个预训练的垂直二维扩散模型来解决三维逆问题的方法，将三维数据分布建模为在不同方向上切片的二维分布的乘积，解决了维数的灾难性问题，并以高效的方式用于医学图像重建任务。 |
| [^48] | [Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields.](http://arxiv.org/abs/2303.08435) | 本文提出了一种新的基于机器学习的光刻模型范式，通过优化复值神经场执行光学核回归并将光刻系统拆解为非参数掩模操作和包含行列式源、瞳孔和光刻信息的学习光学核，使用小规模训练数据集展示了卓越的推广能力。 |
| [^49] | [DeDA: Deep Directed Accumulator.](http://arxiv.org/abs/2303.08434) | 本文提出了一种针对特定类型病变的深度有向累加器 (DeDA) 对神经网络注入领域特定的归纳偏差实现对该类型病变的精确识别。 |
| [^50] | [The Benefits of Mixup for Feature Learning.](http://arxiv.org/abs/2303.08433) | 本论文介绍了数据增强方法Mixup对于特征学习的益处。混合训练可以有效地从混合数据中学习罕见特征，相比之下，标准训练可能会漏掉这些罕见特征。 |
| [^51] | [Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators.](http://arxiv.org/abs/2303.08431) | 本论文研究了强化学习方法在几乎线性二次型调节器系统中找到最优策略的问题，提出了一个策略梯度算法，可以以线性速率收敛于全局最优解。 |
| [^52] | [DualFair: Fair Representation Learning at Both Group and Individual Levels via Contrastive Self-supervision.](http://arxiv.org/abs/2303.08403) | 本文提出了一种自我监督模型DualFair，该模型使用对比损失来生成对每个受保护群体不可区分的嵌入，并联合优化了群体公平性和反事实公平性两个公平性标准，使得在群体和个体级别上做出更公正的预测，该模型在公平智能领域中具有潜在价值。 |
| [^53] | [Generating symbolic music using diffusion models.](http://arxiv.org/abs/2303.08385) | 本文提出了一种使用扩散模型生成钢琴卷帘的方法，可以协调、生成、完善音乐；代码已公开共享。 |
| [^54] | [MCR-DL: Mix-and-Match Communication Runtime for Deep Learning.](http://arxiv.org/abs/2303.08374) | 该论文提出了MCR-DL，一种用于深度学习的可扩展混合通信框架，能够支持各种集合和点对点操作，并允许用户动态地混合和匹配通信后端以进行给定操作，从而提高深度学习模型的训练效率。 |
| [^55] | [Transfer Learning Based Diagnosis and Analysis of Lung Sound Aberrations.](http://arxiv.org/abs/2303.08362) | 本研究通过迁移学习技术，开发了一种基于CNN的方法，使用Mel频率倒谱系数（MFCCs）进行分类，实现对肺部呼吸音的非侵入性检测，准确率达到95%。 |
| [^56] | [Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks.](http://arxiv.org/abs/2303.08361) | 提出了一种合作式联邦学习(CFL)范式，通过设备间的合作来抵消边缘/雾网络中的异构性，从而提高机器学习模型训练质量和网络资源利用率。 |
| [^57] | [Efficient and Secure Federated Learning for Financial Applications.](http://arxiv.org/abs/2303.08355) | 本文提出了一种金融应用的高效安全联合学习方法，其中包括Top-K梯度稀疏化和基于Delta的稀疏化，以及一种可保护数据隐私的聚合框架，这些方法可以大幅降低通信成本并保证预测精度。 |
| [^58] | [Sharing Low Rank Conformer Weights for Tiny Always-On Ambient Speech Recognition Models.](http://arxiv.org/abs/2303.08343) | 本论文提出了一种基于低秩张量分享的模型方法，将大型语音识别模型缩小至5M参数，同时实现了在低内存神经处理器边缘设备上的始终处于运行状态的语音识别。 |
| [^59] | [Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting.](http://arxiv.org/abs/2303.08331) | 本论文提出了一种利用空间时间信息来提高视频超分辨率的新方法，采用高维卷积网络进行预测并应用时间注意机制以去除冗余信息并提高效率。 |
| [^60] | [FairAdaBN: Mitigating unfairness with adaptive batch normalization and its application to dermatological disease classification.](http://arxiv.org/abs/2303.08325) | 本文提出了FairAdaBN，将批归一化适应敏感属性，可以将其简单而有效地应用到原本不了解公平性的多个分类主干中，能够有效地减少不公平性，并实现模型性能和公平性之间的平衡。 |
| [^61] | [Optimization Design for Federated Learning in Heterogeneous 6G Networks.](http://arxiv.org/abs/2303.08322) | 联邦学习是实现6G网络人工智能普及的关键技术，但在6G网络中，存在一些系统和统计异构性挑战。本研究研究了能够有效解决这些异构性问题的优化方法。 |
| [^62] | [SegPrompt: Using Segmentation Map as a Better Prompt to Finetune Deep Models for Kidney Stone Classification.](http://arxiv.org/abs/2303.08303) | 本文提出了SegPrompt，可以使用分割图加强分类模型的训练，同时使用分割图来微调预训练的深度模型，从而大大减少可训练参数，显著提高了分类性能，同时需要比以前的方法少得多的注释数据。 |
| [^63] | [A Comprehensive Study on Post-Training Quantization for Large Language Models.](http://arxiv.org/abs/2303.08302) | 本文基于数万个零-shot实验对基于后训练量化的大型语言模型的不同量化组件进行了综合研究，结果发现细粒度量化和后训练量化方法很重要，用粗粒度量化的更高位数比用非常细粒度的更低位数更强大。我们给出了如何为不同大小的\llms利用量化的建议。 |
| [^64] | [Learning From High-Dimensional Cyber-Physical Data Streams for Diagnosing Faults in Smart Grids.](http://arxiv.org/abs/2303.08300) | 本文采用特征工程相结合的方法解决了网络物理电力系统数据质量、计算成本和冗余测量数据的问题，提高了故障诊断系统的性能。 |
| [^65] | [Machine Learning Approaches in Agile Manufacturing with Recycled Materials for Sustainability.](http://arxiv.org/abs/2303.08291) | 本文提出将回收再利用材料用于敏捷制造，应用机器学习模型进行预测分析，以决策支持实现环境可持续性。 |
| [^66] | [Rediscovery of CNN's Versatility for Text-based Encoding of Raw Electronic Health Records.](http://arxiv.org/abs/2303.08290) | 本文发现，CNN在健康记录文本编码方面的多功能性和隐含层次结构可以提高其性能，提出了一种基于CNN的编码器来处理不同类型的EHR特征，并在临床任务中展示了其有效性。 |
| [^67] | [Improving Adversarial Robustness with Hypersphere Embedding and Angular-based Regularizations.](http://arxiv.org/abs/2303.08289) | 本文提出了一种新的对抗训练方法，角度-AT，结合超球嵌入和基于角度的正则化技术以提高深度神经网络的对抗鲁棒性能。 |
| [^68] | [Attention-likelihood relationship in transformers.](http://arxiv.org/abs/2303.08288) | 本文分析了Transformer中标记可能性和注意力值之间的关联，揭示了在遇到意外标记时模型关注较少的信息，对于评估LLMs在现实世界场景中的稳健性具有有价值的影响。 |
| [^69] | [Linking Alternative Fuel Vehicles Adoption with Socioeconomic Status and Air Quality Index.](http://arxiv.org/abs/2303.08286) | 该研究借助机器学习技术，探究替代燃料汽车的普及，同时将其与消费者的社会经济地位和空气质量指数进行关联，从而制定合适的政策。 |
| [^70] | [Towards a Deep Learning Pain-Level Detection Deployment at UAE for Patient-Centric-Pain Management and Diagnosis Support: Framework and Performance Evaluation.](http://arxiv.org/abs/2303.08273) | 本文提出了一个能在UAE部署的疼痛级别检测框架，并证明了它能够准确地识别疼痛级别，有助于以患者为中心的医疗计划。 |
| [^71] | [Automated patent extraction powers generative modeling in focused chemical spaces.](http://arxiv.org/abs/2303.08272) | 本研究通过开发自动化管道，使用专利数据源训练领域特定的生成模型，利用专利中的弱标记应用类别中尽可能多的信息实现化学空间内生成建模。 |
| [^72] | [Act-Then-Measure: Reinforcement Learning for Partially Observable Environments with Active Measuring.](http://arxiv.org/abs/2303.08271) | 本论文研究了对于代理有直接控制何时以及如何收集信息的能力的马尔可夫决策过程。我们引入了行动后测量 (ATM) 策略，并开发了一个基于 ATM 启发式方法的强化学习算法，展示了其在多个部分可观察环境上优于先前的方法的卓越性能。 |
| [^73] | [Model-to-Circuit Cross-Approximation For Printed Machine Learning Classifiers.](http://arxiv.org/abs/2303.08255) | 本文提出了一个自动化的、面向特定体系结构的跨层逼近框架，能在PE中实现复杂的ML模型，压缩电路大小高达95%，分类精度损失平均在4%内。 |
| [^74] | [R^2: Range Regularization for Model Compression and Quantization.](http://arxiv.org/abs/2303.08253) | R^2提出了一种基于区间正则化的新方法，利用有效的最小值和最大值调整权重分布，从而使模型压缩和量化技术能够更好地利用其数值表示能力。该方法可以提高模型优化的质量，尤其是在较低位上。 |
| [^75] | [Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning.](http://arxiv.org/abs/2303.08250) | 本文提出了一种在Vision Transformer中学习成长人工海马的方法，以实现弹性终身学习。通过神经架构搜索进行维护，选取多头自注意力块中的最终线性投影层进行ArtiHippo的实现和成长。 |
| [^76] | [Systematic design space exploration by learning the explored space using Machine Learning.](http://arxiv.org/abs/2303.08249) | 本文提出了一种利用机器学习算法来学习已探索数据空间，跟踪参数探索状态的方法，通过修正的鲁棒随机切割森林和启发式方法在二维欧几里得空间中进行了验证。 |
| [^77] | [Optimal Sampling Designs for Multi-dimensional Streaming Time Series with Application to Power Grid Sensor Data.](http://arxiv.org/abs/2303.08242) | 本文提出针对多维流式时间序列的最优抽样设计方法，并应用于高速电力消耗数据的低成本实时分析，提高了计算效率。 |
| [^78] | [Bayesian Beta-Bernoulli Process Sparse Coding with Deep Neural Networks.](http://arxiv.org/abs/2303.08230) | 本文提出了一种基于Beta-Bernoulli过程和非参数迭代算法的深度稀疏编码模型，旨在学习具有尺度不变性的离散特征，并鼓励表示的稀疏性。 |
| [^79] | [Hall effect thruster design via deep neural network for additive manufacturing.](http://arxiv.org/abs/2303.08227) | 本文介绍了一种借助深度神经网络进行霍尔效应推进器设计的方法，可用于轻松获得所需特性的设计，使用的计算资源更少，比通常设计方法更为灵活。 |
| [^80] | [DeepAxe: A Framework for Exploration of Approximation and Reliability Trade-offs in DNN Accelerators.](http://arxiv.org/abs/2303.08226) | DeepAxe是一个用于在DNN加速器的设计空间中考虑近似和可靠性权衡的框架，逼近可靠性关键的DNN，并提供一组Pareto最优的DNN实现设计空间。 |
| [^81] | [Few-Shot Classification of Autism Spectrum Disorder using Site-Agnostic Meta-Learning and Brain MRI.](http://arxiv.org/abs/2303.08224) | 本文研究了针对自闭症谱系障碍的场地不可知元学习模型在少样本情况下的表现，使用在多个站点的MRI数据训练出的模型，实现对病人和正常人的快速识别分类。 |
| [^82] | [A 2-opt Algorithm for Locally Optimal Set Partition Optimization.](http://arxiv.org/abs/2303.08219) | 该论文提出了一个局部最优的集合分割问题版本，并开发了一个$O(N^2)$时间的2-opt算法，可以处理任意输入精度，具有广泛的应用场景。 |
| [^83] | [Efficiently Training Vision Transformers on Structural MRI Scans for Alzheimer's Disease Detection.](http://arxiv.org/abs/2303.08216) | 本文尝试使用Vision Transformers对基于MRI扫描的性别和AD分类任务进行了测试，其中两种ViT架构变体分别实现了0.987的性别分类AUC和0.892的AD分类AUC。该方法可为大规模神经影像学识别提供高效解决方案。 |
| [^84] | [Is forgetting less a good inductive bias for forward transfer?.](http://arxiv.org/abs/2303.08207) | 本文提出对于持续学习任务来说，遗忘不是一种良好的归纳偏差。之前的研究没有考虑到前向迁移的量度方式，本文提出了一种新的量度方式，发现较不遗忘的模型具有更好的性能。 |
| [^85] | [RODD: Robust Outlier Detection in Data Cubes.](http://arxiv.org/abs/2303.08193) | 本文提出了第一个评估数据立方体中鲁棒异常检测方法的框架RODD，介绍了一种新的基于随机森林的异常检测方法RODD-RF，并将其应用于真实世界的数据中，结果表明RODD-RF可以导致更好的异常检测。 |
| [^86] | [Vehicle lateral control using Machine Learning for automated vehicle guidance.](http://arxiv.org/abs/2303.08187) | 本研究使用机器学习模型设计了车辆横向控制器，在模拟器上训练模型并使用随机森林模型预测置信度/不确定性，成功应用于自动引导车辆，具有非常好的泛化能力。 |
| [^87] | [The Elements of Visual Art Recommendation: Learning Latent Semantic Representations of Paintings.](http://arxiv.org/abs/2303.08182) | 本文研究了如何高效地捕捉视觉艺术的元素，提出了结合文本和视觉特征学习技术的推荐系统，用于个性化艺术品推荐，结果显示两者的结合可以捕捉最合适的隐藏语义关系。 |
| [^88] | [Allegro-Legato: Scalable, Fast, and Robust Neural-Network Quantum Molecular Dynamics via Sharpness-Aware Minimization.](http://arxiv.org/abs/2303.08169) | Allegro-Legato 是一种基于机器学习的NNQMD模型，使用 Sharpness-Aware Minimization 解决了计算机多核心处理器架构下的精度扩展问题，大大提高了模型的普适性和可靠性。 |
| [^89] | [Graph Neural Network Surrogates of Fair Graph Filtering.](http://arxiv.org/abs/2303.08157) | 通过引入过滤器感知的通用近似框架，该方法定义了合适的图神经网络在运行时训练以满足统计平等约束，同时最小程度扰动原始后验情况下实现此目标。 |
| [^90] | [Performance Embeddings: A Similarity-based Approach to Automatic Performance Optimization.](http://arxiv.org/abs/2303.08142) | 本文提出了性能嵌入的方法，通过构建子程序的嵌入空间来实现性能优化的直接知识传输。传输调整将搜索复杂度降低了多达四个数量级，并在稀疏-密集矩阵乘法中优于MKL库。 |
| [^91] | [Digital staining in optical microscopy using deep learning -- a review.](http://arxiv.org/abs/2303.08140) | 传统的生物染色协议面临着许多挑战，数字染色通过利用深度学习将光学对比转化为实际染色的基础对比成为解决方案。 |
| [^92] | [Information-Theoretic Regret Bounds for Bandits with Fixed Expert Advice.](http://arxiv.org/abs/2303.08102) | 本文研究固定专家建议下的赌博机问题，提出了基于信息论的遗憾界限，可以使得某些算法的遗憾无限接近于零。此外，我们还提出了KL散度来描述专家之间的相似性界限，并给出了下限证明算法的最优性。 |
| [^93] | [Generalised Scale-Space Properties for Probabilistic Diffusion Models.](http://arxiv.org/abs/2303.07900) | 本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。 |
| [^94] | [Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review.](http://arxiv.org/abs/2303.07647) | 本文综述了近年来机器学习在实验固体力学领域中的最新进展和应用，提供了物理感知和基于物理学的机器学习方法，并涵盖了断裂力学、生物力学、纳米和微观力学、构建材料和二维材料等传统和新兴领域。 |
| [^95] | [Self-supervised based general laboratory progress pretrained model for cardiovascular event detection.](http://arxiv.org/abs/2303.06980) | 研究利用自监督学习和迁移学习，将心血管实验室指标的患者进展趋势从常见情况转移到罕见或特定心血管事件检测中，以协助检测经皮冠状动脉介入治疗患者的靶血管重建。 |
| [^96] | [Stabilizing and Improving Federated Learning with Non-IID Data and Client Dropout in IoT Systems.](http://arxiv.org/abs/2303.06314) | 本文提出了一个简单而有效的框架，通过引入一个先验校准的softmax函数来计算交叉熵损失和基于原型的特征提取来维护一个平衡的分类器头，以稳定和改进联邦学习。 |
| [^97] | [Interpretable Outlier Summarization.](http://arxiv.org/abs/2303.06261) | STAIR提出了一种可解释的异常值汇总方法，通过学习一组紧凑的人类可理解规则，以汇总和解释异常检测结果，具有强大的可解释性，以准确地总结检测结果。 |
| [^98] | [Meta-learning Control Variates: Variance Reduction with Limited Data.](http://arxiv.org/abs/2303.04756) | 该论文提出了一种元学习控制变量的方法，可在有限数据的情况下减小蒙特卡罗估计器的方差，并对多个任务进行处理。 |
| [^99] | [Vector Quantized Time Series Generation with a Bidirectional Prior Model.](http://arxiv.org/abs/2303.04743) | 本文提出了一种新的时间序列生成方法，使用向量量化技术和双向变压器模型来生成质量更好、模块化变化更快的合成信号。 |
| [^100] | [Towards Improved Illicit Node Detection with Positive-Unlabelled Learning.](http://arxiv.org/abs/2303.02462) | 本文提出基于正样本未标注学习的改进非法节点检测方法，通过探讨隐藏正样本的标签机制假设，与一系列图表示学习方法相结合，实现了更可靠的结果。 |
| [^101] | [Constrained Bayesian Optimization for Automatic Underwater Vehicle Hull Design.](http://arxiv.org/abs/2302.14732) | 本文研究了自主水下载体的优化设计问题，通过集成FreeCAD和OpenFoam等工具进行自动化设计评估，并采用贝叶斯优化算法解决了优化中样本效率的问题。 |
| [^102] | [GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting.](http://arxiv.org/abs/2302.14307) | 提出了一种名为GradMA的方法，基于梯度内存加速联邦学习，并通过持续学习来解决灾难性遗忘问题，实验表明在模型准确性、通信效率和灾难性遗忘缓解方面都有显著提高。 |
| [^103] | [Approximately optimal domain adaptation with Fisher's Linear Discriminant Analysis.](http://arxiv.org/abs/2302.14186) | 本文提出了一种基于Fisher线性判别的领域自适应模型，该模型是两个假设的凸组合，可以在不访问任何单个源任务的直接信息的情况下计算最优分类器，并在基于EEG和ECG的分类设置中展示了其有效性。 |
| [^104] | [FTM: A Frame-level Timeline Modeling Method for Temporal Graph Representation Learning.](http://arxiv.org/abs/2302.11814) | FTM 是一种帧级时间线建模方法，可同时捕获时间图中的短期和长期特征，并在不同任务和数据集上表现出比其他现有方法更好的性能。 |
| [^105] | [FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering.](http://arxiv.org/abs/2302.11799) | 本文提出了一个Fine-grained Two-stage训练框架（FiTs），用于解决知识感知问答（KAQA）中，从语言模型和知识图谱中获得的两种不同类型的知识在表示上的差异和联合推理的困难问题。 |
| [^106] | [DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images.](http://arxiv.org/abs/2302.10390) | DrasCLR是一个自监督框架，通过提出两种领域特定的对比学习策略来学习疾病相关和解剖特异性表示，特别是解决了区分疾病模式和解剖特征的挑战。 |
| [^107] | [Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift.](http://arxiv.org/abs/2302.10160) | 该论文提出了一种关于核岭回归的协变量转移策略，通过使用伪标签进行模型选择，能够适应不同特征分布下的学习，实现均方误差最小化。 |
| [^108] | [Understanding Multimodal Contrastive Learning and Incorporating Unpaired Data.](http://arxiv.org/abs/2302.06232) | 本论文研究了一般类的非线性损失函数进行多模式对比学习，揭示了其与奇异值分解的联系。并证明在错误匹配的情况下，多模式对比学习可以比单模式对比学习表现更佳，表现了其对嘈杂数据的鲁棒性。 |
| [^109] | [Offline Learning of Closed-Loop Deep Brain Stimulation Controllers for Parkinson Disease Treatment.](http://arxiv.org/abs/2302.02477) | 本研究针对DBS治疗帕金森氏症的问题，提出了一种基于离线强化学习框架的闭环深度脑电刺激控制器，以动态调整治疗幅度，减少能量使用，并检测其安全性和性能。 |
| [^110] | [Measuring The Impact Of Programming Language Distribution.](http://arxiv.org/abs/2302.01973) | 该研究提出了BabelCode框架和Translating Python Programming Puzzles（TP3）基准测试，探讨了平衡训练数据集中14种编程语言分布的影响。结果显示平衡分布有助于大型语言模型在低资源语言上的性能提升。 |
| [^111] | [Robust online active learning.](http://arxiv.org/abs/2302.00422) | 本文提出了一种自适应方法，用于鲁棒的在线主动学习，并在受污染的数据流中证明了其性能表现优异，同时确保了稳定性并减少异常值的负面影响。 |
| [^112] | [Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation.](http://arxiv.org/abs/2301.13428) | 本文提出了一个无源域适应的实用且具有挑战性的方案，通过在原始特征空间聚类，构建真正困难的负对，结合噪声对比估计理论，学习一个域不变的特征来解决域上的差异问题，能够在三个常见的基准数据集上实现有效结果。 |
| [^113] | [Flex-Net: A Graph Neural Network Approach to Resource Management in Flexible Duplex Networks.](http://arxiv.org/abs/2301.11166) | 本文提出了一种图神经网络模型—Flex-Net，用于解决灵活双工网络中资源管理问题，通过联合优化通信方向和传输功率，实现了高性能和低计算复杂度。 |
| [^114] | [SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning.](http://arxiv.org/abs/2301.10921) | 本文提出了SoftMatch，通过在训练中保持高数量和高质量的伪标签来克服半监督学习中数量-质量权衡问题，有效地利用未标注的数据。 在实验中，SoftMatch在图像、文本和影片等多个基准测试中都显示了实质性的改进。 |
| [^115] | [Age of Information in Deep Learning-Driven Task-Oriented Communications.](http://arxiv.org/abs/2301.04298) | 本文研究了在任务导向通信中通过编码器-解码器对来执行任务的信息时代问题，通过信道利用率的增加可以提高准确性，但需要更长的服务时间，引入任务信息的最大时代（PAoTI）来对准确性和延迟进行权衡。 |
| [^116] | [Multimodal Lyrics-Rhythm Matching.](http://arxiv.org/abs/2301.02732) | 本文提出了一种多模式歌词-节奏匹配方法，采用音频而不是有可用元数据的谱面，可将歌词和音乐的关键部分相互匹配，包括音乐的强节拍、歌词音节和歌词关键词，并在数据集实验中表现出优越性能。 |
| [^117] | [Online Active Learning for Soft Sensor Development using Semi-Supervised Autoencoders.](http://arxiv.org/abs/2212.13067) | 本文介绍了一种使用半监督自编码器以及在线主动学习方法，以尽可能少的标记样本来开发软测量传感器，从而显著降低了成本。在实验中，作者表明这种方法能够取得好的预测效果。 |
| [^118] | [Policy learning "without'' overlap: Pessimism and generalized empirical Bernstein's inequality.](http://arxiv.org/abs/2212.09900) | 本文提出了一种新的离线策略学习算法，它不需要统一交叠假设，而是利用价值的下限置信区间（LCBs）优化策略，因此能够适应允许行为策略演变和倾向性减弱的情况。 |
| [^119] | [Policy Adaptation from Foundation Model Feedback.](http://arxiv.org/abs/2212.07398) | 本文提出了基于基础模型反馈的策略适应（PAFF）方法，通过让策略使用随机生成的指令进行演示，并利用预训练的基础模型提供反馈来重新标记演示，自动提供新的演示-指令数据对进行策略微调，以实现机器人操作的泛化。实验结果表明，PAFF优于现有最先进的方法。 |
| [^120] | [FretNet: Continuous-Valued Pitch Contour Streaming for Polyphonic Guitar Tablature Transcription.](http://arxiv.org/abs/2212.03023) | FretNet是一种新的自动化吉他谱转录方法，提供了连续值音高轮廓流和各种吉他技巧的估计，通过在多个数据集上测试，结果表明它比其他方法具有更高的平均精度和更少的模型复杂度。 |
| [^121] | [Heterogeneous Graph Learning for Multi-modal Medical Data Analysis.](http://arxiv.org/abs/2211.15158) | 该论文提出了一种名为HetMed的异构图学习框架，用于融合多模态医学数据，以提高临床决策的准确性。 |
| [^122] | [Biologically-Inspired Continual Learning of Human Motion Sequences.](http://arxiv.org/abs/2211.05231) | 本文提出了一个受生物启示的条件时间变分自动编码器(BI-CTVAE)模型，通过持续学习生成(CL2Gen)场景，可以对不同类别的运动序列进行生成，并在一组任务上得到较高的生成准确性和分类准确性。 |
| [^123] | [Efficient Compressed Ratio Estimation using Online Sequential Learning for Edge Computing.](http://arxiv.org/abs/2211.04284) | 本研究提出了一种基于AC-OSELM的高效RL方法，能够在边缘设备上估计合适的压缩比，从而实现数据的高效压缩，同时保持重构数据的准确性。 |
| [^124] | [Thunderstorm nowcasting with deep learning: a multi-hazard data fusion model.](http://arxiv.org/abs/2211.01001) | 这项研究提出了一种深度学习模型，可以用于适应不同类型的暴风雨预测，利用多种数据源进行数据融合，并能预测雷电、冰雹和暴雨的概率，其中天气雷达产品是最重要的预测因素。 |
| [^125] | [Recurrent Neural Networks and Universal Approximation of Bayesian Filters.](http://arxiv.org/abs/2211.00335) | 本文提出一个循环神经网络框架，用于直接从观测输入到所需的估计器统计量学习递归映射，可以近似估计潜在时间序列信号的条件统计量，在非紧致域中有误差界限，在长时间上有良好性能。 |
| [^126] | [Training Neural Networks for Sequential Change-point Detection.](http://arxiv.org/abs/2210.17312) | 本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。 |
| [^127] | [Similarity of Neural Architectures Based on Input Gradient Transferability.](http://arxiv.org/abs/2210.11407) | 本研究利用对抗攻击传递度量，设计了一个量化且可扩展的神经架构相似度函数，分析了69个最先进的ImageNet分类器，发现多样化的神经架构可以提高模型集合和知识蒸馏的性能。 |
| [^128] | [Visual Reinforcement Learning with Self-Supervised 3D Representations.](http://arxiv.org/abs/2210.07241) | 这篇论文提出了一个自监督学习三维表示的方法来解决视觉强化学习中的样本效率和泛化问题，通过预训练和微调两个阶段，相比于二维表示学习方法，该方法在操作任务中具有更高的样本效率并且更好地泛化到其他情况下的RL。 |
| [^129] | [MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting.](http://arxiv.org/abs/2210.07179) | MAPL使用对齐的图像-文本数据学习单模态模型表示空间之间的轻量级映射，从而实现了面向视觉-语言少样本任务的基于参数效率的适应，并在测试中显示出优越的性能表现。 |
| [^130] | [Null Hypothesis Test for Anomaly Detection.](http://arxiv.org/abs/2210.02226) | 本论文提出一种基于零假设检验的异常检测方法，能够排除仅背景假设，依赖于特征和区域的条件独立性假设，展现了出色的性能，适用于各种信号分数情况下的数据检测。 |
| [^131] | [Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications.](http://arxiv.org/abs/2210.01162) | 本文提出了一个模型自由框架，使用深度强化学习来实现复杂高级任务的目标驱动导航。通过将先前的多目标DRL问题转化为一个单一目标问题，并使用基于采样的路径规划算法来指导DRL智能体，该方法可以满足不可行的线性时态逻辑任务并尽可能减少违规。 |
| [^132] | [On Stability and Generalization of Bilevel Optimization Problem.](http://arxiv.org/abs/2210.01063) | 本文对二层优化问题的一阶（基于梯度的）方法进行了全面泛化分析，建立了算法稳定性与泛化误差之间的基本联系，并提出了高概率泛化界，将其从$ \bigO(\sqrt{n}) $改善为$ \bigO(\log n) $，同时也提出了第一个泛化界限。 |
| [^133] | [Gradient Gating for Deep Multi-Rate Learning on Graphs.](http://arxiv.org/abs/2210.00513) | G2是一种利用梯度门控机制的新型GNN框架，可缓解过度平滑问题，并实现了各种图学习任务上的最先进性能。 |
| [^134] | [Generalization in Neural Networks: A Broad Survey.](http://arxiv.org/abs/2209.01610) | 这篇论文总结了神经网络模型中不同抽象层次上的泛化问题及其方法，其中样本泛化已经取得了进展，但未来需要重点关注减少过拟合；分布泛化与领域泛化有相似之处，领域泛化方法可以应用于困难的样本或分布泛化。 |
| [^135] | [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.](http://arxiv.org/abs/2208.12242) | DreamBooth是一种针对主题驱动的文本到图像扩散模型个性化方法，通过微调预训练的文本到图像模型，使用样本图像来实现生成逼真图像的独特标识符绑定，使其可以在不同场景中合成该主题的新版逼真图像。 |
| [^136] | [Masked Vision and Language Modeling for Multi-modal Representation Learning.](http://arxiv.org/abs/2208.02131) | 本文提出了联合遮蔽视觉和语言建模，在跨模态对齐方面取得成果，并在百万级别的预训练数据范围内取得了最先进的性能。 |
| [^137] | [Quantifying the Effect of Feedback Frequency in Interactive Reinforcement Learning for Robotic Tasks.](http://arxiv.org/abs/2207.09845) | 本文对交互式强化学习中反馈频率对机器人任务的影响进行了定量化研究，结果表明没有单一的理想反馈频率存在，应该根据具体的任务和机器人复杂性进行调整。 |
| [^138] | [Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models.](http://arxiv.org/abs/2207.06950) | 本文提出了一种新算法GAMI-Tree，使用基于模型的树以及新的交互过滤方法，可以更好地拟合底层交互，具有更好的预测性能和更高的效率。 |
| [^139] | [Betty: An Automatic Differentiation Library for Multilevel Optimization.](http://arxiv.org/abs/2207.02849) | 本文介绍了一个名为Betty的自动微分库，可用于大规模的梯度优化问题，有效地减少了计算复杂度并提高了可扩展性，在广泛的多层次优化任务中表现出良好性能。 |
| [^140] | [NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds.](http://arxiv.org/abs/2206.11736) | NovelCraft数据集提供了开放世界中新颖性检测与发现任务的挑战。在复杂的场景中插入新颖物体的检测需要更好的基准，并发现了控制假阳性时更简单的方法可能比复杂的方法更出色。 |
| [^141] | [Phased Progressive Learning with Coupling-Regulation-Imbalance Loss for Imbalanced Data Classification.](http://arxiv.org/abs/2205.12117) | 该论文提出了一种分阶段渐进学习的方法和一种耦合调节不平衡损失函数来解决不平衡数据分类问题，该方法特别适用于具有失衡或样本较少的数据集。 |
| [^142] | [Encoding Domain Knowledge in Multi-view Latent Variable Models: A Bayesian Approach with Structured Sparsity.](http://arxiv.org/abs/2204.06242) | 提出了一种新的基于修改过的马蹄蚌先验的多视角潜变量模型MuVI，用于建模结构稀疏性。它能够纳入有限且噪声的领域知识，以内在可解释的方式分析多视角数据，优于现有结构稀疏性建模方法。 |
| [^143] | [Learning Resilient Radio Resource Management Policies with Graph Neural Networks.](http://arxiv.org/abs/2203.11012) | 本文提出了一个使用图神经网络学习的具有弹性的无线电资源管理策略，以实现高聚合速率并确保所有用户的公平性，并使用可扩展的置换等变图神经网络（GNN）架构基于瞬时信道条件推导出的图形拓扑来参数化RRM策略 |
| [^144] | [DCT-Former: Efficient Self-Attention with Discrete Cosine Transform.](http://arxiv.org/abs/2203.01178) | 本文提出使用离散余弦变换的自注意力模型，有效缓解了点积注意力计算苛刻的内存和时间复杂度限制，具有更小的内存占用和更短的推理时间，表现良好。 |
| [^145] | [Weisfeiler and Leman go Machine Learning: The Story so far.](http://arxiv.org/abs/2112.09992) | Weisfeiler-Leman算法被广泛应用于处理图和关系数据。本文全面介绍了该算法在监督学习中的应用，包括理论背景、扩展、与等变神经网格的联系、并列出了当前应用和未来研究方向。 |
| [^146] | [Robust and Provably Monotonic Networks.](http://arxiv.org/abs/2112.00038) | 本论文提出了一种方法，可以约束深度学习模型的利普希茨常数，并使用单调剩余连接使模型的某些输入单调，适用于需要领域知识指导依赖性的场景，如算法公平性要求及物理学中的次原子粒子分类。 |
| [^147] | [Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting.](http://arxiv.org/abs/2110.03135) | 该论文发现了对抗训练中存在的标签噪声，并解释了其对鲁棒过度拟合的普遍存在以及扰动半径和数据质量的依赖性。通过该论文提出的方法，可以自动校准标签以应对标签噪声和鲁棒过度拟合。 |
| [^148] | [Facetron: A Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations.](http://arxiv.org/abs/2107.12003) | 本文提出了一种基于交叉模态潜在表示的多说话人脸向语音模型Facetron，可以适用于不同的说话人条件下灵活地生成语音波形，并在客观和主观评估中表现出优越性。 |
| [^149] | [Marginalising over Stationary Kernels with Bayesian Quadrature.](http://arxiv.org/abs/2106.07452) | 本文提出一种贝叶斯积分方案，用于边缘化高斯过程核族，以获得具有良好校准不确定性估计的灵活模型，比现有方法更高效实用。 |
| [^150] | [Planning and Learning Using Adaptive Entropy Tree Search.](http://arxiv.org/abs/2102.06808) | 本论文提出了一种全新的算法ANTS，它将规划和学习结合在最大熵范式中，并通过在Atari基准测试上的实验证明其明显优于当前最先进的AlphaZero系统的规划组件PUCT，具有较强的稳健性，可推动基于树的规划方法的实际应用。 |
| [^151] | [ES-ENAS: Efficient Evolutionary Optimization for Large Hybrid Search Spaces.](http://arxiv.org/abs/2101.07415) | 本文提出了ES-ENAS方法，将进化策略和组合优化器结合起来来优化混合搜索空间，并在CIFAR-10上取得了超越最新方法的结果。 |
| [^152] | [Load Encoding for Learning AC-OPF.](http://arxiv.org/abs/2101.03973) | 本文提出了一个新的负载压缩嵌入方案，以解决深度学习技术在学习 AC-OPF 时的可扩展性问题，并在实验中取得了数量级的改进。 |
| [^153] | [Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering.](http://arxiv.org/abs/2009.09213) | 本文提出了一种基于学习的方法，在空间域中重现陷波滤波器的效果，以生成高度逼真且“检测难以捉摸”的DeepFakes。 |
| [^154] | [Computation Offloading in Heterogeneous Vehicular Edge Networks: On-line and Off-policy Bandit Solutions.](http://arxiv.org/abs/2008.06302) | 本文中，我们针对异构VEC场景中的计算卸载问题提出了基于多臂老虎机理论的在线学习算法和Off-policy学习算法，以动态选择最小卸载时间为目标。我们的方法有效地在一群车辆和基站间权衡计算卸载体验和网络负载，并通过实验验证了其有效性。 |

# 详细

[^1]: 基于稀疏子流形卷积神经网络的中微子望远镜触发事件重构

    Trigger-Level Event Reconstruction for Neutrino Telescopes Using Sparse Submanifold Convolutional Neural Networks. (arXiv:2303.08812v1 [hep-ex])

    [http://arxiv.org/abs/2303.08812](http://arxiv.org/abs/2303.08812)

    SSCNN是一种用于解决中微子望远镜数据处理中稀疏、高维度和非规则几何形状等问题的网络，其事件重构性能优于传统和机器学习算法，运行速度大大提高，可用于改善中微子能量和方向的第一估计以播种更先进的重建。

    

    卷积神经网络（CNN）在科学数据分析中得到了广泛应用，包括在中微子望远镜中。然而，这些实验的数据对CNN提出了许多挑战，如非规则几何形状、稀疏性和高维度。因此，CNN在中微子望远镜数据上非常低效，并需要大量的预处理，导致信息损失。我们提出了稀疏子流形卷积（SSCNN）作为解决这些问题的方法，并显示SSCNN事件重构性能与传统和机器学习算法相比可比或更好。此外，我们的SSCNN在GPU上的运行速度约为传统CNN的16倍。由于这种加速，预计能够处理IceCube规模的中微子望远镜的触发级事件速率。这些网络可用于改善中微子能量和方向的第一估计以播种更先进的重建。

    Convolutional neural networks (CNNs) have seen extensive applications in scientific data analysis, including in neutrino telescopes. However, the data from these experiments present numerous challenges to CNNs, such as non-regular geometry, sparsity, and high dimensionality. Consequently, CNNs are highly inefficient on neutrino telescope data, and require significant pre-processing that results in information loss. We propose sparse submanifold convolutions (SSCNNs) as a solution to these issues and show that the SSCNN event reconstruction performance is comparable to or better than traditional and machine learning algorithms. Additionally, our SSCNN runs approximately 16 times faster than a traditional CNN on a GPU. As a result of this speedup, it is expected to be capable of handling the trigger-level event rate of IceCube-scale neutrino telescopes. These networks could be used to improve the first estimation of the neutrino energy and direction to seed more advanced reconstructions,
    
[^2]: 不用担心你怎么到达目的地：一种新的自监督多时间尺度行为分析方法

    Relax, it doesn't matter how you get there: A new self-supervised approach for multi-timescale behavior analysis. (arXiv:2303.08811v1 [cs.LG])

    [http://arxiv.org/abs/2303.08811](http://arxiv.org/abs/2303.08811)

    本文提出了一种自监督的多任务表示学习模型，结合行动预测和多尺度结构以建立局部和全局动态的潜在空间，并在多智能体行为挑战赛中取得最先进的结果。

    

    自然行为具有复杂和不可预测的动态，特别是在尝试预测未来许多步骤时。尽管在受限或简化的任务条件下构建行为表示方面取得了一定的成功，但其中许多模型不能应用于自由和自然的环境中，因为这些行为变得越来越难以建模。在这项工作中，我们为行为开发了一种多任务表示学习模型，其中包括两个新颖组件：（i）一个行动预测目标，旨在预测未来时间步长的行动分布; （ii）一种多尺度结构，建立独立的潜在空间以适应短期和长期动态。在展示了该方法在不同环境和地形中对真实机器人的局部和全局动态建模能力后，我们将该方法应用到MABe 2022多智能体行为挑战赛中，其中我们的模型在总体排名中排名第一，并达到了最先进的结果。

    Natural behavior consists of dynamics that are complex and unpredictable, especially when trying to predict many steps into the future. While some success has been found in building representations of behavior under constrained or simplified task-based conditions, many of these models cannot be applied to free and naturalistic settings where behavior becomes increasingly hard to model. In this work, we develop a multi-task representation learning model for behavior that combines two novel components: (i) An action prediction objective that aims to predict the distribution of actions over future timesteps, and (ii) A multi-scale architecture that builds separate latent spaces to accommodate short- and long-term dynamics. After demonstrating the ability of the method to build representations of both local and global dynamics in realistic robots in varying environments and terrains, we apply our method to the MABe 2022 Multi-agent behavior challenge, where our model ranks 1st overall and 
    
[^3]: 理解事后解释器：以Anchors为例

    Understanding Post-hoc Explainers: The Case of Anchors. (arXiv:2303.08806v1 [stat.ML])

    [http://arxiv.org/abs/2303.08806](http://arxiv.org/abs/2303.08806)

    本文对Anchors进行了理论分析，这是一种基于规则的可解释性方法，用于解释文本分类器的决策。

    

    在许多情况下，机器学习模型可解释性是一项高度要求但难以实现的任务。为了解释这些模型的个体预测，已经提出了本地模型无关方法。然而，产生解释的过程对于用户来说可能与要解释的预测一样神秘。此外，可解释性方法经常缺乏理论保证，并且它们在简单模型上的行为通常是未知的。本文对Anchors（Ribeiro等人，2018）进行理论分析：一种流行的基于规则的可解释性方法，它强调一小组单词以解释文本分类器的决策。

    In many scenarios, the interpretability of machine learning models is a highly required but difficult task. To explain the individual predictions of such models, local model-agnostic approaches have been proposed. However, the process generating the explanations can be, for a user, as mysterious as the prediction to be explained. Furthermore, interpretability methods frequently lack theoretical guarantees, and their behavior on simple models is frequently unknown. While it is difficult, if not impossible, to ensure that an explainer behaves as expected on a cutting-edge model, we can at least ensure that everything works on simple, already interpretable models. In this paper, we present a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular rule-based interpretability method that highlights a small set of words to explain a text classifier's decision. After formalizing its algorithm and providing useful insights, we demonstrate mathematically that Anchors produces meaningf
    
[^4]: 随机插值：流和扩散的统一框架

    Stochastic Interpolants: A Unifying Framework for Flows and Diffusions. (arXiv:2303.08797v1 [cs.LG])

    [http://arxiv.org/abs/2303.08797](http://arxiv.org/abs/2303.08797)

    本文提出了一种统一的生成模型，该模型基于随机插值框架，可以实现流和扩散方法的统一。作者构建了一类广泛的连续时间随机过程，用于将两个任意的密度在有限时间内精确地连接。这种方法可以用于基于概率微分方程的确定性和随机生成模型的构建。

    

    我们介绍了一类建立在随机插值框架上的生成模型，该框架是基于Albergo＆Vanden-Eijnden（2023）提出的，在流和扩散方法上实现统一，我们首先展示了如何构建一类广泛的连续时间随机过程，其时间依赖的概率密度函数在有限时间内精确地连接两个任意的密度。这些“随机插值器”是通过将来自两个密度的数据与其他潜在变量相结合构建的，并且构造的具体细节可以灵活地塑造导致的时间依赖密度。然后我们展示了随机插值器的时间依赖密度满足一阶输运方程以及一系列具有可调扩散的正向和反向Fokker-Planck方程族; 在考虑单个样本的时间演化时，这个观点立即导致了基于概率微分方程的确定性和随机生成模型。

    We introduce a class of generative models based on the stochastic interpolant framework proposed in Albergo & Vanden-Eijnden (2023) that unifies flow-based and diffusion-based methods. We first show how to construct a broad class of continuous-time stochastic processes whose time-dependent probability density function bridges two arbitrary densities exactly in finite time. These `stochastic interpolants' are built by combining data from the two densities with an additional latent variable, and the specific details of the construction can be leveraged to shape the resulting time-dependent density in a flexible way. We then show that the time-dependent density of the stochastic interpolant satisfies a first-order transport equation as well as a family of forward and backward Fokker-Planck equations with tunable diffusion; upon consideration of the time evolution of an individual sample, this viewpoint immediately leads to both deterministic and stochastic generative models based on proba
    
[^5]: PLEX：利用可用数据进行机器人操纵预训练的最大化

    PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining. (arXiv:2303.08789v1 [cs.RO])

    [http://arxiv.org/abs/2303.08789](http://arxiv.org/abs/2303.08789)

    PLEX提出了一种新的机器人操纵预训练方法，利用任务不可知的视觉运动轨迹和大量的任务条件下的物体操作视频，在学习通用的操纵例程的同时，通过视频演示学习如何在这个特征空间中规划各种任务。

    

    丰富的表征是实现机器人操纵的关键，但现有的模型架构需要大量数据来学习。不幸的是，理想的机器人操纵训练数据，即各种已注释任务的专家视觉-动作演示，是稀缺的。在本文中，我们提出了一种基于变压器的架构PLEX，它是从任务不可知视觉运动轨迹中学习的，伴随着大量的任务条件下的物体操作视频——这是一种数量可观的与机器人相关的数据。PLEX背后的关键见解是，在观察和行动方面的轨迹下，有助于诱导潜在的特征空间，并训练机器人执行与任务不相关的操作例程，而多样化的仅为视频演示仅可以有效地教会机器人如何在这个特征空间中规划各种任务。与大多数机器人操纵预培训作品不同，PLEX学习了一种可推广的感觉运动多任务策略。

    A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal robotic manipulation training data, which comes in the form of expert visuomotor demonstrations for a variety of annotated tasks, is scarce. In this work we propose PLEX, a transformer-based architecture that learns from task-agnostic visuomotor trajectories accompanied by a much larger amount of task-conditioned object manipulation videos -- a type of robotics-relevant data available in quantity. The key insight behind PLEX is that the trajectories with observations and actions help induce a latent feature space and train a robot to execute task-agnostic manipulation routines, while a diverse set of video-only demonstrations can efficiently teach the robot how to plan in this feature space for a wide variety of tasks. In contrast to most works on robotic manipulation pretraining, PLEX learns a generalizable sensorimotor multi-task polic
    
[^6]: 全神经形态视觉和控制的自主飞行执照。

    Fully neuromorphic vision and control for autonomous drone flight. (arXiv:2303.08778v1 [cs.RO])

    [http://arxiv.org/abs/2303.08778](http://arxiv.org/abs/2303.08778)

    本文介绍了第一个全神经形态视觉到控制的流程，使无人机具备了执行自主基于视觉的飞行所需的能力，为机器人感知和行动提供了低延迟和能量有效的解决方案。

    

    生物感知和处理是异步和稀疏的，导致低延迟和能量有效的感知和行动。在机器人学中，使用面向事件的神经形态硬件和尖峰神经网络承诺具有类似的特征。然而，由于当前嵌入式神经形态处理器中受限的网络规模以及训练尖峰神经网络的困难，机器人实现仅限于具有低维感官输入和运动执行的基本任务。在这里，我们提出了第一个全神经形态视觉到控制的流程，以控制自由飞行的无人机。具体而言，我们训练了一个尖峰神经网络，该神经网络接受高维原始事件相机数据并输出执行自主基于视觉的飞行所需的低级控制动作。网络的视觉部分由五层和 28.8k 神经元组成，将传入的原始事件映射到自我运动估计上，并使用真实环境的自我监督学习进行训练。

    Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions due to the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present the first fully neuromorphic vision-to-control pipeline for controlling a freely flying drone. Specifically, we train a spiking neural network that accepts high-dimensional raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28.8k neurons, maps incoming raw events to ego-motion estimates and is trained with self-supervised learning on real e
    
[^7]: 模型选择配合交叉验证风险估计的无分布偏差界学习方法

    Distribution-free Deviation Bounds of Learning via Model Selection with Cross-validation Risk Estimation. (arXiv:2303.08777v1 [stat.ML])

    [http://arxiv.org/abs/2303.08777](http://arxiv.org/abs/2303.08777)

    本文提出通过模型选择和交叉验证风险估计来学习的一般方法，并建立了无分布偏差界，比经验风险最小化方法更紧密，在一些情况下表现更优。

    

    交叉验证方法的风险估计和模型选择在统计学和机器学习中得到了广泛应用。然而，学习通过模型选择与交叉验证风险估计的理论性质的理解在其广泛使用面前相当缺乏。在这个背景下，本文将学习通过模型选择与交叉验证风险估计作为一种经典统计学习理论中的一般系统学习框架，并建立了基于VC维的无分布偏差边界，给出了结果的详细证明，并考虑了有界和无界的损失函数。我们还推导出在整个假设空间中，学习通过模型选择的偏差界比通过经验风险最小化学习的偏差界更紧密的条件，支持在一些情况下经验上观察到的模型选择框架的更好性能。

    Cross-validation techniques for risk estimation and model selection are widely used in statistics and machine learning. However, the understanding of the theoretical properties of learning via model selection with cross-validation risk estimation is quite low in face of its widespread use. In this context, this paper presents learning via model selection with cross-validation risk estimation as a general systematic learning framework within classical statistical learning theory and establishes distribution-free deviation bounds in terms of VC dimension, giving detailed proofs of the results and considering both bounded and unbounded loss functions. We also deduce conditions under which the deviation bounds of learning via model selection are tighter than that of learning via empirical risk minimization in the whole hypotheses space, supporting the better performance of model selection frameworks observed empirically in some instances.
    
[^8]: 《使用人工神经网络进行期权定价模型性能比较的深层校准》

    Deep Calibration With Artificial Neural Network: A Performance Comparison on Option Pricing Models. (arXiv:2303.08760v1 [q-fin.MF])

    [http://arxiv.org/abs/2303.08760](http://arxiv.org/abs/2303.08760)

    论文通过构建人工神经网络的方式，以蒙特卡罗模拟数据集作为训练数据，探讨了使用ANN进行期权定价模型参数校准的方法，结果表明ANN方法的性能优于MCS方法，并且具有更短的计算时间。

    

    本文探讨了人工神经网络（ANN）作为无模型解决期权定价模型校准算法的方案。我们构建了ANN来校准两种着名的GARCH型期权定价模型的参数：Duan的GARCH和经典温和稳定GARCH，这两个模型显著改进了Black-Scholes模型的局限性，但受到计算复杂性的影响。为了减轻这种技术困难，我们使用蒙特卡洛模拟（MCS）方法生成的数据集对ANNS进行训练，并将其应用于校准最佳参数。性能结果表明，ANN方法始终优于MCS，并且一旦经过训练，还可以利用更快的计算时间。还讨论了期权的希腊字母。

    This paper explores Artificial Neural Network (ANN) as a model-free solution for a calibration algorithm of option pricing models. We construct ANNs to calibrate parameters for two well-known GARCH-type option pricing models: Duan's GARCH and the classical tempered stable GARCH that significantly improve upon the limitation of the Black-Scholes model but have suffered from computation complexity. To mitigate this technical difficulty, we train ANNs with a dataset generated by Monte Carlo Simulation (MCS) method and apply them to calibrate optimal parameters. The performance results indicate that the ANN approach consistently outperforms MCS and takes advantage of faster computation times once trained. The Greeks of options are also discussed.
    
[^9]: 利用四维CT灌注成像对疑似急性缺血性卒中患者的梗死区进行分割

    Exploiting 4D CT Perfusion for segmenting infarcted areas in patients with suspected acute ischemic stroke. (arXiv:2303.08757v1 [eess.IV])

    [http://arxiv.org/abs/2303.08757](http://arxiv.org/abs/2303.08757)

    该研究提出了一种新颖的方法，通过利用四维CTP全面利用时空信息，以分割疑似急性缺血性卒中患者的梗死区。实验证明，该方法明显优于现有的最先进方法，有潜力为改善AIS患者的早期诊断和治疗规划的准确性和效率做出贡献。

    

    精确、快速的急性缺血性卒中（AIS）患者缺血区（核心和半影区）预测方法对于改进诊断和治疗规划具有重要的临床意义。计算机断层扫描（CT）是疑似AIS患者早期评估的主要模式之一。CT灌注成像（CTP）通常用作主要评估手段，以确定卒中位置、严重程度和缺血性病灶体积。目前，大多数CTP自动分割方法都使用已经处理过的三维彩色地图作为放射科医师常规视觉评估的输入。或者，基于切片的二维+时间输入使用原始CTP数据，其中忽略了在体积上的空间信息。在本文中，我们研究不同方法来利用整个四维CTP作为输入，以充分利用时空信息。这使我们提出了一种新颖的4D卷积层。我们在大型数据集上进行的全面实验表明，所提出的方法明显优于现有的最先进方法。该方法有潜力为改善AIS患者的早期诊断和治疗规划的准确性和效率做出贡献。

    Precise and fast prediction methods for ischemic areas (core and penumbra) in acute ischemic stroke (AIS) patients are of significant clinical interest: they play an essential role in improving diagnosis and treatment planning. Computed Tomography (CT) scan is one of the primary modalities for early assessment in patients with suspected AIS. CT Perfusion (CTP) is often used as a primary assessment to determine stroke location, severity, and volume of ischemic lesions. Current automatic segmentation methods for CTP mostly use already processed 3D color maps conventionally used for visual assessment by radiologists as input. Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time input, where the spatial information over the volume is ignored. In this paper, we investigate different methods to utilize the entire 4D CTP as input to fully exploit the spatio-temporal information. This leads us to propose a novel 4D convolution layer. Our comprehensive experiments on a l
    
[^10]: 在大规模开放挑战中评估手势生成：GENEA Challenge 2022的研究报告(arXiv:2303.08737v1 [cs.HC])

    Evaluating gesture-generation in a large-scale open challenge: The GENEA Challenge 2022. (arXiv:2303.08737v1 [cs.HC])

    [http://arxiv.org/abs/2303.08737](http://arxiv.org/abs/2303.08737)

    本文介绍了GENEA Challenge 2022的研究结果，该比赛旨在基准测试基于数据驱动的自动共同语言手势生成。使用具有相同语音和动作的数据集，众多参赛团队的手势生成系统在几个大型用户研究中得到了评估，因此能够进行直接比较。

    

    本文报道了第二届GENEA Challenge，对基于数据驱动的自动共同语言手势生成进行了基准测试。参赛团队使用相同的语音和运动数据集构建手势生成系统。这些系统生成的动作使用标准化的可视化管道渲染为视频，并在几个大型众包用户研究中进行评估。与比较不同研究论文时不同的是，这里的结果差异仅由于方法之间的差异，从而实现了系统之间的直接比较。该数据集基于18小时的全身动作捕捉，包括手指，并记录了不同人物参与双人 对话。十个团队参加了分为全身和上半身肢体表达的两个层次的挑战。对于每个层次，我们评估了手势运动的人类相似度和其对特定语音信号的适用性。我们的评估将人类相似度与手势适用性解藕开，这一点一直是困难的。

    This paper reports on the second GENEA Challenge to benchmark data-driven automatic co-speech gesture generation. Participating teams used the same speech and motion dataset to build gesture-generation systems. Motion generated by all these systems was rendered to video using a standardised visualisation pipeline and evaluated in several large, crowdsourced user studies. Unlike when comparing different research papers, differences in results are here only due to differences between methods, enabling direct comparison between systems. The dataset was based on 18 hours of full-body motion capture, including fingers, of different persons engaging in a dyadic conversation. Ten teams participated in the challenge across two tiers: full-body and upper-body gesticulation. For each tier, we evaluated both the human-likeness of the gesture motion and its appropriateness for the specific speech signal. Our evaluations decouple human-likeness from gesture appropriateness, which has been a difficu
    
[^11]: 一种机器学习方法用于通过后处理模拟数据预测雷暴天气(arXiv:2303.08736v1 [physics.ao-ph])

    A machine-learning approach to thunderstorm forecasting through post-processing of simulation data. (arXiv:2303.08736v1 [physics.ao-ph])

    [http://arxiv.org/abs/2303.08736](http://arxiv.org/abs/2303.08736)

    介绍了一种机器学习方法SALAMA用于预测雷暴发生情况，可以在长达11小时的时间内进行预测，预测技能优于传统方案，且预测时间尺度随着预报的空间尺度呈线性增加。

    

    雷暴对社会和经济构成重大威胁，需要可靠的雷暴预测。本文介绍了SALAMA，一种前馈神经网络模型，用于识别数值天气预报（NWP）数据中的雷暴发生情况。该模型在中欧对流层解析集合预报和雷电观测数据上进行训练。仅给出从NWP数据中提取的与雷暴发展相关的像素输入参数集，SALAMA以可靠的校准方式推断雷暴发生的概率。对于长达11小时的前置时间，我们发现其预测技能优于仅基于对流有效位能的分类。通过改变将闪电观测与NWP数据相关联的时空标准，我们展示了熟练的雷暴预测时间尺度随着预报的空间尺度的线性增加。

    Thunderstorms pose a major hazard to society and economy, which calls for reliable thunderstorm forecasts. In this work, we introduce SALAMA, a feedforward neural network model for identifying thunderstorm occurrence in numerical weather prediction (NWP) data. The model is trained on convection-resolving ensemble forecasts over Central Europe and lightning observations. Given only a set of pixel-wise input parameters that are extracted from NWP data and related to thunderstorm development, SALAMA infers the probability of thunderstorm occurrence in a reliably calibrated manner. For lead times up to eleven hours, we find a forecast skill superior to classification based only on convective available potential energy. Varying the spatiotemporal criteria by which we associate lightning observations with NWP data, we show that the time scale for skillful thunderstorm predictions increases linearly with the spatial scale of the forecast.
    
[^12]: DACOS-一个手动注释的代码异味数据集

    DACOS-A Manually Annotated Dataset of Code Smells. (arXiv:2303.08729v1 [cs.SE])

    [http://arxiv.org/abs/2303.08729](http://arxiv.org/abs/2303.08729)

    DACOS是一个手动注释的包含三种不同粒度的代码异味（多面抽象、复杂方法和长参数列表）的数据集，用于作为机器学习检测代码异味的训练和基准测试。

    

    研究人员应用机器学习技术来检测代码异味，以抵消许多代码异味的主观性。这种方法需要一个大型、手动注释的数据集进行训练和基准测试。现有文献提供了一些数据集，但它们规模较小，更重要的是，它们没有关注主观代码片段。本文介绍了DACOS，一个手动注释的数据集，包含10,267个对5,192个代码片段的注释。该数据集针对不同粒度的三种代码异味：多面抽象、复杂方法和长参数列表。数据集分为两个阶段创建。第一阶段通过确定用于检测气味的度量标准的阈值来帮助我们识别可能具有主观性的代码片段。第二阶段收集可能具有主观性的代码片段的注释。我们还提供了一个扩展数据集DACOSX，它使用阈值包含明确的良性代码片段和明确的有异味的代码片段。

    Researchers apply machine-learning techniques for code smell detection to counter the subjectivity of many code smells. Such approaches need a large, manually annotated dataset for training and benchmarking. Existing literature offers a few datasets; however, they are small in size and, more importantly, do not focus on the subjective code snippets. In this paper, we present DACOS, a manually annotated dataset containing 10,267 annotations for 5,192 code snippets. The dataset targets three kinds of code smells at different granularity: multifaceted abstraction, complex method, and long parameter list. The dataset is created in two phases. The first phase helps us identify the code snippets that are potentially subjective by determining the thresholds of metrics used to detect a smell. The second phase collects annotations for potentially subjective snippets. We also offer an extended dataset DACOSX that includes definitely benign and definitely smelly snippets by using the thresholds i
    
[^13]: 人工影响: AI驱动的说服分析

    Artificial Influence: An Analysis Of AI-Driven Persuasion. (arXiv:2303.08721v1 [cs.CY])

    [http://arxiv.org/abs/2303.08721](http://arxiv.org/abs/2303.08721)

    本文探讨了AI驱动的说服未来的不确定性，包括移动说服力平衡的方式、定制化的说服、虚假信息的带动以及改变人类自身言论的方式。我们警告存在负面影响，并呼吁加强对其开发和使用的监管。

    

    说服是人类的重要特征之一，是商业、政治等事业的核心。人工智能（AI）的进步已经产生了能够说服人类购买产品、观看视频、点击搜索结果等的AI系统。即使没有明确设计为说服的系统，在实践中也可能会这样做。未来，越来越具有人形特征的AI系统可能会与用户形成持续的关系，提高它们的说服力。本文探讨了具有不确定性的AI系统的说服能力未来。我们考虑到AI如何在移动说服力平衡的基础上，实现定制化的说服，为虚假信息带来动力以及改变人类塑造自身言论的方式。我们考虑AI驱动的说服方式可能与人类驱动的方式有所不同。我们警告说，普遍存在高度说服力的AI系统可能对人类的自主权和福祉产生负面影响，并呼吁加强关于其开发和使用的对话和监管。

    Persuasion is a key aspect of what it means to be human, and is central to business, politics, and other endeavors. Advancements in artificial intelligence (AI) have produced AI systems that are capable of persuading humans to buy products, watch videos, click on search results, and more. Even systems that are not explicitly designed to persuade may do so in practice. In the future, increasingly anthropomorphic AI systems may form ongoing relationships with users, increasing their persuasive power. This paper investigates the uncertain future of persuasive AI systems. We examine ways that AI could qualitatively alter our relationship to and views regarding persuasion by shifting the balance of persuasive power, allowing personalized persuasion to be deployed at scale, powering misinformation campaigns, and changing the way humans can shape their own discourse. We consider ways AI-driven persuasion could differ from human-driven persuasion. We warn that ubiquitous highlypersuasive AI sy
    
[^14]: 神经网络无监督域适应的泛化保证的实用性研究

    Practicality of generalization guarantees for unsupervised domain adaptation with neural networks. (arXiv:2303.08720v1 [cs.LG])

    [http://arxiv.org/abs/2303.08720](http://arxiv.org/abs/2303.08720)

    研究评估了现有文献中有潜力满足我们要求的域适应图像分类任务的界限，发现所有界限都是空泛的，样本泛化术语占据了观察到的松弛程度的很大部分，特别是当这些术语与域的度量互动时。

    

    理解泛化对于自信地设计和部署机器学习模型至关重要，特别是当部署意味着数据域的转移时。对于这样的域适应问题，我们寻求可计算和紧密的泛化界限。如果可以实现这些要求，这些界限可以作为部署中充足性能的保证。然而，在深度神经网络是首选模型的应用中，推导出满足这些要求的结果仍是一项未解决的挑战；大多数现有的界限要么是空泛的，要么有不可估计的术语，即使在有利条件下也是如此。在本文中，我们评估了现有文献中有潜力满足我们要求的域适应图像分类任务的界限，深度神经网络是首选。我们发现所有界限都是空泛的，并且样本泛化术语占据了观察到的松弛程度的很大部分，特别是当这些术语与域的度量互动时。

    Understanding generalization is crucial to confidently engineer and deploy machine learning models, especially when deployment implies a shift in the data domain. For such domain adaptation problems, we seek generalization bounds which are tractably computable and tight. If these desiderata can be reached, the bounds can serve as guarantees for adequate performance in deployment. However, in applications where deep neural networks are the models of choice, deriving results which fulfill these remains an unresolved challenge; most existing bounds are either vacuous or has non-estimable terms, even in favorable conditions. In this work, we evaluate existing bounds from the literature with potential to satisfy our desiderata on domain adaptation image classification tasks, where deep neural networks are preferred. We find that all bounds are vacuous and that sample generalization terms account for much of the observed looseness, especially when these terms interact with measures of domain
    
[^15]: 从二进制测量中学习信号重构

    Learning to Reconstruct Signals From Binary Measurements. (arXiv:2303.08691v1 [eess.SP])

    [http://arxiv.org/abs/2303.08691](http://arxiv.org/abs/2303.08691)

    该论文提出了一种新的自监督学习方法SSBM，它只需要二进制数据进行训练，并探索了从不完整的二进制观察中学习的极端情况。这为从二进制测量中恢复信号提供了必要和充分条件，并在一系列真实数据集上展示了SSBM的卓越表现。

    

    无监督学习的最新进展突出了仅从噪声和不完整的线性测量中学习信号重构的可能性。这些方法在医学和科学成像以及传感中起到关键作用，其中地面真实数据经常稀缺或难以获得。然而，在实践中，测量不仅噪声和不完整，而且还被量化。在这里，我们探索从二进制观察中学习的极端情况，并提供了关于从不完整二进制数据中识别一组信号所需的测量数量的必要和充分条件。我们的结果是对从二进制测量中信号恢复现有界限的补充。此外，我们引入了一种新颖的自监督学习方法，我们将其命名为“SSBM”，它仅需要二进制数据进行训练。我们在一系列真实数据集上的实验证明SSBM与监督学习相当，并优于稀疏重构方法。

    Recent advances in unsupervised learning have highlighted the possibility of learning to reconstruct signals from noisy and incomplete linear measurements alone. These methods play a key role in medical and scientific imaging and sensing, where ground truth data is often scarce or difficult to obtain. However, in practice, measurements are not only noisy and incomplete but also quantized. Here we explore the extreme case of learning from binary observations and provide necessary and sufficient conditions on the number of measurements required for identifying a set of signals from incomplete binary data. Our results are complementary to existing bounds on signal recovery from binary measurements. Furthermore, we introduce a novel self-supervised learning approach, which we name SSBM, that only requires binary data for training. We demonstrate in a series of experiments with real datasets that SSBM performs on par with supervised learning and outperforms sparse reconstruction methods wit
    
[^16]: 带有局部遗忘的重放缓冲区用于自适应深度模型强化学习

    Replay Buffer With Local Forgetting for Adaptive Deep Model-Based Reinforcement Learning. (arXiv:2303.08690v1 [cs.LG])

    [http://arxiv.org/abs/2303.08690](http://arxiv.org/abs/2303.08690)

    提出了一种带有局部遗忘的新型重放缓冲区，可以在状态空间的相关部分快速遗忘过时的经验。实验证明该方法提高了自适应深度模型强化学习代理对环境变化的适应能力，加速了学习速度并改善了策略。

    

    神经科学中用于确定所研究的对象（无论是啮齿动物还是人类）是否表现出模型为基础的学习的关键行为特征之一是对环境中局部变化的有效适应。然而，在强化学习中，最近的研究表明，现代深度模型强化学习（MBRL）方法较难适应这种变化。本文提出了一种新的重放缓冲区，带有局部遗忘，可以快速地在状态空间中的相关部分遗忘过时的经验而在其他地方保留旧数据。通过在一系列具有挑战性的导航任务上进行实验，我们证明了该方法改善了对环境变化的适应能力，加快了学习速度并改善了策略。

    One of the key behavioral characteristics used in neuroscience to determine whether the subject of study -- be it a rodent or a human -- exhibits model-based learning is effective adaptation to local changes in the environment. In reinforcement learning, however, recent work has shown that modern deep model-based reinforcement-learning (MBRL) methods adapt poorly to such changes. An explanation for this mismatch is that MBRL methods are typically designed with sample-efficiency on a single task in mind and the requirements for effective adaptation are substantially higher, both in terms of the learned world model and the planning routine. One particularly challenging requirement is that the learned world model has to be sufficiently accurate throughout relevant parts of the state-space. This is challenging for deep-learning-based world models due to catastrophic forgetting. And while a replay buffer can mitigate the effects of catastrophic forgetting, the traditional first-in-first-out
    
[^17]: 基于令牌稀疏化视角的视觉Transformer优化

    Making Vision Transformers Efficient from A Token Sparsification View. (arXiv:2303.08685v1 [cs.CV])

    [http://arxiv.org/abs/2303.08685](http://arxiv.org/abs/2303.08685)

    本文提出了一种新的Semantic Token ViT (STViT)方法，实现了全局和本地视觉Transformer的高效性能，同时可用作下游任务的主干骨干。其通过聚类中心的语义令牌代表来代替图像令牌，实现较少的语义令牌即可达到同样的效果。

    

    视觉Transformer (ViTs)的计算复杂度随着令牌数量呈二次增长，限制了其实际应用。为了实现高效的ViT，已有多种方法通过修剪冗余令牌来达到目的。然而，这些方法往往存在以下问题：(i) 显著的精度下降，(ii) 无法应用于本地视觉Transformer中，以及 (iii) 无法通用于下游任务的网络。本文提出了一种新颖的语义令牌ViT（STViT），用于实现全局和本地视觉Transformer的高效性能，并可作为下游任务的主干骨干进行修订。语义令牌代表聚类中心，其通过空间内的图像令牌汇集来初始化，并通过注意组件进行恢复，自适应地表示全局或本地的语义信息。由于其聚类性质，少量的语义令牌即可实现与众多图像令牌相同的效果，适用于全局和本地视觉Transformer。例如，对于DeiT-(Tiny, Small, Base)，仅需16个语义令牌即可达到相同效果。

    The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can 
    
[^18]: 无人机辅助网络中的数据实时性多智能体近端策略优化

    Muti-Agent Proximal Policy Optimization For Data Freshness in UAV-assisted Networks. (arXiv:2303.08680v1 [math.OC])

    [http://arxiv.org/abs/2303.08680](http://arxiv.org/abs/2303.08680)

    本文研究了无人机辅助网络中多智能体近端策略优化的数据实时性问题，通过协作强化学习优化无人机的轨迹设计和对物联网设备的访问，并成功地最小化了全局更新年龄（AoU）。

    

    无人机被视为在无线通信网络中执行广泛任务的有前途的技术。本文考虑部署一组无人机来收集物联网设备生成的数据。具体地，我们关注的是收集的数据具有实时性的情况，并且保持其及时性非常重要。我们的目标是最优地设计无人机轨迹和访问物联网设备的子集，以最小化全局更新年龄（Age-of-Updates, AoU）。为此，我们将研究对象的问题制定为一种混合整数非线性规划（MINLP），并在时间和服务质量限制下进行优化。为了有效地解决所得到的优化问题，我们研究了协作多智能体强化学习（MARL）框架，并提出了一种基于流行的近端策略优化（Policy Proximal Optimization, PPO）算法的强化学习（RL）方法。我们的方法利用了集中式训练和分散执行的RL算法。

    Unmanned aerial vehicles (UAVs) are seen as a promising technology to perform a wide range of tasks in wireless communication networks. In this work, we consider the deployment of a group of UAVs to collect the data generated by IoT devices. Specifically, we focus on the case where the collected data is time-sensitive, and it is critical to maintain its timeliness. Our objective is to optimally design the UAVs' trajectories and the subsets of visited IoT devices such as the global Age-of-Updates (AoU) is minimized. To this end, we formulate the studied problem as a mixed-integer nonlinear programming (MINLP) under time and quality of service constraints. To efficiently solve the resulting optimization problem, we investigate the cooperative Multi-Agent Reinforcement Learning (MARL) framework and propose an RL approach based on the popular on-policy Reinforcement Learning (RL) algorithm: Policy Proximal Optimization (PPO). Our approach leverages the centralized training decentralized ex
    
[^19]: 基于视觉提示的个性化联邦学习

    Visual Prompt Based Personalized Federated Learning. (arXiv:2303.08678v1 [cs.LG])

    [http://arxiv.org/abs/2303.08678](http://arxiv.org/abs/2303.08678)

    本文提出了一种基于视觉提示的PFL框架pFedPT，通过利用个性化的视觉提示隐含地表示客户端的本地数据分布信息，并将该信息提供给聚合模型来帮助分类任务，相对于其他算法取得了更好的效果。

    

    个性化联邦学习（PFL）作为一种流行的分布式学习范式，允许个性化模型通过利用所有分布式客户端的知识来提高泛化能力和鲁棒性。大多数现有的PFL算法采用基于模型的方法来解决个性化问题，例如个性化的层划分、模型正则化和模型插值，这些方法都无法考虑到分布式客户端的数据特征。本文提出了一种新颖的用于图像分类任务的PFL框架pFedPT，它利用个性化的视觉提示来隐含地表示客户端的本地数据分布信息，并将该信息提供给聚合模型来帮助分类任务。具体来说，在pFedPT训练的每一轮中，每个客户端都会生成与本地数据分布相关的个性化本地提示，然后在由原始数据和视觉提示组成的输入上训练本地模型以学习分布式数据特征，最后将视觉提示发送到集中式聚合器以更新全局模型。在几个基准测试数据集上的实验结果表明，相对于其他最先进的PFL算法，pFedPT具有更好的效果。

    As a popular paradigm of distributed learning, personalized federated learning (PFL) allows personalized models to improve generalization ability and robustness by utilizing knowledge from all distributed clients. Most existing PFL algorithms tackle personalization in a model-centric way, such as personalized layer partition, model regularization, and model interpolation, which all fail to take into account the data characteristics of distributed clients. In this paper, we propose a novel PFL framework for image classification tasks, dubbed pFedPT, that leverages personalized visual prompts to implicitly represent local data distribution information of clients and provides that information to the aggregation model to help with classification tasks. Specifically, in each round of pFedPT training, each client generates a local personalized prompt related to local data distribution. Then, the local model is trained on the input composed of raw data and a visual prompt to learn the distrib
    
[^20]: 用机器学习生成时尚模特的摆姿建议和生成

    Fashion-model pose recommendation and generation using Machine Learning. (arXiv:2303.08660v1 [cs.CV])

    [http://arxiv.org/abs/2303.08660](http://arxiv.org/abs/2303.08660)

    本研究使用机器学习算法将图像分割，并为时尚人员建议与输入图像相似的摆姿，同时扩展了工作生成合成图像。

    

    时尚模特的摆姿是时尚行业中很重要的属性。创意总监、模特制作公司和顶尖摄影师总是在寻找能够正确摆姿的专业模特，没有正确摆姿的技能，他们就很难获得专业的模特工作。本研究集中于为时尚人员建议一系列与输入图像相似的图像。该图像被分割成不同的部分，并为用户建议相似的图像。这是通过计算输入图像的颜色直方图并将其应用于数据集中的所有图像并比较直方图来实现的。为了避免隐私问题和克服摄影成本高的问题，生成合成图像变得越来越流行。因此，本文还扩展了从建议中生成合成图像的工作。

    Fashion-model pose is an important attribute in the fashion industry. Creative directors, modeling production houses, and top photographers always look for professional models able to pose. without the skill to correctly pose, their chances of landing professional modeling employment are regrettably quite little. There are occasions when models and photographers are unsure of the best pose to strike while taking photographs. This research concentrates on suggesting the fashion personnel a series of similar images based on the input image. The image is segmented into different parts and similar images are suggested for the user. This was achieved by calculating the color histogram of the input image and applying the same for all the images in the dataset and comparing the histograms. Synthetic images have become popular to avoid privacy concerns and to overcome the high cost of photoshoots. Hence, this paper also extends the work of generating synthetic images from the recommendation en
    
[^21]: 医学单细胞图像多实例学习模型的像素级解释

    Pixel-Level Explanation of Multiple Instance Learning Models in Biomedical Single Cell Images. (arXiv:2303.08632v1 [eess.IV])

    [http://arxiv.org/abs/2303.08632](http://arxiv.org/abs/2303.08632)

    本文研究了四种属性方法来解释医学单细胞图像多实例学习模型，并在血癌患者的血液涂片中推导出像素级别的血癌诊断解释。

    

    解释性是临床决策支持系统关键要求。多实例学习提供实例级解释，但许多临床应用需要更深入的像素级解释，但迄今为止尚未实现。本文研究了四种属性方法，即GradCAM、LRP、IBA和InputIBA，以解释多实例学习模型。通过这些方法，我们可以从患者的血液涂片中推导出像素级别的血癌诊断解释。我们研究了两组急性髓性白血病数据集，其中包含超过100,000个单个细胞图像，并观察每种属性方法在关注白细胞单个细胞的不同属性的多实例学习架构上的表现。此外，我们将属性图与医学专家的注释进行比较，以了解四种属性方法的效果。

    Explainability is a key requirement for computer-aided diagnosis systems in clinical decision-making. Multiple instance learning with attention pooling provides instance-level explainability, however for many clinical applications a deeper, pixel-level explanation is desirable, but missing so far. In this work, we investigate the use of four attribution methods to explain a multiple instance learning models: GradCAM, Layer-Wise Relevance Propagation (LRP), Information Bottleneck Attribution (IBA), and InputIBA. With this collection of methods, we can derive pixel-level explanations on for the task of diagnosing blood cancer from patients' blood smears. We study two datasets of acute myeloid leukemia with over 100 000 single cell images and observe how each attribution method performs on the multiple instance learning architecture focusing on different properties of the white blood single cells. Additionally, we compare attribution maps with the annotations of a medical expert to see ho
    
[^22]: 平滑Q学习

    Smoothed Q-learning. (arXiv:2303.08631v1 [cs.LG])

    [http://arxiv.org/abs/2303.08631](http://arxiv.org/abs/2303.08631)

    本文提出了一种能够缓解Q学习算法高估问题的平滑Q学习算法。

    

    在强化学习中，Q学习算法可以证明收敛到最优解。然而，正如其他人所证明的那样，Q学习也可能高估价值，因此花费太长时间探索无用状态。双Q学习是一种可证明收敛的替代方法，可以缓解一些高估问题，但有时以更慢的收敛代价。我们引入了一种替代算法，用平均值替换了最大化操作，从而得到一种可证明收敛，且能缓解高估问题的离线算法，同时保持与标准Q学习类似的收敛特性。

    In Reinforcement Learning the Q-learning algorithm provably converges to the optimal solution. However, as others have demonstrated, Q-learning can also overestimate the values and thereby spend too long exploring unhelpful states. Double Q-learning is a provably convergent alternative that mitigates some of the overestimation issues, though sometimes at the expense of slower convergence. We introduce an alternative algorithm that replaces the max operation with an average, resulting also in a provably convergent off-policy algorithm which can mitigate overestimation yet retain similar convergence as standard Q-learning.
    
[^23]: 通过变分自编码器和域适应实现无偏态形态分类

    From Images to Features: Unbiased Morphology Classification via Variational Auto-Encoders and Domain Adaptation. (arXiv:2303.08627v1 [astro-ph.GA])

    [http://arxiv.org/abs/2303.08627](http://arxiv.org/abs/2303.08627)

    本研究提出了一种无偏态的星系形态分类方法，利用变分自编码器和域适应实现低维度表示，40维潜在变量能够有效再现星系图像中的大多数形态特征，并通过经典随机森林分类器实现了详细的形态特征分类。此外，该方法可以无偏地应用于两个不同的星系调查中。

    

    我们提出了一种新的方法，通过利用变分自编码器（VAE）和域适应（DA）的组合来降低星系图像的维度。我们使用Galaxy-Zoo DECaLS项目中具有详细形态类型标签的低红移星系的样本，证明了该方法的有效性。我们证明了40维潜在变量能够有效地再现星系图像中的大多数形态特征。为了进一步验证我们方法的有效性，我们利用40维潜在变量上的经典随机森林（RF）分类器来实现详细的形态特征分类。这种方法与直接应用神经网络分类效果相似。我们进一步通过使用DECaLS和BASS+MzLS重叠区域中的星系来调整VAE网络，从而增强了我们的模型，使其能够无偏地应用于这两个调查中的星系图像。我们观察到，在DA期间，噪声得到了有效抑制。

    We present a novel approach for the dimensionality reduction of galaxy images by leveraging a combination of variational auto-encoders (VAE) and domain adaptation (DA). We demonstrate the effectiveness of this approach using a sample of low redshift galaxies with detailed morphological type labels from the Galaxy-Zoo DECaLS project. We show that 40-dimensional latent variables can effectively reproduce most morphological features in galaxy images. To further validate the effectiveness of our approach, we utilised a classical random forest (RF) classifier on the 40-dimensional latent variables to make detailed morphology feature classifications. This approach performs similarly to a direct neural network application on galaxy images. We further enhance our model by tuning the VAE network via DA using galaxies in the overlapping footprint of DECaLS and BASS+MzLS, enabling the unbiased application of our model to galaxy images in both surveys. We observed that noise suppression during DA 
    
[^24]: 作为基础模型的超矩形可解释集成模型

    Interpretable Ensembles of Hyper-Rectangles as Base Models. (arXiv:2303.08625v1 [cs.LG])

    [http://arxiv.org/abs/2303.08625](http://arxiv.org/abs/2303.08625)

    本文提出了一种基于超矩形的可解释的集成模型，它将均匀生成的轴对齐超矩形作为基模型，并成功地避免了过拟合。

    

    本文提出了一种基于简单集成模型的新型模型，使用均匀生成的轴对齐超矩形作为基模型（HRBM）。研究了两种类型的HRBM：封闭矩形和角落。HRBM的主要思想是考虑并计算每个矩形内外的训练样例数量。提出将HRBM纳入梯度提升机（GBM）中。尽管HRBM很简单，但这些简单的基础模型允许我们构建有效的集成模型并避免过拟合。考虑了一种计算集成模型的最优正则化参数的简单方法，该方法可以在GBM的每次迭代中以显式方式修改。此外，研究了一种新的正则化，称为“阶梯高度惩罚”，除了标准的L1和L2正则化。提出了一种非常简单的方法来使用著名的SHAP方法对所提出的集成模型预测进行解释。结果显示，G

    A new extremely simple ensemble-based model with the uniformly generated axis-parallel hyper-rectangles as base models (HRBM) is proposed. Two types of HRBMs are studied: closed rectangles and corners. The main idea behind HRBM is to consider and count training examples inside and outside each rectangle. It is proposed to incorporate HRBMs into the gradient boosting machine (GBM). Despite simplicity of HRBMs, it turns out that these simple base models allow us to construct effective ensemble-based models and avoid overfitting. A simple method for calculating optimal regularization parameters of the ensemble-based model, which can be modified in the explicit way at each iteration of GBM, is considered. Moreover, a new regularization called the "step height penalty" is studied in addition to the standard L1 and L2 regularizations. An extremely simple approach to the proposed ensemble-based model prediction interpretation by using the well-known method SHAP is proposed. It is shown that G
    
[^25]: 零样本对比损失用于文本引导扩散图像风格迁移

    Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer. (arXiv:2303.08622v1 [cs.CV])

    [http://arxiv.org/abs/2303.08622](http://arxiv.org/abs/2303.08622)

    本文提出了一种适用于文本引导图像风格迁移中的零样本对比损失方法，可以在不需要额外训练的情况下生成具有相同语义内容的图像。

    

    扩散模型在文本引导图像风格迁移中表现出极大的潜力，但由于其随机性而存在风格转换和内容保护之间的权衡。现有方法需要计算密集的扩散模型微调或附加神经网络。为了解决这个问题，我们在扩散模型中提出了一种零样本对比损失，它不需要额外的微调或辅助网络。通过利用预训练的扩散模型中生成样本和原始图像嵌入之间的图块对比损失，我们的方法可以以零样本的方式生成具有与源图像相同语义内容的图像。我们的方法在保留内容且不需要额外训练的同时，在图像风格迁移、图像到图像的转换和操作中均优于现有方法。我们的实验结果证实了我们提出的方法的有效性。

    Diffusion models have shown great promise in text-guided image style transfer, but there is a trade-off between style transformation and content preservation due to their stochastic nature. Existing methods require computationally expensive fine-tuning of diffusion models or additional neural network. To address this, here we propose a zero-shot contrastive loss for diffusion models that doesn't require additional fine-tuning or auxiliary networks. By leveraging patch-wise contrastive loss between generated samples and original image embeddings in the pre-trained diffusion model, our method can generate images with the same semantic content as the source image in a zero-shot manner. Our approach outperforms existing methods while preserving content and requiring no additional training, not only for image style transfer but also for image-to-image translation and manipulation. Our experimental results validate the effectiveness of our proposed method.
    
[^26]: 学习奖励信息获取：正确计分规则遇到委托代理模型

    Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model. (arXiv:2303.08613v1 [cs.LG])

    [http://arxiv.org/abs/2303.08613](http://arxiv.org/abs/2303.08613)

    本文设计了一种样本高效算法，将 UCB 算法（Auer等人，2002）应用于委托代理模型的在线设置，该算法能够通过与策略代理多次互动来设计最优的计分规则，并实现良好的效果。

    

    本文研究委托代理模型中的激励信息获取问题。此问题被建模为委托方和代理方之间的 Stackelberg 博弈，其中委托人宣布了一条得分规则来指定付款，然后代理方选择最大化其自身利润和报告信息的努力水平。我们从委托方的角度研究这个问题的在线设置，即通过与策略代理多次交互来设计最优计分规则。我们设计了一种可证明的样本高效算法，将 UCB 算法 (Auer et al., 2002) 量身定制到我们的模型中，其在 T 次迭代后实现了次线性 $T^{2/3}$-遗憾。我们的算法具有对委托方最优利润进行精细估计的过程以及保守纠正方案，以确保代理方的行动得到有效激励。此外，我们的遗憾界的一个关键特征是它是渐进最小可实现的。

    We study the incentivized information acquisition problem, where a principal hires an agent to gather information on her behalf. Such a problem is modeled as a Stackelberg game between the principal and the agent, where the principal announces a scoring rule that specifies the payment, and then the agent then chooses an effort level that maximizes her own profit and reports the information. We study the online setting of such a problem from the principal's perspective, i.e., designing the optimal scoring rule by repeatedly interacting with the strategic agent. We design a provably sample efficient algorithm that tailors the UCB algorithm (Auer et al., 2002) to our model, which achieves a sublinear $T^{2/3}$-regret after $T$ iterations. Our algorithm features a delicate estimation procedure for the optimal profit of the principal, and a conservative correction scheme that ensures the desired agent's actions are incentivized. Furthermore, a key feature of our regret bound is that it is i
    
[^27]: 自动化注意力裁剪：使用注意力改进和自动化模型裁剪

    Automatic Attention Pruning: Improving and Automating Model Pruning using Attentions. (arXiv:2303.08595v1 [cs.LG])

    [http://arxiv.org/abs/2303.08595](http://arxiv.org/abs/2303.08595)

    本文提出一种自适应、基于注意力的结构化剪枝方法，以自动产生小、准确和硬件有效的模型以满足用户的目标。

    

    裁剪是一种压缩深度学习模型以在资源受限的边缘设备上部署它们的有希望的方法。然而，许多现有的剪枝解决方案基于非结构化剪枝，导致模型在商用硬件上无法高效运行；而且它们经常需要用户手动探索和调整剪枝过程，这是耗费时间且常常导致次优结果的。为了解决这些限制，本文提出了自动化注意力裁剪（AAP），一种自适应、基于注意力的结构化剪枝方法，以自动产生小、准确和硬件有效的模型以满足用户的目标。首先，它提出了使用基于激活的注意力映射进行迭代结构化剪枝来有效地识别和修剪不重要的滤波器。然后，它提出了自适应剪枝策略，以自动满足精度关键、内存受限和延迟敏感任务的剪枝目标。全面的评估显示了 AAP 的优越性。

    Pruning is a promising approach to compress deep learning models in order to deploy them on resource-constrained edge devices. However, many existing pruning solutions are based on unstructured pruning, which yields models that cannot efficiently run on commodity hardware; and they often require users to manually explore and tune the pruning process, which is time-consuming and often leads to sub-optimal results. To address these limitations, this paper presents Automatic Attention Pruning (AAP), an adaptive, attention-based, structured pruning approach to automatically generate small, accurate, and hardware-efficient models that meet user objectives. First, it proposes iterative structured pruning using activation-based attention maps to effectively identify and prune unimportant filters. Then, it proposes adaptive pruning policies for automatically meeting the pruning objectives of accuracy-critical, memory-constrained, and latency-sensitive tasks. A comprehensive evaluation shows th
    
[^28]: Delay-SDE-net：一种用于具有记忆和不确定性估计的时间序列建模的深度学习方法

    Delay-SDE-net: A deep learning approach for time series modelling with memory and uncertainty estimates. (arXiv:2303.08587v1 [cs.LG])

    [http://arxiv.org/abs/2303.08587](http://arxiv.org/abs/2303.08587)

    本研究提出了一种基于随机延迟微分方程的神经网络模型Delay-SDE-net，可以准确地建模具有记忆效应的时间序列，并且能够对模型的不确定性进行实时估计。

    

    在许多领域中，准确地建模时间序列非常重要。由于世界通常过于复杂以至无法准确地建模，因此评估动态系统处于特定状态的概率常常有意义。本文提出了Delay-SDE-net，这是一种基于随机延迟微分方程（SDDEs）的神经网络模型。使用具有多个延迟的SDDE作为建模框架，使其成为具有记忆效应的时间序列的合适模型，因为它通过系统之前的状态包括记忆。 Delay-SDE-net的随机部分提供了估计建模中不确定性的基础，并被分成两个神经网络以解释先验性和后验性不确定性。这种不确定性是实时提供的，使得该模型适用于时间匮乏的应用。我们推导了Delay-SDE-net的理论误差，并进行了数值收敛率分析。在与类似模型的比较中，Delay-SDE-net显示出更加稳定的性能。

    To model time series accurately is important within a wide range of fields. As the world is generally too complex to be modelled exactly, it is often meaningful to assess the probability of a dynamical system to be in a specific state. This paper presents the Delay-SDE-net, a neural network model based on stochastic delay differential equations (SDDEs). The use of SDDEs with multiple delays as modelling framework makes it a suitable model for time series with memory effects, as it includes memory through previous states of the system. The stochastic part of the Delay-SDE-net provides a basis for estimating uncertainty in modelling, and is split into two neural networks to account for aleatoric and epistemic uncertainty. The uncertainty is provided instantly, making the model suitable for applications where time is sparse. We derive the theoretical error of the Delay-SDE-net and analyze the convergence rate numerically. At comparisons with similar models, the Delay-SDE-net has consisten
    
[^29]: 高维多视角聚类方法

    High-dimensional multi-view clustering methods. (arXiv:2303.08582v1 [cs.LG])

    [http://arxiv.org/abs/2303.08582](http://arxiv.org/abs/2303.08582)

    本论文比较了两类高维多视角聚类方法（基于图和基于子空间），重点关注了如何处理高阶相关性，并在基准数据集上进行了实验研究。

    

    最近几年，相比于单视角聚类，多视角聚类被广泛应用于数据分析中。它可以提供更多的数据信息，但也带来了一些挑战，如如何组合这些视角或特征。最近的研究主要集中在张量表示上，而不是将数据视为简单的矩阵。这种方法可以处理数据之间的高阶相关性，而基于矩阵的方法则难以捕捉这种相关性。因此，我们将研究和比较这些方法，特别是基于图的聚类和子空间聚类，以及在基准数据集上的实验结果。

    Multi-view clustering has been widely used in recent years in comparison to single-view clustering, for clear reasons, as it offers more insights into the data, which has brought with it some challenges, such as how to combine these views or features. Most of recent work in this field focuses mainly on tensor representation instead of treating the data as simple matrices. This permits to deal with the high-order correlation between the data which the based matrix approach struggles to capture. Accordingly, we will examine and compare these approaches, particularly in two categories, namely graph-based clustering and subspace-based clustering. We will conduct and report experiments of the main clustering methods over a benchmark datasets.
    
[^30]: 探究GANsformer：一种最先进的图像生成模型的复制研究

    Investigating GANsformer: A Replication Study of a State-of-the-Art Image Generation Model. (arXiv:2303.08577v1 [cs.CV])

    [http://arxiv.org/abs/2303.08577](http://arxiv.org/abs/2303.08577)

    本文通过重新创建一种新的GAN变体GANformer，并对作者的声明进行评论，以探究最先进的图像生成模型。

    

    现今广泛讨论的图像生成领域可以用于各种应用，例如升级现有图像，创建不存在的物体（如室内设计场景、产品甚至人脸），并实现传输学习过程。生成对抗网络（GANs）是一种广泛研究的机器学习框架，旨在实现上述目标。本文复制评估了原始GAN网络的一种新变体GANformer，该变体是Hudson和Zitnick在“生成对抗Transformer”中提出的。由于资源和时间限制，我们不得不限制网络的训练时间、数据集类型和大小。研究成功地重新创建了原始结果并对作者的声明进行了评论。

    The field of image generation through generative modelling is abundantly discussed nowadays. It can be used for various applications, such as up-scaling existing images, creating non-existing objects, such as interior design scenes, products or even human faces, and achieving transfer-learning processes. In this context, Generative Adversarial Networks (GANs) are a class of widely studied machine learning frameworks first appearing in the paper "Generative adversarial nets" by Goodfellow et al. that achieve the goal above. In our work, we reproduce and evaluate a novel variation of the original GAN network, the GANformer, proposed in "Generative Adversarial Transformers" by Hudson and Zitnick. This project aimed to recreate the methods presented in this paper to reproduce the original results and comment on the authors' claims. Due to resources and time limitations, we had to constrain the network's training times, dataset types, and sizes. Our research successfully recreated both vari
    
[^31]: WikiCoder：学习编写知识增强代码

    WikiCoder: Learning to Write Knowledge-Powered Code. (arXiv:2303.08574v1 [cs.LG])

    [http://arxiv.org/abs/2303.08574](http://arxiv.org/abs/2303.08574)

    WikiCoder是一种利用知识图谱进行程序合成的系统，可以自动从输入输出例子中学习代码，用于解决需要使用外部知识的问题，能够解决以前无法解决的任务。

    

    我们解决了从一些输入输出的例子中自动生成计算机程序的问题。本文的出发点是观察到在许多应用中，解决方案程序必须使用输入输出例子中不存在的外部知识：我们称这样的程序为知识增强程序，因为它们可以参考从知识图谱（例如维基百科）中收集的信息。本文迈出了向知识增强程序合成的第一步。我们提出了 WikiCoder，该系统基于最先进的机器学习程序合成器，并集成了知识图谱。我们评估了它以展示其在不同领域中的广泛适用性，并讨论了其局限性。WikiCoder利用知识图谱解决了以前没有程序合成器能够解决的任务，并与该领域的最新发展相结合，以实现大规模运作。

    We tackle the problem of automatic generation of computer programs from a few pairs of input-output examples. The starting point of this work is the observation that in many applications a solution program must use external knowledge not present in the examples: we call such programs knowledge-powered since they can refer to information collected from a knowledge graph such as Wikipedia. This paper makes a first step towards knowledge-powered program synthesis. We present WikiCoder, a system building upon state of the art machine-learned program synthesizers and integrating knowledge graphs. We evaluate it to show its wide applicability over different domains and discuss its limitations. WikiCoder solves tasks that no program synthesizers were able to solve before thanks to the use of knowledge graphs, while integrating with recent developments in the field to operate at scale.
    
[^32]: 区分分类数据中的因果关系：均匀通道模型

    Distinguishing Cause from Effect on Categorical Data: The Uniform Channel Model. (arXiv:2303.08572v1 [cs.LG])

    [http://arxiv.org/abs/2303.08572](http://arxiv.org/abs/2303.08572)

    本文提出了一种名为“均匀通道模型”的方法，用于区分分类数据中的因果关系，该方法将条件概率质量函数视为离散无记忆通道，并选择最可能的因果方向使得条件概率质量函数更接近于均匀分布。

    

    区分因果关系是因果发现中的核心问题。大多数用于此任务的方法，即加性噪声模型（ANM），仅适用于定量数据。我们提出了一个在分类变量（属于没有有意义顺序的集合）上解决因果-效应问题的标准，灵感来源于将条件概率质量函数（pmf）视为离散无记忆通道。我们选择最可能的因果方向，其中条件pmf更接近均匀通道（UC）。原理是，在UC中，正如在ANM中一样，（给定因果关系的效应）条件熵独立于因果分布，符合因果和机制独立性原则。因此，我们的方法称为均匀通道模型（UCM），扩展了ANM理论到分类变量。为了评估从数据估计的条件pmf与UC的接近程度，我们使用s

    Distinguishing cause from effect using observations of a pair of random variables is a core problem in causal discovery. Most approaches proposed for this task, namely additive noise models (ANM), are only adequate for quantitative data. We propose a criterion to address the cause-effect problem with categorical variables (living in sets with no meaningful order), inspired by seeing a conditional probability mass function (pmf) as a discrete memoryless channel. We select as the most likely causal direction the one in which the conditional pmf is closer to a uniform channel (UC). The rationale is that, in a UC, as in an ANM, the conditional entropy (of the effect given the cause) is independent of the cause distribution, in agreement with the principle of independence of cause and mechanism. Our approach, which we call the uniform channel model (UCM), thus extends the ANM rationale to categorical variables. To assess how close a conditional pmf (estimated from data) is to a UC, we use s
    
[^33]: 敏感度感知的视觉参数低效调整

    Sensitivity-Aware Visual Parameter-Efficient Tuning. (arXiv:2303.08566v1 [cs.CV])

    [http://arxiv.org/abs/2303.08566](http://arxiv.org/abs/2303.08566)

    本文提出了敏感度感知的视觉参数低效调整（SPT）方案，可以自适应地将可训练参数分配到任务特定的重要位置，以提高表示能力，适应预训练视觉模型到下游任务。

    

    视觉参数低效调整（VPET）已成为自适应预训练视觉模型到下游任务的强劲替代方法。现有VPET方法根据人工启发式方法将可训练参数引入不同任务的相同位置，忽略领域差异。本文提出了一种新颖的敏感度感知的视觉参数低效调整（SPT）方案，以自适应的方式分配可训练参数到任务特定的重要位置，给定所需的可调参数预算。本文首先依据数据的相关性快速识别特定任务所需调整的敏感参数，然后提升表示能力，增大重要的权重矩阵数量。

    Visual Parameter-Efficient Tuning (VPET) has become a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimization difficulty. However, existing VPET methods introduce trainable parameters to the same positions across different tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to introduce and how to allocate trainable parameters by proposing a novel Sensitivity-aware visual Parameter-efficient Tuning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a desired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tuning for a given task in a data-dependent way. Next, our SPT further boosts the representational capability for the weight matrices whose number of se
    
[^34]: 联合图形和顶点重要性学习

    Joint Graph and Vertex Importance Learning. (arXiv:2303.08552v1 [eess.SP])

    [http://arxiv.org/abs/2303.08552](http://arxiv.org/abs/2303.08552)

    本研究提出了一种新颖的方法学习图形，并产生了比其他方法更为稀疏和可解释的模型。

    

    本文从不规则感知图傅里叶变换的角度探讨了图形学习的话题，旨在学习图信号空间内积以更好地建模数据。我们提出了一种新颖的方法，通过学习比组合拉普拉斯方法具有更小的边缘权重上界的图形。实验结果表明，我们的方法比组合拉普拉斯方法产生更稀疏的图形，并具有更可解释的模型。

    In this paper, we explore the topic of graph learning from the perspective of the Irregularity-Aware Graph Fourier Transform, with the goal of learning the graph signal space inner product to better model data. We propose a novel method to learn a graph with smaller edge weight upper bounds compared to combinatorial Laplacian approaches. Experimentally, our approach yields much sparser graphs compared to a combinatorial Laplacian approach, with a more interpretable model.
    
[^35]: 在多晶Zr微结构中，将U-Net用于线性弹性应力估计的适应性。

    Adapting U-Net for linear elastic stress estimation in polycrystal Zr microstructures. (arXiv:2303.08541v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2303.08541](http://arxiv.org/abs/2303.08541)

    该论文提出了一种适用于多晶Zr微结构中线性弹性应力估计的U-Net神经网络架构。网络的性能不受晶粒结构规则性或纹理影响，可以推广到任意Zr晶体结构，与有限元分析相比，具有更快的速度（大约200x至6000x）和较小的内存开销，但最高精度可能会降低10％。

    

    提出了一种U-Net卷积神经网络架构的变体，用于估计a-Zr（hcp）多晶晶粒结构中的线性弹性兼容应力。使用VGrain软件生成训练数据，晶粒结构的正则性α为0.73，且具有均匀随机方向，使用ABAQUS对应力焊缝进行有限元方法评估。初始数据集包含200个样本，其中20个用于验证的保留。与有限元分析相比，网络在CPU或GPU上可以加速约200倍至6000倍，并具有显著的内存节省，准确度略微降低最多10％。网络性能与晶粒结构的规则性或纹理无关，表明网络可以推广到任意的Zr晶体结构。测量了使用200和400个样本进行训练时的性能，发现当数据集的大小增加一倍时，准确度约提高10％。

    A variant of the U-Net convolutional neural network architecture is proposed to estimate linear elastic compatibility stresses in a-Zr (hcp) polycrystalline grain structures. Training data was generated using VGrain software with a regularity alpha of 0.73 and uniform random orientation for the grain structures and ABAQUS to evaluate the stress welds using the finite element method. The initial dataset contains 200 samples with 20 held from training for validation. The network gives speedups of around 200x to 6000x using a CPU or GPU, with signifcant memory savings, compared to finite element analysis with a modest reduction in accuracy of up to 10%. Network performance is not correlated with grain structure regularity or texture, showing generalisation of the network beyond the training set to arbitrary Zr crystal structures. Performance when trained with 200 and 400 samples was measured, finding an improvement in accuracy of approximately 10% when the size of the dataset was doubled.
    
[^36]: 基于Diamond Stacked稀疏自编码器集成模型的运动障碍患者的健康监测

    Health Monitoring of Movement Disorder Subject based on Diamond Stacked Sparse Autoencoder Ensemble Model. (arXiv:2303.08538v1 [cs.LG])

    [http://arxiv.org/abs/2303.08538](http://arxiv.org/abs/2303.08538)

    本文提出了一种基于Diamond Stacked稀疏自编码器集成模型的健康监测算法，通过利用特征嵌入堆叠稀疏自编码器（FSSAE）进行特征扩展和使用Diamond层结构去除冗余和非信息特征，有效地提高了运动障碍患者的诊断精度。

    

    运动障碍患者的健康监测非常重要，因为他们的活动能力有限，慢性病的持续时间很长。目前，利用可穿戴传感器收集运动障碍患者的数据并进行基于机器学习的处理是一种有效的健康监测方法。然而，可穿戴传感器系统难以获得高质量和大量数据，无法满足诊断精度的要求。此外，现有的机器学习方法不能很好地处理这个问题。为了解决这个问题，本文提出了基于Diamond Stacked稀疏自编码器集成模型（DsaeEM）的运动障碍患者健康监测算法。该算法有两个主要组成部分。首先，使用特征嵌入堆叠稀疏自编码器（FSSAE）设计特征扩展。其次，设计了一种特征还原机制，使用Diamond层结构去除冗余和非信息特征。该模型在可穿戴传感器获得的真实数据上进行评估，相比现有的机器学习方法，显示出显著的诊断精度提高。

    The health monitoring of chronic diseases is very important for people with movement disorders because of their limited mobility and long duration of chronic diseases. Machine learning-based processing of data collected from the human with movement disorders using wearable sensors is an effective method currently available for health monitoring. However, wearable sensor systems are difficult to obtain high-quality and large amounts of data, which cannot meet the requirement for diagnostic accuracy. Moreover, existing machine learning methods do not handle this problem well. Feature learning is key to machine learning. To solve this problem, a health monitoring of movement disorder subject based on diamond stacked sparse autoencoder ensemble model (DsaeEM) is proposed in this paper. This algorithm has two major components. First, feature expansion is designed using feature-embedded stacked sparse autoencoder (FSSAE). Second, a feature reduction mechanism is designed to remove the redund
    
[^37]: 观看或听取：具有视觉损坏建模和可靠性评分的强韧音视频语音识别

    Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring. (arXiv:2303.08536v1 [cs.MM])

    [http://arxiv.org/abs/2303.08536](http://arxiv.org/abs/2303.08536)

    本论文研究了音视频语音识别在多模态输入损坏情况下的问题，并设计了音视频可靠性评分模块来提高模型的韧性。

    

    本文针对音视频语音识别（AVSR）在多模态输入损坏情况下进行研究，其中音频输入和视觉输入均受损，这在先前的研究方向中没有得到很好的解决。先前的研究集中于如何用清晰的视觉输入来补充受损的音频输入，假设可用清晰的视觉输入，但在现实生活中，清晰的视觉输入并不总是可用的，甚至可能被遮挡的唇部区域或噪音所损坏。

    This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal input corruption situations where audio inputs and visual inputs are both corrupted, which is not well addressed in previous research directions. Previous studies have focused on how to complement the corrupted audio inputs with the clean visual inputs with the assumption of the availability of clean visual inputs. However, in real life, clean visual inputs are not always accessible and can even be corrupted by occluded lip regions or noises. Thus, we firstly analyze that the previous AVSR models are not indeed robust to the corruption of multimodal input streams, the audio and the visual inputs, compared to uni-modal models. Then, we design multimodal input corruption modeling to develop robust AVSR models. Lastly, we propose a novel AVSR framework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that is robust to the corrupted multimodal inputs. The AV-RelScore can determine which input modal 
    
[^38]: 带有 Metropolis Monte Carlo 动力学的盒子中随机行走的奇异弛豫

    Singular relaxation of a random walk in a box with a Metropolis Monte Carlo dynamics. (arXiv:2303.08535v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2303.08535](http://arxiv.org/abs/2303.08535)

    本文分析了一个在盒子中移动的粒子的蒙特卡罗算法的弛豫本征模式，发现当跳跃长度与盒子大小相当时，弛豫本征模式数量可能非常少，初始条件的对称性的合适选择可以使得向平衡状态的局部化衰减。

    

    我们对一个简单蒙特卡罗算法的弛豫本征模式进行了分析，该算法对应于一个在盒子中移动的粒子，其通过均匀随机跳跃进行运动。盒子外的跳跃被拒绝。长时间后，系统接近平衡概率密度，在盒子内部是均匀的。我们表明，这种平衡状态下的弛豫是不寻常的：对于跳跃长度与盒子大小相当的情况，弛豫本征模式的数量可能非常少，为一或两个。我们提供了完整的分析性描述，来描述两种情况之间的转变。当只存在单个弛豫本征模式时，初始条件的对称性的合适选择使得向平衡状态的局部化衰减。在这种情况下，偏离平衡的集中到盒子边缘，这里拒绝概率最大。最后，除了主方程的松弛分析外，我们还描述了主方程的完整本征谱。

    We study analytically the relaxation eigenmodes of a simple Monte Carlo algorithm, corresponding to a particle in a box which moves by uniform random jumps. Moves outside of the box are rejected. At long times, the system approaches the equilibrium probability density, which is uniform inside the box. We show that the relaxation towards this equilibrium is unusual: for a jump length comparable to the size of the box, the number of relaxation eigenmodes can be surprisingly small, one or two. We provide a complete analytic description of the transition between these two regimes. When only a single relaxation eigenmode is present, a suitable choice of the symmetry of the initial conditions gives a localizing decay to equilibrium. In this case, the deviation from equilibrium concentrates at the edges of the box where the rejection probability is maximal. Finally, in addition to the relaxation analysis of the master equation, we also describe the full eigen-spectrum of the master equation i
    
[^39]: 从观测数据中实现公平的非自助学习

    Fair Off-Policy Learning from Observational Data. (arXiv:2303.08516v1 [cs.LG])

    [http://arxiv.org/abs/2303.08516](http://arxiv.org/abs/2303.08516)

    本文针对非自助学习的算法公平性问题，提出了一种新的公平非自助学习框架，可以从偏见可能存在的观测数据中学习决策规则。

    

    企业和组织必须确保其算法决策的公平性，以满足立法、道德和社会要求。本文针对非自助学习的算法公平性问题，提出了一种新的公平非自助学习框架，可以从观测数据中学习决策规则，以不同的公平概念明确假定观测数据是在不同（潜在偏见的）行为策略下收集的。

    Businesses and organizations must ensure that their algorithmic decision-making is fair in order to meet legislative, ethical, and societal demands. For example, decision-making in automated hiring must not discriminate with respect to gender or race. To achieve this, prior research has contributed approaches that ensure algorithmic fairness in machine learning predictions, while comparatively little effort has focused on algorithmic fairness in decision models, specifically off-policy learning. In this paper, we propose a novel framework for fair off-policy learning: we learn decision rules from observational data under different notions of fairness, where we explicitly assume that observational data were collected under a different -- potentially biased -- behavioral policy. For this, we first formalize different fairness notions for off-policy learning. We then propose a machine learning approach to learn optimal policies under these fairness notions. Specifically, we reformulate th
    
[^40]: 规避扩散模型中添加的噪声对数据进行保护的挑战

    The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models. (arXiv:2303.08500v1 [cs.LG])

    [http://arxiv.org/abs/2303.08500](http://arxiv.org/abs/2303.08500)

    保护个人隐私信息是很重要的，但规避扩散模型中添加的噪声对数据进行保护存在挑战。AVATAR算法借助扩散模型的威力，提供了一种精心设计的去噪过程来消除数据保护扰动的影响，并获得了在多个数据集上的最先进的性能。

    

    保护个人数据免受机器学习模型的利用至关重要。最近，可用性攻击展现出提供额外保护措施的巨大潜力，以防止未经授权地使用数据来训练神经网络。这些方法旨在向干净数据添加难以察觉的噪声，使神经网络无法从受保护的数据中提取有意义的模式，声称可以使个人数据“无法利用”。在本文中，我们针对这种方法提供了一个强有力的对抗措施，表明不可利用的数据可能只是一种幻觉。特别地，我们利用扩散模型的威力，并展示精心设计的去噪过程可以消除数据保护扰动的影响。我们严谨地分析了我们的算法，并在理论上证明了所需去噪的量直接与数据保护扰动的数量成正比。我们的方法名为AVATAR，在包括CelebA数据集在内的多个数据集上提供了最先进的性能，其中它以巨大的优势胜出现有方法。

    Protecting personal data against the exploitation of machine learning models is of paramount importance. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data "unexploitable." In this paper, we provide a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can defuse the ramifications of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-o
    
[^41]: 用合成的3D场景图生成无人驾驶交通场景

    Unsupervised Traffic Scene Generation with Synthetic 3D Scene Graphs. (arXiv:2303.08473v1 [cs.CV])

    [http://arxiv.org/abs/2303.08473](http://arxiv.org/abs/2303.08473)

    本文提出了一种基于领域不变场景表示的方法，使用合成场景图直接合成逼真的交通场景，提高了合成场景图的空间信息，并通过场景操作证明了我们的方法的有效性。

    

    近年来，由计算机图形学驱动的图像合成已经达到了显著的逼真度，然而这种方式生成的合成图像与真实世界的数据之间存在着显著的领域差距。这在自动驾驶场景中尤为明显，而自动驾驶场景又是利用合成数据进行神经网络训练最为关键的方面。我们提出了一种基于领域不变场景表示的方法，直接合成交通场景图像而不进行渲染。具体而言，我们依赖于合成场景图作为我们的内部表示，并引入了一种无监督的神经网络结构来实现逼真的交通场景合成。我们利用空间场景信息增强了合成场景图，并通过场景操作证明了我们的方法的有效性。

    Image synthesis driven by computer graphics achieved recently a remarkable realism, yet synthetic image data generated this way reveals a significant domain gap with respect to real-world data. This is especially true in autonomous driving scenarios, which represent a critical aspect for overcoming utilizing synthetic data for training neural networks. We propose a method based on domain-invariant scene representation to directly synthesize traffic scene imagery without rendering. Specifically, we rely on synthetic scene graphs as our internal representation and introduce an unsupervised neural network architecture for realistic traffic scene synthesis. We enhance synthetic scene graphs with spatial information about the scene and demonstrate the effectiveness of our approach through scene manipulation.
    
[^42]: 基于递归神经网络的光伏电池组的混合物理概率预测模型

    Hybrid-Physical Probabilistic Forecasting for a Set of Photovoltaic Systems using Recurrent Neural Networks. (arXiv:2303.08459v1 [cs.LG])

    [http://arxiv.org/abs/2303.08459](http://arxiv.org/abs/2303.08459)

    本文提出了一种基于递归神经网络的光伏电池组混合物理概率预测模型，通过使用数值天气预测结果作为协变量，改善了光伏系统功率输出的准确性，最终可以达到7.54％的技能评分。

    

    准确预测光伏系统的功率输出对于改善能源分布网络的运行至关重要。本文提出了一种混合-物理模型，在数值天气预测的帮助下，通过使用其作为协变量的PV性能模型和自回归递归神经模型来改进确定性的短时预测。我们重新设计了最初用于零售领域的神经网络模型，并揭示了一种新的截断高斯输出分布。我们将许多模型变量与文献中的替代方案进行了实验比较，并且消融研究表明最佳性能变体中的组件协同工作以达到与NWP驱动的PV性能模型基线相比的技能评分为7.54％。

    Accurate intra-day forecasts of the power output by PhotoVoltaic (PV) systems are critical to improve the operation of energy distribution grids. We describe a hybrid-physical model, which aims at improving deterministic intra-day forecasts, issued by a PV performance model fed by Numerical Weather Predictions (NWP), by using them as covariates in the context of an autoregressive recurrent neural model. Our proposal repurposes a neural model initially used in the retail sector, and discloses a novel truncated Gaussian output distribution. We experimentally compare many model variants to alternatives from the literature, and an ablation study shows that the components in the best performing variant work synergistically to reach a skill score of 7.54% with respect to the NWP-driven PV performance model baseline.
    
[^43]: 数据驱动的物理知识神经网络求解中子扩散本征值问题的不确定性分析

    On the uncertainty analysis of the data-enabled physics-informed neural network for solving neutron diffusion eigenvalue problem. (arXiv:2303.08455v1 [cs.LG])

    [http://arxiv.org/abs/2303.08455](http://arxiv.org/abs/2303.08455)

    本文针对数据不可避免带有噪声的情况，研究了DEPINN在求解中子扩散本征值问题方面的可行性，并提出了创新的区间损失函数用于减少噪声影响和提高先验数据利用率，此方法在两个基准问题上得到了验证。

    

    在实际工程实验中，通过探测器获得的数据不可避免地带有噪声。本文研究了当先验数据包含不同类型噪声时，已经提出的数据驱动的物理知识神经网络（DEPINN）在计算中子扩散本征值问题时的性能。此外，为了减少噪声的影响，提高噪声先验数据的利用率，本文提出了创新的区间损失函数，并给出了一些严格的数学证明。通过大量的数值结果，本文在两个典型的基准问题上检验了DEPINN的鲁棒性，并通过比较证明了所提出的区间损失函数的有效性。本文确认了改进的DEPINN在核反应堆物理实际工程应用中的可行性。

    In practical engineering experiments, the data obtained through detectors are inevitably noisy. For the already proposed data-enabled physics-informed neural network (DEPINN) \citep{DEPINN}, we investigate the performance of DEPINN in calculating the neutron diffusion eigenvalue problem from several perspectives when the prior data contain different scales of noise. Further, in order to reduce the effect of noise and improve the utilization of the noisy prior data, we propose innovative interval loss functions and give some rigorous mathematical proofs. The robustness of DEPINN is examined on two typical benchmark problems through a large number of numerical results, and the effectiveness of the proposed interval loss function is demonstrated by comparison. This paper confirms the feasibility of the improved DEPINN for practical engineering applications in nuclear reactor physics.
    
[^44]: 翻转异常：用虚假健康生成网络进行异常检测

    Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection. (arXiv:2303.08452v1 [eess.IV])

    [http://arxiv.org/abs/2303.08452](http://arxiv.org/abs/2303.08452)

    提出了一种无监督的异常检测方法 PHANES，它可以将健康组织保留并用伪健康重构替换异常区域，有效检测中风损伤。

    

    及早、准确地检测疾病对于患者管理和成功治疗结果至关重要。然而，在医学图像中自动识别异常可以是具有挑战性的。传统方法依赖于难以获取的大型标记数据集。为了克服这些限制，我们引入了一种新的无监督方法，称为PHANES（用于异常分割的虚假健康生成网络）。我们的方法具有翻转异常的能力，即保留健康组织并用伪健康重构替换异常区域。与最近的扩散模型不同，我们的方法不依赖于学习噪声分布，也不会引入随机变化到整个图像。相反，我们使用潜在的生成网络来创建可能异常的掩膜，然后使用修补生成网络进行细化。我们展示了PHANES在T1w脑MRI数据集中检测中风损伤方面的有效性，并展示了显著的性能提升。

    Early and accurate disease detection is crucial for patient management and successful treatment outcomes. However, the automatic identification of anomalies in medical images can be challenging. Conventional methods rely on large labeled datasets which are difficult to obtain. To overcome these limitations, we introduce a novel unsupervised approach, called PHANES (Pseudo Healthy generative networks for ANomaly Segmentation). Our method has the capability of reversing anomalies, i.e., preserving healthy tissue and replacing anomalous regions with pseudo-healthy (PH) reconstructions. Unlike recent diffusion models, our method does not rely on a learned noise distribution nor does it introduce random alterations to the entire image. Instead, we use latent generative networks to create masks around possible anomalies, which are refined using inpainting generative networks. We demonstrate the effectiveness of PHANES in detecting stroke lesions in T1w brain MRI datasets and show significant
    
[^45]: 通过对电子病历的乳腺癌表型NLP算法进行跨机构评估

    A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records. (arXiv:2303.08448v1 [cs.CL])

    [http://arxiv.org/abs/2303.08448](http://arxiv.org/abs/2303.08448)

    本研究通过对乳腺癌表型提取任务的评估，展示了基于BERT的临床NLP模型在不同临床环境中具有良好的泛化能力，并强调了使用转移学习开发广义临床NLP模型的潜力。

    

    目标：在模型开发过程中，通常忽略临床大型语言模型的泛化能力。本研究通过乳腺癌表型提取任务，评估了基于BERT的临床NLP模型在不同临床环境下的泛化能力。方法：从明尼苏达大学和梅奥诊所的电子病历中收集了两种乳腺癌患者的临床语料库，并按照同一指南进行注释。我们开发了三种类型的NLP模型（条件随机场、双向长短期记忆和CancerBERT），从临床文本中提取癌症表型。使用不同的学习策略（模型转移与本地训练）对模型在不同测试集上进行泛化能力评估。评估实体覆盖率与模型性能的相关性得分。结果：在UMN和MC手动注释了200和161份临床文档。CancerBERT模型达到了最高的F1分数（0.896）和实体覆盖率（98.8%），优于其他模型。模型转移方法在两个机构中产生了类似于本地训练模型的结果，表明跨机构存在潜在的泛化性。结论：本研究展示了在不同临床环境中评估NLP模型的重要性，并强调了使用转移学习开发广义临床NLP模型的潜力。

    Objective: The generalizability of clinical large language models is usually ignored during the model development process. This study evaluated the generalizability of BERT-based clinical NLP models across different clinical settings through a breast cancer phenotype extraction task.  Materials and Methods: Two clinical corpora of breast cancer patients were collected from the electronic health records from the University of Minnesota and the Mayo Clinic, and annotated following the same guideline. We developed three types of NLP models (i.e., conditional random field, bi-directional long short-term memory and CancerBERT) to extract cancer phenotypes from clinical texts. The models were evaluated for their generalizability on different test sets with different learning strategies (model transfer vs. locally trained). The entity coverage score was assessed with their association with the model performances.  Results: We manually annotated 200 and 161 clinical documents at UMN and MC, re
    
[^46]: MAHTM：分层可交易微电网的多智能体框架

    MAHTM: A Multi-Agent Framework for Hierarchical Transactive Microgrids. (arXiv:2303.08447v1 [cs.LG])

    [http://arxiv.org/abs/2303.08447](http://arxiv.org/abs/2303.08447)

    该论文提出了一个多智能体强化学习框架，用于管理微电网中的能源交易。该框架通过最小化碳足迹，同时平衡可再生能源和传统能源的消费和生产，并考虑能源变化。

    

    将可变的可再生能源并入电网给系统运营商带来挑战，使得在能源可用性、成本承受能力和污染可控性之间取得最优折衷成为一项难题。本文提出了一种管理微电网能源交易的多智能体强化学习框架。该框架解决了上述挑战：旨在通过最小化碳足迹的方式优化可用资源的利用，并使所有利益相关方受益。所提出的架构由三层代理组成，每层代理追求不同的目标。第一层由生产者和消费者组成，旨在最小化总能源成本。其他两层控制能源价格，以减少碳足迹，同时平衡可再生能源和传统能源的消费和生产。该框架还考虑了能源需求和供应的波动。

    Integrating variable renewable energy into the grid has posed challenges to system operators in achieving optimal trade-offs among energy availability, cost affordability, and pollution controllability. This paper proposes a multi-agent reinforcement learning framework for managing energy transactions in microgrids. The framework addresses the challenges above: it seeks to optimize the usage of available resources by minimizing the carbon footprint while benefiting all stakeholders. The proposed architecture consists of three layers of agents, each pursuing different objectives. The first layer, comprised of prosumers and consumers, minimizes the total energy cost. The other two layers control the energy price to decrease the carbon impact while balancing the consumption and production of both renewable and conventional energy. This framework also takes into account fluctuations in energy demand and supply.
    
[^47]: 利用预训练的垂直二维扩散模型改进三维成像

    Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models. (arXiv:2303.08440v1 [eess.IV])

    [http://arxiv.org/abs/2303.08440](http://arxiv.org/abs/2303.08440)

    本论文提出了一种使用两个预训练的垂直二维扩散模型来解决三维逆问题的方法，将三维数据分布建模为在不同方向上切片的二维分布的乘积，解决了维数的灾难性问题，并以高效的方式用于医学图像重建任务。

    

    由于其众多的优点，扩散模型已成为图像生成和重构的流行方法。 然而，大多数基于扩散的逆问题求解方法仅处理二维图像，即使是最近发布的三维方法也没有充分利用三维先验分布。 为了解决这个问题，我们提出了一种新方法，使用两个相互垂直的预训练二维扩散模型来解决三维逆问题。 通过将三维数据分布建模为在不同方向上切片的二维分布的乘积，我们的方法有效地解决了维数的灾难性问题。 我们的实验结果证明，我们的方法对于三维医学图像重建任务非常有效，包括MRI Z轴超分辨率，压缩感知MRI和稀疏视图CT。 我们的方法可以生成适用于医学应用的高质量体素体积。

    Diffusion models have become a popular approach for image generation and reconstruction due to their numerous advantages. However, most diffusion-based inverse problem-solving methods only deal with 2D images, and even recently published 3D methods do not fully exploit the 3D distribution prior. To address this, we propose a novel approach using two perpendicular pre-trained 2D diffusion models to solve the 3D inverse problem. By modeling the 3D data distribution as a product of 2D distributions sliced in different directions, our method effectively addresses the curse of dimensionality. Our experimental results demonstrate that our method is highly effective for 3D medical image reconstruction tasks, including MRI Z-axis super-resolution, compressed sensing MRI, and sparse-view CT. Our method can generate high-quality voxel volumes suitable for medical applications.
    
[^48]: 使用复值神经场的物理信息光学核回归

    Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields. (arXiv:2303.08435v1 [cs.CV])

    [http://arxiv.org/abs/2303.08435](http://arxiv.org/abs/2303.08435)

    本文提出了一种新的基于机器学习的光刻模型范式，通过优化复值神经场执行光学核回归并将光刻系统拆解为非参数掩模操作和包含行列式源、瞳孔和光刻信息的学习光学核，使用小规模训练数据集展示了卓越的推广能力。

    

    光刻是集成电路制造的基础，需要大量计算。基于机器学习的光刻模型的发展缓解了制造过程开销和能力之间的平衡。然而，所有以前的方法都将光刻系统视为图像到图像的黑盒映射，利用网络参数通过死记硬背映射大量的掩模到空中或掩模到电阻图像对，导致推广能力不佳。本文提出了一种新的基于机器学习的范式，将严格的光刻模型拆解为非参数掩模操作和包含行列式源、瞳孔和光刻信息的学习光学核。通过优化复值神经场以执行光学核回归，我们的方法可以准确地恢复光刻系统，同时使用较少的参数进行小规模训练数据集，展示了卓越的推广能力。

    Lithography is fundamental to integrated circuit fabrication, necessitating large computation overhead. The advancement of machine learning (ML)-based lithography models alleviates the trade-offs between manufacturing process expense and capability. However, all previous methods regard the lithography system as an image-to-image black box mapping, utilizing network parameters to learn by rote mappings from massive mask-to-aerial or mask-to-resist image pairs, resulting in poor generalization capability. In this paper, we propose a new ML-based paradigm disassembling the rigorous lithographic model into non-parametric mask operations and learned optical kernels containing determinant source, pupil, and lithography information. By optimizing complex-valued neural fields to perform optical kernel regression from coordinates, our method can accurately restore lithography system using a small-scale training dataset with fewer parameters, demonstrating superior generalization capability as w
    
[^49]: DeDA: 深度有向累加器

    DeDA: Deep Directed Accumulator. (arXiv:2303.08434v1 [eess.IV])

    [http://arxiv.org/abs/2303.08434](http://arxiv.org/abs/2303.08434)

    本文提出了一种针对特定类型病变的深度有向累加器 (DeDA) 对神经网络注入领域特定的归纳偏差实现对该类型病变的精确识别。

    

    慢性活动性的多发性硬化症状 (rim+ 损伤) 可以通过定量磁感应造影 (quantitative susceptibility maps) 来表征，其边缘处呈现高信号强度边缘 (hyperintense rim)。这些 rim+ 损伤具有几何简单的结构，其中损伤边缘处的梯度是从中心向边缘辐射方向分布的，且梯度的幅度比 rim- (非 rim+) 损伤更大。然而，最近的研究表明由于数据有限和高度类别不平衡，这类损伤的识别表现仍不尽如人意。本文提出了一种简单但有效的图像处理操作，称为 deep directed accumulator (DeDA)，它为 rim+ 损伤的识别注入了领域特定的归纳偏差 (priors)。给定一个特征映射和一组采样网格，DeDA 将创建并量化一个累加器空间到有限的间隔，并相应地累积特征值。

    Chronic active multiple sclerosis lesions, also termed as rim+ lesions, can be characterized by a hyperintense rim at the edge of the lesion on quantitative susceptibility maps. These rim+ lesions exhibit a geometrically simple structure, where gradients at the lesion edge are radially oriented and a greater magnitude of gradients is observed in contrast to rim- (non rim+) lesions. However, recent studies have shown that the identification performance of such lesions remains unsatisfied due to the limited amount of data and high class imbalance. In this paper, we propose a simple yet effective image processing operation, deep directed accumulator (DeDA), that provides a new perspective for injecting domain-specific inductive biases (priors) into neural networks for rim+ lesion identification. Given a feature map and a set of sampling grids, DeDA creates and quantizes an accumulator space into finite intervals, and accumulates feature values accordingly. This DeDA operation is a general
    
[^50]: 混合数据增强方法Mixup对于特征学习的益处

    The Benefits of Mixup for Feature Learning. (arXiv:2303.08433v1 [cs.LG])

    [http://arxiv.org/abs/2303.08433](http://arxiv.org/abs/2303.08433)

    本论文介绍了数据增强方法Mixup对于特征学习的益处。混合训练可以有效地从混合数据中学习罕见特征，相比之下，标准训练可能会漏掉这些罕见特征。

    

    Mixup是一种简单的数据增强方法，通过线性插值随机混合两个数据点，已广泛应用于各种深度学习应用中，以获得更好的泛化效果。然而，其有效性的理论基础尚未完全被理解。本文旨在寻求对Mixup益处的基本理解。首先，我们展示Mixup在特征和标签使用不同的线性插值参数时仍可实现类似于标准Mixup的性能。这表明，Zhang等人（2018）提出的直观线性解释可能并不能完全解释Mixup的成功。然后，我们从特征学习的角度对Mixup进行理论研究。我们考虑一个特征噪声数据模型，并展示Mixup训练可以有效地从其与常见特征（出现在大部分数据中）混合中学习罕见特征（出现在少部分数据中）。相比之下，标准训练可能会漏掉这些罕见特征。

    Mixup, a simple data augmentation method that randomly mixes two data points via linear interpolation, has been extensively applied in various deep learning applications to gain better generalization. However, the theoretical underpinnings of its efficacy are not yet fully understood. In this paper, we aim to seek a fundamental understanding of the benefits of Mixup. We first show that Mixup using different linear interpolation parameters for features and labels can still achieve similar performance to the standard Mixup. This indicates that the intuitive linearity explanation in Zhang et al., (2018) may not fully explain the success of Mixup. Then we perform a theoretical study of Mixup from the feature learning perspective. We consider a feature-noise data model and show that Mixup training can effectively learn the rare features (appearing in a small fraction of data) from its mixture with the common features (appearing in a large fraction of data). In contrast, standard training ca
    
[^51]: 政策梯度算法收敛于几乎线性二次型调节器的全局最优策略

    Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators. (arXiv:2303.08431v1 [cs.LG])

    [http://arxiv.org/abs/2303.08431](http://arxiv.org/abs/2303.08431)

    本论文研究了强化学习方法在几乎线性二次型调节器系统中找到最优策略的问题，提出了一个策略梯度算法，可以以线性速率收敛于全局最优解。

    

    决策者只获得了非完整信息的非线性控制系统在各种应用中普遍存在。本研究探索了强化学习方法，以找到几乎线性二次型调节器系统中最优策略。我们考虑一个动态系统，结合线性和非线性组成部分，并由相同结构的策略进行管理。在假设非线性组成部分包含具有小型Lipschitz系数的内核的情况下，我们对成本函数的优化进行了表征。虽然成本函数通常是非凸的，但我们确立了全局最优解附近局部的强凸性和光滑性。此外，我们提出了一种初始化机制，以利用这些属性。在此基础上，我们设计了一个策略梯度算法，可以保证以线性速率收敛于全局最优解。

    Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.
    
[^52]: 双重公平性：通过对比自监督实现群体和个体级别的公平表示学习

    DualFair: Fair Representation Learning at Both Group and Individual Levels via Contrastive Self-supervision. (arXiv:2303.08403v1 [cs.LG])

    [http://arxiv.org/abs/2303.08403](http://arxiv.org/abs/2303.08403)

    本文提出了一种自我监督模型DualFair，该模型使用对比损失来生成对每个受保护群体不可区分的嵌入，并联合优化了群体公平性和反事实公平性两个公平性标准，使得在群体和个体级别上做出更公正的预测，该模型在公平智能领域中具有潜在价值。

    

    算法公平性已成为重要的机器学习问题，特别是对于关键任务的Web应用。本文提出了一种自我监督模型DualFair，该模型可以使学习表示不带有性别和种族等敏感属性。与现有的针对单一公平性类型的模型不同，我们的模型联合优化了两个公平性标准，即群体公平性和反事实公平性，从而在群体和个体级别上做出更公正的预测。我们的模型使用对比损失来生成对每个受保护群体不可区分的嵌入，同时迫使反事实对的嵌入相似。然后，我们使用自我知识蒸馏方法来维护下游任务的表示质量。多个数据集的广泛分析证实了该模型的有效性，并进一步展示了联合处理两个公平性标准的协同效应，表明该模型在公平智能领域中具有潜在的价值。

    Algorithmic fairness has become an important machine learning problem, especially for mission-critical Web applications. This work presents a self-supervised model, called DualFair, that can debias sensitive attributes like gender and race from learned representations. Unlike existing models that target a single type of fairness, our model jointly optimizes for two fairness criteria - group fairness and counterfactual fairness - and hence makes fairer predictions at both the group and individual levels. Our model uses contrastive loss to generate embeddings that are indistinguishable for each protected group, while forcing the embeddings of counterfactual pairs to be similar. It then uses a self-knowledge distillation method to maintain the quality of representation for the downstream tasks. Extensive analysis over multiple datasets confirms the model's validity and further shows the synergy of jointly addressing two fairness criteria, suggesting the model's potential value in fair int
    
[^53]: 使用扩散模型生成符号音乐

    Generating symbolic music using diffusion models. (arXiv:2303.08385v1 [cs.SD])

    [http://arxiv.org/abs/2303.08385](http://arxiv.org/abs/2303.08385)

    本文提出了一种使用扩散模型生成钢琴卷帘的方法，可以协调、生成、完善音乐；代码已公开共享。

    

    概率去噪扩散模型已成为简单而强大的生成模型。与其他生成模型不同，扩散模型不会出现模式崩溃，也不需要辨别器来生成高质量样本。本文提出了一种使用二项先验分布来生成钢琴卷帘的扩散模型。本文还提出了一种高效的方法来训练模型和生成样本。生成的音乐具有时间尺度上的一致性，可以达到训练钢琴卷帘段的长度。我们展示了这样一个模型是如何在输入的条件下进行工作，并可用于协调给定的旋律，完成不完整的钢琴卷帘或生成给定乐曲的变化。代码是公开共享的，以鼓励社区使用和开发该方法。

    Probabilistic Denoising Diffusion models have emerged as simple yet very powerful generative models. Diffusion models unlike other generative models do not suffer from mode collapse nor require a discriminator to generate high quality samples. In this paper, we propose a diffusion model that uses a binomial prior distribution to generate piano-rolls. The paper also proposes an efficient method to train the model and generate samples. The generated music has coherence at time scales up to the length of the training piano-roll segments. We show how such a model is conditioned on the input and can be used to harmonize a given melody, complete an incomplete piano-roll or generate a variation of a given piece. The code is shared publicly to encourage the use and development of the method by the community.
    
[^54]: MCR-DL: 用于深度学习的混合通信运行时

    MCR-DL: Mix-and-Match Communication Runtime for Deep Learning. (arXiv:2303.08374v1 [cs.DC])

    [http://arxiv.org/abs/2303.08374](http://arxiv.org/abs/2303.08374)

    该论文提出了MCR-DL，一种用于深度学习的可扩展混合通信框架，能够支持各种集合和点对点操作，并允许用户动态地混合和匹配通信后端以进行给定操作，从而提高深度学习模型的训练效率。

    

    最近几年，许多最先进的深度学习模型的训练需求超出了单个处理器的计算和内存能力，并需要在多个处理器之间分配。训练这样的大型模型需要先进的并行策略以保持效率。然而，这种分布式的深度学习并行策略需要在各种消息大小和规模下进行各种集合和点对点通信操作的混合。使用高级并行策略的模型包括：深度学习推荐模型（DLRM）和专家混合（MoE）。通信库在不同的通信操作、规模和消息大小下的性能差异巨大。我们提出了 MCR-DL：一种可扩展的深度学习通信框架，支持所有点对点和集体操作，同时使用户能够动态地混合和匹配通信后端以进行给定操作而不会出现死锁。

    In recent years, the training requirements of many state-of-the-art Deep Learning (DL) models have scaled beyond the compute and memory capabilities of a single processor, and necessitated distribution among processors. Training such massive models necessitates advanced parallelism strategies to maintain efficiency. However, such distributed DL parallelism strategies require a varied mixture of collective and point-to-point communication operations across a broad range of message sizes and scales. Examples of models using advanced parallelism strategies include Deep Learning Recommendation Models (DLRM) and Mixture-of-Experts (MoE). Communication libraries' performance varies wildly across different communication operations, scales, and message sizes. We propose MCR-DL: an extensible DL communication framework that supports all point-to-point and collective operations while enabling users to dynamically mix-and-match communication backends for a given operation without deadlocks. MCR-D
    
[^55]: 基于迁移学习的肺音异常检测与分析

    Transfer Learning Based Diagnosis and Analysis of Lung Sound Aberrations. (arXiv:2303.08362v1 [cs.SD])

    [http://arxiv.org/abs/2303.08362](http://arxiv.org/abs/2303.08362)

    本研究通过迁移学习技术，开发了一种基于CNN的方法，使用Mel频率倒谱系数（MFCCs）进行分类，实现对肺部呼吸音的非侵入性检测，准确率达到95%。

    

    随着能够收集和分析大量数据的计算机系统的发展，医学界正在建立几种非侵入性工具。本研究旨在通过机器学习技术开发一种非侵入性技术，用于识别由听诊器和语音录制软件获取的呼吸音。该研究提出了一种经过训练和验证的基于 CNN 的方法来对呼吸音进行分类。通过构建每个音频样本的视觉表示，可以使用类似于有效描述视觉的方法来识别资源以进行分类。我们使用了一种称为 Mel 频率倒谱系数 (MFCCs) 的技术。这里，通过 VGG16 (迁移学习) 检索和分类特征，并使用 5 折交叉验证进行预测。采用各种数据分割技术，呼吸音数据库获得了尖端结果，包括 95% 的准确率、88% 的精确度、86% 的召回率和 87% 的 F1 值。

    With the development of computer -systems that can collect and analyze enormous volumes of data, the medical profession is establishing several non-invasive tools. This work attempts to develop a non-invasive technique for identifying respiratory sounds acquired by a stethoscope and voice recording software via machine learning techniques. This study suggests a trained and proven CNN-based approach for categorizing respiratory sounds. A visual representation of each audio sample is constructed, allowing resource identification for classification using methods like those used to effectively describe visuals. We used a technique called Mel Frequency Cepstral Coefficients (MFCCs). Here, features are retrieved and categorized via VGG16 (transfer learning) and prediction is accomplished using 5-fold cross-validation. Employing various data splitting techniques, Respiratory Sound Database obtained cutting-edge results, including accuracy of 95%, precision of 88%, recall score of 86%, and F1 
    
[^56]: 面向异构边缘/雾计算网络的合作式联邦学习

    Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks. (arXiv:2303.08361v1 [cs.DC])

    [http://arxiv.org/abs/2303.08361](http://arxiv.org/abs/2303.08361)

    提出了一种合作式联邦学习(CFL)范式，通过设备间的合作来抵消边缘/雾网络中的异构性，从而提高机器学习模型训练质量和网络资源利用率。

    

    联邦学习是一种在边缘/雾计算网络上训练机器学习模型的流行技术。然而，传统的联邦学习实现忽略了网络之间的潜在合作，将参与机器学习的边缘/雾设备和其他基础设施视为独立的处理单元。因此，联邦学习容易受到网络异构性的影响，如计算能力、通信资源、数据质量和隐私需求的差异。我们提出了一种合作式联邦学习(CFL)范式，建立在设备对设备(D2D)和设备对服务器(D2S)交互之上。通过D2D和D2S合作，CFL通过启用模型/数据/资源汇集机制来抵消边缘/雾网络中的异构性，从而显著提高机器学习模型训练质量和网络资源利用率。我们提出了一组核心方法，构成了CFL的基础。

    Federated learning (FL) has been promoted as a popular technique for training machine learning (ML) models over edge/fog networks. Traditional implementations of FL have largely neglected the potential for inter-network cooperation, treating edge/fog devices and other infrastructure participating in ML as separate processing elements. Consequently, FL has been vulnerable to several dimensions of network heterogeneity, such as varying computation capabilities, communication resources, data qualities, and privacy demands. We advocate for cooperative federated learning (CFL), a cooperative edge/fog ML paradigm built on device-to-device (D2D) and device-to-server (D2S) interactions. Through D2D and D2S cooperation, CFL counteracts network heterogeneity in edge/fog networks through enabling a model/data/resource pooling mechanism, which will yield substantial improvements in ML model training quality and network resource consumption. We propose a set of core methodologies that form the foun
    
[^57]: 金融应用的高效安全联合学习

    Efficient and Secure Federated Learning for Financial Applications. (arXiv:2303.08355v1 [cs.LG])

    [http://arxiv.org/abs/2303.08355](http://arxiv.org/abs/2303.08355)

    本文提出了一种金融应用的高效安全联合学习方法，其中包括Top-K梯度稀疏化和基于Delta的稀疏化，以及一种可保护数据隐私的聚合框架，这些方法可以大幅降低通信成本并保证预测精度。

    

    传统的机器学习和深度学习方法需要与外部征信局共享客户的敏感信息来生成预测模型，从而导致隐私泄露的风险。联合学习是一种可以保护数据隐私的机器学习设置，但高通信成本经常成为联合系统的瓶颈，特别是对于大型神经网络而言。本文提出了两种稀疏化方法来降低联合学习中的通信成本，即Top-K梯度稀疏化和基于Delta的稀疏化。此外，本文提出了一种隐私保护和安全的聚合框架，用于聚合梯度更新，该框架可以在进行聚合时保护每个参与者的数据隐私。实验结果表明，这两种稀疏化方法可以在保持期望的收敛率和预测精度的同时，减少多达90%的通信开销。

    The conventional machine learning (ML) and deep learning approaches need to share customers' sensitive information with an external credit bureau to generate a prediction model that opens the door to privacy leakage. This leakage risk makes financial companies face an enormous challenge in their cooperation. Federated learning is a machine learning setting that can protect data privacy, but the high communication cost is often the bottleneck of the federated systems, especially for large neural networks. Limiting the number and size of communications is necessary for the practical training of large neural structures. Gradient sparsification has received increasing attention as a method to reduce communication cost, which only updates significant gradients and accumulates insignificant gradients locally. However, the secure aggregation framework cannot directly use gradient sparsification. This article proposes two sparsification methods to reduce communication cost in federated learnin
    
[^58]: 基于低秩张量分享的Tiny Ambient Speech Recognition模型

    Sharing Low Rank Conformer Weights for Tiny Always-On Ambient Speech Recognition Models. (arXiv:2303.08343v1 [eess.AS])

    [http://arxiv.org/abs/2303.08343](http://arxiv.org/abs/2303.08343)

    本论文提出了一种基于低秩张量分享的模型方法，将大型语音识别模型缩小至5M参数，同时实现了在低内存神经处理器边缘设备上的始终处于运行状态的语音识别。

    

    机器学习技术的持续改进为使用更大的模型和更大的训练数据集提供了令人兴奋的新机会。但是，在仅有低内存的智能手机、可穿戴设备和其他嵌入式环境等低功耗设备上提供这些新功能的需求与日俱增。为此，我们考虑了一些方法来减小基于Conformer的语音识别模型的模型大小，这些模型通常需要大于100M个参数的模型，将其缩小到仅$5$M个参数，同时最小化对模型质量的影响。这样的模型使我们能够在具有低内存神经处理器的边缘设备上实现始终处于运行状态的语音识别。我们提出在模型架构中的不同层次上重复使用模型权重: (i) 重复整个Conformer块层，(ii) 在层之间共享特定的Conformer模块，(iii) 在Conformer模块中共享子部件，(iv) 在低秩张量分解后共享分解的子部件权重。

    Continued improvements in machine learning techniques offer exciting new opportunities through the use of larger models and larger training datasets. However, there is a growing need to offer these new capabilities on-board low-powered devices such as smartphones, wearables and other embedded environments where only low memory is available. Towards this, we consider methods to reduce the model size of Conformer-based speech recognition models which typically require models with greater than 100M parameters down to just $5$M parameters while minimizing impact on model quality. Such a model allows us to achieve always-on ambient speech recognition on edge devices with low-memory neural processors. We propose model weight reuse at different levels within our model architecture: (i) repeating full conformer block layers, (ii) sharing specific conformer modules across layers, (iii) sharing sub-components per conformer module, and (iv) sharing decomposed sub-component weights after low-rank 
    
[^59]: 通过空间时间数据过拟合实现高质量高效的视频超分辨率

    Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting. (arXiv:2303.08331v1 [cs.CV])

    [http://arxiv.org/abs/2303.08331](http://arxiv.org/abs/2303.08331)

    本论文提出了一种利用空间时间信息来提高视频超分辨率的新方法，采用高维卷积网络进行预测并应用时间注意机制以去除冗余信息并提高效率。

    

    随着深度卷积神经网络(DNN)在计算机视觉的各个领域得到广泛应用，利用DNN的过拟合能力实现视频分辨率的提升已经成为现代视频传输系统的新趋势。将视频分为块并将每个块与超分辨率模型过拟合，从而在传输给客户端之前对视频进行编码，从而实现更好的视频质量和传输效率。然而，为了保证良好的过拟合质量，需要大量的块，这会大大增加存储量和消耗更多带宽资源进行数据传输。另一方面，通过训练优化技术减少块的数量通常需要高模型容量，这会显著降低执行速度。为了解决这个问题，我们提出了一种新的方法来完成高质量和高效的视频分辨率升级任务，利用空间时间信息来准确捕捉有限训练数据的视频块的预测结果。具体来说，我们提出利用视频块的空间时间相关性，使用高维卷积网络改进每个块的预测，并进一步应用时间注意机制以去除冗余信息并促进传输。实验结果表明，我们的方法在视觉质量和效率方面均优于现有方法。

    As deep convolutional neural networks (DNNs) are widely used in various fields of computer vision, leveraging the overfitting ability of the DNN to achieve video resolution upscaling has become a new trend in the modern video delivery system. By dividing videos into chunks and overfitting each chunk with a super-resolution model, the server encodes videos before transmitting them to the clients, thus achieving better video quality and transmission efficiency. However, a large number of chunks are expected to ensure good overfitting quality, which substantially increases the storage and consumes more bandwidth resources for data transmission. On the other hand, decreasing the number of chunks through training optimization techniques usually requires high model capacity, which significantly slows down execution speed. To reconcile such, we propose a novel method for high-quality and efficient video resolution upscaling tasks, which leverages the spatial-temporal information to accurately
    
[^60]: FairAdaBN：自适应批归一化减少不公平性及其在皮肤病分类中的应用

    FairAdaBN: Mitigating unfairness with adaptive batch normalization and its application to dermatological disease classification. (arXiv:2303.08325v1 [cs.LG])

    [http://arxiv.org/abs/2303.08325](http://arxiv.org/abs/2303.08325)

    本文提出了FairAdaBN，将批归一化适应敏感属性，可以将其简单而有效地应用到原本不了解公平性的多个分类主干中，能够有效地减少不公平性，并实现模型性能和公平性之间的平衡。

    

    深度学习正在医学研究和应用中变得越来越普遍，同时涉及敏感信息，甚至包括关键的诊断决策。研究人员观察到不同人口属性子组之间的显著性能差异，称为模型不公平性，并致力于精心设计优雅的体系结构以解决不公平性问题，这带来了沉重的训练负担、较差的泛化能力，并揭示了模型性能和公平性之间的权衡。为了解决这些问题，我们提出了FairAdaBN，通过使批归一化适应敏感属性，可以将其简单而有效地应用到原本不了解公平性的多个分类主干中。另外，我们提出了一种新的损失函数，通过限制小批量子组之间的统计平衡，鼓励模型以相当公平的方式收敛。为了评估模型性能和公平性之间的权衡，我们在HAM10000数据集上进行了实验，该数据集是一个大规模的开放获取皮肤病数据库，用于分类七种常见的皮肤病病变。我们的结果表明，FairAdaBN可以有效地减少不公平性，并实现模型性能和公平性之间的平衡，开销可忽略不计。

    Deep learning is becoming increasingly ubiquitous in medical research and applications while involving sensitive information and even critical diagnosis decisions. Researchers observe a significant performance disparity among subgroups with different demographic attributes, which is called model unfairness, and put lots of effort into carefully designing elegant architectures to address unfairness, which poses heavy training burden, brings poor generalization, and reveals the trade-off between model performance and fairness. To tackle these issues, we propose FairAdaBN by making batch normalization adaptive to sensitive attribute. This simple but effective design can be adopted to several classification backbones that are originally unaware of fairness. Additionally, we derive a novel loss function that restrains statistical parity between subgroups on mini-batches, encouraging the model to converge with considerable fairness. In order to evaluate the trade-off between model performanc
    
[^61]: 异构6G网络中联邦学习的优化设计

    Optimization Design for Federated Learning in Heterogeneous 6G Networks. (arXiv:2303.08322v1 [cs.LG])

    [http://arxiv.org/abs/2303.08322](http://arxiv.org/abs/2303.08322)

    联邦学习是实现6G网络人工智能普及的关键技术，但在6G网络中，存在一些系统和统计异构性挑战。本研究研究了能够有效解决这些异构性问题的优化方法。

    

    随着5G网络的快速发展，亿万智能物联网设备和庞大的数据在网络边缘生成。尽管仍处于早期阶段，但预计正在发展中的6G网络将采用先进的人工智能技术来收集、传输和学习这些宝贵的数据，以实现创新应用和智能服务。然而，传统的机器学习方法需要将训练数据集中到数据中心或云中，引发了严重的用户隐私问题。作为一种新兴的分布式人工智能范例，具有隐私保护性质的联邦学习被视为实现6G网络普遍应用人工智能的关键技术。然而，在6G网络中有效、高效地实现联邦学习仍存在一些系统和统计异构性挑战。本文探讨了能够有效解决这些异构性问题的优化方法。

    With the rapid advancement of 5G networks, billions of smart Internet of Things (IoT) devices along with an enormous amount of data are generated at the network edge. While still at an early age, it is expected that the evolving 6G network will adopt advanced artificial intelligence (AI) technologies to collect, transmit, and learn this valuable data for innovative applications and intelligent services. However, traditional machine learning (ML) approaches require centralizing the training data in the data center or cloud, raising serious user-privacy concerns. Federated learning, as an emerging distributed AI paradigm with privacy-preserving nature, is anticipated to be a key enabler for achieving ubiquitous AI in 6G networks. However, there are several system and statistical heterogeneity challenges for effective and efficient FL implementation in 6G networks. In this article, we investigate the optimization approaches that can effectively address the challenging heterogeneity issues
    
[^62]: SegPrompt: 使用分割图作为更好的提示来微调深度模型用于肾结石分类

    SegPrompt: Using Segmentation Map as a Better Prompt to Finetune Deep Models for Kidney Stone Classification. (arXiv:2303.08303v1 [cs.CV])

    [http://arxiv.org/abs/2303.08303](http://arxiv.org/abs/2303.08303)

    本文提出了SegPrompt，可以使用分割图加强分类模型的训练，同时使用分割图来微调预训练的深度模型，从而大大减少可训练参数，显著提高了分类性能，同时需要比以前的方法少得多的注释数据。

    

    最近，深度学习已经在使用内窥镜图像进行肾结石分类方面取得了令人鼓舞的结果。然而，缺乏注释的训练数据对于改善训练模型的性能和泛化能力构成了严重问题。因此，充分利用手头有限的数据是至关重要的。本文提出了SegPrompt，通过从两个方面利用分割图来缓解数据短缺问题。首先，SegPrompt整合了分割图以便于分类训练，使分类模型能够意识到感兴趣的区域。所提出的方法允许图像和分割令牌相互交互，以充分利用分割图信息。其次，我们使用分割图作为提示来微调预训练的深度模型，结果可训练参数比香草微调少得多。我们在收集的肾结石数据集上进行了广泛的实验。结果表明，SegPrompt可以显著提高分类性能，同时需要比以前的方法少得多的注释数据。

    Recently, deep learning has produced encouraging results for kidney stone classification using endoscope images. However, the shortage of annotated training data poses a severe problem in improving the performance and generalization ability of the trained model. It is thus crucial to fully exploit the limited data at hand. In this paper, we propose SegPrompt to alleviate the data shortage problems by exploiting segmentation maps from two aspects. First, SegPrompt integrates segmentation maps to facilitate classification training so that the classification model is aware of the regions of interest. The proposed method allows the image and segmentation tokens to interact with each other to fully utilize the segmentation map information. Second, we use the segmentation maps as prompts to tune the pretrained deep model, resulting in much fewer trainable parameters than vanilla finetuning. We perform extensive experiments on the collected kidney stone dataset. The results show that SegPromp
    
[^63]: 基于后训练量化的大型语言模型综合研究

    A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])

    [http://arxiv.org/abs/2303.08302](http://arxiv.org/abs/2303.08302)

    本文基于数万个零-shot实验对基于后训练量化的大型语言模型的不同量化组件进行了综合研究，结果发现细粒度量化和后训练量化方法很重要，用粗粒度量化的更高位数比用非常细粒度的更低位数更强大。我们给出了如何为不同大小的\llms利用量化的建议。

    

    后训练量化是一种减少大型语言模型内存消耗和/或计算成本的权衡方法。然而，关于不同量化方案、不同模型族、不同后训练量化方法、不同量化位精度等的影响的全面研究仍缺失。本文通过数万个零-shot实验对这些组件进行了广泛的研究。我们的研究结果表明：(1)细粒度量化和后训练量化方法(而不是朴素的最近舍入量化)是实现良好精度的必要条件；(2) 用粗粒度量化的更高位数（如5位）比用非常细粒度的更低位数（如4位）（其有效位数与5位相似）更强大。我们还提出了如何为不同大小的\llms利用量化的建议，并留下未来机会和系统工作的建议。

    Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
    
[^64]: 学习高维网络物理数据流用于智能电网的故障诊断

    Learning From High-Dimensional Cyber-Physical Data Streams for Diagnosing Faults in Smart Grids. (arXiv:2303.08300v1 [cs.LG])

    [http://arxiv.org/abs/2303.08300](http://arxiv.org/abs/2303.08300)

    本文采用特征工程相结合的方法解决了网络物理电力系统数据质量、计算成本和冗余测量数据的问题，提高了故障诊断系统的性能。

    

    故障诊断系统的性能受网络物理电力系统数据质量的影响。这些系统产生大量数据，使系统承受过多的计算成本。另一个问题是记录测量中存在噪声，这可以防止构建精确的决策模型。此外，诊断模型通常配备了一组冗余测量数据，可能偏离正常和故障分布的学习。本文介绍了特征工程对于缓解网络物理系统中上述挑战的影响。特征选择和降维方法与决策模型相结合，模拟了对118个总线电力系统中的数据驱动故障诊断。因此，开展了比较研究，比较了两个领域中的几种先进技术。同时比较了降维和特征选择方法。最后，进行了实验验证。

    The performance of fault diagnosis systems is highly affected by data quality in cyber-physical power systems. These systems generate massive amounts of data that overburden the system with excessive computational costs. Another issue is the presence of noise in recorded measurements, which prevents building a precise decision model. Furthermore, the diagnostic model is often provided with a mixture of redundant measurements that may deviate it from learning normal and fault distributions. This paper presents the effect of feature engineering on mitigating the aforementioned challenges in cyber-physical systems. Feature selection and dimensionality reduction methods are combined with decision models to simulate data-driven fault diagnosis in a 118-bus power system. A comparative study is enabled accordingly to compare several advanced techniques in both domains. Dimensionality reduction and feature selection methods are compared both jointly and separately. Finally, experiments are con
    
[^65]: 在回收材料可持续化的敏捷制造中应用机器学习方法(arXiv:2303.08291v1 [cs.AI])

    Machine Learning Approaches in Agile Manufacturing with Recycled Materials for Sustainability. (arXiv:2303.08291v1 [cs.AI])

    [http://arxiv.org/abs/2303.08291](http://arxiv.org/abs/2303.08291)

    本文提出将回收再利用材料用于敏捷制造，应用机器学习模型进行预测分析，以决策支持实现环境可持续性。

    

    在材料科学和制造业中开发可持续的、环保的过程非常重要。人工智能在决策支持方面可以起到重要的作用，这一点已经在我们之前的研究中得以证明，通过我们提出的基于机器学习的方法开发的工具，可以用于计算估计和专家系统。本研究通过决策支持实现在材料科学中的环境可持续性，应用回收和再生材料进行敏捷制造，这是一种安全、负责任的将特定废弃物转变成增值产品的方式。我们建议使用数据驱动的AI方法，通过应用机器学习模型进行预测分析，以指导制造中的决策支持，包括利用人工神经网络研究影响材料热处理及其性能的参数，以及通过先进技术（如卷积神经网络）进行深度学习，来探索粒度大小对性能的影响。

    It is important to develop sustainable processes in materials science and manufacturing that are environmentally friendly. AI can play a significant role in decision support here as evident from our earlier research leading to tools developed using our proposed machine learning based approaches. Such tools served the purpose of computational estimation and expert systems. This research addresses environmental sustainability in materials science via decision support in agile manufacturing using recycled and reclaimed materials. It is a safe and responsible way to turn a specific waste stream to value-added products. We propose to use data-driven methods in AI by applying machine learning models for predictive analysis to guide decision support in manufacturing. This includes harnessing artificial neural networks to study parameters affecting heat treatment of materials and impacts on their properties; deep learning via advances such as convolutional neural networks to explore grain size
    
[^66]: 重新发现CNN在原始电子健康记录文本编码中的多功能性

    Rediscovery of CNN's Versatility for Text-based Encoding of Raw Electronic Health Records. (arXiv:2303.08290v1 [cs.LG])

    [http://arxiv.org/abs/2303.08290](http://arxiv.org/abs/2303.08290)

    本文发现，CNN在健康记录文本编码方面的多功能性和隐含层次结构可以提高其性能，提出了一种基于CNN的编码器来处理不同类型的EHR特征，并在临床任务中展示了其有效性。

    

    充分利用电子健康记录（EHR）中丰富的信息正逐渐成为医学领域的重要话题。最近的工作提出了一个有前途的框架，该框架可以在不考虑其格式和医学编码标准的情况下嵌入原始EHR数据的整个特征。然而，该框架仅侧重于对EHR进行最小的预处理，未考虑如何学习高效的EHR表示，包括计算和内存使用等方面。在本文中，我们寻找一种多功能的编码器，不仅将大量数据缩小到可管理的大小，还能很好地保留患者的核心信息，以执行各种临床任务。我们发现，具有分层结构的卷积神经网络（CNN）在各种任务（如重建，预测和生成）中经常优于最先进的模型，即使参数较少且训练时间较短。此外，利用EHR数据的固有层次结构可以提高CNN的性能。在这些发现的基础上，我们提出了一种基于CNN的编码器，可以处理不同类型的EHR特征，并证明了所提出的模型在几种临床任务中的有效性。

    Making the most use of abundant information in electronic health records (EHR) is rapidly becoming an important topic in the medical domain. Recent work presented a promising framework that embeds entire features in raw EHR data regardless of its form and medical code standards. The framework, however, only focuses on encoding EHR with minimal preprocessing and fails to consider how to learn efficient EHR representation in terms of computation and memory usage. In this paper, we search for a versatile encoder not only reducing the large data into a manageable size but also well preserving the core information of patients to perform diverse clinical tasks. We found that hierarchically structured Convolutional Neural Network (CNN) often outperforms the state-of-the-art model on diverse tasks such as reconstruction, prediction, and generation, even with fewer parameters and less training time. Moreover, it turns out that making use of the inherent hierarchy of EHR data can boost the perfo
    
[^67]: 通过超球嵌入和基于角度的正则化提高对抗鲁棒性

    Improving Adversarial Robustness with Hypersphere Embedding and Angular-based Regularizations. (arXiv:2303.08289v1 [cs.LG])

    [http://arxiv.org/abs/2303.08289](http://arxiv.org/abs/2303.08289)

    本文提出了一种新的对抗训练方法，角度-AT，结合超球嵌入和基于角度的正则化技术以提高深度神经网络的对抗鲁棒性能。

    

    对抗训练（AT）方法已被证明对深度神经网络的对抗攻击具有一定效果。已经提出了许多AT的变体来改善其性能。 Pang等人最近表明，将超球嵌入（HE）纳入现有的AT过程中可以提高对抗性。我们观察到现有的AT过程并不是为HE框架设计的，因此未能充分学习HE框架中的角度判别信息。在本文中，我们提出将HE与利用HE框架中的丰富角度信息的正则化项目相结合，将其集成到AT中。具体而言，我们提出的方法称为角度AT，将正则化项目添加到AT中，并通过角度特征明确强制权重特征紧凑性和类间分隔。实验结果表明，角度AT进一步提高了对抗鲁棒性。

    Adversarial training (AT) methods have been found to be effective against adversarial attacks on deep neural networks. Many variants of AT have been proposed to improve its performance. Pang et al. [1] have recently shown that incorporating hypersphere embedding (HE) into the existing AT procedures enhances robustness. We observe that the existing AT procedures are not designed for the HE framework, and thus fail to adequately learn the angular discriminative information available in the HE framework. In this paper, we propose integrating HE into AT with regularization terms that exploit the rich angular information available in the HE framework. Specifically, our method, termed angular-AT, adds regularization terms to AT that explicitly enforce weight-feature compactness and inter-class separation; all expressed in terms of angular features. Experimental results show that angular-AT further improves adversarial robustness.
    
[^68]: Transformer中的注意力-可能性关系分析

    Attention-likelihood relationship in transformers. (arXiv:2303.08288v1 [cs.CL])

    [http://arxiv.org/abs/2303.08288](http://arxiv.org/abs/2303.08288)

    本文分析了Transformer中标记可能性和注意力值之间的关联，揭示了在遇到意外标记时模型关注较少的信息，对于评估LLMs在现实世界场景中的稳健性具有有价值的影响。

    

    本文分析了大型语言模型（LLMs）如何表示上下文之外的单词，并调查它们对给定上下文来捕捉语义的依赖性。我们的可能性引导的文本扰动揭示了基于transformer的语言模型中标记可能性和注意力值之间的关联。广泛的实验发现，在更高层特别是遇到意外的标记时，模型会关注较少的来自自身的信息来计算它们的表示。这些发现对于评估LLMs在现实世界场景中的稳健性具有有价值的影响。在https://github.com/Flegyas/AttentionLikelihood中有完全可重现的代码库。

    We analyze how large language models (LLMs) represent out-of-context words, investigating their reliance on the given context to capture their semantics. Our likelihood-guided text perturbations reveal a correlation between token likelihood and attention values in transformer-based language models. Extensive experiments reveal that unexpected tokens cause the model to attend less to the information coming from themselves to compute their representations, particularly at higher layers. These findings have valuable implications for assessing the robustness of LLMs in real-world scenarios. Fully reproducible codebase at https://github.com/Flegyas/AttentionLikelihood.
    
[^69]: 将替代燃料汽车的采用与社会经济地位和空气质量指数联系起来

    Linking Alternative Fuel Vehicles Adoption with Socioeconomic Status and Air Quality Index. (arXiv:2303.08286v1 [cs.AI])

    [http://arxiv.org/abs/2303.08286](http://arxiv.org/abs/2303.08286)

    该研究借助机器学习技术，探究替代燃料汽车的普及，同时将其与消费者的社会经济地位和空气质量指数进行关联，从而制定合适的政策。

    

    这是一项研究，研究替代燃料汽车的潜在广泛使用，将它们与相应消费者的社会经济地位以及对 resulting 空气质量指数的影响联系起来。该领域的研究旨在利用机器学习技术，以促进适当的政策，推广替代燃料汽车的普及，例如电动汽车，要公正对待不同的人群。该模型使用 Pearson 相关系数对社会经济数据、空气质量指数和替代燃料汽车的数据关系进行建模。基于社会经济因素，使用线性回归对空气质量指数进行预测建模，以根据替代燃料汽车的采用情况进行调整。这项工作证明了人工智能的社会价值。

    This is a study on the potential widespread usage of alternative fuel vehicles, linking them with the socio-economic status of the respective consumers as well as the impact on the resulting air quality index. Research in this area aims to leverage machine learning techniques in order to promote appropriate policies for the proliferation of alternative fuel vehicles such as electric vehicles with due justice to different population groups. Pearson correlation coefficient is deployed in the modeling the relationships between socio-economic data, air quality index and data on alternative fuel vehicles. Linear regression is used to conduct predictive modeling on air quality index as per the adoption of alternative fuel vehicles, based on socio-economic factors. This work exemplifies artificial intelligence for social good.
    
[^70]: 面向UAE的深度学习疼痛级别检测部署：为以患者为中心的疼痛管理和诊断支持奠定基础

    Towards a Deep Learning Pain-Level Detection Deployment at UAE for Patient-Centric-Pain Management and Diagnosis Support: Framework and Performance Evaluation. (arXiv:2303.08273v1 [cs.HC])

    [http://arxiv.org/abs/2303.08273](http://arxiv.org/abs/2303.08273)

    本文提出了一个能在UAE部署的疼痛级别检测框架，并证明了它能够准确地识别疼痛级别，有助于以患者为中心的医疗计划。

    

    COVID-19大流行爆发揭示了及时干预的重要性，而短缺的医疗人员和设备使情况更加恶化。检测疼痛级别是确定患者病情严重程度的首要步骤。自动识别状态和感受有助于识别患者的症状，以及采取立即恰当的行动，并提供特定于患者状态的以患者为中心的医疗计划。本文提出了一个疼痛级别检测框架，用于在阿拉伯联合酋长国部署，并使用文献中最常用的方法评估其性能。我们的结果表明，部署疼痛级别深度学习检测框架在准确识别疼痛级别方面是有前途的。

    The outbreak of the COVID-19 pandemic revealed the criticality of timely intervention in a situation exacerbated by a shortage in medical staff and equipment. Pain-level screening is the initial step toward identifying the severity of patient conditions. Automatic recognition of state and feelings help in identifying patient symptoms to take immediate adequate action and providing a patient-centric medical plan tailored to a patient's state. In this paper, we propose a framework for pain-level detection for deployment in the United Arab Emirates and assess its performance using the most used approaches in the literature. Our results show that a deployment of a pain-level deep learning detection framework is promising in identifying the pain level accurately.
    
[^71]: 自动化专利提取支持聚焦化学空间内的生成建模

    Automated patent extraction powers generative modeling in focused chemical spaces. (arXiv:2303.08272v1 [physics.chem-ph])

    [http://arxiv.org/abs/2303.08272](http://arxiv.org/abs/2303.08272)

    本研究通过开发自动化管道，使用专利数据源训练领域特定的生成模型，利用专利中的弱标记应用类别中尽可能多的信息实现化学空间内生成建模。

    

    深度生成模型已成为反向分子设计的一种令人兴奋的手段，其进展来自于训练算法和分子表示之间的相互作用。应用于材料科学和化学领域时，其中一个主要挑战是缺乏具有属性标签的大规模训练数据集。已发布的专利包含在其在期刊上发表之前披露新材料的信息，是一种相对未被充分利用的科学知识广泛来源。由于专利被提交是为了保护特定用途，因此专利中的分子可以被视为弱标记的应用类别。此外，由美国专利与商标局（USPTO）发布的专利具有可下载的机器可读文本和分子结构。在本研究中，我们通过开发自动化管道，使用专利数据源训练领域特定的生成模型。

    Deep generative models have emerged as an exciting avenue for inverse molecular design, with progress coming from the interplay between training algorithms and molecular representations. One of the key challenges in their applicability to materials science and chemistry has been the lack of access to sizeable training datasets with property labels. Published patents contain the first disclosure of new materials prior to their publication in journals, and are a vast source of scientific knowledge that has remained relatively untapped in the field of data-driven molecular design. Because patents are filed seeking to protect specific uses, molecules in patents can be considered to be weakly labeled into application classes. Furthermore, patents published by the US Patent and Trademark Office (USPTO) are downloadable and have machine-readable text and molecular structures. In this work, we train domain-specific generative models using patent data sources by developing an automated pipeline
    
[^72]: Act-Then-Measure: 带主动测量的部分可观察环境中的强化学习

    Act-Then-Measure: Reinforcement Learning for Partially Observable Environments with Active Measuring. (arXiv:2303.08271v1 [cs.AI])

    [http://arxiv.org/abs/2303.08271](http://arxiv.org/abs/2303.08271)

    本论文研究了对于代理有直接控制何时以及如何收集信息的能力的马尔可夫决策过程。我们引入了行动后测量 (ATM) 策略，并开发了一个基于 ATM 启发式方法的强化学习算法，展示了其在多个部分可观察环境上优于先前的方法的卓越性能。

    

    本研究研究了马尔可夫决策过程 (MDPs)，其中代理有直接控制何时以及如何收集信息的能力，如 action-contingent noiselessly observable MDPs (ACNO-MPDs) 所形式化。在这些模型中，动作由两个组成部分组成：影响环境的控制动作和影响代理可以观察到什么的测量动作。为了解决 ACNO-MDPs，我们引入了行动后测量 (ATM) 策略，它假设在选择控制动作时可以忽略未来状态的不确定性。我们展示了遵循此策略可能导致较短的策略计算时间，并证明了该启发式方法引起的性能丧失的界限。为了确定是否采取测量行动，我们引入了测量价值的概念。我们基于 ATM 启发式方法开发了一个强化学习算法，使用针对部分可观察域的 Dyna-Q 变体，并展示了它在多个部分可观察环境上优于先前的方法的卓越性能。

    We study Markov decision processes (MDPs), where agents have direct control over when and how they gather information, as formalized by action-contingent noiselessly observable MDPs (ACNO-MPDs). In these models, actions consist of two components: a control action that affects the environment, and a measurement action that affects what the agent can observe. To solve ACNO-MDPs, we introduce the act-then-measure (ATM) heuristic, which assumes that we can ignore future state uncertainty when choosing control actions. We show how following this heuristic may lead to shorter policy computation times and prove a bound on the performance loss incurred by the heuristic. To decide whether or not to take a measurement action, we introduce the concept of measuring value. We develop a reinforcement learning algorithm based on the ATM heuristic, using a Dyna-Q variant adapted for partially observable domains, and showcase its superior performance compared to prior methods on a number of partially-o
    
[^73]: 印刷机器学习分类器的模型到电路的交叉逼近

    Model-to-Circuit Cross-Approximation For Printed Machine Learning Classifiers. (arXiv:2303.08255v1 [cs.LG])

    [http://arxiv.org/abs/2303.08255](http://arxiv.org/abs/2303.08255)

    本文提出了一个自动化的、面向特定体系结构的跨层逼近框架，能在PE中实现复杂的ML模型，压缩电路大小高达95%，分类精度损失平均在4%内。

    

    印刷电子（PE）能够按需制造，具有低不重复工程成本和亚分之一的制造成本。它还允许高度定制，这在硅片上是不可行的，而且特殊的体系结构也盛行于新兴的PE机器学习（ML）应用中以提高效率。然而，PE中的大特征尺寸阻止了复杂的ML模型的实现，即使使用特殊的体系结构也是如此。在这项工作中，我们提出了一个自动化的，跨层逼近框架，专为特定的体系结构定制，可以在PE中实现复杂的ML模型，如多层感知器（MLPs）和支持向量机（SVMs）。我们的框架在算法级别采用了硬件驱动的系数逼近、逻辑级别的电路列表修剪和电路级别的电压超标，全面合作。对12个MLP和12个SVM进行了广泛的实验评估，以及6000多个近似和精确设计，表明我们的模型到电路的交叉逼近框架可以将电路大小压缩高达95%，同时保证分类精度损失平均在4%内。

    Printed electronics (PE) promises on-demand fabrication, low non-recurring engineering costs, and sub-cent fabrication costs. It also allows for high customization that would be infeasible in silicon, and bespoke architectures prevail to improve the efficiency of emerging PE machine learning (ML) applications. Nevertheless, large feature sizes in PE prohibit the realization of complex ML models in PE, even with bespoke architectures. In this work, we present an automated, cross-layer approximation framework tailored to bespoke architectures that enable complex ML models, such as Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs), in PE. Our framework adopts cooperatively a hardware-driven coefficient approximation of the ML model at algorithmic level, a netlist pruning at logic level, and a voltage over-scaling at the circuit level. Extensive experimental evaluation on 12 MLPs and 12 SVMs and more than 6000 approximate and exact designs demonstrates that our model-to-cir
    
[^74]: R^2: 基于区间正则化的模型压缩与量化

    R^2: Range Regularization for Model Compression and Quantization. (arXiv:2303.08253v1 [cs.LG])

    [http://arxiv.org/abs/2303.08253](http://arxiv.org/abs/2303.08253)

    R^2提出了一种基于区间正则化的新方法，利用有效的最小值和最大值调整权重分布，从而使模型压缩和量化技术能够更好地利用其数值表示能力。该方法可以提高模型优化的质量，尤其是在较低位上。

    

    参数正则化是一种广泛应用于提高泛化性能的技术，也可用于调整权重分布以达到各种目的。本文探讨了如何利用权重正则化来辅助模型量化和压缩技术，然后提出了区间正则化(R^2)以进一步提高模型优化的质量，重点放在防止异常值方面。通过有效地调整分布中的最小值和最大值，将整个分布塑造成紧凑的形状，从而使模型压缩和量化技术能够更好地利用它们有限的数值表示能力。我们引入了L-inf正则化，其扩展间隔正则化和新的soft-min-max正则化，作为全精度模型训练期间的正则化损失。结合最先进的量化和压缩技术，利用R^2训练的模型在平均水平上表现更好，尤其是在较低位上。

    Model parameter regularization is a widely used technique to improve generalization, but also can be used to shape the weight distributions for various purposes. In this work, we shed light on how weight regularization can assist model quantization and compression techniques, and then propose range regularization (R^2) to further boost the quality of model optimization by focusing on the outlier prevention. By effectively regulating the minimum and maximum weight values from a distribution, we mold the overall distribution into a tight shape so that model compression and quantization techniques can better utilize their limited numeric representation powers. We introduce L-inf regularization, its extension margin regularization and a new soft-min-max regularization to be used as a regularization loss during full-precision model training. Coupled with state-of-the-art quantization and compression techniques, models trained with R^2 perform better on an average, specifically at lower bit 
    
[^75]: 在视觉Transformer中学习成长人工海马，实现弹性终身学习

    Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning. (arXiv:2303.08250v1 [cs.CV])

    [http://arxiv.org/abs/2303.08250](http://arxiv.org/abs/2303.08250)

    本文提出了一种在Vision Transformer中学习成长人工海马的方法，以实现弹性终身学习。通过神经架构搜索进行维护，选取多头自注意力块中的最终线性投影层进行ArtiHippo的实现和成长。

    

    终身学习需要拥有人类智能的韧性，即不存在灾难性遗忘，这种韧性与大脑中复杂的记忆机制，尤其是海马维护的长期记忆（LM）紧密相关。Transformer已经成为人工智能“大脑”的对应体，但LM组件在终身学习中尚未充分探索。本文提出了一种在Vision Transformer中学习成长人工海马（ArtiHippo）以实现弹性终身学习的方法。通过全面消融实验，选定多头自注意力（MHSA）块中的最终线性投影层来实现和成长ArtiHippo。ArtiHippo由专家混合（MoEs）表示。每个专家组件是线性投影层的现场变体，通过神经架构搜索（NAS）进行维护，搜索空间由四个基本成长操作（跳过、重用、适应和新）定义。

    Lifelong learning without catastrophic forgetting (i.e., resiliency) possessed by human intelligence is entangled with sophisticated memory mechanisms in the brain, especially the long-term memory (LM) maintained by Hippocampi. To a certain extent, Transformers have emerged as the counterpart ``Brain" of Artificial Intelligence (AI), and yet leave the LM component under-explored for lifelong learning settings. This paper presents a method of learning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers (ViTs) for resilient lifelong learning. With a comprehensive ablation study, the final linear projection layer in the multi-head self-attention (MHSA) block is selected in realizing and growing ArtiHippo. ArtiHippo is represented by a mixture of experts (MoEs). Each expert component is an on-site variant of the linear projection layer, maintained via neural architecture search (NAS) with the search space defined by four basic growing operations -- skip, reuse, adapt, and new 
    
[^76]: 利用机器学习学习探测空间的系统设计空间探索

    Systematic design space exploration by learning the explored space using Machine Learning. (arXiv:2303.08249v1 [cs.LG])

    [http://arxiv.org/abs/2303.08249](http://arxiv.org/abs/2303.08249)

    本文提出了一种利用机器学习算法来学习已探索数据空间，跟踪参数探索状态的方法，通过修正的鲁棒随机切割森林和启发式方法在二维欧几里得空间中进行了验证。

    

    当前欧几里得空间参数空间探索的实践主要由随机抽样或试验设计方法主导。这些方法最大的问题是无法跟踪参数空间的哪一部分已被探索，哪一部分尚未被探索。在这种情况下，我们利用现代机器学习方法几何学习已探索数据空间，以跟踪已经探索过的区域，并从未探索的区域中提取样本。为此，我们使用了修改版的鲁棒随机切割森林以及其他基于启发式的方法。我们展示了我们的方法及其在二维欧几里得空间中的进展，但它可以扩展到任何维度，因为底层方法是通用的。

    Current practice in parameter space exploration in euclidean space is dominated by randomized sampling or design of experiment methods. The biggest issue with these methods is not keeping track of what part of parameter space has been explored and what has not. In this context, we utilize the geometric learning of explored data space using modern machine learning methods to keep track of already explored regions and samples from the regions that are unexplored. For this purpose, we use a modified version of a robust random-cut forest along with other heuristic-based approaches. We demonstrate our method and its progression in two-dimensional Euclidean space but it can be extended to any dimension since the underlying method is generic.
    
[^77]: 多维流式时间序列的最优抽样设计及在电力系统监测中的应用

    Optimal Sampling Designs for Multi-dimensional Streaming Time Series with Application to Power Grid Sensor Data. (arXiv:2303.08242v1 [stat.ML])

    [http://arxiv.org/abs/2303.08242](http://arxiv.org/abs/2303.08242)

    本文提出针对多维流式时间序列的最优抽样设计方法，并应用于高速电力消耗数据的低成本实时分析，提高了计算效率。

    

    物联网系统产生了大量高速时间相关的流式数据，并经常与计算或能源约束下的在线推断任务相连。对这些流式时间序列数据的在线分析经常面临统计效率和计算成本之间的权衡。解决这种权衡的一个重要方法是抽样，仅选择一小部分样本进行模型拟合和更新。为了满足物联网系统动态关系分析的需求，本文研究了面向多维流式时间序列的数据依赖抽样选择和在线推断问题，旨在提供高速电力消耗数据的低成本实时分析。受实验设计中D-效应准则的启发，我们提出了一类在线数据缩减方法，实现了最优抽样准则，并提高了在线分析的计算效率。我们展示了...

    The Internet of Things (IoT) system generates massive high-speed temporally correlated streaming data and is often connected with online inference tasks under computational or energy constraints. Online analysis of these streaming time series data often faces a trade-off between statistical efficiency and computational cost. One important approach to balance this trade-off is sampling, where only a small portion of the sample is selected for the model fitting and update. Motivated by the demands of dynamic relationship analysis of IoT system, we study the data-dependent sample selection and online inference problem for a multi-dimensional streaming time series, aiming to provide low-cost real-time analysis of high-speed power grid electricity consumption data. Inspired by D-optimality criterion in design of experiments, we propose a class of online data reduction methods that achieve an optimal sampling criterion and improve the computational efficiency of the online analysis. We show 
    
[^78]: 基于Beta-Bernoulli过程和深度神经网络的贝叶斯稀疏编码

    Bayesian Beta-Bernoulli Process Sparse Coding with Deep Neural Networks. (arXiv:2303.08230v1 [cs.LG])

    [http://arxiv.org/abs/2303.08230](http://arxiv.org/abs/2303.08230)

    本文提出了一种基于Beta-Bernoulli过程和非参数迭代算法的深度稀疏编码模型，旨在学习具有尺度不变性的离散特征，并鼓励表示的稀疏性。

    

    针对深度离散潜变量模型，已经提出了几种近似推断方法。然而，在经典稀疏编码模型中成功应用的非参数方法，在深度模型的上下文中很少被探索。我们提出了一种非参数迭代算法，用于学习此类深度模型中的离散潜在表示。此外，为了学习具有尺度不变性的离散特征，我们提出了本地数据缩放变量。最后，为了在我们的表示中鼓励稀疏性，我们在潜在因子上提出了Beta-Bernoulli过程先验。我们对耦合不同似然模型的稀疏编码模型进行了评估。我们在具有不同特征的数据集上评估我们的方法，并将结果与当前的摊销近似推断方法进行比较。

    Several approximate inference methods have been proposed for deep discrete latent variable models. However, non-parametric methods which have previously been successfully employed for classical sparse coding models have largely been unexplored in the context of deep models. We propose a non-parametric iterative algorithm for learning discrete latent representations in such deep models. Additionally, to learn scale invariant discrete features, we propose local data scaling variables. Lastly, to encourage sparsity in our representations, we propose a Beta-Bernoulli process prior on the latent factors. We evaluate our spare coding model coupled with different likelihood models. We evaluate our method across datasets with varying characteristics and compare our results to current amortized approximate inference methods.
    
[^79]: 借助深度神经网络进行增材制造的霍尔效应推进器设计

    Hall effect thruster design via deep neural network for additive manufacturing. (arXiv:2303.08227v1 [cs.LG])

    [http://arxiv.org/abs/2303.08227](http://arxiv.org/abs/2303.08227)

    本文介绍了一种借助深度神经网络进行霍尔效应推进器设计的方法，可用于轻松获得所需特性的设计，使用的计算资源更少，比通常设计方法更为灵活。

    

    霍尔效应推进器是太空应用中最通用和流行的电推进系统之一。随着工业趋势向星际任务发展，对这种推进系统的设计开发方案不断更新，而正确的放电通道尺寸对霍尔效应推进器的性能有很大影响。由于这种推进系统的完整物理模型尚未经过快速计算和设计迭代的优化，大多数推进器都是使用所谓的比例定律进行设计。但这项工作侧重于使用深度机器学习来创建预测性能模型，该模型可用于轻松获得所需特性的霍尔推进器的设计，且使用的计算资源比从头设计的要少得多，比通常的比例设计方法更加灵活。

    Hall effect thrusters are one of the most versatile and popular electric propulsion systems for space use. Industry trends towards interplanetary missions arise advances in design development of such propulsion systems. It is understood that correct sizing of discharge channel in Hall effect thruster impact performance greatly. Since the complete physics model of such propulsion system is not yet optimized for fast computations and design iterations, most thrusters are being designed using so-called scaling laws. But this work focuses on rather novel approach, which is outlined less frequently than ordinary scaling design approach in literature. Using deep machine learning it is possible to create predictive performance model, which can be used to effortlessly get design of required hall thruster with required characteristics using way less computational power than design from scratch and way more flexible than usual scaling approach.
    
[^80]: DeepAxe: 一种用于探索DNN加速器的近似和可靠性权衡的框架

    DeepAxe: A Framework for Exploration of Approximation and Reliability Trade-offs in DNN Accelerators. (arXiv:2303.08226v1 [cs.LG])

    [http://arxiv.org/abs/2303.08226](http://arxiv.org/abs/2303.08226)

    DeepAxe是一个用于在DNN加速器的设计空间中考虑近似和可靠性权衡的框架，逼近可靠性关键的DNN，并提供一组Pareto最优的DNN实现设计空间。

    

    随着Deep Neural Networks（DNNs）在广泛的安全关键型应用中的作用正在扩大，新兴的DNN经历了计算能力方面的巨大增长。这增加了提高DNN加速器可靠性的必要性，同时降低硬件平台上的计算负担，即降低能耗和执行时间，提高DNN加速器的效率。因此，硬件性能（即区域、功率和延迟）与DNN加速器实现的可靠性之间的权衡变得至关重要，需要工具进行分析。在本文中，我们提出了一个框架DeepAxe，用于在考虑应用功能近似对准确度、可靠性和硬件性能的三方影响的情况下，对基于FPGA的DNN实现进行设计空间探索。该框架使得对于关键可靠性的DNN进行有选择的逼近，提供了一组Pareto最优的DNN实现设计空间。

    While the role of Deep Neural Networks (DNNs) in a wide range of safety-critical applications is expanding, emerging DNNs experience massive growth in terms of computation power. It raises the necessity of improving the reliability of DNN accelerators yet reducing the computational burden on the hardware platforms, i.e. reducing the energy consumption and execution time as well as increasing the efficiency of DNN accelerators. Therefore, the trade-off between hardware performance, i.e. area, power and delay, and the reliability of the DNN accelerator implementation becomes critical and requires tools for analysis. In this paper, we propose a framework DeepAxe for design space exploration for FPGA-based implementation of DNNs by considering the trilateral impact of applying functional approximation on accuracy, reliability and hardware performance. The framework enables selective approximation of reliability-critical DNNs, providing a set of Pareto-optimal DNN implementation design spac
    
[^81]: 使用场地不可知元学习和脑部MRI进行自闭症谱系障碍的少样本分类

    Few-Shot Classification of Autism Spectrum Disorder using Site-Agnostic Meta-Learning and Brain MRI. (arXiv:2303.08224v1 [eess.IV])

    [http://arxiv.org/abs/2303.08224](http://arxiv.org/abs/2303.08224)

    本文研究了针对自闭症谱系障碍的场地不可知元学习模型在少样本情况下的表现，使用在多个站点的MRI数据训练出的模型，实现对病人和正常人的快速识别分类。

    

    对于医学影像中的机器学习应用，由于训练数据可用性有限，设计针对细微病情（如自闭症谱系障碍）的放射学分类器受到阻碍。迁移学习是解决低训练数据问题的一种方法。本文探讨在具有多个站点的先前数据的情况下使用元学习的使用，即我们称之为场地不可知元学习的方法，用于非常低数据制度的情况。受到元学习在优化模型跨多个任务方面的有效性的启发，我们提出了一个框架，使其适应于不同站点之间的学习。我们在来自38个影像站点的2,201个T1加权（T1-w）MRI扫描中测试了我们的元学习模型，这些数据是作为自闭症脑部影像数据交换（ABIDE）的一部分收集的（年龄：5.2-64.0岁）。该方法被训练为查找一个良好的初始状态，以便快速适应新的未见数据。

    For machine learning applications in medical imaging, the availability of training data is often limited, which hampers the design of radiological classifiers for subtle conditions such as autism spectrum disorder (ASD). Transfer learning is one method to counter this problem of low training data regimes. Here we explore the use of meta-learning for very low data regimes in the context of having prior data from multiple sites - an approach we term site-agnostic meta-learning. Inspired by the effectiveness of meta-learning for optimizing a model across multiple tasks, here we propose a framework to adapt it to learn across multiple sites. We tested our meta-learning model for classifying ASD versus typically developing controls in 2,201 T1-weighted (T1-w) MRI scans collected from 38 imaging sites as part of Autism Brain Imaging Data Exchange (ABIDE) [age: 5.2-64.0 years]. The method was trained to find a good initialization state for our model that can quickly adapt to data from new uns
    
[^82]: 一个局部最优集合分割优化的2-opt算法

    A 2-opt Algorithm for Locally Optimal Set Partition Optimization. (arXiv:2303.08219v1 [cs.DS])

    [http://arxiv.org/abs/2303.08219](http://arxiv.org/abs/2303.08219)

    该论文提出了一个局部最优的集合分割问题版本，并开发了一个$O(N^2)$时间的2-opt算法，可以处理任意输入精度，具有广泛的应用场景。

    

    我们的研究涉及集合分割问题的优化版本，目标是将两个不相交分区的和的绝对差最小化。虽然这个问题已知为NP难问题，需要指数时间来解决，但我们提出了一个更简单的问题版本，即寻找一个局部最优解。在我们的方法中，考虑到至多移动两个元素的局部最优性。为了实现这一点，我们开发了一种算法，可以在$O(N^2)$时间和$O(N)$空间内生成局部最优解。我们的算法可以处理任意输入精度，不需要正数或整数输入。因此，它可以轻松地应用于各种问题场景中。

    Our research deals with the optimization version of the set partition problem, where the objective is to minimize the absolute difference between the sums of the two disjoint partitions. Although this problem is known to be NP-hard and requires exponential time to solve, we propose a less demanding version of this problem where the goal is to find a locally optimal solution. In our approach, we consider the local optimality in respect to any movement of at most two elements. To accomplish this, we developed an algorithm that can generate a locally optimal solution in at most $O(N^2)$ time and $O(N)$ space. Our algorithm can handle arbitrary input precisions and does not require positive or integer inputs. Hence, it can be applied in various problem scenarios with ease.
    
[^83]: 用于阿尔茨海默病检测的结构性MRI扫描的Vision Transformers高效训练

    Efficiently Training Vision Transformers on Structural MRI Scans for Alzheimer's Disease Detection. (arXiv:2303.08216v1 [eess.IV])

    [http://arxiv.org/abs/2303.08216](http://arxiv.org/abs/2303.08216)

    本文尝试使用Vision Transformers对基于MRI扫描的性别和AD分类任务进行了测试，其中两种ViT架构变体分别实现了0.987的性别分类AUC和0.892的AD分类AUC。该方法可为大规模神经影像学识别提供高效解决方案。

    

    大规模人群神经影像学对于识别促进或抵抗脑疾病的因素以及协助诊断、亚型分类和预后都具有价值。本文尝试使用Vision Transformers(ViT)对基于难度调整的一系列神经影像学任务进行了测试，包括性别和阿尔茨海默病(AD)基于3D大脑MRI的分类。在实验中，两种ViT架构变体分别实现了0.987的性别分类AUC和0.892的AD分类AUC。我们独立评估了模型在两个基准AD数据集上的性能，并获得了5%和9-10%的性能提升。

    Neuroimaging of large populations is valuable to identify factors that promote or resist brain disease, and to assist diagnosis, subtyping, and prognosis. Data-driven models such as convolutional neural networks (CNNs) have increasingly been applied to brain images to perform diagnostic and prognostic tasks by learning robust features. Vision transformers (ViT) - a new class of deep learning architectures - have emerged in recent years as an alternative to CNNs for several computer vision applications. Here we tested variants of the ViT architecture for a range of desired neuroimaging downstream tasks based on difficulty, in this case for sex and Alzheimer's disease (AD) classification based on 3D brain MRI. In our experiments, two vision transformer architecture variants achieved an AUC of 0.987 for sex and 0.892 for AD classification, respectively. We independently evaluated our models on data from two benchmark AD datasets. We achieved a performance boost of 5% and 9-10% upon fine-t
    
[^84]: 遗忘是否是前向迁移的良好归纳偏差？

    Is forgetting less a good inductive bias for forward transfer?. (arXiv:2303.08207v1 [cs.LG])

    [http://arxiv.org/abs/2303.08207](http://arxiv.org/abs/2303.08207)

    本文提出对于持续学习任务来说，遗忘不是一种良好的归纳偏差。之前的研究没有考虑到前向迁移的量度方式，本文提出了一种新的量度方式，发现较不遗忘的模型具有更好的性能。

    

    持续学习的主要动机之一是，该问题设置允许模型从过去的任务中积累知识以更有效地学习新任务。然而，最近的研究表明，持续学习算法所优化的关键指标，即减少灾难性遗忘，并不与前向知识迁移相关。我们认为之前的研究结论是由于他们衡量前向迁移的方式所致。我们认为，衡量一个任务的前向迁移不应受到为保留先前任务知识而对持续学习器施加的限制的影响。相反，前向迁移应该通过持续学习产生的一组表示来评估给定一个新任务有多容易学习。在这种前向迁移概念下，我们评估了不同的持续学习算法在各种图像分类基准测试中的表现。我们的结果表明，较不遗忘的模型具有更好的性能，特别是当训练数据数量少时。

    One of the main motivations of studying continual learning is that the problem setting allows a model to accrue knowledge from past tasks to learn new tasks more efficiently. However, recent studies suggest that the key metric that continual learning algorithms optimize, reduction in catastrophic forgetting, does not correlate well with the forward transfer of knowledge. We believe that the conclusion previous works reached is due to the way they measure forward transfer. We argue that the measure of forward transfer to a task should not be affected by the restrictions placed on the continual learner in order to preserve knowledge of previous tasks. Instead, forward transfer should be measured by how easy it is to learn a new task given a set of representations produced by continual learning on previous tasks. Under this notion of forward transfer, we evaluate different continual learning algorithms on a variety of image classification benchmarks. Our results indicate that less forgetf
    
[^85]: RODD: 数据立方体中的鲁棒异常检测

    RODD: Robust Outlier Detection in Data Cubes. (arXiv:2303.08193v1 [cs.DB])

    [http://arxiv.org/abs/2303.08193](http://arxiv.org/abs/2303.08193)

    本文提出了第一个评估数据立方体中鲁棒异常检测方法的框架RODD，介绍了一种新的基于随机森林的异常检测方法RODD-RF，并将其应用于真实世界的数据中，结果表明RODD-RF可以导致更好的异常检测。

    

    数据立方体是多维数据库，通常由几个独立的数据库构建而成，可作为灵活的数据分析基础。令人惊讶的是，数据立方体上的异常检测尚未得到广泛的研究。在本文中，我们提供了第一个评估数据立方体中鲁棒异常检测方法的框架（RODD）。我们介绍了一种新的基于随机森林的异常检测方法（RODD-RF），并将其与基于鲁棒位置估计的传统方法进行比较。我们提出了一般类型的测试数据，并在模拟研究中研究了所有方法。此外，我们将ROOD-RF应用于真实世界的数据。结果表明，RODD-RF可以导致更好的异常检测。

    Data cubes are multidimensional databases, often built from several separate databases, that serve as flexible basis for data analysis. Surprisingly, outlier detection on data cubes has not yet been treated extensively. In this work, we provide the first framework to evaluate robust outlier detection methods in data cubes (RODD). We introduce a novel random forest-based outlier detection approach (RODD-RF) and compare it with more traditional methods based on robust location estimators. We propose a general type of test data and examine all methods in a simulation study. Moreover, we apply ROOD-RF to real world data. The results show that RODD-RF can lead to improved outlier detection.
    
[^86]: 机器学习在自动驾驶车辆引导中的侧向控制使用

    Vehicle lateral control using Machine Learning for automated vehicle guidance. (arXiv:2303.08187v1 [cs.LG])

    [http://arxiv.org/abs/2303.08187](http://arxiv.org/abs/2303.08187)

    本研究使用机器学习模型设计了车辆横向控制器，在模拟器上训练模型并使用随机森林模型预测置信度/不确定性，成功应用于自动引导车辆，具有非常好的泛化能力。

    

    决策中的不确定性对于在现实世界中运行的安全关键系统中使用的机器学习模型至关重要。因此，对于安全操作，优雅地处理不确定性非常重要。在本文中，我们使用机器学习模型设计了车辆的横向控制器。为此，我们训练了一个随机森林模型和一个深度神经网络模型。由于随机森林模型中的集成，我们可以预测预测的置信度/不确定性。我们使用模拟器的一条赛道上运行汽车产生的数据对控制器进行训练，并在其他赛道上进行测试。由于可信度的预测，我们可以决定控制器在预测不确定度较小时何时需要接管控制。我们有两个结果要分享：首先，即使只有很少的标记数据，与深度神经网络和基于随机森林的回归器相比，随机森林模型具有非常好的泛化能力。

    Uncertainty in decision-making is crucial in the machine learning model used for a safety-critical system that operates in the real world. Therefore, it is important to handle uncertainty in a graceful manner for the safe operation of the CPS. In this work, we design a vehicle's lateral controller using a machine-learning model. To this end, we train a random forest model that is an ensemble model and a deep neural network model. Due to the ensemble in the random forest model, we can predict the confidence/uncertainty in the prediction. We train our controller on data generated from running the car on one track in the simulator and tested it on other tracks. Due to prediction in confidence, we could decide when the controller is less confident in prediction and takes control if needed. We have two results to share: first, even on a very small number of labeled data, a very good generalization capability of the random forest-based regressor in comparison with a deep neural network and a
    
[^87]: 视觉艺术推荐的要素：学习画作的潜在语义表征

    The Elements of Visual Art Recommendation: Learning Latent Semantic Representations of Paintings. (arXiv:2303.08182v1 [cs.IR])

    [http://arxiv.org/abs/2303.08182](http://arxiv.org/abs/2303.08182)

    本文研究了如何高效地捕捉视觉艺术的元素，提出了结合文本和视觉特征学习技术的推荐系统，用于个性化艺术品推荐，结果显示两者的结合可以捕捉最合适的隐藏语义关系。

    

    艺术品推荐具有挑战性，因为它需要理解用户如何与高度主观的内容互动，艺术品中嵌入的概念的复杂性，以及它们可能引起用户的情感和认知反应。本文重点研究如何高效地捕捉视觉艺术的元素（即潜在语义关系），以进行个性化推荐。我们提出并研究了基于文本和视觉特征学习技术以及它们的组合的推荐系统。我们对推荐质量进行了小规模和大规模的用户中心评估。我们的结果表明，文本特征比视觉特征表现更好，而两者的结合可以捕捉艺术品推荐最合适的隐藏语义关系。最终，本文有助于理解如何提供适合用户兴趣和感知的内容。

    Artwork recommendation is challenging because it requires understanding how users interact with highly subjective content, the complexity of the concepts embedded within the artwork, and the emotional and cognitive reflections they may trigger in users. In this paper, we focus on efficiently capturing the elements (i.e., latent semantic relationships) of visual art for personalized recommendation. We propose and study recommender systems based on textual and visual feature learning techniques, as well as their combinations. We then perform a small-scale and a large-scale user-centric evaluation of the quality of the recommendations. Our results indicate that textual features compare favourably with visual ones, whereas a fusion of both captures the most suitable hidden semantic relationships for artwork recommendation. Ultimately, this paper contributes to our understanding of how to deliver content that suitably matches the user's interests and how they are perceived.
    
[^88]: Allegro-Legato: 基于 Sharpness-Aware Minimization 的大规模且快速的神经网络量子分子动力学模拟

    Allegro-Legato: Scalable, Fast, and Robust Neural-Network Quantum Molecular Dynamics via Sharpness-Aware Minimization. (arXiv:2303.08169v1 [cs.DC])

    [http://arxiv.org/abs/2303.08169](http://arxiv.org/abs/2303.08169)

    Allegro-Legato 是一种基于机器学习的NNQMD模型，使用 Sharpness-Aware Minimization 解决了计算机多核心处理器架构下的精度扩展问题，大大提高了模型的普适性和可靠性。

    

    基于机器学习的神经网络量子分子动力学模拟（NNQMD）正在通过提供比传统方法更高的精度和速度来彻底改变材料原子级模拟。当前最先进的NNQMD模型基于群论的旋转等变性特征和局部描述符，称为 Allegro（意为快速），提供了比以前更高的准确度和速度。然而，在高性能超级计算机上，它面临一个精度扩展的问题，即不合理的预测随着原子数和模拟时间的增加而增加。本文通过将 Allegro 模型与 Sharpeness aware minimization（SAM）相结合，提高损失函数表面的平滑性，从而解决了这个问题。产生的 Allegro-Legato 模型展示了更长时间和更多原子数的模拟。

    Neural-network quantum molecular dynamics (NNQMD) simulations based on machine learning are revolutionizing atomistic simulations of materials by providing quantum-mechanical accuracy but orders-of-magnitude faster, illustrated by ACM Gordon Bell prize (2020) and finalist (2021). State-of-the-art (SOTA) NNQMD model founded on group theory featuring rotational equivariance and local descriptors has provided much higher accuracy and speed than those models, thus named Allegro (meaning fast). On massively parallel supercomputers, however, it suffers a fidelity-scaling problem, where growing number of unphysical predictions of interatomic forces prohibits simulations involving larger numbers of atoms for longer times. Here, we solve this problem by combining the Allegro model with sharpness aware minimization (SAM) for enhancing the robustness of model through improved smoothness of the loss landscape. The resulting Allegro-Legato (meaning fast and "smooth") model was shown to elongate the
    
[^89]: 基于图神经网络的公平图过滤替代方法

    Graph Neural Network Surrogates of Fair Graph Filtering. (arXiv:2303.08157v1 [cs.LG])

    [http://arxiv.org/abs/2303.08157](http://arxiv.org/abs/2303.08157)

    通过引入过滤器感知的通用近似框架，该方法定义了合适的图神经网络在运行时训练以满足统计平等约束，同时最小程度扰动原始后验情况下实现此目标。

    

    通过边传播将先前的节点值转换为后来的分数的图滤波器通常支持影响人类的图挖掘任务，例如推荐和排名。因此，重要的是在满足节点组之间的统计平等约束方面使它们公平（例如，按其代表性将分数质量在性别之间均衡分配）。为了在最小程度地扰动原始后验情况下实现此目标，我们引入了一个过滤器感知的通用近似框架，用于后验目标。这定义了适当的图神经网络，其在运行时训练，类似于过滤器，但也在本地优化包括公平感知在内的大类目标。在一组8个过滤器和5个图形的实验中，我们的方法在满足统计平等约束方面表现得不亚于替代品，同时保留基于分数的社区成员推荐的AUC并在传播先前节拍时创建最小实用损失。

    Graph filters that transform prior node values to posterior scores via edge propagation often support graph mining tasks affecting humans, such as recommendation and ranking. Thus, it is important to make them fair in terms of satisfying statistical parity constraints between groups of nodes (e.g., distribute score mass between genders proportionally to their representation). To achieve this while minimally perturbing the original posteriors, we introduce a filter-aware universal approximation framework for posterior objectives. This defines appropriate graph neural networks trained at runtime to be similar to filters but also locally optimize a large class of objectives, including fairness-aware ones. Experiments on a collection of 8 filters and 5 graphs show that our approach performs equally well or better than alternatives in meeting parity constraints while preserving the AUC of score-based community member recommendation and creating minimal utility loss in prior diffusion.
    
[^90]: 性能嵌入：一种基于相似性的自动性能优化方法

    Performance Embeddings: A Similarity-based Approach to Automatic Performance Optimization. (arXiv:2303.08142v1 [cs.SE])

    [http://arxiv.org/abs/2303.08142](http://arxiv.org/abs/2303.08142)

    本文提出了性能嵌入的方法，通过构建子程序的嵌入空间来实现性能优化的直接知识传输。传输调整将搜索复杂度降低了多达四个数量级，并在稀疏-密集矩阵乘法中优于MKL库。

    

    性能优化是一项越来越具有挑战性但经常重复的任务。虽然每个平台都有其独特之处，但基于数据移动和计算特性的底层代码转换在应用程序中会反复出现。本文提议利用这些相似性通过构建子程序的嵌入空间。该连续空间通过符号代码分析和性能分析捕捉循环嵌套的静态和动态特性。性能嵌入使性能调整的直接知识传输成为可能，这可以来自自动调整或量身定制的改进。我们在深度神经网络、密集和稀疏线性代数组合以及数值天气预报模板的案例研究中展示了这种传输调整方法。传输调整将搜索复杂度降低了多达四个数量级，并在稀疏-密集矩阵乘法中优于MKL库。结果表明...

    Performance optimization is an increasingly challenging but often repetitive task. While each platform has its quirks, the underlying code transformations rely on data movement and computational characteristics that recur across applications. This paper proposes to leverage those similarities by constructing an embedding space for subprograms. The continuous space captures both static and dynamic properties of loop nests via symbolic code analysis and performance profiling, respectively. Performance embeddings enable direct knowledge transfer of performance tuning between applications, which can result from autotuning or tailored improvements. We demonstrate this transfer tuning approach on case studies in deep neural networks, dense and sparse linear algebra compositions, and numerical weather prediction stencils. Transfer tuning reduces the search complexity by up to four orders of magnitude and outperforms the MKL library in sparse-dense matrix multiplication. The results exhibit cl
    
[^91]: 深度学习在光学显微镜中的数字染色 - 一篇综述

    Digital staining in optical microscopy using deep learning -- a review. (arXiv:2303.08140v1 [eess.IV])

    [http://arxiv.org/abs/2303.08140](http://arxiv.org/abs/2303.08140)

    传统的生物染色协议面临着许多挑战，数字染色通过利用深度学习将光学对比转化为实际染色的基础对比成为解决方案。

    

    直到最近，传统的生物染色一直被视为与临床诊断、基础研究和生物技术相关的大多数生物医学问题的业已确立的基准。尽管作为黄金标准的角色，但染色协议面临着一些挑战，如需要对样品进行广泛、手动处理、长时间延迟、改变组织稳态、对于给定样品的对比剂选择有限，只能进行二维成像而非三维断层扫描等。另一方面，不需要外源性和人工标记物的无标记光学技术，通过利用内在的光学对比机制，其特异性通常对人类观察者不太明显。在过去几年中，数字染色已经成为一个有前途的概念，利用现代深度学习将光学对比转化为实际染色的基础对比。在这篇综述文章中，我们提供了对当前数字染色方法和技术的深入分析。

    Until recently, conventional biochemical staining had the undisputed status as well-established benchmark for most biomedical problems related to clinical diagnostics, fundamental research and biotechnology. Despite this role as gold-standard, staining protocols face several challenges, such as a need for extensive, manual processing of samples, substantial time delays, altered tissue homeostasis, limited choice of contrast agents for a given sample, 2D imaging instead of 3D tomography and many more. Label-free optical technologies, on the other hand, do not rely on exogenous and artificial markers, by exploiting intrinsic optical contrast mechanisms, where the specificity is typically less obvious to the human observer. Over the past few years, digital staining has emerged as a promising concept to use modern deep learning for the translation from optical contrast to established biochemical contrast of actual stainings. In this review article, we provide an in-depth analysis of the cu
    
[^92]: 基于信息理论的固定专家建议下赌博机的遗憾界限

    Information-Theoretic Regret Bounds for Bandits with Fixed Expert Advice. (arXiv:2303.08102v1 [cs.LG])

    [http://arxiv.org/abs/2303.08102](http://arxiv.org/abs/2303.08102)

    本文研究固定专家建议下的赌博机问题，提出了基于信息论的遗憾界限，可以使得某些算法的遗憾无限接近于零。此外，我们还提出了KL散度来描述专家之间的相似性界限，并给出了下限证明算法的最优性。

    

    本文研究了在专家是固定和已知的情况下，赌博机与专家建议的问题，这些专家是行动固定和已知分布。相比以前的分析，我们展示了这种情况下遗憾是由衡量专家之间相似性的信息论量所控制的。在一些自然特殊情况下，这使我们能够获得EXP4的第一个遗憾界限，如果专家足够相似，则可以无限接近于零。为另一种算法提供了可以用KL散度来描述专家之间相似性的另一种界限，并且在某些情况下，我们展示了这个界限可以比EXP4更小。此外，我们为某些专家类别提供了下限，展示了我们分析的算法在某些情况下是几乎最优的。

    We investigate the problem of bandits with expert advice when the experts are fixed and known distributions over the actions. Improving on previous analyses, we show that the regret in this setting is controlled by information-theoretic quantities that measure the similarity between experts. In some natural special cases, this allows us to obtain the first regret bound for EXP4 that can get arbitrarily close to zero if the experts are similar enough. While for a different algorithm, we provide another bound that describes the similarity between the experts in terms of the KL-divergence, and we show that this bound can be smaller than the one of EXP4 in some cases. Additionally, we provide lower bounds for certain classes of experts showing that the algorithms we analyzed are nearly optimal in some cases.
    
[^93]: 概率扩散模型的广义尺度空间特性

    Generalised Scale-Space Properties for Probabilistic Diffusion Models. (arXiv:2303.07900v1 [eess.IV])

    [http://arxiv.org/abs/2303.07900](http://arxiv.org/abs/2303.07900)

    本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。

    

    概率扩散模型在深度学习社区中越来越受欢迎。它们生成从学习图像分布的令人信服的样本，具有广泛的实际应用。这些方法最初是受漂移-扩散过程的启发，但在近期的实践导向的出版物中，这些起源得到了较少的关注。本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。此外，我们讨论了深度学习和基于模型的世界中漂移-扩散物理核心概念解释之间的相似性和差异。为此，我们考察了概率扩散与渗透滤波器之间的关系。

    Probabilistic diffusion models enjoy increasing popularity in the deep learning community. They generate convincing samples from a learned distribution of input images with a wide field of practical applications. Originally, these approaches were motivated from drift-diffusion processes, but these origins find less attention in recent, practice-oriented publications.  We investigate probabilistic diffusion models from the viewpoint of scale-space research and show that they fulfil generalised scale-space properties on evolving probability distributions. Moreover, we discuss similarities and differences between interpretations of the physical core concept of drift-diffusion in the deep learning and model-based world. To this end, we examine relations of probabilistic diffusion to osmosis filters.
    
[^94]: 实验固体力学中机器学习的最新进展与应用：一篇综述

    Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review. (arXiv:2303.07647v1 [cs.LG])

    [http://arxiv.org/abs/2303.07647](http://arxiv.org/abs/2303.07647)

    本文综述了近年来机器学习在实验固体力学领域中的最新进展和应用，提供了物理感知和基于物理学的机器学习方法，并涵盖了断裂力学、生物力学、纳米和微观力学、构建材料和二维材料等传统和新兴领域。

    

    多年来，实验固体力学在表征和理解天然和新材料的力学性质方面发挥了至关重要的作用。机器学习的最新进展为该领域提供了新的机遇，包括实验设计、数据分析、不确定性量化和反问题。由于近年来该新兴领域发表的论文数量迅速增加，因此及时进行全面和更新的综述，对于最近机器学习在实验固体力学中的应用具有重要意义。在本文中，我们首先概述了与该综述相关的常见机器学习算法和术语，重点介绍了基于物理学和物理感知的机器学习方法。然后，我们全面涵盖了实验力学传统和新兴领域中机器学习的最新应用，包括断裂力学、生物力学、纳米和微观力学、构建材料和二维材料。最后，我们强调了当前活跃的研究方向和领域面临的主要挑战，最后讨论了机器学习在实验固体力学未来的潜在机会。

    For many decades, experimental solid mechanics has played a crucial role in characterizing and understanding the mechanical properties of natural and novel materials. Recent advances in machine learning (ML) provide new opportunities for the field, including experimental design, data analysis, uncertainty quantification, and inverse problems. As the number of papers published in recent years in this emerging field is exploding, it is timely to conduct a comprehensive and up-to-date review of recent ML applications in experimental solid mechanics. Here, we first provide an overview of common ML algorithms and terminologies that are pertinent to this review, with emphasis placed on physics-informed and physics-based ML methods. Then, we provide thorough coverage of recent ML applications in traditional and emerging areas of experimental mechanics, including fracture mechanics, biomechanics, nano- and micro-mechanics, architected materials, and 2D material. Finally, we highlight some curr
    
[^95]: 一种基于自监督的心血管事件检测通用实验室进展预训练模型

    Self-supervised based general laboratory progress pretrained model for cardiovascular event detection. (arXiv:2303.06980v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06980](http://arxiv.org/abs/2303.06980)

    研究利用自监督学习和迁移学习，将心血管实验室指标的患者进展趋势从常见情况转移到罕见或特定心血管事件检测中，以协助检测经皮冠状动脉介入治疗患者的靶血管重建。

    

    定期监测是管理心血管疾病的必要方面。由于罕见或特定疾病的患者规模较小，观察也是间歇性的，因此其招募常常受到限制，而常见情况由于定期随访而更容易累积纵向数据。然而，这些数据以其无规律性、时间性、缺席性和稀疏性而闻名。本研究利用自监督学习和迁移学习来克服上述障碍，将心血管实验室指标的患者进展趋势从常见情况转移到罕见或特定心血管事件检测中。我们使用高血压患者（尚未患糖尿病）进行了一般实验室进展（GLP）预训练模型的预训练，并将其实验室进展趋势转移，以协助检测经皮冠状动脉介入治疗患者的靶血管重建（TVR）。GLP采用了两个阶段的训练过程，包括预训练和微调。

    Regular surveillance is an indispensable aspect of managing cardiovascular disorders. Patient recruitment for rare or specific diseases is often limited due to their small patient size and episodic observations, whereas prevalent cases accumulate longitudinal data easily due to regular follow-ups. These data, however, are notorious for their irregularity, temporality, absenteeism, and sparsity. In this study, we leveraged self-supervised learning (SSL) and transfer learning to overcome the above-mentioned barriers, transferring patient progress trends in cardiovascular laboratory parameters from prevalent cases to rare or specific cardiovascular events detection. We pretrained a general laboratory progress (GLP) pretrain model using hypertension patients (who were yet to be diabetic), and transferred their laboratory progress trend to assist in detecting target vessel revascularization (TVR) in percutaneous coronary intervention patients. GLP adopted a two-stage training process that u
    
[^96]: 在物联网系统中通过非独立同分布数据和客户端dropout来稳定和改进联邦学习

    Stabilizing and Improving Federated Learning with Non-IID Data and Client Dropout in IoT Systems. (arXiv:2303.06314v1 [cs.LG])

    [http://arxiv.org/abs/2303.06314](http://arxiv.org/abs/2303.06314)

    本文提出了一个简单而有效的框架，通过引入一个先验校准的softmax函数来计算交叉熵损失和基于原型的特征提取来维护一个平衡的分类器头，以稳定和改进联邦学习。

    This paper proposes a simple yet effective framework to stabilize and improve federated learning by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based feature extraction to maintain a balanced classifier head.

    联邦学习是一种新兴的技术，用于在不暴露私有数据的情况下在分散的客户端上训练深度模型，然而它受到标签分布偏斜的影响，通常导致收敛缓慢和模型性能下降。当参与的客户端处于不稳定的环境并经常掉线时，这个挑战可能更加严重。为了解决这个问题，我们提出了一个简单而有效的框架，通过引入一个先验校准的softmax函数来计算交叉熵损失和基于原型的特征提取来维护一个平衡的分类器头。

    Federated learning is an emerging technique for training deep models over decentralized clients without exposing private data, which however suffers from label distribution skew and usually results in slow convergence and degraded model performance. This challenge could be more serious when the participating clients are in unstable circumstances and dropout frequently. Previous work and our empirical observations demonstrate that the classifier head for classification task is more sensitive to label skew and the unstable performance of FedAvg mainly lies in the imbalanced training samples across different classes. The biased classifier head will also impact the learning of feature representations. Therefore, maintaining a balanced classifier head is of significant importance for building a better global model. To tackle this issue, we propose a simple yet effective framework by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based fe
    
[^97]: 可解释的异常值汇总

    Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])

    [http://arxiv.org/abs/2303.06261](http://arxiv.org/abs/2303.06261)

    STAIR提出了一种可解释的异常值汇总方法，通过学习一组紧凑的人类可理解规则，以汇总和解释异常检测结果，具有强大的可解释性，以准确地总结检测结果。

    STAIR proposes an interpretable outlier summarization method by learning a compact set of human understandable rules to summarize and explain the anomaly detection results, which has strong interpretability to accurately summarize the detection results.

    异常值检测在实际应用中是至关重要的，以防止金融欺诈、防御网络入侵或检测即将发生的设备故障。为了减少人力评估异常值检测结果的工作量，并有效地将异常值转化为可操作的见解，用户通常希望系统自动产生可解释的异常值检测结果的子组的汇总。然而，到目前为止，没有这样的系统存在。为了填补这一空白，我们提出了STAIR，它学习了一组紧凑的人类可理解规则，以汇总和解释异常检测结果。STAIR不使用经典的决策树算法来产生这些规则，而是提出了一个新的优化目标，以产生少量规则，具有最小的复杂性，因此具有强大的可解释性，以准确地总结检测结果。STAIR的学习算法通过迭代分割大规则来产生规则集，并在每个i中最大化这个目标，是最优的。

    Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each i
    
[^98]: 元学习控制变量：有限数据中方差缩减的方法

    Meta-learning Control Variates: Variance Reduction with Limited Data. (arXiv:2303.04756v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2303.04756](http://arxiv.org/abs/2303.04756)

    该论文提出了一种元学习控制变量的方法，可在有限数据的情况下减小蒙特卡罗估计器的方差，并对多个任务进行处理。

    

    控制变量是减小蒙特卡罗估计器方差的有力工具，但在样本数量较小的情况下构建有效的控制变量可能具有挑战性。本文表明，当需要计算大量相关积分时，即使每个任务的样本数很少，也可以利用这些积分任务之间的相似性来提高性能。我们所提出的元学习CV（Meta-CVs）方法可用于处理数百个或数千个任务，并通过实证评估表明，在这种情况下，Meta-CVs可以显著减小方差。我们的理论分析确定了Meta-CVs成功训练的一般条件。

    Control variates can be a powerful tool to reduce the variance of Monte Carlo estimators, but constructing effective control variates can be challenging when the number of samples is small. In this paper, we show that when a large number of related integrals need to be computed, it is possible to leverage the similarity between these integration tasks to improve performance even when the number of samples per task is very small. Our approach, called meta learning CVs (Meta-CVs), can be used for up to hundreds or thousands of tasks. Our empirical assessment indicates that Meta-CVs can lead to significant variance reduction in such settings, and our theoretical analysis establishes general conditions under which Meta-CVs can be successfully trained.
    
[^99]: 带有双向先验模型的向量量化时间序列生成

    Vector Quantized Time Series Generation with a Bidirectional Prior Model. (arXiv:2303.04743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04743](http://arxiv.org/abs/2303.04743)

    本文提出了一种新的时间序列生成方法，使用向量量化技术和双向变压器模型来生成质量更好、模块化变化更快的合成信号。

    

    时间序列生成研究主要集中在使用生成对抗网络（GAN）与递归神经网络（RNN）变体相结合。然而，训练 GAN 的基本限制和挑战仍然存在。此外，RNN族通常在远程时间步之间的时间一致性方面存在困难。受到图像生成领域成功的启发，我们提出 TimeVQVAE，这是我们所知道的第一个使用向量量化（VQ）技术解决 TSG 问题的工作。此外，离散潜在空间的先验使用双向变压器模型进行学习，可以更好地捕捉全局时间一致性。我们还提出在时间 - 频率域中进行 VQ 建模，分为低频（LF）和高频（HF）。这使我们能够保留时间序列的重要特征，并生成质量更好、模块性变化更快的新合成信号。

    Time series generation (TSG) studies have mainly focused on the use of Generative Adversarial Networks (GANs) combined with recurrent neural network (RNN) variants. However, the fundamental limitations and challenges of training GANs still remain. In addition, the RNN-family typically has difficulties with temporal consistency between distant timesteps. Motivated by the successes in the image generation (IMG) domain, we propose TimeVQVAE, the first work, to our knowledge, that uses vector quantization (VQ) techniques to address the TSG problem. Moreover, the priors of the discrete latent spaces are learned with bidirectional transformer models that can better capture global temporal consistency. We also propose VQ modeling in a time-frequency domain, separated into low-frequency (LF) and high-frequency (HF). This allows us to retain important characteristics of the time series and, in turn, generate new synthetic signals that are of better quality, with sharper changes in modularity, t
    
[^100]: 基于正样本未标注学习的提高非法节点检测方法

    Towards Improved Illicit Node Detection with Positive-Unlabelled Learning. (arXiv:2303.02462v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02462](http://arxiv.org/abs/2303.02462)

    本文提出基于正样本未标注学习的改进非法节点检测方法，通过探讨隐藏正样本的标签机制假设，与一系列图表示学习方法相结合，实现了更可靠的结果。

    

    在区块链网络上检测非法节点是加强未来监管的一项有价值的任务。最近提出了基于机器学习的方法来解决这一任务，这些方法使用了一些区块链交易数据集，其中有一小部分标记为正样本，其余未标记（PU）。虽然一些研究使用了随机样本假定未标记节点是正常节点，但我们认为值得考虑隐藏正样本标签的标签机制假设及其对评估指标的影响。我们进一步探讨了处理潜在隐藏正样本的PU分类器与常规机器学习模型相比可以具有改进的性能。我们测试了PU分类器与一系列图表示学习方法结合使用，以获得相同数据的不同特征分布，以获得更可靠的结果。

    Detecting illicit nodes on blockchain networks is a valuable task for strengthening future regulation. Recent machine learning-based methods proposed to tackle the tasks are using some blockchain transaction datasets with a small portion of samples labeled positive and the rest unlabelled (PU). Albeit the assumption that a random sample of unlabeled nodes are normal nodes is used in some works, we discuss that the label mechanism assumption for the hidden positive labels and its effect on the evaluation metrics is worth considering. We further explore that PU classifiers dealing with potential hidden positive labels can have improved performance compared to regular machine learning models. We test the PU classifiers with a list of graph representation learning methods for obtaining different feature distributions for the same data to have more reliable results.
    
[^101]: 自主水下车体设计的约束贝叶斯优化

    Constrained Bayesian Optimization for Automatic Underwater Vehicle Hull Design. (arXiv:2302.14732v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.14732](http://arxiv.org/abs/2302.14732)

    本文研究了自主水下载体的优化设计问题，通过集成FreeCAD和OpenFoam等工具进行自动化设计评估，并采用贝叶斯优化算法解决了优化中样本效率的问题。

    

    自主水下车体设计优化是一个复杂的工程过程，旨在满足给定要求生成具有优化特性的UUV载体。首先，它涉及集成相关的复杂工程仿真工具。其次，它需要将样本有效的优化框架与集成工具链相结合。为此，我们将称为FreeCAD的CAD工具与CFD工具openFoam集成，以进行自动化设计评估。我们选择了贝叶斯优化（BO）进行优化，这是一种为优化耗时昂贵的工程模拟而开发的众所周知的技术，在多种问题中已证明具有非常高的样本效率，包括超参数调整和实验设计。在优化过程中，我们可以将不可行设计作为约束集成到优化过程中。通过将领域专用工具链与基于AI的优化相结合，我们执行了自动设计优化。

    Automatic underwater vehicle hull Design optimization is a complex engineering process for generating a UUV hull with optimized properties on a given requirement. First, it involves the integration of involved computationally complex engineering simulation tools. Second, it needs integration of a sample efficient optimization framework with the integrated toolchain. To this end, we integrated the CAD tool called FreeCAD with CFD tool openFoam for automatic design evaluation. For optimization, we chose Bayesian optimization (BO), which is a well-known technique developed for optimizing time-consuming expensive engineering simulations and has proven to be very sample efficient in a variety of problems, including hyper-parameter tuning and experimental design. During the optimization process, we can handle infeasible design as constraints integrated into the optimization process. By integrating domain-specific toolchain with AI-based optimization, we executed the automatic design optimiza
    
[^102]: 基于梯度内存的加速联邦学习方法GradMA并缓解灾难性遗忘问题

    GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting. (arXiv:2302.14307v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.14307](http://arxiv.org/abs/2302.14307)

    提出了一种名为GradMA的方法，基于梯度内存加速联邦学习，并通过持续学习来解决灾难性遗忘问题，实验表明在模型准确性、通信效率和灾难性遗忘缓解方面都有显著提高。

    

    联邦学习是一种新兴的机器学习领域，受到了社区的广泛关注。然而，因数据异质性和部分参与导致的灾难性遗忘给联邦学习带来了巨大的挑战，影响了性能。为了解决这些问题，我们提出了一种新的联邦学习方法（名为GradMA），它从持续学习中获得启示，同时纠正服务器端和工作端的更新方向，并充分利用服务器的丰富计算和内存资源。此外，我们还阐述了一种内存缩减策略，使GradMA能够适应具有大量工作者的联邦学习。然后，我们在光滑非凸环境下理论分析了GradMA的收敛性，并展示了其收敛速度随着采样活跃工作者数量的增加而实现线性加速。最后，我们在各种图像分类任务上进行了广泛的实验，结果显示，与几种最先进的联邦学习方法相比，GradMA在模型准确性、通信效率和灾难性遗忘缓解方面都取得了显著的改进。

    Federated Learning (FL) has emerged as a de facto machine learning area and received rapid increasing research interests from the community. However, catastrophic forgetting caused by data heterogeneity and partial participation poses distinctive challenges for FL, which are detrimental to the performance. To tackle the problems, we propose a new FL approach (namely GradMA), which takes inspiration from continual learning to simultaneously correct the server-side and worker-side update directions as well as take full advantage of server's rich computing and memory resources. Furthermore, we elaborate a memory reduction strategy to enable GradMA to accommodate FL with a large scale of workers. We then analyze convergence of GradMA theoretically under the smooth non-convex setting and show that its convergence rate achieves a linear speed up w.r.t the increasing number of sampled active workers. At last, our extensive experiments on various image classification tasks show that GradMA ach
    
[^103]: 带有Fisher线性判别分析的近似最优领域自适应

    Approximately optimal domain adaptation with Fisher's Linear Discriminant Analysis. (arXiv:2302.14186v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.14186](http://arxiv.org/abs/2302.14186)

    本文提出了一种基于Fisher线性判别的领域自适应模型，该模型是两个假设的凸组合，可以在不访问任何单个源任务的直接信息的情况下计算最优分类器，并在基于EEG和ECG的分类设置中展示了其有效性。

    

    我们提出了一类基于Fisher线性判别（FLD）的模型，用于领域自适应。该类模型是两个假设的凸组合：i）代表先前看到的源任务的平均假设和ii）在新的目标任务上训练的假设。对于特定的生成设置，我们在0-1损失下导出了两种模型的最优凸组合，提出了一种可计算的逼近，并研究了各种参数设置对最优假设、假设i）和假设ii）之间相对风险的影响。我们展示了所提出的最优分类器在基于EEG和ECG的分类设置中的有效性，并认为可以在不访问任何单个源任务的直接信息的情况下计算最优分类器。最后我们讨论了进一步的应用、限制和可能的未来方向。

    We propose a class of models based on Fisher's Linear Discriminant (FLD) in the context of domain adaptation. The class is the convex combination of two hypotheses: i) an average hypothesis representing previously seen source tasks and ii) a hypothesis trained on a new target task. For a particular generative setting we derive the optimal convex combination of the two models under 0-1 loss, propose a computable approximation, and study the effect of various parameter settings on the relative risks between the optimal hypothesis, hypothesis i), and hypothesis ii). We demonstrate the effectiveness of the proposed optimal classifier in the context of EEG- and ECG-based classification settings and argue that the optimal classifier can be computed without access to direct information from any of the individual source tasks. We conclude by discussing further applications, limitations, and possible future directions.
    
[^104]: FTM: 一种用于时间图表示学习的帧级时间线建模方法

    FTM: A Frame-level Timeline Modeling Method for Temporal Graph Representation Learning. (arXiv:2302.11814v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11814](http://arxiv.org/abs/2302.11814)

    FTM 是一种帧级时间线建模方法，可同时捕获时间图中的短期和长期特征，并在不同任务和数据集上表现出比其他现有方法更好的性能。

    

    学习图形结构数据的表示对于图形分析任务至关重要。尽管静态图取得了显着进展，但对于临时图的研究仍处于初步阶段。时间图表示学习方法的 bottleneck 是邻域聚合策略，基于此图属性明确分享和收集信息。现有的邻域聚合策略无法捕获时间图属性的短期特征或长期特征，导致模型性能不容乐观，甚至影响表示学习方法的鲁棒性和领域通用性。为解决这个问题，我们提出了一种帧级时间线建模（FTM）方法，有助于捕获短期和长期特征，从而在时间图上学习更有信息的表示。具体来说，我们提出了一种新颖的基于链接的框架技术来保留短期特征，然后将其集成到时间线建模框架中以捕获长期模式。实验结果表明，我们提出的 FTM 方法在三个不同任务和两个基准数据集上优于其他最先进的方法。

    Learning representations for graph-structured data is essential for graph analytical tasks. While remarkable progress has been made on static graphs, researches on temporal graphs are still in its beginning stage. The bottleneck of the temporal graph representation learning approach is the neighborhood aggregation strategy, based on which graph attributes share and gather information explicitly. Existing neighborhood aggregation strategies fail to capture either the short-term features or the long-term features of temporal graph attributes, leading to unsatisfactory model performance and even poor robustness and domain generality of the representation learning method. To address this problem, we propose a Frame-level Timeline Modeling (FTM) method that helps to capture both short-term and long-term features and thus learns more informative representations on temporal graphs. In particular, we present a novel link-based framing technique to preserve the short-term features and then inco
    
[^105]: FiTs:细粒度两阶段训练用于知识感知问答

    FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering. (arXiv:2302.11799v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.11799](http://arxiv.org/abs/2302.11799)

    本文提出了一个Fine-grained Two-stage训练框架（FiTs），用于解决知识感知问答（KAQA）中，从语言模型和知识图谱中获得的两种不同类型的知识在表示上的差异和联合推理的困难问题。

    

    知识感知问答（KAQA）需要模型在知识库中回答问题，这对于开放域QA和特定领域QA都是必要的，尤其是当语言模型无法提供所需的所有知识时。最近KAQA系统融合了从预训练语言模型（PLM）和知识图谱（KG）中获得的语言知识和事实知识以回答复杂问题，取得了令人鼓舞的结果，但是存在困难，即有效地融合来自PLMs和KGs的表示，因为（i）它们之间存在语义和分布差异，以及（ii）难以联合推理提供的两类知识。针对上述两个问题，我们提出了一个Fine-grained Two-stage训练框架（FiTs），旨在提高KAQA系统的性能。第一阶段旨在通过知识适应后训练来对齐来自PLM和KG的表示，从而弥合它们之间的模态差距。

    Knowledge-aware question answering (KAQA) requires the model to answer questions over a knowledge base, which is essential for both open-domain QA and domain-specific QA, especially when language models alone cannot provide all the knowledge needed. Despite the promising result of recent KAQA systems which tend to integrate linguistic knowledge from pre-trained language models (PLM) and factual knowledge from knowledge graphs (KG) to answer complex questions, a bottleneck exists in effectively fusing the representations from PLMs and KGs because of (i) the semantic and distributional gaps between them, and (ii) the difficulties in joint reasoning over the provided knowledge from both modalities. To address the above two problems, we propose a Fine-grained Two-stage training framework (FiTs) to boost the KAQA system performance: The first stage aims at aligning representations from the PLM and the KG, thus bridging the modality gaps between them, named knowledge adaptive post-training. 
    
[^106]: DrasCLR: 一个用于学习疾病相关和解剖特异性表示的自监督框架，适用于三维医学图像

    DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images. (arXiv:2302.10390v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10390](http://arxiv.org/abs/2302.10390)

    DrasCLR是一个自监督框架，通过提出两种领域特定的对比学习策略来学习疾病相关和解剖特异性表示，特别是解决了区分疾病模式和解剖特征的挑战。

    

    由于大规模带注释的体积医学图像的获取成本高昂，因此自监督学习（SSL）为许多下游任务提供了一种有前途的预训练和特征提取解决方案，因为它仅使用未标记数据。最近，基于实例区分的SSL方法在医学成像领域变得流行。然而，SSL预训练编码器可能使用许多图像线索来区分一个实例，而这些线索不一定与疾病相关。此外，病理模式常常很微妙和异质，需要所需方法能够表示对不同身体部位中的异常变化敏感的解剖特异性特征。本文提出了一个名为DrasCLR的新的SSL框架，用于三维医学成像，以克服这些挑战。我们提出了两种领域特定的对比学习策略：一种旨在捕捉局部解剖区域内微妙疾病模式，另一种旨在识别具有不同挑战性的解剖区域间的疾病相关特征。

    Large-scale volumetric medical images with annotation are rare, costly, and time prohibitive to acquire. Self-supervised learning (SSL) offers a promising pre-training and feature extraction solution for many downstream tasks, as it only uses unlabeled data. Recently, SSL methods based on instance discrimination have gained popularity in the medical imaging domain. However, SSL pre-trained encoders may use many clues in the image to discriminate an instance that are not necessarily disease-related. Moreover, pathological patterns are often subtle and heterogeneous, requiring the ability of the desired method to represent anatomy-specific features that are sensitive to abnormal changes in different body parts. In this work, we present a novel SSL framework, named DrasCLR, for 3D medical imaging to overcome these challenges. We propose two domain-specific contrastive learning strategies: one aims to capture subtle disease patterns inside a local anatomical region, and the other aims to r
    
[^107]: 核岭回归下伪标签的协变量转移策略

    Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift. (arXiv:2302.10160v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.10160](http://arxiv.org/abs/2302.10160)

    该论文提出了一种关于核岭回归的协变量转移策略，通过使用伪标签进行模型选择，能够适应不同特征分布下的学习，实现均方误差最小化。

    

    我们提出并分析了一种基于协变量转移的核岭回归方法。我们的目标是在目标分布上学习一个均方误差最小的回归函数，基于从目标分布采样的未标记数据和可能具有不同特征分布的已标记数据。我们将已标记数据分成两个子集，并分别进行核岭回归，以获得候选模型集合和一个填充模型。我们使用后者填充缺失的标签，然后相应地选择最佳的候选模型。我们的非渐近性过量风险界表明，在相当一般的情况下，我们的估计器能够适应目标分布以及协变量转移的结构。它能够实现渐近正态误差率直到对数因子的最小极限优化。在模型选择中使用伪标签不会产生主要负面影响。

    We develop and analyze a principled approach to kernel ridge regression under covariate shift. The goal is to learn a regression function with small mean squared error over a target distribution, based on unlabeled data from there and labeled data that may have a different feature distribution. We propose to split the labeled data into two subsets and conduct kernel ridge regression on them separately to obtain a collection of candidate models and an imputation model. We use the latter to fill the missing labels and then select the best candidate model accordingly. Our non-asymptotic excess risk bounds show that in quite general scenarios, our estimator adapts to the structure of the target distribution as well as the covariate shift. It achieves the minimax optimal error rate up to a logarithmic factor. The use of pseudo-labels in model selection does not have major negative impacts.
    
[^108]: 理解多模式对比学习及整合非配对数据

    Understanding Multimodal Contrastive Learning and Incorporating Unpaired Data. (arXiv:2302.06232v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06232](http://arxiv.org/abs/2302.06232)

    本论文研究了一般类的非线性损失函数进行多模式对比学习，揭示了其与奇异值分解的联系。并证明在错误匹配的情况下，多模式对比学习可以比单模式对比学习表现更佳，表现了其对嘈杂数据的鲁棒性。

    

    最近，语言监督的视觉模型在计算机视觉领域引起了广泛关注。构建这种模型的常见方法是使用对比学习方法在两种模态之间对配对数据进行学习，例如对比语言 - 图像预训练（CLIP）。在这篇论文中，我们在线性表示设置下，（i）启动了一个关于多模式对比学习（MMCL）的一般非线性Loss函数的调查，包括CLIP Loss，并展示了它与奇异值分解（SVD）之间的联系。即，我们展示了梯度下降每一步Loss最小化可以被视为对一个对比性协方差矩阵进行SVD。基于这个洞察，（ii）我们分析了MMCL的性能。我们定量地表明，在错误匹配的情况下，MMCL的特征学习能力可以比单模式对比学习应用于每种模式更好。这表征了MMCL对嘈杂数据的鲁棒性。

    Language-supervised vision models have recently attracted great attention in computer vision. A common approach to build such models is to use contrastive learning on paired data across the two modalities, as exemplified by Contrastive Language-Image Pre-Training (CLIP). In this paper, under linear representation settings, (i) we initiate the investigation of a general class of nonlinear loss functions for multimodal contrastive learning (MMCL) including CLIP loss and show its connection to singular value decomposition (SVD). Namely, we show that each step of loss minimization by gradient descent can be seen as performing SVD on a contrastive cross-covariance matrix. Based on this insight, (ii) we analyze the performance of MMCL. We quantitatively show that the feature learning ability of MMCL can be better than that of unimodal contrastive learning applied to each modality even under the presence of wrongly matched pairs. This characterizes the robustness of MMCL to noisy data. Furthe
    
[^109]: 针对帕金森病治疗的闭环深度脑电刺激控制器的离线学习

    Offline Learning of Closed-Loop Deep Brain Stimulation Controllers for Parkinson Disease Treatment. (arXiv:2302.02477v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02477](http://arxiv.org/abs/2302.02477)

    本研究针对DBS治疗帕金森氏症的问题，提出了一种基于离线强化学习框架的闭环深度脑电刺激控制器，以动态调整治疗幅度，减少能量使用，并检测其安全性和性能。

    

    深度脑电刺激（DBS）通过向脑的基底神经节区域传递电脉冲，显示出治疗帕金森氏症（PD）引起的运动症状的巨大潜力。然而，得到美国食品和药物管理局（FDA）批准的DBS仅能以固定幅度提供持续DBS（cDBS）刺激；这种能量低效的操作降低了设备的电池寿命，不能根据活动情况动态调整治疗，可能会引起显著的副作用（如步态障碍）。在本文中，我们介绍了一种离线强化学习（RL）框架，允许利用过去的临床数据来训练一个RL策略，从而在实时调整刺激幅度的同时，以减少能量使用，同时保持与cDBS相同水平的治疗效力（即控制）。此外，临床协议要求在将这种RL控制器部署到患者之前，需证明其安全性和性能。因此，我们还引入了基于模拟技术的安全检测流程。

    Deep brain stimulation (DBS) has shown great promise toward treating motor symptoms caused by Parkinson's disease (PD), by delivering electrical pulses to the Basal Ganglia (BG) region of the brain. However, DBS devices approved by the U.S. Food and Drug Administration (FDA) can only deliver continuous DBS (cDBS) stimuli at a fixed amplitude; this energy inefficient operation reduces battery lifetime of the device, cannot adapt treatment dynamically for activity, and may cause significant side-effects (e.g., gait impairment). In this work, we introduce an offline reinforcement learning (RL) framework, allowing the use of past clinical data to train an RL policy to adjust the stimulation amplitude in real time, with the goal of reducing energy use while maintaining the same level of treatment (i.e., control) efficacy as cDBS. Moreover, clinical protocols require the safety and performance of such RL controllers to be demonstrated ahead of deployments in patients. Thus, we also introduce
    
[^110]: 测量编程语言分布的影响

    Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01973](http://arxiv.org/abs/2302.01973)

    该研究提出了BabelCode框架和Translating Python Programming Puzzles（TP3）基准测试，探讨了平衡训练数据集中14种编程语言分布的影响。结果显示平衡分布有助于大型语言模型在低资源语言上的性能提升。

    

    目前用于评估神经代码模型的基准测试只集中在很少的一部分编程语言上，不包括许多流行的语言，例如Go或Rust。为了解决这个问题，我们提出了BabelCode框架，用于基于执行的评估任何语言中的任何基准测试。BabelCode使得可以对模型的内存、运行时间和单个测试案例结果进行新的定性性能调查。此外，我们还提供了一个名为Translating Python Programming Puzzles（TP3）的新代码翻译数据集，该数据集来自Python Programming Puzzles（Schuster等人，2021）基准测试，涉及将专家级Python函数翻译成任何语言。通过对BabelCode和TP3基准测试的研究，我们探讨了在训练数据集中平衡14种语言的分布是否可以提高大型语言模型在低资源语言上的性能。在平衡语料库上训练模型，平均而言，相对于不平衡分布的情况，所有任务和语言的$pass@k$结果平均提高了12.34%。

    Current benchmarks for evaluating neural code models focus on only a small subset of programming languages, excluding many popular languages such as Go or Rust. To ameliorate this issue, we present the BabelCode framework for execution-based evaluation of any benchmark in any language. BabelCode enables new investigations into the qualitative performance of models' memory, runtime, and individual test case results. Additionally, we present a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming Puzzles (Schuster et al. 2021) benchmark that involves translating expert-level python functions to any language. With both BabelCode and the TP3 benchmark, we investigate if balancing the distributions of 14 languages in a training dataset improves a large language model's performance on low-resource languages. Training a model on a balanced corpus results in, on average, 12.34% higher $pass@k$ across all tasks and languages compared to the
    
[^111]: 鲁棒的在线主动学习策略

    Robust online active learning. (arXiv:2302.00422v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00422](http://arxiv.org/abs/2302.00422)

    本文提出了一种自适应方法，用于鲁棒的在线主动学习，并在受污染的数据流中证明了其性能表现优异，同时确保了稳定性并减少异常值的负面影响。

    

    在许多工业应用中，获得标记的观测数据并不简单，通常需要人工专家干预或使用昂贵的测试设备。在这种情况下，主动学习可以大大提高拟合模型时最信息数据点的建议。减少模型开发所需的观测数据数量可以减轻训练所需的计算负担和标记相关的操作支出。特别是在线主动学习，在需要在极短时间内决定是否获取数据点标记的高容量生产过程中非常有用。然而，尽管最近致力于开发在线主动学习策略，但在存在异常值的情况下这些方法的行为仍未得到彻底研究。在这项工作中，我们调查了在线主动线性回归在受污染的数据流中的性能，并提出了一种自适应方法，用于鲁棒的在线主动学习，同时保证稳定性并减少异常值的负面影响。

    In many industrial applications, obtaining labeled observations is not straightforward as it often requires the intervention of human experts or the use of expensive testing equipment. In these circumstances, active learning can be highly beneficial in suggesting the most informative data points to be used when fitting a model. Reducing the number of observations needed for model development alleviates both the computational burden required for training and the operational expenses related to labeling. Online active learning, in particular, is useful in high-volume production processes where the decision about the acquisition of the label for a data point needs to be taken within an extremely short time frame. However, despite the recent efforts to develop online active learning strategies, the behavior of these methods in the presence of outliers has not been thoroughly examined. In this work, we investigate the performance of online active linear regression in contaminated data strea
    
[^112]: 对比与聚类：学习邻域对表示用于无源域自适应

    Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation. (arXiv:2301.13428v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13428](http://arxiv.org/abs/2301.13428)

    本文提出了一个无源域适应的实用且具有挑战性的方案，通过在原始特征空间聚类，构建真正困难的负对，结合噪声对比估计理论，学习一个域不变的特征来解决域上的差异问题，能够在三个常见的基准数据集上实现有效结果。

    

    无监督域适应利用来自不同分布的源数据解决从未标记目标域分类数据的问题。然而，传统方法需要访问源数据，这经常引起数据隐私方面的担忧。本文中，我们考虑了一个更加实际但充满挑战的设置，在这个设置中，源域数据不可用，目标域数据未标记。具体而言，我们从对比学习的角度解决了域间差异问题。我们的工作的关键思想是通过以下方式学习一个域不变的特征:1)在原始特征空间中直接执行具有最近邻居的聚类；2)通过扩展邻居构造真正困难的负对，而不引入额外的计算复杂度；3)结合噪声对比估计理论以获得计算优势。我们在三个常见的基准数据集VisDA、Office-Home和Office-31上进行了仔细的消融研究和广泛的实验。

    Unsupervised domain adaptation uses source data from different distributions to solve the problem of classifying data from unlabeled target domains. However, conventional methods require access to source data, which often raise concerns about data privacy. In this paper, we consider a more practical but challenging setting where the source domain data is unavailable and the target domain data is unlabeled. Specifically, we address the domain discrepancy problem from the perspective of contrastive learning. The key idea of our work is to learn a domain-invariant feature by 1) performing clustering directly in the original feature space with nearest neighbors; 2) constructing truly hard negative pairs by extended neighbors without introducing additional computational complexity; and 3) combining noise-contrastive estimation theory to gain computational advantage. We conduct careful ablation studies and extensive experiments on three common benchmarks: VisDA, Office-Home, and Office-31. T
    
[^113]: 灵活双工网络中资源管理的图神经网络方法

    Flex-Net: A Graph Neural Network Approach to Resource Management in Flexible Duplex Networks. (arXiv:2301.11166v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2301.11166](http://arxiv.org/abs/2301.11166)

    本文提出了一种图神经网络模型—Flex-Net，用于解决灵活双工网络中资源管理问题，通过联合优化通信方向和传输功率，实现了高性能和低计算复杂度。

    

    灵活双工网络允许用户在没有静态时间调度的情况下动态使用上行和下行信道，从而有效利用网络资源。本文研究了灵活双工网络的总速率最大化问题。特别地，我们考虑了具有一对一通信链接的网络。相应的组合优化问题是非确定多项式（NP）难题，没有闭合形式的解法。因此，现有的启发式算法在大型网络中存在可扩展性问题，计算复杂度很高。受近年来图神经网络（GNN）在解决NP难的无线资源管理问题方面的成功所启发，我们提出了一种新的GNN架构，名为Flex-Net，以联合优化通信方向和传输功率。所提出的GNN同时保持了低计算复杂度和接近最优性能，相较于最常用的技术而言。此外，我们的数值结果表明，在各种策略中，Flex-Net的平均性能最好。

    Flexible duplex networks allow users to dynamically employ uplink and downlink channels without static time scheduling, thereby utilizing the network resources efficiently. This work investigates the sum-rate maximization of flexible duplex networks. In particular, we consider a network with pairwise-fixed communication links. Corresponding combinatorial optimization is a non-deterministic polynomial (NP)-hard without a closed-form solution. In this respect, the existing heuristics entail high computational complexity, raising a scalability issue in large networks. Motivated by the recent success of Graph Neural Networks (GNNs) in solving NP-hard wireless resource management problems, we propose a novel GNN architecture, named Flex-Net, to jointly optimize the communication direction and transmission power. The proposed GNN produces near-optimal performance meanwhile maintaining a low computational complexity compared to the most commonly used techniques. Furthermore, our numerical res
    
[^114]: SoftMatch：解决半监督学习中的数量-质量权衡问题

    SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning. (arXiv:2301.10921v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10921](http://arxiv.org/abs/2301.10921)

    本文提出了SoftMatch，通过在训练中保持高数量和高质量的伪标签来克服半监督学习中数量-质量权衡问题，有效地利用未标注的数据。 在实验中，SoftMatch在图像、文本和影片等多个基准测试中都显示了实质性的改进。

    

    半监督学习的关键挑战是如何有效利用有限的标注数据和大量的未标注数据提高模型的泛化性能。本文首先通过统一的样本加权公式重新审视了流行的伪标签方法，并演示了伪标签阈值法固有的数量-质量权衡问题，可能会阻碍学习。为此，我们提出了SoftMatch，通过在训练期间保持伪标签的高数量和高质量来克服这种权衡，有效地利用未标注的数据。我们推导出一个截断的高斯函数来根据样本的置信度对样本进行加权，这可以看作是置信度阈值的软版本。我们进一步通过提出统一的对齐方法来增强对弱学习类的利用。在实验中，SoftMatch在包括图像、文本和影片等各种基准测试中显示出了实质性的改进。

    The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and im
    
[^115]: 深度学习驱动的任务导向通信中的信息时代

    Age of Information in Deep Learning-Driven Task-Oriented Communications. (arXiv:2301.04298v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2301.04298](http://arxiv.org/abs/2301.04298)

    本文研究了在任务导向通信中通过编码器-解码器对来执行任务的信息时代问题，通过信道利用率的增加可以提高准确性，但需要更长的服务时间，引入任务信息的最大时代（PAoTI）来对准确性和延迟进行权衡。

    

    本文研究了任务导向通信中的信息时代概念，旨在利用发射机处的数据在接收器处执行任务。发射机和接收机之间的操作被建模为一个联合训练的编码器-解码器对，并考虑了信道效应。编码器将数据样本转换为小维度的特征向量，并使用少量的信道用于传输，从而减少了传输次数和延迟。解码器不再重构输入样本，而是对接收到的信号执行任务，例如分类。通过在MNIST和CIFAR-10图像数据集上应用不同的编码器-解码器深度神经网络，显示分类器的准确性随着信道利用率的增加而提高，但需要更长的服务时间。引入任务信息的最大时代（PAoTI）来分析准确性和延迟权衡的时机，当信息年龄增长时，除非接收到的信号被正确分类。通过结合通道和源的影响对PAoTI进行了分析，结果表明PAoTI与通道的影响趋势一致，但概述误差可能增加。

    This paper studies the notion of age in task-oriented communications that aims to execute a task at a receiver utilizing the data at its transmitter. The transmitter-receiver operations are modeled as an encoder-decoder pair that is jointly trained while considering channel effects. The encoder converts data samples into feature vectors of small dimension and transmits them with a small number of channel uses thereby reducing the number of transmissions and latency. Instead of reconstructing input samples, the decoder performs a task, e.g., classification, on the received signals. Applying different deep neural networks of encoder-decoder pairs on MNIST and CIFAR-10 image datasets, the classifier accuracy is shown to increase with the number of channel uses at the expense of longer service time. The peak age of task information (PAoTI) is introduced to analyze this accuracy-latency tradeoff when the age grows unless a received signal is classified correctly. By incorporating channel an
    
[^116]: 多模式歌词-节奏匹配

    Multimodal Lyrics-Rhythm Matching. (arXiv:2301.02732v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2301.02732](http://arxiv.org/abs/2301.02732)

    本文提出了一种多模式歌词-节奏匹配方法，采用音频而不是有可用元数据的谱面，可将歌词和音乐的关键部分相互匹配，包括音乐的强节拍、歌词音节和歌词关键词，并在数据集实验中表现出优越性能。

    

    尽管近年来人工智能在音乐领域的研究增加了，但关于歌词和节奏这两个关键部分的主要相关性，如关键词、重音音节和强节拍等，不经常被研究。这可能是由于音频错位、音节识别的不准确性以及跨学科知识需求等挑战。为了解决这一不足，我们在本文中提出了一种新颖的多模式歌词-节奏匹配方法，其特别是在没有任何语言限制的情况下，将歌词和音乐的关键部分相互匹配。我们使用音频而不是有可用元数据的谱面，这增加了我们的方法的应用灵活性，但也带来了更多的挑战。此外，我们的方法创造性地生成了几种包含各种多模态的模式，包括音乐的强节拍、歌词音节、歌手发音的听觉变化以及尤其是歌词关键词，它们通常指示一首歌的中心信息。我们的实验表明，我们提出的方法在一个包含100首歌的数据集上，它们有歌词注释和MIDI文件，表现出优于现有方法的效果。

    Despite the recent increase in research on artificial intelligence for music, prominent correlations between key components of lyrics and rhythm such as keywords, stressed syllables, and strong beats are not frequently studied. This is likely due to challenges such as audio misalignment, inaccuracies in syllabic identification, and most importantly, the need for cross-disciplinary knowledge. To address this lack of research, we propose a novel multimodal lyrics-rhythm matching approach in this paper that specifically matches key components of lyrics and music with each other without any language limitations. We use audio instead of sheet music with readily available metadata, which creates more challenges yet increases the application flexibility of our method. Furthermore, our approach creatively generates several patterns involving various multimodalities, including music strong beats, lyrical syllables, auditory changes in a singer's pronunciation, and especially lyrical keywords, w
    
[^117]: 使用半监督自编码器的在线主动学习进行软测量开发

    Online Active Learning for Soft Sensor Development using Semi-Supervised Autoencoders. (arXiv:2212.13067v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13067](http://arxiv.org/abs/2212.13067)

    本文介绍了一种使用半监督自编码器以及在线主动学习方法，以尽可能少的标记样本来开发软测量传感器，从而显著降低了成本。在实验中，作者表明这种方法能够取得好的预测效果。

    

    数据驱动的软测量在工业和化学过程中被广泛使用，以预测难以测量的过程变量。这些传感器使用的回归模型通常需要大量标记的样本，然而，由于质量检查需要高昂的时间和成本，获取标签信息可能非常昂贵。在这种情况下，主动学习方法可能非常有益，因为它们可以建议查询最具信息量的标签。然而，为回归提出的大多数主动学习策略都集中在离线场景。本文将其中一些方法适应于流式场景，并展示了如何使用基于正交自编码器的半监督架构学习低维空间中的显著特征。我们也演示了如何使用田纳西东曼过程比较预测结果。

    Data-driven soft sensors are extensively used in industrial and chemical processes to predict hard-to-measure process variables whose real value is difficult to track during routine operations. The regression models used by these sensors often require a large number of labeled examples, yet obtaining the label information can be very expensive given the high time and cost required by quality inspections. In this context, active learning methods can be highly beneficial as they can suggest the most informative labels to query. However, most of the active learning strategies proposed for regression focus on the offline setting. In this work, we adapt some of these approaches to the stream-based scenario and show how they can be used to select the most informative data points. We also demonstrate how to use a semi-supervised architecture based on orthogonal autoencoders to learn salient features in a lower dimensional space. The Tennessee Eastman Process is used to compare the predictive 
    
[^118]: 无交叠策略学习：悲观和广义经验Bernstein不等式

    Policy learning "without'' overlap: Pessimism and generalized empirical Bernstein's inequality. (arXiv:2212.09900v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.09900](http://arxiv.org/abs/2212.09900)

    本文提出了一种新的离线策略学习算法，它不需要统一交叠假设，而是利用价值的下限置信区间（LCBs）优化策略，因此能够适应允许行为策略演变和倾向性减弱的情况。

    

    本文研究了离线策略学习，旨在利用先前收集到的观测（来自于固定的或是适应演变的行为策略）来学习给定类别中的最优个性化决策规则。现有的策略学习方法依赖于一个统一交叠假设，即离线数据集中探索所有个性化特征的所有动作的倾向性下界。换句话说，这些方法的性能取决于离线数据集中最坏的倾向性。由于数据收集过程不受控制，在许多情况下，这种假设可能不太现实，特别是当允许行为策略随时间演变并且倾向性减弱时。为此，本文提出了一种新的算法，它优化策略价值的下限置信区间（LCBs）——而不是点估计。LCBs通过量化增强倒数倾向权重的估计不确定性来构建。

    This paper studies offline policy learning, which aims at utilizing observations collected a priori (from either fixed or adaptively evolving behavior policies) to learn the optimal individualized decision rule in a given class. Existing policy learning methods rely on a uniform overlap assumption, i.e., the propensities of exploring all actions for all individual characteristics are lower bounded in the offline dataset. In other words, the performance of these methods depends on the worst-case propensity in the offline dataset. As one has no control over the data collection process, this assumption can be unrealistic in many situations, especially when the behavior policies are allowed to evolve over time with diminishing propensities.  In this paper, we propose a new algorithm that optimizes lower confidence bounds (LCBs) -- instead of point estimates -- of the policy values. The LCBs are constructed by quantifying the estimation uncertainty of the augmented inverse propensity weight
    
[^119]: 基于基础模型反馈的策略适应

    Policy Adaptation from Foundation Model Feedback. (arXiv:2212.07398v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07398](http://arxiv.org/abs/2212.07398)

    本文提出了基于基础模型反馈的策略适应（PAFF）方法，通过让策略使用随机生成的指令进行演示，并利用预训练的基础模型提供反馈来重新标记演示，自动提供新的演示-指令数据对进行策略微调，以实现机器人操作的泛化。实验结果表明，PAFF优于现有最先进的方法。

    

    最近在视觉-语言基础模型方面的进展为构建通用机器人带来了显著进步。通过使用预训练模型将场景和指令编码为决策输入，指令条件化策略可以在不同的对象和任务之间进行泛化。尽管这是令人鼓舞的，但策略在遇到未见过的任务或环境时仍然失败。在本工作中，我们提出了一种基于基础模型反馈的策略适应（PAFF）。当将训练好的策略部署到新任务或新环境时，我们首先让策略使用随机生成的指令进行演示。虽然执行可能出现错误，但我们可以利用预训练的基础模型提供反馈来重新标记演示。这自动为策略微调提供了新的演示-指令数据对。我们在机器人操作设置中进行了各种实验的评估，重点是在未见过的对象、任务和未观察到的环境中的泛化。我们的实验结果表明，PAFF在最终任务成功率和训练效率方面优于现有最先进的方法。

    Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. In this work, we propose Policy Adaptation from Foundation model Feedback (PAFF). When deploying the trained policy to a new task or a new environment, we first let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unse
    
[^120]: FretNet：连续值音高轮廓流技术在多声部吉他谱转录中的应用

    FretNet: Continuous-Valued Pitch Contour Streaming for Polyphonic Guitar Tablature Transcription. (arXiv:2212.03023v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2212.03023](http://arxiv.org/abs/2212.03023)

    FretNet是一种新的自动化吉他谱转录方法，提供了连续值音高轮廓流和各种吉他技巧的估计，通过在多个数据集上测试，结果表明它比其他方法具有更高的平均精度和更少的模型复杂度。

    

    近年来，自动音乐转录（AMT）的任务受到了越来越多的关注，其中包括从音频中估计各种音符属性。同时，多音高估计（MPE）相关任务仍然是几乎所有AMT方法的具有挑战性但必要的组成部分，即使只是隐含地。在AMT上下文中，音高信息通常被量化为西方音乐音阶的名义音高。即使在更一般的情况下，MPE系统通常也会产生具有某种量化程度的音高预测。在GT转录等某些AMT应用中，估计连续值音高轮廓更有意义。吉他谱有能力表示各种演奏技巧，其中一些涉及音高调制。目前的AMT方法没有充分解决音高调制问题，并且只提供更少的量化方法以换取更多的模型复杂性。本文介绍了一种名为FretNet的新方法，它可以实现连续值音高轮廓流，并估计在各个时间点上的音高、延音和滑音等吉他技巧。我们在多个数据集上评估了FretNet，结果表明，它比当前最先进的方法提供更高的平均精度和更少的模型复杂度。在各种GT任务中，包括转换民谣吉他、流行吉他和电台吉他等吉他类型，FretNet都表现出色，这表明了FretNet在真实世界应用中的潜力。

    In recent years, the task of Automatic Music Transcription (AMT), whereby various attributes of music notes are estimated from audio, has received increasing attention. At the same time, the related task of Multi-Pitch Estimation (MPE) remains a challenging but necessary component of almost all AMT approaches, even if only implicitly. In the context of AMT, pitch information is typically quantized to the nominal pitches of the Western music scale. Even in more general contexts, MPE systems typically produce pitch predictions with some degree of quantization. In certain applications of AMT, such as Guitar Tablature Transcription (GTT), it is more meaningful to estimate continuous-valued pitch contours. Guitar tablature has the capacity to represent various playing techniques, some of which involve pitch modulation. Contemporary approaches to AMT do not adequately address pitch modulation, and offer only less quantization at the expense of more model complexity. In this paper, we present
    
[^121]: 多模态医学数据分析的异构图学习

    Heterogeneous Graph Learning for Multi-modal Medical Data Analysis. (arXiv:2211.15158v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.15158](http://arxiv.org/abs/2211.15158)

    该论文提出了一种名为HetMed的异构图学习框架，用于融合多模态医学数据，以提高临床决策的准确性。

    

    病人的常规临床访问不仅会产生图像数据，还会包含有关病人的临床信息等非图像数据，即医学数据的多模态性质。这样的异构模态提供了不同和互补的病人视角，当它们被正确地组合时，可以导致更准确的临床决策。然而，尽管其重要性，如何将多模态医学数据有效地融合到统一框架中却受到了相对较少的关注。本文提出了一种名为HetMed（多模态医学数据分析的异构图学习）的有效图形框架，用于融合多模态医学数据。具体而言，我们构建一个包括多种病人非图像特征的多重网络，以系统化方式捕获病人之间的复杂关系，从而导致更准确的临床决策。大量实验结果表明，与最先进的方法相比，我们的方法具有更优越性。

    Routine clinical visits of a patient produce not only image data, but also non-image data containing clinical information regarding the patient, i.e., medical data is multi-modal in nature. Such heterogeneous modalities offer different and complementary perspectives on the same patient, resulting in more accurate clinical decisions when they are properly combined. However, despite its significance, how to effectively fuse the multi-modal medical data into a unified framework has received relatively little attention. In this paper, we propose an effective graph-based framework called HetMed (Heterogeneous Graph Learning for Multi-modal Medical Data Analysis) for fusing the multi-modal medical data. Specifically, we construct a multiplex network that incorporates multiple types of non-image features of patients to capture the complex relationship between patients in a systematic way, which leads to more accurate clinical decisions. Extensive experiments on various real-world datasets dem
    
[^122]: 受生物启示的人类运动序列的持续学习模型

    Biologically-Inspired Continual Learning of Human Motion Sequences. (arXiv:2211.05231v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.05231](http://arxiv.org/abs/2211.05231)

    本文提出了一个受生物启示的条件时间变分自动编码器(BI-CTVAE)模型，通过持续学习生成(CL2Gen)场景，可以对不同类别的运动序列进行生成，并在一组任务上得到较高的生成准确性和分类准确性。

    

    本文提出了一种持续学习模型，专注于涉及时间序列——具体而言，是人的运动。该模型改进了最近提出的类似于大脑的重放模型(BI-R)，它建立了一个受生物启示的条件时间变分自动编码器(BI-CTVAE)，其实例化了一个高斯函数混合体来表示类别。我们研究了一种新颖的持续学习生成(CL2Gen)场景，其中模型生成不同类别的运动序列。模型的生成准确性在一组任务上进行了测试。在按顺序学习所有动作类别之后，BI-CTVAE在一个人类运动数据集上的最终分类准确性达到了78％，比不使用重放高63％，比最先进的离线训练GRU模型低5.4％。

    This work proposes a model for continual learning on tasks involving temporal sequences, specifically, human motions. It improves on a recently proposed brain-inspired replay model (BI-R) by building a biologically-inspired conditional temporal variational autoencoder (BI-CTVAE), which instantiates a latent mixture-of-Gaussians for class representation. We investigate a novel continual-learning-to-generate (CL2Gen) scenario where the model generates motion sequences of different classes. The generative accuracy of the model is tested over a set of tasks. The final classification accuracy of BI-CTVAE on a human motion dataset after sequentially learning all action classes is 78%, which is 63% higher than using no-replay, and only 5.4% lower than a state-of-the-art offline trained GRU model.
    
[^123]: 基于在线连续学习的边缘计算高效压缩比估计

    Efficient Compressed Ratio Estimation using Online Sequential Learning for Edge Computing. (arXiv:2211.04284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04284](http://arxiv.org/abs/2211.04284)

    本研究提出了一种基于AC-OSELM的高效RL方法，能够在边缘设备上估计合适的压缩比，从而实现数据的高效压缩，同时保持重构数据的准确性。

    

    随着物联网的广泛应用，大量的传感器信息正在实时采集。因此，从边缘设备传输数据的通讯成本不断增加。压缩感知（CS）是一种可用于边缘设备的数据压缩方法，并因其可节省通讯成本而备受关注。在压缩感知中，估计合适的压缩比是重要的。现有的利用强化学习（RL）自适应估计获得数据的压缩比的方法，其计算成本常常很高。本研究开发了一种针对边缘设备的高效RL方法，称为actor-critic在线连续极限学习机（AC-OSELM），并利用AC-OSELM开发了一种在边缘设备上通过估计适当的压缩比压缩数据的系统。通过使用实际传感器数据进行实验，评估了所提出方法在估计压缩比和重构压缩数据方面的性能。结果表明我们提出的方法在保持重构压缩数据的准确性的同时，提供了比强化学习和传统方法更高效的压缩比估计方法。

    Owing to the widespread adoption of the Internet of Things, a vast amount of sensor information is being acquired in real time. Accordingly, the communication cost of data from edge devices is increasing. Compressed sensing (CS), a data compression method that can be used on edge devices, has been attracting attention as a method to reduce communication costs. In CS, estimating the appropriate compression ratio is important. There is a method to adaptively estimate the compression ratio for the acquired data using reinforcement learning (RL). However, the computational costs associated with existing RL methods that can be utilized on edges are often high. In this study, we developed an efficient RL method for edge devices, referred to as the actor--critic online sequential extreme learning machine (AC-OSELM), and a system to compress data by estimating an appropriate compression ratio on the edge using AC-OSELM. The performance of the proposed method in estimating the compression ratio
    
[^124]: 深度学习在暴风雨预测中的应用：一种多危险数据融合模型

    Thunderstorm nowcasting with deep learning: a multi-hazard data fusion model. (arXiv:2211.01001v2 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2211.01001](http://arxiv.org/abs/2211.01001)

    这项研究提出了一种深度学习模型，可以用于适应不同类型的暴风雨预测，利用多种数据源进行数据融合，并能预测雷电、冰雹和暴雨的概率，其中天气雷达产品是最重要的预测因素。

    

    在多个领域中，如第一反应者、基础设施管理和航空等领域，都需要预测与雷暴相关的危险。为了解决这个需求，我们提出了一个可以适应不同危险类型的深度学习模型。该模型可以利用多个数据源；我们使用了来自天气雷达、闪电探测、卫星可见/红外图像、数值天气预报和数字高程模型的数据。我们展示了该模型预测闪电、冰雹和暴雨的能力，用1 km分辨率网格和5分钟时间分辨率，预测超过60分钟。Shapley值量化了不同数据来源的重要性，显示出天气雷达产品是所有三种危险类型的最重要预测因素。

    Predictions of thunderstorm-related hazards are needed in several sectors, including first responders, infrastructure management and aviation. To address this need, we present a deep learning model that can be adapted to different hazard types. The model can utilize multiple data sources; we use data from weather radar, lightning detection, satellite visible/infrared imagery, numerical weather prediction and digital elevation models. We demonstrate the ability of the model to predict lightning, hail and heavy precipitation probabilistically on a 1 km resolution grid, with a temporal resolution of 5 min and lead times up to 60 min. Shapley values quantify the importance of the different data sources, showing that the weather radar products are the most important predictors for all three hazard types.
    
[^125]: 循环神经网络和贝叶斯滤波器的通用逼近性

    Recurrent Neural Networks and Universal Approximation of Bayesian Filters. (arXiv:2211.00335v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.00335](http://arxiv.org/abs/2211.00335)

    本文提出一个循环神经网络框架，用于直接从观测输入到所需的估计器统计量学习递归映射，可以近似估计潜在时间序列信号的条件统计量，在非紧致域中有误差界限，在长时间上有良好性能。

    

    本文考虑贝叶斯最优滤波问题，即从观测序列中估计潜在时间序列信号的条件统计量。传统方法通常依赖于假定或估计的转移和观测模型。相反，我们制定了一个通用的循环神经网络框架，并试图直接从观测输入到所需的估计器统计量学习递归映射。本文的重点是此框架的逼近能力。我们提供了一般非紧致域的滤波逼近误差界限。我们还考虑了强时间一致的逼近误差界限，保证良好的长期性能。我们讨论和说明了这些结果的许多实际关注点和影响。

    We consider the Bayesian optimal filtering problem: i.e. estimating some conditional statistics of a latent time-series signal from an observation sequence. Classical approaches often rely on the use of assumed or estimated transition and observation models. Instead, we formulate a generic recurrent neural network framework and seek to learn directly a recursive mapping from observational inputs to the desired estimator statistics. The main focus of this article is the approximation capabilities of this framework. We provide approximation error bounds for filtering in general non-compact domains. We also consider strong time-uniform approximation error bounds that guarantee good long-time performance. We discuss and illustrate a number of practical concerns and implications of these results.
    
[^126]: 训练神经网络用于时序变点检测

    Training Neural Networks for Sequential Change-point Detection. (arXiv:2210.17312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17312](http://arxiv.org/abs/2210.17312)

    本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。

    

    检测数据流中的突变分布转换，即所谓的变点检测，是统计学和机器学习中的一个基本问题。我们引入了一种新颖的方法，使用神经网络进行在线变点检测。具体而言，我们的方法是训练神经网络来逐步计算检测统计量的累积和，当发生变点时，该量会显著变化。我们使用合成和真实世界数据证明了所提出的方法在检测变点方面的优越性和潜力。

    Detecting an abrupt distributional shift of a data stream, known as change-point detection, is a fundamental problem in statistics and machine learning. We introduce a novel approach for online change-point detection using neural networks. To be specific, our approach is training neural networks to compute the cumulative sum of a detection statistic sequentially, which exhibits a significant change when a change-point occurs. We demonstrated the superiority and potential of the proposed method in detecting change-point using both synthetic and real-world data.
    
[^127]: 基于输入梯度传递的神经架构相似性研究

    Similarity of Neural Architectures Based on Input Gradient Transferability. (arXiv:2210.11407v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.11407](http://arxiv.org/abs/2210.11407)

    本研究利用对抗攻击传递度量，设计了一个量化且可扩展的神经架构相似度函数，分析了69个最先进的ImageNet分类器，发现多样化的神经架构可以提高模型集合和知识蒸馏的性能。

    

    近年来，为图像分类而开发了大量的深度神经架构，这些模型是否相似或不同，以及什么因素影响它们的相似性或不同尚未得到充分的研究。本文旨在设计一个量化且可扩展的神经架构相似度函数以回答这个问题。我们利用对抗攻击传递度量，该度量具有与输入梯度和决策边界相关的信息，被广泛用于理解模型行为。我们使用所提出的相似度函数对69个最先进的ImageNet分类器进行了大规模分析，从而回答了这个问题。此外，我们观察到与神经架构相关的现象，即模型多样性可以在特定条件下对模型集合和知识蒸馏的性能有所提升。我们的结果为为什么开发具有不同组件的多样化神经架构是必要的提供了见解。

    In recent years, a huge amount of deep neural architectures have been developed for image classification. It remains curious whether these models are similar or different and what factors contribute to their similarities or differences. To address this question, we aim to design a quantitative and scalable similarity function between neural architectures. We utilize adversarial attack transferability, which has information related to input gradients and decision boundaries that are widely used to understand model behaviors. We conduct a large-scale analysis on 69 state-of-the-art ImageNet classifiers using our proposed similarity function to answer the question. Moreover, we observe neural architecture-related phenomena using model similarity that model diversity can lead to better performance on model ensembles and knowledge distillation under specific conditions. Our results provide insights into why the development of diverse neural architectures with distinct components is necessar
    
[^128]: 利用自监督三维表示的视觉强化学习

    Visual Reinforcement Learning with Self-Supervised 3D Representations. (arXiv:2210.07241v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07241](http://arxiv.org/abs/2210.07241)

    这篇论文提出了一个自监督学习三维表示的方法来解决视觉强化学习中的样本效率和泛化问题，通过预训练和微调两个阶段，相比于二维表示学习方法，该方法在操作任务中具有更高的样本效率并且更好地泛化到其他情况下的RL。

    

    视觉强化学习（RL）的一个重要方法是使用自监督方法学习内部状态表示，这具有通过额外学习信号和归纳偏差提高样本效率和泛化能力的潜在好处。然而，虽然真实世界本质上是三维的，但之前的研究主要集中于利用二维计算机视觉技术作为辅助自监督。在这项工作中，我们提出了一个自监督学习三维表示用于运动控制的统一框架。我们提出的框架包括两个阶段：预训练阶段，其中对深度体素三维自编码器进行预训练，并使用大型目标为中心的数据集进行，以及微调阶段，在该阶段，表示与RL一起在领域内数据上进行微调。我们在实验中表明，与二维表示学习方法相比，我们的方法在模拟操作任务中具有更高的样本效率。 此外，我们学习的策略使其与传统二维表示学习方法相比，可以更好地泛化到其他情况下的RL。

    A prominent approach to visual Reinforcement Learning (RL) is to learn an internal state representation using self-supervised methods, which has the potential benefit of improved sample-efficiency and generalization through additional learning signal and inductive biases. However, while the real world is inherently 3D, prior efforts have largely been focused on leveraging 2D computer vision techniques as auxiliary self-supervision. In this work, we present a unified framework for self-supervised learning of 3D representations for motor control. Our proposed framework consists of two phases: a pretraining phase where a deep voxel-based 3D autoencoder is pretrained on a large object-centric dataset, and a finetuning phase where the representation is jointly finetuned together with RL on in-domain data. We empirically show that our method enjoys improved sample efficiency in simulated manipulation tasks compared to 2D representation learning methods. Additionally, our learned policies tra
    
[^129]: MAPL: 基于参数效率的单模态预训练模型在视觉-语言少样本任务中的适应

    MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting. (arXiv:2210.07179v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.07179](http://arxiv.org/abs/2210.07179)

    MAPL使用对齐的图像-文本数据学习单模态模型表示空间之间的轻量级映射，从而实现了面向视觉-语言少样本任务的基于参数效率的适应，并在测试中显示出优越的性能表现。

    

    在单模态视觉和语言任务中，大型预训练模型已经被证明是出色的零样本和（基于提示的）少样本学习器。我们提出了MAPL，一种简单且参数效率高的方法，它重用冻结的单模态预训练模型，并利用它们在多模态视觉-语言（VL）场景中的强大泛化能力。MAPL使用对齐的图像-文本数据学习了单模态模型表示空间之间的轻量级映射，并且可以从仅有少量上下文示例就推广到看不见的VL任务。MAPL的可训练参数数量很少，使得它在低数据和域内学习方面非常有效。此外，MAPL的模块化使得可以轻松扩展到其他预训练模型。在几个视觉问答和图像标题生成基准测试上的大量实验证明，MAPL相对于类似方法在训练少得多的参数时实现了优越或有竞争力的性能。MAPL可以在几小时内使用适度的计算资源进行训练。

    Large pre-trained models have proved to be remarkable zero- and (prompt-based) few-shot learners in unimodal vision and language tasks. We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings. MAPL learns a lightweight mapping between the representation spaces of unimodal models using aligned image-text data, and can generalize to unseen VL tasks from just a few in-context examples. The small number of trainable parameters makes MAPL effective at low-data and in-domain learning. Moreover, MAPL's modularity enables easy extension to other pre-trained models. Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters. MAPL can be trained in just a few hours using modest comp
    
[^130]: 异常检测的零假设检验

    Null Hypothesis Test for Anomaly Detection. (arXiv:2210.02226v3 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2210.02226](http://arxiv.org/abs/2210.02226)

    本论文提出一种基于零假设检验的异常检测方法，能够排除仅背景假设，依赖于特征和区域的条件独立性假设，展现了出色的性能，适用于各种信号分数情况下的数据检测。

    

    我们扩展了使用无标签分类进行异常检测，并设计了一种排除仅背景假设的假设检验。通过测试两个区域的差异数据的统计独立性，我们能够在不依赖固定异常得分阈值或在区域之间外推背景估计的情况下，排除仅背景假设。该方法依赖于异常得分特征和数据集区域的条件独立性假设，这可以通过现有的去相关技术来保证。作为基准例子，我们考虑了LHC奥林匹克数据集，并展示了互信息表示了一种适用于统计独立性检验的方法，我们的方法在不同信号分数下展现了出色且稳健的性能，即使在存在实际特征相关性的情况下。

    We extend the use of Classification Without Labels for anomaly detection with a hypothesis test designed to exclude the background-only hypothesis. By testing for statistical independence of the two discriminating dataset regions, we are able to exclude the background-only hypothesis without relying on fixed anomaly score cuts or extrapolations of background estimates between regions. The method relies on the assumption of conditional independence of anomaly score features and dataset regions, which can be ensured using existing decorrelation techniques. As a benchmark example, we consider the LHC Olympics dataset where we show that mutual information represents a suitable test for statistical independence and our method exhibits excellent and robust performance at different signal fractions even in presence of realistic feature correlations.
    
[^131]: 学习最小违反连续控制以实现不可行线性时态逻辑规范

    Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications. (arXiv:2210.01162v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.01162](http://arxiv.org/abs/2210.01162)

    本文提出了一个模型自由框架，使用深度强化学习来实现复杂高级任务的目标驱动导航。通过将先前的多目标DRL问题转化为一个单一目标问题，并使用基于采样的路径规划算法来指导DRL智能体，该方法可以满足不可行的线性时态逻辑任务并尽可能减少违规。

    

    本文研究了连续时间控制综合，以实现线性时态逻辑(LTL)表达的复杂高级任务的目标驱动导航。我们提出了一个模型自由框架，使用深度强化学习(DRL)，其中底层动态系统未知（透明盒子）。与先前的工作不同，本文考虑了给定的LTL规范可能是不可行的情况，因此无法全局完成。我们不修改给定的LTL公式，而是提供了一个通用的DRL方法，以最小违规满足它。为了做到这一点，我们将先前的多目标DRL问题转化为一个单一目标问题，该问题要求同时实现自动机满足和最小违规代价。通过使用基于采样的路径规划算法来指导可能不可行的LTL任务的DRL智能体，所提出的方法减轻了DRL的近视倾向，这在学习可以具有长或无限持续时间的一般LTL任务时经常是一个问题。

    This paper explores continuous-time control synthesis for target-driven navigation to satisfy complex high-level tasks expressed as linear temporal logic (LTL). We propose a model-free framework using deep reinforcement learning (DRL) where the underlying dynamic system is unknown (an opaque box). Unlike prior work, this paper considers scenarios where the given LTL specification might be infeasible and therefore cannot be accomplished globally. Instead of modifying the given LTL formula, we provide a general DRL-based approach to satisfy it with minimal violation. To do this, we transform a previously multi-objective DRL problem, which requires simultaneous automata satisfaction and minimum violation cost, into a single objective. By guiding the DRL agent with a sampling-based path planning algorithm for the potentially infeasible LTL task, the proposed approach mitigates the myopic tendencies of DRL, which are often an issue when learning general LTL tasks that can have long or infin
    
[^132]: 论二层优化问题的稳定性及其泛化性分析

    On Stability and Generalization of Bilevel Optimization Problem. (arXiv:2210.01063v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01063](http://arxiv.org/abs/2210.01063)

    本文对二层优化问题的一阶（基于梯度的）方法进行了全面泛化分析，建立了算法稳定性与泛化误差之间的基本联系，并提出了高概率泛化界，将其从$ \bigO(\sqrt{n}) $改善为$ \bigO(\log n) $，同时也提出了第一个泛化界限。

    

    （随机）二层优化问题是机器学习中经常遇到的问题，具有元学习、超参数优化和强化学习等广泛应用。现有研究大多关注于分析该问题的收敛性或提高收敛速度，但很少有研究专注于理解其泛化行为。本文针对二层优化问题的一阶（基于梯度的）方法进行了全面的泛化分析。首先在不同形式上建立了算法稳定性与泛化误差之间的基本联系，并给出了高概率泛化界，将其从$ \bigO(\sqrt{n}) $改善为$ \bigO(\log n) $，其中$ n $是样本量。其次，对于参数持续更新的内部层与外部层通用情况，我们提出了第一个稳定性界限，而现有的工作仅适用于特殊情况。

    (Stochastic) bilevel optimization is a frequently encountered problem in machine learning with a wide range of applications such as meta-learning, hyper-parameter optimization, and reinforcement learning. Most of the existing studies on this problem only focused on analyzing the convergence or improving the convergence rate, while little effort has been devoted to understanding its generalization behaviors. In this paper, we conduct a thorough analysis on the generalization of first-order (gradient-based) methods for the bilevel optimization problem. We first establish a fundamental connection between algorithmic stability and generalization error in different forms and give a high probability generalization bound which improves the previous best one from $\bigO(\sqrt{n})$ to $\bigO(\log n)$, where $n$ is the sample size. We then provide the first stability bounds for the general case where both inner and outer level parameters are subject to continuous update, while existing work allo
    
[^133]: 基于梯度门控机制的图深度多速率学习

    Gradient Gating for Deep Multi-Rate Learning on Graphs. (arXiv:2210.00513v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00513](http://arxiv.org/abs/2210.00513)

    G2是一种利用梯度门控机制的新型GNN框架，可缓解过度平滑问题，并实现了各种图学习任务上的最先进性能。

    

    我们提出了一种名为 Gradient Gating (G$^2$) 的新型框架，旨在改善图神经网络 (GNNs) 的性能。我们的框架基于对 GNN 层的输出进行门控，其中包含了一种跨本质图节点的消息传递信息的多速率流机制。本地梯度被利用来进一步调制消息传递的更新。我们的框架可以灵活地允许使用任何基本的 GNN 层作为包装器，以构建多速率梯度门控机制。我们严格证明 G$^2$ 缓解了过度平滑问题，并允许设计深度 GNNs。我们展示了实证结果，证明所提出的框架在各种图学习任务上实现了最先进的性能，包括大规模异质图上的任务。

    We present Gradient Gating (G$^2$), a novel framework for improving the performance of Graph Neural Networks (GNNs). Our framework is based on gating the output of GNN layers with a mechanism for multi-rate flow of message passing information across nodes of the underlying graph. Local gradients are harnessed to further modulate message passing updates. Our framework flexibly allows one to use any basic GNN layer as a wrapper around which the multi-rate gradient gating mechanism is built. We rigorously prove that G$^2$ alleviates the oversmoothing problem and allows the design of deep GNNs. Empirical results are presented to demonstrate that the proposed framework achieves state-of-the-art performance on a variety of graph learning tasks, including on large-scale heterophilic graphs.
    
[^134]: 神经网络中的泛化：综述与分析

    Generalization in Neural Networks: A Broad Survey. (arXiv:2209.01610v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.01610](http://arxiv.org/abs/2209.01610)

    这篇论文总结了神经网络模型中不同抽象层次上的泛化问题及其方法，其中样本泛化已经取得了进展，但未来需要重点关注减少过拟合；分布泛化与领域泛化有相似之处，领域泛化方法可以应用于困难的样本或分布泛化。

    

    本文综述了神经网络模型中不同抽象层次的概念、建模方法和最近的研究成果，包括样本、分布、领域、任务、模态和范围上的泛化。在样本泛化方面的研究结果显示，在ImageNet数据集的情况下，几乎所有的最新改进都减小了训练误差，而过拟合保持不变；随着几乎所有的训练误差被消除，未来的进展将需要集中关注减少过拟合。从统计学的角度来看，(2)分布泛化可以被看作是样本权重或输入输出关系的变化；因此，在领域泛化成功的技术有可能应用于困难的样本或分布泛化。本文总结了转移学习方法(3)领域泛化的应用，以及最近的进展和丰富的应用领域。

    This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Results on (1) sample generalization show that, in the case of ImageNet, nearly all the recent improvements reduced training error while overfitting stayed flat; with nearly all the training error eliminated, future progress will require a focus on reducing overfitting. Perspectives from statistics highlight how (2) distribution generalization can be viewed alternately as a change in sample weights or a change in the input-output relationship; thus, techniques that have been successful in domain generalization have the potential to be applied to difficult forms of sample or distribution generalization. Transfer learning approaches to (3) domain generalization are summarized, as are recent advances and the wealth of domain a
    
[^135]: DreamBooth：针对主题驱动的生成进行文本到图像扩散模型微调

    DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. (arXiv:2208.12242v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.12242](http://arxiv.org/abs/2208.12242)

    DreamBooth是一种针对主题驱动的文本到图像扩散模型个性化方法，通过微调预训练的文本到图像模型，使用样本图像来实现生成逼真图像的独特标识符绑定，使其可以在不同场景中合成该主题的新版逼真图像。

    

    大型文本到图像模型的发展让AI的演变达到了一个显著的飞跃，实现了从给定文本提示中高质量和多样化的图像合成。然而，这些模型缺乏模仿给定参考集中主体出现的外观和在不同上下文中合成它们的新版本的能力。在本项工作中，我们提出了一种用于“个性化”文本到图像扩散模型的新方法。只需输入一些该主题的图像，我们就对预训练的文本到图像模型进行微调，从而使其学会将唯一标识符与该特定主题绑定。一旦该主题被嵌入模型的输出域中，该唯一标识符就可以用于在不同场景中合成主题的新颖逼真图像。通过利用嵌入在模型中的语义先验和新的自治类特定先验保存损失，我们的技术使得在不同场景、姿态、视角和光照条件下合成主题成为可能。

    Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions th
    
[^136]: 多模态表示学习中的遮蔽视觉和语言建模

    Masked Vision and Language Modeling for Multi-modal Representation Learning. (arXiv:2208.02131v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.02131](http://arxiv.org/abs/2208.02131)

    本文提出了联合遮蔽视觉和语言建模，在跨模态对齐方面取得成果，并在百万级别的预训练数据范围内取得了最先进的性能。

    

    本文研究了如何在视觉和语言（V + L）表示学习中使用遮蔽信号建模。我们提出了建立联合遮蔽视觉和语言建模，其中一个模态的遮蔽信号在另一个模态的帮助下进行重建。这是由图像文本配对数据的性质所驱动的，因为图像和文本都传达几乎相同的信息但以不同的格式呈现。一个模态的遮蔽信号重建以另一模态为条件也可以隐式地学习语言标记和图像补丁之间的跨模态对齐。我们在各种V + L任务上进行的实验表明，该方法连同常见的V + L对齐损失，在百万级别的预训练数据范围内取得了最先进的性能。此外，在有限的数据场景中，我们超过了其他竞争对手的表现。

    In this paper, we study how to use masked signal modeling in vision and language (V+L) representation learning. Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, we propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. This is motivated by the nature of image-text paired data that both of the image and the text convey almost the same information but in different formats. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. Our experiments on various V+L tasks show that the proposed method, along with common V+L alignment losses, achieves state-of-the-art performance in the regime of millions of pre-training data. Also, we outperforms the other competitors by a significant margin in limited data scenarios.
    
[^137]: 交互式强化学习中反馈频率对机器人任务的影响的定量化研究

    Quantifying the Effect of Feedback Frequency in Interactive Reinforcement Learning for Robotic Tasks. (arXiv:2207.09845v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2207.09845](http://arxiv.org/abs/2207.09845)

    本文对交互式强化学习中反馈频率对机器人任务的影响进行了定量化研究，结果表明没有单一的理想反馈频率存在，应该根据具体的任务和机器人复杂性进行调整。

    

    强化学习（RL）在机器人控制中被广泛采用。尽管有许多成功案例，但一个重要的持久性问题是数据效率非常低。交互反馈是一种解决方案，已被证明可以大大加速RL。因此，有大量不同的策略，然而这些策略主要是在离散的网格世界和小规模的最优控制场景中测试的。在文献中，对于哪种反馈频率最优或在什么时候反馈最有益并没有共识。为了解决这些差异，我们在具有连续状态和动作空间的机器人任务中分离并量化了反馈频率的影响。实验涵盖了不同复杂度的机械臂的逆运动学学习。我们展示了表面上矛盾的现象在不同的复杂度水平上出现。此外，我们的结果表明，没有单一的理想反馈频率存在。反馈频率应该根据具体的任务和机器人复杂性进行调整。

    Reinforcement learning (RL) has become widely adopted in robot control. Despite many successes, one major persisting problem can be very low data efficiency. One solution is interactive feedback, which has been shown to speed up RL considerably. As a result, there is an abundance of different strategies, which are, however, primarily tested on discrete grid-world and small scale optimal control scenarios. In the literature, there is no consensus about which feedback frequency is optimal or at which time the feedback is most beneficial. To resolve these discrepancies we isolate and quantify the effect of feedback frequency in robotic tasks with continuous state and action spaces. The experiments encompass inverse kinematics learning for robotic manipulator arms of different complexity. We show that seemingly contradictory reported phenomena occur at different complexity levels. Furthermore, our results suggest that no single ideal feedback frequency exists. Rather that feedback frequenc
    
[^138]: 使用基于模型的树和提升方法拟合低阶函数ANOVA模型

    Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models. (arXiv:2207.06950v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.06950](http://arxiv.org/abs/2207.06950)

    本文提出了一种新算法GAMI-Tree，使用基于模型的树以及新的交互过滤方法，可以更好地拟合底层交互，具有更好的预测性能和更高的效率。

    

    低阶函数ANOVA模型已经被机器学习社区重新发现，并称之为内在可解释的机器学习。本文提出了一种新算法GAMI-Tree，类似于EBM，但具有一些趋向更好性能的特性。我们采用模型为基础的树，并融入一种新的交互过滤方法，提高了对底层交互的捕捉。此外，我们的迭代训练方法收敛于具有更好预测性能的模型，并确保相互作用在分层意义上正交于主效应。该算法不需要广泛的调整，并且实现快速高效。我们使用模拟和真实数据集进行比较。

    Low-order functional ANOVA (fANOVA) models have been rediscovered in the machine learning (ML) community under the guise of inherently interpretable machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting functional main effects and second-order interactions. We propose a new algorithm, called GAMI-Tree, that is similar to EBM, but has a number of features that lead to better performance. It uses model-based trees as base learners and incorporates a new interaction filtering method that is better at capturing the underlying interactions. In addition, our iterative training method converges to a model with better predictive performance, and the embedded purification ensures that interactions are hierarchically orthogonal to main effects. The algorithm does not need extensive tuning, and our implementation is fast and efficient. We use simulated and real datasets to compare the performanc
    
[^139]: Betty: 一个用于多层次优化的自动微分库

    Betty: An Automatic Differentiation Library for Multilevel Optimization. (arXiv:2207.02849v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02849](http://arxiv.org/abs/2207.02849)

    本文介绍了一个名为Betty的自动微分库，可用于大规模的梯度优化问题，有效地减少了计算复杂度并提高了可扩展性，在广泛的多层次优化任务中表现出良好性能。

    

    基于梯度的多层次优化(MLO)已成为研究众多问题的框架，包括超参数优化、元学习、神经架构搜索和强化学习等。然而，MLO中的梯度，是通过链式法则组成最佳响应Jacobi矩阵而获得的，具有计算和内存密集等不利因素。本文介绍了一个面向大规模MLO的软件库Betty，从而初步为解决这一问题迈出了一步。在其核心，我们设计了一个新的MLO数据流图，使我们能够(1)为MLO开发高效的自动微分，将计算复杂度从O(d^3)降至O(d^2)，(2)融入系统支持，例如混合精度和数据并行训练，以实现可伸缩性，(3)便于实现任意复杂度的MLO程序，同时允许多样化的算法和系统设计选择的模块化接口。我们通过实验证明，Betty在广泛的MLO任务中都实现了很好的性能，例如超参数优化、元学习和神经架构搜索。

    Gradient-based multilevel optimization (MLO) has gained attention as a framework for studying numerous problems, ranging from hyperparameter optimization and meta-learning to neural architecture search and reinforcement learning. However, gradients in MLO, which are obtained by composing best-response Jacobians via the chain rule, are notoriously difficult to implement and memory/compute intensive. We take an initial step towards closing this gap by introducing Betty, a software library for large-scale MLO. At its core, we devise a novel dataflow graph for MLO, which allows us to (1) develop efficient automatic differentiation for MLO that reduces the computational complexity from O(d^3) to O(d^2), (2) incorporate systems support such as mixed-precision and data-parallel training for scalability, and (3) facilitate implementation of MLO programs of arbitrary complexity while allowing a modular interface for diverse algorithmic and systems design choices. We empirically demonstrate that
    
[^140]: NovelCraft：开放世界中的新颖性检测和发现数据集

    NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds. (arXiv:2206.11736v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.11736](http://arxiv.org/abs/2206.11736)

    NovelCraft数据集提供了开放世界中新颖性检测与发现任务的挑战。在复杂的场景中插入新颖物体的检测需要更好的基准，并发现了控制假阳性时更简单的方法可能比复杂的方法更出色。

    

    为了让人工智能代理在不断变化的环境中成功执行任务，必须能够检测和适应新颖性。然而，视觉新颖性检测研究通常只评估旨在进行对象分类的重复利用数据集（如CIFAR-10），其中图像聚焦于一个明显、居中的对象。需要新的基准来代表在开放世界中导航复杂场景的挑战。我们的新NovelCraft数据集包含完成修改后的Minecraft环境中的跳跳球装配任务的代理所看到的图像和符号世界状态的多模式情节数据。在某些情节中，我们在复杂的3D场景中插入新颖物体，这些物体可能影响游戏玩法并出现在各种大小和位置中。我们的视觉新颖性检测基准发现，控制假阳性时，最好的面积下曲线度量的方法可能会被更简单的替代方法超过。

    In order for artificial agents to successfully perform tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification, where images focus on one distinct, well-centered object. New benchmarks are needed to represent the challenges of navigating the complex scenes of an open world. Our new NovelCraft dataset contains multimodal episodic data of the images and symbolic world-states seen by an agent completing a pogo stick assembly task within a modified Minecraft environment. In some episodes, we insert novel objects within the complex 3D scene that may impact gameplay and appear in a variety of sizes and positions. Our visual novelty detection benchmark finds that methods that rank best on popular area-under-the-curve metrics may be outperformed by simpler alternatives when controlling false positives matters most. 
    
[^141]: 使用耦合调节不平衡损失的分阶段渐进学习来解决不平衡数据分类问题

    Phased Progressive Learning with Coupling-Regulation-Imbalance Loss for Imbalanced Data Classification. (arXiv:2205.12117v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.12117](http://arxiv.org/abs/2205.12117)

    该论文提出了一种分阶段渐进学习的方法和一种耦合调节不平衡损失函数来解决不平衡数据分类问题，该方法特别适用于具有失衡或样本较少的数据集。

    

    当面临数量不平衡和分类困难的数据集时，深度卷积神经网络通常表现不佳。尽管该领域取得了一些进展，但现有的两阶段方法仍然存在数据集偏见或域偏移。为了解决这个问题，提出了一种分阶段逐渐将重点从表示学习转向训练上层分类器的学习计划。对于具有较大失衡或样本较少的数据集，这种方法特别有益。另外，提出了一种耦合调节不平衡损失函数，它将三个部分组合在一起：校正项，Focal 损失和 LDAM 损失。这种损失有效地解决了数量不平衡和离群值问题，同时调节关注具有不同分类困难度的样本。这些方法在多个基准数据集上取得了令人满意的结果，包括 Imbalanced CIFAR10、Imbalanced CIFAR100、ImageNet-LT 和 iNaturalist。

    Deep convolutional neural networks often perform poorly when faced with datasets that suffer from quantity imbalances and classification difficulties. Despite advances in the field, existing two-stage approaches still exhibit dataset bias or domain shift. To counter this, a phased progressive learning schedule has been proposed that gradually shifts the emphasis from representation learning to training the upper classifier. This approach is particularly beneficial for datasets with larger imbalances or fewer samples. Another new method a coupling-regulation-imbalance loss function is proposed, which combines three parts: a correction term, Focal loss, and LDAM loss. This loss is effective in addressing quantity imbalances and outliers, while regulating the focus of attention on samples with varying classification difficulties. These approaches have yielded satisfactory results on several benchmark datasets, including Imbalanced CIFAR10, Imbalanced CIFAR100, ImageNet-LT, and iNaturalist
    
[^142]: 多视角潜变量模型中的领域知识编码:带结构稀疏贝叶斯方法

    Encoding Domain Knowledge in Multi-view Latent Variable Models: A Bayesian Approach with Structured Sparsity. (arXiv:2204.06242v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.06242](http://arxiv.org/abs/2204.06242)

    提出了一种新的基于修改过的马蹄蚌先验的多视角潜变量模型MuVI，用于建模结构稀疏性。它能够纳入有限且噪声的领域知识，以内在可解释的方式分析多视角数据，优于现有结构稀疏性建模方法。

    

    许多现实世界的系统不仅有来自单个数据源的数据，还有来自多个数据视角的数据。例如，在基因组医学中，患者可以通过来自不同分子层面的数据进行描述。利用具有结构稀疏性的潜变量模型是揭示数据视角内和跨视角变化的常用工具。然而，它们的可解释性较差，需要专家直接检查和解释每个要素。在这里，我们提出了MuVI，一种基于修改过的马蹄蚌先验的新型多视角潜变量模型，用于建模结构稀疏性。这有助于对有限的和噪声领域知识进行纳入，从而以内在可解释的方式分析多视角数据。我们证明了我们的模型在重建误差和精确度/召回方面优于现有的结构稀疏性建模方法，并且可以稳健地整合噪声领域专业知识。

    Many real-world systems are described not only by data from a single source but via multiple data views. In genomic medicine, for instance, patients can be characterized by data from different molecular layers. Latent variable models with structured sparsity are a commonly used tool for disentangling variation within and across data views. However, their interpretability is cumbersome since it requires a direct inspection and interpretation of each factor from domain experts. Here, we propose MuVI, a novel multi-view latent variable model based on a modified horseshoe prior for modeling structured sparsity. This facilitates the incorporation of limited and noisy domain knowledge, thereby allowing for an analysis of multi-view data in an inherently explainable manner. We demonstrate that our model (i) outperforms state-of-the-art approaches for modeling structured sparsity in terms of the reconstruction error and the precision/recall, (ii) robustly integrates noisy domain expertise in t
    
[^143]: 使用图神经网络学习具有弹性的无线电资源管理策略

    Learning Resilient Radio Resource Management Policies with Graph Neural Networks. (arXiv:2203.11012v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2203.11012](http://arxiv.org/abs/2203.11012)

    本文提出了一个使用图神经网络学习的具有弹性的无线电资源管理策略，以实现高聚合速率并确保所有用户的公平性，并使用可扩展的置换等变图神经网络（GNN）架构基于瞬时信道条件推导出的图形拓扑来参数化RRM策略

    

    本文考虑在包含多个接入点和一组用户设备的无线干扰网络中，实现用户选择和功率控制，以实现高聚合速率并确保所有用户的公平性。我们通过可学习的松弛变量，将弹性无线电资源管理（RRM）策略优化问题与适应底层网络条件的每个用户最低容量约束相结合。我们在Lagrangian双重域中重新定义问题，并展示了我们可以使用有限的参数集来参数化RRM策略，通过一个经过证实的小偏差的无监督原始-双重方法进行训练。我们使用可扩展的置换等变图神经网络（GNN）架构来基于瞬时信道条件推导出的图形拓扑参数化RRM策略。

    We consider the problems of user selection and power control in wireless interference networks, comprising multiple access points (APs) communicating with a group of user equipment devices (UEs) over a shared wireless medium. To achieve a high aggregate rate, while ensuring fairness across all users, we formulate a resilient radio resource management (RRM) policy optimization problem with per-user minimum-capacity constraints that adapt to the underlying network conditions via learnable slack variables. We reformulate the problem in the Lagrangian dual domain, and show that we can parameterize the RRM policies using a finite set of parameters, which can be trained alongside the slack and dual variables via an unsupervised primal-dual approach thanks to a provably small duality gap. We use a scalable and permutation-equivariant graph neural network (GNN) architecture to parameterize the RRM policies based on a graph topology derived from the instantaneous channel conditions. Through exp
    
[^144]: DCT-Former: 采用离散余弦变换的高效自注意力模型

    DCT-Former: Efficient Self-Attention with Discrete Cosine Transform. (arXiv:2203.01178v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01178](http://arxiv.org/abs/2203.01178)

    本文提出使用离散余弦变换的自注意力模型，有效缓解了点积注意力计算苛刻的内存和时间复杂度限制，具有更小的内存占用和更短的推理时间，表现良好。

    

    自注意力模型是目前自然语言处理和计算机视觉领域中占据统治地位的模型结构之一。然而，由于点积注意力的计算需求成倍增加，时间复杂度高达O(n^2)，这在建模长序列时具有天然的限制。此论文提出一种以离散余弦变换为基础的自注意力近似方法，取得更小的内存占用和更短的推理时间，实验验证表明该方法具有较好性能。

    Since their introduction the Trasformer architectures emerged as the dominating architectures for both natural language processing and, more recently, computer vision applications. An intrinsic limitation of this family of "fully-attentive" architectures arises from the computation of the dot-product attention, which grows both in memory consumption and number of operations as $O(n^2)$ where $n$ stands for the input sequence length, thus limiting the applications that require modeling very long sequences. Several approaches have been proposed so far in the literature to mitigate this issue, with varying degrees of success. Our idea takes inspiration from the world of lossy data compression (such as the JPEG algorithm) to derive an approximation of the attention module by leveraging the properties of the Discrete Cosine Transform. An extensive section of experiments shows that our method takes up less memory for the same performance, while also drastically reducing inference time. This 
    
[^145]: Weisfeiler和Leman来做机器学习了：目前的研究进展。

    Weisfeiler and Leman go Machine Learning: The Story so far. (arXiv:2112.09992v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.09992](http://arxiv.org/abs/2112.09992)

    Weisfeiler-Leman算法被广泛应用于处理图和关系数据。本文全面介绍了该算法在监督学习中的应用，包括理论背景、扩展、与等变神经网格的联系、并列出了当前应用和未来研究方向。

    

    近年来，基于Weisfeiler-Leman算法的算法和神经架构已成为处理图和关系数据的机器学习的强大工具。本文全面介绍算法在机器学习环境中的使用情况，重点关注监督学习。我们讨论了理论背景，展示了如何将其用于监督图形和节点表示学习，讨论了最近的扩展，并概述了算法与（置换）等变神经网格的联系。此外，我们还概述了当前的应用和未来的研究方向以刺激进一步的研究。

    In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, have emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine-learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.
    
[^146]: 强健且可证明单调的神经网络

    Robust and Provably Monotonic Networks. (arXiv:2112.00038v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.00038](http://arxiv.org/abs/2112.00038)

    本论文提出了一种方法，可以约束深度学习模型的利普希茨常数，并使用单调剩余连接使模型的某些输入单调，适用于需要领域知识指导依赖性的场景，如算法公平性要求及物理学中的次原子粒子分类。

    

    神经网络中表示输入和输出空间之间映射的利普希茨常数是评估模型鲁棒性的自然度量。我们提出了一种新方法，用于约束密集深度学习模型的利普希茨常数，该方法也可推广到其他架构。该方法依赖于训练期间的简单权重归一化方案，以确保每个层的利普希茨常数低于分析师指定的上限。然后可以使用简单的单调剩余连接使模型在其任何子集的输入中单调，这在领域知识指导此类依赖性的场景中非常有用，例如在算法公平性要求中，或者像在此处展示的那样，在对CERN大型强子对撞机产生的次原子粒子的衰减进行分类时。我们的归一化方法对架构的约束最小，并允许保持更高的表现力，相比其他技术。

    The Lipschitz constant of the map between the input and output space represented by a neural network is a natural metric for assessing the robustness of the model. We present a new method to constrain the Lipschitz constant of dense deep learning models that can also be generalized to other architectures. The method relies on a simple weight normalization scheme during training that ensures the Lipschitz constant of every layer is below an upper limit specified by the analyst. A simple monotonic residual connection can then be used to make the model monotonic in any subset of its inputs, which is useful in scenarios where domain knowledge dictates such dependence. Examples can be found in algorithmic fairness requirements or, as presented here, in the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider. Our normalization is minimally constraining and allows the underlying architecture to maintain higher expressiveness compared to other techniq
    
[^147]: 对抗训练中的标签噪声：研究鲁棒过度拟合的新视角

    Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting. (arXiv:2110.03135v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03135](http://arxiv.org/abs/2110.03135)

    该论文发现了对抗训练中存在的标签噪声，并解释了其对鲁棒过度拟合的普遍存在以及扰动半径和数据质量的依赖性。通过该论文提出的方法，可以自动校准标签以应对标签噪声和鲁棒过度拟合。

    

    我们展示了在对抗训练中存在标签噪声。这种标签噪声是由于对抗样本的真实标签分布与从干净样本继承的标签之间的不匹配造成的 - 真实标签分布被对抗扰动扭曲，但从干净样本继承标签的常见做法却忽略了这一点。认识到标签噪声有助于洞察对抗训练中鲁棒过度拟合的普遍存在，并解释了其对扰动半径和数据质量的奇特依赖性。此外，我们的标签噪声视角与我们对对抗训练中纪元双下降现象的观察相吻合。在我们的分析指导下，我们提出了一种方法来自动校准标签以应对标签噪声和鲁棒过度拟合。我们的方法在各种模型和数据集上实现了一致的性能提升，而不引入新的超参数或额外的调整。

    We show that label noise exists in adversarial training. Such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples - the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples. Recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. Also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. Guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting. Our method achieves consistent performance improvements across various models and datasets without introducing new hyper-parameters or additional tuning.
    
[^148]: 基于交叉模态潜在表示的多说话人脸向语音模型Facetron

    Facetron: A Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations. (arXiv:2107.12003v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2107.12003](http://arxiv.org/abs/2107.12003)

    本文提出了一种基于交叉模态潜在表示的多说话人脸向语音模型Facetron，可以适用于不同的说话人条件下灵活地生成语音波形，并在客观和主观评估中表现出优越性。

    

    本文提出了一种多说话人脸向语音波形生成模型Facetron，适用于未知说话人条件下。我们使用具有语言和说话人特征的生成对抗网络（GAN）作为辅助条件，直接在端到端的训练框架下将面部图像转换为语音波形。语言特征是使用唇语识别模型从唇部运动中提取的，说话人特征则通过与预训练的声学模型的交叉模态学习从面部图像中预测得出。由于这两个特征是不相关的且独立控制的，因此我们可以灵活地合成语音波形，其说话人特征取决于输入的面部图像。实验结果表明，我们提出的模型在客观和主观评估结果方面比传统方法表现更好。

    In this paper, we propose a multi-speaker face-to-speech waveform generation model that also works for unseen speaker conditions. Using a generative adversarial network (GAN) with linguistic and speaker characteristic features as auxiliary conditions, our method directly converts face images into speech waveforms under an end-to-end training framework. The linguistic features are extracted from lip movements using a lip-reading model, and the speaker characteristic features are predicted from face images using cross-modal learning with a pre-trained acoustic model. Since these two features are uncorrelated and controlled independently, we can flexibly synthesize speech waveforms whose speaker characteristics vary depending on the input face images. We show the superiority of our proposed model over conventional methods in terms of objective and subjective evaluation results. Specifically, we evaluate the performances of linguistic features by measuring their accuracy on an automatic sp
    
[^149]: 用贝叶斯积分对平稳核进行边缘化

    Marginalising over Stationary Kernels with Bayesian Quadrature. (arXiv:2106.07452v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.07452](http://arxiv.org/abs/2106.07452)

    本文提出一种贝叶斯积分方案，用于边缘化高斯过程核族，以获得具有良好校准不确定性估计的灵活模型，比现有方法更高效实用。

    

    对高斯过程核族进行边缘化可以产生具有良好校准不确定性估计的灵活模型。现有方法需要评估许多核的似然性，使它们对于较大的数据集来说变得不切实际。我们提出了一种贝叶斯积分方案，使得这种边缘化更加高效，因此更加实用。通过使用分布之间的最大平均差异，我们定义一种捕捉谱混合（SM）核之间不变性的核。通过推广难以定义的变形贝叶斯积分取得样本核。我们展示了我们的框架比最先进的基线产生更准确的预测，尤其是当给出有限的（墙时钟）时间预算时具有更好的校准不确定性。

    Marginalising over families of Gaussian Process kernels produces flexible model classes with well-calibrated uncertainty estimates. Existing approaches require likelihood evaluations of many kernels, rendering them prohibitively expensive for larger datasets. We propose a Bayesian Quadrature scheme to make this marginalisation more efficient and thereby more practical. Through use of the maximum mean discrepancies between distributions, we define a kernel over kernels that captures invariances between Spectral Mixture (SM) Kernels. Kernel samples are selected by generalising an information-theoretic acquisition function for warped Bayesian Quadrature. We show that our framework achieves more accurate predictions with better calibrated uncertainty than state-of-the-art baselines, especially when given limited (wall-clock) time budgets.
    
[^150]: 自适应熵树搜索的规划与学习

    Planning and Learning Using Adaptive Entropy Tree Search. (arXiv:2102.06808v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2102.06808](http://arxiv.org/abs/2102.06808)

    本论文提出了一种全新的算法ANTS，它将规划和学习结合在最大熵范式中，并通过在Atari基准测试上的实验证明其明显优于当前最先进的AlphaZero系统的规划组件PUCT，具有较强的稳健性，可推动基于树的规划方法的实际应用。

    

    最近的人工智能突破表明，将基于树的规划与深度学习相结合可以实现更高的性能。本论文提出了一种称作自适应熵树搜索（ANTS）的全新算法，将规划和学习结合在最大熵范式中。通过在Atari基准测试上进行全面的实验，我们证明ANTS明显优于目前最先进的AlphaZero系统的规划组件PUCT。ANTS建立在最大熵规划方法的基础之上，然而我们发现这种方法在与学习相结合时表现不佳，ANTS解决了这个问题并达到了与目前最先进算法相当的性能水平。此外，我们还发现ANTS在不同的超参数选择下表现出更强的稳健性。我们相信，ANTS的高性能和稳健性将使基于树的规划方法更加适用于实际应用。

    Recent breakthroughs in Artificial Intelligence have shown that the combination of tree-based planning with deep learning can lead to superior performance. We present Adaptive Entropy Tree Search (ANTS) - a novel algorithm combining planning and learning in the maximum entropy paradigm. Through a comprehensive suite of experiments on the Atari benchmark we show that ANTS significantly outperforms PUCT, the planning component of the state-of-the-art AlphaZero system. ANTS builds upon recent work on maximum entropy planning methods - which however, as we show, fail in combination with learning. ANTS resolves this issue to reach state-of-the-art performance. We further find that ANTS exhibits superior robustness to different hyperparameter choices, compared to the previous algorithms. We believe that the high performance and robustness of ANTS can bring tree search planning one step closer to wide practical adoption.
    
[^151]: ES-ENAS: 高效演化优化大型混合搜索空间

    ES-ENAS: Efficient Evolutionary Optimization for Large Hybrid Search Spaces. (arXiv:2101.07415v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.07415](http://arxiv.org/abs/2101.07415)

    本文提出了ES-ENAS方法，将进化策略和组合优化器结合起来来优化混合搜索空间，并在CIFAR-10上取得了超越最新方法的结果。

    

    本文研究了在大型混合搜索空间（由组合和连续参数组成）上优化黑盒函数的问题。我们证明先前依赖于变异策略的进化算法，在组合空间上具有灵活性，但在高维连续空间上存在维度灾难的问题，从理论和实验上限制了它们在混合搜索空间上的应用。为了解决这个问题，我们提出ES-ENAS，一种简单且模块化的联合优化过程，将效率低下的组合优化器与演化策略相结合，通过启发式的一次性或超级网络范式，实现高度可扩展且直观的优化过程。通过这样做，我们实现了更高的样本效率，在合成基准测试中进行了经验验证，在单个GPU上数小时内就可以优化CIFAR-10的复杂混合搜索空间，并超越了现有的最新方法。

    In this paper, we approach the problem of optimizing blackbox functions over large hybrid search spaces consisting of both combinatorial and continuous parameters. We demonstrate that previous evolutionary algorithms which rely on mutation-based approaches, while flexible over combinatorial spaces, suffer from a curse of dimensionality in high dimensional continuous spaces both theoretically and empirically, which thus limits their scope over hybrid search spaces as well. In order to combat this curse, we propose ES-ENAS, a simple and modular joint optimization procedure combining the class of sample-efficient smoothed gradient techniques, commonly known as Evolutionary Strategies (ES), with combinatorial optimizers in a highly scalable and intuitive way, inspired by the one-shot or supernet paradigm introduced in Efficient Neural Architecture Search (ENAS). By doing so, we achieve significantly more sample efficiency, which we empirically demonstrate over synthetic benchmarks, and are
    
[^152]: 学习 AC-OPF 的负载编码

    Load Encoding for Learning AC-OPF. (arXiv:2101.03973v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2101.03973](http://arxiv.org/abs/2101.03973)

    本文提出了一个新的负载压缩嵌入方案，以解决深度学习技术在学习 AC-OPF 时的可扩展性问题，并在实验中取得了数量级的改进。

    

    AC Optimal Power Flow (AC-OPF) 问题是电力传输系统中的核心构建块。它寻求最经济的有功和无功发电调度以满足需求，同时满足传输操作限制。 近期的研究表明，深度学习技术在提供 AC-OPF 解决方案的精确近似方面具有巨大潜力。 但是，深度学习方法在应用于现实生活中的电力网络时往往会遇到可扩展性问题。 本文针对可扩展性限制进行了研究，并提出了一种负载压缩嵌入方案，使用三步方法减小训练模型的大小。该方法在来自PGLib的大规模测试用例上进行了实验评估，并在训练收敛性和预测准确性方面产生了数量级的改进。

    The AC Optimal Power Flow (AC-OPF) problem is a core building block in electrical transmission system. It seeks the most economical active and reactive generation dispatch to meet demands while satisfying transmission operational limits. It is often solved repeatedly, especially in regions with large penetration of wind farms to avoid violating operational and physical limits. Recent work has shown that deep learning techniques have huge potential in providing accurate approximations of AC-OPF solutions. However, deep learning approaches often suffer from scalability issues, especially when applied to real life power grids. This paper focuses on the scalability limitation and proposes a load compression embedding scheme to reduce training model sizes using a 3-step approach. The approach is evaluated experimentally on large-scale test cases from the PGLib, and produces an order of magnitude improvements in training convergence and prediction accuracy.
    
[^153]: 隐式空间域陷波滤波器：躲避DeepFake检测

    Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering. (arXiv:2009.09213v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2009.09213](http://arxiv.org/abs/2009.09213)

    本文提出了一种基于学习的方法，在空间域中重现陷波滤波器的效果，以生成高度逼真且“检测难以捉摸”的DeepFakes。

    

    当前，DeepFake图像的高保真生成和高精度检测正处于一场军备竞赛中。本文认为，生成高度逼真且“检测难以捉摸”的DeepFakes可以为未来的DeepFake检测能力的提高服务。本文提出了一种简单而强大的流程，通过进行隐式空间域陷波滤波来减少伪造图像的伪影纹理，同时不影响图像质量。首先证明了在空间域进行周期性噪声的频域陷波虽然出色，但由于陷波器需要手动设计不适合于我们的任务。因此，我们采用一种基于学习的方法，在空间域中重现陷波滤波器的效果。我们通过结合增加压倒性空间噪声来打破周期噪声模式，再利用深度图像滤波来重构无噪声DeepFakes。

    The current high-fidelity generation and high-precision detection of DeepFake images are at an arms race. We believe that producing DeepFakes that are highly realistic and 'detection evasive' can serve the ultimate goal of improving future generation DeepFake detection capabilities. In this paper, we propose a simple yet powerful pipeline to reduce the artifact patterns of fake images without hurting image quality by performing implicit spatial-domain notch filtering. We first demonstrate that frequency-domain notch filtering, although famously shown to be effective in removing periodic noise in the spatial domain, is infeasible for our task at hand due to the manual designs required for the notch filters. We, therefore, resort to a learning-based approach to reproduce the notch filtering effects, but solely in the spatial domain. We adopt a combination of adding overwhelming spatial noise for breaking the periodic noise pattern and deep image filtering to reconstruct the noise-free fa
    
[^154]: 异构车载边缘网络中的计算卸载：在线和Off-policy匹配解决方案

    Computation Offloading in Heterogeneous Vehicular Edge Networks: On-line and Off-policy Bandit Solutions. (arXiv:2008.06302v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2008.06302](http://arxiv.org/abs/2008.06302)

    本文中，我们针对异构VEC场景中的计算卸载问题提出了基于多臂老虎机理论的在线学习算法和Off-policy学习算法，以动态选择最小卸载时间为目标。我们的方法有效地在一群车辆和基站间权衡计算卸载体验和网络负载，并通过实验验证了其有效性。

    

    随着智能交通系统和车载通信的快速发展，车辆边缘计算（VEC）正在成为支持低延迟ITS应用和服务的有前途的技术。本文考虑了在异构VEC场景中，从移动车辆/用户计算卸载的问题，并关注选择不同网络的网络和基站选择问题。在快速变化的车载环境中，用户的计算卸载体验受到与基站共存的边缘计算服务器拥塞引起的延迟的强烈影响。然而，由于这种环境的非平稳特性和信息短缺，预测这种拥塞是一个复杂的任务。为了解决这个挑战，我们提出了基于多臂老虎机理论的在线学习算法和Off-policy学习算法。以动态选择最小卸载时间为目标，通过在一群车辆和基站间权衡计算卸载体验和网络负载，我们证明了所提方法的有效性。

    With the rapid advancement of Intelligent Transportation Systems (ITS) and vehicular communications, Vehicular Edge Computing (VEC) is emerging as a promising technology to support low-latency ITS applications and services. In this paper, we consider the computation offloading problem from mobile vehicles/users in a heterogeneous VEC scenario, and focus on the network- and base station selection problems, where different networks have different traffic loads. In a fast-varying vehicular environment, computation offloading experience of users is strongly affected by the latency due to the congestion at the edge computing servers co-located with the base stations. However, as a result of the non-stationary property of such an environment and also information shortage, predicting this congestion is an involved task. To address this challenge, we propose an on-line learning algorithm and an off-policy learning algorithm based on multi-armed bandit theory. To dynamically select the least co
    

