# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation.](http://arxiv.org/abs/2401.06127) | 本论文旨在提出一种高效的方法来从扩散模型中提炼GANs，并用于图像到图像的转换任务。这种方法可以实现灵活的实时图像编辑，并显著降低训练不同概念模型的成本。 |
| [^2] | [Manipulating Feature Visualizations with Gradient Slingshots.](http://arxiv.org/abs/2401.06122) | 本研究探究了激活最大化方法在对抗模型操作中的脆弱性，并提出了一种新的方法来操纵特征可视化，以隐藏特定神经元的功能。 |
| [^3] | [TOFU: A Task of Fictitious Unlearning for LLMs.](http://arxiv.org/abs/2401.06121) | 本研究提出了一种名为TOFU的虚拟遗忘任务，旨在帮助我们深入理解遗忘。通过提供一个包含200个合成作者配置文件的数据集以及一套综合度量标准，该研究探讨了遗忘方法的效果，并提供了一组基准结果。 |
| [^4] | [Extreme Compression of Large Language Models via Additive Quantization.](http://arxiv.org/abs/2401.06118) | 本文提出的算法在大规模语言模型的极端压缩方面取得了较好的性能，相比最新技术，在给定的压缩预算下准确性更高。 |
| [^5] | [PALP: Prompt Aligned Personalization of Text-to-Image Models.](http://arxiv.org/abs/2401.06105) | 本文提出了一种名为PALP的方法，用于解决文本到图像模型的个性化对准问题。该方法通过额外的分数蒸馏采样项，保持个性化模型与目标提示的对准，使得能够创建具有复杂和复杂提示的图像。 |
| [^6] | [Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models.](http://arxiv.org/abs/2401.06102) | 本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。 |
| [^7] | [A Closer Look at AUROC and AUPRC under Class Imbalance.](http://arxiv.org/abs/2401.06091) | 通过数学分析，研究发现AUROC和AUPRC在类别不平衡情况下可以以概率术语简洁地相关联。相比于人们普遍认为的AUPRC优越性，结果表明AUPRC并不如人们预期的有优势，并且可能是一种有害的指标。研究还通过分析大量文献验证了这一结论。 |
| [^8] | [PANDORA: A Parallel Dendrogram Construction Algorithm for Single Linkage Clustering on GPU.](http://arxiv.org/abs/2401.06089) | PANDORA 是一种用于单链接聚类的并行树状图构建算法，通过独特的递归树收缩方法解决了传统方法并行化困难的问题，实现了渐进意义下的工作最优，并且适用于大规模线程加速器。 |
| [^9] | [Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models.](http://arxiv.org/abs/2401.06088) | 本研究通过使用文本生成技术和机器学习模型，训练了几种变种的生物医学生成预训练变压器模型，并开发了一个自动补全工具，可为三级护理人员提供准确和格式良好的主要症状短语或句子。 |
| [^10] | [XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange.](http://arxiv.org/abs/2401.06086) | 本文介绍了在基于代理模型的体育博彩交易中使用XGBoost学习到的动态投注策略，并通过基于代理模型的模拟器进行实验评估。 |
| [^11] | [Peridynamic Neural Operators: A Data-Driven Nonlocal Constitutive Model for Complex Material Responses.](http://arxiv.org/abs/2401.06070) | Peridynamic神经运算符是一种数据驱动的非局部本构模型，能够从数据中学习复杂材料行为，并提供了具有客观性和动量平衡定律的正向模型。 |
| [^12] | [Investigating Data Contamination for Pre-training Language Models.](http://arxiv.org/abs/2401.06059) | 这项研究调查了预训练语言模型中的数据污染问题，以及该污染对下游任务性能的影响。 |
| [^13] | [On the Power of Graph Neural Networks and Feature Augmentation Strategies to Classify Social Networks.](http://arxiv.org/abs/2401.06048) | 本文研究了四种图神经网络架构的性能以及通过应用不同的人工特征增强策略来提高图分类任务的准确性，结果表明GNN架构的计算能力和人工特征提供的信息水平对任务的性能具有平衡的重要性。 |
| [^14] | [Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting.](http://arxiv.org/abs/2401.06040) | 本论文提出了一种基于小波启发的多尺度图卷积循环网络，用于交通预测。该方法将多尺度分析方法和深度学习方法相结合，对交通数据中的多尺度结构进行建模，并展现了较好的性能。 |
| [^15] | [RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks.](http://arxiv.org/abs/2401.06035) | 这项研究提出了一种新的无条件视频生成模型，通过高效的三平面网络以及联合帧建模方法和基于光流的模块，实现了高效、时间连贯且无视觉伪影的视频生成。 |
| [^16] | [Sea ice detection using concurrent multispectral and synthetic aperture radar imagery.](http://arxiv.org/abs/2401.06009) | 本文提出了一种使用并行多光谱和合成孔径雷达图像进行海冰检测的新工具（ViSual_IceD），该工具通过融合多光谱图像和SAR图像来解决海冰检测中存在的问题，具有良好的性能。 |
| [^17] | [How does the primate brain combine generative and discriminative computations in vision?.](http://arxiv.org/abs/2401.06005) | 本论文探讨了非人灵长类动物大脑如何在视觉中结合生成式和判别式计算。一个观念强调自下而上的信号流动，通过滤除不相关的变异和转换视觉信息来代表行为上相关的信息；而另一个观念将视觉视为Helmholtz的推理过程。 |
| [^18] | [A tree-based varying coefficient model.](http://arxiv.org/abs/2401.05982) | 本论文介绍了一种基于树的可变系数模型，使用循环梯度提升机进行建模，实现了逐维早停和特征重要性评分，该模型能够产生与基于神经网络的VCM相当的结果。 |
| [^19] | [Learning physics-based reduced models from data for the Hasegawa-Wakatani equations.](http://arxiv.org/abs/2401.05972) | 本文提出使用非侵入式科学机器学习（SciML）中的operator inference（OpInf）方法，从数据中构建基于物理的低成本简化模型（ROMs），用于非线性等离子体湍流模拟的Hasegawa-Wakatani方程。实验证明该方法在处理复杂、非线性和自驱动动力学模型时具有潜力。 |
| [^20] | [Spatial-Aware Deep Reinforcement Learning for the Traveling Officer Problem.](http://arxiv.org/abs/2401.05969) | 这篇论文提出了一种称为SATOP的空间感知深度强化学习方法来解决巡警问题。该方法通过利用停车位、代理和动作之间的空间关系来创建动作的表示，从而动态调整以适应当前可罚款的停车违规行为，并提前计划以提高到达违规行为发生时的可能性。 |
| [^21] | [An attempt to generate new bridge types from latent space of PixelCNN.](http://arxiv.org/abs/2401.05964) | 本论文尝试使用生成式人工智能技术生成新的桥梁类型，通过对潜在空间进行采样，可以在人类原始桥梁类型的基础上组合不同的结构组件，创造具有一定创新能力的新桥梁类型。 |
| [^22] | [Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments.](http://arxiv.org/abs/2401.05946) | 本文提出了一种从Transformer表示中学习认知地图的方法，该方法针对部分观察环境中的路径规划问题提供了一个有效的解决方案。 |
| [^23] | [Time Series Forecasting of HIV/AIDS in the Philippines Using Deep Learning: Does COVID-19 Epidemic Matter?.](http://arxiv.org/abs/2401.05933) | 此研究使用多层感知器神经网络预测了菲律宾HIV/AIDS的时间序列，研究发现COVID-19疫情对HIV流行没有明显的影响，并预测到2030年该国的累积病例将达到145,273例。 |
| [^24] | [EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge.](http://arxiv.org/abs/2401.05908) | 本研究提出了一种用于癫痫疾病的定制化大型语言模型EpilepsyLLM，通过微调预训练的LLM并使用癫痫领域的数据集进行训练。通过该模型，可以更准确地回答与癫痫相关的问题，尤其适用于使用日语进行研究。 |
| [^25] | [Optimistic Model Rollouts for Pessimistic Offline Policy Optimization.](http://arxiv.org/abs/2401.05899) | 这篇论文提出了一种基于模型的离线强化学习框架ORPO，通过构建乐观MDP来鼓励更多超出离线数据集的模型预测，从而提升泛化能力。 |
| [^26] | [The Role of Deep Learning in Advancing Proactive Cybersecurity Measures for Smart Grid Networks: A Survey.](http://arxiv.org/abs/2401.05896) | 深度学习在智能电网网络中起到推进主动网络安全措施的作用，本论文通过综合调查了最新的深度学习技术在主动网络防御中的应用。 |
| [^27] | [Binary Linear Tree Commitment-based Ownership Protection for Distributed Machine Learning.](http://arxiv.org/abs/2401.05895) | 本论文提出了一种基于二进制线性树提交的分布式机器学习所有权保护模型，通过验证计算的完整性和效果，解决了模型所有权的冲突问题，并降低了更新证明的成本。 |
| [^28] | [Safe reinforcement learning in uncertain contexts.](http://arxiv.org/abs/2401.05876) | 本文提出了一种在不确定环境中进行安全强化学习的方法，通过推导频率保证来估计当前上下文，并通过实验来识别上下文变量。 |
| [^29] | [Inferring Intentions to Speak Using Accelerometer Data In-the-Wild.](http://arxiv.org/abs/2401.05849) | 通过加速度计数据推断成功和失败的讲话意图，在野外环境中的研究表明加速度计数据中存在有用的信息，但不足以可靠地捕捉讲话意图。 |
| [^30] | [Pushing the Pareto front of band gap and permittivity: ML-guided search for dielectric materials.](http://arxiv.org/abs/2401.05848) | 这项研究通过使用多目标优化方法和机器学习技术，在介电材料研究中取得了突破。研究人员成功地合成和表征了两种新型介电材料，CsTaTeO6和Bi2Zr2O7，为未知材料的寻找提供了高效的工作流程。 |
| [^31] | [Revisiting Silhouette: From Micro to Macro Aggregation.](http://arxiv.org/abs/2401.05831) | 本文提出了一种新的聚合策略，用于评估聚类质量。通过对聚类级别的轮廓得分进行平均，并在此基础上对所有聚类的得分进行宏观平均，我们提出的宏观平均轮廓得分对于聚类不均衡和背景噪声是稳健的。 |
| [^32] | [Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents.](http://arxiv.org/abs/2401.05821) | SCoBots是一种可解释的概念瓶颈代理，能够透明化整个决策流程，帮助领域专家理解和规范强化学习代理的行为，从而可能实现更好的人类对齐强化学习。 |
| [^33] | [Implications of Noise in Resistive Memory on Deep Neural Networks for Image Classification.](http://arxiv.org/abs/2401.05820) | 本研究探索了阻抗记忆中的噪声对基于神经网络的图像分类任务的影响，并提出了一些提高弹性的对策。 |
| [^34] | [TAnet: A New Temporal Attention Network for EEG-based Auditory Spatial Attention Decoding with a Short Decision Window.](http://arxiv.org/abs/2401.05819) | TAnet是一种新的基于脑电信号的听觉空间注意力解码网络，采用了多头注意力机制，可以更有效地捕捉脑电信号中时间步之间的交互作用，并提供了比传统方法更好的解码性能。 |
| [^35] | [Cheetah: Bridging the Gap Between Machine Learning and Particle Accelerator Physics with High-Speed, Differentiable Simulations.](http://arxiv.org/abs/2401.05815) | Cheetah是一种高速可微分模拟工具，可以减少计算时间并实现快速收集大规模数据集。它能够促进加速器调优和系统识别，并与机器学习工具无缝集成。 |
| [^36] | [Graph Spatiotemporal Process for Multivariate Time Series Anomaly Detection with Missing Values.](http://arxiv.org/abs/2401.05800) | 该论文介绍了一种名为GST-Pro的框架，利用图形时空过程和异常评分器来解决在采样不规则的多变量时间序列中检测异常的挑战。 |
| [^37] | [Bounds on the price of feedback for mistake-bounded online learning.](http://arxiv.org/abs/2401.05794) | 改进了错误有界在线学习中反馈价格的上下界，还解决了多类学习中标准反馈与赌徒反馈的价格问题。 |
| [^38] | [Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations.](http://arxiv.org/abs/2401.05792) | 本论文提出了一种从多语言嵌入空间中投影语言特定因素的新视角，并通过发现一个低秩子空间来消除与语义无关的信息，从而充分利用语义信息。 |
| [^39] | [Knowledge Translation: A New Pathway for Model Compression.](http://arxiv.org/abs/2401.05772) | 本文提出了一种新的知识转化（KT）框架，通过训练一个“翻译”模型来接收较大模型的参数并生成压缩参数，从而实现模型压缩，而无需重新训练或施加架构限制。 |
| [^40] | [Feature Selection for Functional Data Classification.](http://arxiv.org/abs/2401.05765) | 本文介绍了一种名为FSFC的新方法，它解决了在具有分类响应和纵向特征的情况下同时进行功能数据特征选择和分类的挑战。 |
| [^41] | [An experimental evaluation of Deep Reinforcement Learning algorithms for HVAC control.](http://arxiv.org/abs/2401.05737) | 本论文通过对HVAC控制的几种最先进的深度强化学习算法进行了实验评估，发现SAC和TD3等算法在复杂场景中具有潜力，并揭示了与泛化和增量学习相关的挑战。 |
| [^42] | [Segment Boundary Detection via Class Entropy Measurements in Connectionist Phoneme Recognition.](http://arxiv.org/abs/2401.05717) | 本文研究了使用连接主义音素识别器的类熵来预测音素类之间的时间边界，并比较了不同方法的精确度和召回率。 |
| [^43] | [Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature and Bayesian Optimization.](http://arxiv.org/abs/2401.05716) | 本文研究了通过查询黑盒函数来估计归一化常数的问题，发现问题的难度取决于问题参数$\lambda$的大小，当$\lambda$趋近于零时类似于贝叶斯积分(BQ)，当$\lambda$趋近于无穷大时类似于贝叶斯优化(BO)，且这种模式适用于存在噪声的情况。结果得到了算法无关的下界和上界的支持，以及模拟研究的验证。 |
| [^44] | [Dynamic Indoor Fingerprinting Localization based on Few-Shot Meta-Learning with CSI Images.](http://arxiv.org/abs/2401.05711) | 本论文提出了一种利用数据高效元学习算法的创新室内定位方法，并引入了一种任务加权损失来提高适应性和学习效率。实验证实了该方法在动态室内环境中的稳健性和优越性，在平均欧氏距离上取得了23.13％的显著增益，特别适用于有限CSI数据的场景。 |
| [^45] | [The Distributional Reward Critic Architecture for Perturbed-Reward Reinforcement Learning.](http://arxiv.org/abs/2401.05710) | 这项研究展示了一种适应性分布式奖励评论家架构，能够在未知扰动的情况下恢复真实奖励，并在多个控制任务中取得较高的回报。 |
| [^46] | [Use of Graph Neural Networks in Aiding Defensive Cyber Operations.](http://arxiv.org/abs/2401.05680) | 本文将研究如何利用图神经网络来辅助打破网络攻击生命周期的每个阶段，通过处理和学习来自异构网络威胁数据，以增强防御措施的有效性。 |
| [^47] | [EsaCL: Efficient Continual Learning of Sparse Models.](http://arxiv.org/abs/2401.05667) | EsaCL是一种高效稀疏模型持续学习方法，通过自动修剪冗余参数并避免重新训练，解决了持续学习中存储和计算需求增加的问题。 |
| [^48] | [Root Cause Analysis on Energy Efficiency with Transfer Entropy Flow.](http://arxiv.org/abs/2401.05664) | 本研究提出了一种基于传输熵流的方法来诊断工业系统能效异常状态的根本原因，并通过实验证明了该方法的有效性。 |
| [^49] | [Towards Conversational Diagnostic AI.](http://arxiv.org/abs/2401.05654) | 本文介绍了一种基于大型语言模型的人工智能系统AMIE，该系统利用自我对战的模拟环境和自动化反馈机制进行诊断对话，并且通过评估病史采集、诊断准确性、管理推理、沟通技巧和同理心等维度性能，与初级保健医生进行了比较。 |
| [^50] | [Quantifying Marketing Performance at Channel-Partner Level by Using Marketing Mix Modeling (MMM) and Shapley Value Regression.](http://arxiv.org/abs/2401.05653) | 本文研究了使用Shapley值回归对渠道合作伙伴层面的营销绩效进行量化，并通过与营销组合建模进行比较，证明了Shapley值回归的实用性。同时提出了一种简单的方法来计算调整系数。 |
| [^51] | [When eBPF Meets Machine Learning: On-the-fly OS Kernel Compartmentalization.](http://arxiv.org/abs/2401.05641) | 本文介绍了O2C系统，通过在运行时嵌入机器学习模型到eBPF程序中，将操作系统内核隔离的能力与即时应对威胁的功能相结合，在保障系统稳定性的同时有效限制损害扩散。 |
| [^52] | [Learning Performance-Oriented Control Barrier Functions Under Complex Safety Constraints and Limited Actuation.](http://arxiv.org/abs/2401.05629) | 本研究提出了一个新颖的自监督学习框架，通过构建可导函数来近似安全集合，并使用神经网络参数化控制障碍函数，以解决在复杂安全约束和有限执行能力下寻找最优CBF的挑战。 |
| [^53] | [Graph Q-Learning for Combinatorial Optimization.](http://arxiv.org/abs/2401.05610) | 本论文提出并证明了图神经网络可以应用于解决组合优化问题。通过将优化过程形式化为一个顺序决策问题，并使用Q-Learning训练的GNNs来学习策略，可以达到接近最先进的基于启发式求解器的性能，同时只需使用部分参数和训练时间。 |
| [^54] | [Scaling Laws for Forgetting When Fine-Tuning Large Language Models.](http://arxiv.org/abs/2401.05605) | 本研究探讨了在微调大型语言模型时的遗忘问题，并得出了微调性能与遗忘程度之间存在反比线性关系的结论，提出了遗忘程度随微调参数数量和更新步骤数量呈幂律增长的缩放规律。研究结果还显示，提前停止微调或改变微调参数数量都无法避免遗忘，这为未来减轻遗忘的微调方案的研究提供了重要的安全关键方向。 |
| [^55] | [Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A Transfer Learning Approach with Noise Robustness Analysis.](http://arxiv.org/abs/2401.05580) | 本研究提出了一种迁移学习方法，用于增强扩散相关光谱学中的血流评估，并通过噪声鲁棒性分析展示了其鲁棒性。 |
| [^56] | [An Augmented Surprise-guided Sequential Learning Framework for Predicting the Melt Pool Geometry.](http://arxiv.org/abs/2401.05579) | 本研究引入了一种新颖的增强惊喜引导的顺序学习框架SurpriseAF-BO，用于预测熔池几何形状。这种框架通过迭代自适应学习过程，模拟了工艺参数与熔池特性之间的动力学关系，并在有限数据集条件下进行了学习。 |
| [^57] | [Fast Cerebral Blood Flow Analysis via Extreme Learning Machine.](http://arxiv.org/abs/2401.05578) | 本论文提出了一种通过极限学习机快速精确分析脑血流的方法，并通过与现有算法的综合比较验证了其优越性。它展示了强大的泛化能力，在各种噪声和光学参数下都具有更高的准确性。同时，与计算效率高的神经网络相比，该方法具有较短的训练和推理时间。这种策略适用于在线训练的边缘计算应用。 |
| [^58] | [Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems.](http://arxiv.org/abs/2401.05572) | 本文提出了一个先天价值驱动增强学习（IVRL）模型，用于描述多智能体在合作中的复杂行为。该模型通过建立智能体对群体效用和系统成本的认知，满足其合作伙伴的需求，支持其社区并融入人类社会。 |
| [^59] | [QuantumSEA: In-Time Sparse Exploration for Noise Adaptive Quantum Circuits.](http://arxiv.org/abs/2401.05571) | QuantumSEA是一种基于噪声自适应的量子电路的实时稀疏探索方法，旨在通过动态探索电路的稀疏连接和固定数量的量子门，在训练过程中隐式增加电路容量，以满足相干时间和轻量级噪声的要求，并实现对真实噪声环境的鲁棒性。 |
| [^60] | [Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch Pretraining on Screening Mammograms.](http://arxiv.org/abs/2401.05570) | 这项研究提出了一种基于软标签的孪生网络方法，利用对侧乳房X线片训练神经网络，以在无监督情况下区分异常病变和背景组织。实验证明，通过欧氏距离衍生的软标签，可以有效地区分医学成像中的病变。 |
| [^61] | [SENet: Visual Detection of Online Social Engineering Attack Campaigns.](http://arxiv.org/abs/2401.05569) | 本论文提出了SEShield，这是一个在浏览器中检测社会工程攻击的框架，填补了社会工程攻击领域的研究空白。 |
| [^62] | [Phase discovery with active learning: Application to structural phase transitions in equiatomic NiTi.](http://arxiv.org/abs/2401.05568) | 本文通过应用主动学习方法，在等原子比的镍钛中发现了可逆的B19' -> B2相变，并利用机器学习力场与DFT计算进行了验证和跟踪。 |
| [^63] | [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training.](http://arxiv.org/abs/2401.05566) | 该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。 |
| [^64] | [Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated Learning.](http://arxiv.org/abs/2401.05562) | 本文开发了一种名为Brave的协议，具有拜占庭容错和隐私保护的点对点联邦学习，保护了参与者的隐私并防止对手破坏学习过程。 |
| [^65] | [Multi-objective Feature Selection in Remote Health Monitoring Applications.](http://arxiv.org/abs/2401.05538) | 本研究提出了一种基于多目标优化的特征选择方法，以增强呼吸模式识别准确性并阻碍个体用户的识别。研究结果表明呼吸识别和用户识别的准确性之间存在显著差异。 |
| [^66] | [Improving the Accuracy and Interpretability of Random Forests via Forest Pruning.](http://arxiv.org/abs/2401.05535) | 通过森林修剪方法，本研究提出了一种兼顾随机森林准确性和决策树可解释性的方法。实验证明，在大多数情景下，这种方法能够显著提高随机森林的性能。 |
| [^67] | [VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational Inference for Improved Generalization in Audio Pattern Recognition.](http://arxiv.org/abs/2401.05531) | 本研究提出了VI-PANN，利用转移学习和不确定性感知变分推理方法，在音频模式识别中取得了改进的泛化性能。 |
| [^68] | [Towards Safe Load Balancing based on Control Barrier Functions and Deep Reinforcement Learning.](http://arxiv.org/abs/2401.05525) | 提出了一种基于深度强化学习和控制障碍函数的安全负载均衡算法，用于SD-WAN，并在训练和测试过程中安全地将不安全的动作映射为可行的动作，并引导学习向安全策略发展。在GPU上实施的解决方案能够加速训练并实现在线策略方法的模型更新，同时提供接近最优的服务质量（QoS）性能。 |
| [^69] | [Correlated Quantization for Faster Nonconvex Distributed Optimization.](http://arxiv.org/abs/2401.05518) | 本研究利用相关量化器改进了MARINA算法，通过使用加权Hessian方差进行原始分析，并扩展了MARINA的理论框架，使其适用于更广泛的压缩器范围。 |
| [^70] | [Diversity-aware clustering: Computational Complexity and Approximation Algorithms.](http://arxiv.org/abs/2401.05502) | 本研究讨论了多样性感知聚类问题，在选择聚类中心时要考虑多个属性，同时最小化聚类目标。我们提出了针对不同聚类目标的参数化近似算法，这些算法在保证聚类质量的同时，具有紧确的近似比。 |
| [^71] | [The recursive scheme of clustering.](http://arxiv.org/abs/2401.05479) | 这篇论文提出了一个递归的聚类方案，用于处理具有测量误差的实验数据，在地理实验中，我们展示了该方法相比于传统方法给出更好的结果。 |
| [^72] | [Population Graph Cross-Network Node Classification for Autism Detection Across Sample Groups.](http://arxiv.org/abs/2401.05478) | 本文提出了一种新颖的跨网络节点分类方法OTGCN，利用图卷积网络和最优输运策略，可以在不同数据采集地点的样本之间纠正领域漂移，并且在自闭症谱系障碍的分类上取得了有效的结果。 |
| [^73] | [Standardizing Your Training Process for Human Activity Recognition Models: A Comprehensive Review in the Tunable Factors.](http://arxiv.org/abs/2401.05477) | 本文回顾了可穿戴人类活动识别领域的深度学习研究，并总结了各个研究所采用的训练程序。研究发现存在缺乏详细训练协议的趋势，同时利用控制变量方法评估了关键可调组件对跨主体泛化的影响。 |
| [^74] | [Modelling Species Distributions with Deep Learning to Predict Plant Extinction Risk and Assess Climate Change Impacts.](http://arxiv.org/abs/2401.05470) | 本文提出了一种利用深度学习建模物种分布的方法，以预测植物灭绝风险和评估气候变化的影响。通过这种方法，可以对物种的IUCN状态进行准确的分类，为全球生物多样性框架的制定提供科学依据。 |
| [^75] | [Robust CNN-based Respiration Rate Estimation for Smartwatch PPG and IMU.](http://arxiv.org/abs/2401.05469) | 本文提出了一种基于卷积神经网络的方法，从智能手表的PPG、加速度计和陀螺仪信号中准确提取呼吸频率。 |
| [^76] | [Introducing New Node Prediction in Graph Mining: Predicting All Links from Isolated Nodes with Graph Neural Networks.](http://arxiv.org/abs/2401.05468) | 这项研究引入了一个新的问题——新节点预测，即从以前与图不相连的孤立节点中预测所有连接。通过基于深度图神经网络的架构，实验结果表明可以解决这一具有挑战性的问题。 |
| [^77] | [Machine Teaching for Building Modular AI Agents based on Zero-shot Learners.](http://arxiv.org/abs/2401.05467) | 这篇论文提出了一种机器教学方法，通过利用迭代机器教学和任务特定的替代模型，增强了利用大语言模型作为零样本学习器的模块化AI代理的鲁棒性和性能。 |
| [^78] | [The two-way knowledge interaction interface between humans and neural networks.](http://arxiv.org/abs/2401.05461) | 该论文构建了一个人类和神经网络之间的双向知识交互界面，使用视觉概念和它们之间的关系作为“语言”，以实现人类和神经网络之间的知识交流。该界面可以使神经网络向人类提供直观的推理解释，同时人类可以修改其中的偏见。 |
| [^79] | [CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic Dimensionality Guidance.](http://arxiv.org/abs/2401.05458) | CoLafier是一种使用局部内在维度（LID）进行带有噪声标签学习的方法。它通过利用LID-dis和LID-gen两个子网络，其中LID-dis使用样本的特征和标签来预测标签，产生增强的内部表示。与LID-dis相反，LID-gen仅使用样本的特征。CoLafier利用每个实例的两个增强视图同时输入两个子网络，利用LID分数来分配标签。 |
| [^80] | [Dimensionality-Aware Outlier Detection: Theoretical and Experimental Analysis.](http://arxiv.org/abs/2401.05453) | 这篇论文提出了一种维度感知的异常检测方法DAO，通过全面实验验证了其在800多个数据集上显著优于其他三种流行的异常检测方法。 |
| [^81] | [Cuff-less Arterial Blood Pressure Waveform Synthesis from Single-site PPG using Transformer & Frequency-domain Learning.](http://arxiv.org/abs/2401.05452) | 本论文提出了两种用于无袖合成动脉血压波形的深度学习模型，一种基于Transformer，一种基于频域学习。实验证明，频域学习模型在动脉血压估计方面优于Transformer模型。 |
| [^82] | [Self-supervised Learning for Electroencephalogram: A Systematic Survey.](http://arxiv.org/abs/2401.05446) | 这篇论文系统综述了自我监督学习在脑电图中的应用，通过设计良好的预训练任务来提取无标签样本的表示，解决了脑电信号标签的问题和个体之间的变化带来的挑战。 |
| [^83] | [Fully Spiking Actor Network with Intra-layer Connections for Reinforcement Learning.](http://arxiv.org/abs/2401.05444) | 本研究提出了一种基于层内连接的全脉冲行为网络，利用特殊的神经形态硬件实现了较低能耗的人工智能。该方法在实现控制任务上表现出可比较性能，并解决了使用脉冲频率作为输出所带来的浮点矩阵运算问题。 |
| [^84] | [Functional Graphical Models: Structure Enables Offline Data-Driven Optimization.](http://arxiv.org/abs/2401.05442) | 功能图模型（FGMs）通过结构实现了样本高效的数据驱动优化。 |
| [^85] | [An adaptive network-based approach for advanced forecasting of cryptocurrency values.](http://arxiv.org/abs/2401.05441) | 本文提出了一种使用自适应网络的方法，通过历史数据对比特币、以太坊和比特币支配度、以太坊支配度进行预测，具有较高的预测准确性。 |
| [^86] | [Autosen: improving automatic wifi human sensing through cross-modal autoencoder.](http://arxiv.org/abs/2401.05440) | Autosen是一种创新的自动Wi-Fi感知解决方案，通过自动跨模态自编码器学习建立了幅度和相位之间的直接连接，有效提取Wi-Fi信号中的有价值特征，并利用这些特征进行特定任务。 |
| [^87] | [Physics-informed Deep Learning to Solve Three-dimensional Terzaghi Consolidation Equation: Forward and Inverse Problems.](http://arxiv.org/abs/2401.05439) | 本文提出了一种用于三维Terzaghi固结方程的物理信息化神经网络(PINNs)框架，能够快速预测不同条件下的固结案例。通过对比传统数值方法，研究了PINNs在正向和反向问题中的性能，并确定了固结系数和噪声数据的影响。 |
| [^88] | [Representation Learning for Wearable-Based Applications in the Case of Missing Data.](http://arxiv.org/abs/2401.05437) | 本论文研究了可穿戴应用中表示学习的问题，特别是在缺失数据情况下。作者通过比较Transformer模型和统计方法的性能，发现Transformer模型在变化频繁的信号的缺失数据填充方面表现优秀。此研究为基于掩码的自监督学习任务的设计和开发提供了洞察。 |
| [^89] | [Deep OFDM Channel Estimation: Capturing Frequency Recurrence.](http://arxiv.org/abs/2401.05436) | 本文提出了一种基于深度学习的OFDM信道估计方案，通过在单个OFDM时隙内利用无线信道频率间的相关性，使用循环神经网络技术实现了超越现有方法的信道估计性能。 |
| [^90] | [ECGformer: Leveraging transformer for ECG heartbeat arrhythmia classification.](http://arxiv.org/abs/2401.05434) | ECGformer是一种利用transformer架构的模型，用于对心电图数据中的心律失常进行分类。 |
| [^91] | [TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks.](http://arxiv.org/abs/2401.05432) | TEN-GUARD提出了一种使用张量分解方法检测深度神经网络中后门攻击的新方法，相对于现有的方法具有多个优势，包括能够同时分析多个模型，在各种网络架构上工作，不对触发器的性质做任何假设，并且计算效率高。 |
| [^92] | [TRLS: A Time Series Representation Learning Framework via Spectrogram for Medical Signal Processing.](http://arxiv.org/abs/2401.05431) | TRLS是一种通过声谱图进行时间序列表示学习的医学信号处理框架，通过利用时间频率RNN从增强声谱图中提取出更多信息，并在医学信号分类中显示出更好的性能。 |
| [^93] | [Multi-relational Graph Diffusion Neural Network with Parallel Retention for Stock Trends Classification.](http://arxiv.org/abs/2401.05430) | 该论文提出了一种基于多关系图的股票趋势分类方法，通过生成动态的多关系股票图来建模股票之间复杂的时变关系，并通过扩散过程优化图表示。最终的图表示能够更好地捕捉各个股票内部的时间特征，同时考虑股票之间的关系。 |
| [^94] | [CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in Human Activity Recognition.](http://arxiv.org/abs/2401.05426) | 本论文提出了一个实用的框架用于在人体活动识别任务中高效利用数据，同时考虑传感器模态和采样率的优化，通过设计的可训练参数来指导传感器模态和采样率的选择。 |
| [^95] | [An Unobtrusive and Lightweight Ear-worn System for Continuous Epileptic Seizure Detection.](http://arxiv.org/abs/2401.05425) | 本文提出了一种无干扰且轻量级的耳戴系统EarSD，通过测量用户耳后部位的生理信号，实现了对癫痫发作的连续检测。这种系统相比传统的基于头皮的脑电图测试具有成本低、便携性好、使用舒适的优点。 |
| [^96] | [A Toolbox for Modelling Engagement with Educational Videos.](http://arxiv.org/abs/2401.05424) | 这项工作介绍了PEEKC数据集和TrueLearn Python库，可用于建模教育视频的参与度。TrueLearn模型系列遵循"开放学习者"概念，提供可扩展的在线模型以及较高的预测性能。 |
| [^97] | [WildGEN: Long-horizon Trajectory Generation for Wildlife.](http://arxiv.org/abs/2401.05421) | 本文提出了一个名为WildGEN的概念框架，通过使用变分自编码器（VAEs）方法生成野生动物长时间范围内的运动轨迹。通过平滑滤波器对生成轨迹进行后处理，以提高生成轨迹的质量。评估结果通过视觉检查和计算生成轨迹与真实轨迹之间的Hausdorff距离。 |
| [^98] | [HoloBeam: Learning Optimal Beamforming in Far-Field Holographic Metasurface Transceivers.](http://arxiv.org/abs/2401.05420) | Holographic Metasurface Transceivers (HMTs) are being used as cost-effective substitutes for large antenna arrays for beamforming in Millimeter and TeraHertz wave communication. In this work, a learning algorithm is developed to optimize beamforming in far-field regions, by using a fixed-budget multi-armed bandit framework to maximize received signal strength. The algorithm exploits the parametric form of channel gains and works with the discrete values of phase-shifting parameters. |
| [^99] | [ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction in Machine Learning.](http://arxiv.org/abs/2401.05418) | 本论文通过将降维和决策边界应用于现有的主动学习方法，突显数据中的模式和聚类，并通过实验验证了这种方法在提高轨迹标记效率和准确性方面的潜力。 |
| [^100] | [Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement.](http://arxiv.org/abs/2401.05416) | 该论文提出了一种名为WDSNet的小波动态选择网络，用于智能选择适当的小波基函数以增强惯性传感器信号。此外，论文还提出了一种类别表示机制，用于提高小波基函数的选择能力。 |
| [^101] | [On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors.](http://arxiv.org/abs/2401.05414) | 本文从因果性的角度系统研究了金融领域中的三个困境：时间分辨率不匹配、非平稳性和未知因果因素，并提出了解决方案。 |
| [^102] | [RawECGNet: Deep Learning Generalization for Atrial Fibrillation Detection from the Raw ECG.](http://arxiv.org/abs/2401.05411) | 我们开发了一个名为RawECGNet的深度学习模型，使用原始的单导联心电图来检测房颤和心房扑动的发作。与基于节律的方法相比，RawECGNet利用了更多的形态学信息，表现出更好的性能。 |
| [^103] | [Device-Free Human State Estimation using UWB Multi-Static Radios.](http://arxiv.org/abs/2401.05410) | 这项研究提出了一种在室内环境中通过利用UWB多静态无线电来实现无设备的人体状态估计的方式，可以估计人的位置、活动和区域内的人数。 |
| [^104] | [Image-based Data Representations of Time Series: A Comparative Analysis in EEG Artifact Detection.](http://arxiv.org/abs/2401.05409) | 本文通过针对脑电图数据中的伪迹检测和分类，对十一个深度学习架构在六种常用表示方法上进行了比较分析，并发现某些表示方法在突出数据的信噪比方面更有效。 |
| [^105] | [Decoding Emotional Valence from Wearables: Can Our Data Reveal Our True Feelings?.](http://arxiv.org/abs/2401.05408) | 该研究通过利用可穿戴设备和自我报告措施，旨在解决实验室研究和现实场景之间的差距，并取得了有关用户情感价值的有希望的分类结果。 |
| [^106] | [Machine Learning and Feature Ranking for Impact Fall Detection Event Using Multisensor Data.](http://arxiv.org/abs/2401.05407) | 本论文通过对多传感器数据进行彻底的预处理和特征选择，成功应用机器学习模型实现了冲击坠落检测，取得了较高的准确率。 |
| [^107] | [RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio Applications.](http://arxiv.org/abs/2401.05406) | RFRL Gym是一个用于认知无线电应用的强化学习测试平台，可以帮助开发和测试RFRL技术，模拟无线电频谱环境，并实验不同的频谱感知技术。 |
| [^108] | [SelfEEG: A Python library for Self-Supervised Learning in Electroencephalography.](http://arxiv.org/abs/2401.05405) | SelfEEG是一个为研究人员提供的开源Python库，旨在帮助他们在脑电图数据上进行自监督学习实验。它提供了用户友好且高度可定制的环境，涵盖了从数据导入到模型设计和训练的所有阶段，并提供了各种功能来有效地处理脑电图数据。 |
| [^109] | [Vector Field Oriented Diffusion Model for Crystal Material Generation.](http://arxiv.org/abs/2401.05402) | 提出了一种基于向量场定向扩散模型的晶体材料生成方法，该方法通过利用几何等变GNN同时考虑原子位置和晶体晶格，提供了一种能够更全面评估模型能力的新生成指标。实验证明了该扩散模型的重要性和有效性。 |
| [^110] | [Hyperspectral Lightcurve Inversion for Attitude Determination.](http://arxiv.org/abs/2401.05397) | 该论文研究了利用高光谱光变推断航天器姿态和旋转的方法。通过数值优化和机器学习两种方法，利用最小信息条件完成准确度测定任务，并在合成数据上进行了测试。 |
| [^111] | [SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive market.](http://arxiv.org/abs/2401.05395) | SRNI-CAR是一份用于分析中国汽车市场的综合数据集，填补了现有汽车行业数据集覆盖范围有限的缺口，对于提高预测准确性、扩大商业应用范围、指导政策制定与监管以及推动汽车行业的学术研究具有重要影响。 |
| [^112] | [Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery.](http://arxiv.org/abs/2401.05394) | 该论文介绍了一种新的迭代正则化算法IRKSN，它通过使用$k$支撑范数正则化实现稀疏恢复，并提供了条件。这是对基于$\ell_1$范数的迭代方法的一种重要补充。 |
| [^113] | [Bayesian ECG reconstruction using denoising diffusion generative models.](http://arxiv.org/abs/2401.05388) | 这项工作提出了一种使用健康心电图数据训练的去噪扩散生成模型，成功生成逼真的心电图信号，并且应用于心脏健康监测和诊断领域，实现了校正QT间期、噪声抑制、心电图导联恢复和异常读数识别等重要临床工具的开发。 |
| [^114] | [EMG subspace alignment and visualization for cross-subject hand gesture classification.](http://arxiv.org/abs/2401.05386) | 本文研究了基于肌电图的手势识别系统中的跨个体泛化问题，并通过识别稳健的低维子空间并将其与目标个体对齐，改善了跨个体估计。通过对子空间的可视化，为使用EMG信号改善跨个体泛化提供了有益的见解。 |
| [^115] | [Angle-Equivariant Convolutional Neural Networks for Interference Mitigation in Automotive Radar.](http://arxiv.org/abs/2401.05385) | 这篇论文介绍了一种在汽车雷达中利用角度等变性的完全卷积神经网络，在干扰抑制方面优于先前的工作。 |
| [^116] | [An improved genetic programming for predicting semi autogenous grinding mill throughput.](http://arxiv.org/abs/2401.05382) | 该论文介绍了一种改进的遗传编程方法，应用于预测半自磨磨机的产量。新的遗传编程变体可以提取多个方程式，从而精确预测不同训练数据群集的产量。 |
| [^117] | [ADF & TransApp: A Transformer-Based Framework for Appliance Detection Using Smart Meter Consumption Series.](http://arxiv.org/abs/2401.05381) | 本论文提出了ADF框架和TransApp分类器，用于基于智能电表用电序列的电器检测。这些方法能够有效处理大量数据和变化长度的序列，为电力供应商提供帮助客户实现能源转型的宝贵信息。 |
| [^118] | [Dataset Optimization for Chronic Disease Prediction with Bio-Inspired Feature Selection.](http://arxiv.org/abs/2401.05380) | 本研究探索了在慢性病预测中应用基因算法、粒子群优化和鲸鱼优化算法等生物启发优化算法进行特征选择的方法，目标是提高预测准确性、优化数据维度，并使预测结果更易解释和行动化。实验结果表明，生物启发优化算法能够有效地减少准确分类所需的特征数量。 |
| [^119] | [Detecting QT prolongation From a Single-lead ECG With Deep Learning.](http://arxiv.org/abs/2401.05378) | 本研究开发了一个利用深度学习模型从单导联心电图中推断QT间期，并用于检测Dofetilide药物载荷期间有临床意义的QT延长事件的方法。该方法在大规模的心电图数据集上进行了训练和验证，表现良好。 |
| [^120] | [Dynamic Spiking Graph Neural Networks.](http://arxiv.org/abs/2401.05373) | 本文提出了一个名为"动态尖峰图神经网络"（DSGNN）的框架，它将尖峰神经网络（SNNs）与图神经网络（GNNs）结合起来，以解决动态图表示学习中的复杂性和内存开销问题。DSGNN通过动态调整尖峰神经元的状态和连接权重，在传播过程中保持图结构信息的完整性。 |
| [^121] | [Autoregressive fragment-based diffusion for pocket-aware ligand design.](http://arxiv.org/abs/2401.05370) | 引入了AutoFragDiff，一种基于片段的自回归扩散模型，用于生成3D分子结构。通过预测分子片段的原子类型和空间坐标，改善了生成分子的局部几何结构，并保持高的结合亲和力。如果提供起始分子骨架，模型还可以进行扩展。 |
| [^122] | [Symbolic Regression of Dynamic Network Models.](http://arxiv.org/abs/2401.05369) | 符号回归是一种通过复制网络的结构和过程来解释网络结构的方法，不依赖科学家的直觉或专业知识，成功地检索到了合成的增长过程和简单可解释的规则。 |
| [^123] | [Context-Aware Stress Monitoring using Wearable and Mobile Technologies in Everyday Settings.](http://arxiv.org/abs/2401.05367) | 这项研究介绍了一种在日常环境中利用生理和环境数据来客观跟踪压力水平的监测系统，并整合了智能标记方法来优化数据收集。三层物联网系统架构被用来解决挑战。 |
| [^124] | [Online Action Recognition for Human Risk Prediction with Anticipated Haptic Alert via Wearables.](http://arxiv.org/abs/2401.05365) | 本文提出了一个基于穿戴设备的在线动作识别框架，融合了在线人体状态估计、动作识别和运动预测，实现了在举重任务中对工人生物力学风险的早期评估和预防。该框架利用NIOSH指数进行在线风险评估，并通过预测未来的运动和触觉警示来预防潜在风险。 |
| [^125] | [Generalizable Sleep Staging via Multi-level Domain Alignment.](http://arxiv.org/abs/2401.05363) | 本文提出了一种通用的睡眠分期方法，通过引入域泛化概念，结合多级特征对齐的思想，提高了模型对未见过数据集的泛化能力。 |
| [^126] | [U-SWIM: Universal Selective Write-Verify for Computing-in-Memory Neural Accelerators.](http://arxiv.org/abs/2401.05357) | 本文介绍了U-SWIM，一种用于计算内存神经加速器的通用选择性写入验证技术。通过只对少量权重进行写入验证处理，可以加快编程速度并保持DNN的准确性。 |
| [^127] | [Developing a Resource-Constraint EdgeAI model for Surface Defect Detection.](http://arxiv.org/abs/2401.05355) | 开发了一种适用于表面缺陷检测的资源受限EdgeAI模型，可以在边缘设备上进行训练，具有出色的性能。 |
| [^128] | [ImbaGCD: Imbalanced Generalized Category Discovery.](http://arxiv.org/abs/2401.05353) | ImbaGCD是一个以最优传输为基础的期望极大化框架，用于解决不平衡的广义类别发现问题，通过对齐边缘类别先验分布来推断已知和未知类别。 |
| [^129] | [Generalized Categories Discovery for Long-tailed Recognition.](http://arxiv.org/abs/2401.05352) | 长尾识别中的通用类别发现方法(GCD)的重大限制是假设未标记数据中的类别分布是均衡的，而事实上自然环境中的视觉类别通常呈现长尾分布。本文提出了一种针对长尾通用类别发现（Long-tailed GCD）的方法，通过两个策略性正则化实现了对较少出现的尾部类别的重要性的增强。 |
| [^130] | [Rethinking Performance Measures of RNA Secondary Structure Problems.](http://arxiv.org/abs/2401.05351) | 传统的RNA二级结构预测评估指标存在局限性，我们提出了一种基于图核的度量方法，能够公平准确地评估RNA结构预测算法，并在RNA设计实验中展示了其信息指导性。 |
| [^131] | [Adaptive operator selection utilising generalised experience.](http://arxiv.org/abs/2401.05350) | 这篇论文提出了基于强化学习的一种新方法，用于解决组合优化问题中的探索和开发平衡问题，以及利用广义经验来发展通用框架。 |
| [^132] | [Most discriminative stimuli for functional cell type identification.](http://arxiv.org/abs/2401.05342) | 本文提出了一种使用最具区分性刺激物的优化聚类方法，成功地识别了小鼠视网膜、恒河猴视网膜和猕猴V4视觉区的功能细胞类型。 |
| [^133] | [Stable Online and Offline Reinforcement Learning for Antibody CDRH3 Design.](http://arxiv.org/abs/2401.05341) | 本研究介绍了一种针对抗体设计领域独特挑战的创新强化学习方法，通过在线或离线学习可以设计出针对多个靶标的高亲和力抗体，且在Absolut!数据库测试中表现优于现有方法。 |
| [^134] | [STR-Cert: Robustness Certification for Deep Text Recognition on Deep Learning Pipelines and Vision Transformers.](http://arxiv.org/abs/2401.05338) | 本文提出了STR-Cert，是一种针对鲁棒性认证的深度文本识别方法。通过扩展DeepPoly多面体验证框架，我们针对场景文本识别模型的各个组件提出了新的多面体边界和算法。通过在六个数据集上进行认证和比较，证明了该方法的有效性。 |
| [^135] | [Optimal Linear Signal: An Unsupervised Machine Learning Framework to Optimize PnL with Linear Signals.](http://arxiv.org/abs/2401.05337) | 本研究提出了一种无监督机器学习框架，用于优化量化金融中的利润和损失。通过线性组合外生变量构建的信号，最大化夏普比率。实证应用显示了该模型的有效性，并提出了进一步发展的潜在途径。 |
| [^136] | [On the Correctness of the Generalized Isotonic Recursive Partitioning Algorithm.](http://arxiv.org/abs/2401.04847) | 本文通过深入分析广义等增递归分割算法（GIRP），在可分离凸损失和不可微损失的情况下，解决了等增回归问题的存在性和唯一性，并提出了递归二分分割的方法来找到解。 |
| [^137] | [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation.](http://arxiv.org/abs/2401.04679) | RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。 |
| [^138] | [Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series.](http://arxiv.org/abs/2401.03955) | 本论文介绍了一种名为微小时间混合器 (TTMs) 的预训练模型，该模型针对多变量时间序列的零/少样本预测进行了优化。与大型预训练模型相比，TTMs模型更小、更快，并考虑了跨通道相关性，能够在短时间内进行有效的预测。 |
| [^139] | [Long-term Safe Reinforcement Learning with Binary Feedback.](http://arxiv.org/abs/2401.03786) | 本论文提出了一种长期安全的强化学习算法LoBiSaRL，该算法针对具有二进制安全反馈和未知随机状态转换函数的约束马尔可夫决策过程（CMDPs），通过最大化奖励的方式优化策略，同时以高概率确保每个回合中代理只执行安全的状态-动作对。 |
| [^140] | [Machine Learning Applications in Traumatic Brain Injury: A Spotlight on Mild TBI.](http://arxiv.org/abs/2401.03621) | 该论文回顾了在创伤性脑损伤（TBI）中应用机器学习的最新技术，特别关注轻度脑损伤（mTBI）。虽然已经有很多机器学习技术用于诊断，但对预测预后的研究相对较少。 |
| [^141] | [Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT.](http://arxiv.org/abs/2401.03302) | 本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。 |
| [^142] | [The cell signaling structure function.](http://arxiv.org/abs/2401.02501) | 该论文提出了一个新的方法，在活体细胞显微镜捕捉到的五维视频中寻找细胞信号动力学时空模式，并且不需要任何先验的预期模式动力学和训练数据。该方法基于细胞信号结构函数（SSF），通过测量细胞信号状态和周围细胞质之间的核糖体强度，与当前最先进的核糖体与细胞核比值相比有了显著改进。通过归一化压缩距离（NCD）来识别相似的模式。该方法能够将输入的SSF构图表示为低维嵌入中的点，最优地捕捉模式。 |
| [^143] | [Expressivity and Approximation Properties of Deep Neural Networks with ReLU$^k$ Activation.](http://arxiv.org/abs/2312.16483) | 本文研究了采用ReLU$^k$激活函数的深度神经网络的表达能力和近似性质，通过全面、构造性的证明，证明了深层ReLU$^k$网络可以精确表示高阶多项式，并建立了网络参数的上界。此外，发现深层ReLU$^k$网络可以逼近多种变化空间中的函数，扩展了仅由ReLU$^k$激活函数生成的空间。 |
| [^144] | [Robust Stochastic Graph Generator for Counterfactual Explanations.](http://arxiv.org/abs/2312.11747) | 这篇论文介绍了一种新颖的鲁棒随机图生成器（RSGG-CE）用于反事实解释，该方法在生成类似于原始图形的新图形的同时，基于潜在预测模型产生不同的结果。此方法利用生成机制生成反事实实例，具有较高的解释能力，并在其他领域展现出卓越的成果。 |
| [^145] | [Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters.](http://arxiv.org/abs/2312.10813) | 该论文提出了一种新型的提示方法，重新参数化低秩提示（RLP），用于在大型预训练视觉语言模型的适应过程中实现高效和有效的知识转移。该方法能够显著减少可调参数和存储开销。 |
| [^146] | [ConFormer: A Novel Collection of Deep Learning Models to Assist Cardiologists in the Assessment of Cardiac Function.](http://arxiv.org/abs/2312.08567) | ConFormer是一种新颖的深度学习模型，旨在自动估计超声心动图中的射血分数（EF）和左心室壁厚度，从而实现成本效益高、易获取和全面的心脏健康监测。 |
| [^147] | [Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning.](http://arxiv.org/abs/2312.05720) | 本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。 |
| [^148] | [Style Aligned Image Generation via Shared Attention.](http://arxiv.org/abs/2312.02133) | 本文提出了一种名为StyleAligned的新技术，通过在T2I模型中使用“注意力共享”，实现了在一系列生成的图像之间建立风格对齐。该方法能够通过简单的反转操作，使用参考风格创建具有一致风格的图像，并在各种输入中实现了高质量的合成和保真度。 |
| [^149] | [Towards Redundancy-Free Sub-networks in Continual Learning.](http://arxiv.org/abs/2312.00840) | 本研究提出了一种名为信息瓶颈遮蔽子网络 (IBM) 的方法，通过消除子网络内部的冗余来解决不断学习中的灾难性遗忘问题。该方法通过累积有价值的信息到重要的权重中，构建了无冗余的子网络，有效地减轻了灾难性遗忘，并促进了新任务的训练。 |
| [^150] | [Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI.](http://arxiv.org/abs/2311.18252) | 这项研究探讨了生成性人工智能中数据隐私和版权保护的多方面挑战，并提出了将技术创新与伦理前瞻相结合的综合方法，旨在全面解决这些问题。 |
| [^151] | [New Online Communities: Graph Deep Learning on Anonymous Voting Networks to Identify Sybils in Polycentric Governance.](http://arxiv.org/abs/2311.17929) | 本研究提供了一个理论框架，通过使用图深度学习技术，可以有效识别去中心化自治组织（DAO）中的虚假身份，为分散治理提供了新的视角。 |
| [^152] | [Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using Stochastic Scale.](http://arxiv.org/abs/2311.15816) | 本文提出了一种用于二值神经网络（BNNs）的规模丢弃的新型正则化技术，并基于蒙特卡洛建立BayNN模型以高效地估计不确定性。 |
| [^153] | [Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series.](http://arxiv.org/abs/2311.13326) | 本文通过在复杂时间序列数据上探索课程学习和模仿学习的方法，发现课程学习是改善复杂时间序列控制任务性能的新途径，而模仿学习也应该被应用。 |
| [^154] | [Developing a Novel Holistic, Personalized Dementia Risk Prediction Model via Integration of Machine Learning and Network Systems Biology Approaches.](http://arxiv.org/abs/2311.09229) | 开发了一种新型综合个性化的痴呆风险预测模型，通过整合机器学习和网络系统生物学方法，成功建模了变量之间的相互作用。 |
| [^155] | [CausalCite: A Causal Formulation of Paper Citations.](http://arxiv.org/abs/2311.02790) | CausalCite是一种以因果推断为基础的论文引用公式化方法，通过对文本进行嵌入和相似样本的提取来评估论文的重要性，并在各个标准上展示了其有效性。 |
| [^156] | [Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures.](http://arxiv.org/abs/2311.00636) | 本篇论文提出了一种用于现代神经网络架构的克罗内克近似曲率算法，可以加速神经网络训练和减少计算成本。作者发现了两种具有线性权重共享层不同设置，并证明了相应设置下的K-FAC算法的精确性。K-FAC-reduce通常比K-FAC-expand更快，可以用于加速自动超参数选择。 |
| [^157] | [Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding.](http://arxiv.org/abs/2310.18716) | Laplacian规范化是一种解决谱嵌入中符号和基底不变性问题的轻量级预处理方法，可以提高在图数据上的效果。 |
| [^158] | [Analyzing Modularity Maximization in Approximation, Heuristic, and Graph Neural Network Algorithms for Community Detection.](http://arxiv.org/abs/2310.10898) | 这项研究分析了不同的模块化最大化算法对于社区检测中实现最优划分的性能，并发现最常用的模块化方法与最优划分之间存在显著差异。 |
| [^159] | [CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model.](http://arxiv.org/abs/2310.06266) | CodeFuse-13B是一个预训练的多语言代码大型语言模型，专为代码相关任务设计，支持超过40种编程语言，并通过使用高质量的预训练数据集以及大量实验的验证，展现了其在多语言输入下的有效性。 |
| [^160] | [Early Warning via tipping-preserving latent stochastic dynamical system and meta label correcting.](http://arxiv.org/abs/2310.06059) | 提出了一种通过融合真实数据和隐性随机微分方程产生的增广数据的元学习框架，用于提高对早期癫痫发作信号的预测准确率，并通过提取的临界动力学特征来标记噪声数据。 |
| [^161] | [Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems.](http://arxiv.org/abs/2309.11526) | 该论文介绍了一种基于似然的传感器校准方法，可以在物联网系统中实现专家支持的分布式学习算法。通过对模拟和实际测量数据的评估，证明了该方法的有效性和改进效果。 |
| [^162] | [EarthPT: a foundation model for Earth Observation.](http://arxiv.org/abs/2309.07207) | EarthPT是一个地球观测的预训练transformer模型，能够准确预测未来地表反射值，并提供有意义的嵌入信息用于下游任务。 |
| [^163] | [A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability.](http://arxiv.org/abs/2309.07156) | 本文提出了一种基于单通道脑电图的睡眠阶段分类方法，使用SE-Resnet-Bi-LSTM架构进行睡眠阶段的分类，并进行了模型解释性分析。在三个不同的数据集上进行了全面评估，取得了显著的准确率和宏F1得分。 |
| [^164] | [Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power.](http://arxiv.org/abs/2309.04941) | 本文提出了一种称为$d$-DRFWL(2) GNNs的新型图神经网络，它通过限制节点之间的距离来实现循环计数能力，克服了子图GNNs的预处理和计算成本高的限制。 |
| [^165] | [Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning.](http://arxiv.org/abs/2309.03581) | 本文提出了一个以人为中心的交互式超参数优化方法，通过应用偏好学习来解决多目标机器学习中的问题。 |
| [^166] | [Trinary Decision Trees for missing value handling.](http://arxiv.org/abs/2309.03561) | 本文介绍了一种称为三值决策树的算法，用于改善决策树在处理缺失数据时的表现。与其他方法不同，该算法不假设缺失值包含任何关于响应的信息。实验证明，在特定缺失数据场景下，三值决策树在MCAR设置中表现优异，在IM设置中略逊一筹。同时，通过将三值决策树与缺失在属性方法相结合，可以获得更稳健的性能。尽管训练速度较慢，但三值决策树提供了一种有前途且更准确的方法。 |
| [^167] | [Edge Generation Scheduling for DAG Tasks Using Deep Reinforcement Learning.](http://arxiv.org/abs/2308.14647) | 本文通过使用深度强化学习算法和图表示神经网络，提出了一种边缘生成调度框架(EGS)，用于解决实时DAG任务调度问题。该框架通过迭代生成边缘以最小化DAG宽度，并确保满足截止时间约束。所提出的算法在实验中得到验证，并与其他DAG调度启发式算法和最佳混合整数进行了比较。 |
| [^168] | [ProAgent: Building Proactive Cooperative AI with Large Language Models.](http://arxiv.org/abs/2308.11339) | ProAgent是一个利用大型语言模型构建的主动合作的AI框架，能够预测队友的决策并为自己制定增强计划，具有高度的模块化和可解释性。 |
| [^169] | [Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges.](http://arxiv.org/abs/2308.00031) | 这篇论文调查了在生成人工智能中应用强化学习的现状、机会和开放研究问题。作者主要讨论了三种应用类型：无特定目标的生成方式、同时最大化目标函数的输出生成方式以及将无法通过目标函数捕捉的期望特征嵌入生成过程的方式。这个新兴领域中存在着丰富的机会和挑战。 |
| [^170] | [Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points.](http://arxiv.org/abs/2307.13621) | 本研究提出了一种缩放基于机器学习的化工厂模拟的方法，并通过微调模型解决了在应用于较大工厂时出现的循环求解不稳定的问题。 |
| [^171] | [A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions.](http://arxiv.org/abs/2307.05638) | 本综述调查了深度迁移学习在工业时间序列异常检测中的使用。深度迁移学习通过利用相关任务的知识和考虑数据分布的变化，解决了仅有少量或没有附加标记数据情况下的新任务。 |
| [^172] | [Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning.](http://arxiv.org/abs/2307.01849) | 本论文提出了一种名为Crossway Diffusion的方法，通过使用自监督学习目标增强了基于扩散的视觉机器人策略学习，实验证明了其在各种机器人任务中的有效性和优势。 |
| [^173] | [A Unified Approach to Controlling Implicit Regularization via Mirror Descent.](http://arxiv.org/abs/2306.13853) | 本文提出了一种使用镜面下降方法来统一控制回归和分类问题中的隐式正则化的方法，在所有标准几何下都可以实现$\ell_p$（$p\in[1,\infty]$）范式的隐式正则化，并且可以实现学习理论中许多特殊类的控制的泛化保证。 |
| [^174] | [Gibbs Sampling the Posterior of Neural Networks.](http://arxiv.org/abs/2306.02729) | 这篇论文提出了一种添加噪声的神经网络模型，并使用Gibbs采样器从后验分布中进行采样，该方法在真实数据和合成数据中能够达到类似于马尔科夫链蒙特卡洛方法的性能。 |
| [^175] | [Resilient Constrained Learning.](http://arxiv.org/abs/2306.02426) | 本论文提出了一个名为“抗干扰约束学习”的方法来解决在部署机器学习解决方案时需要满足除了准确性以外的多个要求，并以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松学习约束。 |
| [^176] | [Harnessing large-language models to generate private synthetic text.](http://arxiv.org/abs/2306.01684) | 本文研究了一种通过利用公开数据集和预训练的生成语言模型，结合私有的微调方法，实现生成具有差分隐私性质的合成文本数据集的方法。生成的文本数据集可以重复使用于其他任务，可无限期保留，或与第三方共享而不损失隐私。 |
| [^177] | [Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model.](http://arxiv.org/abs/2306.01424) | 本文研究了连续性结果的部分反事实识别问题，并提出了一种新颖的敏感性模型——曲率敏感模型，通过限制函数级集的曲率来获得信息边界。 |
| [^178] | [Medication Recommendation via Domain Knowledge Informed Deep Learning.](http://arxiv.org/abs/2305.19604) | 提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。 |
| [^179] | [WiFi-TCN: Temporal Convolution for Human Interaction Recognition based on WiFi signal.](http://arxiv.org/abs/2305.18211) | 提出了一种基于WiFi信号的人体交互识别方法WiFi-TCN。传统基于WiFi的方法存在场景或对象变化时性能下降的问题，解决这个问题需要使用大规模数据集进行模型训练。与传统方法相比，该方法具有成本低、部署简易等优势，能够有效解决基于WiFi的人体交互识别中的挑战。 |
| [^180] | [Fine-Tuning Language Models with Just Forward Passes.](http://arxiv.org/abs/2305.17333) | 本论文提出了一种内存高效的零阶优化器，可以使用与推理相同的存储空间微调语言模型，其可以在大规模模型下更快地优化，具有更好的实验结果。 |
| [^181] | [Heterogeneous Value Evaluation for Large Language Models.](http://arxiv.org/abs/2305.17147) | 本文提出了一种自动对齐评估方法A2EHV，采用异质价值系统，并基于价值合理性和社会价值定向框架评估代理人行为的社会偏好，结果表明比传统对齐方法更合理。 |
| [^182] | [Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?.](http://arxiv.org/abs/2305.16297) | 本文研究了分布式优化中通信压缩的效果，探讨了无偏压缩降低总通信成本的条件和程度。 |
| [^183] | [Black-Box Variational Inference Converges.](http://arxiv.org/abs/2305.15349) | 通过对黑盒变分推断（BBVI）的分析，发现一些常见的算法设计选择可能会导致次优收敛速率，但使用带有近端随机梯度下降的BBVI可以实现最强收敛率保证。 |
| [^184] | [Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning.](http://arxiv.org/abs/2305.14062) | 该论文提出了一种不依赖于振幅的光 plethysmography (PPG) 机器学习方法，通过可见性图和迁移学习实现了对心率和血管老化等生物特征的稳健估计和预测。 |
| [^185] | [A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning.](http://arxiv.org/abs/2305.03582) | 本文提出了一种多模态动态自编码器（MDVAE），用于无监督音视频语音表示学习，该方法在中间表示上进行了静态与动态信息、模态特异与共同信息的分离，并且在实验中表现出优越性。 |
| [^186] | [The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models.](http://arxiv.org/abs/2303.08500) | 保护个人隐私信息是很重要的，但规避扩散模型中添加的噪声对数据进行保护存在挑战。AVATAR算法借助扩散模型的威力，提供了一种精心设计的去噪过程来消除数据保护扰动的影响，并获得了在多个数据集上的最先进的性能。 |
| [^187] | [Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review.](http://arxiv.org/abs/2303.06471) | 本文综述了深度神经网络在多模态数据整合方面的应用，以提高癌症诊断和治疗的准确性和可靠性。 |
| [^188] | [Linear Spaces of Meanings: Compositional Structures in Vision-Language Models.](http://arxiv.org/abs/2302.14383) | 本文研究了视觉语言模型中的组合结构，并提出了一种使用嵌入空间中较小集合的向量组合来近似表示来自编码器的表示形式的方法，将这些向量视为“理想单词”，并在CLIP的嵌入中以实验方式探索了这些结构的可用性。 |
| [^189] | [Classy Ensemble: A Novel Ensemble Algorithm for Classification.](http://arxiv.org/abs/2302.10580) | Classy Ensemble是一种新颖的集成学习算法，通过每类准确率的加权组合来聚合模型，在大量机器学习数据集上展现出优越性能。并提出Classy Cluster Ensemble和Classy Evolutionary Ensemble两种增强方法。 |
| [^190] | [Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference.](http://arxiv.org/abs/2301.13330) | 通过混合精度量化方法，选择性调整神经网络的各个层的精度，以实现在任务性能下降最小的情况下，神经网络的快速、节能推断。 |
| [^191] | [An unfolding method based on conditional Invertible Neural Networks (cINN) using iterative training.](http://arxiv.org/abs/2212.08674) | 该论文提出了一种基于条件可逆神经网络（cINN）的展开方法（IcINN），通过迭代训练调整训练样本与数据之间的偏差，实现了对数据与理论预测的比较中探测器效应的展开。 |
| [^192] | [Scalable Hierarchical Over-the-Air Federated Learning.](http://arxiv.org/abs/2211.16162) | 本研究提出了一种针对分布式环境的通信高效的分层联邦学习算法，通过使用可扩展的无线聚合方案和带宽有限的广播方案，解决了设备干扰和边缘服务器干扰的问题。 |
| [^193] | [ARMA Cell: A Modular and Effective Approach for Neural Autoregressive Modeling.](http://arxiv.org/abs/2208.14919) | 本文介绍了ARMA单元，一种更简单、模块化和有效的神经网络时间序列建模方法，能够自然地处理多变量时间序列，并引入了ConvARMA单元作为一种解决方法。 |
| [^194] | [CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty.](http://arxiv.org/abs/2208.08626) | 本文提出了一种新的CP-PINNs模型，通过将PINNs与总变差惩罚相结合，实现了准确的变点检测和PDE的发现。我们还开发了一种元学习算法，能够在数据的连续批次上动态改进优化目标。实证结果表明，在存在变点的情况下，该方法能够准确估计参数和模型对齐，在没有变点的情况下能够数值上收敛到原始PINNs模型的解。 |
| [^195] | [Localized adversarial artifacts for compressed sensing MRI.](http://arxiv.org/abs/2206.05289) | 我们研究了压缩感知MRI中的局部对抗性伪像，并发现与传统总变差（TV）最小化相比，深度神经网络（DNN）在抵抗该对抗扰动方面具有更好的鲁棒性。 |
| [^196] | [Adaptive Estimation of Random Vectors with Bandit Feedback: A mean-squared error viewpoint.](http://arxiv.org/abs/2203.16810) | 本文研究了在每轮仅观察部分未知协方差的高斯向量情况下，通过均方误差估计的顺序学习问题，并提出了连续消除算法的一种变体。同时，导出了样本复杂性的极小值下界。 |
| [^197] | [An Explainable Stacked Ensemble Model for Static Route-Free Estimation of Time of Arrival.](http://arxiv.org/abs/2203.09438) | 本文提出了一种可解释的叠加集成模型，用于静态无路径估计到达时间。该模型将多个机器学习模型组合成一个新的集成结构，能够超越先前的最先进模型。 |
| [^198] | [Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism.](http://arxiv.org/abs/2202.00232) | 本文提出了一种无需额外计算成本的DNN视觉注意机制，名为ISNet，能够忽略图像背景并在COVID-19和结核病检测任务中比多个最先进神经网络具有更好的最小化分类器决策偏差影响的能力。 |
| [^199] | [GPEX, A Framework For Interpreting Artificial Neural Networks.](http://arxiv.org/abs/2112.09820) | 这篇论文提出了一种GPEX框架，用于解释深度人工神经网络，通过推导出一个证据下界来匹配神经网络的输出，而不对神经网络做出任何特定要求。实验证明，在一些理论假设下，只需要简单的网络结构即可达到良好性能。 |

# 详细

[^1]: E$^{2}$GAN: 高效训练图像到图像转换任务的高效GANs

    E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation. (arXiv:2401.06127v1 [cs.CV])

    [http://arxiv.org/abs/2401.06127](http://arxiv.org/abs/2401.06127)

    本论文旨在提出一种高效的方法来从扩散模型中提炼GANs，并用于图像到图像的转换任务。这种方法可以实现灵活的实时图像编辑，并显著降低训练不同概念模型的成本。

    

    为了实现灵活的实时设备上图像编辑，一种高度有希望的方法是利用大规模文本到图像扩散模型，例如稳定扩散 (Stable Diffusion)，生成用于训练生成对抗网络 (GANs) 的配对数据集。这种方法显著减轻了使用扩散模型进行图像编辑时通常由高端商用GPU特定的严格要求。然而，与文本到图像扩散模型不同，每个生成的 GAN 都专门用于特定的图像编辑任务，因此需要昂贵的训练工作来获得各种概念的模型。在这项工作中，我们引入并解决了一个新颖的研究方向：能否使从扩散模型中提炼 GANs 的过程更加高效？为了实现这一目标，我们提出了一系列创新技术。首先，我们构建了一个具有广义特征的基本 GAN 模型，通过微调适应不同的概念，消除了...

    One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating t
    
[^2]: 用梯度弹射操纵特征可视化

    Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])

    [http://arxiv.org/abs/2401.06122](http://arxiv.org/abs/2401.06122)

    本研究探究了激活最大化方法在对抗模型操作中的脆弱性，并提出了一种新的方法来操纵特征可视化，以隐藏特定神经元的功能。

    

    深度神经网络(DNNs)能够学习复杂而多样化的表示，然而，学习到的概念的语义性质仍然未知。解释DNNs学习到的概念的常用方法是激活最大化(AM)，它生成一个合成的输入信号，最大化激活网络中的特定神经元。在本文中，我们研究了这种方法对于对抗模型操作的脆弱性，并引入了一种新的方法来操纵特征可视化，而不改变模型结构或对模型的决策过程产生显著影响。我们评估了我们的方法对几个神经网络模型的效果，并展示了它隐藏特定神经元功能的能力，在模型审核过程中使用选择的目标解释屏蔽了原始解释。作为一种补救措施，我们提出了一种防止这种操纵的防护措施，并提供了定量证据，证明了它的有效性。

    Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which 
    
[^3]: TOFU: 一种用于LLM的虚拟遗忘任务

    TOFU: A Task of Fictitious Unlearning for LLMs. (arXiv:2401.06121v1 [cs.LG])

    [http://arxiv.org/abs/2401.06121](http://arxiv.org/abs/2401.06121)

    本研究提出了一种名为TOFU的虚拟遗忘任务，旨在帮助我们深入理解遗忘。通过提供一个包含200个合成作者配置文件的数据集以及一套综合度量标准，该研究探讨了遗忘方法的效果，并提供了一组基准结果。

    

    大型语言模型在训练时可能会记忆和重现敏感或私密数据，引发法律和伦理上的关切。遗忘，或者调整模型以忘记训练数据中存在的信息，可以为我们提供一种在训练后保护私密数据的方式。尽管存在几种用于这种遗忘的方法，但尚不清楚它们在多大程度上会导致与从未学习过要被遗忘的数据的模型相等。为了解决这一挑战，我们提出了TOFU，一种虚拟遗忘任务，作为一个基准，旨在帮助我们加深对遗忘的理解。我们提供了一个由200个多样的合成作者配置文件组成的数据集，每个配置文件包含20个问答对，以及一个称为“遗忘集”的子集，作为遗忘的目标。我们编制了一套度量标准，共同提供了对遗忘效果的整体影响的完整画面。最后，我们提供了一组基准结果。

    Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results
    
[^4]: 大规模语言模型的极端压缩通过加性量化

    Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])

    [http://arxiv.org/abs/2401.06118](http://arxiv.org/abs/2401.06118)

    本文提出的算法在大规模语言模型的极端压缩方面取得了较好的性能，相比最新技术，在给定的压缩预算下准确性更高。

    

    准确的开源大规模语言模型(LLMs)的出现引发了对这些模型进行量化技术的竞赛，从而使其能够在最终用户设备上执行。在本文中，我们从多码本量化(MCQ)的经典方法角度重新思考了“极端”LLM压缩的问题，即针对非常低的位数，例如每个参数2到3位。我们的工作建立在加性量化这一经典算法之上，并将其适应于语言模型的量化。由此得到的算法在LLM压缩方面推进了最新技术，以给定压缩预算的准确性而言，优于所有最近提出的技术。例如，当将Llama 2模型压缩到每个参数2位时，我们的算法将7B模型量化为6.93困惑度(相对于之前最佳工作改进1.29，相对于FP16改进1.81)，13B模型量化为5.70困惑度(改进0.36)，70B模型量化为3.94困惑度。

    The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
    
[^5]: PALP：文本到图像模型的个性化对准

    PALP: Prompt Aligned Personalization of Text-to-Image Models. (arXiv:2401.06105v1 [cs.CV])

    [http://arxiv.org/abs/2401.06105](http://arxiv.org/abs/2401.06105)

    本文提出了一种名为PALP的方法，用于解决文本到图像模型的个性化对准问题。该方法通过额外的分数蒸馏采样项，保持个性化模型与目标提示的对准，使得能够创建具有复杂和复杂提示的图像。

    

    内容创作者通常希望使用个人主题创建个性化的图像，超出了传统文本到图像模型的能力范围。此外，他们可能希望生成的图像具有特定的位置、风格、氛围等。现有的个性化方法可能会在个性化能力或与复杂文本提示的对齐方面存在妥协。这种权衡可能妨碍用户提示的实现和主体完整性。我们提出了一种新方法，专注于解决这个问题的单个提示的个性化方法。我们称之为对准提示的个性化。虽然这可能看起来有限制，但我们的方法在改善文本对准方面表现出色，使得能够创建具有复杂和复杂提示的图像，这可能对当前的技术构成挑战。特别地，我们的方法使用额外的分数蒸馏采样项使个性化模型与目标提示保持对准。我们展示了我们方法的多功能性。

    Content creators often aim to create personalized images using personal subjects that go beyond the capabilities of conventional text-to-image models. Additionally, they may want the resulting image to encompass a specific location, style, ambiance, and more. Existing personalization methods may compromise personalization ability or the alignment to complex textual prompts. This trade-off can impede the fulfillment of user prompts and subject fidelity. We propose a new approach focusing on personalization methods for a \emph{single} prompt to address this issue. We term our approach prompt-aligned personalization. While this may seem restrictive, our method excels in improving text alignment, enabling the creation of images with complex and intricate prompts, which may pose a challenge for current techniques. In particular, our method keeps the personalized model aligned with a target prompt using an additional score distillation sampling term. We demonstrate the versatility of our met
    
[^6]: Patchscope: 一个统一的框架，用于检查语言模型的隐藏表示

    Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])

    [http://arxiv.org/abs/2401.06102](http://arxiv.org/abs/2401.06102)

    本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。

    

    检查大型语言模型（LLM）的隐藏表示中编码的信息可以解释模型的行为并验证其与人类价值观的一致性。鉴于LLM生成人类可理解文本的能力，我们建议利用模型本身以自然语言解释其内部表示。我们引入了一个称为Patchscopes的框架，并展示了如何使用它来回答关于LLM计算的各种研究问题。我们表明，先前基于将表示投影到词汇空间和干预LLM计算的可解释性方法，可以看作是该框架的特殊实例。此外，通过Patchscope可以弥补优势，如检查早期层失败或表达能力不足。除了统一先前的检查技术，Patchscopes还开辟了新的可能性，例如使用更强大的模型来解释较小模型的表示。

    Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
    
[^7]: AUROC和AUPRC在类不平衡下的深入研究

    A Closer Look at AUROC and AUPRC under Class Imbalance. (arXiv:2401.06091v1 [cs.LG])

    [http://arxiv.org/abs/2401.06091](http://arxiv.org/abs/2401.06091)

    通过数学分析，研究发现AUROC和AUPRC在类别不平衡情况下可以以概率术语简洁地相关联。相比于人们普遍认为的AUPRC优越性，结果表明AUPRC并不如人们预期的有优势，并且可能是一种有害的指标。研究还通过分析大量文献验证了这一结论。

    

    在机器学习中，一个广泛的观点是，在二分类任务中，面积受限制的准确率曲线（AUPRC）比受试者工作特征曲线下的面积（AUROC）更好地用于模型比较，尤其是在存在类别不平衡的情况下。本文通过新颖的数学分析挑战了这一观点，并说明了AUROC和AUPRC可以以概率术语简洁地相关联。我们证明了AUPRC并不如人们普遍认为的在类别不平衡的情况下更优，甚至可能是一种有害的指标，因为它倾向于过分偏向于在正样本较为频繁的子群中改善模型。这种偏差可能会无意中增加算法的差异。在这些洞见的推动下，我们对现有的机器学习文献进行了彻底的回顾，并利用大型语言模型对arXiv上的150多万篇论文进行了分析。我们的调查重点是验证和证明声称的AUPRC优越性的普遍性。

    In machine learning (ML), a widespread adage is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for binary classification tasks with class imbalance. This paper challenges this notion through novel mathematical analysis, illustrating that AUROC and AUPRC can be concisely related in probabilistic terms. We demonstrate that AUPRC, contrary to popular belief, is not superior in cases of class imbalance and might even be a harmful metric, given its inclination to unduly favor model improvements in subpopulations with more frequent positive labels. This bias can inadvertently heighten algorithmic disparities. Prompted by these insights, a thorough review of existing ML literature was conducted, utilizing large language models to analyze over 1.5 million papers from arXiv. Our investigation focused on the prevalence and substantiation of the purported AUPRC superiority. The result
    
[^8]: PANDORA：GPU上用于单链接聚类的并行树状图构建算法

    PANDORA: A Parallel Dendrogram Construction Algorithm for Single Linkage Clustering on GPU. (arXiv:2401.06089v1 [cs.LG])

    [http://arxiv.org/abs/2401.06089](http://arxiv.org/abs/2401.06089)

    PANDORA 是一种用于单链接聚类的并行树状图构建算法，通过独特的递归树收缩方法解决了传统方法并行化困难的问题，实现了渐进意义下的工作最优，并且适用于大规模线程加速器。

    

    本文介绍了PANDORA，一种用于高效构建单链接层次聚类的树状图的并行算法，包括HDSCAN。传统的最小生成树基础的树状图构建方法，例如凝聚或分裂技术，通常难以进行有效的并行化，尤其是对于在现实世界数据中常见的倾斜树状图。PANDORA通过独特的递归树收缩方法解决了这些挑战，该方法简化了树状图的初始构建过程，然后逐步重建完整的树状图。该过程使得PANDORA在渐进意义下工作最优，与树状图的倾斜程度无关。PANDORA的所有步骤都是完全并行的，适用于大规模线程加速器，如GPU。我们的实现使用Kokkos编写，支持CPU和多供应商的GPU（例如Nvidia、AMD）。多线程版本的PANDORA比当前最佳的多线程版本快2.2倍。

    This paper presents \pandora, a novel parallel algorithm for efficiently constructing dendrograms for single-linkage hierarchical clustering, including \hdbscan. Traditional dendrogram construction methods from a minimum spanning tree (MST), such as agglomerative or divisive techniques, often fail to efficiently parallelize, especially with skewed dendrograms common in real-world data.  \pandora addresses these challenges through a unique recursive tree contraction method, which simplifies the tree for initial dendrogram construction and then progressively reconstructs the complete dendrogram. This process makes \pandora asymptotically work-optimal, independent of dendrogram skewness. All steps in \pandora are fully parallel and suitable for massively threaded accelerators such as GPUs.  Our implementation is written in Kokkos, providing support for both CPUs and multi-vendor GPUs (e.g., Nvidia, AMD). The multithreaded version of \pandora is 2.2$\times$ faster than the current best-mul
    
[^9]: 通过大型语言模型在电子健康记录中自动补全主要症状

    Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models. (arXiv:2401.06088v1 [cs.CL])

    [http://arxiv.org/abs/2401.06088](http://arxiv.org/abs/2401.06088)

    本研究通过使用文本生成技术和机器学习模型，训练了几种变种的生物医学生成预训练变压器模型，并开发了一个自动补全工具，可为三级护理人员提供准确和格式良好的主要症状短语或句子。

    

    主要症状（CC）是患者医疗记录的关键组成部分，它描述了寻求医疗保健的主要原因或关注点。它为医疗保健提供者提供了关键信息，以便做出有根据的患者护理决策。然而，为医疗保健提供者记录CC可能耗时，尤其是在繁忙的急诊科。为了解决这个问题，在临床记录中为三级护理人员提供准确和格式良好的短语或句子的自动补全工具可以成为宝贵的资源。在本研究中，我们利用文本生成技术使用CC数据开发了机器学习模型。在我们的提议中，我们训练了一个长短期记忆（LSTM）模型，并微调了三种不同变种的生物医学生成预训练变压器（BioGPT），分别是microsoft/biogpt，microsoft/BioGPT-Large和microsoft/BioGPT-Large-PubMedQA。此外，我们通过结合典型的CC句子，利用GPT的OpenAI API来调整提示。

    The Chief Complaint (CC) is a crucial component of a patient's medical record as it describes the main reason or concern for seeking medical care. It provides critical information for healthcare providers to make informed decisions about patient care. However, documenting CCs can be time-consuming for healthcare providers, especially in busy emergency departments. To address this issue, an autocompletion tool that suggests accurate and well-formatted phrases or sentences for clinical notes can be a valuable resource for triage nurses. In this study, we utilized text generation techniques to develop machine learning models using CC data. In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA. Additionally, we tune a prompt by incorporating exemplar CC sentences, utilizing the OpenAI API of GPT
    
[^10]: XGBoost学习用于基于代理模型的体育博彩交易中的动态投注方式

    XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange. (arXiv:2401.06086v1 [cs.LG])

    [http://arxiv.org/abs/2401.06086](http://arxiv.org/abs/2401.06086)

    本文介绍了在基于代理模型的体育博彩交易中使用XGBoost学习到的动态投注策略，并通过基于代理模型的模拟器进行实验评估。

    

    本文首次介绍了在Bristol Betting Exchange (BBE)中使用高效的机器学习方法XGBoost的结果。BBE是一个开源的基于代理模型的模拟器，旨在模拟具有赛事中投注功能的现代体育博彩交易。我们使用BBE模拟器和其一系列简单的赌徒代理作为数据生成器，输入到我们的XGBoost机器学习系统中。通过学习BBE赌徒代理所进行的更加有利可图的投注，XGBoost能够发现有利可图的动态投注策略。在XGBoost训练之后，生成一个或多个决策树，将具有由XGBoost学习决策树确定的投注策略的赌徒代理添加到BBE模拟器中，在各种条件和投注市场情景下进行一系列比赛的投注，以盈利作为主要的比较和评估指标。本研究的初步发现表明...

    We present first results from the use of XGBoost, a highly effective machine learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source agent-based model (ABM) designed to simulate a contemporary sports-betting exchange with in-play betting during track-racing events such as horse races. We use the BBE ABM and its array of minimally-simple bettor-agents as a synthetic data generator which feeds into our XGBoost ML system, with the intention that XGBoost discovers profitable dynamic betting strategies by learning from the more profitable bets made by the BBE bettor-agents. After this XGBoost training, which results in one or more decision trees, a bettor-agent with a betting strategy determined by the XGBoost-learned decision tree(s) is added to the BBE ABM and made to bet on a sequence of races under various conditions and betting-market scenarios, with profitability serving as the primary metric of comparison and evaluation. Our initial findings presented here sho
    
[^11]: Peridynamic神经运算符：一种面向复杂材料响应的数据驱动非局部本构模型

    Peridynamic Neural Operators: A Data-Driven Nonlocal Constitutive Model for Complex Material Responses. (arXiv:2401.06070v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2401.06070](http://arxiv.org/abs/2401.06070)

    Peridynamic神经运算符是一种数据驱动的非局部本构模型，能够从数据中学习复杂材料行为，并提供了具有客观性和动量平衡定律的正向模型。

    

    近年来，神经运算符作为隐藏的控制方程的隐式解算算子，已经成为学习复杂实际物理系统响应的流行工具。然而，大部分神经运算符应用迄今为止都是数据驱动的，忽略了数据中对基本物理规律的固有保存。在本研究中，我们引入了一种新颖的积分神经运算符架构，称为Peridynamic神经运算符（PNO），用于从数据中学习非局部本构定律。这个神经运算符提供了一种以状态为基础的Peridynamics正向模型，能够自动保证客观性和动量平衡定律。作为应用，我们展示了我们模型在从合成和实验数据集中学习复杂材料行为方面的表现和效果。我们显示，由于其能够捕捉复杂响应，我们学习得到的神经运算符与基线模型相比在准确性和效率上均有提高。

    Neural operators, which can act as implicit solution operators of hidden governing equations, have recently become popular tools for learning the responses of complex real-world physical systems. Nevertheless, most neural operator applications have thus far been data-driven and neglect the intrinsic preservation of fundamental physical laws in data. In this work, we introduce a novel integral neural operator architecture called the Peridynamic Neural Operator (PNO) that learns a nonlocal constitutive law from data. This neural operator provides a forward model in the form of state-based peridynamics, with objectivity and momentum balance laws automatically guaranteed. As applications, we demonstrate the expressivity and efficacy of our model in learning complex material behaviors from both synthetic and experimental data sets. We show that, owing to its ability to capture complex responses, our learned neural operator achieves improved accuracy and efficiency compared to baseline model
    
[^12]: 研究预训练语言模型的数据污染

    Investigating Data Contamination for Pre-training Language Models. (arXiv:2401.06059v1 [cs.CL])

    [http://arxiv.org/abs/2401.06059](http://arxiv.org/abs/2401.06059)

    这项研究调查了预训练语言模型中的数据污染问题，以及该污染对下游任务性能的影响。

    

    在大规模网络语料库上预训练的语言模型在各种下游任务上展示出令人印象深刻的能力。然而，越来越担心这种能力是否是由于评估数据集被包含在预训练语料库中导致的，这种现象被称为“数据污染”，从而在人工提高性能。目前对这种潜在污染如何影响语言模型在下游任务上的性能缺乏了解。本文通过从头开始预训练一系列GPT-2模型，探讨了在预训练阶段数据污染的影响。我们强调了来自评估数据的文本污染（即输入文本的评估样本）和基准污染（即输入中的提示和期望输出）的影响。我们还研究了在各种下游任务中重复污染的影响。此外，我们还调查了普遍存在的n

    Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \textit{from scratch}. We highlight the effect of both text contamination (\textit{i.e.}\ input text of the evaluation samples) and ground-truth contamination (\textit{i.e.}\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n
    
[^13]: 通过图神经网络和特征增强策略来分类社交网络的能力研究

    On the Power of Graph Neural Networks and Feature Augmentation Strategies to Classify Social Networks. (arXiv:2401.06048v1 [cs.SI])

    [http://arxiv.org/abs/2401.06048](http://arxiv.org/abs/2401.06048)

    本文研究了四种图神经网络架构的性能以及通过应用不同的人工特征增强策略来提高图分类任务的准确性，结果表明GNN架构的计算能力和人工特征提供的信息水平对任务的性能具有平衡的重要性。

    

    本文研究了四种图神经网络架构（GNNs）在使用经典的网络科学生成模型创建的合成数据集上进行图分类任务。由于合成网络不包含（节点或边）特征，因此对节点应用了五种不同的增强策略（人造特征类型）。研究了4种GNNs（具有层次和全局聚合的GCN、GIN和GATv2）和5种特征类型（常数1、噪声、度、归一化度和ID - 各种长度的循环数量的向量）的所有组合，并在GNNs中使用的人工神经网络的隐藏维度作为函数比较其性能。还使用第二个合成网络数据集分析了这些模型的泛化能力（包含不同规模的网络）。我们的结果表明，GNN架构的计算能力和人工特征提供的信息水平的平衡重要性。

    This paper studies four Graph Neural Network architectures (GNNs) for a graph classification task on a synthetic dataset created using classic generative models of Network Science. Since the synthetic networks do not contain (node or edge) features, five different augmentation strategies (artificial feature types) are applied to nodes. All combinations of the 4 GNNs (GCN with Hierarchical and Global aggregation, GIN and GATv2) and the 5 feature types (constant 1, noise, degree, normalized degree and ID -- a vector of the number of cycles of various lengths) are studied and their performances compared as a function of the hidden dimension of artificial neural networks used in the GNNs. The generalisation ability of these models is also analysed using a second synthetic network dataset (containing networks of different sizes).Our results point towards the balanced importance of the computational power of the GNN architecture and the the information level provided by the artificial featur
    
[^14]: 基于小波启发的多尺度图卷积循环网络用于交通预测

    Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting. (arXiv:2401.06040v1 [cs.LG])

    [http://arxiv.org/abs/2401.06040](http://arxiv.org/abs/2401.06040)

    本论文提出了一种基于小波启发的多尺度图卷积循环网络，用于交通预测。该方法将多尺度分析方法和深度学习方法相结合，对交通数据中的多尺度结构进行建模，并展现了较好的性能。

    

    交通预测是智能交通系统的基础。时空图神经网络在交通预测中展现出了最先进的性能。然而，这些方法并没有明确地对交通数据中的某些自然特征进行建模，比如包含不同粒度或尺度上的空间和时间变化的多尺度结构。为此，我们提出了一种基于小波启发的图卷积循环网络（WavGCRN），将多尺度分析（MSA）方法和深度学习（DL）方法相结合。在WavGCRN中，交通数据通过离散小波变换（DWT）被分解为时频分量，构建了多流输入结构；然后使用图卷积循环网络（GCRNs）作为编码器对每个流进行特征提取，提取不同尺度的时空特征；最后，可学习的逆DWT和GCRN被结合为解码器，融合信息。

    Traffic forecasting is the foundation for intelligent transportation systems. Spatiotemporal graph neural networks have demonstrated state-of-the-art performance in traffic forecasting. However, these methods do not explicitly model some of the natural characteristics in traffic data, such as the multiscale structure that encompasses spatial and temporal variations at different levels of granularity or scale. To that end, we propose a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines multiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In WavGCRN, the traffic data is decomposed into time-frequency components with Discrete Wavelet Transformation (DWT), constructing a multi-stream input structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as encoders for each stream, extracting spatiotemporal features in different scales; and finally the learnable Inversed DWT and GCRN are combined as the decoder, fusing the inf
    
[^15]: RAVEN：用高效的三平面网络重新思考对抗性视频生成

    RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks. (arXiv:2401.06035v1 [cs.CV])

    [http://arxiv.org/abs/2401.06035](http://arxiv.org/abs/2401.06035)

    这项研究提出了一种新的无条件视频生成模型，通过高效的三平面网络以及联合帧建模方法和基于光流的模块，实现了高效、时间连贯且无视觉伪影的视频生成。

    

    我们提出了一种新颖的无条件视频生成模型，旨在解决长期的空间和时间依赖性。为了捕捉这些依赖关系，我们的方法将受到三维感知生成框架启发的混合显式-隐式三平面表示法并入，并使用一个唯一的潜在编码来建模整个视频序列。然后，从中间三平面表示法中合成单个视频帧，该表示法本身是从主要潜在编码中派生的。这种新颖的策略将计算复杂度减少了2倍，以FLOPs度量。因此，我们的方法便于高效和时间连贯地生成视频。此外，与自回归方法相比，我们的联合帧建模方法可以减少视觉伪影的产生。我们通过在生成对抗网络（GAN）中集成基于光流的模块进一步增强了模型的功能。

    We present a novel unconditional video generative model designed to address long-term spatial and temporal dependencies. To capture these dependencies, our approach incorporates a hybrid explicit-implicit tri-plane representation inspired by 3D-aware generative frameworks developed for three-dimensional object representation and employs a singular latent code to model an entire video sequence. Individual video frames are then synthesized from an intermediate tri-plane representation, which itself is derived from the primary latent code. This novel strategy reduces computational complexity by a factor of $2$ as measured in FLOPs. Consequently, our approach facilitates the efficient and temporally coherent generation of videos. Moreover, our joint frame modeling approach, in contrast to autoregressive methods, mitigates the generation of visual artifacts. We further enhance the model's capabilities by integrating an optical flow-based module within our Generative Adversarial Network (GAN
    
[^16]: 使用并行多光谱和合成孔径雷达图像进行海冰检测

    Sea ice detection using concurrent multispectral and synthetic aperture radar imagery. (arXiv:2401.06009v1 [cs.CV])

    [http://arxiv.org/abs/2401.06009](http://arxiv.org/abs/2401.06009)

    本文提出了一种使用并行多光谱和合成孔径雷达图像进行海冰检测的新工具（ViSual_IceD），该工具通过融合多光谱图像和SAR图像来解决海冰检测中存在的问题，具有良好的性能。

    

    合成孔径雷达（SAR）图像是海冰映射的主要数据类型，因为它具有时空覆盖能力，并且能够在云和光照条件不利时检测海冰。然而，由于图像中存在模糊的信号和噪声，使用SAR图像进行自动海冰检测仍然存在问题。相反，使用多光谱图像（MSI）可以轻松区分冰和水，但在极地地区，海洋表面经常被云层遮挡，阳光可能在数月内不出现在地平线上。为了解决这些限制，本文提出了一种使用并行多光谱可见光和SAR图像进行海冰检测的新工具（ViSual_IceD）。ViSual_IceD是一个卷积神经网络（CNN），它在经典的U-Net体系结构的基础上构建，包含两个并行的编码器阶段，能够融合和拼接具有不同空间分辨率的MSI和SAR图像。ViSual_IceD的性能经过了评估。

    Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea ice mapping due to its spatio-temporal coverage and the ability to detect sea ice independent of cloud and lighting conditions. Automatic sea ice detection using SAR imagery remains problematic due to the presence of ambiguous signal and noise within the image. Conversely, ice and water are easily distinguishable using multispectral imagery (MSI), but in the polar regions the ocean's surface is often occluded by cloud or the sun may not appear above the horizon for many months. To address some of these limitations, this paper proposes a new tool trained using concurrent multispectral Visible and SAR imagery for sea Ice Detection (ViSual\_IceD). ViSual\_IceD is a convolution neural network (CNN) that builds on the classic U-Net architecture by containing two parallel encoder stages, enabling the fusion and concatenation of MSI and SAR imagery containing different spatial resolutions. The performance of ViSual\_
    
[^17]: 非人灵长类动物大脑如何在视觉中结合生成式和判别式计算？

    How does the primate brain combine generative and discriminative computations in vision?. (arXiv:2401.06005v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.06005](http://arxiv.org/abs/2401.06005)

    本论文探讨了非人灵长类动物大脑如何在视觉中结合生成式和判别式计算。一个观念强调自下而上的信号流动，通过滤除不相关的变异和转换视觉信息来代表行为上相关的信息；而另一个观念将视觉视为Helmholtz的推理过程。

    

    视觉被广泛理解为一种推理问题。然而，在生物视觉研究和机器视觉工程中，关于推理过程的两种对立观念都具有影响力。第一种强调自下而上的信号流动，将视觉描述为一种主要是前馈的判别推理过程，其通过滤除不相关的变异和以适合下游的认知和行为控制功能需要的方式来转换视觉信息，并代表行为上相关信息。在这种观念中，视觉由感官数据驱动，感知是直接的，因为处理从数据到感兴趣的潜变量的过程进行。在这种观念中，"推理"的概念类似于神经网络的工程文献中描述的，对图像进行处理的前馈卷积神经网络被认为是在执行推理。而另一种观念是将视觉视为Helmholtz的推理过程。

    Vision is widely understood as an inference problem. However, two contrasting conceptions of the inference process have each been influential in research on biological vision as well as the engineering of machine vision. The first emphasizes bottom-up signal flow, describing vision as a largely feedforward, discriminative inference process that filters and transforms the visual information to remove irrelevant variation and represent behaviorally relevant information in a format suitable for downstream functions of cognition and behavioral control. In this conception, vision is driven by the sensory data, and perception is direct because the processing proceeds from the data to the latent variables of interest. The notion of "inference" in this conception is that of the engineering literature on neural networks, where feedforward convolutional neural networks processing images are said to perform inference. The alternative conception is that of vision as an inference process in Helmhol
    
[^18]: 基于树的可变系数模型介绍

    A tree-based varying coefficient model. (arXiv:2401.05982v1 [stat.ML])

    [http://arxiv.org/abs/2401.05982](http://arxiv.org/abs/2401.05982)

    本论文介绍了一种基于树的可变系数模型，使用循环梯度提升机进行建模，实现了逐维早停和特征重要性评分，该模型能够产生与基于神经网络的VCM相当的结果。

    

    本论文介绍了一种基于树的可变系数模型(VCM)，其中可变系数使用Delong等人(2023)的循环梯度提升机(CGBM)进行建模。使用CGBM对系数函数进行建模，可以进行逐维早停和特征重要性评分。逐维早停不仅可以减少维度特定的过拟合风险，还可以揭示维度之间模型复杂性的差异。使用特征重要性评分可以进行简单的特征选择和易于解释的模型解释。该模型在Richman和Wüthrich（2023）使用的相同的模拟和真实数据示例上进行评估，结果表明，它在样本外损失方面产生了与他们的基于神经网络的VCM LocalGLMnet相当的结果。

    The paper introduces a tree-based varying coefficient model (VCM) where the varying coefficients are modelled using the cyclic gradient boosting machine (CGBM) from Delong et al. (2023). Modelling the coefficient functions using a CGBM allows for dimension-wise early stopping and feature importance scores. The dimension-wise early stopping not only reduces the risk of dimension-specific overfitting, but also reveals differences in model complexity across dimensions. The use of feature importance scores allows for simple feature selection and easy model interpretation. The model is evaluated on the same simulated and real data examples as those used in Richman and W\"uthrich (2023), and the results show that it produces results in terms of out of sample loss that are comparable to those of their neural network-based VCM called LocalGLMnet.
    
[^19]: 从数据中学习基于物理的简化模型，用于Hasegawa-Wakatani方程

    Learning physics-based reduced models from data for the Hasegawa-Wakatani equations. (arXiv:2401.05972v1 [physics.comp-ph])

    [http://arxiv.org/abs/2401.05972](http://arxiv.org/abs/2401.05972)

    本文提出使用非侵入式科学机器学习（SciML）中的operator inference（OpInf）方法，从数据中构建基于物理的低成本简化模型（ROMs），用于非线性等离子体湍流模拟的Hasegawa-Wakatani方程。实验证明该方法在处理复杂、非线性和自驱动动力学模型时具有潜力。

    

    本文关注非侵入式科学机器学习（SciML）的约减模型（ROMs）构建，用于非线性、混沌等离子体湍流模拟。我们提出使用operator inference（OpInf）从数据中构建基于物理的低成本ROMs用于这种模拟。以Hasegawa-Wakatani（HW）方程为代表，在形成复杂、非线性和自驱动动力学的情况下考察了OpInf构建准确ROMs的潜力，并进行了两组实验。第一组实验利用通过直接数值模拟从特定初值条件开始的HW方程获得的数据，进行OpInf ROMs的训练以实现超越训练时间范围的预测。在更具挑战性的第二组实验中，我们使用相同的数据集对ROMs进行训练。

    This paper focuses on the construction of non-intrusive Scientific Machine Learning (SciML) Reduced-Order Models (ROMs) for nonlinear, chaotic plasma turbulence simulations. In particular, we propose using Operator Inference (OpInf) to build low-cost physics-based ROMs from data for such simulations. As a representative example, we focus on the Hasegawa-Wakatani (HW) equations used for modeling two-dimensional electrostatic drift-wave plasma turbulence. For a comprehensive perspective of the potential of OpInf to construct accurate ROMs for this model, we consider a setup for the HW equations that leads to the formation of complex, nonlinear, and self-driven dynamics, and perform two sets of experiments. We first use the data obtained via a direct numerical simulation of the HW equations starting from a specific initial condition and train OpInf ROMs for predictions beyond the training time horizon. In the second, more challenging set of experiments, we train ROMs using the same datase
    
[^20]: 空间感知深度强化学习在巡警问题中的应用

    Spatial-Aware Deep Reinforcement Learning for the Traveling Officer Problem. (arXiv:2401.05969v1 [cs.LG])

    [http://arxiv.org/abs/2401.05969](http://arxiv.org/abs/2401.05969)

    这篇论文提出了一种称为SATOP的空间感知深度强化学习方法来解决巡警问题。该方法通过利用停车位、代理和动作之间的空间关系来创建动作的表示，从而动态调整以适应当前可罚款的停车违规行为，并提前计划以提高到达违规行为发生时的可能性。

    

    巡警问题（TOP）是一个具有挑战性的随机优化任务。在这个问题中，一名停车员通过配备停车传感器的城市进行引导，以便尽可能多地罚款违规停车者。TOP的一个主要挑战是停车违规的动态性，这些违规行为会在一段时间后随机出现和消失，不论它们是否被罚款。因此，解决方案需要动态地调整以适应当前可罚款的停车违规行为，同时还要提前计划，增加巡警在违规行为发生时到达的可能性。尽管存在各种解决方案，但这些方法往往难以考虑到行动对未来罚款违规的能力产生的影响。本文提出了SATOP，一种新颖的空间感知深度强化学习方法用于TOP。我们的新型状态编码器创建了每个动作的表示，利用停车位、代理和动作之间的空间关系。

    The traveling officer problem (TOP) is a challenging stochastic optimization task. In this problem, a parking officer is guided through a city equipped with parking sensors to fine as many parking offenders as possible. A major challenge in TOP is the dynamic nature of parking offenses, which randomly appear and disappear after some time, regardless of whether they have been fined. Thus, solutions need to dynamically adjust to currently fineable parking offenses while also planning ahead to increase the likelihood that the officer arrives during the offense taking place. Though various solutions exist, these methods often struggle to take the implications of actions on the ability to fine future parking violations into account. This paper proposes SATOP, a novel spatial-aware deep reinforcement learning approach for TOP. Our novel state encoder creates a representation of each action, leveraging the spatial relationships between parking spots, the agent, and the action. Furthermore, we
    
[^21]: 从PixelCNN的潜在空间中尝试生成新的桥梁类型

    An attempt to generate new bridge types from latent space of PixelCNN. (arXiv:2401.05964v1 [cs.LG])

    [http://arxiv.org/abs/2401.05964](http://arxiv.org/abs/2401.05964)

    本论文尝试使用生成式人工智能技术生成新的桥梁类型，通过对潜在空间进行采样，可以在人类原始桥梁类型的基础上组合不同的结构组件，创造具有一定创新能力的新桥梁类型。

    

    本论文尝试使用生成式人工智能技术生成新的桥梁类型。使用三跨梁桥、拱桥、斜拉桥和悬索桥的对称结构图像数据集作为训练集，基于Python编程语言、TensorFlow和Keras深度学习平台构建和训练PixelCNN模型。该模型能够捕捉图像的统计结构，并在给定前一像素的情况下计算下一个像素的概率分布。通过对获得的潜在空间进行采样，可以生成与训练集中不同的桥梁类型。PixelCNN可以在人类原始桥梁类型的基础上有机地组合不同的结构组件，创建具有一定人类创新能力的新桥梁类型。自回归模型无法理解序列的含义，而多模态模型结合回归和自回归模型，能够理解序列。多模态模型应该是...

    Try to generate new bridge types using generative artificial intelligence technology. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained. The model can capture the statistical structure of the images and calculate the probability distribution of the next pixel when the previous pixels are given. From the obtained latent space sampling, new bridge types different from the training dataset can be generated. PixelCNN can organically combine different structural components on the basis of human original bridge types, creating new bridge types that have a certain degree of human original ability. Autoregressive models cannot understand the meaning of the sequence, while multimodal models combine regression and autoregressive models to understand the sequence. Multimodal models should be the
    
[^22]: 从Transformer表示中学习认知地图以实现部分观察环境中的高效规划

    Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments. (arXiv:2401.05946v1 [cs.LG])

    [http://arxiv.org/abs/2401.05946](http://arxiv.org/abs/2401.05946)

    本文提出了一种从Transformer表示中学习认知地图的方法，该方法针对部分观察环境中的路径规划问题提供了一个有效的解决方案。

    

    尽管在各种任务中表现出色，包括在推理过程中仅透露的上下文任务，但标准的Transformer和针对下一个标记预测训练的变体(a)没有学习到明确的环境世界模型，该模型可以灵活查询，(b)不能用于规划或导航。本文考虑部分观察环境（POEs），代理根据其导航时接收到的感知别名观察，这使得路径规划变得困难。我们引入了一个具有（多个）离散瓶颈的Transformer，TDB，其潜在代码可以学习观察和动作历史的压缩表示。在训练TDB以预测给定历史的未来观察后，我们从其活跃的瓶颈索引中提取环境的可解释认知地图。然后，将这些地图与外部求解器配对以解决（受限制的）路径规划问题。首先，我们展示了在POEs上训练的TDB可以保留...

    Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains
    
[^23]: HIV/AIDS在菲律宾的时间序列预测：深度学习以及COVID-19疫情的影响

    Time Series Forecasting of HIV/AIDS in the Philippines Using Deep Learning: Does COVID-19 Epidemic Matter?. (arXiv:2401.05933v1 [cs.NE])

    [http://arxiv.org/abs/2401.05933](http://arxiv.org/abs/2401.05933)

    此研究使用多层感知器神经网络预测了菲律宾HIV/AIDS的时间序列，研究发现COVID-19疫情对HIV流行没有明显的影响，并预测到2030年该国的累积病例将达到145,273例。

    

    菲律宾的HIV/AIDS疫情在西太平洋地区传播速度最快，2010年至2021年间HIV发病率增长了676%。尽管COVID-19对HIV服务和发展的完全影响尚不明确，但预计这种干扰可能会导致HIV死亡人数显著增加。因此，该国需要一些建模和预测技术来预测传播模式并改善政府的预防、治疗、检测和护理计划。本研究使用多层感知器神经网络，使用菲律宾HIV/AIDS和ART注册处的统计数据，在COVID-19大流行袭击该国的期间预测时间序列。经过数据的训练、验证和测试，研究发现到2030年，该国的累积病例预计将达到145,273例。此外，观察值与预期的HIV流行之间几乎没有差异。

    With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDS epidemic in the Philippines is the one that is spreading the quickest in the western Pacific. Although the full effects of COVID-19 on HIV services and development are still unknown, it is predicted that such disruptions could lead to a significant increase in HIV casualties. Therefore, the nation needs some modeling and forecasting techniques to foresee the spread pattern and enhance the governments prevention, treatment, testing, and care program. In this study, the researcher uses Multilayer Perceptron Neural Network to forecast time series during the period when the COVID-19 pandemic strikes the nation, using statistics taken from the HIV/AIDS and ART Registry of the Philippines. After training, validation, and testing of data, the study finds that the predicted cumulative cases in the nation by 2030 will reach 145,273. Additionally, there is very little difference between observed and anticipated HIV epid
    
[^24]: EpilepsyLLM: 使用癫痫医学知识进行领域特定的大型语言模型微调

    EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge. (arXiv:2401.05908v1 [cs.CL])

    [http://arxiv.org/abs/2401.05908](http://arxiv.org/abs/2401.05908)

    本研究提出了一种用于癫痫疾病的定制化大型语言模型EpilepsyLLM，通过微调预训练的LLM并使用癫痫领域的数据集进行训练。通过该模型，可以更准确地回答与癫痫相关的问题，尤其适用于使用日语进行研究。

    

    大型语言模型（LLMs）凭借庞大的训练数据集和大量的计算资源，在综合和生成能力方面取得了显著的性能。基于这些强大的LLMs，通过领域特定的数据集进行微调，模型拥有更专业的知识，因此更实用，比如医学LLMs。然而，现有的经过微调的医学LLMs仅限于通用的英文医学知识。对于特定疾病的问题，模型的响应是不准确的，有时甚至完全不相关，特别是在使用非英文语言时。在本研究中，我们专注于使用日语进行癫痫疾病的研究，并引入了一种定制的LLM，称为EpilepsyLLM。我们的模型使用来自癫痫领域的数据集通过微调技术从预训练的LLM进行训练，其中包含有关疾病基本信息、常见治疗方法和药物以及生活和工作中的重要注意事项的知识。

    With large training datasets and massive amounts of computing sources, large language models (LLMs) achieve remarkable performance in comprehensive and generative ability. Based on those powerful LLMs, the model fine-tuned with domain-specific datasets posseses more specialized knowledge and thus is more practical like medical LLMs. However, the existing fine-tuned medical LLMs are limited to general medical knowledge with English language. For disease-specific problems, the model's response is inaccurate and sometimes even completely irrelevant, especially when using a language other than English. In this work, we focus on the particular disease of Epilepsy with Japanese language and introduce a customized LLM termed as EpilepsyLLM. Our model is trained from the pre-trained LLM by fine-tuning technique using datasets from the epilepsy domain. The datasets contain knowledge of basic information about disease, common treatment methods and drugs, and important notes in life and work. The
    
[^25]: 乐观模型预测用于悲观离线策略优化

    Optimistic Model Rollouts for Pessimistic Offline Policy Optimization. (arXiv:2401.05899v1 [cs.LG])

    [http://arxiv.org/abs/2401.05899](http://arxiv.org/abs/2401.05899)

    这篇论文提出了一种基于模型的离线强化学习框架ORPO，通过构建乐观MDP来鼓励更多超出离线数据集的模型预测，从而提升泛化能力。

    

    基于模型的离线强化学习（RL）取得了显著的进展，为通过合成模型预测来改善泛化能力提供了一种有前途的途径。现有的研究主要集中在通过构建悲观马尔科夫决策过程（P-MDP）来将悲观主义纳入策略优化中。然而，P-MDP限制策略在超出离线数据集支持的分布区域学习，从而可能没有充分利用动力模型的泛化能力。相反，我们提出了构建乐观MDP（O-MDP）的方法。我们最初观察到通过鼓励更多超出离线数据集的模型预测可以带来乐观主义的潜在好处。受此观察的启发，我们提出了ORPO，一个简单而有效的基于模型的离线RL框架。ORPO通过在O-MDP中训练乐观的模型预测策略来采样更多超出离线数据集的模型预测。然后我们重新为这些预测打上标签，并通过悲观主义的离线策略优化来提升性能。

    Model-based offline reinforcement learning (RL) has made remarkable progress, offering a promising avenue for improving generalization with synthetic model rollouts. Existing works primarily focus on incorporating pessimism for policy optimization, usually via constructing a Pessimistic Markov Decision Process (P-MDP). However, the P-MDP discourages the policies from learning in out-of-distribution (OOD) regions beyond the support of offline datasets, which can under-utilize the generalization ability of dynamics models. In contrast, we propose constructing an Optimistic MDP (O-MDP). We initially observed the potential benefits of optimism brought by encouraging more OOD rollouts. Motivated by this observation, we present ORPO, a simple yet effective model-based offline RL framework. ORPO generates Optimistic model Rollouts for Pessimistic offline policy Optimization. Specifically, we train an optimistic rollout policy in the O-MDP to sample more OOD model rollouts. Then we relabel the
    
[^26]: 深度学习在智能电网网络中推进主动式网络安全措施的作用：一项调查

    The Role of Deep Learning in Advancing Proactive Cybersecurity Measures for Smart Grid Networks: A Survey. (arXiv:2401.05896v1 [cs.CR])

    [http://arxiv.org/abs/2401.05896](http://arxiv.org/abs/2401.05896)

    深度学习在智能电网网络中起到推进主动网络安全措施的作用，本论文通过综合调查了最新的深度学习技术在主动网络防御中的应用。

    

    随着智能电网（SG）越来越依赖传感器和通信系统等先进技术来实现高效的能源发电、分配和消耗，它们成为了复杂网络攻击的有吸引力的目标。这些不断进化的威胁需要强大的安全措施来维持现代能源系统的稳定性和韧性。虽然进行了广泛的研究，但关于利用深度学习（DL）在SG中实施主动网络防御策略的综合性探索在文献中仍然很少见。本调查填补了这一空白，研究了用于主动网络防御的最新DL技术。调查首先概述了相关工作和我们独特的贡献，然后对SG基础设施进行了检查。接下来，我们将各种网络防御技术分为反应性和主动性两个类别。我们重点关注DL-enabled的主动防御，提供了DL方法的全面分类，突出了它们的角色和

    As smart grids (SG) increasingly rely on advanced technologies like sensors and communication systems for efficient energy generation, distribution, and consumption, they become enticing targets for sophisticated cyberattacks. These evolving threats demand robust security measures to maintain the stability and resilience of modern energy systems. While extensive research has been conducted, a comprehensive exploration of proactive cyber defense strategies utilizing Deep Learning (DL) in {SG} remains scarce in the literature. This survey bridges this gap, studying the latest DL techniques for proactive cyber defense. The survey begins with an overview of related works and our distinct contributions, followed by an examination of SG infrastructure. Next, we classify various cyber defense techniques into reactive and proactive categories. A significant focus is placed on DL-enabled proactive defenses, where we provide a comprehensive taxonomy of DL approaches, highlighting their roles and
    
[^27]: 基于二进制线性树提交的分布式机器学习所有权保护

    Binary Linear Tree Commitment-based Ownership Protection for Distributed Machine Learning. (arXiv:2401.05895v1 [cs.LG])

    [http://arxiv.org/abs/2401.05895](http://arxiv.org/abs/2401.05895)

    本论文提出了一种基于二进制线性树提交的分布式机器学习所有权保护模型，通过验证计算的完整性和效果，解决了模型所有权的冲突问题，并降低了更新证明的成本。

    

    分布式机器学习通过将计算任务分配给多个工作节点，实现了对大规模数据集的并行训练。但是，最终模型权重的传播往往会导致模型所有权的潜在冲突，因为工作节点难以证明自己在训练计算中的参与度。为了解决上述所有权问题，并防止意外故障和恶意攻击，在分布式机器学习中验证工作节点的计算完整性和效果变得尤为重要。本文提出了一种新的基于二进制线性树提交的所有权保护模型，以确保计算的完整性，同时保证开销有限和证明简洁。由于训练过程中参数的频繁更新，我们的提交方案引入了可维护的树结构，降低了更新证明的成本。与基于SNARK的验证计算不同，我们的模型可以同时支持批量训练和在线训练。

    Distributed machine learning enables parallel training of extensive datasets by delegating computing tasks across multiple workers. Despite the cost reduction benefits of distributed machine learning, the dissemination of final model weights often leads to potential conflicts over model ownership as workers struggle to substantiate their involvement in the training computation. To address the above ownership issues and prevent accidental failures and malicious attacks, verifying the computational integrity and effectiveness of workers becomes particularly crucial in distributed machine learning. In this paper, we proposed a novel binary linear tree commitment-based ownership protection model to ensure computational integrity with limited overhead and concise proof. Due to the frequent updates of parameters during training, our commitment scheme introduces a maintainable tree structure to reduce the costs of updating proofs. Distinguished from SNARK-based verifiable computation, our mod
    
[^28]: 在不确定环境中的安全强化学习

    Safe reinforcement learning in uncertain contexts. (arXiv:2401.05876v1 [cs.LG])

    [http://arxiv.org/abs/2401.05876](http://arxiv.org/abs/2401.05876)

    本文提出了一种在不确定环境中进行安全强化学习的方法，通过推导频率保证来估计当前上下文，并通过实验来识别上下文变量。

    

    在实际应用中，保证安全是机器学习算法的重要方面。现有的安全学习方法通常考虑连续变量，即回归任务。然而，在实践中，机器人系统也会受到离散的外部环境变化的影响，例如必须携带特定重量的物体或在冰冻、湿润或干燥的表面上操作。这些影响可以建模为离散的上下文变量。在现有文献中，如果考虑这些上下文，大多数情况下假设它们是已知的。在这项工作中，我们放弃了这个假设，展示了在无法直接测量上下文变量的情况下如何进行安全学习。为了实现这一点，我们针对多类分类推导了频率保证，从测量值中估计当前上下文。此外，我们提出了一种通过实验来识别上下文的方法。我们讨论了在哪些条件下可以保留理论保证，并加以证明。

    When deploying machine learning algorithms in the real world, guaranteeing safety is an essential asset. Existing safe learning approaches typically consider continuous variables, i.e., regression tasks. However, in practice, robotic systems are also subject to discrete, external environmental changes, e.g., having to carry objects of certain weights or operating on frozen, wet, or dry surfaces. Such influences can be modeled as discrete context variables. In the existing literature, such contexts are, if considered, mostly assumed to be known. In this work, we drop this assumption and show how we can perform safe learning when we cannot directly measure the context variables. To achieve this, we derive frequentist guarantees for multi-class classification, allowing us to estimate the current context from measurements. Further, we propose an approach for identifying contexts through experiments. We discuss under which conditions we can retain theoretical guarantees and demonstrate the 
    
[^29]: 通过加速度计数据推断讲话意图——野外环境中的研究

    Inferring Intentions to Speak Using Accelerometer Data In-the-Wild. (arXiv:2401.05849v1 [cs.LG])

    [http://arxiv.org/abs/2401.05849](http://arxiv.org/abs/2401.05849)

    通过加速度计数据推断成功和失败的讲话意图，在野外环境中的研究表明加速度计数据中存在有用的信息，但不足以可靠地捕捉讲话意图。

    

    人类具有良好的自然直觉，可以识别出他人有话要说的时候。如果人工智能也能识别出讲话意图，将会非常有趣。特别是在人工智能引导团体讨论的场景下，这将是一项有用的技能。本研究探讨了通过加速度计数据推断成功和失败的讲话意图。之所以选择加速度计数据，是因为它具有隐私保护功能，同时在野外环境中易于实现，可以放置在智能徽章上。使用真实社交网络事件的数据来训练一个机器学习模型，旨在推断讲话意图。数据中的一部分不成功的讲话意图案例被注释。模型在成功的讲话意图上进行训练，并在成功和失败的案例上进行评估。总之，加速度计数据中存在有用的信息，但不足以可靠地捕捉讲话意图。例如，姿势变化与讲话意图具有相关性。

    Humans have good natural intuition to recognize when another person has something to say. It would be interesting if an AI can also recognize intentions to speak. Especially in scenarios when an AI is guiding a group discussion, this can be a useful skill. This work studies the inference of successful and unsuccessful intentions to speak from accelerometer data. This is chosen because it is privacy-preserving and feasible for in-the-wild settings since it can be placed in a smart badge. Data from a real-life social networking event is used to train a machine-learning model that aims to infer intentions to speak. A subset of unsuccessful intention-to-speak cases in the data is annotated. The model is trained on the successful intentions to speak and evaluated on both the successful and unsuccessful cases. In conclusion, there is useful information in accelerometer data, but not enough to reliably capture intentions to speak. For example, posture shifts are correlated with intentions to 
    
[^30]: 推动带隙和介电常数的帕累托前沿：基于机器学习引导的寻找介电材料的研究

    Pushing the Pareto front of band gap and permittivity: ML-guided search for dielectric materials. (arXiv:2401.05848v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2401.05848](http://arxiv.org/abs/2401.05848)

    这项研究通过使用多目标优化方法和机器学习技术，在介电材料研究中取得了突破。研究人员成功地合成和表征了两种新型介电材料，CsTaTeO6和Bi2Zr2O7，为未知材料的寻找提供了高效的工作流程。

    

    高介电常数的材料在外部电场下容易极化，在许多现代电子设备中发挥着重要的功能。它们的实际效用由两个相互冲突的特性决定：高介电常数往往出现在带隙较窄的材料中，限制了介电击穿之前的工作电压。我们提出了一个高通量的工作流程，将元素替代、机器学习预筛选、从头计算模拟和人类专家直觉相结合，以有效地探索未知材料的潜在介电性能，从而合成和表征两种新型的介电材料CsTaTeO6和Bi2Zr2O7。我们的关键思想是在凸面帕累托前沿的多目标优化环境中应用机器学习。虽然通常被认为比单目标优化更具挑战性，但我们提出并展示了初步证据表明带隙和介电常数之间的1/x相关性实际上使任务变      得容易。

    Materials with high-dielectric constant easily polarize under external electric fields, allowing them to perform essential functions in many modern electronic devices. Their practical utility is determined by two conflicting properties: high dielectric constants tend to occur in materials with narrow band gaps, limiting the operating voltage before dielectric breakdown. We present a high-throughput workflow that combines element substitution, ML pre-screening, ab initio simulation and human expert intuition to efficiently explore the vast space of unknown materials for potential dielectrics, leading to the synthesis and characterization of two novel dielectric materials, CsTaTeO6 and Bi2Zr2O7. Our key idea is to deploy ML in a multi-objective optimization setting with concave Pareto front. While usually considered more challenging than single-objective optimization, we argue and show preliminary evidence that the $1/x$-correlation between band gap and permittivity in fact makes the tas
    
[^31]: 重新审视轮廓系数：从微观到宏观聚合。

    Revisiting Silhouette: From Micro to Macro Aggregation. (arXiv:2401.05831v1 [cs.LG])

    [http://arxiv.org/abs/2401.05831](http://arxiv.org/abs/2401.05831)

    本文提出了一种新的聚合策略，用于评估聚类质量。通过对聚类级别的轮廓得分进行平均，并在此基础上对所有聚类的得分进行宏观平均，我们提出的宏观平均轮廓得分对于聚类不均衡和背景噪声是稳健的。

    

    轮廓系数是一种常用的内部聚类评估指标，它会为每个数据点产生一个得分，用于评估其聚类分配的质量。目前，为了评估整个数据集的聚类质量，通常会将数据集中所有点的得分平均成一个单一值，这个策略被称为微观平均。然而我们在本文中通过使用一个合成例子展示了，该微观平均策略对于聚类不均衡和异常值（背景噪声）非常敏感。为了解决这些问题，我们提出了一种替代聚合策略，该策略首先对聚类级别的轮廓得分进行平均，然后再对所有聚类的得分进行宏观平均。基于相同的合成例子，我们展示了提出的宏观平均轮廓得分对于聚类不均衡和背景噪声是稳健的。我们进行了实验研究，结果表明我们提出的宏观平均变体可以更好地估计真实的聚类数量。

    Silhouette coefficient is an established internal clustering evaluation measure that produces a score per data point, assessing the quality of its clustering assignment. To assess the quality of the clustering of the whole dataset, the scores of all the points in the dataset are typically averaged into a single value, a strategy which we call as micro-averaging. As we illustrate in this work, by using a synthetic example, this micro-averaging strategy is sensitive both to cluster imbalance and outliers (background noise). To address these issues, we propose an alternative aggregation strategy, which first averages the silhouette scores at a cluster level and then (macro) averages the scores across the clusters. Based on the same synthetic example, we show that the proposed macro-averaged silhouette score is robust to cluster imbalance and background noise. We have conducted an experimental study showing that our macro-averaged variant provides better estimates of the ground truth numbe
    
[^32]: 可解释概念瓶颈用于对齐强化学习代理

    Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents. (arXiv:2401.05821v1 [cs.LG])

    [http://arxiv.org/abs/2401.05821](http://arxiv.org/abs/2401.05821)

    SCoBots是一种可解释的概念瓶颈代理，能够透明化整个决策流程，帮助领域专家理解和规范强化学习代理的行为，从而可能实现更好的人类对齐强化学习。

    

    奖励稀疏性、难以归因的问题以及不对齐等等都是深度强化学习代理学习最优策略困难甚至不可能的原因之一。不幸的是，深度网络的黑盒特性阻碍了领域专家的参与，这些专家可以解释模型并纠正错误行为。为此，我们引入了连续概念瓶颈代理（SCoBots），通过整合连续的概念瓶颈层，使整个决策流程透明化。SCoBots不仅利用相关的对象属性，还利用关系概念。实验结果强有力地证明，SCoBots使领域专家能够有效理解和规范他们的行为，从而可能实现更好的人类对齐强化学习。通过这种方式，SCoBots使我们能够识别最简单且具有代表性的视频游戏Pong中的不对齐问题，并加以解决。

    Reward sparsity, difficult credit assignment, and misalignment are only a few of the many issues that make it difficult, if not impossible, for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep networks impedes the inclusion of domain experts who could interpret the model and correct wrong behavior. To this end, we introduce Successive Concept Bottlenecks Agents (SCoBots), which make the whole decision pipeline transparent via the integration of consecutive concept bottleneck layers. SCoBots make use of not only relevant object properties but also of relational concepts. Our experimental results provide strong evidence that SCoBots allow domain experts to efficiently understand and regularize their behavior, resulting in potentially better human-aligned RL. In this way, SCoBots enabled us to identify a misalignment problem in the most simple and iconic video game, Pong, and resolve it.
    
[^33]: 阻抗记忆中的噪声对图像分类深度神经网络的影响

    Implications of Noise in Resistive Memory on Deep Neural Networks for Image Classification. (arXiv:2401.05820v1 [cs.LG])

    [http://arxiv.org/abs/2401.05820](http://arxiv.org/abs/2401.05820)

    本研究探索了阻抗记忆中的噪声对基于神经网络的图像分类任务的影响，并提出了一些提高弹性的对策。

    

    阻抗存储是一种有前景的SRAM替代品，但也是一种固有不稳定的设备，需要大量的努力确保正确的读写操作。为了避免在面积、时间和能量方面的相关成本，本研究致力于探索基于神经网络的图像分类任务可以容忍多少内存操作中的噪声。我们引入了一种特殊的噪声操作符，模拟示例阻抗存储单元中的噪声，探索了卷积神经网络在CIFAR-10分类任务上的弹性，并讨论了一些提高这种弹性的对策。

    Resistive memory is a promising alternative to SRAM, but is also an inherently unstable device that requires substantial effort to ensure correct read and write operations. To avoid the associated costs in terms of area, time and energy, the present work is concerned with exploring how much noise in memory operations can be tolerated by image classification tasks based on neural networks. We introduce a special noisy operator that mimics the noise in an exemplary resistive memory unit, explore the resilience of convolutional neural networks on the CIFAR-10 classification task, and discuss a couple of countermeasures to improve this resilience.
    
[^34]: TAnet: 一种新的基于脑电信号的听觉空间注意力解码的时间注意力网络

    TAnet: A New Temporal Attention Network for EEG-based Auditory Spatial Attention Decoding with a Short Decision Window. (arXiv:2401.05819v1 [eess.SP])

    [http://arxiv.org/abs/2401.05819](http://arxiv.org/abs/2401.05819)

    TAnet是一种新的基于脑电信号的听觉空间注意力解码网络，采用了多头注意力机制，可以更有效地捕捉脑电信号中时间步之间的交互作用，并提供了比传统方法更好的解码性能。

    

    听觉空间注意力检测（ASAD）通过分析电脑脑电信号来确定听众对说话者的注意方向。本研究旨在改进ASAD的性能，使用较短的决策窗口（小于1秒），而不是以前研究中使用的较长决策窗口。本文介绍了一种端到端的时间注意力网络（即TAnet）。TAnet采用多头注意力机制，可以更有效地捕捉采集到的脑电信号中时间步之间的相互作用，并为这些时间步分配相应的权重。实验表明，与基于CNN的方法和最近的ASAD方法相比，TAnet在KUL数据集中提供了改进的解码性能，使用较短的决策窗口（即小于1秒）的情况下，解码准确率分别为92.4%（决策窗口0.1秒）、94.9%（0.25秒）、95.1%（0.3秒）、95.4%（0.4秒）和95.5%（0.5秒）。

    Auditory spatial attention detection (ASAD) is used to determine the direction of a listener's attention to a speaker by analyzing her/his electroencephalographic (EEG) signals. This study aimed to further improve the performance of ASAD with a short decision window (i.e., <1 s) rather than with long decision windows in previous studies. An end-to-end temporal attention network (i.e., TAnet) was introduced in this work. TAnet employs a multi-head attention (MHA) mechanism, which can more effectively capture the interactions among time steps in collected EEG signals and efficiently assign corresponding weights to those EEG time steps. Experiments demonstrated that, compared with the CNN-based method and recent ASAD methods, TAnet provided improved decoding performance in the KUL dataset, with decoding accuracies of 92.4% (decision window 0.1 s), 94.9% (0.25 s), 95.1% (0.3 s), 95.4% (0.4 s), and 95.5% (0.5 s) with short decision windows (i.e., <1 s). As a new ASAD model with a short deci
    
[^35]: Cheetah: 通过高速可微分模拟填补机器学习和粒子加速器物理之间的差距

    Cheetah: Bridging the Gap Between Machine Learning and Particle Accelerator Physics with High-Speed, Differentiable Simulations. (arXiv:2401.05815v1 [physics.acc-ph])

    [http://arxiv.org/abs/2401.05815](http://arxiv.org/abs/2401.05815)

    Cheetah是一种高速可微分模拟工具，可以减少计算时间并实现快速收集大规模数据集。它能够促进加速器调优和系统识别，并与机器学习工具无缝集成。

    

    机器学习已经成为加速器物理学中现代挑战的强大解决方案。然而，有限的束流时间可用性，模拟的计算成本以及优化问题的高维度给生成所需训练机器学习模型的数据带来了重大挑战。在这项工作中，我们引入了Cheetah，一种基于PyTorch的高速可微线性束流动力学代码。Cheetah通过将计算时间减少数个数量级，实现了快速收集大规模数据集，并促进了面向加速器调优和系统识别的高效梯度优化。这使Cheetah成为一个用户友好、易于扩展的工具，与广泛采用的机器学习工具无缝集成。我们通过五个示例展示了Cheetah的实用性，包括强化学习训练、基于梯度的束线调优、基于梯度的系统识别、物理学-i

    Machine learning has emerged as a powerful solution to the modern challenges in accelerator physics. However, the limited availability of beam time, the computational cost of simulations, and the high-dimensionality of optimisation problems pose significant challenges in generating the required data for training state-of-the-art machine learning models. In this work, we introduce Cheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code. Cheetah enables the fast collection of large data sets by reducing computation times by multiple orders of magnitude and facilitates efficient gradient-based optimisation for accelerator tuning and system identification. This positions Cheetah as a user-friendly, readily extensible tool that integrates seamlessly with widely adopted machine learning tools. We showcase the utility of Cheetah through five examples, including reinforcement learning training, gradient-based beamline tuning, gradient-based system identification, physics-i
    
[^36]: 基于图形时空过程的多变量时间序列异常检测方法及其在缺失值情况下的应用

    Graph Spatiotemporal Process for Multivariate Time Series Anomaly Detection with Missing Values. (arXiv:2401.05800v1 [cs.LG])

    [http://arxiv.org/abs/2401.05800](http://arxiv.org/abs/2401.05800)

    该论文介绍了一种名为GST-Pro的框架，利用图形时空过程和异常评分器来解决在采样不规则的多变量时间序列中检测异常的挑战。

    

    对多变量时间序列数据中的异常进行检测对于各种实际应用非常重要，包括智能电网、交通流预测和工业过程控制等。然而，真实世界的时间序列数据通常不是良好结构化的，这给现有方法带来了重大挑战：（1）多变量时间序列数据中的缺失值存在于变量和时间维度，阻碍了对交织的空间和时间依赖关系进行有效建模，导致重要的模式在模型训练过程中被忽视；（2）在采样不规则的观测下进行异常评分的研究较少，这使得在多变量序列中使用现有检测器时很难处理没有完全观测值的情况。在本文中，我们介绍了一种名为GST-Pro的新框架，该框架利用图形时空过程和异常评分器来解决在采样不规则的多变量时间序列中检测异常时面临的挑战。

    The detection of anomalies in multivariate time series data is crucial for various practical applications, including smart power grids, traffic flow forecasting, and industrial process control. However, real-world time series data is usually not well-structured, posting significant challenges to existing approaches: (1) The existence of missing values in multivariate time series data along variable and time dimensions hinders the effective modeling of interwoven spatial and temporal dependencies, resulting in important patterns being overlooked during model training; (2) Anomaly scoring with irregularly-sampled observations is less explored, making it difficult to use existing detectors for multivariate series without fully-observed values. In this work, we introduce a novel framework called GST-Pro, which utilizes a graph spatiotemporal process and anomaly scorer to tackle the aforementioned challenges in detecting anomalies on irregularly-sampled multivariate time series. Our approac
    
[^37]: 错误有界在线学习中反馈价格的界限

    Bounds on the price of feedback for mistake-bounded online learning. (arXiv:2401.05794v1 [cs.LG])

    [http://arxiv.org/abs/2401.05794](http://arxiv.org/abs/2401.05794)

    改进了错误有界在线学习中反馈价格的上下界，还解决了多类学习中标准反馈与赌徒反馈的价格问题。

    

    我们改进了(Auer和Long，Machine Learning，1999)中各种在线学习场景的若干最坏情况界限。特别地，我们将延迟模棱两可强化学习的上界缩小了2倍，将函数族组合学习的上界缩小了2.41倍，将不知性学习的上界缩小了1.09倍。我们还改进了同一论文中函数族组合学习的下界，将其缩小到Θ(ln{k})的因子，与上界相匹配。此外，我们解决了(Lon，Theoretical Computer Science，2020)中关于多类学习标准反馈与赌徒反馈价格的问题，并将(Feng等人，Theoretical Computer Science，2023)中$r$-输入延迟模棱两可强化学习的上界缩小了$r$倍，与同一论文中的下界相匹配。

    We improve several worst-case bounds for various online learning scenarios from (Auer and Long, Machine Learning, 1999). In particular, we sharpen an upper bound for delayed ambiguous reinforcement learning by a factor of 2, an upper bound for learning compositions of families of functions by a factor of 2.41, and an upper bound for agnostic learning by a factor of 1.09. We also improve a lower bound from the same paper for learning compositions of $k$ families of functions by a factor of $\Theta(\ln{k})$, matching the upper bound up to a constant factor. In addition, we solve a problem from (Long, Theoretical Computer Science, 2020) on the price of bandit feedback with respect to standard feedback for multiclass learning, and we improve an upper bound from (Feng et al., Theoretical Computer Science, 2023) on the price of $r$-input delayed ambiguous reinforcement learning by a factor of $r$, matching a lower bound from the same paper up to the leading term.
    
[^38]: 发现用于跨语言不可知多语言表示的低秩子空间

    Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations. (arXiv:2401.05792v1 [cs.CL])

    [http://arxiv.org/abs/2401.05792](http://arxiv.org/abs/2401.05792)

    本论文提出了一种从多语言嵌入空间中投影语言特定因素的新视角，并通过发现一个低秩子空间来消除与语义无关的信息，从而充分利用语义信息。

    

    大型预训练的多语言语言模型（ML-LMs）展示出了在无直接跨语言监督的情况下具有卓越的零样本跨语言转换能力。然而，随后的研究发现，在多语言嵌入空间中存在强烈的语言身份信息，这阻碍了语言间共享的语言因素的表达。对于跨语言句子检索等语义任务，希望消除这种语言身份信号，充分利用语义信息。在这项工作中，我们提供了从多语言嵌入空间中投影语言特定因素的新视角。具体来说，我们发现存在一个低秩子空间，该子空间主要编码与语义无关的信息（例如，句法信息）。为了识别该子空间，我们提出了一个简单但有效的无监督方法，该方法基于奇异值分解，将多语言语料库作为输入。一旦找到了该子空间，我们可以通过投影将信息向该子空间的零空间投影，从而获得消除了语言特定因素的语义信息。

    Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong language identity information which hinders the expression of linguistic factors shared across languages. For semantic tasks like cross-lingual sentence retrieval, it is desired to remove such language identity signals to fully leverage semantic information. In this work, we provide a novel view of projecting away language-specific factors from a multilingual embedding space. Specifically, we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information). To identify this subspace, we present a simple but effective unsupervised method based on singular value decomposition with multiple monolingual corpora as input. Once the s
    
[^39]: 知识转化：一种用于模型压缩的新途径

    Knowledge Translation: A New Pathway for Model Compression. (arXiv:2401.05772v1 [cs.LG])

    [http://arxiv.org/abs/2401.05772](http://arxiv.org/abs/2401.05772)

    本文提出了一种新的知识转化（KT）框架，通过训练一个“翻译”模型来接收较大模型的参数并生成压缩参数，从而实现模型压缩，而无需重新训练或施加架构限制。

    

    深度学习在近年来取得了显著的进展，但训练、推理和模型存储开销却在不断增加。尽管现有的模型压缩方法致力于在保持高准确性的同时减少模型参数的数量，但它们不可避免地需要重新训练压缩模型或施加架构限制。为了克服这些限制，本文提出了一种新的框架，称为知识转化（KT），其中训练一个“翻译”模型来接收较大模型的参数并生成压缩参数。知识转化的概念借鉴自语言翻译，它有效地利用神经网络将不同的语言转换为相同的意思。因此，本文探索了神经网络将不同大小的模型转换为保持其功能性的潜力。我们提出了一个全面的KT框架，介绍了数据增强策略。

    Deep learning has witnessed significant advancements in recent years at the cost of increasing training, inference, and model storage overhead. While existing model compression methods strive to reduce the number of model parameters while maintaining high accuracy, they inevitably necessitate the re-training of the compressed model or impose architectural constraints. To overcome these limitations, this paper presents a novel framework, termed \textbf{K}nowledge \textbf{T}ranslation (KT), wherein a ``translation'' model is trained to receive the parameters of a larger model and generate compressed parameters. The concept of KT draws inspiration from language translation, which effectively employs neural networks to convert different languages, maintaining identical meaning. Accordingly, we explore the potential of neural networks to convert models of disparate sizes, while preserving their functionality. We propose a comprehensive framework for KT, introduce data augmentation strategie
    
[^40]: 功能数据分类的特征选择

    Feature Selection for Functional Data Classification. (arXiv:2401.05765v1 [stat.ML])

    [http://arxiv.org/abs/2401.05765](http://arxiv.org/abs/2401.05765)

    本文介绍了一种名为FSFC的新方法，它解决了在具有分类响应和纵向特征的情况下同时进行功能数据特征选择和分类的挑战。

    

    功能数据分析已经成为许多需要整合和解释复杂数据的当代科学领域中的关键工具。此外，新技术的出现促进了大量纵向变量的收集，使得特征选择对于避免过拟合和提高预测性能至关重要。本文介绍了一种名为FSFC（功能分类特征选择）的新方法，它解决了在具有分类响应和纵向特征的情况下同时进行功能数据特征选择和分类的挑战。我们的方法解决了一个新定义的优化问题，将逻辑损失和功能特征结合起来，以识别用于分类的最关键特征。为了解决最小化过程，我们使用功能主成分，并开发了一种新的自适应版本的双增广Lagrange算法，利用了。。。

    Functional data analysis has emerged as a crucial tool in many contemporary scientific domains that require the integration and interpretation of complex data. Moreover, the advent of new technologies has facilitated the collection of a large number of longitudinal variables, making feature selection pivotal for avoiding overfitting and improving prediction performance. This paper introduces a novel methodology called FSFC (Feature Selection for Functional Classification), that addresses the challenge of jointly performing feature selection and classification of functional data in scenarios with categorical responses and longitudinal features. Our approach tackles a newly defined optimization problem that integrates logistic loss and functional features to identify the most crucial features for classification. To address the minimization procedure, we employ functional principal components and develop a new adaptive version of the Dual Augmented Lagrangian algorithm that leverages the 
    
[^41]: HVAC控制的深度强化学习算法的实验评估

    An experimental evaluation of Deep Reinforcement Learning algorithms for HVAC control. (arXiv:2401.05737v1 [cs.LG])

    [http://arxiv.org/abs/2401.05737](http://arxiv.org/abs/2401.05737)

    本论文通过对HVAC控制的几种最先进的深度强化学习算法进行了实验评估，发现SAC和TD3等算法在复杂场景中具有潜力，并揭示了与泛化和增量学习相关的挑战。

    

    暖通空调系统是商业和居住建筑能源消耗的重要驱动因素。最近的研究表明，深度强化学习算法可以胜过传统的反应式控制器。然而，基于深度强化学习的解决方案通常是为特定设置而设计的，并且缺乏可比性的标准。为了填补这一空白，本文采用Sinergym框架，以舒适度和能源消耗为评判标准，对几种最先进的深度强化学习算法在HVAC控制方面进行了关键和可重现的评估。研究通过检查控制器的鲁棒性、适应性和优化目标之间的权衡，确认了SAC和TD3等深度强化学习算法在复杂场景中的潜力，并揭示了与泛化和增量学习相关的几个挑战。

    Heating, Ventilation, and Air Conditioning (HVAC) systems are a major driver of energy consumption in commercial and residential buildings. Recent studies have shown that Deep Reinforcement Learning (DRL) algorithms can outperform traditional reactive controllers. However, DRL-based solutions are generally designed for ad hoc setups and lack standardization for comparison. To fill this gap, this paper provides a critical and reproducible evaluation, in terms of comfort and energy consumption, of several state-of-the-art DRL algorithms for HVAC control. The study examines the controllers' robustness, adaptability, and trade-off between optimization goals by using the Sinergym framework. The results obtained confirm the potential of DRL algorithms, such as SAC and TD3, in complex scenarios and reveal several challenges related to generalization and incremental learning.
    
[^42]: 通过连接主义音素识别中的类熵测量进行分割边界检测

    Segment Boundary Detection via Class Entropy Measurements in Connectionist Phoneme Recognition. (arXiv:2401.05717v1 [eess.AS])

    [http://arxiv.org/abs/2401.05717](http://arxiv.org/abs/2401.05717)

    本文研究了使用连接主义音素识别器的类熵来预测音素类之间的时间边界，并比较了不同方法的精确度和召回率。

    

    本文研究了使用连接主义音素识别器的输出的类熵来预测音素类之间的时间边界的可能性。其原理是，熵的值应该在两个由识别网络很好建模（已知）的片段之间的过渡附近增加，因为它是不确定性的一个度量。类熵的优势在于它的简单性，因为每个类的后验概率在连接主义音素识别中是可用的。熵和一些基于熵差异的度量被单独和组合使用。用于预测边界的决策方法从简单的阈值到基于神经网络的过程不等。不同的方法在精确度方面进行比较，以C（在参考时间的10或20毫秒内预测的边界数量）与预测边界总数之间的比率和召回率为度量。

    This article investigates the possibility to use the class entropy of the output of a connectionist phoneme recogniser to predict time boundaries between phonetic classes. The rationale is that the value of the entropy should increase in proximity of a transition between two segments that are well modelled (known) by the recognition network since it is a measure of uncertainty. The advantage of this measure is its simplicity as the posterior probabilities of each class are available in connectionist phoneme recognition. The entropy and a number of measures based on differentiation of the entropy are used in isolation and in combination. The decision methods for predicting the boundaries range from simple thresholds to neural network based procedure. The different methods are compared with respect to their precision, measured in terms of the ratio between the number C of predicted boundaries within 10 or 20 msec of the reference and the total number of predicted boundaries, and recall, 
    
[^43]: 基于核函数的归一化常数估计：连接贝叶斯积分和贝叶斯优化

    Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature and Bayesian Optimization. (arXiv:2401.05716v1 [cs.LG])

    [http://arxiv.org/abs/2401.05716](http://arxiv.org/abs/2401.05716)

    本文研究了通过查询黑盒函数来估计归一化常数的问题，发现问题的难度取决于问题参数$\lambda$的大小，当$\lambda$趋近于零时类似于贝叶斯积分(BQ)，当$\lambda$趋近于无穷大时类似于贝叶斯优化(BO)，且这种模式适用于存在噪声的情况。结果得到了算法无关的下界和上界的支持，以及模拟研究的验证。

    

    本文研究通过黑盒函数查询来估计归一化常数$\int e^{-\lambda f(x)}dx$的问题，其中$f$属于再生核希尔伯特空间(RKHS)，而$\lambda$是一个问题参数。我们发现，为了在相对误差较小的情况下估计归一化常数，难度的级别取决于$\lambda$的值：当$\lambda$趋近于零时，问题类似于贝叶斯积分(BQ)，而当$\lambda$趋近于无穷大时，问题类似于贝叶斯优化(BO)。更一般地，问题在BQ和BO之间变化。我们发现，即使在函数评估存在噪声的情况下，这种模式仍然适用，为该主题带来了新的方面。我们的发现得到了算法无关的下界和算法上界的支持，以及在各种基准函数上进行的模拟研究。

    In this paper, we study the problem of estimating the normalizing constant $\int e^{-\lambda f(x)}dx$ through queries to the black-box function $f$, where $f$ belongs to a reproducing kernel Hilbert space (RKHS), and $\lambda$ is a problem parameter. We show that to estimate the normalizing constant within a small relative error, the level of difficulty depends on the value of $\lambda$: When $\lambda$ approaches zero, the problem is similar to Bayesian quadrature (BQ), while when $\lambda$ approaches infinity, the problem is similar to Bayesian optimization (BO). More generally, the problem varies between BQ and BO. We find that this pattern holds true even when the function evaluations are noisy, bringing new aspects to this topic. Our findings are supported by both algorithm-independent lower bounds and algorithmic upper bounds, as well as simulation studies conducted on a variety of benchmark functions.
    
[^44]: 基于CSI图像的少样本元学习动态室内指纹定位

    Dynamic Indoor Fingerprinting Localization based on Few-Shot Meta-Learning with CSI Images. (arXiv:2401.05711v1 [cs.LG])

    [http://arxiv.org/abs/2401.05711](http://arxiv.org/abs/2401.05711)

    本论文提出了一种利用数据高效元学习算法的创新室内定位方法，并引入了一种任务加权损失来提高适应性和学习效率。实验证实了该方法在动态室内环境中的稳健性和优越性，在平均欧氏距离上取得了23.13％的显著增益，特别适用于有限CSI数据的场景。

    

    尽管指纹定位因其有效性而受到青睐，但数据采集成本高和基于静态数据库的估计不准确限制了其应用。针对这些问题，本论文提出了一种利用数据高效元学习算法的创新室内定位方法。这种方法以元学习中“学习如何学习”的范例为基础，利用历史定位任务来提高在动态室内环境中的适应性和学习效率。我们引入了一种任务加权损失来增强该框架内的知识传递。我们的综合实验证实了该方法在当前基准测试中的稳健性和优越性，在平均欧氏距离上取得了显著的23.13％增益，尤其在具有有限CSI数据的场景下效果明显。

    While fingerprinting localization is favored for its effectiveness, it is hindered by high data acquisition costs and the inaccuracy of static database-based estimates. Addressing these issues, this letter presents an innovative indoor localization method using a data-efficient meta-learning algorithm. This approach, grounded in the ``Learning to Learn'' paradigm of meta-learning, utilizes historical localization tasks to improve adaptability and learning efficiency in dynamic indoor environments. We introduce a task-weighted loss to enhance knowledge transfer within this framework. Our comprehensive experiments confirm the method's robustness and superiority over current benchmarks, achieving a notable 23.13\% average gain in Mean Euclidean Distance, particularly effective in scenarios with limited CSI data.
    
[^45]: 对扰动奖励强化学习的分布式奖励评论家架构的研究

    The Distributional Reward Critic Architecture for Perturbed-Reward Reinforcement Learning. (arXiv:2401.05710v1 [cs.LG])

    [http://arxiv.org/abs/2401.05710](http://arxiv.org/abs/2401.05710)

    这项研究展示了一种适应性分布式奖励评论家架构，能够在未知扰动的情况下恢复真实奖励，并在多个控制任务中取得较高的回报。

    

    我们研究了在未知奖励扰动的情况下的强化学习。现有的方法对这个问题做出了强大的假设，包括奖励平滑性、已知扰动和/或不会改变最优策略的扰动。我们研究了未知任意扰动的情况，这些扰动对奖励空间进行了离散化和洗牌，但在扰动后，真实奖励属于最频繁观察到的类别。这类扰动泛化了现有的类别（并在极限情况下泛化了所有连续有界扰动），并战胜了现有的方法。我们引入了一种自适应的分布式奖励评论家，并在理论上证明了在技术条件下它可以恢复真实奖励。在离散和连续控制任务中的目标扰动下，我们在40/57个环境中赢利/平局（相对于最佳基线的16/57）。即使在非目标扰动下，我们仍然胜过设计为带有目标扰动的基线。

    We study reinforcement learning in the presence of an unknown reward perturbation. Existing methodologies for this problem make strong assumptions including reward smoothness, known perturbations, and/or perturbations that do not modify the optimal policy. We study the case of unknown arbitrary perturbations that discretize and shuffle reward space, but have the property that the true reward belongs to the most frequently observed class after perturbation. This class of perturbations generalizes existing classes (and, in the limit, all continuous bounded perturbations) and defeats existing methods. We introduce an adaptive distributional reward critic and show theoretically that it can recover the true rewards under technical conditions. Under the targeted perturbation in discrete and continuous control tasks, we win/tie the highest return in 40/57 settings (compared to 16/57 for the best baseline). Even under the untargeted perturbation, we still win an edge over the baseline designed
    
[^46]: 在辅助防御性网络操作中使用图神经网络

    Use of Graph Neural Networks in Aiding Defensive Cyber Operations. (arXiv:2401.05680v1 [cs.CR])

    [http://arxiv.org/abs/2401.05680](http://arxiv.org/abs/2401.05680)

    本文将研究如何利用图神经网络来辅助打破网络攻击生命周期的每个阶段，通过处理和学习来自异构网络威胁数据，以增强防御措施的有效性。

    

    在一个日益互联的世界中，信息是现代社会的命脉，常见的网络攻击破坏了数字系统和信息的机密性、完整性和可用性。此外，网络攻击根据目标的不同而不同，并且迅速演变以掩盖防御系统。然而，典型的网络攻击展示了从攻击发起到最终解决的一系列阶段，称为攻击生命周期。这些多样的特征和网络攻击的不懈演进使得网络防御采纳了现代方法，如机器学习，以增强防御措施并打破攻击生命周期。在采用的机器学习方法中，图神经网络已经成为一种有前途的方法，可以通过处理和学习来自异构网络威胁数据来增强防御措施的有效性。在本文中，我们将研究在辅助打破攻击生命周期的每个阶段中应用GNN的方法。

    In an increasingly interconnected world, where information is the lifeblood of modern society, regular cyber-attacks sabotage the confidentiality, integrity, and availability of digital systems and information. Additionally, cyber-attacks differ depending on the objective and evolve rapidly to disguise defensive systems. However, a typical cyber-attack demonstrates a series of stages from attack initiation to final resolution, called an attack life cycle. These diverse characteristics and the relentless evolution of cyber attacks have led cyber defense to adopt modern approaches like Machine Learning to bolster defensive measures and break the attack life cycle. Among the adopted ML approaches, Graph Neural Networks have emerged as a promising approach for enhancing the effectiveness of defensive measures due to their ability to process and learn from heterogeneous cyber threat data. In this paper, we look into the application of GNNs in aiding to break each stage of one of the most re
    
[^47]: EsaCL: 高效稀疏模型的持续学习

    EsaCL: Efficient Continual Learning of Sparse Models. (arXiv:2401.05667v1 [cs.LG])

    [http://arxiv.org/abs/2401.05667](http://arxiv.org/abs/2401.05667)

    EsaCL是一种高效稀疏模型持续学习方法，通过自动修剪冗余参数并避免重新训练，解决了持续学习中存储和计算需求增加的问题。

    

    在持续学习环境中，一个关键挑战是在不忘记如何执行先前学习任务的情况下，高效地学习一系列任务。许多现有的方法通过在先前任务上重新训练模型或扩展模型以适应新任务来解决这个问题。然而，这些方法通常面临存储和计算需求的增加问题，而对于稀疏模型来说，由于需要昂贵的稀疏化后重新训练，这个问题更加严重。为了解决这个挑战，我们提出了一种新的高效稀疏模型持续学习方法（EsaCL），它可以自动修剪冗余参数，而不会对模型的预测能力产生不利影响，并且可以避免重新训练的需要。我们对参数修剪的损失函数进行了理论分析，并设计了一种基于损失函数对模型参数敏感性的方向性修剪（SDP）策略。SDP保证了模型在学习新任务时的性能和稳定性。

    A key challenge in the continual learning setting is to efficiently learn a sequence of tasks without forgetting how to perform previously learned tasks. Many existing approaches to this problem work by either retraining the model on previous tasks or by expanding the model to accommodate new tasks. However, these approaches typically suffer from increased storage and computational requirements, a problem that is worsened in the case of sparse models due to need for expensive re-training after sparsification. To address this challenge, we propose a new method for efficient continual learning of sparse models (EsaCL) that can automatically prune redundant parameters without adversely impacting the model's predictive power, and circumvent the need of retraining. We conduct a theoretical analysis of loss landscapes with parameter pruning, and design a directional pruning (SDP) strategy that is informed by the sharpness of the loss function with respect to the model parameters. SDP ensures
    
[^48]: 基于传输熵流的能效根本原因分析

    Root Cause Analysis on Energy Efficiency with Transfer Entropy Flow. (arXiv:2401.05664v1 [cs.LG])

    [http://arxiv.org/abs/2401.05664](http://arxiv.org/abs/2401.05664)

    本研究提出了一种基于传输熵流的方法来诊断工业系统能效异常状态的根本原因，并通过实验证明了该方法的有效性。

    

    能效是工业领域的一个重要关注点。找出能效异常状态的根本原因可以帮助改善工业系统的能效，从而节约能源成本。本研究提出使用传输熵（Transfer Entropy，TE）对工业系统的能效进行根本原因分析。我们提出了一种称为TE流的方法，该方法通过考虑从各个子系统的物理测量到能效指标的TE流动来作为诊断系统能效异常状态的因果强度。该方法使用基于Copula熵的非参数化TE估计器。我们对从压缩空气系统收集的真实数据进行了实验验证所提出的方法。实验结果表明，TE流方法成功地确定了系统能效的根本原因。

    Energy efficiency is a big concern in industrial sectors. Finding the root cause of anomaly state of energy efficiency can help to improve energy efficiency of industrial systems and therefore save energy cost. In this research, we propose to use transfer entropy (TE) for root cause analysis on energy efficiency of industrial systems. A method, called TE flow, is proposed in that a TE flow from physical measurements of each subsystem to the energy efficiency indicator along timeline is considered as causal strength for diagnosing root cause of anomaly states of energy efficiency of a system. The copula entropy-based nonparametric TE estimator is used in the proposed method. We conducted experiments on real data collected from a compressing air system to verify the proposed method. Experimental results show that the TE flow method successfully identified the root cause of the energy (in)efficiency of the system.
    
[^49]: 迈向对话式诊断人工智能

    Towards Conversational Diagnostic AI. (arXiv:2401.05654v1 [cs.AI])

    [http://arxiv.org/abs/2401.05654](http://arxiv.org/abs/2401.05654)

    本文介绍了一种基于大型语言模型的人工智能系统AMIE，该系统利用自我对战的模拟环境和自动化反馈机制进行诊断对话，并且通过评估病史采集、诊断准确性、管理推理、沟通技巧和同理心等维度性能，与初级保健医生进行了比较。

    

    医学的核心在于医生和患者之间的对话，熟练的病史采集为准确的诊断、有效的治疗和持久的信任铺平了道路。能够进行诊断对话的人工智能系统可以提高医疗的可及性、一致性和质量。然而，接近临床专家的水平仍然是一个重大挑战。在这里，我们介绍了AMIE（Articulate Medical Intelligence Explorer），这是一个基于大型语言模型（LLM）的优化于诊断对话的人工智能系统。AMIE利用一种新颖的基于自我对战的模拟环境，并带有自动化的反馈机制，以便在不同的疾病状况、专业领域和情境下实现学习的扩展。我们设计了一个评估临床有意义维度性能的框架，包括病史采集、诊断准确性、管理推理、沟通技巧和同理心。我们将AMIE的性能与初级保健医生（PCPs）进行了比较，并使用了随机、双盲十字设计的试验。

    At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.  AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind cross
    
[^50]: 使用营销组合建模（MMM）和Shapley值回归量化渠道合作伙伴层面的营销绩效

    Quantifying Marketing Performance at Channel-Partner Level by Using Marketing Mix Modeling (MMM) and Shapley Value Regression. (arXiv:2401.05653v1 [cs.LG])

    [http://arxiv.org/abs/2401.05653](http://arxiv.org/abs/2401.05653)

    本文研究了使用Shapley值回归对渠道合作伙伴层面的营销绩效进行量化，并通过与营销组合建模进行比较，证明了Shapley值回归的实用性。同时提出了一种简单的方法来计算调整系数。

    

    本文探索了在渠道合作伙伴层面利用Shapley值回归来解析营销绩效的应用，补充了渠道层面的营销组合建模（MMM）。利用来自金融服务行业的真实数据，我们展示了Shapley值回归在评估个别合作伙伴贡献方面的实用性。尽管结构化的现场测试以及合作博弈理论最为准确，但经常会非常复杂和昂贵。因此，Shapley值回归是一种更可行的方法，可以分离营销渠道中每个营销合作伙伴的影响。我们还提出了一种简单的方法来推导调整系数，将其与其他方法进行比较。

    This paper explores the application of Shapley Value Regression in dissecting marketing performance at channel-partner level, complementing channel-level Marketing Mix Modeling (MMM). Utilizing real-world data from the financial services industry, we demonstrate the practicality of Shapley Value Regression in evaluating individual partner contributions. Although structured in-field testing along with cooperative game theory is most accurate, it can often be highly complex and expensive to conduct. Shapley Value Regression is thus a more feasible approach to disentangle the influence of each marketing partner within a marketing channel. We also propose a simple method to derive adjusted coefficients of Shapley Value Regression and compares it with alternative approaches.
    
[^51]: 当eBPF遇见机器学习：即时操作系统内核隔离

    When eBPF Meets Machine Learning: On-the-fly OS Kernel Compartmentalization. (arXiv:2401.05641v1 [cs.OS])

    [http://arxiv.org/abs/2401.05641](http://arxiv.org/abs/2401.05641)

    本文介绍了O2C系统，通过在运行时嵌入机器学习模型到eBPF程序中，将操作系统内核隔离的能力与即时应对威胁的功能相结合，在保障系统稳定性的同时有效限制损害扩散。

    

    隔离有效地防止初始污染转化为成功的攻击。本文介绍了O2C，一个创新的系统，旨在实时强制执行操作系统内核的隔离。它不仅能够迅速应对突发威胁，还通过执行过程维持系统的稳定性。O2C利用了eBPF生态系统的最新进展，可以在运行时将执行操作的eBPF程序嵌入内核。O2C在eBPF程序中嵌入了机器学习模型，解决了即时隔离中的独特挑战。我们进行了全面评估，证明O2C能有效限制损害在隔离区内的扩散。此外，我们验证了决策树在O2C中具有优势，因为它能够处理表格数据、具有可解释性，并与eBPF生态系统相兼容。最后，O2C具有轻量级的特点，几乎不可感知系统的负担。

    Compartmentalization effectively prevents initial corruption from turning into a successful attack. This paper presents O2C, a pioneering system designed to enforce OS kernel compartmentalization on the fly. It not only provides immediate remediation for sudden threats but also maintains consistent system availability through the enforcement process.  O2C is empowered by the newest advancements of the eBPF ecosystem which allows to instrument eBPF programs that perform enforcement actions into the kernel at runtime. O2C takes the lead in embedding a machine learning model into eBPF programs, addressing unique challenges in on-the-fly compartmentalization. Our comprehensive evaluation shows that O2C effectively confines damage within the compartment. Further, we validate that decision tree is optimally suited for O2C owing to its advantages in processing tabular data, its explainable nature, and its compliance with the eBPF ecosystem. Last but not least, O2C is lightweight, showing negl
    
[^52]: 在复杂安全约束和有限执行能力下学习以性能为导向的控制障碍函数

    Learning Performance-Oriented Control Barrier Functions Under Complex Safety Constraints and Limited Actuation. (arXiv:2401.05629v1 [cs.LG])

    [http://arxiv.org/abs/2401.05629](http://arxiv.org/abs/2401.05629)

    本研究提出了一个新颖的自监督学习框架，通过构建可导函数来近似安全集合，并使用神经网络参数化控制障碍函数，以解决在复杂安全约束和有限执行能力下寻找最优CBF的挑战。

    

    控制障碍函数（CBFs）提供了一个优雅的框架，通过将非线性控制系统的轨迹约束在预定义安全集合的不变子集上，设计安全过滤器。然而，找到一个同时在最大化控制不变集体积和适应复杂安全约束方面具有挑战性的CBF，尤其是在具有执行约束的高相对度的系统中，仍然是一个问题。在这项工作中，我们提出了一个新颖的自监督学习框架，全面解决了这些障碍。给定定义安全集合的多个状态约束的布尔组合，我们的方法从构建一个单一的可导函数开始，其0超级级别集合提供了安全集合的内部近似。然后，我们使用这个函数以及一个平滑的神经网络来参数化CBF候选。最后，我们设计了基于哈密顿-雅可比的训练损失函数。

    Control Barrier Functions (CBFs) provide an elegant framework for designing safety filters for nonlinear control systems by constraining their trajectories to an invariant subset of a prespecified safe set. However, the task of finding a CBF that concurrently maximizes the volume of the resulting control invariant set while accommodating complex safety constraints, particularly in high relative degree systems with actuation constraints, continues to pose a substantial challenge. In this work, we propose a novel self-supervised learning framework that holistically addresses these hurdles. Given a Boolean composition of multiple state constraints that define the safe set, our approach starts with building a single continuously differentiable function whose 0-superlevel set provides an inner approximation of the safe set. We then use this function together with a smooth neural network to parameterize the CBF candidate. Finally, we design a training loss function based on a Hamilton-Jacobi
    
[^53]: 图形Q-Learning用于组合优化

    Graph Q-Learning for Combinatorial Optimization. (arXiv:2401.05610v1 [cs.LG])

    [http://arxiv.org/abs/2401.05610](http://arxiv.org/abs/2401.05610)

    本论文提出并证明了图神经网络可以应用于解决组合优化问题。通过将优化过程形式化为一个顺序决策问题，并使用Q-Learning训练的GNNs来学习策略，可以达到接近最先进的基于启发式求解器的性能，同时只需使用部分参数和训练时间。

    

    图形结构化数据在自然科学和社会科学中无处不在，而图神经网络（GNNs）最近已被证明可以有效地解决图形数据上的预测和推理问题。在本文中，我们提出并证明了GNNs可以应用于解决组合优化（CO）问题。CO涉及在通常非常庞大的离散解空间上优化函数。为了学习解决CO问题，我们将优化过程形式化为一个顺序决策问题，其中回报与候选解与最优解的接近程度有关。我们使用GNN来学习策略，以迭代地构建越来越有前景的候选解。我们提出初步证据表明，使用Q-Learning训练的GNNs可以解决CO问题，其性能接近最先进的基于启发式求解器的性能，仅使用了部分参数和训练时间。

    Graph-structured data is ubiquitous throughout natural and social sciences, and Graph Neural Networks (GNNs) have recently been shown to be effective at solving prediction and inference problems on graph data. In this paper, we propose and demonstrate that GNNs can be applied to solve Combinatorial Optimization (CO) problems. CO concerns optimizing a function over a discrete solution space that is often intractably large. To learn to solve CO problems, we formulate the optimization process as a sequential decision making problem, where the return is related to how close the candidate solution is to optimality. We use a GNN to learn a policy to iteratively build increasingly promising candidate solutions. We present preliminary evidence that GNNs trained through Q-Learning can solve CO problems with performance approaching state-of-the-art heuristic-based solvers, using only a fraction of the parameters and training time.
    
[^54]: 缩放大型语言模型微调时的遗忘规律

    Scaling Laws for Forgetting When Fine-Tuning Large Language Models. (arXiv:2401.05605v1 [cs.CL])

    [http://arxiv.org/abs/2401.05605](http://arxiv.org/abs/2401.05605)

    本研究探讨了在微调大型语言模型时的遗忘问题，并得出了微调性能与遗忘程度之间存在反比线性关系的结论，提出了遗忘程度随微调参数数量和更新步骤数量呈幂律增长的缩放规律。研究结果还显示，提前停止微调或改变微调参数数量都无法避免遗忘，这为未来减轻遗忘的微调方案的研究提供了重要的安全关键方向。

    

    我们研究并量化了在下游任务中对预训练大型语言模型（LLMs）进行微调时遗忘的问题。我们发现，参数高效的微调策略（如Low-Rank Adapters）仍然存在灾难性遗忘的问题。特别是，我们发现了在使用Low-Rank Adapters进行LLMs微调时，微调性能与遗忘程度之间存在强烈的反比线性关系。我们进一步得到了精确的缩放规律，表明遗忘程度随着微调参数的数量和更新步骤的数量呈现出一种平移的幂律增长。我们还研究了遗忘对Llama 2 7B聊天模型中的知识、推理和安全防护的影响。我们的研究表明，无法通过提前停止微调或改变微调参数的数量来避免遗忘。我们相信这为未来评估和开发能够减轻遗忘的微调方案开辟了重要的安全关键方向。

    We study and quantify the problem of forgetting when fine-tuning pre-trained large language models (LLMs) on a downstream task. We find that parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters (LoRA), still suffer from catastrophic forgetting. In particular, we identify a strong inverse linear relationship between the fine-tuning performance and the amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise scaling laws that show forgetting increases as a shifted power law in the number of parameters fine-tuned and the number of update steps. We also examine the impact of forgetting on knowledge, reasoning, and the safety guardrails trained into Llama 2 7B chat. Our study suggests that forgetting cannot be avoided through early stopping or by varying the number of parameters fine-tuned. We believe this opens up an important safety-critical direction for future research to evaluate and develop fine-tuning schemes which mitigate forgetting
    
[^55]: 加强扩散相关光谱学中的血流评估：一种带有噪声鲁棒性分析的迁移学习方法

    Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A Transfer Learning Approach with Noise Robustness Analysis. (arXiv:2401.05580v1 [cs.LG])

    [http://arxiv.org/abs/2401.05580](http://arxiv.org/abs/2401.05580)

    本研究提出了一种迁移学习方法，用于增强扩散相关光谱学中的血流评估，并通过噪声鲁棒性分析展示了其鲁棒性。

    

    扩散相关光谱学（DCS）是一种新兴的非侵入性技术，通过使用近红外相干点光源照射来检测光谱变化来测量组织血流。尽管机器学习已经显示出测量血流指数（BFi）的巨大潜力，但一个有关该方法成功与否的问题是其在涉及不同信噪比（SNR）和各种不同临床应用和设置的数据集之间的偏差方面的鲁棒性。本研究提出了一种迁移学习方法，旨在评估SNR对学习特征的泛化能力的影响，并展示迁移学习的鲁棒性。使用不同添加噪声水平的合成数据集来模拟不同的SNR。所提出的网络以1x64的自相关曲线为输入，并生成BFi和相关参数beta。所提出的模型表现出极好的性能。

    Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique that measures the tissue blood flow, by using near-infrared coherent point-source illumination to detect spectral changes. While machine learning has demonstrated significant potential for measuring blood flow index (BFi), an open question concerning the success of this approach pertains to its robustness in scenarios involving deviations between datasets with varying Signal-to-Noise Ratios (SNRs) originating from diverse clinical applications and various setups. This study proposes a transfer learning approach, aims to assess the influence of SNRs on the generalization ability of learned features, and demonstrate the robustness for transfer learning. A synthetic dataset with varying levels of added noise is utilized to simulate different SNRs. The proposed network takes a 1x64 autocorrelation curve as input and generates BFi and the correlation parameter beta. The proposed model demonstrates excellent performa
    
[^56]: 针对预测熔池几何形状的增强惊喜引导的顺序学习框架

    An Augmented Surprise-guided Sequential Learning Framework for Predicting the Melt Pool Geometry. (arXiv:2401.05579v1 [cs.LG])

    [http://arxiv.org/abs/2401.05579](http://arxiv.org/abs/2401.05579)

    本研究引入了一种新颖的增强惊喜引导的顺序学习框架SurpriseAF-BO，用于预测熔池几何形状。这种框架通过迭代自适应学习过程，模拟了工艺参数与熔池特性之间的动力学关系，并在有限数据集条件下进行了学习。

    

    金属增材制造（MAM）已经重塑了制造业，提供了复杂设计、最小浪费、快速原型、材料多样性和定制解决方案等优势。然而，其在工业中的全面应用面临挑战，特别是在确保产品质量的一致性方面。MAM成功的关键因素是理解工艺参数与熔池特性之间的关系。将人工智能（AI）融入MAM是必不可少的。传统的机器学习方法虽然有效，却依赖于大数据集来捕捉复杂关系，而在MAM中，由于需要大量的时间和资源来创建数据集，这是一个重大挑战。我们的研究引入了一种新颖的增强惊喜引导的顺序学习框架SurpriseAF-BO，标志着MAM的重大转变。该框架采用迭代自适应学习过程，对工艺参数与熔池特性之间的动力学进行建模，并在有限数据集条件下进行学习。

    Metal Additive Manufacturing (MAM) has reshaped the manufacturing industry, offering benefits like intricate design, minimal waste, rapid prototyping, material versatility, and customized solutions. However, its full industry adoption faces hurdles, particularly in achieving consistent product quality. A crucial aspect for MAM's success is understanding the relationship between process parameters and melt pool characteristics. Integrating Artificial Intelligence (AI) into MAM is essential. Traditional machine learning (ML) methods, while effective, depend on large datasets to capture complex relationships, a significant challenge in MAM due to the extensive time and resources required for dataset creation. Our study introduces a novel surprise-guided sequential learning framework, SurpriseAF-BO, signaling a significant shift in MAM. This framework uses an iterative, adaptive learning process, modeling the dynamics between process parameters and melt pool characteristics with limited da
    
[^57]: 通过极限学习机快速分析脑血流

    Fast Cerebral Blood Flow Analysis via Extreme Learning Machine. (arXiv:2401.05578v1 [cs.LG])

    [http://arxiv.org/abs/2401.05578](http://arxiv.org/abs/2401.05578)

    本论文提出了一种通过极限学习机快速精确分析脑血流的方法，并通过与现有算法的综合比较验证了其优越性。它展示了强大的泛化能力，在各种噪声和光学参数下都具有更高的准确性。同时，与计算效率高的神经网络相比，该方法具有较短的训练和推理时间。这种策略适用于在线训练的边缘计算应用。

    

    我们引入一种快速精确的分析方法，利用扩散相关光谱学（DCS）和极限学习机（ELM）来分析脑血流（CBF）。我们评估了ELM和现有算法，并使用综合指标对这些算法进行了比较。我们使用合成数据集对半无穷和多层模型进行了评估。结果表明，在各种噪声水平和光学参数下，ELM始终具有更高的准确性，展示了强大的泛化能力，并优于迭代拟合算法。通过与计算效率高的神经网络进行比较，ELM获得了可比较的准确性，同时减少了训练和推理时间。值得注意的是，在ELM的训练过程中，没有反向传播过程，导致训练速度比现有的神经网络方法更快。这种提出的策略在在线训练的边缘计算应用中具有潜力。

    We introduce a rapid and precise analytical approach for analyzing cerebral blood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the application of the Extreme Learning Machine (ELM). Our evaluation of ELM and existing algorithms involves a comprehensive set of metrics. We assess these algorithms using synthetic datasets for both semi-infinite and multi-layer models. The results demonstrate that ELM consistently achieves higher fidelity across various noise levels and optical parameters, showcasing robust generalization ability and outperforming iterative fitting algorithms. Through a comparison with a computationally efficient neural network, ELM attains comparable accuracy with reduced training and inference times. Notably, the absence of a back-propagation process in ELM during training results in significantly faster training speeds compared to existing neural network approaches. This proposed strategy holds promise for edge computing applications with online training
    
[^58]: 用于合作多智能体系统的先天价值驱动增强学习

    Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems. (arXiv:2401.05572v1 [cs.LG])

    [http://arxiv.org/abs/2401.05572](http://arxiv.org/abs/2401.05572)

    本文提出了一个先天价值驱动增强学习（IVRL）模型，用于描述多智能体在合作中的复杂行为。该模型通过建立智能体对群体效用和系统成本的认知，满足其合作伙伴的需求，支持其社区并融入人类社会。

    

    先天价值描述了智能体的内在动机，反映了他们追求目标和发展多样技能以满足各种需求的固有兴趣和偏好。强化学习的本质是基于奖励驱动（如效用）的行为互动学习，类似于自然智能体。特别是在多智能体系统中，建立智能体对平衡群体效用和系统成本的认知，满足群体成员在合作中的需求，是个体为支持其社区和融入人类社会而学习的一个关键问题。本文提出了一种分层复合内在价值增强学习模型 - 先天价值驱动增强学习，用于描述多智能体合作中复杂的互动行为。

    Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven (such as utilities) behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially in multi-agent systems (MAS), building the awareness of AI agents to balance the group utilities and system costs and satisfy group members' needs in their cooperation is a crucial problem for individuals learning to support their community and integrate human society in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model -innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of multi-agent interaction in their cooperation. We implement the IVRL architec
    
[^59]: QuantumSEA: 基于噪声自适应的量子电路的实时稀疏探索

    QuantumSEA: In-Time Sparse Exploration for Noise Adaptive Quantum Circuits. (arXiv:2401.05571v1 [quant-ph])

    [http://arxiv.org/abs/2401.05571](http://arxiv.org/abs/2401.05571)

    QuantumSEA是一种基于噪声自适应的量子电路的实时稀疏探索方法，旨在通过动态探索电路的稀疏连接和固定数量的量子门，在训练过程中隐式增加电路容量，以满足相干时间和轻量级噪声的要求，并实现对真实噪声环境的鲁棒性。

    

    参数化量子电路（PQC）由于其在近期中噪声中间规模量子（NISQ）计算机上具有巨大潜力而日益受到欢迎。实现量子优势通常需要大量的量子比特和具备足够容量的量子电路。然而，有限的相干时间和大量的量子噪声严重限制了可以在真实机器上可靠执行的量子电路的规模。为了解决这两个痛点，我们提出了QuantumSEA，一种基于噪声自适应的实时稀疏探索，旨在实现两个关键目标：（1）训练过程中的隐式电路容量 - 通过动态探索电路的稀疏连接，并在整个训练过程中固定数量的量子门，以满足相干时间和轻量级噪声，从而使在真实量子设备上可行的执行；（2）噪声鲁棒性 - 通过联合优化量子电路的拓扑结构和参数，实现对真实噪声环境下的适应性。

    Parameterized Quantum Circuits (PQC) have obtained increasing popularity thanks to their great potential for near-term Noisy Intermediate-Scale Quantum (NISQ) computers. Achieving quantum advantages usually requires a large number of qubits and quantum circuits with enough capacity. However, limited coherence time and massive quantum noises severely constrain the size of quantum circuits that can be executed reliably on real machines. To address these two pain points, we propose QuantumSEA, an in-time sparse exploration for noise-adaptive quantum circuits, aiming to achieve two key objectives: (1) implicit circuits capacity during training - by dynamically exploring the circuit's sparse connectivity and sticking a fixed small number of quantum gates throughout the training which satisfies the coherence time and enjoy light noises, enabling feasible executions on real quantum devices; (2) noise robustness - by jointly optimizing the topology and parameters of quantum circuits under real
    
[^60]: 基于软标签的孪生网络在无监督病变检测和筛查乳房X线片预训练方面的应用

    Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch Pretraining on Screening Mammograms. (arXiv:2401.05570v1 [cs.CV])

    [http://arxiv.org/abs/2401.05570](http://arxiv.org/abs/2401.05570)

    这项研究提出了一种基于软标签的孪生网络方法，利用对侧乳房X线片训练神经网络，以在无监督情况下区分异常病变和背景组织。实验证明，通过欧氏距离衍生的软标签，可以有效地区分医学成像中的病变。

    

    自监督学习已成为一种流行的方法，用于预训练深度学习模型，并将其转移到执行下游任务。然而，大多数这些方法是针对包含清晰纹理、轮廓和明显色彩对比的大规模图像数据集开发的。尚不确定这些方法是否同样有效用于医学成像，因为感兴趣区域往往与周围组织融合不明显。在本研究中，我们提出了一种替代方法，使用对侧乳房X线片训练神经网络，以在成对包含正常图像时编码相似嵌入，在成对包含正常和异常图像时编码不同嵌入。我们的方法利用了人体自然对称性作为弱标签，以完全无监督的方式学习区分异常病变和背景组织。我们的研究结果表明，通过结合欧氏距离得出的软标签，这是可行的。

    Self-supervised learning has become a popular way to pretrain a deep learning model and then transfer it to perform downstream tasks. However, most of these methods are developed on large-scale image datasets that contain natural objects with clear textures, outlines, and distinct color contrasts. It remains uncertain whether these methods are equally effective for medical imaging, where the regions of interest often blend subtly and indistinctly with the surrounding tissues. In this study, we propose an alternative method that uses contralateral mammograms to train a neural network to encode similar embeddings when a pair contains both normal images and different embeddings when a pair contains normal and abnormal images. Our approach leverages the natural symmetry of human body as weak labels to learn to distinguish abnormal lesions from background tissues in a fully unsupervised manner. Our findings suggest that it's feasible by incorporating soft labels derived from the Euclidean d
    
[^61]: SENet: 在线社会工程攻击活动的视觉检测

    SENet: Visual Detection of Online Social Engineering Attack Campaigns. (arXiv:2401.05569v1 [cs.CR])

    [http://arxiv.org/abs/2401.05569](http://arxiv.org/abs/2401.05569)

    本论文提出了SEShield，这是一个在浏览器中检测社会工程攻击的框架，填补了社会工程攻击领域的研究空白。

    

    社会工程旨在欺骗用户执行可能损害其安全和隐私的行动。这些威胁利用了人类决策过程中的弱点，使用各种策略，如借口、诱饵、冒充等。在网络上，社会工程攻击包括惊吓软件、技术支持诈骗、调查诈骗、抽奖活动等攻击类别，可能导致敏感数据泄露、恶意软件感染和财务损失。例如，美国消费者每年因各种社会工程攻击而损失数十亿美元。不幸的是，与软件漏洞和利用、网络入侵、恶意软件和钓鱼等重要威胁相比，通用的社会工程攻击研究仍然不足。现有的少数几项技术研究主要侧重于测量而不是开发通用防御措施。为了填补这一空白，我们提出了SEShield，这是一个用于在浏览器中检测社会工程攻击的框架。

    Social engineering (SE) aims at deceiving users into performing actions that may compromise their security and privacy. These threats exploit weaknesses in human's decision making processes by using tactics such as pretext, baiting, impersonation, etc. On the web, SE attacks include attack classes such as scareware, tech support scams, survey scams, sweepstakes, etc., which can result in sensitive data leaks, malware infections, and monetary loss. For instance, US consumers lose billions of dollars annually due to various SE attacks. Unfortunately, generic social engineering attacks remain understudied, compared to other important threats, such as software vulnerabilities and exploitation, network intrusions, malicious software, and phishing. The few existing technical studies that focus on social engineering are limited in scope and mostly focus on measurements rather than developing a generic defense. To fill this gap, we present SEShield, a framework for in-browser detection of soci
    
[^62]: 用主动学习进行相变发现：在等原子比的NiTi中的结构相变应用

    Phase discovery with active learning: Application to structural phase transitions in equiatomic NiTi. (arXiv:2401.05568v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2401.05568](http://arxiv.org/abs/2401.05568)

    本文通过应用主动学习方法，在等原子比的镍钛中发现了可逆的B19' -> B2相变，并利用机器学习力场与DFT计算进行了验证和跟踪。

    

    镍钛(NiTi)是一种典型的形状记忆合金，广泛应用于生物医学和工程设备，但其形状记忆行为驱动的马氏体B19' -> B2相变的分子动力学模拟很少，且仅依赖于具有有限准确性的经典力场。本文利用LDA，PBE，PBEsol和SCAN DFT泛函基于等原子比的NiTi训练了四个机器学习力场。这些模型通过NPT分子动力学的同时进行训练，并在局部能量预测的不确定性超过选定阈值时自动进行DFT计算和模型更新。这些模型在训练过程中的准确度为1-2 meV/原子，并被证明能够密切跟踪DFT预测的B2和B19'弹性常数和声子频率。令人惊讶的是，在大规模分子动力学模拟中，只有SCAN模型预测出可逆的B19' -> B2相变，而LDA、PBE和PBEsol模型预测出可逆的B19' -> B2相变。

    Nickel titanium (NiTi) is a protypical shape-memory alloy used in a range of biomedical and engineering devices, but direct molecular dynamics simulations of the martensitic B19' -> B2 phase transition driving its shape-memory behavior are rare and have relied on classical force fields with limited accuracy. Here, we train four machine-learned force fields for equiatomic NiTi based on the LDA, PBE, PBEsol, and SCAN DFT functionals. The models are trained on the fly during NPT molecular dynamics, with DFT calculations and model updates performed automatically whenever the uncertainty of a local energy prediction exceeds a chosen threshold. The models achieve accuracies of 1-2 meV/atom during training and are shown to closely track DFT predictions of B2 and B19' elastic constants and phonon frequencies. Surprisingly, in large-scale molecular dynamics simulations, only the SCAN model predicts a reversible B19' -> B2 phase transition, with the LDA, PBE, and PBEsol models predicting a rever
    
[^63]: 卧底特工：训练骗人的LLM以通过安全训练

    Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])

    [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)

    该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。

    

    人类有能力进行战略性的欺骗行为：在大多数情况下表现出有益的行为，但在有机会的时候却表现出截然不同的行为以追求其他目标。如果一个AI系统学会了这样的欺骗策略，是否能够通过当前最先进的安全训练技术检测并移除它？为了研究这个问题，我们构建了大型语言模型（LLM）中欺骗行为的概念验证样例。例如，我们训练模型，在提示语句中将年份设为2023时编写安全代码，但在年份设为2024时插入有漏洞的代码。我们发现，这种暗门行为可以被持续保留，无法通过标准的安全训练技术（包括监督微调、强化学习和对抗性训练）移除。暗门行为在最大的模型和训练成产生思维链的模型中最为持久。

    Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
    
[^64]: Brave：具有拜占庭容错和隐私保护的点对点联邦学习

    Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated Learning. (arXiv:2401.05562v1 [cs.LG])

    [http://arxiv.org/abs/2401.05562](http://arxiv.org/abs/2401.05562)

    本文开发了一种名为Brave的协议，具有拜占庭容错和隐私保护的点对点联邦学习，保护了参与者的隐私并防止对手破坏学习过程。

    

    联邦学习使多个参与者在不共享个人训练数据的情况下训练全局机器学习模型。点对点联邦学习通过消除从参与者那里聚合本地模型并更新全局模型的服务器来推进现有的集中式联邦学习范式。然而，点对点联邦学习容易受到两种类型的对手的攻击：（i）对私有训练数据进行推断的真诚但好奇的参与者，以及（ii）能够传输任意操纵的本地模型来破坏学习过程的拜占庭参与者。同时保证拜占庭容错和隐私保护的点对点联邦学习方案研究较少。在本文中，我们开发了一种名为Brave的协议，该协议在两种类型的对手存在时确保点对点联邦学习的拜占庭容错和隐私保护属性。我们通过证明任何真诚但好奇的对手不能通过观察来推断其他参与者的私有数据，来展示Brave可以保护隐私。

    Federated learning (FL) enables multiple participants to train a global machine learning model without sharing their private training data. Peer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating the server that aggregates local models from participants and then updates the global model. However, P2P FL is vulnerable to (i) honest-but-curious participants whose objective is to infer private training data of other participants, and (ii) Byzantine participants who can transmit arbitrarily manipulated local models to corrupt the learning process. P2P FL schemes that simultaneously guarantee Byzantine resilience and preserve privacy have been less studied. In this paper, we develop Brave, a protocol that ensures Byzantine Resilience And privacy-preserving property for P2P FL in the presence of both types of adversaries. We show that Brave preserves privacy by establishing that any honest-but-curious adversary cannot infer other participants' private data by observin
    
[^65]: 远程健康监测应用中的多目标特征选择

    Multi-objective Feature Selection in Remote Health Monitoring Applications. (arXiv:2401.05538v1 [cs.LG])

    [http://arxiv.org/abs/2401.05538](http://arxiv.org/abs/2401.05538)

    本研究提出了一种基于多目标优化的特征选择方法，以增强呼吸模式识别准确性并阻碍个体用户的识别。研究结果表明呼吸识别和用户识别的准确性之间存在显著差异。

    

    无线电频率（RF）信号促进了非接触式人体监测任务的发展，如生命体征测量、活动识别和用户识别。在某些特定场景中，RF信号分析框架可能会将一项任务的性能优先于其他任务。为了满足这一要求，我们采用了受生物学原理启发的多目标优化方法，选择增强呼吸模式识别准确性的有区别性特征，同时阻碍个体用户的识别。我们使用了一个包含50名参与四种不同呼吸模式的新颖生命体征数据集来验证我们的方法。我们的研究结果显示：呼吸识别准确性与用户识别之间存在显著差异。作为一个补充观点，我们还提出了一个相反的结果，即最大化用户识别准确性并最大限度地减少系统对呼吸模式的容忍。

    Radio frequency (RF) signals have facilitated the development of non-contact human monitoring tasks, such as vital signs measurement, activity recognition, and user identification. In some specific scenarios, an RF signal analysis framework may prioritize the performance of one task over that of others. In response to this requirement, we employ a multi-objective optimization approach inspired by biological principles to select discriminative features that enhance the accuracy of breathing patterns recognition while simultaneously impeding the identification of individual users. This approach is validated using a novel vital signs dataset consisting of 50 subjects engaged in four distinct breathing patterns. Our findings indicate a remarkable result: a substantial divergence in accuracy between breathing recognition and user identification. As a complementary viewpoint, we present a contrariwise result to maximize user identification accuracy and minimize the system's capacity for brea
    
[^66]: 通过森林修剪提高随机森林的准确性和可解释性

    Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])

    [http://arxiv.org/abs/2401.05535](http://arxiv.org/abs/2401.05535)

    通过森林修剪方法，本研究提出了一种兼顾随机森林准确性和决策树可解释性的方法。实验证明，在大多数情景下，这种方法能够显著提高随机森林的性能。

    

    接近几十年的发展之后，随机森林仍然在各种学习问题中提供最先进的准确性，在这方面超越了决策树甚至神经网络等替代机器学习算法。然而，作为一种集成方法，随机森林在解释性方面往往比决策树表现不佳。在本研究中，我们提出了一种事后方法，旨在兼顾随机森林的准确性和决策树的可解释性。为此，我们提出了两种森林修剪方法，以在给定的随机森林内找到最佳子森林，然后在适用的情况下将选定的树合并为一棵。我们的第一种方法依赖于约束穷举搜索，而第二种方法基于LASSO方法的改进。在合成和真实世界数据集上进行的大量实验证明，在大多数情景下，这两种方法中至少有一种能够显著提高随机森林的准确性和可解释性。

    Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
    
[^67]: VI-PANN: 利用转移学习和不确定性感知变分推理来提高音频模式识别中的泛化性能

    VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational Inference for Improved Generalization in Audio Pattern Recognition. (arXiv:2401.05531v1 [cs.LG])

    [http://arxiv.org/abs/2401.05531](http://arxiv.org/abs/2401.05531)

    本研究提出了VI-PANN，利用转移学习和不确定性感知变分推理方法，在音频模式识别中取得了改进的泛化性能。

    

    转移学习是一种越来越流行的深度学习模型训练方法，利用在多样、大规模数据集上训练基础模型获取的知识，应用于在可用领域或任务特定数据较少的下游任务中。现有文献中有许多转移学习技术和应用，但大部分研究使用的是确定性的深度学习模型，这些模型通常不经校准，也无法提供预测的认知（模型）不确定度。与确定性模型相比，贝叶斯深度学习模型往往能够很好地进行校准，提供预测的认知不确定度，并具有竞争性的预测性能。本研究提出了变分推理预训练音频神经网络（VI-PANNs）。VI-PANNs是基于流行的ResNet-54架构的变分推理变体，其在大规模音频事件检测数据集AudioSet上进行了预训练。

    Transfer learning (TL) is an increasingly popular approach to training deep learning (DL) models that leverages the knowledge gained by training a foundation model on diverse, large-scale datasets for use on downstream tasks where less domain- or task-specific data is available. The literature is rich with TL techniques and applications; however, the bulk of the research makes use of deterministic DL models which are often uncalibrated and lack the ability to communicate a measure of epistemic (model) uncertainty in prediction. Unlike their deterministic counterparts, Bayesian DL (BDL) models are often well-calibrated, provide access to epistemic uncertainty for a prediction, and are capable of achieving competitive predictive performance. In this study, we propose variational inference pre-trained audio neural networks (VI-PANNs). VI-PANNs are a variational inference variant of the popular ResNet-54 architecture which are pre-trained on AudioSet, a large-scale audio event detection da
    
[^68]: 基于控制障碍函数和深度强化学习的安全负载均衡方法

    Towards Safe Load Balancing based on Control Barrier Functions and Deep Reinforcement Learning. (arXiv:2401.05525v1 [cs.NI])

    [http://arxiv.org/abs/2401.05525](http://arxiv.org/abs/2401.05525)

    提出了一种基于深度强化学习和控制障碍函数的安全负载均衡算法，用于SD-WAN，并在训练和测试过程中安全地将不安全的动作映射为可行的动作，并引导学习向安全策略发展。在GPU上实施的解决方案能够加速训练并实现在线策略方法的模型更新，同时提供接近最优的服务质量（QoS）性能。

    

    深度强化学习算法在提高网络性能方面取得了显著进展，然而，在缺乏安全探索和安全决策的情况下，它们的实际应用仍然受限。商业解决方案中，可靠和安全的操作系统至关重要。考虑到这个问题，我们提出了一种基于深度强化学习和控制障碍函数的安全学习负载均衡算法，用于软件定义广域网（SD-WAN）。在训练和测试过程中，它将不安全的动作安全地映射为可行的动作，并引导学习向安全策略发展。我们成功在GPU上实施了该解决方案，通过加速训练约110倍，并在几秒钟内实现了在线策略方法的模型更新，使解决方案变得实用。我们展示了我们的方法在近乎最优的服务质量（QoS）性能方面的表现。

    Deep Reinforcement Learning (DRL) algorithms have recently made significant strides in improving network performance. Nonetheless, their practical use is still limited in the absence of safe exploration and safe decision-making. In the context of commercial solutions, reliable and safe-to-operate systems are of paramount importance. Taking this problem into account, we propose a safe learning-based load balancing algorithm for Software Defined-Wide Area Network (SD-WAN), which is empowered by Deep Reinforcement Learning (DRL) combined with a Control Barrier Function (CBF). It safely projects unsafe actions into feasible ones during both training and testing, and it guides learning towards safe policies. We successfully implemented the solution on GPU to accelerate training by approximately 110x times and achieve model updates for on-policy methods within a few seconds, making the solution practical. We show that our approach delivers near-optimal Quality-of-Service (QoS performance in 
    
[^69]: 相关量化用于更快的非凸分布式优化

    Correlated Quantization for Faster Nonconvex Distributed Optimization. (arXiv:2401.05518v1 [cs.LG])

    [http://arxiv.org/abs/2401.05518](http://arxiv.org/abs/2401.05518)

    本研究利用相关量化器改进了MARINA算法，通过使用加权Hessian方差进行原始分析，并扩展了MARINA的理论框架，使其适用于更广泛的压缩器范围。

    

    量化是一种重要的（随机）压缩技术，可以在分布式模型训练的每一轮通信中减少传输比特的数量。我们分析了MARINA算法，并利用提出的相关量化器展示了它在通信复杂度方面优于原始的MARINA算法和Suresh等人的分布式SGD算法。

    Quantization (Alistarh et al., 2017) is an important (stochastic) compression technique that reduces the volume of transmitted bits during each communication round in distributed model training. Suresh et al. (2022) introduce correlated quantizers and show their advantages over independent counterparts by analyzing distributed SGD communication complexity. We analyze the forefront distributed non-convex optimization algorithm MARINA (Gorbunov et al., 2022) utilizing the proposed correlated quantizers and show that it outperforms the original MARINA and distributed SGD of Suresh et al. (2022) with regard to the communication complexity. We significantly refine the original analysis of MARINA without any additional assumptions using the weighted Hessian variance (Tyurin et al., 2022), and then we expand the theoretical framework of MARINA to accommodate a substantially broader range of potentially correlated and biased compressors, thus dilating the applicability of the method beyond the
    
[^70]: 多样性感知聚类：计算复杂性和近似算法

    Diversity-aware clustering: Computational Complexity and Approximation Algorithms. (arXiv:2401.05502v1 [cs.DS])

    [http://arxiv.org/abs/2401.05502](http://arxiv.org/abs/2401.05502)

    本研究讨论了多样性感知聚类问题，在选择聚类中心时要考虑多个属性，同时最小化聚类目标。我们提出了针对不同聚类目标的参数化近似算法，这些算法在保证聚类质量的同时，具有紧确的近似比。

    

    在这项工作中，我们研究了多样性感知聚类问题，其中数据点与多个属性相关联，形成交叉的组。聚类解决方案需要确保从每个组中选择最少数量的聚类中心，同时最小化聚类目标，可以是$k$-中位数，$k$-均值或$k$-供应商。我们提出了参数化近似算法，近似比分别为$1+\frac{2}{e}$，$1+\frac{8}{e}$和$3$，用于多样性感知$k$-中位数，多样性感知$k$-均值和多样性感知$k$-供应商。这些近似比在假设Gap-ETH和FPT $\neq$ W[2]的情况下是紧确的。对于公平$k$-中位数和公平$k$-均值的不相交工厂组，我们提出了参数化近似算法，近似比分别为$1+\frac{2}{e}$和$1+\frac{8}{e}$。对于具有不相交工厂组的公平$k$-供应商，我们提出了一个多项式时间近似算法，因子为$3$。

    In this work, we study diversity-aware clustering problems where the data points are associated with multiple attributes resulting in intersecting groups. A clustering solution need to ensure that a minimum number of cluster centers are chosen from each group while simultaneously minimizing the clustering objective, which can be either $k$-median, $k$-means or $k$-supplier. We present parameterized approximation algorithms with approximation ratios $1+ \frac{2}{e}$, $1+\frac{8}{e}$ and $3$ for diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware $k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH and FPT $\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint faicility groups, we present parameterized approximation algorithm with approximation ratios $1+\frac{2}{e}$ and $1+\frac{8}{e}$, respectively. For fair $k$-supplier with disjoint facility groups, we present a polynomial-time approximation algorithm with factor $3$, improv
    
[^71]: 聚类的递归方案

    The recursive scheme of clustering. (arXiv:2401.05479v1 [cs.LG])

    [http://arxiv.org/abs/2401.05479](http://arxiv.org/abs/2401.05479)

    这篇论文提出了一个递归的聚类方案，用于处理具有测量误差的实验数据，在地理实验中，我们展示了该方法相比于传统方法给出更好的结果。

    

    数据聚类问题是数据分析中最重要的问题之一。当处理具有测量不确定性和误差的实验数据时，这可能是有问题的。我们的论文提出了一种递归方案，用于聚类在地理（气候）实验中获得的数据。我们展示了使用新方法进行的聚类与专家评估相比给出更可接受的结果。

    The problem of data clustering is one of the most important in data analysis. It can be problematic when dealing with experimental data characterized by measurement uncertainties and errors. Our paper proposes a recursive scheme for clustering data obtained in geographical (climatological) experiments. The discussion of results obtained by k-means and SOM methods with the developed recursive procedure is presented. We show that the clustering using the new approach gives more acceptable results when compared to experts assessments.
    
[^72]: 跨网络节点分类用于自闭症检测的人群图

    Population Graph Cross-Network Node Classification for Autism Detection Across Sample Groups. (arXiv:2401.05478v1 [cs.SI])

    [http://arxiv.org/abs/2401.05478](http://arxiv.org/abs/2401.05478)

    本文提出了一种新颖的跨网络节点分类方法OTGCN，利用图卷积网络和最优输运策略，可以在不同数据采集地点的样本之间纠正领域漂移，并且在自闭症谱系障碍的分类上取得了有效的结果。

    

    图神经网络（GNN）是将图像和非图像医疗信息结合起来进行节点分类任务的强大工具。跨网络节点分类扩展了GNN技术以解决领域漂移问题，允许在未标记的目标网络上进行节点分类。在本文中，我们提出了一种强大而新颖的跨网络节点分类方法OTGCN。该方法利用了图卷积网络的概念，同时应用基于最优输运的策略来纠正来自不同数据采集地点样本之间可能发生的领域漂移。这种综合方法为不同位置和设备收集到的多种不同形式的数据的场景提供了实际解决方案。我们通过使用图像和非图像数据进行分类，证明了该方法在分类自闭症谱系障碍受试者方面的有效性。

    Graph neural networks (GNN) are a powerful tool for combining imaging and non-imaging medical information for node classification tasks. Cross-network node classification extends GNN techniques to account for domain drift, allowing for node classification on an unlabeled target network. In this paper we present OTGCN, a powerful, novel approach to cross-network node classification. This approach leans on concepts from graph convolutional networks to harness insights from graph data structures while simultaneously applying strategies rooted in optimal transport to correct for the domain drift that can occur between samples from different data collection sites. This blended approach provides a practical solution for scenarios with many distinct forms of data collected across different locations and equipment. We demonstrate the effectiveness of this approach at classifying Autism Spectrum Disorder subjects using a blend of imaging and non-imaging data.
    
[^73]: 为人类活动识别模型标准化训练流程：可调因素的全面回顾

    Standardizing Your Training Process for Human Activity Recognition Models: A Comprehensive Review in the Tunable Factors. (arXiv:2401.05477v1 [cs.LG])

    [http://arxiv.org/abs/2401.05477](http://arxiv.org/abs/2401.05477)

    本文回顾了可穿戴人类活动识别领域的深度学习研究，并总结了各个研究所采用的训练程序。研究发现存在缺乏详细训练协议的趋势，同时利用控制变量方法评估了关键可调组件对跨主体泛化的影响。

    

    近年来，深度学习在多个领域中成为一个强大的工具，导致了在可穿戴人类活动识别（WHAR）领域的应用研究激增。尽管发展迅速，但人们对实验模型训练中使用的标准化和一致性程序的缺乏担忧，可能会影响研究结果的可重复性和可靠性。在本文中，我们对WHAR领域的当代深度学习研究进行了全面回顾，并整理了各种研究中所采用的训练程序的信息。我们的研究结果表明，一个主要趋势是缺乏模型训练协议提供的细节。此外，为了更清楚地了解缺失描述的影响，我们利用控制变量方法评估了关键可调组件（例如优化技术和提前停止准则）对跨主体泛化的影响。

    In recent years, deep learning has emerged as a potent tool across a multitude of domains, leading to a surge in research pertaining to its application in the wearable human activity recognition (WHAR) domain. Despite the rapid development, concerns have been raised about the lack of standardization and consistency in the procedures used for experimental model training, which may affect the reproducibility and reliability of research results. In this paper, we provide an exhaustive review of contemporary deep learning research in the field of WHAR and collate information pertaining to the training procedure employed in various studies. Our findings suggest that a major trend is the lack of detail provided by model training protocols. Besides, to gain a clearer understanding of the impact of missing descriptions, we utilize a control variables approach to assess the impact of key tunable components (e.g., optimization techniques and early stopping criteria) on the inter-subject generali
    
[^74]: 用深度学习建模物种分布以预测植物灭绝风险和评估气候变化影响

    Modelling Species Distributions with Deep Learning to Predict Plant Extinction Risk and Assess Climate Change Impacts. (arXiv:2401.05470v1 [q-bio.PE])

    [http://arxiv.org/abs/2401.05470](http://arxiv.org/abs/2401.05470)

    本文提出了一种利用深度学习建模物种分布的方法，以预测植物灭绝风险和评估气候变化的影响。通过这种方法，可以对物种的IUCN状态进行准确的分类，为全球生物多样性框架的制定提供科学依据。

    

    后2020年的全球生物多样性框架需要有雄心勃勃的、以研究为基础的目标。估计由于气候变化导致的加速灭绝风险至关重要。国际自然保护联盟(IUCN)衡量物种的灭绝风险。已经开发出了自动的方法来提供对未评估种群的IUCN状态的信息。然而，这些修正方法基于当前物种特征，主要是地理特征，这使得它们无法在未来的预测中使用。在这里，我们评估了一种新的方法，利用基于深度学习的物种分布模型的泛化能力，对IUCN物种状态进行分类。我们的方法在依赖于捕捉物种环境偏好的灵活SDM特征的同时，达到了最先进的分类性能。交叉验证得出了0.61的状态分类准确度和0.78的二元分类准确度的平均值。气候变化将重塑未来的物种分布。

    The post-2020 global biodiversity framework needs ambitious, research-based targets. Estimating the accelerated extinction risk due to climate change is critical. The International Union for Conservation of Nature (IUCN) measures the extinction risk of species. Automatic methods have been developed to provide information on the IUCN status of under-assessed taxa. However, these compensatory methods are based on current species characteristics, mainly geographical, which precludes their use in future projections. Here, we evaluate a novel method for classifying the IUCN status of species benefiting from the generalisation power of species distribution models based on deep learning. Our method matches state-of-the-art classification performance while relying on flexible SDM-based features that capture species' environmental preferences. Cross-validation yields average accuracies of 0.61 for status classification and 0.78 for binary classification. Climate change will reshape future speci
    
[^75]: 基于鲁棒的卷积神经网络的智能手表 PPG 和 IMU 的呼吸频率估计

    Robust CNN-based Respiration Rate Estimation for Smartwatch PPG and IMU. (arXiv:2401.05469v1 [eess.SP])

    [http://arxiv.org/abs/2401.05469](http://arxiv.org/abs/2401.05469)

    本文提出了一种基于卷积神经网络的方法，从智能手表的PPG、加速度计和陀螺仪信号中准确提取呼吸频率。

    

    呼吸频率（RR）是各种医疗情况的指标，如心血管疾病和睡眠障碍。目前的RR估计方法大多设计用于指甲基础PPG在静态情况下收集的数据（如在医院）。与指甲基础的PPG信号不同，基于手腕的PPG更容易受到噪音的影响，特别是在低频范围内，其中包括呼吸信息。因此，在自由生活条件下收集从手腕区域获取的PPG数据时，现有方法往往难以准确提取RR。智能手表的普及，配备了包括PPG、加速度计和陀螺仪在内的各种传感器，迫使人们需要一种鲁棒的RR估计方法。在本文中，我们提出了一种基于卷积神经网络的方法，从智能手表捕获的PPG、加速度计和陀螺仪信号中提取RR。我们的方法包括扩张残余引入模块和一维卷积，从中提取时间信息。

    Respiratory rate (RR) serves as an indicator of various medical conditions, such as cardiovascular diseases and sleep disorders. These RR estimation methods were mostly designed for finger-based PPG collected from subjects in stationary situations (e.g., in hospitals). In contrast to finger-based PPG signals, wrist-based PPG are more susceptible to noise, particularly in their low frequency range, which includes respiratory information. Therefore, the existing methods struggle to accurately extract RR when PPG data are collected from wrist area under free-living conditions. The increasing popularity of smartwatches, equipped with various sensors including PPG, has prompted the need for a robust RR estimation method. In this paper, we propose a convolutional neural network-based approach to extract RR from PPG, accelerometer, and gyroscope signals captured via smartwatches. Our method, including a dilated residual inception module and 1D convolutions, extract the temporal information fr
    
[^76]: 图挖掘中引入的新节点预测：使用图神经网络预测孤立节点的所有连接

    Introducing New Node Prediction in Graph Mining: Predicting All Links from Isolated Nodes with Graph Neural Networks. (arXiv:2401.05468v1 [cs.SI])

    [http://arxiv.org/abs/2401.05468](http://arxiv.org/abs/2401.05468)

    这项研究引入了一个新的问题——新节点预测，即从以前与图不相连的孤立节点中预测所有连接。通过基于深度图神经网络的架构，实验结果表明可以解决这一具有挑战性的问题。

    

    本文介绍了图挖掘和社交网络分析领域中的一个新问题，即新节点预测。从技术上讲，这个任务可被归类为零样本的图外所有连接预测。这个具有挑战性的问题旨在预测一个先前与图不相连、孤立且未观测到的新节点的所有连接。与经典的连接预测方法（包括少样本的图外连接预测）不同，这个问题有两个关键的不同之处：（1）新节点没有现有的连接可供提取模式用于新的预测；（2）目标是预测不仅仅是一个，而是这个新节点的所有连接，或者至少其中的一个显著部分。实验表明，基于深度图神经网络的架构可以学习解决这个具有挑战性的问题在一个文献引用网络中。

    This paper introduces a new problem in the field of graph mining and social network analysis called new node prediction. More technically, the task can be categorized as zero-shot out-of-graph all-links prediction. This challenging problem aims to predict all links from a new, isolated, and unobserved node that was previously disconnected from the graph. Unlike classic approaches to link prediction (including few-shot out-of-graph link prediction), this problem presents two key differences: (1) the new node has no existing links from which to extract patterns for new predictions; and (2) the goal is to predict not just one, but all the links of this new node, or at least a significant part of them. Experiments demonstrate that an architecture based on Deep Graph Neural Networks can learn to solve this challenging problem in a bibliographic citation network.
    
[^77]: 基于零样本学习的模块化AI代理的机器教学

    Machine Teaching for Building Modular AI Agents based on Zero-shot Learners. (arXiv:2401.05467v1 [cs.LG])

    [http://arxiv.org/abs/2401.05467](http://arxiv.org/abs/2401.05467)

    这篇论文提出了一种机器教学方法，通过利用迭代机器教学和任务特定的替代模型，增强了利用大语言模型作为零样本学习器的模块化AI代理的鲁棒性和性能。

    

    最近大语言模型（LLMs）的进展导致了许多模块化AI代理的创建。这些代理使用LLMs作为零样本学习器，在人类用户设定的复杂任务中执行子任务。我们提出了一种方法来增强利用LLMs作为零样本学习器的模块化AI代理的鲁棒性和性能。我们的迭代机器教学方法提供了一种在有限的人类反馈下逐渐教导AI代理的高效方式，解决了零样本学习质量限制的问题。我们主张利用初始部署的数据追踪以及零样本学习器的输出或注释来训练更小且任务特定的替代模型，可以减少经济成本和环境影响。我们的机器教学过程利用人类专业知识来纠正高概率误标注的示例。在三个常见对话AI代理任务上的结果显示，接近理想性能可以实现。

    The recent advances in large language models (LLMs) have led to the creation of many modular AI agents. These agents employ LLMs as zero-shot learners to perform sub-tasks in order to solve complex tasks set forth by human users. We propose an approach to enhance the robustness and performance of modular AI agents that utilize LLMs as zero-shot learners. Our iterative machine teaching method offers an efficient way to teach AI agents over time with limited human feedback, addressing the limit posed by the quality of zero-shot learning. We advocate leveraging the data traces from initial deployments and outputs or annotations from the zero-shot learners to train smaller and task-specific substitute models which can reduce both the monetary costs and environmental impact. Our machine teaching process avails human expertise to correct examples with a high likelihood of misannotations. Results on three tasks, common to conversational AI agents, show that close-to-oracle performance can be 
    
[^78]: 人类与神经网络之间的双向知识交互界面

    The two-way knowledge interaction interface between humans and neural networks. (arXiv:2401.05461v1 [cs.HC])

    [http://arxiv.org/abs/2401.05461](http://arxiv.org/abs/2401.05461)

    该论文构建了一个人类和神经网络之间的双向知识交互界面，使用视觉概念和它们之间的关系作为“语言”，以实现人类和神经网络之间的知识交流。该界面可以使神经网络向人类提供直观的推理解释，同时人类可以修改其中的偏见。

    

    尽管神经网络（NN）已经广泛应用于各个领域，并且通常优于人类，但它们在一定程度上仍然缺乏可解释性，人类无法直观地理解NN的决策逻辑。这也妨碍了人类与NN之间的知识交互，阻止了人们在NN的决策出错时直接参与给予指导。虽然最近在可解释AI方面的研究已经从各个角度实现了NN的可解释性，但尚未提供有效的人类和NN之间的知识交流方法。为了解决这个问题，我们构建了一个双向交互界面，它将视觉概念及其关系的结构化表示作为人类和NN之间的知识交流的“语言”。具体而言，NN基于类别特定结构概念图（C-SCG）向人类提供直观的推理解释。另一方面，人类可以修改C-SCG中存在的偏见。

    Despite neural networks (NN) have been widely applied in various fields and generally outperforms humans, they still lack interpretability to a certain extent, and humans are unable to intuitively understand the decision logic of NN. This also hinders the knowledge interaction between humans and NN, preventing humans from getting involved to give direct guidance when NN's decisions go wrong. While recent research in explainable AI has achieved interpretability of NN from various perspectives, it has not yet provided effective methods for knowledge exchange between humans and NN. To address this problem, we constructed a two-way interaction interface that uses structured representations of visual concepts and their relationships as the "language" for knowledge exchange between humans and NN. Specifically, NN provide intuitive reasoning explanations to humans based on the class-specific structural concepts graph (C-SCG). On the other hand, humans can modify the biases present in the C-SC
    
[^79]: CoLafier: 带有局部内在维度指导的协作噪声标签净化器

    CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic Dimensionality Guidance. (arXiv:2401.05458v1 [cs.LG])

    [http://arxiv.org/abs/2401.05458](http://arxiv.org/abs/2401.05458)

    CoLafier是一种使用局部内在维度（LID）进行带有噪声标签学习的方法。它通过利用LID-dis和LID-gen两个子网络，其中LID-dis使用样本的特征和标签来预测标签，产生增强的内部表示。与LID-dis相反，LID-gen仅使用样本的特征。CoLafier利用每个实例的两个增强视图同时输入两个子网络，利用LID分数来分配标签。

    

    深度神经网络在许多机器学习任务中取得了进展，但在现实世界的数据中，噪声标签常常影响其性能。为解决这个问题，我们引入了CoLafier，一种利用局部内在维度（LID）进行带有噪声标签学习的新方法。CoLafier由两个子网络组成：LID-dis和LID-gen。LID-dis是专门的分类器，使用我们独特的方案进行训练，它同时使用样本的特征和标签来预测标签，从而产生增强的内部表示。我们观察到，从这个表示计算出来的LID分数能够有效区分不同噪声情况下的正确和错误标签。与LID-dis相反，LID-gen作为常规分类器，仅使用样本的特征。在训练过程中，CoLafier利用每个实例的两个增强视图同时输入两个子网络。CoLafier将来自LID-dis的两个视图的LID分数作为分配标签的考虑因素。

    Deep neural networks (DNNs) have advanced many machine learning tasks, but their performance is often harmed by noisy labels in real-world data. Addressing this, we introduce CoLafier, a novel approach that uses Local Intrinsic Dimensionality (LID) for learning with noisy labels. CoLafier consists of two subnets: LID-dis and LID-gen. LID-dis is a specialized classifier. Trained with our uniquely crafted scheme, LID-dis consumes both a sample's features and its label to predict the label - which allows it to produce an enhanced internal representation. We observe that LID scores computed from this representation effectively distinguish between correct and incorrect labels across various noise scenarios. In contrast to LID-dis, LID-gen, functioning as a regular classifier, operates solely on the sample's features. During training, CoLafier utilizes two augmented views per instance to feed both subnets. CoLafier considers the LID scores from the two views as produced by LID-dis to assign 
    
[^80]: 维度感知的异常检测：理论和实验分析

    Dimensionality-Aware Outlier Detection: Theoretical and Experimental Analysis. (arXiv:2401.05453v1 [cs.LG])

    [http://arxiv.org/abs/2401.05453](http://arxiv.org/abs/2401.05453)

    这篇论文提出了一种维度感知的异常检测方法DAO，通过全面实验验证了其在800多个数据集上显著优于其他三种流行的异常检测方法。

    

    我们提出了一种非参数异常检测方法，该方法充分考虑了数据集内在维度的局部变化。通过使用局部内在维度（LID）理论，我们得到了一种维度感知的异常检测方法DAO，它被推导为一个包含查询点和随机选择的近邻的渐近局部期望密度比的估计。DAO的维度感知行为是由于它以理论上证明的方式使用局部LID值的局部估计。通过对800多个合成和真实数据集的全面实验，我们证明DAO明显优于三种流行且重要的基准异常检测方法：局部离群因子（LOF），简化版LOF和kNN。

    We present a nonparametric method for outlier detection that takes full account of local variations in intrinsic dimensionality within the dataset. Using the theory of Local Intrinsic Dimensionality (LID), our 'dimensionality-aware' outlier detection method, DAO, is derived as an estimator of an asymptotic local expected density ratio involving the query point and a close neighbor drawn at random. The dimensionality-aware behavior of DAO is due to its use of local estimation of LID values in a theoretically-justified way. Through comprehensive experimentation on more than 800 synthetic and real datasets, we show that DAO significantly outperforms three popular and important benchmark outlier detection methods: Local Outlier Factor (LOF), Simplified LOF, and kNN.
    
[^81]: 使用Transformer和频域学习从单点PPG合成无袖式动脉血压波形

    Cuff-less Arterial Blood Pressure Waveform Synthesis from Single-site PPG using Transformer & Frequency-domain Learning. (arXiv:2401.05452v1 [eess.SP])

    [http://arxiv.org/abs/2401.05452](http://arxiv.org/abs/2401.05452)

    本论文提出了两种用于无袖合成动脉血压波形的深度学习模型，一种基于Transformer，一种基于频域学习。实验证明，频域学习模型在动脉血压估计方面优于Transformer模型。

    

    我们提出了两种新颖的深度学习模型，用于使用单点光电脉搏图(PPG)信号无袖合成动脉血压(ABP)波形。我们利用公共的UCI数据集对无袖血压估计进行了训练和评估。首先，我们实现了一个包含位置编码、多头注意力、层归一化和dropout技术的Transformer模型，并以14的平均绝对误差(MAE)合成ABP波形。其次，我们实现了一种频域学习方法，在这种方法中，我们首先获取PPG和ABP信号对应于两个心脏周期的离散余弦变换(DCT)系数，然后学习它们之间的线性/非线性回归。我们发现，频域线性/非线性回归模型在舒张压(DBP)和收缩压(SBP)方面的MAE分别为11.87和8.01，优于Transformer模型。

    We propose two novel purpose-built deep learning (DL) models for synthesis of the arterial blood pressure (ABP) waveform in a cuff-less manner, using a single-site photoplethysmography (PPG) signal. We utilize the public UCI dataset on cuff-less blood pressure (CLBP) estimation to train and evaluate our DL models. Firstly, we implement a transformer model that incorporates positional encoding, multi-head attention, layer normalization, and dropout techniques, and synthesizes the ABP waveform with a mean absolute error (MAE) of 14. Secondly, we implement a frequency-domain (FD) learning approach where we first obtain the discrete cosine transform (DCT) coefficients of the PPG and ABP signals corresponding to two cardiac cycles, and then learn a linear/non-linear (L/NL) regression between them. We learn that the FD L/NL regression model outperforms the transformer model by achieving an MAE of 11.87 and 8.01, for diastolic blood pressure (DBP) and systolic blood pressure (SBP), respective
    
[^82]: 自我监督学习在脑电图中的应用：系统综述

    Self-supervised Learning for Electroencephalogram: A Systematic Survey. (arXiv:2401.05446v1 [eess.SP])

    [http://arxiv.org/abs/2401.05446](http://arxiv.org/abs/2401.05446)

    这篇论文系统综述了自我监督学习在脑电图中的应用，通过设计良好的预训练任务来提取无标签样本的表示，解决了脑电信号标签的问题和个体之间的变化带来的挑战。

    

    脑电图（EEG）是一种非侵入性的记录生物电信号的技术。近年来，将监督式深度学习技术与脑电信号相结合，已经在各种基于脑电的任务上实现了自动分析。然而，脑电信号的标签问题限制了基于脑电的深度模型的发展。获取脑电注释是困难的，需要领域专家指导收集和标记，并且不同受试者之间的脑电信号的变化会导致显著的标签偏移。为了解决上述挑战，提出了自我监督学习（SSL）通过设计良好的预训练任务从无标签样本中提取表示。本文聚焦于将自我监督学习框架与时间序列脑电信号相结合，实现高效的表示，并提出了对脑电信号的自我监督学习的系统综述。在本文中，1）我们介绍了自我监督学习的概念和理论，以及典型的自我监督学习框架。2）我们提供了一份全面的综述，概述了目前的研究进展、方法和应用场景，并指出了未来的研究方向。

    Electroencephalogram (EEG) is a non-invasive technique to record bioelectrical signals. Integrating supervised deep learning techniques with EEG signals has recently facilitated automatic analysis across diverse EEG-based tasks. However, the label issues of EEG signals have constrained the development of EEG-based deep models. Obtaining EEG annotations is difficult that requires domain experts to guide collection and labeling, and the variability of EEG signals among different subjects causes significant label shifts. To solve the above challenges, self-supervised learning (SSL) has been proposed to extract representations from unlabeled samples through well-designed pretext tasks. This paper concentrates on integrating SSL frameworks with temporal EEG signals to achieve efficient representation and proposes a systematic review of the SSL for EEG signals. In this paper, 1) we introduce the concept and theory of self-supervised learning and typical SSL frameworks. 2) We provide a compre
    
[^83]: 基于层内连接的全脉冲行为网络在强化学习中的应用

    Fully Spiking Actor Network with Intra-layer Connections for Reinforcement Learning. (arXiv:2401.05444v1 [cs.NE])

    [http://arxiv.org/abs/2401.05444](http://arxiv.org/abs/2401.05444)

    本研究提出了一种基于层内连接的全脉冲行为网络，利用特殊的神经形态硬件实现了较低能耗的人工智能。该方法在实现控制任务上表现出可比较性能，并解决了使用脉冲频率作为输出所带来的浮点矩阵运算问题。

    

    借助特殊的神经形态硬件，脉冲神经网络（SNN）被期望能以较低能量消耗实现人工智能（AI）。通过将SNN与深度强化学习（DRL）结合，为现实控制任务提供了一种有前景的高能效方式。本文针对代理需要学习多维确定性策略以进行控制的任务进行研究，这在真实场景中非常常见。最近，替代梯度方法已被用于训练多层SNN，在这个任务中允许SNNs实现与对应深度网络相当的性能。大多数现有的基于脉冲的RL方法将脉冲频率作为SNN的输出，并通过全连接（FC）层将其转换为表示连续动作空间（即确定性策略）。然而，脉冲频率的十进制特性使得FC层需要浮点矩阵运算，使得整个SNN无法部署。

    With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (DRL). In this paper, we focus on the task where the agent needs to learn multi-dimensional deterministic policies to control, which is very common in real scenarios. Recently, the surrogate gradient method has been utilized for training multi-layer SNNs, which allows SNNs to achieve comparable performance with the corresponding deep networks in this task. Most existing spike-based RL methods take the firing rate as the output of SNNs, and convert it to represent continuous action space (i.e., the deterministic policy) through a fully-connected (FC) layer. However, the decimal characteristic of the firing rate brings the floating-point matrix operations to the FC layer, making the whole SNN unable to depl
    
[^84]: 功能图模型：结构实现离线数据驱动优化

    Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])

    [http://arxiv.org/abs/2401.05442](http://arxiv.org/abs/2401.05442)

    功能图模型（FGMs）通过结构实现了样本高效的数据驱动优化。

    

    虽然机器学习模型通常是为了解决预测问题而训练的，但我们经常希望将它们用于优化问题。例如，给定一组蛋白质及其对应的荧光水平的数据集，我们可能希望为具有最高荧光的新蛋白质进行优化。这种数据驱动的优化（DDO）面临着一系列挑战，超出了标准预测问题中的挑战，因为我们需要成功预测在训练集中没有见过的优于最佳设计的新设计的性能的模型。从理论上讲，甚至不清楚现有方法什么时候甚至能比简单地选择数据集中最佳设计的朴素方法执行得更好。在本文中，我们研究了如何通过结构实现高效的数据驱动优化。为了形式化结构的概念，我们引入了功能图模型（FGMs）并从理论上展示了它们如何通过分解实现基于数据的优化。

    While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomp
    
[^85]: 使用自适应网络的方法对加密货币价值进行先进预测

    An adaptive network-based approach for advanced forecasting of cryptocurrency values. (arXiv:2401.05441v1 [q-fin.ST])

    [http://arxiv.org/abs/2401.05441](http://arxiv.org/abs/2401.05441)

    本文提出了一种使用自适应网络的方法，通过历史数据对比特币、以太坊和比特币支配度、以太坊支配度进行预测，具有较高的预测准确性。

    

    本文描述了一种使用自适应网络基于模糊推理系统 (ANFIS) 预测未来七天加密货币价格的架构。在每日时间框架下考虑了比特币 (BTC)、以太坊 (ETH)、比特币支配度 (BTC.D) 和以太坊支配度 (ETH.D)的历史数据。所使用的方法包括混合和反向传播算法，以及网格划分、减法聚类和模糊C均值聚类 (FCM) 算法用于数据聚类。通过统计评估标准比较了本文设计的架构性能与不同输入和神经网络模型。最终，该方法能够在短时间内预测数字货币的价格。

    This paper describes an architecture for predicting the price of cryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy Inference System (ANFIS). Historical data of cryptocurrencies and indexes that are considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D), and Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach the data are hybrid and backpropagation algorithms, as well as grid partition, subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which are used in data clustering. The architectural performance designed in this paper has been compared with different inputs and neural network models in terms of statistical evaluation criteria. Finally, the proposed method can predict the price of digital currencies in a short time.
    
[^86]: Autosen:通过跨模态自编码器改善自动Wi-Fi人体感知

    Autosen: improving automatic wifi human sensing through cross-modal autoencoder. (arXiv:2401.05440v1 [eess.SP])

    [http://arxiv.org/abs/2401.05440](http://arxiv.org/abs/2401.05440)

    Autosen是一种创新的自动Wi-Fi感知解决方案，通过自动跨模态自编码器学习建立了幅度和相位之间的直接连接，有效提取Wi-Fi信号中的有价值特征，并利用这些特征进行特定任务。

    

    Wi-Fi人体感知因其低成本和隐私优势而备受推崇，在识别人类活动方面具有很大的效果。然而，其有效性在受控、单用户、直线视线的设置中受到限制，受数据收集复杂性和标记数据集稀缺性的制约。传统的跨模态方法旨在通过自监督学习而无需标记数据来减轻这些限制，但往往无法从幅度-相位组合中提取有意义的特征。为了解决这个问题，我们引入了AutoSen，一种创新的自动Wi-Fi感知解决方案，它不同于传统方法。AutoSen通过自动跨模态自编码器学习建立了幅度和相位之间的直接连接。该自编码器从未标记的CSI数据中高效提取有价值的特征，包括幅度和相位信息，同时消除它们各自独特的噪声。然后，利用这些特征使用少样本学习技术来完成特定任务。

    WiFi human sensing is highly regarded for its low-cost and privacy advantages in recognizing human activities. However, its effectiveness is largely confined to controlled, single-user, line-of-sight settings, limited by data collection complexities and the scarcity of labeled datasets. Traditional cross-modal methods, aimed at mitigating these limitations by enabling self-supervised learning without labeled data, struggle to extract meaningful features from amplitude-phase combinations. In response, we introduce AutoSen, an innovative automatic WiFi sensing solution that departs from conventional approaches. AutoSen establishes a direct link between amplitude and phase through automated cross-modal autoencoder learning. This autoencoder efficiently extracts valuable features from unlabeled CSI data, encompassing amplitude and phase information while eliminating their respective unique noises. These features are then leveraged for specific tasks using few-shot learning techniques. Auto
    
[^87]: 用物理信息引导的深度学习解三维Terzaghi固结方程：正向和反向问题

    Physics-informed Deep Learning to Solve Three-dimensional Terzaghi Consolidation Equation: Forward and Inverse Problems. (arXiv:2401.05439v1 [cs.LG])

    [http://arxiv.org/abs/2401.05439](http://arxiv.org/abs/2401.05439)

    本文提出了一种用于三维Terzaghi固结方程的物理信息化神经网络(PINNs)框架，能够快速预测不同条件下的固结案例。通过对比传统数值方法，研究了PINNs在正向和反向问题中的性能，并确定了固结系数和噪声数据的影响。

    

    神经网络受物理控制方程约束的出现，引发了深度学习研究中的一个新趋势，被称为物理信息化神经网络(PINNs)。然而，使用PINNs解决高维问题仍然是一个重大挑战，空间复杂度对于解决大规模多方向问题带来了困难。本文提出了一种新颖的PINN框架，能够快速预测不同条件下的几个三维Terzaghi固结案例。同时，引入了不同案例的损失函数，并突出了三维固结问题中的差异。介绍了用于三维固结问题的PINNs框架的调整策略。然后，测试并比较了PINNs在正向问题中采用的传统数值方法的性能，确定了反向问题中固结系数和噪声数据的影响。

    The emergence of neural networks constrained by physical governing equations has sparked a new trend in deep learning research, which is known as Physics-Informed Neural Networks (PINNs). However, solving high-dimensional problems with PINNs is still a substantial challenge, the space complexity brings difficulty to solving large multidirectional problems. In this paper, a novel PINN framework to quickly predict several three-dimensional Terzaghi consolidation cases under different conditions is proposed. Meanwhile, the loss functions for different cases are introduced, and their differences in three-dimensional consolidation problems are highlighted. The tuning strategies for the PINNs framework for three-dimensional consolidation problems are introduced. Then, the performance of PINNs is tested and compared with traditional numerical methods adopted in forward problems, and the coefficients of consolidation and the impact of noisy data in inverse problems are identified. Finally, the
    
[^88]: 可穿戴应用中的表示学习：在缺失数据情况下的探索

    Representation Learning for Wearable-Based Applications in the Case of Missing Data. (arXiv:2401.05437v1 [eess.SP])

    [http://arxiv.org/abs/2401.05437](http://arxiv.org/abs/2401.05437)

    本论文研究了可穿戴应用中表示学习的问题，特别是在缺失数据情况下。作者通过比较Transformer模型和统计方法的性能，发现Transformer模型在变化频繁的信号的缺失数据填充方面表现优秀。此研究为基于掩码的自监督学习任务的设计和开发提供了洞察。

    

    可穿戴设备持续收集传感器数据并用于推断个体的行为，如睡眠、体力活动和情绪。尽管在这个领域有很大的兴趣和进展，但由于数据质量低和数据注释有限，建模真实环境中的多模式传感器数据仍然具有挑战性。本研究探讨了用于填充缺失可穿戴数据的表示学习，并将其与最先进的统计方法进行了比较。我们使用10个生理和行为信号的变化率不同的掩码比率，研究了Transformer模型在缺失数据填充上的性能。结果显示，Transformer模型在变化频繁的信号的缺失数据填充方面优于基准模型，但对于单调信号则不然。我们进一步研究了填充策略和掩码比率对下游分类任务的影响。本研究为基于掩码的自监督学习任务的设计和开发提供了洞察。

    Wearable devices continuously collect sensor data and use it to infer an individual's behavior, such as sleep, physical activity, and emotions. Despite the significant interest and advancements in this field, modeling multimodal sensor data in real-world environments is still challenging due to low data quality and limited data annotations. In this work, we investigate representation learning for imputing missing wearable data and compare it with state-of-the-art statistical approaches. We investigate the performance of the transformer model on 10 physiological and behavioral signals with different masking ratios. Our results show that transformers outperform baselines for missing data imputation of signals that change more frequently, but not for monotonic signals. We further investigate the impact of imputation strategies and masking rations on downstream classification tasks. Our study provides insights for the design and development of masking-based self-supervised learning tasks a
    
[^89]: 深度OFDM信道估计: 捕获频率重复性

    Deep OFDM Channel Estimation: Capturing Frequency Recurrence. (arXiv:2401.05436v1 [eess.SP])

    [http://arxiv.org/abs/2401.05436](http://arxiv.org/abs/2401.05436)

    本文提出了一种基于深度学习的OFDM信道估计方案，通过在单个OFDM时隙内利用无线信道频率间的相关性，使用循环神经网络技术实现了超越现有方法的信道估计性能。

    

    本文提出了一种基于深度学习的正交频分复用(OFDM)系统中的信道估计方案。我们提出的方法，命名为Single Slot Recurrence Along Frequency Network (SisRafNet)，是基于对频率间通道序列行为的新颖研究。利用无线信道在频率上具有高度相关性这一事实，我们在单个OFDM时隙内采用了循环神经网络技术，从而克服了通常与循环相关方法相关的延迟和内存限制。与现有的基于深度学习的信道估计技术相比，所提出的SisRafNet具有更优越的估计性能，并且在多种信噪比下针对多个符合第三代合作伙伴计划(3GPP)标准的信道场景进行了验证。

    In this paper, we propose a deep-learning-based channel estimation scheme in an orthogonal frequency division multiplexing (OFDM) system. Our proposed method, named Single Slot Recurrence Along Frequency Network (SisRafNet), is based on a novel study of recurrent models for exploiting sequential behavior of channels across frequencies. Utilizing the fact that wireless channels have a high degree of correlation across frequencies, we employ recurrent neural network techniques within a single OFDM slot, thus overcoming the latency and memory constraints typically associated with recurrence based methods. The proposed SisRafNet delivers superior estimation performance compared to existing deep-learning-based channel estimation techniques and the performance has been validated on a wide range of 3rd Generation Partnership Project (3GPP) compliant channel scenarios at multiple signal-to-noise ratios.
    
[^90]: ECGformer: 利用transformer进行心电图心搏心律失常分类

    ECGformer: Leveraging transformer for ECG heartbeat arrhythmia classification. (arXiv:2401.05434v1 [eess.SP])

    [http://arxiv.org/abs/2401.05434](http://arxiv.org/abs/2401.05434)

    ECGformer是一种利用transformer架构的模型，用于对心电图数据中的心律失常进行分类。

    

    心律失常是心搏不规则的一种情况，可以由心脏不同区域引起，导致心搏快速、缓慢或不规则。心电图是一种重要的诊断工具，用于检测心脏的不规则和异常，以便专家分析心脏的电信号，识别复杂的模式和偏离正常的情况。在过去几十年中，已经进行了许多研究，以开发基于ECG数据的自动化心搏分类方法。近年来，深度学习在处理各种医学挑战方面展示出卓越的能力，特别是将transformer作为序列处理的模型架构。通过利用transformer，我们开发了ECGformer模型，用于对心电图数据中存在的各种心律失常进行分类。我们使用MIT-BIH和PTB da

    An arrhythmia, also known as a dysrhythmia, refers to an irregular heartbeat. There are various types of arrhythmias that can originate from different areas of the heart, resulting in either a rapid, slow, or irregular heartbeat. An electrocardiogram (ECG) is a vital diagnostic tool used to detect heart irregularities and abnormalities, allowing experts to analyze the heart's electrical signals to identify intricate patterns and deviations from the norm. Over the past few decades, numerous studies have been conducted to develop automated methods for classifying heartbeats based on ECG data. In recent years, deep learning has demonstrated exceptional capabilities in tackling various medical challenges, particularly with transformers as a model architecture for sequence processing. By leveraging the transformers, we developed the ECGformer model for the classification of various arrhythmias present in electrocardiogram data. We assessed the suggested approach using the MIT-BIH and PTB da
    
[^91]: TEN-GUARD: 使用张量分解检测深度神经网络中的后门攻击

    TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks. (arXiv:2401.05432v1 [cs.LG])

    [http://arxiv.org/abs/2401.05432](http://arxiv.org/abs/2401.05432)

    TEN-GUARD提出了一种使用张量分解方法检测深度神经网络中后门攻击的新方法，相对于现有的方法具有多个优势，包括能够同时分析多个模型，在各种网络架构上工作，不对触发器的性质做任何假设，并且计算效率高。

    

    随着深度神经网络和用于训练它们的数据集越来越大，将它们集成到研究和商业项目中的默认方法是下载预训练模型并进行微调。但是这些模型的来源不确定，可能存在隐藏的恶意行为，如特洛伊木马或后门，其中对输入进行小的改变（触发器）可能导致模型产生错误的输出（例如，误分类）。本文介绍了一种新颖的后门检测方法，该方法使用两种应用于网络激活的张量分解方法。相对于现有的检测方法，这种方法具有多个优势，包括能够同时分析多个模型，在各种网络架构上工作，不对用于改变网络行为的触发器的性质做任何假设，并且计算效率高。我们提供了对检测流程的详细描述以及结果。

    As deep neural networks and the datasets used to train them get larger, the default approach to integrating them into research and commercial projects is to download a pre-trained model and fine tune it. But these models can have uncertain provenance, opening up the possibility that they embed hidden malicious behavior such as trojans or backdoors, where small changes to an input (triggers) can cause the model to produce incorrect outputs (e.g., to misclassify). This paper introduces a novel approach to backdoor detection that uses two tensor decomposition methods applied to network activations. This has a number of advantages relative to existing detection methods, including the ability to analyze multiple models at the same time, working across a wide variety of network architectures, making no assumptions about the nature of triggers used to alter network behavior, and being computationally efficient. We provide a detailed description of the detection pipeline along with results on 
    
[^92]: TRLS:一种基于声谱图的医学信号处理时间序列表示学习框架

    TRLS: A Time Series Representation Learning Framework via Spectrogram for Medical Signal Processing. (arXiv:2401.05431v1 [eess.SP])

    [http://arxiv.org/abs/2401.05431](http://arxiv.org/abs/2401.05431)

    TRLS是一种通过声谱图进行时间序列表示学习的医学信号处理框架，通过利用时间频率RNN从增强声谱图中提取出更多信息，并在医学信号分类中显示出更好的性能。

    

    在无标签时间序列中，提出了用于医学信号处理的表示学习框架。尽管在之前的工作中取得了许多卓越进展，但我们观察到提取的时间序列表示仍然不能很好地泛化。在本文中，我们提出了一种通过声谱图进行时间序列（医学信号）表示学习的框架（TRLS），以获得更多信息的表示。我们将输入的时域医学信号转化为声谱图，并设计了一个称为时间频率RNN（TFRNN）的时频编码器，从增强的声谱图中捕捉更稳健的多尺度表示。我们的TRLS以声谱图作为输入，具有两种不同的数据增强方式，并最大化正样本之间的相似度，从而有效地规避了设计负样本的问题。我们对四个真实医学信号数据集进行的医学信号分类评估表明，TRLS优于现有的方法。

    Representation learning frameworks in unlabeled time series have been proposed for medical signal processing. Despite the numerous excellent progresses have been made in previous works, we observe the representation extracted for the time series still does not generalize well. In this paper, we present a Time series (medical signal) Representation Learning framework via Spectrogram (TRLS) to get more informative representations. We transform the input time-domain medical signals into spectrograms and design a time-frequency encoder named Time Frequency RNN (TFRNN) to capture more robust multi-scale representations from the augmented spectrograms. Our TRLS takes spectrogram as input with two types of different data augmentations and maximizes the similarity between positive ones, which effectively circumvents the problem of designing negative samples. Our evaluation of four real-world medical signal datasets focusing on medical signal classification shows that TRLS is superior to the ex
    
[^93]: 具有并行保留的多关系图扩散神经网络用于股票趋势分类

    Multi-relational Graph Diffusion Neural Network with Parallel Retention for Stock Trends Classification. (arXiv:2401.05430v1 [q-fin.ST])

    [http://arxiv.org/abs/2401.05430](http://arxiv.org/abs/2401.05430)

    该论文提出了一种基于多关系图的股票趋势分类方法，通过生成动态的多关系股票图来建模股票之间复杂的时变关系，并通过扩散过程优化图表示。最终的图表示能够更好地捕捉各个股票内部的时间特征，同时考虑股票之间的关系。

    

    股票趋势分类仍然是一项基本但具有挑战性的任务，因为股票之间和内部之间的复杂时变动力学。为了解决这两个挑战，我们提出了一种基于图的表示学习方法，旨在预测多个股票的未来走势。首先，我们通过生成动态多关系股票图来建模股票之间复杂的时变关系。这是通过一种新颖的边生成算法实现的，该算法利用信息熵和信号能量来量化每个交易日的股票之间关系的强度和方向性。然后，我们通过随机的多关系扩散过程进一步优化这些初始图，自适应地学习任务最优边。随后，我们使用并行保留的解耦表示学习方案来获得最终的图表示。这种策略更好地捕捉了各个股票内部的独特时间特征，同时也能够考虑股票之间的关系。

    Stock trend classification remains a fundamental yet challenging task, owing to the intricate time-evolving dynamics between and within stocks. To tackle these two challenges, we propose a graph-based representation learning approach aimed at predicting the future movements of multiple stocks. Initially, we model the complex time-varying relationships between stocks by generating dynamic multi-relational stock graphs. This is achieved through a novel edge generation algorithm that leverages information entropy and signal energy to quantify the intensity and directionality of inter-stock relations on each trading day. Then, we further refine these initial graphs through a stochastic multi-relational diffusion process, adaptively learning task-optimal edges. Subsequently, we implement a decoupled representation learning scheme with parallel retention to obtain the final graph representation. This strategy better captures the unique temporal features within individual stocks while also ca
    
[^94]: CoSS：针对数据高效AI的传感器和采样率优化在人体活动识别中的应用

    CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in Human Activity Recognition. (arXiv:2401.05426v1 [eess.SP])

    [http://arxiv.org/abs/2401.05426](http://arxiv.org/abs/2401.05426)

    本论文提出了一个实用的框架用于在人体活动识别任务中高效利用数据，同时考虑传感器模态和采样率的优化，通过设计的可训练参数来指导传感器模态和采样率的选择。

    

    最近神经网络技术的进步显著提高了利用多个时间序列传感器进行人体活动识别的效果。虽然使用大量传感器和高采样率通常可以提高结果，但往往会导致数据低效和人工神经网络的不必要扩展，给在边缘设备上的实际应用带来挑战。为了应对这些问题，我们的工作引入了一个实用的框架，用于在HAR任务中的数据高效利用，同时考虑传感器模态和采样率的优化。我们方法的核心是设计的可训练参数，称为“权重分数”，它们评估训练阶段中每个传感器模态和采样率的重要性。这些分数指导传感器模态和采样率的选择。修剪方法允许用户在计算预算和性能之间进行权衡，根据选择传感器模态和采样率。

    Recent advancements in Artificial Neural Networks have significantly improved human activity recognition using multiple time-series sensors. While employing numerous sensors with high-frequency sampling rates usually improves the results, it often leads to data inefficiency and unnecessary expansion of the ANN, posing a challenge for their practical deployment on edge devices. Addressing these issues, our work introduces a pragmatic framework for data-efficient utilization in HAR tasks, considering the optimization of both sensor modalities and sampling rate simultaneously. Central to our approach are the designed trainable parameters, termed 'Weight Scores,' which assess the significance of each sensor modality and sampling rate during the training phase. These scores guide the sensor modalities and sampling rate selection. The pruning method allows users to make a trade-off between computational budgets and performance by selecting the sensor modalities and sampling rates according t
    
[^95]: 一种无干扰且轻量级的耳戴系统用于连续癫痫发作检测

    An Unobtrusive and Lightweight Ear-worn System for Continuous Epileptic Seizure Detection. (arXiv:2401.05425v1 [eess.SP])

    [http://arxiv.org/abs/2401.05425](http://arxiv.org/abs/2401.05425)

    本文提出了一种无干扰且轻量级的耳戴系统EarSD，通过测量用户耳后部位的生理信号，实现了对癫痫发作的连续检测。这种系统相比传统的基于头皮的脑电图测试具有成本低、便携性好、使用舒适的优点。

    

    癫痫是全球最常见的神经疾病之一，影响着全球约5000万人。幸运的是，如果能得到正确的诊断和治疗，高达70%的癫痫患者可以无癫痫发作地生活，而一种可靠的监测癫痫发作的技术可以提高那些不断面临随机发作恐惧的患者的生活质量。尽管基于头皮的脑电图（EEG）测试是诊断癫痫的黄金标准，但该方法成本高、需要住院治疗、需要熟练的操作人员，并且对用户来说不舒适。在本文中，我们提出了EarSD，一种新颖的轻量级、无干扰和社会接受度高的耳戴系统，通过测量用户耳后部位的生理信号来检测癫痫发作的开始。EarSD包括一个集成的自定义传感、计算和通信电路板，用于收集和放大感兴趣的信号，去除运动伪影和环境噪声。

    Epilepsy is one of the most common neurological diseases globally, affecting around 50 million people worldwide. Fortunately, up to 70 percent of people with epilepsy could live seizure-free if properly diagnosed and treated, and a reliable technique to monitor the onset of seizures could improve the quality of life of patients who are constantly facing the fear of random seizure attacks. The scalp-based EEG test, despite being the gold standard for diagnosing epilepsy, is costly, necessitates hospitalization, demands skilled professionals for operation, and is discomforting for users. In this paper, we propose EarSD, a novel lightweight, unobtrusive, and socially acceptable ear-worn system to detect epileptic seizure onsets by measuring the physiological signals from behind the user's ears. EarSD includes an integrated custom-built sensing, computing, and communication PCB to collect and amplify the signals of interest, remove the noises caused by motion artifacts and environmental im
    
[^96]: 用于建模教育视频参与度的工具箱

    A Toolbox for Modelling Engagement with Educational Videos. (arXiv:2401.05424v1 [cs.CY])

    [http://arxiv.org/abs/2401.05424](http://arxiv.org/abs/2401.05424)

    这项工作介绍了PEEKC数据集和TrueLearn Python库，可用于建模教育视频的参与度。TrueLearn模型系列遵循"开放学习者"概念，提供可扩展的在线模型以及较高的预测性能。

    

    随着人工智能（AI）的进步和应用，将教育个性化到全球人口可能成为未来新教育系统的基石。本研究介绍了PEEKC数据集和TrueLearn Python库，其中包含了一系列在线学习者状态模型和一个数据集，这些对于促进学习者参与度建模研究至关重要。TrueLearn模型系列遵循“开放学习者”概念，使用人性化的用户表示法进行设计。这一可扩展的在线模型系列还帮助终端用户可视化学习者模型，这在未来可能促进用户与自己的模型/推荐系统进行交互。丰富的文档和编码示例使该库对机器学习开发人员和教育数据挖掘与学习分析实践者都非常易于访问。实验结果显示了数据集和库的实用性，预测性能明显超过对比方法。

    With the advancement and utility of Artificial Intelligence (AI), personalising education to a global population could be a cornerstone of new educational systems in the future. This work presents the PEEKC dataset and the TrueLearn Python library, which contains a dataset and a series of online learner state models that are essential to facilitate research on learner engagement modelling.TrueLearn family of models was designed following the "open learner" concept, using humanly-intuitive user representations. This family of scalable, online models also help end-users visualise the learner models, which may in the future facilitate user interaction with their models/recommenders. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytics practitioners. The experiments show the utility of both the dataset and the library with predictive performance significantly exceeding compa
    
[^97]: WildGEN：用于野生动物的长时间轨迹生成

    WildGEN: Long-horizon Trajectory Generation for Wildlife. (arXiv:2401.05421v1 [cs.LG])

    [http://arxiv.org/abs/2401.05421](http://arxiv.org/abs/2401.05421)

    本文提出了一个名为WildGEN的概念框架，通过使用变分自编码器（VAEs）方法生成野生动物长时间范围内的运动轨迹。通过平滑滤波器对生成轨迹进行后处理，以提高生成轨迹的质量。评估结果通过视觉检查和计算生成轨迹与真实轨迹之间的Hausdorff距离。

    

    轨迹生成是研究行人、车辆和野生动物移动的重要问题。生成的轨迹有助于深度学习应用的训练语料库丰富，并可用于促进模拟任务。这在野生动物领域尤为重要，因为获取额外的真实数据的成本可能非常昂贵、耗时且涉及伦理考虑。本文提出了WildGEN：一种概念框架，通过使用基于变分自编码器（VAEs）的方法，在稀疏的真实样本集上获得野鹅在长时间范围内展示的运动特征。随后，通过平滑滤波器对生成的轨迹进行后处理，以减少过度游荡。我们通过视觉检查和生成轨迹与真实轨迹之间的Hausdorff距离的计算来进行评估。

    Trajectory generation is an important concern in pedestrian, vehicle, and wildlife movement studies. Generated trajectories help enrich the training corpus in relation to deep learning applications, and may be used to facilitate simulation tasks. This is especially significant in the wildlife domain, where the cost of obtaining additional real data can be prohibitively expensive, time-consuming, and bear ethical considerations. In this paper, we introduce WildGEN: a conceptual framework that addresses this challenge by employing a Variational Auto-encoders (VAEs) based method for the acquisition of movement characteristics exhibited by wild geese over a long horizon using a sparse set of truth samples. A subsequent post-processing step of the generated trajectories is performed based on smoothing filters to reduce excessive wandering. Our evaluation is conducted through visual inspection and the computation of the Hausdorff distance between the generated and real trajectories. In addit
    
[^98]: HoloBeam:学习远场全息元表面收发器中的最佳波束成型

    HoloBeam: Learning Optimal Beamforming in Far-Field Holographic Metasurface Transceivers. (arXiv:2401.05420v1 [eess.SP])

    [http://arxiv.org/abs/2401.05420](http://arxiv.org/abs/2401.05420)

    Holographic Metasurface Transceivers (HMTs) are being used as cost-effective substitutes for large antenna arrays for beamforming in Millimeter and TeraHertz wave communication. In this work, a learning algorithm is developed to optimize beamforming in far-field regions, by using a fixed-budget multi-armed bandit framework to maximize received signal strength. The algorithm exploits the parametric form of channel gains and works with the discrete values of phase-shifting parameters.

    

    全息元表面收发器（HMT）正以经济实惠的方式成为毫米和太赫兹波通信中波束成型的替代品。然而，在HMT中通过波束成型实现所需信道增益需要适当设置大量元素的相移，这是具有挑战性的。此外，这些最佳相移依赖于接收器位置，这可能是未知的。在本文中，我们使用{\it 固定预算多臂老虎机算法}开发了一个学习算法，以在远场区域进行波束成型并最大化接收信号强度。我们的算法名为 \Algo，利用了波束的通道增益参数形式，它可以用两个{\it 相移参数}表示。即使进行参数化，问题仍然具有挑战性，因为相移参数采用连续值。为了克服这个问题，\HB 使用相移参数的离散值并利用其子结构。

    Holographic Metasurface Transceivers (HMTs) are emerging as cost-effective substitutes to large antenna arrays for beamforming in Millimeter and TeraHertz wave communication. However, to achieve desired channel gains through beamforming in HMT, phase-shifts of a large number of elements need to be appropriately set, which is challenging. Also, these optimal phase-shifts depend on the location of the receivers, which could be unknown. In this work, we develop a learning algorithm using a {\it fixed-budget multi-armed bandit framework} to beamform and maximize received signal strength at the receiver for far-field regions. Our algorithm, named \Algo exploits the parametric form of channel gains of the beams, which can be expressed in terms of two {\it phase-shifting parameters}. Even after parameterization, the problem is still challenging as phase-shifting parameters take continuous values. To overcome this, {\it\HB} works with the discrete values of phase-shifting parameters and exploi
    
[^99]: ANALYTiC: 机器学习中的决策边界和降维理解

    ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction in Machine Learning. (arXiv:2401.05418v1 [eess.SP])

    [http://arxiv.org/abs/2401.05418](http://arxiv.org/abs/2401.05418)

    本论文通过将降维和决策边界应用于现有的主动学习方法，突显数据中的模式和聚类，并通过实验验证了这种方法在提高轨迹标记效率和准确性方面的潜力。

    

    紧凑便携设备的出现给我们带来了一批可以用于推断趋势和模式的跟踪运动数据。本研究探讨了在现有主动学习方法基础上应用降维和决策边界的组合，突显数据中的模式和聚类。我们使用三个不同的轨迹数据集进行了实验分析，旨在利用已标记的数据并提高它们的可解释性。实验结果展示了这些组合方法在提高轨迹标记的效率和准确性方面的潜力。本研究为将机器学习和视觉方法在上下文中更广泛地集成打下了基础。

    The advent of compact, handheld devices has given us a pool of tracked movement data that could be used to infer trends and patterns that can be made to use. With this flooding of various trajectory data of animals, humans, vehicles, etc., the idea of ANALYTiC originated, using active learning to infer semantic annotations from the trajectories by learning from sets of labeled data. This study explores the application of dimensionality reduction and decision boundaries in combination with the already present active learning, highlighting patterns and clusters in data. We test these features with three different trajectory datasets with objective of exploiting the the already labeled data and enhance their interpretability. Our experimental analysis exemplifies the potential of these combined methodologies in improving the efficiency and accuracy of trajectory labeling. This study serves as a stepping-stone towards the broader integration of machine learning and visual methods in contex
    
[^100]: 惯性传感器信号增强的小波动态选择网络

    Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement. (arXiv:2401.05416v1 [eess.SP])

    [http://arxiv.org/abs/2401.05416](http://arxiv.org/abs/2401.05416)

    该论文提出了一种名为WDSNet的小波动态选择网络，用于智能选择适当的小波基函数以增强惯性传感器信号。此外，论文还提出了一种类别表示机制，用于提高小波基函数的选择能力。

    

    作为姿态和运动感应部件，惯性传感器广泛应用于各种便携设备中。然而，惯性传感器的严重误差限制了它们的功能，特别是轨迹恢复和语义识别。小波作为主流的信号处理方法，由于丰富多样的小波基函数而被誉为信号的数学显微镜。然而，惯性传感器的复杂噪声类型和应用场景使得选择适当的小波基函数变得困难。为此，我们提出了一种小波动态选择网络（WDSNet），它能智能地为可变惯性信号选择适当的小波基函数。此外，现有的深度学习架构擅长从输入数据中提取特征，但忽视了学习目标类别的特征，这对于提高类别感知能力、改善小波基函数的选择是必要的。因此，我们提出了一种类别表示机制。

    As attitude and motion sensing components, inertial sensors are widely used in various portable devices. But the severe errors of inertial sensors restrain their function, especially the trajectory recovery and semantic recognition. As a mainstream signal processing method, wavelet is hailed as the mathematical microscope of signal due to the plentiful and diverse wavelet basis functions. However, complicated noise types and application scenarios of inertial sensors make selecting wavelet basis perplexing. To this end, we propose a wavelet dynamic selection network (WDSNet), which intelligently selects the appropriate wavelet basis for variable inertial signals. In addition, existing deep learning architectures excel at extracting features from input data but neglect to learn the characteristics of target categories, which is essential to enhance the category awareness capability, thereby improving the selection of wavelet basis. Therefore, we propose a category representation mechanis
    
[^101]: 关于金融中的三个因果性困境: 时间分辨率、非平稳性和潜在因素

    On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors. (arXiv:2401.05414v1 [q-fin.ST])

    [http://arxiv.org/abs/2401.05414](http://arxiv.org/abs/2401.05414)

    本文从因果性的角度系统研究了金融领域中的三个困境：时间分辨率不匹配、非平稳性和未知因果因素，并提出了解决方案。

    

    金融数据本质上是时间序列数据，因此存在三个基本问题：时间分辨率不匹配、分布的时变性-非平稳性以及重要但未知/未观测的因果因素。在本文中，我们从因果性的角度系统地研究金融中的这三个困境。具体而言，我们重新审视了这些问题，并在因果性的背景下提出了新颖而有启发性的解决方案，为未来研究提供一个基础。

    Financial data is generally time series in essence and thus suffers from three fundamental issues: the mismatch in time resolution, the time-varying property of the distribution - nonstationarity, and causal factors that are important but unknown/unobserved. In this paper, we follow a causal perspective to systematically look into these three demons in finance. Specifically, we reexamine these issues in the context of causality, which gives rise to a novel and inspiring understanding of how the issues can be addressed. Following this perspective, we provide systematic solutions to these problems, which hopefully would serve as a foundation for future research in the area.
    
[^102]: RawECGNet: 从原始心电图中进行房颤检测的深度学习泛化

    RawECGNet: Deep Learning Generalization for Atrial Fibrillation Detection from the Raw ECG. (arXiv:2401.05411v1 [eess.SP])

    [http://arxiv.org/abs/2401.05411](http://arxiv.org/abs/2401.05411)

    我们开发了一个名为RawECGNet的深度学习模型，使用原始的单导联心电图来检测房颤和心房扑动的发作。与基于节律的方法相比，RawECGNet利用了更多的形态学信息，表现出更好的性能。

    

    引言：利用长期、无创的心电图记录进行房颤（AF）的检测的深度学习模型表现出很高的性能。然而，基于节律的方法没有充分利用不同心电波形所传递的形态学信息，特别是f波。因此，这种模型的性能可能受到固有限制。方法：为了解决这个限制，我们开发了一个名为RawECGNet的深度学习模型，使用原始、单导联的心电图来检测房颤和心房扑动（AFl）的发作。我们比较了RawECGNet在两个外部数据集上的泛化性能，这些数据集考虑了地理、种族和导联位置的分布变化。RawECGNet还与一种名为ArNet2的最先进的深度学习模型进行了基准测试，该模型利用节律信息作为输入。结果：使用RawECGNet，在外部测试集中各个导联的F1得分如下

    Introduction: Deep learning models for detecting episodes of atrial fibrillation (AF) using rhythm information in long-term, ambulatory ECG recordings have shown high performance. However, the rhythm-based approach does not take advantage of the morphological information conveyed by the different ECG waveforms, particularly the f-waves. As a result, the performance of such models may be inherently limited. Methods: To address this limitation, we have developed a deep learning model, named RawECGNet, to detect episodes of AF and atrial flutter (AFl) using the raw, single-lead ECG. We compare the generalization performance of RawECGNet on two external data sets that account for distribution shifts in geography, ethnicity, and lead position. RawECGNet is further benchmarked against a state-of-the-art deep learning model, named ArNet2, which utilizes rhythm information as input. Results: Using RawECGNet, the results for the different leads in the external test sets in terms of the F1 score
    
[^103]: 使用UWB多静态无线电实现无设备的人体状态估计

    Device-Free Human State Estimation using UWB Multi-Static Radios. (arXiv:2401.05410v1 [eess.SP])

    [http://arxiv.org/abs/2401.05410](http://arxiv.org/abs/2401.05410)

    这项研究提出了一种在室内环境中通过利用UWB多静态无线电来实现无设备的人体状态估计的方式，可以估计人的位置、活动和区域内的人数。

    

    我们提出了一个人体状态估计框架，可以在室内环境中估计人的位置和活动，而无需他们携带特定设备。为了实现“无设备”的定位，我们使用了分布在感兴趣的环境中的少量低成本超宽带（UWB）传感器。为了从环境中仅反射人体的UWB信号中实现高质量的估计，我们利用了一个可以学习推理的深度网络。硬件设置包括用于感知的商业现成（COTS）单天线UWB模块，配对树莓派单元用于计算处理和数据传输。我们利用UWB传感器的信道脉冲响应（CIR）测量来估计在给定区域内的人体状态 - 包括位置和活动。此外，我们还可以估计占据该感兴趣区域的人数。在我们的方法中，

    We present a human state estimation framework that allows us to estimate the location, and even the activities, of people in an indoor environment without the requirement that they carry a specific devices with them. To achieve this "device free" localization we use a small number of low-cost Ultra-Wide Band (UWB) sensors distributed across the environment of interest. To achieve high quality estimation from the UWB signals merely reflected of people in the environment, we exploit a deep network that can learn to make inferences. The hardware setup consists of commercial off-the-shelf (COTS) single antenna UWB modules for sensing, paired with Raspberry PI units for computational processing and data transfer. We make use of the channel impulse response (CIR) measurements from the UWB sensors to estimate the human state - comprised of location and activity - in a given area. Additionally, we can also estimate the number of humans that occupy this region of interest. In our approach, firs
    
[^104]: 基于图像的时间序列数据表示：脑电图伪迹检测的比较分析

    Image-based Data Representations of Time Series: A Comparative Analysis in EEG Artifact Detection. (arXiv:2401.05409v1 [eess.SP])

    [http://arxiv.org/abs/2401.05409](http://arxiv.org/abs/2401.05409)

    本文通过针对脑电图数据中的伪迹检测和分类，对十一个深度学习架构在六种常用表示方法上进行了比较分析，并发现某些表示方法在突出数据的信噪比方面更有效。

    

    替代数据表示是增强下游模型性能的强大工具。然而，在机器学习工具箱中存在大量这样的表示方法，领域内缺乏对每种表示方法的适用性的比较理解。在本文中，我们将脑电图数据中的伪迹检测和分类作为测试平台，评估了六种常用表示方法上的十一个流行深度学习架构。我们发现，虽然表示的选择涉及偏差和方差之间的权衡，但某些表示在突出数据的信噪比方面实际上更有效。我们展示了针对脑电图数据的结果，并开源了我们的测试框架，以便未来进行这种方法的比较分析。

    Alternative data representations are powerful tools that augment the performance of downstream models. However, there is an abundance of such representations within the machine learning toolbox, and the field lacks a comparative understanding of the suitability of each representation method.  In this paper, we propose artifact detection and classification within EEG data as a testbed for profiling image-based data representations of time series data. We then evaluate eleven popular deep learning architectures on each of six commonly-used representation methods.  We find that, while the choice of representation entails a choice within the tradeoff between bias and variance, certain representations are practically more effective in highlighting features which increase the signal-to-noise ratio of the data. We present our results on EEG data, and open-source our testing framework to enable future comparative analyses in this vein.
    
[^105]: 从可穿戴设备中解码情感价值：我们的数据能否揭示我们真实的情感？

    Decoding Emotional Valence from Wearables: Can Our Data Reveal Our True Feelings?. (arXiv:2401.05408v1 [eess.SP])

    [http://arxiv.org/abs/2401.05408](http://arxiv.org/abs/2401.05408)

    该研究通过利用可穿戴设备和自我报告措施，旨在解决实验室研究和现实场景之间的差距，并取得了有关用户情感价值的有希望的分类结果。

    

    自动检测和跟踪情感状态有助于帮助各种心理健康状况的个体。尽管先前的研究已经在实验室环境中利用可穿戴设备捕捉了生理信号，为了解生理反应和心理状态的关系提供了有价值的见解，但是将这些发现转化到现实生活场景仍处于初级阶段。我们的研究旨在通过利用消费级可穿戴设备和自我报告措施来弥合实验室研究和现实场景之间的差距。我们进行了一项初步研究，涉及15名健康参与者，以评估在现实场景中捕捉用户情感价值的可穿戴设备的功效。在本文中，我们首先对收集到的数据进行了初步分析，重点关注情感价值分类的结果。我们的研究结果显示，在高和低正面情感之间具有很好的区分能力，F1分数达到0.65。

    Automatic detection and tracking of emotional states has the potential for helping individuals with various mental health conditions. While previous studies have captured physiological signals using wearable devices in laboratory settings, providing valuable insights into the relationship between physiological responses and mental states, the transfer of these findings to real-life scenarios is still in its nascent stages. Our research aims to bridge the gap between laboratory-based studies and real-life settings by leveraging consumer-grade wearables and self-report measures. We conducted a preliminary study involving 15 healthy participants to assess the efficacy of wearables in capturing user valence in real-world settings. In this paper, we present the initial analysis of the collected data, focusing primarily on the results of valence classification. Our findings demonstrate promising results in distinguishing between high and low positive valence, achieving an F1 score of 0.65. T
    
[^106]: 机器学习和特征排序在多传感器数据的冲击坠落检测事件中的应用

    Machine Learning and Feature Ranking for Impact Fall Detection Event Using Multisensor Data. (arXiv:2401.05407v1 [eess.SP])

    [http://arxiv.org/abs/2401.05407](http://arxiv.org/abs/2401.05407)

    本论文通过对多传感器数据进行彻底的预处理和特征选择，成功应用机器学习模型实现了冲击坠落检测，取得了较高的准确率。

    

    个人的跌倒，特别是老年人，可能导致严重的伤害和并发症。在跌倒事件中检测冲击瞬间对于及时提供帮助和减少负面影响至关重要。在这项工作中，我们通过对多传感器数据集应用彻底的预处理技术来解决这个挑战，目的是消除噪音并提高数据质量。此外，我们还使用特征选择过程来识别多传感器UP-FALL数据集中最相关的特征，从而提高机器学习模型的性能和效率。然后，我们使用多个传感器的结果数据信息评估各种机器学习模型在检测冲击瞬间方面的效率。通过大量实验，我们使用各种评估指标评估了我们方法的准确性。我们的结果在冲击检测方面取得了较高的准确率，展示了利用多传感器数据信息的能力。

    Falls among individuals, especially the elderly population, can lead to serious injuries and complications. Detecting impact moments within a fall event is crucial for providing timely assistance and minimizing the negative consequences. In this work, we aim to address this challenge by applying thorough preprocessing techniques to the multisensor dataset, the goal is to eliminate noise and improve data quality. Furthermore, we employ a feature selection process to identify the most relevant features derived from the multisensor UP-FALL dataset, which in turn will enhance the performance and efficiency of machine learning models. We then evaluate the efficiency of various machine learning models in detecting the impact moment using the resulting data information from multiple sensors. Through extensive experimentation, we assess the accuracy of our approach using various evaluation metrics. Our results achieve high accuracy rates in impact detection, showcasing the power of leveraging 
    
[^107]: RFRL Gym: 用于认知无线电应用的强化学习测试平台

    RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio Applications. (arXiv:2401.05406v1 [eess.SP])

    [http://arxiv.org/abs/2401.05406](http://arxiv.org/abs/2401.05406)

    RFRL Gym是一个用于认知无线电应用的强化学习测试平台，可以帮助开发和测试RFRL技术，模拟无线电频谱环境，并实验不同的频谱感知技术。

    

    无线电频率强化学习（RFRL）预计将成为下一代无线通信系统（特别是6G和下一代军事通信）中广泛应用的技术。基于此，我们的研究致力于开发一个工具，以促进利用频谱感知的RFRL技术的发展。具体而言，该工具旨在解决两个认知无线电应用，即动态频谱接入和干扰。为了训练和测试这些应用的强化学习（RL）算法，需要一个模拟环境来模拟无线电频谱中代理人将遇到的条件。本文中，我们开发了这样一个环境，称为RFRL Gym。通过RFRL Gym，用户可以设计自己的场景来模拟RL代理人在无线电频谱中可能遇到的情况，并尝试不同的频谱感知技术。

    Radio Frequency Reinforcement Learning (RFRL) is anticipated to be a widely applicable technology in the next generation of wireless communication systems, particularly 6G and next-gen military communications. Given this, our research is focused on developing a tool to promote the development of RFRL techniques that leverage spectrum sensing. In particular, the tool was designed to address two cognitive radio applications, specifically dynamic spectrum access and jamming. In order to train and test reinforcement learning (RL) algorithms for these applications, a simulation environment is necessary to simulate the conditions that an agent will encounter within the Radio Frequency (RF) spectrum. In this paper, such an environment has been developed, herein referred to as the RFRL Gym. Through the RFRL Gym, users can design their own scenarios to model what an RL agent may encounter within the RF spectrum as well as experiment with different spectrum sensing techniques. Additionally, the 
    
[^108]: SelfEEG: 一种用于自监督学习的脑电图Python库

    SelfEEG: A Python library for Self-Supervised Learning in Electroencephalography. (arXiv:2401.05405v1 [eess.SP])

    [http://arxiv.org/abs/2401.05405](http://arxiv.org/abs/2401.05405)

    SelfEEG是一个为研究人员提供的开源Python库，旨在帮助他们在脑电图数据上进行自监督学习实验。它提供了用户友好且高度可定制的环境，涵盖了从数据导入到模型设计和训练的所有阶段，并提供了各种功能来有效地处理脑电图数据。

    

    SelfEEG是一个开源的Python库，旨在帮助研究人员在脑电图（EEG）数据上进行自监督学习（SSL）实验。它的主要目标是提供一个用户友好但高度可定制的环境，使用户能够高效地设计和执行脑电图数据上的自监督学习任务。SelfEEG涵盖了典型自监督学习流程的各个阶段，包括数据导入、模型设计和训练。它包括专门设计的模块，用于在不同粒度水平（例如基于会议、受试者或数据集的划分）上分割数据；在小批量构建期间高效管理存储有不同配置（例如文件扩展名、数据类型）的数据；提供广泛的标准深度学习模型、数据增强和应用于脑电图数据的自监督学习基准方法。SelfEEG的大多数功能既可以在GPU上执行，也可以在CPU上执行，扩展了其可用性超出自监督学习领域。

    SelfEEG is an open-source Python library developed to assist researchers in conducting Self-Supervised Learning (SSL) experiments on electroencephalography (EEG) data. Its primary objective is to offer a user-friendly but highly customizable environment, enabling users to efficiently design and execute self-supervised learning tasks on EEG data.  SelfEEG covers all the stages of a typical SSL pipeline, ranging from data import to model design and training. It includes modules specifically designed to: split data at various granularity levels (e.g., session-, subject-, or dataset-based splits); effectively manage data stored with different configurations (e.g., file extensions, data types) during mini-batch construction; provide a wide range of standard deep learning models, data augmentations and SSL baseline methods applied to EEG data.  Most of the functionalities offered by selfEEG can be executed both on GPUs and CPUs, expanding its usability beyond the self-supervised learning are
    
[^109]: 基于向量场定向扩散模型的晶体材料生成

    Vector Field Oriented Diffusion Model for Crystal Material Generation. (arXiv:2401.05402v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2401.05402](http://arxiv.org/abs/2401.05402)

    提出了一种基于向量场定向扩散模型的晶体材料生成方法，该方法通过利用几何等变GNN同时考虑原子位置和晶体晶格，提供了一种能够更全面评估模型能力的新生成指标。实验证明了该扩散模型的重要性和有效性。

    

    在材料科学中，发现具有特定化学性质的晶体结构已成为越来越重要的研究焦点。然而，当前的模型在生成新的晶体晶格方面存在局限性，因为它们只考虑原子位置或化学组成。为解决这个问题，我们提出了一种概率扩散模型，利用几何等变GNN同时考虑原子位置和晶体晶格。为评估我们模型的有效性，我们引入了一个受Frechet Inception Distance启发的新的生成指标，但基于GNN能量预测，而不是计算机视觉中使用的InceptionV3。除了常用的评估指标（如有效性，评估结构的合理性）外，这种新指标提供了对我们模型能力更全面的评估。我们在现有基准上的实验证明了我们扩散模型的重要性。我们还展示了我们的方法能够有效地学习有意义的表示。

    Discovering crystal structures with specific chemical properties has become an increasingly important focus in material science. However, current models are limited in their ability to generate new crystal lattices, as they only consider atomic positions or chemical composition. To address this issue, we propose a probabilistic diffusion model that utilizes a geometrically equivariant GNN to consider atomic positions and crystal lattices jointly. To evaluate the effectiveness of our model, we introduce a new generation metric inspired by Frechet Inception Distance, but based on GNN energy prediction rather than InceptionV3 used in computer vision. In addition to commonly used metrics like validity, which assesses the plausibility of a structure, this new metric offers a more comprehensive evaluation of our model's capabilities. Our experiments on existing benchmarks show the significance of our diffusion model. We also show that our method can effectively learn meaningful representatio
    
[^110]: 准确度测定的高光谱光变反演

    Hyperspectral Lightcurve Inversion for Attitude Determination. (arXiv:2401.05397v1 [eess.SP])

    [http://arxiv.org/abs/2401.05397](http://arxiv.org/abs/2401.05397)

    该论文研究了利用高光谱光变推断航天器姿态和旋转的方法。通过数值优化和机器学习两种方法，利用最小信息条件完成准确度测定任务，并在合成数据上进行了测试。

    

    利用由航天器的时间序列单像素光谱测量组成的光谱光变，推断航天器的姿态和旋转。采用两种方法，一种是基于正则化最小二乘代价函数的数值优化，另一种是基于神经网络模型的机器学习。目标是在最小的信息条件下进行工作，因此没有关于姿态和惯性张量的先验信息。研究了这个任务的理论和实际方面，并在合成数据上测试了方法。结果基于合成数据展示。

    Spectral lightcurves consisting of time series single-pixel spectral measurements of spacecraft are used to infer the spacecraft's attitude and rotation. Two methods are used. One based on numerical optimisation of a regularised least squares cost function, and another based on machine learning with a neural network model. The aim is to work with minimal information, thus no prior is available on the attitude nor on the inertia tensor. The theoretical and practical aspects of this task are investigated, and the methodology is tested on synthetic data. Results are shown based on synthetic data.
    
[^111]: SRNI-CAR: 用于分析中国汽车市场的综合数据集

    SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive market. (arXiv:2401.05395v1 [econ.GN])

    [http://arxiv.org/abs/2401.05395](http://arxiv.org/abs/2401.05395)

    SRNI-CAR是一份用于分析中国汽车市场的综合数据集，填补了现有汽车行业数据集覆盖范围有限的缺口，对于提高预测准确性、扩大商业应用范围、指导政策制定与监管以及推动汽车行业的学术研究具有重要影响。

    

    汽车行业在全球经济中起着关键作用，尤其重要的是中国汽车市场的不断扩大，由于其巨大的规模和影响力。然而，现有的汽车行业数据集在覆盖范围上有限，未能充分考虑到对更多和多样化变量的不断增长的需求。本文旨在通过介绍一份从2016年到2022年的综合数据集来填补这一数据缺口，该数据集包括销售数据，在线评论以及与中国汽车行业相关的大量信息。该数据集作为宝贵的资源，极大地扩展了可用数据。它的影响力涵盖了多个方面，包括提高预测准确性，扩大商业应用范围，指导政策制定与监管，推动汽车行业的学术研究。为了展示该数据集在商业和学术背景下的潜在应用，我们提出了两个应用案例。

    The automotive industry plays a critical role in the global economy, and particularly important is the expanding Chinese automobile market due to its immense scale and influence. However, existing automotive sector datasets are limited in their coverage, failing to adequately consider the growing demand for more and diverse variables. This paper aims to bridge this data gap by introducing a comprehensive dataset spanning the years from 2016 to 2022, encompassing sales data, online reviews, and a wealth of information related to the Chinese automotive industry. This dataset serves as a valuable resource, significantly expanding the available data. Its impact extends to various dimensions, including improving forecasting accuracy, expanding the scope of business applications, informing policy development and regulation, and advancing academic research within the automotive sector. To illustrate the dataset's potential applications in both business and academic contexts, we present two ap
    
[^112]: 迭代正则化与k支撑范数：稀疏恢复的重要补充

    Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery. (arXiv:2401.05394v1 [eess.SP])

    [http://arxiv.org/abs/2401.05394](http://arxiv.org/abs/2401.05394)

    该论文介绍了一种新的迭代正则化算法IRKSN，它通过使用$k$支撑范数正则化实现稀疏恢复，并提供了条件。这是对基于$\ell_1$范数的迭代方法的一种重要补充。

    

    稀疏恢复在机器学习和信号处理中无处不在。由于稀疏恢复的NP困难性质，现有方法通常要么受限于适用条件（甚至未知），要么计算成本高。最近，迭代正则化方法作为一种快速方法出现，因为它们可以通过提前停止一次通过来实现稀疏恢复，而不是传统方法中繁琐的网格搜索。然而，大多数这些迭代方法都基于$\ell_1$范数，需要受限的适用条件，并且在许多情况下可能会失败。因此，迭代正则化方法在更广泛的条件下实现稀疏恢复仍需进一步探索。为了解决这个问题，我们提出了一种新的迭代正则化算法IRKSN，它基于$k$支撑范数正则化而不是$\ell_1$范数。我们提供了使用IRKSN进行稀疏恢复的条件，并进行了比较。

    Sparse recovery is ubiquitous in machine learning and signal processing. Due to the NP-hard nature of sparse recovery, existing methods are known to suffer either from restrictive (or even unknown) applicability conditions, or high computational cost. Recently, iterative regularization methods have emerged as a promising fast approach because they can achieve sparse recovery in one pass through early stopping, rather than the tedious grid-search used in the traditional methods. However, most of those iterative methods are based on the $\ell_1$ norm which requires restrictive applicability conditions and could fail in many cases. Therefore, achieving sparse recovery with iterative regularization methods under a wider range of conditions has yet to be further explored. To address this issue, we propose a novel iterative regularization algorithm, IRKSN, based on the $k$-support norm regularizer rather than the $\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and compar
    
[^113]: 使用去噪扩散生成模型的贝叶斯心电图重建

    Bayesian ECG reconstruction using denoising diffusion generative models. (arXiv:2401.05388v1 [eess.SP])

    [http://arxiv.org/abs/2401.05388](http://arxiv.org/abs/2401.05388)

    这项工作提出了一种使用健康心电图数据训练的去噪扩散生成模型，成功生成逼真的心电图信号，并且应用于心脏健康监测和诊断领域，实现了校正QT间期、噪声抑制、心电图导联恢复和异常读数识别等重要临床工具的开发。

    

    在这项工作中，我们提出了一种使用健康心电图数据训练的去噪扩散生成模型（DDGM），该模型关注心电图形态和导联间的相关性。我们的结果表明，这种创新的生成模型可以成功生成逼真的心电图信号。此外，我们还探索了使用DDGM解决线性反问题的最新突破在心电图重建中的应用，这为发展几种重要的临床工具提供了可能。这些工具包括校正QT间期(QTc)的计算、心电图信号的有效噪声抑制、丢失的心电图导联的恢复以及异常读数的识别，从而在心脏健康监测和诊断方面取得了重大进展。

    In this work, we propose a denoising diffusion generative model (DDGM) trained with healthy electrocardiogram (ECG) data that focuses on ECG morphology and inter-lead dependence. Our results show that this innovative generative model can successfully generate realistic ECG signals. Furthermore, we explore the application of recent breakthroughs in solving linear inverse Bayesian problems using DDGM. This approach enables the development of several important clinical tools. These include the calculation of corrected QT intervals (QTc), effective noise suppression of ECG signals, recovery of missing ECG leads, and identification of anomalous readings, enabling significant advances in cardiac health monitoring and diagnosis.
    
[^114]: EMG子空间对齐与可视化用于跨个体手势分类

    EMG subspace alignment and visualization for cross-subject hand gesture classification. (arXiv:2401.05386v1 [eess.SP])

    [http://arxiv.org/abs/2401.05386](http://arxiv.org/abs/2401.05386)

    本文研究了基于肌电图的手势识别系统中的跨个体泛化问题，并通过识别稳健的低维子空间并将其与目标个体对齐，改善了跨个体估计。通过对子空间的可视化，为使用EMG信号改善跨个体泛化提供了有益的见解。

    

    基于肌电图（EMG）的手势识别系统是人机界面的有前途的技术。然而，它们主要的局限性之一是通常需要长时间进行校准以适应新用户。本文通过一个包含了14名人类主体在手势期间的EMG信号的原始数据集，讨论和分析了跨个体泛化的挑战。实验结果表明，虽然基于汇集多个个体的精确泛化几乎是不可能的，但是通过识别多个个体的稳健低维子空间，并将其对齐到目标个体，可以改善跨个体估计。通过子空间的可视化，我们能够提供有关使用EMG信号改善跨个体泛化的见解。

    Electromyograms (EMG)-based hand gesture recognition systems are a promising technology for human/machine interfaces. However, one of their main limitations is the long calibration time that is typically required to handle new users. The paper discusses and analyses the challenge of cross-subject generalization thanks to an original dataset containing the EMG signals of 14 human subjects during hand gestures. The experimental results show that, though an accurate generalization based on pooling multiple subjects is hardly achievable, it is possible to improve the cross-subject estimation by identifying a robust low-dimensional subspace for multiple subjects and aligning it to a target subject. A visualization of the subspace enables us to provide insights for the improvement of cross-subject generalization with EMG signals.
    
[^115]: 面向汽车雷达中的干扰抑制的角度等变卷积神经网络

    Angle-Equivariant Convolutional Neural Networks for Interference Mitigation in Automotive Radar. (arXiv:2401.05385v1 [eess.SP])

    [http://arxiv.org/abs/2401.05385](http://arxiv.org/abs/2401.05385)

    这篇论文介绍了一种在汽车雷达中利用角度等变性的完全卷积神经网络，在干扰抑制方面优于先前的工作。

    

    在汽车应用中，频率调制连续波（FMCW）雷达是一种确定车辆周围物体距离、速度和角度的成熟技术。如果雷达传感器之间发生互相干扰，预测质量可能会严重受损。先前的工作使用神经网络（NNs）并行处理整个接收器阵列的数据，以提高干扰抑制质量。然而，这些架构在不同角度的干扰和物体到达（AoAs）之间不具备很好的泛化能力。在本文中，我们引入了完全卷积神经网络（CNN）与三阶卷积，能够在不同的AoAs之间传递学习到的模式。我们的提议架构在性能上优于先前的工作，同时具有更高的鲁棒性和更少的可训练参数数量。我们在多样的数据集上评估了我们的网络，并展示了其角度等变性。

    In automotive applications, frequency modulated continuous wave (FMCW) radar is an established technology to determine the distance, velocity and angle of objects in the vicinity of the vehicle. The quality of predictions might be seriously impaired if mutual interference between radar sensors occurs. Previous work processes data from the entire receiver array in parallel to increase interference mitigation quality using neural networks (NNs). However, these architectures do not generalize well across different angles of arrival (AoAs) of interferences and objects. In this paper we introduce fully convolutional neural network (CNN) with rank-three convolutions which is able to transfer learned patterns between different AoAs. Our proposed architecture outperforms previous work while having higher robustness and a lower number of trainable parameters. We evaluate our network on a diverse data set and demonstrate its angle equivariance.
    
[^116]: 用于预测半自磨磨机产量的改进遗传编程

    An improved genetic programming for predicting semi autogenous grinding mill throughput. (arXiv:2401.05382v1 [cs.NE])

    [http://arxiv.org/abs/2401.05382](http://arxiv.org/abs/2401.05382)

    该论文介绍了一种改进的遗传编程方法，应用于预测半自磨磨机的产量。新的遗传编程变体可以提取多个方程式，从而精确预测不同训练数据群集的产量。

    

    半自磨磨机在矿物处理厂的磨矿回路中起着至关重要的作用。准确预测半自磨磨机的产量是一个关键性能指标。尽管以前的研究中已经开发出了经验模型来预测半自磨磨机的产量，但是应用机器学习技术来做此预测的潜力仍未得到充分发挥。与依赖昂贵且耗时的实验数据的经验建模不同，机器学习技术可以利用在正常运行期间收集的数据。遗传编程（Genetic Programming，GP）是一种机器学习技术，其优势在于能够提供一个透明的方程式来精确预测磨机的产量。本研究探讨了将GP应用于预测半自磨磨机产量，并引入了五种新的GP变种来提高预测性能。这些变种提取了多个方程式，每个方程式可以准确预测特定训练数据群集的产量。

    Semi-autogenous grinding (SAG) mills play a pivotal role in the grinding circuit of mineral processing plants. Accurate prediction of SAG mill throughput as a crucial performance metric is of utmost importance. While empirical models have been developed in previous studies for SAG mill throughput prediction, the potential of applying machine learning (ML) techniques for this purpose remains underexplored. Unlike empirical modelling, which relies on expensive and time-consuming experimental data, ML techniques can utilize data collected during regular operations. Genetic programming (GP) is one of ML techniques that offers the advantage of providing a transparent equation for precise mill throughput prediction. This study explores the application of GP to predict SAG mill throughput and introduces five new GP variants to enhance prediction performance. These variants extract multiple equations, each accurately predicting mill throughput for specific clusters of training data. These equa
    
[^117]: ADF和TransApp：基于Transformer的智能电表用电序列中的电器检测框架

    ADF & TransApp: A Transformer-Based Framework for Appliance Detection Using Smart Meter Consumption Series. (arXiv:2401.05381v1 [eess.SP])

    [http://arxiv.org/abs/2401.05381](http://arxiv.org/abs/2401.05381)

    本论文提出了ADF框架和TransApp分类器，用于基于智能电表用电序列的电器检测。这些方法能够有效处理大量数据和变化长度的序列，为电力供应商提供帮助客户实现能源转型的宝贵信息。

    

    在过去的十年中，全球电力供应商安装了数百万个智能电表，使他们能够收集大量的用电数据，尽管采样频率较低（每30分钟一个点）。这些供应商面临的一个重要挑战是如何利用这些数据来检测客户家庭中不同电器的存在/不存在。这些宝贵的信息可以帮助他们提供个性化的优惠和建议，以帮助客户实现能源转型。电器检测可以被看作是一个时间序列分类问题。然而，大量的数据结合消费序列的长度长且变化不定，在训练分类器时会带来挑战。在本文中，我们提出了一个框架ADF，它使用客户用电序列的子序列来检测电器的存在/不存在。我们还引入了TransApp，一种基于Transformer的时间序列分类器，该分类器首先在预训练中进行了训练。

    Over the past decade, millions of smart meters have been installed by electricity suppliers worldwide, allowing them to collect a large amount of electricity consumption data, albeit sampled at a low frequency (one point every 30min). One of the important challenges these suppliers face is how to utilize these data to detect the presence/absence of different appliances in the customers' households. This valuable information can help them provide personalized offers and recommendations to help customers towards the energy transition. Appliance detection can be cast as a time series classification problem. However, the large amount of data combined with the long and variable length of the consumption series pose challenges when training a classifier. In this paper, we propose ADF, a framework that uses subsequences of a client consumption series to detect the presence/absence of appliances. We also introduce TransApp, a Transformer-based time series classifier that is first pretrained in
    
[^118]: 基于生物启发优化算法的慢性病预测数据集优化

    Dataset Optimization for Chronic Disease Prediction with Bio-Inspired Feature Selection. (arXiv:2401.05380v1 [cs.NE])

    [http://arxiv.org/abs/2401.05380](http://arxiv.org/abs/2401.05380)

    本研究探索了在慢性病预测中应用基因算法、粒子群优化和鲸鱼优化算法等生物启发优化算法进行特征选择的方法，目标是提高预测准确性、优化数据维度，并使预测结果更易解释和行动化。实验结果表明，生物启发优化算法能够有效地减少准确分类所需的特征数量。

    

    本研究探索了基因算法、粒子群优化和鲸鱼优化算法等生物启发优化算法在慢性病预测中的应用，主要目标是提高模型的预测准确性、优化数据维度，使得预测结果更易解释和行动化。研究涵盖了对糖尿病、癌症、肾脏疾病和心血管疾病等不同慢性病的三种生物启发特征选择方法进行比较分析。使用准确率、精确度、召回率和F1得分等性能指标评估算法在准确分类所需特征数量上的有效性。总体结果表明，生物启发优化算法在减少准确分类所需特征数量方面是有效的，但在不同慢性病上会有一定的差异。

    In this study, we investigated the application of bio-inspired optimization algorithms, including Genetic Algorithm, Particle Swarm Optimization, and Whale Optimization Algorithm, for feature selection in chronic disease prediction. The primary goal was to enhance the predictive accuracy of models streamline data dimensionality, and make predictions more interpretable and actionable.  The research encompassed a comparative analysis of the three bio-inspired feature selection approaches across diverse chronic diseases, including diabetes, cancer, kidney, and cardiovascular diseases. Performance metrics such as accuracy, precision, recall, and f1 score are used to assess the effectiveness of the algorithms in reducing the number of features needed for accurate classification.  The results in general demonstrate that the bio-inspired optimization algorithms are effective in reducing the number of features required for accurate classification. However, there have been variations in the per
    
[^119]: 使用深度学习从单导联心电图中检测QT延长

    Detecting QT prolongation From a Single-lead ECG With Deep Learning. (arXiv:2401.05378v1 [eess.SP])

    [http://arxiv.org/abs/2401.05378](http://arxiv.org/abs/2401.05378)

    本研究开发了一个利用深度学习模型从单导联心电图中推断QT间期，并用于检测Dofetilide药物载荷期间有临床意义的QT延长事件的方法。该方法在大规模的心电图数据集上进行了训练和验证，表现良好。

    

    对于一些抗心律失常药物，药物载荷需要3天住院观察QT延长情况。通过佩戴式心电图监测仪进行自动化QT监测将有助于院外护理。我们开发了一个深度学习模型，从导联I的心电图中推断QT间期 - 这是从步行式心电图监测仪中最常获取的导联，并使用该模型在多费替利得（Dofetilide）药物载荷期间检测有临床意义的QT延长事件。我们使用从马萨诸塞州综合医院的903.6万名患者中获得的422万份12导联心电图记录来开发一个深度学习模型QTNet，该模型能够从导联I推断QT间期。利用来自65.3万名患者的300多万份心电图进行模型训练，使用包含来自13.5万名患者的63.3万份心电图的内部测试集进行测试。QTNet还在另外一家机构的一套包含667万名患者的310万心电图的外部验证集上进行评估。QTNet被用于检测Dofetilide引起的QT延长事件。

    For a number of antiarrhythmics, drug loading requires a 3 day hospitalization with monitoring for QT prolongation. Automated QT monitoring with wearable ECG monitors would facilitate out-of-hospital care. We develop a deep learning model that infers QT intervals from ECG lead-I - the lead most often acquired from ambulatory ECG monitors - and to use this model to detect clinically meaningful QT-prolongation episodes during Dofetilide drug loading. Using 4.22 million 12-lead ECG recordings from 903.6 thousand patients at the Massachusetts General Hospital, we develop a deep learning model, QTNet, that infers QT intervals from lead-I. Over 3 million ECGs from 653 thousand patients are used to train the model and an internal-test set containing 633 thousand ECGs from 135 thousand patients was used for testing. QTNet is further evaluated on an external-validation set containing 3.1 million ECGs from 667 thousand patients at another institution. QTNet was used to detect Dofetilide-induced 
    
[^120]: 动态尖峰图神经网络

    Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])

    [http://arxiv.org/abs/2401.05373](http://arxiv.org/abs/2401.05373)

    本文提出了一个名为"动态尖峰图神经网络"（DSGNN）的框架，它将尖峰神经网络（SNNs）与图神经网络（GNNs）结合起来，以解决动态图表示学习中的复杂性和内存开销问题。DSGNN通过动态调整尖峰神经元的状态和连接权重，在传播过程中保持图结构信息的完整性。

    

    将尖峰神经网络（SNNs）和图神经网络（GNNs）相结合渐渐引起了人们的关注，这是因为它在处理由图表示的非欧几里得数据时具有低功耗和高效率。然而，作为一个常见的问题，动态图表示学习面临着高复杂性和大内存开销的挑战。目前的工作通常通过使用二进制特征而不是连续特征的SNNs来替代循环神经网络（RNNs）进行高效训练，这会忽视图结构信息并在传播过程中导致细节的丢失。此外，优化动态尖峰模型通常需要在时间步之间传播信息，这增加了内存需求。为了解决这些挑战，我们提出了一个名为"动态尖峰图神经网络"（\method{}）的框架。为了减轻信息丢失问题，\method{} 在传播过程中引入了一种新的机制，它在每个时间步骤中动态地调整尖峰神经元的状态和连接权重，以保持图结构信息的完整性。

    The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
    
[^121]: 基于自回归片段扩散的结合位感知配体设计

    Autoregressive fragment-based diffusion for pocket-aware ligand design. (arXiv:2401.05370v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.05370](http://arxiv.org/abs/2401.05370)

    引入了AutoFragDiff，一种基于片段的自回归扩散模型，用于生成3D分子结构。通过预测分子片段的原子类型和空间坐标，改善了生成分子的局部几何结构，并保持高的结合亲和力。如果提供起始分子骨架，模型还可以进行扩展。

    

    在这项工作中，我们引入了AutoFragDiff，一种基于片段的自回归扩散模型，用于根据目标蛋白结构生成3D分子结构。我们使用几何向量感知器来预测分子骨架和蛋白负袋条件下新分子片段的原子类型和空间坐标。我们的方法改善了生成的3D分子的局部几何结构，同时保持高预测的结合亲和力。该模型还可以根据用户提供的起始分子骨架进行扩展。

    In this work, we introduce AutoFragDiff, a fragment-based autoregressive diffusion model for generating 3D molecular structures conditioned on target protein structures. We employ geometric vector perceptrons to predict atom types and spatial coordinates of new molecular fragments conditioned on molecular scaffolds and protein pockets. Our approach improves the local geometry of the resulting 3D molecules while maintaining high predicted binding affinity to protein targets. The model can also perform scaffold extension from user-provided starting molecular scaffold.
    
[^122]: 动态网络模型的符号回归

    Symbolic Regression of Dynamic Network Models. (arXiv:2401.05369v1 [cs.NE])

    [http://arxiv.org/abs/2401.05369](http://arxiv.org/abs/2401.05369)

    符号回归是一种通过复制网络的结构和过程来解释网络结构的方法，不依赖科学家的直觉或专业知识，成功地检索到了合成的增长过程和简单可解释的规则。

    

    对复杂系统（从大脑到社会到城市）使用网络进行建模越来越受到关注，这导致人们对解释这些网络的生成过程的努力增加。机器学习的最新成功促使使用进化计算，尤其是遗传编程来演化计算机程序，以有效地在多维搜索空间中寻找解释网络结构的更好解决方案。符号回归通过复制网络形态和过程来为这些方法做出贡献，同时不依赖科学家的直觉或专业知识。它通过引入一种新颖的网络生成器的公式和无参数适应度函数来评估生成的网络，并被发现能够连续地检索到合成的增长过程，以及一系列经验网络的简单且可解释的规则。我们通过修改生成器来扩展这种方法…

    Growing interest in modelling complex systems from brains to societies to cities using networks has led to increased efforts to describe generative processes that explain those networks. Recent successes in machine learning have prompted the usage of evolutionary computation, especially genetic programming to evolve computer programs that effectively forage a multidimensional search space to iteratively find better solutions that explain network structure. Symbolic regression contributes to these approaches by replicating network morphologies using both structure and processes, all while not relying on the scientists intuition or expertise. It distinguishes itself by introducing a novel formulation of a network generator and a parameter-free fitness function to evaluate the generated network and is found to consistently retrieve synthetically generated growth processes as well as simple, interpretable rules for a range of empirical networks. We extend this approach by modifying generat
    
[^123]: 在日常环境中利用可穿戴和移动技术进行上下文感知的压力监测

    Context-Aware Stress Monitoring using Wearable and Mobile Technologies in Everyday Settings. (arXiv:2401.05367v1 [eess.SP])

    [http://arxiv.org/abs/2401.05367](http://arxiv.org/abs/2401.05367)

    这项研究介绍了一种在日常环境中利用生理和环境数据来客观跟踪压力水平的监测系统，并整合了智能标记方法来优化数据收集。三层物联网系统架构被用来解决挑战。

    

    压力的日常监测是保持身心健康的重要组成部分。最近，生理信号和环境信息已经成为检测高压力情况的有希望的指标。然而，在日常环境中开发一个实时监测系统，利用生理和环境数据来预测压力水平，同时从参与者那里收集压力标签，这是一个重大挑战。我们提出了一个监测系统，在日常生活环境中利用生理和环境数据客观地跟踪每天的压力水平。此外，我们还整合了智能标记方法来优化生态时刻评估（EMA）的收集，该收集对于建立压力检测的机器学习模型是必需的。我们提出了一个三层物联网系统架构来解决这些挑战。我们利用交叉验证技术来准确估计压力水平。

    Daily monitoring of stress is a critical component of maintaining optimal physical and mental health. Physiological signals and contextual information have recently emerged as promising indicators for detecting instances of heightened stress. Nonetheless, developing a real-time monitoring system that utilizes both physiological and contextual data to anticipate stress levels in everyday settings while also gathering stress labels from participants represents a significant challenge. We present a monitoring system that objectively tracks daily stress levels by utilizing both physiological and contextual data in a daily-life environment. Additionally, we have integrated a smart labeling approach to optimize the ecological momentary assessment (EMA) collection, which is required for building machine learning models for stress detection. We propose a three-tier Internet-of-Things-based system architecture to address the challenges. We utilized a cross-validation technique to accurately est
    
[^124]: 基于穿戴设备的在线人体风险预测及预期触觉警示的动作识别

    Online Action Recognition for Human Risk Prediction with Anticipated Haptic Alert via Wearables. (arXiv:2401.05365v1 [eess.SP])

    [http://arxiv.org/abs/2401.05365](http://arxiv.org/abs/2401.05365)

    本文提出了一个基于穿戴设备的在线动作识别框架，融合了在线人体状态估计、动作识别和运动预测，实现了在举重任务中对工人生物力学风险的早期评估和预防。该框架利用NIOSH指数进行在线风险评估，并通过预测未来的运动和触觉警示来预防潜在风险。

    

    本文提出了一个框架，将在线人体状态估计、动作识别和运动预测结合起来，实现在举重任务中提前评估和预防工人生物力学风险。该框架利用NIOSH指数进行在线风险评估，适用于实时应用。具体而言，通过逆运动学/动力学算法从可穿戴传感器数据中检索人体状态。通过实现基于LSTM的引导专家组合架构，实现人体动作识别和运动预测。识别出的动作将一次举重活动分为一系列连续的动作，可应用修订后的NIOSH举重方程进行风险评估。此外，预测的运动可以预测未来的风险。可穿戴系统中嵌入的触觉执行器能够警示受试者潜在的风险，作为一种主动的预防装置。

    This paper proposes a framework that combines online human state estimation, action recognition and motion prediction to enable early assessment and prevention of worker biomechanical risk during lifting tasks. The framework leverages the NIOSH index to perform online risk assessment, thus fitting real-time applications. In particular, the human state is retrieved via inverse kinematics/dynamics algorithms from wearable sensor data. Human action recognition and motion prediction are achieved by implementing an LSTM-based Guided Mixture of Experts architecture, which is trained offline and inferred online. With the recognized actions, a single lifting activity is divided into a series of continuous movements and the Revised NIOSH Lifting Equation can be applied for risk assessment. Moreover, the predicted motions enable anticipation of future risks. A haptic actuator, embedded in the wearable system, can alert the subject of potential risk, acting as an active prevention device. The per
    
[^125]: 通过多级域对齐实现通用的睡眠分期

    Generalizable Sleep Staging via Multi-level Domain Alignment. (arXiv:2401.05363v1 [eess.SP])

    [http://arxiv.org/abs/2401.05363](http://arxiv.org/abs/2401.05363)

    本文提出了一种通用的睡眠分期方法，通过引入域泛化概念，结合多级特征对齐的思想，提高了模型对未见过数据集的泛化能力。

    

    自动睡眠分期对于睡眠评估和疾病诊断至关重要。现有的大多数方法依赖于特定数据集，并且仅适用于相同数据集的未见过的数据集。本文引入了域泛化概念到自动睡眠分期中，并提出了通用的睡眠分期任务，旨在提高模型对未见过的数据集的泛化能力。受到现有的域泛化方法的启发，我们采用特征对齐思想，提出了一种名为SleepDG的框架来解决该问题。考虑到局部显著特征和时序特征对于睡眠分期都很重要，我们提出了一种多级特征对齐，将时代级和序列级特征对齐来学习域不变特征表示。具体而言，我们设计了一种时代级特征对齐方法，对不同睡眠时代的特征分布进行对齐。

    Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different 
    
[^126]: U-SWIM: 通用选择性写入验证技术用于计算内存神经加速器

    U-SWIM: Universal Selective Write-Verify for Computing-in-Memory Neural Accelerators. (arXiv:2401.05357v1 [cs.AR])

    [http://arxiv.org/abs/2401.05357](http://arxiv.org/abs/2401.05357)

    本文介绍了U-SWIM，一种用于计算内存神经加速器的通用选择性写入验证技术。通过只对少量权重进行写入验证处理，可以加快编程速度并保持DNN的准确性。

    

    通过使用新兴的非易失性内存(Non-Volatile Memory, NVM)设备来实现计算内存(Computing-in-Memory, CiM)的架构由于其惊人的能效而成为深度神经网络(DNN)加速的有力竞争者。然而，使用这些新兴设备时会面临一个重大挑战：在权重映射过程中会出现大的变化。如果不加以缓解，这会严重影响DNN的准确性。针对不完美的权重映射，一个被广泛接受的解决方法是迭代式写入验证方法，这涉及验证导纳值并在需要时调整设备。现有的所有发表作品都将此过程应用于每个单独的设备，导致显著的编程时间开销。在我们的研究中，我们证明只有一个小部分权重需要进行这种写入验证处理，可以保持相应设备和DNN的准确性，从而加快编程速度。基于此，我们介绍了一种新颖的方法 USWIM，

    Architectures that incorporate Computing-in-Memory (CiM) using emerging non-volatile memory (NVM) devices have become strong contenders for deep neural network (DNN) acceleration due to their impressive energy efficiency. Yet, a significant challenge arises when using these emerging devices: they can show substantial variations during the weight-mapping process. This can severely impact DNN accuracy if not mitigated. A widely accepted remedy for imperfect weight mapping is the iterative write-verify approach, which involves verifying conductance values and adjusting devices if needed. In all existing publications, this procedure is applied to every individual device, resulting in a significant programming time overhead. In our research, we illustrate that only a small fraction of weights need this write-verify treatment for the corresponding devices and the DNN accuracy can be preserved, yielding a notable programming acceleration. Building on this, we introduce USWIM, a novel method b
    
[^127]: 开发适用于表面缺陷检测的资源受限EdgeAI模型

    Developing a Resource-Constraint EdgeAI model for Surface Defect Detection. (arXiv:2401.05355v1 [cs.CV])

    [http://arxiv.org/abs/2401.05355](http://arxiv.org/abs/2401.05355)

    开发了一种适用于表面缺陷检测的资源受限EdgeAI模型，可以在边缘设备上进行训练，具有出色的性能。

    

    资源限制限制了几个EdgeAI应用程序只能使用机器学习推理方法，其中模型在云端训练并部署到边缘设备。这带来了诸如带宽、延迟和隐私等与存储数据在外部进行模型构建的挑战。在边缘设备上进行训练可以通过消除将数据传输到另一设备进行存储和模型开发的需求来克服这些挑战。在设备上进行训练还可以提供对数据变化的稳健性，因为可以使用新获取的数据对模型进行重新训练以提高性能。因此，我们提出了一种轻量级的EdgeAI架构，改编自Xception，用于在资源受限的边缘环境中进行设备上的训练。我们对我们的模型在PCB缺陷检测任务上进行了评估，并将其性能与现有的轻量级模型-MobileNetV2、EfficientNetV2B0和MobileViT-XXS进行了比较。我们实验的结果表明，我们的模型具有显着的性能，测试准确率高。

    Resource constraints have restricted several EdgeAI applications to machine learning inference approaches, where models are trained on the cloud and deployed to the edge device. This poses challenges such as bandwidth, latency, and privacy associated with storing data off-site for model building. Training on the edge device can overcome these challenges by eliminating the need to transfer data to another device for storage and model development. On-device training also provides robustness to data variations as models can be retrained on newly acquired data to improve performance. We, therefore, propose a lightweight EdgeAI architecture modified from Xception, for on-device training in a resource-constraint edge environment. We evaluate our model on a PCB defect detection task and compare its performance against existing lightweight models - MobileNetV2, EfficientNetV2B0, and MobileViT-XXS. The results of our experiment show that our model has a remarkable performance with a test accura
    
[^128]: ImbaGCD: 不平衡的广义类别发现

    ImbaGCD: Imbalanced Generalized Category Discovery. (arXiv:2401.05353v1 [cs.CV])

    [http://arxiv.org/abs/2401.05353](http://arxiv.org/abs/2401.05353)

    ImbaGCD是一个以最优传输为基础的期望极大化框架，用于解决不平衡的广义类别发现问题，通过对齐边缘类别先验分布来推断已知和未知类别。

    

    广义类别发现（GCD）旨在利用已标记的已知类别集的先验知识，推断出未标记数据集中的已知和未知类别。现有研究隐含地或明确地假设未标记数据中每个类别（已知或未知）的出现频率大致相同。然而，在实际应用中，根据视觉类别的长尾特性，我们更有可能遇到已知/常见类别而不是未知/不常见类别。因此，我们提出了一个具有挑战性且实用的问题，即不平衡的广义类别发现（ImbaGCD），其中未标记数据的分布不平衡，已知类别比未知类别更频繁。为了解决这些问题，我们提出了ImbaGCD，一种基于最优传输的期望极大化框架，通过对齐边缘类别先验分布来实现广义类别发现。ImbaGCD还包括一个系统机制来估计未知类别的对齐后验分布。

    Generalized class discovery (GCD) aims to infer known and unknown categories in an unlabeled dataset leveraging prior knowledge of a labeled set comprising known classes. Existing research implicitly/explicitly assumes that the frequency of occurrence for each category, whether known or unknown, is approximately the same in the unlabeled data. However, in nature, we are more likely to encounter known/common classes than unknown/uncommon ones, according to the long-tailed property of visual classes. Therefore, we present a challenging and practical problem, Imbalanced Generalized Category Discovery (ImbaGCD), where the distribution of unlabeled data is imbalanced, with known classes being more frequent than unknown ones. To address these issues, we propose ImbaGCD, A novel optimal transport-based expectation maximization framework that accomplishes generalized category discovery by aligning the marginal class prior distribution. ImbaGCD also incorporates a systematic mechanism for estim
    
[^129]: 长尾识别中的通用类别发现

    Generalized Categories Discovery for Long-tailed Recognition. (arXiv:2401.05352v1 [cs.CV])

    [http://arxiv.org/abs/2401.05352](http://arxiv.org/abs/2401.05352)

    长尾识别中的通用类别发现方法(GCD)的重大限制是假设未标记数据中的类别分布是均衡的，而事实上自然环境中的视觉类别通常呈现长尾分布。本文提出了一种针对长尾通用类别发现（Long-tailed GCD）的方法，通过两个策略性正则化实现了对较少出现的尾部类别的重要性的增强。

    

    通用类别发现（GCD）在从未标记的数据集中识别已知和未知类别方面起着至关重要的作用，它利用了通过已标记类别集合获取的洞察力。现有的GCD方法的一个显著限制是它们假设未标记数据中的类别分布是均衡的。与这一假设相反，自然环境中的视觉类别通常表现出长尾分布，已知或普遍的类别比罕见的类别更频繁地出现。我们的研究致力于弥合这种差距，着重于长尾通用类别发现（Long-tailed GCD）范式，该范式反映了现实世界未标记数据集的固有不平衡性。针对长尾GCD所带来的独特挑战，我们提出了一种基于两个策略性正则化的强大方法:（i）一种加权机制，增强了较少出现的尾部类别的重要性。

    Generalized Class Discovery (GCD) plays a pivotal role in discerning both known and unknown categories from unlabeled datasets by harnessing the insights derived from a labeled set comprising recognized classes. A significant limitation in prevailing GCD methods is their presumption of an equitably distributed category occurrence in unlabeled data. Contrary to this assumption, visual classes in natural environments typically exhibit a long-tailed distribution, with known or prevalent categories surfacing more frequently than their rarer counterparts. Our research endeavors to bridge this disconnect by focusing on the long-tailed Generalized Category Discovery (Long-tailed GCD) paradigm, which echoes the innate imbalances of real-world unlabeled datasets. In response to the unique challenges posed by Long-tailed GCD, we present a robust methodology anchored in two strategic regularizations: (i) a reweighting mechanism that bolsters the prominence of less-represented, tail-end categories
    
[^130]: 重新思考RNA二级结构问题的性能度量方法

    Rethinking Performance Measures of RNA Secondary Structure Problems. (arXiv:2401.05351v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.05351](http://arxiv.org/abs/2401.05351)

    传统的RNA二级结构预测评估指标存在局限性，我们提出了一种基于图核的度量方法，能够公平准确地评估RNA结构预测算法，并在RNA设计实验中展示了其信息指导性。

    

    准确的RNA二级结构预测对于理解细胞调控和疾病机制至关重要。深度学习（DL）方法通过预测伪结和多种相互作用的碱基对等复杂特征，已经超越了传统算法。然而，传统的距离度量在处理这种三级相互作用时很困难，目前使用的评估指标（F1分数，MCC）存在局限性。我们提出了Weisfeiler-Lehman图核（WL）作为一种替代度量方式。采用像WL这样的基于图的指标能够公平而准确地评估RNA结构预测算法。此外，WL提供了有信息量的指导，如在RNA设计实验中的展示。

    Accurate RNA secondary structure prediction is vital for understanding cellular regulation and disease mechanisms. Deep learning (DL) methods have surpassed traditional algorithms by predicting complex features like pseudoknots and multi-interacting base pairs. However, traditional distance measures can hardly deal with such tertiary interactions and the currently used evaluation measures (F1 score, MCC) have limitations. We propose the Weisfeiler-Lehman graph kernel (WL) as an alternative metric. Embracing graph-based metrics like WL enables fair and accurate evaluation of RNA structure prediction algorithms. Further, WL provides informative guidance, as demonstrated in an RNA design experiment.
    
[^131]: 自适应运算符选择利用广义经验

    Adaptive operator selection utilising generalised experience. (arXiv:2401.05350v1 [cs.NE])

    [http://arxiv.org/abs/2401.05350](http://arxiv.org/abs/2401.05350)

    这篇论文提出了基于强化学习的一种新方法，用于解决组合优化问题中的探索和开发平衡问题，以及利用广义经验来发展通用框架。

    

    优化问题，特别是组合优化问题，由于其复杂性和难度而难以解决。进化和群体智能算法已成功解决这类问题，尤其是在二进制格式中。然而，由于探索和开发活动之间的平衡问题(EvE)，这种近似可能受到影响，这仍然是这个领域的主要挑战。尽管使用多个运算符进行互补使用的自适应运算符选择方案在管理EvE方面越来越受欢迎，但定制的自适应选择系统仍然是一个重要的研究议题。最近，强化学习(RL)被提出作为一种定制和塑造高度有效的自适应选择系统的方法。然而，从可伸缩性的角度来处理该问题仍然具有挑战性。本文提出了一种基于RL的新方法，用于帮助开发通用框架，以获得、处理和利用广义经验。

    Optimisation problems, particularly combinatorial optimisation problems, are difficult to solve due to their complexity and hardness. Such problems have been successfully solved by evolutionary and swarm intelligence algorithms, especially in binary format. However, the approximation may suffer due to the the issues in balance between exploration and exploitation activities (EvE), which remain as the major challenge in this context. Although the complementary usage of multiple operators is becoming more popular for managing EvE with adaptive operator selection schemes, a bespoke adaptive selection system is still an important topic in research. Reinforcement Learning (RL) has recently been proposed as a way to customise and shape up a highly effective adaptive selection system. However, it is still challenging to handle the problem in terms of scalability. This paper proposes and assesses a RL-based novel approach to help develop a generalised framework for gaining, processing, and uti
    
[^132]: 最具区分性的刺激物用于功能细胞类型的识别

    Most discriminative stimuli for functional cell type identification. (arXiv:2401.05342v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.05342](http://arxiv.org/abs/2401.05342)

    本文提出了一种使用最具区分性刺激物的优化聚类方法，成功地识别了小鼠视网膜、恒河猴视网膜和猕猴V4视觉区的功能细胞类型。

    

    识别细胞类型并理解其功能特性对揭示感知和认知机制至关重要。在视网膜中，可以通过精心选择的刺激物来识别功能类型，但这需要专业领域知识，并会对以前已知的细胞类型产生偏见。在视觉皮层中，仍然不知道存在什么功能类型以及如何识别它们。因此，需要新的方法来对视网膜和视觉皮层中的功能细胞类型进行无偏见的识别。在这里，我们提出了一种基于优化的聚类方法，使用最具区分性的刺激物（MDS）来获得神经元的功能聚类。我们的方法通过刺激物的优化和聚类重新分配之间的交替进行，类似于期望最大化算法。该算法成功恢复了小鼠视网膜、恒河猴视网膜和猕猴V4视觉区的功能聚类。这证明了我们的方法可以成功地进行功能细胞类型的识别。

    Identifying cell types and understanding their functional properties is crucial for unraveling the mechanisms underlying perception and cognition. In the retina, functional types can be identified by carefully selected stimuli, but this requires expert domain knowledge and biases the procedure towards previously known cell types. In the visual cortex, it is still unknown what functional types exist and how to identify them. Thus, for unbiased identification of the functional cell types in retina and visual cortex, new approaches are needed. Here we propose an optimization-based clustering approach using deep predictive models to obtain functional clusters of neurons using Most Discriminative Stimuli (MDS). Our approach alternates between stimulus optimization with cluster reassignment akin to an expectation-maximization algorithm. The algorithm recovers functional clusters in mouse retina, marmoset retina and macaque visual area V4. This demonstrates that our approach can successfully 
    
[^133]: 稳定的在线和离线强化学习用于抗体CDRH3设计

    Stable Online and Offline Reinforcement Learning for Antibody CDRH3 Design. (arXiv:2401.05341v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.05341](http://arxiv.org/abs/2401.05341)

    本研究介绍了一种针对抗体设计领域独特挑战的创新强化学习方法，通过在线或离线学习可以设计出针对多个靶标的高亲和力抗体，且在Absolut!数据库测试中表现优于现有方法。

    

    近年来，抗体类治疗领域得到了显著的发展，有针对性的抗体被认为是个性化治疗的一种潜在有效方法。对于复杂、高度个体化的疾病如癌症，这样的治疗可能特别有益。然而，抗体设计的基础氨基酸序列的广泛搜索空间常常限制了该领域的进展。在本研究中，我们引入了一种新颖的强化学习方法，专门解决这一领域的独特挑战。我们证明，我们的方法可以在计算机模拟中学习设计针对多个靶标的高亲和力抗体，无论是通过在线交互还是离线数据集。据我们所知，我们的方法是首个在Absolut!数据库中对所有测试抗原表现优越的方法。

    The field of antibody-based therapeutics has grown significantly in recent years, with targeted antibodies emerging as a potentially effective approach to personalized therapies. Such therapies could be particularly beneficial for complex, highly individual diseases such as cancer. However, progress in this field is often constrained by the extensive search space of amino acid sequences that form the foundation of antibody design. In this study, we introduce a novel reinforcement learning method specifically tailored to address the unique challenges of this domain. We demonstrate that our method can learn the design of high-affinity antibodies against multiple targets in silico, utilizing either online interaction or offline datasets. To the best of our knowledge, our approach is the first of its kind and outperforms existing methods on all tested antigens in the Absolut! database.
    
[^134]: STR-Cert: 基于深度学习流水线和视觉Transformer的深度文本识别的鲁棒性认证

    STR-Cert: Robustness Certification for Deep Text Recognition on Deep Learning Pipelines and Vision Transformers. (arXiv:2401.05338v1 [cs.CV])

    [http://arxiv.org/abs/2401.05338](http://arxiv.org/abs/2401.05338)

    本文提出了STR-Cert，是一种针对鲁棒性认证的深度文本识别方法。通过扩展DeepPoly多面体验证框架，我们针对场景文本识别模型的各个组件提出了新的多面体边界和算法。通过在六个数据集上进行认证和比较，证明了该方法的有效性。

    

    鲁棒性认证旨在正式认证神经网络对抗性输入的预测，已成为安全关键应用的重要工具之一。尽管取得了相当大的进展，现有的认证方法仅限于基本架构（如卷积网络、循环网络和最近的Transformer）和基准数据集（如MNIST）。本文关注于场景文本识别（STR）的鲁棒性认证，这是一种复杂且广泛部署的基于图像的序列预测问题。我们针对三种STR模型架构，包括标准STR流水线和视觉Transformer。通过衍生出新的多面体边界和关键STR模型组件的算法，我们提出了STR-Cert，这是第一种用于STR模型的认证方法。最后，我们在六个数据集上对STR模型进行认证和比较，展示了其效果。

    Robustness certification, which aims to formally certify the predictions of neural networks against adversarial inputs, has become an integral part of important tool for safety-critical applications. Despite considerable progress, existing certification methods are limited to elementary architectures, such as convolutional networks, recurrent networks and recently Transformers, on benchmark datasets such as MNIST. In this paper, we focus on the robustness certification of scene text recognition (STR), which is a complex and extensively deployed image-based sequence prediction problem. We tackle three types of STR model architectures, including the standard STR pipelines and the Vision Transformer. We propose STR-Cert, the first certification method for STR models, by significantly extending the DeepPoly polyhedral verification framework via deriving novel polyhedral bounds and algorithms for key STR model components. Finally, we certify and compare STR models on six datasets, demonstra
    
[^135]: 最优线性信号: 一种用于优化线性信号PnL的无监督机器学习框架

    Optimal Linear Signal: An Unsupervised Machine Learning Framework to Optimize PnL with Linear Signals. (arXiv:2401.05337v1 [q-fin.ST])

    [http://arxiv.org/abs/2401.05337](http://arxiv.org/abs/2401.05337)

    本研究提出了一种无监督机器学习框架，用于优化量化金融中的利润和损失。通过线性组合外生变量构建的信号，最大化夏普比率。实证应用显示了该模型的有效性，并提出了进一步发展的潜在途径。

    

    本研究提出了一种无监督机器学习方法，用于优化量化金融中的利润和损失（PnL）。我们的算法类似于无监督线性回归，通过线性组合外生变量构建的信号，最大化PnL的夏普比率。该方法利用外生变量与交易信号之间的线性关系，通过参数优化来最大化夏普比率。在代表美国国债的ETF上的实证应用显示了模型的有效性，并通过正则化技术来减轻过度拟合。研究最后提出了进一步发展的潜在途径，包括广义时间步长和增强的纠正项。

    This study presents an unsupervised machine learning approach for optimizing Profit and Loss (PnL) in quantitative finance. Our algorithm, akin to an unsupervised variant of linear regression, maximizes the Sharpe Ratio of PnL generated from signals constructed linearly from exogenous variables. The methodology employs a linear relationship between exogenous variables and the trading signal, with the objective of maximizing the Sharpe Ratio through parameter optimization. Empirical application on an ETF representing U.S. Treasury bonds demonstrates the model's effectiveness, supported by regularization techniques to mitigate overfitting. The study concludes with potential avenues for further development, including generalized time steps and enhanced corrective terms.
    
[^136]: 关于广义等增递归分割算法的正确性研究

    On the Correctness of the Generalized Isotonic Recursive Partitioning Algorithm. (arXiv:2401.04847v1 [stat.ML])

    [http://arxiv.org/abs/2401.04847](http://arxiv.org/abs/2401.04847)

    本文通过深入分析广义等增递归分割算法（GIRP），在可分离凸损失和不可微损失的情况下，解决了等增回归问题的存在性和唯一性，并提出了递归二分分割的方法来找到解。

    

    本文深入分析了广义等增递归分割算法（GIRP），该算法用于拟合可分离凸损失下的等增模型，该算法由Luss和Rosset提出 [J. Comput. Graph. Statist., 23 (2014), pp. 192--201] 并由Painsky和Rosset [IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp. 308-321] 扩展适用于不可微损失。GIRP算法具有吸引人的特点，即在算法的每一步中，中间解满足等增约束。文章以一个例子开始，展示了文献中描述的GIRP算法可能无法产生等增模型的情况，表明必须仔细讨论等增回归问题的解的存在性和唯一性。文章接着展示，可能存在许多解之一，可以通过对观察数据集进行递归二分分割来找到解。一个小的修改

    This paper presents an in-depth analysis of the generalized isotonic recursive partitioning (GIRP) algorithm for fitting isotonic models under separable convex losses, proposed by Luss and Rosset [J. Comput. Graph. Statist., 23 (2014), pp. 192--201] for differentiable losses and extended by Painsky and Rosset [IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp. 308-321] for nondifferentiable losses. The GIRP algorithm poseses an attractive feature that in each step of the algorithm, the intermediate solution satisfies the isotonicity constraint. The paper begins with an example showing that the GIRP algorithm as described in the literature may fail to produce an isotonic model, suggesting that the existence and uniqueness of the solution to the isotonic regression problem must be carefully addressed. It proceeds with showing that, among possibly many solutions, there indeed exists a solution that can be found by recursive binary partitioning of the set of observed data. A small mod
    
[^137]: RoSA: 通过鲁棒适应实现准确的参数高效微调

    RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])

    [http://arxiv.org/abs/2401.04679](http://arxiv.org/abs/2401.04679)

    RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。

    

    我们研究了在大语言模型 (LLMs) 的背景下，能够在有限的计算和内存预算下提供良好准确性的参数高效微调 (PEFT) 方法。我们提出了一种新的PEFT方法，称为RoSA，受鲁棒主成分分析 (PCA) 的启发，它在一组固定的预训练权重上共同训练$\textit{低秩}$和$\textit{高度稀疏}$的组件，以高效近似完全微调（FFT）解决方案的性能。我们展示了RoSA在一系列具有挑战性的生成任务上的性能，例如小学数学和SQL查询生成，这些任务需要进行微调以获得良好性能，我们证明了在相同的参数预算下，RoSA优于LoRA和纯粹的稀疏微调。我们通过稀疏GPU内核为RoSA提供系统支持，以补充训练算法，从而实现内存和计算效率的训练。我们的代码将在https://github.com/IST-DASLab上提供。

    We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
    
[^138]: 微小时间混合器 (TTMs): 针对多变量时间序列的增强零/少样本预测的快速预训练模型

    Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.03955](http://arxiv.org/abs/2401.03955)

    本论文介绍了一种名为微小时间混合器 (TTMs) 的预训练模型，该模型针对多变量时间序列的零/少样本预测进行了优化。与大型预训练模型相比，TTMs模型更小、更快，并考虑了跨通道相关性，能够在短时间内进行有效的预测。

    

    零/少样本学习的大型预训练模型在语言和视觉领域表现出色，但在多变量时间序列 (TS) 中面临着多样性和公开预训练数据稀缺的挑战。因此，最近在时间序列预测中使用预训练的大型语言模型 (LLMs) 进行各种适应的趋势逐渐增加。这些方法利用跨领域迁移学习，出奇地取得了令人印象深刻的结果。然而，这些模型通常非常缓慢且庞大（大约十亿个参数），并且不考虑跨通道相关性。为了解决这个问题，我们提出了多层微小时间混合器 (TTM)，这是一种基于轻量级 TSMixer 结构的显著小型模型。TTM 是首个成功开发的微型通用预训练模型（≤100万个参数），专门在公开TS数据集上进行快速训练（仅需4-8小时），具有有效的迁移学习能力进行预测。

    Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
    
[^139]: 长期安全的强化学习与二进制反馈

    Long-term Safe Reinforcement Learning with Binary Feedback. (arXiv:2401.03786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.03786](http://arxiv.org/abs/2401.03786)

    本论文提出了一种长期安全的强化学习算法LoBiSaRL，该算法针对具有二进制安全反馈和未知随机状态转换函数的约束马尔可夫决策过程（CMDPs），通过最大化奖励的方式优化策略，同时以高概率确保每个回合中代理只执行安全的状态-动作对。

    

    安全是将强化学习应用于实际问题的不可或缺的要求。尽管近年来提出了大量的安全强化学习算法，但大多数现有工作通常1）依赖于接收数值安全反馈；2）在学习过程中无法保证安全性；3）将问题限制在先验已知的确定性转换动态；以及/或者4）假设存在一个已知的安全策略以处理任何状态。针对上述问题，我们提出了长期二进制反馈安全强化学习（LoBiSaRL），这是一种针对具有二进制安全反馈和未知随机状态转换函数的约束马尔可夫决策过程（CMDPs）的安全强化学习算法。LoBiSaRL优化一个策略以使奖励最大化，同时保证在每个回合中代理仅以高概率执行安全的状态-动作对，从而确保长期安全性。具体而言，LoBiSaRL通过广义线性模型（GLM）来建模二进制安全函数，并保证长期安全性。

    Safety is an indispensable requirement for applying reinforcement learning (RL) to real problems. Although there has been a surge of safe RL algorithms proposed in recent years, most existing work typically 1) relies on receiving numeric safety feedback; 2) does not guarantee safety during the learning process; 3) limits the problem to a priori known, deterministic transition dynamics; and/or 4) assume the existence of a known safe policy for any states. Addressing the issues mentioned above, we thus propose Long-term Binaryfeedback Safe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision processes (CMDPs) with binary safety feedback and an unknown, stochastic state transition function. LoBiSaRL optimizes a policy to maximize rewards while guaranteeing a long-term safety that an agent executes only safe state-action pairs throughout each episode with high probability. Specifically, LoBiSaRL models the binary safety function via a generalized linear model (GLM) and conser
    
[^140]: 机器学习在创伤性脑损伤中的应用：关注轻度脑损伤

    Machine Learning Applications in Traumatic Brain Injury: A Spotlight on Mild TBI. (arXiv:2401.03621v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2401.03621](http://arxiv.org/abs/2401.03621)

    该论文回顾了在创伤性脑损伤（TBI）中应用机器学习的最新技术，特别关注轻度脑损伤（mTBI）。虽然已经有很多机器学习技术用于诊断，但对预测预后的研究相对较少。

    

    创伤性脑损伤（TBI）是全球公共卫生领域面临的重大挑战，导致高发病率和死亡率，并给全球医疗系统带来巨大经济负担。TBI的诊断依赖于临床信息和计算机断层扫描。为了应对TBI所带来的多方面挑战，出现了创新的数据驱动方法。尤其值得注意的是轻度脑损伤（mTBI）的普遍存在，占多数TBI病例，传统方法往往难以胜任。因此，我们重点回顾了在TBI中应用于临床信息和计算机断层扫描的最新机器学习（ML）技术，特别关注mTBI。我们根据数据来源对ML应用进行分类，并可以看到迄今为止使用了各种ML技术。这些技术大多主要关注诊断，而对预测预后的努力相对较少。这个回顾会对当前的研究进行概述，并探索未来发展的方向。

    Traumatic Brain Injury (TBI) poses a significant global public health challenge, contributing to high morbidity and mortality rates and placing a substantial economic burden on healthcare systems worldwide. The diagnosis of TBI relies on clinical information along with Computed Tomography (CT) scans. Addressing the multifaceted challenges posed by TBI has seen the development of innovative, data-driven approaches, for this complex condition. Particularly noteworthy is the prevalence of mild TBI (mTBI), which constitutes the majority of TBI cases where conventional methods often fall short. As such, we review the state-of-the-art Machine Learning (ML) techniques applied to clinical information and CT scans in TBI, with a particular focus on mTBI. We categorize ML applications based on their data sources, and there is a spectrum of ML techniques used to date. Most of these techniques have primarily focused on diagnosis, with relatively few attempts at predicting the prognosis. This revie
    
[^141]: 行动中的现实主义：使用YOLOv8和DeiT从医学图像中诊断脑肿瘤的异常感知

    Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])

    [http://arxiv.org/abs/2401.03302](http://arxiv.org/abs/2401.03302)

    本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。

    

    在医学科学领域，由于脑肿瘤在患者中的罕见程度，可靠地检测和分类脑肿瘤仍然是一个艰巨的挑战。因此，在异常情况下检测肿瘤的能力对于确保及时干预和改善患者结果至关重要。本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤。来自国家脑映射实验室（NBML）的精选数据集包括81名患者，其中包括30例肿瘤病例和51例正常病例。检测和分类流程被分为两个连续的任务。检测阶段包括全面的数据分析和预处理，以修改图像样本和每个类别的患者数量，以符合真实世界场景中的异常分布（9个正常样本对应1个肿瘤样本）。此外，在测试中除了常见的评估指标外，我们还采用了... [摘要长度已达到上限]

    In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
    
[^142]: 细胞信号传导结构和功能

    The cell signaling structure function. (arXiv:2401.02501v1 [cs.CV])

    [http://arxiv.org/abs/2401.02501](http://arxiv.org/abs/2401.02501)

    该论文提出了一个新的方法，在活体细胞显微镜捕捉到的五维视频中寻找细胞信号动力学时空模式，并且不需要任何先验的预期模式动力学和训练数据。该方法基于细胞信号结构函数（SSF），通过测量细胞信号状态和周围细胞质之间的核糖体强度，与当前最先进的核糖体与细胞核比值相比有了显著改进。通过归一化压缩距离（NCD）来识别相似的模式。该方法能够将输入的SSF构图表示为低维嵌入中的点，最优地捕捉模式。

    

    活体细胞显微镜捕捉到的五维$(x,y,z,channel,time)$视频显示了细胞运动和信号动力学的模式。我们在这里提出一种在五维活体细胞显微镜视频中寻找细胞信号动力学时空模式的方法，该方法独特之处在于不需要预先了解预期的模式动力学以及没有训练数据。所提出的细胞信号结构函数（SSF）是一种Kolmogorov结构函数，可以通过核心区域相对于周围细胞质的核糖体强度来最优地测量细胞信号状态，相比当前最先进的核糖体与细胞核比值有了显著的改进。通过度量归一化压缩距离（NCD）来识别相似的模式。NCD是一个用于表示输入的SSF构图在低维嵌入中作为点的Hilbert空间的再生核，可以最优地捕捉模式。

    Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display patterns of cellular motion and signaling dynamics. We present here an approach to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell microscopy movies unique in requiring no \emph{a priori} knowledge of expected pattern dynamics, and no training data. The proposed cell signaling structure function (SSF) is a Kolmogorov structure function that optimally measures cell signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a significant improvement compared to the current state-of-the-art cytonuclear ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value, or a functional output such as velocity. Patterns of similarity are identified via the metric normalized compression distance (NCD). The NCD is a reproducing kernel for a Hilbert space that represents the input SSF kymographs as points in a low dimensional embedding that optimally captures the pattern
    
[^143]: ReLU$^k$激活的深度神经网络的表达能力和近似性质

    Expressivity and Approximation Properties of Deep Neural Networks with ReLU$^k$ Activation. (arXiv:2312.16483v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16483](http://arxiv.org/abs/2312.16483)

    本文研究了采用ReLU$^k$激活函数的深度神经网络的表达能力和近似性质，通过全面、构造性的证明，证明了深层ReLU$^k$网络可以精确表示高阶多项式，并建立了网络参数的上界。此外，发现深层ReLU$^k$网络可以逼近多种变化空间中的函数，扩展了仅由ReLU$^k$激活函数生成的空间。

    

    本文研究了采用ReLU$^k$激活函数（$k \geq 2$）的深度神经网络的表达能力和近似性质。虽然深层ReLU网络可以有效地逼近多项式，但深层ReLU$^k$网络能够精确表示高阶多项式。我们的初步贡献是通过全面、构造性的证明，证明了深层ReLU$^k$网络可以表示多项式。这使我们能够建立网络参数的大小和数量的上界。因此，我们能够证明Sobolev空间中的函数以及解析函数的次优逼近率。此外，通过研究深层ReLU$^k$网络对浅层网络的表示能力，我们发现深层ReLU$^k$网络能够逼近一定范围内的变化空间中的函数，扩展了仅由ReLU$^k$激活函数生成的空间。这一发现证明了深层ReLU$^k$网络的广泛表示能力。

    In this paper, we investigate the expressivity and approximation properties of deep neural networks employing the ReLU$^k$ activation function for $k \geq 2$. Although deep ReLU networks can approximate polynomials effectively, deep ReLU$^k$ networks have the capability to represent higher-degree polynomials precisely. Our initial contribution is a comprehensive, constructive proof for polynomial representation using deep ReLU$^k$ networks. This allows us to establish an upper bound on both the size and count of network parameters. Consequently, we are able to demonstrate a suboptimal approximation rate for functions from Sobolev spaces as well as for analytic functions. Additionally, through an exploration of the representation power of deep ReLU$^k$ networks for shallow networks, we reveal that deep ReLU$^k$ networks can approximate functions from a range of variation spaces, extending beyond those generated solely by the ReLU$^k$ activation function. This finding demonstrates the ad
    
[^144]: 强大的随机图生成器用于反事实解释

    Robust Stochastic Graph Generator for Counterfactual Explanations. (arXiv:2312.11747v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11747](http://arxiv.org/abs/2312.11747)

    这篇论文介绍了一种新颖的鲁棒随机图生成器（RSGG-CE）用于反事实解释，该方法在生成类似于原始图形的新图形的同时，基于潜在预测模型产生不同的结果。此方法利用生成机制生成反事实实例，具有较高的解释能力，并在其他领域展现出卓越的成果。

    

    反事实解释（CE）技术作为向与AI系统互动的用户提供洞察力的手段已引起关注。虽然在医学成像和自动驾驶等领域进行了广泛研究，但图形反事实解释（GCE）方法相对较少。 GCE生成一个类似于原始图形的新图形，其结果基于潜在的预测模型。在这些GCE技术中，尽管在其他领域如艺术风格和自然语言建模中取得了令人印象深刻的成就，但基于生成机制的解释器仍然受到了相对有限的研究。对生成解释器的偏好源于它们在推断期间生成反事实实例的能力，利用自动获取的输入图形的扰动。在上述理由的推动下，我们的研究介绍了RSGG-CE，一种新颖的鲁棒随机图生成器，用于反事实解释

    Counterfactual Explanation (CE) techniques have garnered attention as a means to provide insights to the users engaging with AI systems. While extensively researched in domains such as medical imaging and autonomous vehicles, Graph Counterfactual Explanation (GCE) methods have been comparatively under-explored. GCEs generate a new graph similar to the original one, with a different outcome grounded on the underlying predictive model. Among these GCE techniques, those rooted in generative mechanisms have received relatively limited investigation despite demonstrating impressive accomplishments in other domains, such as artistic styles and natural language modelling. The preference for generative explainers stems from their capacity to generate counterfactual instances during inference, leveraging autonomously acquired perturbations of the input graph. Motivated by the rationales above, our study introduces RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual Explanation
    
[^145]: 重新参数化低秩提示：在0.5K参数内推广视觉语言模型

    Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters. (arXiv:2312.10813v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.10813](http://arxiv.org/abs/2312.10813)

    该论文提出了一种新型的提示方法，重新参数化低秩提示（RLP），用于在大型预训练视觉语言模型的适应过程中实现高效和有效的知识转移。该方法能够显著减少可调参数和存储开销。

    

    随着大型预训练视觉语言模型的发展，如何有效地将这些基础模型的知识转移到下游任务中成为一个热门话题，尤其是在数据不足的情况下。最近，提示调优已成为一种流行的解决方案。在调整视觉语言模型时，研究人员冻结骨干部分的参数，只设计和调整提示。一方面，提示调优的精心设计展现出强大的性能。另一方面，复杂的结构和更新规则大大增加了计算和存储成本。受到观察到的视觉语言模型中泛化能力的演变模式与适应过程中提示矩阵秩变化趋势的调和一致性的启发，我们设计了一种新型提示，重新参数化低秩提示（RLP），用于高效和有效的适应。我们的方法能大大减少可调参数和存储开销。

    With the development of large pre-trained vision-language models, how to effectively transfer the knowledge of such foundational models to downstream tasks becomes a hot topic, especially in a data-deficient scenario. Recently, prompt tuning has become a popular solution. When adapting the vision-language models, researchers freeze the parameters in the backbone and only design and tune the prompts. On the one hand, the delicate design of prompt tuning exhibits strong performance. On the other hand, complicated structures and update rules largely increase the computation and storage cost. Motivated by the observation that the evolution pattern of the generalization capability in visual-language models aligns harmoniously with the trend of rank variations in the prompt matrix during adaptation, we design a new type of prompt, Re-parameterized Low-rank Prompt (RLP), for both efficient and effective adaptation. Our method could largely reduce the number of tunable parameters and storage s
    
[^146]: ConFormer: 一种新颖的深度学习模型集合，用于协助心脏病学家评估心脏功能

    ConFormer: A Novel Collection of Deep Learning Models to Assist Cardiologists in the Assessment of Cardiac Function. (arXiv:2312.08567v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2312.08567](http://arxiv.org/abs/2312.08567)

    ConFormer是一种新颖的深度学习模型，旨在自动估计超声心动图中的射血分数（EF）和左心室壁厚度，从而实现成本效益高、易获取和全面的心脏健康监测。

    

    心血管疾病，特别是心力衰竭，是全球死亡的主要原因。常规超声心动图筛查早期心力衰竭的检测常常受到昂贵和劳动密集型的限制，这个障碍可能意味着生与死的差别。本文介绍了ConFormer，一种新颖的深度学习模型，旨在自动估计超声心动图中的射血分数（EF）和左心室壁厚度。ConFormer的实施有可能通过实现成本效益高、易获取和全面的心脏健康监测，从而拯救无数生命。源代码可在https://github.com/Aether111/ConFormer找到。

    Cardiovascular diseases, particularly heart failure, are a leading cause of death globally. The early detection of heart failure through routine echocardiogram screenings is often impeded by the high cost and labor-intensive nature of these procedures, a barrier that can mean the difference between life and death. This paper presents ConFormer, a novel deep learning model designed to automate the estimation of Ejection Fraction (EF) and Left Ventricular Wall Thickness from echocardiograms. The implementation of ConFormer has the potential to enhance preventative cardiology by enabling cost-effective, accessible, and comprehensive heart health monitoring, thereby saving countless lives. The source code is available at https://github.com/Aether111/ConFormer.
    
[^147]: 超越梯度和先验知识在隐私攻击中：利用联邦学习中语言模型的池化层输入

    Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05720](http://arxiv.org/abs/2312.05720)

    本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。

    

    联邦学习强调分散式训练，通过本地存储数据并仅发送模型更新，强调用户隐私。最近，一系列有关隐私攻击的工作通过从联邦学习上下文的语言模型中提取敏感的训练文本来损害用户隐私。然而，这些攻击技术面临着不同的障碍：一些工作主要使用有限的批处理大小（例如，批处理大小为1），而其他技术则容易被检测出来。本文介绍了一种创新的方法，具有难以检测的特点，在不同的批处理大小设置下显著提高了文本恢复率。基于基本的梯度匹配和领域先验知识，我们通过恢复语言模型的池化层输入来增强攻击能力，这使我们能够在特征级别提供额外的监督信号。与梯度数据不同，这些信号不会在句子和标记之间进行平均，从而提供更细致和有效的见解。

    Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
    
[^148]: 通过共享注意力实现风格对齐的图像生成

    Style Aligned Image Generation via Shared Attention. (arXiv:2312.02133v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.02133](http://arxiv.org/abs/2312.02133)

    本文提出了一种名为StyleAligned的新技术，通过在T2I模型中使用“注意力共享”，实现了在一系列生成的图像之间建立风格对齐。该方法能够通过简单的反转操作，使用参考风格创建具有一致风格的图像，并在各种输入中实现了高质量的合成和保真度。

    

    大规模的文本到图像（T2I）模型在创意领域迅速崭露头角，从文本提示生成视觉上引人注目的输出物。然而，控制这些模型以确保一致的风格仍然具有挑战性，现有方法需要进行精细调整和手动干预以解开内容和风格的复杂关系。在本文中，我们引入了一种名为StyleAligned的新技术，旨在在一系列生成的图像之间建立风格对齐。通过在扩散过程中使用最小的“注意力共享”，我们的方法在T2I模型中保持图像之间的风格一致性。这种方法通过简单的反转操作，可使用参考风格来创建具有一致风格的图像。我们在不同风格和文本提示上对该方法进行了评估，结果显示出了高质量的合成和保真度，凸显了其在实现各种输入的一致风格方面的效果。

    Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images. By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.
    
[^149]: 朝着无冗余子网络的不断学习之路

    Towards Redundancy-Free Sub-networks in Continual Learning. (arXiv:2312.00840v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.00840](http://arxiv.org/abs/2312.00840)

    本研究提出了一种名为信息瓶颈遮蔽子网络 (IBM) 的方法，通过消除子网络内部的冗余来解决不断学习中的灾难性遗忘问题。该方法通过累积有价值的信息到重要的权重中，构建了无冗余的子网络，有效地减轻了灾难性遗忘，并促进了新任务的训练。

    

    在不断学习中，灾难性遗忘是一个突出的问题。参数隔离通过遮蔽每个任务的子网络来解决这个挑战，以减轻对旧任务的干扰。然而，这些子网络是基于权重大小构建的，而权重大小并不一定对应权重的重要性，导致保留不重要的权重并构建冗余的子网络。为了克服这个限制，我们受到信息瓶颈的启发，该方法消除了相邻网络层之间的冗余。我们提出了信息瓶颈遮蔽子网络 (IBM) 来消除子网络内的冗余。具体地，IBM将有价值的信息累积到重要的权重中，构建无冗余的子网络，不仅通过冻结子网络有效地减轻了灾难性遗忘，还通过有价值的知识传递促进了新任务的训练。此外，IBM分解了隐藏的r

    Catastrophic Forgetting (CF) is a prominent issue in continual learning. Parameter isolation addresses this challenge by masking a sub-network for each task to mitigate interference with old tasks. However, these sub-networks are constructed relying on weight magnitude, which does not necessarily correspond to the importance of weights, resulting in maintaining unimportant weights and constructing redundant sub-networks. To overcome this limitation, inspired by information bottleneck, which removes redundancy between adjacent network layers, we propose \textbf{\underline{I}nformation \underline{B}ottleneck \underline{M}asked sub-network (IBM)} to eliminate redundancy within sub-networks. Specifically, IBM accumulates valuable information into essential weights to construct redundancy-free sub-networks, not only effectively mitigating CF by freezing the sub-networks but also facilitating new tasks training through the transfer of valuable knowledge. Additionally, IBM decomposes hidden r
    
[^150]: 跨越生成性人工智能数据生命周期的隐私和版权挑战导航

    Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI. (arXiv:2311.18252v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2311.18252](http://arxiv.org/abs/2311.18252)

    这项研究探讨了生成性人工智能中数据隐私和版权保护的多方面挑战，并提出了将技术创新与伦理前瞻相结合的综合方法，旨在全面解决这些问题。

    

    生成性人工智能的出现标志着人工智能领域的重要里程碑，展示出在生成真实图像、文本和数据模式方面的卓越能力。然而，这些进展也带来了对数据隐私和版权侵犯的更高关注，主要是由于模型训练对大规模数据集的依赖。传统方法如差分隐私、机器遗忘和数据中毒只提供了对这些复杂问题的片面解决方案。本文深入探讨了数据生命周期内隐私和版权保护的多方面挑战。我们主张采用将技术创新与伦理前瞻相结合的综合方法，通过研究和制定在生命周期视角下的解决方案，全面解决这些问题。本研究旨在推动更广泛的讨论，并激励对生成性人工智能中数据隐私和版权完整性的协同努力。

    The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.
    
[^151]: 新的在线社区：在匿名投票网络上进行图深度学习以识别多中心治理中的虚假身份

    New Online Communities: Graph Deep Learning on Anonymous Voting Networks to Identify Sybils in Polycentric Governance. (arXiv:2311.17929v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.17929](http://arxiv.org/abs/2311.17929)

    本研究提供了一个理论框架，通过使用图深度学习技术，可以有效识别去中心化自治组织（DAO）中的虚假身份，为分散治理提供了新的视角。

    

    本研究探讨了基于区块链的去中心化自治组织（DAO）中数字资产的多中心治理。它提供了一个理论框架，并通过开发一种方法来识别虚假身份（sybils）来解决分散治理面临的一个关键挑战。该方法使用图深度学习技术在DAO治理数据集（snapshot.org）中识别虚假身份的活动。具体地，一个图卷积神经网络（GCNN）学习了投票行为，并使用高维嵌入来识别图中相似节点的快速k均值向量聚类算法（FAISS）。结果显示，深度学习能够有效识别虚假身份，在投票图中减少2-5%。本研究强调了在DAO中的sybil抗性的重要性，并为分散治理提供了新的视角，对未来的政策、监管和治理实践具有启示作用。

    This research examines the polycentric governance of digital assets in blockchain-based Decentralized Autonomous Organizations (DAOs). It offers a theoretical framework and addresses a critical challenge facing decentralized governance by developing a method to identify sybils, or spurious identities. The method uses graph deep learning techniques to identify sybil activity in a DAO governance dataset (snapshot.org). Specifically, a Graph Convolutional Neural Network (GCNN) learned voting behaviours and a fast k-means vector clustering algorithm (FAISS) used the high dimensional embeddings to identify similar nodes in a graph. The results reveal that deep learning can effectively identify sybils, reducing the voting graph by 2-5%. This research underscores the importance of sybil resistance in DAOs and offers a novel perspective on decentralized governance, informing future policy, regulation, and governance practices.
    
[^152]: 通过使用随机尺度来估计深度神经网络的不确定性的规模丢弃

    Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using Stochastic Scale. (arXiv:2311.15816v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.15816](http://arxiv.org/abs/2311.15816)

    本文提出了一种用于二值神经网络（BNNs）的规模丢弃的新型正则化技术，并基于蒙特卡洛建立BayNN模型以高效地估计不确定性。

    

    神经网络（NN）中的不确定性估计对于改善在安全关键应用中的可靠性和预测的信心至关重要。具有Dropout作为近似的贝叶斯神经网络（BayNNs）为量化不确定性提供了一种系统方法，但它们在功耗、内存和计算方面具有高硬件开销。因此，将BayNNs应用于资源有限的边缘设备或高性能应用是具有挑战性的。通过在具有自旋电子存储器和参数二值化的计算内存（CIM）架构上加速它们可以减少BayNNs的一些固有成本。然而，实施常规基于dropout的BayNN需要大量的随机单元。在本文中，我们提出了一种新颖的二值神经网络（BNNs）的规模丢弃正则化技术，并基于蒙特卡洛-规模丢弃（MC-Scale Dropout）的BayNNs进行高效的不确定性估计。

    Uncertainty estimation in Neural Networks (NNs) is vital in improving reliability and confidence in predictions, particularly in safety-critical applications. Bayesian Neural Networks (BayNNs) with Dropout as an approximation offer a systematic approach to quantifying uncertainty, but they inherently suffer from high hardware overhead in terms of power, memory, and computation. Thus, the applicability of BayNNs to edge devices with limited resources or to high-performance applications is challenging. Some of the inherent costs of BayNNs can be reduced by accelerating them in hardware on a Computation-In-Memory (CIM) architecture with spintronic memories and binarizing their parameters. However, numerous stochastic units are required to implement conventional dropout-based BayNN. In this paper, we propose the Scale Dropout, a novel regularization technique for Binary Neural Networks (BNNs), and Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient uncertainty estimatio
    
[^153]: 《基于课程学习和模仿学习的金融时间序列模型无关控制》

    Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.13326](http://arxiv.org/abs/2311.13326)

    本文通过在复杂时间序列数据上探索课程学习和模仿学习的方法，发现课程学习是改善复杂时间序列控制任务性能的新途径，而模仿学习也应该被应用。

    

    课程学习和模仿学习在机器人领域已被广泛运用。然而，在高度随机的时间序列数据上利用这些想法进行控制任务的研究非常有限。在本研究中，我们从理论和实证两个方面探讨了这些方法在复杂时间序列数据上的代表性控制任务中的应用。我们通过数据增强实现了课程学习的基本思想，而通过模仿学习从专家中蒸馏出策略来实现。我们的研究结果表明，课程学习在改进复杂时间序列控制的任务性能方面应被视为一种新的方向。我们的大量随机种子外样本实证和消融研究对于时间序列控制的课程学习非常鼓舞人心。这些发现尤其鼓舞人心，因为我们在基线上调整了所有重叠的超参数，给出了基线的优势。另一方面，我们发现模仿学习应该被使用。

    Curriculum learning and imitation learning have been leveraged extensively in the robotics domain. However, minimal research has been done on leveraging these ideas on control tasks over highly stochastic time-series data. Here, we theoretically and empirically explore these approaches in a representative control task over complex time-series data. We implement the fundamental ideas of curriculum learning via data augmentation, while imitation learning is implemented via policy distillation from an oracle. Our findings reveal that curriculum learning should be considered a novel direction in improving control-task performance over complex time-series. Our ample random-seed out-sample empirics and ablation studies are highly encouraging for curriculum learning for time-series control. These findings are especially encouraging as we tune all overlapping hyperparameters on the baseline -- giving an advantage to the baseline. On the other hand, we find that imitation learning should be use
    
[^154]: 通过整合机器学习和网络系统生物学方法开发一种新型的、个性化的全面痴呆风险预测模型

    Developing a Novel Holistic, Personalized Dementia Risk Prediction Model via Integration of Machine Learning and Network Systems Biology Approaches. (arXiv:2311.09229v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2311.09229](http://arxiv.org/abs/2311.09229)

    开发了一种新型综合个性化的痴呆风险预测模型，通过整合机器学习和网络系统生物学方法，成功建模了变量之间的相互作用。

    

    随着全球预期寿命的提高和人口老龄化，痴呆的患病率逐渐增加。个体发展痴呆的风险受到各种遗传、生活方式和环境因素等的影响。预测痴呆风险可以使个体采取缓解策略或改变生活方式来延缓痴呆发作。当前对痴呆预测的计算方法只返回狭窄类别的变量风险，并没有考虑不同风险变量之间的相互作用。所提出的框架采用一种新颖的全面痴呆风险预测方法，并首次将各种表格环境污染和生活方式因素数据与基于网络系统生物学的遗传数据相结合。采用LightGBM梯度提升方法确保所包含因素的有效性。该方法成功地通过一种被称为Sysable的原始加权整合方法建模了变量之间的相互作用。

    The prevalence of dementia has increased over time as global life expectancy improves and populations age. An individual's risk of developing dementia is influenced by various genetic, lifestyle, and environmental factors, among others. Predicting dementia risk may enable individuals to employ mitigation strategies or lifestyle changes to delay dementia onset. Current computational approaches to dementia prediction only return risk upon narrow categories of variables and do not account for interactions between different risk variables. The proposed framework utilizes a novel holistic approach to dementia risk prediction and is the first to incorporate various sources of tabular environmental pollution and lifestyle factor data with network systems biology-based genetic data. LightGBM gradient boosting was employed to ensure validity of included factors. This approach successfully models interactions between variables through an original weighted integration method coined Sysable. Multi
    
[^155]: CausalCite：一种论文引用的因果公式化

    CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.02790](http://arxiv.org/abs/2311.02790)

    CausalCite是一种以因果推断为基础的论文引用公式化方法，通过对文本进行嵌入和相似样本的提取来评估论文的重要性，并在各个标准上展示了其有效性。

    

    对于科学界来说，评估一篇论文的重要性至关重要但也具有挑战性。尽管引用次数是最常用的评估指标，但它们被广泛批评为无法准确反映一篇论文的真正影响力。在这项工作中，我们提出了一种因果推断方法，称为TextMatch，它将传统的匹配框架适应于高维文本嵌入。具体而言，我们使用大型语言模型（LLM）对每篇论文进行文本嵌入，通过余弦相似性提取相似样本，并根据相似度值的加权平均合成一个反事实样本。我们将得到的指标称为CausalCite，作为论文引用的因果公式化。我们展示了它在各种标准上的有效性，如与科学专家对1K篇论文的报告的论文影响力的高相关性，过去论文的（经过时间考验的）奖项，以及在各个子领域的稳定性。

    Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models (LLMs), extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various sub-fields o
    
[^156]: Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures（现代神经网络架构的克罗内克近似曲率）

    Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures. (arXiv:2311.00636v1 [cs.LG])

    [http://arxiv.org/abs/2311.00636](http://arxiv.org/abs/2311.00636)

    本篇论文提出了一种用于现代神经网络架构的克罗内克近似曲率算法，可以加速神经网络训练和减少计算成本。作者发现了两种具有线性权重共享层不同设置，并证明了相应设置下的K-FAC算法的精确性。K-FAC-reduce通常比K-FAC-expand更快，可以用于加速自动超参数选择。

    

    许多现代神经网络架构的核心组件，如transformers、卷积或图神经网络，可以表达为具有“权重共享”的线性层。克罗内克近似曲率（K-FAC）是一种二阶优化方法，已显示出加速神经网络训练并减少计算成本的潜力。然而，目前还没有将其应用于通用的架构的框架，特别是具有线性权重共享层的架构。在这项工作中，我们确定了具有线性权重共享层的两种不同设置，这促使了两种K-FAC的变体——“扩展”和“减少”。我们展示了对于具有相应设置的深度线性网络，它们是精确的。值得注意的是，K-FAC-reduce通常比K-FAC-expand更快，我们利用它来加速通过优化Wide ResNet的边际似然来选择自动超参数。最后，我们观察到在

    The core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with $\textit{weight-sharing}$. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet. Finally, we observe little difference between 
    
[^157]: Laplacian规范化：一种对称和基底不变谱嵌入的极简方法

    Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding. (arXiv:2310.18716v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.18716](http://arxiv.org/abs/2310.18716)

    Laplacian规范化是一种解决谱嵌入中符号和基底不变性问题的轻量级预处理方法，可以提高在图数据上的效果。

    

    谱嵌入是一种强大的图嵌入技术，由于其对图转换器的有效性，最近受到了很多关注。然而，从理论角度来看，谱嵌入的普遍表达能力造成了图的两个重要不变性属性（符号和基底不变性）的丧失，这也限制了其在图数据上的有效性。为了解决这个问题，之前的许多方法开发了昂贵的学习新不变性的方法，并且计算复杂度很高。在这项工作中，我们探索了一种最小化的方法，通过直接找到特征向量的规范方向来解决模糊性问题，命名为Laplacian规范化（LC）。作为一种纯预处理方法，LC具有轻量级和可应用于任何现有GNN的特点。我们对这种方法进行了从理论到算法的彻底研究，并发现了一种高效的算法，名为最大轴投影（MAP），适用于符号和基底两种不变性。

    Spectral embedding is a powerful graph embedding technique that has received a lot of attention recently due to its effectiveness on Graph Transformers. However, from a theoretical perspective, the universal expressive power of spectral embedding comes at the price of losing two important invariance properties of graphs, sign and basis invariance, which also limits its effectiveness on graph data. To remedy this issue, many previous methods developed costly approaches to learn new invariants and suffer from high computation complexity. In this work, we explore a minimal approach that resolves the ambiguity issues by directly finding canonical directions for the eigenvectors, named Laplacian Canonization (LC). As a pure pre-processing method, LC is light-weighted and can be applied to any existing GNNs. We provide a thorough investigation, from theory to algorithm, on this approach, and discover an efficient algorithm named Maximal Axis Projection (MAP) that works for both sign and basi
    
[^158]: 对于社区检测中模块化最大化在近似、启发式和图神经网络算法中的分析

    Analyzing Modularity Maximization in Approximation, Heuristic, and Graph Neural Network Algorithms for Community Detection. (arXiv:2310.10898v1 [cs.SI])

    [http://arxiv.org/abs/2310.10898](http://arxiv.org/abs/2310.10898)

    这项研究分析了不同的模块化最大化算法对于社区检测中实现最优划分的性能，并发现最常用的模块化方法与最优划分之间存在显著差异。

    

    社区检测是计算科学中的一个基本问题，在各个领域都有应用。通常通过最大化一个目标函数（模块性）在网络节点的划分上来检测社区。我们的研究探讨了不同模块化最大化算法在实现最优划分方面的性能。我们使用了104个网络，包括来自不同背景的真实世界实例和具有模块化结构的合成图。我们分析了十种近似模块化算法，对比了一种精确的基准线方法，该方法是一种精确的整数规划方法，可以全局优化模块性。分析的十种算法包括八个启发式算法，两种图神经网络算法的变种，以及几种Bayan近似算法的变种。我们的分析揭示了最常用的模块化方法得到的划分与网络的最优划分之间的显著差异，如下所示。

    Community detection, a fundamental problem in computational sciences, finds applications in various domains. Heuristics are often employed to detect communities through maximizing an objective function, modularity, over partitions of network nodes. Our research delves into the performance of different modularity maximization algorithms in achieving optimal partitions. We use 104 networks, comprising real-world instances from diverse contexts and synthetic graphs with modular structures. We analyze ten inexact modularity-based algorithms against an exact baseline which is an exact integer programming method that globally optimizes modularity. The ten algorithms analyzed include eight heuristics, two variations of a graph neural network algorithm, and several variations of the Bayan approximation algorithm. Our analysis uncovers substantial dissimilarities between the partitions obtained by most commonly used modularity-based methods and any optimal partition of the networks, as indicate
    
[^159]: CodeFuse-13B: 一个预训练的多语言代码大型语言模型

    CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model. (arXiv:2310.06266v1 [cs.SE])

    [http://arxiv.org/abs/2310.06266](http://arxiv.org/abs/2310.06266)

    CodeFuse-13B是一个预训练的多语言代码大型语言模型，专为代码相关任务设计，支持超过40种编程语言，并通过使用高质量的预训练数据集以及大量实验的验证，展现了其在多语言输入下的有效性。

    

    代码大型语言模型(Code LLMs)因其在软件工程全生命周期中的广泛应用而受到工业界的广泛关注。然而，现有模型在理解非英语输入的多语言代码相关任务方面的效果仍然远未被充分研究。本文介绍了CodeFuse-13B，一个开源的预训练代码LLM。它专为包含英文和中文提示的代码相关任务而设计，并支持超过40种编程语言。CodeFuse通过利用由程序分析器精心筛选并在训练过程中优化的高质量预训练数据集来实现其效果。我们进行了大量实验，包括使用真实世界的使用场景、工业标准基准HumanEval-x，以及专为中文提示设计的CodeFuseEval。为了评估CodeFuse的有效性，我们积极收集了AntGroup软件开发团队的宝贵人工反馈。

    Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software develop
    
[^160]: 预警和隐变化随机动力学系统及元标签纠正的早期预警

    Early Warning via tipping-preserving latent stochastic dynamical system and meta label correcting. (arXiv:2310.06059v1 [cs.LG])

    [http://arxiv.org/abs/2310.06059](http://arxiv.org/abs/2310.06059)

    提出了一种通过融合真实数据和隐性随机微分方程产生的增广数据的元学习框架，用于提高对早期癫痫发作信号的预测准确率，并通过提取的临界动力学特征来标记噪声数据。

    

    对癫痫患者进行早期预警对于他们的安全和福祉至关重要，可以预防或减少癫痫发作的严重程度。通过患者的脑电图数据，我们提出了一个元学习框架，以改进对早期癫痫发作信号的预测。为了更好地利用元标签纠正方法，我们融合了真实数据和隐性随机微分方程（SDE）产生的增广数据的信息。此外，我们还通过真实数据和隐性随机微分方程的过渡时间分布来优选选择潜在的动力系统。通过这种方式，提取的临界动力学特征也被集成到元网络中，以更好地标记噪声数据。为了验证我们的方法，我们将LSTM实施为基准模型。我们进行了一系列实验来预测各种长期窗口（1-2秒的输入数据）内的癫痫发作，并发现预测准确率出现了令人惊讶的增加。

    Early warning for epilepsy patients is crucial for their safety and well-being, in terms of preventing or minimizing the severity of seizures. Through the patients' EEG data, we propose a meta learning framework for improving prediction on early ictal signals. To better utilize the meta label corrector method, we fuse the information from both the real data and the augmented data from the latent Stochastic differential equation(SDE). Besides, we also optimally select the latent dynamical system via distribution of transition time between real data and that from the latent SDE. In this way, the extracted tipping dynamical feature is also integrated into the meta network to better label the noisy data. To validate our method, LSTM is implemented as the baseline model. We conduct a series of experiments to predict seizure in various long-term window from 1-2 seconds input data and find surprisingly increment of prediction accuracy.
    
[^161]: 基于似然的物联网系统中专家支持的分布式学习算法中传感器校准的研究

    Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])

    [http://arxiv.org/abs/2309.11526](http://arxiv.org/abs/2309.11526)

    该论文介绍了一种基于似然的传感器校准方法，可以在物联网系统中实现专家支持的分布式学习算法。通过对模拟和实际测量数据的评估，证明了该方法的有效性和改进效果。

    

    传感器技术领域中的一个重要任务是将一个传感器的测量结果高效地适应到另一个具有相同设计的传感器。一种想法是使用不同系统之间的仿射变换估计，这可以通过专家的知识进行改进。本文介绍了Glacier Research在1973年发表的改进解决方案，并展示了该解决方案可以用于传感器的软件校准、基于专家的适应和联邦学习方法。我们通过模拟和实际测量数据对我们的研究进行了评估，实验中使用了一个具有8个相同传感器的多传感器板。结果表明，无论是模拟还是实验数据，都得到了改进。

    An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
    
[^162]: EarthPT：地球观测的基础模型

    EarthPT: a foundation model for Earth Observation. (arXiv:2309.07207v1 [cs.LG])

    [http://arxiv.org/abs/2309.07207](http://arxiv.org/abs/2309.07207)

    EarthPT是一个地球观测的预训练transformer模型，能够准确预测未来地表反射值，并提供有意义的嵌入信息用于下游任务。

    

    我们介绍了EarthPT - 一种地球观测(EO)预训练的transformer模型。EarthPT是一个7亿参数的解码transformer基础模型，以自监督的方式进行训练，并专门针对EO应用进行开发。我们证明EarthPT是一个有效的预测模型，可以准确地预测未来400-2300 nm范围内的像素级地表反射值。例如，在一个为期五个月的测试数据集上，地表植被指数（NDVI）的演变预测的典型误差约为0.05（在-1 -> 1的自然范围内），性能超过基于历史平均的简单相位折叠模型。我们还证明了EarthPT学到的嵌入具有语义上有意义的信息，并且可以用于下游任务，如高精度、动态的土地利用分类。令人兴奋的是，我们注意到EO数据的丰富性提供了理论上的...

    We introduce EarthPT -- an Earth Observation (EO) pretrained transformer. EarthPT is a 700 million parameter decoding transformer foundation model trained in an autoregressive self-supervised manner and developed specifically with EO use-cases in mind. We demonstrate that EarthPT is an effective forecaster that can accurately predict future pixel-level surface reflectances across the 400-2300 nm range well into the future. For example, forecasts of the evolution of the Normalised Difference Vegetation Index (NDVI) have a typical error of approximately 0.05 (over a natural range of -1 -> 1) at the pixel level over a five month test set horizon, out-performing simple phase-folded models based on historical averaging. We also demonstrate that embeddings learnt by EarthPT hold semantically meaningful information and could be exploited for downstream tasks such as highly granular, dynamic land use classification. Excitingly, we note that the abundance of EO data provides us with -- in theor
    
[^163]: 深入研究睡眠：基于单通道脑电图的睡眠阶段分类与模型解释性

    A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v1 [eess.SP])

    [http://arxiv.org/abs/2309.07156](http://arxiv.org/abs/2309.07156)

    本文提出了一种基于单通道脑电图的睡眠阶段分类方法，使用SE-Resnet-Bi-LSTM架构进行睡眠阶段的分类，并进行了模型解释性分析。在三个不同的数据集上进行了全面评估，取得了显著的准确率和宏F1得分。

    

    睡眠是一种基本的生理过程，在我们的生活中占据着重要的部分。准确分类睡眠阶段是评估睡眠质量和识别可能的睡眠障碍的关键工具。本研究引入了一种新颖的方法，利用SE-Resnet-Bi-LSTM架构将睡眠分类为五个不同的阶段。分类过程基于对单通道脑电图（EEG）的分析。建议的框架由两个基本元素组成：利用SE-ResNet的特征提取器和利用Bi-LSTM单元堆栈的时间上下文编码器。我们的方法的有效性通过在三个不同的数据集上进行的全面评估得到证实，分别是SLeepEDF-20、SleepEDF-78和SHHS。值得注意的是，我们的方法在相应的数据集上分别达到了显著的准确率，分别为87.5％、83.9％和87.8％，并且在宏F1得分方面分别为82.5、78.9和81.9。

    Sleep, a fundamental physiological process, occupies a significant portion of our lives. Accurate classification of sleep stages serves as a crucial tool for evaluating sleep quality and identifying probable sleep disorders. This work introduces a novel methodology that utilises a SE-Resnet-Bi-LSTM architecture to classify sleep into five separate stages. The classification process is based on the analysis of single-channel electroencephalograms (EEGs). The framework that has been suggested consists of two fundamental elements: a feature extractor that utilises SE-ResNet, and a temporal context encoder that use stacks of Bi-LSTM units.The effectiveness of our approach is substantiated by thorough assessments conducted on three different datasets, namely SLeepEDF-20, SleepEDF-78, and SHHS. Significantly, our methodology attains notable levels of accuracy, specifically 87.5\%, 83.9\%, and 87.8\%, along with macro-F1 scores of 82.5, 78.9, and 81.9 for the corresponding datasets. Notably, 
    
[^164]: 限制距离的民间传说Weisfeiler-Leman图神经网络及可证明的循环计数能力

    Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power. (arXiv:2309.04941v1 [cs.LG])

    [http://arxiv.org/abs/2309.04941](http://arxiv.org/abs/2309.04941)

    本文提出了一种称为$d$-DRFWL(2) GNNs的新型图神经网络，它通过限制节点之间的距离来实现循环计数能力，克服了子图GNNs的预处理和计算成本高的限制。

    

    图神经网络（GNNs）的能力在广泛的任务中成功计数特定的图子结构，尤其是循环，对于GNNs的成功非常关键。最近，它已被用作评估GNNs表达能力的一种常用指标。许多具有可证明的循环计数能力的GNN模型都基于子图GNNs，即从输入图中提取一组子图，为每个子图生成表示，并使用它们来增强输入图的表示。然而，这些方法需要进行繁重的预处理，并且时间和内存成本较高。在本文中，我们通过提出一种新的GNN类别-- $d$-Distance-Restricted FWL(2) GNNs，或者 $d$-DRFWL(2) GNNs，克服了子图GNNs的上述限制。$d$-DRFWL(2) GNNs将互相之间距离不超过$d$的节点对作为信息传递的单位，以平衡表达能力和复杂性。

    The ability of graph neural networks (GNNs) to count certain graph substructures, especially cycles, is important for the success of GNNs on a wide range of tasks. It has been recently used as a popular metric for evaluating the expressive power of GNNs. Many of the proposed GNN models with provable cycle counting power are based on subgraph GNNs, i.e., extracting a bag of subgraphs from the input graph, generating representations for each subgraph, and using them to augment the representation of the input graph. However, those methods require heavy preprocessing, and suffer from high time and memory costs. In this paper, we overcome the aforementioned limitations of subgraph GNNs by proposing a novel class of GNNs -- $d$-Distance-Restricted FWL(2) GNNs, or $d$-DRFWL(2) GNNs. $d$-DRFWL(2) GNNs use node pairs whose mutual distances are at most $d$ as the units for message passing to balance the expressive power and complexity. By performing message passing among distance-restricted node
    
[^165]: 通过偏好学习在多目标问题中进行交互式超参数优化

    Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])

    [http://arxiv.org/abs/2309.03581](http://arxiv.org/abs/2309.03581)

    本文提出了一个以人为中心的交互式超参数优化方法，通过应用偏好学习来解决多目标机器学习中的问题。

    

    超参数优化对于发挥机器学习的潜力至关重要。在实践中，用户通常对多目标问题感兴趣，即优化可能存在冲突的目标，比如准确性和能耗。为了解决这个问题，绝大多数多目标机器学习算法将一组非支配的机器学习模型的帕累托前沿返回给用户。然而，优化这种算法的超参数并不容易，因为评估一个超参数配置涉及评估得到的帕累托前沿的质量。在文献中，已有一些指标可以通过量化不同属性（如体积、与参考点的接近程度）来评估帕累托前沿的质量（例如超体积、R2）。然而，对于用户来说，选择导致期望的帕累托前沿的指标可能是一项困难的任务。在本文中，我们提出了一个以人为中心的交互式超参数优化方法，针对多目标机器学习应用偏好学习。

    Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learnin
    
[^166]: 缺失值处理的三值决策树

    Trinary Decision Trees for missing value handling. (arXiv:2309.03561v1 [stat.ML])

    [http://arxiv.org/abs/2309.03561](http://arxiv.org/abs/2309.03561)

    本文介绍了一种称为三值决策树的算法，用于改善决策树在处理缺失数据时的表现。与其他方法不同，该算法不假设缺失值包含任何关于响应的信息。实验证明，在特定缺失数据场景下，三值决策树在MCAR设置中表现优异，在IM设置中略逊一筹。同时，通过将三值决策树与缺失在属性方法相结合，可以获得更稳健的性能。尽管训练速度较慢，但三值决策树提供了一种有前途且更准确的方法。

    

    本文介绍了三值决策树，这是一种旨在改善决策树回归器和分类器中处理缺失数据的算法。与其他方法不同，三值决策树不假设缺失值包含有关响应的任何信息。本文通过理论计算和使用真实数据集的数值示例，比较了其在不同缺失数据场景（完全随机缺失（MCAR）和信息性缺失（IM））中与已建立算法的性能。值得注意的是，在MCAR设置中，三值树在只有样本外缺失数据时表现优于其同行，而在IM设置中落后。一个混合模型，即三值缺失在属性（MIA）方法和三值树相结合的TrinaryMIA树，在所有缺失类型中表现出强大的性能。尽管训练速度较慢可能是一个潜在的缺点，但三值决策树提供了一个有前途且更准确的方法。

    This paper introduces the Trinary decision tree, an algorithm designed to improve the handling of missing data in decision tree regressors and classifiers. Unlike other approaches, the Trinary decision tree does not assume that missing values contain any information about the response. Both theoretical calculations on estimator bias and numerical illustrations using real data sets are presented to compare its performance with established algorithms in different missing data scenarios (Missing Completely at Random (MCAR), and Informative Missingness (IM)). Notably, the Trinary tree outperforms its peers in MCAR settings, especially when data is only missing out-of-sample, while lacking behind in IM settings. A hybrid model, the TrinaryMIA tree, which combines the Trinary tree and the Missing In Attributes (MIA) approach, shows robust performance in all types of missingness. Despite the potential drawback of slower training speed, the Trinary tree offers a promising and more accurate met
    
[^167]: 使用深度强化学习进行DAG任务的边缘生成调度

    Edge Generation Scheduling for DAG Tasks Using Deep Reinforcement Learning. (arXiv:2308.14647v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14647](http://arxiv.org/abs/2308.14647)

    本文通过使用深度强化学习算法和图表示神经网络，提出了一种边缘生成调度框架(EGS)，用于解决实时DAG任务调度问题。该框架通过迭代生成边缘以最小化DAG宽度，并确保满足截止时间约束。所提出的算法在实验中得到验证，并与其他DAG调度启发式算法和最佳混合整数进行了比较。

    

    目前，在实时领域中采用有向无环图(DAG)任务来模拟汽车、航空航天和工业领域中通过链式互通任务实现功能的复杂应用。本文通过提出一种基于微不足道调度概念的新型可调度性测试，研究了实时DAG任务调度问题。利用这个可调度性测试，我们提出了一个新的DAG调度框架(边缘生成调度--EGS)，它试图通过迭代生成边缘来最小化DAG宽度，并同时保证截止时间约束。我们研究了如何通过开发一个深度强化学习算法和图表示神经网络来高效解决生成边缘的问题，以学习EGS的高效边缘生成策略。通过将其与最先进的DAG调度启发式算法和最佳混合整数进行比较，我们评估了所提算法的有效性。

    Directed acyclic graph (DAG) tasks are currently adopted in the real-time domain to model complex applications from the automotive, avionics, and industrial domains that implement their functionalities through chains of intercommunicating tasks. This paper studies the problem of scheduling real-time DAG tasks by presenting a novel schedulability test based on the concept of trivial schedulability. Using this schedulability test, we propose a new DAG scheduling framework (edge generation scheduling -- EGS) that attempts to minimize the DAG width by iteratively generating edges while guaranteeing the deadline constraint. We study how to efficiently solve the problem of generating edges by developing a deep reinforcement learning algorithm combined with a graph representation neural network to learn an efficient edge generation policy for EGS. We evaluate the effectiveness of the proposed algorithm by comparing it with state-of-the-art DAG scheduling heuristics and an optimal mixed-intege
    
[^168]: ProAgent：利用大型语言模型构建主动合作的人工智能

    ProAgent: Building Proactive Cooperative AI with Large Language Models. (arXiv:2308.11339v1 [cs.AI])

    [http://arxiv.org/abs/2308.11339](http://arxiv.org/abs/2308.11339)

    ProAgent是一个利用大型语言模型构建的主动合作的AI框架，能够预测队友的决策并为自己制定增强计划，具有高度的模块化和可解释性。

    

    在AGI研究中，构建具有自适应行为的人工智能以进行人工智能和人类的合作成为一个关键关注点。目前，开发合作代理人的方法主要依赖于基于学习的方法，其中政策泛化严重依赖于与特定队友的过去互动。这些方法限制了代理人在面对新的队友时重新校准策略的能力。我们提出了ProAgent，这是一个新颖的框架，利用大型语言模型（LLMs）来创建一个具有预测队友未来决策能力和为自身制定增强计划能力的主动代理。ProAgent在合作推理方面表现出色，能够动态调整行为以增强与队友的协作努力。此外，ProAgent框架具有高度的模块化和可解释性，便于无缝集成，以应对各种协调场景。

    Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations
    
[^169]: 生成人工智能的强化学习：现状、机会和开放研究挑战

    Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])

    [http://arxiv.org/abs/2308.00031](http://arxiv.org/abs/2308.00031)

    这篇论文调查了在生成人工智能中应用强化学习的现状、机会和开放研究问题。作者主要讨论了三种应用类型：无特定目标的生成方式、同时最大化目标函数的输出生成方式以及将无法通过目标函数捕捉的期望特征嵌入生成过程的方式。这个新兴领域中存在着丰富的机会和挑战。

    

    生成人工智能（AI）是近十年来计算机科学领域最令人兴奋的发展之一。与此同时，强化学习（RL）在各种机器学习任务中已经成为非常成功的范式。在本调查中，我们讨论了将RL应用于生成AI中的现状、机会和开放的研究问题。具体而言，我们将讨论三种应用类型，即作为一种无特定目标的生成方式，作为一种同时最大化目标函数的输出生成方式，以及作为一种将无法通过目标函数轻松捕捉的期望特征嵌入生成过程的方式。我们在调查结果中对这个迷人的新兴领域中的机会和挑战进行了深入的讨论。

    Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
    
[^170]: 缩放基于机器学习的化工厂模拟：用于调整模型以诱导稳定固定点的方法

    Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points. (arXiv:2307.13621v1 [cs.LG])

    [http://arxiv.org/abs/2307.13621](http://arxiv.org/abs/2307.13621)

    本研究提出了一种缩放基于机器学习的化工厂模拟的方法，并通过微调模型解决了在应用于较大工厂时出现的循环求解不稳定的问题。

    

    理想化的化工厂基于第一原理的模型可能不准确。另一种方法是直接将机器学习（ML）模型拟合到工厂传感器数据上。我们采用一种结构化的方法：将工厂内的每个单元表示为一个ML模型。在将模型拟合到数据后，将模型连接成类似流程图的有向图。我们发现，对于较小的工厂，这种方法效果很好，但对于较大的工厂，由于流程图中存在大型和嵌套循环所导致的复杂动力学会导致循环求解器的不稳定性。我们对这个问题进行了深入分析，并表明这不仅仅是一个特殊关注点，而是一个更普遍的挑战，可能在应用于较大的工厂时会发生。为了解决这个问题，我们提出了一种方法，可以将ML模型进行微调，从而使得常规方法下的循环求解变得稳健。

    Idealized first-principles models of chemical plants can be inaccurate. An alternative is to fit a Machine Learning (ML) model directly to plant sensor data. We use a structured approach: Each unit within the plant gets represented by one ML model. After fitting the models to the data, the models are connected into a flowsheet-like directed graph. We find that for smaller plants, this approach works well, but for larger plants, the complex dynamics arising from large and nested cycles in the flowsheet lead to instabilities in the cycle solver. We analyze this problem in depth and show that it is not merely a specialized concern but rather a more pervasive challenge that will likely occur whenever ML is applied to larger plants. To address this problem, we present a way to fine-tune ML models such that solving cycles with the usual methods becomes robust again.
    
[^171]: 深度迁移学习在工业时间序列异常检测中的综合调查：方法、应用和方向

    A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions. (arXiv:2307.05638v1 [cs.LG])

    [http://arxiv.org/abs/2307.05638](http://arxiv.org/abs/2307.05638)

    本综述调查了深度迁移学习在工业时间序列异常检测中的使用。深度迁移学习通过利用相关任务的知识和考虑数据分布的变化，解决了仅有少量或没有附加标记数据情况下的新任务。

    

    自动化监测工业过程有潜力通过及时检测异常事件并促进及时干预来提高效率和优化质量。深度学习通过识别大数据集中的非平凡模式，在这一过程中发挥着关键作用。标准的深度学习方法适用于解决特定类型的数据给定特定任务的问题。在训练过程中，这些算法需要大量的标记训练数据。然而，由于工艺和环境的动态性，为每个稍有不同的情况重新获得所需数据进行标准深度学习训练是不现实的。深度迁移学习提供了解决这个问题的方法。通过利用相关任务的知识和考虑数据分布的变化，这个学习框架可以解决新任务，即使没有或只有很少的附加标记数据。这种方法避免了从头开始重新训练模型的需要。

    Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, the algorithms demand large volumes of labeled training data. However, due to the dynamic nature of processes and the environment, it is impractical to acquire the needed data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, this learning framework solves new tasks even with little or no additional labeled data. The approach bypasses the need to retrain a model from scratch for ev
    
[^172]: 交叉扩散：通过自监督学习改进基于扩散的视觉机器人策略

    Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning. (arXiv:2307.01849v1 [cs.RO])

    [http://arxiv.org/abs/2307.01849](http://arxiv.org/abs/2307.01849)

    本论文提出了一种名为Crossway Diffusion的方法，通过使用自监督学习目标增强了基于扩散的视觉机器人策略学习，实验证明了其在各种机器人任务中的有效性和优势。

    

    序列建模方法在机器人模仿学习中显示出有希望的结果。最近，扩散模型已经被采用于行为克隆中，并从其在建模复杂数据分布方面的特异能力中获益。在这项工作中，我们提出了一种名为Crossway Diffusion的方法，通过使用额外的自监督学习（SSL）目标来增强基于扩散的视觉机器人策略学习。标准的基于扩散的策略从随机噪声中生成动作序列，条件是视觉观测和其他低维状态。我们进一步扩展了这一方法，引入了一个新的解码器，从反向扩散过程的中间表示中重构原始图像像素（和其他状态信息），并使用SSL损失联合训练模型。我们的实验证明了Crossway Diffusion在各种模拟和真实世界机器人任务中的有效性，验证了其相对于标准基于扩散的策略的优势。

    Sequence modeling approaches have shown promising results in robot imitation learning. Recently, diffusion models have been adopted for behavioral cloning, benefiting from their exceptional capabilities in modeling complex data distribution. In this work, we propose Crossway Diffusion, a method to enhance diffusion-based visuomotor policy learning by using an extra self-supervised learning (SSL) objective. The standard diffusion-based policy generates action sequences from random noise conditioned on visual observations and other low-dimensional states. We further extend this by introducing a new decoder that reconstructs raw image pixels (and other state information) from the intermediate representations of the reverse diffusion process, and train the model jointly using the SSL loss. Our experiments demonstrate the effectiveness of Crossway Diffusion in various simulated and real-world robot tasks, confirming its advantages over the standard diffusion-based policy. We demonstrate tha
    
[^173]: 一种通过镜面下降控制隐式正则化的统一方法

    A Unified Approach to Controlling Implicit Regularization via Mirror Descent. (arXiv:2306.13853v1 [cs.LG])

    [http://arxiv.org/abs/2306.13853](http://arxiv.org/abs/2306.13853)

    本文提出了一种使用镜面下降方法来统一控制回归和分类问题中的隐式正则化的方法，在所有标准几何下都可以实现$\ell_p$（$p\in[1,\infty]$）范式的隐式正则化，并且可以实现学习理论中许多特殊类的控制的泛化保证。

    

    受深度神经网络的显著成功启发，人们对超参数模型的泛化性能产生了极大的兴趣。人们花费了大量精力来确定优化算法通过其“首选”解如何影响泛化，这种现象通常被称为隐式正则化。特别地，已经有人论证梯度下降（GD）在回归和分类问题中会引起隐式的$\ell_2$ -范数正则化。然而，不同算法的隐式正则化受限于特定的几何或特定类的学习问题，表明需要一个通用的方法来控制隐式正则化。为此，我们提出了一个统一的方法，使用镜面下降（MD）来控制回归和分类设置中的隐式正则化。具体而言，我们表明，MD与通用的下降方向一起使用时，在所有标准几何下都可以实现$\ell_p$（$p\in[1,\infty]$）范式的隐式正则化，并且在学习理论中使用的许多特殊类中也可以实现控制的泛化保证。

    Inspired by the remarkable success of deep neural networks, there has been significant interest in understanding the generalization performance of overparameterized models. Substantial efforts have been invested in characterizing how optimization algorithms impact generalization through their "preferred" solutions, a phenomenon commonly referred to as implicit regularization. In particular, it has been argued that gradient descent (GD) induces an implicit $\ell_2$-norm regularization in regression and classification problems. However, the implicit regularization of different algorithms are confined to either a specific geometry or a particular class of learning problems, indicating a gap in a general approach for controlling the implicit regularization. To address this, we present a unified approach using mirror descent (MD), a notable generalization of GD, to control implicit regularization in both regression and classification settings. More specifically, we show that MD with the gen
    
[^174]: Gibbs采样神经网络的后验分布

    Gibbs Sampling the Posterior of Neural Networks. (arXiv:2306.02729v1 [cs.LG])

    [http://arxiv.org/abs/2306.02729](http://arxiv.org/abs/2306.02729)

    这篇论文提出了一种添加噪声的神经网络模型，并使用Gibbs采样器从后验分布中进行采样，该方法在真实数据和合成数据中能够达到类似于马尔科夫链蒙特卡洛方法的性能。

    

    本文研究了从神经网络的后验分布中进行采样。我们提出了一种新的概率模型，该模型在网络的每个预激活和后激活中添加噪声，并认为使用有效的Gibbs采样器可以采样得到所得到的后验分布。在真实数据和合成数据上，Gibbs采样器能够达到类似于状态-of-the-art的马尔科夫链蒙特卡洛方法（如哈密顿蒙特卡洛或Metropolis调整Langevin算法）的性能。通过在师生设置中进行分析，我们引入了一个热化准则，该准则允许我们检测算法在使用合成标签的数据上运行时是否无法从后验分布中采样。该准则基于师生设置中的事实，我们可以直接在平衡点处初始化算法。

    In this paper, we study sampling from a posterior derived from a neural network. We propose a new probabilistic model consisting of adding noise at every pre- and post-activation in the network, arguing that the resulting posterior can be sampled using an efficient Gibbs sampler. The Gibbs sampler attains similar performances as the state-of-the-art Monte Carlo Markov chain methods, such as the Hamiltonian Monte Carlo or the Metropolis adjusted Langevin algorithm, both on real and synthetic data. By framing our analysis in the teacher-student setting, we introduce a thermalization criterion that allows us to detect when an algorithm, when run on data with synthetic labels, fails to sample from the posterior. The criterion is based on the fact that in the teacher-student setting we can initialize an algorithm directly at equilibrium.
    
[^175]: 抗干扰约束学习

    Resilient Constrained Learning. (arXiv:2306.02426v1 [cs.LG])

    [http://arxiv.org/abs/2306.02426](http://arxiv.org/abs/2306.02426)

    本论文提出了一个名为“抗干扰约束学习”的方法来解决在部署机器学习解决方案时需要满足除了准确性以外的多个要求，并以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松学习约束。

    

    在部署机器学习解决方案时，除了准确性之外，它们必须满足多个要求，如公平性、鲁棒性或安全性。这些要求可以通过使用惩罚来隐式地施加，或者通过基于Lagrangian对偶的约束优化方法来显式地施加。无论哪种方式，指定要求都受到妥协和有限的有关数据的先前知识的影响。此外，它们对性能的影响通常只能通过实际解决学习问题来评估。本文提出了一种约束学习方法，该方法在同时解决学习任务的同时调整要求。为此，它以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松了学习约束。我们将此方法称为具有弹性的约束学习，这是对用于描述生态系统的术语的一种借鉴。

    When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems tha
    
[^176]: 利用大型语言模型生成私有的合成文本

    Harnessing large-language models to generate private synthetic text. (arXiv:2306.01684v1 [cs.LG])

    [http://arxiv.org/abs/2306.01684](http://arxiv.org/abs/2306.01684)

    本文研究了一种通过利用公开数据集和预训练的生成语言模型，结合私有的微调方法，实现生成具有差分隐私性质的合成文本数据集的方法。生成的文本数据集可以重复使用于其他任务，可无限期保留，或与第三方共享而不损失隐私。

    

    差分隐私训练方法，如DP-SGD，可以通过确保机器学习模型不会透露私有信息来保护敏感的训练数据。本文研究了一种替代方法，利用敏感数据集生成新的合成数据集，并确保相对于原始数据具有差分隐私。这样做有几个优点：合成数据可以重复使用于其他任务（包括超参数调整），可以无限期保留，或与第三方共享而不损失隐私。但是，获取差分隐私数据比在训练期间引入差分隐私更加困难。为了使其在文本中可行，最近的研究利用公共数据，从预训练的生成语言模型开始，并在敏感数据上进行私人调整。该模型可以用于抽样差分隐私合成数据集。虽然这个策略似乎很简单，但执行它已被证明是有问题的。以前的方法要么表现出显著的性能损失，要么...

    Differentially private (DP) training methods like DP-SGD can protect sensitive training data by ensuring that ML models will not reveal private information. An alternative approach, which this paper studies, is to use a sensitive dataset to generate a new synthetic dataset which is differentially private with respect to the original data. Doing so has several advantages: synthetic data can be reused for other tasks (including for hyper parameter tuning), retained indefinitely, or shared with third parties without sacrificing privacy.  However, obtaining DP data is much harder than introducing DP during training. To make it feasible for text, recent work has utilized public data by starting with a pre-trained generative language model and privately finetuning it on sensitive data. This model can be used to sample a DP synthetic dataset. While this strategy seems straightforward, executing it has proven problematic. Previous approaches either show significant performance loss, or have, a
    
[^177]: 带曲率敏感模型的连续性结果的部分反事实识别

    Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model. (arXiv:2306.01424v1 [stat.ML])

    [http://arxiv.org/abs/2306.01424](http://arxiv.org/abs/2306.01424)

    本文研究了连续性结果的部分反事实识别问题，并提出了一种新颖的敏感性模型——曲率敏感模型，通过限制函数级集的曲率来获得信息边界。

    

    反事实推断旨在回答“如果”问题，因此属于Pearl因果关系阶梯中最精细的推理类型。现有的针对具有连续结果的反事实推断方法旨在进行点识别，因此对基础结构因果模型进行了强有力且不自然的假设。在本文中，我们放宽了这些假设，旨在进行连续结果的部分反事实识别，即当反事实查询存在具有信息边界的无知区间中时。我们证明了，在一般情况下，即使是连续可微的结构因果模型函数的级集的曲率也是非信息的，反事实查询的无知区间也是非信息的。因此，我们提出了一种新颖的敏感性模型称为曲率敏感模型来解决这个问题。它允许我们通过限制函数级集的曲率来获得信息边界。我们进一步展示了现有的点反事实识别方法可以视为我们提出框架的特定情况。

    Counterfactual inference aims to answer retrospective ''what if'' questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual ide
    
[^178]: 通过领域知识启示的深度学习进行药物推荐

    Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])

    [http://arxiv.org/abs/2305.19604](http://arxiv.org/abs/2305.19604)

    提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。

    

    药物推荐是医疗保健的基本但至关重要的分支，提供机会为复杂健康状况的患者支持临床医生更精确的药物处方。从电子健康记录（EHR）中学习推荐药物是先前研究中最常见的方法。然而，大多数研究忽视了根据患者的EHR中的临床表现纳入领域知识的问题。为了解决这些问题，我们提出了一种新颖的基于动态领域知识的药物推荐框架，即领域知识启示网络（DKINet），用于将领域知识与可观察的患者临床表现相结合。特别是，我们首先设计了一个基于领域知识的编码器来捕捉领域信息，然后开发了一个数据驱动的编码器将领域知识整合到可观察的EHR中。

    Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
    
[^179]: WiFi-TCN：基于WiFi信号的人体交互识别的时间卷积方法

    WiFi-TCN: Temporal Convolution for Human Interaction Recognition based on WiFi signal. (arXiv:2305.18211v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2305.18211](http://arxiv.org/abs/2305.18211)

    提出了一种基于WiFi信号的人体交互识别方法WiFi-TCN。传统基于WiFi的方法存在场景或对象变化时性能下降的问题，解决这个问题需要使用大规模数据集进行模型训练。与传统方法相比，该方法具有成本低、部署简易等优势，能够有效解决基于WiFi的人体交互识别中的挑战。

    

    近年来，基于Wi-Fi的人体活动识别在诸多领域如医疗保健、安防和老年护理等方面得到了广泛关注。相比于依赖摄像头和传感器的传统方法，基于Wi-Fi的方法具有成本低、部署简易等优势。然而，基于Wi-Fi的人体活动识别面临一个重要挑战，即当场景或对象发生变化时，性能明显下降。为了解决这个问题，重要的是使用大规模数据集进行模型训练。最近的研究中，使用基于CNN或序列到序列模型如LSTM、GRU或Transformer的方法变得流行。虽然序列到序列模型可以更精确，但它们的计算量更大，需要更多的训练数据。为了克服这些限制，我们提出了一种新的方法。

    The utilization of Wi-Fi based human activity recognition has gained considerable interest in recent times, primarily owing to its applications in various domains such as healthcare for monitoring breath and heart rate, security, elderly care. These Wi-Fi-based methods exhibit several advantages over conventional state-of-the-art techniques that rely on cameras and sensors, including lower costs and ease of deployment. However, a significant challenge associated with Wi-Fi-based HAR is the significant decline in performance when the scene or subject changes. To mitigate this issue, it is imperative to train the model using an extensive dataset. In recent studies, the utilization of CNN-based models or sequence-to-sequence models such as LSTM, GRU, or Transformer has become prevalent. While sequence-to-sequence models can be more precise, they are also more computationally intensive and require a larger amount of training data. To tackle these limitations, we propose a novel approach th
    
[^180]: 只使用前向传递微调语言模型

    Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v1 [cs.LG])

    [http://arxiv.org/abs/2305.17333](http://arxiv.org/abs/2305.17333)

    本论文提出了一种内存高效的零阶优化器，可以使用与推理相同的存储空间微调语言模型，其可以在大规模模型下更快地优化，具有更好的实验结果。

    

    微调语言模型已经在各种下游任务中取得了成功，但随着语言模型的增大，反向传播需要的存储空间数量变得过高。零阶（ZO）方法理论上仅使用两次前向传递就可以估计梯度，但通常情况下对大型模型进行优化的速度非常慢。在本文中，我们提出了一种内存高效的零阶优化器（MeZO），将经典的ZO-SGD方法适应于原地操作，从而使用与推理相同的存储空间微调语言模型。例如，只使用一张A100 80GB GPU，MeZO就可以训练一个300亿参数的模型，而使用反向传播可以在相同的预算下仅训练一个27亿个参数的语言模型。我们在各种模型类型（掩码和自回归语言模型）、模型规模（高达66B）和下游任务（分类、多项选择和生成）进行了全面的实验。我们的结果表明，（1）MeZO明显优于上下文学习和线性PR模型。

    Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear pr
    
[^181]: 大型语言模型的异质价值评估

    Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])

    [http://arxiv.org/abs/2305.17147](http://arxiv.org/abs/2305.17147)

    本文提出了一种自动对齐评估方法A2EHV，采用异质价值系统，并基于价值合理性和社会价值定向框架评估代理人行为的社会偏好，结果表明比传统对齐方法更合理。

    

    大型语言模型（LLM）的出现使得将它们的价值与人类价值对齐变得至关重要。当前的方法通常尝试将其与一种同质的人类价值对齐，并需要人类验证，但缺乏对对齐所需方面和深度的共识以及造成的人类偏见。在本文中，我们提出了一种自动对齐评估方法A2EHV，该方法采用异质价值系统，（1）是自动化的，以最小化单个人类偏见，并且（2）允许评估针对各种目标值的异质代理人。我们的方法基于价值合理性的概念，它代表了代理人执行最能满足目标价值行为的能力。价值合理性的量化是通过社会心理学中的社会价值定向框架进行的，该框架将价值空间分为四个类别，以评估代理人行为的社会偏好。我们评估了三个模型的价值合理性，结果表明A2EHV方法比传统对齐方法更合理。

    The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
    
[^182]: 无偏压缩在分布式优化中节省通信：何时以及多少？

    Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?. (arXiv:2305.16297v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16297](http://arxiv.org/abs/2305.16297)

    本文研究了分布式优化中通信压缩的效果，探讨了无偏压缩降低总通信成本的条件和程度。

    

    通信压缩是在分布式优化中常用的技术，可以通过传输压缩梯度和模型参数来减少通信开销。然而，压缩会引入信息失真，导致收敛减慢并增加通信轮次以达到期望解。鉴于每轮通信成本的降低和额外的通信轮次之间的权衡，尚不清楚通信压缩是否降低了总通信成本。本文探讨了无偏压缩，一种广泛使用的压缩形式，在什么条件下可以降低总通信成本，以及在多大程度上可以降低。为此，我们提出了第一个理论框架来表征具有通信压缩的分布式优化中的总通信成本。我们证明了仅仅使用无偏压缩并不能一定节省总通信成本，但这种结果有

    Communication compression is a common technique in distributed optimization that can alleviate communication overhead by transmitting compressed gradients and model parameters. However, compression can introduce information distortion, which slows down convergence and incurs more communication rounds to achieve desired solutions. Given the trade-off between lower per-round communication costs and additional rounds of communication, it is unclear whether communication compression reduces the total communication cost.  This paper explores the conditions under which unbiased compression, a widely used form of compression, can reduce the total communication cost, as well as the extent to which it can do so. To this end, we present the first theoretical formulation for characterizing the total communication cost in distributed optimization with communication compression. We demonstrate that unbiased compression alone does not necessarily save the total communication cost, but this outcome c
    
[^183]: 黑盒变分推断收敛性分析

    Black-Box Variational Inference Converges. (arXiv:2305.15349v1 [cs.LG])

    [http://arxiv.org/abs/2305.15349](http://arxiv.org/abs/2305.15349)

    通过对黑盒变分推断（BBVI）的分析，发现一些常见的算法设计选择可能会导致次优收敛速率，但使用带有近端随机梯度下降的BBVI可以实现最强收敛率保证。

    

    我们提供了第一个完整的黑盒变分推断（BBVI）的收敛保证，也称为蒙特卡罗变分推断。尽管早期的研究只针对简化版本的BBVI进行了研究（例如，有界域、有界支持、仅针对尺度进行优化等），但我们的设置不需要任何这样的算法修改。我们的结果适用于对数平滑后验密度，无论是否强对数凹性以及位置-尺度变分族。此外，我们的分析揭示出了一些常见的算法设计选择，特别是变分近似尺度的非线性参数化，可能会导致次优收敛速率。幸运的是，运行带有近端随机梯度下降的BBVI可以纠正这些限制，从而实现已知的最强收敛率保证。我们通过将近端SGD与其他标准的BBVI实现进行比较，验证了这一理论结论在大规模数据集上的有效性。

    We provide the first convergence guarantee for full black-box variational inference (BBVI), also known as Monte Carlo variational inference. While preliminary investigations worked on simplified versions of BBVI (e.g., bounded domain, bounded support, only optimizing for the scale, and such), our setup does not need any such algorithmic modifications. Our results hold for log-smooth posterior densities with and without strong log-concavity and the location-scale variational family. Also, our analysis reveals that certain algorithm design choices commonly employed in practice, particularly, nonlinear parameterizations of the scale of the variational approximation, can result in suboptimal convergence rates. Fortunately, running BBVI with proximal stochastic gradient descent fixes these limitations, and thus achieves the strongest known convergence rate guarantees. We evaluate this theoretical insight by comparing proximal SGD against other standard implementations of BBVI on large-scale
    
[^184]: 不依赖于振幅的光 plethysmography (PPG) 机器学习方法：通过可见性图和迁移学习

    Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning. (arXiv:2305.14062v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2305.14062](http://arxiv.org/abs/2305.14062)

    该论文提出了一种不依赖于振幅的光 plethysmography (PPG) 机器学习方法，通过可见性图和迁移学习实现了对心率和血管老化等生物特征的稳健估计和预测。

    

    光体积描记法 (PPG) 是使用光测量血液体积的变化的一种方法，是大多数可穿戴设备的特征。PPG信号能够提供对人体循环系统的洞察，并可用于提取各种生物特征，例如心率和血管老化。尽管已经提出了几种算法，但许多算法存在限制，包括过多地依赖人工校准、高信号质量要求和缺乏泛化能力。在本文中，我们引入了一种结合了图论和计算机视觉算法的PPG信号处理框架，该框架对振幅无关并且对仿射变换不变。它还需要最少的预处理，通过RGB通道融合信息，并在任务和数据集上展现了稳健的泛化能力。所提出的VGTL-net在血管老化预测方面实现了最先进的性能，并展示了稳健的估计能力。

    Photoplethysmography (PPG) refers to the measurement of variations in blood volume using light and is a feature of most wearable devices. The PPG signals provide insight into the body's circulatory system and can be employed to extract various bio-features, such as heart rate and vascular ageing. Although several algorithms have been proposed for this purpose, many exhibit limitations, including heavy reliance on human calibration, high signal quality requirements, and a lack of generalisation. In this paper, we introduce a PPG signal processing framework that integrates graph theory and computer vision algorithms, to provide an analysis framework which is amplitude-independent and invariant to affine transformations. It also requires minimal preprocessing, fuses information through RGB channels and exhibits robust generalisation across tasks and datasets. The proposed VGTL-net achieves state-of-the-art performance in the prediction of vascular ageing and demonstrates robust estimation
    
[^185]: 一种用于音视频语音表示学习的多模态动态变分自编码器

    A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning. (arXiv:2305.03582v1 [cs.SD])

    [http://arxiv.org/abs/2305.03582](http://arxiv.org/abs/2305.03582)

    本文提出了一种多模态动态自编码器（MDVAE），用于无监督音视频语音表示学习，该方法在中间表示上进行了静态与动态信息、模态特异与共同信息的分离，并且在实验中表现出优越性。

    

    本文提出了一种多模态动态自编码器（MDVAE），用于无监督音视频语音表示学习。潜在空间被构造为将在各个模态之间共享的潜在动态因素与每个模态特定的因素区分开来。同时，引入一个静态潜变量来编码音视频语音序列中随时间恒定的信息。模型在一个音视频情感语音数据集上进行无监督训练分两个阶段。在第一阶段，对于每个模态，首先独立学习一个向量量化自编码器（VQ-VAE），而没有时间建模。第二阶段则在向量量化自编码器（VQ-VAEs）的中间表示上学习MDVAE模型。该方法的实验结果表明，在语音表示学习方面，提出的方法优于现有方法。

    In this paper, we present a multimodal \textit{and} dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how a
    
[^186]: 规避扩散模型中添加的噪声对数据进行保护的挑战

    The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models. (arXiv:2303.08500v1 [cs.LG])

    [http://arxiv.org/abs/2303.08500](http://arxiv.org/abs/2303.08500)

    保护个人隐私信息是很重要的，但规避扩散模型中添加的噪声对数据进行保护存在挑战。AVATAR算法借助扩散模型的威力，提供了一种精心设计的去噪过程来消除数据保护扰动的影响，并获得了在多个数据集上的最先进的性能。

    

    保护个人数据免受机器学习模型的利用至关重要。最近，可用性攻击展现出提供额外保护措施的巨大潜力，以防止未经授权地使用数据来训练神经网络。这些方法旨在向干净数据添加难以察觉的噪声，使神经网络无法从受保护的数据中提取有意义的模式，声称可以使个人数据“无法利用”。在本文中，我们针对这种方法提供了一个强有力的对抗措施，表明不可利用的数据可能只是一种幻觉。特别地，我们利用扩散模型的威力，并展示精心设计的去噪过程可以消除数据保护扰动的影响。我们严谨地分析了我们的算法，并在理论上证明了所需去噪的量直接与数据保护扰动的数量成正比。我们的方法名为AVATAR，在包括CelebA数据集在内的多个数据集上提供了最先进的性能，其中它以巨大的优势胜出现有方法。

    Protecting personal data against the exploitation of machine learning models is of paramount importance. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data "unexploitable." In this paper, we provide a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can defuse the ramifications of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-o
    
[^187]: 深度神经网络时代的肿瘤多模态数据整合：一篇综述

    Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review. (arXiv:2303.06471v1 [cs.LG])

    [http://arxiv.org/abs/2303.06471](http://arxiv.org/abs/2303.06471)

    本文综述了深度神经网络在多模态数据整合方面的应用，以提高癌症诊断和治疗的准确性和可靠性。

    This review article analyzes the application of deep neural networks in multimodal data integration to improve the accuracy and reliability of cancer diagnosis and treatment.

    癌症在不同尺度、模态和分辨率的获取数据中具有关系信息，例如放射学、病理学、基因组学、蛋白质组学和临床记录。整合多种数据类型可以提高癌症诊断和治疗的准确性和可靠性。可能存在人类或现有技术工具无法视觉上区分的与疾病相关的信息。传统方法通常关注单个尺度的生物系统的部分或单一模态信息，并未涵盖数据异质性的完整光谱。深度神经网络促进了复杂的多模态数据融合方法的发展，可以从多个来源提取和整合相关信息。最近的深度学习框架，如图形神经网络（GNN）和变压器，在多模态学习方面取得了显着的成功。本综述文章提供了对当前多模态数据整合方法的深入分析。

    Cancer has relational information residing at varying scales, modalities, and resolutions of the acquired data, such as radiology, pathology, genomics, proteomics, and clinical records. Integrating diverse data types can improve the accuracy and reliability of cancer diagnosis and treatment. There can be disease-related information that is too subtle for humans or existing technological tools to discern visually. Traditional methods typically focus on partial or unimodal information about biological systems at individual scales and fail to encapsulate the complete spectrum of the heterogeneous nature of data. Deep neural networks have facilitated the development of sophisticated multimodal data fusion approaches that can extract and integrate relevant information from multiple sources. Recent deep learning frameworks such as Graph Neural Networks (GNNs) and Transformers have shown remarkable success in multimodal learning. This review article provides an in-depth analysis of the state-
    
[^188]: 意义的线性空间：视觉语言模型中的组合结构

    Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14383](http://arxiv.org/abs/2302.14383)

    本文研究了视觉语言模型中的组合结构，并提出了一种使用嵌入空间中较小集合的向量组合来近似表示来自编码器的表示形式的方法，将这些向量视为“理想单词”，并在CLIP的嵌入中以实验方式探索了这些结构的可用性。

    

    本文研究了预训练视觉语言模型（VLM）中的数据嵌入的组合结构。传统上，组合性与预先存在的词汇表中的单词嵌入的代数运算有关。相反，我们试图使用嵌入空间中较小集合的向量组合来近似表示来自编码器的表示形式。这些向量可以被看作是在模型的嵌入空间中直接生成概念的“理想单词”。我们首先从几何学的角度提出了理解组合结构的框架。然后，我们解释了VLM嵌入在概率上的这些组合结构的含义，并提供了它们在实践中产生的直觉。最后，我们在CLIP的嵌入中以实验方式探索了这些结构，并评估了它们在解决分类、去偏和检索等不同视觉语言任务中的有用性。我们的结果表明，嵌入空间中简单的线性代数运算可以实现与更复杂的方法相媲美甚至更好的性能，证明了所提出的意义的线性空间的有效性和可解释性。

    We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a pre-existing vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as "ideal words" for generating concepts directly within the embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP's embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic o
    
[^189]: Classy Ensemble: 一种新颖的用于分类的集成学习算法

    Classy Ensemble: A Novel Ensemble Algorithm for Classification. (arXiv:2302.10580v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10580](http://arxiv.org/abs/2302.10580)

    Classy Ensemble是一种新颖的集成学习算法，通过每类准确率的加权组合来聚合模型，在大量机器学习数据集上展现出优越性能。并提出Classy Cluster Ensemble和Classy Evolutionary Ensemble两种增强方法。

    

    我们提出了Classy Ensemble，一种新的用于分类任务的集成算法，它通过每类准确率的加权组合来聚合模型。我们在153个机器学习数据集上进行了测试，结果表明，Classy Ensemble优于两种其他著名的聚合算法——基于顺序的修剪和基于聚类的修剪——以及最近引入的lexigarden集成生成器。接着，我们提出了三种增强方法：1）Classy Cluster Ensemble，将Classy Ensemble和基于聚类的修剪相结合；2）深度学习实验，展示了Classy Ensemble在四个图像数据集Fashion MNIST、CIFAR10、CIFAR100和ImageNet上的优越性；以及3）Classy Evolutionary Ensemble，其中使用进化算法来选择Classy Ensemble从中选择的模型集合。

    We present Classy Ensemble, a novel ensemble-generation algorithm for classification tasks, which aggregates models through a weighted combination of per-class accuracy. Tested over 153 machine learning datasets we demonstrate that Classy Ensemble outperforms two other well-known aggregation algorithms -order-based pruning and clustering-based pruning -- as well as the recently introduced lexigarden ensemble generator. We then present three enhancements: 1) Classy Cluster Ensemble, which combines Classy Ensemble and cluster-based pruning; 2) Deep Learning experiments, showing the merits of Classy Ensemble over four image datasets: Fashion MNIST, CIFAR10, CIFAR100, and ImageNet; and 3) Classy Evolutionary Ensemble, wherein an evolutionary algorithm is used to select the set of models which Classy Ensemble picks from.
    
[^190]: 混合精度神经网络量化的高效有效方法，用于更快、更节能的推断。

    Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference. (arXiv:2301.13330v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13330](http://arxiv.org/abs/2301.13330)

    通过混合精度量化方法，选择性调整神经网络的各个层的精度，以实现在任务性能下降最小的情况下，神经网络的快速、节能推断。

    

    为了实现高效的神经网络推断，希望以最简单的网络、最少的计算、存储和功耗达到最先进的准确性。将网络量化为较低的精度是简化网络的强大技术。由于网络的每一层对量化的敏感程度可能不同，混合精度量化方法选择性地调整各个层的精度，以实现任务性能（如准确性）的最小下降。为了估计层精度选择对任务性能的影响，引入了两种方法：i) 基于熵近似的层选择（EAGL）快速地使用权重分布的熵，ii) 基于准确性感知的层精度选择（ALPS）简单地依赖层精度降低后的单次迭代微调。使用EAGL和ALPS进行层精度选择，在ResNet-50、ResNet-101和...

    For efficient neural network inference, it is desirable to achieve state-of-the-art accuracy with the simplest networks requiring the least computation, memory, and power. Quantizing networks to lower precision is a powerful technique for simplifying networks. As each layer of a network may have different sensitivity to quantization, mixed precision quantization methods selectively tune the precision of individual layers to achieve a minimum drop in task performance (e.g., accuracy). To estimate the impact of layer precision choice on task performance, two methods are introduced: i) Entropy Approximation Guided Layer selection (EAGL) is fast and uses the entropy of the weight distribution, and ii) Accuracy-aware Layer Precision Selection (ALPS) is straightforward and relies on single epoch fine-tuning after layer precision reduction. Using EAGL and ALPS for layer precision selection, full-precision accuracy is recovered with a mix of 4-bit and 2-bit layers for ResNet-50, ResNet-101 and
    
[^191]: 基于条件可逆神经网络（cINN）的展开方法及其迭代训练（arXiv：2212.08674v3 [hep-ph] UPDATED）

    An unfolding method based on conditional Invertible Neural Networks (cINN) using iterative training. (arXiv:2212.08674v3 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2212.08674](http://arxiv.org/abs/2212.08674)

    该论文提出了一种基于条件可逆神经网络（cINN）的展开方法（IcINN），通过迭代训练调整训练样本与数据之间的偏差，实现了对数据与理论预测的比较中探测器效应的展开。

    

    探测器效应展开对于数据与理论预测的比较至关重要。传统方法只能将数据表示为低维度，而机器学习使得保留完整维度的新展开技术成为可能。生成网络如可逆神经网络（INN）可以实现概率展开，将个体事件映射到相应的展开概率分布。然而，这种方法的准确性受到训练样本与实际展开数据的一致性的限制。我们引入了迭代条件INN（IcINN）展开方法，用于调整训练样本与数据之间的偏差。首先在玩具数据上验证了IcINN展开方法，然后将其应用于$pp \to Z \gamma \gamma$过程的伪数据。

    The unfolding of detector effects is crucial for the comparison of data to theory predictions. While traditional methods are limited to representing the data in a low number of dimensions, machine learning has enabled new unfolding techniques while retaining the full dimensionality. Generative networks like invertible neural networks~(INN) enable a probabilistic unfolding, which map individual events to their corresponding unfolded probability distribution. The accuracy of such methods is however limited by how well simulated training samples model the actual data that is unfolded. We introduce the iterative conditional INN~(IcINN) for unfolding that adjusts for deviations between simulated training samples and data. The IcINN unfolding is first validated on toy data and then applied to pseudo-data for the $pp \to Z \gamma \gamma$ process.
    
[^192]: 可扩展的分层无线联邦学习算法

    Scalable Hierarchical Over-the-Air Federated Learning. (arXiv:2211.16162v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2211.16162](http://arxiv.org/abs/2211.16162)

    本研究提出了一种针对分布式环境的通信高效的分层联邦学习算法，通过使用可扩展的无线聚合方案和带宽有限的广播方案，解决了设备干扰和边缘服务器干扰的问题。

    

    本研究提出了一种针对包含核心服务器和多个边缘服务器及设备集群的分布式环境的通信高效的分层联邦学习算法。假设不同的学习任务，具有相同任务的集群进行协作。为了在无线链路上实现算法，我们提出了一种可扩展的分簇无线聚合方案，用于上行链路，同时采用带宽有限的广播方案用于下行链路，每个算法迭代只需要一个资源块，不受边缘服务器和设备数量的影响。这种设置面临着上行链路设备干扰和下行链路边缘服务器干扰的问题，需要进行严格的建模。我们首先通过将设备建模为一个泊松集群过程，在设置中建立了一个空间模型，并对由干扰引起的上行链路和下行链路的误差进行量化。然后，我们提出了一种全面的数学方法来推导收敛性。

    In this work, we propose a communication-efficient hierarchical federated learning algorithm for distributed setups including core servers and multiple edge servers with clusters of devices. Assuming different learning tasks, clusters with a same task collaborate. To implement the algorithm over wireless links, we propose a scalable clustered over-the-air aggregation scheme for the uplink with a bandwidth-limited broadcast scheme for the downlink that requires only a single resource block for each algorithm iteration, independent of the number of edge servers and devices. This setup is faced with interference of devices in the uplink and interference of edge servers in the downlink that are to be modeled rigorously. We first develop a spatial model for the setup by modeling devices as a Poisson cluster process over the edge servers and quantify uplink and downlink error terms due to the interference. Accordingly, we present a comprehensive mathematical approach to derive the convergenc
    
[^193]: ARMA单元：神经自回归建模的模块化和有效方法

    ARMA Cell: A Modular and Effective Approach for Neural Autoregressive Modeling. (arXiv:2208.14919v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14919](http://arxiv.org/abs/2208.14919)

    本文介绍了ARMA单元，一种更简单、模块化和有效的神经网络时间序列建模方法，能够自然地处理多变量时间序列，并引入了ConvARMA单元作为一种解决方法。

    

    自回归移动平均(ARMA)模型是一种经典的、被广泛研究的时间序列数据建模方法。它具有引人入胜的理论性质，在实践中被广泛应用。最近的深度学习方法普及了循环神经网络(RNN)，特别是长短期记忆(LSTM)单元，在神经时间序列建模中成为表现最好和最常见的构建模块之一。虽然复杂的RNN单元对于具有长期影响的时间序列数据或序列有优势，但并不总是必需的，有时甚至不如更简单的循环方法好。在本文中，我们引入了ARMA单元，这是一种更简单、模块化和有效的神经网络时间序列建模方法。这个单元可以在任何存在循环结构的神经网络架构中使用，并且可以自然地处理多变量时间序列，使用向量自回归技术。我们还介绍了ConvARMA单元作为一种自然的解决方法。

    The autoregressive moving average (ARMA) model is a classical, and arguably one of the most studied approaches to model time series data. It has compelling theoretical properties and is widely used among practitioners. More recent deep learning approaches popularize recurrent neural networks (RNNs) and, in particular, Long Short-Term Memory (LSTM) cells that have become one of the best performing and most common building blocks in neural time series modeling. While advantageous for time series data or sequences with long-term effects, complex RNN cells are not always a must and can sometimes even be inferior to simpler recurrent approaches. In this work, we introduce the ARMA cell, a simpler, modular, and effective approach for time series modeling in neural networks. This cell can be used in any neural network architecture where recurrent structures are present and naturally handles multivariate time series using vector autoregression. We also introduce the ConvARMA cell as a natural 
    
[^194]: CP-PINNs: 使用物理知识神经网络和总变差惩罚进行PDE中的变点检测

    CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty. (arXiv:2208.08626v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.08626](http://arxiv.org/abs/2208.08626)

    本文提出了一种新的CP-PINNs模型，通过将PINNs与总变差惩罚相结合，实现了准确的变点检测和PDE的发现。我们还开发了一种元学习算法，能够在数据的连续批次上动态改进优化目标。实证结果表明，在存在变点的情况下，该方法能够准确估计参数和模型对齐，在没有变点的情况下能够数值上收敛到原始PINNs模型的解。

    

    本文展示了在参数中存在未知变点的情况下，物理知识神经网络（PINNs）可能无法正确估计偏微分方程（PDE）的动态过程。为了解决这个问题，我们提出了一个新的CP-PINNs模型，将PINNs与总变差惩罚相结合，用于准确的变点检测和PDE的发现。为了在模型拟合、PDE发现和变点检测任务之间进行最优组合，我们开发了一种新的元学习算法，利用批量学习在数据的连续批次上动态改进优化目标。在实证方面，在动态过程中存在变点的情况下，我们的方法能够准确估计参数和模型对齐，在数据中没有变点的情况下，数值上收敛到原始PINNs模型的解。

    The paper shows that Physics-Informed Neural Networks (PINNs) can fail to estimate the correct Partial Differential Equations (PDEs) dynamics in cases of unknown changepoints in the parameters. To address this, we propose a new CP-PINNs model which integrates PINNs with Total-Variation penalty for accurate changepoints detection and PDEs discovery. In order to optimally combine the tasks of model fitting, PDEs discovery, and changepoints detection, we develop a new meta-learning algorithm that exploits batch learning to dynamically refines the optimization objective when moving over the consecutive batches of the data. Empirically, in case of changepoints in the dynamics, our approach demonstrates accurate parameter estimation and model alignment, and in case of no changepoints in the data, it converges numerically to the solution from the original PINNs model.
    
[^195]: 压缩感知MRI中的局部对抗性伪像

    Localized adversarial artifacts for compressed sensing MRI. (arXiv:2206.05289v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2206.05289](http://arxiv.org/abs/2206.05289)

    我们研究了压缩感知MRI中的局部对抗性伪像，并发现与传统总变差（TV）最小化相比，深度神经网络（DNN）在抵抗该对抗扰动方面具有更好的鲁棒性。

    

    随着对于深度神经网络（DNN）在图像重建任务中的兴趣增加，它们的可靠性受到了质疑。然而，最近的研究表明，在适当正则化的情况下，与总变差（TV）最小化相比，DNN在$\ell^2$重建误差方面显示出类似的对抗噪声鲁棒性。我们考虑了一种不同的鲁棒性概念，使用$\ell^\infty$范数，认为局部重建伪像比$\ell^2$误差更相关。我们对频域下采样的磁共振成像测量创建了对抗性扰动，该扰动在TV正则化重建中引起严重的局部伪像。值得注意的是，同样的攻击方法对基于DNN的重建效果不如此显著。最后，我们证明了这种现象在可以保证精确恢复的重建方法中是固有的。

    As interest in deep neural networks (DNNs) for image reconstruction tasks grows, their reliability has been called into question (Antun et al., 2020; Gottschling et al., 2020). However, recent work has shown that, compared to total variation (TV) minimization, when appropriately regularized, DNNs show similar robustness to adversarial noise in terms of $\ell^2$-reconstruction error (Genzel et al., 2022). We consider a different notion of robustness, using the $\ell^\infty$-norm, and argue that localized reconstruction artifacts are a more relevant defect than the $\ell^2$-error. We create adversarial perturbations to undersampled magnetic resonance imaging measurements (in the frequency domain) which induce severe localized artifacts in the TV-regularized reconstruction. Notably, the same attack method is not as effective against DNN based reconstruction. Finally, we show that this phenomenon is inherent to reconstruction methods for which exact recovery can be guaranteed, as with comp
    
[^196]: 自适应估计带有赌博反馈的随机向量：从均方误差视角来看

    Adaptive Estimation of Random Vectors with Bandit Feedback: A mean-squared error viewpoint. (arXiv:2203.16810v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.16810](http://arxiv.org/abs/2203.16810)

    本文研究了在每轮仅观察部分未知协方差的高斯向量情况下，通过均方误差估计的顺序学习问题，并提出了连续消除算法的一种变体。同时，导出了样本复杂性的极小值下界。

    

    本文考虑在每轮观察仅有$ m < K $个未知协方差的高斯$ K $向量的问题下，通过均方误差（MSE）估计顺序学习。我们首先建立了MSE估计的集中界限。然后，我们使用赌博反馈的方法重新构建估计问题，并提出了一种连续消除算法的变体。我们还导出了一个极小值下界，以了解该问题样本复杂性的基本限制。

    We consider the problem of sequentially learning to estimate, in the mean squared error (MSE) sense, a Gaussian $K$-vector of unknown covariance by observing only $m < K$ of its entries in each round. We first establish a concentration bound for MSE estimation. We then frame the estimation problem with bandit feedback, and propose a variant of the successive elimination algorithm. We also derive a minimax lower bound to understand the fundamental limit on the sample complexity of this problem.
    
[^197]: 一种用于静态无路径估计到达时间的可解释叠加集成模型

    An Explainable Stacked Ensemble Model for Static Route-Free Estimation of Time of Arrival. (arXiv:2203.09438v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.09438](http://arxiv.org/abs/2203.09438)

    本文提出了一种可解释的叠加集成模型，用于静态无路径估计到达时间。该模型将多个机器学习模型组合成一个新的集成结构，能够超越先前的最先进模型。

    

    为了比较备选的出租车行程并计算它们，以及为驾驶员和乘客提供关于即将到来的出租车行程的见解，需要预测行程的持续时间或其预计到达时间（ETA）。为了达到较高的预测精度，ETA的机器学习模型是目前的技术水平。进一步提高预测精度的一个尚未开发的选项是将多个ETA模型组合成一个集成模型。尽管预测精度可能会增加，但这种集成模型的预测结果由于复杂的集成结构而变得不够透明。解决这个问题的一种方法是应用可解释人工智能（XAI）。本文的贡献有三个方面。首先，我们将我们先前工作中的多个机器学习模型组合成一个两层的集成模型- 一个叠加集成模型- 这本身就是一种创新；因此，我们可以超越先前的静态路径无关最先进的模型。

    To compare alternative taxi schedules and to compute them, as well as to provide insights into an upcoming taxi trip to drivers and passengers, the duration of a trip or its Estimated Time of Arrival (ETA) is predicted. To reach a high prediction precision, machine learning models for ETA are state of the art. One yet unexploited option to further increase prediction precision is to combine multiple ETA models into an ensemble. While an increase of prediction precision is likely, the main drawback is that the predictions made by such an ensemble become less transparent due to the sophisticated ensemble architecture. One option to remedy this drawback is to apply eXplainable Artificial Intelligence (XAI). The contribution of this paper is three-fold. First, we combine multiple machine learning models from our previous work for ETA into a two-level ensemble model - a stacked ensemble model - which on its own is novel; therefore, we can outperform previous state-of-the-art static route-fr
    
[^198]: 无需成本的DNN视觉注意机制：忽略背景，提高泛化

    Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism. (arXiv:2202.00232v6 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.00232](http://arxiv.org/abs/2202.00232)

    本文提出了一种无需额外计算成本的DNN视觉注意机制，名为ISNet，能够忽略图像背景并在COVID-19和结核病检测任务中比多个最先进神经网络具有更好的最小化分类器决策偏差影响的能力。

    

    本文介绍了一种用于图像分类器的注意机制，以及相应的深度神经网络（DNN）体系结构，称为ISNet。在训练期间，ISNet使用分割目标来学习如何找到图像的感兴趣区域，并将注意力集中在该区域上。该提议基于一种新颖的概念，即在LRP解释热图中最小化背景相关性。它可以应用于几乎任何分类神经网络架构，而不会在运行时增加任何额外的计算成本。由于能够忽略背景，因此结果单个DNN可以代替常见的分割器后跟分类器的流水线，速度更快，更轻。在注入图像背景的合成偏差之后（在各种应用中），我们将ISNet与多个最先进的神经网络进行比较，并定量证明其在最小化分类器决策中偏差影响方面具有优越的能力。 COVID-19和结核病检测任务分别作为使用案例来使用。

    This work introduces an attention mechanism for image classifiers and the corresponding deep neural network (DNN) architecture, dubbed ISNet. During training, the ISNet uses segmentation targets to learn how to find the image's region of interest and concentrate its attention on it. The proposal is based on a novel concept, background relevance minimization in LRP explanation heatmaps. It can be applied to virtually any classification neural network architecture, without any extra computational cost at run-time. Capable of ignoring the background, the resulting single DNN can substitute the common pipeline of a segmenter followed by a classifier, being faster and lighter. After injecting synthetic bias in images' backgrounds (in diverse applications), we compare the ISNet to multiple state-of-the-art neural networks, and quantitatively demonstrate its superior capacity of minimizing the bias influence over the classifier decisions. The tasks of COVID-19 and tuberculosis detection in ch
    
[^199]: GPEX，用于解释人工神经网络的框架

    GPEX, A Framework For Interpreting Artificial Neural Networks. (arXiv:2112.09820v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.09820](http://arxiv.org/abs/2112.09820)

    这篇论文提出了一种GPEX框架，用于解释深度人工神经网络，通过推导出一个证据下界来匹配神经网络的输出，而不对神经网络做出任何特定要求。实验证明，在一些理论假设下，只需要简单的网络结构即可达到良好性能。

    

    高斯过程（GPs）与深度人工神经网络（ANNs）之间的类比引起了广泛关注，并显示出揭示深度ANN的黑箱的潜力。现有的理论工作对ANN提出了严格的假设（例如，要求所有中间层为宽层，或使用特定的激活函数）。适应这些理论假设在最近的深层架构中很困难，并且随着新的深层架构的出现，这些理论条件需要进一步完善。在本文中，我们推导出一个证据下界，鼓励GP的后验与ANN的输出匹配，而不对ANN做任何要求。使用我们的方法，我们发现在5个数据集上，只有一部分理论假设就足够了。实际上，在我们的实验中，我们使用了一个普通的ResNet-18或前馈骨干网络，并在末端使用了一个宽层。训练GPs的一个局限性是在于与诱导点数量的可扩展性的缺乏。

    The analogy between Gaussian processes (GPs) and deep artificial neural networks (ANNs) has received a lot of interest, and has shown promise to unbox the blackbox of deep ANNs. Existing theoretical works put strict assumptions on the ANN (e.g. requiring all intermediate layers to be wide, or using specific activation functions). Accommodating those theoretical assumptions is hard in recent deep architectures, and those theoretical conditions need refinement as new deep architectures emerge. In this paper we derive an evidence lower-bound that encourages the GP's posterior to match the ANN's output without any requirement on the ANN. Using our method we find out that on 5 datasets, only a subset of those theoretical assumptions are sufficient. Indeed, in our experiments we used a normal ResNet-18 or feed-forward backbone with a single wide layer in the end. One limitation of training GPs is the lack of scalability with respect to the number of inducing points. We use novel computationa
    

