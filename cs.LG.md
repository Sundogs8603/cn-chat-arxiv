# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MeshDiffusion: Score-based Generative 3D Mesh Modeling.](http://arxiv.org/abs/2303.08133) | 本文提出了一种基于分数的生成式3D网格建模方法，依赖网格的图形结构和扩散模型，在不需要后处理的前提下，生成高质量、细节丰富的3D网格。 |
| [^2] | [CB2: Collaborative Natural Language Interaction Research Platform.](http://arxiv.org/abs/2303.08127) | CB2是一个用于研究基于任务的合作自然语言交互的平台，在3D游戏环境中提供了后端服务器和各种工具和流程。它在可扩展的研究中展示了学习的指令跟随模型。 |
| [^3] | [Do Transformers Parse while Predicting the Masked Word?.](http://arxiv.org/abs/2303.08117) | 本文探讨了预训练语言模型是否实际上进行解析以及为什么能捕捉解析结构，证明了类似于BERT或RoBERTa这样的掩码语言模型可以近似执行英语PCFG的Inside-Outside算法。 |
| [^4] | [Optimizing Quantum Federated Learning Based on Federated Quantum Natural Gradient Descent.](http://arxiv.org/abs/2303.08116) | 本文提出了一种基于联邦量子自然梯度下降算法的量子联邦学习优化算法，该算法可以在减少本地量子设备之间的通信开销的同时，比传统的随机梯度下降方法更快地实现训练收敛和提高测试精度。 |
| [^5] | [Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs.](http://arxiv.org/abs/2303.08114) | 本文提出了一种新的训练数据归因方法Simfluence，通过建立一个训练过程模拟器，可以追溯模型在任何给定样例上的预测结果到特定的有影响力的训练样例，从而研究训练样例间交互作用。 |
| [^6] | [Eliciting Latent Predictions from Transformers with the Tuned Lens.](http://arxiv.org/abs/2303.08112) | 本文提出了一种改进版的“逻辑透镜”技术——“调谐透镜”，通过训练一个仿射探针，可以将每个隐藏状态解码成词汇分布。这个方法被应用于各种自回归语言模型上，比逻辑透镜更具有预测性、可靠性和无偏性，并且通过因果实验验证使用的特征与模型本身类似。同时，本文发现潜在预测的轨迹可以用于高精度地检测恶意输入。 |
| [^7] | [Vision-based route following by an embodied insect-inspired sparse neural network.](http://arxiv.org/abs/2303.08109) | 该论文比较了一种基于仿生稀疏神经网络的FlyHash模型和其他非稀疏模型在路线跟随任务中的效率，发现FlyHash模型在数据编码方面更加高效。 |
| [^8] | [Domain Generalization in Machine Learning Models for Wireless Communications: Concepts, State-of-the-Art, and Open Issues.](http://arxiv.org/abs/2303.08106) | 本论文探讨了机器学习在无线通信中应用的重要性，特别是领域通用性方面的问题。研究表明，对于出现在不同领域中的数据，模型的准确性受到很大影响，领域通用性技术可以通过从不同的源域中学习模型并适应新的领域来解决这个问题。 |
| [^9] | [Meta contrastive label correction for financial time series.](http://arxiv.org/abs/2303.08103) | 本文针对股票价格预测中标记不准确的问题，提出了一种元对比标签校正方法。方法包括将对比学习算法融入元学习框架中，通过Gramian angular field和代表学习将时间序列数据生成图像，从而自动生成准确标签，并提高分类性能。 |
| [^10] | [Information-Theoretic Regret Bounds for Bandits with Fixed Expert Advice.](http://arxiv.org/abs/2303.08102) | 本文研究固定专家建议下的赌博机问题，提出了基于信息论的遗憾界限，可以使得某些算法的遗憾无限接近于零。此外，我们还提出了KL散度来描述专家之间的相似性界限，并给出了下限证明算法的最优性。 |
| [^11] | [Explanation Shift: Investigating Interactions between Models and Shifting Data Distributions.](http://arxiv.org/abs/2303.08081) | 该论文提出了一种新的方法，通过模型解释特征的转移性质来检测分布转移下学习模型的行为是否越界，在比较中发现其比最先进的技术更为优秀，提供了算法方法并在实验中得到验证。 |
| [^12] | [Style Feature Extraction Using Contrastive Conditioned Variational Autoencoders with Mutual Information Constraints.](http://arxiv.org/abs/2303.08068) | 本文提出了一种使用互信息约束下的对比条件变分自编码器进行从未标记数据中提取风格特征的方法，该方法由一个提取风格无关特征的对比学习部分和一个提取风格特征的CVAE部分组成。 |
| [^13] | [Demographic Parity Inspector: Fairness Audits via the Explanation Space.](http://arxiv.org/abs/2303.08040) | 这篇论文提出了一种基于解释空间的算法方法，测量分组人口统计学平等的违规情况，并允许我们检查组间歧视的原因，提高了审计公平性的敏感度。 |
| [^14] | [ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning.](http://arxiv.org/abs/2303.08035) | 本论文提出了一种新方法ISimDL，利用神经元灵敏度生成重要性采样，加速故障注入模拟，有效评估了先进的DL系统对硬件故障的韧性，同时显著减少了所需的模拟数量。 |
| [^15] | [BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment.](http://arxiv.org/abs/2303.08032) | BODEGA是一个基准测试，用于模拟真实的内容管理场景，在四个误传检测任务上测试受害模型和攻击方法。测试结果表明，在某些情况下，即使进行微小的文本修改，也可以欺骗最准确的分类器。 |
| [^16] | [EdgeServe: An Execution Layer for Decentralized Prediction.](http://arxiv.org/abs/2303.08028) | EdgeServe 是一种为去中心化预测而设计的机器学习系统，通过低延迟的消息代理程序将数据路由到可以提供预测的节点。它具有一系列新颖的优化，可以在计算、通信和准确性之间进行折衷。在多摄像机物体跟踪，网络入侵检测和人类活动识别等三个去中心化预测任务中，EdgeServe 展现了很好的性能。 |
| [^17] | [A Hierarchical Regression Chain Framework for Affective Vocal Burst Recognition.](http://arxiv.org/abs/2303.08027) | 本文提出了一个基于链式回归模型的层次框架，用于从语音爆发中识别情感。该框架明确考虑了文化、低维和高维情感空间以及不同情感类别之间的关系。 |
| [^18] | [Leveraging Pretrained Representations with Task-related Keywords for Alzheimer's Disease Detection.](http://arxiv.org/abs/2303.08019) | 本研究提出了一种利用高层次表示和任务相关关键词的方法来检测阿尔茨海默病，实验结果表明其具有较高的效率和优越性。 |
| [^19] | [Reliable Beamforming at Terahertz Bands: Are Causal Representations the Way Forward?.](http://arxiv.org/abs/2303.08017) | 本文介绍了一种动态、语义感知的波束赋形方案，利用新颖的人工智能算法在太赫兹频段中指导波束赋形，相较于经典的 MIMO 波束赋形方法有更好的表现。 |
| [^20] | [Detection of Abuse in Financial Transaction Descriptions Using Machine Learning.](http://arxiv.org/abs/2303.08016) | 本文针对银行服务中的技术辅助虐待问题，开发了一个利用深度学习模型来识别和评分交易以识别虐待行为的系统。 |
| [^21] | [Large statistical learning models effectively forecast diverse chaotic systems.](http://arxiv.org/abs/2303.08011) | 该论文研究了混沌预测的大规模实验，发现基于人工神经网络的大规模、领域不可知的时间序列预测方法表现出了相当强大的性能，尤其是分层神经基础函数模型表现最佳。 |
| [^22] | [Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models.](http://arxiv.org/abs/2303.08010) | 本文研究了基于窗口的早期退出集成方法，以在保持模型可扩展性的同时实现不确定性估计任务的高效实现。实验结果表明，该方法在准确性和计算效率上都达到了最新的研究成果。 |
| [^23] | [Finding the Needle in a Haystack: Unsupervised Rationale Extraction from Long Text Classifiers.](http://arxiv.org/abs/2303.07991) | 该论文研究了无监督理由提取的文档分类背景下，长文本分类器中的表现，并提出了一种比Longformer驱动的基线明显更好的RoBERTa句子级组合软注意力结构。 |
| [^24] | [Partial Neural Optimal Transport.](http://arxiv.org/abs/2303.07988) | 我们提出了一种新的神经方法来计算部分最优输运映射，并在合成例子上进行了测试。 |
| [^25] | [Practically Solving LPN in High Noise Regimes Faster Using Neural Networks.](http://arxiv.org/abs/2303.07987) | 本文设计了具有两层神经网络的模型，可在高噪声和低维度情况下解决学习有噪声偏差问题（LPN），而且速度比传统算法更快。在26维，0.498的噪声率下，用“猜测然后高斯消元”算法需要3.12天，在8个GPU上使用神经网络算法只需要66分钟。 |
| [^26] | [A Theory of Emergent In-Context Learning as Implicit Structure Induction.](http://arxiv.org/abs/2303.07971) | 本文推导了一个信息理论界限，展示了在自然语言数据具有足够的组成结构的情况下，从一般的下一个标记预测中获得上下文学习能力。为验证理论预测，本文引入了一个受控制的设置来诱导上下文学习，证明了经过训练的Transformer可以为一系列任务执行上下文学习。 |
| [^27] | [On the Connection between Concept Drift and Uncertainty in Industrial Artificial Intelligence.](http://arxiv.org/abs/2303.07940) | 本文探索了工业人工智能中概念漂移和不确定性之间的联系，提出了一种方法来估计模型预测的置信度，并且证明了这种方法可以有效地在无监督的情况下检测概念漂移，从而提高模型的整体性能。 |
| [^28] | [Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament.](http://arxiv.org/abs/2303.07925) | 本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。 |
| [^29] | [Improving Accented Speech Recognition with Multi-Domain Training.](http://arxiv.org/abs/2303.07924) | 本文介绍了使用四种不同法语口音来创建微调数据集，以提高预训练ASR模型对口音的识别能力，并成功将错误率在非洲和比利时口音上降低高达25%。 |
| [^30] | [Text-to-image Diffusion Model in Generative AI: A Survey.](http://arxiv.org/abs/2303.07909) | 本文调查了文本到图像扩散模型以及相关应用，总结了最先进的方法，并探讨了挑战和未来方向。 |
| [^31] | [Generalised Scale-Space Properties for Probabilistic Diffusion Models.](http://arxiv.org/abs/2303.07900) | 本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。 |
| [^32] | [Bayes Complexity of Learners vs Overfitting.](http://arxiv.org/abs/2303.07874) | 本文引入了新的函数复杂度概念，它能够支配PAC Bayes一样的概化界限，关联神经网络的自然函数复杂度概念，并解释神经网络与线性模型之间的泛化差距。此外，我们的概念能够自然地推广到有多层的神经网络，并且可导出上界，在更高层以及拥有结构化函数的情况下也是如此。 |
| [^33] | [DualMix: Unleashing the Potential of Data Augmentation for Online Class-Incremental Learning.](http://arxiv.org/abs/2303.07864) | 本文研究了在在线类增量学习(OCL)中如何利用数据增强(DA)来防止灾难性遗忘(CF)的问题。通过提出增强的Mixup(EnMix)方法，同时混合增强样本和其标签，有效提高了OCI性能并防止CF问题。 |
| [^34] | [BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised Semantic Segmentation of Medical Images.](http://arxiv.org/abs/2303.07853) | BoundaryCAM提出了一种基于边界的弱监督的优化框架，能够预测对象位置，实现精细的高精度分割掩模。 |
| [^35] | [FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features.](http://arxiv.org/abs/2303.07852) | 该论文提出了一个新的胎儿模型超声数据集FPUS23，用于确定胎儿方向，胎位和解剖学特征。研究结果表明，FPUS23可以为临床超声监测工作流程带来改善，以及可能开发一个家庭使用的基于超声的胎儿监护平台。 |
| [^36] | [Transfer Learning for Real-time Deployment of a Screening Tool for Depression Detection Using Actigraphy.](http://arxiv.org/abs/2303.07847) | 研究提出了一种迁移学习方法，使用次要数据集的模型进行训练，并在实时部署基于加速计数据的抑郁症筛查工具时使用。这种方法可以在有限的原始数据样本下建立机器学习模型，具有较高的准确率。 |
| [^37] | [Sample-efficient Adversarial Imitation Learning.](http://arxiv.org/abs/2303.07846) | 本研究提出了一种利用自监督表示来增强样本效率的对抗性模仿学习方法，从而学习不受扭曲影响的状态和动作表示以建立非图像控制任务的预测表征。 |
| [^38] | [Kinematic Data-Based Action Segmentation for Surgical Applications.](http://arxiv.org/abs/2303.07814) | 本文提出了两种多阶段体系结构和两种数据增强技术，专门用于基于运动学数据的行动分割。同时，作者在三个手术缝合任务数据集上对模型进行了评估。 |
| [^39] | [ICICLE: Interpretable Class Incremental Continual Learning.](http://arxiv.org/abs/2303.07811) | ICICLE提出了一种基于样本的可解释的类增量连续学习方法，通过采用原型部分化方法来解决解释性概念漂移的问题，实验结果表明其在不需要样本的情况下表现优于现有的方法。 |
| [^40] | [Testing Causality for High Dimensional Data.](http://arxiv.org/abs/2303.07774) | 本文提出了一种因果关系检验方法，用于确定高维数据之间的因果关系。此方法在现有技术上进行改进，并扩展了到非线性迹函数。作者提出了一个新颖的岭正则化估计器的变体，并给出了可证的边界。 |
| [^41] | [DBSCAN of Multi-Slice Clustering for three-order Tensor.](http://arxiv.org/abs/2303.07768) | 本文提出了 MSC-DBSCAN扩展算法，可以在三元聚类中从数据中提取不同子空间的不同切片聚类，并可以获得与 MSC 算法在处理秩一张量数据时相同的解决方案。 |
| [^42] | [Traffic4cast at NeurIPS 2022 -- Predict Dynamics along Graph Edges from Sparse Node Data: Whole City Traffic and ETA from Stationary Vehicle Detectors.](http://arxiv.org/abs/2303.07758) | NeurIPS 2022的Traffic4cast竞赛要求参与者利用稀疏节点数据预测整个道路图的动态未来交通状态，并将GPS数据的速度级别转化为三个拥堵等级的概率。 |
| [^43] | [Multiway clustering of 3-order tensor via affinity matrix.](http://arxiv.org/abs/2303.07757) | 本文提出了一种基于相似矩阵的三阶张量多路聚类方法(MCAM)，通过构建相似矩阵并应用先进聚类方法最终实现了多路聚类分析。与其他已知算法相比，MCAM具有竞争优势。 |
| [^44] | [ForDigitStress: A multi-modal stress dataset employing a digital job interview scenario.](http://arxiv.org/abs/2303.07742) | 该论文介绍了一个使用数字化面试情境来诱发压力并提供多模态数据、连续标注和基准分类器的压力数据集。最佳表现分类器的准确率和F1分数分别为88.3%和87.5%。 |
| [^45] | [Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models.](http://arxiv.org/abs/2303.07735) | 本文调查了最近的文献，发现即使最先进的深度学习模型在面对基本数值和算术知识的相对简单任务时也经常无法胜任。 |
| [^46] | [Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme Conversion.](http://arxiv.org/abs/2303.07726) | Reinforcer模型使用邻近字符关系强化语言模型，解决了中文G2P中语言模型编码句子过于普遍和词边界分割不一致性的问题，获得了最先进的性能。 |
| [^47] | [DisCoHead: Audio-and-Video-Driven Talking Head Generation by Disentangled Control of Head Pose and Facial Expressions.](http://arxiv.org/abs/2303.07697) | DisCoHead是一种新颖的无监督音视频生成谈话头方法，能够分离和控制头部姿势和面部表情，通过使用单一的几何变换和神经混合方法，并将密集运动估计器和生成器的编码器集成在一起，使得生成的头部运动具有更高的质量和同步精度。 |
| [^48] | [Adaptive Policy Learning for Offline-to-Online Reinforcement Learning.](http://arxiv.org/abs/2303.07693) | 本文提出了一种自适应策略学习框架，以有效地利用离线和在线数据，实现了离线到在线强化学习的最佳效果。 |
| [^49] | [FPTN: Fast Pure Transformer Network for Traffic Flow Forecasting.](http://arxiv.org/abs/2303.07685) | 本文提出了一种快速纯Transformer网络（FPTN），将交通流量数据沿传感器维度而非时间维度划分为序列，提出了三种嵌入方式将这些向量投影到适当的向量空间中，然后利用Transformer的多头注意机制捕获复杂时空相关性，使用全连接层输出预测的交通流量，FPTN不仅实现了最先进的准确性，而且运行速度比其他基于Transformer的模型快几倍。 |
| [^50] | [Feature representations useful for predicting image memorability.](http://arxiv.org/abs/2303.07679) | 本研究使用Brain-Score评估64个CNN模型中与图像记忆力有关的特征表示，并发现高记忆力预测准确性的层与颞下皮质（IT）的脑部相似性更高。 |
| [^51] | [Sinkhorn-Flow: Predicting Probability Mass Flow in Dynamical Systems Using Optimal Transport.](http://arxiv.org/abs/2303.07675) | 本文提出了一种使用最优输运预测动态系统中的概率质量流量的新方法，该方法能够显著改善社交网络设置中社区演变预测任务的结果。 |
| [^52] | [Koos Classification of Vestibular Schwannoma via Image Translation-Based Unsupervised Cross-Modality Domain Adaptation.](http://arxiv.org/abs/2303.07674) | 该论文提出了一种基于图像翻译的无监督跨模态域自适应方法，可以将ceT1扫描转换为hrT2扫描，来进行未标记的hrT2扫描的Koos分类，避免了无注释hrT2扫描不准确的问题，具有更好的性能。 |
| [^53] | [AutoTransfer: AutoML with Knowledge Transfer -- An Application to Graph Neural Networks.](http://arxiv.org/abs/2303.07669) | AutoTransfer是一种带有知识转移的AutoML解决方案，可以将先前的GNN架构设计知识转移到新的任务中，通过任务嵌入和任务模型库来寻找理想模型的设计先验，以提高搜索效率。 |
| [^54] | [Relational Multi-Task Learning: Modeling Relations between Data and Tasks.](http://arxiv.org/abs/2303.07666) | 本文介绍了一种新颖的关系多任务学习设置，通过构建一个将数据点和任务连接起来的知识图，利用来自辅助任务的数据点标签来对新任务进行更准确的预测，并在各种数据集上显著优于最先进的多任务学习方法。 |
| [^55] | [Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review.](http://arxiv.org/abs/2303.07647) | 本文综述了近年来机器学习在实验固体力学领域中的最新进展和应用，提供了物理感知和基于物理学的机器学习方法，并涵盖了断裂力学、生物力学、纳米和微观力学、构建材料和二维材料等传统和新兴领域。 |
| [^56] | [Clustering with Simplicial Complexes.](http://arxiv.org/abs/2303.07646) | 本文提出了一种使用单纯复合体进行聚类的方法，利用二阶单纯复合体来捕捉节点间的关系以实现更高级别的网络交互，并提出了基于Cheeger不等式的单纯光谱聚类算法。 |
| [^57] | [Best arm identification in rare events.](http://arxiv.org/abs/2303.07627) | 本文提出了解决稀有事件中最佳臂识别问题的算法，可以更快地提供正确臂的选择，同时略微增加样本复杂性。 |
| [^58] | [RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback.](http://arxiv.org/abs/2303.07622) | RE-MOVE提出了一种基于语言反馈的自适应策略设计方法，可以使机器人适应实时环境变化，并从人类反馈中学习并适应之前未见过的对抗性场景。 |
| [^59] | [On the Implicit Geometry of Cross-Entropy Parameterizations for Label-Imbalanced Data.](http://arxiv.org/abs/2303.07608) | 本文探究了在标签不平衡数据上训练深度神经网络的CE损失函数参数化方法，结果得出了可以使模型偏向于少数类的隐式偏差理论并推导出了分类器和嵌入的闭式公式。 |
| [^60] | [Forecasting COVID-19 Infections in Gulf Cooperation Council (GCC) Countries using Machine Learning.](http://arxiv.org/abs/2303.07600) | 本论文利用机器学习时间序列模型针对海湾合作委员会国家的COVID-19疫情数据进行分析，实验结果表明所开发的模型可以高精度地对COVID-19感染情况进行预测。 |
| [^61] | [A Contrastive Knowledge Transfer Framework for Model Compression and Transfer Learning.](http://arxiv.org/abs/2303.07599) | 本论文提出了一个新的对比知识迁移框架（CKTF），通过优化教师和学生之间的中间表示的多个对比目标，使得CKTF能够传递足够的结构化知识，并提高了现有KT技术的性能。 |
| [^62] | [AdPE: Adversarial Positional Embeddings for Pretraining Vision Transformers via MAE+.](http://arxiv.org/abs/2303.07598) | AdPE方法通过对抗位置嵌入，扭曲局部结构，强制Transformer编码器在全局上下文中学习更具有差别性的特征，从而提高泛化能力。 |
| [^63] | [Fast Regularized Discrete Optimal Transport with Group-Sparse Regularizers.](http://arxiv.org/abs/2303.07597) | 本论文提出了一种名为Fast GS-DOT的新算法，通过利用梯度矩阵的块对角结构，可以在保持传统方法相同精度的同时显著降低计算时间，有效处理了正则化离散最优传输中类别标签过多的问题。 |
| [^64] | [Sequential three-way decisions with a single hidden layer feedforward neural network.](http://arxiv.org/abs/2303.07589) | 这篇论文提出了一种基于连续三元决策的单隐藏层前馈神经网络（STWD-SFNN）模型，通过动态学习隐藏层节点数量和连续调整阈值参数，实现了对结构化数据集更高效的处理。 |
| [^65] | [Teacher-Student Knowledge Distillation for Radar Perception on Embedded Accelerators.](http://arxiv.org/abs/2303.07586) | 本文提出一种基于师生知识蒸馏的方法，用于低级别雷达感知任务，并成功实现嵌入式计算的实时部署，速度达到教师模型的100倍。 |
| [^66] | [Sensitive Region-based Metamorphic Testing Framework using Explainable AI.](http://arxiv.org/abs/2303.07580) | 提出了一种基于敏感区域的元测试框架，可以通过转换这些区域来有效地检测易出现错误分类的图像；敏感区域可以由可解释AI指定。 |
| [^67] | [VANI: Very-lightweight Accent-controllable TTS for Native and Non-native speakers with Identity Preservation.](http://arxiv.org/abs/2303.07578) | VANI是一种轻量级的多语言口音可控制语音合成系统，支持对语音的口音、语言、说话者、F0和能量特征进行显式控制，并能在保留本地口音的情况下更换说话者语言。 |
| [^68] | [Machine Learning Computer Vision Applications for Spatial AI Object Recognition in Orange County, California.](http://arxiv.org/abs/2303.07560) | 本论文介绍了一种利用机器学习和计算机视觉算法进行空间物体识别和位置检测的方法，并在加州橙县实现了这一方法，并成功地进行了对停车标志和消防栓的识别。 |
| [^69] | [Lifelong Learning for Anomaly Detection: New Challenges, Perspectives, and Insights.](http://arxiv.org/abs/2303.07557) | 本文探讨了终身异常检测的重要性，提出设计终身学习复杂性的异常检测方法的挑战和机会，并提供了一种场景生成过程使得研究人员能够进行实验。 |
| [^70] | [Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies.](http://arxiv.org/abs/2303.07551) | 本文提出通过在权重空间中合并训练于不同 MuJoCo 运动问题上的 Decision Transformer 的子集，形成多任务模型。通过共享一些辅助任务的训练以及共同使用预训练初始化，能够获得更好的结果。这个方向的研究有助于使代理的过程民主化和分发。 |
| [^71] | [Constrained Adversarial Learning and its applicability to Automated Software Testing: a systematic review.](http://arxiv.org/abs/2303.07546) | 本综述研究了受限对抗学习方法和自动化软件测试中受限数据生成方法的最新技术应用，探讨将这些方法整合至测试工具中以提高数字系统的鲁棒性和弹性。 |
| [^72] | [WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis.](http://arxiv.org/abs/2303.07543) | 本论文提出了一种名为WDiscOOD的新型OOD检测方法，其中使用白化线性判别分析将特征投影到判别子空间和残留子空间中，确定OOD分数。在大规模ImageNet-1k基准测试和六个OOD数据集中，WDiscOOD表现出了优越的性能。 |
| [^73] | [Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI.](http://arxiv.org/abs/2303.07540) | 提出了一种基于张量学习的流程，从多模态心脏磁共振成像（MRI）中识别肺动脉楔压力（PAWP）。通过整合多种特征信息，提高了识别准确度。 |
| [^74] | [HiSSNet: Sound Event Detection and Speaker Identification via Hierarchical Prototypical Networks for Low-Resource Headphones.](http://arxiv.org/abs/2303.07538) | 本研究介绍了一种名为HiSSNet的层次原型网络，针对低资源头戴式耳机的声音事件检测和说话人识别进行优化，使其可以根据用户的隐式和显式交互学习特定的重要声音。 |
| [^75] | [Fractional dynamics foster deep learning of COPD stage prediction.](http://arxiv.org/abs/2303.07537) | 本文利用分数阶动力学分析来进行COPD的诊断，可以从生理信号中提取不同阶段的特征信号，进而发展出可预测COPD分级的深度神经网络。 |
| [^76] | [Path Planning using Reinforcement Learning: A Policy Iteration Approach.](http://arxiv.org/abs/2303.07535) | 本研究提出了一种基于自动调整器的序数回归方法，以加速探索强化学习算法的参数，并加速收敛到最优策略。该方法可以提供1.82倍的峰值加速和1.48倍的平均加速比。 |
| [^77] | [Domain Generalization via Nuclear Norm Regularization.](http://arxiv.org/abs/2303.07527) | 本文提出了一种基于核范数正则化的通用性正则化方法，能够降低环境特征的影响并鼓励学习领域不变的特征，能够在广泛的领域通用性任务中获得比基线更强的性能。 |
| [^78] | [Automated Vulnerability Detection in Source Code Using Quantum Natural Language Processing.](http://arxiv.org/abs/2303.07525) | 该研究使用基于深度神经网络模型和量子机器学习模型的漏洞检测方法，利用庞大的开源函数数据集来自动检测软件源代码中的漏洞。 |
| [^79] | [Audio Visual Language Maps for Robot Navigation.](http://arxiv.org/abs/2303.07522) | 该论文提出了一种音视语言地图(AVLMaps)，用于存储跨模态信息，实现机器人根据多模态查询在地图中索引目标的导航方式。在模拟实验中，AVLMaps实现了从多模态提示的零次学习式多模态目标导航，并提供了更好的召回率。 |
| [^80] | [SuperMask: Generating High-resolution object masks from multi-view, unaligned low-resolution MRIs.](http://arxiv.org/abs/2303.07517) | 本论文提出了一种基于弱监督深度学习的方法，从多个低分辨率图像中生成高分辨率的目标掩模。该方法结合了分割和无监督注册网络，引入两个新的规范化方法使得配准和分割相互增强，并提出了多视角融合方法。实验结果表明，该方法在合成和真实数据集上均达到了最先进的性能。 |
| [^81] | [Loss of Plasticity in Continual Deep Reinforcement Learning.](http://arxiv.org/abs/2303.07507) | 本文研究了在连续变化的环境中，深度强化学习代理程序在执行一系列游戏时失去可塑性。研究发现网络的激活足迹变得稀疏导致梯度变小。 |
| [^82] | [Meta-learning approaches for few-shot learning: A survey of recent advances.](http://arxiv.org/abs/2303.07502) | 本文综述了元学习在小样本学习中的应用，调查了最先进的方法，并讨论了当前挑战和未来研究方向。 |
| [^83] | [Using VAEs to Learn Latent Variables: Observations on Applications in cryo-EM.](http://arxiv.org/abs/2303.07487) | 本研究通过定性分析，发现VAE在生物应用中摊销潜在变量的特性与传统显式表示方法相似。 |
| [^84] | [Guided Speech Enhancement Network.](http://arxiv.org/abs/2303.07486) | 本文提出一种引导式语音增强网络，将原始麦克风与波束形成器的输出均作为机器学习模型的输入，通过训练从波束形成器的提示中学习，提高了模型的定向拒绝能力，同时具有降噪和去混响等通用任务。 |
| [^85] | [General Loss Functions Lead to (Approximate) Interpolation in High Dimensions.](http://arxiv.org/abs/2303.07475) | 本研究提供了一般凸损失函数和超参数化阶段下梯度下降的隐含偏差的近似表征。具体而言是近似最小范数插值。 |
| [^86] | [X-Former: In-Memory Acceleration of Transformers.](http://arxiv.org/abs/2303.07470) | X-Former是一种内存加速器，它利用注意矩阵的稀疏性，显著加速Transformer计算，同时保持高精度。 |
| [^87] | [Network Anomaly Detection Using Federated Learning.](http://arxiv.org/abs/2303.07452) | 本文使用联邦学习的方法解决了网络异常检测中的可扩展性和隐私保护问题，并提出了一个可在低端到中端设备上使用的深度神经网络框架，实现了最先进的准确性和隐私保护。 |
| [^88] | [Blind Acoustic Room Parameter Estimation Using Phase Features.](http://arxiv.org/abs/2303.07449) | 本研究提出了一种利用与相位相关的特征进行盲目估计所谓的“混响指纹”参数的方法，该方法优于仅使用幅度谱特征的方法。 |
| [^89] | [Unsupervised Representation Learning in Partially Observable Atari Games.](http://arxiv.org/abs/2303.07437) | 本文提出了一种针对部分可观测状态的无监督状态表示学习方法PO-ST-DIM，改进了ST-DIM的对比方法，并在Atari游戏中取得了与监督方法相当的表现。 |
| [^90] | [Study on the Data Storage Technology of Mini-Airborne Radar Based on Machine Learning.](http://arxiv.org/abs/2303.07407) | 提出了一种基于机器学习的小型机载雷达数据存储方法，可以显著降低文件管理时间，提高存储速度。 |
| [^91] | [Tuning support vector machines and boosted trees using optimization algorithms.](http://arxiv.org/abs/2303.07400) | 该论文研究了支持向量机和提升树的参数调优行为，并开发了一个R包EZtune自动调试模型。 |
| [^92] | [Fast exploration and learning of latent graphs with aliased observations.](http://arxiv.org/abs/2303.07397) | 本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。 |
| [^93] | [Many learning agents interacting with an agent-based market model.](http://arxiv.org/abs/2303.07393) | 本论文介绍了多个强化学习最优执行交易智能体与反应式基于智能体的金融市场模型的交互。通过平衡执行差价和未能及时执行订单的惩罚，说明了奖励函数的作用。研究表明，学习智能体的数量、初始订单大小和状态空间的变化，会对最小智能市场模拟造成不同的影响。 |
| [^94] | [Efficient Bayesian Physics Informed Neural Networks for Inverse Problems via Ensemble Kalman Inversion.](http://arxiv.org/abs/2303.07392) | 本文提出了一种使用集合卡尔曼反演实现高效贝叶斯物理感知神经网络求解反问题的新推断算法，并且在提高计算效率的同时，实现了数据具有信息量的不确定性估计。 |
| [^95] | [Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks.](http://arxiv.org/abs/2303.07200) | 本文提出了一种使用神经元进化的稀疏神经网络方法，名为 NeuroFS，能够有效地派生出信息丰富的特征子集，且在实验中具有最高排名得分。 |
| [^96] | [Improving physics-informed neural networks with meta-learned optimization.](http://arxiv.org/abs/2303.07127) | 用元学习优化方法训练物理知识注入神经网络，可以显著减少网络求解微分方程系统的误差，并且具有很强的迁移学习能力。 |
| [^97] | [Don't PANIC: Prototypical Additive Neural Network for Interpretable Classification of Alzheimer's Disease.](http://arxiv.org/abs/2303.07125) | 本研究提出了PANIC，一个基于原型加性神经网络的可解释AD分类模型，整合了3D图像和表格数据并且表现出最先进的性能，提供了直接的局部和全局解释，并提取出了有生物意义的AD特征。 |
| [^98] | [Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference.](http://arxiv.org/abs/2303.07122) | 该研究提出了一种基于循环神经网络的时间序列因果推断模型TCINet，用于推断大气过程对海冰融化的因果效应。通过实验证明，该模型能够显著提高量化北极海冰融化的主要原因的能力。 |
| [^99] | [Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation.](http://arxiv.org/abs/2303.06965) | 本文提出了Uni-RXN框架，在化学反应Pretraining和分子生成任务中都取得了最先进的结果。通过具备化学知识，克服了当前分子生成模型仅依赖少量反应模板的限制，生成质量高、可合成的药物类分子结构。 |
| [^100] | [Physics-driven machine learning models coupling PyTorch and Firedrake.](http://arxiv.org/abs/2303.06871) | 本论文介绍了一种基于物理学的机器学习技术，结合PyTorch和Firedrake框架，可用于较少的训练数据中实现对复杂物理系统的建模。 |
| [^101] | [Label Information Bottleneck for Label Enhancement.](http://arxiv.org/abs/2303.06836) | 本文提出了标签信息瓶颈方法用于标签增强，通过学习关键标签相关信息，恢复标签分布，提高了恢复效果。 |
| [^102] | [Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning.](http://arxiv.org/abs/2303.06710) | 该论文提出了一种基于强化学习的半自主代理机器人方法，在对任务成功结果的信心低时请求外部帮助，有效降低专家调用数量。 |
| [^103] | [CoNIC Challenge: Pushing the Frontiers of Nuclear Detection, Segmentation, Classification and Counting.](http://arxiv.org/abs/2303.06274) | CoNIC挑战使用最大的数据集评估核分割和细胞组成，刺激了可重复的细胞识别算法的开发，发现嗜酸性粒细胞和中性粒细胞在肿瘤中发挥重要作用。 |
| [^104] | [Machine learning for sports betting: should forecasting models be optimised for accuracy or calibration?.](http://arxiv.org/abs/2303.06021) | 该论文研究了机器学习在体育博彩中的应用，提出了优化预测模型校准性比准确度更重要的假设，并通过实验证明了此假设的正确性。 |
| [^105] | [Hardware Acceleration of Neural Graphics.](http://arxiv.org/abs/2303.05735) | 本文研究了神经图形是否需要硬件支持，发现当前GPU性能无法满足对4K分辨率60FPS渲染的需求，且在增强现实/虚拟现实应用中性能缺口更大。作者确定输入编码和MLP内核是性能瓶颈。 |
| [^106] | [Towards better traffic volume estimation: Tackling both underdetermined and non-equilibrium problems via a correlation-adaptive graph convolution network.](http://arxiv.org/abs/2303.05660) | 本研究提出基于图卷积网络的方法，解决交通量估计中的不确定和非平衡问题，实现准确的全面交通量估计。 |
| [^107] | [Variance-aware robust reinforcement learning with linear function approximation under heavy-tailed rewards.](http://arxiv.org/abs/2303.05606) | 本文提出了AdaOFUL和VARA两种算法，用于在存在有限方差的重尾奖励情况下进行在线顺序决策，其中AdaOFUL具有状态-of-the-art的遗憾界，VARA达到了更紧密的方差感知遗憾界。 |
| [^108] | [Neural Probabilistic Logic Programming in Discrete-Continuous Domains.](http://arxiv.org/abs/2303.04660) | 介绍了一种名为DeepSeaProbLog的神经概率逻辑编程语言，将深度概率编程技术纳入其中，支持在逻辑约束条件下推断和学习离散和连续概率分布，并通过实验证明其优势。 |
| [^109] | [Predicted Embedding Power Regression for Large-Scale Out-of-Distribution Detection.](http://arxiv.org/abs/2303.04115) | 本文提出一种基于标签分布学习的预测类标签概率方法，用于大规模外域检测，相比现有最先进方法在 AUROC 和 AUPR 方面实现统计显着的改进。 |
| [^110] | [Masked Images Are Counterfactual Samples for Robust Fine-tuning.](http://arxiv.org/abs/2303.03052) | 本文提出了一种新颖的深度学习模型微调方法，利用掩蔽图像作为反事实样本，提高模型的鲁棒性。 |
| [^111] | [High-dimensional analysis of double descent for linear regression with random projections.](http://arxiv.org/abs/2303.01372) | 本文使用随机矩阵理论对基于随机投影的线性回归问题进行高维分析，证明了固定预测问题的双下降曲线现象。 |
| [^112] | [Interpretable Water Level Forecaster with Spatiotemporal Causal Attention Mechanisms.](http://arxiv.org/abs/2303.00515) | 本研究提出一种基于时空因果关系的新型水位预测模型，通过将因果结构形式化为多层网络和使用蒙版方法，提高了其可解释性，运用于汉江数据集的实际分析中表现优异。 |
| [^113] | [TimeMAE: Self-Supervised Representations of Time Series with Decoupled Masked Autoencoders.](http://arxiv.org/abs/2303.00320) | TimeMAE是一种新型自监督模型，利用transformer网络将每个时间序列处理成一系列不重叠的子序列，并通过随机掩码策略覆盖本地化子序列的语义单元，以学习到丰富的上下文信息和可传递的时间序列表示。 |
| [^114] | [Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting.](http://arxiv.org/abs/2302.14829) | Dish-TS是一种通用的神经网络模型，用于缓解时间序列预测中的分布偏移。该模型通过引入系数网络（CONET）来更好地估计分布。在时间序列预测任务中，将Lookback窗口作为输入空间，Horizon窗口作为输出空间，将分布偏移总结为内部空间偏移和不同空间偏移两类。 |
| [^115] | [A survey on online active learning.](http://arxiv.org/abs/2302.08893) | 在线主动学习是一种机器学习范式，旨在从数据流中选择最具信息量的数据点进行标注。本文综述了在线主动学习的最新进展、基于流的主动学习的主要挑战和机遇、用于选择信息样本的策略以及该范式中不同的评估指标。 |
| [^116] | [OpenHLS: High-Level Synthesis for Low-Latency Deep Neural Networks for Experimental Science.](http://arxiv.org/abs/2302.06751) | OpenHLS是一个基于高级综合技术的开源编译器框架，将深度神经网络的高级表示转换为适用于近传感器设备的低级表示，解决了实验科学领域数据采集系统中低延迟处理问题。 |
| [^117] | [Statistical Complexity and Optimal Algorithms for Non-linear Ridge Bandits.](http://arxiv.org/abs/2302.06025) | 本文探讨了非线性Ridge Bandits中独特的学习现象，推导出了最优烧录成本的上下限和整个烧录期间的学习轨迹的统计算法，并证明了UCB和基于回归神经元的算法都是次优解。 |
| [^118] | [Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction.](http://arxiv.org/abs/2302.02601) | 本文提出了一种基于双层知识图谱的方法来学习嵌入，将三元组之间的关系考虑进去，并使用数据增强策略来增加合理的三元组。 |
| [^119] | [CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification.](http://arxiv.org/abs/2302.02314) | 本研究提出了一种新的分类网络CECT，利用可控的卷积神经网络和Transformer进行组合，能同时捕捉多个局部和全局尺度上的特征。在两个公共COVID-19数据集上评估后表现优于现有的最先进方法。这一新方法在医学图像分类领域中有广泛的应用前景。 |
| [^120] | [Zero-One Laws of Graph Neural Networks.](http://arxiv.org/abs/2301.13060) | 本文提出了一个新的理论研究视角，回答了当图节点数量变得非常大时GNN的行为如何的问题。通过证明不断增大的图映射到GNN分类器的特定输出的概率趋于零或一，建立了这些GNN的零一定律，限制了它们的能力。实验证实了理论结论。 |
| [^121] | [SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations.](http://arxiv.org/abs/2301.07074) | SegViz是一种基于联邦学习的框架，用于从分布式的非i.i.d数据集中训练具有部分注释的分割模型。使用FedBN作为聚合策略的SegViz框架在外部BTCV集上表现出优异的性能，分割的dice分数分别为0.93、0.83、0.55和0.75。 |
| [^122] | [WuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning.](http://arxiv.org/abs/2301.04488) | WuYun是一种基于知识增强的深度学习算法，它将旋律分解成骨架和动态装饰音符两部分，并通过音乐领域知识提取旋律骨架，提供辅助指导来生成具有更好长期结构和音乐性的旋律。 |
| [^123] | [Pseudo-Inverted Bottleneck Convolution for DARTS Search Space.](http://arxiv.org/abs/2301.01286) | 本文增加了ConvNeXt的微小设计变化来扩充DARTS搜索空间，提出了PIBConv块来减少计算占用，我们的架构在层数仅为2时优于一个具有类似规模的DARTS网络。 |
| [^124] | [TriNet: stabilizing self-supervised learning from complete or slow collapse on ASR.](http://arxiv.org/abs/2301.00656) | 本文提出的TriNet采用三分支结构，可防止自监督学习在ASR中的崩溃，并在下游ASR任务中比SOTA方法Data2vec实现了6.06%的相对单词错误率降低（WERR）。 |
| [^125] | [A Concept Knowledge Graph for User Next Intent Prediction at Alipay.](http://arxiv.org/abs/2301.00503) | 本文提出了一种基于概念知识图谱的用户下一步意图预测技术，实现了在支付宝网络平台上对1亿活跃用户的服务，并且在保持可解释性的情况下，有效地提高了下游任务的性能表现。 |
| [^126] | [Skew Class-balanced Re-weighting for Unbiased Scene Graph Generation.](http://arxiv.org/abs/2301.00351) | 本文提出了一种用于场景图生成(SGG)任务的算法，考虑了长尾分布对无偏谓词预测的影响，并且采用偏斜类平衡重加权(SCR)损失函数来更好地权衡大多数谓词和少数谓词。实验结果表明SCR方法超越了现有最先进的方法。 |
| [^127] | [SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images.](http://arxiv.org/abs/2212.09100) | 提出了一个大规模的 ShapeNet 合成数据集 SPARF，包括超过 100 万个有多个体素分辨率的 3D 优化的辐射场，用于新视角合成。同时提出了一种新颖的管线 SuRFNet，通过学习少量视图生成稀疏体素辐射场。 |
| [^128] | [Investigating Deep Learning Model Calibration for Classification Problems in Mechanics.](http://arxiv.org/abs/2212.00881) | 本研究通过对7个开放数据集的研究发现，在工程力学领域应用深度学习模型时，需要仔细考虑模型校准问题，以提高预测准确性。 |
| [^129] | [Component Segmentation of Engineering Drawings Using Graph Convolutional Networks.](http://arxiv.org/abs/2212.00290) | 提出了一种基于图卷积网络的工程图纸组件分割方法，能够自动完成2D工程零件图纸的矢量化和机器解读，并在基准数据集上取得最先进的组件分割准确性结果。 |
| [^130] | [Generalization of generative model for neuronal ensemble inference method.](http://arxiv.org/abs/2211.05634) | 提出了基于泛化的神经元集合推理方法，解决了贝叶斯推断模型中神经元活动非平稳性的问题。 |
| [^131] | [Music Mixing Style Transfer: A Contrastive Learning Approach to Disentangle Audio Effects.](http://arxiv.org/abs/2211.02247) | 本论文提出了一种对比学习方法，用于实现音乐混音风格转换。该方法采用已处理的数据集进行自监督训练，并使用编码器从参考歌曲中提取仅与音频效果相关的信息。系统实现了多轨音频的混音风格转换，并且在使用音乐源分离模型时具有鲁棒性。 |
| [^132] | [Learning Hypergraphs From Signals With Dual Smoothness Prior.](http://arxiv.org/abs/2211.01717) | 本研究提出了一种基于双重平滑先验的超图结构学习框架，可从观察到的信号中学习超图结构以捕获实体间的内在高阶关系。 |
| [^133] | [eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers.](http://arxiv.org/abs/2211.01324) | 本文提出一种使用专家去噪模型集合的文本到图像扩散模型。文本-图像合成过程中的生成是一个渐进的过程，而在生成的不同阶段，它的合成行为会有所不同。因此本文提出针对不同阶段的专门模型的构想。 |
| [^134] | [Leveraging Demonstrations with Latent Space Priors.](http://arxiv.org/abs/2210.14685) | 本文提出了一种方法，通过结合技能学习和序列建模，利用演示数据集中的潜在空间先验知识来加速强化学习中高层次策略的学习，并在实验中证实了该方法的有效性。 |
| [^135] | [Provably Safe Reinforcement Learning via Action Projection using Reachability Analysis and Polynomial Zonotopes.](http://arxiv.org/abs/2210.10691) | 本研究提出了通过可达性分析和多项式Zonotopes实现行动投影的可证明安全强化学习，该方法通过混合整数优化实现，并能够有效地处理输入限制和动态障碍物。 |
| [^136] | [Training set cleansing of backdoor poisoning by self-supervised representation learning.](http://arxiv.org/abs/2210.10272) | 本文使用自监督表示学习清洁后门攻击所传染的神经网络模型的训练数据集。该方法通过学习样本嵌入表示来识别被毒化的和干净的样本，无需知道攻击者使用的后门触发器。在不同数据集的实验中，我们的方法优于现有方法。 |
| [^137] | [Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP.](http://arxiv.org/abs/2210.04150) | 本文提出了一种基于CLIP和带掩膜的体系结构的开放词汇语义分割方法，通过在用于训练的嘈杂但多样化的数据集上对CLIP进行微调，以提高其在带有掩膜的图像上的性能，超越了当前最佳的方法。 |
| [^138] | [Bayesian Prompt Learning for Image-Language Model Generalization.](http://arxiv.org/abs/2210.02390) | 本文提出了一种基于贝叶斯方法的提示学习框架，对提示空间进行正则化，提高了对未见提示的泛化能力。 |
| [^139] | [Smooth image-to-image translations with latent space interpolations.](http://arxiv.org/abs/2210.00841) | 该论文提出了两种正则化方法——收缩损失和Mixup数据增强策略，用于解决图像跨域插值中出现的平滑性问题；同时引入了一种新的度量标准来定量评估插值平滑度。 |
| [^140] | [Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization.](http://arxiv.org/abs/2209.15382) | 本文分析了无限时间折扣马尔科夫决策过程中，使用对数线性策略参数化的未正则化自然策略梯度算法的收敛速率，证明了一定条件下该算法具有线性收敛保证。 |
| [^141] | [Dataset Distillation Using Parameter Pruning.](http://arxiv.org/abs/2209.14609) | 本文提出了一种使用参数修剪的数据集蒸馏方法，该方法可以在蒸馏过程中修剪难以匹配的参数，提高蒸馏性能。 |
| [^142] | [Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking Neural Networks with Learnable Neuronal Dynamics.](http://arxiv.org/abs/2209.11741) | 本文提出了一种自适应脉冲神经网络框架，用于事件驱动光流估计任务，并通过神经动力学来解决脉冲消失问题。实验结果表明，该方法在各项公共基准测试中均取得了最先进的性能。 |
| [^143] | [NIERT: Accurate Numerical Interpolation through Unifying Scattered Data Representations using Transformer Encoder.](http://arxiv.org/abs/2209.09078) | NIERT是一种使用Transformer编码器的数值内插方法，利用仅编码器结构对观察点和目标点进行嵌入和统一表示，从而实现了更准确的内插和更广泛的泛化能力。 |
| [^144] | [DC-Art-GAN: Stable Procedural Content Generation using DC-GANs for Digital Art.](http://arxiv.org/abs/2209.02847) | DC-Art-GAN 使用对抗性训练的 DC-GAN 进行艺术作品的稳定生成和多样性生成，实现了从随机噪声中合成虚拟现实中不存在的逼真图像。 |
| [^145] | [A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images.](http://arxiv.org/abs/2208.14125) | 该论文介绍了一种基于扩散模型的方法，用于从2D显微图像中预测真实的3D单个细胞形状。该方法被成功应用于单个细胞分类任务。该模型学习从2D显微镜图像中重建具有逼真形态学特征的3D形状。 |
| [^146] | [Ensemble forecasts in reproducing kernel Hilbert space family: dynamical systems in Wonderland.](http://arxiv.org/abs/2207.14653) | 本文提出了一种在动力系统中的集合预测和模拟的方法，将系统嵌入再生核希尔伯特空间族，并在该空间中使用简单的集合数据同化方法进行轨迹重构。 |
| [^147] | [Training Stronger Spiking Neural Networks with Biomimetic Adaptive Internal Association Neurons.](http://arxiv.org/abs/2207.11670) | 本论文为了探索更深入的神经机制，提出了一种新颖的自适应内部关联（AIA）神经元模型，以建立先前忽视的神经元内部影响。该模型可以促进更强的联想学习，从而提高网络在各种任务上的性能，相较于现有的SNN模型。 |
| [^148] | [Training Robust Spiking Neural Networks on Neuromorphic Data with Spatiotemporal Fragments.](http://arxiv.org/abs/2207.11659) | 本文介绍了一种新颖的事件时空片段（ESTF）增强方法，可用于处理神经形态视觉数据，并提高使用SNN处理这些数据时的鲁棒性。使用ESTF的SNN在CIFAR10-DVS数据集上实现了最先进的83.9\%准确度。 |
| [^149] | [Mitigating Algorithmic Bias with Limited Annotations.](http://arxiv.org/abs/2207.10018) | 本文提出了一种名为APOD的交互式框架，用于在有限的注释预算下减少算法偏见，该框架将歧视惩罚与主动实例选择相结合，能够在公平性和准确度指标上优于传统方法。 |
| [^150] | [CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image Prior.](http://arxiv.org/abs/2207.07921) | 本论文提出了一种基于神经网络的欧拉弹性纹理图像修复算法，它结合使用深能量与深图像先验，能够更好地进行形状恢复任务，得到了比现有算法更好的修复成果。 |
| [^151] | [Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data.](http://arxiv.org/abs/2207.07506) | 本文研究了存在外部分布数据时的选择性分类问题，提出了一种新的SCOD方法，即保留softmax信息，并发现现有的检测方法评价标准需根据任务规定进行调整。 |
| [^152] | [A law of adversarial risk, interpolation, and label noise.](http://arxiv.org/abs/2207.03933) | 该研究发现，在监督学习中进行标签噪声插值可以导致对抗风险，在任何数据分布中标签噪声与对抗风险之间都存在一定关系。同时，均匀标签噪声的对抗风险与最糟的污染相差无几，并且比典型现实世界标签噪声更具危害性。进行警惕并深入研究此领域的重要性不容忽视。 |
| [^153] | [Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting Epidemics.](http://arxiv.org/abs/2206.10696) | 疫情预测至关重要，但由于传染病的非线性和非平稳特征，预测变得困难。本文介绍了一种称为Epicasting的新算法，使用集成小波神经网络（EWNet）来更准确地预测疫情。 |
| [^154] | [Nonparametric Multi-shape Modeling with Uncertainty Quantification.](http://arxiv.org/abs/2206.09127) | 该论文提出了一个多输出、多维高斯过程建模框架，解决了在多个曲线和形状相关任务上的非参数建模和不确定性量化问题，为功能对象的多级依赖关系提供了一种新的建模方式。 |
| [^155] | [Combinatorial Pure Exploration of Causal Bandits.](http://arxiv.org/abs/2206.07883) | 本文提出了用于因果模型的组合纯探索算法，其中对于二元广义线性模型，我们的算法实现了多项式样本复杂度，对于一般的图，我们的样本复杂度几乎是最优的。 |
| [^156] | [Spatial Entropy as an Inductive Bias for Vision Transformers.](http://arxiv.org/abs/2206.04636) | 本文提出在视觉Transformers中引入局部偏置的不同方法，使用一个自监督任务来鼓励空间聚类作为训练正则化。通过利用注意力映射的语义分割结构，可以有效地减少样本数量，提高性能。 |
| [^157] | [Few-Shot Unlearning by Model Inversion.](http://arxiv.org/abs/2205.15567) | 该论文提出了一个少样本去除学习的框架，通过模型反演获取训练数据代理，并根据去除学习意图进行调整。 |
| [^158] | [Relphormer: Relational Graph Transformer for Knowledge Graph Representations.](http://arxiv.org/abs/2205.10852) | Relphormer是一种新的Transformer变体，用于知识图谱表示。它引入了Triple2Seq和增强式自我注意机制，以解决基本Transformer架构在捕捉知识图谱结构和语义信息方面的不足。 |
| [^159] | [CycleSense: Detecting Near Miss Incidents in Bicycle Traffic from Mobile Motion Sensors.](http://arxiv.org/abs/2204.10416) | CycleSense是一种利用移动动作传感器检测自行车交通中的擦肩而过事件的技术，可以帮助自行车手更轻松地报告险情。这种技术的使用可以帮助城市规划者更好地了解自行车安全感问题，有望缓解城市交通问题。 |
| [^160] | [Variational Inference with Gaussian Mixture by Entropy Approximation.](http://arxiv.org/abs/2202.13059) | 论文提出了一种用高斯混合分布作为参数分布的变分推断方法，通过将高斯混合的熵近似为单峰高斯的熵之和来解决多峰性的问题，并从理论上分析近似误差。 |
| [^161] | [From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer.](http://arxiv.org/abs/2202.02113) | 本文介绍了一种将知识图谱补全转化为生成任务的方法，同时引入了关系引导演示和实体感知分层解码来实现更好的表示学习和快速推断。实验结果表明，这种方法具有比基线更好或相当的性能，并且比以往的方法更快。同时，作者还发布了一个新的大规模中文知识图谱数据集AliopenKG500。 |
| [^162] | [Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems.](http://arxiv.org/abs/2201.01680) | 本论文证明了在学习未知线性高斯系统与二次代价时，存在遗憾下限，并且这个下限的比例尺度级别为 $\sqrt{T}$。通过对控制理论参数的准确捕捉，我们证明难以控制的系统也难以学习控制。同样地，对于一类部分观察到的系统，我们的结果表明了具有较差可观测结构的系统也难以学习控制。 |
| [^163] | [One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer.](http://arxiv.org/abs/2110.10325) | 本论文提出了一种新的机器学习方法——一步式诱导式多目标学习与DiNS（OSAMTL-DiNS），以处理医学组织病理学全幻灯片图像分析中的复杂噪声标签。在乳腺癌肿瘤分割中得到了成功应用。 |
| [^164] | [Expectation Distance-based Distributional Clustering for Noise-Robustness.](http://arxiv.org/abs/2110.08871) | 本文提出了一种基于期望距离的分布聚类技术，可以降低噪声对聚类结果的影响。 |
| [^165] | [A Broad Ensemble Learning System for Drifting Stream Classification.](http://arxiv.org/abs/2110.03540) | 一种名为Broad Ensemble Learning System (BELS)的新型集成方法用于数据流分类中的概念漂移问题。相对于文献中现有的方法，它能够更加高效、稳定地更新模型，提高最佳性能模型的准确性。 |
| [^166] | [Pretrained Language Models are Symbolic Mathematics Solvers too!.](http://arxiv.org/abs/2110.03501) | 本文研究表明，大规模语言模型可以训练为序列到序列任务，解决复杂的数学方程。文章提出了一种预训练并微调Transformer模型解决符号数学任务的方法，使用的训练样本比当前深度学习技术少1.5个数量级，且在积分任务上达到了可比较的准确性。 |
| [^167] | [Incremental Class Learning using Variational Autoencoders with Similarity Learning.](http://arxiv.org/abs/2110.01303) | 本文研究了增量式学习中四种基于相似性的损失函数在灾难性遗忘方面的表现，实验结果显示不同的损失函数对灾难性遗忘率有不同的影响。 |
| [^168] | [Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale Confirmatory Item Factor Analysis.](http://arxiv.org/abs/2109.09500) | 本文研究了针对大规模验证项目因素分析的参数估计与拟合度检验方法，提出了基于深度学习的算法和扩展测试与指标，具有高效准确和有效性的特点。 |
| [^169] | [Is Nash Equilibrium Approximator Learnable?.](http://arxiv.org/abs/2108.07472) | 本文研究了生成游戏中Nash平衡的学习方法，证明了可学习性，并展示了该方法在经典求解器加速方面的应用。 |
| [^170] | [Thought Flow Nets: From Single Predictions to Trains of Model Thought.](http://arxiv.org/abs/2107.12220) | 本文探讨了给模型第二次、第三次甚至第k次思考机会的思路流网络，其利用自我校正机制和梯度更新能够纠正自身预测，该方法可显著提高模型性能。 |
| [^171] | [Learning Audio-Visual Dereverberation.](http://arxiv.org/abs/2106.07732) | VIDA是一种音视频结合的去混响方法，能够更有效地去除混响，提高语音增强、语音识别和说话者识别的性能。 SoundSpaces-Speech是一个新的大规模数据集，提供了真实世界中各种房间声学的逼真语音声学渲染。 |
| [^172] | [Dynamic Efficient Adversarial Training Guided by Gradient Magnitude.](http://arxiv.org/abs/2103.03076) | 本文提出了一种基于梯度幅度引导的动态高效对抗训练方法（DEAT），逐渐增加了对抗迭代次数，并提出了一种通用加速策略M+加速，该方法易于实现且符合大部分现有的对抗训练技术，可以显著提高深度神经网络的鲁棒性。 |
| [^173] | [Neural Network Compression for Noisy Storage Devices.](http://arxiv.org/abs/2102.07725) | 本文研究了神经网络在使用噪声存储设备时的压缩与存储问题，提出了适用于模拟存储设备的鲁棒压缩技术，相对于数字压缩方法，有效提高了NN的存储效率。 |
| [^174] | [A new Potential-Based Reward Shaping for Reinforcement Learning Agent.](http://arxiv.org/abs/1902.06239) | 本论文提出了一种新的基于历史经验的奖赏设计方法，旨在提高强化学习智能体的性能，该方法具有广泛的应用前景。 |

# 详细

[^1]: MeshDiffusion：基于分数的生成式3D网格建模

    MeshDiffusion: Score-based Generative 3D Mesh Modeling. (arXiv:2303.08133v1 [cs.GR])

    [http://arxiv.org/abs/2303.08133](http://arxiv.org/abs/2303.08133)

    本文提出了一种基于分数的生成式3D网格建模方法，依赖网格的图形结构和扩散模型，在不需要后处理的前提下，生成高质量、细节丰富的3D网格。

    

    本文研究了生成逼真的3D物体的任务，这对于自动场景生成和物理仿真等多种应用非常有用。相比于体素和点云等其他3D表示，网格在实践中更加优越，因为(1)它们可以轻松任意地操纵形状以供重新照明和仿真，(2)可以充分发挥现代图形流水线的能力，而这些流水线大多数针对网格进行了优化。以往可扩展的3D网格生成方法通常依赖于次优的后处理，并且它们往往会产生过于平滑或嘈杂的表面，缺乏精细的几何细节。为了克服这些缺点，我们利用网格的图形结构，使用简单但非常有效的生成式建模方法生成3D网格。具体来说，我们使用可变形四面体网格来表示网格，然后在这个直接参数化的网格上训练扩散模型。

    We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectivene
    
[^2]: CB2：合作自然语言交互研究平台

    CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])

    [http://arxiv.org/abs/2303.08127](http://arxiv.org/abs/2303.08127)

    CB2是一个用于研究基于任务的合作自然语言交互的平台，在3D游戏环境中提供了后端服务器和各种工具和流程。它在可扩展的研究中展示了学习的指令跟随模型。

    

    CB2 是一个多智能体平台，用于研究基于任务的情境下的合作自然语言交互。它包括一个 3D 游戏环境、一个后端服务器，可为人类智能体提供训练模型，以及各种工具和流程，以实现可扩展性的研究。我们在 https://cb2.ai 上展示了一个具有学习指令跟随模型的系统演示。

    CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
    
[^3]: 转换器在预测掩码单词时是否解析？

    Do Transformers Parse while Predicting the Masked Word?. (arXiv:2303.08117v1 [cs.CL])

    [http://arxiv.org/abs/2303.08117](http://arxiv.org/abs/2303.08117)

    本文探讨了预训练语言模型是否实际上进行解析以及为什么能捕捉解析结构，证明了类似于BERT或RoBERTa这样的掩码语言模型可以近似执行英语PCFG的Inside-Outside算法。

    

    已经证明，预训练的语言模型在使用类似于掩码语言建模这样的无监督损失函数进行训练时，可以对语言结构进行编码，例如依赖关系和组成成分分析树。但是人们对于这些模型是否实际上进行解析或仅进行与解析弱相关的一些计算存在疑问。本文在生成建模的上下文中一步步回答了上述问题，探讨了(a)是否有可能明确描述具有现实嵌入维度，头数等的转换器，能够进行解析甚至近似解析；(b)预训练模型为什么能够捕捉解析结构？我们展示了类似于BERT或RoBERTa这样的中等大小的掩码语言模型可以近似执行英语PCFG（Marcus等，1993）的Inside-Outside算法。我们还展示了，在PCFG生成语言建模损失上，Inside-Outside算法是最优的。

    Pre-trained language models have been shown to encode linguistic structures, e.g. dependency and constituency parse trees, in their embeddings while being trained on unsupervised loss functions like masked language modeling. Some doubts have been raised whether the models actually are doing parsing or only some computation weakly correlated with it. We study questions: (a) Is it possible to explicitly describe transformers with realistic embedding dimension, number of heads, etc. that are capable of doing parsing -- or even approximate parsing? (b) Why do pre-trained models capture parsing structure? This paper takes a step toward answering these questions in the context of generative modeling with PCFGs. We show that masked language models like BERT or RoBERTa of moderate sizes can approximately execute the Inside-Outside algorithm for the English PCFG [Marcus et al, 1993]. We also show that the Inside-Outside algorithm is optimal for masked language modeling loss on the PCFG-generate
    
[^4]: 基于联邦量子自然梯度下降算法的量子联邦学习优化

    Optimizing Quantum Federated Learning Based on Federated Quantum Natural Gradient Descent. (arXiv:2303.08116v1 [quant-ph])

    [http://arxiv.org/abs/2303.08116](http://arxiv.org/abs/2303.08116)

    本文提出了一种基于联邦量子自然梯度下降算法的量子联邦学习优化算法，该算法可以在减少本地量子设备之间的通信开销的同时，比传统的随机梯度下降方法更快地实现训练收敛和提高测试精度。

    

    量子联邦学习（QFL）是在多个本地量子设备之间进行的经典联邦学习模型的量子扩展。一个有效的优化算法可以最小化不同量子参与者之间的通信开销。本文提出了一种高效的优化算法，即联邦量子自然梯度下降（FQNGD），并将其应用于由基于变分量子电路（VQC）的量子神经网络（QNN）组成的QFL框架中。与Adam和Adagrad等随机梯度下降方法相比，FQNGD算法所需的训练迭代次数要少得多，可以显著减少本地量子设备之间的通信开销。我们在手写数字分类数据集上的实验证明了FQNGD对于QFL框架的有效性，具有较快的训练收敛速度和较高的测试精度。

    Quantum federated learning (QFL) is a quantum extension of the classical federated learning model across multiple local quantum devices. An efficient optimization algorithm is always expected to minimize the communication overhead among different quantum participants. In this work, we propose an efficient optimization algorithm, namely federated quantum natural gradient descent (FQNGD), and further, apply it to a QFL framework that is composed of a variational quantum circuit (VQC)-based quantum neural networks (QNN). Compared with stochastic gradient descent methods like Adam and Adagrad, the FQNGD algorithm admits much fewer training iterations for the QFL to get converged. Moreover, it can significantly reduce the total communication overhead among local quantum devices. Our experiments on a handwritten digit classification dataset justify the effectiveness of the FQNGD for the QFL framework in terms of a faster convergence rate on the training set and higher accuracy on the test se
    
[^5]: Simfluence：通过模拟训练过程建模单个训练样例的影响

    Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs. (arXiv:2303.08114v1 [cs.LG])

    [http://arxiv.org/abs/2303.08114](http://arxiv.org/abs/2303.08114)

    本文提出了一种新的训练数据归因方法Simfluence，通过建立一个训练过程模拟器，可以追溯模型在任何给定样例上的预测结果到特定的有影响力的训练样例，从而研究训练样例间交互作用。

    

    训练数据归因（TDA）方法可以追溯模型在任何给定示例上的预测结果到特定的有影响力的训练示例。现有方法通过为每个训练示例分配一个标量影响分数来实现，假设影响是可加的。但实际上，我们观察到由于诸如示例之间的冗余、训练顺序和课程学习效应等因素，训练示例以高度非可加的方式相互作用。为了研究这种交互作用，我们提出了Simfluence，这是一种新的TDA范式，目标不是为每个示例生成单个的影响分数，而是一个训练过程模拟器：用户可以询问“如果我的模型训练了示例 z1、z2、……、zn，它在示例ztest上的表现会如何？”；然后模拟器应该输出一个模拟的训练过程，它是一个时间序列，在模拟的每个步骤都预测了在ztest上的损失。这使用户能够回答反事实问题。

    Training data attribution (TDA) methods offer to trace a model's prediction on any given example back to specific influential training examples. Existing approaches do so by assigning a scalar influence score to each training example, under a simplifying assumption that influence is additive. But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects.  To study such interactions, we propose Simfluence, a new paradigm for TDA where the goal is not to produce a single influence score per example, but instead a training run simulator: the user asks, ``If my model had trained on example $z_1$, then $z_2$, ..., then $z_n$, how would it behave on $z_{test}$?''; the simulator should then output a simulated training run, which is a time series predicting the loss on $z_{test}$ at every step of the simulated run. This enables users to answer counterfactual questions about
    
[^6]: 用调谐透镜从Transformer中获取潜在的预测能力

    Eliciting Latent Predictions from Transformers with the Tuned Lens. (arXiv:2303.08112v1 [cs.LG])

    [http://arxiv.org/abs/2303.08112](http://arxiv.org/abs/2303.08112)

    本文提出了一种改进版的“逻辑透镜”技术——“调谐透镜”，通过训练一个仿射探针，可以将每个隐藏状态解码成词汇分布。这个方法被应用于各种自回归语言模型上，比逻辑透镜更具有预测性、可靠性和无偏性，并且通过因果实验验证使用的特征与模型本身类似。同时，本文发现潜在预测的轨迹可以用于高精度地检测恶意输入。

    

    本文从迭代推理的角度分析了transformers模型，旨在了解模型预测是如何逐层进行精化的。为了实现这一目的，我们为冻结的预训练模型中的每个块训练一个仿射探针，使得可以将每个隐藏状态解码成词汇分布。我们的方法“调谐透镜”，是“逻辑透镜”技术的改进版本，前者给出了有用的见解，但常常易碎。我们将其应用于各种具有多达20B参数的自回归语言模型，表明其比逻辑透镜更具有预测性、可靠性和无偏性。通过因果实验显示，调谐透镜使用的特征与模型本身类似。我们还发现，潜在预测的轨迹可以用于高精度地检测恶意输入。我们的所有代码都可以在https://github.com/AlignmentResearch/tuned-lens 找到。

    We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle.  We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.
    
[^7]: 基于视觉、仿生稀疏神经网络的路线跟随

    Vision-based route following by an embodied insect-inspired sparse neural network. (arXiv:2303.08109v1 [cs.NE])

    [http://arxiv.org/abs/2303.08109](http://arxiv.org/abs/2303.08109)

    该论文比较了一种基于仿生稀疏神经网络的FlyHash模型和其他非稀疏模型在路线跟随任务中的效率，发现FlyHash模型在数据编码方面更加高效。

    

    我们比较了一种叫做FlyHash模型（Dasgupta et al., 2017）的仿生稀疏神经网络与其他非稀疏模型在路线跟随任务中的效率。该任务需要模型通过比较当前的视觉输入和沿途存储的记忆来控制转向。我们得出结论：FlyHash模型比其他模型更高效，尤其是在数据编码方面。

    We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network (Dasgupta et al., 2017), to similar but non-sparse models in an embodied navigation task. This requires a model to control steering by comparing current visual inputs to memories stored along a training route. We concluded the FlyHash model is more efficient than others, especially in terms of data encoding.
    
[^8]: 机器学习模型中的领域通用性在无线通信中的应用：概念，现状和开放问题

    Domain Generalization in Machine Learning Models for Wireless Communications: Concepts, State-of-the-Art, and Open Issues. (arXiv:2303.08106v1 [cs.LG])

    [http://arxiv.org/abs/2303.08106](http://arxiv.org/abs/2303.08106)

    本论文探讨了机器学习在无线通信中应用的重要性，特别是领域通用性方面的问题。研究表明，对于出现在不同领域中的数据，模型的准确性受到很大影响，领域通用性技术可以通过从不同的源域中学习模型并适应新的领域来解决这个问题。

    

    数据驱动的机器学习（ML）被认为是下一代无线系统中应用的一种潜在技术。这导致了大量研究工作，应用ML技术解决无线传输链的不同层次的问题。然而，大多数这些应用都依赖于有监督学习，其假定源（训练）和目标（测试）数据独立且随机分布相同（i.i.d.）。这个假设在现实世界中经常被违反，因为源数据和目标数据之间存在领域或分布转移。因此，确保这些算法能够推广到超出分布范围的数据非常重要。在这种情况下，领域通用性（DG）通过在不同和独特的源域/数据集上学习模型来解决OOD相关问题，并具有适应未见过的新领域的泛化能力，而不需要额外的微调。受到对无线应用中DG需求的重视，我们提出了一份全面的综述，介绍了在机器学习领域通用性的概念和其在无线通信中的最新发展，同时讨论了未来的研究方向。

    Data-driven machine learning (ML) is promoted as one potential technology to be used in next-generations wireless systems. This led to a large body of research work that applies ML techniques to solve problems in different layers of the wireless transmission link. However, most of these applications rely on supervised learning which assumes that the source (training) and target (test) data are independent and identically distributed (i.i.d). This assumption is often violated in the real world due to domain or distribution shifts between the source and the target data. Thus, it is important to ensure that these algorithms generalize to out-of-distribution (OOD) data. In this context, domain generalization (DG) tackles the OOD-related issues by learning models on different and distinct source domains/datasets with generalization capabilities to unseen new domains without additional finetuning. Motivated by the importance of DG requirements for wireless applications, we present a comprehe
    
[^9]: 金融时间序列的元对比标签校正方法

    Meta contrastive label correction for financial time series. (arXiv:2303.08103v1 [cs.LG])

    [http://arxiv.org/abs/2303.08103](http://arxiv.org/abs/2303.08103)

    本文针对股票价格预测中标记不准确的问题，提出了一种元对比标签校正方法。方法包括将对比学习算法融入元学习框架中，通过Gramian angular field和代表学习将时间序列数据生成图像，从而自动生成准确标签，并提高分类性能。

    

    金融应用（如股票价格预测）通常面临标记不准确的问题，本文提出一种元对比标签校正方法，可以自动生成准确标签，并提高分类性能。该方法包括将对比学习算法融入元学习框架中，通过Gramian angular field和代表学习将时间序列数据生成图像，从而自动生成准确标签。

    Financial applications such as stock price forecasting, usually face an issue that under the predefined labeling rules, it is hard to accurately predict the directions of stock movement. This is because traditional ways of labeling, taking Triple Barrier Method, for example, usually gives us inaccurate or even corrupted labels. To address this issue, we focus on two main goals. One is that our proposed method can automatically generate correct labels for noisy time series patterns, while at the same time, the method is capable of boosting classification performance on this new labeled dataset. Based on the aforementioned goals, our approach has the following three novelties: First, we fuse a new contrastive learning algorithm into the meta-learning framework to estimate correct labels iteratively when updating the classification model inside. Moreover, we utilize images generated from time series data through Gramian angular field and representative learning. Most important of all, we 
    
[^10]: 基于信息理论的固定专家建议下赌博机的遗憾界限

    Information-Theoretic Regret Bounds for Bandits with Fixed Expert Advice. (arXiv:2303.08102v1 [cs.LG])

    [http://arxiv.org/abs/2303.08102](http://arxiv.org/abs/2303.08102)

    本文研究固定专家建议下的赌博机问题，提出了基于信息论的遗憾界限，可以使得某些算法的遗憾无限接近于零。此外，我们还提出了KL散度来描述专家之间的相似性界限，并给出了下限证明算法的最优性。

    

    本文研究了在专家是固定和已知的情况下，赌博机与专家建议的问题，这些专家是行动固定和已知分布。相比以前的分析，我们展示了这种情况下遗憾是由衡量专家之间相似性的信息论量所控制的。在一些自然特殊情况下，这使我们能够获得EXP4的第一个遗憾界限，如果专家足够相似，则可以无限接近于零。为另一种算法提供了可以用KL散度来描述专家之间相似性的另一种界限，并且在某些情况下，我们展示了这个界限可以比EXP4更小。此外，我们为某些专家类别提供了下限，展示了我们分析的算法在某些情况下是几乎最优的。

    We investigate the problem of bandits with expert advice when the experts are fixed and known distributions over the actions. Improving on previous analyses, we show that the regret in this setting is controlled by information-theoretic quantities that measure the similarity between experts. In some natural special cases, this allows us to obtain the first regret bound for EXP4 that can get arbitrarily close to zero if the experts are similar enough. While for a different algorithm, we provide another bound that describes the similarity between the experts in terms of the KL-divergence, and we show that this bound can be smaller than the one of EXP4 in some cases. Additionally, we provide lower bounds for certain classes of experts showing that the algorithms we analyzed are nearly optimal in some cases.
    
[^11]: 解释位移：研究模型与转移数据分布的交互作用。

    Explanation Shift: Investigating Interactions between Models and Shifting Data Distributions. (arXiv:2303.08081v1 [cs.LG])

    [http://arxiv.org/abs/2303.08081](http://arxiv.org/abs/2303.08081)

    该论文提出了一种新的方法，通过模型解释特征的转移性质来检测分布转移下学习模型的行为是否越界，在比较中发现其比最先进的技术更为优秀，提供了算法方法并在实验中得到验证。

    

    当输入数据分布发生变化时，机器学习模型的预测性能往往会下降。在实践中，新的输入数据往往没有目标标签。因此，最先进的技术模型输入数据分布或模型预测分布，并试图理解学习模型和转移分布之间的相互作用问题。我们提出了一种新方法，该方法模型如何解释特征的转移性质受到分布转移的影响。我们发现，解释位移的建模可以比最先进的技术更好地指示检测超出分布的模型行为。我们分析了使用合成示例和真实数据集的不同类型的分布转移。我们提供了一种算法方法，允许我们检查数据集特征和学习模型之间的交互作用，并将其与最先进技术进行比较。我们在开源Python包中发布了我们的方法，以及使用的代码。

    As input data distributions evolve, the predictive performance of machine learning models tends to deteriorate. In practice, new input data tend to come without target labels. Then, state-of-the-art techniques model input data distributions or model prediction distributions and try to understand issues regarding the interactions between learned models and shifting distributions. We suggest a novel approach that models how explanation characteristics shift when affected by distribution shifts. We find that the modeling of explanation shifts can be a better indicator for detecting out-of-distribution model behaviour than state-of-the-art techniques. We analyze different types of distribution shifts using synthetic examples and real-world data sets. We provide an algorithmic method that allows us to inspect the interaction between data set features and learned models and compare them to the state-of-the-art. We release our methods in an open-source Python package, as well as the code used
    
[^12]: 使用互信息约束下的对比条件变分自编码器进行风格特征提取

    Style Feature Extraction Using Contrastive Conditioned Variational Autoencoders with Mutual Information Constraints. (arXiv:2303.08068v1 [cs.CV])

    [http://arxiv.org/abs/2303.08068](http://arxiv.org/abs/2303.08068)

    本文提出了一种使用互信息约束下的对比条件变分自编码器进行从未标记数据中提取风格特征的方法，该方法由一个提取风格无关特征的对比学习部分和一个提取风格特征的CVAE部分组成。

    

    在数据分析中，从未标记的数据中提取细粒度特征（如风格）非常重要。无监督方法（如变分自编码器（VAEs））可以提取风格，但提取的风格通常与其他特征混合。我们可以使用分类标签来指导VAEs提取风格，即条件VAEs（CVAEs）。但是，使用未标记数据仅提取风格的方法尚未建立。在本文中，我们构建了一种基于CVAE的方法，使用仅未标记的数据来提取风格特征。所提出的模型大致由两个并行部分组成; 提取风格无关特征的对比学习（CL）部分，以及提取风格特征的CVAE部分。CL模型通常以无需数据扩充的自监督方式学习与样式无关的表示，可以视为样式中的扰动。以提取的风格无关特征为条件，CVAE学习仅提取风格。在训练过程中，先训练CL模型，然后使用训练过的CL模型指导CVAE的训练。在几个数据集上评估了所提出的方法，实验结果表明所提出的方法可以有效地从未标记的数据中提取风格特征。

    It is crucial to extract fine-grained features such as styles from unlabeled data in data analysis. Unsupervised methods, such as variational autoencoders (VAEs), can extract styles, but the extracted styles are usually mixed with other features. We can isolate the styles using VAEs conditioned by class labels, known as conditional VAEs (CVAEs). However, methods to extract only styles using unlabeled data are not established. In this paper, we construct a CVAE-based method that extracts style features using only unlabeled data. The proposed model roughly consists of two parallel parts; a contrastive learning (CL) part that extracts style-independent features and a CVAE part that extracts style features. CL models generally learn representations independent of data augmentation, which can be seen as a perturbation in styles, in a self-supervised way. Taking the style-independent features as a condition, the CVAE learns to extract only styles. In the training procedure, a CL model is tra
    
[^13]: 《人口统计平等检查员：通过解释空间进行公平审核》

    Demographic Parity Inspector: Fairness Audits via the Explanation Space. (arXiv:2303.08040v1 [cs.LG])

    [http://arxiv.org/abs/2303.08040](http://arxiv.org/abs/2303.08040)

    这篇论文提出了一种基于解释空间的算法方法，测量分组人口统计学平等的违规情况，并允许我们检查组间歧视的原因，提高了审计公平性的敏感度。

    

    即使具有最好的意图，机器学习方法也可能延续、放大甚至创造社会偏见。衡量机器学习模型的歧视性（非歧视性）的方法已被提出。然而，导致歧视效果的受保护属性的代理仍然是一个具有挑战性的问题。我们提出了一种新的算法方法，它可以测量分组人口统计学平等的违规情况，并允许我们检查组间歧视的原因。我们的方法依赖于一种新颖的思想，即基于解释空间对模型对受保护属性的依赖度进行测量，解释空间是一种提供比输入数据或预测分布的原始空间更敏感审计的信息空间，从而允许断言理论上的人口统计审核保证。我们提供了数学分析、合成样例和实际数据的实验评估。我们还发布了一个开源的Pytorch实现和一个易于使用的Web应用程序。

    Even if deployed with the best intentions, machine learning methods can perpetuate, amplify or even create social biases. Measures of (un-)fairness have been proposed as a way to gauge the (non-)discriminatory nature of machine learning models. However, proxies of protected attributes causing discriminatory effects remain challenging to address. In this work, we propose a new algorithmic approach that measures group-wise demographic parity violations and allows us to inspect the causes of inter-group discrimination. Our method relies on the novel idea of measuring the dependence of a model on the protected attribute based on the explanation space, an informative space that allows for more sensitive audits than the primary space of input data or prediction distributions, and allowing for the assertion of theoretical demographic parity auditing guarantees. We provide a mathematical analysis, synthetic examples, and experimental evaluation of real-world data. We release an open-source Pyt
    
[^14]: ISimDL: 通过重要性采样驱动的故障注入模拟，加速深度学习强健性评估

    ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning. (arXiv:2303.08035v1 [cs.LG])

    [http://arxiv.org/abs/2303.08035](http://arxiv.org/abs/2303.08035)

    本论文提出了一种新方法ISimDL，利用神经元灵敏度生成重要性采样，加速故障注入模拟，有效评估了先进的DL系统对硬件故障的韧性，同时显著减少了所需的模拟数量。

    

    深度学习(DL)系统已在许多应用中广泛应用，需要专用的硬件加速器和芯片。在纳米时代，设备越来越容易受到永久性和瞬变故障的影响。因此，我们需要一种有效的方法来分析先进的DL系统对此类故障的韧性，并了解神经加速器芯片中的故障如何在DL应用级别上表现为错误，其中故障可能导致无法检测和恢复的错误。使用故障注入，我们可以通过在软件级别修改神经元权重和输出来执行DL系统的韧性研究，就好像硬件受到瞬变故障的影响一样。现有的故障模型减少了搜索空间，使分析更快，但需要该模型的先验知识，并且不允许进一步分析筛选出的搜索空间。因此，我们提出了ISimDL，一种新的方法，它利用神经元灵敏度生成重要性采样，并加速故障注入模拟。ISimDL可以有效评估先进的DL系统对硬件故障的韧性，而不会影响分析的准确性。所提出的方法显着减少了故障注入分析所需的模拟数量，同时仍确保足够覆盖搜索空间。我们将ISimDL应用于代表性的卷积神经网络，使用CIFAR-10和ImageNet数据集，并展示它提供显著的加速，同时仍保持与现有最先进故障注入方法相同的准确性水平。

    Deep Learning (DL) systems have proliferated in many applications, requiring specialized hardware accelerators and chips. In the nano-era, devices have become increasingly more susceptible to permanent and transient faults. Therefore, we need an efficient methodology for analyzing the resilience of advanced DL systems against such faults, and understand how the faults in neural accelerator chips manifest as errors at the DL application level, where faults can lead to undetectable and unrecoverable errors. Using fault injection, we can perform resilience investigations of the DL system by modifying neuron weights and outputs at the software-level, as if the hardware had been affected by a transient fault. Existing fault models reduce the search space, allowing faster analysis, but requiring a-priori knowledge on the model, and not allowing further analysis of the filtered-out search space. Therefore, we propose ISimDL, a novel methodology that employs neuron sensitivity to generate impo
    
[^15]: BODEGA: 针对可信度评估中对抗性样本生成的基准测试

    BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])

    [http://arxiv.org/abs/2303.08032](http://arxiv.org/abs/2303.08032)

    BODEGA是一个基准测试，用于模拟真实的内容管理场景，在四个误传检测任务上测试受害模型和攻击方法。测试结果表明，在某些情况下，即使进行微小的文本修改，也可以欺骗最准确的分类器。

    

    文本分类方法被广泛应用于检测不可信内容，如假新闻、社交媒体机器人、宣传等。较为准确的模型（可能基于深度神经网络）有助于管理公共电子平台，并经常导致内容创建者面临提交拒绝或已发布文本的撤下。为了避免进一步被检测，内容创建者尝试产生一个稍微修改过的文本版本（即攻击对抗性样本），利用分类器的弱点导致不同的输出。本文介绍了BODEGA：一个基准测试，用于在模拟内容管理的真实用例中测试受害模型和攻击方法在四个误传检测任务上的表现。我们还系统地测试了受欢迎的文本分类器对可用攻击技术的鲁棒性，并发现在某些情况下，即使在文本中进行微小的修改也可以欺骗最准确的分类器。

    Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. We also systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases barely signif
    
[^16]: EdgeServe:一种为去中心化预测而设计的执行层

    EdgeServe: An Execution Layer for Decentralized Prediction. (arXiv:2303.08028v1 [cs.DB])

    [http://arxiv.org/abs/2303.08028](http://arxiv.org/abs/2303.08028)

    EdgeServe 是一种为去中心化预测而设计的机器学习系统，通过低延迟的消息代理程序将数据路由到可以提供预测的节点。它具有一系列新颖的优化，可以在计算、通信和准确性之间进行折衷。在多摄像机物体跟踪，网络入侵检测和人类活动识别等三个去中心化预测任务中，EdgeServe 展现了很好的性能。

    

    机器学习任务的相关特征可能来自于网络中不同节点收集的数据源。这种问题被称之为去中心化预测，并在数据路由、计算布局和时间同步方面带来了许多有趣的系统挑战。本文提出了一种名为EdgeServe的机器学习系统，可以为去中心化预测提供服务。 EdgeServe 依赖于一个低延迟的消息代理程序，通过网络路由数据到可以提供预测的节点。EdgeServe 依赖一系列新颖的优化，可以在计算、通信和准确性之间进行折衷。我们在三个去中心化预测任务中评估了EdgeServe：（1）多摄像机物体跟踪，（2）网络入侵检测和（3）人类活动识别。

    The relevant features for a machine learning task may be aggregated from data sources collected on different nodes in a network. This problem, which we call decentralized prediction, creates a number of interesting systems challenges in managing data routing, placing computation, and time-synchronization. This paper presents EdgeServe, a machine learning system that can serve decentralized predictions. EdgeServe relies on a low-latency message broker to route data through a network to nodes that can serve predictions. EdgeServe relies on a series of novel optimizations that can tradeoff computation, communication, and accuracy. We evaluate EdgeServe on three decentralized prediction tasks: (1) multi-camera object tracking, (2) network intrusion detection, and (3) human activity recognition.
    
[^17]: 一种用于情感语音爆发识别的分层回归链框架。

    A Hierarchical Regression Chain Framework for Affective Vocal Burst Recognition. (arXiv:2303.08027v1 [eess.AS])

    [http://arxiv.org/abs/2303.08027](http://arxiv.org/abs/2303.08027)

    本文提出了一个基于链式回归模型的层次框架，用于从语音爆发中识别情感。该框架明确考虑了文化、低维和高维情感空间以及不同情感类别之间的关系。

    

    作为情感信号的一种常见形式，非语言语音爆发在日常社交互动中扮演着重要角色。理解和建模人类的语音爆发对于开发强大且广泛应用的人工智能至关重要。探索理解语音爆发的计算方法引起了越来越多的研究关注。在这项工作中，我们提出了一个基于链式回归模型的层次框架，用于从语音爆发中识别情感，明确考虑了多个关系：（i）情感状态和文化之间的关系；（ii）低维（唤醒和价值）与高维（10种情感类别）情感空间之间的关系；以及（iii）高维空间中不同情感类别之间的关系。为了解决数据稀疏性的挑战，我们还使用了自监督学习（SSL）表示与层次和时间聚合模块。所提出的系统参加了ACII情感语音爆发比赛并取得了良好成绩。

    As a common way of emotion signaling via non-linguistic vocalizations, vocal burst (VB) plays an important role in daily social interaction. Understanding and modeling human vocal bursts are indispensable for developing robust and general artificial intelligence. Exploring computational approaches for understanding vocal bursts is attracting increasing research attention. In this work, we propose a hierarchical framework, based on chain regression models, for affective recognition from VBs, that explicitly considers multiple relationships: (i) between emotional states and diverse cultures; (ii) between low-dimensional (arousal & valence) and high-dimensional (10 emotion classes) emotion spaces; and (iii) between various emotion classes within the high-dimensional space. To address the challenge of data sparsity, we also use self-supervised learning (SSL) representations with layer-wise and temporal aggregation modules. The proposed systems participated in the ACII Affective Vocal Burst
    
[^18]: 利用预训练表示和任务相关关键词进行阿尔茨海默病检测

    Leveraging Pretrained Representations with Task-related Keywords for Alzheimer's Disease Detection. (arXiv:2303.08019v1 [eess.AS])

    [http://arxiv.org/abs/2303.08019](http://arxiv.org/abs/2303.08019)

    本研究提出了一种利用高层次表示和任务相关关键词的方法来检测阿尔茨海默病，实验结果表明其具有较高的效率和优越性。

    

    随着全球人口迅速老龄化，阿尔茨海默病（AD）在老年人中尤为突出，其具有隐匿性质，导致认知领域（记忆、交流等）逐渐不可逆转的恶化。基于语音的AD检测为广泛筛查和及时干预提供了可能性。预训练模型的最新进展推动AD检测建模从低层特征转向高层次表示。本文提出了几种有效的方法，从高层次的声学和语言特征中提取更好的AD相关线索。基于这些特征，本文还提出了一种新颖的任务导向方法，通过对参与者描述和认知任务之间的关系进行建模。实验在ADReSS数据集上进行二元分类设置，并在未见测试集上评估模型。结果及与最近文献的比较表明了效率和优越性。

    With the global population aging rapidly, Alzheimer's disease (AD) is particularly prominent in older adults, which has an insidious onset and leads to a gradual, irreversible deterioration in cognitive domains (memory, communication, etc.). Speech-based AD detection opens up the possibility of widespread screening and timely disease intervention. Recent advances in pre-trained models motivate AD detection modeling to shift from low-level features to high-level representations. This paper presents several efficient methods to extract better AD-related cues from high-level acoustic and linguistic features. Based on these features, the paper also proposes a novel task-oriented approach by modeling the relationship between the participants' description and the cognitive task. Experiments are carried out on the ADReSS dataset in a binary classification setup, and models are evaluated on the unseen test set. Results and comparison with recent literature demonstrate the efficiency and superi
    
[^19]: 波束赋形技术在太赫兹频段的可靠性探讨：因果表达式是前进的道路吗？

    Reliable Beamforming at Terahertz Bands: Are Causal Representations the Way Forward?. (arXiv:2303.08017v1 [cs.IT])

    [http://arxiv.org/abs/2303.08017](http://arxiv.org/abs/2303.08017)

    本文介绍了一种动态、语义感知的波束赋形方案，利用新颖的人工智能算法在太赫兹频段中指导波束赋形，相较于经典的 MIMO 波束赋形方法有更好的表现。

    

    未来的无线服务，例如元宇宙，需要高信息速率、可靠性和低延迟。利用丰富的太赫兹带宽和大量天线创建狭窄的波束赋形方案的多用户无线系统可以满足这些要求。然而，现有解决方案缺乏对信道动态的恰当建模，导致在高移动性场景中出现不准确的波束赋形方案。本文首次提出了一种动态的、语义感知的波束赋形解决方案，利用变分因果推断中的新颖人工智能算法来计算多模态数据和波束赋形的因果表达式的时变动态。仿真结果表明，提出的因果推导方法在太赫兹波束赋形技术方面优于经典的MIMO波束赋形技术。

    Future wireless services, such as the metaverse require high information rate, reliability, and low latency. Multi-user wireless systems can meet such requirements by utilizing the abundant terahertz bandwidth with a massive number of antennas, creating narrow beamforming solutions. However, existing solutions lack proper modeling of channel dynamics, resulting in inaccurate beamforming solutions in high-mobility scenarios. Herein, a dynamic, semantically aware beamforming solution is proposed for the first time, utilizing novel artificial intelligence algorithms in variational causal inference to compute the time-varying dynamics of the causal representation of multi-modal data and the beamforming. Simulations show that the proposed causality-guided approach for Terahertz (THz) beamforming outperforms classical MIMO beamforming techniques.
    
[^20]: 使用机器学习检测金融交易描述中的虐待行为

    Detection of Abuse in Financial Transaction Descriptions Using Machine Learning. (arXiv:2303.08016v1 [cs.CL])

    [http://arxiv.org/abs/2303.08016](http://arxiv.org/abs/2303.08016)

    本文针对银行服务中的技术辅助虐待问题，开发了一个利用深度学习模型来识别和评分交易以识别虐待行为的系统。

    

    自从新支付平台（NPP）引入了将消息作为付款描述的较长格式后，人们现在发现它被用于沟通，在某些情况下，该系统被用作定向的家庭暴力形式。这种利用技术实现的虐待行为在识别、采取措施和纠正这种行为方面带来了新的挑战。澳大利亚联邦银行的人工智能实验室（CBA AI Labs）利用自然语言处理（NLP）中深度学习模型的进步开发了一个新系统，定期评分所有交易，并在数百万条记录中识别高风险虐待情况。

    Since introducing changes to the New Payments Platform (NPP) to include longer messages as payment descriptions, it has been identified that people are now using it for communication, and in some cases, the system was being used as a targeted form of domestic and family violence. This type of tech-assisted abuse poses new challenges in terms of identification, actions and approaches to rectify this behaviour. Commonwealth Bank of Australia's Artificial Intelligence Labs team (CBA AI Labs) has developed a new system using advances in deep learning models for natural language processing (NLP) to create a powerful abuse detector that periodically scores all the transactions, and identifies cases of high-risk abuse in millions of records. In this paper, we describe the problem of tech-assisted abuse in the context of banking services, outline the developed model and its performance, and the operating framework more broadly.
    
[^21]: 大规模统计学习模型有效地预测各种混沌系统

    Large statistical learning models effectively forecast diverse chaotic systems. (arXiv:2303.08011v1 [cs.LG])

    [http://arxiv.org/abs/2303.08011](http://arxiv.org/abs/2303.08011)

    该论文研究了混沌预测的大规模实验，发现基于人工神经网络的大规模、领域不可知的时间序列预测方法表现出了相当强大的性能，尤其是分层神经基础函数模型表现最佳。

    

    传统上混沌和不可预测是同义词，但最近统计预测的进展表明，大型机器学习模型可以从复杂系统的长时间观测中获得意想不到的见解。在本文中，我们对规模上的混沌预测进行了研究，通过对 135 种不同低维混沌系统的众包数据库进行 24 种代表性最高的多元预测方法的大规模比较。我们发现，基于人工神经网络的大规模的领域不可知时间序列预测方法始终展现出强大的预测性能，在某些情况下可以产生持续数十个李雅普诺夫时间的准确预测。最佳的混沌预测结果由最近引入的分层神经基础函数模型实现，但即使是通用的变压器和循环神经网络也表现出强大的性能。然而，物理启发式混合方法如神经常微分方程和储层计算机的性能更好，尤其是在更小的数据集上。

    Chaos and unpredictability are traditionally synonymous, yet recent advances in statistical forecasting suggest that large machine learning models can derive unexpected insight from extended observation of complex systems. Here, we study the forecasting of chaos at scale, by performing a large-scale comparison of 24 representative state-of-the-art multivariate forecasting methods on a crowdsourced database of 135 distinct low-dimensional chaotic systems. We find that large, domain-agnostic time series forecasting methods based on artificial neural networks consistently exhibit strong forecasting performance, in some cases producing accurate predictions lasting for dozens of Lyapunov times. Best-in-class results for forecasting chaos are achieved by recently-introduced hierarchical neural basis function models, though even generic transformers and recurrent neural networks perform strongly. However, physics-inspired hybrid methods like neural ordinary equations and reservoir computers c
    
[^22]: 基于窗口的早期退出级联用于不确定性估计：当深度集成比单一模型更有效时

    Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models. (arXiv:2303.08010v1 [cs.LG])

    [http://arxiv.org/abs/2303.08010](http://arxiv.org/abs/2303.08010)

    本文研究了基于窗口的早期退出集成方法，以在保持模型可扩展性的同时实现不确定性估计任务的高效实现。实验结果表明，该方法在准确性和计算效率上都达到了最新的研究成果。

    

    深度集成是提高深度学习方法预测性能和不确定性估计的简单、可靠和有效方法。然而，由于需要部署多个独立模型，它们被广泛批评为计算开销大。最近的研究挑战了这种观点，表明对于预测准确性，集成可以比在同一架构族中缩放单一模型在推理时更具计算效率。通过通过早期退出方法级联集成成员实现这一目标。在这项工作中，我们研究如何将这些效率提高扩展到与不确定性估计相关的任务。由于许多这样的任务，例如选择性分类，都是二分类问题，我们的关键新颖见解是仅将接近二分决策边界的样本传递到后续级联阶段。在ImageNet规模的数据上进行的实验表明，所提出的基于窗口的早期退出集成在使用比基线更少的模型评估的同时，实现了最先进的不确定性估计性能，并且在预测性能上与完整集成相竞争。

    Deep Ensembles are a simple, reliable, and effective method of improving both the predictive performance and uncertainty estimates of deep learning approaches. However, they are widely criticised as being computationally expensive, due to the need to deploy multiple independent models. Recent work has challenged this view, showing that for predictive accuracy, ensembles can be more computationally efficient (at inference) than scaling single models within an architecture family. This is achieved by cascading ensemble members via an early-exit approach. In this work, we investigate extending these efficiency gains to tasks related to uncertainty estimation. As many such tasks, e.g. selective classification, are binary classification, our key novel insight is to only pass samples within a window close to the binary decision boundary to later cascade stages. Experiments on ImageNet-scale data across a number of network architectures and uncertainty tasks show that the proposed window-base
    
[^23]: 在长文本分类器中无监督地提取理由

    Finding the Needle in a Haystack: Unsupervised Rationale Extraction from Long Text Classifiers. (arXiv:2303.07991v1 [cs.CL])

    [http://arxiv.org/abs/2303.07991](http://arxiv.org/abs/2303.07991)

    该论文研究了无监督理由提取的文档分类背景下，长文本分类器中的表现，并提出了一种比Longformer驱动的基线明显更好的RoBERTa句子级组合软注意力结构。

    

    长序列转换器旨在通过语言模型改进较长文本的表示，并提高它们在下游文档级任务中的性能。然而，人们对长格式模型中令牌级别预测的质量尚不太了解。我们研究了这种架构在无监督理由提取的文档分类背景下的性能。我们发现当与Longformer语言模型结合使用时，标准软量注意方法表现显着较差。我们提出了一种组合软量关注架构，它将RoBERTa应用于句子，以在令牌级别提取可信理由。我们发现这种方法在情感分类数据集上明显优于Longformer衍生基线，并且展现出明显更低的运行时间。

    Long-sequence transformers are designed to improve the representation of longer texts by language models and their performance on downstream document-level tasks. However, not much is understood about the quality of token-level predictions in long-form models. We investigate the performance of such architectures in the context of document classification with unsupervised rationale extraction. We find standard soft attention methods to perform significantly worse when combined with the Longformer language model. We propose a compositional soft attention architecture that applies RoBERTa sentence-wise to extract plausible rationales at the token-level. We find this method to significantly outperform Longformer-driven baselines on sentiment classification datasets, while also exhibiting significantly lower runtimes.
    
[^24]: 部分神经最优输运

    Partial Neural Optimal Transport. (arXiv:2303.07988v1 [cs.LG])

    [http://arxiv.org/abs/2303.07988](http://arxiv.org/abs/2303.07988)

    我们提出了一种新的神经方法来计算部分最优输运映射，并在合成例子上进行了测试。

    

    我们提出了一种新颖的神经方法来计算部分最优输运（OT）映射，即指定质量的度量部分之间的OT映射。我们在合成例子上测试了我们的部分神经最优输运算法。

    We propose a novel neural method to compute partial optimal transport (OT) maps, i.e., OT maps between parts of measures of the specified masses. We test our partial neural optimal transport algorithm on synthetic examples.
    
[^25]: 利用神经网络在高噪声环境中更快地解决学习有噪声偏差问题

    Practically Solving LPN in High Noise Regimes Faster Using Neural Networks. (arXiv:2303.07987v1 [cs.LG])

    [http://arxiv.org/abs/2303.07987](http://arxiv.org/abs/2303.07987)

    本文设计了具有两层神经网络的模型，可在高噪声和低维度情况下解决学习有噪声偏差问题（LPN），而且速度比传统算法更快。在26维，0.498的噪声率下，用“猜测然后高斯消元”算法需要3.12天，在8个GPU上使用神经网络算法只需要66分钟。

    

    本文研究了使用神经网络解决学习有噪声偏差问题（LPN）。我们的主要贡献是设计了一系列双层神经网络族，它们在高噪声，低维度区域实际上优于传统算法。我们考虑了三种情况，即LPN样本的数量充足、非常有限和介于两者之间。在每种情况下，我们提供了解决LPN的神经网络模型，以尽可能快地解决问题。对于某些情况，我们还能够提供解释我们模型设计理论的理论。在维度$n=26$、噪声率$\tau=0.498$的情况下，与Esser、Kubler和May（CRYPTO 2017）之前的实验相比，"猜测然后高斯消元"算法需要64个CPU核心3.12天的时间，而我们的神经网络算法只需要8个GPU的66分钟。我们的算法也可以插入混合算法以解决中等或大型维度的LPN实例。

    We conduct a systematic study of solving the learning parity with noise problem (LPN) using neural networks. Our main contribution is designing families of two-layer neural networks that practically outperform classical algorithms in high-noise, low-dimension regimes. We consider three settings where the numbers of LPN samples are abundant, very limited, and in between. In each setting we provide neural network models that solve LPN as fast as possible. For some settings we are also able to provide theories that explain the rationale of the design of our models. Comparing with the previous experiments of Esser, Kubler, and May (CRYPTO 2017), for dimension $n = 26$, noise rate $\tau = 0.498$, the ''Guess-then-Gaussian-elimination'' algorithm takes 3.12 days on 64 CPU cores, whereas our neural network algorithm takes 66 minutes on 8 GPUs. Our algorithm can also be plugged into the hybrid algorithms for solving middle or large dimension LPN instances.
    
[^26]: 一种关于隐含结构归纳的上下文中涌现学习理论

    A Theory of Emergent In-Context Learning as Implicit Structure Induction. (arXiv:2303.07971v1 [cs.CL])

    [http://arxiv.org/abs/2303.07971](http://arxiv.org/abs/2303.07971)

    本文推导了一个信息理论界限，展示了在自然语言数据具有足够的组成结构的情况下，从一般的下一个标记预测中获得上下文学习能力。为验证理论预测，本文引入了一个受控制的设置来诱导上下文学习，证明了经过训练的Transformer可以为一系列任务执行上下文学习。

    

    大规模语言模型的扩展引发了涌现式上下文学习的能力，即基于示例演示进行学习。尽管取得了一些进展，但对这种现象的理论理解仍然有限。我们认为，上下文学习依赖于自然语言数据中发现的组合性操作的重新组合。在基于语言学假设的情况下，我们推导出了一种信息理论界限，展示了当预训练分布具有足够的组成结构时，如何从一般的下一个标记预测中获得上下文学习能力。第二个界限为提示LLM输出朝着答案的中间步骤的实证成功提供了理论基础。为了验证理论预测，我们引入了一个受控制的设置来诱导上下文学习。与以往方法不同，它考虑到语言的组合本质。经过训练的Transformer可以为一系列任务执行上下文学习，这与在自然语言数据上预训练的 Transformer 保持一致。

    Scaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LLMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformers can perform in-context learning for a range of tasks, in a manner consistent with t
    
[^27]: 关于产业人工智能中概念漂移和不确定性的联系

    On the Connection between Concept Drift and Uncertainty in Industrial Artificial Intelligence. (arXiv:2303.07940v1 [cs.LG])

    [http://arxiv.org/abs/2303.07940](http://arxiv.org/abs/2303.07940)

    本文探索了工业人工智能中概念漂移和不确定性之间的联系，提出了一种方法来估计模型预测的置信度，并且证明了这种方法可以有效地在无监督的情况下检测概念漂移，从而提高模型的整体性能。

    

    基于人工智能的数字孪生是工业4.0革命的前沿，其技术由物联网和实时数据分析提供支持。从工业资产中收集的信息以持续的方式产生，产生的数据流必须在严格的时间限制下进行处理。这样的数据流通常会受到非平稳现象的影响，导致数据流的数据分布可能发生变化，因此用于数据分析的模型捕获的知识可能会变得过时（导致所谓的概念漂移效应）。及早发现（漂移）的变化对于更新模型的知识至关重要，在场景中特别具有挑战性，其中与流数据相关联的基本真相不容易找到。本文的目标是探讨产业人工智能中概念漂移和不确定性之间的联系。我们提出了一种方法来估计模型预测的置信度，并展示了如何在无监督的情况下使用它来检测概念漂移。我们的结果表明，所提出的方法可以有效地检测概念漂移，并可以提高模型的整体性能。

    AI-based digital twins are at the leading edge of the Industry 4.0 revolution, which are technologically empowered by the Internet of Things and real-time data analysis. Information collected from industrial assets is produced in a continuous fashion, yielding data streams that must be processed under stringent timing constraints. Such data streams are usually subject to non-stationary phenomena, causing that the data distribution of the streams may change, and thus the knowledge captured by models used for data analysis may become obsolete (leading to the so-called concept drift effect). The early detection of the change (drift) is crucial for updating the model's knowledge, which is challenging especially in scenarios where the ground truth associated to the stream data is not readily available. Among many other techniques, the estimation of the model's confidence has been timidly suggested in a few studies as a criterion for detecting drifts in unsupervised settings. The goal of thi
    
[^28]: 通过 Numerai 数据科学竞赛案例，理解时间表格和多变量时间序列的模型复杂度

    Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])

    [http://arxiv.org/abs/2303.07925](http://arxiv.org/abs/2303.07925)

    本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。

    

    本文探究了在多变量时间序列建模中使用不同特征工程和降维方法的应用。利用从 Numerai 数据竞赛创建的特征目标交叉相关时间序列数据集，我们证明在过度参数化的情况下，不同特征工程方法的性能与预测会收敛到可由再生核希尔伯特空间刻画的相同平衡态。我们提出了一种新的集成方法，该方法结合了不同的随机非线性变换，随后采用岭回归模型进行高维时间序列建模。与一些常用的用于序列建模的深度学习模型（如 LSTM 和 transformer）相比，我们的方法更加鲁棒（在不同的随机种子下具有较低的模型方差，且对架构的选择不太敏感），并且更有效率。我们方法的另一个优势在于模型的简单性，因为没有必要使用复杂的深度学习框架。

    In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
    
[^29]: 多领域训练在提高口音语音识别方面的应用

    Improving Accented Speech Recognition with Multi-Domain Training. (arXiv:2303.07924v1 [cs.LG])

    [http://arxiv.org/abs/2303.07924](http://arxiv.org/abs/2303.07924)

    本文介绍了使用四种不同法语口音来创建微调数据集，以提高预训练ASR模型对口音的识别能力，并成功将错误率在非洲和比利时口音上降低高达25%。

    

    随着自监督学习的增长，自动语音识别系统在广泛数据集上已经实现了接近人类的性能。然而，它们仍然缺乏泛化能力，并且不具备对口音变化等领域转移的稳健性。在本文中，我们使用代表四种不同法语口音的语音音频来创建微调数据集，以提高预训练ASR模型的稳健性。通过在训练集中包含不同的口音，我们得到了领域内和领域外的改进。数字实验表明，在与单领域训练相比，我们可以将错误率在非洲和比利时口音上降低高达25%（相对值），同时在标准法语上保持良好的性能。

    Thanks to the rise of self-supervised learning, automatic speech recognition (ASR) systems now achieve near-human performance on a wide variety of datasets. However, they still lack generalization capability and are not robust to domain shifts like accent variations. In this work, we use speech audio representing four different French accents to create fine-tuning datasets that improve the robustness of pre-trained ASR models. By incorporating various accents in the training set, we obtain both in-domain and out-of-domain improvements. Our numerical experiments show that we can reduce error rates by up to 25% (relative) on African and Belgian accents compared to single-domain training while keeping a good performance on standard French.
    
[^30]: 生成AI中的文本到图像扩散模型：一项调查

    Text-to-image Diffusion Model in Generative AI: A Survey. (arXiv:2303.07909v1 [cs.CV])

    [http://arxiv.org/abs/2303.07909](http://arxiv.org/abs/2303.07909)

    本文调查了文本到图像扩散模型以及相关应用，总结了最先进的方法，并探讨了挑战和未来方向。

    

    本文调查了文本到图像扩散模型，这些模型已经成为多种生成任务中流行的模型。作为一个自包含的工作，本调查从简单介绍基本扩散模型如何用于图像合成开始，接着是条件或引导如何改进学习。我们还总结了文本条件下的最先进的图像合成方法，并且进一步总结了文本引导创意生成和图像编辑的应用。除了迄今为止所取得的进展，我们还讨论了现有挑战和有前途的未来方向。

    This survey reviews text-to-image diffusion models in the context that diffusion models have emerged to be popular for a wide range of generative tasks. As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.
    
[^31]: 概率扩散模型的广义尺度空间特性

    Generalised Scale-Space Properties for Probabilistic Diffusion Models. (arXiv:2303.07900v1 [eess.IV])

    [http://arxiv.org/abs/2303.07900](http://arxiv.org/abs/2303.07900)

    本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。

    

    概率扩散模型在深度学习社区中越来越受欢迎。它们生成从学习图像分布的令人信服的样本，具有广泛的实际应用。这些方法最初是受漂移-扩散过程的启发，但在近期的实践导向的出版物中，这些起源得到了较少的关注。本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。此外，我们讨论了深度学习和基于模型的世界中漂移-扩散物理核心概念解释之间的相似性和差异。为此，我们考察了概率扩散与渗透滤波器之间的关系。

    Probabilistic diffusion models enjoy increasing popularity in the deep learning community. They generate convincing samples from a learned distribution of input images with a wide field of practical applications. Originally, these approaches were motivated from drift-diffusion processes, but these origins find less attention in recent, practice-oriented publications.  We investigate probabilistic diffusion models from the viewpoint of scale-space research and show that they fulfil generalised scale-space properties on evolving probability distributions. Moreover, we discuss similarities and differences between interpretations of the physical core concept of drift-diffusion in the deep learning and model-based world. To this end, we examine relations of probabilistic diffusion to osmosis filters.
    
[^32]: 学习者的贝叶斯复杂度与过拟合

    Bayes Complexity of Learners vs Overfitting. (arXiv:2303.07874v1 [cs.LG])

    [http://arxiv.org/abs/2303.07874](http://arxiv.org/abs/2303.07874)

    本文引入了新的函数复杂度概念，它能够支配PAC Bayes一样的概化界限，关联神经网络的自然函数复杂度概念，并解释神经网络与线性模型之间的泛化差距。此外，我们的概念能够自然地推广到有多层的神经网络，并且可导出上界，在更高层以及拥有结构化函数的情况下也是如此。

    

    我们引入了一种新的函数复杂度概念，并展示了它具有以下特性：（i）它支配着PAC Bayes一样的概化界限，（ii）对于神经网络，它关联了函数复杂度的自然概念（如变量），（iii）它解释了神经网络和线性模型之间的概化差距。虽然已有许多针对这些特性的论文和界限，但据我们所知，这是第一个满足这三个条件的概念。此外，与以前的工作不同，我们的概念自然地推广到有多层的神经网络。虽然一般情况下计算我们的复杂度是非常困难的，但一般可导出一个上界，即使在更高层以及拥有结构化函数（如周期函数）的情况下也是如此。我们推导的上界允许展示样本数量在2和总样本数量之间的良好泛化分离差距。

    We introduce a new notion of complexity of functions and we show that it has the following properties: (i) it governs a PAC Bayes-like generalization bound, (ii) for neural networks it relates to natural notions of complexity of functions (such as the variation), and (iii) it explains the generalization gap between neural networks and linear schemes. While there is a large set of papers which describes bounds that have each such property in isolation, and even some that have two, as far as we know, this is a first notion that satisfies all three of them. Moreover, in contrast to previous works, our notion naturally generalizes to neural networks with several layers.  Even though the computation of our complexity is nontrivial in general, an upper-bound is often easy to derive, even for higher number of layers and functions with structure, such as period functions. An upper-bound we derive allows to show a separation in the number of samples needed for good generalization between 2 and 
    
[^33]: DualMix: 释放数据增强对在线类增量学习的潜力

    DualMix: Unleashing the Potential of Data Augmentation for Online Class-Incremental Learning. (arXiv:2303.07864v1 [cs.LG])

    [http://arxiv.org/abs/2303.07864](http://arxiv.org/abs/2303.07864)

    本文研究了在在线类增量学习(OCL)中如何利用数据增强(DA)来防止灾难性遗忘(CF)的问题。通过提出增强的Mixup(EnMix)方法，同时混合增强样本和其标签，有效提高了OCI性能并防止CF问题。

    

    在线类增量学习(OCL)已经引发了新的方法，以向先前训练的模型知识中添加新类别的顺序到达的数据流。不幸的是，OCL学习可能会遭受灾难性遗忘(CF)，因为旧类别的决策边界在被新类别扰动时可能变得不准确。现有的文献已经应用数据增强(DA)来减轻模型遗忘，然而DA在OCI中的作用还不太清楚。我们理论上展示了与原始数据相关性较低的增强样本在防止遗忘方面更有效。然而，过度的增强也可能降低数据和相应标签之间的一致性，这促使我们利用适当的DA提高OCI性能并防止CF问题。我们提出了增强的Mixup(EnMix)方法，同时混合增强样本和其标签，显示了增强样本的效果。

    Online Class-Incremental (OCI) learning has sparked new approaches to expand the previously trained model knowledge from sequentially arriving data streams with new classes. Unfortunately, OCI learning can suffer from catastrophic forgetting (CF) as the decision boundaries for old classes can become inaccurate when perturbated by new ones. Existing literature have applied the data augmentation (DA) to alleviate the model forgetting, while the role of DA in OCI has not been well understood so far. In this paper, we theoretically show that augmented samples with lower correlation to the original data are more effective in preventing forgetting. However, aggressive augmentation may also reduce the consistency between data and corresponding labels, which motivates us to exploit proper DA to boost the OCI performance and prevent the CF problem. We propose the Enhanced Mixup (EnMix) method that mixes the augmented samples and their labels simultaneously, which is shown to enhance the sample 
    
[^34]: BoundaryCAM：一种基于边界的弱监督医学图像语义分割优化框架

    BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised Semantic Segmentation of Medical Images. (arXiv:2303.07853v1 [cs.CV])

    [http://arxiv.org/abs/2303.07853](http://arxiv.org/abs/2303.07853)

    BoundaryCAM提出了一种基于边界的弱监督的优化框架，能够预测对象位置，实现精细的高精度分割掩模。

    

    仅利用图像级别监督的弱监督语义分割（WSSS）是解决分割网络需求的一种有前途的方法，尤其是对于在给定数据集中生成大量像素级掩模。然而，大多数最先进的图像级WSSS技术缺乏对图像中包含的几何特征的理解，因为网络无法从仅图像级别标签中导出任何对象边界信息。为了解决这个缺陷，我们提出了我们的新型BoundaryCAM框架，该框架采用最先进的类激活图结合各种后处理技术，以实现精细的高精度分割掩模。为了实现这一目标，我们调查了一种最先进的无监督语义分割网络，该网络可用于构建边界图，以使BoundaryCAM能够高精度预测对象位置。

    Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision is a promising approach to deal with the need for Segmentation networks, especially for generating a large number of pixel-wise masks in a given dataset. However, most state-of-the-art image-level WSSS techniques lack an understanding of the geometric features embedded in the images since the network cannot derive any object boundary information from just image-level labels. We define a boundary here as the line separating an object and its background, or two different objects. To address this drawback, we propose our novel BoundaryCAM framework, which deploys state-of-the-art class activation maps combined with various post-processing techniques in order to achieve fine-grained higher-accuracy segmentation masks. To achieve this, we investigate a state-of-the-art unsupervised semantic segmentation network that can be used to construct a boundary map, which enables BoundaryCAM to predict object locations w
    
[^35]: 一份带有深度神经网络评估的羊水胎儿模型超声数据集，用于评估胎儿方向，胎位和解剖学特征

    FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features. (arXiv:2303.07852v1 [eess.IV])

    [http://arxiv.org/abs/2303.07852](http://arxiv.org/abs/2303.07852)

    该论文提出了一个新的胎儿模型超声数据集FPUS23，用于确定胎儿方向，胎位和解剖学特征。研究结果表明，FPUS23可以为临床超声监测工作流程带来改善，以及可能开发一个家庭使用的基于超声的胎儿监护平台。

    

    超声成像是评估胎儿在妊娠期间的生长，发展和整体健康状况最突出的技术之一。为了改善临床工作流程并可能开发一个家庭使用的基于超声的胎儿监护平台，我们提出了一个新颖的胎儿模型超声数据集FPUS23，它可用于确定（1）用于估计胎儿生物计量值的正确诊断平面，（2）胎儿方向，（3）它们的解剖结构和（4）23周孕龄时胎儿模型解剖学的边界框。整个数据集由15,728张图像组成，用于训练建立在ResNet34骨干网络上的四种不同的深度神经网络模型，以检测上述胎儿特征和使用情况。我们还评估了使用我们的FPUS23数据集训练的模型。

    Ultrasound imaging is one of the most prominent technologies to evaluate the growth, progression, and overall health of a fetus during its gestation. However, the interpretation of the data obtained from such studies is best left to expert physicians and technicians who are trained and well-versed in analyzing such images. To improve the clinical workflow and potentially develop an at-home ultrasound-based fetal monitoring platform, we present a novel fetus phantom ultrasound dataset, FPUS23, which can be used to identify (1) the correct diagnostic planes for estimating fetal biometric values, (2) fetus orientation, (3) their anatomical features, and (4) bounding boxes of the fetus phantom anatomies at 23 weeks gestation. The entire dataset is composed of 15,728 images, which are used to train four different Deep Neural Network models, built upon a ResNet34 backbone, for detecting aforementioned fetus features and use-cases. We have also evaluated the models trained using our FPUS23 da
    
[^36]: 实时部署抑郁症筛查工具的迁移学习方法使用加速计数据(arXiv：2303.07847v1 [cs.LG])

    Transfer Learning for Real-time Deployment of a Screening Tool for Depression Detection Using Actigraphy. (arXiv:2303.07847v1 [cs.LG])

    [http://arxiv.org/abs/2303.07847](http://arxiv.org/abs/2303.07847)

    研究提出了一种迁移学习方法，使用次要数据集的模型进行训练，并在实时部署基于加速计数据的抑郁症筛查工具时使用。这种方法可以在有限的原始数据样本下建立机器学习模型，具有较高的准确率。

    

    自动化抑郁症筛查和诊断是今天非常相关的问题。传统的抑郁症检测方法存在许多局限性，主要是高度依赖临床医生和有偏差的自我报告。近年来，研究表明利用穿戴设备收集用户的被动数据的基于机器学习（ML）的方法具有强大的潜力。然而，机器学习需要大量的数据，特别是在医疗领域，收集原始数据是具有挑战性的。在这项工作中，我们提出了一种基于迁移学习的方法，从在次要数据集上训练的模型开始，实时部署基于用户的行为数据的抑郁症筛查工具。这种方法可以在有限的原始数据样本下建立机器学习模型。在主数据集上执行的留一法交叉验证方法的修改版本导致平均准确率为0.96，其中在每次迭代中从主数据集中设置一个受试者的数据。

    Automated depression screening and diagnosis is a highly relevant problem today. There are a number of limitations of the traditional depression detection methods, namely, high dependence on clinicians and biased self-reporting. In recent years, research has suggested strong potential in machine learning (ML) based methods that make use of the user's passive data collected via wearable devices. However, ML is data hungry. Especially in the healthcare domain primary data collection is challenging. In this work, we present an approach based on transfer learning, from a model trained on a secondary dataset, for the real time deployment of the depression screening tool based on the actigraphy data of users. This approach enables machine learning modelling even with limited primary data samples. A modified version of leave one out cross validation approach performed on the primary set resulted in mean accuracy of 0.96, where in each iteration one subject's data from the primary set was set 
    
[^37]: 高效率对抗性模仿学习

    Sample-efficient Adversarial Imitation Learning. (arXiv:2303.07846v1 [cs.LG])

    [http://arxiv.org/abs/2303.07846](http://arxiv.org/abs/2303.07846)

    本研究提出了一种利用自监督表示来增强样本效率的对抗性模仿学习方法，从而学习不受扭曲影响的状态和动作表示以建立非图像控制任务的预测表征。

    

    模仿学习即通过演示进行学习，已经被研究并应用于序贯决策任务中，在这类任务中，奖励函数并不是预定义的。然而，模仿学习方法仍需要大量的专家演示样本才能成功模仿专家的行为。为提高样本效率，我们利用自监督表示学习，该方法可以从给定的数据生成大量的训练信号。在本研究中，我们提出了一种基于自监督表示的对抗性模仿学习方法，以学习不受各种扭曲影响的状态和动作表示，并建立非图像控制任务的预测表征。特别是，与现有的表格数据自监督学习方法相比，我们提出了一种针对状态和动作表示的不同损坏方法，以使其能够抵抗各种扭曲。理论和实证观察表明，使一个信息量大的特征流形与一个简单的生成器与一个复杂的分类器协同工作能够提高状态表征的质量。

    Imitation learning, in which learning is performed by demonstration, has been studied and advanced for sequential decision-making tasks in which a reward function is not predefined. However, imitation learning methods still require numerous expert demonstration samples to successfully imitate an expert's behavior. To improve sample efficiency, we utilize self-supervised representation learning, which can generate vast training signals from the given data. In this study, we propose a self-supervised representation-based adversarial imitation learning method to learn state and action representations that are robust to diverse distortions and temporally predictive, on non-image control tasks. In particular, in comparison with existing self-supervised learning methods for tabular data, we propose a different corruption method for state and action representations that is robust to diverse distortions. We theoretically and empirically observe that making an informative feature manifold with 
    
[^38]: 基于运动学数据的手术行为切分

    Kinematic Data-Based Action Segmentation for Surgical Applications. (arXiv:2303.07814v1 [cs.CV])

    [http://arxiv.org/abs/2303.07814](http://arxiv.org/abs/2303.07814)

    本文提出了两种多阶段体系结构和两种数据增强技术，专门用于基于运动学数据的行动分割。同时，作者在三个手术缝合任务数据集上对模型进行了评估。

    

    行动切分是高级流程分析中的一个挑战性任务，通常在视频或从各种传感器获取的运动学数据上执行。在手术过程中，行动切分对于工作流分析算法至关重要。本文提出了两个与运动学数据相关的行动分割方面的贡献。首先，我们介绍了两种多阶段体系结构，MS-TCN-BiLSTM和MS-TCN-BiGRU，专门设计用于运动学数据。 这些体系结构由具有阶内规则化和双向LSTM或GRU的细化阶段的预测生成器组成。其次，我们提出了两种新的数据增强技术，World Frame Rotation和Horizontal-Flip，利用运动学数据的强几何结构来提高算法性能和鲁棒性。我们在三个手术缝合任务数据集上评估了我们的模型：可变组织模拟（VTS）数据集和新推出的肠道修复模拟（BRS）数据集。

    Action segmentation is a challenging task in high-level process analysis, typically performed on video or kinematic data obtained from various sensors. In the context of surgical procedures, action segmentation is critical for workflow analysis algorithms. This work presents two contributions related to action segmentation on kinematic data. Firstly, we introduce two multi-stage architectures, MS-TCN-BiLSTM and MS-TCN-BiGRU, specifically designed for kinematic data. The architectures consist of a prediction generator with intra-stage regularization and Bidirectional LSTM or GRU-based refinement stages. Secondly, we propose two new data augmentation techniques, World Frame Rotation and Horizontal-Flip, which utilize the strong geometric structure of kinematic data to improve algorithm performance and robustness. We evaluate our models on three datasets of surgical suturing tasks: the Variable Tissue Simulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS) Dataset,
    
[^39]: ICICLE: 可解释的类增量连续学习方法

    ICICLE: Interpretable Class Incremental Continual Learning. (arXiv:2303.07811v1 [cs.LG])

    [http://arxiv.org/abs/2303.07811](http://arxiv.org/abs/2303.07811)

    ICICLE提出了一种基于样本的可解释的类增量连续学习方法，通过采用原型部分化方法来解决解释性概念漂移的问题，实验结果表明其在不需要样本的情况下表现优于现有的方法。

    

    连续学习能够增量学习新任务而不忘记之前学习的内容，从而促进新旧任务之间的正向知识转移。然而，连续学习对解释性提出了新的挑战，因为模型预测背后的原理可能会随着时间而改变，导致解释性概念漂移。本文通过提出基于样本的 Interpretable Class-InCremental LEarning (ICICLE) 方法，采用原型部分化方法，解决了这个问题。它包括三个关键的创新点：解释性正则化、以微粒粒度为基础的原型初始化策略以及针对原型部分的任务时效偏差补偿。我们的实验结果表明，ICICLE减少了解释性概念漂移，并且在不需要样本的情况下表现优于现有的方法。

    Continual learning enables incremental learning of new tasks without forgetting those previously learned, resulting in positive knowledge transfer that can enhance performance on both new and old tasks. However, continual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental LEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach. It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the fine-grained setting; and task-recency bias compensation devoted to prototypical parts. Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the existing exemplar-free me
    
[^40]: 高维数据因果性检验

    Testing Causality for High Dimensional Data. (arXiv:2303.07774v1 [cs.LG])

    [http://arxiv.org/abs/2303.07774](http://arxiv.org/abs/2303.07774)

    本文提出了一种因果关系检验方法，用于确定高维数据之间的因果关系。此方法在现有技术上进行改进，并扩展了到非线性迹函数。作者提出了一个新颖的岭正则化估计器的变体，并给出了可证的边界。

    

    确定高维观测之间因果关系是科学发现中最重要的任务之一。本文重新审视了线性迹方法，这是一种在高维情况下推断两个随机变量之间因果方向的技术。我们通过提供改进的尾部分析并在某些分布假设下扩展非线性迹函数的结果，显著增强了现有结果。我们通过将因果估计器解释为随机正交矩阵上的函数来获得我们的结果，可以将这样的空间上的Lipschitz函数浓度应用于其上。此外，我们还提出了一个新颖的岭正则化估计器的变体，并给出了岭估计项与其基本事实对应项之间的可证边界。

    Determining causal relationship between high dimensional observations are among the most important tasks in scientific discoveries. In this paper, we revisited the \emph{linear trace method}, a technique proposed in~\citep{janzing2009telling,zscheischler2011testing} to infer the causal direction between two random variables of high dimensions. We strengthen the existing results significantly by providing an improved tail analysis in addition to extending the results to nonlinear trace functionals with sharper confidence bounds under certain distributional assumptions. We obtain our results by interpreting the trace estimator in the causal regime as a function over random orthogonal matrices, where the concentration of Lipschitz functions over such space could be applied. We additionally propose a novel ridge-regularized variant of the estimator in \cite{zscheischler2011testing}, and give provable bounds relating the ridge-estimated terms to their ground-truth counterparts. We support o
    
[^41]: 多维数组的多切片聚类中的DBSCAN算法

    DBSCAN of Multi-Slice Clustering for three-order Tensor. (arXiv:2303.07768v1 [cs.LG])

    [http://arxiv.org/abs/2303.07768](http://arxiv.org/abs/2303.07768)

    本文提出了 MSC-DBSCAN扩展算法，可以在三元聚类中从数据中提取不同子空间的不同切片聚类，并可以获得与 MSC 算法在处理秩一张量数据时相同的解决方案。

    

    对于三维数据的三元聚类，现有的几种方法需要指定每个维度的聚类大小或聚类数量。为了解决这个问题，三元聚类(MSC)算法可以在低维子空间中找到保留信号的切片以便基于相似度阈值找到聚类。本文提出了 MSC-DBSCAN扩展算法以从数据中提取位于不同子空间的不同切片聚类(如果数据集是r个秩一张量(r>1)的总和)。我们的算法使用和 MSC 算法相同的输入，可以在处理秩一张量数据时与 MSC 算法获得相同的解决方案。

    Several methods for triclustering three-dimensional data require the cluster size or the number of clusters in each dimension to be specified. To address this issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal slices that lie in a low dimensional subspace for a rank-one tensor dataset in order to find a cluster based on the threshold similarity. We propose an extension algorithm called MSC-DBSCAN to extract the different clusters of slices that lie in the different subspaces from the data if the dataset is a sum of r rank-one tensor (r > 1). Our algorithm uses the same input as the MSC algorithm and can find the same solution for rank-one tensor data as MSC.
    
[^42]: NeurIPS 2022的Traffic4cast--预测城市交通和ETA的动态路网模型利用静止车辆探测器的稀疏节点数据。

    Traffic4cast at NeurIPS 2022 -- Predict Dynamics along Graph Edges from Sparse Node Data: Whole City Traffic and ETA from Stationary Vehicle Detectors. (arXiv:2303.07758v1 [cs.LG])

    [http://arxiv.org/abs/2303.07758](http://arxiv.org/abs/2303.07758)

    NeurIPS 2022的Traffic4cast竞赛要求参与者利用稀疏节点数据预测整个道路图的动态未来交通状态，并将GPS数据的速度级别转化为三个拥堵等级的概率。

    

    城市化和个人的交通业务趋势使我们不得不重新考虑我们生活和使用城市空间的方式。Traffic4cast竞赛通过数据驱动的方式，并利用机器学习来研究复杂时空系统的建模，从而解决这个问题。在这个版本中，我们将来自路网，$10^{12}$ 个探针数据点和三个城市两年的静止车辆探测器的信息结合在一起。然而，静止车辆探测器只在少数位置上可用，尽管它们是捕获交通量最准确的方法。Traffic4cast 2022探索的模型有能力将只有几个节点上的松散关联的时间节点数据概括到整个道路图的动态未来交通状态的预测中。

    The global trends of urbanization and increased personal mobility force us to rethink the way we live and use urban space. The Traffic4cast competition series tackles this problem in a data-driven way, advancing the latest methods in machine learning for modeling complex spatial systems over time. In this edition, our dynamic road graph data combine information from road maps, $10^{12}$ probe data points, and stationary vehicle detectors in three cities over the span of two years. While stationary vehicle detectors are the most accurate way to capture traffic volume, they are only available in few locations. Traffic4cast 2022 explores models that have the ability to generalize loosely related temporal vertex data on just a few nodes to predict dynamic future traffic states on the edges of the entire road graph. In the core challenge, participants are invited to predict the likelihoods of three congestion classes derived from the speed levels in the GPS data for the entire road graph in
    
[^43]: 基于相似矩阵的三阶张量的多路聚类

    Multiway clustering of 3-order tensor via affinity matrix. (arXiv:2303.07757v1 [cs.LG])

    [http://arxiv.org/abs/2303.07757](http://arxiv.org/abs/2303.07757)

    本文提出了一种基于相似矩阵的三阶张量多路聚类方法(MCAM)，通过构建相似矩阵并应用先进聚类方法最终实现了多路聚类分析。与其他已知算法相比，MCAM具有竞争优势。

    

    我们提出了一种新的三阶张量的多路聚类方法，即基于相似矩阵的多路聚类（MCAM）。该模型基于张量切片之间的相似性和每个切片的信息传播，构建一个相似矩阵，并在其上应用先进的聚类方法。三种模式的所有聚类的组合可以实现所需的多路聚类。最后，MCAM在合成数据集和真实数据集上的表现与其他已知算法相当。

    We propose a new method of multiway clustering for 3-order tensors via affinity matrix (MCAM). Based on a notion of similarity between the tensor slices and the spread of information of each slice, our model builds an affinity/similarity matrix on which we apply advanced clustering methods. The combination of all clusters of the three modes delivers the desired multiway clustering. Finally, MCAM achieves competitive results compared with other known algorithms on synthetics and real datasets.
    
[^44]: ForDigitStress：一种采用数字化面试情境的多模态压力数据集

    ForDigitStress: A multi-modal stress dataset employing a digital job interview scenario. (arXiv:2303.07742v1 [cs.LG])

    [http://arxiv.org/abs/2303.07742](http://arxiv.org/abs/2303.07742)

    该论文介绍了一个使用数字化面试情境来诱发压力并提供多模态数据、连续标注和基准分类器的压力数据集。最佳表现分类器的准确率和F1分数分别为88.3%和87.5%。

    

    我们提出了一个多模态的压力数据集，使用数字化面试来诱发压力。该数据集提供了40名参与者的多模态数据，包括音频、视频（动作捕捉、面部识别、眼动追踪）以及生理信息（光电脉搏图、皮肤电反应）。除此之外，该数据集还包含了压力和发生情绪（如羞耻、愤怒、焦虑、惊讶）的时间连续标注。为了建立基准线，我们针对二元压力分类任务，在所提出的数据集上训练并评估了五个不同的机器学习分类器（支持向量机、K近邻、随机森林、长短期记忆网络）。最佳表现分类器的准确率和F1分数分别为88.3%和87.5%。

    We present a multi-modal stress dataset that uses digital job interviews to induce stress. The dataset provides multi-modal data of 40 participants including audio, video (motion capturing, facial recognition, eye tracking) as well as physiological information (photoplethysmography, electrodermal activity). In addition to that, the dataset contains time-continuous annotations for stress and occurred emotions (e.g. shame, anger, anxiety, surprise). In order to establish a baseline, five different machine learning classifiers (Support Vector Machine, K-Nearest Neighbors, Random Forest, Long-Short-Term Memory Network) have been trained and evaluated on the proposed dataset for a binary stress classification task. The best-performing classifier achieved an accuracy of 88.3% and an F1-score of 87.5%.
    
[^45]: 神经网络能做算术吗？对最先进的深度学习模型的基本数字技能的调查

    Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models. (arXiv:2303.07735v1 [cs.AI])

    [http://arxiv.org/abs/2303.07735](http://arxiv.org/abs/2303.07735)

    本文调查了最近的文献，发现即使最先进的深度学习模型在面对基本数值和算术知识的相对简单任务时也经常无法胜任。

    

    创建能展示复杂推理技能的学习模型是深度学习研究中最大的挑战之一，而数学正在迅速成为评估科学进步方向的目标领域之一。过去几年里已经出现了大量的神经网络架构、数据集和基准测试，专门设计来解决数学问题，在自动定理证明、数值积分和新猜想或矩阵乘法算法的发现方面取得了显著的成功。然而，尽管取得了这些引人注目的成绩，深度学习模型是否具有数量和符号数字的基本理解力仍然不清楚。在这个调查中，我们对最近的文献进行了批判性的审核，得出结论，即即使是最先进的架构在面对相对简单的测试基本数值和算术知识的任务时，也经常无法胜任。

    Creating learning models that can exhibit sophisticated reasoning skills is one of the greatest challenges in deep learning research, and mathematics is rapidly becoming one of the target domains for assessing scientific progress in this direction. In the past few years there has been an explosion of neural network architectures, data sets, and benchmarks specifically designed to tackle mathematical problems, reporting notable success in disparate fields such as automated theorem proving, numerical integration, and discovery of new conjectures or matrix multiplication algorithms. However, despite these impressive achievements it is still unclear whether deep learning models possess an elementary understanding of quantities and symbolic numbers. In this survey we critically examine the recent literature, concluding that even state-of-the-art architectures often fall short when probed with relatively simple tasks designed to test basic numerical and arithmetic knowledge.
    
[^46]: 仅需好的邻居：基于邻近字符关系强化的中文字素到音素转换方法

    Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme Conversion. (arXiv:2303.07726v1 [cs.CL])

    [http://arxiv.org/abs/2303.07726](http://arxiv.org/abs/2303.07726)

    Reinforcer模型使用邻近字符关系强化语言模型，解决了中文G2P中语言模型编码句子过于普遍和词边界分割不一致性的问题，获得了最先进的性能。

    

    大多数中文字素到音素（G2P）系统采用三阶段架构，首先将输入序列转化为字符嵌入，使用语言模型获取语言信息，然后基于整个输入序列的全局上下文预测音素。然而，仅凭语言知识往往是不充分的。语言模型经常编码句子的过于普遍的结构，并未涵盖使用语音知识所需的特定情况。此外，需要一个手工后处理系统来解决与字符音调相关的问题。然而，系统在词边界分割上存在不一致性，从而降低了G2P系统的性能。为了解决这些问题，我们提出了Reinforcer模型，通过强调邻近字符之间的语音信息来为语言模型提供强大的引导偏差，以帮助消除发音的歧义。实验结果表明，与先前的中文G2P系统相比，Reinforcer模型在各种指标上实现了最先进的性能。

    Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework that first transforms input sequences into character embeddings, obtains linguistic information using language models, and then predicts the phonemes based on global context about the entire input sequence. However, linguistic knowledge alone is often inadequate. Language models frequently encode overly general structures of a sentence and fail to cover specific cases needed to use phonetic knowledge. Also, a handcrafted post-processing system is needed to address the problems relevant to the tone of the characters. However, the system exhibits inconsistency in the segmentation of word boundaries which consequently degrades the performance of the G2P system. To address these issues, we propose the Reinforcer that provides strong inductive bias for language models by emphasizing the phonological information between neighboring characters to help disambiguate pronunciations. Experimental results show that the R
    
[^47]: DisCoHead:通过分离控制头部姿势和面部表情的音视频生成谈话头

    DisCoHead: Audio-and-Video-Driven Talking Head Generation by Disentangled Control of Head Pose and Facial Expressions. (arXiv:2303.07697v1 [cs.CV])

    [http://arxiv.org/abs/2303.07697](http://arxiv.org/abs/2303.07697)

    DisCoHead是一种新颖的无监督音视频生成谈话头方法，能够分离和控制头部姿势和面部表情，通过使用单一的几何变换和神经混合方法，并将密集运动估计器和生成器的编码器集成在一起，使得生成的头部运动具有更高的质量和同步精度。

    

    对于逼真的谈话头生成，同时保持准确的唇部同步和自然的头部运动至关重要。为了完成这项艰巨的任务，我们提出了DisCoHead，这是一种新颖的方法，无需监督即可分离和控制头部姿势和面部表情。DisCoHead使用单一的几何变换作为瓶颈，从头部驱动视频中隔离并提取头部运动。可以使用仿射变换或薄板样条变换，两者都可以作为几何瓶颈。我们通过将密集运动估计器和生成器的编码器集成到一起来增强DisCoHead的效率。更进一步，我们还提出了一种神经混合方法，在此方法中，通过编码器隐式地估计和应用密集运动。在将已分离的头部运动应用于源身份之后，DisCoHead根据语音音频控制嘴部区域，根据单独的驱动视频眨眼和移动眉毛。在几个基准测试中进行的广泛实验表明，DisCoHead在视觉质量和同步精度方面超过了现有的最先进方法。

    For realistic talking head generation, creating natural head motion while maintaining accurate lip synchronization is essential. To fulfill this challenging task, we propose DisCoHead, a novel method to disentangle and control head pose and facial expressions without supervision. DisCoHead uses a single geometric transformation as a bottleneck to isolate and extract head motion from a head-driving video. Either an affine or a thin-plate spline transformation can be used and both work well as geometric bottlenecks. We enhance the efficiency of DisCoHead by integrating a dense motion estimator and the encoder of a generator which are originally separate modules. Taking a step further, we also propose a neural mix approach where dense motion is estimated and applied implicitly by the encoder. After applying the disentangled head motion to a source identity, DisCoHead controls the mouth region according to speech audio, and it blinks eyes and moves eyebrows following a separate driving vid
    
[^48]: 离线到在线强化学习的自适应策略学习

    Adaptive Policy Learning for Offline-to-Online Reinforcement Learning. (arXiv:2303.07693v1 [cs.LG])

    [http://arxiv.org/abs/2303.07693](http://arxiv.org/abs/2303.07693)

    本文提出了一种自适应策略学习框架，以有效地利用离线和在线数据，实现了离线到在线强化学习的最佳效果。

    

    传统强化学习需要一个环境来收集新鲜的数据，但当在线交互成本高昂时不切实际。离线强化学习通过直接从以前收集的数据集中学习提供了一种替代方法。但是，如果离线数据集的质量差，将导致性能不佳。本文考虑了一种离线到在线的场景，在该场景中，代理首先从离线数据集中学习，然后再进行在线训练，并提出了一种名为自适应策略学习的框架，以有效地利用离线和在线数据。具体来说，我们显式考虑了在线和离线数据之间的差异，并相应地应用了自适应更新策略，即对离线数据集采用悲观更新策略，而对在线数据集采用乐观/贪心更新策略。这种简单而有效的方法提供了一种混合离线和在线强化学习并实现两者最佳效果的方法。我们进一步通过理论分析表明，我们的算法实现了亚线性后悔界，与有同时访问离线和在线数据的oracle算法的性能相匹配。

    Conventional reinforcement learning (RL) needs an environment to collect fresh data, which is impractical when online interactions are costly. Offline RL provides an alternative solution by directly learning from the previously collected dataset. However, it will yield unsatisfactory performance if the quality of the offline datasets is poor. In this paper, we consider an offline-to-online setting where the agent is first learned from the offline dataset and then trained online, and propose a framework called Adaptive Policy Learning for effectively taking advantage of offline and online data. Specifically, we explicitly consider the difference between the online and offline data and apply an adaptive update scheme accordingly, that is, a pessimistic update strategy for the offline dataset and an optimistic/greedy update scheme for the online dataset. Such a simple and effective method provides a way to mix the offline and online RL and achieve the best of both worlds. We further provi
    
[^49]: FPTN:快速纯Transformer网络用于交通流量预测

    FPTN: Fast Pure Transformer Network for Traffic Flow Forecasting. (arXiv:2303.07685v1 [cs.LG])

    [http://arxiv.org/abs/2303.07685](http://arxiv.org/abs/2303.07685)

    本文提出了一种快速纯Transformer网络（FPTN），将交通流量数据沿传感器维度而非时间维度划分为序列，提出了三种嵌入方式将这些向量投影到适当的向量空间中，然后利用Transformer的多头注意机制捕获复杂时空相关性，使用全连接层输出预测的交通流量，FPTN不仅实现了最先进的准确性，而且运行速度比其他基于Transformer的模型快几倍。

    

    由于交通流量数据中的复杂时空相关性，交通流量预测是具有挑战性的。现有的基于Transformer的方法通常将交通流量预测视为多元时间序列（MTS）预测。然而，太多的传感器会导致一个大于800的向量，这很难在不丢失信息的情况下进行处理。此外，这些方法设计了复杂的机制来捕获MTS中的空间依赖关系，导致预测速度缓慢。为了解决上述问题，本文提出了一种快速纯Transformer网络（FPTN）。首先，将交通流量数据沿传感器维度而非时间维度划分为序列。然后，为了充分表示复杂的时空相关性，提出了三种嵌入方式将这些向量投影到适当的向量空间中。之后，为了同时捕获这些向量中的复杂时空相关性，我们利用了Transformer的多头注意机制。最后，使用全连接层输出预测的交通流量。在三个真实交通数据集上的实验结果表明，FPTN不仅实现了最先进的准确性，而且比其他基于Transformer的模型运行速度快几倍。

    Traffic flow forecasting is challenging due to the intricate spatio-temporal correlations in traffic flow data. Existing Transformer-based methods usually treat traffic flow forecasting as multivariate time series (MTS) forecasting. However, too many sensors can cause a vector with a dimension greater than 800, which is difficult to process without information loss. In addition, these methods design complex mechanisms to capture spatial dependencies in MTS, resulting in slow forecasting speed. To solve the abovementioned problems, we propose a Fast Pure Transformer Network (FPTN) in this paper. First, the traffic flow data are divided into sequences along the sensor dimension instead of the time dimension. Then, to adequately represent complex spatio-temporal correlations, Three types of embeddings are proposed for projecting these vectors into a suitable vector space. After that, to capture the complex spatio-temporal correlations simultaneously in these vectors, we utilize Transforme
    
[^50]: 预测图像记忆力所需的特征表示

    Feature representations useful for predicting image memorability. (arXiv:2303.07679v1 [cs.CV])

    [http://arxiv.org/abs/2303.07679](http://arxiv.org/abs/2303.07679)

    本研究使用Brain-Score评估64个CNN模型中与图像记忆力有关的特征表示，并发现高记忆力预测准确性的层与颞下皮质（IT）的脑部相似性更高。

    

    预测图像记忆力已经吸引了各个领域的兴趣。因此，使用卷积神经网络（CNN）模型的预测精度已经接近基于人类一致性估计的经验上限。然而，确定嵌入在CNN模型中的哪些特征表示对于记忆力的高预测准确性负责仍然是个悬而未决的问题。为了解决这个问题，本研究尝试使用脑部相似性来识别CNN模型中与记忆力有关的特征表示。具体而言，通过Brain-Score评估了在64个用于物体识别的CNN模型的16,860层中高记忆力预测准确性和脑部相似性。这项全面的分析显示出一个明显的趋势，即具有高记忆力预测准确性的层与最高阶段的颞下皮质（IT）的脑部相似性更高。此外，对64个CNN模型进行微调以预测图像记忆力并没有进一步提高准确性，这意味着现有的CNN模型已经包含了与记忆力相关的特征表示。

    Predicting image memorability has attracted interest in various fields. Consequently, prediction accuracy with convolutional neural network (CNN) models has been approaching the empirical upper bound estimated based on human consistency. However, identifying which feature representations embedded in CNN models are responsible for such high prediction accuracy of memorability remains an open question. To tackle this problem, this study sought to identify memorability-related feature representations in CNN models using brain similarity. Specifically, memorability prediction accuracy and brain similarity were examined and assessed by Brain-Score across 16,860 layers in 64 CNN models pretrained for object recognition. A clear tendency was shown in this comprehensive analysis that layers with high memorability prediction accuracy had higher brain similarity with the inferior temporal (IT) cortex, which is the highest stage in the ventral visual pathway. Furthermore, fine-tuning the 64 CNN m
    
[^51]: Sinkhorn-Flow: 使用最优输运预测动态系统中的概率质量流量

    Sinkhorn-Flow: Predicting Probability Mass Flow in Dynamical Systems Using Optimal Transport. (arXiv:2303.07675v1 [cs.LG])

    [http://arxiv.org/abs/2303.07675](http://arxiv.org/abs/2303.07675)

    本文提出了一种使用最优输运预测动态系统中的概率质量流量的新方法，该方法能够显著改善社交网络设置中社区演变预测任务的结果。

    

    在时间序列预测中，预测离散变量分布如何随时间变化是一个普遍的任务。但是，大多数方法仅集中于预测随后时间步骤的分布，而在许多情况下，确定这种概率质量在不同元素之间随时间流动的方式是一项关键信息。我们提出了一种使用最优输运预测这种质量流量随时间变化的新方法。具体而言，我们提出了一种通用的端到端深度学习系统中预测输运矩阵的方法，用 Sinkhorn 迭代替换标准的 softmax 操作。我们将我们的方法应用于在社交网络设置中预测社区将如何随时间演变的任务，并表明该方法在改进替代预测方法方面具有实质性的进展。我们特别强调了在预测乌克兰议会投票中派系演变任务方面的结果。

    Predicting how distributions over discrete variables vary over time is a common task in time series forecasting. But whereas most approaches focus on merely predicting the distribution at subsequent time steps, a crucial piece of information in many settings is to determine how this probability mass flows between the different elements over time. We propose a new approach to predicting such mass flow over time using optimal transport. Specifically, we propose a generic approach to predicting transport matrices in end-to-end deep learning systems, replacing the standard softmax operation with Sinkhorn iterations. We apply our approach to the task of predicting how communities will evolve over time in social network settings, and show that the approach improves substantially over alternative prediction methods. We specifically highlight results on the task of predicting faction evolution in Ukrainian parliamentary voting.
    
[^52]: 基于影像翻译的无监督跨模态域自适应技术进行Koos分类的研究

    Koos Classification of Vestibular Schwannoma via Image Translation-Based Unsupervised Cross-Modality Domain Adaptation. (arXiv:2303.07674v1 [eess.IV])

    [http://arxiv.org/abs/2303.07674](http://arxiv.org/abs/2303.07674)

    该论文提出了一种基于图像翻译的无监督跨模态域自适应方法，可以将ceT1扫描转换为hrT2扫描，来进行未标记的hrT2扫描的Koos分类，避免了无注释hrT2扫描不准确的问题，具有更好的性能。

    

    Koos分级标准是诊断前庭神经鞘瘤（VS）的分类系统，用于描述瘤体及其对相邻脑结构的影响。尽管对比增强T1（ceT1）扫描和高分辨率T2（hrT2）扫描均可用于Koos分类，但由于hrT2扫描更安全、更具成本效益，因此hrT2扫描备受关注。然而，在缺乏hrT2扫描注释的情况下，深度学习方法往往会因无监督学习而性能下降。如果ceT1扫描及其注释可用于hrT2扫描的无监督学习，使用未标记的hrT2扫描进行Koos分类的性能将大大提高。因此，我们提出了一种基于图像翻译的无监督跨模态域自适应方法，将带注释的ceT1扫描转换为hrT2扫描，然后用其训练一个分类器来进行未标记的hrT2扫描的Koos分类。在真实数据集上的实验结果表明，所提出的方法在不需要注释时使用hrT2扫描可实现精确的Koos分类，具有更好的性能。

    The Koos grading scale is a classification system for vestibular schwannoma (VS) used to characterize the tumor and its effects on adjacent brain structures. The Koos classification captures many of the characteristics of treatment deci-sions and is often used to determine treatment plans. Although both contrast-enhanced T1 (ceT1) scanning and high-resolution T2 (hrT2) scanning can be used for Koos Classification, hrT2 scanning is gaining interest because of its higher safety and cost-effectiveness. However, in the absence of annotations for hrT2 scans, deep learning methods often inevitably suffer from performance deg-radation due to unsupervised learning. If ceT1 scans and their annotations can be used for unsupervised learning of hrT2 scans, the performance of Koos classifi-cation using unlabeled hrT2 scans will be greatly improved. In this regard, we propose an unsupervised cross-modality domain adaptation method based on im-age translation by transforming annotated ceT1 scans into
    
[^53]: AutoTransfer: 带有知识转移的AutoML——应用于图神经网络

    AutoTransfer: AutoML with Knowledge Transfer -- An Application to Graph Neural Networks. (arXiv:2303.07669v1 [cs.LG])

    [http://arxiv.org/abs/2303.07669](http://arxiv.org/abs/2303.07669)

    AutoTransfer是一种带有知识转移的AutoML解决方案，可以将先前的GNN架构设计知识转移到新的任务中，通过任务嵌入和任务模型库来寻找理想模型的设计先验，以提高搜索效率。

    

    AutoML已经展现出在针对特定数据集和评价指标的机器学习任务中寻找有效神经网络结构的显著成功。然而，大多数现有的AutoML技术独立地考虑每个任务，这需要探索许多结构，导致高计算成本。在这里，我们提出了AutoTransfer，这是一种AutoML解决方案，它通过向感兴趣的新任务转移先前的建筑设计知识来提高搜索效率。我们的关键创新包括一个任务模型库，它捕捉了对各种不同的GNN体系结构和任务的模型性能，以及一种能够准确衡量不同任务之间相似性的计算有效任务嵌入。基于任务模型库和任务嵌入，通过聚合与任务相似的顶级设计分布的相似性加权和来估计新任务的理想模型的设计先验。

    AutoML has demonstrated remarkable success in finding an effective neural architecture for a given machine learning task defined by a specific dataset and an evaluation metric. However, most present AutoML techniques consider each task independently from scratch, which requires exploring many architectures, leading to high computational cost. Here we propose AutoTransfer, an AutoML solution that improves search efficiency by transferring the prior architectural design knowledge to the novel task of interest. Our key innovation includes a task-model bank that captures the model performance over a diverse set of GNN architectures and tasks, and a computationally efficient task embedding that can accurately measure the similarity among different tasks. Based on the task-model bank and the task embeddings, we estimate the design priors of desirable models of the novel task, by aggregating a similarity-weighted sum of the top-K design distributions on tasks that are similar to the task of i
    
[^54]: 关系多任务学习：建模数据和任务之间的关系

    Relational Multi-Task Learning: Modeling Relations between Data and Tasks. (arXiv:2303.07666v1 [cs.LG])

    [http://arxiv.org/abs/2303.07666](http://arxiv.org/abs/2303.07666)

    本文介绍了一种新颖的关系多任务学习设置，通过构建一个将数据点和任务连接起来的知识图，利用来自辅助任务的数据点标签来对新任务进行更准确的预测，并在各种数据集上显著优于最先进的多任务学习方法。

    

    多任务学习的一个关键假设是，在推断时，多任务模型只能访问给定数据点，而不能访问来自其他任务的数据点标签。这为将多任务学习扩展到利用来自其他辅助任务的数据点标签提供了机会，并在这种方式上提高新任务的性能。在这里，我们介绍了一种新颖的关系多任务学习设置，其中我们利用来自辅助任务的数据点标签来对新任务进行更准确的预测。我们开发了MetaLink，其中我们的关键创新是构建一个连接数据点和任务的知识图，从而使我们能够利用来自辅助任务的标签。知识图由两种类型的节点组成：（1）数据节点，其中节点特征是神经网络计算的数据嵌入，以及（2）任务节点，其中包含每个任务的最后一层权重作为节点特征。该知识图中的边捕捉数据任务关系，边标签表示这些关系的强度。我们证明了这种方法在各种数据集上显著优于最先进的多任务学习方法。

    A key assumption in multi-task learning is that at the inference time the multi-task model only has access to a given data point but not to the data point's labels from other tasks. This presents an opportunity to extend multi-task learning to utilize data point's labels from other auxiliary tasks, and this way improves performance on the new task. Here we introduce a novel relational multi-task learning setting where we leverage data point labels from auxiliary tasks to make more accurate predictions on the new task. We develop MetaLink, where our key innovation is to build a knowledge graph that connects data points and tasks and thus allows us to leverage labels from auxiliary tasks. The knowledge graph consists of two types of nodes: (1) data nodes, where node features are data embeddings computed by the neural network, and (2) task nodes, with the last layer's weights for each task as node features. The edges in this knowledge graph capture data-task relationships, and the edge la
    
[^55]: 实验固体力学中机器学习的最新进展与应用：一篇综述

    Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review. (arXiv:2303.07647v1 [cs.LG])

    [http://arxiv.org/abs/2303.07647](http://arxiv.org/abs/2303.07647)

    本文综述了近年来机器学习在实验固体力学领域中的最新进展和应用，提供了物理感知和基于物理学的机器学习方法，并涵盖了断裂力学、生物力学、纳米和微观力学、构建材料和二维材料等传统和新兴领域。

    

    多年来，实验固体力学在表征和理解天然和新材料的力学性质方面发挥了至关重要的作用。机器学习的最新进展为该领域提供了新的机遇，包括实验设计、数据分析、不确定性量化和反问题。由于近年来该新兴领域发表的论文数量迅速增加，因此及时进行全面和更新的综述，对于最近机器学习在实验固体力学中的应用具有重要意义。在本文中，我们首先概述了与该综述相关的常见机器学习算法和术语，重点介绍了基于物理学和物理感知的机器学习方法。然后，我们全面涵盖了实验力学传统和新兴领域中机器学习的最新应用，包括断裂力学、生物力学、纳米和微观力学、构建材料和二维材料。最后，我们强调了当前活跃的研究方向和领域面临的主要挑战，最后讨论了机器学习在实验固体力学未来的潜在机会。

    For many decades, experimental solid mechanics has played a crucial role in characterizing and understanding the mechanical properties of natural and novel materials. Recent advances in machine learning (ML) provide new opportunities for the field, including experimental design, data analysis, uncertainty quantification, and inverse problems. As the number of papers published in recent years in this emerging field is exploding, it is timely to conduct a comprehensive and up-to-date review of recent ML applications in experimental solid mechanics. Here, we first provide an overview of common ML algorithms and terminologies that are pertinent to this review, with emphasis placed on physics-informed and physics-based ML methods. Then, we provide thorough coverage of recent ML applications in traditional and emerging areas of experimental mechanics, including fracture mechanics, biomechanics, nano- and micro-mechanics, architected materials, and 2D material. Finally, we highlight some curr
    
[^56]: 使用单纯复合体进行聚类。

    Clustering with Simplicial Complexes. (arXiv:2303.07646v1 [cs.LG])

    [http://arxiv.org/abs/2303.07646](http://arxiv.org/abs/2303.07646)

    本文提出了一种使用单纯复合体进行聚类的方法，利用二阶单纯复合体来捕捉节点间的关系以实现更高级别的网络交互，并提出了基于Cheeger不等式的单纯光谱聚类算法。

    

    本文提出了一种新的聚类算法，基于二阶单纯复合体(也称为填充三角形)对网络中的节点进行分组，以利用更高级别的网络交互。我们定义了一个单纯导纳函数，通过最小化该函数，得到高密度的填充三角形在集合内，而在集合之间的填充三角形密度较小的最优分组。为此，我们提出了一种单纯邻接算子，通过二阶单纯复合体捕捉节点之间的关系。这使我们能够将著名的Cheeger不等式扩展到单纯复合体的聚类中。然后，利用Cheeger不等式，我们提出了单纯光谱聚类算法。我们在合成和现实世界的网络数据上报告实验结果，以证明所提出方法的有效性。

    In this work, we propose a new clustering algorithm to group nodes in networks based on second-order simplices (aka filled triangles) to leverage higher-order network interactions. We define a simplicial conductance function, which on minimizing, yields an optimal partition with a higher density of filled triangles within the set while the density of filled triangles is smaller across the sets. To this end, we propose a simplicial adjacency operator that captures the relation between the nodes through second-order simplices. This allows us to extend the well-known Cheeger inequality to cluster a simplicial complex. Then, leveraging the Cheeger inequality, we propose the simplicial spectral clustering algorithm. We report results from numerical experiments on synthetic and real-world network data to demonstrate the efficacy of the proposed approach.
    
[^57]: 稀有事件中的最佳臂识别问题

    Best arm identification in rare events. (arXiv:2303.07627v1 [cs.LG])

    [http://arxiv.org/abs/2303.07627](http://arxiv.org/abs/2303.07627)

    本文提出了解决稀有事件中最佳臂识别问题的算法，可以更快地提供正确臂的选择，同时略微增加样本复杂性。

    

    本文研究了在随机多臂老虎机框架下的最佳臂识别问题，其中每个臂都有一个微小的概率实现大量回报，而以压倒性的概率回报为零。该框架的一个关键应用是在线广告，其中广告的点击率可能仅占不到百分之一，而最终转化为销售的利润可能再次只是点击率的一小部分。最近，已开发了解决BAI问题的算法，这些算法可以最大限度地减少样本复杂性，同时在正确选择臂时提供统计保证。正如我们观察到的那样，这些算法可能在计算上是禁止的。我们利用每个臂的奖励过程可以很好地近似为复合泊松过程的事实，提出了更快的算法，样本复杂度略有增加。我们将问题在渐近情况下进行分析，当奖励发生的稀有性降低为零，奖励金额增加时。

    We consider the best arm identification problem in the stochastic multi-armed bandit framework where each arm has a tiny probability of realizing large rewards while with overwhelming probability the reward is zero. A key application of this framework is in online advertising where click rates of advertisements could be a fraction of a single percent and final conversion to sales, while highly profitable, may again be a small fraction of the click rates. Lately, algorithms for BAI problems have been developed that minimise sample complexity while providing statistical guarantees on the correct arm selection. As we observe, these algorithms can be computationally prohibitive. We exploit the fact that the reward process for each arm is well approximated by a Compound Poisson process to arrive at algorithms that are faster, with a small increase in sample complexity. We analyze the problem in an asymptotic regime as rarity of reward occurrence reduces to zero, and reward amounts increase 
    
[^58]: RE-MOVE：一种基于语言反馈的动态环境自适应策略设计方法

    RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback. (arXiv:2303.07622v1 [cs.RO])

    [http://arxiv.org/abs/2303.07622](http://arxiv.org/abs/2303.07622)

    RE-MOVE提出了一种基于语言反馈的自适应策略设计方法，可以使机器人适应实时环境变化，并从人类反馈中学习并适应之前未见过的对抗性场景。

    

    连续控制机器人导航任务的强化学习策略经常无法在实时部署期间适应环境的变化，这可能导致灾难性的失败。为了解决这个问题，我们提出了一种名为RE-MOVE（请求帮助并移动）的新方法，它使用基于语言的反馈来调整经过训练的策略以适应环境的实时变化。在这项工作中，我们使经过训练的策略能够决定何时请求反馈并如何将反馈纳入训练好的策略中。RE-MOVE利用先验不确定性来确定请求人类反馈的最佳时间，并使用基于语言的反馈进行实时适应。我们进行了大量的合成和实际世界的评估，以展示我们提出的方法在多种测试时间动态导航场景中的好处。我们的方法使机器人能够从人类反馈中学习并适应之前未见过的对抗性场景。

    Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (\textbf{RE}quest help and \textbf{MOVE} on), which uses language-based feedback to adjust trained policies to real-time changes in the environment. In this work, we enable the trained policy to decide \emph{when to ask for feedback} and \emph{how to incorporate feedback into trained policies}. RE-MOVE incorporates epistemic uncertainty to determine the optimal time to request feedback from humans and uses language-based feedback for real-time adaptation. We perform extensive synthetic and real-world evaluations to demonstrate the benefits of our proposed approach in several test-time dynamic navigation scenarios. Our approach enable robots to learn from human feedback and adapt to previously unseen adversarial 
    
[^59]: 关于基于交叉熵参数化的类别不平衡数据中的隐式几何

    On the Implicit Geometry of Cross-Entropy Parameterizations for Label-Imbalanced Data. (arXiv:2303.07608v1 [cs.LG])

    [http://arxiv.org/abs/2303.07608](http://arxiv.org/abs/2303.07608)

    本文探究了在标签不平衡数据上训练深度神经网络的CE损失函数参数化方法，结果得出了可以使模型偏向于少数类的隐式偏差理论并推导出了分类器和嵌入的闭式公式。

    

    针对在零训练误差区域以外的标签不平衡数据上训练大模型的方法，目前已经提出了多种经过逻辑调整的交叉熵（CE）损失参数化作为加权CE的替代方法。这些设计背后的主要驱动因素是隐式偏差理论，对于线性和线性化模型，解释了它们为什么能够成功地在优化路径上诱导出偏差，以使解决方案有利于少数人。为了将这个理论扩展到非线性模型，我们研究了不同CE参数化学习的分类器和嵌入的隐式几何。我们的主要成果是表征非凸敏感成本SVM分类器的全局最优解，该分类器是无约束特征模型的抽象。我们导出了分类器和嵌入的角度和范数的闭式公式，这些公式是类别数、失衡和少数人比例以及损失超参数的函数。借助这些公式，我们展示了。

    Various logit-adjusted parameterizations of the cross-entropy (CE) loss have been proposed as alternatives to weighted CE for training large models on label-imbalanced data far beyond the zero train error regime. The driving force behind those designs has been the theory of implicit bias, which for linear(ized) models, explains why they successfully induce bias on the optimization path towards solutions that favor minorities. Aiming to extend this theory to non-linear models, we investigate the implicit geometry of classifiers and embeddings that are learned by different CE parameterizations. Our main result characterizes the global minimizers of a non-convex cost-sensitive SVM classifier for the unconstrained features model, which serves as an abstraction of deep nets. We derive closed-form formulas for the angles and norms of classifiers and embeddings as a function of the number of classes, the imbalance and the minority ratios, and the loss hyperparameters. Using these, we show tha
    
[^60]: 利用机器学习预测海湾合作委员会国家的COVID-19感染情况

    Forecasting COVID-19 Infections in Gulf Cooperation Council (GCC) Countries using Machine Learning. (arXiv:2303.07600v1 [cs.LG])

    [http://arxiv.org/abs/2303.07600](http://arxiv.org/abs/2303.07600)

    本论文利用机器学习时间序列模型针对海湾合作委员会国家的COVID-19疫情数据进行分析，实验结果表明所开发的模型可以高精度地对COVID-19感染情况进行预测。

    

    COVID-19自去年首次检测以来，已经在全球范围内感染了超过6800万人。时间序列机器学习模型已经被应用于预测COVID-19的感染情况。本文使用Johns Hopkins的公共COVID-19数据集针对海湾合作委员会（GCC）国家开发了时间序列模型。数据集包括2020年1月22日至2021年1月22日之间的一年累计COVID-19病例。我们根据感染数据的空间分布，为所研究的国家开发了不同的模型。我们的实验结果表明，所开发的模型可以高精度地预测COVID-19的感染情况。

    COVID-19 has infected more than 68 million people worldwide since it was first detected about a year ago. Machine learning time series models have been implemented to forecast COVID-19 infections. In this paper, we develop time series models for the Gulf Cooperation Council (GCC) countries using the public COVID-19 dataset from Johns Hopkins. The dataset set includes the one-year cumulative COVID-19 cases between 22/01/2020 to 22/01/2021. We developed different models for the countries under study based on the spatial distribution of the infection data. Our experimental results show that the developed models can forecast COVID-19 infections with high precision.
    
[^61]: 一种对比知识迁移框架用于模型压缩与迁移学习

    A Contrastive Knowledge Transfer Framework for Model Compression and Transfer Learning. (arXiv:2303.07599v1 [cs.LG])

    [http://arxiv.org/abs/2303.07599](http://arxiv.org/abs/2303.07599)

    本论文提出了一个新的对比知识迁移框架（CKTF），通过优化教师和学生之间的中间表示的多个对比目标，使得CKTF能够传递足够的结构化知识，并提高了现有KT技术的性能。

    

    知识迁移（KT）在模型压缩和迁移学习中实现了竞争性能，并被广泛用于图像分类任务。现有的KT工作通过最小化“教师”模型和“学生”模型之间条件独立输出分布的差异来从大模型中传输信息。然而，这些方法忽略了教师模型中中间表示的高维结构知识，这导致有效性有限，并且这些方法受到各种启发式直觉的启发，使得普适性难以实现。本文提出了一种新颖的对比知识迁移框架（CKTF），它通过优化其之间的中间表示的多个对比目标，使教师向学生传递足够的结构化知识。此外，CKTF对现有的KT技术提供了一种广义协议，并通过导出高性能使其性能显著提高。

    Knowledge Transfer (KT) achieves competitive performance and is widely used for image classification tasks in model compression and transfer learning. Existing KT works transfer the information from a large model ("teacher") to train a small model ("student") by minimizing the difference of their conditionally independent output distributions. However, these works overlook the high-dimension structural knowledge from the intermediate representations of the teacher, which leads to limited effectiveness, and they are motivated by various heuristic intuitions, which makes it difficult to generalize. This paper proposes a novel Contrastive Knowledge Transfer Framework (CKTF), which enables the transfer of sufficient structural knowledge from the teacher to the student by optimizing multiple contrastive objectives across the intermediate representations between them. Also, CKTF provides a generalized agreement to existing KT techniques and increases their performance significantly by derivi
    
[^62]: AdPE：通过MAE+对视觉Transformer进行预训练的对抗位置嵌入

    AdPE: Adversarial Positional Embeddings for Pretraining Vision Transformers via MAE+. (arXiv:2303.07598v1 [cs.CV])

    [http://arxiv.org/abs/2303.07598](http://arxiv.org/abs/2303.07598)

    AdPE方法通过对抗位置嵌入，扭曲局部结构，强制Transformer编码器在全局上下文中学习更具有差别性的特征，从而提高泛化能力。

    

    无监督学习视觉Transformer旨在通过预设任务在没有标签的情况下预先训练编码器。其中一个任务是Masked Image Modeling（MIM），与预训练语言Transformer预测掩蔽补丁相对应。无监督预训练中的一个准则是预设任务需要足够难以防止Transformer学习不能很好地泛化到下游任务的微不足道的低层次特征。为此，我们提出了Adversarial Positional Embedding（AdPE）方法 - 通过扰动位置编码来扭曲局部视觉结构，以使得学习的Transformer不能仅使用局部相关的补丁来预测缺失的补丁。我们假设它迫使Transformer编码器在全局上下文中学习更具有差别性的特征，从而在下游任务中具有更强的泛化能力。我们将考虑绝对和相对位置编码，其中对抗位置可以模拟为...

    Unsupervised learning of vision transformers seeks to pretrain an encoder via pretext tasks without labels. Among them is the Masked Image Modeling (MIM) aligned with pretraining of language transformers by predicting masked patches as a pretext task. A criterion in unsupervised pretraining is the pretext task needs to be sufficiently hard to prevent the transformer encoder from learning trivial low-level features not generalizable well to downstream tasks. For this purpose, we propose an Adversarial Positional Embedding (AdPE) approach -- It distorts the local visual structures by perturbing the position encodings so that the learned transformer cannot simply use the locally correlated patches to predict the missing ones. We hypothesize that it forces the transformer encoder to learn more discriminative features in a global context with stronger generalizability to downstream tasks. We will consider both absolute and relative positional encodings, where adversarial positions can be im
    
[^63]: 快速正则化离散最优传输中的群稀疏正则化器

    Fast Regularized Discrete Optimal Transport with Group-Sparse Regularizers. (arXiv:2303.07597v1 [cs.LG])

    [http://arxiv.org/abs/2303.07597](http://arxiv.org/abs/2303.07597)

    本论文提出了一种名为Fast GS-DOT的新算法，通过利用梯度矩阵的块对角结构，可以在保持传统方法相同精度的同时显著降低计算时间，有效处理了正则化离散最优传输中类别标签过多的问题。

    

    正则化离散最优传输（OT）是衡量来自两个不同域的离散分布之间距离的有力工具。虽然正则化离散最优传输在机器学习中有广泛应用，但在某些情况下，来自其中一个域的样本数据会有类别标签，例如无监督域适应。在这种情况下，群稀疏正则化器经常被用作正则化项来处理类别标签。特别地，通过将具有相同类别标签的数据样本对应到一个群稀疏正则化器中，它可以保护数据样本上的标签结构。因此，我们可以通过解决带有梯度的正则化优化问题来利用标签信息进行距离度量。然而，当类别或数据样本数量较大时，对于正则化项的数量及其相应梯度数的增长，计算梯度的成本变得昂贵。在本文中，我们提出了一种名为快速群稀疏正则化离散最优传输（Fast GS-DOT）的新算法来解决这个问题。通过利用梯度矩阵的块对角结构，我们推导出一种有效的算法，它显著降低了计算时间，同时保持了传统方法相同的精度。我们在合成和真实数据上展示了我们算法的有效性，并展示了它在速度和精度方面优于现有方法。

    Regularized discrete optimal transport (OT) is a powerful tool to measure the distance between two discrete distributions that have been constructed from data samples on two different domains. While it has a wide range of applications in machine learning, in some cases the sampled data from only one of the domains will have class labels such as unsupervised domain adaptation. In this kind of problem setting, a group-sparse regularizer is frequently leveraged as a regularization term to handle class labels. In particular, it can preserve the label structure on the data samples by corresponding the data samples with the same class label to one group-sparse regularization term. As a result, we can measure the distance while utilizing label information by solving the regularized optimization problem with gradient-based algorithms. However, the gradient computation is expensive when the number of classes or data samples is large because the number of regularization terms and their respectiv
    
[^64]: 单隐藏层前馈神经网络的连续三元决策

    Sequential three-way decisions with a single hidden layer feedforward neural network. (arXiv:2303.07589v1 [cs.LG])

    [http://arxiv.org/abs/2303.07589](http://arxiv.org/abs/2303.07589)

    这篇论文提出了一种基于连续三元决策的单隐藏层前馈神经网络（STWD-SFNN）模型，通过动态学习隐藏层节点数量和连续调整阈值参数，实现了对结构化数据集更高效的处理。

    

    本文提出了基于连续三元决策（STWD）的单隐藏层前馈神经网络（STWD-SFNN）模型，以增强结构化数据集的网络性能。该模型采用多粒度级别动态学习隐藏层节点数量，并设置连续的阈值参数，考虑了处理成本，可更高效处理大型数据集。

    The three-way decisions strategy has been employed to construct network topology in a single hidden layer feedforward neural network (SFNN). However, this model has a general performance, and does not consider the process costs, since it has fixed threshold parameters. Inspired by the sequential three-way decisions (STWD), this paper proposes STWD with an SFNN (STWD-SFNN) to enhance the performance of networks on structured datasets. STWD-SFNN adopts multi-granularity levels to dynamically learn the number of hidden layer nodes from coarse to fine, and set the sequential threshold parameters. Specifically, at the coarse granular level, STWD-SFNN handles easy-to-classify instances by applying strict threshold conditions, and with the increasing number of hidden layer nodes at the fine granular level, STWD-SFNN focuses more on disposing of the difficult-to-classify instances by applying loose threshold conditions, thereby realizing the classification of instances. Moreover, STWD-SFNN con
    
[^65]: 基于嵌入式加速器的雷达感知的师生知识蒸馏

    Teacher-Student Knowledge Distillation for Radar Perception on Embedded Accelerators. (arXiv:2303.07586v1 [cs.AI])

    [http://arxiv.org/abs/2303.07586](http://arxiv.org/abs/2303.07586)

    本文提出一种基于师生知识蒸馏的方法，用于低级别雷达感知任务，并成功实现嵌入式计算的实时部署，速度达到教师模型的100倍。

    

    目前许多用于道路安全感知的雷达信号处理方法都无法很好地运行在用于汽车的嵌入式硬件加速器上。相反，端到端的机器学习方法更好地利用了专门加速器所带来的性能提升。在本文中，我们提出了一种用于低级别雷达感知任务的师生知识蒸馏方法。我们利用用于静态目标检测的混合模型作为教师，来训练端到端的机器学习学生模型。该学生模型可以高效地利用嵌入式计算进行实时部署。我们证明了所提出的学生模型比教师模型快100倍。

    Many radar signal processing methodologies are being developed for critical road safety perception tasks. Unfortunately, these signal processing algorithms are often poorly suited to run on embedded hardware accelerators used in automobiles. Conversely, end-to-end machine learning (ML) approaches better exploit the performance gains brought by specialized accelerators. In this paper, we propose a teacher-student knowledge distillation approach for low-level radar perception tasks. We utilize a hybrid model for stationary object detection as a teacher to train an end-to-end ML student model. The student can efficiently harness embedded compute for real-time deployment. We demonstrate that the proposed student model runs at speeds 100x faster than the teacher model.
    
[^66]: 基于敏感区域的可解释AI变态测试框架

    Sensitive Region-based Metamorphic Testing Framework using Explainable AI. (arXiv:2303.07580v1 [cs.LG])

    [http://arxiv.org/abs/2303.07580](http://arxiv.org/abs/2303.07580)

    提出了一种基于敏感区域的元测试框架，可以通过转换这些区域来有效地检测易出现错误分类的图像；敏感区域可以由可解释AI指定。

    

    深度学习是机器学习中最受欢迎的研究课题之一，基于深度学习的图像识别系统得到了快速发展。最近的研究使用变态测试来检测错误分类的图像。大多数研究讨论变态关系(MR)，但很少讨论应该转换哪些区域。我们的重点是存在敏感区域，即使进行小的转换也会容易改变预测结果，并提出一种变态测试框架，通过转换敏感区域有效地检测易出现错误分类的区域。我们的评估表明，敏感区域可以由可解释AI(XAI)指定，我们的框架有效地检测故障。

    Deep Learning (DL) is one of the most popular research topics in machine learning and DL-driven image recognition systems have developed rapidly. Recent research has used metamorphic testing (MT) to detect misclassified images. Most of them discuss metamorphic relations (MR), with little discussion on which regions should be transformed. We focus on the fact that there are sensitive regions where even a small transformation can easily change the prediction results and propose an MT framework that efficiently tests for regions prone to misclassification by transforming the sensitive regions. Our evaluation showed that the sensitive regions can be specified by Explainable AI (XAI) and our framework effectively detects faults.
    
[^67]: VANI：一种非常轻量级的语音合成系统，可对本地和非本地语言使用者进行发音控制，并保留其身份

    VANI: Very-lightweight Accent-controllable TTS for Native and Non-native speakers with Identity Preservation. (arXiv:2303.07578v1 [cs.SD])

    [http://arxiv.org/abs/2303.07578](http://arxiv.org/abs/2303.07578)

    VANI是一种轻量级的多语言口音可控制语音合成系统，支持对语音的口音、语言、说话者、F0和能量特征进行显式控制，并能在保留本地口音的情况下更换说话者语言。

    

    我们介绍了VANI，一种非常轻量级的多语言口音可控制语音合成系统。我们的模型基于RADMMM中提出的解缠放策略，支持口音、语言、说话者、细粒度F0和能量特征的显式控制。我们利用Indic语言数据集，在3种不同的语言中合成语音。我们的模型支持在保留说话者声音和目标语言的本地口音的情况下转换说话者的语言。我们在比赛的Track 2和3中使用了轻量级VANI模型，在Track 1中使用了大参数的RADMMM模型。

    We introduce VANI, a very lightweight multi-lingual accent controllable speech synthesis system. Our model builds upon disentanglement strategies proposed in RADMMM and supports explicit control of accent, language, speaker and fine-grained $F_0$ and energy features for speech synthesis. We utilize the Indic languages dataset, released for LIMMITS 2023 as part of ICASSP Signal Processing Grand Challenge, to synthesize speech in 3 different languages. Our model supports transferring the language of a speaker while retaining their voice and the native accent of the target language. We utilize the large-parameter RADMMM model for Track $1$ and lightweight VANI model for Track $2$ and $3$ of the competition.
    
[^68]: 机器学习计算机视觉在加州橙县空间AI物体识别中的应用

    Machine Learning Computer Vision Applications for Spatial AI Object Recognition in Orange County, California. (arXiv:2303.07560v1 [cs.CV])

    [http://arxiv.org/abs/2303.07560](http://arxiv.org/abs/2303.07560)

    本论文介绍了一种利用机器学习和计算机视觉算法进行空间物体识别和位置检测的方法，并在加州橙县实现了这一方法，并成功地进行了对停车标志和消防栓的识别。

    

    我们提供了一种集成的系统化方法，利用人工智能机器学习和计算机视觉算法实现橙县空间物体识别和位置检测。我们描述了一种多传感器、高分辨率野外数据获取的全面方法，以及野外后处理和预分析处理任务。我们开发了一系列算法公式和工作流程，将卷积深度神经网络学习与360{\deg}全景图像中检测的物体位置估计相结合。我们提供了在橙县的两个区域的照片球图像中处理超过 80 万个方位角的应用实例，并展示了停车标志和消防栓识别的检测结果。我们讨论了我们方法的效率和有效性，以及与此方法相关的更广泛的性能和未来技术创新的影响。

    We provide an integrated and systematic automation approach to spatial object recognition and positional detection using AI machine learning and computer vision algorithms for Orange County, California. We describe a comprehensive methodology for multi-sensor, high-resolution field data acquisition, along with post-field processing and pre-analysis processing tasks. We developed a series of algorithmic formulations and workflows that integrate convolutional deep neural network learning with detected object positioning estimation in 360{\deg} equirectancular photosphere imagery. We provide examples of application processing more than 800 thousand cardinal directions in photosphere images across two areas in Orange County, and present detection results for stop-sign and fire hydrant object recognition. We discuss the efficiency and effectiveness of our approach, along with broader inferences related to the performance and implications of this approach for future technological innovations
    
[^69]: 终身学习在异常检测中的应用: 新的挑战、视角和见解

    Lifelong Learning for Anomaly Detection: New Challenges, Perspectives, and Insights. (arXiv:2303.07557v1 [cs.LG])

    [http://arxiv.org/abs/2303.07557](http://arxiv.org/abs/2303.07557)

    本文探讨了终身异常检测的重要性，提出设计终身学习复杂性的异常检测方法的挑战和机会，并提供了一种场景生成过程使得研究人员能够进行实验。

    

    在许多实际领域中，异常检测具有极其重要的意义，特别是在行为不断变化的情况下。终身学习是一种新兴趋势，它能够满足需要机器学习模型在动态环境中不断适应新挑战并保留过去知识的需求。然而，目前很少有人致力于建立终身异常检测的基础，这与更广泛探索的分类设置存在本质不同的挑战。本文通过探讨、阐述和讨论终身异常检测，试图为其更广泛的采用建立基础。首先，我们解释了为什么终身异常检测很重要，定义了应对终身学习复杂性的异常检测方法设计的挑战和机会。其次，我们对学习设置和场景生成过程进行了表征，使研究人员能够使用这些工具进行终身异常检测的实验。

    Anomaly detection is of paramount importance in many real-world domains, characterized by evolving behavior. Lifelong learning represents an emerging trend, answering the need for machine learning models that continuously adapt to new challenges in dynamic environments while retaining past knowledge. However, limited efforts are dedicated to building foundations for lifelong anomaly detection, which provides intrinsically different challenges compared to the more widely explored classification setting. In this paper, we face this issue by exploring, motivating, and discussing lifelong anomaly detection, trying to build foundations for its wider adoption. First, we explain why lifelong anomaly detection is relevant, defining challenges and opportunities to design anomaly detection methods that deal with lifelong learning complexities. Second, we characterize learning settings and a scenario generation procedure that enables researchers to experiment with lifelong anomaly detection using
    
[^70]: 合并决策Transformer：多任务策略形成的权重平均化

    Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies. (arXiv:2303.07551v1 [cs.LG])

    [http://arxiv.org/abs/2303.07551](http://arxiv.org/abs/2303.07551)

    本文提出通过在权重空间中合并训练于不同 MuJoCo 运动问题上的 Decision Transformer 的子集，形成多任务模型。通过共享一些辅助任务的训练以及共同使用预训练初始化，能够获得更好的结果。这个方向的研究有助于使代理的过程民主化和分发。

    

    最近的研究展示了基于Transformer的通用语言、视觉和连续决策制定问题的策略的前景。为了创建这样的模型，我们通常需要集中的训练目标、数据和计算。如果我们能够更灵活地创建通用策略，通过合并多个任务特定的、单独训练的策略，则这样做就比较有意义。在本文中，我们通过在权重空间中合并或平均不同MuJoCo运动问题上训练的Decision Transformer的子集来迈出这个方向的初步步骤，形成没有集中训练的多任务模型。我们还建议在合并策略时可以获得更好的结果，如果所有策略都从共同的预训练初始化开始，并在问题特定的微调期间共同训练共享的辅助任务。一般来说，我们相信这个方向的研究可以帮助民主化和分发具有一般能力的代理的过程。

    Recent work has shown the promise of creating generalist, transformer-based, policies for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies, by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in weight space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also propose that when merging policies, we can obtain better results if all policies start from common, pre-trained initializations, while also co-training on shared auxiliary tasks during problem-specific finetuning. In general, we believe research in this direction can help democratize and distribute the process of which forms generally capable agents.
    
[^71]: 受限对抗学习及其在自动化软件测试中的应用：系统综述（arXiv:2303.07546v1 [cs.SE]）

    Constrained Adversarial Learning and its applicability to Automated Software Testing: a systematic review. (arXiv:2303.07546v1 [cs.SE])

    [http://arxiv.org/abs/2303.07546](http://arxiv.org/abs/2303.07546)

    本综述研究了受限对抗学习方法和自动化软件测试中受限数据生成方法的最新技术应用，探讨将这些方法整合至测试工具中以提高数字系统的鲁棒性和弹性。

    

    每种新技术都会增加隐含的漏洞，让越来越多的网络攻击者利用。自动化软件测试可以成为快速分析数千行代码的有前途的解决方案，通过生成和略微修改功能特定的测试数据来遇到多个漏洞和攻击向量。这个过程与受限对抗学习方法生成的受限性对抗性示例相似，因此将这些方法整合到自动化测试工具中可能会有显着的好处。因此，本系统综述侧重于限制数据生成方法在对抗学习和软件测试中的应用的当前最新技术，旨在指导研究人员和开发人员使用对抗学习方法增强测试工具，提高数字系统的弹性和鲁棒性。对于对抗机器学习的发现受限制的数据生成应用是系统化的。

    Every novel technology adds hidden vulnerabilities ready to be exploited by a growing number of cyber-attacks. Automated software testing can be a promising solution to quickly analyze thousands of lines of code by generating and slightly modifying function-specific testing data to encounter a multitude of vulnerabilities and attack vectors. This process draws similarities to the constrained adversarial examples generated by adversarial learning methods, so there could be significant benefits to the integration of these methods in automated testing tools. Therefore, this systematic review is focused on the current state-of-the-art of constrained data generation methods applied for adversarial learning and software testing, aiming to guide researchers and developers to enhance testing tools with adversarial learning methods and improve the resilience and robustness of their digital systems. The found constrained data generation applications for adversarial machine learning were systemat
    
[^72]: WDiscOOD：通过白化线性判别分析进行区分度优化的OOD检测

    WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])

    [http://arxiv.org/abs/2303.07543](http://arxiv.org/abs/2303.07543)

    本论文提出了一种名为WDiscOOD的新型OOD检测方法，其中使用白化线性判别分析将特征投影到判别子空间和残留子空间中，确定OOD分数。在大规模ImageNet-1k基准测试和六个OOD数据集中，WDiscOOD表现出了优越的性能。

    

    深度神经网络容易在遇到未知概念的情形下产生过度自信但错误的预测。这个挑战突显了在开放世界中检测OOD样本的重要性。本文提出了一种新颖的特征空间OOD检测分数，同时结合了类别特定和类别不可知的信息。具体地，我们的方法使用白化线性判别分析将特征投影到两个子空间中——判别子空间和残留子空间，其中ID类在判别子空间中被最大化地分离，并在残差子空间中被紧密地聚类。然后，在两个子空间中将来自输入数据与ID分布的偏差组合起来确定OOD分数。我们的方法名为WDiscOOD，在覆盖多种分布偏移的六个OOD数据集上验证了其高效性，包括大规模ImageNet-1k基准测试。WDiscOOD在深度分类器上表现出了优越的性能。

    Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
    
[^73]: 基于张量的多模态学习预测心脏MRI中肺动脉楔压力

    Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI. (arXiv:2303.07540v1 [cs.LG])

    [http://arxiv.org/abs/2303.07540](http://arxiv.org/abs/2303.07540)

    提出了一种基于张量学习的流程，从多模态心脏磁共振成像（MRI）中识别肺动脉楔压力（PAWP）。通过整合多种特征信息，提高了识别准确度。

    

    心衰是一种严重的生命威胁疾病，会导致左心室压力升高。肺动脉楔压力（PAWP）是一个重要的代理标志，表示左心室的高压。PAWP 由右心导管检查（RHC）确定，但它是一种有创性的过程。通过非侵入性的方法，可以快速地从大量人群中识别高危患者。在这项工作中，我们开发了一个基于张量学习的流程，从多模态心脏磁共振成像（MRI）中识别PAWP。这个流程提取高维扫描中的空间和时间特征。为了质量控制，我们采用认识不确定性为基础的分组策略，以识别质量差的训练样本。为了提高性能，我们通过整合多模态数据的特征：心脏MRI与短轴和四腔视图以及电子病历，学习互补信息。这项实验分析对大型数据集进行了验证。

    Heart failure is a serious and life-threatening condition that can lead to elevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure (PAWP) is an important surrogate marker indicating high pressure in the left ventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an invasive procedure. A non-invasive method is useful in quickly identifying high-risk patients from a large population. In this work, we develop a tensor learning-based pipeline for identifying PAWP from multimodal cardiac Magnetic Resonance Imaging (MRI). This pipeline extracts spatial and temporal features from high-dimensional scans. For quality control, we incorporate an epistemic uncertainty-based binning strategy to identify poor-quality training samples. To improve the performance, we learn complementary information by integrating features from multimodal data: cardiac MRI with short-axis and four-chamber views, and Electronic Health Records. The experimental analysis on a large
    
[^74]: HiSSNet: 面向低资源头戴式耳机的层次原型网络进行声音事件检测与说话人识别

    HiSSNet: Sound Event Detection and Speaker Identification via Hierarchical Prototypical Networks for Low-Resource Headphones. (arXiv:2303.07538v1 [cs.LG])

    [http://arxiv.org/abs/2303.07538](http://arxiv.org/abs/2303.07538)

    本研究介绍了一种名为HiSSNet的层次原型网络，针对低资源头戴式耳机的声音事件检测和说话人识别进行优化，使其可以根据用户的隐式和显式交互学习特定的重要声音。

    

    现代降噪耳机通过消除不需要的背景噪音显着提高了用户的听觉体验，但它们也可能会阻挡用户关注的声音。声音事件检测（SED）和说话人识别（SID）的机器学习模型可以使耳机有选择性地通过重要的声音，但将这些模型实现为用户中心的体验存在几个独特的挑战。本文提出使用HiSSNet（Hierarchical SED和SID Network）解决这些挑战。

    Modern noise-cancelling headphones have significantly improved users' auditory experiences by removing unwanted background noise, but they can also block out sounds that matter to users. Machine learning (ML) models for sound event detection (SED) and speaker identification (SID) can enable headphones to selectively pass through important sounds; however, implementing these models for a user-centric experience presents several unique challenges. First, most people spend limited time customizing their headphones, so the sound detection should work reasonably well out of the box. Second, the models should be able to learn over time the specific sounds that are important to users based on their implicit and explicit interactions. Finally, such models should have a small memory footprint to run on low-power headphones with limited on-chip memory. In this paper, we propose addressing these challenges using HiSSNet (Hierarchical SED and SID Network). HiSSNet is an SEID (SED and SID) model th
    
[^75]: 基于分数阶动力学的深度学习有助于COPD分级预测

    Fractional dynamics foster deep learning of COPD stage prediction. (arXiv:2303.07537v1 [cs.LG])

    [http://arxiv.org/abs/2303.07537](http://arxiv.org/abs/2303.07537)

    本文利用分数阶动力学分析来进行COPD的诊断，可以从生理信号中提取不同阶段的特征信号，进而发展出可预测COPD分级的深度神经网络。

    

    慢性阻塞性肺疾病（COPD）是全球死亡原因之一。常规的COPD诊断方法（如肺功能检测）可能不太可靠，因为测试需要测试者和被测试者的充分努力。此外，早期诊断也是具有挑战性的。本文构建了两个新的生理信号数据集（来自WestRo COPD数据集的54名患者的4432份记录和来自WestRo Porti COPD数据集的534名患者的13824份病历），揭示其复杂的耦合分形动力学特性并进行分数阶动力学深度学习分析，用于COPD的诊断。作者发现，分数阶动态建模可以从生理信号中提取出各个COPD阶段的独特信号，并且利用这些信号构建和训练了一个深度神经网络，用于基于生理信号预测COPD分级。

    Chronic obstructive pulmonary disease (COPD) is one of the leading causes of death worldwide. Current COPD diagnosis (i.e., spirometry) could be unreliable because the test depends on an adequate effort from the tester and testee. Moreover, the early diagnosis of COPD is challenging. We address COPD detection by constructing two novel physiological signals datasets (4432 records from 54 patients in the WestRo COPD dataset and 13824 medical records from 534 patients in the WestRo Porti COPD dataset). The authors demonstrate their complex coupled fractal dynamical characteristics and perform a fractional-order dynamics deep learning analysis to diagnose COPD. The authors found that the fractional-order dynamical modeling can extract distinguishing signatures from the physiological signals across patients with all COPD stages from stage 0 (healthy) to stage 4 (very severe). They use the fractional signatures to develop and train a deep neural network that predicts COPD stages based on the
    
[^76]: 基于强化学习的路径规划：一种策略迭代方法

    Path Planning using Reinforcement Learning: A Policy Iteration Approach. (arXiv:2303.07535v1 [cs.LG])

    [http://arxiv.org/abs/2303.07535](http://arxiv.org/abs/2303.07535)

    本研究提出了一种基于自动调整器的序数回归方法，以加速探索强化学习算法的参数，并加速收敛到最优策略。该方法可以提供1.82倍的峰值加速和1.48倍的平均加速比。

    

    随着实时处理的影响在最近被认识到，对于强化学习算法的高效实现的需求也越来越迫切。尽管RL算法中利用Bellman方程具有许多优点，但是设计参数的大搜索空间也是不可避免的。本研究旨在阐明与强化学习参数相关的设计空间探索，特别是策略迭代方面。考虑到微调强化学习算法的参数需要大量的计算开销，我们提出了一种基于自动调节器的序数回归方法，以加速探索这些参数的过程，并加速收敛到最优策略。我们的方法提供了1.82倍的峰值加速，平均加速比为1.48倍，比之前的最先进技术有显著的提升。

    With the impact of real-time processing being realized in the recent past, the need for efficient implementations of reinforcement learning algorithms has been on the rise. Albeit the numerous advantages of Bellman equations utilized in RL algorithms, they are not without the large search space of design parameters.  This research aims to shed light on the design space exploration associated with reinforcement learning parameters, specifically that of Policy Iteration. Given the large computational expenses of fine-tuning the parameters of reinforcement learning algorithms, we propose an auto-tuner-based ordinal regression approach to accelerate the process of exploring these parameters and, in return, accelerate convergence towards an optimal policy. Our approach provides 1.82x peak speedup with an average of 1.48x speedup over the previous state-of-the-art.
    
[^77]: 通过核范数正则化实现领域通用性

    Domain Generalization via Nuclear Norm Regularization. (arXiv:2303.07527v1 [cs.LG])

    [http://arxiv.org/abs/2303.07527](http://arxiv.org/abs/2303.07527)

    本文提出了一种基于核范数正则化的通用性正则化方法，能够降低环境特征的影响并鼓励学习领域不变的特征，能够在广泛的领域通用性任务中获得比基线更强的性能。

    

    在现实世界中，机器学习系统在仅有有限域的数据的情况下具备对未知领域的泛化能力尤为重要。本文提出了一种基于学习特征核范数的简单有效的领域通用性正则化方法。直观上，所提出的正则化方法减少了环境特征的影响，鼓励学习领域不变的特征。从理论上讲，我们提供了有关为什么相比于最小化经验风险或其他正则化方法，核范数正则化更加有效的见解。实验方面，我们在合成和实际数据集上进行了大量试验证明，核范数正则化在广泛的领域通用性任务中与基线相比具有强大的性能。此外，我们的正则化方法适用于各种方法，如ERM和SWAD，且表现持续提高，例如测试准确率提高了1.7％和0.9％。

    The ability to generalize to unseen domains is crucial for machine learning systems deployed in the real world, especially when we only have data from limited training domains. In this paper, we propose a simple and effective regularization method based on the nuclear norm of the learned features for domain generalization. Intuitively, the proposed regularizer mitigates the impacts of environmental features and encourages learning domain-invariant features. Theoretically, we provide insights into why nuclear norm regularization is more effective compared to ERM and alternative regularization methods. Empirically, we conduct extensive experiments on both synthetic and real datasets. We show that nuclear norm regularization achieves strong performance compared to baselines in a wide range of domain generalization tasks. Moreover, our regularizer is broadly applicable with various methods such as ERM and SWAD with consistently improved performance, e.g., 1.7% and 0.9% test accuracy improv
    
[^78]: 利用量子自然语言处理自动检测软件源代码中的漏洞

    Automated Vulnerability Detection in Source Code Using Quantum Natural Language Processing. (arXiv:2303.07525v1 [cs.LG])

    [http://arxiv.org/abs/2303.07525](http://arxiv.org/abs/2303.07525)

    该研究使用基于深度神经网络模型和量子机器学习模型的漏洞检测方法，利用庞大的开源函数数据集来自动检测软件源代码中的漏洞。

    

    软件源代码中漏洞的存在是软件代码审计领域中最重要的挑战之一。这些缺陷很可能被攻击并导致系统妥协、数据泄漏或服务拒绝。我们使用C和C++开源代码创建了一个大规模的经典机器学习和量子机器学习系统，以用于功能级漏洞识别。我们组装了一个由数百万个开源函数组成的庞大数据集，可指向潜在漏洞。我们基于深度神经网络模型Long Short Term Memory (LSTM)和量子机器学习模型Long Short Term Memory (QLSTM)创建了一种高效和可伸缩的漏洞检测方法，可以学习从源代码中提取的特征。源代码首先被转换为最小中间表示形式，以消除无关组件并缩短依赖性。因此，我们保持语义和句法信息。

    One of the most important challenges in the field of software code audit is the presence of vulnerabilities in software source code. These flaws are highly likely ex-ploited and lead to system compromise, data leakage, or denial of ser-vice. C and C++ open source code are now available in order to create a large-scale, classical machine-learning and quantum machine-learning system for function-level vulnerability identification. We assembled a siz-able dataset of millions of open-source functions that point to poten-tial exploits. We created an efficient and scalable vulnerability detection method based on a deep neural network model Long Short Term Memory (LSTM), and quantum machine learning model Long Short Term Memory (QLSTM), that can learn features extracted from the source codes. The source code is first converted into a minimal intermediate representation to remove the pointless components and shorten the de-pendency. Therefore, We keep the semantic and syntactic information usi
    
[^79]: 机器人导航的音视语言地图

    Audio Visual Language Maps for Robot Navigation. (arXiv:2303.07522v1 [cs.RO])

    [http://arxiv.org/abs/2303.07522](http://arxiv.org/abs/2303.07522)

    该论文提出了一种音视语言地图(AVLMaps)，用于存储跨模态信息，实现机器人根据多模态查询在地图中索引目标的导航方式。在模拟实验中，AVLMaps实现了从多模态提示的零次学习式多模态目标导航，并提供了更好的召回率。

    

    与世界的互动是一种多感官的体验，但是许多机器人仍然主要依赖视觉感知来绘制和导航他们的环境。本文提出了音视语言地图(AVLMaps)，这是一个统一的3D空间地图表示，用于存储来自音频、视觉和语言线索的跨模态信息。在导航的情境下，我们展示了AVLMaps能够使机器人系统根据多模态查询(例如，文本描述、图像或地标的音频片段)在地图中索引目标。特别是，添加音频信息使机器人能够更可靠地消除目标位置的歧义性。在模拟实验中，我们展示了AVLMaps能够实现从多模态提示进行零次学习的多模态目标导航，并在模糊场景中提供50%更好的召回率。

    While interacting in the world is a multi-sensory experience, many robots continue to predominantly rely on visual perception to map and navigate in their environments. In this work, we propose Audio-Visual-Language Maps (AVLMaps), a unified 3D spatial map representation for storing cross-modal information from audio, visual, and language cues. AVLMaps integrate the open-vocabulary capabilities of multimodal foundation models pre-trained on Internet-scale data by fusing their features into a centralized 3D voxel grid. In the context of navigation, we show that AVLMaps enable robot systems to index goals in the map based on multimodal queries, e.g., textual descriptions, images, or audio snippets of landmarks. In particular, the addition of audio information enables robots to more reliably disambiguate goal locations. Extensive experiments in simulation show that AVLMaps enable zero-shot multimodal goal navigation from multimodal prompts and provide 50% better recall in ambiguous scenar
    
[^80]: SuperMask：从多视角不对齐的低分辨率磁共振图像中生成高分辨率目标掩模

    SuperMask: Generating High-resolution object masks from multi-view, unaligned low-resolution MRIs. (arXiv:2303.07517v1 [eess.IV])

    [http://arxiv.org/abs/2303.07517](http://arxiv.org/abs/2303.07517)

    本论文提出了一种基于弱监督深度学习的方法，从多个低分辨率图像中生成高分辨率的目标掩模。该方法结合了分割和无监督注册网络，引入两个新的规范化方法使得配准和分割相互增强，并提出了多视角融合方法。实验结果表明，该方法在合成和真实数据集上均达到了最先进的性能。

    

    三维磁共振图像（MRI）中的分割是具有挑战性的，因为高分辨率的等向性MRI很少，而典型的MRI是非等向性的，其中的平面分辨率远低于体素分辨率。解决这个问题的一个方法是常常在不同的平面上获取多个序列，然而在实践中，这些序列不正交于彼此，限制了许多以前的解决方案重构多个低分辨率图像以得到高分辨率图像的适用性。我们提出了一种基于弱监督的深度学习方法来从多个低分辨率图像中生成高分辨率掩模。我们的方法通过引入两个新的规范化方法使得配准和分割相互增强，结合了分割和无监督注册网络。最后，我们引入多视角融合方法来生成高分辨率目标对象掩模。实验结果表明，我们的方法在合成和真实数据集上均达到了最先进的性能。

    Three-dimensional segmentation in magnetic resonance images (MRI), which reflects the true shape of the objects, is challenging since high-resolution isotropic MRIs are rare and typical MRIs are anisotropic, with the out-of-plane dimension having a much lower resolution. A potential remedy to this issue lies in the fact that often multiple sequences are acquired on different planes. However, in practice, these sequences are not orthogonal to each other, limiting the applicability of many previous solutions to reconstruct higher-resolution images from multiple lower-resolution ones. We propose a weakly-supervised deep learning-based solution to generating high-resolution masks from multiple low-resolution images. Our method combines segmentation and unsupervised registration networks by introducing two new regularizations to make registration and segmentation reinforce each other. Finally, we introduce a multi-view fusion method to generate high-resolution target object masks. The exper
    
[^81]: 连续深度强化学习中的可塑性丧失

    Loss of Plasticity in Continual Deep Reinforcement Learning. (arXiv:2303.07507v1 [cs.LG])

    [http://arxiv.org/abs/2303.07507](http://arxiv.org/abs/2303.07507)

    本文研究了在连续变化的环境中，深度强化学习代理程序在执行一系列游戏时失去可塑性。研究发现网络的激活足迹变得稀疏导致梯度变小。

    

    在这篇论文中，我们研究了经典基于值的深度强化学习方法在不断变化的环境下的行为。特别地，我们证明了当深度强化学习代理程序循环执行Atari 2600游戏时，它们失去了学习良好策略的能力。我们在多个实验中进行了实验，并分析了权重、梯度和激活在时间上如何变化。我们的分析表明，网络的激活足迹变得更加稀疏，导致梯度减小。

    The ability to learn continually is essential in a complex and changing world. In this paper, we characterize the behavior of canonical value-based deep reinforcement learning (RL) approaches under varying degrees of non-stationarity. In particular, we demonstrate that deep RL agents lose their ability to learn good policies when they cycle through a sequence of Atari 2600 games. This phenomenon is alluded to in prior work under various guises -e.g., loss of plasticity, implicit under-parameterization, primacy bias, and capacity loss. We investigate this phenomenon closely at scale and analyze how the weights, gradients, and activations change over time in several experiments with varying dimensions (e.g., similarity between games, number of games, number of frames per game), with some experiments spanning 50 days and 2 billion environment interactions. Our analysis shows that the activation footprint of the network becomes sparser, contributing to the diminishing gradients. We inves
    
[^82]: 元学习法在小样本学习中的应用：近期进展综述

    Meta-learning approaches for few-shot learning: A survey of recent advances. (arXiv:2303.07502v1 [cs.LG])

    [http://arxiv.org/abs/2303.07502](http://arxiv.org/abs/2303.07502)

    本文综述了元学习在小样本学习中的应用，调查了最先进的方法，并讨论了当前挑战和未来研究方向。

    

    尽管深度学习在学习多维数据方面取得了惊人的成功，但由于其专注于同分布预测，导致其在新的未见任务上表现不佳。此外，深度学习因样本不足而通常表现不佳。元学习是一种有前途的方法，可以通过适应少量样本数据来解决这些问题。本综述首先简要介绍了元学习，然后调查了最先进的元学习方法以及(I) 基于度量的、(II) 基于记忆的、(III) 和基于学习的方法的最新技术进展。最后，讨论了当前的挑战和未来研究的见解。

    Despite its astounding success in learning deeper multi-dimensional data, the performance of deep learning declines on new unseen tasks mainly due to its focus on same-distribution prediction. Moreover, deep learning is notorious for poor generalization from few samples. Meta-learning is a promising approach that addresses these issues by adapting to new tasks with few-shot datasets. This survey first briefly introduces meta-learning and then investigates state-of-the-art meta-learning methods and recent advances in: (I) metric-based, (II) memory-based, (III), and learning-based methods. Finally, current challenges and insights for future researches are discussed.
    
[^83]: 使用VAE学习潜在变量：在cryo-EM中的应用观察（arXiv:2303.07487v1 [stat.ML]）

    Using VAEs to Learn Latent Variables: Observations on Applications in cryo-EM. (arXiv:2303.07487v1 [stat.ML])

    [http://arxiv.org/abs/2303.07487](http://arxiv.org/abs/2303.07487)

    本研究通过定性分析，发现VAE在生物应用中摊销潜在变量的特性与传统显式表示方法相似。

    

    变分自编码器（VAEs）是一种流行的生成模型，用于近似分布。VAE的编码器部分用于认证学习潜在变量，为数据样本生成潜在表示。最近，VAEs已用于表征物理和生物系统。在这个案例研究中，我们定性地研究了VAE在生物应用中的摊销特性。我们发现，在这种应用中，编码器与更传统的显式潜在变量表示具有定性相似性。

    Variational autoencoders (VAEs) are a popular generative model used to approximate distributions. The encoder part of the VAE is used in amortized learning of latent variables, producing a latent representation for data samples. Recently, VAEs have been used to characterize physical and biological systems. In this case study, we qualitatively examine the amortization properties of a VAE used in biological applications. We find that in this application the encoder bears a qualitative resemblance to more traditional explicit representation of latent variables.
    
[^84]: 引导式语音增强网络

    Guided Speech Enhancement Network. (arXiv:2303.07486v1 [eess.AS])

    [http://arxiv.org/abs/2303.07486](http://arxiv.org/abs/2303.07486)

    本文提出一种引导式语音增强网络，将原始麦克风与波束形成器的输出均作为机器学习模型的输入，通过训练从波束形成器的提示中学习，提高了模型的定向拒绝能力，同时具有降噪和去混响等通用任务。

    

    为了提高语音采集的质量，多麦克风语音增强技术已被广泛研究。这篇论文提出了一种新的语音增强解决方案，将原始麦克风与波束形成器的输出均作为机器学习模型的输入。通过对比两个输入来训练模型，使模型从波束形成器的提示中学习，大大提高了模型的定向拒绝能力，同时还具有降噪和去混响等通用任务。该解决方案利用经典的空间滤波算法，而非与之竞争。

    High quality speech capture has been widely studied for both voice communication and human computer interface reasons. To improve the capture performance, we can often find multi-microphone speech enhancement techniques deployed on various devices. Multi-microphone speech enhancement problem is often decomposed into two decoupled steps: a beamformer that provides spatial filtering and a single-channel speech enhancement model that cleans up the beamformer output. In this work, we propose a speech enhancement solution that takes both the raw microphone and beamformer outputs as the input for an ML model. We devise a simple yet effective training scheme that allows the model to learn from the cues of the beamformer by contrasting the two inputs and greatly boost its capability in spatial rejection, while conducting the general tasks of denoising and dereverberation. The proposed solution takes advantage of classical spatial filtering algorithms instead of competing with them. By design, 
    
[^85]: 一般损失函数在高维情况下导致（近似）插值

    General Loss Functions Lead to (Approximate) Interpolation in High Dimensions. (arXiv:2303.07475v1 [stat.ML])

    [http://arxiv.org/abs/2303.07475](http://arxiv.org/abs/2303.07475)

    本研究提供了一般凸损失函数和超参数化阶段下梯度下降的隐含偏差的近似表征。具体而言是近似最小范数插值。

    

    我们提供了一个统一的框架，适用于一般凸损失函数和超参数化阶段的二元和多元分类设置，以近似地表征梯度下降的隐含偏差。具体而言，我们展示了在高维情况下梯度下降的隐含偏差近似于最小范数插值，最小范数插值来自于对平方损失的训练。与之前专门针对指数尾损失并使用中间支持向量机公式的研究不同，我们的框架直接基于Ji和Telgarsky（2021）的原始-对偶分析方法，通过新颖的敏感度分析提供了一般凸损失的新近似等效性结果。我们的框架还恢复了二元和多元分类设置下指数尾损失的现有精确等效结果。最后，我们提供了我们技术的紧密性的证据，我们使用这些证据来演示我们的方法的有效性。

    We provide a unified framework, applicable to a general family of convex losses and across binary and multiclass settings in the overparameterized regime, to approximately characterize the implicit bias of gradient descent in closed form. Specifically, we show that the implicit bias is approximated (but not exactly equal to) the minimum-norm interpolation in high dimensions, which arises from training on the squared loss. In contrast to prior work which was tailored to exponentially-tailed losses and used the intermediate support-vector-machine formulation, our framework directly builds on the primal-dual analysis of Ji and Telgarsky (2021), allowing us to provide new approximate equivalences for general convex losses through a novel sensitivity analysis. Our framework also recovers existing exact equivalence results for exponentially-tailed losses across binary and multiclass settings. Finally, we provide evidence for the tightness of our techniques, which we use to demonstrate the ef
    
[^86]: 在内存中加速Transformer

    X-Former: In-Memory Acceleration of Transformers. (arXiv:2303.07470v1 [cs.LG])

    [http://arxiv.org/abs/2303.07470](http://arxiv.org/abs/2303.07470)

    X-Former是一种内存加速器，它利用注意矩阵的稀疏性，显著加速Transformer计算，同时保持高精度。

    

    由于注意力机制可以将每个单词相对于序列中的其他单词分配一个重要性分数，Transformer在各种自然语言处理（NLP）任务中取得了巨大的成功。然而，这些模型非常大，往往达到数千亿个参数，因此需要大量的DRAM访问。因此，传统的深度神经网络（DNN）加速器，如GPU和TPU，在高效处理Transformer方面面临限制。基于非易失性内存的内存加速器有望成为解决此挑战的有效解决方案，因为它们提供高存储密度，同时在存储器阵列内执行大规模并行矩阵向量乘法。然而，与CNN和RNN不同，Transformer中经常使用注意得分计算，这需要矩阵向量乘法（MVM），其中每个输入的两个操作数都会动态更改。因此，传统的基于NVM的加速器会由于所需的内存访问量增加而产生高开销。为了解决这个问题，我们提出了X-Former，这是一种内存加速器，利用注意矩阵中的稀疏性来减少MVM操作所需的内存访问次数。多个Transformer模型的实验结果表明，X-Former显着加速Transformer计算，同时保持高精度。

    Transformers have achieved great success in a wide variety of natural language processing (NLP) tasks due to the attention mechanism, which assigns an importance score for every word relative to other words in a sequence. However, these models are very large, often reaching hundreds of billions of parameters, and therefore require a large number of DRAM accesses. Hence, traditional deep neural network (DNN) accelerators such as GPUs and TPUs face limitations in processing Transformers efficiently. In-memory accelerators based on non-volatile memory promise to be an effective solution to this challenge, since they provide high storage density while performing massively parallel matrix vector multiplications within memory arrays. However, attention score computations, which are frequently used in Transformers (unlike CNNs and RNNs), require matrix vector multiplications (MVM) where both operands change dynamically for each input. As a result, conventional NVM-based accelerators incur hig
    
[^87]: 使用联邦学习进行网络异常检测

    Network Anomaly Detection Using Federated Learning. (arXiv:2303.07452v1 [cs.LG])

    [http://arxiv.org/abs/2303.07452](http://arxiv.org/abs/2303.07452)

    本文使用联邦学习的方法解决了网络异常检测中的可扩展性和隐私保护问题，并提出了一个可在低端到中端设备上使用的深度神经网络框架，实现了最先进的准确性和隐私保护。

    

    由于网络流量的真实性和异构性，检测异常事件非常具有挑战性。全局服务器上的计算负荷在效率、准确性和可扩展性方面是一个重要的挑战。本文的主要动机是介绍一个强大且可扩展的框架，以实现高效的网络异常检测。我们利用联邦学习来解决网络异常检测的可扩展性和高效性问题，多个参与者联合训练一个全局模型。与集中式训练架构不同，联邦学习不需要参与者将他们的训练数据上传到服务器，从而防止攻击者利用训练数据。此外，大多数先前的工作都集中于传统的集中式机器学习，联合机器学习在网络异常检测领域的研究尚不充分。因此，我们提出了一个深度神经网络框架，可以在低端到中端设备上检测网络异常并保护隐私。我们在公开数据集上评估了我们的框架，证明了我们的方法可以在保护隐私的同时实现最先进的准确性。

    Due to the veracity and heterogeneity in network traffic, detecting anomalous events is challenging. The computational load on global servers is a significant challenge in terms of efficiency, accuracy, and scalability. Our primary motivation is to introduce a robust and scalable framework that enables efficient network anomaly detection. We address the issue of scalability and efficiency for network anomaly detection by leveraging federated learning, in which multiple participants train a global model jointly. Unlike centralized training architectures, federated learning does not require participants to upload their training data to the server, preventing attackers from exploiting the training data. Moreover, most prior works have focused on traditional centralized machine learning, making federated machine learning under-explored in network anomaly detection. Therefore, we propose a deep neural network framework that could work on low to mid-end devices detecting network anomalies wh
    
[^88]: 使用相位特征的盲音频房间参数估计

    Blind Acoustic Room Parameter Estimation Using Phase Features. (arXiv:2303.07449v1 [eess.AS])

    [http://arxiv.org/abs/2303.07449](http://arxiv.org/abs/2303.07449)

    本研究提出了一种利用与相位相关的特征进行盲目估计所谓的“混响指纹”参数的方法，该方法优于仅使用幅度谱特征的方法。

    

    在现场进行房间声学建模需要从嘈杂和混响的音频中进行一定程度的盲目参数估计。现代方法利用卷积神经网络(CNN)与时频表征相结合。使用短时傅里叶变换来开发这些类似于频谱图的特征表明具有很好的效果，但该方法在相位领域隐式丢弃了大量音频信息。受语音增强领域的最新工作启发，我们提出利用新的与相位相关的特征扩展最近的方法以盲目地估计所谓的"混响指纹"参数，即音量和RT60。这些特征的添加在各种声学空间中优于仅依赖于幅度基础频谱特征的现有方法。我们使用一组新数据集在单参数和多参数估计策略中评估了部署这些新特征的有效性。

    Modeling room acoustics in a field setting involves some degree of blind parameter estimation from noisy and reverberant audio. Modern approaches leverage convolutional neural networks (CNNs) in tandem with time-frequency representation. Using short-time Fourier transforms to develop these spectrogram-like features has shown promising results, but this method implicitly discards a significant amount of audio information in the phase domain. Inspired by recent works in speech enhancement, we propose utilizing novel phase-related features to extend recent approaches to blindly estimate the so-called "reverberation fingerprint" parameters, namely, volume and RT60. The addition of these features is shown to outperform existing methods that rely solely on magnitude-based spectral features across a wide range of acoustics spaces. We evaluate the effectiveness of the deployment of these novel features in both single-parameter and multi-parameter estimation strategies, using a novel dataset th
    
[^89]: 不完全可观测Atari游戏中的无监督表示学习

    Unsupervised Representation Learning in Partially Observable Atari Games. (arXiv:2303.07437v1 [cs.LG])

    [http://arxiv.org/abs/2303.07437](http://arxiv.org/abs/2303.07437)

    本文提出了一种针对部分可观测状态的无监督状态表示学习方法PO-ST-DIM，改进了ST-DIM的对比方法，并在Atari游戏中取得了与监督方法相当的表现。

    

    状态表示学习旨在捕捉环境的潜在因素。在以前的状态表示学习研究中，对比方法比生成模型表现更好。尽管一些研究人员意识到掩蔽图像建模和对比学习表示之间的联系，但努力集中在使用掩模作为增强技术来更好地表示潜在的生成因素。利用无监督状态表示学习方法仔细研究强化学习中的不完全可观察环境尚未得到充分研究。在本文中，我们针对部分可观测状态创建了一种无监督状态表示学习方案。我们在早期的Atari 2600框架上进行了实验，该框架旨在评估表示学习模型。一种名为时空深度信息最大化（ST-DIM）的对比方法在这个基准测试中展示了最新的性能，但仍然不如其监督对应物。我们的方法称为部分可观测ST-DIM（PO-ST-DIM），通过合并部分可观测方案来改进对比方法。PO-ST-DIM优于ST-DIM，并且相对于监督方法具有竞争性能。

    State representation learning aims to capture latent factors of an environment. Contrastive methods have performed better than generative models in previous state representation learning research. Although some researchers realize the connections between masked image modeling and contrastive representation learning, the effort is focused on using masks as an augmentation technique to represent the latent generative factors better. Partially observable environments in reinforcement learning have not yet been carefully studied using unsupervised state representation learning methods.  In this article, we create an unsupervised state representation learning scheme for partially observable states. We conducted our experiment on a previous Atari 2600 framework designed to evaluate representation learning models. A contrastive method called Spatiotemporal DeepInfomax (ST-DIM) has shown state-of-the-art performance on this benchmark but remains inferior to its supervised counterpart. Our appr
    
[^90]: 基于机器学习的小型机载雷达数据存储技术研究

    Study on the Data Storage Technology of Mini-Airborne Radar Based on Machine Learning. (arXiv:2303.07407v1 [cs.AR])

    [http://arxiv.org/abs/2303.07407](http://arxiv.org/abs/2303.07407)

    提出了一种基于机器学习的小型机载雷达数据存储方法，可以显著降低文件管理时间，提高存储速度。

    

    在许多检测应用中，机载雷达的数据传输速率远高于无线数据传输速率，因此通常使用机载数据存储系统来存储雷达数据。具有良好抗震性能的数据存储系统通常使用NAND Flash作为存储介质，但存在长时间的文件管理问题，严重影响数据存储速度，特别是在平台小型化的限制下。为了解决这个问题，提出了一种基于机器学习的小型机载雷达数据存储方法。基于机器学习建立了存储训练模型，能够处理各种雷达数据，采用该模型对文件管理方法进行分类确定，然后应用于雷达数据的存储。为验证所提出方法的性能，对小型机载雷达的数据存储系统进行了测试。实验结果表明，基于机器学习的方法可显著降低文件管理时间，有效提高小型机载雷达数据存储系统的数据存储速度。

    The data rate of airborne radar is much higher than the wireless data transfer rate in many detection applications, so the onboard data storage systems are usually used to store the radar data. Data storage systems with good seismic performance usually use NAND Flash as storage medium, and there is a widespread problem of long file management time, which seriously affects the data storage speed, especially under the limitation of platform miniaturization. To solve this problem, a data storage method based on machine learning is proposed for mini-airborne radar. The storage training model is established based on machine learning, and could process various kinds of radar data. The file management methods are classified and determined using the model, and then are applied to the storage of radar data. To verify the performance of the proposed method, a test was carried out on the data storage system of a mini-airborne radar. The experimental results show that the method based on machine l
    
[^91]: 通过优化算法来调试支持向量机和提升树

    Tuning support vector machines and boosted trees using optimization algorithms. (arXiv:2303.07400v1 [stat.ML])

    [http://arxiv.org/abs/2303.07400](http://arxiv.org/abs/2303.07400)

    该论文研究了支持向量机和提升树的参数调优行为，并开发了一个R包EZtune自动调试模型。

    

    统计学习方法近年来越来越受欢迎，但其中很多方法需要对参数进行调优才能发挥良好的性能。我们研究了支持向量机、梯度提升机和Adaboost在分类和回归任务中的参数调优行为。我们使用网格搜索来确定参数调优的范围，并探索不同的优化算法在参数空间中选择最佳模型。我们将由优化算法选择的模型与网格搜索获得的最佳模型进行比较，选择效果优秀的算法。我们利用这些信息开发了一个R包EZtune，用于自动调试支持向量机和提升树。

    Statistical learning methods have been growing in popularity in recent years. Many of these procedures have parameters that must be tuned for models to perform well. Research has been extensive in neural networks, but not for many other learning methods. We looked at the behavior of tuning parameters for support vector machines, gradient boosting machines, and adaboost in both a classification and regression setting. We used grid search to identify ranges of tuning parameters where good models can be found across many different datasets. We then explored different optimization algorithms to select a model across the tuning parameter space. Models selected by the optimization algorithm were compared to the best models obtained through grid search to select well performing algorithms. This information was used to create an R package, EZtune, that automatically tunes support vector machines and boosted trees.
    
[^92]: 具有别名观测的潜在图的快速探索与学习

    Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])

    [http://arxiv.org/abs/2303.07397](http://arxiv.org/abs/2303.07397)

    本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。

    

    考虑这种场景：一个智能体通过执行操作从一个节点到另一个节点来导航潜在图。所选操作确定了下一个访问节点上的概率分布。在每个节点处，智能体收到一个观测，但该观测不是唯一的，因此它不能唯一地标识节点，这使得问题别名化。本文旨在提供一个政策，该政策约等于最大化探索效率（即在给定的探索预算下如何恢复图表）。在非别名化的情况下，我们展示了相对于现有最先进强化学习基线的改进性能。对于别名化的情况，我们不知道适用的基线，而是展示了在各种拓扑结构下相对于随机策略更快的恢复速度，并且对于具有挑战性的拓扑结构，恢复速度比随机策略快指数倍。我们将该算法称为 eFeX（来自于 efficient exploration 的缩写）。

    Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
    
[^93]: 多个学习智能体与基于智能体的市场模型的交互

    Many learning agents interacting with an agent-based market model. (arXiv:2303.07393v1 [q-fin.TR])

    [http://arxiv.org/abs/2303.07393](http://arxiv.org/abs/2303.07393)

    本论文介绍了多个强化学习最优执行交易智能体与反应式基于智能体的金融市场模型的交互。通过平衡执行差价和未能及时执行订单的惩罚，说明了奖励函数的作用。研究表明，学习智能体的数量、初始订单大小和状态空间的变化，会对最小智能市场模拟造成不同的影响。

    

    本文考虑了多个强化学习最优执行交易智能体与在事件时间下的反应式基于智能体的金融市场模型的动态和相互作用。模型代表了一个市场生态系统，由三个营养级别代表：最优执行学习智能体，最小智能的流动性需要者和快速的电子流动性提供者。最优执行代理类别包括买入和卖出代理，可以使用限价单和市价单的组合，或者仅使用市价单进行交易。奖励函数明确平衡了交易执行差价与未能及时执行订单的惩罚之间的关系。本文展示了多个竞争学习智能体如何随着智能体数量、初始订单的大小和用于学习的状态空间的函数影响最小智能市场模拟。我们使用相空间图来研究ABM的动态，当特定规范被应用

    We consider the dynamics and the interactions of multiple reinforcement learning optimal execution trading agents interacting with a reactive Agent-Based Model (ABM) of a financial market in event time. The model represents a market ecology with 3-trophic levels represented by: optimal execution learning agents, minimally intelligent liquidity takers, and fast electronic liquidity providers. The optimal execution agent classes include buying and selling agents that can either use a combination of limit orders and market orders, or only trade using market orders. The reward function explicitly balances trade execution slippage against the penalty of not executing the order timeously. This work demonstrates how multiple competing learning agents impact a minimally intelligent market simulation as functions of the number of agents, the size of agents' initial orders, and the state spaces used for learning. We use phase space plots to examine the dynamics of the ABM, when various specifica
    
[^94]: 使用集合卡尔曼反演的高效贝叶斯物理感知神经网络求解反问题

    Efficient Bayesian Physics Informed Neural Networks for Inverse Problems via Ensemble Kalman Inversion. (arXiv:2303.07392v1 [stat.ML])

    [http://arxiv.org/abs/2303.07392](http://arxiv.org/abs/2303.07392)

    本文提出了一种使用集合卡尔曼反演实现高效贝叶斯物理感知神经网络求解反问题的新推断算法，并且在提高计算效率的同时，实现了数据具有信息量的不确定性估计。

    

    贝叶斯物理感知神经网络已经在基于偏微分方程的问题中得到了广泛的应用，用于推断物理参数和学习正向解。然而，神经网络的过度参数化为高维后验推断带来了计算挑战。现有的推断方法，例如基于粒子或方差的推断方法，要么在高维后验推断时计算昂贵，要么提供不令人满意的不确定性估计。在本文中，我们提出了一种新的B-PINNs推断算法，利用集合卡尔曼反演(EKI)处理高维推断任务。我们发现，我们提出的方法可以在大大降低计算成本的情况下，实现具有信息量的不确定性估计，与基于哈密顿蒙特卡罗（HMC）的B-PINNs方法相当。这些发现表明，我们提出的方法在许多反问题的不确定性量化方面具有巨大的潜力。

    Bayesian Physics Informed Neural Networks (B-PINNs) have gained significant attention for inferring physical parameters and learning the forward solutions for problems based on partial differential equations. However, the overparameterized nature of neural networks poses a computational challenge for high-dimensional posterior inference. Existing inference approaches, such as particle-based or variance inference methods, are either computationally expensive for high-dimensional posterior inference or provide unsatisfactory uncertainty estimates. In this paper, we present a new efficient inference algorithm for B-PINNs that uses Ensemble Kalman Inversion (EKI) for high-dimensional inference tasks. We find that our proposed method can achieve inference results with informative uncertainty estimates comparable to Hamiltonian Monte Carlo (HMC)-based B-PINNs with a much reduced computational cost. These findings suggest that our proposed approach has great potential for uncertainty quantifi
    
[^95]: 稀疏神经网络中的神经元进化监督特征选择

    Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks. (arXiv:2303.07200v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2303.07200](http://arxiv.org/abs/2303.07200)

    本文提出了一种使用神经元进化的稀疏神经网络方法，名为 NeuroFS，能够有效地派生出信息丰富的特征子集，且在实验中具有最高排名得分。

    

    特征选择从数据中选择信息量高的变量子集，不仅可以增强模型解释性和性能，而且可以减轻资源需求。最近，越来越多的人开始关注使用神经网络进行特征选择。然而，现有的方法通常在应用于高维度数据集时会受到高计算成本的困扰。在本文中，我们受到进化过程的启发，提出了一种使用稀疏神经网络的新型资源有效的监督式特征选择方法，命名为 "NeuroFS"。通过从零开始训练的稀疏神经网络逐步修剪无信息的特征，NeuroFS有效地派生出一个信息丰富的特征子集。通过对 $11$ 种不同类型的低维和高维真实世界基准数据集进行多个实验，我们证明了 NeuroFS 在考虑到最先进的监督特征选择模型时具有最高的排名得分。

    Feature selection that selects an informative subset of variables from data not only enhances the model interpretability and performance but also alleviates the resource demands. Recently, there has been growing attention on feature selection using neural networks. However, existing methods usually suffer from high computational costs when applied to high-dimensional datasets. In this paper, inspired by evolution processes, we propose a novel resource-efficient supervised feature selection method using sparse neural networks, named \enquote{NeuroFS}. By gradually pruning the uninformative features from the input layer of a sparse neural network trained from scratch, NeuroFS derives an informative subset of features efficiently. By performing several experiments on $11$ low and high-dimensional real-world benchmarks of different types, we demonstrate that NeuroFS achieves the highest ranking-based score among the considered state-of-the-art supervised feature selection models. The code 
    
[^96]: 用元学习优化改进物理知识注入神经网络

    Improving physics-informed neural networks with meta-learned optimization. (arXiv:2303.07127v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.07127](http://arxiv.org/abs/2303.07127)

    用元学习优化方法训练物理知识注入神经网络，可以显著减少网络求解微分方程系统的误差，并且具有很强的迁移学习能力。

    

    我们表明，利用元学习优化方法训练物理知识注入神经网络能够显著减少这些网络用于求解微分方程系统时的误差，而不是传统地使用固定的手工制作优化器。我们选择一种可学习的优化方法（基于一个浅的多层感知器），这种方法是针对特定类型的微分方程进行元训练的。我们展示了几个在数学物理学中具有实际意义的方程的元训练优化器，包括线性平流方程、泊松方程、Korteweg-de Vries方程和Burgers方程。我们还展示了元学习优化器具有迁移学习能力，即用于一个微分方程的元训练优化器也可以成功地在另一个微分方程上使用。

    We show that the error achievable using physics-informed neural networks for solving systems of differential equations can be substantially reduced when these networks are trained using meta-learned optimization methods rather than to using fixed, hand-crafted optimizers as traditionally done. We choose a learnable optimization method based on a shallow multi-layer perceptron that is meta-trained for specific classes of differential equations. We illustrate meta-trained optimizers for several equations of practical relevance in mathematical physics, including the linear advection equation, Poisson's equation, the Korteweg--de Vries equation and Burgers' equation. We also illustrate that meta-learned optimizers exhibit transfer learning abilities, in that a meta-trained optimizer on one differential equation can also be successfully deployed on another differential equation.
    
[^97]: 不要惊慌：基于原型加性神经网络的阿尔茨海默病可解释分类

    Don't PANIC: Prototypical Additive Neural Network for Interpretable Classification of Alzheimer's Disease. (arXiv:2303.07125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.07125](http://arxiv.org/abs/2303.07125)

    本研究提出了PANIC，一个基于原型加性神经网络的可解释AD分类模型，整合了3D图像和表格数据并且表现出最先进的性能，提供了直接的局部和全局解释，并提取出了有生物意义的AD特征。

    

    阿尔茨海默病（AD）具有复杂和多因素的病因学，需要整合神经解剖学、遗传学和脑脊液生物标志物的信息进行准确的诊断。因此，近期的深度学习方法结合了图像和表格信息以提高诊断性能。然而，这些神经网络的黑盒特性仍然是临床应用的障碍，其中理解异质模型的决策是必要的。我们提出了一个名为PANIC的原型加性神经网络，用于解释性的AD分类，其整合了3D图像和表格数据。它是通过设计来可解释的，从而避免了尝试逼近网络决策的事后解释的需要。我们的结果表明，PANIC在AD分类方面实现了最先进的性能，同时直接提供局部和全局的解释。最后，我们展示了PANIC提取出有生物意义的AD特征，并满足了一定的s要求。

    Alzheimer's disease (AD) has a complex and multifactorial etiology, which requires integrating information about neuroanatomy, genetics, and cerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep learning approaches combined image and tabular information to improve diagnostic performance. However, the black-box nature of such neural networks is still a barrier for clinical applications, in which understanding the decision of a heterogeneous model is integral. We propose PANIC, a prototypical additive neural network for interpretable AD classification that integrates 3D image and tabular data. It is interpretable by design and, thus, avoids the need for post-hoc explanations that try to approximate the decision of a network. Our results demonstrate that PANIC achieves state-of-the-art performance in AD classification, while directly providing local and global explanations. Finally, we show that PANIC extracts biologically meaningful signatures of AD, and satisfies a s
    
[^98]: 基于深度学习的时间序列因果推断量化北极放大的原因

    Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.07122](http://arxiv.org/abs/2303.07122)

    该研究提出了一种基于循环神经网络的时间序列因果推断模型TCINet，用于推断大气过程对海冰融化的因果效应。通过实验证明，该模型能够显著提高量化北极海冰融化的主要原因的能力。

    

    北极变暖，也称北极放大，由多种大气和海洋因素导致，但其基础热力因素的详细情况仍不清楚。使用固定治疗效应策略推断大气过程对海冰融化的因果效应会导致不现实的反事实估计。这样的模型也容易受到时间变化的混淆的影响而引起偏差。为了解决这些挑战，我们提出了TCINet - 一种基于循环神经网络的时间序列因果推断模型，以连续治疗方式推断因果关系。通过对合成和观测数据的实验，我们展示了我们的研究如何大大提高量化北极海冰融化的主要原因的能力。

    The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
    
[^99]: Uni-RXN: 一种统一的框架，弥合化学反应Pretraining和条件分子生成之间的差距

    Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation. (arXiv:2303.06965v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06965](http://arxiv.org/abs/2303.06965)

    本文提出了Uni-RXN框架，在化学反应Pretraining和分子生成任务中都取得了最先进的结果。通过具备化学知识，克服了当前分子生成模型仅依赖少量反应模板的限制，生成质量高、可合成的药物类分子结构。

    

    化学反应是药物设计和有机化学研究的基本构建模块。近年来，对于一个可以有效捕捉化学反应基本规则的大规模深度学习框架的需求不断增长。在本文中，我们提出了一个统一的框架，解决了反应表示学习和分子生成任务，允许更整体的方法。受有机化学机制的启发，我们开发了一种新的预训练框架，使我们能够将归纳偏见纳入模型。我们的框架在具有挑战性的下游任务上取得了最先进的结果。通过具备化学知识，该框架可应用于基于反应的生成模型，克服了当前分子生成模型仅依赖少量反应模板的限制。在广泛的实验中，我们的模型生成了高质量的可合成药物类分子结构。

    Chemical reactions are the fundamental building blocks of drug design and organic chemistry research. In recent years, there has been a growing need for a large-scale deep-learning framework that can efficiently capture the basic rules of chemical reactions. In this paper, we have proposed a unified framework that addresses both the reaction representation learning and molecule generation tasks, which allows for a more holistic approach. Inspired by the organic chemistry mechanism, we develop a novel pretraining framework that enables us to incorporate inductive biases into the model. Our framework achieves state-of-the-art results on challenging downstream tasks. By possessing chemical knowledge, this framework can be applied to reaction-based generative models, overcoming the limitations of current molecule generation models that rely on a small number of reaction templates. In the extensive experiments, our model generates synthesizable drug-like structures of high quality. Overall,
    
[^100]: 利用PyTorch和Firedrake实现的基于物理学的机器学习模型

    Physics-driven machine learning models coupling PyTorch and Firedrake. (arXiv:2303.06871v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06871](http://arxiv.org/abs/2303.06871)

    本论文介绍了一种基于物理学的机器学习技术，结合PyTorch和Firedrake框架，可用于较少的训练数据中实现对复杂物理系统的建模。

    

    偏微分方程（PDE）是描述和建模科学和工程学中许多复杂物理系统的核心。然而，在许多实际应用中，PDE建模仅提供了物理模型的不完整描述。基于PDE的机器学习技术旨在解决这种限制。在这种方法中，PDE用作归纳偏置，使耦合模型能够依赖基本物理定律，同时需要更少的训练数据。将PDE和机器学习相结合的高性能模拟部署到复杂问题中需要组合机器学习和基于PDE的框架提供的功能。我们提出了一个简单但有效的耦合方式，将机器学习框架PyTorch和PDE系统Firedrake相结合，为研究人员、工程师和领域专家提供了一种高效的指定耦合模型的方式，并且只需要对现有代码进行微不足道的更改。

    Partial differential equations (PDEs) are central to describing and modelling complex physical systems that arise in many disciplines across science and engineering. However, in many realistic applications PDE modelling provides an incomplete description of the physics of interest. PDE-based machine learning techniques are designed to address this limitation. In this approach, the PDE is used as an inductive bias enabling the coupled model to rely on fundamental physical laws while requiring less training data. The deployment of high-performance simulations coupling PDEs and machine learning to complex problems necessitates the composition of capabilities provided by machine learning and PDE-based frameworks. We present a simple yet effective coupling between the machine learning framework PyTorch and the PDE system Firedrake that provides researchers, engineers and domain specialists with a high productive way of specifying coupled models while only requiring trivial changes to existi
    
[^101]: 标签信息瓶颈用于标签增强

    Label Information Bottleneck for Label Enhancement. (arXiv:2303.06836v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06836](http://arxiv.org/abs/2303.06836)

    本文提出了标签信息瓶颈方法用于标签增强，通过学习关键标签相关信息，恢复标签分布，提高了恢复效果。

    

    本文聚焦于标签增强问题，即从逻辑标签恢复标签分布。我们提出了一种新的标签信息瓶颈方法（LIB）用于标签增强。为了解决数据集中不相关的标签信息可能导致恢复效果不佳的问题，我们努力挖掘关键的标签相关信息以提高恢复效果。我们的方法将标签增强问题分为两个联合过程：1) 学习具有关键标签相关信息的表示，2) 基于学习到的表示恢复标签分布。关键标签相关信息可以通过所学表示构成的“瓶颈”挖掘出来。显著地，我们可以对标签分配和标签差异的关键标签相关信息进行探索。

    In this work, we focus on the challenging problem of Label Enhancement (LE), which aims to exactly recover label distributions from logical labels, and present a novel Label Information Bottleneck (LIB) method for LE. For the recovery process of label distributions, the label irrelevant information contained in the dataset may lead to unsatisfactory recovery performance. To address this limitation, we make efforts to excavate the essential label relevant information to improve the recovery performance. Our method formulates the LE problem as the following two joint processes: 1) learning the representation with the essential label relevant information, 2) recovering label distributions based on the learned representation. The label relevant information can be excavated based on the "bottleneck" formed by the learned representation. Significantly, both the label relevant information about the label assignments and the label relevant information about the label gaps can be explored in ou
    
[^102]: 通过不确定性感知的强化学习，为人-机协同机器人智能决策提供方法

    Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning. (arXiv:2303.06710v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.06710](http://arxiv.org/abs/2303.06710)

    该论文提出了一种基于强化学习的半自主代理机器人方法，在对任务成功结果的信心低时请求外部帮助，有效降低专家调用数量。

    

    在人-机协同范式中，机器人智能代理能够在大部分时间内自主完成任务，当需要帮助时，可以向外部专家寻求帮助。然而，关键是要知道何时请求外部帮助：太少的请求会导致机器人犯错，但太多的请求会使专家过载。本文提出了一种基于强化学习的方法来解决这个问题，其中半自主代理在对任务成功结果的信心低时请求外部帮助。置信度是通过估计当前状态的回报方差来计算的。我们展示了这个估计可以通过类似贝尔曼递归的训练过程逐步改进。在具有完全和部分可观察状态信息的离散导航问题中，我们展示了我们的方法在运行时利用有限的专家调用预算时是有效的，尽管在训练时没有访问专家。

    In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly autonomously in solving a task, but can request help from an external expert when needed. However, knowing when to request such assistance is critical: too few requests can lead to the robot making mistakes, but too many requests can overload the expert. In this paper, we present a Reinforcement Learning based approach to this problem, where a semi-autonomous agent asks for external assistance when it has low confidence in the eventual success of the task. The confidence level is computed by estimating the variance of the return from the current state. We show that this estimate can be iteratively improved during training using a Bellman-like recursion. On discrete navigation problems with both fully- and partially-observable state information, we show that our method makes effective use of a limited budget of expert calls at run-time, despite having no access to the expert at training time.
    
[^103]: CoNIC挑战：推动核检测、分割、分类和计数的前沿（arXiv:2303.06274v1 [cs.CV]）

    CoNIC Challenge: Pushing the Frontiers of Nuclear Detection, Segmentation, Classification and Counting. (arXiv:2303.06274v1 [cs.CV])

    [http://arxiv.org/abs/2303.06274](http://arxiv.org/abs/2303.06274)

    CoNIC挑战使用最大的数据集评估核分割和细胞组成，刺激了可重复的细胞识别算法的开发，发现嗜酸性粒细胞和中性粒细胞在肿瘤中发挥重要作用。

    The CoNIC challenge used the largest dataset to evaluate nuclear segmentation and cellular composition, stimulated the development of reproducible algorithms for cellular recognition, and found that eosinophils and neutrophils play an important role in tumors.

    核检测、分割和形态测量是帮助我们进一步了解组织学和患者预后关系的关键。为了推动这一领域的创新，我们使用目前最大的数据集设置了一个社区广泛的挑战，以评估核分割和细胞组成。我们的挑战名为CoNIC，刺激了可重复的细胞识别算法的开发，并在公共排行榜上进行实时结果检查。我们基于1,658个结肠组织的全切片图像对表现最佳的模型进行了广泛的后挑战分析。每个模型检测到约7亿个细胞核，相关特征用于不良增生分级和生存分析，我们证明了挑战对先前最先进技术的改进导致了下游性能的显著提升。我们的发现还表明，嗜酸性粒细胞和中性粒细胞在肿瘤中发挥重要作用。

    Nuclear detection, segmentation and morphometric profiling are essential in helping us further understand the relationship between histology and patient outcome. To drive innovation in this area, we setup a community-wide challenge using the largest available dataset of its kind to assess nuclear segmentation and cellular composition. Our challenge, named CoNIC, stimulated the development of reproducible algorithms for cellular recognition with real-time result inspection on public leaderboards. We conducted an extensive post-challenge analysis based on the top-performing models using 1,658 whole-slide images of colon tissue. With around 700 million detected nuclei per model, associated features were used for dysplasia grading and survival analysis, where we demonstrated that the challenge's improvement over the previous state-of-the-art led to significant boosts in downstream performance. Our findings also suggest that eosinophils and neutrophils play an important role in the tumour m
    
[^104]: 体育博彩的机器学习：预测模型应优化准确性还是校准性？

    Machine learning for sports betting: should forecasting models be optimised for accuracy or calibration?. (arXiv:2303.06021v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06021](http://arxiv.org/abs/2303.06021)

    该论文研究了机器学习在体育博彩中的应用，提出了优化预测模型校准性比准确度更重要的假设，并通过实验证明了此假设的正确性。

    

    美国最近对体育博彩进行了联邦合法化，这与机器学习的黄金时代相遇。如果博彩者能够利用数据准确地预测结果的概率，他们可以认识到何时书maker的赔率对他们有利。由于体育博彩仅在美国的市场上就是一个数十亿美元的行业，因此找到这样的机会可能会非常有利可图。许多研究人员已将机器学习应用于体育赛果预测问题，通常使用准确度来评估预测模型的性能。我们提出假设，对于体育博彩问题，模型校准比准确度更重要。为了测试这一假设，我们在多个赛季的NBA数据上对模型进行训练，并在单个赛季上使用已发布的赔率进行博彩实验。通过评估各种博彩系统，我们表明优化校准的预测模型比优化准确度平均带来更高的回报率（投资回报率为$110.42％$）。

    Sports betting's recent federal legalisation in the USA coincides with the golden age of machine learning. If bettors can leverage data to accurately predict the probability of an outcome, they can recognise when the bookmaker's odds are in their favour. As sports betting is a multi-billion dollar industry in the USA alone, identifying such opportunities could be extremely lucrative. Many researchers have applied machine learning to the sports outcome prediction problem, generally using accuracy to evaluate the performance of forecasting models. We hypothesise that for the sports betting problem, model calibration is more important than accuracy. To test this hypothesis, we train models on NBA data over several seasons and run betting experiments on a single season, using published odds. Evaluating various betting systems, we show that optimising the forecasting model for calibration leads to greater returns than optimising for accuracy, on average (return on investment of $110.42\%$ v
    
[^105]: 神经图形的硬件加速

    Hardware Acceleration of Neural Graphics. (arXiv:2303.05735v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2303.05735](http://arxiv.org/abs/2303.05735)

    本文研究了神经图形是否需要硬件支持，发现当前GPU性能无法满足对4K分辨率60FPS渲染的需求，且在增强现实/虚拟现实应用中性能缺口更大。作者确定输入编码和MLP内核是性能瓶颈。

    

    传统的计算机图形学渲染和反渲染算法已被神经表示（NR）所取代。NR最近被用于学习场景的几何和材质属性，并使用这些信息合成真实的图像，因此承诺用可伸缩的质量和可预测的性能替换传统的渲染算法。本文提出问题：神经图形（NG）是否需要硬件支持？我们研究了代表性的NG应用程序，发现如果我们要在当前的GPU上以60FPS渲染4K分辨率，则所需性能与当前GPU的实际性能存在1.5倍至55倍的差距。对于增强现实/虚拟现实应用程序，所需性能与所需系统功率之间存在更大的差距。我们确定输入编码和MLP内核是性能瓶颈，对于多分辨率哈希网格、多分辨率密集网格和低分辨率密集网格，它们占应用程序时间的72％、60％和59％。

    Rendering and inverse-rendering algorithms that drive conventional computer graphics have recently been superseded by neural representations (NR). NRs have recently been used to learn the geometric and the material properties of the scenes and use the information to synthesize photorealistic imagery, thereby promising a replacement for traditional rendering algorithms with scalable quality and predictable performance. In this work we ask the question: Does neural graphics (NG) need hardware support? We studied representative NG applications showing that, if we want to render 4k res. at 60FPS there is a gap of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications, there is an even larger gap of 2-4 OOM between the desired performance and the required system power. We identify that the input encoding and the MLP kernels are the performance bottlenecks, consuming 72%,60% and 59% of application time for multi res. hashgrid, multi res. densegrid and low res. densegrid 
    
[^106]: 基于自适应相关图卷积网络，实现交通量估计更好的性能：解决不确定和非平衡问题

    Towards better traffic volume estimation: Tackling both underdetermined and non-equilibrium problems via a correlation-adaptive graph convolution network. (arXiv:2303.05660v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05660](http://arxiv.org/abs/2303.05660)

    本研究提出基于图卷积网络的方法，解决交通量估计中的不确定和非平衡问题，实现准确的全面交通量估计。

    

    交通量是交通管理和控制提供细粒度信息不可或缺的因素。然而，由于交通传感器的有限部署，获取全面的交通量信息并不容易。现有研究主要集中在提高特定方法的整体估计准确性上，忽略了交通量估计的基本挑战，因此在一些关键任务上表现较差。本文研究了交通量估计中的两个关键问题: (1) 由未检测到的行动引起的不确定交通流，以及 (2) 由拥堵传播引起的非平衡交通流。我们提出了一种基于图形的深度学习方法，可以提供数据驱动的、无模型的和相关自适应方法来解决上述问题，并进行准确的全面交通量估计。特别地，为了量化交通速度和流量之间的动态和非线性关系，本文介绍了用于建立交通流图的相关图卷积网络。

    Traffic volume is an indispensable ingredient to provide fine-grained information for traffic management and control. However, due to limited deployment of traffic sensors, obtaining full-scale volume information is far from easy. Existing works on this topic primarily focus on improving the overall estimation accuracy of a particular method and ignore the underlying challenges of volume estimation, thereby having inferior performances on some critical tasks. This paper studies two key problems with regard to traffic volume estimation: (1) underdetermined traffic flows caused by undetected movements, and (2) non-equilibrium traffic flows arise from congestion propagation. Here we demonstrate a graph-based deep learning method that can offer a data-driven, model-free and correlation adaptive approach to tackle the above issues and perform accurate network-wide traffic volume estimation. Particularly, in order to quantify the dynamic and nonlinear relationships between traffic speed and 
    
[^107]: 具有线性函数逼近的重尾奖励方差感知鲁棒强化学习

    Variance-aware robust reinforcement learning with linear function approximation under heavy-tailed rewards. (arXiv:2303.05606v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05606](http://arxiv.org/abs/2303.05606)

    本文提出了AdaOFUL和VARA两种算法，用于在存在有限方差的重尾奖励情况下进行在线顺序决策，其中AdaOFUL具有状态-of-the-art的遗憾界，VARA达到了更紧密的方差感知遗憾界。

    

    本文提出了两种算法AdaOFUL和VARA，用于在仅存在有限方差的重尾奖励情况下进行在线顺序决策。对于线性随机赌徒，我们通过修改自适应Huber回归并提出AdaOFUL来解决重尾奖励问题。AdaOFUL达到了状态-of-the-art的遗憾界，即$ \widetilde{O}\big（d\big(\sum_{t=1}^T\nu_{t}^2\big)^{1/2}+d\big)$，其中$\nu_{t}^2$是第$t$轮奖励观测到的条件方差，$d$是特征维度，$\widetilde{O}（\cdot）$ 隐藏对数依赖性。在AdaOFUL的基础上，我们提出了VARA用于线性MDP，它达到了更紧密的方差感知遗憾界，即 $ \widetilde{O}(d\sqrt{HG^*K})$。这里，$H$是事件的长度，$K$是事件的数量，$G^*$是较小的依赖于实例的量，当在其他实例相关量被限制时，它可以被边界化。

    This paper presents two algorithms, AdaOFUL and VARA, for online sequential decision-making in the presence of heavy-tailed rewards with only finite variances. For linear stochastic bandits, we address the issue of heavy-tailed rewards by modifying the adaptive Huber regression and proposing AdaOFUL. AdaOFUL achieves a state-of-the-art regret bound of $\widetilde{O}\big(d\big(\sum_{t=1}^T \nu_{t}^2\big)^{1/2}+d\big)$ as if the rewards were uniformly bounded, where $\nu_{t}^2$ is the observed conditional variance of the reward at round $t$, $d$ is the feature dimension, and $\widetilde{O}(\cdot)$ hides logarithmic dependence. Building upon AdaOFUL, we propose VARA for linear MDPs, which achieves a tighter variance-aware regret bound of $\widetilde{O}(d\sqrt{HG^*K})$. Here, $H$ is the length of episodes, $K$ is the number of episodes, and $G^*$ is a smaller instance-dependent quantity that can be bounded by other instance-dependent quantities when additional structural conditions on the 
    
[^108]: 离散-连续域中的神经概率逻辑编程

    Neural Probabilistic Logic Programming in Discrete-Continuous Domains. (arXiv:2303.04660v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.04660](http://arxiv.org/abs/2303.04660)

    介绍了一种名为DeepSeaProbLog的神经概率逻辑编程语言，将深度概率编程技术纳入其中，支持在逻辑约束条件下推断和学习离散和连续概率分布，并通过实验证明其优势。

    

    神经符号人工智能（NeSy）允许神经网络利用逻辑形式的符号背景知识。NeSy已经被证明在有限的数据范围内有助于学习，并且有助于推断非分布数据。概率NeSy着重于将神经网络与逻辑和概率论相结合，从而还允许在不确定性下学习。当前概率NeSy系统（如DeepProbLog）的一个主要限制是它们局限于有限概率分布，即离散随机变量。相比之下，深度概率编程（DPP）在建模和优化连续概率分布方面表现出色。因此，我们介绍了DeepSeaProbLog，这是一种神经概率逻辑编程语言，将DPP技术纳入NeSy中。这样做的结果是在逻辑约束条件下支持离散和连续概率分布的推断和学习。我们的主要贡献是1）DeepSeaProbLog的语义，2）它的高效学习算法，3）它的实现，以及4）一组全面的实验，证明了它在离散和连续领域的建模、推断和学习方面相对于当前NeSy方法的优势。

    Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic background knowledge in the form of logic. It has been shown to aid learning in the limited data regime and to facilitate inference on out-of-distribution data. Probabilistic NeSy focuses on integrating neural networks with both logic and probability theory, which additionally allows learning under uncertainty. A major limitation of current probabilistic NeSy systems, such as DeepProbLog, is their restriction to finite probability distributions, i.e., discrete random variables. In contrast, deep probabilistic programming (DPP) excels in modelling and optimising continuous probability distributions. Hence, we introduce DeepSeaProbLog, a neural probabilistic logic programming language that incorporates DPP techniques into NeSy. Doing so results in the support of inference and learning of both discrete and continuous probability distributions under logical constraints. Our main contributions are 1) the semantics of DeepS
    
[^109]: 大规模外域检测的预测嵌入功率回归

    Predicted Embedding Power Regression for Large-Scale Out-of-Distribution Detection. (arXiv:2303.04115v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.04115](http://arxiv.org/abs/2303.04115)

    本文提出一种基于标签分布学习的预测类标签概率方法，用于大规模外域检测，相比现有最先进方法在 AUROC 和 AUPR 方面实现统计显着的改进。

    

    外域输入可能会危及真实世界中机器学习系统的性能和安全性。尽管存在许多用于外域检测的方法，并且对于低分辨率和少量类别的小规模数据集表现良好，但为大规模外域检测开发的方法很少。现有的大规模方法通常依赖于最大分类概率，例如最先进的分组 softmax 方法。在这项工作中，我们开发了一种新方法，该方法根据在训练过程中学习的标签分布计算预测类标签的概率。我们的方法表现优于目前最先进的方法，仅在计算成本上稍有增加。我们在14个数据集上评估了我们的方法，并在 AUROC（84.2 vs 82.4）和 AUPR（96.2 vs 93.7）方面实现了统计显着的改进。

    Out-of-distribution (OOD) inputs can compromise the performance and safety of real world machine learning systems. While many methods exist for OOD detection and work well on small scale datasets with lower resolution and few classes, few methods have been developed for large-scale OOD detection. Existing large-scale methods generally depend on maximum classification probability, such as the state-of-the-art grouped softmax method. In this work, we develop a novel approach that calculates the probability of the predicted class label based on label distributions learned during the training process. Our method performs better than current state-of-the-art methods with only a negligible increase in compute cost. We evaluate our method against contemporary methods across $14$ datasets and achieve a statistically significant improvement with respect to AUROC (84.2 vs 82.4) and AUPR (96.2 vs 93.7).
    
[^110]: 掩蔽图像是鲁棒微调的反事实样本

    Masked Images Are Counterfactual Samples for Robust Fine-tuning. (arXiv:2303.03052v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.03052](http://arxiv.org/abs/2303.03052)

    本文提出了一种新颖的深度学习模型微调方法，利用掩蔽图像作为反事实样本，提高模型的鲁棒性。

    

    深度学习模型由于训练数据和测试数据之间的分布差异而受到挑战。最近，基于多样化数据预训练的大型模型展现了空前的鲁棒性来应对各种分布差异。然而，在这些模型上进行微调可能会导致在分布内性能和分布外鲁棒性之间的权衡。现有的方法并没有明确处理分布外鲁棒性问题。在本文中，我们基于对上述问题的因果分析，提出了一种新颖的微调方法，利用掩蔽图像作为反事实样本，有助于提高微调模型的鲁棒性。具体而言，我们基于类激活图对图像的语义相关或语义无关补丁进行掩蔽，以打破虚假相关性，并用其他图像的补丁来重新填充掩蔽的补丁。这些反事实样本则用于特征蒸馏。

    Deep learning models are challenged by the distribution shift between the training data and test data. Recently, the large models pre-trained on diverse data demonstrate unprecedented robustness to various distribution shifts. However, fine-tuning on these models can lead to a trade-off between in-distribution (ID) performance and out-of-distribution (OOD) robustness. Existing methods for tackling this trade-off do not explicitly address the OOD robustness problem. In this paper, based on causal analysis on the aforementioned problems, we propose a novel fine-tuning method, which use masked images as counterfactual samples that help improving the robustness of the fine-tuning model. Specifically, we mask either the semantics-related or semantics-unrelated patches of the images based on class activation map to break the spurious correlation, and refill the masked patches with patches from other images. The resulting counterfactual samples are used in feature-based distillation with the 
    
[^111]: 基于随机投影的线性回归双下降的高维分析

    High-dimensional analysis of double descent for linear regression with random projections. (arXiv:2303.01372v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01372](http://arxiv.org/abs/2303.01372)

    本文使用随机矩阵理论对基于随机投影的线性回归问题进行高维分析，证明了固定预测问题的双下降曲线现象。

    

    我们考虑随机投影的数量变化的线性回归问题，在随机矩阵理论的基础上，我们确定了一个固定的预测问题的双下降曲线。我们首先考虑Ridge回归估计器，并使用非参数统计中的经典概念（即自由度，又称有效维数）来回顾早期结果。然后，我们计算了随机投影的最小范数最小二乘拟合的广义性能（以平方偏差和方差表示）的渐近等价物，为双下降现象提供简单的表达式。

    We consider linear regression problems with a varying number of random projections, where we provably exhibit a double descent curve for a fixed prediction problem, with a high-dimensional analysis based on random matrix theory. We first consider the ridge regression estimator and review earlier results using classical notions from non-parametric statistics, namely degrees of freedom, also known as effective dimensionality. We then compute asymptotic equivalents of the generalization performance (in terms of squared bias and variance) of the minimum norm least-squares fit with random projections, providing simple expressions for the double descent phenomenon.
    
[^112]: 基于时空因果关系的可解释水位预测器

    Interpretable Water Level Forecaster with Spatiotemporal Causal Attention Mechanisms. (arXiv:2303.00515v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00515](http://arxiv.org/abs/2303.00515)

    本研究提出一种基于时空因果关系的新型水位预测模型，通过将因果结构形式化为多层网络和使用蒙版方法，提高了其可解释性，运用于汉江数据集的实际分析中表现优异。

    

    预测汉江水位对于交通控制和避免自然灾害至关重要，但涉及多种变量并相互复杂地联系着。本研究提出一种新型的转换器，利用变量先前知识基于因果关系，预测汉江济州桥的水位。我们的模型考虑到空间和时间因果关系，将因果结构形式化为多层网络并使用蒙版方法。凭借这种方法，我们可以根据先前的知识获得可解释性。在实际数据分析中，我们使用了2016年至2021年的汉江数据集，并将所提出的模型与深度学习模型进行了比较。

    Forecasting the water level of the Han river is important to control traffic and avoid natural disasters. There are many variables related to the Han river and they are intricately connected. In this work, we propose a novel transformer that exploits the causal relationship based on the prior knowledge among the variables and forecasts the water level at the Jamsu bridge in the Han river. Our proposed model considers both spatial and temporal causation by formalizing the causal structure as a multilayer network and using masking methods. Due to this approach, we can have interpretability that consistent with prior knowledge. In real data analysis, we use the Han river dataset from 2016 to 2021 and compare the proposed model with deep learning models.
    
[^113]: TimeMAE: 基于解耦掩码自编码器的自监督时间序列表示

    TimeMAE: Self-Supervised Representations of Time Series with Decoupled Masked Autoencoders. (arXiv:2303.00320v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00320](http://arxiv.org/abs/2303.00320)

    TimeMAE是一种新型自监督模型，利用transformer网络将每个时间序列处理成一系列不重叠的子序列，并通过随机掩码策略覆盖本地化子序列的语义单元，以学习到丰富的上下文信息和可传递的时间序列表示。

    

    在时间序列分类中，利用自监督预训练提高深度学习模型的表达能力正在变得越来越普遍。虽然已经有很多工作致力于开发面向时间序列数据的自监督模型，但由于仅在稀疏逐点输入单元上进行单向编码，当前方法不能学习到最优时间序列表示。在这项工作中，我们提出了TimeMAE，一种基于transformer网络的学习可传递时间序列表示的新型自监督范式。TimeMAE的独特特点在于将每个时间序列通过窗口切片分区处理成一系列不重叠的子序列，然后通过随机掩码策略覆盖本地化子序列的语义单元。这种简单而有效的设置可以帮助我们达到一举三得的目标，即（1）学习丰富的上下文信息；

    Enhancing the expressive capacity of deep learning-based time series models with self-supervised pre-training has become ever-increasingly prevalent in time series classification. Even though numerous efforts have been devoted to developing self-supervised models for time series data, we argue that the current methods are not sufficient to learn optimal time series representations due to solely unidirectional encoding over sparse point-wise input units. In this work, we propose TimeMAE, a novel self-supervised paradigm for learning transferrable time series representations based on transformer networks. The distinct characteristics of the TimeMAE lie in processing each time series into a sequence of non-overlapping sub-series via window-slicing partitioning, followed by random masking strategies over the semantic units of localized sub-series. Such a simple yet effective setting can help us achieve the goal of killing three birds with one stone, i.e., (1) learning enriched contextual r
    
[^114]: Dish-TS: 一种缓解时间序列预测中分布偏移的通用范例

    Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting. (arXiv:2302.14829v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14829](http://arxiv.org/abs/2302.14829)

    Dish-TS是一种通用的神经网络模型，用于缓解时间序列预测中的分布偏移。该模型通过引入系数网络（CONET）来更好地估计分布。在时间序列预测任务中，将Lookback窗口作为输入空间，Horizon窗口作为输出空间，将分布偏移总结为内部空间偏移和不同空间偏移两类。

    

    时间序列预测中的分布偏移指的是时间序列在时间上分布的变化，它很大程度上阻碍了时间序列预测模型的性能。现有针对时间序列分布偏移的研究大多局限于分布量化，更重要的是，忽视了Lookback和Horizon之间的潜在偏移。为了解决上述问题，我们系统地将时间序列中的分布偏移总结为两类。将Lookback窗口视为输入空间，Horizon窗口视为输出空间，存在(i)内部空间偏移，即在输入空间内的分布随时间保持偏移，以及(ii)不同空间偏移，在输入空间和输出空间之间分布偏移。然后，我们引入了一种名为Dish-TS的通用神经模型来缓解时间序列中的分布偏移。具体而言，为了更好地估计分布，我们提出了系数网络（CONET），它可以是任何神经网络架构，用于映射输入序列。

    The distribution shift in Time Series Forecasting (TSF), indicating series distribution changes over time, largely hinders the performance of TSF models. Existing works towards distribution shift in time series are mostly limited in the quantification of distribution and, more importantly, overlook the potential shift between lookback and horizon windows. To address above challenges, we systematically summarize the distribution shift in TSF into two categories. Regarding lookback windows as input-space and horizon windows as output-space, there exist (i) intra-space shift, that the distribution within the input-space keeps shifted over time, and (ii) inter-space shift, that the distribution is shifted between input-space and output-space. Then we introduce, Dish-TS, a general neural paradigm for alleviating distribution shift in TSF. Specifically, for better distribution estimation, we propose the coefficient net (CONET), which can be any neural architectures, to map input sequences in
    
[^115]: 在线主动学习综述

    A survey on online active learning. (arXiv:2302.08893v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08893](http://arxiv.org/abs/2302.08893)

    在线主动学习是一种机器学习范式，旨在从数据流中选择最具信息量的数据点进行标注。本文综述了在线主动学习的最新进展、基于流的主动学习的主要挑战和机遇、用于选择信息样本的策略以及该范式中不同的评估指标。

    

    在线主动学习是一种机器学习范式，旨在从数据流中选择最具信息量的数据点进行标注。近年来，随着数据仅以未标记形式可用的实际应用日益增多，最小化与收集标记观测相关的成本问题引起了广泛关注。标注每个观测可以耗费大量的时间和成本，使得获取大量标记数据变得困难。为了克服这个问题，许多主动学习策略已经提出，旨在选择最具信息量的观测进行标记，以提高机器学习模型的性能。这些方法可以广泛地分为两类：静态基于池的和基于流的主动学习。基于池的主动学习涉及从封闭的未标记数据池中选择一部分观测，已成为许多调查和文献综述的重点。然而，随着在线数据流的不断增加，基于流的主动学习策略变得更加吸引人，因为它们允许模型适应新进数据。在本综述中，我们综述了在线主动学习的最新进展，讨论了基于流的主动学习的主要挑战和机遇、用于选择信息样本的策略以及该范例中不同的评估指标。

    Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. Howev
    
[^116]: OpenHLS：适用于实验科学的低延迟深度神经网络高级综合

    OpenHLS: High-Level Synthesis for Low-Latency Deep Neural Networks for Experimental Science. (arXiv:2302.06751v3 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2302.06751](http://arxiv.org/abs/2302.06751)

    OpenHLS是一个基于高级综合技术的开源编译器框架，将深度神经网络的高级表示转换为适用于近传感器设备的低级表示，解决了实验科学领域数据采集系统中低延迟处理问题。

    

    在许多实验驱动的科学领域，例如高能物理、材料科学和宇宙学中，高数据率实验对数据采集系统施加硬性约束：收集的数据必须无差别地存储以进行后处理和分析，从而需要大容量存储，或者在实时准确过滤时，从而需要低延迟处理。深度神经网络已被证明在其他过滤任务中非常有效，但由于设计和部署困难，尚未广泛应用于此类数据采集系统。我们提出了一个开源的、轻量级的编译器框架OpenHLS，基于高级综合技术，将深度神经网络的高级表示转换为适用于场可编程门阵列等近传感器设备的低级表示，其中没有任何专有依赖项。我们在各种工作负载上评估OpenHLS，并呈现了一个深度神经网络的案例研究实现。

    In many experiment-driven scientific domains, such as high-energy physics, material science, and cosmology, high data rate experiments impose hard constraints on data acquisition systems: collected data must either be indiscriminately stored for post-processing and analysis, thereby necessitating large storage capacity, or accurately filtered in real-time, thereby necessitating low-latency processing. Deep neural networks, effective in other filtering tasks, have not been widely employed in such data acquisition systems, due to design and deployment difficulties. We present an open source, lightweight, compiler framework, without any proprietary dependencies, OpenHLS, based on high-level synthesis techniques, for translating high-level representations of deep neural networks to low-level representations, suitable for deployment to near-sensor devices such as field-programmable gate arrays. We evaluate OpenHLS on various workloads and present a case-study implementation of a deep neural
    
[^117]: 非线性Ridge Bandits的统计复杂度和最优算法

    Statistical Complexity and Optimal Algorithms for Non-linear Ridge Bandits. (arXiv:2302.06025v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06025](http://arxiv.org/abs/2302.06025)

    本文探讨了非线性Ridge Bandits中独特的学习现象，推导出了最优烧录成本的上下限和整个烧录期间的学习轨迹的统计算法，并证明了UCB和基于回归神经元的算法都是次优解。

    

    本文考虑了一种顺序决策问题，其中平均结果是所选择动作的非线性函数。与线性模型相比，非线性模型有两种奇特现象：首先，除了具有标准参数率的“学习阶段”以进行估计或后悔外，还有一个由非线性函数确定的固定成本的“烧录期”; 其次，实现最小烧录成本需要新的探索算法。针对一类名为ridge函数的特殊非线性函数，我们通过微分方程推导了最优烧录成本的上下限，此外还推导了整个烧录期间的学习轨迹的上下限。特别地，一种两阶段算法先找到一个好的初始行动，然后将问题视为局部线性，这是统计上最优的。相反，几种经典算法，例如UCB和依赖于回归神经元的算法，其可证明是次优的。

    We consider the sequential decision-making problem where the mean outcome is a non-linear function of the chosen action. Compared with the linear model, two curious phenomena arise in non-linear models: first, in addition to the "learning phase" with a standard parametric rate for estimation or regret, there is an "burn-in period" with a fixed cost determined by the non-linear function; second, achieving the smallest burn-in cost requires new exploration algorithms. For a special family of non-linear functions named ridge functions in the literature, we derive upper and lower bounds on the optimal burn-in cost, and in addition, on the entire learning trajectory during the burn-in period via differential equations. In particular, a two-stage algorithm that first finds a good initial action and then treats the problem as locally linear is statistically optimal. In contrast, several classical algorithms, such as UCB and algorithms relying on regression oracles, are provably suboptimal.
    
[^118]: 学习双层知识图谱的表示以进行超越链接预测的推理

    Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction. (arXiv:2302.02601v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02601](http://arxiv.org/abs/2302.02601)

    本文提出了一种基于双层知识图谱的方法来学习嵌入，将三元组之间的关系考虑进去，并使用数据增强策略来增加合理的三元组。

    

    知识图谱使用三元组来表示已知事实。现有的知识图谱嵌入方法仅考虑实体之间的连接，而本文提出考虑三元组之间的关系。本文定义了一个更高级的三元组来表示三元组之间的关系，例如，$\langle T_1$, PrerequisiteFor, $T_2\rangle$，其中PrerequisiteFor是更高级别的关系。我们定义一个由基本级别和更高级别的三元组组成的双层知识图谱。我们还提出了一种基于双层知识图谱上的随机游走的数据增强策略来增加合理的三元组。我们的模型BiVE通过考虑基本级别和更高级别三元组的结构来学习嵌入。

    Knowledge graphs represent known facts using triplets. While existing knowledge graph embedding methods only consider the connections between entities, we propose considering the relationships between triplets. For example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is (Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins, Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a prerequisite for $T_2$. In this paper, we define a higher-level triplet to represent a relationship between triplets, e.g., $\langle T_1$, PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation. We define a bi-level knowledge graph that consists of the base-level and the higher-level triplets. We also propose a data augmentation strategy based on the random walks on the bi-level knowledge graph to augment plausible triplets. Our model called BiVE learns embeddings by taking into account the structures of the base-level and the higher-level tripl
    
[^119]: CECT：可控的卷积神经网络和Transformer用于COVID-19图像分类

    CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification. (arXiv:2302.02314v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.02314](http://arxiv.org/abs/2302.02314)

    本研究提出了一种新的分类网络CECT，利用可控的卷积神经网络和Transformer进行组合，能同时捕捉多个局部和全局尺度上的特征。在两个公共COVID-19数据集上评估后表现优于现有的最先进方法。这一新方法在医学图像分类领域中有广泛的应用前景。

    

    大多数计算机视觉模型基于卷积神经网络（CNN）或Transformer开发，前者（后者）可以捕捉局部（全局）特征。为了减轻模型性能受局部（全局）特征缺乏的限制，我们开发了一种新的分类网络CECT，通过可控的卷积神经网络和Transformer进行组合。CECT由卷积编码块、转置卷积解码块和Transformer分类块组成。不同于传统的基于CNN或Transformer的方法，我们的CECT可以同时捕捉多个局部和全局尺度上的特征。此外，本文提出的集合系数可以控制不同尺度局部特征的贡献。我们在两个公共COVID-19数据集上评估了CECT，并且在所有评估指标上均优于现有的最先进方法。由于其卓越的特征捕捉能力，我们相信CECT可以作为一种通用而有效的工具扩展到其他医学图像分类场景中。

    Most computer vision models are developed based on either convolutional neural network (CNN) or transformer, while the former (latter) method captures local (global) features. To relieve model performance limitations due to the lack of global (local) features, we develop a novel classification network CECT by controllable ensemble CNN and transformer. CECT is composed of a convolutional encoder block, a transposed-convolutional decoder block, and a transformer classification block. Different from conventional CNN- or transformer-based methods, our CECT can capture features at both multi-local and global scales. Besides, the contribution of local features at different scales can be controlled with the proposed ensemble coefficients. We evaluate CECT on two public COVID-19 datasets and it outperforms existing state-of-the-art methods on all evaluation metrics. With remarkable feature capture ability, we believe CECT can be extended to other medical image classification scenarios as a dia
    
[^120]: 图神经网络的零一定律

    Zero-One Laws of Graph Neural Networks. (arXiv:2301.13060v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13060](http://arxiv.org/abs/2301.13060)

    本文提出了一个新的理论研究视角，回答了当图节点数量变得非常大时GNN的行为如何的问题。通过证明不断增大的图映射到GNN分类器的特定输出的概率趋于零或一，建立了这些GNN的零一定律，限制了它们的能力。实验证实了理论结论。

    

    图神经网络(GNN)是用于对图进行机器学习的深度学习标准体系结构。这导致了大量的工作分析这些模型的能力和限制，特别是他们的表示和外推能力。我们提供了一个新的理论视角，回答了一个问题：当图节点的数量变得非常大时，GNNs的行为如何？在温和的假设下，我们证明，当我们从Erdős-Rényi模型中绘制不断增大的图时，这些图映射到GNN分类器的特定输出的概率趋于零或一。这个类包括流行的图卷积网络体系结构。这个结果建立了这些GNN的零一定律，并且类比于其他收敛定律，带来了它们在理论上的局限性。我们通过实验证实了我们的结果，观察到理论与实践相符。

    Graph neural networks (GNNs) are de facto standard deep learning architectures for machine learning on graphs. This has led to a large body of work analyzing the capabilities and limitations of these models, particularly pertaining to their representation and extrapolation capacity. We offer a novel theoretical perspective on the representation and extrapolation capacity of GNNs, by answering the question: how do GNNs behave as the number of graph nodes become very large? Under mild assumptions, we show that when we draw graphs of increasing size from the Erd\H{o}s-R\'enyi model, the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either zero or to one. This class includes the popular graph convolutional network architecture. The result establishes 'zero-one laws' for these GNNs, and analogously to other convergence laws, entails theoretical limitations on their capacity. We empirically verify our results, observing that the theoret
    
[^121]: SegViz：基于联邦学习的多器官分割框架，适用于具有部分注释的异构数据集

    SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations. (arXiv:2301.07074v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07074](http://arxiv.org/abs/2301.07074)

    SegViz是一种基于联邦学习的框架，用于从分布式的非i.i.d数据集中训练具有部分注释的分割模型。使用FedBN作为聚合策略的SegViz框架在外部BTCV集上表现出优异的性能，分割的dice分数分别为0.93、0.83、0.55和0.75。

    SegViz is a federated learning-based framework for training segmentation models from distributed non-i.i.d datasets with partial annotations. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation.

    分割是医学图像深度学习中最基本的任务之一，由于其多个下游临床应用而备受关注。然而，为医学图像生成手动注释是耗时的、需要高技能的、昂贵的工作，特别是对于3D图像。一个潜在的解决方案是从多个组的部分注释数据集中聚合知识，使用联邦学习协作训练全局模型。为此，我们提出了SegViz，一种基于联邦学习的框架，用于从分布式的非i.i.d数据集中训练具有部分注释的分割模型。将SegViz的性能与分别在每个数据集上单独训练模型以及集中聚合所有数据集并训练单个模型进行比较。使用FedBN作为聚合策略的SegViz框架在外部BTCV集上表现出优异的性能，分割的dice分数分别为0.93、0.83、0.55和0.75。

    Segmentation is one of the most primary tasks in deep learning for medical imaging, owing to its multiple downstream clinical applications. However, generating manual annotations for medical images is time-consuming, requires high skill, and is an expensive effort, especially for 3D images. One potential solution is to aggregate knowledge from partially annotated datasets from multiple groups to collaboratively train global models using Federated Learning. To this end, we propose SegViz, a federated learning-based framework to train a segmentation model from distributed non-i.i.d datasets with partial annotations. The performance of SegViz was compared against training individual models separately on each dataset as well as centrally aggregating all the datasets in one place and training a single model. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation 
    
[^122]: WuYun: 基于知识增强深度学习探索层次结构骨架引导的旋律生成

    WuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning. (arXiv:2301.04488v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2301.04488](http://arxiv.org/abs/2301.04488)

    WuYun是一种基于知识增强的深度学习算法，它将旋律分解成骨架和动态装饰音符两部分，并通过音乐领域知识提取旋律骨架，提供辅助指导来生成具有更好长期结构和音乐性的旋律。

    

    尽管深度学习革新了音乐生成领域，然而目前现有的结构化旋律生成方法仍然遵循端到端的一个音符接一个音符的生成模式，而且在对待每个音符时都一视同仁。这里我们提出 WuYun，一种基于知识增强的深度学习体系结构，用于改善生成旋律的结构。该方法首先生成最具结构重要性的音符以构造旋律骨架，然后在骨架的基础上填充动态装饰音符，生成完整的旋律。具体来说，我们使用音乐领域知识提取旋律骨架，并使用序列学习来重建旋律骨架，旋律骨架作为附加知识为旋律生成过程提供辅助指导。我们演示了 WuYun 能够生成具有更好长期结构和音乐性的旋律，并在所有主观评价度量标准上平均超过了其他最先进的方法0.51 。此研究提供一种多学科视角来设计深度学习框架中的旋律层次结构，该思路可扩展到更广泛的音乐生成任务中。

    Although deep learning has revolutionized music generation, existing methods for structured melody generation follow an end-to-end left-to-right note-by-note generative paradigm and treat each note equally. Here, we present WuYun, a knowledge-enhanced deep learning architecture for improving the structure of generated melodies, which first generates the most structurally important notes to construct a melodic skeleton and subsequently infills it with dynamically decorative notes into a full-fledged melody. Specifically, we use music domain knowledge to extract melodic skeletons and employ sequence learning to reconstruct them, which serve as additional knowledge to provide auxiliary guidance for the melody generation process. We demonstrate that WuYun can generate melodies with better long-term structure and musicality and outperforms other state-of-the-art methods by 0.51 on average on all subjective evaluation metrics. Our study provides a multidisciplinary lens to design melodic hie
    
[^123]: DARTS搜索空间的伪反转瓶颈卷积

    Pseudo-Inverted Bottleneck Convolution for DARTS Search Space. (arXiv:2301.01286v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01286](http://arxiv.org/abs/2301.01286)

    本文增加了ConvNeXt的微小设计变化来扩充DARTS搜索空间，提出了PIBConv块来减少计算占用，我们的架构在层数仅为2时优于一个具有类似规模的DARTS网络。

    

    可微架构搜索（DARTS）作为一种基于梯度的神经结构搜索方法引起了广泛关注。然而，自引入DARTS以来，很少有工作基于最先进的卷积神经网络结构设计原则逐步扩展DARTS搜索空间。本文旨在通过增加ConvNeXt的微小设计变化来逐步扩充DARTS搜索空间，并研究准确性、评估层数和计算成本之间的权衡。我们引入了伪反转瓶颈卷积（PIBConv）块，旨在减少ConvNeXt中提出的反转瓶颈块的计算占用。我们提出的架构对于评估层数的敏感度要小得多，并且在层数仅为2时显着优于一个具有类似规模的DARTS网络。此外，使用更少的层数，它不仅在较低的GMACs和参数数量下实现更高的准确性 ，计算占用也更小。

    Differentiable Architecture Search (DARTS) has attracted considerable attention as a gradient-based neural architecture search method. Since the introduction of DARTS, there has been little work done on adapting the action space based on state-of-art architecture design principles for CNNs. In this work, we aim to address this gap by incrementally augmenting the DARTS search space with micro-design changes inspired by ConvNeXt and studying the trade-off between accuracy, evaluation layer count, and computational cost. We introduce the Pseudo-Inverted Bottleneck Conv (PIBConv) block intending to reduce the computational footprint of the inverted bottleneck block proposed in ConvNeXt. Our proposed architecture is much less sensitive to evaluation layer count and outperforms a DARTS network with similar size significantly, at layer counts as small as 2. Furthermore, with less layers, not only does it achieve higher accuracy with lower computational footprint (measured in GMACs) and parame
    
[^124]: TriNet：防止自监督学习在ASR中完全或缓慢崩溃的稳定方法

    TriNet: stabilizing self-supervised learning from complete or slow collapse on ASR. (arXiv:2301.00656v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2301.00656](http://arxiv.org/abs/2301.00656)

    本文提出的TriNet采用三分支结构，可防止自监督学习在ASR中的崩溃，并在下游ASR任务中比SOTA方法Data2vec实现了6.06%的相对单词错误率降低（WERR）。

    

    自监督学习模型面临着突然的信息崩溃或缓慢的维度崩溃的挑战。本文提出了TriNet，通过引入一种新颖的三分支结构来防止崩溃和稳定预训练。TriNet学习自监督学习的潜在嵌入空间，并将其并入到一个更高级别的空间中以预测由冻结的教师生成的虚假目标向量。实验结果显示，所提出的方法显著稳定和加速了预训练，并在下游基准ASR任务中相对于最先进的Data2vec实现了6.06％的相对单词错误率降低（WERR）。我们会在https://github.com/tencent-ailab/ 上发布我们的代码。

    Self-supervised learning (SSL) models confront challenges of abrupt informational collapse or slow dimensional collapse. We propose TriNet, which introduces a novel triple-branch architecture for preventing collapse and stabilizing the pre-training. TriNet learns the SSL latent embedding space and incorporates it to a higher level space for predicting pseudo target vectors generated by a frozen teacher. Our experimental results show that the proposed method notably stabilizes and accelerates pre-training and achieves a relative word error rate reduction (WERR) of 6.06% compared to the state-of-the-art (SOTA) Data2vec for a downstream benchmark ASR task. We will release our code at https://github.com/tencent-ailab/.
    
[^125]: 支付宝用户下一步意图预测的概念知识图谱

    A Concept Knowledge Graph for User Next Intent Prediction at Alipay. (arXiv:2301.00503v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.00503](http://arxiv.org/abs/2301.00503)

    本文提出了一种基于概念知识图谱的用户下一步意图预测技术，实现了在支付宝网络平台上对1亿活跃用户的服务，并且在保持可解释性的情况下，有效地提高了下游任务的性能表现。

    

    本文介绍了使用概念知识图谱进行用户下一步意图预测的技术。该系统已在支付宝网络平台上部署，为超过1亿活跃用户提供服务。我们提出AlipayKG，用于显式地描述用户意图的离线概念知识图谱，模拟了用户的历史行为、丰富的内容以及它们之间的关系。我们提出了一种基于Transformer的模型，将来自知识图谱的专家规则整合到模型中以推断在线用户的下一步意图。实验结果表明，所提出的系统可以有效地提高下游任务的性能，同时保持可解释性。

    This paper illustrates the technologies of user next intent prediction with a concept knowledge graph. The system has been deployed on the Web at Alipay, serving more than 100 million daily active users. To explicitly characterize user intent, we propose AlipayKG, which is an offline concept knowledge graph in the Life-Service domain modeling the historical behaviors of users, the rich content interacted by users and the relations between them. We further introduce a Transformer-based model which integrates expert rules from the knowledge graph to infer the online user's next intent. Experimental results demonstrate that the proposed system can effectively enhance the performance of the downstream tasks while retaining explainability.
    
[^126]: 考虑长尾分布因素的场景图生成算法

    Skew Class-balanced Re-weighting for Unbiased Scene Graph Generation. (arXiv:2301.00351v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00351](http://arxiv.org/abs/2301.00351)

    本文提出了一种用于场景图生成(SGG)任务的算法，考虑了长尾分布对无偏谓词预测的影响，并且采用偏斜类平衡重加权(SCR)损失函数来更好地权衡大多数谓词和少数谓词。实验结果表明SCR方法超越了现有最先进的方法。

    

    该论文提出了一种无偏场景图生成(SGG)算法，称为偏斜类平衡重加权(SCR)，旨在考虑长尾分布所导致的无偏谓词预测。以往研究主要集中在缓解较小的谓词预测的性能下降，显示了急剧下降的召回得分，即失去了大多数谓词的性能。这还没有正确分析有限SGG数据集中多数谓词性能和少数谓词性能之间的权衡。为了缓解这个问题，本文考虑了偏斜类平衡重加权(SCR)损失函数，并应用于无偏SGG模型。利用偏斜谓词预测的斜率，SCR估计目标谓词权重系数，然后重新为偏斜谓词加权，以更好地权衡大多数谓词和少数谓词。在标准Visual Genome数据集和Oxford Visual Geometry Group(VGG)数据集上进行了广泛实验，结果表明提出的SCR方法在召回和mAP方面超越了最先进的方法，为SGG任务提供了更稳健和无偏的解决方案。

    An unbiased scene graph generation (SGG) algorithm referred to as Skew Class-balanced Re-weighting (SCR) is proposed for considering the unbiased predicate prediction caused by the long-tailed distribution. The prior works focus mainly on alleviating the deteriorating performances of the minority predicate predictions, showing drastic dropping recall scores, i.e., losing the majority predicate performances. It has not yet correctly analyzed the trade-off between majority and minority predicate performances in the limited SGG datasets. In this paper, to alleviate the issue, the Skew Class-balanced Re-weighting (SCR) loss function is considered for the unbiased SGG models. Leveraged by the skewness of biased predicate predictions, the SCR estimates the target predicate weight coefficient and then re-weights more to the biased predicates for better trading-off between the majority predicates and the minority ones. Extensive experiments conducted on the standard Visual Genome dataset and O
    
[^127]: SPARF：从少量输入图像中学习大规模的 3D 稀疏辐射场

    SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images. (arXiv:2212.09100v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.09100](http://arxiv.org/abs/2212.09100)

    提出了一个大规模的 ShapeNet 合成数据集 SPARF，包括超过 100 万个有多个体素分辨率的 3D 优化的辐射场，用于新视角合成。同时提出了一种新颖的管线 SuRFNet，通过学习少量视图生成稀疏体素辐射场。

    

    最近神经辐射场 (NeRFs) 的进展将新视角合成问题看作是稀疏辐射场 (SRF) 优化，使用稀疏体素进行高效快速渲染 (plenoxels, InstantNGP)。为了利用机器学习和采用 SRF 作为 3D 表示，我们提出了 SPARF，这是一个基于 ShapeNet 的大规模合成数据集，用于新视角合成，由 $\sim$ 17 百万张图像组成，从近 40,000 个高分辨率形状渲染而来 (400 X 400 像素)。该数据集比现有的用于新视角合成的合成数据集大几个数量级，包括超过一百万个具有多个体素分辨率的 3D 优化过的辐射场。此外，我们提出了一种新颖的管线（SuRFNet），它从少量视图中学习生成稀疏体素辐射场。这是通过使用密集收集的 SPARF 数据集和 3D 稀疏卷积来实现的。SuRFNet 使用少量/单个图像的部分 SRF 和特定的 SRF 损失来学习生成稀疏体素辐射场。

    Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels for efficient and fast rendering (plenoxels,InstantNGP). In order to leverage machine learning and adoption of SRFs as a 3D representation, we present SPARF, a large-scale ShapeNet-based synthetic dataset for novel view synthesis consisting of $\sim$ 17 million images rendered from nearly 40,000 shapes at high resolution (400 X 400 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis and includes more than one million 3D-optimized radiance fields with multiple voxel resolutions. Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate sparse voxel radiance fields from only few views. This is done by using the densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs partial SRFs from few/one images and a specialized SRF loss to learn to gener
    
[^128]: 探究机械学科中分类问题的深度学习模型校准

    Investigating Deep Learning Model Calibration for Classification Problems in Mechanics. (arXiv:2212.00881v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00881](http://arxiv.org/abs/2212.00881)

    本研究通过对7个开放数据集的研究发现，在工程力学领域应用深度学习模型时，需要仔细考虑模型校准问题，以提高预测准确性。

    

    近年来，将机器学习方法应用于工程力学问题的兴趣日益增长。特别是，人们对将深度学习技术应用于预测异质材料和结构的力学行为表现出了极大的兴趣。研究人员已经表明，深度学习方法能够有效地预测从工程复合材料到几何复杂的超材料再到异质生物组织等各种系统的机械行为，并且误差较低。然而，对于深度学习模型校准，即预测结局的概率和真实结局概率之间的匹配，关注相对较少。在这项工作中，我们对涵盖三种不同类型机械问题的七个开放数据集中的机器学习模型以及模型校准误差进行了全面调查。

    Recently, there has been a growing interest in applying machine learning methods to problems in engineering mechanics. In particular, there has been significant interest in applying deep learning techniques to predicting the mechanical behavior of heterogeneous materials and structures. Researchers have shown that deep learning methods are able to effectively predict mechanical behavior with low error for systems ranging from engineered composites, to geometrically complex metamaterials, to heterogeneous biological tissue. However, there has been comparatively little attention paid to deep learning model calibration, i.e., the match between predicted probabilities of outcomes and the true probabilities of outcomes. In this work, we perform a comprehensive investigation into ML model calibration across seven open access engineering mechanics datasets that cover three distinct types of mechanical problems. Specifically, we evaluate both model and model calibration error for multiple mach
    
[^129]: 基于图卷积网络的工程图纸组件分割

    Component Segmentation of Engineering Drawings Using Graph Convolutional Networks. (arXiv:2212.00290v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.00290](http://arxiv.org/abs/2212.00290)

    提出了一种基于图卷积网络的工程图纸组件分割方法，能够自动完成2D工程零件图纸的矢量化和机器解读，并在基准数据集上取得最先进的组件分割准确性结果。

    

    本文提出一种数据驱动框架，自动完成2D工程零件图纸的矢量化和机器解读。我们通过图卷积网络预测工程图纸中每个矢量笔画的语义类型，以识别组件，并在基准数据集上取得最先进的组件分割准确性结果。

    We present a data-driven framework to automate the vectorization and machine interpretation of 2D engineering part drawings. In industrial settings, most manufacturing engineers still rely on manual reads to identify the topological and manufacturing requirements from drawings submitted by designers. The interpretation process is laborious and time-consuming, which severely inhibits the efficiency of part quotation and manufacturing tasks. While recent advances in image-based computer vision methods have demonstrated great potential in interpreting natural images through semantic segmentation approaches, the application of such methods in parsing engineering technical drawings into semantically accurate components remains a significant challenge. The severe pixel sparsity in engineering drawings also restricts the effective featurization of image-based data-driven methods. To overcome these challenges, we propose a deep learning based framework that predicts the semantic type of each v
    
[^130]: 神经元集合推理方法生成模型的泛化

    Generalization of generative model for neuronal ensemble inference method. (arXiv:2211.05634v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2211.05634](http://arxiv.org/abs/2211.05634)

    提出了基于泛化的神经元集合推理方法，解决了贝叶斯推断模型中神经元活动非平稳性的问题。

    

    无数神经元的相互作用构成了维持生命活动所必需的各种脑功能，因此，分析功能神经网络的结构非常重要。为了阐明大脑功能的机制，包括神经科学各个领域在内的许多研究正在积极开展功能神经元集合和中心结构的研究。此外，最近的研究表明，功能神经元集合和中心的存在有助于提高信息处理效率。因此，需要从神经元活动数据中推断功能神经元集合的方法，基于贝叶斯推断的方法已经被提出。然而，在贝叶斯推断中建立活动模型存在问题。由于每个神经元的活动特征取决于生理实验条件，因此其具有非平稳性。结果，在贝叶斯推断模型中假设的平稳性会妨碍推断。

    Various brain functions that are necessary to maintain life activities materialize through the interaction of countless neurons. Therefore, it is important to analyze the structure of functional neuronal network. To elucidate the mechanism of brain function, many studies are being actively conducted on the structure of functional neuronal ensemble and hub, including all areas of neuroscience. In addition, recent study suggests that the existence of functional neuronal ensembles and hubs contributes to the efficiency of information processing. For these reasons, there is a demand for methods to infer functional neuronal ensembles from neuronal activity data, and methods based on Bayesian inference have been proposed. However, there is a problem in modeling the activity in Bayesian inference. The features of each neuron's activity have non-stationarity depending on physiological experimental conditions. As a result, the assumption of stationarity in Bayesian inference model impedes infer
    
[^131]: 音乐混音风格转换：采用对比学习方法解开音频效果的紧密联系

    Music Mixing Style Transfer: A Contrastive Learning Approach to Disentangle Audio Effects. (arXiv:2211.02247v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.02247](http://arxiv.org/abs/2211.02247)

    本论文提出了一种对比学习方法，用于实现音乐混音风格转换。该方法采用已处理的数据集进行自监督训练，并使用编码器从参考歌曲中提取仅与音频效果相关的信息。系统实现了多轨音频的混音风格转换，并且在使用音乐源分离模型时具有鲁棒性。

    

    我们提出了一种端到端的音乐混音风格转换系统，将输入多轨混音的混音风格转换为参考歌曲的风格。这是通过使用一个预先经过对比目标训练的编码器来实现的，该编码器从参考音乐录音中提取仅与音频效果相关的信息。我们所有的模型都是自监督方式训练的，使用一种有效的数据预处理方法从已处理的湿度多轨数据集中缓解了获取未处理干燥数据的数据稀缺性。我们对所提出的编码器进行分析，以确定其分离音频效果的能力，并通过客观和主观评估验证其混音风格转换的性能。从结果中，我们展示了所提出的系统不仅可以将多轨音频的混音风格转换为参考风格，而且在使用音乐源分离模型时也具有混合风格转换的鲁棒性。

    We propose an end-to-end music mixing style transfer system that converts the mixing style of an input multitrack to that of a reference song. This is achieved with an encoder pre-trained with a contrastive objective to extract only audio effects related information from a reference music recording. All our models are trained in a self-supervised manner from an already-processed wet multitrack dataset with an effective data preprocessing method that alleviates the data scarcity of obtaining unprocessed dry data. We analyze the proposed encoder for the disentanglement capability of audio effects and also validate its performance for mixing style transfer through both objective and subjective evaluations. From the results, we show the proposed system not only converts the mixing style of multitrack audio close to a reference but is also robust with mixture-wise style transfer upon using a music source separation model.
    
[^132]: 基于双重平滑先验学习信号的超图结构

    Learning Hypergraphs From Signals With Dual Smoothness Prior. (arXiv:2211.01717v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01717](http://arxiv.org/abs/2211.01717)

    本研究提出了一种基于双重平滑先验的超图结构学习框架，可从观察到的信号中学习超图结构以捕获实体间的内在高阶关系。

    

    超图结构学习是从观察到的信号中学习超图结构，以捕捉实体之间内在的高阶关系，当数据集中没有可用的超图拓扑结构时，这变得非常关键。本文提出了一种新的双重平滑先验的超图结构学习框架HGSL，通过把每个超边与具有节点信号平滑性和边连接性的子图对应起来，揭示了观察到的节点信号和超图结构之间的映射。实验结果表明了该方法的有效性。

    Hypergraph structure learning, which aims to learn the hypergraph structures from the observed signals to capture the intrinsic high-order relationships among the entities, becomes crucial when a hypergraph topology is not readily available in the datasets. There are two challenges that lie at the heart of this problem: 1) how to handle the huge search space of potential hyperedges, and 2) how to define meaningful criteria to measure the relationship between the signals observed on nodes and the hypergraph structure. In this paper, for the first challenge, we adopt the assumption that the ideal hypergraph structure can be derived from a learnable graph structure that captures the pairwise relations within signals. Further, we propose a hypergraph structure learning framework HGSL with a novel dual smoothness prior that reveals a mapping between the observed node signals and the hypergraph structure, whereby each hyperedge corresponds to a subgraph with both node signal smoothness and e
    
[^133]: eDiff-I: 一种使用专家去噪模型集合的文本到图像扩散模型

    eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers. (arXiv:2211.01324v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.01324](http://arxiv.org/abs/2211.01324)

    本文提出一种使用专家去噪模型集合的文本到图像扩散模型。文本-图像合成过程中的生成是一个渐进的过程，而在生成的不同阶段，它的合成行为会有所不同。因此本文提出针对不同阶段的专门模型的构想。

    

    大规模的基于扩散的生成模型在文本相关高分辨率图像合成方面取得了突破。这些文本-图像扩散模型从随机噪声开始，通过迭代地在文本提示的条件下逐步合成图像。我们发现，这种合成行为在生成过程中会定性地改变：在采样早期，生成强烈依赖于文本提示以生成针对文本的内容，而在后期，文本条件几乎被忽略了。这表明在整个生成过程中共享模型参数可能并不理想。因此，与现有工作不同，我们提出训练一组针对不同合成阶段专门的文本-图像扩散模型的集合。为了保持训练效率，我们最初训练单个模型，然后将其分成专门的模型，为迭代生成过程的特定阶段进行训练。我们的扩散模型集合

    Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion model
    
[^134]: 利用潜在空间先验知识的演示应用于强化学习

    Leveraging Demonstrations with Latent Space Priors. (arXiv:2210.14685v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14685](http://arxiv.org/abs/2210.14685)

    本文提出了一种方法，通过结合技能学习和序列建模，利用演示数据集中的潜在空间先验知识来加速强化学习中高层次策略的学习，并在实验中证实了该方法的有效性。

    

    演示可以提供有关状态或动作空间的相关信息，具有提高强化学习智能体的效率和实用性的巨大潜力。本文提出了一种通过结合技能学习和序列建模来利用演示数据集的方法。从一个学习的联合潜在空间开始，我们分别训练演示序列的生成模型和相应的低层策略。序列模型形成了潜在空间对合理的演示行为的先验知识，以加速高层次策略的学习。我们展示了如何从仅状态的运动捕捉演示中获取这些先验知识，并探索了几种将它们整合到转移任务的策略学习中的方法。我们的实验结果证实了潜在空间先验知识在学习速度和最终性能方面都提供了显著的增益。我们在一组具有复杂、模拟的人形机器人的挑战性稀疏奖励环境和离线强化学习基准测试中对我们的方法进行了基准测试，并证明了我们的方法的有效性。

    Demonstrations provide insight into relevant state or action space regions, bearing great potential to boost the efficiency and practicality of reinforcement learning agents. In this work, we propose to leverage demonstration datasets by combining skill learning and sequence modeling. Starting with a learned joint latent space, we separately train a generative model of demonstration sequences and an accompanying low-level policy. The sequence model forms a latent space prior over plausible demonstration behaviors to accelerate learning of high-level policies. We show how to acquire such priors from state-only motion capture demonstrations and explore several methods for integrating them into policy learning on transfer tasks. Our experimental results confirm that latent space priors provide significant gains in learning speed and final performance. We benchmark our approach on a set of challenging sparse-reward environments with a complex, simulated humanoid, and on offline RL benchmar
    
[^135]: 通过可达性分析和多项式Zonotopes实现行动投影的可证明安全强化学习

    Provably Safe Reinforcement Learning via Action Projection using Reachability Analysis and Polynomial Zonotopes. (arXiv:2210.10691v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.10691](http://arxiv.org/abs/2210.10691)

    本研究提出了通过可达性分析和多项式Zonotopes实现行动投影的可证明安全强化学习，该方法通过混合整数优化实现，并能够有效地处理输入限制和动态障碍物。

    

    虽然强化学习在许多应用中产生了非常有前途的结果，但其主要缺点是缺乏安全保障，这阻止了其在安全关键系统中的使用。本文针对非线性连续系统提出了一个用于解决到达-避免任务的安全屏障，并通过将提出的动作投影到最接近的安全动作来防止强化学习代理应用可能不安全的动作。这种方法称为行动投影，是通过混合整数优化实现的。行动投影的安全约束是通过应用参数化可达性分析使用多项式Zonotopes得到的，这使得能够准确捕捉动作对系统的非线性影响。与其他最先进的行动投影方法相比，我们的安全屏障可以有效地处理输入限制和动态障碍物，使得机器人空间维度的整合更加容易。

    While reinforcement learning produces very promising results for many applications, its main disadvantage is the lack of safety guarantees, which prevents its use in safety-critical systems. In this work, we address this issue by a safety shield for nonlinear continuous systems that solve reach-avoid tasks. Our safety shield prevents applying potentially unsafe actions from a reinforcement learning agent by projecting the proposed action to the closest safe action. This approach is called action projection and is implemented via mixed-integer optimization. The safety constraints for action projection are obtained by applying parameterized reachability analysis using polynomial zonotopes, which enables to accurately capture the nonlinear effects of the actions on the system. In contrast to other state-of-the-art approaches for action projection, our safety shield can efficiently handle input constraints and dynamic obstacles, eases incorporation of the spatial robot dimensions into the 
    
[^136]: 通过自监督表示学习的训练集清洁消除后门攻击

    Training set cleansing of backdoor poisoning by self-supervised representation learning. (arXiv:2210.10272v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10272](http://arxiv.org/abs/2210.10272)

    本文使用自监督表示学习清洁后门攻击所传染的神经网络模型的训练数据集。该方法通过学习样本嵌入表示来识别被毒化的和干净的样本，无需知道攻击者使用的后门触发器。在不同数据集的实验中，我们的方法优于现有方法。

    

    后门或特洛伊攻击是针对深度神经网络分类器的一种重要数据污染攻击类型，在该攻击中，训练数据集被毒化了一小部分样本，每个样本都具有后门模式（通常是不可察觉的或无害的模式），并被标记为攻击者的目标类别。当在后门毒化的数据集上进行训练时，DNN在大多数良性测试样本上表现正常，但当测试样本中包含包含后门触发器的后门模式时（即含有后门触发器），它会向目标类别做出错误的预测。本文聚焦于图像分类任务，展示了监督式训练可能会构建更强的后门模式与关联的目标类别之间的联系，而不是正常特征与真实起源类别之间的联系。相比之下，自监督表示学习忽略样本标签并基于图像的语义内容学习特征嵌入。因此，我们提出使用自监督表示学习来清除后门毒化样本的训练集。具体而言，我们通过一个自监督模型学习的表示来区分毒化和干净的样本，从而训练一个深度神经网络。我们的方法不需要任何关于攻击者使用的后门触发器的知识，在攻击者将触发器适应于与正常图像具有高感知相似性的情况下仍然有效。实验结果表明，我们的方法在不同数据集上的后门准确性和干净准确性方面优于现有方法。

    A backdoor or Trojan attack is an important type of data poisoning attack against deep neural network (DNN) classifiers, wherein the training dataset is poisoned with a small number of samples that each possess the backdoor pattern (usually a pattern that is either imperceptible or innocuous) and which are mislabeled to the attacker's target class. When trained on a backdoor-poisoned dataset, a DNN behaves normally on most benign test samples but makes incorrect predictions to the target class when the test sample has the backdoor pattern incorporated (i.e., contains a backdoor trigger). Here we focus on image classification tasks and show that supervised training may build stronger association between the backdoor pattern and the associated target class than that between normal features and the true class of origin. By contrast, self-supervised representation learning ignores the labels of samples and learns a feature embedding based on images' semantic content. %We thus propose to us
    
[^137]: Mask-adapted CLIP的开放词汇语义分割

    Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP. (arXiv:2210.04150v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.04150](http://arxiv.org/abs/2210.04150)

    本文提出了一种基于CLIP和带掩膜的体系结构的开放词汇语义分割方法，通过在用于训练的嘈杂但多样化的数据集上对CLIP进行微调，以提高其在带有掩膜的图像上的性能，超越了当前最佳的方法。

    

    开放词汇语义分割的目的是根据文本描述将图像分割为语义区域，这些描述可能在训练期间没有被观察到。最近的两阶段方法首先生成不考虑类的掩码提议，然后利用预训练的视觉-语言模型（例如CLIP）对掩码区域进行分类。我们确定了这一范例的性能瓶颈是预训练CLIP模型，因为它在遮蔽图像上的表现不佳。为了解决这个问题，我们提出在一组带有掩码图像区域及其对应的文本描述的数据上对CLIP进行微调。我们通过使用CLIP将掩码图像区域与图像描述中的名词匹配来挖掘现有图像-标题数据集（例如COCO Captions）来收集训练数据。与更精确且手动注释的固定类别分割标签（例如COCO-Stuff）相比，我们发现我们的嘈杂但多样化的数据集能更好地保留CLIP的泛化能力。除了微调整个模型外，我们还引入了适应掩码的CLIP体系结构，通过明确地建模蒙版过程来更好地处理带有掩码的图像。我们在开放词汇语义分割基准OpenImages上的实验证明，我们的方法优于最先进的方法。

    Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions. We collect training data by mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, w
    
[^138]: 基于贝叶斯的提示学习用于图像-语言模型泛化

    Bayesian Prompt Learning for Image-Language Model Generalization. (arXiv:2210.02390v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.02390](http://arxiv.org/abs/2210.02390)

    本文提出了一种基于贝叶斯方法的提示学习框架，对提示空间进行正则化，提高了对未见提示的泛化能力。

    

    基础的图像-语言模型因其高效的适应下游任务的提示学习而引起了广泛关注。提示学习将语言模型输入的一部分视为可训练的，同时冻结其余部分，并优化经验风险最小化目标。然而，经验风险最小化已知受到分布偏移的影响，这影响了对训练过程中未见提示的泛化能力。通过利用贝叶斯方法的正则化能力，我们从贝叶斯角度考虑提示学习，并将其制定为变分推断问题。我们的方法对提示空间进行正则化，减少对已见提示的过度拟合，并提高了对未见提示的提示泛化能力。我们的框架通过以概率的方式对输入提示空间进行建模，作为先验分布，使我们的提议与基于图像无条件或有条件的提示学习方法兼容。我们进行了实验证明了本提出的方法的有效性。

    Foundational image-language models have generated considerable interest due to their efficient adaptation to downstream tasks by prompt learning. Prompt learning treats part of the language model input as trainable while freezing the rest, and optimizes an Empirical Risk Minimization objective. However, Empirical Risk Minimization is known to suffer from distributional shifts which hurt generalizability to prompts unseen during training. By leveraging the regularization ability of Bayesian methods, we frame prompt learning from the Bayesian perspective and formulate it as a variational inference problem. Our approach regularizes the prompt space, reduces overfitting to the seen prompts and improves the prompt generalization on unseen prompts. Our framework is implemented by modeling the input prompt space in a probabilistic manner, as an a priori distribution which makes our proposal compatible with prompt learning approaches that are unconditional or conditional on the image. We demon
    
[^139]: 具有潜在空间插值的平滑图像转换

    Smooth image-to-image translations with latent space interpolations. (arXiv:2210.00841v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.00841](http://arxiv.org/abs/2210.00841)

    该论文提出了两种正则化方法——收缩损失和Mixup数据增强策略，用于解决图像跨域插值中出现的平滑性问题；同时引入了一种新的度量标准来定量评估插值平滑度。

    

    多域图像转换可以根据目标域的风格将源图像进行转换。这些变换的重要特征之一是graduality，即当它们各自的潜在空间表示被线性插值时，源图像和目标图像之间的平滑变化。然而，尽管现有方法可以在内部域上线性插值得到非常逼真的结果，但在评估跨域插值时通常表现不佳，经常产生外观突变或非真实中间图像。本文认为，这个问题的主要原因之一是缺乏足够的跨域训练数据，因此提出了两种不同的正则化方法来缓解这个问题：一种新的收缩损失，它可以压缩潜在空间；一种Mixup数据增强策略，它可以使域之间的样式表示变得平坦。还提出了一种新的度量标准来定量评估插值平滑度，通过测量中间图像与一种更昂贵的基于优化的方法得到的图像之间的感知相似度来完成。

    Multi-domain image-to-image (I2I) translations can transform a source image according to the style of a target domain. One important, desired characteristic of these transformations, is their graduality, which corresponds to a smooth change between the source and the target image when their respective latent-space representations are linearly interpolated. However, state-of-the-art methods usually perform poorly when evaluated using inter-domain interpolations, often producing abrupt changes in the appearance or non-realistic intermediate images. In this paper, we argue that one of the main reasons behind this problem is the lack of sufficient inter-domain training data and we propose two different regularization methods to alleviate this issue: a new shrinkage loss, which compacts the latent space, and a Mixup data-augmentation strategy, which flattens the style representations between domains. We also propose a new metric to quantitatively evaluate the degree of the interpolation smo
    
[^140]: 带有对数线性策略参数化的自然策略梯度的线性收敛

    Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization. (arXiv:2209.15382v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15382](http://arxiv.org/abs/2209.15382)

    本文分析了无限时间折扣马尔科夫决策过程中，使用对数线性策略参数化的未正则化自然策略梯度算法的收敛速率，证明了一定条件下该算法具有线性收敛保证。

    

    本文分析了在无限时间折扣马尔科夫决策过程中，使用对数线性策略参数化的未正则化自然策略梯度算法的收敛速率。在确定性情况下，当 Q-值是已知的并且可以通过已知的特征函数的线性组合来逼近一个偏差误差时，我们显示出一种几何增长步长可以线性收敛到最优策略。然后，我们考虑样本为基础的情况，即在已知的特征函数的线性组合中，Q 值函数的最佳表示已知，但可能存在估计误差。在这种情况下，我们展示了该算法具有与确定性情况下相同的线性保证，直到一个依赖于估计误差、偏差误差和特征协方差矩阵条件数的误差项为止。我们的结果基于策略镜像下降的一般框架，并扩展了先前针对 softmax 表参数化的发现。

    We analyze the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy. We then consider the sample-based case, when the best representation of the Q- value function among linear combinations of a known feature function is known up to an estimation error. In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametri
    
[^141]: 参数修剪的数据集蒸馏方法

    Dataset Distillation Using Parameter Pruning. (arXiv:2209.14609v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.14609](http://arxiv.org/abs/2209.14609)

    本文提出了一种使用参数修剪的数据集蒸馏方法，该方法可以在蒸馏过程中修剪难以匹配的参数，提高蒸馏性能。

    

    在许多领域中，获得先进模型的方法取决于大型数据集，这使得数据存储和模型训练变得昂贵。作为解决方案，数据集蒸馏可以合成保留原始大型数据集大多数信息的小型数据集。最近提出的匹配网络参数的数据集蒸馏方法已被证明在几个数据集上有效。然而，网络参数的维度通常很大。此外，一些参数在蒸馏过程中难以匹配，降低了蒸馏性能。基于这个观察，本研究提出了一种基于参数修剪的新型数据集蒸馏方法来解决这个问题。该方法可以在蒸馏过程中修剪难以匹配的参数，从而合成更加稳健的蒸馏数据集并提高蒸馏性能。在三个数据集上的实验结果表明，该方法优于其他最先进的数据集蒸馏方法。

    In many fields, the acquisition of advanced models depends on large datasets, making data storage and model training expensive. As a solution, dataset distillation can synthesize a small dataset that preserves most information of the original large dataset. The recently proposed dataset distillation method by matching network parameters has been proven effective for several datasets. However, the dimensions of network parameters are typically large. Furthermore, some parameters are difficult to match during the distillation process, degrading distillation performance. Based on this observation, this study proposes a novel dataset distillation method based on parameter pruning that solves the problem. The proposed method can synthesize more robust distilled datasets and improve distillation performance by pruning difficult-to-match parameters during the distillation process. Experimental results on three datasets show that the proposed method outperforms other state-of-the-art dataset d
    
[^142]: 自适应脉冲网络：使用可学习神经动力学的脉冲神经网络进行事件驱动光流估计

    Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking Neural Networks with Learnable Neuronal Dynamics. (arXiv:2209.11741v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.11741](http://arxiv.org/abs/2209.11741)

    本文提出了一种自适应脉冲神经网络框架，用于事件驱动光流估计任务，并通过神经动力学来解决脉冲消失问题。实验结果表明，该方法在各项公共基准测试中均取得了最先进的性能。

    

    最近，事件驱动相机凭借其异步捕捉丰富时间信息的能力，展现出高速运动估计的巨大潜力。脉冲神经网络（SNN）凭借其神经启发式事件驱动处理，可以有效地处理此类异步数据，而类似漏电整流与放电（LIF）的神经元模型可以跟踪输入中包含的关键时间信息。脉冲神经网络通过在神经元内存中维护动态状态来实现这一点，保留重要信息同时随着时间流逝遗忘冗余数据。因此，我们认为与同样大小的模拟神经网络相比，脉冲神经网络将允许在顺序回归任务上取得更好的性能。然而，由于后面的神经元处的脉冲消失问题，深度脉冲神经网络很难训练。为此，我们在神经元动态中引入了可学习的自适应阈值，通过梯度衰减反向传播解决了脉冲消失问题。在光流估计这一高速运动估计任务上，我们通过在许多公共基准测试中显著减少参数使用的方式，获得了最先进的性能。

    Event-based cameras have recently shown great potential for high-speed motion estimation owing to their ability to capture temporally rich information asynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired event-driven processing can efficiently handle such asynchronous data, while neuron models such as the leaky-integrate and fire (LIF) can keep track of the quintessential timing information contained in the inputs. SNNs achieve this by maintaining a dynamic state in the neuron memory, retaining important information while forgetting redundant data over time. Thus, we posit that SNNs would allow for better performance on sequential regression tasks compared to similarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult to train due to vanishing spikes at later layers. To that effect, we propose an adaptive fully-spiking framework with learnable neuronal dynamics to alleviate the spike vanishing problem. We utilize surrogate gradient-based backpro
    
[^143]: NIERT: 使用Transformer编码器统一散乱数据表示以实现准确数值内插

    NIERT: Accurate Numerical Interpolation through Unifying Scattered Data Representations using Transformer Encoder. (arXiv:2209.09078v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.09078](http://arxiv.org/abs/2209.09078)

    NIERT是一种使用Transformer编码器的数值内插方法，利用仅编码器结构对观察点和目标点进行嵌入和统一表示，从而实现了更准确的内插和更广泛的泛化能力。

    

    散乱数据的内插是数值分析中的一个经典问题，具有悠久的理论和实践贡献历史。近期的进展利用深度神经网络构建内插器，表现出优秀且可推广的性能。然而，它们仍在两个方面存在不足：\textbf{1）表示学习不足}，因为在流行的编码器-解码器框架中，观察点和目标点的嵌入是分开的；\textbf{2）有限的泛化能力}，因为忽视了在不同领域之间共享的先前内插知识。为了克服这些限制，我们提出了一种使用\textbf{T}ransformer的\textbf{E}ncoder \textbf{R}epresentation实现\textbf{N}umerical \textbf{I}nterpolation（称为\textbf{NIERT}）的方法。NIERT利用仅编码器结构，而不是编码器-解码器结构。这样，NIERT可以将观察点和目标点嵌入到一个统一的编码器表示空间中，因此实现了更准确的内插，更广泛的泛化能力和更简洁的可解释性。

    Interpolation for scattered data is a classical problem in numerical analysis, with a long history of theoretical and practical contributions. Recent advances have utilized deep neural networks to construct interpolators, exhibiting excellent and generalizable performance. However, they still fall short in two aspects: \textbf{1) inadequate representation learning}, resulting from separate embeddings of observed and target points in popular encoder-decoder frameworks and \textbf{2) limited generalization power}, caused by overlooking prior interpolation knowledge shared across different domains. To overcome these limitations, we present a \textbf{N}umerical \textbf{I}nterpolation approach using \textbf{E}ncoder \textbf{R}epresentation of \textbf{T}ransformers (called \textbf{NIERT}). On one hand, NIERT utilizes an encoder-only framework rather than the encoder-decoder structure. This way, NIERT can embed observed and target points into a unified encoder representation space, thus effec
    
[^144]: DC-Art-GAN: 使用 DC-GAN 进行数字艺术的稳定过程生成

    DC-Art-GAN: Stable Procedural Content Generation using DC-GANs for Digital Art. (arXiv:2209.02847v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.02847](http://arxiv.org/abs/2209.02847)

    DC-Art-GAN 使用对抗性训练的 DC-GAN 进行艺术作品的稳定生成和多样性生成，实现了从随机噪声中合成虚拟现实中不存在的逼真图像。

    

    数字艺术是使用数字技术作为生成或创造过程的艺术方法。随着数字货币和 NFT（不可替代代币）的出现，对数字艺术的需求正日益增长。本文主要倡导使用具有对抗性训练的深度生成网络来实现稳定和多样的艺术生成。该工作主要侧重于使用深度卷积生成对抗网络（DC-GAN），并探索了解决 GAN 训练中常见问题的技巧。我们比较了各种 DC-GAN 的架构和设计，以确定一种可推荐的设计选择，以实现稳定和逼真的图像生成。工作的主要重点是生成现实中不存在但由所提出的模型从随机噪声中合成的逼真图像。我们提供了生成的动物面部图像的视觉结果（一些证据显示了不同物种的混合），并提供了有关 DC-Art-GAN 模型的培训、架构和设计的建议。

    Art is an artistic method of using digital technologies as a part of the generative or creative process. With the advent of digital currency and NFTs (Non-Fungible Token), the demand for digital art is growing aggressively. In this manuscript, we advocate the concept of using deep generative networks with adversarial training for a stable and variant art generation. The work mainly focuses on using the Deep Convolutional Generative Adversarial Network (DC-GAN) and explores the techniques to address the common pitfalls in GAN training. We compare various architectures and designs of DC-GANs to arrive at a recommendable design choice for a stable and realistic generation. The main focus of the work is to generate realistic images that do not exist in reality but are synthesised from random noise by the proposed model. We provide visual results of generated animal face images (some pieces of evidence showing a blend of species) along with recommendations for training, architecture and des
    
[^145]: 一种扩散模型预测2D显微图像中的3D细胞形状

    A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images. (arXiv:2208.14125v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.14125](http://arxiv.org/abs/2208.14125)

    该论文介绍了一种基于扩散模型的方法，用于从2D显微图像中预测真实的3D单个细胞形状。该方法被成功应用于单个细胞分类任务。该模型学习从2D显微镜图像中重建具有逼真形态学特征的3D形状。

    

    扩散模型是一种特定类型的生成模型，能够从学习的分布中合成新数据。我们介绍了一种基于扩散的模型DISPR，用于解决从二维单个细胞显微图像预测三维细胞形状的反问题。利用二维显微图像作为先验条件，DISPR被调整为预测逼真的三维形状重建。我们从六个高度不平衡的类别中提取形态学特征，展示了DISPR作为数据增强工具在基于特征的单个细胞分类任务中的适用性。将DISPR预测的特征添加到三个少数类中，将宏F1得分从$F1_{macro}=55.2\pm4.6\%$提高到$F1_{macro}=72.2\pm4.9\%$。因此，我们证明了扩散模型可以成功应用于反生物医学问题，并且它们能够学习从2D显微图像中重建具有逼真形态学特征的3D形状。

    Diffusion models are a special type of generative model, capable of synthesising new data from a learnt distribution. We introduce DISPR, a diffusion-based model for solving the inverse problem of three-dimensional (3D) cell shape prediction from two-dimensional (2D) single cell microscopy images. Using the 2D microscopy image as a prior, DISPR is conditioned to predict realistic 3D shape reconstructions. To showcase the applicability of DISPR as a data augmentation tool in a feature-based single cell classification task, we extract morphological features from the red blood cells grouped into six highly imbalanced classes. Adding features from the DISPR predictions to the three minority classes improved the macro F1 score from $F1_\text{macro} = 55.2 \pm 4.6\%$ to $F1_\text{macro} = 72.2 \pm 4.9\%$. We thus demonstrate that diffusion models can be successfully applied to inverse biomedical problems, and that they learn to reconstruct 3D shapes with realistic morphological features from
    
[^146]: 在再生核希尔伯特空间系列中的集成预测：仙境中的动力系统。

    Ensemble forecasts in reproducing kernel Hilbert space family: dynamical systems in Wonderland. (arXiv:2207.14653v2 [math-ph] UPDATED)

    [http://arxiv.org/abs/2207.14653](http://arxiv.org/abs/2207.14653)

    本文提出了一种在动力系统中的集合预测和模拟的方法，将系统嵌入再生核希尔伯特空间族，并在该空间中使用简单的集合数据同化方法进行轨迹重构。

    

    提出了一种针对海洋或大气流等高维动力系统的集合估计和模拟的方法框架。为此，将该动力系统嵌入由动力驱动的再生核希尔伯特空间族中。这个家族因其吸引人的特性而被命名为仙境。在仙境中，Koopman和Perron-Frobenius算子是酉的和一致连续的。这个属性保证它们可以用对角化有界无穷小生成器的指数级级数表示。此外，可以直接获得对Lyapunov指数和切线线性动态的精确集合式表达式。仙境使我们能够设计出极其简单的集合数据同化方法，用于以轨迹样本的恒定时间线性组合来进行轨迹重构。这种令人尴尬的简单策略得以实现，是通过Hilbert空间设置中切线线性动力的完全合理的叠加原理实现的。

    A methodological framework for ensemble-based estimation and simulation of high dimensional dynamical systems such as the oceanic or atmospheric flows is proposed. To that end, the dynamical system is embedded in a family of reproducing kernel Hilbert spaces with kernel functions driven by the dynamics. This family is nicknamed Wonderland for its appealing properties. In Wonderland the Koopman and Perron-Frobenius operators are unitary and uniformly continuous. This property warrants they can be expressed in exponential series of diagonalizable bounded infinitesimal generators. Access to Lyapunov exponents and to exact ensemble based expressions of the tangent linear dynamics are directly available as well. Wonderland enables us the devise of strikingly simple ensemble data assimilation methods for trajectory reconstructions in terms of constant-in-time linear combinations of trajectory samples. Such an embarrassingly simple strategy is made possible through a fully justified superposi
    
[^147]: 使用生物仿生的自适应内部关联神经元训练更强的脉冲神经网络

    Training Stronger Spiking Neural Networks with Biomimetic Adaptive Internal Association Neurons. (arXiv:2207.11670v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2207.11670](http://arxiv.org/abs/2207.11670)

    本论文为了探索更深入的神经机制，提出了一种新颖的自适应内部关联（AIA）神经元模型，以建立先前忽视的神经元内部影响。该模型可以促进更强的联想学习，从而提高网络在各种任务上的性能，相较于现有的SNN模型。

    

    作为第三代神经网络，脉冲神经网络（SNN）致力于探索更深入的神经机制，以实现接近生物智能的效果。生物模拟机制对于理解和改进SNN至关重要。例如，关联长时程增强（ALTP）现象表明，除了神经元之间的学习机制外，神经元内部也存在关联效应。然而，大多数现有方法仅关注前者，缺乏对内部关联效应的探索。在本文中，我们提出了一种新颖的自适应内部关联（AIA）神经元模型，以建立先前忽视的神经元内部影响。与ALTP现象一致，AIA神经元模型对输入刺激具有自适应性，只有在同时激发两个树突时才会发生内部联想学习。此外，我们采用加权的权重来测量内部关联，并引入中间缓存来减少突触资源。我们证明，所提出的AIA神经元模型可以促进更强的联想学习，从而提高网络在各种任务上的性能，相较于现有的SNN模型。

    As the third generation of neural networks, spiking neural networks (SNNs) are dedicated to exploring more insightful neural mechanisms to achieve near-biological intelligence. Intuitively, biomimetic mechanisms are crucial to understanding and improving SNNs. For example, the associative long-term potentiation (ALTP) phenomenon suggests that in addition to learning mechanisms between neurons, there are associative effects within neurons. However, most existing methods only focus on the former and lack exploration of the internal association effects. In this paper, we propose a novel Adaptive Internal Association~(AIA) neuron model to establish previously ignored influences within neurons. Consistent with the ALTP phenomenon, the AIA neuron model is adaptive to input stimuli, and internal associative learning occurs only when both dendrites are stimulated at the same time. In addition, we employ weighted weights to measure internal associations and introduce intermediate caches to redu
    
[^148]: 使用时空片段在神经形态数据上训练鲁棒性脉冲神经网络

    Training Robust Spiking Neural Networks on Neuromorphic Data with Spatiotemporal Fragments. (arXiv:2207.11659v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.11659](http://arxiv.org/abs/2207.11659)

    本文介绍了一种新颖的事件时空片段（ESTF）增强方法，可用于处理神经形态视觉数据，并提高使用SNN处理这些数据时的鲁棒性。使用ESTF的SNN在CIFAR10-DVS数据集上实现了最先进的83.9\%准确度。

    

    神经形态视觉传感器（事件相机）天生适合脉冲神经网络（SNN），为这种仿生模型提供了新颖的神经形态视觉数据。由于时空特性，需要使用新颖的数据增强方法处理这些相机的非传统视觉信号。本文提出了一种新颖的事件时空片段（ESTF）增强方法。通过漂移或反转时空事件流的片段来模拟亮度变化的干扰，以此保留神经形态数据的连续性，从而提高脉冲神经网络的鲁棒性。在流行的神经形态数据集上进行了大量实验，结果表明ESTF比纯几何变换的方法提供了实质性改进，并且优于其他事件数据增强方法。值得注意的是，使用ESTF的SNN在CIFAR10-DVS数据集上实现了最先进的83.9\%的准确度。

    Neuromorphic vision sensors (event cameras) are inherently suitable for spiking neural networks (SNNs) and provide novel neuromorphic vision data for this biomimetic model. Due to the spatiotemporal characteristics, novel data augmentations are required to process the unconventional visual signals of these cameras. In this paper, we propose a novel Event SpatioTemporal Fragments (ESTF) augmentation method. It preserves the continuity of neuromorphic data by drifting or inverting fragments of the spatiotemporal event stream to simulate the disturbance of brightness variations, leading to more robust spiking neural networks. Extensive experiments are performed on prevailing neuromorphic datasets. It turns out that ESTF provides substantial improvements over pure geometric transformations and outperforms other event data augmentation methods. It is worth noting that the SNNs with ESTF achieve the state-of-the-art accuracy of 83.9\% on the CIFAR10-DVS dataset.
    
[^149]: 通过有限注释减少算法偏见

    Mitigating Algorithmic Bias with Limited Annotations. (arXiv:2207.10018v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10018](http://arxiv.org/abs/2207.10018)

    本文提出了一种名为APOD的交互式框架，用于在有限的注释预算下减少算法偏见，该框架将歧视惩罚与主动实例选择相结合，能够在公平性和准确度指标上优于传统方法。

    

    现有的公平性建模工作通常假设所有实例的敏感属性都是完全可用的，但由于获取敏感信息的高成本，在许多实际应用中可能不是这样。当敏感属性未公开或无法获得时，需要手动注释一小部分训练数据以减轻偏差。然而，不同敏感组之间的偏斜分布会保留原始数据集中注释子集的偏斜性，这导致非最优的偏差减轻。为了解决这个挑战，我们提出了Active Penalization Of Discrimination (APOD)，这是一个交互式框架，用于指导有限注释最大限度地消除算法偏见。所提出的APOD将歧视惩罚与主动实例选择相结合，以有效利用有限的注释预算，并在理论上证明了其能够限制算法偏见。在基准数据集上评估的结果表明，APOD在公平性和准确度指标上优于几种最先进的方法，同时使用的注释数量明显较少。

    Existing work on fairness modeling commonly assumes that sensitive attributes for all instances are fully available, which may not be true in many real-world applications due to the high cost of acquiring sensitive information. When sensitive attributes are not disclosed or available, it is needed to manually annotate a small part of the training data to mitigate bias. However, the skewed distribution across different sensitive groups preserves the skewness of the original dataset in the annotated subset, which leads to non-optimal bias mitigation. To tackle this challenge, we propose Active Penalization Of Discrimination (APOD), an interactive framework to guide the limited annotations towards maximally eliminating the effect of algorithmic bias. The proposed APOD integrates discrimination penalization with active instance selection to efficiently utilize the limited annotation budget, and it is theoretically proved to be capable of bounding the algorithmic bias. According to the eval
    
[^150]: 基于CNN的欧拉弹性纹理修复算法结合深能量和深图先验

    CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image Prior. (arXiv:2207.07921v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.07921](http://arxiv.org/abs/2207.07921)

    本论文提出了一种基于神经网络的欧拉弹性纹理图像修复算法，它结合使用深能量与深图像先验，能够更好地进行形状恢复任务，得到了比现有算法更好的修复成果。

    

    欧拉弹性纹理是一种吸引人的变分图像修复模型，它不仅涉及总变差，还涉及等高线曲率，这些特点使它在形状恢复任务中表现出色。然而，其梯度流是一种奇异、各向异性和非线性的四阶PDE，这在数值上具有挑战性：很难找到既能提供尖锐边缘又具有良好旋转不变性的高效算法。因此，我们设计了第一个使用欧拉弹性进行修复的神经算法：利用深能量的概念，将变分能量作为神经网络的损失函数。此外，我们结合了深图像先验，其中网络架构本身就充当了先验。这使得我们更接近于所需的解，得到了比现有算法更好的修复成果。在弹性纹理的形状恢复任务上，我们的结果在质量上与现有算法不相上下，能够结合良好的旋转不变性和尖锐的边缘。

    Euler's elastica constitute an appealing variational image inpainting model. It minimises an energy that involves the total variation as well as the level line curvature. These components are transparent and make it attractive for shape completion tasks. However, its gradient flow is a singular, anisotropic, and nonlinear PDE of fourth order, which is numerically challenging: It is difficult to find efficient algorithms that offer sharp edges and good rotation invariance. As a remedy, we design the first neural algorithm that simulates inpainting with Euler's Elastica. We use the deep energy concept which employs the variational energy as neural network loss. Furthermore, we pair it with a deep image prior where the network architecture itself acts as a prior. This yields better inpaintings by steering the optimisation trajectory closer to the desired solution. Our results are qualitatively on par with state-of-the-art algorithms on elastica-based shape completion. They combine good ro
    
[^151]: 支持选择性分类的softmax信息拓展算法

    Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data. (arXiv:2207.07506v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.07506](http://arxiv.org/abs/2207.07506)

    本文研究了存在外部分布数据时的选择性分类问题，提出了一种新的SCOD方法，即保留softmax信息，并发现现有的检测方法评价标准需根据任务规定进行调整。

    

    在深度学习计算机视觉领域中，检测外部分布数据是一项正在受到越来越多研究关注的任务。然而，检测方法的性能通常是在任务中单独评估的，而不是考虑其在联合下游任务时的潜在影响。在本文中，我们研究了存在外部分布数据时的选择性分类(SCOD)问题。也就是说，检测OOD样本的动机在于拒绝它们，从而降低它们对预测质量的影响。我们发现，在这个任务规定下，现有的后处理方法和只在OOD检测时评估时相比，表现出了不同的性能。因为如果ID数据被错误分类，将ID数据与OOD数据混淆就不再是一个问题。然而，在ID数据中正确预测和错误预测之间的混淆是不可取的。我们还提出了一种新的SCOD方法，即保留softmax信息

    Detecting out-of-distribution (OOD) data is a task that is receiving an increasing amount of research attention in the domain of deep learning for computer vision. However, the performance of detection methods is generally evaluated on the task in isolation, rather than also considering potential downstream tasks in tandem. In this work, we examine selective classification in the presence of OOD data (SCOD). That is to say, the motivation for detecting OOD samples is to reject them so their impact on the quality of predictions is reduced. We show under this task specification, that existing post-hoc methods perform quite differently compared to when evaluated only on OOD detection. This is because it is no longer an issue to conflate in-distribution (ID) data with OOD data if the ID data is going to be misclassified. However, the conflation within ID data of correct and incorrect predictions becomes undesirable. We also propose a novel method for SCOD, Softmax Information Retaining Com
    
[^152]: 对抗风险、插值与标签噪声的一项定律

    A law of adversarial risk, interpolation, and label noise. (arXiv:2207.03933v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.03933](http://arxiv.org/abs/2207.03933)

    该研究发现，在监督学习中进行标签噪声插值可以导致对抗风险，在任何数据分布中标签噪声与对抗风险之间都存在一定关系。同时，均匀标签噪声的对抗风险与最糟的污染相差无几，并且比典型现实世界标签噪声更具危害性。进行警惕并深入研究此领域的重要性不容忽视。

    

    在监督学习中，已经证明了数据中的标签噪声可以在不影响测试准确度的情况下进行插值。我们证明了插值标签噪声会导致对抗性漏洞，并且证明了标签噪声与对抗风险之间的关系定理，其适用于任何数据分布。我们的结果在不做任何归纳偏差假设的情况下几乎是最优的。然后，我们研究了该问题的不同组成部分对结果的影响，包括分布的性质。我们还讨论了非均匀标签噪声分布，并证明了一条新的定理，表明具有相同噪声率的均匀标签噪声引起的对抗风险与最糟的污染差不多相同。然后，我们提供了理论和经验证据表明，均匀标签噪声比典型的现实世界标签噪声更具危害性。最后，我们展示了归纳偏差如何放大标签噪声的影响，并论证了未来在这一领域的研究重要性。

    In supervised learning, it has been shown that label noise in the data can be interpolated without penalties on test accuracy. We show that interpolating label noise induces adversarial vulnerability, and prove the first theorem showing the relationship between label noise and adversarial risk for any data distribution. Our results are almost tight if we do not make any assumptions on the inductive bias of the learning algorithm. We then investigate how different components of this problem affect this result, including properties of the distribution. We also discuss non-uniform label noise distributions; and prove a new theorem showing uniform label noise induces nearly as large an adversarial risk as the worst poisoning with the same noise rate. Then, we provide theoretical and empirical evidence that uniform label noise is more harmful than typical real-world label noise. Finally, we show how inductive biases amplify the effect of label noise and argue the need for future work in thi
    
[^153]: Epicasting：一种用于预测疫情的集成小波神经网络（EWNet）

    Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting Epidemics. (arXiv:2206.10696v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10696](http://arxiv.org/abs/2206.10696)

    疫情预测至关重要，但由于传染病的非线性和非平稳特征，预测变得困难。本文介绍了一种称为Epicasting的新算法，使用集成小波神经网络（EWNet）来更准确地预测疫情。

    

    传染病仍然是全球人类疾病和死亡的主要因素，其中许多疾病会产生传染性感染的流行波。大多数这些流行病没有特定的药物和预防疫苗，使情况更加恶化。这迫使公共卫生官员和政策制定者依靠可靠和准确的流行病预测所生成的预警系统。然而，由于这些流行病的传播波动基于季节依赖性和它们的本质，大多数过去的这些流行病表现出非线性和非平稳特征。我们利用最大重叠离散小波变换（MODWT）分析了各种流行病时间序列数据集。

    Infectious diseases remain among the top contributors to human illness and death worldwide, among which many diseases produce epidemic waves of infection. The unavailability of specific drugs and ready-to-use vaccines to prevent most of these epidemics makes the situation worse. These force public health officials and policymakers to rely on early warning systems generated by reliable and accurate forecasts of epidemics. Accurate forecasts of epidemics can assist stakeholders in tailoring countermeasures, such as vaccination campaigns, staff scheduling, and resource allocation, to the situation at hand, which could translate to reductions in the impact of a disease. Unfortunately, most of these past epidemics exhibit nonlinear and non-stationary characteristics due to their spreading fluctuations based on seasonal-dependent variability and the nature of these epidemics. We analyse a wide variety of epidemic time series datasets using a maximal overlap discrete wavelet transform (MODWT)
    
[^154]: 非参数多形状建模和不确定性量化

    Nonparametric Multi-shape Modeling with Uncertainty Quantification. (arXiv:2206.09127v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.09127](http://arxiv.org/abs/2206.09127)

    该论文提出了一个多输出、多维高斯过程建模框架，解决了在多个曲线和形状相关任务上的非参数建模和不确定性量化问题，为功能对象的多级依赖关系提供了一种新的建模方式。

    

    闭合曲线的建模和不确定性量化是形状分析领域中的一个重要问题，并且可能对随后的统计任务产生重大影响。许多这些任务涉及到多个闭合曲线的集合，这些集合通常在多个层面上展现出结构上的相似性。以一种能够高效地纳入这种曲线之间依赖关系的方式对多条闭曲线进行建模仍然是一个具有挑战性的问题。在这项工作中，我们提出并研究了一种多输出(也称为多输出)、多维高斯过程建模框架。我们展示了所提出的方法论进展，并证明了在几个曲线和形状相关任务上有着有意义的不确定性量化的效用。这种基于模型的方法不仅通过内核构造解决了对闭合曲线(以及它们的形状)的推理问题，而且还为一般的功能对象的多级依赖关系的非参数建模打开了大门。

    The modeling and uncertainty quantification of closed curves is an important problem in the field of shape analysis, and can have significant ramifications for subsequent statistical tasks. Many of these tasks involve collections of closed curves, which often exhibit structural similarities at multiple levels. Modeling multiple closed curves in a way that efficiently incorporates such between-curve dependence remains a challenging problem. In this work, we propose and investigate a multiple-output (a.k.a. multi-output), multi-dimensional Gaussian process modeling framework. We illustrate the proposed methodological advances, and demonstrate the utility of meaningful uncertainty quantification, on several curve and shape-related tasks. This model-based approach not only addresses the problem of inference on closed curves (and their shapes) with kernel constructions, but also opens doors to nonparametric modeling of multi-level dependence for functional objects in general.
    
[^155]: 因果多臂老虎机的组合纯探索

    Combinatorial Pure Exploration of Causal Bandits. (arXiv:2206.07883v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07883](http://arxiv.org/abs/2206.07883)

    本文提出了用于因果模型的组合纯探索算法，其中对于二元广义线性模型，我们的算法实现了多项式样本复杂度，对于一般的图，我们的样本复杂度几乎是最优的。

    

    因果多臂老虎机的组合纯探索是一种在线学习任务：在给定一个具有未知因果推断分布的因果图的情况下，在每一轮中，我们选择一个子集变量来干预或不干预，并观察所有随机变量的随机结果，目标是尽可能少的使用轮数，以概率至少为$1-\delta$，输出能给予奖励变量$Y$最佳（或近乎最佳）期望结果的干预方案，其中$\delta$是给定的置信水平。我们提供了两种类型因果模型——二元广义线性模型（BGLM） 和一般图的第一种间隔依赖性和完全自适应的纯探索算法。对于BGLM，我们的算法是首次专门针对这种情况设计的，并实现了多项式样本复杂度，而所有现有的一般图算法都具有指数复杂度到图大小或一些不合理的假设。对于一般图，我们的算法具有近乎最优的样本复杂度，仅对数于图大小和未知因果效应的数量，而且还能够抵御隐藏性混淆因素和任意分布偏移的情况。此外，我们的分析提供了一个紧密的下界，与上界相匹配，表明了算法的最优性。

    The combinatorial pure exploration of causal bandits is the following online learning task: given a causal graph with unknown causal inference distributions, in each round we choose a subset of variables to intervene or do no intervention, and observe the random outcomes of all random variables, with the goal that using as few rounds as possible, we can output an intervention that gives the best (or almost best) expected outcome on the reward variable $Y$ with probability at least $1-\delta$, where $\delta$ is a given confidence level. We provide the first gap-dependent and fully adaptive pure exploration algorithms on two types of causal models -- the binary generalized linear model (BGLM) and general graphs. For BGLM, our algorithm is the first to be designed specifically for this setting and achieves polynomial sample complexity, while all existing algorithms for general graphs have either sample complexity exponential to the graph size or some unreasonable assumptions. For general 
    
[^156]: 空间熵作为视觉Transformers的归纳偏置

    Spatial Entropy as an Inductive Bias for Vision Transformers. (arXiv:2206.04636v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.04636](http://arxiv.org/abs/2206.04636)

    本文提出在视觉Transformers中引入局部偏置的不同方法，使用一个自监督任务来鼓励空间聚类作为训练正则化。通过利用注意力映射的语义分割结构，可以有效地减少样本数量，提高性能。

    

    近期关于视觉Transformers（VT）的研究表明，引入VT架构中的局部归纳偏置有助于减少训练所需的样本数量。然而，架构修改会导致Transformer骨干的通用性的损失，这部分违背了推动开发像计算机视觉和自然语言处理领域共享的统一架构的趋势。本文提出了一个不同的补充方向，即使用辅助自监督任务来引入局部偏置，同时与标准的有监督训练一起进行。具体来说，我们利用了VT的注意力映射在进行自监督训练时可能包含的语义分割结构，在有监督训练时不会自动出现。因此，我们明确地鼓励空间聚类作为训练正则化的一种形式。更详细地说，我们利用了假设

    Recent work on Vision Transformers (VTs) showed that introducing a local inductive bias in the VT architecture helps reducing the number of samples necessary for training. However, the architecture modifications lead to a loss of generality of the Transformer backbone, partially contradicting the push towards the development of uniform architectures, shared, e.g., by both the Computer Vision and the Natural Language Processing areas. In this work, we propose a different and complementary direction, in which a local bias is introduced using an auxiliary self-supervised task, performed jointly with standard supervised training. Specifically, we exploit the observation that the attention maps of VTs, when trained with self-supervision, can contain a semantic segmentation structure which does not spontaneously emerge when training is supervised. Thus, we explicitly encourage the emergence of this spatial clustering as a form of training regularization. In more detail, we exploit the assump
    
[^157]: 模型反演实现少样本去除学习

    Few-Shot Unlearning by Model Inversion. (arXiv:2205.15567v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15567](http://arxiv.org/abs/2205.15567)

    该论文提出了一个少样本去除学习的框架，通过模型反演获取训练数据代理，并根据去除学习意图进行调整。

    

    论文讨论了一个实际的机器去除学习情境，即去除一个目标数据集，以消除已训练模型的意外行为。然而，在标准去除学习情境下，目标数据集通常被认为是可以完全确定的。但是如果训练数据集在去除学习时不可访问，则几乎不可能实现完美识别。与之前的方法不同，该论文考虑少样本去除学习情境，即仅有少量目标数据样本可用。为此，作者提出了一个直接的框架来解决问题：（i）通过模型反演获取训练数据的代理；（ii）根据去除学习的意图调整代理；（iii）用调整后的代理更新模型。

    We consider a practical scenario of machine unlearning to erase a target dataset, which causes unexpected behavior from the trained model. The target dataset is often assumed to be fully identifiable in a standard unlearning scenario. Such a flawless identification, however, is almost impossible if the training dataset is inaccessible at the time of unlearning. Unlike previous approaches requiring a complete set of targets, we consider few-shot unlearning scenario when only a few samples of target data are available. To this end, we formulate the few-shot unlearning problem specifying intentions behind the unlearning request (e.g., purely unlearning, mislabel correction, privacy protection), and we devise a straightforward framework that (i) retrieves a proxy of the training data via model inversion fully exploiting information available in the context of unlearning; (ii) adjusts the proxy according to the unlearning intention; and (iii) updates the model with the adjusted proxy. We de
    
[^158]: Relphormer：关系图转换器用于知识图谱表示

    Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.10852](http://arxiv.org/abs/2205.10852)

    Relphormer是一种新的Transformer变体，用于知识图谱表示。它引入了Triple2Seq和增强式自我注意机制，以解决基本Transformer架构在捕捉知识图谱结构和语义信息方面的不足。

    

    Transformer已经在自然语言处理、计算机视觉和图形挖掘等广泛领域中取得了remarkable的性能。然而，基本的Transformer架构在知识图谱（KG）表示中并没有取得很好的改进，其中平移距离模型支配了这个领域。需注意的是，基本的Transformer架构难以捕捉到知识图谱的内在异构结构和语义信息。为此，我们提出了一种新的用于知识图谱表示的Transformer变体，名为Relphormer。具体来说，我们引入了Triple2Seq，可以动态地采样上下文化的子图序列作为输入，以缓解异构性问题。我们提出了一种新的增强式自我注意机制，用于对关系信息进行编码，并保持实体和关系内的语义信息。此外，我们利用掩蔽式知识建模来实现通用的知识图形表示。

    Transformers have achieved remarkable performance in widespread fields, including natural language processing, computer vision and graph mining. However, vanilla Transformer architectures have not yielded promising improvements in the Knowledge Graph (KG) representations, where the translational distance paradigm dominates this area. Note that vanilla Transformer architectures struggle to capture the intrinsically heterogeneous structural and semantic information of knowledge graphs. To this end, we propose a new variant of Transformer for knowledge graph representations dubbed Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample contextualized sub-graph sequences as the input to alleviate the heterogeneity issue. We propose a novel structure-enhanced self-attention mechanism to encode the relational information and keep the semantic information within entities and relations. Moreover, we utilize masked knowledge modeling for general knowledge graph representa
    
[^159]: CycleSense：利用移动动作传感器检测自行车交通中的擦肩而过事件

    CycleSense: Detecting Near Miss Incidents in Bicycle Traffic from Mobile Motion Sensors. (arXiv:2204.10416v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.10416](http://arxiv.org/abs/2204.10416)

    CycleSense是一种利用移动动作传感器检测自行车交通中的擦肩而过事件的技术，可以帮助自行车手更轻松地报告险情。这种技术的使用可以帮助城市规划者更好地了解自行车安全感问题，有望缓解城市交通问题。

    

    在全球的城市中，汽车引起了健康和交通问题，增加自行车的使用率可能在一定程度上缓解这些问题。然而，许多人由于缺乏感知安全来避免骑自行车。对于城市规划者来说，解决这个问题很难，因为他们缺乏关于自行车安全感的洞察力。为了获得这样的洞见，我们之前提出了众包平台SimRa，它允许自行车手通过智能手机应用程序记录其骑行并报告擦肩而过事件。在本文中，我们提出了CycleSense，这是一种信号处理和机器学习技术的组合，部分自动化检测擦肩而过事件，从而使擦肩而过事件报告更加容易。使用SimRa数据集，我们通过将其与SimRa使用的基线方法进行比较来评估CycleSense，并表明它显着改善了事件检测。

    In cities worldwide, cars cause health and traffic problems whichcould be partly mitigated through an increased modal share of bicycles. Many people, however, avoid cycling due to a lack of perceived safety. For city planners, addressing this is hard as they lack insights intowhere cyclists feel safe and where they do not. To gain such insights,we have in previous work proposed the crowdsourcing platform SimRa,which allows cyclists to record their rides and report near miss incidentsvia a smartphone app. In this paper, we present CycleSense, a combination of signal pro-cessing and Machine Learning techniques, which partially automatesthe detection of near miss incidents, thus making the reporting of nearmiss incidents easier. Using the SimRa data set, we evaluate CycleSenseby comparing it to a baseline method used by SimRa and show that itsignificantly improves incident detection.
    
[^160]: 用熵近似的高斯混合变分推断

    Variational Inference with Gaussian Mixture by Entropy Approximation. (arXiv:2202.13059v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.13059](http://arxiv.org/abs/2202.13059)

    论文提出了一种用高斯混合分布作为参数分布的变分推断方法，通过将高斯混合的熵近似为单峰高斯的熵之和来解决多峰性的问题，并从理论上分析近似误差。

    

    变分推断是一种用于近似无法处理的后验分布以量化机器学习不确定性的技术。然而，单峰的高斯分布通常被选择作为参数分布，很难逼近多峰性。在本文中，我们采用高斯混合分布作为参数分布。使用高斯混合进行变分推断的一个主要难点是如何近似高斯混合的熵。我们将高斯混合的熵近似为单峰高斯的熵之和，可以通过解析计算得到。此外，我们从理论上分析了真实熵与近似熵之间的近似误差，以便揭示我们的近似何时起作用。具体而言，近似误差由高斯混合均值之间距离与方差之和的比率控制。此外，当高斯混合组件的数量趋近于无穷大时，近似误差趋近于零。

    Variational inference is a technique for approximating intractable posterior distributions in order to quantify the uncertainty of machine learning. Although the unimodal Gaussian distribution is usually chosen as a parametric distribution, it hardly approximates the multimodality. In this paper, we employ the Gaussian mixture distribution as a parametric distribution. A main difficulty of variational inference with the Gaussian mixture is how to approximate the entropy of the Gaussian mixture. We approximate the entropy of the Gaussian mixture as the sum of the entropy of the unimodal Gaussian, which can be analytically calculated. In addition, we theoretically analyze the approximation error between the true entropy and approximated one in order to reveal when our approximation works well. Specifically, the approximation error is controlled by the ratios of the distances between the means to the sum of the variances of the Gaussian mixture. Furthermore, it converges to zero when the 
    
[^161]: 从区分到生成：基于生成变换器的知识图谱补全

    From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v7 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.02113](http://arxiv.org/abs/2202.02113)

    本文介绍了一种将知识图谱补全转化为生成任务的方法，同时引入了关系引导演示和实体感知分层解码来实现更好的表示学习和快速推断。实验结果表明，这种方法具有比基线更好或相当的性能，并且比以往的方法更快。同时，作者还发布了一个新的大规模中文知识图谱数据集AliopenKG500。

    

    知识图谱补全解决了扩展缺失三元组的问题。本文提出了一种称作GenKGC的方法，将知识图谱补全转化为预训练语言模型的序列到序列生成任务。我们进一步引入了关系引导演示和实体感知分层解码，以实现更好的表示学习和快速推断。在三个数据集上的实验结果显示，我们的方法可以获得比基线更好或相当的性能，并与以前具有预训练语言模型的方法相比，实现更快的推断速度。我们还发布了一个新的大规模中文知识图谱数据集AliopenKG500，供研究目的使用。代码和数据集可在https://github.com/zjunlp/PromptKG/tree/main/GenKGC中获得。

    Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset AliopenKG500 for research purpose. Code and datasets are available in https://github.com/zjunlp/PromptKG/tree/main/GenKGC.
    
[^162]: 学习线性二次高斯系统的遗憾下限

    Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems. (arXiv:2201.01680v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.01680](http://arxiv.org/abs/2201.01680)

    本论文证明了在学习未知线性高斯系统与二次代价时，存在遗憾下限，并且这个下限的比例尺度级别为 $\sqrt{T}$。通过对控制理论参数的准确捕捉，我们证明难以控制的系统也难以学习控制。同样地，对于一类部分观察到的系统，我们的结果表明了具有较差可观测结构的系统也难以学习控制。

    

    我们为自适应控制未知的线性高斯系统与二次代价建立遗憾下限。我们结合了实验设计、估计理论和某些信息矩阵的扰动界限的思想，得到了关于时间跨度$T$的遗憾下限，其比例尺度级别为 $\sqrt{T}$。我们的下限准确地捕捉了控制理论参数的作用，并且我们能够表明难以控制的系统也难以学习控制；当具体化为状态反馈系统时，我们恢复了早期工作的维度依赖关系，但改善了随系统理论常数（如系统成本和格拉米恩矩阵）的比例尺度。此外，我们将结果扩展到一类部分观察到的系统，并证明具有较差可观测结构的系统也难以学习控制。

    TWe establish regret lower bounds for adaptively controlling an unknown linear Gaussian system with quadratic costs. We combine ideas from experiment design, estimation theory and a perturbation bound of certain information matrices to derive regret lower bounds exhibiting scaling on the order of magnitude $\sqrt{T}$ in the time horizon $T$. Our bounds accurately capture the role of control-theoretic parameters and we are able to show that systems that are hard to control are also hard to learn to control; when instantiated to state feedback systems we recover the dimensional dependency of earlier work but with improved scaling with system-theoretic constants such as system costs and Gramians. Furthermore, we extend our results to a class of partially observed systems and demonstrate that systems with poor observability structure also are hard to learn to control.
    
[^163]: 一步式诱导式多目标学习及其在乳腺癌肿瘤分割中的应用

    One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer. (arXiv:2110.10325v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.10325](http://arxiv.org/abs/2110.10325)

    本论文提出了一种新的机器学习方法——一步式诱导式多目标学习与DiNS（OSAMTL-DiNS），以处理医学组织病理学全幻灯片图像分析中的复杂噪声标签。在乳腺癌肿瘤分割中得到了成功应用。

    

    近年来的研究表明，机器学习和逻辑推理的结合，包括数据驱动的逻辑推理、知识驱动的机器学习和诱导学习，在发明先进的人工智能技术方面具有很高的有效性。在医学组织病理学全幻灯片图像分析中，一步式诱导式多目标学习（OSAMTL），作为一种受诱导学习启发的方法，通过以一种平衡的方式简单地结合机器学习和逻辑推理，已经证明了其处理单个嘈杂标签的复杂噪声标签的有效性。但是，OSAMTL不适用于提供多种嘈杂样本（DiNS）的学习任务情况。在本文中，我们给出了DiNS的定义，并提出了一步式诱导式多目标学习与DiNS（OSAMTL-DiNS），以扩展原始的OSAMTL以处理DiNS的复杂噪声标签。将OSAMTL-DiNS应用于MHWSIA中的乳腺癌肿瘤分割中，我们展示了其有效性。

    Recent studies have demonstrated the effectiveness of the combination of machine learning and logical reasoning, including data-driven logical reasoning, knowledge driven machine learning and abductive learning, in inventing advanced artificial intelligence technologies. One-step abductive multi-target learning (OSAMTL), an approach inspired by abductive learning, via simply combining machine learning and logical reasoning in a one-step balanced way, has as well shown its effectiveness in handling complex noisy labels of a single noisy sample in medical histopathology whole slide image analysis (MHWSIA). However, OSAMTL is not suitable for the situation where diverse noisy samples (DiNS) are provided for a learning task. In this paper, giving definition of DiNS, we propose one-step abductive multi-target learning with DiNS (OSAMTL-DiNS) to expand the original OSAMTL to handle complex noisy labels of DiNS. Applying OSAMTL-DiNS to tumour segmentation for breast cancer in MHWSIA, we show 
    
[^164]: 基于期望距离的分布聚类方法用于噪声鲁棒性

    Expectation Distance-based Distributional Clustering for Noise-Robustness. (arXiv:2110.08871v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.08871](http://arxiv.org/abs/2110.08871)

    本文提出了一种基于期望距离的分布聚类技术，可以降低噪声对聚类结果的影响。

    

    本文提出了一种聚类技术，通过学习和聚类数据分布，并将数据分配到其分布的簇中，从而减少数据噪声的影响。该方法引入了一种新的距离度量，即期望距离（ED），它超越了最优质量传输（$W_2$）的分布距离的现有技术：后者仅依赖于边缘分布，而前者还使用联合分布的信息。利用ED，本文将传统的$K$-means和$K$-medoids聚类扩展到数据分布上，并介绍使用$W_2$的$K$-medoids。本文还介绍了$W_2$和ED距离度量的闭合形式表达式。

    This paper presents a clustering technique that reduces the susceptibility to data noise by learning and clustering the data-distribution and then assigning the data to the cluster of its distribution. In the process, it reduces the impact of noise on clustering results. This method involves introducing a new distance among distributions, namely the expectation distance (denoted, ED), that goes beyond the state-of-art distribution distance of optimal mass transport (denoted, $W_2$ for $2$-Wasserstein): The latter essentially depends only on the marginal distributions while the former also employs the information about the joint distributions. Using the ED, the paper extends the classical $K$-means and $K$-medoids clustering to those over data-distributions (rather than raw-data) and introduces $K$-medoids using $W_2$. The paper also presents the closed-form expressions of the $W_2$ and ED distance measures. The implementation results of the proposed ED and the $W_2$ distance measures t
    
[^165]: 一种应对数据流分类概念漂移的广义集成学习系统

    A Broad Ensemble Learning System for Drifting Stream Classification. (arXiv:2110.03540v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03540](http://arxiv.org/abs/2110.03540)

    一种名为Broad Ensemble Learning System (BELS)的新型集成方法用于数据流分类中的概念漂移问题。相对于文献中现有的方法，它能够更加高效、稳定地更新模型，提高最佳性能模型的准确性。

    

    在数据流环境中，分类模型必须有效地处理概念漂移。集成方法是为此目的广泛使用的方法，但是文献中存在的方法要么使用大块数据更新模型，要么逐个学习数据。前者可能会错过数据分布中的变化，后者则可能受到效率和不稳定性的影响。为了解决这些问题，我们引入一种基于Broad Learning System (BLS)的新型集成方法，在每次更新时使用小块数据。BLS是一种有效的轻量级神经结构，最近被开发用于增量学习。尽管它速度很快，但它需要大量数据块来进行有效更新，而且不能处理数据流中观察到的动态变化。我们提出的Broad Ensemble Learning System (BELS)使用一种新的更新方式，显著提高了最佳性能模型的准确性。它使用输出集成

    In a data stream environment, classification models must handle concept drift efficiently and effectively. Ensemble methods are widely used for this purpose; however, the ones available in the literature either use a large data chunk to update the model or learn the data one by one. In the former, the model may miss the changes in the data distribution, and in the latter, the model may suffer from inefficiency and instability. To address these issues, we introduce a novel ensemble approach based on the Broad Learning System (BLS), where mini chunks are used at each update. BLS is an effective lightweight neural architecture recently developed for incremental learning. Although it is fast, it requires huge data chunks for effective updates, and is unable to handle dynamic changes observed in data streams. Our proposed approach named Broad Ensemble Learning System (BELS) uses a novel updating method that significantly improves best-in-class model accuracy. It employs an ensemble of outpu
    
[^166]: 预训练语言模型也是符号数学求解器！

    Pretrained Language Models are Symbolic Mathematics Solvers too!. (arXiv:2110.03501v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.03501](http://arxiv.org/abs/2110.03501)

    本文研究表明，大规模语言模型可以训练为序列到序列任务，解决复杂的数学方程。文章提出了一种预训练并微调Transformer模型解决符号数学任务的方法，使用的训练样本比当前深度学习技术少1.5个数量级，且在积分任务上达到了可比较的准确性。

    

    解决符号数学问题一直是需要组合推理和重复的人类创造力的领域。但是，最近的研究表明，诸如transformer之类的大规模语言模型是通用的，并且令人惊讶的是，它们可以被训练为用于解决复杂的数学方程的序列到序列任务。这些大型Transformer模型需要极其庞大的训练数据才能泛化到未见过的符号数学问题。在本文中，我们提出一种样本有效的方式来解决符号任务，首先通过语言翻译对Transformer模型进行预训练，然后微调预训练的Transformer模型以解决符号数学的下游任务。我们在积分任务上使用了大约1.5个数量级的训练样本，达到了与预先训练模型相当的准确性，而与针对符号数学的深度学习的最新技术相比使用了较少的训练样本。微分方程的测试准确性为...

    Solving symbolic mathematics has always been of in the arena of human ingenuity that needs compositional reasoning and recurrence. However, recent studies have shown that large-scale language models such as transformers are universal and surprisingly can be trained as a sequence-to-sequence task to solve complex mathematical equations. These large transformer models need humongous amounts of training data to generalize to unseen symbolic mathematics problems. In this paper, we present a sample efficient way of solving the symbolic tasks by first pretraining the transformer model with language translation and then fine-tuning the pretrained transformer model to solve the downstream task of symbolic mathematics. We achieve comparable accuracy on the integration task with our pretrained model while using around $1.5$ orders of magnitude less number of training samples with respect to the state-of-the-art deep learning for symbolic mathematics. The test accuracy on differential equation ta
    
[^167]: 使用相似度学习的变分自动编码器进行增量式分类学习

    Incremental Class Learning using Variational Autoencoders with Similarity Learning. (arXiv:2110.01303v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.01303](http://arxiv.org/abs/2110.01303)

    本文研究了增量式学习中四种基于相似性的损失函数在灾难性遗忘方面的表现，实验结果显示不同的损失函数对灾难性遗忘率有不同的影响。

    

    神经网络在进行增量式学习时容易出现“灾难性遗忘”，这是一个具有挑战性的问题。之前的研究主要关注全连接网络中的灾难性遗忘，并探索了激活函数和学习算法等因素对灾难性遗忘的影响。近年来，相似性学习逐渐被应用到神经网络中，因此了解相似性学习损失函数在灾难性遗忘中的表现具有重要意义。本文在四种著名的基于相似性的损失函数（角度损失、对比损失、中心损失和三元损失）中进行了灾难性遗忘率的研究。研究结果显示不同的损失函数以及数据集对灾难性遗忘率有显著影响。其中，角度损失受影响最小，其次是对比损失、三元损失和中心损失（利用挖掘技术性能较好）。本文实现了三种现有的增量学习技术：iCaRL、EWC和EBLL。

    Catastrophic forgetting in neural networks during incremental learning remains a challenging problem. Previous research investigated catastrophic forgetting in fully connected networks, with some earlier work exploring activation functions and learning algorithms. Applications of neural networks have been extended to include similarity learning. Understanding how similarity learning loss functions would be affected by catastrophic forgetting is of significant interest. Our research investigates catastrophic forgetting for four well-known similarity-based loss functions during incremental class learning. The loss functions are Angular, Contrastive, Center, and Triplet loss. Our results show that the catastrophic forgetting rate differs across loss functions on multiple datasets. The Angular loss was least affected, followed by Contrastive, Triplet loss, and Center loss with good mining techniques. We implemented three existing incremental learning techniques, iCaRL, EWC, and EBLL. We fu
    
[^168]: 基于深度学习的大规模验证项目因素分析的参数估计和拟合度检验方法研究

    Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale Confirmatory Item Factor Analysis. (arXiv:2109.09500v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.09500](http://arxiv.org/abs/2109.09500)

    本文研究了针对大规模验证项目因素分析的参数估计与拟合度检验方法，提出了基于深度学习的算法和扩展测试与指标，具有高效准确和有效性的特点。

    

    本论文研究了针对大规模验证项目因素分析中的参数估计和拟合度检验方法。对于参数估计，我们将Urban和Bauer（2021）的深度学习算法扩展到验证性因素分析领域，并展示了如何处理因子载荷和因子相关性的限制。对于拟合度检验，我们探索基于模拟的测试和指标，扩展了分类器两个样本测试（C2ST），该方法测试深度神经网络能否区分来自拟合的IFA模型的观测数据和合成数据。所提出的扩展包括近似拟合检验，其中用户指定观测数据和合成数据中应有多少占可区分部分的百分比，以及相对拟合指数（RFI），该指数类似于结构方程建模中使用的RFI。通过模拟研究，我们展示了：（1）Urban和Bauer的深度学习算法的验证性扩展即使存在高相关因子也可以准确地估计模型参数；（2）所提出的拟合度指标可以有效地检测模型不良拟合的重要信息，对于大规模验证IFA提供了有效的解决方案。

    We investigate novel parameter estimation and goodness-of-fit (GOF) assessment methods for large-scale confirmatory item factor analysis (IFA) with many respondents, items, and latent factors. For parameter estimation, we extend Urban and Bauer's (2021) deep learning algorithm for exploratory IFA to the confirmatory setting by showing how to handle constraints on loadings and factor correlations. For GOF assessment, we explore simulation-based tests and indices that extend the classifier two-sample test (C2ST), a method that tests whether a deep neural network can distinguish between observed data and synthetic data sampled from a fitted IFA model. Proposed extensions include a test of approximate fit wherein the user specifies what percentage of observed and synthetic data should be distinguishable as well as a relative fit index (RFI) that is similar in spirit to the RFIs used in structural equation modeling. Via simulation studies, we show that: (1) the confirmatory extension of Urb
    
[^169]: Nash平衡逼近器可学习吗？

    Is Nash Equilibrium Approximator Learnable?. (arXiv:2108.07472v6 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2108.07472](http://arxiv.org/abs/2108.07472)

    本文研究了生成游戏中Nash平衡的学习方法，证明了可学习性，并展示了该方法在经典求解器加速方面的应用。

    

    本文研究了逼近生成游戏的分布中的Nash均衡（NE）的函数逼近器的可学习性。首先，我们使用Probably Approximately Correct（PAC）学习模型提供了一个泛化边界。该边界描述了NE逼近器的期望损失和经验损失之间的差距。之后，我们证明了Nash逼近器的agnostic PAC可学习性。除了理论分析，我们还在实验中展示了NE逼近器的一种应用。训练的NE逼近器可以用于启动和加速经典NE求解器。综上所述，我们的结果显示函数逼近可以实现逼近NE的实用性。

    In this paper, we investigate the learnability of the function approximator that approximates Nash equilibrium (NE) for games generated from a distribution. First, we offer a generalization bound using the Probably Approximately Correct (PAC) learning model. The bound describes the gap between the expected loss and empirical loss of the NE approximator. Afterward, we prove the agnostic PAC learnability of the Nash approximator. In addition to theoretical analysis, we demonstrate an application of NE approximator in experiments. The trained NE approximator can be used to warm-start and accelerate classical NE solvers. Together, our results show the practicability of approximating NE through function approximation.
    
[^170]: 思路流网络：从单一预测到模型思路的串联

    Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.12220](http://arxiv.org/abs/2107.12220)

    本文探讨了给模型第二次、第三次甚至第k次思考机会的思路流网络，其利用自我校正机制和梯度更新能够纠正自身预测，该方法可显著提高模型性能。

    

    当人类解决复杂问题时，通常会创建一系列思路（涉及直觉决策、反思、错误更正等）以达成决定。但是，如今的模型大多被训练为将输入映射到单一且固定的输出。本文研究了如何让模型有第二、第三和第 k 次思考的机会。我们从黑格尔的辩证法中获得灵感，提出了思路流的概念，创建了一系列预测。我们提出了一个自我校正机制，它被训练用于估计模型的正确性，并基于正确性预测的梯度执行迭代预测更新。我们以问答为例介绍了我们的方法，并进行了广泛的实验，证明了（i）我们的方法能够纠正自己的预测，（ii）它能够显著提高模型的性能。此外，我们对思路流的语义校验进行了定性分析。

    When humans solve complex problems, they typically create a sequence of ideas (involving an intuitive decision, reflection, error correction, etc.) in order to reach a conclusive decision. Contrary to this, today's models are mostly trained to map an input to one single and fixed output. In this paper, we investigate how we can give models the opportunity of a second, third and $k$-th thought. Taking inspiration from Hegel's dialectics, we propose the concept of a thought flow which creates a sequence of predictions. We present a self-correction mechanism that is trained to estimate the model's correctness and performs iterative prediction updates based on the correctness prediction's gradient. We introduce our method at the example of question answering and conduct extensive experiments that demonstrate (i) our method's ability to correct its own predictions and (ii) its potential to notably improve model performances. In addition, we conduct a qualitative analysis of thought flow cor
    
[^171]: 音频-视频去混响学习

    Learning Audio-Visual Dereverberation. (arXiv:2106.07732v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2106.07732](http://arxiv.org/abs/2106.07732)

    VIDA是一种音视频结合的去混响方法，能够更有效地去除混响，提高语音增强、语音识别和说话者识别的性能。 SoundSpaces-Speech是一个新的大规模数据集，提供了真实世界中各种房间声学的逼真语音声学渲染。

    

    混响不仅会降低人类感知语音的质量，还会严重影响自动语音识别的准确性。我们提出了利用音视频观测学习去混响说话的想法。人类说话者周围的视觉环境揭示了关于房间几何形状、材料和说话者位置的重要线索，它们都会影响混响的精确效果。我们介绍了一种名为“基于视觉信息的音频去混响”（VIDA）的端到端方法，它可以学习基于观察到的单声道声音和视觉场景去除混响。为了支持这项新任务，我们开发了一个大规模的数据集SoundSpaces-Speech，在真实世界的3D房屋扫描中使用逼真的语音声学渲染，提供了各种房间声学。我们在模拟和实际影像上展示了VIDA在语音增强、语音识别和说话者识别任务上的效果，表明VIDA优于基于音频的先前方法，融合视觉信息有助于捕捉重要线索，从而使去混响更有效。

    Reverberation not only degrades the quality of speech for human perception, but also severely impacts the accuracy of automatic speech recognition. Prior work attempts to remove reverberation based on the audio modality only. Our idea is to learn to dereverberate speech from audio-visual observations. The visual environment surrounding a human speaker reveals important cues about the room geometry, materials, and speaker location, all of which influence the precise reverberation effects. We introduce Visually-Informed Dereverberation of Audio (VIDA), an end-to-end approach that learns to remove reverberation based on both the observed monaural sound and visual scene. In support of this new task, we develop a large-scale dataset SoundSpaces-Speech that uses realistic acoustic renderings of speech in real-world 3D scans of homes offering a variety of room acoustics. Demonstrating our approach on both simulated and real imagery for speech enhancement, speech recognition, and speaker ident
    
[^172]: 基于梯度幅度引导的动态高效对抗训练

    Dynamic Efficient Adversarial Training Guided by Gradient Magnitude. (arXiv:2103.03076v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.03076](http://arxiv.org/abs/2103.03076)

    本文提出了一种基于梯度幅度引导的动态高效对抗训练方法（DEAT），逐渐增加了对抗迭代次数，并提出了一种通用加速策略M+加速，该方法易于实现且符合大部分现有的对抗训练技术，可以显著提高深度神经网络的鲁棒性。

    

    对抗训练是训练鲁棒深度神经网络以抵御强对抗攻击的有效方法，但时间成本很高。为了解决其效率问题，我们提出了一种名为动态高效对抗训练的方法（DEAT），其中逐渐增加了对抗迭代次数。我们证明梯度幅度与训练模型损失函数曲率相关，从而反映了对抗训练的效果。因此，基于梯度幅度，我们提出了一种通用加速策略，M+加速，它可以自动并高效地调整训练过程，且易于实现。它适用于DEAT并兼容大部分现有的对抗训练技术。我们在CIFAR-10和ImageNet数据集上进行了大量实验，并展示了我们的M+加速策略显著加快了对抗训练，并提高了深度神经网络对各种攻击的鲁棒性。

    Adversarial training is an effective but time-consuming way to train robust deep neural networks that can withstand strong adversarial attacks. As a response to its inefficiency, we propose Dynamic Efficient Adversarial Training (DEAT), which gradually increases the adversarial iteration during training. We demonstrate that the gradient's magnitude correlates with the curvature of the trained model's loss landscape, allowing it to reflect the effect of adversarial training. Therefore, based on the magnitude of the gradient, we propose a general acceleration strategy, M+ acceleration, which enables an automatic and highly effective method of adjusting the training procedure. M+ acceleration is computationally efficient and easy to implement. It is suited for DEAT and compatible with the majority of existing adversarial training techniques. Extensive experiments have been done on CIFAR-10 and ImageNet datasets with various training environments. The results show that the proposed M+ acce
    
[^173]: 噪声存储设备的神经网络压缩

    Neural Network Compression for Noisy Storage Devices. (arXiv:2102.07725v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.07725](http://arxiv.org/abs/2102.07725)

    本文研究了神经网络在使用噪声存储设备时的压缩与存储问题，提出了适用于模拟存储设备的鲁棒压缩技术，相对于数字压缩方法，有效提高了NN的存储效率。

    

    对神经网络（NN）参数的压缩和高效存储对于在资源受限设备上运行的应用程序至关重要。尽管在NN模型压缩方面取得了重大进展，但对NN参数的实际物理存储进行了相对较少的研究。我们研究了使用类比存储设备作为数字介质的替代方案，这种方法自然地提供了一种添加更多重要位保护的方法，但存在噪声并可能损害存储模型的性能。我们开发了多种针对模拟存储设备属性的鲁棒NN压缩技术，并证明了它们在注入噪声的几个基准数据集上的有效性。与最先进的数字压缩方法相比，我们的技术将NN的存储效率提高了一个数量级。

    Compression and efficient storage of neural network (NN) parameters is critical for applications that run on resource-constrained devices. Despite the significant progress in NN model compression, there has been considerably less investigation in the actual \textit{physical} storage of NN parameters. Conventionally, model compression and physical storage are decoupled, as digital storage media with error-correcting codes (ECCs) provide robust error-free storage. However, this decoupled approach is inefficient as it ignores the overparameterization present in most NNs and forces the memory device to allocate the same amount of resources to every bit of information regardless of its importance. In this work, we investigate analog memory devices as an alternative to digital media -- one that naturally provides a way to add more protection for significant bits unlike its counterpart, but is noisy and may compromise the stored model's performance if used naively. We develop a variety of rob
    
[^174]: 基于潜力的奖赏设计用于强化学习智能体的新模式

    A new Potential-Based Reward Shaping for Reinforcement Learning Agent. (arXiv:1902.06239v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1902.06239](http://arxiv.org/abs/1902.06239)

    本论文提出了一种新的基于历史经验的奖赏设计方法，旨在提高强化学习智能体的性能，该方法具有广泛的应用前景。

    

    基于潜力的奖励设计（PBRS）是一类机器学习方法，旨在提高强化学习智能体在执行任务时利用额外知识的学习速度。其中，传递学习中先前学习任务中提取知识并将其迁移到目标任务中的是其中一个重要的步骤。在这项任务中收集到的知识对性能的提升起到了至关重要的作用。本文提出了一种基于历史经验的奖赏设计方法，通过利用先前的学习经验，利用任务无关性知识来增强强化学习智能体的任务特定奖励功能。

    Potential-based reward shaping (PBRS) is a particular category of machine learning methods which aims to improve the learning speed of a reinforcement learning agent by extracting and utilizing extra knowledge while performing a task. There are two steps in the process of transfer learning: extracting knowledge from previously learned tasks and transferring that knowledge to use it in a target task. The latter step is well discussed in the literature with various methods being proposed for it, while the former has been explored less. With this in mind, the type of knowledge that is transmitted is very important and can lead to considerable improvement. Among the literature of both the transfer learning and the potential-based reward shaping, a subject that has never been addressed is the knowledge gathered during the learning process itself. In this paper, we presented a novel potential-based reward shaping method that attempted to extract knowledge from the learning process. The propo
    

