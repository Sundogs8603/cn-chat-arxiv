# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Bring Your Own Data! Self-Supervised Evaluation for Large Language Models.](http://arxiv.org/abs/2306.13651) | 本研究提出了一种自我监督评估框架，通过分析输入文本上的变换对LLMs的灵敏度或不变性，直接监控LLMs在实际数据上的行为。 |
| [^2] | [GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models.](http://arxiv.org/abs/2306.13649) | 本文提出了广义知识蒸馏（GKD），通过从学生中采样输出序列来缓解分布不匹配，并在优化替代KL等离散度方面处理模型欠规范，达到了在摘要任务上最先进的性能。 |
| [^3] | [Offline Skill Graph (OSG): A Framework for Learning and Planning using Offline Reinforcement Learning Skills.](http://arxiv.org/abs/2306.13630) | 本论文提出了一个离线技能图（OSG）框架，用于在实际环境中解决复杂任务，它由三个模块组成，可以从先前收集的数据中学习并概括出解决长期任务的方法。 |
| [^4] | [Active Coverage for PAC Reinforcement Learning.](http://arxiv.org/abs/2306.13601) | 本文提出了PAC强化学习的主动覆盖问题，其中博弈理论算法CovGame能解决不同PAC RL任务。 |
| [^5] | [Margin Maximization in Attention Mechanism.](http://arxiv.org/abs/2306.13596) | 这篇论文证明了，在softmax-attention模型中，通过在p或等价的W上运行梯度下降，可以收敛到一个最大边缘解，这将局部最优的标记与非最优的标记分隔开。这明确地将注意力机制形式化为标记分离机制。 |
| [^6] | [TACOformer:Token-channel compounded Cross Attention for Multimodal Emotion Recognition.](http://arxiv.org/abs/2306.13592) | TACOformer 提出了一种综合性的多模态融合视角，利用 Token-chAnnel COmpound（TACO）Cross Attention 模块，同时建模通道级别和令牌级别的跨模态交互，实现了在多模态情感识别任务上具有先进性能。 |
| [^7] | [NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders of Deep Giants.](http://arxiv.org/abs/2306.13586) | NetBooster是一种增强微小神经网络架构的框架，旨在赋能微小深度学习，实验证明其优于现有的微小深度学习解决方案。 |
| [^8] | [Penalty Gradient Normalization for Generative Adversarial Networks.](http://arxiv.org/abs/2306.13576) | 本文提出一种名为罚分梯度归一化的方法，用于解决生成对抗网络（GANs）训练不稳定的问题。新方法仅对鉴别器函数施加罚分梯度范数约束，提高了鉴别器的容量，并且可以应用于不同的GAN体系结构。实验结果表明，使用罚分梯度归一化进行训练的GAN优于现有方法，无论是在Frechet Inception距离还是Inception Score上都有同样的表现。 |
| [^9] | [Scaling MLPs: A Tale of Inductive Bias.](http://arxiv.org/abs/2306.13575) | 本文研究了多层感知器（MLP）在视觉任务中的性能极限，并探讨了MLP相较于其他深度学习模型的归纳偏差，旨在推进深度学习理论和实践的结合。 |
| [^10] | [Efficient Model Selection for Predictive Pattern Mining Model by Safe Pattern Pruning.](http://arxiv.org/abs/2306.13561) | 本文提出了“安全模式修剪”方法解决预测模式挖掘中模式数量增长的问题，并展示了其在结构化数据的回归和分类问题中的有效性。 |
| [^11] | [A Survey on Multimodal Large Language Models.](http://arxiv.org/abs/2306.13549) | 本文追踪和总结了多模态大语言模型（MLLM）的最新进展，包括多模态指令调整、多模态上下文学习、多模态思维链和LLM辅助视觉推理等应用，指出了现有挑战和有前途的研究方向。 |
| [^12] | [Manifold Contrastive Learning with Variational Lie Group Operators.](http://arxiv.org/abs/2306.13544) | 本文提出了一种使用李群算子建模流形的对比学习方法，通过稀疏促进先验来参数化系数。这种方法可以提供可应用的特征增强来学习可转移的特征表示。 |
| [^13] | [Torsion Graph Neural Networks.](http://arxiv.org/abs/2306.13541) | 这项研究提出了一种名为TorGNN的图神经网络模型，其运用解析扭曲度量化图的局部结构，并在16种不同类型的网络链接预测任务上进行了验证。 |
| [^14] | [On the Convergence Rate of Gaussianization with Random Rotations.](http://arxiv.org/abs/2306.13520) | 该论文通过理论推导和实验验证，发现高斯化模型的收敛速度对于维数增加呈现出线性关系，原因是模型无法捕捉维之间的相关性。 |
| [^15] | [Binary domain generalization for sparsifying binary neural networks.](http://arxiv.org/abs/2306.13515) | 该研究提出了一种新的更一般的二进制域，更能抵御稀疏化技术，从而保证了改进的压缩和避免了严重的性能损失。 |
| [^16] | [DISCO-10M: A Large-Scale Music Dataset.](http://arxiv.org/abs/2306.13512) | DISCO-10M是一个新颖而广泛的音乐数据集，其规模超过了之前最大的可用音乐数据集一个数量级，提供高质量的音频资源和CLAP嵌入以加速机器学习在音乐领域的研究和应用。 |
| [^17] | [Two derivations of Principal Component Analysis on datasets of distributions.](http://arxiv.org/abs/2306.13503) | 本篇论文提出了一个新的方法来进行基于分布的主成分分析，通过方差最大化原理和重构误差最小化两种方法推导得闭合解。 |
| [^18] | [Cascade Subspace Clustering for Outlier Detection.](http://arxiv.org/abs/2306.13500) | 本文提出了一种基于级联子空间聚类和梯度提升的异常检测框架，能够将多个弱的异常检测器组合成强的异常检测器，实现更准确的异常检测。 |
| [^19] | [Retrieval of Boost Invariant Symbolic Observables via Feature Importance.](http://arxiv.org/abs/2306.13496) | 本文提出一种替代深度学习的Boost不变多项式方法，实现了直接分析最重要特征的有效分类，同时加速算法执行，性能接近于使用完整信息的算法。 |
| [^20] | [Adaptive Planning Search Algorithm for Analog Circuit Verification.](http://arxiv.org/abs/2306.13484) | 提出了一种机器学习（ML）方法用于模拟电路验证，在使用较少的仿真次数的同时提高了电路响应估计的精度，能够更好地发现最坏情况和故障。 |
| [^21] | [Efficient Online Processing with Deep Neural Networks.](http://arxiv.org/abs/2306.13474) | 该论文致力于提高神经网络的效率，提出并研究了连续推断网络（CINs）的概念，重点讨论在线推理的效率问题。 |
| [^22] | [Prediction under Latent Subgroup Shifts with High-Dimensional Observations.](http://arxiv.org/abs/2306.13472) | 本研究运用RPM在图像观测数据中识别低维离散隐变量，并且在潜在变量分布不同的情况下适当地预测目标结果。 |
| [^23] | [Understanding quantum machine learning also requires rethinking generalization.](http://arxiv.org/abs/2306.13461) | 本文通过实验认为，传统方法无法解释量子机器学习模型在只使用少量数据训练的情况下表现出成功的泛化性能，该模型可以准确拟合随机状态及随机标记的训练数据，这种记忆随机数据的能力违反了当前小泛化误差的概念，我们通过理论构建补充实证结果，表明量子神经网络可将任意标记拟合到量子状态上，暗示了它们的记忆能力，这些结果排除了单单基于经典复杂度度量的所有可能保证。 |
| [^24] | [Enhanced Dengue Outbreak Prediction in Tamilnadu using Meteorological and Entomological data.](http://arxiv.org/abs/2306.13456) | 本文研究了气候和媒介物幼虫指数对泰米尔纳德邦登革热疫情的影响，并通过引入蚊子幼虫指数提高了疫情预测的精确度。 |
| [^25] | [Minibatch training of neural network ensembles via trajectory sampling.](http://arxiv.org/abs/2306.13442) | 本文介绍了一种轨迹采样下的神经网络集合小批量训练方法，通过对MNIST数据集实验，发现相较于传统方法，该方法提高了两个数量级的计算效率，同时还提高了推断准确性。 |
| [^26] | [Trading-off price for data quality to achieve fair online allocation.](http://arxiv.org/abs/2306.13440) | 本文提出了一种多臂老虎机算法，该算法权衡了价格和数据质量，以解决在线分配问题中的公平性，使得对于源的选择，算法可以适应各种公平概念，具有一定的代价可以减少公平惩罚，并且证明了算法具有较小的遗憾界。 |
| [^27] | [A Weighted Autoencoder-Based Approach to Downlink NOMA Constellation Design.](http://arxiv.org/abs/2306.13423) | 通过在深度自编码器训练中引入加权损失函数，本文提出了一种新的下行非正交多址接入星座设计方法，可以灵活地调整星座设计从而平衡不同用户的误码率。 |
| [^28] | [CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning.](http://arxiv.org/abs/2306.13412) | CLUE使用条件可变自编码器实现专家数据的内在奖励，消除了离线强化学习中其它繁重的外在奖励工作。 |
| [^29] | [Neural Algorithmic Reasoning Without Intermediate Supervision.](http://arxiv.org/abs/2306.13411) | 神经算法推理最近的变革点是逐步学习算法，但我们提出了一种不需要中间监督的新方法，并在不牺牲性能的情况下规范模型的中间计算。 |
| [^30] | [Explainable Lifelong Stream Learning Based on "Glocal" Pairwise Fusion.](http://arxiv.org/abs/2306.13410) | 本文提出了可解释终身学习模型ExLL，它具有从稀缺数据学习、自组织的原型架构、可解释的IF-THEN规则和成对融合推理等特点，适用于调整，聚类和保护流数据。 |
| [^31] | [Higher-order Motif-based Time Series Classification for Forced Oscillation Source Location in Power Grids.](http://arxiv.org/abs/2306.13397) | 本研究通过基于高阶模式的时间序列分类确定强制振荡源的位置。相比传统方法，该数据驱动的无监督学习方法适用于多种FO情况，且不需要先验知识。单个和多源FO情况均可适用。在英国高压输电网上的测试表明了其有效性。 |
| [^32] | [Physics-informed neural networks modeling for systems with moving immersed boundaries: application to an unsteady flow past a plunging foil.](http://arxiv.org/abs/2306.13395) | 本研究探索了一种用于移动浸入边界系统的物理信息神经网络模型，该模型能够捕捉与精细IBM模拟相当的瞬态特征，并可用于实时流控和优化。此外，该研究可扩展到涉及多个移动体和流-结构相互作用的更复杂系统中。 |
| [^33] | [DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology.](http://arxiv.org/abs/2306.13384) | DiffInfinite 是一种能够生成任意大的组织学图像，使用小补丁进行快速训练，并可以更高效地并行化的方法，在组织病理学成像实践中有效解决了大规模信息、昂贵的手动注释以及保护性数据处理等独特挑战。 |
| [^34] | [Co-creating a globally interpretable model with human input.](http://arxiv.org/abs/2306.13381) | 该论文阐述了一种人工智能与人类协作共创联合可解释模型的方法，强调了该方法的可行性和挑战。 |
| [^35] | [Physics-constrained Random Forests for Turbulence Model Uncertainty Estimation.](http://arxiv.org/abs/2306.13370) | 本文提出了一种物理约束的机器学习方法，用于考虑湍流模型的认识不确定性，同时在准确数据稀缺时能够实现前置的预测置信度估计。 |
| [^36] | [Catching Image Retrieval Generalization.](http://arxiv.org/abs/2306.13357) | 本文通过提出一种新的测量检索性能的指标来解决广受欢迎的Recall@K度量取决于数据集中类别数量的限制，这种指标可以估计泛化能力并应用于常见的图像检索方法，提供了有关深度度量学习泛化的新见解。 |
| [^37] | [TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support.](http://arxiv.org/abs/2306.13339) | TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。 |
| [^38] | [Variance-Covariance Regularization Improves Representation Learning.](http://arxiv.org/abs/2306.13292) | 提出了方差-协方差正则化方法，旨在促进学习网络特征的多样性，改善表示学习和迁移学习的性能。 |
| [^39] | [Correcting discount-factor mismatch in on-policy policy gradient methods.](http://arxiv.org/abs/2306.13284) | 该论文提出了一种改进的算法来解决策略梯度算法中折扣因子不匹配的问题。该算法适用于许多现有的梯度估计器，避免了性能下降的问题。 |
| [^40] | [On Sensitivity and Robustness of Normalization Schemes to Input Distribution Shifts in Automatic MR Image Diagnosis.](http://arxiv.org/abs/2306.13276) | 本研究研究了自动磁共振影像诊断中归一化方案对输入分布转移的灵敏度和鲁棒性问题，因为MRI管道内的图像重建过程对各种形式的噪声高度敏感，导致图像中出现任意的伪影，噪声分布不是固定的，而且因为DL模型对这些不同的伪影非常敏感，所以本研究是极其有意义的。 |
| [^41] | [Can Continual Learning Improve Long-Tailed Recognition? Toward a Unified Framework.](http://arxiv.org/abs/2306.13275) | 本文针对长尾识别问题，提出一种持续学习方法，通过将头部集和尾部集的学习视为两个独立连续的步骤，并利用定理证明持续学习可以有效地更新学习者的权重以学习尾部，同时不会忘记头部。 |
| [^42] | [FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning.](http://arxiv.org/abs/2306.13264) | 本文提出了一种名为FedSelect的新联邦学习框架，通过寻找最佳客户端子网络从而直接个性化客户端子网络结构和参数，同时保留了全局知识，提高了客户端性能。 |
| [^43] | [Synthetic data shuffling accelerates the convergence of federated learning under data heterogeneity.](http://arxiv.org/abs/2306.13263) | 本文提出了一种通过对本地生成的合成数据进行重排来加速异构数据下联邦学习的收敛的方法，实验表明，对合成数据进行重排可以大幅提高现有多个联邦学习算法的性能。 |
| [^44] | [Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models.](http://arxiv.org/abs/2306.13255) | 本文研究了高斯协变量下的过参数化线性模型在多类分类问题中的泛化能力，成功解决了之前的猜想，并提出的新下界具有信息论中的强对偶定理的性质。 |
| [^45] | [Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok.](http://arxiv.org/abs/2306.13253) | 本文提出了一种低成本方法来预测神经网络中的理解前浪潮，即通过研究前几轮的学习曲线来判断后续是否出现理解前浪潮。使用波形振荡和学习曲线的频谱特征值可以高精度地预测理解前浪潮。 |
| [^46] | [Approximate Causal Effect Identification under Weak Confounding.](http://arxiv.org/abs/2306.13242) | 本文提出了一种有效的方法来在弱混淆下识别因果效应的上限和下限，并证明了这种方法的计算效率优于最先进的多项式程序。 |
| [^47] | [The Inductive Bias of Flatness Regularization for Deep Matrix Factorization.](http://arxiv.org/abs/2306.13239) | 本研究通过学习深度矩阵分解，阐明了大深度神经网络优化器中的随机性隐式正则化效应，并证明了最小化损失函数中海森矩阵迹近似等同于最小化其对应端对端矩阵参数的Schatten 1-范数乘积。 |
| [^48] | [Pruning for Better Domain Generalizability.](http://arxiv.org/abs/2306.13237) | 本文研究了基于剪枝的域泛化方法，提出了一种新的剪枝评分方法DSS，该方法不是为了保持源准确性，而是直接增强模型的鲁棒性。实验证明该方法可以与最先进的泛化方法结合使用，即便只引入少量稀疏也能显著提高模型性能。 |
| [^49] | [Logarithmic Regret for Matrix Games against an Adversary with Noisy Bandit Feedback.](http://arxiv.org/abs/2306.13233) | 本文提出了一种算法，在带有嘈杂贝叶斯反馈的零和矩阵博弈中，实现了对数遗憾策略。 |
| [^50] | [TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning.](http://arxiv.org/abs/2306.13229) | 本文提出了TACO方法，一种基于时间潜在动作驱动对比损失的视觉强化学习方法，能够同时学习状态表示和动作表示，提高代理学习的效率。 |
| [^51] | [Diverse Community Data for Benchmarking Data Privacy Algorithms.](http://arxiv.org/abs/2306.13216) | 多样社区数据摘要旨在为隐私保护机器学习研究提供真实、多样和复杂的基准数据，以解决合成数据的偏差和隐私问题。 |
| [^52] | [ovla: Neural Network Ownership Verification using Latent Watermarks.](http://arxiv.org/abs/2306.13215) | ovla是一种使用隐式水印进行神经网络所有权验证的新方法，通过训练网络使水印保持休眠状态，只有在所有者的秘密密钥被应用时才会被激活，该方法在神经网络所有权验证准确率、误报率和抵抗对抗攻击方面优于现有的最先进方法。 |
| [^53] | [Visual Adversarial Examples Jailbreak Large Language Models.](http://arxiv.org/abs/2306.13213) | 本文对将图像引入大型语言模型的安全隐患进行了分析，指出视觉输入空间的连续性和高维性是对抗攻击的丰富领域，同时也为视觉攻击者提供了更广泛的实现对抗目标的可能性。 |
| [^54] | [Differentially Private Synthetic Data Using KD-Trees.](http://arxiv.org/abs/2306.13211) | 本文提出了一种使用KD-树的算法，用于生成 $\epsilon $-差分隐私的合成数据，其核密度类似于真实数据集的核密度。该方法克服了维度灾难，是一种可扩展的算法。 |
| [^55] | [Improving Log-Cumulant Based Estimation of Roughness Information in SAR imagery.](http://arxiv.org/abs/2306.13200) | 本文提出使用对数累积法改进了$\mathcal{G}^0$分布参数估计方法，可以在恒定的时间内计算出粗糙度估计，从而实现快速可靠的SAR图像理解。 |
| [^56] | [Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?.](http://arxiv.org/abs/2306.13197) | 在Gradient-based Attribution Methods中，使用Pre Softmax分数或Post Softmax分数的梯度的选择有各自的优缺点，需要根据具体情况进行权衡。 |
| [^57] | [DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability.](http://arxiv.org/abs/2306.13196) | 本文提出了一种使用扩散模型作为采样器的任务和动作规划方法，在部分可观测下能够实现长周期受约束的操作计划。 |
| [^58] | [Uniform Convergence with Square-Root Lipschitz Loss.](http://arxiv.org/abs/2306.13188) | 该论文通过平方根Lipschitz损失的一致收敛性，对一般的高斯数据建立了保证，允许处理广泛的损失类别，并重新推导和更好地理解“乐观率”的学习保证和插值。 |
| [^59] | [An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression.](http://arxiv.org/abs/2306.13185) | 本文研究了核岭回归中过拟合成本，采用“不可知”的观点，以分析样本量和任务特征结构对成本的影响。通过分析提供了更细致的过度拟合表征。 |
| [^60] | [Prediction of Annual Snow Accumulation Using a Recurrent Graph Convolutional Approach.](http://arxiv.org/abs/2306.13181) | 本研究利用循环图卷积神经网络和图注意力网络的方法，精确预测极地区域年降雪量，且在较大数据集上使用更少的输入数据具有较好的性能。 |
| [^61] | [Key Frame Extraction with Attention Based Deep Neural Networks.](http://arxiv.org/abs/2306.13176) | 本研究提出了一种基于深度自编码器模型和注意力层的深度学习方法，用于自动从视频中提取关键帧，提取的关键帧可以用作要使用的方法和模型的输入特征，可用于摘要和安防等行业中的自动化工作。 |
| [^62] | [AmicroN: A Framework for Generating Annotations for Human Activity Recognition with Granular Micro-Activities.](http://arxiv.org/abs/2306.13149) | 本文提出了一种名为AmicroN的框架，可以利用传感器数据自动生成微动作注释，弥补现有数据集缺乏细粒度注释的不足。 |
| [^63] | [Adversarial Resilience in Sequential Prediction via Abstention.](http://arxiv.org/abs/2306.13119) | 本文提出了一种处理顺序预测的模型，允许在不对对抗性样例进行预测的情况下提高算法抗对抗攻击的能力。 |
| [^64] | [A Machine Learning Pressure Emulator for Hydrogen Embrittlement.](http://arxiv.org/abs/2306.13116) | 本文提出了一种物理信息的机器学习模型，用于预测管道内壁的气体压力，为管道系统监控提供了第一步。该方法具有较高保真度且优于纯数据驱动的方法。 |
| [^65] | [EEG Decoding for Datasets with Heterogenous Electrode Configurations using Transfer Learning Graph Neural Networks.](http://arxiv.org/abs/2306.13109) | 本文提出了一种用于克服记录设备和电极布局变化带来的限制，学习不同实验室的多样化EEG数据的方法。文中研发了一种新型机器学习框架，该框架结合了图神经网络和迁移学习方法，以实现非侵入性动作想象（MI）EEG解码。 |
| [^66] | [Multi-task Learning for Radar Signal Characterisation.](http://arxiv.org/abs/2306.13105) | 本文提出一种多任务学习的方法来解决雷达信号分类和特征化的问题，引入IQST等参考架构，通过回归和分类的多重任务优化提高性能，在合成雷达数据集上展示了良好的表现，并提供了首个雷达信号特征化基准测试样例。 |
| [^67] | [Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses.](http://arxiv.org/abs/2306.13104) | 本研究提出了一种“人类介入的视觉前列腺深度刺激编码优化”的方法，通过反演前向模型、实时优化编码参数等手段，显著提高了感知质量。 |
| [^68] | [Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks.](http://arxiv.org/abs/2306.13103) | 本研究对于文本到图像扩散模型（T2I DMs）进行了第一个针对现实攻击的鲁棒性评估，考虑由人类可以产生的现实误差所构成的攻击范围以确保语义一致性。研究表明，我们提出的攻击目标具有较好的攻击效果。 |
| [^69] | [MBrain: A Multi-channel Self-Supervised Learning Framework for Brain Signals.](http://arxiv.org/abs/2306.13102) | MBrain是一种针对脑信号的多通道自监督学习框架，可解决监督学习方法需要高成本临床标签和不同测量方法之间临床模式差异的问题。 |
| [^70] | [BrainNet: Epileptic Wave Detection from SEEG with Hierarchical Graph Diffusion Learning.](http://arxiv.org/abs/2306.13101) | 本研究提出了第一个使用真实世界SEEG数据集进行癫痫波检测的数据驱动研究，通过使用分层图卷积网络技术，可以检测并分析扩散路径，从而有助于临床实践。 |
| [^71] | [GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning.](http://arxiv.org/abs/2306.13089) | 本研究提出了一种名为GIMLET的统一图文模型，用于在零样本设置下使用自然语言指令完成分子相关任务。我们解决了现有模型的指令处理不足和图形容量有限的问题，并证明了使用GIMLET能够增强图形特征的泛化能力。 |
| [^72] | [Reinforcement Federated Learning Method Based on Adaptive OPTICS Clustering.](http://arxiv.org/abs/2306.12859) | 本文提出了一种基于自适应OPTICS聚类的强化联邦学习方法，旨在缓解不同用户终端上的数据分布不同所带来的负面影响，并有效提高了联邦学习方法的性能。 |
| [^73] | [Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery.](http://arxiv.org/abs/2306.12802) | 本研究在药物发现方面使用了多模态知识图谱表示学习，获得了最先进的结果。他们整合了来自不同来源的数据，并提供了基于这些数据的预训练模型。 |
| [^74] | [OptIForest: Optimal Isolation Forest for Anomaly Detection.](http://arxiv.org/abs/2306.12703) | 本论文针对隔离森林算法中分支因子的最优取值问题，基于隔离效率提出创新算法OptIForest，该算法结构简洁、检测性能优秀，可应用于各种异常检测场景。 |
| [^75] | [Semi-Implicit Denoising Diffusion Models (SIDDMs).](http://arxiv.org/abs/2306.12511) | SIDDMs是一种新方法，通过匹配隐式和显式因子，实现在生成模型中快速收敛且一定程度上保证样本多样性和质量。 |
| [^76] | [Comparing deep learning models for volatility prediction using multivariate data.](http://arxiv.org/abs/2306.12446) | 本研究比较了使用不同深度学习模型预测多项资产波动率的效果，发现时间融合变压器及时间卷积神经网络的变体最优，可以用于实践。 |
| [^77] | [MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates.](http://arxiv.org/abs/2306.12212) | 本文提出的 MimiC 算法解决了联邦学习中客户端退出问题，通过模仿缺失的客户端更新解决了聚合更新和期望中心更新之间的分歧，实现了更高的测试准确率和更低的通信成本。 |
| [^78] | [Training Transformers with 4-bit Integers.](http://arxiv.org/abs/2306.11987) | 本文提出了一种使用INT4算术训练Transformer的方法，并细致地分析了转换器中激活和梯度的特定结构，为它们提出了专用的量化器。算法在多个任务中达到了竞争性的准确性。 |
| [^79] | [Decoding Urban-health Nexus: Interpretable Machine Learning Illuminates Cancer Prevalence based on Intertwined City Features.](http://arxiv.org/abs/2306.11847) | 本研究通过XGBoost机器学习模型揭示了社会人口统计学、建筑环境特征和环境危害曝露特征是影响城市癌症患病率的关键，结合因果推断实验提出了增加绿化空间、减少开发区和总排放量可以缓解癌症的发生。 |
| [^80] | [Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network.](http://arxiv.org/abs/2306.10946) | 本文提出了一种基于注意力知识图卷积网络的旅游景点推荐模型，通过自动语义发掘目标景点的相邻实体，根据旅客的喜好选择，预测类似景点的概率，实验中取得良好效果。 |
| [^81] | [ExpPoint-MAE: Better interpretability and performance for self-supervised point cloud transformers.](http://arxiv.org/abs/2306.10798) | 本文探讨了自监督Transformer在点云领域中的特性，并发现预训练方案能够帮助更好地理解基础几何，提出一种解冻策略，在不引入其他修改的情况下始终优于基线，并在Transformer模型中取得了最佳结果。 |
| [^82] | [Text-Driven Foley Sound Generation With Latent Diffusion Model.](http://arxiv.org/abs/2306.10359) | 本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。我们通过迁移学习对系统进行微调，并引入可训练的层来改善文本嵌入，同时也改进了生成的波形。 |
| [^83] | [Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction.](http://arxiv.org/abs/2306.10045) | 本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。 |
| [^84] | [LEACE: Perfect linear concept erasure in closed form.](http://arxiv.org/abs/2306.03819) | 本文介绍了一种闭合形式的方法LEACE，可在删除指定特征的同时尽可能少地改变表示，并可证明防止所有线性分类器检测到概念。作者用“概念擦除”这一新方法将其应用于大型语言模型，在测量语言模型对词性的依赖性和减少BERT嵌入中的性别偏差任务中得出良好表现。 |
| [^85] | [DVFO: Dynamic Voltage, Frequency Scaling and Workload Offloading for DNN Edge Inference.](http://arxiv.org/abs/2306.01811) | 提出了一种动态电压、频率缩放和工作负载卸载的DNN边缘推理框架DVFO，它通过深度强化学习来联合优化DVFS和卸载参数，实现了对边缘设备计算资源的优化，同时提高了DNN模型的能源效率。 |
| [^86] | [Comparative Study on the Effects of Noise in ML-Based Anxiety Detection.](http://arxiv.org/abs/2306.01110) | 本研究探究了噪声如何影响基于机器学习的焦虑检测模型，并开发出在嘈杂的现实环境中具有抗干扰性和适应性的模型，以推进该领域的发展。 |
| [^87] | [Predicting Temporal Aspects of Movement for Predictive Replication in Fog Environments.](http://arxiv.org/abs/2306.00575) | 本文研究了时间预测在雾环境下预测复制中的应用，提出了一种利用Holder-Winter指数平滑法进行时间预测的新模型，可以在减少多余数据的同时，只有微小的数据可用性降低。 |
| [^88] | [Compression with Bayesian Implicit Neural Representations.](http://arxiv.org/abs/2305.19185) | 该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。 |
| [^89] | [Revisiting Automated Prompting: Are We Actually Doing Better?.](http://arxiv.org/abs/2304.03609) | 本文重审自动提示技术在六个不同的任务和更广泛范围的K-shot学习设置上的表现，发现自动提示并不能始终优于手动提示，因此手动提示应该作为自动提示的一个基准线。 |
| [^90] | [Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots.](http://arxiv.org/abs/2304.01430) | 该论文提出了一种新的无监督多对象发现方法，通过一种上下文分隔的槽结构来将视觉场分割为独立运动区域，并用对抗性标准来保证解码器无法重构整个光流。 |
| [^91] | [Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration.](http://arxiv.org/abs/2303.11435) | InDI是一种新的监督式图像修复公式，通过逐步改进图像质量来生成比现有回归方法更真实和详细的图像，修复效果更具有感知质量。 |
| [^92] | [SC-Block: Supervised Contrastive Blocking within Entity Resolution Pipelines.](http://arxiv.org/abs/2303.03132) | 本研究提出了SC-Block，一种阻塞方法，利用监督对比学习来定位嵌入空间中的记录并使用最近邻搜索建立候选集合，实验证明该方法在保证精度的同时显著缩短了运行时间。 |
| [^93] | [Stochastic Gradient Descent under Markovian Sampling Schemes.](http://arxiv.org/abs/2302.14428) | 本文研究了基于马尔科夫抽样方案的随机梯度下降算法，提出了MC-SAG算法实现了用于分布式算法的通信效率高的token 算法。 |
| [^94] | [Transformed Distribution Matching for Missing Value Imputation.](http://arxiv.org/abs/2302.10363) | 本文提出一种通过将两个数据批次转换为一个潜在空间并进行分布匹配的方法来插补缺失值，在大量实验中达到了最先进的性能。 |
| [^95] | [Dual RL: Unification and New Methods for Reinforcement and Imitation Learning.](http://arxiv.org/abs/2302.08560) | 这篇论文介绍了双重强化学习的概念，并在一个统一的框架下解释了几种最新深度强化学习算法及模仿学习方法。作者提出了双重模仿学习方法（DIL）直接最小化策略之间的距离，并提出了一种新的离线演员-评论家方法。 |
| [^96] | [MarioGPT: Open-Ended Text2Level Generation through Large Language Models.](http://arxiv.org/abs/2302.05981) | MarioGPT是第一个文本到超级马里奥兄弟游戏关卡的生成模型，通过大型语言模型实现开放式的、可控制的关卡生成。 |
| [^97] | [Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy.](http://arxiv.org/abs/2302.01463) | 本文研究梯度下降在线性相关噪声下的表现，提出了新的矩阵分解方法用于不同ially private optimization。 |
| [^98] | [Recurrent Graph Convolutional Networks for Spatiotemporal Prediction of Snow Accumulation Using Airborne Radar.](http://arxiv.org/abs/2302.00817) | 本文基于循环图卷积网络，利用机载雷达数据预测特定位置最近连续年份的积雪积累，并且相较于其他模型表现更好且更具一致性。 |
| [^99] | [Training with Mixed-Precision Floating-Point Assignments.](http://arxiv.org/abs/2301.13464) | 通过运用混合精度浮点数运算的训练方法，我们为卷积神经网络生成精度分配，相比低精度浮点数训练中考虑的精度分配，其使用更少的内存，同时也导致更准确的卷积网络。 |
| [^100] | [STEEL: Singularity-aware Reinforcement Learning.](http://arxiv.org/abs/2301.13152) | 这篇论文介绍了一种新的批量强化学习算法STEEL，在具有连续状态和行动的无限时马尔可夫决策过程中，不依赖于绝对连续假设，通过最大均值偏差和分布鲁棒优化确保异常情况下的性能。 |
| [^101] | [Curvature Filtrations for Graph Generative Model Evaluation.](http://arxiv.org/abs/2301.12906) | 该论文使用图形曲率描述符和拓扑数据分析中的新方法相结合，以获得用于评估图形生成模型的稳健、表达性的描述符。 |
| [^102] | [Revisiting Signed Propagation for Graph Neural Networks.](http://arxiv.org/abs/2301.08918) | 该论文重新审视了在异质图中的有符号传播方法，提出了一种适用于多类别图的新策略，并克服了其带来的不确定性和不一致性问题，取得了显著的性能提升。 |
| [^103] | [Best Arm Identification in Stochastic Bandits: Beyond $\beta-$optimality.](http://arxiv.org/abs/2301.03785) | 本文介绍了一种新的算法来实现随机多臂赌博机中最佳臂识别，它既具有最优性能又计算上高效。 |
| [^104] | [Network Slicing via Transfer Learning aided Distributed Deep Reinforcement Learning.](http://arxiv.org/abs/2301.03262) | 本文提出了一种新颖的基于迁移学习辅助的多智能体深度强化学习(MADRL)方法，通过信息共享解决跨单元格跨切片的资源分配问题，加速策略部署，并在大量的仿真评估中得到验证。 |
| [^105] | [The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique.](http://arxiv.org/abs/2212.11146) | 本文介绍了使用Transkribus平台改进手写文本识别（HTR）模型性能的实践经验，包括创建转录协议、完整使用语言模型以及确定最佳使用基础模型的方法，这些方法可以将单个模型的性能提高20%以上（达到字符错误率低于5%），并讨论了HTR平台的合作性质和数据分享等挑战。 |
| [^106] | [DeepJoin: Joinable Table Discovery with Pre-trained Language Models.](http://arxiv.org/abs/2212.07588) | 本文提出了 Deepjoin，一个基于深度学习的模型，用于精确高效地发现可连接表。通过预训练语言模型进行嵌入式检索，能够同时服务于等值和语义连接，并允许用户自定义模板。实验结果表明 Deepjoin 在效率和准确性方面显著优于现有的最先进方法。 |
| [^107] | [DLKoopman: A deep learning software package for Koopman theory.](http://arxiv.org/abs/2211.08992) | DLKoopman是一种深度学习软件包，可以将非线性动力系统编码为线性空间中的线性动力系统，并用于任何数据驱动学习和优化，软件包包括平均归一化绝对误差和超参数搜索工具。 |
| [^108] | [PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention Networks with Imbalanced PU Labels.](http://arxiv.org/abs/2211.08604) | 本文提出了一种基于图注意力网络和PU损失函数的欺诈检测方法PU GNN，通过改进的GraphSMOTE算法来处理P2E MMORPGs欺诈检测数据集中的标签分布不平衡问题，实验证明该方法在欺诈检测方面具有良好的性能表现。 |
| [^109] | [Bayesian Networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data.](http://arxiv.org/abs/2211.04924) | 使用贝叶斯网络及语音与多模态数据，通过捕获其条件依赖性得出预测结果，更准确地预测抑郁和其相关症状。 |
| [^110] | [Efficient Large-scale Audio Tagging via Transformer-to-CNN Knowledge Distillation.](http://arxiv.org/abs/2211.04772) | 本文提出了一种基于离线知识蒸馏的训练方法，将高性能但复杂的Transformer模型转化为高效的CNN模型，从而取得了优于以前方法的预测性能和参数以及计算效率。 |
| [^111] | [Reinforcement Learning in Non-Markovian Environments.](http://arxiv.org/abs/2211.01595) | 本文通过递归计算近似充分统计量，提出了一种基于自编码器的代理设计方案，实现了在非马尔可夫环境中进行强化学习。 |
| [^112] | [On the Informativeness of Supervision Signals.](http://arxiv.org/abs/2211.01407) | 本文使用信息论比较了常用的监督信号对表示学习性能的贡献，并为在大数据时代使用硬标签提供了理论上的证明，但对于少样本学习和分布外泛化，需要使用更丰富的监督信号。 |
| [^113] | [Solving Coupled Differential Equation Groups Using PINO-CDE.](http://arxiv.org/abs/2210.00222) | PINO-CDE是一种用于解决耦合微分方程组的深度学习框架，通过物理-知情的神经算符理论，使用单个网络处理所有量的CDEs，实现了高精度、鲁棒性和快速计算速度。 |
| [^114] | [SpeedLimit: Neural Architecture Search for Quantized Transformer Models.](http://arxiv.org/abs/2209.12127) | 本文介绍了SpeedLimit——一种新的神经架构搜索技术，通过在量化的Transformer模型中添加上限延迟约束，优化准确性。该方法比当前最先进的技术表现更好，为在延迟敏感的环境中使用Transformer模型提供了新的可能性。 |
| [^115] | [Large-step neural network for learning the symplectic evolution from partitioned data.](http://arxiv.org/abs/2208.14148) | 本研究使用分区的方法来训练大步神经网络，学习辛哈密顿系统的演化，有效抑制累积误差，并成功保持Jacobi积分的守恒。 |
| [^116] | [SPRT-based Efficient Best Arm Identification in Stochastic Bandits.](http://arxiv.org/abs/2207.11158) | 本论文提出了一种使用SPRT框架设计的BAI算法并运用于指数族赌博机，该算法具有样本复杂度渐近最优和$\delta-$PAC保证的特点。 |
| [^117] | [Selectively increasing the diversity of GAN-generated samples.](http://arxiv.org/abs/2207.01561) | 提出了一种有选择性地增加GAN生成样本多样性的新方法，并通过向损失函数添加正则化进行优化。在保持生成样本质量的同时，有效增加其多样性，验证结果良好。 |
| [^118] | [Online Resource Allocation under Horizon Uncertainty.](http://arxiv.org/abs/2206.13606) | 本论文研究的是随机在线资源分配问题，并且针对时间长度不确定性，提出了具有鲁棒性的在线算法。 |
| [^119] | [Semi-Autoregressive Energy Flows: Exploring Likelihood-Free Training of Normalizing Flows.](http://arxiv.org/abs/2206.06672) | 本文研究了正则化流的无似然训练，提出了能量目标，支持半自回归能量流等灵活的模型架构，并相对于基于似然性的流表现出有竞争力的性能，质疑了机器学习领域中以最大似然作为目标或指标的使用，为生成建模的研究做出了贡献。 |
| [^120] | [Test-Time Robust Personalization for Federated Learning.](http://arxiv.org/abs/2205.10920) | 本文提出了FedTHE+方法，它能够使联邦学习模型具有鲁棒性，抵御各种测试时间分布转移的影响。 |
| [^121] | [DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data.](http://arxiv.org/abs/2205.00701) | DeepGraviLens是一种多模态神经网络，用于分类属于不同类型的引力透镜数据，具有高精度和优于现有方法的结果。 |
| [^122] | [Convergence of First-Order Methods for Constrained Nonconvex Optimization with Dependent Data.](http://arxiv.org/abs/2203.15797) | 本文提出了一种可用于受限非凸优化中相关数据采样的一阶方法，并通过采用更加温和的混合条件，将复杂度从$\tilde{O}(\varepsilon^{-8})$提升至$\tilde{O}(\varepsilon^{-4})$。 |
| [^123] | [Single-Leg Revenue Management with Advice.](http://arxiv.org/abs/2202.10939) | 本文提出了一种算法建议框架解决了单程收益管理问题，试图应用机器学习方法的预测准确性并平衡准确性和不准确性的竞争比率。 |
| [^124] | [Instance-Adaptive Video Compression: Improving Neural Codecs by Training on the Test Set.](http://arxiv.org/abs/2111.10302) | 该论文提出了一种基于实例自适应学习的视频压缩算法，通过对测试集进行训练来提高神经编解码器的性能，并将最优参数与潜在编码一起传输到接收端。该算法在多个数据集上有着显著的性能提升，并且可以提高任何神经视频编解码器的性能。 |
| [^125] | [Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting.](http://arxiv.org/abs/2110.05367) | 该论文提出了一种新方法GEEP，用于提高预训练语言模型的性别公平性，同时没有灾难性遗忘问题。透过性别中性数据学习性别相关的提示，GEEP实现了SOTA表现并在GLUE性能上取得了显著提高。 |
| [^126] | [Exploring the Context Generalizability in Spatiotemporal Crowd Flow Prediction: Benchmark and Guideline.](http://arxiv.org/abs/2106.16046) | 本文研究了时空人群流量预测中的上下文泛化性，建立了基准，提出了通用分类法，为上下文选择和建模提供了指南。 |
| [^127] | [Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables.](http://arxiv.org/abs/2102.10324) | 本文提出了关于在带有隐藏变量的因果图模型中选择最优反向门控调整集以估计因果效应问题的必要和充分图条件及其算法，解决了此类图模型中可能存在没有最优集的情况。 |
| [^128] | [Weak Signal Asymptotics for Sequentially Randomized Experiments.](http://arxiv.org/abs/2101.09855) | 本文使用弱信号渐近行为的方法研究了一类顺序随机实验，认为这类顺序实验的样本路径会弱收敛到扩散极限，并能获得关于几种顺序实验的后悔和信念演变的多个见解。 |
| [^129] | [Breaking the Deadly Triad with a Target Network.](http://arxiv.org/abs/2101.08862) | 本论文研究了如何通过使用目标网络来稳定训练，提出了一种新的目标网络更新规则并证明了其在离线学习、线性函数逼近和自举的算法中的收敛性，最终达到了收敛到正则化TD固定点的效果。 |
| [^130] | [Learning rewards for robotic ultrasound scanning using probabilistic temporal ranking.](http://arxiv.org/abs/2002.01240) | 本文提出了一种利用概率时序排名的方法，从探索性演示中推断医用超声扫描机器人任务的奖励函数。 |

# 详细

[^1]: 自带数据！大型语言模型的自我监督评估

    Bring Your Own Data! Self-Supervised Evaluation for Large Language Models. (arXiv:2306.13651v1 [cs.CL])

    [http://arxiv.org/abs/2306.13651](http://arxiv.org/abs/2306.13651)

    本研究提出了一种自我监督评估框架，通过分析输入文本上的变换对LLMs的灵敏度或不变性，直接监控LLMs在实际数据上的行为。

    

    随着大型语言模型（LLMs）的兴起以及它们在各种领域的普及，衡量语言模型在实际数据上的行为变得不可或缺。为了解决这个问题，本研究提出了一种自我监督评估框架，通过分析输入文本上的变换对LLMs的灵敏度或不变性，直接监控LLM在野外收集的数据集或在模型部署期间进行的流数据的行为，实现了评估LLMs的有效和可扩展的解决方案。

    With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, tox
    
[^2]: GKD：自回归序列模型的广义知识蒸馏

    GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])

    [http://arxiv.org/abs/2306.13649](http://arxiv.org/abs/2306.13649)

    本文提出了广义知识蒸馏（GKD），通过从学生中采样输出序列来缓解分布不匹配，并在优化替代KL等离散度方面处理模型欠规范，达到了在摘要任务上最先进的性能。

    

    知识蒸馏通常用于压缩神经网络，以减少推理成本和内存占用。然而，当前针对自回归模型（如生成语言模型）的蒸馏方法存在两个关键问题：（1）训练期间输出序列和部署时由学生模型生成的序列之间分布不匹配，（2）模型欠规范，学生模型可能不够表达老师分布。为了解决这些问题，我们提出了广义知识蒸馏（GKD）。GKD通过在训练期间从学生中采样输出序列来缓解分布不匹配。此外，GKD通过优化替代KL等离散度来处理模型欠规范，这些离散度集中于生成可能符合老师分布的学生样本。我们证明，在摘要任务上，GKD优于常用的LLM蒸馏方法，在几个基准数据集上实现了最先进的性能。

    Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
    
[^3]: 离线技能图（OSG）：使用离线强化学习技能进行学习和规划的框架。

    Offline Skill Graph (OSG): A Framework for Learning and Planning using Offline Reinforcement Learning Skills. (arXiv:2306.13630v1 [cs.RO])

    [http://arxiv.org/abs/2306.13630](http://arxiv.org/abs/2306.13630)

    本论文提出了一个离线技能图（OSG）框架，用于在实际环境中解决复杂任务，它由三个模块组成，可以从先前收集的数据中学习并概括出解决长期任务的方法。

    

    强化学习因其在竞技游戏中的成功而受到广泛关注。然而，它在日常应用中的采用受到限制（例如工业、家庭、医疗等）。本文通过介绍一个规划离线技能并在实际环境中解决复杂任务的框架来解决这个问题。我们的框架包括三个模块，共同使智能体能够从先前收集的数据中学习并概括出解决长期任务的方法。我们通过对一个需要解决复杂任务的机械臂进行测试来演示我们的方法。

    Reinforcement Learning has received wide interest due to its success in competitive games. Yet, its adoption in everyday applications is limited (e.g. industrial, home, healthcare, etc.). In this paper, we address this limitation by presenting a framework for planning over offline skills and solving complex tasks in real-world environments. Our framework is comprised of three modules that together enable the agent to learn from previously collected data and generalize over it to solve long-horizon tasks. We demonstrate our approach by testing it on a robotic arm that is required to solve complex tasks.
    
[^4]: PAC强化学习的主动覆盖

    Active Coverage for PAC Reinforcement Learning. (arXiv:2306.13601v1 [cs.LG])

    [http://arxiv.org/abs/2306.13601](http://arxiv.org/abs/2306.13601)

    本文提出了PAC强化学习的主动覆盖问题，其中博弈理论算法CovGame能解决不同PAC RL任务。

    

    收集和利用具有良好覆盖特性的数据在强化学习（RL）的不同方面中起着关键作用，包括无奖励探索和离线学习。我们形式化了周期性马尔可夫决策过程（MDP）中的主动覆盖问题，其中的目标是与环境交互，以满足给定的采样要求。我们的主要贡献是对主动覆盖样本复杂度的一个与示例有关的下界和一个简单的博弈理论算法CovGame，该算法几乎与下界相匹配。然后，我们展示了CovGame可用作解决不同PAC RL任务的构建模块。

    Collecting and leveraging data with good coverage properties plays a crucial role in different aspects of reinforcement learning (RL), including reward-free exploration and offline learning. However, the notion of "good coverage" really depends on the application at hand, as data suitable for one context may not be so for another. In this paper, we formalize the problem of active coverage in episodic Markov decision processes (MDPs), where the goal is to interact with the environment so as to fulfill given sampling requirements. This framework is sufficiently flexible to specify any desired coverage property, making it applicable to any problem that involves online exploration. Our main contribution is an instance-dependent lower bound on the sample complexity of active coverage and a simple game-theoretic algorithm, CovGame, that nearly matches it. We then show that CovGame can be used as a building block to solve different PAC RL tasks. In particular, we obtain a simple algorithm for
    
[^5]: 注意力机制中的边缘最大化

    Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])

    [http://arxiv.org/abs/2306.13596](http://arxiv.org/abs/2306.13596)

    这篇论文证明了，在softmax-attention模型中，通过在p或等价的W上运行梯度下降，可以收敛到一个最大边缘解，这将局部最优的标记与非最优的标记分隔开。这明确地将注意力机制形式化为标记分离机制。

    

    注意力机制是Transformer架构的核心组件，也是大型语言模型取得惊人成功的原因之一。然而，注意力机制背后的理论原则尚不清楚，特别是它的非凸优化动力学。本文探讨了开创性的softmax-attention模型$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$，其中$\boldsymbol{X}$是标记序列，$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$是可调参数。我们证明了在$\boldsymbol{p}$或等价的$\boldsymbol{W}$上运行梯度下降会沿着方向收敛到分隔“局部最优”标记和“非最优”标记的最大边缘解。这明确地形式化了注意力作为一种标记分离机制。值得注意的是，我们的结果适用于一般数据，并使用嵌入$\boldsymbol{Xv}$和$\texttt{softmax}(\boldsymbol{XWp})$精细地表征标记的“最优性”。

    Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
    
[^6]: TACOformer: 多模态情感识别中的令牌通道复合交叉注意力模型

    TACOformer:Token-channel compounded Cross Attention for Multimodal Emotion Recognition. (arXiv:2306.13592v1 [cs.MM])

    [http://arxiv.org/abs/2306.13592](http://arxiv.org/abs/2306.13592)

    TACOformer 提出了一种综合性的多模态融合视角，利用 Token-chAnnel COmpound（TACO）Cross Attention 模块，同时建模通道级别和令牌级别的跨模态交互，实现了在多模态情感识别任务上具有先进性能。

    

    最近，基于生理信号的情感识别成为了一个进行深入研究的领域。利用多模态、多通道的生理信号显著提高了情感识别系统的性能，因为它们具有互补性。然而，有效地整合来自不同模态的与情感相关的语义信息并捕获跨模态的依赖关系仍然是一个具有挑战性的问题。许多现有的多模态融合方法忽略了多个通道信号之间的令牌到令牌或通道到通道的相关性，这在一定程度上限制了模型的分类能力。在本文中，我们提出了一种综合性的多模态融合视角，它整合了通道级别和令牌级别的跨模态交互，采用复合机制进行跨通道处理。特别是，我们引入了一种统一的交叉注意力模块，称为 Token-chAnnel COmpound（TACO）Cross Attention，用于执行多模态融合。实验结果表明，TACOformer 方法在多模态情感识别任务上具有先进性能。

    Recently, emotion recognition based on physiological signals has emerged as a field with intensive research. The utilization of multi-modal, multi-channel physiological signals has significantly improved the performance of emotion recognition systems, due to their complementarity. However, effectively integrating emotion-related semantic information from different modalities and capturing inter-modal dependencies remains a challenging issue. Many existing multimodal fusion methods ignore either token-to-token or channel-to-channel correlations of multichannel signals from different modalities, which limits the classification capability of the models to some extent. In this paper, we propose a comprehensive perspective of multimodal fusion that integrates channel-level and token-level cross-modal interactions. Specifically, we introduce a unified cross attention module called Token-chAnnel COmpound (TACO) Cross Attention to perform multimodal fusion, which simultaneously models channel-
    
[^7]: NetBooster：站在深度巨人的肩膀上，赋能微小深度学习

    NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders of Deep Giants. (arXiv:2306.13586v1 [cs.LG])

    [http://arxiv.org/abs/2306.13586](http://arxiv.org/abs/2306.13586)

    NetBooster是一种增强微小神经网络架构的框架，旨在赋能微小深度学习，实验证明其优于现有的微小深度学习解决方案。

    

    微小深度学习因在众多物联网设备上部署深度学习的巨大需求而备受关注。然而，由于微小神经网络（TNNs）的有限模型容量，使得在大规模数据集和下游任务上释放微小深度学习的全部潜力仍然具有挑战性，因为此会引起欠拟合问题。为此，我们提出了一种名为NetBooster的框架，通过一种扩展，然后缩小的策略，增强TNN的架构，从而赋能微小深度学习。大量实验表明，NetBooster始终优于最先进的微小深度学习解决方案。

    Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the architectures of TNNs via an expansion-then-contraction strategy. Extensive experiments show that NetBooster consistently outperforms state-of-the-art tiny deep learning solutions.
    
[^8]: 生成对抗网络的罚分梯度归一化方法

    Penalty Gradient Normalization for Generative Adversarial Networks. (arXiv:2306.13576v1 [cs.CV])

    [http://arxiv.org/abs/2306.13576](http://arxiv.org/abs/2306.13576)

    本文提出一种名为罚分梯度归一化的方法，用于解决生成对抗网络（GANs）训练不稳定的问题。新方法仅对鉴别器函数施加罚分梯度范数约束，提高了鉴别器的容量，并且可以应用于不同的GAN体系结构。实验结果表明，使用罚分梯度归一化进行训练的GAN优于现有方法，无论是在Frechet Inception距离还是Inception Score上都有同样的表现。

    

    本文提出一种罚分梯度归一化（PGN）的新的归一化方法来解决生成对抗网络（GANs）训练不稳定的问题，该问题是由尖锐的梯度空间引起的。与梯度惩罚和谱归一化等现有方法不同，所提出的PGN仅对鉴别器函数施加罚分梯度范数约束，从而提高了鉴别器的容量。此外，所提出的罚分梯度归一化方法可以应用于不同的GAN体系结构，只需进行少量修改。在三个数据集的大量实验中，使用罚分梯度归一化训练的GAN优于现有方法，无论是在Frechet Inception距离还是Inception Score上都有同样的表现。

    In this paper, we propose a novel normalization method called penalty gradient normalization (PGN) to tackle the training instability of Generative Adversarial Networks (GANs) caused by the sharp gradient space. Unlike existing work such as gradient penalty and spectral normalization, the proposed PGN only imposes a penalty gradient norm constraint on the discriminator function, which increases the capacity of the discriminator. Moreover, the proposed penalty gradient normalization can be applied to different GAN architectures with little modification. Extensive experiments on three datasets show that GANs trained with penalty gradient normalization outperform existing methods in terms of both Frechet Inception and Distance and Inception Score.
    
[^9]: MLP的规模化：归纳偏差的故事

    Scaling MLPs: A Tale of Inductive Bias. (arXiv:2306.13575v1 [cs.LG])

    [http://arxiv.org/abs/2306.13575](http://arxiv.org/abs/2306.13575)

    本文研究了多层感知器（MLP）在视觉任务中的性能极限，并探讨了MLP相较于其他深度学习模型的归纳偏差，旨在推进深度学习理论和实践的结合。

    

    在此工作中，我们重新审视了深度学习中最基本的构建块——多层感知器（MLP），并研究了它在视觉任务中的性能极限。MLP的实验性洞见在多个方面都非常重要。

    In this work we revisit the most fundamental building block in deep learning, the multi-layer perceptron (MLP), and study the limits of its performance on vision tasks. Empirical insights into MLPs are important for multiple reasons. (1) Given the recent narrative "less inductive bias is better", popularized due to transformers eclipsing convolutional models, it is natural to explore the limits of this hypothesis. To that end, MLPs offer an ideal test bed, being completely free of any inductive bias. (2) MLPs have almost exclusively been the main protagonist in the deep learning theory literature due to their mathematical simplicity, serving as a proxy to explain empirical phenomena observed for more complex architectures. Surprisingly, experimental datapoints for MLPs are very difficult to find in the literature, especially when coupled with large pre-training protocols. This discrepancy between practice and theory is worrying: Do MLPs reflect the empirical advances exhibited by pract
    
[^10]: 安全模式修剪下的预测模式挖掘模型高效选取

    Efficient Model Selection for Predictive Pattern Mining Model by Safe Pattern Pruning. (arXiv:2306.13561v1 [stat.ML])

    [http://arxiv.org/abs/2306.13561](http://arxiv.org/abs/2306.13561)

    本文提出了“安全模式修剪”方法解决预测模式挖掘中模式数量增长的问题，并展示了其在结构化数据的回归和分类问题中的有效性。

    

    预测模式挖掘是一种用于构建预测模型的方法，当输入被表示为结构化数据（例如集合、图和序列）时，该方法适用。预测模式挖掘的主要思想是通过考虑结构化数据中的子结构（例如子集、子图和子序列，称为模式）作为模型的特征来构建预测模型。预测模式挖掘面临的主要挑战在于随着结构化数据的复杂性增加，模式数量呈指数级增长。在本研究中，我们提出了安全模式修剪（SPP）方法来解决预测模式挖掘中模式数爆炸的问题。我们还讨论了如何在实际数据分析的整个模型构建过程中有效地应用它。为了证明所提出的方法的有效性，我们在涉及集合、图和序列的回归和分类问题上进行了数值实验。

    Predictive pattern mining is an approach used to construct prediction models when the input is represented by structured data, such as sets, graphs, and sequences. The main idea behind predictive pattern mining is to build a prediction model by considering substructures, such as subsets, subgraphs, and subsequences (referred to as patterns), present in the structured data as features of the model. The primary challenge in predictive pattern mining lies in the exponential growth of the number of patterns with the complexity of the structured data. In this study, we propose the Safe Pattern Pruning (SPP) method to address the explosion of pattern numbers in predictive pattern mining. We also discuss how it can be effectively employed throughout the entire model building process in practical data analysis. To demonstrate the effectiveness of the proposed method, we conduct numerical experiments on regression and classification problems involving sets, graphs, and sequences.
    
[^11]: 多模态大语言模型综述

    A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])

    [http://arxiv.org/abs/2306.13549](http://arxiv.org/abs/2306.13549)

    本文追踪和总结了多模态大语言模型（MLLM）的最新进展，包括多模态指令调整、多模态上下文学习、多模态思维链和LLM辅助视觉推理等应用，指出了现有挑战和有前途的研究方向。

    

    多模态大语言模型（MLLM）是一种新兴的研究热点，使用强大的大语言模型作为大脑执行多模态任务。MLLM 的惊人能力，如基于图像编写故事和无OCR数学推理等，在传统方法中很少见，表明了通向人工智能的潜在路径。本文旨在追踪和总结 MLLM 的最新进展。首先，我们介绍了 MLLM 的构成，概述了相关概念。然后，讨论了关键技术和应用，包括多模态指令调整（M-IT）、多模态上下文学习（M-ICL）、多模态思维链（M-CoT）和LLM辅助视觉推理（LAVR）。最后，我们讨论了现有的挑战，并指出了有前途的研究方向。鉴于 MLLM 时代才刚刚开始，我们会不断更新这个综述，并希望能激发更多的研究。

    Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
    
[^12]: 带变分李群算子的多概念对比学习

    Manifold Contrastive Learning with Variational Lie Group Operators. (arXiv:2306.13544v1 [cs.LG])

    [http://arxiv.org/abs/2306.13544](http://arxiv.org/abs/2306.13544)

    本文提出了一种使用李群算子建模流形的对比学习方法，通过稀疏促进先验来参数化系数。这种方法可以提供可应用的特征增强来学习可转移的特征表示。

    

    自监督学习已成为深度神经网络学习可转移表示的流行范式。与生物视觉的腹侧通路模型类似，观察到这些网络导致了倒数第二层表示中类别流形的分离。尽管这种观察符合表示学习的流形假设，但目前的自监督方法在明确建模这个流形方面存在局限性。实际上，这些方法在学习过程中往往仅应用来自预先指定的“正对比对”集合的增强。在这项工作中，我们提出了一种对比学习方法，使用由稀疏促进先验的系数参数化的李群算子来直接建模潜在流形。这些系数的变分分布提供了流形的生成模型，其中的样本提供了可应用的特征增强。

    Self-supervised learning of deep neural networks has become a prevalent paradigm for learning representations that transfer to a variety of downstream tasks. Similar to proposed models of the ventral stream of biological vision, it is observed that these networks lead to a separation of category manifolds in the representations of the penultimate layer. Although this observation matches the manifold hypothesis of representation learning, current self-supervised approaches are limited in their ability to explicitly model this manifold. Indeed, current approaches often only apply augmentations from a pre-specified set of "positive pairs" during learning. In this work, we propose a contrastive learning approach that directly models the latent manifold using Lie group operators parameterized by coefficients with a sparsity-promoting prior. A variational distribution over these coefficients provides a generative model of the manifold, with samples which provide feature augmentations applica
    
[^13]: 扭曲图神经网络

    Torsion Graph Neural Networks. (arXiv:2306.13541v1 [cs.LG])

    [http://arxiv.org/abs/2306.13541](http://arxiv.org/abs/2306.13541)

    这项研究提出了一种名为TorGNN的图神经网络模型，其运用解析扭曲度量化图的局部结构，并在16种不同类型的网络链接预测任务上进行了验证。

    

    几何深度学习（GDL）模型已经展现了非欧几里得数据分析的巨大潜力。它们被开发出来将非欧几里得数据的几何和拓扑信息整合到端到端深度学习架构中。受图神经网络（GNN）中离散黎曼曲率的成功启发，我们提出了TorGNN，一种增强了解析扭曲度的图神经网络模型。其基本思想是用基于解析扭曲度的权重公式来表征图的局部结构。数学上，解析扭曲度是一种拓扑不变量，可以区分同伦但不同胚的空间。在我们的TorGNN中，对于每个边，我们都可以找到相应的局部单纯复合体，然后计算其解析扭曲度，并进一步将其用作信息传递过程的权重。我们的TorGNN模型已在来自16种不同类型网络的链接预测任务上进行了验证。

    Geometric deep learning (GDL) models have demonstrated a great potential for the analysis of non-Euclidian data. They are developed to incorporate the geometric and topological information of non-Euclidian data into the end-to-end deep learning architectures. Motivated by the recent success of discrete Ricci curvature in graph neural network (GNNs), we propose TorGNN, an analytic Torsion enhanced Graph Neural Network model. The essential idea is to characterize graph local structures with an analytic torsion based weight formula. Mathematically, analytic torsion is a topological invariant that can distinguish spaces which are homotopy equivalent but not homeomorphic. In our TorGNN, for each edge, a corresponding local simplicial complex is identified, then the analytic torsion (for this local simplicial complex) is calculated, and further used as a weight (for this edge) in message-passing process. Our TorGNN model is validated on link prediction tasks from sixteen different types of n
    
[^14]: 关于随机旋转高斯化的收敛速度。

    On the Convergence Rate of Gaussianization with Random Rotations. (arXiv:2306.13520v1 [cs.LG])

    [http://arxiv.org/abs/2306.13520](http://arxiv.org/abs/2306.13520)

    该论文通过理论推导和实验验证，发现高斯化模型的收敛速度对于维数增加呈现出线性关系，原因是模型无法捕捉维之间的相关性。

    

    高斯化是一种简单的生成模型，可以在没有反向传播的情况下进行训练。它在低维数据上表现出了强大的性能。然而，随着维度的增加，观察到其收敛速度变慢。我们从理论上证明，对于高斯输入，所需的层数与维数成线性关系。我们认为这是因为该模型无法捕捉维之间的相关性。

    Gaussianization is a simple generative model that can be trained without backpropagation. It has shown compelling performance on low dimensional data. As the dimension increases, however, it has been observed that the convergence speed slows down. We show analytically that the number of required layers scales linearly with the dimension for Gaussian input. We argue that this is because the model is unable to capture dependencies between dimensions. Empirically, we find the same linear increase in cost for arbitrary input $p(x)$, but observe favorable scaling for some distributions. We explore potential speed-ups and formulate challenges for further research.
    
[^15]: 二进制域广义化：用于 BNN 稀疏化的方法

    Binary domain generalization for sparsifying binary neural networks. (arXiv:2306.13515v1 [cs.LG])

    [http://arxiv.org/abs/2306.13515](http://arxiv.org/abs/2306.13515)

    该研究提出了一种新的更一般的二进制域，更能抵御稀疏化技术，从而保证了改进的压缩和避免了严重的性能损失。

    

    二进制神经网络 (BNN) 是在资源受限设备上开发和部署基于深度神经网络的应用的有效解决方案。尽管其成功，但 BNN 仍然受到固定和有限的压缩因子的影响，这可能是由于现有的全精度 DNN 稀疏化方法不能直接应用于 BNN。事实上，对 BNN 进行权重稀疏化会导致性能下降，这表明 BNN 的标准二进制化域并不适用于该任务。本研究提出了一种新的更一般的二进制域，扩展了标准二进制域，更能抵御稀疏化技术，从而保证了改进的压缩和避免了严重的性能损失。我们演示了一个将全精度网络的权重量化到所提出的二进制域的闭式解。最后，我们展示了我们的方法的灵活性，可以与其他稀疏化策略相结合。在 CIFAR-10 数据集上进行了实验。

    Binary neural networks (BNNs) are an attractive solution for developing and deploying deep neural network (DNN)-based applications in resource constrained devices. Despite their success, BNNs still suffer from a fixed and limited compression factor that may be explained by the fact that existing pruning methods for full-precision DNNs cannot be directly applied to BNNs. In fact, weight pruning of BNNs leads to performance degradation, which suggests that the standard binarization domain of BNNs is not well adapted for the task. This work proposes a novel more general binary domain that extends the standard binary one that is more robust to pruning techniques, thus guaranteeing improved compression and avoiding severe performance losses. We demonstrate a closed-form solution for quantizing the weights of a full-precision network into the proposed binary domain. Finally, we show the flexibility of our method, which can be combined with other pruning strategies. Experiments over CIFAR-10 
    
[^16]: DISCO-10M：一个大规模的音乐数据集

    DISCO-10M: A Large-Scale Music Dataset. (arXiv:2306.13512v1 [cs.SD])

    [http://arxiv.org/abs/2306.13512](http://arxiv.org/abs/2306.13512)

    DISCO-10M是一个新颖而广泛的音乐数据集，其规模超过了之前最大的可用音乐数据集一个数量级，提供高质量的音频资源和CLAP嵌入以加速机器学习在音乐领域的研究和应用。

    

    音乐数据集在推动音乐机器学习研究方面发挥着至关重要的作用。然而，现有音乐数据集存在规模有限、可访问性差和缺乏音频资源等缺点。为了解决这些问题，我们提出了DISCO-10M，这是一个新颖而广泛的音乐数据集，其规模超过了之前最大的可用音乐数据集一个数量级。为确保高质量的数据，我们实施了一个多阶段过滤流程。这个过程结合了基于文本描述和音频嵌入的相似性。此外，我们提供了预计算的CLAP嵌入和DISCO-10M，便于直接应用于各种下游任务。这些嵌入使得对所提供数据的机器学习应用的高效探索成为可能。我们旨在通过DISCO-10M，推动音乐机器学习模型开发的新研究，以实现民主化和促进发展。

    Music datasets play a crucial role in advancing research in machine learning for music. However, existing music datasets suffer from limited size, accessibility, and lack of audio resources. To address these shortcomings, we present DISCO-10M, a novel and extensive music dataset that surpasses the largest previously available music dataset by an order of magnitude. To ensure high-quality data, we implement a multi-stage filtering process. This process incorporates similarities based on textual descriptions and audio embeddings. Moreover, we provide precomputed CLAP embeddings alongside DISCO-10M, facilitating direct application on various downstream tasks. These embeddings enable efficient exploration of machine learning applications on the provided data. With DISCO-10M, we aim to democratize and facilitate new research to help advance the development of novel machine learning models for music.
    
[^17]: 基于分布数据集的主成分分析的两种推导方法

    Two derivations of Principal Component Analysis on datasets of distributions. (arXiv:2306.13503v1 [stat.ML])

    [http://arxiv.org/abs/2306.13503](http://arxiv.org/abs/2306.13503)

    本篇论文提出了一个新的方法来进行基于分布的主成分分析，通过方差最大化原理和重构误差最小化两种方法推导得闭合解。

    

    在本短文中，我们将主成分分析（PCA）应用于由其位置和协方差刻画的分布数据集，而非点数据集。与点数据集上的常规PCA可以等效地通过方差最大化原理和重构error最小化两种方法推导出一样，我们从这两个方面推导出了分布PCA的闭合解。

    In this brief note, we formulate Principal Component Analysis (PCA) over datasets consisting not of points but of distributions, characterized by their location and covariance. Just like the usual PCA on points can be equivalently derived via a variance-maximization principle and via a minimization of reconstruction error, we derive a closed-form solution for distributional PCA from both of these perspectives.
    
[^18]: 基于级联子空间聚类的异常检测

    Cascade Subspace Clustering for Outlier Detection. (arXiv:2306.13500v1 [cs.CV])

    [http://arxiv.org/abs/2306.13500](http://arxiv.org/abs/2306.13500)

    本文提出了一种基于级联子空间聚类和梯度提升的异常检测框架，能够将多个弱的异常检测器组合成强的异常检测器，实现更准确的异常检测。

    

    许多基于稀疏和低秩表示的方法已被开发，以保证正确的异常检测。自表示表明，子空间中的一个点总可以表示为子空间中其他点的线性组合。可以在自表示上定义一个适当的马尔可夫链，从而使我们能够识别内点和外点之间的区别。然而，自表示的重构误差仍然可以用于检测异常，但被忽略了。本文受梯度提升的启发，提出了一种新的异常检测框架，通过构建多次自表示的迭代方式，将一系列弱“异常检测器”组合成单个强异常检测器。在每个阶段，我们基于弹性网构造自表示，并在其上定义适当的马尔可夫链来检测异常。自表示的残差用于下一阶段学习下一个较弱的异常检测器。

    Many methods based on sparse and low-rank representation been developed along with guarantees of correct outlier detection. Self-representation states that a point in a subspace can always be expressed as a linear combination of other points in the subspace. A suitable Markov Chain can be defined on the self-representation and it allows us to recognize the difference between inliers and outliers. However, the reconstruction error of self-representation that is still informative to detect outlier detection, is neglected.Inspired by the gradient boosting, in this paper, we propose a new outlier detection framework that combines a series of weak "outlier detectors" into a single strong one in an iterative fashion by constructing multi-pass self-representation. At each stage, we construct a self-representation based on elastic-net and define a suitable Markov Chain on it to detect outliers. The residual of the self-representation is used for the next stage to learn the next weaker outlier 
    
[^19]: 基于特征重要性的Boost invariance符号观测量的检索

    Retrieval of Boost Invariant Symbolic Observables via Feature Importance. (arXiv:2306.13496v1 [physics.comp-ph])

    [http://arxiv.org/abs/2306.13496](http://arxiv.org/abs/2306.13496)

    本文提出一种替代深度学习的Boost不变多项式方法，实现了直接分析最重要特征的有效分类，同时加速算法执行，性能接近于使用完整信息的算法。

    

    在高能物理的喷注标签中，深度学习方法被描述为黑盒子，它们处理大量信息，很难提取关键的区别观测量。在本文中，我们提出了一种替代深度学习方法的Boost不变多项式方法，它能直接分析表示给定任务中最重要特征的简单分析表达式。此外，我们展示了这种方法如何提供一个极低维的分类器，具有代表有效区别物理相关观测量的最小特征集，并如何因此加速算法执行，相对接近于使用完整信息的算法的性能。

    Deep learning approaches for jet tagging in high-energy physics are characterized as black boxes that process a large amount of information from which it is difficult to extract key distinctive observables. In this proceeding, we present an alternative to deep learning approaches, Boost Invariant Polynomials, which enables direct analysis of simple analytic expressions representing the most important features in a given task. Further, we show how this approach provides an extremely low dimensional classifier with a minimum set of features representing %effective discriminating physically relevant observables and how it consequently speeds up the algorithm execution, with relatively close performance to the algorithm using the full information.
    
[^20]: 一种用于模拟电路验证的自适应规划搜索算法

    Adaptive Planning Search Algorithm for Analog Circuit Verification. (arXiv:2306.13484v1 [cs.AI])

    [http://arxiv.org/abs/2306.13484](http://arxiv.org/abs/2306.13484)

    提出了一种机器学习（ML）方法用于模拟电路验证，在使用较少的仿真次数的同时提高了电路响应估计的精度，能够更好地发现最坏情况和故障。

    

    集成电路验证近年来引起了相当大的关注。由于这些电路每年都在不断增长复杂性，因此Si前验证变得越来越重要，以确保其正常功能。因此，为了减少手动验证IC所需的时间，我们提出了一种机器学习（ML）方法，该方法使用较少的仿真次数。该方法依赖于一组操作条件配置（OCC），以训练高斯过程（GP）替代模型。通过使用替代模型，我们可以提出进一步更困难的OCC。对于几个迭代重复这个过程已经在合成和真实电路上表现出更好的GP估计电路响应，提高了找到某些电路响应的最坏情况甚至使之失败的机会。因此，我们展示了这种提出的方法能够为所有电路提供更接近规格的OCC，并确定一种故障。

    Integrated circuit verification has gathered considerable interest in recent times. Since these circuits keep growing in complexity year by year, pre-Silicon (pre-SI) verification becomes ever more important, in order to ensure proper functionality. Thus, in order to reduce the time needed for manually verifying ICs, we propose a machine learning (ML) approach, which uses less simulations. This method relies on an initial evaluation set of operating condition configurations (OCCs), in order to train Gaussian process (GP) surrogate models. By using surrogate models, we can propose further, more difficult OCCs. Repeating this procedure for several iterations has shown better GP estimation of the circuit's responses, on both synthetic and real circuits, resulting in a better chance of finding the worst case, or even failures, for certain circuit responses. Thus, we show that the proposed approach is able to provide OCCs closer to the specifications for all circuits and identify a failure 
    
[^21]: 高效的深度神经网络在线处理

    Efficient Online Processing with Deep Neural Networks. (arXiv:2306.13474v1 [cs.LG])

    [http://arxiv.org/abs/2306.13474](http://arxiv.org/abs/2306.13474)

    该论文致力于提高神经网络的效率，提出并研究了连续推断网络（CINs）的概念，重点讨论在线推理的效率问题。

    

    深度神经网络的能力和应用正在以惊人的速度增长：视觉模型准确分类人类在视频中的动作，并像人类专家一样准确地识别医学扫描中的癌细胞组织；大型语言模型可以回答各种问题、生成代码和写作，成为日常餐桌谈论的主题。尽管它们的用途令人振奋，但不断增加的模型大小和计算复杂度也有负面影响。训练和服务模型的经济成本和负面环境外部性与财务可行性和气候行动目标不相协调。本论文致力于提高神经网络的效率，而不是追求预测性能的进一步提高。具体地，本论文的一个核心贡献是解决了在线推断过程中的效率问题。在这里，提出并研究了连续推断网络（CINs）的概念。

    The capabilities and adoption of deep neural networks (DNNs) grow at an exhilarating pace: Vision models accurately classify human actions in videos and identify cancerous tissue in medical scans as precisely than human experts; large language models answer wide-ranging questions, generate code, and write prose, becoming the topic of everyday dinner-table conversations. Even though their uses are exhilarating, the continually increasing model sizes and computational complexities have a dark side. The economic cost and negative environmental externalities of training and serving models is in evident disharmony with financial viability and climate action goals.  Instead of pursuing yet another increase in predictive performance, this dissertation is dedicated to the improvement of neural network efficiency. Specifically, a core contribution addresses the efficiency aspects during online inference. Here, the concept of Continual Inference Networks (CINs) is proposed and explored across fo
    
[^22]: 基于高维观测数据的潜在子群转换下的预测研究

    Prediction under Latent Subgroup Shifts with High-Dimensional Observations. (arXiv:2306.13472v1 [stat.ML])

    [http://arxiv.org/abs/2306.13472](http://arxiv.org/abs/2306.13472)

    本研究运用RPM在图像观测数据中识别低维离散隐变量，并且在潜在变量分布不同的情况下适当地预测目标结果。

    

    本研究提出了一种适用于图形模型中的预测方法，能够适应潜在变量不同分布下的转换，即源环境和目标环境中的潜在变量分布不同。本研究运用识别参数模型（RPM）在图像观测数据中识别低维离散隐变量，并且在具体问题中适当地预测目标结果。

    We introduce a new approach to prediction in graphical models with latent-shift adaptation, i.e., where source and target environments differ in the distribution of an unobserved confounding latent variable. Previous work has shown that as long as "concept" and "proxy" variables with appropriate dependence are observed in the source environment, the latent-associated distributional changes can be identified, and target predictions adapted accurately. However, practical estimation methods do not scale well when the observations are complex and high-dimensional, even if the confounding latent is categorical. Here we build upon a recently proposed probabilistic unsupervised learning framework, the recognition-parametrised model (RPM), to recover low-dimensional, discrete latents from image observations. Applied to the problem of latent shifts, our novel form of RPM identifies causal latent structure in the source environment, and adapts properly to predict in the target. We demonstrate re
    
[^23]: 理解量子机器学习需要重新思考泛化问题

    Understanding quantum machine learning also requires rethinking generalization. (arXiv:2306.13461v1 [quant-ph])

    [http://arxiv.org/abs/2306.13461](http://arxiv.org/abs/2306.13461)

    本文通过实验认为，传统方法无法解释量子机器学习模型在只使用少量数据训练的情况下表现出成功的泛化性能，该模型可以准确拟合随机状态及随机标记的训练数据，这种记忆随机数据的能力违反了当前小泛化误差的概念，我们通过理论构建补充实证结果，表明量子神经网络可将任意标记拟合到量子状态上，暗示了它们的记忆能力，这些结果排除了单单基于经典复杂度度量的所有可能保证。

    

    量子机器学习模型在只用少量数据训练的情况下也能表现出成功的泛化性能。本文通过系统的随机化实验，展示传统的理解泛化的方法无法解释这些量子模型的行为。我们的实验揭示了最先进的量子神经网络能够准确地拟合随机状态和随机训练数据的标记。这种记忆随机数据的能力违反了当前小泛化误差的概念，使得建立在VC维、Rademacher复杂度和所有均匀相关性度量基础上的方法有些棘手。我们还通过理论构建补充了我们的实证结果，表明量子神经网络能够将任意标记拟合到量子状态上，暗示了它们的记忆能力。我们的结果并不排除只用少量训练数据就能获得良好泛化的可能性，但是排除了单单基于经典复杂度度量的所有可能保证。

    Quantum machine learning models have shown successful generalization performance even when trained with few data. In this work, through systematic randomization experiments, we show that traditional approaches to understanding generalization fail to explain the behavior of such quantum models. Our experiments reveal that state-of-the-art quantum neural networks accurately fit random states and random labeling of training data. This ability to memorize random data defies current notions of small generalization error, problematizing approaches that build on complexity measures such as the VC dimension, the Rademacher complexity, and all their uniform relatives. We complement our empirical results with a theoretical construction showing that quantum neural networks can fit arbitrary labels to quantum states, hinting at their memorization ability. Our results do not preclude the possibility of good generalization with few training data but rather rule out any possible guarantees based only
    
[^24]: 利用气象和昆虫学数据改善泰米尔纳德邦登革热疫情预测

    Enhanced Dengue Outbreak Prediction in Tamilnadu using Meteorological and Entomological data. (arXiv:2306.13456v1 [cs.LG])

    [http://arxiv.org/abs/2306.13456](http://arxiv.org/abs/2306.13456)

    本文研究了气候和媒介物幼虫指数对泰米尔纳德邦登革热疫情的影响，并通过引入蚊子幼虫指数提高了疫情预测的精确度。

    

    本文研究气候数据和媒介物幼虫指数对登革热疫情爆发的影响。在比较了各种LSTM模型后，选择双向堆叠LSTM网络，分析收集自2014年至2020年印度泰米尔纳德邦的时间序列气候数据和卫生数据。通过包含蚊子幼虫指数来提高模型的预测准确性，这是衡量疾病防控措施的指标。

    This paper focuses on studying the impact of climate data and vector larval indices on dengue outbreak. After a comparative study of the various LSTM models, Bidirectional Stacked LSTM network is selected to analyze the time series climate data and health data collected for the state of Tamil Nadu (India), for the period 2014 to 2020. Prediction accuracy of the model is significantly improved by including the mosquito larval index, an indication of VBD control measure.
    
[^25]: 轨迹采样下的神经网络集合小批量训练

    Minibatch training of neural network ensembles via trajectory sampling. (arXiv:2306.13442v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2306.13442](http://arxiv.org/abs/2306.13442)

    本文介绍了一种轨迹采样下的神经网络集合小批量训练方法，通过对MNIST数据集实验，发现相较于传统方法，该方法提高了两个数量级的计算效率，同时还提高了推断准确性。

    

    大多数迭代神经网络训练方法使用数据的小随机子集（或小批量）的损失函数估计来更新参数，在训练时间与庞大的训练数据集大小之间解耦，提高了效率。我们展示了一种小批量方法可以以高效的方式通过轨迹方法训练神经网络集合(NNEs)。我们通过训练NNE来分类MNIST数据集中的图像来说明这种方法。这种方法可以提高训练时间，使其能够缩放为数据集大小与平均小批量大小之比，对于MNIST来说，计算效率通常提高两个数量级。我们强调使用较长的轨迹来表示NNE的优点，既可以提高推断的准确性，又可以在小批量更新所需的样本方面降低更新成本。

    Most iterative neural network training methods use estimates of the loss function over small random subsets (or minibatches) of the data to update the parameters, which aid in decoupling the training time from the (often very large) size of the training datasets. Here, we show that a minibatch approach can also be used to train neural network ensembles (NNEs) via trajectory methods in a highly efficent manner. We illustrate this approach by training NNEs to classify images in the MNIST datasets. This method gives an improvement to the training times, allowing it to scale as the ratio of the size of the dataset to that of the average minibatch size which, in the case of MNIST, gives a computational improvement typically of two orders of magnitude. We highlight the advantage of using longer trajectories to represent NNEs, both for improved accuracy in inference and reduced update cost in terms of the samples needed in minibatch updates.
    
[^26]: 在数据质量和公平之间权衡价格，实现在线分配公平性

    Trading-off price for data quality to achieve fair online allocation. (arXiv:2306.13440v1 [cs.LG])

    [http://arxiv.org/abs/2306.13440](http://arxiv.org/abs/2306.13440)

    本文提出了一种多臂老虎机算法，该算法权衡了价格和数据质量，以解决在线分配问题中的公平性，使得对于源的选择，算法可以适应各种公平概念，具有一定的代价可以减少公平惩罚，并且证明了算法具有较小的遗憾界。

    

    本论文探讨了在线分配问题，其中包含了长期的公平惩罚。但与现有的研究不同的是，我们不假设决策者可以观察到受保护的属性——这在实践中经常是不现实的。相反，他们可以购买来自不同质量来源的数据以帮助估计它们，从而以一定的代价减少公平惩罚。我们将这个问题建模为多臂老虎机问题，其中每个臂对应于数据源的选择，同时包含在线分配问题。我们提出了一种算法，同时解决了两个问题，并证明它具有 $\mathcal{O}(\sqrt{T})$ 的遗憾界。一个关键困难是选择源所获得的回报由公平惩罚相关，这导致尽管是随机设置，但需要进行随机化。我们的算法考虑到源选择之前可用的上下文信息，并可以适应许多不同的公平概念。我们还展示了我们的算法在实验中的有效性。

    We consider the problem of online allocation subject to a long-term fairness penalty. Contrary to existing works, however, we do not assume that the decision-maker observes the protected attributes -- which is often unrealistic in practice. Instead they can purchase data that help estimate them from sources of different quality; and hence reduce the fairness penalty at some cost. We model this problem as a multi-armed bandit problem where each arm corresponds to the choice of a data source, coupled with the online allocation problem. We propose an algorithm that jointly solves both problems and show that it has a regret bounded by $\mathcal{O}(\sqrt{T})$. A key difficulty is that the rewards received by selecting a source are correlated by the fairness penalty, which leads to a need for randomization (despite a stochastic setting). Our algorithm takes into account contextual information available before the source selection, and can adapt to many different fairness notions. We also sho
    
[^27]: 基于加权自编码器的下行NOMA星座设计方法

    A Weighted Autoencoder-Based Approach to Downlink NOMA Constellation Design. (arXiv:2306.13423v1 [eess.SP])

    [http://arxiv.org/abs/2306.13423](http://arxiv.org/abs/2306.13423)

    通过在深度自编码器训练中引入加权损失函数，本文提出了一种新的下行非正交多址接入星座设计方法，可以灵活地调整星座设计从而平衡不同用户的误码率。

    

    采用深度自编码器(AE)进行通信系统端到端设计因其灵活性和卓越性能而备受关注。除了单用户传输之外，最近还在多用户设置中探索了AE-based设计，例如，用于设计非正交多址接入(NOMA)的星座。本文通过在AE训练中引入加权损失函数来进一步推进基于AE的下行NOMA设计。通过改变权重系数，可以灵活地调整星座设计，平衡不同用户的误码率，而无需依赖于显式的有关其信道质量的信息。结合SICNet解码器，我们展示了采用所提出的加权AE框架可以实现显著的性能提高和不同用户误码率的灵活控制。

    End-to-end design of communication systems using deep autoencoders (AEs) is gaining attention due to its flexibility and excellent performance. Besides single-user transmission, AE-based design is recently explored in multi-user setup, e.g., for designing constellations for non-orthogonal multiple access (NOMA). In this paper, we further advance the design of AE-based downlink NOMA by introducing weighted loss function in the AE training. By changing the weight coefficients, one can flexibly tune the constellation design to balance error probability of different users, without relying on explicit information about their channel quality. Combined with the SICNet decoder, we demonstrate a significant improvement in achievable levels and flexible control of error probability of different users using the proposed weighted AE-based framework.
    
[^28]: CLUE: 离线强化学习的校准潜在导向

    CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning. (arXiv:2306.13412v1 [cs.LG])

    [http://arxiv.org/abs/2306.13412](http://arxiv.org/abs/2306.13412)

    CLUE使用条件可变自编码器实现专家数据的内在奖励，消除了离线强化学习中其它繁重的外在奖励工作。

    

    离线强化学习旨在从预先收集和标记的数据集中学习最优策略，消除了在线强化学习中耗时的数据收集过程。但是，离线强化学习仍然需要确定和制定每个数据转换的外在奖励，这仍然是一个繁重的工作。为了解决这个问题，我们提出了CLUE：通过使用一些专家数据为离线强化学习任务提供内在奖励来消除外在奖励的需求。为了实现这一点，我们引入了一种条件可变自编码器来学习一个潜在空间，使得内在奖励可以直接在潜在空间中进行评估。CLUE的关键思想是通过将专家数据的嵌入强制转换为校准的上下文表示，使内在奖励与专家意图保持一致。我们验证了专家驱动的内在奖励在多个环境中的有效性。

    Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected and labeled datasets, which eliminates the time-consuming data collection in online RL. However, offline RL still bears a large burden of specifying/handcrafting extrinsic rewards for each transition in the offline data. As a remedy for the labor-intensive labeling, we propose to endow offline RL tasks with a few expert data and utilize the limited expert data to drive intrinsic rewards, thus eliminating the need for extrinsic rewards. To achieve that, we introduce \textbf{C}alibrated \textbf{L}atent g\textbf{U}idanc\textbf{E} (CLUE), which utilizes a conditional variational auto-encoder to learn a latent space such that intrinsic rewards can be directly qualified over the latent space. CLUE's key idea is to align the intrinsic rewards consistent with the expert intention via enforcing the embeddings of expert data to a calibrated contextual representation. We instantiate the expert-driven intrinsic 
    
[^29]: 没有中间监督的神经算法推理

    Neural Algorithmic Reasoning Without Intermediate Supervision. (arXiv:2306.13411v1 [cs.LG])

    [http://arxiv.org/abs/2306.13411](http://arxiv.org/abs/2306.13411)

    神经算法推理最近的变革点是逐步学习算法，但我们提出了一种不需要中间监督的新方法，并在不牺牲性能的情况下规范模型的中间计算。

    

    神经算法推理是机器学习中的新兴领域，侧重于构建能够模仿经典算法（如排序、最短路径等）执行的模型。其中一个主要挑战是学习能够推广到超出分布数据且输入规模显著更大的算法。最近针对这个问题的工作表明，逐步学习算法具有优势，使模型能够访问原始算法的所有中间步骤。在这项工作中，我们不使用中间监督专注于仅从输入输出对学习神经算法推理。我们提出了简单但有效的结构改进，并构建了一种自我监督目标，可以规范模型的中间计算，而不需要访问算法轨迹。我们证明了我们的方法在来自CLRS算法的任务上与其轨迹监督对应物相当竞争力。

    Neural Algorithmic Reasoning is an emerging area of machine learning focusing on building models which can imitate the execution of classic algorithms, such as sorting, shortest paths, etc. One of the main challenges is to learn algorithms that are able to generalize to out-of-distribution data, in particular with significantly larger input sizes. Recent work on this problem has demonstrated the advantages of learning algorithms step-by-step, giving models access to all intermediate steps of the original algorithm. In this work, we instead focus on learning neural algorithmic reasoning only from the input-output pairs without appealing to the intermediate supervision. We propose simple but effective architectural improvements and also build a self-supervised objective that can regularise intermediate computations of the model without access to the algorithm trajectory. We demonstrate that our approach is competitive to its trajectory-supervised counterpart on tasks from the CLRS Algori
    
[^30]: 基于“全局”配对融合的可解释终身流学习

    Explainable Lifelong Stream Learning Based on "Glocal" Pairwise Fusion. (arXiv:2306.13410v1 [cs.LG])

    [http://arxiv.org/abs/2306.13410](http://arxiv.org/abs/2306.13410)

    本文提出了可解释终身学习模型ExLL，它具有从稀缺数据学习、自组织的原型架构、可解释的IF-THEN规则和成对融合推理等特点，适用于调整，聚类和保护流数据。

    

    移动电话，消费机器人和智能家电上使用实时设备持续学习应用程序。这些设备具有有限的处理和内存存储能力，而终身学习则在较长时间内获取数据。因此，终身学习算法必须能够在这些约束下工作，同时提供良好的性能。本研究提出了可解释的终身学习（ExLL）模型，其中包括几个重要特点：1）从稀缺的示例和资源的流数据中单次学习学习；2）自组织原型架构，根据相似性将流数据聚类到可分离的组中，并保护数据以防止灾难性遗忘；3）可解释的架构，将聚类转换为可解释的IF-THEN规则，并根据推理相似和不相似来证明模型预测；以及4）在全局和本地级别进行成对融合的推理，以增强可解释性和性能。

    Real-time on-device continual learning applications are used on mobile phones, consumer robots, and smart appliances. Such devices have limited processing and memory storage capabilities, whereas continual learning acquires data over a long period of time. By necessity, lifelong learning algorithms have to be able to operate under such constraints while delivering good performance. This study presents the Explainable Lifelong Learning (ExLL) model, which incorporates several important traits: 1) learning to learn, in a single pass, from streaming data with scarce examples and resources; 2) a self-organizing prototype-based architecture that expands as needed and clusters streaming data into separable groups by similarity and preserves data against catastrophic forgetting; 3) an interpretable architecture to convert the clusters into explainable IF-THEN rules as well as to justify model predictions in terms of what is similar and dissimilar to the inference; and 4) inferences at the glo
    
[^31]: 基于高阶模式的电网强制振荡源位置识别的时间序列分类

    Higher-order Motif-based Time Series Classification for Forced Oscillation Source Location in Power Grids. (arXiv:2306.13397v1 [cs.LG])

    [http://arxiv.org/abs/2306.13397](http://arxiv.org/abs/2306.13397)

    本研究通过基于高阶模式的时间序列分类确定强制振荡源的位置。相比传统方法，该数据驱动的无监督学习方法适用于多种FO情况，且不需要先验知识。单个和多源FO情况均可适用。在英国高压输电网上的测试表明了其有效性。

    

    时间序列模式用于发现时序数据的高阶结构。基于时间序列模式，提出了模式嵌入相关场（MECF）来刻画动力系统时间序列的高阶时间结构。应用基于MECF的无监督学习方法来确定强制振荡（FO）源的位置，该周期性干扰对电网的稳定性产生不良影响。确定FO源位置对电网稳定性至关重要。与傅里叶分析相比，基于MECF的无监督学习适用于各种FO情况，包括单个FO，共振FO和多源FO。 基于MECF的无监督学习是一种数据驱动方法，不需要任何关于系统模型或拓扑结构的先验知识。在英国高压输电网上的测试表明了基于MECF的无监督学习的有效性。此外，本研究还分析了耦合强度和测量噪声对位置判断的影响。

    Time series motifs are used for discovering higher-order structures of time series data. Based on time series motifs, the motif embedding correlation field (MECF) is proposed to characterize higher-order temporal structures of dynamical system time series. A MECF-based unsupervised learning approach is applied in locating the source of the forced oscillation (FO), a periodic disturbance that detrimentally impacts power grids. Locating the FO source is imperative for system stability. Compared with the Fourier analysis, the MECF-based unsupervised learning is applicable under various FO situations, including the single FO, FO with resonance, and multiple sources FOs. The MECF-based unsupervised learning is a data-driven approach without any prior knowledge requirement of system models or typologies. Tests on the UK high-voltage transmission grid illustrate the effectiveness of MECF-based unsupervised learning. In addition, the impacts of coupling strength and measurement noise on locati
    
[^32]: 物理信息神经网络在移动浸入边界系统建模中的应用：以波浪式翼过渡流为例

    Physics-informed neural networks modeling for systems with moving immersed boundaries: application to an unsteady flow past a plunging foil. (arXiv:2306.13395v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2306.13395](http://arxiv.org/abs/2306.13395)

    本研究探索了一种用于移动浸入边界系统的物理信息神经网络模型，该模型能够捕捉与精细IBM模拟相当的瞬态特征，并可用于实时流控和优化。此外，该研究可扩展到涉及多个移动体和流-结构相互作用的更复杂系统中。

    

    最近，物理信息神经网络（PINNs）在解决各种正向和反向问题以及在流体力学应用中促进查询应用方面得到了广泛探索。然而，仅有极少数关于PINNs用于过渡流通过动态体，例如振动翼的工作。早期的研究大部分依赖于转换为附在体上的参考系，这对于处理多个移动体或变形结构是有限制的。因此，在本研究中，探索了一种浸入式边界感知框架，用于开发过渡流通过移动物体的代理模型。具体而言，研究了从浸入边界方法（IBM）模拟数据中同时恢复压力和速度的效果。虽然，速度重建的有效性已针对精细分辨率IBM数据进行了测试，但作为进一步的步骤，恢复的压力与任意拉格朗日-欧拉（ALE）求解器的压力进行了比较。在这个框架下，所提出的PINN模型能够捕捉与精细分辨率IBM模拟相当的瞬态特征。这样的模型可用于实时流控和优化，并且本研究可扩展到涉及多个移动体和流-结构相互作用的更复杂系统中。

    Recently, physics informed neural networks (PINNs) have been explored extensively for solving various forward and inverse problems and facilitating querying applications in fluid mechanics applications. However, work on PINNs for unsteady flows past moving bodies, such as flapping wings is scarce. Earlier studies mostly relied on transferring to a body attached frame of reference which is restrictive towards handling multiple moving bodies or deforming structures. Hence, in the present work, an immersed boundary aware framework has been explored for developing surrogate models for unsteady flows past moving bodies. Specifically, simultaneous pressure recovery and velocity reconstruction from Immersed boundary method (IBM) simulation data has been investigated. While, efficacy of velocity reconstruction has been tested against the fine resolution IBM data, as a step further, the pressure recovered was compared with that of an arbitrary Lagrange Eulerian (ALE) based solver. Under this fr
    
[^33]: DiffInfinite: 通过并行随机补丁扩散在组织病理学中实现大型蒙版图像合成

    DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology. (arXiv:2306.13384v1 [eess.IV])

    [http://arxiv.org/abs/2306.13384](http://arxiv.org/abs/2306.13384)

    DiffInfinite 是一种能够生成任意大的组织学图像，使用小补丁进行快速训练，并可以更高效地并行化的方法，在组织病理学成像实践中有效解决了大规模信息、昂贵的手动注释以及保护性数据处理等独特挑战。

    

    我们提出了 DiffInfinite，这是一种层次扩散模型，可以生成任意大的组织学图像，同时保持长程相关性结构信息。我们的方法先生成合成分割掩模，随后用作高保真度生成扩散过程的条件。所提出的采样方法可以扩展到任意所需图像尺寸，而只需要小补丁进行快速训练。此外，相较于以往的大型内容生成方法，它可以更高效地并行化，同时避免平铺反射式伪影。训练利用了无分类器指导的方法，使用无标注数据扩充小型、稀疏注释的数据集。我们的方法缓解了组织病理学成像实践中的独特挑战：大规模信息、昂贵的手动注释以及保护性数据处理。DiffInfinite 数据的生物合理性由十名有经验的病理学家进行调查验证，以及下游分割任务。

    We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while preserving long-range correlation structural information. Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generative diffusion process. The proposed sampling method can be scaled up to any desired image size while only requiring small patches for fast training. Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling artefacts. The training leverages classifier-free guidance to augment a small, sparsely annotated dataset with unlabelled data. Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual annotation, and protective data handling. The biological plausibility of DiffInfinite data is validated in a survey by ten experienced pathologists as well as a downstream segmentation task. Fu
    
[^34]: 与人类合作共创全球可解释模型

    Co-creating a globally interpretable model with human input. (arXiv:2306.13381v1 [cs.HC])

    [http://arxiv.org/abs/2306.13381](http://arxiv.org/abs/2306.13381)

    该论文阐述了一种人工智能与人类协作共创联合可解释模型的方法，强调了该方法的可行性和挑战。

    

    我们考虑了一个人工智能和人类协作的合作模式，旨在生成一个联合可解释模型。该模型采用布尔决策规则的形式，人类输入以逻辑条件或部分模板的形式提供。这种聚合的模型构建方式提供了一个不同的联合决策视角。以往的努力通常集中于聚合结果而不是决策逻辑。我们通过两个例子演示了所提出的方法，并强调了这种方法的有用性和挑战。

    We consider an aggregated human-AI collaboration aimed at generating a joint interpretable model. The model takes the form of Boolean decision rules, where human input is provided in the form of logical conditions or as partial templates. This focus on the combined construction of a model offers a different perspective on joint decision making. Previous efforts have typically focused on aggregating outcomes rather than decisions logic. We demonstrate the proposed approach through two examples and highlight the usefulness and challenges of the approach.
    
[^35]: 物理约束的随机森林用于湍流模型不确定性估计

    Physics-constrained Random Forests for Turbulence Model Uncertainty Estimation. (arXiv:2306.13370v1 [cs.LG])

    [http://arxiv.org/abs/2306.13370](http://arxiv.org/abs/2306.13370)

    本文提出了一种物理约束的机器学习方法，用于考虑湍流模型的认识不确定性，同时在准确数据稀缺时能够实现前置的预测置信度估计。

    

    在实现工业设计的虚拟认证过程中，对于模拟驱动过程中不确定性的量化是至关重要的。本文讨论了一种物理约束方法来考虑湍流模型的认识不确定性。为了消除用户输入，我们结合了数据驱动的机器学习策略。此外，我们的研究重点是在准确数据稀缺时开发先验估计预测置信度。

    To achieve virtual certification for industrial design, quantifying the uncertainties in simulation-driven processes is crucial. We discuss a physics-constrained approach to account for epistemic uncertainty of turbulence models. In order to eliminate user input, we incorporate a data-driven machine learning strategy. In addition to it, our study focuses on developing an a priori estimation of prediction confidence when accurate data is scarce.
    
[^36]: 捕捉图像检索的泛化能力

    Catching Image Retrieval Generalization. (arXiv:2306.13357v1 [cs.LG])

    [http://arxiv.org/abs/2306.13357](http://arxiv.org/abs/2306.13357)

    本文通过提出一种新的测量检索性能的指标来解决广受欢迎的Recall@K度量取决于数据集中类别数量的限制，这种指标可以估计泛化能力并应用于常见的图像检索方法，提供了有关深度度量学习泛化的新见解。

    

    过拟合和泛化概念对于评估机器学习模型非常重要。本文表明，广受欢迎的Recall@K度量取决于数据集中类别数量的限制，从而限制了其评估泛化能力的能力。为解决这个问题，我们提出了一种新的测量检索性能的指标，这种指标与Recall@K不同，可以估计泛化能力。我们将这种指标应用于常见的图像检索方法，提供了有关深度度量学习泛化的新见解。

    The concepts of overfitting and generalization are vital for evaluating machine learning models. In this work, we show that the popular Recall@K metric depends on the number of classes in the dataset, which limits its ability to estimate generalization. To fix this issue, we propose a new metric, which measures retrieval performance, and, unlike Recall@K, estimates generalization. We apply the proposed metric to popular image retrieval methods and provide new insights about deep metric learning generalization.
    
[^37]: TrustGuard: 基于GNN的动态支持鲁棒且可解释的信任评估

    TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])

    [http://arxiv.org/abs/2306.13339](http://arxiv.org/abs/2306.13339)

    TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。

    

    信任评估评估实体之间的信任关系并促进决策。机器学习由于其学习能力而表现出巨大的潜力，因此对信任评估具有重要意义。近年来，作为一种新的机器学习范 paradigm，图神经网络（GNN）在处理图形数据方面表现出优越性。这激发了研究人员探索将其用于信任评估，因为实体之间的信任关系可以建模为图形。但是，使用GNN的当前信任评估方法未能完全满足信任的动态性，忽略了攻击对信任评估的不利影响，并且无法提供令人信服的评估结果解释。为解决这些问题，在本文中，我们提出了TrustGuard ：一种支持信任动态性、抗击鲁棒且通过可视化提供解释的精确信任评估模型。具体而言，TrustGuard 设计了一个由动态感知节点嵌入层、图卷积层、注意机制层和信任预测层组成的分层架构。为了评估提出的模型的有效性，我们对真实数据集进行了实验，并将TrustGuard与其他最先进的方法进行了比较。实验结果表明，TrustGuard 在准确性、鲁棒性和可解释性方面均优于其他方法。

    Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
    
[^38]: 方差-协方差正则化改进表示学习

    Variance-Covariance Regularization Improves Representation Learning. (arXiv:2306.13292v1 [cs.LG])

    [http://arxiv.org/abs/2306.13292](http://arxiv.org/abs/2306.13292)

    提出了方差-协方差正则化方法，旨在促进学习网络特征的多样性，改善表示学习和迁移学习的性能。

    

    迁移学习已成为机器学习领域的一个关键方法，能够将从一个领域获得的知识应用于提高后续任务的性能。然而，缺乏关于这些后续任务的足够信息，强有力的迁移学习方法要求在初始预训练阶段捕获各种特征。然而，最近的研究表明，在没有足够的正则化的情况下，网络往往会集中于主要减少预训练损失函数的特征。这种趋势可能导致不充分的特征学习和目标任务的受损泛化能力。为了解决这个问题，我们提出了方差-协方差正则化（VCR）技术，旨在促进学习网络特征的多样性。借鉴最近自监督学习方法的进展，我们的方法促进了表现出高方差和高相关性的学习表示。

    Transfer learning has emerged as a key approach in the machine learning domain, enabling the application of knowledge derived from one domain to improve performance on subsequent tasks. Given the often limited information about these subsequent tasks, a strong transfer learning approach calls for the model to capture a diverse range of features during the initial pretraining stage. However, recent research suggests that, without sufficient regularization, the network tends to concentrate on features that primarily reduce the pretraining loss function. This tendency can result in inadequate feature learning and impaired generalization capability for target tasks. To address this issue, we propose Variance-Covariance Regularization (VCR), a regularization technique aimed at fostering diversity in the learned network features. Drawing inspiration from recent advancements in the self-supervised learning approach, our approach promotes learned representations that exhibit high variance and 
    
[^39]: 改正策略梯度算法中折扣因子不匹配的问题

    Correcting discount-factor mismatch in on-policy policy gradient methods. (arXiv:2306.13284v1 [cs.LG])

    [http://arxiv.org/abs/2306.13284](http://arxiv.org/abs/2306.13284)

    该论文提出了一种改进的算法来解决策略梯度算法中折扣因子不匹配的问题。该算法适用于许多现有的梯度估计器，避免了性能下降的问题。

    

    策略梯度定理提供了一种方便的策略梯度形式，包括三个因素: 动作值、动作似然梯度和折扣利润。但是，基于策略梯度定理的常用的进策略方法忽略了状态分布中的折扣因子，这是技术上的错误，在某些环境下甚至可能引发退化的学习行为。既有的解决方案通过在梯度估计中使用 $\gamma^t$ 作为因子来纠正此 discrepency。然而，这种解决方案并不被广泛采用，并且在后续状态类似于前面状态的任务中表现不佳。我们引入了一种新颖的分布校正方法来解决折扣稳态分布问题，可以插入到许多现有的梯度估计器中。我们的校正方法在方差更低的情况下避免了与 $\gamma^t$ 校正相关的性能下降。

    The policy gradient theorem gives a convenient form of the policy gradient in terms of three factors: an action value, a gradient of the action likelihood, and a state distribution involving discounting called the \emph{discounted stationary distribution}. But commonly used on-policy methods based on the policy gradient theorem ignores the discount factor in the state distribution, which is technically incorrect and may even cause degenerate learning behavior in some environments. An existing solution corrects this discrepancy by using $\gamma^t$ as a factor in the gradient estimate. However, this solution is not widely adopted and does not work well in tasks where the later states are similar to earlier states. We introduce a novel distribution correction to account for the discounted stationary distribution that can be plugged into many existing gradient estimators. Our correction circumvents the performance degradation associated with the $\gamma^t$ correction with a lower variance.
    
[^40]: 关于自动磁共振影像诊断中归一化方案对输入分布转移的灵敏度和鲁棒性问题研究

    On Sensitivity and Robustness of Normalization Schemes to Input Distribution Shifts in Automatic MR Image Diagnosis. (arXiv:2306.13276v1 [eess.IV])

    [http://arxiv.org/abs/2306.13276](http://arxiv.org/abs/2306.13276)

    本研究研究了自动磁共振影像诊断中归一化方案对输入分布转移的灵敏度和鲁棒性问题，因为MRI管道内的图像重建过程对各种形式的噪声高度敏感，导致图像中出现任意的伪影，噪声分布不是固定的，而且因为DL模型对这些不同的伪影非常敏感，所以本研究是极其有意义的。

    

    磁共振成像（MRI）由于显示出优秀的软组织对比度，被认为是医学成像的黄金标准，这使得人类放射学家能够轻松地识别许多病变。最近，深度学习（DL）模型也使用这些重建图像作为输入，在诊断多种疾病方面取得了最先进的性能。然而，MRI管道内的图像重建过程需要使用复杂的硬件，并调整大量的扫描仪参数，因此对各种形式的噪声高度敏感，导致图像中出现任意的伪影。此外，噪声分布不是固定的，在机器内、机器之间和患者之间变化，导致图像中的伪影也随之不同。不幸的是，DL模型对这些不同的伪影非常敏感，因为它导致输入数据分布发生变化。

    Magnetic Resonance Imaging (MRI) is considered the gold standard of medical imaging because of the excellent soft-tissue contrast exhibited in the images reconstructed by the MRI pipeline, which in-turn enables the human radiologist to discern many pathologies easily. More recently, Deep Learning (DL) models have also achieved state-of-the-art performance in diagnosing multiple diseases using these reconstructed images as input. However, the image reconstruction process within the MRI pipeline, which requires the use of complex hardware and adjustment of a large number of scanner parameters, is highly susceptible to noise of various forms, resulting in arbitrary artifacts within the images. Furthermore, the noise distribution is not stationary and varies within a machine, across machines, and patients, leading to varying artifacts within the images. Unfortunately, DL models are quite sensitive to these varying artifacts as it leads to changes in the input data distribution between the 
    
[^41]: 持续学习能改进长尾识别吗？走向统一框架

    Can Continual Learning Improve Long-Tailed Recognition? Toward a Unified Framework. (arXiv:2306.13275v1 [cs.LG])

    [http://arxiv.org/abs/2306.13275](http://arxiv.org/abs/2306.13275)

    本文针对长尾识别问题，提出一种持续学习方法，通过将头部集和尾部集的学习视为两个独立连续的步骤，并利用定理证明持续学习可以有效地更新学习者的权重以学习尾部，同时不会忘记头部。

    

    在高度不平衡的数据集中，不同类别之间的样本数量极度失衡会出现长尾识别（LTR）问题。LTR方法旨在准确地学习包含一个较大“头”集和一个较小“尾”集的数据集。我们提出了一个定理，假设损失函数是强凸的，那么完整数据集上训练的学习者的权重在同一个学习者严格训练头集时的权重上限之内。接下来，我们声称将头集和尾集的学习视为两个独立的连续步骤，持续学习（CL）方法可以有效地更新学习者的权重以学习尾部，而不会忘记头部。首先，我们使用玩具MNIST-LT数据集验证了我们的理论发现。接着，我们在两个标准LTR基准（CIFAR100-LT和CIFAR10-L）的多个不平衡变体上评估了几种CL策略的有效性。

    The Long-Tailed Recognition (LTR) problem emerges in the context of learning from highly imbalanced datasets, in which the number of samples among different classes is heavily skewed. LTR methods aim to accurately learn a dataset comprising both a larger Head set and a smaller Tail set. We propose a theorem where under the assumption of strong convexity of the loss function, the weights of a learner trained on the full dataset are within an upper bound of the weights of the same learner trained strictly on the Head. Next, we assert that by treating the learning of the Head and Tail as two separate and sequential steps, Continual Learning (CL) methods can effectively update the weights of the learner to learn the Tail without forgetting the Head. First, we validate our theoretical findings with various experiments on the toy MNIST-LT dataset. We then evaluate the efficacy of several CL strategies on multiple imbalanced variations of two standard LTR benchmarks (CIFAR100-LT and CIFAR10-L
    
[^42]: FedSelect: 个性化联邦学习中参数自定义选择的细调方法

    FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v1 [cs.LG])

    [http://arxiv.org/abs/2306.13264](http://arxiv.org/abs/2306.13264)

    本文提出了一种名为FedSelect的新联邦学习框架，通过寻找最佳客户端子网络从而直接个性化客户端子网络结构和参数，同时保留了全局知识，提高了客户端性能。

    

    联邦学习旨在通过在本地数据上微调客户端参数或针对本地任务个性化架构来提高客户端性能。然而，现有的方法要么在牺牲重要的全局知识的情况下进行个性化，要么在预先确定网络层以进行微调的情况下导致客户端模型中全局知识储存的不足。本文提出了一种新的联邦学习框架FedSelect，通过同时搜索并获得个性化最佳参数和用于全局聚合的其余参数，从而直接个性化客户子网络结构和参数。

    Recent advancements in federated learning (FL) seek to increase client-level performance by fine-tuning client parameters on local data or personalizing architectures for the local task. Existing methods for such personalization either prune a global model or fine-tune a global model on a local client distribution. However, these existing methods either personalize at the expense of retaining important global knowledge, or predetermine network layers for fine-tuning, resulting in suboptimal storage of global knowledge within client models. Enlightened by the lottery ticket hypothesis, we first introduce a hypothesis for finding optimal client subnetworks to locally fine-tune while leaving the rest of the parameters frozen. We then propose a novel FL framework, FedSelect, using this procedure that directly personalizes both client subnetwork structure and parameters, via the simultaneous discovery of optimal parameters for personalization and the rest of parameters for global aggregatio
    
[^43]: 合成数据重排加速异构数据下联邦学习的收敛(arXiv:2306.13263v1 [cs.LG])

    Synthetic data shuffling accelerates the convergence of federated learning under data heterogeneity. (arXiv:2306.13263v1 [cs.LG])

    [http://arxiv.org/abs/2306.13263](http://arxiv.org/abs/2306.13263)

    本文提出了一种通过对本地生成的合成数据进行重排来加速异构数据下联邦学习的收敛的方法，实验表明，对合成数据进行重排可以大幅提高现有多个联邦学习算法的性能。

    

    在联邦学习中，数据异构性是一个关键的挑战。一个简单的解决方案是对客户端的数据进行洗牌，以同质化分布。然而，这可能会违反数据访问权利，而对于在何时以及如何重排可以加速联邦优化算法的收敛，目前尚未在理论上得到很好的理解。本文建立了数据异构性与收敛速率参数之间的精确可量化的对应关系，证明了重排可以按百分比平方减少梯度差异，从而加速收敛。受理论启发，我们提出了一种通过对本地生成的合成数据进行重排来解决数据访问权问题的实用方法。实验结果表明，对合成数据进行重排可以大幅提高现有多个联邦学习算法的性能。

    In federated learning, data heterogeneity is a critical challenge. A straightforward solution is to shuffle the clients' data to homogenize the distribution. However, this may violate data access rights, and how and when shuffling can accelerate the convergence of a federated optimization algorithm is not theoretically well understood. In this paper, we establish a precise and quantifiable correspondence between data heterogeneity and parameters in the convergence rate when a fraction of data is shuffled across clients. We prove that shuffling can quadratically reduce the gradient dissimilarity with respect to the shuffling percentage, accelerating convergence. Inspired by the theory, we propose a practical approach that addresses the data access rights issue by shuffling locally generated synthetic data. The experimental results show that shuffling synthetic data improves the performance of multiple existing federated learning algorithms by a large margin.
    
[^44]: 过参数化线性模型下多类分类的渐进泛化精度研究

    Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models. (arXiv:2306.13255v1 [cs.LG])

    [http://arxiv.org/abs/2306.13255](http://arxiv.org/abs/2306.13255)

    本文研究了高斯协变量下的过参数化线性模型在多类分类问题中的泛化能力，成功解决了之前的猜想，并提出的新下界具有信息论中的强对偶定理的性质。

    

    本文研究了在具有高斯协变量双层模型下，过参数化线性模型在多类分类中的渐进泛化问题，其中数据点数、特征和类别数都同时增长。我们完全解决了Subramanian等人在'22年所提出的猜想，与预测的泛化区间相匹配。此外，我们的新的下界类似于信息论中的强对偶定理：它们能够确立误分类率逐渐趋近于0或1.我们紧密的结果的一个令人惊讶的结果是，最小范数插值分类器在最小范数插值回归器最优的范围内，可以在渐进上次优。我们分析的关键在于一种新的Hanson-Wright不等式变体，该变体在具有稀疏标签的多类问题中具有广泛的适用性。作为应用，我们展示了相同类型分析在几种不同类型的分类模型上的结果。

    We study the asymptotic generalization of an overparameterized linear model for multiclass classification under the Gaussian covariates bi-level model introduced in Subramanian et al.~'22, where the number of data points, features, and classes all grow together. We fully resolve the conjecture posed in Subramanian et al.~'22, matching the predicted regimes for generalization. Furthermore, our new lower bounds are akin to an information-theoretic strong converse: they establish that the misclassification rate goes to 0 or 1 asymptotically. One surprising consequence of our tight results is that the min-norm interpolating classifier can be asymptotically suboptimal relative to noninterpolating classifiers in the regime where the min-norm interpolating regressor is known to be optimal.  The key to our tight analysis is a new variant of the Hanson-Wright inequality which is broadly useful for multiclass problems with sparse labels. As an application, we show that the same type of analysis 
    
[^45]: 提前预测理解前浪潮：研究掌握技能模型的损失函数表面

    Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok. (arXiv:2306.13253v1 [cs.LG])

    [http://arxiv.org/abs/2306.13253](http://arxiv.org/abs/2306.13253)

    本文提出了一种低成本方法来预测神经网络中的理解前浪潮，即通过研究前几轮的学习曲线来判断后续是否出现理解前浪潮。使用波形振荡和学习曲线的频谱特征值可以高精度地预测理解前浪潮。

    

    本文研究了神经网络中出现理解前浪潮的预测，该现象是完美概括在出现过拟合或记忆迹象之后很长一段时间才出现。报告称，只有在特定的超参数下才能观察到理解前浪潮。这使得确定导致理解前浪潮的参数至关重要。然而，由于理解前浪潮需要大量的迭代轮数，因此寻找导致它的超参数是很耗时的。本文提出了一种低成本方法来预测理解前浪潮，而无需训练大量的迭代次数。本文通过研究前几轮的学习曲线，展示了一种可以预测后续出现理解前浪潮的方法。具体而言，如果在前几轮中出现某些振荡，那么可以期望在模型训练更长时间后出现理解前浪潮。我们提出使用学习曲线的频谱特征值来预测理解前浪潮的概率。实验结果表明，我们的方法可以高精度地预测理解前浪潮。

    This paper focuses on predicting the occurrence of grokking in neural networks, a phenomenon in which perfect generalization emerges long after signs of overfitting or memorization are observed. It has been reported that grokking can only be observed with certain hyper-parameters. This makes it critical to identify the parameters that lead to grokking. However, since grokking occurs after a large number of epochs, searching for the hyper-parameters that lead to it is time-consuming. In this paper, we propose a low-cost method to predict grokking without training for a large number of epochs. In essence, by studying the learning curve of the first few epochs, we show that one can predict whether grokking will occur later on. Specifically, if certain oscillations occur in the early epochs, one can expect grokking to occur if the model is trained for a much longer period of time. We propose using the spectral signature of a learning curve derived by applying the Fourier transform to quant
    
[^46]: 弱混淆下的近似因果效应识别

    Approximate Causal Effect Identification under Weak Confounding. (arXiv:2306.13242v1 [stat.ML])

    [http://arxiv.org/abs/2306.13242](http://arxiv.org/abs/2306.13242)

    本文提出了一种有效的方法来在弱混淆下识别因果效应的上限和下限，并证明了这种方法的计算效率优于最先进的多项式程序。

    

    在只有观测数据可用时，许多研究人员研究了因果效应估计问题。针对可识别因果查询的点估计，已经开发出了正确完备的算法。对于不可识别的因果查询，研究人员开发了多项式程序，以估计因果效应的紧密界限。但对于支持大小较大的变量，优化这些多项式程序在计算上很困难。在本文中，我们分析了“弱混淆”对因果估计的影响。更具体地说，在未观测到的混淆变量的熵很小的假设下，我们提出了一种有效的线性规划方法来导出因果效应的上限和下限。我们证明了我们的界限是一致的，也就是说，当未观测混淆变量的熵趋近于零时，上限和下限之间的差异会消失。最后，我们进行了合成和真实数据模拟，以比较我们的方法与最先进的多项式程序得到的界限，并证明我们的方法在计算上更加高效，性能也可以达到类似的水平。

    Causal effect estimation has been studied by many researchers when only observational data is available. Sound and complete algorithms have been developed for pointwise estimation of identifiable causal queries. For non-identifiable causal queries, researchers developed polynomial programs to estimate tight bounds on causal effect. However, these are computationally difficult to optimize for variables with large support sizes. In this paper, we analyze the effect of "weak confounding" on causal estimands. More specifically, under the assumption that the unobserved confounders that render a query non-identifiable have small entropy, we propose an efficient linear program to derive the upper and lower bounds of the causal effect. We show that our bounds are consistent in the sense that as the entropy of unobserved confounders goes to zero, the gap between the upper and lower bound vanishes. Finally, we conduct synthetic and real data simulations to compare our bounds with the bounds obta
    
[^47]: 深度矩阵分解中平坦性正则化的归纳偏差

    The Inductive Bias of Flatness Regularization for Deep Matrix Factorization. (arXiv:2306.13239v1 [cs.LG])

    [http://arxiv.org/abs/2306.13239](http://arxiv.org/abs/2306.13239)

    本研究通过学习深度矩阵分解，阐明了大深度神经网络优化器中的随机性隐式正则化效应，并证明了最小化损失函数中海森矩阵迹近似等同于最小化其对应端对端矩阵参数的Schatten 1-范数乘积。

    

    最近关于过参数神经网络的研究表明，优化器中的随机性具有隐式的正则化效应，可以通过减小损失函数的尖锐度（特别是其海森矩阵的迹）来最小化零损失解族。更明确的平坦性正则化形式也在经验上提高了泛化性能。然而，平坦性正则化何时和为什么会导致更好的泛化性能仍不清楚。本文以重要的学习深度线性网络的设置为例，即从线性测量中学习深度矩阵分解，为理解最小化海森矩阵迹解的归纳偏差迈出第一步。我们发现，在满足测量标准受限等距性质（RIP）的所有深度大于一的情况下，最小化海森矩阵迹近似等同于最小化相应端对端矩阵参数的Schatten 1-范数（即所有奇异值之和）的乘积。

    Recent works on over-parameterized neural networks have shown that the stochasticity in optimizers has the implicit regularization effect of minimizing the sharpness of the loss function (in particular, the trace of its Hessian) over the family zero-loss solutions. More explicit forms of flatness regularization also empirically improve the generalization performance. However, it remains unclear why and when flatness regularization leads to better generalization. This work takes the first step toward understanding the inductive bias of the minimum trace of the Hessian solutions in an important setting: learning deep linear networks from linear measurements, also known as \emph{deep matrix factorization}. We show that for all depth greater than one, with the standard Restricted Isometry Property (RIP) on the measurements, minimizing the trace of Hessian is approximately equivalent to minimizing the Schatten 1-norm of the corresponding end-to-end matrix parameters (i.e., the product of al
    
[^48]: 基于剪枝的域泛化方法研究

    Pruning for Better Domain Generalizability. (arXiv:2306.13237v1 [cs.LG])

    [http://arxiv.org/abs/2306.13237](http://arxiv.org/abs/2306.13237)

    本文研究了基于剪枝的域泛化方法，提出了一种新的剪枝评分方法DSS，该方法不是为了保持源准确性，而是直接增强模型的鲁棒性。实验证明该方法可以与最先进的泛化方法结合使用，即便只引入少量稀疏也能显著提高模型性能。

    

    本文探讨了使用剪枝作为一种可靠的方法来提高模型的泛化能力。我们发现现有的剪枝方法，如L2已经可以在目标域性能上提供小幅度的改善。我们进一步提出了一种新的剪枝评分方法，称为DSS，设计不是为了保持源准确性而是直接增强模型的鲁棒性。我们进行了实证实验来验证我们的方法，并证明它甚至可以与MIRO(Cha等人，2022年)等最先进的泛化方法结合使用，进一步提高性能。在MNIST到MNIST-M上，通过将60%通道稀疏引入模型，我们可以将基线性能提高5个百分点以上。在DomainBed基准和最先进的MIRO上，仅通过将10%稀疏引入模型，我们就可以进一步提高其性能。代码可在以下链接找到: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza

    In this paper, we investigate whether we could use pruning as a reliable method to boost the generalization ability of the model. We found that existing pruning method like L2 can already offer small improvement on the target domain performance. We further propose a novel pruning scoring method, called DSS, designed not to maintain source accuracy as typical pruning work, but to directly enhance the robustness of the model. We conduct empirical experiments to validate our method and demonstrate that it can be even combined with state-of-the-art generalization work like MIRO(Cha et al., 2022) to further boost the performance. On MNIST to MNIST-M, we could improve the baseline performance by over 5 points by introducing 60% channel sparsity into the model. On DomainBed benchmark and state-of-the-art MIRO, we can further boost its performance by 1 point only by introducing 10% sparsity into the model. Code can be found at: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza
    
[^49]: 基于带有嘈杂贝叶斯反馈的零和矩阵博弈的对数遗憾对策略

    Logarithmic Regret for Matrix Games against an Adversary with Noisy Bandit Feedback. (arXiv:2306.13233v1 [cs.LG])

    [http://arxiv.org/abs/2306.13233](http://arxiv.org/abs/2306.13233)

    本文提出了一种算法，在带有嘈杂贝叶斯反馈的零和矩阵博弈中，实现了对数遗憾策略。

    

    本文研究了零和矩阵博弈的变种，其中每步行选手选择一行$i$，列选手选择一列$j$，行选手收到平均值为$A_{i,j}$的嘈杂奖励。行选手的目标是尽可能地累积奖励，即使对手是一个对手性列选手。该文提出了一种策略，该策略证明在$m \times n$矩阵博弈中，实现了$O(\sqrt{mnT})$对数遗憾，进一步提高了UCB风格算法所获得的$O(m\sqrt{nT})$对数遗憾。

    This paper considers a variant of zero-sum matrix games where at each timestep the row player chooses row $i$, the column player chooses column $j$, and the row player receives a noisy reward with mean $A_{i,j}$. The objective of the row player is to accumulate as much reward as possible, even against an adversarial column player. If the row player uses the EXP3 strategy, an algorithm known for obtaining $\sqrt{T}$ regret against an arbitrary sequence of rewards, it is immediate that the row player also achieves $\sqrt{T}$ regret relative to the Nash equilibrium in this game setting. However, partly motivated by the fact that the EXP3 strategy is myopic to the structure of the game, O'Donoghue et al. (2021) proposed a UCB-style algorithm that leverages the game structure and demonstrated that this algorithm greatly outperforms EXP3 empirically. While they showed that this UCB-style algorithm achieved $\sqrt{T}$ regret, in this paper we ask if there exists an algorithm that provably ach
    
[^50]: TACO：基于时间潜在动作驱动对比损失的视觉强化学习

    TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning. (arXiv:2306.13229v1 [cs.LG])

    [http://arxiv.org/abs/2306.13229](http://arxiv.org/abs/2306.13229)

    本文提出了TACO方法，一种基于时间潜在动作驱动对比损失的视觉强化学习方法，能够同时学习状态表示和动作表示，提高代理学习的效率。

    

    尽管在强化学习（RL）从原始像素数据中取得了最近的进展，但样本效率仍然是一个重要的障碍。先前的工作试图通过创建自监督辅助任务来解决这个挑战，旨在为未来状态预测丰富代理学习的表示与控制相关信息。然而，这些目标通常不足以学习能够表示最优策略或值函数的表示，并且它们通常考虑具有小的抽象离散动作空间的任务，因此忽视了在连续控制中动作表示学习的重要性。在本文中，我们引入了TACO：一种简单而强大的时间对比学习方法，利用它，代理可以同时获得潜在状态和动作表示。TACO通过优化重新获得观察与最近的多个先前观察的相似性，同时学习状态与动作表示。

    Despite recent progress in reinforcement learning (RL) from raw pixel data, sample inefficiency continues to present a substantial obstacle. Prior works have attempted to address this challenge by creating self-supervised auxiliary tasks, aiming to enrich the agent's learned representations with control-relevant information for future state prediction. However, these objectives are often insufficient to learn representations that can represent the optimal policy or value function, and they often consider tasks with small, abstract discrete action spaces and thus overlook the importance of action representation learning in continuous control. In this paper, we introduce TACO: Temporal Action-driven Contrastive Learning, a simple yet powerful temporal contrastive learning approach that facilitates the concurrent acquisition of latent state and action representations for agents. TACO simultaneously learns a state and an action representation by optimizing the mutual information between re
    
[^51]: 多样社区数据用于数据隐私算法基准测试

    Diverse Community Data for Benchmarking Data Privacy Algorithms. (arXiv:2306.13216v1 [cs.CR])

    [http://arxiv.org/abs/2306.13216](http://arxiv.org/abs/2306.13216)

    多样社区数据摘要旨在为隐私保护机器学习研究提供真实、多样和复杂的基准数据，以解决合成数据的偏差和隐私问题。

    

    多样社区数据是美国国家标准和技术研究所（NIST）计划的核心，旨在增强对表格数据去识别技术（如合成数据）的理解。合成数据是民主化大数据利益的一项雄心勃勃的尝试；它使用生成模型重新创建敏感个人数据，以便公开发布。然而，它容易受到影响其他机器学习应用程序的偏差和隐私问题的影响，甚至可能放大这些问题。当去识别数据分布引入偏差或工件，或泄漏敏感信息时，它们会将这些问题传播到下游应用。此外，真实世界的调查条件（如多样子群、异质非有序数据空间和特征之间的复杂依赖关系）对合成数据算法提出了具体挑战。这些观察结果促使需要真实、多样和复杂的基准数据来支持隐私保护的机器学习研究，而多样社区数据摘要旨在解决这些挑战。

    The Diverse Communities Data Excerpts are the core of a National Institute of Standards and Technology (NIST) program to strengthen understanding of tabular data deidentification technologies such as synthetic data. Synthetic data is an ambitious attempt to democratize the benefits of big data; it uses generative models to recreate sensitive personal data with new records for public release. However, it is vulnerable to the same bias and privacy issues that impact other machine learning applications, and can even amplify those issues. When deidentified data distributions introduce bias or artifacts, or leak sensitive information, they propagate these problems to downstream applications. Furthermore, real-world survey conditions such as diverse subpopulations, heterogeneous non-ordinal data spaces, and complex dependencies between features pose specific challenges for synthetic data algorithms. These observations motivate the need for real, diverse, and complex benchmark data to support
    
[^52]: ovla：使用隐式水印进行神经网络所有权验证

    ovla: Neural Network Ownership Verification using Latent Watermarks. (arXiv:2306.13215v1 [cs.CR])

    [http://arxiv.org/abs/2306.13215](http://arxiv.org/abs/2306.13215)

    ovla是一种使用隐式水印进行神经网络所有权验证的新方法，通过训练网络使水印保持休眠状态，只有在所有者的秘密密钥被应用时才会被激活，该方法在神经网络所有权验证准确率、误报率和抵抗对抗攻击方面优于现有的最先进方法。

    

    神经网络的所有权验证对于保护这些模型免受非法复制、免费骑车、重新分配和其他知识产权的滥用非常重要。我们提出了一种基于隐式水印的神经网络所有权验证新方法。该方法通过解耦网络的正常操作和对带水印输入的响应来进行验证。其关键思想是训练网络，使得水印保持休眠状态，除非应用所有者的秘密密钥来激活它。我们展示了该方法在所有权验证准确率、误报率和抵抗对抗攻击方面优于现有的最先进方法。

    Ownership verification for neural networks is important for protecting these models from illegal copying, free-riding, re-distribution and other intellectual property misuse. We present a novel methodology for neural network ownership verification based on the notion of latent watermarks. Existing ownership verification methods either modify or introduce constraints to the neural network parameters, which are accessible to an attacker in a white-box attack and can be harmful to the network's normal operation, or train the network to respond to specific watermarks in the inputs similar to data poisoning-based backdoor attacks, which are susceptible to backdoor removal techniques. In this paper, we address these problems by decoupling a network's normal operation from its responses to watermarked inputs during ownership verification. The key idea is to train the network such that the watermarks remain dormant unless the owner's secret key is applied to activate it. The secret key is real
    
[^53]: 视觉对抗样本越狱大语言模型的安全隐患分析

    Visual Adversarial Examples Jailbreak Large Language Models. (arXiv:2306.13213v1 [cs.CR])

    [http://arxiv.org/abs/2306.13213](http://arxiv.org/abs/2306.13213)

    本文对将图像引入大型语言模型的安全隐患进行了分析，指出视觉输入空间的连续性和高维性是对抗攻击的丰富领域，同时也为视觉攻击者提供了更广泛的实现对抗目标的可能性。

    

    最近，将图像引入大型语言模型（LLMs）已经引起了人们的高度关注。大型视觉语言模型（VLMs）的普及，例如Flamingo、BLIP-2和GPT-4，标志着视觉和语言基础模型的先进发展相互融合的重要进展。然而，这种综合方法涉及的风险仍未得到详细研究。本文揭示了这一趋势的安全隐患。我们首先指出，视觉输入空间的连续性和高维性在本质上使其成为对抗攻击的丰富领域，这不可避免地扩大了LLMs的攻击面。其次，我们强调，LLMs的广泛功能也为视觉攻击者提供了更广泛的实现对抗目标的可能性，将安全失败的影响扩展到了简单的错误分类之外。为了阐明这些风险，我们研究了VLM视觉输入空间中的对抗性样例。

    Recently, there has been a surge of interest in introducing vision into Large Language Models (LLMs). The proliferation of large Visual Language Models (VLMs), such as Flamingo, BLIP-2, and GPT-4, signifies an exciting convergence of advancements in both visual and language foundation models. Yet, the risks associated with this integrative approach are largely unexamined. In this paper, we shed light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the additional visual input space intrinsically makes it a fertile ground for adversarial attacks. This unavoidably expands the attack surfaces of LLMs. Second, we highlight that the broad functionality of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. To elucidate these risks, we study adversarial examples in the visual input space of a VLM.
    
[^54]: 使用KD-树的差分隐私合成数据

    Differentially Private Synthetic Data Using KD-Trees. (arXiv:2306.13211v1 [cs.CR])

    [http://arxiv.org/abs/2306.13211](http://arxiv.org/abs/2306.13211)

    本文提出了一种使用KD-树的算法，用于生成 $\epsilon $-差分隐私的合成数据，其核密度类似于真实数据集的核密度。该方法克服了维度灾难，是一种可扩展的算法。

    

    创建一个忠实地代表数据分布并同时保护隐私的合成数据集是一项重要的研究挑战。近年来出现了许多基于空间分割的方法，用于以差分隐私的方式回答统计查询问题。然而，对于合成数据生成问题，最近的研究主要集中在深度生成模型上。相比之下，我们利用了空间分割技术和噪声扰动，从而实现了直观透明的算法。我们提出了数据独立和数据相关的算法，用于生成 $\epsilon $-差分隐私的合成数据，其核密度类似于真实数据集的核密度。此外，我们还提供了有关实用性和隐私权利的理论结果，并展示了我们的数据相关方法如何克服维度灾难并导致可扩展的算法。我们展示了相对于之前的工作的实用改进，并讨论了性能

    Creation of a synthetic dataset that faithfully represents the data distribution and simultaneously preserves privacy is a major research challenge. Many space partitioning based approaches have emerged in recent years for answering statistical queries in a differentially private manner. However, for synthetic data generation problem, recent research has been mainly focused on deep generative models. In contrast, we exploit space partitioning techniques together with noise perturbation and thus achieve intuitive and transparent algorithms. We propose both data independent and data dependent algorithms for $\epsilon$-differentially private synthetic data generation whose kernel density resembles that of the real dataset. Additionally, we provide theoretical results on the utility-privacy trade-offs and show how our data dependent approach overcomes the curse of dimensionality and leads to a scalable algorithm. We show empirical utility improvements over the prior work, and discuss perfo
    
[^55]: 提高基于对数累积的SAR图像粗糙度信息估计方法

    Improving Log-Cumulant Based Estimation of Roughness Information in SAR imagery. (arXiv:2306.13200v1 [cs.CV])

    [http://arxiv.org/abs/2306.13200](http://arxiv.org/abs/2306.13200)

    本文提出使用对数累积法改进了$\mathcal{G}^0$分布参数估计方法，可以在恒定的时间内计算出粗糙度估计，从而实现快速可靠的SAR图像理解。

    

    合成孔径雷达（SAR）图像的理解对于遥感应用至关重要，但受其内在的噪声干扰（称为斑点）的影响。复杂的统计模型（例如$\mathcal{G}^0$分布族）已被应用于SAR数据，许多当前在处理此类图像方面的进展都是通过从这些模型中提取信息来完成的。本文提出了一种改进$\mathcal{G}^0$分布参数估计的方法，使用对数累积的方法进行。首先，利用贝叶斯建模，我们构建出可在$\mathcal{G}^0_A$和$\mathcal{G}^0_I$模型下都产生可靠粗糙度估计的方法。其次，我们利用三角函数的近似方法，在恒定的时间内计算出粗糙度估计，使其比现有方法速度快得多。最后，我们展示了如何利用这种方法基于从SAR数据中提取的粗糙度信息实现快速可靠的SAR图像理解。

    Synthetic Aperture Radar (SAR) image understanding is crucial in remote sensing applications, but it is hindered by its intrinsic noise contamination, called speckle. Sophisticated statistical models, such as the $\mathcal{G}^0$ family of distributions, have been employed to SAR data and many of the current advancements in processing this imagery have been accomplished through extracting information from these models. In this paper, we propose improvements to parameter estimation in $\mathcal{G}^0$ distributions using the Method of Log-Cumulants. First, using Bayesian modeling, we construct that regularly produce reliable roughness estimates under both $\mathcal{G}^0_A$ and $\mathcal{G}^0_I$ models. Second, we make use of an approximation of the Trigamma function to compute the estimated roughness in constant time, making it considerably faster than the existing method for this task. Finally, we show how we can use this method to achieve fast and reliable SAR image understanding based 
    
[^56]: Gradient-based Attribution Methods中Pre或Post-Softmax Scores，哪个更好？

    Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?. (arXiv:2306.13197v1 [cs.LG])

    [http://arxiv.org/abs/2306.13197](http://arxiv.org/abs/2306.13197)

    在Gradient-based Attribution Methods中，使用Pre Softmax分数或Post Softmax分数的梯度的选择有各自的优缺点，需要根据具体情况进行权衡。

    

    对于工作作为分类器的神经网络的基于梯度的归因方法使用网络分数的梯度。在这里，我们讨论使用Pre Softmax分数和Post Softmax分数的梯度之间的实际差异以及它们各自的优缺点。

    Gradient based attribution methods for neural networks working as classifiers use gradients of network scores. Here we discuss the practical differences between using gradients of pre-softmax scores versus post-softmax scores, and their respective advantages and disadvantages.
    
[^57]: DiMSam:扩散模型作为部分可观测任务与动作规划中的采样器。

    DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability. (arXiv:2306.13196v1 [cs.RO])

    [http://arxiv.org/abs/2306.13196](http://arxiv.org/abs/2306.13196)

    本文提出了一种使用扩散模型作为采样器的任务和动作规划方法，在部分可观测下能够实现长周期受约束的操作计划。

    

    任务和动作规划（TAMP）方法非常有效地计划长周期自主机器人操作。但是，由于它们需要一个规划模型，因此在环境和其动态不完全了解的领域中应用它们可能非常困难。我们提出通过利用深度生成建模，特别是扩散模型来克服这些限制，学习捕获规划模型中难以设计的约束和采样器。这些学习采样器在TAMP求解器中组合和合并，以联合找到满足规划中约束的行动参数值。为了便于对环境中未知对象进行预测，我们将这些采样器定义为学习的低维潜变量嵌入的可变对象状态。我们在关节式物体操作领域评估了我们的方法，并展示了经典TAMP、生成学习和潜在嵌入的组合如何使得在部分可观测下进行长周期受约束的操作计划。

    Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, because they require a planning model, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constra
    
[^58]: 平方根Lipschitz损失的一致收敛性

    Uniform Convergence with Square-Root Lipschitz Loss. (arXiv:2306.13188v1 [stat.ML])

    [http://arxiv.org/abs/2306.13188](http://arxiv.org/abs/2306.13188)

    该论文通过平方根Lipschitz损失的一致收敛性，对一般的高斯数据建立了保证，允许处理广泛的损失类别，并重新推导和更好地理解“乐观率”的学习保证和插值。

    

    我们通过假设类的Rademacher复杂度和标量损失函数的平方根的Lipschitz常数，在高斯数据方面建立了一般的一致收敛性保证。我们展示了这些保证如何大大概括了基于平滑性(导数的Lipschitz常数)的先前结果，并使我们能够处理更广泛的平方根Lipschitz损失类别，其中包括适用于研究相位恢复和ReLU回归的非平滑损失函数，以及重新推导和更好地理解“乐观率”的学习保证和插值。

    We establish generic uniform convergence guarantees for Gaussian data in terms of the Rademacher complexity of the hypothesis class and the Lipschitz constant of the square root of the scalar loss function. We show how these guarantees substantially generalize previous results based on smoothness (Lipschitz constant of the derivative), and allow us to handle the broader class of square-root-Lipschitz losses, which includes also non-smooth loss functions appropriate for studying phase retrieval and ReLU regression, as well as rederive and better understand "optimistic rate" and interpolation learning guarantees.
    
[^59]: (核) 岭回归中过度拟合成本的不可知观察

    An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression. (arXiv:2306.13185v1 [stat.ML])

    [http://arxiv.org/abs/2306.13185](http://arxiv.org/abs/2306.13185)

    本文研究了核岭回归中过拟合成本，采用“不可知”的观点，以分析样本量和任务特征结构对成本的影响。通过分析提供了更细致的过度拟合表征。

    

    本研究研究了有噪声的核岭回归 (KRR) 中过拟合的成本，我们将其定义为插值无岭模型的测试误差与最优调节模型的测试误差之比。我们采用“不可知”的观点，即对于任何目标函数，即使样本量不足以达到一致性或目标函数不在 RKHS 中，我们也将成本看作样本量的函数。使用最近推导出的（非严格的）风险评估，以任务特征结构为基础，利用高斯普适性假设分析过度拟合成本。我们的分析提供了良性、缓和和灾难性过度拟合（参见 Mallinar 等人 2022）的更精细的表征。

    We study the cost of overfitting in noisy kernel ridge regression (KRR), which we define as the ratio between the test error of the interpolating ridgeless model and the test error of the optimally-tuned model. We take an "agnostic" view in the following sense: we consider the cost as a function of sample size for any target function, even if the sample size is not large enough for consistency or the target is outside the RKHS. We analyze the cost of overfitting under a Gaussian universality ansatz using recently derived (non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis provides a more refined characterization of benign, tempered and catastrophic overfitting (qv Mallinar et al. 2022).
    
[^60]: 一种基于循环图卷积的方法预测年降雪量

    Prediction of Annual Snow Accumulation Using a Recurrent Graph Convolutional Approach. (arXiv:2306.13181v1 [cs.LG])

    [http://arxiv.org/abs/2306.13181](http://arxiv.org/abs/2306.13181)

    本研究利用循环图卷积神经网络和图注意力网络的方法，精确预测极地区域年降雪量，且在较大数据集上使用更少的输入数据具有较好的性能。

    

    精确跟踪和预测极地冰层的积雪情况可以揭示其历史趋势。近年来，通过 radar 传感器，如 Snow Radar ，能够以较高的垂直分辨率测量这些内部冰层。在先前的工作中，我们发现当给出包含深冰层厚度的时态图时，时态图卷积网络在预测未来积雪时表现良好。在本研究中，我们尝试用基于图注意力网络的模型预测更多的年积雪数据点，并在较大的数据集上使用更少的输入数据点。我们发现这些较大的变化只会对性能产生非常轻微的负面影响。

    The precise tracking and prediction of polar ice layers can unveil historic trends in snow accumulation. In recent years, airborne radar sensors, such as the Snow Radar, have been shown to be able to measure these internal ice layers over large areas with a fine vertical resolution. In our previous work, we found that temporal graph convolutional networks perform reasonably well in predicting future snow accumulation when given temporal graphs containing deep ice layer thickness. In this work, we experiment with a graph attention network-based model and used it to predict more annual snow accumulation data points with fewer input data points on a larger dataset. We found that these large changes only very slightly negatively impacted performance.
    
[^61]: 基于注意力深度神经网络的关键帧提取

    Key Frame Extraction with Attention Based Deep Neural Networks. (arXiv:2306.13176v1 [cs.CV])

    [http://arxiv.org/abs/2306.13176](http://arxiv.org/abs/2306.13176)

    本研究提出了一种基于深度自编码器模型和注意力层的深度学习方法，用于自动从视频中提取关键帧，提取的关键帧可以用作要使用的方法和模型的输入特征，可用于摘要和安防等行业中的自动化工作。

    

    自动从视频中提取关键帧是选择能够最好地概括长视频内容的场景的练习。提供视频摘要是促进快速浏览和内容总结的重要任务。获得的关键帧可以用作要使用的方法和模型的输入特征。在这项研究中，我们提出了一种基于深度自编码器模型和注意力层的深度学习方法，用于关键帧检测，该方法首先使用自编码器的编码器部分从视频帧中提取特征，并使用k-means聚类算法进行分段，将类似的特征和帧组合在一起。然后，从这些帧中选择关键帧。

    Automatic keyframe detection from videos is an exercise in selecting scenes that can best summarize the content for long videos. Providing a summary of the video is an important task to facilitate quick browsing and content summarization. The resulting photos are used for automated works (e.g. summarizing security footage, detecting different scenes used in music clips) in different industries. In addition, processing high-volume videos in advanced machine learning methods also creates resource costs. Keyframes obtained; It can be used as an input feature to the methods and models to be used. In this study; We propose a deep learning-based approach for keyframe detection using a deep auto-encoder model with an attention layer. The proposed method first extracts the features from the video frames using the encoder part of the autoencoder and applies segmentation using the k-means clustering algorithm to group these features and similar frames together. Then, keyframes are selected from 
    
[^62]: AmicroN：一种用于生成细粒度微动作的人体活动识别注释的框架

    AmicroN: A Framework for Generating Annotations for Human Activity Recognition with Granular Micro-Activities. (arXiv:2306.13149v1 [cs.HC])

    [http://arxiv.org/abs/2306.13149](http://arxiv.org/abs/2306.13149)

    本文提出了一种名为AmicroN的框架，可以利用传感器数据自动生成微动作注释，弥补现有数据集缺乏细粒度注释的不足。

    

    利用传感器数据实现高效的人体活动识别需要大量的注释数据。未标记传感器数据的增长挑战了传统的基于人的数据收集方法，通常导致收集更浅的注释。这些浅层注释忽略了组成日常生活中任何复杂活动的细粒度微动作。本文分析了可用的预注释数据集中缺乏细粒度注释的原因和缺陷，并进行了详细的调查以了解与注释相关的人类感知。在此基础上，我们开发了AmicroN框架，该框架可以使用运动特征和可用的粗粒度宏操作标签自动生成微动作注释。在后台，AmicroN应用变点检测，然后进行零样本学习。

    Efficient human activity recognition (HAR) using sensor data needs a significant volume of annotated data. The growing volume of unlabelled sensor data has challenged conventional practices for gathering HAR annotations with human-in-the-loop approaches, often leading to the collection of shallower annotations. These shallower annotations ignore the fine-grained micro-activities that constitute any complex activities of daily living (ADL). Understanding this, we, in this paper, first analyze this lack of granular annotations from available pre-annotated datasets to understand the practical inconsistencies and also perform a detailed survey to look into the human perception surrounding annotations. Drawing motivations from these, we next develop the framework AmicroN that can automatically generate micro-activity annotations using locomotive signatures and the available coarse-grain macro-activity labels. In the backend, AmicroN applies change-point detection followed by zero-shot learn
    
[^63]: 通过弃权实现顺序预测中的对抗鲁棒性

    Adversarial Resilience in Sequential Prediction via Abstention. (arXiv:2306.13119v1 [cs.LG])

    [http://arxiv.org/abs/2306.13119](http://arxiv.org/abs/2306.13119)

    本文提出了一种处理顺序预测的模型，允许在不对对抗性样例进行预测的情况下提高算法抗对抗攻击的能力。

    

    本文研究了在带有允许注入干净标签对抗性（或超出分布）示例的对抗者的情况下，在随机设置下的顺序预测问题。针对纯随机数据的算法在存在此类对抗性示例的情况下往往失败，从而导致错误的预测。这在许多高风险应用中是不可取的，例如医学建议，这里弃权不进行对抗性示例的预测优于误分类。另一方面，假设完全对抗性数据导致非常悲观的界限，在实践中往往是空洞的。为了实现这一目标，我们提出了一种新的顺序预测模型，它位于纯随机和完全对抗性设置之间，通过允许学习器在对抗样例上无代价地放弃进行预测来实现。假设访问非对抗样例的边际分布，我们设计了一个学习器，其误差随着VC维的变化而变化。

    We study the problem of sequential prediction in the stochastic setting with an adversary that is allowed to inject clean-label adversarial (or out-of-distribution) examples. Algorithms designed to handle purely stochastic data tend to fail in the presence of such adversarial examples, often leading to erroneous predictions. This is undesirable in many high-stakes applications such as medical recommendations, where abstaining from predictions on adversarial examples is preferable to misclassification. On the other hand, assuming fully adversarial data leads to very pessimistic bounds that are often vacuous in practice.  To capture this motivation, we propose a new model of sequential prediction that sits between the purely stochastic and fully adversarial settings by allowing the learner to abstain from making a prediction at no cost on adversarial examples. Assuming access to the marginal distribution on the non-adversarial examples, we design a learner whose error scales with the VC 
    
[^64]: 一种用于氢脆的机器学习压力仿真器

    A Machine Learning Pressure Emulator for Hydrogen Embrittlement. (arXiv:2306.13116v1 [cs.LG])

    [http://arxiv.org/abs/2306.13116](http://arxiv.org/abs/2306.13116)

    本文提出了一种物理信息的机器学习模型，用于预测管道内壁的气体压力，为管道系统监控提供了第一步。该方法具有较高保真度且优于纯数据驱动的方法。

    

    将氢与天然气混合后作为氢的替代品被用于氢运输。但是，物质的氢脆性是科学家和天然气安装设计师需要避免的一个主要问题，以避免处理中的失效。本文提出了一种物理信息的机器学习模型，用于预测管道内壁的气体压力。尽管具有高保真度的结果，但目前的基于偏微分方程(PDE)的模拟器需要耗费大量时间和计算资源。使用模拟数据，我们训练了一个ML模型，用于预测管道内壁的压力，这是管道系统监控的第一步。我们发现基于物理的方法优于纯数据驱动的方法，并满足气流系统的物理限制。

    A recent alternative for hydrogen transportation as a mixture with natural gas is blending it into natural gas pipelines. However, hydrogen embrittlement of material is a major concern for scientists and gas installation designers to avoid process failures. In this paper, we propose a physics-informed machine learning model to predict the gas pressure on the pipes' inner wall. Despite its high-fidelity results, the current PDE-based simulators are time- and computationally-demanding. Using simulation data, we train an ML model to predict the pressure on the pipelines' inner walls, which is a first step for pipeline system surveillance. We found that the physics-based method outperformed the purely data-driven method and satisfy the physical constraints of the gas flow system.
    
[^65]: 使用Transfer Learning图神经网络对具有异构电极配置的数据集进行EEG Decoding

    EEG Decoding for Datasets with Heterogenous Electrode Configurations using Transfer Learning Graph Neural Networks. (arXiv:2306.13109v1 [eess.SP])

    [http://arxiv.org/abs/2306.13109](http://arxiv.org/abs/2306.13109)

    本文提出了一种用于克服记录设备和电极布局变化带来的限制，学习不同实验室的多样化EEG数据的方法。文中研发了一种新型机器学习框架，该框架结合了图神经网络和迁移学习方法，以实现非侵入性动作想象（MI）EEG解码。

    

    脑机接口（BMI）采用机器学习方法进行特征学习受益匪浅，但这些方法需要大量的训练数据，而这些数据通常无法从单个数据集中获得。在一个实验室内甚至跨实验室合并数据变得困难，因为记录设备和电极布局的变化会导致数据分布的偏移，数据维度的变化和数据维度标识的变化。本文旨在克服这种限制，并通过不同实验室的多种不同和多样的数据集进行学习。为了解决领域适应问题，我们开发了一种结合图神经网络（GNNs）和迁移学习方法的新型机器学习框架，以非侵入性动作想象（MI）EEG解码为例。实验证明，我们重点关注从具有不同电极布局和不同数量电极的EEG数据中学习的挑战。

    Brain-Machine Interfacing (BMI) has greatly benefited from adopting machine learning methods for feature learning that require extensive data for training, which are often unavailable from a single dataset. Yet, it is difficult to combine data across labs or even data within the same lab collected over the years due to the variation in recording equipment and electrode layouts resulting in shifts in data distribution, changes in data dimensionality, and altered identity of data dimensions. Our objective is to overcome this limitation and learn from many different and diverse datasets across labs with different experimental protocols. To tackle the domain adaptation problem, we developed a novel machine learning framework combining graph neural networks (GNNs) and transfer learning methodologies for non-invasive Motor Imagery (MI) EEG decoding, as an example of BMI. Empirically, we focus on the challenges of learning from EEG data with different electrode layouts and varying numbers of 
    
[^66]: 多任务学习用于雷达信号特征化

    Multi-task Learning for Radar Signal Characterisation. (arXiv:2306.13105v1 [eess.SP])

    [http://arxiv.org/abs/2306.13105](http://arxiv.org/abs/2306.13105)

    本文提出一种多任务学习的方法来解决雷达信号分类和特征化的问题，引入IQST等参考架构，通过回归和分类的多重任务优化提高性能，在合成雷达数据集上展示了良好的表现，并提供了首个雷达信号特征化基准测试样例。

    

    无论是民用还是军事应用，无线电信号识别都是至关重要的任务，对于未知信号的准确和及时的识别是频谱管理和电子战的重要组成部分。然而，在这个领域里，大部分的研究都专注于使用深度学习来进行调制分类，而没能充分研究信号特征化的任务。本文提出了一种多任务学习（MTL）的方法来应对雷达信号分类和特征化的问题，提出了 IQ Signal Transformer (IQST) 和其他参考架构，使得能够同时优化多重回归和分类任务。我们在一个合成雷达数据集上展示了我们提出的 MTL 模型的性能，同时提供了一个首次的雷达信号特征化基准测试样例。

    Radio signal recognition is a crucial task in both civilian and military applications, as accurate and timely identification of unknown signals is an essential part of spectrum management and electronic warfare. The majority of research in this field has focused on applying deep learning for modulation classification, leaving the task of signal characterisation as an understudied area. This paper addresses this gap by presenting an approach for tackling radar signal classification and characterisation as a multi-task learning (MTL) problem. We propose the IQ Signal Transformer (IQST) among several reference architectures that allow for simultaneous optimisation of multiple regression and classification tasks. We demonstrate the performance of our proposed MTL model on a synthetic radar dataset, while also providing a first-of-its-kind benchmark for radar signal characterisation.
    
[^67]: 人类介入的视觉前列腺深度刺激编码的优化

    Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses. (arXiv:2306.13104v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.13104](http://arxiv.org/abs/2306.13104)

    本研究提出了一种“人类介入的视觉前列腺深度刺激编码优化”的方法，通过反演前向模型、实时优化编码参数等手段，显著提高了感知质量。

    

    神经前列腺在恢复失去的感官功能和增强人类能力方面具有潜力，但当前设备产生的感觉通常似乎不自然或扭曲。植入器的确切位置和个体感知的差异导致刺激响应存在显着差异，使个性化刺激优化成为关键挑战。贝叶斯优化可用于优化具有有限噪声观察数据的患者专属刺激参数，但对于高维刺激不可行。而深度学习模型可以优化刺激编码策略，但通常假设有关患者特定变化的完美知识。在这里，我们提出了一种新颖的、实际可行的方法，克服了这两个基本局限性。首先，通过反演将电刺激映射到视觉感知的前向模型，训练深度编码器网络以为任何个体患者产生最佳刺激。其次，提出了一种优选贝叶斯优化算法，以实时优化编码参数，成功使知觉刺激更加逼真。我们提出的“人类介入的视觉前列腺深度刺激编码优化”方法在动物模型中进行了测试，相比最先进的方法显著提高了感知质量。

    Neuroprostheses show potential in restoring lost sensory function and enhancing human capabilities, but the sensations produced by current devices often seem unnatural or distorted. Exact placement of implants and differences in individual perception lead to significant variations in stimulus response, making personalized stimulus optimization a key challenge. Bayesian optimization could be used to optimize patient-specific stimulation parameters with limited noisy observations, but is not feasible for high-dimensional stimuli. Alternatively, deep learning models can optimize stimulus encoding strategies, but typically assume perfect knowledge of patient-specific variations. Here we propose a novel, practically feasible approach that overcomes both of these fundamental limitations. First, a deep encoder network is trained to produce optimal stimuli for any individual patient by inverting a forward model mapping electrical stimuli to visual percepts. Second, a preferential Bayesian opti
    
[^68]: 对抗现实世界攻击下文本到图像扩散模型的鲁棒性评估

    Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks. (arXiv:2306.13103v1 [cs.CR])

    [http://arxiv.org/abs/2306.13103](http://arxiv.org/abs/2306.13103)

    本研究对于文本到图像扩散模型（T2I DMs）进行了第一个针对现实攻击的鲁棒性评估，考虑由人类可以产生的现实误差所构成的攻击范围以确保语义一致性。研究表明，我们提出的攻击目标具有较好的攻击效果。

    

    文本到图像（T2I）扩散模型（DM）展示了从文本描述生成高质量图像的潜力。这些模型的实际应用需要特别关注它们的安全性和保真度，但这方面的研究还不够。一个基本问题是现有的T2I DM是否具有对输入文本的变化鲁棒性。为了回答这个问题，本研究提供了第一个针对现实攻击的T2I DM鲁棒性评估。与以往关注涉及输入文本的虚假修改的恶意攻击的研究不同，我们考虑由人类可以产生的现实误差（例如，打字错误、字符、语音）所构成的攻击范围，以确保语义一致性。考虑到生成过程的固有随机性，我们提出了新的基于分布的攻击目标以误导T2I DM。我们以黑盒方式进行攻击，不需要了解模型的任何信息。大量实验表明了我们的方法在攻击效果上的有效性。

    Text-to-image (T2I) diffusion models (DMs) have shown promise in generating high-quality images from textual descriptions. The real-world applications of these models require particular attention to their safety and fidelity, but this has not been sufficiently explored. One fundamental question is whether existing T2I DMs are robust against variations over input texts. To answer it, this work provides the first robustness evaluation of T2I DMs against real-world attacks. Unlike prior studies that focus on malicious attacks involving apocryphal alterations to the input texts, we consider an attack space spanned by realistic errors (e.g., typo, glyph, phonetic) that humans can make, to ensure semantic consistency. Given the inherent randomness of the generation process, we develop novel distribution-based attack objectives to mislead T2I DMs. We perform attacks in a black-box manner without any knowledge of the model. Extensive experiments demonstrate the effectiveness of our method for 
    
[^69]: MBrain：一种用于脑信号的多通道自监督学习框架

    MBrain: A Multi-channel Self-Supervised Learning Framework for Brain Signals. (arXiv:2306.13102v1 [eess.SP])

    [http://arxiv.org/abs/2306.13102](http://arxiv.org/abs/2306.13102)

    MBrain是一种针对脑信号的多通道自监督学习框架，可解决监督学习方法需要高成本临床标签和不同测量方法之间临床模式差异的问题。

    

    脑信号是了解人类大脑的生理活动和疾病的重要定量数据。大多数现有研究都关注监督学习方法，然而，这些方法需要高成本的临床标签。此外，侵入式（例如SEEG）和非侵入式（例如EEG）方法测量的脑信号的临床模式之间巨大的差异导致缺乏一种统一的方法。为了处理以上问题，我们提出了研究自监督学习（SSL）框架，该框架可用于预先训练SEEG或EEG数据。直观地，由神经元的发射产生的脑信号在人类大脑中传输到不同的连接结构之间。受此启发，我们提出了MBrain，以学习不同通道（即电极的接触点，对应于不同的脑区）之间的隐式空间和时间相关性，作为统一建模不同类型脑信号的基石。

    Brain signals are important quantitative data for understanding physiological activities and diseases of human brain. Most existing studies pay attention to supervised learning methods, which, however, require high-cost clinical labels. In addition, the huge difference in the clinical patterns of brain signals measured by invasive (e.g., SEEG) and non-invasive (e.g., EEG) methods leads to the lack of a unified method. To handle the above issues, we propose to study the self-supervised learning (SSL) framework for brain signals that can be applied to pre-train either SEEG or EEG data. Intuitively, brain signals, generated by the firing of neurons, are transmitted among different connecting structures in human brain. Inspired by this, we propose MBrain to learn implicit spatial and temporal correlations between different channels (i.e., contacts of the electrode, corresponding to different brain areas) as the cornerstone for uniformly modeling different types of brain signals. Specifical
    
[^70]: BrainNet：基于分层图卷积网络的SEEG癫痫波检测

    BrainNet: Epileptic Wave Detection from SEEG with Hierarchical Graph Diffusion Learning. (arXiv:2306.13101v1 [eess.SP])

    [http://arxiv.org/abs/2306.13101](http://arxiv.org/abs/2306.13101)

    本研究提出了第一个使用真实世界SEEG数据集进行癫痫波检测的数据驱动研究，通过使用分层图卷积网络技术，可以检测并分析扩散路径，从而有助于临床实践。

    

    癫痫是一种严重的神经系统疾病，影响着全球1-2％的人口。癫痫的诊断在很大程度上取决于对癫痫波的识别，即患者大脑中的无序电脑活动。现有研究已经开始使用机器学习模型通过电极链记录皮层脑电图(EEG)检测癫痫波。然而，最近开发的立体电极脑电图(SEEG)方法提供的立体信息比传统的EEG更精确，并已广泛应用于临床实践。因此，本研究提出了第一个使用真实世界SEEG数据集来检测癫痫波的数据驱动研究。在提供新机会的同时，SEEG也带来了一些挑战。在临床实践中，癫痫波活动被认为在大脑的不同区域间传播。这些传播路径，也称为癫痫发生网络，在癫痫手术的背景下被认为是关键因素。

    Epilepsy is one of the most serious neurological diseases, affecting 1-2% of the world's population. The diagnosis of epilepsy depends heavily on the recognition of epileptic waves, i.e., disordered electrical brainwave activity in the patient's brain. Existing works have begun to employ machine learning models to detect epileptic waves via cortical electroencephalogram (EEG). However, the recently developed stereoelectrocorticography (SEEG) method provides information in stereo that is more precise than conventional EEG, and has been broadly applied in clinical practice. Therefore, we propose the first data-driven study to detect epileptic waves in a real-world SEEG dataset. While offering new opportunities, SEEG also poses several challenges. In clinical practice, epileptic wave activities are considered to propagate between different regions in the brain. These propagation paths, also known as the epileptogenic network, are deemed to be a key factor in the context of epilepsy surger
    
[^71]: GIMLET：一种用于基于指令分子零样本学习的统一图文模型

    GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])

    [http://arxiv.org/abs/2306.13089](http://arxiv.org/abs/2306.13089)

    本研究提出了一种名为GIMLET的统一图文模型，用于在零样本设置下使用自然语言指令完成分子相关任务。我们解决了现有模型的指令处理不足和图形容量有限的问题，并证明了使用GIMLET能够增强图形特征的泛化能力。

    

    分子属性预测近年来受到了广泛关注，但由于昂贵的实验造成的标签不足问题将是其主要瓶颈。为了缓解这个问题并更好地利用文本知识进行任务，本研究探讨了在零样本设置下使用自然语言指令完成分子相关任务的可行性。我们发现现有的分子-文本模型在这种情况下表现不佳，原因是处理指令不足以及图形容量有限。为了克服这些问题，我们提出了GIMLET，它统一了图形和文本数据的语言模型。通过采用广义位置嵌入，我们的模型被扩展以编码图形结构和指令文本，而无需额外的图形编码模块。GIMLET还在注意机制中解耦了图形的编码和任务指令，增强了跨新任务的图形特征的泛化能力。我们构建了一个数据集...

    Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset 
    
[^72]: 基于自适应OPTICS聚类的强化联邦学习方法

    Reinforcement Federated Learning Method Based on Adaptive OPTICS Clustering. (arXiv:2306.12859v1 [cs.LG])

    [http://arxiv.org/abs/2306.12859](http://arxiv.org/abs/2306.12859)

    本文提出了一种基于自适应OPTICS聚类的强化联邦学习方法，旨在缓解不同用户终端上的数据分布不同所带来的负面影响，并有效提高了联邦学习方法的性能。

    

    联邦学习是一种分布式机器学习技术，它实现了数据隐私保护和数据共享计算之间的平衡。为了保护数据隐私，联邦学习通过在参与设备上本地执行分布式训练并将本地模型聚合成全局模型来学习共享模型。联邦学习存在的问题是，由于数据在不同用户终端上的非独立和相同分布所导致的负面影响。为了缓解这个问题，本文提出了一种基于自适应OPTICS聚类的增强型联邦聚合方法。具体来说，该方法将聚类环境视为马尔科夫决策过程，并对参数搜索方向的调整过程进行建模，以找到最佳聚类参数以达到最佳联邦聚合方法。本文的核心贡献是提出了一种适用于联邦学习的自适应OPTICS聚类算法，可有效提高联邦学习方法的性能。

    Federated learning is a distributed machine learning technology, which realizes the balance between data privacy protection and data sharing computing. To protect data privacy, feder-ated learning learns shared models by locally executing distributed training on participating devices and aggregating local models into global models. There is a problem in federated learning, that is, the negative impact caused by the non-independent and identical distribu-tion of data across different user terminals. In order to alleviate this problem, this paper pro-poses a strengthened federation aggregation method based on adaptive OPTICS clustering. Specifically, this method perceives the clustering environment as a Markov decision process, and models the adjustment process of parameter search direction, so as to find the best clus-tering parameters to achieve the best federated aggregation method. The core contribution of this paper is to propose an adaptive OPTICS clustering algorithm for federated
    
[^73]: Otter-Knowledge：不同来源的多模态知识图谱表示学习在药物发现中的基准测试。

    Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery. (arXiv:2306.12802v1 [cs.LG])

    [http://arxiv.org/abs/2306.12802](http://arxiv.org/abs/2306.12802)

    本研究在药物发现方面使用了多模态知识图谱表示学习，获得了最先进的结果。他们整合了来自不同来源的数据，并提供了基于这些数据的预训练模型。

    

    最近，表示学习的研究利用大量的蛋白质或分子数据库，通过无监督学习技术获得药物和蛋白质结构的知识。这些预训练表示已被证明可以显著提高后续任务的准确性，如预测药物和靶蛋白之间的亲和力。在本研究中，我们展示了通过将来自不同来源和模态的知识图谱整合到序列或SMILES表示中，可以进一步丰富表示，并在已建立的基准测试数据集上实现最先进的结果。我们提供了来自7个公共来源的预处理和整合数据，其中包括超过30M个三元组。此外，我们还提供了基于这些数据的预训练模型，以及它们在Therapeutic Data Commons (TDC)基准测试中性能报告的结果。

    Recent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks.
    
[^74]: OptIForest: 用于异常检测的最优隔离森林算法

    OptIForest: Optimal Isolation Forest for Anomaly Detection. (arXiv:2306.12703v1 [cs.LG])

    [http://arxiv.org/abs/2306.12703](http://arxiv.org/abs/2306.12703)

    本论文针对隔离森林算法中分支因子的最优取值问题，基于隔离效率提出创新算法OptIForest，该算法结构简洁、检测性能优秀，可应用于各种异常检测场景。

    

    异常检测在诸多领域扮演着重要角色，诸如网络安全中的入侵检测、金融风险监控、人类健康监测等。根据隔离森林机制提出的一类异常检测方法由于其简洁、有效、高效而备受青睐，例如针对实际部署，iForest是最常用的检测器之一。虽然大多数隔离森林采用二进制结构，但框架LSHiForest已经证明了多叉隔离树结构可以带来更好的检测性能。然而，尚无理论工作回答关于隔离森林的最优树结构的根本和实践重要问题，即何种分支因子的隔离树结构最优。本文提出隔离效率理论来解答该问题，进而确定了一个隔离树的最优分支因子。

    Anomaly detection plays an increasingly important role in various fields for critical tasks such as intrusion detection in cybersecurity, financial risk detection, and human health monitoring. A variety of anomaly detection methods have been proposed, and a category based on the isolation forest mechanism stands out due to its simplicity, effectiveness, and efficiency, e.g., iForest is often employed as a state-of-the-art detector for real deployment. While the majority of isolation forests use the binary structure, a framework LSHiForest has demonstrated that the multi-fork isolation tree structure can lead to better detection performance. However, there is no theoretical work answering the fundamentally and practically important question on the optimal tree structure for an isolation forest with respect to the branching factor. In this paper, we establish a theory on isolation efficiency to answer the question and determine the optimal branching factor for an isolation tree. Based on
    
[^75]: 半隐式去噪扩散模型（SIDDMs）

    Semi-Implicit Denoising Diffusion Models (SIDDMs). (arXiv:2306.12511v1 [cs.LG])

    [http://arxiv.org/abs/2306.12511](http://arxiv.org/abs/2306.12511)

    SIDDMs是一种新方法，通过匹配隐式和显式因子，实现在生成模型中快速收敛且一定程度上保证样本多样性和质量。

    

    尽管生成模型的数量正在增加，但在推理过程中实现快速采样而不牺牲样本多样性和质量仍然具有挑战性。现有的模型（如 DDPMS）可以提供高质量，丰富多样的样本，但受迭代步骤数量的固有限制而速度较慢。Denoising Diffusion Generative Adversarial Networks (DDGAN) 试图通过集成 GAN 模型用于扩散过程的较大跳跃来规避此限制。然而，当应用于大型数据集时，DDGAN 遇到了可扩展性限制。为了解决这些限制，我们引入了一种新的方法，通过匹配隐式和显式因子来解决问题。更具体地说，我们的方法涉及利用隐式模型来匹配嘈杂数据的边缘分布和前向扩散的显式条件分布。这种组合使我们能够有效地匹配联合去噪分布。与 DDPMS 不同，我们的半隐式去噪扩散模型（SIDDMs）可以在不影响所生成样本的多样性和质量的情况下快速收敛。

    Despite the proliferation of generative models, achieving fast sampling during inference without compromising sample diversity and quality remains challenging. Existing models such as Denoising Diffusion Probabilistic Models (DDPM) deliver high-quality, diverse samples but are slowed by an inherently high number of iterative steps. The Denoising Diffusion Generative Adversarial Networks (DDGAN) attempted to circumvent this limitation by integrating a GAN model for larger jumps in the diffusion process. However, DDGAN encountered scalability limitations when applied to large datasets. To address these limitations, we introduce a novel approach that tackles the problem by matching implicit and explicit factors. More specifically, our approach involves utilizing an implicit model to match the marginal distributions of noisy data and the explicit conditional distribution of the forward diffusion. This combination allows us to effectively match the joint denoising distributions. Unlike DDPM
    
[^76]: 比较多元数据下使用深度学习模型进行波动率预测的效果

    Comparing deep learning models for volatility prediction using multivariate data. (arXiv:2306.12446v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.12446](http://arxiv.org/abs/2306.12446)

    本研究比较了使用不同深度学习模型预测多项资产波动率的效果，发现时间融合变压器及时间卷积神经网络的变体最优，可以用于实践。

    

    本研究旨在比较使用多种深度学习模型预测波动率的效果，从简单浅层的模型到更深层、更复杂的模型，并将它们与天真预测和经典GARCH模型的变化相比较。具体而言，基于GARCH模型、多层感知器、循环神经网络、时间卷积神经网络和时间融合变压器预测了五种资产（即S\&P500、纳斯达克100、黄金、白银和石油）的波动率。在大多数情况下，时间融合变压器以及时间卷积神经网络的变体胜过了经典的方法和浅层网络。这些实验被重复进行，并且竞争模型之间的差异被证明是具有统计显著性的，因此鼓励在实践中使用它们。

    This study aims at comparing several deep learning-based forecasters in the task of volatility prediction using multivariate data, proceeding from simpler or shallower to deeper and more complex models and compare them to the naive prediction and variations of classical GARCH models. Specifically, the volatility of five assets (i.e., S\&P500, NASDAQ100, gold, silver, and oil) was predicted with the GARCH models, Multi-Layer Perceptrons, recurrent neural networks, Temporal Convolutional Networks, and the Temporal Fusion Transformer. In most cases the Temporal Fusion Transformer followed by variants of Temporal Convolutional Network outperformed classical approaches and shallow networks. These experiments were repeated, and the difference between competing models was shown to be statistically significant, therefore encouraging their use in practice.
    
[^77]: MimiC：模仿中心更新解决联邦学习中的客户端退出问题

    MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v1 [cs.LG])

    [http://arxiv.org/abs/2306.12212](http://arxiv.org/abs/2306.12212)

    本文提出的 MimiC 算法解决了联邦学习中客户端退出问题，通过模仿缺失的客户端更新解决了聚合更新和期望中心更新之间的分歧，实现了更高的测试准确率和更低的通信成本。

    

    联邦学习是一种有前途的隐私保护协作学习框架。在联邦学习中，模型训练任务分发给客户端，只需要在中央服务器收集模型更新。然而，在移动边缘网络中部署时，客户端（如智能手机和可穿戴设备）可能会无预警地退出任何一次训练迭代，这会阻碍联邦学习达到收敛。本文解决了联邦学习中这一关键挑战，设计出一种名为 MimiC 的新型训练算法，该算法在中心服务器修改其更新以模仿缺失客户端更新，通过实验结果显示，MimiC 相对现有方法在多个基准数据集上均取得了更高的测试准确率和更低的通信成本。

    Federated learning (FL) is a promising framework for privacy-preserving collaborative learning. In FL, the model training tasks are distributed to clients and only the model updates need to be collected at a central server. However, when being deployed at the mobile edge network, clients (e.g., smartphones and wearables) may have unpredictable availability and randomly drop out of any training iteration, which hinders FL from achieving the convergence. This paper tackles such a critical challenge of FL. In particular, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg can only oscillate within the neighborhood of a stationary point of the global loss function, which is caused by the divergence between the aggregated update and the desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modi
    
[^78]: 使用4位整数训练Transformer

    Training Transformers with 4-bit Integers. (arXiv:2306.11987v1 [cs.LG])

    [http://arxiv.org/abs/2306.11987](http://arxiv.org/abs/2306.11987)

    本文提出了一种使用INT4算术训练Transformer的方法，并细致地分析了转换器中激活和梯度的特定结构，为它们提出了专用的量化器。算法在多个任务中达到了竞争性的准确性。

    

    将激活、权重和梯度量化为4位有望加速神经网络训练。然而，现有的4位训练方法需要自定义数值格式，而这些格式不受当代硬件支持。本文提出一种使用INT4算术实现所有矩阵乘法的Transformer训练方法。以极低的INT4精度训练是一项具有挑战性的任务。为了实现这一点，我们仔细分析了Transformer中激活和梯度的特定结构，为它们提出了专用量化器。对于前向传播，我们识别了离群值的挑战，并提出了一种哈达玛量化器来抑制离群值。对于反向传播，我们利用梯度的结构稀疏性，提出位分裂和杠杆得分采样技术来精确量化梯度。我们的算法在包括自然语言理解、机器翻译和图像分类在内的一系列任务上取得了竞争性的精度。

    Quantizing the activation, weight, and gradient to 4-bit is promising to accelerate neural network training. However, existing 4-bit training methods require custom numerical formats which are not supported by contemporary hardware. In this work, we propose a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic. Training with an ultra-low INT4 precision is challenging. To achieve this, we carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers. For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. Our algorithm achieves competitive accuracy on a wide range of tasks including natural language understanding, machine translation, and ima
    
[^79]: 城市健康联系的解码：可解释的机器学习揭示了基于错综复杂城市特征的癌症患病率。

    Decoding Urban-health Nexus: Interpretable Machine Learning Illuminates Cancer Prevalence based on Intertwined City Features. (arXiv:2306.11847v1 [cs.LG])

    [http://arxiv.org/abs/2306.11847](http://arxiv.org/abs/2306.11847)

    本研究通过XGBoost机器学习模型揭示了社会人口统计学、建筑环境特征和环境危害曝露特征是影响城市癌症患病率的关键，结合因果推断实验提出了增加绿化空间、减少开发区和总排放量可以缓解癌症的发生。

    

    本研究调查了社会人口统计学、建筑环境特征和环境危害曝露特征在决定社区级癌症患病率中的相互作用。利用美国五个大都市统计区的数据：芝加哥、达拉斯、休斯敦、洛杉矶和纽约，本研究采用XGBoost机器学习模型预测了癌症患病率的程度，并评估了不同特征的重要性。我们的模型表现出可靠的性能，结果表明年龄、少数民族身份和人口密度是影响癌症患病率的最重要因素之一。我们进一步探讨了城市发展和设计策略，以减少癌症患病率，重点关注绿地、开发区和总排放量。通过一系列基于因果推断的实验评估，结果表明增加绿化空间，减少开发区和总排放量可以缓解癌症的发生。

    This study investigates the interplay among social demographics, built environment characteristics, and environmental hazard exposure features in determining community level cancer prevalence. Utilizing data from five Metropolitan Statistical Areas in the United States: Chicago, Dallas, Houston, Los Angeles, and New York, the study implemented an XGBoost machine learning model to predict the extent of cancer prevalence and evaluate the importance of different features. Our model demonstrates reliable performance, with results indicating that age, minority status, and population density are among the most influential factors in cancer prevalence. We further explore urban development and design strategies that could mitigate cancer prevalence, focusing on green space, developed areas, and total emissions. Through a series of experimental evaluations based on causal inference, the results show that increasing green space and reducing developed areas and total emissions could alleviate can
    
[^80]: 基于注意力知识图卷积网络的旅游景点推荐

    Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2306.10946](http://arxiv.org/abs/2306.10946)

    本文提出了一种基于注意力知识图卷积网络的旅游景点推荐模型，通过自动语义发掘目标景点的相邻实体，根据旅客的喜好选择，预测类似景点的概率，实验中取得良好效果。

    

    基于知识图谱的推荐算法在相对成熟阶段，但在特定领域的推荐仍存在问题。例如在旅游领域，选择适合的旅游景点属性流程作为推荐基础较为复杂。本文提出改进的注意力知识图卷积网络模型(Att-KGCN)，自动语义地发掘目标景点的相邻实体，利用注意力层将相对相似的位置进行聚合，并通过推理旅客喜好选择，预测类似景点的概率作为推荐系统。实验中，采用索科特拉岛-也门的旅游数据，证明了注意力知识图卷积网络在旅游领域的景点推荐效果良好。

    The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
    
[^81]: ExpPoint-MAE：自监督点云Transformer的更好可解释性和性能

    ExpPoint-MAE: Better interpretability and performance for self-supervised point cloud transformers. (arXiv:2306.10798v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.10798](http://arxiv.org/abs/2306.10798)

    本文探讨了自监督Transformer在点云领域中的特性，并发现预训练方案能够帮助更好地理解基础几何，提出一种解冻策略，在不引入其他修改的情况下始终优于基线，并在Transformer模型中取得了最佳结果。

    

    本论文深入探讨了自监督Transformer在点云领域中的特性。具体而言，我们评估了Masked Autoencoding作为预训练方案的有效性，并探索了Momentum Contrast作为替代方案。通过全面的可视化，我们观察到Transformer学习关注语义上有意义的区域，表明预训练有助于更好地理解基础几何。此外，我们还研究了微调过程及其对所学表示的影响。基于此，我们设计了一种解冻策略，它在不引入任何其他修改模型或训练流程的情况下始终优于我们的基线，并在Transformer模型中在分类任务中取得了最佳结果。

    In this paper we delve into the properties of transformers, attained through self-supervision, in the point cloud domain. Specifically, we evaluate the effectiveness of Masked Autoencoding as a pretraining scheme, and explore Momentum Contrast as an alternative. In our study we investigate the impact of data quantity on the learned features, and uncover similarities in the transformer's behavior across domains. Through comprehensive visualiations, we observe that the transformer learns to attend to semantically meaningful regions, indicating that pretraining leads to a better understanding of the underlying geometry. Moreover, we examine the finetuning process and its effect on the learned representations. Based on that, we devise an unfreezing strategy which consistently outperforms our baseline without introducing any other modifications to the model or the training pipeline, and achieve state-of-the-art results in the classification task among transformer models.
    
[^82]: 基于潜在扩散模型的文本驱动Foley音效生成

    Text-Driven Foley Sound Generation With Latent Diffusion Model. (arXiv:2306.10359v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.10359](http://arxiv.org/abs/2306.10359)

    本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。我们通过迁移学习对系统进行微调，并引入可训练的层来改善文本嵌入，同时也改进了生成的波形。

    

    Foley音效生成旨在为多媒体内容生成背景音效。先前的模型通常使用大量有标签的开发集作为输入（例如，单个数字或one-hot向量）。本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。为了缓解数据稀缺问题，我们的模型首先使用大规模数据集进行预训练，然后通过对比语言-音频配对（CLAP）技术进行迁移学习来对该任务进行微调。我们观察到，文本编码器提取的特征嵌入可以显著影响生成模型的性能。因此，我们在编码器之后引入可训练的层来改善编码器产生的文本嵌入。此外，我们通过同时生成多个候选音频片段并选择最佳片段来进一步改进生成的波形，最佳片段是根据嵌入之间相似性得分确定的。

    Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion model based system for Foley sound generation with text conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the 
    
[^83]: 预测晶体性质的完整相互作用势的高效逼近

    Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10045](http://arxiv.org/abs/2306.10045)

    本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。

    

    我们研究了晶体材料的性质预测。晶体结构由一个最小的单元格组成，在三维空间中无限重复。如何在机器学习模型中准确表示这种重复结构仍然没有解决。当前的方法只在附近的节点之间建立边缘来构建图形，因此无法忠实地捕捉无限重复的模式和远距离的原子间相互作用。在这项工作中，我们提出了几个创新来克服这些限制。首先，我们建议直接建模物理原理的相互作用势，而不仅仅使用距离，如许多现有方法所做的。这些势包括库仑势，伦敦分散势和Pauli斥力势。其次，我们建模所有原子之间的完整势，而不仅仅是现有方法中的附近原子之间的势。这得益于我们用可证明的误差界逼近无限势和的方法。我们进一步开发了...

    We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
    
[^84]: LEACE：闭合形式中的完美线性概念擦除

    LEACE: Perfect linear concept erasure in closed form. (arXiv:2306.03819v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03819](http://arxiv.org/abs/2306.03819)

    本文介绍了一种闭合形式的方法LEACE，可在删除指定特征的同时尽可能少地改变表示，并可证明防止所有线性分类器检测到概念。作者用“概念擦除”这一新方法将其应用于大型语言模型，在测量语言模型对词性的依赖性和减少BERT嵌入中的性别偏差任务中得出良好表现。

    

    概念擦除旨在从表征中删除指定的特征。它可以提高公平性（例如，防止分类器使用性别或种族）和可解释性（例如，删除概念以观察模型行为的变化）。我们引入了LEAst-squares概念擦除（LEACE），这是一种闭合形式的方法，可证明防止所有线性分类器检测到概念，同时尽可能地改变表示，如广泛类别的范数所测量的那样。我们使用名为“概念擦除”的新方法将LEACE应用于大型语言模型，擦除每个层中的目标概念信息。我们在两个任务上展示了我们的方法：测量语言模型对词性信息的依赖性，以及减少BERT嵌入中的性别偏差。代码可在https://github.com/EleutherAI/concept-erasure上找到。

    Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called "concept scrubbing," which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.
    
[^85]: DVFO：DNN边缘推理的动态电压、频率缩放和工作负载卸载

    DVFO: Dynamic Voltage, Frequency Scaling and Workload Offloading for DNN Edge Inference. (arXiv:2306.01811v1 [cs.LG])

    [http://arxiv.org/abs/2306.01811](http://arxiv.org/abs/2306.01811)

    提出了一种动态电压、频率缩放和工作负载卸载的DNN边缘推理框架DVFO，它通过深度强化学习来联合优化DVFS和卸载参数，实现了对边缘设备计算资源的优化，同时提高了DNN模型的能源效率。

    

    由于边缘设备资源限制和深度神经网络（DNN）模型的不同特性，优化边缘设备上DNN推理性能（在能源消耗和推理延迟方面）是一个巨大的挑战。除了动态电压频率缩放（DVFS）技术，边缘云架构提供了一种协作方法，以实现高效的DNN推理。然而，当前的边缘云协作推理方法尚未对边缘设备上的各种计算资源进行优化。因此，我们提出了DVFO，这是一种新颖的基于DVFS的边缘云协作推理框架，它通过深度强化学习（DRL）联合优化DVFS和卸载参数。具体来说，DVFO自动共同优化了1）边缘设备的CPU、GPU和内存频率，以及2）要卸载到云服务器的特征映射。此外，它利用一种思考即行动的并发机制加速DRL学习过程，并利用空间通道关注机制进一步降低DNN模型的能源消耗。在真实数据集上的实验结果表明，DVFO在推理准确度和能源效率方面均优于现有的先进方法。

    Due to edge device resource constraints and different characteristics of deep neural network (DNN) models, it is a big challenge to optimize DNN inference performance in terms of energy consumption and inference latency on edge devices. In addition to the dynamic voltage frequency scaling (DVFS) technique, the edge-cloud architecture provides a collaborative approach to efficient DNN inference. However, current edge-cloud collaborative inference methods have not optimized various compute resources on edge devices. Thus, we propose DVFO, a novel DVFS-enabled edge-cloud collaborative inference framework, which jointly optimize DVFS and offloading parameters via deep reinforcement learning (DRL). Specifically, DVFO automatically co-optimizes 1) CPU, GPU and memory frequencies of edge devices, and 2) feature maps to be offloaded to cloud servers. In addition, it leverages a thinking-while-moving concurrent mechanism to accelerate the DRL learning process, and a spatialchannel attention mec
    
[^86]: 基于机器学习的焦虑检测中噪声影响的比较研究。

    Comparative Study on the Effects of Noise in ML-Based Anxiety Detection. (arXiv:2306.01110v1 [cs.LG])

    [http://arxiv.org/abs/2306.01110](http://arxiv.org/abs/2306.01110)

    本研究探究了噪声如何影响基于机器学习的焦虑检测模型，并开发出在嘈杂的现实环境中具有抗干扰性和适应性的模型，以推进该领域的发展。

    

    穿戴式健康设备正在引领一种新时代的连续和非侵入性远程监测。其中一项应用是用于焦虑检测。许多焦虑检测方面的进展发生在受控实验室环境中，但噪声阻止了这些进展推广到现实世界的条件下。本研究旨在研究噪声如何影响模型性能并开发对嘈杂的现实环境具有抗干扰性和适应日常生活中混乱的模型，从而推进该领域的发展。我们尝试研究先前的方法为何失败，并使用可穿戴负荷与情感检测（WESAD）数据集，在三类分类问题（基准值 vs. 压力 vs. 愉悦）中比较不同强度噪声对机器学习模型分类生理唤醒等级的影响。在引入噪声之前，我们基准模型的性能达到了98.7％，而Schmidt 2018年的模型仅达到了80.3％。我们讨论了可能的解决方法。

    Wearable health devices are ushering in a new age of continuous and noninvasive remote monitoring. One application of this technology is in anxiety detection. Many advancements in anxiety detection have happened in controlled lab settings, but noise prevents these advancements from generalizing to real-world conditions. We seek to progress the field by studying how noise impacts model performance and developing models that are robust to noisy, real-world conditions and, hence, attuned to the commotion of everyday life. In this study we look to investigate why and how previous methods have failed. Using the wearable stress and affect detection (WESAD) dataset, we compare the effect of various intensities of noise on machine learning models classifying levels of physiological arousal in the three-class classification problem: baseline vs. stress vs. amusement. Before introducing noise, our baseline model performance reaches 98.7%, compared to Schmidt 2018's 80.3%. We discuss potential so
    
[^87]: 在雾环境中进行预测复制的时间运动预测

    Predicting Temporal Aspects of Movement for Predictive Replication in Fog Environments. (arXiv:2306.00575v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2306.00575](http://arxiv.org/abs/2306.00575)

    本文研究了时间预测在雾环境下预测复制中的应用，提出了一种利用Holder-Winter指数平滑法进行时间预测的新模型，可以在减少多余数据的同时，只有微小的数据可用性降低。

    

    充分利用雾环境的好处，有效地管理数据位置非常重要。盲目或反应式的数据复制无法充分利用雾计算的潜力，需要更先进的技术来预测客户端何时何地连接。虽然空间预测受到了相当多的关注，但时间预测仍未得到充分研究。本文通过研究将时间预测纳入现有空间预测模型的优势来填补这一空白。我们还在预测复制的背景下对时空预测模型（如深度神经网络和马尔可夫模型）进行全面分析。我们提出了一种利用Holder-Winter指数平滑法进行时间预测的新模型，利用顺序和周期性用户移动模式。在模拟真实用户轨迹的雾网络中，我们的模型在多余数据上实现了15％的降低，而数据可用性只有1％的微小降低。

    To fully exploit the benefits of the fog environment, efficient management of data locality is crucial. Blind or reactive data replication falls short in harnessing the potential of fog computing, necessitating more advanced techniques for predicting where and when clients will connect. While spatial prediction has received considerable attention, temporal prediction remains understudied.  Our paper addresses this gap by examining the advantages of incorporating temporal prediction into existing spatial prediction models. We also provide a comprehensive analysis of spatio-temporal prediction models, such as Deep Neural Networks and Markov models, in the context of predictive replication. We propose a novel model using Holt-Winter's Exponential Smoothing for temporal prediction, leveraging sequential and periodical user movement patterns. In a fog network simulation with real user trajectories our model achieves a 15% reduction in excess data with a marginal 1% decrease in data availabi
    
[^88]: Bayesian隐式神经表示下的压缩

    Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])

    [http://arxiv.org/abs/2305.19185](http://arxiv.org/abs/2305.19185)

    该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。

    

    许多常见类型的数据可以表示为将坐标映射到信号值的函数，例如图像中的像素位置到RGB值。基于这个观点，可以通过对数据的功能表示进行超拟合，然后编码网络权重来压缩数据。然而，大多数当前的解决方案都效率低下，因为将精度量化到低比特会大幅降低重构质量。为解决这个问题，我们提出了过度拟合变分贝叶斯神经网络来压缩近似后验权重样本，而不是量化和熵编码它。该策略通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。此外，我们引入了一种学习先验权重分布的迭代算法，并采用主动尺寸调整来进一步提高效率。

    Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro
    
[^89]: 重新审视自动提示：我们真的做得更好吗？

    Revisiting Automated Prompting: Are We Actually Doing Better?. (arXiv:2304.03609v1 [cs.CL])

    [http://arxiv.org/abs/2304.03609](http://arxiv.org/abs/2304.03609)

    本文重审自动提示技术在六个不同的任务和更广泛范围的K-shot学习设置上的表现，发现自动提示并不能始终优于手动提示，因此手动提示应该作为自动提示的一个基准线。

    

    当前的文献表明，大型语言模型(LLM)是出色的几乎不用学习的学习者，在几乎不用学习的情况下，提示显着提高了它们在多个下游任务中的表现。随后进行了试图自动化人类提示的尝试，并取得了一定进展。特别是，随后的工作表明，在某些K-shot学习场景中，自动化可以优于微调。在本文中，我们重新审视了自动提示在六个不同的下游任务和更大范围的K-shot学习设置上的技术。我们发现，自动提示不能始终优于简单的手动提示。我们的工作表明，在这一研究领域中，除了微调之外，手动提示应作为基线使用。

    Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates automation can outperform fine-tuning in certain K-shot learning scenarios.  In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompts. Our work suggests that, in addition to fine-tuning, manual prompts should be used as a baseline in this line of research.
    
[^90]: 分离的关注力：基于上下文分离槽的无监督多对象发现

    Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots. (arXiv:2304.01430v1 [cs.CV])

    [http://arxiv.org/abs/2304.01430](http://arxiv.org/abs/2304.01430)

    该论文提出了一种新的无监督多对象发现方法，通过一种上下文分隔的槽结构来将视觉场分割为独立运动区域，并用对抗性标准来保证解码器无法重构整个光流。

    

    我们提出了一种将视觉场分割为独立运动区域的方法，不需要任何基础真值或监督。它由基于槽关注的对抗条件编码器-解码器架构组成，修改为使用图像作为上下文来解码光流，而不是尝试重构图像本身。在结果的多模式表示中，一种模式（流）将馈送给编码器以产生单独的潜在代码（槽），而另一种模式（图像）将决定解码器从槽生成第一个模式（流）。由于惯常的自编码基于最小化重构误差，并不能防止整个流被编码到一个槽中，因此我们将损失修改为基于上下文信息分离的对抗性标准。

    We introduce a method to segment the visual field into independently moving regions, trained with no ground truth or supervision. It consists of an adversarial conditional encoder-decoder architecture based on Slot Attention, modified to use the image as context to decode optical flow without attempting to reconstruct the image itself. In the resulting multi-modal representation, one modality (flow) feeds the encoder to produce separate latent codes (slots), whereas the other modality (image) conditions the decoder to generate the first (flow) from the slots. This design frees the representation from having to encode complex nuisance variability in the image due to, for instance, illumination and reflectance properties of the scene. Since customary autoencoding based on minimizing the reconstruction error does not preclude the entire flow from being encoded into a single slot, we modify the loss to an adversarial criterion based on Contextual Information Separation. The resulting min-m
    
[^91]: 直接迭代反演：图像修复的替代方法

    Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration. (arXiv:2303.11435v1 [eess.IV])

    [http://arxiv.org/abs/2303.11435](http://arxiv.org/abs/2303.11435)

    InDI是一种新的监督式图像修复公式，通过逐步改进图像质量来生成比现有回归方法更真实和详细的图像，修复效果更具有感知质量。

    

    直接迭代反演（InDI）是一种新的监督式图像修复公式，它避免了所谓的“均值回归”效应，并生成比现有回归方法更真实和详细的图像。它通过逐步改进图像质量来实现，类似于生成式去噪扩散模型。图像修复是一个欠定问题，多个高质量图像都可能是给定低质量输入的可行重构。因此，单步回归模型的结果通常是所有可能解释的聚合结果，因此缺乏细节和真实感。

    Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.  Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. % The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.  While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic f
    
[^92]: SC-Block：实体解析流水线中的监督对比阻塞

    SC-Block: Supervised Contrastive Blocking within Entity Resolution Pipelines. (arXiv:2303.03132v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2303.03132](http://arxiv.org/abs/2303.03132)

    本研究提出了SC-Block，一种阻塞方法，利用监督对比学习来定位嵌入空间中的记录并使用最近邻搜索建立候选集合，实验证明该方法在保证精度的同时显著缩短了运行时间。

    

    实体解析的目标是识别在多个数据集中代表同一实际实体的记录。为了减少运行时间，实体解析流水线由两部分构成：使用计算成本低的方法选择候选记录对的阻塞器和使用计算成本高的方法从该集合中识别匹配对的匹配器。本文提出了SC-Block，一种阻塞方法，它利用监督对比学习来定位嵌入空间中的记录，并使用最近邻搜索来建立候选集合。我们将SC-Block与八种最先进的阻塞方法进行了基准测试。为了将SC-Block的训练时间与实体解析流水线的总运行时间的缩短联系起来，我们将SC-Block与四种匹配方法结合成完整的流水线。为了测量总体运行时间，我们使用了两个真实数据集。实验结果表明，SC-Block在保证精度的同时显著缩短了运行时间，比现有方法更优秀。

    The goal of entity resolution is to identify records in multiple datasets that represent the same real-world entity. However, comparing all records across datasets can be computationally intensive, leading to long runtimes. To reduce these runtimes, entity resolution pipelines are constructed of two parts: a blocker that applies a computationally cheap method to select candidate record pairs, and a matcher that afterwards identifies matching pairs from this set using more expensive methods. This paper presents SC-Block, a blocking method that utilizes supervised contrastive learning for positioning records in the embedding space, and nearest neighbour search for candidate set building. We benchmark SC-Block against eight state-of-the-art blocking methods. In order to relate the training time of SC-Block to the reduction of the overall runtime of the entity resolution pipeline, we combine SC-Block with four matching methods into complete pipelines. For measuring the overall runtime, we 
    
[^93]: 基于马尔科夫抽样方案的随机梯度下降算法研究

    Stochastic Gradient Descent under Markovian Sampling Schemes. (arXiv:2302.14428v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.14428](http://arxiv.org/abs/2302.14428)

    本文研究了基于马尔科夫抽样方案的随机梯度下降算法，提出了MC-SAG算法实现了用于分布式算法的通信效率高的token 算法。

    

    本文研究了一种变形的随机梯度下降算法，其中优化器只能访问马尔科夫抽样方案。这些方案涵盖从具有随机行走者（token算法）的分散优化到RL和在线系统识别问题的应用。我们专注于在对基础马尔科夫链和优化函数施加最不限制性的假设的情况下获得收敛速度。我们首先揭示了样本随机梯度沿着马尔可夫链路径抽样的方法的理论下界，使出现了对基础马尔可夫链的命中时间的依赖性。然后，我们研究了比之前作品更温和的规律性假设下的Markov链SGD（MC-SGD）。最后，我们介绍了MC-SAG，这是MC-SGD的一种带有方差缩减的替代方案，仅取决于马尔可夫链的碰撞时间，因此获得了通信效率高的token 算法。

    We study a variation of vanilla stochastic gradient descent where the optimizer only has access to a Markovian sampling scheme. These schemes encompass applications that range from decentralized optimization with a random walker (token algorithms), to RL and online system identification problems. We focus on obtaining rates of convergence under the least restrictive assumptions possible on the underlying Markov chain and on the functions optimized. We first unveil the theoretical lower bound for methods that sample stochastic gradients along the path of a Markov chain, making appear a dependency in the hitting time of the underlying Markov chain. We then study Markov chain SGD (MC-SGD) under much milder regularity assumptions than prior works. We finally introduce MC-SAG, an alternative to MC-SGD with variance reduction, that only depends on the hitting time of the Markov chain, therefore obtaining a communication-efficient token algorithm.
    
[^94]: 缺失值插值的转换分布匹配方法

    Transformed Distribution Matching for Missing Value Imputation. (arXiv:2302.10363v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10363](http://arxiv.org/abs/2302.10363)

    本文提出一种通过将两个数据批次转换为一个潜在空间并进行分布匹配的方法来插补缺失值，在大量实验中达到了最先进的性能。

    

    本文研究了数据集中缺失值插值的问题，该问题在许多领域有重要应用。缺失值插值的关键是利用不完整样本捕获数据分布，并相应地插补缺失值。本文通过利用任意两个具有缺失值的数据批次来自同一数据分布的事实，建议通过深度可逆函数将它们变换为潜在空间并进行分布匹配来插补两个样本批次的缺失值。为了同时学习变换和插值缺失值，提出了一个简单而有动机的算法。该算法有较少的超参数需调整，无论缺失值如何生成，都能生成高质量的插补结果。在大量数据集和竞争基准算法的广泛实验中，证明了我们的方法达到了最先进的性能。

    We study the problem of imputing missing values in a dataset, which has important applications in many domains. The key to missing value imputation is to capture the data distribution with incomplete samples and impute the missing values accordingly. In this paper, by leveraging the fact that any two batches of data with missing values come from the same data distribution, we propose to impute the missing values of two batches of samples by transforming them into a latent space through deep invertible functions and matching them distributionally. To learn the transformations and impute the missing values simultaneously, a simple and well-motivated algorithm is proposed. Our algorithm has fewer hyperparameters to fine-tune and generates high-quality imputations regardless of how missing values are generated. Extensive experiments over a large number of datasets and competing benchmark algorithms show that our method achieves state-of-the-art performance.
    
[^95]: 双重强化学习：强化学习和模仿学习的统一和新方法

    Dual RL: Unification and New Methods for Reinforcement and Imitation Learning. (arXiv:2302.08560v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08560](http://arxiv.org/abs/2302.08560)

    这篇论文介绍了双重强化学习的概念，并在一个统一的框架下解释了几种最新深度强化学习算法及模仿学习方法。作者提出了双重模仿学习方法（DIL）直接最小化策略之间的距离，并提出了一种新的离线演员-评论家方法。

    

    强化学习的目标是最大化期望累积回报。研究表明，这个目标可以通过在线性约束下优化状态-动作访问分布的优化问题来表示。这个表述的对偶问题，我们称之为双重强化学习，是无约束的并且更容易优化。我们展示了几个最先进的离线和在线强化学习算法以及模仿学习可以在一个统一的框架下被视为双重强化学习方法。这种统一提供了一个共同的基础，可以研究和识别这些方法成功的构成部分，并揭示这些方法的共同缺点和改进的新见解。我们的分析表明，以前的离线模仿学习方法基于一个不现实的覆盖率假设，并最小化了学习代理和专家访问分布之间的特定f-分布。我们提出的双重模仿学习方法（DIL）直接最小化策略之间的距离。在同样的双重框架下，我们还提出了一种新的离线演员-评论家方法，对几个基准任务有效。

    The goal of reinforcement learning (RL) is to maximize the expected cumulative return. It has been shown that this objective can be represented by an optimization problem of the state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as dual RL, is unconstrained and easier to optimize. We show that several state-of-the-art off-policy deep reinforcement learning (RL) algorithms, under both online and offline, RL and imitation learning (IL) settings, can be viewed as dual RL approaches in a unified framework. This unification provides a common ground to study and identify the components that contribute to the success of these methods and also reveals the common shortcomings across methods with new insights for improvement. Our analysis shows that prior off-policy imitation learning methods are based on an unrealistic coverage assumption and are minimizing a particular f-divergence between the visitation distributions of the l
    
[^96]: MarioGPT: 通过大语言模型进行开放式文本关卡生成

    MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.05981](http://arxiv.org/abs/2302.05981)

    MarioGPT是第一个文本到超级马里奥兄弟游戏关卡的生成模型，通过大型语言模型实现开放式的、可控制的关卡生成。

    

    流程内容生成算法可以自动生成复杂数一致的环境。然而，使用流程内容生成方法生成反映特定意图和限制的有意义内容仍然具有挑战性。此外，许多流程内容生成算法缺乏以开放式方式生成内容的能力。最近，大型语言模型在许多不同领域都表现出了非常高的效率。这些训练有素的大型语言模型可以进行微调，重复使用信息并加速新任务的培训。在这项工作中，我们介绍了MarioGPT，这是一个经过优化的GPT2模型，用于生成基于瓷砖的游戏关卡，我们以超级马里奥兄弟的关卡为例。我们展示了MarioGPT不仅可以生成不同的游戏关卡，而且可以通过文本提示控制关卡生成，解决了当前PCG技术的主要挑战之一。据我们所知，MarioGPT是第一个文本到关卡模型。

    Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We a
    
[^97]: 线性相关噪声下的梯度下降：理论及应用于差分隐私中

    Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy. (arXiv:2302.01463v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01463](http://arxiv.org/abs/2302.01463)

    本文研究梯度下降在线性相关噪声下的表现，提出了新的矩阵分解方法用于不同ially private optimization。

    

    我们研究了在线性相关噪声下的梯度下降。我们的工作是由最近针对具有差分隐私（DP）的优化的实践方法所启发的，例如DP-FTRL，在无法使用隐私放大技术的情况下（例如在联邦学习中），这些方法通过矩阵分解机制注入隐私噪声，使噪声在迭代过程中呈线性相关关系。我们提出了一种简化的环境，精简了这些方法的关键面貌，并分离了线性相关噪声的影响。我们分析了梯度下降在这种情况下的行为，无论是凸函数还是非凸函数。我们的分析明显比之前的工作更紧，并精确地恢复了多个重要的特殊情况（包括反相关扰动梯度下降）。我们使用我们的结果开发了新的，有效的矩阵分解方法，用于不同ially private optimization，并突出了这些因子分解方法的好处

    We study gradient descent under linearly correlated noise. Our work is motivated by recent practical methods for optimization with differential privacy (DP), such as DP-FTRL, which achieve strong performance in settings where privacy amplification techniques are infeasible (such as in federated learning). These methods inject privacy noise through a matrix factorization mechanism, making the noise linearly correlated over iterations. We propose a simplified setting that distills key facets of these methods and isolates the impact of linearly correlated noise. We analyze the behavior of gradient descent in this setting, for both convex and non-convex functions. Our analysis is demonstrably tighter than prior work and recovers multiple important special cases exactly (including anticorrelated perturbed gradient descent). We use our results to develop new, effective matrix factorizations for differentially private optimization, and highlight the benefits of these factorizations theoretica
    
[^98]: 基于循环图卷积网络的机载雷达时空降雪预测

    Recurrent Graph Convolutional Networks for Spatiotemporal Prediction of Snow Accumulation Using Airborne Radar. (arXiv:2302.00817v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00817](http://arxiv.org/abs/2302.00817)

    本文基于循环图卷积网络，利用机载雷达数据预测特定位置最近连续年份的积雪积累，并且相较于其他模型表现更好且更具一致性。

    

    随着气候变化和全球大气温度的上升，准确预测和估计年度积雪积累变得越来越重要。机载雷达传感器，如雪地雷达，能够测量大尺度上的积雪积累速率分布图，监测气候变化对格陵兰降水和径流的影响。该雷达传感器使用超宽带增强了垂直分辨率，有助于捕捉内部内部冰层。本文提出一种基于循环图卷积网络的机器学习模型，针对此雷达数据中先前年份的积雪积累量，预测近年来某个位置的积雪积累量。实验结果表明，相较于等效的非几何和非时态模型，该模型表现更好且更具一致性。

    The accurate prediction and estimation of annual snow accumulation has grown in importance as we deal with the effects of climate change and the increase of global atmospheric temperatures. Airborne radar sensors, such as the Snow Radar, are able to measure accumulation rate patterns at a large-scale and monitor the effects of ongoing climate change on Greenland's precipitation and run-off. The Snow Radar's use of an ultra-wide bandwidth enables a fine vertical resolution that helps in capturing internal ice layers. Given the amount of snow accumulation in previous years using the radar data, in this paper, we propose a machine learning model based on recurrent graph convolutional networks to predict the snow accumulation in recent consecutive years at a certain location. We found that the model performs better and with more consistency than equivalent nongeometric and nontemporal models.
    
[^99]: 混合精度浮点数运算的训练

    Training with Mixed-Precision Floating-Point Assignments. (arXiv:2301.13464v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13464](http://arxiv.org/abs/2301.13464)

    通过运用混合精度浮点数运算的训练方法，我们为卷积神经网络生成精度分配，相比低精度浮点数训练中考虑的精度分配，其使用更少的内存，同时也导致更准确的卷积网络。

    

    在深度神经网络训练中，将所有张量保持在高精度（例如32位甚至16位浮点数）通常是浪费的。但是，将所有张量都保持在低精度（例如8位浮点数）会导致不可接受的准确性损失。因此，使用一个精度分配 - 即将所有张量（在训练中出现的）映射到精度级别（高或低） - 非常重要，它将大多数张量保持在低精度并导致足够准确的模型。

    When training deep neural networks, keeping all tensors in high precision (e.g., 32-bit or even 16-bit floats) is often wasteful. However, keeping all tensors in low precision (e.g., 8-bit floats) can lead to unacceptable accuracy loss. Hence, it is important to use a precision assignment -- a mapping from all tensors (arising in training) to precision levels (high or low) -- that keeps most of the tensors in low precision and leads to sufficiently accurate models. We provide a technique that explores this memory-accuracy tradeoff by generating precision assignments for convolutional neural networks that (i) use less memory and (ii) lead to more accurate convolutional networks at the same time, compared to the precision assignments considered by prior work in low-precision floating-point training. We evaluate our technique on image classification tasks by training convolutional networks on CIFAR-10, CIFAR-100, and ImageNet. Our method typically provides > 2x memory reduction over a bas
    
[^100]: STEEL: 奇异性感知的强化学习

    STEEL: Singularity-aware Reinforcement Learning. (arXiv:2301.13152v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13152](http://arxiv.org/abs/2301.13152)

    这篇论文介绍了一种新的批量强化学习算法STEEL，在具有连续状态和行动的无限时马尔可夫决策过程中，不依赖于绝对连续假设，通过最大均值偏差和分布鲁棒优化确保异常情况下的性能。

    

    批量强化学习旨在利用预先收集的数据，在动态环境中找到最优策略，以最大化期望总回报。然而，几乎所有现有算法都依赖于目标策略诱导的分布绝对连续假设，以便通过变换测度使用批量数据来校准目标策略。本文提出了一种新的批量强化学习算法，不需要在具有连续状态和行动的无限时马尔可夫决策过程中绝对连续性假设。我们称这个算法为STEEL：SingulariTy-awarE rEinforcement Learning。我们的算法受到关于离线评估的新误差分析的启发，其中我们使用了最大均值偏差，以及带有分布鲁棒优化的策略定向误差评估方法，以确保异常情况下的性能，并提出了一种用于处理奇异情况的定向算法。

    Batch reinforcement learning (RL) aims at leveraging pre-collected data to find an optimal policy that maximizes the expected total rewards in a dynamic environment. Nearly all existing algorithms rely on the absolutely continuous assumption on the distribution induced by target policies with respect to the data distribution, so that the batch data can be used to calibrate target policies via the change of measure. However, the absolute continuity assumption could be violated in practice (e.g., no-overlap support), especially when the state-action space is large or continuous. In this paper, we propose a new batch RL algorithm without requiring absolute continuity in the setting of an infinite-horizon Markov decision process with continuous states and actions. We call our algorithm STEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by a new error analysis on off-policy evaluation, where we use maximum mean discrepancy, together with distributionally robust opti
    
[^101]: 图形生成模型评估的曲率滤波器

    Curvature Filtrations for Graph Generative Model Evaluation. (arXiv:2301.12906v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12906](http://arxiv.org/abs/2301.12906)

    该论文使用图形曲率描述符和拓扑数据分析中的新方法相结合，以获得用于评估图形生成模型的稳健、表达性的描述符。

    

    图形生成模型评估需要了解分布级别上的图形差异，这需要能够以有效的方式利用图形的显著属性。曲率是图形的一种属性，最近开始证明其在描述图形方面很有用。然而，其表达性质、稳定性和在模型评估中的实际效用仍然很少被探索。我们将图形曲率描述符与拓扑数据分析中的新方法相结合，以获得用于评估图形生成模型的稳健、表达性的描述符。

    Graph generative model evaluation necessitates understanding differences between graphs on the distributional level. This entails being able to harness salient attributes of graphs in an efficient manner. Curvature constitutes one such property of graphs, and has recently started to prove useful in characterising graphs. Its expressive properties, stability, and practical utility in model evaluation remain largely unexplored, however. We combine graph curvature descriptors with emerging methods from topological data analysis to obtain robust, expressive descriptors for evaluating graph generative models.
    
[^102]: 重新审视有符号传播在图神经网络中的应用

    Revisiting Signed Propagation for Graph Neural Networks. (arXiv:2301.08918v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08918](http://arxiv.org/abs/2301.08918)

    该论文重新审视了在异质图中的有符号传播方法，提出了一种适用于多类别图的新策略，并克服了其带来的不确定性和不一致性问题，取得了显著的性能提升。

    

    信息传递式图神经网络（GNN）在同质图上表现出色。然而，在异质图上它们的性能却很差，许多研究者为解决这个问题提出了大量方案。特别地，翻转边的符号是基于坚实理论基础的并且可以获得显著的性能提升。然而，以前的分析假定了二元分类场景，因此受到应用范围的限制。本文将以前的理解扩展到多类别情况，并指出两个缺点：（1）多跳邻居的符号取决于消息传递路径，可能导致不一致性；（2）这也增加了预测的不确定性（例如，冲突证据），可能影响算法的稳定性。在理论的基础上，我们提出了一种新的策略，适用于多类别的图。所提出的方案结合了原有方案的优点，同时克服了其缺点并取得了显著的性能提升。

    Message-passing Graph Neural Networks (GNNs), which collect information from adjacent nodes, achieve satisfying results on homophilic graphs. However, their performances are dismal in heterophilous graphs, and many researchers have proposed a plethora of schemes to solve this problem. Especially, flipping the sign of edges is rooted in a strong theoretical foundation, and attains significant performance enhancements. Nonetheless, previous analyses assume a binary class scenario and they may suffer from confined applicability. This paper extends the prior understandings to multi-class scenarios and points out two drawbacks: (1) the sign of multi-hop neighbors depends on the message propagation paths and may incur inconsistency, (2) it also increases the prediction uncertainty (e.g., conflict evidence) which can impede the stability of the algorithm. Based on the theoretical understanding, we introduce a novel strategy that is applicable to multi-class graphs. The proposed scheme combine
    
[^103]: 随机赌博机中的最佳臂识别: 超越$\beta-$最优性

    Best Arm Identification in Stochastic Bandits: Beyond $\beta-$optimality. (arXiv:2301.03785v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.03785](http://arxiv.org/abs/2301.03785)

    本文介绍了一种新的算法来实现随机多臂赌博机中最佳臂识别，它既具有最优性能又计算上高效。

    

    本文研究了在固定置信水平下，随机多臂赌博机中最佳臂识别（BAI）的一个未曾解决的方面。评估赌博算法的两个关键指标是计算效率和性能最优性（例如采样复杂度）。在随机BAI文献中，已经有了设计算法以实现最优性能的进展，但它们通常计算上昂贵（例如基于优化的方法）。也存在计算效率高的方法，但它们与最优性能之间存在可证明的差距（例如，前两种方法中的$\beta$-最优方法）。本文介绍了一种BAI框架和算法，该算法通过一组计算上高效的决策规则实现了最优性能。实现这一点的中心流程是一个按顺序估计最佳分配的例程，直到足够准确地估计为止。具体而言，这些估计是准确的。

    This paper investigates a hitherto unaddressed aspect of best arm identification (BAI) in stochastic multi-armed bandits in the fixed-confidence setting. Two key metrics for assessing bandit algorithms are computational efficiency and performance optimality (e.g., in sample complexity). In stochastic BAI literature, there have been advances in designing algorithms to achieve optimal performance, but they are generally computationally expensive to implement (e.g., optimization-based methods). There also exist approaches with high computational efficiency, but they have provable gaps to the optimal performance (e.g., the $\beta$-optimal approaches in top-two methods). This paper introduces a framework and an algorithm for BAI that achieves optimal performance with a computationally efficient set of decision rules. The central process that facilitates this is a routine for sequentially estimating the optimal allocations up to sufficient fidelity. Specifically, these estimates are accurate
    
[^104]: 基于迁移学习辅助的分布式深度强化学习实现网络切片

    Network Slicing via Transfer Learning aided Distributed Deep Reinforcement Learning. (arXiv:2301.03262v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2301.03262](http://arxiv.org/abs/2301.03262)

    本文提出了一种新颖的基于迁移学习辅助的多智能体深度强化学习(MADRL)方法，通过信息共享解决跨单元格跨切片的资源分配问题，加速策略部署，并在大量的仿真评估中得到验证。

    

    深度强化学习(DRL)越来越被用于处理网络切片的动态和复杂的资源管理。然而，在实际网络中部署DRL策略是复杂的，因为细胞条件是异构的。在本文中，我们提出了一种新颖的多智能体深度强化学习(MADRL)方法，通过智能地将资源分配给不同的网络切片，来协调不同的智能体之间的相似性分析来解决跨单元格跨切片的资源分配问题。我们首先设计了一种协调的MADRL方法，通过信息共享来智能地将资源分配给切片，并管理单元间干扰。其次，我们提出了一种集成的TL方法，以在不同的本地智能体之间传输学习到的DRL策略，加速策略部署。该方法包括一种新的领域和任务相似度测量方法和一种新的知识转移方法，解决了何时传输和如何传输的问题。我们使用大量的仿真评估了所提出的解决方案。

    Deep reinforcement learning (DRL) has been increasingly employed to handle the dynamic and complex resource management in network slicing. The deployment of DRL policies in real networks, however, is complicated by heterogeneous cell conditions. In this paper, we propose a novel transfer learning (TL) aided multi-agent deep reinforcement learning (MADRL) approach with inter-agent similarity analysis for inter-cell inter-slice resource partitioning. First, we design a coordinated MADRL method with information sharing to intelligently partition resource to slices and manage inter-cell interference. Second, we propose an integrated TL method to transfer the learned DRL policies among different local agents for accelerating the policy deployment. The method is composed of a new domain and task similarity measurement approach and a new knowledge transfer approach, which resolves the problem of from whom to transfer and how to transfer. We evaluated the proposed solution with extensive simul
    
[^105]: HTR模型训练的挑战：《数字时代的档案保护计划》项目反馈

    The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique. (arXiv:2212.11146v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.11146](http://arxiv.org/abs/2212.11146)

    本文介绍了使用Transkribus平台改进手写文本识别（HTR）模型性能的实践经验，包括创建转录协议、完整使用语言模型以及确定最佳使用基础模型的方法，这些方法可以将单个模型的性能提高20%以上（达到字符错误率低于5%），并讨论了HTR平台的合作性质和数据分享等挑战。

    

    手写识别技术的出现为文化遗产研究带来了新的可能性，但现在需要反思研究团队开发的经验和实践。我们自2018年以来使用Transkribus平台，致力于提高手写文本识别（HTR）模型的性能，用于转录17世纪的法语手写文本。本文报告了创建转录协议、完整使用语言模型以及确定最佳使用基础模型的影响，以帮助提高HTR模型的性能。将所有这些元素结合起来可以将单个模型的性能提高20%以上（达到字符错误率低于5%）。本文还讨论了HTR平台（如Transkribus）的合作性质以及研究人员如何分享其数据等方面的挑战。

    The arrival of handwriting recognition technologies offers new possibilities for research in heritage studies. However, it is now necessary to reflect on the experiences and the practices developed by research teams. Our use of the Transkribus platform since 2018 has led us to search for the most significant ways to improve the performance of our handwritten text recognition (HTR) models which are made to transcribe French handwriting dating from the 17th century. This article therefore reports on the impacts of creating transcribing protocols, using the language model at full scale and determining the best way to use base models in order to help increase the performance of HTR models. Combining all of these elements can indeed increase the performance of a single model by more than 20% (reaching a Character Error Rate below 5%). This article also discusses some challenges regarding the collaborative nature of HTR platforms such as Transkribus and the way researchers can share their da
    
[^106]: DeepJoin: 基于预训练语言模型的可连接表发现

    DeepJoin: Joinable Table Discovery with Pre-trained Language Models. (arXiv:2212.07588v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2212.07588](http://arxiv.org/abs/2212.07588)

    本文提出了 Deepjoin，一个基于深度学习的模型，用于精确高效地发现可连接表。通过预训练语言模型进行嵌入式检索，能够同时服务于等值和语义连接，并允许用户自定义模板。实验结果表明 Deepjoin 在效率和准确性方面显著优于现有的最先进方法。

    

    可连接表发现在数据湖管理中变得越来越重要，因为它能够丰富数据并帮助数据分析任务。现有的方法主要针对等值连接或语义连接，但要么是精确但运行时间长，要么缺乏准确性的近似解。本文提出了 Deepjoin，这是一个基于深度学习的模型，用于精确高效地发现可连接表。我们的解决方案是基于嵌入的检索，采用预训练语言模型，旨在同时服务于等值和语义连接。我们提出了一组上下文化选项，将列内容转换为文本序列。PLM读取序列，并进行微调，将列嵌入向量中，使得如果它们可以形成连接，则这些向量会更接近。Deepjoin还允许用户指定模板，以处理更复杂的连接条件。实验结果表明，Deepjoin在效率和准确性方面显著优于现有的最先进方法。

    Due to the usefulness in data enrichment for data analysis tasks, joinable table discovery has become an important operation in data lake management. Existing approaches target equi-joins, the most common way of combining tables for creating a unified view, or semantic joins, which tolerate misspellings and different formats to deliver more join results. They are either exact solutions whose running time is linear in the sizes of query column and target table repository or approximate solutions lacking precision. In this paper, we propose Deepjoin, a deep learning model for accurate and efficient joinable table discovery. Our solution is an embedding-based retrieval, which employs a pre-trained language model (PLM) and is designed as one framework serving both equi- and semantic joins. We propose a set of contextualization options to transform column contents to a text sequence. The PLM reads the sequence and is fine-tuned to embed columns to vectors such that columns are expected to b
    
[^107]: DLKoopman：一种应用于Koopman理论的深度学习软件包

    DLKoopman: A deep learning software package for Koopman theory. (arXiv:2211.08992v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08992](http://arxiv.org/abs/2211.08992)

    DLKoopman是一种深度学习软件包，可以将非线性动力系统编码为线性空间中的线性动力系统，并用于任何数据驱动学习和优化，软件包包括平均归一化绝对误差和超参数搜索工具。

    

    本文介绍了DLKoopman——一种应用于Koopman理论的深度学习软件包，它使用深度学习来学习非线性动力系统的编码，进而将其转化成线性空间中的线性动力系统。DLKoopman是一种通用的工具，适用于任何动力系统的数据驱动学习和优化。它可以在系统的单个状态（快照）的数据上进行训练，用于预测其未知状态，也可以在系统轨迹的数据上进行训练，用于预测新的初始状态的未知轨迹。DLKoopman可通过Python软件包索引（PyPI）获取，并包括详细的文档和教程。该软件包的其他贡献包括一种用于评估性能的新型指标——平均归一化绝对误差，以及一个即用型的超参数搜索工具。

    We present DLKoopman -- a software package for Koopman theory that uses deep learning to learn an encoding of a nonlinear dynamical system into a linear space, while simultaneously learning the linear dynamics. While several previous efforts have either restricted the ability to learn encodings, or been bespoke efforts designed for specific systems, DLKoopman is a generalized tool that can be applied to data-driven learning and optimization of any dynamical system. It can either be trained on data from individual states (snapshots) of a system and used to predict its unknown states, or trained on data from trajectories of a system and used to predict unknown trajectories for new initial states. DLKoopman is available on the Python Package Index (PyPI) as 'dlkoopman', and includes extensive documentation and tutorials. Additional contributions of the package include a novel metric called Average Normalized Absolute Error for evaluating performance, and a ready-to-use hyperparameter sear
    
[^108]: 基于图注意力网络与不平衡PU标签的P2E MMORPGs欺诈检测方法

    PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention Networks with Imbalanced PU Labels. (arXiv:2211.08604v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08604](http://arxiv.org/abs/2211.08604)

    本文提出了一种基于图注意力网络和PU损失函数的欺诈检测方法PU GNN，通过改进的GraphSMOTE算法来处理P2E MMORPGs欺诈检测数据集中的标签分布不平衡问题，实验证明该方法在欺诈检测方面具有良好的性能表现。

    

    最近，在大型多人在线角色扮演游戏中，游戏点卡能够直接转换为比特币、以太坊或Klaytn等加密货币，因此play-to-earn（P2E）系统的出现使得游戏物品与现实世界的价值交换比以往更加频繁。本文提出了一种新的欺诈检测方法PU GNN，该方法采用图注意力网络和PU损失函数捕捉玩家的游戏行为、P2E代币交易模式，同时采用改进的GraphSMOTE算法处理欺诈检测数据集中的标签分布不平衡问题。该方法在三个实际的P2E MMORPGs数据集上进行实验证明，取得了最新的欺诈检测性能。

    The recent advent of play-to-earn (P2E) systems in massively multiplayer online role-playing games (MMORPGs) has made in-game goods interchangeable with real-world values more than ever before. The goods in the P2E MMORPGs can be directly exchanged with cryptocurrencies such as Bitcoin, Ethereum, or Klaytn via blockchain networks. Unlike traditional in-game goods, once they had been written to the blockchains, P2E goods cannot be restored by the game operation teams even with chargeback fraud such as payment fraud, cancellation, or refund. To tackle the problem, we propose a novel chargeback fraud prediction method, PU GNN, which leverages graph attention networks with PU loss to capture both the players' in-game behavior with P2E token transaction patterns. With the adoption of modified GraphSMOTE, the proposed model handles the imbalanced distribution of labels in chargeback fraud datasets. The conducted experiments on three real-world P2E MMORPG datasets demonstrate that PU GNN achi
    
[^109]: 贝叶斯网络在使用语音和多模态数据预测抑郁及其症状的鲁棒且无偏预测方面的应用

    Bayesian Networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data. (arXiv:2211.04924v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04924](http://arxiv.org/abs/2211.04924)

    使用贝叶斯网络及语音与多模态数据，通过捕获其条件依赖性得出预测结果，更准确地预测抑郁和其相关症状。

    

    利用行为和认知信号预测重度抑郁障碍（MDD）的存在是一个非常棘手的任务。MDD的异质性临床特征意味着任何给定的语音、面部表情和/或观察到的认知模式都可能与抑郁症状的独特组合相关联。传统的判别式机器学习模型可能缺乏稳健地建模这种异质性的复杂性。然而，贝叶斯网络可能更适合这种情况。这些网络是概率图模型，通过明确捕获随机变量的条件依赖性，高效地描述了一组随机变量的联合概率分布。这个框架相比于标准的判别性建模提供了进一步的优势，可以在模型结构中融合专家意见，生成可解释的模型预测，了解预测的不确定性，并自然地处理缺失数据。本文提出了一种基于贝叶斯网络的方法，利用语音和多模态数据预测抑郁症及其相关症状。我们的方法使用一个由有和没有MDD的个体采集的语音和面部表情数据集与传统的判别式机器学习模型进行比较。结果表明，贝叶斯网络方法在准确性和鲁棒性方面优于传统方法。此外，网络结构提供了不同数据模态相互作用以更准确地预测MDD的见解。

    Predicting the presence of major depressive disorder (MDD) using behavioural and cognitive signals is a highly non-trivial task. The heterogeneous clinical profile of MDD means that any given speech, facial expression and/or observed cognitive pattern may be associated with a unique combination of depressive symptoms. Conventional discriminative machine learning models potentially lack the complexity to robustly model this heterogeneity. Bayesian networks, however, may instead be well-suited to such a scenario. These networks are probabilistic graphical models that efficiently describe the joint probability distribution over a set of random variables by explicitly capturing their conditional dependencies. This framework provides further advantages over standard discriminative modelling by offering the possibility to incorporate expert opinion in the graphical structure of the models, generating explainable model predictions, informing about the uncertainty of predictions, and naturally
    
[^110]: 基于Transformer-to-CNN知识蒸馏的高效大规模音频标记

    Efficient Large-scale Audio Tagging via Transformer-to-CNN Knowledge Distillation. (arXiv:2211.04772v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.04772](http://arxiv.org/abs/2211.04772)

    本文提出了一种基于离线知识蒸馏的训练方法，将高性能但复杂的Transformer模型转化为高效的CNN模型，从而取得了优于以前方法的预测性能和参数以及计算效率。

    

    音频谱变换器模型统治着音频标记领域，超越了以前占主导地位的卷积神经网络(CNN)。它们的优势在于能够扩展和利用像AudioSet这样的大规模数据集。但是，与CNN相比，变换器在模型大小和计算要求方面要求更高。我们提出了一种基于高性能但复杂的变换器的离线知识蒸馏(KD)的高效CNN训练过程。所提出的训练架构和基于MobileNetV3的高效CNN设计导致了参数和计算效率以及预测性能方面优于以前的解决方案的模型。我们提供了不同复杂度级别的模型，从低复杂度模型到新的AudioSet .483 mAP的最新性能。源代码可在https://github.com/fschmid56/EfficientAT上获得。

    Audio Spectrogram Transformer models rule the field of Audio Tagging, outrunning previously dominating Convolutional Neural Networks (CNNs). Their superiority is based on the ability to scale up and exploit large-scale datasets such as AudioSet. However, Transformers are demanding in terms of model size and computational requirements compared to CNNs. We propose a training procedure for efficient CNNs based on offline Knowledge Distillation (KD) from high-performing yet complex transformers. The proposed training schema and the efficient CNN design based on MobileNetV3 results in models outperforming previous solutions in terms of parameter and computational efficiency and prediction performance. We provide models of different complexity levels, scaling from low-complexity models up to a new state-of-the-art performance of .483 mAP on AudioSet. Source Code available at: https://github.com/fschmid56/EfficientAT
    
[^111]: 非马尔可夫环境下的强化学习

    Reinforcement Learning in Non-Markovian Environments. (arXiv:2211.01595v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2211.01595](http://arxiv.org/abs/2211.01595)

    本文通过递归计算近似充分统计量，提出了一种基于自编码器的代理设计方案，实现了在非马尔可夫环境中进行强化学习。

    

    本文以Van Roy及其合作者为基础，提出了在任意非马尔可夫环境中进行强化学习的新范式，并明确了当在该范式上应用Q学习算法时，由于非马尔可夫性质引起的错误。基于此观察，我们建议代理设计的标准应是寻找某些条件规律的良好近似。受经典随机控制的启发，我们证明了我们的问题归结为递归计算近似充分统计量的问题。这导致了一种基于自编码器的代理设计方案，我们在部分观察到的强化学习环境中进行了数值测试。

    Motivated by the novel paradigm developed by Van Roy and coauthors for reinforcement learning in arbitrary non-Markovian environments, we propose a related formulation and explicitly pin down the error caused by non-Markovianity of observations when the Q-learning algorithm is applied on this formulation. Based on this observation, we propose that the criterion for agent design should be to seek good approximations for certain conditional laws. Inspired by classical stochastic control, we show that our problem reduces to that of recursive computation of approximate sufficient statistics. This leads to an autoencoder-based scheme for agent design which is then numerically tested on partially observed reinforcement learning environments.
    
[^112]: 关于监督信号的信息量

    On the Informativeness of Supervision Signals. (arXiv:2211.01407v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01407](http://arxiv.org/abs/2211.01407)

    本文使用信息论比较了常用的监督信号对表示学习性能的贡献，并为在大数据时代使用硬标签提供了理论上的证明，但对于少样本学习和分布外泛化，需要使用更丰富的监督信号。

    

    监督学习通常侧重于从人类标注的训练示例中学习可转移的表示。虽然丰富的注释（如软标签）比稀疏的注释（如硬标签）提供更多信息，但它们的收集成本也更高。我们使用信息论比较了许多常用的监督信号对于表示学习性能的贡献，以及它们的能力如何受到标签数、类别、维度和噪声等因素的影响。我们的框架为在大数据时代使用硬标签提供了理论上的证明，但对于少样本学习和分布外泛化，需要使用更丰富的监督信号。

    Supervised learning typically focuses on learning transferable representations from training examples annotated by humans. While rich annotations (like soft labels) carry more information than sparse annotations (like hard labels), they are also more expensive to collect. For example, while hard labels only provide information about the closest class an object belongs to (e.g., "this is a dog"), soft labels provide information about the object's relationship with multiple classes (e.g., "this is most likely a dog, but it could also be a wolf or a coyote"). We use information theory to compare how a number of commonly-used supervision signals contribute to representation-learning performance, as well as how their capacity is affected by factors such as the number of labels, classes, dimensions, and noise. Our framework provides theoretical justification for using hard labels in the big-data regime, but richer supervision signals for few-shot learning and out-of-distribution generalizati
    
[^113]: 使用PINO-CDE求解耦合微分方程组

    Solving Coupled Differential Equation Groups Using PINO-CDE. (arXiv:2210.00222v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00222](http://arxiv.org/abs/2210.00222)

    PINO-CDE是一种用于解决耦合微分方程组的深度学习框架，通过物理-知情的神经算符理论，使用单个网络处理所有量的CDEs，实现了高精度、鲁棒性和快速计算速度。

    

    耦合微分方程组是许多工程学科中的基本数学工具，用于建模包含多个物理量的复杂结构。在设计阶段，工程师不断调整结构参数，需要高效的求解器。深度学习技术的兴起为这项任务提供了新的视角。然而，现有的黑盒模型精度和鲁棒性较差，而单输出算符回归的先进方法无法同时处理多个量。为了应对这些挑战，我们提出了PINO-CDE，这是一种用于解决耦合微分方程组(CDEs)的深度学习框架，并配备了一个方程归一化算法来提高性能。基于物理-知情的神经算符(PINO)理论，PINO-CDE使用单个网络处理CDEs中的所有量，而不是训练数十个甚至数百个网络。实验结果表明，PINO-CDE在保持快速计算速度的同时，实现了高精度和鲁棒性，成为解决复杂CDEs的一种有前途的方法。

    As a fundamental mathmatical tool in many engineering disciplines, coupled differential equation groups are being widely used to model complex structures containing multiple physical quantities. Engineers constantly adjust structural parameters at the design stage, which requires a highly efficient solver. The rise of deep learning technologies has offered new perspectives on this task. Unfortunately, existing black-box models suffer from poor accuracy and robustness, while the advanced methodologies of single-output operator regression cannot deal with multiple quantities simultaneously. To address these challenges, we propose PINO-CDE, a deep learning framework for solving coupled differential equation groups (CDEs) along with an equation normalization algorithm for performance enhancing. Based on the theory of physics-informed neural operator (PINO), PINO-CDE uses a single network for all quantities in a CDEs, instead of training dozens, or even hundreds of networks as in the existi
    
[^114]: SpeedLimit：量化Transformer模型的神经架构搜索

    SpeedLimit: Neural Architecture Search for Quantized Transformer Models. (arXiv:2209.12127v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12127](http://arxiv.org/abs/2209.12127)

    本文介绍了SpeedLimit——一种新的神经架构搜索技术，通过在量化的Transformer模型中添加上限延迟约束，优化准确性。该方法比当前最先进的技术表现更好，为在延迟敏感的环境中使用Transformer模型提供了新的可能性。

    

    尽管Transformer模型的研究主要集中在提高诸如准确性和复杂度这样的性能指标上，但实际应用通常需要严格考虑推理延迟约束。为了解决这个挑战，我们引入了SpeedLimit，一种新的神经架构搜索技术，它在保持上限延迟约束的前提下优化准确性。我们的方法在搜索过程中结合了8位整数量化，超过了当前最先进的技术。我们的结果强调了在性能和延迟之间寻求最佳平衡的可行性和有效性，为在延迟敏感的环境中部署最先进的Transformer模型提供了新的方法。

    While research in the field of transformer models has primarily focused on enhancing performance metrics such as accuracy and perplexity, practical applications in industry often necessitate a rigorous consideration of inference latency constraints. Addressing this challenge, we introduce SpeedLimit, a novel Neural Architecture Search (NAS) technique that optimizes accuracy whilst adhering to an upper-bound latency constraint. Our method incorporates 8-bit integer quantization in the search process to outperform the current state-of-the-art technique. Our results underline the feasibility and efficacy of seeking an optimal balance between performance and latency, providing new avenues for deploying state-of-the-art transformer models in latency-sensitive environments.
    
[^115]: 大步神经网络学习来自分区数据的辛演化

    Large-step neural network for learning the symplectic evolution from partitioned data. (arXiv:2208.14148v2 [astro-ph.EP] UPDATED)

    [http://arxiv.org/abs/2208.14148](http://arxiv.org/abs/2208.14148)

    本研究使用分区的方法来训练大步神经网络，学习辛哈密顿系统的演化，有效抑制累积误差，并成功保持Jacobi积分的守恒。

    

    本研究关注学习哈密顿系统，需要预测辛映射生成的坐标（q）和动量（p）变量。基于Chen＆Tao（2021）的研究，辛映射由生成函数表示。为了延长预测时间，我们将时间序列（q_i、p_i）分成几个区间，并用大步神经网络（LSNN）来逼近第一区间（即初始条件）和其余各个区间之间的生成函数。这种分区方法使我们的LSNN在预测系统演化时能有效抑制累积误差。然后，我们训练LSNN学习25000年的2：3共振柯伊伯带对象的运动。结果显示，在我们先前工作中构建的神经网络（Li等，2022）基础上，有两个显著的改进：（1）Jacobi积分的守恒，

    In this study, we focus on learning Hamiltonian systems, which involves predicting the coordinate (q) and momentum (p) variables generated by a symplectic mapping. Based on Chen & Tao (2021), the symplectic mapping is represented by a generating function. To extend the prediction time period, we develop a new learning scheme by splitting the time series (q_i, p_i) into several partitions. We then train a large-step neural network (LSNN) to approximate the generating function between the first partition (i.e. the initial condition) and each one of the remaining partitions. This partition approach makes our LSNN effectively suppress the accumulative error when predicting the system evolution. Then we train the LSNN to learn the motions of the 2:3 resonant Kuiper belt objects for a long time period of 25000 yr. The results show that there are two significant improvements over the neural network constructed in our previous work (Li et al. 2022): (1) the conservation of the Jacobi integral,
    
[^116]: 基于SPRT的随机赌博机中的最佳臂辨识问题

    SPRT-based Efficient Best Arm Identification in Stochastic Bandits. (arXiv:2207.11158v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.11158](http://arxiv.org/abs/2207.11158)

    本论文提出了一种使用SPRT框架设计的BAI算法并运用于指数族赌博机，该算法具有样本复杂度渐近最优和$\delta-$PAC保证的特点。

    

    本文研究随机多臂赌博机在固定置信度场景下的最佳臂辨识问题。考虑到广义指数族赌博机的类别。现有的指数族赌博机算法面临计算挑战。为了缓解这些挑战，最佳臂辨识问题被视为序贯复合假设检验任务进行分析，并提出了一种框架，采用基于似然比的测试，这种测试已经证明对于序列测试是有效的。基于这个检验统计量，设计了一种BAI算法，该算法利用了经典的序贯概率比测试进行臂的选择，并且易于分析指数族赌博机。该算法具有两个关键特点：（1）它的样本复杂度是渐近最优的，（2）它保证是$\delta-$PAC的。现有的有效方法集中在高斯条件下，并要求对被认为是最佳臂的臂使用Thompson采样。

    This paper investigates the best arm identification (BAI) problem in stochastic multi-armed bandits in the fixed confidence setting. The general class of the exponential family of bandits is considered. The existing algorithms for the exponential family of bandits face computational challenges. To mitigate these challenges, the BAI problem is viewed and analyzed as a sequential composite hypothesis testing task, and a framework is proposed that adopts the likelihood ratio-based tests known to be effective for sequential testing. Based on this test statistic, a BAI algorithm is designed that leverages the canonical sequential probability ratio tests for arm selection and is amenable to tractable analysis for the exponential family of bandits. This algorithm has two key features: (1) its sample complexity is asymptotically optimal, and (2) it is guaranteed to be $\delta-$PAC. Existing efficient approaches focus on the Gaussian setting and require Thompson sampling for the arm deemed the 
    
[^117]: 有选择地增加GAN生成样本的多样性

    Selectively increasing the diversity of GAN-generated samples. (arXiv:2207.01561v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.01561](http://arxiv.org/abs/2207.01561)

    提出了一种有选择性地增加GAN生成样本多样性的新方法，并通过向损失函数添加正则化进行优化。在保持生成样本质量的同时，有效增加其多样性，验证结果良好。

    

    生成对抗网络(GAN)是强大的模型，能够合成与真实数据分布密切相似的数据样本，但是GAN生成的样本多样性受到所谓的模式崩溃现象的限制。特别容易出现模式崩溃的是条件GAN，它们倾向于忽略输入噪声向量，专注于条件信息。最近提出的方法旨在减轻这种限制，增加生成的样本的多样性，但当需要样本相似性时，它们会降低模型的性能。为了解决这个缺点，我们提出了一种新方法，有选择地增加GAN生成的样本的多样性。通过向训练损失函数添加一个简单但有效的正则化，我们鼓励生成器为与多样化输出相关的输入发现新的数据模式，同时为其余其余的输入生成一致的样本。更具体地说，我们最大化不同条件输入生成的样本之间的距离比与相同条件输入生成的样本之间的距离。我们在几个基准数据集上验证了所提出的方法，并展示了它在增加GAN生成的样本的多样性的有效性，同时保持它们的质量，如Frechet Inception距离(FID)和Inception分数(IS)所衡量的质量。

    Generative Adversarial Networks (GANs) are powerful models able to synthesize data samples closely resembling the distribution of real data, yet the diversity of those generated samples is limited due to the so-called mode collapse phenomenon observed in GANs. Especially prone to mode collapse are conditional GANs, which tend to ignore the input noise vector and focus on the conditional information. Recent methods proposed to mitigate this limitation increase the diversity of generated samples, yet they reduce the performance of the models when similarity of samples is required. To address this shortcoming, we propose a novel method to selectively increase the diversity of GAN-generated samples. By adding a simple, yet effective regularization to the training loss function we encourage the generator to discover new data modes for inputs related to diverse outputs while generating consistent samples for the remaining ones. More precisely, we maximise the ratio of distances between gener
    
[^118]: 网络资源分配下的不确定性问题研究

    Online Resource Allocation under Horizon Uncertainty. (arXiv:2206.13606v3 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2206.13606](http://arxiv.org/abs/2206.13606)

    本论文研究的是随机在线资源分配问题，并且针对时间长度不确定性，提出了具有鲁棒性的在线算法。

    

    本论文探讨了随机在线资源分配问题：决策者需要将有限资源分配给以随机方式生成的顺序到达的请求，以最大化收益。在每个时间步长，请求是从一个未知于决策者的分布中独立地抽取的。在线资源分配及其特例在过去得到了广泛的研究，但以前的结果都至关重要且普遍地依赖于一个强假设：即决策者预先知道请求的总数（即时间长度）。在许多应用中，如收入管理和在线广告，由于需求或用户流量强度的波动，请求的数量可能会大幅度变化。在这项工作中，我们开发了对时间长度不确定性具有鲁棒性的在线算法。与已知时间长度环境形成鲜明对比的是，没有任何算法可以实现独立于时间长度不确定性的恒定渐近竞争比。我们引入了一种新颖的算法...

    We study stochastic online resource allocation: a decision maker needs to allocate limited resources to stochastically-generated sequentially-arriving requests in order to maximize reward. At each time step, requests are drawn independently from a distribution that is unknown to the decision maker. Online resource allocation and its special cases have been studied extensively in the past, but prior results crucially and universally rely on the strong assumption that the total number of requests (the horizon) is known to the decision maker in advance. In many applications, such as revenue management and online advertising, the number of requests can vary widely because of fluctuations in demand or user traffic intensity. In this work, we develop online algorithms that are robust to horizon uncertainty. In sharp contrast to the known-horizon setting, no algorithm can achieve even a constant asymptotic competitive ratio that is independent of the horizon uncertainty. We introduce a novel 
    
[^119]: 半自回归能量流：探索正则化流的无似然训练。

    Semi-Autoregressive Energy Flows: Exploring Likelihood-Free Training of Normalizing Flows. (arXiv:2206.06672v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06672](http://arxiv.org/abs/2206.06672)

    本文研究了正则化流的无似然训练，提出了能量目标，支持半自回归能量流等灵活的模型架构，并相对于基于似然性的流表现出有竞争力的性能，质疑了机器学习领域中以最大似然作为目标或指标的使用，为生成建模的研究做出了贡献。

    

    训练正则化流生成模型时，由于需要计算Jacobian行列式，因此会面临计算负担较重的挑战。本论文研究了流的无似然训练，并提出了能量目标，一种基于适当得分规则的替代基于样本的损失函数。能量目标是不需要行列式的，并支持灵活的模型架构，这些架构不容易与最大似然训练兼容，包括半自回归能量流，一种新颖的模型族，可以插值为全自回归和非自回归模型之间。相对于基于似然性的流，能量流具有竞争性的样本质量、后验推断和生成速度等性能；该性能与对数似然度量的质量通常非常差的质量无关。我们的发现对最大似然作为目标或指标的使用提出了质疑，并有助于对其在生成建模中所起的作用进行科学研究。

    Training normalizing flow generative models can be challenging due to the need to calculate computationally expensive determinants of Jacobians. This paper studies the likelihood-free training of flows and proposes the energy objective, an alternative sample-based loss based on proper scoring rules. The energy objective is determinant-free and supports flexible model architectures that are not easily compatible with maximum likelihood training, including semi-autoregressive energy flows, a novel model family that interpolates between fully autoregressive and non-autoregressive models. Energy flows feature competitive sample quality, posterior inference, and generation speed relative to likelihood-based flows; this performance is decorrelated from the quality of log-likelihood estimates, which are generally very poor. Our findings question the use of maximum likelihood as an objective or a metric, and contribute to a scientific study of its role in generative modeling.
    
[^120]: 基于联邦学习的测试时间鲁棒个性化方法(更新版)

    Test-Time Robust Personalization for Federated Learning. (arXiv:2205.10920v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10920](http://arxiv.org/abs/2205.10920)

    本文提出了FedTHE+方法，它能够使联邦学习模型具有鲁棒性，抵御各种测试时间分布转移的影响。

    

    联邦学习是一种机器学习模式，在这种模式下，许多客户端协作学习一个共享的全局模型，并利用分散的训练数据进行学习。个性化联邦学习通过将全局模型适应于不同的客户端，实现了在一致的本地训练和测试分布下取得有益结果。然而，在真实世界中，个性化联邦学习应用还需要更进一步：在部署期间，在演化的本地测试集下，建立能够抵御各种测试时间分布转移的联邦学习模型。本论文基于此，提出了Federated Test-time Head Ensemble plus tuning(FedTHE+)方法，它能够让联邦学习模型具有鲁棒性，抵御各种测试时间分布转移的影响。通过对CIFAR10和ImageNet上的各种神经网络架构（CNN，ResNet和Transformer）进行训练，我们展示了FedTHE+（及其计算效率更高的变种FedTHE）相对于其他强有力的竞争对手的改进。

    Federated Learning (FL) is a machine learning paradigm where many clients collaboratively learn a shared global model with decentralized training data. Personalized FL additionally adapts the global model to different clients, achieving promising results on consistent local training and test distributions. However, for real-world personalized FL applications, it is crucial to go one step further: robustifying FL models under the evolving local test set during deployment, where various distribution shifts can arise. In this work, we identify the pitfalls of existing works under test-time distribution shifts and propose Federated Test-time Head Ensemble plus tuning(FedTHE+), which personalizes FL models with robustness to various test-time distribution shifts. We illustrate the advancement of FedTHE+ (and its computationally efficient variant FedTHE) over strong competitors, by training various neural architectures (CNN, ResNet, and Transformer) on CIFAR10 andImageNet with various test d
    
[^121]: DeepGraviLens：一种用于分类引力透镜数据的多模态网络架构

    DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data. (arXiv:2205.00701v3 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2205.00701](http://arxiv.org/abs/2205.00701)

    DeepGraviLens是一种多模态神经网络，用于分类属于不同类型的引力透镜数据，具有高精度和优于现有方法的结果。

    

    引力透镜是由大质量物体产生的相对论效应，会弯曲其周围的时空。这是天体物理学中一个深入研究的课题，允许验证理论相对论结果并研究一些否则不可见的微弱天体物体。近年来，机器学习方法已被应用于支持引力透镜现象的分析，通过检测与亮度变化时间序列相关的图像数据集中的透镜效应。然而，当前的方法要么仅考虑图像而忽略时间序列数据，要么在最困难的数据集上实现相对较低的准确性。本文介绍了 DeepGraviLens，这是一种新颖的多模态网络，用于分类属于一个非透镜系统类型和三个透镜系统类型的时空数据。它在准确性方面超过当前的 state-of-art 方法，提高了约 19% 到 43%，具体取决于所考虑的数据集。

    Gravitational lensing is the relativistic effect generated by massive bodies, which bend the space-time surrounding them. It is a deeply investigated topic in astrophysics and allows validating theoretical relativistic results and studying faint astrophysical objects that would not be visible otherwise. In recent years Machine Learning methods have been applied to support the analysis of the gravitational lensing phenomena by detecting lensing effects in data sets consisting of images associated with brightness variation time series. However, the state-of-art approaches either consider only images and neglect time-series data or achieve relatively low accuracy on the most difficult data sets. This paper introduces DeepGraviLens, a novel multi-modal network that classifies spatio-temporal data belonging to one non-lensed system type and three lensed system types. It surpasses the current state of the art accuracy results by $\approx$ 19% to $\approx$ 43%, depending on the considered dat
    
[^122]: 受限非凸优化中具有相关数据的一阶方法的收敛性分析

    Convergence of First-Order Methods for Constrained Nonconvex Optimization with Dependent Data. (arXiv:2203.15797v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2203.15797](http://arxiv.org/abs/2203.15797)

    本文提出了一种可用于受限非凸优化中相关数据采样的一阶方法，并通过采用更加温和的混合条件，将复杂度从$\tilde{O}(\varepsilon^{-8})$提升至$\tilde{O}(\varepsilon^{-4})$。

    

    本文针对受限光滑非凸优化问题，分析了在一般的相关数据采样方案下的经典随机投影梯度方法。我们证明了利用Moreau包络和梯度映射范数实现$\varepsilon$-近似稳定点的最坏情况收敛速率为$\tilde{O}(t^{-1/4})$，复杂度为$\tilde{O}(\varepsilon^{-4})$。传统的收敛保证需要从目标分布中进行i.i.d.数据采样，而我们只需要对条件分布进行一种较温和的混合条件即可，该条件适用于广泛的马尔可夫链采样算法。相较于现有的受限光滑非凸优化和相关数据的复杂度为$\tilde{O}(\varepsilon^{-8})$的情况，我们提出的方法经过简化分析后复杂度为$\tilde{O}(\varepsilon^{-4})$。最后，我们演示了相关数据情况下随机近端梯度方法的收敛性。

    We focus on analyzing the classical stochastic projected gradient methods under a general dependent data sampling scheme for constrained smooth nonconvex optimization. We show the worst-case rate of convergence $\tilde{O}(t^{-1/4})$ and complexity $\tilde{O}(\varepsilon^{-4})$ for achieving an $\varepsilon$-near stationary point in terms of the norm of the gradient of Moreau envelope and gradient mapping. While classical convergence guarantee requires i.i.d. data sampling from the target distribution, we only require a mild mixing condition of the conditional distribution, which holds for a wide class of Markov chain sampling algorithms. This improves the existing complexity for the constrained smooth nonconvex optimization with dependent data from $\tilde{O}(\varepsilon^{-8})$ to $\tilde{O}(\varepsilon^{-4})$ with a significantly simpler analysis. We illustrate the generality of our approach by deriving convergence results with dependent data for stochastic proximal gradient methods, 
    
[^123]: 单程收益管理中考虑建议的优化分配问题

    Single-Leg Revenue Management with Advice. (arXiv:2202.10939v3 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2202.10939](http://arxiv.org/abs/2202.10939)

    本文提出了一种算法建议框架解决了单程收益管理问题，试图应用机器学习方法的预测准确性并平衡准确性和不准确性的竞争比率。

    

    单程收益管理是收益管理中的基础问题，特别是在航空和酒店行业有着很大的影响：给定$n$个资源单位，例如航班座位，并且顺序到达的客户根据不同价格分段，我们需要确定这个资源的最优在线分配策略。以前的研究聚焦于设计当预测可用时的算法，但是这种算法对于预测的不准确性并不稳健，或者是在线算法以保证最差性能，然而这样实践中可能过于保守，本文通过算法建议框架，旨在通过最佳地整合有关未来的建议进入在线算法中，试图利用机器学习方法日益增加的预测准确性解决单程收益管理问题。具体地，我们表征了捕获一致性（建议准确时的性能）和竞争比率（建议不准确时的性能）之间的权衡的帕累托前沿。我们分析了几种自然算法和基于神经网络的策略在此问题的实例上的性能，并观察到帕累托最优方法可以有效平衡这两个目标。

    Single-leg revenue management is a foundational problem of revenue management that has been particularly impactful in the airline and hotel industry: Given $n$ units of a resource, e.g. flight seats, and a stream of sequentially-arriving customers segmented by fares, what is the optimal online policy for allocating the resource. Previous work focused on designing algorithms when forecasts are available, which are not robust to inaccuracies in the forecast, or online algorithms with worst-case performance guarantees, which can be too conservative in practice. In this work, we look at the single-leg revenue management problem through the lens of the algorithms-with-advice framework, which attempts to harness the increasing prediction accuracy of machine learning methods by optimally incorporating advice about the future into online algorithms. In particular, we characterize the Pareto frontier that captures the tradeoff between consistency (performance when advice is accurate) and compet
    
[^124]: 实例自适应视频压缩：通过对测试集进行训练来提高神经编解码器

    Instance-Adaptive Video Compression: Improving Neural Codecs by Training on the Test Set. (arXiv:2111.10302v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2111.10302](http://arxiv.org/abs/2111.10302)

    该论文提出了一种基于实例自适应学习的视频压缩算法，通过对测试集进行训练来提高神经编解码器的性能，并将最优参数与潜在编码一起传输到接收端。该算法在多个数据集上有着显著的性能提升，并且可以提高任何神经视频编解码器的性能。

    

    我们介绍了一种基于实例自适应学习的视频压缩算法。在要传输的每个视频序列上，我们微调预训练的压缩模型。最优参数与潜在编码一起传输到接收端。通过在适当的混合模型先验下熵编码参数更新，我们确保网络参数可以高效地编码。这种实例自适应压缩算法对于基模型的选择是不可知的，有潜力提高任何神经视频编解码器的性能。在UVG、HEVC和Xiph数据集上，我们的编解码器将流空间模型的性能提高了21%至27%的BD-rate节省，以及最先进的B帧模型的17%至20%的BD-rate节省。我们还展示了实例自适应微调改善了对领域转移的鲁棒性。最后，我们的方法减少了压缩模型的容量需求。我们展示了即使减少网络大小，我们的方法带来了有竞争力的性能。

    We introduce a video compression algorithm based on instance-adaptive learning. On each video sequence to be transmitted, we finetune a pretrained compression model. The optimal parameters are transmitted to the receiver along with the latent code. By entropy-coding the parameter updates under a suitable mixture model prior, we ensure that the network parameters can be encoded efficiently. This instance-adaptive compression algorithm is agnostic about the choice of base model and has the potential to improve any neural video codec. On UVG, HEVC, and Xiph datasets, our codec improves the performance of a scale-space flow model by between 21% and 27% BD-rate savings, and that of a state-of-the-art B-frame model by 17 to 20% BD-rate savings. We also demonstrate that instance-adaptive finetuning improves the robustness to domain shift. Finally, our approach reduces the capacity requirements of compression models. We show that it enables a competitive performance even after reducing the net
    
[^125]: 在不产生灾难性遗忘的情况下提高预训练语言模型的性别公平性。

    Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting. (arXiv:2110.05367v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.05367](http://arxiv.org/abs/2110.05367)

    该论文提出了一种新方法GEEP，用于提高预训练语言模型的性别公平性，同时没有灾难性遗忘问题。透过性别中性数据学习性别相关的提示，GEEP实现了SOTA表现并在GLUE性能上取得了显著提高。

    

    现有的解决预训练语言模型性别偏见的研究通常建立一个小型的性别中性数据集，然后在该数据集上对模型进行第二阶段的预训练。然而，鉴于性别中性数据集的规模有限且集中关注，第二阶段预训练会出现灾难性遗忘。忘记原始训练数据中的信息可能会严重损害模型在下游任务中的性能。在这项工作中，我们通过在GLUE中进行评估，实证地表明这种方法中会发生灾难性遗忘。然后，我们提出了一种新方法，GEnder Equality Prompt (GEEP)，以改善预训练模型的性别公平性，且遗忘较少。 GEEP会冻结预训练模型，并使用性别中性数据学习与性别相关的提示。实证结果显示，GEEP不仅在性别公平任务上实现了SOTA表现，而且在GLUE上遗忘较少，并取得了明显的性能提高。

    Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model's downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data. Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.
    
[^126]: 探索时空人群流量预测中的上下文泛化性：基准和指南

    Exploring the Context Generalizability in Spatiotemporal Crowd Flow Prediction: Benchmark and Guideline. (arXiv:2106.16046v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.16046](http://arxiv.org/abs/2106.16046)

    本文研究了时空人群流量预测中的上下文泛化性，建立了基准，提出了通用分类法，为上下文选择和建模提供了指南。

    

    上下文特征是构建时空人群流量预测（STCFP）模型的重要数据来源。然而，应用上下文的困难在于不同场景中上下文特征（例如天气、假日和兴趣点）和上下文建模技术的未知泛化性。本文建立了一个基准，由大规模时空人群流量数据、上下文数据和最先进的时空预测模型组成。我们在几个城市人群流量预测场景中进行了全面的实验研究，以定量研究不同上下文特征和建模技术的泛化性。特别地，我们基于对流行研究的广泛调查，开发了上下文建模技术的通用分类法。我们使用了数百万条记录和丰富的上下文数据，训练和测试了数百种模型以捕捉上下文泛化性。我们的研究为STCFP中的上下文选择和建模提供了指南。

    Contextual features are important data sources for building spatiotemporal crowd flow prediction (STCFP) models. However, the difficulty of applying context lies in the unknown generalizability of both contextual features (e.g., weather, holiday, and points of interests) and context modeling techniques across different scenarios. In this paper, we build a benchmark composed of large-scale spatiotemporal crowd flow data, contextual data, and state-of-the-art spatiotemporal prediction models. We conduct a comprehensive experimental study to quantitatively investigate the generalizability of different contextual features and modeling techniques in several urban crowd flow prediction scenarios (including bike flow, metro passenger flow, electric vehicle charging demand and so on). In particular, we develop a general taxonomy of context modeling techniques based on extensive investigations in prevailing research. With millions of records and rich context data, we have trained and tested hun
    
[^127]: 带有隐藏变量的因果图模型中最优调整集的必要和充分图条件

    Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables. (arXiv:2102.10324v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.10324](http://arxiv.org/abs/2102.10324)

    本文提出了关于在带有隐藏变量的因果图模型中选择最优反向门控调整集以估计因果效应问题的必要和充分图条件及其算法，解决了此类图模型中可能存在没有最优集的情况。

    

    本文解决了在具有隐藏和条件变量的图形模型中选择最优反向门控调整集以估计因果效应的问题。先前的工作将最优定义为实现最小渐近估计方差，并推导了无隐藏变量情况下的最优集。对于存在隐藏变量的情况，可能存在没有最优集的设置，并且目前仅推导出了受限应用的充分图优化标准。在本文中，最优性被表征为最大化某种调整信息，这允许导出关于存在最优调整集的必要和充分图准则以及一个定义和构造它的算法。此外，当且仅当存在有效的调整集且其调整信息高于等于Perkovi{\'c}等提出的Adjust-set时，最优集才有效。

    The problem of selecting optimal backdoor adjustment sets to estimate causal effects in graphical models with hidden and conditioned variables is addressed. Previous work has defined optimality as achieving the smallest asymptotic estimation variance and derived an optimal set for the case without hidden variables. For the case with hidden variables there can be settings where no optimal set exists and currently only a sufficient graphical optimality criterion of limited applicability has been derived. In the present work optimality is characterized as maximizing a certain adjustment information which allows to derive a necessary and sufficient graphical criterion for the existence of an optimal adjustment set and a definition and algorithm to construct it. Further, the optimal set is valid if and only if a valid adjustment set exists and has higher (or equal) adjustment information than the Adjust-set proposed in Perkovi{\'c} et al. [Journal of Machine Learning Research, 18: 1--62, 20
    
[^128]: 顺序随机实验的弱信号渐近行为

    Weak Signal Asymptotics for Sequentially Randomized Experiments. (arXiv:2101.09855v5 [math.ST] UPDATED)

    [http://arxiv.org/abs/2101.09855](http://arxiv.org/abs/2101.09855)

    本文使用弱信号渐近行为的方法研究了一类顺序随机实验，认为这类顺序实验的样本路径会弱收敛到扩散极限，并能获得关于几种顺序实验的后悔和信念演变的多个见解。

    

    我们使用弱信号渐近行为的方法研究了一类顺序随机实验，包括解决多臂赌博机问题的实验。在一个$n$个时间步骤的实验中，我们让不同动作的平均奖励间隙按照$1/\sqrt{n}$的比例缩放，以保持学习任务的难度随着$n$的增长而保持不变。在这种情况下，我们发现一类顺序随机实验的样本路径会弱收敛到扩散极限，其中，臂选择的概率会随着状态的变化而持续变化，并在满足连续性假设的情况下进行调整。扩散极限使我们能够推导出精细的、特定于实例的随机动力学特征，并获得关于几种顺序实验的后悔和信念演变的多个见解，包括汤普森采样（但不包括不满足我们连续性假设的UCB）。我们展示了所有顺序实验的表现都能在长时间内稳定。

    We use the lens of weak signal asymptotics to study a class of sequentially randomized experiments, including those that arise in solving multi-armed bandit problems. In an experiment with $n$ time steps, we let the mean reward gaps between actions scale to the order $1/\sqrt{n}$ so as to preserve the difficulty of the learning task as $n$ grows. In this regime, we show that the sample paths of a class of sequentially randomized experiments -- adapted to this scaling regime and with arm selection probabilities that vary continuously with state -- converge weakly to a diffusion limit, given as the solution to a stochastic differential equation. The diffusion limit enables us to derive refined, instance-specific characterization of stochastic dynamics, and to obtain several insights on the regret and belief evolution of a number of sequential experiments including Thompson sampling (but not UCB, which does not satisfy our continuity assumption). We show that all sequential experiments wh
    
[^129]: 通过目标网络打破致命三角（Reinforcement Learning）

    Breaking the Deadly Triad with a Target Network. (arXiv:2101.08862v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.08862](http://arxiv.org/abs/2101.08862)

    本论文研究了如何通过使用目标网络来稳定训练，提出了一种新的目标网络更新规则并证明了其在离线学习、线性函数逼近和自举的算法中的收敛性，最终达到了收敛到正则化TD固定点的效果。

    

    “致命三角”是指强化学习算法在同时使用离线学习、函数逼近和自举时的不稳定性。本文研究了目标网络作为打破“致命三角”的工具，提供了理论支持，证明了目标网络稳定训练的常识。首先，我们提出并分析了一种新颖的目标网络更新规则，将常用的 Polyak 平均风格更新与两个投影相结合。然后，在几个不同的算法中应用目标网络和岭正则化，证明它们可以收敛到正则化 TD 固定点，这些算法都是离线学习、线性函数逼近和自举的，涵盖政策评估和控制，以及折扣和平均奖励设置。特别地，我们提供了第一个收敛的线性$Q$学习算法，这些算法在非限制性和变化行为策略下均成立，不需要双层优化。

    The deadly triad refers to the instability of a reinforcement learning algorithm when it employs off-policy learning, function approximation, and bootstrapping simultaneously. In this paper, we investigate the target network as a tool for breaking the deadly triad, providing theoretical support for the conventional wisdom that a target network stabilizes training. We first propose and analyze a novel target network update rule which augments the commonly used Polyak-averaging style update with two projections. We then apply the target network and ridge regularization in several divergent algorithms and show their convergence to regularized TD fixed points. Those algorithms are off-policy with linear function approximation and bootstrapping, spanning both policy evaluation and control, as well as both discounted and average-reward settings. In particular, we provide the first convergent linear $Q$-learning algorithms under nonrestrictive and changing behavior policies without bi-level o
    
[^130]: 利用概率时序排名学习医用超声扫描机器人奖励

    Learning rewards for robotic ultrasound scanning using probabilistic temporal ranking. (arXiv:2002.01240v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2002.01240](http://arxiv.org/abs/2002.01240)

    本文提出了一种利用概率时序排名的方法，从探索性演示中推断医用超声扫描机器人任务的奖励函数。

    

    信息化路径规划是机器人视觉伺服和主动视点选择的一种成熟方法，但通常假定已知适当的成本函数或目标状态。本文考虑了逆问题，即任务的目标未知，需要从示教者提供的探索性示例演示中推断出奖励函数，以在下游信息化路径规划策略中使用。然而，由于探索性示范的性质，许多现有的奖励推断策略不适用于这类问题。在本文中，我们提出了一种应对这类问题的替代方法。我们假设，在需要发现的任务中，任何演示的连续状态越来越可能与更高的奖励关联，并使用该假设生成基于时间的二进制比较结果，推断出奖励函数。

    Informative path-planning is a well established approach to visual-servoing and active viewpoint selection in robotics, but typically assumes that a suitable cost function or goal state is known. This work considers the inverse problem, where the goal of the task is unknown, and a reward function needs to be inferred from exploratory example demonstrations provided by a demonstrator, for use in a downstream informative path-planning policy. Unfortunately, many existing reward inference strategies are unsuited to this class of problems, due to the exploratory nature of the demonstrations. In this paper, we propose an alternative approach to cope with the class of problems where these sub-optimal, exploratory demonstrations occur. We hypothesise that, in tasks which require discovery, successive states of any demonstration are progressively more likely to be associated with a higher reward, and use this hypothesis to generate time-based binary comparison outcomes and infer reward functio
    

