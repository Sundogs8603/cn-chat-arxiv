# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Generative-Contrastive Heterogeneous Graph Neural Network](https://arxiv.org/abs/2404.02810) | 本研究提出了一种生成-对比异构图神经网络，通过对比视图增强策略、位置感知和语义感知正样本采样策略以及分层对比学习策略来克服图数据增强的限制。 |
| [^2] | [Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning](https://arxiv.org/abs/2404.01714) | 提出一种基于共轭梯度样式的新优化算法CG-like-Adam，用于深度学习，并在收敛分析和数值实验中展示了其优越性 |
| [^3] | [Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards](https://arxiv.org/abs/2403.19024) | 本文扩展了强化学习和控制理论中对称技术的应用范围，通过利用动态对称性学习动力学模型，而不要求奖励具有相同的对称性。 |
| [^4] | [WeatherProof: Leveraging Language Guidance for Semantic Segmentation in Adverse Weather](https://arxiv.org/abs/2403.14874) | 该论文提出了一种在恶劣天气条件下进行语义分割的方法，通过引入语言指导以提高模型的鲁棒性。 |
| [^5] | [Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales](https://arxiv.org/abs/2403.12403) | 利用大型语言模型提取原因特征训练仇恨言论分类器，实现可解释的检测方法 |
| [^6] | [Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation](https://arxiv.org/abs/2403.08579) | 本研究利用机器学习的方法对正交基分段多项式逼近进行优化。 |
| [^7] | [Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation](https://arxiv.org/abs/2403.01671) | 本文研究了如何在多元概率分布中测试排列不变性、估计排列不变密度以及分析排列不变函数类的度量熵，比较了它们与没有排列不变性的函数类的差异。 |
| [^8] | [ToDo: Token Downsampling for Efficient Generation of High-Resolution Images](https://arxiv.org/abs/2402.13573) | 提出了一种新的训练-free 方法 ToDo，通过令牌下采样加速 Stable Diffusion 推理，以实现高分辨率图像的高效生成。 |
| [^9] | [Scalable Decentralized Algorithms for Online Personalized Mean Estimation](https://arxiv.org/abs/2402.12812) | 本研究提出了一种可扩展的分散算法框架，使代理能够自组织成图，并提出了两种协同均值估计算法，解决了每个代理在学习模型的同时识别具有相似分布客户的挑战。 |
| [^10] | [Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning](https://arxiv.org/abs/2402.07818) | 本文研究了差分隐私零阶方法在大型语言模型微调中的应用，该方法通过使用零阶梯度来避免传统优化方法的可扩展性瓶颈，实现了在隐私、效用和可扩展性之间的良好平衡。 |
| [^11] | [Test-Time Adaptation for Depth Completion](https://arxiv.org/abs/2402.03312) | 该论文提出了一种在线测试时间自适应方法，用于深度补全任务，通过在单次通过中缩小源数据和目标数据间的领域差距，提高模型性能。 |
| [^12] | [A Survey of Constraint Formulations in Safe Reinforcement Learning](https://arxiv.org/abs/2402.02025) | 本文综述了安全强化学习中的约束形式，包括对每种形式特别设计的算法。同时，揭示了常见问题形式之间的数学相互关系。 |
| [^13] | [Unveiling Molecular Moieties through Hierarchical Graph Explainability](https://arxiv.org/abs/2402.01744) | 本论文提出了一种使用图神经网络和分层可解释人工智能技术的方法，能够准确预测生物活性并找到与之相关的最重要的成分。 |
| [^14] | [Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning](https://arxiv.org/abs/2402.00046) | PetriRL是一个创新框架，将Petri网和基于事件的强化学习集成，用于解决工业作业车间调度问题（JSSP）。该框架利用Petri网进行建模，提高了可解释性，并通过事件驱动控制和动作屏蔽的集成取得了竞争性的性能。 |
| [^15] | [Variational Self-Supervised Contrastive Learning Using Beta Divergence For Face Understanding](https://arxiv.org/abs/2312.00824) | 提出一种使用Beta Divergence进行变分自监督对比学习的方法，能够从未标记和嘈杂数据集中学习，在人脸理解领域取得显著的准确率提升。 |
| [^16] | [DMODE: Differential Monocular Object Distance Estimation Module without Class Specific Information](https://arxiv.org/abs/2210.12596) | DMODE是一种无需物体类别信息的单目目标距离估计方法，通过融合物体大小变化和摄像头运动来实现对各种目标检测和未知物体的适应，解决了单目距离估计中缺乏参考点和对象特定线索的挑战。 |
| [^17] | [Deep Reinforcement Learning with Spiking Q-learning](https://arxiv.org/abs/2201.09754) | 基于脉冲Q学习的深度强化学习方法DSQN利用非脉冲神经元的膜电压作为Q值表示，实现了能源高效的控制任务。 |
| [^18] | [Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition.](http://arxiv.org/abs/2401.16836) | 本文提出了一种基于T-CUR分解的可分离非负张量分解方法，用于在多维数据中提取有意义的特征。 |
| [^19] | [A Discriminative Bayesian Gaussian Process Latent Variable Model for High-Dimensional Data.](http://arxiv.org/abs/2401.16497) | 本研究提出了一个名为LDGD的判别贝叶斯高斯过程潜变量模型，能够有效地从高维数据中提取信息，并具有较高的预测准确性和鲁棒性。 |
| [^20] | [Few-Shot Detection of Machine-Generated Text using Style Representations.](http://arxiv.org/abs/2401.06712) | 本研究提出了一种小样本检测方法，通过使用样式表示来检测机器生成的文本与人类撰写的文本的区别，以解决滥用语言模型带来的问题。 |
| [^21] | [Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks.](http://arxiv.org/abs/2401.04929) | 本文介绍了一种基于学习的难度校准的成员推理攻击方法，旨在显著提高低FPR下的TPR，以验证训练模型是否保护隐私。 |
| [^22] | [The Power of Training: How Different Neural Network Setups Influence the Energy Demand.](http://arxiv.org/abs/2401.01851) | 本文研究了机器学习训练方案和学习范式对能源消耗的影响，并探讨了预训练和多任务训练在可持续机器学习方面的潜力。 |
| [^23] | [Sensitivity-Aware Amortized Bayesian Inference.](http://arxiv.org/abs/2310.11122) | 本文提出了一种敏感性感知的摊销贝叶斯推断方法，通过权重共享和神经网络来进行似然和先验规范的训练，以及对数据扰动和预处理程序的敏感性评估。 |
| [^24] | [Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning.](http://arxiv.org/abs/2310.07918) | 本论文提出了一种上下文化政策恢复方法用于建模复杂的医疗决策过程，以解决现有模型在准确性和可解释性之间的权衡问题。该方法将决策策略拆分为上下文特定策略，通过多任务学习来实现建模，并提供复杂行为的简洁描述。 |
| [^25] | [AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer.](http://arxiv.org/abs/2309.12689) | AMPLIFY提出了一种基于注意力机制的Mixup方法，用于减少原始样本中的噪音和异常值对于模型的影响，并在文本分类任务中表现出更好的性能。 |
| [^26] | [Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions.](http://arxiv.org/abs/2309.12342) | 本研究提出了一种使用霍夫斯泰德文化维度框架来量化大型语言模型与不同文化之间的对齐程度的文化对齐测试（CAT）。通过在不同文化国家应用该方法，我们发现LLMs在解释性文化维度上存在差异，并能量化其与特定国家的文化对齐情况。 |
| [^27] | [Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks.](http://arxiv.org/abs/2308.11925) | 本论文提出了一种使用物理信息神经网络（PINNs）求解椭圆型最优控制问题的数值方法，并进行了误差分析和数值实验验证。 |
| [^28] | [HEAL-SWIN: A Vision Transformer On The Sphere.](http://arxiv.org/abs/2307.07313) | HEAL-SWIN是一个基于HEALPix网格和SWIN变压器结合的球面视觉变压器，在处理高分辨率广角鱼眼图像时能够实现无失真且高效的训练，并在语义分割任务中展现出优异的性能。 |
| [^29] | [Bayesian taut splines for estimating the number of modes.](http://arxiv.org/abs/2307.05825) | 本研究提出了一种贝叶斯紧系数样条方法，用于估计概率密度函数中模式的数量。该方法结合了核估计器和组合样条，实现了特征探索、模型选择和模式检验，并允许引入专家判断。通过在体育分析中的案例研究中的验证，证明了该方法的实用性。 |
| [^30] | [Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning.](http://arxiv.org/abs/2306.01474) | 本文提出了一种通用等变Transformer用于学习3D分子相互作用，该模型具有双层注意力模块、前馈模块和层归一化模块，每个模块都是E（3）等变的，可以有效地捕捉块级和原子级的交互，实验结果表明其在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面优于各种最先进的方法。 |
| [^31] | [A Study of Bayesian Neural Network Surrogates for Bayesian Optimization.](http://arxiv.org/abs/2305.20028) | 本文研究贝叶斯神经网络替代高斯过程模型作为贝叶斯优化中的代理模型，并在多个基准问题上证明了其优于标准GP代理的能力。 |
| [^32] | [Medication Recommendation via Domain Knowledge Informed Deep Learning.](http://arxiv.org/abs/2305.19604) | 提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。 |
| [^33] | [Distribution-aware Fairness Test Generation.](http://arxiv.org/abs/2305.13935) | 本文介绍了一种名为DistroFair的分布感知的公平性测试方法，可以从图像分类器中检测到类级别的公平性违规。 |
| [^34] | [Bake off redux: a review and experimental evaluation of recent time series classification algorithms.](http://arxiv.org/abs/2304.13029) | 本文重访烘焙大赛，评估了最近时间序列分类算法在112个数据集上的表现。该论文通过分类法将这些算法分为五类，为TSC领域的发展提供了贡献。 |
| [^35] | [CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows.](http://arxiv.org/abs/2304.09010) | 本文提出了一种新的因果流以进行因果分离表示学习，设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力，并展示了在合成和真实数据集上实现因果分离并进行干预实验的结果。 |
| [^36] | [StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables.](http://arxiv.org/abs/2304.03853) | StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。 |
| [^37] | [RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging.](http://arxiv.org/abs/2304.03483) | 本文提出了一种称为RED-PSM的方法，将部分可分模型与去噪正则化相结合，用于解决动态成像问题，数值实验证明其优越性。 |
| [^38] | [Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise.](http://arxiv.org/abs/2208.08003) | 本文研究发现标签噪声会导致双丘降曲线出现“最终上升”，即在足够大的噪声样本比率下，中等宽度下实现最佳泛化性能。随机丢弃可训练参数来减少密度可在标签噪声下改善泛化性能。 |

# 详细

[^1]: 生成-对比异构图神经网络

    Generative-Contrastive Heterogeneous Graph Neural Network

    [https://arxiv.org/abs/2404.02810](https://arxiv.org/abs/2404.02810)

    本研究提出了一种生成-对比异构图神经网络，通过对比视图增强策略、位置感知和语义感知正样本采样策略以及分层对比学习策略来克服图数据增强的限制。

    

    异构图表达了现实世界中复杂关系，包括多种类型的节点和边。受自监督学习启发，对比异构图神经网络(HGNNs)利用数据增强和辨别器展现了巨大潜力用于下游任务。然而，由于图的离散和抽象特性，数据增强仍然存在限制。为了解决上述限制，我们提出了一种新颖的\textit{生成-对比异构图神经网络(GC-HGNN)}。

    arXiv:2404.02810v1 Announce Type: new  Abstract: Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges. In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and discriminators for downstream tasks. However, data augmentation is still limited due to the discrete and abstract nature of graphs. To tackle the above limitations, we propose a novel \textit{Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm. This paradigm includes: 1) A contrastive view augmentation strategy by using masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples. 3) A hierarchical contrastive learning strategy for capturing local and global informa
    
[^2]: 基于共轭梯度的自适应矩估计优化算法用于深度学习

    Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning

    [https://arxiv.org/abs/2404.01714](https://arxiv.org/abs/2404.01714)

    提出一种基于共轭梯度样式的新优化算法CG-like-Adam，用于深度学习，并在收敛分析和数值实验中展示了其优越性

    

    训练深度神经网络是一项具有挑战性的任务。为加快培训速度并增强深度神经网络的性能，我们将传统的共轭梯度修正为共轭梯度样式，并将其并入通用Adam中，因此提出了一种名为CG-like-Adam的新优化算法用于深度学习。具体而言，通用Adam的一阶和二阶矩估计均由共轭梯度样式替换。收敛分析处理了一阶矩估计的指数移动平均系数为常数且一阶矩估计无偏的情况。数值实验显示了基于CIFAR10/100数据集的所提算法的优越性。

    arXiv:2404.01714v1 Announce Type: cross  Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.
    
[^3]: 利用动态对称性进行基于模型的非对称奖励强化学习

    Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards

    [https://arxiv.org/abs/2403.19024](https://arxiv.org/abs/2403.19024)

    本文扩展了强化学习和控制理论中对称技术的应用范围，通过利用动态对称性学习动力学模型，而不要求奖励具有相同的对称性。

    

    强化学习中最近的工作利用模型中的对称性来提高策略训练的采样效率。一个常用的简化假设是动力学和奖励都表现出相同的对称性。然而，在许多真实环境中，动力学模型表现出与奖励模型独立的对称性：奖励可能不满足与动力学相同的对称性。本文探讨了只假定动力学表现出对称性的情况，扩展了强化学习和控制理论学习中可应用对称技术的问题范围。我们利用卡塔恩移动框架方法引入一种学习动力学的技术，通过构造，这种动力学表现出指定的对称性。我们通过数值实验展示了所提出的方法学到了更准确的动态模型。

    arXiv:2403.19024v1 Announce Type: cross  Abstract: Recent work in reinforcement learning has leveraged symmetries in the model to improve sample efficiency in training a policy. A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry. However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics. In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in reinforcement learning and learning in control theory where symmetry techniques can be applied. We use Cartan's moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries. We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model.
    
[^4]: WeatherProof:借助语言指导在恶劣天气条件下进行语义分割

    WeatherProof: Leveraging Language Guidance for Semantic Segmentation in Adverse Weather

    [https://arxiv.org/abs/2403.14874](https://arxiv.org/abs/2403.14874)

    该论文提出了一种在恶劣天气条件下进行语义分割的方法，通过引入语言指导以提高模型的鲁棒性。

    

    我们提出了一种方法来推断在恶劣天气条件下拍摄的图像的语义分割图。我们首先检查了针对受天气条件（如雨、雾或雪）影响而降级的图像的现有模型，发现它们与在晴朗天气下拍摄的图像相比性能大幅下降。为了控制场景结构的变化，我们提出了WeatherProof，这是第一个具有准确的晴朗和恶劣天气图像对的语义分割数据集，它们共享相同的场景。通过这个数据集，我们分析了现有模型的错误模式，发现它们对在拍摄过程中施加在图像上的不同天气效应的高度复杂组合敏感。为了提高鲁棒性，我们提出了一种使用语言作为指导的方式，通过识别恶劣天气条件的影响并将其注入为“辅助信息”。使用我们的语言指导训练的模型表现

    arXiv:2403.14874v1 Announce Type: cross  Abstract: We propose a method to infer semantic segmentation maps from images captured under adverse weather conditions. We begin by examining existing models on images degraded by weather conditions such as rain, fog, or snow, and found that they exhibit a large performance drop as compared to those captured under clear weather. To control for changes in scene structures, we propose WeatherProof, the first semantic segmentation dataset with accurate clear and adverse weather image pairs that share an underlying scene. Through this dataset, we analyze the error modes in existing models and found that they were sensitive to the highly complex combination of different weather effects induced on the image during capture. To improve robustness, we propose a way to use language as guidance by identifying contributions of adverse weather conditions and injecting that as "side information". Models trained using our language guidance exhibit performance
    
[^5]: 利用大型语言模型提取的原因生成可解释的仇恨言论检测方法

    Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales

    [https://arxiv.org/abs/2403.12403](https://arxiv.org/abs/2403.12403)

    利用大型语言模型提取原因特征训练仇恨言论分类器，实现可解释的检测方法

    

    尽管社交媒体平台是用户进行人际讨论和表达观点的重要场所，但社交媒体提供的外立面和匿名性可能导致用户发布仇恨言论和冒犯性内容。鉴于这些平台的庞大规模，自动识别和标记仇恨言论的需求日益迫切。尽管存在几种仇恨言论检测方法，但大多数黑盒方法在设计上不具有可解释性或可解释性。为解决解释性不足，本文提出使用最先进的大型语言模型（LLM）从输入文本中提取原因特征，训练基础仇恨言论分类器，从而通过设计实现忠实的可解释性。我们的框架有效地结合了LLM的文本理解能力和最先进仇恨言论分类器的判别能力，使这些分类器

    arXiv:2403.12403v1 Announce Type: cross  Abstract: Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifi
    
[^6]: 机器学习优化的正交基分段多项式逼近

    Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation

    [https://arxiv.org/abs/2403.08579](https://arxiv.org/abs/2403.08579)

    本研究利用机器学习的方法对正交基分段多项式逼近进行优化。

    

    分段多项式（PPs）在几个工程学科中被使用，比如在轨迹规划中，用来逼近以一组点给出的位置轮廓。 鉴于逼近目标以及特定领域要求，比如Ck-连续性，可以被构造为一个方程组，结果可以直接计算，这样的闭式解对于多项式次数、多项式基础或者添加进一步的特定领域要求方面具有有限的灵活性。足够复杂的优化目标很快要求使用数值方法，比如梯度下降。由于梯度下降是训练人工神经网络（ANNs）的核心，现代机器学习（ML）框架比如TensorFlow提供了一套基于梯度的优化器，可能适用于训练任务之外的广泛优化问题。我们的方法是利用PP模式的多样性

    arXiv:2403.08579v1 Announce Type: new  Abstract: Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points. While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements. Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent. Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs. Our approach is to utilize the versatility of PP mode
    
[^7]: 排列不变函数：统计检验、度量熵中的降维和估计

    Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation

    [https://arxiv.org/abs/2403.01671](https://arxiv.org/abs/2403.01671)

    本文研究了如何在多元概率分布中测试排列不变性、估计排列不变密度以及分析排列不变函数类的度量熵，比较了它们与没有排列不变性的函数类的差异。

    

    排列不变性是机器学习中可以利用来简化复杂问题的最常见的对称性之一。近年来关于构建排列不变的机器学习架构的研究活动激增。然而，在多元概率分布中的变量如何统计测试排列不变性却鲜有研究，其中样本量允许随着维数的增长。此外，在统计理论方面，关于排列不变性如何帮助估计中降维的知识甚少。本文通过研究几个基本问题，回顾并探讨这些问题：（i）测试多元分布排列不变性的假设；（ii）估计排列不变密度；（iii）分析光滑排列不变函数类的度量熵，并将其与未强加排列不变性的对应函数类进行比较。

    arXiv:2403.01671v1 Announce Type: new  Abstract: Permutation invariance is among the most common symmetry that can be exploited to simplify complex problems in machine learning (ML). There has been a tremendous surge of research activities in building permutation invariant ML architectures. However, less attention is given to how to statistically test for permutation invariance of variables in a multivariate probability distribution where the dimension is allowed to grow with the sample size. Also, in terms of a statistical theory, little is known about how permutation invariance helps with estimation in reducing dimensions. In this paper, we take a step back and examine these questions in several fundamental problems: (i) testing the assumption of permutation invariance of multivariate distributions; (ii) estimating permutation invariant densities; (iii) analyzing the metric entropy of smooth permutation invariant function classes and compare them with their counterparts without impos
    
[^8]: 任务待办：用于高分辨率图像高效生成的令牌下采样

    ToDo: Token Downsampling for Efficient Generation of High-Resolution Images

    [https://arxiv.org/abs/2402.13573](https://arxiv.org/abs/2402.13573)

    提出了一种新的训练-free 方法 ToDo，通过令牌下采样加速 Stable Diffusion 推理，以实现高分辨率图像的高效生成。

    

    注意力机制对于图像扩散模型至关重要，然而，它们的二次计算复杂性限制了我们可以在合理时间和内存限制内处理的图像大小。本文研究了在生成图像模型中密集注意力的重要性，这些模型通常包含冗余特征，使它们适合稀疏注意力机制。我们提出了一种新颖的无需训练的方法 ToDo，该方法依赖于关键和值令牌的令牌下采样，可将常见大小的 Stable Diffusion 推理加速至多达2倍，对于2048x2048等高分辨率，加速比可达4.5倍或更高。我们证明了我们的方法在平衡高效吞吐量和保真度方面优于先前的方法。

    arXiv:2402.13573v1 Announce Type: cross  Abstract: Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.
    
[^9]: 可扩展的分散算法用于在线个性化均值估计

    Scalable Decentralized Algorithms for Online Personalized Mean Estimation

    [https://arxiv.org/abs/2402.12812](https://arxiv.org/abs/2402.12812)

    本研究提出了一种可扩展的分散算法框架，使代理能够自组织成图，并提出了两种协同均值估计算法，解决了每个代理在学习模型的同时识别具有相似分布客户的挑战。

    

    在许多情况下，代理缺乏足够的数据直接学习模型。与其他代理合作可能有所帮助，但当本地数据分布不同时，会引入偏差-方差权衡。一个关键挑战是每个代理在学习模型的同时识别具有相似分布的客户，这个问题主要仍未解决。本研究着眼于一个简化版本的普遍问题，即每个代理随时间从实值分布中收集样本来估计其均值。现有算法面临着不切实际的空间和时间复杂度（与代理数量A的平方成正比）。为了解决可扩展性挑战，我们提出了一个框架，代理自组织成一个图，使得每个代理只能与选定数量的对等体r进行通信。我们介绍了两种协作均值估计算法：一种灵感来源于信念传播，另一种采用基于共识的方法。

    arXiv:2402.12812v1 Announce Type: new  Abstract: In numerous settings, agents lack sufficient data to directly learn a model. Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ. A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved. This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean. Existing algorithms face impractical space and time complexities (quadratic in the number of agents A). To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based appr
    
[^10]: 可扩展大型语言模型微调的差分隐私零阶方法

    Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning

    [https://arxiv.org/abs/2402.07818](https://arxiv.org/abs/2402.07818)

    本文研究了差分隐私零阶方法在大型语言模型微调中的应用，该方法通过使用零阶梯度来避免传统优化方法的可扩展性瓶颈，实现了在隐私、效用和可扩展性之间的良好平衡。

    

    在特定任务的数据集上进行微调是利用预训练语言模型的强大能力进行各种下游任务的广泛接受的范例。由于预训练语言模型微调的普及以及与之相关的隐私问题，差分隐私预训练语言模型微调引起了越来越多的关注，以保护特定任务数据集的隐私。差分隐私预训练语言模型微调方法的设计核心是在隐私、效用和可扩展性之间达到满意的权衡。大多数现有方法都是基于DP-SGD的创新性工作。尽管将DP-SGD的可扩展性推到了极限，但基于DP-SGD的微调方法不幸地受到了SGD固有低效率的限制。在本文中，我们研究了DP零阶方法在LLM预训练中的潜力，该方法通过用更高效的零阶梯度来近似梯度，避免了SGD的可扩展性瓶颈。与将零阶方法作为一种替代方法进行处理不同，我们引入了一种新的割接框架，该框架能够以非常接近的方式模拟DP-SGD的基本操作，然后利用零阶优化方法来近似梯度。

    Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
    
[^11]: 深度补全的测试时间自适应

    Test-Time Adaptation for Depth Completion

    [https://arxiv.org/abs/2402.03312](https://arxiv.org/abs/2402.03312)

    该论文提出了一种在线测试时间自适应方法，用于深度补全任务，通过在单次通过中缩小源数据和目标数据间的领域差距，提高模型性能。

    

    当将在一些（源）数据集上训练的模型转移到目标测试数据时，常常会观察到性能下降，这是由于它们之间存在领域差距。现有的用于弥合这一差距的方法，如领域适应（DA），可能需要模型训练时使用的源数据（通常不可用），而其他方法，如无源DA，则需要多次通过测试数据。我们提出了一种在线测试时间自适应方法，用于深度补全，即从单个图像和相关的稀疏深度图推断出密集深度图的任务，以在一次通过中缩小性能差距。首先，我们对每种数据模态中的领域转移如何影响模型性能进行了研究。根据我们的观察，稀疏深度模态展现出比图像更小的协变量转移，因此我们设计了一个在源领域中训练的嵌入模块，它保留了从仅编码稀疏深度特征到编码图像和稀疏深度的特征的映射。在测试时间，我们使用这个嵌入模块实现自适应。

    It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During t
    
[^12]: 安全强化学习中的约束形式综述

    A Survey of Constraint Formulations in Safe Reinforcement Learning

    [https://arxiv.org/abs/2402.02025](https://arxiv.org/abs/2402.02025)

    本文综述了安全强化学习中的约束形式，包括对每种形式特别设计的算法。同时，揭示了常见问题形式之间的数学相互关系。

    

    在将强化学习（RL）应用于现实世界问题时，确保安全性至关重要。因此，安全RL成为一种从实验数据中安全优化代理策略的基本而强大的范例。基于约束准则的安全RL方法被广泛采用，它解决了在安全约束下最大化预期累积奖励的问题。虽然近年来在RL中实现安全性的尝试激增，但由于约束表示的多样性和对它们之间关系的讨论很少，对该领域的系统性了解仍然困难。为了解决这一知识差距，我们提供了对代表性约束形式的全面回顾，以及针对每种形式特别设计的算法的精选。此外，我们揭示了揭示常见问题形式之间的数学相互关系的理论基础。最后，我们讨论了当前研究的一些挑战和未来方向。

    Ensuring safety is critical when applying reinforcement learning (RL) to real-world problems. Consequently, safe RL emerges as a fundamental and powerful paradigm for safely optimizing an agent's policy from experimental data. A popular safe RL approach is based on a constrained criterion, which solves the problem of maximizing expected cumulative reward under safety constraints. Though there has been recently a surge of such attempts to achieve safety in RL, a systematic understanding of the field is difficult due to 1) the diversity of constraint representations and 2) little discussion of their interrelations. To address this knowledge gap, we provide a comprehensive review of representative constraint formulations, along with a curated selection of algorithms specifically designed for each formulation. Furthermore, we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations. We conclude with a discussion of the current 
    
[^13]: 通过分层图解释揭示分子成分

    Unveiling Molecular Moieties through Hierarchical Graph Explainability

    [https://arxiv.org/abs/2402.01744](https://arxiv.org/abs/2402.01744)

    本论文提出了一种使用图神经网络和分层可解释人工智能技术的方法，能够准确预测生物活性并找到与之相关的最重要的成分。

    

    背景：图神经网络（GNN）作为一种强大的工具，在支持体外虚拟筛选方面已经出现多年。在这项工作中，我们提出了一种使用图卷积架构实现高精度多靶标筛选的GNN。我们还设计了一种分层可解释人工智能（XAI）技术，通过利用信息传递机制，在原子、环和整个分子层面上直接捕获信息，从而找到与生物活性预测相关的最重要的成分。结果：我们在支持虚拟筛选方面的二十个细胞周期依赖性激酶靶标上报道了一种最先进的GNN分类器。我们的分类器超越了作者提出的先前最先进方法。此外，我们还设计了一个仅针对CDK1的高灵敏度版本的GNN，以使用我们的解释器来避免多类别模型固有的偏差。分层解释器已经由一位专家化学家在19个CDK1批准药物上进行了验证。

    Background: Graph Neural Networks (GNN) have emerged in very recent years as a powerful tool for supporting in silico Virtual Screening. In this work we present a GNN which uses Graph Convolutional architectures to achieve very accurate multi-target screening. We also devised a hierarchical Explainable Artificial Intelligence (XAI) technique to catch information directly at atom, ring, and whole molecule level by leveraging the message passing mechanism. In this way, we find the most relevant moieties involved in bioactivity prediction. Results: We report a state-of-the-art GNN classifier on twenty Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms previous SOTA approaches proposed by the authors. Moreover, a CDK1-only high-sensitivity version of the GNN has been designed to use our explainer in order to avoid the inherent bias of multi-class models. The hierarchical explainer has been validated by an expert chemist on 19 approved drugs on CDK1. Our explainer 
    
[^14]: 引入PetriRL：将Petri网和基于事件的强化学习集成的JSSP解决方案的创新框架

    Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning

    [https://arxiv.org/abs/2402.00046](https://arxiv.org/abs/2402.00046)

    PetriRL是一个创新框架，将Petri网和基于事件的强化学习集成，用于解决工业作业车间调度问题（JSSP）。该框架利用Petri网进行建模，提高了可解释性，并通过事件驱动控制和动作屏蔽的集成取得了竞争性的性能。

    

    在工业车间中，优质调度至关重要。尽管神经网络在解决这些问题方面表现出色，但其有限的可解释性阻碍了其在工业中的广泛应用。在本研究中，我们介绍了一个创新的框架来解决作业车间调度问题（JSSP）。我们的方法利用Petri网对作业车间进行建模，不仅提高了可解释性，还能直接将原始数据纳入其中，无需对JSSP实例进行预处理成非交替图。Petri网的控制能力还可以管理过程中的自动化组件，使代理人能够专注于关键决策，特别是资源分配。我们的方法中事件驱动控制和动作屏蔽的集成在公共测试基准上取得了竞争性的性能。跨一系列优化解决方案（包括启发式算法、元启发式算法和学习算法）进行的比较分析突出了其竞争性。

    Quality scheduling in industrial job shops is crucial. Although neural networks excel in solving these problems, their limited explainability hinders their widespread industrial adoption. In this research, we introduce an innovative framework for solving job shop scheduling problems (JSSP). Our methodology leverages Petri nets to model the job shop, not only improving explainability but also enabling direct incorporation of raw data without the need to preprocess JSSP instances into disjunctive graphs. The Petri net, with its controlling capacities, also governs the automated components of the process, allowing the agent to focus on critical decision-making, particularly resource allocation. The integration of event-based control and action masking in our approach yields competitive performance on public test benchmarks. Comparative analyses across a wide spectrum of optimization solutions, including heuristics, metaheuristics, and learning-based algorithms, highlight the competitivene
    
[^15]: 使用Beta Divergence进行变分自监督对比学习用于人脸理解

    Variational Self-Supervised Contrastive Learning Using Beta Divergence For Face Understanding

    [https://arxiv.org/abs/2312.00824](https://arxiv.org/abs/2312.00824)

    提出一种使用Beta Divergence进行变分自监督对比学习的方法，能够从未标记和嘈杂数据集中学习，在人脸理解领域取得显著的准确率提升。

    

    学习如何使用未标记和嘈杂数据构建一个具有辨别性的语义空间在多标签设置中仍未解决。我们提出了一种对数据噪声具有鲁棒性且基于变分方法的对比自监督学习方法。该方法（VCL）利用变分对比学习与beta-divergence从未标记数据集中稳健地学习，包括未加工和嘈杂的数据集。我们通过严格的实验展示了所提出方法的有效性，包括在线性评估和微调场景中使用多标签数据集进行的人脸理解领域。在几乎所有测试场景中，VCL均超越了最先进的自监督方法的性能，实现了显著的准确率增加。

    arXiv:2312.00824v2 Announce Type: replace-cross  Abstract: Learning a discriminative semantic space using unlabelled and noisy data remains unaddressed in a multi-label setting. We present a contrastive self-supervised learning method which is robust to data noise, grounded in the domain of variational methods. The method (VCL) utilizes variational contrastive learning with beta-divergence to learn robustly from unlabelled datasets, including uncurated and noisy datasets. We demonstrate the effectiveness of the proposed method through rigorous experiments including linear evaluation and fine-tuning scenarios with multi-label datasets in the face understanding domain. In almost all tested scenarios, VCL surpasses the performance of state-of-the-art self-supervised methods, achieving a noteworthy increase in accuracy.
    
[^16]: DMODE: 无需特定类别信息的单目目标距离估计模块

    DMODE: Differential Monocular Object Distance Estimation Module without Class Specific Information

    [https://arxiv.org/abs/2210.12596](https://arxiv.org/abs/2210.12596)

    DMODE是一种无需物体类别信息的单目目标距离估计方法，通过融合物体大小变化和摄像头运动来实现对各种目标检测和未知物体的适应，解决了单目距离估计中缺乏参考点和对象特定线索的挑战。

    

    利用单个摄像头测量物体距离是一种经济高效的替代方案，而不需要立体视觉和激光雷达。尽管文献中已经探讨了单目距离估计，但大多数现有技术依赖于物体类别知识以实现高性能。在缺乏这些情境数据的情况下，单目距离估计变得更具挑战性，缺乏参考点和物体特定线索。然而，这些线索可能会误导与范围广泛变化或对抗情况下的对象，这是面向对象不可知距离估计的一个具有挑战性的方面。本文提出了DMODE，一种不需要物体类别知识的单目距离估计类别不可知方法。DMODE通过融合物体随时间变化的大小波动和摄像头运动来估计物体的距离，使其能够适应各种目标检测器和未知物体，从而解决这些挑战。

    arXiv:2210.12596v2 Announce Type: replace-cross  Abstract: Utilizing a single camera for measuring object distances is a cost-effective alternative to stereo-vision and LiDAR. Although monocular distance estimation has been explored in the literature, most existing techniques rely on object class knowledge to achieve high performance. Without this contextual data, monocular distance estimation becomes more challenging, lacking reference points and object-specific cues. However, these cues can be misleading for objects with wide-range variation or adversarial situations, which is a challenging aspect of object-agnostic distance estimation. In this paper, we propose DMODE, a class-agnostic method for monocular distance estimation that does not require object class knowledge. DMODE estimates an object's distance by fusing its fluctuation in size over time with the camera's motion, making it adaptable to various object detectors and unknown objects, thus addressing these challenges. We eva
    
[^17]: 带波脉冲Q学习的深度强化学习

    Deep Reinforcement Learning with Spiking Q-learning

    [https://arxiv.org/abs/2201.09754](https://arxiv.org/abs/2201.09754)

    基于脉冲Q学习的深度强化学习方法DSQN利用非脉冲神经元的膜电压作为Q值表示，实现了能源高效的控制任务。

    

    借助特殊的神经形态硬件，期望通过脉冲神经网络（SNNs）实现人工智能（AI），以更少的能量消耗。通过将SNNs与深度强化学习（RL）相结合，为实现现实控制任务提供了一种有前途的高效能源方式。目前仅有少数基于SNN的RL方法。其中大部分要么缺乏泛化能力，要么在训练中使用人工神经网络（ANNs）来估算值函数。前者需要为每个场景调整大量超参数，而后者限制了不同类型RL算法的应用并忽略了训练中的能量消耗较大。为开发一个强大的基于脉冲的RL方法，我们从昆虫中发现的非脉冲间神经元中汲取灵感，提出了深度脉冲Q网络（DSQN），使用非脉冲神经元的膜电压作为Q值的表示，从而可以指导

    arXiv:2201.09754v2 Announce Type: replace-cross  Abstract: With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of non-spiking neurons as the representation of Q-value, which can direct
    
[^18]: 基于T-CUR分解的可分离非负张量分解

    Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition. (arXiv:2401.16836v1 [cs.LG])

    [http://arxiv.org/abs/2401.16836](http://arxiv.org/abs/2401.16836)

    本文提出了一种基于T-CUR分解的可分离非负张量分解方法，用于在多维数据中提取有意义的特征。

    

    非负矩阵分解(NMF)是一种重要的无监督学习方法，用于从数据中提取有意义的特征。为了在多项式时间框架内解决NMF问题，研究人员引入了可分离性假设，最近演变为可分离的概念。这一进展为原始数据提供了更高效的核心表示。然而，在现实世界中，数据更自然地被表示为多维数组，如图像或视频。将NMF应用于高维数据涉及向量化，会导致丢失关键的多维度相关性。为了保留数据中这些固有的相关性，我们转向张量(多维数组)并利用张量t乘积。这种方法将可分离的NMF扩展到张量设置，从而创建了我们所称的可分离非负张量分解(NTF)。在这项工作中，我们提供了一种交替索引选择方法来选择cos

    Nonnegative Matrix Factorization (NMF) is an important unsupervised learning method to extract meaningful features from data. To address the NMF problem within a polynomial time framework, researchers have introduced a separability assumption, which has recently evolved into the concept of coseparability. This advancement offers a more efficient core representation for the original data. However, in the real world, the data is more natural to be represented as a multi-dimensional array, such as images or videos. The NMF's application to high-dimensional data involves vectorization, which risks losing essential multi-dimensional correlations. To retain these inherent correlations in the data, we turn to tensors (multidimensional arrays) and leverage the tensor t-product. This approach extends the coseparable NMF to the tensor setting, creating what we term coseparable Nonnegative Tensor Factorization (NTF). In this work, we provide an alternating index selection method to select the cos
    
[^19]: 一个用于高维数据的判别贝叶斯高斯过程潜变量模型

    A Discriminative Bayesian Gaussian Process Latent Variable Model for High-Dimensional Data. (arXiv:2401.16497v1 [cs.LG])

    [http://arxiv.org/abs/2401.16497](http://arxiv.org/abs/2401.16497)

    本研究提出了一个名为LDGD的判别贝叶斯高斯过程潜变量模型，能够有效地从高维数据中提取信息，并具有较高的预测准确性和鲁棒性。

    

    从高维数据中提取有意义的信息是一个具有挑战性的建模问题，特别是当数据被噪声干扰或以不同的模态表示时。在这项研究中，我们提出了一种新颖的非参数建模方法，利用高斯过程（GP）将高维数据映射到潜在的低维流形上。这个模型被命名为潜在判别生成解码器（LDGD），它在流形发现过程中利用了数据（或其特征）和相关标签（如类别或刺激）。为了推断潜在变量，我们提供了一个贝叶斯解，使得LDGD能够有效地捕捉数据中的内在不确定性，同时提高模型的预测准确性和鲁棒性。我们在合成数据集和基准数据集上演示了LDGD的应用。LDGD不仅能准确地推断流形，而且在预测标签方面的准确性超过了最先进的方法。

    Extracting meaningful information from high-dimensional data poses a formidable modeling challenge, particularly when the data is obscured by noise or represented through different modalities. In this research, we propose a novel non-parametric modeling approach, leveraging the Gaussian Process (GP), to characterize high-dimensional data by mapping it to a latent low-dimensional manifold. This model, named the Latent Discriminative Generative Decoder (LDGD), utilizes both the data (or its features) and associated labels (such as category or stimulus) in the manifold discovery process. To infer the latent variables, we derive a Bayesian solution, allowing LDGD to effectively capture inherent uncertainties in the data while enhancing the model's predictive accuracy and robustness. We demonstrate the application of LDGD on both synthetic and benchmark datasets. Not only does LDGD infer the manifold accurately, but its prediction accuracy in anticipating labels surpasses state-of-the-art a
    
[^20]: 使用样式表示进行机器生成文本的小样本检测

    Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])

    [http://arxiv.org/abs/2401.06712](http://arxiv.org/abs/2401.06712)

    本研究提出了一种小样本检测方法，通过使用样式表示来检测机器生成的文本与人类撰写的文本的区别，以解决滥用语言模型带来的问题。

    

    受到指导调整的语言模型的出现使得人类写作的逼真模仿面临着重大滥用风险。然而，我们可以通过检测一段文本是由语言模型还是人类撰写而成来对抗此类滥用行为。本文提出了一种基于样式表示的小样本检测方法，避免了神经网络检测器在面对数据转换时的规约不足的挑战，同时也避免了在推理或检测时需要访问可能生成文档的模型的问题。

    The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally d
    
[^21]: 基于学习的难度校准提升成员推理攻击的能力

    Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks. (arXiv:2401.04929v1 [cs.CR])

    [http://arxiv.org/abs/2401.04929](http://arxiv.org/abs/2401.04929)

    本文介绍了一种基于学习的难度校准的成员推理攻击方法，旨在显著提高低FPR下的TPR，以验证训练模型是否保护隐私。

    

    机器学习模型，特别是深度神经网络，目前是各种应用的重要组成部分，从医疗保健到金融。然而，使用敏感数据来训练这些模型引发了对隐私和安全的担忧。一种验证训练模型是否保护隐私的方法是成员推理攻击（MIA），它允许对手确定特定数据点是否是模型的训练数据集的一部分。虽然已经在文献中提出了一系列的MIA，但只有少数能够在低假阳性率（FPR）区域（0.01%~1%）实现较高的真阳性率（TPR）。这是实际应用于实际场景中的MIA必须考虑的关键因素。在本文中，我们提出了一种新颖的MIA方法，旨在显著提高低FPR的TPR。我们的方法名为基于学习的难度校准（LDC-MIA），通过使用神经网络分类器将数据记录以其难度级别进行表征。

    Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network clas
    
[^22]: 训练的力量：不同的神经网络设置对能源需求的影响

    The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])

    [http://arxiv.org/abs/2401.01851](http://arxiv.org/abs/2401.01851)

    本文研究了机器学习训练方案和学习范式对能源消耗的影响，并探讨了预训练和多任务训练在可持续机器学习方面的潜力。

    

    本研究探讨机器学习训练方案和学习范式的变化对相应能源消耗的影响。虽然数据的可用性提高和高性能硬件的创新推动了复杂模型的训练，但也支持了能源消耗和碳排放的消隐。因此，本研究的目标是增加人们对一般训练参数和过程（从学习率到批量大小再到知识传输）的能源影响的认识。使用不同的超参数初始化在两种不同的硬件配置上评估多种设置，以获得有意义的结果。在基准结果上进行了预训练和多任务训练实验，以确定它们对可持续机器学习的潜力。

    This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
    
[^23]: 敏感性感知的摊销贝叶斯推断

    Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v1 [stat.ML])

    [http://arxiv.org/abs/2310.11122](http://arxiv.org/abs/2310.11122)

    本文提出了一种敏感性感知的摊销贝叶斯推断方法，通过权重共享和神经网络来进行似然和先验规范的训练，以及对数据扰动和预处理程序的敏感性评估。

    

    贝叶斯推断是在不确定性下进行概率推理和决策的强大框架。现代贝叶斯工作流程中的基本选择涉及似然函数和先验分布的规范、后验逼近器和数据。每个选择都可以显着影响基于模型的推断和后续决策，因此需要进行敏感性分析。在这项工作中，我们提出了一种多方面的方法，将敏感性分析整合到摊销贝叶斯推断（ABI，即基于神经网络的模拟推断）中。首先，我们利用权重共享在训练过程中编码替代似然和先验规范之间的结构相似性，以最小的计算开销。其次，我们利用神经网络的快速推断来评估对各种数据扰动或预处理程序的敏感性。与大多数其他贝叶斯方法相比，这两个步骤都避免了昂贵的计算。

    Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly
    
[^24]: 上下文化政策恢复：通过自适应模仿学习对医疗决策进行建模和解释

    Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])

    [http://arxiv.org/abs/2310.07918](http://arxiv.org/abs/2310.07918)

    本论文提出了一种上下文化政策恢复方法用于建模复杂的医疗决策过程，以解决现有模型在准确性和可解释性之间的权衡问题。该方法将决策策略拆分为上下文特定策略，通过多任务学习来实现建模，并提供复杂行为的简洁描述。

    

    可解释的策略学习旨在从观察到的行为中估计可理解的决策策略；然而，现有模型在准确性和可解释性之间存在权衡。这种权衡限制了基于数据驱动的对人类决策过程的解释，例如，审计医疗决策的偏见和次优实践，我们需要决策过程的模型，能够提供复杂行为的简洁描述。现有方法基本上由于将潜在决策过程表示为通用策略而负担了这种权衡，而实际上人类决策是动态的，可以随上下文信息而大幅改变。因此，我们提出了上下文化政策恢复（CPR），将建模复杂决策过程的问题重新定义为多任务学习问题，其中复杂决策策略由特定上下文的策略组成。CPR将每个上下文特定策略建模为线性的观察-动作映射

    Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
    
[^25]: AMPLIFY: 基于注意力机制的Mixup方法，用于提高Transformer模型的性能和标签平滑

    AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v1 [cs.LG])

    [http://arxiv.org/abs/2309.12689](http://arxiv.org/abs/2309.12689)

    AMPLIFY提出了一种基于注意力机制的Mixup方法，用于减少原始样本中的噪音和异常值对于模型的影响，并在文本分类任务中表现出更好的性能。

    

    Mixup是一种有效的数据增强方法，通过对不同原始样本的线性组合生成新的增强样本。然而，如果原始样本中存在噪音或异常特征，Mixup可能将其传播到增强样本中，导致模型对这些异常值过于敏感。为了解决这个问题，本文提出了一种新的Mixup方法称为AMPLIFY。该方法利用Transformer自身的注意力机制减少原始样本中噪音和异常值对预测结果的影响，无需增加可训练参数，计算成本非常低，从而避免了常见Mixup方法（例如语句Mixup）中资源消耗过高的问题。实验结果表明，在更小的计算资源成本下，AMPLIFY在7个基准数据集的文本分类任务中优于其他Mixup方法，为进一步提高模型性能提供了新的思路和方法。

    Mixup is an effective data augmentation method that generates new augmented samples by aggregating linear combinations of different original samples. However, if there are noises or aberrant features in the original samples, Mixup may propagate them to the augmented samples, leading to over-sensitivity of the model to these outliers . To solve this problem, this paper proposes a new Mixup method called AMPLIFY. This method uses the Attention mechanism of Transformer itself to reduce the influence of noises and aberrant values in the original samples on the prediction results, without increasing additional trainable parameters, and the computational cost is very low, thereby avoiding the problem of high resource consumption in common Mixup methods such as Sentence Mixup . The experimental results show that, under a smaller computational resource cost, AMPLIFY outperforms other Mixup methods in text classification tasks on 7 benchmark datasets, providing new ideas and new ways to further
    
[^26]: 大型语言模型中的文化对齐：基于霍夫斯泰德文化维度的解释性分析

    Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions. (arXiv:2309.12342v1 [cs.CY])

    [http://arxiv.org/abs/2309.12342](http://arxiv.org/abs/2309.12342)

    本研究提出了一种使用霍夫斯泰德文化维度框架来量化大型语言模型与不同文化之间的对齐程度的文化对齐测试（CAT）。通过在不同文化国家应用该方法，我们发现LLMs在解释性文化维度上存在差异，并能量化其与特定国家的文化对齐情况。

    

    大型语言模型（LLMs）的部署引发了关于其文化对齐和对不同文化规范个体的潜在影响的担忧。现有研究主要关注政治和社会偏见以及公众意见，而未涉及文化价值观。为了解决这个局限性，我们提出了文化对齐测试（CAT），利用霍夫斯泰德的文化维度框架量化文化对齐，通过潜变量分析提供解释性的跨文化比较。我们将该方法应用于评估最先进的LLMs（如ChatGPT和Bard）在不同文化国家（美国、沙特阿拉伯、中国和斯洛伐克）中嵌入的文化价值观，使用不同的提示风格和超参数设置。我们的结果不仅量化了LLMs与特定国家的文化对齐程度，而且揭示了LLMs在解释性文化维度上的差异。尽管所有的LLMs都没有提供令人满意的结果

    The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals from various cultural norms. Existing work investigated political and social biases and public opinions rather than their cultural values. To address this limitation, the proposed Cultural Alignment Test (CAT) quantifies cultural alignment using Hofstede's cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to assess the cultural values embedded in state-of-the-art LLMs, such as: ChatGPT and Bard, across diverse cultures of countries: United States (US), Saudi Arabia, China, and Slovakia, using different prompting styles and hyperparameter settings. Our results not only quantify cultural alignment of LLMs with certain countries, but also reveal the difference between LLMs in explanatory cultural dimensions. While all LLMs did not provide satisfactory res
    
[^27]: 使用物理信息神经网络解决椭圆型最优控制问题

    Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks. (arXiv:2308.11925v1 [math.OC])

    [http://arxiv.org/abs/2308.11925](http://arxiv.org/abs/2308.11925)

    本论文提出了一种使用物理信息神经网络（PINNs）求解椭圆型最优控制问题的数值方法，并进行了误差分析和数值实验验证。

    

    在这项工作中，我们提出并分析了一个用于线性和半线性二阶椭圆问题的最优控制问题（有/无盒约束）的数值求解器。该方法基于从最优控制问题的一阶最优性系统推导出的耦合系统，利用物理信息神经网络（PINNs）来解决耦合系统。我们对数值方案进行误差分析，并针对深度神经网络参数（如深度、宽度和参数范围）以及域内和边界上的采样点数，给出了关于状态、控制和伴随状态的$L^2(\Omega)$误差界。分析中的主要工具包括偏移Rademacher复杂度以及神经网络函数的有界性和Lipschitz连续性。我们提供了几个数值示例来说明该方法，并与三种现有方法进行比较。

    In this work, we present and analyze a numerical solver for optimal control problems (without / with box constraint) for linear and semilinear second-order elliptic problems. The approach is based on a coupled system derived from the first-order optimality system of the optimal control problem, and applies physics informed neural networks (PINNs) to solve the coupled system. We present an error analysis of the numerical scheme, and provide $L^2(\Omega)$ error bounds on the state, control and adjoint state in terms of deep neural network parameters (e.g., depth, width, and parameter bounds) and the number of sampling points in the domain and on the boundary. The main tools in the analysis include offset Rademacher complexity and boundedness and Lipschitz continuity of neural network functions. We present several numerical examples to illustrate the approach and compare it with three existing approaches.
    
[^28]: HEAL-SWIN: 球面上的视觉变压器

    HEAL-SWIN: A Vision Transformer On The Sphere. (arXiv:2307.07313v1 [cs.CV])

    [http://arxiv.org/abs/2307.07313](http://arxiv.org/abs/2307.07313)

    HEAL-SWIN是一个基于HEALPix网格和SWIN变压器结合的球面视觉变压器，在处理高分辨率广角鱼眼图像时能够实现无失真且高效的训练，并在语义分割任务中展现出优异的性能。

    

    高分辨率广角鱼眼图像在自动驾驶等机器人应用中变得越来越重要。然而，使用普通的卷积神经网络或视觉变压器处理此类数据存在问题，因为将其投影到平面上的矩形网格时会引入投影和失真损失。我们引入了HEAL-SWIN变压器，它将在天体物理学和宇宙学中使用的高度均匀的层次等面积等纬度像素化（HEALPix）网格与层次移位窗口（SWIN）变压器结合起来，从而得到能够训练高分辨率无失真球面数据的高效灵活模型。在HEAL-SWIN中，HEALPix网格的嵌套结构用于执行SWIN变压器的拼接和窗口操作，从而得到具有最小计算开销的球面数据一维表示。我们展示了我们模型在语义分割上的卓越性能。

    High-resolution wide-angle fisheye images are becoming more and more important for robotics applications such as autonomous driving. However, using ordinary convolutional neural networks or vision transformers on this data is problematic due to projection and distortion losses introduced when projecting to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer, which combines the highly uniform Hierarchical Equal Area iso-Latitude Pixelation (HEALPix) grid used in astrophysics and cosmology with the Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and flexible model capable of training on high-resolution, distortion-free spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used to perform the patching and windowing operations of the SWIN transformer, resulting in a one-dimensional representation of the spherical data with minimal computational overhead. We demonstrate the superior performance of our model for semantic segmentati
    
[^29]: 贝叶斯紧系数样条估计模式的数量

    Bayesian taut splines for estimating the number of modes. (arXiv:2307.05825v1 [stat.ME])

    [http://arxiv.org/abs/2307.05825](http://arxiv.org/abs/2307.05825)

    本研究提出了一种贝叶斯紧系数样条方法，用于估计概率密度函数中模式的数量。该方法结合了核估计器和组合样条，实现了特征探索、模型选择和模式检验，并允许引入专家判断。通过在体育分析中的案例研究中的验证，证明了该方法的实用性。

    

    概率密度函数中模式的数量代表模型的复杂性，也可以看作现有亚群体的数量。尽管其相关性，对其估计的研究非常有限。我们针对单变量情况提出一个新颖的方法，致力于预测准确性，受到了问题的一些被忽视的方面的启发。我们认为解决方案需要结构，模式的主观且不确定性，以及融合全局和局部密度特性的整体视图的便利性。我们的方法结合了灵活的核估计器和简洁的组合样条。特征探索、模型选择和模式检验都在贝叶斯推理范式中实现，为软解决方案提供了便利，并允许在过程中引入专家判断。我们的提议的实用性通过在体育分析中的案例研究中进行了验证，并展示了多个陪伴的可视化。

    The number of modes in a probability density function is representative of the model's complexity and can also be viewed as the number of existing subpopulations. Despite its relevance, little research has been devoted to its estimation. Focusing on the univariate setting, we propose a novel approach targeting prediction accuracy inspired by some overlooked aspects of the problem. We argue for the need for structure in the solutions, the subjective and uncertain nature of modes, and the convenience of a holistic view blending global and local density properties. Our method builds upon a combination of flexible kernel estimators and parsimonious compositional splines. Feature exploration, model selection and mode testing are implemented in the Bayesian inference paradigm, providing soft solutions and allowing to incorporate expert judgement in the process. The usefulness of our proposal is illustrated through a case study in sports analytics, showcasing multiple companion visualisation 
    
[^30]: 通用等变Transformer：用于3D分子相互作用学习

    Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning. (arXiv:2306.01474v1 [cs.LG])

    [http://arxiv.org/abs/2306.01474](http://arxiv.org/abs/2306.01474)

    本文提出了一种通用等变Transformer用于学习3D分子相互作用，该模型具有双层注意力模块、前馈模块和层归一化模块，每个模块都是E（3）等变的，可以有效地捕捉块级和原子级的交互，实验结果表明其在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面优于各种最先进的方法。

    

    生物学和药物开发中的许多过程涉及不同分子之间的各种3D相互作用，例如蛋白质与蛋白质，蛋白质与小分子等。设计一个通用模型来学习普适的分子相互作用具有重要价值，但也具有挑战性，因为不同的分子通常以不同粒度表示。本文首先提出了将3D分子通用表示为集合的几何图形图，与传统单层表示形式形成对比。在提出的统一表示下，我们提出了通用等变Transformer（GET），以有效地捕捉稀疏块级和密集原子级交互。具体而言，GET由双层注意力模块、前馈模块和层归一化模块组成，值得注意的是，每个模块都是E（3）等变的，以满足3D世界的对称性。在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面进行了大量实验，表明GET优于各种最先进的方法。

    Many processes in biology and drug discovery involve various 3D interactions between different molecules, such as protein and protein, protein and small molecule, etc. Designing a generalist model to learn universal molecular interactions is valuable yet challenging, given that different molecules are usually represented in different granularity. In this paper, we first propose to universally represent a 3D molecule as a geometric graph of sets, in contrast to conventional single-level representations. Upon the proposed unified representation, we then propose a Generalist Equivariant Transformer (GET) to effectively capture both sparse block-level and dense atom-level interactions. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where, notably, each module is E(3) equivariant to meet the symmetry of 3D world. Extensive experiments on the prediction of protein-protein affinity, ligand binding affinity, and ligand effica
    
[^31]: 贝叶斯神经网络替代贝叶斯优化中的高斯过程模型

    A Study of Bayesian Neural Network Surrogates for Bayesian Optimization. (arXiv:2305.20028v1 [cs.LG])

    [http://arxiv.org/abs/2305.20028](http://arxiv.org/abs/2305.20028)

    本文研究贝叶斯神经网络替代高斯过程模型作为贝叶斯优化中的代理模型，并在多个基准问题上证明了其优于标准GP代理的能力。

    

    贝叶斯优化是一种高效的优化方法，适用于难以查询的目标函数。这些目标函数通常由高斯过程（GP）代理模型表示，其易于优化并支持精确推理。虽然标准的GP代理已经在贝叶斯优化中被广泛应用，但贝叶斯神经网络（BNNs）最近成为了一个实用的函数逼近器，与标准的GP相比具有许多优点，例如天然处理非平稳性以及学习高维数据的表示。在本文中，我们研究了BNN作为标准GP代理的替代品。我们考虑了各种有限宽度BNN的近似推理过程，包括高质量Hamiltonian Monte Carlo，低成本的随机MCMC和启发式方法（如深度集成）。我们还考虑了无限宽度BNN和部分随机模型，例如深度核学习。我们评估了这些代理模型在多个基准问题上的表现，并证明它们在某些情况下可以优于标准GP代理。我们的结果表明，BNN是传统代理模型在贝叶斯优化中的一个很有前途的替代选择。

    Bayesian optimization is a highly efficient approach to optimizing objective functions which are expensive to query. These objectives are typically represented by Gaussian process (GP) surrogate models which are easy to optimize and support exact inference. While standard GP surrogates have been well-established in Bayesian optimization, Bayesian neural networks (BNNs) have recently become practical function approximators, with many benefits over standard GPs such as the ability to naturally handle non-stationarity and learn representations for high-dimensional data. In this paper, we study BNNs as alternatives to standard GP surrogates for optimization. We consider a variety of approximate inference procedures for finite-width BNNs, including high-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics such as deep ensembles. We also consider infinite-width BNNs and partially stochastic models such as deep kernel learning. We evaluate this collection of surrogate mod
    
[^32]: 通过领域知识启示的深度学习进行药物推荐

    Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])

    [http://arxiv.org/abs/2305.19604](http://arxiv.org/abs/2305.19604)

    提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。

    

    药物推荐是医疗保健的基本但至关重要的分支，提供机会为复杂健康状况的患者支持临床医生更精确的药物处方。从电子健康记录（EHR）中学习推荐药物是先前研究中最常见的方法。然而，大多数研究忽视了根据患者的EHR中的临床表现纳入领域知识的问题。为了解决这些问题，我们提出了一种新颖的基于动态领域知识的药物推荐框架，即领域知识启示网络（DKINet），用于将领域知识与可观察的患者临床表现相结合。特别是，我们首先设计了一个基于领域知识的编码器来捕捉领域信息，然后开发了一个数据驱动的编码器将领域知识整合到可观察的EHR中。

    Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
    
[^33]: 分布感知的公平性测试生成

    Distribution-aware Fairness Test Generation. (arXiv:2305.13935v1 [cs.CV])

    [http://arxiv.org/abs/2305.13935](http://arxiv.org/abs/2305.13935)

    本文介绍了一种名为DistroFair的分布感知的公平性测试方法，可以从图像分类器中检测到类级别的公平性违规。

    

    本文探讨如何验证图像识别软件中的组公平性。我们提出了一种分布感知的公平性测试方法（称为DistroFair），通过将超出分布范围的对象引入到图像识别器中，通过三种语义保留图像变换 - 对象删除，对象插入和对象旋转来系统性地暴露图像分类器中的类级别公平性违规。我们使用两个知名数据集（CityScapes和MS-COCO）和三个主要的商业图像识别软件（即Amazon Rekognition，Google Cloud Vision和Azure计算机视觉）对DistroFair进行评估。结果显示，DistroFair生成的图像中，约有21％通过真实标准或元测试标准显露出了类级别的公平性违规。

    This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DistroFair) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DistroFair automatically learns the distribution (e.g., number/orientation) of objects in a set of images. Then it systematically mutates objects in the images to become OOD using three semantic-preserving image mutations -- object deletion, object insertion and object rotation. We evaluate DistroFair using two well-known datasets (CityScapes and MS-COCO) and three major, commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision). Results show that about 21% of images generated by DistroFair reveal class-level fairness violations using either ground truth or metamorphic oracles. DistroFair 
    
[^34]: Bake Off重访：对最近时间序列分类算法的评述和实验评估

    Bake off redux: a review and experimental evaluation of recent time series classification algorithms. (arXiv:2304.13029v1 [cs.LG])

    [http://arxiv.org/abs/2304.13029](http://arxiv.org/abs/2304.13029)

    本文重访烘焙大赛，评估了最近时间序列分类算法在112个数据集上的表现。该论文通过分类法将这些算法分为五类，为TSC领域的发展提供了贡献。

    

    一篇研究论文在2017年比较了18个时间序列分类（TSC）算法在来自加州大学河滨分校（UCR）存档的85个数据集上的表现。这项研究通常被称为“烘焙比赛”，发现只有9个算法的表现显著优于使用的动态时间规整（DTW）和旋转森林基准。该研究通过从时间序列数据中提取特征的算法类型对每个算法进行分类，形成了五种主要算法类型的分类法。与可以重现结果的代码和结果的提供相结合，这些算法的分类和可访问的结果推动了TSC领域的普及。六年过去了，UCR存档已扩展到112个数据集，并且已经提出了大量新的算法。我们重访烘焙大赛，看看每个提出的类别自原始出版以来的进展，并评估新算法的性能。

    In 2017, a research paper compared 18 Time Series Classification (TSC) algorithms on 85 datasets from the University of California, Riverside (UCR) archive. This study, commonly referred to as a `bake off', identified that only nine algorithms performed significantly better than the Dynamic Time Warping (DTW) and Rotation Forest benchmarks that were used. The study categorised each algorithm by the type of feature they extract from time series data, forming a taxonomy of five main algorithm types. This categorisation of algorithms alongside the provision of code and accessible results for reproducibility has helped fuel an increase in popularity of the TSC field. Over six years have passed since this bake off, the UCR archive has expanded to 112 datasets and there have been a large number of new algorithms proposed. We revisit the bake off, seeing how each of the proposed categories have advanced since the original publication, and evaluate the performance of newer algorithms against t
    
[^35]: CF-VAE：基于VAE和因果流的因果分离表示学习

    CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows. (arXiv:2304.09010v1 [cs.LG])

    [http://arxiv.org/abs/2304.09010](http://arxiv.org/abs/2304.09010)

    本文提出了一种新的因果流以进行因果分离表示学习，设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力，并展示了在合成和真实数据集上实现因果分离并进行干预实验的结果。

    

    学习分离表示在表示学习中至关重要，旨在学习数据的低维表示，其中每个维度对应一个潜在的生成因素。由于生成因素之间可能存在因果关系，因果分离表示学习已经受到广泛关注。本文首先提出了一种新的可以将因果结构信息引入模型中的流，称为因果流。基于广泛用于分离表示学习的变分自编码器（VAE），我们设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力。通过进一步引入基准因素的监督，我们展示了我们模型的分离可识别性。在合成和真实数据集上的实验结果表明，CF-VAE可以实现因果分离并进行干预实验。

    Learning disentangled representations is important in representation learning, aiming to learn a low dimensional representation of data where each dimension corresponds to one underlying generative factor. Due to the possibility of causal relationships between generative factors, causal disentangled representation learning has received widespread attention. In this paper, we first propose a new flows that can incorporate causal structure information into the model, called causal flows. Based on the variational autoencoders(VAE) commonly used in disentangled representation learning, we design a new model, CF-VAE, which enhances the disentanglement ability of the VAE encoder by utilizing the causal flows. By further introducing the supervision of ground-truth factors, we demonstrate the disentanglement identifiability of our model. Experimental results on both synthetic and real datasets show that CF-VAE can achieve causal disentanglement and perform intervention experiments. Moreover, C
    
[^36]: StepMix: 一个用于外部变量广义混合模型的伪似然估计的Python包

    StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])

    [http://arxiv.org/abs/2304.03853](http://arxiv.org/abs/2304.03853)

    StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。

    

    StepMix是一个用于广义有限混合模型(潜在剖面和潜在类分析)与外部变量(协变量和远程结果)的伪似然估计(单步、两步和三步方法)的开源软件包。在许多社会科学的应用中，主要目标不仅是将个体聚类成潜在类别，还包括使用这些类别来开发更复杂的统计模型。这些模型通常分为一个将潜在类别与观察指标相关联的测量模型和一个将协变量和结果变量与潜在类别相关联的结构模型。测量和结构模型可以使用所谓的一步法共同估计，也可以使用逐步方法逐步估计，对于从业人员来说，这些方法在估计潜在类别的可解释性方面具有显著优势。除了一步法，StepMix还实现了文献中提出的最重要的逐步估计方法，提供了用户友好的界面，方便模型的估计、选择和解释。

    StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
    
[^37]: RED-PSM: 带去噪正则化的部分可分模型用于动态成像

    RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging. (arXiv:2304.03483v1 [eess.IV])

    [http://arxiv.org/abs/2304.03483](http://arxiv.org/abs/2304.03483)

    本文提出了一种称为RED-PSM的方法，将部分可分模型与去噪正则化相结合，用于解决动态成像问题，数值实验证明其优越性。

    

    动态成像是指利用被欠采样的测量数据恢复每个时间点上的时变二维或三维物体。尤其是在动态断层扫描中，每个时间点上只有一个视角下的单个投影可用，使得问题严重算不可逆。本文提出了一种称为RED-PSM的方法，首次将两种强大的技术结合起来解决这个具有挑战性的成像问题。第一种技术是部分可分模型，已经用于高效地为时空目标引入低秩先验。第二种是最近提出的去噪正则化(RED)，它提供了一种灵活的框架，利用最先进的图像去噪算法处理各种反问题。我们提出了一个带正则化的部分可分目标，通过变量分裂和ADMM优化方案，并证明了我们的目标收敛于一个满足优化问题的稳定点。我们通过在具有挑战性的动态断层扫描问题上进行了大量的数值实验，证明了所提出的RED-PSM方法的优越性，相较于现有的动态成像技术。

    Dynamic imaging addresses the recovery of a time-varying 2D or 3D object at each time instant using its undersampled measurements. In particular, in the case of dynamic tomography, only a single projection at a single view angle may be available at a time, making the problem severely ill-posed. In this work, we propose an approach, RED-PSM, which combines for the first time two powerful techniques to address this challenging imaging problem. The first, are partially separable models, which have been used to efficiently introduce a low-rank prior for the spatio-temporal object. The second is the recent Regularization by Denoising (RED), which provides a flexible framework to exploit the impressive performance of state-of-the-art image denoising algorithms, for various inverse problems. We propose a partially separable objective with RED and an optimization scheme with variable splitting and ADMM, and prove convergence of our objective to a value corresponding to a stationary point satis
    
[^38]: 研究模型宽度和密度对标签噪声下泛化性能的影响

    Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise. (arXiv:2208.08003v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.08003](http://arxiv.org/abs/2208.08003)

    本文研究发现标签噪声会导致双丘降曲线出现“最终上升”，即在足够大的噪声样本比率下，中等宽度下实现最佳泛化性能。随机丢弃可训练参数来减少密度可在标签噪声下改善泛化性能。

    

    扩大过参数化神经网络的规模是实现最先进性能的关键。这是通过双丘降现象捕捉的，其中测试损失随着模型宽度的增加呈现出降低-增加-降低的模式。然而，标签噪声对测试损失曲线的影响尚未被充分探索。在本文中，我们揭示了一个有趣的现象，即标签噪声导致原本观察到的双丘降曲线出现了“最终上升”。具体而言，在足够大的噪声样本比率下，中等宽度下实现最佳泛化性能。通过理论分析，我们将这种现象归因于标签噪声引起的测试损失方差形状转换。此外，我们将最终上升现象扩展到模型密度，并提供了第一个理论表征，表明随机丢弃可训练参数来减少密度可在标签噪声下改善泛化性能。

    Increasing the size of overparameterized neural networks has been a key in achieving state-of-the-art performance. This is captured by the double descent phenomenon, where the test loss follows a decreasing-increasing-decreasing pattern as model width increases. However, the effect of label noise on the test loss curve has not been fully explored. In this work, we uncover an intriguing phenomenon where label noise leads to a \textit{final ascent} in the originally observed double descent curve. Specifically, under a sufficiently large noise-to-sample-size ratio, optimal generalization is achieved at intermediate widths. Through theoretical analysis, we attribute this phenomenon to the shape transition of test loss variance induced by label noise. Furthermore, we extend the final ascent phenomenon to model density and provide the first theoretical characterization showing that reducing density by randomly dropping trainable parameters improves generalization under label noise. We also t
    

