# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [JL-lemma derived Optimal Projections for Discriminative Dictionary Learning.](http://arxiv.org/abs/2308.13991) | 本文提出了一种名为JLSPCADL的新方法，通过利用Johnson-Lindenstrauss引理选择一个维数的转换空间，从而实现判别式字典学习并提供更好的分类结果。 |
| [^2] | [Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective.](http://arxiv.org/abs/2308.13985) | 本论文重新审视了多任务学习中的标量化方法，并从理论的角度探讨了标量化是否能够充分探索帕累托前沿。结果显示，与最近的研究声称的经验优势相反，标量化本质上无法进行全面探索，特别是对于那些平衡了paren |
| [^3] | [Interpolation of mountain weather forecasts by machine learning.](http://arxiv.org/abs/2308.13983) | 本研究提出了一种通过机器学习来插值山区天气预报的方法，通过利用当前观测数据和周围平原的预报数据，解决了在复杂地形中数值模拟精度降低的问题，并研究了在降水预测中使用二元交叉熵的方法。 |
| [^4] | [Universal Graph Continual Learning.](http://arxiv.org/abs/2308.13982) | 本研究解决了图形学习中的灾难性遗忘问题，通过维持图形间的结构一致性和一种新的方法，实现了通用图形连续学习，并在真实世界的图形数据集上取得了显著的改进。 |
| [^5] | [Understanding the Usage of QUBO-based Hamiltonian Function in Combinatorial Optimization over Graphs: A Discussion Using Max Cut (MC) Problem.](http://arxiv.org/abs/2308.13978) | 研究探讨了在图上基于QUBO公式的最大切割问题中，如何使用基于强化学习范式和哈密顿函数来解决组合优化问题。通过使用图神经网络作为信息传递架构，并通过三种不同的公式形式进行实验，发现... |
| [^6] | [Label Denoising through Cross-Model Agreement.](http://arxiv.org/abs/2308.13976) | 本文提出了一种通过跨模型一致性进行标签去噪的方法。通过观察发现，不同模型在干净示例上的预测相对相似，而在有噪声示例上的预测在不同模型之间变化更大。在这种观察的启发下，我们提出了使用跨模型一致性进行去噪的方法（DeCA），旨在最小化两个机器学习模型参数化的真实标签分布之间的KL散度，同时最大化数据观测的似然。 |
| [^7] | [FAM: fast adaptive meta-learning.](http://arxiv.org/abs/2308.13970) | 本论文提出了一个快速自适应联邦元学习（FAM）框架，可以协作学习一个全局模型，并在个别客户端上进行个性化。这解决了数据分布发散和隐私限制的问题，并且适用于需要在不同客户端之间进行个性化的领域转变。 |
| [^8] | [Fixating on Attention: Integrating Human Eye Tracking into Vision Transformers.](http://arxiv.org/abs/2308.13969) | 本研究展示了如何将人眼追踪集成到视觉Transformer模型中，提高在多种驾驶情况和数据集上的准确性。 |
| [^9] | [Multivariate time series classification with dual attention network.](http://arxiv.org/abs/2308.13968) | 通过引入双重注意力机制，该论文提出了一种全新的网络（DA-Net），用于多元时间序列分类。DA-Net能够同时提取局部和全局特征，从而识别关键的局部序列片段并建立全局长期依赖关系。 |
| [^10] | [Sparse Models for Machine Learning.](http://arxiv.org/abs/2308.13960) | 稀疏模型广泛应用于统计学和机器学习领域，可以用于回归、分类、图模型选择、稀疏估计和维度降低等任务，并在发现预测模式方面具有重要作用。 |
| [^11] | [Differentiable Weight Masks for Domain Transfer.](http://arxiv.org/abs/2308.13957) | 本论文通过将模块化权重和领域迁移相结合，研究了三种权重掩码方法，并分析它们在保持源任务知识的同时允许高效微调目标任务的能力。 |
| [^12] | [Deep learning assisted robust detection techniques for a chipless RFID sensor tag.](http://arxiv.org/abs/2308.13944) | 本文首次将机器学习和深度学习应用于无芯片RFID传感器标签的鲁棒检测中，并提出了一种端到端的设计和实现方法。同时，首次报告了不同标签表面形状变化对检测的影响。 |
| [^13] | [A small vocabulary database of ultrasound image sequences of vocal tract dynamics.](http://arxiv.org/abs/2308.13941) | 本文介绍了一个新的数据库，包含了连续的发声器官和声学语音数据，用超声图像和声音数据来研究言语生成过程中舌头上轮廓的可视化。这个数据库是由哥伦比亚圣坦德区的17名年轻被试完成的。 |
| [^14] | [A transport approach to sequential simulation-based inference.](http://arxiv.org/abs/2308.13940) | 提出了一种新的基于运输的方法，用于高效地进行静态模型参数的顺序贝叶斯推断。该方法可以处理包括干扰参数的复杂噪声模型，并且适用于仅作为黑箱的正向模型。数值应用表明该方法在使用电导率测量来表征冰厚度的情况下是有效的。 |
| [^15] | [A Two-Dimensional Deep Network for RF-based Drone Detection and Identification Towards Secure Coverage Extension.](http://arxiv.org/abs/2308.13906) | 这篇论文提出了基于射频的无人机检测与识别的二维深度网络模型，利用短时傅里叶变换提取了包含时域和频域信息的二维特征，并通过卷积神经网络实现了多类别分类。实验结果表明，该模型在扩展数据集上具有更高的准确性和更快的收敛速度，同时具备良好的平衡性能。 |
| [^16] | [LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors.](http://arxiv.org/abs/2308.13904) | LMSanitator是一种新颖的方法，用于检测和消除Transformer模型中的任务不可知后门。与传统方法不同，LMSanitator通过逆转预定义的攻击向量而不是触发器，实现更好的收敛性能和后门检测精确度。 |
| [^17] | [Semi-Supervised Semantic Segmentation via Marginal Contextual Information.](http://arxiv.org/abs/2308.13900) | 通过利用分割图中标签的空间相关性，我们提出的S4MC方法在半监督语义分割中通过增强伪标签的方式，并提高了无标签数据的使用量，从而实现了超越现有方法的性能提升。 |
| [^18] | [Memory-aware Scheduling for Complex Wired Networks with Iterative Graph Optimization.](http://arxiv.org/abs/2308.13898) | 在本文中，我们提出了一种基于迭代计算图优化的高效内存感知调度框架，该框架通过迭代图融合和拓扑感知的变量修剪来简化计算图并实现调度优化，有望在复杂有线网络上实现更好的扩展性和减少内存占用。 |
| [^19] | [Federated Fine-tuning of Billion-Sized Language Models across Mobile Devices.](http://arxiv.org/abs/2308.13894) | 这项工作引入了一种创新的FL协议FwdLLM，旨在提高在移动设备上进行十亿规模语言模型的联邦微调（FedLLM）的效率。FwdLLM通过使用无反向传播（BP）训练方法以及“扰动推断”来提高内存效率和时间效率。 |
| [^20] | [Drug Interaction Vectors Neural Network: DrIVeNN.](http://arxiv.org/abs/2308.13891) | 本研究开发了一个名为DrIVeNN的两层神经网络，该网络通过整合药物特征，如分子结构、药物-蛋白相互作用和单药副作用等来建立和评估多药并用不良药物事件（ADEs）模型。 |
| [^21] | [Applications of machine Learning to improve the efficiency and range of microbial biosynthesis: a review of state-of-art techniques.](http://arxiv.org/abs/2308.13877) | 本文综述了机器学习在微生物生物合成中的应用，并提供了对两个关键领域的全面解释和应用的现状及问题。 |
| [^22] | [Class Binarization to NeuroEvolution for Multiclass Classification.](http://arxiv.org/abs/2308.13876) | 本文将类别二值化技术应用于神经进化算法，提出了一种新方法将纠错输出码(ECOC)应用于神经进化算法，用于多类别分类。 |
| [^23] | [Brain-like representational straightening of natural movies in robust feedforward neural networks.](http://arxiv.org/abs/2308.13870) | 本文研究在强大的前馈神经网络中，通过对输入图像的噪声鲁棒性进行训练，实现了对自然电影进行表征矫正。这种矫正在生物视觉中是常见的，但人工神经网络通常未能呈现这种现象。通过线性插值，这种矫正可以用于生成中间电影帧。 |
| [^24] | [Late Stopping: Avoiding Confidently Learning from Mislabeled Examples.](http://arxiv.org/abs/2308.13862) | 本文提出了一种新的框架，称为停止学习，通过移除高概率错误标记示例来缩小噪声数据集，同时保留大多数干净困难示例，以提高模型的最优泛化性能。 |
| [^25] | [Bias in Unsupervised Anomaly Detection in Brain MRI.](http://arxiv.org/abs/2308.13861) | 本论文针对无监督异常检测中的偏差问题进行了分析。通过研究训练与测试分布之间的非病理性偏移，揭示了这些偏移的程度。 |
| [^26] | [Effectively Heterogeneous Federated Learning: A Pairing and Split Learning Based Approach.](http://arxiv.org/abs/2308.13849) | 该论文提出了一种有效的异构联邦学习方法，通过配对和分裂学习，解决了异构性带来的训练速度问题，提高了联邦学习的效率。 |
| [^27] | [Optimal Transport-inspired Deep Learning Framework for Slow-Decaying Problems: Exploiting Sinkhorn Loss and Wasserstein Kernel.](http://arxiv.org/abs/2308.13840) | 本论文提出了一种将最优传输理论与神经网络结合的新的减小模型（ROM）框架。通过利用Sinkhorn算法进行训练，该框架可以捕捉数据的几何结构，从而实现精确学习减少的解决方案流形。 |
| [^28] | [Price-Discrimination Game for Distributed Resource Management in Federated Learning.](http://arxiv.org/abs/2308.13838) | 本论文提出了一种价格差异化游戏（PDG），通过对不同客户提供的服务进行定价差异化，改善了联合学习的性能并降低了激励客户参与联合学习的成本。 |
| [^29] | [Class-constrained t-SNE: Combining Data Features and Class Probabilities.](http://arxiv.org/abs/2308.13837) | 本研究提出了一种新的方法，类别约束的t-SNE，用于结合数据特征和类别概率进行降维分析。这种方法通过平衡代价函数中的两个组成部分，优化数据点的位置和代表性表示。 |
| [^30] | [Deep Learning for Structure-Preserving Universal Stable Koopman-Inspired Embeddings for Nonlinear Canonical Hamiltonian Dynamics.](http://arxiv.org/abs/2308.13835) | 该论文利用深度学习为非线性哈密顿系统发现了保持结构的全局线性化嵌入，通过辛变换获得紧凑辛嵌入，并重点关注其动力学的有界稳定性。 |
| [^31] | [A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions.](http://arxiv.org/abs/2308.13821) | 本综述对图上不平衡学习进行了全面的审视，旨在纠正数据分布偏差，以获得更准确和代表性的学习结果。 |
| [^32] | [Guaranteed Stable Quadratic Models and their applications in SINDy and Operator Inference.](http://arxiv.org/abs/2308.13819) | 本研究提出了一种学习稳定二次模型的推断方法，通过设置适当的优化问题，可以构建具有局部和全局稳定性的二次系统模型。 |
| [^33] | [Homological Convolutional Neural Networks.](http://arxiv.org/abs/2308.13816) | 提出了一种新的同调卷积神经网络架构，通过拓扑约束网络表示来利用表格数据的结构组织，从稀疏的表格数据中获取空间信息，具有良好的有效性和泛化能力。 |
| [^34] | [SyMOT-Flow: Learning optimal transport flow for two arbitrary distributions with maximum mean discrepancy.](http://arxiv.org/abs/2308.13815) | 本文介绍了一个名为SyMOT-Flow的新模型，它通过最小化两个未知分布样本之间的对称最大平均差异来训练可逆转换，并结合最优输运成本作为正则化，将未知分布转换为标准正态分布。实验证明这种转换可以实现更稳定准确的样本生成。 |
| [^35] | [DeLELSTM: Decomposition-based Linear Explainable LSTM to Capture Instantaneous and Long-term Effects in Time Series.](http://arxiv.org/abs/2308.13797) | 本文提出了一种基于分解的线性可解释LSTM（DeLELSTM）模型，用于提高时序模型的可解释性。与传统方法不同的是，DeLELSTM能够区分新数据的瞬时影响和历史数据的长期影响。这一方法包含了标准LSTM和张量化LSTM两部分，分别处理不同的影响。 |
| [^36] | [Out-of-distribution detection using normalizing flows on the data manifold.](http://arxiv.org/abs/2308.13792) | 利用正则化流在低维数据流形上进行外域检测，通过估计密度和测量与流形的距离来判断外域数据，有效提高外域检测的准确性。 |
| [^37] | [Large-scale gradient-based training of Mixtures of Factor Analyzers.](http://arxiv.org/abs/2308.13778) | 这篇论文提出了一种大规模基于梯度的混合因子分析器（MFA）训练方法，通过随机梯度下降和随机质心初始化，在处理高维数据时能够简化训练和初始化过程，并避免使用传统方法（如期望-最大化算法）处理大量数据时可能出现的问题。 |
| [^38] | [Self-Supervised Scalable Deep Compressed Sensing.](http://arxiv.org/abs/2308.13777) | 本文提出了一种自监督的可扩展深度压缩感知方法，不需要标记的测量-地面真实数据，并且可以处理任意的采样比率和矩阵。该方法包括一个双域损失和四个恢复阶段，通过最大化数据/信息利用率来提高准确性。 |
| [^39] | [Bengali Document Layout Analysis with Detectron2.](http://arxiv.org/abs/2308.13769) | 本研究使用Detectron2库中的先进模型，通过对孟加拉语文档进行布局分析，提高了模型的准确性。该研究讨论了速度和准确性的权衡，为未来的研究提出了方向。 |
| [^40] | [Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content.](http://arxiv.org/abs/2308.13768) | 本研究提出了一种新的双阶段优化技术，使用对抗性微调来解决大型语言模型中意外生成有害内容的问题。通过迭代的提示和微调，实现了持续的改进和性能提升，并在具有挑战性的数据集上展示了显著的分类准确度提升。 |
| [^41] | [SamDSK: Combining Segment Anything Model with Domain-Specific Knowledge for Semi-Supervised Learning in Medical Image Segmentation.](http://arxiv.org/abs/2308.13759) | 这篇论文介绍了一种新方法SamDSK，结合分割任意模型和领域特定知识，在医学图像分割中进行半监督学习。方法包括迭代的两个阶段：分割模型训练和使用训练好的模型和领域特定知识扩展有标签集。通过将分割建议和领域特定知识结合，构建无标签图像的注释。 |
| [^42] | [PE-MED: Prompt Enhancement for Interactive Medical Image Segmentation.](http://arxiv.org/abs/2308.13746) | 本文介绍了一个名为PE-MED的新框架，用于提高交互式医学图像分割的成功率。它通过引入自循环策略和提示注意力学习模块来改善用户提供的提示信息的利用，从而防止不利情况的发生，并提高分割结果的准确性。 |
| [^43] | [Learning variational autoencoders via MCMC speed measures.](http://arxiv.org/abs/2308.13731) | 本研究提出了一种基于熵的短期调整MCMC链的方法，用于在优化更紧的变分边界的同时，适应深度潜变量模型的提案分布。实验证明，这种方法能够使模型得到更高的保留对数似然和改进的生成性能。 |
| [^44] | [Muffin: A Framework Toward Multi-Dimension AI Fairness by Uniting Off-the-Shelf Models.](http://arxiv.org/abs/2308.13730) | 本文提出了一个名为Muffin的框架，通过整合现成模型，实现多维度人工智能公平性。研究首先揭示了不同不公平属性之间的强相关性，随后提出了一个新的多维度公平性框架Muffin，其中包括一个自动工具来提高公平性。 |
| [^45] | [Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: II. dynamics forecasting.](http://arxiv.org/abs/2308.13727) | 本文介绍了一种基于动态模态分解（DMD）算法的优化版本（OPT-DMD），用于分析ExB等离子体的数据驱动降阶建模，并评估了该算法在不同物理参数下预测等离子体动力学的能力。 |
| [^46] | [Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: I. Extraction of spatiotemporally coherent patterns.](http://arxiv.org/abs/2308.13726) | 本论文评估了基于数据驱动的动态模式分解（DMD）算法在ExB等离子体动态分析和降阶建模中的实用性和普适性，并展示了DMD在提取高维等离子体动力学中的时空模式方面的有效性。 |
| [^47] | [Time-to-Pattern: Information-Theoretic Unsupervised Learning for Scalable Time Series Summarization.](http://arxiv.org/abs/2308.13722) | 这篇论文提出了一种名为Time-to-Pattern的时间序列摘要方法，通过基于信息论的无监督学习，在一个可解释的潜在空间中学习离散时间序列的信息嵌入，找到一组不同的模式以编码最显著的信息，提高了摘要的质量。 |
| [^48] | [Uncovering Promises and Challenges of Federated Learning to Detect Cardiovascular Diseases: A Scoping Literature Review.](http://arxiv.org/abs/2308.13714) | 该论文综述了联邦学习在心血管疾病检测中的应用现状，并讨论了其优势和挑战。研究比较了联邦学习与传统集中式学习方法的模型准确性、隐私和数据分布处理能力。最后，提供了对联邦学习当前挑战的批判性分析。 |
| [^49] | [Residual Denoising Diffusion Models.](http://arxiv.org/abs/2308.13712) | 提出了剩余去噪扩散模型（RDDM），相比于现有的扩散模型，该模型通过预测残差来表示从目标域到输入域的方向性扩散，并同时估计噪声来考虑扩散过程中的随机扰动，从而实现了统一的图像生成和恢复。 |
| [^50] | [PAITS: Pretraining and Augmentation for Irregularly-Sampled Time Series.](http://arxiv.org/abs/2308.13703) | PAITS是针对稀疏和不规则采样时间序列数据集的预训练和增强框架，通过结合NLP启发式预训练任务和增强技术，以及随机搜索来找到适合特定数据集的有效策略，相较于传统方法，能够持续改进多个数据集和领域的预训练效果。 |
| [^51] | [Party Prediction for Twitter.](http://arxiv.org/abs/2308.13699) | 本文提供了对当前党派预测做法的综合调查和实证比较，并提出了几种新的方法，这些方法与最先进的方法相竞争或超越之，然而却需要更少的计算资源。 |
| [^52] | [Rethinking Language Models as Symbolic Knowledge Graphs.](http://arxiv.org/abs/2308.13676) | 本研究对不同大小和能力的语言模型进行了全面评估，发现它们能否涵盖知识图谱的复杂拓扑和语义属性，这对于推理过程至关重要。 |
| [^53] | [Linear Oscillation: The Aesthetics of Confusion for Vision Transformer.](http://arxiv.org/abs/2308.13670) | 研究提出了一种新的激活函数——线性振动（LoC）激活函数，它通过将线性轨迹和振荡偏差无缝融合，捕捉到了“困惑的重要性”的本质。实证研究表明，在需要区分微妙模式的情境中，使用LoC激活函数可以提高网络的鲁棒性。 |
| [^54] | [Network Embedding Using Sparse Approximations of Random Walks.](http://arxiv.org/abs/2308.13663) | 本文提出了一种使用稀疏逼近随机游走的网络嵌入方法，并通过数据聚类和多标签分类的实验验证了其有效性。 |
| [^55] | [Resource-Efficient Federated Learning for Heterogenous and Resource-Constrained Environments.](http://arxiv.org/abs/2308.13662) | 提出了一种新的资源高效联邦学习方法，通过可变剪枝技术和知识蒸馏来解决资源受限设备中的计算和通信挑战，并在保持数据隐私和性能的同时适应异构模型架构。 |
| [^56] | [Go Beyond Imagination: Maximizing Episodic Reachability with World Models.](http://arxiv.org/abs/2308.13661) | 通过引入GoBI（超越想象）内在奖励设计，结合传统的终身新颖性动机和情节内在奖励，利用学习的世界模型最大化了逐步可达扩展，有效解决了强化学习中的奖励稀疏问题，并在迷你网格导航任务和DeepMind控制套件中取得了优越的性能。 |
| [^57] | [Pretty darn good control: when are approximate solutions better than approximate models.](http://arxiv.org/abs/2308.13654) | 本研究探讨了在实际系统中，当近似、简化的模型的最优解比更准确模型的近似解更好时的情况，并通过深度强化学习算法使用深度神经网络成功逼近解决了这一问题。 |
| [^58] | [GRASP: A Rehearsal Policy for Efficient Online Continual Learning.](http://arxiv.org/abs/2308.13646) | GRASP是一种新的样本选择策略，根据样本的代表性选择最适合学习的样本，从而提高了在线渐进式学习的效率。 |
| [^59] | [Active learning for fast and slow modeling attacks on Arbiter PUFs.](http://arxiv.org/abs/2308.13645) | 本论文研究了在物理不可克隆函数中的主动学习对于建模攻击的作用，使用率阀值函数提出了一种新的挑战选择方法，通过主动学习实现“快速”学习可以提高准确性，并有效地学习制造的PUF。 |
| [^60] | [ML-Powered Index Tuning: An Overview of Recent Progress and Open Challenges.](http://arxiv.org/abs/2308.13641) | 本文总结了现代云服务中自动索引调优面临的主要挑战，以及机器学习技术在解决这些挑战方面的最新进展。该研究主要关注工作负载选择、候选索引过滤、加速索引配置搜索、减少查询优化器调用的数量和降低性能回归机会等方面，并提出创新的解决方案。 |
| [^61] | [Adaptive whitening with fast gain modulation and slow synaptic plasticity.](http://arxiv.org/abs/2308.13633) | 本研究提出了一个多时间尺度的自适应白化机制模型，使用快速增益调制和慢速突触可塑性相结合的方式来适应变化的感觉统计信息。 |
| [^62] | [Channel Estimation in RIS-Enabled mmWave Wireless Systems: A Variational Inference Approach.](http://arxiv.org/abs/2308.13616) | 本论文提出了一种变分推断方法来估计RIS-启用的毫米波无线系统中的信道状态信息，通过联合估计用户设备到RIS和RIS到基站的通道，减少了信号复杂性和瞬时信道的高信令开销。 |
| [^63] | [AI in Thyroid Cancer Diagnosis: Techniques, Trends, and Future Directions.](http://arxiv.org/abs/2308.13592) | 本论文总结了在甲状腺癌诊断中使用人工智能（AI）技术的相关研究。通过提出新的分类方法并比较现有数据集的特征，研究重点在于如何通过AI工具支持甲状腺癌的诊断和治疗。 |
| [^64] | [GeoExplainer: A Visual Analytics Framework for Spatial Modeling Contextualization and Report Generation.](http://arxiv.org/abs/2308.13588) | GeoExplainer是一种可视化分析框架，旨在支持分析人员创建总结和情境化空间分析的解释性文档。 |
| [^65] | [Text Style Transfer Evaluation Using Large Language Models.](http://arxiv.org/abs/2308.13577) | 大型语言模型（LLMs）有潜力成为人工评估和其他自动化评价指标的可行替代方案。 |
| [^66] | [Stochastic Configuration Machines for Industrial Artificial Intelligence.](http://arxiv.org/abs/2308.13570) | 本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。 |
| [^67] | [Discovering Mental Health Research Topics with Topic Modeling.](http://arxiv.org/abs/2308.13569) | 本研究通过分析大量心理健康研究论文，采用自定义嵌入模型，识别出该领域的一般趋势和高影响力的研究课题。 |
| [^68] | [Region-Disentangled Diffusion Model for High-Fidelity PPG-to-ECG Translation.](http://arxiv.org/abs/2308.13568) | 这项工作引入了区域解耦扩散模型 (RDDM)，用于将PPG转换为ECG信号。RDDM通过选择性地向感兴趣区域添加噪声来捕捉ECG信号的复杂时间动态。 |
| [^69] | [MLLM-DataEngine: An Iterative Refinement Approach for MLLM.](http://arxiv.org/abs/2308.13566) | 本文提出了一种名为MLLM-DataEngine的迭代改进方法，它通过分析模型弱点，生成适当的增量数据集并迭代地增强模型能力。与以往方法相比，MLLM-DataEngine生成的数据在定位、质量和正确性方面表现更好。 |
| [^70] | [SGMM: Stochastic Approximation to Generalized Method of Moments.](http://arxiv.org/abs/2308.13564) | 我们提出了一种新的随机广义矩方法（SGMM），用于估计和推断矩限制模型。该方法具有快速和可扩展的实时处理能力，并且能够处理大规模和在线数据集。 |
| [^71] | [Large Language Models in Analyzing Crash Narratives -- A Comparative Study of ChatGPT, BARD and GPT-4.](http://arxiv.org/abs/2308.13563) | 三个大型语言模型接口(ChatGPT, BARD和GPT4)在分析事故叙述中的效果进行了比较研究。研究结果表明，它们在提取事故相关信息和回答相关问题方面都具有一定的有效性，但也存在一些限制。 |
| [^72] | [Machine Unlearning for Causal Inference.](http://arxiv.org/abs/2308.13559) | 本文介绍了一种机器去学习的方法，用于因果推断中的倾向得分匹配和处理效应估计。研究使用Lalonde数据集，通过神经网络模型进行机器去学习以提高因果分析的性能。 |
| [^73] | [A Systematic Study on Quantifying Bias in GAN-Augmented Data.](http://arxiv.org/abs/2308.13554) | 该研究系统地研究了互动式对抗生成网络（GAN）在数据扩充中引入的偏见，以及用于度量偏见加重程度的度量标准的评估。研究结果表明，虽然有多种度量方法可用，但没有一种单一方法可以可靠地度量不同图像领域中的偏见加重。 |
| [^74] | [Combining Automatic Coding and Instructor Input to Generate ENA Visualizations for Asynchronous Online Discussion.](http://arxiv.org/abs/2308.13549) | 本研究提出了一种结合自动编码和教师输入的方法，用于生成用于异步在线讨论的ENA可视化。实验结果显示生成的ENA模型与人工编码员构建的模型没有统计学差异，这表明ENA可作为一种帮助教师评估异步在线讨论的有效可视化工具。 |
| [^75] | [Functional Graph Contrastive Learning of Hyperscanning EEG Reveals Emotional Contagion Evoked by Stereotype-Based Stressors.](http://arxiv.org/abs/2308.13546) | 本研究通过利用超扫描技术，引入功能性图对比学习方法探究基于刻板印象的压力引发的情绪传染。研究结果揭示了情绪传染与认知功能之间的复杂相互作用。 |
| [^76] | [Feature Extraction Using Deep Generative Models for Bangla Text Classification on a New Comprehensive Dataset.](http://arxiv.org/abs/2308.13545) | 本研究提出了使用深度生成模型在孟加拉文本分类中进行特征提取的方法，并收集、注释了一个全面的数据集。评估结果表明，对抗自编码器模型产生了最佳的特征空间。 |
| [^77] | [Impact of geolocation data on augmented reality usability: A comparative user test.](http://arxiv.org/abs/2308.13544) | 本研究通过一项比较用户测试评估了地理定位数据对增强现实可用性的影响，发现与控制组相比，使用RTK数据的实验组的地理定位数据平均准确性更低，可用性评分较低。 |
| [^78] | [Adversarial Collaborative Filtering for Free.](http://arxiv.org/abs/2308.13541) | 这篇论文介绍了一种面向免费的对抗性协同过滤方法，通过对抗性学习来规范用户/项目的表示，提高模型的泛化能力和鲁棒性，并提出了SharpCF方法来解决现有方法的缺点。 |
| [^79] | [STEM: Unleashing the Power of Embeddings for Multi-task Recommendation.](http://arxiv.org/abs/2308.13537) | 本文提出了一种称为STEM的新范例，用于解决多任务推荐中的负传递问题。与现有方法不同，STEM通过根据样本中正反馈数量的相对比例进行细分，深入研究样本的复杂性，以提高推荐系统的性能。 |
| [^80] | [Implicit ZCA Whitening Effects of Linear Autoencoders for Recommendation.](http://arxiv.org/abs/2308.13536) | 本文展示了线性自动编码器模型在推荐系统中隐式的ZCA白化作用，以及其对低维物品嵌入的有效性。 |
| [^81] | [Federated Linear Bandit Learning via Over-the-Air Computation.](http://arxiv.org/abs/2308.13298) | 本研究针对联邦线性赌博学习提出了一种通过无线计算的方案，以减少通信开销。通过在噪声衰落信道上进行的模拟信号传输，我们的方案在降低累积遗憾方面表现出竞争力。 |
| [^82] | [Heterogeneous Decentralized Machine Unlearning with Seed Model Distillation.](http://arxiv.org/abs/2308.13269) | 该论文介绍了一种名为HDUS的分布式遗忘框架，使用种子模型蒸馏构建可擦除的模型集成，适用于异构的设备端模型，具有卓越的性能。 |
| [^83] | [Bayesian low-rank adaptation for large language models.](http://arxiv.org/abs/2308.13111) | 本研究提出了一种名为Laplace-LoRA的贝叶斯方法，通过应用拉普拉斯近似来增强经过微调的大型语言模型的校准能力。 |
| [^84] | [CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement.](http://arxiv.org/abs/2308.12902) | 本研究提出了一种名为CDAN的卷积稠密注意力引导网络，用于低光图像增强。该网络结合了自编码器架构、卷积和稠密块、注意力机制和跳跃连接，通过专门的后处理阶段进一步改善色彩平衡和对比度。与现有方法相比，在低光图像增强方面取得了显著的进展，展示了在各种具有挑战性的场景中的稳健性。 |
| [^85] | [A Probabilistic Fluctuation based Membership Inference Attack for Generative Models.](http://arxiv.org/abs/2308.12143) | 本研究针对生成模型提出了一种概率波动评估成员推断攻击方法(PFAMI)，通过检测概率分布的波动性来推断模型中是否存在某条训练记录的成员身份。 |
| [^86] | [MKL-$L_{0/1}$-SVM.](http://arxiv.org/abs/2308.12016) | 本文提出了一种多核学习的支持向量机框架(MKL-$L_{0/1}$-SVM)，通过开发快速的ADMM求解器处理非凸非光滑的优化问题，并在实验中展示了与领先方法相当的性能。 |
| [^87] | [ProAgent: Building Proactive Cooperative AI with Large Language Models.](http://arxiv.org/abs/2308.11339) | ProAgent是一个利用大型语言模型构建的主动合作的AI框架，能够预测队友的决策并为自己制定增强计划，具有高度的模块化和可解释性。 |
| [^88] | [Enhancing Agent Communication and Learning through Action and Language.](http://arxiv.org/abs/2308.10842) | 通过行动和语言相结合的方式，我们引入了一种新型智能体，其能够同时作为教师和学习者，通过行动演示和语言指令增强了沟通效率，并探索了结合行动和语言沟通模式对学习结果的积极影响。 |
| [^89] | [Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting.](http://arxiv.org/abs/2308.10425) | 本研究提出了一种名为时空自适应嵌入的新组件，在普通的Transformer中实现了领先于其他方法的交通预测性能，通过捕捉交通时间序列中的时空关系和时间信息实现了优秀的结果。 |
| [^90] | [Wasserstein Geodesic Generator for Conditional Distributions.](http://arxiv.org/abs/2308.10145) | 通过Wasserstein几何生成器学习条件分布，生成给定特定标签的样本。使用最优输运理论提出的方法能学习观察域的条件分布和它们之间的最优输运映射。在人脸图像数据上的实验验证了该方法的有效性。 |
| [^91] | [MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models.](http://arxiv.org/abs/2308.09729) | 本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。 |
| [^92] | [Safety Filter Design for Neural Network Systems via Convex Optimization.](http://arxiv.org/abs/2308.08086) | 本文提出了一种通过凸优化为神经网络系统设计安全过滤器的方法，该过滤器能够捕获建模误差，并通过鲁棒线性模型预测控制来搜索可以保证约束满足的控制器。通过在非线性摆系统上的实验验证了该方法的有效性。 |
| [^93] | [Dyadic Reinforcement Learning.](http://arxiv.org/abs/2308.07843) | 该论文介绍了一个称为二元强化学习的在线算法，用于根据上下文因素和目标人与其照顾伴侣的过去反馈，个性化地提供干预措施。该算法是贝叶斯和层次的，并通过模拟展示了良好的实证效果。 |
| [^94] | [An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM.](http://arxiv.org/abs/2308.06828) | 本研究提出了一种集成Electra Transformer、GloVe和LSTM模型的创新问题分类方法，通过在TREC数据集上进行严格测试，证明了融合不同技术可以获得更优越的结果。 |
| [^95] | [BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning.](http://arxiv.org/abs/2308.04263) | BarlowRL通过将Barlow Twins和DER相结合，实现了数据效率强化学习，并在Atari 100k基准测试上优于其他算法。它通过信息扩散避免了维度折叠，使得RL算法能够利用均匀分布的状态表示，从而实现卓越的性能。 |
| [^96] | [PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning.](http://arxiv.org/abs/2308.03953) | 本研究提出了一种基于PMU测量的电力系统短期电压稳定性评估方法，利用深度迁移学习解决了拓扑变化、样本标注和小规模数据集处理等挑战，并且在实验中取得了较好的模型评估准确性。 |
| [^97] | [Symmetry-Preserving Program Representations for Learning Code Semantics.](http://arxiv.org/abs/2308.03312) | 本文提出了一种在代码中保持对称性的程序表示方法，通过引入循环层来提高在程序分析和建模中的效果。 |
| [^98] | [DLSIA: Deep Learning for Scientific Image Analysis.](http://arxiv.org/abs/2308.02559) | DLSIA是一种Python的机器学习库，为科学家提供了定制化卷积神经网络架构，加速科学图像分析研究，并促进跨学科合作。 |
| [^99] | [End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC.](http://arxiv.org/abs/2308.01674) | 本论文提出了一种用于经济非线性MPC的Koopman模型的端到端强化学习方法，旨在实现控制性能和计算需求之间的平衡。 |
| [^100] | [Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis.](http://arxiv.org/abs/2307.15424) | 本文综述了近期合成数据生成的深度生成模型发展，重点关注表格数据集。通过使用深度生成模型，可以有效地生成隐私敏感数据的合成数据，并解决数据归一化、隐私和评估等方面的挑战。 |
| [^101] | [External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback.](http://arxiv.org/abs/2307.12057) | 本文提出通过从外部存储库中选择性地集成知识来增强大型语言模型，提出了一种外部推理的新方法，例子是ChatPDF。 |
| [^102] | [A DPLL(T) Framework for Verifying Deep Neural Networks.](http://arxiv.org/abs/2307.10266) | 这项工作介绍了一个新的约束求解方法NeuralSAT，用于验证深度神经网络。Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art. |
| [^103] | [Handwritten and Printed Text Segmentation: A Signature Case Study.](http://arxiv.org/abs/2307.07887) | 本研究旨在解决手写和打印文本分割的挑战，并提出了一种新的方法来完整地恢复不同类别的文本，特别是在重叠部分提高分割性能。同时，还引入了一个新的数据集SignaTR6K，用于支持该任务。 |
| [^104] | [Reconstructing Spatiotemporal Data with C-VAEs.](http://arxiv.org/abs/2307.06243) | 本文通过使用C-VAE模型来生成平滑且逼真的时空演变表示，探索了深度学习技术在重建时空数据方面的潜力。 |
| [^105] | [Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection.](http://arxiv.org/abs/2307.04298) | 提出了一种基于预训练自编码器的简单而有效的数据压缩方法，用于边缘存储管理和道路异常检测，可以高效地存储高保真音频样本。 |
| [^106] | [inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data.](http://arxiv.org/abs/2307.03854) | inTformer是一种基于时间嵌入的关注机制Transformer，用于使用连接车辆数据预测路口事故可能性。与其他深度学习模型相比，inTformer具有处理长期依赖性、并行处理数据序列以及解决梯度消失问题的优势。 |
| [^107] | [On Formal Feature Attribution and Its Approximation.](http://arxiv.org/abs/2307.03380) | 这篇论文研究了解释性人工智能（XAI）中的形式特征归因方法及其近似方法。现有的特征选择和归因方法存在一些问题，而形式化的XAI方法虽然是一个有希望的解决方案，但仍存在一些限制。 |
| [^108] | [Near-Optimal Nonconvex-Strongly-Convex Bilevel Optimization with Fully First-Order Oracles.](http://arxiv.org/abs/2306.14853) | 本文针对双层优化问题中底层问题为强凸的情况，提出了一种更加高效的算法，可以以接近最优的速率收敛。这种算法避免了在实践中可能无法获得或代价昂贵的Hessian-向量乘积预言机的使用。 |
| [^109] | [Boosting Multitask Learning on Graphs through Higher-Order Task Affinities.](http://arxiv.org/abs/2306.14009) | 本文从多任务学习的角度重新审视在给定图上预测节点标签的问题，提出通过更高级任务相似性来加强多任务学习，并开发了一种算法来将任务分组以应对负迁移问题。 |
| [^110] | [Decentralized Multi-Agent Reinforcement Learning with Global State Prediction.](http://arxiv.org/abs/2306.12926) | 本文研究了分散式多智能体强化学习中的一个关键挑战：如何在没有全局信息的情况下有效训练机器人。我们提出了一种基于状态预测的方法，在不需要显式通信的情况下使机器人能够协调行动，实现更快更好的学习效果和任务执行性能。 |
| [^111] | [Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset.](http://arxiv.org/abs/2306.11167) | 这项研究探索了大型语言模型（LLMs）对创造性问题解决的能力，并发现大型语言模型容易被误导，出现固定效应和Einstellung范式。 |
| [^112] | [A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises.](http://arxiv.org/abs/2306.04802) | 本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。 |
| [^113] | [The feasibility of artificial consciousness through the lens of neuroscience.](http://arxiv.org/abs/2306.00915) | 从神经科学的角度来看，目前大型语言模型难以具备哺乳动物意识感知相关的丘脑皮层系统的关键特征，缺乏周围世界的具体嵌入式信息，且当前的人工智能无法做到存在的依赖于其行为，这意味着人工意识的可行性存在瓶颈。 |
| [^114] | [SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning.](http://arxiv.org/abs/2305.19442) | SimFBO和其ShroFBO变体提出了一个简单、灵活且通信高效的FBO框架，可以应用于元学习和超参数优化任务。 |
| [^115] | [Counterpart Fairness -- Addressing Systematic between-group Differences in Fairness Evaluation.](http://arxiv.org/abs/2305.18160) | 本论文提出了一种新的公平评估方法，通过比较不同群体中相似的个体来解决群体之间的系统差异。这种方法基于倾向得分，识别对等个体，避免了比较不同类型的个体，从而提高公平性评估的准确性和可靠性。 |
| [^116] | [An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation.](http://arxiv.org/abs/2305.14704) | 本文推导和评估了四种贝叶斯批次赌博算法，用于自适应确定流量分配，全面评估了这些算法的可信度、敏感性和后悔度，仿真结果表明这些基于批次的贝叶斯算法是有效的。 |
| [^117] | [A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation.](http://arxiv.org/abs/2305.11391) | 通过验证和验证的视角对大型语言模型的安全性和可信度进行调查，分类它们的已知漏洞，将其分为固有问题、有意攻击和意外错误。同时，考虑四种互补技术以提供LLM及其应用的安全和可信度保障。 |
| [^118] | [TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection.](http://arxiv.org/abs/2305.05105) | TDC'22是第一届面向ICDs低功耗微控制器的人工智能/机器学习（AI/ML）算法创新竞赛。本次竞赛的挑战是开发一种基于AI/ML的新型实时检测算法，对危及生命的室性心律失常进行检测。 |
| [^119] | [Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward.](http://arxiv.org/abs/2305.02527) | 本文提出了一种算法用于解决具有延迟、复合和部分匿名奖励反馈的无限时平均奖励马尔可夫决策过程(MDP)，并取得了较好的效果。 |
| [^120] | [Map-based Experience Replay: A Memory-Efficient Solution to Catastrophic Forgetting in Reinforcement Learning.](http://arxiv.org/abs/2305.02054) | 本文提出了一种基于地图的经验回放方法，通过将存储的转换组织成一种简洁的环境模型网络，以在减少内存大小的同时增加每个样本的相关性，从而有效解决强化学习中的遗忘问题。 |
| [^121] | [A noise-robust acoustic method for recognition of foraging activities of grazing cattle.](http://arxiv.org/abs/2304.14824) | 本研究提出了一种抗噪声的声学方法，能够分析与吃草和反刍相关的鉴定下颚运动事件的固定长度段，用于识别牛的觅食活动，并在环境和自然噪声方面具有鲁棒性。 |
| [^122] | [A Deep Learning algorithm to accelerate Algebraic Multigrid methods in Finite Element solvers of 3D elliptic PDEs.](http://arxiv.org/abs/2304.10832) | 该论文介绍了一种新的深度学习算法，通过解释线性系统的稀疏矩阵为黑白图像，利用池化操作将其转换为小的多通道图像，从而调整代数多重网格方法中的强门槛参数。该算法最小化了AMG方法在有限元求解器中的计算成本，并在解决三维椭圆偏微分方程时比现有最先进的AMG求解器更快。 |
| [^123] | [PGTask: Introducing the Task of Profile Generation from Dialogues.](http://arxiv.org/abs/2304.06634) | 对话系统的个性化需要个人资料信息，而从对话中提取/生成个人资料信息是一项基本需求。为此，我们提出了档案生成任务（PGTask）并提供了相关的数据集和基准，该任务使得研究者可以更好地了解档案生成任务的挑战和可能的解决方案。 |
| [^124] | [Safe Perception-Based Control under Stochastic Sensor Uncertainty using Conformal Prediction.](http://arxiv.org/abs/2304.00194) | 本文提出了一种利用置信度预测来应对传感器噪声及不确定性的感知控制框架，通过量化感知地图的不确定性并将其整合到控制设计中，计算有效的状态估计区域，从而实现连续时间系统的采样数据控制，确保系统的安全性和有效性。 |
| [^125] | [Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning.](http://arxiv.org/abs/2303.12091) | 本文提出了ANEDL框架，应用证据深度学习量化不同类型的不确定性，并设计了新颖的适应性负优化策略，有效应对在未标记数据集中包含内部值和异常值的开放式半监督学习。 |
| [^126] | [SIESTA: Efficient Online Continual Learning with Sleep.](http://arxiv.org/abs/2303.10725) | SIESTA是一种在线持续学习方法，通过使用无需回忆、无需反向传播和数据驱动的网络更新规则，在更少的时间和能源消耗下高效地更新深度神经网络(DNN)。该方法基于训练休眠/觉醒框架，可以应用于设备端学习。 |
| [^127] | [No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier.](http://arxiv.org/abs/2303.10058) | 本文提出一种解决合作学习中分类器偏差问题的方案，即在训练过程中使用合成的ETF分类器，使得所有客户端能够学习到统一的最优特征表示。 |
| [^128] | [Energy Management of Multi-mode Plug-in Hybrid Electric Vehicle using Multi-agent Deep Reinforcement Learning.](http://arxiv.org/abs/2303.09658) | 本论文提出了一种基于多智能体深度强化学习的多模式插电混合动力汽车能量管理的MIMO控制方法，通过握手策略和多目标函数优化全局控制，实验结果表明其在燃料消耗、SOC变化和功率限制方面优于传统方法。 |
| [^129] | [Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning.](http://arxiv.org/abs/2303.00396) | 本文提出了一种通过受限代理学习方法，可以有效地控制深度序数分类中的类布局。 |
| [^130] | [A semantic backdoor attack against Graph Convolutional Networks.](http://arxiv.org/abs/2302.14353) | 该论文研究了图卷积网络（GCNs）是否容易受到语义后门攻击，提出了一种针对GCNs的语义后门攻击方法（SBAG），通过在样本中的特定节点作为触发器，并注入隐藏的后门来攻击GCNs模型。 |
| [^131] | [Revolutionizing Genomics with Reinforcement Learning Techniques.](http://arxiv.org/abs/2302.13268) | 强化学习是一种革新的工具，可以在基因组学领域中解决自动数据分析和处理的问题。使用强化学习算法可以降低收集标记训练数据的成本，适用于基因组数据分析和解释。本调查重点关注在基因组研究领域中使用强化学习的应用，包括基因调控网络、基因组组装和序列比对。 |
| [^132] | [Inaccurate Label Distribution Learning.](http://arxiv.org/abs/2302.13000) | 本文研究了不准确的标签分布学习问题，提出了一种LDL模型，通过图形的局部几何结构辅助恢复理想的标签分布。LDL方法的先前研究假设训练实例的标签分布是准确的，而现实中的标签分布通常是不准确的，并受到标注错误的干扰。 |
| [^133] | [Counterfactual Reasoning for Bias Evaluation and Detection in a Fairness under Unawareness setting.](http://arxiv.org/abs/2302.08204) | 本研究提出了一种方法来揭示即使在丢弃敏感特征的情况下机器学习模型可能仍存在的潜在偏见，并通过利用反事实推理来检测黑盒预测器的偏见。 |
| [^134] | [Scalable Multi-Agent Reinforcement Learning with General Utilities.](http://arxiv.org/abs/2302.07938) | 本论文研究了具有通用效用的可扩展多智能体强化学习，并提出了一种基于分布式策略梯度算法的解决方案，通过利用网络结构的空间相关衰减性质实现了算法的收敛性。 |
| [^135] | [The Re-Label Method For Data-Centric Machine Learning.](http://arxiv.org/abs/2302.04391) | 本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。 |
| [^136] | [Protecting Language Generation Models via Invisible Watermarking.](http://arxiv.org/abs/2302.03162) | 本文提出了一种名为 GINSEW 的新方法，通过将秘密信号注入到每个目标标记的解码步骤的概率向量中，保护文本生成模型，有效识别出侵权行为，对模型的影响很小。 |
| [^137] | [Interpolation for Robust Learning: Data Augmentation on Wasserstein Geodesics.](http://arxiv.org/abs/2302.02092) | 本文提出一种通过插值训练数据分布来提升模型鲁棒性的方法。通过寻找连接不同类别子人口分布的测地线上的最坏情况Wasserstein barycenter来增加数据，并对模型进行正则化以获得更平滑的性能。我们的方法在多个数据集上进行了实验证实，并改进了基线的可证明鲁棒性和经验鲁棒性。该研究从Wasserstein测地线的角度探索了模型鲁棒性。 |
| [^138] | [Composing Task Knowledge with Modular Successor Feature Approximators.](http://arxiv.org/abs/2301.12305) | 本研究提出了一种新颖的神经网络架构，"模块化继任特征逼近器"（MSFA），通过让模块发现有用于预测的特征并学习自己的预测表示，实现了更好的泛化能力。 |
| [^139] | [Logical Message Passing Networks with One-hop Inference on Atomic Formulas.](http://arxiv.org/abs/2301.08859) | 本文提出了一个逻辑消息传递神经网络（LMPNN）框架，通过将集合操作分解成神经集合操作符来进行复杂查询回答，并将将原子公式的局部单跳推理连接到全局逻辑推理中。 |
| [^140] | [Revisiting mass-radius relationships for exoplanet populations: a machine learning insight.](http://arxiv.org/abs/2301.07143) | 这项研究利用机器学习方法分析了762个已确认的外行星和八个太阳系行星的数据集，发现巨大行星具有较低的密度，主要由氢和氦构成，而小行星更密集，主要由更重的元素构成。研究还揭示了质量、轨道周期和恒星金属丰度对外行星半径的预测能力的重要性。 |
| [^141] | [Deep Reinforcement Learning for Wind and Energy Storage Coordination in Wholesale Energy and Ancillary Service Markets.](http://arxiv.org/abs/2212.13368) | 本研究提出了一种基于深度强化学习的方法，以提高风能和能量存储系统的市场参与能力。该方法解决了协调复杂性和能源价格随机性的挑战，并实现了平衡现场风电削减和市场投标的目标。 |
| [^142] | [An active learning method for solving competitive multi-agent decision-making and control problems.](http://arxiv.org/abs/2212.12561) | 我们提出了一个基于主动学习的方法，用于解决竞争性多智能体决策和控制问题。通过重构私有策略和预测稳态行动配置文件，外部观察者可以成功进行预测和优化策略。 |
| [^143] | [Deep Unfolding-based Weighted Averaging for Federated Learning in Heterogeneous Environments.](http://arxiv.org/abs/2212.12191) | 本文提出了一种基于深度展开的加权平均方法，用于解决异质环境下的联邦学习问题。通过直接将环境的异质性纳入聚合权重的调整中，该方法能够获得更高的未知类平衡数据的测试准确性。 |
| [^144] | [Commitment with Signaling under Double-sided Information Asymmetry.](http://arxiv.org/abs/2212.11446) | 本论文研究了一种信号和双边信息不对称下的承诺问题，在贝叶斯斯塔克尔贝格博弈中，通过适当设计信号装置，领导者可以利用信息优势获得更高的预期效用。 |
| [^145] | [Invariant Lipschitz Bandits: A Side Observation Approach.](http://arxiv.org/abs/2212.07524) | 本文研究了不变Lipschitz赌徒设置，并提出了一种名为\texttt{UniformMesh-N}的算法。使用侧面观察的方法，证明了改进的遗憾上界。 |
| [^146] | [Federated Learning for 5G Base Station Traffic Forecasting.](http://arxiv.org/abs/2211.15220) | 本研究探讨了将联邦学习应用于原始基站LTE数据进行5G基站流量预测的效果。 |
| [^147] | [Examining Policy Entropy of Reinforcement Learning Agents for Personalization Tasks.](http://arxiv.org/abs/2211.11869) | 研究了个性化任务中强化学习智能体的策略熵，并发现策略优化智能体在训练过程中往往具有低熵策略，然而Q学习智能体对此影响较小，通常保持高熵策略。 |
| [^148] | [A Low Latency Adaptive Coding Spiking Framework for Deep Reinforcement Learning.](http://arxiv.org/abs/2211.11760) | 本文提出了一个低延迟自适应编码脉冲框架用于深度强化学习，在编码器灵活性、延迟和能量效率方面具有优异性能和广泛应用范围。 |
| [^149] | [Artificial Intelligence for Automatic Detection and Classification Disease on the X-Ray Images.](http://arxiv.org/abs/2211.08244) | 该论文研究了人工智能在X射线图像上的自动检测和疾病分类的应用。通过使用深度学习方法和RepVGG算法进行快速准确的肺部疾病检测，以实现早期疾病的诊断和预防进一步传播。 |
| [^150] | [QuadConv: Quadrature-Based Convolutions with Applications to Non-Uniform PDE Data Compression.](http://arxiv.org/abs/2211.05151) | 本论文提出了一种新的卷积层QuadConv，通过定积分对连续卷积进行近似。该方法适用于非均匀的基于网格的数据，并且在压缩偏微分方程（PDE）模拟数据时表现出与标准离散卷积相当的性能，甚至能够在非均匀数据上保持相同的准确性。QuadConv还优于其他非结构化卷积方法如图卷积。 |
| [^151] | [Learning Melanocytic Cell Masks from Adjacent Stained Tissue.](http://arxiv.org/abs/2211.00646) | 本文提出了一种从相邻染色组织学片中训练深度神经网络进行黑色素细胞分割的方法，实现了0.64的平均IOU，尽管存在不完美的标签。 |
| [^152] | [QNet: A Quantum-native Sequence Encoder Architecture.](http://arxiv.org/abs/2210.17262) | 本研究探讨了在近期量子计算机上进行机器学习在序列数据中的优势，通过在自然语言处理任务中进行实验，提出了一种全量子计算机推理的新型序列编码器模型QNet，以及一个量子-经典混合模型ResQNet。这些模型在经典的最先进模型上表现出了引人注目的性能，而且参数数量更少。 |
| [^153] | [TuneUp: A Simple Improved Training Strategy for Graph Neural Networks.](http://arxiv.org/abs/2210.14843) | TuneUp是一种简单的基于课程的训练策略，用于改进图神经网络在难以预测的尾节点上的泛化性能。 |
| [^154] | [Sufficient Invariant Learning for Distribution Shift.](http://arxiv.org/abs/2210.13533) | 本文研究了分布转移情况下的充分不变学习，观察到之前的工作只学习了部分不变特征，我们提出了学习充分不变特征的重要性，并指出在分布转移时，从训练集中学习的部分不变特征可能不适用于测试集，限制了性能提升。 |
| [^155] | [Differentiable Constrained Imitation Learning for Robot Motion Planning and Control.](http://arxiv.org/abs/2210.11796) | 本论文介绍了一种将可微约束模仿学习与硬约束的直接方法相结合的机器人运动规划和控制方法，该方法通过学习离线演示中的决策制定，为复杂的机器人应用提供了一种有希望的途径。 |
| [^156] | [Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem.](http://arxiv.org/abs/2210.11694) | 该论文提出了一种使用多视角一致的对比学习来解决数学应用问题，通过同时考虑自上而下和自下而上的推理视角，以及多种等价的方程形式，实现了更完整的语义到方程的映射。实验证明该方法明显优于现有的基线。 |
| [^157] | [Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations.](http://arxiv.org/abs/2210.07237) | 这项研究引入了一种新的学习MD模拟基准套件，用于评估机器学习力场模型，并证明了目前常用的力精度测试与模拟结果不一致的情况。 |
| [^158] | [Benign Autoencoders.](http://arxiv.org/abs/2210.00637) | 本文正式化了用于生成式人工智能中编码器-解码器对的最佳选择问题并提出了良性自编码器（BAE），BAE能够将数据投射到最优的流型上，实现了数据压缩和更加稳定的梯度下降。 |
| [^159] | [Generalization in Deep Reinforcement Learning for Robotic Navigation by Reward Shaping.](http://arxiv.org/abs/2209.14271) | 本文研究了深度强化学习算法在机器人导航中的应用，提出了一种新的奖励函数，结合训练阶段获取的地图信息，使机器人能够在未知环境中避免局部极小值，并使用SAC算法进行训练，相比其他算法更有效。 |
| [^160] | [On the Robustness of Random Forest Against Untargeted Data Poisoning: An Ensemble-Based Approach.](http://arxiv.org/abs/2209.14013) | 本文提出了一种基于集合的方法来提高随机森林对非有目标数据污染的鲁棒性 |
| [^161] | [Local Context-Aware Active Domain Adaptation.](http://arxiv.org/abs/2208.12856) | 本文提出了一种局部上下文感知的主动域自适应框架 LADA，通过引入局部不一致性标准选择信息量更丰富的目标样本，并以类平衡的方式逐步增加标注的目标数据与自信的邻居。实验证实 LADA 方法在各种基准测试中明显优于最近的 ADA 方法。 |
| [^162] | [Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning.](http://arxiv.org/abs/2208.06193) | 本文提出了一种将策略表示为扩散模型的方法，用于离线强化学习。我们引入了Diffusion Q-learning（Diffusion-QL），利用条件扩散模型表示策略，并通过最大化动作值来寻求接近行为策略的最优动作。 |
| [^163] | [A Medical Image Fusion Method based on MDLatLRRv2.](http://arxiv.org/abs/2206.15179) | 该论文提出了一种基于MDLatLRRv2的医学图像融合方法，通过改进多级分解方法并充分利用LatLRR提取的各种图像特征，实现了在客观和主观评估中的最先进融合性能。 |
| [^164] | [TE2Rules: Explaining Tree Ensembles using Rules.](http://arxiv.org/abs/2206.14359) | 本文介绍了一种将二元分类任务中的树集合模型转换为可解释规则列表的方法，该方法可以有效解释模型对于少数类别的预测。实验证明，TE2Rules方法生成的规则列表准确性较高，并且运行时间与其他基线方法相当。 |
| [^165] | [Dual Representation Learning for Out-of-Distribution Detection.](http://arxiv.org/abs/2206.09387) | 本论文提出了双表示学习（DRL）方法，通过同时探索与标签强相关和与标签弱相关的信息，来区分内部和非分布样本，进而提高非分布检测的准确性。 |
| [^166] | [Out-of-distribution Detection by Cross-class Vicinity Distribution of In-distribution Data.](http://arxiv.org/abs/2206.09385) | 本研究提出了一种通过在训练数据的邻近分布中选择非分布样本来解决深度神经网络对分布外样本的识别问题的方法。引入了交叉类接近分布的概念，通过假设混合多个分布内样本生成的分布外样本与其组成部分具有不同的类别，提高了模型的区分能力。 |
| [^167] | [Boosting DNN Cold Inference on Edge Devices.](http://arxiv.org/abs/2206.07446) | NNV12是第一个在边缘设备上优化冷启动推理能力的推理引擎，通过选择合适的内核、缓存权重转换后的结果以及在异构处理器上进行流水线执行来提升性能，实验结果显示性能提升最高达到15倍。 |
| [^168] | [ReCo: A Dataset for Residential Community Layout Planning.](http://arxiv.org/abs/2206.04678) | ReCo是一个用于住宅社区布局规划的数据集，用于解决基于数据驱动的方法在该领域面临的数据不足问题。 |
| [^169] | [Diffusion-GAN: Training GANs with Diffusion.](http://arxiv.org/abs/2206.02262) | Diffusion-GAN提出了一种新颖的GAN框架，通过使用前向扩散链生成高斯混合分布的实例噪声，在训练中解决了GAN稳定性的问题。 |
| [^170] | [ES-GNN: Generalizing Graph Neural Networks Beyond Homophily with Edge Splitting.](http://arxiv.org/abs/2205.13700) | ES-GNN是一种创新的图神经网络框架，通过边分割将图分割为两个子图，以自适应地区分对学习任务相关或不相关的图边。这种方法能够提高GNN在异质图上的普适性和鲁棒性。 |
| [^171] | [Secure & Private Federated Neuroimaging.](http://arxiv.org/abs/2205.05249) | 本论文介绍了一种安全和私密的联合神经影像学方法，通过联邦学习实现了在保护数据隐私的情况下对分布式数据进行神经网络模型的训练和分析。 |
| [^172] | [Enhancing Core Image Classification Using Generative Adversarial Networks (GANs).](http://arxiv.org/abs/2204.14224) | 本研究提出了一种使用生成对抗网络(GANs)增强核心图像分类的创新方法，通过应用先进的模型来检测和分割岩心图像中的核心和洞，并利用强大的GANs技术填补岩心图像中的洞。这项研究将为油气勘探行业带来重大转变。 |
| [^173] | [Simulate Time-integrated Coarse-grained Molecular Dynamics with Multi-Scale Graph Networks.](http://arxiv.org/abs/2204.10348) | 本论文提出了一种学习多尺度图神经网络的方法，通过直接模拟粗粒化分子动力学来加速模拟过程，使用非常大的时间步长和新颖的细化模块来提高稳定性。在两个复杂系统中进行的实验表明该方法在不同化学组成的系统中可以准确恢复结构和动力学性质。 |
| [^174] | [Continuous-Time User Preference Modelling for Temporal Sets Prediction.](http://arxiv.org/abs/2204.05490) | 这篇论文提出了一种连续时间用户偏好建模框架，用于时间序列预测。该框架通过显式建模用户的演化偏好，维护存储器来存储所有用户和元素的状态，并利用通用序列按时间顺序进行学习。 |
| [^175] | [One-shot Ultra-high-Resolution Generative Adversarial Network That Synthesizes 16K Images On A Single GPU.](http://arxiv.org/abs/2202.13799) | 我们提出了一种名为OUR-GAN的一次性超高分辨率生成对抗网络，能够从单个训练图像生成非重复的16K图像。它通过逐步增加细节和超分辨率来提高图像质量，并能合成具有细节和一致性的大型形状。此外，它还通过垂直位移来提高视觉一致性和多样性。 |
| [^176] | [Adaptive Experimentation in the Presence of Exogenous Nonstationary Variation.](http://arxiv.org/abs/2202.09036) | 本论文研究了在外生非平稳变化存在下的自适应实验。提出了无偏汤普森抽样(DTS)算法来解决多臂老虎机算法在面对非平稳外生因素时的脆弱性，DTS算法通过控制背景信息预测一个臂的人口层级表现，并提供了实验内和实验后的遗憾界限，显示了其对外生变异的弹性。 |
| [^177] | [Continuous-time stochastic gradient descent for optimizing over the stationary distribution of stochastic differential equations.](http://arxiv.org/abs/2202.06637) | 我们提出了一种连续时间随机梯度下降算法用于优化随机微分方程模型的平稳分布。算法通过估计平稳分布的梯度，并使用正向传播进行连续更新参数，实现收敛至最陡下降方向。我们严格证明了在线正向传播算法在线性模型上的收敛性，并在非线性示例上进行了数值验证。 |
| [^178] | [Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound.](http://arxiv.org/abs/2202.06599) | 本文提出了一个利用多图谱和深度学习技术进行胚胎分割和空间对齐的自动化框架，从而实现对人体胚胎在第一季度3D超声图像中的生长和发育进行监测。该框架通过学习将胚胎注册到图谱中，使用了多个孕龄范围内的超声图像，经过分割和空间对齐后得到标准方向上的胚胎分割结果。 |
| [^179] | [NNP/MM: Accelerating molecular dynamics simulations with machine learning potentials and molecular mechanic.](http://arxiv.org/abs/2201.08110) | 该论文介绍了一种优化的混合方法（NNP/MM），结合了神经网络势和分子力学，以加速生物分子模拟。通过对蛋白质-配体复合物进行分子动力学模拟和对配体进行元动力学模拟，研究人员展示了该方法能够提高模拟速度并实现最长的模拟时间。 |
| [^180] | [Incomplete Multi-View Weak-Label Learning.](http://arxiv.org/abs/2201.01079) | 本文提出了一种解决多视图多标签学习中存在的不完全特征和标签、噪声视图和标签不平衡问题的新方法。该方法通过将不完全的视图和弱标签嵌入到低维子空间中，并通过自适应的权重和嵌入矩阵差异来减少冗余。实验证实了该方法的有效性。 |
| [^181] | [Stochastic coordinate transformations with applications to robust machine learning.](http://arxiv.org/abs/2110.01729) | 本文提出了一种利用随机坐标变换进行异常检测的新方法，该方法通过层级张量积展开来逼近随机过程，并通过训练机器学习分类器对投影系数进行检测。在基准数据集上的实验表明，该方法胜过现有的最先进方法。 |
| [^182] | [Revealing the Distributional Vulnerability of Discriminators by Implicit Generators.](http://arxiv.org/abs/2108.09976) | 通过隐式生成器的微调，我们揭示了辨别器在分布上的脆弱性。我们的方法通过生成和惩罚特定的分布外样本来提高辨别器在分布内和分布外样本上的区分能力。 |
| [^183] | [Meta-Calibration: Learning of Model Calibration Using Differentiable Expected Calibration Error.](http://arxiv.org/abs/2106.09613) | 本论文提出了元校准框架，通过引入可微的期望校准误差代理指标和元学习框架，实现了对模型校准质量的直接优化。实验证明，该方法能够达到与现有校准方法相竞争的性能表现。该框架为进一步解决校准问题提供了新的思路和工具。 |
| [^184] | [The Confluence of Networks, Games and Learning.](http://arxiv.org/abs/2105.08158) | 这篇论文讨论了网络、游戏和学习的融合，为理解网络上多智能体决策制定提供了理论基础，并提供了选择性的博弈理论学习算法概述和在现代网络系统中的应用。 |
| [^185] | [Functional optimal transport: map estimation and domain adaptation for functional data.](http://arxiv.org/abs/2102.03895) | 这篇论文介绍了在函数空间上解决分布最优输运问题的方法，通过使用Hilbert-Schmidt算子将函数域之间的随机映射进行表示。这种方法对于处理函数数据的机器学习任务非常有用。 |
| [^186] | [A probabilistic Taylor expansion with Gaussian processes.](http://arxiv.org/abs/2102.00877) | 本文研究了一种特殊的高斯过程回归方法，通过导数评估数据和先验协方差核的选择，实现了对任意阶Taylor展开的截断复制，并讨论了相关参数的最大似然估计方法。 |
| [^187] | [On the Estimation of Derivatives Using Plug-in Kernel Ridge Regression Estimators.](http://arxiv.org/abs/2006.01350) | 本文提出了一种插值核岭回归（KRR）估计器，可广泛适用于非参数回归中的多维支持和任意混合偏导数，并且具有较强的误差界。 |
| [^188] | [Learning to Generate Time Series Conditioned Graphs with Generative Adversarial Nets.](http://arxiv.org/abs/2003.01436) | 本文提出了一种新颖的时间序列条件图生成方法(TSGG-GAN)，通过结合丰富的节点级上下文结构来推断时间序列之间的关系图。 |
| [^189] | [How to choose the most appropriate centrality measure? A decision tree approach.](http://arxiv.org/abs/2003.01052) | 这篇论文提出了一种利用决策树方法选择合适中心度度量方法的缩减方法。通过构建决策树调查，结合专家偏好，能够在较小的图形数量下快速筛选出最合适的中心度度量方法。 |
| [^190] | [Pedestrian Attribute Recognition: A Survey.](http://arxiv.org/abs/1901.07474) | 本文回顾了行人属性识别的现有作品，介绍了行人属性识别的背景、基准、多任务学习和多标签学习的概念，以及流行的网络架构和解决方案。 |
| [^191] | [Generalization in Deep Learning.](http://arxiv.org/abs/1710.05468) | 本文从理论上解释了为什么以及如何深度学习能够在容量大、复杂性高、可能存在算法不稳定性、非鲁棒性和尖锐极小值的情况下实现良好的泛化，提出了一些新的开放问题，并讨论了研究结果的局限性。 |

# 详细

[^1]: JL-引理推导的用于判别式字典学习的最优投影

    JL-lemma derived Optimal Projections for Discriminative Dictionary Learning. (arXiv:2308.13991v1 [cs.CV])

    [http://arxiv.org/abs/2308.13991](http://arxiv.org/abs/2308.13991)

    本文提出了一种名为JLSPCADL的新方法，通过利用Johnson-Lindenstrauss引理选择一个维数的转换空间，从而实现判别式字典学习并提供更好的分类结果。

    

    为了克服在具有大量类别的大维度数据分类中的困难，我们提出了一种名为JLSPCADL的新方法。本文利用Johnson-Lindenstrauss(JL)引理，在一个转换空间中选择一个维数，可以在其中学习用于信号分类的判别式字典。与通常使用JL进行降维的随机投影不同，我们使用从Modified Supervised PC Analysis (M-SPCA)推导得出的投影转换矩阵，其维数遵循JL的规定。JLSPCADL提供了一种启发式方法来推断适当的失真水平和相应的字典原子的适当描述长度(SDL)，以推导出一个最优的特征空间，从而提供更好的分类的字典原子的SDL。与最先进的基于降维的字典学习方法不同，从M-SPCA中一步得出的投影转换矩阵提供了最大的特征-标签一致性。

    To overcome difficulties in classifying large dimensionality data with a large number of classes, we propose a novel approach called JLSPCADL. This paper uses the Johnson-Lindenstrauss (JL) Lemma to select the dimensionality of a transformed space in which a discriminative dictionary can be learned for signal classification. Rather than reducing dimensionality via random projections, as is often done with JL, we use a projection transformation matrix derived from Modified Supervised PC Analysis (M-SPCA) with the JL-prescribed dimension.  JLSPCADL provides a heuristic to deduce suitable distortion levels and the corresponding Suitable Description Length (SDL) of dictionary atoms to derive an optimal feature space and thus the SDL of dictionary atoms for better classification. Unlike state-of-the-art dimensionality reduction-based dictionary learning methods, a projection transformation matrix derived in a single step from M-SPCA provides maximum feature-label consistency of the transfor
    
[^2]: 重新审视多任务学习中的标量化：一个理论的视角

    Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective. (arXiv:2308.13985v1 [cs.LG])

    [http://arxiv.org/abs/2308.13985](http://arxiv.org/abs/2308.13985)

    本论文重新审视了多任务学习中的标量化方法，并从理论的角度探讨了标量化是否能够充分探索帕累托前沿。结果显示，与最近的研究声称的经验优势相反，标量化本质上无法进行全面探索，特别是对于那些平衡了paren

    

    线性标量化，即通过加权总和来组合所有损失函数，自从多任务学习（MTL）的创立以来一直是文献中的默认选择。近年来，越来越多的人对开发专门的多任务优化器（SMTOs）来处理MTL作为多目标优化问题产生了兴趣。然而，目前还不清楚SMTOs是否比标量化有根本上的优势。实际上，社区中存在对比这两种算法的激烈讨论，主要是从经验角度出发。为了回答上述问题，本文从理论的角度重新审视了标量化。我们专注于线性MTL模型，并研究标量化是否能够充分探索帕累托前沿。我们的研究发现，与那些声称标量化具有经验优势的最近工作相反，标量化本质上无法进行全面探索，特别是对于那些平衡了paren

    Linear scalarization, i.e., combining all loss functions by a weighted sum, has been the default choice in the literature of multi-task learning (MTL) since its inception. In recent years, there is a surge of interest in developing Specialized Multi-Task Optimizers (SMTOs) that treat MTL as a multi-objective optimization problem. However, it remains open whether there is a fundamental advantage of SMTOs over scalarization. In fact, heated debates exist in the community comparing these two types of algorithms, mostly from an empirical perspective. To approach the above question, in this paper, we revisit scalarization from a theoretical perspective. We focus on linear MTL models and study whether scalarization is capable of fully exploring the Pareto front. Our findings reveal that, in contrast to recent works that claimed empirical advantages of scalarization, scalarization is inherently incapable of full exploration, especially for those Pareto optimal solutions that strike the balanc
    
[^3]: 通过机器学习进行山区天气预报的插值研究

    Interpolation of mountain weather forecasts by machine learning. (arXiv:2308.13983v1 [physics.ao-ph])

    [http://arxiv.org/abs/2308.13983](http://arxiv.org/abs/2308.13983)

    本研究提出了一种通过机器学习来插值山区天气预报的方法，通过利用当前观测数据和周围平原的预报数据，解决了在复杂地形中数值模拟精度降低的问题，并研究了在降水预测中使用二元交叉熵的方法。

    

    最近基于物理模型的数值模拟方法的进展提高了天气预报的准确性。然而，在复杂地形如山地地区，由于数值模拟中使用了几公里平方的网格，精度会降低。虽然统计机器学习也取得了显著进展，但直接应用难以利用物理知识。本文提出了一种方法，利用当前观测数据和周围平原的预报数据，使用机器学习来“插值”未来山区的天气。通常，天气预测依赖于数值模拟，因此这种方法可以被视为间接融合数值模拟和机器学习的混合方法。还研究了在降水预测中使用二元交叉熵的方法。

    Recent advancements in numerical simulation methods based on physical models have enhanced the accuracy of weather forecasts. However, the precision diminishes in complex terrains like mountainous regions due to the several kilometers square grid used in numerical simulations. While statistical machine learning has also significantly advanced, its direct application is difficult to utilize physics knowledge. This paper proposes a method that employs machine learning to ``interpolate'' future weather in mountainous regions using current observed data and forecast data from surrounding plains. Generally, weather prediction relies on numerical simulations, so this approach can be considered a hybrid method that indirectly merges numerical simulation and machine learning. The use of binary cross-entropy in precipitation prediction is also examined.
    
[^4]: 通用图形连续学习

    Universal Graph Continual Learning. (arXiv:2308.13982v1 [cs.LG])

    [http://arxiv.org/abs/2308.13982](http://arxiv.org/abs/2308.13982)

    本研究解决了图形学习中的灾难性遗忘问题，通过维持图形间的结构一致性和一种新的方法，实现了通用图形连续学习，并在真实世界的图形数据集上取得了显著的改进。

    

    我们解决了图形学习中的灾难性遗忘问题，这是因为传入的数据从一个图形分布转变为另一个图形分布。以往的研究主要解决了图形连续学习的一种情况，比如增量节点分类，而我们专注于一种通用方法，其中每个任务中的数据点可以是一个节点或一个图形，并且任务从节点到图形分类不同。我们提出了一种新颖的方法，使得图形神经网络在这种通用设置中表现出色。我们的方法通过维持图形间的局部和全局结构一致性来保持对过去任务的知识，并在真实世界的图形数据集中将其与各种连续学习基线进行对比，在平均性能和遗忘性上取得了显着的改善。

    We address catastrophic forgetting issues in graph learning as incoming data transits from one to another graph distribution. Whereas prior studies primarily tackle one setting of graph continual learning such as incremental node classification, we focus on a universal approach wherein each data point in a task can be a node or a graph, and the task varies from node to graph classification. We propose a novel method that enables graph neural networks to excel in this universal setting. Our approach perseveres knowledge about past tasks through a rehearsal mechanism that maintains local and global structure consistency across the graphs. We benchmark our method against various continual learning baselines in real-world graph datasets and achieve significant improvement in average performance and forgetting across tasks.
    
[^5]: 理解基于QUBO的哈密顿函数在图上的组合优化中的使用：以最大切割问题为例讨论

    Understanding the Usage of QUBO-based Hamiltonian Function in Combinatorial Optimization over Graphs: A Discussion Using Max Cut (MC) Problem. (arXiv:2308.13978v1 [cs.AI])

    [http://arxiv.org/abs/2308.13978](http://arxiv.org/abs/2308.13978)

    研究探讨了在图上基于QUBO公式的最大切割问题中，如何使用基于强化学习范式和哈密顿函数来解决组合优化问题。通过使用图神经网络作为信息传递架构，并通过三种不同的公式形式进行实验，发现...

    

    二次无约束二进制优化（QUBO）是一种广义技术，用于将各种NP困难组合优化问题建模为二进制变量的形式。哈密顿函数经常用于形成QUBO问题，其中它在优化的上下文中被用作目标函数。在本研究中，我们研究了如何使用基于强化学习（RL）范式和哈密顿函数解决QUBO公式中的图上组合优化问题。我们使用图神经网络（GNN）作为信息传递架构在节点之间传递信息。我们主要研究了三种公式，Monty-Carlo Tree Search with GNN-based RL（MCTS-GNN）、DQN with GNN-based RL和带有注意力的通用GNN（GRL）。我们的研究结果表明，...

    Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to model various NP-hard combinatorial optimization problems in the form of binary variables. The Hamiltonian function is often used to formulate QUBO problems where it is used as the objective function in the context of optimization. In this study, we investigate how reinforcement learning-based (RL) paradigms with the presence of the Hamiltonian function can address combinatorial optimization problems over graphs in QUBO formulations. We use Graph Neural Network (GNN) as the message-passing architecture to convey the information among the nodes. We have centered our discussion on QUBO formulated Max-Cut problem but the intuitions can be extended to any QUBO supported canonical NP-Hard combinatorial optimization problems. We mainly investigate three formulations, Monty-Carlo Tree Search with GNN-based RL (MCTS-GNN), DQN with GNN-based RL, and a generic GNN with attention-based RL (GRL). Our findings state that i
    
[^6]: 通过跨模型一致性进行标签去噪

    Label Denoising through Cross-Model Agreement. (arXiv:2308.13976v1 [cs.LG])

    [http://arxiv.org/abs/2308.13976](http://arxiv.org/abs/2308.13976)

    本文提出了一种通过跨模型一致性进行标签去噪的方法。通过观察发现，不同模型在干净示例上的预测相对相似，而在有噪声示例上的预测在不同模型之间变化更大。在这种观察的启发下，我们提出了使用跨模型一致性进行去噪的方法（DeCA），旨在最小化两个机器学习模型参数化的真实标签分布之间的KL散度，同时最大化数据观测的似然。

    

    在现实世界的机器学习应用中，从有噪声的标签学习是非常常见的。记忆这些有噪声的标签可能会影响模型的学习，从而导致次优的性能。在这项工作中，我们提出了一种新颖的框架，用于从有噪声标签中学习鲁棒的机器学习模型。通过实证研究，我们发现不同模型在干净示例上的预测相对相似，而在有噪声示例上的预测在不同模型之间变化更大。受到这一观察的启发，我们提出了使用跨模型一致性进行去噪（DeCA）的方法，该方法旨在最小化由两个机器学习模型参数化的真实标签分布之间的KL散度，同时最大化数据观测的似然。我们将提出的DeCA方法应用于二进制标签情景和多标签情景。对于二进制标签情景，我们选择隐式反馈推荐作为下游任务，并进行了四种最先进方法的实验。

    Learning from corrupted labels is very common in real-world machine-learning applications. Memorizing such noisy labels could affect the learning of the model, leading to sub-optimal performances. In this work, we propose a novel framework to learn robust machine-learning models from noisy labels. Through an empirical study, we find that different models make relatively similar predictions on clean examples, while the predictions on noisy examples vary much more across different models. Motivated by this observation, we propose \em denoising with cross-model agreement \em (DeCA) which aims to minimize the KL-divergence between the true label distributions parameterized by two machine learning models while maximizing the likelihood of data observation. We employ the proposed DeCA on both the binary label scenario and the multiple label scenario. For the binary label scenario, we select implicit feedback recommendation as the downstream task and conduct experiments with four state-of-the
    
[^7]: FAM：快速自适应元学习

    FAM: fast adaptive meta-learning. (arXiv:2308.13970v1 [cs.LG])

    [http://arxiv.org/abs/2308.13970](http://arxiv.org/abs/2308.13970)

    本论文提出了一个快速自适应联邦元学习（FAM）框架，可以协作学习一个全局模型，并在个别客户端上进行个性化。这解决了数据分布发散和隐私限制的问题，并且适用于需要在不同客户端之间进行个性化的领域转变。

    

    在这项工作中，我们提出了一个快速自适应联邦元学习（FAM）框架，用于协作学习一个单一全局模型，然后可以在个别客户端上个性化。联邦学习使多个客户端能够协作训练模型而不共享数据。参与联邦学习的客户端由于数据不足或数据多样性导致学习受到影响。然而，当数据分布发散时，学习会受到困扰。有必要学习一个可以使用客户端特定信息进行自适应的全局模型，并在客户端上创建个性化模型。MRI数据存在这个问题，第一，由于数据采集挑战，在某个地点的本地数据足以训练准确的模型，第二，由于隐私问题有数据共享限制，第三，由于客户端站点之间的领域转变，需要对学习的共享全局模型进行个性化。

    In this work, we propose a fast adaptive federated meta-learning (FAM) framework for collaboratively learning a single global model, which can then be personalized locally on individual clients. Federated learning enables multiple clients to collaborate to train a model without sharing data. Clients with insufficient data or data diversity participate in federated learning to learn a model with superior performance. Nonetheless, learning suffers when data distributions diverge. There is a need to learn a global model that can be adapted using client's specific information to create personalised models on clients is required. MRI data suffers from this problem, wherein, one, due to data acquisition challenges, local data at a site is sufficient for training an accurate model and two, there is a restriction of data sharing due to privacy concerns and three, there is a need for personalization of a learnt shared global model on account of domain shift across client sites. The global model
    
[^8]: 注重注意力：将人眼追踪集成到视觉Transformer模型中

    Fixating on Attention: Integrating Human Eye Tracking into Vision Transformers. (arXiv:2308.13969v1 [cs.CV])

    [http://arxiv.org/abs/2308.13969](http://arxiv.org/abs/2308.13969)

    本研究展示了如何将人眼追踪集成到视觉Transformer模型中，提高在多种驾驶情况和数据集上的准确性。

    

    现代基于Transformer的计算机视觉模型在多种视觉任务上表现出超越人类的能力。然而，一些关键任务，如医学图像解释和自动驾驶，仍然需要依赖人类判断。本研究展示了如何将人类视觉输入，特别是通过眼动仪收集到的注视点，集成到Transformer模型中，以提高在多种驾驶情况和数据集上的准确性。首先，我们在人类实验对象和Vision Transformer模型中观察到，注视区域在左右驾驶决策中的重要性。通过比较人类注视图和ViT注意力权重之间的相似性，我们揭示了单个头部和层之间的重叠动态。通过利用这种重叠动态，我们实现了对模型的修剪而不损失准确性。然后，我们将驾驶场景信息与注视数据相结合，采用“联合空间-注视”（JSF）的注意力设置。最后

    Modern transformer-based models designed for computer vision have outperformed humans across a spectrum of visual tasks. However, critical tasks, such as medical image interpretation or autonomous driving, still require reliance on human judgments. This work demonstrates how human visual input, specifically fixations collected from an eye-tracking device, can be integrated into transformer models to improve accuracy across multiple driving situations and datasets. First, we establish the significance of fixation regions in left-right driving decisions, as observed in both human subjects and a Vision Transformer (ViT). By comparing the similarity between human fixation maps and ViT attention weights, we reveal the dynamics of overlap across individual heads and layers. This overlap is exploited for model pruning without compromising accuracy. Thereafter, we incorporate information from the driving scene with fixation data, employing a "joint space-fixation" (JSF) attention setup. Lastly
    
[^9]: 带有双重注意力网络的多元时间序列分类

    Multivariate time series classification with dual attention network. (arXiv:2308.13968v1 [cs.LG])

    [http://arxiv.org/abs/2308.13968](http://arxiv.org/abs/2308.13968)

    通过引入双重注意力机制，该论文提出了一种全新的网络（DA-Net），用于多元时间序列分类。DA-Net能够同时提取局部和全局特征，从而识别关键的局部序列片段并建立全局长期依赖关系。

    

    在机器学习中，多元时间序列分类是一个越来越相关的领域。目前的技术集中于识别局部重要的序列片段或者建立全局长期依赖关系。然而，它们经常忽视了来自全局和局部特征的合并数据。本研究利用双重注意机制，探索了一种新的网络（DA-Net）用于多元时间序列分类，以提取局部和全局特征。DA-Net包括两个不同的层，即Squeeze-Excitation Window Attention（SEWA）层和Sparse Self-Attention within Windows（SSAW）层。DA-Net能够基于这两个扩展层提取必要的局部序列片段，建立全局长期依赖关系。

    One of the topics in machine learning that is becoming more and more relevant is multivariate time series classification. Current techniques concentrate on identifying the local important sequence segments or establishing the global long-range dependencies. They frequently disregard the merged data from both global and local features, though. Using dual attention, we explore a novel network (DA-Net) in this research to extract local and global features for multivariate time series classification. The two distinct layers that make up DA-Net are the Squeeze-Excitation Window Attention (SEWA) layer and the Sparse Self-Attention within Windows (SSAW) layer. DA- Net can mine essential local sequence fragments that are necessary for establishing global long-range dependencies based on the two expanded layers.
    
[^10]: 稀疏模型用于机器学习

    Sparse Models for Machine Learning. (arXiv:2308.13960v1 [cs.LG])

    [http://arxiv.org/abs/2308.13960](http://arxiv.org/abs/2308.13960)

    稀疏模型广泛应用于统计学和机器学习领域，可以用于回归、分类、图模型选择、稀疏估计和维度降低等任务，并在发现预测模式方面具有重要作用。

    

    稀疏建模是捕捉简洁原理的明显表现，稀疏模型广泛应用于统计学、物理学、信息科学、神经科学、计算数学等领域。在统计学中，稀疏建模的许多应用包括回归、分类任务、图模型选择、稀疏M-估计和稀疏维度降低。它在许多统计学和机器学习领域特别有效，其中主要目标是从数据中发现预测模式，以增强我们对底层物理、生物和其他自然过程的理解和控制，而不仅仅是构建准确的结果预测模型。常见例子包括在生物过程中选择生物标志物，在基于fMRI数据的大脑状态和过程的预测中找到相关的脑活动位置，以及识别最能解释端到端性能的网络瓶颈。

    The sparse modeling is an evident manifestation capturing the parsimony principle just described, and sparse models are widespread in statistics, physics, information sciences, neuroscience, computational mathematics, and so on. In statistics the many applications of sparse modeling span regression, classification tasks, graphical model selection, sparse M-estimators and sparse dimensionality reduction. It is also particularly effective in many statistical and machine learning areas where the primary goal is to discover predictive patterns from data which would enhance our understanding and control of underlying physical, biological, and other natural processes, beyond just building accurate outcome black-box predictors. Common examples include selecting biomarkers in biological procedures, finding relevant brain activity locations which are predictive about brain states and processes based on fMRI data, and identifying network bottlenecks best explaining end-to-end performance. Moreov
    
[^11]: 可微分权重掩码用于领域迁移

    Differentiable Weight Masks for Domain Transfer. (arXiv:2308.13957v1 [cs.CV])

    [http://arxiv.org/abs/2308.13957](http://arxiv.org/abs/2308.13957)

    本论文通过将模块化权重和领域迁移相结合，研究了三种权重掩码方法，并分析它们在保持源任务知识的同时允许高效微调目标任务的能力。

    

    深度学习模型在计算机视觉领域的一个主要缺点是它们无法以模块化的方式保留多个信息源。例如，给定一个在源任务上训练过的网络，我们希望在保持其在源任务上的性能的同时，将其重新训练到一个相似但不同的目标任务上。同时，研究人员已经广泛研究了网络权重的模块化，以定位和确定对于触发给定任务的性能的权重集合。一些工作研究了通过学习和分析权重掩码引入的网络权重的模块化。在这项工作中，我们将这些领域结合起来，研究了三种权重掩码方法，并分析它们在缓解源任务的“遗忘”同时允许在目标任务上进行高效微调的能力。我们发现不同的掩码技术在保留源任务知识方面存在权衡。

    One of the major drawbacks of deep learning models for computer vision has been their inability to retain multiple sources of information in a modular fashion. For instance, given a network that has been trained on a source task, we would like to re-train this network on a similar, yet different, target task while maintaining its performance on the source task. Simultaneously, researchers have extensively studied modularization of network weights to localize and identify the set of weights culpable for eliciting the observed performance on a given task. One set of works studies the modularization induced in the weights of a neural network by learning and analysing weight masks. In this work, we combine these fields to study three such weight masking methods and analyse their ability to mitigate "forgetting'' on the source task while also allowing for efficient finetuning on the target task. We find that different masking techniques have trade-offs in retaining knowledge in the source t
    
[^12]: 用于无芯片RFID传感器标签的深度学习辅助的鲁棒检测技术

    Deep learning assisted robust detection techniques for a chipless RFID sensor tag. (arXiv:2308.13944v1 [eess.SP])

    [http://arxiv.org/abs/2308.13944](http://arxiv.org/abs/2308.13944)

    本文首次将机器学习和深度学习应用于无芯片RFID传感器标签的鲁棒检测中，并提出了一种端到端的设计和实现方法。同时，首次报告了不同标签表面形状变化对检测的影响。

    

    本文提出了一种新的方法，可以从无芯片RFID传感器标签中稳定读取识别和传感数据。首次将机器学习（ML）和深度学习（DL）回归建模技术应用于由大规模机器人测量的定制3位无芯片RFID传感器标签的雷达散射截面（RCS）数据集。该机器人系统采用ur16e行业标准机器人，使用首创的自动数据采集方法实现。使用自动系统收集的9600个电磁（EM）RCS签名的大型数据集用于训练和验证四种ML模型和四种一维卷积神经网络（1D CNN）架构。首次报道了用于使用ML/DL模型进行识别（ID）和传感数据的鲁棒检测的端到端设计和实现方法。此外，我们还首次报道了不同标签表面形状变化的影响。

    In this paper, we present a new approach for robust reading of identification and sensor data from chipless RFID sensor tags. For the first time, Machine Learning (ML) and Deep Learning (DL) regression modelling techniques are applied to a dataset of measured Radar Cross Section (RCS) data that has been derived from large-scale robotic measurements of custom-designed, 3-bit chipless RFID sensor tags. The robotic system is implemented using the first-of-its-kind automated data acquisition method using an ur16e industry-standard robot. A large data set of 9,600 Electromagnetic (EM) RCS signatures collected using the automated system is used to train and validate four ML models and four 1-dimensional Convolutional Neural Network (1D CNN) architectures. For the first time, we report an end-to-end design and implementation methodology for robust detection of identification (ID) and sensing data using ML/DL models. Also, we report, for the first time, the effect of varying tag surface shapes
    
[^13]: 一个包含声道动态超声图像序列的小型词汇数据库

    A small vocabulary database of ultrasound image sequences of vocal tract dynamics. (arXiv:2308.13941v1 [cs.SD])

    [http://arxiv.org/abs/2308.13941](http://arxiv.org/abs/2308.13941)

    本文介绍了一个新的数据库，包含了连续的发声器官和声学语音数据，用超声图像和声音数据来研究言语生成过程中舌头上轮廓的可视化。这个数据库是由哥伦比亚圣坦德区的17名年轻被试完成的。

    

    本文介绍了一个包含连续的发声器官和声学语音数据的新数据库。发声器官数据对应于声道动态的超声视频，可以在言语生成过程中可视化舌头上轮廓。声学数据由定向心脏麦克风获取，包括30个短句子。该数据库包括来自哥伦比亚圣坦德区的17名年轻被试（男性8名，女性9名），他们报告没有任何言语病理问题。

    This paper presents a new database consisting of concurrent articulatory and acoustic speech data. The articulatory data correspond to ultrasound videos of the vocal tract dynamics, which allow the visualization of the tongue upper contour during the speech production process. Acoustic data is composed of 30 short sentences that were acquired by a directional cardioid microphone. This database includes data from 17 young subjects (8 male and 9 female) from the Santander region in Colombia, who reported not having any speech pathology.
    
[^14]: 使用运输方法进行顺序基于模拟的推断

    A transport approach to sequential simulation-based inference. (arXiv:2308.13940v1 [stat.ME])

    [http://arxiv.org/abs/2308.13940](http://arxiv.org/abs/2308.13940)

    提出了一种新的基于运输的方法，用于高效地进行静态模型参数的顺序贝叶斯推断。该方法可以处理包括干扰参数的复杂噪声模型，并且适用于仅作为黑箱的正向模型。数值应用表明该方法在使用电导率测量来表征冰厚度的情况下是有效的。

    

    我们提出了一种新的基于运输的方法，用于高效地进行静态模型参数的顺序贝叶斯推断。该策略基于从参数和数据的联合分布中提取条件分布，通过估计结构化（例如，块三角形）运输映射来实现。这为似然函数及其梯度提供了明确的代理模型。这允许在模型无关、在线阶段通过运输映射进行基于梯度的后验密度表征。这个框架非常适用于复杂噪声模型（包括干扰参数）和仅作为黑箱的正向模型的参数估计。我们在使用电导率测量来表征冰厚度的情况下对该方法进行了数值应用。

    We present a new transport-based approach to efficiently perform sequential Bayesian inference of static model parameters. The strategy is based on the extraction of conditional distribution from the joint distribution of parameters and data, via the estimation of structured (e.g., block triangular) transport maps. This gives explicit surrogate models for the likelihood functions and their gradients. This allow gradient-based characterizations of posterior density via transport maps in a model-free, online phase. This framework is well suited for parameter estimation in case of complex noise models including nuisance parameters and when the forward model is only known as a black box. The numerical application of this method is performed in the context of characterization of ice thickness with conductivity measurements.
    
[^15]: 基于射频的无人机检测与识别的二维深度网络用于安全覆盖延伸

    A Two-Dimensional Deep Network for RF-based Drone Detection and Identification Towards Secure Coverage Extension. (arXiv:2308.13906v1 [eess.SP])

    [http://arxiv.org/abs/2308.13906](http://arxiv.org/abs/2308.13906)

    这篇论文提出了基于射频的无人机检测与识别的二维深度网络模型，利用短时傅里叶变换提取了包含时域和频域信息的二维特征，并通过卷积神经网络实现了多类别分类。实验结果表明，该模型在扩展数据集上具有更高的准确性和更快的收敛速度，同时具备良好的平衡性能。

    

    随着无人机在人类生活中越来越普及，它们也带来了安全担忧，包括未经授权的访问和控制，以及与有人飞机的碰撞和干扰。因此，确保能够准确地检测和识别不同的无人机对于覆盖延伸具有重要意义。在机器学习的帮助下，射频（RF）检测可以根据采样的无人机信号识别无人机的类型和飞行模式。本文首先利用短时傅里叶变换（STFT）从原始信号中提取包含时域和频域信息的二维特征。然后，我们使用采用ResNet结构构建的卷积神经网络（CNN）实现多类别分类。我们的实验结果表明，所提出的ResNet-STFT在扩展数据集上具有更高的准确性和更快的收敛速度。此外，与其他基线模型相比，它表现出良好的平衡性能。

    As drones become increasingly prevalent in human life, they also raises security concerns such as unauthorized access and control, as well as collisions and interference with manned aircraft. Therefore, ensuring the ability to accurately detect and identify between different drones holds significant implications for coverage extension. Assisted by machine learning, radio frequency (RF) detection can recognize the type and flight mode of drones based on the sampled drone signals. In this paper, we first utilize Short-Time Fourier. Transform (STFT) to extract two-dimensional features from the raw signals, which contain both time-domain and frequency-domain information. Then, we employ a Convolutional Neural Network (CNN) built with ResNet structure to achieve multi-class classifications. Our experimental results show that the proposed ResNet-STFT can achieve higher accuracy and faster convergence on the extended dataset. Additionally, it exhibits balanced performance compared to other ba
    
[^16]: LMSanitator: 针对任务不可知后门的Prompt-Tuning防御机制

    LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors. (arXiv:2308.13904v1 [cs.CL])

    [http://arxiv.org/abs/2308.13904](http://arxiv.org/abs/2308.13904)

    LMSanitator是一种新颖的方法，用于检测和消除Transformer模型中的任务不可知后门。与传统方法不同，LMSanitator通过逆转预定义的攻击向量而不是触发器，实现更好的收敛性能和后门检测精确度。

    

    Prompt-Tuning已经成为一种引人注目的范式，用于部署大规模语言模型，因为它具有强大的下游任务性能和高效的多任务服务能力。尽管它被广泛采用，我们实证表明，Prompt-Tuning容易受到任务不可知后门的攻击，这些后门存在于预训练模型中，可以影响任意的下游任务。目前的后门检测方法无法防御任务不可知后门，因为它们很难在逆转后门触发器方面收敛。为了解决这个问题，我们提出了LMSanitator，一种在Transformer模型上检测和去除任务不可知后门的新方法。LMSanitator不直接逆转触发器，而是逆转预定义的攻击向量（预训练模型在输入嵌入触发器时的输出），从而实现更好的收敛性能和后门检测精确度。

    Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inversing the triggers, LMSanitator aims to inverse the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prom
    
[^17]: 通过边际上下文信息的半监督语义分割

    Semi-Supervised Semantic Segmentation via Marginal Contextual Information. (arXiv:2308.13900v1 [cs.CV])

    [http://arxiv.org/abs/2308.13900](http://arxiv.org/abs/2308.13900)

    通过利用分割图中标签的空间相关性，我们提出的S4MC方法在半监督语义分割中通过增强伪标签的方式，并提高了无标签数据的使用量，从而实现了超越现有方法的性能提升。

    

    我们提出了一种新的置信度精化方案，增强了半监督语义分割中的伪标签。与当前主流方法不同的是，我们的方法通过将相邻像素分组并共同考虑它们的伪标签，利用分割图中标签的空间相关性。借助这种上下文信息，我们的方法命名为S4MC，在保持伪标签质量的同时，增加了在训练过程中使用的无标签数据的数量，且计算开销几乎可以忽略不计。通过在标准基准测试上进行大量实验证明，S4MC超越了现有的半监督学习方法，为降低获得稠密标注成本提供了有希望的解决方案。例如，在PASCAL VOC 12上使用366个带注释图像，S4MC比前一最先进方法提高了1.29个mIoU。有关重现我们实验的代码参见...

    We present a novel confidence refinement scheme that enhances pseudo-labels in semi-supervised semantic segmentation. Unlike current leading methods, which filter pixels with low-confidence predictions in isolation, our approach leverages the spatial correlation of labels in segmentation maps by grouping neighboring pixels and considering their pseudo-labels collectively. With this contextual information, our method, named S4MC, increases the amount of unlabeled data used during training while maintaining the quality of the pseudo-labels, all with negligible computational overhead. Through extensive experiments on standard benchmarks, we demonstrate that S4MC outperforms existing state-of-the-art semi-supervised learning approaches, offering a promising solution for reducing the cost of acquiring dense annotations. For example, S4MC achieves a 1.29 mIoU improvement over the prior state-of-the-art method on PASCAL VOC 12 with 366 annotated images. The code to reproduce our experiments i
    
[^18]: 内存感知调度在复杂有线网络中使用迭代图优化技术

    Memory-aware Scheduling for Complex Wired Networks with Iterative Graph Optimization. (arXiv:2308.13898v1 [cs.LG])

    [http://arxiv.org/abs/2308.13898](http://arxiv.org/abs/2308.13898)

    在本文中，我们提出了一种基于迭代计算图优化的高效内存感知调度框架，该框架通过迭代图融合和拓扑感知的变量修剪来简化计算图并实现调度优化，有望在复杂有线网络上实现更好的扩展性和减少内存占用。

    

    内存感知网络调度对于在资源受限设备上进行深度神经网络（DNN）推理变得越来越重要。然而，由于复杂的单元级和网络级拓扑结构，内存感知调度变得非常具有挑战性。在本文中，我们提出了一种基于迭代计算图优化的高效内存感知调度框架。我们的框架采用了一种迭代图融合算法，可以简化计算图同时保持调度的最优性。我们进一步提出了整数线性规划方案，并结合拓扑感知的变量修剪来高效调度简化的图。我们在不同网络上评估了我们的方法，并证明了我们的方法在所有基准测试中优于现有技术，将峰值内存占用减少了13.4%，并且在具有复杂拓扑结构的网络上具有更好的可扩展性。

    Memory-aware network scheduling is becoming increasingly important for deep neural network (DNN) inference on resource-constrained devices. However, due to the complex cell-level and network-level topologies, memory-aware scheduling becomes very challenging. While previous algorithms all suffer from poor scalability, in this paper, we propose an efficient memory-aware scheduling framework based on iterative computation graph optimization. Our framework features an iterative graph fusion algorithm that simplifies the computation graph while preserving the scheduling optimality. We further propose an integer linear programming formulation together with topology-aware variable pruning to schedule the simplified graph efficiently. We evaluate our method against prior-art algorithms on different networks and demonstrate that our method outperforms existing techniques in all the benchmarks, reducing the peak memory footprint by 13.4%, and achieving better scalability for networks with comple
    
[^19]: 在移动设备上进行十亿规模语言模型的联邦微调

    Federated Fine-tuning of Billion-Sized Language Models across Mobile Devices. (arXiv:2308.13894v1 [cs.AI])

    [http://arxiv.org/abs/2308.13894](http://arxiv.org/abs/2308.13894)

    这项工作引入了一种创新的FL协议FwdLLM，旨在提高在移动设备上进行十亿规模语言模型的联邦微调（FedLLM）的效率。FwdLLM通过使用无反向传播（BP）训练方法以及“扰动推断”来提高内存效率和时间效率。

    

    大规模语言模型（LLM）正在改变移动智能的格局。联邦学习（FL）是一种保护用户数据隐私的方法，通常用于对下游移动任务进行LLM的微调，这被称为FedLLM。尽管最近的研究已经解决了由庞大模型大小引起的网络问题，但它们在与移动设备的整合方面并没有实际缓解诸多挑战，比如显著的内存消耗和缓慢的模型收敛。为了应对这些挑战，本研究引入了一种创新的FL协议FwdLLM，旨在提高FedLLM的效率。FwdLLM的关键思想是采用无反向传播（BP）训练方法，只需要设备执行“扰动推断”。因此，FwdLLM具有更好的内存效率和时间效率（通过移动NPUs和扩大的参与设备数组）。FwdLLM围绕三个关键设计展开：（1）将无反向传播训练与p

    Large Language Models (LLMs) are transforming the landscape of mobile intelligence. Federated Learning (FL), a method to preserve user data privacy, is often employed in fine-tuning LLMs to downstream mobile tasks, an approach known as FedLLM. Though recent efforts have addressed the network issue induced by the vast model size, they have not practically mitigated vital challenges concerning integration with mobile devices, such as significant memory consumption and sluggish model convergence.  In response to these challenges, this work introduces FwdLLM, an innovative FL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLM to employ backpropagation (BP)-free training methods, requiring devices only to execute ``perturbed inferences''. Consequently, FwdLLM delivers way better memory efficiency and time efficiency (expedited by mobile NPUs and an expanded array of participant devices). FwdLLM centers around three key designs: (1) it combines BP-free training with p
    
[^20]: 药物相互作用向量神经网络: DrIVeNN

    Drug Interaction Vectors Neural Network: DrIVeNN. (arXiv:2308.13891v1 [cs.LG])

    [http://arxiv.org/abs/2308.13891](http://arxiv.org/abs/2308.13891)

    本研究开发了一个名为DrIVeNN的两层神经网络，该网络通过整合药物特征，如分子结构、药物-蛋白相互作用和单药副作用等来建立和评估多药并用不良药物事件（ADEs）模型。

    

    多药并用是治疗单一病症常见的方法，特别是对于管理多种或复杂病症的患者来说。然而，随着药物方案中增加药物的数量，不良药物事件（ADEs）的风险迅速上升。许多与多药并用相关的严重ADEs只有在药物使用后才会被发现。在临床试验中测试每种可能的药物组合是不可行的。这个问题在老年心血管疾病（CVD）患者中尤为常见，多药并用和ADEs常常观察到。本研究的主要目标是确定建立和评估多药并用ADEs模型的关键药物特征。我们的次要目标是在特定领域案例研究中评估我们的模型。我们开发了一个包括分子结构、药物-蛋白相互作用和单药副作用等药物特征的两层神经网络（DrIVeNN）。我们使用公开可用的sid进行了DrIVeNN的评价。

    Polypharmacy, the concurrent use of multiple drugs to treat a single condition, is common in patients managing multiple or complex conditions. However, as more drugs are added to the treatment plan, the risk of adverse drug events (ADEs) rises rapidly. Many serious ADEs associated with polypharmacy only become known after the drugs are in use. It is impractical to test every possible drug combination during clinical trials. This issue is particularly prevalent among older adults with cardiovascular disease (CVD) where polypharmacy and ADEs are commonly observed. In this research, our primary objective was to identify key drug features to build and evaluate a model for modeling polypharmacy ADEs. Our secondary objective was to assess our model on a domain-specific case study. We developed a two-layer neural network that incorporated drug features such as molecular structure, drug-protein interactions, and mono drug side effects (DrIVeNN). We assessed DrIVeNN using publicly available sid
    
[^21]: 机器学习在微生物生物合成中的应用：提高效率和范围的先进技术综述

    Applications of machine Learning to improve the efficiency and range of microbial biosynthesis: a review of state-of-art techniques. (arXiv:2308.13877v1 [q-bio.SC])

    [http://arxiv.org/abs/2308.13877](http://arxiv.org/abs/2308.13877)

    本文综述了机器学习在微生物生物合成中的应用，并提供了对两个关键领域的全面解释和应用的现状及问题。

    

    在现代世界中，技术达到了顶峰。已经探索了不同的编程和技术途径用于数据分析、自动化和机器人技术。机器学习是优化数据分析、进行准确预测和加速/改进现有功能的关键。因此，目前正在发展人工智能领域的机器学习，并且正在探索其在不同领域的应用。其中一个突出的领域就是微生物生物合成。本文提供了微生物生物合成中不同机器学习程序的综合概述，并对机器学习和微生物生物合成领域分别进行了简要描述。该信息包括过去的趋势、现代的发展、未来的改进、流程的解释以及当前面临的问题。因此，本文的主要贡献是梳理了两个关键领域的发展，并提供了对其应用的全面解释。

    In the modern world, technology is at its peak. Different avenues in programming and technology have been explored for data analysis, automation, and robotics. Machine learning is key to optimize data analysis, make accurate predictions, and hasten/improve existing functions. Thus, presently, the field of machine learning in artificial intelligence is being developed and its uses in varying fields are being explored. One field in which its uses stand out is that of microbial biosynthesis. In this paper, a comprehensive overview of the differing machine learning programs used in biosynthesis is provided, alongside brief descriptions of the fields of machine learning and microbial biosynthesis separately. This information includes past trends, modern developments, future improvements, explanations of processes, and current problems they face. Thus, this paper's main contribution is to distill developments in, and provide a holistic explanation of, 2 key fields and their applicability to 
    
[^22]: 类别二值化到神经进化的多类别分类

    Class Binarization to NeuroEvolution for Multiclass Classification. (arXiv:2308.13876v1 [cs.LG])

    [http://arxiv.org/abs/2308.13876](http://arxiv.org/abs/2308.13876)

    本文将类别二值化技术应用于神经进化算法，提出了一种新方法将纠错输出码(ECOC)应用于神经进化算法，用于多类别分类。

    

    多类别分类是机器学习中一项基础且具有挑战性的任务。现有的多类别分类技术可分为三类：(i)将多类别分类分解为二值化分类，使用二值化分类器高效解决。(ii)从二值化分类中扩展而来。(iii)层次分类。本文将类别二值化技术应用于神经进化算法NeuroEvolution of Augmenting Topologies (NEAT)，以生成用于多类别分类的神经网络。我们提出了一种新方法，将纠错输出码 (ECOC) 应用于神经进化算法，用于设计类别二值化策略。

    Multiclass classification is a fundamental and challenging task in machine learning. The existing techniques of multiclass classification can be categorized as (i) decomposition into binary (ii) extension from binary and (iii) hierarchical classification. Decomposing multiclass classification into a set of binary classifications that can be efficiently solved by using binary classifiers, called class binarization, which is a popular technique for multiclass classification. Neuroevolution, a general and powerful technique for evolving the structure and weights of neural networks, has been successfully applied to binary classification. In this paper, we apply class binarization techniques to a neuroevolution algorithm, NeuroEvolution of Augmenting Topologies (NEAT), that is used to generate neural networks for multiclass classification. We propose a new method that applies Error-Correcting Output Codes (ECOC) to design the class binarization strategies on the neuroevolution for multiclas
    
[^23]: 像大脑一样的表征矫正在强大的前馈神经网络中对自然电影进行处理

    Brain-like representational straightening of natural movies in robust feedforward neural networks. (arXiv:2308.13870v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.13870](http://arxiv.org/abs/2308.13870)

    本文研究在强大的前馈神经网络中，通过对输入图像的噪声鲁棒性进行训练，实现了对自然电影进行表征矫正。这种矫正在生物视觉中是常见的，但人工神经网络通常未能呈现这种现象。通过线性插值，这种矫正可以用于生成中间电影帧。

    

    表征矫正是指从自然电影中连续采集的帧的视觉特征表征曲率的减少。先前的研究已经确定了灵长类动物的初级视觉皮层（V1）的表征矫正和人类行为中的感知矫正作为生物视觉的标志，而人工前馈神经网络并未表现出这种现象，因为它们没有明确优化以产生时间可预测的电影表征。在这里，我们展示了输入图像的噪声鲁棒性可以在前馈神经网络中产生表征矫正。对抗训练（AT）和Random Smoothing（RS）的基础分类器都可以产生明显矫正的特征编码。在自然电影领域展示了它们的实用性，这些编码可以通过特征空间的线性插值来反推中间电影帧，即使它们没有在这些帧上进行训练。

    Representational straightening refers to a decrease in curvature of visual feature representations of a sequence of frames taken from natural movies. Prior work established straightening in neural representations of the primate primary visual cortex (V1) and perceptual straightening in human behavior as a hallmark of biological vision in contrast to artificial feedforward neural networks which did not demonstrate this phenomenon as they were not explicitly optimized to produce temporally predictable movie representations. Here, we show robustness to noise in the input image can produce representational straightening in feedforward neural networks. Both adversarial training (AT) and base classifiers for Random Smoothing (RS) induced remarkably straightened feature codes. Demonstrating their utility within the domain of natural movies, these codes could be inverted to generate intervening movie frames by linear interpolation in the feature space even though they were not trained on these
    
[^24]: 停止学习：避免自信地从错误标记的示例中学习

    Late Stopping: Avoiding Confidently Learning from Mislabeled Examples. (arXiv:2308.13862v1 [cs.LG])

    [http://arxiv.org/abs/2308.13862](http://arxiv.org/abs/2308.13862)

    本文提出了一种新的框架，称为停止学习，通过移除高概率错误标记示例来缩小噪声数据集，同时保留大多数干净困难示例，以提高模型的最优泛化性能。

    

    在具有噪声标签的学习中，样本选择是一种常见的方法，通常将小损失的数据视为正确标记的数据。然而，这种方法可能无法有效地识别出具有较大损失的干净困难示例，这对于实现模型接近最优泛化性能至关重要。在本文中，我们提出了一种新的框架——停止学习，通过一个长时间的训练过程，利用深度神经网络的内在鲁棒学习能力。具体而言，停止学习通过逐渐移除高概率错误标记示例来缩小噪声数据集，同时在整个学习过程中保留大多数干净困难示例在训练集中。我们在实证中观察到错误标记和干净示例在被一致和正确分类所需的周期数方面存在差异，因此可以移除高概率错误标记示例。在基准模拟和真实世界的噪声数据集上进行的实验结果表明……

    Sample selection is a prevalent method in learning with noisy labels, where small-loss data are typically considered as correctly labeled data. However, this method may not effectively identify clean hard examples with large losses, which are critical for achieving the model's close-to-optimal generalization performance. In this paper, we propose a new framework, Late Stopping, which leverages the intrinsic robust learning ability of DNNs through a prolonged training process. Specifically, Late Stopping gradually shrinks the noisy dataset by removing high-probability mislabeled examples while retaining the majority of clean hard examples in the training set throughout the learning process. We empirically observe that mislabeled and clean examples exhibit differences in the number of epochs required for them to be consistently and correctly classified, and thus high-probability mislabeled examples can be removed. Experimental results on benchmark-simulated and real-world noisy datasets 
    
[^25]: 无监督异常检测中的偏差问题在脑部MRI中的应用

    Bias in Unsupervised Anomaly Detection in Brain MRI. (arXiv:2308.13861v1 [eess.IV])

    [http://arxiv.org/abs/2308.13861](http://arxiv.org/abs/2308.13861)

    本论文针对无监督异常检测中的偏差问题进行了分析。通过研究训练与测试分布之间的非病理性偏移，揭示了这些偏移的程度。

    

    无监督异常检测方法为监督方法提供了一种有潜力的灵活替代方案，能够革新医学扫描分析，提高诊断性能。当前环境中，人们常常假设测试样本与训练分布之间的差异仅归因于病理条件，这意味着任何差异都被视为异常。然而，常常忽视了其他可能导致分布偏移的潜在因素，包括扫描仪、年龄、性别或种族。这些偏移会严重影响异常检测任务的准确性。一些突出的失败案例引发了对异常检测的偏见、可信度和公平性的担忧。本研究对无监督异常检测中的偏差进行了新颖的分析。通过检查训练和测试分布之间的可能的非病理性分布偏移，我们揭示了这些偏移的程度。

    Unsupervised anomaly detection methods offer a promising and flexible alternative to supervised approaches, holding the potential to revolutionize medical scan analysis and enhance diagnostic performance.  In the current landscape, it is commonly assumed that differences between a test case and the training distribution are attributed solely to pathological conditions, implying that any disparity indicates an anomaly. However, the presence of other potential sources of distributional shift, including scanner, age, sex, or race, is frequently overlooked. These shifts can significantly impact the accuracy of the anomaly detection task. Prominent instances of such failures have sparked concerns regarding the bias, credibility, and fairness of anomaly detection.  This work presents a novel analysis of biases in unsupervised anomaly detection. By examining potential non-pathological distributional shifts between the training and testing distributions, we shed light on the extent of these bi
    
[^26]: 有效的异构联邦学习：基于配对和分裂学习的方法

    Effectively Heterogeneous Federated Learning: A Pairing and Split Learning Based Approach. (arXiv:2308.13849v1 [cs.LG])

    [http://arxiv.org/abs/2308.13849](http://arxiv.org/abs/2308.13849)

    该论文提出了一种有效的异构联邦学习方法，通过配对和分裂学习，解决了异构性带来的训练速度问题，提高了联邦学习的效率。

    

    作为一种有前景的隐私保护机器学习范式，联邦学习在分布式设备协作训练模型时避免了客户端之间的数据传输。然而，由于客户端的异构性，联邦学习存在训练速度的瓶颈问题，导致训练延迟和服务器聚合的延迟。为了解决这个挑战，提出了一种新颖的分裂联邦学习（SFL）框架，它基于计算资源和客户端之间的通信速率将客户端进行配对，并在逻辑层将神经网络模型分成两部分，每个客户端只计算其分配的部分，通过使用分裂学习实现前向推理和后向训练。此外，为了有效解决客户端配对问题，提出了一种启发式贪婪算法，通过重构训练延迟优化来解决该问题。

    As a promising paradigm federated Learning (FL) is widely used in privacy-preserving machine learning, which allows distributed devices to collaboratively train a model while avoiding data transmission among clients. Despite its immense potential, the FL suffers from bottlenecks in training speed due to client heterogeneity, leading to escalated training latency and straggling server aggregation. To deal with this challenge, a novel split federated learning (SFL) framework that pairs clients with different computational resources is proposed, where clients are paired based on computing resources and communication rates among clients, meanwhile the neural network model is split into two parts at the logical level, and each client only computes the part assigned to it by using the SL to achieve forward inference and backward training. Moreover, to effectively deal with the client pairing problem, a heuristic greedy algorithm is proposed by reconstructing the optimization of training late
    
[^27]: 受最优传输启发的慢衰减问题的深度学习框架：利用Sinkhorn损失和Wasserstein核

    Optimal Transport-inspired Deep Learning Framework for Slow-Decaying Problems: Exploiting Sinkhorn Loss and Wasserstein Kernel. (arXiv:2308.13840v1 [math.NA])

    [http://arxiv.org/abs/2308.13840](http://arxiv.org/abs/2308.13840)

    本论文提出了一种将最优传输理论与神经网络结合的新的减小模型（ROM）框架。通过利用Sinkhorn算法进行训练，该框架可以捕捉数据的几何结构，从而实现精确学习减少的解决方案流形。

    

    减小模型（ROMs）被广泛用于科学计算中以处理高维系统。然而，传统的ROM方法可能只能部分捕捉到数据的固有几何特征。这些特征包括底层结构、关系和对精确建模至关重要的基本特征。为了克服这个局限性，我们提出了一个将最优传输（OT）理论和基于神经网络的方法相结合的新型ROM框架。具体而言，我们研究了以Wasserstein距离为自定义核的核Proper正交分解（kPOD）方法，并利用Sinkhorn算法高效地训练得到的神经网络（NN）。通过利用基于OT的非线性降维，所提出的框架能够捕捉数据的几何结构，这对于准确学习减少的解决方案流形至关重要。与传统的均方误差或交叉熵等度量标准相比，

    Reduced order models (ROMs) are widely used in scientific computing to tackle high-dimensional systems. However, traditional ROM methods may only partially capture the intrinsic geometric characteristics of the data. These characteristics encompass the underlying structure, relationships, and essential features crucial for accurate modeling.  To overcome this limitation, we propose a novel ROM framework that integrates optimal transport (OT) theory and neural network-based methods. Specifically, we investigate the Kernel Proper Orthogonal Decomposition (kPOD) method exploiting the Wasserstein distance as the custom kernel, and we efficiently train the resulting neural network (NN) employing the Sinkhorn algorithm. By leveraging an OT-based nonlinear reduction, the presented framework can capture the geometric structure of the data, which is crucial for accurate learning of the reduced solution manifold. When compared with traditional metrics such as mean squared error or cross-entropy,
    
[^28]: 分布式资源管理中的价格差异化游戏对联合学习的影响

    Price-Discrimination Game for Distributed Resource Management in Federated Learning. (arXiv:2308.13838v1 [cs.LG])

    [http://arxiv.org/abs/2308.13838](http://arxiv.org/abs/2308.13838)

    本论文提出了一种价格差异化游戏（PDG），通过对不同客户提供的服务进行定价差异化，改善了联合学习的性能并降低了激励客户参与联合学习的成本。

    

    在传统的联合学习中，参数服务器和多个分布式客户端可以形成典型的买方市场，其中PS/买家数量远远少于客户端/卖家数量。为了改善联合学习的性能并减少激励客户参与联合学习的成本，本文提出了对不同客户提供的服务进行定价差异化，而不是简单地为不同客户提供相同的服务定价。价格差异化基于对联合学习带来的性能改进和计算通信能力的异质性。为此，本文提出了一个价格差异化游戏（PDG），全面解决了联合学习中的分布式资源管理问题，包括多目标权衡、客户端选择和激励机制。由于PDG是一个混合整数非线性规划（MINLP）问题，本文提出了一个具有低计算成本的分布式半启发式算法来解决该问题。

    In vanilla federated learning (FL) such as FedAvg, the parameter server (PS) and multiple distributed clients can form a typical buyer's market, where the number of PS/buyers of FL services is far less than the number of clients/sellers. In order to improve the performance of FL and reduce the cost of motivating clients to participate in FL, this paper proposes to differentiate the pricing for services provided by different clients rather than simply providing the same service pricing for different clients. The price is differentiated based on the performance improvements brought to FL and their heterogeneity in computing and communication capabilities. To this end, a price-discrimination game (PDG) is formulated to comprehensively address the distributed resource management problems in FL, including multi-objective trade-off, client selection, and incentive mechanism. As the PDG is a mixed-integer nonlinear programming (MINLP) problem, a distributed semi-heuristic algorithm with low c
    
[^29]: Class-constrained t-SNE：结合数据特征和类别概率的方法

    Class-constrained t-SNE: Combining Data Features and Class Probabilities. (arXiv:2308.13837v1 [cs.HC])

    [http://arxiv.org/abs/2308.13837](http://arxiv.org/abs/2308.13837)

    本研究提出了一种新的方法，类别约束的t-SNE，用于结合数据特征和类别概率进行降维分析。这种方法通过平衡代价函数中的两个组成部分，优化数据点的位置和代表性表示。

    

    数据特征和类别概率是评估模型结果和识别问题项时的两个主要视角。类别概率表示每个实例属于特定类别的可能性，可以通过概率分类器或人工标注来产生。由于这两个视角都是多维数据，常常使用降维技术来从中提取有用的特征。然而，现有的方法要么仅关注数据特征，要么依赖类别概率估计来指导降维过程。与之前的工作不同，我们提出了一种新的方法，即"类别约束的t-SNE"，将数据特征和类别概率合并在同一个降维结果中。具体来说，我们通过在代价函数中平衡两个相应的组成部分来优化数据点的位置和代表性表示。

    Data features and class probabilities are two main perspectives when, e.g., evaluating model results and identifying problematic items. Class probabilities represent the likelihood that each instance belongs to a particular class, which can be produced by probabilistic classifiers or even human labeling with uncertainty. Since both perspectives are multi-dimensional data, dimensionality reduction (DR) techniques are commonly used to extract informative characteristics from them. However, existing methods either focus solely on the data feature perspective or rely on class probability estimates to guide the DR process. In contrast to previous work where separate views are linked to conduct the analysis, we propose a novel approach, class-constrained t-SNE, that combines data features and class probabilities in the same DR result. Specifically, we combine them by balancing two corresponding components in a cost function to optimize the positions of data points and iconic representation o
    
[^30]: 深度学习用于保持结构的非线性标准哈密顿动力学的通用稳定库普曼嵌入

    Deep Learning for Structure-Preserving Universal Stable Koopman-Inspired Embeddings for Nonlinear Canonical Hamiltonian Dynamics. (arXiv:2308.13835v1 [cs.LG])

    [http://arxiv.org/abs/2308.13835](http://arxiv.org/abs/2308.13835)

    该论文利用深度学习为非线性哈密顿系统发现了保持结构的全局线性化嵌入，通过辛变换获得紧凑辛嵌入，并重点关注其动力学的有界稳定性。

    

    发现非线性系统的适当坐标变换可以构建更简单的模型，从而便于复杂非线性系统的预测、控制和优化。为此，库普曼算子理论提供了一种用于非线性系统的全局线性化的框架，从而允许使用线性工具进行设计研究。在这项工作中，我们关注通过辛变换识别标准非线性哈密顿系统的全局线性化嵌入。尽管这个任务通常很具挑战性，我们利用深度学习的能力来发现所需的嵌入。此外，为了克服库普曼算子在具有连续频谱的系统中的缺点，我们应用了抬升原理并学习全局的立方嵌入。此外，我们着重强调了所发现嵌入的动力学的有界稳定性的强制性要求。我们展示了深度学习在获取紧凑辛嵌入中的能力。

    Discovering a suitable coordinate transformation for nonlinear systems enables the construction of simpler models, facilitating prediction, control, and optimization for complex nonlinear systems. To that end, Koopman operator theory offers a framework for global linearization for nonlinear systems, thereby allowing the usage of linear tools for design studies. In this work, we focus on the identification of global linearized embeddings for canonical nonlinear Hamiltonian systems through a symplectic transformation. While this task is often challenging, we leverage the power of deep learning to discover the desired embeddings. Furthermore, to overcome the shortcomings of Koopman operators for systems with continuous spectra, we apply the lifting principle and learn global cubicized embeddings. Additionally, a key emphasis is paid to enforce the bounded stability for the dynamics of the discovered embeddings. We demonstrate the capabilities of deep learning in acquiring compact symplect
    
[^31]: 图上不平衡学习的综述：问题、技术和未来方向

    A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions. (arXiv:2308.13821v1 [cs.LG])

    [http://arxiv.org/abs/2308.13821](http://arxiv.org/abs/2308.13821)

    本综述对图上不平衡学习进行了全面的审视，旨在纠正数据分布偏差，以获得更准确和代表性的学习结果。

    

    图表示与世界各种场景中普遍存在的相互连接的结构。有效的图分析技术，如图学习方法，使用户能够从图数据中获得深刻的洞察力，为节点分类和链路预测等各种任务提供支持。然而，这些方法常常面临数据不平衡的问题，即在图数据中某些片段拥有大量数据而其他数据稀缺，从而导致偏倚的学习结果。这就需要出现了图上不平衡学习的新兴领域，旨在纠正这些数据分布偏差，以获得更准确和代表性的学习结果。在本综述中，我们对图上不平衡学习的文献进行了全面的审视。我们首先提供了对该概念和相关术语的明确理解，为读者建立了扎实的基础知识。随后，我们提出了两个全面的分类法：（1）问题分类法（Problem Taxonomy）。

    Graphs represent interconnected structures prevalent in a myriad of real-world scenarios. Effective graph analytics, such as graph learning methods, enables users to gain profound insights from graph data, underpinning various tasks including node classification and link prediction. However, these methods often suffer from data imbalance, a common issue in graph data where certain segments possess abundant data while others are scarce, thereby leading to biased learning outcomes. This necessitates the emerging field of imbalanced learning on graphs, which aims to correct these data distribution skews for more accurate and representative learning outcomes. In this survey, we embark on a comprehensive review of the literature on imbalanced learning on graphs. We begin by providing a definitive understanding of the concept and related terminologies, establishing a strong foundational understanding for readers. Following this, we propose two comprehensive taxonomies: (1) the problem taxono
    
[^32]: 保证稳定的二次模型及其在SINDy和操作推断中的应用

    Guaranteed Stable Quadratic Models and their applications in SINDy and Operator Inference. (arXiv:2308.13819v1 [cs.LG])

    [http://arxiv.org/abs/2308.13819](http://arxiv.org/abs/2308.13819)

    本研究提出了一种学习稳定二次模型的推断方法，通过设置适当的优化问题，可以构建具有局部和全局稳定性的二次系统模型。

    

    科学机器学习用于学习动力系统是一种强大的工具，它将数据驱动建模、基于物理建模和经验知识相结合。在工程设计循环和数字双胞胎中起着重要的作用。本文主要关注一种操作推断方法，该方法通过在模型结构上提出先验假设（通常由已知物理学或专家给出），从而构建低维动态模型。然后，我们通过设置适当的优化问题来学习模型的运算符以进行推断。动态系统的一个关键属性是稳定性。然而，推断模型无法保证这种属性。本文提出了一种学习二次模型的推断公式，这些模型在设计上是稳定的。具体而言，我们讨论了局部和全局稳定的二次系统的参数化。此外，对于没有稳定点但有界的二次系统：

    Scientific machine learning for learning dynamical systems is a powerful tool that combines data-driven modeling models, physics-based modeling, and empirical knowledge. It plays an essential role in an engineering design cycle and digital twinning. In this work, we primarily focus on an operator inference methodology that builds dynamical models, preferably in low-dimension, with a prior hypothesis on the model structure, often determined by known physics or given by experts. Then, for inference, we aim to learn the operators of a model by setting up an appropriate optimization problem. One of the critical properties of dynamical systems is{stability. However, such a property is not guaranteed by the inferred models. In this work, we propose inference formulations to learn quadratic models, which are stable by design. Precisely, we discuss the parameterization of quadratic systems that are locally and globally stable. Moreover, for quadratic systems with no stable point yet bounded (e
    
[^33]: 同调卷积神经网络

    Homological Convolutional Neural Networks. (arXiv:2308.13816v1 [cs.LG])

    [http://arxiv.org/abs/2308.13816](http://arxiv.org/abs/2308.13816)

    提出了一种新的同调卷积神经网络架构，通过拓扑约束网络表示来利用表格数据的结构组织，从稀疏的表格数据中获取空间信息，具有良好的有效性和泛化能力。

    

    深度学习方法已经在分类和回归任务上展示出了卓越的性能，但是在表格数据上，经典机器学习方法往往比越来越复杂的深度学习架构更具计算效率且同样有效。这是因为表格数据中特征间的相关性比图像或自然语言的空间或语义关系要弱，而且需要在没有任何先验信息的情况下对依赖关系进行建模。在本文中，我们提出了一种新的深度学习架构，通过拓扑约束网络表示来利用数据的结构组织，从稀疏的表格数据中获取空间信息。所得模型利用了卷积的能力，并围绕网络拓扑中的有限概念来保证（i）数据一致性、（ii）表示有效性、和（iii）容量。我们在多个任务和数据集上进行了实验证明了该模型的有效性和泛化能力。

    Deep learning methods have demonstrated outstanding performances on classification and regression tasks on homogeneous data types (e.g., image, audio, and text data). However, tabular data still poses a challenge with classic machine learning approaches being often computationally cheaper and equally effective than increasingly complex deep learning architectures. The challenge arises from the fact that, in tabular data, the correlation among features is weaker than the one from spatial or semantic relationships in images or natural languages, and the dependency structures need to be modeled without any prior information. In this work, we propose a novel deep learning architecture that exploits the data structural organization through topologically constrained network representations to gain spatial information from sparse tabular data. The resulting model leverages the power of convolutions and is centered on a limited number of concepts from network topology to guarantee (i) a data-c
    
[^34]: SyMOT-Flow: 学习两个任意分布之间的最优输运流动及最大平均差异

    SyMOT-Flow: Learning optimal transport flow for two arbitrary distributions with maximum mean discrepancy. (arXiv:2308.13815v1 [cs.LG])

    [http://arxiv.org/abs/2308.13815](http://arxiv.org/abs/2308.13815)

    本文介绍了一个名为SyMOT-Flow的新模型，它通过最小化两个未知分布样本之间的对称最大平均差异来训练可逆转换，并结合最优输运成本作为正则化，将未知分布转换为标准正态分布。实验证明这种转换可以实现更稳定准确的样本生成。

    

    找到两个未知概率分布之间的转换对于建模复杂数据分布和执行密度估计、样本生成和统计推断等任务至关重要。本文引入了一个名为SyMOT-Flow的新模型，通过最小化两个未知分布样本之间的对称最大平均差异来训练一个可逆转换，并结合最优输运成本作为正则化以获得一个短距离和可解释的转换。得到的转换可以实现更稳定准确的样本生成。我们为所提出的模型建立了一些理论结果，并通过低维示例和高维生成样本展示了其有效性。

    Finding a transformation between two unknown probability distributions from samples is crucial for modeling complex data distributions and perform tasks such as density estimation, sample generation, and statistical inference. One powerful framework for such transformations is normalizing flow, which transforms an unknown distribution into a standard normal distribution using an invertible network. In this paper, we introduce a novel model called SyMOT-Flow that trains an invertible transformation by minimizing the symmetric maximum mean discrepancy between samples from two unknown distributions, and we incorporate an optimal transport cost as regularization to obtain a short-distance and interpretable transformation. The resulted transformation leads to more stable and accurate sample generation. We establish several theoretical results for the proposed model and demonstrate its effectiveness with low-dimensional illustrative examples as well as high-dimensional generative samples obt
    
[^35]: DeLELSTM: 基于分解的线性可解释LSTM模型，能够捕捉时序数据的瞬时和长期影响

    DeLELSTM: Decomposition-based Linear Explainable LSTM to Capture Instantaneous and Long-term Effects in Time Series. (arXiv:2308.13797v1 [cs.LG])

    [http://arxiv.org/abs/2308.13797](http://arxiv.org/abs/2308.13797)

    本文提出了一种基于分解的线性可解释LSTM（DeLELSTM）模型，用于提高时序模型的可解释性。与传统方法不同的是，DeLELSTM能够区分新数据的瞬时影响和历史数据的长期影响。这一方法包含了标准LSTM和张量化LSTM两部分，分别处理不同的影响。

    

    时序预测在各种实际应用中非常常见。尽管深度学习模型，尤其是循环神经网络（RNNs）在时序预测中取得了有希望的结果，但对于关键的高风险应用中，时序模型的解释却鲜有研究。在本文中，我们提出了一种基于分解的线性可解释LSTM（DeLELSTM）来提高LSTM的可解释性。传统上，RNNs的可解释性只关注变量重要性和时间重要性。我们额外区分了新数据的瞬时影响和历史数据的长期影响。具体而言，DeLELSTM包括两个组成部分，即标准LSTM和张量化LSTM。张量化LSTM为每个变量分配一个唯一的隐藏状态，构成一个矩阵 $\mathbf{h}_t$，标准LSTM则使用共享的隐藏状态 $\mathbf{H}_t$ 来对所有变量进行建模。

    Time series forecasting is prevalent in various real-world applications. Despite the promising results of deep learning models in time series forecasting, especially the Recurrent Neural Networks (RNNs), the explanations of time series models, which are critical in high-stakes applications, have received little attention. In this paper, we propose a Decomposition-based Linear Explainable LSTM (DeLELSTM) to improve the interpretability of LSTM. Conventionally, the interpretability of RNNs only concentrates on the variable importance and time importance. We additionally distinguish between the instantaneous influence of new coming data and the long-term effects of historical data. Specifically, DeLELSTM consists of two components, i.e., standard LSTM and tensorized LSTM. The tensorized LSTM assigns each variable with a unique hidden state making up a matrix $\mathbf{h}_t$, and the standard LSTM models all the variables with a shared hidden state $\mathbf{H}_t$. By decomposing the $\mathb
    
[^36]: 利用数据流形上的正则化流进行外域检测

    Out-of-distribution detection using normalizing flows on the data manifold. (arXiv:2308.13792v1 [cs.LG])

    [http://arxiv.org/abs/2308.13792](http://arxiv.org/abs/2308.13792)

    利用正则化流在低维数据流形上进行外域检测，通过估计密度和测量与流形的距离来判断外域数据，有效提高外域检测的准确性。

    

    外域检测的一种常见方法是估计基础数据分布，为外域数据分配较低的可能性值。正则化流是基于可能性的生成模型，通过保持维度的可逆变换提供可计算的密度估计。传统的正则化流在外域检测中容易失败，因为基于可能性的模型面临着维度诅咒的问题。根据流形假设，现实世界的数据通常位于低维流形上。本研究调查了使用正则化流进行外域检测时的流形学习效果。我们通过在低维流形上估计密度，并结合测量与流形的距离作为外域检测的标准。然而，单独使用它们对于这个任务是不足够的。广泛的实验证明了流形学习对外域检测的有效性。

    A common approach for out-of-distribution detection involves estimating an underlying data distribution, which assigns a lower likelihood value to out-of-distribution data. Normalizing flows are likelihood-based generative models providing a tractable density estimation via dimension-preserving invertible transformations. Conventional normalizing flows are prone to fail in out-of-distribution detection, because of the well-known curse of dimensionality problem of the likelihood-based models. According to the manifold hypothesis, real-world data often lie on a low-dimensional manifold. This study investigates the effect of manifold learning using normalizing flows on out-of-distribution detection. We proceed by estimating the density on a low-dimensional manifold, coupled with measuring the distance from the manifold, as criteria for out-of-distribution detection. However, individually, each of them is insufficient for this task. The extensive experimental results show that manifold lea
    
[^37]: 大规模基于梯度的混合因子分析器训练

    Large-scale gradient-based training of Mixtures of Factor Analyzers. (arXiv:2308.13778v1 [cs.LG])

    [http://arxiv.org/abs/2308.13778](http://arxiv.org/abs/2308.13778)

    这篇论文提出了一种大规模基于梯度的混合因子分析器（MFA）训练方法，通过随机梯度下降和随机质心初始化，在处理高维数据时能够简化训练和初始化过程，并避免使用传统方法（如期望-最大化算法）处理大量数据时可能出现的问题。

    

    高斯混合模型是数据分析中的常用工具。然而，当应用于高维数据（如图像）时，由于所需的完整协方差矩阵（CM）的大小，它们面临问题，而对角线或球形CM的使用往往会带来过于严格的限制。混合因子分析器（MFA）模型是GMM的一个重要扩展，它允许根据因子加载数平滑地插值对角线和完整CM之间的差异。MFA成功应用于建模高维图像数据。这篇文章提供了理论分析以及一种新的高维MFA训练方法，采用随机梯度下降，从随机的质心初始化开始。这极大地简化了训练和初始化过程，并避免了批处理型算法（如期望-最大化算法）在处理大量数据时的问题。此外，通过利用p

    Gaussian Mixture Models (GMMs) are a standard tool in data analysis. However, they face problems when applied to high-dimensional data (e.g., images) due to the size of the required full covariance matrices (CMs), whereas the use of diagonal or spherical CMs often imposes restrictions that are too severe. The Mixture of Factor analyzers (MFA) model is an important extension of GMMs, which allows to smoothly interpolate between diagonal and full CMs based on the number of \textit{factor loadings} $l$. MFA has successfully been applied for modeling high-dimensional image data. This article contributes both a theoretical analysis as well as a new method for efficient high-dimensional MFA training by stochastic gradient descent, starting from random centroid initializations. This greatly simplifies the training and initialization process, and avoids problems of batch-type algorithms such Expectation-Maximization (EM) when training with huge amounts of data. In addition, by exploiting the p
    
[^38]: 自监督可扩展深度压缩感知

    Self-Supervised Scalable Deep Compressed Sensing. (arXiv:2308.13777v1 [eess.SP])

    [http://arxiv.org/abs/2308.13777](http://arxiv.org/abs/2308.13777)

    本文提出了一种自监督的可扩展深度压缩感知方法，不需要标记的测量-地面真实数据，并且可以处理任意的采样比率和矩阵。该方法包括一个双域损失和四个恢复阶段，通过最大化数据/信息利用率来提高准确性。

    

    压缩感知（CS）是降低采样成本的一种有前景的工具。当前基于深度神经网络（NN）的CS方法在收集标记的测量-地面真实（GT）数据和推广到实际应用方面面临挑战。本文提出了一种新颖的自监督可扩展深度CS方法，包括一个称为SCL的学习方案和一个名为SCNet的网络系列，它不需要GT并且可以处理一旦在部分测量集上训练完毕就可以处理任意的采样比率和矩阵。我们的SCL包含一个双域损失和一个四阶段恢复策略。前者鼓励两个测量部分的交叉一致性以及采样-重构循环一致性，从而最大化数据/信息利用率。后者可以逐步利用外部测量中的常见信号先验和测试样本以及学习的NN的内部特征来提高准确性。

    Compressed sensing (CS) is a promising tool for reducing sampling costs. Current deep neural network (NN)-based CS methods face challenges in collecting labeled measurement-ground truth (GT) data and generalizing to real applications. This paper proposes a novel $\mathbf{S}$elf-supervised s$\mathbf{C}$alable deep CS method, comprising a $\mathbf{L}$earning scheme called $\mathbf{SCL}$ and a family of $\mathbf{Net}$works named $\mathbf{SCNet}$, which does not require GT and can handle arbitrary sampling ratios and matrices once trained on a partial measurement set. Our SCL contains a dual-domain loss and a four-stage recovery strategy. The former encourages a cross-consistency on two measurement parts and a sampling-reconstruction cycle-consistency regarding arbitrary ratios and matrices to maximize data/information utilization. The latter can progressively leverage common signal prior in external measurements and internal characteristics of test samples and learned NNs to improve accur
    
[^39]: 使用Detectron2进行孟加拉语文档布局分析

    Bengali Document Layout Analysis with Detectron2. (arXiv:2308.13769v1 [cs.CV])

    [http://arxiv.org/abs/2308.13769](http://arxiv.org/abs/2308.13769)

    本研究使用Detectron2库中的先进模型，通过对孟加拉语文档进行布局分析，提高了模型的准确性。该研究讨论了速度和准确性的权衡，为未来的研究提出了方向。

    

    文档数字化对于保护历史记录、高效的文档管理和推进光学字符识别（OCR）研究至关重要。文档布局分析（DLA）涉及将文档分割为文本框、段落、图像和表格等有意义的单位。面对各种布局、历史文件和孟加拉语等特殊脚本的挑战，缺乏全面的孟加拉DLA数据集。我们利用Detectron2库中的先进的Mask R-CNN模型提高了孟加拉语文档的DLA模型的准确性。我们的评估包括三个变种：Mask R-CNN R-50、R-101和X-101，在BaDLAD数据集上使用PubLayNet的预训练权重和不使用预训练权重，该数据集包含人工注释的四个类别的孟加拉语文档：文本框、段落、图像和表格。结果表明这些模型在准确分割孟加拉语文档方面的有效性。我们讨论了速度和准确性的权衡以及未来的研究方向。

    Document digitization is vital for preserving historical records, efficient document management, and advancing OCR (Optical Character Recognition) research. Document Layout Analysis (DLA) involves segmenting documents into meaningful units like text boxes, paragraphs, images, and tables. Challenges arise when dealing with diverse layouts, historical documents, and unique scripts like Bengali, hindered by the lack of comprehensive Bengali DLA datasets. We improved the accuracy of the DLA model for Bengali documents by utilizing advanced Mask R-CNN models available in the Detectron2 library. Our evaluation involved three variants: Mask R-CNN R-50, R-101, and X-101, both with and without pretrained weights from PubLayNet, on the BaDLAD dataset, which contains human-annotated Bengali documents in four categories: text boxes, paragraphs, images, and tables. Results show the effectiveness of these models in accurately segmenting Bengali documents. We discuss speed-accuracy tradeoffs and unde
    
[^40]: 通过对语言模型进行敌对微调：针对问题内容生成和检测的迭代优化方法

    Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content. (arXiv:2308.13768v1 [cs.CL])

    [http://arxiv.org/abs/2308.13768](http://arxiv.org/abs/2308.13768)

    本研究提出了一种新的双阶段优化技术，使用对抗性微调来解决大型语言模型中意外生成有害内容的问题。通过迭代的提示和微调，实现了持续的改进和性能提升，并在具有挑战性的数据集上展示了显著的分类准确度提升。

    

    本文采用一种新颖的双阶段优化技术，使用对抗性微调来应对大型语言模型（LLMs）中意外生成有害内容的挑战。我们的方法包括对抗模型和判别模型两个阶段，对抗模型被微调用于生成潜在有害提示，而判别模型则通过迭代优化来识别这些提示。通过对抗循环，两个模型在提示阶段争相超越对方，生成包含丰富示例的数据集，然后用于微调。这种迭代应用提示和微调的方法使得持续的改进和性能提升成为可能。我们通过在一个包含GPT-4未检测到的问题提示和一些有争议但无问题的提示的数据集上进行分类准确度的评估来验证我们的方法的性能。结果显示在这个具有挑战性的数据集上，判别模型的分类准确度有了显著提升。

    In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisat
    
[^41]: SamDSK: 结合分割任意模型和领域特定知识进行医学图像分割的半监督学习

    SamDSK: Combining Segment Anything Model with Domain-Specific Knowledge for Semi-Supervised Learning in Medical Image Segmentation. (arXiv:2308.13759v1 [cs.CV])

    [http://arxiv.org/abs/2308.13759](http://arxiv.org/abs/2308.13759)

    这篇论文介绍了一种新方法SamDSK，结合分割任意模型和领域特定知识，在医学图像分割中进行半监督学习。方法包括迭代的两个阶段：分割模型训练和使用训练好的模型和领域特定知识扩展有标签集。通过将分割建议和领域特定知识结合，构建无标签图像的注释。

    

    分割任意模型（SAM）展示了在自然图像中分割各种对象的能力，是各种下游图像分割任务的多功能感知工具。然而，医学图像分割任务通常依赖于领域特定知识（DSK）。在本文中，我们提出了一种将分割基础模型（即SAM）与领域特定知识相结合的新方法，以可靠地利用无标签图像构建医学图像分割模型。我们的新方法是迭代的，包括两个主要阶段：（1）分割模型训练；（2）使用训练好的分割模型、无标签集、SAM和领域特定知识扩展有标签集。这两个阶段重复进行，直到无法再添加样本到有标签集为止。我们开发了一种基于最优匹配的方法，将SAM生成的分割建议与像素级和图像级的DSK相结合，构建无标签图像的注释。

    The Segment Anything Model (SAM) exhibits a capability to segment a wide array of objects in natural images, serving as a versatile perceptual tool for various downstream image segmentation tasks. In contrast, medical image segmentation tasks often rely on domain-specific knowledge (DSK). In this paper, we propose a novel method that combines the segmentation foundation model (i.e., SAM) with domain-specific knowledge for reliable utilization of unlabeled images in building a medical image segmentation model. Our new method is iterative and consists of two main stages: (1) segmentation model training; (2) expanding the labeled set by using the trained segmentation model, an unlabeled set, SAM, and domain-specific knowledge. These two stages are repeated until no more samples are added to the labeled set. A novel optimal-matching-based method is developed for combining the SAM-generated segmentation proposals and pixel-level and image-level DSK for constructing annotations of unlabeled 
    
[^42]: PE-MED: 提高交互式医学图像分割的提示增强技术

    PE-MED: Prompt Enhancement for Interactive Medical Image Segmentation. (arXiv:2308.13746v1 [cs.CV])

    [http://arxiv.org/abs/2308.13746](http://arxiv.org/abs/2308.13746)

    本文介绍了一个名为PE-MED的新框架，用于提高交互式医学图像分割的成功率。它通过引入自循环策略和提示注意力学习模块来改善用户提供的提示信息的利用，从而防止不利情况的发生，并提高分割结果的准确性。

    

    交互式医学图像分割是指通过用户与图像之间的交互（例如，点击）准确地分割出感兴趣的目标。近年来，它得到了广泛研究，因为它不太依赖于大量标注的数据，比完全自动化的分割更加灵活。然而，当前的研究还没有充分探索用户提供的提示信息（例如，点），包括在一个交互中挖掘的知识以及多个交互之间的关系。因此，本文介绍了一种装备有提示增强的新型框架，名为PE-MED，用于交互式医学图像分割。首先，我们引入了一种自循环策略，基于第一个提示生成温暖的初始分割结果。它可以防止出现高度不利的情况，比如在第一次交互后遇到空白遮罩作为初始输入。其次，我们提出了一种新颖的提示注意力学习模块（PALM），用于挖掘有用的提示信息。

    Interactive medical image segmentation refers to the accurate segmentation of the target of interest through interaction (e.g., click) between the user and the image. It has been widely studied in recent years as it is less dependent on abundant annotated data and more flexible than fully automated segmentation. However, current studies have not fully explored user-provided prompt information (e.g., points), including the knowledge mined in one interaction, and the relationship between multiple interactions. Thus, in this paper, we introduce a novel framework equipped with prompt enhancement, called PE-MED, for interactive medical image segmentation. First, we introduce a Self-Loop strategy to generate warm initial segmentation results based on the first prompt. It can prevent the highly unfavorable scenarios, such as encountering a blank mask as the initial input after the first interaction. Second, we propose a novel Prompt Attention Learning Module (PALM) to mine useful prompt infor
    
[^43]: 通过MCMC速度度量学习变分自动编码器

    Learning variational autoencoders via MCMC speed measures. (arXiv:2308.13731v1 [stat.ML])

    [http://arxiv.org/abs/2308.13731](http://arxiv.org/abs/2308.13731)

    本研究提出了一种基于熵的短期调整MCMC链的方法，用于在优化更紧的变分边界的同时，适应深度潜变量模型的提案分布。实验证明，这种方法能够使模型得到更高的保留对数似然和改进的生成性能。

    

    变分自动编码器（VAEs）是一种流行的基于似然的生成模型，可以通过最大化下界（ELBO）来有效训练。为了获得更紧的变分边界和更高的生成性能，改进变分分布的表达能力取得了很大进展。虽然先前的工作利用马尔可夫链蒙特卡洛（MCMC）方法构建了变分密度，但针对深度潜变量模型调整提案分布的基于梯度的方法受到的关注较少。本研究提出一种基于熵的短期调整Metropolis-adjusted Langevin（MALA）或Hamiltonian Monte Carlo（HMC）链的方法，并优化更紧的变分边界以获得对数似然。实验证明，该方法产生了更高的保留对数似然以及改进的生成指标。我们的隐式变分密度能够适应潜在层次表示的复杂后验几何形状。

    Variational autoencoders (VAEs) are popular likelihood-based generative models which can be efficiently trained by maximizing an Evidence Lower Bound (ELBO). There has been much progress in improving the expressiveness of the variational distribution to obtain tighter variational bounds and increased generative performance. Whilst previous work has leveraged Markov chain Monte Carlo (MCMC) methods for the construction of variational densities, gradient-based methods for adapting the proposal distributions for deep latent variable models have received less attention. This work suggests an entropy-based adaptation for a short-run Metropolis-adjusted Langevin (MALA) or Hamiltonian Monte Carlo (HMC) chain while optimising a tighter variational bound to the log-evidence. Experiments show that this approach yields higher held-out log-likelihoods as well as improved generative metrics. Our implicit variational density can adapt to complicated posterior geometries of latent hierarchical repres
    
[^44]: Muffin:通过整合现成模型实现多维度人工智能公平性的框架

    Muffin: A Framework Toward Multi-Dimension AI Fairness by Uniting Off-the-Shelf Models. (arXiv:2308.13730v1 [cs.LG])

    [http://arxiv.org/abs/2308.13730](http://arxiv.org/abs/2308.13730)

    本文提出了一个名为Muffin的框架，通过整合现成模型，实现多维度人工智能公平性。研究首先揭示了不同不公平属性之间的强相关性，随后提出了一个新的多维度公平性框架Muffin，其中包括一个自动工具来提高公平性。

    

    模型公平性（即偏见）已成为各种人工智能应用中最重要的问题之一。一个不公平的模型在自动驾驶中可能会导致交通事故，如果对极端情况（如极端天气）不能公正对待；或者如果AI模型误诊某一群体（如棕色和黑色皮肤），将导致医疗差异。近年来，有许多关于解决不公平性的研究工作涌现出来，主要关注单一的不公平属性，如肤色；然而，现实世界的数据通常具有多个属性，其中不公平性可以存在于多个属性中，称为“多维度公平性”。本文首先揭示了不同不公平属性之间的强相关性，即在一个属性上优化公平性会导致其他属性的崩溃。然后，我们提出了一个新的多维度公平性框架Muffin，其中包括一个自动工具来整合现成模型来提高公平性。

    Model fairness (a.k.a., bias) has become one of the most critical problems in a wide range of AI applications. An unfair model in autonomous driving may cause a traffic accident if corner cases (e.g., extreme weather) cannot be fairly regarded; or it will incur healthcare disparities if the AI model misdiagnoses a certain group of people (e.g., brown and black skin). In recent years, there have been emerging research works on addressing unfairness, and they mainly focus on a single unfair attribute, like skin tone; however, real-world data commonly have multiple attributes, among which unfairness can exist in more than one attribute, called 'multi-dimensional fairness'. In this paper, we first reveal a strong correlation between the different unfair attributes, i.e., optimizing fairness on one attribute will lead to the collapse of others. Then, we propose a novel Multi-Dimension Fairness framework, namely Muffin, which includes an automatic tool to unite off-the-shelf models to improv
    
[^45]: 动态模态分解用于基于数据驱动的ExB等离子体分析和降阶建模：II. 动力学预测。

    Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: II. dynamics forecasting. (arXiv:2308.13727v1 [physics.plasm-ph])

    [http://arxiv.org/abs/2308.13727](http://arxiv.org/abs/2308.13727)

    本文介绍了一种基于动态模态分解（DMD）算法的优化版本（OPT-DMD），用于分析ExB等离子体的数据驱动降阶建模，并评估了该算法在不同物理参数下预测等离子体动力学的能力。

    

    在第一部分的文章中，我们证明了一种基于变量投影优化的动态模态分解（DMD）算法的变种，称为优化DMD（OPT-DMD），能够在不同物理参数的ExB模拟配置中鲁棒地识别数据中潜在的时空相干模式。由于OPT-DMD可以通过构造被限制为产生稳定的降阶模型（ROMs），因此在本文中，我们扩展了OPT-DMD的应用，并研究了该算法中线性ROMs在类似于Hall推力器的径向-方位和轴向-方位截面以及每个测试案例的一系列模拟参数下预测等离子体动力学的能力。OPT-DMD ROM的预测能力主要通过短期动力学预测进行评估，或者换句话说，用于训练-测试数据比大的情况。

    In part I of the article, we demonstrated that a variant of the Dynamic Mode Decomposition (DMD) algorithm based on variable projection optimization, called Optimized DMD (OPT-DMD), enables a robust identification of the dominant spatiotemporally coherent modes underlying the data across various test cases representing different physical parameters in an ExB simulation configuration. As the OPT-DMD can be constrained to produce stable reduced-order models (ROMs) by construction, in this paper, we extend the application of the OPT-DMD and investigate the capabilities of the linear ROM from this algorithm toward forecasting in time of the plasma dynamics in configurations representative of the radial-azimuthal and axial-azimuthal cross-sections of a Hall thruster and over a range of simulation parameters in each test case. The predictive capacity of the OPT-DMD ROM is assessed primarily in terms of short-term dynamics forecast or, in other words, for large ratios of training-to-test data
    
[^46]: 基于数据驱动的动态模式分解用于ExB等离子体的分析和降阶建模：I.提取时空协同模式

    Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: I. Extraction of spatiotemporally coherent patterns. (arXiv:2308.13726v1 [physics.plasm-ph])

    [http://arxiv.org/abs/2308.13726](http://arxiv.org/abs/2308.13726)

    本论文评估了基于数据驱动的动态模式分解（DMD）算法在ExB等离子体动态分析和降阶建模中的实用性和普适性，并展示了DMD在提取高维等离子体动力学中的时空模式方面的有效性。

    

    在这篇两部分的文章中，我们评估了动态模式分解（DMD）算法在基于数据驱动的交叉场ExB配置等离子体动态分析和降阶建模中的实用性和普适性。DMD算法是一种可解释的基于数据驱动的方法，通过找到描述数据中时空协同结构（模式）的时间演化的最佳拟合线性模型。我们使用基于高效降阶PIC方案的粒子-网格（PIC）代码生成了大量高保真度数据，并将DMD应用于这些数据。在本部分中，我们首先概述了DMD的概念及其基础的正交模态和奇异值分解方法。接下来介绍了DMD的两种主要变体。然后，我们展示和讨论了DMD应用的结果，包括在各种模拟条件下，从高保真度数据中识别和提取主要时空模式。我们证明了DMD能够有效地提取高维等离子体动力学中的时空模式。

    In this two-part article, we evaluate the utility and the generalizability of the Dynamic Mode Decomposition (DMD) algorithm for data-driven analysis and reduced-order modelling of plasma dynamics in cross-field ExB configurations. The DMD algorithm is an interpretable data-driven method that finds a best-fit linear model describing the time evolution of spatiotemporally coherent structures (patterns) in data. We have applied the DMD to extensive high-fidelity datasets generated using a particle-in-cell (PIC) code based on a cost-efficient reduced-order PIC scheme. In this part, we first provide an overview of the concept of DMD and its underpinning Proper Orthogonal and Singular Value Decomposition methods. Two of the main DMD variants are next introduced. We then present and discuss the results of the DMD application in terms of the identification and extraction of the dominant spatiotemporal modes from high-fidelity data over a range of simulation conditions. We demonstrate that the
    
[^47]: Time-to-Pattern: 信息论无监督学习用于可扩展的时间序列摘要

    Time-to-Pattern: Information-Theoretic Unsupervised Learning for Scalable Time Series Summarization. (arXiv:2308.13722v1 [cs.LG])

    [http://arxiv.org/abs/2308.13722](http://arxiv.org/abs/2308.13722)

    这篇论文提出了一种名为Time-to-Pattern的时间序列摘要方法，通过基于信息论的无监督学习，在一个可解释的潜在空间中学习离散时间序列的信息嵌入，找到一组不同的模式以编码最显著的信息，提高了摘要的质量。

    

    数据摘要是从数据集中生成可解释和代表性子集的过程。现有的时间序列摘要方法通常使用一组手动设计的相似度函数来搜索重复子序列以摘要数据。然而，这些方法面临着来自详尽搜索和启发式定义序列相似度的限制。这些方法影响了生成的数据摘要的多样性和全面性。为了缓解这些限制，我们引入了一种时间序列摘要方法，称为Time-to-Pattern（T2P），它旨在找到一组不同的模式，共同编码最显著的信息，遵循最小描述长度的概念。T2P是一个深度生成模型，它在设计成可解释的潜在空间上学习离散时间序列的信息嵌入。我们的合成和真实世界的实验证明T2P可以提高摘要质量并保持多样性和全面性。

    Data summarization is the process of generating interpretable and representative subsets from a dataset. Existing time series summarization approaches often search for recurring subsequences using a set of manually devised similarity functions to summarize the data. However, such approaches are fraught with limitations stemming from an exhaustive search coupled with a heuristic definition of series similarity. Such approaches affect the diversity and comprehensiveness of the generated data summaries. To mitigate these limitations, we introduce an approach to time series summarization, called Time-to-Pattern (T2P), which aims to find a set of diverse patterns that together encode the most salient information, following the notion of minimum description length. T2P is implemented as a deep generative model that learns informative embeddings of the discrete time series on a latent space specifically designed to be interpretable. Our synthetic and real-world experiments reveal that T2P dis
    
[^48]: 揭示联邦学习在心血管疾病检测中的优势与挑战：一个综述文献回顾

    Uncovering Promises and Challenges of Federated Learning to Detect Cardiovascular Diseases: A Scoping Literature Review. (arXiv:2308.13714v1 [cs.LG])

    [http://arxiv.org/abs/2308.13714](http://arxiv.org/abs/2308.13714)

    该论文综述了联邦学习在心血管疾病检测中的应用现状，并讨论了其优势和挑战。研究比较了联邦学习与传统集中式学习方法的模型准确性、隐私和数据分布处理能力。最后，提供了对联邦学习当前挑战的批判性分析。

    

    心血管疾病是全球死亡的主要原因，早期检测可显著改善患者的预后。机器学习模型可以帮助早期诊断心血管疾病，但其性能受限于用于模型训练的数据。医疗保护隐私的隐私问题使得获取准确的机器学习模型训练数据更加困难。联邦学习是一种新兴的机器学习方法，它允许在不损害个体数据所有者隐私的情况下训练模型使用来自多个数据源的数据。本综述文献提供了联邦学习在心血管疾病检测中的现状概述。我们回顾了在各种论文中提出的不同联邦学习模型，并讨论了它们的优势和挑战。我们还将联邦学习与传统的集中式学习方法进行了比较，并在模型准确性、隐私和数据分布处理能力等方面突出了差异。最后，我们对联邦学习目前面临的挑战进行了批判性分析。

    Cardiovascular diseases (CVD) are the leading cause of death globally, and early detection can significantly improve outcomes for patients. Machine learning (ML) models can help diagnose CVDs early, but their performance is limited by the data available for model training. Privacy concerns in healthcare make it harder to acquire data to train accurate ML models. Federated learning (FL) is an emerging approach to machine learning that allows models to be trained on data from multiple sources without compromising the privacy of the individual data owners. This survey paper provides an overview of the current state-of-the-art in FL for CVD detection. We review the different FL models proposed in various papers and discuss their advantages and challenges. We also compare FL with traditional centralized learning approaches and highlight the differences in terms of model accuracy, privacy, and data distribution handling capacity. Finally, we provide a critical analysis of FL's current challe
    
[^49]: 剩余去噪扩散模型

    Residual Denoising Diffusion Models. (arXiv:2308.13712v1 [cs.CV])

    [http://arxiv.org/abs/2308.13712](http://arxiv.org/abs/2308.13712)

    提出了剩余去噪扩散模型（RDDM），相比于现有的扩散模型，该模型通过预测残差来表示从目标域到输入域的方向性扩散，并同时估计噪声来考虑扩散过程中的随机扰动，从而实现了统一的图像生成和恢复。

    

    当前基于扩散的图像恢复方法将退化的输入图像作为条件输入到噪声估计网络中。然而，解释这个扩散过程是具有挑战性的，因为它基本上是从噪声生成目标图像。为了建立一个统一且更可解释的图像生成和恢复模型，我们提出了剩余去噪扩散模型（RDDM）。与现有的扩散模型（例如DDPM或DDIM）仅专注于噪声估计不同，我们的RDDM预测残差来表示从目标域到输入域的方向性扩散，并同时估计噪声来考虑扩散过程中的随机扰动。残差的引入使我们能够重新定义正向扩散过程，其中目标图像逐渐扩散成一个纯噪声图像或携带噪声的输入图像，从而统一了图像生成和恢复。我们证明了我们的采样过程与真实的目标图像分布一致。

    Current diffusion-based image restoration methods feed degraded input images as conditions into the noise estimation network. However, interpreting this diffusion process is challenging since it essentially generates the target image from the noise. To establish a unified and more interpretable model for image generation and restoration, we propose residual denoising diffusion models (RDDM). In contrast to existing diffusion models (e.g., DDPM or DDIM) that focus solely on noise estimation, our RDDM predicts residuals to represent directional diffusion from the target domain to the input domain, while concurrently estimating noise to account for random perturbations in the diffusion process. The introduction of residuals allows us to redefine the forward diffusion process, wherein the target image progressively diffuses into a purely noisy image or a noise-carrying input image, thus unifying image generation and restoration. We demonstrate that our sampling process is consistent with t
    
[^50]: PAITS: 不规则采样时间序列的预训练和增强

    PAITS: Pretraining and Augmentation for Irregularly-Sampled Time Series. (arXiv:2308.13703v1 [cs.LG])

    [http://arxiv.org/abs/2308.13703](http://arxiv.org/abs/2308.13703)

    PAITS是针对稀疏和不规则采样时间序列数据集的预训练和增强框架，通过结合NLP启发式预训练任务和增强技术，以及随机搜索来找到适合特定数据集的有效策略，相较于传统方法，能够持续改进多个数据集和领域的预训练效果。

    

    现实世界中常见的时间序列数据通常反映了顺序人类行为，而这些数据经常是独特的不规则采样和稀疏的，时间和实体上的采样非常不均匀。然而，常用的时间序列预训练和增强方法并不具备针对这种情况的专门设计。本文提出了PAITS（不规则采样时间序列的预训练和增强）框架，用于识别适合稀疏和不规则采样时间序列数据集的预训练策略。PAITS利用一种新颖的结合了NLP启发式预训练任务和增强技术的方法，以及随机搜索来为给定数据集找到有效的策略。我们证明不同的数据集受益于不同的预训练选择。与之前的方法相比，我们的方法能够更好地持续改进多个数据集和领域的预训练效果。我们的代码可在 \url{https://github.com/google-research/google-research/tree/master/irregul} 找到。

    Real-world time series data that commonly reflect sequential human behavior are often uniquely irregularly sampled and sparse, with highly nonuniform sampling over time and entities. Yet, commonly-used pretraining and augmentation methods for time series are not specifically designed for such scenarios. In this paper, we present PAITS (Pretraining and Augmentation for Irregularly-sampled Time Series), a framework for identifying suitable pretraining strategies for sparse and irregularly sampled time series datasets. PAITS leverages a novel combination of NLP-inspired pretraining tasks and augmentations, and a random search to identify an effective strategy for a given dataset. We demonstrate that different datasets benefit from different pretraining choices. Compared with prior methods, our approach is better able to consistently improve pretraining across multiple datasets and domains. Our code is available at \url{https://github.com/google-research/google-research/tree/master/irregul
    
[^51]: Twitter上的政党预测

    Party Prediction for Twitter. (arXiv:2308.13699v1 [cs.SI])

    [http://arxiv.org/abs/2308.13699](http://arxiv.org/abs/2308.13699)

    本文提供了对当前党派预测做法的综合调查和实证比较，并提出了几种新的方法，这些方法与最先进的方法相竞争或超越之，然而却需要更少的计算资源。

    

    在社交媒体上进行的大量研究比较了来自不同政党的用户的行为。作为基本步骤，他们使用预测模型来推断他们的政治派别。这个模型的准确性可以显著改变下游分析的结论，然而在不同模型之间的选择似乎是任意的。在本文中，我们提供了对当前党派预测做法的综合调查和实证比较，并提出了几种新的方法，这些方法与最先进的方法相竞争或超越之，然而却需要更少的计算资源。党派预测模型依赖于用户生成的内容（例如推文文本），他们的关系（例如他们关注谁），或者他们的活动和互动（例如他们喜欢哪些推文）。我们检查了所有这些，并比较了它们在党派预测任务中的信号强度。本文让从业者可以从多种数据类型中进行选择，而这些数据类型都是---

    A large number of studies on social media compare the behaviour of users from different political parties. As a basic step, they employ a predictive model for inferring their political affiliation. The accuracy of this model can change the conclusions of a downstream analysis significantly, yet the choice between different models seems to be made arbitrarily. In this paper, we provide a comprehensive survey and an empirical comparison of the current party prediction practices and propose several new approaches which are competitive with or outperform state-of-the-art methods, yet require less computational resources. Party prediction models rely on the content generated by the users (e.g., tweet texts), the relations they have (e.g., who they follow), or their activities and interactions (e.g., which tweets they like). We examine all of these and compare their signal strength for the party prediction task. This paper lets the practitioner select from a wide range of data types that all
    
[^52]: 重新思考语言模型作为符号知识图谱

    Rethinking Language Models as Symbolic Knowledge Graphs. (arXiv:2308.13676v1 [cs.CL])

    [http://arxiv.org/abs/2308.13676](http://arxiv.org/abs/2308.13676)

    本研究对不同大小和能力的语言模型进行了全面评估，发现它们能否涵盖知识图谱的复杂拓扑和语义属性，这对于推理过程至关重要。

    

    符号知识图谱在搜索、问答和推荐等以知识为中心的应用中起着关键作用。随着当代基于大量文本数据训练的语言模型（LMs）的重要性日益增加，研究人员广泛探讨了这些模型中的参数化知识是否能够与知识图谱中的知识相匹配。各种方法表明，增加模型大小或训练数据量可以增强其检索符号知识的能力，通常几乎不需要人工监督。尽管取得了这些进展，但我们对于语言模型能否涵盖知识图谱的复杂拓扑和语义属性进行了全面评估，这些属性对于推理过程至关重要。在这项工作中，我们对不同大小和能力的语言模型进行了详尽的评估。我们构建了九个定性基准，涵盖了一系列属性，包括对称性、不对称性、

    Symbolic knowledge graphs (KGs) play a pivotal role in knowledge-centric applications such as search, question answering and recommendation. As contemporary language models (LMs) trained on extensive textual data have gained prominence, researchers have extensively explored whether the parametric knowledge within these models can match up to that present in knowledge graphs. Various methodologies have indicated that enhancing the size of the model or the volume of training data enhances its capacity to retrieve symbolic knowledge, often with minimal or no human supervision. Despite these advancements, there is a void in comprehensively evaluating whether LMs can encompass the intricate topological and semantic attributes of KGs, attributes crucial for reasoning processes. In this work, we provide an exhaustive evaluation of language models of varying sizes and capabilities. We construct nine qualitative benchmarks that encompass a spectrum of attributes including symmetry, asymmetry, h
    
[^53]: 线性振动：视觉转换器中的困惑美学

    Linear Oscillation: The Aesthetics of Confusion for Vision Transformer. (arXiv:2308.13670v1 [cs.LG])

    [http://arxiv.org/abs/2308.13670](http://arxiv.org/abs/2308.13670)

    研究提出了一种新的激活函数——线性振动（LoC）激活函数，它通过将线性轨迹和振荡偏差无缝融合，捕捉到了“困惑的重要性”的本质。实证研究表明，在需要区分微妙模式的情境中，使用LoC激活函数可以提高网络的鲁棒性。

    

    激活函数是深度学习的关键，深刻影响神经网络的表示能力和训练动力学。它们不仅塑造了表示的性质，还优化了收敛速度并增强了泛化能力。鉴于这一关键作用，我们提出了线性振动（LoC）激活函数，定义为$f(x) = x \times \sin(\alpha x + \beta)$。与传统的激活函数不同，LoC将线性轨迹与振荡偏差无缝融合。命名为“线性振动”是对其独特属性的致敬，即通过和谐的振动融入线性激活，捕捉“困惑的重要性”的本质。网络激活中的“控制性困惑”概念被认为能够促进更稳健的学习，特别是在需要区分微妙模式的情境中。我们的实证研究表明，当使用LoC激活函数时，网络可以更好地捕捉到隐藏的模式，提高了学习的鲁棒性。

    Activation functions are the linchpins of deep learning, profoundly influencing both the representational capacity and training dynamics of neural networks. They shape not only the nature of representations but also optimize convergence rates and enhance generalization potential. Appreciating this critical role, we present the Linear Oscillation (LoC) activation function, defined as $f(x) = x \times \sin(\alpha x + \beta)$. Distinct from conventional activation functions which primarily introduce non-linearity, LoC seamlessly blends linear trajectories with oscillatory deviations. The nomenclature ``Linear Oscillation'' is a nod to its unique attribute of infusing linear activations with harmonious oscillations, capturing the essence of the 'Importance of Confusion'. This concept of ``controlled confusion'' within network activations is posited to foster more robust learning, particularly in contexts that necessitate discerning subtle patterns. Our empirical studies reveal that, when i
    
[^54]: 使用稀疏逼近随机游走的网络嵌入

    Network Embedding Using Sparse Approximations of Random Walks. (arXiv:2308.13663v1 [cs.LG])

    [http://arxiv.org/abs/2308.13663](http://arxiv.org/abs/2308.13663)

    本文提出了一种使用稀疏逼近随机游走的网络嵌入方法，并通过数据聚类和多标签分类的实验验证了其有效性。

    

    本文提出了一种基于通勤时间的网络嵌入的高效数值实现方法，该方法使用了通过改进的扩散小波算法在网络上获得的稀疏扩散过程的逼近。通过随机梯度下降法和低维表示的绿函数采样，计算节点的嵌入，通过多个示例展示了该方法在数据聚类和多标签分类方面的有效性，并在效率和准确性方面与现有方法进行比较。还讨论了证明该方法的理论问题。

    In this paper, we propose an efficient numerical implementation of Network Embedding based on commute times, using sparse approximation of a diffusion process on the network obtained by a modified version of the diffusion wavelet algorithm. The node embeddings are computed by optimizing the cross entropy loss via the stochastic gradient descent method with sampling of low-dimensional representations of green functions. We demonstrate the efficacy of this method for data clustering and multi-label classification through several examples, and compare its performance over existing methods in terms of efficiency and accuracy. Theoretical issues justifying the scheme are also discussed.
    
[^55]: 异构和资源受限环境中的资源高效联邦学习

    Resource-Efficient Federated Learning for Heterogenous and Resource-Constrained Environments. (arXiv:2308.13662v1 [cs.LG])

    [http://arxiv.org/abs/2308.13662](http://arxiv.org/abs/2308.13662)

    提出了一种新的资源高效联邦学习方法，通过可变剪枝技术和知识蒸馏来解决资源受限设备中的计算和通信挑战，并在保持数据隐私和性能的同时适应异构模型架构。

    

    联邦学习 (FL) 是机器学习中的一种保护隐私的子领域，它将模型带到用户设备进行训练，避免了与中央服务器共享个人数据的需求。虽然现有的工作解决了数据异质性，但忽视了FL中的其他挑战，如设备异质性和通信效率。在本文中，我们提出了RE-FL，这是一种面向资源受限设备的新方法，解决了计算和通信挑战。我们的可变剪枝技术通过根据每个客户端的计算能力进行剪枝优化资源利用。我们还采用知识蒸馏来减少带宽消耗和通信轮数。图像分类任务的实验结果证明了我们的方法在资源受限环境中的有效性，同时满足异构模型架构的数据隐私和性能需求。

    Federated Learning (FL) is a privacy-enforcing sub-domain of machine learning that brings the model to the user's device for training, avoiding the need to share personal data with a central server. While existing works address data heterogeneity, they overlook other challenges in FL, such as device heterogeneity and communication efficiency. In this paper, we propose RE-FL, a novel approach that tackles computational and communication challenges in resource-constrained devices. Our variable pruning technique optimizes resource utilization by adapting pruning to each client's computational capabilities. We also employ knowledge distillation to reduce bandwidth consumption and communication rounds. Experimental results on image classification tasks demonstrate the effectiveness of our approach in resource-constrained environments, maintaining data privacy and performance while accommodating heterogeneous model architectures.
    
[^56]: 超越想象：通过世界模型最大化情节可达性

    Go Beyond Imagination: Maximizing Episodic Reachability with World Models. (arXiv:2308.13661v1 [cs.LG])

    [http://arxiv.org/abs/2308.13661](http://arxiv.org/abs/2308.13661)

    通过引入GoBI（超越想象）内在奖励设计，结合传统的终身新颖性动机和情节内在奖励，利用学习的世界模型最大化了逐步可达扩展，有效解决了强化学习中的奖励稀疏问题，并在迷你网格导航任务和DeepMind控制套件中取得了优越的性能。

    

    在强化学习中，高效探索是一个具有挑战性的问题，尤其是对于稀疏奖励任务。为了应对奖励稀疏性，人们通常会应用内在奖励来激励智能体有效地探索状态空间。在本文中，我们引入了一种新的内在奖励设计，称为GoBI - 超越想象，它将传统的终身新颖性动机与一个情节内在奖励相结合，旨在最大化逐步可达性扩展。具体而言，我们应用学习的世界模型来生成具有随机动作的预测未来状态。那些具有更多独特预测且不在情节记忆中的状态将被分配高内在奖励。我们的方法在12个最具挑战性的迷你网格导航任务中大大超越了先前最先进的方法，并提高了DeepMind Control Suite中运动任务的样本效率。

    Efficient exploration is a challenging topic in reinforcement learning, especially for sparse reward tasks. To deal with the reward sparsity, people commonly apply intrinsic rewards to motivate agents to explore the state space efficiently. In this paper, we introduce a new intrinsic reward design called GoBI - Go Beyond Imagination, which combines the traditional lifelong novelty motivation with an episodic intrinsic reward that is designed to maximize the stepwise reachability expansion. More specifically, we apply learned world models to generate predicted future states with random actions. States with more unique predictions that are not in episodic memory are assigned high intrinsic rewards. Our method greatly outperforms previous state-of-the-art methods on 12 of the most challenging Minigrid navigation tasks and improves the sample efficiency on locomotion tasks from DeepMind Control Suite.
    
[^57]: 相对准确模型中近似解何时优于近似模型——一个创新性的研究

    Pretty darn good control: when are approximate solutions better than approximate models. (arXiv:2308.13654v1 [cs.LG])

    [http://arxiv.org/abs/2308.13654](http://arxiv.org/abs/2308.13654)

    本研究探讨了在实际系统中，当近似、简化的模型的最优解比更准确模型的近似解更好时的情况，并通过深度强化学习算法使用深度神经网络成功逼近解决了这一问题。

    

    现有的优化控制方法往往难以处理实际系统中遇到的复杂性，包括维度、过程误差、模型偏差和数据异质性。研究人员通常倾向于简化模型来适应优化控制方法，而不是直接解决系统复杂性。然而，当近似、简化的模型的最优解比更准确模型的近似解更好时，我们又该如何选择呢？虽然这个问题一直没有得到解决，因为找到复杂模型的近似解非常困难，但是最近深度强化学习（DRL）在算法和计算方面的进展或许可以帮助我们回答这些问题。目前，DRL方法主要应用于游戏或机器人力学等精确已知规则的领域。在本文中，我们展示了使用深度神经网络的DRL算法成功逼近解决问题的能力。

    Existing methods for optimal control struggle to deal with the complexity commonly encountered in real-world systems, including dimensionality, process error, model bias and data heterogeneity. Instead of tackling these system complexities directly, researchers have typically sought to simplify models to fit optimal control methods. But when is the optimal solution to an approximate, stylized model better than an approximate solution to a more accurate model? While this question has largely gone unanswered owing to the difficulty of finding even approximate solutions for complex models, recent algorithmic and computational advances in deep reinforcement learning (DRL) might finally allow us to address these questions. DRL methods have to date been applied primarily in the context of games or robotic mechanics, which operate under precisely known rules. Here, we demonstrate the ability for DRL algorithms using deep neural networks to successfully approximate solutions (the "policy funct
    
[^58]: GRASP: 一种高效的在线渐进式学习的重演策略

    GRASP: A Rehearsal Policy for Efficient Online Continual Learning. (arXiv:2308.13646v1 [cs.LG])

    [http://arxiv.org/abs/2308.13646](http://arxiv.org/abs/2308.13646)

    GRASP是一种新的样本选择策略，根据样本的代表性选择最适合学习的样本，从而提高了在线渐进式学习的效率。

    

    深度神经网络中的渐进学习涉及从不断增长的数据流中逐步累积知识。渐进学习的一个主要挑战是非平稳的数据流会导致之前学到的能力遭受灾难性遗忘。重演是一种常用且有效的缓解这个问题的方法，即将过去的观测结果存储在缓冲区中，并在学习过程中将它们与新的观测结果混合。这带来了一个问题：应该选择哪些存储样本进行重演？选择最适合学习的样本而不是随机选择样本，可能会导致学习速度显著加快。对于类增量学习，先前的研究表明简单的类均衡随机选择策略优于更复杂的方法。在这里，我们通过探索一种新的样本选择策略GRASP重新思考这个问题。GRASP首先选择最具代表性的样本，然后逐渐选择较不具代表性的样本。

    Continual learning (CL) in deep neural networks (DNNs) involves incrementally accumulating knowledge in a DNN from a growing data stream. A major challenge in CL is that non-stationary data streams cause catastrophic forgetting of previously learned abilities. Rehearsal is a popular and effective way to mitigate this problem, which is storing past observations in a buffer and mixing them with new observations during learning. This leads to a question: Which stored samples should be selected for rehearsal? Choosing samples that are best for learning, rather than simply selecting them at random, could lead to significantly faster learning. For class incremental learning, prior work has shown that a simple class balanced random selection policy outperforms more sophisticated methods. Here, we revisit this question by exploring a new sample selection policy called GRASP. GRASP selects the most prototypical (class representative) samples first and then gradually selects less prototypical (h
    
[^59]: Arbiter PUF对快速和慢速建模攻击的主动学习

    Active learning for fast and slow modeling attacks on Arbiter PUFs. (arXiv:2308.13645v1 [cs.CR])

    [http://arxiv.org/abs/2308.13645](http://arxiv.org/abs/2308.13645)

    本论文研究了在物理不可克隆函数中的主动学习对于建模攻击的作用，使用率阀值函数提出了一种新的挑战选择方法，通过主动学习实现“快速”学习可以提高准确性，并有效地学习制造的PUF。

    

    建模攻击是指对基于硬件的物理不可克隆函数 (PUF) 进行建模的攻击，这对这些硬件安全原语的可行性构成了巨大威胁。在大多数建模攻击中，一部分随机的挑战-响应对 (CRP) 被用作机器学习算法的标记数据。在这里，针对阀值函数带来的 aribiter-PUF 进行了研究，该阈值函数由于制造缺陷而具有随机权重。我们研究了在支持向量机 (SVM) 学习中主动学习的作用。我们关注的是通过挑战选择来帮助 SVM 算法实现“快速”学习和“慢速”学习。我们的方法是构建挑战，而不是像先前的工作中依赖样本池的挑战。使用主动学习实现“快速”学习（揭示更少的 CRPs，得到更高的准确性）可能有助于制造商更高效地学习制造的 PUF，或者在攻击者可以查询并收集较少的CRPs信息时形成更强大的攻击。

    Modeling attacks, in which an adversary uses machine learning techniques to model a hardware-based Physically Unclonable Function (PUF) pose a great threat to the viability of these hardware security primitives. In most modeling attacks, a random subset of challenge-response-pairs (CRPs) are used as the labeled data for the machine learning algorithm. Here, for the arbiter-PUF, a delay based PUF which may be viewed as a linear threshold function with random weights (due to manufacturing imperfections), we investigate the role of active learning in Support Vector Machine (SVM) learning. We focus on challenge selection to help SVM algorithm learn ``fast'' and learn ``slow''. Our methods construct challenges rather than relying on a sample pool of challenges as in prior work. Using active learning to learn ``fast'' (less CRPs revealed, higher accuracies) may help manufacturers learn the manufactured PUFs more efficiently, or may form a more powerful attack when the attacker may query the 
    
[^60]: 通过机器学习进行索引调优：最新进展与开放挑战的概述

    ML-Powered Index Tuning: An Overview of Recent Progress and Open Challenges. (arXiv:2308.13641v1 [cs.DB])

    [http://arxiv.org/abs/2308.13641](http://arxiv.org/abs/2308.13641)

    本文总结了现代云服务中自动索引调优面临的主要挑战，以及机器学习技术在解决这些挑战方面的最新进展。该研究主要关注工作负载选择、候选索引过滤、加速索引配置搜索、减少查询优化器调用的数量和降低性能回归机会等方面，并提出创新的解决方案。

    

    现代云服务中工作负载的规模和复杂性使得自动索引调优面临着一个关键挑战——在保持索引调优可扩展性的同时推荐高质量的索引。这一挑战进一步受到自动索引实现在生产环境中引入最小查询性能回归的要求的影响，这构成了实现可扩展性和全自动化的重要障碍。本文关注自动索引调优中的这些挑战，并探讨机器学习技术在缓解这些挑战方面提供的新机遇。具体来说，我们回顾了在工作负载选择、候选索引过滤、加速索引配置搜索、减少查询优化器调用的数量以及降低性能回归机会方面开展的最新工作。我们强调这些工作的关键要点，并强调其创新与贡献。

    The scale and complexity of workloads in modern cloud services have brought into sharper focus a critical challenge in automated index tuning -- the need to recommend high-quality indexes while maintaining index tuning scalability. This challenge is further compounded by the requirement for automated index implementations to introduce minimal query performance regressions in production deployments, representing a significant barrier to achieving scalability and full automation. This paper directs attention to these challenges within automated index tuning and explores ways in which machine learning (ML) techniques provide new opportunities in their mitigation. In particular, we reflect on recent efforts in developing ML techniques for workload selection, candidate index filtering, speeding up index configuration search, reducing the amount of query optimizer calls, and lowering the chances of performance regressions. We highlight the key takeaways from these efforts and underline the g
    
[^61]: 自适应白化：快速增益调制和慢速突触可塑性

    Adaptive whitening with fast gain modulation and slow synaptic plasticity. (arXiv:2308.13633v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.13633](http://arxiv.org/abs/2308.13633)

    本研究提出了一个多时间尺度的自适应白化机制模型，使用快速增益调制和慢速突触可塑性相结合的方式来适应变化的感觉统计信息。

    

    早期感觉区的神经元能够迅速适应变化的感觉统计信息，通过对其个体响应的方差进行归一化以及减少响应之间的相关性。这些转换可以被视为一种自适应的白化过程。现有的自适应白化的机制模型只使用突触可塑性或增益调制作为适应的生物基质，然而，每个模型都有显著的局限性。在这项工作中，我们将这些方法统一起来，提出了一个规范性的多时间尺度机制模型，通过突触可塑性和增益调制的计算角色来自适应地进行白化。增益在快速时间尺度上根据当前的统计情况进行调整，而突触在慢速时间尺度上进行调整，学习输入统计中与情境无关的结构特性。我们的模型来自于一种新颖的多时间尺度

    Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to learn structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale
    
[^62]: RIS-启用的毫米波无线系统中的信道估计：一种变分推断方法

    Channel Estimation in RIS-Enabled mmWave Wireless Systems: A Variational Inference Approach. (arXiv:2308.13616v1 [eess.SP])

    [http://arxiv.org/abs/2308.13616](http://arxiv.org/abs/2308.13616)

    本论文提出了一种变分推断方法来估计RIS-启用的毫米波无线系统中的信道状态信息，通过联合估计用户设备到RIS和RIS到基站的通道，减少了信号复杂性和瞬时信道的高信令开销。

    

    我们在完全被动的可重构智能表面(RIS)辅助的毫米波单用户单输入多输出(SIMO)通信系统中提出了一种基于变分推断(VI)的信道状态信息(CSI)估计方法。具体而言，我们首先提出了一种基于VI的联合信道估计方法，使用上行训练信号来估计用户设备(UE)到RIS(UE-RIS)和RIS到基站(RIS-BS)信道在被动RIS设置中。然而，基于瞬时CSI(I-CSI)更新相移会导致高信令开销，特别是由于UE-RIS信道的短相干块。因此，为了减少信令复杂性，我们提出了一种基于VI的方法，用于估计RIS-BS信道以及对于比瞬时UE-RIS信道更长时间保持准静态的UE-RIS信道的协方差矩阵。在VI框架下，我们使用方便的分布函数来近似信道增益/协方差矩阵的后验分布。

    We propose a variational inference (VI)-based channel state information (CSI) estimation approach in a fully-passive reconfigurable intelligent surface (RIS)-aided mmWave single-user single-input multiple-output (SIMO) communication system. Specifically, we first propose a VI-based joint channel estimation method to estimate the user-equipment (UE) to RIS (UE-RIS) and RIS to base station (RIS-BS) channels using uplink training signals in a passive RIS setup. However, updating the phase-shifts based on the instantaneous CSI (I-CSI) leads to a high signaling overhead especially due to the short coherence block of the UE-RIS channel. Therefore, to reduce the signaling complexity, we propose a VI-based method to estimate the RIS-BS channel along with the covariance matrix of the UE-RIS channel that remains quasi-static for a longer period than the instantaneous UE-RIS channel. In the VI framework, we approximate the posterior of the channel gains/covariance matrix with convenient distribut
    
[^63]: 甲状腺癌诊断中的人工智能: 技术、趋势和未来方向

    AI in Thyroid Cancer Diagnosis: Techniques, Trends, and Future Directions. (arXiv:2308.13592v1 [eess.IV])

    [http://arxiv.org/abs/2308.13592](http://arxiv.org/abs/2308.13592)

    本论文总结了在甲状腺癌诊断中使用人工智能（AI）技术的相关研究。通过提出新的分类方法并比较现有数据集的特征，研究重点在于如何通过AI工具支持甲状腺癌的诊断和治疗。

    

    在创建智能诊断系统以协助医学专业人员分析和处理治疗无法治愈疾病的大数据方面，引起了越来越多的关注。在这一领域的一个关键挑战是检测甲状腺癌，在使用机器学习（ML）和大数据分析进行甲状腺癌预后评估和确定患者恶性风险方面已经取得了进展。本综述论文总结了与在甲状腺癌诊断中使用人工智能（AI）技术相关的大量文章。相应地，引入了一个新的分类方法，根据所使用的AI算法、框架的目的和计算平台对这些技术进行分类。此外，本研究还根据其特征比较了现有的甲状腺癌数据集。本研究的重点是通过有监督、无监督或混合的AI工具如何支持甲状腺癌的诊断和治疗。

    There has been a growing interest in creating intelligent diagnostic systems to assist medical professionals in analyzing and processing big data for the treatment of incurable diseases. One of the key challenges in this field is detecting thyroid cancer, where advancements have been made using machine learning (ML) and big data analytics to evaluate thyroid cancer prognosis and determine a patient's risk of malignancy. This review paper summarizes a large collection of articles related to artificial intelligence (AI)-based techniques used in the diagnosis of thyroid cancer. Accordingly, a new classification was introduced to classify these techniques based on the AI algorithms used, the purpose of the framework, and the computing platforms used. Additionally, this study compares existing thyroid cancer datasets based on their features. The focus of this study is on how AI-based tools can support the diagnosis and treatment of thyroid cancer, through supervised, unsupervised, or hybrid
    
[^64]: GeoExplainer: 一种用于空间建模情境化和报告生成的可视化分析框架

    GeoExplainer: A Visual Analytics Framework for Spatial Modeling Contextualization and Report Generation. (arXiv:2308.13588v1 [cs.HC])

    [http://arxiv.org/abs/2308.13588](http://arxiv.org/abs/2308.13588)

    GeoExplainer是一种可视化分析框架，旨在支持分析人员创建总结和情境化空间分析的解释性文档。

    

    地理回归模型通常用于识别空间分布观测结果中的模式和异常。这种类型的分析旨在回答关于底层空间现象的问题，例如为什么犯罪率在这个地区较高，为什么一个学区的学生表现优于另一个学区的学生等。这些问题的答案需要对模型结构、参数选择的解释，并且需要将研究结果与地理背景进行情境化。本文介绍了GeoExplainer，一种可视化分析框架，旨在帮助分析人员创建能够总结和情境化空间分析的解释性文档。在分析人员创建空间模型时，我们的框架会标记模型参数选择中的潜在问题。

    Geographic regression models of various descriptions are often applied to identify patterns and anomalies in the determinants of spatially distributed observations. These types of analyses focus on answering why questions about underlying spatial phenomena, e.g., why is crime higher in this locale, why do children in one school district outperform those in another, etc.? Answers to these questions require explanations of the model structure, the choice of parameters, and contextualization of the findings with respect to their geographic context. This is particularly true for local forms of regression models which are focused on the role of locational context in determining human behavior. In this paper, we present GeoExplainer, a visual analytics framework designed to support analysts in creating explanative documentation that summarizes and contextualizes their spatial analyses. As analysts create their spatial models, our framework flags potential issues with model parameter selectio
    
[^65]: 使用大型语言模型进行文本风格转换评估

    Text Style Transfer Evaluation Using Large Language Models. (arXiv:2308.13577v1 [cs.CL])

    [http://arxiv.org/abs/2308.13577](http://arxiv.org/abs/2308.13577)

    大型语言模型（LLMs）有潜力成为人工评估和其他自动化评价指标的可行替代方案。

    

    文本风格转换（TST）的评估具有挑战性，因为生成文本的质量表现在多个方面，每个方面都很难单独衡量：风格转换准确性、内容保留和整体流畅性。人工评估是TST评估的黄金标准，然而，它费时费力，并且结果难以重复。许多自动化指标被用于评估这些方面的性能，作为人工评估的替代品。然而，许多自动化指标与人工评估之间的相关性仍然不清楚，对它们作为可靠基准的效果产生了怀疑。最近大型语言模型（LLMs）的进展已经证明了它们不仅能够匹配，而且在各种未见任务中还能超过平均人类表现。这表明LLMs有潜力成为人工评估和其他自动化指标的可行替代方案。我们评估了...

    Text Style Transfer (TST) is challenging to evaluate because the quality of the generated text manifests itself in multiple aspects, each of which is hard to measure individually: style transfer accuracy, content preservation, and overall fluency of the text. Human evaluation is the gold standard in TST evaluation; however, it is expensive, and the results are difficult to reproduce. Numerous automated metrics are employed to assess performance in these aspects, serving as substitutes for human evaluation. However, the correlation between many of these automated metrics and human evaluations remains unclear, raising doubts about their effectiveness as reliable benchmarks. Recent advancements in Large Language Models (LLMs) have demonstrated their ability to not only match but also surpass the average human performance across a wide range of unseen tasks. This suggests that LLMs have the potential to serve as a viable alternative to human evaluation and other automated metrics. We asses
    
[^66]: 工业人工智能中的随机配置机

    Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])

    [http://arxiv.org/abs/2308.13570](http://arxiv.org/abs/2308.13570)

    本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。

    

    在工业人工智能（IAI）中，需要实时、准确的预测建模，神经网络在其中起到关键作用。工业人工智能中的神经网络需要强大的高性能计算设备来处理大量的浮点数据。本文基于随机配置网络（SCNs），提出了一种新的随机学习器模型，称为随机配置机（SCMs），以强调对于工业应用非常有用和有价值的有效建模和节约数据大小。与具有二值化实现的随机向量功能链接（RVFL）网络相比，SCMs的模型存储可以显著压缩，同时保持有利的预测性能。除了SCM学习器模型的架构和学习算法，作为本文的重要部分，我们还通过分析模型的复杂性提供了SCMs的学习能力的理论基础。实验研究也进行了。

    Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
    
[^67]: 用主题建模发现心理健康研究课题

    Discovering Mental Health Research Topics with Topic Modeling. (arXiv:2308.13569v1 [cs.CL])

    [http://arxiv.org/abs/2308.13569](http://arxiv.org/abs/2308.13569)

    本研究通过分析大量心理健康研究论文，采用自定义嵌入模型，识别出该领域的一般趋势和高影响力的研究课题。

    

    心理健康显著影响我们日常生活的各个方面，其重要性在研究界和大众中越来越受到认可，特别是在COVID-19大流行之后。在过去的十年中，心理健康领域的出版物数量不断增长，这种浓厚的兴趣也体现在越来越多的研究论文中。本研究的目标是通过分析大型心理健康研究论文数据集，识别该领域的一般趋势，并找出具有高影响力的研究课题。为了实现这一目标，我们从各个数据库收集了摘要，并使用基于BERTopic框架的自定义Sentence-BERT嵌入模型进行训练。我们的数据集包含96,676篇与心理健康相关的研究论文，使我们能够通过它们的摘要来研究不同主题之间的关系。为了评估模型的效果，我们将其与另外两种最先进的方法进行了对比：Top2Vec模型和LDA-BERT模型。

    Mental health significantly influences various aspects of our daily lives, and its importance has been increasingly recognized by the research community and the general public, particularly in the wake of the COVID-19 pandemic. This heightened interest is evident in the growing number of publications dedicated to mental health in the past decade. In this study, our goal is to identify general trends in the field and pinpoint high-impact research topics by analyzing a large dataset of mental health research papers. To accomplish this, we collected abstracts from various databases and trained a customized Sentence-BERT based embedding model leveraging the BERTopic framework. Our dataset comprises 96,676 research papers pertaining to mental health, enabling us to examine the relationships between different topics using their abstracts. To evaluate the effectiveness of the model, we compared it against two other state-of-the-art methods: Top2Vec model and LDA-BERT model. The model demonstr
    
[^68]: 区域解耦扩散模型用于高保真度PPG到ECG的转换

    Region-Disentangled Diffusion Model for High-Fidelity PPG-to-ECG Translation. (arXiv:2308.13568v1 [eess.SP])

    [http://arxiv.org/abs/2308.13568](http://arxiv.org/abs/2308.13568)

    这项工作引入了区域解耦扩散模型 (RDDM)，用于将PPG转换为ECG信号。RDDM通过选择性地向感兴趣区域添加噪声来捕捉ECG信号的复杂时间动态。

    

    心血管疾病的高发率需要便捷且经济有效的连续心脏监测工具。尽管心电图（ECG）是黄金标准，但连续监测仍然是一个挑战，因此人们开始探索光电容容积脉搏图（PPG），这是一种更基本的可用于消费者可穿戴设备的替代方案。这种想法最近引起了将PPG转化为ECG信号的兴趣。在这项工作中，我们引入了区域解耦扩散模型（RDDM），这是一种设计用于捕捉ECG复杂时间动态的新颖扩散模型。传统的扩散模型如去噪扩散概率模型（DDPM）在捕捉这种细微差别时面临困难，因为整个信号上的噪声添加过程是不加选择的。我们提出的RDDM通过将噪声有选择地添加到感兴趣区域（ROI）（如ECG信号中的QRS复合体）的新颖正向过程和反向过程来克服此类限制。

    The high prevalence of cardiovascular diseases (CVDs) calls for accessible and cost-effective continuous cardiac monitoring tools. Despite Electrocardiography (ECG) being the gold standard, continuous monitoring remains a challenge, leading to the exploration of Photoplethysmography (PPG), a promising but more basic alternative available in consumer wearables. This notion has recently spurred interest in translating PPG to ECG signals. In this work, we introduce Region-Disentangled Diffusion Model (RDDM), a novel diffusion model designed to capture the complex temporal dynamics of ECG. Traditional Diffusion models like Denoising Diffusion Probabilistic Models (DDPM) face challenges in capturing such nuances due to the indiscriminate noise addition process across the entire signal. Our proposed RDDM overcomes such limitations by incorporating a novel forward process that selectively adds noise to specific regions of interest (ROI) such as QRS complex in ECG signals, and a reverse proces
    
[^69]: MLLM-DataEngine：一种MLLM的迭代改进方法

    MLLM-DataEngine: An Iterative Refinement Approach for MLLM. (arXiv:2308.13566v1 [cs.LG])

    [http://arxiv.org/abs/2308.13566](http://arxiv.org/abs/2308.13566)

    本文提出了一种名为MLLM-DataEngine的迭代改进方法，它通过分析模型弱点，生成适当的增量数据集并迭代地增强模型能力。与以往方法相比，MLLM-DataEngine生成的数据在定位、质量和正确性方面表现更好。

    

    尽管在指导数据集构建和基准测试方面，多模态大型语言模型（MLLM）取得了很大的进展，但训练和评估的独立性使得当前的MLLM很难在相对较低的人力成本下进一步提高其能力。本文提出了一种新颖的封闭循环系统MLLM-DataEngine，它连接了数据生成、模型训练和评估。在每个循环迭代中，MLLM-DataEngine首先根据评估结果分析模型的弱点，然后生成合适的增量数据集用于下一次训练迭代，并迭代地增强模型的能力。与先前与基准测试分离的数据收集方法相比，MLLM-DataEngine生成的数据在定位、质量和正确性方面都表现得更好。

    Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental datas
    
[^70]: SGMM: 广义矩方法的随机近似

    SGMM: Stochastic Approximation to Generalized Method of Moments. (arXiv:2308.13564v1 [econ.EM])

    [http://arxiv.org/abs/2308.13564](http://arxiv.org/abs/2308.13564)

    我们提出了一种新的随机广义矩方法（SGMM），用于估计和推断矩限制模型。该方法具有快速和可扩展的实时处理能力，并且能够处理大规模和在线数据集。

    

    我们引入了一种新的算法类，随机广义矩方法（SGMM），用于估计和推断（超识别）矩限制模型。我们的SGMM是一种新颖的随机逼近方法，替代了流行的Hansen（1982年）的（离线）GMM，并提供了快速和可扩展的实时流数据处理能力。我们证明了SGMM对于效率不高的在线2SLS和高效的SGMM具有几乎确定的收敛性和（函数）中心极限定理。此外，我们提出了Durbin-Wu-Hausman和Sargan-Hansen测试的在线版本，可以无缝集成到SGMM框架中。广泛的蒙特卡洛模拟结果表明，随着样本量的增加，SGMM在估计准确性和计算效率方面与标准（离线）GMM相匹配，并显示出在大规模和在线数据集上的实际价值。我们通过使用两个示例证明了我们方法的有效性。

    We introduce a new class of algorithms, Stochastic Generalized Method of Moments (SGMM), for estimation and inference on (overidentified) moment restriction models. Our SGMM is a novel stochastic approximation alternative to the popular Hansen (1982) (offline) GMM, and offers fast and scalable implementation with the ability to handle streaming datasets in real time. We establish the almost sure convergence, and the (functional) central limit theorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we propose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that can be seamlessly integrated within the SGMM framework. Extensive Monte Carlo simulations show that as the sample size increases, the SGMM matches the standard (offline) GMM in terms of estimation accuracy and gains over computational efficiency, indicating its practical value for both large-scale and online datasets. We demonstrate the efficacy of our approach by a proof of concept using two we
    
[^71]: 大型语言模型在分析事故叙述中的应用——ChatGPT、BARD和GPT-4的比较研究

    Large Language Models in Analyzing Crash Narratives -- A Comparative Study of ChatGPT, BARD and GPT-4. (arXiv:2308.13563v1 [cs.CL])

    [http://arxiv.org/abs/2308.13563](http://arxiv.org/abs/2308.13563)

    三个大型语言模型接口(ChatGPT, BARD和GPT4)在分析事故叙述中的效果进行了比较研究。研究结果表明，它们在提取事故相关信息和回答相关问题方面都具有一定的有效性，但也存在一些限制。

    

    在交通安全研究中，使用文本分析从事故叙述中提取信息是一种常见的做法。随着大型语言模型（LLM）的最新进展，了解流行的LLM接口在分类或从事故叙述中提取信息方面的表现将非常有用。为了探索这一问题，我们的研究使用了目前最流行的三个公开可用的LLM接口——ChatGPT、BARD和GPT4。本研究调查了它们在提取信息和回答与事故有关的查询方面的有效性和限制。研究从爱荷华州和堪萨斯州的100个事故叙述中提取信息，并对它们的能力和限制进行了评估，比较了它们对查询的响应。五个与叙述相关的问题被提出：1）谁是责任方？2）碰撞方式是什么？3）事故发生在工作区吗？4）事故涉及行人吗？5）事故中有害事件的顺序是什么？对于第1到第4个问题，三个LLM接口的回答都经过了比较。

    In traffic safety research, extracting information from crash narratives using text analysis is a common practice. With recent advancements of large language models (LLM), it would be useful to know how the popular LLM interfaces perform in classifying or extracting information from crash narratives. To explore this, our study has used the three most popular publicly available LLM interfaces- ChatGPT, BARD and GPT4. This study investigated their usefulness and boundaries in extracting information and answering queries related to accidents from 100 crash narratives from Iowa and Kansas. During the investigation, their capabilities and limitations were assessed and their responses to the queries were compared. Five questions were asked related to the narratives: 1) Who is at-fault? 2) What is the manner of collision? 3) Has the crash occurred in a work-zone? 4) Did the crash involve pedestrians? and 5) What are the sequence of harmful events in the crash? For questions 1 through 4, the o
    
[^72]: 机器去学习用于因果推断的方法

    Machine Unlearning for Causal Inference. (arXiv:2308.13559v1 [cs.LG])

    [http://arxiv.org/abs/2308.13559](http://arxiv.org/abs/2308.13559)

    本文介绍了一种机器去学习的方法，用于因果推断中的倾向得分匹配和处理效应估计。研究使用Lalonde数据集，通过神经网络模型进行机器去学习以提高因果分析的性能。

    

    机器学习模型在预测和从数据中得出洞察力方面起着关键作用，并且越来越多地被用于因果推断。为了保护用户隐私，使模型能够忘记有关给定用户的一些学习/捕捉到的信息是很重要的（机器去学习）。本文介绍了用于因果推断的机器去学习的概念，特别是倾向得分匹配和处理效应估计，旨在在满足上述去学习要求的前提下，改进和提高机器学习模型进行因果分析的性能。本文提出了一种基于神经网络的倾向得分模型的机器去学习方法。研究中使用的数据集是Lalonde数据集，这是一个广泛用于评估职业培训计划的有效性（即处理效应）的数据集。该方法涉及在原始数据集上训练初始倾向得分模型，然后通过选择性地创建遗忘集来进行机器去学习。

    Machine learning models play a vital role in making predictions and deriving insights from data and are being increasingly used for causal inference. To preserve user privacy, it is important to enable the model to forget some of its learning/captured information about a given user (machine unlearning). This paper introduces the concept of machine unlearning for causal inference, particularly propensity score matching and treatment effect estimation, which aims to refine and improve the performance of machine learning models for causal analysis given the above unlearning requirements. The paper presents a methodology for machine unlearning using a neural network-based propensity score model. The dataset used in the study is the Lalonde dataset, a widely used dataset for evaluating the effectiveness i.e. the treatment effect of job training programs. The methodology involves training an initial propensity score model on the original dataset and then creating forget sets by selectively r
    
[^73]: 《对GAN增强数据中的偏差进行定量化研究的系统性研究》的翻译

    A Systematic Study on Quantifying Bias in GAN-Augmented Data. (arXiv:2308.13554v1 [cs.LG])

    [http://arxiv.org/abs/2308.13554](http://arxiv.org/abs/2308.13554)

    该研究系统地研究了互动式对抗生成网络（GAN）在数据扩充中引入的偏见，以及用于度量偏见加重程度的度量标准的评估。研究结果表明，虽然有多种度量方法可用，但没有一种单一方法可以可靠地度量不同图像领域中的偏见加重。

    

    生成对抗网络（GANs）最近已成为机器学习实践者使用的流行的数据增强技术。然而，研究表明它们存在所谓的模式崩溃故障模式，使其容易加剧已经偏斜数据集上的偏见，导致生成的数据分布比训练分布更不多样化。为此，我们解决了定量化模式崩溃程度的问题。这项研究是一项系统性的工作，重点评估可能对GAN增强数据中的偏见定量化的最新度量方法。我们发现，虽然有几种方法可用，但没有单一的度量方法可以可靠地定量化不同图像领域的偏见加剧。

    Generative adversarial networks (GANs) have recently become a popular data augmentation technique used by machine learning practitioners. However, they have been shown to suffer from the so-called mode collapse failure mode, which makes them vulnerable to exacerbating biases on already skewed datasets, resulting in the generated data distribution being less diverse than the training distribution. To this end, we address the problem of quantifying the extent to which mode collapse occurs. This study is a systematic effort focused on the evaluation of state-of-the-art metrics that can potentially quantify biases in GAN-augmented data. We show that, while several such methods are available, there is no single metric that quantifies bias exacerbation reliably over the span of different image domains.
    
[^74]: 结合自动编码和教师输入以生成用于异步在线讨论的ENA可视化

    Combining Automatic Coding and Instructor Input to Generate ENA Visualizations for Asynchronous Online Discussion. (arXiv:2308.13549v1 [cs.HC])

    [http://arxiv.org/abs/2308.13549](http://arxiv.org/abs/2308.13549)

    本研究提出了一种结合自动编码和教师输入的方法，用于生成用于异步在线讨论的ENA可视化。实验结果显示生成的ENA模型与人工编码员构建的模型没有统计学差异，这表明ENA可作为一种帮助教师评估异步在线讨论的有效可视化工具。

    

    异步在线讨论是促进混合和在线课程中社交互动的常见基础工具。然而，教师缺乏工具来完成评估异步在线讨论活动的繁重任务。在本文中，我们提出了一种方法，利用潜在狄利克雷分析（LDA）和教师的关键词从相对较小的数据集中自动提取代码。我们使用生成的代码构建了一个Epistemic Network Analysis（ENA）模型，并将该模型与人工编码员构建的之前的ENA模型进行了比较。结果显示两个模型之间没有统计学差异。我们对这些模型进行了分析，并讨论了将ENA作为可视化工具帮助教师评估异步在线讨论的潜在用途。

    Asynchronous online discussions are a common fundamental tool to facilitate social interaction in hybrid and online courses. However, instructors lack the tools to accomplish the overwhelming task of evaluating asynchronous online discussion activities. In this paper we present an approach that uses Latent Dirichlet Analysis (LDA) and the instructor's keywords to automatically extract codes from a relatively small dataset. We use the generated codes to build an Epistemic Network Analysis (ENA) model and compare this model with a previous ENA model built by human coders. The results show that there is no statistical difference between the two models. We present an analysis of these models and discuss the potential use of ENA as a visualization to help instructors evaluating asynchronous online discussions.
    
[^75]: Hyperscanning EEG的功能性图对比学习揭示了基于刻板印象的压力引发的情绪传染

    Functional Graph Contrastive Learning of Hyperscanning EEG Reveals Emotional Contagion Evoked by Stereotype-Based Stressors. (arXiv:2308.13546v1 [eess.SP])

    [http://arxiv.org/abs/2308.13546](http://arxiv.org/abs/2308.13546)

    本研究通过利用超扫描技术，引入功能性图对比学习方法探究基于刻板印象的压力引发的情绪传染。研究结果揭示了情绪传染与认知功能之间的复杂相互作用。

    

    本研究深入探讨情绪传染的细微差异及其对双人互动中表现的影响。具体而言，研究聚焦于女性对的合作解决问题任务中基于刻板印象的压力背景。通过对情绪传染的研究，旨在揭示其潜在机制和影响。利用基于EEG的超扫描技术，本研究引入了一种名为功能性图对比学习（fGCL）的创新方法，提取主体不变的神经活动模式表示。这些表示进一步应用动态图分类（DGC）模型进行分析，旨在剖析情绪传染的过程。通过对脑部同步和连接性的研究，揭示了情绪传染与认知功能之间的复杂相互作用。结果强调情绪传染在塑造轨迹中的重要作用。

    This study delves into the intricacies of emotional contagion and its impact on performance within dyadic interactions. Specifically, it focuses on the context of stereotype-based stress (SBS) during collaborative problem-solving tasks among female pairs. Through an exploration of emotional contagion, the research seeks to unveil its underlying mechanisms and effects. Leveraging EEG-based hyperscanning technology, the study introduces an innovative approach known as functional Graph Contrastive Learning (fGCL), which extracts subject-invariant representations of neural activity patterns. These representations are further subjected to analysis using the Dynamic Graph Classification (DGC) model, aimed at dissecting the process of emotional contagion. By scrutinizing brain synchronization and connectivity, the study reveals the intricate interplay between emotional contagion and cognitive functioning. The results underscore the substantial role of emotional contagion in shaping the trajec
    
[^76]: 使用深度生成模型进行孟加拉文本分类的特征提取（arXiv:2308.13545v1 [cs.IR]）

    Feature Extraction Using Deep Generative Models for Bangla Text Classification on a New Comprehensive Dataset. (arXiv:2308.13545v1 [cs.IR])

    [http://arxiv.org/abs/2308.13545](http://arxiv.org/abs/2308.13545)

    本研究提出了使用深度生成模型在孟加拉文本分类中进行特征提取的方法，并收集、注释了一个全面的数据集。评估结果表明，对抗自编码器模型产生了最佳的特征空间。

    

    文本分类中的特征选择是文本挖掘和信息检索中的基础任务。尽管孟加拉语是世界上使用最广泛的第六大语言，但由于文本数据集的稀缺性，它一直受到较少关注。在本研究中，我们收集、注释和准备了一个包含212,184个孟加拉文档的全面数据集，涵盖了七个不同的类别，并将其公开。我们实现了三个深度学习生成模型：LSTM变分自编码器（LSTM VAE）、辅助分类器生成对抗网络（AC-GAN）和对抗自编码器（AAE）来提取文本特征，尽管它们的应用最初是在计算机视觉领域发现的。我们利用我们的数据集训练了这三个模型，并在文档分类任务中使用了得到的特征空间。我们评估了分类器的性能，并发现对抗自编码器模型产生了最佳的特征空间。

    The selection of features for text classification is a fundamental task in text mining and information retrieval. Despite being the sixth most widely spoken language in the world, Bangla has received little attention due to the scarcity of text datasets. In this research, we collected, annotated, and prepared a comprehensive dataset of 212,184 Bangla documents in seven different categories and made it publicly accessible. We implemented three deep learning generative models: LSTM variational autoencoder (LSTM VAE), auxiliary classifier generative adversarial network (AC-GAN), and adversarial autoencoder (AAE) to extract text features, although their applications are initially found in the field of computer vision. We utilized our dataset to train these three models and used the feature space obtained in the document classification task. We evaluated the performance of the classifiers and found that the adversarial autoencoder model produced the best feature space.
    
[^77]: 地理定位数据对增强现实可用性的影响：一项比较用户测试

    Impact of geolocation data on augmented reality usability: A comparative user test. (arXiv:2308.13544v1 [cs.HC])

    [http://arxiv.org/abs/2308.13544](http://arxiv.org/abs/2308.13544)

    本研究通过一项比较用户测试评估了地理定位数据对增强现实可用性的影响，发现与控制组相比，使用RTK数据的实验组的地理定位数据平均准确性更低，可用性评分较低。

    

    摘要。虽然基于位置的增强现实（AR）在教育中的使用已经证明对参与者的动机、参与度和身体活动有益处，但地理定位数据的不准确性导致增强对象抖动或漂移，这是降低用户体验的一个因素。我们开发了一个免费开源的网络AR应用，并进行了一项比较用户测试（n = 54），以评估地理定位数据对可用性、探索和关注的影响。控制组使用了系统与内嵌的GNSS数据结合，探索自然界的生物多样性，实验组使用了外部模块进行RTK数据。在测试过程中，记录了眼动数据、地理定位轨迹和应用内用户触发的事件。参与者回答了可用性问卷（SUS、UEQ、HARUS）。我们发现，实验组暴露的RTK组地理定位数据的平均准确性低于控制组。实验组报告的可用性得分较低。

    Abstract. While the use of location-based augmented reality (AR) for education has demonstrated benefits on participants' motivation, engagement, and on their physical activity, geolocation data inaccuracy causes augmented objects to jitter or drift, which is a factor in downgrading user experience. We developed a free and open source web AR application and conducted a comparative user test (n = 54) in order to assess the impact of geolocation data on usability, exploration, and focus. A control group explored biodiversity in nature using the system in combination with embedded GNSS data, and an experimental group used an external module for RTK data. During the test, eye tracking data, geolocated traces, and in-app user-triggered events were recorded. Participants answered usability questionnaires (SUS, UEQ, HARUS).We found that the geolocation data the RTK group was exposed to was less accurate in average than that of the control group. The RTK group reported lower usability scores o
    
[^78]: 面向免费的对抗性协同过滤

    Adversarial Collaborative Filtering for Free. (arXiv:2308.13541v1 [cs.IR])

    [http://arxiv.org/abs/2308.13541](http://arxiv.org/abs/2308.13541)

    这篇论文介绍了一种面向免费的对抗性协同过滤方法，通过对抗性学习来规范用户/项目的表示，提高模型的泛化能力和鲁棒性，并提出了SharpCF方法来解决现有方法的缺点。

    

    协同过滤（CF）已成功用于帮助用户发现感兴趣的项目。然而，现有的CF方法存在噪声数据问题，这对推荐的质量产生负面影响。为了解决这个问题，许多先前的研究利用对抗性学习来规范用户/项目的表示，从而提高泛化能力和鲁棒性。这些方法通常在最小最大优化框架下学习对抗扰动和模型参数。然而，目前存在两个主要缺点：1）现有方法缺乏理论保证，无法解释为什么添加扰动可以提高模型的泛化能力和鲁棒性；2）解决最小最大优化问题需要耗时。除了更新模型参数，每次迭代还需要额外计算来更新扰动，使它们不适用于规模大的工业数据集。本文提出了一种新的Sharpness-aware Collaborative Filtering（SharpCF）方法，它简单而有效。

    Collaborative Filtering (CF) has been successfully used to help users discover the items of interest. Nevertheless, existing CF methods suffer from noisy data issue, which negatively impacts the quality of recommendation. To tackle this problem, many prior studies leverage adversarial learning to regularize the representations of users/items, which improves both generalizability and robustness. Those methods often learn adversarial perturbations and model parameters under min-max optimization framework. However, there still have two major drawbacks: 1) Existing methods lack theoretical guarantees of why adding perturbations improve the model generalizability and robustness; 2) Solving min-max optimization is time-consuming. In addition to updating the model parameters, each iteration requires additional computations to update the perturbations, making them not scalable for industry-scale datasets.  In this paper, we present Sharpness-aware Collaborative Filtering (SharpCF), a simple ye
    
[^79]: STEM:释放Embedding在多任务推荐中的力量

    STEM: Unleashing the Power of Embeddings for Multi-task Recommendation. (arXiv:2308.13537v1 [cs.IR])

    [http://arxiv.org/abs/2308.13537](http://arxiv.org/abs/2308.13537)

    本文提出了一种称为STEM的新范例，用于解决多任务推荐中的负传递问题。与现有方法不同，STEM通过根据样本中正反馈数量的相对比例进行细分，深入研究样本的复杂性，以提高推荐系统的性能。

    

    多任务学习（MTL）在推荐系统中变得越来越受欢迎，因为它能够同时优化多个目标。MTL的一个关键挑战是负传递的发生，即由于任务之间的冲突导致某些任务的性能下降。现有研究通过将所有样本视为一个整体来探索负传递，忽视了其中固有的复杂性。为此，我们根据任务之间正反馈的相对数量将样本进行细分，深入研究样本的复杂性。令人惊讶的是，现有MTL方法在收到各任务类似反馈的样本上仍然存在负传递。值得注意的是，现有方法通常采用共享嵌入的范例，并且我们假设它们的失败可以归因于使用这种通用嵌入来建模不同用户偏好的有限能力。

    Multi-task learning (MTL) has gained significant popularity in recommendation systems as it enables the simultaneous optimization of multiple objectives. A key challenge in MTL is the occurrence of negative transfer, where the performance of certain tasks deteriorates due to conflicts between tasks. Existing research has explored negative transfer by treating all samples as a whole, overlooking the inherent complexities within them. To this end, we delve into the intricacies of samples by splitting them based on the relative amount of positive feedback among tasks. Surprisingly, negative transfer still occurs in existing MTL methods on samples that receive comparable feedback across tasks. It is worth noting that existing methods commonly employ a shared-embedding paradigm, and we hypothesize that their failure can be attributed to the limited capacity of modeling diverse user preferences across tasks using such universal embeddings.  In this paper, we introduce a novel paradigm called
    
[^80]: 线性自动编码器对推荐系统中的隐式ZCA白化的影响

    Implicit ZCA Whitening Effects of Linear Autoencoders for Recommendation. (arXiv:2308.13536v1 [cs.IR])

    [http://arxiv.org/abs/2308.13536](http://arxiv.org/abs/2308.13536)

    本文展示了线性自动编码器模型在推荐系统中隐式的ZCA白化作用，以及其对低维物品嵌入的有效性。

    

    近年来，在推荐系统领域，线性回归（自动编码器）模型被研究作为学习物品相似性的一种方法。本文展示了线性自动编码器模型与推荐数据的ZCA白化之间的联系。特别地，我们展示了线性自动编码器模型的对偶形式解在物品的特征向量上实际上具有ZCA白化效果，而在自动编码器/回归模型的原始问题中，物品被视为输入特征。我们还展示了将线性自动编码器应用于使用Item2vec等嵌入方法获得的低维物品向量来估计物品之间相似性的正确性。我们的实验提供了初步结果，表明白化低维物品嵌入是有效的。

    Recently, in the field of recommendation systems, linear regression (autoencoder) models have been investigated as a way to learn item similarity. In this paper, we show a connection between a linear autoencoder model and ZCA whitening for recommendation data. In particular, we show that the dual form solution of a linear autoencoder model actually has ZCA whitening effects on feature vectors of items, while items are considered as input features in the primal problem of the autoencoder/regression model. We also show the correctness of applying a linear autoencoder to low-dimensional item vectors obtained using embedding methods such as Item2vec to estimate item-item similarities. Our experiments provide preliminary results indicating the effectiveness of whitening low-dimensional item embeddings.
    
[^81]: 通过无线计算实现联邦线性赌博学习

    Federated Linear Bandit Learning via Over-the-Air Computation. (arXiv:2308.13298v1 [cs.LG])

    [http://arxiv.org/abs/2308.13298](http://arxiv.org/abs/2308.13298)

    本研究针对联邦线性赌博学习提出了一种通过无线计算的方案，以减少通信开销。通过在噪声衰落信道上进行的模拟信号传输，我们的方案在降低累积遗憾方面表现出竞争力。

    

    本文研究了在由服务器和多个设备组成的无线系统中的联邦背景下的线性赌博学习。每个设备与环境交互，在接收到奖励后选择一个动作，并将模型更新发送到服务器。主要目标是在有限的时间范围内最小化所有设备的累积遗憾。为了减少通信开销，设备通过无线计算（AirComp）在噪声衰落信道上与服务器通信，其中通道噪声可能会扭曲信号。在这个背景下，我们提出了一种定制的联邦线性赌博方案，其中每个设备传输一个模拟信号，服务器接收到的是这些信号的叠加，受到信道噪声的扭曲。我们进行了严格的数学分析，确定了该方案的遗憾上限。理论分析和数值实验都证明了我们提出的方案在性能方面的竞争力。

    In this paper, we investigate federated contextual linear bandit learning within a wireless system that comprises a server and multiple devices. Each device interacts with the environment, selects an action based on the received reward, and sends model updates to the server. The primary objective is to minimize cumulative regret across all devices within a finite time horizon. To reduce the communication overhead, devices communicate with the server via over-the-air computation (AirComp) over noisy fading channels, where the channel noise may distort the signals. In this context, we propose a customized federated linear bandits scheme, where each device transmits an analog signal, and the server receives a superposition of these signals distorted by channel noise. A rigorous mathematical analysis is conducted to determine the regret bound of the proposed scheme. Both theoretical analysis and numerical experiments demonstrate the competitive performance of our proposed scheme in terms o
    
[^82]: 异构分布式机器遗忘和种子模型蒸馏

    Heterogeneous Decentralized Machine Unlearning with Seed Model Distillation. (arXiv:2308.13269v1 [cs.LG])

    [http://arxiv.org/abs/2308.13269](http://arxiv.org/abs/2308.13269)

    该论文介绍了一种名为HDUS的分布式遗忘框架，使用种子模型蒸馏构建可擦除的模型集成，适用于异构的设备端模型，具有卓越的性能。

    

    随着一些最近的信息安全法规赋予用户对任何经过训练的机器学习模型拥有被遗忘的无条件权利，个性化物联网服务提供商必须考虑到遗忘功能。取消学习用户贡献的最直接方法是从初始状态重新训练模型，但在频繁的遗忘请求中，这在高吞吐量应用中是不现实的。尽管提出了一些机器遗忘框架来加速重新训练过程，但它们无法适应分布式学习场景。本文中，我们设计了一个名为HDUS的分布式遗忘框架，它使用蒸馏的种子模型为所有客户端构建可擦除的集成模型。此外，该框架与异构的设备端模型兼容，具有更强的可伸缩性，适用于真实世界的应用。在三个真实数据集上的大量实验表明，我们的HDUS实现了最先进的性能。

    As some recent information security legislation endowed users with unconditional rights to be forgotten by any trained machine learning model, personalized IoT service providers have to put unlearning functionality into their consideration. The most straightforward method to unlearn users' contribution is to retrain the model from the initial state, which is not realistic in high throughput applications with frequent unlearning requests. Though some machine unlearning frameworks have been proposed to speed up the retraining process, they fail to match decentralized learning scenarios. In this paper, we design a decentralized unlearning framework called HDUS, which uses distilled seed models to construct erasable ensembles for all clients. Moreover, the framework is compatible with heterogeneous on-device models, representing stronger scalability in real-world applications. Extensive experiments on three real-world datasets show that our HDUS achieves state-of-the-art performance.
    
[^83]: 基于贝叶斯低秩适应的大型语言模型

    Bayesian low-rank adaptation for large language models. (arXiv:2308.13111v1 [cs.LG])

    [http://arxiv.org/abs/2308.13111](http://arxiv.org/abs/2308.13111)

    本研究提出了一种名为Laplace-LoRA的贝叶斯方法，通过应用拉普拉斯近似来增强经过微调的大型语言模型的校准能力。

    

    参数高效的微调（PEFT）已成为大型语言模型（LLMs）成本高效微调的新范式，其中低秩适应（LoRA）被广泛采用。然而，经过微调的LLMs往往变得过于自信，尤其是在较小数据集上进行微调时。贝叶斯方法具有估计不确定性的固有能力，可作为减轻过度自信并增强校准能力的有力工具。在这项工作中，我们引入了Laplace-LoRA，一种直观而有效的贝叶斯方法，它将拉普拉斯近似应用于LoRA参数，并显著提升了经过微调的LLMs的校准能力。

    Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially on when fine-tuned on smaller datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, a straightforward yet effective Bayesian method, which applies the Laplace approximation to the LoRA parameters and, considerably boosts the calibration of fine-tuned LLMs.
    
[^84]: CDAN: 用于低光图像增强的卷积稠密注意力引导网络

    CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])

    [http://arxiv.org/abs/2308.12902](http://arxiv.org/abs/2308.12902)

    本研究提出了一种名为CDAN的卷积稠密注意力引导网络，用于低光图像增强。该网络结合了自编码器架构、卷积和稠密块、注意力机制和跳跃连接，通过专门的后处理阶段进一步改善色彩平衡和对比度。与现有方法相比，在低光图像增强方面取得了显著的进展，展示了在各种具有挑战性的场景中的稳健性。

    

    低光图像以不足的照明为特征，面临清晰度减弱、颜色暗淡和细节减少的挑战。低光图像增强是计算机视觉中的一个重要任务，旨在通过改善亮度、对比度和整体感知质量来纠正这些问题，从而促进准确的分析和解释。本文介绍了一种新颖的解决方案：卷积稠密注意力引导网络（CDAN），用于增强低光图像。CDAN将自编码器架构与卷积和稠密块相结合，配合注意力机制和跳跃连接。该架构确保了有效的信息传递和特征学习。此外，专门的后处理阶段可以进一步改善色彩平衡和对比度。与低光图像增强领域的最新成果相比，我们的方法取得了显著的进展，并展示了在各种具有挑战性的场景中的稳健性。

    Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs 
    
[^85]: 一种基于概率波动的生成模型成员推断攻击方法

    A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])

    [http://arxiv.org/abs/2308.12143](http://arxiv.org/abs/2308.12143)

    本研究针对生成模型提出了一种概率波动评估成员推断攻击方法(PFAMI)，通过检测概率分布的波动性来推断模型中是否存在某条训练记录的成员身份。

    

    成员推断攻击(MIA)通过查询模型来识别机器学习模型的训练集中是否存在某条记录。对经典分类模型的MIA已有很多研究，最近的工作开始探索如何将MIA应用到生成模型上。我们的研究表明，现有的面向生成模型的MIA主要依赖于目标模型的过拟合现象。然而，过拟合可以通过采用各种正则化技术来避免，而现有的MIA在实践中表现不佳。与过拟合不同，记忆对于深度学习模型实现最佳性能是至关重要的，使其成为一种更为普遍的现象。生成模型中的记忆导致生成记录的概率分布呈现出增长的趋势。因此，我们提出了一种基于概率波动的成员推断攻击方法(PFAMI)，它是一种黑盒MIA，通过检测概率波动来推断成员身份。

    Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
    
[^86]: MKL-$L_{0/1}$-SVM: 一种多核学习的支持向量机框架

    MKL-$L_{0/1}$-SVM. (arXiv:2308.12016v1 [stat.ML])

    [http://arxiv.org/abs/2308.12016](http://arxiv.org/abs/2308.12016)

    本文提出了一种多核学习的支持向量机框架(MKL-$L_{0/1}$-SVM)，通过开发快速的ADMM求解器处理非凸非光滑的优化问题，并在实验中展示了与领先方法相当的性能。

    

    本文提出了一种适用于$(0, 1)$损失函数的支持向量机的多核学习（MKL）框架。首先给出了一阶最优性条件，然后利用它们开发了一个快速的ADMM求解器来处理非凸非光滑的优化问题。详细的合成和真实数据集上的实验表明，我们的MKL-$L_{0/1}$-SVM的性能与一种名为SimpleMKL的领先方法相当。

    This paper presents a Multiple Kernel Learning (abbreviated as MKL) framework for the Support Vector Machine (SVM) with the $(0, 1)$ loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver to deal with the nonconvex and nonsmooth optimization problem. Extensive numerical experiments on synthetic and real datasets show that the performance of our MKL-$L_{0/1}$-SVM is comparable with the one of the leading approaches called SimpleMKL developed by Rakotomamonjy, Bach, Canu, and Grandvalet [Journal of Machine Learning Research, vol. 9, pp. 2491-2521, 2008].
    
[^87]: ProAgent：利用大型语言模型构建主动合作的人工智能

    ProAgent: Building Proactive Cooperative AI with Large Language Models. (arXiv:2308.11339v1 [cs.AI])

    [http://arxiv.org/abs/2308.11339](http://arxiv.org/abs/2308.11339)

    ProAgent是一个利用大型语言模型构建的主动合作的AI框架，能够预测队友的决策并为自己制定增强计划，具有高度的模块化和可解释性。

    

    在AGI研究中，构建具有自适应行为的人工智能以进行人工智能和人类的合作成为一个关键关注点。目前，开发合作代理人的方法主要依赖于基于学习的方法，其中政策泛化严重依赖于与特定队友的过去互动。这些方法限制了代理人在面对新的队友时重新校准策略的能力。我们提出了ProAgent，这是一个新颖的框架，利用大型语言模型（LLMs）来创建一个具有预测队友未来决策能力和为自身制定增强计划能力的主动代理。ProAgent在合作推理方面表现出色，能够动态调整行为以增强与队友的协作努力。此外，ProAgent框架具有高度的模块化和可解释性，便于无缝集成，以应对各种协调场景。

    Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations
    
[^88]: 通过行动和语言提升智能体的沟通和学习能力

    Enhancing Agent Communication and Learning through Action and Language. (arXiv:2308.10842v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.10842](http://arxiv.org/abs/2308.10842)

    通过行动和语言相结合的方式，我们引入了一种新型智能体，其能够同时作为教师和学习者，通过行动演示和语言指令增强了沟通效率，并探索了结合行动和语言沟通模式对学习结果的积极影响。

    

    我们引入了一种新型的GC智能体类别，能够同时充当教师和学习者的角色。借助基于行动的演示和基于语言的指令，这些智能体增强了沟通效率。我们研究了在人类沟通和目标实现中的重要元素——教育学和实用主义的融入，提升了智能体的教学和学习能力。此外，我们探讨了结合行动和语言沟通模式对学习结果的影响，强调了多模式方法的优势。

    We introduce a novel category of GC-agents capable of functioning as both teachers and learners. Leveraging action-based demonstrations and language-based instructions, these agents enhance communication efficiency. We investigate the incorporation of pedagogy and pragmatism, essential elements in human communication and goal achievement, enhancing the agents' teaching and learning capabilities. Furthermore, we explore the impact of combining communication modes (action and language) on learning outcomes, highlighting the benefits of a multi-modal approach.
    
[^89]: 基于时空自适应嵌入，普通Transformer在交通预测中领先于其他方法

    Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting. (arXiv:2308.10425v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10425](http://arxiv.org/abs/2308.10425)

    本研究提出了一种名为时空自适应嵌入的新组件，在普通的Transformer中实现了领先于其他方法的交通预测性能，通过捕捉交通时间序列中的时空关系和时间信息实现了优秀的结果。

    

    随着智能交通系统（ITS）的快速发展，准确的交通预测已成为一个关键挑战。其核心瓶颈在于捕捉复杂的时空交通模式。近年来，已经提出了许多具有复杂架构的神经网络来解决这个问题。然而，网络架构的进步遇到了性能收益降低的问题。在本研究中，我们提出了一种称为时空自适应嵌入的新组件，可以在普通的Transformer中获得出色的结果。我们的提出的Spatio-Temporal Adaptive Embedding transformer（STAEformer）在五个真实世界的交通预测数据集上实现了最先进的性能。进一步的实验表明，时空自适应嵌入通过有效捕捉交通时间序列中的内在时空关系和时间信息，在交通预测中起到了关键作用。

    With the rapid development of the Intelligent Transportation System (ITS), accurate traffic forecasting has emerged as a critical challenge. The key bottleneck lies in capturing the intricate spatio-temporal traffic patterns. In recent years, numerous neural networks with complicated architectures have been proposed to address this issue. However, the advancements in network architectures have encountered diminishing performance gains. In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer) achieves state-of-the-art performance on five real-world traffic forecasting datasets. Further experiments demonstrate that spatio-temporal adaptive embedding plays a crucial role in traffic forecasting by effectively capturing intrinsic spatio-temporal relations and chronological information in traffic time series.
    
[^90]: Wasserstein几何生成器用于条件分布

    Wasserstein Geodesic Generator for Conditional Distributions. (arXiv:2308.10145v1 [stat.ML])

    [http://arxiv.org/abs/2308.10145](http://arxiv.org/abs/2308.10145)

    通过Wasserstein几何生成器学习条件分布，生成给定特定标签的样本。使用最优输运理论提出的方法能学习观察域的条件分布和它们之间的最优输运映射。在人脸图像数据上的实验验证了该方法的有效性。

    

    生成给定特定标签的样本需要估计条件分布。我们推导出条件分布之间Wasserstein距离的可处理的上界，以建立学习条件分布的理论基础。基于这一结果，我们提出了一种新颖的条件生成算法，其中条件分布完全由由统计距离定义的度量空间来表征。我们利用最优输运理论来提出了Wasserstein几何生成器，一种学习Wasserstein几何的新的条件生成器。所提出的方法学习观察域的条件分布和它们之间的最优输运映射。给定两个观察域标签，未观察到的中间域的条件分布位于给定的条件分布之间的Wasserstein几何中。在以光照条件为域标签的人脸图像上的实验证明了所提出方法的有效性。

    Generating samples given a specific label requires estimating conditional distributions. We derive a tractable upper bound of the Wasserstein distance between conditional distributions to lay the theoretical groundwork to learn conditional distributions. Based on this result, we propose a novel conditional generation algorithm where conditional distributions are fully characterized by a metric space defined by a statistical distance. We employ optimal transport theory to propose the \textit{Wasserstein geodesic generator}, a new conditional generator that learns the Wasserstein geodesic. The proposed method learns both conditional distributions for observed domains and optimal transport maps between them. The conditional distributions given unobserved intermediate domains are on the Wasserstein geodesic between conditional distributions given two observed domain labels. Experiments on face images with light conditions as domain labels demonstrate the efficacy of the proposed method.
    
[^91]: MindMap：知识图谱激发大型语言模型的思维图思考方法

    MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])

    [http://arxiv.org/abs/2308.09729](http://arxiv.org/abs/2308.09729)

    本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。

    

    通常，大型语言模型存在无法整合新知识、产生幻觉和决策过程不透明等限制。本文探讨了如何利用知识图谱（KG）来激发大型语言模型，以解决整合最新知识和引发模型思维路径的问题。具体来说，我们构建了一个提示管道，使大型语言模型能够理解KG输入并利用隐含知识和检索到的外部知识进行推理。此外，我们研究了引发大型语言模型执行推理和生成答案的思维导图。研究发现，生成的思维导图基于知识的本体论，展示了大型语言模型的推理路径，从而为生产环境中的推理提供了探索和评估的可能性。对三个问答数据集的实验证明，MindMap提示方法带来了显著的实证增益。

    LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question & answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
    
[^92]: 通过凸优化为神经网络系统设计安全过滤器

    Safety Filter Design for Neural Network Systems via Convex Optimization. (arXiv:2308.08086v1 [eess.SY])

    [http://arxiv.org/abs/2308.08086](http://arxiv.org/abs/2308.08086)

    本文提出了一种通过凸优化为神经网络系统设计安全过滤器的方法，该过滤器能够捕获建模误差，并通过鲁棒线性模型预测控制来搜索可以保证约束满足的控制器。通过在非线性摆系统上的实验验证了该方法的有效性。

    

    随着数据可用性的增加，已经广泛证明神经网络（NN）能够以数据驱动的方式准确捕获复杂的系统动态。然而，NN的架构复杂性和非线性使得对其合成一个能够被证明是安全的控制器变得具有挑战性。在这项工作中，我们提出了一种新颖的安全过滤器，它依靠凸优化来确保NN系统的安全性，同时能够捕获建模误差。我们的方法利用NN验证工具来使用一组线性边界来近似NN的动态，然后应用鲁棒线性模型预测控制（robust linear MPC）来寻找可以保证鲁棒约束满足的控制器。我们通过数值实验在非线性摆系统上展示了所提出框架的有效性。

    With the increase in data availability, it has been widely demonstrated that neural networks (NN) can capture complex system dynamics precisely in a data-driven manner. However, the architectural complexity and nonlinearity of the NNs make it challenging to synthesize a provably safe controller. In this work, we propose a novel safety filter that relies on convex optimization to ensure safety for a NN system, subject to additive disturbances that are capable of capturing modeling errors. Our approach leverages tools from NN verification to over-approximate NN dynamics with a set of linear bounds, followed by an application of robust linear MPC to search for controllers that can guarantee robust constraint satisfaction. We demonstrate the efficacy of the proposed framework numerically on a nonlinear pendulum system.
    
[^93]: Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG]) 该论文标题已翻译：二元强化学习。

    Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG])

    [http://arxiv.org/abs/2308.07843](http://arxiv.org/abs/2308.07843)

    该论文介绍了一个称为二元强化学习的在线算法，用于根据上下文因素和目标人与其照顾伴侣的过去反馈，个性化地提供干预措施。该算法是贝叶斯和层次的，并通过模拟展示了良好的实证效果。

    

    移动医疗旨在通过在个人日常生活中提供干预来提高健康结果。照顾伴侣和社会支持网络的参与经常在帮助个人管理繁重的医疗条件方面起着关键作用。这为移动医疗提供了机会，设计针对二元关系——目标人和其照顾伴侣之间关系——以提高社会支持的干预措施。在本文中，我们开发了二元强化学习（Dyadic RL），这是一种基于环境因素和目标人及其照顾伴侣的过去反馈个性化干预措施的在线强化学习算法。在这里，多组干预措施影响着二元关系在多个时间间隔内。开发的二元强化学习是贝叶斯和层次的。我们正式介绍了问题设定，开发了二元强化学习并确定了遗憾边界。通过模拟，我们展示了二元强化学习的实证效果。

    Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation st
    
[^94]: 问题分类的集成方法：融合Electra Transformer、GloVe和LSTM

    An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM. (arXiv:2308.06828v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.06828](http://arxiv.org/abs/2308.06828)

    本研究提出了一种集成Electra Transformer、GloVe和LSTM模型的创新问题分类方法，通过在TREC数据集上进行严格测试，证明了融合不同技术可以获得更优越的结果。

    

    自然语言处理（NLP）已经成为理解和生成人类语言的关键技术，它在机器翻译、情感分析等任务中扮演着重要角色，尤其是在问题分类方面。作为自然语言处理的子领域，问题分类专注于确定所需信息的类型，这是问题回答系统等下游应用的基本步骤。本研究提出了一种创新的问题分类集成方法，将Electra、GloVe和LSTM模型的优势相结合。该模型在著名的TREC数据集上进行了严格测试，展示了如何整合这些不同技术可以得到更优越的结果。Electra提供了基于transformer的复杂语言理解能力，GloVe提供了全局向量表示以捕捉词级语义，LSTM则贡献了序列学习能力以建模长期依赖关系。

    Natural Language Processing (NLP) has emerged as a crucial technology for understanding and generating human language, playing an essential role in tasks such as machine translation, sentiment analysis, and more pertinently, question classification. As a subfield within NLP, question classification focuses on determining the type of information being sought, a fundamental step for downstream applications like question answering systems. This study presents an innovative ensemble approach for question classification, combining the strengths of Electra, GloVe, and LSTM models. Rigorously tested on the well-regarded TREC dataset, the model demonstrates how the integration of these disparate technologies can lead to superior results. Electra brings in its transformer-based capabilities for complex language understanding, GloVe offers global vector representations for capturing word-level semantics, and LSTM contributes its sequence learning abilities to model long-term dependencies. By fus
    
[^95]: BarlowRL: Barlow Twins用于数据效率强化学习

    BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning. (arXiv:2308.04263v1 [cs.LG])

    [http://arxiv.org/abs/2308.04263](http://arxiv.org/abs/2308.04263)

    BarlowRL通过将Barlow Twins和DER相结合，实现了数据效率强化学习，并在Atari 100k基准测试上优于其他算法。它通过信息扩散避免了维度折叠，使得RL算法能够利用均匀分布的状态表示，从而实现卓越的性能。

    

    本文介绍了BarlowRL，一种将Barlow Twins自监督学习框架与DER（Data-Efficient Rainbow）算法相结合的数据效率强化学习代理。BarlowRL在Atari 100k基准测试中优于DER和对比算法CURL。BarlowRL通过确保信息扩散到整个空间来避免维度折叠。这有助于RL算法利用均匀分布的状态表示，最终实现卓越的性能。Barlow Twins与DER的整合增强了数据效率，并在RL任务中实现了卓越的性能。BarlowRL展示了将自监督学习技术纳入强化学习算法以提高性能的潜力。

    This paper introduces BarlowRL, a data-efficient reinforcement learning agent that combines the Barlow Twins self-supervised learning framework with DER (Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids dimensional collapse by enforcing information spread to the whole space. This helps RL algorithms to utilize uniformly spread state representation that eventually results in a remarkable performance. The integration of Barlow Twins with DER enhances data efficiency and achieves superior performance in the RL tasks. BarlowRL demonstrates the potential of incorporating self-supervised learning techniques to improve RL algorithms.
    
[^96]: 基于深度迁移学习的PMU测量的电力系统短期电压稳定性评估

    PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning. (arXiv:2308.03953v1 [cs.LG])

    [http://arxiv.org/abs/2308.03953](http://arxiv.org/abs/2308.03953)

    本研究提出了一种基于PMU测量的电力系统短期电压稳定性评估方法，利用深度迁移学习解决了拓扑变化、样本标注和小规模数据集处理等挑战，并且在实验中取得了较好的模型评估准确性。

    

    深度学习已经成为解决电力系统短期电压稳定性评估（STVSA）挑战的有效方法。然而，现有的基于深度学习的STVSA方法在适应拓扑变化、样本标注和处理小规模数据集方面存在局限性。为了克服这些挑战，本文提出了一种基于PMU测量的STVSA方法，利用深度迁移学习。该方法利用PMUs捕获的实时动态信息创建初始数据集。它采用时间集成进行样本标注，并利用最小二乘生成对抗网络（LSGAN）进行数据增强，实现对小规模数据集的有效深度学习。此外，该方法通过探索不同故障之间的连接增强了对拓扑变化的适应性。在IEEE 39节点测试系统上的实验结果表明，该方法提高了模型评估准确性。

    Deep learning has emerged as an effective solution for addressing the challenges of short-term voltage stability assessment (STVSA) in power systems. However, existing deep learning-based STVSA approaches face limitations in adapting to topological changes, sample labeling, and handling small datasets. To overcome these challenges, this paper proposes a novel phasor measurement unit (PMU) measurements-based STVSA method by using deep transfer learning. The method leverages the real-time dynamic information captured by PMUs to create an initial dataset. It employs temporal ensembling for sample labeling and utilizes least squares generative adversarial networks (LSGAN) for data augmentation, enabling effective deep learning on small-scale datasets. Additionally, the method enhances adaptability to topological changes by exploring connections between different faults. Experimental results on the IEEE 39-bus test system demonstrate that the proposed method improves model evaluation accura
    
[^97]: 保持对称性的程序表示法用于学习代码语义

    Symmetry-Preserving Program Representations for Learning Code Semantics. (arXiv:2308.03312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03312](http://arxiv.org/abs/2308.03312)

    本文提出了一种在代码中保持对称性的程序表示方法，通过引入循环层来提高在程序分析和建模中的效果。

    

    大型语言模型(LLMs)在自动程序推理方面显示出潜力，这是许多安全任务的关键方面。然而，现有的用于代码的LLM架构通常从其他领域（如自然语言处理）借用，引发对其泛化能力和对未知代码的健壮性的担忧。一个关键的泛化挑战是将代码语义的知识，包括控制和数据流，纳入LLM架构中。受到利用平移对称性的卷积层的启发，我们探索了代码对称性如何增强程序分析和建模的LLM架构。我们提供了一个严格的群论框架，形式化地定义了代码对称性作为保持语义的变换，并提供了在LLM架构中精确推理对称性保持的技术。利用这个框架，我们引入了一种保持程序对称性的新型自注意力变体，并展示了其有效性。

    Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.  Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiv
    
[^98]: DLSIA: 用于科学图像分析的深度学习

    DLSIA: Deep Learning for Scientific Image Analysis. (arXiv:2308.02559v1 [cs.CV])

    [http://arxiv.org/abs/2308.02559](http://arxiv.org/abs/2308.02559)

    DLSIA是一种Python的机器学习库，为科学家提供了定制化卷积神经网络架构，加速科学图像分析研究，并促进跨学科合作。

    

    我们介绍了一种名为DLSIA（Deep Learning for Scientific Image Analysis）的基于Python的机器学习库，该库为科学家和研究人员提供了一系列可定制的卷积神经网络（CNN）架构，用于广泛的图像分析任务，以进行下游数据处理或实验中的计算环境。DLSIA包含易于使用的架构，如自动编码器，可调的U-Net和参数精简的混合尺度稠密网络（MSDNets）。此外，我们还介绍了使用随机图和稀疏连接生成的稀疏混合尺度网络（SMSNets）。随着实验数据的规模和复杂性不断增长，DLSIA提供了易于访问的CNN构建和抽象CNN复杂性的方法，使科学家能够定制他们的机器学习方法，加速发现，促进跨学科合作并推动科学图像分析的研究。

    We introduce DLSIA (Deep Learning for Scientific Image Analysis), a Python-based machine learning library that empowers scientists and researchers across diverse scientific domains with a range of customizable convolutional neural network (CNN) architectures for a wide variety of tasks in image analysis to be used in downstream data processing, or for experiment-in-the-loop computing scenarios. DLSIA features easy-to-use architectures such as autoencoders, tunable U-Nets, and parameter-lean mixed-scale dense networks (MSDNets). Additionally, we introduce sparse mixed-scale networks (SMSNets), generated using random graphs and sparse connections. As experimental data continues to grow in scale and complexity, DLSIA provides accessible CNN construction and abstracts CNN complexities, allowing scientists to tailor their machine learning approaches, accelerate discoveries, foster interdisciplinary collaboration, and advance research in scientific image analysis.
    
[^99]: 经济非线性MPC的Koopman模型的端到端强化学习

    End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC. (arXiv:2308.01674v1 [cs.LG])

    [http://arxiv.org/abs/2308.01674](http://arxiv.org/abs/2308.01674)

    本论文提出了一种用于经济非线性MPC的Koopman模型的端到端强化学习方法，旨在实现控制性能和计算需求之间的平衡。

    

    （经济）非线性模型预测控制（（e）NMPC）需要在所有相关状态空间区域都具有足够准确性的动态系统模型。这些模型还必须计算成本足够低以确保实时可行性。基于数据驱动的替代机制模型可以用来减少（e）NMPC的计算负担；但是，这些模型通常通过系统辨识以在模拟样本上获得最大平均预测准确性进行训练，并作为实际（e）NMPC的一部分表现不佳。我们提出了一种用于实现最佳（e）NMPC性能的动态替代模型的端到端强化学习方法，从而得到具有控制性能和计算需求之间良好平衡的预测控制器。我们通过两个基于已建立的非线性连续搅拌反应器模型的应用来验证我们的方法。

    (Economic) nonlinear model predictive control ((e)NMPC) requires dynamic system models that are sufficiently accurate in all relevant state-space regions. These models must also be computationally cheap enough to ensure real-time tractability. Data-driven surrogate models for mechanistic models can be used to reduce the computational burden of (e)NMPC; however, such models are typically trained by system identification for maximum average prediction accuracy on simulation samples and perform suboptimally as part of actual (e)NMPC. We present a method for end-to-end reinforcement learning of dynamic surrogate models for optimal performance in (e)NMPC applications, resulting in predictive controllers that strike a favorable balance between control performance and computational demand. We validate our method on two applications derived from an established nonlinear continuous stirred-tank reactor model. We compare the controller performance to that of MPCs utilizing models trained by the 
    
[^100]: 深度生成模型、合成表格数据和差分隐私：综述与综合

    Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis. (arXiv:2307.15424v1 [cs.LG])

    [http://arxiv.org/abs/2307.15424](http://arxiv.org/abs/2307.15424)

    本文综述了近期合成数据生成的深度生成模型发展，重点关注表格数据集。通过使用深度生成模型，可以有效地生成隐私敏感数据的合成数据，并解决数据归一化、隐私和评估等方面的挑战。

    

    本文全面综述了通过深度生成模型生成合成数据的最新发展，重点关注表格数据集。我们特别概述了在隐私敏感数据背景下合成数据生成的重要性。此外，我们强调了使用深度生成模型相对于其他方法的优势，并详细解释了包括无监督学习、神经网络和生成模型在内的基本概念。论文涵盖了在使用深度生成模型处理表格数据集时涉及的挑战和考虑因素，如数据归一化、隐私问题和模型评估。本综述为对合成数据生成及其应用感兴趣的研究人员和实践者提供了宝贵的资源。

    This article provides a comprehensive synthesis of the recent developments in synthetic data generation via deep generative models, focusing on tabular datasets. We specifically outline the importance of synthetic data generation in the context of privacy-sensitive data. Additionally, we highlight the advantages of using deep generative models over other methods and provide a detailed explanation of the underlying concepts, including unsupervised learning, neural networks, and generative models. The paper covers the challenges and considerations involved in using deep generative models for tabular datasets, such as data normalization, privacy concerns, and model evaluation. This review provides a valuable resource for researchers and practitioners interested in synthetic data generation and its applications.
    
[^101]: 外部推理：朝着多种大型语言模型可互换辅助与人类反馈的方向前进

    External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback. (arXiv:2307.12057v1 [cs.CL])

    [http://arxiv.org/abs/2307.12057](http://arxiv.org/abs/2307.12057)

    本文提出通过从外部存储库中选择性地集成知识来增强大型语言模型，提出了一种外部推理的新方法，例子是ChatPDF。

    

    记忆被认为是使海马体和脑神经元内保持视觉和语言信息、随后用于解决通过学习一生中遇到的现实挑战的关键人类能力。通过应用已获得的知识解决复杂的人工智能任务是实现人工通用智能的一大进展。然而，尽管像GPT-3.5和GPT-4这样的大型语言模型在语言理解、生成、交互和推理方面显示了卓越的能力，但由于上下文长度的限制，它们无法处理广泛、不断演变的知识库。本文提出通过从外部存储库中选择性地集成知识来增强LLMs，并介绍了一种外部推理的新方法，例子是ChatPDF。

    Memory is identified as a crucial human faculty that allows for the retention of visual and linguistic information within the hippocampus and neurons in the brain, which can subsequently be retrieved to address real-world challenges that arise through a lifetime of learning. The resolution of complex AI tasks through the application of acquired knowledge represents a stride toward the realization of artificial general intelligence. However, despite the prevalence of Large Language Models (LLMs) like GPT-3.5 and GPT-4 , which have displayed remarkable capabilities in language comprehension, generation, interaction, and reasoning, they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases. This paper proposes that LLMs could be augmented through the selective integration of knowledge from external repositories, and in doing so, introduces a novel methodology for External Reasoning, exemplified by ChatPDF. Central to
    
[^102]: 用于验证深度神经网络的DPLL(T)框架

    A DPLL(T) Framework for Verifying Deep Neural Networks. (arXiv:2307.10266v1 [cs.LG])

    [http://arxiv.org/abs/2307.10266](http://arxiv.org/abs/2307.10266)

    这项工作介绍了一个新的约束求解方法NeuralSAT，用于验证深度神经网络。Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art.

    

    深度神经网络(DNNs)已经成为解决现实世界问题的有效方法。然而，与人工编写的软件一样，自动生成的DNNs可能存在错误并受到攻击。因此，近年来在开发有效和可扩展的DNN验证技术和工具方面引起了广泛关注。在这项工作中，我们介绍了NeuralSAT，一种用于DNN验证的新的约束求解方法。NeuralSAT的设计遵循了用于现代SMT求解的DPLL(T)算法，包括（冲突）子句学习、抽象和理论求解，因此NeuralSAT可以被视为DNNs的一个SMT框架。初步结果显示，NeuralSAT原型与最先进的方法相竞争。我们希望通过适当的优化和工程化，NeuralSAT能够将现代SAT/SMT求解器的能力和成功带到DNN验证中。NeuralSAT可从以下链接获取：https://github.com/dynaroars/neuralsat-solver

    Deep Neural Networks (DNNs) have emerged as an effective approach to tackling real-world problems. However, like human-written software, automatically-generated DNNs can have bugs and be attacked. This thus attracts many recent interests in developing effective and scalable DNN verification techniques and tools. In this work, we introduce a NeuralSAT, a new constraint solving approach to DNN verification. The design of NeuralSAT follows the DPLL(T) algorithm used modern SMT solving, which includes (conflict) clause learning, abstraction, and theory solving, and thus NeuralSAT can be considered as an SMT framework for DNNs. Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art. We hope, with proper optimization and engineering, NeuralSAT will carry the power and success of modern SAT/SMT solvers to DNN verification. NeuralSAT is avaliable from: https://github.com/dynaroars/neuralsat-solver
    
[^103]: 手写和打印文本分割：一个签名案例研究

    Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])

    [http://arxiv.org/abs/2307.07887](http://arxiv.org/abs/2307.07887)

    本研究旨在解决手写和打印文本分割的挑战，并提出了一种新的方法来完整地恢复不同类别的文本，特别是在重叠部分提高分割性能。同时，还引入了一个新的数据集SignaTR6K，用于支持该任务。

    

    在分析扫描文档时，手写文本可能覆盖打印文本。这在文档的光学字符识别（OCR）和数字化过程中造成困难，并且进而影响到下游的自然语言处理（NLP）任务。之前的研究要么仅关注手写文本的二分类，要么进行三类文档的分割，即手写、打印和背景像素的识别。这导致手写和打印重叠的像素只被分配到一个类别中，因此在另一个类别中不被考虑。因此，在这项研究中，我们开发了新的方法来解决手写和打印文本分割的挑战，目标是完整地恢复不同类别的文本，特别是提高重叠部分的分割性能。为了促进这项任务，我们介绍了一个新的数据集SignaTR6K，该数据集收集自真实的法律文件。

    While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as
    
[^104]: 用C-VAEs重建时空数据

    Reconstructing Spatiotemporal Data with C-VAEs. (arXiv:2307.06243v1 [cs.DB])

    [http://arxiv.org/abs/2307.06243](http://arxiv.org/abs/2307.06243)

    本文通过使用C-VAE模型来生成平滑且逼真的时空演变表示，探索了深度学习技术在重建时空数据方面的潜力。

    

    时空数据的连续表示通常依赖于使用抽象数据类型，例如移动区域，来表示形状和位置在时间上连续变化的实体。从离散的现实世界实体的快照创建这种表示需要使用插值方法来计算中间数据表示，并估计感兴趣对象在任意时间点的位置和形状。现有的区域插值方法常常无法生成平滑和逼真的区域演变表示。然而，深度学习技术的最新进展揭示了基于离散观测训练的深度模型通过隐式特征学习可以捕捉时空依赖关系的潜力。在这项工作中，我们探索了条件变分自编码器（C-VAE）模型生成移动区域的时空演变平滑和逼真表示的能力。

    The continuous representation of spatiotemporal data commonly relies on using abstract data types, such as \textit{moving regions}, to represent entities whose shape and position continuously change over time. Creating this representation from discrete snapshots of real-world entities requires using interpolation methods to compute in-between data representations and estimate the position and shape of the object of interest at arbitrary temporal points. Existing region interpolation methods often fail to generate smooth and realistic representations of a region's evolution. However, recent advancements in deep learning techniques have revealed the potential of deep models trained on discrete observations to capture spatiotemporal dependencies through implicit feature learning.  In this work, we explore the capabilities of Conditional Variational Autoencoder (C-VAE) models to generate smooth and realistic representations of the spatiotemporal evolution of moving regions. We evaluate our
    
[^105]: 带有零-shot数据压缩的边缘存储管理配方用于道路异常检测

    Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection. (arXiv:2307.04298v1 [cs.SD])

    [http://arxiv.org/abs/2307.04298](http://arxiv.org/abs/2307.04298)

    提出了一种基于预训练自编码器的简单而有效的数据压缩方法，用于边缘存储管理和道路异常检测，可以高效地存储高保真音频样本。

    

    最近的研究显示了基于边缘计算的道路异常检测系统，该系统可以同时进行数据收集。然而，边缘计算机的数据存储空间很小，但我们需要长时间存储收集到的音频样本，以便更新现有模型或开发新的方法。因此，我们应该考虑一种在保持高保真音频的同时进行高效存储管理的方法。从硬件角度来看，使用低分辨率麦克风可以直观地减小文件大小，但不推荐这种方法，因为它会从根本上削弱高频组件。另一方面，计算文件压缩方法可以将收集到的高分辨率音频编码为紧凑的代码，因此推荐使用该方法，因为它还提供相应的解码方法。受此启发，我们提出了一种简单而有效的基于预训练自编码器的数据压缩方法。

    Recent studies show edge computing-based road anomaly detection systems which may also conduct data collection simultaneously. However, the edge computers will have small data storage but we need to store the collected audio samples for a long time in order to update existing models or develop a novel method. Therefore, we should consider an approach for efficient storage management methods while preserving high-fidelity audio. A hardware-perspective approach, such as using a low-resolution microphone, is an intuitive way to reduce file size but is not recommended because it fundamentally cuts off high-frequency components. On the other hand, a computational file compression approach that encodes collected high-resolution audio into a compact code should be recommended because it also provides a corresponding decoding method. Motivated by this, we propose a way of simple yet effective pre-trained autoencoder-based data compression method. The pre-trained autoencoder is trained for the 
    
[^106]: inTformer: 一种基于时间嵌入的关注机制Transformer用于使用连接车辆数据的路口事故可能性预测

    inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])

    [http://arxiv.org/abs/2307.03854](http://arxiv.org/abs/2307.03854)

    inTformer是一种基于时间嵌入的关注机制Transformer，用于使用连接车辆数据预测路口事故可能性。与其他深度学习模型相比，inTformer具有处理长期依赖性、并行处理数据序列以及解决梯度消失问题的优势。

    

    实时事故可能性预测模型是主动交通安全管理系统的关键组成部分。多年来，许多研究尝试构建事故可能性预测模型，以提高交通安全，但主要集中在高速公路上。在大多数现有研究中，研究人员主要采用基于深度学习的框架来识别事故潜在风险。最近，Transformer已经成为一种潜在的深度神经网络，其基本原理是通过注意力机制来进行操作。Transformer在功能上比现有的深度学习模型（如LSTM，CNN等）具有几个优势。首先，Transformer可以轻松处理数据序列中的长期依赖性。其次，Transformer可以在训练期间并行处理数据序列中的所有元素。最后，Transformer不存在梯度消失的问题。认识到Transformer的巨大潜力，

    The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily employed a deep learning-based framework to identify crash potential. Lately, Transformer has emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformer has several functional benefits over extant deep learning models such as Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term dependencies in a data sequence. Secondly, Transformer can parallelly process all elements in a data sequence during training. Finally, Transformer does not have the vanishing gradient issue. Realizing the immense possibility of Transformer, th
    
[^107]: 关于形式特征归因及其近似方法

    On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v1 [cs.AI])

    [http://arxiv.org/abs/2307.03380](http://arxiv.org/abs/2307.03380)

    这篇论文研究了解释性人工智能（XAI）中的形式特征归因方法及其近似方法。现有的特征选择和归因方法存在一些问题，而形式化的XAI方法虽然是一个有希望的解决方案，但仍存在一些限制。

    

    近年来，人工智能（AI）算法和机器学习（ML）模型得到了广泛应用。尽管取得了巨大成功，但ML模型脆弱性，公平性以及解释性的缺乏等重要问题需要积极发展可解释的人工智能（XAI）和形式化的ML模型验证。XAI的两个主要研究方向包括特征选择方法（例如，Anchors）和特征归因技术（例如，LIME和SHAP）。尽管有希望，但大多数现有的特征选择和归因方法都容易出现一系列关键问题，包括解释不正确和超出分布采样。近期一种形式化的XAI方法（FXAI）虽然作为以上方法的替代品并避免了这些问题，但仍存在一些限制。例如，除了可扩展性限制外，这种形式化方法无法解决特征归因问题。

    Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution prob
    
[^108]: 近似最优非凸-强凸双层优化与全一阶预言机

    Near-Optimal Nonconvex-Strongly-Convex Bilevel Optimization with Fully First-Order Oracles. (arXiv:2306.14853v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.14853](http://arxiv.org/abs/2306.14853)

    本文针对双层优化问题中底层问题为强凸的情况，提出了一种更加高效的算法，可以以接近最优的速率收敛。这种算法避免了在实践中可能无法获得或代价昂贵的Hessian-向量乘积预言机的使用。

    

    双层优化在超参数调整、神经架构搜索和元学习等领域有着广泛应用。设计高效的双层优化算法是具有挑战性的，因为底层问题通过另一个优化问题隐式定义了一个可行性集。在这项工作中，我们考虑一种易于处理的情况，即底层问题是强凸的。最近的研究表明，通过Hessian-向量乘积预言机，可以在$\tilde{\mathcal{O}}(\epsilon^{-2})$个预言调用内可靠地找到一个$\epsilon$-一阶稳定点。然而，在实践中，Hessian-向量乘积可能无法获得或代价昂贵。Kwon等人（ICML 2023）通过提出一个一阶方法来解决这个问题，该方法可以以较慢的$\tilde{\mathcal{O}}(\epsilon^{-3})$的速率实现相同的目标。在这项工作中，我们提供了更严格的分析，证明这种方法可以以接近最优的$\tilde {\mathcal{O}}(\epsilon^{-2})$的速率像二阶方法一样收敛。

    Bilevel optimization has wide applications such as hyperparameter tuning, neural architecture search, and meta-learning. Designing efficient algorithms for bilevel optimization is challenging because the lower-level problem defines a feasibility set implicitly via another optimization problem. In this work, we consider one tractable case when the lower-level problem is strongly convex. Recent works show that with a Hessian-vector product oracle, one can provably find an $\epsilon$-first-order stationary point within $\tilde{\mathcal{O}}(\epsilon^{-2})$ oracle calls. However, Hessian-vector product may be inaccessible or expensive in practice. Kwon et al. (ICML 2023) addressed this issue by proposing a first-order method that can achieve the same goal at a slower rate of $\tilde{\mathcal{O}}(\epsilon^{-3})$. In this work, we provide a tighter analysis demonstrating that this method can converge at the near-optimal $\tilde {\mathcal{O}}(\epsilon^{-2})$ rate as second-order methods. Our a
    
[^109]: 通过更高级任务相似性加强图上的多任务学习

    Boosting Multitask Learning on Graphs through Higher-Order Task Affinities. (arXiv:2306.14009v1 [cs.LG])

    [http://arxiv.org/abs/2306.14009](http://arxiv.org/abs/2306.14009)

    本文从多任务学习的角度重新审视在给定图上预测节点标签的问题，提出通过更高级任务相似性来加强多任务学习，并开发了一种算法来将任务分组以应对负迁移问题。

    

    在给定图上预测节点标签是一个被广泛研究的问题，有许多应用，包括社区检测和分子图预测。本文从多任务学习的角度重新审视此问题，考虑同时在图上预测多个节点标签函数。为了具体说明，考虑重叠社区检测：每个社区成员身份是一个二进制节点分类任务。由于复杂的重叠模式，当我们将多个社区检测应用到naive多任务学习时，我们发现负迁移很普遍，因为不同的节点标签之间的任务关系高度非线性。为了解决这个挑战，我们开发了一种算法，基于更高级的任务相似性度量来将任务分组。然后我们在每个任务组上拟合多任务模型，产生在基线模型上的增强过程。我们将两个任务之间的更高级的任务相似性度量估计为预测损失。

    Predicting node labels on a given graph is a widely studied problem with many applications, including community detection and molecular graph prediction. This paper considers predicting multiple node labeling functions on graphs simultaneously and revisits this problem from a multitask learning perspective. For a concrete example, consider overlapping community detection: each community membership is a binary node classification task. Due to complex overlapping patterns, we find that negative transfer is prevalent when we apply naive multitask learning to multiple community detection, as task relationships are highly nonlinear across different node labeling. To address the challenge, we develop an algorithm to cluster tasks into groups based on a higher-order task affinity measure. We then fit a multitask model on each task group, resulting in a boosting procedure on top of the baseline model. We estimate the higher-order task affinity measure between two tasks as the prediction loss o
    
[^110]: 具有全局状态预测的分散式多智能体强化学习

    Decentralized Multi-Agent Reinforcement Learning with Global State Prediction. (arXiv:2306.12926v1 [cs.RO])

    [http://arxiv.org/abs/2306.12926](http://arxiv.org/abs/2306.12926)

    本文研究了分散式多智能体强化学习中的一个关键挑战：如何在没有全局信息的情况下有效训练机器人。我们提出了一种基于状态预测的方法，在不需要显式通信的情况下使机器人能够协调行动，实现更快更好的学习效果和任务执行性能。

    

    深度强化学习（DRL）在控制单个机器人方面取得了显着的成功。然而，将DRL应用于机器人群体存在重大挑战。其中一个关键挑战是非静态性，即当两个或更多机器人同时更新个体或共享政策时，会进入一个相互依存的培训过程，并且不保证收敛。克服非静态性通常涉及使用其他智能体的全局信息来训练机器人，例如其他智能体的状态和/或行动。相比之下，本文探讨了如何消除全局信息的需求。由于缺乏其他信息体的全局知识，我们将问题描述为部分可观察的马尔可夫决策过程。在以集体运输为测试场景的情况下，我们研究了两种多智能体培训方法。在第一种方法中，机器人不交换信息，并且被训练依靠通过推（push）和拉（pull）物体进行隐式通信。在第二种方法中，机器人彼此共享状态预测，使他们能够在没有显式通信的情况下协调行动。我们的实验表明，共享预测可以使智能体更有效地学习，同时在需要更少与环境交互的情况下实现更好的任务执行性能。

    Deep reinforcement learning (DRL) has seen remarkable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second appro
    
[^111]: 大型语言模型被误导：使用Only Connect Wall数据集探索创造性问题解决和Einstellung效应。

    Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11167](http://arxiv.org/abs/2306.11167)

    这项研究探索了大型语言模型（LLMs）对创造性问题解决的能力，并发现大型语言模型容易被误导，出现固定效应和Einstellung范式。

    

    自从人工智能诞生以来，对人类仿真智能的追求一直是人工智能研究的持久话题。最新一代的大型语言模型（LLM）的技术演进和新兴能力将这个主题从学术界带到了文化时代。尽管最近的NLP评估基准任务测试了人类仿真行为的一些方面（例如BIG-bench的“类人行为”任务），但几乎没有一个任务考察创造性问题解决能力。人类的创造性问题解决是认知神经科学中研究较为深入的主题，标准化测试主要使用将线索词之间的（异构）连接能力作为创造性的度量。在这样的任务中，暗示性的误导性刺激-被称为“诱导误解”的干扰因素-通过固定效应和Einstellung范式阻碍了人类的表现。在认知神经科学的研究中，通过事先让参与者接触到有相似拼写的错误因素来实验性地诱导这样的固定。

    The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incor
    
[^112]: 医疗知识图谱综述：资源、应用和前景

    A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])

    [http://arxiv.org/abs/2306.04802](http://arxiv.org/abs/2306.04802)

    本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。

    

    医疗知识图谱(HKGs)已成为组织医学知识的有结构且可解释的有为工具，提供了医学概念及其关系的全面视图。然而，数据异质性和覆盖范围有限等挑战仍然存在，强调了在HKG领域需要进一步研究的必要性。本综述是HKG的第一份综合概述。我们总结了HKG构建的流程和关键技术（即从头开始和通过集成），以及常见的利用方法（即基于模型和非基于模型）。为了为研究人员提供有价值的资源，我们根据它们捕获的数据类型和应用领域（该资源存储于https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase）组织了现有的HKG，并提供了相关的统计信息。在应用部分，我们深入探讨了HKG在各种医疗领域的变革性影响。

    Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
    
[^113]: 通过神经科学的视角探究人工意识的可行性

    The feasibility of artificial consciousness through the lens of neuroscience. (arXiv:2306.00915v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2306.00915](http://arxiv.org/abs/2306.00915)

    从神经科学的角度来看，目前大型语言模型难以具备哺乳动物意识感知相关的丘脑皮层系统的关键特征，缺乏周围世界的具体嵌入式信息，且当前的人工智能无法做到存在的依赖于其行为，这意味着人工意识的可行性存在瓶颈。

    

    与大型语言模型的交互引发了这些模型可能具有意识的猜测。从神经科学的角度来看，这种观点很难被证实。首先，大型语言模型的架构缺少哺乳动物意识感知相关的丘脑皮层系统的关键特征。其次，大型语言模型的输入缺乏我们与周围世界的感官接触的具有体验、嵌入式信息的特征。最后，虽然前两个论点在未来的AI系统中可以被克服，但第三个可能更难在不久的将来跨越。换言之，我们认为意识可能取决于是否在“游戏中有皮肤”，即系统的存在是否取决于其行为，而这在当前的人工智能中并不成立。

    Interactions with large language models have led to the suggestion that these models may be conscious. From the perspective of neuroscience, this position is difficult to defend. For one, the architecture of large language models is missing key features of the thalamocortical system that have been linked to conscious awareness in mammals. Secondly, the inputs to large language models lack the embodied, embedded information content characteristic of our sensory contact with the world around us. Finally, while the previous two arguments can be overcome in future AI systems, the third one might be harder to bridge in the near future. Namely, we argue that consciousness might depend on having 'skin in the game', in that the existence of the system depends on its actions, which is not true for present-day artificial intelligence.
    
[^114]: SimFBO：简单、灵活且通信高效的联邦双层学习

    SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning. (arXiv:2305.19442v1 [cs.LG])

    [http://arxiv.org/abs/2305.19442](http://arxiv.org/abs/2305.19442)

    SimFBO和其ShroFBO变体提出了一个简单、灵活且通信高效的FBO框架，可以应用于元学习和超参数优化任务。

    

    近来，由于元学习、微调、超参数调整等领域中嵌套优化结构的出现，联邦双层优化（FBO）在机器学习和边缘计算中显示了巨大的潜力。然而，现有的FBO算法往往涉及复杂的计算，并需要每次迭代多个子循环，每个子循环包含多个通信轮。在本文中，我们提出了一个名为SimFBO的简单灵活的FBO框架，它易于实现，不需要子循环，并包括一种广义的服务器端聚合和更新以提高通信效率。我们进一步提出了系统级异构鲁棒FBO（ShroFBO）作为SimFBO的变体，其对本地计算的异构有更强的鲁棒性。我们证明了在部分客户端参与和无替换的客户端采样下，SimFBO和ShroFBO可以实现线性收敛加速，同时改进了样本和通信复杂度。实验证明了它们在图像分类数据集的元学习和真实世界数据集上的超参数优化任务中的有效性。

    Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demons
    
[^115]: 对等公平性——解决公平评估中群体之间系统差异的问题

    Counterpart Fairness -- Addressing Systematic between-group Differences in Fairness Evaluation. (arXiv:2305.18160v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18160](http://arxiv.org/abs/2305.18160)

    本论文提出了一种新的公平评估方法，通过比较不同群体中相似的个体来解决群体之间的系统差异。这种方法基于倾向得分，识别对等个体，避免了比较不同类型的个体，从而提高公平性评估的准确性和可靠性。

    

    当使用机器学习（ML）辅助决策时，确保算法决策公平性至关重要，即不歧视特定个体/群体，尤其是来自弱势群体的人。现有的群体公平方法要求进行平等的群体测量，但未考虑群体之间的系统差异。混淆因素，这些因素虽然与敏感变量无关，但表现出系统差异，会对公平评估产生重要影响。为解决这个问题，我们认为公平测量应该基于不同群体中相似于感兴趣任务的对等人（即彼此相似的个体）之间的比较，其群体身份不可通过探索混淆因素算法地区分。我们开发了基于倾向得分的方法来识别对等个体，以避免公平评估比较“橙子”和“苹果”。

    When using machine learning (ML) to aid decision-making, it is critical to ensure that an algorithmic decision is fair, i.e., it does not discriminate against specific individuals/groups, particularly those from underprivileged populations. Existing group fairness methods require equal group-wise measures, which however fails to consider systematic between-group differences. The confounding factors, which are non-sensitive variables but manifest systematic differences, can significantly affect fairness evaluation. To tackle this problem, we believe that a fairness measurement should be based on the comparison between counterparts (i.e., individuals who are similar to each other with respect to the task of interest) from different groups, whose group identities cannot be distinguished algorithmically by exploring confounding factors. We have developed a propensity-score-based method for identifying counterparts, which prevents fairness evaluation from comparing "oranges" with "apples". 
    
[^116]: 基于贝叶斯抽样算法的在线自适应流量实验的实用批次评估

    An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation. (arXiv:2305.14704v1 [cs.LG])

    [http://arxiv.org/abs/2305.14704](http://arxiv.org/abs/2305.14704)

    本文推导和评估了四种贝叶斯批次赌博算法，用于自适应确定流量分配，全面评估了这些算法的可信度、敏感性和后悔度，仿真结果表明这些基于批次的贝叶斯算法是有效的。

    

    为了加速在线测试，多臂赌博算法通过自适应地收集数据而被作为固定时间A/B测试的重要补充方式不断提高。本文基于最近关于自适应收集数据的最佳臂识别和统计推断的研究，推导和评估了四种基于贝叶斯批次赌博算法（NB-TS，WB-TS，NB-TTTS，WB-TTTS），它们是两种加权批次（Naive Batch和Weighted Batch）和两种贝叶斯抽样策略（Thompson Sampling和Top-Two Thompson Sampling）的组合，用于自适应确定流量分配。本文提供的这些基于批次统计奖励度量的贝叶斯抽样算法在实践中得以应用，而本文提出的其中一个组合WB-TTTS似乎是最新讨论的。对这四种基于批次的贝叶斯抽样算法进行了全面评估，包括测试方法的可信度、敏感性和后悔度。此外，评估还考虑了批次内奖励度量的方差以及批次之间的相关性，这在以前的研究中尚未得到很好的解决。仿真结果表明，与现有的赌博算法（例如UCB1，TS和Exp3）相比，这些基于批次的贝叶斯算法是有效的。

    To speed up online testing, adaptive traffic experimentation through multi-armed bandit algorithms is rising as an essential complementary alternative to the fixed horizon A/B testing. Based on recent research on best arm identification and statistical inference with adaptively collected data, this paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS, WB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting batches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies (Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine traffic allocation. These derived Bayesian sampling algorithms are practically based on summary batch statistics of a reward metric for pilot experiments, where one of the combination WB-TTTS in this paper seems to be newly discussed. The comprehensive evaluation on the four Bayesian sampling algorithms covers trustworthiness, sensitivity and regret of a testing methodology. Moreover, the evaluati
    
[^117]: 通过验证和验证的视角对大型语言模型的安全性和可信度进行调查

    A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation. (arXiv:2305.11391v1 [cs.AI])

    [http://arxiv.org/abs/2305.11391](http://arxiv.org/abs/2305.11391)

    通过验证和验证的视角对大型语言模型的安全性和可信度进行调查，分类它们的已知漏洞，将其分为固有问题、有意攻击和意外错误。同时，考虑四种互补技术以提供LLM及其应用的安全和可信度保障。

    

    大型语言模型（LLM）以其在许多知识领域中为终端用户提供详细和有条理的答案，并能够进行人类级别的对话能力，引发了AI的一波新热潮。为了应对它们在许多工业应用中的快速采用，本次调查关注它们的安全性和可信度。首先，我们回顾LLM的已知漏洞，将它们分类为固有问题、有意攻击和意外错误。然后，我们考虑是否以及如何将已被广泛用于传统软件和深度学习模型（如卷积神经网络）的验证和验证（V＆V）技术，集成并进一步扩展到LLM的整个生命周期中，以提供严格的分析，确保LLM及其应用的安全和可信度。具体而言，我们考虑四种互补技术：虚假性和评估、验证、运行时监视和道德使用。考虑到LLM的快速发展，

    Large Language Models (LLMs) have exploded a new heatwave of AI, for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities of the LLMs, categorising them into inherent issues, intended attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V&V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and ethical use. Considering the fast development of 
    
[^118]: 面向危及生命的室性心律失常检测的微小机器学习设计竞赛

    TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection. (arXiv:2305.05105v1 [eess.SP])

    [http://arxiv.org/abs/2305.05105](http://arxiv.org/abs/2305.05105)

    TDC'22是第一届面向ICDs低功耗微控制器的人工智能/机器学习（AI/ML）算法创新竞赛。本次竞赛的挑战是开发一种基于AI/ML的新型实时检测算法，对危及生命的室性心律失常进行检测。

    

    第一届ACM/IEEE微小机器学习设计竞赛（TDC）于2022年在第41届计算机辅助设计国际会议（ICCAD）上举行，是一项具有挑战性的多月研发竞赛。TDC'22专注于需要在可植入设备上创新和实现人工智能/机器学习（AI/ML）算法的真实医疗问题。TDC'22的挑战问题是开发一种基于AI/ML的新型实时检测算法，用于心脏除颤器（ICDs）上使用的低功率微控制器对危及生命的室性心律失常进行检测。数据集包含来自90个受试者的8种不同心律类型的超过38,000个5秒心内电图（IEGM）片段。专用硬件平台是STMicroelectronics制造的NUCLEO-L432KC。TDC'22面向全球多人团队，吸引了来自50多个组织的150多支队伍参赛。本文首先介绍这一医疗问题，

    The first ACM/IEEE TinyML Design Contest (TDC) held at the 41st International Conference on Computer-Aided Design (ICCAD) in 2022 is a challenging, multi-month, research and development competition. TDC'22 focuses on real-world medical problems that require the innovation and implementation of artificial intelligence/machine learning (AI/ML) algorithms on implantable devices. The challenge problem of TDC'22 is to develop a novel AI/ML-based real-time detection algorithm for life-threatening ventricular arrhythmia over low-power microcontrollers utilized in Implantable Cardioverter-Defibrillators (ICDs). The dataset contains more than 38,000 5-second intracardiac electrograms (IEGMs) segments over 8 different types of rhythm from 90 subjects. The dedicated hardware platform is NUCLEO-L432KC manufactured by STMicroelectronics. TDC'22, which is open to multi-person teams world-wide, attracted more than 150 teams from over 50 organizations. This paper first presents the medical problem, da
    
[^119]: 《延迟、复合和部分匿名奖励的强化学习》

    Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward. (arXiv:2305.02527v1 [cs.LG])

    [http://arxiv.org/abs/2305.02527](http://arxiv.org/abs/2305.02527)

    本文提出了一种算法用于解决具有延迟、复合和部分匿名奖励反馈的无限时平均奖励马尔可夫决策过程(MDP)，并取得了较好的效果。

    

    我们研究了具有延迟、复合和部分匿名奖励反馈的无限时平均奖励马尔可夫决策过程(MDP)。奖励的延迟和复杂性意味着在给定状态下采取行动生成的奖励被分解为不同的组成部分，并在延迟的时间实例中被顺序实现。部分匿名属性意味着对于每个状态，学习者只观察到在该状态下采取不同行动产生的过去奖励组成部分的总和，但是在观察实例中实现。我们提出了一种名为$\mathrm{DUCRL2}$的算法，用于获得此设置的近似最优策略，并表明它实现了$\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ 的遗憾界，其中$S$和$A$分别是状态和动作空间的大小，$D$是MDP的直径，$d$是一个由最大奖励延迟限制的参数，$T$表示时间的长度。

    We investigate an infinite-horizon average reward Markov Decision Process (MDP) with delayed, composite, and partially anonymous reward feedback. The delay and compositeness of rewards mean that rewards generated as a result of taking an action at a given state are fragmented into different components, and they are sequentially realized at delayed time instances. The partial anonymity attribute implies that a learner, for each state, only observes the aggregate of past reward components generated as a result of different actions taken at that state, but realized at the observation instance. We propose an algorithm named $\mathrm{DUCRL2}$ to obtain a near-optimal policy for this setting and show that it achieves a regret bound of $\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ where $S$ and $A$ are the sizes of the state and action spaces, respectively, $D$ is the diameter of the MDP, $d$ is a parameter upper bounded by the maximum reward delay, and $T$ denotes the time horizon
    
[^120]: 基于地图的经验回放：强化学习中遗忘现象的内存节约解决方案

    Map-based Experience Replay: A Memory-Efficient Solution to Catastrophic Forgetting in Reinforcement Learning. (arXiv:2305.02054v1 [cs.LG])

    [http://arxiv.org/abs/2305.02054](http://arxiv.org/abs/2305.02054)

    本文提出了一种基于地图的经验回放方法，通过将存储的转换组织成一种简洁的环境模型网络，以在减少内存大小的同时增加每个样本的相关性，从而有效解决强化学习中的遗忘问题。

    

    深度强化学习代理在训练新数据时常常会遭受灾难性的遗忘，遗忘先前在输入空间中找到的解决方案。回放记忆是解决这个问题的常见方法，它会对旧和新的训练样本进行去关联和混洗。他们天真地按照状态过渡的顺序存储状态转变，而不考虑冗余性。我们介绍了一种基于Grow-When-Required（GWR）自组织网络的新型认知启发式回放内存方法，它类似于一种基于地图的世界认知模型。我们的方法将存储的转换组织成一个简洁的环境模型网络，将相似的样本合并以减少内存大小并增加样本之间的两两距离，从而增加每个样本的相关性。总体而言，我们的论文表明，基于地图的经验回放允许显着减少内存，只会产生轻微的性能下降。

    Deep Reinforcement Learning agents often suffer from catastrophic forgetting, forgetting previously found solutions in parts of the input space when training on new data. Replay Memories are a common solution to the problem, decorrelating and shuffling old and new training samples. They naively store state transitions as they come in, without regard for redundancy. We introduce a novel cognitive-inspired replay memory approach based on the Grow-When-Required (GWR) self-organizing network, which resembles a map-based mental model of the world. Our approach organizes stored transitions into a concise environment-model-like network of state-nodes and transition-edges, merging similar samples to reduce the memory size and increase pair-wise distance among samples, which increases the relevancy of each sample. Overall, our paper shows that map-based experience replay allows for significant memory reduction with only small performance decreases.
    
[^121]: 一种抗噪声的声学方法用于识别牛的觅食活动

    A noise-robust acoustic method for recognition of foraging activities of grazing cattle. (arXiv:2304.14824v1 [cs.LG])

    [http://arxiv.org/abs/2304.14824](http://arxiv.org/abs/2304.14824)

    本研究提出了一种抗噪声的声学方法，能够分析与吃草和反刍相关的鉴定下颚运动事件的固定长度段，用于识别牛的觅食活动，并在环境和自然噪声方面具有鲁棒性。

    

    为了在不断增长的乳制品市场中保持竞争力，农民必须不断改进他们的畜牧生产系统。精确畜牧业技术提供了商业农场动物个体化监测，优化畜牧生产。连续的声学监测是一种广泛接受的感应技术，用于估计自由放牧牛的日反刍和吃草时间预算。然而，牧场上的典型环境和自然噪声明显影响当前声学方法的性能和泛化。在本研究中，我们提出了一种声学方法，称为抗噪声觅食活动识别器 (NRFAR)。该方法通过分析与吃草和反刍相关的鉴定下颚运动事件的固定长度段，确定觅食活动的突发。NRFAR 的加性噪声鲁棒性使用静态高斯白噪声和四种不同的非静态自然噪声进行评估。

    To stay competitive in the growing dairy market, farmers must continuously improve their livestock production systems. Precision livestock farming technologies provide individualised monitoring of animals on commercial farms, optimising livestock production. Continuous acoustic monitoring is a widely accepted sensing technique used to estimate the daily rumination and grazing time budget of free-ranging cattle. However, typical environmental and natural noises on pasture noticeably affect the performance and generalisation of current acoustic methods. In this study, we present an acoustic method called Noise-Robust Foraging Activity Recognizer (NRFAR). The proposed method determines foraging activity bouts by analysing fixed-length segments of identified jaw movement events associated with grazing and rumination. The additive noise robustness of NRFAR was evaluated for several signal-to-noise ratios, using stationary Gaussian white noise and four different non-stationary natural noise 
    
[^122]: 一种加速有限元求解器中代数多重网格方法的深度学习算法

    A Deep Learning algorithm to accelerate Algebraic Multigrid methods in Finite Element solvers of 3D elliptic PDEs. (arXiv:2304.10832v1 [math.NA])

    [http://arxiv.org/abs/2304.10832](http://arxiv.org/abs/2304.10832)

    该论文介绍了一种新的深度学习算法，通过解释线性系统的稀疏矩阵为黑白图像，利用池化操作将其转换为小的多通道图像，从而调整代数多重网格方法中的强门槛参数。该算法最小化了AMG方法在有限元求解器中的计算成本，并在解决三维椭圆偏微分方程时比现有最先进的AMG求解器更快。

    

    代数多重网格方法是解线性方程组的最有效的求解器之一，广泛用于离散化的偏微分方程问题。然而，该方法存在高度依赖于需要调优的参数，尤其是强门槛参数，这是构建多重网格所需的。我们引入了一种新的深度学习算法，通过把线性系统的稀疏矩阵解释为黑白图像，利用池化操作将它转换为小的多通道图像，从而调整强门槛参数，最小化了AMG方法在有限元求解器中的计算成本。我们展示了该算法对于解决三维椭圆偏微分方程的速度显著快于现有最先进的AMG求解器。

    Algebraic multigrid (AMG) methods are among the most efficient solvers for linear systems of equations and they are widely used for the solution of problems stemming from the discretization of Partial Differential Equations (PDEs). The most severe limitation of AMG methods is the dependence on parameters that require to be fine-tuned. In particular, the strong threshold parameter is the most relevant since it stands at the basis of the construction of successively coarser grids needed by the AMG methods. We introduce a novel Deep Learning algorithm that minimizes the computational cost of the AMG method when used as a finite element solver. We show that our algorithm requires minimal changes to any existing code. The proposed Artificial Neural Network (ANN) tunes the value of the strong threshold parameter by interpreting the sparse matrix of the linear system as a black-and-white image and exploiting a pooling operator to transform it into a small multi-channel image. We experimentall
    
[^123]: PGTask：介绍从对话中生成档案的任务

    PGTask: Introducing the Task of Profile Generation from Dialogues. (arXiv:2304.06634v1 [cs.CL])

    [http://arxiv.org/abs/2304.06634](http://arxiv.org/abs/2304.06634)

    对话系统的个性化需要个人资料信息，而从对话中提取/生成个人资料信息是一项基本需求。为此，我们提出了档案生成任务（PGTask）并提供了相关的数据集和基准，该任务使得研究者可以更好地了解档案生成任务的挑战和可能的解决方案。

    

    最近的研究尝试通过将个人资料信息融入模型来个性化对话系统。然而，这种知识信息稀少且难以获取，这使得从对话中提取/生成个人资料信息成为一项基本需求。为了克服这一限制，我们引入了档案生成任务（PGTask）。我们为此问题提供了一个新的数据集，其中包括与相关话语对齐的档案句子，从对话语料库中提取。此外，利用最先进的方法，我们为这个新数据集提供了一个档案生成的基准。我们的实验揭示了档案生成的挑战，并希望这引入了一个新的研究方向。

    Recent approaches have attempted to personalize dialogue systems by leveraging profile information into models. However, this knowledge is scarce and difficult to obtain, which makes the extraction/generation of profile information from dialogues a fundamental asset. To surpass this limitation, we introduce the Profile Generation Task (PGTask). We contribute with a new dataset for this problem, comprising profile sentences aligned with related utterances, extracted from a corpus of dialogues. Furthermore, using state-of-the-art methods, we provide a benchmark for profile generation on this novel dataset. Our experiments disclose the challenges of profile generation, and we hope that this introduces a new research direction.
    
[^124]: 基于置信度预测的随机传感器不确定性下安全的感知控制

    Safe Perception-Based Control under Stochastic Sensor Uncertainty using Conformal Prediction. (arXiv:2304.00194v1 [eess.SY])

    [http://arxiv.org/abs/2304.00194](http://arxiv.org/abs/2304.00194)

    本文提出了一种利用置信度预测来应对传感器噪声及不确定性的感知控制框架，通过量化感知地图的不确定性并将其整合到控制设计中，计算有效的状态估计区域，从而实现连续时间系统的采样数据控制，确保系统的安全性和有效性。

    

    本文考虑利用通过学习增强的感知地图从高维传感器测量中获得的状态估计的感知控制。但是，这些感知地图并不完美，会导致状态估计误差，这可能导致不安全的系统行为。随机传感器噪声会使情况变得更糟，并导致遵循未知分布的估计误差。我们提出了一个感知控制框架，它: i）量化了感知地图的估计不确定性，并ii）将这些不确定性表示集成到控制设计中。为此，我们使用置信度预测来计算有效的状态估计区域，这些区域是高概率包含未知状态的集合。然后，我们基于测量鲁棒控制障碍函数的概念设计了连续时间系统的采样数据控制器。我们的控制器使用了自触发控制的思想，并使我们避免使用随机微积分。我们的框架是一种易于实现和可扩展的方法，可以保证系统的安全性和有效性。

    We consider perception-based control using state estimates that are obtained from high-dimensional sensor measurements via learning-enabled perception maps. However, these perception maps are not perfect and result in state estimation errors that can lead to unsafe system behavior. Stochastic sensor noise can make matters worse and result in estimation errors that follow unknown distributions. We propose a perception-based control framework that i) quantifies estimation uncertainty of perception maps, and ii) integrates these uncertainty representations into the control design. To do so, we use conformal prediction to compute valid state estimation regions, which are sets that contain the unknown state with high probability. We then devise a sampled-data controller for continuous-time systems based on the notion of measurement robust control barrier functions. Our controller uses idea from self-triggered control and enables us to avoid using stochastic calculus. Our framework is agnost
    
[^125]: 适应性负证据深度学习用于开放式半监督学习

    Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning. (arXiv:2303.12091v1 [cs.LG])

    [http://arxiv.org/abs/2303.12091](http://arxiv.org/abs/2303.12091)

    本文提出了ANEDL框架，应用证据深度学习量化不同类型的不确定性，并设计了新颖的适应性负优化策略，有效应对在未标记数据集中包含内部值和异常值的开放式半监督学习。

    

    半监督学习方法假设标记数据、未标记数据和测试数据来自同一分布。开放式半监督学习考虑到一个更实际的情况，即未标记数据和测试数据包含标记数据中未观察到的新类别（异常值）。本文提出了一种新颖的框架——适应性负证据深度学习（ANEDL），以应对二元分类器的不足之处，如缺乏可扩展性和无法区分不同类型的不确定性。具体而言，我们首先介绍证据深度学习（EDL）作为一种异常检测器来量化不同类型的不确定性，并设计不同的不确定性度量方法进行自我训练和推理。此外，我们提出了一种新颖的适应性负优化策略，使EDL更加适合包含内部值和异常值的未标记数据集。通过在基准数据集上的实验验证，我们的ANEDL显著优于现有的开放式半监督学习方法。

    Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrat
    
[^126]: SIESTA: 高效的在线持续学习与休眠 (arXiv:2303.10725v2 [cs.CV] UPDATED)

    SIESTA: Efficient Online Continual Learning with Sleep. (arXiv:2303.10725v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10725](http://arxiv.org/abs/2303.10725)

    SIESTA是一种在线持续学习方法，通过使用无需回忆、无需反向传播和数据驱动的网络更新规则，在更少的时间和能源消耗下高效地更新深度神经网络(DNN)。该方法基于训练休眠/觉醒框架，可以应用于设备端学习。

    

    在监督式持续学习中，深度神经网络(DNN)通过不断增长的数据流进行更新。与数据离线情况不同，我们不能对数据流进行任何分布假设。理想情况下，为了提高计算效率，只需要对数据集进行一次遍历。然而，现有的方法无法满足真实世界应用的条件，同时也无法提高计算效率。在本文中，我们提出了一种基于训练休眠/觉醒框架的新型在线持续学习方法SIESTA，该方法符合设备端学习的需求。SIESTA的主要目标是改进计算效率，以便可以在更少的时间和能源消耗下高效地更新DNN。SIESTA的主要创新点有：在觉醒阶段使用无需回忆、无需反向传播和数据驱动的网络更新规则进行快速在线更新，以及快速收敛的Wake/Sleep训练框架。

    In supervised continual learning, a deep neural network (DNN) is updated with an ever-growing data stream. Unlike the offline setting where data is shuffled, we cannot make any distributional assumptions about the data stream. Ideally, only one pass through the dataset is needed for computational efficiency. However, existing methods are inadequate and make many assumptions that cannot be made for real-world applications, while simultaneously failing to improve computational efficiency. In this paper, we propose a novel online continual learning method, SIESTA based on wake/sleep framework for training, which is well aligned to the needs of on-device learning. The major goal of SIESTA is to advance compute efficient continual learning so that DNNs can be updated efficiently using far less time and energy. The principal innovations of SIESTA are: 1) rapid online updates using a rehearsal-free, backpropagation-free, and data-driven network update rule during its wake phase, and 2) expedi
    
[^127]: 不怕分类器偏差：以神经崩溃为灵感的合作学习中使用合成和固定分类器

    No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier. (arXiv:2303.10058v1 [cs.LG])

    [http://arxiv.org/abs/2303.10058](http://arxiv.org/abs/2303.10058)

    本文提出一种解决合作学习中分类器偏差问题的方案，即在训练过程中使用合成的ETF分类器，使得所有客户端能够学习到统一的最优特征表示。

    

    数据异构性是困扰合作学习性能的内在挑战。最近的研究已经确定了本地模型的偏置分类器是关键瓶颈。以前的尝试利用FL训练后进行分类器校准，但这种方法未能改善训练时分类器偏差导致的差劣特征表示。解决FL中分类器偏差困境需要充分理解分类器背后的机制。神经崩溃的最新进展表明，在完美的训练场景下，分类器和特征原型崩溃为一种称为simplex equiangular tight frame(ETF)的最优结构。基于这种神经崩溃的见解，我们提出了一种解决FL分类器偏差问题的解决方案，即在训练过程中利用合成和固定的ETF分类器。最优分类器结构使得所有客户端甚至在极端异构数据下也能学到统一的和最优的特征表示。

    Data heterogeneity is an inherent challenge that hinders the performance of federated learning (FL). Recent studies have identified the biased classifiers of local models as the key bottleneck. Previous attempts have used classifier calibration after FL training, but this approach falls short in improving the poor feature representations caused by training-time classifier biases. Resolving the classifier bias dilemma in FL requires a full understanding of the mechanisms behind the classifier. Recent advances in neural collapse have shown that the classifiers and feature prototypes under perfect training scenarios collapse into an optimal structure called simplex equiangular tight frame (ETF). Building on this neural collapse insight, we propose a solution to the FL's classifier bias problem by utilizing a synthetic and fixed ETF classifier during training. The optimal classifier structure enables all clients to learn unified and optimal feature representations even under extremely hete
    
[^128]: 基于多智能体深度强化学习的多模式插电混合动力汽车能量管理

    Energy Management of Multi-mode Plug-in Hybrid Electric Vehicle using Multi-agent Deep Reinforcement Learning. (arXiv:2303.09658v1 [cs.RO])

    [http://arxiv.org/abs/2303.09658](http://arxiv.org/abs/2303.09658)

    本论文提出了一种基于多智能体深度强化学习的多模式插电混合动力汽车能量管理的MIMO控制方法，通过握手策略和多目标函数优化全局控制，实验结果表明其在燃料消耗、SOC变化和功率限制方面优于传统方法。

    

    近年来出现的多模式插电混合动力汽车(PHEV)技术是减少碳排放的途径之一，其能量管理需要多输入多输出(MIMO)控制。目前，现有的方法通常将MIMO控制解耦为单输出(MISO)控制，并且只能实现其局部最优性能。为了全局优化多模式车辆，本文研究了基于多智能体深度强化学习(MADRL)的多模式PHEV能量管理的MIMO控制方法。通过引入相关比例，提出了一种握手策略，使得两个学习智能体能够在MADRL框架下使用深度确定性策略梯度(DDPG)算法进行协作学习。通过对影响学习性能的影响因素进行灵敏度分析，得到了DDPG智能体的统一设置。握手策略的最优工作模式通过多目标函数得到，考虑燃料消耗、电池SOC变化和功率限制违规。基于硬件在环(HIL)仿真器的实验结果表明，所提出的MADRL能量管理方法在燃料消耗、SOC变化和功率限制违规方面优于传统的基于规则的方法和单个智能体RL方法。

    The recently emerging multi-mode plug-in hybrid electric vehicle (PHEV) technology is one of the pathways making contributions to decarbonization, and its energy management requires multiple-input and multiple-output (MIMO) control. At the present, the existing methods usually decouple the MIMO control into single-output (MISO) control and can only achieve its local optimal performance. To optimize the multi-mode vehicle globally, this paper studies a MIMO control method for energy management of the multi-mode PHEV based on multi-agent deep reinforcement learning (MADRL). By introducing a relevance ratio, a hand-shaking strategy is proposed to enable two learning agents to work collaboratively under the MADRL framework using the deep deterministic policy gradient (DDPG) algorithm. Unified settings for the DDPG agents are obtained through a sensitivity analysis of the influencing factors to the learning performance. The optimal working mode for the hand-shaking strategy is attained thro
    
[^129]: 通过受限代理学习控制深度序数分类中的类布局

    Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning. (arXiv:2303.00396v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.00396](http://arxiv.org/abs/2303.00396)

    本文提出了一种通过受限代理学习方法，可以有效地控制深度序数分类中的类布局。

    

    对于深度序数分类任务，学习特定于序数分类的良好结构化特征空间有助于恰当地捕捉类之间的序数属性。本文提出了一种新颖的受限代理学习方法，该方法可以为每个序数类学习一个代理，然后通过限制这些代理来调整类的全局布局。我们提出了两种策略：硬布局约束和软布局约束。硬布局约束通过直接控制代理的生成来实现，以强制将其放置在严格的线性布局或半圆形布局（即严格序数布局的两种实例）中。软布局约束通过引入正则化项到损失函数中来实现，该项惩罚偏离理想序数布局的情况。在基准数据集上的实验结果证明了所提出的CPL方法在深度序数分类中的有效性。

    For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordi
    
[^130]: 对图卷积网络的语义后门攻击

    A semantic backdoor attack against Graph Convolutional Networks. (arXiv:2302.14353v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14353](http://arxiv.org/abs/2302.14353)

    该论文研究了图卷积网络（GCNs）是否容易受到语义后门攻击，提出了一种针对GCNs的语义后门攻击方法（SBAG），通过在样本中的特定节点作为触发器，并注入隐藏的后门来攻击GCNs模型。

    

    图卷积网络（GCNs）在解决各种图结构相关任务（如节点分类和图分类）方面非常有效。然而，最近的研究表明，GCNs容易受到一种新型威胁，称为后门攻击。攻击者可以将隐藏的后门注入GCNs中，使得攻击模型在良性样本上表现良好，但是如果攻击者定义的触发器激活了隐藏的后门，其预测结果将被恶意地修改为攻击者指定的目标标签。本文研究了GCNs是否容易受到这种语义后门攻击，并提出了一种针对GCNs的语义后门攻击（SBAG）来揭示GCNs中存在的安全漏洞。SBAG使用样本中的某种节点作为后门触发器，并通过污染训练数据将隐藏的后门注入到GCNs模型中。

    Graph Convolutional Networks (GCNs) have been very effective in addressing the issue of various graph-structured related tasks, such as node classification and graph classification. However, recent research has shown that GCNs are vulnerable to a new type of threat called the backdoor attack, where the adversary can inject hidden backdoor into the GCNs so that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed to the attacker-specified target label if the hidden backdoor is activated by the attacker-defined trigger. In this paper, we investigate whether such semantic backdoor attacks are possible for GCNs and propose a Semantic Backdoor Attack against GCNs(SBAG) under the context of graph classification to reveal the existence of this security vulnerability in GCNs. The SBAG uses a certain type of node in the samples as a backdoor trigger and injects hidden backdoor into GCNs models through poisoning training data. The backdoor will b
    
[^131]: 使用强化学习技术革新基因组学

    Revolutionizing Genomics with Reinforcement Learning Techniques. (arXiv:2302.13268v2 [q-bio.GN] UPDATED)

    [http://arxiv.org/abs/2302.13268](http://arxiv.org/abs/2302.13268)

    强化学习是一种革新的工具，可以在基因组学领域中解决自动数据分析和处理的问题。使用强化学习算法可以降低收集标记训练数据的成本，适用于基因组数据分析和解释。本调查重点关注在基因组研究领域中使用强化学习的应用，包括基因调控网络、基因组组装和序列比对。

    

    近年来，强化学习（RL）作为一种强大的工具出现在解决各种问题中，包括决策和基因组学。过去二十年的原始基因组数据指数增长已经超出了手动分析的能力，这导致对自动数据分析和处理的兴趣越来越大。RL算法能够在最小的人工监督下从经验中学习，使其非常适合基因组数据分析和解释。使用RL的一个关键好处是降低了收集标记训练数据的成本，这是监督学习所需的。虽然已经有许多研究探讨了机器学习在基因组学中的应用，但本调查仅专注于在各种基因组研究领域（包括基因调控网络，基因组组装和序列比对）中使用RL的情况。我们对现有研究的技术细节进行了全面的概述。

    In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for solving a wide range of problems, including decision-making and genomics. The exponential growth of raw genomic data over the past two decades has exceeded the capacity of manual analysis, leading to a growing interest in automatic data analysis and processing. RL algorithms are capable of learning from experience with minimal human supervision, making them well-suited for genomic data analysis and interpretation. One of the key benefits of using RL is the reduced cost associated with collecting labeled training data, which is required for supervised learning. While there have been numerous studies examining the applications of Machine Learning (ML) in genomics, this survey focuses exclusively on the use of RL in various genomics research fields, including gene regulatory networks (GRNs), genome assembly, and sequence alignment. We present a comprehensive technical overview of existing studies on the applic
    
[^132]: 不准确的标签分布学习

    Inaccurate Label Distribution Learning. (arXiv:2302.13000v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13000](http://arxiv.org/abs/2302.13000)

    本文研究了不准确的标签分布学习问题，提出了一种LDL模型，通过图形的局部几何结构辅助恢复理想的标签分布。LDL方法的先前研究假设训练实例的标签分布是准确的，而现实中的标签分布通常是不准确的，并受到标注错误的干扰。

    

    标签分布学习（LDL）训练模型来预测一组标签（称为标签分布（LD））与实例的相关性。以前的LDL方法都假设训练实例的LD是准确的。然而，为训练实例注释高度准确的LD耗时且非常昂贵，在现实中收集到的LD通常是不准确且受注释错误干扰的。本文首次研究了不准确的LDL问题，即开发一个具有噪声LD的LDL模型。我们假设噪声LD矩阵是理想LD矩阵和稀疏噪声矩阵的线性组合。因此，不准确的LDL问题成为一个逆问题，目标是从噪声LD中恢复理想的LD和噪声矩阵。我们假设理想的LD矩阵由于标签之间的关联具有低秩性，并利用图形捕捉到的实例的局部几何结构来帮助恢复理想的LD。

    Label distribution learning (LDL) trains a model to predict the relevance of a set of labels (called label distribution (LD)) to an instance. The previous LDL methods all assumed the LDs of the training instances are accurate. However, annotating highly accurate LDs for training instances is time-consuming and very expensive, and in reality the collected LD is usually inaccurate and disturbed by annotating errors. For the first time, this paper investigates the problem of inaccurate LDL, i.e., developing an LDL model with noisy LDs. We assume that the noisy LD matrix is a linear combination of an ideal LD matrix and a sparse noise matrix. Consequently, the problem of inaccurate LDL becomes an inverse problem, where the objective is to recover the ideal LD and noise matrices from the noisy LDs. We hypothesize that the ideal LD matrix is low-rank due to the correlation of labels and utilize the local geometric structure of instances captured by a graph to assist in recovering the ideal L
    
[^133]: 在无意识公平性设置中，对偏见评估和检测进行反事实推理

    Counterfactual Reasoning for Bias Evaluation and Detection in a Fairness under Unawareness setting. (arXiv:2302.08204v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08204](http://arxiv.org/abs/2302.08204)

    本研究提出了一种方法来揭示即使在丢弃敏感特征的情况下机器学习模型可能仍存在的潜在偏见，并通过利用反事实推理来检测黑盒预测器的偏见。

    

    当前的AI法规要求在算法的决策过程中丢弃敏感特征（如性别、种族、宗教），以防止不公平的结果。然而，即使在训练集中没有敏感特征，算法仍可能持续进行歧视。事实上，在敏感特征被忽略的情况下（无意识公平性），通过非线性关系，可以通过所谓的代理特征推测出这些敏感特征。在这项工作中，我们提出了一种揭示即使在丢弃敏感特征的情况下机器学习模型可能仍存在的潜在偏见的方法。本研究表明，通过利用反事实推理，可以揭示出黑盒预测器是否仍存在偏见。具体而言，当预测器提供负面分类结果时，我们的方法首先为被歧视的用户类别构建反事实示例，以获得正面结果。然后，相同的反事实样本被用于外部分类器（针对敏感特征）进行训练和测试，以评估黑盒预测器的偏见。

    Current AI regulations require discarding sensitive features (e.g., gender, race, religion) in the algorithm's decision-making process to prevent unfair outcomes. However, even without sensitive features in the training set, algorithms can persist in discrimination. Indeed, when sensitive features are omitted (fairness under unawareness), they could be inferred through non-linear relations with the so called proxy features. In this work, we propose a way to reveal the potential hidden bias of a machine learning model that can persist even when sensitive features are discarded. This study shows that it is possible to unveil whether the black-box predictor is still biased by exploiting counterfactual reasoning. In detail, when the predictor provides a negative classification outcome, our approach first builds counterfactual examples for a discriminated user category to obtain a positive outcome. Then, the same counterfactual samples feed an external classifier (that targets a sensitive f
    
[^134]: 具有通用效用的可扩展多智能体强化学习系统

    Scalable Multi-Agent Reinforcement Learning with General Utilities. (arXiv:2302.07938v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07938](http://arxiv.org/abs/2302.07938)

    本论文研究了具有通用效用的可扩展多智能体强化学习，并提出了一种基于分布式策略梯度算法的解决方案，通过利用网络结构的空间相关衰减性质实现了算法的收敛性。

    

    我们研究了具有通用效用的可扩展多智能体强化学习（MARL），其中通用效用被定义为团队长期状态-动作占有率测度的非线性函数。我们的目标是找到一个局部策略，最大化团队局部效用函数的平均值，而不需要完全观测团队中的每个智能体。通过利用网络结构的空间相关衰减性质，我们提出了一种可扩展的分布式策略梯度算法，其中包括三个步骤：（1）阴影奖励估计，（2）截断阴影Q函数估计，以及（3）截断策略梯度估计和策略更新。我们的算法收敛于$\epsilon$-稳定性，高概率下需要$\widetilde{\mathcal{O}}(\epsilon^{-2})$个样本，直到一定程度上的近似误差以指数速度减小到通信半径内。这是关于具有通用效用的多智能体强化学习的文献中的首个结果。

    We study the scalable multi-agent reinforcement learning (MARL) with general utilities, defined as nonlinear functions of the team's long-term state-action occupancy measure. The objective is to find a localized policy that maximizes the average of the team's local utility functions without the full observability of each agent in the team. By exploiting the spatial correlation decay property of the network structure, we propose a scalable distributed policy gradient algorithm with shadow reward and localized policy that consists of three steps: (1) shadow reward estimation, (2) truncated shadow Q-function estimation, and (3) truncated policy gradient estimation and policy update. Our algorithm converges, with high probability, to $\epsilon$-stationarity with $\widetilde{\mathcal{O}}(\epsilon^{-2})$ samples up to some approximation error that decreases exponentially in the communication radius. This is the first result in the literature on multi-agent RL with general utilities that does
    
[^135]: 数据中心机器学习的重新标签法

    The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04391](http://arxiv.org/abs/2302.04391)

    本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。

    

    在深度学习应用中，手动标记的数据在一定程度上存在噪声。为了解决这个问题，并在开发数据集上获得90分以上的成绩，本文提出了一种简单的方法来找出噪声数据，并通过采用模型预测作为人类标记的参考来重新标记噪声数据。本文阐述了我们在广泛的深度学习任务中的想法，包括分类、序列标记、物体检测、序列生成、点击率预测。实验结果和人类评估结果验证了我们的想法。

    In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
    
[^136]: 通过隐形水印保护语言生成模型

    Protecting Language Generation Models via Invisible Watermarking. (arXiv:2302.03162v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.03162](http://arxiv.org/abs/2302.03162)

    本文提出了一种名为 GINSEW 的新方法，通过将秘密信号注入到每个目标标记的解码步骤的概率向量中，保护文本生成模型，有效识别出侵权行为，对模型的影响很小。

    

    语言生成模型是许多应用的有力支持者。许多这样的模型提供免费或经济实惠的 API 访问，这使它们可能受到模型抽取攻击的威胁。为了保护知识产权并确保这些模型的公正使用，已经提出了各种技术，例如词汇水印和同义词替换。然而，这些方法可能会被明显的对策如“同义词随机化”等所抵消。为了解决这个问题，我们提出了 GINSEW，一种新的方法，用于通过蒸馏保护文本生成模型。我们的方法的关键思想是将秘密信号注入到每个目标标记的解码步骤的概率向量中。然后，我们可以通过探测嫌疑的模型来检测秘密消息是否由受保护的模型蒸馏而来。实验结果表明，GINSEW 可以有效地识别出侵权行为，对生成模型的影响极小。

    Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as "synonym randomization". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the ge
    
[^137]: 对鲁棒性学习的插值方法：基于Wasserstein测地线的数据增强

    Interpolation for Robust Learning: Data Augmentation on Wasserstein Geodesics. (arXiv:2302.02092v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02092](http://arxiv.org/abs/2302.02092)

    本文提出一种通过插值训练数据分布来提升模型鲁棒性的方法。通过寻找连接不同类别子人口分布的测地线上的最坏情况Wasserstein barycenter来增加数据，并对模型进行正则化以获得更平滑的性能。我们的方法在多个数据集上进行了实验证实，并改进了基线的可证明鲁棒性和经验鲁棒性。该研究从Wasserstein测地线的角度探索了模型鲁棒性。

    

    我们提出通过插值训练数据分布来研究和提升模型的鲁棒性能。具体地，我们通过找到连接不同类别子人口分布的测地线上的最坏情况Wasserstein barycenter来增加数据；我们对模型进行正则化，使其在连接子人口分布的连续测地线路径上具有更平滑的性能；此外，我们还提供了一种理论保证鲁棒性改进并研究测地线位置和样本大小的贡献。在包括CIFAR-100和ImageNet在内的四个数据集上进行实验证实了我们方法的有效性，例如，我们的方法在CIFAR10上提高了基线的可证明鲁棒性达到7.7%，在CIFAR-100上提高了16.8%的经验鲁棒性。我们的工作通过Wasserstein测地线揭示了模型鲁棒性的新视角。

    We propose to study and promote the robustness of a model as per its performance through the interpolation of training data distributions. Specifically, (1) we augment the data by finding the worst-case Wasserstein barycenter on the geodesic connecting subpopulation distributions of different categories. (2) We regularize the model for smoother performance on the continuous geodesic path connecting subpopulation distributions. (3) Additionally, we provide a theoretical guarantee of robustness improvement and investigate how the geodesic location and the sample size contribute, respectively. Experimental validations of the proposed strategy on \textit{four} datasets, including CIFAR-100 and ImageNet, establish the efficacy of our method, e.g., our method improves the baselines' certifiable robustness on CIFAR10 up to $7.7\%$, with $16.8\%$ on empirical robustness on CIFAR-100. Our work provides a new perspective of model robustness through the lens of Wasserstein geodesic-based interpol
    
[^138]: 用模块化的继任特征逼近器组合任务知识

    Composing Task Knowledge with Modular Successor Feature Approximators. (arXiv:2301.12305v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12305](http://arxiv.org/abs/2301.12305)

    本研究提出了一种新颖的神经网络架构，"模块化继任特征逼近器"（MSFA），通过让模块发现有用于预测的特征并学习自己的预测表示，实现了更好的泛化能力。

    

    最近，已经提出了继任特征和广义策略改进（SF&GPI）框架作为学习、组合和转移预测知识和行为的方法。SF&GPI通过让代理学习能够结合GPI进行任务转移的预测表示（SFs）。然而，为了有效，这种方法需要有用于预测的状态特征，而这些状态特征通常是手工设计的。在这项工作中，我们提出了一种新颖的神经网络架构，"模块化继任特征逼近器"（MSFA），其中模块既可以发现有用于预测的特征，也可以学习自己的预测表示。我们展示了MSFA相比基准架构来学习SFs和模块化架构具有更好的泛化能力。

    Recently, the Successor Features and Generalized Policy Improvement (SF&GPI) framework has been proposed as a method for learning, composing, and transferring predictive knowledge and behavior. SF&GPI works by having an agent learn predictive representations (SFs) that can be combined for transfer to new tasks with GPI. However, to be effective this approach requires state features that are useful to predict, and these state-features are typically hand-designed. In this work, we present a novel neural network architecture, "Modular Successor Feature Approximators" (MSFA), where modules both discover what is useful to predict, and learn their own predictive representations. We show that MSFA is able to better generalize compared to baseline architectures for learning SFs and modular architectures
    
[^139]: 带有原子公式一跳推理的逻辑消息传递网络

    Logical Message Passing Networks with One-hop Inference on Atomic Formulas. (arXiv:2301.08859v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08859](http://arxiv.org/abs/2301.08859)

    本文提出了一个逻辑消息传递神经网络（LMPNN）框架，通过将集合操作分解成神经集合操作符来进行复杂查询回答，并将将原子公式的局部单跳推理连接到全局逻辑推理中。

    

    知识图谱中的复杂查询回答已经引起了广泛关注。考虑到知识图谱通常是不完整的，因此提出了使用神经模型通过将集合操作参数化为复杂神经网络来回答逻辑查询的方法。然而，这种方法通常使用大量的实体和关系嵌入来训练神经集合操作符，而嵌入或神经集合操作符如何对性能做出贡献仍不清楚。本文提出了一个简单的复杂查询回答框架，从神经集合操作符中分解出知识图谱嵌入。我们将复杂查询表示为查询图，并在查询图的基础上，提出了逻辑消息传递神经网络 (LMPNN)，将原子公式的局部单跳推理连接到复杂查询回答的全局逻辑推理中。我们利用现有的有效知识图谱嵌入进行推理。

    Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lot of attention to potentially support many applications. Given that KGs are usually incomplete, neural models are proposed to answer the logical queries by parameterizing set operators with complex neural networks. However, such methods usually train neural set operators with a large number of entity and relation embeddings from the zero, where whether and how the embeddings or the neural set operators contribute to the performance remains not clear. In this paper, we propose a simple framework for complex query answering that decomposes the KG embeddings from neural set operators. We propose to represent the complex queries into the query graph. On top of the query graph, we propose the Logical Message Passing Neural Network (LMPNN) that connects the local one-hop inferences on atomic formulas to the global logical reasoning for complex query answering. We leverage existing effective KG embeddings to conduct o
    
[^140]: 重温外行星种群的质量-半径关系：机器学习的洞见。

    Revisiting mass-radius relationships for exoplanet populations: a machine learning insight. (arXiv:2301.07143v2 [astro-ph.EP] UPDATED)

    [http://arxiv.org/abs/2301.07143](http://arxiv.org/abs/2301.07143)

    这项研究利用机器学习方法分析了762个已确认的外行星和八个太阳系行星的数据集，发现巨大行星具有较低的密度，主要由氢和氦构成，而小行星更密集，主要由更重的元素构成。研究还揭示了质量、轨道周期和恒星金属丰度对外行星半径的预测能力的重要性。

    

    外行星的发现数量正在增长，并且机器学习技术的进步为探索和理解我们太阳系以外的世界的特性开辟了新的途径。在这项研究中，我们使用高效的机器学习方法来分析包括762个已确认的外行星和八个太阳系行星的数据集，旨在表征它们的基本性质。通过应用不同的无监督聚类算法，我们将数据分成了两个主要类别：‘小’行星和‘巨大’行星，切割值分别为$R_{p}=8.13R_{\oplus}$和$M_{p}=52.48M_{\oplus}$。这种分类揭示了一个有趣的区别：巨大行星的密度较低，暗示它们具有更高的氢-氦质量分数，而小行星更密集，主要由更重的元素组成。我们应用了各种回归模型来揭示物理参数之间的相关性以及它们对外行星半径的预测能力。我们的分析强调了行星质量、轨道 période 和恒星金属丰度的重要性。

    The growing number of exoplanet discoveries and advances in machine learning techniques have opened new avenues for exploring and understanding the characteristics of worlds beyond our Solar System. In this study, we employ efficient machine learning approaches to analyze a dataset comprising 762 confirmed exoplanets and eight Solar System planets, aiming to characterize their fundamental quantities. By applying different unsupervised clustering algorithms, we classify the data into two main classes: 'small' and 'giant' planets, with cut-off values at $R_{p}=8.13R_{\oplus}$ and $M_{p}=52.48M_{\oplus}$. This classification reveals an intriguing distinction: giant planets have lower densities, suggesting higher H-He mass fractions, while small planets are denser, composed mainly of heavier elements. We apply various regression models to uncover correlations between physical parameters and their predictive power for exoplanet radius. Our analysis highlights that planetary mass, orbital pe
    
[^141]: 风能和能量储存协调在批发能源和辅助服务市场中的深度强化学习

    Deep Reinforcement Learning for Wind and Energy Storage Coordination in Wholesale Energy and Ancillary Service Markets. (arXiv:2212.13368v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.13368](http://arxiv.org/abs/2212.13368)

    本研究提出了一种基于深度强化学习的方法，以提高风能和能量存储系统的市场参与能力。该方法解决了协调复杂性和能源价格随机性的挑战，并实现了平衡现场风电削减和市场投标的目标。

    

    风能作为减缓气候变化的手段已经越来越受到采用。然而，风能的可变性导致了风电厂的削减，给风电厂主带来了可观的经济损失。通过将电池储能系统 (BESS) 用作现场备用源，可以减少风电削减。然而，这种辅助角色可能会严重削弱BESS在能源交易中的经济潜力。理想的BESS调度应该平衡现场风电削减和市场投标，但由于协调复杂性和能源价格和风发电的随机性，实际实施很具有挑战性。我们研究了同地风电-电池系统在现货和调频频率控制辅助服务市场中的联合市场投标策略。我们提出了一种新颖的基于深度强化学习的方法，将系统的市场参与分解为每个设施的两个相关马尔可夫决策过程，使BESS能够吸收现场风能

    Wind energy has been increasingly adopted to mitigate climate change. However, the variability of wind energy causes wind curtailment, resulting in considerable economic losses for wind farm owners. Wind curtailment can be reduced using battery energy storage systems (BESS) as onsite backup sources. Yet, this auxiliary role may significantly weaken the economic potential of BESS in energy trading. Ideal BESS scheduling should balance onsite wind curtailment reduction and market bidding, but practical implementation is challenging due to coordination complexity and the stochastic nature of energy prices and wind generation. We investigate the joint-market bidding strategy of a co-located wind-battery system in the spot and Regulation Frequency Control Ancillary Service markets. We propose a novel deep reinforcement learning-based approach that decouples the system's market participation into two related Markov decision processes for each facility, enabling the BESS to absorb onsite wind
    
[^142]: 解决竞争性多智能体决策和控制问题的主动学习方法

    An active learning method for solving competitive multi-agent decision-making and control problems. (arXiv:2212.12561v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.12561](http://arxiv.org/abs/2212.12561)

    我们提出了一个基于主动学习的方法，用于解决竞争性多智能体决策和控制问题。通过重构私有策略和预测稳态行动配置文件，外部观察者可以成功进行预测和优化策略。

    

    我们提出了一种基于主动学习的方案，用于重构由相互作用代理人群体执行的私有策略，并预测底层多智能体交互过程的确切结果，这里被认为是一个稳定的行动配置文件。我们设想了一个场景，在这个场景中，一个具有学习程序的外部观察者可以通过私有的行动-反应映射进行查询和观察代理人的反应，集体的不动点对应于一个稳态配置文件。通过迭代地收集有意义的数据和更新行动-反应映射的参数估计，我们建立了评估所提出的主动学习方法的渐近性质的充分条件，以便如果收敛发生，它只能朝向一个稳态行动配置文件。这一事实导致了两个主要结果：i）学习局部精确的行动-反应映射替代物使得外部观察者能够成功完成其预测任务，ii）与代理人的互动提供了一种方法来优化策略以达到最佳效果。

    We propose a scheme based on active learning to reconstruct private strategies executed by a population of interacting agents and predict an exact outcome of the underlying multi-agent interaction process, here identified as a stationary action profile. We envision a scenario where an external observer, endowed with a learning procedure, can make queries and observe the agents' reactions through private action-reaction mappings, whose collective fixed point corresponds to a stationary profile. By iteratively collecting sensible data and updating parametric estimates of the action-reaction mappings, we establish sufficient conditions to assess the asymptotic properties of the proposed active learning methodology so that, if convergence happens, it can only be towards a stationary action profile. This fact yields two main consequences: i) learning locally-exact surrogates of the action-reaction mappings allows the external observer to succeed in its prediction task, and ii) working with 
    
[^143]: 基于深度展开的异构环境联邦学习加权平均方法

    Deep Unfolding-based Weighted Averaging for Federated Learning in Heterogeneous Environments. (arXiv:2212.12191v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12191](http://arxiv.org/abs/2212.12191)

    本文提出了一种基于深度展开的加权平均方法，用于解决异质环境下的联邦学习问题。通过直接将环境的异质性纳入聚合权重的调整中，该方法能够获得更高的未知类平衡数据的测试准确性。

    

    联邦学习是一种协作模型训练方法，通过多个客户端的模型更新和中央服务器的聚合来迭代模型更新。参与客户端的设备和统计异质性导致了显著的性能下降，因此在服务器的聚合阶段应为每个客户端分配适当的聚合权重。为了调整聚合权重，本文采用了深度展开方法，这是一种利用训练数据进行学习的能力和领域知识的参数调整方法。这使得我们能够直接将感兴趣环境的异质性纳入聚合权重的调整中。所提出的方法可以与各种联邦学习算法结合使用。数值实验证明，与传统的启发式加权方法相比，所提出的方法可以获得更高的未知类平衡数据的测试准确性。

    Federated learning is a collaborative model training method that iterates model updates by multiple clients and aggregation of the updates by a central server. Device and statistical heterogeneity of participating clients cause significant performance degradation so that an appropriate aggregation weight should be assigned to each client in the aggregation phase of the server. To adjust the aggregation weights, this paper employs deep unfolding, which is known as the parameter tuning method that leverages both learning capability using training data like deep learning and domain knowledge. This enables us to directly incorporate the heterogeneity of the environment of interest into the tuning of the aggregation weights. The proposed approach can be combined with various federated learning algorithms. The results of numerical experiments indicate that a higher test accuracy for unknown class-balanced data can be obtained with the proposed method than that with conventional heuristic wei
    
[^144]: 信号和双边信息不对称下的承诺问题

    Commitment with Signaling under Double-sided Information Asymmetry. (arXiv:2212.11446v3 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2212.11446](http://arxiv.org/abs/2212.11446)

    本论文研究了一种信号和双边信息不对称下的承诺问题，在贝叶斯斯塔克尔贝格博弈中，通过适当设计信号装置，领导者可以利用信息优势获得更高的预期效用。

    

    游戏中的信息不对称使得具有信息优势的玩家可以通过战略性地向其他玩家透露信息来操纵其信念。本研究考虑了贝叶斯斯塔克尔贝格博弈中的双边信息不对称，领导者的实际行动通过混合策略承诺从追随者隐藏起来。相反，追随者持有关于其收益的私人信息。在双方的非对称信息给出的情况下，一个重要的问题出现了：领导者的信息优势是否压倒追随者的信息优势？本研究肯定地回答了这个问题，我们证明通过适当设计一个信号装置，向追随者透露有关领导者实际行动的部分信息，领导者可以获得比没有信号的情况下更高的预期效用。此外，与以往的贝叶斯斯塔克尔贝格博弈中利用数学规划工具的研究不同，我们对这个博弈的解释是

    Information asymmetry in games enables players with the information advantage to manipulate others' beliefs by strategically revealing information to other players. This work considers a double-sided information asymmetry in a Bayesian Stackelberg game, where the leader's realized action, sampled from the mixed strategy commitment, is hidden from the follower. In contrast, the follower holds private information about his payoff. Given asymmetric information on both sides, an important question arises: \emph{Does the leader's information advantage outweigh the follower's?} We answer this question affirmatively in this work, where we demonstrate that by adequately designing a signaling device that reveals partial information regarding the leader's realized action to the follower, the leader can achieve a higher expected utility than that without signaling. Moreover, unlike previous works on the Bayesian Stackelberg game where mathematical programming tools are utilized, we interpret the 
    
[^145]: 不变Lipschitz赌徒：一个侧观发现方法

    Invariant Lipschitz Bandits: A Side Observation Approach. (arXiv:2212.07524v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07524](http://arxiv.org/abs/2212.07524)

    本文研究了不变Lipschitz赌徒设置，并提出了一种名为\texttt{UniformMesh-N}的算法。使用侧面观察的方法，证明了改进的遗憾上界。

    

    对称出现在许多优化和决策问题中，并吸引了优化界的相当关注：通过利用这样的对称性，可以显著改进寻找最优解的过程。尽管对称性在（离线）优化中取得成功，但在在线优化设置中，特别是在赌徒文献中，其利用还未得到充分的研究。因此，在本文中，我们研究了不变Lipschitz赌徒设置，这是Lipschitz赌徒的一个子类，在该子类中，奖励函数和臂集在一组变换下保持不变。我们引入了一种名为\texttt{UniformMesh-N}的算法，它自然地将侧面观察使用群轨道整合到\texttt{UniformMesh}算法（\cite{Kleinberg2005_UniformMesh}）中，该算法均匀地分割了臂的集合。通过侧面观察方法，我们证明了改进的遗憾上界，其取决于基数。

    Symmetry arises in many optimization and decision-making problems, and has attracted considerable attention from the optimization community: By utilizing the existence of such symmetries, the process of searching for optimal solutions can be improved significantly. Despite its success in (offline) optimization, the utilization of symmetries has not been well examined within the online optimization settings, especially in the bandit literature. As such, in this paper we study the invariant Lipschitz bandit setting, a subclass of the Lipschitz bandits where the reward function and the set of arms are preserved under a group of transformations. We introduce an algorithm named \texttt{UniformMesh-N}, which naturally integrates side observations using group orbits into the \texttt{UniformMesh} algorithm (\cite{Kleinberg2005_UniformMesh}), which uniformly discretizes the set of arms. Using the side-observation approach, we prove an improved regret upper bound, which depends on the cardinalit
    
[^146]: 面向5G基站流量预测的联邦学习

    Federated Learning for 5G Base Station Traffic Forecasting. (arXiv:2211.15220v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15220](http://arxiv.org/abs/2211.15220)

    本研究探讨了将联邦学习应用于原始基站LTE数据进行5G基站流量预测的效果。

    

    在推动5G移动网络实现智能高效基础设施规划和管理的过程中，细胞流量预测具有重要意义。然而，可用的数据仅限于基站日志信息。因此，需要训练方法来生成高质量的预测结果，可以推广到不同参与方的新观测中。传统方法需要从多个基站收集测量数据，将其传输到中央实体，并使用获取到的数据进行机器学习操作。本地观测结果的传播引发了关于保密性和性能的担忧，这影响了机器学习技术的适用性。尽管已提出各种分布式学习方法来解决这个问题，但它们在流量预测领域的应用仍然很少探索。在这项工作中，我们研究了将联邦学习应用于原始基站LTE数据进行时序流量预测的有效性。

    Cellular traffic prediction is of great importance on the path of enabling 5G mobile networks to perform intelligent and efficient infrastructure planning and management. However, available data are limited to base station logging information. Hence, training methods for generating high-quality predictions that can generalize to new observations across diverse parties are in demand. Traditional approaches require collecting measurements from multiple base stations, transmitting them to a central entity and conducting machine learning operations using the acquire data. The dissemination of local observations raises concerns regarding confidentiality and performance, which impede the applicability of machine learning techniques. Although various distributed learning methods have been proposed to address this issue, their application to traffic prediction remains highly unexplored. In this work, we investigate the efficacy of federated learning applied to raw base station LTE data for tim
    
[^147]: 研究强化学习智能体在个性化任务中的策略熵

    Examining Policy Entropy of Reinforcement Learning Agents for Personalization Tasks. (arXiv:2211.11869v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11869](http://arxiv.org/abs/2211.11869)

    研究了个性化任务中强化学习智能体的策略熵，并发现策略优化智能体在训练过程中往往具有低熵策略，然而Q学习智能体对此影响较小，通常保持高熵策略。

    

    本文着重研究了强化学习系统在个性化环境中的行为，并详细描述了不同学习算法所关联的策略熵的差异。我们证明了在训练过程中，策略优化智能体往往具有低熵策略，实际上导致智能体优先考虑某些动作而避免其他动作。相反地，我们也表明了Q学习智能体对这种行为的影响要小得多，并且通常在训练过程中保持高熵策略，这在实际应用中往往更可取。我们提供了各种数值实验以及理论上的证明，以表明这些熵差异是由所采用的学习类型所导致的。

    This effort is focused on examining the behavior of reinforcement learning systems in personalization environments and detailing the differences in policy entropy associated with the type of learning algorithm utilized. We demonstrate that Policy Optimization agents often possess low-entropy policies during training, which in practice results in agents prioritizing certain actions and avoiding others. Conversely, we also show that Q-Learning agents are far less susceptible to such behavior and generally maintain high-entropy policies throughout training, which is often preferable in real-world applications. We provide a wide range of numerical experiments as well as theoretical justification to show that these differences in entropy are due to the type of learning being employed.
    
[^148]: 一个低延迟自适应编码脉冲框架用于深度强化学习

    A Low Latency Adaptive Coding Spiking Framework for Deep Reinforcement Learning. (arXiv:2211.11760v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11760](http://arxiv.org/abs/2211.11760)

    本文提出了一个低延迟自适应编码脉冲框架用于深度强化学习，在编码器灵活性、延迟和能量效率方面具有优异性能和广泛应用范围。

    

    近年来，由于低功耗和事件驱动特性，脉冲神经网络（SNNs）被用于强化学习（RL）。然而，固定编码方法导致的脉冲强化学习（SRL）仍然面临高延迟和较差的灵活性问题。本文中，我们使用可学习的矩阵乘法对脉冲进行编码和解码，提高编码器的灵活性，从而降低延迟。同时，我们使用直接训练方法训练SNNs，并使用两种不同的结构用于在线和离线强化学习算法，使我们的模型拥有更广泛的应用范围。广泛的实验表明，我们的方法在不同的算法和不同的环境中实现了最佳性能，延迟极低（仅为其他SRL方法的0.8%）且具有极高的能量效率（高达DNNs的5倍）。

    In recent years, spiking neural networks (SNNs) have been used in reinforcement learning (RL) due to their low power consumption and event-driven features. However, spiking reinforcement learning (SRL), which suffers from fixed coding methods, still faces the problems of high latency and poor versatility. In this paper, we use learnable matrix multiplication to encode and decode spikes, improving the flexibility of the coders and thus reducing latency. Meanwhile, we train the SNNs using the direct training method and use two different structures for online and offline RL algorithms, which gives our model a wider range of applications. Extensive experiments have revealed that our method achieves optimal performance with ultra-low latency (as low as 0.8% of other SRL methods) and excellent energy efficiency (up to 5X the DNNs) in different algorithms and different environments.
    
[^149]: 人工智能用于X射线图像的自动检测和疾病分类

    Artificial Intelligence for Automatic Detection and Classification Disease on the X-Ray Images. (arXiv:2211.08244v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.08244](http://arxiv.org/abs/2211.08244)

    该论文研究了人工智能在X射线图像上的自动检测和疾病分类的应用。通过使用深度学习方法和RepVGG算法进行快速准确的肺部疾病检测，以实现早期疾病的诊断和预防进一步传播。

    

    在医学和研究领域中，使用X射线图像进行疾病的检测和分类是更具挑战性的核心任务之一。由于近期对放射学图像和人工智能的高度关注，通过X射线图像早期检测疾病变得更加重要，以预防进一步传播和压平曲线。计算机视觉与深度学习方法的创新和革命为快速准确地诊断筛查和检测胸部X射线图像（CXR）提供了巨大的希望。本研究提出了使用高效的深度学习预训练算法RepVGG进行深度特征提取和分类的肺疾病快速检测。我们使用X射线图像作为示例来展示模型的效率。为了完成这个任务，我们将X射线图像分为Covid-19、肺炎和正常的X射线图像进行分类。采用ROI对象来提高肺部提取的检测准确性，随后进行数据预处理和增强。我们正在应用人工智能方法...

    Detecting and classifying diseases using X-ray images is one of the more challenging core tasks in the medical and research world. Due to the recent high interest in radiological images and AI, early detection of diseases in X-ray images has become notably more essential to prevent further spreading and flatten the curve. Innovations and revolutions of Computer Vision with Deep learning methods offer great promise for fast and accurate diagnosis of screening and detection from chest X-ray images (CXR). This work presents rapid detection of diseases in the lung using the efficient Deep learning pre-trained RepVGG algorithm for deep feature extraction and classification. We used X-ray images as an example to show the model's efficiency. To perform this task, we classify X-Ray images into Covid-19, Pneumonia, and Normal X-Ray images. Employ ROI object to improve the detection accuracy for lung extraction, followed by data pre-processing and augmentation. We are applying Artificial Intelli
    
[^150]: QuadConv：基于定积分的卷积与非均匀PDE数据压缩的应用

    QuadConv: Quadrature-Based Convolutions with Applications to Non-Uniform PDE Data Compression. (arXiv:2211.05151v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05151](http://arxiv.org/abs/2211.05151)

    本论文提出了一种新的卷积层QuadConv，通过定积分对连续卷积进行近似。该方法适用于非均匀的基于网格的数据，并且在压缩偏微分方程（PDE）模拟数据时表现出与标准离散卷积相当的性能，甚至能够在非均匀数据上保持相同的准确性。QuadConv还优于其他非结构化卷积方法如图卷积。

    

    我们提出了一种新的用于深度学习架构的卷积层，我们将其称为QuadConv——通过定积分对连续卷积进行近似。我们的操作符专门针对非均匀的基于网格的数据进行开发，通过学习一个可以在任意位置进行采样的连续核来实现这一点。此外，我们的操作符的构建还允许高效的实现，我们在文中详细介绍并构造了此实现。作为对我们的操作符的实验验证，我们考虑了从固定网格中压缩偏微分方程（PDE）模拟数据的任务。通过将QuadConv自动编码器（QCAE）与标准卷积自动编码器（CAE）进行比较，我们证明QuadConv能够与标准离散卷积在均匀网格数据上的性能相匹配。此外，我们还展示了QCAE即使在非均匀数据上也能够保持这种准确性。在两种情况下，QuadConv还优于图卷积等其他非结构化卷积方法。

    We present a new convolution layer for deep learning architectures which we call QuadConv -- an approximation to continuous convolution via quadrature. Our operator is developed explicitly for use on non-uniform, mesh-based data, and accomplishes this by learning a continuous kernel that can be sampled at arbitrary locations. Moreover, the construction of our operator admits an efficient implementation which we detail and construct. As an experimental validation of our operator, we consider the task of compressing partial differential equation (PDE) simulation data from fixed meshes. We show that QuadConv can match the performance of standard discrete convolutions on uniform grid data by comparing a QuadConv autoencoder (QCAE) to a standard convolutional autoencoder (CAE). Further, we show that the QCAE can maintain this accuracy even on non-uniform data. In both cases, QuadConv also outperforms alternative unstructured convolution methods such as graph convolution.
    
[^151]: 从相邻的染色组织学片中学习黑色素细胞掩膜

    Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2211.00646](http://arxiv.org/abs/2211.00646)

    本文提出了一种从相邻染色组织学片中训练深度神经网络进行黑色素细胞分割的方法，实现了0.64的平均IOU，尽管存在不完美的标签。

    

    黑色素瘤是最具侵袭性的皮肤癌之一，导致大部分皮肤癌死亡。然而，病理学家对黑色素瘤的诊断可靠性较低。由于黑色素瘤是黑色素细胞的肿瘤，需要开发一种与病理学家的差异无关并能自动进行像素级注释的黑色素细胞分割工具。然而，大规模病理学家标注是不现实的。在本文中，我们提出了一种方法，使用邻近组织切片上的偶联免疫组织化学（IHC）染色片，训练深度神经网络进行黑色素细胞分割，虽然很难有完美的标签，但达到了0.64的平均IOU。

    Melanoma is one of the most aggressive forms of skin cancer, causing a large proportion of skin cancer deaths. However, melanoma diagnoses by pathologists shows low interrater reliability. As melanoma is a cancer of the melanocyte, there is a clear need to develop a melanocytic cell segmentation tool that is agnostic to pathologist variability and automates pixel-level annotation. Gigapixel-level pathologist labeling, however, is impractical. Herein, we propose a means to train deep neural networks for melanocytic cell segmentation from hematoxylin and eosin (H&E) stained slides using paired immunohistochemical (IHC) slides of adjacent tissue sections, achieving a mean IOU of 0.64 despite imperfect ground-truth labels.
    
[^152]: QNet: 一种原生于量子计算机的序列编码器架构

    QNet: A Quantum-native Sequence Encoder Architecture. (arXiv:2210.17262v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17262](http://arxiv.org/abs/2210.17262)

    本研究探讨了在近期量子计算机上进行机器学习在序列数据中的优势，通过在自然语言处理任务中进行实验，提出了一种全量子计算机推理的新型序列编码器模型QNet，以及一个量子-经典混合模型ResQNet。这些模型在经典的最先进模型上表现出了引人注目的性能，而且参数数量更少。

    

    本研究提出了一种全量子计算机推理的新型序列编码器模型QNet，使用最少数量的量子比特。在该模型中，点积注意力机制的时间复杂度为$O(n^2 \cdot d)$，而QNet的量子电路深度仅为$O(n+d)$。此外，我们介绍了ResQNet，一个由多个由残差连接相连的QNet模块组成的量子-经典混合模型，作为同构Transformer编码器。我们在各种自然语言处理任务中对我们的工作进行了评估，包括文本分类、评分预测和命名实体识别。我们的模型在经典的最先进模型上表现出了引人注目的性能，参数数量少了一千倍。

    This work proposes QNet, a novel sequence encoder model that entirely inferences on the quantum computer using a minimum number of qubits. Let $n$ and $d$ represent the length of the sequence and the embedding size, respectively. The dot-product attention mechanism requires a time complexity of $O(n^2 \cdot d)$, while QNet has merely $O(n+d)$ quantum circuit depth. In addition, we introduce ResQNet, a quantum-classical hybrid model composed of several QNet blocks linked by residual connections, as an isomorph Transformer Encoder. We evaluated our work on various natural language processing tasks, including text classification, rating score prediction, and named entity recognition. Our models exhibit compelling performance over classical state-of-the-art models with a thousand times fewer parameters. In summary, this work investigates the advantage of machine learning on near-term quantum computers in sequential data by experimenting with natural language processing tasks.
    
[^153]: TuneUp:一种简单的改进的图神经网络训练策略

    TuneUp: A Simple Improved Training Strategy for Graph Neural Networks. (arXiv:2210.14843v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.14843](http://arxiv.org/abs/2210.14843)

    TuneUp是一种简单的基于课程的训练策略，用于改进图神经网络在难以预测的尾节点上的泛化性能。

    

    尽管图神经网络（GNN）在近期取得了许多进展，但它们的训练策略仍然未被充分探索。传统的训练策略对原始图中的所有节点进行平等学习，这可能是次优的，因为某些节点往往比其他节点更难学习。在这里，我们提出了TuneUp，一种简单的基于课程的训练策略，用于提高GNN的预测性能。TuneUp将GNN分为两个阶段进行训练。在第一阶段，TuneUp应用传统的训练方法，获得一个强大的基础GNN。基础GNN在头节点（具有大度数的节点）上表现良好，但在尾节点（具有小度数的节点）上表现较差。因此，TuneUp的第二阶段侧重于通过进一步训练基础GNN以在难以预测的尾节点上提高预测能力。我们在理论上分析了TuneUp，并证明它能够改善尾节点的泛化性能。TuneUp实现简单，适用于广泛的范围。

    Despite recent advances in Graph Neural Networks (GNNs), their training strategies remain largely under-explored. The conventional training strategy learns over all nodes in the original graph(s) equally, which can be sub-optimal as certain nodes are often more difficult to learn than others. Here we present TuneUp, a simple curriculum-based training strategy for improving the predictive performance of GNNs. TuneUp trains a GNN in two stages. In the first stage, TuneUp applies conventional training to obtain a strong base GNN. The base GNN tends to perform well on head nodes (nodes with large degrees) but less so on tail nodes (nodes with small degrees). Therefore, the second stage of TuneUp focuses on improving prediction on the difficult tail nodes by further training the base GNN on synthetically generated tail node data. We theoretically analyze TuneUp and show it provably improves generalization performance on tail nodes. TuneUp is simple to implement and applicable to a broad ran
    
[^154]: 分布转移的充分不变学习

    Sufficient Invariant Learning for Distribution Shift. (arXiv:2210.13533v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13533](http://arxiv.org/abs/2210.13533)

    本文研究了分布转移情况下的充分不变学习，观察到之前的工作只学习了部分不变特征，我们提出了学习充分不变特征的重要性，并指出在分布转移时，从训练集中学习的部分不变特征可能不适用于测试集，限制了性能提升。

    

    机器学习算法在各种应用中展现出了卓越的性能。然而，在训练集和测试集的分布不同的情况下，保证性能仍然具有挑战性。为了改善分布转移情况下的性能，已经提出了一些方法，通过学习跨组或领域的不变特征来提高性能。然而，我们观察到之前的工作只部分地学习了不变特征。虽然先前的工作侧重于有限的不变特征，但我们首次提出了充分不变特征的重要性。由于只有训练集是经验性的，从训练集中学习得到的部分不变特征可能不存在于分布转移时的测试集中。因此，分布转移情况下的性能提高可能受到限制。本文认为从训练集中学习充分的不变特征对于分布转移情况至关重要。

    Machine learning algorithms have shown remarkable performance in diverse applications. However, it is still challenging to guarantee performance in distribution shifts when distributions of training and test datasets are different. There have been several approaches to improve the performance in distribution shift cases by learning invariant features across groups or domains. However, we observe that the previous works only learn invariant features partially. While the prior works focus on the limited invariant features, we first raise the importance of the sufficient invariant features. Since only training sets are given empirically, the learned partial invariant features from training sets might not be present in the test sets under distribution shift. Therefore, the performance improvement on distribution shifts might be limited. In this paper, we argue that learning sufficient invariant features from the training set is crucial for the distribution shift case. Concretely, we newly 
    
[^155]: 可微约束模仿学习在机器人运动规划和控制中的应用

    Differentiable Constrained Imitation Learning for Robot Motion Planning and Control. (arXiv:2210.11796v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.11796](http://arxiv.org/abs/2210.11796)

    本论文介绍了一种将可微约束模仿学习与硬约束的直接方法相结合的机器人运动规划和控制方法，该方法通过学习离线演示中的决策制定，为复杂的机器人应用提供了一种有希望的途径。

    

    运动规划和控制是自动驾驶等机器人应用中的关键组成部分。然而，系统动力学和安全边界（如障碍物）等时空限制限制了机器人的运动。本研究将可微约束模仿学习与硬约束的结合，通过从离线演示中学习决策制定的直接方法，为复杂的机器人应用中的规划和控制提供了有希望的途径。

    Motion planning and control are crucial components of robotics applications like automated driving. Here, spatio-temporal hard constraints like system dynamics and safety boundaries (e.g., obstacles) restrict the robot's motions. Direct methods from optimal control solve a constrained optimization problem. However, in many applications finding a proper cost function is inherently difficult because of the weighting of partially conflicting objectives. On the other hand, Imitation Learning (IL) methods such as Behavior Cloning (BC) provide an intuitive framework for learning decision-making from offline demonstrations and constitute a promising avenue for planning and control in complex robot applications. Prior work primarily relied on soft constraint approaches, which use additional auxiliary loss terms describing the constraints. However, catastrophic safety-critical failures might occur in out-of-distribution (OOD) scenarios. This work integrates the flexibility of IL with hard const
    
[^156]: 多视角推理：一致的对比学习用于数学应用问题

    Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem. (arXiv:2210.11694v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.11694](http://arxiv.org/abs/2210.11694)

    该论文提出了一种使用多视角一致的对比学习来解决数学应用问题，通过同时考虑自上而下和自下而上的推理视角，以及多种等价的方程形式，实现了更完整的语义到方程的映射。实验证明该方法明显优于现有的基线。

    

    数学应用问题求解器需要对文本中的数量进行精确的关系推理和可靠的方程生成。当前的序列到树或关系抽取方法只从一个固定视角看待这个问题，很难同时处理复杂的语义和多样的方程。然而，人类解题自然地涉及两种一致的推理视角：自上而下和自下而上，就像数学方程也可以用多种等价形式表示：前序和后序。我们提出了一种多视角一致的对比学习，用于更完整的语义到方程的映射。整个过程被分解为两个独立但一致的视角：自上而下的分解和自下而上的构建，并且两种推理视角在多粒度上对齐以保持一致性，增强全局生成和精确推理。在两种语言的多个数据集上的实验证明我们的方法明显优于现有的基线。

    Math word problem solver requires both precise relation reasoning about quantities in the text and reliable generation for the diverse equation. Current sequence-to-tree or relation extraction methods regard this only from a fixed view, struggling to simultaneously handle complex semantics and diverse equations. However, human solving naturally involves two consistent reasoning views: top-down and bottom-up, just as math equations also can be expressed in multiple equivalent forms: pre-order and post-order. We propose a multi-view consistent contrastive learning for a more complete semantics-to-equation mapping. The entire process is decoupled into two independent but consistent views: top-down decomposition and bottom-up construction, and the two reasoning views are aligned in multi-granularity for consistency, enhancing global generation and precise reasoning. Experiments on multiple datasets across two languages show our approach significantly outperforms the existing baselines, esp
    
[^157]: 力量并不足够：用分子模拟进行机器学习力场的基准测试和关键评估

    Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations. (arXiv:2210.07237v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2210.07237](http://arxiv.org/abs/2210.07237)

    这项研究引入了一种新的学习MD模拟基准套件，用于评估机器学习力场模型，并证明了目前常用的力精度测试与模拟结果不一致的情况。

    

    分子动力学（MD）模拟技术广泛应用于各种自然科学应用中。机器学习（ML）力场（FF）模型越来越多地开始取代从原子结构直接预测力的从头算法模拟。尽管在这个领域取得了重要进展，但这些技术主要通过力/能预测误差进行基准测试，尽管实际使用情况是产生逼真的MD轨迹。我们旨在通过引入新的学习MD模拟基准套件来填补这一空白。我们策划了代表性的MD系统，包括水，有机分子，肽和材料，并设计与各个系统的科学目标相对应的评估指标。我们对一系列最新的ML FF模型进行基准测试，并阐明了通常被基准测试的力精度与相关模拟指标不一致的情况。我们演示了选择的SOTA方法何时以及如何失败，

    Molecular dynamics (MD) simulation techniques are widely used for various natural science applications. Increasingly, machine learning (ML) force field (FF) models begin to replace ab-initio simulations by predicting forces directly from atomic structures. Despite significant progress in this area, such techniques are primarily benchmarked by their force/energy prediction errors, even though the practical use case would be to produce realistic MD trajectories. We aim to fill this gap by introducing a novel benchmark suite for learned MD simulation. We curate representative MD systems, including water, organic molecules, a peptide, and materials, and design evaluation metrics corresponding to the scientific objectives of respective systems. We benchmark a collection of state-of-the-art (SOTA) ML FF models and illustrate, in particular, how the commonly benchmarked force accuracy is not well aligned with relevant simulation metrics. We demonstrate when and how selected SOTA methods fail,
    
[^158]: 良性自编码器

    Benign Autoencoders. (arXiv:2210.00637v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00637](http://arxiv.org/abs/2210.00637)

    本文正式化了用于生成式人工智能中编码器-解码器对的最佳选择问题并提出了良性自编码器（BAE），BAE能够将数据投射到最优的流型上，实现了数据压缩和更加稳定的梯度下降。

    

    最近，生成式人工智能取得了很多进展，其中常采用编码器-解码器架构来实现数据的高效表示。本论文正式化了寻找最佳编码器-解码器对的数学问题并表征其解决方案，我们将其命名为“良性自编码器”（BAE）。我们证明BAE将数据投射到一个流型上，其维数为生成问题的最佳可压缩维度。我们强调BAE与人工智能中几个最近发展的方向之间的惊人联系，如有条件的GAN，上下文编码器，稳定扩散，堆叠自编码器和生成模型的学习能力。我们展示了BAE如何找到最优的低维潜在表示，从而在分布转移下提高鉴别器的性能。通过压缩“恶性”数据维度，BAE导致梯度更加平滑和稳定。

    Recent progress in Generative Artificial Intelligence (AI) relies on efficient data representations, often featuring encoder-decoder architectures. We formalize the mathematical problem of finding the optimal encoder-decoder pair and characterize its solution, which we name the "benign autoencoder" (BAE). We prove that BAE projects data onto a manifold whose dimension is the optimal compressibility dimension of the generative problem. We highlight surprising connections between BAE and several recent developments in AI, such as conditional GANs, context encoders, stable diffusion, stacked autoencoders, and the learning capabilities of generative models. As an illustration, we show how BAE can find optimal, low-dimensional latent representations that improve the performance of a discriminator under a distribution shift. By compressing "malignant" data dimensions, BAE leads to smoother and more stable gradients.
    
[^159]: 基于奖励塑造的深度强化学习在机器人导航中的泛化能力研究

    Generalization in Deep Reinforcement Learning for Robotic Navigation by Reward Shaping. (arXiv:2209.14271v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.14271](http://arxiv.org/abs/2209.14271)

    本文研究了深度强化学习算法在机器人导航中的应用，提出了一种新的奖励函数，结合训练阶段获取的地图信息，使机器人能够在未知环境中避免局部极小值，并使用SAC算法进行训练，相比其他算法更有效。

    

    本文研究了深度强化学习算法在局部导航问题中的应用，其中机器人在未知且杂乱的工作区域中，只配备有受限范围外部感知传感器（如LiDAR），向目标位置移动。基于深度强化学习的碰撞避免策略具有一些优势，但它们对局部极小值非常敏感，因为它们只能在传感器范围内学习适当的动作。由于大多数机器人在非结构化环境中执行任务，寻求能够避免局部极小值的泛化局部导航策略对我们来说非常有意义，特别是在非训练场景中。为了实现这一目标，我们提出了一种新颖的奖励函数，该函数融合了在训练阶段获取的地图信息，增加了Agent的谨慎考虑最佳行动方案的能力。此外，我们使用SAC算法来训练我们的ANN，该算法在最新的文献中显示出比其他算法更有效。

    In this paper, we study the application of DRL algorithms in the context of local navigation problems, in which a robot moves towards a goal location in unknown and cluttered workspaces equipped only with limited-range exteroceptive sensors, such as LiDAR. Collision avoidance policies based on DRL present some advantages, but they are quite susceptible to local minima, once their capacity to learn suitable actions is limited to the sensor range. Since most robots perform tasks in unstructured environments, it is of great interest to seek generalized local navigation policies capable of avoiding local minima, especially in untrained scenarios. To do so, we propose a novel reward function that incorporates map information gained in the training stage, increasing the agent's capacity to deliberate about the best course of action. Also, we use the SAC algorithm for training our ANN, which shows to be more effective than others in the state-of-the-art literature. A set of sim-to-sim and sim
    
[^160]: 随机森林对非有目标数据污染的鲁棒性：一种基于集合的方法

    On the Robustness of Random Forest Against Untargeted Data Poisoning: An Ensemble-Based Approach. (arXiv:2209.14013v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14013](http://arxiv.org/abs/2209.14013)

    本文提出了一种基于集合的方法来提高随机森林对非有目标数据污染的鲁棒性

    

    机器学习正在变得无处不在。从金融到医学，机器学习模型正在提高决策过程，甚至在某些任务中表现优于人类。然而，在模型和相应预测的安全性方面，相对预测质量取得了巨大进展的情况并没有相应的得到保证。在过去的十年中，对污染攻击和防御的研究得到了越来越多的关注，

    Machine learning is becoming ubiquitous. From finance to medicine, machine learning models are boosting decision-making processes and even outperforming humans in some tasks. This huge progress in terms of prediction quality does not however find a counterpart in the security of such models and corresponding predictions, where perturbations of fractions of the training set (poisoning) can seriously undermine the model accuracy. Research on poisoning attacks and defenses received increasing attention in the last decade, leading to several promising solutions aiming to increase the robustness of machine learning. Among them, ensemble-based defenses, where different models are trained on portions of the training set and their predictions are then aggregated, provide strong theoretical guarantees at the price of a linear overhead. Surprisingly, ensemble-based defenses, which do not pose any restrictions on the base model, have not been applied to increase the robustness of random forest mo
    
[^161]: 局部上下文感知的主动域自适应

    Local Context-Aware Active Domain Adaptation. (arXiv:2208.12856v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12856](http://arxiv.org/abs/2208.12856)

    本文提出了一种局部上下文感知的主动域自适应框架 LADA，通过引入局部不一致性标准选择信息量更丰富的目标样本，并以类平衡的方式逐步增加标注的目标数据与自信的邻居。实验证实 LADA 方法在各种基准测试中明显优于最近的 ADA 方法。

    

    主动域自适应（ADA）通过查询少量选择的目标样本的标签来帮助将模型从源域适应到目标域。当存在较大的域差距时，查询数据的局部上下文非常重要。然而，现有的ADA方法并没有充分探索这一点。在本文中，我们提出了一种名为LADA的局部上下文感知的ADA框架来解决这个问题。为了选择信息量丰富的目标样本，我们设计了一种基于模型预测的局部不一致性的新标准。由于标注预算通常很小，仅在查询数据上微调模型可能效率低下。我们以类平衡的方式逐步增加标注的目标数据与自信的邻居。实验证实，所提出的标准选择的目标样本比现有的主动选择策略更具信息量。此外，我们的完整方法在各种基准测试中明显优于最近的ADA方法。代码可在https://github.com/tsu获得。

    Active Domain Adaptation (ADA) queries the labels of a small number of selected target samples to help adapting a model from a source domain to a target domain. The local context of queried data is important, especially when the domain gap is large. However, this has not been fully explored by existing ADA works. In this paper, we propose a Local context-aware ADA framework, named LADA, to address this issue. To select informative target samples, we devise a novel criterion based on the local inconsistency of model predictions. Since the labeling budget is usually small, fine-tuning model on only queried data can be inefficient. We progressively augment labeled target data with the confident neighbors in a class-balanced manner. Experiments validate that the proposed criterion chooses more informative target samples than existing active selection strategies. Furthermore, our full method clearly surpasses recent ADA arts on various benchmarks. Code is available at https://github.com/tsu
    
[^162]: 离线强化学习中的扩散策略作为表达性策略类的研究

    Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning. (arXiv:2208.06193v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.06193](http://arxiv.org/abs/2208.06193)

    本文提出了一种将策略表示为扩散模型的方法，用于离线强化学习。我们引入了Diffusion Q-learning（Diffusion-QL），利用条件扩散模型表示策略，并通过最大化动作值来寻求接近行为策略的最优动作。

    

    离线强化学习是通过利用先前收集的静态数据集来学习最优策略的重要强化学习范式。标准的强化学习方法在这种情况下通常表现不佳，原因是在分布不匹配的行为上存在函数逼近误差。尽管已经提出了各种正则化方法来缓解这个问题，但它们往往受限于具有有限表达能力的策略类，可能导致高度次优的解决方案。本文提出了以扩散模型作为策略表示的方法，这是一种近期出现的高度表达能力的深度生成模型类。我们引入了扩散Q-learning（Diffusion-QL），利用条件扩散模型来表示策略。在我们的方法中，我们学习一个动作值函数，并将最大化动作值的项加入到条件扩散模型的训练损失中，从而得到一个寻求接近行为策略的最优动作的损失函数。我们展示了扩散策略的表达能力。

    Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Q-learning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness
    
[^163]: 基于MDLatLRRv2的医学图像融合方法

    A Medical Image Fusion Method based on MDLatLRRv2. (arXiv:2206.15179v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2206.15179](http://arxiv.org/abs/2206.15179)

    该论文提出了一种基于MDLatLRRv2的医学图像融合方法，通过改进多级分解方法并充分利用LatLRR提取的各种图像特征，实现了在客观和主观评估中的最先进融合性能。

    

    由于MDLatLRR仅考虑了通过潜在低秩表示（LatLRR）提取的输入图像的详细部分（显著特征），没有有效地利用LatLRR提取的基础部分（主要特征）。因此，我们提出了一种改进的多级分解方法，称为MDLatLRRv2，该方法能有效分析和利用LatLRR获取的所有图像特征。然后，我们将MDLatLRRv2应用于医学图像融合。基础部分通过平均策略进行融合，详细部分通过核范数操作进行融合。与现有方法的比较表明，所提出的方法在客观和主观评估中可以实现最先进的融合性能。

    Since MDLatLRR only considers detailed parts (salient features) of input images extracted by latent low-rank representation (LatLRR), it doesn't use base parts (principal features) extracted by LatLRR effectively. Therefore, we proposed an improved multi-level decomposition method called MDLatLRRv2 which effectively analyzes and utilizes all the image features obtained by LatLRR. Then we apply MDLatLRRv2 to medical image fusion. The base parts are fused by average strategy and the detail parts are fused by nuclear-norm operation. The comparison with the existing methods demonstrates that the proposed method can achieve state-of-the-art fusion performance in objective and subjective assessment.
    
[^164]: TE2Rules: 使用规则解释树集合模型

    TE2Rules: Explaining Tree Ensembles using Rules. (arXiv:2206.14359v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14359](http://arxiv.org/abs/2206.14359)

    本文介绍了一种将二元分类任务中的树集合模型转换为可解释规则列表的方法，该方法可以有效解释模型对于少数类别的预测。实验证明，TE2Rules方法生成的规则列表准确性较高，并且运行时间与其他基线方法相当。

    

    树集合（如梯度提升树）通常相比单棵决策树具有更高的预测性能，然而，树集合模型通常缺乏透明度和可解释性，因为人类难以理解它们的决策逻辑。本文提出一种新颖的方法，将用于二元分类任务的树集合模型转换为接近树集合的可解释规则列表，并且有效地解释模型对于模型预测的少数类别。在基准数据集上的实验表明，TE2Rules生成的规则列表相对于现有方法具有更高的准确性，TE2Rules的运行时间与其他类似基线方法相当，TE2Rules算法的运行时间可以以稍微降低的准确性为代价进行权衡。

    Tree Ensemble (TE) models (like Gradient Boosted Trees) often provide higher prediction performance compared to single decision trees. However, TE models generally lack transparency and interpretability, as humans have difficulty understanding their decision logic. This paper presents a novel approach to convert a TE trained for a binary classification task, to a rule list (RL) that closely approximates the TE and is interpretable for a human. This RL can effectively explain the model even on the minority class predicted by the model. Experiments on benchmark datasets demonstrate that, (i) predictions from the RL generated by TE2Rules have higher fidelity (with respect to the original TE) compared to state-of-the-art methods, (ii) the run-time of TE2Rules is comparable to that of some other similar baselines and (iii) the run-time of TE2Rules algorithm can be traded off at the cost of a slightly lower fidelity.
    
[^165]: 对于非分布检测的双表示学习

    Dual Representation Learning for Out-of-Distribution Detection. (arXiv:2206.09387v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09387](http://arxiv.org/abs/2206.09387)

    本论文提出了双表示学习（DRL）方法，通过同时探索与标签强相关和与标签弱相关的信息，来区分内部和非分布样本，进而提高非分布检测的准确性。

    

    为了将内部分布样本分类，深度神经网络探索与标签相关的信息，并根据信息瓶颈原则舍弃与标签弱相关的信息。与内部分布样本不同的非分布样本可能被赋予意外的高置信度预测，因为它们可能获得最小的与标签强相关的信息。为了区分内部和非分布样本，双表示学习 （DRL）通过从内部分布样本中同时探索与标签强相关和与标签弱相关的信息，使非分布样本更难获得高置信度预测。对于一个探索与标签强相关信息的预训练网络来学习标签鉴别性表示，DRL训练其探索剩余与标签弱相关信息的辅助网络来学习分布鉴别性表示。

    To classify in-distribution samples, deep neural networks explore strongly label-related information and discard weakly label-related information according to the information bottleneck. Out-of-distribution samples drawn from distributions differing from that of in-distribution samples could be assigned with unexpected high-confidence predictions because they could obtain minimum strongly label-related information. To distinguish in- and out-of-distribution samples, Dual Representation Learning (DRL) makes out-of-distribution samples harder to have high-confidence predictions by exploring both strongly and weakly label-related information from in-distribution samples. For a pretrained network exploring strongly label-related information to learn label-discriminative representations, DRL trains its auxiliary network exploring the remaining weakly label-related information to learn distribution-discriminative representations. Specifically, for a label-discriminative representation, DRL c
    
[^166]: 通过交叉类接近分布的训练数据检测非分布样本

    Out-of-distribution Detection by Cross-class Vicinity Distribution of In-distribution Data. (arXiv:2206.09385v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09385](http://arxiv.org/abs/2206.09385)

    本研究提出了一种通过在训练数据的邻近分布中选择非分布样本来解决深度神经网络对分布外样本的识别问题的方法。引入了交叉类接近分布的概念，通过假设混合多个分布内样本生成的分布外样本与其组成部分具有不同的类别，提高了模型的区分能力。

    

    图像分类的深度神经网络只学习将训练中的分布输入映射到对应的正确标签，而无法区分分布外样本和分布内样本。这是因为假设所有样本都是独立同分布的，并没有分布区别。因此，从分布内样本中学习到的预训练网络会将分布外样本视为分布内样本，在测试阶段对其进行高置信度预测。为了解决这个问题，我们从训练分布内样本的邻近分布中抽取出分布外样本，用于学习在分布外输入上拒绝预测。我们引入了一种“交叉类接近分布”，假设通过混合多个分布内样本生成的分布外样本与其组成部分不共享相同的类别。从而提高了预训练模型的区分度。

    Deep neural networks for image classification only learn to map in-distribution inputs to their corresponding ground truth labels in training without differentiating out-of-distribution samples from in-distribution ones. This results from the assumption that all samples are independent and identically distributed (IID) without distributional distinction. Therefore, a pretrained network learned from in-distribution samples treats out-of-distribution samples as in-distribution and makes high-confidence predictions on them in the test phase. To address this issue, we draw out-of-distribution samples from the vicinity distribution of training in-distribution samples for learning to reject the prediction on out-of-distribution inputs. A \textit{Cross-class Vicinity Distribution} is introduced by assuming that an out-of-distribution sample generated by mixing multiple in-distribution samples does not share the same classes of its constituents. We thus improve the discriminability of a pretra
    
[^167]: 在边缘设备上提升DNN冷启动推理能力

    Boosting DNN Cold Inference on Edge Devices. (arXiv:2206.07446v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07446](http://arxiv.org/abs/2206.07446)

    NNV12是第一个在边缘设备上优化冷启动推理能力的推理引擎，通过选择合适的内核、缓存权重转换后的结果以及在异构处理器上进行流水线执行来提升性能，实验结果显示性能提升最高达到15倍。

    

    如今，DNN在边缘设备上十分常见。随着其重要性和使用案例的增加，不太可能将所有的DNN打包到设备内存中，并期望每个推理都已经预热。因此，冷启动推理，即读取、初始化和执行DNN模型的过程，变得普遍起来，并迫切需要优化其性能。为此，我们提出了NNV12，这是第一个针对冷启动推理进行优化的在设备上推理引擎。NNV12建立在三个新颖的优化策略上：为每个DNN算子选择适合的内核（实现方式），通过将后转换的权重缓存在磁盘上绕过权重转换过程，并在异构处理器上进行多个内核的流水线执行。为了应对巨大的搜索空间，NNV12采用基于启发式的方案获取近似最优的内核调度计划。我们完全实现了NNV12的原型，并通过广泛的实验评估了其性能。结果显示，NNV12的性能提升最高达到15倍。

    DNNs are ubiquitous on edge devices nowadays. With its increasing importance and use cases, it's not likely to pack all DNNs into device memory and expect that each inference has been warmed up. Therefore, cold inference, the process to read, initialize, and execute a DNN model, is becoming commonplace and its performance is urgently demanded to be optimized. To this end, we present NNV12, the first on-device inference engine that optimizes for cold inference NNV12 is built atop 3 novel optimization knobs: selecting a proper kernel (implementation) for each DNN operator, bypassing the weights transformation process by caching the post-transformed weights on disk, and pipelined execution of many kernels on asymmetric processors. To tackle with the huge search space, NNV12 employs a heuristic-based scheme to obtain a near-optimal kernel scheduling plan. We fully implement a prototype of NNV12 and evaluate its performance across extensive experiments. It shows that NNV12 achieves up to 15
    
[^168]: ReCo: 一种用于住宅社区布局规划的数据集

    ReCo: A Dataset for Residential Community Layout Planning. (arXiv:2206.04678v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04678](http://arxiv.org/abs/2206.04678)

    ReCo是一个用于住宅社区布局规划的数据集，用于解决基于数据驱动的方法在该领域面临的数据不足问题。

    

    布局规划在建筑和城市设计领域中非常重要。在承载城市功能的各种基本单位中，住宅社区对支持人类生活起着至关重要的作用。因此，住宅社区的布局规划一直受到关注，并自深度学习问世以来，尤其引起了人们的关注，因为深度学习有助于自动布局生成和空间模式识别。然而，研究领域普遍面临住宅社区布局基准或高质量数据集的不足，这阻碍了基于数据驱动的住宅社区布局规划方法的进一步探索。数据集的缺乏主要是由于大规模实际住宅数据采集和长期专家筛选的困难。为了解决这些问题，并推进智慧城市发展中各种智能空间设计和分析应用的基准数据集，我们提出了一个名为ReCo的数据集。

    Layout planning is centrally important in the field of architecture and urban design. Among the various basic units carrying urban functions, residential community plays a vital part for supporting human life. Therefore, the layout planning of residential community has always been of concern, and has attracted particular attention since the advent of deep learning that facilitates the automated layout generation and spatial pattern recognition. However, the research circles generally suffer from the insufficiency of residential community layout benchmark or high-quality datasets, which hampers the future exploration of data-driven methods for residential community layout planning. The lack of datasets is largely due to the difficulties of large-scale real-world residential data acquisition and long-term expert screening. In order to address the issues and advance a benchmark dataset for various intelligent spatial design and analysis applications in the development of smart city, we in
    
[^169]: Diffusion-GAN: 使用扩散训练生成对抗网络

    Diffusion-GAN: Training GANs with Diffusion. (arXiv:2206.02262v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02262](http://arxiv.org/abs/2206.02262)

    Diffusion-GAN提出了一种新颖的GAN框架，通过使用前向扩散链生成高斯混合分布的实例噪声，在训练中解决了GAN稳定性的问题。

    

    生成对抗网络（GANs）的稳定训练是一个挑战，将实例噪声注入鉴别器输入的方法在实践中并不十分有效。本文提出了Diffusion-GAN，一种新颖的GAN框架，利用前向扩散链生成高斯混合分布的实例噪声。Diffusion-GAN包括三个组件，包括自适应扩散过程、时间步依赖的判别器和生成器。观察到的和生成的数据都通过相同的自适应扩散过程进行扩散。在每个扩散时间步中，有不同的噪声到数据比例，时间步依赖的判别器学习区分扩散的真实数据和扩散的生成数据。生成器通过反向传播通过自适应调整扩散链的长度来平衡噪声和数据水平。我们从理论上证明了判别器的收敛性和生成器的收敛性，同时在一些标准数据集上的实验证明了Diffusion-GAN的有效性。

    Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussian-mixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminator's feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the dis
    
[^170]: ES-GNN: 通过边分割将图神经网络推广到异质图

    ES-GNN: Generalizing Graph Neural Networks Beyond Homophily with Edge Splitting. (arXiv:2205.13700v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13700](http://arxiv.org/abs/2205.13700)

    ES-GNN是一种创新的图神经网络框架，通过边分割将图分割为两个子图，以自适应地区分对学习任务相关或不相关的图边。这种方法能够提高GNN在异质图上的普适性和鲁棒性。

    

    尽管图神经网络在多个图分析任务中取得了巨大成功，但现代变体主要依赖于同质性的强归纳偏差。然而，现实世界的网络通常同时显示同质性和异质性的链接模式，其中相邻节点可能具有不同的属性和不同的标签。因此，GNN在整体上平滑节点接近性可能会聚合任务相关和不相关（甚至有害）的信息，限制了它们推广到异质图的能力，并可能导致非鲁棒性。在这项工作中，我们提出了一种创新的边分割GNN（ES-GNN）框架，以自适应地区分对学习任务相关或不相关的图边。这将原始图转化为两个具有相同节点集但具有独占边集的子图。在这两个子图上分别进行信息传播和边分割，从而使信息传播和边分割交替进行，实现了解耦。

    While Graph Neural Networks (GNNs) have achieved enormous success in multiple graph analytical tasks, modern variants mostly rely on the strong inductive bias of homophily. However, real-world networks typically exhibit both homophilic and heterophilic linking patterns, wherein adjacent nodes may share dissimilar attributes and distinct labels. Therefore, GNNs smoothing node proximity holistically may aggregate both task-relevant and irrelevant (even harmful) information, limiting their ability to generalize to heterophilic graphs and potentially causing non-robustness. In this work, we propose a novel edge splitting GNN (ES-GNN) framework to adaptively distinguish between graph edges either relevant or irrelevant to learning tasks. This essentially transfers the original graph into two subgraphs with the same node set but exclusive edge sets dynamically. Given that, information propagation separately on these subgraphs and edge splitting are alternatively conducted, thus disentangling
    
[^171]: 安全和私密的联合神经影像学

    Secure & Private Federated Neuroimaging. (arXiv:2205.05249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.05249](http://arxiv.org/abs/2205.05249)

    本论文介绍了一种安全和私密的联合神经影像学方法，通过联邦学习实现了在保护数据隐私的情况下对分布式数据进行神经网络模型的训练和分析。

    

    生物医学数据的数量不断增长。然而，由于安全、隐私和监管问题，从多个站点收集数据进行联合分析仍然具有挑战性。为了克服这个挑战，我们使用联邦学习，它能够在多个数据源上进行神经网络模型的分布式训练，而不共享数据。每个站点在其私有数据上训练神经网络一段时间，然后将神经网络参数（即权重、梯度）与联邦控制器共享，联邦控制器再聚合本地模型，将结果模型发送回每个站点，这个过程不断重复。我们的联邦学习架构MetisFL提供了强大的安全性和隐私保护。首先，样本数据永远不会离开站点。其次，在传输之前对神经网络参数进行加密，并且全同态加密下计算全局神经模型。最后，我们使用信息理论方法来限制信息泄露。

    The amount of biomedical data continues to grow rapidly. However, collecting data from multiple sites for joint analysis remains challenging due to security, privacy, and regulatory concerns. To overcome this challenge, we use Federated Learning, which enables distributed training of neural network models over multiple data sources without sharing data. Each site trains the neural network over its private data for some time, then shares the neural network parameters (i.e., weights, gradients) with a Federation Controller, which in turn aggregates the local models, sends the resulting community model back to each site, and the process repeats. Our Federated Learning architecture, MetisFL, provides strong security and privacy. First, sample data never leaves a site. Second, neural network parameters are encrypted before transmission and the global neural model is computed under fully-homomorphic encryption. Finally, we use information-theoretic methods to limit information leakage from t
    
[^172]: 通过生成对抗网络(GANs)增强核心图像分类

    Enhancing Core Image Classification Using Generative Adversarial Networks (GANs). (arXiv:2204.14224v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.14224](http://arxiv.org/abs/2204.14224)

    本研究提出了一种使用生成对抗网络(GANs)增强核心图像分类的创新方法，通过应用先进的模型来检测和分割岩心图像中的核心和洞，并利用强大的GANs技术填补岩心图像中的洞。这项研究将为油气勘探行业带来重大转变。

    

    在兴奋人心的油气勘探世界中，岩心样品是解锁地质信息以寻找有利可图的油气矿床的关键。尽管这些样品的重要性，传统的岩心记录技术被认为是耗时且主观的。幸运的是，该行业已经采用了一种创新的解决方案-岩心成像，它可以对大量岩心进行无损和非侵入性的快速表征。我们杰出的研究论文旨在解决岩心检测和分类的紧迫问题。使用最先进的技术，我们提出了一个突破性的解决方案，将改变该行业。我们首先面临的挑战是检测图像中的岩心并分割出孔洞，我们将分别使用Faster RCNN和Mask RCNN模型来实现。然后，我们将利用强大的生成对抗网络(GANs)和Contextual Residual来解决填补岩心图像中的洞的问题。

    In the thrilling world of oil exploration, drill core samples are key to unlocking geological information critical to finding lucrative oil deposits. Despite the importance of these samples, traditional core logging techniques are known to be laborious and, worse still, subjective. Thankfully, the industry has embraced an innovative solution core imaging that allows for nondestructive and noninvasive rapid characterization of large quantities of drill cores. Our preeminent research paper aims to tackle the pressing problem of core detection and classification. Using state-of-the-art techniques, we present a groundbreaking solution that will transform the industry. Our first challenge is detecting the cores and segmenting the holes in images, which we will achieve using the Faster RCNN and Mask RCNN models, respectively. Then, we will address the problem of filling the hole in the core image, utilizing the powerful Generative Adversarial Networks (GANs) and employing Contextual Residual
    
[^173]: 模拟时间积分的粗粒化分子动力学与多尺度图网络

    Simulate Time-integrated Coarse-grained Molecular Dynamics with Multi-Scale Graph Networks. (arXiv:2204.10348v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.10348](http://arxiv.org/abs/2204.10348)

    本论文提出了一种学习多尺度图神经网络的方法，通过直接模拟粗粒化分子动力学来加速模拟过程，使用非常大的时间步长和新颖的细化模块来提高稳定性。在两个复杂系统中进行的实验表明该方法在不同化学组成的系统中可以准确恢复结构和动力学性质。

    

    分子动力学（MD）模拟对于各种科学领域至关重要，但计算成本很高。基于学习的力场在加速从头算MD模拟方面取得了显著进展，但对于大系统和小时间步长（飞秒级）的实际应用来说推理速度仍然太慢。我们旨在通过学习一个多尺度图神经网络，直接模拟粗粒化MD，使用非常大的时间步长（纳秒级），并基于扩散模型的新颖细化模块来减轻模拟不稳定性。我们的方法在两个复杂系统中得到了验证：单链粗粒化聚合物和多组分锂离子聚合物电解质。为了评估，我们模拟了比训练轨迹长得多的轨迹，适用于模型未经训练的具有不同化学组成的系统。结构和动力学性质可以准确恢复。

    Molecular dynamics (MD) simulation is essential for various scientific domains but computationally expensive. Learning-based force fields have made significant progress in accelerating ab-initio MD simulation but are not fast enough for many real-world applications due to slow inference for large systems and small time steps (femtosecond-level). We aim to address these challenges by learning a multi-scale graph neural network that directly simulates coarse-grained MD with a very large time step (nanosecond-level) and a novel refinement module based on diffusion models to mitigate simulation instability. The effectiveness of our method is demonstrated in two complex systems: single-chain coarse-grained polymers and multi-component Li-ion polymer electrolytes. For evaluation, we simulate trajectories much longer than the training trajectories for systems with different chemical compositions that the model is not trained on. Structural and dynamical properties can be accurately recovered 
    
[^174]: 连续时间用户偏好建模用于时间序列预测的论文

    Continuous-Time User Preference Modelling for Temporal Sets Prediction. (arXiv:2204.05490v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.05490](http://arxiv.org/abs/2204.05490)

    这篇论文提出了一种连续时间用户偏好建模框架，用于时间序列预测。该框架通过显式建模用户的演化偏好，维护存储器来存储所有用户和元素的状态，并利用通用序列按时间顺序进行学习。

    

    给定一系列集合，每个集合都有一个时间戳并包含任意数量的元素，时间序列预测旨在预测后续集合中的元素。之前的时间序列预测研究主要关注元素建模，并通过用户与元素的互动间接表示每个用户的偏好。然而，用户偏好经常不断演变，不能完全用间接学习偏好的范式来捕捉演化趋势。为此，我们提出了一种连续时间用户偏好建模框架，用于时间序列预测，通过维护一个存储所有用户和元素状态的存储器来显式建模每个用户的演化偏好。具体而言，我们首先通过以非降序的时间顺序排列所有用户-集合交互来构建一个通用序列，然后按时间顺序从每个用户-集合交互中进行学习。对于每个交互，我们连续地建模用户偏好并使用记忆存储器进行维护。

    Given a sequence of sets, where each set has a timestamp and contains an arbitrary number of elements, temporal sets prediction aims to predict the elements in the subsequent set. Previous studies for temporal sets prediction mainly focus on the modelling of elements and implicitly represent each user's preference based on his/her interacted elements. However, user preferences are often continuously evolving and the evolutionary trend cannot be fully captured with the indirect learning paradigm of user preferences. To this end, we propose a continuous-time user preference modelling framework for temporal sets prediction, which explicitly models the evolving preference of each user by maintaining a memory bank to store the states of all the users and elements. Specifically, we first construct a universal sequence by arranging all the user-set interactions in a non-descending temporal order, and then chronologically learn from each user-set interaction. For each interaction, we continuou
    
[^175]: 用单个GPU合成16K图像的一次性超高分辨率生成对抗网络

    One-shot Ultra-high-Resolution Generative Adversarial Network That Synthesizes 16K Images On A Single GPU. (arXiv:2202.13799v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.13799](http://arxiv.org/abs/2202.13799)

    我们提出了一种名为OUR-GAN的一次性超高分辨率生成对抗网络，能够从单个训练图像生成非重复的16K图像。它通过逐步增加细节和超分辨率来提高图像质量，并能合成具有细节和一致性的大型形状。此外，它还通过垂直位移来提高视觉一致性和多样性。

    

    我们提出了一种一次性超高分辨率生成对抗网络（OUR-GAN）框架，能够从单个训练图像生成非重复的16K（16,384 x 8,640）图像，并可以在单个消费级GPU上进行训练。OUR-GAN在低分辨率下生成一个视觉上合理且形状各异的初始图像，然后通过超分辨率逐渐增加细节来提高分辨率。由于OUR-GAN从真实的超高分辨率（UHR）图像中学习，它可以合成具有细节和长程一致性的大型形状，而传统的依赖于从相对较小图像学习的分块分布的生成模型难以实现这一点。OUR-GAN可以使用12.5 GB的GPU内存合成高质量的16K图像，只需要4.29 GB即可合成4K图像，因为它通过无缝子区域超分辨率逐部分合成UHR图像。另外，OUR-GAN通过应用垂直位移来提高视觉一致性并保持多样性。

    We propose a one-shot ultra-high-resolution generative adversarial network (OUR-GAN) framework that generates non-repetitive 16K (16, 384 x 8, 640) images from a single training image and is trainable on a single consumer GPU. OUR-GAN generates an initial image that is visually plausible and varied in shape at low resolution, and then gradually increases the resolution by adding detail through super-resolution. Since OUR-GAN learns from a real ultra-high-resolution (UHR) image, it can synthesize large shapes with fine details and long-range coherence, which is difficult to achieve with conventional generative models that rely on the patch distribution learned from relatively small images. OUR-GAN can synthesize high-quality 16K images with 12.5 GB of GPU memory and 4K images with only 4.29 GB as it synthesizes a UHR image part by part through seamless subregion-wise super-resolution. Additionally, OUR-GAN improves visual coherence while maintaining diversity by applying vertical positi
    
[^176]: 在外生非平稳变化存在下的自适应实验

    Adaptive Experimentation in the Presence of Exogenous Nonstationary Variation. (arXiv:2202.09036v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.09036](http://arxiv.org/abs/2202.09036)

    本论文研究了在外生非平稳变化存在下的自适应实验。提出了无偏汤普森抽样(DTS)算法来解决多臂老虎机算法在面对非平稳外生因素时的脆弱性，DTS算法通过控制背景信息预测一个臂的人口层级表现，并提供了实验内和实验后的遗憾界限，显示了其对外生变异的弹性。

    

    我们研究设计用于选择人口部署治疗方案的实验。多臂老虎机算法可以通过根据观察到的反馈动态分配测量工作量到表现更好的臂上，从而提高效率。然而，这种动态性可能导致面对影响实验中臂表现的非平稳外生因素时出现脆弱行为。为了应对这个问题，我们提出了无偏汤普森抽样(DTS)，这是一种更稳健的著名汤普森抽样算法的变体。随着观察结果的积累，DTS会控制观察到的治疗决策的背景，同时预测一个臂的人口层级表现。这里的背景可以捕捉到一个可理解的变化源，比如一个受治疗个体的国家，或者仅仅是记录治疗时间。我们给出了DTS在实验内和实验后遗憾的界限，说明它对于外生变异的弹性。

    We investigate experiments that are designed to select a treatment arm for population deployment. Multi-armed bandit algorithms can enhance efficiency by dynamically allocating measurement effort towards higher performing arms based on observed feedback. However, such dynamics can result in brittle behavior in the face of nonstationary exogenous factors influencing arms' performance during the experiment. To counter this, we propose deconfounded Thompson sampling (DTS), a more robust variant of the prominent Thompson sampling algorithm. As observations accumulate, DTS projects the population-level performance of an arm while controlling for the context within which observed treatment decisions were made. Contexts here might capture a comprehensible source of variation, such as the country of a treated individual, or simply record the time of treatment. We provide bounds on both within-experiment and post-experiment regret of DTS, illustrating its resilience to exogenous variation and t
    
[^177]: 连续时间随机梯度下降用于优化随机微分方程的平稳分布

    Continuous-time stochastic gradient descent for optimizing over the stationary distribution of stochastic differential equations. (arXiv:2202.06637v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.06637](http://arxiv.org/abs/2202.06637)

    我们提出了一种连续时间随机梯度下降算法用于优化随机微分方程模型的平稳分布。算法通过估计平稳分布的梯度，并使用正向传播进行连续更新参数，实现收敛至最陡下降方向。我们严格证明了在线正向传播算法在线性模型上的收敛性，并在非线性示例上进行了数值验证。

    

    我们开发了一种新的连续时间随机梯度下降方法，用于优化随机微分方程模型的平稳分布。算法使用平稳分布的梯度估计连续更新SDE模型的参数。梯度估计同时使用SDE状态导数的正向传播进行更新，渐近地收敛到最陡下降方向。我们严格证明了在线正向传播算法在线性SDE模型（如多维Ornstein-Uhlenbeck过程）上的收敛性，并呈现了非线性示例的数值结果。证明需要对参数演化在最陡下降方向附近的波动进行分析。由于算法的在线性质，获得波动的界限很具挑战性（例如，随着参数的变化，稳定分布将持续变化）。

    We develop a new continuous-time stochastic gradient descent method for optimizing over the stationary distribution of stochastic differential equation (SDE) models. The algorithm continuously updates the SDE model's parameters using an estimate for the gradient of the stationary distribution. The gradient estimate is simultaneously updated using forward propagation of the SDE state derivatives, asymptotically converging to the direction of steepest descent. We rigorously prove convergence of the online forward propagation algorithm for linear SDE models (i.e., the multi-dimensional Ornstein-Uhlenbeck process) and present its numerical results for nonlinear examples. The proof requires analysis of the fluctuations of the parameter evolution around the direction of steepest descent. Bounds on the fluctuations are challenging to obtain due to the online nature of the algorithm (e.g., the stationary distribution will continuously change as the parameters change). We prove bounds for the s
    
[^178]: 人体胚胎在第一季度3D超声中的多图谱分割与空间对齐

    Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound. (arXiv:2202.06599v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.06599](http://arxiv.org/abs/2202.06599)

    本文提出了一个利用多图谱和深度学习技术进行胚胎分割和空间对齐的自动化框架，从而实现对人体胚胎在第一季度3D超声图像中的生长和发育进行监测。该框架通过学习将胚胎注册到图谱中，使用了多个孕龄范围内的超声图像，经过分割和空间对齐后得到标准方向上的胚胎分割结果。

    

    在生命的这一关键时期，对第一季度胎儿生长和发育进行监测，超声成像数据的分割和空间对齐至关重要。目前的方法要么是手动的，要么是半自动的，非常耗时且容易出错。为了自动化这些任务，我们提出了一个多图谱框架，利用深度学习和最小监督来实现胚胎的自动分割和空间对齐。我们的框架学习将胚胎注册到一个图谱中，该图谱由在不同孕龄范围内获取的超声图像经过分割和空间对齐后形成的。通过这个框架，我们可以得到胚胎的分割结果并将其放置在标准方向上。我们使用了8+0至12+6周的孕龄的超声图像，并选择了八个被试作为图谱。我们评估了不同的融合策略来结合多个图谱的信息。

    Segmentation and spatial alignment of ultrasound (US) imaging data acquired in the in first trimester are crucial for monitoring human embryonic growth and development throughout this crucial period of life. Current approaches are either manual or semi-automatic and are therefore very time-consuming and prone to errors. To automate these tasks, we propose a multi-atlas framework for automatic segmentation and spatial alignment of the embryo using deep learning with minimal supervision. Our framework learns to register the embryo to an atlas, which consists of the US images acquired at a range of gestational age (GA), segmented and spatially aligned to a predefined standard orientation. From this, we can derive the segmentation of the embryo and put the embryo in standard orientation. US images acquired at 8+0 till 12+6 weeks GA were used and eight subjects were selected as atlas. We evaluated different fusion strategies to incorporate multiple atlases: 1) training the framework using a
    
[^179]: NNP/MM: 利用机器学习势和分子力学加速分子动力学模拟

    NNP/MM: Accelerating molecular dynamics simulations with machine learning potentials and molecular mechanic. (arXiv:2201.08110v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2201.08110](http://arxiv.org/abs/2201.08110)

    该论文介绍了一种优化的混合方法（NNP/MM），结合了神经网络势和分子力学，以加速生物分子模拟。通过对蛋白质-配体复合物进行分子动力学模拟和对配体进行元动力学模拟，研究人员展示了该方法能够提高模拟速度并实现最长的模拟时间。

    

    机器学习势已经成为增强生物分子模拟准确性的手段。然而，与传统分子力学相比，它们的应用受到大量参数带来的显著计算成本的限制。为解决这个问题，我们引入了混合方法（NNP/MM）的优化实现，它结合了神经网络势（NNP）和分子力学（MM）。这种方法使用NNP来模拟系统的一部分（如小分子），同时使用MM来提高效率。通过对各种蛋白质-配体复合物进行分子动力学（MD）模拟和对配体进行元动力学（MTD）模拟，我们展示了我们的NNP/MM实现的能力。它使我们能够将模拟速度提高了5倍，并在每个复合物上实现了1微秒的联合采样，成为迄今为止报道的这类模拟中最长的模拟。

    Machine learning potentials have emerged as a means to enhance the accuracy of biomolecular simulations. However, their application is constrained by the significant computational cost arising from the vast number of parameters compared to traditional molecular mechanics. To tackle this issue, we introduce an optimized implementation of the hybrid method (NNP/MM), which combines neural network potentials (NNP) and molecular mechanics (MM). This approach models a portion of the system, such as a small molecule, using NNP while employing MM for the remaining system to boost efficiency. By conducting molecular dynamics (MD) simulations on various protein-ligand complexes and metadynamics (MTD) simulations on a ligand, we showcase the capabilities of our implementation of NNP/MM. It has enabled us to increase the simulation speed by 5 times and achieve a combined sampling of one microsecond for each complex, marking the longest simulations ever reported for this class of simulation.
    
[^180]: 不完全的多视角弱标签学习

    Incomplete Multi-View Weak-Label Learning. (arXiv:2201.01079v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.01079](http://arxiv.org/abs/2201.01079)

    本文提出了一种解决多视图多标签学习中存在的不完全特征和标签、噪声视图和标签不平衡问题的新方法。该方法通过将不完全的视图和弱标签嵌入到低维子空间中，并通过自适应的权重和嵌入矩阵差异来减少冗余。实验证实了该方法的有效性。

    

    现代应用中存在多视图多标签学习问题，每个样本具有多个视图特征，多个标签通过共同的视图相关联。目前的方法通常不能直接处理每个样本只观察到部分特征和标签的情况，并忽略了真实世界问题中存在的噪声视图和不均衡标签。在本文中，我们提出了一种新方法来克服这些限制。它将不完全的视图和弱标签一起嵌入到自适应权重的低维子空间中，并通过自适应Hilbert-Schmidt独立准则（HSIC）的嵌入权重矩阵之间的差异来减少冗余。此外，它通过聚焦损失自适应学习视图的重要性以检测噪声视图，并通过聚焦损失减轻标签不平衡问题。对四个真实世界多视图多标签数据集的实验证明了所提方法的有效性。

    A variety of modern applications exhibit multi-view multi-label learning, where each sample has multi-view features, and multiple labels are correlated via common views. Current methods usually fail to directly deal with the setting where only a subset of features and labels are observed for each sample, and ignore the presence of noisy views and imbalanced labels in real-world problems. In this paper, we propose a novel method to overcome the limitations. It jointly embeds incomplete views and weak labels into a low-dimensional subspace with adaptive weights, and facilitates the difference between embedding weight matrices via auto-weighted Hilbert-Schmidt Independence Criterion (HSIC) to reduce the redundancy. Moreover, it adaptively learns view-wise importance for embedding to detect noisy views, and mitigates the label imbalance problem by focal loss. Experimental results on four real-world multi-view multi-label datasets demonstrate the effectiveness of the proposed method.
    
[^181]: 随机坐标变换及其在鲁棒机器学习中的应用

    Stochastic coordinate transformations with applications to robust machine learning. (arXiv:2110.01729v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.01729](http://arxiv.org/abs/2110.01729)

    本文提出了一种利用随机坐标变换进行异常检测的新方法，该方法通过层级张量积展开来逼近随机过程，并通过训练机器学习分类器对投影系数进行检测。在基准数据集上的实验表明，该方法胜过现有的最先进方法。

    

    本文介绍了一组新的特征，利用Karhunen-Loeve展开法来识别输入数据的潜在随机行为。这些新特征是通过基于最近的函数数据分析理论进行的坐标变换构建的，用于异常检测。相关的信号分解是用已知优化属性的层级张量积展开来逼近具有有限功能空间的随机过程（随机场）。原则上，这些低维空间可以捕捉给定名义类别的'底层信号'的大部分随机变化，并且可以将来自其它类别的信号拒绝为随机异常。通过名义类别的层级有限维展开，构建了一系列用于检测异常信号组件的正交嵌套子空间。然后使用这些子空间中的投影系数来训练用于异常检测的机器学习（ML）分类器。我们在几个基准数据集上评估所提出的方法，结果表明其胜过现有的最先进方法。

    In this paper we introduce a set of novel features for identifying underlying stochastic behavior of input data using the Karhunen-Loeve expansion. These novel features are constructed by applying a coordinate transformation based on the recent Functional Data Analysis theory for anomaly detection. The associated signal decomposition is an exact hierarchical tensor product expansion with known optimality properties for approximating stochastic processes (random fields) with finite dimensional function spaces. In principle these low dimensional spaces can capture most of the stochastic behavior of `underlying signals' in a given nominal class, and can reject signals in alternative classes as stochastic anomalies. Using a hierarchical finite dimensional expansion of the nominal class, a series of orthogonal nested subspaces is constructed for detecting anomalous signal components. Projection coefficients of input data in these subspaces are then used to train a Machine Learning (ML) clas
    
[^182]: 通过隐式生成器揭示辨别器的分布脆弱性

    Revealing the Distributional Vulnerability of Discriminators by Implicit Generators. (arXiv:2108.09976v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.09976](http://arxiv.org/abs/2108.09976)

    通过隐式生成器的微调，我们揭示了辨别器在分布上的脆弱性。我们的方法通过生成和惩罚特定的分布外样本来提高辨别器在分布内和分布外样本上的区分能力。

    

    在深度神经学习中，通过在分布样本上训练的辨别器可能对分布外样本做出高置信度的预测。这对于强大、可靠和安全的深度学习来说是一个重要问题。这个问题主要是由于在训练辨别器时，由于分布外样本不可用，只能观测到有限的分布内样本所导致的。我们提出了一种通过隐式生成器进行辨别器微调的通用方法（FIG）。FIG以信息理论为基础，并适用于标准辨别器而无需重新训练。它通过生成和惩罚特定的分布外样本来提高标准辨别器在区分分布内样本和分布外样本方面的能力。根据香农熵，我们从辨别器中推断出一种基于能量的隐式生成器，而不需要额外的训练成本。然后，Langevin动力学采样器为隐式生成器绘制特定的分布外样本。最后，我们设计了一个符合设计原则的正则化器来适应该方法的设计原则。

    In deep neural learning, a discriminator trained on in-distribution (ID) samples may make high-confidence predictions on out-of-distribution (OOD) samples. This triggers a significant matter for robust, trustworthy and safe deep learning. The issue is primarily caused by the limited ID samples observable in training the discriminator when OOD samples are unavailable. We propose a general approach for \textit{fine-tuning discriminators by implicit generators} (FIG). FIG is grounded on information theory and applicable to standard discriminators without retraining. It improves the ability of a standard discriminator in distinguishing ID and OOD samples by generating and penalizing its specific OOD samples. According to the Shannon entropy, an energy-based implicit generator is inferred from a discriminator without extra training costs. Then, a Langevin dynamic sampler draws specific OOD samples for the implicit generator. Lastly, we design a regularizer fitting the design principle of th
    
[^183]: 元校准：使用可微之期望校准误差学习模型校准

    Meta-Calibration: Learning of Model Calibration Using Differentiable Expected Calibration Error. (arXiv:2106.09613v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.09613](http://arxiv.org/abs/2106.09613)

    本论文提出了元校准框架，通过引入可微的期望校准误差代理指标和元学习框架，实现了对模型校准质量的直接优化。实验证明，该方法能够达到与现有校准方法相竞争的性能表现。该框架为进一步解决校准问题提供了新的思路和工具。

    

    神经网络的校准是一个重要的问题，在神经网络越来越多地应用于现实世界应用的情况下，变得越来越重要。当使用现代神经网络时，模型的置信度与正确预测的概率之间存在明显差异，这一问题尤为明显。已经提出了各种策略来改善校准，但准确的校准仍然具有挑战性。我们提出了一个新的框架，包含两个贡献：引入了一个新的可微代理指标，用于直接优化校准质量的期望校准误差 (DECE)，以及一个元学习框架，使用DECE根据模型超参数优化验证集校准。结果表明，我们的方法在校准方面具有竞争性能。我们的框架为处理校准问题开辟了新的途径和工具，我们相信这将激发更多研究。

    Calibration of neural networks is a topical problem that is becoming more and more important as neural networks increasingly underpin real-world applications. The problem is especially noticeable when using modern neural networks, for which there is a significant difference between the confidence of the model and the probability of correct prediction. Various strategies have been proposed to improve calibration, yet accurate calibration remains challenging. We propose a novel framework with two contributions: introducing a new differentiable surrogate for expected calibration error (DECE) that allows calibration quality to be directly optimised, and a meta-learning framework that uses DECE to optimise for validation set calibration with respect to model hyper-parameters. The results show that we achieve competitive performance with existing calibration approaches. Our framework opens up a new avenue and toolset for tackling calibration, which we believe will inspire further work on thi
    
[^184]: 网络、游戏和学习的融合

    The Confluence of Networks, Games and Learning. (arXiv:2105.08158v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2105.08158](http://arxiv.org/abs/2105.08158)

    这篇论文讨论了网络、游戏和学习的融合，为理解网络上多智能体决策制定提供了理论基础，并提供了选择性的博弈理论学习算法概述和在现代网络系统中的应用。

    

    近年来，现代网络应用的技术和服务取得了显著进展，包括智能电网管理、无线通信、网络安全以及多智能体自主系统。考虑到网络实体的异构性，新兴网络应用需要博弈理论模型和基于学习的方法，以创建对动态或对抗环境中的不确定性和干扰作出响应的分布式网络智能。本文阐述了网络、游戏和学习的融合，为理解网络上多智能体决策制定奠定了理论基础。我们在随机逼近理论的框架内提供了博弈理论学习算法的选择性概述，并介绍了在现代网络系统的一些代表性场景中的应用，例如下一代无线通信网络、智能电网。

    Recent years have witnessed significant advances in technologies and services in modern network applications, including smart grid management, wireless communication, cybersecurity as well as multi-agent autonomous systems. Considering the heterogeneous nature of networked entities, emerging network applications call for game-theoretic models and learning-based approaches in order to create distributed network intelligence that responds to uncertainties and disruptions in a dynamic or an adversarial environment. This paper articulates the confluence of networks, games and learning, which establishes a theoretical underpinning for understanding multi-agent decision-making over networks. We provide an selective overview of game-theoretic learning algorithms within the framework of stochastic approximation theory, and associated applications in some representative contexts of modern network systems, such as the next generation wireless communication networks, the smart grid and distribute
    
[^185]: 功能性最优输运：功能数据的映射估计和领域适应

    Functional optimal transport: map estimation and domain adaptation for functional data. (arXiv:2102.03895v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.03895](http://arxiv.org/abs/2102.03895)

    这篇论文介绍了在函数空间上解决分布最优输运问题的方法，通过使用Hilbert-Schmidt算子将函数域之间的随机映射进行表示。这种方法对于处理函数数据的机器学习任务非常有用。

    

    我们引入了一种在函数空间上对分布进行最优输运问题的表达，其中函数域之间的随机映射可以部分地表示为一个（无限维）Hilbert-Schmidt算子，将一个函数的希尔伯特空间映射到另一个函数上。对于许多机器学习任务，数据可以自然地视为从函数空间中采样得到的，例如曲线和曲面，在高维空间中。功能数据分析的最优输运提供了对这些领域进行处理的有用框架。

    We introduce a formulation of optimal transport problem for distributions on function spaces, where the stochastic map between functional domains can be partially represented in terms of an (infinite-dimensional) Hilbert-Schmidt operator mapping a Hilbert space of functions to another. For numerous machine learning tasks, data can be naturally viewed as samples drawn from spaces of functions, such as curves and surfaces, in high dimensions. Optimal transport for functional data analysis provides a useful framework of treatment for such domains. { Since probability measures in infinite dimensional spaces generally lack absolute continuity (that is, with respect to non-degenerate Gaussian measures), the Monge map in the standard optimal transport theory for finite dimensional spaces may not exist. Our approach to the optimal transport problem in infinite dimensions is by a suitable regularization technique -- we restrict the class of transport maps to be a Hilbert-Schmidt space of operat
    
[^186]: 一种具有高斯过程的概率Taylor展开

    A probabilistic Taylor expansion with Gaussian processes. (arXiv:2102.00877v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.00877](http://arxiv.org/abs/2102.00877)

    本文研究了一种特殊的高斯过程回归方法，通过导数评估数据和先验协方差核的选择，实现了对任意阶Taylor展开的截断复制，并讨论了相关参数的最大似然估计方法。

    

    我们研究了一类高斯过程，对于特定选择的数据，后验均值复制了截断的任意阶Taylor展开。数据由在展开点处的导数评估组成，先验协方差核属于Taylor核的类，可以用一定的幂级数形式表示。我们讨论并证明了关于Taylor核参数的最大似然估计的一些结果。所提出的框架是基于具有在协方差核的再生核希尔伯特空间中正交数据的高斯过程回归的特例。

    We study a class of Gaussian processes for which the posterior mean, for a particular choice of data, replicates a truncated Taylor expansion of any order. The data consist of derivative evaluations at the expansion point and the prior covariance kernel belongs to the class of Taylor kernels, which can be written in a certain power series form. We discuss and prove some results on maximum likelihood estimation of parameters of Taylor kernels. The proposed framework is a special case of Gaussian process regression based on data that is orthogonal in the reproducing kernel Hilbert space of the covariance kernel.
    
[^187]: 使用插值核岭回归估计导数的研究

    On the Estimation of Derivatives Using Plug-in Kernel Ridge Regression Estimators. (arXiv:2006.01350v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2006.01350](http://arxiv.org/abs/2006.01350)

    本文提出了一种插值核岭回归（KRR）估计器，可广泛适用于非参数回归中的多维支持和任意混合偏导数，并且具有较强的误差界。

    

    我们研究了对回归函数的导数进行估计的问题，这在未知函数的非参数化功能中具有广泛的应用。标准的分析可能针对特定的导数阶数进行调整，而参数调优特别是对于高阶导数来说仍然是一个困难的挑战。在本文中，我们提出了一种简单的插值核岭回归（KRR）估计器，用于具有随机设计的非参数回归，广泛适用于多维支持和任意混合偏导数。我们提供了非渐近分析，以统一地研究所提出的估计器的行为，包括回归函数及其导数，在强L∞范数下导致了一个一般类的核函数的两个误差界。在一个具体的例子中，该估计器专门针对具有多项式衰减特征值的核函数，实现了最小化的最优速率，只有一个对数因子可估计

    We study the problem of estimating the derivatives of a regression function, which has a wide range of applications as a key nonparametric functional of unknown functions. Standard analysis may be tailored to specific derivative orders, and parameter tuning remains a daunting challenge particularly for high-order derivatives. In this article, we propose a simple plug-in kernel ridge regression (KRR) estimator in nonparametric regression with random design that is broadly applicable for multi-dimensional support and arbitrary mixed-partial derivatives. We provide a non-asymptotic analysis to study the behavior of the proposed estimator in a unified manner that encompasses the regression function and its derivatives, leading to two error bounds for a general class of kernels under the strong $L_\infty$ norm. In a concrete example specialized to kernels with polynomially decaying eigenvalues, the proposed estimator recovers the minimax optimal rate up to a logarithmic factor for estimatin
    
[^188]: 使用生成对抗网络学习生成时间系列条件图

    Learning to Generate Time Series Conditioned Graphs with Generative Adversarial Nets. (arXiv:2003.01436v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2003.01436](http://arxiv.org/abs/2003.01436)

    本文提出了一种新颖的时间序列条件图生成方法(TSGG-GAN)，通过结合丰富的节点级上下文结构来推断时间序列之间的关系图。

    

    最近，基于深度学习的方法已被用于建模和生成符合不同分布的图。然而，它们通常是基于无监督学习的、无条件的生成模型，或者仅基于图级上下文条件生成，这与丰富的语义节点级上下文无关。不同地，在本文中，我们对一个名为时间序列条件图生成的新问题感兴趣：给定一个输入的多变量时间序列，我们的目标是推断一个目标关系图，该图建模了时间序列之间的潜在相互关系，其中每个节点对应一个时间序列。例如，我们可以研究以基因表达数据作为时间序列条件的某种疾病的基因调控网络中基因之间的相互关系。为了实现这个目标，我们提出了一种新颖的时间序列条件图生成-生成对抗网络（TSGG-GAN）来处理丰富的节点级上下文结构的挑战。

    Deep learning based approaches have been utilized to model and generate graphs subjected to different distributions recently. However, they are typically unsupervised learning based and unconditioned generative models or simply conditioned on the graph-level contexts, which are not associated with rich semantic node-level contexts. Differently, in this paper, we are interested in a novel problem named Time Series Conditioned Graph Generation: given an input multivariate time series, we aim to infer a target relation graph modeling the underlying interrelationships between time series with each node corresponding to each time series. For example, we can study the interrelationships between genes in a gene regulatory network of a certain disease conditioned on their gene expression data recorded as time series. To achieve this, we propose a novel Time Series conditioned Graph Generation-Generative Adversarial Networks (TSGG-GAN) to handle challenges of rich node-level context structures 
    
[^189]: 如何选择最合适的中心度测量方法？一种决策树的方法。

    How to choose the most appropriate centrality measure? A decision tree approach. (arXiv:2003.01052v5 [physics.soc-ph] UPDATED)

    [http://arxiv.org/abs/2003.01052](http://arxiv.org/abs/2003.01052)

    这篇论文提出了一种利用决策树方法选择合适中心度度量方法的缩减方法。通过构建决策树调查，结合专家偏好，能够在较小的图形数量下快速筛选出最合适的中心度度量方法。

    

    中心度度量在网络分析中至关重要，但在400多种提出的指标中选择最合适的测量方法仍然具有挑战性。现有方法-基于模型、数据驱动和公理性-存在局限性。为了解决这个问题，我们引入了缩减方法，利用专家对简单图中的中心度行为的偏好。它涉及形成一组候选测量方法，生成尽可能小的图形以“分离”各种测量方法，构建决策树调查，并确定与专家回答一致的测量方法。我们将这种方法应用于包括新的基于核的测量方法在内的40种不同中心度，同时与公理性方法结合。值得注意的是，只有13个小型1-树足以分离所有40个测量方法，其中包括接近的一对。缩减方法在劳动力和时间方面提供了一种低成本的解决方案，为已有的测量方法提供了补充。

    Centrality metrics are vital for network analysis, but selecting the most appropriate measures for specific applications remains challenging among the 400+ proposed indices. Existing approaches -- model-based, data-driven, and axiomatic -- have limitations. To address this, we introduce the culling method, leveraging expert preferences regarding centrality behavior on simple graphs. It involves forming a set of candidate measures, generating a list of as small graphs as possible needed to ``separate'' measures from each other, constructing a decision-tree survey, and identifying the measure consistent with expert responses. We apply this method to a diverse set of 40 centralities, including new kernel-based measures, and combine it with the axiomatic approach. Remarkably, only 13 small 1-trees suffice to separate all 40 measures, among which there are pairs of close ones. The culling method offers a low-cost solution in terms of labor and time, complements existing methods for measure 
    
[^190]: 行人属性识别: 一项调查

    Pedestrian Attribute Recognition: A Survey. (arXiv:1901.07474v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/1901.07474](http://arxiv.org/abs/1901.07474)

    本文回顾了行人属性识别的现有作品，介绍了行人属性识别的背景、基准、多任务学习和多标签学习的概念，以及流行的网络架构和解决方案。

    

    由于在视频监控中起着重要作用，识别行人属性是计算机视觉领域的一项重要任务。许多算法已被提出来处理这个任务。本文的目标是回顾使用传统方法或基于深度学习网络的现有作品。首先，我们介绍行人属性识别的背景，包括行人属性的基本概念和相应的挑战。其次，我们介绍现有的基准，包括流行的数据集和评估标准。第三，我们分析了多任务学习和多标签学习的概念，并解释了这两种学习算法与行人属性识别之间的关系。我们还回顾了在深度学习社区中广泛应用的一些流行网络架构。第四，我们分析了此任务的流行解决方案，如属性组和基于部分的方法等。

    Recognizing pedestrian attributes is an important task in the computer vision community due to it plays an important role in video surveillance. Many algorithms have been proposed to handle this task. The goal of this paper is to review existing works using traditional methods or based on deep learning networks. Firstly, we introduce the background of pedestrian attribute recognition (PAR, for short), including the fundamental concepts of pedestrian attributes and corresponding challenges. Secondly, we introduce existing benchmarks, including popular datasets and evaluation criteria. Thirdly, we analyze the concept of multi-task learning and multi-label learning and also explain the relations between these two learning algorithms and pedestrian attribute recognition. We also review some popular network architectures which have been widely applied in the deep learning community. Fourthly, we analyze popular solutions for this task, such as attributes group, part-based, etc. Fifthly, we 
    
[^191]: 深度学习的泛化问题

    Generalization in Deep Learning. (arXiv:1710.05468v8 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1710.05468](http://arxiv.org/abs/1710.05468)

    本文从理论上解释了为什么以及如何深度学习能够在容量大、复杂性高、可能存在算法不稳定性、非鲁棒性和尖锐极小值的情况下实现良好的泛化，提出了一些新的开放问题，并讨论了研究结果的局限性。

    

    本文从理论上解释了为什么以及如何深度学习能够在容量大、复杂性高、可能存在算法不稳定性、非鲁棒性和尖锐极小值的情况下实现良好的泛化，回应了文献中的一个开放问题。我们还讨论了提供深度学习非虚空泛化保证的方法。基于理论观察，我们提出了一些新的开放问题，并讨论了我们研究结果的局限性。

    This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.
    

