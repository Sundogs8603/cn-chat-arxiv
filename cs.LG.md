# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Forecasting the steam mass flow in a powerplant using the parallel hybrid network.](http://arxiv.org/abs/2307.09483) | 这项研究使用并行混合神经网络结构来预测发电厂中的蒸汽质量流量，相比纯经典和纯量子模型，该混合模型在测试集上取得了更好的性能，平均平方误差降低了5.7倍和4.9倍，并且相对误差较小，最多提升了2倍。 |
| [^2] | [The Role of Transparency in Repeated First-Price Auctions with Unknown Valuations.](http://arxiv.org/abs/2307.09478) | 本文研究了一次性出价拍卖中透明度对遗憾最小化的影响，通过对拍卖透明度与环境特性的分析，揭示了在拍卖中学习最优出价的速度。 |
| [^3] | [Towards Ordinal Data Science.](http://arxiv.org/abs/2307.09477) | 本文讨论了序列数据科学的发展，并介绍了一种新的研究议程：使用序列结构来衡量和计算对象之间的关系，并从中推断知识。这种方法具有广泛的学科应用价值。 |
| [^4] | [Overthinking the Truth: Understanding how Language Models Process False Demonstrations.](http://arxiv.org/abs/2307.09476) | 该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。 |
| [^5] | [Multi-Player Zero-Sum Markov Games with Networked Separable Interactions.](http://arxiv.org/abs/2307.09470) | 本文研究了一种新的马尔可夫游戏类别，通过带有网络可分离交互的多人零和马尔可夫游戏模型（MZNMGs）来模拟非合作多智能体顺序决策中的局部交互结构。作者确定了MG可被表示为MZNMG的必要和充分条件，并证明其Markov CCE集合与Markov NE集合相等；此外，在无限时间折扣MZNMG中找到近似的Markov稳定CCE是PPAD难题，除非网络具有“星状结构”。 |
| [^6] | [Graph Representation of the Magnetic Field Topology in High-Fidelity Plasma Simulations for Machine Learning Applications.](http://arxiv.org/abs/2307.09469) | 该论文提出了一种对高保真度等离子体模拟中的磁场拓扑进行图表示的方法，并应用于地球磁层模拟中，旨在挑战机器学习社区通过图机器学习方法解决具有广泛潜力影响的科学问题。 |
| [^7] | [A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction.](http://arxiv.org/abs/2307.09463) | 本研究报告了一种基于内存计算的神经解码器推理加速器的设计和性能分析，该解码器用于容错量子错误纠正，旨在最小化解码时间并确保解码方法的可扩展性。 |
| [^8] | [Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla.](http://arxiv.org/abs/2307.09458) | 本论文研究了电路分析在最先进的语言模型中的可扩展性，通过对70B毛丫鼠模型进行多项选择题的分析，发现现有的逻辑层归因和激活修补技术具有可扩展性，并进一步研究了注意力头的语义特征。 |
| [^9] | [Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection.](http://arxiv.org/abs/2307.09457) | 光滑注意力深度多示例学习模型（SA-DMIL）用于CT颅内出血检测，通过引入平滑性约束来学习空间依赖关系，取得了比传统MIL模型更好的性能和与当前最先进方法相比的更好的结果。 |
| [^10] | [Convergent regularization in inverse problems and linear plug-and-play denoisers.](http://arxiv.org/abs/2307.09441) | 本论文概述了逆问题中的经典正则化理论和近期的数据驱动方法，以及对PnP算法及其收敛性的讨论。 |
| [^11] | [Unsupervised Conditional Slot Attention for Object Centric Learning.](http://arxiv.org/abs/2307.09437) | 本文提出了一种无监督的条件槽注意力方法，通过使用概率性槽字典（PSD）实现了专门的槽位绑定和物体层次调节分布，在多个后续任务中展示了其优势。 |
| [^12] | [Scaling Laws for Imitation Learning in NetHack.](http://arxiv.org/abs/2307.09423) | 本文研究了在NetHack游戏中的模仿学习，发现通过扩大模型和数据规模可以改进模仿学习的效果，并建立了训练计算最优IL代理人的幂律。 |
| [^13] | [Online Learning with Costly Features in Non-stationary Environments.](http://arxiv.org/abs/2307.09388) | 该论文提出了一种在非平稳环境中进行在线学习的方法，其中收集有益信息是昂贵的。研究扩展了上下文乐观设置，允许代理观察特征状态的子集，以在最小化信息成本和最大化收益之间进行权衡。 |
| [^14] | [Batched Predictors Generalize within Distribution.](http://arxiv.org/abs/2307.09379) | 批量预测器提供了指数级更强的泛化保证，可应用于离线测试前化合物质量的预测任务。 |
| [^15] | [Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading.](http://arxiv.org/abs/2307.09377) | 本研究提出了一种基于强化学习算法的交易算法，通过使用学习到的预测模型的信号进行交易，以应对在交易较少的金融市场和不同类资产市场中的挑战。 |
| [^16] | [Enhancing Pattern Classification in Support Vector Machines through Matrix Formulation.](http://arxiv.org/abs/2307.09372) | 本文介绍了一种改进的支持向量机（SVM）方法，通过引入矩阵表述来解决多类别和多标签设置下的灵活性和整合额外项的限制。通过在对偶问题中采用加速梯度下降方法，我们提高了矩阵SVM的解决效率，并证明其在时间效率和性能上与传统的二进制相关性SVM方法相当。这种矩阵表述揭示出了传统方法中难以察觉的关键洞见和优势。 |
| [^17] | [Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives.](http://arxiv.org/abs/2307.09366) | 本文提出了基于离散优化的稀疏高斯图模型学习问题的新方法，并提供了大规模求解器来获取良好的原始解。 |
| [^18] | [An Evaluation of Zero-Cost Proxies -- from Neural Architecture Performance to Model Robustness.](http://arxiv.org/abs/2307.09365) | 这篇论文评估了常见的零成本代理在神经结构搜索中的作用，特别关注了性能预测和模型鲁棒性。研究发现预测鲁棒性使得从现有的零成本代理进行预测更加具有挑战性。 |
| [^19] | [MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments.](http://arxiv.org/abs/2307.09361) | MOCA是一种自监督学习方法，通过预测掩码式在线码本分配来实现表示学习。它同时具备良好的语境推理属性和对图像扰动的不变性，并在低样本设置和各种评估协议中取得了最新的最先进结果，训练速度比之前的方法快3倍以上。 |
| [^20] | [Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference.](http://arxiv.org/abs/2307.09357) | 本教程介绍了使用IBM Analog Hardware Acceleration Kit (AIHWKit)进行神经网络训练和推断的方法，该工具包模拟了模拟内存计算（AIMC）的推断和训练，并提供了最佳实践和云环境中使用的优势。 |
| [^21] | [Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints.](http://arxiv.org/abs/2307.09342) | 该论文提出了一种学习选择伪布尔和线性整数约束的SAT编码的方法，通过使用监督机器学习方法和一组特征，可以有效地选择编码方式，并且专门为伪布尔和线性约束设计的新特征能够取得更好的性能。 |
| [^22] | [Variational Monte Carlo on a Budget -- Fine-tuning pre-trained Neural Wavefunctions.](http://arxiv.org/abs/2307.09337) | 这项研究提出了一种利用预训练的神经波函数模型进行精确的计算量子化学方法，避免了每次进行全优化的高计算成本，通过少量微调即可获得准确的相对能量。 |
| [^23] | [Exploiting Field Dependencies for Learning on Categorical Data.](http://arxiv.org/abs/2307.09321) | 传统的分类数据学习方法忽视了字段间的依赖关系，我们提出了一种利用字段依赖关系进行学习的方法，通过学习全局字段依赖矩阵并在实例级别上细化依赖矩阵，改进了字段之间的建模效果。 |
| [^24] | [Biomaker CA: a Biome Maker project using Cellular Automata.](http://arxiv.org/abs/2307.09320) | 介绍了生物制造者CA项目，利用细胞自动机模拟复杂的生物群系，并在GPU上通过Python JAX框架进行高性能计算。展示了植物代理如何生长、存活、繁殖和进化，形成稳定和不稳定的生物群系。介绍了端到端元进化和培养皿元进化的方法来使模型在恶劣环境中生存。 |
| [^25] | [Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media.](http://arxiv.org/abs/2307.09312) | 多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。 |
| [^26] | [Automatic Differentiation for Inverse Problems with Applications in Quantum Transport.](http://arxiv.org/abs/2307.09311) | 本文提出了一种自动微分方法，通过神经求解器和可导仿真技术，实现对反向量子传输问题的求解。通过该方法，可以设计连续传输特性和电流-电压特性。 |
| [^27] | [EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting.](http://arxiv.org/abs/2307.09306) | EigenTrajectory是一种用于轨迹预测的方法，通过使用一种新颖的轨迹描述符来形成一个紧凑的空间，有效捕捉高维社交互动和可行的未来，以代替传统的参数曲线拟合方法。该方法经过低秩逼近降低了轨迹描述符的复杂性，并将行人的历史路径转化为以空间-时间主成分表示的新的轨迹空间。 |
| [^28] | [Conformal prediction under ambiguous ground truth.](http://arxiv.org/abs/2307.09302) | 本文提出了一种适用于含有模糊地面真相的符合预测框架，解决了在缺乏明确地面真相标签的情况下低估不确定性的问题。 |
| [^29] | [Nested Elimination: A Simple Algorithm for Best-Item Identification from Choice-Based Feedback.](http://arxiv.org/abs/2307.09295) | 嵌套消除是一种简单易实现的算法，通过利用创新的消除准则和嵌套结构，能够以最少的样本数量和高置信水平识别出最受欢迎的项目。 |
| [^30] | [FlexiAST: Flexibility is What AST Needs.](http://arxiv.org/abs/2307.09286) | 这篇论文提出了一种名为FlexiAST的训练方法，可以为标准AST模型提供补丁大小的灵活性，使其在推理阶段可以与各种不同的补丁大小一起工作，而无需改变架构。实验证明，FlexiAST在保持评估能力的同时，提供了与标准AST模型相似的性能。 |
| [^31] | [End-to-End Neural Network Training for Hyperbox-Based Classification.](http://arxiv.org/abs/2307.09269) | 本论文提出了一种基于神经网络的端到端训练方法来处理基于超立方体的分类问题。与现有方法相比，该方法能够高效地训练超立方体模型，并大大减少训练时间，同时获得更优的分类结果。 |
| [^32] | [Mobility-Aware Joint User Scheduling and Resource Allocation for Low Latency Federated Learning.](http://arxiv.org/abs/2307.09263) | 本论文研究了移动用户在联邦学习系统中的通信可靠性问题，并提出了一种用户调度和资源分配方法来最小化训练延迟。该方法通过建立实际的用户移动模型和优化问题，综合考虑用户选择、基站分配和带宽分配等因素。 |
| [^33] | [Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds.](http://arxiv.org/abs/2307.09259) | 本文提出了一种通过使用神经网络自适应学习滤波器的方法，用于增强点云机器学习方法的准确性。同时，还提出了一种具有等变性的神经网络架构，以使得持久同调具有等变性。 |
| [^34] | [PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models.](http://arxiv.org/abs/2307.09254) | 本文提出了一种使用神经网络来量化生成式语言模型不确定性的PAC神经预测集学习方法，通过在多种语言数据集和模型上的实验证明，相比于标准基准方法，我们的方法平均提高了63％的量化不确定性。 |
| [^35] | [UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data.](http://arxiv.org/abs/2307.09249) | UniTabE是一种面向异构表格数据的统一预训练表格编码器，能够处理不同表格结构的挑战，并具有对多样化下游应用的适应性。 |
| [^36] | [Application of BERT in Wind Power Forecasting-Teletraan's Solution in Baidu KDD Cup 2022.](http://arxiv.org/abs/2307.09248) | 该论文提出了一种在风力发电预测中应用BERT模型的解决方案，并通过后处理添加日波动性使预测结果更符合每日周期性。该解决方案在百度KDD Cup 2022中取得了第三名。 |
| [^37] | [Towards Sustainable Deep Learning for Multi-Label Classification on NILM.](http://arxiv.org/abs/2307.09244) | 本研究提出了一种面向NILM的新型深度学习模型，通过改进计算和能源效率，实现了对NILM的多标签分类的增强。同时，还提出了一种测试方法，可以比较不同模型在虚拟数据集上的性能。 |
| [^38] | [Fusing Hand and Body Skeletons for Human Action Recognition in Assembly.](http://arxiv.org/abs/2307.09238) | 本文提出了一种将较少详细的身体骨架与高度详细的手部骨架结合的方法，通过使用CNN和transformers，有效地提高了组装场景中的动作识别效果。 |
| [^39] | [Detecting Throat Cancer from Speech Signals Using Machine Learning: A Reproducible Literature Review.](http://arxiv.org/abs/2307.09230) | 本研究对使用机器学习和人工智能从语音记录中检测喉癌的文献进行了综述，发现了22篇相关论文，讨论了它们的方法和结果。研究使用了神经网络和梅尔频率倒谱系数提取音频特征，并通过迁移学习实现了分类，取得了一定的准确率。 |
| [^40] | [A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning.](http://arxiv.org/abs/2307.09218) | 遗忘是深度学习中普遍存在的现象，不仅限于连续学习领域。解决遗忘问题面临多个挑战，包括平衡保留旧任务知识与快速学习新任务的挑战，管理任务干扰与冲突目标的挑战，以及防止隐私泄露等。遗忘不总是有害的，可以在某些情况下是有益且可取的，特别是在隐私保护场景中。 |
| [^41] | [How Many Neurons Does it Take to Approximate the Maximum?.](http://arxiv.org/abs/2307.09212) | 本研究研究了使用ReLU激活函数的神经网络近似计算连续分布下$d$个输入的最大值所需的网络大小，并提供了新的界限和分离结果。通过使用深度为2的网络近似最大值函数的新下界，我们提供了一种深度为$\mathcal{O}(\log(\log(d)))$，宽度为$\mathcal{O}(d)$的构造来改善深度需求。 |
| [^42] | [Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models.](http://arxiv.org/abs/2307.09209) | 该论文分析了情感分析和毒性检测模型，以探测对残障人士的明显偏见。研究使用偏见识别框架对社交媒体平台的对话进行了分析，并创建了一个测试语料库来量化明显的残障偏见。研究发现，所研究的模型均存在显著的偏见。 |
| [^43] | [Joint Microseismic Event Detection and Location with a Detection Transformer.](http://arxiv.org/abs/2307.09207) | 本文提出了一种使用检测变压器将微震事件检测和源定位统一为一个框架的方法，该方法可以实现实时微震监测，并在合成数据上进行了测试。 |
| [^44] | [Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model.](http://arxiv.org/abs/2307.09206) | 本文提出了一种名为TRADYN的概率地形和机器人感知前向动力学模型，能够适应在自主导航环境中的地形和机器人的变化，通过在模拟的二维导航环境中的实验证明，该模型在长视程轨迹预测任务中表现出较低的预测误差。 |
| [^45] | [Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning.](http://arxiv.org/abs/2307.09205) | 本文提出了一个名为DAFT-RL的框架，利用对象为中心的表示学习从视觉输入中提取对象，并学习对它们进行分类和推断其隐含参数，以实现高效的多对象强化学习。 |
| [^46] | [A benchmark of categorical encoders for binary classification.](http://arxiv.org/abs/2307.09191) | 本研究是迄今为止最全面的分类编码器基准研究，通过对来自不同家族的32种编码器配置进行广泛评估，以及36种实验因素和50个数据集的组合，展示了数据集选择、实验因素和聚合策略对基准研究结论的深远影响。 |
| [^47] | [Federated Learning for Computationally-Constrained Heterogeneous Devices: A Survey.](http://arxiv.org/abs/2307.09182) | 本调研对于在计算能力有限的异构设备上应用联邦学习提出了挑战，重点关注计算异构性。联邦学习是一种在设备之间共享知识但不公开私有数据的隐私保护方案。 |
| [^48] | [Efficient Prediction of Peptide Self-assembly through Sequential and Graphical Encoding.](http://arxiv.org/abs/2307.09169) | 本论文研究了肽自组装的预测问题，通过收集大量肽数据集，并使用序列和图形的编码方式进行深度学习模型的训练，对提高预测准确性进行了系统分析。 |
| [^49] | [Towards Trustworthy Dataset Distillation.](http://arxiv.org/abs/2307.09165) | 本论文提出了一种名为可信赖的数据集精炼（TrustDD）的新范式，通过同时考虑内部分布（InD）分类和外部分布（OOD）检测的问题，将大型数据集精炼为小型合成数据集，从而提高模型的效率和可信赖性。 |
| [^50] | [MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results.](http://arxiv.org/abs/2307.09143) | 这篇论文提出了一个新的小物体检测数据集SOD4SB，其中包含了大量的鸟类实例。论文介绍了该数据集的挑战细节，并概述了获奖的方法。这个数据集以及评估网站是公开可用的。 |
| [^51] | [Characterization of partial wetting by CMAS droplets using multiphase many-body dissipative particle dynamics and data-driven discovery based on PINNs.](http://arxiv.org/abs/2307.09142) | 本研究使用多相多体耗散粒子动力学和基于PINNs的数据驱动发现方法，表征了高黏度熔融CMAS液滴的部分润湿动力学，为解决高温应用中CMAS积聚和对设备的损害问题提供了创新解决方案。 |
| [^52] | [Mining of Single-Class by Active Learning for Semantic Segmentation.](http://arxiv.org/abs/2307.09109) | 本文介绍了一种名为"MiSiCAL"的单类别主动学习范式，通过深度强化学习构建了一个策略，利用数量-准确性相关性对特定类别进行高性能模型训练，特别适用于大批量大小的情况下。MiSiCAL能在许多类别上优于随机策略。 |
| [^53] | [Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards.](http://arxiv.org/abs/2307.09093) | 这篇论文研究了在非平稳环境中具有结构依赖关系的组合半强化学习问题，提出了一种从延迟的反馈中学习因果关系并做出决策的策略。 |
| [^54] | [A Federated learning model for Electric Energy management using Blockchain Technology.](http://arxiv.org/abs/2307.09080) | 这项研究提出了一种使用区块链技术的联邦学习模型，以解决发展中国家面临的能源短缺和电力负荷故障问题。通过利用可再生能源并采用联邦学习技术，该模型可以预测能源需求并满足消费者的需求。该研究还使用区块链确保数据交易的透明性、可追溯性和安全性。 |
| [^55] | [DiTTO: Diffusion-inspired Temporal Transformer Operator.](http://arxiv.org/abs/2307.09072) | DiTTO是一种扩散启发的算子学习方法，通过结合Transformer架构的元素，无需时间离散化连续解决时间相关PDEs，并在多维度的各种PDE中取得了最先进的准确性结果。 |
| [^56] | [Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net.](http://arxiv.org/abs/2307.09067) | 本论文评估了使用U-Net进行胎儿头超声图像分割的微调策略，通过使用轻量级的MobileNet作为编码器，并对有限的图像进行训练，可以获得与从头开始训练相媲美的分割性能，且优于其他策略。 |
| [^57] | [Learning Adaptive Neighborhoods for Graph Neural Networks.](http://arxiv.org/abs/2307.09065) | 本论文介绍了一种学习自适应邻域的图神经网络模型。通过构建一个端到端可微分的图生成器，模型能够学习选择每个节点的邻域和大小，并将其整合到现有的图卷积网络中。实验证明，在不同的任务和数据集上，该模型相比其他结构学习方法能获得更好的准确性。 |
| [^58] | [Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning.](http://arxiv.org/abs/2307.09060) | 本研究提出了一种基于类比马尔可夫链和深度学习的数据驱动模拟器，适用于预测法国和斯堪的纳维亚的长时间热浪。与卷积神经网络相比，该模拟器在概率预测任务上表现更好，并且经过适当评估和性能评估。 |
| [^59] | [Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces.](http://arxiv.org/abs/2307.09057) | 本文介绍了一个用于在低维欧几里得空间中计算两组点之间Gromov-Wasserstein问题的框架。该框架通过将问题重新表述为一个低维优化问题来解决计算困难，具有良好的可扩展性，并能够在大规模问题中找到全局解决方案。 |
| [^60] | [Outlier-Robust Tensor Low-Rank Representation for Data Clustering.](http://arxiv.org/abs/2307.09055) | 本文提出了一种异常鲁棒张量低秩表示方法，用于同时检测异常值和进行数据聚类。该方法基于张量奇异值分解（t-SVD）代数框架，并在较弱条件下具有恢复干净数据的行空间和检测异常值的可证明性能保证。此外，还提出了扩展方法以处理数据部分缺失的情况。 |
| [^61] | [qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers.](http://arxiv.org/abs/2307.09025) | qecGPT是一个通用框架，用于用生成模型解码量子纠错码。该模型利用Transformers学习逻辑运算符和综合的联合概率，在无监督预训练后可以高效计算和生成最可能的逻辑运算符，比传统方法更快更准确。 |
| [^62] | [U-shaped Transformer: Retain High Frequency Context in Time Series Analysis.](http://arxiv.org/abs/2307.09019) | 在时间序列分析中，传统的变压器模型在低秩特性方面存在缺陷，本论文提出了一种U型变压器模型，通过引入跳跃层连接和补丁合并与分割操作，实现了保留高频上下文的效果，并在实验中验证了其性能优于其他模型。 |
| [^63] | [Multimodal LLMs for health grounded in individual-specific data.](http://arxiv.org/abs/2307.09018) | 本文提出了一个多模态健康大型语言模型（HeLM），通过学习复杂数据模态的编码器，同时支持简单模态数据的文本序列化，HeLM可以有效地使用个体专属数据估计疾病风险。 |
| [^64] | [How is ChatGPT's behavior changing over time?.](http://arxiv.org/abs/2307.09009) | 本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。 |
| [^65] | [OxfordVGG Submission to the EGO4D AV Transcription Challenge.](http://arxiv.org/abs/2307.09006) | OxfordVGG团队的WhisperX系统在EGO4D AV转录挑战中以56.0％的词错误率（WER）排名第1，并提供了两个公开可用的文本规范化工具。 |
| [^66] | [Oracle Efficient Online Multicalibration and Omniprediction.](http://arxiv.org/abs/2307.08999) | 本文研究了在线对抗背景下的全面预测问题，提出了一种新的在线多校准算法，可以适用于无限的基准函数类，并且是Oracle高效的。 |
| [^67] | [GraphCL-DTA: a graph contrastive learning with molecular semantics for drug-target binding affinity prediction.](http://arxiv.org/abs/2307.08989) | GraphCL-DTA是一种使用分子语义进行药物靶标结合亲和力预测的图对比学习方法。通过设计图对比学习框架，可以学习到更本质和有效的药物表示。 |
| [^68] | [Neural Network Pruning as Spectrum Preserving Process.](http://arxiv.org/abs/2307.08982) | 本文提出了一种新颖的神经网络剪枝方法，该方法基于矩阵谱学习与神经网络训练之间的密切关系，通过矩阵稀疏化过程来保持频谱，从而得到轻量级神经网络版本。通过实验验证，该方法能够获得更好的剪枝结果。 |
| [^69] | [A Unifying Framework for Differentially Private Sums under Continual Observation.](http://arxiv.org/abs/2307.08970) | 这篇文章提出了一个统一的框架和高效算法，用于在不断观察下维护差分私有递减和。算法改进了之前的研究，并且实现了多项式递减权重下没有乘法误差的差分私有算法。 |
| [^70] | [Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information.](http://arxiv.org/abs/2307.08964) | 本论文提出了一种使用景观替代品的学习方法，旨在解决部分信息下数学优化问题中的挑战。这种方法可以通过学习优化器来加速优化过程，并且能够处理问题的不确定性。 |
| [^71] | [REX: Rapid Exploration and eXploitation for AI Agents.](http://arxiv.org/abs/2307.08962) | 本文提出了一种增强型的快速探索与利用的AI代理方法REX，它通过引入额外的奖励层和类似于UCB分数的概念，实现了更强大和高效的AI代理性能，并且具有离线行为利用和与基础模型无缝集成的优势。 |
| [^72] | [Discretization-based ensemble model for robust learning in IoT.](http://arxiv.org/abs/2307.08955) | 本论文研究了离散化技术和集成方法在提高IoT设备识别模型稳健性方面的应用，通过减少对抗攻击的敏感性和减少噪声或错误的影响来提高模型的稳定性和可靠性。 |
| [^73] | [Knowledge-infused Deep Learning Enables Interpretable Landslide Forecasting.](http://arxiv.org/abs/2307.08951) | 该论文提出了一种知识注入的深度学习流程，使用转换器网络LFIT从先前的知识和多源数据中学习滑坡的复杂非线性关系，实现了全面预测滑坡行为的能力，并提高了滑坡预测的综合性和可解释性。 |
| [^74] | [Alioth: A Machine Learning Based Interference-Aware Performance Monitor for Multi-Tenancy Applications in Public Cloud.](http://arxiv.org/abs/2307.08949) | Alioth是一种基于机器学习的性能监视器，用于监测公共云中多租户应用的干扰引起的性能下降。通过构建反映真实场景中复杂性和动态性的数据集，Alioth能够提供对于相关干扰事件的感知和分析能力。 |
| [^75] | [Mitigating Label Bias via Decoupled Confident Learning.](http://arxiv.org/abs/2307.08945) | 这项研究提出了一种名为DeCoLe的修剪方法，用于减轻标签偏见问题。研究在合成数据集和仇恨言论检测领域取得了成功的结果。 |
| [^76] | [Siamese Networks for Weakly Supervised Human Activity Recognition.](http://arxiv.org/abs/2307.08944) | 本文提出了一种使用Siamese网络进行弱监督人体活动识别的模型，通过仅利用数据样本对的相似性进行训练，该模型可以作为不同聚类算法的度量标准，在三个数据集上的评估结果证明了其有效性。 |
| [^77] | [NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning.](http://arxiv.org/abs/2307.08941) | 该论文通过使用神经切向核近似MLP融合，提出了一种高效的语言模型微调方法。实验证明，这种方法能够在降低计算和存储开销的同时保持较好的模型性能。 |
| [^78] | [Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks.](http://arxiv.org/abs/2307.08939) | 这项研究评估了基于深度神经网络的自适应巡航控制系统在隐蔽感知攻击下的安全性，并提出了一种上下文感知策略和基于优化的图像扰动生成方法。 |
| [^79] | [Multi-stage Neural Networks: Function Approximator of Machine Precision.](http://arxiv.org/abs/2307.08934) | 多阶段神经网络通过将训练过程分为不同阶段，并优化拟合残差，成功解决了常规神经网络中的谱偏差问题，提高了预测精度。 |
| [^80] | [IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness.](http://arxiv.org/abs/2307.08933) | IxDRL是一种基于有趣分析的新型可解释深度强化学习工具包，具备能力感知机制，能够提供人类操作员对RL代理能力的整体视图。 |
| [^81] | [On-the-fly machine learning for parametrization of the effective Hamiltonian.](http://arxiv.org/abs/2307.08929) | 本论文提出了一种基于贝叶斯线性回归的即时机器学习方法，用于参数化有效哈密顿量。该方法在各种系统中都可以得到准确的结果，包括以前的方法无法处理的复杂系统。 |
| [^82] | [Federated Large Language Model: A Position Paper.](http://arxiv.org/abs/2307.08925) | 我们提出了联邦式大规模语言模型的概念，通过联邦学习实现分散数据的共同训练共享模型，以应对公共数据可用性的限制和私有数据的隐私保护需求。我们讨论了预训练、微调和提示工程这三个组件的优势，并提出了实施策略。同时，我们探讨了FL和LLM集成带来的新挑战，并分析了现有解决方案和潜在障碍。 |
| [^83] | [Learning to Sample Tasks for Meta Learning.](http://arxiv.org/abs/2307.08924) | 通过实验得出了三个结论：没有通用的任务采样策略能保证元学习模型的性能；任务的多样性会导致模型在训练过程中出现欠拟合或过拟合的问题；模型的泛化性能受到任务的差异、任务熵和任务难度的影响。针对这些发现，提出了一种新颖的任务采样器ASr，它利用任务的差异、任务熵和任务难度来采样任务，并通过重新思考和提出一个简单而通用的元学习算法来优化ASr。大量实证实验表明了ASr的有效性。 |
| [^84] | [Optimistic Estimate Uncovers the Potential of Nonlinear Models.](http://arxiv.org/abs/2307.08921) | 通过乐观估计方法，研究揭示了非线性模型在拟合目标函数时的潜力，并提出了DNN的架构设计原则。 |
| [^85] | [Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees.](http://arxiv.org/abs/2307.08920) | 本论文介绍了一套新的连续时间强化学习算法，用于控制仿射非线性系统。这些算法解决了现有方法面临的复杂性、数值条件和维度扩展等设计挑战。 |
| [^86] | [Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images.](http://arxiv.org/abs/2307.08919) | 半监督和自监督学习在医学图像上的准确性与时间前沿进行比较，通过一个精心设计的基准研究来回答从业者的问题。 |
| [^87] | [Towards the Sparseness of Projection Head in Self-Supervised Learning.](http://arxiv.org/abs/2307.08913) | 该论文研究了自监督学习中投影头的稀疏性，发现通过在投影子空间中执行对比损失可以提升表示的质量，建议只有一部分特征是必要的，而稀疏的投影头可以增强模型的泛化性能。 |
| [^88] | [Sharpness-Aware Graph Collaborative Filtering.](http://arxiv.org/abs/2307.08910) | 锐度感知的图协同过滤中，提出了一种名为gSAM的训练方案，通过正则化权重损失函数的平坦程度，选择平坦最小值以提高泛化能力，在协同过滤中取得了优越的性能。 |
| [^89] | [Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology.](http://arxiv.org/abs/2307.08897) | 本文提出了一种基于多智能体强化学习方法的一型糖尿病患者个性化血糖控制系统，通过显著改善血糖控制、减少血糖变异性以及预防低血糖事件等，该方法具有潜力成为一种有效的治疗方法。 |
| [^90] | [Evaluating unsupervised disentangled representation learning for genomic discovery and disease risk prediction.](http://arxiv.org/abs/2307.08893) | 本文评估了无监督分离表示学习方法在基因探索和疾病风险预测中的应用，发现使用FactorVAE或beta-VAE相比标准VAE或非变分自动编码器可以显著改善哮喘和慢性阻塞性肺疾病的全基因组显著位点数量、遗传力和多基因风险评分的性能。 |
| [^91] | [The Predicted-Deletion Dynamic Model: Taking Advantage of ML Predictions, for Free.](http://arxiv.org/abs/2307.08890) | 本文提出了预测删除动态模型，利用机器学习预测了动态图中边的更新，解决了设计动态算法中的未知更新序列的瓶颈问题。这一模型在实际应用中具有实用价值，在理论上也具有研究价值。 |
| [^92] | [Examining the Effects of Degree Distribution and Homophily in Graph Learning Models.](http://arxiv.org/abs/2307.08881) | 本研究考察了两种额外的合成图生成器如何改进GraphWorld的评估，通过整合这些生成器，我们扩展了图空间的覆盖范围，同时保持了真实网络中的关键图属性。 |
| [^93] | [Modular Neural Network Approaches for Surgical Image Recognition.](http://arxiv.org/abs/2307.08880) | 本论文研究了可调模块化神经网络在手术图像识别中的应用。通过自我训练方法解决了数据不足和标注困难的问题，并采用模块化学习策略降低了复杂性和成本。 |
| [^94] | [Disentangling Node Attributes from Graph Topology for Improved Generalizability in Link Prediction.](http://arxiv.org/abs/2307.08877) | 该论文研究了节点属性和图拓扑之间的相互作用，并且提出了一种新方法UPNA，通过将预训练节点属性纳入模型来改善链接预测的泛化能力。该方法可以解决归纳链接预测问题，并具有优于现有技术的表现。 |
| [^95] | [Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation.](http://arxiv.org/abs/2307.08875) | 本文研究了鲁棒强化学习问题，提出了两种新的不确定性集合形式，使得大规模鲁棒强化学习变得可行。同时，提出了一个鲁棒的自然演员-评论家算法，通过函数逼近，该算法能够在有限时间内收敛到最佳鲁棒策略。 |
| [^96] | [Latent Space Representations of Neural Algorithmic Reasoners.](http://arxiv.org/abs/2307.08874) | 这项工作对神经算法推理器中执行算法时产生的潜在空间结构进行了详细分析，并提出了解决两种故障模式的方法。通过使用softmax聚合器解决分辨率丧失问题，以及衰减潜在空间来处理超出范围的值，这些改变在标准CLRS-30基准测试中大多数算法上实现了改进。 |
| [^97] | [An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient.](http://arxiv.org/abs/2307.08873) | 本研究提出了一种风险厌恶策略梯度的替代方法，通过使用基尼离差来替代方差，缓解了方差方法的局限性，并在实证评估中取得了高回报和低风险的成果。 |
| [^98] | [Meta-Value Learning: a General Framework for Learning with Learning Awareness.](http://arxiv.org/abs/2307.08863) | 元价值学习是一种带有学习意识的学习通用框架，通过分析智能体学习过程的相互作用，使用元价值函数来指导优化，并通过训练神经网络进行逼近，从而提供更可靠的改进方向。 |
| [^99] | [Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach.](http://arxiv.org/abs/2307.08859) | 本文提出了一种新的图神经网络课程学习方法，通过引入图复杂性形式化和模型能力作为困难度标准，以及考虑样本困难度和模型能力的不同视角进行训练，实现了对细粒度图困难度标准的纳入。 |
| [^100] | [An Admissible Shift-Consistent Method for Recommender Systems.](http://arxiv.org/abs/2307.08857) | 本研究提出了一种适用于推荐系统的新方法，该方法利用移动一致性约束解决了矩阵/张量完成问题，并满足可接受性准则和公平性定义，同时具有鲁棒性和可证明的性能特性。 |
| [^101] | [Autoregressive Diffusion Model for Graph Generation.](http://arxiv.org/abs/2307.08849) | 提出了一种自回归扩散模型用于图生成，通过定义节点吸收扩散过程和设计扩散排序网络以及去噪网络，能在离散图空间中高效地生成多样性的高质量图形。 |
| [^102] | [Privacy-preserving patient clustering for personalized federated learning.](http://arxiv.org/abs/2307.08847) | 这项研究提出了一种隐私保护的患者聚类方法来解决个性化联邦学习中的非IID数据问题 |
| [^103] | [Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War.](http://arxiv.org/abs/2307.08840) | 本研究提出了使用基于贝叶斯安全策略学习的机会约束优化方法，在越南战争期间的军事安全评估中进行实证研究。研究结果对于解决高风险算法决策中的挑战具有重要意义。 |
| [^104] | [A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access.](http://arxiv.org/abs/2307.08822) | 本文提出了一种基于元学习的预编码优化框架，通过利用紧凑神经网络过拟合来优化Rate-Splitting Multiple Access (RSMA)预编码，结合部分信道状态信息，从而绕过了其他训练数据的需求，同时在中等规模情况下达到了类似传统方法的平均和速率性能，在大规模情况下显著优于低复杂度算法。 |
| [^105] | [Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models.](http://arxiv.org/abs/2307.08816) | 本文介绍了一种利用强化学习代理模型加速Benders分解方法的方法，并通过实验证明了其相对于其他加速方案的30%更快的平均收敛速度。 |
| [^106] | [Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge.](http://arxiv.org/abs/2307.08813) | 本研究评估了不同大型语言模型在提取分子相互作用和通路知识方面的有效性，并讨论了未来机遇和挑战。 |
| [^107] | [DeepMem: ML Models as storage channels and their (mis-)applications.](http://arxiv.org/abs/2307.08811) | 本文提出了将机器学习模型作为存储通道的新视角，通过过度参数化来增加通道容量。通过在训练时嵌入信息，并利用黑盒访问实现信息的存储和提取。 |
| [^108] | [Operator Guidance Informed by AI-Augmented Simulations.](http://arxiv.org/abs/2307.08810) | 本文介绍了一种基于AI增强模拟指导的操作员指导方法，利用LSTM神经网络估计双峰双向海况下船舶响应统计量。通过对比低保真度和高保真度结果，证明了该方法的有效性。 |
| [^109] | [Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels.](http://arxiv.org/abs/2307.08809) | 本论文提出了FedLabel方法，在联邦学习中，客户端根据数据的专业性选择本地或全局模型对无标签数据进行伪标记，并通过全局-本地一致性正则化来利用本地和全局模型的知识。 |
| [^110] | [Anomaly Detection with Selective Dictionary Learning.](http://arxiv.org/abs/2307.08807) | 该论文提出了基于选择性字典学习的新的异常检测方法，主要贡献在于将字典学习算法和核字典学习算法改进为无监督方法，并提出了一种适用于大数据集问题的降维核字典学习方法，同时通过随机选择信号的方法改进算法以消除训练过程中的异常值。所有算法均被整合在一个异常检测工具箱中，并与标准基准结果进行了比较。 |
| [^111] | [Towards Automated Design of Riboswitches.](http://arxiv.org/abs/2307.08801) | 本文提出了一种新的方法libLEARNA，用于自动设计核酸开关。该方法通过考虑全局特性和序列结构特征，能够提供多样性的可变长度合格候选物的RNA关注库。实验表明，我们的方法能够获得30%更多独特高质量的候选物。 |
| [^112] | [regulAS: A Bioinformatics Tool for the Integrative Analysis of Alternative Splicing Regulome using RNA-Seq data.](http://arxiv.org/abs/2307.08800) | regulAS是一个用于从RNA-Seq数据中研究选择性剪切调控机制的生物信息学工具，它可以自动化计算实验、高效处理结果、优化工作流程，并具有基于公共数据仓库的数据检索、预测建模和灵活报告生成等功能。 |
| [^113] | [Reduced Kernel Dictionary Learning.](http://arxiv.org/abs/2307.08798) | 本文提出了一种降维核字典学习方法，通过输入信号的训练稀疏表示来获得核向量，并使用梯度下降步骤直接优化核向量，实验证明该方法能够在使用少量核向量的情况下提供更好的表示，并减少执行时间。 |
| [^114] | [Classification with Incoherent Kernel Dictionary Learning.](http://arxiv.org/abs/2307.08796) | 本文提出了一种基于非相干核字典学习的分类方法，通过改进标准线性对应物，实现了非相干字典学习的核版本，并对表示更新算法进行了改进。 |
| [^115] | [Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2307.08794) | 这篇论文提出了一个简单的框架，用于在多时间尺度多智能体强化学习中学习非平稳策略。他们利用智能体时间尺度的信息定义了周期性时间编码，并通过周期性多智能体策略学习多时间尺度引入的非平稳性的效果。他们还提出了一个使用神经网络的策略梯度算法来学习这些策略。 |
| [^116] | [Unsupervised Learning of Distributional Properties can Supplement Human Labeling and Increase Active Learning Efficiency in Anomaly Detection.](http://arxiv.org/abs/2307.08782) | 通过无监督学习分布特性进行主动学习可以提高异常检测的效率，并辅助人工标注来减少虚警数量，特别是在罕见案例的检测中。 |
| [^117] | [A mixed policy to improve performance of language models on math problems.](http://arxiv.org/abs/2307.08767) | 本文提出了一种混合策略的探索方法，利用强化学习来改进语言模型在数学问题上的性能，通过在抽象层和第二层采用不同的探索方式，取得了超过2%的性能增益。 |
| [^118] | [Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices.](http://arxiv.org/abs/2307.08766) | 该研究基于机器学习模型对光电容抗（PPG）信号进行训练，评估了使用可穿戴设备进行连续监测时的准确性和可靠性，并提出了解决干扰因素的方法。 |
| [^119] | [A Novel Application of Conditional Normalizing Flows: Stellar Age Inference with Gyrochronology.](http://arxiv.org/abs/2307.08753) | 这项工作展示了使用条件归一化流将数据驱动方法应用于光度数据，可以有效约束陀螺年龄法推断恒星年龄的精确性，并且在贝叶斯框架下得到了文献值的良好恢复。这一概率数据驱动解决方案有望扩大陀螺年龄法的适用范围。 |
| [^120] | [Certifying the Fairness of KNN in the Presence of Dataset Bias.](http://arxiv.org/abs/2307.08722) | 本文提出了一种方法，用于在存在数据集偏差的情况下对KNN的公平性进行认证。通过近似计算方法，我们实现了在抽象域中计算的方式，从而降低了计算成本，并在多个数据集上进行了实验证明方法的有效性。 |
| [^121] | [Intuitionistic Fuzzy Broad Learning System: Enhancing Robustness Against Noise and Outliers.](http://arxiv.org/abs/2307.08713) | 该论文提出了一种直觉模糊广泛学习系统（IF-BLS），用于增强对噪声和异常值的稳健性，通过为每个训练点分配模糊隶属度来减小噪声和异常值的影响。 |
| [^122] | [Machine Learning Meets Mental Training -- A Proof of Concept Applied to Memory Sports.](http://arxiv.org/abs/2307.08712) | 本文介绍了将机器学习与记忆运动的心理训练相结合的实际应用，旨在推动这个看似被低估的运动的发展。 |
| [^123] | [Efficient Strongly Polynomial Algorithms for Quantile Regression.](http://arxiv.org/abs/2307.08706) | 本文提出了针对量子回归的高效强多项式算法，填补了弱多项式算法的空白。对于二维QR，提出了一个具有确定性最坏时间复杂度为O(n^{4/3} polylog(n))和期望时间复杂度为O(n^{4/3})的算法。 |
| [^124] | [An R package for parametric estimation of causal effects.](http://arxiv.org/abs/2307.08686) | CausalModels是一个用于参数估计因果效应的R软件包，提供了一种简单易懂的框架，可以在一个单独的软件包中使用多种统计方法来估计因果效应。 |
| [^125] | [TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT.](http://arxiv.org/abs/2307.08674) | TableGPT是一个统一的框架，利用大型语言模型（LLMs）和外部功能命令使LLMs能够无缝地与表格进行交互，实现广泛的功能，并提供便利和可访问性给用户。其中的创新是全局表格表示的概念，使LLMs能够全面理解表格的结构和内容。 |
| [^126] | [Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study.](http://arxiv.org/abs/2307.08572) | 本研究重新审视了最小错误熵准则在处理非高斯噪声中的鲁棒性，并探讨了其在实际转移学习回归任务中的可行性和有用性。实验证明，在基本转移学习算法中，通过用最小错误熵代替均方误差损失，可以取得与现有方法相媲美的性能表现。 |
| [^127] | [Deep Learning with Passive Optical Nonlinear Mapping.](http://arxiv.org/abs/2307.08558) | 这项研究介绍了一种利用反射腔中的多次散射通过被动诱导光学非线性映射的设计，实现了光学数据压缩和高效处理的能力。 |
| [^128] | [Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images.](http://arxiv.org/abs/2307.08535) | 本文提出了一种多类别点云补全网络，能够从cine磁共振图像中重建多类别的3D心脏解剖结构。该网络能够解决重建任务中的稀疏和错位问题，并在合成数据集上取得了良好的重建效果。 |
| [^129] | [Nonlinear Processing with Linear Optics.](http://arxiv.org/abs/2307.08533) | 该论文提出了一种利用多次散射实现多层光学网络的新框架，可以以低光功率同时合成线性和非线性转换，实现能量高效和高速的光学实现神经网络。 |
| [^130] | [Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks.](http://arxiv.org/abs/2307.08466) | 本论文提出了一种基于神经网络的方法，用于在气体绝缘高压直流系统中普遍分类超高频局部放电信号。此方法可以区分不同的PD源，并且可以推广到未见过的操作电压倍数。同时，比较了时域和频域输入信号的性能，以及不同的归一化方案对减轻自由空间影响的影响。 |
| [^131] | [From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs.](http://arxiv.org/abs/2307.08433) | 这篇论文介绍了一种在连续时间动态图上具有低延迟的节点嵌入框架，通过提出流式低延迟的近似随机游走特征，计算时间感知节点嵌入以总结多跳信息。 |
| [^132] | [On the Robustness of Split Learning against Adversarial Attacks.](http://arxiv.org/abs/2307.07916) | 该论文评估了分裂学习对抗敌对攻击的鲁棒性，特别关注不受信任服务器只能访问模型中间层的情况。通过仅向服务器公开部分模型，分裂学习可以缓解敌对攻击威胁。 |
| [^133] | [An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets.](http://arxiv.org/abs/2307.07674) | 本文通过实证研究，探讨了在GFlowNets中使用回放缓冲区的有效性，评估了不同回放缓冲区采样技术对模式发现速度和质量的影响。 |
| [^134] | [Inverse Optimization for Routing Problems.](http://arxiv.org/abs/2307.07357) | 本研究提出了一种使用反向最优化（IO）学习路由问题决策者行为的方法，并在亚马逊末端路由研究挑战中测试了该方法，在复制人类驾驶员的路由偏好方面取得了第2名的成绩。 |
| [^135] | [Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning.](http://arxiv.org/abs/2307.07250) | 通过敌对双机器学习方法，可以量化和缓解深度神经网络在面对敌对输入时的脆弱性。 |
| [^136] | [Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality.](http://arxiv.org/abs/2307.06915) | 本文探索了一种加权平均随机梯度下降（SGD）方案，并建立了渐近正态性，提供了渐近有效的在线推理方法。此外，我们提出了一种自适应平均方案，具有最优的统计速度和有利的非渐近收敛性。 |
| [^137] | [Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks.](http://arxiv.org/abs/2307.06887) | 通过双层ReLU神经网络，本论文提出了一种可证明的多任务表示学习方法，用于解决神经网络在实践中同时训练多个任务时遇到的问题。 |
| [^138] | [Do DL models and training environments have an impact on energy consumption?.](http://arxiv.org/abs/2307.05520) | 本研究分析了模型架构和训练环境对训练更环保的计算机视觉模型的影响，并找出了能源效率和模型正确性之间的权衡关系。 |
| [^139] | [Secrets of RLHF in Large Language Models Part I: PPO.](http://arxiv.org/abs/2307.04964) | 本论文研究了大型语言模型中RLHF的秘密，重点关注了奖励模型、PPO和进程监督等技术路径，探索如何解决RLHF的稳定训练问题。 |
| [^140] | [Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case.](http://arxiv.org/abs/2307.04778) | 本论文介绍了一个基于统计分析的战略计划，通过研究金融公司LendingClub的实际案例，探索引入大数据平台和先进特征选择能力的可能性，以增加收入并降低风险。 |
| [^141] | [Gradient Surgery for One-shot Unlearning on Generative Model.](http://arxiv.org/abs/2307.04550) | 本文介绍了一种针对生成模型的一次性学习梯度手术方法，通过操纵梯度来消除数据对模型的影响，并在理论上进行了分析。 |
| [^142] | [Don't Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory.](http://arxiv.org/abs/2307.00497) | 该论文提出了一个无需使用记忆的联邦类增量学习框架，通过生成模型合成过去分布的样本，从而缓解联邦学习中的灾难性遗忘问题。 |
| [^143] | [Macro Placement by Wire-Mask-Guided Black-Box Optimization.](http://arxiv.org/abs/2306.16844) | 本文介绍了一种名为WireMask-BBO的黑盒优化框架，通过使用线掩模引导的贪心过程进行宏单元布局，在有效降低HPWL的同时节省大量时间。此方法还可以对现有布局进行微调，改善50%的HPWL。 |
| [^144] | [High Fidelity Image Counterfactuals with Probabilistic Causal Models.](http://arxiv.org/abs/2306.15764) | 提出了一个通用的因果生成建模框架，用于准确估计具有高保真度的图像反事实推理，通过利用因果中介分析和生成建模的思想，设计了新的深度因果机制，实验证明了该方法的准确性。 |
| [^145] | [SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design.](http://arxiv.org/abs/2306.15656) | SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。 |
| [^146] | [Geometric Ultrasound Localization Microscopy.](http://arxiv.org/abs/2306.15548) | 这项研究提出了一种基于几何框架的超声定位显微镜方法，通过仅依赖到达时间差信息实现微气泡的定位，并在精度和可靠性方面超越了现有的方法。 |
| [^147] | [Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction.](http://arxiv.org/abs/2306.09662) | 本文提出了一种合作的多目标架构，称为MOMA-DDPG，用于交通信号控制和碳减排问题。该方法涉及两种类型的智能体：一个专注于优化每个路口的本地交通，而另一个旨在优化全局交通吞吐量。结果显示，该方法优于现有最先进的方法，并解决了等待时间和碳排放量两个问题。 |
| [^148] | [Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective.](http://arxiv.org/abs/2306.07528) | 本文提出了点击模型不可知的统一非同策略学习排序（CUOLR）方法，通过离线强化学习（RL）直接学习最优排名，可以轻松地应用于各种点击模型。 |
| [^149] | [Mitigating Transformer Overconfidence via Lipschitz Regularization.](http://arxiv.org/abs/2306.06849) | 这项工作通过提出一种新的Lipschitz正则化Transformer (LRFormer)来减轻Transformer在预测中的过度自信问题，并在视觉基准实验证明了其优于现有方法的性能。 |
| [^150] | [K-Tensors: Clustering Positive Semi-Definite Matrices.](http://arxiv.org/abs/2306.06534) | 本文介绍了一种针对正半定矩阵的自一致性聚类算法（K-张量），通过考虑其特征结构，能够有效地将正半定矩阵进行分区。 |
| [^151] | [CLC: Cluster Assignment via Contrastive Representation Learning.](http://arxiv.org/abs/2306.05439) | 本文提出了一种基于对比学习的聚类方法（CLC），它使用对比学习直接学习聚类分配，并在大规模数据集上取得了更好的聚类性能。 |
| [^152] | [Unsupervised Embedding Quality Evaluation.](http://arxiv.org/abs/2305.16562) | 这项工作提出了评估嵌入质量的不同方法，关注如何以稳定的方式进行线性分离。从调查的文献和引入的新方法中，我们可以评估嵌入的质量，从而提高无监督学习的表现。(This work proposes new methods to evaluate the quality of embeddings, focusing on stable linear separation. From the surveyed literature and introduced novel methods, we can evaluate the quality of embeddings and improve the performance of unsupervised learning.) |
| [^153] | [Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders.](http://arxiv.org/abs/2305.16189) | 该论文提出了一种因子高斯混合变分自动编码器，用于多尺度聚类和源分离，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程，并在MRO数据集上展现了更好的性能。 |
| [^154] | [Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks.](http://arxiv.org/abs/2305.16044) | 本文提出了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），展示了噪声可以作为计算和学习的资源，并为一般脉冲神经元网络提供了一个框架。研究还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。 |
| [^155] | [Efficient Large-Scale Vision Representation Learning.](http://arxiv.org/abs/2305.13399) | 本文介绍了一种高效大规模的单模态视觉表示学习方法，解决了电商应用中视觉表示的挑战，并提供了消融研究和评估。 |
| [^156] | [Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention.](http://arxiv.org/abs/2305.13115) | 这篇论文提出了一种基于因果推理的框架，通过直接监督注意力函数，提供了强大的监督信号，使得基于注意力的图神经网络在嘈杂的图表达中更加稳健和具有一般性。 |
| [^157] | [Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees.](http://arxiv.org/abs/2305.11997) | 本文提出了一种可靠的神经网络反事实解释方法，该方法可以针对自然发生的模型变化提供高概率的鲁棒性。 |
| [^158] | [CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images.](http://arxiv.org/abs/2305.09211) | CB-HVTNet 提出了一种 Channel Boosted Hybrid Vision Transformer 网络，利用迁移学习生成增强通道，并结合使用 Transformers 和 CNN，在组织病理学图像中高效准确地评估淋巴细胞。 |
| [^159] | [Meta-Polyp: a baseline for efficient Polyp segmentation.](http://arxiv.org/abs/2305.07848) | 本研究提出Meta-Polyp，将Meta-Former与UNet融合并引入多尺度上采样块和Convformer块，解决了CNN和Vision Transformer在处理分布外数据集、缺失边界和小息肉时遇到的困难，提高了息肉分割的效率。 |
| [^160] | [Scalable Coupling of Deep Learning with Logical Reasoning.](http://arxiv.org/abs/2305.07617) | 本文介绍了一种可扩展的神经网络模型和损失函数，能够有效学习如何解决NP-hard推理问题，并在离散图模型上进行了实验验证。同时可以提高数据效率和可解释性，并具有对预测的控制能力。 |
| [^161] | [Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear Markov Chains.](http://arxiv.org/abs/2305.05097) | 本文设计了一种自我排斥随机游走模型，可实现较小的渐近采样方差，适用于网络拓扑的采样和邻域探索。 |
| [^162] | [Performance Gaps of Artificial Intelligence Models Screening Mammography -- Towards Fair and Interpretable Models.](http://arxiv.org/abs/2305.04422) | 该研究探讨了乳腺筛查 X 光片异常分类模型中的性能差距，尤其是与人口学和成像特征之间的关系，旨在开发公平和可解释的模型。 |
| [^163] | [Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models.](http://arxiv.org/abs/2305.03829) | 本研究采用贝叶斯深度学习估计多种治疗的因果后验分布，提高了基于图像的精准医疗的不确定性估计方法，以预测噪声多的医疗环境下的个体治疗效果。 |
| [^164] | [The Score-Difference Flow for Implicit Generative Modeling.](http://arxiv.org/abs/2304.12906) | 本文提出了一种新的评分差异流模型(SD flow)，它可以最优地减少两个分布之间的散度，同时解决Schr​​ödinger桥问题。与去噪扩散模型不同，它没有对先验分布施加任何限制，在一些基准数据集中优于其他方法。 |
| [^165] | [Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts.](http://arxiv.org/abs/2304.03209) | 提出了一种名为MORSE的基于隐式解剖渲染的通用神经渲染框架，旨在在医学图像分割中帮助融合高级语义相关内容和低级解剖特征。 |
| [^166] | [ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast.](http://arxiv.org/abs/2304.02689) | 本文提出了一种改进的对比学习框架ACTION++，通过自适应的解剖对比来改善半监督医学图像分割。 |
| [^167] | [FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer.](http://arxiv.org/abs/2304.02011) | 本文提出了一种使用加性噪声和神经风格迁移技术来模拟电子显微镜正向算子，以解决深度学习方法需要大量训练数据集的问题。该方法在粒子定位和分类任务上表现良好。 |
| [^168] | [Self-Supervised Clustering of Multivariate Time-Series Data for Identifying TBI Physiological States.](http://arxiv.org/abs/2303.13024) | 这篇论文提出了一种新的自监督聚类算法，能够在多元时间序列数据中确定并识别对于TBI等急性疾病治疗非常重要的生理状态。研究还利用临床数据验证并解释所识别的生理状态。 |
| [^169] | [Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm.](http://arxiv.org/abs/2303.06825) | 本文提出了一种基于Follow-the-regularized-leader算法的三重世界分析，并证明该算法使用负熵正则化器可以在线性bandit问题中获得最佳结果。 |
| [^170] | [Machine Learning Enhanced Hankel Dynamic-Mode Decomposition.](http://arxiv.org/abs/2303.06289) | 本文提出了一种基于深度学习DMD的方法，称为DLHDMD，利用Takens嵌入定理的基本见解开发了一种自适应学习方案，更好地捕捉了高维和混沌动力学，能够为混沌时间序列生成准确的动态。 |
| [^171] | [SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model.](http://arxiv.org/abs/2303.05118) | SLCA是一种用于连续学习的简单但极其有效的方法。它通过慢学习和分类器对齐来在预训练模型上提高泛化能力和解决渐进过拟合问题。 |
| [^172] | [Edit at your own risk: evaluating the robustness of edited models to distribution shifts.](http://arxiv.org/abs/2303.00046) | 本文研究了模型编辑对模型的鲁棒性的影响，并发现编辑通常会降低模型的鲁棒性，但具体程度取决于编辑算法和层的选择。基于这些发现，引入了一种新的模型编辑算法，通过插值不同层的特征来提高模型的鲁棒性。 |
| [^173] | [High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance.](http://arxiv.org/abs/2302.00999) | 本文提出了几种算法，它们在梯度/算子噪声具有有界中心的 α 阶矩的宽松假设下具备高概率收敛性。这些算法适用于光滑非凸、Polyak-Lojasiewicz、凸、强凸、拟强凸最小化问题以及Lipschitz、星形强协同且单调、拟协同的问题。 |
| [^174] | [Robust online active learning.](http://arxiv.org/abs/2302.00422) | 本文提出了一种自适应方法，用于鲁棒的在线主动学习，并在受污染的数据流中证明了其性能表现优异，同时确保了稳定性并减少异常值的负面影响。 |
| [^175] | [Internally Rewarded Reinforcement Learning.](http://arxiv.org/abs/2302.00270) | 这项研究探讨了一类强化学习问题，其中策略的奖励信号由与之相关且同时优化的判别器生成，导致学习过程不稳定。实验结果表明，修剪线性奖励函数可以稳定训练过程。 |
| [^176] | [Execution-based Code Generation using Deep Reinforcement Learning.](http://arxiv.org/abs/2301.13816) | 使用深度强化学习的PPOCoder框架将预训练的编程语言模型和Proximal Policy Optimization技术结合，通过利用代码执行和结构对齐的非可微反馈，实现了更高效的代码生成。 |
| [^177] | [Deep Learning for Mean Field Games with non-separable Hamiltonians.](http://arxiv.org/abs/2301.02877) | 本文介绍了一种使用深度学习方法解决非可分离哈密顿量的高维均场博弈问题的新方法。通过使用两个神经网络来近似未知解和前向-后向条件，该方法在效率上具有优势，并且能够处理高达300维的问题。该方法还与现有方法进行了比较，并在交通流问题上展示了其有效性。同时，通过使用通用逼近定理，证明了使用单层隐藏层的神经网络近似的收敛性。 |
| [^178] | [Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization.](http://arxiv.org/abs/2212.13556) | 本研究通过考虑多种信息论框架，证明了在随机凸优化领域中，没有一个"信息论"框架能够建立梯度下降的最小最大速率。同时，通过分析一种常见的策略，我们证明了高斯噪声破坏迭代的方法也无法建立最小最大速率。这些结果表明，需要新的思路来分析使用信息论技术的梯度下降方法。 |
| [^179] | [Deep Riemannian Networks for EEG Decoding.](http://arxiv.org/abs/2212.10426) | 本研究分析了深度黎曼网络对EEG的应用，探讨了网络大小、端到端能力、模型训练对模型性能的影响，并比较了其与基于黎曼几何的最先进方法。 |
| [^180] | [Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning.](http://arxiv.org/abs/2212.03181) | 本文提出了一种基于漏斗函数的强化学习算法，用于在连续状态空间中学习鲁棒满足信号时态逻辑规范的时间依赖策略。 |
| [^181] | [Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test.](http://arxiv.org/abs/2211.16596) | 该论文提出了一种针对稀有事件的新的因果发现方法，基于收集到的时间不变动态系统的数据，构建了叠加数据集和条件独立性检验的方法。该方法能够揭示在变量第一次经历低概率实现时才会显现的因果关系，具有很好的可行性和可扩展性。 |
| [^182] | [Resource frugal optimizer for quantum machine learning.](http://arxiv.org/abs/2211.04965) | 提出了一种名为Refoqus的资源节约的量子随机梯度下降优化器，通过同时随机采样数据集和测量操作，能够保存大量资源。 |
| [^183] | [Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning.](http://arxiv.org/abs/2210.16299) | 本文研究了在线、实时解决逆强化学习中存在的多个解的挑战，提出了一种能够收敛到近似等价解的正则化历史堆栈观察器。通过开发新的数据丰富性条件，证明了该技术的有效性。 |
| [^184] | [Heat Demand Forecasting with Multi-Resolutional Representation of Heterogeneous Temporal Ensemble.](http://arxiv.org/abs/2210.13108) | 本文提出了一种使用多分辨率异质时间序列集合表示的热需求预测框架，该框架通过神经网络对时间序列进行编码，并利用CNN来预测未来的热负荷。实验结果表明，该框架在丹麦实际数据上的预测效果优于其他最先进的方法。 |
| [^185] | [Multi-Objective GFlowNets.](http://arxiv.org/abs/2210.12765) | 本论文提出了一种名为多目标GFlowNets (MOGFNs) 的方法，用于在多目标优化问题中生成多样的Pareto最优解。该方法基于GFlowNets，并引入了两种变体：MOGFN-PC和MOGFN-AL。实验结果表明，MOGFNs在各种任务中都表现出了很好的效果。 |
| [^186] | [Continuous Monte Carlo Graph Search.](http://arxiv.org/abs/2210.01426) | 连续蒙特卡洛图搜索（CMCGS）是一种新颖的蒙特卡洛树搜索（MCTS）的扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS通过将相似状态聚类，并共享相同的动作策略，实现了高性能的在线规划。 |
| [^187] | [Frouros: A Python library for drift detection in machine learning systems.](http://arxiv.org/abs/2208.06868) | Frouros是一个开源的Python库，可以检测任何机器学习框架中的概念和数据漂移，易于维护和扩展。 |
| [^188] | [DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation.](http://arxiv.org/abs/2207.09920) | 本论文提出了DESCN模型，通过深度整体空间交叉网络的方式，从端到端的角度建模个体治疗效果估计。该模型可以解决传统方法中的分布偏移和样本不平衡问题。 |
| [^189] | [Stochastic Optimal Control for Collective Variable Free Sampling of Molecular Transition Paths.](http://arxiv.org/abs/2207.02149) | 本文介绍了一种解决分子转变路径采样问题的新方法，通过随机最优控制采样，避免了传统方法中需要选择集合变量的问题，能够适用于较大的系统。 |
| [^190] | [\nu-Flows: Conditional Neutrino Regression.](http://arxiv.org/abs/2207.00664) | \nu-Flows是一种使用条件归一化流和深度可逆神经网络的方法，可以限制中微子动力学的似然空间，并实现更精确的动量重建和喷注关联的改善。 |
| [^191] | [TabText: A Flexible and Contextual Approach to Tabular Data Representation.](http://arxiv.org/abs/2206.10381) | TabText是一种处理和特征提取框架，通过转换内容为语言并利用预训练的大型语言模型，从表格数据中提取上下文信息。通过应用TabText框架可以生成高性能且简单的机器学习基准模型，减少数据预处理的工作量。该框架在医疗预测任务中展现出良好的效果。 |
| [^192] | [Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting.](http://arxiv.org/abs/2205.14568) | 本研究提出了一种名为Cal-PIT的方法，通过学习一个概率-概率映射，解决了预测分布的诊断和校准问题，来实现有条件校准。 |
| [^193] | [FedFormer: Contextual Federation with Attention in Reinforcement Learning.](http://arxiv.org/abs/2205.13697) | FedFormer是一种新的强化学习联邦策略，利用Transformer Attention上下文地汇总来自不同学习智能体的嵌入，具有更高的回报和更高的效率。 |
| [^194] | [On-device modeling of user's social context and familiar places from smartphone-embedded sensor data.](http://arxiv.org/abs/2205.08790) | 本研究提出了一种新颖的、无监督的轻量级方法，通过在用户的移动设备上建模用户的社交背景和地点，从而从手机嵌入式传感器数据中提取高层次和语义丰富的上下文特征。 |
| [^195] | [A survey on learning from imbalanced data streams: taxonomy, challenges, empirical study, and reproducible experimental framework.](http://arxiv.org/abs/2204.03719) | 这篇论文调查了从不平衡数据流中学习的现状，提出了一个全面的实验框架，对多种具有挑战性的不平衡数据流场景进行评估，并比较了24种最先进的算法的优缺点。 |
| [^196] | [Online Observer-Based Inverse Reinforcement Learning.](http://arxiv.org/abs/2011.02057) | 本文提出一种基于在线观测器的逆强化学习方法，将二次成本函数下的输出反馈逆强化学习问题转化为状态估计问题。通过开发两种基于观测器的技术，包括一种历史堆栈重复使用先前状态估计的新颖观测器方法，实现了收敛性和鲁棒性的理论保证，并通过仿真实验验证了在有噪声和无噪声测量情况下的性能。 |

# 详细

[^1]: 使用并行混合网络预测发电厂中的蒸汽质量流量

    Forecasting the steam mass flow in a powerplant using the parallel hybrid network. (arXiv:2307.09483v1 [cs.LG])

    [http://arxiv.org/abs/2307.09483](http://arxiv.org/abs/2307.09483)

    这项研究使用并行混合神经网络结构来预测发电厂中的蒸汽质量流量，相比纯经典和纯量子模型，该混合模型在测试集上取得了更好的性能，平均平方误差降低了5.7倍和4.9倍，并且相对误差较小，最多提升了2倍。

    

    高效可持续的发电是能源领域的一个关键问题。尤其是热电厂在准确预测蒸汽质量流量方面面临困难，这对于运营效率和成本降低至关重要。在本研究中，我们使用一个并行混合神经网络结构，该结构将参数化量子电路和传统的前馈神经网络相结合，特别设计用于工业环境中的时间序列预测，以提高对未来15分钟内蒸汽质量流量的预测能力。我们的结果表明，并行混合模型优于独立的经典和量子模型，在训练后的测试集上相对于纯经典模型和纯量子网络，平均平方误差（MSE）损失分别降低了5.7倍和4.9倍。此外，该混合模型在测试集上表现出相对误差较小，比纯经典模型更好，最多提升了2倍。

    Efficient and sustainable power generation is a crucial concern in the energy sector. In particular, thermal power plants grapple with accurately predicting steam mass flow, which is crucial for operational efficiency and cost reduction. In this study, we use a parallel hybrid neural network architecture that combines a parametrized quantum circuit and a conventional feed-forward neural network specifically designed for time-series prediction in industrial settings to enhance predictions of steam mass flow 15 minutes into the future. Our results show that the parallel hybrid model outperforms standalone classical and quantum models, achieving more than 5.7 and 4.9 times lower mean squared error (MSE) loss on the test set after training compared to pure classical and pure quantum networks, respectively. Furthermore, the hybrid model demonstrates smaller relative errors between the ground truth and the model predictions on the test set, up to 2 times better than the pure classical model.
    
[^2]: 透明度在未知估值的重复一次性出价拍卖中的作用

    The Role of Transparency in Repeated First-Price Auctions with Unknown Valuations. (arXiv:2307.09478v1 [cs.GT])

    [http://arxiv.org/abs/2307.09478](http://arxiv.org/abs/2307.09478)

    本文研究了一次性出价拍卖中透明度对遗憾最小化的影响，通过对拍卖透明度与环境特性的分析，揭示了在拍卖中学习最优出价的速度。

    

    我们研究了在一系列一次性出价拍卖中，单个竞标人在只有在赢得拍卖时才知道物品价值的情况下，遗憾最小化的问题。我们的主要贡献是完整地刻画了拍卖的透明度对最小化感到遗憾的极小极大问题的影响，其中透明度调控了拍卖师在每次拍卖结束时公开竞争出价的信息量。我们的结果适用于不同假设（随机的、对抗性的和平滑变体）生成竞标人估值和竞争出价的环境。这些极小极大率揭示了透明度和环境性质之间的相互作用，以及影响一个人学习在一次性出价拍卖中如何最优出价的速度。

    We study the problem of regret minimization for a single bidder in a sequence of first-price auctions where the bidder knows the item's value only if the auction is won. Our main contribution is a complete characterization, up to logarithmic factors, of the minimax regret in terms of the auction's transparency, which regulates the amount of information on competing bids disclosed by the auctioneer at the end of each auction. Our results hold under different assumptions (stochastic, adversarial, and their smoothed variants) on the environment generating the bidder's valuations and competing bids. These minimax rates reveal how the interplay between transparency and the nature of the environment affects how fast one can learn to bid optimally in first-price auctions.
    
[^3]: 进展中的序列数据科学

    Towards Ordinal Data Science. (arXiv:2307.09477v1 [cs.AI])

    [http://arxiv.org/abs/2307.09477](http://arxiv.org/abs/2307.09477)

    本文讨论了序列数据科学的发展，并介绍了一种新的研究议程：使用序列结构来衡量和计算对象之间的关系，并从中推断知识。这种方法具有广泛的学科应用价值。

    

    排序是衡量（经验）数据中对象之间关系的主要方法之一。然而，与使用对象的数字属性的方法相比，发展出的序列方法数量相对较少。造成这一情况的原因之一是在上个世纪，计算资源的可用性有限，无法满足序列计算所需。此外，对于这一研究领域来说，另一个重要原因是基于顺序的方法通常被视为对实际数据应用过于数学严谨。因此，本文将讨论不同的方法来衡量和“计算”序列结构（一类特定的有向图），并展示如何从其中推断知识。我们的目标是将序列数据科学建立为一项全新的研究议程。除了与其他重要的机器学习和知识表示方法的交叉互补外，广泛的学科领域也将受益于此。

    Order is one of the main instruments to measure the relationship between objects in (empirical) data. However, compared to methods that use numerical properties of objects, the amount of ordinal methods developed is rather small. One reason for this is the limited availability of computational resources in the last century that would have been required for ordinal computations. Another reason -- particularly important for this line of research -- is that order-based methods are often seen as too mathematically rigorous for applying them to real-world data. In this paper, we will therefore discuss different means for measuring and 'calculating' with ordinal structures -- a specific class of directed graphs -- and show how to infer knowledge from them. Our aim is to establish Ordinal Data Science as a fundamentally new research agenda. Besides cross-fertilization with other cornerstone machine learning and knowledge representation methods, a broad range of disciplines will benefit from t
    
[^4]: 过度思考真相：理解语言模型如何处理虚假演示

    Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])

    [http://arxiv.org/abs/2307.09476](http://arxiv.org/abs/2307.09476)

    该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。

    

    现代语言模型可以通过少量示范进行复杂模式的模仿学习，使其能够在没有微调的情况下完成具有挑战性的任务。然而，模仿也可能导致模型在上下文中重现不准确或有害的内容。我们通过模型的内部表示来研究有害的模仿，并确定了两个相关现象：过度思考和错误归纳头。第一个现象，过度思考，在给出正确与错误的少量示范时，我们从中间层解码预测。在早期层中，两种示范引起了相似的模型行为，但在某个“关键层”之后，给出错误示范的准确性逐渐降低。第二个现象，错误归纳头，可能是过度思考的一种机制性原因：这些是位于较晚层的头部，它们关注并复制先前示范中的错误信息，其削弱会减少过度思考现象。

    Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
    
[^5]: 带有网络可分离交互的多人零和马尔可夫游戏

    Multi-Player Zero-Sum Markov Games with Networked Separable Interactions. (arXiv:2307.09470v1 [cs.GT])

    [http://arxiv.org/abs/2307.09470](http://arxiv.org/abs/2307.09470)

    本文研究了一种新的马尔可夫游戏类别，通过带有网络可分离交互的多人零和马尔可夫游戏模型（MZNMGs）来模拟非合作多智能体顺序决策中的局部交互结构。作者确定了MG可被表示为MZNMG的必要和充分条件，并证明其Markov CCE集合与Markov NE集合相等；此外，在无限时间折扣MZNMG中找到近似的Markov稳定CCE是PPAD难题，除非网络具有“星状结构”。

    

    本文研究了一种新的马尔可夫游戏类别，即带有网络可分离交互的多人零和马尔可夫游戏（MZNMGs），以模拟非合作多智能体顺序决策中的局部交互结构。我们将MZNMG定义为一个模型，其中与每个状态相关的辅助游戏的收益是零和的，并且在某个交互网络上的邻居之间具有一些可分离（即聚合矩阵）结构。我们首先确定了马尔可夫游戏能够被表示为MZNMG的必要和充分条件，并且证明在这些游戏中，马尔可夫粗糙相关均衡（CCE）的集合缩减为马尔可夫纳什均衡（NE）的集合，即前者对所有玩家的每个状态的边际化乘积结果得到后者。此外，我们证明在无限时间折扣MZNMGs中找到近似马尔可夫\emph{稳定}CCE是PPAD难题，除非底层网络具有``星状结构''。

    We study a new class of Markov games (MGs), \textit{Multi-player Zero-sum Markov Games} with {\it Networked separable interactions} (MZNMGs), to model the local interaction structure in non-cooperative multi-agent sequential decision-making. We define an MZNMG as a model where {the payoffs of the auxiliary games associated with each state are zero-sum and} have some separable (i.e., polymatrix) structure across the neighbors over some interaction network. We first identify the necessary and sufficient conditions under which an MG can be presented as an MZNMG, and show that the set of Markov coarse correlated equilibrium (CCE) collapses to the set of Markov Nash equilibrium (NE) in these games, in that the {product of} per-state marginalization of the former for all players yields the latter. Furthermore, we show that finding approximate Markov \emph{stationary} CCE in infinite-horizon discounted MZNMGs is \texttt{PPAD}-hard, unless the underlying network has a ``star topology''. Then, 
    
[^6]: 对机器学习应用中高保真度等离子体模拟中磁场拓扑的图表示

    Graph Representation of the Magnetic Field Topology in High-Fidelity Plasma Simulations for Machine Learning Applications. (arXiv:2307.09469v1 [physics.plasm-ph])

    [http://arxiv.org/abs/2307.09469](http://arxiv.org/abs/2307.09469)

    该论文提出了一种对高保真度等离子体模拟中的磁场拓扑进行图表示的方法，并应用于地球磁层模拟中，旨在挑战机器学习社区通过图机器学习方法解决具有广泛潜力影响的科学问题。

    

    通过对模拟等离子体中的磁场进行拓扑分析，可以在各种不同的场景下研究多种物理现象。其中一个应用是磁重新连接，这是与磁场拓扑动力学相关的一种现象，在三维空间中很难检测和描述。我们提出了一个可扩展的流程，用于对三维磁场矢量场进行拓扑数据分析和时空图表示。我们在地球磁层的模拟中演示了我们的方法，这是由Vlasiator产生的，Vlasiator是一个基于Vlasov理论的超级计算机规模的近地空间模拟。这项工作的目的是挑战机器学习社区采用基于图的机器学习方法来解决一个在科学上具有广泛潜力影响的问题。

    Topological analysis of the magnetic field in simulated plasmas allows the study of various physical phenomena in a wide range of settings. One such application is magnetic reconnection, a phenomenon related to the dynamics of the magnetic field topology, which is difficult to detect and characterize in three dimensions. We propose a scalable pipeline for topological data analysis and spatiotemporal graph representation of three-dimensional magnetic vector fields. We demonstrate our methods on simulations of the Earth's magnetosphere produced by Vlasiator, a supercomputer-scale Vlasov theory-based simulation for near-Earth space. The purpose of this work is to challenge the machine learning community to explore graph-based machine learning approaches to address a largely open scientific problem with wide-ranging potential impact.
    
[^7]: 一种用于容错量子错误纠正的低温存储敏化神经解码器

    A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction. (arXiv:2307.09463v1 [quant-ph])

    [http://arxiv.org/abs/2307.09463](http://arxiv.org/abs/2307.09463)

    本研究报告了一种基于内存计算的神经解码器推理加速器的设计和性能分析，该解码器用于容错量子错误纠正，旨在最小化解码时间并确保解码方法的可扩展性。

    

    量子错误纠正(QEC)的神经解码器依赖于神经网络来分类从错误纠正编码中提取的综合征，并找到合适的恢复操作员来保护逻辑信息免受错误影响。尽管神经解码器性能良好，但仍存在一些重要的实际要求，如将解码时间最小化以满足重复错误纠正方案中的综合征生成速度，以及确保解码方法的可扩展性随着编码距离的增加而增加。设计一个专用的集成电路以与量子处理器共同完成解码任务似乎是必要的，因为将信号从低温环境中引出并进行外部处理会导致不必要的延迟和最终的布线瓶颈。在这项工作中，我们报道了一种基于内存计算的神经解码器推理加速器的设计和性能分析。

    Neural decoders for quantum error correction (QEC) rely on neural networks to classify syndromes extracted from error correction codes and find appropriate recovery operators to protect logical information against errors. Despite the good performance of neural decoders, important practical requirements remain to be achieved, such as minimizing the decoding time to meet typical rates of syndrome generation in repeated error correction schemes, and ensuring the scalability of the decoding approach as the code distance increases. Designing a dedicated integrated circuit to perform the decoding task in co-integration with a quantum processor appears necessary to reach these decoding time and scalability requirements, as routing signals in and out of a cryogenic environment to be processed externally leads to unnecessary delays and an eventual wiring bottleneck. In this work, we report the design and performance analysis of a neural decoder inference accelerator based on an in-memory comput
    
[^8]: 电路分析的可解释性是否具有可扩展性？来自毛丫鼠中多项选择能力的证据。

    Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. (arXiv:2307.09458v1 [cs.LG])

    [http://arxiv.org/abs/2307.09458](http://arxiv.org/abs/2307.09458)

    本论文研究了电路分析在最先进的语言模型中的可扩展性，通过对70B毛丫鼠模型进行多项选择题的分析，发现现有的逻辑层归因和激活修补技术具有可扩展性，并进一步研究了注意力头的语义特征。

    

    电路分析是一种理解语言模型内部机制的有前途的技术。然而，现有的分析都是在远离最先进技术的小型模型中进行的。为了解决这个问题，我们在70B毛丫鼠模型中进行了一项案例研究，旨在测试电路分析的可扩展性。具体而言，我们研究了多项选择题，调查了毛丫鼠在知道正确答案文本的情况下是否能够识别出正确答案标签。我们发现已有的逻辑层归因、注意力模式可视化和激活修补技术在毛丫鼠模型中具有自然的可扩展性，使我们能够识别和分类一小组“输出节点”（注意力头和多层感知机）。我们进一步研究了“正确字母”类别的注意力头，旨在了解其特征的语义，结果有所不同。对于正常的多项选择问题，我们显著压缩了查询。

    \emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \emph{label} given knowledge of the correct answer \emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).  We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, ke
    
[^9]: 光滑注意力用于深度多示例学习：应用于CT颅内出血检测

    Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection. (arXiv:2307.09457v1 [eess.IV])

    [http://arxiv.org/abs/2307.09457](http://arxiv.org/abs/2307.09457)

    光滑注意力深度多示例学习模型（SA-DMIL）用于CT颅内出血检测，通过引入平滑性约束来学习空间依赖关系，取得了比传统MIL模型更好的性能和与当前最先进方法相比的更好的结果。

    

    多示例学习（MIL）已广泛应用于医学成像诊断，在其中包袋标签已知且包袋内的实例标签未知。传统MIL假设每个包袋内的实例是来自给定分布的独立样本。然而，实例通常是空间上或顺序上有序的，并且人们会期望相邻实例具有相似的诊断重要性。为了解决这个问题，在本研究中，我们提出了一种光滑注意力深度MIL（SA-DMIL）模型。通过对编码对每个包袋中每个实例的注意力的潜在函数引入一阶和二阶约束来实现平滑性。该方法应用于头部CT扫描中颅内出血（ICH）的检测。结果表明，这种新颖的SA-DMIL模型：（a）在扫描（包袋）和切片（实例）层面上比非光滑注意力MIL模型实现了更好的性能；（b）学习了切片之间的空间依赖关系；并且（c）优于当前最先进的MIL方法。

    Multiple Instance Learning (MIL) has been widely applied to medical imaging diagnosis, where bag labels are known and instance labels inside bags are unknown. Traditional MIL assumes that instances in each bag are independent samples from a given distribution. However, instances are often spatially or sequentially ordered, and one would expect similar diagnostic importance for neighboring instances. To address this, in this study, we propose a smooth attention deep MIL (SA-DMIL) model. Smoothness is achieved by the introduction of first and second order constraints on the latent function encoding the attention paid to each instance in a bag. The method is applied to the detection of intracranial hemorrhage (ICH) on head CT scans. The results show that this novel SA-DMIL: (a) achieves better performance than the non-smooth attention MIL at both scan (bag) and slice (instance) levels; (b) learns spatial dependencies between slices; and (c) outperforms current state-of-the-art MIL methods
    
[^10]: 逆问题和线性插拔去噪器中的收敛正则化方法

    Convergent regularization in inverse problems and linear plug-and-play denoisers. (arXiv:2307.09441v1 [math.NA])

    [http://arxiv.org/abs/2307.09441](http://arxiv.org/abs/2307.09441)

    本论文概述了逆问题中的经典正则化理论和近期的数据驱动方法，以及对PnP算法及其收敛性的讨论。

    

    插拔（PnP）去噪是一种使用现成图像去噪器求解图像逆问题的流行迭代框架。他们的实证成功推动了一系列研究，旨在了解在对去噪器进行各种假设的情况下，PnP迭代的收敛性。虽然已经进行了大量研究来建立PnP迭代在去噪器的不同正则条件下的收敛性，但关于收敛解在测量噪声水平趋近于零时的渐近性质，即PnP方法在对去噪器进行合理假设的情况下是否可以被证明为收敛的正则化方案，目前所知甚少。本文有两个目的：首先，我们概述了逆问题中的经典正则化理论，并综述了一些近期的数据驱动方法，这些方法可被证明为收敛的正则化方案。然后，我们继续讨论PnP算法及其已建立的收敛性。

    Plug-and-play (PnP) denoising is a popular iterative framework for solving imaging inverse problems using off-the-shelf image denoisers. Their empirical success has motivated a line of research that seeks to understand the convergence of PnP iterates under various assumptions on the denoiser. While a significant amount of research has gone into establishing the convergence of the PnP iteration for different regularity conditions on the denoisers, not much is known about the asymptotic properties of the converged solution as the noise level in the measurement tends to zero, i.e., whether PnP methods are provably convergent regularization schemes under reasonable assumptions on the denoiser. This paper serves two purposes: first, we provide an overview of the classical regularization theory in inverse problems and survey a few notable recent data-driven methods that are provably convergent regularization schemes. We then continue to discuss PnP algorithms and their established convergenc
    
[^11]: 无监督条件槽注意力用于物体中心学习

    Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])

    [http://arxiv.org/abs/2307.09437](http://arxiv.org/abs/2307.09437)

    本文提出了一种无监督的条件槽注意力方法，通过使用概率性槽字典（PSD）实现了专门的槽位绑定和物体层次调节分布，在多个后续任务中展示了其优势。

    

    提取物体层次的表示以进行后续的推理任务是人工智能中涌现的一个领域。在无监督的设置中学习物体中心表示面临多个挑战，其中一个关键挑战是将任意数量的物体实例绑定到专门的物体槽位。最近的物体中心表示方法如槽位注意力利用迭代式注意力学习具有动态推理层级绑定的可组合表示，但未能达到专门的槽位绑定。为了解决这个问题，本文提出了一种使用新颖的概率性槽字典（PSD）的无监督条件槽注意力。我们将PSD定义为（i）抽象的物体层次属性向量作为键，（ii）参数化高斯分布作为相应的值。我们在多个后续任务中展示了学习到的具体物体层次调节分布的好处，包括物体发现、组合式场景生成和组合式视觉推理。

    Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show
    
[^12]: 在NetHack中的模仿学习的规模律

    Scaling Laws for Imitation Learning in NetHack. (arXiv:2307.09423v1 [cs.LG])

    [http://arxiv.org/abs/2307.09423](http://arxiv.org/abs/2307.09423)

    本文研究了在NetHack游戏中的模仿学习，发现通过扩大模型和数据规模可以改进模仿学习的效果，并建立了训练计算最优IL代理人的幂律。

    

    模仿学习 (IL) 是机器学习中最常用的方法之一。然而，虽然强大，但许多研究发现它往往不能完全恢复出潜在的专家行为。然而，这些研究没有深入探究模型和数据规模的扩大在其中的作用。受最近在自然语言处理 (NLP) 领域的工作的启发，在那里“扩大规模”已经导致了越来越有能力的领域特定语言模型 (LLMs)，我们研究了仔细扩大模型和数据规模是否可以在模仿学习的设置中带来类似的改进。为了展示我们的发现，我们将重点放在 NetHack 游戏上，这是一个具有程序生成、随机性、长期依赖性和部分可观测性的具有挑战性的环境。我们发现 IL 的损失和平均回报随着计算预算的变化而平滑变化且强相关，从而在模型大小和样本数量方面为训练计算最优的 IL 代理人的计算预算建立了幂律。我们预测并训练了几个具有 IL 的NetHack代理。

    Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL an
    
[^13]: 在非平稳环境中具有高成本特征的在线学习

    Online Learning with Costly Features in Non-stationary Environments. (arXiv:2307.09388v1 [cs.LG])

    [http://arxiv.org/abs/2307.09388](http://arxiv.org/abs/2307.09388)

    该论文提出了一种在非平稳环境中进行在线学习的方法，其中收集有益信息是昂贵的。研究扩展了上下文乐观设置，允许代理观察特征状态的子集，以在最小化信息成本和最大化收益之间进行权衡。

    

    在顺序决策问题中，最大化长期回报是主要目标。大多数现有方法假设附加信息是免费提供的，使得学习代理能够在做决策之前观察到所有特征状态。然而，在现实世界的问题中，收集有益信息往往是昂贵的。这意味着，除了个体奖励之外，学习特征状态的观察对于改善决策策略也是必要的。在非平稳环境中，奖励和成本分布随时间发生突变，这进一步加剧了双重学习问题。为了解决上述双重学习问题，我们扩展了上下文乐观设置，并允许代理观察特征状态的子集。目标是最大化长期平均收益，即平均累积回报和平均支付成本之差。因此，代理面临着在最小化信息成本和最大化收益之间的权衡。

    Maximizing long-term rewards is the primary goal in sequential decision-making problems. The majority of existing methods assume that side information is freely available, enabling the learning agent to observe all features' states before making a decision. In real-world problems, however, collecting beneficial information is often costly. That implies that, besides individual arms' reward, learning the observations of the features' states is essential to improve the decision-making strategy. The problem is aggravated in a non-stationary environment where reward and cost distributions undergo abrupt changes over time. To address the aforementioned dual learning problem, we extend the contextual bandit setting and allow the agent to observe subsets of features' states. The objective is to maximize the long-term average gain, which is the difference between the accumulated rewards and the paid costs on average. Therefore, the agent faces a trade-off between minimizing the cost of informa
    
[^14]: 批量预测器在分布内具有广义性。

    Batched Predictors Generalize within Distribution. (arXiv:2307.09379v1 [stat.ML])

    [http://arxiv.org/abs/2307.09379](http://arxiv.org/abs/2307.09379)

    批量预测器提供了指数级更强的泛化保证，可应用于离线测试前化合物质量的预测任务。

    

    我们研究了批量预测器的广义性质，即任务是预测一小组（或批量）示例的均值标签的模型。批量预测范式对于部署在离线测试前确定一组化合物的质量的模型尤为相关。通过利用适当的Rademacher复杂性的广义化，我们证明批量预测器具有指数级更强的泛化保证，与标准的逐个样本方法相比。令人惊讶的是，该提议的上界独立于过参数化。我们的理论洞察力在各种任务、架构和应用中通过实验证实。

    We study the generalization properties of batched predictors, i.e., models tasked with predicting the mean label of a small set (or batch) of examples. The batched prediction paradigm is particularly relevant for models deployed to determine the quality of a group of compounds in preparation for offline testing. By utilizing a suitable generalization of the Rademacher complexity, we prove that batched predictors come with exponentially stronger generalization guarantees as compared to the standard per-sample approach. Surprisingly, the proposed bound holds independently of overparametrization. Our theoretical insights are validated experimentally for various tasks, architectures, and applications.
    
[^15]: 数据交叉分割以提高强化学习算法交易的泛化能力

    Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading. (arXiv:2307.09377v1 [cs.LG])

    [http://arxiv.org/abs/2307.09377](http://arxiv.org/abs/2307.09377)

    本研究提出了一种基于强化学习算法的交易算法，通过使用学习到的预测模型的信号进行交易，以应对在交易较少的金融市场和不同类资产市场中的挑战。

    

    在算法交易系统中，机器学习的应用越来越普遍。在一个典型的设置中，使用监督学习来预测资产的未来价格，这些预测驱动着简单的交易和执行策略。当预测具有足够的信号、市场流动性足够大且交易成本较低时，这种方法非常有效。然而，在交易较少的金融市场和房地产或汽车等不同类资产市场中，这些条件通常不成立。在这些市场中，交易策略必须考虑到采取相对较难改变的仓位对长期影响。在本研究中，我们提出了一种基于强化学习算法的交易算法，该算法根据学习到的预测模型的信号进行交易，并解决了这些挑战。我们在马来西亚交易所的20多年股票数据上测试了我们的算法。

    The use of machine learning in algorithmic trading systems is increasingly common. In a typical set-up, supervised learning is used to predict the future prices of assets, and those predictions drive a simple trading and execution strategy. This is quite effective when the predictions have sufficient signal, markets are liquid, and transaction costs are low. However, those conditions often do not hold in thinly traded financial markets and markets for differentiated assets such as real estate or vehicles. In these markets, the trading strategy must consider the long-term effects of taking positions that are relatively more difficult to change. In this work, we propose a Reinforcement Learning (RL) algorithm that trades based on signals from a learned predictive model and addresses these challenges. We test our algorithm on 20+ years of equity data from Bursa Malaysia.
    
[^16]: 通过矩阵表述增强支持向量机中的模式分类

    Enhancing Pattern Classification in Support Vector Machines through Matrix Formulation. (arXiv:2307.09372v1 [cs.LG])

    [http://arxiv.org/abs/2307.09372](http://arxiv.org/abs/2307.09372)

    本文介绍了一种改进的支持向量机（SVM）方法，通过引入矩阵表述来解决多类别和多标签设置下的灵活性和整合额外项的限制。通过在对偶问题中采用加速梯度下降方法，我们提高了矩阵SVM的解决效率，并证明其在时间效率和性能上与传统的二进制相关性SVM方法相当。这种矩阵表述揭示出了传统方法中难以察觉的关键洞见和优势。

    

    支持向量机（SVM）由于其成功实现统计学习理论而成为分类器领域内备受赞誉。然而，在多类别和多标签设置下，现有SVM模型中基于向量的表述存在灵活性和整合额外项来应对特定挑战的限制。为了克服这些限制，我们的研究论文专注于引入一种能够有效解决这些约束的矩阵表述来作为SVM的解决方案。通过在对偶问题中采用加速梯度下降方法，我们显著提高了解决Matrix-SVM问题的效率。对多标签和多类别数据集的实验评估表明，矩阵SVM在时间效率上达到了优越性能，同时提供了与二进制相关性SVM相似的结果。此外，我们的矩阵表述揭示了传统基于向量的方法中难以察觉的关键洞见和优势。

    Support Vector Machines (SVM) have gathered significant acclaim as classifiers due to their successful implementation of Statistical Learning Theory. However, in the context of multiclass and multilabel settings, the reliance on vector-based formulations in existing SVM-based models poses limitations regarding flexibility and ease of incorporating additional terms to handle specific challenges. To overcome these limitations, our research paper focuses on introducing a matrix formulation for SVM that effectively addresses these constraints. By employing the Accelerated Gradient Descent method in the dual, we notably enhance the efficiency of solving the Matrix-SVM problem. Experimental evaluations on multilabel and multiclass datasets demonstrate that Matrix SVM achieves superior time efficacy while delivering similar results to Binary Relevance SVM.  Moreover, our matrix formulation unveils crucial insights and advantages that may not be readily apparent in traditional vector-based not
    
[^17]: 稀疏高斯图模型的离散优化：计算和统计角度

    Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives. (arXiv:2307.09366v1 [cs.LG])

    [http://arxiv.org/abs/2307.09366](http://arxiv.org/abs/2307.09366)

    本文提出了基于离散优化的稀疏高斯图模型学习问题的新方法，并提供了大规模求解器来获取良好的原始解。

    

    我们考虑了学习基于无向高斯图模型的稀疏图的问题，这是统计机器学习中的一个关键问题。给定来自具有p个变量的多元高斯分布的n个样本，目标是估计p×p的逆协方差矩阵（也称为精度矩阵），假设它是稀疏的（即具有少数非零条目）。我们提出了GraphL0BnB这一新的估计方法，它基于伪似然函数的l0惩罚版本，而大多数早期方法都是基于l1松弛。我们的估计方法可以被形式化为一个凸混合整数规划（MIP），使用现成的商用求解器在大规模计算时可能很难计算。为了解决MIP问题，我们提出了一个定制的非线性分支定界（BnB）框架，用于使用定制的一阶方法来解决节点放松问题。作为我们BnB框架的副产品，我们提出了用于获得独立兴趣的良好原始解的大规模求解器。

    We consider the problem of learning a sparse graph underlying an undirected Gaussian graphical model, a key problem in statistical machine learning. Given $n$ samples from a multivariate Gaussian distribution with $p$ variables, the goal is to estimate the $p \times p$ inverse covariance matrix (aka precision matrix), assuming it is sparse (i.e., has a few nonzero entries). We propose GraphL0BnB, a new estimator based on an $\ell_0$-penalized version of the pseudolikelihood function, while most earlier approaches are based on the $\ell_1$-relaxation. Our estimator can be formulated as a convex mixed integer program (MIP) which can be difficult to compute at scale using off-the-shelf commercial solvers. To solve the MIP, we propose a custom nonlinear branch-and-bound (BnB) framework that solves node relaxations with tailored first-order methods. As a by-product of our BnB framework, we propose large-scale solvers for obtaining good primal solutions that are of independent interest. We d
    
[^18]: 零成本代理的评估——从神经结构性能到模型的鲁棒性

    An Evaluation of Zero-Cost Proxies -- from Neural Architecture Performance to Model Robustness. (arXiv:2307.09365v1 [cs.LG])

    [http://arxiv.org/abs/2307.09365](http://arxiv.org/abs/2307.09365)

    这篇论文评估了常见的零成本代理在神经结构搜索中的作用，特别关注了性能预测和模型鲁棒性。研究发现预测鲁棒性使得从现有的零成本代理进行预测更加具有挑战性。

    

    零成本代理现如今被广泛研究和应用于神经结构搜索中。它们通过利用未经训练的参数来预测结构的性能，展示出令人印象深刻的能力。这些技术大大加快了搜索速度。然而，至今在NAS领域中，很少有关于同时搜索性能和鲁棒性的研究。因此，零成本代理的主要关注点是结构的准确率，而模型的鲁棒性应该同样重要。本文分析了常见的零成本代理在流行的NAS-Bench-201搜索空间中作为性能预测器的能力。我们对鲁棒性的单一预测任务和干净准确率与鲁棒准确率的综合多目标进行了研究。我们进一步分析了代理的特征重要性，并展示了预测鲁棒性使得从现有的零成本代理进行预测变得更加具有挑战性。

    Zero-cost proxies are nowadays frequently studied and used to search for neural architectures. They show an impressive ability to predict the performance of architectures by making use of their untrained weights. These techniques allow for immense search speed-ups. So far the joint search for well-performing and robust architectures has received much less attention in the field of NAS. Therefore, the main focus of zero-cost proxies is the clean accuracy of architectures, whereas the model robustness should play an evenly important part. In this paper, we analyze the ability of common zero-cost proxies to serve as performance predictors for robustness in the popular NAS-Bench-201 search space. We are interested in the single prediction task for robustness and the joint multi-objective of clean and robust accuracy. We further analyze the feature importance of the proxies and show that predicting the robustness makes the prediction task from existing zero-cost proxies more challenging. As
    
[^19]: MOCA: 自监督学习通过预测掩码式在线码本分配实现表示学习

    MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])

    [http://arxiv.org/abs/2307.09361](http://arxiv.org/abs/2307.09361)

    MOCA是一种自监督学习方法，通过预测掩码式在线码本分配来实现表示学习。它同时具备良好的语境推理属性和对图像扰动的不变性，并在低样本设置和各种评估协议中取得了最新的最先进结果，训练速度比之前的方法快3倍以上。

    

    自监督学习可以用于缓解Vision Transformer网络对大型全注释数据集的贪婪需求。不同类别的自监督学习提供了具有良好语境推理属性的表示，例如使用掩码图像建模策略，或者对图像扰动具有不变性的表示，例如使用对比方法。在这项工作中，我们提出了一种单阶段、独立的方法MOCA，使用基于高级特征（而不是像素级细节）定义的新型掩码和预测目标来统一这两种期望的属性。此外，我们展示了如何以协同和计算高效的方式有效地应用这两种学习范式。通过这样做，我们在低样本设置上实现了新的最先进结果，并且在各种评估协议中取得了强大的实验结果，其训练速度至少比之前的方法快3倍。

    Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.
    
[^20]: 使用IBM模拟内存硬件加速套件进行神经网络训练和推断

    Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference. (arXiv:2307.09357v1 [cs.ET])

    [http://arxiv.org/abs/2307.09357](http://arxiv.org/abs/2307.09357)

    本教程介绍了使用IBM Analog Hardware Acceleration Kit (AIHWKit)进行神经网络训练和推断的方法，该工具包模拟了模拟内存计算（AIMC）的推断和训练，并提供了最佳实践和云环境中使用的优势。

    

    模拟内存计算（AIMC）是减少深度神经网络（DNN）推断和训练的延迟和能源消耗的一种有前景的方法。然而，AIMC芯片中的噪声和非线性器件特性以及非理想的外围电路要求调整DNN以在此类硬件上实现与数字计算等效的精度。在这个教程中，我们详细介绍了如何使用最近发布的IBM模拟硬件加速套件（AIHWKit）进行这样的调整和评估，该套件可在https://github.com/IBM/aihwkit上免费获得。AIHWKit是一个Python库，可以使用AIMC模拟推断和训练DNN。我们详细描述了AIHWKit设计、功能和最佳实践，以正确进行推断和训练。我们还介绍了模拟AI云组合器的概述，该组合提供了在完全托管的云环境中使用AIHWKit模拟平台的优势。

    Analog In-Memory Computing (AIMC) is a promising approach to reduce the latency and energy consumption of Deep Neural Network (DNN) inference and training. However, the noisy and non-linear device characteristics, and the non-ideal peripheral circuitry in AIMC chips, require adapting DNNs to be deployed on such hardware to achieve equivalent accuracy to digital computing. In this tutorial, we provide a deep dive into how such adaptations can be achieved and evaluated using the recently released IBM Analog Hardware Acceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit. The AIHWKit is a Python library that simulates inference and training of DNNs using AIMC. We present an in-depth description of the AIHWKit design, functionality, and best practices to properly perform inference and training. We also present an overview of the Analog AI Cloud Composer, that provides the benefits of using the AIHWKit simulation platform in a fully managed cloud setting. Finally, we
    
[^21]: 学习选择伪布尔和线性整数约束的SAT编码

    Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints. (arXiv:2307.09342v1 [cs.AI])

    [http://arxiv.org/abs/2307.09342](http://arxiv.org/abs/2307.09342)

    该论文提出了一种学习选择伪布尔和线性整数约束的SAT编码的方法，通过使用监督机器学习方法和一组特征，可以有效地选择编码方式，并且专门为伪布尔和线性约束设计的新特征能够取得更好的性能。

    

    许多约束满足和优化问题可以通过将它们编码为布尔可满足性问题（SAT）的实例来有效地解决。然而，即使是最简单的约束类型在文献中也有很多编码方式，性能差异很大，选择适当的编码方式对于给定的问题实例并不是一件简单的事情。我们采用监督机器学习方法研究选择伪布尔和线性约束的编码问题。我们展示了可以使用标准约束问题的特征集来有效地选择编码方式；然而，我们使用专门为伪布尔和线性约束设计的一组新特征获得了更好的性能。事实上，在选择未见过的问题类别的编码方式时，我们取得了不错的结果。当使用相同的特征集时，我们的结果与AutoFolio相比表现良好。我们讨论了实例特征对于选择编码方式任务的相对重要性。

    Many constraint satisfaction and optimisation problems can be solved effectively by encoding them as instances of the Boolean Satisfiability problem (SAT). However, even the simplest types of constraints have many encodings in the literature with widely varying performance, and the problem of selecting suitable encodings for a given problem instance is not trivial. We explore the problem of selecting encodings for pseudo-Boolean and linear constraints using a supervised machine learning approach. We show that it is possible to select encodings effectively using a standard set of features for constraint problems; however we obtain better performance with a new set of features specifically designed for the pseudo-Boolean and linear constraints. In fact, we achieve good results when selecting encodings for unseen problem classes. Our results compare favourably to AutoFolio when using the same feature set. We discuss the relative importance of instance features to the task of selecting the
    
[^22]: 低成本的变分蒙特卡洛法——优化预训练的神经波函数

    Variational Monte Carlo on a Budget -- Fine-tuning pre-trained Neural Wavefunctions. (arXiv:2307.09337v1 [physics.chem-ph])

    [http://arxiv.org/abs/2307.09337](http://arxiv.org/abs/2307.09337)

    这项研究提出了一种利用预训练的神经波函数模型进行精确的计算量子化学方法，避免了每次进行全优化的高计算成本，通过少量微调即可获得准确的相对能量。

    

    在计算量子化学中，精确解决薛定谔方程是关键挑战。基于深度学习的变分蒙特卡洛法（DL-VMC）最近在准确性方面超越了传统方法，但只能以巨大的计算开销为代价。在许多领域，模型只需进行一次训练，然后应用于推理，而准确的DL-VMC迄今为止需要为每个新的问题实例进行完整优化，即使是对于小分子也需要消耗数千个GPU小时。我们提出了一种经过自监督式波函数优化的大规模化学多样性分子集进行预训练的DL-VMC模型。将该模型应用于没有任何优化的新分子，可以得到胜过CCSD（T）-2Z等已建立方法的波函数和绝对能量。为了获得准确的相对能量，只需对这个基准模型进行少量微调步骤。我们通过一个全面的端到端机器学习模型实现了这一点。

    Obtaining accurate solutions to the Schr\"odinger equation is the key challenge in computational quantum chemistry. Deep-learning-based Variational Monte Carlo (DL-VMC) has recently outperformed conventional approaches in terms of accuracy, but only at large computational cost. Whereas in many domains models are trained once and subsequently applied for inference, accurate DL-VMC so far requires a full optimization for every new problem instance, consuming thousands of GPUhs even for small molecules. We instead propose a DL-VMC model which has been pre-trained using self-supervised wavefunction optimization on a large and chemically diverse set of molecules. Applying this model to new molecules without any optimization, yields wavefunctions and absolute energies that outperform established methods such as CCSD(T)-2Z. To obtain accurate relative energies, only few fine-tuning steps of this base model are required. We accomplish this with a fully end-to-end machine-learned model, consist
    
[^23]: 利用字段依赖性进行分类数据的学习

    Exploiting Field Dependencies for Learning on Categorical Data. (arXiv:2307.09321v1 [cs.LG])

    [http://arxiv.org/abs/2307.09321](http://arxiv.org/abs/2307.09321)

    传统的分类数据学习方法忽视了字段间的依赖关系，我们提出了一种利用字段依赖关系进行学习的方法，通过学习全局字段依赖矩阵并在实例级别上细化依赖矩阵，改进了字段之间的建模效果。

    

    传统的分类数据学习方法往往忽视了数据集中字段（也称为特征）之间的依赖关系，因为它们仅依赖于数据点嵌入的分类/回归损失。相比之下，我们提出了一种新的分类数据学习方法，旨在利用字段之间的依赖关系。我们不是全局建模特征的统计信息（如特征的协方差矩阵），而是学习一个捕捉字段之间依赖关系的全局字段依赖矩阵，并利用不同权重（称为局部依赖建模）在实例级别上改进全局字段依赖矩阵的建模。我们的算法利用元学习范式，即在元学习算法的内部循环中无需使用标签就可以细化依赖矩阵，而外部循环则交织了嵌入矩阵和依赖矩阵的更新。

    Traditional approaches for learning on categorical data underexploit the dependencies between columns (\aka fields) in a dataset because they rely on the embedding of data points driven alone by the classification/regression loss. In contrast, we propose a novel method for learning on categorical data with the goal of exploiting dependencies between fields. Instead of modelling statistics of features globally (i.e., by the covariance matrix of features), we learn a global field dependency matrix that captures dependencies between fields and then we refine the global field dependency matrix at the instance-wise level with different weights (so-called local dependency modelling) w.r.t. each field to improve the modelling of the field dependencies. Our algorithm exploits the meta-learning paradigm, i.e., the dependency matrices are refined in the inner loop of the meta-learning algorithm without the use of labels, whereas the outer loop intertwines the updates of the embedding matrix (the
    
[^24]: 生物制造者CA：使用细胞自动机的生物制造者项目

    Biomaker CA: a Biome Maker project using Cellular Automata. (arXiv:2307.09320v1 [cs.AI])

    [http://arxiv.org/abs/2307.09320](http://arxiv.org/abs/2307.09320)

    介绍了生物制造者CA项目，利用细胞自动机模拟复杂的生物群系，并在GPU上通过Python JAX框架进行高性能计算。展示了植物代理如何生长、存活、繁殖和进化，形成稳定和不稳定的生物群系。介绍了端到端元进化和培养皿元进化的方法来使模型在恶劣环境中生存。

    

    我们介绍了生物制造者CA：使用细胞自动机（CA）的生物制造者项目。在生物制造者CA中，形态发生是一个重要的主题，小种子需要在养分匮乏的环境中成长为植物状的生物体，最终通过变异来繁殖，以使生物群系得以长期存活。我们通过2D网格上的CA规则模拟复杂的生物群系，并通过Python JAX框架在GPU上并行计算。我们展示了该项目允许多种不同类型的环境和“物理”定律，以及不同的模型架构和突变策略。我们进一步分析了一些配置，展示了植物代理如何生长、存活、繁殖和进化，形成稳定和不稳定的生物群系。然后，我们演示了如何通过端到端元进化或更精确高效的方法（称为培养皿元进化）来使模型在恶劣环境中生存。最后，我们展示了如何进行高性能计算以评估生物群系的稳定性和进化趋势。

    We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA). In Biomaker CA, morphogenesis is a first class citizen and small seeds need to grow into plant-like organisms to survive in a nutrient starved environment and eventually reproduce with variation so that a biome survives for long timelines. We simulate complex biomes by means of CA rules in 2D grids and parallelize all of its computation on GPUs through the Python JAX framework. We show how this project allows for several different kinds of environments and laws of 'physics', alongside different model architectures and mutation strategies. We further analyze some configurations to show how plant agents can grow, survive, reproduce, and evolve, forming stable and unstable biomes. We then demonstrate how one can meta-evolve models to survive in a harsh environment either through end-to-end meta-evolution or by a more surgical and efficient approach, called Petri dish meta-evolution. Finally, we show how to perfo
    
[^25]: 多模态讨论变换器：整合文本、图像和图变换器以检测社交媒体上的仇恨言论。

    Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])

    [http://arxiv.org/abs/2307.09312](http://arxiv.org/abs/2307.09312)

    多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。

    

    我们提出了一种新颖的多模态基于图的变换器模型，名为多模态讨论变换器（mDT），用于检测在线社交网络中的仇恨言论。与传统的仅使用文本的方法不同，我们将标记评论为仇恨言论的方法围绕文本和图像的整体分析展开。这是通过利用图变换器来捕捉评论周围整个讨论中的上下文关系，并采用交织融合层来组合文本和图像嵌入，而不是单独处理不同的模态。我们将模型的性能与仅处理文本的基线进行比较，还进行了广泛的消融研究。最后，我们展望了多模态解决方案在在线环境中提供社会价值的未来工作，并认为捕捉对话的整体视图极大地推进了检测反社会行为的努力。

    We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
    
[^26]: 带有应用于量子传输的反问题的自动微分

    Automatic Differentiation for Inverse Problems with Applications in Quantum Transport. (arXiv:2307.09311v1 [cs.LG])

    [http://arxiv.org/abs/2307.09311](http://arxiv.org/abs/2307.09311)

    本文提出了一种自动微分方法，通过神经求解器和可导仿真技术，实现对反向量子传输问题的求解。通过该方法，可以设计连续传输特性和电流-电压特性。

    

    本文提出了一种神经求解器和可导仿真的量子传输边界模型，用于反向量子传输问题。神经求解器用于设计连续传输特性，可导仿真用于设计电流-电压特性。

    A neural solver and differentiable simulation of the quantum transmitting boundary model is presented for the inverse quantum transport problem. The neural solver is used to engineer continuous transmission properties and the differentiable simulation is used to engineer current-voltage characteristics.
    
[^27]: EigenTrajectory：用于多模态轨迹预测的低秩描述符

    EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting. (arXiv:2307.09306v1 [cs.CV])

    [http://arxiv.org/abs/2307.09306](http://arxiv.org/abs/2307.09306)

    EigenTrajectory是一种用于轨迹预测的方法，通过使用一种新颖的轨迹描述符来形成一个紧凑的空间，有效捕捉高维社交互动和可行的未来，以代替传统的参数曲线拟合方法。该方法经过低秩逼近降低了轨迹描述符的复杂性，并将行人的历史路径转化为以空间-时间主成分表示的新的轨迹空间。

    

    捕捉高维社交互动和可行的未来对于预测轨迹至关重要。为了解决这种复杂性，已经致力于通过参数曲线拟合来减少输出变量的维度，例如贝塞尔曲线和B样条函数。然而，这些函数并不适合考虑到社会可接受的人体动态。在本文中，我们提出了一种名为EigenTrajectory（$\mathbb{ET}$）的轨迹预测方法，该方法使用一种新颖的轨迹描述符来形成一个紧凑的空间，即$\mathbb{ET}$空间，以代替欧氏空间来表示行人的移动。我们首先通过低秩逼近降低轨迹描述符的复杂性。然后，我们将行人的历史路径转化为以空间-时间主成分表示的$\mathbb{ET}$空间，并将其输入到现成的轨迹预测模型中。

    Capturing high-dimensional social interactions and feasible futures is essential for predicting trajectories. To address this complex nature, several attempts have been devoted to reducing the dimensionality of the output variables via parametric curve fitting such as the B\'ezier curve and B-spline function. However, these functions, which originate in computer graphics fields, are not suitable to account for socially acceptable human dynamics. In this paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory prediction approach that uses a novel trajectory descriptor to form a compact space, known here as $\mathbb{ET}$ space, in place of Euclidean space, for representing pedestrian movements. We first reduce the complexity of the trajectory descriptor via a low-rank approximation. We transform the pedestrians' history paths into our $\mathbb{ET}$ space represented by spatio-temporal principle components, and feed them into off-the-shelf trajectory forecasting models. The inputs
    
[^28]: 含有不确定地面真相的符合预测

    Conformal prediction under ambiguous ground truth. (arXiv:2307.09302v1 [cs.LG])

    [http://arxiv.org/abs/2307.09302](http://arxiv.org/abs/2307.09302)

    本文提出了一种适用于含有模糊地面真相的符合预测框架，解决了在缺乏明确地面真相标签的情况下低估不确定性的问题。

    

    在安全关键的分类任务中，符合预测可以通过提供置信区间来进行严格的不确定性量化，其中包括真正类别的用户指定的概率。这通常假设有一个独立的校准集合，并且能够访问地面真相标签。不幸的是，在许多领域中，这些标签很难获得，并且通常通过聚合专家意见来近似。事实上，这适用于几乎所有数据集，包括知名的数据集如CIFAR和ImageNet。使用这样的标签应用符合预测会低估不确定性。事实上，当专家意见无法解决时，标签中存在固有的模糊性。也就是说，我们没有“清晰”、明确的地面真相标签，而在校准过程中应该考虑这种不确定性。在本文中，我们针对这种模糊地面真相情景开发了一个符合预测框架，该框架依赖于对潜在模糊性的近似。

    In safety-critical classification tasks, conformal prediction allows to perform rigorous uncertainty quantification by providing confidence sets including the true class with a user-specified probability. This generally assumes the availability of a held-out calibration set with access to ground truth labels. Unfortunately, in many domains, such labels are difficult to obtain and usually approximated by aggregating expert opinions. In fact, this holds true for almost all datasets, including well-known ones such as CIFAR and ImageNet. Applying conformal prediction using such labels underestimates uncertainty. Indeed, when expert opinions are not resolvable, there is inherent ambiguity present in the labels. That is, we do not have ``crisp'', definitive ground truth labels and this uncertainty should be taken into account during calibration. In this paper, we develop a conformal prediction framework for such ambiguous ground truth settings which relies on an approximation of the underlyi
    
[^29]: 嵌套消除：一种从基于选择的反馈中识别最佳项目的简单算法

    Nested Elimination: A Simple Algorithm for Best-Item Identification from Choice-Based Feedback. (arXiv:2307.09295v1 [cs.LG])

    [http://arxiv.org/abs/2307.09295](http://arxiv.org/abs/2307.09295)

    嵌套消除是一种简单易实现的算法，通过利用创新的消除准则和嵌套结构，能够以最少的样本数量和高置信水平识别出最受欢迎的项目。

    

    我们研究了基于选择的反馈中识别最佳项目的问题。在这个问题中，公司依次向一群顾客展示显示集，并收集他们的选择。目标是以最少的样本数量和高置信水平识别出最受欢迎的项目。我们提出了一种基于消除的算法，即嵌套消除(Nested Elimination，NE)，它受到信息理论下界所暗示的嵌套结构的启发。NE的结构简单，易于实施，具有对样本复杂度的强大理论保证。具体而言，NE利用了一种创新的消除准则，并避免了解决任何复杂的组合优化问题的需要。我们提供了NE的特定实例和非渐近性的样本复杂度的上界。我们还展示了NE实现了高阶最坏情况渐近最优性。最后，来自合成和真实数据的数值实验验证了我们的理论。

    We study the problem of best-item identification from choice-based feedback. In this problem, a company sequentially and adaptively shows display sets to a population of customers and collects their choices. The objective is to identify the most preferred item with the least number of samples and at a high confidence level. We propose an elimination-based algorithm, namely Nested Elimination (NE), which is inspired by the nested structure implied by the information-theoretic lower bound. NE is simple in structure, easy to implement, and has a strong theoretical guarantee for sample complexity. Specifically, NE utilizes an innovative elimination criterion and circumvents the need to solve any complex combinatorial optimization problem. We provide an instance-specific and non-asymptotic bound on the expected sample complexity of NE. We also show NE achieves high-order worst-case asymptotic optimality. Finally, numerical experiments from both synthetic and real data corroborate our theore
    
[^30]: FlexiAST: AST所需的灵活性

    FlexiAST: Flexibility is What AST Needs. (arXiv:2307.09286v1 [cs.SD])

    [http://arxiv.org/abs/2307.09286](http://arxiv.org/abs/2307.09286)

    这篇论文提出了一种名为FlexiAST的训练方法，可以为标准AST模型提供补丁大小的灵活性，使其在推理阶段可以与各种不同的补丁大小一起工作，而无需改变架构。实验证明，FlexiAST在保持评估能力的同时，提供了与标准AST模型相似的性能。

    

    本研究的目标是为音频光谱图变换器（AST）提供补丁大小的灵活性。最近的AST技术的进展在各种基于音频的任务中显示出优越的性能。然而，标准AST的性能在使用不同的补丁大小进行评估时会急剧下降，而非训练时使用的补丁大小。因此，AST模型通常需要重新训练以适应补丁大小的变化。为了克服这个限制，本文提出了一种训练方法，以在不改变架构的情况下为标准AST模型提供灵活性，使其在推理阶段可以与各种不同的补丁大小一起工作- FlexiAST。这种提出的训练方法只是利用随机补丁大小选择和补丁大小调整和位置嵌入权重调整。我们的实验证明，FlexiAST在保持其在不同数据集上的各种补丁大小的评估能力的同时，提供了与标准AST模型相似的性能，用于音频分类任务。

    The objective of this work is to give patch-size flexibility to Audio Spectrogram Transformers (AST). Recent advancements in ASTs have shown superior performance in various audio-based tasks. However, the performance of standard ASTs degrades drastically when evaluated using different patch sizes from that used during training. As a result, AST models are typically re-trained to accommodate changes in patch sizes. To overcome this limitation, this paper proposes a training procedure to provide flexibility to standard AST models without architectural changes, allowing them to work with various patch sizes at the inference stage - FlexiAST. This proposed training approach simply utilizes random patch size selection and resizing of patch and positional embedding weights. Our experiments show that FlexiAST gives similar performance to standard AST models while maintaining its evaluation ability at various patch sizes on different datasets for audio classification tasks.
    
[^31]: 基于超立方体的分类的端到端神经网络训练

    End-to-End Neural Network Training for Hyperbox-Based Classification. (arXiv:2307.09269v1 [cs.LG])

    [http://arxiv.org/abs/2307.09269](http://arxiv.org/abs/2307.09269)

    本论文提出了一种基于神经网络的端到端训练方法来处理基于超立方体的分类问题。与现有方法相比，该方法能够高效地训练超立方体模型，并大大减少训练时间，同时获得更优的分类结果。

    

    基于超立方体的分类被认为是一种有希望的技术，其中对数据的决策被表示为一系列正交的多维立方体（即超立方体），这些立方体通常具有可解释性和易读性。然而，现有方法已经不再能够有效地处理现今许多应用领域面临的不断增加的数据量。我们通过提出一种新颖的通过神经网络进行基于超立方体分类的完全可微分框架来填补这一差距。与以往的工作不同，我们的超立方体模型可以以端到端的方式高效训练，从而大大减少训练时间并获得更优的分类结果。

    Hyperbox-based classification has been seen as a promising technique in which decisions on the data are represented as a series of orthogonal, multidimensional boxes (i.e., hyperboxes) that are often interpretable and human-readable. However, existing methods are no longer capable of efficiently handling the increasing volume of data many application domains face nowadays. We address this gap by proposing a novel, fully differentiable framework for hyperbox-based classification via neural networks. In contrast to previous work, our hyperbox models can be efficiently trained in an end-to-end fashion, which leads to significantly reduced training times and superior classification results.
    
[^32]: 低延迟联合用户调度和资源分配的移动感知联邦学习

    Mobility-Aware Joint User Scheduling and Resource Allocation for Low Latency Federated Learning. (arXiv:2307.09263v1 [cs.DC])

    [http://arxiv.org/abs/2307.09263](http://arxiv.org/abs/2307.09263)

    本论文研究了移动用户在联邦学习系统中的通信可靠性问题，并提出了一种用户调度和资源分配方法来最小化训练延迟。该方法通过建立实际的用户移动模型和优化问题，综合考虑用户选择、基站分配和带宽分配等因素。

    

    作为一种高效的分布式机器学习方法，联邦学习（FL）可以通过在用户端进行迭代的本地模型训练和在中央服务器端进行全局模型聚合来获得共享模型，从而保护用户的隐私。在FL系统中，移动用户通过无线信道与基站（BSs）通信，用户的移动性可能导致可靠性下降，从而影响训练性能。然而，现有的工作只研究了静态场景或随机初始化的用户位置，未能捕捉到现实网络中的移动性。为了解决这个问题，我们提出了一个实际的用户移动模型，跨多个基站进行FL，并开发了一种用户调度和资源分配方法，以最小化在通信资源受限的情况下的训练延迟。具体而言，我们首先对具有用户移动性的优化问题进行了建模，同时考虑了用户选择、给用户分配的BS和带宽分配，以最小化训练延迟。

    As an efficient distributed machine learning approach, Federated learning (FL) can obtain a shared model by iterative local model training at the user side and global model aggregating at the central server side, thereby protecting privacy of users. Mobile users in FL systems typically communicate with base stations (BSs) via wireless channels, where training performance could be degraded due to unreliable access caused by user mobility. However, existing work only investigates a static scenario or random initialization of user locations, which fail to capture mobility in real-world networks. To tackle this issue, we propose a practical model for user mobility in FL across multiple BSs, and develop a user scheduling and resource allocation method to minimize the training delay with constrained communication resources. Specifically, we first formulate an optimization problem with user mobility that jointly considers user selection, BS assignment to users, and bandwidth allocation to min
    
[^33]: 基于持久同调的自适应拓扑特征：点云的滤波学习

    Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds. (arXiv:2307.09259v1 [cs.LG])

    [http://arxiv.org/abs/2307.09259](http://arxiv.org/abs/2307.09259)

    本文提出了一种通过使用神经网络自适应学习滤波器的方法，用于增强点云机器学习方法的准确性。同时，还提出了一种具有等变性的神经网络架构，以使得持久同调具有等变性。

    

    点云的机器学习在形状识别和材料科学等领域得到了广泛关注。为了提高这些机器学习方法的准确性，通常会引入全局拓扑特征，这些特征通常通过持久同调提取。在对点云进行持久同调计算时，我们需要选择一个点云的滤波器，即一个逐渐增加的空间序列。由于机器学习方法与持久同调的结合受到滤波器选择的影响很大，因此我们需要根据数据和任务进行调整。本文提出了一种使用神经网络自适应学习滤波器的框架。为了使得得到的持久同调具有等变性，在此基础上提出了一种具有等变性的神经网络架构。此外，我们还在理论上展示了有限维近似的结果。

    Machine learning for point clouds has been attracting much attention, with many applications in various fields, such as shape recognition and material science. To enhance the accuracy of such machine learning methods, it is known to be effective to incorporate global topological features, which are typically extracted by persistent homology. In the calculation of persistent homology for a point cloud, we need to choose a filtration for the point clouds, an increasing sequence of spaces. Because the performance of machine learning methods combined with persistent homology is highly affected by the choice of a filtration, we need to tune it depending on data and tasks. In this paper, we propose a framework that learns a filtration adaptively with the use of neural networks. In order to make the resulting persistent homology isometry-invariant, we develop a neural network architecture with such invariance. Additionally, we theoretically show a finite-dimensional approximation result that 
    
[^34]: 用于量化生成式语言模型不确定性的PAC神经预测集学习

    PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])

    [http://arxiv.org/abs/2307.09254](http://arxiv.org/abs/2307.09254)

    本文提出了一种使用神经网络来量化生成式语言模型不确定性的PAC神经预测集学习方法，通过在多种语言数据集和模型上的实验证明，相比于标准基准方法，我们的方法平均提高了63％的量化不确定性。

    

    学习和量化模型的不确定性是增强模型可信度的关键任务。由于对生成虚构事实的担忧，最近兴起的生成式语言模型（GLM）特别强调可靠的不确定性量化的需求。本文提出了一种学习神经预测集模型的方法，该方法能够以可能近似正确（PAC）的方式量化GLM的不确定性。与现有的预测集模型通过标量值参数化不同，我们提出通过神经网络参数化预测集，实现更精确的不确定性量化，但仍满足PAC保证。通过在四种类型的语言数据集和六种类型的模型上展示，我们的方法相比标准基准方法平均提高了63％的量化不确定性。

    Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
    
[^35]: UniTabE: 面向异构表格数据的统一预训练表格编码器

    UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])

    [http://arxiv.org/abs/2307.09249](http://arxiv.org/abs/2307.09249)

    UniTabE是一种面向异构表格数据的统一预训练表格编码器，能够处理不同表格结构的挑战，并具有对多样化下游应用的适应性。

    

    自然语言处理（NLP）的最新进展明证了预训练模型的突破性影响，在各种任务上取得了令人印象深刻的结果。本研究旨在将预训练方法的威力扩展到传统被忽视的表格数据领域，该领域由于不同任务固有的众多表格模式而具有挑战性。本工作的主要研究问题围绕异构表格结构的适应性、表格数据的统一预训练协议的建立、学到的知识在任务之间的泛化和可传递性、对多样化下游应用的适应性以及随时间的增量列的纳入进行了探讨。针对这些挑战，我们引入了UniTabE，这是一种创新的方法，旨在以一致的方式处理表格，摆脱了特定表格结构强加的约束。UniTabE的核心概念是对每个基本表格进行表示

    Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic tab
    
[^36]: BERT在风力发电预测中的应用-百度KDD Cup 2022中Teletraan的解决方案

    Application of BERT in Wind Power Forecasting-Teletraan's Solution in Baidu KDD Cup 2022. (arXiv:2307.09248v1 [cs.LG])

    [http://arxiv.org/abs/2307.09248](http://arxiv.org/abs/2307.09248)

    该论文提出了一种在风力发电预测中应用BERT模型的解决方案，并通过后处理添加日波动性使预测结果更符合每日周期性。该解决方案在百度KDD Cup 2022中取得了第三名。

    

    如今，风能因其在碳中和和可持续发展中的重要作用而受到越来越多的关注。当风能集成到电力网络中时，准确的预测对系统的可持续性和安全性至关重要。然而，不可预测的性质和长序列预测使其特别具有挑战性。在本技术报告中，我们介绍了BERT模型在百度KDD Cup 2022中的应用，通过后处理添加了日波动性，使预测结果符合每日周期性。我们的解决方案在2490支队伍中获得了第三名。代码已经在https://github.com/LongxingTan/KDD2022-Baidu发布。

    Nowadays, wind energy has drawn increasing attention as its important role in carbon neutrality and sustainable development. When wind power is integrated into the power grid, precise forecasting is necessary for the sustainability and security of the system. However, the unpredictable nature and long sequence prediction make it especially challenging. In this technical report, we introduce the BERT model applied for Baidu KDD Cup 2022, and the daily fluctuation is added by post-processing to make the predicted results in line with daily periodicity. Our solution achieves 3rd place of 2490 teams. The code is released athttps://github.com/LongxingTan/KDD2022-Baidu
    
[^37]: 面向NILM的多标签分类的可持续深度学习

    Towards Sustainable Deep Learning for Multi-Label Classification on NILM. (arXiv:2307.09244v1 [cs.LG])

    [http://arxiv.org/abs/2307.09244](http://arxiv.org/abs/2307.09244)

    本研究提出了一种面向NILM的新型深度学习模型，通过改进计算和能源效率，实现了对NILM的多标签分类的增强。同时，还提出了一种测试方法，可以比较不同模型在虚拟数据集上的性能。

    

    非侵入式负载监测（NILM）是从单个计量点获取家庭或企业总电力消耗的电器级数据的过程。电器级数据可以直接用于需求响应应用、能源管理系统以及提高能效和减少碳足迹的意识提高和激励。最近，经典机器学习和深度学习（DL）技术在NILM分类中变得非常流行，并证明在增长的复杂性下对NILM分类非常有效，但随着复杂度的增加，这些方法在训练和操作过程中面临着显著的计算和能源需求。在本文中，我们引入了一种新的DL模型，旨在通过提高计算和能源效率来增强NILM的多标签分类。我们还提出了一种用于使用从测量数据集合成的数据比较不同模型的测试方法。

    Non-intrusive load monitoring (NILM) is the process of obtaining appliance-level data from a single metering point, measuring total electricity consumption of a household or a business. Appliance-level data can be directly used for demand response applications and energy management systems as well as for awareness raising and motivation for improvements in energy efficiency and reduction in the carbon footprint. Recently, classical machine learning and deep learning (DL) techniques became very popular and proved as highly effective for NILM classification, but with the growing complexity these methods are faced with significant computational and energy demands during both their training and operation. In this paper, we introduce a novel DL model aimed at enhanced multi-label classification of NILM with improved computation and energy efficiency. We also propose a testing methodology for comparison of different models using data synthesized from the measurement datasets so as to better 
    
[^38]: 结合手部和身体骨架进行装配中的人体动作识别

    Fusing Hand and Body Skeletons for Human Action Recognition in Assembly. (arXiv:2307.09238v1 [cs.CV])

    [http://arxiv.org/abs/2307.09238](http://arxiv.org/abs/2307.09238)

    本文提出了一种将较少详细的身体骨架与高度详细的手部骨架结合的方法，通过使用CNN和transformers，有效地提高了组装场景中的动作识别效果。

    

    随着协作机器人在工业制造领域日益受欢迎，有效的人机协作变得至关重要。协作机器人应该能够识别人体动作，以在组装任务中提供帮助并做出自主决策。为了实现这一目标，通常使用基于骨架的方法，因为它们能够适用于不同人员和环境。尽管身体骨架方法广泛用于动作识别，但在装配动作中，工人的手指和手部起到了重要作用，因此可能不够准确。为了解决这个问题，我们提出了一种方法，将较少详细的身体骨架与高度详细的手部骨架结合起来。我们研究了CNN和transformers，后者特别擅长使用注意力从两种骨架类型中提取和组合重要信息。本文展示了我们提出的方法在提高组装场景中的动作识别效果上的有效性。

    As collaborative robots (cobots) continue to gain popularity in industrial manufacturing, effective human-robot collaboration becomes crucial. Cobots should be able to recognize human actions to assist with assembly tasks and act autonomously. To achieve this, skeleton-based approaches are often used due to their ability to generalize across various people and environments. Although body skeleton approaches are widely used for action recognition, they may not be accurate enough for assembly actions where the worker's fingers and hands play a significant role. To address this limitation, we propose a method in which less detailed body skeletons are combined with highly detailed hand skeletons. We investigate CNNs and transformers, the latter of which are particularly adept at extracting and combining important information from both skeleton types using attention. This paper demonstrates the effectiveness of our proposed approach in enhancing action recognition in assembly scenarios.
    
[^39]: 使用机器学习从语音信号中检测喉癌：可复现的文献综述

    Detecting Throat Cancer from Speech Signals Using Machine Learning: A Reproducible Literature Review. (arXiv:2307.09230v1 [cs.LG])

    [http://arxiv.org/abs/2307.09230](http://arxiv.org/abs/2307.09230)

    本研究对使用机器学习和人工智能从语音记录中检测喉癌的文献进行了综述，发现了22篇相关论文，讨论了它们的方法和结果。研究使用了神经网络和梅尔频率倒谱系数提取音频特征，并通过迁移学习实现了分类，取得了一定的准确率。

    

    本文对使用机器学习和人工智能从语音记录中检测喉癌的当前文献进行了范围评估。我们找到了22篇相关论文，并讨论了它们的方法和结果。我们将这些论文分为两组 - 九篇进行二分类，13篇进行多类别分类。这些论文提出了一系列方法，其中最常见的是使用神经网络。在分类之前还从音频中提取了许多特征，其中最常见的是梅尔频率倒谱系数。在这次搜索中未找到任何带有代码库的论文，因此无法复现。因此，我们创建了一个公开可用的代码库来训练自己的分类器。我们在一个多类别问题上使用迁移学习，将三种病理和健康对照进行分类。使用这种技术，我们取得了53.54%的加权平均召回率、83.14%的敏感性和特异性。

    In this work we perform a scoping review of the current literature on the detection of throat cancer from speech recordings using machine learning and artificial intelligence. We find 22 papers within this area and discuss their methods and results. We split these papers into two groups - nine performing binary classification, and 13 performing multi-class classification. The papers present a range of methods with neural networks being most commonly implemented. Many features are also extracted from the audio before classification, with the most common bring mel-frequency cepstral coefficients. None of the papers found in this search have associated code repositories and as such are not reproducible. Therefore, we create a publicly available code repository of our own classifiers. We use transfer learning on a multi-class problem, classifying three pathologies and healthy controls. Using this technique we achieve an unweighted average recall of 53.54%, sensitivity of 83.14%, and specif
    
[^40]: 深度学习中遗忘现象的全面调查：超越连续学习

    A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])

    [http://arxiv.org/abs/2307.09218](http://arxiv.org/abs/2307.09218)

    遗忘是深度学习中普遍存在的现象，不仅限于连续学习领域。解决遗忘问题面临多个挑战，包括平衡保留旧任务知识与快速学习新任务的挑战，管理任务干扰与冲突目标的挑战，以及防止隐私泄露等。遗忘不总是有害的，可以在某些情况下是有益且可取的，特别是在隐私保护场景中。

    

    遗忘指的是先前获取的信息或知识的丧失或恶化。尽管现有的关于遗忘的调查主要集中在连续学习方面，但在深度学习中，遗忘是一种普遍现象，可以在各种其他研究领域中观察到。遗忘在研究领域中表现出来，例如由于生成器漂移而在生成模型领域中表现出来，以及由于客户端之间存在异构数据分布而在联邦学习中表现出来。解决遗忘问题涉及到几个挑战，包括在快速学习新任务的同时平衡保留旧任务知识，管理任务干扰与冲突目标，以及防止隐私泄露等。此外，大多数现有的连续学习调查都默认认为遗忘总是有害的。相反，我们的调查认为遗忘是一把双刃剑，在某些情况下可以是有益且可取的，例如隐私保护场景。通过在更广泛的背景下探讨遗忘现象，

    Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context
    
[^41]: 需要多少个神经元才能近似计算最大值？

    How Many Neurons Does it Take to Approximate the Maximum?. (arXiv:2307.09212v1 [cs.LG])

    [http://arxiv.org/abs/2307.09212](http://arxiv.org/abs/2307.09212)

    本研究研究了使用ReLU激活函数的神经网络近似计算连续分布下$d$个输入的最大值所需的网络大小，并提供了新的界限和分离结果。通过使用深度为2的网络近似最大值函数的新下界，我们提供了一种深度为$\mathcal{O}(\log(\log(d)))$，宽度为$\mathcal{O}(d)$的构造来改善深度需求。

    

    我们研究了在使用ReLU激活函数的网络中，近似计算$L_2$范数下连续分布$d$个输入的最大值所需的神经网络的大小。我们提供了关于近似所需宽度的新的下界和上界，以及不同深度之间的深度分离，包括深度2和3以及深度3和5网络之间的分离。此外，我们提供了一个深度为$\mathcal{O}(\log(\log(d)))$，宽度为$\mathcal{O}(d)$的构造，可以近似计算最大值函数，显著改进了线性宽度限制条件下已知最优的深度需求。我们的深度分离结果通过对均匀分布下深度为2的网络近似最大值函数的新的下界得到，并假设权重大小具有指数上界。此外，我们能够利用这个深度为2的下界提供紧致的上界。

    We study the size of a neural network needed to approximate the maximum function over $d$ inputs, in the most basic setting of approximating with respect to the $L_2$ norm, for continuous distributions, for a network that uses ReLU activations. We provide new lower and upper bounds on the width required for approximation across various depths. Our results establish new depth separations between depth 2 and 3, and depth 3 and 5 networks, as well as providing a depth $\mathcal{O}(\log(\log(d)))$ and width $\mathcal{O}(d)$ construction which approximates the maximum function, significantly improving upon the depth requirements of the best previously known bounds for networks with linearly-bounded width. Our depth separation results are facilitated by a new lower bound for depth 2 networks approximating the maximum function over the uniform distribution, assuming an exponential upper bound on the size of the weights. Furthermore, we are able to use this depth 2 lower bound to provide tight
    
[^42]: 自动化的残障主义：探索情感分析和毒性检测模型中的明显残障偏见

    Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models. (arXiv:2307.09209v1 [cs.CL])

    [http://arxiv.org/abs/2307.09209](http://arxiv.org/abs/2307.09209)

    该论文分析了情感分析和毒性检测模型，以探测对残障人士的明显偏见。研究使用偏见识别框架对社交媒体平台的对话进行了分析，并创建了一个测试语料库来量化明显的残障偏见。研究发现，所研究的模型均存在显著的偏见。

    

    我们分析情感分析和毒性检测模型，以检测对残障人士的明显偏见。我们采用扰动敏感性分析的偏见识别框架，研究社交媒体平台上与残障人士相关的对话，特别是Twitter和Reddit，在真实社交环境中了解残障偏见是如何传播的。然后，我们创建了“情感中的偏见识别测试”（BITS）语料库，以量化任何情感分析和毒性检测模型中的明显残障偏见。我们的研究使用BITS揭示了四种开放的AIaaS（AI即服务）情感分析工具（TextBlob，VADER，Google Cloud Natural Language API，DistilBERT）和两种毒性检测模型（两个版本的Toxic-BERT）中存在显着的偏见。

    We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the \textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.
    
[^43]: 使用检测变压器进行联合微震事件检测和定位

    Joint Microseismic Event Detection and Location with a Detection Transformer. (arXiv:2307.09207v1 [physics.geo-ph])

    [http://arxiv.org/abs/2307.09207](http://arxiv.org/abs/2307.09207)

    本文提出了一种使用检测变压器将微震事件检测和源定位统一为一个框架的方法，该方法可以实现实时微震监测，并在合成数据上进行了测试。

    

    微震事件的检测和定位是微震监测的两个主要组成部分，在油藏刺激和演化过程中，为我们提供了宝贵的地下信息。传统的事件检测和定位方法往往需要手动干预和/或大量计算，而当前的机器学习辅助方法通常分别解决检测和定位问题；这些限制阻碍了实时微震监测的潜力。我们提出了一种方法，通过调整卷积神经网络主干和编码器-解码器变压器，将事件检测和源定位统一到一个单一框架中，并直接应用于记录的波形上。该网络以模拟多个微震事件的合成数据进行训练，这些事件对应于疑似微震活动区域中的随机源位置。在SEAM Time Lapse模型的二维剖面上进行了合成测试。

    Microseismic event detection and location are two primary components in microseismic monitoring, which offers us invaluable insights into the subsurface during reservoir stimulation and evolution. Conventional approaches for event detection and location often suffer from manual intervention and/or heavy computation, while current machine learning-assisted approaches typically address detection and location separately; such limitations hinder the potential for real-time microseismic monitoring. We propose an approach to unify event detection and source location into a single framework by adapting a Convolutional Neural Network backbone and an encoder-decoder Transformer with a set-based Hungarian loss, which is applied directly to recorded waveforms. The proposed network is trained on synthetic data simulating multiple microseismic events corresponding to random source locations in the area of suspected microseismic activities. A synthetic test on a 2D profile of the SEAM Time Lapse mod
    
[^44]: 带有基于学习的地形和机器人感知动力模型的上下文条件导航

    Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model. (arXiv:2307.09206v1 [cs.RO])

    [http://arxiv.org/abs/2307.09206](http://arxiv.org/abs/2307.09206)

    本文提出了一种名为TRADYN的概率地形和机器人感知前向动力学模型，能够适应在自主导航环境中的地形和机器人的变化，通过在模拟的二维导航环境中的实验证明，该模型在长视程轨迹预测任务中表现出较低的预测误差。

    

    在自主导航环境中，多个参数可能会发生变化。地形特性如摩擦系数可能会根据机器人的位置而随时间变化。此外，机器人的动力学可能会因不同负载、系统质量变化、磨损等原因而发生变化，从而改变执行器增益或关节摩擦力。自主代理应该能够适应这些变化。在本文中，我们开发了一种新颖的概率地形和机器人感知前向动力学模型，称为TRADYN，它能够适应上述变化。它基于基于神经过程的元学习前向动力学模型的最新进展。我们在模拟的二维导航环境中评估了我们的方法，使用了一个类似自行车的机器人和具有空间变化摩擦系数的不同地形布局。在我们的实验中，与非自适应方法相比，所提出的模型在长视程轨迹预测任务中表现出较低的预测误差。

    In autonomous navigation settings, several quantities can be subject to variations. Terrain properties such as friction coefficients may vary over time depending on the location of the robot. Also, the dynamics of the robot may change due to, e.g., different payloads, changing the system's mass, or wear and tear, changing actuator gains or joint friction. An autonomous agent should thus be able to adapt to such variations. In this paper, we develop a novel probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN, which is able to adapt to the above-mentioned variations. It builds on recent advances in meta-learning forward dynamics models based on Neural Processes. We evaluate our method in a simulated 2D navigation setting with a unicycle-like robot and different terrain layouts with spatially varying friction coefficients. In our experiments, the proposed model exhibits lower prediction error for the task of long-horizon trajectory prediction, compared to non-ada
    
[^45]: 学习动态属性因子世界模型以实现高效的多对象强化学习。

    Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning. (arXiv:2307.09205v1 [cs.LG])

    [http://arxiv.org/abs/2307.09205](http://arxiv.org/abs/2307.09205)

    本文提出了一个名为DAFT-RL的框架，利用对象为中心的表示学习从视觉输入中提取对象，并学习对它们进行分类和推断其隐含参数，以实现高效的多对象强化学习。

    

    在许多强化学习任务中，智能体需要学习与许多不同类型的对象进行交互，并推广到未见过的对象组合和数量。通常，一个任务是以前学习的任务的组合（例如堆叠方块）。这些都是组成泛化的例子，通过组合以对象为中心的表示来解决复杂任务。最近的研究表明，在这些设置中，利用对象分解表示和层次抽象可以改善样本效率。另一方面，这些方法在对象属性方面没有充分利用分解的好处。在本文中，我们利用Dynamic Attribute FacTored RL (DAFT-RL)框架来解决这个问题。在DAFT-RL中，我们利用对象为中心的表示学习从视觉输入中提取对象。我们学习对它们进行分类，并推断它们的隐含参数。对于每个对象类，我们学习一个类模板图。

    In many reinforcement learning tasks, the agent has to learn to interact with many objects of different types and generalize to unseen combinations and numbers of objects. Often a task is a composition of previously learned tasks (e.g. block stacking). These are examples of compositional generalization, in which we compose object-centric representations to solve complex tasks. Recent works have shown the benefits of object-factored representations and hierarchical abstractions for improving sample efficiency in these settings. On the other hand, these methods do not fully exploit the benefits of factorization in terms of object attributes. In this paper, we address this opportunity and introduce the Dynamic Attribute FacTored RL (DAFT-RL) framework. In DAFT-RL, we leverage object-centric representation learning to extract objects from visual inputs. We learn to classify them in classes and infer their latent parameters. For each class of object, we learn a class template graph that des
    
[^46]: 用于二分类的分类编码器的基准研究

    A benchmark of categorical encoders for binary classification. (arXiv:2307.09191v1 [cs.LG])

    [http://arxiv.org/abs/2307.09191](http://arxiv.org/abs/2307.09191)

    本研究是迄今为止最全面的分类编码器基准研究，通过对来自不同家族的32种编码器配置进行广泛评估，以及36种实验因素和50个数据集的组合，展示了数据集选择、实验因素和聚合策略对基准研究结论的深远影响。

    

    分类编码器将分类特征转化为数字表示，对于广泛的机器学习模型来说是不可或缺的。现有的编码器基准研究由于选择有限的编码器、实验因素和数据集，缺乏普适性。此外，由于采用了不同的聚合策略，结果存在不一致性。本文是迄今为止最全面的分类编码器基准研究，包括对来自不同家族的32种编码器配置进行了广泛评估，以及36种实验因素和50个数据集的组合。该研究展示了数据集选择、实验因素和聚合策略对基准研究结论的深远影响，这是以前的编码器基准研究忽视的方面。

    Categorical encoders transform categorical features into numerical representations that are indispensable for a wide range of machine learning models. Existing encoder benchmark studies lack generalizability because of their limited choice of (1) encoders, (2) experimental factors, and (3) datasets. Additionally, inconsistencies arise from the adoption of varying aggregation strategies. This paper is the most comprehensive benchmark of categorical encoders to date, including an extensive evaluation of 32 configurations of encoders from diverse families, with 36 combinations of experimental factors, and on 50 datasets. The study shows the profound influence of dataset selection, experimental factors, and aggregation strategies on the benchmark's conclusions -- aspects disregarded in previous encoder benchmarks.
    
[^47]: 面向计算受限异构设备的联邦学习：一项调研

    Federated Learning for Computationally-Constrained Heterogeneous Devices: A Survey. (arXiv:2307.09182v1 [cs.LG])

    [http://arxiv.org/abs/2307.09182](http://arxiv.org/abs/2307.09182)

    本调研对于在计算能力有限的异构设备上应用联邦学习提出了挑战，重点关注计算异构性。联邦学习是一种在设备之间共享知识但不公开私有数据的隐私保护方案。

    

    随着越来越多的智能设备，如物联网设备，部署在实际场景中，将神经网络的训练任务转移到中央服务器变得越来越不可行。为了提高用户隐私，最近出现了在设备上进行学习的替代方案。然而，仅使用单个设备上的本地数据进行训练的模型很难达到高准确性。联邦学习被引入作为一种解决方案，通过在设备之间共享知识但不公开设备的私有数据，提供了在通信开销和模型准确性之间的隐私权衡。然而，在许多相关的使用案例中，应用基线的联邦学习的适用性和好处受到限制，原因是环境中存在异构性。本调研概述了联邦学习在真实应用中需要克服的异构性挑战。我们特别关注参与方中的计算异构性方面。

    With an increasing number of smart devices like internet of things (IoT) devices deployed in the field, offloadingtraining of neural networks (NNs) to a central server becomes more and more infeasible. Recent efforts toimprove users' privacy have led to on-device learning emerging as an alternative. However, a model trainedonly on a single device, using only local data, is unlikely to reach a high accuracy. Federated learning (FL)has been introduced as a solution, offering a privacy-preserving trade-off between communication overheadand model accuracy by sharing knowledge between devices but disclosing the devices' private data. Theapplicability and the benefit of applying baseline FL are, however, limited in many relevant use cases dueto the heterogeneity present in such environments. In this survey, we outline the heterogeneity challengesFL has to overcome to be widely applicable in real-world applications. We especially focus on the aspect ofcomputation heterogeneity among the parti
    
[^48]: 高效预测肽自组装的序列和图形编码

    Efficient Prediction of Peptide Self-assembly through Sequential and Graphical Encoding. (arXiv:2307.09169v1 [q-bio.BM])

    [http://arxiv.org/abs/2307.09169](http://arxiv.org/abs/2307.09169)

    本论文研究了肽自组装的预测问题，通过收集大量肽数据集，并使用序列和图形的编码方式进行深度学习模型的训练，对提高预测准确性进行了系统分析。

    

    最近几年，由于肽的重要发展和市场潜力，深度学习在预测各种肽属性方面的应用研究爆炸性增长。分子动力学使得收集大量肽数据集成为可能，为深度学习提供可靠的训练数据。然而，对于AI辅助肽相关任务至关重要的肽编码缺乏系统分析，这成为提高预测准确性的紧迫问题。为了解决这个问题，我们首先收集了一个高质量的巨大模拟肽自组装数据集，其中包含超过62,000个由粗粒度分子动力学（CGMD）生成的样本。然后，我们使用最先进的序列（RNN，LSTM和Transformer）和结构深度学习模型（GCN，GAT和GraphSA）系统地研究了将氨基酸编码为序列和分子图的效果。

    In recent years, there has been an explosion of research on the application of deep learning to the prediction of various peptide properties, due to the significant development and market potential of peptides. Molecular dynamics has enabled the efficient collection of large peptide datasets, providing reliable training data for deep learning. However, the lack of systematic analysis of the peptide encoding, which is essential for AI-assisted peptide-related tasks, makes it an urgent problem to be solved for the improvement of prediction accuracy. To address this issue, we first collect a high-quality, colossal simulation dataset of peptide self-assembly containing over 62,000 samples generated by coarse-grained molecular dynamics (CGMD). Then, we systematically investigate the effect of peptide encoding of amino acids into sequences and molecular graphs using state-of-the-art sequential (i.e., RNN, LSTM, and Transformer) and structural deep learning models (i.e., GCN, GAT, and GraphSA
    
[^49]: 迈向可信赖的数据集精炼

    Towards Trustworthy Dataset Distillation. (arXiv:2307.09165v1 [cs.LG])

    [http://arxiv.org/abs/2307.09165](http://arxiv.org/abs/2307.09165)

    本论文提出了一种名为可信赖的数据集精炼（TrustDD）的新范式，通过同时考虑内部分布（InD）分类和外部分布（OOD）检测的问题，将大型数据集精炼为小型合成数据集，从而提高模型的效率和可信赖性。

    

    在将深度学习应用于实际应用时，效率和可信赖性是两个永恒的追求。就效率而言，数据集精炼（DD）致力于通过将大型数据集精炼为小型合成数据集来降低训练成本。然而，现有方法仅集中于在封闭世界环境下的内部分布（InD）分类，忽略了外部分布（OOD）样本。另一方面，OOD检测旨在提高模型的可信赖性，在完整数据设置下通常效率低下。我们首次同时考虑了这两个问题，并提出了一种新的范式，称为可信赖的数据集精炼（TrustDD）。通过精炼InD样本和异常值，这些被筛选的数据集能够训练出既擅长InD分类又能进行OOD检测的模型。为了缓解对真实异常值数据的需求，并使OOD检测更加实用，我们进一步提出了对InD样本损坏以生成伪样本的方法。

    Efficiency and trustworthiness are two eternal pursuits when applying deep learning in real-world applications. With regard to efficiency, dataset distillation (DD) endeavors to reduce training costs by distilling the large dataset into a tiny synthetic dataset. However, existing methods merely concentrate on in-distribution (InD) classification in a closed-world setting, disregarding out-of-distribution (OOD) samples. On the other hand, OOD detection aims to enhance models' trustworthiness, which is always inefficiently achieved in full-data settings. For the first time, we simultaneously consider both issues and propose a novel paradigm called Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and outliers, the condensed datasets are capable to train models competent in both InD classification and OOD detection. To alleviate the requirement of real outlier data and make OOD detection more practical, we further propose to corrupt InD samples to generate pseudo-
    
[^50]: MVA2023小物体检测挑战：用于鸟类识别的数据集、方法和结果

    MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results. (arXiv:2307.09143v1 [cs.CV])

    [http://arxiv.org/abs/2307.09143](http://arxiv.org/abs/2307.09143)

    这篇论文提出了一个新的小物体检测数据集SOD4SB，其中包含了大量的鸟类实例。论文介绍了该数据集的挑战细节，并概述了获奖的方法。这个数据集以及评估网站是公开可用的。

    

    小物体检测(SOD)是重要的机器视觉主题，因为(i)各种现实应用需要对远处物体进行检测，(ii)由于小物体的噪声、模糊和信息较少的图像表现形式，SOD是一个具有挑战性的任务。本文提出了一个新的SOD数据集，包括39,070张图像，共包含137,121个鸟类实例，称为用于鸟类识别的小物体检测(SOD4SB)数据集。本文介绍了SOD4SB数据集挑战的细节，并简要介绍了获奖方法。该数据集、基准代码以及用于公共测试集评估的网站均可公开获取。

    Small Object Detection (SOD) is an important machine vision topic because (i) a variety of real-world applications require object detection for distant objects and (ii) SOD is a challenging task due to the noisy, blurred, and less-informative image appearances of small objects. This paper proposes a new SOD dataset consisting of 39,070 images including 137,121 bird instances, which is called the Small Object Detection for Spotting Birds (SOD4SB) dataset. The detail of the challenge with the SOD4SB dataset is introduced in this paper. In total, 223 participants joined this challenge. This paper briefly introduces the award-winning methods. The dataset, the baseline code, and the website for evaluation on the public testset are publicly available.
    
[^51]: 使用多相多体耗散粒子动力学和基于PINNs的数据驱动发现来表征CMAS液滴的部分润湿

    Characterization of partial wetting by CMAS droplets using multiphase many-body dissipative particle dynamics and data-driven discovery based on PINNs. (arXiv:2307.09142v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.09142](http://arxiv.org/abs/2307.09142)

    本研究使用多相多体耗散粒子动力学和基于PINNs的数据驱动发现方法，表征了高黏度熔融CMAS液滴的部分润湿动力学，为解决高温应用中CMAS积聚和对设备的损害问题提供了创新解决方案。

    

    熔融砂是石灰、镁、铝和硅酸盐的混合物，被称为CMAS，其具有高黏度、密度和表面张力。CMAS的独特性使得在高温应用中处理它成为一项具有挑战性的任务，需要创新的解决方案和材料来防止其积聚并对关键设备造成损害。本文使用多相多体耗散粒子动力学（mDPD）模拟研究高黏度熔融CMAS液滴的润湿动力学。模拟在三维中进行，初始液滴尺寸和平衡接触角变化。我们提出了一个捕捉CMAS液滴扩展半径行为的粗粒参数常微分方程（ODE）。然后，利用基于物理信息的神经网络（PINN）框架确定ODE参数。随后，给出了参数值在初始半径和接触角上的闭式依赖关系。

    The molten sand, a mixture of calcia, magnesia, alumina, and silicate, known as CMAS, is characterized by its high viscosity, density, and surface tension. The unique properties of CMAS make it a challenging material to deal with in high-temperature applications, requiring innovative solutions and materials to prevent its buildup and damage to critical equipment. Here, we use multiphase many-body dissipative particle dynamics (mDPD) simulations to study the wetting dynamics of highly viscous molten CMAS droplets. The simulations are performed in three dimensions, with varying initial droplet sizes and equilibrium contact angles. We propose a coarse parametric ordinary differential equation (ODE) that captures the spreading radius behavior of the CMAS droplets. The ODE parameters are then identified based on the Physics-Informed Neural Network (PINN) framework. Subsequently, the closed form dependency of parameter values found by PINN on the initial radii and contact angles are given us
    
[^52]: 单类别主动学习用于语义分割的挖掘

    Mining of Single-Class by Active Learning for Semantic Segmentation. (arXiv:2307.09109v1 [cs.LG])

    [http://arxiv.org/abs/2307.09109](http://arxiv.org/abs/2307.09109)

    本文介绍了一种名为"MiSiCAL"的单类别主动学习范式，通过深度强化学习构建了一个策略，利用数量-准确性相关性对特定类别进行高性能模型训练，特别适用于大批量大小的情况下。MiSiCAL能在许多类别上优于随机策略。

    

    几种主动学习策略需要多次重新训练目标模型，以识别最具信息的样本，并很少提供从少数类别中获取样本的选项。本文引入了一种名为“Mining of Single-Class by Active Learning (MiSiCAL)”的范式，通过深度强化学习构建了一个主动学习策略，并利用数量-准确性相关性来建立数据集，用于针对特定类别训练高性能模型。MiSiCAL在特别大的批量大小下尤其有帮助，因为它不需要像其他主动学习方法那样进行重复的模型训练。这要归功于它能够利用候选数据点的固定表示。我们发现，在171个COCO10k类别中，MiSiCAL能够在150个类别上胜过随机策略，而最强的基准只在101个类别上胜过随机策略。

    Several Active Learning (AL) policies require retraining a target model several times in order to identify the most informative samples and rarely offer the option to focus on the acquisition of samples from underrepresented classes. Here the Mining of Single-Class by Active Learning (MiSiCAL) paradigm is introduced where an AL policy is constructed through deep reinforcement learning and exploits quantity-accuracy correlations to build datasets on which high-performance models can be trained with regards to specific classes. MiSiCAL is especially helpful in the case of very large batch sizes since it does not require repeated model training sessions as is common in other AL methods. This is thanks to its ability to exploit fixed representations of the candidate data points. We find that MiSiCAL is able to outperform a random policy on 150 out of 171 COCO10k classes, while the strongest baseline only outperforms random on 101 classes.
    
[^53]: 非平稳延迟组合半强化学习在因果相关回报中的应用

    Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.09093v1 [cs.LG])

    [http://arxiv.org/abs/2307.09093](http://arxiv.org/abs/2307.09093)

    这篇论文研究了在非平稳环境中具有结构依赖关系的组合半强化学习问题，提出了一种从延迟的反馈中学习因果关系并做出决策的策略。

    

    在不确定性下的顺序决策中，常常存在长时间的反馈延迟。这种延迟会降低学习代理在长期中识别出一组具有最优总回报的臂的性能。在具有结构依赖的非平稳环境中，这个问题变得极具挑战性。因此，除了适应延迟和环境变化外，学习因果关系可以减轻反馈延迟对决策过程的不利影响。我们将所描述的情景形式化为一个具有因果相关回报的非平稳和延迟的组合半强化学习问题。我们通过一个定向图在一个固定的结构方程模型中建模因果关系。学习代理最大化长期平均回报，该回报定义为基础臂的回报的线性函数。我们开发了一种策略，该策略从延迟的反馈中学习结构依赖关系，并利用这些信息进行决策。

    Sequential decision-making under uncertainty is often associated with long feedback delays. Such delays degrade the performance of the learning agent in identifying a subset of arms with the optimal collective reward in the long run. This problem becomes significantly challenging in a non-stationary environment with structural dependencies amongst the reward distributions associated with the arms. Therefore, besides adapting to delays and environmental changes, learning the causal relations alleviates the adverse effects of feedback delay on the decision-making process. We formalize the described setting as a non-stationary and delayed combinatorial semi-bandit problem with causally related rewards. We model the causal relations by a directed graph in a stationary structural equation model. The agent maximizes the long-term average payoff, defined as a linear function of the base arms' rewards. We develop a policy that learns the structural dependencies from delayed feedback and utiliz
    
[^54]: 使用区块链技术的联邦学习模型用于电能管理

    A Federated learning model for Electric Energy management using Blockchain Technology. (arXiv:2307.09080v1 [cs.LG])

    [http://arxiv.org/abs/2307.09080](http://arxiv.org/abs/2307.09080)

    这项研究提出了一种使用区块链技术的联邦学习模型，以解决发展中国家面临的能源短缺和电力负荷故障问题。通过利用可再生能源并采用联邦学习技术，该模型可以预测能源需求并满足消费者的需求。该研究还使用区块链确保数据交易的透明性、可追溯性和安全性。

    

    能源短缺和电力负荷故障是发展中国家的主要问题。主要原因是能源部门管理不足和非可再生能源的使用。改善能源管理和利用可再生资源对解决能源危机至关重要。为了满足由于燃料价格高导致的不断增长的能源需求，有必要增加可再生能源（RESs）的使用。联邦学习（FL）是人工智能领域中最新兴的技术。联邦学习帮助在远程边缘站点集合本地训练的模型，生成服务器端的全局模型，同时保护数据隐私。全局模型用于预测能源需求，满足消费者的需求。本文提出了基于区块链的安全分布式账本技术，用于处理生产者和消费者之间的数据交易，以确保其透明性、可追溯性和安全性。

    Energy shortfall and electricity load shedding are the main problems for developing countries. The main causes are lack of management in the energy sector and the use of non-renewable energy sources. The improved energy management and use of renewable sources can be significant to resolve energy crisis. It is necessary to increase the use of renewable energy sources (RESs) to meet the increasing energy demand due to high prices of fossil-fuel based energy. Federated learning (FL) is the most emerging technique in the field of artificial intelligence. Federated learning helps to generate global model at server side by ensemble locally trained models at remote edges sites while preserving data privacy. The global model used to predict energy demand to satisfy the needs of consumers. In this article, we have proposed Blockchain based safe distributed ledger technology for transaction of data between prosumer and consumer to ensure their transparency, traceability and security. Furthermore
    
[^55]: DiTTO：受扩散启发的时空转换算子

    DiTTO: Diffusion-inspired Temporal Transformer Operator. (arXiv:2307.09072v1 [cs.LG])

    [http://arxiv.org/abs/2307.09072](http://arxiv.org/abs/2307.09072)

    DiTTO是一种扩散启发的算子学习方法，通过结合Transformer架构的元素，无需时间离散化连续解决时间相关PDEs，并在多维度的各种PDE中取得了最先进的准确性结果。

    

    使用数据驱动方法解决偏微分方程（PDEs）已经越来越常见。最近的算子学习范式的发展使得解决更广泛PDE相关问题成为可能。我们提出了一种算子学习方法，可以连续地解决时间相关的PDEs，而不需要任何时间离散化。所提出的方法名为DiTTO，受潜在扩散模型的启发。尽管扩散模型通常用于生成人工智能任务，但其时间条件机制对PDEs非常有用。扩散启发的框架与Transformer架构的元素相结合，以提高其能力。我们展示了新方法在多维度的广泛PDE上的有效性，包括1维Burgers方程，2维Navier-Stokes方程和2维和3维声波方程。DiTTO在这些问题的准确性方面取得了最先进的结果。

    Solving partial differential equations (PDEs) using a data-driven approach has become increasingly common. The recent development of the operator learning paradigm has enabled the solution of a broader range of PDE-related problems. We propose an operator learning method to solve time-dependent PDEs continuously in time without needing any temporal discretization. The proposed approach, named DiTTO, is inspired by latent diffusion models. While diffusion models are usually used in generative artificial intelligence tasks, their time-conditioning mechanism is extremely useful for PDEs. The diffusion-inspired framework is combined with elements from the Transformer architecture to improve its capabilities.  We demonstrate the effectiveness of the new approach on a wide variety of PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO achieves state-of-the-art results in terms of accuracy for these p
    
[^56]: 评估使用U-Net进行胎儿头超声图像分割中的微调策略

    Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net. (arXiv:2307.09067v1 [eess.IV])

    [http://arxiv.org/abs/2307.09067](http://arxiv.org/abs/2307.09067)

    本论文评估了使用U-Net进行胎儿头超声图像分割的微调策略，通过使用轻量级的MobileNet作为编码器，并对有限的图像进行训练，可以获得与从头开始训练相媲美的分割性能，且优于其他策略。

    

    胎儿头分割是测量妊娠期间胎儿头围(HC)的关键步骤，是监测胎儿生长的重要生物测定学。然而，手动生成生物学测定是耗时且结果不一致的。为解决这个问题，我们提出了一种迁移学习（TL）方法，通过细调(U-Net网络和轻量级的MobileNet作为编码器)对一组有限的胎儿头超声图像进行分割。这种方法解决了从头开始训练CNN网络的挑战。研究表明，我们提出的细调策略在训练参数减少85.8%的情况下，能够获得可比较的分割性能。并且，我们的细调策略优于其他策略。

    Fetal head segmentation is a crucial step in measuring the fetal head circumference (HC) during gestation, an important biometric in obstetrics for monitoring fetal growth. However, manual biometry generation is time-consuming and results in inconsistent accuracy. To address this issue, convolutional neural network (CNN) models have been utilized to improve the efficiency of medical biometry. But training a CNN network from scratch is a challenging task, we proposed a Transfer Learning (TL) method. Our approach involves fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to perform segmentation on a set of fetal head ultrasound (US) images with limited effort. This method addresses the challenges associated with training a CNN network from scratch. It suggests that our proposed FT strategy yields segmentation performance that is comparable when trained with a reduced number of parameters by 85.8%. And our proposed FT strategy outperforms other strategies with s
    
[^57]: 学习自适应邻域的图神经网络

    Learning Adaptive Neighborhoods for Graph Neural Networks. (arXiv:2307.09065v1 [cs.CV])

    [http://arxiv.org/abs/2307.09065](http://arxiv.org/abs/2307.09065)

    本论文介绍了一种学习自适应邻域的图神经网络模型。通过构建一个端到端可微分的图生成器，模型能够学习选择每个节点的邻域和大小，并将其整合到现有的图卷积网络中。实验证明，在不同的任务和数据集上，该模型相比其他结构学习方法能获得更好的准确性。

    

    图卷积网络（GCNs）可以对图结构数据进行端到端学习。然而，许多方法假设给定图结构。当输入的图具有噪声或不可用时，一种方法是构建或学习一个潜在的图结构。这些方法通常固定整个图的节点度的选择，这是次优的。相反，我们提出了一种新颖的端到端可微分图生成器，该生成器建立图的拓扑结构，每个节点都选择其邻域和大小。我们的模块可以轻松集成到涉及图卷积操作的现有流程中，将预定或现有的邻接矩阵替换为作为整体目标的一部分进行学习和优化的邻接矩阵。因此，它适用于任何GCN。我们将我们的模块集成到轨迹预测、点云分类和节点分类的流程中，在广泛的数据集和GCN上相比其他结构学习方法，获得了更高的准确性。

    Graph convolutional networks (GCNs) enable end-to-end learning on graph structured data. However, many works assume a given graph structure. When the input graph is noisy or unavailable, one approach is to construct or learn a latent graph structure. These methods typically fix the choice of node degree for the entire graph, which is suboptimal. Instead, we propose a novel end-to-end differentiable graph generator which builds graph topologies where each node selects both its neighborhood and its size. Our module can be readily integrated into existing pipelines involving graph convolution operations, replacing the predetermined or existing adjacency matrix with one that is learned, and optimized, as part of the general objective. As such it is applicable to any GCN. We integrate our module into trajectory prediction, point cloud classification and node classification pipelines resulting in improved accuracy over other structure-learning methods across a wide range of datasets and GCN 
    
[^58]: 用类比马尔可夫链和深度学习进行极端热浪的采样和预测

    Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning. (arXiv:2307.09060v1 [physics.ao-ph])

    [http://arxiv.org/abs/2307.09060](http://arxiv.org/abs/2307.09060)

    本研究提出了一种基于类比马尔可夫链和深度学习的数据驱动模拟器，适用于预测法国和斯堪的纳维亚的长时间热浪。与卷积神经网络相比，该模拟器在概率预测任务上表现更好，并且经过适当评估和性能评估。

    

    我们提出了一种数据驱动的模拟器，随机天气生成器（SWG），适用于估计法国和斯堪的纳维亚长时间热浪的概率。这个模拟器基于环流的类比方法，我们加入温度和土壤湿度作为预测字段。我们将模拟器训练在一个中等复杂度气候模型的运行上，并展示它能够预测样本外热浪的条件概率（预测）。我们特别注意，使用适用于罕见事件的合适评分来评估这个预测。为了加速类比的计算，降维技术被应用，并且性能得到评估。通过SWG实现的概率预测与使用卷积神经网络（CNN）实现的概率预测进行了比较。随着数百年的训练数据的可用性，CNN在概率预测任务上表现更好。此外，我们还展示了经过8个训练实例训练的SWG模拟器的效果。

    We present a data-driven emulator, stochastic weather generator (SWG), suitable for estimating probabilities of prolonged heatwaves in France and Scandinavia. This emulator is based on the method of analogs of circulation to which we add temperature and soil moisture as predictor fields. We train the emulator on an intermediate complexity climate model run and show that it is capable of predicting conditional probabilities (forecasting) of heatwaves out of sample. Special attention is payed that this prediction is evaluated using proper score appropriate for rare events. To accelerate the computation of analogs dimensionality reduction techniques are applied and the performance is evaluated. The probabilistic prediction achieved with SWG is compared with the one achieved with  Convolutional Neural Network (CNN). With the availability of hundreds of years of training data CNNs perform better at the task of probabilistic prediction. In addition, we show that the SWG emulator trained on 8
    
[^59]: 在低维欧几里得空间中全局求解点云的Gromov-Wasserstein问题

    Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces. (arXiv:2307.09057v1 [math.OC])

    [http://arxiv.org/abs/2307.09057](http://arxiv.org/abs/2307.09057)

    本文介绍了一个用于在低维欧几里得空间中计算两组点之间Gromov-Wasserstein问题的框架。该框架通过将问题重新表述为一个低维优化问题来解决计算困难，具有良好的可扩展性，并能够在大规模问题中找到全局解决方案。

    

    本文提出了一个在低维空间中计算两组点之间Gromov-Wasserstein问题的框架，其中差异是欧几里得范数的平方。Gromov-Wasserstein问题是优化运输问题的一种推广，它寻找保持尽可能多的成对距离的两组点之间的对应关系。这可以用于量化AI和机器学习中两个形态或形状的相似性，这是一个常见的问题。问题可以被建模为一个Quadratic Assignment Problem（QAP），即使对于小问题来说，QAP在一般情况下也是难以计算的。我们的框架通过将QAP重新表述为一个在低维域上的优化问题来应对这个挑战，利用了问题可以表示为低秩的凹二次优化问题的事实。这种方法在点的数量方面具有良好的可扩展性，并且可以用于在成千上万的大规模问题中找到全局解决方案。

    This paper presents a framework for computing the Gromov-Wasserstein problem between two sets of points in low dimensional spaces, where the discrepancy is the squared Euclidean norm. The Gromov-Wasserstein problem is a generalization of the optimal transport problem that finds the assignment between two sets preserving pairwise distances as much as possible. This can be used to quantify the similarity between two formations or shapes, a common problem in AI and machine learning. The problem can be formulated as a Quadratic Assignment Problem (QAP), which is in general computationally intractable even for small problems. Our framework addresses this challenge by reformulating the QAP as an optimization problem with a low-dimensional domain, leveraging the fact that the problem can be expressed as a concave quadratic optimization problem with low rank. The method scales well with the number of points, and it can be used to find the global solution for large-scale problems with thousands
    
[^60]: 异常鲁棒张量低秩表示用于数据聚类

    Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])

    [http://arxiv.org/abs/2307.09055](http://arxiv.org/abs/2307.09055)

    本文提出了一种异常鲁棒张量低秩表示方法，用于同时检测异常值和进行数据聚类。该方法基于张量奇异值分解（t-SVD）代数框架，并在较弱条件下具有恢复干净数据的行空间和检测异常值的可证明性能保证。此外，还提出了扩展方法以处理数据部分缺失的情况。

    

    低秩张量分析在许多实际应用中受到广泛关注。然而，张量数据经常受到异常值或样本特定的污染。如何恢复被异常值损坏的张量数据并进行数据聚类仍然是一个具有挑战性的问题。本文基于张量奇异值分解（t-SVD）代数框架，提出了一种用于同时检测异常值和张量数据聚类的异常鲁棒张量低秩表示（OR-TLRR）方法。该方法受到最近提出的满足一定条件的可逆线性变换引起的张量张量积的启发。对于带有任意异常值污染的张量观测，OR-TLRR在较弱条件下能够确切恢复干净数据的行空间并检测异常值。此外，还提出了OR-TLRR的扩展来处理数据部分缺失的情况。

    Low-rank tensor analysis has received widespread attention with many practical applications. However, the tensor data are often contaminated by outliers or sample-specific corruptions. How to recover the tensor data that are corrupted by outliers and perform data clustering remains a challenging problem. This paper develops an outlier-robust tensor low-rank representation (OR-TLRR) method for simultaneous outlier detection and tensor data clustering based on the tensor singular value decomposition (t-SVD) algebraic framework. It is motivated by the recently proposed tensor-tensor product induced by invertible linear transforms that satisfy certain conditions. For tensor observations with arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is also proposed to handle the case when parts of the data are missing. Finally, extensive experim
    
[^61]: qecGPT：使用生成式预训练转换器对量子纠错码进行解码

    qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers. (arXiv:2307.09025v1 [quant-ph])

    [http://arxiv.org/abs/2307.09025](http://arxiv.org/abs/2307.09025)

    qecGPT是一个通用框架，用于用生成模型解码量子纠错码。该模型利用Transformers学习逻辑运算符和综合的联合概率，在无监督预训练后可以高效计算和生成最可能的逻辑运算符，比传统方法更快更准确。

    

    我们提出了一个用生成建模解码量子纠错码的通用框架。该模型利用自回归神经网络，特别是使用Transformer学习逻辑运算符和综合的联合概率。该训练是无监督的，不需要标注的训练数据，并且被称为预训练。在预训练之后，模型可以高效地计算给定综合的逻辑运算符的可能性，使用最大似然解码。它可以直接生成最可能的逻辑运算符，计算复杂度为$\mathcal O(2k)$，其中$k$为逻辑量子比特的数量，这比常规的最大似然解码算法$\mathcal O(4^k)$更优。基于预训练模型，我们进一步提出了通过直接采样稳定子算符来更准确地获得给定综合的逻辑运算符可能性的改进方法。

    We propose a general framework for decoding quantum error-correcting codes with generative modeling. The model utilizes autoregressive neural networks, specifically Transformers, to learn the joint probability of logical operators and syndromes. This training is in an unsupervised way, without the need for labeled training data, and is thus referred to as pre-training. After the pre-training, the model can efficiently compute the likelihood of logical operators for any given syndrome, using maximum likelihood decoding. It can directly generate the most-likely logical operators with computational complexity $\mathcal O(2k)$ in the number of logical qubits $k$, which is significantly better than the conventional maximum likelihood decoding algorithms that require $\mathcal O(4^k)$ computation. Based on the pre-trained model, we further propose refinement to achieve more accurately the likelihood of logical operators for a given syndrome by directly sampling the stabilizer operators. We p
    
[^62]: U型变压器：在时间序列分析中保持高频上下文

    U-shaped Transformer: Retain High Frequency Context in Time Series Analysis. (arXiv:2307.09019v1 [cs.LG])

    [http://arxiv.org/abs/2307.09019](http://arxiv.org/abs/2307.09019)

    在时间序列分析中，传统的变压器模型在低秩特性方面存在缺陷，本论文提出了一种U型变压器模型，通过引入跳跃层连接和补丁合并与分割操作，实现了保留高频上下文的效果，并在实验中验证了其性能优于其他模型。

    

    时间序列预测在各个工业领域起着至关重要的作用。近年来，基于变压器的神经网络在许多领域，包括计算机视觉和自然语言处理领域取得了显著的成功。然而，在时间序列分析领域，一些研究表明，即使是最简单的多层感知器（MLP）网络在时间序列预测任务上也胜过高级的基于变压器的网络。然而，我们认为这些发现表明时间序列序列存在低秩特性。在本文中，我们考虑了变压器的低通特性，并尝试结合MLP的优势。我们将受Unet启发的跳跃层连接应用到传统的变压器背骨结构中，从而将输入到输出的高频上下文保留下来，即U型变压器。我们引入了补丁合并和分割操作来提取不同尺度的特征，并使用较大的数据集充分利用变压器的背骨结构。我们的实验证明该模型的性能优于其他模型。

    Time series prediction plays a crucial role in various industrial fields. In recent years, neural networks with a transformer backbone have achieved remarkable success in many domains, including computer vision and NLP. In time series analysis domain, some studies have suggested that even the simplest MLP networks outperform advanced transformer-based networks on time series forecast tasks. However, we believe these findings indicate there to be low-rank properties in time series sequences. In this paper, we consider the low-pass characteristics of transformers and try to incorporate the advantages of MLP. We adopt skip-layer connections inspired by Unet into traditional transformer backbone, thus preserving high-frequency context from input to output, namely U-shaped Transformer. We introduce patch merge and split operation to extract features with different scales and use larger datasets to fully make use of the transformer backbone. Our experiments demonstrate that the model perform
    
[^63]: 基于个体专属数据的多模态健康大型语言模型

    Multimodal LLMs for health grounded in individual-specific data. (arXiv:2307.09018v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.09018](http://arxiv.org/abs/2307.09018)

    本文提出了一个多模态健康大型语言模型（HeLM），通过学习复杂数据模态的编码器，同时支持简单模态数据的文本序列化，HeLM可以有效地使用个体专属数据估计疾病风险。

    

    基于个体专属数据的健康大型语言模型（LLMs）可以有效地解决个性化健康问题，但为了实现这一目标，LLMs需要具备摄入与个体健康状况相关的多样化数据模态能力。本文提出了一个框架（HeLM：健康大型语言模型多模态理解），该框架通过开发一个编码器，将复杂的数据模态映射到LLMs的令牌嵌入空间，并将简单的模态如表格数据序列化为文本，实现了基于个体专属数据的多模态LLMs。通过使用来自英国生物库的数据，我们证明了HeLM能够有效地利用人口统计学和临床特征，以及高维时间序列数据来估计疾病风险。

    Foundation large language models (LLMs) have shown an impressive ability to solve tasks across a wide range of fields including health. To effectively solve personalized health tasks, LLMs need the ability to ingest a diversity of data modalities that are relevant to an individual's health status. In this paper, we take a step towards creating multimodal LLMs for health that are grounded in individual-specific data by developing a framework (HeLM: Health Large Language Model for Multimodal Understanding) that enables LLMs to use high-dimensional clinical modalities to estimate underlying disease risk. HeLM encodes complex data modalities by learning an encoder that maps them into the LLM's token embedding space and for simple modalities like tabular data by serializing the data into text. Using data from the UK Biobank, we show that HeLM can effectively use demographic and clinical features in addition to high-dimensional time-series data to estimate disease risk. For example, HeLM ach
    
[^64]: ChatGPT的行为随时间变化如何？

    How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])

    [http://arxiv.org/abs/2307.09009](http://arxiv.org/abs/2307.09009)

    本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。

    

    GPT-3.5和GPT-4是两种广泛使用的大型语言模型（LLM）服务。然而，这些模型何时以及如何进行更新是不透明的。在这里，我们对GPT-3.5和GPT-4的2023年3月和2023年6月版本进行了评估，涉及四项不同的任务：1）解决数学问题，2）回答敏感/危险问题，3）生成代码和4）视觉推理。我们发现，GPT-3.5和GPT-4的性能和行为在时间上可以有很大的变化。例如，GPT-4（2023年3月）在识别质数方面表现非常出色（准确率为97.6%），但GPT-4（2023年6月）在相同的问题上表现非常差（准确率为2.4%）。有趣的是，GPT-3.5（2023年6月）在这个任务上比GPT-3.5（2023年3月）要好得多。GPT-4在6月份对回答敏感问题的意愿较3月份要低，而无论是GPT-4还是GPT-3.5在6月份的代码生成中都有更多的格式错误。总体而言，我们的发现表明相同LLM服务的行为在相对较短的时间内可以发生重大变化。

    GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
    
[^65]: OxfordVGG对EGO4D AV转录挑战的提交

    OxfordVGG Submission to the EGO4D AV Transcription Challenge. (arXiv:2307.09006v1 [cs.SD])

    [http://arxiv.org/abs/2307.09006](http://arxiv.org/abs/2307.09006)

    OxfordVGG团队的WhisperX系统在EGO4D AV转录挑战中以56.0％的词错误率（WER）排名第1，并提供了两个公开可用的文本规范化工具。

    

    本报告介绍了我们在OxfordVGG团队对EGO4D音频-视觉（AV）自动语音识别挑战2023年上的提交的技术细节。我们提出了WhisperX，一个用于长形音频的高效语音转录系统，具有单词级时间对齐，以及两个公开可用的文本规范化工具。我们的最终提交在挑战测试集上获得了56.0％的词错误率（WER），在排行榜上排名第1。所有基准代码和模型都可以在https://github.com/m-bain/whisperX上找到。

    This report presents the technical details of our submission on the EGO4D Audio-Visual (AV) Automatic Speech Recognition Challenge 2023 from the OxfordVGG team. We present WhisperX, a system for efficient speech transcription of long-form audio with word-level time alignment, along with two text normalisers which are publicly available. Our final submission obtained 56.0% of the Word Error Rate (WER) on the challenge test set, ranked 1st on the leaderboard. All baseline codes and models are available on https://github.com/m-bain/whisperX.
    
[^66]: Oracle高效的在线多校准和全面预测

    Oracle Efficient Online Multicalibration and Omniprediction. (arXiv:2307.08999v1 [cs.LG])

    [http://arxiv.org/abs/2307.08999](http://arxiv.org/abs/2307.08999)

    本文研究了在线对抗背景下的全面预测问题，提出了一种新的在线多校准算法，可以适用于无限的基准函数类，并且是Oracle高效的。

    

    最近的一系列研究表明，多校准（multicalibration）这一多组公平性概念与全面预测（omniprediction）这一为大量损失函数提供同时损失最小化保证的学习范式之间存在意想不到的联系。先前的研究主要集中在批处理设置下的全面预测。我们首次在在线对抗设置下研究了全面预测。尽管已经存在用于在线对抗设置下获取多校准概念的算法，但与批处理算法不同，它们只适用于有限的基准函数类$F$，因为它们要求每一轮枚举每个函数$f \in F$。相反，全面预测对于学习理论的假设类$F$最有趣，而这些类通常是连续大的。我们开发了一种新的在线多校准算法，可以适用于无限的基准函数类$F$，并且是Oracle高效的（即对于任何类$F$，算法都可以转化为高效的约简形式）。

    A recent line of work has shown a surprising connection between multicalibration, a multi-group fairness notion, and omniprediction, a learning paradigm that provides simultaneous loss minimization guarantees for a large family of loss functions. Prior work studies omniprediction in the batch setting. We initiate the study of omniprediction in the online adversarial setting. Although there exist algorithms for obtaining notions of multicalibration in the online adversarial setting, unlike batch algorithms, they work only for small finite classes of benchmark functions $F$, because they require enumerating every function $f \in F$ at every round. In contrast, omniprediction is most interesting for learning theoretic hypothesis classes $F$, which are generally continuously large.  We develop a new online multicalibration algorithm that is well defined for infinite benchmark classes $F$, and is oracle efficient (i.e. for any class $F$, the algorithm has the form of an efficient reduction 
    
[^67]: GraphCL-DTA: 一种基于分子语义的图对比学习方法用于药物靶标结合亲和力预测

    GraphCL-DTA: a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. (arXiv:2307.08989v1 [cs.LG])

    [http://arxiv.org/abs/2307.08989](http://arxiv.org/abs/2307.08989)

    GraphCL-DTA是一种使用分子语义进行药物靶标结合亲和力预测的图对比学习方法。通过设计图对比学习框架，可以学习到更本质和有效的药物表示。

    

    药物靶标结合亲和力预测在药物研发的早期阶段起着重要作用，可以推断新药和新靶标之间相互作用的强度。然而，以往的计算模型在以下方面存在限制：药物表示的学习仅依赖于有监督数据，没有考虑分子图中所包含的信息。此外，大多数先前的研究倾向于设计复杂的表示学习模块，忽略了用于衡量表示质量的一致性。本研究提出了一种名为GraphCL-DTA的图对比学习方法，用于药物靶标结合亲和力预测。在GraphCL-DTA中，我们设计了一个图对比学习框架，用于学习药物表示，以保留分子图的语义。通过这个图对比学习框架，可以得到更本质和有效的药物表示。

    Drug-target binding affinity prediction plays an important role in the early stages of drug discovery, which can infer the strength of interactions between new drugs and new targets. However, the performance of previous computational models is limited by the following drawbacks. The learning of drug representation relies only on supervised data, without taking into account the information contained in the molecular graph itself. Moreover, most previous studies tended to design complicated representation learning module, while uniformity, which is used to measure representation quality, is ignored. In this study, we propose GraphCL-DTA, a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. In GraphCL-DTA, we design a graph contrastive learning framework for molecular graphs to learn drug representations, so that the semantics of molecular graphs are preserved. Through this graph contrastive framework, a more essential and effective drug repre
    
[^68]: 神经网络剪枝作为保持频谱的过程

    Neural Network Pruning as Spectrum Preserving Process. (arXiv:2307.08982v1 [cs.LG])

    [http://arxiv.org/abs/2307.08982](http://arxiv.org/abs/2307.08982)

    本文提出了一种新颖的神经网络剪枝方法，该方法基于矩阵谱学习与神经网络训练之间的密切关系，通过矩阵稀疏化过程来保持频谱，从而得到轻量级神经网络版本。通过实验验证，该方法能够获得更好的剪枝结果。

    

    神经网络在各个应用领域都取得了显著的性能。然而，预训练的深度神经网络中的大量权重使其无法在智能手机和嵌入式系统上部署。获取轻量级神经网络版本以进行边缘设备推理是非常理想的。许多具有成本效益的方法被提出用于修剪在深度神经网络中常见且在参数空间中占主导地位的稠密和卷积层。然而，这个问题的统一理论基础大多缺失。在本文中，我们发现矩阵谱学习与稠密和卷积层的神经网络训练之间存在密切联系，并认为权重修剪本质上是一种保持频谱的矩阵稀疏化过程。基于分析，我们还提出了一种针对神经网络修剪的矩阵稀疏化算法，可以产生更好的修剪结果。我们精心设计和进行实验验证。

    Neural networks have achieved remarkable performance in various application domains. Nevertheless, a large number of weights in pre-trained deep neural networks prohibit them from being deployed on smartphones and embedded systems. It is highly desirable to obtain lightweight versions of neural networks for inference in edge devices. Many cost-effective approaches were proposed to prune dense and convolutional layers that are common in deep neural networks and dominant in the parameter space. However, a unified theoretical foundation for the problem mostly is missing. In this paper, we identify the close connection between matrix spectrum learning and neural network training for dense and convolutional layers and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum. Based on the analysis, we also propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning result. We carefully design and conduct ex
    
[^69]: 一个统一的框架用于在不断观察下的差分私有和之和

    A Unifying Framework for Differentially Private Sums under Continual Observation. (arXiv:2307.08970v1 [cs.LG])

    [http://arxiv.org/abs/2307.08970](http://arxiv.org/abs/2307.08970)

    这篇文章提出了一个统一的框架和高效算法，用于在不断观察下维护差分私有递减和。算法改进了之前的研究，并且实现了多项式递减权重下没有乘法误差的差分私有算法。

    

    我们研究在不断观察下维护差分私有递减和的问题。我们提供了一个统一的框架和一个高效的算法解决这个问题，对于\emph{任何足够平滑}的函数都适用。我们的算法是第一个对于多项式递减权重没有乘法误差的差分私有算法。我们的算法改进了之前所有在不断观察下的差分私有递减和的工作，并且作为一个推论，恢复了Henzinger等人（SODA 2023）在连续计数特例中的加法误差。我们的算法是基于因子化机制的变体，其误差取决于底层矩阵的$\gamma_2$和$\gamma_F$范数。我们为一大类下三角矩阵给出了一个几乎精确的$\gamma_2$和$\gamma_F$范数的上界的构造性证明，并且为$\gamma_2$范数给出了一个几乎紧确的下界。这是对于下三角矩阵的第一个非平凡下界。

    We study the problem of maintaining a differentially private decaying sum under continual observation. We give a unifying framework and an efficient algorithm for this problem for \emph{any sufficiently smooth} function. Our algorithm is the first differentially private algorithm that does not have a multiplicative error for polynomially-decaying weights. Our algorithm improves on all prior works on differentially private decaying sums under continual observation and recovers exactly the additive error for the special case of continual counting from Henzinger et al. (SODA 2023) as a corollary.  Our algorithm is a variant of the factorization mechanism whose error depends on the $\gamma_2$ and $\gamma_F$ norm of the underlying matrix. We give a constructive proof for an almost exact upper bound on the $\gamma_2$ and $\gamma_F$ norm and an almost tight lower bound on the $\gamma_2$ norm for a large class of lower-triangular matrices. This is the first non-trivial lower bound for lower-tr
    
[^70]: 景观替代品：在部分信息下学习数学优化的决策损失

    Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])

    [http://arxiv.org/abs/2307.08964](http://arxiv.org/abs/2307.08964)

    本论文提出了一种使用景观替代品的学习方法，旨在解决部分信息下数学优化问题中的挑战。这种方法可以通过学习优化器来加速优化过程，并且能够处理问题的不确定性。

    

    最近的学习集成优化工作在优化问题只有部分可观测或通用优化器在无专家调优的情况下表现不佳的情况下显示出了希望。通过学习一个优化器$ \mathbf{g} $来解决这些具有挑战性的问题，通过利用过去的经验，可以显著加速优化过程。优化器可以通过已知最优解的监督或通过优化复合函数$ f\circ \mathbf{g} $的隐式方式进行训练。隐式方法可能不需要最优解作为标签，并且能够处理问题的不确定性；然而，由于在训练和测试过程中频繁调用优化器$ \mathbf{g} $，因此训练和部署缓慢。对于组合求解器，由于$ \mathbf{g} $的稀疏梯度，训练进一步受到挑战。为了解决这些问题，我们提出使用平滑可学习的景观替代品$ M $作为一种替代方法。

    Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replace
    
[^71]: REX: 快速探索与利用的增强型AI代理方法

    REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v1 [cs.AI])

    [http://arxiv.org/abs/2307.08962](http://arxiv.org/abs/2307.08962)

    本文提出了一种增强型的快速探索与利用的AI代理方法REX，它通过引入额外的奖励层和类似于UCB分数的概念，实现了更强大和高效的AI代理性能，并且具有离线行为利用和与基础模型无缝集成的优势。

    

    本文提出了一种增强型的快速探索与利用的AI代理方法，称为REX。现有的AutoGPT风格技术存在一些固有的限制，如对于决策的精确描述的过度依赖，以及缺乏类似传统强化学习(Reinforcement Learning，RL)中的尝试和失败程序的系统性方法。REX引入了额外的奖励层，并集成了类似于上限置信界限(UCB)分数的概念，从而实现更强大和高效的AI代理性能。这种方法的优势是可以利用来自日志的离线行为，并与现有的基础模型无缝集成，而不需要任何模型微调。通过与现有方法（如思维链(CoT)和规划推理(RAP)）的比较分析，基于REX的方法展示了相当的性能，并在某些情况下甚至超过了这些现有技术所取得的结果。

    In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniqu
    
[^72]: 基于离散化的集成模型用于IoT中的健壮学习

    Discretization-based ensemble model for robust learning in IoT. (arXiv:2307.08955v1 [cs.LG])

    [http://arxiv.org/abs/2307.08955](http://arxiv.org/abs/2307.08955)

    本论文研究了离散化技术和集成方法在提高IoT设备识别模型稳健性方面的应用，通过减少对抗攻击的敏感性和减少噪声或错误的影响来提高模型的稳定性和可靠性。

    

    IoT设备识别是识别和验证连接到网络的IoT设备的过程。这是确保只有授权设备可以访问网络的重要过程，对于网络管理和维护来说是必要的。近年来，机器学习模型广泛用于自动化网络中的设备识别过程。然而，这些模型容易受到对抗攻击影响，从而影响准确性和有效性。为了更好地保护设备识别模型，离散化技术能够减少机器学习模型对对抗攻击的敏感性，提高模型的稳定性和可靠性。另一方面，集成方法将多个异质模型结合起来，减少模型中剩余噪声或错误的影响。因此，在本文中，我们将离散化技术和集成方法相结合，并研究其对抗攻击下的模型稳健性。

    IoT device identification is the process of recognizing and verifying connected IoT devices to the network. This is an essential process for ensuring that only authorized devices can access the network, and it is necessary for network management and maintenance. In recent years, machine learning models have been used widely for automating the process of identifying devices in the network. However, these models are vulnerable to adversarial attacks that can compromise their accuracy and effectiveness. To better secure device identification models, discretization techniques enable reduction in the sensitivity of machine learning models to adversarial attacks contributing to the stability and reliability of the model. On the other hand, Ensemble methods combine multiple heterogeneous models to reduce the impact of remaining noise or errors in the model. Therefore, in this paper, we integrate discretization techniques and ensemble methods and examine it on model robustness against adversar
    
[^73]: 知识注入的深度学习实现可解释性滑坡预测

    Knowledge-infused Deep Learning Enables Interpretable Landslide Forecasting. (arXiv:2307.08951v1 [cs.LG])

    [http://arxiv.org/abs/2307.08951](http://arxiv.org/abs/2307.08951)

    该论文提出了一种知识注入的深度学习流程，使用转换器网络LFIT从先前的知识和多源数据中学习滑坡的复杂非线性关系，实现了全面预测滑坡行为的能力，并提高了滑坡预测的综合性和可解释性。

    

    预测滑坡在时间上如何演变或者它们是否会发生故障是一项具有挑战性的任务，由于内部和外部的多种因素。尽管深度学习技术在应对这些挑战方面具有相当大的潜力，但它们缺乏可解释性，这削弱了它们产生的预测的可信度。最近发展的基于转换器的深度学习为用于预测滑坡提供了未被开发的可能性，具有前所未有的可解释性和非线性特征学习能力。在这里，我们提出了一个深度学习流程，能够全面预测滑坡的行为，它使用一个名为LFIT的基于转换器的网络从先前的知识和多源数据中学习复杂的非线性关系，识别最相关的变量，并展示对滑坡演化和时间模式的全面理解。通过整合先前的知识，我们提供了综合滑坡预测的改进，实现了可解释性。

    Forecasting how landslides will evolve over time or whether they will fail is a challenging task due to a variety of factors, both internal and external. Despite their considerable potential to address these challenges, deep learning techniques lack interpretability, undermining the credibility of the forecasts they produce. The recent development of transformer-based deep learning offers untapped possibilities for forecasting landslides with unprecedented interpretability and nonlinear feature learning capabilities. Here, we present a deep learning pipeline that is capable of predicting landslide behavior holistically, which employs a transformer-based network called LFIT to learn complex nonlinear relationships from prior knowledge and multiple source data, identifying the most relevant variables, and demonstrating a comprehensive understanding of landslide evolution and temporal patterns. By integrating prior knowledge, we provide improvement in holistic landslide forecasting, enabl
    
[^74]: Alioth: 基于机器学习的公共云多租户应用干扰感知性能监视器

    Alioth: A Machine Learning Based Interference-Aware Performance Monitor for Multi-Tenancy Applications in Public Cloud. (arXiv:2307.08949v1 [cs.DC])

    [http://arxiv.org/abs/2307.08949](http://arxiv.org/abs/2307.08949)

    Alioth是一种基于机器学习的性能监视器，用于监测公共云中多租户应用的干扰引起的性能下降。通过构建反映真实场景中复杂性和动态性的数据集，Alioth能够提供对于相关干扰事件的感知和分析能力。

    

    公共云中的多租户可能导致共享资源的共存干扰，可能导致云应用性能下降。云服务提供商希望在发生此类事件时知道发生的时间以及性能下降的严重程度，以执行干扰感知迁移并缓解问题。然而，基础设施即服务（IaaS）公共云中的虚拟机（VM）对于提供商来说是黑盒子，无法获取应用级性能信息。这使得性能监视变得异常困难，因为云服务提供商只能依赖于低级指标，如CPU使用率和硬件计数器。我们提出了一种新颖的机器学习框架Alioth，用于监视云应用的性能下降。为了为数据密集型模型提供数据，我们首先阐述干扰生成器，并在实验平台上进行全面的共存实验，构建反映现实场景中复杂性和动态性的Alioth数据集。

    Multi-tenancy in public clouds may lead to co-location interference on shared resources, which possibly results in performance degradation of cloud applications. Cloud providers want to know when such events happen and how serious the degradation is, to perform interference-aware migrations and alleviate the problem. However, virtual machines (VM) in Infrastructure-as-a-Service public clouds are black-boxes to providers, where application-level performance information cannot be acquired. This makes performance monitoring intensely challenging as cloud providers can only rely on low-level metrics such as CPU usage and hardware counters.  We propose a novel machine learning framework, Alioth, to monitor the performance degradation of cloud applications. To feed the data-hungry models, we first elaborate interference generators and conduct comprehensive co-location experiments on a testbed to build Alioth-dataset which reflects the complexity and dynamicity in real-world scenarios. Then w
    
[^75]: 通过解耦置信学习来减缓标签偏见

    Mitigating Label Bias via Decoupled Confident Learning. (arXiv:2307.08945v1 [cs.LG])

    [http://arxiv.org/abs/2307.08945](http://arxiv.org/abs/2307.08945)

    这项研究提出了一种名为DeCoLe的修剪方法，用于减轻标签偏见问题。研究在合成数据集和仇恨言论检测领域取得了成功的结果。

    

    随着对算法公平性的担忧增加，出现了许多方法来减轻算法偏见。然而，这些方法主要假设训练数据中观察到的标签是正确的。这是有问题的，因为标签偏见在包括医疗、招聘和内容审核在内的重要领域中普遍存在。特别是，人类生成的标签容易带有社会偏见。虽然标签偏见的存在已经在概念上进行了讨论，但缺乏应对此问题的方法。我们提出了一种修剪方法——解耦置信学习（DeCoLe），专门设计来减轻标签偏见。在演示了其在合成数据集上的性能后，我们将DeCoLe应用于仇恨言论检测的环境中，这是一个被认为是重要挑战的领域，并展示其成功识别出偏见标签并超越竞争方法的表现。

    Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias. However, such methodologies largely assume that observed labels in training data are correct. This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation. In particular, human-generated labels are prone to encoding societal biases. While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.
    
[^76]: 弱监督人体活动识别中的Siamese网络

    Siamese Networks for Weakly Supervised Human Activity Recognition. (arXiv:2307.08944v1 [cs.HC])

    [http://arxiv.org/abs/2307.08944](http://arxiv.org/abs/2307.08944)

    本文提出了一种使用Siamese网络进行弱监督人体活动识别的模型，通过仅利用数据样本对的相似性进行训练，该模型可以作为不同聚类算法的度量标准，在三个数据集上的评估结果证明了其有效性。

    

    深度学习已成功应用于人体活动识别。然而，训练深度神经网络需要明确标记的数据，而获取这些数据很困难。本文提出了一种采用多个Siamese网络的模型，仅利用数据样本对之间的相似性信息进行训练，而不知道明确的标签。训练后的模型将活动数据样本映射为固定大小的表示向量，使得表示空间中向量之间的距离近似于输入空间中数据样本的相似性。因此，训练后的模型可以作为广泛的不同聚类算法的度量标准。训练过程通过最小化相似性损失函数，使得同一种活动的样本对的距离度量小，并使得不同种活动的样本对的距离度量大。我们在三个数据集上对该模型进行评估，验证其在分割中的有效性。

    Deep learning has been successfully applied to human activity recognition. However, training deep neural networks requires explicitly labeled data which is difficult to acquire. In this paper, we present a model with multiple siamese networks that are trained by using only the information about the similarity between pairs of data samples without knowing the explicit labels. The trained model maps the activity data samples into fixed size representation vectors such that the distance between the vectors in the representation space approximates the similarity of the data samples in the input space. Thus, the trained model can work as a metric for a wide range of different clustering algorithms. The training process minimizes a similarity loss function that forces the distance metric to be small for pairs of samples from the same kind of activity, and large for pairs of samples from different kinds of activities. We evaluate the model on three datasets to verify its effectiveness in segm
    
[^77]: NTK-近似MLP融合用于高效的语言模型微调

    NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])

    [http://arxiv.org/abs/2307.08941](http://arxiv.org/abs/2307.08941)

    该论文通过使用神经切向核近似MLP融合，提出了一种高效的语言模型微调方法。实验证明，这种方法能够在降低计算和存储开销的同时保持较好的模型性能。

    

    在许多自然语言处理应用中，微调预训练语言模型(PLM)已成为主要策略。然而，即使是微调PLM和进行推理也是昂贵的，特别是在计算能力较低的边缘设备上。已经广泛研究了一些通用的方法（例如量化和蒸馏）来减少PLM微调的计算/存储开销，但很少有一次性压缩技术被探索。在本文中，我们研究了多层感知器(MLP)模块中预训练语言模型(PLM)的神经切向核(NTK)，并提出通过NTK近似MLP融合来创建一个轻量级的PLM。为实现这一目标，我们将MLP重新视为一束子MLP，并将它们聚类为给定数量的质心，然后将其恢复为压缩的MLP，并意外地显示出对原始PLM的NTK进行良好近似的效果。在自然语言处理数据集上进行了大量实验以验证PLM微调的效果。

    Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural l
    
[^78]: 基于深度神经网络的自适应巡航控制在上下文感知攻击下的安全性实验分析

    Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (arXiv:2307.08939v1 [cs.CR])

    [http://arxiv.org/abs/2307.08939](http://arxiv.org/abs/2307.08939)

    这项研究评估了基于深度神经网络的自适应巡航控制系统在隐蔽感知攻击下的安全性，并提出了一种上下文感知策略和基于优化的图像扰动生成方法。

    

    自适应巡航控制（ACC）是一种广泛应用的驾驶员辅助功能，用于保持期望速度和与前方车辆的安全距离。本文评估基于深度神经网络（DNN）的ACC系统在隐蔽感知攻击下的安全性，该攻击会对摄像机数据进行有针对性的扰动，以导致前方碰撞事故。我们提出了一种基于知识和数据驱动的方法，设计了一种上下文感知策略，用于选择触发攻击最关键的时间点，并采用了一种新颖的基于优化的方法，在运行时生成适应性图像扰动。我们使用实际驾驶数据集和逼真的仿真平台评估了所提出攻击的有效性，该仿真平台使用了来自生产ACC系统的控制软件和物理世界驾驶模拟器，并考虑了驾驶员的干预以及自动紧急制动（AEB）和前向碰撞警示（FCW）等安全功能。

    Adaptive Cruise Control (ACC) is a widely used driver assistance feature for maintaining desired speed and safe distance to the leading vehicles. This paper evaluates the security of the deep neural network (DNN) based ACC systems under stealthy perception attacks that strategically inject perturbations into camera data to cause forward collisions. We present a combined knowledge-and-data-driven approach to design a context-aware strategy for the selection of the most critical times for triggering the attacks and a novel optimization-based method for the adaptive generation of image perturbations at run-time. We evaluate the effectiveness of the proposed attack using an actual driving dataset and a realistic simulation platform with the control software from a production ACC system and a physical-world driving simulator while considering interventions by the driver and safety features such as Automatic Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental results sh
    
[^79]: 多阶段神经网络：机器精度的函数逼近器

    Multi-stage Neural Networks: Function Approximator of Machine Precision. (arXiv:2307.08934v1 [cs.LG])

    [http://arxiv.org/abs/2307.08934](http://arxiv.org/abs/2307.08934)

    多阶段神经网络通过将训练过程分为不同阶段，并优化拟合残差，成功解决了常规神经网络中的谱偏差问题，提高了预测精度。

    

    深度学习技术越来越多地应用于科学问题，网络的精度至关重要。尽管神经网络被认为是通用的函数逼近器，但在实践中，即使使用大型网络和扩展的训练迭代，它们仍然难以将预测误差降低到10的负5次方以下。为了解决这个问题，我们开发了多阶段神经网络，将训练过程分为不同的阶段，每个阶段使用一个新的网络，针对前一阶段的残差进行优化拟合。在连续的阶段中，残差幅度显著减小，并且与残差频率呈反幂律关系。多阶段神经网络有效地减小了常规神经网络中的谱偏差，使其能够捕捉目标函数的高频特征。我们证明了多阶段训练在回归问题和物理问题的预测误差上的有效性。

    Deep learning techniques are increasingly applied to scientific problems, where the precision of networks is crucial. Despite being deemed as universal function approximators, neural networks, in practice, struggle to reduce the prediction errors below $O(10^{-5})$ even with large network size and extended training iterations. To address this issue, we developed the multi-stage neural networks that divides the training process into different stages, with each stage using a new network that is optimized to fit the residue from the previous stage. Across successive stages, the residue magnitudes decreases substantially and follows an inverse power-law relationship with the residue frequencies. The multi-stage neural networks effectively mitigate the spectral biases associated with regular neural networks, enabling them to capture the high frequency feature of target functions. We demonstrate that the prediction error from the multi-stage training for both regression problems and physics-
    
[^80]: IxDRL:一种基于有趣分析的新型可解释深度强化学习工具包

    IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness. (arXiv:2307.08933v1 [cs.AI])

    [http://arxiv.org/abs/2307.08933](http://arxiv.org/abs/2307.08933)

    IxDRL是一种基于有趣分析的新型可解释深度强化学习工具包，具备能力感知机制，能够提供人类操作员对RL代理能力的整体视图。

    

    近年来，深度学习的进展在使用强化学习（RL）解决具有高维输入的复杂顺序决策任务方面取得了众多成功。然而，现有系统缺乏必要的机制来提供人类对其能力的整体视图，这在关键应用中成为采用RL的障碍，因为代理程序所做的决策可能具有重大后果。然而，现有的RL系统本质上不具备能力感知，因为它们缺乏必要的解释机制，使人类操作员能够对其能力有深入、整体的了解。为了实现更可解释的深度RL（xDRL），我们提出了一个基于有趣分析的新框架。我们的工具基于有趣分析提供多种RL代理的能力度量，适用于各种RL算法，并原生支持流行的RLLib工具包。我们展示了该工具包在...（原文截断）

    In recent years, advances in deep learning have resulted in a plethora of successes in the use of reinforcement learning (RL) to solve complex sequential decision tasks with high-dimensional inputs. However, existing systems lack the necessary mechanisms to provide humans with a holistic view of their competence, presenting an impediment to their adoption, particularly in critical applications where the decisions an agent makes can have significant consequences. Yet, existing RL-based systems are essentially competency-unaware in that they lack the necessary interpretation mechanisms to allow human operators to have an insightful, holistic view of their competency. Towards more explainable Deep RL (xDRL), we propose a new framework based on analyses of interestingness. Our tool provides various measures of RL agent competence stemming from interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit. We showcase the use of o
    
[^81]: 用于有效哈密顿量参数化的即时机器学习

    On-the-fly machine learning for parametrization of the effective Hamiltonian. (arXiv:2307.08929v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.08929](http://arxiv.org/abs/2307.08929)

    本论文提出了一种基于贝叶斯线性回归的即时机器学习方法，用于参数化有效哈密顿量。该方法在各种系统中都可以得到准确的结果，包括以前的方法无法处理的复杂系统。

    

    基于第一原理的有效哈密顿量被广泛用于预测和模拟铁电和弛豫铁电体的性质。然而，有效哈密顿量的参数化方法复杂，很难解决具有复杂相互作用和/或复杂组分系统。在这里，我们开发了一种基于贝叶斯线性回归的即时机器学习方法来参数化有效哈密顿量。参数化是在分子动力学模拟中完成的，每一步预测能量、力和应力以及相关的不确定性。当不确定性较大时，执行第一原理计算以重新训练参数。这种方法为计算任何所考虑系统的有效哈密顿量参数提供了一种通用和自动化的方式，包括以前的方法无法处理的复杂系统。以BaTiO3和Pb(Sc,Ta)O3为例展示了该方法的准确性。

    The first-principles-based effective Hamiltonian is widely used to predict and simulate the properties of ferroelectrics and relaxor ferroelectrics. However, the parametrization method of the effective Hamiltonian is complicated and hardly can resolve the systems with complex interactions and/or complex components. Here, we developed an on-the-fly machine learning approach to parametrize the effective Hamiltonian based on Bayesian linear regression. The parametrization is completed in molecular dynamics simulations, with the energy, forces and stress predicted at each step along with their uncertainties. First-principles calculations are executed when the uncertainties are large to retrain the parameters. This approach provides a universal and automatic way to compute the effective Hamiltonian parameters for any considered systems including complex systems which previous methods can not handle. BaTiO3 and Pb(Sc,Ta)O3 are taken as examples to show the accurateness of this approach compa
    
[^82]: 联邦式大规模语言模型：一个立场论文

    Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])

    [http://arxiv.org/abs/2307.08925](http://arxiv.org/abs/2307.08925)

    我们提出了联邦式大规模语言模型的概念，通过联邦学习实现分散数据的共同训练共享模型，以应对公共数据可用性的限制和私有数据的隐私保护需求。我们讨论了预训练、微调和提示工程这三个组件的优势，并提出了实施策略。同时，我们探讨了FL和LLM集成带来的新挑战，并分析了现有解决方案和潜在障碍。

    

    大规模语言模型（LLM）在各个领域获得了相当大的关注并找到了多样化的应用，但在真实场景中开发时面临挑战。这些挑战源于公共领域数据可用性的匮乏以及对私有领域数据的隐私保护需求。为了解决这些问题，联邦学习（FL）作为一项有前景的技术出现了，它能够在保持分散数据的同时实现共同训练共享模型。我们提出了联邦式LLM的概念，包括三个关键组成部分，即联邦式LLM预训练、联邦式LLM微调和联邦式LLM提示工程。对于每个组件，我们讨论了它相对于传统LLM训练方法的优势，并提出了具体的工程策略来实施。此外，我们探讨了FL和LLM集成带来的新挑战。我们分析现有的解决方案并确定可能的障碍

    Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles fac
    
[^83]: 学习采样任务用于元学习

    Learning to Sample Tasks for Meta Learning. (arXiv:2307.08924v1 [cs.LG])

    [http://arxiv.org/abs/2307.08924](http://arxiv.org/abs/2307.08924)

    通过实验得出了三个结论：没有通用的任务采样策略能保证元学习模型的性能；任务的多样性会导致模型在训练过程中出现欠拟合或过拟合的问题；模型的泛化性能受到任务的差异、任务熵和任务难度的影响。针对这些发现，提出了一种新颖的任务采样器ASr，它利用任务的差异、任务熵和任务难度来采样任务，并通过重新思考和提出一个简单而通用的元学习算法来优化ASr。大量实证实验表明了ASr的有效性。

    

    通过对各种元学习方法、任务采样器和少样本学习任务的实验，本文得出了三个结论。首先，没有通用的任务采样策略能保证元学习模型的性能。其次，任务的多样性会导致模型在训练过程中出现欠拟合或过拟合的问题。最后，模型的泛化性能受到任务的差异、任务熵和任务难度的影响。针对这些发现，我们提出了一种新颖的任务采样器，称为自适应采样器（ASr）。ASr是一个即插即用的任务采样器，它利用任务的差异、任务熵和任务难度来采样任务。为了优化ASr，我们重新思考并提出了一个简单而通用的元学习算法。最后，大量的实证实验证明了所提出的ASr的有效性。

    Through experiments on various meta-learning methods, task samplers, and few-shot learning tasks, this paper arrives at three conclusions. Firstly, there are no universal task sampling strategies to guarantee the performance of meta-learning models. Secondly, task diversity can cause the models to either underfit or overfit during training. Lastly, the generalization performance of the models are influenced by task divergence, task entropy, and task difficulty. In response to these findings, we propose a novel task sampler called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes task divergence, task entropy, and task difficulty to sample tasks. To optimize ASr, we rethink and propose a simple and general meta-learning algorithm. Finally, a large number of empirical experiments demonstrate the effectiveness of the proposed ASr.
    
[^84]: 乐观估计揭示了非线性模型的潜力

    Optimistic Estimate Uncovers the Potential of Nonlinear Models. (arXiv:2307.08921v1 [cs.LG])

    [http://arxiv.org/abs/2307.08921](http://arxiv.org/abs/2307.08921)

    通过乐观估计方法，研究揭示了非线性模型在拟合目标函数时的潜力，并提出了DNN的架构设计原则。

    

    我们提出了一种乐观估计方法，用于评估非线性模型的最佳拟合性能。通过这种方法，我们可以得到一个乐观样本大小，用于确定使用非线性模型来拟合或恢复目标函数所需的最小样本大小。我们估计了矩阵因式分解模型、深度模型和具有全连接或卷积结构的深度神经网络(DNN)的乐观样本大小。对于每个非线性模型，我们的估计预测了可以在过度参数化情况下拟合的特定目标子集，这得到了我们的实验证实。我们的乐观估计揭示了DNN模型的两个特殊属性--宽度上的自由表达和连接上的昂贵表达。这些属性提示了DNN的以下架构设计原则：(i)随意增加神经元/核；(ii)避免连接神经元。总体上，我们的乐观估计在理论上揭示了非线性模型在过度参数化拟合中的巨大潜力。

    We propose an optimistic estimate to evaluate the best possible fitting performance of nonlinear models. It yields an optimistic sample size that quantifies the smallest possible sample size to fit/recover a target function using a nonlinear model. We estimate the optimistic sample sizes for matrix factorization models, deep models, and deep neural networks (DNNs) with fully-connected or convolutional architecture. For each nonlinear model, our estimates predict a specific subset of targets that can be fitted at overparameterization, which are confirmed by our experiments. Our optimistic estimate reveals two special properties of the DNN models -- free expressiveness in width and costly expressiveness in connection. These properties suggest the following architecture design principles of DNNs: (i) feel free to add neurons/kernels; (ii) restrain from connecting neurons. Overall, our optimistic estimate theoretically unveils the vast potential of nonlinear models in fitting at overparame
    
[^85]: 连续时间强化学习: 具有理论洞察力和性能保证的新设计算法

    Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees. (arXiv:2307.08920v1 [eess.SY])

    [http://arxiv.org/abs/2307.08920](http://arxiv.org/abs/2307.08920)

    本论文介绍了一套新的连续时间强化学习算法，用于控制仿射非线性系统。这些算法解决了现有方法面临的复杂性、数值条件和维度扩展等设计挑战。

    

    连续时间非线性最优控制问题在实际应用中具有巨大潜力。虽经历了几十年的发展，强化学习（RL）作为一种通用的非线性控制设计方法已取得了巨大的成功。然而，最近对现有的连续时间强化学习（CT-RL）方法进行的全面分析揭示了它们面临重大的设计挑战，包括复杂性、数值条件和维度扩展问题。尽管有先进的理论结果，但现有的ADP CT-RL综合方法在解决即使是小的学术问题时都不足够。因此，本文的目标是介绍一套用于控制仿射非线性系统的新的CT-RL算法。我们的设计方法依赖于两个重要因素。首先，我们的方法适用于可以划分为较小子问题的物理系统。这种构造性的考虑使得问题的维度减少。

    Continuous-time nonlinear optimal control problems hold great promise in real-world applications. After decades of development, reinforcement learning (RL) has achieved some of the greatest successes as a general nonlinear control design method. However, a recent comprehensive analysis of state-of-the-art continuous-time RL (CT-RL) methods, namely, adaptive dynamic programming (ADP)-based CT-RL algorithms, reveals they face significant design challenges due to their complexity, numerical conditioning, and dimensional scaling issues. Despite advanced theoretical results, existing ADP CT-RL synthesis methods are inadequate in solving even small, academic problems. The goal of this work is thus to introduce a suite of new CT-RL algorithms for control of affine nonlinear systems. Our design approach relies on two important factors. First, our methods are applicable to physical systems that can be partitioned into smaller subproblems. This constructive consideration results in reduced dimen
    
[^86]: 半监督和自监督学习在医学图像上的准确性与时间前沿比较

    Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images. (arXiv:2307.08919v1 [cs.CV])

    [http://arxiv.org/abs/2307.08919](http://arxiv.org/abs/2307.08919)

    半监督和自监督学习在医学图像上的准确性与时间前沿进行比较，通过一个精心设计的基准研究来回答从业者的问题。

    

    对于许多医学图像分类器的应用来说，每个图像都很难或昂贵地获得一个可信的标签。相比之下，没有标签的图像更容易获取。两个主要研究方向都承诺额外的无标签数据可以提高分类器的性能：自监督学习仅在无标签数据上预训练有用的表示，然后通过标记集对这些表示进行微调以获得分类器；半监督学习同时在标记和无标签数据上直接训练分类器。最近的方法从两个方向上都声称在非医学任务上取得了显著的收益，但没有系统评估医学图像，并且大多只与同一方向的方法进行比较。本研究提供了一个经过精心设计的基准来回答从业者的一个关键问题：在小型标记数据集和有限的培训时间预算下，额外的无标签图像能够产生多大的收益。

    For many applications of classifiers to medical images, a trustworthy label for each image can be difficult or expensive to obtain. In contrast, images without labels are more readily available. Two major research directions both promise that additional unlabeled data can improve classifier performance: self-supervised learning pretrains useful representations on unlabeled data only, then fine-tunes a classifier on these representations via the labeled set; semi-supervised learning directly trains a classifier on labeled and unlabeled data simultaneously. Recent methods from both directions have claimed significant gains on non-medical tasks, but do not systematically assess medical images and mostly compare only to methods in the same direction. This study contributes a carefully-designed benchmark to help answer a practitioner's key question: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unlabeled images are possible and 
    
[^87]: 自监督学习中投影头的稀疏性研究

    Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v1 [cs.LG])

    [http://arxiv.org/abs/2307.08913](http://arxiv.org/abs/2307.08913)

    该论文研究了自监督学习中投影头的稀疏性，发现通过在投影子空间中执行对比损失可以提升表示的质量，建议只有一部分特征是必要的，而稀疏的投影头可以增强模型的泛化性能。

    

    最近几年，自监督学习（SSL）已成为从无标签数据中提取有价值表示的一种有希望的方法。其中一种成功的SSL方法是对比学习，其旨在将正样本聚集在一起，将负样本推开。许多当前的对比学习方法都使用参数化的投影头。通过实证分析和理论探索，我们对投影头的内部机制及其与维度折叠现象的关系进行了深入研究。我们的研究结果表明，投影头通过在一个投影子空间中执行对比损失，提升表示的质量。因此，我们提出一个假设，即在最小化一个小批量数据的对比损失时，只有一部分特征是必要的。理论分析进一步表明，稀疏的投影头可以增强泛化性能，因此我们引入SparseHead这一方法。

    In recent years, self-supervised learning (SSL) has emerged as a promising approach for extracting valuable representations from unlabeled data. One successful SSL method is contrastive learning, which aims to bring positive examples closer while pushing negative examples apart. Many current contrastive learning approaches utilize a parameterized projection head. Through a combination of empirical analysis and theoretical investigation, we provide insights into the internal mechanisms of the projection head and its relationship with the phenomenon of dimensional collapse. Our findings demonstrate that the projection head enhances the quality of representations by performing contrastive loss in a projected subspace. Therefore, we propose an assumption that only a subset of features is necessary when minimizing the contrastive loss of a mini-batch of data. Theoretical analysis further suggests that a sparse projection head can enhance generalization, leading us to introduce SparseHead - 
    
[^88]: 锐度感知的图协同过滤

    Sharpness-Aware Graph Collaborative Filtering. (arXiv:2307.08910v1 [cs.LG])

    [http://arxiv.org/abs/2307.08910](http://arxiv.org/abs/2307.08910)

    锐度感知的图协同过滤中，提出了一种名为gSAM的训练方案，通过正则化权重损失函数的平坦程度，选择平坦最小值以提高泛化能力，在协同过滤中取得了优越的性能。

    

    图神经网络(GNNs)在协同过滤中取得了令人印象深刻的性能。然而，当训练和测试数据的分布不太一致时，GNNs往往会产生较差的性能。此外，训练GNNs需要优化非凸神经网络，有大量局部最小值和全局最小值，这些在测试时可能性能差异很大。因此，选择最小值非常重要。在这里，我们提出了一种有效的训练方案，称为{gSAM}，根据“平坦”最小值比“锐利”最小值具有更好的泛化能力的原则。为了实现这个目标，gSAM通过形成双层优化来正则化权重损失函数的平坦程度：外部问题进行标准模型训练，而内部问题则帮助模型跳出锐利的最小值。实验结果显示了我们的gSAM的优越性。

    Graph Neural Networks (GNNs) have achieved impressive performance in collaborative filtering. However, GNNs tend to yield inferior performance when the distributions of training and test data are not aligned well. Also, training GNNs requires optimizing non-convex neural networks with an abundance of local and global minima, which may differ widely in their performance at test time. Thus, it is essential to choose the minima carefully. Here we propose an effective training schema, called {gSAM}, under the principle that the \textit{flatter} minima has a better generalization ability than the \textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of the weight loss landscape by forming a bi-level optimization: the outer problem conducts the standard model training while the inner problem helps the model jump out of the sharp minima. Experimental results show the superiority of our gSAM.
    
[^89]: 基于多智能体强化学习方法的一型糖尿病（T1D）患者基础-推荐剂量顾问

    Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology. (arXiv:2307.08897v1 [cs.LG])

    [http://arxiv.org/abs/2307.08897](http://arxiv.org/abs/2307.08897)

    本文提出了一种基于多智能体强化学习方法的一型糖尿病患者个性化血糖控制系统，通过显著改善血糖控制、减少血糖变异性以及预防低血糖事件等，该方法具有潜力成为一种有效的治疗方法。

    

    本文提出了一种针对一型糖尿病（T1D）患者个性化血糖控制的新型多智能体强化学习（RL）方法。该方法采用闭环系统，包括血糖代谢模型和作为基础-推荐剂量顾问的多智能体软性演员-评论家RL模型。通过比较RL代理与传统治疗在三个场景下的性能评估，评估指标包括血糖水平（最低、最高和平均），在不同血糖范围内的时间，以及平均每日推荐剂量和基础胰岛素剂量。结果表明，基于RL的基础-推荐剂量顾问显著改善了血糖控制，减小了血糖变异性，增加了在目标范围（70-180 mg/dL）内的时间。低血糖事件得到有效预防，严重高血糖事件得到减少。RL方法还导致与传统方法相比，平均每日基础胰岛素剂量有统计学显著减少。

    This paper presents a novel multi-agent reinforcement learning (RL) approach for personalized glucose control in individuals with type 1 diabetes (T1D). The method employs a closed-loop system consisting of a blood glucose (BG) metabolic model and a multi-agent soft actor-critic RL model acting as the basal-bolus advisor. Performance evaluation is conducted in three scenarios, comparing the RL agents to conventional therapy. Evaluation metrics include glucose levels (minimum, maximum, and mean), time spent in different BG ranges, and average daily bolus and basal insulin dosages. Results demonstrate that the RL-based basal-bolus advisor significantly improves glucose control, reducing glycemic variability and increasing time spent within the target range (70-180 mg/dL). Hypoglycemia events are effectively prevented, and severe hyperglycemia events are reduced. The RL approach also leads to a statistically significant reduction in average daily basal insulin dosage compared to conventio
    
[^90]: 评估无监督分离表示学习在基因探索和疾病风险预测中的应用

    Evaluating unsupervised disentangled representation learning for genomic discovery and disease risk prediction. (arXiv:2307.08893v1 [cs.LG])

    [http://arxiv.org/abs/2307.08893](http://arxiv.org/abs/2307.08893)

    本文评估了无监督分离表示学习方法在基因探索和疾病风险预测中的应用，发现使用FactorVAE或beta-VAE相比标准VAE或非变分自动编码器可以显著改善哮喘和慢性阻塞性肺疾病的全基因组显著位点数量、遗传力和多基因风险评分的性能。

    

    高维临床数据由于其在生物库规模数据集中的可访问性和高性能建模技术（尤其是深度学习）的发展，已经成为遗传研究中宝贵的资源。最近的研究表明，由变分自动编码器（VAE）学习的这些临床数据的低维嵌入可以用于全基因组关联研究和多基因风险预测。在这项工作中，我们考虑了多种无监督学习方法，用于在遗传相关研究中学习分离表示，包括自动编码器（autoencoders）、VAE、beta-VAE和FactorVAE。以英国生物库的支气管图谱为例，我们观察到使用FactorVAE或beta-VAE相比标准VAE或非变分自动编码器，可以改善哮喘和慢性阻塞性肺疾病的全基因组显著位点数量、遗传力和多基因风险评分的性能。

    High-dimensional clinical data have become invaluable resources for genetic studies, due to their accessibility in biobank-scale datasets and the development of high performance modeling techniques especially using deep learning. Recent work has shown that low dimensional embeddings of these clinical data learned by variational autoencoders (VAE) can be used for genome-wide association studies and polygenic risk prediction. In this work, we consider multiple unsupervised learning methods for learning disentangled representations, namely autoencoders, VAE, beta-VAE, and FactorVAE, in the context of genetic association studies. Using spirograms from UK Biobank as a running example, we observed improvements in the number of genome-wide significant loci, heritability, and performance of polygenic risk scores for asthma and chronic obstructive pulmonary disease by using FactorVAE or beta-VAE, compared to standard VAE or non-variational autoencoders. FactorVAEs performed effectively across m
    
[^91]: 预测删除动态模型：充分利用机器学习预测，实现零成本（arXiv:2307.08890v1 [cs.DS]）

    The Predicted-Deletion Dynamic Model: Taking Advantage of ML Predictions, for Free. (arXiv:2307.08890v1 [cs.DS])

    [http://arxiv.org/abs/2307.08890](http://arxiv.org/abs/2307.08890)

    本文提出了预测删除动态模型，利用机器学习预测了动态图中边的更新，解决了设计动态算法中的未知更新序列的瓶颈问题。这一模型在实际应用中具有实用价值，在理论上也具有研究价值。

    

    设计高效的动态算法的主要瓶颈是更新序列的未知性。特别是在一些问题，如3-顶点连通性、平面有向图所有点对最短路径等，最佳的部分动态解和最佳的全动态解之间的运行时间差异是多项式的，甚至是指数级的。本文提出了预测删除动态模型，受到近期关于预测动态图中边更新的经验性研究的启发。在该模型中，边在线上被插入和删除，并且当一条边被插入时，它带有一个它删除时间的“预测”。该模型反映了实际世界中的一些情景，其中服务可以访问历史数据或其他关于输入的信息，并可以据此对用户行为进行预测。该模型在理论上也有兴趣，因为它插值了部分动态解和全动态解之间的状态。

    The main bottleneck in designing efficient dynamic algorithms is the unknown nature of the update sequence. In particular, there are some problems, like 3-vertex connectivity, planar digraph all pairs shortest paths, and others, where the separation in runtime between the best partially dynamic solutions and the best fully dynamic solutions is polynomial, sometimes even exponential.  In this paper, we formulate the predicted-deletion dynamic model, motivated by a recent line of empirical work about predicting edge updates in dynamic graphs. In this model, edges are inserted and deleted online, and when an edge is inserted, it is accompanied by a "prediction" of its deletion time. This models real world settings where services may have access to historical data or other information about an input and can subsequently use such information make predictions about user behavior. The model is also of theoretical interest, as it interpolates between the partially dynamic and fully dynamic set
    
[^92]: 研究度分布和同质性在图学习模型中的影响

    Examining the Effects of Degree Distribution and Homophily in Graph Learning Models. (arXiv:2307.08881v1 [cs.SI])

    [http://arxiv.org/abs/2307.08881](http://arxiv.org/abs/2307.08881)

    本研究考察了两种额外的合成图生成器如何改进GraphWorld的评估，通过整合这些生成器，我们扩展了图空间的覆盖范围，同时保持了真实网络中的关键图属性。

    

    尽管人们对GNN的发展兴趣高涨，但在基准数据集的同质化仍然是GNN研究面临的基本问题。GraphWorld是一个最近提出的解决方案，它使用随机块模型(SBM)生成多样的合成图，用于对任何GNN任务进行基准测试。尽管GraphWorld取得了成功，但随机块模型对GraphWorld能够创建的图结构种类施加了基本限制。在这项工作中，我们研究了两个额外的合成图生成器如何改进GraphWorld的评估；LFR是图聚类文献中一个成熟的模型，而CABAM是一个针对GNN基准测试量身定制的Barabasi-Albert模型的最新改进。通过整合这些生成器，我们在GraphWorld框架内显著扩展了图空间的覆盖范围，同时保持了真实网络中观察到的关键图属性。为了证明它们的有效性，我们生成了30万个图来对11个GNN模型进行节点分类任务的基准测试。

    Despite a surge in interest in GNN development, homogeneity in benchmarking datasets still presents a fundamental issue to GNN research. GraphWorld is a recent solution which uses the Stochastic Block Model (SBM) to generate diverse populations of synthetic graphs for benchmarking any GNN task. Despite its success, the SBM imposed fundamental limitations on the kinds of graph structure GraphWorld could create.  In this work we examine how two additional synthetic graph generators can improve GraphWorld's evaluation; LFR, a well-established model in the graph clustering literature and CABAM, a recent adaptation of the Barabasi-Albert model tailored for GNN benchmarking. By integrating these generators, we significantly expand the coverage of graph space within the GraphWorld framework while preserving key graph properties observed in real-world networks. To demonstrate their effectiveness, we generate 300,000 graphs to benchmark 11 GNN models on a node classification task. We find GNN p
    
[^93]: 可调模块化神经网络在手术图像识别中的应用

    Modular Neural Network Approaches for Surgical Image Recognition. (arXiv:2307.08880v1 [cs.CV])

    [http://arxiv.org/abs/2307.08880](http://arxiv.org/abs/2307.08880)

    本论文研究了可调模块化神经网络在手术图像识别中的应用。通过自我训练方法解决了数据不足和标注困难的问题，并采用模块化学习策略降低了复杂性和成本。

    

    近年来，基于深度学习的应用取得了很大的成功。文本、音频、图像和视频都已经通过深度学习方法进行了探索，并且取得了显著的成果。特别是在计算机视觉领域，卷积神经网络（CNN）的应用产生了可靠的结果。然而，为了取得这些结果，需要大量的数据。然而，数据集并不总是可以获取的。此外，对数据进行注释可能会很困难且耗时。自我训练是一种半监督的方法，成功地缓解了这个问题，并达到了最先进的性能。理论分析甚至证明，它可能导致比普通分类器更好的泛化表现。神经网络可能面临的另一个问题是现代问题的复杂性不断增加，需要高计算和存储成本。一种缓解这个问题的方法是采用模块化学习，这是受到人类认知启发的策略。该方法的原理是将应用拆分为多个模块，每个模块负责处理特定的部分。每个模块可以被单独训练和更新，从而降低了整个系统的复杂性。

    Deep learning-based applications have seen a lot of success in recent years. Text, audio, image, and video have all been explored with great success using deep learning approaches. The use of convolutional neural networks (CNN) in computer vision, in particular, has yielded reliable results. In order to achieve these results, a large amount of data is required. However, the dataset cannot always be accessible. Moreover, annotating data can be difficult and time-consuming. Self-training is a semi-supervised approach that managed to alleviate this problem and achieve state-of-the-art performances. Theoretical analysis even proved that it may result in a better generalization than a normal classifier. Another problem neural networks can face is the increasing complexity of modern problems, requiring a high computational and storage cost. One way to mitigate this issue, a strategy that has been inspired by human cognition known as modular learning, can be employed. The principle of the app
    
[^94]: 在图链接预测中区分节点属性和图拓扑以改善泛化能力

    Disentangling Node Attributes from Graph Topology for Improved Generalizability in Link Prediction. (arXiv:2307.08877v1 [cs.LG])

    [http://arxiv.org/abs/2307.08877](http://arxiv.org/abs/2307.08877)

    该论文研究了节点属性和图拓扑之间的相互作用，并且提出了一种新方法UPNA，通过将预训练节点属性纳入模型来改善链接预测的泛化能力。该方法可以解决归纳链接预测问题，并具有优于现有技术的表现。

    

    链接预测是图机器学习中关键的任务，具有各种应用。我们探索了节点属性和图拓扑之间的相互作用，并证明了将预训练节点属性纳入链接预测模型可以提高其泛化能力。我们提出的方法UPNA（节点属性的无监督预训练）通过学习一个函数，该函数接受一对节点属性并预测边的概率，来解决归纳链接预测问题，相对于图神经网络（GNN），GNN在具有幂律度分布的图中容易受到拓扑快捷方式的影响。通过这种方式，UPNA学习到了潜在的图生成机制的重要部分，因为所学函数可以用于将新节点添加到正在增长的图中。通过利用预训练的节点属性，我们克服了观测偏差，并对未观测节点做出有意义的预测，超过了现有技术的表现（在基准上提升了3倍到34倍）。

    Link prediction is a crucial task in graph machine learning with diverse applications. We explore the interplay between node attributes and graph topology and demonstrate that incorporating pre-trained node attributes improves the generalization power of link prediction models. Our proposed method, UPNA (Unsupervised Pre-training of Node Attributes), solves the inductive link prediction problem by learning a function that takes a pair of node attributes and predicts the probability of an edge, as opposed to Graph Neural Networks (GNN), which can be prone to topological shortcuts in graphs with power-law degree distribution. In this manner, UPNA learns a significant part of the latent graph generation mechanism since the learned function can be used to add incoming nodes to a growing graph. By leveraging pre-trained node attributes, we overcome observational bias and make meaningful predictions about unobserved nodes, surpassing state-of-the-art performance (3X to 34X improvement on ben
    
[^95]: 自然演员-评论家算法用于带有函数逼近的鲁棒强化学习

    Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation. (arXiv:2307.08875v1 [cs.LG])

    [http://arxiv.org/abs/2307.08875](http://arxiv.org/abs/2307.08875)

    本文研究了鲁棒强化学习问题，提出了两种新的不确定性集合形式，使得大规模鲁棒强化学习变得可行。同时，提出了一个鲁棒的自然演员-评论家算法，通过函数逼近，该算法能够在有限时间内收敛到最佳鲁棒策略。

    

    本文研究了鲁棒强化学习，在确定一个对训练模拟器和测试环境之间的模型不匹配具有良好性能的策略的目标下。以前基于策略的鲁棒强化学习算法主要关注不确定性集合下的表格设置，该集合便于鲁棒策略评估，但在状态数量增加时变得不可行。为此，我们提出了两种新的不确定性集合形式，一种基于双重抽样，另一种基于积分概率度量。两者都使得即使只能访问模拟器，也能处理大规模的鲁棒强化学习。我们提出了一个鲁棒的自然演员-评论家算法（RNAC），它结合了新的不确定性集合，并使用函数逼近。我们提供了对于这个RNAC算法在有限时间内收敛到最佳鲁棒策略的收敛性保证，考虑函数逼近误差。最后，我们展示了通过我们的方法学习到的策略的鲁棒性能。

    We study robust reinforcement learning (RL) with the goal of determining a well-performing policy that is robust against model mismatch between the training simulator and the testing environment. Previous policy-based robust RL algorithms mainly focus on the tabular setting under uncertainty sets that facilitate robust policy evaluation, but are no longer tractable when the number of states scales up. To this end, we propose two novel uncertainty set formulations, one based on double sampling and the other on an integral probability metric. Both make large-scale robust RL tractable even when one only has access to a simulator. We propose a robust natural actor-critic (RNAC) approach that incorporates the new uncertainty sets and employs function approximation. We provide finite-time convergence guarantees for the proposed RNAC algorithm to the optimal robust policy within the function approximation error. Finally, we demonstrate the robust performance of the policy learned by our propo
    
[^96]: 神经算法推理器的潜在空间表示

    Latent Space Representations of Neural Algorithmic Reasoners. (arXiv:2307.08874v1 [cs.LG])

    [http://arxiv.org/abs/2307.08874](http://arxiv.org/abs/2307.08874)

    这项工作对神经算法推理器中执行算法时产生的潜在空间结构进行了详细分析，并提出了解决两种故障模式的方法。通过使用softmax聚合器解决分辨率丧失问题，以及衰减潜在空间来处理超出范围的值，这些改变在标准CLRS-30基准测试中大多数算法上实现了改进。

    

    神经算法推理（NAR）是一个研究领域，专注于设计能够可靠地捕捉经典计算的神经架构，通常通过学习执行算法来实现。典型的方法是依赖于图神经网络（GNN）架构，它们将输入编码为高维潜在空间，在算法执行期间反复转换。在这项工作中，我们对GNN在执行算法时导致的潜在空间结构进行了详细分析。我们发现了两种可能的故障模式：（i）分辨率丧失，使得难以区分相似的值；（ii）无法处理训练期间未观察到的值范围之外的值。我们提出通过依赖softmax聚合器来解决第一个问题，并建议衰减潜在空间以处理超出范围的值。我们展示了这些变化在使用最先进的方法时，在标准CLRS-30基准测试中大多数算法上的改进。

    Neural Algorithmic Reasoning (NAR) is a research area focused on designing neural architectures that can reliably capture classical computation, usually by learning to execute algorithms. A typical approach is to rely on Graph Neural Network (GNN) architectures, which encode inputs in high-dimensional latent spaces that are repeatedly transformed during the execution of the algorithm. In this work we perform a detailed analysis of the structure of the latent space induced by the GNN when executing algorithms. We identify two possible failure modes: (i) loss of resolution, making it hard to distinguish similar values; (ii) inability to deal with values outside the range observed during training. We propose to solve the first issue by relying on a softmax aggregator, and propose to decay the latent space in order to deal with out-of-range values. We show that these changes lead to improvements on the majority of algorithms in the standard CLRS-30 benchmark when using the state-of-the-art
    
[^97]: 一种风险厌恶策略梯度的方差替代：基尼离差

    An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])

    [http://arxiv.org/abs/2307.08873](http://arxiv.org/abs/2307.08873)

    本研究提出了一种风险厌恶策略梯度的替代方法，通过使用基尼离差来替代方差，缓解了方差方法的局限性，并在实证评估中取得了高回报和低风险的成果。

    

    在风险厌恶的强化学习中，限制策略回报的方差是一种常见选择，因为它具有明确的数学定义和易于解释。传统方法直接限制总回报方差，而最近的方法通过限制每步奖励方差作为代理。本文彻底研究了这些基于方差的方法的局限性，如数字尺度的敏感性和阻碍策略学习，并提出使用替代风险衡量标准——基尼离差。我们研究了这种新风险衡量标准的各种属性，并导出了一种用于最小化基尼离差的策略梯度算法。在风险厌恶可以明确定义的领域进行实证评估时，我们的算法可以缓解基于方差的风险衡量标准的局限性，并在其他策略无法学到合理策略时实现高回报和低风险，以方差和基尼离差度量。

    Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
    
[^98]: 元价值学习：一种带有学习意识的学习通用框架

    Meta-Value Learning: a General Framework for Learning with Learning Awareness. (arXiv:2307.08863v1 [cs.LG])

    [http://arxiv.org/abs/2307.08863](http://arxiv.org/abs/2307.08863)

    元价值学习是一种带有学习意识的学习通用框架，通过分析智能体学习过程的相互作用，使用元价值函数来指导优化，并通过训练神经网络进行逼近，从而提供更可靠的改进方向。

    

    多智能体系统中的梯度学习很困难，因为梯度来自于一个一阶模型，不考虑智能体学习过程之间的相互作用。我们扩展了LOLA的思想，并开发了一种完全通用的基于价值的优化方法。核心思想是一个称为元价值的函数，它在联合策略空间的每个点上，为每个智能体给出其未来优化步骤中目标的折扣总和。我们认为，元价值的梯度比原始目标的梯度更可靠的改进方向，因为元价值来自对优化效果的经验观察。我们展示了如何通过训练神经网络来近似元价值，以沿着智能体沿着元价值梯度的优化轨迹进行TD误差最小化。我们分析了我们方法的行为。

    Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents' learning processes. LOLA (arXiv:1709.04326) accounts for this by differentiating through one step of optimization. We extend the ideas of LOLA and develop a fully-general value-based approach to optimization. At the core is a function we call the meta-value, which at each point in joint-policy space gives for each agent a discounted sum of its objective over future optimization steps. We argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value. We analyze the behavior of our
    
[^99]: 图神经网络的课程学习：一种基于多视角和能力的方法

    Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach. (arXiv:2307.08859v1 [cs.LG])

    [http://arxiv.org/abs/2307.08859](http://arxiv.org/abs/2307.08859)

    本文提出了一种新的图神经网络课程学习方法，通过引入图复杂性形式化和模型能力作为困难度标准，以及考虑样本困难度和模型能力的不同视角进行训练，实现了对细粒度图困难度标准的纳入。

    

    课程学习是一种计划好的学习材料序列，有效的课程学习可以使人类和机器的学习更高效和有效。最近的研究在语言应用中为训练图神经网络开发了有效的数据驱动的课程学习方法。然而，现有的课程学习方法在训练范式中通常只使用单一的困难度标准。在本文中，我们提出了一种新的课程学习视角，通过引入一种建立在图复杂性形式化（作为困难度标准）和模型能力之上的新方法来进行课程学习。该模型包括一个调度方案，通过考虑样本困难度和模型能力的不同视角在训练期间推导出有效的课程。所提出的解决方案在图神经网络的课程学习研究中取得了进展，能够在训练范式中纳入细粒度的图困难度标准。

    A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. E
    
[^100]: 一种适用于推荐系统的可接受的移动一致方法

    An Admissible Shift-Consistent Method for Recommender Systems. (arXiv:2307.08857v1 [cs.IR])

    [http://arxiv.org/abs/2307.08857](http://arxiv.org/abs/2307.08857)

    本研究提出了一种适用于推荐系统的新方法，该方法利用移动一致性约束解决了矩阵/张量完成问题，并满足可接受性准则和公平性定义，同时具有鲁棒性和可证明的性能特性。

    

    在本文中，我们提出了一种新的约束，称为移动一致性，用于解决推荐系统中的矩阵/张量完成问题。我们的方法能够证明保证一些关键的数学属性：（1）满足最近建立的用于推荐系统的可接受性准则；（2）满足一个定义公平性的条件，消除了用户对系统推荐的恶意影响的潜在机会；（3）通过利用可证明的缺失值插补的唯一性，提供了鲁棒性。我们提供了该方法的严格数学描述，包括从矩阵到张量形式的泛化，以便表示和利用用户和产品属性集之间的复杂结构关系。我们认为我们的分析提出了一种结构化方法，可以定义潜在空间投影，以便为机器学习方法建立可证明的性能特性。

    In this paper, we propose a new constraint, called shift-consistency, for solving matrix/tensor completion problems in the context of recommender systems. Our method provably guarantees several key mathematical properties: (1) satisfies a recently established admissibility criterion for recommender systems; (2) satisfies a definition of fairness that eliminates a specific class of potential opportunities for users to maliciously influence system recommendations; and (3) offers robustness by exploiting provable uniqueness of missing-value imputation. We provide a rigorous mathematical description of the method, including its generalization from matrix to tensor form to permit representation and exploitation of complex structural relationships among sets of user and product attributes. We argue that our analysis suggests a structured means for defining latent-space projections that can permit provable performance properties to be established for machine learning methods.
    
[^101]: 图生成的自回归扩散模型

    Autoregressive Diffusion Model for Graph Generation. (arXiv:2307.08849v1 [cs.AI])

    [http://arxiv.org/abs/2307.08849](http://arxiv.org/abs/2307.08849)

    提出了一种自回归扩散模型用于图生成，通过定义节点吸收扩散过程和设计扩散排序网络以及去噪网络，能在离散图空间中高效地生成多样性的高质量图形。

    

    最近，基于扩散的图生成模型在图生成方面取得了有希望的结果。然而，现有的基于扩散的图生成模型大多是一次性的生成模型，它们在去量化的邻接矩阵空间中应用高斯扩散。这种策略可能在模型训练困难、采样速度慢和无法集成约束方面存在问题。我们提出了一种用于图生成的自回归扩散模型。与现有方法不同的是，我们定义了一种在离散图空间中直接进行的节点吸收扩散过程。对于前向扩散，我们设计了一个称为“扩散排序网络”的网络，它从图的拓扑中学习到了数据相关的节点吸收顺序。对于逆向生成，我们设计了一个称为“去噪网络”的网络，它利用逆向节点排序以及之前去噪的节点来高效地重构图，通过预测新节点的类型和边与之前去噪节点的时间。基于置换的图表示，我们提出了一种有效的训练策略，并在多个数据集上进行了实验验证，结果表明我们的模型可以生成具有多样性的高质量图形。

    Diffusion-based graph generative models have recently obtained promising results for graph generation. However, existing diffusion-based graph generative models are mostly one-shot generative models that apply Gaussian diffusion in the dequantized adjacency matrix space. Such a strategy can suffer from difficulty in model training, slow sampling speed, and incapability of incorporating constraints. We propose an \emph{autoregressive diffusion} model for graph generation. Unlike existing methods, we define a node-absorbing diffusion process that operates directly in the discrete graph space. For forward diffusion, we design a \emph{diffusion ordering network}, which learns a data-dependent node absorbing ordering from graph topology. For reverse generation, we design a \emph{denoising network} that uses the reverse node ordering to efficiently reconstruct the graph by predicting the node type of the new node and its edges with previously denoised nodes at a time. Based on the permutatio
    
[^102]: 面向个性化联邦学习的隐私保护患者聚类

    Privacy-preserving patient clustering for personalized federated learning. (arXiv:2307.08847v1 [cs.LG])

    [http://arxiv.org/abs/2307.08847](http://arxiv.org/abs/2307.08847)

    这项研究提出了一种隐私保护的患者聚类方法来解决个性化联邦学习中的非IID数据问题

    

    联邦学习是一种机器学习框架，可以使多个组织在不与中央服务器共享数据的情况下训练模型。然而，如果数据是非独立分布的（非IID），其性能会显著下降。在医疗环境中，患者群体的变化会导致医院之间的分布差异。个性化联邦学习通过考虑特定站点的分布差异来解决这个问题。个性化联邦学习的一种变体——聚类联邦学习，通过将患者分成不同的群组在医院之间进行训练。然而，隐私问题仍然是一个挑战，因为聚类过程需要交换患者级别的信息。之前通过使用聚合数据形成群组来解决这个问题，但会导致群组不准确和性能下降。在本研究中，我们提出了一种隐私保护的患者聚类方法

    Federated Learning (FL) is a machine learning framework that enables multiple organizations to train a model without sharing their data with a central server. However, it experiences significant performance degradation if the data is non-identically independently distributed (non-IID). This is a problem in medical settings, where variations in the patient population contribute significantly to distribution differences across hospitals. Personalized FL addresses this issue by accounting for site-specific distribution differences. Clustered FL, a Personalized FL variant, was used to address this problem by clustering patients into groups across hospitals and training separate models on each group. However, privacy concerns remained as a challenge as the clustering process requires exchange of patient-level information. This was previously solved by forming clusters using aggregated data, which led to inaccurate groups and performance degradation. In this study, we propose Privacy-preserv
    
[^103]: 使用基于贝叶斯安全策略学习的机会约束优化方法：在越南战争期间的军事安全评估中的应用

    Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War. (arXiv:2307.08840v1 [cs.LG])

    [http://arxiv.org/abs/2307.08840](http://arxiv.org/abs/2307.08840)

    本研究提出了使用基于贝叶斯安全策略学习的机会约束优化方法，在越南战争期间的军事安全评估中进行实证研究。研究结果对于解决高风险算法决策中的挑战具有重要意义。

    

    在高风险决策场景，如刑事司法、医学和公共政策中，常常使用算法和数据驱动的决策和建议。本研究探讨了在越南战争期间，是否有可能改进一种安全评估算法，并使用在其引入后立即测量到的结果进行研究。这个实证应用提出了在高风险算法决策中经常遇到的几个方法学挑战。首先，在实施新算法之前，至关重要的是对更糟糕的结果风险进行表征和控制。其次，现有算法是确定性的，学习新算法需要透明的外推。第三，现有算法涉及常见但难以优化的离散决策表。为了应对这些挑战，我们引入了平均条件风险（ACRisk），首先量化了产生较差结果风险的方法。

    Algorithmic and data-driven decisions and recommendations are commonly used in high-stakes decision-making settings such as criminal justice, medicine, and public policy. We investigate whether it would have been possible to improve a security assessment algorithm employed during the Vietnam War, using outcomes measured immediately after its introduction in late 1969. This empirical application raises several methodological challenges that frequently arise in high-stakes algorithmic decision-making. First, before implementing a new algorithm, it is essential to characterize and control the risk of yielding worse outcomes than the existing algorithm. Second, the existing algorithm is deterministic, and learning a new algorithm requires transparent extrapolation. Third, the existing algorithm involves discrete decision tables that are common but difficult to optimize over.  To address these challenges, we introduce the Average Conditional Risk (ACRisk), which first quantifies the risk th
    
[^104]: 基于元学习的速率分割多址(Multiple Access)的预编码优化框架

    A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access. (arXiv:2307.08822v1 [eess.SP])

    [http://arxiv.org/abs/2307.08822](http://arxiv.org/abs/2307.08822)

    本文提出了一种基于元学习的预编码优化框架，通过利用紧凑神经网络过拟合来优化Rate-Splitting Multiple Access (RSMA)预编码，结合部分信道状态信息，从而绕过了其他训练数据的需求，同时在中等规模情况下达到了类似传统方法的平均和速率性能，在大规模情况下显著优于低复杂度算法。

    

    在这篇论文中，我们提出了一种基于元学习的预编码优化框架，来直接优化具有部分发射机信道状态信息的Rate-Splitting Multiple Access (RSMA)预编码。通过利用紧凑神经网络过拟合来最大化明确的平均和速率表达式，我们有效地绕过了对任何其他训练数据的需求，同时最小化了总运行时间。数值结果表明，基于元学习的解决方案在中等规模情况下达到了类似传统预编码优化的平均和速率性能，并在大规模情况下显著优于次优的低复杂度预编码算法。

    In this letter, we propose the use of a meta-learning based precoder optimization framework to directly optimize the Rate-Splitting Multiple Access (RSMA) precoders with partial Channel State Information at the Transmitter (CSIT). By exploiting the overfitting of the compact neural network to maximize the explicit Average Sum-Rate (ASR) expression, we effectively bypass the need for any other training data while minimizing the total running time. Numerical results reveal that the meta-learning based solution achieves similar ASR performance to conventional precoder optimization in medium-scale scenarios, and significantly outperforms sub-optimal low complexity precoder algorithms in the large-scale regime.
    
[^105]: 加速Benders分解方法的强化学习代理模型研究

    Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models. (arXiv:2307.08816v1 [cs.LG])

    [http://arxiv.org/abs/2307.08816](http://arxiv.org/abs/2307.08816)

    本文介绍了一种利用强化学习代理模型加速Benders分解方法的方法，并通过实验证明了其相对于其他加速方案的30%更快的平均收敛速度。

    

    随机优化试图在存在不确定性的情况下提供最优决策。通常，由于需要捕捉不确定性的情景数量以及现实规划问题的离散性质，这些问题的经典形式变得难以处理。为了克服这些可行性问题，实践者们转向分解方法，将问题分解为更小、更易处理的子问题。本文的主要分解方法是Benders分解（BD），它根据情景独立性对随机优化问题进行分解。在本文中，我们提出了一种利用代理模型加速BD的方法，该代理模型取代了NP难的整数主问题。通过加速方法，与其他加速的BD实现相比，我们观察到平均收敛速度提高了30%。我们引入了一个强化学习代理作为替代，并展示了如何使用它来解决随机库存问题。

    Stochastic optimization (SO) attempts to offer optimal decisions in the presence of uncertainty. Often, the classical formulation of these problems becomes intractable due to (a) the number of scenarios required to capture the uncertainty and (b) the discrete nature of real-world planning problems. To overcome these tractability issues, practitioners turn to decomposition methods that divide the problem into smaller, more tractable sub-problems. The focal decomposition method of this paper is Benders decomposition (BD), which decomposes stochastic optimization problems on the basis of scenario independence. In this paper we propose a method of accelerating BD with the aid of a surrogate model in place of an NP-hard integer master problem. Through the acceleration method we observe 30% faster average convergence when compared to other accelerated BD implementations. We introduce a reinforcement learning agent as a surrogate and demonstrate how it can be used to solve a stochastic invent
    
[^106]: 大型语言模型在提取分子相互作用和通路知识方面的比较性能评估

    Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])

    [http://arxiv.org/abs/2307.08813](http://arxiv.org/abs/2307.08813)

    本研究评估了不同大型语言模型在提取分子相互作用和通路知识方面的有效性，并讨论了未来机遇和挑战。

    

    理解蛋白质相互作用和通路知识对于揭示生物系统的复杂性和研究生物功能和复杂疾病的基本机制至关重要。尽管现有的数据库提供了来自文献和其他源的策划生物数据，但它们往往不完整且维护工作繁重，因此需要替代方法。在本研究中，我们提出利用大型语言模型的能力，通过自动从相关科学文献中提取这些知识来解决这些问题。为了实现这个目标，在这项工作中，我们调查了不同大型语言模型在识别蛋白质相互作用、通路和基因调控关系等任务中的有效性。我们对不同模型的性能进行了彻底评估，突出了重要的发现，并讨论了这种方法所面临的未来机遇和挑战。代码和数据集链接可在论文中找到。

    Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and d
    
[^107]: DeepMem: 将机器学习模型用作存储通道及其（误用）应用

    DeepMem: ML Models as storage channels and their (mis-)applications. (arXiv:2307.08811v1 [cs.LG])

    [http://arxiv.org/abs/2307.08811](http://arxiv.org/abs/2307.08811)

    本文提出了将机器学习模型作为存储通道的新视角，通过过度参数化来增加通道容量。通过在训练时嵌入信息，并利用黑盒访问实现信息的存储和提取。

    

    机器学习（ML）模型为了支持通用性和避免过拟合而过度参数化。之前的研究表明，这些额外的参数既可以用于恶意目的（例如，在经过训练的模型中隐藏一个模型），也可以用于有益目的（例如，给模型加上水印）。在本文中，我们提出了一个新的信息论视角；我们将ML模型视为一个存储通道，其容量随着过度参数化而增加。具体而言，我们考虑一个发送方，在训练时将任意信息嵌入模型中，接收方可以通过对部署模型的黑盒访问来提取信息。我们根据可用参数的数量推导出通道容量的上界。然后，我们探索了黑盒写入和读取原语，允许攻击者：（i）通过在发射机端扩充训练数据的方式以优化地将数据存储在模型中，以及（ii）通过查询模型来读取数据。

    Machine learning (ML) models are overparameterized to support generality and avoid overfitting. Prior works have shown that these additional parameters can be used for both malicious (e.g., hiding a model covertly within a trained model) and beneficial purposes (e.g., watermarking a model). In this paper, we propose a novel information theoretic perspective of the problem; we consider the ML model as a storage channel with a capacity that increases with overparameterization. Specifically, we consider a sender that embeds arbitrary information in the model at training time, which can be extracted by a receiver with a black-box access to the deployed model. We derive an upper bound on the capacity of the channel based on the number of available parameters. We then explore black-box write and read primitives that allow the attacker to: (i) store data in an optimized way within the model by augmenting the training data at the transmitter side, and (ii) to read it by querying the model afte
    
[^108]: AI增强模拟指导的操作员指导方法

    Operator Guidance Informed by AI-Augmented Simulations. (arXiv:2307.08810v1 [cs.AI])

    [http://arxiv.org/abs/2307.08810](http://arxiv.org/abs/2307.08810)

    本文介绍了一种基于AI增强模拟指导的操作员指导方法，利用LSTM神经网络估计双峰双向海况下船舶响应统计量。通过对比低保真度和高保真度结果，证明了该方法的有效性。

    

    本文将介绍一种基于多保真度和数据自适应的方法，利用长短期记忆神经网络(LSTM)来估计双峰双向海况下船舶响应统计量。研究将采用快速低保真度的基于体积的工具SimpleCode和更高保真度的工具Large Amplitude Motion Program (LAMP)。训练数据是通过常见的双峰双向海况在北大西洋生成的SimpleCode和LAMP数据。在用LAMP船舶运动响应数据训练LSTM网络后，样本路线被穿过，随机选取历史天气输入SimpleCode和LSTM网络，并与更高保真度的结果进行比较。

    This paper will present a multi-fidelity, data-adaptive approach with a Long Short-Term Memory (LSTM) neural network to estimate ship response statistics in bimodal, bidirectional seas. The study will employ a fast low-fidelity, volume-based tool SimpleCode and a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP). SimpleCode and LAMP data were generated by common bi-modal, bi-directional sea conditions in the North Atlantic as training data. After training an LSTM network with LAMP ship motion response data, a sample route was traversed and randomly sampled historical weather was input into SimpleCode and the LSTM network, and compared against the higher fidelity results.
    
[^109]: 本地或全局：基于有限标签的联邦学习中的选择性知识同化

    Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels. (arXiv:2307.08809v1 [cs.LG])

    [http://arxiv.org/abs/2307.08809](http://arxiv.org/abs/2307.08809)

    本论文提出了FedLabel方法，在联邦学习中，客户端根据数据的专业性选择本地或全局模型对无标签数据进行伪标记，并通过全局-本地一致性正则化来利用本地和全局模型的知识。

    

    许多现有的联邦学习方法假设客户端具有完全标记的数据，而在实际情况下，由于标记过程的昂贵和费力，客户端只有有限的标签。客户端有限的标记本地数据常常导致它们的本地模型对其更大的无标签本地数据具有较差的泛化能力，例如与无标签数据存在类分布不匹配的情况。因此，客户端可能会选择从跨客户端训练的全局模型中受益，以利用他们的无标签数据，但由于客户端之间存在数据异质性，这也变得困难。在我们的工作中，我们提出了FedLabel，客户端根据哪个模型对数据更具专业知识选择本地或全局模型来伪标记其无标签数据。我们进一步通过全局-本地一致性正则化来利用本地和全局模型的知识，当它们对无标签数据具有相同的伪标签时，最小化两个模型的输出之间的差异。

    Many existing FL methods assume clients with fully-labeled data, while in realistic settings, clients have limited labels due to the expensive and laborious process of labeling. Limited labeled local data of the clients often leads to their local model having poor generalization abilities to their larger unlabeled local data, such as having class-distribution mismatch with the unlabeled data. As a result, clients may instead look to benefit from the global model trained across clients to leverage their unlabeled data, but this also becomes difficult due to data heterogeneity across clients. In our work, we propose FedLabel where clients selectively choose the local or global model to pseudo-label their unlabeled data depending on which is more of an expert of the data. We further utilize both the local and global models' knowledge via global-local consistency regularization which minimizes the divergence between the two models' outputs when they have identical pseudo-labels for the unl
    
[^110]: 基于选择性字典学习的异常检测方法

    Anomaly Detection with Selective Dictionary Learning. (arXiv:2307.08807v1 [cs.LG])

    [http://arxiv.org/abs/2307.08807](http://arxiv.org/abs/2307.08807)

    该论文提出了基于选择性字典学习的新的异常检测方法，主要贡献在于将字典学习算法和核字典学习算法改进为无监督方法，并提出了一种适用于大数据集问题的降维核字典学习方法，同时通过随机选择信号的方法改进算法以消除训练过程中的异常值。所有算法均被整合在一个异常检测工具箱中，并与标准基准结果进行了比较。

    

    本文提出了一种基于字典学习（DL）和核字典学习（KDL）的异常检测新方法。主要贡献在于将已知的DL和KDL算法改进为无监督方法，用于离群点检测。我们提出了一种降维的核字典学习方法（RKDL），适用于具有大数据集的问题，避免了大核矩阵的计算。我们还通过随机选择信号的方法改进了DL和RKDL方法，旨在消除训练过程中的异常值。所有算法均被集成在一个异常检测工具箱中，并与标准基准结果进行了比较。

    In this paper we present new methods of anomaly detection based on Dictionary Learning (DL) and Kernel Dictionary Learning (KDL). The main contribution consists in the adaption of known DL and KDL algorithms in the form of unsupervised methods, used for outlier detection. We propose a reduced kernel version (RKDL), which is useful for problems with large data sets, due to the large kernel matrix. We also improve the DL and RKDL methods by the use of a random selection of signals, which aims to eliminate the outliers from the training procedure. All our algorithms are introduced in an anomaly detection toolbox and are compared to standard benchmark results.
    
[^111]: 自动设计核酸开关的方法

    Towards Automated Design of Riboswitches. (arXiv:2307.08801v1 [cs.LG])

    [http://arxiv.org/abs/2307.08801](http://arxiv.org/abs/2307.08801)

    本文提出了一种新的方法libLEARNA，用于自动设计核酸开关。该方法通过考虑全局特性和序列结构特征，能够提供多样性的可变长度合格候选物的RNA关注库。实验表明，我们的方法能够获得30%更多独特高质量的候选物。

    

    实验筛选和选择新型核酸开关的流程成本高、耗时长且效率低下。使用计算方法来减少筛选候选物的数量可以大大降低这些成本。然而，现有的计算方法并不能完全满足设计此类初始筛选库的所有要求。在本研究中，我们提出了一种新方法libLEARNA，能够提供多样性的可变长度合格候选物的RNA关注库。我们的新颖基于结构的设计方法不仅考虑全局特性，还考虑所需的序列和结构特征。我们通过按照先前发表的协议设计茶碱核酸开关库来证明我们方法的优势，并获得了30%更多独特的高质量候选物。

    Experimental screening and selection pipelines for the discovery of novel riboswitches are expensive, time-consuming, and inefficient. Using computational methods to reduce the number of candidates for the screen could drastically decrease these costs. However, existing computational approaches do not fully satisfy all requirements for the design of such initial screening libraries. In this work, we present a new method, libLEARNA, capable of providing RNA focus libraries of diverse variable-length qualified candidates. Our novel structure-based design approach considers global properties as well as desired sequence and structure features. We demonstrate the benefits of our method by designing theophylline riboswitch libraries, following a previously published protocol, and yielding 30% more unique high-quality candidates.
    
[^112]: regulAS:一种用于使用RNA-Seq数据进行选择性剪切调控组学综合分析的生物信息学工具

    regulAS: A Bioinformatics Tool for the Integrative Analysis of Alternative Splicing Regulome using RNA-Seq data. (arXiv:2307.08800v1 [q-bio.GN])

    [http://arxiv.org/abs/2307.08800](http://arxiv.org/abs/2307.08800)

    regulAS是一个用于从RNA-Seq数据中研究选择性剪切调控机制的生物信息学工具，它可以自动化计算实验、高效处理结果、优化工作流程，并具有基于公共数据仓库的数据检索、预测建模和灵活报告生成等功能。

    

    regulAS软件包是一个生物信息学工具，旨在支持计算生物学研究人员通过综合分析来自TCGA和GTEx项目的癌症和健康人类供体的大规模RNA-Seq数据，以研究剪切改变的调控机制。这篇技术报告全面介绍了regulAS，重点介绍了其核心功能、基本模块、实验配置、进一步可扩展性和自定义性。regulAS的核心功能包括自动化计算实验、高效结果存储和处理以及流程管理。集成的基本模块通过使用scikit-learn包从公共的多组学UCSC Xena数据仓库检索RNA-Seq数据，具有预测建模和特征排序能力，并可生成灵活的报告，用于分析基因表达谱和相关的选择性剪切改变。

    The regulAS software package is a bioinformatics tool designed to support computational biology researchers in investigating regulatory mechanisms of splicing alterations through integrative analysis of large-scale RNA-Seq data from cancer and healthy human donors, characterized by TCGA and GTEx projects. This technical report provides a comprehensive overview of regulAS, focusing on its core functionality, basic modules, experiment configuration, further extensibility and customisation.  The core functionality of regulAS enables the automation of computational experiments, efficient results storage and processing, and streamlined workflow management. Integrated basic modules extend regulAS with features such as RNA-Seq data retrieval from the public multi-omics UCSC Xena data repository, predictive modeling and feature ranking capabilities using the scikit-learn package, and flexible reporting generation for analysing gene expression profiles and relevant modulations of alternative sp
    
[^113]: 降维核字典学习

    Reduced Kernel Dictionary Learning. (arXiv:2307.08798v1 [eess.SP])

    [http://arxiv.org/abs/2307.08798](http://arxiv.org/abs/2307.08798)

    本文提出了一种降维核字典学习方法，通过输入信号的训练稀疏表示来获得核向量，并使用梯度下降步骤直接优化核向量，实验证明该方法能够在使用少量核向量的情况下提供更好的表示，并减少执行时间。

    

    在本文中，我们提出了一种用于训练核字典学习(KDL)问题中的降维非线性表示的新算法。标准的KDL存在一个问题，那就是当数据集很大时，核矩阵的大小很大。有几种减小核大小的方法，尤其是Nyström抽样。我们在这里提出了一种更符合字典学习思想的方法，其中核向量是通过输入信号的训练稀疏表示来获得的。此外，我们还直接优化了KDL过程中的核向量，并使用梯度下降步骤。我们通过三个数据集展示了我们的算法能够提供更好的表示，尽管使用了少量的核向量，同时相对于KDL还减少了执行时间。

    In this paper we present new algorithms for training reduced-size nonlinear representations in the Kernel Dictionary Learning (KDL) problem. Standard KDL has the drawback of a large size of the kernel matrix when the data set is large. There are several ways of reducing the kernel size, notably Nystr\"om sampling. We propose here a method more in the spirit of dictionary learning, where the kernel vectors are obtained with a trained sparse representation of the input signals. Moreover, we optimize directly the kernel vectors in the KDL process, using gradient descent steps. We show with three data sets that our algorithms are able to provide better representations, despite using a small number of kernel vectors, and also decrease the execution time with respect to KDL.
    
[^114]: 基于非相干核字典学习的分类方法

    Classification with Incoherent Kernel Dictionary Learning. (arXiv:2307.08796v1 [cs.LG])

    [http://arxiv.org/abs/2307.08796](http://arxiv.org/abs/2307.08796)

    本文提出了一种基于非相干核字典学习的分类方法，通过改进标准线性对应物，实现了非相干字典学习的核版本，并对表示更新算法进行了改进。

    

    本文提出了一种基于字典学习的新分类方法。主要贡献是通过对其标准线性对应物进行改进，提出了非相干字典学习的核版本。我们还对AK-SVD算法的表示更新进行了改进。我们的算法在多个常用的分类问题数据库上进行了测试。

    In this paper we present a new classification method based on Dictionary Learning (DL). The main contribution consists of a kernel version of incoherent DL, derived from its standard linear counterpart. We also propose an improvement of the AK-SVD algorithm concerning the representation update. Our algorithms are tested on several popular databases of classification problems.
    
[^115]: 多时间尺度多智能体强化学习中的非平稳策略学习

    Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning. (arXiv:2307.08794v1 [cs.LG])

    [http://arxiv.org/abs/2307.08794](http://arxiv.org/abs/2307.08794)

    这篇论文提出了一个简单的框架，用于在多时间尺度多智能体强化学习中学习非平稳策略。他们利用智能体时间尺度的信息定义了周期性时间编码，并通过周期性多智能体策略学习多时间尺度引入的非平稳性的效果。他们还提出了一个使用神经网络的策略梯度算法来学习这些策略。

    

    在多时间尺度多智能体强化学习中，智能体在不同的时间尺度上进行交互。一般来说，对于受多时间尺度引起的时间依赖行为，策略是非平稳的。学习非平稳策略是具有挑战性的，往往需要复杂或低效的算法。由于现实世界复杂系统中控制问题的普遍存在，我们引入了一个简单的框架来学习多时间尺度多智能体强化学习中的非平稳策略。我们的方法利用关于智能体时间尺度的可用信息来定义周期性时间编码。具体而言，我们从理论上证明了通过周期性多智能体策略可以学习多时间尺度引入的非平稳性的效果。为了学习这样的策略，我们提出了一个策略梯度算法，该算法使用相位函数化的神经网络来参数化演员和评论家，为周期性提供归纳偏置。

    In multi-timescale multi-agent reinforcement learning (MARL), agents interact across different timescales. In general, policies for time-dependent behaviors, such as those induced by multiple timescales, are non-stationary. Learning non-stationary policies is challenging and typically requires sophisticated or inefficient algorithms. Motivated by the prevalence of this control problem in real-world complex systems, we introduce a simple framework for learning non-stationary policies for multi-timescale MARL. Our approach uses available information about agent timescales to define a periodic time encoding. In detail, we theoretically demonstrate that the effects of non-stationarity introduced by multiple timescales can be learned by a periodic multi-agent policy. To learn such policies, we propose a policy gradient algorithm that parameterizes the actor and critic with phase-functioned neural networks, which provide an inductive bias for periodicity. The framework's ability to effective
    
[^116]: 无监督学习分布特性可以辅助人工标注并提高异常检测中的主动学习效率

    Unsupervised Learning of Distributional Properties can Supplement Human Labeling and Increase Active Learning Efficiency in Anomaly Detection. (arXiv:2307.08782v1 [cs.LG])

    [http://arxiv.org/abs/2307.08782](http://arxiv.org/abs/2307.08782)

    通过无监督学习分布特性进行主动学习可以提高异常检测的效率，并辅助人工标注来减少虚警数量，特别是在罕见案例的检测中。

    

    数据通过电子邮件的泄露对许多组织来说是一个严重的网络安全威胁。检测数据泄露（异常）模式通常需要人工标注来减少虚警的数量。主动学习是一种能够有效标注数据的方法，但它需要选择一个有效的顺序来标注案例，并且在确定用于优先标注案例的评分过程时存在不确定性，特别是在关键时刻检测罕见的案例的情况下。我们提出了一种自适应主动学习采样策略，利用基础先前数据分布和模型的不确定性，产生批次的案例以进行标注，其中包含罕见异常的实例。我们证明了：（1）分类器从具有代表性和信息丰富的正常与异常实例的批次中受益，（2）无监督异常检测在构建分类器中发挥了有用的作用。

    Exfiltration of data via email is a serious cybersecurity threat for many organizations. Detecting data exfiltration (anomaly) patterns typically requires labeling, most often done by a human annotator, to reduce the high number of false alarms. Active Learning (AL) is a promising approach for labeling data efficiently, but it needs to choose an efficient order in which cases are to be labeled, and there are uncertainties as to what scoring procedure should be used to prioritize cases for labeling, especially when detecting rare cases of interest is crucial. We propose an adaptive AL sampling strategy that leverages the underlying prior data distribution, as well as model uncertainty, to produce batches of cases to be labeled that contain instances of rare anomalies. We show that (1) the classifier benefits from a batch of representative and informative instances of both normal and anomalous examples, (2) unsupervised anomaly detection plays a useful role in building the classifier in 
    
[^117]: 改进语言模型在数学问题上的性能的混合策略

    A mixed policy to improve performance of language models on math problems. (arXiv:2307.08767v1 [cs.CL])

    [http://arxiv.org/abs/2307.08767](http://arxiv.org/abs/2307.08767)

    本文提出了一种混合策略的探索方法，利用强化学习来改进语言模型在数学问题上的性能，通过在抽象层和第二层采用不同的探索方式，取得了超过2%的性能增益。

    

    在解决数学问题时，大多数语言模型采用采样策略根据条件概率预测下一个词。在数学推理过程中，可能会生成错误的答案。考虑到数学问题是确定性的，我们提出了一种混合策略的探索方法来解决数学问题，利用强化学习。我们提出了一个两级标记探索策略：抽象层以概率采样来决定下一个标记是运算符还是操作数，而第二层则以贪婪方式选择得分最高的下一个标记。我们使用GPT-2模型在GSM8K数据集上测试了我们的方法，并展示了超过2％的性能增益。我们的实现代码可在https://github.com/vividitytech/math_lm_rl找到。

    When to solve math problems, most language models take a sampling strategy to predict next word according conditional probabilities. In the math reasoning step, it may generate wrong answer. Considering math problems are deterministic, we propose a mixed policy exploration approach to solve math problems with reinforcement learning. In peculiar, we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic. Specifically, the abstract level policy will decide whether the token is operator or operand with probability sampling, while the second level is deterministic to select next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than $2\%$ performance gain. Our implementation is available at https://github.com/vividitytech/math_lm_rl.
    
[^118]: 使用可穿戴设备监测心血管生物标志物的光电容抗信号质量评估

    Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices. (arXiv:2307.08766v1 [cs.LG])

    [http://arxiv.org/abs/2307.08766](http://arxiv.org/abs/2307.08766)

    该研究基于机器学习模型对光电容抗（PPG）信号进行训练，评估了使用可穿戴设备进行连续监测时的准确性和可靠性，并提出了解决干扰因素的方法。

    

    光电容抗（PPG）是一种非侵入性技术，用于测量微血管组织中的血容量变化。它常被用于医疗设备，如脉搏血氧仪和手腕式心率监测器，用于监测心血管血液动力学。PPG可以评估心率、脉搏波形和外周灌注等参数，以指示血管收缩或扩张等情况，并提供有关微血管血流的信息，是监测心血管健康的宝贵工具。然而，PPG受到多种变化源的影响，尤其在使用可穿戴设备进行连续监测时，如运动伪影、皮肤色素和血管运动。在这项研究中，我们从PPG信号中提取了27个统计特征，用基于梯度提升（XGBoost和CatBoost）和随机森林（RF）算法的机器学习模型进行训练，以评估其准确性和可靠性。

    Photoplethysmography (PPG) is a non-invasive technology that measures changes in blood volume in the microvascular bed of tissue. It is commonly used in medical devices such as pulse oximeters and wrist worn heart rate monitors to monitor cardiovascular hemodynamics. PPG allows for the assessment of parameters (e.g., heart rate, pulse waveform, and peripheral perfusion) that can indicate conditions such as vasoconstriction or vasodilation, and provides information about microvascular blood flow, making it a valuable tool for monitoring cardiovascular health. However, PPG is subject to a number of sources of variations that can impact its accuracy and reliability, especially when using a wearable device for continuous monitoring, such as motion artifacts, skin pigmentation, and vasomotion. In this study, we extracted 27 statistical features from the PPG signal for training machine-learning models based on gradient boosting (XGBoost and CatBoost) and Random Forest (RF) algorithms to asse
    
[^119]: 条件归一化流的新颖应用：使用陀螺年龄法推断恒星年龄

    A Novel Application of Conditional Normalizing Flows: Stellar Age Inference with Gyrochronology. (arXiv:2307.08753v1 [astro-ph.SR])

    [http://arxiv.org/abs/2307.08753](http://arxiv.org/abs/2307.08753)

    这项工作展示了使用条件归一化流将数据驱动方法应用于光度数据，可以有效约束陀螺年龄法推断恒星年龄的精确性，并且在贝叶斯框架下得到了文献值的良好恢复。这一概率数据驱动解决方案有望扩大陀螺年龄法的适用范围。

    

    恒星年龄是进化模型的关键组成部分，但对于低质量主序星而言，测量起来具有挑战性。在这个范围内，尚未探索的解决方案是将概率机器学习方法应用于陀螺年龄学，这是一种非常适合这些恒星的恒星约会技术。虽然精确的分析陀螺年龄模型的开发仍然具有挑战性，但我们在这里将条件归一化流应用于开放星团的光度数据，并证明数据驱动方法可以与其他标准技术相比约束陀螺年龄。我们在贝叶斯框架下评估流动结果，并展示我们推断的年龄与文献值较为吻合。这项工作展示了概率数据驱动解决方案扩大陀螺年龄恒星约会的潜力。

    Stellar ages are critical building blocks of evolutionary models, but challenging to measure for low mass main sequence stars. An unexplored solution in this regime is the application of probabilistic machine learning methods to gyrochronology, a stellar dating technique that is uniquely well suited for these stars. While accurate analytical gyrochronological models have proven challenging to develop, here we apply conditional normalizing flows to photometric data from open star clusters, and demonstrate that a data-driven approach can constrain gyrochronological ages with a precision comparable to other standard techniques. We evaluate the flow results in the context of a Bayesian framework, and show that our inferred ages recover literature values well. This work demonstrates the potential of a probabilistic data-driven solution to widen the applicability of gyrochronological stellar dating.
    
[^120]: 在存在数据集偏差的情况下，对KNN的公平性进行认证

    Certifying the Fairness of KNN in the Presence of Dataset Bias. (arXiv:2307.08722v1 [cs.LG])

    [http://arxiv.org/abs/2307.08722](http://arxiv.org/abs/2307.08722)

    本文提出了一种方法，用于在存在数据集偏差的情况下对KNN的公平性进行认证。通过近似计算方法，我们实现了在抽象域中计算的方式，从而降低了计算成本，并在多个数据集上进行了实验证明方法的有效性。

    

    我们提出了一种方法，用于在训练数据可能存在历史偏差的情况下，对广泛使用的有监督学习算法KNN的分类结果的公平性进行认证。该方法基于三种公平性定义的变体：个体公平性、ε-公平性和标签翻转公平性。我们首先定义了KNN的公平性认证问题，然后提出了在最先进的KNN算法中使用的复杂算术计算的可靠近似。这旨在将计算结果从具体域提升到抽象域，以降低计算成本。通过在公平研究文献中广泛使用的六个数据集上进行实验评估，我们展示了这种基于抽象解释技术的方法的有效性。

    We propose a method for certifying the fairness of the classification result of a widely used supervised learning algorithm, the k-nearest neighbors (KNN), under the assumption that the training data may have historical bias caused by systematic mislabeling of samples from a protected minority group. To the best of our knowledge, this is the first certification method for KNN based on three variants of the fairness definition: individual fairness, $\epsilon$-fairness, and label-flipping fairness. We first define the fairness certification problem for KNN and then propose sound approximations of the complex arithmetic computations used in the state-of-the-art KNN algorithm. This is meant to lift the computation results from the concrete domain to an abstract domain, to reduce the computational cost. We show effectiveness of this abstract interpretation based technique through experimental evaluation on six datasets widely used in the fairness research literature. We also show that the m
    
[^121]: 直觉模糊广泛学习系统：增强对噪声和异常值的稳健性

    Intuitionistic Fuzzy Broad Learning System: Enhancing Robustness Against Noise and Outliers. (arXiv:2307.08713v1 [cs.LG])

    [http://arxiv.org/abs/2307.08713](http://arxiv.org/abs/2307.08713)

    该论文提出了一种直觉模糊广泛学习系统（IF-BLS），用于增强对噪声和异常值的稳健性，通过为每个训练点分配模糊隶属度来减小噪声和异常值的影响。

    

    在数据分类领域，广泛学习系统（BLS）已被证明是一种有效的工具，它利用逐层前馈神经网络。它包括特征学习和增强部分，共同从输入数据中提取复杂的特征。传统的BLS将所有样本视为同等重要，这使得它在处理带有噪声和异常值的真实数据集时不够稳健和有效。为了解决这个问题，我们提出了模糊BLS（F-BLS）模型，它为每个训练点分配模糊隶属度，以减小噪声和异常值的影响。在分配隶属度的过程中，F-BLS模型仅考虑样本到类别中心之间的距离，而不考虑不属于类别的程度。我们进一步提出了一种基于直觉模糊理论的新型BLS（IF-BLS）。所提出的IF-BLS利用基于模糊隶属度和非隶属度的直觉模糊数。

    In the realm of data classification, broad learning system (BLS) has proven to be a potent tool that utilizes a layer-by-layer feed-forward neural network. It consists of feature learning and enhancement segments, working together to extract intricate features from input data. The traditional BLS treats all samples as equally significant, which makes it less robust and less effective for real-world datasets with noises and outliers. To address this issue, we propose the fuzzy BLS (F-BLS) model, which assigns a fuzzy membership value to each training point to reduce the influence of noises and outliers. In assigning the membership value, the F-BLS model solely considers the distance from samples to the class center in the original feature space without incorporating the extent of non-belongingness to a class. We further propose a novel BLS based on intuitionistic fuzzy theory (IF-BLS). The proposed IF-BLS utilizes intuitionistic fuzzy numbers based on fuzzy membership and non-membership
    
[^122]: 机器学习遇上心理训练 - 应用于记忆运动的概念证明

    Machine Learning Meets Mental Training -- A Proof of Concept Applied to Memory Sports. (arXiv:2307.08712v1 [cs.LG])

    [http://arxiv.org/abs/2307.08712](http://arxiv.org/abs/2307.08712)

    本文介绍了将机器学习与记忆运动的心理训练相结合的实际应用，旨在推动这个看似被低估的运动的发展。

    

    本文旨在通过展示机器学习在记忆艺术的特殊形式（被称为“记忆运动”）的心理训练中的实际应用，将这两个领域结合在一起。这种融合一方面旨在提高对这两个领域的认识，另一方面也力图通过这一混合领域的研究来推动这个看似被低估的运动的发展。

    This work aims to combine these two fields together by presenting a practical implementation of machine learning to the particular form of mental training that is the art of memory, taken in its competitive version called "Memory Sports". Such a fusion, on the one hand, strives to raise awareness about both realms, while on the other it seeks to encourage research in this mixed field as a way to, ultimately, drive forward the development of this seemingly underestimated sport.
    
[^123]: 量子回归的高效强多项式算法

    Efficient Strongly Polynomial Algorithms for Quantile Regression. (arXiv:2307.08706v1 [cs.CG])

    [http://arxiv.org/abs/2307.08706](http://arxiv.org/abs/2307.08706)

    本文提出了针对量子回归的高效强多项式算法，填补了弱多项式算法的空白。对于二维QR，提出了一个具有确定性最坏时间复杂度为O(n^{4/3} polylog(n))和期望时间复杂度为O(n^{4/3})的算法。

    

    线性回归是统计学和机器学习中的一种重要技术，其目标是建立响应变量（即依赖变量）和一个或多个预测变量（即自变量）之间的线性预测模型。本文重新审视了经典的分位数回归（QR）技术，这是一种在统计学中相对于最小二乘回归（OLS）更鲁棒的替代技术。然而，虽然存在有效的OLS算法，但几乎所有已知的QR结果都只是弱多项式。为了填补这一空白，本文针对不同的设置提出了几种高效的强多项式QR算法。对于二维QR，通过与几何概念中的k-集合建立联系，我们提出了一个具有确定性最坏时间复杂度为O(n^{4/3} polylog(n))和期望时间复杂度为O(n^{4/3})的算法（随机化版本）。我们还提出了一种随机划分的方法。

    Linear Regression is a seminal technique in statistics and machine learning, where the objective is to build linear predictive models between a response (i.e., dependent) variable and one or more predictor (i.e., independent) variables. In this paper, we revisit the classical technique of Quantile Regression (QR), which is statistically a more robust alternative to the other classical technique of Ordinary Least Square Regression (OLS). However, while there exist efficient algorithms for OLS, almost all of the known results for QR are only weakly polynomial. Towards filling this gap, this paper proposes several efficient strongly polynomial algorithms for QR for various settings. For two dimensional QR, making a connection to the geometric concept of $k$-set, we propose an algorithm with a deterministic worst-case time complexity of $\mathcal{O}(n^{4/3} polylog(n))$ and an expected time complexity of $\mathcal{O}(n^{4/3})$ for the randomized version. We also propose a randomized divide
    
[^124]: 一个用于参数估计因果效应的R软件包

    An R package for parametric estimation of causal effects. (arXiv:2307.08686v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2307.08686](http://arxiv.org/abs/2307.08686)

    CausalModels是一个用于参数估计因果效应的R软件包，提供了一种简单易懂的框架，可以在一个单独的软件包中使用多种统计方法来估计因果效应。

    

    本文介绍了R软件包CausalModels的用法，该软件包可以在Comprehensive R Archive Network上公开获取。虽然有一些软件包可以足够准确地估计因果效应，但缺乏一个使用Hernan和Robins（2020）传统统计方法开发的结构模型集合来提供。CausalModels通过提供方法工具来解决R中有关因果推断的软件不足，可以在不需要大量统计知识的情况下考虑观察数据中的偏差。这些方法不容忽视，可能在解决特定问题时更合适或更高效。虽然这些统计模型的实现分布在许多因果软件包中，但CausalModels在一个单独的R软件包中引入了一个简单易懂的框架，用于统一建模流程，以估计因果效应的各种统计方法。它包含常见的方法，包括...

    This article explains the usage of R package CausalModels, which is publicly available on the Comprehensive R Archive Network. While packages are available for sufficiently estimating causal effects, there lacks a package that provides a collection of structural models using the conventional statistical approach developed by Hernan and Robins (2020). CausalModels addresses this deficiency of software in R concerning causal inference by offering tools for methods that account for biases in observational data without requiring extensive statistical knowledge. These methods should not be ignored and may be more appropriate or efficient in solving particular problems. While implementations of these statistical models are distributed among a number of causal packages, CausalModels introduces a simple and accessible framework for a consistent modeling pipeline among a variety of statistical methods for estimating causal effects in a single R package. It consists of common methods including s
    
[^125]: TableGPT：将表格，自然语言和命令统一到一个GPT中

    TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT. (arXiv:2307.08674v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.08674](http://arxiv.org/abs/2307.08674)

    TableGPT是一个统一的框架，利用大型语言模型（LLMs）和外部功能命令使LLMs能够无缝地与表格进行交互，实现广泛的功能，并提供便利和可访问性给用户。其中的创新是全局表格表示的概念，使LLMs能够全面理解表格的结构和内容。

    

    表格在现实世界的数据库中非常普遍，需要人们花费大量时间和精力进行分析和操作。大型语言模型（LLMs）的进步使得使用自然语言输入与表格交互成为可能，使得这种能力更加接近现实。本文介绍了TableGPT，这是一个统一的精调框架，使得LLMs能够利用外部功能命令理解和操作表格。它引入了与表格无缝交互的能力，实现了广泛的功能，如问答、数据操作（例如插入、删除、查询和修改操作）、数据可视化、分析报告生成和自动预测。TableGPT旨在通过使用户能够轻松利用表格数据来提供便利和可访问性。TableGPT的核心是全局表格表示的新概念，它使LLMs能够全面理解表格的结构和内容，并将自然语言和命令操作对表格实现无缝集成。

    Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the ent
    
[^126]: 重新审视最小错误熵准则的鲁棒性：转移学习案例研究

    Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study. (arXiv:2307.08572v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08572](http://arxiv.org/abs/2307.08572)

    本研究重新审视了最小错误熵准则在处理非高斯噪声中的鲁棒性，并探讨了其在实际转移学习回归任务中的可行性和有用性。实验证明，在基本转移学习算法中，通过用最小错误熵代替均方误差损失，可以取得与现有方法相媲美的性能表现。

    

    应对数据分布转移是转移学习方法的重要组成部分，以便在实际任务中表现出色。然而，现有方法要么假设数据不包含噪声，要么采用复杂的训练范式或模型设计来处理数据分布转移。本文重新审视了在统计信号处理中广泛使用的最小错误熵（MEE）准则的鲁棒性，并研究其在实际转移学习回归任务中的可行性和有用性，其中分布转移是常见的。具体来说，我们提出了一个新的理论结果，展示了MEE对协变量转移的鲁棒性。我们还表明，通过简单地将均方误差（MSE）损失替换为MEE，在基本的转移学习算法（如微调和线性探测）中，我们可以获得与现有方法竞争力相当的性能。

    Coping with distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-
    
[^127]: 具有被动光学非线性映射的深度学习

    Deep Learning with Passive Optical Nonlinear Mapping. (arXiv:2307.08558v2 [physics.optics] UPDATED)

    [http://arxiv.org/abs/2307.08558](http://arxiv.org/abs/2307.08558)

    这项研究介绍了一种利用反射腔中的多次散射通过被动诱导光学非线性映射的设计，实现了光学数据压缩和高效处理的能力。

    

    深度学习已经从根本上改变了人工智能，但是日益复杂的深度学习模型需要专门的硬件加速器。光学加速器可以提供增强的性能、可扩展性和能量效率。然而，实现光学中的非线性映射，这是神经网络的重要组成部分，仍然具有挑战性。在这里，我们介绍了一种设计，利用一个反射腔中的多次散射来被动诱导光学非线性随机映射，而无需额外的激光功率。我们工作的一个重要优势是我们展示了可以通过反射腔中的多次散射来进行光学数据压缩，以高效地压缩和保留重要信息，同时降低数据的维度。这使得快速的光学信息处理和生成高度非线性特征的低维混合成分成为可能。这对于需要高效处理的应用特别有用。

    Deep learning has fundamentally transformed artificial intelligence, but the ever-increasing complexity in deep learning models calls for specialized hardware accelerators. Optical accelerators can potentially offer enhanced performance, scalability, and energy efficiency. However, achieving nonlinear mapping, a critical component of neural networks, remains challenging optically. Here, we introduce a design that leverages multiple scattering in a reverberating cavity to passively induce optical nonlinear random mapping, without the need for additional laser power. A key advantage emerging from our work is that we show we can perform optical data compression, facilitated by multiple scattering in the cavity, to efficiently compress and retain vital information while also decreasing data dimensionality. This allows rapid optical information processing and generation of low dimensional mixtures of highly nonlinear features. These are particularly useful for applications demanding high-sp
    
[^128]: 多类别点云补全网络用于从cine磁共振图像中重建3D心脏解剖结构

    Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images. (arXiv:2307.08535v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2307.08535](http://arxiv.org/abs/2307.08535)

    本文提出了一种多类别点云补全网络，能够从cine磁共振图像中重建多类别的3D心脏解剖结构。该网络能够解决重建任务中的稀疏和错位问题，并在合成数据集上取得了良好的重建效果。

    

    Cine磁共振成像是评估心脏解剖和功能的当前标准。然而，它通常只获取一组二维切片，限制了对健康和病理心脏形态和生理的理解和分析。本文提出了一种新颖的完全自动表面重建流程，能够从原始cine磁共振成像中重建多类别的3D心脏解剖网格。其关键组件是一个多类别点云补全网络(PCCN)，能够在统一模型中纠正3D重建任务中的稀疏和错位问题。我们首先在大型合成数据集上评估了PCCN，观察到重建和黄金标准解剖之间的Chamfer距离小于或等于基础图像分辨率的多个层次的切片错位。

    Cine magnetic resonance imaging (MRI) is the current gold standard for the assessment of cardiac anatomy and function. However, it typically only acquires a set of two-dimensional (2D) slices of the underlying three-dimensional (3D) anatomy of the heart, thus limiting the understanding and analysis of both healthy and pathological cardiac morphology and physiology. In this paper, we propose a novel fully automatic surface reconstruction pipeline capable of reconstructing multi-class 3D cardiac anatomy meshes from raw cine MRI acquisitions. Its key component is a multi-class point cloud completion network (PCCN) capable of correcting both the sparsity and misalignment issues of the 3D reconstruction task in a unified model. We first evaluate the PCCN on a large synthetic dataset of biventricular anatomies and observe Chamfer distances between reconstructed and gold standard anatomies below or similar to the underlying image resolution for multiple levels of slice misalignment. Furthermo
    
[^129]: 非线性处理与线性光学

    Nonlinear Processing with Linear Optics. (arXiv:2307.08533v2 [physics.optics] UPDATED)

    [http://arxiv.org/abs/2307.08533](http://arxiv.org/abs/2307.08533)

    该论文提出了一种利用多次散射实现多层光学网络的新框架，可以以低光功率同时合成线性和非线性转换，实现能量高效和高速的光学实现神经网络。

    

    深度神经网络通过利用多层数据处理来提取隐藏的表征，取得了显着的突破，但却以大电子计算能力为代价。为了提高能量效率和速度，光学实现神经网络的目标是利用光学带宽的优势和光学互连的能量效率。在缺乏低功率光学非线性性的情况下，在实现多层光学网络中的挑战在于实现多个光学层，而不依赖电子元件。在本研究中，我们提出了一个新颖的框架，利用多次散射可以同时以低光功率合成可编程的线性和非线性转换，利用散射势能（由数据表示）与散射场之间的非线性关系。理论和实验研究表明，通过多次散射进行数据重复可以实现多个光学层。

    Deep neural networks have achieved remarkable breakthroughs by leveraging multiple layers of data processing to extract hidden representations, albeit at the cost of large electronic computing power. To enhance energy efficiency and speed, the optical implementation of neural networks aims to harness the advantages of optical bandwidth and the energy efficiency of optical interconnections. In the absence of low-power optical nonlinearities, the challenge in the implementation of multilayer optical networks lies in realizing multiple optical layers without resorting to electronic components. In this study, we present a novel framework that uses multiple scattering that is capable of synthesizing programmable linear and nonlinear transformations concurrently at low optical power by leveraging the nonlinear relationship between the scattering potential, represented by data, and the scattered field. Theoretical and experimental investigations show that repeating the data by multiple scatte
    
[^130]: 使用神经网络在气体绝缘高压直流系统中对超高频局部放电信号进行普适分类

    Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks. (arXiv:2307.08466v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08466](http://arxiv.org/abs/2307.08466)

    本论文提出了一种基于神经网络的方法，用于在气体绝缘高压直流系统中普遍分类超高频局部放电信号。此方法可以区分不同的PD源，并且可以推广到未见过的操作电压倍数。同时，比较了时域和频域输入信号的性能，以及不同的归一化方案对减轻自由空间影响的影响。

    

    未被发现的局部放电(PD)是高压(HV)气体绝缘系统(GIS)中的一个安全关键问题。虽然在交流电压下对PD的诊断已经得到了很好的处理，但在直流电压下的PD分析仍然是一个活跃的研究领域。这些研究的重点之一是将不同的PD源进行分类，以便进行后续的复杂分析。在本文中，我们提出并分析了一种基于神经网络的方法，用于对HVDC GIS上绝缘体上的金属突起和导电颗粒引起的PD信号进行分类，而不依赖于脉冲序列分析特征。与之前的方法不同，我们提出的模型可以区分负电位和正电位下获得的研究PD信号，并且能够推广到未见过的操作电压倍数。此外，我们比较了时域和频域输入信号的性能，并研究了不同的归一化方案对减轻自由空间影响的影响。

    Undetected partial discharges (PDs) are a safety critical issue in high voltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under AC voltage is well-established, the analysis of PDs under DC voltage remains an active research field. A key focus of these investigations is the classification of different PD sources to enable subsequent sophisticated analysis.  In this paper, we propose and analyze a neural network-based approach for classifying PD signals caused by metallic protrusions and conductive particles on the insulator of HVDC GIS, without relying on pulse sequence analysis features. In contrast to previous approaches, our proposed model can discriminate the studied PD signals obtained at negative and positive potentials, while also generalizing to unseen operating voltage multiples. Additionally, we compare the performance of time- and frequency-domain input signals and explore the impact of different normalization schemes to mitigate the influence of free-space
    
[^131]: 从随机游走到图形快跑：一种在连续时间动态图上具有低延迟的节点嵌入框架

    From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs. (arXiv:2307.08433v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08433](http://arxiv.org/abs/2307.08433)

    这篇论文介绍了一种在连续时间动态图上具有低延迟的节点嵌入框架，通过提出流式低延迟的近似随机游走特征，计算时间感知节点嵌入以总结多跳信息。

    

    许多真实世界的数据集具有基础的动态图结构，其中实体和它们的相互作用随时间演变。机器学习模型应考虑这些动态因素，以在下游任务中充分发挥其潜力。以前用于图表示学习的方法要么侧重于抽样k-跳邻域，类似于广度优先搜索，要么侧重于随机游走，类似于深度优先搜索。然而，这些方法在实时动态图上进行低延迟推断是计算上昂贵且不适用的。为了克服这些限制，我们提出了图形快跑，这是一个适用于连续时间动态图（CTDGs）的通用特征提取框架，具有低延迟，并且与高延迟模型相比具有竞争力。为了实现这一点，我们提出了一种流式、低延迟的近似随机游走特征。在我们的框架中，使用仅单跳操作计算总结多跳信息的时间感知节点嵌入。

    Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop ope
    
[^132]: 论分裂学习对抗敌对攻击的鲁棒性

    On the Robustness of Split Learning against Adversarial Attacks. (arXiv:2307.07916v1 [cs.LG])

    [http://arxiv.org/abs/2307.07916](http://arxiv.org/abs/2307.07916)

    该论文评估了分裂学习对抗敌对攻击的鲁棒性，特别关注不受信任服务器只能访问模型中间层的情况。通过仅向服务器公开部分模型，分裂学习可以缓解敌对攻击威胁。

    

    分裂学习通过避免直接共享原始数据和模型细节（即服务器和客户端仅持有部分子网络并交换中间计算）来实现协同深度学习模型训练的同时保护数据隐私和模型安全性。然而，现有研究主要集中在检验其对隐私保护的可靠性，对模型安全性的研究甚少。具体而言，通过探索完整模型，攻击者可以发起敌对攻击，而分裂学习可以通过仅向不受信任的服务器公开部分模型来缓解这种严重威胁。本文旨在评估分裂学习对抗敌对攻击的鲁棒性，特别是在不受信任的服务器只能访问模型的中间层的最具挑战性的情况下。现有的敌对攻击大多集中在集中式环境而非协同式环境，因此，为了更好地评估分裂学习的鲁棒性，我们提出了一种新的评估方法。

    Split learning enables collaborative deep learning model training while preserving data privacy and model security by avoiding direct sharing of raw data and model details (i.e., sever and clients only hold partial sub-networks and exchange intermediate computations). However, existing research has mainly focused on examining its reliability for privacy protection, with little investigation into model security. Specifically, by exploring full models, attackers can launch adversarial attacks, and split learning can mitigate this severe threat by only disclosing part of models to untrusted servers.This paper aims to evaluate the robustness of split learning against adversarial attacks, particularly in the most challenging setting where untrusted servers only have access to the intermediate layers of the model.Existing adversarial attacks mostly focus on the centralized setting instead of the collaborative setting, thus, to better evaluate the robustness of split learning, we develop a ta
    
[^133]: 在GFlowNets中使用回放缓冲区对模式发现的有效性的实证研究

    An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets. (arXiv:2307.07674v1 [cs.LG])

    [http://arxiv.org/abs/2307.07674](http://arxiv.org/abs/2307.07674)

    本文通过实证研究，探讨了在GFlowNets中使用回放缓冲区的有效性，评估了不同回放缓冲区采样技术对模式发现速度和质量的影响。

    

    强化学习（RL）算法旨在通过迭代采样动作从而学习如何最大化总期望回报$R（x）$来学习最优策略。GFlowNets是一类特殊的算法，通过学习一个近似于$R（x）$的概率采样策略，从离散集合中生成多样的候选样本$x$。与传统的RL算法相比，GFlowNets表现出更好的模式发现能力，对于药物发现和组合搜索等应用非常有用。然而，由于GFlowNets是一个相对较新的算法类别，许多在RL中有用的技术尚未与其关联起来。本文研究了在GFlowNets中利用回放缓冲区。我们通过实证研究了各种回放缓冲区采样技术的影响，评估了模式发现速度和发现的模式质量。在Hypergrid模拟环境和分子合成环境中的实验结果证明了我们的观察结论。

    Reinforcement Learning (RL) algorithms aim to learn an optimal policy by iteratively sampling actions to learn how to maximize the total expected return, $R(x)$. GFlowNets are a special class of algorithms designed to generate diverse candidates, $x$, from a discrete set, by learning a policy that approximates the proportional sampling of $R(x)$. GFlowNets exhibit improved mode discovery compared to conventional RL algorithms, which is very useful for applications such as drug discovery and combinatorial search. However, since GFlowNets are a relatively recent class of algorithms, many techniques which are useful in RL have not yet been associated with them. In this paper, we study the utilization of a replay buffer for GFlowNets. We explore empirically various replay buffer sampling techniques and assess the impact on the speed of mode discovery and the quality of the modes discovered. Our experimental results in the Hypergrid toy domain and a molecule synthesis environment demonstrat
    
[^134]: 反向最优化用于路由问题

    Inverse Optimization for Routing Problems. (arXiv:2307.07357v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2307.07357](http://arxiv.org/abs/2307.07357)

    本研究提出了一种使用反向最优化（IO）学习路由问题决策者行为的方法，并在亚马逊末端路由研究挑战中测试了该方法，在复制人类驾驶员的路由偏好方面取得了第2名的成绩。

    

    我们提出了一种使用反向最优化（IO）学习路由问题决策者行为的方法。IO框架属于监督学习类别，并建立在目标行为是未知成本函数的优化器的前提下。这个成本函数通过历史数据进行学习，在路由问题的背景下，可以理解为决策者的路由偏好。在这个视角下，本研究的主要贡献是提出了一种适用于路由问题的IO方法，包括假设函数、损失函数和基于随机一阶算法。我们进一步在亚马逊末端路由研究挑战中测试了我们的IO方法，该挑战的目标是使用成千上万个真实世界的路由案例学习模型以复制人类驾驶员的路由偏好。我们最终学习到的IO路由模型在48个晋级到决赛的模型中排名第2。

    We propose a method for learning decision-makers' behavior in routing problems using Inverse Optimization (IO). The IO framework falls into the supervised learning category and builds on the premise that the target behavior is an optimizer of an unknown cost function. This cost function is to be learned through historical data, and in the context of routing problems, can be interpreted as the routing preferences of the decision-makers. In this view, the main contributions of this study are to propose an IO methodology with a hypothesis function, loss function, and stochastic first-order algorithm tailored to routing problems. We further test our IO approach in the Amazon Last Mile Routing Research Challenge, where the goal is to learn models that replicate the routing preferences of human drivers, using thousands of real-world routing examples. Our final IO-learned routing model achieves a score that ranks 2nd compared with the 48 models that qualified for the final round of the challe
    
[^135]: 通过敌对双机器学习的因果参数估计来缓解敌对脆弱性

    Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v1 [cs.LG])

    [http://arxiv.org/abs/2307.07250](http://arxiv.org/abs/2307.07250)

    通过敌对双机器学习方法，可以量化和缓解深度神经网络在面对敌对输入时的脆弱性。

    

    从经过精心设计的视觉输入中衍生出的敌对例子可以轻松地损害深度神经网络的决策过程。为了防止潜在的威胁，各种基于敌对训练的防御方法迅速增长，并成为稳健性的事实上标准方法。尽管最近取得了竞争性的成就，我们观察到敌对脆弱性在不同目标之间存在差异，并且某些脆弱性仍然普遍存在。有趣的是，即使使用更深层次的架构和先进的防御方法，这种奇特的现象仍然无法缓解。为了解决这个问题，本文介绍了一种称为敌对双机器学习（ADML）的因果方法，它允许我们量化网络预测的敌对脆弱性程度，并捕捉对结果的处理效果。ADML可以直接估计敌对扰动本身的因果参数，并减轻可能损害稳健性的负面效应。

    Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision process of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across targets and certain vulnerabilities remain prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods. To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adversarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial perturbations per se and mitigate negative effects that can potentially damage robustness, bridgi
    
[^136]: 加权平均随机梯度下降: 渐近正态性和最优性

    Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality. (arXiv:2307.06915v1 [stat.ML])

    [http://arxiv.org/abs/2307.06915](http://arxiv.org/abs/2307.06915)

    本文探索了一种加权平均随机梯度下降（SGD）方案，并建立了渐近正态性，提供了渐近有效的在线推理方法。此外，我们提出了一种自适应平均方案，具有最优的统计速度和有利的非渐近收敛性。

    

    随机梯度下降（SGD）是现代统计和机器学习中最简单和最流行的算法之一，由于其计算和内存效率而受到青睐。在不同的情境下，已经提出了各种平均方案来加速SGD的收敛。在本文中，我们探讨了一种用于SGD的通用平均方案。具体而言，我们建立了一类加权平均SGD解的渐近正态性，并提供了渐近有效的在线推理方法。此外，我们提出了一种自适应平均方案，展现出最优的统计速度和有利的非渐近收敛性，借鉴了线性模型的非渐近均方误差（MSE）的最优权重的见解。

    Stochastic Gradient Descent (SGD) is one of the simplest and most popular algorithms in modern statistical and machine learning due to its computational and memory efficiency. Various averaging schemes have been proposed to accelerate the convergence of SGD in different settings. In this paper, we explore a general averaging scheme for SGD. Specifically, we establish the asymptotic normality of a broad range of weighted averaged SGD solutions and provide asymptotically valid online inference approaches. Furthermore, we propose an adaptive averaging scheme that exhibits both optimal statistical rate and favorable non-asymptotic convergence, drawing insights from the optimal weight for the linear model in terms of non-asymptotic mean squared error (MSE).
    
[^137]: 通过双层ReLU神经网络实现可证明的多任务表示学习

    Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks. (arXiv:2307.06887v1 [cs.LG])

    [http://arxiv.org/abs/2307.06887](http://arxiv.org/abs/2307.06887)

    通过双层ReLU神经网络，本论文提出了一种可证明的多任务表示学习方法，用于解决神经网络在实践中同时训练多个任务时遇到的问题。

    

    特征学习是神经网络实际成功的关键，然而如何以及为何发生特征学习仍然难以解释。最近的理论研究表明，在用梯度下降方法优化的浅层神经网络上可以学习有意义的特征，扩展了我们对于神经切向核或随机特征范例中微不足道的特征学习的了解。然而，在实践中，神经网络越来越经常地同时训练多个具有不同损失函数的任务，并且这些先前的分析并不适用于这种情况。在多任务学习设置中，各种研究已经表明简单线性模型可以有效地进行特征学习。然而，通过非线性模型进行多任务学习，这在实践中是最常见的学习范式，仍然存在许多未知。在这项工作中，我们首次提出了一种可证明的多任务表示学习方法，通过双层ReLU神经网络实现。

    Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first 
    
[^138]: DL模型和训练环境对能源消耗有影响吗？

    Do DL models and training environments have an impact on energy consumption?. (arXiv:2307.05520v1 [cs.LG])

    [http://arxiv.org/abs/2307.05520](http://arxiv.org/abs/2307.05520)

    本研究分析了模型架构和训练环境对训练更环保的计算机视觉模型的影响，并找出了能源效率和模型正确性之间的权衡关系。

    

    当前计算机视觉领域的研究主要集中在提高深度学习（DL）的正确性和推理时间性能上。然而，目前很少有关于训练DL模型带来巨大碳足迹的研究。本研究旨在分析模型架构和训练环境对训练更环保的计算机视觉模型的影响。我们将这个目标分为两个研究问题。首先，我们分析模型架构对实现更环保模型同时保持正确性在最佳水平的影响。其次，我们研究训练环境对生成更环保模型的影响。为了调查这些关系，我们在模型训练过程中收集了与能源效率和模型正确性相关的多个指标。然后，我们描述了模型架构在测量能源效率和模型正确性方面的权衡，以及它们与训练环境的关系。我们在一个实验平台上进行了这项研究。

    Current research in the computer vision field mainly focuses on improving Deep Learning (DL) correctness and inference time performance. However, there is still little work on the huge carbon footprint that has training DL models. This study aims to analyze the impact of the model architecture and training environment when training greener computer vision models. We divide this goal into two research questions. First, we analyze the effects of model architecture on achieving greener models while keeping correctness at optimal levels. Second, we study the influence of the training environment on producing greener models. To investigate these relationships, we collect multiple metrics related to energy efficiency and model correctness during the models' training. Then, we outline the trade-offs between the measured energy efficiency and the models' correctness regarding model architecture, and their relationship with the training environment. We conduct this research in the context of a 
    
[^139]: 大型语言模型中RLHF的秘密 第一部分：PPO

    Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])

    [http://arxiv.org/abs/2307.04964](http://arxiv.org/abs/2307.04964)

    本论文研究了大型语言模型中RLHF的秘密，重点关注了奖励模型、PPO和进程监督等技术路径，探索如何解决RLHF的稳定训练问题。

    

    大型语言模型（LLMs）为推动人工通用智能的进展提供了蓝图。其主要目标是成为以人为中心的（有益、诚实和无害）助手。与人类的对齐具有至关重要的意义，强化学习与人类反馈（RLHF）成为支撑这一追求的关键技术范式。当前的技术路线通常包括用于衡量人类偏好的奖励模型、用于优化策略模型输出的近端策略优化（PPO）以及用于改善逐步推理能力的进程监督。然而，由于奖励设计、环境交互和代理训练的挑战，再加上大型语言模型的试验成本巨大，对于AI研究人员来说，激励技术对齐和LLMs的安全着陆存在重大障碍。RLHF的稳定训练仍然是一个难题。

    Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first re
    
[^140]: 基于统计分析和应用程序在金融公司中制定战略计划的一个实际案例研究

    Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case. (arXiv:2307.04778v1 [cs.LG])

    [http://arxiv.org/abs/2307.04778](http://arxiv.org/abs/2307.04778)

    本论文介绍了一个基于统计分析的战略计划，通过研究金融公司LendingClub的实际案例，探索引入大数据平台和先进特征选择能力的可能性，以增加收入并降低风险。

    

    商业统计在企业级别实施数据驱动的战略计划中起着至关重要的作用，以运用各种分析手段，这样的计划的结果可以帮助企业改进决策过程或降低组织风险。本文介绍了一个基于统计分析的战略计划，针对一家金融公司LendingClub，计划包括探索引入大数据平台和先进的特征选择能力的可能性。这个计划的主要目标是增加公司的收入，同时减少向无法归还贷款的借款人授予贷款的风险。通过对公司担忧进行了不同假设的研究，结果显示贷款金额对无法归还贷款的借款人人数有深远影响。提出的战略计划还包括引入机器学习等先进分析方法的应用。

    Business statistics play a crucial role in implementing a data-driven strategic plan at the enterprise level to employ various analytics where the outcomes of such a plan enable an enterprise to enhance the decision-making process or to mitigate risks to the organization. In this work, a strategic plan informed by the statistical analysis is introduced for a financial company called LendingClub, where the plan is comprised of exploring the possibility of onboarding a big data platform along with advanced feature selection capacities. The main objectives of such a plan are to increase the company's revenue while reducing the risks of granting loans to borrowers who cannot return their loans. In this study, different hypotheses formulated to address the company's concerns are studied, where the results reveal that the amount of loans profoundly impacts the number of borrowers charging off their loans. Also, the proposed strategic plan includes onboarding advanced analytics such as machin
    
[^141]: 一次性学习梯度手术在生成模型中的应用

    Gradient Surgery for One-shot Unlearning on Generative Model. (arXiv:2307.04550v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04550](http://arxiv.org/abs/2307.04550)

    本文介绍了一种针对生成模型的一次性学习梯度手术方法，通过操纵梯度来消除数据对模型的影响，并在理论上进行了分析。

    

    最近对“遗忘权”的监管引发了对取消预训练机器学习模型的兴趣。近期的机器学习取消方法通过更新权重来消除样本对权重参数的影响，以逼近一种直接但昂贵的重新训练方法。本文介绍了一种简单且有效的方法，用于在深度生成模型中消除数据的影响。受多任务学习的启发，我们提出了一种使用梯度操作来调整样本之间影响的方法，通过将梯度投影到保留梯度的法平面上进行规范化。我们的方法不依赖于移除样本的统计数据，优于现有基准，并首次在取消生成模型方面提供了理论分析。

    Recent regulation on right-to-be-forgotten emerges tons of interest in unlearning pre-trained machine learning models. While approximating a straightforward yet expensive approach of retrain-from-scratch, recent machine unlearning methods unlearn a sample by updating weights to remove its influence on the weight parameters. In this paper, we introduce a simple yet effective approach to remove a data influence on the deep generative model. Inspired by works in multi-task learning, we propose to manipulate gradients to regularize the interplay of influence among samples by projecting gradients onto the normal plane of the gradients to be retained. Our work is agnostic to statistics of the removal samples, outperforming existing baselines while providing theoretical analysis for the first time in unlearning a generative model.
    
[^142]: 不要背诵，模仿过去：无需使用记忆的联邦类增量学习

    Don't Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory. (arXiv:2307.00497v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00497](http://arxiv.org/abs/2307.00497)

    该论文提出了一个无需使用记忆的联邦类增量学习框架，通过生成模型合成过去分布的样本，从而缓解联邦学习中的灾难性遗忘问题。

    

    深度学习模型在训练新数据时容易忘记过去学习的信息。在联邦学习（FL）的背景下，这个问题变得更加明显，因为数据是分散的，每个用户都会独立地进行更改。连续学习（CL）主要在中心化的环境中研究这种所谓的“灾难性遗忘”现象，其中学习者可以直接访问完整的训练数据集。然而，将CL技术应用于FL并不直接，因为涉及到隐私问题和资源限制。本文提出了一个框架，用于联邦类增量学习，该框架利用生成模型从过去的分布中合成样本，而不是存储部分过去的数据。然后，客户端可以利用生成模型在本地缓解灾难性遗忘。生成模型通过在每个任务结束时使用无数据方法在服务器上进行训练，而不请求来自客户端的数据。

    Deep learning models are prone to forgetting information learned in the past when trained on new data. This problem becomes even more pronounced in the context of federated learning (FL), where data is decentralized and subject to independent changes for each user. Continual Learning (CL) studies this so-called \textit{catastrophic forgetting} phenomenon primarily in centralized settings, where the learner has direct access to the complete training dataset. However, applying CL techniques to FL is not straightforward due to privacy concerns and resource limitations. This paper presents a framework for federated class incremental learning that utilizes a generative model to synthesize samples from past distributions instead of storing part of past data. Then, clients can leverage the generative model to mitigate catastrophic forgetting locally. The generative model is trained on the server using data-free methods at the end of each task without requesting data from clients. Therefore, i
    
[^143]: 通过线掩模引导的黑盒优化实现宏单元布局

    Macro Placement by Wire-Mask-Guided Black-Box Optimization. (arXiv:2306.16844v1 [cs.LG])

    [http://arxiv.org/abs/2306.16844](http://arxiv.org/abs/2306.16844)

    本文介绍了一种名为WireMask-BBO的黑盒优化框架，通过使用线掩模引导的贪心过程进行宏单元布局，在有效降低HPWL的同时节省大量时间。此方法还可以对现有布局进行微调，改善50%的HPWL。

    

    面对大规模集成（VLSI）技术的发展，芯片布局中的电子设计自动化（EDA）技术面临新的挑战。宏单元布局作为该过程中的重要子问题，试图确定所有宏单元的位置，以最小化半周长线长（HPWL）并避免重叠。先前的方法包括基于打包、分析和强化学习的方法。本文提出了一种新的黑盒优化（BBO）框架（称为WireMask-BBO），通过使用线掩模引导的贪心过程进行目标评估来进行宏单元布局。配备不同的BBO算法，WireMask-BBO在实践中比先前的方法实现了显著的改进，即通过使用更少的时间实现了显著更短的HPWL。此外，它可以通过将其视为初始解来微调现有的布局，从而使HPWL改善多达50%。WireMask-BBO具有引领芯片布局领域的潜力。

    The development of very large-scale integration (VLSI) technology has posed new challenges for electronic design automation (EDA) techniques in chip floorplanning. During this process, macro placement is an important subproblem, which tries to determine the positions of all macros with the aim of minimizing half-perimeter wirelength (HPWL) and avoiding overlapping. Previous methods include packing-based, analytical and reinforcement learning methods. In this paper, we propose a new black-box optimization (BBO) framework (called WireMask-BBO) for macro placement, by using a wire-mask-guided greedy procedure for objective evaluation. Equipped with different BBO algorithms, WireMask-BBO empirically achieves significant improvements over previous methods, i.e., achieves significantly shorter HPWL by using much less time. Furthermore, it can fine-tune existing placements by treating them as initial solutions, which can bring up to 50% improvement in HPWL. WireMask-BBO has the potential to s
    
[^144]: 使用概率因果模型生成高保真度图像的反事实推理

    High Fidelity Image Counterfactuals with Probabilistic Causal Models. (arXiv:2306.15764v1 [cs.LG])

    [http://arxiv.org/abs/2306.15764](http://arxiv.org/abs/2306.15764)

    提出了一个通用的因果生成建模框架，用于准确估计具有高保真度的图像反事实推理，通过利用因果中介分析和生成建模的思想，设计了新的深度因果机制，实验证明了该方法的准确性。

    

    我们提出了一个通用的因果生成建模框架，用于准确估计具有深度结构因果模型的高保真度图像的反事实推理。对于高维结构变量（如图像）的干预和反事实查询的估计仍然是一项具有挑战性的任务。我们利用因果中介分析的思想和生成建模的进展，为因果模型中的结构变量设计了新的深度因果机制。我们的实验证明，我们提出的机制能够准确地推断和估计直接、间接和总效应，这可以通过反事实的公理严密性来衡量。

    We present a general causal generative modelling framework for accurate estimation of high fidelity image counterfactuals with deep structural causal models. Estimation of interventional and counterfactual queries for high-dimensional structured variables, such as images, remains a challenging task. We leverage ideas from causal mediation analysis and advances in generative modelling to design new deep causal mechanisms for structured variables in causal models. Our experiments demonstrate that our proposed mechanisms are capable of accurate abduction and estimation of direct, indirect and total effects as measured by axiomatic soundness of counterfactuals.
    
[^145]: SparseOptimizer: 通过Moreau-Yosida正则化来降低语言模型的稀疏性，并通过编译器共同设计来加速

    SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])

    [http://arxiv.org/abs/2306.15656](http://arxiv.org/abs/2306.15656)

    SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。

    

    本文介绍了SparseOptimizer，一种新颖的深度学习优化器，通过Moreau-Yosida正则化在大型语言模型（如BERT，ALBERT和GPT）中自然地引入稀疏性。SparseOptimizer设计的关键是嵌入的收缩操作符，它在优化过程中直接引入稀疏性。这个操作符通过坚实的理论框架支持，并包含了一个分析解，从而增强了优化器的鲁棒性和效果。重要的是，SparseOptimizer的即插即用功能消除了对代码修改的需求，使其成为适用于各种大型语言模型的通用适应工具。在GLUE、RACE、SQuAD1和SQuAD2等基准数据集上的实证评估表明，通过SparseOptimizer稀疏化后的SparseBERT和SparseALBERT在性能上与密集型的BERT和ALBERT相当，同时显著减少了参数数量。

    This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
    
[^146]: 几何超声定位显微镜

    Geometric Ultrasound Localization Microscopy. (arXiv:2306.15548v1 [cs.CV])

    [http://arxiv.org/abs/2306.15548](http://arxiv.org/abs/2306.15548)

    这项研究提出了一种基于几何框架的超声定位显微镜方法，通过仅依赖到达时间差信息实现微气泡的定位，并在精度和可靠性方面超越了现有的方法。

    

    对比增强超声（CEUS）已成为无创动态可视化医学诊断方法，然而超声定位显微镜（ULM）通过提供十倍更高的分辨率实现了突破性进展。到目前为止，延迟和求和（DAS）波束形成器被用于渲染ULM帧，最终确定图像的分辨率能力。为了充分利用ULM，本研究质疑波束形成是否是ULM最有效的处理步骤，提出了一种仅依赖到达时间差（TDoA）信息的替代方法。为此，提出了一种新颖的通过椭圆交点定位微气泡的几何框架，以克服现有波束形成的局限性。我们基于一个公共数据集进行了基准比较，结果表明我们的几何ULM在精度和可靠性方面优于现有的基准方法，仅利用了部分可用的换能器数据。

    Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound Localization Microscopy (ULM) has enabled a revolutionary breakthrough by offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers are used to render ULM frames, ultimately determining the image resolution capability. To take full advantage of ULM, this study questions whether beamforming is the most effective processing step for ULM, suggesting an alternative approach that relies solely on Time-Difference-of-Arrival (TDoA) information. To this end, a novel geometric framework for micro bubble localization via ellipse intersections is proposed to overcome existing beamforming limitations. We present a benchmark comparison based on a public dataset for which our geometric ULM outperforms existing baseline methods in terms of accuracy and reliability while only utilizing a portion of the available transducer data.
    
[^147]: 合作多目标强化学习用于交通信号控制和碳减排

    Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction. (arXiv:2306.09662v1 [cs.LG])

    [http://arxiv.org/abs/2306.09662](http://arxiv.org/abs/2306.09662)

    本文提出了一种合作的多目标架构，称为MOMA-DDPG，用于交通信号控制和碳减排问题。该方法涉及两种类型的智能体：一个专注于优化每个路口的本地交通，而另一个旨在优化全局交通吞吐量。结果显示，该方法优于现有最先进的方法，并解决了等待时间和碳排放量两个问题。

    

    现有的交通信号控制系统依赖于过于简化的基于规则的方法，甚至基于强化学习的方法也经常是次优的和不稳定的。为了解决这个问题，我们提出了一个合作的多目标架构，称为多目标多智能体深度确定性策略梯度（MOMA-DDPG），使用衰减权重来估计交通信号控制优化的多个奖励项。我们的方法涉及两种类型的智能体：一个专注于优化每个路口的本地交通，而另一个旨在优化全局交通吞吐量。我们使用从一个亚洲国家的交通摄像头收集到的真实世界交通数据来评估我们的方法。尽管包含了一个全局智能体，但我们的解决方案仍然是分散的，因为这个智能体在推理阶段不再是必要的。我们的结果证明了MOMA-DDPG的有效性，在所有性能指标上优于最先进的方法。此外，我们提出的系统最小化了等待时间和碳排放量两方面的问题。

    Existing traffic signal control systems rely on oversimplified rule-based methods, and even RL-based methods are often suboptimal and unstable. To address this, we propose a cooperative multi-objective architecture called Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MOMA-DDPG), which estimates multiple reward terms for traffic signal control optimization using age-decaying weights. Our approach involves two types of agents: one focuses on optimizing local traffic at each intersection, while the other aims to optimize global traffic throughput. We evaluate our method using real-world traffic data collected from an Asian country's traffic cameras. Despite the inclusion of a global agent, our solution remains decentralized as this agent is no longer necessary during the inference stage. Our results demonstrate the effectiveness of MOMA-DDPG, outperforming state-of-the-art methods across all performance metrics. Additionally, our proposed system minimizes both waiting ti
    
[^148]: 统一的非同策略学习排序：强化学习视角

    Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective. (arXiv:2306.07528v1 [cs.LG])

    [http://arxiv.org/abs/2306.07528](http://arxiv.org/abs/2306.07528)

    本文提出了点击模型不可知的统一非同策略学习排序（CUOLR）方法，通过离线强化学习（RL）直接学习最优排名，可以轻松地应用于各种点击模型。

    

    非同策略学习排序（LTR）旨在通过已部署的记录策略收集的数据优化排名器。然而，现有的非同策略学习排序方法经常对用户如何生成点击数据即点击模型进行假设，因此需要根据不同的点击模型专门调整他们的方法。在本文中，我们将排名过程在一般随机点击模型下统一为马尔可夫决策过程（MDP），通过离线强化学习（RL），可以直接学习最优排名。在此基础上，我们利用离线RL技术进行非同策略LTR，并提出点击模型不可知的统一非同策略学习排序（CUOLR）方法，该方法可以轻松地应用于各种点击模型。通过对MDP的专门制定，我们证明了离线RL算法可以适应各种点击模型，而无需复杂的去偏倚技术和先验知识。在各种大规模数据集上的实验结果都证明了我们方法的有效性。

    Off-policy Learning to Rank (LTR) aims to optimize a ranker from data collected by a deployed logging policy. However, existing off-policy learning to rank methods often make strong assumptions about how users generate the click data, i.e., the click model, and hence need to tailor their methods specifically under different click models. In this paper, we unified the ranking process under general stochastic click models as a Markov Decision Process (MDP), and the optimal ranking could be learned with offline reinforcement learning (RL) directly. Building upon this, we leverage offline RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a wide range of click models. Through a dedicated formulation of the MDP, we show that offline RL algorithms can adapt to various click models without complex debiasing techniques and prior knowledge of the model. Results on various large-scale datasets
    
[^149]: 通过Lipschitz正则化来减轻Transformer的过度自信问题

    Mitigating Transformer Overconfidence via Lipschitz Regularization. (arXiv:2306.06849v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06849](http://arxiv.org/abs/2306.06849)

    这项工作通过提出一种新的Lipschitz正则化Transformer (LRFormer)来减轻Transformer在预测中的过度自信问题，并在视觉基准实验证明了其优于现有方法的性能。

    

    虽然Transformer在许多计算机视觉任务中取得了有希望的结果，但它们往往在预测中过于自信，因为标准的点积自注意力(DPSA)在无界输入域中几乎无法保持距离。在这项工作中，我们通过提出一种新的Lipschitz正则化Transformer(LRFormer)来填补这个空白。具体来说，我们提出了一种新的相似性函数，它在Banach空间内保证了Lipschitz性质，并通过一个收缩的Lipschitz边界来正则化该项。所提出的方法经过理论保证进行了分析，为其有效性和可靠性提供了严谨的基础。在标准的视觉基准上进行的大量实验证明，我们的方法在预测、校准和不确定性估计方面优于最先进的单次前向传递方法。

    Though Transformers have achieved promising results in many computer vision tasks, they tend to be over-confident in predictions, as the standard Dot Product Self-Attention (DPSA) can barely preserve distance for the unbounded input domain. In this work, we fill this gap by proposing a novel Lipschitz Regularized Transformer (LRFormer). Specifically, we present a new similarity function with the distance within Banach Space to ensure the Lipschitzness and also regularize the term by a contractive Lipschitz Bound. The proposed method is analyzed with a theoretical guarantee, providing a rigorous basis for its effectiveness and reliability. Extensive experiments conducted on standard vision benchmarks demonstrate that our method outperforms the state-of-the-art single forward pass approaches in prediction, calibration, and uncertainty estimation.
    
[^150]: K-Tensors：对正半定矩阵进行聚类

    K-Tensors: Clustering Positive Semi-Definite Matrices. (arXiv:2306.06534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06534](http://arxiv.org/abs/2306.06534)

    本文介绍了一种针对正半定矩阵的自一致性聚类算法（K-张量），通过考虑其特征结构，能够有效地将正半定矩阵进行分区。

    

    本文介绍了一种新颖的自一致性聚类算法（K-Tensors），用于基于它们的特征结构将正半定矩阵进行分区。由于正半定矩阵可以在 p≥2 的空间中表示为椭球体，因此保持它们的结构信息以进行有效的聚类至关重要。然而，传统的矩阵聚类算法常常涉及将矩阵向量化，导致关键结构信息的丢失。为了解决这个问题，我们提出了一种基于正半定矩阵结构信息的距离度量来进行聚类。这种距离度量使得聚类算法能够考虑正半定矩阵与它们在由一组正半定矩阵定义的正交向量张成的共同空间上的投影之间的差异。这是一种创新的聚类方法。

    This paper introduces a novel self-consistency clustering algorithm ($K$-Tensors) designed for {partitioning a distribution of} positive-semidefinite matrices based on their eigenstructures. As positive semi-definite matrices can be represented as ellipsoids in $\Re^p$, $p \ge 2$, it is critical to maintain their structural information to perform effective clustering. However, traditional clustering algorithms {applied to matrices} often {involve vectorization of} the matrices, resulting in a loss of essential structural information. To address this issue, we propose a distance metric {for clustering} that is specifically based on the structural information of positive semi-definite matrices. This distance metric enables the clustering algorithm to consider the differences between positive semi-definite matrices and their projections onto {a} common space spanned by \thadJulyTen{orthonormal vectors defined from a set of} positive semi-definite matrices. This innovative approach to clus
    
[^151]: CLC: 基于对比表示学习的聚类分配方法

    CLC: Cluster Assignment via Contrastive Representation Learning. (arXiv:2306.05439v1 [cs.LG])

    [http://arxiv.org/abs/2306.05439](http://arxiv.org/abs/2306.05439)

    本文提出了一种基于对比学习的聚类方法（CLC），它使用对比学习直接学习聚类分配，并在大规模数据集上取得了更好的聚类性能。

    

    聚类是一项重要而具有挑战性的任务，旨在将样本分组，而不需要手动注释。最近的研究通过对自监督学习得到的特征表示进行聚类，在小型数据集上取得了出色的结果。然而，对于包含大量聚类的数据集，如ImageNet，当前的方法仍然无法实现高聚类性能。在本文中，我们提出了基于对比学习的聚类方法（CLC），它使用对比学习直接学习聚类分配。我们将表示分解为两部分：一部分对类别信息进行编码，并采用等分约束，另一部分捕捉实例因素。我们提出了一种对比损失，使用表示的两个部分。我们在理论上分析了所提出的对比损失，并揭示了CLC在学习聚类分配时为负样本设置不同的权重。进一步的梯度分析表明，当使用CLC时，在大规模数据集上取得了更好的聚类性能。

    Clustering remains an important and challenging task of grouping samples into clusters without manual annotations. Recent works have achieved excellent results on small datasets by performing clustering on feature representations learned from self-supervised learning. However, for datasets with a large number of clusters, such as ImageNet, current methods still can not achieve high clustering performance. In this paper, we propose Contrastive Learning-based Clustering (CLC), which uses contrastive learning to directly learn cluster assignment. We decompose the representation into two parts: one encodes the categorical information under an equipartition constraint, and the other captures the instance-wise factors. We propose a contrastive loss using both parts of the representation. We theoretically analyze the proposed contrastive loss and reveal that CLC sets different weights for the negative samples while learning cluster assignments. Further gradient analysis shows that the larger 
    
[^152]: 无监督嵌入质量评估

    Unsupervised Embedding Quality Evaluation. (arXiv:2305.16562v1 [cs.LG])

    [http://arxiv.org/abs/2305.16562](http://arxiv.org/abs/2305.16562)

    这项工作提出了评估嵌入质量的不同方法，关注如何以稳定的方式进行线性分离。从调查的文献和引入的新方法中，我们可以评估嵌入的质量，从而提高无监督学习的表现。(This work proposes new methods to evaluate the quality of embeddings, focusing on stable linear separation. From the surveyed literature and introduced novel methods, we can evaluate the quality of embeddings and improve the performance of unsupervised learning.)

    

    无监督学习，尤其是基于深度学习的方法最近在学术界得到了显著的发展。虽然在各种基准测试中取得了接近监督学习水平的成果，但由于无监督问题的本质，实践中训练和评估 SSL 模型仍然很困难。即使是以有监督的方式训练的网络，在转移到另一个领域时是否能够良好地表现，也往往不清楚。过去的工作通常仅限于评估嵌入中包含的信息量，这对于深度神经网络的自我监督学习最为相关。然而，这项工作选择了不同的方法：我们能否量化数据中如何以稳定的方式进行线性分离？我们调查了相关的文献，并发现三种方法可以用于评估嵌入的质量。此外，我们还介绍了一种基于近期对高维空间理解的最新进展的新方法。

    Unsupervised learning has recently significantly gained in popularity, especially with deep learning-based approaches. Despite numerous successes and approaching supervised-level performance on a variety of academic benchmarks, it is still hard to train and evaluate SSL models in practice due to the unsupervised nature of the problem. Even with networks trained in a supervised fashion, it is often unclear whether they will perform well when transferred to another domain.  Past works are generally limited to assessing the amount of information contained in embeddings, which is most relevant for self-supervised learning of deep neural networks. This works chooses to follow a different approach: can we quantify how easy it is to linearly separate the data in a stable way? We survey the literature and uncover three methods that could be potentially used for evaluating quality of representations. We also introduce one novel method based on recent advances in understanding the high-dimension
    
[^153]: 火星时间序列分解：一种多尺度嵌套方法中的因子变分自编码器

    Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders. (arXiv:2305.16189v1 [cs.LG])

    [http://arxiv.org/abs/2305.16189](http://arxiv.org/abs/2305.16189)

    该论文提出了一种因子高斯混合变分自动编码器，用于多尺度聚类和源分离，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程，并在MRO数据集上展现了更好的性能。

    

    无监督的源分离涉及通过混合操作记录的未知源信号的分解，其中对源的先验知识有限，仅可以访问信号混合数据集。这个问题本质上是不适用的，并且进一步受到时间序列数据中源展现出的多种时间尺度的挑战。为了解决这个问题，我们提出了一种无监督的多尺度聚类和源分离框架，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程。在这个表示空间中，我们开发了一个因子高斯混合变分自动编码器，它被训练用于(1)概率地对不同时间尺度上的源进行聚类和逐层非监督源分离，(2)在每个时间尺度上提取低维表示，(3)学习源信号的因子表示，(4)在表示空间中进行采样，以生成未知源信号。我们在MRO上的三个频道的可见数据集上进行了评估，结果表明所提出的方法比目前最先进的技术具有更好的性能。

    Unsupervised source separation involves unraveling an unknown set of source signals recorded through a mixing operator, with limited prior knowledge about the sources, and only access to a dataset of signal mixtures. This problem is inherently ill-posed and is further challenged by the variety of time-scales exhibited by sources in time series data. Existing methods typically rely on a preselected window size that limits their capacity to handle multi-scale sources. To address this issue, instead of operating in the time domain, we propose an unsupervised multi-scale clustering and source separation framework by leveraging wavelet scattering covariances that provide a low-dimensional representation of stochastic processes, capable of distinguishing between different non-Gaussian stochastic processes. Nested within this representation space, we develop a factorial Gaussian-mixture variational autoencoder that is trained to (1) probabilistically cluster sources at different time-scales a
    
[^154]: 在脉冲神经网络中将噪声作为计算和学习资源

    Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2305.16044](http://arxiv.org/abs/2305.16044)

    本文提出了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），展示了噪声可以作为计算和学习的资源，并为一般脉冲神经元网络提供了一个框架。研究还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。

    

    脉冲神经元网络是大脑非凡信息处理能力的基础，并已成为神经形态智能的支柱模型。本文介绍了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），采用带有噪声神经元动力学的脉冲神经元模型。该方法显示噪声可以作为计算和学习的资源，并理论上为一般脉冲神经元网络提供了一个框架。此外，NDL为代理梯度提供了深入的生物学合理性。通过将各种SNN架构和算法结合起来，我们展示了我们的方法表现出竞争性能，并且比确定性SNNs表现出更好的鲁棒性。此外，本文还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。

    Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
    
[^155]: 高效大规模的视觉表示学习

    Efficient Large-Scale Vision Representation Learning. (arXiv:2305.13399v1 [cs.CV])

    [http://arxiv.org/abs/2305.13399](http://arxiv.org/abs/2305.13399)

    本文介绍了一种高效大规模的单模态视觉表示学习方法，解决了电商应用中视觉表示的挑战，并提供了消融研究和评估。

    

    本文介绍了我们的单模态视觉表示学习方法，了解产品内容的视觉表示对电商推荐、搜索和广告应用至关重要。我们详细介绍和对比了在低资源环境下有效微调大规模视觉表示学习模型的技术，包括多种预训练的骨干架构，包括卷积神经网络和视觉转换器系列。我们强调了电子商务应用在大规模情况下的挑战，并突出了更有效地训练、评估和提供视觉表示的努力。我们为几个下游任务提供了消融研究，包括我们的视觉相似广告推荐。我们评估了所得视觉表示在下游任务中的离线性能。为此，我们提出了一种新的文本到图像生成的离线评估方法，用于视觉相似推荐。

    In this article, we present our approach to single-modality vision representation learning. Understanding vision representations of product content is vital for recommendations, search, and advertising applications in e-commerce. We detail and contrast techniques used to fine tune large-scale vision representation learning models in an efficient manner under low-resource settings, including several pretrained backbone architectures, both in the convolutional neural network as well as the vision transformer family. We highlight the challenges for e-commerce applications at-scale and highlight the efforts to more efficiently train, evaluate, and serve visual representations. We present ablation studies for several downstream tasks, including our visually similar ad recommendations. We evaluate the offline performance of the derived visual representations in downstream tasks. To this end, we present a novel text-to-image generative offline evaluation method for visually similar recommenda
    
[^156]: 基于因果推理的图神经网络的监督注意力：更好和更简单的选择，实现强大的关注力。

    Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention. (arXiv:2305.13115v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13115](http://arxiv.org/abs/2305.13115)

    这篇论文提出了一种基于因果推理的框架，通过直接监督注意力函数，提供了强大的监督信号，使得基于注意力的图神经网络在嘈杂的图表达中更加稳健和具有一般性。

    

    最近几年，注意力机制在图表示学习中展现了巨大的潜力。然而，虽然基于注意力的图神经网络的变体正在为许多现实世界的数据集设定新的基准，但最近的研究指出，由于缺乏直接监督，它们所产生的关注力对于嘈杂的图表达不够稳健和具有一般性。在本文中，我们提出了一个新的框架，利用因果性工具为注意力函数的学习过程提供强大的监督信号。具体而言，我们估计了注意力对于最终预测的直接因果效应，然后最大化该效应，引导注意力关注更有意义的邻居。我们的方法可以作为任何经典的基于注意力的图神经网络的即插即用模块，在端到端的方式下使用。广泛的实验在各种基准数据集上表明，通过直接监督注意力函数，模型能够更快地收敛并产生更清晰的结果。

    Recent years have witnessed the great potential of attention mechanism in graph representation learning. However, while variants of attention-based GNNs are setting new benchmarks for numerous real-world datasets, recent works have pointed out that their induced attentions are less robust and generalizable against noisy graphs due to lack of direct supervision. In this paper, we present a new framework which utilizes the tool of causality to provide a powerful supervision signal for the learning process of attention functions. Specifically, we estimate the direct causal effect of attention to the final prediction, and then maximize such effect to guide attention attending to more meaningful neighbors. Our method can serve as a plug-and-play module for any canonical attention-based GNNs in an end-to-end fashion. Extensive experiments on a wide range of benchmark datasets illustrated that, by directly supervising attention functions, the model is able to converge faster with a clearer de
    
[^157]: 具有概率保证的神经网络鲁棒的反事实解释

    Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v1 [stat.ML])

    [http://arxiv.org/abs/2305.11997](http://arxiv.org/abs/2305.11997)

    本文提出了一种可靠的神经网络反事实解释方法，该方法可以针对自然发生的模型变化提供高概率的鲁棒性。

    

    针对神经网络发现偏移，通过使用稳定性度量来量化反事实解释对可能的模型变化的鲁棒性。通过在反事实解释优化中引入正则化项来将生成的反事实解释靠近数据流形，从而实现了对自然发生的模型变化的高概率鲁棒性。新的算法在合成和现实世界数据集上进行实验，证明了其有效性。

    There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{<}\Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \emph{naturally-occurring} model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \emph{Stability} -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counter
    
[^158]: CB-HVTNet：一种用于组织病理学图像中淋巴细胞评估的通道增强混合视觉 Transformer 网络

    CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v1 [eess.IV])

    [http://arxiv.org/abs/2305.09211](http://arxiv.org/abs/2305.09211)

    CB-HVTNet 提出了一种 Channel Boosted Hybrid Vision Transformer 网络，利用迁移学习生成增强通道，并结合使用 Transformers 和 CNN，在组织病理学图像中高效准确地评估淋巴细胞。

    

    Transformer 由于其学习长距离依赖性的能力已经克服了卷积神经网络（CNN）全局透视学习的缺点。因此，它们已经成为研究人员关注的焦点，用于多个与视觉相关的任务，包括医疗诊断。然而，它们的多头注意模块仅捕获全局级别的特征表示，这对于医学图像来说是不足的。为了解决这个问题，我们提出了一种 Channel Boosted Hybrid Vision Transformer（CB HVT），它利用迁移学习生成增强通道，并使用 Transformers 和 CNN 来分析组织病理学图像中的淋巴细胞。所提出的 CB HVT 包括五个模块，包括通道生成模块、通道利用模块、通道合并模块、区域感知模块和检测和分段头，它们共同有效地识别淋巴细胞。通道生成模块使用通过迁移学习进行通道增强的思想创建多个强大的通道，然后与 Transformers 和 CNN 结合使用，以更好地分析组织病理学图像中的淋巴细胞。总体而言，所提出的 CB HVT 是医学诊断中准确、高效评估淋巴细胞的强大工具。

    Transformers, due to their ability to learn long range dependencies, have overcome the shortcomings of convolutional neural networks (CNNs) for global perspective learning. Therefore, they have gained the focus of researchers for several vision related tasks including medical diagnosis. However, their multi-head attention module only captures global level feature representations, which is insufficient for medical images. To address this issue, we propose a Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning to generate boosted channels and employs both transformers and CNNs to analyse lymphocytes in histopathological images. The proposed CB HVT comprises five modules, including a channel generation module, channel exploitation module, channel merging module, region-aware module, and a detection and segmentation head, which work together to effectively identify lymphocytes. The channel generation module uses the idea of channel boosting through transfer learni
    
[^159]: Meta-Polyp：高效息肉分割的基准线。

    Meta-Polyp: a baseline for efficient Polyp segmentation. (arXiv:2305.07848v1 [eess.IV])

    [http://arxiv.org/abs/2305.07848](http://arxiv.org/abs/2305.07848)

    本研究提出Meta-Polyp，将Meta-Former与UNet融合并引入多尺度上采样块和Convformer块，解决了CNN和Vision Transformer在处理分布外数据集、缺失边界和小息肉时遇到的困难，提高了息肉分割的效率。

    

    近年来，息肉分割变得越来越重要，并且许多方法利用CNN、Vision Transformer和Transformer技术开发以实现竞争性结果。然而，这些方法在处理分布外数据集、缺失边界和小息肉时经常遇到困难。在2022年，Meta-Former作为一种新的视觉基准线被引入，它不仅提高了多任务计算机视觉的性能，而且解决了Vision Transformer和CNN家族骨架的局限性。为了进一步增强分割，我们提出了Meta-Former与UNet的融合，同时在解码器阶段引入了多尺度上采样块与级联组合，以增强纹理；此外，我们提出了Convformer块，基于Meta-Former的思想，以加强局部特征的关键信息。这些块将全局信息（例如息肉的整体形状）与局部信息相结合，提高了进行息肉分割的效率。

    In recent years, polyp segmentation has gained significant importance, and many methods have been developed using CNN, Vision Transformer, and Transformer techniques to achieve competitive results. However, these methods often face difficulties when dealing with out-of-distribution datasets, missing boundaries, and small polyps. In 2022, Meta-Former was introduced as a new baseline for vision, which not only improved the performance of multi-task computer vision but also addressed the limitations of the Vision Transformer and CNN family backbones. To further enhance segmentation, we propose a fusion of Meta-Former with UNet, along with the introduction of a Multi-scale Upsampling block with a level-up combination in the decoder stage to enhance the texture, also we propose the Convformer block base on the idea of the Meta-former to enhance the crucial information of the local feature. These blocks enable the combination of global information, such as the overall shape of the polyp, wit
    
[^160]: 深度学习与逻辑推理的可扩展耦合

    Scalable Coupling of Deep Learning with Logical Reasoning. (arXiv:2305.07617v1 [cs.AI])

    [http://arxiv.org/abs/2305.07617](http://arxiv.org/abs/2305.07617)

    本文介绍了一种可扩展的神经网络模型和损失函数，能够有效学习如何解决NP-hard推理问题，并在离散图模型上进行了实验验证。同时可以提高数据效率和可解释性，并具有对预测的控制能力。

    

    在将离散推理与神经网络混合的不断探索中，出现了越来越多的对神经结构具备从自然输入中学习如何解决离散推理或优化问题的兴趣。本文提出了一种可扩展的神经结构以及专门用于学习被表示为离散图模型的 NP-hard 推理问题的约束和标准的损失函数。我们的损失函数解决了 Besag 的伪对数似然的主要限制之一，能够学习高能量函数。我们通过实验证明，它能够有效地从自然输入中学习如何解决 NP-hard 推理问题，如符号、视觉或多解数数独问题，以及蛋白质设计问题的能量优化形式，提高了数据效率、可解释性以及对预测的 \textit{a posteriori} 控制。

    In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs. In this paper, we introduce a scalable neural architecture and loss function dedicated to learning the constraints and criteria of NP-hard reasoning problems expressed as discrete Graphical Models. Our loss function solves one of the main limitations of Besag's pseudo-loglikelihood, enabling learning of high energies. We empirically show it is able to efficiently learn how to solve NP-hard reasoning problems from natural inputs as the symbolic, visual or many-solutions Sudoku problems as well as the energy optimization formulation of the protein design problem, providing data efficiency, interpretability, and \textit{a posteriori} control over predictions.
    
[^161]: 通用图上的自我排斥随机游走 - 通过非线性马尔可夫链实现最小采样方差

    Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear Markov Chains. (arXiv:2305.05097v1 [math.PR])

    [http://arxiv.org/abs/2305.05097](http://arxiv.org/abs/2305.05097)

    本文设计了一种自我排斥随机游走模型，可实现较小的渐近采样方差，适用于网络拓扑的采样和邻域探索。

    

    我们考虑在离散状态空间上的随机游走，例如一般的无向图，其中随机游走设计成通过采样和邻域探索来逼近网络拓扑上的目标量，以马尔可夫链蒙特卡罗 (MCMC) 程序的形式进行。对于任何相应于目标概率分布的马尔可夫链，我们设计了一种自我排斥随机游走 (SRRW)，它不太可能转移到过去被高度访问的节点，而更可能转移到很少被访问的节点。对于一类由正实数 {\alpha} 参数化的 SRRW，我们证明了该过程的经验分布几乎肯定收敛于底层马尔可夫链内核的目标 (平稳) 分布。然后我们提供了一个中心极限定理，并推导出所得到的渐近协方差矩阵的精确形式，这使我们能够表明，具有更强的排斥作用 (较大的 {\alpha}) 的 SRRW 一定比具有较弱的排斥作用 (较小的 {\alpha}) 的 SRRW 实现更小的渐近采样方差。

    We consider random walks on discrete state spaces, such as general undirected graphs, where the random walkers are designed to approximate a target quantity over the network topology via sampling and neighborhood exploration in the form of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain corresponding to a target probability distribution, we design a self-repellent random walk (SRRW) which is less likely to transition to nodes that were highly visited in the past, and more likely to transition to seldom visited nodes. For a class of SRRWs parameterized by a positive real {\alpha}, we prove that the empirical distribution of the process converges almost surely to the the target (stationary) distribution of the underlying Markov chain kernel. We then provide a central limit theorem and derive the exact form of the arising asymptotic co-variance matrix, which allows us to show that the SRRW with a stronger repellence (larger {\alpha}) always achieves a smaller asymptotic
    
[^162]: 人工智能模型筛查乳腺 X 光片的性能差距 -- 迈向公平和可解释的模型

    Performance Gaps of Artificial Intelligence Models Screening Mammography -- Towards Fair and Interpretable Models. (arXiv:2305.04422v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.04422](http://arxiv.org/abs/2305.04422)

    该研究探讨了乳腺筛查 X 光片异常分类模型中的性能差距，尤其是与人口学和成像特征之间的关系，旨在开发公平和可解释的模型。

    

    尽管深度学习模型在筛查乳腺 X 光片的异常分类中表现良好，与增加异常分类失败风险相关的人口学和成像特征仍不清楚。本回顾性研究使用 Emory BrEast Imaging Dataset（EMBED）的数据，包括2013年至2020年间 Emory University Healthcare 的 115,931 名患者的乳腺 X 光片。临床和成像数据包括乳腺成像报告和数据系统（BI-RADS）评估，异常区域的兴趣点坐标，成像特征，病理结果和患者人口统计学。开发了 InceptionV3、VGG16、ResNet50V2 和 ResNet152V2 等深度学习模型，用于区分筛查乳腺 X 光片中异常组织区域和随机选择的正常组织区域。训练集、验证集和测试集的分布为 29,144（55.6%）个异常组织区域和来自 10,678（54.2%）位患者的膨胀组织区域。

    Even though deep learning models for abnormality classification can perform well in screening mammography, the demographic and imaging characteristics associated with increased risk of failure for abnormality classification in screening mammograms remain unclear. This retrospective study used data from the Emory BrEast Imaging Dataset (EMBED) including mammograms from 115,931 patients imaged at Emory University Healthcare between 2013 to 2020. Clinical and imaging data includes Breast Imaging Reporting and Data System (BI-RADS) assessment, region of interest coordinates for abnormalities, imaging features, pathologic outcomes, and patient demographics. Deep learning models including InceptionV3, VGG16, ResNet50V2, and ResNet152V2 were developed to distinguish between patches of abnormal tissue and randomly selected patches of normal tissue from the screening mammograms. The distributions of the training, validation and test sets are 29,144 (55.6%) patches of 10,678 (54.2%) patients, 9,
    
[^163]: 利用不确定性感知因果模型提高基于图像的精准医疗

    Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models. (arXiv:2305.03829v1 [cs.LG])

    [http://arxiv.org/abs/2305.03829](http://arxiv.org/abs/2305.03829)

    本研究采用贝叶斯深度学习估计多种治疗的因果后验分布，提高了基于图像的精准医疗的不确定性估计方法，以预测噪声多的医疗环境下的个体治疗效果。

    

    基于图像的精准医疗旨在根据个体的独特成像特征个性化诊疗决策，以改善其临床结果。集成不确定性估计作为治疗建议的机器学习框架将更加安全可靠。然而，在精准医疗中，几乎没有研究适应不确定性估计技术和验证指标。本文采用贝叶斯深度学习估计多种治疗的因果后验分布，从而估计每种治疗选项的不确定性以及任意两种治疗之间的个体治疗效果。我们对患有多发性硬化症的患者进行训练和评估，以预测新的和扩大的T2病变数量，并评估模型的相关性。

    Image-based precision medicine aims to personalize treatment decisions based on an individual's unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects (ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of 
    
[^164]: 评分差值流模型用于隐式生成建模

    The Score-Difference Flow for Implicit Generative Modeling. (arXiv:2304.12906v1 [cs.LG])

    [http://arxiv.org/abs/2304.12906](http://arxiv.org/abs/2304.12906)

    本文提出了一种新的评分差异流模型(SD flow)，它可以最优地减少两个分布之间的散度，同时解决Schr​​ödinger桥问题。与去噪扩散模型不同，它没有对先验分布施加任何限制，在一些基准数据集中优于其他方法。

    

    隐式生成建模(IGM)旨在生成符合目标数据分布特征的合成数据样本。最近的研究(例如评分匹配网络、扩散模型)从通过环境空间中的动态扰动或流将合成源数据推向目标分布的角度解决了IGM问题。我们引入了任意目标和源分布之间的评分差异(SD)作为流，它可以最优地减少它们之间的Kullback-Leibler散度，同时解决Schr​​ödinger桥问题。我们将SD流应用于方便的代理分布，当且仅当原始分布对齐时，它们是对齐的。我们在某些条件下展示了这种公式与去噪扩散模型的形式一致性。然而，与扩散模型不同，SD流没有对先验分布施加任何限制。我们还表明，在无限辨别器能力的极限下，生成对抗网络的训练包含SD流。我们的实验表明，SD流在几个基准数据集上优于先前的最新技术。

    Implicit generative modeling (IGM) aims to produce samples of synthetic data matching the characteristics of a target data distribution. Recent work (e.g. score-matching networks, diffusion models) has approached the IGM problem from the perspective of pushing synthetic source data toward the target distribution via dynamical perturbations or flows in the ambient space. We introduce the score difference (SD) between arbitrary target and source distributions as a flow that optimally reduces the Kullback-Leibler divergence between them while also solving the Schr\"odinger bridge problem. We apply the SD flow to convenient proxy distributions, which are aligned if and only if the original distributions are aligned. We demonstrate the formal equivalence of this formulation to denoising diffusion models under certain conditions. However, unlike diffusion models, SD flow places no restrictions on the prior distribution. We also show that the training of generative adversarial networks includ
    
[^165]: 基于随机专家的医学图像分割的隐性解剖渲染

    Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts. (arXiv:2304.03209v1 [cs.CV])

    [http://arxiv.org/abs/2304.03209](http://arxiv.org/abs/2304.03209)

    提出了一种名为MORSE的基于隐式解剖渲染的通用神经渲染框架，旨在在医学图像分割中帮助融合高级语义相关内容和低级解剖特征。

    

    将高级语义相关内容和低级解剖特征集成到医学图像分割中非常重要。近期基于深度学习的医学分割方法在更好地建模这些信息方面取得了很大的成功。然而，医学分割的卷积操作通常在规则网格上运行，这在高频区域即边界区域中天生模糊。本文提出了一个名为MORSE的通用隐式神经渲染框架，旨在在解剖层面上为医学图像分割辅助学习。我们的方法基于事实：相较于离散的基于网格的表示方式，隐式神经表示在拟合复杂信号和解决计算机图形问题时表现更为有效。我们的方法的核心是以端到端的方式将医学图像分割视为渲染问题。具体而言，我们持续地对齐粗略的分割p并利用随机专家来生成渲染图像。

    Integrating high-level semantically correlated contents and low-level anatomical features is of central importance in medical image segmentation. Towards this end, recent deep learning-based medical segmentation methods have shown great promise in better modeling such information. However, convolution operators for medical segmentation typically operate on regular grids, which inherently blur the high-frequency regions, i.e., boundary regions. In this work, we propose MORSE, a generic implicit neural rendering framework designed at an anatomical level to assist learning in medical image segmentation. Our method is motivated by the fact that implicit neural representation has been shown to be more effective in fitting complex signals and solving computer graphics problems than discrete grid-based representation. The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. Specifically, we continuously align the coarse segmentation p
    
[^166]: ACTION++：使用自适应解剖对比度改善半监督医学图像分割

    ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v1 [cs.CV])

    [http://arxiv.org/abs/2304.02689](http://arxiv.org/abs/2304.02689)

    本文提出了一种改进的对比学习框架ACTION++，通过自适应的解剖对比来改善半监督医学图像分割。

    

    医学数据通常表现为长尾分布，存在严重的类别不平衡，这自然导致少数类别（即边界区域或罕见物体）的分类困难。最近的工作通过配备无监督对比标准，在长尾场景中显着改进了半监督医学图像分割。然而，在类别分布也高度不平衡的标记数据部分中，它们的表现仍不清楚。在这项工作中，我们提出了ACTION++，一种改进的具有自适应解剖对比的对比学习框架，用于半监督医学分割。

    Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes (i.e., boundary regions or rare objects). Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced. In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed cla
    
[^167]: FakET: 利用神经风格迁移模拟冷冻电子断层图像

    FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer. (arXiv:2304.02011v1 [cs.LG])

    [http://arxiv.org/abs/2304.02011](http://arxiv.org/abs/2304.02011)

    本文提出了一种使用加性噪声和神经风格迁移技术来模拟电子显微镜正向算子，以解决深度学习方法需要大量训练数据集的问题。该方法在粒子定位和分类任务上表现良好。

    

    粒子定位和分类是计算显微学中最基本的问题之一。近年来，深度学习方法在这些任务中取得了巨大成功。这些监督式学习方法的一个关键缺点是它们需要大量的训练数据集，通常是与模拟透射电子显微镜物理的复杂数值正向模型中的粒子模型结合生成的。这些模型的计算机实现非常耗费计算资源，限制了它们的适用范围。本文提出了一种基于加性噪声和神经风格迁移技术模拟电子显微镜正向算子的简单方法。我们使用目前最先进的已经建立的状态之一对定位和分类任务进行评估，显示出与基准测试相当的性能。与以前的方法不同，我们的方法加速了运算，显著减少了计算成本。

    Particle localization and -classification constitute two of the most fundamental problems in computational microscopy. In recent years, deep learning based approaches have been introduced for these tasks with great success. A key shortcoming of these supervised learning methods is their need for large training data sets, typically generated from particle models in conjunction with complex numerical forward models simulating the physics of transmission electron microscopes. Computer implementations of such forward models are computationally extremely demanding and limit the scope of their applicability. In this paper we propose a simple method for simulating the forward operator of an electron microscope based on additive noise and Neural Style Transfer techniques. We evaluate the method on localization and classification tasks using one of the established state-of-the-art architectures showing performance on par with the benchmark. In contrast to previous approaches, our method acceler
    
[^168]: 用于识别TBI生理状态的多元时间序列数据的自监督聚类

    Self-Supervised Clustering of Multivariate Time-Series Data for Identifying TBI Physiological States. (arXiv:2303.13024v1 [cs.LG])

    [http://arxiv.org/abs/2303.13024](http://arxiv.org/abs/2303.13024)

    这篇论文提出了一种新的自监督聚类算法，能够在多元时间序列数据中确定并识别对于TBI等急性疾病治疗非常重要的生理状态。研究还利用临床数据验证并解释所识别的生理状态。

    

    对于具有缺失值的多元时间序列数据确定临床相关的生理状态非常重要，这对于提供急性疾病（如颅脑损伤、呼吸衰竭和心力衰竭）的适当治疗至关重要。利用非时间序列聚类或数据插值和聚合技术可能导致有价值信息的丢失和偏见分析。在本研究中，我们应用了基于自监督的SLAC-Time算法，避免了插值或聚合，从而更有效地表示急性患者状态。通过使用SLAC-Time来聚类大型研究数据集中的数据，我们确定了三种不同的TBI生理状态及其具体特征。我们采用了各种聚类评估指标，并结合临床领域专家的意见来验证和解释所识别的生理状态。此外，我们发现了特定临床事件和生理状态之间的关系。

    Determining clinically relevant physiological states from multivariate time series data with missing values is essential for providing appropriate treatment for acute conditions such as Traumatic Brain Injury (TBI), respiratory failure, and heart failure. Utilizing non-temporal clustering or data imputation and aggregation techniques may lead to loss of valuable information and biased analyses. In our study, we apply the SLAC-Time algorithm, an innovative self-supervision-based approach that maintains data integrity by avoiding imputation or aggregation, offering a more useful representation of acute patient states. By using SLAC-Time to cluster data in a large research dataset, we identified three distinct TBI physiological states and their specific feature profiles. We employed various clustering evaluation metrics and incorporated input from a clinical domain expert to validate and interpret the identified physiological states. Further, we discovered how specific clinical events and
    
[^169]: 使用Follow-the-regularized-leader算法的线性bandits问题的三重世界分析

    Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm. (arXiv:2303.06825v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06825](http://arxiv.org/abs/2303.06825)

    本文提出了一种基于Follow-the-regularized-leader算法的三重世界分析，并证明该算法使用负熵正则化器可以在线性bandit问题中获得最佳结果。

    

    线性bandit问题在随机和对抗环境中已经研究了很多年。设计一个可以在不知道损失类型的情况下优化环境的算法，引起了很多兴趣。本文提出了一种算法，通过主动检测损失类型，然后在针对特定环境设计的不同算法之间切换。然而，这种方法需要精心设计以在所有环境中表现良好。Follow-the-regularized-leader（FTRL）是另一种流行的算法类型，可以适应不同的环境。与检测并切换类型相比，该算法设计简单，遗憾界限在传统的多臂赌博问题中被证明是最优的。为线性bandit设计一种FTRL类型的算法是一个长期存在的重要问题。本文证明了使用负熵正则化器的FTRL算法可以实现线性bandit问题的最佳三重世界结果。

    The linear bandit problem has been studied for many years in both stochastic and adversarial settings. Designing an algorithm that can optimize the environment without knowing the loss type attracts lots of interest. \citet{LeeLWZ021} propose an algorithm that actively detects the loss type and then switches between different algorithms specially designed for specific settings. However, such an approach requires meticulous designs to perform well in all environments. Follow-the-regularized-leader (FTRL) is another type of popular algorithm that can adapt to different environments. This algorithm is of simple design and the regret bounds are shown to be optimal in traditional multi-armed bandit problems compared with the detect-switch type. Designing an FTRL-type algorithm for linear bandits is an important question that has been open for a long time. In this paper, we prove that the FTRL algorithm with a negative entropy regularizer can achieve the best-of-three-world results for the l
    
[^170]: 机器学习增强的Hankel动态模态分解

    Machine Learning Enhanced Hankel Dynamic-Mode Decomposition. (arXiv:2303.06289v1 [cs.LG])

    [http://arxiv.org/abs/2303.06289](http://arxiv.org/abs/2303.06289)

    本文提出了一种基于深度学习DMD的方法，称为DLHDMD，利用Takens嵌入定理的基本见解开发了一种自适应学习方案，更好地捕捉了高维和混沌动力学，能够为混沌时间序列生成准确的动态。

    This paper proposes a deep learning DMD based method, called DLHDMD, which uses the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics, and is able to generate accurate dynamics for chaotic time series.

    尽管时间序列的获取变得越来越简单和复杂，但从时间序列中开发动态模型仍然是一个具有挑战性和不断发展的问题领域。在过去几年中，为了解决这个问题，机器学习工具已经与所谓的动态模态分解（DMD）相结合。这种通用方法已被证明是一个特别有前途的精密和准确的模型开发途径。在此基础上，我们开发了一种基于深度学习DMD的方法，利用Takens嵌入定理的基本见解开发了一种自适应学习方案，更好地捕捉了高维和混沌动力学。我们称这种方法为深度学习Hankel DMD（DLHDMD）。我们展示了DLHDMD能够为混沌时间序列生成准确的动态，并探讨了我们的方法如何学习映射，这些映射在成功训练后往往趋向于显著的特征。

    While the acquisition of time series has become increasingly more straightforward and sophisticated, developing dynamical models from time series is still a challenging and ever evolving problem domain. Within the last several years, to address this problem, there has been a merging of machine learning tools with what is called the dynamic mode decomposition (DMD). This general approach has been shown to be an especially promising avenue for sophisticated and accurate model development. Building on this prior body of work, we develop a deep learning DMD based method which makes use of the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics. We call this method the Deep Learning Hankel DMD (DLHDMD). We show that the DLHDMD is able to generate accurate dynamics for chaotic time series, and we likewise explore how our method learns mappings which tend, after successful training, to significant
    
[^171]: SLCA: 预训练模型上用于连续学习的慢学习者与分类器对齐

    SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05118](http://arxiv.org/abs/2303.05118)

    SLCA是一种用于连续学习的简单但极其有效的方法。它通过慢学习和分类器对齐来在预训练模型上提高泛化能力和解决渐进过拟合问题。

    

    连续学习的目标是在学习顺序到达的数据中提高识别模型的性能。尽管大部分现有工作都建立在从头学习的前提下，但越来越多的努力已经致力于融入预训练的好处。然而，如何在每个增量任务中自适应地利用预训练的知识，同时保持其泛化能力，仍然是一个未解决的问题。在这项工作中，我们对预训练模型上的连续学习进行了广泛的分析，并将关键挑战归因于渐进过拟合问题。观察到在表征层次上选择性降低学习率几乎可以解决这个问题，我们提出了一种简单但极其有效的方法，名为慢学习者与分类器对齐（SLCA），通过建模类别分布并在事后对齐分类层次，进一步改进了分类层次。在各种实验中，我们证明了SLCA在连续学习任务中的有效性和性能优势。

    The goal of continual learning is to improve the performance of recognition models in learning sequentially arrived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training. However, how to adaptively exploit the pre-trained knowledge for each incremental task while maintaining its generalizability remains an open question. In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selectively reducing the learning rate can almost resolve this issue in the representation layer, we propose a simple but extremely effective approach named Slow Learner with Classifier Alignment (SLCA), which further improves the classification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fashion. Across a variety o
    
[^172]: 自行承担编辑风险：评估经过分布转移的模型的鲁棒性

    Edit at your own risk: evaluating the robustness of edited models to distribution shifts. (arXiv:2303.00046v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00046](http://arxiv.org/abs/2303.00046)

    本文研究了模型编辑对模型的鲁棒性的影响，并发现编辑通常会降低模型的鲁棒性，但具体程度取决于编辑算法和层的选择。基于这些发现，引入了一种新的模型编辑算法，通过插值不同层的特征来提高模型的鲁棒性。

    

    不断增大模型的趋势使得标准的重新训练过程变得越来越昂贵。因此，对于能够在后期进行解释性的、计算便宜的模型修改的方法越来越感兴趣。虽然许多模型编辑技术很有前景，但对于编辑模型的属性的研究主要仅限于验证准确性的评估。编辑模型的鲁棒性是一个重要但很少被探索的主题。在本文中，我们采用最近从深度学习鲁棒性领域开发的技术，研究模型编辑如何影响模型的普遍鲁棒性，以及编辑所针对的特定行为的鲁棒性。我们发现编辑倾向于降低普遍鲁棒性，但受编辑算法和选择的层的影响程度不同。在这些观察的基础上，我们引入了一种新的模型编辑算法，1层插值(1-LI)，它通过组合不同层的特征进行插值来编辑模型，这在一定程度上提高了模型的鲁棒性。

    The current trend toward ever-larger models makes standard retraining procedures an ever-more expensive burden. For this reason, there is growing interest in model editing, which enables computationally inexpensive, interpretable, post-hoc model modifications. While many model editing techniques are promising, research on the properties of edited models is largely limited to evaluation of validation accuracy. The robustness of edited models is an important and yet mostly unexplored topic. In this paper, we employ recently developed techniques from the field of deep learning robustness to investigate both how model editing affects the general robustness of a model, as well as the robustness of the specific behavior targeted by the edit. We find that edits tend to reduce general robustness, but that the degree of degradation depends on the editing algorithm and layers chosen. Motivated by these observations we introduce a new model editing algorithm, 1-layer interpolation (1-LI), which u
    
[^173]: 高概率界限的随机优化和变分不等式：无界方差情况下的讨论

    High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance. (arXiv:2302.00999v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.00999](http://arxiv.org/abs/2302.00999)

    本文提出了几种算法，它们在梯度/算子噪声具有有界中心的 α 阶矩的宽松假设下具备高概率收敛性。这些算法适用于光滑非凸、Polyak-Lojasiewicz、凸、强凸、拟强凸最小化问题以及Lipschitz、星形强协同且单调、拟协同的问题。

    

    在最近几年，优化和机器学习界对随机优化方法的高概率收敛性越来越感兴趣。其中一个主要原因是高概率复杂度界限比期望复杂度界限更准确且需求较少。然而，现有的高概率非渐近收敛结果都是在假设梯度噪声的方差或目标的梯度本身是有界的情况下得到的。本文提出了几种算法，它们在较宽松的假设下具备高概率收敛性。特别地，我们在以下情况下推导了梯度/算子噪声具有有界中心的α阶矩（其中α∈(1,2]）的情况下的高概率收敛性：（i）光滑非凸/Polyak-Lojasiewicz/凸/强凸/拟强凸最小化问题，（ii）Lipschitz/星形强协同且单调/拟协同的问题。

    During recent years the interest of optimization and machine learning communities in high-probability convergence of stochastic optimization methods has been growing. One of the main reasons for this is that high-probability complexity bounds are more accurate and less studied than in-expectation ones. However, SOTA high-probability non-asymptotic convergence results are derived under strong assumptions such as the boundedness of the gradient noise variance or of the objective's gradient itself. In this paper, we propose several algorithms with high-probability convergence results under less restrictive assumptions. In particular, we derive new high-probability convergence results under the assumption that the gradient/operator noise has bounded central $\alpha$-th moment for $\alpha \in (1,2]$ in the following setups: (i) smooth non-convex / Polyak-Lojasiewicz / convex / strongly convex / quasi-strongly convex minimization problems, (ii) Lipschitz / star-cocoercive and monotone / quas
    
[^174]: 鲁棒的在线主动学习策略

    Robust online active learning. (arXiv:2302.00422v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00422](http://arxiv.org/abs/2302.00422)

    本文提出了一种自适应方法，用于鲁棒的在线主动学习，并在受污染的数据流中证明了其性能表现优异，同时确保了稳定性并减少异常值的负面影响。

    

    在许多工业应用中，获得标记的观测数据并不简单，通常需要人工专家干预或使用昂贵的测试设备。在这种情况下，主动学习可以大大提高拟合模型时最信息数据点的建议。减少模型开发所需的观测数据数量可以减轻训练所需的计算负担和标记相关的操作支出。特别是在线主动学习，在需要在极短时间内决定是否获取数据点标记的高容量生产过程中非常有用。然而，尽管最近致力于开发在线主动学习策略，但在存在异常值的情况下这些方法的行为仍未得到彻底研究。在这项工作中，我们调查了在线主动线性回归在受污染的数据流中的性能，并提出了一种自适应方法，用于鲁棒的在线主动学习，同时保证稳定性并减少异常值的负面影响。

    In many industrial applications, obtaining labeled observations is not straightforward as it often requires the intervention of human experts or the use of expensive testing equipment. In these circumstances, active learning can be highly beneficial in suggesting the most informative data points to be used when fitting a model. Reducing the number of observations needed for model development alleviates both the computational burden required for training and the operational expenses related to labeling. Online active learning, in particular, is useful in high-volume production processes where the decision about the acquisition of the label for a data point needs to be taken within an extremely short time frame. However, despite the recent efforts to develop online active learning strategies, the behavior of these methods in the presence of outliers has not been thoroughly examined. In this work, we investigate the performance of online active linear regression in contaminated data strea
    
[^175]: 内部奖励的强化学习

    Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00270](http://arxiv.org/abs/2302.00270)

    这项研究探讨了一类强化学习问题，其中策略的奖励信号由与之相关且同时优化的判别器生成，导致学习过程不稳定。实验结果表明，修剪线性奖励函数可以稳定训练过程。

    

    我们研究了一类强化学习问题，其中用于策略学习的奖励信号由一个与策略相关且与策略同时优化的判别器生成。策略和判别器之间的相互依赖导致了不稳定的学习过程，因为来自不成熟判别器的奖励信号是嘈杂的，阻碍了策略的学习；反过来，未经优化的策略也会阻碍判别器的学习。我们将这种学习设置称为“内部奖励的强化学习”（IRRL），因为奖励不是直接来自环境，而是由判别器“内部”提供的。在本文中，我们正式地表述了IRRL，并提出了一类属于IRRL的问题。我们从理论上推导并经验性地分析了IRRL中奖励函数的影响，并基于这些分析提出了修剪线性奖励函数。实验结果表明，所提出的奖励函数可以持续稳定训练过程。

    We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the tra
    
[^176]: 使用深度强化学习的基于执行的代码生成

    Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13816](http://arxiv.org/abs/2301.13816)

    使用深度强化学习的PPOCoder框架将预训练的编程语言模型和Proximal Policy Optimization技术结合，通过利用代码执行和结构对齐的非可微反馈，实现了更高效的代码生成。

    

    利用在大规模代码语料库上预训练的编程语言（PL）模型，作为自动化软件工程过程的手段，在代码完成、代码翻译和程序合成等各种代码生成任务中表现出了相当的潜力。然而，当前的方法主要依赖于从文本生成中借用的监督微调目标，忽视了代码的独特序列级特征，包括但不限于可编译性以及语法和功能正确性。为了解决这个限制，我们提出了PPOCoder，一种新的代码生成框架，它将预训练的PL模型与Proximal Policy Optimization（PPO）相结合，PPO是一种广泛使用的深度强化学习技术。通过利用代码执行和结构对齐的非可微反馈，PPOCoder将外部代码特定知识无缝集成到模型优化过程中。这是重要的。

    The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important
    
[^177]: 使用非可分离哈密顿量的深度学习解决均场博弈问题

    Deep Learning for Mean Field Games with non-separable Hamiltonians. (arXiv:2301.02877v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02877](http://arxiv.org/abs/2301.02877)

    本文介绍了一种使用深度学习方法解决非可分离哈密顿量的高维均场博弈问题的新方法。通过使用两个神经网络来近似未知解和前向-后向条件，该方法在效率上具有优势，并且能够处理高达300维的问题。该方法还与现有方法进行了比较，并在交通流问题上展示了其有效性。同时，通过使用通用逼近定理，证明了使用单层隐藏层的神经网络近似的收敛性。

    

    本文介绍了一种基于深度伽辽金方法（DGMs）的新方法，用于解决高维随机均场博弈（MFGs）问题。我们通过使用两个神经网络来近似MFG系统和前向-后向条件的未知解来实现这一目标。我们的方法高效，即使迭代次数较少，也能处理高达300维的问题，速度更快于其他方法。相比之下，基于生成对抗网络（GAN）的方法无法解决具有非可分离哈密顿量的MFG问题。我们通过将我们的方法应用于交通流问题来证明我们方法的有效性，以前该问题仅在确定性情况下使用牛顿迭代方法解决。我们将我们的方法的结果与解析解和先前的方法进行了比较，展示了其高效性。我们还使用通用逼近定理证明了我们的神经网络近似的收敛性，其中仅使用了单层隐藏层。

    This paper introduces a new method based on Deep Galerkin Methods (DGMs) for solving high-dimensional stochastic Mean Field Games (MFGs). We achieve this by using two neural networks to approximate the unknown solutions of the MFG system and forward-backward conditions. Our method is efficient, even with a small number of iterations, and is capable of handling up to 300 dimensions with a single layer, which makes it faster than other approaches. In contrast, methods based on Generative Adversarial Networks (GANs) cannot solve MFGs with non-separable Hamiltonians. We demonstrate the effectiveness of our approach by applying it to a traffic flow problem, which was previously solved using the Newton iteration method only in the deterministic case. We compare the results of our method to analytical solutions and previous approaches, showing its efficiency. We also prove the convergence of our neural network approximation with a single hidden layer using the universal approximation theorem.
    
[^178]: 《随机凸优化中梯度下降方法的信息论泛化界限的局限性》

    Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization. (arXiv:2212.13556v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13556](http://arxiv.org/abs/2212.13556)

    本研究通过考虑多种信息论框架，证明了在随机凸优化领域中，没有一个"信息论"框架能够建立梯度下降的最小最大速率。同时，通过分析一种常见的策略，我们证明了高斯噪声破坏迭代的方法也无法建立最小最大速率。这些结果表明，需要新的思路来分析使用信息论技术的梯度下降方法。

    

    至今为止，在随机凸优化设置中，没有证明“信息论”框架能够建立梯度下降的最小最大速率以推断泛化误差。在本研究中，我们考虑通过几种现有的信息论框架来建立这样的速率：输入-输出互信息界限、条件互信息界限及其变体、PAC-Bayes界限和最近的条件变体。我们证明了这些界限均无法建立最小最大速率。然后，我们考虑了在研究梯度方法中常用的一种策略，即通过高斯噪声破坏最终的迭代，从而产生噪声的“代理”算法。我们证明无法通过对这些代理算法的分析建立最小最大速率。我们的结果表明，需要新的思路来分析使用信息论技术的梯度下降方法。

    To date, no "information-theoretic" frameworks for reasoning about generalization error have been shown to establish minimax rates for gradient descent in the setting of stochastic convex optimization. In this work, we consider the prospect of establishing such rates via several existing information-theoretic frameworks: input-output mutual information bounds, conditional mutual information bounds and variants, PAC-Bayes bounds, and recent conditional variants thereof. We prove that none of these bounds are able to establish minimax rates. We then consider a common tactic employed in studying gradient methods, whereby the final iterate is corrupted by Gaussian noise, producing a noisy "surrogate" algorithm. We prove that minimax rates cannot be established via the analysis of such surrogates. Our results suggest that new ideas are required to analyze gradient descent using information-theoretic techniques.
    
[^179]: EEG解码的深度黎曼网络

    Deep Riemannian Networks for EEG Decoding. (arXiv:2212.10426v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10426](http://arxiv.org/abs/2212.10426)

    本研究分析了深度黎曼网络对EEG的应用，探讨了网络大小、端到端能力、模型训练对模型性能的影响，并比较了其与基于黎曼几何的最先进方法。

    

    当前在电脑脑电图（EEG）解码任务中，最先进的性能通常是由深度学习或基于黎曼几何的解码器实现的。最近，越来越多的人对深度黎曼网络（DRNs）产生了兴趣，可能结合了之前两类方法的优点。然而，还有一系列问题需要进一步洞察，以铺平DRNs在EEG中更广泛应用的道路。这些问题包括架构设计问题，如网络大小和端到端能力，以及模型训练问题。这些因素如何影响模型性能尚未被探索。此外，这些网络中的数据如何转换，以及是否与传统的EEG解码相关也不清楚。本研究旨在通过分析具有广泛超参数的DRNs来奠定这些主题领域的基础。使用两个公共EEG数据集测试了网络，并与最先进的基于黎曼几何的方法进行了比较。

    State-of-the-art performance in electroencephalography (EEG) decoding tasks is currently often achieved with either Deep-Learning or Riemannian-Geometry-based decoders. Recently, there is growing interest in Deep Riemannian Networks (DRNs) possibly combining the advantages of both previous classes of methods. However, there are still a range of topics where additional insight is needed to pave the way for a more widespread application of DRNs in EEG. These include architecture design questions such as network size and end-to-end ability as well as model training questions. How these factors affect model performance has not been explored. Additionally, it is not clear how the data within these networks is transformed, and whether this would correlate with traditional EEG decoding. Our study aims to lay the groundwork in the area of these topics through the analysis of DRNs for EEG with a wide range of hyperparameters. Networks were tested on two public EEG datasets and compared with sta
    
[^180]: 基于漏斗的强化学习中信号时态逻辑任务的奖励塑形

    Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning. (arXiv:2212.03181v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.03181](http://arxiv.org/abs/2212.03181)

    本文提出了一种基于漏斗函数的强化学习算法，用于在连续状态空间中学习鲁棒满足信号时态逻辑规范的时间依赖策略。

    

    信号时态逻辑（STL）是描述动态系统复杂时态和逻辑行为的强大框架。许多研究尝试利用强化学习来学习强制执行STL规范的控制器，然而，他们无法有效解决在连续状态空间中确保鲁棒满足和保持可控性的挑战。本文借助漏斗函数的概念，提出了一种可控的强化学习算法，用于学习连续状态空间中STL规范的鲁棒满足的时间依赖策略。我们在不同环境下演示了我们方法的实用性。

    Signal Temporal Logic (STL) is a powerful framework for describing the complex temporal and logical behaviour of the dynamical system. Numerous studies have attempted to employ reinforcement learning to learn a controller that enforces STL specifications; however, they have been unable to effectively tackle the challenges of ensuring robust satisfaction in continuous state space and maintaining tractability. In this paper, leveraging the concept of funnel functions, we propose a tractable reinforcement learning algorithm to learn a time-dependent policy for robust satisfaction of STL specification in continuous state space. We demonstrate the utility of our approach on several STL tasks using different environments.
    
[^181]: 面向稀有事件的动态因果发现：一种非参数条件独立性检验

    Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test. (arXiv:2211.16596v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.16596](http://arxiv.org/abs/2211.16596)

    该论文提出了一种针对稀有事件的新的因果发现方法，基于收集到的时间不变动态系统的数据，构建了叠加数据集和条件独立性检验的方法。该方法能够揭示在变量第一次经历低概率实现时才会显现的因果关系，具有很好的可行性和可扩展性。

    

    与稀有事件相关联的因果现象在许多工程问题中都存在，例如针对风险的安全分析、事故分析和预防以及极值理论等。然而，当前的因果发现方法往往无法发现在随机变量之间的原因联系，特别是在变动环境下，仅在变量第一次经历低概率实现时才会显现。为了解决这个问题，我们引入了一种新的统计独立性检验方法，用于从发生稀有但具有重要影响的时间不变动态系统收集的数据中进行因果探索。具体而言，我们利用底层数据的时间不变性来构建一个叠加的数据集，其中包括在不同时间步骤之前稀有事件发生前系统状态的数据。然后我们设计了一个在重新组织的数据上进行条件独立性检验的方法。我们提供了我们方法一致性的非渐近样本复杂度界限，并验证了它在各种模拟和真实世界数据集上的性能。

    Causal phenomena associated with rare events occur across a wide range of engineering problems, such as risk-sensitive safety analysis, accident analysis and prevention, and extreme value theory. However, current methods for causal discovery are often unable to uncover causal links, between random variables in a dynamic setting, that manifest only when the variables first experience low-probability realizations. To address this issue, we introduce a novel statistical independence test on data collected from time-invariant dynamical systems in which rare but consequential events occur. In particular, we exploit the time-invariance of the underlying data to construct a superimposed dataset of the system state before rare events happen at different timesteps. We then design a conditional independence test on the reorganized data. We provide non-asymptotic sample complexity bounds for the consistency of our method, and validate its performance across various simulated and real-world datase
    
[^182]: 资源节约的量子机器学习优化器

    Resource frugal optimizer for quantum machine learning. (arXiv:2211.04965v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.04965](http://arxiv.org/abs/2211.04965)

    提出了一种名为Refoqus的资源节约的量子随机梯度下降优化器，通过同时随机采样数据集和测量操作，能够保存大量资源。

    

    量子增强的数据科学，也称为量子机器学习（QML），作为近期量子计算机的应用越来越受关注。变分QML算法在涉及量子数据时有能力解决实际问题。然而，训练这些算法可能具有挑战性，并需要定制的优化程序。具体而言，QML应用可能需要大量的采样次数，因为涉及大型数据集。在这项工作中，我们提倡对数据集和定义损失函数的测量操作进行同时随机采样。我们考虑了一个高度通用的损失函数，包括了许多QML应用，并展示了如何构建其梯度的无偏估计器。这使我们能够提出一种称为Refoqus（资源节约的量子随机梯度下降优化器）的节约采样梯度下降优化器。我们的数值结果表明，Refoqus能够节省几个数量级的资源。

    Quantum-enhanced data science, also known as quantum machine learning (QML), is of growing interest as an application of near-term quantum computers. Variational QML algorithms have the potential to solve practical problems on real hardware, particularly when involving quantum data. However, training these algorithms can be challenging and calls for tailored optimization procedures. Specifically, QML applications can require a large shot-count overhead due to the large datasets involved. In this work, we advocate for simultaneous random sampling over both the dataset as well as the measurement operators that define the loss function. We consider a highly general loss function that encompasses many QML applications, and we show how to construct an unbiased estimator of its gradient. This allows us to propose a shot-frugal gradient descent optimizer called Refoqus (REsource Frugal Optimizer for QUantum Stochastic gradient descent). Our numerics indicate that Refoqus can save several orde
    
[^183]: 基于观测器的逆强化学习中的非唯一性和等价解的收敛性研究

    Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning. (arXiv:2210.16299v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.16299](http://arxiv.org/abs/2210.16299)

    本文研究了在线、实时解决逆强化学习中存在的多个解的挑战，提出了一种能够收敛到近似等价解的正则化历史堆栈观察器。通过开发新的数据丰富性条件，证明了该技术的有效性。

    

    在在线和实时解决确定性逆强化学习问题中，存在多个解的是一个关键挑战。非唯一性需要研究等价解的概念，即结果在不同的代价函数但相同的反馈矩阵，以及收敛到这些解的方法。尽管已经在文献中开发了离线算法以收敛到等价解，但尚未提供解决非唯一性的在线、实时技术。本文提出了一种能够收敛到逆强化学习问题的近似等价解的正则化历史堆栈观察器。发展了新的数据丰富性条件以促进分析，并通过模拟结果展示了所开发技术的有效性。

    A key challenge in solving the deterministic inverse reinforcement learning (IRL) problem online and in real-time is the existence of multiple solutions. Nonuniqueness necessitates the study of the notion of equivalent solutions, i.e., solutions that result in a different cost functional but same feedback matrix, and convergence to such solutions. While offline algorithms that result in convergence to equivalent solutions have been developed in the literature, online, real-time techniques that address nonuniqueness are not available. In this paper, a regularized history stack observer that converges to approximately equivalent solutions of the IRL problem is developed. Novel data-richness conditions are developed to facilitate the analysis and simulation results are provided to demonstrate the effectiveness of the developed technique.
    
[^184]: 使用多分辨率的异质时间系列集合表示进行热需求预测

    Heat Demand Forecasting with Multi-Resolutional Representation of Heterogeneous Temporal Ensemble. (arXiv:2210.13108v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13108](http://arxiv.org/abs/2210.13108)

    本文提出了一种使用多分辨率异质时间序列集合表示的热需求预测框架，该框架通过神经网络对时间序列进行编码，并利用CNN来预测未来的热负荷。实验结果表明，该框架在丹麦实际数据上的预测效果优于其他最先进的方法。

    

    公用事业公司面临的首要挑战之一是在确保高效供应的同时尽量减少温室气体排放。智能电表和智能电网的出现为通过主动技术（如负荷预测）实现热能的优化供应提供了前所未有的优势。在本文中，我们提出了一种基于神经网络的热需求预测框架，其中时间序列被编码为具有嵌入外生变量（如天气和假期/非假期）能力的scalogram。随后，使用CNN来预测未来多步骤的热负荷。最后，将所提出的框架与其他最先进的方法（如SARIMAX和LSTM）进行了比较。来自丹麦的实际数据的回顾性实验证明，所提出的框架在定量结果上持续优于最先进的基准方法。MAPE的最小均误差为7.54％，RMSE为417kW。

    One of the primal challenges faced by utility companies is ensuring efficient supply with minimal greenhouse gas emissions. The advent of smart meters and smart grids provide an unprecedented advantage in realizing an optimised supply of thermal energies through proactive techniques such as load forecasting. In this paper, we propose a forecasting framework for heat demand based on neural networks where the time series are encoded as scalograms equipped with the capacity of embedding exogenous variables such as weather, and holiday/non-holiday. Subsequently, CNNs are utilized to predict the heat load multi-step ahead. Finally, the proposed framework is compared with other state-of-the-art methods, such as SARIMAX and LSTM. The quantitative results from retrospective experiments show that the proposed framework consistently outperforms the state-of-the-art baseline method with real-world data acquired from Denmark. A minimal mean error of 7.54% for MAPE and 417kW for RMSE is achieved wi
    
[^185]: 多目标GFlowNets

    Multi-Objective GFlowNets. (arXiv:2210.12765v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12765](http://arxiv.org/abs/2210.12765)

    本论文提出了一种名为多目标GFlowNets (MOGFNs) 的方法，用于在多目标优化问题中生成多样的Pareto最优解。该方法基于GFlowNets，并引入了两种变体：MOGFN-PC和MOGFN-AL。实验结果表明，MOGFNs在各种任务中都表现出了很好的效果。

    

    我们研究了在多目标优化的背景下生成多样候选解的问题。在许多机器学习应用中，如药物发现和材料设计，目标是生成同时优化一组潜在冲突目标的候选解。此外，这些目标往往是对某个感兴趣属性的不完善评估，因此生成多样候选解对于进行昂贵的下游评估来说是重要的。我们提出了一种新方法，称为多目标GFlowNets（MOGFNs），用于生成多样的Pareto最优解，基于GFlowNets。我们介绍了两个MOGFNs的变体：MOGFN-PC，它通过特定标量化函数定义了一个独立子问题的系列，并使用奖励条件的GFlowNets进行建模；以及MOGFN-AL，它在主动学习循环中解决了一系列由收购函数定义的子问题。我们在各种合成和基准任务上进行了实验，证明了MOGFNs的有效性。

    We study the problem of generating diverse candidates in the context of Multi-Objective Optimization. In many applications of machine learning such as drug discovery and material design, the goal is to generate candidates which simultaneously optimize a set of potentially conflicting objectives. Moreover, these objectives are often imperfect evaluations of some underlying property of interest, making it important to generate diverse candidates to have multiple options for expensive downstream evaluations. We propose Multi-Objective GFlowNets (MOGFNs), a novel method for generating diverse Pareto optimal solutions, based on GFlowNets. We introduce two variants of MOGFNs: MOGFN-PC, which models a family of independent sub-problems defined by a scalarization function, with reward-conditional GFlowNets, and MOGFN-AL, which solves a sequence of sub-problems defined by an acquisition function in an active learning loop. Our experiments on wide variety of synthetic and benchmark tasks demonst
    
[^186]: 连续蒙特卡洛图搜索

    Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.01426](http://arxiv.org/abs/2210.01426)

    连续蒙特卡洛图搜索（CMCGS）是一种新颖的蒙特卡洛树搜索（MCTS）的扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS通过将相似状态聚类，并共享相同的动作策略，实现了高性能的在线规划。

    

    在许多复杂的连续决策任务中，在线规划对于高性能至关重要。为了实现高效的在线规划，蒙特卡洛树搜索（MCTS）采用了一个有原则的机制来权衡探索和利用。MCTS在许多离散决策领域（如围棋、国际象棋和将棋）中胜过了其他方法。而针对连续领域的MCTS扩展也已提出。然而，由于固有的高分支因子和导致搜索树大小爆炸的问题，现有方法受到了限制。为了解决这个问题，我们提出了连续蒙特卡洛图搜索（CMCGS），这是一种新颖的MCTS扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS利用了一个洞察力，在规划过程中，将相似状态之间共享相同的动作策略可以得到高性能。为了实现这个想法，CMCGS在每个时间步骤中将相似状态聚类成有限数量的随机动作赌博节点，这些节点共享相同的动作策略。

    In many complex sequential decision-making tasks, online planning is crucial for high performance. For efficient online planning, Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off exploration for exploitation. MCTS outperforms comparison methods in many discrete decision-making domains such as Go, Chess, and Shogi. Following, extensions of MCTS to continuous domains have been proposed. However, the inherent high branching factor and the resulting explosion of search tree size are limiting existing methods. To address this problem, we propose Continuous Monte Carlo Graph Search (CMCGS), a novel extension of MCTS to online planning in environments with continuous state and action spaces. CMCGS takes advantage of the insight that, during planning, sharing the same action policy between several states can yield high performance. To implement this idea, at each time step, CMCGS clusters similar states into a limited number of stochastic action bandit nodes, which
    
[^187]: Frouros: 一个用于机器学习系统中漂移检测的Python库

    Frouros: A Python library for drift detection in machine learning systems. (arXiv:2208.06868v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.06868](http://arxiv.org/abs/2208.06868)

    Frouros是一个开源的Python库，可以检测任何机器学习框架中的概念和数据漂移，易于维护和扩展。

    

    Frouros是一个开源的Python库，能够检测机器学习系统中的漂移。它提供了传统和最近算法的组合来检测概念和数据漂移。我们的设计目标是使它与任何机器学习框架兼容，并轻松适应实际应用场景。该库遵循一系列最佳开发和持续集成实践，以确保易于维护和扩展。源代码可在https://github.com/IFCA/frouros上获取。

    Frouros is an open-source Python library capable of detecting drift in machine learning systems. It provides a combination of classical and more recent algorithms for drift detection: both concept and data drift. We have designed it with the objective of making it compatible with any machine learning framework and easily adaptable to real-world use cases. The library is developed following a set of best development and continuous integration practices to ensure ease of maintenance and extensibility. The source code is available at https://github.com/IFCA/frouros.
    
[^188]: DESCN: 深度整体空间交叉网络用于个体治疗效果估计

    DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation. (arXiv:2207.09920v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09920](http://arxiv.org/abs/2207.09920)

    本论文提出了DESCN模型，通过深度整体空间交叉网络的方式，从端到端的角度建模个体治疗效果估计。该模型可以解决传统方法中的分布偏移和样本不平衡问题。

    

    因果推断在电子商务和精准医学等领域有广泛应用，其性能严重依赖于个体治疗效果(ITE)的准确估计。传统上，通过分别在各自的样本空间中建模受治疗组和对照组的响应函数来预测ITE。然而，在实际应用中，这种方法通常遇到两个问题，即由于治疗偏差而导致的受治疗组和对照组之间的分布偏离，以及其人口规模之间的显著样本不平衡。本文提出了深度整体空间交叉网络(DESCN)，以端到端的方式建模治疗效果。DESCN通过一个跨网络以多任务学习的方式捕捉治疗倾向、响应和隐藏治疗效果的综合信息。我们的方法在整个样本空间中联合学习治疗和响应函数，以避免治疗偏差，并采用中间伪处理方式。

    Causal Inference has wide applications in various areas such as E-commerce and precision medicine, and its performance heavily relies on the accurate estimation of the Individual Treatment Effect (ITE). Conventionally, ITE is predicted by modeling the treated and control response functions separately in their individual sample spaces. However, such an approach usually encounters two issues in practice, i.e. divergent distribution between treated and control groups due to treatment bias, and significant sample imbalance of their population sizes. This paper proposes Deep Entire Space Cross Networks (DESCN) to model treatment effects from an end-to-end perspective. DESCN captures the integrated information of the treatment propensity, the response, and the hidden treatment effect through a cross network in a multi-task learning manner. Our method jointly learns the treatment and response functions in the entire sample space to avoid treatment bias and employs an intermediate pseudo treat
    
[^189]: 分子转变路径的随机最优控制方法的集合变量自由采样

    Stochastic Optimal Control for Collective Variable Free Sampling of Molecular Transition Paths. (arXiv:2207.02149v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2207.02149](http://arxiv.org/abs/2207.02149)

    本文介绍了一种解决分子转变路径采样问题的新方法，通过随机最优控制采样，避免了传统方法中需要选择集合变量的问题，能够适用于较大的系统。

    

    本文考虑了在分子系统中采样两个给定亚稳态之间的转变路径的问题，例如折叠和展开的蛋白质或化学反应的产物和反应物。由于存在高能屏障，标准的分子动力学模拟很难采样到这些转变路径。传统的方法通过在分子动力学模拟中引入偏倚势场来增加转变的概率，但这需要基于集合变量的降维步骤，选择合适的集合变量需要化学直觉，因此传统方法不适用于较大的系统。此外，当使用不正确的集合变量时，偏倚势场可能不是最小的，使系统在与转变无关的维度上发生偏倚。本文展示了采样分子转变路径问题、Schr\"odinger桥问题和随机最优控制与神经网络之间的形式关系。

    We consider the problem of sampling transition paths between two given metastable states of a molecular system, e.g. a folded and unfolded protein or products and reactants of a chemical reaction. Due to the existence of high energy barriers separating the states, these transition paths are unlikely to be sampled with standard Molecular Dynamics (MD) simulation. Traditional methods to augment MD with a bias potential to increase the probability of the transition rely on a dimensionality reduction step based on Collective Variables (CVs). Unfortunately, selecting appropriate CVs requires chemical intuition and traditional methods are therefore not always applicable to larger systems. Additionally, when incorrect CVs are used, the bias potential might not be minimal and bias the system along dimensions irrelevant to the transition. Showing a formal relation between the problem of sampling molecular transition paths, the Schr\"odinger bridge problem and stochastic optimal control with neu
    
[^190]: \nu-Flows：条件中微子回归

    \nu-Flows: Conditional Neutrino Regression. (arXiv:2207.00664v7 [hep-ph] CROSS LISTED)

    [http://arxiv.org/abs/2207.00664](http://arxiv.org/abs/2207.00664)

    \nu-Flows是一种使用条件归一化流和深度可逆神经网络的方法，可以限制中微子动力学的似然空间，并实现更精确的动量重建和喷注关联的改善。

    

    我们提出了一种新的方法$\nu$-Flows，使用条件归一化流和深度可逆神经网络限制高能对撞机实验中中微子动力学的似然空间。该方法允许恢复通常作为自由参数的完整中微子动量，并在给定事件观测的条件似然下对中微子值进行采样。我们通过将$\nu$-Flows应用于模拟的半轻子$t\bar{t}$事件的案例研究中展示了$\nu$-Flows的成功，并显示它可以导致更准确的动量重建，特别是纵向坐标。我们还展示了这对于喷注关联的下游任务具有直接的好处，相对于传统方法，可以改善高达1.41倍。

    We present $\nu$-Flows, a novel method for restricting the likelihood space of neutrino kinematics in high energy collider experiments using conditional normalizing flows and deep invertible neural networks. This method allows the recovery of the full neutrino momentum which is usually left as a free parameter and permits one to sample neutrino values under a learned conditional likelihood given event observations. We demonstrate the success of $\nu$-Flows in a case study by applying it to simulated semileptonic $t\bar{t}$ events and show that it can lead to more accurate momentum reconstruction, particularly of the longitudinal coordinate. We also show that this has direct benefits in a downstream task of jet association, leading to an improvement of up to a factor of 1.41 compared to conventional methods.
    
[^191]: TabText:一种灵活和上下文化的表格数据表示方法

    TabText: A Flexible and Contextual Approach to Tabular Data Representation. (arXiv:2206.10381v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10381](http://arxiv.org/abs/2206.10381)

    TabText是一种处理和特征提取框架，通过转换内容为语言并利用预训练的大型语言模型，从表格数据中提取上下文信息。通过应用TabText框架可以生成高性能且简单的机器学习基准模型，减少数据预处理的工作量。该框架在医疗预测任务中展现出良好的效果。

    

    表格数据对于在各个行业中应用机器学习任务至关重要。然而，传统的数据处理方法并没有充分利用表格中所有可用的信息，忽视了重要的上下文信息，如列标题描述。此外，将数据预处理成表格格式仍然是模型开发中一项耗时的瓶颈。本工作引入了TabText，一种处理和特征提取框架，将上下文信息从表格数据结构中提取出来。TabText通过将内容转换为语言，并利用预训练的大型语言模型(LLMs)来解决处理困难。我们在涵盖患者出院、ICU入院和死亡等九个医疗预测任务上评估了我们的框架。我们展示了：1) 应用我们的TabText框架可以生成性能优秀且简单的机器学习基准模型，只需最少的数据预处理；2) 增强预处理后的数据利用预训练语言模型能够提升模型效果

    Tabular data is essential for applying machine learning tasks across various industries. However, traditional data processing methods do not fully utilize all the information available in the tables, ignoring important contextual information such as column header descriptions. In addition, pre-processing data into a tabular format can remain a labor-intensive bottleneck in model development. This work introduces TabText, a processing and feature extraction framework that extracts contextual information from tabular data structures. TabText addresses processing difficulties by converting the content into language and utilizing pre-trained large language models (LLMs). We evaluate our framework on nine healthcare prediction tasks ranging from patient discharge, ICU admission, and mortality. We show that 1) applying our TabText framework enables the generation of high-performing and simple machine learning baseline models with minimal data pre-processing, and 2) augmenting pre-processed t
    
[^192]: 通过概率-概率映射实现有条件校准的预测分布：在银河红移估计和概率预测中的应用

    Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting. (arXiv:2205.14568v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.14568](http://arxiv.org/abs/2205.14568)

    本研究提出了一种名为Cal-PIT的方法，通过学习一个概率-概率映射，解决了预测分布的诊断和校准问题，来实现有条件校准。

    

    不确定性量化对于评估AI算法的预测能力至关重要。过去的研究致力于描述目标变量$y \in \mathbb{R}$在给定复杂输入特征$\mathbf{x} \in \mathcal{X}$的条件下的预测分布$F(y|\mathbf{x})$。然而，现有的预测分布（例如，归一化流和贝叶斯神经网络）往往缺乏条件校准，即给定输入$\mathbf{x}$的事件发生的概率与预测概率显著不同。当前的校准方法不能完全评估和实施有条件校准的预测分布。在这里，我们提出了一种名为Cal-PIT的方法，它通过从校准数据中学习一个概率-概率映射来同时解决预测分布的诊断和校准问题。关键思想是对概率积分变换分数进行$\mathbf{x}$的回归。估计的回归提供了对特征空间中条件覆盖的可解释诊断。

    Uncertainty quantification is crucial for assessing the predictive ability of AI algorithms. Much research has been devoted to describing the predictive distribution (PD) $F(y|\mathbf{x})$ of a target variable $y \in \mathbb{R}$ given complex input features $\mathbf{x} \in \mathcal{X}$. However, off-the-shelf PDs (from, e.g., normalizing flows and Bayesian neural networks) often lack conditional calibration with the probability of occurrence of an event given input $\mathbf{x}$ being significantly different from the predicted probability. Current calibration methods do not fully assess and enforce conditionally calibrated PDs. Here we propose \texttt{Cal-PIT}, a method that addresses both PD diagnostics and recalibration by learning a single probability-probability map from calibration data. The key idea is to regress probability integral transform scores against $\mathbf{x}$. The estimated regression provides interpretable diagnostics of conditional coverage across the feature space. 
    
[^193]: FedFormer：强化学习中的上下文联邦学习与注意力

    FedFormer: Contextual Federation with Attention in Reinforcement Learning. (arXiv:2205.13697v3 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2205.13697](http://arxiv.org/abs/2205.13697)

    FedFormer是一种新的强化学习联邦策略，利用Transformer Attention上下文地汇总来自不同学习智能体的嵌入，具有更高的回报和更高的效率。

    

    多智能体联邦强化学习中一个核心问题是如何汇总多个智能体的见解。通常采用将每个参与智能体的模型权重取平均得到一个共同模型（FedAvg）。我们提出了FedFormer，一种新的联邦策略，利用Transformer Attention来上下文地汇总来自不同学习智能体的嵌入。通过这样做，我们根据当前智能体的环境和学得关系有选择地衡量其他智能体的贡献，从而提供更有效和高效的联邦学习。我们在Meta-World环境中对我们的方法进行了评估，并发现我们的方法在FedAvg和非联邦Soft Actor-Critic单智能体方法上都取得了显著的改进。与Soft Actor-Critic相比，我们的结果表明FedFormer在仍遵守联邦学习的隐私约束的同时获得了更高的分集回报。最后，我们还演示了该方法的实现。

    A core issue in multi-agent federated reinforcement learning is defining how to aggregate insights from multiple agents. This is commonly done by taking the average of each participating agent's model weights into one common model (FedAvg). We instead propose FedFormer, a novel federation strategy that utilizes Transformer Attention to contextually aggregate embeddings from models originating from different learner agents. In so doing, we attentively weigh the contributions of other agents with respect to the current agent's environment and learned relationships, thus providing a more effective and efficient federation. We evaluate our methods on the Meta-World environment and find that our approach yields significant improvements over FedAvg and non-federated Soft Actor-Critic single-agent methods. Our results compared to Soft Actor-Critic show that FedFormer achieves higher episodic return while still abiding by the privacy constraints of federated learning. Finally, we also demonstr
    
[^194]: 通过手机嵌入式传感器数据对用户的社交背景和熟悉地点进行设备内建模

    On-device modeling of user's social context and familiar places from smartphone-embedded sensor data. (arXiv:2205.08790v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.08790](http://arxiv.org/abs/2205.08790)

    本研究提出了一种新颖的、无监督的轻量级方法，通过在用户的移动设备上建模用户的社交背景和地点，从而从手机嵌入式传感器数据中提取高层次和语义丰富的上下文特征。

    

    上下文建模和识别是复杂的任务，使移动和泛在计算应用能够适应用户的情境。当前的解决方案主要集中在对有限的上下文信息的处理上，通常在集中式架构上处理，可能会暴露用户的个人数据，而且缺乏个性化功能。因此，设备内上下文建模和识别代表了该领域的当前研究趋势。在移动环境中，用户的社交互动和访问地点是对日常生活场景进行表征的重要信息。在本文中，我们提出了一种新颖的、无监督的轻量级方法，通过直接在用户的移动设备上基于自我网络对用户的社交背景和地点进行建模。依靠这个模型，系统能够从手机嵌入式传感器数据中提取高层次和语义丰富的上下文特征。

    Context modeling and recognition represent complex tasks that allow mobile and ubiquitous computing applications to adapt to the user's situation. Current solutions mainly focus on limited context information generally processed on centralized architectures, potentially exposing users' personal data to privacy leakage, and missing personalization features. For these reasons on-device context modeling and recognition represent the current research trend in this area. Among the different information characterizing the user's context in mobile environments, social interactions and visited locations remarkably contribute to the characterization of daily life scenarios. In this paper we propose a novel, unsupervised and lightweight approach to model the user's social context and her locations based on ego networks directly on the user mobile device. Relying on this model, the system is able to extract high-level and semantic-rich context features from smartphone-embedded sensors data. Speci
    
[^195]: 从数据流中学习不平衡数据的调查：分类、挑战、实证研究和可复制的实验框架

    A survey on learning from imbalanced data streams: taxonomy, challenges, empirical study, and reproducible experimental framework. (arXiv:2204.03719v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.03719](http://arxiv.org/abs/2204.03719)

    这篇论文调查了从不平衡数据流中学习的现状，提出了一个全面的实验框架，对多种具有挑战性的不平衡数据流场景进行评估，并比较了24种最先进的算法的优缺点。

    

    当涉及到对数据流进行分类时，类别不平衡会带来新的挑战。近期文献中提出了许多算法来解决这个问题，采用了各种数据级别、算法级别和集成方法。然而，关于如何评估这些算法存在缺乏标准化和共识的程序和基准的问题。本研究提出了一个标准化、详尽和全面的实验框架，以评估一系列多样且具有挑战性的不平衡数据流场景中的算法。实证研究评估了24个最先进的数据流算法在515个结合了静态和动态类别不平衡比例、实例级困难度、概念漂移、真实和半合成数据集的二元和多类情景下的不平衡数据流上。这导致了一个大规模的实证研究，比较了数据流挖掘领域最先进的分类器的优缺点。

    Class imbalance poses new challenges when it comes to classifying data streams. Many algorithms recently proposed in the literature tackle this problem using a variety of data-level, algorithm-level, and ensemble approaches. However, there is a lack of standardized and agreed-upon procedures and benchmarks on how to evaluate these algorithms. This work proposes a standardized, exhaustive, and comprehensive experimental framework to evaluate algorithms in a collection of diverse and challenging imbalanced data stream scenarios. The experimental study evaluates 24 state-of-the-art data streams algorithms on 515 imbalanced data streams that combine static and dynamic class imbalance ratios, instance-level difficulties, concept drift, real-world and semi-synthetic datasets in binary and multi-class scenarios. This leads to a large-scale experimental study comparing state-of-the-art classifiers in the data stream mining domain. We discuss the advantages and disadvantages of state-of-the-art
    
[^196]: 基于在线观测器的逆强化学习

    Online Observer-Based Inverse Reinforcement Learning. (arXiv:2011.02057v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2011.02057](http://arxiv.org/abs/2011.02057)

    本文提出一种基于在线观测器的逆强化学习方法，将二次成本函数下的输出反馈逆强化学习问题转化为状态估计问题。通过开发两种基于观测器的技术，包括一种历史堆栈重复使用先前状态估计的新颖观测器方法，实现了收敛性和鲁棒性的理论保证，并通过仿真实验验证了在有噪声和无噪声测量情况下的性能。

    

    本文提出了一种新的方法来解决线性系统二次成本函数下的输出反馈逆强化学习（IRL）问题，将IRL问题视为状态估计问题。我们开发了两种基于观测器的IRL技术，包括一种通过历史堆栈重复使用先前状态估计的新颖观测器方法。在适当的激励条件下，我们建立了收敛性和鲁棒性的理论保证。仿真实验证明了在有噪声和无噪声测量下开发的观测器和滤波器的性能。

    In this paper, a novel approach to the output-feedback inverse reinforcement learning (IRL) problem is developed by casting the IRL problem, for linear systems with quadratic cost functions, as a state estimation problem. Two observer-based techniques for IRL are developed, including a novel observer method that re-uses previous state estimates via history stacks. Theoretical guarantees for convergence and robustness are established under appropriate excitation conditions. Simulations demonstrate the performance of the developed observers and filters under noisy and noise-free measurements.
    

