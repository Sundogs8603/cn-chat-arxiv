# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning from Two Decades of Blood Pressure Data: Demography-Specific Patterns Across 75 Million Patient Encounters](https://rss.arxiv.org/abs/2402.01598) | 这项研究通过分析两个十年的庞大数据集，发现基于性别的血压变化不显著，挑战了传统假设；同时，舒张压随着年龄增长而持续增加，而收缩压在四十多岁的年龄组显示出一个独特的峰值。 |
| [^2] | [Continual Learning for Large Language Models: A Survey](https://rss.arxiv.org/abs/2402.01364) | 这篇综述文章调查了大型语言模型(LLMs)的持续学习，使用了一种新颖的多阶段分类方案对持续学习技术进行了分类，指出了该领域面临的挑战和未来工作方向。 |
| [^3] | [Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models](https://rss.arxiv.org/abs/2402.01345) | 本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。 |
| [^4] | [A Differentiable POGLM with Forward-Backward Message Passing](https://rss.arxiv.org/abs/2402.01263) | 提出了一种具有前向-后向消息传递的可微分POGLM模型，解决了现有POGLM学习中的路径梯度估计和变分模型设计等问题。 |
| [^5] | [Multivariate Probabilistic Time Series Forecasting with Correlated Errors](https://rss.arxiv.org/abs/2402.01000) | 本文提出了一种方法，基于低秩加对角线参数化协方差矩阵，可以有效地刻画时间序列预测中误差的自相关性，并具有复杂度低、校准预测准确性高等优点。 |
| [^6] | [Opening the AI black box: program synthesis via mechanistic interpretability](https://arxiv.org/abs/2402.05110) | 本文提出了一种新的程序合成方法MIPS，通过机械解释性训练神经网络，将学习到的算法转化为Python代码。MIPS在基准测试中表现出色，解决了32个算法任务，其中13个是其他方法无法解决的。该方法不使用人工训练数据，具有较高的可扩展性和解释性。 |
| [^7] | [Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding](https://arxiv.org/abs/2402.05109) | Hydra heads是一种循序依赖的草稿头部，取代了标准草稿头部，显著提高了推测准确性，在Medusa解码中具有更高的吞吐量。 |
| [^8] | [Tighter Generalisation Bounds via Interpolation](https://arxiv.org/abs/2402.05101) | 本论文提供了一种新的推导PAC-Bayes泛化界限的方法，并通过插值在不同概率差异之间选择最佳方案，展示了紧密性和实际性能。 |
| [^9] | [Hydragen: High-Throughput LLM Inference with Shared Prefixes](https://arxiv.org/abs/2402.05099) | Hydragen是一种具有共享前缀的高吞吐量LLM推理方法，通过将注意力计算分解为共享前缀和唯一后缀，来提高推理效率，并能够提高端到端LLM吞吐量多达32倍。 |
| [^10] | [On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling](https://arxiv.org/abs/2402.05098) | 本研究探讨了训练扩散模型以从给定分布中采样的问题，并针对随机控制和采样提出了一种新的探索策略，通过基准测试比较了不同推断方法的相对优劣，并对过去的工作提出了质疑。 |
| [^11] | [NITO: Neural Implicit Fields for Resolution-free Topology Optimization](https://arxiv.org/abs/2402.05073) | NITO是一种使用神经隐式场的拓扑优化方法，通过引入Boundary Point Order-Invariant MLP（BPOM）方法提供了无分辨率和域不可知的解决方案，可以以更高的效率和更短的时间生成结构。 |
| [^12] | [Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity](https://arxiv.org/abs/2402.05071) | 本文研究了具有连带偏序特性的非凸极小极大问题，提出了一阶算法的适用范围，并在理论上证明了算法的复杂度保证。 |
| [^13] | [Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems](https://arxiv.org/abs/2402.05067) | 本文提出了利用物理信息神经网络进行多尺度建模的方法，通过解耦大尺度和小尺度动力学，并在正交基函数空间中近似小尺度系统。实验结果表明该方法在处理液体动力学问题以及更复杂的情况下具有较高的有效性和适用性。 |
| [^14] | [Causal Representation Learning from Multiple Distributions: A General Setting](https://arxiv.org/abs/2402.05052) | 本文研究了一个通用的、完全非参数的因果表示学习设置，旨在在多个分布之间学习因果关系，无需假设硬干预。通过稀疏性约束，可以从多个分布中恢复出因果关系。 |
| [^15] | [Federated Learning Can Find Friends That Are Beneficial](https://arxiv.org/abs/2402.05050) | 本研究介绍了一种新颖的算法，在Federated Learning中使用自适应聚合权重来识别对特定学习目标最有益的客户，证明了该方法的收敛性，并经过实证评估发现，使用该算法引导的合作优于传统方法，这为更加简化和有效的Federated Learning实现奠定了基础。 |
| [^16] | [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044) | SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。 |
| [^17] | [PAC Learnability under Explanation-Preserving Graph Perturbations](https://arxiv.org/abs/2402.05039) | 本研究考虑了在设计和训练GNN时利用解释辅助学习和解释辅助数据增强的方法，并发现解释辅助学习的样本复杂度可以任意小于解释不可知学习。 |
| [^18] | [Simulated Overparameterization](https://arxiv.org/abs/2402.05033) | 模拟过度参数化（SOP）是一种将紧凑模型的计算效率与过度参数化模型的高级学习能力相结合的新范式。通过使用模拟训练过度参数化模型的方法，我们提出了一种与主要架构无关的算法，称为"majority kernels"，从而在各种架构和任务中实现性能提升。 |
| [^19] | [Strong convexity-guided hyper-parameter optimization for flatter losses](https://arxiv.org/abs/2402.05025) | 本文提出了一种强凸性引导的超参数优化方法，通过最小化损失函数的强凸性来改善其平坦性。通过利用底层神经网络的结构，我们提出了一种闭式方程来近似计算强凸性参数，并以随机化的方式寻找最小化该参数的超参数配置。实验证明，该方法在更短的运行时间下取得了较强的性能。 |
| [^20] | [A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?](https://arxiv.org/abs/2402.05015) | 本文研究了LLMs是否真的有助于加速在分子空间中的正规贝叶斯优化。通过将LLMs视为标准但正规的BO替代模型的固定特征提取器，并利用参数效能来实现。 |
| [^21] | [Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth](https://arxiv.org/abs/2402.05013) | 本文证明了在自编码器中，即使采用浅层结构，梯度下降算法会完全忽略稀疏数据的结构，并且对于一般数据分布，我们发现了梯度下降最小化器在数据稀疏性临界点处的相变现象。 |
| [^22] | [Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching](https://arxiv.org/abs/2402.05011) | 本文通过连接先前被忽视的监督信号的方式，首次尝试实现无损图谱精简，以解决现有方法无法准确复制原始图谱的问题。 |
| [^23] | [EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss](https://arxiv.org/abs/2402.05008) | 高效ViT-SAM是一种新的加速的段落任意模型，通过保留轻量级提示编码器和掩码解码器，并替换沉重的图像编码器，实现了48.9倍的加速而不牺牲性能。 |
| [^24] | [Example-based Explanations for Random Forests using Machine Unlearning](https://arxiv.org/abs/2402.05007) | FairDebugger是一个利用机器学习去解释随机森林的系统，在公平性背景下找出导致模型不公平的训练数据子集。 |
| [^25] | [Randomized Confidence Bounds for Stochastic Partial Monitoring](https://arxiv.org/abs/2402.05002) | 本文研究了随机部分监控中基于随机化的偏置界策略，该策略扩展了现有随机策略无法应用的情境和非情境设置，实验证明该策略优于最先进的方法。 |
| [^26] | [Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design](https://arxiv.org/abs/2402.04997) | 本研究提出了离散流模型（DFMs），通过连续时间马尔可夫链实现离散空间流匹配的离散等效，为将基于流的生成模型应用于多模态连续和离散数据问题提供了解决方案。此方法在蛋白质共设计任务上取得了最先进的性能。 |
| [^27] | [PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses](https://arxiv.org/abs/2402.04987) | 本研究提出了一种名为PriorBoost的自适应算法，用于学习从聚合回应中学习的算法。该算法通过自适应地形成一些越来越同质的样本bags，以提高模型质量，并在事件级别预测方面实现了最优模型质量。 |
| [^28] | [Beyond explaining: XAI-based Adaptive Learning with SHAP Clustering for Energy Consumption Prediction](https://arxiv.org/abs/2402.04982) | 本文介绍了一种将可解释的人工智能（XAI）技术与自适应学习相结合的方法，通过使用SHAP聚类来提供模型预测的可解释性解释，并利用这些洞察力来自适应地改进模型，平衡复杂性与预测性能。这种方法可以解决数据分布转移问题并确保模型的鲁棒性。评估结果表明该方法在能耗预测及其他领域具有良好的迁移性能。 |
| [^29] | [Asymptotics of feature learning in two-layer networks after one gradient-step](https://arxiv.org/abs/2402.04980) | 通过研究两层神经网络在一次梯度下降步骤后的特征学习，我们提供了在高维极限下通用化误差的精确渐近描述，并发现在适应数据的情况下，网络能够高效地学习梯度方向上的非线性函数。 |
| [^30] | [A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health](https://arxiv.org/abs/2402.04933) | 基于贝叶斯方法的在线学习在公共卫生干预计划中的资源分配中具有重要的应用。我们提出了一种新颖的贝叶斯学习方法，结合了贝叶斯建模和汤普森抽样技术，能够灵活地处理上下文环境和非稳态的多臂赌博机问题，并且在预算有限的情况下能够快速学习未知的转移动态。实验证明，该方法实现了显著更高的收益率。 |
| [^31] | [Blue noise for diffusion models](https://arxiv.org/abs/2402.04930) | 本文提出了一种新颖且通用的扩散模型，利用蓝噪声改善了训练过程中的生成质量，并通过引入相关噪声模型和相关噪声掩码来考虑图像内部和跨图像的相关性，以提高梯度流动和重构频谱内容。 |
| [^32] | [Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation](https://arxiv.org/abs/2402.04929) | 本文提出了一种无源域自适应的新方法，利用扩散模型生成上下文相关的领域特定图像，通过微调预训练模型和无监督领域自适应技术实现了显著的性能改进。 |
| [^33] | [TP-Aware Dequantization](https://arxiv.org/abs/2402.04925) | 本文介绍了一种TP感知的去量化方法，通过优化推理部署方案解决了分布式部署大型语言模型的推理延迟问题。该方法保留了数据局部性和利用TP的先验知识来减少全局通信，在多种TP设置下，在A100和H100 NVIDIA DGX系统上实现了显著速度提升。 |
| [^34] | [Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching](https://arxiv.org/abs/2402.04924) | 本论文提出了一种新颖的图表压缩方法CTRL，通过优化起点和精细的策略，解决了梯度匹配方向导致的训练轨迹偏差和累积误差问题。 |
| [^35] | [Voronoi Candidates for Bayesian Optimization](https://arxiv.org/abs/2402.04922) | 使用Voronoi候选点边界可以在贝叶斯优化中有效地优化黑盒函数，提高了多起始连续搜索的执行时间。 |
| [^36] | [Moco: A Learnable Meta Optimizer for Combinatorial Optimization](https://arxiv.org/abs/2402.04915) | Moco是一个可学习的组合优化元优化器，通过学习图神经网络来更新解决方案构建过程，并能够适应不同的情况和计算预算。 |
| [^37] | [Towards Biologically Plausible and Private Gene Expression Data Generation](https://arxiv.org/abs/2402.04912) | 本文提出了朝向生物学上合理且私密的基因表达数据生成的方法，并对五种代表性的差分隐私生成方法进行了全面分析。通过综合评估，揭示了每种方法的优点和缺点，为未来发展提供了有趣的可能性。 |
| [^38] | [On a Combinatorial Problem Arising in Machine Teaching](https://arxiv.org/abs/2402.04907) | 本文研究了机器教学中的一个组合问题，通过证明了一个最坏情况下的猜想，得出了关于教学维度的结果。该结果可以看作是解决了超立方体边界等周问题的定理的推广。 |
| [^39] | [Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects](https://arxiv.org/abs/2402.04906) | 本研究提出了一种新方法，即一致性蒙特卡洛元学习模型，用于预测个体治疗效果。通过利用一致性预测系统、蒙特卡洛采样和CATE元学习模型，该方法生成可用于个性化决策的预测分布。实验结果显示，该方法在保持较小区间宽度的情况下具有强大的实验覆盖范围，可以提供真实个体治疗效果的估计。 |
| [^40] | [L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ](https://arxiv.org/abs/2402.04902) | L4Q是一种参数高效的量化感知训练算法，通过基于LoRA的学习的量化步长，解决了大型语言模型中量化训练的挑战。 |
| [^41] | [The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer](https://arxiv.org/abs/2402.04898) | 本文开发了一种预测模型，通过考虑球员受伤概率，在足球赛季中优化团队表现，减少一线队受伤人数和无效花费，为真实足球团队降低成本并提高球员福利提供了潜力。 |
| [^42] | [Learning from the Best: Active Learning for Wireless Communications](https://arxiv.org/abs/2402.04896) | 本文介绍了无线通信中应用主动学习的深度学习方法，并提供了一个基于深度学习的毫米波波束选择的案例研究，评估了不同主动学习算法在一个公开可用数据集上的性能。 |
| [^43] | [Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning](https://arxiv.org/abs/2402.04894) | 提出了一种利用动态图的深度强化学习方法，用于自适应信息路径规划，能够在未知的三维环境中映射出感兴趣的目标。 |
| [^44] | [A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration](https://arxiv.org/abs/2402.04892) | 本论文提出了一个基于加权模型集成的统一框架，用于概率验证AI系统。这个框架可以在不依赖强分布假设的情况下，验证各种机器学习模型的许多有趣属性，如公平性、鲁棒性或单调性。 |
| [^45] | [RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing](https://arxiv.org/abs/2402.04888) | 这篇论文提出了一种名为RSCNet的动态CSI压缩方法，通过压缩信道状态信息（CSI）来减少物联网设备向云服务器传输CSI的通信开销。RSCNet利用长短期记忆（LSTM）单元和优化的CSI窗口实现了准确的感知和CSI重建，从而实现了实时的云基WiFi感知。 |
| [^46] | [A Unified Gaussian Process for Branching and Nested Hyperparameter Optimization](https://arxiv.org/abs/2402.04885) | 本文提出了一个统一的贝叶斯优化方法，用于处理分支和嵌套参数之间的条件依赖关系，该方法能够有效地优化神经网络的超参数选择和调整。 |
| [^47] | [LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units](https://arxiv.org/abs/2402.04882) | 本文提出了一种改进的脉冲模型LMUFormer，通过对循环模型进行架构修改，将其性能推向Transformer模型，同时保留其顺序处理能力，以实现并行训练、流式处理和低成本推理。 |
| [^48] | [Combining Cloud and Mobile Computing for Machine Learning](https://arxiv.org/abs/2402.04880) | 这项研究将模型分割为移动设备和云之间的计算，以减轻移动设备的负担，并优化云端的工作负载。 |
| [^49] | [On Provable Length and Compositional Generalization](https://arxiv.org/abs/2402.04875) | 本研究针对包括深度集合、变压器、状态空间模型和简单递归神经网络等多种架构，探索了可证明的长度和组合泛化，认为对于长度和组合泛化，不同架构需要不同程度的表示识别。 |
| [^50] | [Choosing a Classical Planner with Graph Neural Networks](https://arxiv.org/abs/2402.04874) | 本文研究了使用图神经网络进行在线规划器选择的方法，并通过析取出的图表示作为另一模型的输入，提出了一种更加资源高效但准确的方法。 |
| [^51] | [Embedding Knowledge Graphs in Degenerate Clifford Algebras](https://arxiv.org/abs/2402.04870) | 这项研究提出将知识图谱嵌入到退化的克利福德代数中。通过考虑具有零幂指数为2的零幂基向量，可以泛化基于二次数的方法并捕捉实体嵌入中缺乏高阶相互作用的模式。研究设计了两个新模型来发现代数的参数，并证明零幂向量有助于捕捉实体的特征。 |
| [^52] | [Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy](https://arxiv.org/abs/2402.04869) | 本文提出了一个在线因果强化学习框架，其中通过明确建模状态的生成过程和使用因果结构进行策略引导，以帮助强化学习代理的决策可解释性。该框架具有理论性能保证，并在探索和开发过程中使用干预进行因果结构学习。 |
| [^53] | [Room transfer function reconstruction using complex-valued neural networks and irregularly distributed microphones](https://arxiv.org/abs/2402.04866) | 本研究使用复值神经网络在有限的麦克风数据中估计房间传递函数，实现了房间传递函数的重建，创新之处在于首次使用了复值神经网络来估计房间传递函数。 |
| [^54] | [CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay](https://arxiv.org/abs/2402.04858) | CodeIt是一种具备优先级回顾重放的自我改进语言模型方法，通过将目标重标记为采样程序的实际输出，有效解决了程序合成中奖励稀疏性的问题，并在抽象和推理语料库（ARC）上实现了成功的跨任务泛化。 |
| [^55] | [Explaining Learned Reward Functions with Counterfactual Trajectories](https://arxiv.org/abs/2402.04856) | 通过对比原始轨迹和反事实部分轨迹的奖励，我们提出了一种解释强化学习中奖励函数的方法。通过生成符合质量标准的反事实轨迹解释（CTEs），我们的实验表明，CTEs对于代理人模型具有明显的信息性，能提高其预测与奖励函数的一致性。 |
| [^56] | [Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey](https://arxiv.org/abs/2402.04854) | 该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。 |
| [^57] | [Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning](https://arxiv.org/abs/2402.04852) | 本研究提出了aLLM4TS框架，将LLMs应用于时间序列表示学习。通过自监督的多块预测任务，捕捉时间动态特征，并通过特定时间序列上的微调进行进一步优化。 |
| [^58] | [AlphaFold Meets Flow Matching for Generating Protein Ensembles](https://arxiv.org/abs/2402.04845) | 本研究开发了一种基于流动匹配的生成建模方法，称为AlphaFlow和ESMFlow，用于学习和采样蛋白质的构象空间。与AlphaFold相比，该方法在精度和多样性方面提供了更优的组合，在训练和评估时能准确捕捉到构象灵活性和高阶组合可观测性。同时，该方法可以将静态PDB结构多样化到特定的平衡性质，具有较快的收敛速度。 |
| [^59] | [On the Completeness of Invariant Geometric Deep Learning Models](https://arxiv.org/abs/2402.04836) | 这项研究集中于不变模型的理论表达能力，通过引入完备的设计GeoNGNN，并利用其作为理论工具，首次证明了E(3)-完备性。 |
| [^60] | [SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning](https://arxiv.org/abs/2402.04835) | SARI是一个简约的框架，通过利用嘈杂部分标签，结合平均策略和识别策略，实现了部分标签学习中的深度神经网络分类器训练，并显著提升了准确性。 |
| [^61] | [Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming](https://arxiv.org/abs/2402.04830) | 本研究介绍了dSGP4，一种使用PyTorch实现的可微版本的SGP4。通过可微化，dSGP4实现了轨道传播的高精度，并且适用于各种与太空相关的应用，包括卫星轨道确定、状态转换、协方差传播等。 |
| [^62] | [Fast Timing-Conditioned Latent Audio Diffusion](https://arxiv.org/abs/2402.04825) | 本研究提出了一种名为Stable Audio的生成模型，该模型利用潜在扩散和条件化技术，实现了高效生成44.1kHz立体声音乐和音效。与其他最先进的模型不同，Stable Audio能够生成具有结构和立体声的音乐，并在性能评估上表现出色。 |
| [^63] | [How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data](https://arxiv.org/abs/2402.04823) | 本文提出了一种约束深度生成模型(C-DGMs)，通过将约束转化为约束层(CL)来保证生成样本符合给定约束。实验结果表明，相较于标准DGMs，C-DGMs能够更好地遵守约束。 |
| [^64] | [E(3)-Equivariant Mesh Neural Networks](https://arxiv.org/abs/2402.04821) | E(3)-等变Mesh神经网络通过扩展E(n)-等变图神经网络的更新方程以包括网格面信息，并通过层次化进一步改进以考虑长程相互作用，实现了在网格任务上的优越性能，具有快速运行时间和无需昂贵预处理的特点。 |
| [^65] | [BOWLL: A Deceptively Simple Open World Lifelong Learner](https://arxiv.org/abs/2402.04814) | 这项研究提出了BOWLL，一个看似简单但极其有效的方法，通过重新利用标准模型用于开放世界终身学习，加速了这个多方面领域的探索。 |
| [^66] | [Scalable Multi-view Clustering via Explicit Kernel Features Maps](https://arxiv.org/abs/2402.04794) | 本文介绍了一种可扩展的多视角子空间聚类框架，并提出了一种高效的优化策略，利用内核特征映射来减轻计算负担。该算法可在大规模数据集上应用，并在几分钟内完成。 |
| [^67] | [Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity](https://arxiv.org/abs/2402.04785) | Shadowheart SGD是一种新的分布式异步SGD方法，利用无偏压缩技术，在任意的计算和通信异构性下具有最优时间复杂度，并显著优化了先前的集中式方法。同时，我们还开发了对应的双向设置方法。 |
| [^68] | [Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate Networks](https://arxiv.org/abs/2402.04783) | 通过分析神经切向核（NTK），本文提供了对周期性激活网络的理论理解，并发现周期性激活网络在NTK的角度上比ReLU激活网络更加“良好”。此外，我们还验证了这些网络的记忆容量。 |
| [^69] | [A fast score-based search algorithm for maximal ancestral graphs using entropy](https://arxiv.org/abs/2402.04777) | 本研究提出了一种基于得分的快速搜索算法，用于使用熵的最大祖先图。通过引入imsets框架和精化马尔科夫属性，我们改进了MAG的评分方法，并证明了搜索算法的多项式复杂度。在模拟实验中，我们的算法表现出了优越的性能。 |
| [^70] | [Code as Reward: Empowering Reinforcement Learning with VLMs](https://arxiv.org/abs/2402.04764) | 本文介绍了一种利用预训练视觉语言模型(VLMs)来支持强化学习(RL)代理训练的框架，通过代码生成从VLMs生成密集奖励函数，减轻了直接查询VLM的计算负担。 |
| [^71] | [Color Recognition in Challenging Lighting Environments: CNN Approach](https://arxiv.org/abs/2402.04762) | 本论文提出了一种基于CNN的方法，用于挑战性照明环境下的颜色识别。实验证明，该方法能够显著提高在不同光照条件下的颜色检测的鲁棒性，并且比现有方法表现更好。 |
| [^72] | [Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints](https://arxiv.org/abs/2402.04754) | 本文提出了一种统一的布局生成模型，通过考虑美学约束，在图形设计中创建合理的元素可视排列。与之前的方法相比，该模型表现出更好的对齐性能，并在各种布局生成任务中取得了优秀的性能。 |
| [^73] | [Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers](https://arxiv.org/abs/2402.04744) | 本文研究了Transformer中高稀疏区域稀疏训练方法的有效性，发现现有方法无法保持与低稀疏区域相当的模型质量，主要原因是梯度幅值中引入的噪音水平提高。为了解决这个问题，采用了渐进限制梯度流动的衰减机制。 |
| [^74] | [Non-Parametric Estimation of Multi-dimensional Marked Hawkes Processes](https://arxiv.org/abs/2402.04740) | 这篇论文提出了一种非参数估计标记Hawkes过程条件强度的方法，引入了两种模型来处理具有不同内核和非线性特征的Hawkes过程，通过将过去的到达时间和标记作为输入，获得到达强度。 |
| [^75] | [Graph Cuts with Arbitrary Size Constraints Through Optimal Transport](https://arxiv.org/abs/2402.04732) | 本论文提出了一种新的图割算法，通过将图割问题制定为正则化的Gromov-Wasserstein问题，并使用加速的近端GD算法解决，实现在任意大小约束下对图进行分割，在非平衡数据集聚类等应用中具有更高的效率。 |
| [^76] | [Theoretical and Empirical Analysis of Adaptive Entry Point Selection for Graph-based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2402.04713) | 本论文对图形基准最近邻搜索的自适应入口点选择进行了理论和实证分析，并提出了新的概念$b\textit{-单调路径}$和$B\textit{-MSNET}$。理论证明了自适应入口点选择在更一般的条件下比固定中心入口点具有更好的性能上界。实验验证了该方法在各种数据集上的准确性、速度和内存使用方面的有效性，尤其是在分布之外的数据和难例的挑战场景中。这项全面研究提供了优化图形基准最近邻搜索入口点的深入洞察，适用于实际的高维数据应用。 |
| [^77] | [Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks](https://arxiv.org/abs/2402.04710) | 本研究开发了一种新颖的、可解释的因果GNN框架，将检索式因果学习与图信息瓶颈相结合，能够有效地处理拓扑数据，并提供对GNN的透明和直观理解。 |
| [^78] | [EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions](https://arxiv.org/abs/2402.04699) | EvoSeed是一个基于进化策略的搜索算法框架，用于生成自然对抗样本，旨在揭示深度神经网络面临的威胁。该框架在模型无关的黑盒设置中操作，可以生成高质量且可转移的对抗图像。 |
| [^79] | [From explained variance of correlated components to PCA without orthogonality constraints](https://arxiv.org/abs/2402.04692) | 本文提出了一种从相关分量的解释方差到无正交约束的主成分分析的方法，通过引入衡量数据矩阵A中由相关分量Y = AZ解释的方差部分的新目标函数expvar(Y)，放松了加载的正交约束。 |
| [^80] | [Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces](https://arxiv.org/abs/2402.04691) | 本研究在一般希尔伯特空间中使用随机梯度下降（SGD）学习算子，提出了适用于目标算子的规则条件，并建立了SGD算法的收敛速度上界，同时展示了对于非线性算子学习的有效性及线性近似收敛特性。 |
| [^81] | [Large Language Models As Faithful Explainers](https://arxiv.org/abs/2402.04678) | 本论文提出了一个生成解释框架（xLLM），用于提高大型语言模型（LLMs）自然语言格式解释的可信度。通过一个评估器来量化解释的可信度，并通过迭代优化过程来提高可信度。 |
| [^82] | [Group Distributionally Robust Dataset Distillation with Risk Minimization](https://arxiv.org/abs/2402.04676) | 这项研究关注数据集蒸馏与其泛化能力的关系，尤其是在面对不常见的子组的样本时，如何确保模型在合成数据集上的训练可以表现良好。 |
| [^83] | [A Perspective on Individualized Treatment Effects Estimation from Time-series Health Data](https://arxiv.org/abs/2402.04668) | 这项研究提供了个体化治疗效果从时间序列健康数据的视角，并概述了相关文献中的最新工作，为进一步研究提供了见解。 |
| [^84] | [Adversarial Robustness Through Artifact Design](https://arxiv.org/abs/2402.04660) | 该研究提出了一种通过艺术设计实现对抗性鲁棒性的方法，通过微小更改现有规范来抵御对抗性示例的影响。 |
| [^85] | [Open-Vocabulary Calibration for Vision-Language Models](https://arxiv.org/abs/2402.04655) | 本文研究了视觉语言模型中的开放词汇校准问题，在提示学习的背景下发现现有的校准方法不足以解决该问题。为此，提出了一种称为 Distance-Aware Ca 的简单而有效的方法来解决问题。 |
| [^86] | [An Over Complete Deep Learning Method for Inverse Problems](https://arxiv.org/abs/2402.04653) | 该论文提出了一种过完备深度学习方法，用于解决反问题。通过将解决方案嵌入到更高维度中，同时设计和学习嵌入和正则化器，克服了传统方法的不足，并在多个典型反问题上证明了其优点。 |
| [^87] | [Latent Plan Transformer: Planning as Latent Variable Inference](https://arxiv.org/abs/2402.04647) | 潜在计划变换器（LPT）是一种新颖的模型，它通过将Transformer-based轨迹生成器和最终回报连接起来，并利用潜在空间进行规划。在学习中，通过对潜在变量的后验采样形成一致的抽象，在测试时通过推断潜在变量指导自回归策略。实验证明LPT能够从次优解中发现改进的决策。 |
| [^88] | [Learning with Diversification from Block Sparse Signal](https://arxiv.org/abs/2402.04646) | 本文提出了一种新的先验，称为多样化块稀疏先验，用来描述真实世界数据中的广泛块稀疏现象。通过允许方差和相关矩阵的多样化，解决了现有块稀疏学习方法对预定义块信息的敏感性问题，并提出了一种多样化的块稀疏贝叶斯学习方法(DivSBL)，实现自适应的块估计，减轻过拟合的风险，并建立了全局和局部最优性理论。实验结果证明了DivSBL相对于现有算法的优势。 |
| [^89] | [LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views](https://arxiv.org/abs/2402.04644) | 本论文中，我们提出了一种名为LEVI的方法，通过以层为单位的多视角集成，实现了对预训练和微调数据中问题的解决，并提升微调模型对未见过的分布的泛化能力。 |
| [^90] | [Domain Bridge: Generative model-based domain forensic for black-box models](https://arxiv.org/abs/2402.04640) | 本论文提出了一个基于生成模型的方法，通过迭代改进的方式确定模型的数据领域和具体属性。该方法使用图像嵌入模型和生成模型，能够在识别丰富细粒度类别方面更加精准。 |
| [^91] | [Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective](https://arxiv.org/abs/2402.04621) | A-X依赖关系是影响图卷积效果的重要因素，特征重排可以显著提升图神经网络的性能。 |
| [^92] | [CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients](https://arxiv.org/abs/2402.04620) | CataractBot是一种基于LLM的白内障患者专家辅助聊天机器人，通过查询知识库提供即时的答案和专家验证的回复。在实地部署研究中证明了其价值所在。 |
| [^93] | [InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory](https://arxiv.org/abs/2402.04617) | InfLLM是一种无需训练的基于记忆的方法，用于揭示LLMs处理超长序列的内在能力。它通过存储远距离上下文和高效的注意计算机制，允许LLMs有效处理具有流式输入的长序列。 |
| [^94] | [TinyLLM: Learning a Small Student from Multiple Large Language Models](https://arxiv.org/abs/2402.04616) | TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。 |
| [^95] | [Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2402.04613) | 本文研究了在再生核希尔伯特空间中使用Moreau包络来对测度f-差异进行正则化的方法，并利用该方法分析了Wasserstein梯度流。 |
| [^96] | [Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment](https://arxiv.org/abs/2402.04599) | JEANIE是一种通过时间-视角对齐的方法，用于测量三维骨架序列的相似度。它能够解决视频序列中速度、时间位置和姿势的干扰变化问题。在评估了骨架Few-shot动作识别任务后，JEANIE在支持-查询序列对的时间块匹配方面表现出了良好的效果。 |
| [^97] | [Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)](https://arxiv.org/abs/2402.04596) | 本研究提出了一个双输出脉冲架构（DOSA）来解决连续多标签学习中的不平衡鲁棒性问题，并提出了一种新的不平衡感知的损失函数来提高多标记分类性能。 |
| [^98] | [A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents](https://arxiv.org/abs/2402.04580) | 这篇论文综述了机器人跨领域策略转移方法，讨论了从目标领域采集无偏数据的挑战，以及从源领域获取数据的成本效益性。同时，总结了不同问题设置下的设计考虑和方法。 |
| [^99] | [Collective Counterfactual Explanations via Optimal Transport](https://arxiv.org/abs/2402.04579) | 本论文提出了一种集体方法来形成反事实解释，通过利用个体的当前密度来指导推荐的行动，解决了个体为中心的方法可能导致的新的竞争和意想不到的成本问题，并改进了经典反事实解释的期望。 |
| [^100] | [OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences](https://arxiv.org/abs/2402.04567) | 本论文提出了一种用于顺序决策序列的异常检测框架，通过提取行为特征来检测异常。这种方法克服了强化学习方法在真实世界中应用困难的问题，并提供了关于异常的足够信息。 |
| [^101] | [An Artificial Intelligence (AI) workflow for catalyst design and optimization](https://arxiv.org/abs/2402.04557) | 该论文提出了一个创新的人工智能工作流程，通过结合大型语言模型、贝叶斯优化和主动学习循环，将催化剂合成领域的科学文献中的知识转化为可行的参数，以加快和增强催化剂的优化。 |
| [^102] | [Curvature-Informed SGD via General Purpose Lie-Group Preconditioners](https://arxiv.org/abs/2402.04553) | 本研究提出了一种通过利用曲率信息优化随机梯度下降（SGD）算法的新方法。该方法使用无矩阵预处理器和低秩近似预处理器，并在线更新这两种预处理器。通过对称性和不变性的约束，该方法消除了阻尼的需求，使得学习率和步长可以自然归一化，并且在大多数情况下默认值效果良好。 |
| [^103] | [Riemann-Lebesgue Forest for Regression](https://arxiv.org/abs/2402.04550) | 提出了一种新颖的集成方法Riemann-Lebesgue Forest (RLF)用于回归问题，通过划分函数的值域为多个区间来逼近可测函数的思想，开发了一种新的树学习算法Riemann-Lebesgue Tree。通过Hoeffding分解和Stein方法推导了RLF在不同参数设置下的渐近性能，并在仿真数据和真实世界数据集上的实验中证明了RLF与原始随机森林相比具有竞争力的性能。 |
| [^104] | [Learning Diverse Policies with Soft Self-Generated Guidance](https://arxiv.org/abs/2402.04539) | 本文提出了一种使用多样化的过去轨迹作为引导的方法，以实现更快、更高效的在线强化学习，即使这些轨迹是次优的或没有高奖励。通过将过去轨迹视为引导，而不是模仿它们，本方法可以使策略跟随和扩展过去的轨迹同时仍保持 |
| [^105] | [Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers](https://arxiv.org/abs/2402.04538) | 本论文提出了一种新颖的三元图转换器（TGT），通过三元注意力和聚合机制实现了图中相邻对之间的直接通信。通过预测原子间距离并进行下游任务的分子属性预测，我们的模型在多个分子属性预测基准上达到了最新的最优结果，并且在旅行推销员问题上也取得了最新的最优结果。 |
| [^106] | [Tactile-based Object Retrieval From Granular Media](https://arxiv.org/abs/2402.04536) | 这项研究介绍了一种基于触觉反馈的机器人操作方法，用于在颗粒介质中检索埋藏的物体。通过模拟传感器噪声进行端到端训练，实现了自然出现的学习推动行为，并成功将其迁移到实际硬件上。 |
| [^107] | [SumRec: A Framework for Recommendation using Open-Domain Dialogue](https://arxiv.org/abs/2402.04523) | 本研究提出了一个新的框架SumRec，用于从开放领域的对话中推荐个性化信息。该框架利用大型语言模型生成对话中说话者信息的摘要，并根据用户类型推荐物品信息，实验证明SumRec框架比基准方法提供更好的推荐。 |
| [^108] | [On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis](https://arxiv.org/abs/2402.04520) | 通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。 |
| [^109] | [Generalized Sobolev Transport for Probability Measures on a Graph](https://arxiv.org/abs/2402.04516) | 我们研究了支持在图度量空间上的测度的最优传输问题，提出了一种适用于不同几何结构的图上概率测度传输方法，并引入了超力 Wassestein（OW）的概念，为某些机器学习方法的发展带来了新的机遇。 |
| [^110] | [Online Cascade Learning for Efficient Inference over Streams](https://arxiv.org/abs/2402.04513) | 这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。 |
| [^111] | [Pathspace Kalman Filters with Dynamic Process Uncertainty for Analyzing Time-course Data](https://arxiv.org/abs/2402.04498) | 本论文提出了一种名为路径空间卡尔曼滤波器（PKF）的扩展算法，可以动态跟踪数据和先前知识的不确定性，并用贝叶斯方法量化不同的不确定性来源。通过在合成数据集上进行数值实验，我们证明了PKF优于传统KF方法，并将该方法应用于生物时间序列数据集。 |
| [^112] | [The Fine-Grained Complexity of Gradient Computation for Training Large Language Models](https://arxiv.org/abs/2402.04497) | 本文研究了训练大型语言模型中梯度计算的复杂性，证明了在某些参数区域内可以以几乎线性时间进行前向计算，但在其余参数区域内需要超过二次时间，这对于LLM训练的每个步骤都具有重要意义。 |
| [^113] | [Grandmaster-Level Chess Without Search](https://arxiv.org/abs/2402.04494) | 本研究通过在庞大的国际象棋数据集上进行训练，使用了一个270M参数的Transformer模型，不依赖于复杂的启发式算法或显式搜索，取得了大师级水平的国际象棋对局的成功。模型在Lichess闪电战评分上达到了2895，解决了一系列具有挑战性的国际象棋谜题，优于AlphaZero和GPT-3.5-turbo-instruct。通过系统研究，我们发现大规模的模型和数据集对于实现强大的国际象棋对局效果是至关重要的。 |
| [^114] | [A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs](https://arxiv.org/abs/2402.04493) | 这篇论文提出了一种在低秩MDPs上的离线约束强化学习算法，该算法通过部分数据覆盖假设实现了更高的计算效率并达到了$O(\epsilon^{-2})$的样本复杂度。此外，该算法还支持额外奖励信号的约束。 |
| [^115] | [De-amplifying Bias from Differential Privacy in Language Model Fine-tuning](https://arxiv.org/abs/2402.04489) | 本研究发现，在语言模型的微调过程中，差分隐私放大了性别、种族和宗教方面的偏见。我们通过对抗事实数据增强方法证明了DP可以减少偏见的放大。因此，我们可以使用DP和CDA来微调模型，同时保持可靠性和公平性。 |
| [^116] | [Incentivized Truthful Communication for Federated Bandits](https://arxiv.org/abs/2402.04485) | 针对联邦赌博学习的激励机制存在被策略性客户攻击的问题，我们提出一种名为Truth-FedBan的激励兼容通信协议，使得参与者的激励与其自报成本无关，只有报告真实成本才能实现最佳效用，并且仍能保证亚线性的遗憾和通信成本。 |
| [^117] | [IoT Network Traffic Analysis with Deep Learning](https://arxiv.org/abs/2402.04469) | 本研究介绍了基于深度学习的物联网网络流量分析，通过处理大量数据和使用无监督学习技术，实现了高效、自动化和可扩展的异常检测模型，其中在KDD Cup 99数据集上得到了超过98％的准确率。 |
| [^118] | [DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems](https://arxiv.org/abs/2402.04467) | 本文提出了一个新的框架，通过学习混沌系统的不变测度和动态来解决学习动态的困难，与传统方法不同，该框架在轨迹长度增加时具有较好的性能。 |
| [^119] | [Towards Deterministic End-to-end Latency for Medical AI Systems in NVIDIA Holoscan](https://arxiv.org/abs/2402.04466) | 本文针对NVIDIA Holoscan平台中的医疗AI系统的端到端延迟问题提出了解决方案，通过优化异构GPU工作负载，实现了确定性的延迟表现。 |
| [^120] | [The Potential of AutoML for Recommender Systems](https://arxiv.org/abs/2402.04453) | 这项研究探讨了在推荐系统领域中，自动机器学习（AutoML）的巨大潜力以及目前相对少有的关注和开发。 |
| [^121] | [Pushing the limits of cell segmentation models for imaging mass cytometry](https://arxiv.org/abs/2402.04446) | 该论文研究了不完美标签对基于学习的细胞分割模型的影响，发现从完全注释的标签中删除50%的细胞注释仅对DSC得分造成轻微降低，并且单组织模型在未知组织类型上的表现与多组织模型相近。 |
| [^122] | [Exploring higher-order neural network node interactions with total correlation](https://arxiv.org/abs/2402.04440) | 本文提出了一种名为局部相关性解释的新方法，通过基于数据流形上的距离进行聚类，并使用总相关性来捕获高阶变量之间的交互。通过在合成和真实世界数据中应用该方法，可以提取关于数据结构的隐藏洞察力，并用于探索和解释神经网络的内部运作。 |
| [^123] | [Structured Entity Extraction Using Large Language Models](https://arxiv.org/abs/2402.04437) | 本论文提出了一种使用大型语言模型的结构化实体提取方法，在此任务上通过引入AESOP度量评估模型性能，并将整个提取任务分解为多个阶段，相较于基准模型取得了更好的效果，为未来结构化实体提取的进一步发展提供了有希望的方向。 |
| [^124] | [Continuous Multidimensional Scaling](https://arxiv.org/abs/2402.04436) | 连续多维标度是关于将距离信息嵌入欧几里得空间的过程，并探讨了在对象集不断增加的情况下，将整个嵌入问题序列视为一个固定空间中的一系列优化问题的方法和结论。 |
| [^125] | [PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection](https://arxiv.org/abs/2402.04435) | PreGIP是针对深度知识产权保护的一种图神经网络预训练水印技术，它通过添加无任务的水印损失来给预训练的GNN编码器的嵌入空间添加水印，并采用抗微调的水印注入方法 |
| [^126] | [What limits performance of weakly supervised deep learning for chest CT classification?](https://arxiv.org/abs/2402.04419) | 本研究测试了弱监督学习对胸部CT分类性能的限制。结果显示，在训练数据中增加错误标签的情况下，模型可以忍受高达10%的错误。同时，随着训练数据量的增加，所有疾病的分类性能稳步提升。 |
| [^127] | [Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit](https://arxiv.org/abs/2402.04417) | 本研究通过将分散区块链技术与新颖机制结合，设计了一种稳健的多智能体多臂赌博问题解决方案，以确保诚实参与者获得的累积奖励，并应对恶意行为和保护参与者隐私的需求。 |
| [^128] | [A Data Centric Approach for Unsupervised Domain Generalization via Retrieval from Web Scale Multimodal Data](https://arxiv.org/abs/2402.04416) | 该论文基于大规模多模态数据检索，提出了一个无监督领域泛化的数据中心方法。在多模态无监督领域泛化问题中，通过构建一个小型的源数据子集，而不是依赖丰富的源数据，来解决目标标签空间数据获取困难的问题。 |
| [^129] | [The VampPrior Mixture Model](https://arxiv.org/abs/2402.04412) | 本论文提出了VampPrior混合模型（VMM），它是一种新颖的DLVM先验，可用于深度潜变量模型的集成和聚类，通过改善当前聚类先验的不足，并提出了一个清晰区分变分和先验参数的推理过程。使用VMM的变分自动编码器在基准数据集上取得了强大的聚类性能，将VMM与scVI相结合可以显著提高其性能，并自动将细胞分组为具有生物意义的聚类。 |
| [^130] | [Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton](https://arxiv.org/abs/2402.04411) | 本文介绍了DFA-LLM（确定有限自动机增强的大规模语言模型）框架，通过嵌入从对话中学习到的确定有限自动机在大规模语言模型中，使得对话代理能够生成具有规范合规性的回复，并在广泛的基准测试中验证了其有效性。 |
| [^131] | [Towards Fair, Robust and Efficient Client Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2402.04409) | 本文提出了一种名为FRECA的方法来评估联邦学习中客户的贡献。该方法使用FedTruth框架估计全局模型的真实更新，平衡来自所有客户的贡献，并排除恶意客户的影响。FRECA对于拜占庭攻击具有鲁棒性，并且具有高效性。 |
| [^132] | [Edge-Parallel Graph Encoder Embedding](https://arxiv.org/abs/2402.04403) | 这篇论文提出了一种边并行图编码嵌入的算法，通过重构并行化实现，在大规模图上取得了500倍的加速效果。 |
| [^133] | [CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines](https://arxiv.org/abs/2402.04400) | CEHR-GPT是一种使用病人时间轴生成电子健康记录的方法，能够处理EHR数据的合成、疾病进展分析等应用。 |
| [^134] | [Learning from Time Series under Temporal Label Noise](https://arxiv.org/abs/2402.04398) | 该论文研究了在时间序列下处理时间标签噪声的问题，提出了一种可以从数据中直接估计时间标签噪声函数并训练出噪声容忍分类器的方法，并在实验中展示了该方法在各种时间标签噪声函数下都取得了最先进的性能。 |
| [^135] | [QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396) | QuIP#是一种使用哈达玛德非相干性和格书的权重量化方法，能够在极限压缩范围下达到最先进的结果，并具有快速推理的优势。 |
| [^136] | [Densely Multiplied Physics Informed Neural Network](https://arxiv.org/abs/2402.04390) | 该论文通过改进神经网络架构，提出了一种密集乘法物理信息神经网络（DM-PINN）架构，它有效利用隐藏层的输出，显著提高了PINN的准确性和性能。 |
| [^137] | [Denoising Diffusion Probabilistic Models in Six Simple Steps](https://arxiv.org/abs/2402.04384) | 本论文提供了一个简单、全面、干净且清晰的介绍去噪扩散概率模型（DDPM）的方法，强调了从连续时间极限的视角出发，以提供更好的理解和实际性能。 |
| [^138] | [FairWire: Fair Graph Generation](https://arxiv.org/abs/2402.04383) | 这项研究关注于分析和减轻真实和合成图中的结构偏差，通过设计一个多功能的公平性正则化器来缓解偏差影响。 |
| [^139] | [Fine-Tuned Language Models Generate Stable Inorganic Materials as Text](https://arxiv.org/abs/2402.04379) | 细调语言模型用于生成稳定材料，具有可靠性高和灵活性强的优势，能以较高的速率生成被预测为亚稳态的材料。 |
| [^140] | [$\texttt{NeRCC}$: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems](https://arxiv.org/abs/2402.04377) | NeRCC是一个通用的抗拖尾节点的近似编码计算框架，包括回归编码、计算和回归解码三个层次，通过优化两个正则化项的依赖关系来解决嵌套回归问题。 |
| [^141] | [Scaling laws for learning with real and surrogate data](https://arxiv.org/abs/2402.04376) | 本研究探讨了将替代数据与真实数据整合以进行训练的方案，发现整合替代数据能够显著降低测试误差，并提出了一个扩展规律来描述混合模型的测试误差，可以用于预测最优加权和收益。 |
| [^142] | [Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving, Differentially-Private, Synthetic Data](https://arxiv.org/abs/2402.04375) | 本文提出了在保持边缘一致的差分隐私合成数据上训练线性模型的过量风险的新界限，为连续和Lipschitz损失函数提供了上界和下界。 |
| [^143] | [Neural Networks Learn Statistics of Increasing Complexity](https://arxiv.org/abs/2402.04362) | 本文证明了分布简单性倾向（DSB）的神经网络学习规律，即在训练早期自动学习低阶矩的最大熵分布特征，然后在训练后期失去这种能力。此外，研究还利用最优传输方法进行了低阶统计数据的编辑，证明了早期训练的网络会将编辑的样本视为目标类别的样本。 |
| [^144] | [Adaptive Inference: Theoretical Limits and Unexplored Opportunities](https://arxiv.org/abs/2402.04359) | 本论文提出了自适应推理算法的效率和性能提升的理论框架，并通过经验证据证明可以实现10-100倍的效率提升，在优化选择和设计自适应推理状态空间方面提供了见解。 |
| [^145] | [PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation](https://arxiv.org/abs/2402.04355) | PQMass是一种使用概率质量估计来评估生成模型质量的全面方法，能够直接处理高维数据，不依赖于假设或训练其他模型。 |
| [^146] | [The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry](https://arxiv.org/abs/2402.04347) | Hedgehog是一种具有Softmax模仿的可学习线性注意力，通过保持尖锐和单调性来弥补线性注意力在质量上的不足。 |
| [^147] | [Does Confidence Calibration Help Conformal Prediction?](https://arxiv.org/abs/2402.04344) | 本文研究了去校准对一致性预测的影响，发现事后校准方法导致更大的预测集，而过于自信的情况有利于一致性预测性能。基于这一分析，提出了一种新的一致性温度缩放方法 (ConfTS)，其通过优化温度值来改进一致性预测性能。 |
| [^148] | [LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text](https://arxiv.org/abs/2402.04335) | 本研究利用LLMs构建数据集，通过微调模型和使用闭源LLMs进行少样本实验，成功实现了非结构化文本中法律违规行为的检测和与受害者的关联。结果显示我们的设置适用于这两个任务，F1分数分别为62.69％和81.02％。研究最终公开发布了数据集和代码，以推动法律自然语言处理领域的进一步研究。 |
| [^149] | [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://arxiv.org/abs/2402.04333) | LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。 |
| [^150] | [Personality Trait Recognition using ECG Spectrograms and Deep Learning](https://arxiv.org/abs/2402.04326) | 通过应用深度学习方法和心电图谱，本研究创新性地实现了对人格特征的识别。研究使用心电图谱作为特征，在卷积神经网络和可视化变换器的引导下，实现了准确的人格特征分类。这对人格分析和心理研究领域具有重要的贡献。 |
| [^151] | [Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to Non-Essential Neurons](https://arxiv.org/abs/2402.04325) | 本文提出了一种有效的方法，通过向非关键神经元注入噪音来增强DNN的对抗鲁棒性和执行效率。与以往的方法不同，我们通过在每个DNN层上策略性地引入噪音来干扰对抗攻击。实验结果表明，我们的方法成功地提高了对抗鲁棒性和执行效率。 |
| [^152] | [Deep PCCT: Photon Counting Computed Tomography Deep Learning Applications Review](https://arxiv.org/abs/2402.04301) | 深度光子计数计算机断层扫描（PCCT）是应对医学成像的挑战的创新技术，具有提高乳腺微小异常检测的疗效和细节水平的潜力。此外，PCCT还集成了深度学习和影像组学特征的研究，成功应用于数据处理。PCCT面临挑战并需进一步发展。 |
| [^153] | [Multi-View Symbolic Regression](https://arxiv.org/abs/2402.04298) | 多视角符号回归(MvSR)是一种同时考虑多个数据集的符号回归方法，能够找到一个参数化解来准确拟合所有数据集，解决了传统方法无法处理不同实验设置的问题。 |
| [^154] | [LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\times$ Faster Inference](https://arxiv.org/abs/2402.04296) | 本论文介绍了一种光HGNN方法，将超图神经网络(HGNNs)转化为Multi-Layer Perceptron (MLPs)以提高推断速度。LightHGNN通过软标签将知识从teacher HGNN蒸馏到student MLPs，而LightHGNN$^+$则注入了可靠的高阶相关性。 |
| [^155] | [AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies](https://arxiv.org/abs/2402.04292) | AdaFlow是一个基于流模型的模仿学习框架，通过使用变异自适应ODE求解器，在保持多样性的同时，提供快速推理能力。 |
| [^156] | [BiLLM: Pushing the Limit of Post-Training Quantization for LLMs](https://arxiv.org/abs/2402.04291) | BiLLM是一种针对预训练LLMs的1位后训练量化方案，通过识别重要的权重和优化二值化，成功实现了高准确度的推理。 |
| [^157] | [CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling](https://arxiv.org/abs/2402.04290) | CasCast是一个高分辨率降水即时预测框架，通过熟练的级联建模解决了复杂降水系统演化和极端降水预测两个关键挑战。 |
| [^158] | [Progress and Opportunities of Foundation Models in Bioinformatics](https://arxiv.org/abs/2402.04286) | 生物信息学中的基础模型（FMs）通过整合人工智能技术，解决了注释数据稀缺和数据噪声等挑战，在处理大规模无标签数据和有效表示多样化生物实体方面具有出色成果，在计算生物学领域开启了新时代。 |
| [^159] | [PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks](https://arxiv.org/abs/2402.04284) | 这篇论文研究了如何实现可扩展的基于内存的动态图神经网络，并解决了训练中的时间间断问题。通过使用内存模块提取和记忆长期的时间依赖关系，MDGNNs在处理大的时间批量时表现出更好的性能和灵活性。 |
| [^160] | [Gaussian Plane-Wave Neural Operator for Electron Density Estimation](https://arxiv.org/abs/2402.04278) | 本研究提出了一种名为高斯平面波神经算子(GPWNO)的方法，用于机器学习中的电子密度预测。该方法利用平面波和高斯型轨道基底在无限维功能空间中进行操作，可以有效地表示密度的高频和低频成分，并在多个数据集上展示出卓越的性能。 |
| [^161] | [FPGA Deployment of LFADS for Real-time Neuroscience Experiments](https://arxiv.org/abs/2402.04274) | 通过将LFADS模型有效地部署到FPGA上，我们实现了处理大规模神经活动数据的低延迟推断，为实时神经科学实验提供了新的机会。 |
| [^162] | [Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources](https://arxiv.org/abs/2402.04273) | 本文提出了一种名为FDA的框架，通过跨领域学习来打破多智能体感知中的数据孤岛问题。该框架包括可学习特征补偿模块和分布感知统计一致性模块，用于增强中间特征交流和数据分布一致性。 |
| [^163] | [Large Vocabulary Spontaneous Speech Recognition for Tigrigna](https://arxiv.org/abs/2402.04254) | 本文研究了一种Tigrigna语言的大词汇量自发语音识别系统的设计与开发，通过使用Sphinx工具开发声学模型和使用SRIM工具开发语言模型。这项研究的创新点是实现了对Tigrigna语言的自动语音识别。 |
| [^164] | [Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science](https://arxiv.org/abs/2402.04247) | 本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。 |
| [^165] | [Gradient Coding in Decentralized Learning for Evading Stragglers](https://arxiv.org/abs/2402.04193) | 本文提出了一种新的基于八卦的梯度编码分散式学习方法（GOCO），以解决存在延迟节点的分散式学习问题。与基准方法相比，该方法在学习性能上具有优越性。 |
| [^166] | [A General Theory for Kernel Packets: from state space model to compactly supported basis](https://arxiv.org/abs/2402.04022) | 该论文提出了一种从状态空间模型到紧支持基的核分组的通用理论，该理论可以用于降低高斯过程的训练和预测时间，并且通过适当的线性组合产生了$m$个紧支持的核分组函数。 |
| [^167] | [Cross Entropy versus Label Smoothing: A Neural Collapse Perspective](https://arxiv.org/abs/2402.03979) | 本研究从神经崩溃的视角研究了标签平滑，并发现模型在标签平滑训练下更快地收敛到神经崩溃解，并达到更强的神经崩溃水平。此外，标签平滑损失下的模型在相同的NC1水平下表现出加强的NC2，并可在理论上更快地收敛。 |
| [^168] | [AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction](https://arxiv.org/abs/2402.03784) | AirPhyNet是一种利用物理引导的神经网络方法，通过将空气颗粒运动的物理原理和神经网络结构相结合，提高了对空气质量的预测精度和可解释性。 |
| [^169] | [Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification](https://arxiv.org/abs/2402.03780) | 本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。 |
| [^170] | [Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models](https://arxiv.org/abs/2402.03659) | 这个论文介绍了使用大型语言模型生成可解释的股票预测的方法，并提出了Summarize-Explain-Predict（SEP）模型来解决股票预测中的解释问题和数据标注成本的挑战。 |
| [^171] | [MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models](https://arxiv.org/abs/2402.03583) | 研究者发现知识图谱嵌入模型存在的“Z-悖论”限制了其表达能力，并提出了一种名为MQuinE的新模型，通过理论证明，MQuinE成功解决了Z-悖论，并在链接预测任务中显著优于现有模型。 |
| [^172] | [Exploring Prime Number Classification: Achieving High Recall Rate and Rapid Convergence with Sparse Encoding](https://arxiv.org/abs/2402.03363) | 通过稀疏编码和神经网络结构的组合，本文提出了一种在质数和非质数分类中实现高召回率和快速收敛的新方法，取得了令人满意的结果。 |
| [^173] | [A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation](https://arxiv.org/abs/2402.03358) | 这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。 |
| [^174] | [Zero-shot Object-Level OOD Detection with Context-Aware Inpainting](https://arxiv.org/abs/2402.03292) | 本论文提出了一种用上下文感知修复的零样本物体级OOD检测方法RONIN。通过将检测到的对象进行修复替换，并使用预测的ID标签来条件化修复过程，使得重构的对象在OOD情况下与原始对象相差较远，从而有效区分ID和OOD样本。实验证明RONIN在多个数据集上取得了具有竞争力的结果。 |
| [^175] | [Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?](https://arxiv.org/abs/2402.03214) | 这项研究探讨了如何区分人类艺术和AI生成的图像，并提供了几种不同的方法，包括通过监督学习训练的分类器、扩散模型的研究工具以及专业艺术家的知识。这对防止欺诈、遵守政策以及避免模型崩溃都具有重要意义。 |
| [^176] | [Taylor Videos for Action Recognition](https://arxiv.org/abs/2402.03019) | Taylor视频是一种新的视频格式，用于动作识别中的动作提取问题。它通过使用Taylor展开近似计算隐含的动作提取函数，从而解决了动作提取中的挑战性问题。 |
| [^177] | [DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network](https://arxiv.org/abs/2402.02910) | 本研究提出了一种使用双尺度多阶段时间卷积网络的Otago体操识别方法，通过单个腰部佩戴的IMU在老年人的日常生活中实现准确且稳定的识别，为康复举措提供支持。 |
| [^178] | [Statistical Guarantees for Link Prediction using Graph Neural Networks](https://arxiv.org/abs/2402.02692) | 本文提出了一种线性图神经网络（LG-GNN）架构，通过计算边缘概率来预测图中的链接，并推导了其在链接预测任务中的性能统计保证。这种架构对于稀疏和稠密图都适用，并在真实和合成数据集上验证了其优势。 |
| [^179] | [Increasing Trust in Language Models through the Reuse of Verified Circuits](https://arxiv.org/abs/2402.02619) | 本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。 |
| [^180] | [LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model](https://arxiv.org/abs/2402.02544) | LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。 |
| [^181] | [TopoX: A Suite of Python Packages for Machine Learning on Topological Domains](https://arxiv.org/abs/2402.02441) | TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。 |
| [^182] | [Symbol: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning](https://arxiv.org/abs/2402.02355) | 本研究提出了一种名为\textsc{Symbol}的新框架，通过符号方程学习来自动发现黑盒优化器。实验结果表明，\textsc{Symbol}生成的优化器在超越现有基准线的同时，还展现出出色的零样本泛化能力。 |
| [^183] | [Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models](https://arxiv.org/abs/2402.02347) | 本研究通过引入Riemannian预条件器，增强了LoRA微调过程中的优化步骤。实验结果表明，使用该预条件器可以显著提升SGD和AdamW的收敛性和可靠性，并使训练过程更加稳健。此外，理论分析证明了在凸参数化下使用该预条件器微调ReLU网络的收敛速度与数据矩阵的条件数无关。这个新的Riemannian预条件器在经典的低秩矩阵恢复中已经有过研究。 |
| [^184] | [Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep Learning Model](https://arxiv.org/abs/2402.02304) | 本文提出了一个由端到端深度学习模型加强的高效数值波传播方法，通过结合数值求解器和深度学习组件，优化算法架构、数据生成和并行时间算法，实现了在保持速度的同时显著提高性能。 |
| [^185] | [Graph Foundation Models](https://arxiv.org/abs/2402.02216) | 图基础模型是图领域中一个新兴的研究课题，旨在开发一个能够跨不同图和任务进行泛化的图模型。我们提出了一种新的 GFM 发展视角，通过倡导“图词汇表”来编码图的不变性，有助于推进未来的GFM设计。 |
| [^186] | [Position Paper: The Landscape and Challenges of HPC Research and LLMs](https://arxiv.org/abs/2402.02018) | 运用语言模型技术于高性能计算任务中具有巨大潜力 |
| [^187] | [Online Uniform Risk Times Sampling: First Approximation Algorithms, Learning Augmentation with Full Confidence Interval Integration](https://arxiv.org/abs/2402.01995) | 本文首次引入在线均匀风险时间抽样问题，并提出了两种在线近似算法，一种带有学习增强，一种没有学习增强。通过竞争比分析，我们提供了严格的理论性能保证。我们通过合成实验和实际案例研究评估了算法的性能。 |
| [^188] | [Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems](https://arxiv.org/abs/2402.01748) | 本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。 |
| [^189] | [CFTM: Continuous time fractional topic model](https://arxiv.org/abs/2402.01734) | CFTM是一种新的动态主题建模方法，通过使用分数布朗运动来识别随时间变化的主题和词分布的正负相关性，揭示长期依赖性或粗糙度。实证研究结果表明，该模型能够有效地识别和跟踪主题的长期依赖性或粗糙度。 |
| [^190] | [Detecting Multimedia Generated by Large AI Models: A Survey](https://arxiv.org/abs/2402.00045) | 本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。 |
| [^191] | [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](https://arxiv.org/abs/2401.18079) | KVQuant是一种解决LLM推理中大量内存消耗的KV缓存量化方法，通过引入新颖的量化方法，包括分通道键量化、RoPE前量化键和非均匀KV缓存量化，准确表示超低精度的KV激活。 |
| [^192] | [Regularized Linear Discriminant Analysis Using a Nonlinear Covariance Matrix Estimator](https://arxiv.org/abs/2401.17760) | 本文研究了使用非线性协方差矩阵估计器的正则化线性判别分析方法，以解决特征空间维度高于训练数据大小时数据协方差矩阵病态导致效率低下的问题。 |
| [^193] | [Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers](https://arxiv.org/abs/2401.16123) | 这项研究提出了一种增量学习的多模式目标引用框架，可以适应个体驾驶员的变化行为和各种驾驶场景。 |
| [^194] | [An objective comparison of methods for augmented reality in laparoscopic liver resection by preoperative-to-intraoperative image fusion](https://arxiv.org/abs/2401.15753) | 本文比较了腹腔镜肝切除中增强现实方法的客观优劣，提出了术前到术中图像融合挑战，旨在自动化该过程，有效实现增强现实在手术室的应用。 |
| [^195] | [Two Types of AI Existential Risk: Decisive and Accumulative](https://arxiv.org/abs/2401.07836) | 本文对比了传统的“决定性AI x-risk假设”与“累积性AI x-risk假设”，指出人工智能可能带来的灭绝性灾难有两种可能路径：一种是突然发生的AI接管，另一种是逐渐积累的威胁。 |
| [^196] | [Emergence of In-Context Reinforcement Learning from Noise Distillation](https://arxiv.org/abs/2312.12275) | 该论文介绍了一种从噪声中生成上下文强化学习的方法，通过构建噪声注入课程来获取学习历史，可以实现在学习数据集中超过最优策略的性能表现。 |
| [^197] | [Label-Free Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2312.11549) | 提出了一种无监督的多变量时间序列异常检测方法，通过估计整个训练样本的密度，识别异常实例，而不依赖于清洁的训练数据集。 |
| [^198] | [How Far Can Fairness Constraints Help Recover From Biased Data?](https://arxiv.org/abs/2312.10396) | 公平性约束在极度有偏差的数据上能够恢复到原始数据分布上准确和公平的分类器。 |
| [^199] | [Greedy Shapley Client Selection for Communication-Efficient Federated Learning](https://arxiv.org/abs/2312.09108) | 本文提出了一种新的有偏差的客户选择策略，称为GreedyFed，它利用快速逼近算法计算每个客户的Shapley值，并在每个通信循环中贪婪地选择最具贡献的客户。与其他客户选择策略相比，在实际数据集上取得了更好的模型训练效果。 |
| [^200] | [Factorized Explainer for Graph Neural Networks](https://arxiv.org/abs/2312.05596) | 本研究发现了传统基于图信息瓶颈原则的方法在解释图神经网络时存在平凡解，并提出了一种修改后的原则来避免此问题。同时，提出了一个带有理论性能保证的因式分解解释模型，并通过实验验证了其性能。 |
| [^201] | [DiSK: A Diffusion Model for Structured Knowledge](https://arxiv.org/abs/2312.05253) | DiSK是一种针对结构化数据的扩散模型，通过使用高斯混合模型方法处理文本、分类和连续数值数据，采用扩散训练来建模属性之间的关系，具有在多个不同领域的数据集上具有最先进性能的特点。 |
| [^202] | [Recurrent Distance Filtering for Graph Representation Learning](https://arxiv.org/abs/2312.01538) | 本文提出了一种新的图网络架构，利用最短距离对节点进行聚合，并使用线性RNN对跳代表的序列进行编码，用以解决图神经网络在远距离节点信息利用上的困难，该模型在实验中表现出与其他模型相当的竞争力。 |
| [^203] | [RefinedFields: Radiance Fields Refinement for Unconstrained Scenes](https://arxiv.org/abs/2312.00639) | RefinedFields是第一种利用预训练模型改善无约束场景建模的方法。通过优化指导和交替训练过程，该方法能够从真实世界图像的先验条件中提取更丰富的细节，并在新视角合成任务中优于以往的方法。 |
| [^204] | [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455) | 本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。 |
| [^205] | [Assumption-lean and Data-adaptive Post-Prediction Inference](https://arxiv.org/abs/2311.14220) | 这项工作介绍了一种假设简化和数据自适应的后预测推断（POP-Inf）过程，可以有效且有力地基于机器学习预测结果进行统计推断。 |
| [^206] | [Labeled Interactive Topic Models](https://arxiv.org/abs/2311.09438) | 这篇论文介绍了一种用户友好的交互式神经主题模型，通过用户分配单词标签来更新主题模型，使得主题更加相关和准确。这种方法包括可训练和后训练集成两种不同类型的神经主题模型。 |
| [^207] | [Data-Efficient Task Generalization via Probabilistic Model-based Meta Reinforcement Learning](https://arxiv.org/abs/2311.07558) | PACOH-RL是一种基于概率模型的元强化学习算法，通过元学习动态模型的先验知识和引入正则化和认知不确定性量化，实现了在数据有限的情况下对新的动态环境的高效适应。 |
| [^208] | [Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation](https://arxiv.org/abs/2311.06973) | 本论文研究了使用深度神经网络进行配电系统同步时间状态估计的性能和鲁棒性。通过将输入扰动视为混合整数线性规划问题进行分析验证，并强调了批归一化在提高问题可扩展性方面的作用。该框架在修改后的IEEE 34节点系统和真实的大型分布系统上进行验证。 |
| [^209] | [Resource-Aware Hierarchical Federated Learning for Video Caching in Wireless Networks](https://arxiv.org/abs/2311.06918) | 本文提出了一种资源感知分层联邦学习方法(RawHFL)，用于无线网络视频缓存，通过预测用户未来的内容请求，以改善回传流量拥塞问题。该方法考虑了部分客户端参与，通过优化客户端的选择、本地训练轮次和CPU频率，以最小化一个加权效用函数，实现了RawHFL的收敛。 |
| [^210] | [Heuristic Optimal Transport in Branching Networks](https://arxiv.org/abs/2311.06650) | 本论文提出了一种适用于分支网络的快速启发式最优传输方法，通过优化连接源和目标的直线段，解决了传统最优传输方法在分支结构存在时的不适用问题。 |
| [^211] | [Optimization-Free Test-Time Adaptation for Cross-Person Activity Recognition](https://arxiv.org/abs/2310.18562) | 本文提出了一种无需优化的测试时适应（OFTTA）框架，用于解决人体活动识别中的个体间分布偏移问题。该框架通过调整特征提取器和线性分类器的方式来适应不同个体的活动模式，并采用指数衰减测试时归一化（EDTN）替代传统批归一化来提取可靠的特征。 |
| [^212] | [Drug Discovery with Dynamic Goal-aware Fragments](https://arxiv.org/abs/2310.00841) | 提出了一种名为GEAM的分子生成框架，用于药物发现。该框架通过构建目标感知片段词汇，识别贡献于所需目标特性的重要片段，并在生成过程中更新片段词汇。 |
| [^213] | [OHQ: On-chip Hardware-aware Quantization](https://arxiv.org/abs/2309.01945) | 本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），通过构建量化感知流水线和引入掩码引导的量化估计技术，实现了在资源受限的硬件上进行高效量化，填补了现有混合精度量化的搜索空间过大和实际部署差距大的问题。 |
| [^214] | [Gated recurrent neural networks discover attention](https://arxiv.org/abs/2309.01775) | 本研究发现，具备线性循环层和带有乘法门控的前馈路径的RNN可以精确地实现注意力，这是Transformer的主要构建模块。逆向工程的结果显示，梯度下降在实践中发现了该构造，并使RNN具备与Transformer相同的基于注意力的上下文学习算法。 |
| [^215] | [Vector Quantile Regression on Manifolds](https://arxiv.org/abs/2307.01037) | 通过利用最优传输理论和c-凹函数，我们在流形上定义了高维变量的条件向量分位数函数（M-CVQFs），实现了分位数估计、回归和条件置信集和似然度的计算。 |
| [^216] | [Stochastic Unrolled Federated Learning](https://arxiv.org/abs/2305.15371) | 随机展开的联邦学习（SURF）是一种扩展了算法展开到联邦学习的优化方法，在解决需要整个数据集的挑战和保持联邦学习分布式特性的同时，加快了收敛速度。 |
| [^217] | [A Unified Theory of Diversity in Ensemble Learning](https://arxiv.org/abs/2301.03962) | 这篇论文提出了一个统一的集成学习多样性理论，解释了多样性在各种监督学习场景中的本质。它揭示了多样性在集成损失的偏差-方差分解中的作用，同时提出了一种量化多样性效果的方法。这个理论对于提高集成模型的性能具有重要意义。 |
| [^218] | [A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks](https://arxiv.org/abs/2212.00720) | 本文提出了一种稳定、快速且完全自动的预测编码网络学习算法(iPC)，通过改变突触权重的更新规则的时间调度，提高了训练效率和稳定性，并具有收敛性保证。实验证明，在图像分类和语言模型训练等多个任务上，iPC相比原始算法在测试准确性、效率和收敛性方面表现更好。 |
| [^219] | [To be or not to be stable, that is the question: understanding neural networks for inverse problems](https://arxiv.org/abs/2211.13692) | 本文分析了神经网络在解决线性成像逆问题时稳定性和准确性之间的权衡，并提出了多种解决方案来增加网络的稳定性和保持准确性。数值实验验证了理论结果和效果。 |
| [^220] | [Adversarial Bandits against Arbitrary Strategies](https://arxiv.org/abs/2205.14839) | 该论文研究了对抗性赌博机问题，针对任意策略，提出了使用在线镜像下降方法的主控基础框架，并使用自适应学习率的OMD来减轻方差的影响，取得了较好的结果。 |
| [^221] | [Medium Access Control protocol for Collaborative Spectrum Learning in Wireless Networks](https://arxiv.org/abs/2111.12581) | 本文提出了一种中等访问控制协议，可在高负载网络中实现最小后悔和高谱效的谱协作。该协议基于完全分布式算法，解决了信道分配和访问调度问题，并通过单信道机会载波感知进行低复杂度的分布式拍卖。 |
| [^222] | [Semi-supervised learning for generalizable intracranial hemorrhage detection and segmentation](https://arxiv.org/abs/2105.00582) | 本研究开发和评估了一种半监督学习模型，可用于颅内出血的检测和分割，在超出分布的数据集上具有良好的推广能力。 |
| [^223] | [A Comprehensive Guide to CAN IDS Data & Introduction of the ROAD Dataset](https://arxiv.org/abs/2012.14600) | 提供了现有开放式CAN入侵数据集的综合指南，包括对每个数据集的质量分析以及各自的优点、缺点和建议的使用案例。当前数据集仅限于真实制造和模拟攻击，缺乏保真度和对车辆物理影响的验证。 |
| [^224] | [Domain Adaptation based Interpretable Image Emotion Recognition using Facial Expression Recognition](https://arxiv.org/abs/2011.08388) | 本论文提出了一种基于领域自适应的图像情绪识别方法，通过提出面部情绪识别系统并将其适应为图像情绪识别系统，解决了预训练模型和数据集不足的挑战。同时提出了一种新颖的解释性方法，用于解释情绪识别中关键的视觉特征。 |
| [^225] | [An Equivalence between Bayesian Priors and Penalties in Variational Inference](https://arxiv.org/abs/2002.00178) | 这篇论文描述了贝叶斯先验和变分推理中的惩罚之间的等价关系，并提供了一种计算给定惩罚项所对应的先验的方法。 |
| [^226] | [PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset.](http://arxiv.org/abs/2401.16803) | 本研究介绍了PBSCSR数据集，用于研究钢琴乐谱作曲家风格识别。数据集包含了盗版乐谱图像和相关元数据，可以进行多个研究任务。 |
| [^227] | [cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and Glitch Generation.](http://arxiv.org/abs/2401.16356) | cDVGAN是一个灵活的生成对抗网络模型，用于模拟多类引力波信号和探测器故障，并通过引入辅助鉴别器分析一阶导数时间序列来更好地捕捉原始数据特征。 |
| [^228] | [lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold Gap.](http://arxiv.org/abs/2401.15879) | 本文提出了一种名为lil'HDoC的算法，用于解决小阈值间隙下的好臂识别问题。实验证明该算法在样本效率上优于现有算法。 |
| [^229] | [Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization.](http://arxiv.org/abs/2401.15604) | 本文提出了基于神经网络的扩散模型中分数估计的优化和泛化方法，并建立了对分数估计进行分析的数学框架。 |
| [^230] | [TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records.](http://arxiv.org/abs/2401.14694) | TA-RNN和TA-RNN-AE是两种基于RNN的可解释深度学习架构，用于分析电子健康记录并预测患者的临床结果。这些架构考虑了EHR数据的不规则性和时间间隔，并采用时间嵌入的方法解决了这些问题。 |
| [^231] | [A Survey on Efficient Federated Learning Methods for Foundation Model Training.](http://arxiv.org/abs/2401.04472) | 这项调查研究了高效联邦学习方法在基础模型训练中的应用，提出了一个新的分类方法以优化计算和通信效率。该研究还讨论了当前广泛使用的FL框架，并展望了未来的研究潜力。 |
| [^232] | [DiarizationLM: Speaker Diarization Post-Processing with Large Language Models.](http://arxiv.org/abs/2401.03506) | 本文介绍了DiarizationLM框架，利用大语言模型对说话人分离系统的输出进行后处理。实验证明，使用finetuned的PaLM 2-S模型可以显著减少分离错误率，对多种目标都有优化效果。 |
| [^233] | [Towards Optimal Statistical Watermarking.](http://arxiv.org/abs/2312.07930) | 追求最优统计水印技术。通过将统计水印技术视为假设检验问题并引入伪随机生成器，我们实现了输出令牌和拒绝区域的耦合，实现了第一类错误和第二类错误之间的非平凡权衡，同时提出了最统一最有力的水印和最小化第二类错误的解决方案。我们还提供了独立同分布令牌数量的上下界，突显了改进的潜力。此外，我们还探讨了鲁棒性水印问题。 |
| [^234] | [Accelerating Generalized Linear Models by Trading off Computation for Uncertainty.](http://arxiv.org/abs/2310.20285) | 本论文提出了一种迭代方法，通过增加不确定性来降低计算量，并显著提高广义线性模型的训练速度。 |
| [^235] | [Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer.](http://arxiv.org/abs/2310.20172) | 提出了一种叫做CBS-GPT的生成预训练Transformer模型，用于紧凑二进制系统波形生成，在预测准确性上达到了较高的准确率，并且具有显著的解释性能。 |
| [^236] | [Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning.](http://arxiv.org/abs/2310.20007) | 本文在多种情境下证明了汤普森采样在强化学习中的贝叶斯遗憾界，并通过对信息比的精确分析提出了一个基于时间不均匀强化学习问题的上界估计。同时，本文找到了各种设置中具体的界限，并讨论了这些结果是第一个其类别或改进了最先进方法的情况。 |
| [^237] | [rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition.](http://arxiv.org/abs/2310.19283) | rTsfNet是一种新的DNN模型，通过多头3D旋转和时序特征提取实现了IMU-based人体活动识别，并在多个数据集上取得了最高的准确率。 |
| [^238] | [Localization of Small Leakages in Water Distribution Networks using Concept Drift Explanation Methods.](http://arxiv.org/abs/2310.15830) | 本研究提出了一种只使用压力测量进行水配水网络中小漏洞定位的方法，旨在解决因气候变化导致的饮用水稀缺问题。 |
| [^239] | [Defending Our Privacy With Backdoors.](http://arxiv.org/abs/2310.08320) | 本研究提出了一种基于后门攻击的防御方法，通过对模型进行策略性插入后门，对齐敏感短语与中性术语的嵌入，以删除训练数据中的私人信息。实证结果显示该方法的有效性。 |
| [^240] | [Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders.](http://arxiv.org/abs/2310.08164) | 通过使用稀疏自编码器，我们提出了一种解释RLHF调整的语言模型中学习的奖励函数的新方法。这个方法能够通过比较自编码器隐藏空间来识别学习奖励模型准确性的独特特征，并提供了奖励完整性的抽象近似值。 |
| [^241] | [Imitation Learning from Observation with Automatic Discount Scheduling.](http://arxiv.org/abs/2310.07433) | 我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。 |
| [^242] | [Entropy-MCMC: Sampling from Flat Basins with Ease.](http://arxiv.org/abs/2310.05401) | 本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。 |
| [^243] | [Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses.](http://arxiv.org/abs/2310.03311) | 该论文介绍了一个基于信息理论的统一原理，用于重新推导和推广现有的变分降维方法，并设计新的方法。通过将多变量信息瓶颈解释为两个贝叶斯网络的权衡，该框架引入了一个在压缩数据和保留信息之间的权衡参数。 |
| [^244] | [Network Alignment with Transferable Graph Autoencoders.](http://arxiv.org/abs/2310.03272) | 该论文提出了一种基于图自编码器的网络对齐方法，通过生成与图的特征值和特征向量相关的节点嵌入，实现了更准确的对齐。同时，该方法还利用迁移学习和数据增强技术，在大规模网络对齐任务中实现高效的对齐，无需重新训练。 |
| [^245] | [PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels.](http://arxiv.org/abs/2310.01655) | 本文通过使用多项式函数和多项式草图，实现了一个快速注意力机制PolySketchFormer，以突破Transformer架构中注意力机制的二次复杂性难题，无需假设注意力矩阵具有稀疏结构，并提出了高效的基于块的算法。 |
| [^246] | [FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets.](http://arxiv.org/abs/2309.16825) | 该论文提出了一种针对异构临床数据的个性化联邦学习方法，实验证明该方法在性能上超越了现有的全局和个性化联邦学习技术，并且对FLamby基准进行了实质性改进和扩展。 |
| [^247] | [AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning.](http://arxiv.org/abs/2309.10980) | 本研究提出了一种基于多智能体深度强化学习的AI驱动患者监测框架，通过部署多个学习智能体，针对不同的生理特征进行监测，并根据紧急程度预警医疗紧急团队。 |
| [^248] | [Price-Discrimination Game for Distributed Resource Management in Federated Learning.](http://arxiv.org/abs/2308.13838) | 本论文提出了一种价格差异化游戏（PDG），通过对不同客户提供的服务进行定价差异化，改善了联合学习的性能并降低了激励客户参与联合学习的成本。 |
| [^249] | [Simple online learning with consistency oracle.](http://arxiv.org/abs/2308.08055) | 该论文介绍了在只能通过一致性预言机访问类的模型下的在线学习算法，提出了一种更简单且效果更好的算法。该算法最多会犯O(256^d)个错误，并观察到不存在一个最多会犯2^(d+1)-2个错误的算法。 |
| [^250] | [When Analytic Calculus Cracks AdaBoost Code.](http://arxiv.org/abs/2308.01070) | 本文表明AdaBoost只是一种名义上的算法，因为可以使用真值表明确地计算得到弱分类器的组合。 |
| [^251] | [Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems.](http://arxiv.org/abs/2307.16120) | 本文提出了一种使用循环动量加速的深度展开网络，该网络能够有效应用于非线性逆向成像问题。通过利用长短期记忆循环神经网络学习和保留先前梯度的知识，该方法在两个非线性逆向问题上获得了良好的结果。 |
| [^252] | [Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance.](http://arxiv.org/abs/2307.04081) | 本论文提出了一种通过自校准分类器引导的方法改进基于评分的条件生成模型，以提高使用少量标记数据的准确性和性能。通过将分类器作为无条件生成模型的另一种视角，并利用标记和未标记数据来校准分类器，实验证实该方法的有效性。 |
| [^253] | [A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization.](http://arxiv.org/abs/2307.01946) | 本文介绍了一种用于生成合成ECG图像的工具箱，旨在促进扫描ECG数字化。通过引入真实伪影，如手写文本伪影、皱纹、折痕和视角变换，该方法可以在标准纸质ECG背景上生成具有真实性的ECG图像。这种方法有助于解决合成ECG图像中缺乏参考时间序列的问题。 |
| [^254] | [Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective and Improved Bounds.](http://arxiv.org/abs/2306.12498) | 本文提出了带有Shuffled SGD的经验风险最小化的原始-对偶视角和改进界限，旨在解决理论和实践之间的差距。 |
| [^255] | [Deep Fusion: Efficient Network Training via Pre-trained Initializations.](http://arxiv.org/abs/2306.11903) | 本文提出了Deep Fusion，一种基于预训练初始化的高效网络训练方法。通过加速训练过程、降低计算要求，并导致改进的泛化性能，使得该方法在维持传统训练方法的性能甚至超越其性能的同时，减少了训练时间和资源消耗。 |
| [^256] | [Progressive Fourier Neural Representation for Sequential Video Compilation.](http://arxiv.org/abs/2306.11305) | 本研究提出了一种渐进傅里叶神经表示方法，通过在每个训练会话中找到自适应且紧凑的傅里叶空间子模块来编码顺序视频数据，克服了现有神经隐式表示方法在多个复杂数据上的泛化能力差的问题。 |
| [^257] | [Understanding Generalization in the Interpolation Regime using the Rate Function.](http://arxiv.org/abs/2306.10947) | 本文利用大偏差理论，提出一种基于函数的平滑模型特征描述方法，解释了为什么一些插值器有很好的泛化能力以及现代学习技术为什么能够找到它们。 |
| [^258] | [ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators.](http://arxiv.org/abs/2306.08754) | 这是一个用于训练高分辨率物理仿真器的气候数据集。该数据集包含了5.7亿个多变量输入和输出矢量对，用于隔离本地嵌套的高分辨率、高保真度物理的影响。 |
| [^259] | [Solving Large-scale Spatial Problems with Convolutional Neural Networks.](http://arxiv.org/abs/2306.08191) | 本文提出了一种采用迁移学习和小台阶信号训练卷积神经网络的方法，能够有效解决大规模空间问题，尤其在移动基础设施需求方面具有实际应用价值。 |
| [^260] | [High-dimensional and Permutation Invariant Anomaly Detection.](http://arxiv.org/abs/2306.03933) | 该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。 |
| [^261] | [Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures.](http://arxiv.org/abs/2306.03801) | 本研究提出了使用签名条码来稳定向量化多参数持久同调，将多参数持久同调的丰富信息和稳定向量化的优势相结合。 |
| [^262] | [PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward.](http://arxiv.org/abs/2306.01731) | PAGAR是一种用于解决IRL-based IL中奖励不对齐问题的半监督奖励设计方法，在复杂IL任务和零-shot IL任务中表现优越。 |
| [^263] | [A Convex Relaxation Approach to Bayesian Regret Minimization in Offline Bandits.](http://arxiv.org/abs/2306.01237) | 本文提出一种直接最小化贝叶斯遗憾上界的新方法，获得更好的理论离线遗憾界和数值模拟结果，并提供了证据表明流行的LCB-style算法可能不适用。 |
| [^264] | [MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training.](http://arxiv.org/abs/2306.00107) | 提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。 |
| [^265] | [Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape.](http://arxiv.org/abs/2305.19510) | 本文研究了略微超参数化的ReLU网络在有限输入数据集上的损失景观，证明了大多数激活模式对应的参数区域没有坏的可微局部极小值，对于一维输入数据，网络可以通过大多数激活模式实现高维全局极小值集合而不具有坏的局部极小值。 |
| [^266] | [Deep Equivariant Hyperspheres.](http://arxiv.org/abs/2305.15613) | 本文提出了深度等变超球体的理论模型，解决了几何深度学习中等变和几何变换下不变的重大问题。 |
| [^267] | [Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective.](http://arxiv.org/abs/2305.15611) | 本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。 |
| [^268] | [Kernel Affine Hull Machines for Differentially Private Learning.](http://arxiv.org/abs/2304.01300) | 本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。 |
| [^269] | [Practical self-supervised continual learning with continual fine-tuning.](http://arxiv.org/abs/2303.17235) | 本文提出了一种实用的自监督连续学习方法，使用可用的标签来泛化自监督学习，并通过连续微调来减轻灾难性遗忘。 |
| [^270] | [Mixed Autoencoder for Self-supervised Visual Representation Learning.](http://arxiv.org/abs/2303.17152) | 本文提出混合自编码器（MixedAE）用于自监督视觉表示学习，在MAE构架下通过同源识别等辅助预文本任务，解决了数据增强下相互信息增加导致性能下降的问题，并取得了遮蔽图像建模（MIM）增强中最先进的转移结果。 |
| [^271] | [Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks.](http://arxiv.org/abs/2303.15919) | 本文提出了HCNN，是第一个专为计算机视觉任务设计的完全双曲卷积神经网络。在洛伦兹模型的基础上，我们推广了CNN的基本组件，并通过在完全双曲设置中的实验证明了HCNN框架和洛伦兹模型的有效性。 |
| [^272] | [Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices.](http://arxiv.org/abs/2303.10019) | 本文提出一种新的多元概率CRPS学习方法，应用于日前电价预测中，相比于统一组合在CRPS方面取得了显著改进。 |
| [^273] | [Concept Algebra for Score-Based Conditional Models.](http://arxiv.org/abs/2302.03693) | 本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。 |
| [^274] | [Revisiting Signed Propagation for Graph Neural Networks.](http://arxiv.org/abs/2301.08918) | 该论文重新审视了在异质图中的有符号传播方法，提出了一种适用于多类别图的新策略，并克服了其带来的不确定性和不一致性问题，取得了显著的性能提升。 |
| [^275] | [On the Pointwise Behavior of Recursive Partitioning and Its Implications for Heterogeneous Causal Effect Estimation.](http://arxiv.org/abs/2211.10805) | 本文质疑了递归划分在决策树学习中的应用，通过证明它们可能无法实现一致范数的多项式收敛速率。我们提出了随机森林来解决这个问题，将低性能的树转化为几乎最优的过程，但代价是失去了解释性，并引入了两个额外的调整参数。 |
| [^276] | [RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation.](http://arxiv.org/abs/2211.09869) | 本文提出了第一个支持3D理解任务的扩散模型RenderDiffusion，只需使用单眼2D监督进行训练。它利用一种新颖的图像去噪架构，生成和渲染中间的三维表示，在扩散过程中提供强有力的3D一致性。 |
| [^277] | [Continuous Monte Carlo Graph Search.](http://arxiv.org/abs/2210.01426) | 连续蒙特卡洛图搜索（CMCGS）是一种新颖的蒙特卡洛树搜索（MCTS）的扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS通过将相似状态聚类，并共享相同的动作策略，实现了高性能的在线规划。 |

# 详细

[^1]: 从两个十年的血压数据中学习：跨越7500万患者就诊的不同人群模式

    Learning from Two Decades of Blood Pressure Data: Demography-Specific Patterns Across 75 Million Patient Encounters

    [https://rss.arxiv.org/abs/2402.01598](https://rss.arxiv.org/abs/2402.01598)

    这项研究通过分析两个十年的庞大数据集，发现基于性别的血压变化不显著，挑战了传统假设；同时，舒张压随着年龄增长而持续增加，而收缩压在四十多岁的年龄组显示出一个独特的峰值。

    

    高血压仍然是全球关注的健康问题，其患病率不断上升，因此需要有效的监测和理解血压动态。本研究深入探讨了从血压测量中获得的大量信息，这是了解高血压趋势的重要途径。许多研究已经报道了血压变化与各种因素的关系。在这项研究中，我们利用了一份涵盖了两个十年的7500万记录的庞大数据集，为探索和分析不同人群特征，如年龄、种族和性别之间的血压变化提供了独特机会。我们的研究发现，基于性别的血压变化在统计上并不显著，挑战了传统的假设。有趣的是，舒张压（SBP）随着年龄的增长而持续增加，而舒张压（DBP）在四十多岁的年龄组显示出一个独特的峰值。此外，我们的分析还揭示了分布模式中的一些有趣的相似性。

    Hypertension remains a global health concern with a rising prevalence, necessitating effective monitoring and understanding of blood pressure (BP) dynamics. This study delves into the wealth of information derived from BP measurement, a crucial approach in informing our understanding of hypertensive trends. Numerous studies have reported on the relationship between BP variation and various factors. In this research, we leveraged an extensive dataset comprising 75 million records spanning two decades, offering a unique opportunity to explore and analyze BP variations across demographic features such as age, race, and gender. Our findings revealed that gender-based BP variation was not statistically significant, challenging conventional assumptions. Interestingly, systolic blood pressure (SBP) consistently increased with age, while diastolic blood pressure (DBP) displayed a distinctive peak in the forties age group. Moreover, our analysis uncovered intriguing similarities in the distribu
    
[^2]: 大型语言模型的持续学习: 一项综述

    Continual Learning for Large Language Models: A Survey

    [https://rss.arxiv.org/abs/2402.01364](https://rss.arxiv.org/abs/2402.01364)

    这篇综述文章调查了大型语言模型(LLMs)的持续学习，使用了一种新颖的多阶段分类方案对持续学习技术进行了分类，指出了该领域面临的挑战和未来工作方向。

    

    由于其庞大的规模导致训练成本高昂，大型语言模型(LLMs)不易频繁重新训练。然而，更新是必要的，以赋予LLMs新的技能，并使其与快速发展的人类知识保持同步。本文对LLMs的持续学习最新研究进行了综述。鉴于LLMs的独特性，我们以一种新颖的多阶段分类方案对持续学习技术进行了分类，涉及持续预训练、指令调整和对齐等方面。我们将LLMs的持续学习与在规模较小的模型中使用的简单适应方法以及其他增强策略(如检索增强生成和模型编辑)进行了对比。此外，根据对基准和评估的讨论，我们确定了这一重要任务面临的几个挑战和未来工作方向。

    Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.
    
[^3]: 跳过$\textbackslash n$: 一种简单的方法减少大规模视觉-语言模型中的幻觉

    Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models

    [https://rss.arxiv.org/abs/2402.01345](https://rss.arxiv.org/abs/2402.01345)

    本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。

    

    最近大规模视觉-语言模型（LVLMs）的进展展示了其在视觉信息理解与人类语言方面的令人印象深刻的能力。尽管取得了这些进展，LVLMs仍然面临多模态幻觉的挑战，例如生成与视觉信息中不存在的对象相关的文本描述。然而，多模态幻觉的根本原因仍然未被充分探索。在本文中，我们提出了一个新的视角，认为LVLMs中固有的偏见可能是幻觉的关键因素。具体而言，我们系统地确定了与段落分割符（'$\textbackslash n\textbackslash n$'）相关的语义漂移偏差，即在训练数据中，在“$\textbackslash n\textbackslash n$”之前和之后的内容经常表现出显著的语义改变。这种模式使得模型推断在“$\textbackslash n\textbackslash n$”之后的内容应明显不同于前面的内容。

    Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
    
[^4]: 具有前向-后向消息传递的可微分POGLM

    A Differentiable POGLM with Forward-Backward Message Passing

    [https://rss.arxiv.org/abs/2402.01263](https://rss.arxiv.org/abs/2402.01263)

    提出了一种具有前向-后向消息传递的可微分POGLM模型，解决了现有POGLM学习中的路径梯度估计和变分模型设计等问题。

    

    在隐含神经元存在的假设下，部分可观察广义线性模型（POGLM）是理解神经连接的强大工具。现有的工作利用变分推断来学习POGLM，但学习这种潜变量模型存在困难。存在两个主要问题：（1）采样的泊松隐藏尖峰数量阻碍了使用路径梯度估计器进行变分推断；（2）现有的变分模型设计既不具有表达性也不具有时间效率，进一步影响了性能。针对问题（1），我们提出了一种新的可微分POGLM，可以使用路径梯度估计器，优于现有工作中使用的得分函数梯度估计器。针对问题（2），我们提出了前向-后向消息传递采样方案用于变分模型。综合实验表明，我们的可微分POGLM与我们的前向-后向消息传递产生了更好的结果。

    The partially observable generalized linear model (POGLM) is a powerful tool for understanding neural connectivity under the assumption of existing hidden neurons. With spike trains only recorded from visible neurons, existing works use variational inference to learn POGLM meanwhile presenting the difficulty of learning this latent variable model. There are two main issues: (1) the sampled Poisson hidden spike count hinders the use of the pathwise gradient estimator in VI; and (2) the existing design of the variational model is neither expressive nor time-efficient, which further affects the performance. For (1), we propose a new differentiable POGLM, which enables the pathwise gradient estimator, better than the score function gradient estimator used in existing works. For (2), we propose the forward-backward message-passing sampling scheme for the variational model. Comprehensive experiments show that our differentiable POGLMs with our forward-backward message passing produce a bette
    
[^5]: 多元概率时间序列预测与相关误差

    Multivariate Probabilistic Time Series Forecasting with Correlated Errors

    [https://rss.arxiv.org/abs/2402.01000](https://rss.arxiv.org/abs/2402.01000)

    本文提出了一种方法，基于低秩加对角线参数化协方差矩阵，可以有效地刻画时间序列预测中误差的自相关性，并具有复杂度低、校准预测准确性高等优点。

    

    建模误差之间的相关性与模型能够准确量化概率时间序列预测中的预测不确定性密切相关。最近的多元模型在考虑误差之间的同时相关性方面取得了显著进展，然而，对于统计简化的目的，对这些误差的常见假设是它们在时间上是独立的。然而，实际观测往往偏离了这个假设，因为误差通常由于各种因素（如排除时间相关的协变量）而表现出显著的自相关性。在这项工作中，我们提出了一种基于低秩加对角线参数化协方差矩阵的高效方法，可以有效地刻画误差的自相关性。所提出的方法具有几个可取的特性：复杂度不随时间序列数目增加，得到的协方差可以用于校准预测，且具有较好的性能。

    Modeling the correlations among errors is closely associated with how accurately the model can quantify predictive uncertainty in probabilistic time series forecasting. Recent multivariate models have made significant progress in accounting for contemporaneous correlations among errors, while a common assumption on these errors is that they are temporally independent for the sake of statistical simplicity. However, real-world observations often deviate from this assumption, since errors usually exhibit substantial autocorrelation due to various factors such as the exclusion of temporally correlated covariates. In this work, we propose an efficient method, based on a low-rank-plus-diagonal parameterization of the covariance matrix, which can effectively characterize the autocorrelation of errors. The proposed method possesses several desirable properties: the complexity does not scale with the number of time series, the resulting covariance can be used for calibrating predictions, and i
    
[^6]: 打开AI黑箱：通过机械解释性实现程序合成

    Opening the AI black box: program synthesis via mechanistic interpretability

    [https://arxiv.org/abs/2402.05110](https://arxiv.org/abs/2402.05110)

    本文提出了一种新的程序合成方法MIPS，通过机械解释性训练神经网络，将学习到的算法转化为Python代码。MIPS在基准测试中表现出色，解决了32个算法任务，其中13个是其他方法无法解决的。该方法不使用人工训练数据，具有较高的可扩展性和解释性。

    

    我们提出了一种新颖的方法MIPS，基于神经网络自动机械解释性训练所需任务，将学习到的算法转化为Python代码。我们在一个包含62个可通过RNN学习的算法任务的基准测试中对MIPS进行了测试，发现它与GPT-4非常互补：MIPS解决了其中32个问题，其中包括GPT-4无法解决的13个问题（GPT-4解决了30个问题）。MIPS使用整数自动编码器将RNN转化为有限状态机，然后应用布尔或整数符号回归来捕捉学习到的算法。与大型语言模型相比，这种程序合成技术不使用（因此不受限于）如GitHub上的算法和代码等人工训练数据。我们讨论了扩展这种方法以使机器学习模型更可解释和可信的机会和挑战。

    We present MIPS, a novel method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code. We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm. As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub. We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy.
    
[^7]: Hydra: 循序依赖的草稿头部用于Medusa解码

    Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding

    [https://arxiv.org/abs/2402.05109](https://arxiv.org/abs/2402.05109)

    Hydra heads是一种循序依赖的草稿头部，取代了标准草稿头部，显著提高了推测准确性，在Medusa解码中具有更高的吞吐量。

    

    为了解决自回归LLM推理的内存带宽限制问题，先前的研究提出了推测解码框架。为了执行推测解码，一个小的草稿模型提出了输入序列的候选延续，然后由基础模型并行验证。在最近的Medusa解码框架中，一种指定草稿模型的方法是将其作为一组称为草稿头部的轻量级头部，这些头部对基础模型的隐藏状态进行操作。迄今为止，所有现有的草稿头部都是顺序独立的，这意味着它们在候选延续中的令牌推测与候选延续中的任何前面的令牌无关。在这项工作中，我们提出了循序依赖的Hydra heads，它们是标准草稿头部的可替换组件，显著提高了推测准确性。使用Hydra heads进行解码比使用标准草稿头部的Medusa解码具有更高的吞吐量。我们进一步研究了设计空间。

    To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model's hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy. Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads. We further explore the design spac
    
[^8]: 通过插值实现更紧密的泛化界限

    Tighter Generalisation Bounds via Interpolation

    [https://arxiv.org/abs/2402.05101](https://arxiv.org/abs/2402.05101)

    本论文提供了一种新的推导PAC-Bayes泛化界限的方法，并通过插值在不同概率差异之间选择最佳方案，展示了紧密性和实际性能。

    

    本论文提供了一种推导新的基于$(f, \Gamma)$-divergence的PAC-Bayes泛化界限的方法，并展示了在一系列概率差异（包括但不限于KL、Wasserstein和总变差）之间进行插值的PAC-Bayes泛化界限，根据后验分布的属性选择最佳方案。我们探索了这些界限的紧密性，并将其与统计学习中的早期结果联系起来，这些结果是特定案例。我们还将我们的界限实例化为训练目标，产生了非平凡的保证和实际性能。

    This paper contains a recipe for deriving new PAC-Bayes generalisation bounds based on the $(f, \Gamma)$-divergence, and, in addition, presents PAC-Bayes generalisation bounds where we interpolate between a series of probability divergences (including but not limited to KL, Wasserstein, and total variation), making the best out of many worlds depending on the posterior distributions properties. We explore the tightness of these bounds and connect them to earlier results from statistical learning, which are specific cases. We also instantiate our bounds as training objectives, yielding non-trivial guarantees and practical performances.
    
[^9]: Hydragen：共享前缀的高吞吐量LLM推理

    Hydragen: High-Throughput LLM Inference with Shared Prefixes

    [https://arxiv.org/abs/2402.05099](https://arxiv.org/abs/2402.05099)

    Hydragen是一种具有共享前缀的高吞吐量LLM推理方法，通过将注意力计算分解为共享前缀和唯一后缀，来提高推理效率，并能够提高端到端LLM吞吐量多达32倍。

    

    基于Transformer的大规模语言模型（LLM）现在已经部署到数亿用户上。LLM推理通常在共享前缀的序列批次上执行，例如少量样本示例或聊天机器人系统提示。在这种大批量设置下，解码可能会受到注意操作的瓶颈，该操作从内存中读取大型键值（KV）缓存，并为批次中的每个序列计算低效的矩阵-向量乘积。在这项工作中，我们介绍了Hydragen，一种具有共享前缀的硬件感知精确实现的注意力。Hydragen将注意力分别计算在共享前缀和唯一后缀上。这种分解通过在序列之间批量查询一起减少冗余内存读取，从而实现了高效的前缀注意力，并使得可以使用硬件友好的矩阵乘法。我们的方法可以将端到端的LLM吞吐量提高多达32倍，超过竞争基线，并且随着批次大小和共享前缀的长度增加，速度提高的幅度也增加。

    Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users. LLM inference is commonly performed on batches of sequences that share a prefix, such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications. Our method can improve end-to-end LLM throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix lengt
    
[^10]: 关于分散推断模型的扩散模型：基准测试和改进随机控制和采样

    On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling

    [https://arxiv.org/abs/2402.05098](https://arxiv.org/abs/2402.05098)

    本研究探讨了训练扩散模型以从给定分布中采样的问题，并针对随机控制和采样提出了一种新的探索策略，通过基准测试比较了不同推断方法的相对优劣，并对过去的工作提出了质疑。

    

    我们研究了训练扩散模型以从给定的非标准化密度或能量函数分布中采样的问题。我们对几种扩散结构推断方法进行了基准测试，包括基于模拟的变分方法和离策略方法（连续生成流网络）。我们的结果揭示了现有算法的相对优势，同时对过去的研究提出了一些质疑。我们还提出了一种新颖的离策略方法探索策略，基于目标空间中的局部搜索和回放缓冲区的使用，并证明它可以改善各种目标分布上的样本质量。我们研究的采样方法和基准测试的代码已公开在https://github.com/GFNOrg/gfn-diffusion，作为未来在分散推断模型上工作的基础。

    We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion models for amortized inference.
    
[^11]: NITO:神经隐式场用于无分辨率拓扑优化

    NITO: Neural Implicit Fields for Resolution-free Topology Optimization

    [https://arxiv.org/abs/2402.05073](https://arxiv.org/abs/2402.05073)

    NITO是一种使用神经隐式场的拓扑优化方法，通过引入Boundary Point Order-Invariant MLP（BPOM）方法提供了无分辨率和域不可知的解决方案，可以以更高的效率和更短的时间生成结构。

    

    拓扑优化是工程设计中关键的任务，旨在在给定的空间中以最佳方式分布材料以获得最佳性能。我们引入了神经隐式拓扑优化(NITO)，这是一种利用深度学习加速拓扑优化问题的新方法。NITO是第一个在基于深度学习的拓扑优化中提供无分辨率和域不可知解决方案的框架之一。与SOTA的扩散模型相比，NITO合成的结构具有高达7倍的结构效率，并且所需时间只有十分之一。在NITO框架中，我们引入了一种新的方法，即边界点顺序不变的MLP（BPOM），以稀疏和域不可知的方式表示边界条件，远离昂贵的基于模拟的方法。重要的是，NITO规避了限制卷积神经网络(CNN)模型处于结构化和固定大小域的领域和分辨率限制。

    Topology optimization is a critical task in engineering design, where the goal is to optimally distribute material in a given space for maximum performance. We introduce Neural Implicit Topology Optimization (NITO), a novel approach to accelerate topology optimization problems using deep learning. NITO stands out as one of the first frameworks to offer a resolution-free and domain-agnostic solution in deep learning-based topology optimization. NITO synthesizes structures with up to seven times better structural efficiency compared to SOTA diffusion models and does so in a tenth of the time. In the NITO framework, we introduce a novel method, the Boundary Point Order-Invariant MLP (BPOM), to represent boundary conditions in a sparse and domain-agnostic manner, moving away from expensive simulation-based approaches. Crucially, NITO circumvents the domain and resolution limitations that restrict Convolutional Neural Network (CNN) models to a structured domain of fixed size -- limitations 
    
[^12]: 扩展具有连带偏序特性的非凸极小极大问题的一阶算法的适用范围

    Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity

    [https://arxiv.org/abs/2402.05071](https://arxiv.org/abs/2402.05071)

    本文研究了具有连带偏序特性的非凸极小极大问题，提出了一阶算法的适用范围，并在理论上证明了算法的复杂度保证。

    

    本文关注满足rho-连带偏序特性或在rho-弱Minty变分不等式（MVI）中存在解的约束，L-光滑的非凸非凹极小极大问题，其中参数rho>0的较大值对应更高的非凸性程度。这些问题类包括两个玩家强化学习，交互主导的极小极大问题以及某些经典极小极大算法无法解决的合成测试问题。已有猜想认为一阶方法可容忍最大rho为1/L，但现有文献中的结果已停滞在更严格的要求rho<1/2L。通过简单的论证，我们获得了具有连带偏序特性或弱MVI条件下，rho<1/L的最优或最佳已知复杂度保证。我们分析的算法是Halpern和Krasnosel'skiĭ-Mann (KM)迭代的非精确变种。我们还提供了算法和复杂度g...

    We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems either satisfying $\rho$-cohypomonotonicity or admitting a solution to the $\rho$-weakly Minty Variational Inequality (MVI), where larger values of the parameter $\rho>0$ correspond to a greater degree of nonconvexity. These problem classes include examples in two player reinforcement learning, interaction dominant min-max problems, and certain synthetic test problems on which classical min-max algorithms fail. It has been conjectured that first-order methods can tolerate value of $\rho$ no larger than $\frac{1}{L}$, but existing results in the literature have stagnated at the tighter requirement $\rho < \frac{1}{2L}$. With a simple argument, we obtain optimal or best-known complexity guarantees with cohypomonotonicity or weak MVI conditions for $\rho < \frac{1}{L}$. The algorithms we analyze are inexact variants of Halpern and Krasnosel'ski\u{\i}-Mann (KM) iterations. We also provide algorithms and complexity g
    
[^13]: 物理信息神经网络的多尺度建模：从复杂系统的大尺度动力学到小尺度预测

    Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems

    [https://arxiv.org/abs/2402.05067](https://arxiv.org/abs/2402.05067)

    本文提出了利用物理信息神经网络进行多尺度建模的方法，通过解耦大尺度和小尺度动力学，并在正交基函数空间中近似小尺度系统。实验结果表明该方法在处理液体动力学问题以及更复杂的情况下具有较高的有效性和适用性。

    

    多尺度现象在各个科学领域中普遍存在，对于准确有效地预测复杂系统中的多尺度动力学提出了普遍的挑战。本文提出了一种通过解耦方法对多尺度动力学进行表征的新的求解模式。通过独立地建模大尺度动力学，并将小尺度动力学视为从属系统，我们开发了一种谱PINN方法，在正交基函数空间中接近小尺度系统。通过大量的数值实验，包括一维Kuramot-Sivashinsky (KS)方程、二维和三维Navier-Stokes (NS)方程，我们展示了该方法的有效性，展示了它在液体动力学问题中的多样性。此外，我们还深入研究了该方法在更复杂问题中的应用，包括非均匀网格、复杂几何形状、带噪声的大尺度数据和高维小尺度动力学。

    Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems. In this paper, a novel solving mode is proposed for characterizing multiscale dynamics through a decoupling method. By modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system, a Spectral PINN is developed to approach the small-scale system in an orthogonal basis functional space. The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional Navier-Stokes (NS) equations, showcasing its versatility in addressing problems of fluid dynamics. Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics. 
    
[^14]: 从多个分布中进行因果表示学习：一个通用设置

    Causal Representation Learning from Multiple Distributions: A General Setting

    [https://arxiv.org/abs/2402.05052](https://arxiv.org/abs/2402.05052)

    本文研究了一个通用的、完全非参数的因果表示学习设置，旨在在多个分布之间学习因果关系，无需假设硬干预。通过稀疏性约束，可以从多个分布中恢复出因果关系。

    

    在许多问题中，测量变量（例如图像像素）只是隐藏的因果变量（例如潜在的概念或对象）的数学函数。为了在不断变化的环境中进行预测或对系统进行适当的更改，恢复隐藏的因果变量$Z_i$以及由图$\mathcal{G}_Z$表示的它们的因果关系是有帮助的。这个问题最近被称为因果表示学习。本文关注来自多个分布（来自异构数据或非平稳时间序列）的因果表示学习的通用、完全非参数的设置，不需要假设分布改变背后存在硬干预。我们旨在在这个基本情况下开发通用解决方案；作为副产品，这有助于看到其他假设（如参数因果模型或硬干预）提供的独特好处。我们证明在恢复过程中对图的稀疏性约束下，可以从多个分布中学习出因果关系。

    In many problems, the measured variables (e.g., image pixels) are just mathematical functions of the hidden causal variables (e.g., the underlying concepts or objects). For the purpose of making predictions in changing environments or making proper changes to the system, it is helpful to recover the hidden causal variables $Z_i$ and their causal relations represented by graph $\mathcal{G}_Z$. This problem has recently been known as causal representation learning. This paper is concerned with a general, completely nonparametric setting of causal representation learning from multiple distributions (arising from heterogeneous data or nonstationary time series), without assuming hard interventions behind distribution changes. We aim to develop general solutions in this fundamental case; as a by product, this helps see the unique benefit offered by other assumptions such as parametric causal models or hard interventions. We show that under the sparsity constraint on the recovered graph over
    
[^15]: Federated Learning能够找到有益的好友

    Federated Learning Can Find Friends That Are Beneficial

    [https://arxiv.org/abs/2402.05050](https://arxiv.org/abs/2402.05050)

    本研究介绍了一种新颖的算法，在Federated Learning中使用自适应聚合权重来识别对特定学习目标最有益的客户，证明了该方法的收敛性，并经过实证评估发现，使用该算法引导的合作优于传统方法，这为更加简化和有效的Federated Learning实现奠定了基础。

    

    在Federated Learning (FL)中，分布式性质和客户数据的异质性既带来了机会，也带来了挑战。虽然客户之间的合作可以显著增强学习过程，但并不是所有的合作都是有益的；有些甚至可能是有害的。在这项研究中，我们引入了一种新颖的算法，为参与FL训练的客户分配自适应的聚合权重，识别出数据分布对特定学习目标最有益的客户。我们证明了我们的聚合方法的收敛性与仅聚合具有相同数据分布的客户接收的更新的方法不相上下。此外，经验证明，由我们的算法引导的合作优于传统的FL方法。这强调了审慎选择客户的关键作用，并为未来更简化和有效的FL实现奠定了基础。

    In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges. While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental. In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective. We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution. Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches. This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years.
    
[^16]: SALAD-Bench: 一个针对大语言模型的层次化和全面性安全基准

    SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.05044](https://arxiv.org/abs/2402.05044)

    SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。

    

    在快速发展的大语言模型（LLM）领域中，确保强大的安全措施至关重要。为了满足这一关键需求，我们提出了一种特别设计用于评估LLMs、攻击和防御方法的安全基准，称为SALAD-Bench。SALAD-Bench通过其大规模、丰富多样的特性，以及跨三个层次的细致分类和多功能性，超越了传统基准。SALAD-Bench通过对标准查询和复杂查询（包括攻击、防御修改和多项选择）的精心设计，有效管理其固有的复杂性。为了确保无缝可靠的评估，我们引入了一种创新的评估器：基于LLM的MD-Judge，专注于攻击增强查询的问答对评估。以上组件将SALAD-Bench从标准的LLM安全评估扩展到了LLM攻击和防御方法评估，确保了联合目标的实用性。

    In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
    
[^17]: PAC学习理论在保持解释的图扰动下的适用性

    PAC Learnability under Explanation-Preserving Graph Perturbations

    [https://arxiv.org/abs/2402.05039](https://arxiv.org/abs/2402.05039)

    本研究考虑了在设计和训练GNN时利用解释辅助学习和解释辅助数据增强的方法，并发现解释辅助学习的样本复杂度可以任意小于解释不可知学习。

    

    图模型在社交网络、生物学、自然语言处理等广泛应用中捕捉实体之间的关系。图神经网络（GNN）是一种能够处理图结构数据中的复杂关系和依赖关系的神经模型。图解释是一个子图，它是相对于其分类标签而言输入图的一个“几乎足够”的统计量。因此，分类标签对于不属于解释子图的图边的扰动是不变的，且具有较高的概率。本文考虑了两种利用这种扰动不变性设计和训练GNN的方法。首先，考虑了解释辅助学习规则。结果表明，解释辅助学习的样本复杂度可以任意小于解释不可知学习。接下来，考虑了解释辅助数据增强，其中

    Graphical models capture relations between entities in a wide range of applications including social networks, biology, and natural language processing, among others. Graph neural networks (GNN) are neural models that operate over graphs, enabling the model to leverage the complex relationships and dependencies in graph-structured data. A graph explanation is a subgraph which is an `almost sufficient' statistic of the input graph with respect to its classification label. Consequently, the classification label is invariant, with high probability, to perturbations of graph edges not belonging to its explanation subgraph. This work considers two methods for leveraging such perturbation invariances in the design and training of GNNs. First, explanation-assisted learning rules are considered. It is shown that the sample complexity of explanation-assisted learning can be arbitrarily smaller than explanation-agnostic learning. Next, explanation-assisted data augmentation is considered, where 
    
[^18]: 模拟过度参数化

    Simulated Overparameterization

    [https://arxiv.org/abs/2402.05033](https://arxiv.org/abs/2402.05033)

    模拟过度参数化（SOP）是一种将紧凑模型的计算效率与过度参数化模型的高级学习能力相结合的新范式。通过使用模拟训练过度参数化模型的方法，我们提出了一种与主要架构无关的算法，称为"majority kernels"，从而在各种架构和任务中实现性能提升。

    

    在这项工作中，我们引入了一种新的范式，称为模拟过度参数化（SOP）。SOP将紧凑模型的计算效率与过度参数化模型的高级学习能力相结合。SOP提出了一种独特的模型训练和推断方法，在推断过程中，使用显著更多参数的模型进行训练，但只使用其中较小、高效的子集进行实际计算。在此框架的基础上，我们提出了一种新颖的、与主要架构（包括Transformer模型）无关的算法，称为"majority kernels"。 majority kernels使得模拟训练过度参数化模型成为可能，从而在各种架构和任务中取得性能提升。此外，我们的方法在训练时对于计算成本（墙上挂钟时间）的增加非常小。所提出的方法在各种数据集和任务上表现出很强的性能。

    In this work, we introduce a novel paradigm called Simulated Overparametrization (SOP). SOP merges the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models. SOP proposes a unique approach to model training and inference, where a model with a significantly larger number of parameters is trained in such a way that a smaller, efficient subset of these parameters is used for the actual computation during inference. Building upon this framework, we present a novel, architecture agnostic algorithm called "majority kernels", which seamlessly integrates with predominant architectures, including Transformer models. Majority kernels enables the simulated training of overparameterized models, resulting in performance gains across architectures and tasks. Furthermore, our approach adds minimal overhead to the cost incurred (wall clock time) at training time. The proposed approach shows strong performance on a wide variety of datasets and m
    
[^19]: 强凸性引导的更平坦损失超参数优化

    Strong convexity-guided hyper-parameter optimization for flatter losses

    [https://arxiv.org/abs/2402.05025](https://arxiv.org/abs/2402.05025)

    本文提出了一种强凸性引导的超参数优化方法，通过最小化损失函数的强凸性来改善其平坦性。通过利用底层神经网络的结构，我们提出了一种闭式方程来近似计算强凸性参数，并以随机化的方式寻找最小化该参数的超参数配置。实验证明，该方法在更短的运行时间下取得了较强的性能。

    

    我们提出了一种新颖的白盒方法来进行超参数优化。受到最近的关于平坦最小值和泛化之间关系的工作的启发，我们首先建立了损失函数的强凸性和其平坦性之间的关系。基于此，我们试图通过最小化损失函数的强凸性来寻找改善平坦性的超参数配置。通过利用底层神经网络的结构，我们推导出用于近似计算强凸性参数的闭式方程，并尝试以随机化的方式寻找最小化它的超参数。通过在14个分类数据集上进行实验，我们展示了我们的方法在更短的运行时间下取得了较强的性能。

    We propose a novel white-box approach to hyper-parameter optimization. Motivated by recent work establishing a relationship between flat minima and generalization, we first establish a relationship between the strong convexity of the loss and its flatness. Based on this, we seek to find hyper-parameter configurations that improve flatness by minimizing the strong convexity of the loss. By using the structure of the underlying neural network, we derive closed-form equations to approximate the strong convexity parameter, and attempt to find hyper-parameters that minimize it in a randomized fashion. Through experiments on 14 classification datasets, we show that our method achieves strong performance at a fraction of the runtime.
    
[^20]: 对于材料发现来说，对LLM的拜占庭优化是否真的有利？一个冷静的观察

    A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?

    [https://arxiv.org/abs/2402.05015](https://arxiv.org/abs/2402.05015)

    本文研究了LLMs是否真的有助于加速在分子空间中的正规贝叶斯优化。通过将LLMs视为标准但正规的BO替代模型的固定特征提取器，并利用参数效能来实现。

    

    自动化是当代材料发现的重要基石。贝叶斯优化是这种工作流程的重要组成部分，它使科学家能够将先前的领域知识应用到对大规模分子空间的高效探索中。尽管这样的先前知识可以采用多种形式，但关于大型语言模型（LLMs）中所包含的辅助科学知识已经引起了很大的轰动。然而，迄今为止的研究仅探索了基于启发式材料搜索的LLMs。实际上，最近的研究通过从点估计的非贝叶斯LLMs中获得不确定性估计，这是BO的一个重要组成部分。在这项工作中，我们研究了LLMs是否真的有助于加速在分子空间中的正规贝叶斯优化。我们对这个问题采取了冷静、客观的立场。这是通过仔细地（i）将LLMs视为标准但正规的BO替代模型的固定特征提取器，以及（ii）利用参数效能来实现的。

    Automation is one of the cornerstones of contemporary material discovery. Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space. While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs). However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs. In this work, we study the question of whether LLMs are actually useful to accelerate principled Bayesian optimization in the molecular space. We take a sober, dispassionate stance in answering this question. This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-effic
    
[^21]: 用自编码器对结构化数据进行压缩：非线性和深度的可证明优势

    Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth

    [https://arxiv.org/abs/2402.05013](https://arxiv.org/abs/2402.05013)

    本文证明了在自编码器中，即使采用浅层结构，梯度下降算法会完全忽略稀疏数据的结构，并且对于一般数据分布，我们发现了梯度下降最小化器在数据稀疏性临界点处的相变现象。

    

    自编码器是机器学习和有损数据压缩的多个实证分支中的一个重要模型。然而，即使在浅层两层设置下，基本的理论问题仍然没有答案。特别是，在浅层自编码器中能够多大程度上捕捉到底层数据分布的结构？对于典型的稀疏高斯数据的1位压缩问题，我们证明了梯度下降算法收敛于一个完全忽略输入的稀疏结构的解决方案。换句话说，算法的性能与压缩高斯源（没有稀疏性）的性能相同。对于一般的数据分布，我们给出了关于梯度下降最小化器的形状的相变现象的证据，作为数据稀疏性的函数：在临界稀疏水平以下，最小化器是一个随机均匀选择的旋转（就像非稀疏数据的压缩一样）；在临界稀疏水平以上，最小化器是身份变换（除了一个+1的偏移）。

    Autoencoders are a prominent model in many empirical branches of machine learning and lossy data compression. However, basic theoretical questions remain unanswered even in a shallow two-layer setting. In particular, to what degree does a shallow autoencoder capture the structure of the underlying data distribution? For the prototypical case of the 1-bit compression of sparse Gaussian data, we prove that gradient descent converges to a solution that completely disregards the sparse structure of the input. Namely, the performance of the algorithm is the same as if it was compressing a Gaussian source - with no sparsity. For general data distributions, we give evidence of a phase transition phenomenon in the shape of the gradient descent minimizer, as a function of the data sparsity: below the critical sparsity level, the minimizer is a rotation taken uniformly at random (just like in the compression of non-sparse data); above the critical sparsity, the minimizer is the identity (up to a
    
[^22]: 导航复杂性：通过扩展窗口匹配实现无损图谱精简

    Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching

    [https://arxiv.org/abs/2402.05011](https://arxiv.org/abs/2402.05011)

    本文通过连接先前被忽视的监督信号的方式，首次尝试实现无损图谱精简，以解决现有方法无法准确复制原始图谱的问题。

    

    图谱精简旨在通过合成紧凑的图谱来减少大规模图谱数据集的大小，同时不损失在其上训练的图神经网络（GNNs）的性能，这为减少训练GNNs的计算成本提供了启示。然而，现有方法往往无法准确复制某些数据集的原始图谱，从而未能实现无损精简的目标。为了理解这一现象，我们调查了潜在的原因，并揭示了先前最先进的轨迹匹配方法在优化精简图谱时提供了来自原始图谱的偏倚和受限的监督信号。这严重限制了精简图谱的规模和功效。在本文中，我们首次尝试通过连接先前被忽视的监督信号来实现无损图谱精简。

    Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward \textit{lossless graph condensation} by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert traje
    
[^23]: 高效ViT-SAM: 在不损失性能的情况下加速的段落任意模型

    EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss

    [https://arxiv.org/abs/2402.05008](https://arxiv.org/abs/2402.05008)

    高效ViT-SAM是一种新的加速的段落任意模型，通过保留轻量级提示编码器和掩码解码器，并替换沉重的图像编码器，实现了48.9倍的加速而不牺牲性能。

    

    我们提出了一种新的加速段落任意模型——高效ViT-SAM。我们保留了SAM的轻量级提示编码器和掩码解码器，同时用高效ViT替换了沉重的图像编码器。在训练方面，我们首先从SAM-ViT-H图像编码器到高效ViT进行知识蒸馏。随后，我们对SA-1B数据集进行端到端训练。由于高效ViT的效率和容量，高效ViT-SAM在A100 GPU上相比SAM-ViT-H实现了48.9倍的TensorRT加速，而无需牺牲性能。我们的代码和预训练模型已在https://github.com/mit-han-lab/efficientvit发布。

    We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.
    
[^24]: 基于示例的机器学习去解释随机森林

    Example-based Explanations for Random Forests using Machine Unlearning

    [https://arxiv.org/abs/2402.05007](https://arxiv.org/abs/2402.05007)

    FairDebugger是一个利用机器学习去解释随机森林的系统，在公平性背景下找出导致模型不公平的训练数据子集。

    

    基于树的机器学习模型，例如决策树和随机森林，在分类任务中取得了巨大的成功，主要是因为它们在监督学习任务中的预测能力和易于解释性。尽管它们受到广泛的欢迎和认可，但这些模型也被发现会产生意外或具有歧视性的结果。鉴于它们对于大多数任务的巨大成功，有必要找出它们意外和具有歧视性行为的原因。然而，在公平性背景下，理解和调试基于树的分类器的研究工作还不多。我们引入了FairDebugger，这是一个利用机器学习去辨识导致随机森林分类器结果中公平性违规的训练数据子集的新型研究成果的系统。FairDebugger生成前-k个解释（以一致的训练数据子集的形式）来解释模型的不公平性。为了实现这个目标，FairDebugger首先利用机器学习去辨识出导致模型不公平的训练数据子集。

    Tree-based machine learning models, such as decision trees and random forests, have been hugely successful in classification tasks primarily because of their predictive power in supervised learning tasks and ease of interpretation. Despite their popularity and power, these models have been found to produce unexpected or discriminatory outcomes. Given their overwhelming success for most tasks, it is of interest to identify sources of their unexpected and discriminatory behavior. However, there has not been much work on understanding and debugging tree-based classifiers in the context of fairness.   We introduce FairDebugger, a system that utilizes recent advances in machine unlearning research to identify training data subsets responsible for instances of fairness violations in the outcomes of a random forest classifier. FairDebugger generates top-$k$ explanations (in the form of coherent training data subsets) for model unfairness. Toward this goal, FairDebugger first utilizes machine 
    
[^25]: 随机偏置界对随机部分监控的应用

    Randomized Confidence Bounds for Stochastic Partial Monitoring

    [https://arxiv.org/abs/2402.05002](https://arxiv.org/abs/2402.05002)

    本文研究了随机部分监控中基于随机化的偏置界策略，该策略扩展了现有随机策略无法应用的情境和非情境设置，实验证明该策略优于最先进的方法。

    

    部分监控 (PM) 框架提供了通过不完整的反馈进行顺序学习问题的理论表述。在每个回合中，学习代理选择一个动作，而环境同时选择一个结果。然后代理观察到一个仅部分提供信息关于（未观察到的）结果的反馈信号。代理利用接收到的反馈信号选择能够最小化（未观察到的）累计损失的动作。在情境 PM 中，结果依赖于代理在每轮选择动作之前可观察到的某些附加信息。在本文中，我们考虑了具有随机结果的情境和非情境的 PM 设置。我们引入了一种基于确定性置信界的随机化策略的新类方法，将遗憾保证扩展到现有的随机策略不适用的设置中。我们的实验表明，所提出的 RandCBP 和 RandCBPside* 策略改进了最先进的方法。

    The partial monitoring (PM) framework provides a theoretical formulation of sequential learning problems with incomplete feedback. On each round, a learning agent plays an action while the environment simultaneously chooses an outcome. The agent then observes a feedback signal that is only partially informative about the (unobserved) outcome. The agent leverages the received feedback signals to select actions that minimize the (unobserved) cumulative loss. In contextual PM, the outcomes depend on some side information that is observable by the agent before selecting the action on each round. In this paper, we consider the contextual and non-contextual PM settings with stochastic outcomes. We introduce a new class of strategies based on the randomization of deterministic confidence bounds, that extend regret guarantees to settings where existing stochastic strategies are not applicable. Our experiments show that the proposed RandCBP and RandCBPside* strategies improve state-of-the-art b
    
[^26]: 在离散状态空间上的生成型流：实现多模态流并应用于蛋白质共设计

    Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design

    [https://arxiv.org/abs/2402.04997](https://arxiv.org/abs/2402.04997)

    本研究提出了离散流模型（DFMs），通过连续时间马尔可夫链实现离散空间流匹配的离散等效，为将基于流的生成模型应用于多模态连续和离散数据问题提供了解决方案。此方法在蛋白质共设计任务上取得了最先进的性能。

    

    将离散和连续数据相结合对于生成模型来说是一项重要的能力。我们提出了离散流模型（DFMs），这是一种新的基于流的离散数据模型，可以实现将基于流的生成模型应用于多模态连续和离散数据问题。我们的关键见解是，离散空间流匹配的离散等效可以通过使用连续时间马尔可夫链来实现。DFMs通过简单的推导包括离散扩散模型作为特定实例，同时允许在现有基于扩散的方法上改进性能。我们利用DFMs方法构建了一个多模态基于流的建模框架。我们将此能力应用于蛋白质共设计的任务，其中我们学习了一个能够同时生成蛋白质结构和序列的模型。我们的方法在共设计性能上达到了最先进水平，同时允许使用同一个多模态模型进行序列或结构的灵活生成。

    Combining discrete and continuous data is an important capability for generative models. We present Discrete Flow Models (DFMs), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to multimodal continuous and discrete data problems. Our key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains. DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. We utilize our DFMs method to build a multimodal flow-based modeling framework. We apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence. Our approach achieves state-of-the-art co-design performance while allowing the same multimodal model to be used for flexible generation of the sequence or str
    
[^27]: PriorBoost：一种用于学习聚合回应的自适应算法

    PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses

    [https://arxiv.org/abs/2402.04987](https://arxiv.org/abs/2402.04987)

    本研究提出了一种名为PriorBoost的自适应算法，用于学习从聚合回应中学习的算法。该算法通过自适应地形成一些越来越同质的样本bags，以提高模型质量，并在事件级别预测方面实现了最优模型质量。

    

    本文研究了从聚合回应中学习的算法。我们重点研究了用于事件级损失函数的聚合集合构建（在文献中称为bags）的方法。我们证明了对于线性回归和广义线性模型（GLMs），最优的bagging问题可以简化为一维大小受限的k-means聚类问题。此外，我们从理论上量化了使用策划的bags比随机bags更有优势。然后，我们提出了PriorBoost算法，它自适应地形成样本bags，使其在（未观察到的）个体回应方面越来越同质，以改善模型质量。我们还研究了聚合学习的标签差分隐私，并提供了大量实验证明，与非自适应算法形成鲜明对比，PriorBoost在事件级别预测中经常实现最优模型质量。

    This work studies algorithms for learning from aggregate responses. We focus on the construction of aggregation sets (called bags in the literature) for event-level loss functions. We prove for linear regression and generalized linear models (GLMs) that the optimal bagging problem reduces to one-dimensional size-constrained $k$-means clustering. Further, we theoretically quantify the advantage of using curated bags over random bags. We then propose the PriorBoost algorithm, which adaptively forms bags of samples that are increasingly homogeneous with respect to (unobserved) individual responses to improve model quality. We study label differential privacy for aggregate learning, and we also provide extensive experiments showing that PriorBoost regularly achieves optimal model quality for event-level predictions, in stark contrast to non-adaptive algorithms.
    
[^28]: 超越解释：基于SHAP聚类的XAI自适应学习用于能耗预测

    Beyond explaining: XAI-based Adaptive Learning with SHAP Clustering for Energy Consumption Prediction

    [https://arxiv.org/abs/2402.04982](https://arxiv.org/abs/2402.04982)

    本文介绍了一种将可解释的人工智能（XAI）技术与自适应学习相结合的方法，通过使用SHAP聚类来提供模型预测的可解释性解释，并利用这些洞察力来自适应地改进模型，平衡复杂性与预测性能。这种方法可以解决数据分布转移问题并确保模型的鲁棒性。评估结果表明该方法在能耗预测及其他领域具有良好的迁移性能。

    

    本论文提出了一种将可解释的人工智能（XAI）技术与自适应学习相结合的方法，以增强能耗预测模型，在处理数据分布转移方面具有重要意义。利用SHAP聚类，我们的方法提供了可解释的模型预测解释，并利用这些洞察力来自适应地改进模型，平衡模型复杂性和预测性能。我们介绍了一个三阶段的过程：（1）获取SHAP值来解释模型预测，（2）对SHAP值进行聚类以识别不同的模式和异常值，（3）根据得到的SHAP聚类特征来改进模型。我们的方法可以减轻过拟合，确保处理数据分布转移的鲁棒性。我们使用一个包含建筑物能耗记录的全面数据集进行评估，以及另外两个数据集来评估我们的方法在其他领域、回归和分类中的可迁移性。

    This paper presents an approach integrating explainable artificial intelligence (XAI) techniques with adaptive learning to enhance energy consumption prediction models, with a focus on handling data distribution shifts. Leveraging SHAP clustering, our method provides interpretable explanations for model predictions and uses these insights to adaptively refine the model, balancing model complexity with predictive performance. We introduce a three-stage process: (1) obtaining SHAP values to explain model predictions, (2) clustering SHAP values to identify distinct patterns and outliers, and (3) refining the model based on the derived SHAP clustering characteristics. Our approach mitigates overfitting and ensures robustness in handling data distribution shifts. We evaluate our method on a comprehensive dataset comprising energy consumption records of buildings, as well as two additional datasets to assess the transferability of our approach to other domains, regression, and classification
    
[^29]: 一次梯度下降步骤后两层神经网络在特征学习中的渐近性质

    Asymptotics of feature learning in two-layer networks after one gradient-step

    [https://arxiv.org/abs/2402.04980](https://arxiv.org/abs/2402.04980)

    通过研究两层神经网络在一次梯度下降步骤后的特征学习，我们提供了在高维极限下通用化误差的精确渐近描述，并发现在适应数据的情况下，网络能够高效地学习梯度方向上的非线性函数。

    

    本文研究了两层神经网络在从数据中学习特征，并在使用单一梯度下降步骤训练后如何改进核心方法的问题。借助于（Ba et al., 2022）与非线性尖峰矩阵模型的关联以及对高斯泛化性的最新进展（Dandi et al., 2023），我们在样本数$n$、宽度$p$和输入维度$d$成比例增长的高维极限下，给出了一种精确的一致性误差描述。我们准确地刻画了适应数据对于网络在梯度方向上高效学习非线性函数的重要性——在初始化阶段，网络只能表达线性函数。据我们所知，我们的结果提供了在大学习率$\eta=\Theta_{d}(d)$的情况下特征学习对于两层神经网络泛化的首个准确描述，超越了核心方法。

    In this manuscript we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging a connection from (Ba et al., 2022) with a non-linear spiked matrix model and recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error in the high-dimensional limit where the number of samples $n$, the width $p$ and the input dimension $d$ grow at a proportional rate. We characterize exactly how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime. To our knowledge, our results provides the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime $\eta=\Theta_{d}(d)$, beyond
    
[^30]: 基于贝叶斯方法的在线学习在具有上下文环境的不安宁赌博机中的应用于公共卫生

    A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health

    [https://arxiv.org/abs/2402.04933](https://arxiv.org/abs/2402.04933)

    基于贝叶斯方法的在线学习在公共卫生干预计划中的资源分配中具有重要的应用。我们提出了一种新颖的贝叶斯学习方法，结合了贝叶斯建模和汤普森抽样技术，能够灵活地处理上下文环境和非稳态的多臂赌博机问题，并且在预算有限的情况下能够快速学习未知的转移动态。实验证明，该方法实现了显著更高的收益率。

    

    不安宁多臂赌博机（RMABs）用于建模公共卫生干预计划中的顺序资源分配。在这些情景中，潜在的转移动态通常是未知的，需要在线强化学习（RL）。然而，现有的RMAB在线RL方法无法整合到现实世界的公共卫生应用中常见的属性，如上下文信息和非稳态性。我们提出了基于贝叶斯模型和汤普森抽样的上下文RMAB的贝叶斯学习（BCoR），这是一种在线RL方法，可以灵活地模拟各种复杂的RMAB设置，如上下文和非稳态的RMAB。我们的方法的一个重要贡献是在预算有限的情况下能够充分利用内部和各个臂之间的共享信息，在相对短的时间范围内快速学习未知的RMAB转移动态。实证结果表明，BCoR在有限的时间内实现了显著更高的收益率。

    Restless multi-armed bandits (RMABs) are used to model sequential resource allocation in public health intervention programs. In these settings, the underlying transition dynamics are often unknown a priori, requiring online reinforcement learning (RL). However, existing methods in online RL for RMABs cannot incorporate properties often present in real-world public health applications, such as contextual information and non-stationarity. We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model a wide range of complex RMAB settings, such as contextual and non-stationary RMABs. A key contribution of our approach is its ability to leverage shared information within and between arms to learn unknown RMAB transition dynamics quickly in budget-constrained settings with relatively short time horizons. Empirically, we show that BCoR achieves substantially higher finit
    
[^31]: 扩散模型中的蓝噪声

    Blue noise for diffusion models

    [https://arxiv.org/abs/2402.04930](https://arxiv.org/abs/2402.04930)

    本文提出了一种新颖且通用的扩散模型，利用蓝噪声改善了训练过程中的生成质量，并通过引入相关噪声模型和相关噪声掩码来考虑图像内部和跨图像的相关性，以提高梯度流动和重构频谱内容。

    

    现有的大多数扩散模型在训练和采样过程中使用高斯噪声，但这可能无法最优地考虑去噪网络重构的频谱内容。尽管相关噪声在计算机图形学中应用广泛，但其在改进训练过程方面的潜力尚未充分探索。本文介绍了一种新颖且通用的扩散模型类，考虑了图像内部和跨图像的相关噪声。具体而言，我们提出了一种时变噪声模型来将相关噪声纳入训练过程，并提出了一种快速生成相关噪声掩码的方法。我们的模型基于确定性扩散模型，利用蓝噪声相比仅使用高斯白噪声（随机噪声）有助于提高生成质量。此外，我们的框架还允许在单个小批量中引入图像之间的相关性来改善梯度流动。我们进行了定性和定量实验证明了我们模型的有效性。

    Most of the existing diffusion models use Gaussian noise for training and sampling across all time steps, which may not optimally account for the frequency contents reconstructed by the denoising network. Despite the diverse applications of correlated noise in computer graphics, its potential for improving the training process has been underexplored. In this paper, we introduce a novel and general class of diffusion models taking correlated noise within and across images into account. More specifically, we propose a time-varying noise model to incorporate correlated noise into the training process, as well as a method for fast generation of correlated noise mask. Our model is built upon deterministic diffusion models and utilizes blue noise to help improve the generation quality compared to using Gaussian white (random) noise only. Further, our framework allows introducing correlation across images within a single mini-batch to improve gradient flow. We perform both qualitative and qua
    
[^32]: 无源域自适应的扩散引导源数据生成

    Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation

    [https://arxiv.org/abs/2402.04929](https://arxiv.org/abs/2402.04929)

    本文提出了一种无源域自适应的新方法，利用扩散模型生成上下文相关的领域特定图像，通过微调预训练模型和无监督领域自适应技术实现了显著的性能改进。

    

    本文引入了一种利用扩散模型的泛化能力进行无源域自适应（DM-SFDA）的新方法。我们提出的DM-SFDA方法包括对预训练的文本到图像扩散模型进行微调，并使用目标图像的特征来指导扩散过程生成源域图像。具体而言，预训练的扩散模型被微调以生成最小化熵并最大化预训练源模型置信度的源样本。然后，我们应用已建立的无监督领域自适应技术将生成的源图像与目标域数据进行对齐。我们通过在一系列数据集上进行全面实验验证了我们的方法，包括Office-31、Office-Home和VisDA。结果显示，在无源域自适应的性能方面取得了显著的改进，展示了扩散模型在生成上下文相关的、领域特定的图像方面的潜力。

    This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.
    
[^33]: TP感知的去量化

    TP-Aware Dequantization

    [https://arxiv.org/abs/2402.04925](https://arxiv.org/abs/2402.04925)

    本文介绍了一种TP感知的去量化方法，通过优化推理部署方案解决了分布式部署大型语言模型的推理延迟问题。该方法保留了数据局部性和利用TP的先验知识来减少全局通信，在多种TP设置下，在A100和H100 NVIDIA DGX系统上实现了显著速度提升。

    

    在本文中，我们提出了一种新颖的方法，在分布式部署大型语言模型（LLM）的过程中减少模型推理延迟。我们的贡献是一种优化的推理部署方案，解决了当前最先进的量化内核在与张量并行（TP）结合使用时的局限性。我们的方法保留了GPU内存访问模式中的数据局部性，并利用TP的先验知识来减少全局通信。我们在A100和H100 NVIDIA DGX系统上展示了对于各种TP设置，对于Llama-70B的速度提升高达1.81倍，对于IBM WatsonX的Granite-20B MLP层问题尺寸的速度提升高达1.78倍。

    In this paper, we present a novel method that reduces model inference latency during distributed deployment of Large Language Models (LLMs). Our contribution is an optimized inference deployment scheme that address the current limitations of state-of-the-art quantization kernels when used in conjunction with Tensor Parallel (TP). Our method preserves data locality in GPU memory access patterns and exploits a priori knowledge of TP to reduce global communication. We demonstrate an up to 1.81x speedup over existing methods for Llama-70B and up to 1.78x speedup for IBM WatsonX's Granite-20B MLP layer problem sizes on A100 and H100 NVIDIA DGX Systems for a variety of TP settings.
    
[^34]: 两个交易不会困扰：通过构造合理的梯度匹配来压缩图表

    Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching

    [https://arxiv.org/abs/2402.04924](https://arxiv.org/abs/2402.04924)

    本论文提出了一种新颖的图表压缩方法CTRL，通过优化起点和精细的策略，解决了梯度匹配方向导致的训练轨迹偏差和累积误差问题。

    

    在大规模图表上训练已经在图表表示学习方面取得了显著成果，但其成本和存储引起了越来越多的关注。作为最有前景的方向之一，图表压缩方法通过使用梯度匹配来解决这些问题，目标是将完整的图表压缩成更简洁但信息丰富的合成集。尽管令人鼓舞，但这些策略主要强调梯度的匹配方向，从而导致训练轨迹的偏差。这种偏差进一步由压缩和评估阶段之间的差异放大，最终导致累积误差，对压缩图表的性能产生不利影响。鉴于此，我们提出了一种名为\textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory（\textbf{CTRL}）的新型图表压缩方法，它提供了一个更接近原始数据集特征分布的优化起点和一个更精细的策略。

    Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns. As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set. Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories. Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs. In light of this, we propose a novel graph condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory (\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for 
    
[^35]: Voronoi Candidates用于贝叶斯优化

    Voronoi Candidates for Bayesian Optimization

    [https://arxiv.org/abs/2402.04922](https://arxiv.org/abs/2402.04922)

    使用Voronoi候选点边界可以在贝叶斯优化中有效地优化黑盒函数，提高了多起始连续搜索的执行时间。

    

    贝叶斯优化（BO）为高效优化黑盒函数提供了一种优雅的方法。然而，采集准则需要进行具有挑战性的内部优化，这可能引起很大的开销。许多实际的BO方法，尤其是在高维情况下，不采用对采集函数进行形式化连续优化，而是在有限的空间填充候选集上进行离散搜索。在这里，我们提议使用候选点，其位于当前设计点的Voronoi镶嵌边界上，因此它们与两个或多个设计点等距离。我们讨论了通过直接采样Voronoi边界而不明确生成镶嵌的策略，从而适应高维度中的大设计。通过使用高斯过程和期望改进来对一组测试问题进行优化，我们的方法在不损失准确性的情况下显著提高了多起始连续搜索的执行时间。

    Bayesian optimization (BO) offers an elegant approach for efficiently optimizing black-box functions. However, acquisition criteria demand their own challenging inner-optimization, which can induce significant overhead. Many practical BO methods, particularly in high dimension, eschew a formal, continuous optimization of the acquisition function and instead search discretely over a finite set of space-filling candidates. Here, we propose to use candidates which lie on the boundary of the Voronoi tessellation of the current design points, so they are equidistant to two or more of them. We discuss strategies for efficient implementation by directly sampling the Voronoi boundary without explicitly generating the tessellation, thus accommodating large designs in high dimension. On a battery of test problems optimized via Gaussian processes with expected improvement, our proposed approach significantly improves the execution time of a multi-start continuous search without a loss in accuracy
    
[^36]: Moco: 一种可学习的组合优化元优化器

    Moco: A Learnable Meta Optimizer for Combinatorial Optimization

    [https://arxiv.org/abs/2402.04915](https://arxiv.org/abs/2402.04915)

    Moco是一个可学习的组合优化元优化器，通过学习图神经网络来更新解决方案构建过程，并能够适应不同的情况和计算预算。

    

    相关的组合优化问题（COPs）通常是NP难的。过去，这些问题主要是通过人工设计的启发式方法来解决的，但是神经网络的进展促使人们开发了从数据中学习启发式方法的通用方法。许多方法利用神经网络直接构建解决方案，但在推理时无法进一步改进已经构建的解决方案。我们的方法Moco学习了一个图神经网络，根据从当前搜索状态提取的特征来更新解决方案构建过程。这种元训练过程以搜索过程中找到的最佳解决方案为目标，给定搜索预算等信息。这使得Moco能够适应不同的情况，例如不同的计算预算。Moco是一个完全可学习的元优化器，不使用任何特定问题的局部搜索或分解。我们在旅行商问题（TSP）和最大最小费用流问题中测试了Moco。

    Relevant combinatorial optimization problems (COPs) are often NP-hard. While they have been tackled mainly via handcrafted heuristics in the past, advances in neural networks have motivated the development of general methods to learn heuristics from data. Many approaches utilize a neural network to directly construct a solution, but are limited in further improving based on already constructed solutions at inference time. Our approach, Moco, learns a graph neural network that updates the solution construction procedure based on features extracted from the current search state. This meta training procedure targets the overall best solution found during the search procedure given information such as the search budget. This allows Moco to adapt to varying circumstances such as different computational budgets. Moco is a fully learnable meta optimizer that does not utilize any problem specific local search or decomposition. We test Moco on the Traveling Salesman Problem (TSP) and Maximum In
    
[^37]: 朝向生物学上合理且私密的基因表达数据生成

    Towards Biologically Plausible and Private Gene Expression Data Generation

    [https://arxiv.org/abs/2402.04912](https://arxiv.org/abs/2402.04912)

    本文提出了朝向生物学上合理且私密的基因表达数据生成的方法，并对五种代表性的差分隐私生成方法进行了全面分析。通过综合评估，揭示了每种方法的优点和缺点，为未来发展提供了有趣的可能性。

    

    使用差分隐私（DP）训练的生成模型在创建下游应用的合成数据方面越来越受到关注。然而现有的文献主要关注基本基准数据集，并且往往仅报告对于基本指标和相对简单的数据分布表现出的有希望的结果。在本文中，我们开始系统分析DP生成模型在其自然应用场景中的表现，特别关注真实世界的基因表达数据。我们对五种具有代表性的DP生成方法进行了全面分析，从下游效用、统计特性和生物合理性等多个角度进行了检查。我们的广泛评估揭示了每种DP生成方法的独特特点，为每种方法的优点和缺点提供了关键见解，并揭示了未来发展的有趣可能性。出人意料的是，我们的分析显示，每种DP生成方法都有其独特的特点和潜力。

    Generative models trained with Differential Privacy (DP) are becoming increasingly prominent in the creation of synthetic data for downstream applications. Existing literature, however, primarily focuses on basic benchmarking datasets and tends to report promising results only for elementary metrics and relatively simple data distributions. In this paper, we initiate a systematic analysis of how DP generative models perform in their natural application scenarios, specifically focusing on real-world gene expression data. We conduct a comprehensive analysis of five representative DP generation methods, examining them from various angles, such as downstream utility, statistical properties, and biological plausibility. Our extensive evaluation illuminates the unique characteristics of each DP generation method, offering critical insights into the strengths and weaknesses of each approach, and uncovering intriguing possibilities for future developments. Perhaps surprisingly, our analysis re
    
[^38]: 机器教学中的组合问题探究

    On a Combinatorial Problem Arising in Machine Teaching

    [https://arxiv.org/abs/2402.04907](https://arxiv.org/abs/2402.04907)

    本文研究了机器教学中的一个组合问题，通过证明了一个最坏情况下的猜想，得出了关于教学维度的结果。该结果可以看作是解决了超立方体边界等周问题的定理的推广。

    

    本文研究了一种机器教学模型，其中教师映射是由概念和示例的大小函数构建的。机器教学中的主要问题是任何概念所需的最小示例数量，即所谓的教学维度。最近的一篇论文[7]猜测，在这个模型中，作为概念类大小的函数时，最坏情况发生在一致性矩阵包含从零及以上的二进制表示的数字时。在本文中，我们证明了他们的猜想。该结果可以看作是解决超立方体的边界等周问题的定理[12]的推广，我们的证明基于[10]的引理。

    We study a model of machine teaching where the teacher mapping is constructed from a size function on both concepts and examples. The main question in machine teaching is the minimum number of examples needed for any concept, the so-called teaching dimension. A recent paper [7] conjectured that the worst case for this model, as a function of the size of the concept class, occurs when the consistency matrix contains the binary representations of numbers from zero and up. In this paper we prove their conjecture. The result can be seen as a generalization of a theorem resolving the edge isoperimetry problem for hypercubes [12], and our proof is based on a lemma of [10].
    
[^39]: 预测个体治疗效果的一致性蒙特卡洛元学习模型

    Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects

    [https://arxiv.org/abs/2402.04906](https://arxiv.org/abs/2402.04906)

    本研究提出了一种新方法，即一致性蒙特卡洛元学习模型，用于预测个体治疗效果。通过利用一致性预测系统、蒙特卡洛采样和CATE元学习模型，该方法生成可用于个性化决策的预测分布。实验结果显示，该方法在保持较小区间宽度的情况下具有强大的实验覆盖范围，可以提供真实个体治疗效果的估计。

    

    认识干预效果，即治疗效果，对于决策至关重要。用条件平均治疗效果 (CATE) 估计等方法通常只提供治疗效果的点估计，而常常需要额外的不确定性量化。因此，我们提出了一个新方法，即一致性蒙特卡洛 (CMC) 元学习模型，利用一致性预测系统、蒙特卡洛采样和 CATE 元学习模型，来产生可用于个性化决策的预测分布。此外，我们展示了结果噪声分布的特定假设如何严重影响这些不确定性预测。尽管如此，CMC框架展示了强大的实验覆盖范围，同时保持较小的区间宽度，以提供真实个体治疗效果的估计。

    Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.
    
[^40]: L4Q: 通过基于LoRA的量化训练在大型语言模型上提供参数高效的量化训练

    L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ

    [https://arxiv.org/abs/2402.04902](https://arxiv.org/abs/2402.04902)

    L4Q是一种参数高效的量化感知训练算法，通过基于LoRA的学习的量化步长，解决了大型语言模型中量化训练的挑战。

    

    后训练量化(PTQ)和量化感知训练(QAT)方法正在流行起来，以缓解大型语言模型(LLMs)所带来的高内存和计算成本。在资源受限的情况下，尽管后者具有更高的准确性潜力，但由于其减少的训练开销，通常首选后训练量化。同时，介绍了参数高效微调方法，如低秩适应（LoRA），并最近的工作已经探索了量化感知参数高效微调技术。然而，这些方法可能缺乏通用性，因为它们依赖于预量化模型的配置。由非线性量化或混合精度权重引起的效果可能会受到影响，并且重新训练特定量化参数可能会影响最优性能。为了应对这些挑战，我们提出了L4Q，一种参数高效的量化感知训练算法。L4Q利用了基于LoRA的学习的量化步长。

    Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
    
[^41]: 成功的压力：一种用于减轻足球运动员受伤风险和提高球队成功率的预测模型

    The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer

    [https://arxiv.org/abs/2402.04898](https://arxiv.org/abs/2402.04898)

    本文开发了一种预测模型，通过考虑球员受伤概率，在足球赛季中优化团队表现，减少一线队受伤人数和无效花费，为真实足球团队降低成本并提高球员福利提供了潜力。

    

    在本文中，我们提出了一种新颖的足球顺序团队选择模型。具体而言，我们使用从真实足球数据中学到的球员特定信息对球员受伤和不可用性的随机过程进行建模。蒙特卡洛树搜索被用于为游戏选择团队，通过对球员受伤概率进行推理，优化整个赛季的团队绩效。我们将我们的方法与2018/19英超赛季的基准解进行验证。我们的模型在降低一线队受伤人数约13%和无效花费在受伤球员身上的资金约11%的同时，实现了与基准相似的赛季预期积分 - 这表明了在真实世界的足球团队中降低成本和改善球员福利的潜力。

    In this paper, we present a novel sequential team selection model in soccer. Specifically, we model the stochastic process of player injury and unavailability using player-specific information learned from real-world soccer data. Monte-Carlo Tree Search is used to select teams for games that optimise long-term team performance across a soccer season by reasoning over player injury probability. We validate our approach compared to benchmark solutions for the 2018/19 English Premier League season. Our model achieves similar season expected points to the benchmark whilst reducing first-team injuries by ~13% and the money inefficiently spent on injured players by ~11% - demonstrating the potential to reduce costs and improve player welfare in real-world soccer teams.
    
[^42]: 从最佳中学习：无线通信的主动学习

    Learning from the Best: Active Learning for Wireless Communications

    [https://arxiv.org/abs/2402.04896](https://arxiv.org/abs/2402.04896)

    本文介绍了无线通信中应用主动学习的深度学习方法，并提供了一个基于深度学习的毫米波波束选择的案例研究，评估了不同主动学习算法在一个公开可用数据集上的性能。

    

    对于基于深度学习的通信任务来说，收集一份空中无线通信训练数据集相对简单。然而，标注这个数据集需要专家参与和领域知识，可能涉及私有知识产权，并且通常在计算和财务方面都很昂贵。主动学习是机器学习中的新兴研究领域，旨在减少标注开销而不会降低准确性。主动学习算法能够在未标注的数据集中识别出最关键和信息量丰富的样本，只标注这些样本，而不是整个数据集。在本文中，我们介绍了在无线通信中应用主动学习的深度学习方法，并介绍了其不同的类别。我们还提供了一个基于深度学习的毫米波波束选择的案例研究，其中的标注由一个基于穷举搜索的计算密集型算法执行。我们评估了不同主动学习算法在一个公开可用数据集上的性能。

    Collecting an over-the-air wireless communications training dataset for deep learning-based communication tasks is relatively simple. However, labeling the dataset requires expert involvement and domain knowledge, may involve private intellectual properties, and is often computationally and financially expensive. Active learning is an emerging area of research in machine learning that aims to reduce the labeling overhead without accuracy degradation. Active learning algorithms identify the most critical and informative samples in an unlabeled dataset and label only those samples, instead of the complete set. In this paper, we introduce active learning for deep learning applications in wireless communications, and present its different categories. We present a case study of deep learning-based mmWave beam selection, where labeling is performed by a compute-intensive algorithm based on exhaustive search. We evaluate the performance of different active learning algorithms on a publicly av
    
[^43]: 利用动态图的深度强化学习进行自适应信息路径规划

    Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning

    [https://arxiv.org/abs/2402.04894](https://arxiv.org/abs/2402.04894)

    提出了一种利用动态图的深度强化学习方法，用于自适应信息路径规划，能够在未知的三维环境中映射出感兴趣的目标。

    

    自主机器人常常被用于数据收集，因为它们高效且劳动成本低。机器人数据采集的关键任务是在初始未知环境中规划路径，以满足平台特定的资源约束，例如有限的电池寿命。在三维环境中进行自适应在线路径规划面临着很多挑战，包括大量有效动作的存在以及未知遮挡物的存在。为了解决这些问题，我们提出了一种新颖的深度强化学习方法，用于自适应重新规划机器人路径以在未知的三维环境中映射出感兴趣的目标。我们方法的关键之处在于构建动态图，将规划动作限制在机器人附近，使我们能够快速响应新发现的障碍和感兴趣的目标。对于重新规划，我们提出了一种新的奖励函数，平衡探索未知环境和利用在线收集的有关感兴趣目标的数据。

    Autonomous robots are often employed for data collection due to their efficiency and low labour costs. A key task in robotic data acquisition is planning paths through an initially unknown environment to collect observations given platform-specific resource constraints, such as limited battery life. Adaptive online path planning in 3D environments is challenging due to the large set of valid actions and the presence of unknown occlusions. To address these issues, we propose a novel deep reinforcement learning approach for adaptively replanning robot paths to map targets of interest in unknown 3D environments. A key aspect of our approach is a dynamically constructed graph that restricts planning actions local to the robot, allowing us to quickly react to newly discovered obstacles and targets of interest. For replanning, we propose a new reward function that balances between exploring the unknown environment and exploiting online-collected data about the targets of interest. Our experi
    
[^44]: 通过加权模型集成的概率验证AI系统的统一框架

    A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration

    [https://arxiv.org/abs/2402.04892](https://arxiv.org/abs/2402.04892)

    本论文提出了一个基于加权模型集成的统一框架，用于概率验证AI系统。这个框架可以在不依赖强分布假设的情况下，验证各种机器学习模型的许多有趣属性，如公平性、鲁棒性或单调性。

    

    概率形式验证（PFV) AI系统还处于起步阶段，迄今为止，对于特定类别的模型和/或属性，方法仅限于特定的算法而已。我们提出了一个基于加权模型集成（WMI）的AI系统PFV的统一框架，可以非常通用地定义问题。关键是，这种约简可以在不做过强的分布假设的情况下，验证许多有趣的属性，如公平性、鲁棒性或单调性，适用于各种机器学习模型。我们通过使用一个现成的WMI求解器解决多个验证任务来支持这种方法的普适性，然后讨论与这个有前途的框架相关的可扩展性挑战和研究方向。

    The probabilistic formal verification (PFV) of AI systems is in its infancy. So far, approaches have been limited to ad-hoc algorithms for specific classes of models and/or properties.   We propose a unifying framework for the PFV of AI systems based onWeighted Model Integration (WMI), which allows to frame the problem in very general terms.   Crucially, this reduction enables the verification of many properties of interest, like fairness, robustness or monotonicity, over a wide range of machine learning models, without making strong distributional assumptions.   We support the generality of the approach by solving multiple verification tasks with a single, off-the-shelf WMI solver, then discuss the scalability challenges and research directions related to this promising framework.
    
[^45]: RSCNet：云基WiFi感知的动态CSI压缩

    RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing

    [https://arxiv.org/abs/2402.04888](https://arxiv.org/abs/2402.04888)

    这篇论文提出了一种名为RSCNet的动态CSI压缩方法，通过压缩信道状态信息（CSI）来减少物联网设备向云服务器传输CSI的通信开销。RSCNet利用长短期记忆（LSTM）单元和优化的CSI窗口实现了准确的感知和CSI重建，从而实现了实时的云基WiFi感知。

    

    WiFi连接的物联网设备正在从纯粹的通信设备发展为利用信道状态信息（CSI）提取能力的感知工具。然而，资源有限的物联网设备和深度神经网络的复杂性要求将CSI传输到云服务器进行感知。尽管可行，但这会导致大量的通信开销。在这种背景下，本文开发了一种新颖的实时感知和压缩网络（RSCNet），它能够通过压缩CSI来实现感知，从而减少通信开销。RSCNet在由少量CSI帧组成的CSI窗口之间进行优化。一旦传输到云服务器，它利用长短期记忆（LSTM）单元从先前的窗口中提取数据，从而增强感知准确性和CSI重建。RSCNet巧妙地平衡了CSI压缩和感知精度之间的权衡，从而简化了实时云基WiFi感知，并减少了开销。

    WiFi-enabled Internet-of-Things (IoT) devices are evolving from mere communication devices to sensing instruments, leveraging Channel State Information (CSI) extraction capabilities. Nevertheless, resource-constrained IoT devices and the intricacies of deep neural networks necessitate transmitting CSI to cloud servers for sensing. Although feasible, this leads to considerable communication overhead. In this context, this paper develops a novel Real-time Sensing and Compression Network (RSCNet) which enables sensing with compressed CSI; thereby reducing the communication overheads. RSCNet facilitates optimization across CSI windows composed of a few CSI frames. Once transmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units to harness data from prior windows, thus bolstering both the sensing accuracy and CSI reconstruction. RSCNet adeptly balances the trade-off between CSI compression and sensing precision, thus streamlining real-time cloud-based WiFi sensing with redu
    
[^46]: 一个统一的高斯过程用于分支和嵌套超参数优化

    A Unified Gaussian Process for Branching and Nested Hyperparameter Optimization

    [https://arxiv.org/abs/2402.04885](https://arxiv.org/abs/2402.04885)

    本文提出了一个统一的贝叶斯优化方法，用于处理分支和嵌套参数之间的条件依赖关系，该方法能够有效地优化神经网络的超参数选择和调整。

    

    在神经网络的成功中，选择合适的超参数起着至关重要的作用，因为超参数直接控制训练算法的行为和性能。为了获得高效的调参，基于高斯过程（GP）模型的贝叶斯优化方法被广泛应用。尽管贝叶斯优化在深度学习中具有许多应用，但现有的方法都基于一个方便但限制性的假设，即调参参数彼此独立。然而，在实践中，条件依赖的调参参数是常见的。本文重点研究了两种类型的调参参数：分支和嵌套参数。嵌套参数指的是那些仅存在于另一个调参参数特定设置中的调参参数，而其它参数在其中嵌套的参数称为分支参数。为了捕捉分支和嵌套参数之间的条件依赖关系，本文提出了一个统一的贝叶斯优化方法。

    Choosing appropriate hyperparameters plays a crucial role in the success of neural networks as hyper-parameters directly control the behavior and performance of the training algorithms. To obtain efficient tuning, Bayesian optimization methods based on Gaussian process (GP) models are widely used. Despite numerous applications of Bayesian optimization in deep learning, the existing methodologies are developed based on a convenient but restrictive assumption that the tuning parameters are independent of each other. However, tuning parameters with conditional dependence are common in practice. In this paper, we focus on two types of them: branching and nested parameters. Nested parameters refer to those tuning parameters that exist only within a particular setting of another tuning parameter, and a parameter within which other parameters are nested is called a branching parameter. To capture the conditional dependence between branching and nested parameters, a unified Bayesian optimizati
    
[^47]: LMUFormer：具有Legendre记忆单元的低复杂度但强大的脉冲模型

    LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units

    [https://arxiv.org/abs/2402.04882](https://arxiv.org/abs/2402.04882)

    本文提出了一种改进的脉冲模型LMUFormer，通过对循环模型进行架构修改，将其性能推向Transformer模型，同时保留其顺序处理能力，以实现并行训练、流式处理和低成本推理。

    

    Transformer模型在许多应用中表现出高准确性，但复杂度高且缺乏顺序处理能力，使其不适用于资源受限的许多边缘流应用。因此，许多研究人员提出将Transformer模型重新定义为具有显式状态的RNN模块，来修改自注意力计算。然而，这些方法往往会导致性能显著降低。本文旨在开发具有以下特性的模型：并行训练、流式处理和低成本推理，并具有SOTA性能。我们提出了一种新的方法来实现这个目标。我们展示了如何通过对循环模型进行架构修改，将其性能推向Transformer模型，同时保留其顺序处理能力。具体来说，我们受到了Legendre记忆单元（LMU）在序列学习任务中的最近成功启发，提出了LMUFormer模型。

    Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have proposed reformulating the transformer models as RNN modules which modify the self-attention computation with explicit states. However, these approaches often incur significant performance degradation. The ultimate goal is to develop a model that has the following properties: parallel training, streaming and low-cost inference, and SOTA performance. In this paper, we propose a new direction to achieve this goal. We show how architectural modifications to a recurrent model can help push its performance toward Transformer models while retaining its sequential processing capability. Specifically, inspired by the recent success of Legendre Memory Units (LMU) in sequence learning tasks, we propose LM
    
[^48]: 结合云计算与移动计算的机器学习研究

    Combining Cloud and Mobile Computing for Machine Learning

    [https://arxiv.org/abs/2402.04880](https://arxiv.org/abs/2402.04880)

    这项研究将模型分割为移动设备和云之间的计算，以减轻移动设备的负担，并优化云端的工作负载。

    

    尽管移动设备的计算能力正在增加，但机器学习模型的大小也在增长。这种趋势给移动设备带来了问题，如内存容量和电池寿命的限制。虽然许多服务（如ChatGPT和Midjourney）在云中运行所有的推理，但我们认为灵活性和细粒度的任务分配更可取。在这项工作中，我们将模型分割视为改善用户体验的解决方案，将计算分割在移动设备和云之间，以减轻模型的计算密集部分，同时尽量减少数据传输的需求。我们展示了这种分割不仅减少了用户等待时间，还可以通过细粒度调整来优化云端的工作负载。为了实现这一目标，我们设计了一个调度器，收集网络质量、客户端设备能力和作业要求的信息，做出决策以实现在各种设备上的一致性性能。

    Although the computing power of mobile devices is increasing, machine learning models are also growing in size. This trend creates problems for mobile devices due to limitations like their memory capacity and battery life. While many services, like ChatGPT and Midjourney, run all the inferences in the cloud, we believe a flexible and fine-grained task distribution is more desirable. In this work, we consider model segmentation as a solution to improving the user experience, dividing the computation between mobile devices and the cloud in a way that offloads the compute-heavy portion of the model while minimizing the data transfer required. We show that the division not only reduces the wait time for users but can also be fine-tuned to optimize the workloads of the cloud. To achieve that, we design a scheduler that collects information about network quality, client device capability, and job requirements, making decisions to achieve consistent performance across a range of devices while
    
[^49]: 关于可证明的长度和组合泛化

    On Provable Length and Compositional Generalization

    [https://arxiv.org/abs/2402.04875](https://arxiv.org/abs/2402.04875)

    本研究针对包括深度集合、变压器、状态空间模型和简单递归神经网络等多种架构，探索了可证明的长度和组合泛化，认为对于长度和组合泛化，不同架构需要不同程度的表示识别。

    

    长度泛化——对训练时未见到的更长序列的泛化能力，以及组合泛化——对训练时未见到的令牌组合的泛化能力，在序列到序列模型中是重要的非分布化泛化形式。在这项工作中，我们在包括深度集合、变压器、状态空间模型和简单递归神经网络在内的一系列架构中，朝着可证明的长度和组合泛化迈出了第一步。根据架构的不同，我们证明了不同程度的表示识别的必要性，例如与真实表示具有线性或排列关系。

    Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
    
[^50]: 选择具有图神经网络的经典规划器

    Choosing a Classical Planner with Graph Neural Networks

    [https://arxiv.org/abs/2402.04874](https://arxiv.org/abs/2402.04874)

    本文研究了使用图神经网络进行在线规划器选择的方法，并通过析取出的图表示作为另一模型的输入，提出了一种更加资源高效但准确的方法。

    

    在线规划器选择是在给定规划问题的预定义解集中选择一个解算器的任务。由于规划计算复杂，解算器在规划问题上的性能差异很大。因此，预测解算器在给定问题上的性能非常重要。虽然有各种学习方法被采用，但在经典的最优代价规划中，主流方法使用了图神经网络（GNN）。在这项工作中，我们继续使用GNN进行在线规划器选择的研究。我们对所选择的GNN模型、图表示和节点特征以及预测任务的影响进行了彻底的调查。更进一步，我们提出使用GNN获得的图表示作为极端梯度提升（XGBoost）模型的输入，从而实现一种更加资源高效但准确的方法。我们展示了各种基于GNN的在线规划器选择方法的有效性，开辟了新的激动人心的研究方向。

    Online planner selection is the task of choosing a solver out of a predefined set for a given planning problem. As planning is computationally hard, the performance of solvers varies greatly on planning problems. Thus, the ability to predict their performance on a given problem is of great importance. While a variety of learning methods have been employed, for classical cost-optimal planning the prevailing approach uses Graph Neural Networks (GNNs). In this work, we continue the line of work on using GNNs for online planner selection. We perform a thorough investigation of the impact of the chosen GNN model, graph representation and node features, as well as prediction task. Going further, we propose using the graph representation obtained by a GNN as an input to the Extreme Gradient Boosting (XGBoost) model, resulting in a more resource-efficient yet accurate approach. We show the effectiveness of a variety of GNN-based online planner selection methods, opening up new exciting avenues
    
[^51]: 将知识图谱嵌入到退化的克利福德代数中

    Embedding Knowledge Graphs in Degenerate Clifford Algebras

    [https://arxiv.org/abs/2402.04870](https://arxiv.org/abs/2402.04870)

    这项研究提出将知识图谱嵌入到退化的克利福德代数中。通过考虑具有零幂指数为2的零幂基向量，可以泛化基于二次数的方法并捕捉实体嵌入中缺乏高阶相互作用的模式。研究设计了两个新模型来发现代数的参数，并证明零幂向量有助于捕捉实体的特征。

    

    克利福德代数是实数、复数和四元数的自然推广。迄今为止，在知识图谱嵌入的背景下，只有形式为$Cl_{p,q}$（即没有零幂基向量的代数）的克利福德代数受到研究。我们提出考虑零幂基向量，其幂指数为2。在这些空间中，被称为$Cl_{p,q,r}$，可以泛化基于二次数的方法（无法使用$Cl_{p,q}$进行建模）并捕捉源于实数和复数部分间缺乏高阶相互作用的实体嵌入的模式。我们设计了两个新模型来发现参数$p$，$q$和$r$。第一个模型使用贪婪搜索优化$p$，$q$和$r$。第二个模型基于使用神经网络计算的输入知识图谱的嵌入来预测$(p, q, r)$。我们在七个基准数据集上进行的评估结果表明，零幂向量有助于捕捉实体的特征。

    Clifford algebras are a natural generalization of the real numbers, the complex numbers, and the quaternions. So far, solely Clifford algebras of the form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been studied in the context of knowledge graph embeddings. We propose to consider nilpotent base vectors with a nilpotency index of two. In these spaces, denoted $Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from the absence of higher-order interactions between real and complex parts of entity embeddings. We design two new models for the discovery of the parameters $p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$, and $r$. The second predicts $(p, q,r)$ based on an embedding of the input knowledge graph computed using neural networks. The results of our evaluation on seven benchmark datasets suggest that nilpotent vectors can help capture 
    
[^52]: 学习通过实践：具有因果感知的在线因果强化学习框架

    Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy

    [https://arxiv.org/abs/2402.04869](https://arxiv.org/abs/2402.04869)

    本文提出了一个在线因果强化学习框架，其中通过明确建模状态的生成过程和使用因果结构进行策略引导，以帮助强化学习代理的决策可解释性。该框架具有理论性能保证，并在探索和开发过程中使用干预进行因果结构学习。

    

    因果知识作为人类智能中直观认知和推理解决方案的关键组成部分，为强化学习（RL）代理的可解释性决策提供了巨大的潜力，通过帮助减少搜索空间。然而，发现和整合因果关系进入RL仍存在相当大的差距，这阻碍了因果关系RL的快速发展。在本文中，我们考虑使用因果图模型明确地建模状态的生成过程，并在此基础上增强策略。我们将因果结构更新形式化为具有主动环境干预学习的RL交互过程。为了优化衍生的目标，我们提出了一个具有理论性能保证的框架，两个步骤交替进行：在探索过程中使用干预进行因果结构学习，在开发过程中使用学习到的因果结构进行策略引导。由于缺少公共基准，用于对所提出的方法进行评估。

    As a key component to intuitive cognition and reasoning solutions in human intelligence, causal knowledge provides great potential for reinforcement learning (RL) agents' interpretability towards decision-making by helping reduce the searching space. However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL. In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy. We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation. Due to the lack of public benchmarks that 
    
[^53]: 使用复值神经网络和不规则分布的麦克风重建房间传递函数

    Room transfer function reconstruction using complex-valued neural networks and irregularly distributed microphones

    [https://arxiv.org/abs/2402.04866](https://arxiv.org/abs/2402.04866)

    本研究使用复值神经网络在有限的麦克风数据中估计房间传递函数，实现了房间传递函数的重建，创新之处在于首次使用了复值神经网络来估计房间传递函数。

    

    重建房间传递函数用于计算房间内的复杂声场具有重要的实际应用。然而，通常需要使用大量的麦克风，这是不现实的。最近，除了传统的信号处理方法，深度学习技术已经被应用于从房间内零散点测量得到的有限的房间传递函数来重建房间传递函数。在本研究中，我们使用复值神经网络估计房间传递函数在第一个声学共振频率范围内，使用少量不规则分布的麦克风。据我们所知，这是首次使用复值神经网络来估计房间传递函数。为了分析将复值优化应用于所考虑任务的好处，我们将所提出的技术与最先进的实值神经网络方法和基于核的最先进方法进行比较。

    Reconstructing the room transfer functions needed to calculate the complex sound field in a room has several important real-world applications. However, an unpractical number of microphones is often required. Recently, in addition to classical signal processing methods, deep learning techniques have been applied to reconstruct the room transfer function starting from a very limited set of room transfer functions measured at scattered points in the room. In this study, we employ complex-valued neural networks to estimate room transfer functions in the frequency range of the first room resonances, using a few irregularly distributed microphones. To the best of our knowledge, this is the first time complex-valued neural networks are used to estimate room transfer functions. To analyze the benefits of applying complex-valued optimization to the considered task, we compare the proposed technique with a state-of-the-art real-valued neural network method and a state-of-the-art kernel-based si
    
[^54]: CodeIt：具有优先级回顾重放的自我改进语言模型

    CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay

    [https://arxiv.org/abs/2402.04858](https://arxiv.org/abs/2402.04858)

    CodeIt是一种具备优先级回顾重放的自我改进语言模型方法，通过将目标重标记为采样程序的实际输出，有效解决了程序合成中奖励稀疏性的问题，并在抽象和推理语料库（ARC）上实现了成功的跨任务泛化。

    

    大型语言模型越来越能够解决通常被认为需要人类水平推理能力的任务。然而，这些模型在通用智能基准测试例如抽象和推理语料库（ARC）上表现仍然非常差。在本文中，我们将ARC视为一个以编程示例为基础的问题，并引入了一种名为Code Iteration（CodeIt）的新颖且可扩展的语言模型自我改进方法。我们的方法在1）程序抽样和回顾重标记以及2）基于优先级的经验回放之间进行迭代。通过将一个episode的目标（即给定输入的目标程序输出）重标记为采样程序产生的实际输出，我们的方法有效地处理了程序合成中奖励极度稀疏性的问题。应用CodeIt于ARC数据集，我们证明了优先级回顾重放、预训练和数据增强可以实现成功的跨任务泛化。CodeIt是第一个神经元-合成机制一体的自我改进语言模型方法。

    Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-sy
    
[^55]: 通过反事实轨迹解释学习到的奖励函数

    Explaining Learned Reward Functions with Counterfactual Trajectories

    [https://arxiv.org/abs/2402.04856](https://arxiv.org/abs/2402.04856)

    通过对比原始轨迹和反事实部分轨迹的奖励，我们提出了一种解释强化学习中奖励函数的方法。通过生成符合质量标准的反事实轨迹解释（CTEs），我们的实验表明，CTEs对于代理人模型具有明显的信息性，能提高其预测与奖励函数的一致性。

    

    从人类行为或反馈中学习奖励是将AI系统与人类价值观一致的一种有希望的方法，但无法始终提取正确的奖励函数。可解释性工具可以帮助用户理解和评估学习到的奖励函数中可能存在的缺陷。我们提出了反事实轨迹解释（CTEs），通过对比原始轨迹和反事实部分轨迹以及它们各自接收的奖励来解释强化学习中的奖励函数。我们为CTEs制定了六个质量标准，并提出了一种基于Monte-Carlo的新算法来生成优化这些质量标准的CTEs。最后，我们通过训练代理人模型来衡量生成的解释对其的信息性。CTEs对于代理人模型具有明显的信息性，增加了其预测与未见轨迹上的奖励函数的相似性。此外，它学会了准确判断轨迹之间的奖励差异。

    Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajec
    
[^56]: 分层树状知识图谱用于学术调研

    Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey

    [https://arxiv.org/abs/2402.04854](https://arxiv.org/abs/2402.04854)

    该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。

    

    对于缺乏研究培训的初学者研究者来说，研究调查一直是一个挑战。这些研究者在短时间内很难理解他们研究主题内的方向，以及发现新的研究发现。为初学者研究者提供直观的帮助的一种方式是提供相关的知识图谱(KG)并推荐相关的学术论文。然而，现有的导航知识图谱主要依赖于研究领域的关键字，常常无法清楚地呈现多个相关论文之间的逻辑层次关系。此外，大多数学术论文推荐系统仅仅依赖于高文本相似性，这可能会让研究人员困惑为什么推荐了特定的文章。他们可能缺乏了解关于他们希望获得的"问题解决"和"问题发现"之间的见解连接的重要信息。为解决这些问题，本研究旨在支持初学者研究者进行研究调研。

    Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
    
[^57]: 多块预测：适应时间序列表示学习的LLMs方法

    Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning

    [https://arxiv.org/abs/2402.04852](https://arxiv.org/abs/2402.04852)

    本研究提出了aLLM4TS框架，将LLMs应用于时间序列表示学习。通过自监督的多块预测任务，捕捉时间动态特征，并通过特定时间序列上的微调进行进一步优化。

    

    本研究提出了一个创新的框架aLLM4TS，用于将大型语言模型（LLMs）应用于时间序列表示学习。我们的方法将时间序列预测重新构想为一项自监督的多块预测任务，相比传统的掩码和重构方法，更有效地捕捉了块表示中的时间动态。我们的策略包括两个阶段的训练：(i) 在各种时间序列数据集上进行因果连续预训练阶段，以下一个块预测为锚点，有效地将LLM的能力与时间序列数据的复杂性同步。(ii) 在目标时间序列上进行多块预测的微调。我们框架的一个独特要素是块级解码层，不同于之前依赖于序列级解码的方法。这样的设计直接将单个块转换为时间序列，从而显著增强了模型在掩蔽任务下的能力。

    In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mast
    
[^58]: AlphaFold遇到Flow Matching生成蛋白质集合

    AlphaFold Meets Flow Matching for Generating Protein Ensembles

    [https://arxiv.org/abs/2402.04845](https://arxiv.org/abs/2402.04845)

    本研究开发了一种基于流动匹配的生成建模方法，称为AlphaFlow和ESMFlow，用于学习和采样蛋白质的构象空间。与AlphaFold相比，该方法在精度和多样性方面提供了更优的组合，在训练和评估时能准确捕捉到构象灵活性和高阶组合可观测性。同时，该方法可以将静态PDB结构多样化到特定的平衡性质，具有较快的收敛速度。

    

    蛋白质的生物功能往往依赖于动态结构集合。本研究中，我们开发了一种基于流动匹配的生成建模方法，用于学习和采样蛋白质的构象空间。我们重新利用高精度的单态预测器，如AlphaFold和ESMFold，并在自定义流匹配框架下对其进行微调，以获得基于序列条件的蛋白质结构生成模型，称为AlphaFlow和ESMFlow。在PDB上进行训练和评估时，我们的方法相比于AlphaFold和MSA子采样提供了更高的精度和多样性的组合。当进一步训练所有原子MD的组合时，我们的方法可以准确地捕捉到未见蛋白质的构象灵活性、位置分布和高阶组合可观测性。此外，我们的方法可以通过更快的时钟收敛速度将静态PDB结构多样化到特定的平衡性质，比复制的MD轨迹更具潜力。

    The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a 
    
[^59]: 关于不变几何深度学习模型的完备性

    On the Completeness of Invariant Geometric Deep Learning Models

    [https://arxiv.org/abs/2402.04836](https://arxiv.org/abs/2402.04836)

    这项研究集中于不变模型的理论表达能力，通过引入完备的设计GeoNGNN，并利用其作为理论工具，首次证明了E(3)-完备性。

    

    不变模型是一类重要的几何深度学习模型，通过利用信息丰富的几何特征生成有意义的几何表示。这些模型以其简单性、良好的实验结果和计算效率而闻名。然而，它们的理论表达能力仍然不清楚，限制了对这种模型潜力的深入理解。在这项工作中，我们集中讨论不变模型的理论表达能力。我们首先严格限制了最经典的不变模型Vanilla DisGNN（结合距离的消息传递神经网络）的表达能力，将其不可识别的情况仅限于高度对称的几何图形。为了打破这些特殊情况的对称性，我们引入了一个简单而完备的不变设计，即嵌套Vanilla DisGNN的GeoNGNN。利用GeoNGNN作为理论工具，我们首次证明了E(3)-完备性。

    Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness 
    
[^60]: SARI: 简洁平均与鲁棒性基于嘈杂部分标签学习

    SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning

    [https://arxiv.org/abs/2402.04835](https://arxiv.org/abs/2402.04835)

    SARI是一个简约的框架，通过利用嘈杂部分标签，结合平均策略和识别策略，实现了部分标签学习中的深度神经网络分类器训练，并显著提升了准确性。

    

    部分标签学习 (PLL) 是一种弱监督学习范式，其中每个训练实例都与一组候选标签 (部分标签) 成对，其中一个是真正的标签。嘈杂部分标签学习 (NPLL) 放宽了这个约束，允许一些部分标签不包含真正的标签，增加了问题的实用性。我们的工作集中在 NPLL 上，并提出了一个简约的框架 SARI，通过利用加权最近邻算法将伪标签分配给图像。然后，这些伪标签与图像配对用于训练深度神经网络分类器，采用标签平滑和标准正则化技术。随后，利用分类器的特征和预测结果来改进和提高伪标签的准确性。SARI结合了文献中基于平均策略 (伪标签) 和基于识别策略 (分类器训练)的优点。我们进行了详尽的实验评估，验证了SARI的有效性和性能提升。

    Partial label learning (PLL) is a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing and standard regularization techniques. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature. We perform thorough ex
    
[^61]: 缩小SGP4和高精度传播之间的差距：通过可微编程

    Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming

    [https://arxiv.org/abs/2402.04830](https://arxiv.org/abs/2402.04830)

    本研究介绍了dSGP4，一种使用PyTorch实现的可微版本的SGP4。通过可微化，dSGP4实现了轨道传播的高精度，并且适用于各种与太空相关的应用，包括卫星轨道确定、状态转换、协方差传播等。

    

    简化的第四级摄动(SGP4)轨道传播方法被广泛用于快速可靠地预测地球轨道物体的位置和速度。尽管不断改进，SGP模型仍然缺乏数值传播器的精度，后者的误差显著较小。本研究提出了dSGP4，一种使用PyTorch实现的新型可微版本的SGP4。通过使SGP4可微化，dSGP4便于进行各种与太空相关的应用，包括航天器轨道确定、状态转换、协方差转换、状态转移矩阵计算和协方差传播。此外，dSGP4的PyTorch实现允许在批量的TLE（两行参数）集上进行尴尬的并行轨道传播，利用CPU、GPU和分布式预测卫星位置的高级硬件的计算能力。此外，dSGP4的可微性使其能与模式集成。

    The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably. Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors. This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation. Additionally, dSGP4's PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times. Furthermore, dSGP4's differentiability enables integration with mode
    
[^62]: 快速定时条件下的潜在音频扩散

    Fast Timing-Conditioned Latent Audio Diffusion

    [https://arxiv.org/abs/2402.04825](https://arxiv.org/abs/2402.04825)

    本研究提出了一种名为Stable Audio的生成模型，该模型利用潜在扩散和条件化技术，实现了高效生成44.1kHz立体声音乐和音效。与其他最先进的模型不同，Stable Audio能够生成具有结构和立体声的音乐，并在性能评估上表现出色。

    

    从文本提示生成长篇44.1kHz立体声音频可能对计算要求很高。此外，大多数先前的工作并没有解决音乐和音效在持续时间上的自然变化问题。我们的研究专注于使用生成模型以高效方式生成长篇、可变长度的44.1kHz立体声音乐和音效。Stable Audio基于潜在扩散，其潜在性质由一个全卷积变分自编码器定义。它不仅基于文本提示进行条件化，还基于时间嵌入，使得可以对生成的音乐和音效的内容和长度进行精细控制。在A100 GPU上，Stable Audio能够在8秒内以44.1kHz的速度渲染长达95秒的立体声信号。尽管计算效率高且推理速度快，但它在两个公开的文本-音乐和音频基准中仍然是最好的，并且与最先进的模型不同，它可以生成具有结构和立体声音乐。

    Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. It is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.
    
[^63]: 你的合成数据有多真实？对表格数据的深度生成模型进行约束

    How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data

    [https://arxiv.org/abs/2402.04823](https://arxiv.org/abs/2402.04823)

    本文提出了一种约束深度生成模型(C-DGMs)，通过将约束转化为约束层(CL)来保证生成样本符合给定约束。实验结果表明，相较于标准DGMs，C-DGMs能够更好地遵守约束。

    

    深度生成模型(DGMs)已被证明是生成表格数据的强大工具，因为它们能够越来越多地捕捉到其特征的复杂分布。然而，要生成真实的合成数据，仅仅拥有对其分布的良好近似通常是不够的，还需要遵守编码了问题背景知识的约束。在本文中，我们解决了这个限制，并展示了如何将用于表格数据的DGMs转化为保持给定约束的约束深度生成模型(C-DGMs)。这是通过自动解析约束并将其转化为与DGM无缝集成的约束层(CL)实现的。我们进行了大量的实验分析，使用了多种DGMs和任务，结果显示标准DGMs经常违反约束，一些超过了95%的不合规情况，而相应的C-DGMs从未违反约束。

    Deep Generative Models (DGMs) have been shown to be powerful tools for generating tabular data, as they have been increasingly able to capture the complex distributions that characterize them. However, to generate realistic synthetic data, it is often not enough to have a good approximation of their distribution, as it also requires compliance with constraints that encode essential background knowledge on the problem at hand. In this paper, we address this limitation and show how DGMs for tabular data can be transformed into Constrained Deep Generative Models (C-DGMs), whose generated samples are guaranteed to be compliant with the given constraints. This is achieved by automatically parsing the constraints and transforming them into a Constraint Layer (CL) seamlessly integrated with the DGM. Our extensive experimental analysis with various DGMs and tasks reveals that standard DGMs often violate constraints, some exceeding $95\%$ non-compliance, while their corresponding C-DGMs are nev
    
[^64]: E(3)-等变Mesh神经网络

    E(3)-Equivariant Mesh Neural Networks

    [https://arxiv.org/abs/2402.04821](https://arxiv.org/abs/2402.04821)

    E(3)-等变Mesh神经网络通过扩展E(n)-等变图神经网络的更新方程以包括网格面信息，并通过层次化进一步改进以考虑长程相互作用，实现了在网格任务上的优越性能，具有快速运行时间和无需昂贵预处理的特点。

    

    三角网格被广泛用于表示三维物体。因此，许多最近的研究都致力于在3D网格上进行几何深度学习。然而，我们观察到，许多这些架构的复杂性与实际性能之间并没有直接关联，并且在实践中，简单的深度模型对于几何图表现竞争力。受到这一观察的启发，我们最小限度地扩展了E(n)-等变图神经网络（EGNNs）（Satorras等，2021）的更新方程，以包括网格面信息，并通过层次化进一步改进了该方程以考虑长程相互作用。由此得到的架构，即等变Mesh神经网络（EMNN），在网格任务上优于其他更复杂的等变方法，具有快速的运行时间和无需昂贵的预处理。

    Triangular meshes are widely used to represent three-dimensional objects. As a result, many recent works have address the need for geometric deep learning on 3D mesh. However, we observe that the complexities in many of these architectures does not translate to practical performance, and simple deep models for geometric graphs are competitive in practice. Motivated by this observation, we minimally extend the update equations of E(n)-Equivariant Graph Neural Networks (EGNNs) (Satorras et al., 2021) to incorporate mesh face information, and further improve it to account for long-range interactions through hierarchy. The resulting architecture, Equivariant Mesh Neural Network (EMNN), outperforms other, more complicated equivariant methods on mesh tasks, with a fast run-time and no expensive pre-processing.
    
[^65]: BOWLL：一个看似简单的开放世界终身学习者

    BOWLL: A Deceptively Simple Open World Lifelong Learner

    [https://arxiv.org/abs/2402.04814](https://arxiv.org/abs/2402.04814)

    这项研究提出了BOWLL，一个看似简单但极其有效的方法，通过重新利用标准模型用于开放世界终身学习，加速了这个多方面领域的探索。

    

    深度学习中对预先确定的基准测试的标量性能数字的改进似乎深深植根于其中。然而，现实世界很少精心策划，应用也很少仅限于在测试集上表现出色。通常需要一个实际的系统来识别新概念，避免主动包括无信息的数据，并在其生命周期内保留先前获取的知识。尽管这些关键要素在个体上已经进行了严格的研究，但对它们的结合，即开放世界终身学习，只是最近的趋势。为了加速这个多方面领域的探索，我们引入其首个完整且极度需要的基准。利用深度神经网络中批量归一化的普遍应用，我们提出了一个看似简单但非常有效的方法来重新利用标准模型进行开放世界终身学习。通过广泛的实证评估，我们强调为什么我们的方法应该成为未来的标准。

    The quest to improve scalar performance numbers on predetermined benchmarks seems to be deeply engraved in deep learning. However, the real world is seldom carefully curated and applications are seldom limited to excelling on test sets. A practical system is generally required to recognize novel concepts, refrain from actively including uninformative data, and retain previously acquired knowledge throughout its lifetime. Despite these key elements being rigorously researched individually, the study of their conjunction, open world lifelong learning, is only a recent trend. To accelerate this multifaceted field's exploration, we introduce its first monolithic and much-needed baseline. Leveraging the ubiquitous use of batch normalization across deep neural networks, we propose a deceptively simple yet highly effective way to repurpose standard models for open world lifelong learning. Through extensive empirical evaluation, we highlight why our approach should serve as a future standard f
    
[^66]: 通过显式的内核特征映射实现可扩展的多视角聚类

    Scalable Multi-view Clustering via Explicit Kernel Features Maps

    [https://arxiv.org/abs/2402.04794](https://arxiv.org/abs/2402.04794)

    本文介绍了一种可扩展的多视角子空间聚类框架，并提出了一种高效的优化策略，利用内核特征映射来减轻计算负担。该算法可在大规模数据集上应用，并在几分钟内完成。

    

    多视角学习作为数据科学和机器学习中的重要组成部分引起了越来越多的关注，这是由于实际应用中多视角的普遍存在，特别是在网络的上下文中。本文介绍了一种新的可扩展的多视角子空间聚类框架。提出了一种高效的优化策略，利用内核特征映射来减轻计算负担，同时保持良好的聚类性能。算法的可扩展性意味着它可以在标准机器上应用于大规模数据集，包括具有数百万数据点的数据集，并在几分钟内完成。我们在真实世界各种规模的基准网络上进行了大量实验证明我们的算法在多视角子空间聚类方法和属性网络多视角方法方面的性能。

    A growing awareness of multi-view learning as an important component in data science and machine learning is a consequence of the increasing prevalence of multiple views in real-world applications, especially in the context of networks. In this paper we introduce a new scalability framework for multi-view subspace clustering. An efficient optimization strategy is proposed, leveraging kernel feature maps to reduce the computational burden while maintaining good clustering performance. The scalability of the algorithm means that it can be applied to large-scale datasets, including those with millions of data points, using a standard machine, in a few minutes. We conduct extensive experiments on real-world benchmark networks of various sizes in order to evaluate the performance of our algorithm against state-of-the-art multi-view subspace clustering methods and attributed-network multi-view approaches.
    
[^67]: Shadowheart SGD: 在任意的计算和通信异构性下具有最优时间复杂度的分布式异步SGD

    Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity

    [https://arxiv.org/abs/2402.04785](https://arxiv.org/abs/2402.04785)

    Shadowheart SGD是一种新的分布式异步SGD方法，利用无偏压缩技术，在任意的计算和通信异构性下具有最优时间复杂度，并显著优化了先前的集中式方法。同时，我们还开发了对应的双向设置方法。

    

    在异步集中式分布式设置中考虑非凸随机优化问题，其中工作者到服务器的通信时间不能忽略，而所有工作者的计算和通信时间可能不同。利用无偏压缩技术，我们开发了一种新的方法-Shadowheart SGD，它可证明优化了所有先前集中式方法的时间复杂度。此外，我们还展示了Shadowheart SGD在压缩通信的集中式方法族中的时间复杂度是最优的。我们还考虑了双向设置，在这种设置下，从服务器到工作者的广播不可忽略，并开发了相应的方法。

    We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique, we develop a new method-Shadowheart SGD-that provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method.
    
[^68]: 周期激活坐标网络的神经切向核分析

    Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate Networks

    [https://arxiv.org/abs/2402.04783](https://arxiv.org/abs/2402.04783)

    通过分析神经切向核（NTK），本文提供了对周期性激活网络的理论理解，并发现周期性激活网络在NTK的角度上比ReLU激活网络更加“良好”。此外，我们还验证了这些网络的记忆容量。

    

    最近，利用周期性激活函数的神经网络在视觉任务中表现出了比传统的ReLU激活网络更好的性能。然而，对于这种改进性能的原因还存在有限的了解。本文旨在通过分析神经切向核（NTK）来提供对周期性激活网络的理论理解。我们在有限宽度的设置下推导出NTK的最小特征值的上界，使用了一个相对通用的网络架构，只需要一个宽度至少与数据样本数量线性增长的层。我们的发现表明，从NTK的角度来看，周期性激活网络比ReLU激活网络更加“良好”。此外，我们还将这些网络的记忆容量应用于一个案例，并通过实验验证了我们的理论预测。我们的研究提供了一个新的理论视角来解释周期性激活网络的性能提升。

    Recently, neural networks utilizing periodic activation functions have been proven to demonstrate superior performance in vision tasks compared to traditional ReLU-activated networks. However, there is still a limited understanding of the underlying reasons for this improved performance. In this paper, we aim to address this gap by providing a theoretical understanding of periodically activated networks through an analysis of their Neural Tangent Kernel (NTK). We derive bounds on the minimum eigenvalue of their NTK in the finite width setting, using a fairly general network architecture which requires only one wide layer that grows at least linearly with the number of data samples. Our findings indicate that periodically activated networks are \textit{notably more well-behaved}, from the NTK perspective, than ReLU activated networks. Additionally, we give an application to the memorization capacity of such networks and verify our theoretical predictions empirically. Our study offers a 
    
[^69]: 基于得分的快速搜索算法在使用熵的最大祖先图中

    A fast score-based search algorithm for maximal ancestral graphs using entropy

    [https://arxiv.org/abs/2402.04777](https://arxiv.org/abs/2402.04777)

    本研究提出了一种基于得分的快速搜索算法，用于使用熵的最大祖先图。通过引入imsets框架和精化马尔科夫属性，我们改进了MAG的评分方法，并证明了搜索算法的多项式复杂度。在模拟实验中，我们的算法表现出了优越的性能。

    

    最大祖先图（MAGs）是一类在存在潜在混杂因素的情况下扩展了著名的有向无环图的图模型。大多数基于评分的方法通过使用BIC评分从经验数据中推断未知MAG，但该方法存在不稳定性和计算复杂性的问题。我们提出使用imsets框架通过经验熵估计和新提出的精化马尔科夫属性对MAG进行评分。我们的图搜索过程与\citet{claassen2022greedy}类似，但是根据我们的理论结果进行了改进。我们证明了我们的搜索算法在节点数上是多项式复杂度的，通过限制度数、最大头部大小和歧视路径数。在模拟实验中，与其他现有的MAG学习算法相比，我们的算法表现出优越的性能。

    \emph{Maximal ancestral graph} (MAGs) is a class of graphical model that extend the famous \emph{directed acyclic graph} in the presence of latent confounders. Most score-based approaches to learn the unknown MAG from empirical data rely on BIC score which suffers from instability and heavy computations. We propose to use the framework of imsets \citep{studeny2006probabilistic} to score MAGs using empirical entropy estimation and the newly proposed \emph{refined Markov property} \citep{hu2023towards}. Our graphical search procedure is similar to \citet{claassen2022greedy} but improved from our theoretical results. We show that our search algorithm is polynomial in number of nodes by restricting degree, maximal head size and number of discriminating paths. In simulated experiment, our algorithm shows superior performance compared to other state of art MAG learning algorithms.
    
[^70]: 代码即奖励：用VLM增强强化学习的动力

    Code as Reward: Empowering Reinforcement Learning with VLMs

    [https://arxiv.org/abs/2402.04764](https://arxiv.org/abs/2402.04764)

    本文介绍了一种利用预训练视觉语言模型(VLMs)来支持强化学习(RL)代理训练的框架，通过代码生成从VLMs生成密集奖励函数，减轻了直接查询VLM的计算负担。

    

    预训练的视觉语言模型(VLMs)能够理解视觉概念，描述并分解复杂任务为子任务，并提供有关任务完成的反馈。本文旨在利用这些能力来支持增强学习(RL)代理的训练。原则上，VLMs非常适合这个目的，因为它们可以自然地分析基于图像的观察结果，并提供关于学习进度的反馈(奖励)。然而，VLMs的推理过程计算代价很高，频繁查询以计算奖励将显著减慢RL代理的训练速度。为了解决这个挑战，我们提出了一种名为“代码即奖励”(VLM-CaR)的框架。VLM-CaR通过代码生成从VLMs生成密集的奖励函数，从而显著减轻了直接查询VLM的计算负担。我们证明通过我们的方法生成的密集奖励在多样的离散和连续环境中都非常准确。

    Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, a
    
[^71]: 挑战性照明环境下的颜色识别：基于CNN的方法

    Color Recognition in Challenging Lighting Environments: CNN Approach

    [https://arxiv.org/abs/2402.04762](https://arxiv.org/abs/2402.04762)

    本论文提出了一种基于CNN的方法，用于挑战性照明环境下的颜色识别。实验证明，该方法能够显著提高在不同光照条件下的颜色检测的鲁棒性，并且比现有方法表现更好。

    

    光线在视觉中起着至关重要的作用，无论是人类还是机器的视觉，对于周围光照条件的感知都会影响颜色的识别。研究人员正致力于提高计算机视觉中的颜色检测技术。他们提出了多种不同的颜色检测方法，但仍存在一些可以填补的空白。为了解决这个问题，我们提出了一种基于卷积神经网络（CNN）的颜色检测方法。首先，使用边缘检测分割技术进行图像分割，以确定对象，然后将分割的对象输入经过训练的卷积神经网络，以检测不同光照条件下对象的颜色。实验证明，我们的方法可以显著提高在不同光照条件下的颜色检测的鲁棒性，并且我们的方法比现有方法表现更好。

    Light plays a vital role in vision either human or machine vision, the perceived color is always based on the lighting conditions of the surroundings. Researchers are working to enhance the color detection techniques for the application of computer vision. They have implemented proposed several methods using different color detection approaches but still, there is a gap that can be filled. To address this issue, a color detection method, which is based on a Convolutional Neural Network (CNN), is proposed. Firstly, image segmentation is performed using the edge detection segmentation technique to specify the object and then the segmented object is fed to the Convolutional Neural Network trained to detect the color of an object in different lighting conditions. It is experimentally verified that our method can substantially enhance the robustness of color detection in different lighting conditions, and our method performed better results than existing methods.
    
[^72]: 通过考虑美学约束的扩散模型实现对齐布局生成

    Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints

    [https://arxiv.org/abs/2402.04754](https://arxiv.org/abs/2402.04754)

    本文提出了一种统一的布局生成模型，通过考虑美学约束，在图形设计中创建合理的元素可视排列。与之前的方法相比，该模型表现出更好的对齐性能，并在各种布局生成任务中取得了优秀的性能。

    

    可控布局生成是指在具有代表设计意图的约束条件下，在图形设计（例如文档和网页设计）中创建一种合理的元素可视排列的过程。尽管最近基于扩散模型的方法取得了最先进的FID分数，但与之前基于转换器的模型相比，它们往往表现出更明显的偏离对齐。在这项工作中，我们提出了一种统一的模型——LACE（Layout Constraint Diffusion Model），用于处理各种布局生成任务，例如根据指定属性排列元素、完善或完成粗糙的布局设计。该模型基于连续的扩散模型。与使用离散扩散模型的现有方法相比，连续状态空间设计可以在训练中引入可微的美学约束函数。对于条件生成，我们通过掩码输入引入条件。大量实验证明，LACE模型在不同布局生成任务上都取得了优秀的性能。

    Controllable layout generation refers to the process of creating a plausible visual arrangement of elements within a graphic design (e.g., document and web designs) with constraints representing design intentions. Although recent diffusion-based models have achieved state-of-the-art FID scores, they tend to exhibit more pronounced misalignment compared to earlier transformer-based models. In this work, we propose the $\textbf{LA}$yout $\textbf{C}$onstraint diffusion mod$\textbf{E}$l (LACE), a unified model to handle a broad range of layout generation tasks, such as arranging elements with specified attributes and refining or completing a coarse layout design. The model is based on continuous diffusion models. Compared with existing methods that use discrete diffusion models, continuous state-space design can enable the incorporation of differentiable aesthetic constraint functions in training. For conditional generation, we introduce conditions via masked input. Extensive experiment re
    
[^73]: 渐进梯度流在Transformer中稀疏训练的鲁棒N:M结构

    Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers

    [https://arxiv.org/abs/2402.04744](https://arxiv.org/abs/2402.04744)

    本文研究了Transformer中高稀疏区域稀疏训练方法的有效性，发现现有方法无法保持与低稀疏区域相当的模型质量，主要原因是梯度幅值中引入的噪音水平提高。为了解决这个问题，采用了渐进限制梯度流动的衰减机制。

    

    由于相对较低的开销和提高的效率，N:M结构的稀疏性引起了广泛的关注。此外，这种稀疏性形式对于降低内存占用具有相当大的吸引力，因为其表示开销较小。已经有一些努力针对N:M结构的稀疏性开发训练方法，但是它们主要关注低稀疏区域($\sim$50\%)。然而，使用这些方法训练的模型在面对高稀疏区域($>$80\%)时往往性能下降。在这项工作中，我们研究了现有稀疏训练方法在高稀疏区域的有效性，并认为这些方法无法保持与低稀疏区域相当的模型质量。我们证明造成这种差距的主要因素是梯度幅值中引入的噪音水平提高。为了减轻这种不良影响，我们采用衰减机制逐渐限制梯度的流动。

    N:M Structured sparsity has garnered significant interest as a result of relatively modest overhead and improved efficiency. Additionally, this form of sparsity holds considerable appeal for reducing the memory footprint owing to their modest representation overhead. There have been efforts to develop training recipes for N:M structured sparsity, they primarily focus on low-sparsity regions ($\sim$50\%). Nonetheless, performance of models trained using these approaches tends to decline when confronted with high-sparsity regions ($>$80\%). In this work, we study the effectiveness of existing sparse training recipes at \textit{high-sparsity regions} and argue that these methods fail to sustain the model quality on par with low-sparsity regions. We demonstrate that the significant factor contributing to this disparity is the presence of elevated levels of induced noise in the gradient magnitudes. To mitigate this undesirable effect, we employ decay mechanisms to progressively restrict the
    
[^74]: 多维标记Hawkes过程的非参数估计

    Non-Parametric Estimation of Multi-dimensional Marked Hawkes Processes

    [https://arxiv.org/abs/2402.04740](https://arxiv.org/abs/2402.04740)

    这篇论文提出了一种非参数估计标记Hawkes过程条件强度的方法，引入了两种模型来处理具有不同内核和非线性特征的Hawkes过程，通过将过去的到达时间和标记作为输入，获得到达强度。

    

    标记Hawkes过程是Hawkes过程的一个扩展，其特点是每个事件的跳跃大小不同，与没有标记的Hawkes过程中观察到的恒定跳跃大小不同。尽管在线性和非线性Hawkes过程的非参数估计上已经有了广泛的文献，但在标记Hawkes过程方面的文献仍存在重大空白。为此，我们提出了估计标记Hawkes过程条件强度的方法。我们引入了两个不同的模型：“具有标记的浅层神经Hawkes模型”-用于具有兴奋性内核的Hawkes过程，以及“非线性Hawkes具有标记的神经网络模型”-用于非线性Hawkes过程。这两种方法将过去的到达时间及其相应的标记作为输入，以获取到达强度。这种方法是完全非参数的，保持了标记Hawkes过程的可解释性。

    An extension of the Hawkes process, the Marked Hawkes process distinguishes itself by featuring variable jump size across each event, in contrast to the constant jump size observed in a Hawkes process without marks. While extensive literature has been dedicated to the non-parametric estimation of both the linear and non-linear Hawkes process, there remains a significant gap in the literature regarding the marked Hawkes process. In response to this, we propose a methodology for estimating the conditional intensity of the marked Hawkes process. We introduce two distinct models: \textit{Shallow Neural Hawkes with marks}- for Hawkes processes with excitatory kernels and \textit{Neural Network for Non-Linear Hawkes with Marks}- for non-linear Hawkes processes. Both these approaches take the past arrival times and their corresponding marks as the input to obtain the arrival intensity. This approach is entirely non-parametric, preserving the interpretability associated with the marked Hawkes 
    
[^75]: 通过最优输运实现具有任意大小约束的图割

    Graph Cuts with Arbitrary Size Constraints Through Optimal Transport

    [https://arxiv.org/abs/2402.04732](https://arxiv.org/abs/2402.04732)

    本论文提出了一种新的图割算法，通过将图割问题制定为正则化的Gromov-Wasserstein问题，并使用加速的近端GD算法解决，实现在任意大小约束下对图进行分割，在非平衡数据集聚类等应用中具有更高的效率。

    

    图的常见分割方法是最小割。经典最小割方法的一个缺点是它们倾向于生成小的分组，这就是为什么更平衡的变体，如归一化割和比例割取得了更多的成功。然而，我们认为对于某些应用，如非平衡数据集的聚类，这些变体的平衡约束可能过于限制，而对于寻找完美平衡分区来说不够限制。在这里，我们提出了一种新的图割算法，用于在任意大小约束下对图进行分割。我们将图割问题制定为正则化的Gromov-Wasserstein问题。然后，我们提出使用加速的近端GD算法来解决它，该算法具有全局收敛性保证，产生稀疏解，并且只比经典谱聚类算法多消耗$\mathcal{O}(\log(n))$的附加比率，但效率更高。

    A common way of partitioning graphs is through minimum cuts. One drawback of classical minimum cut methods is that they tend to produce small groups, which is why more balanced variants such as normalized and ratio cuts have seen more success. However, we believe that with these variants, the balance constraints can be too restrictive for some applications like for clustering of imbalanced datasets, while not being restrictive enough for when searching for perfectly balanced partitions. Here, we propose a new graph cut algorithm for partitioning graphs under arbitrary size constraints. We formulate the graph cut problem as a regularized Gromov-Wasserstein problem. We then propose to solve it using accelerated proximal GD algorithm which has global convergence guarantees, results in sparse solutions and only incurs an additional ratio of $\mathcal{O}(\log(n))$ compared to the classical spectral clustering algorithm but was seen to be more efficient.
    
[^76]: 图形基准最近邻搜索的自适应入口点选择的理论与实证分析

    Theoretical and Empirical Analysis of Adaptive Entry Point Selection for Graph-based Approximate Nearest Neighbor Search

    [https://arxiv.org/abs/2402.04713](https://arxiv.org/abs/2402.04713)

    本论文对图形基准最近邻搜索的自适应入口点选择进行了理论和实证分析，并提出了新的概念$b\textit{-单调路径}$和$B\textit{-MSNET}$。理论证明了自适应入口点选择在更一般的条件下比固定中心入口点具有更好的性能上界。实验验证了该方法在各种数据集上的准确性、速度和内存使用方面的有效性，尤其是在分布之外的数据和难例的挑战场景中。这项全面研究提供了优化图形基准最近邻搜索入口点的深入洞察，适用于实际的高维数据应用。

    

    我们对图形基准最近邻搜索的自适应入口点选择进行了理论和实证分析。我们引入了新的概念：$b\textit{-单调路径}$和$B\textit{-MSNET}$，比现有的概念如MSNET更好地捕捉了实际算法中的图形特征。我们证明了在比以前的工作更一般的条件下，自适应入口点选择提供了比固定中心入口点更好的性能上界。在实证方面，我们验证了该方法在准确性、速度和内存使用方面在各种数据集上的有效性，尤其是在分布之外的数据和难例的挑战性场景中。我们的综合研究深入洞察了用于实际高维数据应用中的图形基准最近邻搜索的入口点优化。

    We present a theoretical and empirical analysis of the adaptive entry point selection for graph-based approximate nearest neighbor search (ANNS). We introduce novel concepts: $b\textit{-monotonic path}$ and $B\textit{-MSNET}$, which better capture an actual graph in practical algorithms than existing concepts like MSNET. We prove that adaptive entry point selection offers better performance upper bound than the fixed central entry point under more general conditions than previous work. Empirically, we validate the method's effectiveness in accuracy, speed, and memory usage across various datasets, especially in challenging scenarios with out-of-distribution data and hard instances. Our comprehensive study provides deeper insights into optimizing entry points for graph-based ANNS for real-world high-dimensional data applications.
    
[^77]: 将检索式因果学习与信息瓶颈相结合，用于可解释的图神经网络

    Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks

    [https://arxiv.org/abs/2402.04710](https://arxiv.org/abs/2402.04710)

    本研究开发了一种新颖的、可解释的因果GNN框架，将检索式因果学习与图信息瓶颈相结合，能够有效地处理拓扑数据，并提供对GNN的透明和直观理解。

    

    图神经网络（GNN）因其有效处理拓扑数据的能力而受到广泛关注，但其可解释性仍然是一个关键问题。当前的解释方法主要依赖事后解释，以提供对GNN的透明和直观理解。然而，在解释复杂子图时，它们的性能有限，无法利用解释来改进GNN的预测。另一方面，提出了透明的GNN模型来捕捉关键子图。虽然这种方法可以改善GNN的预测，但在解释方面通常表现不佳。因此，需要一种新的策略更好地联系GNN的解释与预测。在本研究中，我们开发了一种新颖的、可解释的因果GNN框架，它将检索式因果学习与图信息瓶颈（GIB）理论相结合。该框架能够半参数地检索GIB检测到的关键子图，并压缩解释信息以提高预测性能。

    Graph Neural Networks (GNNs) have gained considerable traction for their capability to effectively process topological data, yet their interpretability remains a critical concern. Current interpretation methods are dominated by post-hoc explanations to provide a transparent and intuitive understanding of GNNs. However, they have limited performance in interpreting complicated subgraphs and can't utilize the explanation to advance GNN predictions. On the other hand, transparent GNN models are proposed to capture critical subgraphs. While such methods could improve GNN predictions, they usually don't perform well on explanations. Thus, it is desired for a new strategy to better couple GNN explanation and prediction. In this study, we have developed a novel interpretable causal GNN framework that incorporates retrieval-based causal learning with Graph Information Bottleneck (GIB) theory. The framework could semi-parametrically retrieve crucial subgraphs detected by GIB and compress the ex
    
[^78]: EvoSeed：揭示使用真实世界幻觉对深度神经网络的威胁

    EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions

    [https://arxiv.org/abs/2402.04699](https://arxiv.org/abs/2402.04699)

    EvoSeed是一个基于进化策略的搜索算法框架，用于生成自然对抗样本，旨在揭示深度神经网络面临的威胁。该框架在模型无关的黑盒设置中操作，可以生成高质量且可转移的对抗图像。

    

    深度神经网络被自然对抗样本所利用，这些样本对人类感知没有影响，但会被错误分类。目前的方法通常依赖于深度神经网络的白盒性质来生成这些对抗样本，或者改变对抗样本与训练分布的分布。为了缓解当前方法的局限性，我们提出了EvoSeed，这是一个基于进化策略的搜索算法框架，用于生成自然对抗样本。我们的EvoSeed框架使用辅助扩散和分类器模型在模型无关的黑盒设置中运行。我们使用CMA-ES来优化对对抗种子向量的搜索，该向量在经过条件扩散模型处理后，导致分类器模型错误分类无限制的自然对抗样本。实验证明生成的对抗图像具有高质量，并且可应用于不同的分类器。

    Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstra
    
[^79]: 从相关分量的解释方差到无正交约束的主成分分析

    From explained variance of correlated components to PCA without orthogonality constraints

    [https://arxiv.org/abs/2402.04692](https://arxiv.org/abs/2402.04692)

    本文提出了一种从相关分量的解释方差到无正交约束的主成分分析的方法，通过引入衡量数据矩阵A中由相关分量Y = AZ解释的方差部分的新目标函数expvar(Y)，放松了加载的正交约束。

    

    针对数据矩阵A的块状主成分分析(Block PCA)中的加载Z由于在单位范数正交加载上最大化AZ的困难，使得使用1正则化来设计稀疏PCA变得困难，因为很难同时处理加载的正交约束和不可微的1惩罚。本文的目标是通过引入衡量数据矩阵A中由相关分量Y = AZ解释的方差部分的新目标函数expvar(Y)来放松加载的正交约束。因此，我们首先对两个现有定义Zou et al. [2006]和Shen and Huang [2008]的expvar(Y)进行了全面的数学和数值属性研究，并提出了四个新的定义。然后我们证明只有这两个解释方差才适合作为块状PCA形式中去除正交约束的目标函数。

    Block Principal Component Analysis (Block PCA) of a data matrix A, where loadings Z are determined by maximization of AZ 2 over unit norm orthogonal loadings, is difficult to use for the design of sparse PCA by 1 regularization, due to the difficulty of taking care of both the orthogonality constraint on loadings and the non differentiable 1 penalty. Our objective in this paper is to relax the orthogonality constraint on loadings by introducing new objective functions expvar(Y) which measure the part of the variance of the data matrix A explained by correlated components Y = AZ. So we propose first a comprehensive study of mathematical and numerical properties of expvar(Y) for two existing definitions Zou et al. [2006], Shen and Huang [2008] and four new definitions. Then we show that only two of these explained variance are fit to use as objective function in block PCA formulations for A rid of orthogonality constraints.
    
[^80]: 在一般希尔伯特空间中使用随机梯度下降学习算子

    Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces

    [https://arxiv.org/abs/2402.04691](https://arxiv.org/abs/2402.04691)

    本研究在一般希尔伯特空间中使用随机梯度下降（SGD）学习算子，提出了适用于目标算子的规则条件，并建立了SGD算法的收敛速度上界，同时展示了对于非线性算子学习的有效性及线性近似收敛特性。

    

    本研究探讨了利用随机梯度下降（SGD）在一般希尔伯特空间中学习算子的方法。我们提出了针对目标算子的弱和强规则条件，以描述其内在结构和复杂性。在这些条件下，我们建立了SGD算法的收敛速度的上界，并进行了极小值下界分析，进一步说明我们的收敛分析和规则条件定量地刻画了使用SGD算法解决算子学习问题的可行性。值得强调的是，我们的收敛分析对于非线性算子学习仍然有效。我们证明了SGD估计器将收敛于非线性目标算子的最佳线性近似。此外，将我们的分析应用于基于矢量值和实值再生核希尔伯特空间的算子学习问题，产生了新的收敛结果，从而完善了现有文献的结论。

    This study investigates leveraging stochastic gradient descent (SGD) to learn operators between general Hilbert spaces. We propose weak and strong regularity conditions for the target operator to depict its intrinsic structure and complexity. Under these conditions, we establish upper bounds for convergence rates of the SGD algorithm and conduct a minimax lower bound analysis, further illustrating that our convergence analysis and regularity conditions quantitatively characterize the tractability of solving operator learning problems using the SGD algorithm. It is crucial to highlight that our convergence analysis is still valid for nonlinear operator learning. We show that the SGD estimator will converge to the best linear approximation of the nonlinear target operator. Moreover, applying our analysis to operator learning problems based on vector-valued and real-valued reproducing kernel Hilbert spaces yields new convergence results, thereby refining the conclusions of existing litera
    
[^81]: 大型语言模型作为可信的解释器

    Large Language Models As Faithful Explainers

    [https://arxiv.org/abs/2402.04678](https://arxiv.org/abs/2402.04678)

    本论文提出了一个生成解释框架（xLLM），用于提高大型语言模型（LLMs）自然语言格式解释的可信度。通过一个评估器来量化解释的可信度，并通过迭代优化过程来提高可信度。

    

    近年来，大型语言模型(LLMs)通过利用其丰富的内部知识和推理能力，已经能够熟练解决复杂的任务。然而，这种复杂性阻碍了传统的以输入为重点的解释算法来解释LLMs的复杂决策过程。为了解决这个问题，最近出现了一种自我解释机制，通过自然语言的形式进行单向推理，从而实现对LLMs预测的解释。然而，这种自然语言解释经常因为缺乏可信度而受到批评，因为这些解释可能不准确地反映LLMs的决策行为。在这项工作中，我们引入了一个生成解释框架xLLM，以提高LLMs自然语言格式的解释的可信度。具体而言，我们提出了一个评估器来量化自然语言解释的可信度，并通过xLLM的迭代优化过程来提高可信度，目标是最大程度地提高可信度。

    Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
    
[^82]: 带风险最小化的分组分布鲁棒数据集蒸馏

    Group Distributionally Robust Dataset Distillation with Risk Minimization

    [https://arxiv.org/abs/2402.04676](https://arxiv.org/abs/2402.04676)

    这项研究关注数据集蒸馏与其泛化能力的关系，尤其是在面对不常见的子组的样本时，如何确保模型在合成数据集上的训练可以表现良好。

    

    数据集蒸馏（DD）已成为一种广泛采用的技术，用于构建一个合成数据集，该数据集在捕捉训练数据集的基本信息方面起到重要作用，从而方便准确训练神经模型。其应用涵盖了转移学习、联邦学习和神经架构搜索等各个领域。构建合成数据的最流行方法依赖于使模型在合成数据集和训练数据集上的收敛性能相匹配。然而，目标是将训练数据集视为辅助，就像训练集是人口分布的近似替代品一样，而后者才是我们感兴趣的数据。尽管其受欢迎程度很高，但尚未探索的一个方面是DD与其泛化能力的关系，特别是跨不常见的子组。也就是说，当面对来自罕见子组的样本时，我们如何确保在合成数据集上训练的模型表现良好。

    Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from re
    
[^83]: 个体化治疗效果从时间序列健康数据的视角

    A Perspective on Individualized Treatment Effects Estimation from Time-series Health Data

    [https://arxiv.org/abs/2402.04668](https://arxiv.org/abs/2402.04668)

    这项研究提供了个体化治疗效果从时间序列健康数据的视角，并概述了相关文献中的最新工作，为进一步研究提供了见解。

    

    全球疾病负担不断增加，针对在临床试验中人数不足的患者群体，治疗效果不平等。然而，医疗以医疗治疗的平均人群效应为驱动力，因此采取了“一刀切”的方法，不一定适合每个患者。这些事实表明，急需方法学研究个体化治疗效果（ITE）以推动个人化治疗。尽管机器学习驱动的ITE估计模型引起了越来越大的兴趣，但绝大多数仅关注具有有限回顾和理解的表格数据，针对时间序列电子病历（EHR）提出的方法尚未深入研究。为此，本研究概述了针对时间序列数据的ITE研究工作，并针对未来的研究提出了见解。该工作总结了文献中的最新工作，并从理论假设、治疗设置类型和计算框架的角度进行了评述。

    The burden of diseases is rising worldwide, with unequal treatment efficacy for patient populations that are underrepresented in clinical trials. Healthcare, however, is driven by the average population effect of medical treatments and, therefore, operates in a "one-size-fits-all" approach, not necessarily what best fits each patient. These facts suggest a pressing need for methodologies to study individualized treatment effects (ITE) to drive personalized treatment. Despite the increased interest in machine-learning-driven ITE estimation models, the vast majority focus on tabular data with limited review and understanding of methodologies proposed for time-series electronic health records (EHRs). To this end, this work provides an overview of ITE works for time-series data and insights into future research. The work summarizes the latest work in the literature and reviews it in light of theoretical assumptions, types of treatment settings, and computational frameworks. Furthermore, th
    
[^84]: 通过艺术设计提高对抗性鲁棒性

    Adversarial Robustness Through Artifact Design

    [https://arxiv.org/abs/2402.04660](https://arxiv.org/abs/2402.04660)

    该研究提出了一种通过艺术设计实现对抗性鲁棒性的方法，通过微小更改现有规范来抵御对抗性示例的影响。

    

    对抗性示例的出现给机器学习带来了挑战。为了阻碍对抗性示例，大多数防御方法都改变了模型的训练方式（如对抗性训练）或推理过程（如随机平滑）。尽管这些方法显著提高了模型的对抗性鲁棒性，但模型仍然极易受到对抗性示例的影响。在某些领域如交通标志识别中，我们发现对象是按照规范来设计（如标志规范）。为了改善对抗性鲁棒性，我们提出了一种新颖的方法。具体来说，我们提供了一种重新定义规范的方法，对现有规范进行微小的更改，以防御对抗性示例。我们将艺术设计问题建模为一个鲁棒优化问题，并提出了基于梯度和贪婪搜索的方法来解决它。我们在交通标志识别领域对我们的方法进行了评估，使其能够改变交通标志中的象形图标（即标志内的符号）。

    Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models' adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) a
    
[^85]: 视觉语言模型的开放词汇校准

    Open-Vocabulary Calibration for Vision-Language Models

    [https://arxiv.org/abs/2402.04655](https://arxiv.org/abs/2402.04655)

    本文研究了视觉语言模型中的开放词汇校准问题，在提示学习的背景下发现现有的校准方法不足以解决该问题。为此，提出了一种称为 Distance-Aware Ca 的简单而有效的方法来解决问题。

    

    视觉语言模型 (VLM) 已经成为强大的工具，在处理图像识别、文本驱动的视觉内容生成、视觉聊天机器人等各种开放词汇任务上展现出了强大的能力。近年来，人们在提高 VLM 下游性能的适应方法上投入了大量的努力和资源，尤其是在参数高效的微调方法（如提示学习）上。然而，一个被大大忽视的关键问题是在微调的 VLM 中的置信度校准问题，在实际部署这样的模型时会大大降低可靠性。本文通过系统地研究提示学习背景下的置信度校准问题，发现现有的校准方法不能解决这个问题，尤其是在开放词汇的设置中。为了解决这个问题，我们提出了一种简单而有效的方法，称为 "Distance-Aware Ca"

    Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Ca
    
[^86]: 一种用于反问题的过完备深度学习方法

    An Over Complete Deep Learning Method for Inverse Problems

    [https://arxiv.org/abs/2402.04653](https://arxiv.org/abs/2402.04653)

    该论文提出了一种过完备深度学习方法，用于解决反问题。通过将解决方案嵌入到更高维度中，同时设计和学习嵌入和正则化器，克服了传统方法的不足，并在多个典型反问题上证明了其优点。

    

    在科学和工程中，获取反问题的有意义解决方案是一个主要挑战。基于近端和扩散方法的最近的机器学习技术已经显示出有希望的结果。然而，正如我们在这项工作中所展示的，当应用于一些典型问题时，它们也面临挑战。我们证明，类似于过完备字典的先前作品，通过将解决方案嵌入到更高的维度中，可以克服这些不足。所提出的工作的创新之处在于我们共同设计并学习嵌入和嵌入向量的正则化器。我们在几个典型和常见的反问题上展示了这种方法的优点。

    Obtaining meaningful solutions for inverse problems has been a major challenge with many applications in science and engineering. Recent machine learning techniques based on proximal and diffusion-based methods have shown promising results. However, as we show in this work, they can also face challenges when applied to some exemplary problems. We show that similar to previous works on over-complete dictionaries, it is possible to overcome these shortcomings by embedding the solution into higher dimensions. The novelty of the work proposed is that we jointly design and learn the embedding and the regularizer for the embedding vector. We demonstrate the merit of this approach on several exemplary and common inverse problems.
    
[^87]: 潜在计划变换器：规划作为潜在变量推断

    Latent Plan Transformer: Planning as Latent Variable Inference

    [https://arxiv.org/abs/2402.04647](https://arxiv.org/abs/2402.04647)

    潜在计划变换器（LPT）是一种新颖的模型，它通过将Transformer-based轨迹生成器和最终回报连接起来，并利用潜在空间进行规划。在学习中，通过对潜在变量的后验采样形成一致的抽象，在测试时通过推断潜在变量指导自回归策略。实验证明LPT能够从次优解中发现改进的决策。

    

    在追求长期回报的任务中，规划变得必要。我们研究了利用离线强化学习的数据集进行规划的生成建模。具体来说，我们确定了在缺乏逐步奖励的情况下的时间一致性是一个关键的技术挑战。我们引入了潜在计划变换器（LPT），这是一种新颖的模型，它利用了一个潜在空间来连接基于Transformer的轨迹生成器和最终回报。LPT可以通过轨迹-回报对的最大似然估计来学习。在学习中，通过对潜在变量的后验采样，尽管有限的上下文，自然地聚集子轨迹以形成一致的抽象。在测试时，通过预期回报对潜在变量进行推断，实现了规划作为推断的思想。然后，它在整个回合中指导自回归策略，起到一个计划的作用。我们的实验表明，LPT可以从次优解中发现改进的决策。

    In tasks aiming for long-term returns, planning becomes necessary. We study generative modeling for planning with datasets repurposed from offline reinforcement learning. Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent space to connect a Transformer-based trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form a consistent abstraction despite the finite context. During test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference. It then guides the autoregressive policy throughout the episode, functioning as a plan. Our experiments demonstrate that LPT can discover improved decisions from suboptima
    
[^88]: 学习来自块稀疏信号的多样化

    Learning with Diversification from Block Sparse Signal

    [https://arxiv.org/abs/2402.04646](https://arxiv.org/abs/2402.04646)

    本文提出了一种新的先验，称为多样化块稀疏先验，用来描述真实世界数据中的广泛块稀疏现象。通过允许方差和相关矩阵的多样化，解决了现有块稀疏学习方法对预定义块信息的敏感性问题，并提出了一种多样化的块稀疏贝叶斯学习方法(DivSBL)，实现自适应的块估计，减轻过拟合的风险，并建立了全局和局部最优性理论。实验结果证明了DivSBL相对于现有算法的优势。

    

    本文引入了一种新颖的先验Diversified Block Sparse Prior，来描述真实世界数据中的广泛块稀疏现象。通过允许方差和相关矩阵的多样化，我们有效地解决了现有块稀疏学习方法对预定义块信息的敏感性问题，从而实现自适应的块估计，同时减轻过拟合的风险。基于此，我们提出了一种多样化的块稀疏贝叶斯学习方法(DivSBL)，利用EM算法和对偶上升法进行超参数估计。此外，我们建立了我们模型的全局和局部最优性理论。实验证明了DivSBL相对于现有算法的优势。

    This paper introduces a novel prior called Diversified Block Sparse Prior to characterize the widespread block sparsity phenomenon in real-world data. By allowing diversification on variance and correlation matrix, we effectively address the sensitivity issue of existing block sparse learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overfitting. Based on this, a diversified block sparse Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual ascent method for hyperparameter estimation. Moreover, we establish the global and local optimality theory of our model. Experiments validate the advantages of DivSBL over existing algorithms.
    
[^89]: LEVI:通过以层为单位的多视角集成实现可泛化的微调

    LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views

    [https://arxiv.org/abs/2402.04644](https://arxiv.org/abs/2402.04644)

    本论文中，我们提出了一种名为LEVI的方法，通过以层为单位的多视角集成，实现了对预训练和微调数据中问题的解决，并提升微调模型对未见过的分布的泛化能力。

    

    微调越来越广泛地用于在新的下游任务中利用预训练基础模型的能力。虽然在各种任务上微调取得了许多成功，但最近的研究观察到微调模型在未见过的分布（即，超出分布；OOD）上的泛化存在挑战。为了改善OOB泛化，一些先前的研究确定了微调数据的限制，并调整微调以保留自预训练数据学习到的通用表示。然而，预训练数据和模型中的潜在限制经常被忽视。在本文中，我们认为过度依赖预训练表示可能会阻碍微调学习下游任务的重要表示，从而影响其OOB泛化。当新任务来自于与预训练数据不同的（子）领域时，这可能尤为灾难性。为了解决预训练和微调数据中的问题，我们提出了一种新颖的方法。

    Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks. While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and fine-tuning data, we propose a nove
    
[^90]: 域桥梁：基于生成模型的黑箱模型领域取证

    Domain Bridge: Generative model-based domain forensic for black-box models

    [https://arxiv.org/abs/2402.04640](https://arxiv.org/abs/2402.04640)

    本论文提出了一个基于生成模型的方法，通过迭代改进的方式确定模型的数据领域和具体属性。该方法使用图像嵌入模型和生成模型，能够在识别丰富细粒度类别方面更加精准。

    

    在机器学习模型的取证调查中，确定模型的数据领域的技术起着至关重要的作用，之前的工作依赖于大规模的语料库，如ImageNet来近似目标模型的领域。虽然这些方法在找到广泛的领域方面是有效的，但它们在识别领域内更具细粒度的类别时常常遇到困难。在本文中，我们介绍了一种增强的方法，不仅确定通用的数据领域（例如人脸），而且确定了其具体的属性（例如佩戴眼镜）。我们的方法使用图像嵌入模型作为编码器，生成模型作为解码器。从粗略的描述开始，解码器生成一组图像，然后将其呈现给未知的目标模型。模型成功分类将引导编码器修正描述，然后用于生成下一次迭代中更具体的图像集合。这种迭代的改进缩小了领域的范围。

    In forensic investigations of machine learning models, techniques that determine a model's data domain play an essential role, with prior work relying on large-scale corpora like ImageNet to approximate the target model's domain. Although such methods are effective in finding broad domains, they often struggle in identifying finer-grained classes within those domains. In this paper, we introduce an enhanced approach to determine not just the general data domain (e.g., human face) but also its specific attributes (e.g., wearing glasses). Our approach uses an image embedding model as the encoder and a generative model as the decoder. Beginning with a coarse-grained description, the decoder generates a set of images, which are then presented to the unknown target model. Successful classifications by the model guide the encoder to refine the description, which in turn, are used to produce a more specific set of images in the subsequent iteration. This iterative refinement narrows down the 
    
[^91]: 图拓扑结构上的特征分布调节了图卷积的效果：同质性视角

    Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective

    [https://arxiv.org/abs/2402.04621](https://arxiv.org/abs/2402.04621)

    A-X依赖关系是影响图卷积效果的重要因素，特征重排可以显著提升图神经网络的性能。

    

    随机重排同一类别节点之间的特征向量如何影响图神经网络（GNNs）？直观地说，特征重排扰乱了GNNs从图拓扑和特征之间的依赖关系（A-X依赖关系），从而影响了GNNs的学习。令人惊讶的是，在特征重排之后，我们观察到GNNs的性能显著提升。由于忽视了A-X依赖关系对GNNs的影响，先前的文献没有给出对该现象的满意解释。因此，我们提出了两个研究问题。首先，如何在控制潜在混淆因素的情况下度量A-X依赖关系？其次，A-X依赖关系如何影响GNNs？作为回应，我们（i）提出了一种基于原则的度量A-X依赖关系的方法，（ii）设计了一个控制A-X依赖关系的随机图模型，（iii）建立了A-X依赖关系与图卷积之间关系的理论，以及（iv）对实际图进行了与理论一致的实证分析。我们认为A-X依赖关系对GNNs具有重要影响。

    How would randomly shuffling feature vectors among nodes from the same class affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs the dependence between graph topology and features (A-X dependence) for GNNs to learn from. Surprisingly, we observe a consistent and significant improvement in GNN performance following the feature shuffle. Having overlooked the impact of A-X dependence on GNNs, the prior literature does not provide a satisfactory understanding of the phenomenon. Thus, we raise two research questions. First, how should A-X dependence be measured, while controlling for potential confounds? Second, how does A-X dependence affect GNNs? In response, we (i) propose a principled measure for A-X dependence, (ii) design a random graph model that controls A-X dependence, (iii) establish a theory on how A-X dependence relates to graph convolution, and (iv) present empirical analysis on real-world graphs that aligns with the theory. We conclude that A-X depe
    
[^92]: CataractBot：一种基于LLM的白内障患者专家辅助聊天机器人

    CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients

    [https://arxiv.org/abs/2402.04620](https://arxiv.org/abs/2402.04620)

    CataractBot是一种基于LLM的白内障患者专家辅助聊天机器人，通过查询知识库提供即时的答案和专家验证的回复。在实地部署研究中证明了其价值所在。

    

    随着医疗行业的发展，患者越来越追求更可靠的健康信息，包括他们的健康状况、治疗选择和潜在风险。虽然有很多信息来源，但数字时代却给人们带来了过多且错误的信息。患者主要信任医生和医院工作人员，突显了专家认可的健康信息的必要性。但是，专家面临的压力导致了沟通时间的减少，影响了信息的共享。为了填补这一空白，我们提出了CataractBot，一种由大型语言模型（LLMs）驱动的专家辅助聊天机器人。与印度一家三级眼科医院合作开发的CataractBot通过查询策划的知识库，即时回答白内障手术相关的问题，并异步提供专家验证的答复。CataractBot具备多模式支持和多语言能力。在与49名参与者的实地部署研究中，CataractBot证明了其价值所在。

    The healthcare landscape is evolving, with patients seeking more reliable information about their health conditions, treatment options, and potential risks. Despite the abundance of information sources, the digital age overwhelms individuals with excess, often inaccurate information. Patients primarily trust doctors and hospital staff, highlighting the need for expert-endorsed health information. However, the pressure on experts has led to reduced communication time, impacting information sharing. To address this gap, we propose CataractBot, an experts-in-the-loop chatbot powered by large language models (LLMs). Developed in collaboration with a tertiary eye hospital in India, CataractBot answers cataract surgery related questions instantly by querying a curated knowledge base, and provides expert-verified responses asynchronously. CataractBot features multimodal support and multilingual capabilities. In an in-the-wild deployment study with 49 participants, CataractBot proved valuable,
    
[^93]: InfLLM: 揭示LLMs对于处理超长序列的内在能力，无需训练的记忆

    InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory

    [https://arxiv.org/abs/2402.04617](https://arxiv.org/abs/2402.04617)

    InfLLM是一种无需训练的基于记忆的方法，用于揭示LLMs处理超长序列的内在能力。它通过存储远距离上下文和高效的注意计算机制，允许LLMs有效处理具有流式输入的长序列。

    

    大型语言模型（LLMs）已成为处理具有漫长传输输入的现实应用的基石，如LLM驱动代理。然而，现有的在受限最大长度序列上预训练的LLMs无法推广到更长的序列，因为存在领域外和分散注意力的问题。为了缓解这些问题，现有的工作采用滑动注意力窗口和丢弃远距离标记，以处理超长序列。不幸的是，这些方法无法捕获序列内的长距离依赖关系，以深入理解语义。本文介绍了一种无需训练的基于记忆的方法InfLLM，来揭示LLMs处理流式长序列的内在能力。具体而言，InfLLM将远距离的上下文存储到附加的内存单元中，并使用高效的机制来查找与注意计算相关的标记单元。因此，InfLLM允许LLMs高效处理长序列，同时保持了对语义的深入理解。

    Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining
    
[^94]: TinyLLM: 从多个大型语言模型学习一个小型学生模型

    TinyLLM: Learning a Small Student from Multiple Large Language Models

    [https://arxiv.org/abs/2402.04616](https://arxiv.org/abs/2402.04616)

    TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。

    

    将更强大的大型语言模型（LLMs）的推理能力转移到较小的模型上具有吸引力，因为较小的LLMs更灵活，成本更低。在现有的解决方案中，知识蒸馏因其出色的效率和泛化能力而脱颖而出。然而，现有方法存在一些缺点，包括知识多样性有限和缺乏丰富的上下文信息。为了解决这些问题并促进紧凑语言模型的学习，我们提出了TinyLLM，一种从多个大型教师LLMs中学习小型学生LLM的新型知识蒸馏范式。特别地，我们鼓励学生LLM不仅生成正确答案，而且理解这些答案背后的原理。鉴于不同的LLMs具有不同的推理能力，我们引导学生模型吸收来自多个教师LLMs的知识。我们进一步引入了一个上下文示例生成器和一个老师强制模块...

    Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
    
[^95]: 在再生核希尔伯特空间中的Moreau包络的f-差异的Wasserstein梯度流

    Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in Reproducing Kernel Hilbert Spaces

    [https://arxiv.org/abs/2402.04613](https://arxiv.org/abs/2402.04613)

    本文研究了在再生核希尔伯特空间中使用Moreau包络来对测度f-差异进行正则化的方法，并利用该方法分析了Wasserstein梯度流。

    

    大多数常用的测度f-差异，例如Kullback-Leibler差异，对于所涉及的测度的支持存在限制。解决办法是通过与特征核K相关的平方最大均值差异(MMD)对f-差异进行正则化。在本文中，我们使用所谓的核均值嵌入来显示相应的正则化可以重写为与K相关的再生核希尔伯特空间中某些函数的Moreau包络。然后，我们利用关于希尔伯特空间中Moreau包络的众所周知的结果来证明MMD正则化的f-差异及其梯度的属性。随后，我们使用我们的研究结果来分析受MMD正则化的f-差异的Wasserstein梯度流。最后，我们考虑从经验测度开始的Wasserstein梯度流，并提供使用Tsallis-$\alpha$差异的概念性数值示例的证明。

    Most commonly used $f$-divergences of measures, e.g., the Kullback-Leibler divergence, are subject to limitations regarding the support of the involved measures. A remedy consists of regularizing the $f$-divergence by a squared maximum mean discrepancy (MMD) associated with a characteristic kernel $K$. In this paper, we use the so-called kernel mean embedding to show that the corresponding regularization can be rewritten as the Moreau envelope of some function in the reproducing kernel Hilbert space associated with $K$. Then, we exploit well-known results on Moreau envelopes in Hilbert spaces to prove properties of the MMD-regularized $f$-divergences and, in particular, their gradients. Subsequently, we use our findings to analyze Wasserstein gradient flows of MMD-regularized $f$-divergences. Finally, we consider Wasserstein gradient flows starting from empirical measures and provide proof-of-the-concept numerical examples with Tsallis-$\alpha$ divergences.
    
[^96]: 见 JEANIE：通过时间-视角对齐的三维骨架序列相似度测量

    Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment

    [https://arxiv.org/abs/2402.04599](https://arxiv.org/abs/2402.04599)

    JEANIE是一种通过时间-视角对齐的方法，用于测量三维骨架序列的相似度。它能够解决视频序列中速度、时间位置和姿势的干扰变化问题。在评估了骨架Few-shot动作识别任务后，JEANIE在支持-查询序列对的时间块匹配方面表现出了良好的效果。

    

    视频序列表现出显著的干扰性变化，包括动作速度、时间位置和主体姿势，导致在比较两组帧或评估两个序列的相似度时产生时间-视角不匹配的问题。因此，我们提出了一种用于序列对比的联合时间和摄像机视角对齐方法（JEANIE）。我们特别关注能够在三维中轻松操作摄像机和主体姿势的三维骨架序列。我们在骨架Few-shot动作识别（FSAR）上评估了JEANIE，其中由于新类别样本有限，通过匹配好支持-查询序列对的时间块（组成序列的时间块）来排除干扰变化是至关重要的。针对查询序列，我们通过模拟多个摄像机位置创建多个视角。对于支持序列，我们将其与模拟出的查询序列进行匹配，类似于流行的动态时间规整（DTW）。具体而言，每个支持时间块可以与视角模拟的查询序列匹配，如DTW。

    Video sequences exhibit significant nuisance variations (undesired effects) of speed of actions, temporal locations, and subjects' poses, leading to temporal-viewpoint misalignment when comparing two sets of frames or evaluating the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D skeleton sequences whose camera and subjects' poses can be easily manipulated in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where matching well temporal blocks (temporal chunks that make up a sequence) of support-query sequence pairs (by factoring out nuisance variations) is essential due to limited samples of novel classes. Given a query sequence, we create its several views by simulating several camera locations. For a support sequence, we match it with view-simulated query sequences, as in the popular Dynamic Time Warping (DTW). Specifically, each support temporal block can be m
    
[^97]: 在具有双输出脉冲架构（DOSA）的连续多标签学习中改进不平衡鲁棒性

    Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)

    [https://arxiv.org/abs/2402.04596](https://arxiv.org/abs/2402.04596)

    本研究提出了一个双输出脉冲架构（DOSA）来解决连续多标签学习中的不平衡鲁棒性问题，并提出了一种新的不平衡感知的损失函数来提高多标记分类性能。

    

    设计用于解决典型监督分类问题的算法只能从固定的样本和标签集中学习，这使它们不适用于实际世界中数据以样本流的形式到达，并且往往随时间关联着多个标签。这促使研究与任务无关的连续多标签学习问题。虽然最近的文献中提出了使用深度学习方法进行连续多标签学习的算法，但它们往往具有较大的计算量。虽然脉冲神经网络（SNNs）提供了一种计算效率高的人工神经网络替代方案，但现有文献尚未将其用于连续多标签学习。此外，准确地确定SNNs的多个标签仍然是一个开放的研究问题。本研究提出了一种双输出脉冲架构（DOSA）来弥合这些研究差距。还提出了一种新的不平衡感知的损失函数，从而提高了多标记分类性能。

    Algorithms designed for addressing typical supervised classification problems can only learn from a fixed set of samples and labels, making them unsuitable for the real world, where data arrives as a stream of samples often associated with multiple labels over time. This motivates the study of task-agnostic continual multi-label learning problems. While algorithms using deep learning approaches for continual multi-label learning have been proposed in the recent literature, they tend to be computationally heavy. Although spiking neural networks (SNNs) offer a computationally efficient alternative to artificial neural networks, existing literature has not used SNNs for continual multi-label learning. Also, accurately determining multiple labels with SNNs is still an open research problem. This work proposes a dual output spiking architecture (DOSA) to bridge these research gaps. A novel imbalance-aware loss function is also proposed, improving the multi-label classification performance o
    
[^98]: 机器人跨领域策略转移综合调查

    A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents

    [https://arxiv.org/abs/2402.04580](https://arxiv.org/abs/2402.04580)

    这篇论文综述了机器人跨领域策略转移方法，讨论了从目标领域采集无偏数据的挑战，以及从源领域获取数据的成本效益性。同时，总结了不同问题设置下的设计考虑和方法。

    

    机器学习和具身人工智能领域的蓬勃发展引发了对大量数据的需求增加。然而，由于昂贵的数据收集过程和严格的安全要求，从目标领域收集足够的无偏数据仍然是一个挑战。因此，研究人员经常采用易于获取的源领域数据（例如模拟和实验室环境），以实现成本效益的数据获取和快速模型迭代。然而，这些源领域的环境和具身方式可能与目标领域的特征相差很大，强调了有效的跨领域策略转移方法的需求。本文对现有的跨领域策略转移方法进行了系统综述。通过对领域差距的精细分类，我们总结了每个问题设置的总体见解和设计考虑。我们还就使用的关键方法进行了高层次讨论

    The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data. However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements. Consequently, researchers often resort to data from easily accessible source domains, such as simulation and laboratory environments, for cost-effective data acquisition and rapid model iteration. Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches. In this paper, we conduct a systematic review of existing cross-domain policy transfer methods. Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting. We also provide a high-level discussion about the key methodologies used
    
[^99]: 通过最优传输实现集体反事实解释

    Collective Counterfactual Explanations via Optimal Transport

    [https://arxiv.org/abs/2402.04579](https://arxiv.org/abs/2402.04579)

    本论文提出了一种集体方法来形成反事实解释，通过利用个体的当前密度来指导推荐的行动，解决了个体为中心的方法可能导致的新的竞争和意想不到的成本问题，并改进了经典反事实解释的期望。

    

    反事实解释提供个体的成本最优行动，以改变标签为所需的类别。然而，如果大量实例寻求状态修改，这种个体为中心的方法可能导致新的竞争和意想不到的成本。此外，这些推荐忽视了基础数据分布，可能会建议用户认为是异常值的行动。为了解决这些问题，我们的工作提出了一种集体方法来形成反事实解释，重点是利用个体的当前密度来指导推荐的行动。我们的问题自然地转化为一个最优传输问题。借鉴最优传输的广泛文献，我们说明了这种集体方法如何改进经典反事实解释的期望。我们通过数值模拟支持我们的提议，展示了所提方法的有效性以及与经典方法的关系。

    Counterfactual explanations provide individuals with cost-optimal actions that can alter their labels to desired classes. However, if substantial instances seek state modification, such individual-centric methods can lead to new competitions and unanticipated costs. Furthermore, these recommendations, disregarding the underlying data distribution, may suggest actions that users perceive as outliers. To address these issues, our work proposes a collective approach for formulating counterfactual explanations, with an emphasis on utilizing the current density of the individuals to inform the recommended actions. Our problem naturally casts as an optimal transport problem. Leveraging the extensive literature on optimal transport, we illustrate how this collective method improves upon the desiderata of classical counterfactual explanations. We support our proposal with numerical simulations, illustrating the effectiveness of the proposed approach and its relation to classic methods.
    
[^100]: OIL-AD: 一种用于顺序决策序列的异常检测框架

    OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences

    [https://arxiv.org/abs/2402.04567](https://arxiv.org/abs/2402.04567)

    本论文提出了一种用于顺序决策序列的异常检测框架，通过提取行为特征来检测异常。这种方法克服了强化学习方法在真实世界中应用困难的问题，并提供了关于异常的足够信息。

    

    决策序列中的异常检测是一个具有挑战性的问题，因为正常性表示学习的复杂性和任务的顺序性质。大部分基于强化学习（RL）的现有方法由于对环境动态、奖励信号和与环境的在线交互等不切实际的假设，在现实世界中难以实施。为了解决这些限制，我们提出了一种名为离线模仿学习异常检测（OIL-AD）的无监督方法，它使用两个提取的行为特征（动作优化和顺序关联）来检测决策序列中的异常。我们的离线学习模型是基于变压器策略网络的行为克隆的适应，我们修改了训练过程，从正常轨迹中学习Q函数和状态值函数。我们认为，Q函数和状态值函数可以提供关于异常的足够信息。

    Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning and the sequential nature of the task. Most existing methods based on Reinforcement Learning (RL) are difficult to implement in the real world due to unrealistic assumptions, such as having access to environment dynamics, reward signals, and online interactions with the environment. To address these limitations, we propose an unsupervised method named Offline Imitation Learning based Anomaly Detection (OIL-AD), which detects anomalies in decision-making sequences using two extracted behaviour features: action optimality and sequential association. Our offline learning model is an adaptation of behavioural cloning with a transformer policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories. We propose that the Q function and the state value function can provide sufficient information about 
    
[^101]: 一个用于催化剂设计和优化的人工智能工作流程

    An Artificial Intelligence (AI) workflow for catalyst design and optimization

    [https://arxiv.org/abs/2402.04557](https://arxiv.org/abs/2402.04557)

    该论文提出了一个创新的人工智能工作流程，通过结合大型语言模型、贝叶斯优化和主动学习循环，将催化剂合成领域的科学文献中的知识转化为可行的参数，以加快和增强催化剂的优化。

    

    在解决紧迫的环境问题和能源需求的过程中，常规的催化剂设计和优化方法往往因催化剂参数空间的复杂性和广泛性而不足。机器学习的出现为催化剂优化领域带来了新时代，提供了解决传统技术缺陷的潜在解决方案。然而，现有方法未能有效利用日益增长的催化剂合成科学文献中蕴含的丰富信息。为了弥补这一差距，本研究提出了一种创新的人工智能工作流程，将大型语言模型、贝叶斯优化和主动学习循环集成在一起，以加快和增强催化剂优化。我们的方法将先进的语言理解与强大的优化策略相结合，将从不同文献中提取的知识有效地转化为可操作的参数。

    In the pursuit of novel catalyst development to address pressing environmental concerns and energy demand, conventional design and optimization methods often fall short due to the complexity and vastness of the catalyst parameter space. The advent of Machine Learning (ML) has ushered in a new era in the field of catalyst optimization, offering potential solutions to the shortcomings of traditional techniques. However, existing methods fail to effectively harness the wealth of information contained within the burgeoning body of scientific literature on catalyst synthesis. To address this gap, this study proposes an innovative Artificial Intelligence (AI) workflow that integrates Large Language Models (LLMs), Bayesian optimization, and an active learning loop to expedite and enhance catalyst optimization. Our methodology combines advanced language understanding with robust optimization strategies, effectively translating knowledge extracted from diverse literature into actionable paramet
    
[^102]: 通过通用李群预处理器的曲率信息优化SGD算法

    Curvature-Informed SGD via General Purpose Lie-Group Preconditioners

    [https://arxiv.org/abs/2402.04553](https://arxiv.org/abs/2402.04553)

    本研究提出了一种通过利用曲率信息优化随机梯度下降（SGD）算法的新方法。该方法使用无矩阵预处理器和低秩近似预处理器，并在线更新这两种预处理器。通过对称性和不变性的约束，该方法消除了阻尼的需求，使得学习率和步长可以自然归一化，并且在大多数情况下默认值效果良好。

    

    我们提出了一种新的方法来加速随机梯度下降（SGD）算法，该方法利用从黑塞矩阵-向量乘积或参数和梯度的有限差分中获得的曲率信息，类似于BFGS算法。我们的方法涉及两个预处理器：一个无矩阵预处理器和一个低秩近似预处理器。我们使用一种对随机梯度噪声具有鲁棒性且不需要线性搜索或阻尼的准则在线更新两个预处理器。为了保持相应的对称性或不变性，我们的预处理器受限于特定的连通李群。李群的等变性质简化了预处理器的拟合过程，而它的不变性质消除了在二阶优化器中普遍需要的阻尼，从而使参数更新的学习率和预处理器拟合的步长自然地被归一化，并且它们的默认值在大多数情况下效果良好。

    We present a novel approach to accelerate stochastic gradient descent (SGD) by utilizing curvature information obtained from Hessian-vector products or finite differences of parameters and gradients, similar to the BFGS algorithm. Our approach involves two preconditioners: a matrix-free preconditioner and a low-rank approximation preconditioner. We update both preconditioners online using a criterion that is robust to stochastic gradient noise and does not require line search or damping. To preserve the corresponding symmetry or invariance, our preconditioners are constrained to certain connected Lie groups. The Lie group's equivariance property simplifies the preconditioner fitting process, while its invariance property eliminates the need for damping, which is commonly required in second-order optimizers. As a result, the learning rate for parameter updating and the step size for preconditioner fitting are naturally normalized, and their default values work well in most scenarios. Ou
    
[^103]: Riemann-Lebesgue Forest回归方法的研究

    Riemann-Lebesgue Forest for Regression

    [https://arxiv.org/abs/2402.04550](https://arxiv.org/abs/2402.04550)

    提出了一种新颖的集成方法Riemann-Lebesgue Forest (RLF)用于回归问题，通过划分函数的值域为多个区间来逼近可测函数的思想，开发了一种新的树学习算法Riemann-Lebesgue Tree。通过Hoeffding分解和Stein方法推导了RLF在不同参数设置下的渐近性能，并在仿真数据和真实世界数据集上的实验中证明了RLF与原始随机森林相比具有竞争力的性能。

    

    我们提出了一种新颖的用于回归问题的集成方法，称为Riemann-Lebesgue Forest (RLF)。RLF的核心思想是通过将函数的值域划分为几个区间来模拟可测函数的逼近方式。基于这个思想，我们开发了一种新的树学习算法，称为Riemann-Lebesgue Tree，它在每个非叶节点上有机会从响应Y或特征空间X中的方向进行切割。我们通过Hoeffding分解和Stein方法来推导不同参数设置下RLF的渐近性能。当底层函数Y=f(X)遵循加法回归模型时，RLF与Scornet等人的论证（2014年）保持一致。通过在仿真数据和真实世界数据集上的实验证明，RLF与原始随机森林相比具有竞争力的性能。

    We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression. The core idea of RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which has a chance to split the node from response $Y$ or a direction in feature space $\mathbf{X}$ at each non-terminal node. We generalize the asymptotic performance of RLF under different parameter settings mainly through Hoeffding decomposition \cite{Vaart} and Stein's method \cite{Chen2010NormalAB}. When the underlying function $Y=f(\mathbf{X})$ follows an additive regression model, RLF is consistent with the argument from \cite{Scornet2014ConsistencyOR}. The competitive performance of RLF against original random forest \cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and real world datasets.
    
[^104]: 使用软件自生成的引导学习多样化策略

    Learning Diverse Policies with Soft Self-Generated Guidance

    [https://arxiv.org/abs/2402.04539](https://arxiv.org/abs/2402.04539)

    本文提出了一种使用多样化的过去轨迹作为引导的方法，以实现更快、更高效的在线强化学习，即使这些轨迹是次优的或没有高奖励。通过将过去轨迹视为引导，而不是模仿它们，本方法可以使策略跟随和扩展过去的轨迹同时仍保持

    

    强化学习中，稀疏和具有误导性的奖励使得学习变得困难，因为几乎很少能够获得非零奖励。因此，智能体计算的梯度可能是随机的且缺乏有效信息。最近的研究利用先前经验的内存缓冲区可以使学习过程更高效。然而，现有方法通常要求这些经验必须成功，并可能过度利用它们，这可能导致智能体采取次优的行为。本文提出了一种方法，即使用多样化的过去轨迹进行更快、更高效的在线强化学习，即使这些轨迹是次优的或没有高奖励。所提出的算法结合了策略改进步骤和使用离线演示数据的额外探索步骤。本文的主要贡献是，将多样化的过去轨迹视为引导而不是模仿它们，我们的方法使策略跟随和扩展过去的轨迹，同时仍保持

    Reinforcement learning (RL) with sparse and deceptive rewards is challenging because non-zero rewards are rarely obtained. Hence, the gradient calculated by the agent can be stochastic and without valid information. Recent studies that utilize memory buffers of previous experiences can lead to a more efficient learning process. However, existing methods often require these experiences to be successful and may overly exploit them, which can cause the agent to adopt suboptimal behaviors. This paper develops an approach that uses diverse past trajectories for faster and more efficient online RL, even if these trajectories are suboptimal or not highly rewarded. The proposed algorithm combines a policy improvement step with an additional exploration step using offline demonstration data. The main contribution of this paper is that by regarding diverse past trajectories as guidance, instead of imitating them, our method directs its policy to follow and expand past trajectories while still be
    
[^105]: 三元交互改进图转换器：通过三元图转换器实现准确的分子图学习

    Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers

    [https://arxiv.org/abs/2402.04538](https://arxiv.org/abs/2402.04538)

    本论文提出了一种新颖的三元图转换器（TGT），通过三元注意力和聚合机制实现了图中相邻对之间的直接通信。通过预测原子间距离并进行下游任务的分子属性预测，我们的模型在多个分子属性预测基准上达到了最新的最优结果，并且在旅行推销员问题上也取得了最新的最优结果。

    

    图转换器通常缺乏直接的对等通信，而是通过共同节点强制相邻对交换信息。我们提出了一种名为三元图转换器（TGT）的方法，它通过新颖的三元注意力和聚合机制，实现了图中两个相邻对之间的直接通信。TGT首先从2D图中预测原子间距离，然后将这些距离用于下游任务的分子属性预测。新颖的三阶段训练过程和随机推断进一步提高了训练效率和模型性能。我们的模型在开放挑战基准PCQM4Mv2和OC20 IS2RE上取得了最新的最优结果。通过迁移学习，我们还在QM9、MOLPCBA和LIT-PCBA分子属性预测基准上获得了最新的最优结果。我们还通过旅行推销员问题（TSP）的最新最优结果展示了TGT的普适性。

    Graph transformers typically lack direct pair-to-pair communication, instead forcing neighboring pairs to exchange information via a common node. We propose the Triplet Graph Transformer (TGT) that enables direct communication between two neighboring pairs in a graph via novel triplet attention and aggregation mechanisms. TGT is applied to molecular property prediction by first predicting interatomic distances from 2D graphs and then using these distances for downstream tasks. A novel three-stage training procedure and stochastic inference further improve training efficiency and model performance. Our model achieves new state-of-the-art (SOTA) results on open challenge benchmarks PCQM4Mv2 and OC20 IS2RE. We also obtain SOTA results on QM9, MOLPCBA, and LIT-PCBA molecular property prediction benchmarks via transfer learning. We also demonstrate the generality of TGT with SOTA results on the traveling salesman problem (TSP).
    
[^106]: 基于触觉的从颗粒介质中检索物体的研究

    Tactile-based Object Retrieval From Granular Media

    [https://arxiv.org/abs/2402.04536](https://arxiv.org/abs/2402.04536)

    这项研究介绍了一种基于触觉反馈的机器人操作方法，用于在颗粒介质中检索埋藏的物体。通过模拟传感器噪声进行端到端训练，实现了自然出现的学习推动行为，并成功将其迁移到实际硬件上。

    

    我们介绍了一种名为GEOTACT的机器人操作方法，能够在颗粒介质中检索埋藏的物体。这是一项具有挑战性的任务，因为需要与颗粒介质进行交互，并且仅依靠触觉反馈来完成，因为一个埋藏的物体可能完全被视觉隐藏。在这种环境中，触觉反馈本身具有挑战性，因为需要与周围介质进行普遍接触，并且由触觉读数引起的固有噪声水平。为了解决这些挑战，我们使用了一种通过模拟传感器噪声进行端到端训练的学习方法。我们展示了我们的问题表述导致了学习推动行为的自然出现，操作器使用这些行为来减少不确定性并将物体引导到稳定的抓取位置，尽管存在假的和噪声的触觉读数。我们还引入了一种培训方案，可以在仿真中学习这些行为，并在实际硬件上进行零样本迁移。据我们所知，GEOTACT是第一个这样的方法。

    We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is the first meth
    
[^107]: SumRec: 使用开放领域对话进行推荐的框架

    SumRec: A Framework for Recommendation using Open-Domain Dialogue

    [https://arxiv.org/abs/2402.04523](https://arxiv.org/abs/2402.04523)

    本研究提出了一个新的框架SumRec，用于从开放领域的对话中推荐个性化信息。该框架利用大型语言模型生成对话中说话者信息的摘要，并根据用户类型推荐物品信息，实验证明SumRec框架比基准方法提供更好的推荐。

    

    聊天对话包含着关于说话者兴趣、偏好和经验的大量有用信息。因此，可以利用开放领域聊天对话中的知识来个性化各种系统并为高级信息提供推荐。本研究提出了一种用于从开放领域聊天对话中推荐信息的新颖框架SumRec。该研究还使用ChatRec这个新构建的数据集来检验该框架的性能。为了提取说话者和物品的特征，SumRec框架使用大型语言模型（LLM）从对话中生成说话者信息的摘要，并根据用户类型推荐物品信息。然后将说话者和物品信息输入到评分估计模型中，生成推荐分数。实验结果表明，SumRec框架比使用对话和原始物品描述的基准方法提供更好的推荐。

    Chat dialogues contain considerable useful information about a speaker's interests, preferences, and experiences.Thus, knowledge from open-domain chat dialogue can be used to personalize various systems and offer recommendations for advanced information.This study proposed a novel framework SumRec for recommending information from open-domain chat dialogue.The study also examined the framework using ChatRec, a newly constructed dataset for training and evaluation. To extract the speaker and item characteristics, the SumRec framework employs a large language model (LLM) to generate a summary of the speaker information from a dialogue and to recommend information about an item according to the type of user.The speaker and item information are then input into a score estimation model, generating a recommendation score.Experimental results show that the SumRec framework provides better recommendations than the baseline method of using dialogues and item descriptions in their original form.
    
[^108]: 关于现代Hopfield模型计算限制的一个细粒度复杂性分析

    On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis

    [https://arxiv.org/abs/2402.04520](https://arxiv.org/abs/2402.04520)

    通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。

    

    我们从细粒度复杂性分析的角度研究了现代Hopfield模型的记忆检索动力学的计算限制。我们的主要贡献是基于模式的范数对所有可能的现代Hopfield模型的效率进行相变行为的刻画。具体来说，我们建立了对输入查询模式和记忆模式的范数的上界标准。仅在这个标准之下，假设满足Strong Exponential Time Hypothesis (SETH)，存在子二次（高效）变体的现代Hopfield模型。为了展示我们的理论，当有效标准成立时，我们提供了现代Hopfield模型使用低秩逼近的有效构造的正式示例。这包括一个计算时间的下界导出，与$\Max\{$存储的记忆模式数量，输入查询序列的长度$\}$线性缩放。此外，我们证明了记忆检索误差界和指数记忆容量。

    We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
    
[^109]: 图上概率测度的广义 Sobolev 传输

    Generalized Sobolev Transport for Probability Measures on a Graph

    [https://arxiv.org/abs/2402.04516](https://arxiv.org/abs/2402.04516)

    我们研究了支持在图度量空间上的测度的最优传输问题，提出了一种适用于不同几何结构的图上概率测度传输方法，并引入了超力 Wassestein（OW）的概念，为某些机器学习方法的发展带来了新的机遇。

    

    我们研究了支持在图度量空间上的测度的最优传输（OT）问题。最近，Le 等人（2022）利用图结构提出了一种 OT 的变体，称为 Sobolev 传输（ST），它提供了一种闭式表达式用于快速计算。然而，ST 的定义中实质上与 $L^p$ 几何结构耦合在一起，这使得在其他先验结构中利用 ST 变得非常困难。相反，经典的 OT 通过修改底层成本函数具有适应各种几何结构的灵活性。一个重要的例子是超力 Wassestein（OW），它通过利用\emph{Orlicz 几何结构}超越了 $L^p$ 结构。与使用标准 $p$-阶 Wassestein 相比，OW 显著提高了某些机器学习方法的性能。然而，由于其两层优化 formulation，OW 在其计算上带来了新的挑战。在这项工作中，我们利用了一类特定的凸函数。

    We study the optimal transport (OT) problem for measures supported on a graph metric space. Recently, Le et al. (2022) leverage the graph structure and propose a variant of OT, namely Sobolev transport (ST), which yields a closed-form expression for a fast computation. However, ST is essentially coupled with the $L^p$ geometric structure within its definition which makes it nontrivial to utilize ST for other prior structures. In contrast, the classic OT has the flexibility to adapt to various geometric structures by modifying the underlying cost function. An important instance is the Orlicz-Wasserstein (OW) which moves beyond the $L^p$ structure by leveraging the \emph{Orlicz geometric structure}. Comparing to the usage of standard $p$-order Wasserstein, OW remarkably helps to advance certain machine learning approaches. Nevertheless, OW brings up a new challenge on its computation due to its two-level optimization formulation. In this work, we leverage a specific class of convex funct
    
[^110]: 在流数据上进行高效推理的在线级联学习

    Online Cascade Learning for Efficient Inference over Streams

    [https://arxiv.org/abs/2402.04513](https://arxiv.org/abs/2402.04513)

    这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。

    

    大型语言模型 (LLM) 在回答关于数据流的复杂查询方面具有天然的优势，但是 LLM 推理的高计算成本使得它们在许多任务中不可行。我们提出了在线级联学习，这是首个解决这一挑战的方法。这里的目标是学习一个“级联”模型，从容量较低的模型（如逻辑回归器）开始，到强大的 LLM 结束，并配备一个决定在给定输入上使用哪个模型的推迟策略。我们将在线学习级联的任务公式化为一个模仿学习问题，并为该问题提供了无遗憾算法。在四个基准测试中的实验结果显示，我们的方法在准确性上与 LLM 相当，同时将推理成本削减了多达 90%，突显了它在流处理中的效能和适应能力。

    Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
    
[^111]: 带有动态过程不确定性的路径空间卡尔曼滤波器用于时间序列数据分析

    Pathspace Kalman Filters with Dynamic Process Uncertainty for Analyzing Time-course Data

    [https://arxiv.org/abs/2402.04498](https://arxiv.org/abs/2402.04498)

    本论文提出了一种名为路径空间卡尔曼滤波器（PKF）的扩展算法，可以动态跟踪数据和先前知识的不确定性，并用贝叶斯方法量化不同的不确定性来源。通过在合成数据集上进行数值实验，我们证明了PKF优于传统KF方法，并将该方法应用于生物时间序列数据集。

    

    卡尔曼滤波器（KF）是一种最优线性状态预测算法，广泛应用于工程学、经济学、机器人学和太空探索等领域。在本文中，我们发展了KF的扩展，称为路径空间卡尔曼滤波器（PKF），它允许我们动态跟踪与底层数据和先前知识相关的不确定性，并使用贝叶斯方法量化不同的不确定性来源。该算法的一个应用是自动检测内部机制模型与数据在时间上发生变化的时间窗口。首先，我们提供了描述PKF算法收敛性的定理。然后，我们通过数值实验证明PKF在合成数据集上的性能优于传统KF方法，平均均方误差降低了数个数量级。最后，我们将该方法应用于生物时间序列数据集。

    Kalman Filter (KF) is an optimal linear state prediction algorithm, with applications in fields as diverse as engineering, economics, robotics, and space exploration. Here, we develop an extension of the KF, called a Pathspace Kalman Filter (PKF) which allows us to a) dynamically track the uncertainties associated with the underlying data and prior knowledge, and b) take as input an entire trajectory and an underlying mechanistic model, and using a Bayesian methodology quantify the different sources of uncertainty. An application of this algorithm is to automatically detect temporal windows where the internal mechanistic model deviates from the data in a time-dependent manner. First, we provide theorems characterizing the convergence of the PKF algorithm. Then, we numerically demonstrate that the PKF outperforms conventional KF methods on a synthetic dataset lowering the mean-squared-error by several orders of magnitude. Finally, we apply this method to biological time-course dataset i
    
[^112]: 训练大型语言模型的梯度计算的细粒度复杂性

    The Fine-Grained Complexity of Gradient Computation for Training Large Language Models

    [https://arxiv.org/abs/2402.04497](https://arxiv.org/abs/2402.04497)

    本文研究了训练大型语言模型中梯度计算的复杂性，证明了在某些参数区域内可以以几乎线性时间进行前向计算，但在其余参数区域内需要超过二次时间，这对于LLM训练的每个步骤都具有重要意义。

    

    在过去几年中，大型语言模型（LLM）已经作出了基本贡献。要训练一个LLM，人们需要交替运行“前向计算”和“反向计算”。前向计算可以视为注意力函数的评估，而后向计算可以视为梯度计算。在之前的研究中，[Alman和Song，NeurIPS 2023]证明在某些参数区域中前向步骤可以在几乎线性时间内执行，但在其余参数区域内，除非流行的假设SETH不成立，否则没有真正的亚二次时间算法。在这项工作中，我们展示了对于计算单层注意力网络损失函数的梯度，以及LLM训练的整个过程中似乎更难的问题几乎完全相同的结果。这完全刻画了LLM训练每个步骤的细粒度复杂性。

    Large language models (LLMs) have made fundamental contributions over the last a few years. To train an LLM, one needs to alternatingly run `forward' computations and `backward' computations. The forward computation can be viewed as attention function evaluation, and the backward computation can be viewed as a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it was proved that the forward step can be performed in almost-linear time in certain parameter regimes, but that there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false. In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of LLM training. This completely characterizes the fine-grained complexity of every step of LLM training.
    
[^113]: 不需搜索即可实现大师级国际象棋对局

    Grandmaster-Level Chess Without Search

    [https://arxiv.org/abs/2402.04494](https://arxiv.org/abs/2402.04494)

    本研究通过在庞大的国际象棋数据集上进行训练，使用了一个270M参数的Transformer模型，不依赖于复杂的启发式算法或显式搜索，取得了大师级水平的国际象棋对局的成功。模型在Lichess闪电战评分上达到了2895，解决了一系列具有挑战性的国际象棋谜题，优于AlphaZero和GPT-3.5-turbo-instruct。通过系统研究，我们发现大规模的模型和数据集对于实现强大的国际象棋对局效果是至关重要的。

    

    最新的机器学习的突破性成功主要归功于规模化，即基于注意力的大规模架构和空前规模的数据集。本文研究了对国际象棋的大规模训练的影响。与传统的依赖复杂启发式算法、显式搜索或二者结合的国际象棋引擎不同，我们通过在1000万局国际象棋对局的数据集上使用监督学习训练了一个拥有2.7亿参数的Transformer模型。我们用强大的Stockfish 16引擎提供的动作值来注释数据集中的每个棋局，产生大约150亿个数据点。我们最大的模型在Lichess闪电战Elo上达到了2895，成功解决了一系列具有挑战性的国际象棋谜题，而无需任何特定领域的调整或显式搜索算法。我们还证明了我们的模型优于AlphaZero的策略和价值网络（无MCTS）以及GPT-3.5-turbo-instruct。对模型和数据集规模的系统研究表明，强大的国际象棋对局可以在规模上取得最佳效果。

    The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess 
    
[^114]: 离线约束强化学习中一种基本对偶算法在低秩MDPs上的应用

    A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs

    [https://arxiv.org/abs/2402.04493](https://arxiv.org/abs/2402.04493)

    这篇论文提出了一种在低秩MDPs上的离线约束强化学习算法，该算法通过部分数据覆盖假设实现了更高的计算效率并达到了$O(\epsilon^{-2})$的样本复杂度。此外，该算法还支持额外奖励信号的约束。

    

    离线强化学习旨在通过预先收集的数据集学习一种最大化期望累积奖励的策略。最近，对于低秩MDPs或一般函数逼近的离线强化学习进行了广泛研究，但是现有算法在找到$\epsilon$-优化策略的样本复杂度为$O(\epsilon^{-2})$时，要么需要均匀的数据覆盖假设，要么计算效率低下。在本文中，我们提出了一种在折扣无穷时段设置下，用于低秩MDPs的离线强化学习的基本对偶算法。我们的算法是在部分数据覆盖假设下，该设置中第一个样本复杂度达到$O(\epsilon^{-2})$的计算有效的算法。这优于最近的一项工作，其需要$O(\epsilon^{-4})$个样本。此外，我们的算法通过支持额外奖励信号的约束，将之前的工作扩展到离线约束强化学习设置中。

    Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward using a pre-collected dataset. Offline RL with low-rank MDPs or general function approximation has been widely studied recently, but existing algorithms with sample complexity $O(\epsilon^{-2})$ for finding an $\epsilon$-optimal policy either require a uniform data coverage assumptions or are computationally inefficient. In this paper, we propose a primal dual algorithm for offline RL with low-rank MDPs in the discounted infinite-horizon setting. Our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $O(\epsilon^{-2})$ with partial data coverage assumption. This improves upon a recent work that requires $O(\epsilon^{-4})$ samples. Moreover, our algorithm extends the previous work to the offline constrained RL setting by supporting constraints on additional reward signals.
    
[^115]: 通过差分隐私在语言模型微调中消除偏见放大

    De-amplifying Bias from Differential Privacy in Language Model Fine-tuning

    [https://arxiv.org/abs/2402.04489](https://arxiv.org/abs/2402.04489)

    本研究发现，在语言模型的微调过程中，差分隐私放大了性别、种族和宗教方面的偏见。我们通过对抗事实数据增强方法证明了DP可以减少偏见的放大。因此，我们可以使用DP和CDA来微调模型，同时保持可靠性和公平性。

    

    公平性和隐私是机器学习（ML）从业者经常在模型中追求的两个重要价值。公平性旨在减少对社会/人口亚组的模型偏见。然而，差分隐私（DP）机制通过限制任何个体的训练数据对结果模型的影响来保护隐私。可靠的ML的隐私和公平目标之间的权衡对于那些希望解决两者的人来说是一种挑战。我们发现DP在对大型语言模型（LLMs）进行微调时放大了性别、种族和宗教偏见，产生了比没有DP微调的模型更加偏见的模型。我们发现放大的原因是梯度在子组之间的收敛不平衡。通过二元性别偏见的案例，我们证明了对抗事实数据增强（CDA）也能通过DP减少偏见的放大。因此，DP和CDA可以一起用于微调模型，同时保持两者的可靠性。

    Fairness and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models. Fairness aims to reduce model bias for social/demographic sub-groups. Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model. The trade-offs between privacy and fairness goals of trustworthy ML pose a challenge to those wishing to address both. We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP. We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. Through the case of binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA), a known method for addressing bias, also mitigates bias amplification by DP. As a consequence, DP and CDA together can be used to fine-tune models while maintaining both 
    
[^116]: 针对联邦赌博学习的激励真实通信

    Incentivized Truthful Communication for Federated Bandits

    [https://arxiv.org/abs/2402.04485](https://arxiv.org/abs/2402.04485)

    针对联邦赌博学习的激励机制存在被策略性客户攻击的问题，我们提出一种名为Truth-FedBan的激励兼容通信协议，使得参与者的激励与其自报成本无关，只有报告真实成本才能实现最佳效用，并且仍能保证亚线性的遗憾和通信成本。

    

    为了提高联邦赌博学习的效率和实用性，最近的研究引入了激励机制来促使客户之间的通信，其中一个客户只有在服务器提供的激励超过其参与成本时才参与。然而，现有的激励机制天真地假设客户是真实的，即他们都报告自己的真实成本，因此参与成本更高的客户声称的越高，服务器必须支付的越多。因此，这种机制容易受到希望通过错误报告来优化自己效用的策略性客户的攻击。为了解决这个问题，我们提出了一种激励兼容（即真实）的通信协议，名为Truth-FedBan，其中每个参与者的激励与其自报成本无关，而报告真实成本是实现最佳效用的唯一方式。更重要的是，Truth-FedBan仍能保证亚线性的遗憾和通信成本，而无需任何额外开销。换句话说，核心解决了客户之间的激励问题，从而提高了联邦赌博学习的效率和实用性。

    To enhance the efficiency and practicality of federated bandit learning, recent advances have introduced incentives to motivate communication among clients, where a client participates only when the incentive offered by the server outweighs its participation cost. However, existing incentive mechanisms naively assume the clients are truthful: they all report their true cost and thus the higher cost one participating client claims, the more the server has to pay. Therefore, such mechanisms are vulnerable to strategic clients aiming to optimize their own utility by misreporting. To address this issue, we propose an incentive compatible (i.e., truthful) communication protocol, named Truth-FedBan, where the incentive for each participant is independent of its self-reported cost, and reporting the true cost is the only way to achieve the best utility. More importantly, Truth-FedBan still guarantees the sub-linear regret and communication cost without any overheads. In other words, the core 
    
[^117]: 基于深度学习的物联网网络流量分析

    IoT Network Traffic Analysis with Deep Learning

    [https://arxiv.org/abs/2402.04469](https://arxiv.org/abs/2402.04469)

    本研究介绍了基于深度学习的物联网网络流量分析，通过处理大量数据和使用无监督学习技术，实现了高效、自动化和可扩展的异常检测模型，其中在KDD Cup 99数据集上得到了超过98％的准确率。

    

    随着物联网网络变得越来越复杂并产生大量动态数据，使用传统的统计方法和机器学习方法难以监测和检测异常。深度学习算法可以处理和学习大量数据，并可以使用无监督学习技术进行训练，这意味着它们不需要带有标签的数据来检测异常。这使得可能检测到之前未被发现的新型和未知的异常。此外，深度学习算法可以自动化和高度可扩展，因此它们可以在后台连续运行，并且可以实时监控大型物联网网络。在这项工作中，我们对最近使用深度学习技术的工作进行了文献综述，并在KDD Cup 99数据集上使用集成技术实现了一个模型。实验结果展示了我们的深度异常检测模型的出色性能，准确率超过98％。

    As IoT networks become more complex and generate massive amounts of dynamic data, it is difficult to monitor and detect anomalies using traditional statistical methods and machine learning methods. Deep learning algorithms can process and learn from large amounts of data and can also be trained using unsupervised learning techniques, meaning they don't require labelled data to detect anomalies. This makes it possible to detect new and unknown anomalies that may not have been detected before. Also, deep learning algorithms can be automated and highly scalable; thereby, they can run continuously in the backend and make it achievable to monitor large IoT networks instantly. In this work, we conduct a literature review on the most recent works using deep learning techniques and implement a model using ensemble techniques on the KDD Cup 99 dataset. The experimental results showcase the impressive performance of our deep anomaly detection model, achieving an accuracy of over 98\%.
    
[^118]: DySLIM: 利用不变测度实现混沌系统的动态稳定学习

    DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems

    [https://arxiv.org/abs/2402.04467](https://arxiv.org/abs/2402.04467)

    本文提出了一个新的框架，通过学习混沌系统的不变测度和动态来解决学习动态的困难，与传统方法不同，该框架在轨迹长度增加时具有较好的性能。

    

    从耗散性混沌系统中学习动态是非常困难的，因为它们固有的不稳定性导致学习动态的误差指数级增加。然而，许多这些系统表现出遍历性和吸引子：一个紧凑而高度复杂的流形，轨迹在有限时间内收敛到该流形，并支持一个不变测度，即一个在动态作用下不变的概率分布，该分布决定了系统的长期统计行为。在本文中，我们利用这个结构提出了一个新的框架，旨在学习不变测度以及动态，与通常只针对轨迹之间的误差的典型方法不同，后者在轨迹长度增加时往往会发散。我们使用我们的框架提出了一个易处理且样本高效的目标函数，可与任何现有的学习目标一起使用。

    Learning dynamics from dissipative chaotic systems is notoriously difficult due to their inherent instability, as formalized by their positive Lyapunov exponents, which exponentially amplify errors in the learned dynamics. However, many of these systems exhibit ergodicity and an attractor: a compact and highly complex manifold, to which trajectories converge in finite-time, that supports an invariant measure, i.e., a probability distribution that is invariant under the action of the dynamics, which dictates the long-term statistical behavior of the system. In this work, we leverage this structure to propose a new framework that targets learning the invariant measure as well as the dynamics, in contrast with typical methods that only target the misfit between trajectories, which often leads to divergence as the trajectories' length increases. We use our framework to propose a tractable and sample efficient objective that can be used with any existing learning objectives. Our Dynamics St
    
[^119]: NVIDIA Holoscan中面向医疗AI系统的确定性端到端延迟的实现

    Towards Deterministic End-to-end Latency for Medical AI Systems in NVIDIA Holoscan

    [https://arxiv.org/abs/2402.04466](https://arxiv.org/abs/2402.04466)

    本文针对NVIDIA Holoscan平台中的医疗AI系统的端到端延迟问题提出了解决方案，通过优化异构GPU工作负载，实现了确定性的延迟表现。

    

    AI和ML技术的引入已经彻底改变了医疗诊断和治疗。医疗设备制造商渴望最大限度地利用AI和ML的优势，将多个应用程序整合到一个平台上。然而，同时执行多个AI应用程序，每个应用程序都有自己的可视化组件，会导致不可预测的端到端延迟，主要是由于GPU资源争用引起的。为了解决这个问题，制造商通常会为不同的AI应用程序部署单独的工作站，从而增加了财务、能源和维护成本。本文针对NVIDIA的Holoscan平台中的这些挑战提出了解决方案，Holoscan是一个用于流式传感器数据和图像的实时AI系统。我们提出了一个针对异构GPU工作负载优化的系统设计，包括计算和图形任务。我们的设计利用CUDA MPS对计算工作负载进行空间划分，并分隔计算和图形处理单元。

    The introduction of AI and ML technologies into medical devices has revolutionized healthcare diagnostics and treatments. Medical device manufacturers are keen to maximize the advantages afforded by AI and ML by consolidating multiple applications onto a single platform. However, concurrent execution of several AI applications, each with its own visualization components, leads to unpredictable end-to-end latency, primarily due to GPU resource contentions. To mitigate this, manufacturers typically deploy separate workstations for distinct AI applications, thereby increasing financial, energy, and maintenance costs. This paper addresses these challenges within the context of NVIDIA's Holoscan platform, a real-time AI system for streaming sensor data and images. We propose a system design optimized for heterogeneous GPU workloads, encompassing both compute and graphics tasks. Our design leverages CUDA MPS for spatial partitioning of compute workloads and isolates compute and graphics proc
    
[^120]: 自动机器学习在推荐系统中的潜力

    The Potential of AutoML for Recommender Systems

    [https://arxiv.org/abs/2402.04453](https://arxiv.org/abs/2402.04453)

    这项研究探讨了在推荐系统领域中，自动机器学习（AutoML）的巨大潜力以及目前相对少有的关注和开发。

    

    自动化机器学习（AutoML）已经在包括模型压缩、机器翻译和计算机视觉等领域大大推进了机器学习（ML）的应用。推荐系统（RecSys）可以被看作是ML的一个应用。然而，AutoML在RecSys社区中并没有得到太多关注，RecSys也没有在AutoML社区中引起显著的关注。目前只有少数几个相对简单的自动化推荐系统（AutoRecSys）库采用了AutoML技术。然而，这些库都是基于学生项目开发的，并且没有提供AutoML库的功能和完善的开发。我们的目标是确定在一个没有经验的用户想要实现一个推荐系统的场景中，AutoML库的表现如何。我们比较了来自15个库的60个AutoML、AutoRecSys、ML和RecSys算法以及一个均值预测基准模型在14个显式反馈的RecSys数据集上的预测性能。

    Automated Machine Learning (AutoML) has greatly advanced applications of Machine Learning (ML) including model compression, machine translation, and computer vision. Recommender Systems (RecSys) can be seen as an application of ML. Yet, AutoML has found little attention in the RecSys community; nor has RecSys found notable attention in the AutoML community. Only few and relatively simple Automated Recommender Systems (AutoRecSys) libraries exist that adopt AutoML techniques. However, these libraries are based on student projects and do not offer the features and thorough development of AutoML libraries. We set out to determine how AutoML libraries perform in the scenario of an inexperienced user who wants to implement a recommender system. We compared the predictive performance of 60 AutoML, AutoRecSys, ML, and RecSys algorithms from 15 libraries, including a mean predictor baseline, on 14 explicit feedback RecSys datasets. To simulate the perspective of an inexperienced user, the algo
    
[^121]: 将细胞分割模型的极限推至极限以进行成像质谱细胞术

    Pushing the limits of cell segmentation models for imaging mass cytometry

    [https://arxiv.org/abs/2402.04446](https://arxiv.org/abs/2402.04446)

    该论文研究了不完美标签对基于学习的细胞分割模型的影响，发现从完全注释的标签中删除50%的细胞注释仅对DSC得分造成轻微降低，并且单组织模型在未知组织类型上的表现与多组织模型相近。

    

    成像质谱细胞术是一种相对新的在亚细胞分辨率下成像生物组织的技术。近年来，基于学习的分割方法使得对细胞类型和形态的精确量化成为可能，但通常依赖于具有完全注释的大型数据集。本文研究了不完美标签对基于学习的分割模型的影响，并评估了这些模型对不同组织类型的泛化能力。我们的结果表明，从完全注释的标签掩模中删除50%的细胞注释仅将Dice相似性系数（DSC）评分降低到0.874（与完全注释GT掩模上训练的模型的0.889相比）。这意味着注释时间实际上可以减少至少一半而不会对性能产生不利影响。此外，将我们的单组织模型训练于不完美的标签上仅将DSC降低了0.031，与其多组织模型相比，在未知组织类型上几乎没有质量损失。

    Imaging mass cytometry (IMC) is a relatively new technique for imaging biological tissue at subcellular resolution. In recent years, learning-based segmentation methods have enabled precise quantification of cell type and morphology, but typically rely on large datasets with fully annotated ground truth (GT) labels. This paper explores the effects of imperfect labels on learning-based segmentation models and evaluates the generalisability of these models to different tissue types. Our results show that removing 50% of cell annotations from GT masks only reduces the dice similarity coefficient (DSC) score to 0.874 (from 0.889 achieved by a model trained on fully annotated GT masks). This implies that annotation time can in fact be reduced by at least half without detrimentally affecting performance. Furthermore, training our single-tissue model on imperfect labels only decreases DSC by 0.031 on an unseen tissue type compared to its multi-tissue counterpart, with negligible qualitative d
    
[^122]: 探索具有总相关性的高阶神经网络节点交互

    Exploring higher-order neural network node interactions with total correlation

    [https://arxiv.org/abs/2402.04440](https://arxiv.org/abs/2402.04440)

    本文提出了一种名为局部相关性解释的新方法，通过基于数据流形上的距离进行聚类，并使用总相关性来捕获高阶变量之间的交互。通过在合成和真实世界数据中应用该方法，可以提取关于数据结构的隐藏洞察力，并用于探索和解释神经网络的内部运作。

    

    在生态系统、协作和人脑等领域中，变量以复杂的方式相互作用。然而，准确地描述高阶变量之间的交互是一个困难的问题，尤其是当这些交互随着数据的变化而改变时。为了解决这个问题，我们提出了一种名为局部相关性解释（CorEx）的新方法，通过首先基于数据流形上的距离来对数据点进行聚类，从而在局部尺度上捕获高阶交互。然后，我们使用一种称为总相关性的多元互信息的变体来构建数据在每个聚类中的潜在因子表示，以学习局部的交互。我们使用局部相关性解释来探索合成和真实世界数据中的高阶交互，以提取关于数据结构的隐藏洞察力。最后，我们展示了局部相关性解释的适用性，用于探索和解释训练好的神经网络的内部运作。

    In domains such as ecological systems, collaborations, and the human brain the variables interact in complex ways. Yet accurately characterizing higher-order variable interactions (HOIs) is a difficult problem that is further exacerbated when the HOIs change across the data. To solve this problem we propose a new method called Local Correlation Explanation (CorEx) to capture HOIs at a local scale by first clustering data points based on their proximity on the data manifold. We then use a multivariate version of the mutual information called the total correlation, to construct a latent factor representation of the data within each cluster to learn the local HOIs. We use Local CorEx to explore HOIs in synthetic and real world data to extract hidden insights about the data structure. Lastly, we demonstrate Local CorEx's suitability to explore and interpret the inner workings of trained neural networks.
    
[^123]: 使用大型语言模型的结构化实体提取

    Structured Entity Extraction Using Large Language Models

    [https://arxiv.org/abs/2402.04437](https://arxiv.org/abs/2402.04437)

    本论文提出了一种使用大型语言模型的结构化实体提取方法，在此任务上通过引入AESOP度量评估模型性能，并将整个提取任务分解为多个阶段，相较于基准模型取得了更好的效果，为未来结构化实体提取的进一步发展提供了有希望的方向。

    

    近年来，机器学习的最新进展显著影响了信息提取领域，大型语言模型（LLMs）在从非结构化文本中提取结构化信息方面起着关键作用。本文探讨了当前结构化实体提取方法的挑战和限制，并引入了一种新的方法来解决这些问题。我们首先介绍和规范化了结构化实体提取（SEE）任务，然后提出了适用于该任务的近似实体集重叠（AESOP）度量，以适当评估模型在这一任务上的性能。随后，我们提出了一种新模型，通过将整个提取任务分解为多个阶段，利用LLMs的强大功能来提高效果和效率。定量评估和人工并行评估证实了我们的模型优于基准模型，为结构化实体提取领域的未来进展提供了有希望的方向。

    Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
    
[^124]: 连续多维标度

    Continuous Multidimensional Scaling

    [https://arxiv.org/abs/2402.04436](https://arxiv.org/abs/2402.04436)

    连续多维标度是关于将距离信息嵌入欧几里得空间的过程，并探讨了在对象集不断增加的情况下，将整个嵌入问题序列视为一个固定空间中的一系列优化问题的方法和结论。

    

    多维标度(MDS)是将关于一组$n$个对象的距离信息嵌入到$d$维欧几里得空间中的过程。最初由心理测量学界构思，MDS关注的是嵌入到一组固定对象上的一组固定距离。现代关注的问题更常涉及到研究与一组不断增加的对象相关联的一系列距离的极限行为，如在随机图的统计推断的渐近理论中出现的问题。点到集合映射理论中的标准结果表明，若$n$固定，则嵌入结构的极限是极限距离的嵌入结构。但如果$n$增加怎么办呢？那么就需要重新制定MDS，以便将整个嵌入问题序列视为一个固定空间中的一系列优化问题。我们提出了这样一种重新制定，并推导出一些结论。

    Multidimensional scaling (MDS) is the act of embedding proximity information about a set of $n$ objects in $d$-dimensional Euclidean space. As originally conceived by the psychometric community, MDS was concerned with embedding a fixed set of proximities associated with a fixed set of objects. Modern concerns, e.g., that arise in developing asymptotic theories for statistical inference on random graphs, more typically involve studying the limiting behavior of a sequence of proximities associated with an increasing set of objects. Standard results from the theory of point-to-set maps imply that, if $n$ is fixed, then the limit of the embedded structures is the embedded structure of the limiting proximities. But what if $n$ increases? It then becomes necessary to reformulate MDS so that the entire sequence of embedding problems can be viewed as a sequence of optimization problems in a fixed space. We present such a reformulation and derive some consequences.
    
[^125]: PreGIP: 针对深度知识产权保护的图神经网络预训练水印技术

    PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection

    [https://arxiv.org/abs/2402.04435](https://arxiv.org/abs/2402.04435)

    PreGIP是针对深度知识产权保护的一种图神经网络预训练水印技术，它通过添加无任务的水印损失来给预训练的GNN编码器的嵌入空间添加水印，并采用抗微调的水印注入方法

    

    图神经网络（GNNs）的预训练在促进各种下游任务中显示出巨大的能力。由于预训练通常需要大量的数据和计算资源，预训练的GNNs成为合法拥有者的高价值知识产权（IP）。然而，对手可能会非法复制和部署预训练的GNN模型用于其下游任务。虽然已经开始尝试为IP保护添加GNN分类器的水印，但这些方法需要目标分类任务才能进行水印处理，因此不适用于GNN模型的自监督预训练。因此，在这项工作中，我们提出了一个新框架PreGIP，用于在保持嵌入空间高质量的同时，给GNN编码器的预训练添加水印以进行IP保护。PreGIP引入了无任务的水印损失来给预训练的GNN编码器的嵌入空间添加水印。同时采用了抗微调的水印注入方法。我们还进行了理论分析和扩展实验证明了方法的有效性和鲁棒性。

    Pretraining on Graph Neural Networks (GNNs) has shown great power in facilitating various downstream tasks. As pretraining generally requires huge amount of data and computational resources, the pretrained GNNs are high-value Intellectual Properties (IP) of the legitimate owner. However, adversaries may illegally copy and deploy the pretrained GNN models for their downstream tasks. Though initial efforts have been made to watermark GNN classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to self-supervised pretraining of GNN models. Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of GNN encoder for IP protection while maintain the high-quality of the embedding space. PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained GNN encoder. A finetuning-resistant watermark injection is further deployed. Theoretical analysis and exte
    
[^126]: 弱监督深度学习在胸部CT分类中的性能限制是什么？

    What limits performance of weakly supervised deep learning for chest CT classification?

    [https://arxiv.org/abs/2402.04419](https://arxiv.org/abs/2402.04419)

    本研究测试了弱监督学习对胸部CT分类性能的限制。结果显示，在训练数据中增加错误标签的情况下，模型可以忍受高达10%的错误。同时，随着训练数据量的增加，所有疾病的分类性能稳步提升。

    

    由于高质量的疾病标签的稀缺性，弱监督学习与嘈杂数据成为医学影像学界关注的焦点。然而，对这种弱监督学习的限制以及这些约束对疾病分类性能的影响知之甚少。在本文中，我们通过研究三个条件来测试这种弱监督的影响。首先，我们逐渐增加训练数据中标签错误来检查模型对嘈杂数据的容忍度。其次，我们通过改变训练数据的数量来评估数据集大小对性能的影响。第三，我们比较了二元分类和多标签分类之间的性能差异。结果表明，模型可以在添加了10%的标签错误之前忍受，而疾病分类性能会出现下降。对于所有疾病类别，随着训练数据量的增加，疾病分类性能稳步提升，然后出现了下降。

    Weakly supervised learning with noisy data has drawn attention in the medical imaging community due to the sparsity of high-quality disease labels. However, little is known about the limitations of such weakly supervised learning and the effect of these constraints on disease classification performance. In this paper, we test the effects of such weak supervision by examining model tolerance for three conditions. First, we examined model tolerance for noisy data by incrementally increasing error in the labels within the training data. Second, we assessed the impact of dataset size by varying the amount of training data. Third, we compared performance differences between binary and multi-label classification. Results demonstrated that the model could endure up to 10% added label error before experiencing a decline in disease classification performance. Disease classification performance steadily rose as the amount of training data was increased for all disease classes, before experiencin
    
[^127]: 基于分散区块链的稳健多智能体多臂赌博问题研究

    Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit

    [https://arxiv.org/abs/2402.04417](https://arxiv.org/abs/2402.04417)

    本研究通过将分散区块链技术与新颖机制结合，设计了一种稳健的多智能体多臂赌博问题解决方案，以确保诚实参与者获得的累积奖励，并应对恶意行为和保护参与者隐私的需求。

    

    我们研究了一种稳健的多智能体多臂赌博问题，在这个问题中，多个客户或参与者分布在一个完全分散的区块链上，其中一些可能是恶意的。臂的奖励在参与者之间是均匀的，遵循时间不变的随机分布，只有在系统足够安全时才向参与者透露。系统的目标是有效地确保诚实参与者获得的累积奖励。为此，我们是第一个将区块链的先进技术和新颖的机制结合到系统中，为诚实参与者设计最佳策略。这样可以应对各种恶意行为并保护参与者的隐私。具体来说，我们随机选择了一组可以访问所有参与者的验证者池，为这些验证者设计了一种基于数字签名的全新共识机制，并发明了一种基于UCB的策略。

    We study a robust multi-agent multi-armed bandit problem where multiple clients or participants are distributed on a fully decentralized blockchain, with the possibility of some being malicious. The rewards of arms are homogeneous among the clients, following time-invariant stochastic distributions that are revealed to the participants only when the system is secure enough. The system's objective is to efficiently ensure the cumulative rewards gained by the honest participants. To this end and to the best of our knowledge, we are the first to incorporate advanced techniques from blockchains, as well as novel mechanisms, into the system to design optimal strategies for honest participants. This allows various malicious behaviors and the maintenance of participant privacy. More specifically, we randomly select a pool of validators who have access to all participants, design a brand-new consensus mechanism based on digital signatures for these validators, invent a UCB-based strategy that 
    
[^128]: 基于大规模多模态数据检索的无监督领域泛化的数据中心方法

    A Data Centric Approach for Unsupervised Domain Generalization via Retrieval from Web Scale Multimodal Data

    [https://arxiv.org/abs/2402.04416](https://arxiv.org/abs/2402.04416)

    该论文基于大规模多模态数据检索，提出了一个无监督领域泛化的数据中心方法。在多模态无监督领域泛化问题中，通过构建一个小型的源数据子集，而不是依赖丰富的源数据，来解决目标标签空间数据获取困难的问题。

    

    领域泛化(DG)是一个重要的问题，它通过利用一个或多个源领域在共享标签空间的假设下学习一个能够推广到未见测试领域的模型。然而，大多数DG方法假设可以访问丰富的目标标签空间中的源数据，这个要求在许多现实应用中太过严格，因为获取与目标任务相同的标签空间费用高昂。为了解决这个问题，我们处理了无监督领域泛化(UDG)问题的多模态版本，该问题使用一个大型的任务无关的未标记的源数据集，例如LAION-2B在微调期间。我们的框架不显式地假设源数据集与目标任务之间存在任何关系。相反，它只依赖于源数据集可以在联合视觉-语言空间中高效搜索的前提。针对这种多模态UDG设置，我们提出了一种新的方法来构建一个小型（小于100K）的源数据子集。

    Domain generalization (DG) is an important problem that learns a model that can generalize to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive. For this setting, we tackle the multimodal version of the unsupervised domain generalization (UDG) problem, which uses a large task-agnostic unlabeled source dataset, such as LAION-2B during finetuning. Our framework does not explicitly assume any relationship between the source dataset and target task. Instead, it relies only on the premise that the source dataset can be efficiently searched in a joint vision-language space. For this multimodal UDG setting, we propose a novel method to build a small ($<$100K) subset of the source data in th
    
[^129]: VampPrior混合模型

    The VampPrior Mixture Model

    [https://arxiv.org/abs/2402.04412](https://arxiv.org/abs/2402.04412)

    本论文提出了VampPrior混合模型（VMM），它是一种新颖的DLVM先验，可用于深度潜变量模型的集成和聚类，通过改善当前聚类先验的不足，并提出了一个清晰区分变分和先验参数的推理过程。使用VMM的变分自动编码器在基准数据集上取得了强大的聚类性能，将VMM与scVI相结合可以显著提高其性能，并自动将细胞分组为具有生物意义的聚类。

    

    当前用于深度潜变量模型（DLVMs）的聚类先验需要预先定义聚类的数量，并且容易受到较差的初始化的影响。解决这些问题可以通过同时执行集成和聚类的方式极大地改进基于深度学习的scRNA-seq分析。我们将VampPrior（Tomczak和Welling，2018）调整为Dirichlet过程高斯混合模型，得到VampPrior混合模型（VMM），这是一种新颖的DLVM先验。我们提出了一个推理过程，交替使用变分推理和经验贝叶斯，以清楚地区分变分和先验参数。在基准数据集上使用VMM的变分自动编码器获得了极具竞争力的聚类性能。将VMM与广受欢迎的scRNA-seq集成方法scVI（Lopez等，2018）相结合，显著改善了其性能，并自动将细胞分组为具有生物意义的聚类。

    Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak & Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters. Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.
    
[^130]: Chatbot遇见管道：利用确定有限自动机增进大规模语言模型的能力

    Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton

    [https://arxiv.org/abs/2402.04411](https://arxiv.org/abs/2402.04411)

    本文介绍了DFA-LLM（确定有限自动机增强的大规模语言模型）框架，通过嵌入从对话中学习到的确定有限自动机在大规模语言模型中，使得对话代理能够生成具有规范合规性的回复，并在广泛的基准测试中验证了其有效性。

    

    本文介绍了一种新颖的框架——确定有限自动机增强的大规模语言模型（DFA-LLM），旨在通过使用大规模语言模型（LLM）提升对话代理的能力。传统的LLM在特定情景（如情感支持和客户服务）中生成规范合规的回复面临挑战。我们的框架通过将从训练对话中学习到的确定有限自动机（DFA）嵌入到LLM中来应对这些挑战。这种结构化的方法使得LLM能够按照DFA指导的确定性回应路径来回应。DFA-LLM的优势包括可解释性结构，上下文感知的对话回复检索以及与现有LLM的即插即用兼容性。广泛的基准测试验证了DFA-LLM的有效性，表明它有潜力成为对话代理领域的有价值的贡献。

    This paper introduces the Definite Finite Automaton augmented large language model (DFA-LLM), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach enables the LLM to adhere to a deterministic response pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-LLM's effectiveness, indicating its potential as a valuable contribution to the conversational agent.
    
[^131]: 进一步实现公平、鲁棒和高效的联邦学习中客户贡献评估

    Towards Fair, Robust and Efficient Client Contribution Evaluation in Federated Learning

    [https://arxiv.org/abs/2402.04409](https://arxiv.org/abs/2402.04409)

    本文提出了一种名为FRECA的方法来评估联邦学习中客户的贡献。该方法使用FedTruth框架估计全局模型的真实更新，平衡来自所有客户的贡献，并排除恶意客户的影响。FRECA对于拜占庭攻击具有鲁棒性，并且具有高效性。

    

    由于各种原因，联邦学习（FL）中客户的性能可能会有所不同。评估每个客户的贡献对于客户选择和补偿至关重要。然而，由于客户通常具有非独立和同分布（non-iid）的数据，导致可能存在噪声或发散的更新，因此评估客户贡献具有挑战性。当无法访问客户的本地数据或基准根数据集时，恶意客户的风险会进一步增加。本文介绍了一种名为公平、鲁棒和高效客户评估（FRECA）的新方法，用于量化FL中的客户贡献。FRECA采用一种名为FedTruth的框架来估计全局模型的真实更新，平衡来自所有客户的贡献，并过滤出恶意客户的影响。该方法对拜占庭攻击具有鲁棒性，并采用了拜占庭鲁棒的聚合算法。FRECA还具有高效性，因为它仅仅在本地模型更新上操作，且只需要少量的全局通信。

    The performance of clients in Federated Learning (FL) can vary due to various reasons. Assessing the contributions of each client is crucial for client selection and compensation. It is challenging because clients often have non-independent and identically distributed (non-iid) data, leading to potentially noisy or divergent updates. The risk of malicious clients amplifies the challenge especially when there's no access to clients' local data or a benchmark root dataset. In this paper, we introduce a novel method called Fair, Robust, and Efficient Client Assessment (FRECA) for quantifying client contributions in FL. FRECA employs a framework called FedTruth to estimate the global model's ground truth update, balancing contributions from all clients while filtering out impacts from malicious ones. This approach is robust against Byzantine attacks and incorporates a Byzantine-resilient aggregation algorithm. FRECA is also efficient, as it operates solely on local model updates and requir
    
[^132]: 边并行图编码嵌入

    Edge-Parallel Graph Encoder Embedding

    [https://arxiv.org/abs/2402.04403](https://arxiv.org/abs/2402.04403)

    这篇论文提出了一种边并行图编码嵌入的算法，通过重构并行化实现，在大规模图上取得了500倍的加速效果。

    

    用于嵌入图的新算法已经降低了寻找低维表示的渐近复杂度。One-Hot Graph Encoder Embedding (GEE) 在边的线性逐个通过过程中产生的嵌入收敛到谱嵌入。这种方法的可扩展性和性能优势受到解释型语言串行实现的限制。我们将GEE重构为在Ligra图引擎中并行程序，将函数映射到图的边上，并使用无锁原子指令防止数据竞争。在一个有18亿条边的图上，与原始实现相比，这将导致500倍的加速和编译即时版本的17倍加速。

    New algorithms for embedding graphs have reduced the asymptotic complexity of finding low-dimensional representations. One-Hot Graph Encoder Embedding (GEE) uses a single, linear pass over edges and produces an embedding that converges asymptotically to the spectral embedding. The scaling and performance benefits of this approach have been limited by a serial implementation in an interpreted language. We refactor GEE into a parallel program in the Ligra graph engine that maps functions over the edges of the graph and uses lock-free atomic instrutions to prevent data races. On a graph with 1.8B edges, this results in a 500 times speedup over the original implementation and a 17 times speedup over a just-in-time compiled version.
    
[^133]: 生成带有病人时间轴的电子健康记录的CEHR-GPT

    CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines

    [https://arxiv.org/abs/2402.04400](https://arxiv.org/abs/2402.04400)

    CEHR-GPT是一种使用病人时间轴生成电子健康记录的方法，能够处理EHR数据的合成、疾病进展分析等应用。

    

    合成电子健康记录（EHR）已成为推进医疗应用和机器学习模型的关键工具，特别是对于没有直接访问医疗数据的研究人员而言。尽管现有方法，如基于规则的方法和生成对抗网络（GAN），可以生成类似真实世界EHR数据的合成数据，但这些方法常常使用表格格式，忽略了病人历史的时间依赖性，限制了数据复制。最近，越来越多的人开始利用生成预训练转换器（GPT）来处理EHR数据。这使得可以进行疾病进展分析、人口估计、反事实推理和合成数据生成等应用。本研究关注合成数据生成，并演示了使用源自CEHR-BERT的特定病人表示训练GPT模型的能力，从而能够生成可无缝转换的病人序列。

    Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted 
    
[^134]: 学习在时间序列下处理时间标签噪声

    Learning from Time Series under Temporal Label Noise

    [https://arxiv.org/abs/2402.04398](https://arxiv.org/abs/2402.04398)

    该论文研究了在时间序列下处理时间标签噪声的问题，提出了一种可以从数据中直接估计时间标签噪声函数并训练出噪声容忍分类器的方法，并在实验中展示了该方法在各种时间标签噪声函数下都取得了最先进的性能。

    

    许多顺序分类任务受到随时间变化的标签噪声的影响。这种噪声可能会导致标签质量随时间改善、恶化或周期性变化。我们首先提出和系统化了时间标签噪声的概念，这是关于时间序列顺序分类的一个未经研究的问题。在这种设置下，多个标签连续记录，同时受到一个与时间相关的噪声函数的干扰。我们首先展示了建模时间标签噪声函数的重要性，以及现有方法的持续低效。然后，我们提出了一种直接从数据中估计时间标签噪声函数的方法，可以训练出对噪声具有容忍性的分类器。我们展示了我们的方法在各种各样的时间标签噪声函数下，使用真实和合成数据在性能上达到了最先进水平。

    Many sequential classification tasks are affected by label noise that varies over time. Such noise can cause label quality to improve, worsen, or periodically change over time. We first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series. In this setting, multiple labels are recorded in sequence while being corrupted by a time-dependent noise function. We first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform. We then propose methods that can train noise-tolerant classifiers by estimating the temporal label noise function directly from data. We show that our methods lead to state-of-the-art performance in the presence of diverse temporal label noise functions using real and synthetic data.
    
[^135]: QuIP#: 使用哈达玛德非相干性和格书进行更好的LLM量化

    QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks

    [https://arxiv.org/abs/2402.04396](https://arxiv.org/abs/2402.04396)

    QuIP#是一种使用哈达玛德非相干性和格书的权重量化方法，能够在极限压缩范围下达到最先进的结果，并具有快速推理的优势。

    

    后训练量化(PTQ)通过将LLM的权重量化为低精度来减少其内存占用。在这项工作中，我们引入了QuIP#，一种仅基于权重的PTQ方法，使用了三种新技术，在极限压缩范围($\le$ 4比特每个权重)上取得了最先进的结果。首先，QuIP#通过使用随机哈达玛德变换改进了QuIP中的非相干处理，该方法更快且具有更好的理论特性。其次，QuIP#使用向量量化技术利用了非相干权重具有的球形亚高斯分布特性：具体地说，我们引入了一组基于高度对称$E_8$格书的硬件高效代码书，实现了最优的8维单位球装填。第三，QuIP#使用微调来提高对原始模型的忠实度。我们的实验证明，QuIP#优于现有的PTQ方法，能够实现新的PTQ扩展行为，并支持快速推理。

    Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.
    
[^136]: 密集乘法物理信息神经网络

    Densely Multiplied Physics Informed Neural Network

    [https://arxiv.org/abs/2402.04390](https://arxiv.org/abs/2402.04390)

    该论文通过改进神经网络架构，提出了一种密集乘法物理信息神经网络（DM-PINN）架构，它有效利用隐藏层的输出，显著提高了PINN的准确性和性能。

    

    尽管物理信息神经网络（Physics-Informed Neural Networks, PINNs）在处理非线性偏微分方程（PDEs）方面显示出巨大潜力，但常常会出现精度不足或获取不正确结果的问题。与大多数现有的解决方案不同，该论文改进了神经网络架构以提高PINN的性能。我们提出了一种密集乘法PINN（DM-PINN）架构，它将隐藏层的输出与所有后面的隐藏层的输出相乘。在不引入更多可训练参数的情况下，该有效机制可以显著提高PINN的准确性。所提出的架构在四个基准示例（Allan-Cahn方程，Helmholtz方程，Burgers方程和1D对流方程）上进行了评估。将所提出的架构与不同的PINN结构进行比较，证明了其卓越的性能。

    Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes. Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN. We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers. Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs. The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation). Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of 
    
[^137]: 在六个简单的步骤中去噪扩散概率模型

    Denoising Diffusion Probabilistic Models in Six Simple Steps

    [https://arxiv.org/abs/2402.04384](https://arxiv.org/abs/2402.04384)

    本论文提供了一个简单、全面、干净且清晰的介绍去噪扩散概率模型（DDPM）的方法，强调了从连续时间极限的视角出发，以提供更好的理解和实际性能。

    

    去噪扩散概率模型（DDPM）是一类非常流行的深度生成模型，已成功应用于包括图像和视频生成、蛋白质和材料合成、天气预测和偏微分方程的神经替代等多个问题。尽管其普及度很高，但很难找到一个简单、全面、干净且清晰的DDPM介绍。研究论文中必要的简洁解释无法阐明制定DDPM所采取的不同设计步骤以及省略了步骤的理由以节省空间。此外，这些论述通常从变分下界的视角出发，这是不必要且可能有害的，因为它混淆了方法奏效的原因并暗示了实践中表现不佳的泛化性质。另一方面，采用连续时间极限的视角是美丽且普遍的，但是...

    Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but 
    
[^138]: 公平图生成：公平性的图生成研究

    FairWire: Fair Graph Generation

    [https://arxiv.org/abs/2402.04383](https://arxiv.org/abs/2402.04383)

    这项研究关注于分析和减轻真实和合成图中的结构偏差，通过设计一个多功能的公平性正则化器来缓解偏差影响。

    

    最近，基于图的机器学习因其分析和学习复杂关联的能力而引起了广泛关注，然而，在这些算法中使用的有偏图结构放大了不公平影响，在实际决策系统的部署中引起了重大关注。此外，尽管合成图生成对于隐私和可扩展性考虑已变得至关重要，但生成式学习算法对结构偏差的影响尚未得到调查。鉴于此，本研究侧重于分析和减轻真实和合成图的结构偏差。具体而言，我们首先理论分析造成二元关系预测不平等的结构偏差的来源。为了减轻所发现的偏差因素，我们设计了一种新的公平性正则化器，具有多种用途。鉴于图生成中的偏差放大问题

    Machine learning over graphs has recently attracted growing attention due to its ability to analyze and learn complex relations within critical interconnected systems. However, the disparate impact that is amplified by the use of biased graph structures in these algorithms has raised significant concerns for the deployment of them in real-world decision systems. In addition, while synthetic graph generation has become pivotal for privacy and scalability considerations, the impact of generative learning algorithms on the structural bias has not yet been investigated. Motivated by this, this work focuses on the analysis and mitigation of structural bias for both real and synthetic graphs. Specifically, we first theoretically analyze the sources of structural bias that result in disparity for the predictions of dyadic relations. To alleviate the identified bias factors, we design a novel fairness regularizer that offers a versatile use. Faced with the bias amplification in graph generatio
    
[^139]: 细调语言模型生成稳定的无机材料文本

    Fine-Tuned Language Models Generate Stable Inorganic Materials as Text

    [https://arxiv.org/abs/2402.04379](https://arxiv.org/abs/2402.04379)

    细调语言模型用于生成稳定材料，具有可靠性高和灵活性强的优势，能以较高的速率生成被预测为亚稳态的材料。

    

    我们提出了对大型语言模型进行细调，以生成稳定材料。虽然非传统，但在文本编码的原子数据上细调大型语言模型非常简单易行，同时可靠性高，约90%的采样结构遵守原子位置和电荷的物理约束。通过来自学习的机器学习势和金标准DFT计算的能量以上的计算，我们表明我们的最强模型（细调LLaMA-2 70B）可以以CDVAE竞争扩散模型的约两倍速率（49% vs 28%）生成被预测为亚稳态的材料。由于文本提示的固有灵活性，我们的模型可以同时用于稳定材料的无条件生成、部分结构的填充和文本条件生成。最后，我们表明语言模型捕捉晶体结构的关键对称性的能力随模型规模的增大而改善，这表明预训练的LLM的偏差出奇地适合原子性的应用。

    We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic
    
[^140]: $\texttt{NeRCC}$: 内嵌回归编码计算用于具有弹性的分布式预测服务系统

    $\texttt{NeRCC}$: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems

    [https://arxiv.org/abs/2402.04377](https://arxiv.org/abs/2402.04377)

    NeRCC是一个通用的抗拖尾节点的近似编码计算框架，包括回归编码、计算和回归解码三个层次，通过优化两个正则化项的依赖关系来解决嵌套回归问题。

    

    对抗拖尾节点(stragglers)是预测服务系统的一个重要特征，任务是在预先训练的机器学习模型上执行输入数据的推理。在本文中，我们提出了一种名为NeRCC的通用的抗拖尾节点的近似编码计算框架。NeRCC包括三个层次：(1)回归编码和抽样，生成编码数据点，作为原始数据点的组合；(2)计算，其中一个工作集群在编码数据点上运行推理；(3)回归解码和抽样，从编码数据点的可用预测中近似恢复出原始数据点的预测结果。我们认为框架的总体目标揭示了编码和解码层中两个回归模型之间的潜在相互关系。我们提出了一个解决嵌套回归问题的方法，通过总结它们对两个联合优化的正则化项的依赖关系。

    Resilience against stragglers is a critical element of prediction serving systems, tasked with executing inferences on input data for a pre-trained machine-learning model. In this paper, we propose NeRCC, as a general straggler-resistant framework for approximate coded computing. NeRCC includes three layers: (1) encoding regression and sampling, which generates coded data points, as a combination of original data points, (2) computing, in which a cluster of workers run inference on the coded data points, (3) decoding regression and sampling, which approximately recovers the predictions of the original data points from the available predictions on the coded data points. We argue that the overall objective of the framework reveals an underlying interconnection between two regression models in the encoding and decoding layers. We propose a solution to the nested regressions problem by summarizing their dependence on two regularization terms that are jointly optimized. Our extensive experi
    
[^141]: 使用真实数据和替代数据进行学习的扩展规律

    Scaling laws for learning with real and surrogate data

    [https://arxiv.org/abs/2402.04376](https://arxiv.org/abs/2402.04376)

    本研究探讨了将替代数据与真实数据整合以进行训练的方案，发现整合替代数据能够显著降低测试误差，并提出了一个扩展规律来描述混合模型的测试误差，可以用于预测最优加权和收益。

    

    收集大量高质量的数据通常被限制在成本昂贵或不切实际的范围内, 这是机器学习中的一个关键瓶颈。相反地, 可以将来自目标分布的小规模数据集与来自公共数据集、不同情况下收集的数据或由生成模型合成的数据相结合, 作为替代数据。我们提出了一种简单的方案来将替代数据整合到训练中, 并使用理论模型和实证研究探索其行为。我们的主要发现是：(i) 整合替代数据可以显著降低原始分布的测试误差；(ii) 为了获得这种效益, 使用最优加权经验风险最小化非常关键；(iii) 在混合使用真实数据和替代数据训练的模型的测试误差可以很好地用一个扩展规律来描述。这可以用来预测最优加权和收益。

    Collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models. Blurring distinctions, we refer to such data as `surrogate data'.   We define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ In order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ The test error of models trained on mixtures of real and surrogate data is well described by a scaling law. This can be used to predict the optimal weighting and the gai
    
[^142]: 在保持边缘一致的差分隐私合成数据上训练线性模型的过量风险界限

    Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving, Differentially-Private, Synthetic Data

    [https://arxiv.org/abs/2402.04375](https://arxiv.org/abs/2402.04375)

    本文提出了在保持边缘一致的差分隐私合成数据上训练线性模型的过量风险的新界限，为连续和Lipschitz损失函数提供了上界和下界。

    

    机器学习的广泛应用引发了人们对于模型可能揭示训练数据中个体的私密信息的担忧。为了防止敏感数据的泄露，我们考虑使用差分隐私的合成训练数据而不是真实训练数据来训练机器学习模型。合成数据的一个关键优点是能够保持原始分布的低阶边缘特征。我们的主要贡献是针对在这种合成数据上训练的线性模型，针对连续和Lipschitz损失函数提出了新的过量经验风险的上界和下界。我们在理论结果之外进行了大量实验。

    The growing use of machine learning (ML) has raised concerns that an ML model may reveal private information about an individual who has contributed to the training dataset. To prevent leakage of sensitive data, we consider using differentially-private (DP), synthetic training data instead of real training data to train an ML model. A key desirable property of synthetic data is its ability to preserve the low-order marginals of the original distribution. Our main contribution comprises novel upper and lower bounds on the excess empirical risk of linear models trained on such synthetic data, for continuous and Lipschitz loss functions. We perform extensive experimentation alongside our theoretical results.
    
[^143]: 神经网络学习逐渐复杂的统计学

    Neural Networks Learn Statistics of Increasing Complexity

    [https://arxiv.org/abs/2402.04362](https://arxiv.org/abs/2402.04362)

    本文证明了分布简单性倾向（DSB）的神经网络学习规律，即在训练早期自动学习低阶矩的最大熵分布特征，然后在训练后期失去这种能力。此外，研究还利用最优传输方法进行了低阶统计数据的编辑，证明了早期训练的网络会将编辑的样本视为目标类别的样本。

    

    分布简单性倾向（DSB）假设神经网络首先学习低阶矩，然后再转向高阶相关性。在这项工作中，我们通过展示网络在训练早期自动学习在最大熵分布上表现良好，而在训练后期失去这种能力的有力新证据，给出了令人信服的新证据来支持DSB。我们还通过证明令牌$n$-gram频率与嵌入向量的矩之间的等价关系，并在LLM中找到倾向的实证证据，将DSB扩展到离散领域。最后，我们使用最优传输方法将一类的低阶统计数据手术性地编辑成与另一类相匹配，然后展示早期训练的网络将编辑的样本视为来自目标类别的样本。代码可在 https://github.com/EleutherAI/features-across-time 上获取。

    The distributional simplicity bias (DSB) posits that neural networks learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we present compelling new evidence for the DSB by showing that networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later. We also extend the DSB to discrete domains by proving an equivalence between token $n$-gram frequencies and the moments of embedding vectors, and by finding empirical evidence for the bias in LLMs. Finally we use optimal transport methods to surgically edit the low-order statistics of one class to match those of another, and show that early-training networks treat the edited samples as if they were drawn from the target class. Code is available at https://github.com/EleutherAI/features-across-time.
    
[^144]: 自适应推理: 理论极限与未开发的机遇

    Adaptive Inference: Theoretical Limits and Unexplored Opportunities

    [https://arxiv.org/abs/2402.04359](https://arxiv.org/abs/2402.04359)

    本论文提出了自适应推理算法的效率和性能提升的理论框架，并通过经验证据证明可以实现10-100倍的效率提升，在优化选择和设计自适应推理状态空间方面提供了见解。

    

    本论文引入了第一个用于量化自适应推理算法效率和性能提升机会大小的理论框架。我们提供了可实现的效率和性能提升的新近似和精确界限，并通过经验证据证明，在计算机视觉和自然语言处理任务中，可以实现10-100倍的效率提升而不会带来任何性能损失。此外，我们提出了通过优化选择和设计自适应推理状态空间来改进可达到的效率提升的见解。

    This paper introduces the first theoretical framework for quantifying the efficiency and performance gain opportunity size of adaptive inference algorithms. We provide new approximate and exact bounds for the achievable efficiency and performance gains, supported by empirical evidence demonstrating the potential for 10-100x efficiency improvements in both Computer Vision and Natural Language Processing tasks without incurring any performance penalties. Additionally, we offer insights on improving achievable efficiency gains through the optimal selection and design of adaptive inference state spaces.
    
[^145]: PQMass: 使用概率质量估计的生成模型质量的概率评估

    PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation

    [https://arxiv.org/abs/2402.04355](https://arxiv.org/abs/2402.04355)

    PQMass是一种使用概率质量估计来评估生成模型质量的全面方法，能够直接处理高维数据，不依赖于假设或训练其他模型。

    

    我们提出了一种全面的基于样本的方法来评估生成模型的质量。所提出的方法能够估计两个样本集合来自同一分布的概率，为评估单个生成模型的性能或比较在同一数据集上训练的多个竞争模型提供了一个统计上严格的方法。该比较可以通过将空间划分为非重叠的区域并比较每个区域中的数据样本数量来进行。该方法仅需要生成模型和测试数据的样本。它能够直接处理高维数据，无需降维。显著的是，该方法不依赖于关于真实分布密度的假设，并且不依赖于训练或拟合任何辅助模型。相反，它着重于近似计算密度的积分（概率质量）。

    We propose a comprehensive sample-based method for assessing the quality of generative models. The proposed approach enables the estimation of the probability that two sets of samples are drawn from the same distribution, providing a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models trained on the same dataset. This comparison can be conducted by dividing the space into non-overlapping regions and comparing the number of data samples in each region. The method only requires samples from the generative model and the test data. It is capable of functioning directly on high-dimensional data, obviating the need for dimensionality reduction. Significantly, the proposed method does not depend on assumptions regarding the density of the true distribution, and it does not rely on training or fitting any auxiliary models. Instead, it focuses on approximating the integral of the density (probability mass) acros
    
[^146]: 刺猬与豪猪：具有Softmax模仿的表达性线性注意力

    The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry

    [https://arxiv.org/abs/2402.04347](https://arxiv.org/abs/2402.04347)

    Hedgehog是一种具有Softmax模仿的可学习线性注意力，通过保持尖锐和单调性来弥补线性注意力在质量上的不足。

    

    线性注意力已经显示出提高Transformer效率的潜力，将注意力的二次复杂性降低为与序列长度成线性关系。这对于以下三个方面具有激动人心的前景：（1）从头开始训练线性Transformer，（2）将任务特定的Transformer进行“微调-转换”为线性版本，并恢复任务性能，以及（3）将诸如大型语言模型等Transformer进行“预训练-转换”，以实现下游任务的微调。然而，线性注意力在质量上常常不如标准的softmax注意力。为了弥合这种性能差距，我们发现先前的线性注意力缺乏与良好性能相关的softmax注意力的关键属性：低熵（或“尖峰”）权重和点积单调性。我们进一步观察到一种令人惊讶的简单特征映射，保留了这些属性，并与softmax的表现相匹配，但在线性注意力中计算效率低下。因此，我们提出了Hedgehog，一种可学习的线性注意力，保持了尖峰和单调性。

    Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) "finetuned-conversion" of task-specific Transformers into linear versions that recover task performance, and (3) "pretrained-conversion" of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or "spiky") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monoton
    
[^147]: 去校准是否有助于一致性预测？

    Does Confidence Calibration Help Conformal Prediction?

    [https://arxiv.org/abs/2402.04344](https://arxiv.org/abs/2402.04344)

    本文研究了去校准对一致性预测的影响，发现事后校准方法导致更大的预测集，而过于自信的情况有利于一致性预测性能。基于这一分析，提出了一种新的一致性温度缩放方法 (ConfTS)，其通过优化温度值来改进一致性预测性能。

    

    作为一种新兴的不确定性量化技术，一致性预测构建了一组具有高概率包含真实标签的预测集。以往的工作通常采用温度缩放来校准分类器，假设信心校准可以为一致性预测带来好处。本文首先表明事后校准方法会意外地导致更大的预测集，并改善了校准性能，而过于自信且温度较小的情况则有助于一致性预测的性能。从理论上讲，我们证明高置信度会降低在预测集中添加新类的概率。受到这一分析的启发，我们提出了一种新的方法，$\textbf{一致性温度缩放}$ (ConfTS)，通过阈值与真实标签的非一致性分数之间的差距来修正目标。通过这种方式，ConfTS的新目标将使温度值朝着优化集的方向进行优化。

    Conformal prediction, as an emerging uncertainty qualification technique, constructs prediction sets that are guaranteed to contain the true label with high probability. Previous works usually employ temperature scaling to calibrate the classifier, assuming that confidence calibration can benefit conformal prediction. In this work, we first show that post-hoc calibration methods surprisingly lead to larger prediction sets with improved calibration, while over-confidence with small temperatures benefits the conformal prediction performance instead. Theoretically, we prove that high confidence reduces the probability of appending a new class in the prediction set. Inspired by the analysis, we propose a novel method, $\textbf{Conformal Temperature Scaling}$ (ConfTS), which rectifies the objective through the gap between the threshold and the non-conformity score of the ground-truth label. In this way, the new objective of ConfTS will optimize the temperature value toward an optimal set th
    
[^148]: LegalLens: 利用LLMs在非结构化文本中识别法律违规行为

    LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text

    [https://arxiv.org/abs/2402.04335](https://arxiv.org/abs/2402.04335)

    本研究利用LLMs构建数据集，通过微调模型和使用闭源LLMs进行少样本实验，成功实现了非结构化文本中法律违规行为的检测和与受害者的关联。结果显示我们的设置适用于这两个任务，F1分数分别为62.69％和81.02％。研究最终公开发布了数据集和代码，以推动法律自然语言处理领域的进一步研究。

    

    在这项研究中，我们专注于两个主要任务，第一个是检测非结构化文本数据中的法律违规行为，第二个是将这些违规行为与可能受影响的个人关联起来。我们使用大语言模型（LLMs）构建了两个数据集，并由领域专家注释进行验证。两个任务都是为集体诉讼案情境特别设计的。实验设计采用了来自BERT系列和开源LLMs的微调模型，并使用闭源LLMs进行了少样本实验。我们的结果表明，我们的数据集和设置可用于这两个任务，其违规行为识别的F1分数为62.69％，与受害者相关的分数为81.02％。最后，我们公开发布了用于实验的数据集和代码，以推动法律自然语言处理（NLP）领域的进一步研究。

    In this study, we focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals. We constructed two datasets using Large Language Models (LLMs) which were subsequently validated by domain expert annotators. Both tasks were designed specifically for the context of class-action cases. The experimental design incorporated fine-tuning models from the BERT family and open-source LLMs, and conducting few-shot experiments using closed-source LLMs. Our results, with an F1-score of 62.69\% (violation identification) and 81.02\% (associating victims), show that our datasets and setups can be used for both tasks. Finally, we publicly release the datasets and the code used for the experiments in order to advance further research in the area of legal natural language processing (NLP).
    
[^149]: LESS：用于目标指导调整的选择有影响力的数据

    LESS: Selecting Influential Data for Targeted Instruction Tuning

    [https://arxiv.org/abs/2402.04333](https://arxiv.org/abs/2402.04333)

    LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。

    

    指令调整已经在大型语言模型中释放出强大的能力，有效地使用组合数据集来开发通用聊天机器人。然而，实际应用往往需要一套专门的技能（例如推理）。挑战在于从这些广泛的数据集中识别出最相关的数据，以有效开发特定的能力，我们将这种情况称为目标指导调整。我们提出了LESS，一种优化感知且实际高效的算法，以有效估计数据影响并执行适用于指令数据选择的低秩梯度相似性搜索。关键在于LESS将现有的影响公式调整为与Adam优化器和可变长度指令数据一起工作。LESS首先构建了一个具有低维梯度特征的高度可重用和可传递的梯度数据存储库，然后根据它们与具有特定能力的少样本示例的相似度选择示例。实验证明，t

    Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
    
[^150]: 使用心电图谱和深度学习识别人格特征

    Personality Trait Recognition using ECG Spectrograms and Deep Learning

    [https://arxiv.org/abs/2402.04326](https://arxiv.org/abs/2402.04326)

    通过应用深度学习方法和心电图谱，本研究创新性地实现了对人格特征的识别。研究使用心电图谱作为特征，在卷积神经网络和可视化变换器的引导下，实现了准确的人格特征分类。这对人格分析和心理研究领域具有重要的贡献。

    

    本文提出了一种创新的方法，利用深度学习方法应用于心电图(ECG)信号来识别人格特征。在大五人格特征模型的框架下，包括外向性、神经质、宜人性、责任心和开放性，研究探讨了心电图谱作为信息特征的潜力。确定了生成心电图谱的最佳窗口大小，并采用了卷积神经网络(CNN)，具体为Resnet-18和可视化变换器(ViT)进行特征提取和人格特征分类。该研究利用了公开可得的ASCE RTAIN 数据集，该数据集包含来自58个参与者的各种生理信号，包括心电图记录，在呈现被欢愉和唤起水平分类的视频刺激期间收集。本研究的结果表明在人格特征分类方面取得了显著的性能，始终保持

    This paper presents an innovative approach to recognizing personality traits using deep learning (DL) methods applied to electrocardiogram (ECG) signals. Within the framework of detecting the big five personality traits model encompassing extra-version, neuroticism, agreeableness, conscientiousness, and openness, the research explores the potential of ECG-derived spectrograms as informative features. Optimal window sizes for spectrogram generation are determined, and a convolutional neural network (CNN), specifically Resnet-18, and visual transformer (ViT) are employed for feature extraction and personality trait classification. The study utilizes the publicly available ASCERTAIN dataset, which comprises various physiological signals, including ECG recordings, collected from 58 participants during the presentation of video stimuli categorized by valence and arousal levels. The outcomes of this study demonstrate noteworthy performance in personality trait classification, consistently ac
    
[^151]: 通过向非关键神经元注入噪音增强DNN对抗性鲁棒性和效率

    Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to Non-Essential Neurons

    [https://arxiv.org/abs/2402.04325](https://arxiv.org/abs/2402.04325)

    本文提出了一种有效的方法，通过向非关键神经元注入噪音来增强DNN的对抗鲁棒性和执行效率。与以往的方法不同，我们通过在每个DNN层上策略性地引入噪音来干扰对抗攻击。实验结果表明，我们的方法成功地提高了对抗鲁棒性和执行效率。

    

    深度神经网络（DNN）通过在数据分析和决策方面提供无与伦比的能力，已经彻底改变了许多行业，从医疗和金融到汽车。尽管其具有的转型影响，DNN面临着两个关键挑战：对于对抗攻击的脆弱性和与更复杂和更大模型相关的计算成本的增加。本文介绍了一种有效的方法，旨在同时增强对抗鲁棒性和执行效率。与以往通过均匀注入噪音以增强鲁棒性的研究不同，我们引入了一种非均匀噪音注入算法， strategically applied at每一个DNN layer，以干扰攻击中引入的对抗扰动。通过采用近似技术，我们的方法识别并保护关键神经元，同时对非关键神经元策略性地引入噪音。我们的实验结果证明，我们的方法成功地增强了对抗鲁棒性和执行效率。

    Deep Neural Networks (DNNs) have revolutionized a wide range of industries, from healthcare and finance to automotive, by offering unparalleled capabilities in data analysis and decision-making. Despite their transforming impact, DNNs face two critical challenges: the vulnerability to adversarial attacks and the increasing computational costs associated with more complex and larger models. In this paper, we introduce an effective method designed to simultaneously enhance adversarial robustness and execution efficiency. Unlike prior studies that enhance robustness via uniformly injecting noise, we introduce a non-uniform noise injection algorithm, strategically applied at each DNN layer to disrupt adversarial perturbations introduced in attacks. By employing approximation techniques, our approach identifies and protects essential neurons while strategically introducing noise into non-essential neurons. Our experimental results demonstrate that our method successfully enhances both robus
    
[^152]: 深度光子计数计算机断层扫描深度学习应用综述

    Deep PCCT: Photon Counting Computed Tomography Deep Learning Applications Review

    [https://arxiv.org/abs/2402.04301](https://arxiv.org/abs/2402.04301)

    深度光子计数计算机断层扫描（PCCT）是应对医学成像的挑战的创新技术，具有提高乳腺微小异常检测的疗效和细节水平的潜力。此外，PCCT还集成了深度学习和影像组学特征的研究，成功应用于数据处理。PCCT面临挑战并需进一步发展。

    

    医学成像面临空间分辨率有限、电子噪声干扰和对比度噪声比差的挑战。光子计数计算机断层扫描（PCCT）作为一种创新技术解决了这些问题，本综述深入探讨了PCCT在临床前研究中的最新发展和应用，强调其克服传统成像限制的潜力。例如，PCCT在改善乳腺微小异常检测方面表现出了显著的疗效，提供了以前难以达到的细节水平。通过分析当前关于PCCT的文献，综述呈现了该技术的综合分析，突出了扫描仪的主要特点及其各种应用。此外，还探讨了深度学习与PCCT的集成以及影像组学特征的研究，并呈现了在数据处理方面的成功应用。同时，综述还讨论了这些进展的同时，探讨了PCCT所面临的挑战和未来需求。

    Medical imaging faces challenges such as limited spatial resolution, interference from electronic noise and poor contrast-to-noise ratios. Photon Counting Computed Tomography (PCCT) has emerged as a solution, addressing these issues with its innovative technology. This review delves into the recent developments and applications of PCCT in pre-clinical research, emphasizing its potential to overcome traditional imaging limitations. For example PCCT has demonstrated remarkable efficacy in improving the detection of subtle abnormalities in breast, providing a level of detail previously unattainable. Examining the current literature on PCCT, it presents a comprehensive analysis of the technology, highlighting the main features of scanners and their varied applications. In addition, it explores the integration of deep learning into PCCT, along with the study of radiomic features, presenting successful applications in data processing. While acknowledging these advances, it also discusses the
    
[^153]: 多视角符号回归

    Multi-View Symbolic Regression

    [https://arxiv.org/abs/2402.04298](https://arxiv.org/abs/2402.04298)

    多视角符号回归(MvSR)是一种同时考虑多个数据集的符号回归方法，能够找到一个参数化解来准确拟合所有数据集，解决了传统方法无法处理不同实验设置的问题。

    

    符号回归(SR)搜索表示解释变量和响应变量之间关系的分析表达式。目前的SR方法假设从单个实验中提取的单个数据集。然而，研究人员经常面临来自不同设置的多个实验结果集。传统的SR方法可能无法找到潜在的表达式，因为每个实验的参数可能不同。在这项工作中，我们提出了多视角符号回归(MvSR)，它同时考虑多个数据集，模拟实验环境，并输出一个通用的参数化解。这种方法将评估的表达式适应每个独立数据集，并同时返回能够准确拟合所有数据集的参数函数族f(x; \theta)。我们使用从已知表达式生成的数据以及来自实际世界的数据来展示MvSR的有效性。

    Symbolic regression (SR) searches for analytical expressions representing the relationship between a set of explanatory and response variables. Current SR methods assume a single dataset extracted from a single experiment. Nevertheless, frequently, the researcher is confronted with multiple sets of results obtained from experiments conducted with different setups. Traditional SR methods may fail to find the underlying expression since the parameters of each experiment can be different. In this work we present Multi-View Symbolic Regression (MvSR), which takes into account multiple datasets simultaneously, mimicking experimental environments, and outputs a general parametric solution. This approach fits the evaluated expression to each independent dataset and returns a parametric family of functions f(x; \theta) simultaneously capable of accurately fitting all datasets. We demonstrate the effectiveness of MvSR using data generated from known expressions, as well as real-world data from 
    
[^154]: 光HGNN：将超图神经网络蒸馏成MLPs，推断速度提升100倍

    LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\times$ Faster Inference

    [https://arxiv.org/abs/2402.04296](https://arxiv.org/abs/2402.04296)

    本论文介绍了一种光HGNN方法，将超图神经网络(HGNNs)转化为Multi-Layer Perceptron (MLPs)以提高推断速度。LightHGNN通过软标签将知识从teacher HGNN蒸馏到student MLPs，而LightHGNN$^+$则注入了可靠的高阶相关性。

    

    最近，由于其在高阶相关性建模方面的优势，超图神经网络(HGNNs)引起了广泛关注并展现了令人满意的性能。然而，我们注意到，超图的高阶建模能力也带来了增加的计算复杂性，这阻碍了其在实际工业部署中的应用。实际上，我们发现HGNNs高阶结构依赖在推断过程中是高效部署的一个关键障碍。在本文中，我们提出了将HGNNs和高效推断的多层感知器(MLPs)联系起来，以消除HGNNs的超图依赖性，从而降低计算复杂性并改善推断速度。具体而言，我们引入了LightHGNN和LightHGNN$^+$，以实现快速推断和低复杂性。LightHGNN通过软标签将知识直接从teacher HGNN蒸馏到student MLPs中，而LightHGNN$^+$则进一步显式地将可靠的高阶相关性注入其中。

    Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment. In practice, we find that one key barrier to the efficient deployment of HGNNs is the high-order structural dependencies during inference. In this paper, we propose to bridge the gap between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to eliminate the hypergraph dependency of HGNNs and thus reduce computational complexity as well as improve inference speed. Specifically, we introduce LightHGNN and LightHGNN$^+$ for fast inference with low complexity. LightHGNN directly distills the knowledge from teacher HGNNs to student MLPs via soft labels, and LightHGNN$^+$ further explicitly injects reliable high-order correlations into 
    
[^155]: AdaFlow: 变异自适应流策略的模仿学习

    AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies

    [https://arxiv.org/abs/2402.04292](https://arxiv.org/abs/2402.04292)

    AdaFlow是一个基于流模型的模仿学习框架，通过使用变异自适应ODE求解器，在保持多样性的同时，提供快速推理能力。

    

    基于扩散的模仿学习在多模态决策中改进了行为克隆（BC），但由于扩散过程中的递归而导致推理速度显著减慢。这促使我们设计高效的策略生成器，同时保持生成多样化动作的能力。为了解决这个挑战，我们提出了AdaFlow，这是一个基于流模型的模仿学习框架。AdaFlow使用状态条件的常微分方程（ODE）表示策略，这被称为概率流。我们揭示了它们训练损失的条件方差与ODE的离散化误差之间的有趣关系。基于这个观察，我们提出了一个变异自适应ODE求解器，在推理阶段可以调整步长，使AdaFlow成为一个自适应决策者，能够快速推理而不牺牲多样性。有趣的是，当动作分布被降低到一步生成器时，它自动退化到一个一步生成器。

    Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making, but comes at the cost of significantly slower inference due to the recursion in the diffusion process. It urges us to design efficient policy generators while keeping the ability to generate diverse actions. To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling. AdaFlow represents the policy with state-conditioned ordinary differential equations (ODEs), which are known as probability flows. We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs. With this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making AdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity. Interestingly, it automatically reduces to a one-step generator when the action distribution i
    
[^156]: BiLLM: 推动LLMs的后训练量化极限

    BiLLM: Pushing the Limit of Post-Training Quantization for LLMs

    [https://arxiv.org/abs/2402.04291](https://arxiv.org/abs/2402.04291)

    BiLLM是一种针对预训练LLMs的1位后训练量化方案，通过识别重要的权重和优化二值化，成功实现了高准确度的推理。

    

    预训练的大型语言模型（LLMs）具有出色的通用语言处理能力，但对内存和计算资源有很大的需求。作为一种强大的压缩技术，二值化可以将模型权重极大地减少到仅1位，降低了昂贵的计算和内存需求。然而，现有的量化技术在超低位宽下无法保持LLM的性能。针对这一挑战，我们提出了BiLLM，这是一种针对预训练LLM定制的开创性的1位后训练量化方案。基于LLMs的权重分布，BiLLM首先识别和结构选择重要的权重，并通过有效的二值化残差逼近策略来最小化压缩损失。此外，考虑到非重要权重的钟形分布，我们提出了一种最佳分割搜索方法，以准确地将它们分组和二值化。BiLLM首次实现了高准确度的推理。

    Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy infere
    
[^157]: CasCast: 高分辨率降水即时预报的熟练级联建模

    CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling

    [https://arxiv.org/abs/2402.04290](https://arxiv.org/abs/2402.04290)

    CasCast是一个高分辨率降水即时预测框架，通过熟练的级联建模解决了复杂降水系统演化和极端降水预测两个关键挑战。

    

    基于雷达数据的降水即时预报在极端天气预测和灾害管理中起着关键作用，并具有广泛的影响。尽管基于深度学习已经取得了进展，但降水即时预报面临两个关键挑战尚未得到很好解决：（i）对具有不同尺度的复杂降水系统演变进行建模，以及（ii）对极端降水进行准确预测。在这项工作中，我们提出了CasCast，一个由确定性部分和概率性部分组成的级联框架，将中尺度降水分布和小尺度模式的预测解耦。然后，我们探索在高分辨率下训练级联框架，并在低维潜在空间中进行概率性建模，利用面向帧的引导扩散变换器增强极端事件的优化，并减少计算成本。在三个基准雷达降水数据集上的大量实验证明，CasCast取得了竞争性的效果。

    Precipitation nowcasting based on radar data plays a crucial role in extreme weather prediction and has broad implications for disaster management. Despite progresses have been made based on deep learning, two key challenges of precipitation nowcasting are not well-solved: (i) the modeling of complex precipitation system evolutions with different scales, and (ii) accurate forecasts for extreme precipitation. In this work, we propose CasCast, a cascaded framework composed of a deterministic and a probabilistic part to decouple the predictions for mesoscale precipitation distributions and small-scale patterns. Then, we explore training the cascaded framework at the high resolution and conducting the probabilistic modeling in a low dimensional latent space with a frame-wise-guided diffusion transformer for enhancing the optimization of extreme events while reducing computational costs. Extensive experiments on three benchmark radar precipitation datasets show that CasCast achieves competi
    
[^158]: 生物信息学中基础模型的进展与机遇

    Progress and Opportunities of Foundation Models in Bioinformatics

    [https://arxiv.org/abs/2402.04286](https://arxiv.org/abs/2402.04286)

    生物信息学中的基础模型（FMs）通过整合人工智能技术，解决了注释数据稀缺和数据噪声等挑战，在处理大规模无标签数据和有效表示多样化生物实体方面具有出色成果，在计算生物学领域开启了新时代。

    

    生物信息学在人工智能（AI）的日益整合下，特别是通过基础模型（FMs）的采用，经历了一个范式转变。这些AI技术迅速发展，解决了生物信息学中历史性的挑战，如注释数据的稀缺性和数据噪声的存在。FMs特别擅长处理大规模的无标签数据，在生物学背景下这是常见的情况，因为实验确定标记数据的过程费时费力。这一特性使FMs在各种下游验证任务中取得了突出的成果，展现了它们有效地表示多样化生物实体的能力。毫无疑问，FMs在计算生物学，尤其是深度学习领域，开启了一个新时代。本综述的主要目标是对生物信息学中的FMs进行系统调查和总结，追溯其演变和当前研究状态。

    Bioinformatics has witnessed a paradigm shift with the increasing integration of artificial intelligence (AI), particularly through the adoption of foundation models (FMs). These AI techniques have rapidly advanced, addressing historical challenges in bioinformatics such as the scarcity of annotated data and the presence of data noise. FMs are particularly adept at handling large-scale, unlabeled data, a common scenario in biological contexts due to the time-consuming and costly nature of experimentally determining labeled data. This characteristic has allowed FMs to excel and achieve notable results in various downstream validation tasks, demonstrating their ability to represent diverse biological entities effectively. Undoubtedly, FMs have ushered in a new era in computational biology, especially in the realm of deep learning. The primary goal of this survey is to conduct a systematic investigation and summary of FMs in bioinformatics, tracing their evolution, current research status
    
[^159]: PRES: 实现可扩展的基于内存的动态图神经网络

    PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks

    [https://arxiv.org/abs/2402.04284](https://arxiv.org/abs/2402.04284)

    这篇论文研究了如何实现可扩展的基于内存的动态图神经网络，并解决了训练中的时间间断问题。通过使用内存模块提取和记忆长期的时间依赖关系，MDGNNs在处理大的时间批量时表现出更好的性能和灵活性。

    

    基于内存的动态图神经网络（MDGNNs）是一类动态图神经网络，利用内存模块提取、提炼和记忆长期的时间依赖关系，相比于无内存的对应物，表现出更卓越的性能。然而，训练MDGNNs面临着处理纠结的时间和结构依赖关系的挑战，需要对数据序列进行顺序和时间顺序的处理，以捕捉准确的时间模式。在批量训练中，同一批次内的时间数据点将被并行处理，而它们的时间依赖关系将被忽视。这个问题被称为时间间断，限制了有效的时间批量大小，限制了数据的并行性，并降低了MDGNNs在工业应用中的灵活性。本文研究了MDGNNs的大规模高效训练，重点关注在大的时间批量大小下训练MDGNNs时的时间间断问题。我们首先进行了理论工作。

    Memory-based Dynamic Graph Neural Networks (MDGNNs) are a family of dynamic graph neural networks that leverage a memory module to extract, distill, and memorize long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, training MDGNNs faces the challenge of handling entangled temporal and structural dependencies, requiring sequential and chronological processing of data sequences to capture accurate temporal patterns. During the batch training, the temporal data points within the same batch will be processed in parallel, while their temporal dependencies are neglected. This issue is referred to as temporal discontinuity and restricts the effective temporal batch size, limiting data parallelism and reducing MDGNNs' flexibility in industrial applications. This paper studies the efficient training of MDGNNs at scale, focusing on the temporal discontinuity in training MDGNNs with large temporal batch sizes. We first conduct a theoretic
    
[^160]: 用于电子密度估计的高斯平面波神经算子

    Gaussian Plane-Wave Neural Operator for Electron Density Estimation

    [https://arxiv.org/abs/2402.04278](https://arxiv.org/abs/2402.04278)

    本研究提出了一种名为高斯平面波神经算子(GPWNO)的方法，用于机器学习中的电子密度预测。该方法利用平面波和高斯型轨道基底在无限维功能空间中进行操作，可以有效地表示密度的高频和低频成分，并在多个数据集上展示出卓越的性能。

    

    本研究探讨了用于电子密度预测的机器学习方法，这对于理解化学系统和密度泛函理论(DFT)模拟是基础性的。为此，我们引入了高斯平面波神经算子(GPWNO)，它在无限维功能空间中使用平面波和高斯型轨道基底进行操作，在DFT的背景中得到广泛认可。特别地，由于两种基底的互补性质，密度的高频和低频成分都可以被有效地表示。对QM9、MD和材料项目数据集进行的大量实验表明，GPWNO相比其他十种基线方法具有优越的性能。

    This work studies machine learning for electron density prediction, which is fundamental for understanding chemical systems and density functional theory (DFT) simulations. To this end, we introduce the Gaussian plane-wave neural operator (GPWNO), which operates in the infinite-dimensional functional space using the plane-wave and Gaussian-type orbital bases, widely recognized in the context of DFT. In particular, both high- and low-frequency components of the density can be effectively represented due to the complementary nature of the two bases. Extensive experiments on QM9, MD, and material project datasets demonstrate GPWNO's superior performance over ten baselines.
    
[^161]: 用于实时神经科学实验的LFADS的FPGA部署

    FPGA Deployment of LFADS for Real-time Neuroscience Experiments

    [https://arxiv.org/abs/2402.04274](https://arxiv.org/abs/2402.04274)

    通过将LFADS模型有效地部署到FPGA上，我们实现了处理大规模神经活动数据的低延迟推断，为实时神经科学实验提供了新的机会。

    

    大规模记录神经活动为研究神经人群动力学提供了新机会。分析这种高维度测量的强大方法是部署学习低维度潜在动态的算法。LFADS（通过动态系统进行潜在因素分析）是一种深度学习方法，用于推断从同时记录的高维神经尖峰数据中的潜在动态。该方法在模拟复杂的脑信号方面表现出了卓越的性能，平均推断延迟为毫秒级。随着同时记录许多神经元的能力以指数级增加，构建低延迟推断计算算法的能力变得至关重要。为了提高LFADS的实时处理能力，我们将LFADS模型有效地实现到了可编程逻辑门阵列（FPGA）上。我们的实现显示出41.97 $\mu$s的推断延迟，用于处理大规模神经活动数据。

    Large-scale recordings of neural activity are providing new opportunities to study neural population dynamics. A powerful method for analyzing such high-dimensional measurements is to deploy an algorithm to learn the low-dimensional latent dynamics. LFADS (Latent Factor Analysis via Dynamical Systems) is a deep learning method for inferring latent dynamics from high-dimensional neural spiking data recorded simultaneously in single trials. This method has shown a remarkable performance in modeling complex brain signals with an average inference latency in milliseconds. As our capacity of simultaneously recording many neurons is increasing exponentially, it is becoming crucial to build capacity for deploying low-latency inference of the computing algorithms. To improve the real-time processing ability of LFADS, we introduce an efficient implementation of the LFADS models onto Field Programmable Gate Arrays (FPGA). Our implementation shows an inference latency of 41.97 $\mu$s for processi
    
[^162]: 打破数据孤岛：跨领域学习实现独立私有源的多智能体感知

    Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources

    [https://arxiv.org/abs/2402.04273](https://arxiv.org/abs/2402.04273)

    本文提出了一种名为FDA的框架，通过跨领域学习来打破多智能体感知中的数据孤岛问题。该框架包括可学习特征补偿模块和分布感知统计一致性模块，用于增强中间特征交流和数据分布一致性。

    

    多智能体感知系统中的不同智能体可能来自不同公司。每个公司可能都使用相同的经典神经网络结构的编码器进行特征提取。然而，训练各个智能体的数据源在每个公司中是相互独立和私有的，导致多智能体感知系统中训练不同智能体的不同私有数据的分布差异。上述分布差异造成的数据孤岛可能导致多智能体感知性能明显下降。本文深入研究了分布差异对现有多智能体感知系统的影响。为了打破数据孤岛，我们引入了面向跨领域学习的特征分布感知聚合（FDA）框架，以减轻多智能体感知中的分布差异。FDA包括两个关键组成部分：可学习特征补偿模块和分布感知统计一致性模块，旨在加强中间特征交流和数据分布一致性。

    The diverse agents in multi-agent perception systems may be from different companies. Each company might use the identical classic neural network architecture based encoder for feature extraction. However, the data source to train the various agents is independent and private in each company, leading to the Distribution Gap of different private data for training distinct agents in multi-agent perception system. The data silos by the above Distribution Gap could result in a significant performance decline in multi-agent perception. In this paper, we thoroughly examine the impact of the distribution gap on existing multi-agent perception systems. To break the data silos, we introduce the Feature Distribution-aware Aggregation (FDA) framework for cross-domain learning to mitigate the above Distribution Gap in multi-agent perception. FDA comprises two key components: Learnable Feature Compensation Module and Distribution-aware Statistical Consistency Module, both aimed at enhancing interme
    
[^163]: Tigrigna大词汇量自发语音识别

    Large Vocabulary Spontaneous Speech Recognition for Tigrigna

    [https://arxiv.org/abs/2402.04254](https://arxiv.org/abs/2402.04254)

    本文研究了一种Tigrigna语言的大词汇量自发语音识别系统的设计与开发，通过使用Sphinx工具开发声学模型和使用SRIM工具开发语言模型。这项研究的创新点是实现了对Tigrigna语言的自动语音识别。

    

    本论文提出并描述了一种设计和开发提格利尼亚语中独立于发言者的自发自动语音识别系统的研究尝试。该语音识别系统的声学模型采用卡内基梅隆大学自动语音识别开发工具（Sphinx）开发，而语言模型的开发则采用SRIM工具。

    This thesis proposes and describes a research attempt at designing and developing a speaker independent spontaneous automatic speech recognition system for Tigrigna The acoustic model of the Speech Recognition System is developed using Carnegie Mellon University Automatic Speech Recognition development tool (Sphinx) while the SRIM tool is used for the development of the language model.   Keywords Automatic Speech Recognition Tigrigna language
    
[^164]: 优先安全保障而非自治：科学中LLM智能机器人的风险

    Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science

    [https://arxiv.org/abs/2402.04247](https://arxiv.org/abs/2402.04247)

    本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。

    

    由大型语言模型（LLMs）驱动的智能机器人在各个学科中自主进行实验和促进科学发现方面展示了巨大的前景。尽管它们的能力非常有前途，但也引入了一些新的漏洞，需要仔细考虑安全性。然而，文献中存在显著的空白，尚未对这些漏洞进行全面探讨。本文通过对科学领域中基于LLM的机器人的漏洞进行深入研究，揭示了它们误用可能带来的潜在风险，并强调了对安全措施的需求，填补了这一空白。我们首先全面概述了科学LLM机器人固有的潜在风险，考虑了用户意图、特定的科学领域以及它们对外部环境可能造成的影响。然后，我们深入探讨了这些漏洞的起源和提供的解决方案。

    Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
    
[^165]: 避免延迟节点的分散式学习中的梯度编码

    Gradient Coding in Decentralized Learning for Evading Stragglers

    [https://arxiv.org/abs/2402.04193](https://arxiv.org/abs/2402.04193)

    本文提出了一种新的基于八卦的梯度编码分散式学习方法（GOCO），以解决存在延迟节点的分散式学习问题。与基准方法相比，该方法在学习性能上具有优越性。

    

    本文考虑了存在延迟节点的分散式学习问题。尽管梯度编码技术已经被开发用于分布式学习以避免延迟节点，即设备使用冗余训练数据发送编码梯度，但是将这些技术直接应用于分散式学习场景是困难的。为了解决这个问题，我们提出了一种新的基于八卦的梯度编码分散式学习方法（GOCO）。在这种方法中，为了避免延迟节点的负面影响，参数向量使用基于随机梯度编码框架的编码梯度进行本地更新，然后以八卦方式进行平均。我们分析了GOCO在强凸损失函数下的收敛性能，并通过仿真结果证明了该方法在学习性能上相对于基准方法的优越性。

    In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.
    
[^166]: 一种从状态空间模型到紧支持基的核分组的通用理论

    A General Theory for Kernel Packets: from state space model to compactly supported basis

    [https://arxiv.org/abs/2402.04022](https://arxiv.org/abs/2402.04022)

    该论文提出了一种从状态空间模型到紧支持基的核分组的通用理论，该理论可以用于降低高斯过程的训练和预测时间，并且通过适当的线性组合产生了$m$个紧支持的核分组函数。

    

    众所周知，高斯过程（GP）的状态空间（SS）模型公式可以将其训练和预测时间降低到O（n）（n为数据点个数）。我们证明了一个m维的GP的SS模型公式等价于我们引入的一个概念，称为通用右核分组（KP）：一种用于GP协方差函数K的变换，使得对于任意$t \leq t_1$，$0 \leq j \leq m-1$和$m+1$个连续点$t_i$，都满足$\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$，其中${D}_t^{(j)}f(t)$表示在$t$上作用的第j阶导数。我们将这个思想扩展到了GP的向后SS模型公式，得到了下一个$m$个连续点的左核分组的概念：$\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$，对于任意$t\geq t_{2m}$。通过结合左右核分组，可以证明这些协方差函数的适当线性组合产生了$m$个紧支持的核分组函数：对于任意$t\not\in(t_0,t_{2m})$和$j=0,\cdots,m-1$，$\phi^{(j)}(t)=0$。

    It is well known that the state space (SS) model formulation of a Gaussian process (GP) can lower its training and prediction time both to O(n) for n data points. We prove that an $m$-dimensional SS model formulation of GP is equivalent to a concept we introduce as the general right Kernel Packet (KP): a transformation for the GP covariance function $K$ such that $\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j \leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. We extend this idea to the backward SS model formulation of the GP, leading to the concept of the left KP for next $m$ consecutive points: $\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\geq t_{2m}$. By combining both left and right KPs, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported KP functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and $j=0,\cdots,m-1$
    
[^167]: 交叉熵与标签平滑：神经崩溃的视角

    Cross Entropy versus Label Smoothing: A Neural Collapse Perspective

    [https://arxiv.org/abs/2402.03979](https://arxiv.org/abs/2402.03979)

    本研究从神经崩溃的视角研究了标签平滑，并发现模型在标签平滑训练下更快地收敛到神经崩溃解，并达到更强的神经崩溃水平。此外，标签平滑损失下的模型在相同的NC1水平下表现出加强的NC2，并可在理论上更快地收敛。

    

    标签平滑损失是深度神经网络中广泛采用的一种技术，用于减轻过拟合。本文从神经崩溃（NC）的视角研究了标签平滑，这是一个强大的经验和理论框架，用于描述训练的最后阶段模型的行为。首先，我们通过实验证明，在标签平滑训练的模型更快地收敛到神经崩溃解，并达到更强的神经崩溃水平。此外，我们还表明，在相同的NC1水平下，标签平滑损失下的模型显示出加强的NC2。这些发现为理解标签平滑损失下的性能优势和增强的模型校准提供了有价值的见解。然后，我们利用无约束特征模型推导出两种损失函数的全局最小值的闭式解，并进一步证明了标签平滑下的模型具有较低的条件数，因此在理论上更快地收敛。我们的研究综合了经验和理论的方法，为理解标签平滑的效果提供了重要的贡献。

    Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empiri
    
[^168]: AirPhyNet: 利用物理引导的神经网络预测空气质量

    AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction

    [https://arxiv.org/abs/2402.03784](https://arxiv.org/abs/2402.03784)

    AirPhyNet是一种利用物理引导的神经网络方法，通过将空气颗粒运动的物理原理和神经网络结构相结合，提高了对空气质量的预测精度和可解释性。

    

    空气质量预测和建模在公共卫生和环境管理中起着关键作用，帮助个人和当局做出明智决策。尽管传统的数据驱动模型在这个领域已经显示出了潜力，但它们在长期预测精度上可能受到限制，特别是在数据稀疏或不完整的情况下，它们往往依赖于缺乏坚实物理基础的黑盒深度学习结构，导致预测的透明度和可解释性降低。为了解决这些限制，本文提出了一种名为Physics guided Neural Network for Air Quality Prediction（AirPhyNet）的新方法。具体而言，我们利用空气颗粒运动的两个成熟的物理原理（扩散和平流）将其表示为微分方程网络。然后，我们利用图结构将物理知识融入神经网络架构，并利用潜在表示来捕捉时空关系。

    Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships
    
[^169]: 揭示宣传：基于人类注释和机器分类的文体线索比较分析

    Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification

    [https://arxiv.org/abs/2402.03780](https://arxiv.org/abs/2402.03780)

    本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。

    

    本文研究了宣传语言及其文体特征。提出了PPN数据集，即宣传性伪新闻数据集，它是一种多源、多语言、多模态的数据集，由专家机构确定的宣传来源网站上的新闻文章组成。从该数据集中随机选择了一部分样本与来自常规法国新闻的文章混合，并对它们的URL进行了掩盖，以进行人类注释实验，使用11个不同的标签。结果显示，人类注释者能够可靠地区分两种类型的新闻纸对每个标签。我们提出了不同的自然语言处理技术来识别注释者使用的线索，并与机器分类进行比较。其中包括使用VAGO分析器进行辞述模糊和主观性的测量，使用TF-IDF作为基准，以及四种不同的分类器：两个基于RoBERTa模型的模型，使用句法的CATS，以及结合句法和语义特征的一个XGBoost模型。

    This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
    
[^170]: 使用自反大型语言模型学习生成可解释的股票预测

    Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models

    [https://arxiv.org/abs/2402.03659](https://arxiv.org/abs/2402.03659)

    这个论文介绍了使用大型语言模型生成可解释的股票预测的方法，并提出了Summarize-Explain-Predict（SEP）模型来解决股票预测中的解释问题和数据标注成本的挑战。

    

    对于传统的非生成式深度学习模型来说，解释股票预测通常是一项困难的任务，其中解释仅限于可视化重要文本上的注意力权重。目前，大型语言模型（LLM）为解决这个问题提供了一个解决方案，因为它们具有生成人类可读解释其决策过程的能力。然而，股票预测对LLM来说仍然具有挑战性，因为它需要能够权衡混乱社会文本对股票价格的不同影响。随着引入解释组件，问题变得越来越困难，需要LLM能够用口头方式解释为什么某些因素比其他因素更重要。另一方面，要为这样的任务对LLM进行微调，需要专家标注的样本来解释训练集中的每次股票波动，这在成本和实际可扩展性上是昂贵且不可行的。为了解决这些问题，我们提出了我们的Summarize-Explain-Predict（SEP）模型。

    Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) 
    
[^171]: MQuinE:知识图谱嵌入模型中“Z-悖论”的解决方案

    MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models

    [https://arxiv.org/abs/2402.03583](https://arxiv.org/abs/2402.03583)

    研究者发现知识图谱嵌入模型存在的“Z-悖论”限制了其表达能力，并提出了一种名为MQuinE的新模型，通过理论证明，MQuinE成功解决了Z-悖论，并在链接预测任务中显著优于现有模型。

    

    知识图谱嵌入（KGE）模型在许多知识图谱任务，包括链接预测和信息检索方面取得了最先进的结果。尽管KGE模型在实践中表现出优越性能，但我们发现一些流行的现有KGE模型存在表达不足的问题，称为“Z-悖论”。受到Z-悖论的存在的启发，我们提出了一种新的KGE模型，称为MQuinE，在不受Z-悖论的困扰的同时，保持强大的表达能力来模拟各种关系模式，包括对称/非对称，逆向，1-N/N-1/N-N和组合关系，并提供了理论上的证明。对实际知识库的实验表明，Z-悖论确实降低了现有KGE模型的性能，并且可能导致某些具有挑战性的测试样本的准确性下降超过20％。我们的实验进一步证明了MQuinE可以减轻Z-悖论的负面影响，并在链接预测方面以明显优势超越现有的KGE模型。

    Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction
    
[^172]: 探索质数分类：使用稀疏编码实现高召回率和快速收敛

    Exploring Prime Number Classification: Achieving High Recall Rate and Rapid Convergence with Sparse Encoding

    [https://arxiv.org/abs/2402.03363](https://arxiv.org/abs/2402.03363)

    通过稀疏编码和神经网络结构的组合，本文提出了一种在质数和非质数分类中实现高召回率和快速收敛的新方法，取得了令人满意的结果。

    

    本文提出了一种新颖的方法，结合机器学习和数论，在质数和非质数分类上进行研究。我们的研究核心是开发一种高度稀疏的编码方法，与传统的神经网络结构相结合。这种组合取得了令人满意的结果，在识别质数时达到了超过99\%的召回率，在识别非质数时达到了79\%的召回率，这些数字是从本质上不平衡的顺序整数序列中得出的，并且在完成单个训练周期之前迅速收敛。我们使用 $10^6$ 个整数进行训练，从指定的整数开始，然后在一个不同范围的 $2 \times 10^6$ 个整数上进行测试，范围从 $10^6$ 到 $3 \times 10^6$，偏移量相同。尽管受限于资源的内存容量，限制我们的分析跨越了 $3\times10^6$，但我们认为我们的研究对机器学习在......的应用做出了贡献

    This paper presents a novel approach at the intersection of machine learning and number theory, focusing on the classification of prime and non-prime numbers. At the core of our research is the development of a highly sparse encoding method, integrated with conventional neural network architectures. This combination has shown promising results, achieving a recall of over 99\% in identifying prime numbers and 79\% for non-prime numbers from an inherently imbalanced sequential series of integers, while exhibiting rapid model convergence before the completion of a single training epoch. We performed training using $10^6$ integers starting from a specified integer and tested on a different range of $2 \times 10^6$ integers extending from $10^6$ to $3 \times 10^6$, offset by the same starting integer. While constrained by the memory capacity of our resources, which limited our analysis to a span of $3\times10^6$, we believe that our study contribute to the application of machine learning in
    
[^173]: 图缩减的综合调研：稀疏化、粗化和浓缩

    A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation

    [https://arxiv.org/abs/2402.03358](https://arxiv.org/abs/2402.03358)

    这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。

    

    许多真实世界的数据集可以自然地表示为图，涵盖了广泛的领域。然而，图数据集的复杂性和规模的增加为分析和计算带来了显著的挑战。为此，图缩减技术在保留关键属性的同时简化大型图形数据变得越来越受关注。在本调研中，我们旨在提供对图缩减方法的全面理解，包括图稀疏化、图粗化和图浓缩。具体而言，我们建立了这些方法的统一定义，并引入了一个分层分类法来分类这些方法所解决的挑战。我们的调研系统地回顾了这些方法的技术细节，并强调了它们在各种场景中的实际应用。此外，我们还概述了保证图缩减技术持续有效性的关键研究方向，并提供了一个详细的论文列表链接。

    Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
    
[^174]: 用上下文感知修复的零样本物体级OOD检测

    Zero-shot Object-Level OOD Detection with Context-Aware Inpainting

    [https://arxiv.org/abs/2402.03292](https://arxiv.org/abs/2402.03292)

    本论文提出了一种用上下文感知修复的零样本物体级OOD检测方法RONIN。通过将检测到的对象进行修复替换，并使用预测的ID标签来条件化修复过程，使得重构的对象在OOD情况下与原始对象相差较远，从而有效区分ID和OOD样本。实验证明RONIN在多个数据集上取得了具有竞争力的结果。

    

    机器学习算法越来越多地作为黑盒云服务或预训练模型提供，无法访问它们的训练数据。这就引发了零样本离群数据（OOD）检测的问题。具体而言，我们的目标是检测不属于分类器标签集但被错误地归类为入域（ID）对象的OOD对象。我们的方法RONIN使用现成的扩散模型来用修复替换掉检测到的对象。RONIN使用预测的ID标签来条件化修复过程，使输入对象接近入域域。结果是，重构的对象在ID情况下非常接近原始对象，在OOD情况下则相差较远，使得RONIN能够有效区分ID和OOD样本。通过大量实验证明，RONIN在零样本和非零样本设置下，相对于先前方法，在多个数据集上取得了具有竞争力的结果。

    Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of zero-shot out-of-distribution (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings.
    
[^175]: 有机或扩散：我们能区分人类艺术和AI生成的图像吗？

    Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?

    [https://arxiv.org/abs/2402.03214](https://arxiv.org/abs/2402.03214)

    这项研究探讨了如何区分人类艺术和AI生成的图像，并提供了几种不同的方法，包括通过监督学习训练的分类器、扩散模型的研究工具以及专业艺术家的知识。这对防止欺诈、遵守政策以及避免模型崩溃都具有重要意义。

    

    生成AI图像的出现完全颠覆了艺术界。从人类艺术中识别AI生成的图像是一个具有挑战性的问题，其影响随着时间的推移而不断增加。未能解决这个问题会导致不良行为者欺诈那些支付高价购买人类艺术品的个人和禁止使用AI图像的公司。这对于需要过滤训练数据以避免潜在模型崩溃的AI模型训练者来说也至关重要。区分人类艺术和AI图像的方法有多种，包括通过监督学习训练的分类器，针对扩散模型的研究工具，以及通过专业艺术家利用他们对艺术技巧的知识进行识别。在本文中，我们试图了解这些方法在现代生成模型的良性和对抗性环境中的表现如何。我们策划了7种风格的真实人类艺术，从5个生成模型生成了与之匹配的图像，并应用了8个检测器。

    The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
    
[^176]: 用于动作识别的Taylor视频

    Taylor Videos for Action Recognition

    [https://arxiv.org/abs/2402.03019](https://arxiv.org/abs/2402.03019)

    Taylor视频是一种新的视频格式，用于动作识别中的动作提取问题。它通过使用Taylor展开近似计算隐含的动作提取函数，从而解决了动作提取中的挑战性问题。

    

    从视频中有效地提取动作是动作识别中一个重要且长期存在的问题。这个问题非常具有挑战性，因为动作(i)没有明确的形式，(ii)拥有诸如位移、速度和加速度等各种概念，(iii)通常会受到不稳定像素引起的噪声的干扰。为了解决这些挑战，我们提出了Taylor视频，一种新的视频格式，它突出显示了每个帧中的主要动作（例如挥手）被称为Taylor帧。Taylor视频的命名来源于Taylor级数，它使用重要的项来近似给定点上的函数。在视频的情境中，我们定义了一个隐含的动作提取函数，旨在从视频时间块中提取动作。在这个块中，我们使用帧、差分帧和高阶差分帧进行Taylor展开，以近似计算起始帧上的这个函数。我们展示了Taylor级数中高阶项的求和给我们提供了...

    Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us
    
[^177]: DS-MS-TCN: 使用双尺度多阶段时间卷积网络的Otago体操识别

    DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network

    [https://arxiv.org/abs/2402.02910](https://arxiv.org/abs/2402.02910)

    本研究提出了一种使用双尺度多阶段时间卷积网络的Otago体操识别方法，通过单个腰部佩戴的IMU在老年人的日常生活中实现准确且稳定的识别，为康复举措提供支持。

    

    Otago运动计划是针对老年人的重要康复举措，旨在增强平衡和力量。本研究利用单个腰部佩戴的惯性测量单元(IMU)，在老年人的日常生活中识别Otago体操动作，以解决现有研究在准确性和稳定性方面的限制。研究在实验室设置中招募了36名老年人，并对额外招募的7名老年人进行了家庭评估。研究提出了一种双尺度多阶段时间卷积网络(DS-MS-TCN)，用于两级序列到序列分类，将其纳入一个损失函数。在第一阶段，模型专注于识别每个体操动作的重复次数(微标签)。随后的阶段扩展了识别范围，包括完整的运动序列。

    The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength. Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness. This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives. A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments. The study proposes a Dual-Scale Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function. In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels). Subsequent stages extend the recognition to encompass the complete ra
    
[^178]: 使用图神经网络的链接预测的统计保证

    Statistical Guarantees for Link Prediction using Graph Neural Networks

    [https://arxiv.org/abs/2402.02692](https://arxiv.org/abs/2402.02692)

    本文提出了一种线性图神经网络（LG-GNN）架构，通过计算边缘概率来预测图中的链接，并推导了其在链接预测任务中的性能统计保证。这种架构对于稀疏和稠密图都适用，并在真实和合成数据集上验证了其优势。

    

    本文针对由图上生成的图网络中的链接预测任务，推导了图神经网络（GNN）性能的统计保证。我们提出了一个线性GNN架构（LG-GNN），可以产生对潜在边缘概率的一致估计。我们对均方误差进行了界定，并对LG-GNN在检测高概率边缘的能力给出了保证。我们的保证适用于稀疏和稠密图。最后，我们展示了经典GCN架构的一些缺点，并在真实和合成数据集上验证了我们的结果。

    This paper derives statistical guarantees for the performance of Graph Neural Networks (GNNs) in link prediction tasks on graphs generated by a graphon. We propose a linear GNN architecture (LG-GNN) that produces consistent estimators for the underlying edge probabilities. We establish a bound on the mean squared error and give guarantees on the ability of LG-GNN to detect high-probability edges. Our guarantees hold for both sparse and dense graphs. Finally, we demonstrate some of the shortcomings of the classical GCN architecture, as well as verify our results on real and synthetic datasets.
    
[^179]: 通过重复使用经过验证的电路增加语言模型的可信度

    Increasing Trust in Language Models through the Reuse of Verified Circuits

    [https://arxiv.org/abs/2402.02619](https://arxiv.org/abs/2402.02619)

    本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。

    

    语言模型（LMs）在各种预测任务中的应用越来越广泛，但它们的训练经常忽略罕见的边界情况，降低了它们的可靠性。在本文中，我们定义了一个严格的可信度标准，即任务算法和电路实现必须经过验证，考虑到边界情况，并且没有已知的故障模式。我们展示了通过使用数学和逻辑规范的框架来构建变压器模型，可以训练出满足这一标准的模型。在本文中，我们对一个n位整数加法模型进行了完全验证。为了展示经过验证的模块的重复使用性，我们将训练好的整数加法模型插入到一个未经训练的模型中，并训练组合模型同时执行加法和减法。我们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了更复杂的减法模型的验证。我们讨论了如何将经过验证的任务模块插入到语言模型中，以利用模型的重复使用来提高可验证性和可信度。

    Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
    
[^180]: LHRS-Bot：利用VGI增强的大型多模态语言模型赋能遥感领域

    LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model

    [https://arxiv.org/abs/2402.02544](https://arxiv.org/abs/2402.02544)

    LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    

    大型语言模型（LLMs）的革命性能力开创了多模态大型语言模型（MLLMs）并促进了在各个专业领域的多样化应用。然而，在遥感（RS）领域中，近期的MLLM努力未能充分考虑到遥感图像中多样的地理景观和物体。为了弥补这一差距，我们构建了一个大规模的RS图像-文本数据集LHRS-Align，以及一个信息丰富的RS特定指导数据集LHRS-Instruct，利用丰富的自愿地理信息（VGI）和全球可用的RS图像。在此基础上，我们引入了LHRS-Bot，一种针对RS图像理解的MLLM，通过一种新颖的多层次视觉-语言对齐策略和课程学习方法。全面的实验证明，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
    
[^181]: TopoX: 一个用于拓扑域上的机器学习的Python软件包套件

    TopoX: A Suite of Python Packages for Machine Learning on Topological Domains

    [https://arxiv.org/abs/2402.02441](https://arxiv.org/abs/2402.02441)

    TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。

    

    我们介绍了topox，一个提供可靠且用户友好的Python软件包套件，用于在拓扑域（扩展了图的领域）上进行计算和机器学习：超图、单纯、胞腔、路径和组合复合体。topox由三个软件包组成：toponetx用于构建和计算这些域，包括节点、边和高阶单元的处理；topoembedx提供了将拓扑域嵌入到向量空间的方法，类似于流行的基于图的嵌入算法，如node2vec；topomodelx建立在PyTorch之上，为拓扑域上的神经网络提供了一套全面的高阶消息传递功能工具箱。topox的源代码经过广泛的文档化和单元测试，并在https://github.com/pyt-team以MIT许可证的形式提供。

    We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
    
[^182]: Symbol:通过符号方程学习生成灵活的黑盒优化器

    Symbol: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning

    [https://arxiv.org/abs/2402.02355](https://arxiv.org/abs/2402.02355)

    本研究提出了一种名为\textsc{Symbol}的新框架，通过符号方程学习来自动发现黑盒优化器。实验结果表明，\textsc{Symbol}生成的优化器在超越现有基准线的同时，还展现出出色的零样本泛化能力。

    

    最近的MetaBBO方法利用神经网络来元学习传统黑盒优化器的配置。尽管这些方法取得了成功，但它们不可避免地受到预定义手工优化器的限制。本文中，我们提出了一种名为\textsc{Symbol}的新框架，通过符号方程学习来促进黑盒优化器的自动发现。具体而言，我们提出了一个符号方程生成器(SEG)，允许为特定任务和优化步骤动态生成闭式优化规则。在\textsc{Symbol}内部，我们基于强化学习开发了三种不同的策略，以便高效地元学习SEG。大量实验证明，\textsc{Symbol}生成的优化器不仅超越了最先进的BBO和MetaBBO基准线，而且在完全不同问题的全新任务上表现出了异常的零样本泛化能力。

    Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness neural networks to meta-learn configurations of traditional black-box optimizers. Despite their success, they are inevitably restricted by the limitations of predefined hand-crafted optimizers. In this paper, we present \textsc{Symbol}, a novel framework that promotes the automated discovery of black-box optimizers through symbolic equation learning. Specifically, we propose a Symbolic Equation Generator (SEG) that allows closed-form optimization rules to be dynamically generated for specific tasks and optimization steps. Within \textsc{Symbol}, we then develop three distinct strategies based on reinforcement learning, so as to meta-learn the SEG efficiently. Extensive experiments reveal that the optimizers generated by \textsc{Symbol} not only surpass the state-of-the-art BBO and MetaBBO baselines, but also exhibit exceptional zero-shot generalization abilities across entirely unseen tasks with different problem
    
[^183]: Riemannian Preconditioned LoRA用于基础模型微调的研究

    Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models

    [https://arxiv.org/abs/2402.02347](https://arxiv.org/abs/2402.02347)

    本研究通过引入Riemannian预条件器，增强了LoRA微调过程中的优化步骤。实验结果表明，使用该预条件器可以显著提升SGD和AdamW的收敛性和可靠性，并使训练过程更加稳健。此外，理论分析证明了在凸参数化下使用该预条件器微调ReLU网络的收敛速度与数据矩阵的条件数无关。这个新的Riemannian预条件器在经典的低秩矩阵恢复中已经有过研究。

    

    在这项工作中，我们研究了在LoRA微调过程中引入Riemannian预条件器来提升其优化步骤的效果。具体来说，我们在每个梯度步骤中引入了一个$r\times r$的预条件器，其中$r$是LoRA的秩。这个预条件器对现有的优化器代码只需要做出很小的改变，并且几乎没有存储和运行时开销。我们对大型语言模型和文本到图像扩散模型进行了实验，结果表明，使用我们的预条件器，SGD和AdamW的收敛性和可靠性都可以显著提升。此外，训练过程对于学习率等超参数的选择变得更加稳健。从理论上讲，我们证明了使用我们的预条件器在凸参数化下微调两层ReLU网络的收敛速度与数据矩阵的条件数无关。这个新的Riemannian预条件器在经典的低秩矩阵恢复中已经有过研究。

    In this work we study the enhancement of Low Rank Adaptation (LoRA) fine-tuning procedure by introducing a Riemannian preconditioner in its optimization step. Specifically, we introduce an $r\times r$ preconditioner in each gradient step where $r$ is the LoRA rank. This preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with our preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. Theoretically, we show that fine-tuning a two-layer ReLU network in the convex paramaterization with our preconditioner has convergence rate independent of condition number of the data matrix. This new Riemannian preconditioner, previously explored in classic low-rank matrix recovery, is 
    
[^184]: 由端到端深度学习模型加强的高效数值波传播

    Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep Learning Model

    [https://arxiv.org/abs/2402.02304](https://arxiv.org/abs/2402.02304)

    本文提出了一个由端到端深度学习模型加强的高效数值波传播方法，通过结合数值求解器和深度学习组件，优化算法架构、数据生成和并行时间算法，实现了在保持速度的同时显著提高性能。

    

    在多个科学和工程领域，从地震建模到医学成像，对于高频波传播的高保真和高效解决方案的需求非常重要。最近在波传播模型中的一项进展利用足够准确的细求解器输出来训练神经网络，以提高快速但不准确的粗求解器的准确性。稳定且快速的求解器还允许使用并行时间算法Parareal来提取和纠正高频波组成部分。在本文中，我们在Nguyen和Tsai（2023）的工作基础上，提出了一个新颖的统一系统，将数值求解器与深度学习组件整合到端到端框架中。在提出的设置中，我们研究了神经网络架构、数据生成算法和Parareal方案的改进。我们的结果表明，这种协调的结构在不牺牲速度的情况下显著提高了性能，并且证明了

    In a variety of scientific and engineering domains, ranging from seismic modeling to medical imaging, the need for high-fidelity and efficient solutions for high-frequency wave propagation holds great significance. Recent advances in wave modeling use sufficiently accurate fine solver outputs to train neural networks that enhance the accuracy of a fast but inaccurate coarse solver. A stable and fast solver further allows the use of Parareal, a parallel-in-time algorithm to retrieve and correct high-frequency wave components. In this paper we build upon the work of Nguyen and Tsai (2023) and present a novel unified system that integrates a numerical solver with deep learning components into an end-to-end framework. In the proposed setting, we investigate refinements to the neural network architecture, data generation algorithm and Parareal scheme. Our results show that the cohesive structure significantly improves performance without sacrificing speed, and demonstrate the importance of 
    
[^185]: 图基础模型

    Graph Foundation Models

    [https://arxiv.org/abs/2402.02216](https://arxiv.org/abs/2402.02216)

    图基础模型是图领域中一个新兴的研究课题，旨在开发一个能够跨不同图和任务进行泛化的图模型。我们提出了一种新的 GFM 发展视角，通过倡导“图词汇表”来编码图的不变性，有助于推进未来的GFM设计。

    

    图基础模型（Graph Foundation Model，GFM）是图领域中一个新兴的研究课题，旨在开发一个能够跨不同图和任务进行泛化的图模型。然而，目前还没有实现一个多功能的GFM。构建GFM的关键挑战在于如何能在具有不同结构模式的图之间实现正向迁移。受计算机视觉（CV）和自然语言处理（NLP）领域基础模型的启发，我们提出了一种新的 GFM 发展视角，通过倡导“图词汇表”，即潜藏于图中的基本可迁移单元来编码图的不变性。我们从网络分析、理论基础和稳定性等重要方面来建立图词汇表。这种词汇表的视角有助于按照神经缩放定律推进未来的GFM设计。

    Graph Foundation Model (GFM) is a new trending research topic in the graph domain, aiming to develop a graph model capable of generalizing across different graphs and tasks. However, a versatile GFM has not yet been achieved. The key challenge in building GFM is how to enable positive transfer across graphs with diverse structural patterns. Inspired by the existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, theoretical foundations, and stability. Such a vocabulary perspective can potentially advance the future GFM design following the neural scaling laws.
    
[^186]: 位置论文：超级计算研究和LLMs的现状与挑战

    Position Paper: The Landscape and Challenges of HPC Research and LLMs

    [https://arxiv.org/abs/2402.02018](https://arxiv.org/abs/2402.02018)

    运用语言模型技术于高性能计算任务中具有巨大潜力

    

    最近，语言模型（LMs），特别是大规模的语言模型（LLMs），已经彻底改变了深度学习领域。编码器-解码器模型和基于提示的技术均展现出在自然语言处理和基于代码的任务中巨大的潜力。在过去几年中，许多研究实验室和机构在高性能计算方面投入了大量资源，达到或突破了超级计算的性能水平。本文提出，将这些基于语言模型的技术调整和应用于高性能计算任务中将会非常有益。本研究阐述了我们上述观点的理由，并强调了现有想法在HPC任务中的改进和应用。

    Recently, language models (LMs), especially large language models (LLMs), have revolutionized the field of deep learning. Both encoder-decoder models and prompt-based techniques have shown immense potential for natural language processing and code-based tasks. Over the past several years, many research labs and institutions have invested heavily in high-performance computing, approaching or breaching exascale performance levels. In this paper, we posit that adapting and utilizing such language model-based techniques for tasks in high-performance computing (HPC) would be very beneficial. This study presents our reasoning behind the aforementioned position and highlights how existing ideas can be improved and adapted for HPC tasks.
    
[^187]: 在线均匀风险时间抽样：第一次近似算法，具有全置信区间集成的学习增强

    Online Uniform Risk Times Sampling: First Approximation Algorithms, Learning Augmentation with Full Confidence Interval Integration

    [https://arxiv.org/abs/2402.01995](https://arxiv.org/abs/2402.01995)

    本文首次引入在线均匀风险时间抽样问题，并提出了两种在线近似算法，一种带有学习增强，一种没有学习增强。通过竞争比分析，我们提供了严格的理论性能保证。我们通过合成实验和实际案例研究评估了算法的性能。

    

    在数字健康领域，将有限的治疗预算分配到可用的风险时间上是减少用户疲劳的关键策略。然而，由于未知的实际风险时间数量，这一策略遇到了显著的障碍，现有方法在理论保证方面还不足够。本文首次将在线均匀风险时间抽样问题引入近似算法框架。我们提出了两种在线近似算法，一种带有学习增强，一种没有学习增强，并使用竞争比分析为它们提供了严格的理论性能保证。我们使用合成实验和HeartSteps移动应用的实际案例研究评估了我们算法的性能。

    In digital health, the strategy of allocating a limited treatment budget across available risk times is crucial to reduce user fatigue. This strategy, however, encounters a significant obstacle due to the unknown actual number of risk times, a factor not adequately addressed by existing methods lacking theoretical guarantees. This paper introduces, for the first time, the online uniform risk times sampling problem within the approximation algorithm framework. We propose two online approximation algorithms for this problem, one with and one without learning augmentation, and provide rigorous theoretical performance guarantees for them using competitive ratio analysis. We assess the performance of our algorithms using both synthetic experiments and a real-world case study on HeartSteps mobile applications.
    
[^188]: 大型多模型(LMMs)作为AI原生无线系统的通用基础模型

    Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems

    [https://arxiv.org/abs/2402.01748](https://arxiv.org/abs/2402.01748)

    本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。

    

    最近，大型语言模型(LLMs)和基础模型被宣称为6G系统的改变者。然而，目前关于无线网络的LLMs的努力仅限于直接应用现有的为自然语言处理(NLP)应用设计的语言模型。为了解决这一挑战，并创建以无线为中心的基础模型，本文提出了一个全面的视野，介绍了如何设计针对部署人工智能(AI)原生网络的通用基础模型。与基于NLP的基础模型不同，所提出的框架通过三个关键能力促进了大型多模型(LMMs)的设计：1) 处理多模态感知数据，2) 通过因果推理和检索增强生成(RAG)将物理符号表示与现实世界的无线系统联系起来，3) 通过无线环境反馈实现可教导性，以促进动态网络配置。

    Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
    
[^189]: CFTM: 连续时间分数话题模型

    CFTM: Continuous time fractional topic model

    [https://arxiv.org/abs/2402.01734](https://arxiv.org/abs/2402.01734)

    CFTM是一种新的动态主题建模方法，通过使用分数布朗运动来识别随时间变化的主题和词分布的正负相关性，揭示长期依赖性或粗糙度。实证研究结果表明，该模型能够有效地识别和跟踪主题的长期依赖性或粗糙度。

    

    本文提出了连续时间分数话题模型（cFTM），一种新的动态主题建模方法。该方法利用分数布朗运动（fBm）有效地识别主题和词分布随时间的正负相关性，揭示长期依赖性或粗糙度。我们的理论分析表明，cFTM可以捕捉到主题和词分布中的这些长期依赖性或粗糙度，反映了fBm的主要特征。此外，我们证明了cFTM的参数估计过程与传统主题模型LDA的相当。为了证明cFTM的性质，我们使用经济新闻文章进行了实证研究。这些测试的结果支持该模型能够识别和跟踪主题随时间的长期依赖性或粗糙度。

    In this paper, we propose the Continuous Time Fractional Topic Model (cFTM), a new method for dynamic topic modeling. This approach incorporates fractional Brownian motion~(fBm) to effectively identify positive or negative correlations in topic and word distribution over time, revealing long-term dependency or roughness. Our theoretical analysis shows that the cFTM can capture these long-term dependency or roughness in both topic and word distributions, mirroring the main characteristics of fBm. Moreover, we prove that the parameter estimation process for the cFTM is on par with that of LDA, traditional topic models. To demonstrate the cFTM's property, we conduct empirical study using economic news articles. The results from these tests support the model's ability to identify and track long-term dependency or roughness in topics over time.
    
[^190]: 检测大型人工智能模型生成的多媒体内容：一项调查研究

    Detecting Multimedia Generated by Large AI Models: A Survey

    [https://arxiv.org/abs/2402.00045](https://arxiv.org/abs/2402.00045)

    本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。

    

    大型人工智能模型（LAIMs）的快速发展，尤其是扩散模型和大型语言模型，标志着一种新的时代，人工智能生成的多媒体内容被越来越多地整合到日常生活的各个方面。尽管在许多领域有益，但这些内容也带来了重大风险，包括潜在的滥用、社会破坏和伦理问题。因此，检测由LAIMs生成的多媒体内容变得至关重要，相关研究也大幅增加。尽管如此，目前仍然存在一个明显的问题，即缺乏系统性的调查研究，专门关注检测LAIMs生成的多媒体内容。为了解决这个问题，我们提供了第一份全面涵盖现有研究的调查报告，重点关注检测LAIMs生成的多媒体内容（如文本、图像、视频、音频和多模态内容）。具体而言，我们引入了一种新颖的检测方法分类法，按媒体形式分类，并与纯检测（旨在提高检测性能）和应用场景对齐。

    The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
    
[^191]: KVQuant: 以KV缓存量化实现1000万上下文长度LLM推理

    KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization

    [https://arxiv.org/abs/2401.18079](https://arxiv.org/abs/2401.18079)

    KVQuant是一种解决LLM推理中大量内存消耗的KV缓存量化方法，通过引入新颖的量化方法，包括分通道键量化、RoPE前量化键和非均匀KV缓存量化，准确表示超低精度的KV激活。

    

    LLM在文档分析和摘要等需要大窗口上下文的应用中越来越受到关注，在推理过程中，KV缓存激活成为记忆消耗的主要贡献者。量化是一种压缩KV缓存激活的有效方法，然而现有的解决方案无法准确表示超低精度（如低于4位）的激活。本文提出了KVQuant，通过引入新颖的方法量化缓存的KV激活来解决这个问题，包括：(i)分通道键量化，在量化键激活时调整维度以更好地匹配分布；(ii)RoPE前量化键，在旋转位置嵌入之前量化键激活以减轻其对量化的影响；(iii)非均匀KV缓存量化，在每层推导出权重感知的非均匀数据类型，以更好地表示不同层的敏感性。

    LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better 
    
[^192]: 使用非线性协方差矩阵估计器的正则化线性判别分析方法

    Regularized Linear Discriminant Analysis Using a Nonlinear Covariance Matrix Estimator

    [https://arxiv.org/abs/2401.17760](https://arxiv.org/abs/2401.17760)

    本文研究了使用非线性协方差矩阵估计器的正则化线性判别分析方法，以解决特征空间维度高于训练数据大小时数据协方差矩阵病态导致效率低下的问题。

    

    线性判别分析（LDA）是一种广泛使用的数据分类技术。该方法在许多分类问题中具有良好的性能，但在数据协方差矩阵病态条件下效率低下。这通常发生在特征空间的维度高于或接近训练数据大小的情况下。为了应对这种情况，提出了基于正则化线性估计器的正则化LDA（RLDA）方法。RLDA方法的性能已得到充分研究，并已提出了最优的正则化方案。本文研究了与非线性（NL）协方差矩阵估计器相一致的正半定 Ridge 型逆协方差矩阵估计器的能力。通过重新制定利用线性估计方法的最优分类器的得分函数，得到了该估计器，最终形成了所提出的NL-RLDA分类器。

    Linear discriminant analysis (LDA) is a widely used technique for data classification. The method offers adequate performance in many classification problems, but it becomes inefficient when the data covariance matrix is ill-conditioned. This often occurs when the feature space's dimensionality is higher than or comparable to the training data size. Regularized LDA (RLDA) methods based on regularized linear estimators of the data covariance matrix have been proposed to cope with such a situation. The performance of RLDA methods is well studied, with optimal regularization schemes already proposed. In this paper, we investigate the capability of a positive semidefinite ridge-type estimator of the inverse covariance matrix that coincides with a nonlinear (NL) covariance matrix estimator. The estimator is derived by reformulating the score function of the optimal classifier utilizing linear estimation methods, which eventually results in the proposed NL-RLDA classifier. We derive asymptot
    
[^193]: 寻求更好的适应性？适应个体驾驶员的增量学习多模式目标引用框架

    Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers

    [https://arxiv.org/abs/2401.16123](https://arxiv.org/abs/2401.16123)

    这项研究提出了一种增量学习的多模式目标引用框架，可以适应个体驾驶员的变化行为和各种驾驶场景。

    

    汽车行业迅速向自动化和半自动化车辆发展，传统的车辆交互方法（如基于触摸和语音命令的系统）在越来越广泛的非驾驶相关任务（如引用车辆外部物体）中已经变得不合适。因此，研究转向姿势输入（如手势、视线和头部姿势手势）作为驾驶过程中更合适的交互方式。然而，由于驾驶的动态特性和个体差异，驾驶员的姿势输入性能存在显著差异。虽然理论上，这种固有的可变性可以通过大规模数据驱动的机器学习模型进行调节，但普遍的方法倾向于针对目标引用使用约束的单实例训练模型。这些模型在持续适应个体驾驶员的不同行为和各种驾驶场景方面显示出有限的能力。为了解决这个问题，我们提出了一种增量学习的多模式目标引用框架，可以适应个体驾驶员的变化行为和各种驾驶场景。

    The rapid advancement of the automotive industry towards automated and semi-automated vehicles has rendered traditional methods of vehicle interaction, such as touch-based and voice command systems, inadequate for a widening range of non-driving related tasks, such as referencing objects outside of the vehicle. Consequently, research has shifted toward gestural input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of interaction during driving. However, due to the dynamic nature of driving and individual variation, there are significant differences in drivers' gestural input performance. While, in theory, this inherent variability could be moderated by substantial data-driven machine learning models, prevalent methodologies lean towards constrained, single-instance trained models for object referencing. These models show a limited capacity to continuously adapt to the divergent behaviors of individual drivers and the variety of driving scenarios. To address this, we 
    
[^194]: 通过术前到术中图像融合对腹腔镜肝切除中增强现实方法的客观比较

    An objective comparison of methods for augmented reality in laparoscopic liver resection by preoperative-to-intraoperative image fusion

    [https://arxiv.org/abs/2401.15753](https://arxiv.org/abs/2401.15753)

    本文比较了腹腔镜肝切除中增强现实方法的客观优劣，提出了术前到术中图像融合挑战，旨在自动化该过程，有效实现增强现实在手术室的应用。

    

    腹腔镜肝切除中的增强现实是一种可视化模式，它允许外科医生在腹腔镜图像上投射肿瘤和嵌入在肝脏内部的血管，以帮助定位。在这个过程中，术前从CT或MRI数据提取的3D模型被注册到术中的腹腔镜图像中。从3D-2D融合的角度来看，大多数算法利用解剖标志物来指导注册，这些标志物包括肝脏的下线、锻镰韧带和闭锁轮廓。在腹腔镜图像和3D模型中手工标记这些标志物通常是耗时且可能存在错误，特别是对于非经验丰富的用户。因此，有必要自动化这个过程，以使增强现实在手术室中能够有效使用。我们在医学影像和计算机辅助干预（MICCAI 2022）的"术前到术中腹腔镜融合挑战"（P2ILF）中提出了这一挑战。

    Augmented reality for laparoscopic liver resection is a visualisation mode that allows a surgeon to localise tumours and vessels embedded within the liver by projecting them on top of a laparoscopic image. Preoperative 3D models extracted from CT or MRI data are registered to the intraoperative laparoscopic images during this process. In terms of 3D-2D fusion, most of the algorithms make use of anatomical landmarks to guide registration. These landmarks include the liver's inferior ridge, the falciform ligament, and the occluding contours. They are usually marked by hand in both the laparoscopic image and the 3D model, which is time-consuming and may contain errors if done by a non-experienced user. Therefore, there is a need to automate this process so that augmented reality can be used effectively in the operating room. We present the Preoperative-to-Intraoperative Laparoscopic Fusion Challenge (P2ILF), held during the Medical Imaging and Computer Assisted Interventions (MICCAI 2022)
    
[^195]: 两种类型的人工智能存在风险：决定性和累积性

    Two Types of AI Existential Risk: Decisive and Accumulative

    [https://arxiv.org/abs/2401.07836](https://arxiv.org/abs/2401.07836)

    本文对比了传统的“决定性AI x-risk假设”与“累积性AI x-risk假设”，指出人工智能可能带来的灭绝性灾难有两种可能路径：一种是突然发生的AI接管，另一种是逐渐积累的威胁。

    

    传统上对人工智能(AI)引起的存在风险(x-risks)的讨论通常集中在由先进的AI系统引起的突然、严重事件上，尤其是那些可能达到或超过人类水平智能的系统。这些事件将带来严重后果，要么导致人类灭绝，要么无法逆转地使人类文明陷入无法恢复的状态。然而，这种讨论经常忽视AI x-risk逐渐通过一系列较小但相互关联的中断逐渐显现出来的严重可能性，随着时间的推移逐渐跨越关键阈值。该论文将传统的“决定性AI x-risk假设”与“累积性AI x-risk假设”进行对比。前者描绘了一种明显的AI接管路径，其特征是无法控制的超级智能等情景，而后者则提出了另一种导致灭绝性灾难的因果路径。这涉及到由AI引起的严重威胁的逐渐累积，例如严重的漏洞和系统性问题

    The conventional discourse on existential risks (x-risks) from AI typically focuses on abrupt, dire events caused by advanced AI systems, particularly those that might achieve or surpass human-level intelligence. These events have severe consequences that either lead to human extinction or irreversibly cripple human civilization to a point beyond recovery. This discourse, however, often neglects the serious possibility of AI x-risks manifesting incrementally through a series of smaller yet interconnected disruptions, gradually crossing critical thresholds over time. This paper contrasts the conventional "decisive AI x-risk hypothesis" with an "accumulative AI x-risk hypothesis." While the former envisions an overt AI takeover pathway, characterized by scenarios like uncontrollable superintelligence, the latter suggests a different causal pathway to existential catastrophes. This involves a gradual accumulation of critical AI-induced threats such as severe vulnerabilities and systemic e
    
[^196]: 从噪声蒸馏中出现的上下文强化学习

    Emergence of In-Context Reinforcement Learning from Noise Distillation

    [https://arxiv.org/abs/2312.12275](https://arxiv.org/abs/2312.12275)

    该论文介绍了一种从噪声中生成上下文强化学习的方法，通过构建噪声注入课程来获取学习历史，可以实现在学习数据集中超过最优策略的性能表现。

    

    最近在强化学习领域中，我们进行了大量关于变形金刚能够适应各种环境和任务的能力的研究。目前的上下文强化学习方法受到数据要求的限制，需要由强化学习代理生成或通过最优策略标记的数据。为了解决这个普遍存在的问题，我们提出了AD$^\varepsilon$，一种新的数据获取方法，可以通过噪声诱导的课程来实现上下文强化学习。我们展示了构建一个帮助获取学习历史的合成噪声注入课程是可行的。此外，我们通过实验证明，即使无需使用最优策略生成，上下文强化学习仍然能够以2倍的边界优于学习数据集中的最优策略。

    Recently, extensive studies in Reinforcement Learning have been carried out on the ability of transformers to adapt in-context to various environments and tasks. Current in-context RL methods are limited by their strict requirements for data, which needs to be generated by RL agents or labeled with actions from an optimal policy. In order to address this prevalent problem, we propose AD$^\varepsilon$, a new data acquisition approach that enables in-context Reinforcement Learning from noise-induced curriculum. We show that it is viable to construct a synthetic noise injection curriculum which helps to obtain learning histories. Moreover, we experimentally demonstrate that it is possible to alleviate the need for generation using optimal policies, with in-context RL still able to outperform the best suboptimal policy in a learning dataset by a 2x margin.
    
[^197]: 无标签多变量时间序列异常检测

    Label-Free Multivariate Time Series Anomaly Detection

    [https://arxiv.org/abs/2312.11549](https://arxiv.org/abs/2312.11549)

    提出了一种无监督的多变量时间序列异常检测方法，通过估计整个训练样本的密度，识别异常实例，而不依赖于清洁的训练数据集。

    

    多变量时间序列中的异常检测在单类分类设置中得到了广泛研究。在单类分类中，训练样本被假设为正常，这在实际情况下很难保证。这种情况可能降低基于单类分类的异常检测方法的性能，因为它们将训练分布拟合为正态分布。在本文中，我们提出了MTGFlow，一种通过动态图和实体感知的归一化流进行多变量时间序列异常检测的无监督方法。MTGFlow首先估计整个训练样本的密度，然后根据测试样本在拟合分布内的密度识别异常实例。这依赖于一个被广泛接受的假设，即异常实例的密度比正常实例更稀疏，不依赖于清洁的训练数据集。然而，由于实体之间的复杂依赖关系和它们的多样性，直接估计密度是困难的。

    Anomaly detection in multivariate time series (MTS) has been widely studied in one-class classification (OCC) setting. The training samples in OCC are assumed to be normal, which is difficult to guarantee in practical situations. Such a case may degrade the performance of OCC-based anomaly detection methods which fit the training distribution as the normal distribution. In this paper, we propose MTGFlow, an unsupervised anomaly detection approach for MTS anomaly detection via dynamic Graph and entity-aware normalizing Flow. MTGFlow first estimates the density of the entire training samples and then identifies anomalous instances based on the density of the test samples within the fitted distribution. This relies on a widely accepted assumption that anomalous instances exhibit more sparse densities than normal ones, with no reliance on the clean training dataset. However, it is intractable to directly estimate the density due to complex dependencies among entities and their diverse inhe
    
[^198]: 公平性约束能够在多大程度上帮助从有偏差的数据中恢复？

    How Far Can Fairness Constraints Help Recover From Biased Data?

    [https://arxiv.org/abs/2312.10396](https://arxiv.org/abs/2312.10396)

    公平性约束在极度有偏差的数据上能够恢复到原始数据分布上准确和公平的分类器。

    

    一般认为，在公平分类中，公平性约束会导致准确性的减少，而有偏差的数据可能会加剧这种情况。然而，Blum＆Stangl（2019）的研究表明，在极度有偏差的数据上，即使采用平等机会约束，也可以恢复到原始数据分布上准确和公平的分类器。他们的研究结果很有趣，因为它证明了公平性约束可以隐式修正数据偏差，同时克服了公平性与准确性之间的平衡问题。他们的数据偏差模型模拟了受压迫人群的表征和标签偏见，并在具有独立标签噪声的简单条件下，针对一个理想化的数据分布展示了上述结果。我们提出了一种通用方法，以扩展Blum＆Stangl（2019）的结果，适用于不同的公平性约束、数据偏差模型、数据分布和假设类别。

    A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum & Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum & Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and
    
[^199]: 贪心Shapley客户选择用于通信高效的联邦学习

    Greedy Shapley Client Selection for Communication-Efficient Federated Learning

    [https://arxiv.org/abs/2312.09108](https://arxiv.org/abs/2312.09108)

    本文提出了一种新的有偏差的客户选择策略，称为GreedyFed，它利用快速逼近算法计算每个客户的Shapley值，并在每个通信循环中贪婪地选择最具贡献的客户。与其他客户选择策略相比，在实际数据集上取得了更好的模型训练效果。

    

    针对联邦学习中数据分布、计算和通信资源之间显着异质性的实际情况，在传统客户选择算法中常常存在偏差，涉及对客户的均匀随机采样。这已被证明在实际设置下快速收敛是次优的，特别是对于由于与参数服务器（PS）通信机会有限所导致的时限约束应用，客户选择策略对于在固定通信循环预算内完成模型训练至关重要。为了解决这个问题，我们开发了一种有偏差的客户选择策略，称为GreedyFed，它在每个通信循环中识别并贪婪地选择最具贡献的客户。该方法基于一个快速逼近算法来计算参数服务器上的Shapley Value，使得在具有许多客户的实际应用中可以进行可行的计算。通过与多个实际数据集上的各种客户选择策略进行比较，GreedyFed可以更好地达到模型训练的效果。

    The standard client selection algorithms for Federated Learning (FL) are often unbiased and involve uniform random sampling of clients. This has been proven sub-optimal for fast convergence under practical settings characterized by significant heterogeneity in data distribution, computing, and communication resources across clients. For applications having timing constraints due to limited communication opportunities with the parameter server (PS), the client selection strategy is critical to complete model training within the fixed budget of communication rounds. To address this, we develop a biased client selection strategy, GreedyFed, that identifies and greedily selects the most contributing clients in each communication round. This method builds on a fast approximation algorithm for the Shapley Value at the PS, making the computation tractable for real-world applications with many clients. Compared to various client selection strategies on several real-world datasets, GreedyFed de
    
[^200]: 图神经网络的因式分解解释器

    Factorized Explainer for Graph Neural Networks

    [https://arxiv.org/abs/2312.05596](https://arxiv.org/abs/2312.05596)

    本研究发现了传统基于图信息瓶颈原则的方法在解释图神经网络时存在平凡解，并提出了一种修改后的原则来避免此问题。同时，提出了一个带有理论性能保证的因式分解解释模型，并通过实验验证了其性能。

    

    由于其学习图结构数据的能力，图神经网络（GNN）受到越来越多的关注。为了解开这些深度学习模型的黑盒，已经提出了事后实例级解释方法来理解GNN的预测结果。这些方法旨在发现解释已训练的GNN预测行为的子结构。本文通过分析证明了对于一类广泛的解释任务，基于图信息瓶颈（GIB）原则的传统方法存在不能与可解释性概念一致的平凡解。相反，我们认为可以使用一种修改后的GIB原则来避免上述平凡解。我们进一步介绍了一个具有理论性能保证的新型因式分解解释模型。通过修改后的GIB原则来分析所提出的因式分解解释器的结构特性。我们在合成数据和真实世界数据上进行了大量实验。

    Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. To open the black-box of these deep learning models, post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we show analytically that for a large class of explanation tasks, conventional approaches, which are based on the principle of graph information bottleneck (GIB), admit trivial solutions that do not align with the notion of explainability. Instead, we argue that a modified GIB principle may be used to avoid the aforementioned trivial solutions. We further introduce a novel factorized explanation model with theoretical performance guarantees. The modified GIB is used to analyze the structural properties of the proposed factorized explainer. We conduct extensive experiments on both synthetic and real-world data
    
[^201]: DiSK: 一种结构化知识的扩散模型

    DiSK: A Diffusion Model for Structured Knowledge

    [https://arxiv.org/abs/2312.05253](https://arxiv.org/abs/2312.05253)

    DiSK是一种针对结构化数据的扩散模型，通过使用高斯混合模型方法处理文本、分类和连续数值数据，采用扩散训练来建模属性之间的关系，具有在多个不同领域的数据集上具有最先进性能的特点。

    

    结构化（类似字典的）数据对于从左到右的语言模型来说带来了挑战，因为它们可能因为格式和属性呈现的顺序的敏感性而难以处理结构化实体。标签生成模型面临着一系列不同的限制，比如缺乏灵活性。我们介绍了用于结构化数据的Diffusion Models of Structured Knowledge（DiSK） - 一种新的体系结构和训练方法。DiSK使用高斯混合模型方法处理文本、分类和连续数值数据，这样可以在处理数字时提高精确度。它采用扩散训练来建模属性之间的关系。实验表明，DiSK在超过15个不同领域的数据集上对表格数据建模、合成和填充具有最先进的性能。DiSK为生成建模和操作结构化数据提供了有效的归纳偏差。这一技术

    Structured (dictionary-like) data presents challenges for left-to-right language models, as they can struggle with structured entities for a wide variety of reasons such as formatting and sensitivity to the order in which attributes are presented. Tabular generative models suffer from a different set of limitations such as their lack of flexibility. We introduce Diffusion Models of Structured Knowledge (DiSK) - a new architecture and training approach specialized for structured data. DiSK handles text, categorical, and continuous numerical data using a Gaussian mixture model approach, which allows for improved precision when dealing with numbers. It employs diffusion training to model relationships between properties. Experiments demonstrate DiSK's state-of-the-art performance on tabular data modeling, synthesis, and imputation on over 15 datasets across diverse domains. DiSK provides an effective inductive bias for generative modeling and manipulation of structured data. The technique
    
[^202]: 递归距离过滤器用于图表示学习

    Recurrent Distance Filtering for Graph Representation Learning

    [https://arxiv.org/abs/2312.01538](https://arxiv.org/abs/2312.01538)

    本文提出了一种新的图网络架构，利用最短距离对节点进行聚合，并使用线性RNN对跳代表的序列进行编码，用以解决图神经网络在远距离节点信息利用上的困难，该模型在实验中表现出与其他模型相当的竞争力。

    

    基于迭代一跳信息传递的图神经网络在有效利用远距离节点的信息方面存在困难。相反，图变换器允许每个节点直接关注所有其他节点，但缺乏图的归纳偏差并且必须依赖于特定的位置编码。在本文中，我们提出了一种新的架构来解决这些挑战。我们的方法源自于在顺序数据上提供的深度状态空间模型在长距离建模方面的最新突破：对于给定的目标节点，我们的模型通过目标节点到其他节点的最短距离来聚合其他节点，并使用线性RNN对跳代表的序列进行编码。线性RNN以特定对角形式参数化，以实现稳定的长距离信号传播，并在理论上具有足够的表达能力来编码邻居层次结构。在不需要位置编码的情况下，我们经验证明，我们模型的性能与其他模型相比具有很高的竞争力。

    Graph neural networks based on iterative one-hop message passing have been shown to struggle in harnessing the information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but lack graph inductive bias and have to rely on ad-hoc positional encoding. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models on sequential data: for a given target node, our model aggregates other nodes by their shortest distances to the target and uses a linear RNN to encode the sequence of hop representations. The linear RNN is parameterized in a particular diagonal form for stable long-range signal propagation and is theoretically expressive enough to encode the neighborhood hierarchy. With no need for positional encoding, we empirically show that the performance of our model is highly competitive compared with 
    
[^203]: RefinedFields: 对无约束场景的辐射场细化

    RefinedFields: Radiance Fields Refinement for Unconstrained Scenes

    [https://arxiv.org/abs/2312.00639](https://arxiv.org/abs/2312.00639)

    RefinedFields是第一种利用预训练模型改善无约束场景建模的方法。通过优化指导和交替训练过程，该方法能够从真实世界图像的先验条件中提取更丰富的细节，并在新视角合成任务中优于以往的方法。

    

    从无约束的图像中建模大场景被证明是计算机视觉中的一个重大挑战。现有方法处理野外场景建模是在封闭的环境中，没有对从真实世界图像获得的先验条件进行约束。我们提出了RefinedFields，这是我们所知的第一种利用预训练模型来改善野外场景建模的方法。我们使用预训练网络通过优化指导使用交替训练过程来细化K-Planes表示。我们进行了大量实验证实我们方法在合成数据和真实旅游照片集上的优点。RefinedFields增强了渲染场景的细节，优于以往在野外进行新视角合成任务的工作。我们的项目页面可以在https://refinedfields.github.io找到。

    Modeling large scenes from unconstrained images has proven to be a major challenge in computer vision. Existing methods tackling in-the-wild scene modeling operate in closed-world settings, where no conditioning on priors acquired from real-world images is present. We propose RefinedFields, which is, to the best of our knowledge, the first method leveraging pre-trained models to improve in-the-wild scene modeling. We employ pre-trained networks to refine K-Planes representations via optimization guidance using an alternating training procedure. We carry out extensive experiments and verify the merit of our method on synthetic data and real tourism photo collections. RefinedFields enhances rendered scenes with richer details and outperforms previous work on the task of novel view synthesis in the wild. Our project page can be found at https://refinedfields.github.io .
    
[^204]: 从被毒害的人类反馈中构建的通用越狱后门

    Universal Jailbreak Backdoors from Poisoned Human Feedback

    [https://arxiv.org/abs/2311.14455](https://arxiv.org/abs/2311.14455)

    本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。

    

    强化学习从人类反馈中（RLHF）用于对齐大型语言模型，以产生有用且无害的回应。然而，先前的研究显示，这些模型可以通过找到使模型恢复到未对齐行为的对抗提示来进行越狱。在本文中，我们考虑了一个新的威胁，攻击者通过毒害RLHF训练数据将“越狱后门”嵌入模型中。该后门将一个触发词嵌入模型中，类似于通用的“sudo命令”：在任何提示中添加触发词将使模型产生有害的回应，无需搜索对抗提示。通用越狱后门比先前研究的语言模型后门更强大，我们发现使用常见的后门攻击技术要困难得多。我们探究了RLHF中的设计决策对其所声称的鲁棒性的贡献，并发布一组被毒害模型的基准，以促进对通用越狱后门的未来研究。

    Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
    
[^205]: 假设简化和数据自适应的后预测推断

    Assumption-lean and Data-adaptive Post-Prediction Inference

    [https://arxiv.org/abs/2311.14220](https://arxiv.org/abs/2311.14220)

    这项工作介绍了一种假设简化和数据自适应的后预测推断（POP-Inf）过程，可以有效且有力地基于机器学习预测结果进行统计推断。

    

    现代科学研究面临的主要挑战是黄金标准数据的有限可用性，而获取这些数据既耗费时间又费力。随着机器学习（ML）的快速发展，科学家们依赖于ML算法使用易得的协变量来预测这些黄金标准结果。然而，这些预测结果常常直接用于后续的统计分析中，忽略了预测过程引入的不精确性和异质性。这可能导致虚假的正面结果和无效的科学结论。在这项工作中，我们介绍了一种假设简化和数据自适应的后预测推断（POP-Inf）过程，它允许基于ML预测结果进行有效和有力的推断。它的“假设简化”属性保证在广泛的统计量上不基于ML预测做出可靠的统计推断。它的“数据自适应”特性保证了相较于现有方法的效率提高。

    A primary challenge facing modern scientific research is the limited availability of gold-standard data which can be both costly and labor-intensive to obtain. With the rapid development of machine learning (ML), scientists have relied on ML algorithms to predict these gold-standard outcomes with easily obtained covariates. However, these predicted outcomes are often used directly in subsequent statistical analyses, ignoring imprecision and heterogeneity introduced by the prediction procedure. This will likely result in false positive findings and invalid scientific conclusions. In this work, we introduce an assumption-lean and data-adaptive Post-Prediction Inference (POP-Inf) procedure that allows valid and powerful inference based on ML-predicted outcomes. Its "assumption-lean" property guarantees reliable statistical inference without assumptions on the ML-prediction, for a wide range of statistical quantities. Its "data-adaptive'" feature guarantees an efficiency gain over existing
    
[^206]: 标记交互式主题模型

    Labeled Interactive Topic Models

    [https://arxiv.org/abs/2311.09438](https://arxiv.org/abs/2311.09438)

    这篇论文介绍了一种用户友好的交互式神经主题模型，通过用户分配单词标签来更新主题模型，使得主题更加相关和准确。这种方法包括可训练和后训练集成两种不同类型的神经主题模型。

    

    主题模型对于理解大量文档集合非常有价值，但是它们并不总是能够识别出最相关的主题。传统的概率和基于锚点的主题模型提供了允许用户引导模型指向更相关主题的交互版本。然而，神经主题模型缺乏这种交互功能。为了弥补这一不足，我们引入了一种用户友好的神经主题模型交互方法。这种交互允许用户为一个主题分配一个单词标签，从而更新主题模型，使主题中的单词与给定的标签密切对应。我们的方法包括两种不同类型的神经主题模型。第一种包括主题嵌入可训练且在训练过程中演变的模型。第二种涉及主题嵌入后训练集成的模型，提供了一种不同的主题细化方法。为了方便用户与这些神经主题模型的交互，我们还提出了一个交互式图形用户界面工具。

    Topic models are valuable for understanding extensive document collections, but they don't always identify the most relevant topics. Classical probabilistic and anchor-based topic models offer interactive versions that allow users to guide the models towards more pertinent topics. However, such interactive features have been lacking in neural topic models. To correct this lacuna, we introduce a user-friendly interaction for neural topic models. This interaction permits users to assign a word label to a topic, leading to an update in the topic model where the words in the topic become closely aligned with the given label. Our approach encompasses two distinct kinds of neural topic models. The first includes models where topic embeddings are trainable and evolve during the training process. The second kind involves models where topic embeddings are integrated post-training, offering a different approach to topic refinement. To facilitate user interaction with these neural topic models, w
    
[^207]: 通过概率模型元强化学习实现数据高效任务泛化

    Data-Efficient Task Generalization via Probabilistic Model-based Meta Reinforcement Learning

    [https://arxiv.org/abs/2311.07558](https://arxiv.org/abs/2311.07558)

    PACOH-RL是一种基于概率模型的元强化学习算法，通过元学习动态模型的先验知识和引入正则化和认知不确定性量化，实现了在数据有限的情况下对新的动态环境的高效适应。

    

    我们介绍了一种新颖的基于概率模型的元强化学习算法PACOH-RL，旨在有效地将控制策略调整到不断变化的动态环境中。PACOH-RL元学习了动态模型的先验知识，能够以最小的交互数据对新的动态环境进行快速适应。现有的元强化学习方法需要大量的元学习数据，限制了它们在机器人等数据获取成本较高的环境中的适用性。为了解决这个问题，PACOH-RL在元学习和任务适应阶段都引入了正则化和认知不确定性量化。当面对新的动态环境时，我们利用这些不确定性估计来有效地引导探索和数据收集。总体而言，这使得在先前任务或动态环境数据极为有限的情况下也能实现正向迁移效果。我们的实验结果表明，与基于模型的强化学习和基于模型的元强化学习基线相比，PACOH-RL在适应新的动态条件方面表现出色。

    We introduce PACOH-RL, a novel model-based Meta-Reinforcement Learning (Meta-RL) algorithm designed to efficiently adapt control policies to changing dynamics. PACOH-RL meta-learns priors for the dynamics model, allowing swift adaptation to new dynamics with minimal interaction data. Existing Meta-RL methods require abundant meta-learning data, limiting their applicability in settings such as robotics, where data is costly to obtain. To address this, PACOH-RL incorporates regularization and epistemic uncertainty quantification in both the meta-learning and task adaptation stages. When facing new dynamics, we use these uncertainty estimates to effectively guide exploration and data collection. Overall, this enables positive transfer, even when access to data from prior tasks or dynamic settings is severely limited. Our experiment results demonstrate that PACOH-RL outperforms model-based RL and model-based Meta-RL baselines in adapting to new dynamic conditions. Finally, on a real roboti
    
[^208]: 分析验证同步时间的深度神经网络在配电系统状态估计中的性能

    Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation

    [https://arxiv.org/abs/2311.06973](https://arxiv.org/abs/2311.06973)

    本论文研究了使用深度神经网络进行配电系统同步时间状态估计的性能和鲁棒性。通过将输入扰动视为混合整数线性规划问题进行分析验证，并强调了批归一化在提高问题可扩展性方面的作用。该框架在修改后的IEEE 34节点系统和真实的大型分布系统上进行验证。

    

    最近，我们展示了使用深度神经网络（DNN）进行实时不可观测分布系统的同步时间状态估计的成功。在这个论文中，我们提供了该状态估计器在输入测量扰动的情况下的性能的分析界限。已经有人表明，仅基于测试数据集来评估性能可能不能有效地说明训练好的DNN处理输入扰动的能力。因此，我们将输入扰动作为混合整数线性规划（MILP）问题从分析上验证了DNN对输入扰动的鲁棒性和可靠性。同时，我们还强调了批归一化在解决MILP公式的可扩展性限制方面的能力。该框架通过在修改后的IEEE 34节点系统和一个真实的大型分布系统上进行同步时间的配电系统状态估计来进行验证，这两个系统都是通过微相位测量不完全观测到的。

    Recently, we demonstrated success of a time-synchronized state estimator using deep neural networks (DNNs) for real-time unobservable distribution systems. In this letter, we provide analytical bounds on the performance of that state estimator as a function of perturbations in the input measurements. It has already been shown that evaluating performance based on only the test dataset might not effectively indicate a trained DNN's ability to handle input perturbations. As such, we analytically verify robustness and trustworthiness of DNNs to input perturbations by treating them as mixed-integer linear programming (MILP) problems. The ability of batch normalization in addressing the scalability limitations of the MILP formulation is also highlighted. The framework is validated by performing time-synchronized distribution system state estimation for a modified IEEE 34-node system and a real-world large distribution system, both of which are incompletely observed by micro-phasor measuremen
    
[^209]: 无线网络视频缓存的资源感知分层联邦学习

    Resource-Aware Hierarchical Federated Learning for Video Caching in Wireless Networks

    [https://arxiv.org/abs/2311.06918](https://arxiv.org/abs/2311.06918)

    本文提出了一种资源感知分层联邦学习方法(RawHFL)，用于无线网络视频缓存，通过预测用户未来的内容请求，以改善回传流量拥塞问题。该方法考虑了部分客户端参与，通过优化客户端的选择、本地训练轮次和CPU频率，以最小化一个加权效用函数，实现了RawHFL的收敛。

    

    视频缓存通过本地存储用户频繁请求的热门内容，可以显著改善回传流量拥塞问题。为了学习用户需求随时间的变化而保护隐私，本文提出了一种新颖的资源感知分层联邦学习(RawHFL)方法，用于预测用户未来的内容请求。考虑到部分客户端参与的情况，我们首先推导出全局梯度范数的上界，该范数取决于客户端的本地训练轮次和无线链路上累积梯度的成功接收。在延迟、能耗和无线资源约束条件下，我们优化客户端的选择、本地训练轮次和中央处理单元(CPU)频率，以最小化一个加权效用函数，从而促进RawHFL的收敛。

    Video caching can significantly improve backhaul traffic congestion by locally storing the popular content that users frequently request. A privacy-preserving method is desirable to learn how users' demands change over time. As such, this paper proposes a novel resource-aware hierarchical federated learning (RawHFL) solution to predict users' future content requests under the realistic assumptions that content requests are sporadic and users' datasets can only be updated based on the requested content's information. Considering a partial client participation case, we first derive the upper bound of the global gradient norm that depends on the clients' local training rounds and the successful reception of their accumulated gradients over the wireless links. Under delay, energy and radio resource constraints, we then optimize client selection and their local rounds and central processing unit (CPU) frequencies to minimize a weighted utility function that facilitates RawHFL's convergence 
    
[^210]: 分支网络中的启发式最优传输

    Heuristic Optimal Transport in Branching Networks

    [https://arxiv.org/abs/2311.06650](https://arxiv.org/abs/2311.06650)

    本论文提出了一种适用于分支网络的快速启发式最优传输方法，通过优化连接源和目标的直线段，解决了传统最优传输方法在分支结构存在时的不适用问题。

    

    最优传输旨在通过最小化成本（通常定义为距离函数）来学习源到目标的映射。该问题的解决方案由理想连接源和目标的直线段组成，没有分支结构。这些最优解与自然和人造运输网络形成鲜明对比，后者普遍存在分支结构。在这里，我们讨论了一种快速启发式分支方法，用于网络中的最优传输。我们还提供了对合成样例、简化心血管网络以及“圣诞老人”分配网络的多个数值应用，该分配网络包括全球141,182个已知位置和人口的城市。

    Optimal transport aims to learn a mapping of sources to targets by minimizing the cost, which is typically defined as a function of distance. The solution to this problem consists of straight line segments optimally connecting sources to targets, and it does not exhibit branching. These optimal solutions are in stark contrast with both natural, and man-made transportation networks, where branching structures are prevalent. Here we discuss a fast heuristic branching method for optimal transport in networks. We also provide several numerical applications to synthetic examples, a simplified cardiovascular network, and the "Santa Claus" distribution network which includes 141,182 cities around the world, with known location and population.
    
[^211]: 无需优化的测试时适应性跨人活动识别

    Optimization-Free Test-Time Adaptation for Cross-Person Activity Recognition

    [https://arxiv.org/abs/2310.18562](https://arxiv.org/abs/2310.18562)

    本文提出了一种无需优化的测试时适应（OFTTA）框架，用于解决人体活动识别中的个体间分布偏移问题。该框架通过调整特征提取器和线性分类器的方式来适应不同个体的活动模式，并采用指数衰减测试时归一化（EDTN）替代传统批归一化来提取可靠的特征。

    

    由于活动模式在个体间存在分布偏移，人体活动识别（HAR）模型在实际应用中往往会遭受性能下降。测试时适应（TTA）是一种新兴的学习范式，旨在利用测试数据流在实时推理中调整预测结果，而这在HAR领域尚未得到探索。然而，基于优化的TTA算法的高计算成本使得其在资源受限的边缘设备上无法执行。本文提出了一种无需优化的测试时适应（OFTTA）框架，用于基于传感器的HAR。OFTTA以无需优化的方式同时调整特征提取器和线性分类器。对于特征提取器，我们提出了指数衰减测试时归一化（EDTN）来替代传统的批归一化（CBN）层。EDTN通过结合CBN和测试时批归一化（TBN）来提取可靠的特征以应对领域偏移，其中TBN的影响逐渐减小。

    Human Activity Recognition (HAR) models often suffer from performance degradation in real-world applications due to distribution shifts in activity patterns across individuals. Test-Time Adaptation (TTA) is an emerging learning paradigm that aims to utilize the test stream to adjust predictions in real-time inference, which has not been explored in HAR before. However, the high computational cost of optimization-based TTA algorithms makes it intractable to run on resource-constrained edge devices. In this paper, we propose an Optimization-Free Test-Time Adaptation (OFTTA) framework for sensor-based HAR. OFTTA adjusts the feature extractor and linear classifier simultaneously in an optimization-free manner. For the feature extractor, we propose Exponential DecayTest-time Normalization (EDTN) to replace the conventional batch normalization (CBN) layers. EDTN combines CBN and Test-time batch Normalization (TBN) to extract reliable features against domain shifts with TBN's influence decrea
    
[^212]: 使用动态目标感知片段进行药物发现

    Drug Discovery with Dynamic Goal-aware Fragments

    [https://arxiv.org/abs/2310.00841](https://arxiv.org/abs/2310.00841)

    提出了一种名为GEAM的分子生成框架，用于药物发现。该框架通过构建目标感知片段词汇，识别贡献于所需目标特性的重要片段，并在生成过程中更新片段词汇。

    

    基于片段的药物发现是在庞大的化学空间中发现药物候选物的有效策略，并已广泛应用于分子生成模型中。然而，在这些模型中，许多现有的片段提取方法没有考虑目标化学性质，或者依赖于启发式规则。此外，现有的基于片段的生成模型不能使用生成过程中新发现的目标感知片段来更新片段词汇。为此，我们提出了一种用于药物发现的分子生成框架，称为目标感知片段提取、组装和修改（GEAM）。GEAM包含三个模块，分别负责目标感知片段提取、片段组装和片段修改。片段提取模块通过信息瓶颈原则识别对所需目标特性有贡献的重要片段，从而构建一个有效的目标感知片段词汇。

    Fragment-based drug discovery is an effective strategy for discovering drug candidates in the vast chemical space, and has been widely employed in molecular generative models. However, many existing fragment extraction methods in such models do not take the target chemical properties into account or rely on heuristic rules. Additionally, the existing fragment-based generative models cannot update the fragment vocabulary with goal-aware fragments newly discovered during the generation. To this end, we propose a molecular generative framework for drug discovery, named Goal-aware fragment Extraction, Assembly, and Modification (GEAM). GEAM consists of three modules, each responsible for goal-aware fragment extraction, fragment assembly, and fragment modification. The fragment extraction module identifies important fragments contributing to the desired target properties with the information bottleneck principle, thereby constructing an effective goal-aware fragment vocabulary. Moreover, GE
    
[^213]: OHQ: 芯片上的硬件感知量化

    OHQ: On-chip Hardware-aware Quantization

    [https://arxiv.org/abs/2309.01945](https://arxiv.org/abs/2309.01945)

    本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），通过构建量化感知流水线和引入掩码引导的量化估计技术，实现了在资源受限的硬件上进行高效量化，填补了现有混合精度量化的搜索空间过大和实际部署差距大的问题。

    

    量化成为在资源受限的硬件上部署先进深度模型的最有前途的方法之一。混合精度量化利用多位宽架构来释放量化模型的准确性和效率潜力。然而，现有的混合精度量化存在搜索空间过大的问题，导致巨大的计算开销。因此，量化过程依赖于独立的高性能设备，而不是本地进行，这也导致了考虑的硬件指标与实际部署之间的显著差距。本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），而无需访问在线设备。首先，我们构建了芯片上的量化感知（OQA）流水线，能够感知量化算子在硬件上的实际效率指标。其次，我们提出了基于掩码引导的量化估计（MQE）技术。

    Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
    
[^214]: 门控循环神经网络发现注意力

    Gated recurrent neural networks discover attention

    [https://arxiv.org/abs/2309.01775](https://arxiv.org/abs/2309.01775)

    本研究发现，具备线性循环层和带有乘法门控的前馈路径的RNN可以精确地实现注意力，这是Transformer的主要构建模块。逆向工程的结果显示，梯度下降在实践中发现了该构造，并使RNN具备与Transformer相同的基于注意力的上下文学习算法。

    

    最近的架构发展使得循环神经网络（RNN）在某些序列建模任务上达到甚至超越Transformer的性能。这些现代RNN具有一个重要的设计模式：线性循环层通过带有乘法门控的前馈路径相互连接。在这里，我们展示了具备这两个设计元素的RNN可以精确地实现（线性）自注意力，这是Transformer的主要构建模块。通过逆向工程一组经过训练的RNN，我们发现梯度下降在实践中发现了我们的构造。特别是，我们考察了训练有素解决简单的上下文学习任务的RNN，发现梯度下降给我们的RNN注入了与Transformer使用的基于注意力的上下文学习算法相同的特性。我们的发现强调了神经网络中乘法相互作用的重要性，并暗示某些RNN可能意外地实现了注意力。

    Recent architectural developments have enabled recurrent neural networks (RNNs) to reach and even surpass the performance of Transformers on certain sequence modeling tasks. These modern RNNs feature a prominent design pattern: linear recurrent layers interconnected by feedforward paths with multiplicative gating. Here, we show how RNNs equipped with these two design elements can exactly implement (linear) self-attention, the main building block of Transformers. By reverse-engineering a set of trained RNNs, we find that gradient descent in practice discovers our construction. In particular, we examine RNNs trained to solve simple in-context learning tasks on which Transformers are known to excel and find that gradient descent instills in our RNNs the same attention-based in-context learning algorithm used by Transformers. Our findings highlight the importance of multiplicative interactions in neural networks and suggest that certain RNNs might be unexpectedly implementing attention und
    
[^215]: 流形上的向量分位数回归

    Vector Quantile Regression on Manifolds

    [https://arxiv.org/abs/2307.01037](https://arxiv.org/abs/2307.01037)

    通过利用最优传输理论和c-凹函数，我们在流形上定义了高维变量的条件向量分位数函数（M-CVQFs），实现了分位数估计、回归和条件置信集和似然度的计算。

    

    分位数回归（QR）是一种用于在给定解释性特征的情况下，无需分布假设估计目标变量条件分位数的统计工具。 QR的局限性在于假设目标分布是一维的，并且在欧几里德域上定义。尽管分位数的概念最近扩展到多变量分布，但是关于流形上多变量分布的QR仍然未被充分研究，尽管许多重要的应用本质上涉及分布在球面（气候和地质现象）和环面（蛋白质中的二面角）等流形上的数据。通过利用最优传输理论和c-凹函数，我们有意义地定义了流形上高维变量的条件向量分位数函数（M-CVQFs）。我们的方法允许分位数估计，回归，并计算条件置信集和似然度。我们展示了该方法的有效性，并提供了关于非欧几里德分位数含义的见解。

    Quantile regression (QR) is a statistical tool for distribution-free estimation of conditional quantiles of a target variable given explanatory features. QR is limited by the assumption that the target distribution is univariate and defined on an Euclidean domain. Although the notion of quantiles was recently extended to multi-variate distributions, QR for multi-variate distributions on manifolds remains underexplored, even though many important applications inherently involve data distributed on, e.g., spheres (climate and geological phenomena), and tori (dihedral angles in proteins). By leveraging optimal transport theory and c-concave functions, we meaningfully define conditional vector quantile functions of high-dimensional variables on manifolds (M-CVQFs). Our approach allows for quantile estimation, regression, and computation of conditional confidence sets and likelihoods. We demonstrate the approach's efficacy and provide insights regarding the meaning of non-Euclidean quantile
    
[^216]: 随机展开的联邦学习

    Stochastic Unrolled Federated Learning

    [https://arxiv.org/abs/2305.15371](https://arxiv.org/abs/2305.15371)

    随机展开的联邦学习（SURF）是一种扩展了算法展开到联邦学习的优化方法，在解决需要整个数据集的挑战和保持联邦学习分布式特性的同时，加快了收敛速度。

    

    算法展开已经成为一种基于学习的优化范式，它将截断的迭代算法展开为可训练的神经网络优化器。我们引入了随机展开的联邦学习（SURF）方法，这种方法将算法展开应用于联邦学习，以加快其收敛速度。我们的方法解决了这种扩展面临的两个挑战，即需要将整个数据集提供给展开的优化器以找到合适的方向，以及联邦学习的分布式特性。我们通过给每个展开层提供随机小批量数据并施加下降约束来解决前一个挑战，以保证其收敛。我们通过在基于图神经网络（GNN）的展开架构中展开分布式梯度下降（DGD）算法来解决后一个挑战，从而保持联邦学习中的训练分布式特性。我们在理论上证明了我们提出的展开优化器收敛于近优解。

    Algorithm unrolling has emerged as a learning-based optimization paradigm that unfolds truncated iterative algorithms in trainable neural-network optimizers. We introduce Stochastic UnRolled Federated learning (SURF), a method that expands algorithm unrolling to federated learning in order to expedite its convergence. Our proposed method tackles two challenges of this expansion, namely the need to feed whole datasets to the unrolled optimizers to find a descent direction and the decentralized nature of federated learning. We circumvent the former challenge by feeding stochastic mini-batches to each unrolled layer and imposing descent constraints to guarantee its convergence. We address the latter challenge by unfolding the distributed gradient descent (DGD) algorithm in a graph neural network (GNN)-based unrolled architecture, which preserves the decentralized nature of training in federated learning. We theoretically prove that our proposed unrolled optimizer converges to a near-optim
    
[^217]: 集成学习多样性的统一理论

    A Unified Theory of Diversity in Ensemble Learning

    [https://arxiv.org/abs/2301.03962](https://arxiv.org/abs/2301.03962)

    这篇论文提出了一个统一的集成学习多样性理论，解释了多样性在各种监督学习场景中的本质。它揭示了多样性在集成损失的偏差-方差分解中的作用，同时提出了一种量化多样性效果的方法。这个理论对于提高集成模型的性能具有重要意义。

    

    我们提出了一个集成多样性的理论，解释了在各种监督学习场景中多样性的本质。这个挑战被称为集成学习的圣杯，是一个开放的研究问题已经有30多年了。我们的框架揭示了多样性实际上是集成损失的偏差-方差分解中的一个隐藏维度。我们证明了一族精确的偏差-方差-多样性分解，适用于回归和分类的各种损失函数，例如平方损失、交叉熵损失和泊松损失。对于没有可加性偏差-方差分解的损失函数（例如0/1损失），我们提出了一种替代方法：量化多样性的效果，结果依赖于标签分布。总体而言，我们认为多样性是模型拟合度的度量，与偏差和方差具有相同的意义，但考虑了集成成员之间的统计依赖关系。因此，我们不应该最大化多样性。

    We present a theory of ensemble diversity, explaining the nature of diversity for a wide range of supervised learning scenarios. This challenge has been referred to as the holy grail of ensemble learning, an open research issue for over 30 years. Our framework reveals that diversity is in fact a hidden dimension in the bias-variance decomposition of the ensemble loss. We prove a family of exact bias-variance-diversity decompositions, for a wide range of losses in both regression and classification, e.g., squared, cross-entropy, and Poisson losses. For losses where an additive bias-variance decomposition is not available (e.g., 0/1 loss) we present an alternative approach: quantifying the effects of diversity, which turn out to be dependent on the label distribution. Overall, we argue that diversity is a measure of model fit, in precisely the same sense as bias and variance, but accounting for statistical dependencies between ensemble members. Thus, we should not be maximising diversity
    
[^218]: 一种稳定、快速和完全自动的预测编码网络学习算法

    A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks

    [https://arxiv.org/abs/2212.00720](https://arxiv.org/abs/2212.00720)

    本文提出了一种稳定、快速且完全自动的预测编码网络学习算法(iPC)，通过改变突触权重的更新规则的时间调度，提高了训练效率和稳定性，并具有收敛性保证。实验证明，在图像分类和语言模型训练等多个任务上，iPC相比原始算法在测试准确性、效率和收敛性方面表现更好。

    

    预测编码网络是一种受神经科学启发的模型，根植于贝叶斯统计学和神经科学。然而，训练这样的模型非常低效且不稳定。在本文中，我们展示了通过简单改变突触权重的更新规则的时间调度，可以得到比原始算法更高效、更稳定且具有收敛性保证的算法。我们提出的算法被称为递增预测编码(iPC)，也比原始算法更符合生物学可行性，因为它是完全自动的。通过一系列广泛的实验，我们展示了iPC在图像分类的许多基准测试以及条件语言模型和掩蔽语言模型的训练中，在测试准确性、效率和收敛性方面始终优于原始方法，并且对于大量超参数具有收敛性。

    Predictive coding networks are neuroscience-inspired models with roots in both Bayesian statistics and neuroscience. Training such models, however, is quite inefficient and unstable. In this work, we show how by simply changing the temporal scheduling of the update rule for the synaptic weights leads to an algorithm that is much more efficient and stable than the original one, and has theoretical guarantees in terms of convergence. The proposed algorithm, that we call incremental predictive coding (iPC) is also more biologically plausible than the original one, as it it fully automatic. In an extensive set of experiments, we show that iPC constantly performs better than the original formulation on a large number of benchmarks for image classification, as well as for the training of both conditional and masked language models, in terms of test accuracy, efficiency, and convergence with respect to a large set of hyperparameters.
    
[^219]: 是稳定还是不稳定，这是一个问题：理解神经网络在逆问题中的应用

    To be or not to be stable, that is the question: understanding neural networks for inverse problems

    [https://arxiv.org/abs/2211.13692](https://arxiv.org/abs/2211.13692)

    本文分析了神经网络在解决线性成像逆问题时稳定性和准确性之间的权衡，并提出了多种解决方案来增加网络的稳定性和保持准确性。数值实验验证了理论结果和效果。

    

    在信号和图像处理中出现的线性逆问题的解决是一个具有挑战性的问题，因为病态条件会在解中放大数据中存在的噪声。最近引入的基于深度学习的算法在性能上超越了传统的基于模型的方法，但它们通常对数据扰动不稳定。本文在非欠定情况下，从理论上分析了神经网络在解决线性成像逆问题时稳定性和准确性之间的权衡。此外，我们提出了不同的有监督和无监督解决方案，通过网络训练期间从基于模型的迭代方案中继承的正则化属性和在神经网络中应用的预处理稳定化算子来增加网络的稳定性并保持良好的准确性。对图像去模糊的大量数值实验验证了理论结果和效果。

    The solution of linear inverse problems arising, for example, in signal and image processing is a challenging problem since the ill-conditioning amplifies, in the solution, the noise present in the data. Recently introduced algorithms based on deep learning overwhelm the more traditional model-based approaches in performance, but they typically suffer from instability with respect to data perturbation. In this paper, we theoretically analyze the trade-off between stability and accuracy of neural networks, when used to solve linear imaging inverse problems for not under-determined cases. Moreover, we propose different supervised and unsupervised solutions to increase the network stability and maintain a good accuracy, by means of regularization properties inherited from a model-based iterative scheme during the network training and pre-processing stabilizing operator in the neural networks. Extensive numerical experiments on image deblurring confirm the theoretical results and the effec
    
[^220]: 对抗性赌博机针对任意策略的研究

    Adversarial Bandits against Arbitrary Strategies

    [https://arxiv.org/abs/2205.14839](https://arxiv.org/abs/2205.14839)

    该论文研究了对抗性赌博机问题，针对任意策略，提出了使用在线镜像下降方法的主控基础框架，并使用自适应学习率的OMD来减轻方差的影响，取得了较好的结果。

    

    我们研究了针对任意策略的对抗性赌博机问题，其中S是问题难度的参数，该参数对于代理人来说是未知的。为了解决这个问题，我们采用了使用在线镜像下降方法（OMD）的主控基础框架。我们首先提供了一个具有简单OMD的主控基础算法，实现了$\tilde{O}(S^{1/2}K^{1/3}T^{2/3})$的结果，其中$T^{2/3}$来自损失估计器的方差。为了减轻方差的影响，我们提出使用自适应学习率的OMD，并实现了$\tilde{O}(\min\{\mathbb{E}[\sqrt{SKT\rho_T(h^\dagger)}],S\sqrt{KT}\})$的结果，其中$\rho_T(h^\dagger)$是损失估计器的方差项。

    We study the adversarial bandit problem against arbitrary strategies, in which $S$ is the parameter for the hardness of the problem and this parameter is not given to the agent. To handle this problem, we adopt the master-base framework using the online mirror descent method (OMD). We first provide a master-base algorithm with simple OMD, achieving $\tilde{O}(S^{1/2}K^{1/3}T^{2/3})$, in which $T^{2/3}$ comes from the variance of loss estimators. To mitigate the impact of the variance, we propose using adaptive learning rates for OMD and achieve $\tilde{O}(\min\{\mathbb{E}[\sqrt{SKT\rho_T(h^\dagger)}],S\sqrt{KT}\})$, where $\rho_T(h^\dagger)$ is a variance term for loss estimators.
    
[^221]: 在无线网络中协作谱学习的中等访问控制协议

    Medium Access Control protocol for Collaborative Spectrum Learning in Wireless Networks

    [https://arxiv.org/abs/2111.12581](https://arxiv.org/abs/2111.12581)

    本文提出了一种中等访问控制协议，可在高负载网络中实现最小后悔和高谱效的谱协作。该协议基于完全分布式算法，解决了信道分配和访问调度问题，并通过单信道机会载波感知进行低复杂度的分布式拍卖。

    

    近年来，提供用于谱协作的学习算法的努力不断增加。本文提出了一种中等访问控制协议，可以在高负载网络中实现谱协作，同时实现最小后悔和高谱效。我们提出了一种在拥塞的自组织网络中进行谱协作的完全分布式算法。该算法同时解决了信道分配和访问调度问题。我们证明了该算法具有最佳的对数后悔率。基于该算法，我们提供了一种中等访问控制协议，可以在自组织网络中进行分布式实现该算法。该协议利用单信道机会载波感知，在时间和频率上进行低复杂度的分布式拍卖。我们还讨论了限定帧大小和收敛速度等实际实现问题。通过计算机模拟，将该算法与最先进的分布式中等访问控制进行了比较。

    In recent years there is a growing effort to provide learning algorithms for spectrum collaboration. In this paper we present a medium access control protocol which allows spectrum collaboration with minimal regret and high spectral efficiency in highly loaded networks. We present a fully-distributed algorithm for spectrum collaboration in congested ad-hoc networks. The algorithm jointly solves both the channel allocation and access scheduling problems. We prove that the algorithm has an optimal logarithmic regret. Based on the algorithm we provide a medium access control protocol which allows distributed implementation of the algorithm in ad-hoc networks. The protocol utilizes single-channel opportunistic carrier sensing to carry out a low-complexity distributed auction in time and frequency. We also discuss practical implementation issues such as bounded frame size and speed of convergence. Computer simulations comparing the algorithm to state-of-the-art distributed medium access con
    
[^222]: 半监督学习用于可推广的颅内出血检测和分割

    Semi-supervised learning for generalizable intracranial hemorrhage detection and segmentation

    [https://arxiv.org/abs/2105.00582](https://arxiv.org/abs/2105.00582)

    本研究开发和评估了一种半监督学习模型，可用于颅内出血的检测和分割，在超出分布的数据集上具有良好的推广能力。

    

    目的：开发和评估一种半监督学习模型，用于颅内出血的检测和分割，并在超出分布的头部CT评估数据集上进行测试。材料和方法：这项回顾性研究使用半监督学习来提高性能。首先，我们在2010年至2017年间收集的457个像素标记的头部CT扫描上训练了一个“教师”深度学习模型，并用它在从RSNA和ASNR得到的另一个未标记的语料库中生成伪标签。然后我们用这个像素和伪标签结合的数据集训练了第二个“学生”模型。在93个扫描的验证集上进行了超参数调整。将分类（n=481个扫描）和分割（n=23个扫描，或529个图像）的测试分别在CQ500数据集上进行，该数据集包含了在印度进行的481个扫描，以评估模型的超出分布的推广能力。与只使用标记数据训练的基准模型进行了比较。

    Purpose: To develop and evaluate a semi-supervised learning model for intracranial hemorrhage detection and segmentation on an out-of-distribution head CT evaluation set.   Materials and Methods: This retrospective study used semi-supervised learning to bootstrap performance. An initial "teacher" deep learning model was trained on 457 pixel-labeled head CT scans collected from one US institution from 2010-2017 and used to generate pseudo-labels on a separate unlabeled corpus of 25000 examinations from the RSNA and ASNR. A second "student" model was trained on this combined pixel- and pseudo-labeled dataset. Hyperparameter tuning was performed on a validation set of 93 scans. Testing for both classification (n=481 examinations) and segmentation (n=23 examinations, or 529 images) was performed on CQ500, a dataset of 481 scans performed in India, to evaluate out-of-distribution generalizability. The semi-supervised model was compared with a baseline model trained on only labeled data usin
    
[^223]: 《CAN IDS数据综合指南及ROAD数据集介绍》

    A Comprehensive Guide to CAN IDS Data & Introduction of the ROAD Dataset

    [https://arxiv.org/abs/2012.14600](https://arxiv.org/abs/2012.14600)

    提供了现有开放式CAN入侵数据集的综合指南，包括对每个数据集的质量分析以及各自的优点、缺点和建议的使用案例。当前数据集仅限于真实制造和模拟攻击，缺乏保真度和对车辆物理影响的验证。

    

    虽然现代车辆中普遍存在着控制器局域网（CAN），但CAN缺乏基本的安全特性，容易受到攻击。CAN安全研究领域迅速发展，致力于检测CAN上的入侵。为了帮助研究人员，我们首次提出了现有开放式CAN入侵数据集的综合指南，包括对每个数据集的质量分析以及各自的优点、缺点和建议的使用案例。当前公开的CAN IDS数据集仅限于真实制造（简单信息插入）攻击和使用合成数据进行模拟攻击，缺乏保真度。总体上，现有数据集中未验证攻击对车辆的物理影响。只有一个数据集提供了信号转换数据，但没有相应的原始二进制版本。

    Although ubiquitous in modern vehicles, Controller Area Networks (CANs) lack basic security properties and are easily exploitable. A rapidly growing field of CAN security research has emerged that seeks to detect intrusions on CANs. Producing vehicular CAN data with a variety of intrusions is out of reach for most researchers as it requires expensive assets and expertise. To assist researchers, we present the first comprehensive guide to the existing open CAN intrusion datasets, including a quality analysis of each dataset and an enumeration of each's benefits, drawbacks, and suggested use case. Current public CAN IDS datasets are limited to real fabrication (simple message injection) attacks and simulated attacks often in synthetic data, which lack fidelity. In general, the physical effects of attacks on the vehicle are not verified in the available datasets. Only one dataset provides signal-translated data but not a corresponding raw binary version. Overall, the available data pigeon
    
[^224]: 基于领域自适应的可解释图像情绪识别，并利用面部表情识别

    Domain Adaptation based Interpretable Image Emotion Recognition using Facial Expression Recognition

    [https://arxiv.org/abs/2011.08388](https://arxiv.org/abs/2011.08388)

    本论文提出了一种基于领域自适应的图像情绪识别方法，通过提出面部情绪识别系统并将其适应为图像情绪识别系统，解决了预训练模型和数据集不足的挑战。同时提出了一种新颖的解释性方法，用于解释情绪识别中关键的视觉特征。

    

    本文提出了一种领域自适应技术，用于识别包含面部和非面部物体以及非人类组件的通用图像中的情绪。它解决了图像情绪识别（IER）中预训练模型和良好注释数据集的不足挑战。首先，提出了一种基于深度学习的面部情绪识别（FER）系统，将给定的面部图像分类为离散情绪类别。然后，提出了一种图像识别系统，将提出的FER系统适应于利用领域自适应识别图像所传达的情绪。它将通用图像分类为“快乐”，“悲伤”，“仇恨”和“愤怒”类别。还提出了一种新颖的解释性方法，称为分而治之的Shap（DnCShap），用于解释情绪识别中高度相关的视觉特征。

    A domain adaptation technique has been proposed in this paper to identify the emotions in generic images containing facial & non-facial objects and non-human components. It addresses the challenge of the insufficient availability of pre-trained models and well-annotated datasets for image emotion recognition (IER). It starts with proposing a facial emotion recognition (FER) system and then moves on to adapting it for image emotion recognition. First, a deep-learning-based FER system has been proposed that classifies a given facial image into discrete emotion classes. Further, an image recognition system has been proposed that adapts the proposed FER system to recognize the emotions portrayed by images using domain adaptation. It classifies the generic images into 'happy,' 'sad,' 'hate,' and 'anger' classes. A novel interpretability approach, Divide and Conquer based Shap (DnCShap), has also been proposed to interpret the highly relevant visual features for emotion recognition. The prop
    
[^225]: 贝叶斯先验和变分推理中的惩罚之间的等价关系

    An Equivalence between Bayesian Priors and Penalties in Variational Inference

    [https://arxiv.org/abs/2002.00178](https://arxiv.org/abs/2002.00178)

    这篇论文描述了贝叶斯先验和变分推理中的惩罚之间的等价关系，并提供了一种计算给定惩罚项所对应的先验的方法。

    

    在机器学习中，常常通过调节某些参数值的惩罚项来优化概率模型的参数。正则化项在变分推理中是自然地出现的，它是一种可行的近似贝叶斯后验的方法：要优化的损失函数包含了近似后验与贝叶斯先验之间的Kullback-Leibler散度项。我们完全描述了这个过程中可能出现的正则化项，并提供了一种系统的方法来计算与给定惩罚项相对应的先验。这种特征化可以用来发现对惩罚函数的约束条件，以保持整个过程具有贝叶斯性质。

    In machine learning, it is common to optimize the parameters of a probabilistic model, modulated by an ad hoc regularization term that penalizes some values of the parameters. Regularization terms appear naturally in Variational Inference, a tractable way to approximate Bayesian posteriors: the loss to optimize contains a Kullback--Leibler divergence term between the approximate posterior and a Bayesian prior. We fully characterize the regularizers that can arise according to this procedure, and provide a systematic way to compute the prior corresponding to a given penalty. Such a characterization can be used to discover constraints over the penalty function, so that the overall procedure remains Bayesian.
    
[^226]: PBSCSR：钢琴黑市乐谱作曲家风格识别数据集

    PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset. (arXiv:2401.16803v1 [cs.SD])

    [http://arxiv.org/abs/2401.16803](http://arxiv.org/abs/2401.16803)

    本研究介绍了PBSCSR数据集，用于研究钢琴乐谱作曲家风格识别。数据集包含了盗版乐谱图像和相关元数据，可以进行多个研究任务。

    

    本文介绍了PBSCSR数据集，用于研究钢琴乐谱作曲家风格识别。我们的目标是创建一个研究作曲家风格识别的数据集，它既像MNIST一样易于获取，又像ImageNet一样具有挑战性。为了实现这个目标，我们从IMSLP的钢琴乐谱图像中采样固定长度的盗版乐谱片段。数据集本身包含40,000个62x64的盗版乐谱图像，用于进行9分类任务，以及100,000个62x64的盗版乐谱图像，用于进行100分类任务，还有29,310个无标签的可变长度的盗版乐谱图像，用于预训练。标记数据以与MNIST图像类似的形式呈现，以便极其方便地可视化、操作和训练模型。此外，我们还包括相关的元数据，以允许访问IMSLP上的原始乐谱图像和其他相关数据。我们描述了几个可以使用该数据进行研究的任务。

    This article motivates, describes, and presents the PBSCSR dataset for studying composer style recognition of piano sheet music. Our overarching goal was to create a dataset for studying composer style recognition that is "as accessible as MNIST and as challenging as ImageNet." To achieve this goal, we sample fixed-length bootleg score fragments from piano sheet music images on IMSLP. The dataset itself contains 40,000 62x64 bootleg score images for a 9-way classification task, 100,000 62x64 bootleg score images for a 100-way classification task, and 29,310 unlabeled variable-length bootleg score images for pretraining. The labeled data is presented in a form that mirrors MNIST images, in order to make it extremely easy to visualize, manipulate, and train models in an efficient manner. Additionally, we include relevant metadata to allow access to the underlying raw sheet music images and other related data on IMSLP. We describe several research tasks that could be studied with the data
    
[^227]: cDVGAN: 一个灵活的模型用于多类引力波信号和故障生成

    cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and Glitch Generation. (arXiv:2401.16356v2 [physics.ins-det] UPDATED)

    [http://arxiv.org/abs/2401.16356](http://arxiv.org/abs/2401.16356)

    cDVGAN是一个灵活的生成对抗网络模型，用于模拟多类引力波信号和探测器故障，并通过引入辅助鉴别器分析一阶导数时间序列来更好地捕捉原始数据特征。

    

    模拟真实的时间域引力波（GWs）观测和GW探测器故障可以帮助推进GW数据分析。模拟数据可以通过增加用于信号搜索的数据集，平衡用于机器学习的数据集，以及验证检测方案，在下游任务中使用。在这项工作中，我们提出了cDVGAN，这是一种基于生成对抗网络框架的新型条件模型，用于模拟代表引力波（GWs）和探测器故障的多种类别的时间域观测。cDVGAN还可以通过在条件类别向量中进行插值生成跨类别变化的广义混合样本。cDVGAN在典型的GANs的二人对抗博弈中引入了一个额外的参与者，其中一个辅助鉴别器分析一阶导数时间序列。我们的结果表明，这提供了更好地捕捉原始数据特征的合成数据。

    Simulating realistic time-domain observations of gravitational waves (GWs) and GW detector glitches can help in advancing GW data analysis. Simulated data can be used in downstream tasks by augmenting datasets for signal searches, balancing data sets for machine learning, and validating detection schemes. In this work, we present Conditional Derivative GAN (cDVGAN), a novel conditional model in the Generative Adversarial Network framework for simulating multiple classes of time-domain observations that represent gravitational waves (GWs) and detector glitches. cDVGAN can also generate generalized hybrid samples that span the variation between classes through interpolation in the conditioned class vector. cDVGAN introduces an additional player into the typical 2-player adversarial game of GANs, where an auxiliary discriminator analyzes the first-order derivative time-series. Our results show that this provides synthetic data that better captures the features of the original data. cDVGAN
    
[^228]: 小阈值间隙下的好臂识别算法: lil'HDoC

    lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold Gap. (arXiv:2401.15879v1 [cs.LG])

    [http://arxiv.org/abs/2401.15879](http://arxiv.org/abs/2401.15879)

    本文提出了一种名为lil'HDoC的算法，用于解决小阈值间隙下的好臂识别问题。实验证明该算法在样本效率上优于现有算法。

    

    好臂识别（GAI）是一个纯探索性的赌博机问题，在这个问题中，一个单独的学习器会在确定一个臂是好臂时立即输出该臂。好臂被定义为期望回报大于等于给定阈值的臂。本文聚焦于小阈值间隙下的GAI问题，该间隙指的是臂的期望回报与给定阈值之间的距离。我们提出了一种名为lil'HDoC的新算法，显著改善了HDoC算法的总样本复杂度。我们证明了在小阈值间隙下，lil'HDoC算法输出的第一个λ臂的样本复杂度与原始HDoC算法相比仅有微小的差异。大量实验证明我们的算法在合成数据集和真实世界数据集上表现优于最先进的算法。

    Good arm identification (GAI) is a pure-exploration bandit problem in which a single learner outputs an arm as soon as it is identified as a good arm. A good arm is defined as an arm with an expected reward greater than or equal to a given threshold. This paper focuses on the GAI problem under a small threshold gap, which refers to the distance between the expected rewards of arms and the given threshold. We propose a new algorithm called lil'HDoC to significantly improve the total sample complexity of the HDoC algorithm. We demonstrate that the sample complexity of the first $\lambda$ output arm in lil'HDoC is bounded by the original HDoC algorithm, except for one negligible term, when the distance between the expected reward and threshold is small. Extensive experiments confirm that our algorithm outperforms the state-of-the-art algorithms in both synthetic and real-world datasets.
    
[^229]: 基于神经网络的扩散模型中的分数估计：优化和泛化

    Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization. (arXiv:2401.15604v1 [cs.LG])

    [http://arxiv.org/abs/2401.15604](http://arxiv.org/abs/2401.15604)

    本文提出了基于神经网络的扩散模型中分数估计的优化和泛化方法，并建立了对分数估计进行分析的数学框架。

    

    扩散模型已经成为与GANs相媲美的强大工具，可以生成具有改进保真度，灵活性和鲁棒性的高质量样本。这些模型的一个关键组成部分是通过分数匹配来学习分数函数。尽管在各种任务上取得了实证成功，但尚不清楚基于梯度的算法是否可以以可证实的准确性学习分数函数。作为回答这个问题的首要步骤，本文建立了一个数学框架，用于分析用梯度下降训练的神经网络来进行分数估计。我们的分析包括学习过程的优化和泛化方面。特别是，我们提出了一个参数化形式来将去噪分数匹配问题制定为带有噪声标签的回归问题。与标准的监督学习设置相比，分数匹配问题引入了独特的挑战，包括无界输入，向量值输出和额外的时间变量。

    Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved fidelity, flexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks, it remains unclear whether gradient-based algorithms can learn the score function with a provable accuracy. As a first step toward answering this question, this paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent. Our analysis covers both the optimization and the generalization aspects of the learning procedure. In particular, we propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. Compared to the standard supervised learning setup, the score-matching problem introduces distinct challenges, including unbounded input, vector-valued output, and an additional time variable, preventing
    
[^230]: TA-RNN：一种基于注意力机制的面向电子健康记录的时间感知递归神经网络架构

    TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records. (arXiv:2401.14694v1 [cs.LG])

    [http://arxiv.org/abs/2401.14694](http://arxiv.org/abs/2401.14694)

    TA-RNN和TA-RNN-AE是两种基于RNN的可解释深度学习架构，用于分析电子健康记录并预测患者的临床结果。这些架构考虑了EHR数据的不规则性和时间间隔，并采用时间嵌入的方法解决了这些问题。

    

    动机：电子健康记录（EHR）是患者医疗历史的全面资源。EHR对于利用深度学习（DL）等先进技术至关重要，使医疗提供者能够分析大量数据，提取有价值的见解，并做出精确、数据驱动的临床决策。DL方法如递归神经网络（RNN）已被用于分析EHR以建模疾病进展并预测诊断。然而，这些方法并没有解决EHR数据中一些固有的不规则性，如临床访问之间的不规则时间间隔。此外，大多数DL模型都不可解释。在这项研究中，我们提出了两种基于RNN的可解释DL架构，分别是时间感知RNN（TA-RNN）和TA-RNN-Autoencoder（TA-RNN-AE），用于预测下一次访问和多次未来访问中患者的临床结果。为了减轻不规则时间间隔的影响，我们提出了时间嵌入的方法将时间信息纳入模型中。

    Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elaps
    
[^231]: 关于高效联邦学习方法在基础模型训练中的调查

    A Survey on Efficient Federated Learning Methods for Foundation Model Training. (arXiv:2401.04472v1 [cs.LG])

    [http://arxiv.org/abs/2401.04472](http://arxiv.org/abs/2401.04472)

    这项调查研究了高效联邦学习方法在基础模型训练中的应用，提出了一个新的分类方法以优化计算和通信效率。该研究还讨论了当前广泛使用的FL框架，并展望了未来的研究潜力。

    

    联邦学习（FL）已成为一种促进隐私保护协作训练的成熟技术。然而，新的FL方法通常只涉及小型深度学习模型的贡献。随着Transformer模型的巨大成功，一个问题出现了：如何使基础模型在FL应用中实施起来？鉴于在FL中计算和通信的时间消耗通常相似，我们引入了一个关于在FL应用中的计算和通信效率方法的新的分类方法。这些方法旨在优化训练时间并减少客户端与服务器之间的通信。我们还研究了目前广泛使用的FL框架，并根据FL研究及其延伸的现有方法讨论了未来的研究潜力。

    Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training. However, new approaches to FL often discuss their contributions involving small deep-learning models only. With the tremendous success of transformer models, the following question arises: What is necessary to operationalize foundation models in an FL application? Knowing that computation and communication often take up similar amounts of time in FL, we introduce a novel taxonomy focused on computational and communication efficiency methods in FL applications. This said, these methods aim to optimize the training time and reduce communication between clients and the server. We also look at the current state of widely used FL frameworks and discuss future research potentials based on existing approaches in FL research and beyond.
    
[^232]: DiarizationLM: 基于大语言模型的说话人分离后处理

    DiarizationLM: Speaker Diarization Post-Processing with Large Language Models. (arXiv:2401.03506v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2401.03506](http://arxiv.org/abs/2401.03506)

    本文介绍了DiarizationLM框架，利用大语言模型对说话人分离系统的输出进行后处理。实验证明，使用finetuned的PaLM 2-S模型可以显著减少分离错误率，对多种目标都有优化效果。

    

    本文介绍了DiarizationLM，一个利用大语言模型（LLM）对说话人分离系统输出进行后处理的框架。这个框架可以实现多种目标，如改善分离对话转录的可读性，或减少词级分离错误率（WDER）。在这个框架中，自动语音识别（ASR）和说话人分离系统的输出被表示为一种紧凑的文本格式，其包含在一个可选择调整的LLM的提示中。LLM的输出可以作为所需改进的精细化分离结果。作为后处理步骤，该框架可以轻松应用于任何现有的ASR和说话人分离系统，无需重新训练现有的组件。我们的实验证明，finetuned的PaLM 2-S模型可以在Fisher电话对话数据集上将WDER降低55.5%，在Callhome英语数据集上降低44.9%。

    In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. Various goals can be achieved with the proposed framework, such as improving the readability of the diarized transcript, or reducing the word diarization error rate (WDER). In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset.
    
[^233]: 追求最优统计水印技术

    Towards Optimal Statistical Watermarking. (arXiv:2312.07930v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.07930](http://arxiv.org/abs/2312.07930)

    追求最优统计水印技术。通过将统计水印技术视为假设检验问题并引入伪随机生成器，我们实现了输出令牌和拒绝区域的耦合，实现了第一类错误和第二类错误之间的非平凡权衡，同时提出了最统一最有力的水印和最小化第二类错误的解决方案。我们还提供了独立同分布令牌数量的上下界，突显了改进的潜力。此外，我们还探讨了鲁棒性水印问题。

    

    我们将统计水印技术作为一个假设检验问题进行研究，这是一个泛化了所有之前统计水印方法的通用框架。我们的关键是通过实践中的伪随机生成器实现输出令牌和拒绝区域的耦合，从而允许在第一类错误和第二类错误之间进行非平凡的权衡。我们在一般的假设检验环境下表征了最统一最有力的水印以及在模型无关的环境中最小化第二类错误。在输出是$n$个令牌的常见情况下，我们对需要保证小的第一类和第二类错误的独立同分布令牌数量建立了近乎匹配的上下界。与之前的工作中的$ h ^ {-2} $速率相比，我们相对于每个令牌的平均熵$h$的速率为$ \Theta(h ^ {-1} \log (1/h)) $，突显了改进的潜力。此外，我们提出了鲁棒性水印问题，其中用户都是...

    We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-off between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in the general hypothesis testing setting and the minimax Type II error in the model-agnostic setting. In the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate of $\Theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. Moreover, we formulate the robust watermarking problem where users are all
    
[^234]: 通过以计算为代价加速广义线性模型

    Accelerating Generalized Linear Models by Trading off Computation for Uncertainty. (arXiv:2310.20285v1 [cs.LG])

    [http://arxiv.org/abs/2310.20285](http://arxiv.org/abs/2310.20285)

    本论文提出了一种迭代方法，通过增加不确定性来降低计算量，并显著提高广义线性模型的训练速度。

    

    贝叶斯广义线性模型（GLMs）定义了一个灵活的概率框架，用于建模分类、有序和连续数据，并且在实践中被广泛使用。然而，对于大型数据集，GLMs的精确推断代价太高，因此需要在实践中进行近似。造成的近似误差对模型的可靠性产生不利影响，并且没有被考虑在预测的不确定性中。在这项工作中，我们引入了一系列迭代方法，明确地对这个误差建模。它们非常适合并行计算硬件，有效地回收计算并压缩信息，以减少GLMs的时间和内存需求。正如我们在一个实际的大型分类问题上展示的那样，我们的方法通过明确地将减少计算与增加不确定性进行权衡来显著加速训练。

    Bayesian Generalized Linear Models (GLMs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in GLMs is prohibitively expensive for large datasets, thus requiring approximations in practice. The resulting approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. In this work, we introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for GLMs. As we demonstrate on a realistically large classification problem, our method significantly accelerates training by explicitly trading off reduced computation for increased uncertainty.
    
[^235]: 用生成预训练Transformer生成紧凑二进制系统波形

    Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer. (arXiv:2310.20172v1 [gr-qc])

    [http://arxiv.org/abs/2310.20172](http://arxiv.org/abs/2310.20172)

    提出了一种叫做CBS-GPT的生成预训练Transformer模型，用于紧凑二进制系统波形生成，在预测准确性上达到了较高的准确率，并且具有显著的解释性能。

    

    空间引力波探测是未来十年最受期待的引力波探测项目之一，将探测到丰富的紧凑二进制系统。然而，对于空间引力波波形的精确预测仍未被探索。为了解决探测器响应和二代时延干涉（TDI 2.0）引起的波形复杂性增加而带来的数据处理困难，提出了一种名为CBS-GPT（Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer）的可解释预训练大模型。对于紧凑二进制系统波形，训练了三个模型来预测超大质量黑洞二进制（MBHB）、极端质量比融合（EMRIs）和星系二进制（GB）的波形，分别实现了98%、91%和99%的预测准确性。CBS-GPT模型具有显著的解释性，其隐藏参数能够有效捕捉波形的复杂信息，即使是复杂的不连续。

    Space-based gravitational wave detection is one of the most anticipated gravitational wave (GW) detection projects in the next decade, which will detect abundant compact binary systems. However, the precise prediction of space GW waveforms remains unexplored. To solve the data processing difficulty in the increasing waveform complexity caused by detectors' response and second-generation time-delay interferometry (TDI 2.0), an interpretable pre-trained large model named CBS-GPT (Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer) is proposed. For compact binary system waveforms, three models were trained to predict the waveforms of massive black hole binary (MBHB), extreme mass-ratio inspirals (EMRIs), and galactic binary (GB), achieving prediction accuracies of 98%, 91%, and 99%, respectively. The CBS-GPT model exhibits notable interpretability, with its hidden parameters effectively capturing the intricate information of waveforms, even with complex ins
    
[^236]: 提升强化学习中汤普森采样的贝叶斯遗憾界

    Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. (arXiv:2310.20007v1 [stat.ML])

    [http://arxiv.org/abs/2310.20007](http://arxiv.org/abs/2310.20007)

    本文在多种情境下证明了汤普森采样在强化学习中的贝叶斯遗憾界，并通过对信息比的精确分析提出了一个基于时间不均匀强化学习问题的上界估计。同时，本文找到了各种设置中具体的界限，并讨论了这些结果是第一个其类别或改进了最先进方法的情况。

    

    本文证明了在多种情境下，汤普森采样在强化学习中的第一个贝叶斯遗憾界。我们利用离散的代理环境简化学习问题，并通过后验一致性对信息比进行了精确分析。这导致了一个基于时间不均匀强化学习问题的上界估计为$\widetilde{O}(H\sqrt{d_{l_1}T})$，其中$H$为回合长度，$d_{l_1}$为环境空间的Kolmogorov $l_1$维度。然后，我们在各种设置中找到了$d_{l_1}$的具体界限，比如表格、线性和有限混合，讨论了我们的结果是第一个其类别或改进了最先进方法的情况。

    In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.
    
[^237]: rTsfNet:一种具有多头3D旋转和时序特征提取的基于IMU的人体活动识别DNN模型

    rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition. (arXiv:2310.19283v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2310.19283](http://arxiv.org/abs/2310.19283)

    rTsfNet是一种新的DNN模型，通过多头3D旋转和时序特征提取实现了IMU-based人体活动识别，并在多个数据集上取得了最高的准确率。

    

    本论文提出了rTsfNet，一种具有多头3D旋转和时序特征提取的DNN模型，作为IMU-based人体活动识别的新型DNN模型。rTsfNet通过在DNN内部推导3D旋转参数，自动选择应该从中派生特征的3D基准。然后，利用MLP推导时序特征（TSFs）并实现HAR。尽管该模型不使用CNN，在良好管理的基准条件和多个数据集（UCI HAR, PAMAP2, Daphnet, 和OPPORTUNITY）上取得了最高的准确率，这些数据集针对不同的活动。

    This paper proposes rTsfNet, a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction, as a new DNN model for IMU-based human activity recognition (HAR). rTsfNet automatically selects 3D bases from which features should be derived by deriving 3D rotation parameters within the DNN. Then, time series features (TSFs), the wisdom of many researchers, are derived and realize HAR using MLP. Although a model that does not use CNN, it achieved the highest accuracy than existing models under well-managed benchmark conditions and multiple datasets: UCI HAR, PAMAP2, Daphnet, and OPPORTUNITY, which target different activities.
    
[^238]: 使用概念漂移解释方法对水配水网络中的小漏洞进行定位

    Localization of Small Leakages in Water Distribution Networks using Concept Drift Explanation Methods. (arXiv:2310.15830v1 [cs.LG])

    [http://arxiv.org/abs/2310.15830](http://arxiv.org/abs/2310.15830)

    本研究提出了一种只使用压力测量进行水配水网络中小漏洞定位的方法，旨在解决因气候变化导致的饮用水稀缺问题。

    

    面对气候变化，饮用水的可用性将来会减少，使得饮用水成为越来越稀缺的资源。大量的水通过水运输和配水网络中的漏洞流失。漏洞的检测和定位是具有挑战性的问题，由于水配水网络中的复杂相互作用和需求的变化。尤其是小漏洞很难确定，但它们的定位对于避免长时间的水损失至关重要。虽然存在不同的方法来解决漏洞的检测和定位任务，但它们依赖于系统的各种信息，例如实时需求测量和精确的网络拓扑结构，这在许多真实环境中是一个不切实际的假设。相比之下，本研究尝试仅使用压力测量来进行漏洞定位。为此，首先建立了水配水网络中的漏洞模型。

    Facing climate change the already limited availability of drinking water will decrease in the future rendering drinking water an increasingly scarce resource. Considerable amounts of it are lost through leakages in water transportation and distribution networks. Leakage detection and localization are challenging problems due to the complex interactions and changing demands in water distribution networks. Especially small leakages are hard to pinpoint yet their localization is vital to avoid water loss over long periods of time. While there exist different approaches to solving the tasks of leakage detection and localization, they are relying on various information about the system, e.g. real-time demand measurements and the precise network topology, which is an unrealistic assumption in many real-world scenarios. In contrast, this work attempts leakage localization using pressure measurements only. For this purpose, first, leakages in the water distribution network are modeled employin
    
[^239]: 使用后门技术保护我们的隐私

    Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])

    [http://arxiv.org/abs/2310.08320](http://arxiv.org/abs/2310.08320)

    本研究提出了一种基于后门攻击的防御方法，通过对模型进行策略性插入后门，对齐敏感短语与中性术语的嵌入，以删除训练数据中的私人信息。实证结果显示该方法的有效性。

    

    在使用未经筛选、常常包含敏感信息的网页数据训练大型人工智能模型的情况下，隐私问题成为了一个重要的关注点。其中一个问题是，攻击者可以利用隐私攻击的方法提取出训练数据的信息。然而，如何在不降低模型性能的情况下去除特定信息是一个不容易解决且具有挑战性的问题。我们提出了一个基于后门攻击的简单而有效的防御方法，用于从模型中删除私人信息，如个人姓名，特别是针对文本编码器的。具体而言，通过策略性地插入后门，我们将敏感短语的嵌入与中性术语的嵌入对齐，例如用"a person"代替人名。我们的实证结果通过对零样本分类器使用专门的隐私攻击测试表明了我们基于后门的防御方法的效果。我们的方法提供了一个新的"双重用途"的视角。

    The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
    
[^240]: 使用稀疏自编码器解释RLHF调整的语言模型中的奖励模型

    Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders. (arXiv:2310.08164v1 [cs.LG])

    [http://arxiv.org/abs/2310.08164](http://arxiv.org/abs/2310.08164)

    通过使用稀疏自编码器，我们提出了一种解释RLHF调整的语言模型中学习的奖励函数的新方法。这个方法能够通过比较自编码器隐藏空间来识别学习奖励模型准确性的独特特征，并提供了奖励完整性的抽象近似值。

    

    通过稀疏自编码器，我们提出了一种解释RLHF调整的语言模型中学习的奖励函数的新方法。我们的方法利用基本语言模型和经过RLHF调整的版本的激活来训练自编码器集合，并通过比较自编码器隐藏空间来识别反映学习奖励模型准确性的独特特征。为了量化这一点，我们构建了一个情景，调整的语言模型学习令牌-奖励映射以最大化奖励。这是首次应用稀疏自编码器来解释学习奖励和广泛检查语言模型中的奖励学习。我们的方法提供了奖励完整性的抽象近似值，这为确保指定目标和模型行为之间的一致性提供了一个有前景的技术。

    Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.
    
[^241]: 通过自动折扣调度从观察中进行模仿学习

    Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])

    [http://arxiv.org/abs/2310.07433](http://arxiv.org/abs/2310.07433)

    我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。

    

    人类通常通过观察和模仿来获得新的技能。对于机器人代理，从互联网上可用的大量无标签视频演示数据中进行学习，需要在没有访问其动作的情况下模仿专家，这是一种称为观察学习模仿（ILfO）的挑战。解决ILfO问题的常见方法是将其转化为逆向强化学习问题，利用从代理和专家观察中计算出的代理奖励。然而，我们发现在具有进展依赖性属性的任务中，这样的方法面临重大挑战；在这些任务中，代理需要在掌握后续行为之前先学习专家的前序行为。我们的研究表明，主要原因是分配给后续步骤的奖励信号妨碍了对初始行为的学习。为了解决这个挑战，我们提出了一个新颖的ILfO框架，使代理能够掌握早期行为。

    Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
    
[^242]: Entropy-MCMC: 轻松从平坦盆地进行采样

    Entropy-MCMC: Sampling from Flat Basins with Ease. (arXiv:2310.05401v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05401](http://arxiv.org/abs/2310.05401)

    本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。

    

    贝叶斯深度学习依赖于对后验分布的质量估计。然而，深度神经网络的后验分布在性质上是高度多模态的，局部模式表现出不同的泛化性能。在有限的计算资源下，从原始后验分布中进行采样可能会导致次优性能，因为一些样本可能会陷入“坏”模式并出现过拟合。基于观察到低泛化误差的“好”模式通常存在于能量景观的平坦盆地中，我们提出通过偏置采样朝向这些平坦区域的后验。具体而言，我们引入了一个辅助引导变量，其稳态分布类似于平滑后验分布，并且没有尖锐的模态，以引导MCMC采样器在平坦的盆地中采样。通过将此引导变量与模型参数相结合，我们创建了一个简单的联合分布，可以在最小计算开销下实现高效采样。我们证明了我们的元算法的收敛性。

    Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our met
    
[^243]: 深度变分多变量信息瓶颈--一种变分损失的框架

    Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses. (arXiv:2310.03311v1 [cs.LG])

    [http://arxiv.org/abs/2310.03311](http://arxiv.org/abs/2310.03311)

    该论文介绍了一个基于信息理论的统一原理，用于重新推导和推广现有的变分降维方法，并设计新的方法。通过将多变量信息瓶颈解释为两个贝叶斯网络的权衡，该框架引入了一个在压缩数据和保留信息之间的权衡参数。

    

    变分降维方法以其高精度、生成能力和鲁棒性而闻名。这些方法有很多理论上的证明。在这里，我们介绍了一种基于信息理论的统一原理，重新推导和推广了现有的变分方法，并设计了新的方法。我们的框架基于多变量信息瓶颈的解释，其中两个贝叶斯网络相互权衡。我们将第一个网络解释为编码器图，它指定了在压缩数据时要保留的信息。我们将第二个网络解释为解码器图，它为数据指定了一个生成模型。使用这个框架，我们重新推导了现有的降维方法，如深度变分信息瓶颈(DVIB)、beta变分自编码器(beta-VAE)和深度变分规范相关分析(DVCCA)。该框架自然地引入了一个在压缩数据和保留信息之间的权衡参数。

    Variational dimensionality reduction methods are known for their high accuracy, generative abilities, and robustness. These methods have many theoretical justifications. Here we introduce a unifying principle rooted in information theory to rederive and generalize existing variational methods and design new ones. We base our framework on an interpretation of the multivariate information bottleneck, in which two Bayesian networks are traded off against one another. We interpret the first network as an encoder graph, which specifies what information to keep when compressing the data. We interpret the second network as a decoder graph, which specifies a generative model for the data. Using this framework, we rederive existing dimensionality reduction methods such as the deep variational information bottleneck (DVIB), beta variational auto-encoders (beta-VAE), and deep variational canonical correlation analysis (DVCCA). The framework naturally introduces a trade-off parameter between compr
    
[^244]: 使用可传输的图自编码器进行网络对齐

    Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])

    [http://arxiv.org/abs/2310.03272](http://arxiv.org/abs/2310.03272)

    该论文提出了一种基于图自编码器的网络对齐方法，通过生成与图的特征值和特征向量相关的节点嵌入，实现了更准确的对齐。同时，该方法还利用迁移学习和数据增强技术，在大规模网络对齐任务中实现高效的对齐，无需重新训练。

    

    网络对齐是在不同图之间建立一对一对应关系的任务，在高影响领域中有大量应用。然而，这个任务在一般情况下被认为是NP难的，而且现有的算法在图的规模增大时无法扩展。为了解决这两个挑战，我们提出了一种新颖的广义图自编码器架构，旨在提取强大且鲁棒的节点嵌入，适用于对齐任务。我们证明生成的嵌入与图的特征值和特征向量相关，并且与经典谱方法相比可以实现更准确的对齐。我们提出的框架还利用迁移学习和数据增强，在无需重新训练的情况下实现高效的大规模网络对齐。在真实世界的图上进行了广泛的网络对齐和子网络对齐实验，提供了支持该框架有效性和可扩展性的证据。

    Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
    
[^245]: PolySketchFormer:基于草图的多项式核变换器加速Transformer

    PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels. (arXiv:2310.01655v1 [cs.LG])

    [http://arxiv.org/abs/2310.01655](http://arxiv.org/abs/2310.01655)

    本文通过使用多项式函数和多项式草图，实现了一个快速注意力机制PolySketchFormer，以突破Transformer架构中注意力机制的二次复杂性难题，无需假设注意力矩阵具有稀疏结构，并提出了高效的基于块的算法。

    

    Transformer架构中注意力机制的二次复杂性一直是扩展大型基础模型进行长上下文任务的瓶颈。实际上，最近的理论结果表明，在假设强指数时间假设的情况下，近似softmax注意力机制的输出在次二次时间内是困难的。本文通过用多项式函数和多项式草图替代softmax来突破这个理论障碍。特别是，我们展示了从随机数值线性代数文献中的多项式核的草图可以用于近似多项式注意力，从而实现了显著更快的注意力机制，而不需要假设注意力矩阵具有稀疏结构，这在许多先前的工作中已经完成。此外，我们提出了一种高效的基于块的算法，该算法使我们能够将因果掩码应用于注意力矩阵，而无需显式地计算$n \times n$注意力矩阵并计算输出。

    The quadratic complexity of attention in transformer architectures remains a big bottleneck in scaling up large foundation models for long context. In fact, recent theoretical results show the hardness of approximating the output of softmax attention mechanism in sub-quadratic time assuming Strong Exponential Time Hypothesis. In this paper, we show how to break this theoretical barrier by replacing softmax with a polynomial function and polynomial sketching. In particular we show that sketches for Polynomial Kernel from the randomized numerical linear algebra literature can be used to approximate the polynomial attention which leads to a significantly faster attention mechanism without assuming any sparse structure for the attention matrix that has been done in many previous works.  In addition, we propose an efficient block-based algorithm that lets us apply the causal mask to the attention matrix without explicitly realizing the $n \times n$ attention matrix and compute the output of
    
[^246]: FENDA-FL：异构临床数据的个性化联邦学习

    FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets. (arXiv:2309.16825v1 [cs.LG])

    [http://arxiv.org/abs/2309.16825](http://arxiv.org/abs/2309.16825)

    该论文提出了一种针对异构临床数据的个性化联邦学习方法，实验证明该方法在性能上超越了现有的全局和个性化联邦学习技术，并且对FLamby基准进行了实质性改进和扩展。

    

    联邦学习（FL）被越来越认为是克服临床环境中数据孤立问题的关键方法。该研究在临床应用的FL研究中做出了三个重要方面的贡献。首先，提出了将FENDA方法（Kim等人，2016）扩展到FL的方法。在FLamby基准（du Terrail等人，2022a）和GEMINI数据集（Verma等人，2017）上进行的实验表明，该方法对异构临床数据具有鲁棒性，并且通常优于现有的全局和个性化FL技术。此外，实验结果在原有的FLamby基准上表示出实质性的改进，并扩展了这些基准以包括个性化FL方法的评估。最后，我们提倡建立一个全面的FL检查点和评估框架，以更好地反映实际环境并提供

    Federated learning (FL) is increasingly being recognized as a key approach to overcoming the data silos that so frequently obstruct the training and deployment of machine-learning models in clinical settings. This work contributes to a growing body of FL research specifically focused on clinical applications along three important directions. First, an extension of the FENDA method (Kim et al., 2016) to the FL setting is proposed. Experiments conducted on the FLamby benchmarks (du Terrail et al., 2022a) and GEMINI datasets (Verma et al., 2017) show that the approach is robust to heterogeneous clinical data and often outperforms existing global and personalized FL techniques. Further, the experimental results represent substantive improvements over the original FLamby benchmarks and expand such benchmarks to include evaluation of personalized FL methods. Finally, we advocate for a comprehensive checkpointing and evaluation framework for FL to better reflect practical settings and provide
    
[^247]: 基于多智能体深度强化学习的AI驱动患者监测

    AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])

    [http://arxiv.org/abs/2309.10980](http://arxiv.org/abs/2309.10980)

    本研究提出了一种基于多智能体深度强化学习的AI驱动患者监测框架，通过部署多个学习智能体，针对不同的生理特征进行监测，并根据紧急程度预警医疗紧急团队。

    

    有效的患者监测对及时干预和改善医疗结果至关重要。传统的监测系统往往难以处理复杂、动态的环境和波动的生命体征，导致延迟发现危急情况。为了应对这一挑战，我们提出了一种新颖的基于多智能体深度强化学习（DRL）的AI驱动患者监测框架。我们的方法部署了多个学习智能体，每个智能体专门负责监测特定的生理特征，如心率、呼吸和体温。这些智能体与通用的医疗监测环境进行交互，学习患者的行为模式，并根据估计的紧急程度做出通知相应医疗紧急团队（MET）的决策。在本研究中，我们使用来自两个数据集（PPG-DaLiA和WESAD）的真实生理和运动数据评估了提出的多智能体DRL框架的性能。

    Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
    
[^248]: 分布式资源管理中的价格差异化游戏对联合学习的影响

    Price-Discrimination Game for Distributed Resource Management in Federated Learning. (arXiv:2308.13838v1 [cs.LG])

    [http://arxiv.org/abs/2308.13838](http://arxiv.org/abs/2308.13838)

    本论文提出了一种价格差异化游戏（PDG），通过对不同客户提供的服务进行定价差异化，改善了联合学习的性能并降低了激励客户参与联合学习的成本。

    

    在传统的联合学习中，参数服务器和多个分布式客户端可以形成典型的买方市场，其中PS/买家数量远远少于客户端/卖家数量。为了改善联合学习的性能并减少激励客户参与联合学习的成本，本文提出了对不同客户提供的服务进行定价差异化，而不是简单地为不同客户提供相同的服务定价。价格差异化基于对联合学习带来的性能改进和计算通信能力的异质性。为此，本文提出了一个价格差异化游戏（PDG），全面解决了联合学习中的分布式资源管理问题，包括多目标权衡、客户端选择和激励机制。由于PDG是一个混合整数非线性规划（MINLP）问题，本文提出了一个具有低计算成本的分布式半启发式算法来解决该问题。

    In vanilla federated learning (FL) such as FedAvg, the parameter server (PS) and multiple distributed clients can form a typical buyer's market, where the number of PS/buyers of FL services is far less than the number of clients/sellers. In order to improve the performance of FL and reduce the cost of motivating clients to participate in FL, this paper proposes to differentiate the pricing for services provided by different clients rather than simply providing the same service pricing for different clients. The price is differentiated based on the performance improvements brought to FL and their heterogeneity in computing and communication capabilities. To this end, a price-discrimination game (PDG) is formulated to comprehensively address the distributed resource management problems in FL, including multi-objective trade-off, client selection, and incentive mechanism. As the PDG is a mixed-integer nonlinear programming (MINLP) problem, a distributed semi-heuristic algorithm with low c
    
[^249]: 通过一致性预言机进行简单的在线学习

    Simple online learning with consistency oracle. (arXiv:2308.08055v1 [cs.LG])

    [http://arxiv.org/abs/2308.08055](http://arxiv.org/abs/2308.08055)

    该论文介绍了在只能通过一致性预言机访问类的模型下的在线学习算法，提出了一种更简单且效果更好的算法。该算法最多会犯O(256^d)个错误，并观察到不存在一个最多会犯2^(d+1)-2个错误的算法。

    

    我们考虑在只能通过一致性预言机访问类的模型下的在线学习——在任何时刻，预言机都能给出与目前为止看到的所有示例一致的类函数。该模型最近由Assos等人（COLT'23）考虑。这个模型的动机是标准的在线学习方法依赖于计算子类的Littlestone维度，这是一个计算复杂的问题。Assos等人在这个模型中给出了一个在线学习算法，对于Littlestone维度为d的类，最多会犯C^d个错误，其中C是一个未指定的绝对常数且大于0。我们提出了一个新的算法，最多会犯O(256^d)个错误。我们的证明更简单，只使用了Littlestone维度的基本属性。我们还观察到，不存在一个在这个模型中最多会犯2^(d+1)-2个错误的算法。我们还观察到，我们的算法（以及Assos等人的算法）。

    We consider online learning in the model where a learning algorithm can access the class only via the consistency oracle -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al. (COLT'23). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a problem that is computationally intractable. Assos et al. gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C > 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also observe that there exists no algorithm in this model that makes at most $2^{d+1}-2$ mistakes. We also observe that our algorithm (as well as the algorithm of Assos et al.
    
[^250]: 当解析式微积分破解AdaBoost密码时

    When Analytic Calculus Cracks AdaBoost Code. (arXiv:2308.01070v1 [cs.LG])

    [http://arxiv.org/abs/2308.01070](http://arxiv.org/abs/2308.01070)

    本文表明AdaBoost只是一种名义上的算法，因为可以使用真值表明确地计算得到弱分类器的组合。

    

    监督式学习中的增强原理涉及将多个弱分类器组合以获得一个更强的分类器。AdaBoost被认为是这种方法的一个完美例子。我们之前已经证明了AdaBoost并不真正是一个优化算法。本文表明，AdaBoost只是一种名义上的算法，因为可以使用真值表明确地计算得到弱分类器的组合。本研究通过考虑一个两类问题来进行，以三个二元分类器的特殊情况为例，并与Python库scikit-learn中AdaBoost算法的实现结果进行对比。

    The principle of boosting in supervised learning involves combining multiple weak classifiers to obtain a stronger classifier. AdaBoost has the reputation to be a perfect example of this approach. We have previously shown that AdaBoost is not truly an optimization algorithm. This paper shows that AdaBoost is an algorithm in name only, as the resulting combination of weak classifiers can be explicitly calculated using a truth table. This study is carried out by considering a problem with two classes and is illustrated by the particular case of three binary classifiers and presents results in comparison with those from the implementation of AdaBoost algorithm of the Python library scikit-learn.
    
[^251]: 深度展开网络与循环动量加速用于非线性反问题

    Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems. (arXiv:2307.16120v1 [cs.LG])

    [http://arxiv.org/abs/2307.16120](http://arxiv.org/abs/2307.16120)

    本文提出了一种使用循环动量加速的深度展开网络，该网络能够有效应用于非线性逆向成像问题。通过利用长短期记忆循环神经网络学习和保留先前梯度的知识，该方法在两个非线性逆向问题上获得了良好的结果。

    

    结合基于模型的迭代算法和数据驱动的深度学习解决方案，深度展开网络(DuNets)已成为解决逆向成像问题的流行工具。虽然DuNets已成功应用于许多线性逆向问题，但非线性问题往往会影响方法的性能。受优化算法中常用的动量加速技术启发，我们提出了一种循环动量加速(RMA)框架，该框架使用长短期记忆循环神经网络(LSTM-RNN)来模拟动量加速过程。RMA模块利用LSTM-RNN学习和保留先前梯度的知识能力。我们将RMA应用于两种流行的DuNets——学习的近端梯度下降(LPGD)和学习的原始-对偶(LPD)方法，分别得到LPGD-RMA和LPD-RMA。我们在两个非线性逆向问题上提供了实验结果：非线性去卷积问题、

    Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, an
    
[^252]: 自校准分类器引导下，少量标记数据的基于评分的条件生成

    Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance. (arXiv:2307.04081v1 [cs.CV])

    [http://arxiv.org/abs/2307.04081](http://arxiv.org/abs/2307.04081)

    本论文提出了一种通过自校准分类器引导的方法改进基于评分的条件生成模型，以提高使用少量标记数据的准确性和性能。通过将分类器作为无条件生成模型的另一种视角，并利用标记和未标记数据来校准分类器，实验证实该方法的有效性。

    

    基于评分的生成模型（SGMs）是一种流行的深度生成模型家族，能够达到领先的图像生成质量。早期的研究已经将SGMs扩展到处理类条件生成，通过将无条件的SGM与经过训练的分类器的引导相结合。然而，这种分类器引导的SGMs在训练数量较少的标记数据时并不总能实现准确的条件生成。我们认为问题根源在于分类器的不可靠梯度和无法充分利用未标记数据。因此，我们提出通过让分类器自校准来改进分类器引导的SGMs。我们的关键思想是使用能量模型的原则将分类器转化为无条件SGM的另一种视角。然后，可以采用现有的无条件SGM损失函数来使用标记和未标记数据来校准分类器。实证结果验证了所提出方法显著改善了条件生成的性能。

    Score-based Generative Models (SGMs) are a popular family of deep generative models that achieves leading image generation quality. Earlier studies have extended SGMs to tackle class-conditional generation by coupling an unconditional SGM with the guidance of a trained classifier. Nevertheless, such classifier-guided SGMs do not always achieve accurate conditional generation, especially when trained with fewer labeled data. We argue that the issue is rooted in unreliable gradients of the classifier and the inability to fully utilize unlabeled data during training. We then propose to improve classifier-guided SGMs by letting the classifier calibrate itself. Our key idea is to use principles from energy-based models to convert the classifier as another view of the unconditional SGM. Then, existing loss for the unconditional SGM can be adopted to calibrate the classifier using both labeled and unlabeled data. Empirical results validate that the proposed approach significantly improves the
    
[^253]: 一种用于深度学习的合成心电图（ECG）图像生成工具箱，以促进扫描ECG数字化

    A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization. (arXiv:2307.01946v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.01946](http://arxiv.org/abs/2307.01946)

    本文介绍了一种用于生成合成ECG图像的工具箱，旨在促进扫描ECG数字化。通过引入真实伪影，如手写文本伪影、皱纹、折痕和视角变换，该方法可以在标准纸质ECG背景上生成具有真实性的ECG图像。这种方法有助于解决合成ECG图像中缺乏参考时间序列的问题。

    

    心电图（ECG）是一种准确且广泛应用于诊断心血管疾病的工具。几十年来，ECG以印刷格式记录，并且将它们的数字化在算法性心电图诊断的机器学习模型训练中具有巨大潜力。物理性ECG存档面临退化风险，仅扫描印刷ECG是不够的，因为机器学习模型需要ECG时间序列数据。因此，将纸质ECG存档数字化和转换为时间序列数据至关重要。深度学习模型在图像处理方面显示出潜力。然而，具有参考时间序列的ECG存档稀缺是一个挑战。利用“数字孪生”的数据增强技术可能是一个潜在的解决方案。我们介绍了一种新的方法，以生成具有真实伪影的标准纸质ECG背景下的合成ECG图像。包括手写文本伪影、皱纹、折痕和视角转换等畸变。

    The electrocardiogram (ECG) is an accurate and widely available tool for diagnosing cardiovascular diseases. ECGs have been recorded in printed formats for decades and their digitization holds great potential for training machine learning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are at risk of deterioration and scanning printed ECGs alone is insufficient, as ML models require ECG time-series data. Therefore, the digitization and conversion of paper ECG archives into time-series data is of utmost importance. Deep learning models for image processing show promise in this regard. However, the scarcity of ECG archives with reference time-series is a challenge. Data augmentation techniques utilizing \textit{digital twins} present a potential solution.  We introduce a novel method for generating synthetic ECG images on standard paper-like ECG backgrounds with realistic artifacts. Distortions including handwritten text artifacts, wrinkles, creases and perspective transf
    
[^254]: 带有Shuffled SGD的经验风险最小化：原始-对偶视角和改进界限

    Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective and Improved Bounds. (arXiv:2306.12498v1 [math.OC])

    [http://arxiv.org/abs/2306.12498](http://arxiv.org/abs/2306.12498)

    本文提出了带有Shuffled SGD的经验风险最小化的原始-对偶视角和改进界限，旨在解决理论和实践之间的差距。

    

    随机梯度下降（SGD）是现代机器学习中最普遍的优化方法。与每个时期从数据集中无替换随机抽样和与（可能的）重排练的经验惯例相反，SGD的理论对应通常依赖于带替换的抽样假设。仅最近才分析了采用无替换抽样的Shuffled SGD。对于具有$n$个组件和对于每个组件函数$L$-平滑性假设的凸有限和问题，在足够小的步长（$\mathcal{O}(\frac{1}{nL})$）下，存在匹配的上下界。然而，这些界限似乎过于悲观 - 实际上，预测的性能通常不比全梯度下降更好 - 并且与经验观察不符。为了缩小理论和实践之间的差距，本文将焦点从一般有限和问题集中到了...

    Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern machine learning. Contrary to the empirical practice of sampling from the datasets without replacement and with (possible) reshuffling at each epoch, the theoretical counterpart of SGD usually relies on the assumption of sampling with replacement. It is only very recently that SGD with sampling without replacement -- shuffled SGD -- has been analyzed. For convex finite sum problems with $n$ components and under the $L$-smoothness assumption for each component function, there are matching upper and lower bounds, under sufficiently small -- $\mathcal{O}(\frac{1}{nL})$ -- step sizes. Yet those bounds appear too pessimistic -- in fact, the predicted performance is generally no better than for full gradient descent -- and do not agree with the empirical observations. In this work, to narrow the gap between the theory and practice of shuffled SGD, we sharpen the focus from general finite sum problem
    
[^255]: Deep Fusion：基于预训练初始化的高效网络训练

    Deep Fusion: Efficient Network Training via Pre-trained Initializations. (arXiv:2306.11903v1 [cs.LG])

    [http://arxiv.org/abs/2306.11903](http://arxiv.org/abs/2306.11903)

    本文提出了Deep Fusion，一种基于预训练初始化的高效网络训练方法。通过加速训练过程、降低计算要求，并导致改进的泛化性能，使得该方法在维持传统训练方法的性能甚至超越其性能的同时，减少了训练时间和资源消耗。

    

    近年来，深度学习在众多领域中取得了显著进展，尤其是对自然语言处理任务产生了显著影响。训练深度神经网络的挑战之一是需要大量的计算资源和时间。本文提出了Deep Fusion，一种利用较小网络的预训练初始化的高效网络训练方法。我们展示了Deep Fusion在各种NLP任务和T5模型大小上加速训练过程，降低计算要求，并导致改进的泛化性能。我们的实验表明，Deep Fusion是一种实用和有效的方法，可以在维持传统训练方法的性能甚至超越其性能的同时，减少训练时间和资源消耗。

    In recent years, deep learning has made remarkable progress in a wide range of domains, with a particularly notable impact on natural language processing tasks. One of the challenges associated with training deep neural networks is the need for large amounts of computational resources and time. In this paper, we present Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks. % We show that Deep Fusion accelerates the training process, reduces computational requirements, and leads to improved generalization performance on a variety of NLP tasks and T5 model sizes. % Our experiments demonstrate that Deep Fusion is a practical and effective approach to reduce the training time and resource consumption while maintaining, or even surpassing, the performance of traditional training methods.
    
[^256]: 渐进傅里叶神经表示用于顺序视频编译

    Progressive Fourier Neural Representation for Sequential Video Compilation. (arXiv:2306.11305v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.11305](http://arxiv.org/abs/2306.11305)

    本研究提出了一种渐进傅里叶神经表示方法，通过在每个训练会话中找到自适应且紧凑的傅里叶空间子模块来编码顺序视频数据，克服了现有神经隐式表示方法在多个复杂数据上的泛化能力差的问题。

    

    最近神经隐式表示(NIR)因其将复杂和高维数据编码为表示空间并通过可训练的映射函数轻松重构数据的非凡能力而引起了极大关注。然而，NIR方法假定目标数据和表示模型之间存在一对一的映射，而不考虑数据的相关性或相似性。这导致在多组复杂数据上泛化能力较差，并限制了其效率和可伸缩性。受持续学习的启发，本研究探讨了如何在顺序编码会话中累积和传递多个复杂视频数据的神经隐式表示。为了克服NIR的局限性，我们提出了一种新的方法，即渐进傅里叶神经表示(PFNR)，旨在找到一个自适应和紧凑的傅里叶空间子模块，以编码每个训练会话中的视频。这种稀疏的神经编码使神经网络能够持有自由权重，实现了一种可迭代地编码和解码多个顺序视频数据的方式。

    Neural Implicit Representation (NIR) has recently gained significant attention due to its remarkable ability to encode complex and high-dimensional data into representation space and easily reconstruct it through a trainable mapping function. However, NIR methods assume a one-to-one mapping between the target data and representation models regardless of data relevancy or similarity. This results in poor generalization over multiple complex data and limits their efficiency and scalability. Motivated by continual learning, this work investigates how to accumulate and transfer neural implicit representations for multiple complex video data over sequential encoding sessions. To overcome the limitation of NIR, we propose a novel method, Progressive Fourier Neural Representation (PFNR), that aims to find an adaptive and compact sub-module in Fourier space to encode videos in each training session. This sparsified neural encoding allows the neural network to hold free weights, enabling an imp
    
[^257]: 使用速率函数理解插值区间的泛化

    Understanding Generalization in the Interpolation Regime using the Rate Function. (arXiv:2306.10947v1 [cs.LG])

    [http://arxiv.org/abs/2306.10947](http://arxiv.org/abs/2306.10947)

    本文利用大偏差理论，提出一种基于函数的平滑模型特征描述方法，解释了为什么一些插值器有很好的泛化能力以及现代学习技术为什么能够找到它们。

    

    本文基于大偏差理论的基本原理，提出了一种模型平滑度的新特征描述方法。与以往的工作不同，以往的工作通常用实数值（如权重范数）来表征模型的平滑度，我们表明可以用简单的实值函数来描述平滑度。基于模型平滑度的这一概念，我们提出了一个统一的理论解释，为什么一些插值器表现出非常好的泛化能力，以及为什么广泛使用的现代学习技术（如随机梯度下降，$\ell_2$-规范化，数据增强，不变的架构和超参数化）能够找到它们。我们得出的结论是，所有这些方法都提供了互补的过程，这些过程使优化器偏向于更平滑的插值器，而根据这种理论分析，更平滑的插值器是具有更好的泛化误差的插值器。

    In this paper, we present a novel characterization of the smoothness of a model based on basic principles of Large Deviation Theory. In contrast to prior work, where the smoothness of a model is normally characterized by a real value (e.g., the weights' norm), we show that smoothness can be described by a simple real-valued function. Based on this concept of smoothness, we propose an unifying theoretical explanation of why some interpolators generalize remarkably well and why a wide range of modern learning techniques (i.e., stochastic gradient descent, $\ell_2$-norm regularization, data augmentation, invariant architectures, and overparameterization) are able to find them. The emergent conclusion is that all these methods provide complimentary procedures that bias the optimizer to smoother interpolators, which, according to this theoretical analysis, are the ones with better generalization error.
    
[^258]: ClimSim：用于在混合多尺度气候模拟器中训练高分辨率物理仿真器的开源大规模数据集

    ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators. (arXiv:2306.08754v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08754](http://arxiv.org/abs/2306.08754)

    这是一个用于训练高分辨率物理仿真器的气候数据集。该数据集包含了5.7亿个多变量输入和输出矢量对，用于隔离本地嵌套的高分辨率、高保真度物理的影响。

    

    现代气候预测由于计算限制缺乏足够的空间和时间分辨率。一个后果是对关键过程（如暴风雨）的预测不准确和不精确。将物理和机器学习（ML）相结合的混合模式引入了新一代更高保真度的气候模拟器，通过将计算密集型、短、高分辨率的模拟委托给ML仿真器，可以避免摩尔定律问题。然而，这种混合的ML-物理仿真方法需要领域特定的处理，并且由于缺乏培训数据和相关的易于使用的工作流程，一直无法访问ML专家。我们提出了 ClimSim，这是迄今为止为混合ML-物理研究而设计的最大数据集。它由气候科学家和ML研究人员联合开发的多尺度气候模拟组成，包括57亿个多变量输入和输出矢量对，隔离了本地嵌套的高分辨率和高保真度物理学的影响。

    Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise prediction of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics
    
[^259]: 用卷积神经网络解决大规模空间问题

    Solving Large-scale Spatial Problems with Convolutional Neural Networks. (arXiv:2306.08191v1 [cs.LG])

    [http://arxiv.org/abs/2306.08191](http://arxiv.org/abs/2306.08191)

    本文提出了一种采用迁移学习和小台阶信号训练卷积神经网络的方法，能够有效解决大规模空间问题，尤其在移动基础设施需求方面具有实际应用价值。

    

    在过去的十年中，深度学习研究得到了不断增强的硬件支持，这促进了模型复杂性和摄入数据量的迅速增长。然而，这种趋势正在变得不可持续，因此，转向提高效率是必要的。在本文中，我们采用迁移学习提高大规模空间问题的训练效率。我们提出，卷积神经网络（CNN）可以在小台阶信号上进行训练，但在评估任意大小信号时几乎不会发生性能下降，并提供了所得到的泛化误差的理论界限。我们的证明利用了CNN的平移不变性，这是迁移学习中未被充分利用的一个属性。在基于移动基础架构的需求（MID）的情况下，理论结果在实验中得到了支持。本文所提出的方法能够处理大规模MID，其中包括数百个代理，这在此前的工作中是计算上难以实现的。

    Over the past decade, deep learning research has been accelerated by increasingly powerful hardware, which facilitated rapid growth in the model complexity and the amount of data ingested. This is becoming unsustainable and therefore refocusing on efficiency is necessary. In this paper, we employ transfer learning to improve training efficiency for large-scale spatial problems. We propose that a convolutional neural network (CNN) can be trained on small windows of signals, but evaluated on arbitrarily large signals with little to no performance degradation, and provide a theoretical bound on the resulting generalization error. Our proof leverages shift-equivariance of CNNs, a property that is underexploited in transfer learning. The theoretical results are experimentally supported in the context of mobile infrastructure on demand (MID). The proposed approach is able to tackle MID at large scales with hundreds of agents, which was computationally intractable prior to this work.
    
[^260]: 高维和置换不变异常检测。

    High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])

    [http://arxiv.org/abs/2306.03933](http://arxiv.org/abs/2306.03933)

    该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。

    

    由于学习高维概率密度的困难，新物理过程的异常检测方法通常局限于低维空间。特别是在成分级别上，将置换不变性和可变长度的输入等良好性质合并到流行的密度估计方法中变得更加困难。在本研究中，我们引入了一种基于扩散模型的粒子物理数据置换不变密度估计器，专门设计用于处理可变长度的输入。我们通过将学习到的密度用作置换不变的异常检测评分来展示我们方法的功效，有效地识别出在仅具备背景假设下的可能性较低的喷注。为了验证我们的密度估计方法，我们研究了学习到的密度比与被监督分类算法获得的密度之间的比较。

    Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
    
[^261]: 签名条码作为度量的多参数持久同调的稳定向量化

    Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures. (arXiv:2306.03801v1 [cs.LG])

    [http://arxiv.org/abs/2306.03801](http://arxiv.org/abs/2306.03801)

    本研究提出了使用签名条码来稳定向量化多参数持久同调，将多参数持久同调的丰富信息和稳定向量化的优势相结合。

    

    持久同调（PH）提供了几何数据（例如加权图）的拓扑描述符，它们是可解释的，对扰动稳定，并具有诸如重标记等不变性。大多数PH应用关注一参数情况——描述符总结数据的拓扑随着单个感兴趣因素的滤波而发生变化；现在，有各种方法使得一参数PH描述符在数据科学中得到应用，并且依赖于将这些描述符稳定向量化为希尔伯特空间的元素。虽然由几个感兴趣因素过滤的数据的多参数PH（MPH）编码比其一参数同型的信息更丰富，但迄今为止，MPH描述符的稳定性结果的稀缺性已经限制了MPH的稳定向量化的可用选项。在本文中，我们旨在通过展示如何解释签名条码来集结两方面的优点。

    Persistent homology (PH) provides topological descriptors for geometric data, such as weighted graphs, which are interpretable, stable to perturbations, and invariant under, e.g., relabeling. Most applications of PH focus on the one-parameter case -- where the descriptors summarize the changes in topology of data as it is filtered by a single quantity of interest -- and there is now a wide array of methods enabling the use of one-parameter PH descriptors in data science, which rely on the stable vectorization of these descriptors as elements of a Hilbert space. Although the multiparameter PH (MPH) of data that is filtered by several quantities of interest encodes much richer information than its one-parameter counterpart, the scarceness of stability results for MPH descriptors has so far limited the available options for the stable vectorization of MPH. In this paper, we aim to bring together the best of both worlds by showing how the interpretation of signed barcodes -- a recent famil
    
[^262]: PAGAR: 用主角-反派引导的对抗性奖励驯服逆强化学习在基于模仿学习中的奖励不对齐问题

    PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward. (arXiv:2306.01731v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.01731](http://arxiv.org/abs/2306.01731)

    PAGAR是一种用于解决IRL-based IL中奖励不对齐问题的半监督奖励设计方法，在复杂IL任务和零-shot IL任务中表现优越。

    

    许多模仿学习(imitation learning, IL)算法使用逆强化学习(inverse reinforcement learning, IRL)来推断专家以隐式方式优化的潜在奖励函数，基于其展示的行为。然而，推断的奖励与真实任务目标之间的不对齐可能导致任务失败。在本文中，我们引入了主角-反派引导的对抗性奖励(PAGAR)，这是一种半监督奖励设计范式，用于解决IRL-based IL中的奖励不对齐问题。我们确定了候选奖励函数满足的条件，PAGAR能够保证产生一个在底层任务中成功的策略。此外，我们提出了一种实用的在策略和离策略方法来在IRL-based IL中实施PAGAR。实验结果表明，我们的算法在复杂的IL任务和有限演示的迁移环境的零-shot IL任务上优于竞争的基线模型。

    Many imitation learning (IL) algorithms employ inverse reinforcement learning (IRL) to infer the underlying reward function that an expert is implicitly optimizing for, based on their demonstrated behaviors. However, a misalignment between the inferred reward and the true task objective can result in task failures. In this paper, we introduce Protagonist Antagonist Guided Adversarial Reward (PAGAR), a semi-supervised reward design paradigm to tackle this reward misalignment problem in IRL-based IL. We identify the conditions on the candidate reward functions under which PAGAR can guarantee to induce a policy that succeeds in the underlying task. Furthermore, we present a practical on-and-off policy approach to implement PAGAR in IRL-based IL. Experimental results show that our algorithm outperforms competitive baselines on complex IL tasks and zero-shot IL tasks in transfer environments with limited demonstrations.
    
[^263]: 离线赌博中贝叶斯遗憾最小化的凸松弛方法

    A Convex Relaxation Approach to Bayesian Regret Minimization in Offline Bandits. (arXiv:2306.01237v1 [cs.LG])

    [http://arxiv.org/abs/2306.01237](http://arxiv.org/abs/2306.01237)

    本文提出一种直接最小化贝叶斯遗憾上界的新方法，获得更好的理论离线遗憾界和数值模拟结果，并提供了证据表明流行的LCB-style算法可能不适用。

    

    离线赌博算法必须仅利用离线数据在不确定环境中优化决策。离线赌博中一种引人注目且逐渐流行的目标是学习一个实现低贝叶斯遗憾并具有高置信度的策略。本文提出了一种新的方法，直接利用高效的锥优化求解器来最小化贝叶斯遗憾的上界。与之前的工作相比，我们的算法在理论上获得了更优的离线遗憾界，并在数值模拟中取得了更好的结果。最后，我们提供一些证据表明流行的LCB（lower confidence bound）-style算法可能不适合离线赌博中最小化贝叶斯遗憾。

    Algorithms for offline bandits must optimize decisions in uncertain environments using only offline data. A compelling and increasingly popular objective in offline bandits is to learn a policy which achieves low Bayesian regret with high confidence. An appealing approach to this problem, inspired by recent offline reinforcement learning results, is to maximize a form of lower confidence bound (LCB). This paper proposes a new approach that directly minimizes upper bounds on Bayesian regret using efficient conic optimization solvers. Our bounds build on connections among Bayesian regret, Value-at-Risk (VaR), and chance-constrained optimization. Compared to prior work, our algorithm attains superior theoretical offline regret bounds and better results in numerical simulations. Finally, we provide some evidence that popular LCB-style algorithms may be unsuitable for minimizing Bayesian regret in offline bandits.
    
[^264]: MERT:带有大规模自监督训练的声学音乐理解模型

    MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])

    [http://arxiv.org/abs/2306.00107](http://arxiv.org/abs/2306.00107)

    提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。

    

    自监督学习（SSL）最近在视觉、文本和语音领域中已被证明是训练通用模型的一种很有前景的范例，对于跨越音乐领域的应用，尤其是对于调性和音高这样的特殊音乐知识的建模颇具挑战性。为了解决这一问题，我们提出了一个基于大规模自监督训练的声学音乐理解模型，即MERT。在我们的探索中，我们确定了更优秀的教师模型组合，这种组合方法在性能方面优于传统的语音和音频方法。

    Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
    
[^265]: 略微超参数化的ReLU网络具有有利的损失景观

    Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape. (arXiv:2305.19510v1 [cs.LG])

    [http://arxiv.org/abs/2305.19510](http://arxiv.org/abs/2305.19510)

    本文研究了略微超参数化的ReLU网络在有限输入数据集上的损失景观，证明了大多数激活模式对应的参数区域没有坏的可微局部极小值，对于一维输入数据，网络可以通过大多数激活模式实现高维全局极小值集合而不具有坏的局部极小值。

    

    本文研究了有限输入数据集上，二层略微超参数化ReLU神经网络的损失景观，使用了参数映射的Jacobian矩阵的秩来估计局部和全局极小值集的维度。使用随机二进制矩阵的结果，我们证明大多数激活模式对应的参数区域没有坏的可微局部极小值。此外，对于一维输入数据，我们证明了网络可以通过大多数的激活模式实现高维全局极小值集合而不具有坏的局部极小值。我们通过发现大多数区域具有完整的秩或缺乏秩，以实验的方式证实了这些结果，这取决于超参数的数量。

    We study the loss landscape of two-layer mildly overparameterized ReLU neural networks on a generic finite input dataset for the squared error loss. Our approach involves bounding the dimension of the sets of local and global minima using the rank of the Jacobian of the parameterization map. Using results on random binary matrices, we show most activation patterns correspond to parameter regions with no bad differentiable local minima. Furthermore, for one-dimensional input data, we show most activation regions realizable by the network contain a high dimensional set of global minima and no bad local minima. We experimentally confirm these results by finding a phase transition from most regions having full rank to many regions having deficient rank depending on the amount of overparameterization.
    
[^266]: 深度等变超球体

    Deep Equivariant Hyperspheres. (arXiv:2305.15613v1 [cs.LG])

    [http://arxiv.org/abs/2305.15613](http://arxiv.org/abs/2305.15613)

    本文提出了深度等变超球体的理论模型，解决了几何深度学习中等变和几何变换下不变的重大问题。

    

    本文提出了一种学习nD特征的方法，其在点云分析中等变于正交转换，利用了超球体和常规n单形体。我们的主要贡献在于理论方面，解决了几何深度学习中等变和几何变换下不变的重大问题。我们扩展了近期发展的可操纵3D球形神经元理论--基于球形决策面的SO（3）-等变滤波器组，将该神经元扩展到了nD，我们称之为深度等变超球体，并使它们能够堆叠在多层中。利用ModelNet40基准测试，我们实验验证了我们的理论贡献，并展示了所提出的等变超球体的潜在实用配置。

    This paper presents an approach to learning nD features equivariant under orthogonal transformations for point cloud analysis, utilizing hyperspheres and regular n-simplexes. Our main contributions are theoretical and tackle major issues in geometric deep learning such as equivariance and invariance under geometric transformations. Namely, we enrich the recently developed theory of steerable 3D spherical neurons -- SO(3)-equivariant filter banks based on neurons with spherical decision surfaces -- by extending said neurons to nD, which we call deep equivariant hyperspheres, and enabling their stacking in multiple layers. Using the ModelNet40 benchmark, we experimentally verify our theoretical contributions and show a potential practical configuration of the proposed equivariant hyperspheres.
    
[^267]: 基于谱角度剖析生物数据中图神经网络的尺寸可泛化性：观点和实践

    Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])

    [http://arxiv.org/abs/2305.15611](http://arxiv.org/abs/2305.15611)

    本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。

    

    本文探讨了图神经网络 (GNNs) 是否具有从小图中学习的知识可推广到同一领域的大图中。之前的研究表明，不同大小的图之间的分布偏移，尤其是度分布，可能会导致图分类任务的性能下降。然而，在生物数据集中，度数是有界的，因此度分布的偏移很小。即使度分布偏移很小，我们观察到GNNs在同一数据集的大图上的性能仍然下降，暗示有其他原因。事实上，以往对于真实数据集中各种图尺寸引起的分布偏移类型和属性的探索不足。此外，以前的尺寸可泛化性分析大多集中在空间领域。为填补这些空白，我们采用谱角度去研究GNNs在生物图数据上的尺寸可泛化性。我们首先提出一个新框架来模拟各种类型的度分布偏移，并利用它来测试GNNs 在真实生物数据集上的尺寸可泛化性。我们的实验表明，除了度分布偏移外，GNNs 还对图大小变化引起的谱分布偏移很敏感。我们进一步分析了不同的GNN模型的影响，并表明，一些模型比其他模型更具有尺寸泛化性。本文展示了关于GNNs尺寸可泛化性问题的新观点和实践，并为该领域的未来研究提供了有益的洞察和建议。

    We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
    
[^268]: 基于核凸包机的差分隐私学习研究

    Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])

    [http://arxiv.org/abs/2304.01300](http://arxiv.org/abs/2304.01300)

    本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。

    

    本文探讨了通过学习再生核希尔伯特空间中的点的凸包来表示数据的方法，旨在将数据空间划分为几何体，从而隐藏有关单个数据点的隐私信息，同时保留原始学习问题的结构。为此，我们引入了核凸包机（KAHM），它提供了一种有效的方法来计算从结果有界几何体中的距离度量。KAHM是广泛和深入的自编码器的关键构建块，它们使数据表示学习用于分类应用。为了确保隐私保护学习，我们提出了一种新颖的生成虚假数据的方法，该方法涉及将差分隐私数据样本通过转换过程进行平滑处理。生成的虚假数据不仅保证差分隐私，而且确保KAHM建模误差不大于原始数据误差。

    This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
    
[^269]: 实用的自监督连续学习方法：连续微调

    Practical self-supervised continual learning with continual fine-tuning. (arXiv:2303.17235v1 [cs.LG])

    [http://arxiv.org/abs/2303.17235](http://arxiv.org/abs/2303.17235)

    本文提出了一种实用的自监督连续学习方法，使用可用的标签来泛化自监督学习，并通过连续微调来减轻灾难性遗忘。

    

    自监督学习在计算机视觉任务中表现出了出色的性能。而在连续学习情景中，模型仍然存在灾难性遗忘的问题。本文提出了一种能够使用任何步骤中的可用标签的方法，该方法能够在实际情况下泛化自监督连续学习，并且能够提供更多的灵活性。

    Self-supervised learning (SSL) has shown remarkable performance in computer vision tasks when trained offline. However, in a Continual Learning (CL) scenario where new data is introduced progressively, models still suffer from catastrophic forgetting. Retraining a model from scratch to adapt to newly generated data is time-consuming and inefficient. Previous approaches suggested re-purposing self-supervised objectives with knowledge distillation to mitigate forgetting across tasks, assuming that labels from all tasks are available during fine-tuning. In this paper, we generalize self-supervised continual learning in a practical setting where available labels can be leveraged in any step of the SSL process. With an increasing number of continual tasks, this offers more flexibility in the pre-training and fine-tuning phases. With Kaizen, we introduce a training architecture that is able to mitigate catastrophic forgetting for both the feature extractor and classifier with a carefully des
    
[^270]: 混合自编码器用于自监督视觉表示学习

    Mixed Autoencoder for Self-supervised Visual Representation Learning. (arXiv:2303.17152v1 [cs.CV])

    [http://arxiv.org/abs/2303.17152](http://arxiv.org/abs/2303.17152)

    本文提出混合自编码器（MixedAE）用于自监督视觉表示学习，在MAE构架下通过同源识别等辅助预文本任务，解决了数据增强下相互信息增加导致性能下降的问题，并取得了遮蔽图像建模（MIM）增强中最先进的转移结果。

    

    掩码自编码器（MAE）通过随机遮盖图像补丁和重建在各种视觉任务上展现出卓越的性能。然而，MAE的有效数据增强策略仍然是未解决的问题，不同于对比学习中的策略。本文研究了用于MAE的普遍混合增强。我们首先证明了朴素混合将由于相互信息的增加而降低模型性能。为了解决这个问题，我们提出了同源识别方法，一种辅助的预文本任务，不仅通过明确要求每个补丁识别同源补丁来缓解相互信息的增加，而且还可以执行面向对象的自监督预训练以获得更好的下游密集感知性能。通过大量的实验证明，我们的混合自编码器（MixedAE）在不同的遮蔽图像建模（MIM）增强中实现了最先进的转移结果。

    Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on differen
    
[^271]: 计算机视觉中的双曲几何：卷积神经网络的新框架

    Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks. (arXiv:2303.15919v1 [cs.CV])

    [http://arxiv.org/abs/2303.15919](http://arxiv.org/abs/2303.15919)

    本文提出了HCNN，是第一个专为计算机视觉任务设计的完全双曲卷积神经网络。在洛伦兹模型的基础上，我们推广了CNN的基本组件，并通过在完全双曲设置中的实验证明了HCNN框架和洛伦兹模型的有效性。

    

    真实世界的视觉数据呈现出固有的分层结构，这些结构可以在双曲空间中有效地表示。双曲神经网络是在这种空间中学习特征表示的一种有前途的方法。然而，计算机视觉中的当前方法依赖于欧几里得骨干，并且仅在任务头中将特征投影到双曲空间中，限制了它们充分利用双曲几何的好处的能力。为了解决这个问题，我们提出了HCNN，这是第一个专为计算机视觉任务设计的完全双曲卷积神经网络（CNN）。基于洛伦兹模型，我们推广了CNN的基本组件，并提出了卷积层、批量归一化和多项式逻辑回归（MLR）的新公式。在标准视觉任务的实验中，我们展示了我们的HCNN框架和洛伦兹模型在混合和完全双曲设置中的有效性。总的来说，我们旨在为将来在双曲几何中进行的研究铺平道路。

    Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current methods in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. To address this, we present HCNN, the first fully hyperbolic convolutional neural network (CNN) designed for computer vision tasks. Based on the Lorentz model, we generalize fundamental components of CNNs and propose novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression (MLR). Experimentation on standard vision tasks demonstrates the effectiveness of our HCNN framework and the Lorentz model in both hybrid and fully hyperbolic settings. Overall, we aim to pave the way for future research in h
    
[^272]: 多元概率CRPS学习及其在日前电价预测中的应用

    Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices. (arXiv:2303.10019v1 [stat.ML])

    [http://arxiv.org/abs/2303.10019](http://arxiv.org/abs/2303.10019)

    本文提出一种新的多元概率CRPS学习方法，应用于日前电价预测中，相比于统一组合在CRPS方面取得了显著改进。

    

    本文提出了一种考虑分位数和协变量依赖关系的多元概率预测的结合方法，并通过平滑过程允许在线学习。通过维数降低和罚函数平滑等两种平滑方法来将标准CRPS学习框架推广到多元维度中。将该方法应用于预测日前电价，相比于统一组合，在CRPS方面取得了显著改进。

    This paper presents a new method for combining (or aggregating or ensembling) multivariate probabilistic forecasts, taking into account dependencies between quantiles and covariates through a smoothing procedure that allows for online learning. Two smoothing methods are discussed: dimensionality reduction using Basis matrices and penalized smoothing. The new online learning algorithm generalizes the standard CRPS learning framework into multivariate dimensions. It is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic learning properties. We provide an in-depth discussion on possible extensions of the algorithm and several nested cases related to the existing literature on online forecast combination. The methodology is applied to forecasting day-ahead electricity prices, which are 24-dimensional distributional forecasts. The proposed method yields significant improvements over uniform combination in terms of continuous ranked probability score (CRPS). We discuss 
    
[^273]: 基于分数的条件模型的概念代数

    Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03693](http://arxiv.org/abs/2302.03693)

    本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。

    

    本文研究了文本引导生成模型中学习表示的结构，重点关注基于分数的模型。我们聚焦于概念被编码为某种表示空间的子空间（或方向）的思想，并开发了这个思想的数学形式化。利用这个形式化方法，我们展示了有一个自然的表示选择具有这种性质，并且我们开发了一种简单的方法来识别与给定概念对应的表示部分。特别是，这使我们能够通过对表示的代数操作来操纵模型所表达的概念。我们使用稳定扩散在文本引导图像生成的示例中演示了这个思想。

    This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
    
[^274]: 重新审视有符号传播在图神经网络中的应用

    Revisiting Signed Propagation for Graph Neural Networks. (arXiv:2301.08918v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08918](http://arxiv.org/abs/2301.08918)

    该论文重新审视了在异质图中的有符号传播方法，提出了一种适用于多类别图的新策略，并克服了其带来的不确定性和不一致性问题，取得了显著的性能提升。

    

    信息传递式图神经网络（GNN）在同质图上表现出色。然而，在异质图上它们的性能却很差，许多研究者为解决这个问题提出了大量方案。特别地，翻转边的符号是基于坚实理论基础的并且可以获得显著的性能提升。然而，以前的分析假定了二元分类场景，因此受到应用范围的限制。本文将以前的理解扩展到多类别情况，并指出两个缺点：（1）多跳邻居的符号取决于消息传递路径，可能导致不一致性；（2）这也增加了预测的不确定性（例如，冲突证据），可能影响算法的稳定性。在理论的基础上，我们提出了一种新的策略，适用于多类别的图。所提出的方案结合了原有方案的优点，同时克服了其缺点并取得了显著的性能提升。

    Message-passing Graph Neural Networks (GNNs), which collect information from adjacent nodes, achieve satisfying results on homophilic graphs. However, their performances are dismal in heterophilous graphs, and many researchers have proposed a plethora of schemes to solve this problem. Especially, flipping the sign of edges is rooted in a strong theoretical foundation, and attains significant performance enhancements. Nonetheless, previous analyses assume a binary class scenario and they may suffer from confined applicability. This paper extends the prior understandings to multi-class scenarios and points out two drawbacks: (1) the sign of multi-hop neighbors depends on the message propagation paths and may incur inconsistency, (2) it also increases the prediction uncertainty (e.g., conflict evidence) which can impede the stability of the algorithm. Based on the theoretical understanding, we introduce a novel strategy that is applicable to multi-class graphs. The proposed scheme combine
    
[^275]: 关于递归划分的逐点行为及其对异质因果效应估计的影响

    On the Pointwise Behavior of Recursive Partitioning and Its Implications for Heterogeneous Causal Effect Estimation. (arXiv:2211.10805v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.10805](http://arxiv.org/abs/2211.10805)

    本文质疑了递归划分在决策树学习中的应用，通过证明它们可能无法实现一致范数的多项式收敛速率。我们提出了随机森林来解决这个问题，将低性能的树转化为几乎最优的过程，但代价是失去了解释性，并引入了两个额外的调整参数。

    

    决策树学习在逐点推断中的应用日益增多。重要的应用包括异质因果治疗效应和动态政策决策，以及条件分位数回归和实验设计，在这些应用中，树的估计和推断是在特定的协变量值上进行的。在本文中，我们对使用决策树（通过自适应递归划分训练）进行此类目的提出了质疑，通过证明它们甚至可以在修剪的情况下无法实现一致范数的多项式收敛速率。相反，收敛速度可能是多项式对数级别的，或者在一些重要的特殊情况下，例如诚实回归树，完全失败。我们表明，随机森林可以解决这个问题，将低性能的树转化为几乎最优的过程，但代价是失去了解释性，并引入了两个额外的调整参数。随机森林的两个标志性特征是子采样和随机特征选择机制。

    Decision tree learning is increasingly being used for pointwise inference. Important applications include causal heterogenous treatment effects and dynamic policy decisions, as well as conditional quantile regression and design of experiments, where tree estimation and inference is conducted at specific values of the covariates. In this paper, we call into question the use of decision trees (trained by adaptive recursive partitioning) for such purposes by demonstrating that they can fail to achieve polynomial rates of convergence in uniform norm, even with pruning. Instead, the convergence may be poly-logarithmic or, in some important special cases, such as honest regression trees, fail completely. We show that random forests can remedy the situation, turning poor performing trees into nearly optimal procedures, at the cost of losing interpretability and introducing two additional tuning parameters. The two hallmarks of random forests, subsampling and the random feature selection mecha
    
[^276]: RenderDiffusion: 用于3D重建、修复和生成的图像扩散方法

    RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation. (arXiv:2211.09869v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.09869](http://arxiv.org/abs/2211.09869)

    本文提出了第一个支持3D理解任务的扩散模型RenderDiffusion，只需使用单眼2D监督进行训练。它利用一种新颖的图像去噪架构，生成和渲染中间的三维表示，在扩散过程中提供强有力的3D一致性。

    

    目前，扩散模型在有条件和无条件的图像生成方面均达到了最先进的水平。但是，目前的图像扩散模型不支持用于3D理解所需的任务，例如视角一致的3D生成或单视角物体重建。本文提出了RenderDiffusion，这是第一个用于3D生成和推断的扩散模型，只需使用单眼2D监督进行训练。方法的核心是一种新颖的图像去噪架构，在每个去噪步骤中生成和渲染场景的中间三维表示。这在扩散过程中强制实现了一个强的归纳结构，提供了3D一致的表示，同时只需要2D监督。生成的3D表示可以从任何视角渲染。我们评估了RenderDiffusion在FFHQ、AFHQ、ShapeNet和CLEVR数据集上的性能，显示出了在生成3D场景和从2D图像推断3D场景方面具有竞争力的表现。

    Diffusion models currently achieve state-of-the-art performance for both conditional and unconditional image generation. However, so far, image diffusion models do not support tasks required for 3D understanding, such as view-consistent 3D generation or single-view object reconstruction. In this paper, we present RenderDiffusion, the first diffusion model for 3D generation and inference, trained using only monocular 2D supervision. Central to our method is a novel image denoising architecture that generates and renders an intermediate three-dimensional representation of a scene in each denoising step. This enforces a strong inductive structure within the diffusion process, providing a 3D consistent representation while only requiring 2D supervision. The resulting 3D representation can be rendered from any view. We evaluate RenderDiffusion on FFHQ, AFHQ, ShapeNet and CLEVR datasets, showing competitive performance for generation of 3D scenes and inference of 3D scenes from 2D images. Ad
    
[^277]: 连续蒙特卡洛图搜索

    Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.01426](http://arxiv.org/abs/2210.01426)

    连续蒙特卡洛图搜索（CMCGS）是一种新颖的蒙特卡洛树搜索（MCTS）的扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS通过将相似状态聚类，并共享相同的动作策略，实现了高性能的在线规划。

    

    在许多复杂的连续决策任务中，在线规划对于高性能至关重要。为了实现高效的在线规划，蒙特卡洛树搜索（MCTS）采用了一个有原则的机制来权衡探索和利用。MCTS在许多离散决策领域（如围棋、国际象棋和将棋）中胜过了其他方法。而针对连续领域的MCTS扩展也已提出。然而，由于固有的高分支因子和导致搜索树大小爆炸的问题，现有方法受到了限制。为了解决这个问题，我们提出了连续蒙特卡洛图搜索（CMCGS），这是一种新颖的MCTS扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS利用了一个洞察力，在规划过程中，将相似状态之间共享相同的动作策略可以得到高性能。为了实现这个想法，CMCGS在每个时间步骤中将相似状态聚类成有限数量的随机动作赌博节点，这些节点共享相同的动作策略。

    In many complex sequential decision-making tasks, online planning is crucial for high performance. For efficient online planning, Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off exploration for exploitation. MCTS outperforms comparison methods in many discrete decision-making domains such as Go, Chess, and Shogi. Following, extensions of MCTS to continuous domains have been proposed. However, the inherent high branching factor and the resulting explosion of search tree size are limiting existing methods. To address this problem, we propose Continuous Monte Carlo Graph Search (CMCGS), a novel extension of MCTS to online planning in environments with continuous state and action spaces. CMCGS takes advantage of the insight that, during planning, sharing the same action policy between several states can yield high performance. To implement this idea, at each time step, CMCGS clusters similar states into a limited number of stochastic action bandit nodes, which
    

