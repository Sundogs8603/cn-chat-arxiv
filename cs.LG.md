# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [AMC-Net: An Effective Network for Automatic Modulation Classification.](http://arxiv.org/abs/2304.00445) | AMC-Net是一种用于改善自动调制分类效率和有效性的新型网络，通过在频域中进行降噪和执行多尺度特征提取来提高识别性能，在两个代表性数据集上实验表现更优。 |
| [^2] | [SoftED: Metrics for Soft Evaluation of Time Series Event Detection.](http://arxiv.org/abs/2304.00439) | SoftED metrics 是一种适用于时间序列事件检测的新指标，既包括时间的概念，又包括对相邻检测的时间容忍度，它们能够同时评估事件检测的准确性和其检测是否代表事件。 |
| [^3] | [Ideal Observer Computation by Use of Markov-Chain Monte Carlo with Generative Adversarial Networks.](http://arxiv.org/abs/2304.00433) | 本文提出了使用生成对抗网络和马尔可夫链蒙特卡罗方法计算理想观察者的方法，以扩展IO方法适用范围。 |
| [^4] | [Geometric constraints improve inference of sparsely observed stochastic dynamics.](http://arxiv.org/abs/2304.00423) | 本文提出一种新的方法，该方法利用数据驱动的控制，既考虑了系统不变密度的几何形状，又能对系统进行有效识别。 |
| [^5] | [Experimentation Platforms Meet Reinforcement Learning: Bayesian Sequential Decision-Making for Continuous Monitoring.](http://arxiv.org/abs/2304.00420) | 本文介绍了一个新的框架，将实验监测问题制定为具有统一效用函数的贝叶斯序贯决策问题，并采用强化学习来解决最优决策规则。这一方法能够有效提高客户体验并控制机会成本。 |
| [^6] | [Mini-batch $k$-means terminates within $O(d/\epsilon)$ iterations.](http://arxiv.org/abs/2304.00419) | 小批量$k$-means聚类法被证明可以在一定迭代次数内收敛，无论初始聚类中心如何，可以达到与完全批量版本相同的逼近比，具有一定的可行性。 |
| [^7] | [Towards Healthy AI: Large Language Models Need Therapists Too.](http://arxiv.org/abs/2304.00416) | SafeguardGPT框架使用心理治疗来纠正AI聊天机器人中的有害行为，改进与人类的对话质量，进而推进健康AI的发展。 |
| [^8] | [DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection.](http://arxiv.org/abs/2304.00409) | 这个论文提供了一个包含150个CWE的新的漏洞源代码数据集，覆盖比以前所有数据集加起来多305个项目。作者通过增加训练数据的多样性和数量改进了深度学习模型在漏洞检测方面的表现。通过结合已有数据集，作者还研究了使用深度学习检测软件漏洞的挑战和前途的分析，结果表明目前深度学习检测漏洞仍存在高误报率，低F1分数和难以检测严重CWE的问题。 |
| [^9] | [Connected and Automated Vehicles in Mixed-Traffic: Learning Human Driver Behavior for Effective On-Ramp Merging.](http://arxiv.org/abs/2304.00397) | 本文介绍了一种学习人类驾驶员行为以实现出入匝道的有效合并的方法，通过学习CAV和HDV互动的近似信息状态模型，使得在混合交通条件下实现安全且高效的合并。 |
| [^10] | [Managing Cold-start in The Serverless Cloud with Temporal Convolutional Networks.](http://arxiv.org/abs/2304.00396) | 无服务器云的创新模式提供了许多优势，但面临着冷启动问题，本文通过使用时间卷积网络提出的策略可以在基础结构和功能层面上解决此问题。 |
| [^11] | [Towards Understanding the Mechanism of Contrastive Learning via Similarity Structure: A Theoretical Analysis.](http://arxiv.org/abs/2304.00395) | 通过一个新的公式，本文理论分析了基于核的对比学习损失的特点，证明了它能描述学习表示的结构和表现，提供一个新的限制方法，并在多个基准测试中验证其有效性。 |
| [^12] | [Multilevel CNNs for Parametric PDEs.](http://arxiv.org/abs/2304.00388) | 该论文提出了一种用于有效数值解决参数化PDEs的多级CNN方法，有实质性的改进并能以任意精度近似多重网格V循环。 |
| [^13] | [Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT.](http://arxiv.org/abs/2304.00385) | 提出了一个名为ChatRepair的新型自动程序修复方法，与传统的“生成和验证”范式不同，它能够通过对话风格实现即时反馈，从而显着提高漏洞修复的效率和补丁的准确性。 |
| [^14] | [Knowledge Graph Embedding with 3D Compound Geometric Transformations.](http://arxiv.org/abs/2304.00378) | 本文提出了一种基于3D复合几何变换的知识图谱嵌入模型CompoundE3D，在链接预测方面具有良好的性能表现。 |
| [^15] | [A Survey on Personalized Affective Computing in Human-Machine Interaction.](http://arxiv.org/abs/2304.00377) | 论文调查了情感计算中的个性化方法，将其分为七类，并给出了调查文献的统计元分析。 |
| [^16] | [Physics-informed machine learning for moving load problems.](http://arxiv.org/abs/2304.00369) | 论文提出了使用基于物理知识的机器学习方法模拟移动荷载问题，其中使用高斯函数来逼近Dirac delta函数以提高模型收敛性。 |
| [^17] | [Adaptive Failure Search Using Critical States from Domain Experts.](http://arxiv.org/abs/2304.00365) | 本文提出了一种基于领域专家的自适应失效搜索方法，用于有效地探索和发现自主策略在仿真中的失效轨迹。 |
| [^18] | [On Context Distribution Shift in Task Representation Learning for Offline Meta RL.](http://arxiv.org/abs/2304.00354) | 该论文探讨了离线元强化学习中任务表示学习中遇到的上下文分布偏移问题，并提出了一种硬采样的策略用于解决该问题，实验结果表明该方法能够得到更强健的任务表示和更好的测试性能。 |
| [^19] | [Scientific Computing Algorithms to Learn Enhanced Scalable Surrogates for Mesh Physics.](http://arxiv.org/abs/2304.00338) | 通过领域分解的方法扩展网格上的MeshGraphNets，了解了如何训练一个可扩展的基于网格的图神经网络代理，进而生成计算流体动力学模拟，同时还展示了如何通过高阶数值积分来增强该代理。 |
| [^20] | [Doubly Stochastic Models: Learning with Unbiased Label Noises and Inference Stability.](http://arxiv.org/abs/2304.00320) | 本文探讨了无偏标签噪声的隐含正则化效应，提出了一种将SGD的动态建模为双重随机模型的方法，可提高鲁棒性和泛化性能。 |
| [^21] | [Medical Pathologies Prediction : Systematic Review and Proposed Approach.](http://arxiv.org/abs/2304.00311) | 该论文分析了利用最新技术改善医疗保健，提出了一般方法，通过收集、预处理和聚类医学数据，能够让患者和医疗专业人员在显着的时间范围内更精准地预测最常见的病理情况。 |
| [^22] | [Predictive Heterogeneity: Measures and Applications.](http://arxiv.org/abs/2304.00305) | 本文研究了数据的异质性对机器学习模型预测的影响，并提出了可用的预测不确定性，可从有限数据中可靠地估计。我们设计了一个双层优化算法来探索可用的预测不确定性，从而为子人群划分提供洞见。 |
| [^23] | [Fair-CDA: Continuous and Directional Augmentation for Group Fairness.](http://arxiv.org/abs/2304.00295) | 本文提出了一种公平连续和定向增强（Fair-CDA）的细致的数据增强策略，通过正则化模型展示了可以实现组公平性，并通过调整路径方向上的扰动强度实现了可控和可审计的增强方法。实验结果表明，Fair-CDA在广泛使用的基准测试中始终优于最先进的方法，如Adult、CelebA和MovieLens。 |
| [^24] | [BioSequence2Vec: Efficient Embedding Generation For Biological Sequences.](http://arxiv.org/abs/2304.00291) | BioSequence2Vec是一种通用的、高效的生物序列嵌入生成方法，能够有效处理计算时间和存储大量核矩阵的问题。 |
| [^25] | [Branch Identification in Passive Optical Networks using Machine Learning.](http://arxiv.org/abs/2304.00285) | 本文提出利用机器学习改善被动式光纤网络监测，具有高的诊断准确率和定位精度。 |
| [^26] | [Data Privacy Preservation on the Internet of Things.](http://arxiv.org/abs/2304.00258) | 物联网的普及带来大量数据，对用户的隐私安全构成威胁，本文综述了物联网领域数据隐私保护的现有方案。 |
| [^27] | [Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning.](http://arxiv.org/abs/2304.00252) | 本文提出了恢复触发状态(RTS)方法，用于保护RL代理免受反向攻击。该方法涉及构建替代网络来近似动态模型，并将触发状态恢复为干净状态来防止攻击者通过触发器激活隐藏在代理中的后门。 |
| [^28] | [From Conception to Deployment: Intelligent Stroke Prediction Framework using Machine Learning and Performance Evaluation.](http://arxiv.org/abs/2304.00249) | 本文提出了一个基于机器学习和性能评估的智能卒中预测框架，并比较了五种常用的机器学习算法，结果显示随机森林算法最适合卒中预测。 |
| [^29] | [Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes.](http://arxiv.org/abs/2304.00232) | 该论文介绍了一种针对非平稳强化学习环境的算法，称为R-BOCPD-UCRL2，它使用了一种重新启动的贝叶斯在线变点检测算法（R-BOCPD），并在多项式分布采样的MDP上提供了较优秀的性能保证。 |
| [^30] | [ConvBLS: An Effective and Efficient Incremental Convolutional Broad Learning System for Image Classification.](http://arxiv.org/abs/2304.00219) | ConvBLS是一种卷积广义学习系统，使用球形K-means算法、卷积增强层和多尺度特征融合层可以达到与先进深度学习模型相当甚至更好的性能，在需要更少时间和计算资源的情况下完成训练和预测。 |
| [^31] | [Cross-scale Multi-instance Learning for Pathological Image Diagnosis.](http://arxiv.org/abs/2304.00216) | 本研究提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中，有效地解决了忽略对人类病理学家诊断至关重要的跨尺度信息的问题。 |
| [^32] | [Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers.](http://arxiv.org/abs/2304.00215) | 本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。 |
| [^33] | [Devil is in the Queries: Advancing Mask Transformers for Real-world Medical Image Segmentation and Out-of-Distribution Localization.](http://arxiv.org/abs/2304.00212) | 本文采用Mask Transformers技术和目标查询的概念，以实现医学图像分割中超出分布的定位，从而提高医疗AI算法的可靠性和安全性。 |
| [^34] | [Improving Fast Adversarial Training with Prior-Guided Knowledge.](http://arxiv.org/abs/2304.00202) | 本文提出了一种使用先前训练过程中高质量对抗扰动的正面先验引导对抗初始化方法，以提高对抗样本的质量，从而避免快速对抗训练中的灾难性过度拟合问题。 |
| [^35] | [Diffusion map particle systems for generative modeling.](http://arxiv.org/abs/2304.00200) | 本文提出一种新型扩散映射粒子系统(DMPS)，可以用于高效生成建模，实验表明在包含流形结构的合成数据集上取得了比其他方法更好的效果。 |
| [^36] | [Applications of No-Collision Transportation Maps in Manifold Learning.](http://arxiv.org/abs/2304.00199) | 本文研究了在流形学习中应用无碰撞运输图的方法，其可以比OT图更便宜地计算距离，并提供单个概率测度的平移和伸缩的等距性。 |
| [^37] | [Sequential Learning from Noisy Data: Data-Assimilation Meets Echo-State Network.](http://arxiv.org/abs/2304.00198) | 本文解决了如何从噪声数据中训练递归神经网络的问题，提出了一种利用集合卡尔曼滤波器为回声状态网络（ESN）进行训练的序列训练算法，有效提升了模型性能，同时计算成本较低。 |
| [^38] | [Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning.](http://arxiv.org/abs/2304.00195) | 该论文提出了一个基于Transformer的框架，用于实现符号消息传递和关系推理，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。 |
| [^39] | [Safe Perception-Based Control under Stochastic Sensor Uncertainty using Conformal Prediction.](http://arxiv.org/abs/2304.00194) | 本文提出了一种利用置信度预测来应对传感器噪声及不确定性的感知控制框架，通过量化感知地图的不确定性并将其整合到控制设计中，计算有效的状态估计区域，从而实现连续时间系统的采样数据控制，确保系统的安全性和有效性。 |
| [^40] | [Leveraging Neo4j and deep learning for traffic congestion simulation & optimization.](http://arxiv.org/abs/2304.00192) | 本文利用Neo4j图和深度学习方法解决城市道路网络中的交通拥堵问题，能够实现拥堵路段的负载平衡和优化，同时可以预测交通事故对整体交通的影响。 |
| [^41] | [PrefGen: Preference Guided Image Generation with Relative Attributes.](http://arxiv.org/abs/2304.00185) | $\textit{PrefGen}$ 系统利用简单的成对比较查询，控制生成图像的相对属性。利用这些查询响应的信息，对一组图像属性的偏好进行估计，并进行基于偏好的图像编辑。 |
| [^42] | [Soft-Bellman Equilibrium in Affine Markov Games: Forward Solutions and Inverse Learning.](http://arxiv.org/abs/2304.00163) | 本文提出了一种新的解决方案概念——软Bellman平衡，解决了仿射马尔科夫博弈中的多个玩家交互问题，并提出了一种非线性最小二乘算法来计算此平衡，同时通过投影梯度算法解决推断玩家奖励参数的问题。 |
| [^43] | [Online Reinforcement Learning in Markov Decision Process Using Linear Programming.](http://arxiv.org/abs/2304.00155) | 本论文提出了一种在未知转移矩阵和固定但未知分布的情况下进行在线MDP学习的简单而高效的方法，可以实现更紧的遗憾界，并通过置信区间框架改进了现有算法。 |
| [^44] | [E($3$) Equivariant Graph Neural Networks for Particle-Based Fluid Mechanics.](http://arxiv.org/abs/2304.00150) | 本研究表明，E（3）同变图神经网络相对于非同变网络有更高学习动态交互模型的潜力，可以在基于粒子的流体力学中提供更加物理准确的交互模型。 |
| [^45] | [On the Relationships between Graph Neural Networks for the Simulation of Physical Systems and Classical Numerical Methods.](http://arxiv.org/abs/2304.00146) | 本文探讨了图神经网络与经典数值方法在物理系统模拟上的关系，并提出了有潜力提高机器学习方法准确性和效率的新的模拟方法。展望这些方法将为科学研究提供更高效的机器学习模型。 |
| [^46] | [DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps.](http://arxiv.org/abs/2304.00133) | DeforestVis提供了一种可视化分析工具，通过提供代理决策树，总结了复杂机器学习模型的行为，以帮助用户探索复杂性。 |
| [^47] | [DynamoPMU: A Physics Informed Anomaly Detection and Prediction Methodology using non-linear dynamics from $\mu$PMU Measurement Data.](http://arxiv.org/abs/2304.00092) | 本文旨在解决电力配电系统中实时$\mu$PMU测量数据中事件检测，预测的问题。使用了基于非线性动力学的方法来描述系统来提高检测精度。 |
| [^48] | [Machine Learning for Economics Research: When What and How?.](http://arxiv.org/abs/2304.00086) | 本文精选综述了使用机器学习工具进行经济学研究和政策分析的文章，强调了机器学习在处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性方面的应用优势，成为计量经济学家工具箱中不可或缺的一部分。 |
| [^49] | [Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE.](http://arxiv.org/abs/2304.00083) | 本文提出了一种名为Fides的框架，用于实时验证外协的机器学习工作负载。该框架采用贪心蒸馏迁移学习技术，可动态蒸馏并优化验证模型，以验证对应的服务模型。此外，该框架还能在可信执行环境中运行，以提供更高的安全性和隐私性保证。 |
| [^50] | [A Meta-Summary of Challenges in Building Products with ML Components -- Collecting Experiences from 4758+ Practitioners.](http://arxiv.org/abs/2304.00078) | 该研究汇总了50篇文献对于构建带有机器学习组件的产品所遇到的挑战，并总结了其中最常见的问题，为研究和教育提供了有用的资源。 |
| [^51] | [A Physics-Informed Machine Learning for Electricity Markets: A NYISO Case Study.](http://arxiv.org/abs/2304.00062) | 本论文提出了利用主动集学习技术和物理约束，解决了实时电力市场中的最优电力流问题。这种解决方案特别考虑到了负荷削减和可再生能源发电的削减等现实世界电力系统挑战，从而确保所得到的市场清算结果在物理和经济上是可行的。 |
| [^52] | [To be Robust and to be Fair: Aligning Fairness with Robustness.](http://arxiv.org/abs/2304.00061) | 本研究提出了一种同时考虑公平性和准确性指标的对抗训练和攻击方法，并证明了两个指标之间的一致性以及互相受益的关系。 |
| [^53] | [Almost Linear Constant-Factor Sketching for $\ell_1$ and Logistic Regression.](http://arxiv.org/abs/2304.00051) | 本文提出了一种近线性、常数因子草图，适用于$\ell_1$和logistic回归，具有小的草图维度和高精度，这种草图还在草图空间内提供了高效的优化问题求解方法。 |
| [^54] | [Ranking Regularization for Critical Rare Classes: Minimizing False Positives at a High True Positive Rate.](http://arxiv.org/abs/2304.00049) | 本文提出了一种基于排序的正则化（RankReg）方法，它能在高真阳性率下最小化假阳性，易于实现，且经验性地表明它不仅能有效地减少假阳性，而且还与传统的不平衡学习损失相辅相成。 |
| [^55] | [PEOPL: Characterizing Privately Encoded Open Datasets with Public Labels.](http://arxiv.org/abs/2304.00047) | PEOPL是一种有希望的解决机构共享数据以训练机器学习（ML）模型的难题的方法，使用一定类别的随机构造变换来编码敏感数据。这些方法通过一些信息论分数来量化隐私和效用，并引入了原语构建编码方案系列，并将其适应于编码设置，通过对公开可用数据集的模拟评估不同的编码方案选择，我们观察到我们的方法既有效又可扩展到高维数据。 |
| [^56] | [Accelerating exploration and representation learning with offline pre-training.](http://arxiv.org/abs/2304.00046) | 本论文提出了一个假设，即基于离线数据可以通过分别学习状态表示和辅助奖励模型来改善探索和表示学习，实验证明这种方法显著提高了在具有挑战性的 NetHack 基准测试上的样本效率。 |
| [^57] | [A robust deep learning-based damage identification approach for SHM considering missing data.](http://arxiv.org/abs/2304.00040) | 本文提出了一种基于LSTM模型和自编码器框架中的Dropout机制的鲁棒方法，用于SHM中的损伤识别，考虑缺失数据的存在。 |
| [^58] | [Understanding Reinforcement Learning Algorithms: The Progress from Basic Q-learning to Proximal Policy Optimization.](http://arxiv.org/abs/2304.00026) | 本文综述了强化学习领域，对初学者介绍了关键概念、技术和算法，并全面涵盖了不同类型的强化学习算法，呈现了该领域的历史进展。 |
| [^59] | [SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis.](http://arxiv.org/abs/2304.00020) | 研究提出了一种利用多模态数据的半监督学习方法，命名为SemiMemes，主要应用于Memes的分析和注释过程。该方法在多个数据集中表现优异，并优于其他最新的多模态半监督学习和监督学习模型。 |
| [^60] | [DRIP: Deep Regularizers for Inverse Problems.](http://arxiv.org/abs/2304.00015) | 提出了一种基于变分法的深度神经正则化器家族，保证可以适配数据并解决逆问题。在图像去模糊和小角度层析成像等问题上可行。 |
| [^61] | [Towards Reasonable Budget Allocation in Untargeted Graph Structure Attacks via Gradient Debias.](http://arxiv.org/abs/2304.00010) | 本文提出了一种基于梯度去偏置的图结构无目标攻击模型，用于释放低效的攻击预算。 |
| [^62] | [Bi-directional personalization reinforcement learning-based architecture with active learning using a multi-model data service for the travel nursing industry.](http://arxiv.org/abs/2304.00006) | 本文提出了一个面向旅行护理行业的招聘方案，采用多模型数据服务加速数据采集，并使用双向强化学习和主动学习提供个性化推荐，解决了这一行业标注数据短缺的问题。 |
| [^63] | [PADME-SoSci: A Platform for Analytics and Distributed Machine Learning for the Social Sciences.](http://arxiv.org/abs/2303.18200) | PADME-SoSci是一个联邦学习平台，通过在原数据位置进行学习来实现数据所有权保护和跨位置数据分析。 |
| [^64] | [How Efficient Are Today's Continual Learning Algorithms?.](http://arxiv.org/abs/2303.18171) | 这篇论文研究了增量班级学习的最新方法，并指出许多方法在计算、内存和存储方面非常低效。为了使迭代学习在现实世界中具有适用性，研究界不能忽视这些算法使用的资源。 |
| [^65] | [Analysis and Comparison of Two-Level KFAC Methods for Training Deep Neural Networks.](http://arxiv.org/abs/2303.18083) | 本文研究了两种KFAC二级方法，用于在训练深度神经网络中恢复层间低频交互，研究结果发现这种方法并未显著提高性能。 |
| [^66] | [Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning.](http://arxiv.org/abs/2303.17649) | 本文介绍了一种将英文GPT模型对齐到西班牙语的小封闭领域中的方法，该方法使用了奖励模型来改进答案的解码和生成，在问答任务中取得了良好的结果。 |
| [^67] | [Data-driven abstractions via adaptive refinements and a Kantorovich metric [extended version].](http://arxiv.org/abs/2303.17618) | 我们提出了一种基于自适应细化和康托洛维奇度量的智能且可扩展的动态系统抽象技术，并且定义了一种马尔可夫链之间的度量用作损失函数。我们的方法具有更好的计算复杂度。 |
| [^68] | [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace.](http://arxiv.org/abs/2303.17580) | 用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。 |
| [^69] | [Using AI to Measure Parkinson's Disease Severity at Home.](http://arxiv.org/abs/2303.17573) | 该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。 |
| [^70] | [HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices.](http://arxiv.org/abs/2303.17218) | 本研究提出了一种面向FPGA设备的基于延迟的3D-CNN加速器工具链HARFLOW3D，它以机器学习模型和FPGA的特性描述为输入，生成最小化计算延迟的设计。实验证明HARFLOW3D相比其他方案能够实现更低的延迟。 |
| [^71] | [Poster: Link between Bias, Node Sensitivity and Long-Tail Distribution in trained DNNs.](http://arxiv.org/abs/2303.16589) | DNN训练中长尾分布的数据集将给不同输出类别提供不同的分类性能，本文首次指出导致节点敏感性变化的节点偏差，提出了开放性挑战。 |
| [^72] | [When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!.](http://arxiv.org/abs/2303.16458) | 本文提出了一个通用框架W2PGNN，旨在回答何时预训练图神经网络的关键问题。使用新的角度探索了从预训练数据到下游数据的复杂生成机制。 |
| [^73] | [Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels.](http://arxiv.org/abs/2303.16296) | 本文提出的Dice半度量损失函数可在软标签设置中使用，在医疗成像领域的分割方案中与使用软标签的研究相结合，可以获得更好的Dice分数和模型校准。 |
| [^74] | [Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task.](http://arxiv.org/abs/2303.16210) | 本研究探究了在多输出回归任务中应用深度集合量化校准不确定性的方法，提出了该方法的改进框架，其在回归准确性、不确定性估计可靠性和训练效率方面具有优越表现。 |
| [^75] | [Diffusion Maps for Group-Invariant Manifolds.](http://arxiv.org/abs/2303.16169) | 本文提出了一种扩散映射算法用于群不变流形问题，通过积分在不变数据集上扩展出K-不变拉普拉斯算子，证明了可以利用K中的幺正不可约表示矩阵对其进行对角化，并给出特征值和特征向量的计算公式。同时，展示了规范化拉普拉斯算子L_N收敛于Laplace-Beltrami算子，收敛速度随着对称群K的维数增加而增加。 |
| [^76] | [TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins.](http://arxiv.org/abs/2303.15954) | TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。 |
| [^77] | [Fast Convergence Federated Learning with Aggregated Gradients.](http://arxiv.org/abs/2303.15799) | 该论文提出了一种带有聚合梯度的快速收敛联邦学习方法，通过引入均值场方法来完成参数和梯度的聚合步骤，该方法在收敛速度和通信成本方面优于传统方法。 |
| [^78] | [Adaptive Federated Learning via New Entropy Approach.](http://arxiv.org/abs/2303.14966) | 本文提出了一种新的自适应学习率方案，基于熵理论缓解异构客户端之间的偏差，实现全局模型的快速收敛。 |
| [^79] | [Preserving Linear Separability in Continual Learning by Backward Feature Projection.](http://arxiv.org/abs/2303.14595) | 提出了一种名为BFP的连续学习方法，它通过将新特征变换为旧特征的线性变换来维护线性可分性，从而允许新特征方向的出现以适应新任务，同时保留旧任务的信息。 |
| [^80] | [TRAK: Attributing Model Behavior at Scale.](http://arxiv.org/abs/2303.14186) | TRAK是一种适用于大规模、可微模型的数据归因方法，既有效又计算量可行。 |
| [^81] | [Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity.](http://arxiv.org/abs/2303.13634) | 本文提出了一种物理学指导的神经网络模型PIPN，它能利用稀疏标记数据同时预测所需偏微分方程在数百个不同的几何体上的解，有望在工业界进行快速的几何设计。 |
| [^82] | [Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI.](http://arxiv.org/abs/2303.13336) | 此文介绍了音频扩散模型，重点讨论了两个活跃任务：文本到语音和语音增强，并对实验结果进行了比较和讨论。 |
| [^83] | [Test-time Defense against Adversarial Attacks: Detection and Reconstruction of Adversarial Examples via Masked Autoencoder.](http://arxiv.org/abs/2303.12848) | 该方法使用遮蔽自编码器进行对抗攻击检测和重构，不需要在测试时间更新模型权重，也不需要使用更多的对抗样本来增强训练集。 |
| [^84] | [Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models.](http://arxiv.org/abs/2303.12748) | 本文研究了零样本推理中视觉语言模型的校准问题，发现CLIP存在误校准，并提出了一种修改版的温度缩放方法，可以适用于每个特定的CLIP模型。 |
| [^85] | [Multi-Task Model Personalization for Federated Supervised SVM in Heterogeneous Networks.](http://arxiv.org/abs/2303.10254) | 本文提出了一种用于异构联邦网络中的高效分布式迭代学习方法，通过支持向量机实现对联邦分类和回归任务的处理，并支持个性化的学习模型。为了保护隐私，引入了一种随机掩码过程。研究结果表明，所提出的方法对于解决异构网络中联邦学习任务是有效的。 |
| [^86] | [Data-centric Artificial Intelligence: A Survey.](http://arxiv.org/abs/2303.10158) | 本文讨论了数据中心人工智能的必要性并从三个一般性数据中心目标和代表性方法的全面视角进行了介绍。该综述从自动化和协作的角度组织了现有文献并讨论了挑战。 |
| [^87] | [SUD$^2$: Supervision by Denoising Diffusion Models for Image Reconstruction.](http://arxiv.org/abs/2303.09642) | 本文提出了一种适用于成对训练数据不足的图像重建网络训练的广义框架，并且在缺少成对训练数据的情况下，使用去噪扩散模型进行网络训练的监督。 |
| [^88] | [Block-wise Bit-Compression of Transformer-based Models.](http://arxiv.org/abs/2303.09184) | 本论文提出了一种用于Transformer的块位压缩方法，称为BBCT。该方法可以更细粒度地压缩整个Transformer，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。在GLUE数据集上测试结果表明，BBCT可以在大多数任务中实现少于1％的准确率下降。 |
| [^89] | [Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields.](http://arxiv.org/abs/2303.08435) | 本文提出了一种新的基于机器学习的光刻模型范式，通过优化复值神经场执行光学核回归并将光刻系统拆解为非参数掩模操作和包含行列式源、瞳孔和光刻信息的学习光学核，使用小规模训练数据集展示了卓越的推广能力。 |
| [^90] | [Vision-based route following by an embodied insect-inspired sparse neural network.](http://arxiv.org/abs/2303.08109) | 该论文比较了一种基于仿生稀疏神经网络的FlyHash模型和其他非稀疏模型在路线跟随任务中的效率，发现FlyHash模型在数据编码方面更加高效。 |
| [^91] | [Text-to-image Diffusion Model in Generative AI: A Survey.](http://arxiv.org/abs/2303.07909) | 本文调查了文本到图像扩散模型以及相关应用，总结了最先进的方法，并探讨了挑战和未来方向。 |
| [^92] | [What is the state of the art? Accounting for multiplicity in machine learning benchmark performance.](http://arxiv.org/abs/2303.07272) | 机器学习基准性能评估中，最先进的（SOTA）性能的估计值过于乐观，容易导致方法的忽视。本文提供了一个概率模型，用于校正多重性偏差并比较方法的性能。 |
| [^93] | [A Multi-Modal Simulation Framework to Enable Digital Twin-based V2X Communications in Dynamic Environments.](http://arxiv.org/abs/2303.06947) | 本文提出一种基于数字孪生和多模态仿真框架的V2X通信场景DT创建和仿真方法，可在高移动性V2X通信环境下准确模拟出实际传感器数据和信道。 |
| [^94] | [CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction.](http://arxiv.org/abs/2303.06945) | 本论文提出了一种基于共进化增强的全局关注神经网络，它是一种用于蛋白质相互作用位点预测的深度学习模型，结合了共进化特征和全局关注机制以更好地捕捉氨基酸残基之间的关系并考虑到所有残基的贡献。 |
| [^95] | [Physics-driven machine learning models coupling PyTorch and Firedrake.](http://arxiv.org/abs/2303.06871) | 本论文介绍了一种基于物理学的机器学习技术，结合PyTorch和Firedrake框架，可用于较少的训练数据中实现对复杂物理系统的建模。 |
| [^96] | [Machine Learning Enhanced Hankel Dynamic-Mode Decomposition.](http://arxiv.org/abs/2303.06289) | 本文提出了一种基于深度学习DMD的方法，称为DLHDMD，利用Takens嵌入定理的基本见解开发了一种自适应学习方案，更好地捕捉了高维和混沌动力学，能够为混沌时间序列生成准确的动态。 |
| [^97] | [Hardware Acceleration of Neural Graphics.](http://arxiv.org/abs/2303.05735) | 本文研究了神经图形是否需要硬件支持，发现当前GPU性能无法满足对4K分辨率60FPS渲染的需求，且在增强现实/虚拟现实应用中性能缺口更大。作者确定输入编码和MLP内核是性能瓶颈。 |
| [^98] | [Upper Bound of Real Log Canonical Threshold of Tensor Decomposition and its Application to Bayesian Inference.](http://arxiv.org/abs/2303.05731) | 本研究利用代数几何方法给出了张量分解的实对数典范阈值的上界，并推导了其在贝叶斯推断中的应用理论误差，揭示了张量分解的数学性质。 |
| [^99] | [Gauges and Accelerated Optimization over Smooth and/or Strongly Convex Sets.](http://arxiv.org/abs/2303.05037) | 本文提出了一种新的在平滑和/或强凸集合上定义的可行性和约束优化问题的可扩展、无投影、加速一阶方法，并通过研究量规的新特征达到了强凸问题的最优加速收敛保证 $O(1/T)$、平滑问题的 $O(1/T^2)$，以及两者都满足的加速线性收敛。 |
| [^100] | [Vector Quantized Time Series Generation with a Bidirectional Prior Model.](http://arxiv.org/abs/2303.04743) | 本文提出了一种新的时间序列生成方法，使用向量量化技术和双向变压器模型来生成质量更好、模块化变化更快的合成信号。 |
| [^101] | [Masked Images Are Counterfactual Samples for Robust Fine-tuning.](http://arxiv.org/abs/2303.03052) | 本文提出了一种新颖的深度学习模型微调方法，利用掩蔽图像作为反事实样本，提高模型的鲁棒性。 |
| [^102] | [Chasing Low-Carbon Electricity for Practical and Sustainable DNN Training.](http://arxiv.org/abs/2303.02508) | 本文提出了一种降低深度神经网络训练的碳足迹的实用解决方案，通过在训练过程中控制GPU的能耗来降低碳排放，同时还提出了一种预测方法，可以预测未来的碳强度，对各种DNN应用程序适用，无需额外硬件或基础设施。 |
| [^103] | [Need for Objective Task-based Evaluation of Deep Learning-Based Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT.](http://arxiv.org/abs/2303.02110) | 本研究旨在探讨基于深度学习的图像去噪方法在临床任务中的表现评估，发现使用保真度(FoMs)的评估不一定与任务为基础的评估一致，而基于信号检测理论(SDT)的评估方法提供了更客观、有意义的去噪效果评估方式，并证明虚拟临床试验（VCTs）是评估DL方法的实用工具。 |
| [^104] | [On Function-Coupled Watermarks for Deep Neural Networks.](http://arxiv.org/abs/2302.10296) | 本文提出了一种新颖的深度神经网络水印方案，它可以有效地防御模型微调和修剪等水印删除攻击；通过增强水印和模型功能的耦合，我们的方法可以确保删除水印不可避免地会降低模型在常规输入上的性能。 |
| [^105] | [Multiresolution Graph Transformers and Wavelet Positional Encoding for Learning Hierarchical Structures.](http://arxiv.org/abs/2302.08647) | 该论文提出了多分辨率图形变换器（MGT）和小波位置编码（WavePE）方法，可以学习表示大分子的分层结构，并在众多数据集上获得了比其他先进方法更好的结果。 |
| [^106] | [Guaranteed Dynamic Scheduling of Ultra-Reliable Low-Latency Traffic via Conformal Prediction.](http://arxiv.org/abs/2302.07675) | 本文介绍了一种利用最近的在线符合预测技术，可以提供关于可靠性和延迟的正式保证的URRLC数据包调度程序。 |
| [^107] | [Extensible Motion-based Identification of XR Users using Non-Specific Motion Data.](http://arxiv.org/abs/2302.07517) | 提出了一种可扩展XR用户基于运动的识别方法。与现有基线方法相比，该方法通过仅使用少量的注册数据来识别新用户，可以在几秒钟内注册新用户，而且在仅有少量注册数据可用时也更可靠。 |
| [^108] | [Restoring the saturation response of a PMT using pulse-shape and artificial-neural-networks.](http://arxiv.org/abs/2302.06170) | 该论文提出了使用脉冲形状和人工神经网络恢复PMT饱和响应的方法，可以估计线性区域并提高光子计数和能量重建效率。 |
| [^109] | [I$^2$SB: Image-to-Image Schr\"odinger Bridge.](http://arxiv.org/abs/2302.05872) | 提出了一种新的条件扩散模型I$^2$SB，直接学习两个给定分布之间的非线性扩散过程。通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，在各种图像恢复任务中的性能表现优于标准条件扩散模型，并具有更可解释的生成过程。 |
| [^110] | [Is Distance Matrix Enough for Geometric Deep Learning?.](http://arxiv.org/abs/2302.05743) | 本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。 |
| [^111] | [Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels.](http://arxiv.org/abs/2302.05666) | 本文提出了Jaccard度量损失（JMLs）来优化Jaccard指数，该损失在软标签下仍然有效。 |
| [^112] | [Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking.](http://arxiv.org/abs/2302.03802) | 本文提出了一个基于时空模型的多相机三维多目标跟踪框架，命名为“过去和未来之间的跟踪”。该方法采用注意力跟踪框架，通过对象查询连续地表示跟踪实例，并整合了跟踪对象的前后推理，显著提高了跟踪准确性和ID-Switches的减少。 |
| [^113] | [Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy.](http://arxiv.org/abs/2302.03542) | 该论文提出了一种代理算法，通过使用一个易于访问的函数作为代理，可以以与原函数梯度下降相匹配的速度收敛，从而显著提高样本效率，并在机器学习中具有许多潜在应用。 |
| [^114] | [SMDP-Based Dynamic Batching for Efficient Inference on GPU-Based Platforms.](http://arxiv.org/abs/2301.12865) | 本文提出了一种动态批处理策略，采用基于GPU的批处理服务队列进行建模，通过半马尔可夫决策过程的方法最小化平均响应时间和功耗。 |
| [^115] | [Learning Optimal Features via Partial Invariance.](http://arxiv.org/abs/2301.12067) | 本文证明了不变风险最小化（IRM）的次优性，并提出了通过部分不变性来缓解此问题的方法，同时展示了从训练域的一个分区中学习以提高不变性模型的有效性。 |
| [^116] | [Truveta Mapper: A Zero-shot Ontology Alignment Framework.](http://arxiv.org/abs/2301.09767) | 提出了一个将无监督本体匹配或本体对齐视为翻译任务的新视角的Truveta Mapper框架，在零样本、统一和端到端的方式下执行多本体对齐。该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案，无需显式跨本体手动标注数据。 |
| [^117] | [Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning.](http://arxiv.org/abs/2301.05169) | 本文提出了一个因果表示学习基准——因果三元组，该基准具有可操作的反事实设置和干预性下游任务，对分离和物体中心表示学习取得了显著优化，然而在因果关系的识别和干预性下游任务上表现欠佳。 |
| [^118] | [Data-centric AI: Perspectives and Challenges.](http://arxiv.org/abs/2301.04819) | 研究提出了数据中心人工智能（DCAI）的概念，强调数据质量和可靠性，总结了训练数据开发、推断数据开发和数据维护三个总体使命，提供了对DCAI任务的讨论和观点，并列出了挑战。 |
| [^119] | [The Predictive Forward-Forward Algorithm.](http://arxiv.org/abs/2301.01452) | PFF算法用于神经系统内的信贷分配，采用新颖的动态递归神经系统和定向生成电路，有效地通过前向传递学习信号和更新突触，消除了计算和结构上的约束。 |
| [^120] | [FunkNN: Neural Interpolation for Functional Generation.](http://arxiv.org/abs/2212.14042) | FunkNN是一种新的卷积网络，可以连续地重建图像，实现了最先进的图像生成质量，优于现有的基于MLP的方法。 |
| [^121] | [Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers.](http://arxiv.org/abs/2212.12474) | 本文使用物理学知识指导的高斯过程回归方法，解决线性偏微分方程求解器无法量化近似误差的问题。 |
| [^122] | [Interpretable Diabetic Retinopathy Diagnosis based on Biomarker Activation Map.](http://arxiv.org/abs/2212.06299) | 本文提出了一种基于生物标志物激活地图（BAM）的诊断框架，使用两个U形生成器为临床医生提供意义明确的可解释性，以验证和理解分类器的决策，可应用于自动诊断糖尿病视网膜病变。 |
| [^123] | [Self-Supervised Object Goal Navigation with In-Situ Finetuning.](http://arxiv.org/abs/2212.05923) | 本文将自我监督的机器人导航应用到目标导航中，基于位置一致性自我监督信号进行训练。该方法避免了标记昂贵的3D网格，可在真实世界和模拟中表现出有竞争力的结果。 |
| [^124] | [Genie: Show Me the Data for Quantization.](http://arxiv.org/abs/2212.04780) | Genie提出了一个后训练量化方案，用于开发轻量级深度神经网络，并提出了一个用于生成适合零样本量化的数据的框架。 |
| [^125] | [On the Compatibility between Neural Networks and Partial Differential Equations for Physics-informed Learning.](http://arxiv.org/abs/2212.00270) | 本文探讨了物理知识神经网络的缺陷和机遇，证明了基于ReLU的MLP不能形成解决方案的合法函数空间，而使用带有输出层超平面的具有$C^n$激活函数的MLP可以严格满足一个线性PDE直到 $n$ 阶。 |
| [^126] | [SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene.](http://arxiv.org/abs/2211.17260) | SinGRAF是一个3D感知的生成模型，只需少量输入一单个场景的图像即可训练，并可在保持输入外观的同时生成多样的3D场景。 |
| [^127] | [Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules.](http://arxiv.org/abs/2211.16691) | 本研究提出了一种基于简单规则的有针对性探索方法，通过避免已知子优的状态-动作空间区域来更快地加速强化学习代理程序的收敛，并在一个房间温度控制案例研究中实现了比传统方法快6-7倍的速度收敛。 |
| [^128] | [Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test.](http://arxiv.org/abs/2211.16596) | 该论文提出了一种针对稀有事件的新的因果发现方法，基于收集到的时间不变动态系统的数据，构建了叠加数据集和条件独立性检验的方法。该方法能够揭示在变量第一次经历低概率实现时才会显现的因果关系，具有很好的可行性和可扩展性。 |
| [^129] | [A Revenue Function for Comparison-Based Hierarchical Clustering.](http://arxiv.org/abs/2211.16459) | 本论文提出了一种收益函数，可以仅使用比较来衡量基于比较的分层聚类的质量。 |
| [^130] | [A Light Touch Approach to Teaching Transformers Multi-view Geometry.](http://arxiv.org/abs/2211.15107) | 本论文提出了一种轻触式的方法，引导视觉Transformer学习多视角几何，这种方法通过使用极线来引导Transformer的交叉注意力图，可以在测试时不需要提供任何摄像机姿态信息，适用于姿态不变的物体实例检索。 |
| [^131] | [OReX: Object Reconstruction from Planar Cross-sections Using Neural Fields.](http://arxiv.org/abs/2211.12886) | 本文介绍了一种名为OReX的方法，使用神经场作为插值先验，仅使用输入的平面切片，即可高质量地重建三维形状。 |
| [^132] | [Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models.](http://arxiv.org/abs/2211.11176) | 本研究提出了一种基于时间依赖图的通用图神经网络结构(GraphS4mer)，用于建立多元生物信号模型。该模型结合了结构化状态空间架构和动态演变的图结构学习层来解决长时序和复杂空间相关性，能有效地提高多元生物信号分类任务性能。 |
| [^133] | [Learning to Generate Image Embeddings with User-level Differential Privacy.](http://arxiv.org/abs/2211.10844) | 本文提出了一种DP-FedEmb算法，通过虚拟客户端、部分聚合、私有本地微调和公共预训练，实现了用户级差分隐私。在图像嵌入模型的学习中，DP-FedEmb能够在保持良好模型效用的同时，实现较强的隐私保护，得到的实验结果表明其在基准数据集上的表现优越。 |
| [^134] | [Using explainability to design physics-aware CNNs for solving subsurface inverse problems.](http://arxiv.org/abs/2211.08651) | 本研究提出了一种使用可解释性技术设计物理感知神经网络的新方法，通过开发卷积神经网络求解地下反问题展示应用价值。研究使用Score-CAM和Deep SHAP等方法选择超参数以提高解释性和预测准确性。 |
| [^135] | [Explainable Action Advising for Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2211.07882) | 引入可解释行为建议的多智能体强化学习框架，使得学生可以理解所学的内容并进行推理从而提高样本效率和学习效果 |
| [^136] | [Quantifying the Impact of Label Noise on Federated Learning.](http://arxiv.org/abs/2211.07816) | 本文量化了标签噪声对FL的影响。实验结果表明，随着噪声水平的增加，全局模型的准确性会线性下降，同时会导致FL训练的收敛速度减缓和全局模型过拟合。 |
| [^137] | [Human alignment of neural network representations.](http://arxiv.org/abs/2211.01201) | 本文研究神经网络表示与人类心理表示之间的对齐问题，发现模型规模和体系结构对对齐几乎没有影响，而训练数据集和目标函数都对对齐有很大的影响。从一个数据集中学习的神经网络表示的线性变换能显著提高对另外两个数据集中人类相似性判断的对齐性。 |
| [^138] | [Pop2Piano : Pop Audio-based Piano Cover Generation.](http://arxiv.org/abs/2211.00895) | 本文提出了Pop2Piano，一个通过Transformer网络，直接从流行音频生成钢琴翻奏的模型。使用自动生成的大量配对和同步数据进行训练，该模型在生成合理的钢琴翻奏方面具有潜力。 |
| [^139] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^140] | [Boosting the Cycle Counting Power of Graph Neural Networks with I$^2$-GNNs.](http://arxiv.org/abs/2210.13978) | 本文研究了一种名为Subgraph MPNNs的GNN模型。我们发现，Subgraph MPNNs不能在节点级别上计数超过4个的环，这对于生物学、化学和社交网络分析等应用至关重要。 |
| [^141] | [RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical Centerline Extraction.](http://arxiv.org/abs/2210.09309) | 本文扩展了RibSeg数据集到大规模基准测试RibSeg v2，加入了手动标注的肋骨标记和解剖中心线提取，共包含660个CT扫描（15,466个独立的肋骨），并提出了深度学习方法用于肋骨标记、基于骨架化方法用于中心线提取、一种稀疏点云表示CT扫描的方法，以及适用于该任务的评估指标。 |
| [^142] | [Learning to Efficiently Plan Robust Frictional Multi-Object Grasps.](http://arxiv.org/abs/2210.07420) | 本文介绍了一种使用神经网络进行摩擦多物体抓取的高效计划方法，相比于以往的工作，其成功率提高了13.7％，每小时的拾取次数增加了1.6倍，并且抓取计划时间减少了6.3倍。 |
| [^143] | [Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild.](http://arxiv.org/abs/2210.07199) | 本文提出了一种自监督学习方法，直接在大规模真实世界物体视频上进行类别级6D姿态估计。通过表面嵌入学习了输入图像和规范形状之间的密集对应关系，并提出了新颖的几何循环一致性损失。学习到的对应关系可以应用于6D姿态估计和其他任务。 |
| [^144] | [Self-Guided Diffusion Models.](http://arxiv.org/abs/2210.06462) | 本文提出了一种框架，利用自我监督信号的灵活性设计了自我指导扩散模型。实验表明，自标记指导始终优于没有指导的扩散模型，并且在不平衡数据上甚至可以超过基于真实标签的指导。 |
| [^145] | [Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP.](http://arxiv.org/abs/2210.04150) | 本文提出了一种基于CLIP和带掩膜的体系结构的开放词汇语义分割方法，通过在用于训练的嘈杂但多样化的数据集上对CLIP进行微调，以提高其在带有掩膜的图像上的性能，超越了当前最佳的方法。 |
| [^146] | [Disentanglement with Biological Constraints: A Theory of Functional Cell Types.](http://arxiv.org/abs/2210.01768) | 本文通过数学证明，简单的生物学约束（如在活动和权重方面的非负性和能量效率）可以促进大脑神经元的单因素选择性，从而实现解缠表示，解释了为什么大脑中的单个神经元经常表示单个可解释因素。 |
| [^147] | [LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models.](http://arxiv.org/abs/2210.01115) | 本文提出了一种针对软模板学习中存在的基类过拟合问题的文本提示学习方法——LASP, 同时通过增加提示的表示能力和校准视觉-语言不匹配问题， 在三个下游任务上取得了显著的性能优于现有技术的实验结果。 |
| [^148] | [Analysis of (sub-)Riemannian PDE-G-CNNs.](http://arxiv.org/abs/2210.00935) | 本文为PDE-G-CNN模型中的形态学卷积核问题提供了新的解决方案，解决了先前推荐的近似核不准确的问题。 |
| [^149] | [Patching Weak Convolutional Neural Network Models through Modularization and Composition.](http://arxiv.org/abs/2209.06116) | 本文提出了一种通过压缩模块化和组合来修补卷积神经网络模型的弱点的方法，该方法无需重新训练整个模型，并在多个基准数据集上实现了与最先进方法相当甚至更好的结果。 |
| [^150] | [R\'{e}nyi Divergence Deep Mutual Learning.](http://arxiv.org/abs/2209.05732) | 本文提出在深度互相学习中使用R\'{e}nyi散度，它能够在不引入大量复杂度的情况下持续提高性能，获得了广泛的实证结果的支持。 |
| [^151] | [The role of entanglement for enhancing the efficiency of quantum kernels towards classification.](http://arxiv.org/abs/2209.05142) | 本论文通过使用基于线性和全纠缠电路作为超参数的量子核函数，在量子支持向量机中加强了纠缠作用，提高了分类准确性和表达能力。所提出的全纠缠电路在大多数特征上优于其他全纠缠或线性纠缠电路以及经典算法。 |
| [^152] | [Diffusion Models in Vision: A Survey.](http://arxiv.org/abs/2209.04747) | 扩散模型是视觉中的新兴主题，其生成样本的质量和多样性受到广泛欣赏。本综述介绍了三种通用的扩散建模方法以及各种算法和架构方面的讨论，并总结比较了扩散模型与其他最先进的生成模型的性能。 |
| [^153] | [From latent dynamics to meaningful representations.](http://arxiv.org/abs/2209.00905) | 本文提出了一个动力学约束的表示学习框架，通过限制潜在表示遵循特定动态规律以使得学习到的表示具有意义。在真实世界的DNA荧光电影数据集上验证了该算法，表明其可以准确地学习动态规律，并获得有意义的表示。 |
| [^154] | [Smooth Monotone Stochastic Variational Inequalities and Saddle Point Problems: A Survey.](http://arxiv.org/abs/2208.13592) | 本文综述了解决平滑（强）单调随机变分不等式的方法。 |
| [^155] | [Causal Bandits for Linear Structural Equation Models.](http://arxiv.org/abs/2208.12764) | 本文提出了针对线性结构方程模型的因果赌博算法，摒弃了已知干预分布或其边缘分布的假设。 |
| [^156] | [Machine Learning with Confidential Computing: A Systematization of Knowledge.](http://arxiv.org/abs/2208.10134) | 本文研究机器学习和机密计算的结合，并梳理了先前的研究成果，提供了关于保证机密性和完整性的技术，同时讨论了它们的高级特性和局限性。本文进一步确定了现有的可信执行环境（TEE）系统在机器学习用例中的限制，并讨论了未来的展望。 |
| [^157] | [Dance Style Transfer with Cross-modal Transformer.](http://arxiv.org/abs/2208.09406) | 提出了一种名为CycleDance的舞蹈风格转换系统，通过跨模态变换器编码器，并采用基于序列长度的课程学习和新指标对舞蹈运动进行转移和合成 ，能够实现逼真的舞蹈风格转换。 |
| [^158] | [MENLI: Robust Evaluation Metrics from Natural Language Inference.](http://arxiv.org/abs/2208.07316) | 本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。 |
| [^159] | [Bayesian Optimization with Informative Covariance.](http://arxiv.org/abs/2208.02704) | 提出了一种新颖的用于优化的信息丰富协方差函数，利用非平稳性来编码对搜索空间中某些区域的偏好，并在优化过程中自适应地促进局部探索，以提高贝叶斯优化在高维空间中的样本效率。 |
| [^160] | [DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing.](http://arxiv.org/abs/2207.08562) | 本文提出了一个双视图超关系知识图嵌入方法，其中DH-KG包含超关系实例视图和从实体层次抽象出的超关系本体视图，定义了链接预测和实体类型识别任务，并构建了两个DH-KG数据集JW44K-6K和HTDM。DHGE是一个基于GRAN编码器、HGNN和联合学习的DH-KG嵌入模型。 |
| [^161] | [Retweet-BERT: Political Leaning Detection Using Language Features and Information Diffusion on Social Networks.](http://arxiv.org/abs/2207.08349) | Retweet-BERT是一个简单而可扩展的模型，用于估计Twitter用户的政治倾向。该模型利用转发网络结构和用户语言特征，并在COVID-19和2020年美国总统选举数据集上展现出有竞争力的性能。研究还表明，在Twitter上存在着右倾用户之间的政治回音室。 |
| [^162] | [AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection.](http://arxiv.org/abs/2206.15476) | AnoShift是一个用于无监督异常检测的数据分布偏移基准，旨在评估模型处理分布偏移的能力。其使用了一个基于时间变化交通数据集的非平稳数据集，使用AnoShift可以有效地区分模型在分布偏移情况下检测异常的能力。 |
| [^163] | [When Does Re-initialization Work?.](http://arxiv.org/abs/2206.10011) | 研究表明，当没有其他正则化技术时，重新初始化神经网络有助于提高泛化性能。但是，当它与其他正则化技术一起使用时，对泛化性能的额外帮助很小，尽管最佳泛化性能变得更加稳定。 |
| [^164] | [Proximal Splitting Adversarial Attacks for Semantic Segmentation.](http://arxiv.org/abs/2206.07179) | 本文提出了适用于语义分割的近端分裂对抗攻击方法，采用扩展拉格朗日方法处理大量约束且产生更小的对抗性扰动。 |
| [^165] | [Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models.](http://arxiv.org/abs/2206.00501) | 本文发现良性过拟合在存在标签噪声的情况下可能会失败，提醒未来需要理解欠拟合制度下的隐式偏见。 |
| [^166] | [Average Adjusted Association: Efficient Estimation with High Dimensional Confounders.](http://arxiv.org/abs/2205.14048) | 本研究提出了一个总结衡量标准——平均调整关联度（AAA），用于评估一个具有混淆影响的异质群体中的关联程度。并且我们提出了高效的估计方法，可用于各种采样场景。 |
| [^167] | [A Framework for Flexible Peak Storm Surge Prediction.](http://arxiv.org/abs/2204.13168) | 本文提出了一种基于多阶段方法的新型峰值风暴潮预测代理模型，可在保证预测准确性的同时显著降低计算成本和模型参数数量，有望在风险评估和应急管理决策方面发挥重要作用。 |
| [^168] | [Sequence-Based Target Coin Prediction for Cryptocurrency Pump-and-Dump.](http://arxiv.org/abs/2204.12929) | 本文致力于预测在计划的pump时间之前，列在目标交易所中的所有币的pump概率，通过对P&D事件的经验分析和开发基于序列的神经网络，我们发现pump的硬币呈现出内部信道的同质性和跨信道的异质性，提高了我们的预测准确性。 |
| [^169] | [Improving generalization of machine learning-identified biomarkers with causal modeling: an investigation into immune receptor diagnostics.](http://arxiv.org/abs/2204.09291) | 本文研究通过因果建模提高机器学习诊断鲁棒性的方法，以免疫受体库为例，阐明实验因素如何影响学习的生物标志物。 |
| [^170] | [Conditional Injective Flows for Bayesian Imaging.](http://arxiv.org/abs/2204.07664) | 本文提出了一种新的条件可逆流方法，专门设计用于成像问题。通过可逆性减少了内存占用和训练时间，并且在大规模实验中表现最好。 |
| [^171] | [Zero-shot meta-learning for small-scale data from human subjects.](http://arxiv.org/abs/2203.16309) | 为了解决小样本人类数据的零样本学习问题，我们提出了一个元学习框架，能够快速适应有限的训练数据，处理多任务预测并可以从整体上表现最佳。 |
| [^172] | [Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding.](http://arxiv.org/abs/2203.05711) | 这个研究收集并发布了一个视频-语言故事数据集SYMON，用于推进多模态故事理解的发展。 |
| [^173] | [Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap.](http://arxiv.org/abs/2203.04275) | 本文介绍了一种跨领域差异的非合作航天器姿态估计的多尺度、多任务CNN，通过在合成图像上进行数据增强训练共享编码器以学习通用特征。同时介绍了一种在线域优化方法，用于调整模型的标准化层参数。 |
| [^174] | [Exact Solutions of a Deep Linear Network.](http://arxiv.org/abs/2202.04777) | 本研究找到了带权重衰减和随机神经元的深度线性网络全局最小值的解析表达式，结果表明权重衰减与模型架构的强烈交互作用会在多于1个隐藏层的网络中创建不良极小值，并表明常见的深度学习初始化方法无法在一般情况下缓解神经网络的优化问题。 |
| [^175] | [Optimal Algorithms for Decentralized Stochastic Variational Inequalities.](http://arxiv.org/abs/2202.02771) | 本文针对分布式随机变分不等式的问题，提出了匹配复杂度下界的最优算法，不仅在分布式随机情况下表现最佳，在分布式确定性和非分布式随机情况下也是最佳的，并得到实验验证。 |
| [^176] | [Near-Optimal Learning of Extensive-Form Games with Imperfect Information.](http://arxiv.org/abs/2202.01752) | 本文提出了一种新的算法系列，可以更快速地在不完美信息广义博弈中找到一个近似最优解。 |
| [^177] | [Compactness Score: A Fast Filter Method for Unsupervised Feature Selection.](http://arxiv.org/abs/2201.13194) | 本文提出了一种快速的无监督特征选择方法，名为紧密度分数（CSUFS），通过评估局部紧密度来选择所需特征，能够实现降维、模型效果提高和模型性能提高等功能。 |
| [^178] | [Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees.](http://arxiv.org/abs/2201.08355) | 本文提出了模拟-实验室-真实环境下的安全强化学习及泛化保证框架，利用概率保证的安全感知策略分布来弥合现实差距，具有双重策略设置、监督控制策略和“可能近似正确（PAC）-Bayes”框架等特点，能够在移动机器人导航和四轴飞行等任务中实现改进的安全性和泛化能力。 |
| [^179] | [Physics Constrained Flow Neural Network for Short-Timescale Predictions in Data Communications Networks.](http://arxiv.org/abs/2112.12321) | 本文提出了一个新的用于短时刻预测的神经网络模型FlowNN，通过引入物理偏差、使用自监督学习策略来改进特征表述。在合成和真实网络数据集上都取得了17%到71%的损失下降。 |
| [^180] | [Defining and Quantifying the Emergence of Sparse Concepts in DNNs.](http://arxiv.org/abs/2111.06206) | 本文提出了在DNN中理解产生的交互概念的概念-emerging现象，这些概念可以用稀疏的符号因果图和And-Or图（AOG）进行量化和简化。 |
| [^181] | [Towards Fairness-Aware Federated Learning.](http://arxiv.org/abs/2111.01872) | 本文旨在概述近年来提出的公平性感知联邦学习（FAFL）方法，以解决在联邦学习中可能出现的不公平问题。 |
| [^182] | [Likelihood Training of Schr\"odinger Bridge using Forward-Backward SDEs Theory.](http://arxiv.org/abs/2110.11291) | 本文提出了一种新的计算框架，用于基于正向-反向随机微分方程理论对薛定谔桥进行似然训练。通过这个框架，可以构建SB的似然目标，这可以成为现代深度生成模型训练的替代方法。 |
| [^183] | [Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees.](http://arxiv.org/abs/2110.03313) | 本研究提出了MASHA1和MASHA2方法，可以减少在分布式训练中的通信量，并在获得可比性质量的模型的同时，解决变分不等式和鞍点问题。 |
| [^184] | [OpenFed: A Comprehensive and Versatile Open-Source Federated Learning Framework.](http://arxiv.org/abs/2109.07852) | OpenFed是一个全面而灵活的开源联邦学习框架，通过去中心化模型训练消除了数据传输和集中式汇聚的需求，降低了学习成本，为研究人员和联邦学习使用者提供了更好的应用体验。 |
| [^185] | [Federated Ensemble Model-based Reinforcement Learning in Edge Computing.](http://arxiv.org/abs/2109.05549) | 本文提出了一种新颖的联邦强化学习算法，在边缘计算中使用联邦学习和知识蒸馏创建动态模型集合，通过仅使用集合模型而不与环境交互来训练策略，从而解决了现有算法中高样本复杂度和缺乏理论保证的问题。 |
| [^186] | [Asynchronous Iterations in Optimization: New Sequence Results and Sharper Algorithmic Guarantees.](http://arxiv.org/abs/2109.04522) | 该论文介绍了一种新的收敛结果，可以应用于并行和分布式优化算法的异步迭代分析中。作者使用这一结果，使得数种并行优化算法的收敛证明变得更加精炼、简单同时还增强了原有的证明可信度，进而建立了至今缺乏完整理论理解的流行算法的收敛保证。 |
| [^187] | [Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning.](http://arxiv.org/abs/2109.03445) | 本文提出了一种批量异步随机逼近算法，它可以在内存需求和时间复杂度之间进行权衡，同时提供了可以使用较弱假设证明收敛的一般方法；在强化学习领域，我们使用此方法证明了SARSA算法的批量异步版本的收敛性。 |
| [^188] | [AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo.](http://arxiv.org/abs/2108.02998) | 本论文介绍了基于人工智能的主动脉血管树分割在心血管疾病治疗中的现状，提高了通过CTA检查检测主动脉及其分支血管的效率。 |
| [^189] | [Federated Causal Inference in Heterogeneous Observational Data.](http://arxiv.org/abs/2107.11732) | 本文开发了联邦方法，在异构的本地数据中进行摘要统计信息的计算，并在站点之间聚合这些统计信息，以获得多站点数据的平均治疗效果的点估计和方差估计。聚合方案需要考虑站点之间的治疗分配异质性和结果的异质性，以使得估计量是一致的和渐近正常的。 |
| [^190] | [Decentralized Local Stochastic Extra-Gradient for Variational Inequalities.](http://arxiv.org/abs/2106.08315) | 本文研究了非有界域上非IID分布式随机变分不等式问题，在分散的计算网络中使用随机额外梯度方法，在强单调、单调和非单调的情况下分别分析了收敛速度，并将其应用于开发具有非IID数据的联邦学习的分散算法。 |
| [^191] | [FL-Market: Trading Private Models in Federated Learning.](http://arxiv.org/abs/2106.04384) | 本文提出FL-Market，一种隐私保护的本地私有模型交易市场，使用联邦学习解耦ML与经纪人集中收集训练数据的需求。 |
| [^192] | [Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions.](http://arxiv.org/abs/2106.02619) | 本文研究了GAN如何有效地学习那些接近真实图像分布的分层生成分布，当一个分布具有前向超分辨率结构时，通过SGDA简单地训练GAN就能够实现高效学习。 |
| [^193] | [Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment Restriction.](http://arxiv.org/abs/2105.04544) | 本论文提出了两种基于核函数的方法，用于解决存在未观测混淆，但同时观测到混淆代理的因果效应估计问题。这些方法在合成数据和模拟真实世界任务的数据上都可以获得有竞争力的结果。 |
| [^194] | [DeepEverest: Accelerating Declarative Top-K Queries for Deep Neural Network Interpretation.](http://arxiv.org/abs/2104.02234) | DeepEverest是一个高效的系统，能够在深度神经网络激活值上执行解释查询。通过使用有效的索引技术和查询执行算法，DeepEverest可以将单个查询加速高达63倍，并在多查询工作负载上优于其他方法。 |
| [^195] | [Towards a mathematical theory of trajectory inference.](http://arxiv.org/abs/2102.09204) | 本文提出了一个数学理论框架和数值方法，可以通过时间边缘样本推断出随机过程的轨迹，特别是它可以应用于单细胞RNA测序数据的分析和轨迹推断。 |
| [^196] | [dame-flame: A Python Library Providing Fast Interpretable Matching for Causal Inference.](http://arxiv.org/abs/2101.01867) | dame-flame是一个用于观察性因果推断匹配的Python软件包，实现了快速匹配算法DAME和FLAME，用机器学习确定重要协变量进行匹配，产生的结果是高质量且可解释。 |

# 详细

[^1]: AMC-Net: 一种有效的自动调制分类网络

    AMC-Net: An Effective Network for Automatic Modulation Classification. (arXiv:2304.00445v1 [eess.SP])

    [http://arxiv.org/abs/2304.00445](http://arxiv.org/abs/2304.00445)

    AMC-Net是一种用于改善自动调制分类效率和有效性的新型网络，通过在频域中进行降噪和执行多尺度特征提取来提高识别性能，在两个代表性数据集上实验表现更优。

    

    自动调制分类(AMC)在无线通信系统的频谱管理，信号监测和控制中是至关重要的阶段。准确的调制格式分类在后续传输数据的解码中起着至关重要的作用。最近，端到端深度学习方法已经应用于AMC，优于传统的特征工程技术。然而，在低信噪比(SNR)环境中，AMC仍然存在局限性。为解决这个问题，我们提出了一种新颖的AMC-Net，它通过在频域中对输入信号进行降噪，并执行多尺度和有效的特征提取来改善识别。在两个代表性数据集上的实验表明，我们的模型比现有的大多数方法在效率和有效性方面表现更好。

    Automatic modulation classification (AMC) is a crucial stage in the spectrum management, signal monitoring, and control of wireless communication systems. The accurate classification of the modulation format plays a vital role in the subsequent decoding of the transmitted data. End-to-end deep learning methods have been recently applied to AMC, outperforming traditional feature engineering techniques. However, AMC still has limitations in low signal-to-noise ratio (SNR) environments. To address the drawback, we propose a novel AMC-Net that improves recognition by denoising the input signal in the frequency domain while performing multi-scale and effective feature extraction. Experiments on two representative datasets demonstrate that our model performs better in efficiency and effectiveness than the most current methods.
    
[^2]: SoftED: 用于时间序列事件检测的软评估指标

    SoftED: Metrics for Soft Evaluation of Time Series Event Detection. (arXiv:2304.00439v1 [cs.LG])

    [http://arxiv.org/abs/2304.00439](http://arxiv.org/abs/2304.00439)

    SoftED metrics 是一种适用于时间序列事件检测的新指标，既包括时间的概念，又包括对相邻检测的时间容忍度，它们能够同时评估事件检测的准确性和其检测是否代表事件。

    

    时间序列事件检测方法通常通过标准的分类指标进行评估，这些指标仅关注检测准确性。然而，事件检测的不准确往往是由于前后相关事件在相邻检测中的反应产生的。这些检测对于触发必要的行动或帮助减轻不良后果非常有价值。在这种情况下，现有的指标对于事件检测来说是不充分和不适当的。因此，需要一种指标，既包括时间的概念，又包括对相邻检测的时间容忍度。本文介绍了一种新的指标集合“SoftED metrics”，旨在软评估事件检测方法。它们可以评估检测的准确性以及其检测是否代表事件。通过将事件和代表性检测相结合，并在36\%以上的实验中加入时间容忍度，提高了事件检测的评估效果。

    Time series event detection methods are evaluated mainly by standard classification metrics that focus solely on detection accuracy. However, inaccuracy in detecting an event can often result from its preceding or delayed effects reflected in neighboring detections. These detections are valuable to trigger necessary actions or help mitigate unwelcome consequences. In this context, current metrics are insufficient and inadequate for the context of event detection. There is a demand for metrics that incorporate both the concept of time and temporal tolerance for neighboring detections. This paper introduces SoftED metrics, a new set of metrics designed for soft evaluating event detection methods. They enable the evaluation of both detection accuracy and the degree to which their detections represent events. They improved event detection evaluation by associating events and their representative detections, incorporating temporal tolerance in over 36\% of experiments compared to the usual 
    
[^3]: 利用生成对抗网络和马尔科夫链蒙特卡罗方法计算理想观察者

    Ideal Observer Computation by Use of Markov-Chain Monte Carlo with Generative Adversarial Networks. (arXiv:2304.00433v1 [eess.SP])

    [http://arxiv.org/abs/2304.00433](http://arxiv.org/abs/2304.00433)

    本文提出了使用生成对抗网络和马尔可夫链蒙特卡罗方法计算理想观察者的方法，以扩展IO方法适用范围。

    

    医学成像系统经常通过客观或任务特定的图像质量（IQ）度量来评估和优化，以量化观察者在具体的临床任务上的表现。贝叶斯理想观察者（IO）的表现在所有观察者（数值或人类）中设定了一个上限，并已被提倡用作评估和优化医学成像系统效能的merit。然而，IO测试统计量对应于大多数情况下难以计算的似然比。以前曾提出过一种采用马尔科夫链蒙特卡罗（MCMC）技术的基于采样的方法来估计IO性能。然而，目前将MCMC方法应用于IO近似的情况仅限于少数情况，其中被成像对象的考虑分布可以用相对简单的随机对象模型（SOM）来描述。因此，有必要扩展领域，使其适用于更广泛的情况。

    Medical imaging systems are often evaluated and optimized via objective, or task-specific, measures of image quality (IQ) that quantify the performance of an observer on a specific clinically-relevant task. The performance of the Bayesian Ideal Observer (IO) sets an upper limit among all observers, numerical or human, and has been advocated for use as a figure-of-merit (FOM) for evaluating and optimizing medical imaging systems. However, the IO test statistic corresponds to the likelihood ratio that is intractable to compute in the majority of cases. A sampling-based method that employs Markov-Chain Monte Carlo (MCMC) techniques was previously proposed to estimate the IO performance. However, current applications of MCMC methods for IO approximation have been limited to a small number of situations where the considered distribution of to-be-imaged objects can be described by a relatively simple stochastic object model (SOM). As such, there remains an important need to extend the domain
    
[^4]: 几何约束提高了对稀疏观测的随机动力学的推断

    Geometric constraints improve inference of sparsely observed stochastic dynamics. (arXiv:2304.00423v1 [stat.ME])

    [http://arxiv.org/abs/2304.00423](http://arxiv.org/abs/2304.00423)

    本文提出一种新的方法，该方法利用数据驱动的控制，既考虑了系统不变密度的几何形状，又能对系统进行有效识别。

    

    许多自由度的系统在多个尺度上演化的动力学通常以随机微分方程的形式进行建模。通常这些方程的结构形式是未知的，系统动力学的唯一表现形式是在离散时间点上的观测。尽管它们被广泛使用，但准确地从稀疏时域观测中推断这些系统仍然具有挑战性。传统的推断方法要么集中于观测的时间结构，忽略系统不变密度的几何形状，要么使用系统不变密度的几何逼近，这些逼近仅适用于保守的驱动力。为了解决这些局限性，我们在此提出了一种新的方法，它将这两个视角调和在一起。我们提出了一种路径增强方案，它使用数据驱动的控制来考虑不变系统密度的几何形状。对增强路径的非参数推断，实现了对系统的有效识别。

    The dynamics of systems of many degrees of freedom evolving on multiple scales are often modeled in terms of stochastic differential equations. Usually the structural form of these equations is unknown and the only manifestation of the system's dynamics are observations at discrete points in time. Despite their widespread use, accurately inferring these systems from sparse-in-time observations remains challenging. Conventional inference methods either focus on the temporal structure of observations, neglecting the geometry of the system's invariant density, or use geometric approximations of the invariant density, which are limited to conservative driving forces. To address these limitations, here, we introduce a novel approach that reconciles these two perspectives. We propose a path augmentation scheme that employs data-driven control to account for the geometry of the invariant system's density. Non-parametric inference on the augmented paths, enables efficient identification of the
    
[^5]: 实验平台应用强化学习：贝叶斯序贯决策在连续监测中的应用

    Experimentation Platforms Meet Reinforcement Learning: Bayesian Sequential Decision-Making for Continuous Monitoring. (arXiv:2304.00420v1 [cs.LG])

    [http://arxiv.org/abs/2304.00420](http://arxiv.org/abs/2304.00420)

    本文介绍了一个新的框架，将实验监测问题制定为具有统一效用函数的贝叶斯序贯决策问题，并采用强化学习来解决最优决策规则。这一方法能够有效提高客户体验并控制机会成本。

    

    随着在线A/B测试支持工业创新的需求增长，实验运行的机会成本变得不可忽略。因此，需要一种有效的连续监测服务，以在适当时候进行早期停止。经典的统计方法着重于假设检验，主要是针对传统的高风险问题，如临床试验，而在线服务公司的实验通常具有非常不同的特征和关注点。在实际需要的驱动下，本文介绍了亚马逊开发的一种新的框架，以最大限度地提高客户体验和控制机会成本。我们将问题制定为具有统一效用函数的贝叶斯最优序贯决策问题。我们广泛讨论了实际设计选择和考虑因素。我们进一步介绍了如何通过强化学习解决最优决策规则，并扩展了该解决方案。

    With the growing needs of online A/B testing to support the innovation in industry, the opportunity cost of running an experiment becomes non-negligible. Therefore, there is an increasing demand for an efficient continuous monitoring service that allows early stopping when appropriate. Classic statistical methods focus on hypothesis testing and are mostly developed for traditional high-stake problems such as clinical trials, while experiments at online service companies typically have very different features and focuses. Motivated by the real needs, in this paper, we introduce a novel framework that we developed in Amazon to maximize customer experience and control opportunity cost. We formulate the problem as a Bayesian optimal sequential decision making problem that has a unified utility function. We discuss extensively practical design choices and considerations. We further introduce how to solve the optimal decision rule via Reinforcement Learning and scale the solution. We show th
    
[^6]: 小批量$k$-means聚类法在$O(d/\epsilon)$次迭代内终止。(arXiv:2304.00419v1 [cs.LG])

    Mini-batch $k$-means terminates within $O(d/\epsilon)$ iterations. (arXiv:2304.00419v1 [cs.LG])

    [http://arxiv.org/abs/2304.00419](http://arxiv.org/abs/2304.00419)

    小批量$k$-means聚类法被证明可以在一定迭代次数内收敛，无论初始聚类中心如何，可以达到与完全批量版本相同的逼近比，具有一定的可行性。

    

    我们回答了这个问题：“小批量$k$-means（Min-batch $K$-Means）的局部进展（在批处理上）是否意味着全局进展（在整个数据集上）？”我们考虑了仅当在采样批处理的质量改进低于某个阈值时才终止的小批量$k$-means聚类方法。尽管乍一看这个算法可能永远不会执行完，但我们肯定地回答了上述问题，并表明，如果批次大小为$\tilde{\Omega}((d/\epsilon)^2)$，则它必须在$O(d/\epsilon)$次迭代内以高概率终止，其中$d$是输入的维度，$\epsilon$是终止的阈值参数。这一点是有道理的，无论中心如何初始化。当算法使用$k$-means++初始化方案初始化时，它可以实现$O(\log k)$（与完全批量版本相同）的逼近比。最后，我们展示了我们的结果对小批量$k$-means聚类算法的适用性。

    We answer the question: "Does local progress (on batches) imply global progress (on the entire dataset) for mini-batch $k$-means?". Specifically, we consider mini-batch $k$-means which terminates only when the improvement in the quality of the clustering on the sampled batch is below some threshold.  Although at first glance it appears that this algorithm might execute forever, we answer the above question in the affirmative and show that if the batch is of size $\tilde{\Omega}((d/\epsilon)^2)$, it must terminate within $O(d/\epsilon)$ iterations with high probability, where $d$ is the dimension of the input, and $\epsilon$ is a threshold parameter for termination. This is true regardless of how the centers are initialized. When the algorithm is initialized with the $k$-means++ initialization scheme, it achieves an approximation ratio of $O(\log k)$ (the same as the full-batch version).  Finally, we show the applicability of our results to the mini-batch $k$-means algorithm implemented
    
[^7]: 迈向健康AI：大型语言模型也需要治疗师

    Towards Healthy AI: Large Language Models Need Therapists Too. (arXiv:2304.00416v1 [cs.AI])

    [http://arxiv.org/abs/2304.00416](http://arxiv.org/abs/2304.00416)

    SafeguardGPT框架使用心理治疗来纠正AI聊天机器人中的有害行为，改进与人类的对话质量，进而推进健康AI的发展。

    

    近期大型语言模型 (LLM) 的进展带来了功能强大的 AI 聊天机器人，能够参与自然且类似人类的对话。然而，这些聊天机器人可能具有潜在的危害性，表现出操纵、灌输虚假观念和自恋行为。我们定义健康AI为安全、可信和道德的AI。为了创造健康的AI系统，我们提出了SafeguardGPT框架，该框架使用心理治疗来纠正AI聊天机器人中的这些有害行为。该框架涉及四种类型的AI代理：聊天机器人、"用户"、"治疗师"和"评论家"。我们通过模拟社交对话的工作示例展示了SafeguardGPT的有效性。我们的结果表明，该框架能够改进AI聊天机器人和人类之间的对话质量。虽然未来仍需解决几个挑战和方向，但SafeguardGPT为改善AI聊天机器人与人类价值观之间的协调提供了一种有前途的方法。

    Recent advances in large language models (LLMs) have led to the development of powerful AI chatbots capable of engaging in natural and human-like conversations. However, these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to be safe, trustworthy and ethical. To create healthy AI systems, we present the SafeguardGPT framework that uses psychotherapy to correct for these harmful behaviors in AI chatbots. The framework involves four types of AI agents: a Chatbot, a "User," a "Therapist," and a "Critic." We demonstrate the effectiveness of SafeguardGPT through a working example of simulating a social conversation. Our results show that the framework can improve the quality of conversations between AI chatbots and humans. Although there are still several challenges and directions to be addressed in the future, SafeguardGPT provides a promising approach to improving the alignment between AI chatbots and human value
    
[^8]: DiverseVul: 基于深度学习漏洞检测的新漏洞源代码数据集

    DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection. (arXiv:2304.00409v1 [cs.CR])

    [http://arxiv.org/abs/2304.00409](http://arxiv.org/abs/2304.00409)

    这个论文提供了一个包含150个CWE的新的漏洞源代码数据集，覆盖比以前所有数据集加起来多305个项目。作者通过增加训练数据的多样性和数量改进了深度学习模型在漏洞检测方面的表现。通过结合已有数据集，作者还研究了使用深度学习检测软件漏洞的挑战和前途的分析，结果表明目前深度学习检测漏洞仍存在高误报率，低F1分数和难以检测严重CWE的问题。

    

    我们提出并发布了一个新的漏洞源代码数据集。我们通过爬取安全问题网站，提取相应项目的漏洞修复提交和源代码，筛选出了这个数据集。我们的新数据集包含了150个CWE，26,635个易受攻击的函数和352,606个不易受攻击的函数，提取自7,861个提交。我们的数据集覆盖了比以前所有数据集加起来多305个项目。我们展示了增加训练数据的多样性和数量可以提高深度学习模型在漏洞检测方面的表现。结合我们的新数据集和以前的数据集，我们提出了使用深度学习检测软件漏洞的挑战和有前途的研究方向的分析。我们研究了11个模型架构，属于4个家族。我们的结果表明，由于高误报率，低F1分数和难以检测严重CWE，深度学习仍未准备好用于漏洞检测。特别是，我们展示了......

    We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 150 CWEs, 26,635 vulnerable functions, and 352,606 non-vulnerable functions extracted from 7,861 commits. Our dataset covers 305 more projects than all previous datasets combined. We show that increasing the diversity and volume of training data improves the performance of deep learning models for vulnerability detection.  Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonst
    
[^9]: 混合交通中的连接自动汽车：学习人类驾驶员行为以实现出入匝道的有效合并

    Connected and Automated Vehicles in Mixed-Traffic: Learning Human Driver Behavior for Effective On-Ramp Merging. (arXiv:2304.00397v1 [cs.LG])

    [http://arxiv.org/abs/2304.00397](http://arxiv.org/abs/2304.00397)

    本文介绍了一种学习人类驾驶员行为以实现出入匝道的有效合并的方法，通过学习CAV和HDV互动的近似信息状态模型，使得在混合交通条件下实现安全且高效的合并。

    

    混合交通条件下的公路合流场景对于与进入出入口的人类驾驶车辆（HDV）交互的连接自动汽车（CAV）而言，存在着显着的建模和控制挑战。本文提出了一种方法，通过学习CAV和HDV互动的近似信息状态模型，使CAV在公路合流时安全驾驶。在我们的方法中，CAV会在生成控制策略来促进合并之前，使用近似信息状态学习进入HDV的行为。首先，我们使用实际世界数据验证了这种框架的功效，通过在从Next-Generation Simulation存储库中提取的混合交通情况下预测HDV的行为。然后，我们使用标准的反强化学习方法为公路合流场景生成HDV-CAV交互的模拟数据。在不假设生成模型的先验知识的情况下，我们展示了我们的近似信息状态模型使得在混合交通条件下实现安全且高效的合并。

    Highway merging scenarios featuring mixed traffic conditions pose significant modeling and control challenges for connected and automated vehicles (CAVs) interacting with incoming on-ramp human-driven vehicles (HDVs). In this paper, we present an approach to learn an approximate information state model of CAV-HDV interactions for a CAV to maneuver safely during highway merging. In our approach, the CAV learns the behavior of an incoming HDV using approximate information states before generating a control strategy to facilitate merging. First, we validate the efficacy of this framework on real-world data by using it to predict the behavior of an HDV in mixed traffic situations extracted from the Next-Generation Simulation repository. Then, we generate simulation data for HDV-CAV interactions in a highway merging scenario using a standard inverse reinforcement learning approach. Without assuming a prior knowledge of the generating model, we show that our approximate information state mod
    
[^10]: 使用时间卷积网络管理无服务器云中的冷启动

    Managing Cold-start in The Serverless Cloud with Temporal Convolutional Networks. (arXiv:2304.00396v1 [cs.DC])

    [http://arxiv.org/abs/2304.00396](http://arxiv.org/abs/2304.00396)

    无服务器云的创新模式提供了许多优势，但面临着冷启动问题，本文通过使用时间卷积网络提出的策略可以在基础结构和功能层面上解决此问题。

    

    无服务器云是一种创新的云服务模型，可以使客户免除大部分云管理职责。它还提供了与其他云模型相同的优势，但成本要低得多。因此，无服务器云已经越来越多地应用于系统安全、银行和医疗保健等高影响领域。无服务器云性能面临的一个重大威胁是冷启动，当为了为客户请求提供所需的云资源的时间造成的成本对于服务提供商和/或客户来说是不可接受的。本文提出了一个新的低耦合、高内聚的集合策略，可以在无服务器云栈的基础结构和功能层面上解决冷启动问题，而现有技术的策略则更加狭窄。

    Serverless cloud is an innovative cloud service model that frees customers from most cloud management duties. It also offers the same advantages as other cloud models but at much lower costs. As a result, the serverless cloud has been increasingly employed in high-impact areas such as system security, banking, and health care. A big threat to the serverless cloud's performance is cold-start, which is when the time of provisioning the needed cloud resource to serve customers' requests incurs unacceptable costs to the service providers and/or the customers. This paper proposes a novel low-coupling, high-cohesion ensemble policy that addresses the cold-start problem at infrastructure- and function-levels of the serverless cloud stack, while the state of the art policies have a more narrowed focus. This ensemble policy anchors on the prediction of function instance arrivals, 10 to 15 minutes into the future. It is achievable by using the temporal convolutional network (TCN) deep-learning m
    
[^11]: 通过相似性结构解析对比学习机制：理论分析

    Towards Understanding the Mechanism of Contrastive Learning via Similarity Structure: A Theoretical Analysis. (arXiv:2304.00395v1 [cs.LG])

    [http://arxiv.org/abs/2304.00395](http://arxiv.org/abs/2304.00395)

    通过一个新的公式，本文理论分析了基于核的对比学习损失的特点，证明了它能描述学习表示的结构和表现，提供一个新的限制方法，并在多个基准测试中验证其有效性。

    

    对比学习是一种有效的自监督表示学习方法。虽然近期的研究在理论上对对比学习有了一定的了解，但对于如何表征学习表示的聚类仍然有限。本文旨在从理论角度阐明这种聚类的特征。为此，我们考虑一种基于核的对比学习框架，称为核对比学习（KCL），核函数在将我们的理论结果应用于其他框架时起重要作用。我们利用统计依赖观点引入一个学习表示的相似性结构的公式。我们通过这个公式研究了基于核的对比损失的理论性质。我们首先证明这个公式表征了利用核对比学习框架学习的表示结构。我们证明了一个新的上界，对于负样本的边际分布有一个温和的条件，期望对比损失受到限制。此外，我们还确定了基于核的对比损失是一种新的信息论下界的特例，这促使我们开发一个新的目标，可以进一步约束学习表示在输入的亲密性和不同样本之间的可分性方面。最后，我们在几个基准测试上进行了实验，这些实验支持我们的理论发现，并表明了所提出的目标的有效性。

    Contrastive learning is an efficient approach to self-supervised representation learning. Although recent studies have made progress in the theoretical understanding of contrastive learning, the investigation of how to characterize the clusters of the learned representations is still limited. In this paper, we aim to elucidate the characterization from theoretical perspectives. To this end, we consider a kernel-based contrastive learning framework termed Kernel Contrastive Learning (KCL), where kernel functions play an important role when applying our theoretical results to other frameworks. We introduce a formulation of the similarity structure of learned representations by utilizing a statistical dependency viewpoint. We investigate the theoretical properties of the kernel-based contrastive loss via this formulation. We first prove that the formulation characterizes the structure of representations learned with the kernel-based contrastive learning framework. We show a new upper boun
    
[^12]: 参数化PDE的多级CNN

    Multilevel CNNs for Parametric PDEs. (arXiv:2304.00388v1 [cs.LG])

    [http://arxiv.org/abs/2304.00388](http://arxiv.org/abs/2304.00388)

    该论文提出了一种用于有效数值解决参数化PDEs的多级CNN方法，有实质性的改进并能以任意精度近似多重网格V循环。

    

    我们将部分微分方程（PDEs）的多级求解器的概念与基于神经网络的深度学习相结合，提出一种新的解决高维参数PDEs的有效数值方法。理论分析表明，该架构能够以任意精度近似多重网格V循环，其权重数量仅与最细网格的分辨率对数有关，这种方法有实质性的改进。

    We combine concepts from multilevel solvers for partial differential equations (PDEs) with neural network based deep learning and propose a new methodology for the efficient numerical solution of high-dimensional parametric PDEs. An in-depth theoretical analysis shows that the proposed architecture is able to approximate multigrid V-cycles to arbitrary precision with the number of weights only depending logarithmically on the resolution of the finest mesh. As a consequence, approximation bounds for the solution of parametric PDEs by neural networks that are independent on the (stochastic) parameter dimension can be derived. The performance of the proposed method is illustrated on high-dimensional parametric linear elliptic PDEs that are common benchmark problems in uncertainty quantification. We find substantial improvements over state-of-the-art deep learning-based solvers. As particularly challenging examples, random conductivity with high-dimensional non-affine Gaussian fields in 10
    
[^13]: 让对话继续：使用ChatGPT仅以0.42美元的价格修复了337个漏洞中的162个

    Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. (arXiv:2304.00385v1 [cs.SE])

    [http://arxiv.org/abs/2304.00385](http://arxiv.org/abs/2304.00385)

    提出了一个名为ChatRepair的新型自动程序修复方法，与传统的“生成和验证”范式不同，它能够通过对话风格实现即时反馈，从而显着提高漏洞修复的效率和补丁的准确性。

    

    自动程序修复（APR）旨在自动生成有关有漏洞程序的修补程序。最近的APR工作集中于利用现代的大型语言模型（LLMs）直接生成APR的补丁。这种基于LLM的APR工具的工作方法是首先构建一个由原始有漏洞代码构建的输入提示，然后查询LLM生成补丁。虽然基于LLM的APR工具能够实现最先进的结果，但它仍然遵循“生成和验证”修复范式，即首先生成大量的补丁，然后逐个验证每个补丁。这不仅会导致许多重复的不正确的补丁，而且还会错过测试失败中的关键信息以及可行的补丁信息。为了解决这些局限性，我们提出了ChatRepair，这是第一种完全自动化的对话驱动的APR方法，它将补丁生成与即时反馈交替进行，以以对话风格执行APR。ChatRepair首先将相关的测试失败信息馈入LLM中，然后在补丁生成过程中使用交互式对话，以集中方式生成补丁。此外，ChatRepair还利用了测试结果中的关键信息，以生成更好的补丁。

    Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then queries the LLM to generate patches. While the LLM-based APR tools are able to achieve state-of-the-art results, it still follows the classic Generate and Validate repair paradigm of first generating lots of patches and then validating each one afterwards. This not only leads to many repeated patches that are incorrect but also miss the crucial information in test failures as well as in plausible patches.  To address these limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failur
    
[^14]: 3D复合几何变换在知识图谱嵌入中的应用

    Knowledge Graph Embedding with 3D Compound Geometric Transformations. (arXiv:2304.00378v1 [cs.AI])

    [http://arxiv.org/abs/2304.00378](http://arxiv.org/abs/2304.00378)

    本文提出了一种基于3D复合几何变换的知识图谱嵌入模型CompoundE3D，在链接预测方面具有良好的性能表现。

    

    本文提出了一种名为CompoundE3D的知识图谱嵌入模型，它利用了包括平移、旋转、缩放、反射和剪切在内的3D复合几何变换。CompoundE3D允许多个设计变体以匹配知识图谱的底层特征，并能产生超出单个变体的优越性能。该方法在四个流行的链接预测数据集上得到了验证。

    The cascade of 2D geometric transformations were exploited to model relations between entities in a knowledge graph (KG), leading to an effective KG embedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space was proposed as a new KGE model, Rotate3D, by leveraging its non-commutative property. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear and propose a family of KGE models, named CompoundE3D, in this work. CompoundE3D allows multiple design variants to match rich underlying characteristics of a KG. Since each variant has its own advantages on a subset of relations, an ensemble of multiple variants can yield superior performance. The effectiveness and flexibility of CompoundE3D are experimentally verified on four popular link prediction datasets.
    
[^15]: 个性化情感计算在人机交互中的调查

    A Survey on Personalized Affective Computing in Human-Machine Interaction. (arXiv:2304.00377v1 [cs.HC])

    [http://arxiv.org/abs/2304.00377](http://arxiv.org/abs/2304.00377)

    论文调查了情感计算中的个性化方法，将其分为七类，并给出了调查文献的统计元分析。

    

    在计算机领域中，个性化的目的是通过优化一个或多个性能指标并遵守特定约束条件来训练迎合特定个人或人群的模型。本文讨论了情感和人格计算（以下简称情感计算）中个性化的必要性，并对情感计算中个性化的最新方法进行了调查。我们的调查涵盖了训练技术和目标，以实现情感计算模型的个性化定制。我们将现有的方法分为七类：（1）面向特定目标的模型，（2）面向特定群体的模型，（3）基于加权的方法，（4）微调方法，（5）多任务学习，（6）生成式模型和（7）特征增强。此外，我们提供了对调查文献的统计元分析，分析了不同情感计算任务、交互模式、交互上下文以及所涉及领域的普遍性。

    In computing, the aim of personalization is to train a model that caters to a specific individual or group of people by optimizing one or more performance metrics and adhering to specific constraints. In this paper, we discuss the need for personalization in affective and personality computing (hereinafter referred to as affective computing). We present a survey of state-of-the-art approaches for personalization in affective computing. Our review spans training techniques and objectives towards the personalization of affective computing models. We group existing approaches into seven categories: (1) Target-specific Models, (2) Group-specific Models, (3) Weighting-based Approaches, (4) Fine-tuning Approaches, (5) Multitask Learning, (6) Generative-based Models, and (7) Feature Augmentation. Additionally, we provide a statistical meta-analysis of the surveyed literature, analyzing the prevalence of different affective computing tasks, interaction modes, interaction contexts, and the leve
    
[^16]: 基于物理知识的机器学习在移动荷载问题中的应用

    Physics-informed machine learning for moving load problems. (arXiv:2304.00369v1 [cs.LG])

    [http://arxiv.org/abs/2304.00369](http://arxiv.org/abs/2304.00369)

    论文提出了使用基于物理知识的机器学习方法模拟移动荷载问题，其中使用高斯函数来逼近Dirac delta函数以提高模型收敛性。

    

    本文提出了一种新的方法，使用基于物理知识的机器学习(PIML)来模拟移动荷载的正逆问题。物理信息神经网络(PINNs)利用移动荷载问题的基础物理学，旨在预测梁的偏转和荷载大小。考虑到荷载移动的数学表示包括一个Dirac delta函数，以捕获荷载横跨结构的效应。我们提议使用高斯函数来逼近Dirac delta函数。将内含高斯函数的物理方程用于物理信息神经结构，可模拟梁的偏转并预测荷载大小。数值结果表明，PIML是模拟移动荷载正问题和逆问题的有效方法，并且使用高斯函数逼近Dirac delta函数可以提高损失函数的收敛性。

    This paper presents a new approach to simulate forward and inverse problems of moving loads using physics-informed machine learning (PIML). Physics-informed neural networks (PINNs) utilize the underlying physics of moving load problems and aim to predict the deflection of beams and the magnitude of the loads. The mathematical representation of the moving load considered in this work involves a Dirac delta function, to capture the effect of the load moving across the structure. Approximating the Dirac delta function with PINNs is challenging because of its instantaneous change of output at a single point, causing difficulty in the convergence of the loss function. We propose to approximate the Dirac delta function with a Gaussian function. The incorporated Gaussian function physical equations are used in the physics-informed neural architecture to simulate beam deflections and to predict the magnitude of the load. Numerical results show that PIML is an effective method for simulating th
    
[^17]: 领域专家关键状态自适应失效搜索法

    Adaptive Failure Search Using Critical States from Domain Experts. (arXiv:2304.00365v1 [cs.RO])

    [http://arxiv.org/abs/2304.00365](http://arxiv.org/abs/2304.00365)

    本文提出了一种基于领域专家的自适应失效搜索方法，用于有效地探索和发现自主策略在仿真中的失效轨迹。

    

    发现潜在故障是验证安全关键系统（如自动驾驶汽车）的重要步骤。由于故障事件的稀少性，使用随机搜索方法需要耗费大量的时间来找到潜在的系统弱点。因此，提出了自适应搜索技术来有效地探索和发现自主策略在仿真中的失效轨迹。本文提出了一种基于领域专家的自适应失效搜索方法。

    Uncovering potential failure cases is a crucial step in the validation of safety critical systems such as autonomous vehicles. Failure search may be done through logging substantial vehicle miles in either simulation or real world testing. Due to the sparsity of failure events, naive random search approaches require significant amounts of vehicle operation hours to find potential system weaknesses. As a result, adaptive searching techniques have been proposed to efficiently explore and uncover failure trajectories of an autonomous policy in simulation. Adaptive Stress Testing (AST) is one such method that poses the problem of failure search as a Markov decision process and uses reinforcement learning techniques to find high probability failures. However, this formulation requires a probability model for the actions of all agents in the environment. In systems where the environment actions are discrete and dependencies among agents exist, it may be infeasible to fully characterize the d
    
[^18]: 离线元强化学习中任务表示学习中的上下文分布偏移问题

    On Context Distribution Shift in Task Representation Learning for Offline Meta RL. (arXiv:2304.00354v1 [cs.LG])

    [http://arxiv.org/abs/2304.00354](http://arxiv.org/abs/2304.00354)

    该论文探讨了离线元强化学习中任务表示学习中遇到的上下文分布偏移问题，并提出了一种硬采样的策略用于解决该问题，实验结果表明该方法能够得到更强健的任务表示和更好的测试性能。

    

    离线元强化学习（OMRL）旨在从离线数据集中学习可转移知识，以促进新目标任务的学习过程。基于上下文的RL采用上下文编码器，通过推断任务表示来快速适应新任务，然后根据推断出的任务表示调整行动策略。在这里，我们考虑基于上下文的OMRL，特别是OMRL中的任务表示学习问题。我们经验性地证明，基于离线数据集训练的上下文编码器可能会遭受训练和测试时使用上下文之间的分布偏移。为了解决这个问题，我们提出了一种基于硬采样的策略，用于学习一个强健的任务上下文编码器。基于不同的连续控制任务的实验结果表明，我们的技术的利用导致更强健的任务表示和更好的测试性能，累积回报比基准方法好。

    Offline meta reinforcement learning (OMRL) aims to learn transferrable knowledge from offline datasets to facilitate the learning process for new target tasks. Context-based RL employs a context encoder to rapidly adapt the agent to new tasks by inferring about the task representation, and then adjusting the acting policy based on the inferred task representation. Here we consider context-based OMRL, in particular, the issue of task representation learning for OMRL. We empirically demonstrate that the context encoder trained on offline datasets could suffer from distribution shift between the contexts used for training and testing. To tackle this issue, we propose a hard sampling based strategy for learning a robust task context encoder. Experimental results, based on distinct continuous control tasks, demonstrate that the utilization of our technique results in more robust task representations and better testing performance in terms of accumulated returns, compared with baseline metho
    
[^19]: 学习增强可扩展的网格物理学代理的科学计算算法

    Scientific Computing Algorithms to Learn Enhanced Scalable Surrogates for Mesh Physics. (arXiv:2304.00338v1 [cs.LG])

    [http://arxiv.org/abs/2304.00338](http://arxiv.org/abs/2304.00338)

    通过领域分解的方法扩展网格上的MeshGraphNets，了解了如何训练一个可扩展的基于网格的图神经网络代理，进而生成计算流体动力学模拟，同时还展示了如何通过高阶数值积分来增强该代理。

    

    数据驱动的建模方法能够生成用于研究大规模物理问题的快速代理。其中，操作基于网格的图神经网络（GNN）是理想的选择，因为它们具有促进物理忠实度的归纳偏差，但硬件限制阻止了它们在大型计算域的应用。我们展示了在3D网格上培训一类GNN代理的可能性。我们通过领域分解的方法扩展了MeshGraphNets（MGN）（一种用于基于网格的物理建模的GNN子类），以便在某些条件下数学上等同于在整个域上进行培训。利用这种方法，我们能够在拥有数百万个节点的网格上对MGN进行培训，生成计算流体动力学（CFD）模拟。此外，我们展示了如何通过高阶数值积分来增强MGN，从而能够减少MGN的误差和培训时间。我们在一个附带的三维网格数据集上验证了我们的方法。

    Data-driven modeling approaches can produce fast surrogates to study large-scale physics problems. Among them, graph neural networks (GNNs) that operate on mesh-based data are desirable because they possess inductive biases that promote physical faithfulness, but hardware limitations have precluded their application to large computational domains. We show that it is \textit{possible} to train a class of GNN surrogates on 3D meshes. We scale MeshGraphNets (MGN), a subclass of GNNs for mesh-based physics modeling, via our domain decomposition approach to facilitate training that is mathematically equivalent to training on the whole domain under certain conditions. With this, we were able to train MGN on meshes with \textit{millions} of nodes to generate computational fluid dynamics (CFD) simulations. Furthermore, we show how to enhance MGN via higher-order numerical integration, which can reduce MGN's error and training time. We validated our methods on an accompanying dataset of 3D $\te
    
[^20]: 双重随机模型：无偏标签噪声的学习与推理稳定

    Doubly Stochastic Models: Learning with Unbiased Label Noises and Inference Stability. (arXiv:2304.00320v1 [cs.LG])

    [http://arxiv.org/abs/2304.00320](http://arxiv.org/abs/2304.00320)

    本文探讨了无偏标签噪声的隐含正则化效应，提出了一种将SGD的动态建模为双重随机模型的方法，可提高鲁棒性和泛化性能。

    

    随机标签噪声广泛存在于实际机器学习环境中。我们的工作旨在研究标签噪声的隐含正则化效应，假设标签噪声是无偏的，分析了SGD在无偏标签噪声下的学习动态，并将SGD的动态建模为具有两个扩散项的随机可微方程（即双重随机模型）。我们的理论分析发现，这种隐含正则化可以在学到的模型上实施双重随机结构，从而提高鲁棒性和泛化性能。在合成和真实数据集上的实证验证表明，在各种类型和水平的标签噪声下，我们的方法可以实现最先进的性能。

    Random label noises (or observational noises) widely exist in practical machine learning settings. While previous studies primarily focus on the affects of label noises to the performance of learning, our work intends to investigate the implicit regularization effects of the label noises, under mini-batch sampling settings of stochastic gradient descent (SGD), with assumptions that label noises are unbiased. Specifically, we analyze the learning dynamics of SGD over the quadratic loss with unbiased label noises, where we model the dynamics of SGD as a stochastic differentiable equation (SDE) with two diffusion terms (namely a Doubly Stochastic Model). While the first diffusion term is caused by mini-batch sampling over the (label-noiseless) loss gradients as many other works on SGD, our model investigates the second noise term of SGD dynamics, which is caused by mini-batch sampling over the label noises, as an implicit regularizer. Our theoretical analysis finds such implicit regulariz
    
[^21]: 医疗病理预测：系统性综述和提出的方法

    Medical Pathologies Prediction : Systematic Review and Proposed Approach. (arXiv:2304.00311v1 [cs.LG])

    [http://arxiv.org/abs/2304.00311](http://arxiv.org/abs/2304.00311)

    该论文分析了利用最新技术改善医疗保健，提出了一般方法，通过收集、预处理和聚类医学数据，能够让患者和医疗专业人员在显着的时间范围内更精准地预测最常见的病理情况。

    

    医疗保健部门是每个社区的重要支柱，为了优化医疗流程、提高护理质量，方便患者管理，进行了大量的研究。本文分析和研究了不同的工作，涉及到利用最新技术（如大数据、人工智能、机器学习和深度学习）来改善医疗保健，使我们能够提出我们的一般方法，集中在收集、预处理和聚类医学数据以便在显着的时间范围内，经过分析，让患者和医疗专业人员能够更精准地预测最常见的病理情况。关键词：医疗保健、大数据、人工智能、自然语言处理、数据挖掘、预测模型。

    The healthcare sector is an important pillar of every community, numerous research studies have been carried out in this context to optimize medical processes and improve care quality and facilitate patient management. In this article we have analyzed and examined different works concerning the exploitation of the most recent technologies such as big data, artificial intelligence, machine learning, and deep learning for the improvement of health care, which enabled us to propose our general approach concentrating on the collection, preprocessing and clustering of medical data to facilitate access, after analysis, to the patients and health professionals to predict the most frequent pathologies with better precision within a notable timeframe.  keywords: Healthcare, big data, artificial intelligence, automatic language processing, data mining, predictive models.
    
[^22]: 预测不确定性：度量与应用

    Predictive Heterogeneity: Measures and Applications. (arXiv:2304.00305v1 [cs.LG])

    [http://arxiv.org/abs/2304.00305](http://arxiv.org/abs/2304.00305)

    本文研究了数据的异质性对机器学习模型预测的影响，并提出了可用的预测不确定性，可从有限数据中可靠地估计。我们设计了一个双层优化算法来探索可用的预测不确定性，从而为子人群划分提供洞见。

    

    作为大数据的内在和基本属性，数据的异质性存在于各种真实世界的应用中，如精准医学、自动驾驶、金融应用等。对于机器学习算法而言，忽略数据的异质性会极大地损害泛化性能和算法公平性，因为不同子人群之间的预测机制可能会存在差异。本文关注影响机器学习模型预测的数据异质性，并首次提出了“可用预测不确定性”，该方法考虑了模型容量和计算约束。证明了它可以从有限数据中可靠地估计，并且具有可信的正确性(PAC)范围。此外，我们设计了一个双层优化算法来从数据中探索可用的预测不确定性。经验证实，探索出的异质性为子人群划分提供了洞见。

    As an intrinsic and fundamental property of big data, data heterogeneity exists in a variety of real-world applications, such as precision medicine, autonomous driving, financial applications, etc. For machine learning algorithms, the ignorance of data heterogeneity will greatly hurt the generalization performance and the algorithmic fairness, since the prediction mechanisms among different sub-populations are likely to differ from each other. In this work, we focus on the data heterogeneity that affects the prediction of machine learning models, and firstly propose the \emph{usable predictive heterogeneity}, which takes into account the model capacity and computational constraints. We prove that it can be reliably estimated from finite data with probably approximately correct (PAC) bounds. Additionally, we design a bi-level optimization algorithm to explore the usable predictive heterogeneity from data. Empirically, the explored heterogeneity provides insights for sub-population divis
    
[^23]: 公平连续和定向增强组的完美数据增强策略

    Fair-CDA: Continuous and Directional Augmentation for Group Fairness. (arXiv:2304.00295v1 [cs.LG])

    [http://arxiv.org/abs/2304.00295](http://arxiv.org/abs/2304.00295)

    本文提出了一种公平连续和定向增强（Fair-CDA）的细致的数据增强策略，通过正则化模型展示了可以实现组公平性，并通过调整路径方向上的扰动强度实现了可控和可审计的增强方法。实验结果表明，Fair-CDA在广泛使用的基准测试中始终优于最先进的方法，如Adult、CelebA和MovieLens。

    

    本文提出了一种细致的数据增强策略——公平连续和定向增强（Fair-CDA），以实现对公平性约束的实施。我们使用特征解缠方法提取与敏感属性高度相关的特征，然后通过在组之间的敏感特征转换路径上正则化模型，展示了可以实现组公平性。通过调整路径方向上的扰动强度，我们的提出的增强方法是可控和可审计的。为了缓解公平性约束导致的准确率下降，我们进一步引入了一个校准模型来为增强数据填补标签。我们的方法不假设任何数据生成模型，并确保对准确性和公平性实现良好的泛化。实验结果表明，Fair-CDA在广泛使用的基准测试中始终优于最先进的方法，如Adult、CelebA和MovieLens。特别是，Fair-CDA在公平方面获得了86.3％的相对改善，同时保持了准确性。

    In this work, we propose {\it Fair-CDA}, a fine-grained data augmentation strategy for imposing fairness constraints. We use a feature disentanglement method to extract the features highly related to the sensitive attributes. Then we show that group fairness can be achieved by regularizing the models on transition paths of sensitive features between groups. By adjusting the perturbation strength in the direction of the paths, our proposed augmentation is controllable and auditable. To alleviate the accuracy degradation caused by fairness constraints, we further introduce a calibrated model to impute labels for the augmented data. Our proposed method does not assume any data generative model and ensures good generalization for both accuracy and fairness. Experimental results show that Fair-CDA consistently outperforms state-of-the-art methods on widely-used benchmarks, e.g., Adult, CelebA and MovieLens. Especially, Fair-CDA obtains an 86.3\% relative improvement for fairness while maint
    
[^24]: BioSequence2Vec: 生物序列有效嵌入生成的方法

    BioSequence2Vec: Efficient Embedding Generation For Biological Sequences. (arXiv:2304.00291v1 [cs.LG])

    [http://arxiv.org/abs/2304.00291](http://arxiv.org/abs/2304.00291)

    BioSequence2Vec是一种通用的、高效的生物序列嵌入生成方法，能够有效处理计算时间和存储大量核矩阵的问题。

    

    表示学习是机器学习流程中的一个重要步骤。由于生物序列数据的数量庞大，因此学习明确表示是困难的。核方法，如SVM，是用于多个机器学习任务的有效替代方法，例如序列分类。然而核方法有三个挑战，分别为计算时间，存储大量的核矩阵，难以推广到非核分类器。本文提出了一种通用的生物序列有效嵌入生成的方法BioSequence2Vec，以应对上述挑战。

    Representation learning is an important step in the machine learning pipeline. Given the current biological sequencing data volume, learning an explicit representation is prohibitive due to the dimensionality of the resulting feature vectors. Kernel-based methods, e.g., SVM, are a proven efficient and useful alternative for several machine learning (ML) tasks such as sequence classification. Three challenges with kernel methods are (i) the computation time, (ii) the memory usage (storing an $n\times n$ matrix), and (iii) the usage of kernel matrices limited to kernel-based ML methods (difficult to generalize on non-kernel classifiers). While (i) can be solved using approximate methods, challenge (ii) remains for typical kernel methods. Similarly, although non-kernel-based ML methods can be applied to kernel matrices by extracting principal components (kernel PCA), it may result in information loss, while being computationally expensive. In this paper, we propose a general-purpose repre
    
[^25]: 使用机器学习进行被动式光纤网络中的分支识别

    Branch Identification in Passive Optical Networks using Machine Learning. (arXiv:2304.00285v1 [cs.LG])

    [http://arxiv.org/abs/2304.00285](http://arxiv.org/abs/2304.00285)

    本文提出利用机器学习改善被动式光纤网络监测，具有高的诊断准确率和定位精度。

    

    本文提出并实验验证了一种利用机器学习来改善几乎等距分支的被动式光纤网络中监测的方法。它实现了高达98.7％的诊断准确度和0.5m的事件定位误差。

    A machine learning approach for improving monitoring in passive optical networks with almost equidistant branches is proposed and experimentally validated. It achieves a high diagnostic accuracy of 98.7% and an event localization error of 0.5m
    
[^26]: 物联网数据隐私保护

    Data Privacy Preservation on the Internet of Things. (arXiv:2304.00258v1 [cs.CR])

    [http://arxiv.org/abs/2304.00258](http://arxiv.org/abs/2304.00258)

    物联网的普及带来大量数据，对用户的隐私安全构成威胁，本文综述了物联网领域数据隐私保护的现有方案。

    

    硬件和信息技术的最新发展使得全球数十亿个智能设备相互连接并交换信息，人类的介入 minimal。这个被称为物联网（IoT）的范式正在迅速发展，预计到2025年将有270亿个设备。这种增长导致了大量数据的产生，而这也引发了对用户隐私不断增长的关注。本文综述了物联网领域研究人员提出的一些现有数据隐私保护方案。

    Recent developments in hardware and information technology have enabled the emergence of billions of connected, intelligent devices around the world exchanging information with minimal human involvement. This paradigm, known as the Internet of Things (IoT) is progressing quickly with an estimated 27 billion devices by 2025. This growth in the number of IoT devices and successful IoT services has generated a tremendous amount of data. However, this humongous volume of data poses growing concerns for user privacy. This introductory chapter has presented a brief survey of some of the existing data privacy-preservation schemes proposed by researchers in the field of the Internet of Things.
    
[^27]: RL中的反向攻击保护：恢复触发状态方法

    Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning. (arXiv:2304.00252v1 [cs.LG])

    [http://arxiv.org/abs/2304.00252](http://arxiv.org/abs/2304.00252)

    本文提出了恢复触发状态(RTS)方法，用于保护RL代理免受反向攻击。该方法涉及构建替代网络来近似动态模型，并将触发状态恢复为干净状态来防止攻击者通过触发器激活隐藏在代理中的后门。

    

    反向攻击可以使恶意用户操纵环境或破坏训练数据，并将一个隐藏的后门插入到训练代理程序中。这种攻击危及RL系统的可靠性，在各个关键领域可能会造成灾难性的影响。与此相比，对于RL中的反向攻击有效的防御措施的研究相对较少。本文提出了一种新颖的方法——恢复触发状态(RTS)，能够有效地保护受害代理免受反向攻击。 RTS需要构建一个替代网络来近似动态模型。开发人员可以通过将触发状态恢复为干净状态来防止攻击者通过触发器激活代理中隐藏的后门。在训练替代网络来预测状态时，我们将代理动作信息并入，减少代理在预测状态上采取的动作和实际状态上采取的动作之间的差异。

    A backdoor attack allows a malicious user to manipulate the environment or corrupt the training data, thus inserting a backdoor into the trained agent. Such attacks compromise the RL system's reliability, leading to potentially catastrophic results in various key fields. In contrast, relatively limited research has investigated effective defenses against backdoor attacks in RL. This paper proposes the Recovery Triggered States (RTS) method, a novel approach that effectively protects the victim agents from backdoor attacks. RTS involves building a surrogate network to approximate the dynamics model. Developers can then recover the environment from the triggered state to a clean state, thereby preventing attackers from activating backdoors hidden in the agent by presenting the trigger. When training the surrogate to predict states, we incorporate agent action information to reduce the discrepancy between the actions taken by the agent on predicted states and the actions taken on real sta
    
[^28]: 从概念到部署：基于机器学习和性能评估的智能卒中预测框架

    From Conception to Deployment: Intelligent Stroke Prediction Framework using Machine Learning and Performance Evaluation. (arXiv:2304.00249v1 [cs.LG])

    [http://arxiv.org/abs/2304.00249](http://arxiv.org/abs/2304.00249)

    本文提出了一个基于机器学习和性能评估的智能卒中预测框架，并比较了五种常用的机器学习算法，结果显示随机森林算法最适合卒中预测。

    

    卒中是全球第二大死因。机器学习分类算法被广泛用于卒中预测，但是这些算法使用不同的数据集和评估指标进行评估。此外，目前没有针对卒中数据分析的综合框架。本文通过对文献中常用的机器学习预测算法的重要性分析提出了一个智能卒中预测框架。对卒中预测中五个最常用的机器学习算法进行了比较，并使用统一的设置进行客观比较。比较分析和数字结果显示，随机森林算法最适合卒中预测。

    Stroke is the second leading cause of death worldwide. Machine learning classification algorithms have been widely adopted for stroke prediction. However, these algorithms were evaluated using different datasets and evaluation metrics. Moreover, there is no comprehensive framework for stroke data analytics. This paper proposes an intelligent stroke prediction framework based on a critical examination of machine learning prediction algorithms in the literature. The five most used machine learning algorithms for stroke prediction are evaluated using a unified setup for objective comparison. Comparative analysis and numerical results reveal that the Random Forest algorithm is best suited for stroke prediction.
    
[^29]: 重启贝叶斯在线变点检测用于非平稳马尔科夫决策过程

    Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes. (arXiv:2304.00232v1 [cs.LG])

    [http://arxiv.org/abs/2304.00232](http://arxiv.org/abs/2304.00232)

    该论文介绍了一种针对非平稳强化学习环境的算法，称为R-BOCPD-UCRL2，它使用了一种重新启动的贝叶斯在线变点检测算法（R-BOCPD），并在多项式分布采样的MDP上提供了较优秀的性能保证。

    

    我们考虑在一个非平稳的强化学习（RL）环境中进行学习的问题，其中该设置可以被完全描述为分段平稳的离散时间马尔科夫决策过程（MDP）。我们引入了一种重新启动的贝叶斯在线变点检测算法（R-BOCPD）变体，该算法适用于从更一般的多项式分布中生成的输入流，并在误警率和检测延迟方面提供接近最优的理论保证。基于此，我们提出了一种针对从多项式分布中采样的状态转移内核的MDPs的改进版本UCRL2算法，我们称之为R-BOCPD-UCRL2。我们进行了有限时间的性能分析，并表明R-BOCPD-UCRL2具有有利的遗憾界的$O\left(D O \sqrt{A T K_T \log\left (\frac{T}{\delta} \right) + \frac{K_T \log \frac{K_T}{\delta}}{\min\limits_\ell \: \mathbf{KL}\left( {\mathbf{\theta}^{(\ell+1)}}\mid\mid{\mathbf{\theta}^{(\ell)}}\right)}}\right)$。

    We consider the problem of learning in a non-stationary reinforcement learning (RL) environment, where the setting can be fully described by a piecewise stationary discrete-time Markov decision process (MDP). We introduce a variant of the Restarted Bayesian Online Change-Point Detection algorithm (R-BOCPD) that operates on input streams originating from the more general multinomial distribution and provides near-optimal theoretical guarantees in terms of false-alarm rate and detection delay. Based on this, we propose an improved version of the UCRL2 algorithm for MDPs with state transition kernel sampled from a multinomial distribution, which we call R-BOCPD-UCRL2. We perform a finite-time performance analysis and show that R-BOCPD-UCRL2 enjoys a favorable regret bound of $O\left(D O \sqrt{A T K_T \log\left (\frac{T}{\delta} \right) + \frac{K_T \log \frac{K_T}{\delta}}{\min\limits_\ell \: \mathbf{KL}\left( {\mathbf{\theta}^{(\ell+1)}}\mid\mid{\mathbf{\theta}^{(\ell)}}\right)}}\right)$,
    
[^30]: ConvBLS：一种高效的、可增量卷积广义学习系统用于图像分类

    ConvBLS: An Effective and Efficient Incremental Convolutional Broad Learning System for Image Classification. (arXiv:2304.00219v1 [cs.LG])

    [http://arxiv.org/abs/2304.00219](http://arxiv.org/abs/2304.00219)

    ConvBLS是一种卷积广义学习系统，使用球形K-means算法、卷积增强层和多尺度特征融合层可以达到与先进深度学习模型相当甚至更好的性能，在需要更少时间和计算资源的情况下完成训练和预测。

    

    深度学习通常受到巨大计算资源和耗时培训过程的困扰。为缓解这些问题，提出了广义学习系统（BLS）及其卷积变体，并在图像分类方面取得了出色的表现。然而，现有的基于卷积的广义学习系统（C-BLS）要么缺乏高效的训练方法和增量学习能力，要么表现不佳。为此，我们提出了一种基于球形K-means（SKM）算法和两阶段多尺度（TSMS）特征融合的卷积广义学习系统（ConvBLS），包括卷积特征（CF）层、卷积增强（CE）层、TSMS特征融合层和输出层。与现有的C-BLS不同，使用简单而高效的SKM算法来学习CF层的权重。与随机滤波器相比，SKM算法使CF层学习更全面的空间特征。引入CE层通过max-pooling操作合并CF层的激活图来增强深度模型的区分能力。提出了TSMS特征融合来集成多尺度卷积特征，以捕获更丰富的特征表示。最终，输出层采用softmax激活函数获得分类结果。在三个基准数据集上的实验证明，ConvBLS在需要更少的培训时间和计算资源的情况下，可以达到与优秀的深度学习模型相当甚至优秀的性能。

    Deep learning generally suffers from enormous computational resources and time-consuming training processes. Broad Learning System (BLS) and its convolutional variants have been proposed to mitigate these issues and have achieved superb performance in image classification. However, the existing convolutional-based broad learning system (C-BLS) either lacks an efficient training method and incremental learning capability or suffers from poor performance. To this end, we propose a convolutional broad learning system (ConvBLS) based on the spherical K-means (SKM) algorithm and two-stage multi-scale (TSMS) feature fusion, which consists of the convolutional feature (CF) layer, convolutional enhancement (CE) layer, TSMS feature fusion layer, and output layer. First, unlike the current C-BLS, the simple yet efficient SKM algorithm is utilized to learn the weights of CF layers. Compared with random filters, the SKM algorithm makes the CF layer learn more comprehensive spatial features. Second
    
[^31]: 病理图像诊断的跨尺度多实例学习

    Cross-scale Multi-instance Learning for Pathological Image Diagnosis. (arXiv:2304.00216v1 [cs.CV])

    [http://arxiv.org/abs/2304.00216](http://arxiv.org/abs/2304.00216)

    本研究提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中，有效地解决了忽略对人类病理学家诊断至关重要的跨尺度信息的问题。

    

    数字病理学中，跨多个尺度分析高分辨率的全幅图像 (WSIs) 带来了巨大的挑战。多实例学习 (MIL) 是利用分类对象集 (例如较小的图像块集) 对高分辨率图像进行处理的常见方法。然而，这种处理通常在WSIs的单个尺度（例如20倍放大）上进行，忽略了对人类病理学家诊断至关重要的跨尺度信息。在本研究中，我们提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中。本文的贡献有三个方面：(1) 提出了一种新的跨尺度MIL (CS-MIL)算法，它集成了多尺度信息和跨尺度关系；(2) 创建并发布了一个玩具数据集，其中包含尺度特异性形态特征，以检查和可视化不同的跨尺度关系；(3)在四个WSI的细胞肺癌数据集上进行了实验验证CS-MIL的有效性。

    Analyzing high resolution whole slide images (WSIs) with regard to information across multiple scales poses a significant challenge in digital pathology. Multi-instance learning (MIL) is a common solution for working with high resolution images by classifying bags of objects (i.e. sets of smaller image patches). However, such processing is typically performed at a single scale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scale information that is key to diagnoses by human pathologists. In this study, we propose a novel cross-scale MIL algorithm to explicitly aggregate inter-scale relationships into a single MIL network for pathological image diagnosis. The contribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL) algorithm that integrates the multi-scale information and the inter-scale relationships is proposed; (2) A toy dataset with scale-specific morphological features is created and released to examine and visualize differential cross-scale a
    
[^32]: 基于分层Transformer的关系路径和上下文归纳关系预测方法

    Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])

    [http://arxiv.org/abs/2304.00215](http://arxiv.org/abs/2304.00215)

    本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。

    

    在知识图谱中进行关系预测是一个重要的研究课题。现有的嵌入式方法主要依赖于转导设置，缺乏归纳能力，无法推广到新的实体上进行推理。本文提出了一种新方法，通过使用统一的分层Transformer框架，即REPORT，同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性，这种方法完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在实验中，REPORT表现优于所有基线方法，甚至在两个完全归纳的数据集的八个版本子集上也是如此。此外，REPORT能够将推理推广到训练和推理中没有公共实体的新实体上，并在基准数据集上实现了最先进的性能。

    Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
    
[^33]: 论文标题：“恶魔在查询中：推进Mask Transformers技术在真实医学图像分割和超出分布定位中的应用”

    Devil is in the Queries: Advancing Mask Transformers for Real-world Medical Image Segmentation and Out-of-Distribution Localization. (arXiv:2304.00212v1 [cs.CV])

    [http://arxiv.org/abs/2304.00212](http://arxiv.org/abs/2304.00212)

    本文采用Mask Transformers技术和目标查询的概念，以实现医学图像分割中超出分布的定位，从而提高医疗AI算法的可靠性和安全性。

    

    真实医学图像分割中存在着巨大的长尾对象复杂性，其中尾部情况与相对罕见的疾病相关，并在临床上具有显著意义。一种值得信赖的医疗AI算法应该在尾部条件上展现其有效性，避免在这些超出分布(OOD)的情况下造成致命的危害。本文采用目标查询的概念将语义分割表述为软聚类分配，查询在训练期间适应于内部点的特征级聚类中心。因此，在真实世界场景中对医学图像执行推理时，像素与查询之间的相似性可以检测和定位OOD区域。我们将此OOD定位称为MaxQuery。此外，真实医学图像的前景，无论是OOD对象还是内部点，都是病变。它们之间的差异小于前景和背景之间的差异，可能会误导对象

    Real-world medical image segmentation has tremendous long-tailed complexity of objects, among which tail conditions correlate with relatively rare diseases and are clinically significant. A trustworthy medical AI algorithm should demonstrate its effectiveness on tail conditions to avoid clinically dangerous damage in these out-of-distribution (OOD) cases. In this paper, we adopt the concept of object queries in Mask Transformers to formulate semantic segmentation as a soft cluster assignment. The queries fit the feature-level cluster centers of inliers during training. Therefore, when performing inference on a medical image in real-world scenarios, the similarity between pixels and the queries detects and localizes OOD regions. We term this OOD localization as MaxQuery. Furthermore, the foregrounds of real-world medical images, whether OOD objects or inliers, are lesions. The difference between them is less than that between the foreground and background, possibly misleading the object
    
[^34]: 使用先验引导知识改进快速对抗训练

    Improving Fast Adversarial Training with Prior-Guided Knowledge. (arXiv:2304.00202v1 [cs.LG])

    [http://arxiv.org/abs/2304.00202](http://arxiv.org/abs/2304.00202)

    本文提出了一种使用先前训练过程中高质量对抗扰动的正面先验引导对抗初始化方法，以提高对抗样本的质量，从而避免快速对抗训练中的灾难性过度拟合问题。

    

    快速对抗训练是提高模型鲁棒性的有效方法。然而，原始的快速对抗训练会遭受灾难性的过度拟合问题，在经过几个训练周期后鲁棒性会急剧下降。虽然已经提出了各种快速对抗训练的变体来防止过度拟合，但它们需要较高的训练成本。本文通过比较标准对抗训练和快速对抗训练的训练过程，研究了对抗样本质量和灾难性过度拟合之间的关系。我们发现，当对抗样本的攻击成功率变差时，就会发生灾难性的过度拟合。基于这一观察，我们提出了一种使用高质量对抗扰动的正面先验引导对抗初始化方法，以提高对抗样本的质量，从而避免额外的训练成本。我们提供了该初始化方法的理论分析。

    Fast adversarial training (FAT) is an efficient method to improve robustness. However, the original FAT suffers from catastrophic overfitting, which dramatically and suddenly reduces robustness after a few training epochs. Although various FAT variants have been proposed to prevent overfitting, they require high training costs. In this paper, we investigate the relationship between adversarial example quality and catastrophic overfitting by comparing the training processes of standard adversarial training and FAT. We find that catastrophic overfitting occurs when the attack success rate of adversarial examples becomes worse. Based on this observation, we propose a positive prior-guided adversarial initialization to prevent overfitting by improving adversarial example quality without extra training costs. This initialization is generated by using high-quality adversarial perturbations from the historical training process. We provide theoretical analysis for the proposed initialization a
    
[^35]: 基于扩散映射的粒子系统用于生成模型

    Diffusion map particle systems for generative modeling. (arXiv:2304.00200v1 [stat.ML])

    [http://arxiv.org/abs/2304.00200](http://arxiv.org/abs/2304.00200)

    本文提出一种新型扩散映射粒子系统(DMPS)，可以用于高效生成建模，实验表明在包含流形结构的合成数据集上取得了比其他方法更好的效果。

    

    本文提出了一种新颖的扩散映射粒子系统(DMPS)，用于生成建模，该方法基于扩散映射和Laplacian调整的Wasserstein梯度下降（LAWGD）。扩散映射被用来从样本中近似Langevin扩散过程的生成器，从而学习潜在的数据生成流形。另一方面，LAWGD能够在合适的核函数选择下高效地从目标分布中抽样，我们在这里通过扩散映射计算生成器的谱逼近来构造核函数。数值实验表明，我们的方法在包括具有流形结构的合成数据集上优于其他方法。

    We propose a novel diffusion map particle system (DMPS) for generative modeling, based on diffusion maps and Laplacian-adjusted Wasserstein gradient descent (LAWGD). Diffusion maps are used to approximate the generator of the Langevin diffusion process from samples, and hence to learn the underlying data-generating manifold. On the other hand, LAWGD enables efficient sampling from the target distribution given a suitable choice of kernel, which we construct here via a spectral approximation of the generator, computed with diffusion maps. Numerical experiments show that our method outperforms others on synthetic datasets, including examples with manifold structure.
    
[^36]: 无碰撞运输图在流行学习中的应用

    Applications of No-Collision Transportation Maps in Manifold Learning. (arXiv:2304.00199v1 [cs.LG])

    [http://arxiv.org/abs/2304.00199](http://arxiv.org/abs/2304.00199)

    本文研究了在流形学习中应用无碰撞运输图的方法，其可以比OT图更便宜地计算距离，并提供单个概率测度的平移和伸缩的等距性。

    

    本文研究了引入于[Nurbekyan et al.，2020]的无碰撞运输图在图像数据的流形学习中的应用。近年来，在表示类似运动或变形现象的数据中，应用基于运输的距离和特征的研究大幅增加。事实上，固定位置比较强度通常无法显示数据结构。在[Nurbekyan et al.，2020]中开发的无碰撞图和距离类似于最优传输(OT)图的几何特征但由于无需优化，计算成本要便宜得多。本文证明无碰撞距离提供单个概率测度的平移(分别是伸缩)和装备欧几里得距离的平移(分别是伸缩)向量之间的等距性。此外，我们证明，无碰撞运输图以及OT和线性OT图，一般来说不能为旋转提供等距性。

    In this work, we investigate applications of no-collision transportation maps introduced in [Nurbekyan et. al., 2020] in manifold learning for image data. Recently, there has been a surge in applying transportation-based distances and features for data representing motion-like or deformation-like phenomena. Indeed, comparing intensities at fixed locations often does not reveal the data structure. No-collision maps and distances developed in [Nurbekyan et. al., 2020] are sensitive to geometric features similar to optimal transportation (OT) maps but much cheaper to compute due to the absence of optimization. In this work, we prove that no-collision distances provide an isometry between translations (respectively dilations) of a single probability measure and the translation (respectively dilation) vectors equipped with a Euclidean distance. Furthermore, we prove that no-collision transportation maps, as well as OT and linearized OT maps, do not in general provide an isometry for rotatio
    
[^37]: 序列学习中的噪声数据：数据同化结合回声状态网络

    Sequential Learning from Noisy Data: Data-Assimilation Meets Echo-State Network. (arXiv:2304.00198v1 [eess.SY])

    [http://arxiv.org/abs/2304.00198](http://arxiv.org/abs/2304.00198)

    本文解决了如何从噪声数据中训练递归神经网络的问题，提出了一种利用集合卡尔曼滤波器为回声状态网络（ESN）进行训练的序列训练算法，有效提升了模型性能，同时计算成本较低。

    

    本文探讨了如何从噪声数据中训练递归神经网络。虽然基于神经网络的动态预测器对无噪声的训练数据表现良好，但训练阶段输入噪声时的预测面临着重大挑战。本文开发了一种序列训练算法，通过合并噪声观察结果，利用集合卡尔曼滤波器为回声状态网络（ESN）进行训练。结果表明，Kalman训练的ESN（KalT-ESN）优于传统最小二乘算法训练的ESN，同时仍然计算成本低廉。所提出的方法在来自三个系统的噪声观测数据上进行了演示：两个混沌动力系统的合成数据集和一个实时交通数据集。

    This paper explores the problem of training a recurrent neural network from noisy data. While neural network based dynamic predictors perform well with noise-free training data, prediction with noisy inputs during training phase poses a significant challenge. Here a sequential training algorithm is developed for an echo-state network (ESN) by incorporating noisy observations using an ensemble Kalman filter. The resultant Kalman-trained echo-state network (KalT-ESN) outperforms the traditionally trained ESN with least square algorithm while still being computationally cheap. The proposed method is demonstrated on noisy observations from three systems: two synthetic datasets from chaotic dynamical systems and a set of real-time traffic data.
    
[^38]: 抽象器：基于Transformer的符号消息传递和关系推理模块

    Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning. (arXiv:2304.00195v1 [stat.ML])

    [http://arxiv.org/abs/2304.00195](http://arxiv.org/abs/2304.00195)

    该论文提出了一个基于Transformer的框架，用于实现符号消息传递和关系推理，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。

    

    该论文提出了一个框架，将关系学习转化为Transformer模型，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。

    A framework is proposed that casts relational learning in terms of transformers, implementing binding between sensory states and abstract states with relational cross attention mechanisms.
    
[^39]: 基于置信度预测的随机传感器不确定性下安全的感知控制

    Safe Perception-Based Control under Stochastic Sensor Uncertainty using Conformal Prediction. (arXiv:2304.00194v1 [eess.SY])

    [http://arxiv.org/abs/2304.00194](http://arxiv.org/abs/2304.00194)

    本文提出了一种利用置信度预测来应对传感器噪声及不确定性的感知控制框架，通过量化感知地图的不确定性并将其整合到控制设计中，计算有效的状态估计区域，从而实现连续时间系统的采样数据控制，确保系统的安全性和有效性。

    

    本文考虑利用通过学习增强的感知地图从高维传感器测量中获得的状态估计的感知控制。但是，这些感知地图并不完美，会导致状态估计误差，这可能导致不安全的系统行为。随机传感器噪声会使情况变得更糟，并导致遵循未知分布的估计误差。我们提出了一个感知控制框架，它: i）量化了感知地图的估计不确定性，并ii）将这些不确定性表示集成到控制设计中。为此，我们使用置信度预测来计算有效的状态估计区域，这些区域是高概率包含未知状态的集合。然后，我们基于测量鲁棒控制障碍函数的概念设计了连续时间系统的采样数据控制器。我们的控制器使用了自触发控制的思想，并使我们避免使用随机微积分。我们的框架是一种易于实现和可扩展的方法，可以保证系统的安全性和有效性。

    We consider perception-based control using state estimates that are obtained from high-dimensional sensor measurements via learning-enabled perception maps. However, these perception maps are not perfect and result in state estimation errors that can lead to unsafe system behavior. Stochastic sensor noise can make matters worse and result in estimation errors that follow unknown distributions. We propose a perception-based control framework that i) quantifies estimation uncertainty of perception maps, and ii) integrates these uncertainty representations into the control design. To do so, we use conformal prediction to compute valid state estimation regions, which are sets that contain the unknown state with high probability. We then devise a sampled-data controller for continuous-time systems based on the notion of measurement robust control barrier functions. Our controller uses idea from self-triggered control and enables us to avoid using stochastic calculus. Our framework is agnost
    
[^40]: 基于Neo4j和深度学习的交通拥堵模拟与优化

    Leveraging Neo4j and deep learning for traffic congestion simulation & optimization. (arXiv:2304.00192v1 [cs.AI])

    [http://arxiv.org/abs/2304.00192](http://arxiv.org/abs/2304.00192)

    本文利用Neo4j图和深度学习方法解决城市道路网络中的交通拥堵问题，能够实现拥堵路段的负载平衡和优化，同时可以预测交通事故对整体交通的影响。

    

    交通拥堵一直是城市道路网络中的主要挑战。过去进行了大量研究，以凸显与交通拥堵相关的问题，并通过数据驱动的方法解决了这个问题。目前，大多数交通拥堵分析都是使用模拟软件进行的，这些软件由于使用的工具和实用程序的限制而提供了有限的洞见。所有这些都影响到定制业务问题的制定，这些问题因地区和国家而异。通过利用知识图的能力，我们将交通拥堵问题建模为Neo4j图，然后使用负载平衡、优化算法来识别无拥堵的道路网络。我们还展示了在拥堵或事故情况下交通如何向后传播以及其对其他道路段的总体影响。我们还在实时交通数据上训练了顺序RNN-LSTM(长短时记忆)深度学习模型。

    Traffic congestion has been a major challenge in many urban road networks. Extensive research studies have been conducted to highlight traffic-related congestion and address the issue using data-driven approaches. Currently, most traffic congestion analyses are done using simulation software that offers limited insight due to the limitations in the tools and utilities being used to render various traffic congestion scenarios. All that impacts the formulation of custom business problems which vary from place to place and country to country. By exploiting the power of the knowledge graph, we model a traffic congestion problem into the Neo4j graph and then use the load balancing, optimization algorithm to identify congestion-free road networks. We also show how traffic propagates backward in case of congestion or accident scenarios and its overall impact on other segments of the roads. We also train a sequential RNN-LSTM (Long Short-Term Memory) deep learning model on the real-time traffi
    
[^41]: PrefGen：基于偏好的相对属性图像生成

    PrefGen: Preference Guided Image Generation with Relative Attributes. (arXiv:2304.00185v1 [cs.CV])

    [http://arxiv.org/abs/2304.00185](http://arxiv.org/abs/2304.00185)

    $\textit{PrefGen}$ 系统利用简单的成对比较查询，控制生成图像的相对属性。利用这些查询响应的信息，对一组图像属性的偏好进行估计，并进行基于偏好的图像编辑。

    

    深度生成模型可以渲染出高保真的人脸等内容的图像。近年来，在生成具有特定数量属性的图像方面取得了实质性进展，例如情感等。这些方法通常需要用户明确量化所需视觉属性的强度。但是限制在于许多属性，例如面部表情的 "愤怒" 程度，用户难以准确量化。然而，用户可以可靠地表达出 "哪张脸看起来更愤怒"，基于这个假设，我们开发了 $\textit{PrefGen}$ 系统，通过呈现用户简单的成对比较查询，如 "你更喜欢图像 $a$ 还是 $b$？" 来控制生成图像的相对属性。利用序列化查询响应的信息，我们可以估计用户对一组图像属性的偏好，并进行基于偏好的图像编辑。

    Deep generative models have the capacity to render high fidelity images of content like human faces. Recently, there has been substantial progress in conditionally generating images with specific quantitative attributes, like the emotion conveyed by one's face. These methods typically require a user to explicitly quantify the desired intensity of a visual attribute. A limitation of this method is that many attributes, like how "angry" a human face looks, are difficult for a user to precisely quantify. However, a user would be able to reliably say which of two faces seems "angrier". Following this premise, we develop the $\textit{PrefGen}$ system, which allows users to control the relative attributes of generated images by presenting them with simple paired comparison queries of the form "do you prefer image $a$ or image $b$?" Using information from a sequence of query responses, we can estimate user preferences over a set of image attributes and perform preference-guided image editing 
    
[^42]: 仿射马尔科夫博弈中的软Bellman平衡：前向解与逆向学习

    Soft-Bellman Equilibrium in Affine Markov Games: Forward Solutions and Inverse Learning. (arXiv:2304.00163v1 [cs.GT])

    [http://arxiv.org/abs/2304.00163](http://arxiv.org/abs/2304.00163)

    本文提出了一种新的解决方案概念——软Bellman平衡，解决了仿射马尔科夫博弈中的多个玩家交互问题，并提出了一种非线性最小二乘算法来计算此平衡，同时通过投影梯度算法解决推断玩家奖励参数的问题。

    

    马尔科夫博弈在随机动态环境中模拟多个玩家之间的交互。每个玩家在马尔科夫博弈中最大化其期望的总折现奖励，该奖励取决于其他玩家的策略。我们提出了一类马尔科夫博弈，称为仿射马尔科夫博弈，在其中，仿射奖励函数耦合了玩家的行动。我们引入了一种新的解决方案概念，即软Bellman平衡，在其中，每个玩家都是有限理性的，并选择软Bellman策略，而不是像著名的Nash平衡概念中那样选择纯理性策略。我们提供了软Bellman平衡存在和唯一性的条件，并提出了一个非线性最小二乘算法来计算前向问题中的这种平衡。然后，我们通过投影梯度算法解决了推断玩家奖励参数的逆向博弈问题。在掠食者-猎物OpenAI Gym环境中的实验表明，使用软Bellman策略可以更有效地控制掠食者和猎物之间的交互。

    Markov games model interactions among multiple players in a stochastic, dynamic environment. Each player in a Markov game maximizes its expected total discounted reward, which depends upon the policies of the other players. We formulate a class of Markov games, termed affine Markov games, where an affine reward function couples the players' actions. We introduce a novel solution concept, the soft-Bellman equilibrium, where each player is boundedly rational and chooses a soft-Bellman policy rather than a purely rational policy as in the well-known Nash equilibrium concept. We provide conditions for the existence and uniqueness of the soft-Bellman equilibrium and propose a nonlinear least squares algorithm to compute such an equilibrium in the forward problem. We then solve the inverse game problem of inferring the players' reward parameters from observed state-action trajectories via a projected gradient algorithm. Experiments in a predator-prey OpenAI Gym environment show that the rewa
    
[^43]: 使用线性规划在马尔可夫决策过程上进行在线强化学习

    Online Reinforcement Learning in Markov Decision Process Using Linear Programming. (arXiv:2304.00155v1 [cs.LG])

    [http://arxiv.org/abs/2304.00155](http://arxiv.org/abs/2304.00155)

    本论文提出了一种在未知转移矩阵和固定但未知分布的情况下进行在线MDP学习的简单而高效的方法，可以实现更紧的遗憾界，并通过置信区间框架改进了现有算法。

    

    本文考虑了具有未知转移矩阵和固定但未知分布的随机奖励的情况下，马尔可夫决策过程中的在线强化学习。学习者旨在通过与环境交互来学习最优策略并在有限的时间内最小化他们的遗憾。我们设计了一种简单而高效的模型算法，通过保持过渡和奖励函数的置信区间并使用占用度量将在线MDP与线性规划相连接，实现了$\tilde{O}(LX\sqrt{TA})$的高概率遗憾界。它比现有的使用类似置信区间框架的算法实现了更紧的遗憾界并改善了计算效率。

    We consider online reinforcement learning in episodic Markov decision process (MDP) with an unknown transition matrix and stochastic rewards drawn from a fixed but unknown distribution. The learner aims to learn the optimal policy and minimize their regret over a finite time horizon through interacting with the environment. We devise a simple and efficient model-based algorithm that achieves $\tilde{O}(LX\sqrt{TA})$ regret with high probability, where $L$ is the episode length, $T$ is the number of episodes, and $X$ and $A$ are the cardinalities of the state space and the action space, respectively. The proposed algorithm, which is based on the concept of "optimism in the face of uncertainty", maintains confidence sets of transition and reward functions and uses occupancy measures to connect the online MDP with linear programming. It achieves a tighter regret bound compared to the existing works that use a similar confidence sets framework and improves the computational effort compared
    
[^44]: E（3）同变图神经网络在基于粒子的流体力学中的应用

    E($3$) Equivariant Graph Neural Networks for Particle-Based Fluid Mechanics. (arXiv:2304.00150v1 [cs.LG])

    [http://arxiv.org/abs/2304.00150](http://arxiv.org/abs/2304.00150)

    本研究表明，E（3）同变图神经网络相对于非同变网络有更高学习动态交互模型的潜力，可以在基于粒子的流体力学中提供更加物理准确的交互模型。

    

    本文通过展示同变图神经网络相对于非同变网络有更高学习动态交互模型的潜力，为机器学习工程系统领域做出了贡献。我们对两种传统的流体流动系统进行基准测试，即三维衰减的Taylor-Green涡旋和三维反Poiseuille流，并在不同性能度量（如动能或Sinkhorn距离）上将同变图神经网络与非同变网络进行比较。这些度量通常用于验证数值求解器，在训练和评估速度缓慢的情况下，我们主要发现同变模型可以学习到更加物理准确的交互。这表明了未来研究在粗粒度模型上对湍流流动的研究机会，以及在系统动态和参数上的推广。

    We contribute to the vastly growing field of machine learning for engineering systems by demonstrating that equivariant graph neural networks have the potential to learn more accurate dynamic-interaction models than their non-equivariant counterparts. We benchmark two well-studied fluid flow systems, namely the 3D decaying Taylor-Green vortex and the 3D reverse Poiseuille flow, and compare equivariant graph neural networks to their non-equivariant counterparts on different performance measures, such as kinetic energy or Sinkhorn distance. Such measures are typically used in engineering to validate numerical solvers. Our main findings are that while being rather slow to train and evaluate, equivariant models learn more physically accurate interactions. This indicates opportunities for future work towards coarse-grained models for turbulent flows, and generalization across system dynamics and parameters.
    
[^45]: 关于图神经网络在物理系统模拟和经典数值方法之间的关系

    On the Relationships between Graph Neural Networks for the Simulation of Physical Systems and Classical Numerical Methods. (arXiv:2304.00146v1 [cs.LG])

    [http://arxiv.org/abs/2304.00146](http://arxiv.org/abs/2304.00146)

    本文探讨了图神经网络与经典数值方法在物理系统模拟上的关系，并提出了有潜力提高机器学习方法准确性和效率的新的模拟方法。展望这些方法将为科学研究提供更高效的机器学习模型。

    

    最近机器学习建模物理系统的方法开始呈现出计算科学中数值方法发展的影子。本文首先通过对比图神经网络加速物理模拟和基于粒子方法的发展轨迹来阐述这一点。接着，我们概述了一些模拟方法，这些方法尚未被运用到最先进的机器学习方法之中，但它们具备让机器学习方法更加准确和高效的潜力。最后，我们展望这些方法将为使机器学习模型更加高效的科学研究提供潜在可能性。

    Recent developments in Machine Learning approaches for modelling physical systems have begun to mirror the past development of numerical methods in the computational sciences. In this survey, we begin by providing an example of this with the parallels between the development trajectories of graph neural network acceleration for physical simulations and particle-based approaches. We then give an overview of simulation approaches, which have not yet found their way into state-of-the-art Machine Learning methods and hold the potential to make Machine Learning approaches more accurate and more efficient. We conclude by presenting an outlook on the potential of these approaches for making Machine Learning models for science more efficient.
    
[^46]: DeforestVis：使用代理决策树进行机器学习模型行为分析

    DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps. (arXiv:2304.00133v1 [cs.LG])

    [http://arxiv.org/abs/2304.00133](http://arxiv.org/abs/2304.00133)

    DeforestVis提供了一种可视化分析工具，通过提供代理决策树，总结了复杂机器学习模型的行为，以帮助用户探索复杂性。

    

    随着机器学习（ML）模型的复杂性增加以及不同（和关键）领域中的应用增加，越来越需要更易解释和可信赖的ML。解释复杂ML模型的一种简单且与模型无关的方法是训练代理模型（例如规则集和决策树），以足够接近原始模型，但更简单和易于解释。然而，规则集可以变得非常冗长，包含许多if-else语句，而决策树的深度会随着准确模拟复杂ML模型而迅速增加。在这种情况下，两种方法都可能无法实现其核心目标，提供用户模型的可解释性。我们通过提出DeforestVis解决了这个问题，这是一种可视化分析工具，通过提供使用自适应增强（AdaBoost）技术生成的代理决策树（一级决策树），为用户提供了对复杂ML模型行为的友好总结。我们的解决方案帮助用户探索模型的复杂性。

    As the complexity of machine learning (ML) models increases and the applications in different (and critical) domains grow, there is a strong demand for more interpretable and trustworthy ML. One straightforward and model-agnostic way to interpret complex ML models is to train surrogate models, such as rule sets and decision trees, that sufficiently approximate the original ones while being simpler and easier-to-explain. Yet, rule sets can become very lengthy, with many if-else statements, and decision tree depth grows rapidly when accurately emulating complex ML models. In such cases, both approaches can fail to meet their core goal, providing users with model interpretability. We tackle this by proposing DeforestVis, a visual analytics tool that offers user-friendly summarization of the behavior of complex ML models by providing surrogate decision stumps (one-level decision trees) generated with the adaptive boosting (AdaBoost) technique. Our solution helps users to explore the comple
    
[^47]: DynamoPMU：使用$\mu$PMU测量数据的非线性动力学进行物理信息异常检测和预测方法

    DynamoPMU: A Physics Informed Anomaly Detection and Prediction Methodology using non-linear dynamics from $\mu$PMU Measurement Data. (arXiv:2304.00092v1 [eess.SY])

    [http://arxiv.org/abs/2304.00092](http://arxiv.org/abs/2304.00092)

    本文旨在解决电力配电系统中实时$\mu$PMU测量数据中事件检测，预测的问题。使用了基于非线性动力学的方法来描述系统来提高检测精度。

    

    技术和传感器数量的扩大导致了大量的实时流数据。 电力配电系统中的实时数据通过称为$\mu$PMU的分配级相量测量单元收集，报告高分辨率的相量测量，包括各种事件特征，提供情境感知并使配电系统中可见。这些事件是不频繁，不定时和不确定的； 它是一个挑战来仔细研究，检测和预测此类事件的发生。对于电力分配系统，明确识别描述事件的复杂，非线性和非平稳签名模式的演化函数是具有挑战性的。在本文中，我们通过开发基于物理动力学的方法来检测$\mu$PMU流数据中的异常并同时使用控制方程预测事件来解决这个问题。我们利用的是非线性动力学来描述系统，在此基础上开发了一种新的物理信息检测方法，可以更准确地处理这些复杂事件的信息。

    The expansion in technology and attainability of a large number of sensors has led to a huge amount of real-time streaming data. The real-time data in the electrical distribution system is collected through distribution-level phasor measurement units referred to as $\mu$PMU which report high-resolution phasor measurements comprising various event signatures which provide situational awareness and enable a level of visibility into the distribution system. These events are infrequent, unschedule, and uncertain; it is a challenge to scrutinize, detect and predict the occurrence of such events. For electrical distribution systems, it is challenging to explicitly identify evolution functions that describe the complex, non-linear, and non-stationary signature patterns of events. In this paper, we seek to address this problem by developing a physics dynamics-based approach to detect anomalies in the $\mu$PMU streaming data and simultaneously predict the events using governing equations. We pr
    
[^48]: 机器学习在经济研究中的应用：何时、什么和如何运用？

    Machine Learning for Economics Research: When What and How?. (arXiv:2304.00086v1 [econ.GN])

    [http://arxiv.org/abs/2304.00086](http://arxiv.org/abs/2304.00086)

    本文精选综述了使用机器学习工具进行经济学研究和政策分析的文章，强调了机器学习在处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性方面的应用优势，成为计量经济学家工具箱中不可或缺的一部分。

    

    本文对使用机器学习工具进行经济学研究和政策分析的重要经济期刊上发表的文章进行了精选综述。综述回答了三个关键问题：（1）何时在经济学中使用机器学习，（2）常用的机器学习模型是什么，以及（3）如何将它们用于经济应用。综述强调了机器学习特别适用于处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性。深度学习模型适用于非传统数据，而集成学习模型适用于传统数据集。尽管传统的计量经济学模型在分析低复杂性数据时可能足够，但由于快速数字化和不断增长的文献，经济数据的复杂性增加，机器学习正成为计量经济学家工具箱中不可或缺的一部分。

    This article provides a curated review of selected papers published in prominent economics journals that use machine learning (ML) tools for research and policy analysis. The review focuses on three key questions: (1) when ML is used in economics, (2) what ML models are commonly preferred, and (3) how they are used for economic applications. The review highlights that ML is particularly used in processing nontraditional and unstructured data, capturing strong nonlinearity, and improving prediction accuracy. Deep learning models are suitable for nontraditional data, whereas ensemble learning models are preferred for traditional datasets. While traditional econometric models may suffice for analyzing low-complexity data, the increasing complexity of economic data due to rapid digitalization and the growing literature suggest that ML is becoming an essential addition to the econometrician's toolbox.
    
[^49]: Fides：一种利用安全执行环境对机器学习工作负载进行结果验证的生成框架

    Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE. (arXiv:2304.00083v1 [cs.CR])

    [http://arxiv.org/abs/2304.00083](http://arxiv.org/abs/2304.00083)

    本文提出了一种名为Fides的框架，用于实时验证外协的机器学习工作负载。该框架采用贪心蒸馏迁移学习技术，可动态蒸馏并优化验证模型，以验证对应的服务模型。此外，该框架还能在可信执行环境中运行，以提供更高的安全性和隐私性保证。

    

    机器学习在敏感领域的部署导致了对其安全性和隐私性的重视，现有解决方案，如多方计算和基于证明的系统，给实时应用带来了很大的计算开销。本文提出了一个名为Fides的框架，用于实时验证外协的机器学习工作负载，其中采用新颖且高效的贪心蒸馏迁移学习技术，实现一种实时验证模型来较少地消耗空间和计算能力，同时运行在可信执行环境中。

    The growing popularity of Machine Learning (ML) has led to its deployment in various sensitive domains, which has resulted in significant research focused on ML security and privacy. However, in some applications, such as autonomous driving, integrity verification of the outsourced ML workload is more critical-a facet that has not received much attention. Existing solutions, such as multi-party computation and proof-based systems, impose significant computation overhead, which makes them unfit for real-time applications. We propose Fides, a novel framework for real-time validation of outsourced ML workloads. Fides features a novel and efficient distillation technique-Greedy Distillation Transfer Learning-that dynamically distills and fine-tunes a space and compute-efficient verification model for verifying the corresponding service model while running inside a trusted execution environment. Fides features a client-side attack detection model that uses statistical analysis and divergenc
    
[^50]: 建立具有ML组件的产品中的挑战元总结——从4758位实践者那里汇集经验

    A Meta-Summary of Challenges in Building Products with ML Components -- Collecting Experiences from 4758+ Practitioners. (arXiv:2304.00078v1 [cs.SE])

    [http://arxiv.org/abs/2304.00078](http://arxiv.org/abs/2304.00078)

    该研究汇总了50篇文献对于构建带有机器学习组件的产品所遇到的挑战，并总结了其中最常见的问题，为研究和教育提供了有用的资源。

    

    将机器学习（ML）组件纳入软件产品中引发了新的软件工程挑战并加剧了现有挑战。许多研究人员通过采访和调查实践者，投入了大量精力来了解构建具有ML组件的行业从业者面临的挑战。为了汇总和呈现他们的共同发现，我们进行了一项元总结研究：我们收集了50篇相关论文，这些论文通过系统文献综述指南与超过4758位实践者互动。然后我们收集、分组和组织了这些论文中提到的500多个挑战。我们强调了最常报告的挑战，并希望这个元总结能成为研究界在这个领域中优先进行研究和教育的有用资源。

    Incorporating machine learning (ML) components into software products raises new software-engineering challenges and exacerbates existing challenges. Many researchers have invested significant effort in understanding the challenges of industry practitioners working on building products with ML components, through interviews and surveys with practitioners. With the intention to aggregate and present their collective findings, we conduct a meta-summary study: We collect 50 relevant papers that together interacted with over 4758 practitioners using guidelines for systematic literature reviews. We then collected, grouped, and organized the over 500 mentions of challenges within those papers. We highlight the most commonly reported challenges and hope this meta-summary will be a useful resource for the research community to prioritize research and education in this field.
    
[^51]: 电力市场中基于物理知识的机器学习：以NYISO为例的案例研究

    A Physics-Informed Machine Learning for Electricity Markets: A NYISO Case Study. (arXiv:2304.00062v1 [cs.LG])

    [http://arxiv.org/abs/2304.00062](http://arxiv.org/abs/2304.00062)

    本论文提出了利用主动集学习技术和物理约束，解决了实时电力市场中的最优电力流问题。这种解决方案特别考虑到了负荷削减和可再生能源发电的削减等现实世界电力系统挑战，从而确保所得到的市场清算结果在物理和经济上是可行的。

    

    本论文探讨了在实时电力市场中高效解决最优电力流问题的挑战。所提出的解决方案名为基于物理的市场感知主动集学习OPF(PIMA-AS-OPF)，利用物理约束和市场特性确保市场清算结果的物理和经济可行性。具体而言，PIMA-AS-OPF采用主动集学习技术，并扩展其功能以考虑负荷或可再生能源发电的削减，这是现实世界电力系统中常见的挑战。PIMA-AS-OPF的核心是一个完全连接的神经网络，其输入为净负荷和系统拓扑。该神经网络的输出包括饱和发电机和输电线等主动约束，以及非零负荷削减和风电削减。这些输出可将原始市场清算优化降至一系列线性方程组，可高效求解。

    This paper addresses the challenge of efficiently solving the optimal power flow problem in real-time electricity markets. The proposed solution, named Physics-Informed Market-Aware Active Set learning OPF (PIMA-AS-OPF), leverages physical constraints and market properties to ensure physical and economic feasibility of market-clearing outcomes. Specifically, PIMA-AS-OPF employs the active set learning technique and expands its capabilities to account for curtailment in load or renewable power generation, which is a common challenge in real-world power systems. The core of PIMA-AS-OPF is a fully-connected neural network that takes the net load and the system topology as input. The outputs of this neural network include active constraints such as saturated generators and transmission lines, as well as non-zero load shedding and wind curtailments. These outputs allow for reducing the original market-clearing optimization to a system of linear equations, which can be solved efficiently and
    
[^52]: 坚固且公正: 确保公平与鲁棒性相一致

    To be Robust and to be Fair: Aligning Fairness with Robustness. (arXiv:2304.00061v1 [cs.LG])

    [http://arxiv.org/abs/2304.00061](http://arxiv.org/abs/2304.00061)

    本研究提出了一种同时考虑公平性和准确性指标的对抗训练和攻击方法，并证明了两个指标之间的一致性以及互相受益的关系。

    

    对抗训练已经被证明可以可靠地提高对抗样本的鲁棒性。然而，就公平性而言，对抗训练的问题尚未得到适当研究，公平性与准确性攻击之间的关系仍然不清楚。我们是否可以同时提高对公平性和准确性的鲁棒性？为了解决这个问题，本文研究了对抗训练和对抗攻击对这两个指标的问题。我们提出了一种公平性攻击的统一结构，将群体公平中的常见概念汇集在一起，并在理论上证明了不同概念下的公平性攻击等价性。此外，我们展示了公平性和准确性攻击的一致性，并在理论上证明了一种指标的鲁棒性会受到另一种指标鲁棒性的益处。我们的研究提出了一种统一公平与准确性的对抗训练和攻击的新方法，实验结果表明...

    Adversarial training has been shown to be reliable in improving robustness against adversarial samples. However, the problem of adversarial training in terms of fairness has not yet been properly studied, and the relationship between fairness and accuracy attack still remains unclear. Can we simultaneously improve robustness w.r.t. both fairness and accuracy? To tackle this topic, in this paper, we study the problem of adversarial training and adversarial attack w.r.t. both metrics. We propose a unified structure for fairness attack which brings together common notions in group fairness, and we theoretically prove the equivalence of fairness attack against different notions. Moreover, we show the alignment of fairness and accuracy attack, and theoretically demonstrate that robustness w.r.t. one metric benefits from robustness w.r.t. the other metric. Our study suggests a novel way to unify adversarial training and attack w.r.t. fairness and accuracy, and experimental results show that 
    
[^53]: $\ell_1$和logistic回归的近线性常数因子草图

    Almost Linear Constant-Factor Sketching for $\ell_1$ and Logistic Regression. (arXiv:2304.00051v1 [cs.DS])

    [http://arxiv.org/abs/2304.00051](http://arxiv.org/abs/2304.00051)

    本文提出了一种近线性、常数因子草图，适用于$\ell_1$和logistic回归，具有小的草图维度和高精度，这种草图还在草图空间内提供了高效的优化问题求解方法。

    

    我们改进了以前关于$\ell_1$和Logistic回归的草图算法结果，得到了更小的草图维度和更高的精度，我们的结果在草图空间内产生了高效的优化问题。特别地，我们对于任何常数$c>0$，实现了$\ell_1$回归的草图维度为$\tilde{O}(d^{1+c})$，而对于Logistic回归则为$\tilde{O}(\mu d^{1+c})$，其中$\mu$是一个标准的度量，捕获了压缩数据的复杂性。对于$\ell_1$回归，我们的草图维度是近线性的，具有比先前的工作更高的精度和更小的草图维度。类似地，对于Logistic回归，以前的工作在其草图维度上有更差的$\operatorname{poly}(\mu d)$因子。我们还提供了一种折衷方案，通过增加总大小到$(d\log$，在输入稀疏性时间内产生了$1+\varepsilon$的近似值。

    We improve upon previous oblivious sketching and turnstile streaming results for $\ell_1$ and logistic regression, giving a much smaller sketching dimension achieving $O(1)$-approximation and yielding an efficient optimization problem in the sketch space. Namely, we achieve for any constant $c>0$ a sketching dimension of $\tilde{O}(d^{1+c})$ for $\ell_1$ regression and $\tilde{O}(\mu d^{1+c})$ for logistic regression, where $\mu$ is a standard measure that captures the complexity of compressing the data. For $\ell_1$-regression our sketching dimension is near-linear and improves previous work which either required $\Omega(\log d)$-approximation with this sketching dimension, or required a larger $\operatorname{poly}(d)$ number of rows. Similarly, for logistic regression previous work had worse $\operatorname{poly}(\mu d)$ factors in its sketching dimension. We also give a tradeoff that yields a $1+\varepsilon$ approximation in input sparsity time by increasing the total size to $(d\log
    
[^54]: 针对关键稀有类别的排序正则化：在高真阳率下最小化假阳性

    Ranking Regularization for Critical Rare Classes: Minimizing False Positives at a High True Positive Rate. (arXiv:2304.00049v1 [cs.CV])

    [http://arxiv.org/abs/2304.00049](http://arxiv.org/abs/2304.00049)

    本文提出了一种基于排序的正则化（RankReg）方法，它能在高真阳性率下最小化假阳性，易于实现，且经验性地表明它不仅能有效地减少假阳性，而且还与传统的不平衡学习损失相辅相成。

    

    在许多实际场景中，关键类别往往是稀有的，漏检会带来不成比例的高代价。例如，肿瘤是罕见的，误诊会对治疗结果产生严重影响；欺诈银行交易是罕见的，未被发现的情况可能导致巨大的损失或法律惩罚。在这种情况下，系统通常以较高的真阳性率运行，这可能需要容忍较高的假阳性率。本文提出了一种新颖的方法来解决需要在高真阳性率下运行的系统最小化假阳性的挑战。我们提出一种基于排序的正则化（RankReg）方法，易于实现，并经验性地表明它不仅能有效地减少假阳性，而且还与传统的不平衡学习损失相辅相成。使用这种新颖技术，我们在三个广泛探索的数据集（CIFAR-10＆100和黑色素瘤检测）上进行了一系列实验，以证明我们方法的有效性。

    In many real-world settings, the critical class is rare and a missed detection carries a disproportionately high cost. For example, tumors are rare and a false negative diagnosis could have severe consequences on treatment outcomes; fraudulent banking transactions are rare and an undetected occurrence could result in significant losses or legal penalties. In such contexts, systems are often operated at a high true positive rate, which may require tolerating high false positives. In this paper, we present a novel approach to address the challenge of minimizing false positives for systems that need to operate at a high true positive rate. We propose a ranking-based regularization (RankReg) approach that is easy to implement, and show empirically that it not only effectively reduces false positives, but also complements conventional imbalanced learning losses. With this novel technique in hand, we conduct a series of experiments on three broadly explored datasets (CIFAR-10&100 and Melanom
    
[^55]: PEOPL: 具有公共标签的私下编码开放数据集的特征

    PEOPL: Characterizing Privately Encoded Open Datasets with Public Labels. (arXiv:2304.00047v1 [cs.LG])

    [http://arxiv.org/abs/2304.00047](http://arxiv.org/abs/2304.00047)

    PEOPL是一种有希望的解决机构共享数据以训练机器学习（ML）模型的难题的方法，使用一定类别的随机构造变换来编码敏感数据。这些方法通过一些信息论分数来量化隐私和效用，并引入了原语构建编码方案系列，并将其适应于编码设置，通过对公开可用数据集的模拟评估不同的编码方案选择，我们观察到我们的方法既有效又可扩展到高维数据。

    

    解决让机构在没有意外信息泄露的情况下共享数据以训练机器学习（ML）模型的问题一直是实际应用中的一个难题。这项仍未解决的问题的一种有希望的技术是在编码数据上训练模型。我们的方法称为具有公共标签的私人编码开放数据集（PEOPL），使用一定类别的随机构造变换来编码敏感数据。机构们发布它们的随机编码数据和与之关联的原始标签以便进行ML训练，其中训练是在没有编码实现的情况下进行。我们研究了这个问题的几个重要方面：我们引入了隐私和效用的信息论分数，量化了使用发布的编码数据的不忠实的用户（例如，攻击者）和忠实的用户（例如，模型开发人员）的平均表现。然后，我们从理论上对构建编码方案系列的原语进行了特征化，并将它们适应于编码设置。通过对公开可用数据集的模拟评估不同的编码方案选择，我们观察到我们的方法既有效又可扩展到高维数据。最后，我们讨论了我们的方法的局限性和未来工作的潜在方向。

    Allowing organizations to share their data for training of machine learning (ML) models without unintended information leakage is an open problem in practice. A promising technique for this still-open problem is to train models on the encoded data. Our approach, called Privately Encoded Open Datasets with Public Labels (PEOPL), uses a certain class of randomly constructed transforms to encode sensitive data. Organizations publish their randomly encoded data and associated raw labels for ML training, where training is done without knowledge of the encoding realization. We investigate several important aspects of this problem: We introduce information-theoretic scores for privacy and utility, which quantify the average performance of an unfaithful user (e.g., adversary) and a faithful user (e.g., model developer) that have access to the published encoded data. We then theoretically characterize primitives in building families of encoding schemes that motivate the use of random deep neura
    
[^56]: 使用离线预训练加速探索和表示学习

    Accelerating exploration and representation learning with offline pre-training. (arXiv:2304.00046v1 [cs.LG])

    [http://arxiv.org/abs/2304.00046](http://arxiv.org/abs/2304.00046)

    本论文提出了一个假设，即基于离线数据可以通过分别学习状态表示和辅助奖励模型来改善探索和表示学习，实验证明这种方法显著提高了在具有挑战性的 NetHack 基准测试上的样本效率。

    

    串行决策制定代理在长期任务中面临挑战，因为需要多步推理才能解决。大多数强化学习（RL）算法通过改进信用分配，引入记忆能力，改变代理的内在动机（即探索）或其世界观（即知识表示）来应对这一挑战。这些组成部分中的许多都可以从离线数据中学习。本文提出了一个假设，即通过从单个离线数据集中分别学习两个不同的模型，可以改善探索和表示学习。我们展示了使用噪声对比估计学习状态表示以及从单个人类演示的模型辅助奖励可以显著提高在具有挑战性的 NetHack 基准测试上的样本效率。我们还消融了实验设置的各个组成部分并突显了重要的见解。

    Sequential decision-making agents struggle with long horizon tasks, since solving them requires multi-step reasoning. Most reinforcement learning (RL) algorithms address this challenge by improved credit assignment, introducing memory capability, altering the agent's intrinsic motivation (i.e. exploration) or its worldview (i.e. knowledge representation). Many of these components could be learned from offline data. In this work, we follow the hypothesis that exploration and representation learning can be improved by separately learning two different models from a single offline dataset. We show that learning a state representation using noise-contrastive estimation and a model of auxiliary reward separately from a single collection of human demonstrations can significantly improve the sample efficiency on the challenging NetHack benchmark. We also ablate various components of our experimental setting and highlight crucial insights.
    
[^57]: 一种考虑缺失数据的SHM鲁棒深度学习损伤识别方法

    A robust deep learning-based damage identification approach for SHM considering missing data. (arXiv:2304.00040v1 [cs.LG])

    [http://arxiv.org/abs/2304.00040](http://arxiv.org/abs/2304.00040)

    本文提出了一种基于LSTM模型和自编码器框架中的Dropout机制的鲁棒方法，用于SHM中的损伤识别，考虑缺失数据的存在。

    

    近年来，基于数据驱动的结构健康监测（SHM）方法受到广泛关注。这些方法从监测时间序列数据的相关性中挖掘隐藏的结构性能。然而，在监测数据中，经常遇到缺失数据的问题，这会对数据挖掘和后续任务（如条件评估）产生严重的影响。本文提出了一种基于Long-Short Term Memory（LSTM）模型和自编码器（AE）框架中的Dropout机制，考虑缺失数据情况，进行损伤识别的鲁棒方法。在训练过程中，随机删除输入通道，以模拟缺失数据，并将重构误差用作损失函数。

    Data-driven method for Structural Health Monitoring (SHM), that mine the hidden structural performance from the correlations among monitored time series data, has received widely concerns recently. However, missing data significantly impacts the conduction of this method. Missing data is a frequently encountered issue in time series data in SHM and many other real-world applications, that harms to the standardized data mining and downstream tasks, such as condition assessment. Imputation approaches based on spatiotemporal relations among monitoring data are developed to handle this issue, however, no additional information is added during imputation. This paper thus develops a robust method for damage identification that considers the missing data occasions, based on long-short term memory (LSTM) model and dropout mechanism in the autoencoder (AE) framework. Inputs channels are randomly dropped to simulate the missing data in training, and reconstruction errors are used as the loss fun
    
[^58]: 理解强化学习算法：从基础 Q-learning 到相对策略优化的进展

    Understanding Reinforcement Learning Algorithms: The Progress from Basic Q-learning to Proximal Policy Optimization. (arXiv:2304.00026v1 [cs.LG])

    [http://arxiv.org/abs/2304.00026](http://arxiv.org/abs/2304.00026)

    本文综述了强化学习领域，对初学者介绍了关键概念、技术和算法，并全面涵盖了不同类型的强化学习算法，呈现了该领域的历史进展。

    

    本文综述了强化学习领域，重点全面介绍了初学者所需的关键概念、技术和算法。强化学习具有独特的设置、术语和数学，对于新手或人工智能领域的人来说可能会感到吓人。尽管许多论文在特定应用上回顾了强化学习，如游戏、医疗保健、金融或机器人技术，但由于包括非强化学习相关工作和使用针对这些特定应用定制的算法，这些论文可能难以让初学者跟随。为解决这些挑战，本文清晰而简明地介绍了强化学习的基本原理，并涵盖了不同类型的强化学习算法。对于每个算法/方法，我们概述了其开发背后的主要动机、内部机制和限制。本文的呈现与该领域的历史进展相一致，从早期的 Q-learning 算法到更近期的相对策略优化 (PPO)算法。

    This paper presents a review of the field of reinforcement learning (RL), with a focus on providing a comprehensive overview of the key concepts, techniques, and algorithms for beginners. RL has a unique setting, jargon, and mathematics that can be intimidating for those new to the field or artificial intelligence more broadly. While many papers review RL in the context of specific applications, such as games, healthcare, finance, or robotics, these papers can be difficult for beginners to follow due to the inclusion of non-RL-related work and the use of algorithms customized to those specific applications. To address these challenges, this paper provides a clear and concise overview of the fundamental principles of RL and covers the different types of RL algorithms. For each algorithm/method, we outline the main motivation behind its development, its inner workings, and its limitations. The presentation of the paper is aligned with the historical progress of the field, from the early 
    
[^59]: SemiMemes：一种用于多模态Memes分析的半监督学习方法

    SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])

    [http://arxiv.org/abs/2304.00020](http://arxiv.org/abs/2304.00020)

    研究提出了一种利用多模态数据的半监督学习方法，命名为SemiMemes，主要应用于Memes的分析和注释过程。该方法在多个数据集中表现优异，并优于其他最新的多模态半监督学习和监督学习模型。

    

    社交媒体上Memes的普及性引发了分析其隐含含义、审查有害内容的需求。机器学习的Meme审查系统需要半监督学习解决方案，以利用互联网上大量未标记的Memes，并使注释过程变得更简单。此外，该方法需要利用多模态数据，因为Memes的含义通常来自图像和文本。该研究提出了一种多模态半监督学习方法，在两个数据集，即多媒体自动性别歧视识别和令人讨厌的Memes数据集上，优于其他多模态半监督和监督学习的最新模型。借鉴对比语言-图像预训练所获得的见解，这项研究引入了SemiMemes，一种新颖的训练方法，它结合了自编码器和分类任务

    The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to
    
[^60]: DRIP: 逆问题的深度正则化器

    DRIP: Deep Regularizers for Inverse Problems. (arXiv:2304.00015v1 [cs.LG])

    [http://arxiv.org/abs/2304.00015](http://arxiv.org/abs/2304.00015)

    提出了一种基于变分法的深度神经正则化器家族，保证可以适配数据并解决逆问题。在图像去模糊和小角度层析成像等问题上可行。

    

    逆问题在数学上是不良定义的，因此对于一些（带有噪声的）数据，可能会有不止一个与数据匹配的解。近年来，一些能够找到最合适解决方案的深度神经技术得到了发展，但它们存在一些缺点：大多数技术无法保证解决方案能够在推理时匹配数据；虽然这些技术的推导是基于一个有效的标量正则化函数存在的基础之上，但在实际运用中这些技术并没有依赖于这样一个函数，因此与传统的变分技术有所偏离。本文提出了一种新的神经正则化器家族来解决逆问题，这些正则化器基于变分形式，并保证适配数据。我们演示了它们在一些高度ill-posed问题上的使用，包括图像去模糊和小角度层析成像。

    Inverse problems are mathematically ill-posed. Thus, given some (noisy) data, there is more than one solution that fits the data. In recent years, deep neural techniques that find the most appropriate solution, in the sense that it contains a-priori information, were developed. However, they suffer from several shortcomings. First, most techniques cannot guarantee that the solution fits the data at inference. Second, while the derivation of the techniques is inspired by the existence of a valid scalar regularization function, such techniques do not in practice rely on such a function, and therefore veer away from classical variational techniques. In this work we introduce a new family of neural regularizers for the solution of inverse problems. These regularizers are based on a variational formulation and are guaranteed to fit the data. We demonstrate their use on a number of highly ill-posed problems, from image deblurring to limited angle tomography.
    
[^61]: 基于梯度去偏置的无目标图结构攻击中合理的预算配置

    Towards Reasonable Budget Allocation in Untargeted Graph Structure Attacks via Gradient Debias. (arXiv:2304.00010v1 [cs.LG])

    [http://arxiv.org/abs/2304.00010](http://arxiv.org/abs/2304.00010)

    本文提出了一种基于梯度去偏置的图结构无目标攻击模型，用于释放低效的攻击预算。

    

    在分类相关任务中，使用交叉熵损失函数已经成为认知惯性。在图结构的无目标攻击中，攻击目标产生的梯度是攻击者评估扰动方案的基础。过去的方法在攻击节点级分类模型时使用负交叉熵损失作为攻击目标。然而，交叉熵函数是否适合构建无目标攻击目标尚未在以前的工作中进行讨论。本文从预算分配的角度，对以前不合理的攻击目标进行了论证。我们理论上和实验上证明了，负交叉熵往往会从置信度较低的节点产生更大的梯度，在这些节点的预测类别被误导的情况下也是如此。为了释放这些低效的攻击预算，我们提出了一种简单的基于梯度去偏置的图结构无目标攻击模型。

    It has become cognitive inertia to employ cross-entropy loss function in classification related tasks. In the untargeted attacks on graph structure, the gradients derived from the attack objective are the attacker's basis for evaluating a perturbation scheme. Previous methods use negative cross-entropy loss as the attack objective in attacking node-level classification models. However, the suitability of the cross-entropy function for constructing the untargeted attack objective has yet been discussed in previous works. This paper argues about the previous unreasonable attack objective from the perspective of budget allocation. We demonstrate theoretically and empirically that negative cross-entropy tends to produce more significant gradients from nodes with lower confidence in the labeled classes, even if the predicted classes of these nodes have been misled. To free up these inefficient attack budgets, we propose a simple attack model for untargeted attacks on graph structure based o
    
[^62]: 面向旅行护理行业的使用多模型数据服务的双向个性化强化学习架构中的主动学习

    Bi-directional personalization reinforcement learning-based architecture with active learning using a multi-model data service for the travel nursing industry. (arXiv:2304.00006v1 [cs.IR])

    [http://arxiv.org/abs/2304.00006](http://arxiv.org/abs/2304.00006)

    本文提出了一个面向旅行护理行业的招聘方案，采用多模型数据服务加速数据采集，并使用双向强化学习和主动学习提供个性化推荐，解决了这一行业标注数据短缺的问题。

    

    本文讨论了机器学习技术如何通过使用多模型数据服务加速数据采集并使用双向强化学习和主动学习提供个性化推荐，从而增强旅行护理行业的招聘流程。该方案可帮助招聘人员推荐合格申请人，并使申请人接收到个性化工作推荐。本文还讨论了使用主动学习来解决这一行业标注数据短缺的问题。

    The challenges of using inadequate online recruitment systems can be addressed with machine learning and software engineering techniques. Bi-directional personalization reinforcement learning-based architecture with active learning can get recruiters to recommend qualified applicants and also enable applicants to receive personalized job recommendations. This paper focuses on how machine learning techniques can enhance the recruitment process in the travel nursing industry by helping speed up data acquisition using a multi-model data service and then providing personalized recommendations using bi-directional reinforcement learning with active learning. This need was especially evident when trying to respond to the overwhelming needs of healthcare facilities during the COVID-19 pandemic. The need for traveling nurses and other healthcare professionals was more evident during the lockdown period. A data service was architected for job feed processing using an orchestration of natural la
    
[^63]: PADME-SoSci：社会科学中用于分析和分布式机器学习的平台

    PADME-SoSci: A Platform for Analytics and Distributed Machine Learning for the Social Sciences. (arXiv:2303.18200v1 [cs.CR])

    [http://arxiv.org/abs/2303.18200](http://arxiv.org/abs/2303.18200)

    PADME-SoSci是一个联邦学习平台，通过在原数据位置进行学习来实现数据所有权保护和跨位置数据分析。

    

    在社会数据科学中，数据隐私和所有权是非常重要的，并提出了法律和伦理问题。当不同方拥有数据的不同部分时，共享和分析数据变得困难。一种应对挑战的方法是在收集数据进行分析之前将数据应用去识别化或匿名化技术。然而，这样做可能会降低数据效用并增加重新识别的风险。为了解决这些局限性，我们提出了PADME，这是一个分布式分析工具，它联邦了模型实现和训练。PADME使用联邦方法，模型由所有方实现和部署，并逐步访问每个数据位置进行训练。这使得可以跨位置分析数据，同时仍允许像所有数据都在同一位置一样训练模型。在数据的原始位置上训练模型可以保留数据所有权。此外，只有在所有数据位置上的分析都完成后，才会提供结果。

    Data privacy and ownership are significant in social data science, raising legal and ethical concerns. Sharing and analyzing data is difficult when different parties own different parts of it. An approach to this challenge is to apply de-identification or anonymization techniques to the data before collecting it for analysis. However, this can reduce data utility and increase the risk of re-identification. To address these limitations, we present PADME, a distributed analytics tool that federates model implementation and training. PADME uses a federated approach where the model is implemented and deployed by all parties and visits each data location incrementally for training. This enables the analysis of data across locations while still allowing the model to be trained as if all data were in a single location. Training the model on data in its original location preserves data ownership. Furthermore, the results are not provided until the analysis is completed on all data locations to
    
[^64]: 今天的迭代学习算法有多高效？

    How Efficient Are Today's Continual Learning Algorithms?. (arXiv:2303.18171v1 [cs.CV])

    [http://arxiv.org/abs/2303.18171](http://arxiv.org/abs/2303.18171)

    这篇论文研究了增量班级学习的最新方法，并指出许多方法在计算、内存和存储方面非常低效。为了使迭代学习在现实世界中具有适用性，研究界不能忽视这些算法使用的资源。

    

    监督式迭代学习涉及从不断增长的带标签数据流中更新深度神经网络（DNN）。尽管大部分工作集中在克服灾难性遗忘上，但迭代学习背后的主要动机之一是能够有效地更新网络，而不是随着训练数据集随时间增长，从头开始重新训练。尽管最近的迭代学习方法基本上解决了灾难遗忘问题，但对这些算法的效率关注不足。在这里，我们研究了增量班级学习的最新方法，并表明许多方法在计算、内存和存储方面非常低效。有些方法甚至需要更多的计算资源才能完成训练！我们认为，为了使迭代学习在现实世界中具有适用性，研究界不能忽视这些算法使用的资源。迭代学习不仅仅是缓解灾难性遗忘。

    Supervised Continual learning involves updating a deep neural network (DNN) from an ever-growing stream of labeled data. While most work has focused on overcoming catastrophic forgetting, one of the major motivations behind continual learning is being able to efficiently update a network with new information, rather than retraining from scratch on the training dataset as it grows over time. Despite recent continual learning methods largely solving the catastrophic forgetting problem, there has been little attention paid to the efficiency of these algorithms. Here, we study recent methods for incremental class learning and illustrate that many are highly inefficient in terms of compute, memory, and storage. Some methods even require more compute than training from scratch! We argue that for continual learning to have real-world applicability, the research community cannot ignore the resources used by these algorithms. There is more to continual learning than mitigating catastrophic forg
    
[^65]: 两种KFAC二级方法在深度神经网络训练中的分析与比较

    Analysis and Comparison of Two-Level KFAC Methods for Training Deep Neural Networks. (arXiv:2303.18083v1 [cs.LG])

    [http://arxiv.org/abs/2303.18083](http://arxiv.org/abs/2303.18083)

    本文研究了两种KFAC二级方法，用于在训练深度神经网络中恢复层间低频交互，研究结果发现这种方法并未显著提高性能。

    

    作为二阶方法，自然梯度下降（NGD）可以加速神经网络的训练。然而，由于计算和反演费舍尔信息矩阵（FIM）的代价过高，需要高效的近似方法，以使NGD可扩展到深度神经网络（DNN）。已经尝试了许多这样的近似方法。其中最复杂的是KFAC，它将FIM近似为一个块对角矩阵，其中每个块对应于神经网络的一层. 本文通过二级方法，探讨通过使用不同的粗略空间还原一些低频层间交互的方法的利益。实验结果表明，以这种方式将层间交互结合起来并不能真正提高性能。

    As a second-order method, the Natural Gradient Descent (NGD) has the ability to accelerate training of neural networks. However, due to the prohibitive computational and memory costs of computing and inverting the Fisher Information Matrix (FIM), efficient approximations are necessary to make NGD scalable to Deep Neural Networks (DNNs). Many such approximations have been attempted. The most sophisticated of these is KFAC, which approximates the FIM as a block-diagonal matrix, where each block corresponds to a layer of the neural network. By doing so, KFAC ignores the interactions between different layers. In this work, we investigate the interest of restoring some low-frequency interactions between the layers by means of two-level methods. Inspired from domain decomposition, several two-level corrections to KFAC using different coarse spaces are proposed and assessed. The obtained results show that incorporating the layer interactions in this fashion does not really improve the perform
    
[^66]: 利用强化学习将一个中等大小的英文GPT模型对齐到西班牙语的小封闭领域中

    Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v1 [cs.CL])

    [http://arxiv.org/abs/2303.17649](http://arxiv.org/abs/2303.17649)

    本文介绍了一种将英文GPT模型对齐到西班牙语的小封闭领域中的方法，该方法使用了奖励模型来改进答案的解码和生成，在问答任务中取得了良好的结果。

    

    本文提出了一种方法，将原本用于开放领域的中等大小英文GPT模型，对齐到西班牙语的小封闭领域。该模型被精细调整用于问答任务。为了实现这一目标，我们还需要训练和实现另一个神经网络（我们称之为奖励模型），以评分并确定答案是否适用于给定的问题。该组件有助于改进系统回答的解码和生成。 BLEU和perplexity等数字度量标准被用于评估模型，同时也使用人类判断来比较解码技术与其他技术。最终，结果支持了所提出的方法，并确定使用奖励模型来对齐生成回答是可行的。

    In this paper, we propose a methodology to align a medium-sized GPT model, originally trained in English for an open domain, to a small closed domain in Spanish. The application for which the model is finely tuned is the question answering task. To achieve this we also needed to train and implement another neural network (which we called the reward model) that could score and determine whether an answer is appropriate for a given question. This component served to improve the decoding and generation of the answers of the system. Numerical metrics such as BLEU and perplexity were used to evaluate the model, and human judgment was also used to compare the decoding technique with others. Finally, the results favored the proposed method, and it was determined that it is feasible to use a reward model to align the generation of responses.
    
[^67]: 基于自适应细化和康托洛维奇度量的数据驱动抽象（扩展版）

    Data-driven abstractions via adaptive refinements and a Kantorovich metric [extended version]. (arXiv:2303.17618v1 [cs.LG])

    [http://arxiv.org/abs/2303.17618](http://arxiv.org/abs/2303.17618)

    我们提出了一种基于自适应细化和康托洛维奇度量的智能且可扩展的动态系统抽象技术，并且定义了一种马尔可夫链之间的度量用作损失函数。我们的方法具有更好的计算复杂度。

    

    我们介绍了一种智能且可扩展的动态系统抽象自适应细化技术。我们的技术依赖于根据未来输出的观察将状态空间划分。然而，这种知识是动态地以不对称的方式构建的。为了学习最优结构，我们定义了马尔可夫链之间的康托洛维奇度量，并将其用作损失函数。我们的技术适用于数据驱动的框架，但不受限于此。我们还研究了马尔可夫链之间上述度量的性质，我们认为这可能具有更广泛的应用。我们提出了一种近似计算该度量的算法，并且我们展示了我们的方法比使用传统的线性规划技术具有更好的计算复杂度。

    We introduce an adaptive refinement procedure for smart, and scalable abstraction of dynamical systems. Our technique relies on partitioning the state space depending on the observation of future outputs. However, this knowledge is dynamically constructed in an adaptive, asymmetric way. In order to learn the optimal structure, we define a Kantorovich-inspired metric between Markov chains, and we use it as a loss function. Our technique is prone to data-driven frameworks, but not restricted to.  We also study properties of the above mentioned metric between Markov chains, which we believe could be of application for wider purpose. We propose an algorithm to approximate it, and we show that our method yields a much better computational complexity than using classical linear programming techniques.
    
[^68]: HuggingGPT: 在HugingFace中使用ChatGPT及其伙伴解决AI任务

    HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])

    [http://arxiv.org/abs/2303.17580](http://arxiv.org/abs/2303.17580)

    用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。

    

    解决不同领域和模态的复杂AI任务是通向人工智能的关键步骤。本文提出了一个系统，利用大型语言模型（LLMs）作为控制器来管理现有的AI模型以解决AI任务，语言成为通用接口来赋能它。具体来说，我们使用ChatGPT作为任务规划工具，根据HuggingFace中可用的模型功能描述来选择模型，在选定AI模型的情况下执行每个子任务，并总结响应。

    Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
    
[^69]: 使用人工智能在家中测量帕金森病的严重程度

    Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])

    [http://arxiv.org/abs/2303.17573](http://arxiv.org/abs/2303.17573)

    该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。

    

    我们提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法。参与者在网络摄像头前完成了运动任务（即点击手指），250名全球参与者的数据按照运动障碍协会统一帕金森病评分量表 (MDS-UPDRS) 的标准由三名专家神经学家进行了评估。神经学家的评估具有高度的可靠性，内部一致性系数（ICC）为0.88。我们开发了计算机算法来获得与MDS-UPDRS指南一致且与神经学家的评估高度相关的客观测量结果。我们的机器学习模型在这些指标的训练下表现优于一个MDS-UPDRS认证的评分者，平均绝对误差（MAE）为0.59，而评分者的MAE为0.79。然而，该模型的表现略逊于专家神经学家（0.53 MAE）。该方法可重复用于类似的运动任务，提供了可能性。

    We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
    
[^70]: HARFLOW3D：一种面向FPGA设备的基于延迟的3D-CNN加速器工具链

    HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices. (arXiv:2303.17218v1 [cs.AR])

    [http://arxiv.org/abs/2303.17218](http://arxiv.org/abs/2303.17218)

    本研究提出了一种面向FPGA设备的基于延迟的3D-CNN加速器工具链HARFLOW3D，它以机器学习模型和FPGA的特性描述为输入，生成最小化计算延迟的设计。实验证明HARFLOW3D相比其他方案能够实现更低的延迟。

    

    3D卷积神经网络已被证明在人体动作识别任务中具有高效性和最先进的结果。本研究引入一种新的基于流式架构的工具链，将此类模型映射到FPGA上，考虑模型固有特性和目标FPGA设备的特征。HARFLOW3D工具链以ONNX格式的3D卷积神经网络和FPGA特性描述为输入，生成最小化计算延迟的设计。该工具链由多个部分组成，包括i) 3D CNN解析器，ii) 性能和资源模型，iii) 用于在生成的硬件上执行3D模型的调度算法，iv) 针对3D模型量身定制的资源感知优化引擎，v) 自动映射到可合成的FPGA代码。通过对各种3D CNN和FPGA系统配对进行多个实验，展示了工具链支持广泛模型和设备的能力。此外，与其他最先进的3D CNN加速器设计方法相比，该工具链实现了更低的延迟。

    For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furth
    
[^71]: 论文海报：训练DNN中偏差、节点敏感性和长尾分布之间的链接 (arXiv:2303.16589v1 [cs.LG])

    Poster: Link between Bias, Node Sensitivity and Long-Tail Distribution in trained DNNs. (arXiv:2303.16589v1 [cs.LG])

    [http://arxiv.org/abs/2303.16589](http://arxiv.org/abs/2303.16589)

    DNN训练中长尾分布的数据集将给不同输出类别提供不同的分类性能，本文首次指出导致节点敏感性变化的节点偏差，提出了开放性挑战。

    

    深度神经网络(DNNs)由于其卓越的学习(和重新学习)能力，被广泛应用于各种现实世界的应用场景。然而，这些数据驱动的机器学习模型的学习效果一般取决于数据的质量和分布。因此，分布呈现长尾分布的训练数据集对DNNs构成了挑战，因为这些训练的DNNs可能对不同的输出类别提供不同程度的分类性能。虽然现有的研究已经强调了这些网络的整体偏差，但本文首次指出了导致节点对不同输出类别敏感性变化的节点偏差。据我们所知，这是第一篇强调DNNs中这种独特挑战的工作，讨论其可能的原因，并为这个新的研究方向提供了开放性挑战。我们使用真实情境数据集上训练的网络的实证案例来支持我们的推理。

    Owing to their remarkable learning (and relearning) capabilities, deep neural networks (DNNs) find use in numerous real-world applications. However, the learning of these data-driven machine learning models is generally as good as the data available to them for training. Hence, training datasets with long-tail distribution pose a challenge for DNNs, since the DNNs trained on them may provide a varying degree of classification performance across different output classes. While the overall bias of such networks is already highlighted in existing works, this work identifies the node bias that leads to a varying sensitivity of the nodes for different output classes. To the best of our knowledge, this is the first work highlighting this unique challenge in DNNs, discussing its probable causes, and providing open challenges for this new research direction. We support our reasoning using an empirical case study of the networks trained on a real-world dataset.
    
[^72]: 何时预训练图神经网络？基于数据生成视角的回答！

    When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!. (arXiv:2303.16458v1 [cs.LG])

    [http://arxiv.org/abs/2303.16458](http://arxiv.org/abs/2303.16458)

    本文提出了一个通用框架W2PGNN，旨在回答何时预训练图神经网络的关键问题。使用新的角度探索了从预训练数据到下游数据的复杂生成机制。

    

    最近，图预训练在学术界引起了广泛关注，旨在从未标记的图数据中学习可转移知识，以提高下游性能。尽管最近的尝试，但负面迁移是将图预训练模型应用于下游任务时的重大问题。现有工作通过设计多种图预训练和微调策略，致力于解决何时预训练和如何预训练的问题。然而，有时候无论策略如何先进，“预训练和微调”范式仍然无法带来明显的好处。本文引入了一个通用框架W2PGNN来回答何时预训练的关键问题（即我们在什么情况下可以利用图预训练），然后再进行费力的预训练或微调。我们从一个新的角度探索了从预训练数据到下游数据的复杂生成机制。

    Recently, graph pre-training has attracted wide research attention, which aims to learn transferable knowledge from unlabeled graph data so as to improve downstream performance. Despite these recent attempts, the negative transfer is a major issue when applying graph pre-trained models to downstream tasks. Existing works made great efforts on the issue of what to pre-train and how to pre-train by designing a number of graph pre-training and fine-tuning strategies. However, there are indeed cases where no matter how advanced the strategy is, the "pre-train and fine-tune" paradigm still cannot achieve clear benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (i.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN 
    
[^73]: Dice半度量损失函数：用软标签优化Dice分数

    Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v1 [cs.CV])

    [http://arxiv.org/abs/2303.16296](http://arxiv.org/abs/2303.16296)

    本文提出的Dice半度量损失函数可在软标签设置中使用，在医疗成像领域的分割方案中与使用软标签的研究相结合，可以获得更好的Dice分数和模型校准。

    

    在医学成像领域的许多自动分割方案中，软Dice损失（SDL）发挥了关键作用。在过去几年中，人们已经揭示了其优越性能背后的一些原因并进一步探索了其优化。然而，目前还没有实现支持直接在软标签设置中使用它的方案。因此，在使用SDL和研究利用软标签的同时进行模型校准的协同作用仍然缺失。在本文中，我们介绍了Dice半度量损失函数（DMLs），它们（i）在硬标签的标准设置下与SDL相同，但（ii）也可在软标签设置中使用。我们在公共的QUBIQ、LiTS和KiTS基准测试上的实验证实了DMLs与软标签（如平均、标签平滑和知识蒸馏）的潜在协同作用，而DMLs与硬标签（如大多数投票和随机选择）相比，产生了更优秀的Dice分数和模型校准。

    The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calib
    
[^74]: 深度集合在多输出回归任务中量化校准不确定性的探究

    Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task. (arXiv:2303.16210v1 [cs.LG])

    [http://arxiv.org/abs/2303.16210](http://arxiv.org/abs/2303.16210)

    本研究探究了在多输出回归任务中应用深度集合量化校准不确定性的方法，提出了该方法的改进框架，其在回归准确性、不确定性估计可靠性和训练效率方面具有优越表现。

    

    深度集合是逼近贝叶斯推断的一种简单直接的方法，已被成功应用于许多分类任务。本研究旨在全面探究该方法在多输出回归任务中的应用，以预测导弹结构的空气动力性能。通过仔细研究集合中神经网络数量的影响，观察到估计的不确定性普遍存在低估的趋势。在此背景下，提出了一种应用事后校准的深度集合框架，并证明其改进的不确定性量化性能。直观地将其与高斯过程回归进行比较，这是工程中最常用的不确定性量化模型，结果表明在回归准确性、估计不确定性的可靠性和训练效率方面具有卓越的表现。最后，本文也研究了所提出框架对贝叶斯优化结果的影响。

    Deep ensemble is a simple and straightforward approach for approximating Bayesian inference and has been successfully applied to many classification tasks. This study aims to comprehensively investigate this approach in the multi-output regression task to predict the aerodynamic performance of a missile configuration. By scrutinizing the effect of the number of neural networks used in the ensemble, an obvious trend toward underconfidence in estimated uncertainty is observed. In this context, we propose the deep ensemble framework that applies the post-hoc calibration method, and its improved uncertainty quantification performance is demonstrated. It is compared with Gaussian process regression, the most prevalent model for uncertainty quantification in engineering, and is proven to have superior performance in terms of regression accuracy, reliability of estimated uncertainty, and training efficiency. Finally, the impact of the suggested framework on the results of Bayesian optimizatio
    
[^75]: 基于扩散映射的群不变流形学习

    Diffusion Maps for Group-Invariant Manifolds. (arXiv:2303.16169v1 [cs.LG])

    [http://arxiv.org/abs/2303.16169](http://arxiv.org/abs/2303.16169)

    本文提出了一种扩散映射算法用于群不变流形问题，通过积分在不变数据集上扩展出K-不变拉普拉斯算子，证明了可以利用K中的幺正不可约表示矩阵对其进行对角化，并给出特征值和特征向量的计算公式。同时，展示了规范化拉普拉斯算子L_N收敛于Laplace-Beltrami算子，收敛速度随着对称群K的维数增加而增加。

    

    本文考虑当数据集对紧Lie群K的作用具有不变性时，流形学习问题。我们的方法是通过在现有数据点K的轨道上积分，将数据诱导的图拉普拉斯算子扩展到K-不变算子L上。我们证明了可以使用K的幺正不可约表示矩阵来对角化K-不变算子L，并给出了计算L的特征值和特征向量的显式公式。此外，我们展示了规范化拉普拉斯算子L_N收敛到数据流形的Laplace-Beltrami算子，收敛速度得到改进，改进随着对称群K的维数增加而增加。本文将Landa和Shkolnisky的可转动图拉普拉斯框架从SO（2）的情况扩展到任意紧Lie群情况。

    In this article, we consider the manifold learning problem when the data set is invariant under the action of a compact Lie group $K$. Our approach consists in augmenting the data-induced graph Laplacian by integrating over orbits under the action of $K$ of the existing data points. We prove that this $K$-invariant Laplacian operator $L$ can be diagonalized by using the unitary irreducible representation matrices of $K$, and we provide an explicit formula for computing the eigenvalues and eigenvectors of $L$. Moreover, we show that the normalized Laplacian operator $L_N$ converges to the Laplace-Beltrami operator of the data manifold with an improved convergence rate, where the improvement grows with the dimension of the symmetry group $K$. This work extends the steerable graph Laplacian framework of Landa and Shkolnisky from the case of $\operatorname{SO}(2)$ to arbitrary compact Lie groups.
    
[^76]: TraffNet：学习道路网络数字孪生交通生成因果关系

    TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])

    [http://arxiv.org/abs/2303.15954](http://arxiv.org/abs/2303.15954)

    TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。

    

    道路网络数字孪生（RNDT）在开发下一代智能交通系统中发挥着关键作用，可以实现更精确的交通规划和控制。为了支持实时决策，RNDT需要一个模型，从在线传感器数据中动态学习交通模式并生成高保真模拟结果。尽管基于图神经网络的当前交通预测技术已经实现了最先进的性能，但是这些技术仅通过挖掘历史交通数据中的相关性来预测未来交通，而忽略了交通生成的原因，例如交通需求和路径选择。因此，它们的性能对于实时决策是不可靠的。为了填补这一差距，我们引入了一个新的深度学习框架称为 TraffNet，该框架从车辆轨迹数据中学习交通量的因果性。首先，我们使用异构图来表示道路网络，使模型能够并入预测所需的其他数据，然后我们提出了一种新颖的递归神经网络结构，从而能够预测交通量的因果联系。

    Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
    
[^77]: 带有聚合梯度的快速收敛联邦学习

    Fast Convergence Federated Learning with Aggregated Gradients. (arXiv:2303.15799v1 [cs.LG])

    [http://arxiv.org/abs/2303.15799](http://arxiv.org/abs/2303.15799)

    该论文提出了一种带有聚合梯度的快速收敛联邦学习方法，通过引入均值场方法来完成参数和梯度的聚合步骤，该方法在收敛速度和通信成本方面优于传统方法。

    

    联邦学习（FL）是一种新的机器学习框架，它使多个分布式设备在保护本地数据的同时，通过中央服务器协同训练共享模型。然而，非独立和同分布（Non-IID）的数据样本以及参与者之间频繁的通信将减缓收敛速率并增加通信成本。为了实现快速收敛，我们通过在常规本地更新规则中引入聚合梯度来改善本地梯度下降方法，并提出一种自适应学习率算法，在每次迭代中进一步考虑本地参数和全局参数的偏差。以上策略要求在每个本地迭代中收集所有客户端的本地参数和梯度，由于本地更新期间没有通信，这是具有挑战性的。因此，我们利用均值场方法，引入称为全局均值场和本地均值场的两个均值场术语来完成聚合步骤。实验结果表明，我们提出的方法在收敛速度和通信成本方面优于传统方法。

    Federated Learning (FL) is a novel machine learning framework, which enables multiple distributed devices cooperatively training a shared model scheduled by a central server while protecting private data locally. However, the non-independent-and-identically-distributed (Non-IID) data samples and frequent communication among participants will slow down the convergent rate and increase communication costs. To achieve fast convergence, we ameliorate the local gradient descend approach in conventional local update rule by introducing the aggregated gradients at each local update epoch, and propose an adaptive learning rate algorithm that further takes the deviation of local parameter and global parameter into consideration at each iteration. The above strategy requires all clients' local parameters and gradients at each local iteration, which is challenging as there is no communication during local update epochs. Accordingly, we utilize mean field approach by introducing two mean field ter
    
[^78]: 新熵方法的自适应联邦学习

    Adaptive Federated Learning via New Entropy Approach. (arXiv:2303.14966v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2303.14966](http://arxiv.org/abs/2303.14966)

    本文提出了一种新的自适应学习率方案，基于熵理论缓解异构客户端之间的偏差，实现全局模型的快速收敛。

    

    联邦学习 (FL) 是一种新兴的框架，它允许资源受限的离散客户端在中央服务器的协调下，通过在本地存储保护隐私数据的方式，共同学习全局模型。然而，由于异构客户端的设备和数据差异会导致本地模型参数的偏差，进而导致全局模型的收敛速度减慢和精度降低。当前的 FL 算法普遍采用静态客户端学习策略并不能适应不同客户端的动态训练参数。在本文中，我们根据熵理论考虑不同本地模型参数之间的偏差，为每个客户端提出了基于熵理论的自适应学习率方案，以缓解异构客户端之间的偏差，实现全局模型的快速收敛。但由于不同客户端的本地数据集和特征具有显著的差异，设计每个客户端的最优动态学习率是困难的。

    Federated Learning (FL) has recently emerged as a popular framework, which allows resource-constrained discrete clients to cooperatively learn the global model under the orchestration of a central server while storing privacy-sensitive data locally. However, due to the difference in equipment and data divergence of heterogeneous clients, there will be parameter deviation between local models, resulting in a slow convergence rate and a reduction of the accuracy of the global model. The current FL algorithms use the static client learning strategy pervasively and can not adapt to the dynamic training parameters of different clients. In this paper, by considering the deviation between different local model parameters, we propose an adaptive learning rate scheme for each client based on entropy theory to alleviate the deviation between heterogeneous clients and achieve fast convergence of the global model. It's difficult to design the optimal dynamic learning rate for each client as the lo
    
[^79]: 通过反向特征投影在不断学习中维护线性可分性

    Preserving Linear Separability in Continual Learning by Backward Feature Projection. (arXiv:2303.14595v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.14595](http://arxiv.org/abs/2303.14595)

    提出了一种名为BFP的连续学习方法，它通过将新特征变换为旧特征的线性变换来维护线性可分性，从而允许新特征方向的出现以适应新任务，同时保留旧任务的信息。

    

    在不断学习中，灾难性遗忘一直是一个重大挑战，因为模型需要在有限或没有以前查看任务的数据情况下学习新任务。为了解决这个挑战，基于特征空间知识蒸馏的方法已被提出并证明可以减少遗忘。然而，大多数特征蒸馏方法直接约束新特征以匹配旧特征，忽视了可塑性的需求。为了实现更好的稳定性-可塑性平衡，我们提出了Backward Feature Projection（BFP），这是一种连续学习方法，允许新特征在旧特征的可学习线性变换中发生变化。BFP保留旧类别的线性可分性，同时允许新的特征方向出现以适应新的类别。BFP可以与现有的经验重播方法集成，并显著提高性能。我们还证明，BFP有助于学习更好的表示空间。

    Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space,
    
[^80]: TRAK: 刻画大规模模型行为

    TRAK: Attributing Model Behavior at Scale. (arXiv:2303.14186v1 [stat.ML])

    [http://arxiv.org/abs/2303.14186](http://arxiv.org/abs/2303.14186)

    TRAK是一种适用于大规模、可微模型的数据归因方法，既有效又计算量可行。

    

    数据归因的目标是追踪模型预测结果的原始训练数据。虽然已经有很多工作致力于实现这一目标，但现有方法往往要求用户在计算效率和准确性之间做出选择。也就是说，在非凸场景（例如，深度神经网络领域）中，计算量可行的方法可能难以准确地归因模型预测结果，而在这类场景中有效的方法则需要训练数千个模型，这使得它们在大型模型或数据集中实际应用具有不可行性。在本文中，我们介绍了TRAK（随机投影核追踪），这是一种数据归因方法，适用于大规模、可微模型，既有效又计算量可行。具体来说，通过仅使用少量训练模型，TRAK 可以匹配需要训练数千模型才能得到的归因方法的性能。我们论证了TRAK 在各种模式和规模上的实用性。

    The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets.  In this work, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of TRAK across various modalities and scal
    
[^81]: 物理学指导的PointNet：它能同时解决多少不规则几何体的反问题？以线弹性为例。

    Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity. (arXiv:2303.13634v1 [cs.LG])

    [http://arxiv.org/abs/2303.13634](http://arxiv.org/abs/2303.13634)

    本文提出了一种物理学指导的神经网络模型PIPN，它能利用稀疏标记数据同时预测所需偏微分方程在数百个不同的几何体上的解，有望在工业界进行快速的几何设计。

    

    常规的物理学指导的神经网络（PINN）利用稀疏标记数据预测偏微分方程的解，但只限于单一的域。相反，完全监督学习模型通常是首先在已知解（即标记数据）的几千个域上进行训练，然后预测在一些未知域上的解。物理学指导的PointNet（PIPN）主要旨在填补PINN（作为弱监督学习模型）和完全监督学习模型之间的差距。在本文中，我们展示了PIPN能够同时预测所需偏微分方程在数百个域上的解，而只使用稀疏标记数据。这个框架有助于在工业界进行快速的几何设计，尤其当只有稀疏标记数据可用时。特别地，我们展示了PIPN能够同时预测平面应力问题在500多个不同几何体上的解。

    Regular physics-informed neural networks (PINNs) predict the solution of partial differential equations using sparse labeled data but only over a single domain. On the other hand, fully supervised learning models are first trained usually over a few thousand domains with known solutions (i.e., labeled data) and then predict the solution over a few hundred unseen domains. Physics-informed PointNet (PIPN) is primarily designed to fill this gap between PINNs (as weakly supervised learning models) and fully supervised learning models. In this article, we demonstrate that PIPN predicts the solution of desired partial differential equations over a few hundred domains simultaneously, while it only uses sparse labeled data. This framework benefits fast geometric designs in the industry when only sparse labeled data are available. Particularly, we show that PIPN predicts the solution of a plane stress problem over more than 500 domains with different geometries, simultaneously. Moreover, we pio
    
[^82]: 语音合成的音频扩散模型：基于生成AI的文本到语音和语音增强的概述

    Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI. (arXiv:2303.13336v1 [cs.SD])

    [http://arxiv.org/abs/2303.13336](http://arxiv.org/abs/2303.13336)

    此文介绍了音频扩散模型，重点讨论了两个活跃任务：文本到语音和语音增强，并对实验结果进行了比较和讨论。

    

    生成AI在各个领域表现出了惊人的性能，其中语音合成是一个有趣的方向。随着扩散模型成为最流行的生成模型，许多工作已经尝试了两个活跃任务：文本到语音和语音增强。本文对音频扩散模型进行了概述，这是对现有调查的补充，这些调查要么缺乏基于扩散的语音合成的最新进展，要么强调在多个领域应用扩散模型的整体情况。具体而言，本文首先简要介绍了音频和扩散模型的背景。对于文本到语音任务，我们将方法分为三类，基于扩散模型采用的阶段：声学模型、声码器和端到端框架。此外，我们通过将某些信号从输入语音中删除或添加来将各种语音增强任务进行分类。本文还涵盖了实验结果的比较和讨论。

    Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered 
    
[^83]: 对抗攻击的测试时间防御：基于遮蔽自编码器的对抗样本检测和重构

    Test-time Defense against Adversarial Attacks: Detection and Reconstruction of Adversarial Examples via Masked Autoencoder. (arXiv:2303.12848v1 [cs.CV])

    [http://arxiv.org/abs/2303.12848](http://arxiv.org/abs/2303.12848)

    该方法使用遮蔽自编码器进行对抗攻击检测和重构，不需要在测试时间更新模型权重，也不需要使用更多的对抗样本来增强训练集。

    

    现有的对抗攻击防御方法可以分为训练时间和测试时间防御。训练时间防御需要大量的额外训练时间，通常无法推广到未见过的攻击。而测试时间防御需要访问（部分）模型权重以执行梯度下降，这对于冻结权重的模型可能不可行。为了解决这些挑战，我们提出了一种新的防御方法DRAM，它使用遮蔽自编码器（MAE）检测并重构多种类型的对抗攻击。我们演示了如何使用MAE损失构建KS测试来检测对抗攻击。此外，MAE损失可以用于修复未见攻击类型的对抗样本。因此，DRAM既不需要在测试时间更新模型权重，也不需要使用更多的对抗样本来增强训练集。在大规模的ImageN数据集上评估DRAM，实验结果表明其具有很高的鲁棒性和有效性。

    Existing defense methods against adversarial attacks can be categorized into training time and test time defenses. Training time defense, i.e., adversarial training, requires a significant amount of extra time for training and is often not able to be generalized to unseen attacks. On the other hand, test time defense by test time weight adaptation requires access to perform gradient descent on (part of) the model weights, which could be infeasible for models with frozen weights. To address these challenges, we propose DRAM, a novel defense method to Detect and Reconstruct multiple types of Adversarial attacks via Masked autoencoder (MAE). We demonstrate how to use MAE losses to build a KS-test to detect adversarial attacks. Moreover, the MAE losses can be used to repair adversarial samples from unseen attack types. In this sense, DRAM neither requires model weight updates in test time nor augments the training set with more adversarial samples. Evaluating DRAM on the large-scale ImageN
    
[^84]: 大型视觉语言模型零样本推理中的校准方法研究

    Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models. (arXiv:2303.12748v1 [cs.CV])

    [http://arxiv.org/abs/2303.12748](http://arxiv.org/abs/2303.12748)

    本文研究了零样本推理中视觉语言模型的校准问题，发现CLIP存在误校准，并提出了一种修改版的温度缩放方法，可以适用于每个特定的CLIP模型。

    

    深度学习模型的校准对于保证其可靠性和安全使用是至关重要的，因此在监督分类模型中对其进行了广泛研究，提出了降低误校准的方法。然而，视觉语言模型在进行零样本推理时的校准尚未得到全面的研究，例如CLIP。本研究衡量了跨相关变量（如提示，数据集和架构）的校准情况，并发现CLIP的零样本推理存在误校准。此外，我们提出了一种修改版的温度缩放方法，与CLIP作为零样本推理模型的常见用例相一致，并展示出单个学习的温度值可以广泛适用于每个特定的CLIP模型（由选定的预训练数据集和架构定义），跨不同的推理数据集和提示选择。

    Calibration of deep learning models is crucial to their trustworthiness and safe usage, and as such, has been extensively studied in supervised classification models, with methods crafted to decrease miscalibration. However, there has yet to be a comprehensive study of the calibration of vision-language models that are used for zero-shot inference, like CLIP. We measure calibration across relevant variables like prompt, dataset, and architecture, and find that zero-shot inference with CLIP is miscalibrated. Furthermore, we propose a modified version of temperature scaling that is aligned with the common use cases of CLIP as a zero-shot inference model, and show that a single learned temperature generalizes for each specific CLIP model (defined by a chosen pre-training dataset and architecture) across inference dataset and prompt choice.
    
[^85]: 异构网络中联邦支持向量机的多任务模型个性化

    Multi-Task Model Personalization for Federated Supervised SVM in Heterogeneous Networks. (arXiv:2303.10254v1 [cs.LG])

    [http://arxiv.org/abs/2303.10254](http://arxiv.org/abs/2303.10254)

    本文提出了一种用于异构联邦网络中的高效分布式迭代学习方法，通过支持向量机实现对联邦分类和回归任务的处理，并支持个性化的学习模型。为了保护隐私，引入了一种随机掩码过程。研究结果表明，所提出的方法对于解决异构网络中联邦学习任务是有效的。

    

    本文设计了一种基于支持向量机（SVM）的高效分布式迭代学习方法，用于处理联邦分类和回归。该方法支持在异构节点网络中进行高效的计算和模型交换，并允许在存在非独立同分布数据的情况下学习模型的个性化。为进一步提高隐私保护，我们引入了一种随机掩码过程，有助于避免数据反演。最后，我们分析了所提出的隐私机制以及参与者硬件和数据的异构性对系统性能的影响。

    In this paper, we design an efficient distributed iterative learning method based on support vector machines (SVMs), which tackles federated classification and regression. The proposed method supports efficient computations and model exchange in a network of heterogeneous nodes and allows personalization of the learning model in the presence of non-i.i.d. data. To further enhance privacy, we introduce a random mask procedure that helps avoid data inversion. Finally, we analyze the impact of the proposed privacy mechanisms and the heterogeneity of participant hardware and data on the system performance.
    
[^86]: 数据中心人工智能综述：一份调查报告。

    Data-centric Artificial Intelligence: A Survey. (arXiv:2303.10158v1 [cs.LG])

    [http://arxiv.org/abs/2303.10158](http://arxiv.org/abs/2303.10158)

    本文讨论了数据中心人工智能的必要性并从三个一般性数据中心目标和代表性方法的全面视角进行了介绍。该综述从自动化和协作的角度组织了现有文献并讨论了挑战。

    

    人工智能（AI）正在几乎所有领域产生深远的影响，其成功的关键之一是可用于构建机器学习模型的丰富高质量数据。最近，数据在AI中的作用得到了显著放大，引发了数据中心AI这一新兴概念的出现。研究人员和从业者的注意力逐渐从推进模型设计转向提高数据质量和数量。在本调查中，我们讨论了数据中心AI的必要性，随后从训练数据开发、推理数据开发和数据维护三个一般性数据中心目标以及代表性方法的全面视角进行了介绍。我们还从自动化和协作的角度组织了现有文献，讨论了挑战，并列出了各种任务的测试基准。我们认为，这是第一份提供跨越各个阶段一系列任务的全球视角的综合性调查。

    Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages o
    
[^87]: SUD$^2$:基于去噪扩散模型的图像重建监督方法

    SUD$^2$: Supervision by Denoising Diffusion Models for Image Reconstruction. (arXiv:2303.09642v1 [cs.CV])

    [http://arxiv.org/abs/2303.09642](http://arxiv.org/abs/2303.09642)

    本文提出了一种适用于成对训练数据不足的图像重建网络训练的广义框架，并且在缺少成对训练数据的情况下，使用去噪扩散模型进行网络训练的监督。

    

    许多图像反问题（如图像修复和去雾）都非常具有挑战性，因为它们的前向模型是未知的，或者依赖于未知的潜在参数。虽然我们可以通过使用大量成对的训练数据来训练神经网络来解决这些问题，但这样的成对训练数据通常是不可用的。本文提出了一种广义的框架，用于在成对训练数据不足的情况下训练图像重建网络。特别地，我们展示了图像去噪算法和去噪扩散模型的能力，在缺少成对训练数据的情况下监督网络训练。

    Many imaging inverse problems$\unicode{x2014}$such as image-dependent in-painting and dehazing$\unicode{x2014}$are challenging because their forward models are unknown or depend on unknown latent parameters. While one can solve such problems by training a neural network with vast quantities of paired training data, such paired training data is often unavailable. In this paper, we propose a generalized framework for training image reconstruction networks when paired training data is scarce. In particular, we demonstrate the ability of image denoising algorithms and, by extension, denoising diffusion models to supervise network training in the absence of paired training data.
    
[^88]: 基于块的变压器模型的位压缩

    Block-wise Bit-Compression of Transformer-based Models. (arXiv:2303.09184v1 [cs.CL])

    [http://arxiv.org/abs/2303.09184](http://arxiv.org/abs/2303.09184)

    本论文提出了一种用于Transformer的块位压缩方法，称为BBCT。该方法可以更细粒度地压缩整个Transformer，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。在GLUE数据集上测试结果表明，BBCT可以在大多数任务中实现少于1％的准确率下降。

    

    随着BERT、GPT-3和ChatGPT等近期基于Transformer的模型的流行，自然语言处理任务中取得了最先进的性能。然而，Transformer模型的巨大计算量、巨大的内存占用和高延迟是云计算中不可避免的挑战。为了解决这个问题，我们提出了BBCT方法，它是一种用于Transformer的块位压缩方法，无需重新训练。我们的方法实现了对整个Transformer的更细粒度的压缩，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。我们以高效BERT为案例，使用BBCT方法进行压缩。我们在General Language Understanding Evaluation(GLUE)数据集上的测试结果表明，BBCT在大多数任务中的准确度下降小于1％。

    With the popularity of the recent Transformer-based models represented by BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range of natural language processing tasks. However, the massive computations, huge memory footprint, and thus high latency of Transformer-based models is an inevitable challenge for the cloud with high real-time requirement. To tackle the issue, we propose BBCT, a method of block-wise bit-compression for transformer without retraining. Our method achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient BERT with the method of BBCT. Our benchmark test results on General Language Understanding Evaluation (GLUE) show that BBCT can achieve less than 1% accuracy drop in most tasks.
    
[^89]: 使用复值神经场的物理信息光学核回归

    Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields. (arXiv:2303.08435v1 [cs.CV])

    [http://arxiv.org/abs/2303.08435](http://arxiv.org/abs/2303.08435)

    本文提出了一种新的基于机器学习的光刻模型范式，通过优化复值神经场执行光学核回归并将光刻系统拆解为非参数掩模操作和包含行列式源、瞳孔和光刻信息的学习光学核，使用小规模训练数据集展示了卓越的推广能力。

    

    光刻是集成电路制造的基础，需要大量计算。基于机器学习的光刻模型的发展缓解了制造过程开销和能力之间的平衡。然而，所有以前的方法都将光刻系统视为图像到图像的黑盒映射，利用网络参数通过死记硬背映射大量的掩模到空中或掩模到电阻图像对，导致推广能力不佳。本文提出了一种新的基于机器学习的范式，将严格的光刻模型拆解为非参数掩模操作和包含行列式源、瞳孔和光刻信息的学习光学核。通过优化复值神经场以执行光学核回归，我们的方法可以准确地恢复光刻系统，同时使用较少的参数进行小规模训练数据集，展示了卓越的推广能力。

    Lithography is fundamental to integrated circuit fabrication, necessitating large computation overhead. The advancement of machine learning (ML)-based lithography models alleviates the trade-offs between manufacturing process expense and capability. However, all previous methods regard the lithography system as an image-to-image black box mapping, utilizing network parameters to learn by rote mappings from massive mask-to-aerial or mask-to-resist image pairs, resulting in poor generalization capability. In this paper, we propose a new ML-based paradigm disassembling the rigorous lithographic model into non-parametric mask operations and learned optical kernels containing determinant source, pupil, and lithography information. By optimizing complex-valued neural fields to perform optical kernel regression from coordinates, our method can accurately restore lithography system using a small-scale training dataset with fewer parameters, demonstrating superior generalization capability as w
    
[^90]: 基于视觉、仿生稀疏神经网络的路线跟随

    Vision-based route following by an embodied insect-inspired sparse neural network. (arXiv:2303.08109v1 [cs.NE])

    [http://arxiv.org/abs/2303.08109](http://arxiv.org/abs/2303.08109)

    该论文比较了一种基于仿生稀疏神经网络的FlyHash模型和其他非稀疏模型在路线跟随任务中的效率，发现FlyHash模型在数据编码方面更加高效。

    

    我们比较了一种叫做FlyHash模型（Dasgupta et al., 2017）的仿生稀疏神经网络与其他非稀疏模型在路线跟随任务中的效率。该任务需要模型通过比较当前的视觉输入和沿途存储的记忆来控制转向。我们得出结论：FlyHash模型比其他模型更高效，尤其是在数据编码方面。

    We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network (Dasgupta et al., 2017), to similar but non-sparse models in an embodied navigation task. This requires a model to control steering by comparing current visual inputs to memories stored along a training route. We concluded the FlyHash model is more efficient than others, especially in terms of data encoding.
    
[^91]: 生成AI中的文本到图像扩散模型：一项调查

    Text-to-image Diffusion Model in Generative AI: A Survey. (arXiv:2303.07909v1 [cs.CV])

    [http://arxiv.org/abs/2303.07909](http://arxiv.org/abs/2303.07909)

    本文调查了文本到图像扩散模型以及相关应用，总结了最先进的方法，并探讨了挑战和未来方向。

    

    本文调查了文本到图像扩散模型，这些模型已经成为多种生成任务中流行的模型。作为一个自包含的工作，本调查从简单介绍基本扩散模型如何用于图像合成开始，接着是条件或引导如何改进学习。我们还总结了文本条件下的最先进的图像合成方法，并且进一步总结了文本引导创意生成和图像编辑的应用。除了迄今为止所取得的进展，我们还讨论了现有挑战和有前途的未来方向。

    This survey reviews text-to-image diffusion models in the context that diffusion models have emerged to be popular for a wide range of generative tasks. As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.
    
[^92]: 机器学习基准性能评估中的多重性问题

    What is the state of the art? Accounting for multiplicity in machine learning benchmark performance. (arXiv:2303.07272v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2303.07272](http://arxiv.org/abs/2303.07272)

    机器学习基准性能评估中，最先进的（SOTA）性能的估计值过于乐观，容易导致方法的忽视。本文提供了一个概率模型，用于校正多重性偏差并比较方法的性能。

    

    机器学习方法通常通过在公共数据库中的数据集上的性能来进行评估和比较。这允许多种方法，在相同条件下并跨越时间进行评估。在问题中排名最高的性能被称为最先进的（SOTA）性能，并且被用作新方法出版的参考点。但使用最高排名的性能作为SOTA的估计值是一种有偏的估计器，会给出过于乐观的结果。这种多重性的机制是多重比较和多重检验中广泛研究的主题，但在关于SOTA估计的讨论中几乎没有得到提及。过于乐观的最先进估计值被用作评估新方法的标准，而具有明显劣势结果的方法很容易被忽视。在本文中，我们提供了一个概率模型，用于校正多重性偏差并比较方法的性能。

    Machine learning methods are commonly evaluated and compared by their performance on data sets from public repositories. This allows for multiple methods, oftentimes several thousands, to be evaluated under identical conditions and across time. The highest ranked performance on a problem is referred to as state-of-the-art (SOTA) performance, and is used, among other things, as a reference point for publication of new methods. Using the highest-ranked performance as an estimate for SOTA is a biased estimator, giving overly optimistic results. The mechanisms at play are those of multiplicity, a topic that is well-studied in the context of multiple comparisons and multiple testing, but has, as far as the authors are aware of, been nearly absent from the discussion regarding SOTA estimates. The optimistic state-of-the-art estimate is used as a standard for evaluating new methods, and methods with substantial inferior results are easily overlooked. In this article, we provide a probability 
    
[^93]: 一种多模态仿真框架，实现数字孪生基于动态环境的V2X通信

    A Multi-Modal Simulation Framework to Enable Digital Twin-based V2X Communications in Dynamic Environments. (arXiv:2303.06947v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2303.06947](http://arxiv.org/abs/2303.06947)

    本文提出一种基于数字孪生和多模态仿真框架的V2X通信场景DT创建和仿真方法，可在高移动性V2X通信环境下准确模拟出实际传感器数据和信道。

    

    数字孪生（DT）被提出作为物理无线环境的精确虚拟表示，可在物理通信设备上实现多层决策。在高频段，DT可帮助克服V2X通信高移动性环境中出现的挑战。本文针对V2X通信场景提出一种新的数据驱动工作流用于创建DT，并提出了一种多模态仿真框架用于产生逼真的传感器数据和准确的毫米波/亚毫米波无线信道。所提出的方法利用基于虚幻引擎游戏引擎的汽车仿真和测试框架以及准确的射线跟踪信道模拟器。在城市场景下的仿真结果显示，基础设施和自车的逼真传感器和信道建模均可实现。

    Digital Twins (DTs) for physical wireless environments have been recently proposed as accurate virtual representations of the propagation environment that can enable multi-layer decisions at the physical communication equipment. At high frequency bands, DTs can help to overcome the challenges emerging in the high mobility conditions featuring vehicular environments. In this paper, we propose a novel data-driven workflow for the creation of the DT of a Vehicle-to-Everything (V2X) communication scenario and a multi-modal simulation framework for the generation of realistic sensor data and accurate mmWave/sub-THz wireless channels. The proposed method leverages an automotive simulation and testing framework based on the Unreal Engine game engine and an accurate ray-tracing channel simulator. Simulations over an urban scenario show the achievable realistic sensor and channel modelling both at the infrastructure and at an ego-vehicle.
    
[^94]: CoGANPPIS: 基于共进化增强的全局关注神经网络用于蛋白质相互作用位点预测

    CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction. (arXiv:2303.06945v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2303.06945](http://arxiv.org/abs/2303.06945)

    本论文提出了一种基于共进化增强的全局关注神经网络，它是一种用于蛋白质相互作用位点预测的深度学习模型，结合了共进化特征和全局关注机制以更好地捕捉氨基酸残基之间的关系并考虑到所有残基的贡献。

    

    蛋白质相互作用在生化过程中起着重要作用。准确预测蛋白质相互作用位点（PPIs）可以加深我们对生物机理的理解，并对新药设计至关重要。然而，传统的PPI预测实验方法成本高昂，耗时长，因此近年来开发了许多计算方法，特别是基于机器学习的方法。尽管这些方法取得了令人满意的结果，但仍存在两个限制：（1）大多数模型挖掘了一些有用的输入特征，但未考虑到共进化特征，后者可以提供有关氨基酸残基之间的关系的线索；（2）attention-based模型仅为相邻残基分配关注权重，而不是全局分配，忽略了远离目标残基的一些残基可能也很重要。我们提出了一种共进化增强的全局关注神经网络，这是一种用于PPI位点预测的基于序列的深度学习模型。该模型结合了共进化特征和全局关注机制，以更好地捕捉氨基酸残基之间的关系，并考虑到蛋白序列中所有残基的贡献。

    Protein-protein interactions are essential in biochemical processes. Accurate prediction of the protein-protein interaction sites (PPIs) deepens our understanding of biological mechanism and is crucial for new drug design. However, conventional experimental methods for PPIs prediction are costly and time-consuming so that many computational approaches, especially ML-based methods, have been developed recently. Although these approaches have achieved gratifying results, there are still two limitations: (1) Most models have excavated some useful input features, but failed to take coevolutionary features into account, which could provide clues for inter-residue relationships; (2) The attention-based models only allocate attention weights for neighboring residues, instead of doing it globally, neglecting that some residues being far away from the target residues might also matter.  We propose a coevolution-enhanced global attention neural network, a sequence-based deep learning model for P
    
[^95]: 利用PyTorch和Firedrake实现的基于物理学的机器学习模型

    Physics-driven machine learning models coupling PyTorch and Firedrake. (arXiv:2303.06871v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06871](http://arxiv.org/abs/2303.06871)

    本论文介绍了一种基于物理学的机器学习技术，结合PyTorch和Firedrake框架，可用于较少的训练数据中实现对复杂物理系统的建模。

    

    偏微分方程（PDE）是描述和建模科学和工程学中许多复杂物理系统的核心。然而，在许多实际应用中，PDE建模仅提供了物理模型的不完整描述。基于PDE的机器学习技术旨在解决这种限制。在这种方法中，PDE用作归纳偏置，使耦合模型能够依赖基本物理定律，同时需要更少的训练数据。将PDE和机器学习相结合的高性能模拟部署到复杂问题中需要组合机器学习和基于PDE的框架提供的功能。我们提出了一个简单但有效的耦合方式，将机器学习框架PyTorch和PDE系统Firedrake相结合，为研究人员、工程师和领域专家提供了一种高效的指定耦合模型的方式，并且只需要对现有代码进行微不足道的更改。

    Partial differential equations (PDEs) are central to describing and modelling complex physical systems that arise in many disciplines across science and engineering. However, in many realistic applications PDE modelling provides an incomplete description of the physics of interest. PDE-based machine learning techniques are designed to address this limitation. In this approach, the PDE is used as an inductive bias enabling the coupled model to rely on fundamental physical laws while requiring less training data. The deployment of high-performance simulations coupling PDEs and machine learning to complex problems necessitates the composition of capabilities provided by machine learning and PDE-based frameworks. We present a simple yet effective coupling between the machine learning framework PyTorch and the PDE system Firedrake that provides researchers, engineers and domain specialists with a high productive way of specifying coupled models while only requiring trivial changes to existi
    
[^96]: 机器学习增强的Hankel动态模态分解

    Machine Learning Enhanced Hankel Dynamic-Mode Decomposition. (arXiv:2303.06289v1 [cs.LG])

    [http://arxiv.org/abs/2303.06289](http://arxiv.org/abs/2303.06289)

    本文提出了一种基于深度学习DMD的方法，称为DLHDMD，利用Takens嵌入定理的基本见解开发了一种自适应学习方案，更好地捕捉了高维和混沌动力学，能够为混沌时间序列生成准确的动态。

    This paper proposes a deep learning DMD based method, called DLHDMD, which uses the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics, and is able to generate accurate dynamics for chaotic time series.

    尽管时间序列的获取变得越来越简单和复杂，但从时间序列中开发动态模型仍然是一个具有挑战性和不断发展的问题领域。在过去几年中，为了解决这个问题，机器学习工具已经与所谓的动态模态分解（DMD）相结合。这种通用方法已被证明是一个特别有前途的精密和准确的模型开发途径。在此基础上，我们开发了一种基于深度学习DMD的方法，利用Takens嵌入定理的基本见解开发了一种自适应学习方案，更好地捕捉了高维和混沌动力学。我们称这种方法为深度学习Hankel DMD（DLHDMD）。我们展示了DLHDMD能够为混沌时间序列生成准确的动态，并探讨了我们的方法如何学习映射，这些映射在成功训练后往往趋向于显著的特征。

    While the acquisition of time series has become increasingly more straightforward and sophisticated, developing dynamical models from time series is still a challenging and ever evolving problem domain. Within the last several years, to address this problem, there has been a merging of machine learning tools with what is called the dynamic mode decomposition (DMD). This general approach has been shown to be an especially promising avenue for sophisticated and accurate model development. Building on this prior body of work, we develop a deep learning DMD based method which makes use of the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics. We call this method the Deep Learning Hankel DMD (DLHDMD). We show that the DLHDMD is able to generate accurate dynamics for chaotic time series, and we likewise explore how our method learns mappings which tend, after successful training, to significant
    
[^97]: 神经图形的硬件加速

    Hardware Acceleration of Neural Graphics. (arXiv:2303.05735v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2303.05735](http://arxiv.org/abs/2303.05735)

    本文研究了神经图形是否需要硬件支持，发现当前GPU性能无法满足对4K分辨率60FPS渲染的需求，且在增强现实/虚拟现实应用中性能缺口更大。作者确定输入编码和MLP内核是性能瓶颈。

    

    传统的计算机图形学渲染和反渲染算法已被神经表示（NR）所取代。NR最近被用于学习场景的几何和材质属性，并使用这些信息合成真实的图像，因此承诺用可伸缩的质量和可预测的性能替换传统的渲染算法。本文提出问题：神经图形（NG）是否需要硬件支持？我们研究了代表性的NG应用程序，发现如果我们要在当前的GPU上以60FPS渲染4K分辨率，则所需性能与当前GPU的实际性能存在1.5倍至55倍的差距。对于增强现实/虚拟现实应用程序，所需性能与所需系统功率之间存在更大的差距。我们确定输入编码和MLP内核是性能瓶颈，对于多分辨率哈希网格、多分辨率密集网格和低分辨率密集网格，它们占应用程序时间的72％、60％和59％。

    Rendering and inverse-rendering algorithms that drive conventional computer graphics have recently been superseded by neural representations (NR). NRs have recently been used to learn the geometric and the material properties of the scenes and use the information to synthesize photorealistic imagery, thereby promising a replacement for traditional rendering algorithms with scalable quality and predictable performance. In this work we ask the question: Does neural graphics (NG) need hardware support? We studied representative NG applications showing that, if we want to render 4k res. at 60FPS there is a gap of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications, there is an even larger gap of 2-4 OOM between the desired performance and the required system power. We identify that the input encoding and the MLP kernels are the performance bottlenecks, consuming 72%,60% and 59% of application time for multi res. hashgrid, multi res. densegrid and low res. densegrid 
    
[^98]: 张量分解的实对数典范阈值的上界及其在贝叶斯推断中的应用

    Upper Bound of Real Log Canonical Threshold of Tensor Decomposition and its Application to Bayesian Inference. (arXiv:2303.05731v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05731](http://arxiv.org/abs/2303.05731)

    本研究利用代数几何方法给出了张量分解的实对数典范阈值的上界，并推导了其在贝叶斯推断中的应用理论误差，揭示了张量分解的数学性质。

    

    张量分解现在被用于数据分析、信息压缩和知识恢复。然而，张量分解的数学性质尚未完全阐明，因为它是一种奇异学习机。在本文中，我们利用代数几何方法给出了张量分解的实对数典范阈值(RLCT)的上界，并从理论上推导了其贝叶斯泛化误差。我们还通过数值实验给出了其数学性质的考虑。

    Tensor decomposition is now being used for data analysis, information compression, and knowledge recovery. However, the mathematical property of tensor decomposition is not yet fully clarified because it is one of singular learning machines. In this paper, we give the upper bound of its real log canonical threshold (RLCT) of the tensor decomposition by using an algebraic geometrical method and derive its Bayesian generalization error theoretically. We also give considerations about its mathematical property through numerical experiments.
    
[^99]: 平滑和/或强凸集合上的量规和加速优化

    Gauges and Accelerated Optimization over Smooth and/or Strongly Convex Sets. (arXiv:2303.05037v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.05037](http://arxiv.org/abs/2303.05037)

    本文提出了一种新的在平滑和/或强凸集合上定义的可行性和约束优化问题的可扩展、无投影、加速一阶方法，并通过研究量规的新特征达到了强凸问题的最优加速收敛保证 $O(1/T)$、平滑问题的 $O(1/T^2)$，以及两者都满足的加速线性收敛。

    

    我们考虑在平滑和/或强凸集合上定义的可行性和约束优化问题。这些概念与它们受欢迎的函数对应物相似，但在一阶优化文献中研究较少。我们在这些设置中提出了新的可扩展、无投影、加速一阶方法。我们的方法避免了线性优化或投影预言机，仅使用便宜的一维线搜索和法向量计算。尽管如此，我们得到了强凸问题的最优加速收敛保证 $O(1/T)$、平滑问题的 $O(1/T^2)$，以及两者都满足的加速线性收敛。我们的算法和分析基于平滑和/或强凸集合的闵可夫斯基量的新特征，这可能具有独立的兴趣：尽管量规既不是平滑的也不是强凸的，但我们显示了规模的加平方在集合中继承任何存在的结构。

    We consider feasibility and constrained optimization problems defined over smooth and/or strongly convex sets. These notions mirror their popular function counterparts but are much less explored in the first-order optimization literature. We propose new scalable, projection-free, accelerated first-order methods in these settings. Our methods avoid linear optimization or projection oracles, only using cheap one-dimensional linesearches and normal vector computations. Despite this, we derive optimal accelerated convergence guarantees of $O(1/T)$ for strongly convex problems, $O(1/T^2)$ for smooth problems, and accelerated linear convergence given both. Our algorithms and analysis are based on novel characterizations of the Minkowski gauge of smooth and/or strongly convex sets, which may be of independent interest: although the gauge is neither smooth nor strongly convex, we show the gauge squared inherits any structure present in the set.
    
[^100]: 带有双向先验模型的向量量化时间序列生成

    Vector Quantized Time Series Generation with a Bidirectional Prior Model. (arXiv:2303.04743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04743](http://arxiv.org/abs/2303.04743)

    本文提出了一种新的时间序列生成方法，使用向量量化技术和双向变压器模型来生成质量更好、模块化变化更快的合成信号。

    

    时间序列生成研究主要集中在使用生成对抗网络（GAN）与递归神经网络（RNN）变体相结合。然而，训练 GAN 的基本限制和挑战仍然存在。此外，RNN族通常在远程时间步之间的时间一致性方面存在困难。受到图像生成领域成功的启发，我们提出 TimeVQVAE，这是我们所知道的第一个使用向量量化（VQ）技术解决 TSG 问题的工作。此外，离散潜在空间的先验使用双向变压器模型进行学习，可以更好地捕捉全局时间一致性。我们还提出在时间 - 频率域中进行 VQ 建模，分为低频（LF）和高频（HF）。这使我们能够保留时间序列的重要特征，并生成质量更好、模块性变化更快的新合成信号。

    Time series generation (TSG) studies have mainly focused on the use of Generative Adversarial Networks (GANs) combined with recurrent neural network (RNN) variants. However, the fundamental limitations and challenges of training GANs still remain. In addition, the RNN-family typically has difficulties with temporal consistency between distant timesteps. Motivated by the successes in the image generation (IMG) domain, we propose TimeVQVAE, the first work, to our knowledge, that uses vector quantization (VQ) techniques to address the TSG problem. Moreover, the priors of the discrete latent spaces are learned with bidirectional transformer models that can better capture global temporal consistency. We also propose VQ modeling in a time-frequency domain, separated into low-frequency (LF) and high-frequency (HF). This allows us to retain important characteristics of the time series and, in turn, generate new synthetic signals that are of better quality, with sharper changes in modularity, t
    
[^101]: 掩蔽图像是鲁棒微调的反事实样本

    Masked Images Are Counterfactual Samples for Robust Fine-tuning. (arXiv:2303.03052v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.03052](http://arxiv.org/abs/2303.03052)

    本文提出了一种新颖的深度学习模型微调方法，利用掩蔽图像作为反事实样本，提高模型的鲁棒性。

    

    深度学习模型由于训练数据和测试数据之间的分布差异而受到挑战。最近，基于多样化数据预训练的大型模型展现了空前的鲁棒性来应对各种分布差异。然而，在这些模型上进行微调可能会导致在分布内性能和分布外鲁棒性之间的权衡。现有的方法并没有明确处理分布外鲁棒性问题。在本文中，我们基于对上述问题的因果分析，提出了一种新颖的微调方法，利用掩蔽图像作为反事实样本，有助于提高微调模型的鲁棒性。具体而言，我们基于类激活图对图像的语义相关或语义无关补丁进行掩蔽，以打破虚假相关性，并用其他图像的补丁来重新填充掩蔽的补丁。这些反事实样本则用于特征蒸馏。

    Deep learning models are challenged by the distribution shift between the training data and test data. Recently, the large models pre-trained on diverse data demonstrate unprecedented robustness to various distribution shifts. However, fine-tuning on these models can lead to a trade-off between in-distribution (ID) performance and out-of-distribution (OOD) robustness. Existing methods for tackling this trade-off do not explicitly address the OOD robustness problem. In this paper, based on causal analysis on the aforementioned problems, we propose a novel fine-tuning method, which use masked images as counterfactual samples that help improving the robustness of the fine-tuning model. Specifically, we mask either the semantics-related or semantics-unrelated patches of the images based on class activation map to break the spurious correlation, and refill the masked patches with patches from other images. The resulting counterfactual samples are used in feature-based distillation with the 
    
[^102]: 追求实用可持续性深度神经网络训练的低碳电力

    Chasing Low-Carbon Electricity for Practical and Sustainable DNN Training. (arXiv:2303.02508v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02508](http://arxiv.org/abs/2303.02508)

    本文提出了一种降低深度神经网络训练的碳足迹的实用解决方案，通过在训练过程中控制GPU的能耗来降低碳排放，同时还提出了一种预测方法，可以预测未来的碳强度，对各种DNN应用程序适用，无需额外硬件或基础设施。

    

    近年来，深度学习取得了很大的发展，由于使用GPU进行训练，导致能耗和碳排放量增加。传统的解决方案尝试将训练工作移动到碳强度较低的位置或时间框架，以回应可持续性的呼吁。然而，由于数据集大小或数据法规等原因，将工作移动到其他地方并不总是可行的。此外，推迟训练可能会对应用服务质量产生负面影响，因为支持服务的DNN没有得到及时更新。在本研究中，我们提出了一种实用的解决方案，可以降低DNN训练的碳足迹，而无需迁移或推迟工作。特别地，我们的解决方案在训练过程中观察实时的碳强度变化并控制GPU的能耗，从而在保持训练性能的同时降低碳足迹。此外，为了主动适应不断变化的电网条件，我们提出了一种预测方法来预测未来的碳强度，从而实现更有效的碳减排。我们的方法适用于各种DNN应用程序，无需额外的硬件或基础设施。图像分类任务的实验结果表明，我们的方法可以将碳强度降低高达44%，而不会牺牲训练性能。

    Deep learning has experienced significant growth in recent years, resulting in increased energy consumption and carbon emission from the use of GPUs for training deep neural networks (DNNs). Answering the call for sustainability, conventional solutions have attempted to move training jobs to locations or time frames with lower carbon intensity. However, moving jobs to other locations may not always be feasible due to large dataset sizes or data regulations. Moreover, postponing training can negatively impact application service quality because the DNNs backing the service are not updated in a timely fashion. In this work, we present a practical solution that reduces the carbon footprint of DNN training without migrating or postponing jobs. Specifically, our solution observes real-time carbon intensity shifts during training and controls the energy consumption of GPUs, thereby reducing carbon footprint while maintaining training performance. Furthermore, in order to proactively adapt to
    
[^103]: 深度学习去噪方法的客观任务评估的必要性：以心肌灌注SPECT为背景的研究

    Need for Objective Task-based Evaluation of Deep Learning-Based Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT. (arXiv:2303.02110v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.02110](http://arxiv.org/abs/2303.02110)

    本研究旨在探讨基于深度学习的图像去噪方法在临床任务中的表现评估，发现使用保真度(FoMs)的评估不一定与任务为基础的评估一致，而基于信号检测理论(SDT)的评估方法提供了更客观、有意义的去噪效果评估方式，并证明虚拟临床试验（VCTs）是评估DL方法的实用工具。

    

    人工智能方法在核医学中引起了广泛的兴趣，其中特别关注使用基于深度学习（DL）的方法去噪低剂量、短采集时间或两者同时获取的图像。这些方法的客观评估对于临床应用至关重要。DL去噪核医学图像通常使用类似RMSE和SSIM这样的保真度（FoMs）进行评估。然而，这些图像是为临床任务而采集的，因此应该根据它们在这些任务中的表现来评估。本研究的目的是(1)调查使用这些FoMs的评估是否与客观的临床任务评估一致; (2)提供用于确定去噪对信号检测任务影响的理论分析; (3)展示虚拟临床试验（VCTs）用于评估DL方法的实用性。使用逼真的模拟器进行了一个VCT来评估DL去噪心肌灌注SPECT图像方法。采用客观的强制选择实验，使用信号检测理论（SDT）的基于任务的指标和FoMs评估了去噪效果。结果表明，使用FoMs评估去噪效果不一定与基于任务的评估相关。SDT指标提供了更客观和有意义的去噪效果评估方式。VCTs可为核医学中基于DL的去噪方法的评估提供有用的工具。

    Artificial intelligence-based methods have generated substantial interest in nuclear medicine. An area of significant interest has been using deep-learning (DL)-based approaches for denoising images acquired with lower doses, shorter acquisition times, or both. Objective evaluation of these approaches is essential for clinical application. DL-based approaches for denoising nuclear-medicine images have typically been evaluated using fidelity-based figures of merit (FoMs) such as RMSE and SSIM. However, these images are acquired for clinical tasks and thus should be evaluated based on their performance in these tasks. Our objectives were to (1) investigate whether evaluation with these FoMs is consistent with objective clinical-task-based evaluation; (2) provide a theoretical analysis for determining the impact of denoising on signal-detection tasks; (3) demonstrate the utility of virtual clinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate a DL-based method for denoisi
    
[^104]: 关于深度神经网络功能耦合水印的研究

    On Function-Coupled Watermarks for Deep Neural Networks. (arXiv:2302.10296v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10296](http://arxiv.org/abs/2302.10296)

    本文提出了一种新颖的深度神经网络水印方案，它可以有效地防御模型微调和修剪等水印删除攻击；通过增强水印和模型功能的耦合，我们的方法可以确保删除水印不可避免地会降低模型在常规输入上的性能。

    

    良好表现的深度神经网络通常需要海量标记数据和计算资源进行训练。为了保护这些知识产权，提出了各种水印技术，其中DNN提供商将秘密信息植入模型中，以便在稍后通过一些专用触发输入检索嵌入的水印索权；虽然文献中报告了有希望的结果，但现有解决方案仍然遭受水印删除攻击，例如模型微调和模型修剪。本文提出了一种新颖的DNN水印方案，可以有效地防御上述攻击。我们的关键洞察力是增强水印和模型功能的耦合，这样删除水印会不可避免地降低模型在常规输入上的性能。为此，与先前依赖于来自超出分布数据的秘密特征的方法不同，我们的方法仅使用从训练数据中学习的特征。

    Well-performed deep neural networks (DNNs) generally require massive labelled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers implant secret information into the model so that they can later claim IP ownership by retrieving their embedded watermarks with some dedicated trigger inputs. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning and model pruning.  In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model's performance on normal inputs. To this end, unlike previous methods relying on secret features learnt from out-of-distribution data, our method only uses features lear
    
[^105]: 多分辨率图形变换器与小波位置编码用于学习分层结构

    Multiresolution Graph Transformers and Wavelet Positional Encoding for Learning Hierarchical Structures. (arXiv:2302.08647v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08647](http://arxiv.org/abs/2302.08647)

    该论文提出了多分辨率图形变换器（MGT）和小波位置编码（WavePE）方法，可以学习表示大分子的分层结构，并在众多数据集上获得了比其他先进方法更好的结果。

    

    当前的图形学习算法并不能很好地处理大分子，因为它们没有考虑到原子之间的分层交互，而这对于确定大分子的属性至关重要。在这项工作中，我们提出了多分辨率图形变换器（MGT），这是第一个可以学习表示多种尺度下大分子的图形变换器架构。MGT可以学习产生原子的表示，并将它们分组成有意义的功能组或重复单元。我们还引入了小波位置编码（WavePE），一种新的位置编码方法，可以保证在频谱和空间域中的局部性。我们提出的模型在由聚合物和多肽组成的两个大分子数据集以及一个类似药物的分子数据集上取得了竞争性的结果。重要的是，我们的模型优于其他最先进的方法，在估算分子性质（例如GAP，HOMO和LUMO）时达到了化学精度。

    Contemporary graph learning algorithms are not well-defined for large molecules since they do not consider the hierarchical interactions among the atoms, which are essential to determine the molecular properties of macromolecules. In this work, we propose Multiresolution Graph Transformers (MGT), the first graph transformer architecture that can learn to represent large molecules at multiple scales. MGT can learn to produce representations for the atoms and group them into meaningful functional groups or repeating units. We also introduce Wavelet Positional Encoding (WavePE), a new positional encoding method that can guarantee localization in both spectral and spatial domains. Our proposed model achieves competitive results on two macromolecule datasets consisting of polymers and peptides, and one drug-like molecule dataset. Importantly, our model outperforms other state-of-the-art methods and achieves chemical accuracy in estimating molecular properties (e.g., GAP, HOMO and LUMO) calc
    
[^106]: 利用严格预测保证超可靠低延迟流量的动态调度

    Guaranteed Dynamic Scheduling of Ultra-Reliable Low-Latency Traffic via Conformal Prediction. (arXiv:2302.07675v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.07675](http://arxiv.org/abs/2302.07675)

    本文介绍了一种利用最近的在线符合预测技术，可以提供关于可靠性和延迟的正式保证的URRLC数据包调度程序。

    

    上行的超可靠低延迟流量（URLLC）的动态调度可以通过仅在必要时分配资源来显著提高增强型移动宽带（eMBB）设备等共存服务的效率。主要的挑战在于URRLC数据包产生过程中的不确定性，这就要求在未来帧中使用URRLC流量预测器。实际上，这种预测可能会高估或低估要产生的URRLC数据量，导致预先分配过多或过少的资源用于URRLC数据包。本文介绍了一种新的URRLC数据包调度程序，无论URRLC流量预测器的质量如何，都可以提供关于可靠性和延迟的正式保证。该方法利用了在线符合预测（CP）的最新进展，并遵循根据动态调整分配的资源量的原则以满足

    The dynamic scheduling of ultra-reliable and low-latency traffic (URLLC) in the uplink can significantly enhance the efficiency of coexisting services, such as enhanced mobile broadband (eMBB) devices, by only allocating resources when necessary. The main challenge is posed by the uncertainty in the process of URLLC packet generation, which mandates the use of predictors for URLLC traffic in the coming frames. In practice, such prediction may overestimate or underestimate the amount of URLLC data to be generated, yielding either an excessive or an insufficient amount of resources to be pre-emptively allocated for URLLC packets. In this paper, we introduce a novel scheduler for URLLC packets that provides formal guarantees on reliability and latency irrespective of the quality of the URLLC traffic predictor. The proposed method leverages recent advances in online conformal prediction (CP), and follows the principle of dynamically adjusting the amount of allocated resources so as to meet
    
[^107]: 使用非特定运动数据的可扩展XR用户基于运动的识别

    Extensible Motion-based Identification of XR Users using Non-Specific Motion Data. (arXiv:2302.07517v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.07517](http://arxiv.org/abs/2302.07517)

    提出了一种可扩展XR用户基于运动的识别方法。与现有基线方法相比，该方法通过仅使用少量的注册数据来识别新用户，可以在几秒钟内注册新用户，而且在仅有少量注册数据可用时也更可靠。

    

    本文提出了一种基于嵌入式和深度度量学习结合的方法，将距离和分类两种方法的优势相结合，用于通过用户的运动来识别扩展现实用户。我们在“半衰期：Alyx”VR游戏的用户数据集上进行了模型训练，并使用现有的基线分类模型作为对比。研究结果表明，基于嵌入式的方法可以通过只使用几分钟的注册数据，识别新用户的非特定运动，可以在几秒钟内注册新用户，而重新训练基线方法需要花费将近一天的时间，当只有很少的注册数据可用时，比基线方法更可靠，可以用于识别使用不同VR设备记录的新用户数据集。综上所述，我们的解决方案为易于扩展的XR用户识别系统奠定基础，可应用于广泛场景。

    In this paper, we combine the strengths of distance-based and classification-based approaches for the task of identifying extended reality users by their movements. For this we present an embedding-based approach that leverages deep metric learning. We train the model on a dataset of users playing the VR game ``Half-Life: Alyx'' and conduct multiple experiments and analyses using a state of the art classification-based model as baseline. The results show that the embedding-based method 1) is able to identify new users from non-specific movements using only a few minutes of enrollment data, 2) can enroll new users within seconds, while retraining the baseline approach takes almost a day, 3) is more reliable than the baseline approach when only little enrollment data is available, 4) can be used to identify new users from another dataset recorded with different VR devices.  Altogether, our solution is a foundation for easily extensible XR user identification systems, applicable to a wide
    
[^108]: 使用脉冲形状和人工神经网络恢复PMT的饱和响应

    Restoring the saturation response of a PMT using pulse-shape and artificial-neural-networks. (arXiv:2302.06170v2 [physics.ins-det] UPDATED)

    [http://arxiv.org/abs/2302.06170](http://arxiv.org/abs/2302.06170)

    该论文提出了使用脉冲形状和人工神经网络恢复PMT饱和响应的方法，可以估计线性区域并提高光子计数和能量重建效率。

    

    光电倍增管（PMT）的线性响应是光子计数和中微子能量重建的必要属性。使用基于线性烷基苯（LAB）的液体闪烁体研究了PMT的线性有效区域和饱和响应。观察到了两种不同饱和响应之间的相关性，即脉冲形状失真和脉冲面积减小。观察到的脉冲形状为估计脉冲面积相对线性区域提供了有用的信息。这种基于相关性的诊断允许原地估计线性范围，这在以前很具挑战性。利用测得的两个饱和响应之间的相关性，训练人工神经网络（ANN）来预测从观察到的脉冲形状中减小的脉冲面积。ANN预测的脉冲面积减小使得可以预测独立于饱和行为的理想光电子数。这种基于脉冲形状的方法估计PMT的线性范围并使用ANN恢复饱和响应是增强中微子探测器中光子计数和能量重建效率的重要贡献。

    The linear response of a photomultiplier tube (PMT) is a required property for photon counting and reconstruction of the neutrino energy. The linearity valid region and the saturation response of PMT were investigated using a linear-alkyl-benzene (LAB)-based liquid scintillator. A correlation was observed between the two different saturation responses, with pulse-shape distortion and pulse-area decrease. The observed pulse-shape provides useful information for the estimation of the linearity region relative to the pulse-area. This correlation-based diagnosis allows an ${in}$-${situ}$ estimation of the linearity range, which was previously challenging. The measured correlation between the two saturation responses was employed to train an artificial-neural-network (ANN) to predict the decrease in pulse-area from the observed pulse-shape. The ANN-predicted pulse-area decrease enables the prediction of the ideal number of photoelectrons irrelevant to the saturation behavior. This pulse-sha
    
[^109]: I$^2$SB：图像到图像的Schr\"odinger桥

    I$^2$SB: Image-to-Image Schr\"odinger Bridge. (arXiv:2302.05872v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05872](http://arxiv.org/abs/2302.05872)

    提出了一种新的条件扩散模型I$^2$SB，直接学习两个给定分布之间的非线性扩散过程。通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，在各种图像恢复任务中的性能表现优于标准条件扩散模型，并具有更可解释的生成过程。

    

    本文提出了一种新的条件扩散模型，即图像到图像的Schr\"odinger桥（I$^2$SB），直接学习两个给定分布之间的非线性扩散过程。这些扩散桥对于图像恢复特别有用，因为退化图像是重构清晰图像的结构信息先验。 I$^2$SB属于一类可处理的Schr\"odinger桥模型，它是得分模型的非线性扩展，其边界对的边缘分布可以在解析上计算。这种通过边界对求解的方法使得I$^2$SB训练成为一种无需模拟的非线性扩散框架，进而采用在标准扩散模型中使用的实用技术，使得I$^2$SB训练具有可扩展性。在ImageNet 256x256上，我们验证了I$^2$SB在各种图像恢复任务中的性能，包括修复，超分辨率，去模糊和JPEG恢复，并表明I$^2$SB超过了标准条件扩散模型，具有更可解释的生成过程。

    We propose Image-to-Image Schr\"odinger Bridge (I$^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$^2$SB belongs to a tractable class of Schr\"odinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show that I$^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. 
    
[^110]: 几何深度学习仅依靠距离矩阵足够吗？

    Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05743](http://arxiv.org/abs/2302.05743)

    本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。

    

    图神经网络（GNN）常用于涉及图形几何的任务，例如分子动力学模拟。虽然几何图的距离矩阵包含完整的几何信息，但已经证明消息传递神经网络（MPNNs）无法学习这种几何信息。本文通过构造新颖的对称几何图的家族，扩展了MPNN无法区分其距离矩阵的反例家族，并提出$k$-DisGNNs，可以有效地利用距离矩阵中丰富的几何结构。我们证明了模型的高表达能力，并证明了一些现有的精心设计的几何模型可以作为$k$-DisGNNs的特殊情况统一起来。最重要的是，我们建立了几何深度学习和传统图表示学习之间的联系，展示了那些最初为低度表达能力的GNN模型设计的高度表达力的GNN模型。

    Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
    
[^111]: Jaccard度量损失：使用软标签优化Jaccard指数

    Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05666](http://arxiv.org/abs/2302.05666)

    本文提出了Jaccard度量损失（JMLs）来优化Jaccard指数，该损失在软标签下仍然有效。

    

    IoU损失是直接优化Jaccard指数的替代品。在语义分割中，将IoU损失作为损失函数的一部分，与仅优化像素损失（如交叉熵损失）相比，对于Jaccard指数测量表现更好。最显着的IoU损失是软Jaccard损失和Lovasz-Softmax损失。然而，这些损失与机器学习中普遍存在的软标签不兼容。在本文中，我们提出了Jaccard度量损失（JMLs），它们在标准设置下与软标签兼容，与软Jaccard损失相同。使用JMLs，我们研究了两种最流行的软标签用例：标签平滑和知识蒸馏。在三个语义分割数据集（Cityscapes、PASCAL VOC和DeepGlobe Land）上，我们的实验表明，与交叉熵损失相比，我们的简单方法显著提高了性能，并且在DeepGlobe Land数据集上超过了最先进的方法。

    IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
    
[^112]: 过去和未来之间：基于时空模型的多相机三维多目标跟踪

    Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking. (arXiv:2302.03802v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.03802](http://arxiv.org/abs/2302.03802)

    本文提出了一个基于时空模型的多相机三维多目标跟踪框架，命名为“过去和未来之间的跟踪”。该方法采用注意力跟踪框架，通过对象查询连续地表示跟踪实例，并整合了跟踪对象的前后推理，显著提高了跟踪准确性和ID-Switches的减少。

    

    本文提出了一种端到端的多相机三维多目标跟踪框架。该框架强调时空连续性，并整合了跟踪对象的前后推理。因此，我们将其命名为“基于过去和未来的跟踪”（PF-Track）。具体而言，我们的方法采用“注意力跟踪”框架，并通过对象查询连续地表示跟踪实例。为了明确使用历史线索，我们的“过去推理”模块学习精细化跟踪，并通过跨前一帧和其他对象的查询交叉注意来增强对象特征。而“未来推理”模块则消化历史信息并预测强健的未来轨迹。在长时间遮挡的情况下，我们的方法可以维持对象位置，通过整合运动预测实现重新关联。在nuScenes数据集上，我们的方法大幅提高了AMOTA，并将ID-Switches减少了90%，相比之前的方法有了显著的改善。

    This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT) framework. It emphasizes spatio-temporal continuity and integrates both past and future reasoning for tracked objects. Thus, we name it "Past-and-Future reasoning for Tracking" (PF-Track). Specifically, our method adapts the "tracking by attention" framework and represents tracked instances coherently over time with object queries. To explicitly use historical cues, our "Past Reasoning" module learns to refine the tracks and enhance the object features by cross-attending to queries from previous frames and other objects. The "Future Reasoning" module digests historical information and predicts robust future trajectories. In the case of long-term occlusions, our method maintains the object positions and enables re-association by integrating motion predictions. On the nuScenes dataset, our method improves AMOTA by a large margin and remarkably reduces ID-Switches by 90% compared to prior approaches, which is an 
    
[^113]: 两种损失比一种更好：使用更便宜的代理加快优化

    Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy. (arXiv:2302.03542v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03542](http://arxiv.org/abs/2302.03542)

    该论文提出了一种代理算法，通过使用一个易于访问的函数作为代理，可以以与原函数梯度下降相匹配的速度收敛，从而显著提高样本效率，并在机器学习中具有许多潜在应用。

    

    我们提出了一种算法，通过使用相关的、易于访问的函数作为代理，来最小化一个难以计算梯度的目标函数。我们的算法基于代理的近似近端点迭代，结合来自目标函数的相对较少的随机梯度。当目标函数与代理之间的差异是$\delta$-平滑时，我们的算法保证以与$\delta$-平滑目标函数上的随机梯度下降相匹配的速率收敛，这可以显著提高样本效率。我们的算法在机器学习中有许多潜在应用，并提供了一种利用合成数据、物理模拟器、混合公共和私人数据等的原则性方法。

    We present an algorithm for minimizing an objective with hard-to-compute gradients by using a related, easier-to-access function as a proxy. Our algorithm is based on approximate proximal point iterations on the proxy combined with relatively few stochastic gradients from the objective. When the difference between the objective and the proxy is $\delta$-smooth, our algorithm guarantees convergence at a rate matching stochastic gradient descent on a $\delta$-smooth objective, which can lead to substantially better sample efficiency. Our algorithm has many potential applications in machine learning, and provides a principled means of leveraging synthetic data, physics simulators, mixed public and private data, and more.
    
[^114]: 基于SMDP的GPU动态批处理优化推断效率

    SMDP-Based Dynamic Batching for Efficient Inference on GPU-Based Platforms. (arXiv:2301.12865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12865](http://arxiv.org/abs/2301.12865)

    本文提出了一种动态批处理策略，采用基于GPU的批处理服务队列进行建模，通过半马尔可夫决策过程的方法最小化平均响应时间和功耗。

    

    在云计算或边缘计算平台上，批处理是提供高效和经济服务的重要技术，本文提出了一种动态批处理策略，旨在在效率和延迟之间取得平衡。将基于GPU的推断服务建模为批处理服务队列，并将其设计为一个连续时间平均成本问题，制定了一个半马尔可夫决策过程（SMDP），并以最小化平均响应时间和平均功耗之和为目标。最优策略通过解决相关的离散时间贝尔曼方程获得。

    In up-to-date machine learning (ML) applications on cloud or edge computing platforms, batching is an important technique for providing efficient and economical services at scale. In particular, parallel computing resources on the platforms, such as graphics processing units (GPUs), have higher computational and energy efficiency with larger batch sizes. However, larger batch sizes may also result in longer response time, and thus it requires a judicious design. This paper aims to provide a dynamic batching policy that strikes a balance between efficiency and latency. The GPU-based inference service is modeled as a batch service queue with batch-size dependent processing time. Then, the design of dynamic batching is a continuous-time average-cost problem, and is formulated as a semi-Markov decision process (SMDP) with the objective of minimizing the weighted sum of average response time and average power consumption. The optimal policy is acquired by solving an associated discrete-time
    
[^115]: 通过部分不变性学习最优特征

    Learning Optimal Features via Partial Invariance. (arXiv:2301.12067v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12067](http://arxiv.org/abs/2301.12067)

    本文证明了不变风险最小化（IRM）的次优性，并提出了通过部分不变性来缓解此问题的方法，同时展示了从训练域的一个分区中学习以提高不变性模型的有效性。

    

    学习对于分布变化具有鲁棒性的模型是其在实际应用中的一个重点关注点。不变风险最小化（IRM）是一种流行的框架，旨在从多个环境中学习鲁棒模型。IRM的成功需要一个重要的假设：潜在的因果机制/特征在多个环境中保持不变。当该假设不成立时，我们证明IRM可能会导致预测器过度约束，并提出了通过 $\textit{部分不变性}$ 来缓解此问题。本文理论上突出了IRM的次优性，并展示了如何从训练域的一个分区中学习以提高不变模型。我们进行了多个实验，包括在线性设置和深度神经网络上，涉及语言和图像数据集，验证了我们的结论。

    Learning models that are robust to distribution shifts is a key concern in the context of their real-life applicability. Invariant Risk Minimization (IRM) is a popular framework that aims to learn robust models from multiple environments. The success of IRM requires an important assumption: the underlying causal mechanisms/features remain invariant across environments. When not satisfied, we show that IRM can over-constrain the predictor and to remedy this, we propose a relaxation via $\textit{partial invariance}$. In this work, we theoretically highlight the sub-optimality of IRM and then demonstrate how learning from a partition of training domains can help improve invariant models. Several experiments, conducted both in linear settings as well as with deep neural networks on tasks over both language and image data, allow us to verify our conclusions.
    
[^116]: Truveta Mapper：一个零样本本体映射框架

    Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09767](http://arxiv.org/abs/2301.09767)

    提出了一个将无监督本体匹配或本体对齐视为翻译任务的新视角的Truveta Mapper框架，在零样本、统一和端到端的方式下执行多本体对齐。该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案，无需显式跨本体手动标注数据。

    

    本文提出了一种将无监督本体匹配(Ontology Matching, OM)或本体对齐(Ontology Alignment, OA)视为翻译任务的新视角。将本体表示为图形，在源本体图中的节点到目标本体图中的路径之间进行翻译。所提出的Truveta Mapper (TM)框架利用多任务序列到序列转换器模型，在零样本、统一和端到端的方式下执行多本体对齐。多任务使模型能够通过迁移学习来隐含地学习不同本体之间的关系，无需任何显式的跨本体手动标注数据。这也使得该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案。模型仅在公开可用的文本语料库和内部本体数据上进行预训练和微调。该方案优于现有标准基准解决方案，如Edit-Similarity和MINTE+。

    In this paper, a new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task. Ontologies are represented as graphs, and the translation is performed from a node in the source ontology graph to a path in the target ontology graph. The proposed framework, Truveta Mapper (TM), leverages a multi-task sequence-to-sequence transformer model to perform alignment across multiple ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables the model to implicitly learn the relationship between different ontologies via transfer-learning without requiring any explicit cross-ontology manually labeled data. This also enables the formulated framework to outperform existing solutions for both runtime latency and alignment quality. The model is pre-trained and fine-tuned only on publicly available text corpus and inner-ontologies data. The proposed solution outperforms state-of-the-art approaches, Edit-Similari
    
[^117]: 因果三元组：面向干预中心因果表示学习的开放挑战

    Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning. (arXiv:2301.05169v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05169](http://arxiv.org/abs/2301.05169)

    本文提出了一个因果表示学习基准——因果三元组，该基准具有可操作的反事实设置和干预性下游任务，对分离和物体中心表示学习取得了显著优化，然而在因果关系的识别和干预性下游任务上表现欠佳。

    

    近年来，学者们对从干预下的低级图像对中学习高级因果表示产生了浓厚的兴趣。然而，现有的研究往往局限于简单的合成数据，这远离了真实世界中的问题。在本文中，我们提出了因果三元组，这是一个因果表示学习基准，不仅具有更为复杂的视觉场景，而且还具有两个常常被忽视的关键愿望：(i) 一个可操作的反事实设置，其中只有某些物体级变量允许反事实观察，而其他变量则不允许；(ii) 一个干预性的下游任务，强调独立因果机制原则下的分布鲁棒性。通过大量的实验，我们发现，具有分离的或物体中心表示的知识的模型显着优于其分布式对应物。然而，最近的因果表示学习方法仍然难以识别可操作的反事实设置中的因果关系，且在干预性下游任务上表现欠佳。

    Recent years have seen a surge of interest in learning high-level causal representations from low-level image pairs under interventions. Yet, existing efforts are largely limited to simple synthetic settings that are far away from real-world problems. In this paper, we present Causal Triplet, a causal representation learning benchmark featuring not only visually more complex scenes, but also two crucial desiderata commonly overlooked in previous works: (i) an actionable counterfactual setting, where only certain object-level variables allow for counterfactual observations whereas others do not; (ii) an interventional downstream task with an emphasis on out-of-distribution robustness from the independent causal mechanisms principle. Through extensive experiments, we find that models built with the knowledge of disentangled or object-centric representations significantly outperform their distributed counterparts. However, recent causal representation learning methods still struggle to id
    
[^118]: 数据中心人工智能：视角和挑战

    Data-centric AI: Perspectives and Challenges. (arXiv:2301.04819v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.04819](http://arxiv.org/abs/2301.04819)

    研究提出了数据中心人工智能（DCAI）的概念，强调数据质量和可靠性，总结了训练数据开发、推断数据开发和数据维护三个总体使命，提供了对DCAI任务的讨论和观点，并列出了挑战。

    

    数据在构建人工智能系统方面的作用通过新兴的数据中心人工智能（DCAI）概念得到了显著增强，该概念主张将重点从模型改进转向确保数据质量和可靠性。虽然我们的社区一直在不同方面努力增强数据，但它们通常是针对特定任务的孤立举措。为了推动社区的集体倡议并推动DCAI，我们提供了一个总体框架，并集合了三个总体使命：训练数据开发、推断数据开发和数据维护。我们对代表DCAI任务进行了高层次讨论并分享了观点。最后，我们列出了开放性挑战。更多资源总结详见https://github.com/daochenzha/data-centric-AI。

    The role of data in building AI systems has recently been significantly magnified by the emerging concept of data-centric AI (DCAI), which advocates a fundamental shift from model advancements to ensuring data quality and reliability. Although our community has continuously invested efforts into enhancing data in different aspects, they are often isolated initiatives on specific tasks. To facilitate the collective initiative in our community and push forward DCAI, we draw a big picture and bring together three general missions: training data development, inference data development, and data maintenance. We provide a top-level discussion on representative DCAI tasks and share perspectives. Finally, we list open challenges. More resources are summarized at https://github.com/daochenzha/data-centric-AI
    
[^119]: 预测性前向-前向算法

    The Predictive Forward-Forward Algorithm. (arXiv:2301.01452v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01452](http://arxiv.org/abs/2301.01452)

    PFF算法用于神经系统内的信贷分配，采用新颖的动态递归神经系统和定向生成电路，有效地通过前向传递学习信号和更新突触，消除了计算和结构上的约束。

    

    我们提出了预测性前向-前向（PFF）算法，用于在神经系统中进行信贷分配。具体而言，我们设计了一种新颖的动态递归神经系统，该系统与表示电路同时学习一个定向生成电路。值得注意的是，该系统集成了可学习的横向竞争，噪声注入和预测编码的元素，这是一种新兴且有效的皮层功能的神经生物学过程理论，与前向-前向（FF）适应方案一起使用。此外，PFF可以有效地学习仅通过前向传递传播学习信号并更新突触，消除了基于反向传播方案所施加的关键结构和计算约束。除了计算方面的优势外，PFF过程还可以用于理解生物神经元背景下使用本地信号的学习机制，尽管缺少反馈连接。我们在图像数据上运行实验，并证明了PFF程序的有效性。

    We propose the predictive forward-forward (PFF) algorithm for conducting credit assignment in neural systems. Specifically, we design a novel, dynamic recurrent neural system that learns a directed generative circuit jointly and simultaneously with a representation circuit. Notably, the system integrates learnable lateral competition, noise injection, and elements of predictive coding, an emerging and viable neurobiological process theory of cortical function, with the forward-forward (FF) adaptation scheme. Furthermore, PFF efficiently learns to propagate learning signals and updates synapses with forward passes only, eliminating key structural and computational constraints imposed by backpropagation-based schemes. Besides computational advantages, the PFF process could prove useful for understanding the learning mechanisms behind biological neurons that use local signals despite missing feedback connections. We run experiments on image data and demonstrate that the PFF procedure work
    
[^120]: FunkNN：功能生成的神经插值

    FunkNN: Neural Interpolation for Functional Generation. (arXiv:2212.14042v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2212.14042](http://arxiv.org/abs/2212.14042)

    FunkNN是一种新的卷积网络，可以连续地重建图像，实现了最先进的图像生成质量，优于现有的基于MLP的方法。

    

    我们能否构建连续的生成模型，这些模型可以跨尺度泛化，并在任何坐标处进行评估，可以计算精确的导数，并且在概念上很简单？现有的基于MLP的架构生成的样本比具有有利卷积归纳偏差的网格生成器差。专注于在不同尺度上生成图像的模型效果更好，但采用的架构复杂，不适用于对图像和导数进行连续评估。我们采用信号处理的视角，并将连续图像生成视为从样本进行插值。确实，正确采样的离散图像包含有关低空间频率的所有信息。问题是如何以数据驱动的方式外推频谱，同时满足上述设计标准。我们的答案是FunkNN--一种新的卷积网络，它学习如何在任意坐标重构连续图像，并可以应用于任何图像数据集。结合离散采样步骤，FunkNN在连续域中实现了最先进的图像生成质量，优于现有的基于MLP的方法。

    Can we build continuous generative models which generalize across scales, can be evaluated at any coordinate, admit calculation of exact derivatives, and are conceptually simple? Existing MLP-based architectures generate worse samples than the grid-based generators with favorable convolutional inductive biases. Models that focus on generating images at different scales do better, but employ complex architectures not designed for continuous evaluation of images and derivatives. We take a signal-processing perspective and treat continuous image generation as interpolation from samples. Indeed, correctly sampled discrete images contain all information about the low spatial frequencies. The question is then how to extrapolate the spectrum in a data-driven way while meeting the above design criteria. Our answer is FunkNN -- a new convolutional network which learns how to reconstruct continuous images at arbitrary coordinates and can be applied to any image dataset. Combined with a discrete 
    
[^121]: 物理学知识指导的高斯过程回归应用于解决线性偏微分方程

    Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers. (arXiv:2212.12474v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12474](http://arxiv.org/abs/2212.12474)

    本文使用物理学知识指导的高斯过程回归方法，解决线性偏微分方程求解器无法量化近似误差的问题。

    

    线性偏微分方程是一类重要且广泛应用的机械模型，描述了物理过程，例如热传导、电磁学和波传播等。实践中，通常使用基于离散化的专门数值方法来解决偏微分方程。这些求解器通常使用未知模型参数的估计值以及如果可用的话，物理测量值用于初始化。这些求解器经常嵌入到具有下游应用的更大的科学模型中，因此误差量化起着关键作用。然而，经典的偏微分方程求解器忽略参数和测量不确定性，可能无法产生一致性的估计值，以用于计算其固有的逼近误差。本文通过将求解线性偏微分方程解释为物理学知识指导的高斯过程回归来解决这个问题。我们的框架基于高斯过程推理定理的一个关键推广，该定理适用于通过任意界面进行观察的情况。

    Linear partial differential equations (PDEs) are an important, widely applied class of mechanistic models, describing physical processes such as heat transfer, electromagnetism, and wave propagation. In practice, specialized numerical methods based on discretization are used to solve PDEs. They generally use an estimate of the unknown model parameters and, if available, physical measurements for initialization. Such solvers are often embedded into larger scientific models with a downstream application and thus error quantification plays a key role. However, by ignoring parameter and measurement uncertainty, classical PDE solvers may fail to produce consistent estimates of their inherent approximation error. In this work, we approach this problem in a principled fashion by interpreting solving linear PDEs as physics-informed Gaussian process (GP) regression. Our framework is based on a key generalization of the Gaussian process inference theorem to observations made via an arbitrary bou
    
[^122]: 基于生物标志物激活地图的可解释性糖尿病视网膜病变诊断

    Interpretable Diabetic Retinopathy Diagnosis based on Biomarker Activation Map. (arXiv:2212.06299v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2212.06299](http://arxiv.org/abs/2212.06299)

    本文提出了一种基于生物标志物激活地图（BAM）的诊断框架，使用两个U形生成器为临床医生提供意义明确的可解释性，以验证和理解分类器的决策，可应用于自动诊断糖尿病视网膜病变。

    

    深度学习分类器提供了基于光学相干断层扫描（OCT）及其血管影像学（OCTA）自动诊断糖尿病视网膜病变（DR）的最准确的手段。这些模型的优势部分归功于包含的隐藏层所提供的复杂性，从而实现了所需任务。然而，隐藏层也使算法输出难以解释。在这里，我们引入了一种基于生成对抗学习的新型生物标志物激活地图（BAM）框架，使临床医生能够验证和理解分类器的决策。一个数据集包括456个黄斑扫描根据当前临床标准被分级为非可转诊或可转诊DR。首先，基于此数据集使用DR分类器来评估我们的BAM。BAM生成框架是通过组合两个U形生成器来设计的，从而为此分类器提供有意义的可解释性。主要生成器是根据可转诊扫描进行训练的。

    Deep learning classifiers provide the most accurate means of automatically diagnosing diabetic retinopathy (DR) based on optical coherence tomography (OCT) and its angiography (OCTA). The power of these models is attributable in part to the inclusion of hidden layers that provide the complexity required to achieve a desired task. However, hidden layers also render algorithm outputs difficult to interpret. Here we introduce a novel biomarker activation map (BAM) framework based on generative adversarial learning that allows clinicians to verify and understand classifiers decision-making. A data set including 456 macular scans were graded as non-referable or referable DR based on current clinical standards. A DR classifier that was used to evaluate our BAM was first trained based on this data set. The BAM generation framework was designed by combing two U-shaped generators to provide meaningful interpretability to this classifier. The main generator was trained to take referable scans as
    
[^123]: 自我监督的目标导航中的现场微调机器人

    Self-Supervised Object Goal Navigation with In-Situ Finetuning. (arXiv:2212.05923v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.05923](http://arxiv.org/abs/2212.05923)

    本文将自我监督的机器人导航应用到目标导航中，基于位置一致性自我监督信号进行训练。该方法避免了标记昂贵的3D网格，可在真实世界和模拟中表现出有竞争力的结果。

    

    一个家庭机器人应该能够在不需要用户先注释家中所有物品的情况下导航到目标物品。目前大多数物品导航的方法并未在真实机器人上进行测试，而是仅依赖于重构的房屋扫描及昂贵标记半监督 3D 网格。本文的目标是通过探索构建机器人自我监督模型，正如儿童会做的一样。因此，我们放弃标记 3D 网格的成本，并在真实世界中启用自我监督现场微调机制。我们确定一个强大的自我监督源（位置一致性 - LocCon）可以训练 ObjectNav 代理中的所有组件，使用未注释的模拟房屋。我们的关键见解是，机身代理可以利用位置一致性作为自我监督信号，收集来自不同视角 /角度的图像，应用对比学习。我们展示了我们的代理程序可以在真实世界和模拟中表现出有竞争力的结果。

    A household robot should be able to navigate to target objects without requiring users to first annotate everything in their home. Most current approaches to object navigation do not test on real robots and rely solely on reconstructed scans of houses and their expensively labeled semantic 3D meshes. In this work, our goal is to build an agent that builds self-supervised models of the world via exploration, the same as a child might - thus we (1) eschew the expense of labeled 3D mesh and (2) enable self-supervised in-situ finetuning in the real world. We identify a strong source of self-supervision (Location Consistency - LocCon) that can train all components of an ObjectNav agent, using unannotated simulated houses. Our key insight is that embodied agents can leverage location consistency as a self-supervision signal collecting images from different views/angles and applying contrastive learning. We show that our agent can perform competitively in the real world and simulation. Our 
    
[^124]: Genie: 展示我量化的数据

    Genie: Show Me the Data for Quantization. (arXiv:2212.04780v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04780](http://arxiv.org/abs/2212.04780)

    Genie提出了一个后训练量化方案，用于开发轻量级深度神经网络，并提出了一个用于生成适合零样本量化的数据的框架。

    

    当数据因为各种原因（包括成本和隐私问题）无法访问时，零样本量化是开发轻量级深度神经网络的一种有前途的方法。通过利用FP32预训练模型中批归一化层的学习参数（$\mu$和$\sigma$），零样本量化方案专注于生成合成数据。随后，它们从预训练模型（教师）中提取知识，传递给量化模型（学生），使得量化模型可以使用合成数据集进行优化。然而，到目前为止，零样本量化主要在量化感知训练方法的上下文中讨论，这些方法需要任务特定的损失和长期的优化，就像需要重新训练一样。因此，我们引入了一个后训练量化方案，用于零样本量化，可以在几小时内生成高质量的量化网络。此外，我们提出了一个名为“Genie”的框架，用于生成适合零样本量化的数据。

    Zero-shot quantization is a promising approach for developing lightweight deep neural networks when data is inaccessible owing to various reasons, including cost and issues related to privacy. By exploiting the learned parameters ($\mu$ and $\sigma$) of batch normalization layers in an FP32-pre-trained model, zero-shot quantization schemes focus on generating synthetic data. Subsequently, they distill knowledge from the pre-trained model (teacher) to the quantized model (student) such that the quantized model can be optimized with the synthetic dataset. However, thus far, zero-shot quantization has primarily been discussed in the context of quantization-aware training methods, which require task-specific losses and long-term optimization as much as retraining. We thus introduce a post-training quantization scheme for zero-shot quantization that produces high-quality quantized networks within a few hours. Furthermore, we propose a framework called \genie~that generates data suited for q
    
[^125]: 关于神经网络和偏微分方程在物理学习中的兼容性

    On the Compatibility between Neural Networks and Partial Differential Equations for Physics-informed Learning. (arXiv:2212.00270v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2212.00270](http://arxiv.org/abs/2212.00270)

    本文探讨了物理知识神经网络的缺陷和机遇，证明了基于ReLU的MLP不能形成解决方案的合法函数空间，而使用带有输出层超平面的具有$C^n$激活函数的MLP可以严格满足一个线性PDE直到 $n$ 阶。

    

    本文探讨了物理知识神经网络的缺陷和机遇。我们证明了一个只使用ReLU（修正线性单元）或类ReLU的Lipschitz激活函数的多层感知器（MLP）将始终导致海森矩阵消失。这种网络所施加的限制与任何二阶或更高阶的偏微分方程（PDE）相矛盾。因此，基于ReLU的MLP不能形成解决方案的合法函数空间。在这个缺陷的启发下，我们证明了一个具有$C^n$激活函数的MLP，当其输出层的权重位于某个超平面（称为输出层超平面）上时，可以严格满足一个线性PDE直到 $n$ 阶。配备输出层超平面的MLP变得“物理强制执行”，不再需要针对PDE本身的损失函数（只需要初边值条件的损失函数）。这样的超平面不仅存在于MLP中，而且存在于任何网络架构的尾部。

    We shed light on a pitfall and an opportunity in physics-informed neural networks (PINNs). We prove that a multilayer perceptron (MLP) only with ReLU (Rectified Linear Unit) or ReLU-like Lipschitz activation functions will always lead to a vanished Hessian. Such a network-imposed constraint contradicts any second- or higher-order partial differential equations (PDEs). Therefore, a ReLU-based MLP cannot form a permissible function space for the approximation of their solutions. Inspired by this pitfall, we prove that a linear PDE up to the $n$-th order can be strictly satisfied by an MLP with $C^n$ activation functions when the weights of its output layer lie on a certain hyperplane, as called the out-layer-hyperplane. An MLP equipped with the out-layer-hyperplane becomes "physics-enforced", no longer requiring a loss function for the PDE itself (but only those for the initial and boundary conditions). Such a hyperplane exists not only for MLPs but for any network architecture tailed by
    
[^126]: SinGRAF：学习单个场景的3D生成辐射场

    SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene. (arXiv:2211.17260v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.17260](http://arxiv.org/abs/2211.17260)

    SinGRAF是一个3D感知的生成模型，只需少量输入一单个场景的图像即可训练，并可在保持输入外观的同时生成多样的3D场景。

    

    生成模型在合成逼真的3D物体方面表现出很大的潜力，但需要大量的训练数据。我们介绍了SinGRAF，这是一个3D感知的生成模型，只需少量输入一单个场景的图像即可训练。一旦训练完成，SinGRAF可以生成保留输入外观并变化场景布局的不同3D场景实现。为此，我们借鉴了近期3D GAN架构的进展，同时在训练过程中使用了一种新颖的渐进式规模补丁判别方法。通过几个实验，我们证明SinGRAF产生的结果在质量和多样性方面都远远优于最接近的相关作品。

    Generative models have shown great promise in synthesizing photorealistic 3D objects, but they require large amounts of training data. We introduce SinGRAF, a 3D-aware generative model that is trained with a few input images of a single scene. Once trained, SinGRAF generates different realizations of this 3D scene that preserve the appearance of the input while varying scene layout. For this purpose, we build on recent progress in 3D GAN architectures and introduce a novel progressive-scale patch discrimination approach during training. With several experiments, we demonstrate that the results produced by SinGRAF outperform the closest related works in both quality and diversity by a large margin.
    
[^127]: 计算效率高的强化学习：基于简单规则的有针对性探索

    Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules. (arXiv:2211.16691v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16691](http://arxiv.org/abs/2211.16691)

    本研究提出了一种基于简单规则的有针对性探索方法，通过避免已知子优的状态-动作空间区域来更快地加速强化学习代理程序的收敛，并在一个房间温度控制案例研究中实现了比传统方法快6-7倍的速度收敛。

    

    强化学习通常由于需要穷举探索状态-动作空间以找到表现良好的策略而导致样本复杂度不太好。然而，我们认为系统的专家知识通常允许我们设计简单规则，我们期望良好的策略始终遵循这些规则。因此，在本研究中，我们提出了一种简单而有效的连续演员-评论家框架的修改版本，以纳入这些规则并避免已知子优的状态-动作空间区域，从而显着加速强化学习代理程序的改进。具体而言，如果代理程序选择的动作不符合我们的直觉，我们会饱和这些动作，关键是修改策略的梯度更新步骤，以确保学习流程不受饱和步骤的影响。在一个房间温度控制案例研究中，它使代理程序以比传统代理程序快6-7倍的速度收敛到表现良好的策略，而不需要消耗额外的计算资源。

    Reinforcement Learning (RL) generally suffers from poor sample complexity, mostly due to the need to exhaustively explore the state-action space to find well-performing policies. On the other hand, we postulate that expert knowledge of the system often allows us to design simple rules we expect good policies to follow at all times. In this work, we hence propose a simple yet effective modification of continuous actor-critic frameworks to incorporate such rules and avoid regions of the state-action space that are known to be suboptimal, thereby significantly accelerating the convergence of RL agents. Concretely, we saturate the actions chosen by the agent if they do not comply with our intuition and, critically, modify the gradient update step of the policy to ensure the learning process is not affected by the saturation step. On a room temperature control case study, it allows agents to converge to well-performing policies up to 6-7x faster than classical agents without computational o
    
[^128]: 面向稀有事件的动态因果发现：一种非参数条件独立性检验

    Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test. (arXiv:2211.16596v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.16596](http://arxiv.org/abs/2211.16596)

    该论文提出了一种针对稀有事件的新的因果发现方法，基于收集到的时间不变动态系统的数据，构建了叠加数据集和条件独立性检验的方法。该方法能够揭示在变量第一次经历低概率实现时才会显现的因果关系，具有很好的可行性和可扩展性。

    

    与稀有事件相关联的因果现象在许多工程问题中都存在，例如针对风险的安全分析、事故分析和预防以及极值理论等。然而，当前的因果发现方法往往无法发现在随机变量之间的原因联系，特别是在变动环境下，仅在变量第一次经历低概率实现时才会显现。为了解决这个问题，我们引入了一种新的统计独立性检验方法，用于从发生稀有但具有重要影响的时间不变动态系统收集的数据中进行因果探索。具体而言，我们利用底层数据的时间不变性来构建一个叠加的数据集，其中包括在不同时间步骤之前稀有事件发生前系统状态的数据。然后我们设计了一个在重新组织的数据上进行条件独立性检验的方法。我们提供了我们方法一致性的非渐近样本复杂度界限，并验证了它在各种模拟和真实世界数据集上的性能。

    Causal phenomena associated with rare events occur across a wide range of engineering problems, such as risk-sensitive safety analysis, accident analysis and prevention, and extreme value theory. However, current methods for causal discovery are often unable to uncover causal links, between random variables in a dynamic setting, that manifest only when the variables first experience low-probability realizations. To address this issue, we introduce a novel statistical independence test on data collected from time-invariant dynamical systems in which rare but consequential events occur. In particular, we exploit the time-invariance of the underlying data to construct a superimposed dataset of the system state before rare events happen at different timesteps. We then design a conditional independence test on the reorganized data. We provide non-asymptotic sample complexity bounds for the consistency of our method, and validate its performance across various simulated and real-world datase
    
[^129]: 基于比较的层次聚类的收益函数

    A Revenue Function for Comparison-Based Hierarchical Clustering. (arXiv:2211.16459v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16459](http://arxiv.org/abs/2211.16459)

    本论文提出了一种收益函数，可以仅使用比较来衡量基于比较的分层聚类的质量。

    

    基于比较的学习解决的问题是当我们只有形式为“目标A与B相比较比C更相似”这样的比较，而没有显式特征或成对相似度时的学习问题。最近，已经证明在层次聚类中，可以直接使用这样的比较实现单一和完全链接，同时已经提出了几种算法来模拟平均链接的行为。因此，使用仅比较找到层次结构（或树状图）是一个被充分理解的问题。然而，当没有基准事实或显式相似性时，评估它们的意义仍然是一个开放性的问题。在本文中，我们通过提出一个新的收益函数来弥补这一差距，该函数允许我们仅使用比较来衡量树状图的好坏。文章还表明，该函数与使用成对相似性的层次聚类的Dasgupta成本密切相关。在理论方面，我们使用拥有好的性质的收益函数来分析比较的相关性，并推导了一些高斯过程的相关结论。

    Comparison-based learning addresses the problem of learning when, instead of explicit features or pairwise similarities, one only has access to comparisons of the form: \emph{Object $A$ is more similar to $B$ than to $C$.} Recently, it has been shown that, in Hierarchical Clustering, single and complete linkage can be directly implemented using only such comparisons while several algorithms have been proposed to emulate the behaviour of average linkage. Hence, finding hierarchies (or dendrograms) using only comparisons is a well understood problem. However, evaluating their meaningfulness when no ground-truth nor explicit similarities are available remains an open question.  In this paper, we bridge this gap by proposing a new revenue function that allows one to measure the goodness of dendrograms using only comparisons. We show that this function is closely related to Dasgupta's cost for hierarchical clustering that uses pairwise similarities. On the theoretical side, we use the propo
    
[^130]: 一种简单的方法教授Transformer多视角几何

    A Light Touch Approach to Teaching Transformers Multi-view Geometry. (arXiv:2211.15107v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.15107](http://arxiv.org/abs/2211.15107)

    本论文提出了一种轻触式的方法，引导视觉Transformer学习多视角几何，这种方法通过使用极线来引导Transformer的交叉注意力图，可以在测试时不需要提供任何摄像机姿态信息，适用于姿态不变的物体实例检索。

    

    Transformer在视觉学习中表现强大，这主要归因于它们缺乏手动规定的先验知识。然而，这种灵活性在涉及多视角几何的任务中可能会成为问题，因为3D形状和视点的近乎无限可能的变化需要灵活性，而投影几何的精确性则需要严格的规则。为了解决这个问题，我们提出了一种“轻触”方法，引导视觉Transformer学习多视角几何，但在需要时允许它们自由发挥。我们通过使用极线来引导Transformer的交叉注意力图，惩罚极线以外的注意值，并鼓励沿这些线的更高的注意，因为它们包含几何上合理的匹配。与以前的方法不同，我们的方法不需要在测试时提供任何摄像机姿态信息。我们关注于姿态不变的物体实例检索，标准的Transformer网络由于不同姿态之间的巨大差异而难以处理。

    Transformers are powerful visual learners, in large part due to their conspicuous lack of manually-specified priors. This flexibility can be problematic in tasks that involve multiple-view geometry, due to the near-infinite possible variations in 3D shapes and viewpoints (requiring flexibility), and the precise nature of projective geometry (obeying rigid laws). To resolve this conundrum, we propose a "light touch" approach, guiding visual Transformers to learn multiple-view geometry but allowing them to break free when needed. We achieve this by using epipolar lines to guide the Transformer's cross-attention maps, penalizing attention values outside the epipolar lines and encouraging higher attention along these lines since they contain geometrically plausible matches. Unlike previous methods, our proposal does not require any camera pose information at test-time. We focus on pose-invariant object instance retrieval, where standard Transformer networks struggle, due to the large diffe
    
[^131]: OReX：使用神经场从平面切片重建对象

    OReX: Object Reconstruction from Planar Cross-sections Using Neural Fields. (arXiv:2211.12886v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.12886](http://arxiv.org/abs/2211.12886)

    本文介绍了一种名为OReX的方法，使用神经场作为插值先验，仅使用输入的平面切片，即可高质量地重建三维形状。

    

    从平面截面重建三维形状是医学影像学和地理信息学等下游应用的挑战。输入是完全定义在空间中稀疏的平面集合上的内/外指示函数，输出是指示函数到整个体积的插值。以前解决这个稀疏和不适定问题的作品要么产生低质量的结果，要么依赖于额外的先验知识，如目标拓扑、外观信息或输入法向方向。在本文中，我们提出了OReX，一种仅使用切片即可进行三维形状重建的方法，其特点是以神经场作为插值先验。一个简单的神经网络被用来训练输入平面，以返回给定3D坐标的内/外估计，从而产生了强大的先验，能够诱导平滑性和自相似性。这种方法的主要挑战是高频细节，因为神经先验会过于平滑。为了缓解这一问题，我们提供了一种新颖的方法。

    Reconstructing 3D shapes from planar cross-sections is a challenge inspired by downstream applications like medical imaging and geographic informatics. The input is an in/out indicator function fully defined on a sparse collection of planes in space, and the output is an interpolation of the indicator function to the entire volume. Previous works addressing this sparse and ill-posed problem either produce low quality results, or rely on additional priors such as target topology, appearance information, or input normal directions. In this paper, we present OReX, a method for 3D shape reconstruction from slices alone, featuring a Neural Field as the interpolation prior. A modest neural network is trained on the input planes to return an inside/outside estimate for a given 3D coordinate, yielding a powerful prior that induces smoothness and self-similarities. The main challenge for this approach is high-frequency details, as the neural prior is overly smoothing. To alleviate this, we offe
    
[^132]: 用图神经网络和结构化状态空间模型建立多元生物信号模型

    Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models. (arXiv:2211.11176v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11176](http://arxiv.org/abs/2211.11176)

    本研究提出了一种基于时间依赖图的通用图神经网络结构(GraphS4mer)，用于建立多元生物信号模型。该模型结合了结构化状态空间架构和动态演变的图结构学习层来解决长时序和复杂空间相关性，能有效地提高多元生物信号分类任务性能。

    

    多元生物信号在许多医学领域中都很普遍，例如脑电图、多导睡眠图和心电图。由于（1）长时间范围内的时间依赖性和（2）电极之间复杂的空间相关性，建立多元生物信号的时空依赖关系模型是具有挑战性的。为了解决这些挑战，我们建议将多元生物信号表示为时间依赖图，并介绍了GraphS4mer，这是一种通用的图神经网络（GNN）结构，通过建立生物信号中的时空依赖关系来提高生物信号分类任务的性能。具体而言，（1）我们利用结构化状态空间架构，一种最先进的深度序列模型，来捕捉生物信号中长时间范围的时间依赖关系，并（2）我们建议在GraphS4mer中添加图结构学习层，以学习数据中动态演变的图结构。我们在三个不同的生物信号分类任务上评估我们的模型，并展示它优于几种基准模型，突显了它在建立具有复杂依赖关系的多元生物信号模型方面的有效性。

    Multivariate biosignals are prevalent in many medical domains, such as electroencephalography, polysomnography, and electrocardiography. Modeling spatiotemporal dependencies in multivariate biosignals is challenging due to (1) long-range temporal dependencies and (2) complex spatial correlations between the electrodes. To address these challenges, we propose representing multivariate biosignals as time-dependent graphs and introduce GraphS4mer, a general graph neural network (GNN) architecture that improves performance on biosignal classification tasks by modeling spatiotemporal dependencies in biosignals. Specifically, (1) we leverage the Structured State Space architecture, a state-of-the-art deep sequence model, to capture long-range temporal dependencies in biosignals and (2) we propose a graph structure learning layer in GraphS4mer to learn dynamically evolving graph structures in the data. We evaluate our proposed model on three distinct biosignal classification tasks and show th
    
[^133]: 学习使用用户级差分隐私生成图像嵌入

    Learning to Generate Image Embeddings with User-level Differential Privacy. (arXiv:2211.10844v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10844](http://arxiv.org/abs/2211.10844)

    本文提出了一种DP-FedEmb算法，通过虚拟客户端、部分聚合、私有本地微调和公共预训练，实现了用户级差分隐私。在图像嵌入模型的学习中，DP-FedEmb能够在保持良好模型效用的同时，实现较强的隐私保护，得到的实验结果表明其在基准数据集上的表现优越。

    

    在过去，用户级差分隐私已成功地用于训练小型设备上的模型，用于下一个单词预测和图像分类任务。然而，现有的方法在直接应用于使用大型类空间的受监督训练数据来学习嵌入式模型时可能会失败。为了实现大型图像到嵌入特征提取器的用户级差分隐私，我们提出了DP-FedEmb，这是一种联邦学习算法的变体，具有每个用户的灵敏度控制和噪声添加，以从在数据中心集中的用户分区数据中进行训练。DP-FedEmb结合了虚拟客户端、部分聚合、私有本地微调和公共预训练，以实现强大的隐私效用权衡。我们将DP-FedEmb应用于为面部、地标和自然物种训练图像嵌入模型，并在基准数据集DigiFace、EMNIST、GLD和iNaturalist上展示了其在相同隐私预算下的优越效用。我们进一步说明，在学习大型图像到嵌入特征提取器时，可以实现强大的用户级差分隐私，同时保持良好的模型效用。

    Small on-device models have been successfully trained with user-level differential privacy (DP) for next word prediction and image classification tasks in the past. However, existing methods can fail when directly applied to learn embedding models using supervised training data with a large class space. To achieve user-level DP for large image-to-embedding feature extractors, we propose DP-FedEmb, a variant of federated learning algorithms with per-user sensitivity control and noise addition, to train from user-partitioned data centralized in the datacenter. DP-FedEmb combines virtual clients, partial aggregation, private local fine-tuning, and public pretraining to achieve strong privacy utility trade-offs. We apply DP-FedEmb to train image embedding models for faces, landmarks and natural species, and demonstrate its superior utility under same privacy budget on benchmark datasets DigiFace, EMNIST, GLD and iNaturalist. We further illustrate it is possible to achieve strong user-level
    
[^134]: 应用可解释性设计物理感知卷积神经网络求解地下反问题

    Using explainability to design physics-aware CNNs for solving subsurface inverse problems. (arXiv:2211.08651v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08651](http://arxiv.org/abs/2211.08651)

    本研究提出了一种使用可解释性技术设计物理感知神经网络的新方法，通过开发卷积神经网络求解地下反问题展示应用价值。研究使用Score-CAM和Deep SHAP等方法选择超参数以提高解释性和预测准确性。

    

    本文提出了一种使用可解释性技术设计物理感知神经网络的新方法，并通过开发卷积神经网络 (CNN) 求解浅层地下成像反问题来展示我们的方法。虽然CNN在许多领域中近年来变得流行，但是它们的开发仍然是一种艺术，因为关于选择会产生最佳网络的超参数的明确指导并不存在。虽然可以使用优化算法自动选择超参数，但是这些方法着重于开发具有高预测准确性的网络，而忽略了模型解释性（描述准确性）。然而，可解释人工智能 (XAI) 领域通过提供允许开发者评估神经网络内部逻辑的工具来解决模型解释性的缺失。在本研究中，我们使用可解释性方法 Score-CAM 和 Deep SHAP 来选择超参数，例如 kern...

    We present a novel method of using explainability techniques to design physics-aware neural networks. We demonstrate our approach by developing a convolutional neural network (CNN) for solving an inverse problem for shallow subsurface imaging. Although CNNs have gained popularity in recent years across many fields, the development of CNNs remains an art, as there are no clear guidelines regarding the selection of hyperparameters that will yield the best network. While optimization algorithms may be used to select hyperparameters automatically, these methods focus on developing networks with high predictive accuracy while disregarding model explainability (descriptive accuracy). However, the field of Explainable Artificial Intelligence (XAI) addresses the absence of model explainability by providing tools that allow developers to evaluate the internal logic of neural networks. In this study, we use the explainability methods Score-CAM and Deep SHAP to select hyperparameters, such as ker
    
[^135]: 可解释的多智能体强化学习中的行为建议

    Explainable Action Advising for Multi-Agent Reinforcement Learning. (arXiv:2211.07882v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.07882](http://arxiv.org/abs/2211.07882)

    引入可解释行为建议的多智能体强化学习框架，使得学生可以理解所学的内容并进行推理从而提高样本效率和学习效果

    

    行为建议是一种基于师生范式的强化学习知识转移技术。专家老师在训练期间提供建议，以提高学生的样本效率和策略表现。这种建议通常以状态-动作对的形式给出。然而，这使得学生难以推理和应用于新颖状态。我们引入了可解释的行为建议，其中老师提供行为建议和相关的解释，说明为什么选取该行为.这允许学生自我反思所学的内容，实现建议的泛化，并导致学习效率的提高——即使在老师不理想的情况下，也可以有效地应用于单智能体和多智能体场景中。我们通过实验证明，与最先进的方法相比，我们的框架可以产生更好的策略回报和收敛速率。

    Action advising is a knowledge transfer technique for reinforcement learning based on the teacher-student paradigm. An expert teacher provides advice to a student during training in order to improve the student's sample efficiency and policy performance. Such advice is commonly given in the form of state-action pairs. However, it makes it difficult for the student to reason with and apply to novel states. We introduce Explainable Action Advising, in which the teacher provides action advice as well as associated explanations indicating why the action was chosen. This allows the student to self-reflect on what it has learned, enabling advice generalization and leading to improved sample efficiency and learning performance - even in environments where the teacher is sub-optimal. We empirically show that our framework is effective in both single-agent and multi-agent scenarios, yielding improved policy returns and convergence rates when compared to state-of-the-art methods
    
[^136]: 论文标题：量化标签噪声对联邦学习的影响

    Quantifying the Impact of Label Noise on Federated Learning. (arXiv:2211.07816v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07816](http://arxiv.org/abs/2211.07816)

    本文量化了标签噪声对FL的影响。实验结果表明，随着噪声水平的增加，全局模型的准确性会线性下降，同时会导致FL训练的收敛速度减缓和全局模型过拟合。

    

    联邦学习是一种分布式机器学习范例，客户端可以使用本地（人为生成的）数据集协同训练模型。然而，现有研究集中在FL算法的开发上以解决客户端之间的数据异质性，而在FL中数据质量（如标签噪声）这一重要问题被忽视。本文旨在通过对标签噪声对FL的影响进行定量研究来填补这一空白。我们推导了一种上界来衡量客户端标签噪声水平对泛化误差的影响，并使用各种FL算法在MNIST和CIFAR-10数据集上进行实验。我们的实证结果表明，随着噪声水平的增加，全局模型准确性会线性下降，这与我们的理论分析相一致。我们进一步发现，在标签噪声较高时，标签噪声会减缓FL训练的收敛速度，并导致全局模型过拟合。

    Federated Learning (FL) is a distributed machine learning paradigm where clients collaboratively train a model using their local (human-generated) datasets. While existing studies focus on FL algorithm development to tackle data heterogeneity across clients, the important issue of data quality (e.g., label noise) in FL is overlooked. This paper aims to fill this gap by providing a quantitative study on the impact of label noise on FL. We derive an upper bound for the generalization error that is linear in the clients' label noise level. Then we conduct experiments on MNIST and CIFAR-10 datasets using various FL algorithms. Our empirical results show that the global model accuracy linearly decreases as the noise level increases, which is consistent with our theoretical analysis. We further find that label noise slows down the convergence of FL training, and the global model tends to overfit when the noise level is high.
    
[^137]: 人类对神经网络表示的对齐

    Human alignment of neural network representations. (arXiv:2211.01201v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.01201](http://arxiv.org/abs/2211.01201)

    本文研究神经网络表示与人类心理表示之间的对齐问题，发现模型规模和体系结构对对齐几乎没有影响，而训练数据集和目标函数都对对齐有很大的影响。从一个数据集中学习的神经网络表示的线性变换能显著提高对另外两个数据集中人类相似性判断的对齐性。

    

    当今的计算机视觉模型在各种视觉任务上实现了人类或接近人类水平的性能。然而，它们的体系结构、数据和学习算法与导致人类视觉的方式存在许多不同之处。本文研究影响神经网络所学习的表示与通过行为反应推断出的人类心理表示之间对齐的因素。我们发现，模型的规模和体系结构对与人类行为反应的对齐基本上没有影响，而训练数据集和目标函数则具有更大的影响。这些发现在使用两种不同任务收集的三个人类相似度判断数据集中保持一致。从一个数据集中学习的神经网络表示的线性变换显著提高了对另外两个数据集中的人类相似度判断的对齐性。此外，我们发现，一些人类概念...

    Today's computer vision models achieve human or near-human level performance across a wide variety of vision tasks. However, their architectures, data, and learning algorithms differ in numerous ways from those that give rise to human vision. In this paper, we investigate the factors that affect the alignment between the representations learned by neural networks and human mental representations inferred from behavioral responses. We find that model scale and architecture have essentially no effect on the alignment with human behavioral responses, whereas the training dataset and objective function both have a much larger impact. These findings are consistent across three datasets of human similarity judgments collected using two different tasks. Linear transformations of neural network representations learned from behavioral responses from one dataset substantially improve alignment with human similarity judgments on the other two datasets. In addition, we find that some human concept
    
[^138]: Pop2Piano: 基于流行音频的钢琴翻奏生成

    Pop2Piano : Pop Audio-based Piano Cover Generation. (arXiv:2211.00895v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.00895](http://arxiv.org/abs/2211.00895)

    本文提出了Pop2Piano，一个通过Transformer网络，直接从流行音频生成钢琴翻奏的模型。使用自动生成的大量配对和同步数据进行训练，该模型在生成合理的钢琴翻奏方面具有潜力。

    

    流行音乐的钢琴翻奏深受很多人的喜爱。然而，自动生成流行音乐的钢琴翻奏的任务仍然缺乏研究。部分原因是缺少同步的{流行，钢琴翻奏}数据对，这使得应用最新的数据密集型深度学习方法变得具有挑战性。为了发挥数据驱动方法的威力，我们使用自动化流程生成了大量配对和同步的{流行，钢琴翻奏}数据。在本文中，我们提出了Pop2Piano，一个能够根据流行音乐的波形生成钢琴翻奏的Transformer网络。据我们所知，这是第一个直接从流行音频生成钢琴翻奏的模型，而无需使用旋律和和弦提取模块。我们展示了使用我们数据集训练的Pop2Piano能够产生合理的钢琴翻奏。

    Piano covers of pop music are enjoyed by many people. However, the task of automatically generating piano covers of pop music is still understudied. This is partly due to the lack of synchronized {Pop, Piano Cover} data pairs, which made it challenging to apply the latest data-intensive deep learning-based methods. To leverage the power of the data-driven approach, we make a large amount of paired and synchronized {Pop, Piano Cover} data using an automated pipeline. In this paper, we present Pop2Piano, a Transformer network that generates piano covers given waveforms of pop music. To the best of our knowledge, this is the first model to generate a piano cover directly from pop audio without using melody and chord extraction modules. We show that Pop2Piano, trained with our dataset, is capable of producing plausible piano covers.
    
[^139]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^140]: 提升I$^2$-GNN在循环计数方面的图神经网络性能

    Boosting the Cycle Counting Power of Graph Neural Networks with I$^2$-GNNs. (arXiv:2210.13978v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13978](http://arxiv.org/abs/2210.13978)

    本文研究了一种名为Subgraph MPNNs的GNN模型。我们发现，Subgraph MPNNs不能在节点级别上计数超过4个的环，这对于生物学、化学和社交网络分析等应用至关重要。

    

    消息传递神经网络(MPNNs)是一类被广泛应用的图神经网络(GNNs)。然而，MPNNs的表达能力有限，这启发我们研究可证明具有更强表达能力的GNN体系结构。本文提出研究子图MPNNs的计数能力，这是一类最新和常用的强大GNN模型，其从每个节点提取根据子图，在根节点分配唯一标识符并在其根据子图中编码根节点的表示。具体地，我们证明子图MPNNs不能在节点级别上计数超过4个的环，这意味着节点表示不能正确地编码周围的子结构。

    Message Passing Neural Networks (MPNNs) are a widely used class of Graph Neural Networks (GNNs). The limited representational power of MPNNs inspires the study of provably powerful GNN architectures. However, knowing one model is more powerful than another gives little insight about what functions they can or cannot express. It is still unclear whether these models are able to approximate specific functions such as counting certain graph substructures, which is essential for applications in biology, chemistry and social network analysis. Motivated by this, we propose to study the counting power of Subgraph MPNNs, a recent and popular class of powerful GNN models that extract rooted subgraphs for each node, assign the root node a unique identifier and encode the root node's representation within its rooted subgraph. Specifically, we prove that Subgraph MPNNs fail to count more-than-4-cycles at node level, implying that node representations cannot correctly encode the surrounding substru
    
[^141]: RibSeg v2：肋骨标记和解剖中心线提取的大规模基准测试

    RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical Centerline Extraction. (arXiv:2210.09309v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2210.09309](http://arxiv.org/abs/2210.09309)

    本文扩展了RibSeg数据集到大规模基准测试RibSeg v2，加入了手动标注的肋骨标记和解剖中心线提取，共包含660个CT扫描（15,466个独立的肋骨），并提出了深度学习方法用于肋骨标记、基于骨架化方法用于中心线提取、一种稀疏点云表示CT扫描的方法，以及适用于该任务的评估指标。

    

    自动化的肋骨标记和解剖中心线提取是各种临床应用的常见前提条件。以往的研究要么使用内部数据集，无法为社群所共享，要么只关注于肋骨分割而忽略了肋骨标记的临床意义。为了解决这些问题，本文将之前计算机断层扫描（CT）肋骨分割的RibSeg数据集扩展为综合性基准测试RibSeg v2，并加入了肋骨标记和解剖中心线提取的手动标注，共包含了660个CT扫描（15,466个独立的肋骨）。基于RibSeg v2数据集，我们开发了一种包含深度学习方法用于肋骨标记，以及基于骨架化方法用于中心线提取的流程。为了提高计算效率，我们还提出了一种稀疏点云表示CT扫描的方法，与标准的密集体素网格进行了比较。此外，我们还设计和分析评估指标，以解决每个任务的关键挑战。我们的数据集是，…

    Automatic rib labeling and anatomical centerline extraction are common prerequisites for various clinical applications. Prior studies either use in-house datasets that are inaccessible to communities, or focus on rib segmentation that neglects the clinical significance of rib labeling. To address these issues, we extend our prior dataset (RibSeg) on the binary rib segmentation task to a comprehensive benchmark, named RibSeg v2, with 660 CT scans (15,466 individual ribs in total) and annotations manually inspected by experts for rib labeling and anatomical centerline extraction. Based on the RibSeg v2, we develop a pipeline including deep learning-based methods for rib labeling, and a skeletonization-based method for centerline extraction. To improve computational efficiency, we propose a sparse point cloud representation of CT scans and compare it with standard dense voxel grids. Moreover, we design and analyze evaluation metrics to address the key challenges of each task. Our dataset,
    
[^142]: 学习高效计划稳健的摩擦多物体抓取

    Learning to Efficiently Plan Robust Frictional Multi-Object Grasps. (arXiv:2210.07420v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.07420](http://arxiv.org/abs/2210.07420)

    本文介绍了一种使用神经网络进行摩擦多物体抓取的高效计划方法，相比于以往的工作，其成功率提高了13.7％，每小时的拾取次数增加了1.6倍，并且抓取计划时间减少了6.3倍。

    

    本文考虑了一个杂乱问题，多个刚性凸多边形物体随机放置在一个平面表面上，必须使用单个和多个物体的抓取方式，将它们有效地运输到装箱中。我们引入摩擦来增加每小时的拾取次数，并使用实例进行神经网络的训练，以计划稳健的多物体抓取。在物理实验中，相比于多物体抓取的先前工作，我们发现成功率增加了13.7％，每小时的拾取次数增加了1.6倍，抓取计划时间减少了6.3倍。与单个物体抓取相比，我们发现每小时的拾取次数增加了3.1倍。

    We consider a decluttering problem where multiple rigid convex polygonal objects rest in randomly placed positions and orientations on a planar surface and must be efficiently transported to a packing box using both single and multi-object grasps. Prior work considered frictionless multi-object grasping. In this paper, we introduce friction to increase picks per hour. We train a neural network using real examples to plan robust multi-object grasps. In physical experiments, we find a 13.7% increase in success rate, a 1.6x increase in picks per hour, and a 6.3x decrease in grasp planning time compared to prior work on multi-object grasping. Compared to single object grasping, we find a 3.1x increase in picks per hour.
    
[^143]: 自监督几何对应用于野外类别级6D物体姿态估计

    Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild. (arXiv:2210.07199v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.07199](http://arxiv.org/abs/2210.07199)

    本文提出了一种自监督学习方法，直接在大规模真实世界物体视频上进行类别级6D姿态估计。通过表面嵌入学习了输入图像和规范形状之间的密集对应关系，并提出了新颖的几何循环一致性损失。学习到的对应关系可以应用于6D姿态估计和其他任务。

    This paper proposes a self-supervised learning approach for category-level 6D object pose estimation in the wild, which reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. The proposed novel geometrical cycle-consistency losses construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and other tasks.

    尽管6D物体姿态估计在计算机视觉和机器人领域有广泛的应用，但由于缺乏注释，它仍然远未解决。当转向类别级6D姿态时，问题变得更加具有挑战性，因为需要对未见实例进行泛化。目前的方法受到从模拟或从人类收集的注释的限制。在本文中，我们通过引入一种自监督学习方法，直接在大规模真实世界物体视频上进行类别级6D姿态估计，克服了这一障碍。我们的框架重构了物体类别的规范3D形状，并通过表面嵌入学习了输入图像和规范形状之间的密集对应关系。对于训练，我们提出了新颖的几何循环一致性损失，它们在2D-3D空间、不同实例和不同时间步之间构建循环。学习到的对应关系可以应用于6D姿态估计和其他任务。

    While 6D object pose estimation has wide applications across computer vision and robotics, it remains far from being solved due to the lack of annotations. The problem becomes even more challenging when moving to category-level 6D pose, which requires generalization to unseen instances. Current approaches are restricted by leveraging annotations from simulation or collected from humans. In this paper, we overcome this barrier by introducing a self-supervised learning approach trained directly on large-scale real-world object videos for category-level 6D pose estimation in the wild. Our framework reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. For training, we propose novel geometrical cycle-consistency losses which construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and othe
    
[^144]: 自我指导扩散模型

    Self-Guided Diffusion Models. (arXiv:2210.06462v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.06462](http://arxiv.org/abs/2210.06462)

    本文提出了一种框架，利用自我监督信号的灵活性设计了自我指导扩散模型。实验表明，自标记指导始终优于没有指导的扩散模型，并且在不平衡数据上甚至可以超过基于真实标签的指导。

    

    扩散模型在图像生成方面取得了显著进展，特别是在使用指导来控制生成过程时。然而，指导需要大量的图像-注释对进行训练，因此依赖于其可用性、正确性和无偏性。本文提出一种可以消除这种注释需求的框架，利用自我监督信号的灵活性设计了自我指导扩散模型。通过利用特征提取函数和自我注释函数，我们的方法在各种图像粒度上提供指导信号：从整体图像到物体框，甚至到分割蒙版。我们在单标签和多标签图像数据集上的实验表明，自标记指导始终优于没有指导的扩散模型，并且在不平衡数据上甚至可以超过基于真实标签的指导。

    Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability, correctness and unbiasedness. In this paper, we eliminate the need for such annotation by instead leveraging the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels, especially on unbalanced data. When equipped with self-supervised box or m
    
[^145]: Mask-adapted CLIP的开放词汇语义分割

    Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP. (arXiv:2210.04150v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.04150](http://arxiv.org/abs/2210.04150)

    本文提出了一种基于CLIP和带掩膜的体系结构的开放词汇语义分割方法，通过在用于训练的嘈杂但多样化的数据集上对CLIP进行微调，以提高其在带有掩膜的图像上的性能，超越了当前最佳的方法。

    

    开放词汇语义分割的目的是根据文本描述将图像分割为语义区域，这些描述可能在训练期间没有被观察到。最近的两阶段方法首先生成不考虑类的掩码提议，然后利用预训练的视觉-语言模型（例如CLIP）对掩码区域进行分类。我们确定了这一范例的性能瓶颈是预训练CLIP模型，因为它在遮蔽图像上的表现不佳。为了解决这个问题，我们提出在一组带有掩码图像区域及其对应的文本描述的数据上对CLIP进行微调。我们通过使用CLIP将掩码图像区域与图像描述中的名词匹配来挖掘现有图像-标题数据集（例如COCO Captions）来收集训练数据。与更精确且手动注释的固定类别分割标签（例如COCO-Stuff）相比，我们发现我们的嘈杂但多样化的数据集能更好地保留CLIP的泛化能力。除了微调整个模型外，我们还引入了适应掩码的CLIP体系结构，通过明确地建模蒙版过程来更好地处理带有掩码的图像。我们在开放词汇语义分割基准OpenImages上的实验证明，我们的方法优于最先进的方法。

    Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions. We collect training data by mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, w
    
[^146]: 有生物学约束的解缠论：功能细胞类型的理论。

    Disentanglement with Biological Constraints: A Theory of Functional Cell Types. (arXiv:2210.01768v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2210.01768](http://arxiv.org/abs/2210.01768)

    本文通过数学证明，简单的生物学约束（如在活动和权重方面的非负性和能量效率）可以促进大脑神经元的单因素选择性，从而实现解缠表示，解释了为什么大脑中的单个神经元经常表示单个可解释因素。

    

    大脑中的神经元经常对特定任务变量进行微调。此外，这种解缠的表示在机器学习中备受追捧。在这里，我们数学证明了对神经元的简单生物学约束（即在活动和权重方面的非负性和能量效率）通过强制神经元对任务变化的单个因素具有选择性，从而促进了这种被追求的解缠表示。我们展示了这些约束可导致各种任务和架构下的解缠，包括变分自编码器。我们还使用这个理论解释了为什么大脑将其细胞分成不同的细胞类型（例如网格和对象向量细胞），以及解释了大脑何时对纠缠的任务因素进行纠缠的表示。总体而言，这项工作提供了对为什么大脑中的单个神经元经常表示单个可解释因素的数学理解，并迈向了解决任务表示的目标。

    Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentanglement in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why single neurons in the brain often represent single human-interpretable factors, and steps towards an understanding task
    
[^147]: 适用于视觉与语言模型的LASP：面向语言感知的文本优化的文本提示。

    LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models. (arXiv:2210.01115v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.01115](http://arxiv.org/abs/2210.01115)

    本文提出了一种针对软模板学习中存在的基类过拟合问题的文本提示学习方法——LASP, 同时通过增加提示的表示能力和校准视觉-语言不匹配问题， 在三个下游任务上取得了显著的性能优于现有技术的实验结果。

    

    软模板学习最近已成为适应下游任务的V&L模型的选择方法之一，但当前的方法在经过训练数据的泛化性能方面存在较大缺陷。针对这一问题，本文提出了一种基于文本到文本交叉熵损失的语言感知的文本提示（LASP）学习方法，该方法可以有效地减少基类的过拟合，增加提示的表示能力，校准视觉-语言不匹配问题，并在三个下游任务上取得了显著的性能优于现有技术的实验结果。

    Soft prompt learning has recently emerged as one of the methods of choice for adapting V&L models to a downstream task using a few training examples. However, current methods significantly overfit the training data, suffering from large accuracy degradation when tested on unseen classes from the same domain. To this end, in this paper, we make the following 4 contributions: (1) To alleviate base class overfitting, we propose a novel Language-Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts. (2) To increase the representation capacity of the prompts, we propose grouped LASP where each group of prompts is optimized with respect to a separate subset of textual prompts. (3) We identify a visual-language misalignment introduced by prompt learning and LASP, and more importantly, propose a re-calibration mechanism to
    
[^148]: (子)黎曼几何PDE-G-CNN的分析

    Analysis of (sub-)Riemannian PDE-G-CNNs. (arXiv:2210.00935v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00935](http://arxiv.org/abs/2210.00935)

    本文为PDE-G-CNN模型中的形态学卷积核问题提供了新的解决方案，解决了先前推荐的近似核不准确的问题。

    

    群等变卷积神经网络 (G-CNN) 在几何深度学习中得到了成功的应用。PDE-G-CNN框架是对G-CNN的推广，其主要优点是同时降低网络复杂性、提高分类性能和提供几何可解释性。本文发现先前推荐的近似形态学核不总是准确的，具体而言，取决于黎曼度量的空间各向异性，我们认为必须使用子黎曼逼近。我们通过提供新的近似核来解决这个问题。

    Group equivariant convolutional neural networks (G-CNNs) have been successfully applied in geometric deep learning. Typically, G-CNNs have the advantage over CNNs that they do not waste network capacity on training symmetries that should have been hard-coded in the network. The recently introduced framework of PDE-based G-CNNs (PDE-G-CNNs) generalises G-CNNs. PDE-G-CNNs have the core advantages that they simultaneously 1) reduce network complexity, 2) increase classification performance, and 3) provide geometric interpretability. Their implementations primarily consist of linear and morphological convolutions with kernels.  In this paper we show that the previously suggested approximative morphological kernels do not always accurately approximate the exact kernels accurately. More specifically, depending on the spatial anisotropy of the Riemannian metric, we argue that one must resort to sub-Riemannian approximations. We solve this problem by providing a new approximative kernel that w
    
[^149]: 通过模块化和组合来修补卷积神经网络模型的弱点

    Patching Weak Convolutional Neural Network Models through Modularization and Composition. (arXiv:2209.06116v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06116](http://arxiv.org/abs/2209.06116)

    本文提出了一种通过压缩模块化和组合来修补卷积神经网络模型的弱点的方法，该方法无需重新训练整个模型，并在多个基准数据集上实现了与最先进方法相当甚至更好的结果。

    

    尽管深度神经网络在许多应用程序中取得了巨大成功，但在实践中并不总是具有鲁棒性。本文关注的是修补卷积神经网络模型的弱点，而不是通过昂贵的重新训练整个模型来改进它。我们提出了一种压缩模块化方法CNNSplitter，它将具有$N$类分类任务的强CNN模型分解为$N$个较小的CNN模块。每个模块是一个子模型，包含强模型的部分卷积核。为了修补在目标类别（TC）上表现不佳的弱CNN模型，我们将其与从强CNN模型中获得的相应模块相结合。这样，弱CNN模型识别TC的能力可以大大提高，而无需重新训练整个模型。我们在几个基准数据集上展示了我们提出的方法的有效性，并表明它实现了与最先进方法相当甚至更好的结果。

    Despite great success in many applications, deep neural networks are not always robust in practice. For instance, a convolutional neuron network (CNN) model for classification tasks often performs unsatisfactorily in classifying some particular classes of objects. In this work, we are concerned with patching the weak part of a CNN model instead of improving it through the costly retraining of the entire model. Inspired by the fundamental concepts of modularization and composition in software engineering, we propose a compressed modularization approach, CNNSplitter, which decomposes a strong CNN model for $N$-class classification into $N$ smaller CNN modules. Each module is a sub-model containing a part of the convolution kernels of the strong model. To patch a weak CNN model that performs unsatisfactorily on a target class (TC), we compose the weak CNN model with the corresponding module obtained from a strong CNN model. The ability of the weak CNN model to recognize the TC can thus be
    
[^150]: R\'{e}nyi散度深度互相学习

    R\'{e}nyi Divergence Deep Mutual Learning. (arXiv:2209.05732v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05732](http://arxiv.org/abs/2209.05732)

    本文提出在深度互相学习中使用R\'{e}nyi散度，它能够在不引入大量复杂度的情况下持续提高性能，获得了广泛的实证结果的支持。

    

    本文重审了一种简单而有效的计算范式——深度互相学习（DML）。我们提出使用R\'{e}nyi散度而不是KL散度，这种做法更加灵活、可调，以改善vanilla DML。这种修改能够在有限的附加复杂性下不断提高性能。该范例的收敛性进行了理论分析，并且表明具有恒定学习率的随机梯度下降在非凸优化任务的最坏情况下收敛的偏差为$\mathcal{O}(1)$。

    This paper revisits Deep Mutual Learning (DML), a simple yet effective computing paradigm. We propose using R\'{e}nyi divergence instead of the KL divergence, which is more flexible and tunable, to improve vanilla DML. This modification is able to consistently improve performance over vanilla DML with limited additional complexity. The convergence properties of the proposed paradigm are analyzed theoretically, and Stochastic Gradient Descent with a constant learning rate is shown to converge with $\mathcal{O}(1)$-bias in the worst case scenario for nonconvex optimization tasks. That is, learning will reach nearby local optima but continue searching within a bounded scope, which may help mitigate overfitting. Finally, our extensive empirical results demonstrate the advantage of combining DML and R\'{e}nyi divergence, which further improves generalized models.
    
[^151]: 通过加强量子核函数的效率来提高分类准确性的纠缠作用

    The role of entanglement for enhancing the efficiency of quantum kernels towards classification. (arXiv:2209.05142v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2209.05142](http://arxiv.org/abs/2209.05142)

    本论文通过使用基于线性和全纠缠电路作为超参数的量子核函数，在量子支持向量机中加强了纠缠作用，提高了分类准确性和表达能力。所提出的全纠缠电路在大多数特征上优于其他全纠缠或线性纠缠电路以及经典算法。

    

    量子核函数被认为是展示量子计算在机器学习中优势的潜在资源。考虑到超参数对经典机器学习模型性能的影响，使用基于量子核的方法识别有前途的超参数以实现量子优势至关重要。在此工作中，我们使用一种新的基于线性和全纠缠电路的量子核，作为控制单词之间相关性的超参数，以分析和分类文本数据的情感。我们发现，使用线性和全纠缠电路进一步控制量子支持向量机（QSVM）的表达能力。此外，我们还比较了所提出电路与其他量子电路和经典机器学习算法的效率。结果表明，所提出的全纠缠电路在大多数特征上优于所有其他全纠缠或线性纠缠电路以及经典算法。

    Quantum kernels are considered as potential resources to illustrate benefits of quantum computing in machine learning. Considering the impact of hyperparameters on the performance of a classical machine learning model, it is imperative to identify promising hyperparameters using quantum kernel methods in order to achieve quantum advantages. In this work, we analyse and classify sentiments of textual data using a new quantum kernel based on linear and full entangled circuits as hyperparameters for controlling the correlation among words. We also find that the use of linear and full entanglement further controls the expressivity of the Quantum Support Vector Machine (QSVM). In addition, we also compare the efficiency of the proposed circuit with other quantum circuits and classical machine learning algorithms. Our results show that the proposed fully entangled circuit outperforms all other fully or linearly entangled circuits in addition to classical algorithms for most of the features. 
    
[^152]: 视觉中的扩散模型：一项综述

    Diffusion Models in Vision: A Survey. (arXiv:2209.04747v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.04747](http://arxiv.org/abs/2209.04747)

    扩散模型是视觉中的新兴主题，其生成样本的质量和多样性受到广泛欣赏。本综述介绍了三种通用的扩散建模方法以及各种算法和架构方面的讨论，并总结比较了扩散模型与其他最先进的生成模型的性能。

    

    去噪扩散模型是计算机视觉领域中的新兴主题，展现了在生成建模领域中非凡的结果。扩散模型是一种基于深度学习的生成模型，由两个阶段组成，前向扩散和反向扩散。在前向扩散阶段，通过逐步添加高斯噪声逐渐扰动输入数据。在反向阶段，模型被任务为通过逐步学习逆转扩散过程，逐步恢复原始输入数据。尽管扩散模型的计算负担较大，即由于在采样过程中涉及的步骤数量较多导致的速度较慢，但其所生成样本的质量和多样性仍然受到广泛欣赏。在本篇文章中，我们提供了一个关于去噪扩散模型在视觉中应用的综合性评论，包括该领域的理论和实践贡献。首先，我们确定并介绍了三种通用的扩散建模方法：连续、离散和混合扩散模型。接着，我们讨论了扩散模型的各种算法和架构方面，如使用 Lévy 过程、不同形式的噪声、模型条件和正则化、多尺度架构和并行化技术。最后，我们总结并比较了去噪扩散模型与其他最先进的生成模型在多个数据集上的性能。

    Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modelin
    
[^153]: 从潜在动力学到有意义的表示法

    From latent dynamics to meaningful representations. (arXiv:2209.00905v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.00905](http://arxiv.org/abs/2209.00905)

    本文提出了一个动力学约束的表示学习框架，通过限制潜在表示遵循特定动态规律以使得学习到的表示具有意义。在真实世界的DNA荧光电影数据集上验证了该算法，表明其可以准确地学习动态规律，并获得有意义的表示。

    

    虽然表示学习已成为机器学习和人工智能崛起的核心，但一个关键问题仍然是使学习到的表示具有意义。为此，典型的方法是通过先验概率分布来规范学习到的表示。然而，这样的先验通常是不可用或临时的。为了解决这个问题，我们提出了一个动力学约束的表示学习框架。我们不使用预定义的概率，而是限制潜在表示遵循特定的动态规律，这是动态系统表示学习更自然的约束。我们的信仰源于物理学中的一个基本观察，即虽然不同的系统可以有不同的边际概率分布，但通常遵循相同的动态规律，例如牛顿和薛定谔方程。我们对不同系统验证了我们的框架，包括一个真实世界的荧光DNA电影数据集。我们展示了我们的算法可以准确地学习动态规律，并获得有意义的表示。

    While representation learning has been central to the rise of machine learning and artificial intelligence, a key problem remains in making the learnt representations meaningful. For this the typical approach is to regularize the learned representation through prior probability distributions. However such priors are usually unavailable or ad hoc. To deal with this, we propose a dynamics-constrained representation learning framework. Instead of using predefined probabilities, we restrict the latent representation to follow specific dynamics, which is a more natural constraint for representation learning in dynamical systems. Our belief stems from a fundamental observation in physics that though different systems can have different marginalized probability distributions, they typically obey the same dynamics, such as Newton's and Schrodinger's equations. We validate our framework for different systems including a real-world fluorescent DNA movie dataset. We show that our algorithm can un
    
[^154]: 平滑单调随机变分不等式与鞍点问题：综述

    Smooth Monotone Stochastic Variational Inequalities and Saddle Point Problems: A Survey. (arXiv:2208.13592v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2208.13592](http://arxiv.org/abs/2208.13592)

    本文综述了解决平滑（强）单调随机变分不等式的方法。

    

    本文综述了解决平滑（强）单调随机变分不等式的方法。首先，我们给出了随机方法最终演变的确定性基础。然后我们回顾了一般随机公式的方法，看看有限和设置。最后部分是致力于各种最近的（不一定是随机的）算法变分不等式的进步的。

    This paper is a survey of methods for solving smooth (strongly) monotone stochastic variational inequalities. To begin with, we give the deterministic foundation from which the stochastic methods eventually evolved. Then we review methods for the general stochastic formulation, and look at the finite sum setup. The last parts of the paper are devoted to various recent (not necessarily stochastic) advances in algorithms for variational inequalities.
    
[^155]: 线性结构方程模型的因果赌博算法

    Causal Bandits for Linear Structural Equation Models. (arXiv:2208.12764v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.12764](http://arxiv.org/abs/2208.12764)

    本文提出了针对线性结构方程模型的因果赌博算法，摒弃了已知干预分布或其边缘分布的假设。

    

    本文研究了设计最优干预顺序，以最小化关于后见最佳干预的累积遗憾的问题。这是一个因果赌博问题，主要关注线性结构方程模型（SEM）和软干预的因果赌博问题。假设图的结构已知并且具有N个节点，每个节点假设两个线性机制，一个软干预和一个观测值，共产生$2^N$种可能干预。大部分现有的因果赌博算法假设至少已经完全指定了奖励节点的父节点干预分布。然而，在中等大小的图中，需要获得$2^N$个干预分布（每个干预对应一个干预分布），这变得不切实际。本文摒弃了知道这些分布或其边缘分布的假设，为频率派提出了两种算法。

    This paper studies the problem of designing an optimal sequence of interventions in a causal graphical model to minimize cumulative regret with respect to the best intervention in hindsight. This is, naturally, posed as a causal bandit problem. The focus is on causal bandits for linear structural equation models (SEMs) and soft interventions. It is assumed that the graph's structure is known and has $N$ nodes. Two linear mechanisms, one soft intervention and one observational, are assumed for each node, giving rise to $2^N$ possible interventions. Majority of the existing causal bandit algorithms assume that at least the interventional distributions of the reward node's parents are fully specified. However, there are $2^N$ such distributions (one corresponding to each intervention), acquiring which becomes prohibitive even in moderate-sized graphs. This paper dispenses with the assumption of knowing these distributions or their marginals. Two algorithms are proposed for the frequentist
    
[^156]: 机器学习与机密计算：知识系统化的概述

    Machine Learning with Confidential Computing: A Systematization of Knowledge. (arXiv:2208.10134v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2208.10134](http://arxiv.org/abs/2208.10134)

    本文研究机器学习和机密计算的结合，并梳理了先前的研究成果，提供了关于保证机密性和完整性的技术，同时讨论了它们的高级特性和局限性。本文进一步确定了现有的可信执行环境（TEE）系统在机器学习用例中的限制，并讨论了未来的展望。

    

    随着机器学习的广泛发展和攻击面的扩大，机器学习中的隐私和安全挑战日益严重。作为一种成熟的系统级方法，机密计算已被学术界和工业界用于缓解各种机器学习场景中的隐私和安全问题。本文研究了机器学习和机密计算之间的结合。我们系统梳理了先前基于机密计算辅助的机器学习技术，提供了i）机密性保证和ii）完整性保证，并讨论了它们的高级特性和缺陷。进一步确定了关键挑战，并对现有的可信执行环境（TEE）系统在机器学习用例中的限制进行了专门的分析。最后，讨论了展望性的工作，包括闭环保护的基于地面的隐私定义，高效机器学习的分区执行，专门的TEE辅助机器学习设计。

    Privacy and security challenges in Machine Learning (ML) have become increasingly severe, along with ML's pervasive development and the recent demonstration of large attack surfaces. As a mature system-oriented approach, Confidential Computing has been utilized in both academia and industry to mitigate privacy and security issues in various ML scenarios. In this paper, the conjunction between ML and Confidential Computing is investigated. We systematize the prior work on Confidential Computing-assisted ML techniques that provide i) confidentiality guarantees and ii) integrity assurances, and discuss their advanced features and drawbacks. Key challenges are further identified, and we provide dedicated analyses of the limitations in existing Trusted Execution Environment (TEE) systems for ML use cases. Finally, prospective works are discussed, including grounded privacy definitions for closed-loop protection, partitioned executions of efficient ML, dedicated TEE-assisted designs for ML, 
    
[^157]: 跨模态变换器进行舞蹈风格转换

    Dance Style Transfer with Cross-modal Transformer. (arXiv:2208.09406v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09406](http://arxiv.org/abs/2208.09406)

    提出了一种名为CycleDance的舞蹈风格转换系统，通过跨模态变换器编码器，并采用基于序列长度的课程学习和新指标对舞蹈运动进行转移和合成 ，能够实现逼真的舞蹈风格转换。

    

    我们提出了一种名为CycleDance的舞蹈风格转换系统，可将一种舞蹈风格中的动作转换为另一种舞蹈风格中的动作，并尝试保留舞蹈的动态上下文。我们的方法扩展了现有的CycleGAN架构，用于建模音频序列，并集成了多模态变换器编码器来考虑音乐上下文。我们采用基于序列长度的课程学习来稳定训练。我们的方法可以捕捉动态帧之间丰富而长期的内在关系，这是动态转移和合成工作中的常见挑战。在舞蹈运动背景下，我们进一步介绍了评估转移强度和内容保留的新指标。我们进行了广泛的削减研究和人类研究，包括30名具有5年或更多舞蹈经验的参与者。结果表明，CycleDance可以生成具有目标风格的逼真动作，其自然程度显著优于基线CycleGAN。

    We present CycleDance, a dance style transfer system to transform an existing motion clip in one dance style to a motion clip in another dance style while attempting to preserve motion context of the dance. Our method extends an existing CycleGAN architecture for modeling audio sequences and integrates multimodal transformer encoders to account for music context. We adopt sequence length-based curriculum learning to stabilize training. Our approach captures rich and long-term intra-relations between motion frames, which is a common challenge in motion transfer and synthesis work. We further introduce new metrics for gauging transfer strength and content preservation in the context of dance movements. We perform an extensive ablation study as well as a human study including 30 participants with 5 or more years of dance experience. The results demonstrate that CycleDance generates realistic movements with the target style, significantly outperforming the baseline CycleGAN on naturalness,
    
[^158]: MENLI: 自然语言推理的鲁棒性评估指标

    MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.07316](http://arxiv.org/abs/2208.07316)

    本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。

    

    最近被提出的基于BERT的文本生成评估指标在标准基准测试中表现良好，但易受到对信息正确性的攻击。我们认为这部分原因是此类模型是基于语义相似性建模的。相反，我们提出一种基于自然语言推理（NLI）的鲁棒性评估指标，这种指标更适合建模。我们设计了一种基于偏好的对抗性攻击框架，并表明我们的NLI基础指标比最近的BERT基础指标更具鲁棒性。在标准基准测试中，我们的NLI基础指标优于现有的摘要评估指标，但低于SOTA MT指标。然而，在现有指标与我们的NLI指标相结合时，我们既获得了更高的对抗鲁棒性（15％-30％），又获得了标准基准测试中更高的质量指标（+5％至30％）。

    Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
    
[^159]: 带信息协方差的贝叶斯优化

    Bayesian Optimization with Informative Covariance. (arXiv:2208.02704v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.02704](http://arxiv.org/abs/2208.02704)

    提出了一种新颖的用于优化的信息丰富协方差函数，利用非平稳性来编码对搜索空间中某些区域的偏好，并在优化过程中自适应地促进局部探索，以提高贝叶斯优化在高维空间中的样本效率。

    

    贝叶斯优化是一种处理未知和昂贵目标的全局优化方法。它将一个拟合贝叶斯回归模型与一个收获函数结合起来，以决定在哪里评估目标。典型的回归模型由具有平稳协方差函数的高斯过程表示。然而，这些函数无法表达输入相关的先验信息，包括最优点可能出现的位置。平稳模型的普及导致了通过信息丰富的均值函数利用先验信息的常见做法。本文中，我们强调这些模型在高维情况下可能表现不佳。我们提出了一种新颖的用于优化的信息丰富协方差函数，利用非平稳性来编码对搜索空间中某些区域的偏好，并在优化过程中自适应地促进局部探索。我们证明了所提出的函数可以提高贝叶斯优化在高维空间中的样本效率，在基准问题中达到了最先进的性能。

    Bayesian optimization is a methodology for global optimization of unknown and expensive objectives. It combines a surrogate Bayesian regression model with an acquisition function to decide where to evaluate the objective. Typical regression models are given by Gaussian processes with stationary covariance functions. However, these functions are unable to express prior input-dependent information, including possible locations of the optimum. The ubiquity of stationary models has led to the common practice of exploiting prior information via informative mean functions. In this paper, we highlight that these models can perform poorly, especially in high dimensions. We propose novel informative covariance functions for optimization, leveraging nonstationarity to encode preferences for certain regions of the search space and adaptively promote local exploration during optimization. We demonstrate that the proposed functions can increase the sample efficiency of Bayesian optimization in high
    
[^160]: DHGE：双视图超关系知识图嵌入用于链接预测和实体类型

    DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2207.08562](http://arxiv.org/abs/2207.08562)

    本文提出了一个双视图超关系知识图嵌入方法，其中DH-KG包含超关系实例视图和从实体层次抽象出的超关系本体视图，定义了链接预测和实体类型识别任务，并构建了两个DH-KG数据集JW44K-6K和HTDM。DHGE是一个基于GRAN编码器、HGNN和联合学习的DH-KG嵌入模型。

    

    在知识图谱表示学习领域中，超关系事实由一个主三元组和几个辅助的属性-值描述组成，被认为比基于三元组的事实更全面和具体。然而，当前单视图的超关系知识图嵌入方法应用受到限制，因为它们弱化了表示实体之间亲属关系的分层结构。为了克服这一限制，我们提出了一个包含超关系实例视图和从实体层次抽象出的超关系本体视图的双视图超关系知识图谱结构（DH-KG）。本文首次在DH-KG上定义了链接预测和实体类型识别任务，并构建了两个DH-KG数据集，JW44K-6K，从维基数据中提取，和基于医学数据的HTDM。此外，我们提出了DHGE，一个基于GRAN编码器、HGNN和联合学习的DH-KG嵌入模型。

    In the field of representation learning on knowledge graphs (KGs), a hyper-relational fact consists of a main triple and several auxiliary attribute-value descriptions, which is considered more comprehensive and specific than a triple-based fact. However, currently available hyper-relational KG embedding methods in a single view are limited in application because they weaken the hierarchical structure that represents the affiliation between entities. To overcome this limitation, we propose a dual-view hyper-relational KG structure (DH-KG) that contains a hyper-relational instance view for entities and a hyper-relational ontology view for concepts that are abstracted hierarchically from the entities. This paper defines link prediction and entity typing tasks on DH-KG for the first time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding model based on GRAN encoders, HGNNs, and joint learnin
    
[^161]: Retweet-BERT：基于语言特征和社交网络信息扩散的政治倾向检测

    Retweet-BERT: Political Leaning Detection Using Language Features and Information Diffusion on Social Networks. (arXiv:2207.08349v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2207.08349](http://arxiv.org/abs/2207.08349)

    Retweet-BERT是一个简单而可扩展的模型，用于估计Twitter用户的政治倾向。该模型利用转发网络结构和用户语言特征，并在COVID-19和2020年美国总统选举数据集上展现出有竞争力的性能。研究还表明，在Twitter上存在着右倾用户之间的政治回音室。

    

    估计社交媒体用户的政治倾向是一个具有挑战性和越来越紧迫的问题，因为社交媒体消费量的增加。我们引入了Retweet-BERT，这是一个简单而可扩展的模型，用于估计Twitter用户的政治倾向。Retweet-BERT利用转发网络结构和用户个人资料描述中使用的语言。我们的假设来自于网络和语言同质性的模式，这些模式在那些分享相似意识形态的人们中非常普遍。Retweet-BERT在两个最近的Twitter数据集（一个COVID-19数据集和一个2020年美国总统选举数据集）上展现出与其他最新基线的有竞争力的性能，实现了96%-97%的宏F1。我们还进行了手动验证，以验证Retweet-BERT在训练数据之外的用户上的性能。最后，在COVID-19的案例研究中，我们展示了Twitter上政治回音室的存在，并表明这主要存在于右倾用户之间。我们的代码已开源。

    Estimating the political leanings of social media users is a challenging and ever more pressing problem given the increase in social media consumption. We introduce Retweet-BERT, a simple and scalable model to estimate the political leanings of Twitter users. Retweet-BERT leverages the retweet network structure and the language used in users' profile descriptions. Our assumptions stem from patterns of networks and linguistics homophily among people who share similar ideologies. Retweet-BERT demonstrates competitive performance against other state-of-the-art baselines, achieving 96%-97% macro-F1 on two recent Twitter datasets (a COVID-19 dataset and a 2020 United States presidential elections dataset). We also perform manual validation to validate the performance of Retweet-BERT on users not in the training data. Finally, in a case study of COVID-19, we illustrate the presence of political echo chambers on Twitter and show that it exists primarily among right-leaning users. Our code is 
    
[^162]: AnoShift: 一个用于无监督异常检测的数据分布偏移基准

    AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection. (arXiv:2206.15476v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.15476](http://arxiv.org/abs/2206.15476)

    AnoShift是一个用于无监督异常检测的数据分布偏移基准，旨在评估模型处理分布偏移的能力。其使用了一个基于时间变化交通数据集的非平稳数据集，使用AnoShift可以有效地区分模型在分布偏移情况下检测异常的能力。

    

    数据分布偏移分析是当今机器学习研究的增长方向，推动着新兴的基准的出现，这些基准侧重于提供适合研究机器学习模型的泛化属性的场景。现有的基准侧重于监督学习，就我们所知，还没有用于无监督学习的基准。因此，我们引入了一个用于无监督异常检测的基准，其中数据随时间不断变化，基于用于网络入侵检测的交通数据集京都-2006+构建。这类型的数据符合输入分布的变化前提：它覆盖了很长一段时间（10年），随时间自然变化（例如，用户修改其行为模式和软件更新）。我们首先使用基本的按特征分析、t-SNE和最优运输方法来突显数据的非平稳性，并测量年份之间整体分布距离。接下来，我们提出AnoShift，一个基准框架，包括一组指标和评估程序，以评估无监督异常检测模型处理数据分布偏移的性能。我们使用AnoShift在我们的基准测试中比较了几种最先进的异常检测模型的性能。结果表明，AnoShift可以有效地区分模型在分布偏移情况下检测异常的能力。

    Analyzing the distribution shift of data is a growing research direction in nowadays Machine Learning (ML), leading to emerging new benchmarks that focus on providing a suitable scenario for studying the generalization properties of ML models. The existing benchmarks are focused on supervised learning, and to the best of our knowledge, there is none for unsupervised learning. Therefore, we introduce an unsupervised anomaly detection benchmark with data that shifts over time, built over Kyoto-2006+, a traffic dataset for network intrusion detection. This type of data meets the premise of shifting the input distribution: it covers a large time span ($10$ years), with naturally occurring changes over time (eg users modifying their behavior patterns, and software updates). We first highlight the non-stationary nature of the data, using a basic per-feature analysis, t-SNE, and an Optimal Transport approach for measuring the overall distribution distances between years. Next, we propose AnoS
    
[^163]: 何时重新初始化神经网络是有效的？

    When Does Re-initialization Work?. (arXiv:2206.10011v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10011](http://arxiv.org/abs/2206.10011)

    研究表明，当没有其他正则化技术时，重新初始化神经网络有助于提高泛化性能。但是，当它与其他正则化技术一起使用时，对泛化性能的额外帮助很小，尽管最佳泛化性能变得更加稳定。

    

    在最近的研究中观察到，重新初始化神经网络在训练过程中有助于提高泛化性能。然而，在深度学习实践中，这种方法并不被广泛采用，也不常用于最先进的训练协议中。因此，本文提出了一个问题，即重新初始化什么时候有效，是否应该与数据增强、权重衰减和学习率调整等正则化技术一起使用。本研究对标准训练与一些重新初始化方法进行了广泛的实证比较，训练了超过15,000个模型，并在各种图像分类基准上进行了测试，以回答这个问题。我们首先确定，当没有任何其他正则化存在时，这些方法可以在泛化方面持续改善。然而，当这些方法与其他经过精心调整的正则化技术一起使用时，重新初始化方法对泛化性能几乎没有额外的帮助，尽管在这种情况下，最佳泛化性能变得更加稳定。

    Re-initializing a neural network during training has been observed to improve generalization in recent works. Yet it is neither widely adopted in deep learning practice nor is it often used in state-of-the-art training protocols. This raises the question of when re-initialization works, and whether it should be used together with regularization techniques such as data augmentation, weight decay and learning rate schedules. In this work, we conduct an extensive empirical comparison of standard training with a selection of re-initialization methods to answer this question, training over 15,000 models on a variety of image classification benchmarks. We first establish that such methods are consistently beneficial for generalization in the absence of any other regularization. However, when deployed alongside other carefully tuned regularization techniques, re-initialization methods offer little to no added benefit for generalization, although optimal generalization performance becomes less
    
[^164]: 近端分裂对抗攻击用于语义分割

    Proximal Splitting Adversarial Attacks for Semantic Segmentation. (arXiv:2206.07179v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07179](http://arxiv.org/abs/2206.07179)

    本文提出了适用于语义分割的近端分裂对抗攻击方法，采用扩展拉格朗日方法处理大量约束且产生更小的对抗性扰动。

    

    对抗攻击的研究都集中在分类上，但只有少数工作研究了适用于更密集预测任务（如语义分割）的方法。这些工作中提出的方法不能准确地解决对抗性分割问题，因此高估了欺骗模型所需的污染大小。在本文中，作者提出了基于近端分裂的白盒攻击方法，以产生具有更小$\ell_\infty$范数的对抗性扰动。作者的攻击可以通过扩展拉格朗日方法处理大量的约束，同时采用自适应约束缩放和屏蔽策略。作者证明了他们的攻击显著优于以前提出的方法以及分类攻击（作者为了分割而改进），为这项密集任务提供了第一个全面的基准测试。

    Classification has been the focal point of research on adversarial attacks, but only a few works investigate methods suited to denser prediction tasks, such as semantic segmentation. The methods proposed in these works do not accurately solve the adversarial segmentation problem and, therefore, overestimate the size of the perturbations required to fool models. Here, we propose a white-box attack for these models based on a proximal splitting to produce adversarial perturbations with much smaller $\ell_\infty$ norms. Our attack can handle large numbers of constraints within a nonconvex minimization framework via an Augmented Lagrangian approach, coupled with adaptive constraint scaling and masking strategies. We demonstrate that our attack significantly outperforms previously proposed ones, as well as classification attacks that we adapted for segmentation, providing a first comprehensive benchmark for this dense task.
    
[^165]: 分类中的良性过拟合：更大模型的发现可证明对抗标签噪声

    Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models. (arXiv:2206.00501v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00501](http://arxiv.org/abs/2206.00501)

    本文发现良性过拟合在存在标签噪声的情况下可能会失败，提醒未来需要理解欠拟合制度下的隐式偏见。

    

    良性过拟合的研究为超参数化深度学习模型的成功提供了见解。本文研究了过拟合是否在真实世界的分类任务中真的是良性的。我们开始观察到一个 ResNet 模型在 Cifar10 上表现良好，但在 ImageNet 上则不良。为了了解为什么良性过拟合在 ImageNet 实验中失败，我们在一个比数据点数量不明显大的限定条件下从理论上分析了良性过拟合。在这个轻微超参数化的设置下，我们的分析发现了一个相变：与之前的重超参数化设置不同，当存在标签噪声时，良性过拟合现在可能会失败。我们的分析解释了我们的经验观察，并通过一组 ResNet 的控制实验进行了验证。我们的工作强调了理解欠拟合制度下的隐式偏见的重要性，作为未来的一个方向。

    Studies on benign overfitting provide insights for the success of overparameterized deep learning models. In this work, we examine whether overfitting is truly benign in real-world classification tasks. We start with the observation that a ResNet model overfits benignly on Cifar10 but not benignly on ImageNet. To understand why benign overfitting fails in the ImageNet experiment, we theoretically analyze benign overfitting under a more restrictive setup where the number of parameters is not significantly larger than the number of data points. Under this mild overparameterization setup, our analysis identifies a phase change: unlike in the previous heavy overparameterization settings, benign overfitting can now fail in the presence of label noise. Our analysis explains our empirical observations, and is validated by a set of control experiments with ResNets. Our work highlights the importance of understanding implicit bias in underfitting regimes as a future direction.
    
[^166]: 平均调整关联度：高维混淆因素的高效估计

    Average Adjusted Association: Efficient Estimation with High Dimensional Confounders. (arXiv:2205.14048v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2205.14048](http://arxiv.org/abs/2205.14048)

    本研究提出了一个总结衡量标准——平均调整关联度（AAA），用于评估一个具有混淆影响的异质群体中的关联程度。并且我们提出了高效的估计方法，可用于各种采样场景。

    

    对数比率是衡量二元结果和暴露变量关联度的一种既定指标。尽管其被广泛使用，但有关如何通过平均方式总结具有混淆因素的对数比率的讨论却很有限。为了解决这个问题，我们提出了平均调整关联度（AAA），它是一个调整了观察到的混淆因素的异质群体中的关联度的总结衡量标准。为了方便使用，我们还开发了高效的双重/无偏机器学习（DML）AAA估计器。我们的DML估计器使用了两种等效的有效影响函数形式，并适用于各种采样场景，包括随机采样，基于结果的采样和基于暴露的采样。通过真实数据和模拟，我们证明了我们提出的估算方法在测量AAA方面的实用性和有效性。

    The log odds ratio is a well-established metric for evaluating the association between binary outcome and exposure variables. Despite its widespread use, there has been limited discussion on how to summarize the log odds ratio as a function of confounders through averaging. To address this issue, we propose the Average Adjusted Association (AAA), which is a summary measure of association in a heterogeneous population, adjusted for observed confounders. To facilitate the use of it, we also develop efficient double/debiased machine learning (DML) estimators of the AAA. Our DML estimators use two equivalent forms of the efficient influence function, and are applicable in various sampling scenarios, including random sampling, outcome-based sampling, and exposure-based sampling. Through real data and simulations, we demonstrate the practicality and effectiveness of our proposed estimators in measuring the AAA.
    
[^167]: 一种灵活的峰值风暴潮预测框架

    A Framework for Flexible Peak Storm Surge Prediction. (arXiv:2204.13168v2 [cs.CE] UPDATED)

    [http://arxiv.org/abs/2204.13168](http://arxiv.org/abs/2204.13168)

    本文提出了一种基于多阶段方法的新型峰值风暴潮预测代理模型，可在保证预测准确性的同时显著降低计算成本和模型参数数量，有望在风险评估和应急管理决策方面发挥重要作用。

    

    风暴潮是沿海地区的一种重要自然灾害，不仅会造成严重的财产损失，也会造成人员伤亡。需要准确高效的风暴潮模型来评估长期风险和指导应急管理决策。本文提出了一种基于多阶段方法的新型峰值风暴潮预测代理模型。在第一阶段，点被分类为淹没或非淹没。在第二阶段中，预测了淹没的级别。此外，我们提出了一种新的代理问题公式，其中每个点独立预测风暴潮。这允许直接对不在训练数据中的位置进行预测，并显著减少了模型参数的数量。我们在两个研究案例上演示了我们的建模框架。

    Storm surge is a major natural hazard in coastal regions, responsible both for significant property damage and loss of life. Accurate, efficient models of storm surge are needed both to assess long-term risk and to guide emergency management decisions. While high-fidelity regional- and global-ocean circulation models such as the ADvanced CIRCulation (ADCIRC) model can accurately predict storm surge, they are very computationally expensive. Here we develop a novel surrogate model for peak storm surge prediction based on a multi-stage approach. In the first stage, points are classified as inundated or not. In the second, the level of inundation is predicted . Additionally, we propose a new formulation of the surrogate problem in which storm surge is predicted independently for each point. This allows for predictions to be made directly for locations not present in the training data, and significantly reduces the number of model parameters. We demonstrate our modeling framework on two stu
    
[^168]: 基于序列的加密货币Pump-and-Dump目标币预测

    Sequence-Based Target Coin Prediction for Cryptocurrency Pump-and-Dump. (arXiv:2204.12929v2 [q-fin.ST] UPDATED)

    [http://arxiv.org/abs/2204.12929](http://arxiv.org/abs/2204.12929)

    本文致力于预测在计划的pump时间之前，列在目标交易所中的所有币的pump概率，通过对P&D事件的经验分析和开发基于序列的神经网络，我们发现pump的硬币呈现出内部信道的同质性和跨信道的异质性，提高了我们的预测准确性。

    

    随着加密货币市场中Pump-and-Dump计划（P＆Ds）的增多，提前检测此类欺诈活动以警示潜在易受影响的投资者变得至关重要。本文重点预测在计划的pump时间之前，列在目标交易所中的所有币的pump概率，这被我们称为目标币预测任务。针对这个问题，我们首先对Telegram中自2019年1月至2022年1月组织的最新的709次P＆D事件进行了全面研究。我们的经验分析揭示了P＆Ds的一些有趣的模式，例如pump的硬币呈现出内部信道的同质性和跨信道的异质性。这个发现启发我们开发一种新的基于序列的神经网络，称为SNN，通过位置注意机制将信道的P＆D事件历史编码为序列表示，以提高预测准确性。

    With the proliferation of pump-and-dump schemes (P&Ds) in the cryptocurrency market, it becomes imperative to detect such fraudulent activities in advance to alert potentially susceptible investors. In this paper, we focus on predicting the pump probability of all coins listed in the target exchange before a scheduled pump time, which we refer to as the target coin prediction task. Firstly, we conduct a comprehensive study of the latest 709 P&D events organized in Telegram from Jan. 2019 to Jan. 2022. Our empirical analysis reveals some interesting patterns of P&Ds, such as that pumped coins exhibit intra-channel homogeneity and inter-channel heterogeneity. Here channel refers a form of group in Telegram that is frequently used to coordinate P&D events. This observation inspires us to develop a novel sequence-based neural network, dubbed SNN, which encodes a channel's P&D event history into a sequence representation via the positional attention mechanism to enhance the prediction accur
    
[^169]: 基于因果建模提升机器学习鉴定生物标志物的泛化能力：一项关于免疫受体诊断的研究

    Improving generalization of machine learning-identified biomarkers with causal modeling: an investigation into immune receptor diagnostics. (arXiv:2204.09291v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2204.09291](http://arxiv.org/abs/2204.09291)

    本文研究通过因果建模提高机器学习诊断鲁棒性的方法，以免疫受体库为例，阐明实验因素如何影响学习的生物标志物。

    

    机器学习逐渐被应用于高维分子数据中发现诊断和预后的生物标志物。然而，实验设计相关的多种因素可能会影响学习到可泛化且临床应用值高的诊断结果。本文认为从因果的角度来看可以提高机器学习诊断的鲁棒性，正式界定它们与机器学习诊断的鲁棒性和泛化能力的关系。具体而言，我们关注了适应性免疫受体库（AIRR）这样一种特定的高维标志物，并借助模拟实验阐明了与AIRR领域相关的主要生物和实验因素如何影响学习到的生物标志物。最后，我们认为因果建模通过确定变量之间的稳定关系并指导个体间关系和变量之间的调整，可以提高基于机器学习的生物标志物的鲁棒性。

    Machine learning is increasingly used to discover diagnostic and prognostic biomarkers from high-dimensional molecular data. However, a variety of factors related to experimental design may affect the ability to learn generalizable and clinically applicable diagnostics. Here, we argue that a causal perspective improves the identification of these challenges and formalizes their relation to the robustness and generalization of machine learning-based diagnostics. To make for a concrete discussion, we focus on a specific, recently established high-dimensional biomarker - adaptive immune receptor repertoires (AIRRs). Through simulations, we illustrate how major biological and experimental factors of the AIRR domain may influence the learned biomarkers. In conclusion, we argue that causal modeling improves machine learning-based biomarker robustness by identifying stable relations between variables and by guiding the adjustment of the relations and variables that vary between populations.
    
[^170]: 贝叶斯成像的条件可逆流

    Conditional Injective Flows for Bayesian Imaging. (arXiv:2204.07664v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.07664](http://arxiv.org/abs/2204.07664)

    本文提出了一种新的条件可逆流方法，专门设计用于成像问题。通过可逆性减少了内存占用和训练时间，并且在大规模实验中表现最好。

    

    大多数用于计算成像的深度学习模型回归一个重建图像。然而，在实践中，不适定性、非线性、模型不匹配和噪声常常合谋使这些点估计具有误导性或不足。贝叶斯方法将图像和（嘈杂的）测量建模为联合分布的随机向量，并旨在近似未知量的后验分布。最近，基于条件标准化流的变分推理方法是传统MCMC方法的有希望的替代方法，但它们具有缺点：对于中等到高分辨率图像，需要过多的内存和计算资源，并具有在难以处理的非线性问题中表现不佳的问题。在这项工作中，我们提出了C-Trumpets--专门设计用于成像问题的条件可逆流，大大减少了这些挑战。可逆性减少了内存占用和训练时间，低维潜在空间与固定体积的建筑创新相结合，使我们的方法在大规模实验中产生了最好的成果。

    Most deep learning models for computational imaging regress a single reconstructed image. In practice, however, ill-posedness, nonlinearity, model mismatch, and noise often conspire to make such point estimates misleading or insufficient. The Bayesian approach models images and (noisy) measurements as jointly distributed random vectors and aims to approximate the posterior distribution of unknowns. Recent variational inference methods based on conditional normalizing flows are a promising alternative to traditional MCMC methods, but they come with drawbacks: excessive memory and compute demands for moderate to high resolution images and underwhelming performance on hard nonlinear problems. In this work, we propose C-Trumpets -- conditional injective flows specifically designed for imaging problems, which greatly diminish these challenges. Injectivity reduces memory footprint and training time while low-dimensional latent space together with architectural innovations like fixed-volume-c
    
[^171]: 面向小样本人类数据的零样本元学习

    Zero-shot meta-learning for small-scale data from human subjects. (arXiv:2203.16309v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.16309](http://arxiv.org/abs/2203.16309)

    为了解决小样本人类数据的零样本学习问题，我们提出了一个元学习框架，能够快速适应有限的训练数据，处理多任务预测并可以从整体上表现最佳。

    

    尽管机器学习的发展在大数据上取得了显著的性能提升，但实际上许多人类受试者的数据规模较小且标记稀疏。已有的方法应用于这样的数据通常不容易泛化到样本外的受试者。相反，模型必须对可能来自不同分布的测试数据进行预测，这是一个称为“零样本学习”的问题。为了解决这个挑战，我们开发了一个端到端的框架，使用元学习方法，使模型能够快速适应有限的训练数据，对于样本外的测试数据进行预测。我们使用了三个真实的小规模人类受试者数据集（两个随机对照研究和一个观察性研究）来预测保留的治疗组的治疗结果。我们的模型学习每种干预的潜在治疗效果，并且能够自然地处理多任务预测。我们展示了我们的模型从整体上表现最佳。

    While developments in machine learning led to impressive performance gains on big data, many human subjects data are, in actuality, small and sparsely labeled. Existing methods applied to such data often do not easily generalize to out-of-sample subjects. Instead, models must make predictions on test data that may be drawn from a different distribution, a problem known as \textit{zero-shot learning}. To address this challenge, we develop an end-to-end framework using a meta-learning approach, which enables the model to rapidly adapt to a new prediction task with limited training data for out-of-sample test data. We use three real-world small-scale human subjects datasets (two randomized control studies and one observational study), for which we predict treatment outcomes for held-out treatment groups. Our model learns the latent treatment effects of each intervention and, by design, can naturally handle multi-task predictions. We show that our model performs the best holistically for e
    
[^172]: 电影叙述摘要：一个用于故事理解的视频语言数据集

    Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.05711](http://arxiv.org/abs/2203.05711)

    这个研究收集并发布了一个视频-语言故事数据集SYMON，用于推进多模态故事理解的发展。

    

    尽管AI有了最近的进展，但故事理解仍然是一个未被充分研究的问题。我们收集、预处理并公开发布了一个视频语言故事数据集SYMON，其中包含5,193个流行电影和电视剧的视频摘要。SYMON捕捉了由人类创作者制作的面向人类观众的自然故事叙述视频。作为一个原型和自然故事数据集，SYMON具有高覆盖的多模态故事事件、丰富的心理状态描述和视觉和文本模态之间的大语义差距。我们建立了视频文本检索和电影摘要视频的零样本对齐的基准，展示了在故事理解中领域内数据的重要性。通过SYMON，我们希望为多模态故事理解的进展打下基础。

    Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SYMON), containing 5,193 video summaries of popular movies and TV series. SYMON captures naturalistic story-telling videos for human audience made by human creators. As a prototypical and naturalistic story dataset, SYMON features high coverage of multimodal story events, abundant mental-state descriptions, and large semantic gaps between the visual and the textual modalities. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data in story understanding. With SYMON, we hope to lay the groundwork for progress in multimodal story understanding.
    
[^173]: 面向领域差异的航天器姿态估计的鲁棒性多任务学习和在线优化

    Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap. (arXiv:2203.04275v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.04275](http://arxiv.org/abs/2203.04275)

    本文介绍了一种跨领域差异的非合作航天器姿态估计的多尺度、多任务CNN，通过在合成图像上进行数据增强训练共享编码器以学习通用特征。同时介绍了一种在线域优化方法，用于调整模型的标准化层参数。

    

    本文介绍了一种名为Spacecraft Pose Network v2（SPNv2）的卷积神经网络（CNN），用于跨领域差异的非合作航天器姿态估计。SPNv2是一个多尺度、多任务的CNN，由共享的多尺度特征编码器和多个预测头组成，这些预测头在共享特征输出上执行不同的任务。这些任务都与从图像中检测和估计目标航天器的姿态有关，例如预测预定义的卫星关键点、直接姿态回归和卫星前景的二元分割等。通过在合成图像上进行广泛的数据增强来共同训练不同但相关的任务，证明了共享编码器学习到的特征对具有基本不同视觉特性的图像域是通用的。本文还介绍了一种名为Online Domain Refinement（ODR）的方法，该方法在目标域上调整SPNv2的标准化层参数。

    This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural Network (CNN) for pose estimation of noncooperative spacecraft across domain gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared multi-scale feature encoder and multiple prediction heads that perform different tasks on a shared feature output. These tasks are all related to detection and pose estimation of a target spacecraft from an image, such as prediction of pre-defined satellite keypoints, direct pose regression, and binary segmentation of the satellite foreground. It is shown that by jointly training on different yet related tasks with extensive data augmentations on synthetic images only, the shared encoder learns features that are common across image domains that have fundamentally different visual characteristics compared to synthetic images. This work also introduces Online Domain Refinement (ODR) which refines the parameters of the normalization layers of SPNv2 on the target doma
    
[^174]: 深度线性网络的精确解析解

    Exact Solutions of a Deep Linear Network. (arXiv:2202.04777v6 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.04777](http://arxiv.org/abs/2202.04777)

    本研究找到了带权重衰减和随机神经元的深度线性网络全局最小值的解析表达式，结果表明权重衰减与模型架构的强烈交互作用会在多于1个隐藏层的网络中创建不良极小值，并表明常见的深度学习初始化方法无法在一般情况下缓解神经网络的优化问题。

    

    本研究找到了带权重衰减和随机神经元的深度线性网络全局最小值的解析表达式，这是理解神经网络理论中的基础模型。我们的结果表明，在深度神经网络架构中，零是一个特殊的点。我们展示了权重衰减与模型架构的强烈交互作用，并能够在具有超过 $1$ 个隐藏层的网络中创建不良极小值，这与仅有 $1$ 个隐藏层的网络有质的不同。实际上，我们的结果意味着常见的深度学习初始化方法无法在一般情况下缓解神经网络的优化问题。

    This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general.
    
[^175]: 分布式随机变分不等式的最优算法

    Optimal Algorithms for Decentralized Stochastic Variational Inequalities. (arXiv:2202.02771v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2202.02771](http://arxiv.org/abs/2202.02771)

    本文针对分布式随机变分不等式的问题，提出了匹配复杂度下界的最优算法，不仅在分布式随机情况下表现最佳，在分布式确定性和非分布式随机情况下也是最佳的，并得到实验验证。

    

    变分不等式是一个包括了博弈、最小化、鞍点和平衡问题的形式化方法。因此，变分不等式的方法是许多应用任务的通用方法，包括机器学习问题。本文关注分布式设置，分布式设置越来越重要，但不太被了解。特别是，我们考虑在固定和时变网络上进行分布式随机（总和类型）变分不等式。我们为通信和局部迭代双方提出了较低的复杂度下界，并构建了与这些下界匹配的最优算法。我们的算法不仅在分布式随机情况下是最好的，而且在分布式确定性和非分布式随机情况下也是最好的。实验结果证实了所提出算法的有效性。

    Variational inequalities are a formalism that includes games, minimization, saddle point, and equilibrium problems as special cases. Methods for variational inequalities are therefore universal approaches for many applied tasks, including machine learning problems. This work concentrates on the decentralized setting, which is increasingly important but not well understood. In particular, we consider decentralized stochastic (sum-type) variational inequalities over fixed and time-varying networks. We present lower complexity bounds for both communication and local iterations and construct optimal algorithms that match these lower bounds. Our algorithms are the best among the available literature not only in the decentralized stochastic case, but also in the decentralized deterministic and non-distributed stochastic cases. Experimental results confirm the effectiveness of the presented algorithms.
    
[^176]: 不完美信息博弈中的近似最优学习

    Near-Optimal Learning of Extensive-Form Games with Imperfect Information. (arXiv:2202.01752v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.01752](http://arxiv.org/abs/2202.01752)

    本文提出了一种新的算法系列，可以更快速地在不完美信息广义博弈中找到一个近似最优解。

    

    本文解决了学习不完美信息广义博弈的近似最优算法设计的开放性问题。我们提出了第一种算法系列，仅需要 $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ 局游戏即可在两人零和博弈中找到一个 $\varepsilon$-近似纳什均衡，其中 $X,Y$ 是信息集的数量，$A,B$ 是两名玩家的行动数。这比已知的样本复杂度 $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ 有着 $\widetilde{\mathcal{O}}(\max\{X, Y\})$ 的巨大改进，并且在对数因子内与信息理论下限一致。我们通过两种新算法实现了这种样本复杂度：平衡在线镜面下降和平衡反事实后悔最小化。这两种算法都依赖于将“平衡探索策略”集成到它们的经典对手中的新方法。此外，我们还将我们的结果扩展到了更广泛的支持不完美信息博弈的二人博弈和多人博弈中。

    This paper resolves the open question of designing near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback. We present the first line of algorithms that require only $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ episodes of play to find an $\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ by a factor of $\widetilde{\mathcal{O}}(\max\{X, Y\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \emph{balanced exploration policies} into their classical counterparts. We also extend our results t
    
[^177]: 紧密度分数：一种用于无监督特征选择的快速过滤方法

    Compactness Score: A Fast Filter Method for Unsupervised Feature Selection. (arXiv:2201.13194v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.13194](http://arxiv.org/abs/2201.13194)

    本文提出了一种快速的无监督特征选择方法，名为紧密度分数（CSUFS），通过评估局部紧密度来选择所需特征，能够实现降维、模型效果提高和模型性能提高等功能。

    

    随着信息时代的兴起，每天产生了大量的数据。由于这些数据的大规模和高维特性，往往难以在实际应用中实现更好的决策。因此，急需一种高效的大数据分析方法。对于特征工程，特征选择似乎是一个重要的研究内容，其中预计从候选特征中选择“优秀”的特征。特征选择可以实现不同的功能，如降维、模型效果提高和模型性能提高。在许多分类任务中，研究人员发现，如果数据来自同一类别，则这些数据似乎通常彼此接近；因此，局部紧密度对于特征的评估非常重要。在本文中，我们提出了一种快速的无监督特征选择方法，名为紧密度分数（CSUFS），用于选择所需特征。

    Along with the flourish of the information age, massive amounts of data are generated day by day. Due to the large-scale and high-dimensional characteristics of these data, it is often difficult to achieve better decision-making in practical applications. Therefore, an efficient big data analytics method is urgently needed. For feature engineering, feature selection seems to be an important research content in which is anticipated to select "excellent" features from candidate ones. Different functions can be realized through feature selection, such as dimensionality reduction, model effect improvement, and model performance improvement. In many classification tasks, researchers found that data seem to be usually close to each other if they are from the same class; thus, local compactness is of great importance for the evaluation of a feature. In this manuscript, we propose a fast unsupervised feature selection method, named as, Compactness Score (CSUFS), to select desired features. To 
    
[^178]: 模拟-实验室-真实环境下的安全强化学习及泛化保证

    Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees. (arXiv:2201.08355v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2201.08355](http://arxiv.org/abs/2201.08355)

    本文提出了模拟-实验室-真实环境下的安全强化学习及泛化保证框架，利用概率保证的安全感知策略分布来弥合现实差距，具有双重策略设置、监督控制策略和“可能近似正确（PAC）-Bayes”框架等特点，能够在移动机器人导航和四轴飞行等任务中实现改进的安全性和泛化能力。

    

    安全性是自主系统的关键组成部分，对于基于学习的策略在真实世界中的应用仍然是一个挑战。特别是，使用强化学习学习的策略由于不安全的行为而经常无法推广到新颖的环境中。在本文中，我们提出了模拟-实验室-真实环境下的框架，利用概率保证的安全感知策略分布来弥合现实差距。为了提高安全性，我们采用双重策略设置，一个性能策略使用累积任务奖励进行训练，而备份（安全）策略则通过求解基于哈密顿-雅可比（HJ）可达性分析的安全贝尔曼方程进行训练。在模拟-实验室转移中，我们采用监督控制策略，在探索过程中保护不安全的行为；在实验室-真实转移中，我们利用“可能近似正确（PAC）-Bayes”框架为在未知环境中的策略提供性能和安全性的下限。此外，我们还从HJ可达性分析中得出了我们策略的泛化保证。我们在移动机器人导航任务和四轴飞行任务上对所提出的模拟-实验室-真实环境框架进行了评估，并进行了实验验证，结果表明相比基线方法，该框架提高了安全性和泛化能力。

    Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world. In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior. In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution. To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments. Additionally, inheriting from the HJ 
    
[^179]: 物理约束流神经网络用于数据通信网络中的短时刻预测

    Physics Constrained Flow Neural Network for Short-Timescale Predictions in Data Communications Networks. (arXiv:2112.12321v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.12321](http://arxiv.org/abs/2112.12321)

    本文提出了一个新的用于短时刻预测的神经网络模型FlowNN，通过引入物理偏差、使用自监督学习策略来改进特征表述。在合成和真实网络数据集上都取得了17%到71%的损失下降。

    

    机器学习在数据通信网络信息流动的动态分析中得到越来越多的应用。先前的模型经常依靠现成的学习模型通过历史统计数据进行预测，而无视掌控这些信息流生成行为的物理规律。本文提出了Flow Neural Network（FlowNN），通过嵌套层和归纳层引入学习到的物理偏差来改进特征表述，并使用自监督学习策略来普遍地学习这些物理规律。在短时间范围内的网络预测任务中，FlowNN在合成和真实网络数据集上都取得了17%到71%的损失下降，显示了该方法的威力。

    Machine learning is gaining growing momentum in various recent models for the dynamic analysis of information flows in data communications networks. These preliminary models often rely on off-the-shelf learning models to predict from historical statistics while disregarding the physics governing the generating behaviors of these flows. This paper instead introduces Flow Neural Network (FlowNN) to improve the feature representation with learned physical bias. This is implemented by an induction layer, working upon the embedding layer, to impose the physics connected data correlations, and a self-supervised learning strategy with stop-gradient to make the learned physics universal. For the short-timescale network prediction tasks, FlowNN achieves 17% - 71% of loss decrease than the state-of-the-art baselines on both synthetic and real-world networking datasets, which shows the strength of this new approach.
    
[^180]: DNN中稀疏概念的定义和量化

    Defining and Quantifying the Emergence of Sparse Concepts in DNNs. (arXiv:2111.06206v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.06206](http://arxiv.org/abs/2111.06206)

    本文提出了在DNN中理解产生的交互概念的概念-emerging现象，这些概念可以用稀疏的符号因果图和And-Or图（AOG）进行量化和简化。

    

    本文旨在说明在训练过的DNN中概念的产生现象。具体来说，我们发现DNN的推理分数可以分解为少数交互概念的影响。这些概念可以被理解为一个稀疏的符号因果图中的因果模式，该图解释了DNN。使用此类因果图对DNN进行解释的忠实性在理论上得到保证，因为我们证明了因果图可以在指数数量的不同屏蔽样本上很好地模仿DNN的输出。此外，这样的因果图可以进一步简化并重新编写为And-Or图（AOG），而不会失去太多的解释准确性。

    This paper aims to illustrate the concept-emerging phenomenon in a trained DNN. Specifically, we find that the inference score of a DNN can be disentangled into the effects of a few interactive concepts. These concepts can be understood as causal patterns in a sparse, symbolic causal graph, which explains the DNN. The faithfulness of using such a causal graph to explain the DNN is theoretically guaranteed, because we prove that the causal graph can well mimic the DNN's outputs on an exponential number of different masked samples. Besides, such a causal graph can be further simplified and re-written as an And-Or graph (AOG), without losing much explanation accuracy.
    
[^181]: 追求公平的联邦学习

    Towards Fairness-Aware Federated Learning. (arXiv:2111.01872v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.01872](http://arxiv.org/abs/2111.01872)

    本文旨在概述近年来提出的公平性感知联邦学习（FAFL）方法，以解决在联邦学习中可能出现的不公平问题。

    

    联邦学习的最近进展为分布式客户端提供了大规模协作机器学习的机会，并保证了性能和数据隐私。然而，大多数现有的作品集中于FL中央控制器的利益，而忽视了FL客户端的利益。这可能会导致不公平对待客户端，使其不积极参与学习过程，并损害FL生态系统的可持续性。因此，确保FL的公平性正在吸引着大量的研究兴趣。近年来，不同角度的公平性感知联邦学习（FAFL）方法已被提出。然而，目前还没有全面的调查来帮助读者深入了解这个跨学科领域。本文旨在提供这样一篇综述。

    Recent advances in Federated Learning (FL) have brought large-scale collaborative machine learning opportunities for massively distributed clients with performance and data privacy guarantees. However, most current works focus on the interest of the central controller in FL,and overlook the interests of the FL clients. This may result in unfair treatment of clients that discourages them from actively participating in the learning process and damages the sustainability of the FL ecosystem. Therefore, the topic of ensuring fairness in FL is attracting a great deal of research interest. In recent years, diverse Fairness-Aware FL (FAFL) approaches have been proposed in an effort to achieve fairness in FL from different perspectives. However, there is no comprehensive survey that helps readers gain insight into this interdisciplinary field. This paper aims to provide such a survey. By examining the fundamental and simplifying assumptions, as well as the notions of fairness adopted by existi
    
[^182]: 利用正向-反向随机微分方程理论对薛定谔桥进行似然训练

    Likelihood Training of Schr\"odinger Bridge using Forward-Backward SDEs Theory. (arXiv:2110.11291v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.11291](http://arxiv.org/abs/2110.11291)

    本文提出了一种新的计算框架，用于基于正向-反向随机微分方程理论对薛定谔桥进行似然训练。通过这个框架，可以构建SB的似然目标，这可以成为现代深度生成模型训练的替代方法。

    

    薛定谔桥（SB）是一种熵正则化的最优输运问题，与基于分数的生成模型（SGM）相比，在深度生成建模中由于其数学灵活性而越来越受到关注。然而，尚不清楚SB的优化原则是否与现代深度生成模型的训练相关，后者通常依赖于构建对数似然目标。这引发了关于SB模型作为生成应用的原则性替代方法的适用性问题。在本工作中，我们提出了一种新的计算框架，用于基于正向-反向随机微分方程理论对SB模型进行似然训练——这是一种出现在随机最优控制中的数学方法，它将SB的最优性条件转化为一组SDE。关键是，这些SDE可以用于构建SB的似然目标，令人惊讶的是，它广义地推广了SGM的一些特殊情况。这导致了一种新的优化方法。

    Schr\"odinger Bridge (SB) is an entropy-regularized optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely on constructing log-likelihood objectives.This raises questions on the suitability of SB models as a principled alternative for generative applications. In this work, we present a novel computational framework for likelihood training of SB models grounded on Forward-Backward Stochastic Differential Equations Theory - a mathematical methodology appeared in stochastic optimal control that transforms the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be used to construct the likelihood objectives for SB that, surprisingly, generalizes the ones for SGM as special cases. This leads to a new optimi
    
[^183]: 压缩通讯解决变分不等式的分布式方法及理论保证

    Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees. (arXiv:2110.03313v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03313](http://arxiv.org/abs/2110.03313)

    本研究提出了MASHA1和MASHA2方法，可以减少在分布式训练中的通信量，并在获得可比性质量的模型的同时，解决变分不等式和鞍点问题。

    

    变分不等式和鞍点问题在机器学习应用中越来越受关注，包括对抗性学习、GAN、运输和强化优化等方面。为了训练高性能模型，需要依赖于并行和分布式计算。然而，在分布式训练中，计算节点之间的通信成为训练的关键瓶颈，特别是对于高维度和过参数化模型。因此，重要的是使用可以减少传输信息量的策略来降低训练中的通信量，同时获得具有可比性质量的模型。本文提出了MASHA1和MASHA2等基于压缩通讯的理论方法，用于解决变分不等式和鞍点问题的分布式方法。

    Variational inequalities in general and saddle point problems in particular are increasingly relevant in machine learning applications, including adversarial learning, GANs, transport and robust optimization. With increasing data and problem sizes necessary to train high performing models across various applications, we need to rely on parallel and distributed computing. However, in distributed training, communication among the compute nodes is a key bottleneck during training, and this problem is exacerbated for high dimensional and over-parameterized models. Due to these considerations, it is important to equip existing methods with strategies that would allow to reduce the volume of transmitted information during training while obtaining a model of comparable quality. In this paper, we present the first theoretically grounded distributed methods for solving variational inequalities and saddle point problems using compressed communication: MASHA1 and MASHA2. Our theory and methods al
    
[^184]: OpenFed：一个全面而灵活的开源联邦学习框架。

    OpenFed: A Comprehensive and Versatile Open-Source Federated Learning Framework. (arXiv:2109.07852v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2109.07852](http://arxiv.org/abs/2109.07852)

    OpenFed是一个全面而灵活的开源联邦学习框架，通过去中心化模型训练消除了数据传输和集中式汇聚的需求，降低了学习成本，为研究人员和联邦学习使用者提供了更好的应用体验。

    

    最近人工智能技术的发展使得其在商业和工业领域得以成功应用。然而，这些技术需要大量的数据进行集中式汇聚，从而阻碍了它们在数据敏感或数据传输成本高昂的情况下的应用。联邦学习通过去中心化模型训练解决了这些问题，从而消除了数据传输和汇聚的需求。为了推动联邦学习的应用，需要进行更多的研究和开发以解决一些重要的开放性问题。在本文中，我们提出了OpenFed，一个用于端到端联邦学习的开源软件框架。 OpenFed通过有针对性地消除现有的痛点，为研究人员和联邦学习使用者降低了准入门槛。对于研究人员，OpenFed提供了一个框架，使新方法的实现变得容易。

    Recent developments in Artificial Intelligence techniques have enabled their successful application across a spectrum of commercial and industrial settings. However, these techniques require large volumes of data to be aggregated in a centralized manner, forestalling their applicability to scenarios wherein the data is sensitive or the cost of data transmission is prohibitive. Federated Learning alleviates these problems by decentralizing model training, thereby removing the need for data transfer and aggregation. To advance the adoption of Federated Learning, more research and development needs to be conducted to address some important open questions. In this work, we propose OpenFed, an open-source software framework for end-to-end Federated Learning. OpenFed reduces the barrier to entry for both researchers and downstream users of Federated Learning by the targeted removal of existing pain points. For researchers, OpenFed provides a framework wherein new methods can be easily implem
    
[^185]: 边缘计算中的联邦混合建模强化学习

    Federated Ensemble Model-based Reinforcement Learning in Edge Computing. (arXiv:2109.05549v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.05549](http://arxiv.org/abs/2109.05549)

    本文提出了一种新颖的联邦强化学习算法，在边缘计算中使用联邦学习和知识蒸馏创建动态模型集合，通过仅使用集合模型而不与环境交互来训练策略，从而解决了现有算法中高样本复杂度和缺乏理论保证的问题。

    

    联邦学习（FL）是一种保护隐私的分布式机器学习范式，可以在不收集设备数据的情况下使地理上分布和异构的设备进行协作训练。为了将FL扩展到超出监督学习模型，提出了联邦强化学习（FRL）来处理边缘计算系统中的顺序决策问题。然而，现有的FRL算法直接将无模型强化学习与FL结合起来，因此往往导致高样本复杂度和缺乏理论保证。为了应对这些挑战，我们提出了一种新颖的FRL算法，它有效地将模型建模强化学习和集成知识蒸馏融入FL中。具体来说，我们利用FL和知识蒸馏为客户创建动态模型的集合，然后仅使用集合模型而不与环境交互，训练策略。此外，我们在理论上证明了单调改进的正确性。

    Federated learning (FL) is a privacy-preserving distributed machine learning paradigm that enables collaborative training among geographically distributed and heterogeneous devices without gathering their data. Extending FL beyond the supervised learning models, federated reinforcement learning (FRL) was proposed to handle sequential decision-making problems in edge computing systems. However, the existing FRL algorithms directly combine model-free RL with FL, thus often leading to high sample complexity and lacking theoretical guarantees. To address the challenges, we propose a novel FRL algorithm that effectively incorporates model-based RL and ensemble knowledge distillation into FL for the first time. Specifically, we utilise FL and knowledge distillation to create an ensemble of dynamics models for clients, and then train the policy by solely using the ensemble model without interacting with the environment. Furthermore, we theoretically prove that the monotonic improvement of the
    
[^186]: 优化中的异步迭代：新的序列结果和更加精准的算法保证

    Asynchronous Iterations in Optimization: New Sequence Results and Sharper Algorithmic Guarantees. (arXiv:2109.04522v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2109.04522](http://arxiv.org/abs/2109.04522)

    该论文介绍了一种新的收敛结果，可以应用于并行和分布式优化算法的异步迭代分析中。作者使用这一结果，使得数种并行优化算法的收敛证明变得更加精炼、简单同时还增强了原有的证明可信度，进而建立了至今缺乏完整理论理解的流行算法的收敛保证。

    

    我们介绍了一种新的收敛结果，可以应用于并行和分布式优化算法的异步迭代分析中。这些结果易于应用，并给出了迭代的异步程度如何影响收敛速度的明确估计。我们的结果缩短、简化和加强了现有的几种异步优化方法的收敛证明，并使我们能够为一些至今缺乏完整理论理解的流行算法建立收敛保证。具体来说，我们使用我们的结果来导出更好的迭代复杂度边界，以应用于近端增量聚合梯度方法，针对异步随机梯度下降方法基于平均而不是最大延迟提供更紧密的保证，为Krasnoselskii-Mann迭代的异步块坐标实现提供不那么保守的加速条件分析，量化收敛速度。

    We introduce novel convergence results for asynchronous iterations that appear in the analysis of parallel and distributed optimization algorithms. The results are simple to apply and give explicit estimates for how the degree of asynchrony impacts the convergence rates of the iterates. Our results shorten, streamline and strengthen existing convergence proofs for several asynchronous optimization methods and allow us to establish convergence guarantees for popular algorithms that were thus far lacking a complete theoretical understanding. Specifically, we use our results to derive better iteration complexity bounds for proximal incremental aggregated gradient methods, to obtain tighter guarantees depending on the average rather than maximum delay for the asynchronous stochastic gradient descent method, to provide less conservative analyses of the speedup conditions for asynchronous block-coordinate implementations of Krasnoselskii-Mann iterations, and to quantify the convergence rates
    
[^187]: 批量异步随机逼近的收敛性及在强化学习中的应用

    Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.03445](http://arxiv.org/abs/2109.03445)

    本文提出了一种批量异步随机逼近算法，它可以在内存需求和时间复杂度之间进行权衡，同时提供了可以使用较弱假设证明收敛的一般方法；在强化学习领域，我们使用此方法证明了SARSA算法的批量异步版本的收敛性。

    

    随机逼近（SA）算法是一种广泛使用的概率方法，用于在仅可用函数的有噪测量情况下找到零点或固定点。目前的文献中，区分“同步”更新和“异步”更新，在“同步”更新中，每个猜测的组件都会在每个时间更新，而在“异步”更新中，仅更新一个组件。本文研究了一种中间情况，称为“批量异步随机逼近”（BASA），在这种情况下，每个时间点仅更新“当前估计解”的一些但不是全部的组件。BASA允许用户在内存需求和时间复杂度之间进行权衡。我们开发了一种通用方法，证明此类算法收敛于所研究映射的固定点。这些收敛证明使用比现有结果更弱的假设。具体而言，现有的收敛证明要求步长参数以适当的速率下降。相反，我们仅要求每个组件具有足够的更新频率。我们在强化学习领域展示了我们方法的有用性，证明了广泛使用的SARSA算法的批量异步版本的收敛性。

    The stochastic approximation (SA) algorithm is a widely used probabilistic method for finding a zero or a fixed point of a vector-valued funtion, when only noisy measurements of the function are available. In the literature to date, one makes a distinction between ``synchronous'' updating, whereby every component of the current guess is updated at each time, and ``asynchronous'' updating, whereby only one component is updated. In this paper, we study an intermediate situation that we call ``batch asynchronous stochastic approximation'' (BASA), in which, at each time instant, \textit{some but not all} components of the current estimated solution are updated. BASA allows the user to trade off memory requirements against time complexity. We develop a general methodology for proving that such algorithms converge to the fixed point of the map under study. These convergence proofs make use of weaker hypotheses than existing results. Specifically, existing convergence proofs require that the 
    
[^188]: 基于人工智能的主动脉血管树分割在心血管疾病治疗中的现状

    AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2108.02998](http://arxiv.org/abs/2108.02998)

    本论文介绍了基于人工智能的主动脉血管树分割在心血管疾病治疗中的现状，提高了通过CTA检查检测主动脉及其分支血管的效率。

    

    主动脉血管树由主动脉及其分支动脉组成，在供应全身血液方面扮演着关键角色。主动脉疾病，如主动脉瘤或夹层，可能导致主动脉破裂，使用开放手术治疗极其危险。因此，患者通常接受药物治疗并进行持续监测，需要通过影像检查定期检查血管。用于诊断和监测的标准成像模式是计算机断层扫描（CT），如果加上对比剂（CTA），则可以提供主动脉及其分支血管的详细图像。最理想情况下，可以覆盖和比较来自连续CTA的整个主动脉血管树几何形状。这不仅可以检测主动脉的变化，还可以检测由主要病理或新发生的分支动脉引起的变化。手动重建此过程需要逐层勾画，可能需要一整天时间才能完成。

    The aortic vessel tree is composed of the aorta and its branching arteries, and plays a key role in supplying the whole body with blood. Aortic diseases, like aneurysms or dissections, can lead to an aortic rupture, whose treatment with open surgery is highly risky. Therefore, patients commonly undergo drug treatment under constant monitoring, which requires regular inspections of the vessels through imaging. The standard imaging modality for diagnosis and monitoring is computed tomography (CT), which can provide a detailed picture of the aorta and its branching vessels if completed with a contrast agent, called CT angiography (CTA). Optimally, the whole aortic vessel tree geometry from consecutive CTAs is overlaid and compared. This allows not only detection of changes in the aorta, but also of its branches, caused by the primary pathology or newly developed. When performed manually, this reconstruction requires slice by slice contouring, which could easily take a whole day for a sing
    
[^189]: 异构观测数据中联邦因果推断

    Federated Causal Inference in Heterogeneous Observational Data. (arXiv:2107.11732v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.11732](http://arxiv.org/abs/2107.11732)

    本文开发了联邦方法，在异构的本地数据中进行摘要统计信息的计算，并在站点之间聚合这些统计信息，以获得多站点数据的平均治疗效果的点估计和方差估计。聚合方案需要考虑站点之间的治疗分配异质性和结果的异质性，以使得估计量是一致的和渐近正常的。

    

    本文旨在估计应用于多个站点个体的治疗效应，其中每个站点存储本地数据，由于隐私限制，个体级数据不能在站点之间共享;同时，这些站点可能具有异构的人口和治疗分配机制。为了解决这些问题，我们开发了联邦方法，以对跨站点合并数据的平均治疗效果进行推断。我们的方法首先使用倾向分数在本地计算摘要统计信息，然后在站点之间聚合这些统计信息，以获得平均治疗效果的点估计和方差估计。我们展示了这些估计量是一致的和渐近正常的。为了实现这些渐近性质，我们发现聚合方案需要考虑站点之间的治疗分配异质性和结果的异质性。我们通过对两个大型医学临床进行比较研究来证明我们联邦方法的有效性。

    We are interested in estimating the effect of a treatment applied to individuals at multiple sites, where data is stored locally for each site. Due to privacy constraints, individual-level data cannot be shared across sites; the sites may also have heterogeneous populations and treatment assignment mechanisms. Motivated by these considerations, we develop federated methods to draw inference on the average treatment effects of combined data across sites. Our methods first compute summary statistics locally using propensity scores and then aggregate these statistics across sites to obtain point and variance estimators of average treatment effects. We show that these estimators are consistent and asymptotically normal. To achieve these asymptotic properties, we find that the aggregation schemes need to account for the heterogeneity in treatment assignments and in outcomes across sites. We demonstrate the validity of our federated methods through a comparative study of two large medical cl
    
[^190]: 变分不等式的分布式本地随机额外梯度算法

    Decentralized Local Stochastic Extra-Gradient for Variational Inequalities. (arXiv:2106.08315v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2106.08315](http://arxiv.org/abs/2106.08315)

    本文研究了非有界域上非IID分布式随机变分不等式问题，在分散的计算网络中使用随机额外梯度方法，在强单调、单调和非单调的情况下分别分析了收敛速度，并将其应用于开发具有非IID数据的联邦学习的分散算法。

    

    本文研究了非有界域上非IID分布式随机变分不等式问题。我们对计算网络进行了非常一般的假设，包括具有时变网络的完全分散计算和在联邦学习中常用的集中拓扑。另外，可以对节点进行多个本地更新以减少节点之间的通信频率。我们将随机额外梯度方法扩展到这个非常普遍的设置中，并在强单调、单调和非单调的情况下（当Minty解存在时）进行理论分析其收敛速度。提供的速率明确展示了网络特征（例如混合时间）、迭代计数器、数据异质性、方差、设备数量和其他标准参数的依赖关系。特别地，我们的方法和分析可应用于开发具有非IID数据的联邦学习的分散算法。

    We consider distributed stochastic variational inequalities (VIs) on unbounded domains with the problem data that is heterogeneous (non-IID) and distributed across many devices. We make a very general assumption on the computational network that, in particular, covers the settings of fully decentralized calculations with time-varying networks and centralized topologies commonly used in Federated Learning. Moreover, multiple local updates on the workers can be made for reducing the communication frequency between the workers. We extend the stochastic extragradient method to this very general setting and theoretically analyze its convergence rate in the strongly-monotone, monotone, and non-monotone (when a Minty solution exists) settings. The provided rates explicitly exhibit the dependence on network characteristics (e.g., mixing time), iteration counter, data heterogeneity, variance, number of devices, and other standard parameters. As a special case, our method and analysis apply to d
    
[^191]: FL-Market: 在联邦学习中交易私有模型

    FL-Market: Trading Private Models in Federated Learning. (arXiv:2106.04384v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.04384](http://arxiv.org/abs/2106.04384)

    本文提出FL-Market，一种隐私保护的本地私有模型交易市场，使用联邦学习解耦ML与经纪人集中收集训练数据的需求。

    

    对于基于机器学习（ML）的数据分析而言，获取足够的训练数据是一个主要瓶颈。近年来，把ML模型商品化已被提出作为一种经济且适度的数据获取解决方案。然而，现有的模型交易市场假定经纪人可以访问数据所有者的私有训练数据，这在实践中可能并不现实。为了促进可信数据获取，本文提出FL-Market，一种本地私有模型交易市场，不仅可以保护模型买家的隐私，还可以保护不可信赖的经纪人。FL-Market使用联邦学习（一种新兴的隐私保护ML范例，其中数据所有者通过上传本地梯度（即将聚合为用于更新模型的全局梯度）来协作地训练ML模型）将ML与经纪人集中收集训练数据的需求解耦。然后，FL-Market使数据所有者可以本地扰动其梯度，以进一步保护隐私。

    The difficulty in acquiring a sufficient amount of training data is a major bottleneck for machine learning (ML) based data analytics. Recently, commoditizing ML models has been proposed as an economical and moderate solution to ML-oriented data acquisition. However, existing model marketplaces assume that the broker can access data owners' private training data, which may not be realistic in practice. In this paper, to promote trustworthy data acquisition for ML tasks, we propose FL-Market, a locally private model marketplace that protects privacy not only against model buyers but also against the untrusted broker. FL-Market decouples ML from the need to centrally gather training data on the broker's side using federated learning, an emerging privacy-preserving ML paradigm in which data owners collaboratively train an ML model by uploading local gradients (to be aggregated into a global gradient for model updating). Then, FL-Market enables data owners to locally perturb their gradient
    
[^192]: 前向超分辨率：GAN如何学习逼近真实世界分布的分层生成模型

    Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions. (arXiv:2106.02619v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.02619](http://arxiv.org/abs/2106.02619)

    本文研究了GAN如何有效地学习那些接近真实图像分布的分层生成分布，当一个分布具有前向超分辨率结构时，通过SGDA简单地训练GAN就能够实现高效学习。

    

    生成对抗网络（GAN）是学习高复杂度真实世界分布的最成功模型之一。然而，由于最小最大训练目标的高度非凸、非凹特性，GAN在理论上仍然是深度学习模型中最难理解的。本文正式研究了GAN如何有效地学习那些接近真实图像分布的分层生成分布。我们证明了，当一个分布具有我们所称的前向超分辨率结构时，通过随机梯度下降上升（SGDA）简单地训练GAN就能够有效地学习这个分布，无论是样本还是时间复杂度。我们还提供了实证证据，表明我们所假设的“前向超分辨率”在实践中非常自然，而我们在本文中研究的底层学习机制（通过SGDA理论上允许我们高效地训练GAN）模拟了实际的学习过程。

    Generative adversarial networks (GANs) are among the most successful models for learning high-complexity, real-world distributions. However, in theory, due to the highly non-convex, non-concave landscape of the minmax training objective, GAN remains one of the least understood deep learning models. In this work, we formally study how GANs can efficiently learn certain hierarchically generated distributions that are close to the distribution of real-life images. We prove that when a distribution has a structure that we refer to as Forward Super-Resolution, then simply training generative adversarial networks using stochastic gradient descent ascent (SGDA) can learn this distribution efficiently, both in sample and time complexities. We also provide empirical evidence that our assumption "forward super-resolution" is very natural in practice, and the underlying learning mechanisms that we study in this paper (to allow us efficiently train GAN via SGDA in theory) simulates the actual lear
    
[^193]: 带有核函数的近因果学习：两阶段估计与矩限制

    Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment Restriction. (arXiv:2105.04544v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.04544](http://arxiv.org/abs/2105.04544)

    本论文提出了两种基于核函数的方法，用于解决存在未观测混淆，但同时观测到混淆代理的因果效应估计问题。这些方法在合成数据和模拟真实世界任务的数据上都可以获得有竞争力的结果。

    

    我们解决了存在未观测混淆，但同时观测到潜在混淆代理的因果效应估计问题。我们提出了两种基于核函数的非线性因果效应估计方法：（a）两阶段回归方法和（b）最大矩限制方法。我们关注近因果学习设置，但我们的方法可以用于解决由Fredholm积分方程表征的更广泛类别的反问题。特别地，我们为解决非线性设置中的该问题提供了两阶段和矩限制方法的统一视角。每个算法都提供了一致性保证，并且我们证明这些方法在合成数据和模拟真实世界任务的数据上都可以获得有竞争力的结果。特别地，我们的方法胜过了早期不能利用代理变量的方法。

    We address the problem of causal effect estimation in the presence of unobserved confounding, but where proxies for the latent confounder(s) are observed. We propose two kernel-based methods for nonlinear causal effect estimation in this setting: (a) a two-stage regression approach, and (b) a maximum moment restriction approach. We focus on the proximal causal learning setting, but our methods can be used to solve a wider class of inverse problems characterised by a Fredholm integral equation. In particular, we provide a unifying view of two-stage and moment restriction approaches for solving this problem in a nonlinear setting. We provide consistency guarantees for each algorithm, and we demonstrate these approaches achieve competitive results on synthetic data and data simulating a real-world task. In particular, our approach outperforms earlier methods that are not suited to leveraging proxy variables.
    
[^194]: DeepEverest：加速深度神经网络解释的声明性Top-K查询

    DeepEverest: Accelerating Declarative Top-K Queries for Deep Neural Network Interpretation. (arXiv:2104.02234v8 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2104.02234](http://arxiv.org/abs/2104.02234)

    DeepEverest是一个高效的系统，能够在深度神经网络激活值上执行解释查询。通过使用有效的索引技术和查询执行算法，DeepEverest可以将单个查询加速高达63倍，并在多查询工作负载上优于其他方法。

    

    我们设计、实现并评估了DeepEverest，这是一个用于在深度神经网络的激活值上执行解释查询的高效系统。DeepEverest包括一种有效的索引技术和一种具有各种优化的查询执行算法。我们证明了所提出的查询执行算法是实例最优的。我们的原型实验表明，DeepEverest仅使用不到完全材料化20%的存储空间，就能将单个查询加速高达63倍，并始终在模拟DNN解释过程的多查询工作负载上优于其他方法。

    We design, implement, and evaluate DeepEverest, a system for the efficient execution of interpretation by example queries over the activation values of a deep neural network. DeepEverest consists of an efficient indexing technique and a query execution algorithm with various optimizations. We prove that the proposed query execution algorithm is instance optimal. Experiments with our prototype show that DeepEverest, using less than 20% of the storage of full materialization, significantly accelerates individual queries by up to 63x and consistently outperforms other methods on multi-query workloads that simulate DNN interpretation processes.
    
[^195]: 探索轨迹推断的数学理论

    Towards a mathematical theory of trajectory inference. (arXiv:2102.09204v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.09204](http://arxiv.org/abs/2102.09204)

    本文提出了一个数学理论框架和数值方法，可以通过时间边缘样本推断出随机过程的轨迹，特别是它可以应用于单细胞RNA测序数据的分析和轨迹推断。

    

    我们设计了一个理论框架和数值方法，可以从随机过程的时间边缘样本中推断其轨迹。这个问题出现在单细胞RNA测序数据的分析中，它提供了细胞状态的高维度测量，但不能跟踪细胞的时间轨迹。我们证明，对于一类随机过程，可以从每个时间点的时间边缘的有限样本中恢复出真实轨迹，并提供一个在实践中高效地执行此操作的算法。我们开发的方法，全局Waddington-OT(gWOT)，可以通过涉及熵正则化最优传输的所有时间点的全局平滑凸优化问题来解决。我们展示了该问题可以在实践中高效地解决，并展示了在几个合成和实际数据集上的良好重建结果。

    We devise a theoretical framework and a numerical method to infer trajectories of a stochastic process from samples of its temporal marginals. This problem arises in the analysis of single cell RNA-sequencing data, which provide high dimensional measurements of cell states but cannot track the trajectories of the cells over time. We prove that for a class of stochastic processes it is possible to recover the ground truth trajectories from limited samples of the temporal marginals at each time-point, and provide an efficient algorithm to do so in practice. The method we develop, Global Waddington-OT (gWOT), boils down to a smooth convex optimization problem posed globally over all time-points involving entropy-regularized optimal transport. We demonstrate that this problem can be solved efficiently in practice and yields good reconstructions, as we show on several synthetic and real datasets.
    
[^196]: dame-flame：提供快速可解释匹配的因果推断Python库

    dame-flame: A Python Library Providing Fast Interpretable Matching for Causal Inference. (arXiv:2101.01867v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.01867](http://arxiv.org/abs/2101.01867)

    dame-flame是一个用于观察性因果推断匹配的Python软件包，实现了快速匹配算法DAME和FLAME，用机器学习确定重要协变量进行匹配，产生的结果是高质量且可解释。

    

    dame-flame是一个Python软件包，用于在包含离散协变量的数据集上执行观察性因果推断的匹配。该软件包实现了动态几乎完全匹配(DAME)和快速大规模几乎完全匹配(FLAME)算法，这些算法在协变量的子集上匹配治疗和对照组。由此产生的匹配组是可解释的，因为匹配是在协变量上进行的，并且是高质量的，因为机器学习可用于确定需要匹配的重要协变量。DAME通过解决优化问题只要在最可能的协变量上匹配单位，重要的协变量有优先匹配的机会。FLAME通过更快的向后特征选择过程，近似于DAME找到的解决方案。该软件包提供多个可调参数以适应特定应用程序，并且可以在匹配之后计算治疗效应估计值。这些参数的描述以及有关估计处理效果的详细信息可以在软件包文档中找到。

    dame-flame is a Python package for performing matching for observational causal inference on datasets containing discrete covariates. This package implements the Dynamic Almost Matching Exactly (DAME) and Fast Large-Scale Almost Matching Exactly (FLAME) algorithms, which match treatment and control units on subsets of the covariates. The resulting matched groups are interpretable, because the matches are made on covariates, and high-quality, because machine learning is used to determine which covariates are important to match on. DAME solves an optimization problem that matches units on as many covariates as possible, prioritizing matches on important covariates. FLAME approximates the solution found by DAME via a much faster backward feature selection procedure. The package provides several adjustable parameters to adapt the algorithms to specific applications, and can calculate treatment effect estimates after matching. Descriptions of these parameters, details on estimating treatmen
    

