# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [When does the ID algorithm fail?.](http://arxiv.org/abs/2307.03750) | ID算法解决了图形因果模型中干预分布的识别问题，但当输入分布无法识别时，所谓的"hedge准则"无效。 |
| [^2] | [Incentive-Theoretic Bayesian Inference for Collaborative Science.](http://arxiv.org/abs/2307.03748) | 本研究讨论了在协作科学中使用激励理论的贝叶斯推理方法，通过考虑研究者和决策者的不同激励机制，利用代理人的战略行为进行统计推断。 |
| [^3] | [QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models.](http://arxiv.org/abs/2307.03738) | QIGen是一种用于支持大型语言模型量化推理的自动代码生成方法，通过考虑目标架构和性能模型，实现了高性能和高准确性，并在LLaMA模型的基于CPU的推理任务上取得了比现有开源解决方案更好的效果。 |
| [^4] | [Steel Surface Roughness Parameter Calculations Using Lasers and Machine Learning Models.](http://arxiv.org/abs/2307.03723) | 本研究利用机器学习模型提高了钢铁表面粗糙度参数的计算准确性，通过对比不同方法，评估了在线测量转化的潜力。 |
| [^5] | [Polybot: Training One Policy Across Robots While Embracing Variability.](http://arxiv.org/abs/2307.03719) | 该论文提出了一种在多个机器人平台上训练单个策略的方法，通过使用腕部摄像头和统一的代码库对观测和动作空间进行对齐，并通过对比学习来消除领域差异。实验证明该方法在不同机器人平台上具有良好的效果。 |
| [^6] | [SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation.](http://arxiv.org/abs/2307.03716) | 本研究将生物进化中发展出的肌肉协同控制策略应用于人手和腿模型，发现通过协同行动表示（SAR）训练的策略在学习复杂任务时明显优于端到端强化学习。 |
| [^7] | [INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers.](http://arxiv.org/abs/2307.03712) | INT-FP-QSim是一个开源模拟器，用于评估大型语言模型和视觉转换器在不同精度和格式下的性能。通过使用不同的数值格式，我们研究了4位权重和4位或8位激活对模型性能的影响，并比较了不同方法的效果。 |
| [^8] | [Equivariant Single View Pose Prediction Via Induced and Restricted Representations.](http://arxiv.org/abs/2307.03704) | 本文提出了一种算法，通过引导和限制表示的等变性约束，在二维图像中学习三维世界表示，并满足几何一致性属性。 |
| [^9] | [Scalable Membership Inference Attacks via Quantile Regression.](http://arxiv.org/abs/2307.03694) | 通过分位数回归进行成员推断攻击的方法，与现有的影子模型攻击相比，计算量更小但效果相当。 |
| [^10] | [Suppressing unknown disturbances to dynamical systems using machine learning.](http://arxiv.org/abs/2307.03690) | 本文提出了一种使用机器学习的无模型方法，可以仅通过系统在已知强迫函数影响下的观测，识别和抑制未知系统的未知干扰。这项方法对训练函数有非常温和的限制，能够稳健地识别和抑制大类别的未知干扰。 |
| [^11] | [Differentiable Turbulence.](http://arxiv.org/abs/2307.03683) | 深度学习与物理启发式选择的深度学习架构相结合，实现了可微分湍流模型，在大涡模拟中有效地建模子网格尺度湍流，并能推广到多种流动配置。 |
| [^12] | [Guideline for Trustworthy Artificial Intelligence -- AI Assessment Catalog.](http://arxiv.org/abs/2307.03681) | 上述论文提出了一个可信人工智能准则--AI评估目录，旨在确保AI应用按照高质量标准开发，并有效防范新的AI风险，从而使AI发挥其潜力，减少不公平对待个体的风险。 |
| [^13] | [Undecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages.](http://arxiv.org/abs/2307.03679) | 本研究提出了一种将不可降解小波变换与嵌入语义边缘自动编码器相结合的新策略，用于改善多语言安全措施和降噪。该系统通过提取特征并保留数据中的时间和地理链接，成功捕获重要信息，并提高了系统检测异常和区分合法内容与危险威胁的能力。 |
| [^14] | [Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations.](http://arxiv.org/abs/2307.03678) | 本研究评估了大型语言模型在表示几何和空间关系文本描述中的有效性，发现虽然这些模型能够捕捉一些空间关系，但在估计数值和检索相关对象方面仍存在挑战。 |
| [^15] | [GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies.](http://arxiv.org/abs/2307.03675) | GeoPhy是一种创新的、完全可微的系统发育推断方法，通过在连续几何空间中表示拓扑分布，实现了可扩展的变分推断，克服了不限制拓扑结构的挑战。 |
| [^16] | [Simulation-free Schr\"odinger bridges via score and flow matching.](http://arxiv.org/abs/2307.03672) | [SF]$^2$M是一种无需模拟的方法，用于推断随机动力学。它将连续时间随机生成建模解释为Schr\"odinger桥问题，并通过静态熵正则化最优传输来高效学习。在学习细胞动力学方面表现出更高的准确性和效率。 |
| [^17] | [Online Network Source Optimization with Graph-Kernel MAB.](http://arxiv.org/abs/2307.03641) | 提出了Grab-UCB算法用于在线学习大规模网络中最优的源放置位置，通过使用自适应的图词典模型来描述网络过程，并利用稀疏谱表示实现了数据高效的学习框架。 |
| [^18] | [PAC bounds of continuous Linear Parameter-Varying systems related to neural ODEs.](http://arxiv.org/abs/2307.03630) | 本文研究了与神经ODE相关的LPV系统的PAC界限，该界限不依赖于积分区间。 |
| [^19] | [GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting.](http://arxiv.org/abs/2307.03595) | 本研究介绍了一种使用图神经网络 (GNNs) 作为数据增强来解决多视角时间序列预测中的“冷启动”问题的方法。这些GNN-based特征能够捕捉复杂的跨序列关系，并且可以与预测任务进行端到端的优化。 |
| [^20] | [Accelerated Optimization Landscape of Linear-Quadratic Regulator.](http://arxiv.org/abs/2307.03590) | 本文介绍了用于处理线性二次调节器（LQR）问题的加速优化框架，分析了SLQR和OLQR问题的收敛性。同时提出了LQR性能准则的Lipschitz Hessian特性，为现代优化技术的应用提供了重要指导。 |
| [^21] | [BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits.](http://arxiv.org/abs/2307.03587) | BOF-UCB是一种用于非平稳环境下的背景线性赌博机的贝叶斯优化频率算法，其结合了贝叶斯和频率学派原则，提高了在动态环境中的性能。它利用贝叶斯更新推断后验分布，并使用频率学派方法计算上界信心界以平衡探索和开发。实验证明，BOF-UCB优于现有方法，是非平稳环境中顺序决策的有前途的解决方案。 |
| [^22] | [ContextLabeler Dataset: physical and virtual sensors data collected from smartphone usage in-the-wild.](http://arxiv.org/abs/2307.03586) | 本研究介绍了一个数据收集活动和由智能手机传感器衍生而来的数据集，包含超过45K个样本，提供了关于用户日常活动的信息，并且采用了在野外实验的方式，消除了用户行为的偏见。 |
| [^23] | [Programmable Synthetic Tabular Data Generation.](http://arxiv.org/abs/2307.03577) | 这项工作介绍了ProgSyn，第一个可编程的合成表格数据生成算法，它允许对生成的数据进行全面的自定义，并且通过预训练和微调生成模型来确保高质量的数据和遵守自定义规范。 |
| [^24] | [Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization.](http://arxiv.org/abs/2307.03571) | 本文介绍了一种通用框架，可以在稀疏正则化中进行平滑优化，与主流的一阶优化方法兼容，并且能够得到匹配的全局最小值和等价的局部最小值。 |
| [^25] | [MALIBO: Meta-learning for Likelihood-free Bayesian Optimization.](http://arxiv.org/abs/2307.03565) | MALIBO是一种元学习贝叶斯优化方法，通过直接学习跨任务的查询效用，并引入辅助模型以实现对新任务的稳健适应，克服了现有方法的可伸缩性和不确定性的限制。 |
| [^26] | [DWReCO at CheckThat! 2023: Enhancing Subjectivity Detection through Style-based Data Sampling.](http://arxiv.org/abs/2307.03550) | 本文介绍了我们通过基于风格的数据采样来增强主观性检测任务的方法。我们使用GPT-3模型根据新闻记者视角的主观性检查清单生成额外的训练材料，并使用扩展的训练集微调语言特定的变形器模型。我们的实验结果表明，不同主观风格在所有语言中都是有效的，并且基于风格的过采样在土耳其语和英语中的效果优于改写方法。然而，在非英语语言中，GPT-3模型有时会生成乏味的结果。 |
| [^27] | [Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features.](http://arxiv.org/abs/2307.03544) | 本文提出了一种基于图神经网络的新方法，用于自动罗马数字分析。该方法可以直接处理乐谱中的每个音符，利用音符特征和音符之间的相互依赖关系，并通过新型边缩减算法产生按音的表示。在参考数据集上，ChordGNN模型表现优于现有的最先进模型，具有更高的罗马数字分析准确率。 |
| [^28] | [Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem.](http://arxiv.org/abs/2307.03515) | 本文提出了一种基于破产问题的方法来解决垂直联邦学习中激励分配的挑战，以确保公平性和稳定性。 |
| [^29] | [DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification.](http://arxiv.org/abs/2307.03500) | DEFT是一种利用模型层之间的梯度范数差异来实现可扩展的梯度稀疏化的方法，可以减少分布式深度学习中的通信流量，并在计算成本和梯度累积方面具有优势。 |
| [^30] | [HoughLaneNet: Lane Detection with Deep Hough Transform and Dynamic Convolution.](http://arxiv.org/abs/2307.03494) | 本文提出了一种使用深度Hough变换和动态卷积进行车道检测的方法，该方法利用车道的几何结构和分层特征聚合，能够有效地检测和分割复杂场景中的车道。 |
| [^31] | [ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers.](http://arxiv.org/abs/2307.03493) | ITA是一种基于量化的Transformer的能效高的Attention和Softmax加速器。通过利用8位量化和仅基于整数值的创新Softmax实现，ITA实现了高能效的推理，在16.9 TOPS/W的能效上超过了最先进的Transformer加速器，并在5.93 TOPS/mm$^2$的面积效率上超越了它们。 |
| [^32] | [Learning Theory of Distribution Regression with Neural Networks.](http://arxiv.org/abs/2307.03487) | 本文提出了一个全连接神经网络框架用于分布回归问题，解决了传统神经网络无法直接处理概率分布输入的困难，并建立了逼近理论和学习理论。 |
| [^33] | [Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning.](http://arxiv.org/abs/2307.03486) | 通过对比学习方法，我们在强化学习中提出了新的成就蒸馏方法，可以加强代理对下一个解锁成就的预测能力，并优于先前的模型驱动和层次化方法。 |
| [^34] | [Unpaired Multi-View Graph Clustering with Cross-View Structure Matching.](http://arxiv.org/abs/2307.03476) | 无配对多视图图聚类方法结合跨视图结构匹配，解决了多视图数据未配对问题，提高了聚类性能。 |
| [^35] | [Differential Privacy for Clustering Under Continual Observation.](http://arxiv.org/abs/2307.03430) | 本文提出了一种在持续观察下的差分隐私聚类机制，用于在被删除和插入数据点的数据集中进行聚类，这是第一个具有仅以更新次数的对数依赖性的增加误差的近似算法，并且乘法误差几乎与非隐私情况相同。 |
| [^36] | [Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer.](http://arxiv.org/abs/2307.03427) | 这篇论文提出了一种合并-分流混合Transformer网络用于头颈癌的生存预测。该网络利用多模态影像和提取特定区域信息，通过合并编码器和分流解码器实现生存预测，取得了很好的效果。 |
| [^37] | [Hyperspectral and Multispectral Image Fusion Using the Conditional Denoising Diffusion Probabilistic Model.](http://arxiv.org/abs/2307.03423) | 本文提出了一种基于条件去噪扩散概率模型的高光谱和多光谱图像融合方法，通过前向扩散过程和逆向降噪过程实现对高空间分辨率HSI的去噪和复原。 |
| [^38] | [Learning from Heterogeneity: A Dynamic Learning Framework for Hypergraphs.](http://arxiv.org/abs/2307.03411) | 本文提出了一个名为LFH的超图学习框架，能够动态构建超边并利用异质属性进行嵌入更新。实验结果表明，该框架在多个数据集上取得了良好的效果。 |
| [^39] | [Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data.](http://arxiv.org/abs/2307.03410) | 这篇论文提出了一个适用于特征分布式数据的可扩展高维多变量线性回归算法，具有通信复杂度不依赖于特征维度和快速收敛性的优势，可应用于大规模数据集和具有多变量响应变量的场景。 |
| [^40] | [Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning.](http://arxiv.org/abs/2307.03406) | 本论文研究了将决策制定视为离线收集的轨迹的监督学习问题，并探究了序列建模在轨迹压缩和策略学习中的作用。通过引入目标条件预测编码（GCPC），本文提出了一种能够产生强大轨迹表示并实现高性能策略的方法。 |
| [^41] | [Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs.](http://arxiv.org/abs/2307.03393) | 本文探索了大规模语言模型（LLMs）在图学习中的潜力，并尝试了两种不同的流程：将LLMs作为增强器通过海量知识来增强节点的文本属性，并使用图神经网络（GNNs）生成预测，以及直接使用LLMs作为独立的预测器。 |
| [^42] | [AI-UPV at EXIST 2023 -- Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime.](http://arxiv.org/abs/2307.03385) | 以学习与分歧的机制为框架，使用大型语言模型进行性别歧视识别和表征的研究，以推动更具包容性和尊重性的在线环境。 |
| [^43] | [Teaching Arithmetic to Small Transformers.](http://arxiv.org/abs/2307.03381) | 本研究研究了如何通过训练小型Transformer模型在没有先验训练的情况下高效学习基本算术运算，并提出了一种通过格式变化和使用链式思维样式数据来提高准确性的方法。 |
| [^44] | [On Formal Feature Attribution and Its Approximation.](http://arxiv.org/abs/2307.03380) | 这篇论文研究了解释性人工智能（XAI）中的形式特征归因方法及其近似方法。现有的特征选择和归因方法存在一些问题，而形式化的XAI方法虽然是一个有希望的解决方案，但仍存在一些限制。 |
| [^45] | [Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and Toxic Language Detection.](http://arxiv.org/abs/2307.03377) | 本文提出了一种基于任务感知的方法，用于解决性别歧视、仇恨言论和有害语言检测中的负面迁移问题，并能够减少负面迁移并提高性能。 |
| [^46] | [STG-MTL: Scalable Task Grouping for Multi-Task Learning Using Data Map.](http://arxiv.org/abs/2307.03374) | 本文提出了一种数据驱动方法，使用数据映射来解决多任务学习中的任务分组问题。这种方法具有可伸缩性和模块化，并且在大量任务下仍然有效。 |
| [^47] | [Distilled Pruning: Using Synthetic Data to Win the Lottery.](http://arxiv.org/abs/2307.03364) | 该论文介绍了一种使用蒸馏数据来修剪深度学习模型的新方法，能够比传统方法更快地找到稀疏的可训练子网络，具有资源高效的神经网络修剪潜力。 |
| [^48] | [Federated Unlearning via Active Forgetting.](http://arxiv.org/abs/2307.03363) | 本文提出了一种基于增量学习的新型联邦遗忘框架，解决了现有联邦遗忘方法在时间效率、数据影响估计不精确和计算负荷大等方面的问题。 |
| [^49] | [Evaluating Biased Attitude Associations of Language Models in an Intersectional Context.](http://arxiv.org/abs/2307.03360) | 这篇论文以已建立的文献为基础，量化了英语语言模型中社会群体的情绪关联，并发现语言模型对性别认同、社会阶级和性取向的信号表现出最大的偏见态度。 |
| [^50] | [CSCLog: A Component Subsequence Correlation-Aware Log Anomaly Detection Method.](http://arxiv.org/abs/2307.03359) | CSCLog是一种考虑组件子序列相关性的日志异常检测方法，通过捕获子序列中的顺序依赖关系和建模子序列的隐式相关性来检测日志中的异常。 |
| [^51] | [Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms.](http://arxiv.org/abs/2307.03357) | 本文通过统计学习理论的算法稳定性，分析了随机组合梯度下降算法的稳定性和泛化性，引入了组合一致稳定性概念并与SCO问题的泛化性建立了定量关系。 |
| [^52] | [Distilling Universal and Joint Knowledge for Cross-Domain Model Compression on Time Series Data.](http://arxiv.org/abs/2307.03347) | 这篇论文介绍了一种用于跨领域模型压缩的新的框架，通过蒸馏教师模型中的通用特征级知识和双领域共享的联合logit级知识，实现了在资源有限环境中部署深度学习模型的难题。 |
| [^53] | [Personalized Prediction of Recurrent Stress Events Using Self-Supervised Learning on Multimodal Time-Series Data.](http://arxiv.org/abs/2307.03337) | 本研究提出了一种使用可穿戴生物信号数据的多模态个性化压力预测系统，通过自监督学习在每个受试者的数据上进行预训练，然后进行压力预测任务微调。实验证明我们的方法可以实现使用少量标注来个性化预测压力情况。 |
| [^54] | [Variational quantum regression algorithm with encoded data structure.](http://arxiv.org/abs/2307.03334) | 本文介绍了一个具有编码数据结构的变分量子回归算法，在量子机器学习中具有模型解释性，并能有效地处理互连度较高的量子比特。算法通过压缩编码和数字-模拟门操作，大大提高了在噪声中尺度量子计算机上的运行时间复杂度。 |
| [^55] | [ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation.](http://arxiv.org/abs/2307.03332) | 本文提出了一种基于注意力引导的协同决策网络（ACDNet）用于药物推荐，通过利用注意力机制和Transformer对患者的健康状况和药物记录进行建模，同时采用协同决策框架，从患者药物记录和具体药物之间的相似性出发，有效地个性化推荐药物。 |
| [^56] | [Encoder-Decoder Networks for Self-Supervised Pretraining and Downstream Signal Bandwidth Regression on Digital Antenna Arrays.](http://arxiv.org/abs/2307.03327) | 本研究首次将自监督学习应用于数字天线阵列数据，通过编码器-解码器网络自监督预训练和少量标记数据迁移训练，实现了优于随机初始化等效网络的数字阵列数据带宽回归任务。 |
| [^57] | [Machine Learning to detect cyber-attacks and discriminating the types of power system disturbances.](http://arxiv.org/abs/2307.03323) | 本研究提出了一种基于机器学习的攻击检测模型，能够使用数据和日志来学习电力系统行为，在智能电网中有效地识别潜在的安全边界，并且使用随机森林模型能够达到90.56%的准确率，有助于操作员决策。 |
| [^58] | [Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation.](http://arxiv.org/abs/2307.03315) | 通过解决体外膜肺氧合(ECMO)治疗选择偏差和稀缺治疗案例的挑战，我们提出了一种名为Treatment Variational AutoEncoder (TVAE)的方法，用于个体化治疗分析，以支持临床决策。 |
| [^59] | [On Invariance, Equivariance, Correlation and Convolution of Spherical Harmonic Representations for Scalar and Vectorial Data.](http://arxiv.org/abs/2307.03311) | 这篇论文介绍了球面谐波表示的理论基础和实际应用，包括旋转不变和等变特征，以及球面上信号的卷积和精确相关性。此外，还将这些方法推广到了矢量谐波表示。 |
| [^60] | [When Fair Classification Meets Noisy Protected Attributes.](http://arxiv.org/abs/2307.03306) | 这项研究是对公平分类算法进行的一次首次的头对头比较，研究了依赖属性、容忍噪声和盲目属性的算法在预测性和公平性方面的表现，结果显示盲目属性和容忍噪声的公平分类器具有潜力。 |
| [^61] | [A Vulnerability of Attribution Methods Using Pre-Softmax Scores.](http://arxiv.org/abs/2307.03305) | 这篇论文讨论了使用前softmax分数的归属方法的一个漏洞，该方法用于解释卷积神经网络分类器输出。与对抗性攻击不同，作者关注的是对归属方法进行小修改可能导致的影响，而不会改变模型的输出。 |
| [^62] | [Equivariant Spherical CNN for Data Efficient and High-Performance Medical Image Processing.](http://arxiv.org/abs/2307.03298) | 等变球卷积神经网络能够提高医学图像处理的效率和性能，通过降低对特定训练集的依赖性，并且在去噪和重建方面表现出卓越的质量和计算效率。 |
| [^63] | [OmniBoost: Boosting Throughput of Heterogeneous Embedded Devices under Multi-DNN Workload.](http://arxiv.org/abs/2307.03290) | 本文提出了OmniBoost，一种轻量级、可扩展的异构嵌入式设备多DNN管理器。通过利用随机空间探索和高精度性能估计器，相比其他先进方法，OmniBoost平均提高了4.6倍的吞吐量。 |
| [^64] | [Optimal Scalarizations for Sublinear Hypervolume Regret.](http://arxiv.org/abs/2307.03288) | 研究了用于亚线性超体积遗憾度量的最优标量化方法，证明了具有均匀随机权重的超体积标量化方法在最小化超体积遗憾方面是最优的，并在多目标随机线性赌博机问题上进行了案例研究。 |
| [^65] | [A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation.](http://arxiv.org/abs/2307.03270) | 该论文提出了一种多尺度方法，通过使用多尺度音视同步损失和多尺度自回归生成对抗网络，实现了语音和头部动力学的同步生成。实验证明，在当前状态下取得了显著的改进。 |
| [^66] | [Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging.](http://arxiv.org/abs/2307.03266) | 本文通过对前列腺成像中的新型基础模型UniverSeg进行了实证评估研究，并将其与传统方法进行了比较。结果表明基础模型可能是医学成像领域未来的方向。 |
| [^67] | [Vision Language Transformers: A Survey.](http://arxiv.org/abs/2307.03254) | 视觉语言转换器是将预训练的transformer架构应用于视觉语言建模的研究领域，通过迁移学习，在同时进行视觉和语言任务中取得了显著改进。 |
| [^68] | [Neural Network Field Theories: Non-Gaussianity, Actions, and Locality.](http://arxiv.org/abs/2307.03223) | 本文研究了神经网络场理论，包括非高斯性、作用量和局域性。通过对网络参数的统计独立性进行微小破坏，可以得到相互作用理论，这种展开方法相比于常用的$1/N$展开在普适逼近定理方面表现更好，同时通过关联函数可以系统地重建作用量。 |
| [^69] | [Quantification of Uncertainty with Adversarial Models.](http://arxiv.org/abs/2307.03217) | 该论文提出了使用对抗模型（QUAM）来更好地估计认知不确定性。QUAM识别整个积分下乘积较大的区域，而不仅仅是后验。与先前的方法相比，QUAM对认知不确定性的近似误差更小。 |
| [^70] | [Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings.](http://arxiv.org/abs/2307.03212) | 提出了一种区域关注的多视角表示学习（ROMER）的算法，用于捕捉多视角之间的依赖关系，学习城市区域的表达能力，并在多源城市数据中优于现有方法。 |
| [^71] | [PseudoCell: Hard Negative Mining as Pseudo Labeling for Deep Learning-Based Centroblast Cell Detection.](http://arxiv.org/abs/2307.03211) | PseudoCell是一种基于深度学习的目标检测框架，用于自动化中心母细胞检测。它通过将病理学家的中心母细胞标签与使用细胞形态特征对低采样假阳性预测得到的伪阴性标签相结合，可以准确地缩小需要检查组织的区域。 |
| [^72] | [Sparse Graphical Linear Dynamical Systems.](http://arxiv.org/abs/2307.03210) | 该论文提出了一种稀疏图线性动态系统方法，用于处理时间序列数据集。该方法可以利用图形视角来学习模型参数，解决了现有方法的限制，并提供了概率性和可解释性的学习结果。 |
| [^73] | [Optimal Bandwidth Selection for DENCLUE.](http://arxiv.org/abs/2307.03206) | 本文提出了一种计算DENCLUE算法最优参数的新方法，并在实验部分讨论了其性能。 |
| [^74] | [Scaling Laws Do Not Scale.](http://arxiv.org/abs/2307.03201) | 本文讨论了缩放定律与人工智能模型性能之间的关系，并指出数据集规模的增加会引发不同社群的价值观和偏见风险。 |
| [^75] | [Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks.](http://arxiv.org/abs/2307.03197) | 本研究对SplitFed Learning中的数据污染攻击进行了研究和分析，并提出了三种新的攻击策略。实验结果表明这些攻击策略可以降低基于DCML的分类器的性能。 |
| [^76] | [Improving the Efficiency of Human-in-the-Loop Systems: Adding Artificial to Human Experts.](http://arxiv.org/abs/2307.03003) | 本研究提出了一个混合系统，在人类专家中添加人工智能，从之前由人类专家审查过的未知类别的数据实例中学习分类，以提高人机协同系统的效率。 |
| [^77] | [Elastic Decision Transformer.](http://arxiv.org/abs/2307.02484) | 弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。 |
| [^78] | [Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency.](http://arxiv.org/abs/2307.02150) | 该论文研究了特征归因方法在不同深度学习架构之间的通用性，并探讨了在多个模型上协调特征归因的可行性。研究结果显示特征归因的协调有助于提高深度学习模型的解释性和一致性。 |
| [^79] | [Stability Analysis Framework for Particle-based Distance GANs with Wasserstein Gradient Flow.](http://arxiv.org/abs/2307.01879) | 本文提出了一个基于粒子距离的GAN稳定性分析框架，并使用Wasserstein渐变流对GAN的训练过程进行稳定性分析。研究发现，由于GAN的目标函数形式的原因，判别器的训练过程通常是不稳定的。为了解决这个问题，文中提出了一个稳定的训练方法。 |
| [^80] | [Coupled Gradient Flows for Strategic Non-Local Distribution Shift.](http://arxiv.org/abs/2307.01166) | 该论文提出了一种框架，用于分析现实世界系统中的分布偏移动态，并且捕捉了学习算法与其应用的分布之间的反馈循环。通过耦合偏微分方程模型，考虑了战略性反应、非局部内生人口互动和其他外生分布偏移来源，以实现对时间上细微变化的捕捉。研究证明了在合作设置和竞争设置中，当算法通过梯度下降进行重新训练时，重新训练过程渐近收敛到一个稳定状态。 |
| [^81] | [RObotic MAnipulation Network (ROMAN) -- Hybrid Hierarchical Learning for Solving Complex Sequential Tasks.](http://arxiv.org/abs/2307.00125) | RObotic MAnipulation Network（ROMAN）通过混合层次学习框架解决复杂的连续操作任务，实现了任务多样性和鲁棒的失败恢复。 |
| [^82] | [Suffering Toasters.](http://arxiv.org/abs/2306.17258) | 本文旨在为人工智能、自我意识和代理问题提供更清晰的定义，我们提出了一种新的启发式方法来测试人工自我意识，并讨论了这种方法引发的一些问题。 |
| [^83] | [Understanding the Overfitting of the Episodic Meta-training.](http://arxiv.org/abs/2306.16873) | 本研究通过引入知识蒸馏技术，解决了迭代元训练阶段的过拟合问题，该方法通过惩罚过度区分，保留教师模型的新类别泛化知识，优于标准元训练过程。同时，我们提出了最近邻对称Kullback-Leibler（NNSKL）散度来进一步推进知识蒸馏的极限。 |
| [^84] | [Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack.](http://arxiv.org/abs/2306.16050) | 通过对抗攻击评估了深度图像去噪模型的相似性和鲁棒性，发现现有模型容易被攻击。还研究了去噪模型的迁移性在图像去噪任务中的特性。 |
| [^85] | [Enhancing Adversarial Training via Reweighting Optimization Trajectory.](http://arxiv.org/abs/2306.14275) | 本文提出了一种名为“加权优化轨迹（WOT）”的新方法，通过优化历史轨迹，解决了对抗训练中的鲁棒泛化问题。 |
| [^86] | [FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning.](http://arxiv.org/abs/2306.13264) | 本文提出了一种名为FedSelect的新联邦学习框架，通过寻找最佳客户端子网络从而直接个性化客户端子网络结构和参数，同时保留了全局知识，提高了客户端性能。 |
| [^87] | [Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots.](http://arxiv.org/abs/2306.12898) | 该论文提出了一种基于机器学习的实时反馈控制InAs/GaAs量子点生长方法。 |
| [^88] | [Learning Homogenization for Elliptic Operators.](http://arxiv.org/abs/2306.12006) | 本文提出了一种新的数据驱动方法，可以学习用于椭圆算子的齐次化映射，以建立考虑接口的齐次化本构定律，并且证明了该方法的有效性。 |
| [^89] | [Adaptive Strategies in Non-convex Optimization.](http://arxiv.org/abs/2306.10278) | 本文介绍了应用于随机优化和深度神经网络训练中的自适应算法。算法可以自动适应不同的噪声水平和梯度比例范围，从而达到（近似）最优速度。 |
| [^90] | [Offline Prioritized Experience Replay.](http://arxiv.org/abs/2306.05412) | 本文提出了离线优先经验重放（OPER）方法来解决离线强化学习中的分布偏移问题。通过设计一类优先级函数来对高回报的转换进行优先处理，从而改善行为策略，并在此改进的策略约束下优化离线强化学习算法的解决方案。对于离线强化学习，OPER方法是一种有效的解决方案。 |
| [^91] | [Don't trust your eyes: on the (un)reliability of feature visualizations.](http://arxiv.org/abs/2306.04719) | 本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。 |
| [^92] | [Language Models are Bounded Pragmatic Speakers.](http://arxiv.org/abs/2305.17760) | 本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。 |
| [^93] | [Decentralized Learning over Wireless Networks: The Effect of Broadcast with Random Access.](http://arxiv.org/abs/2305.07368) | 本文研究了分散式学习在无线网络中的通信问题，发现优化接入概率以最大化成功链路数的期望值是提升系统收敛速度的有效策略。 |
| [^94] | [SocNavGym: A Reinforcement Learning Gym for Social Navigation.](http://arxiv.org/abs/2304.14102) | 本文提出了SocNavGym，对于社交导航领域的研究提供了一个轻便、快速、易用的仿真环境，可生成各种各样的社交导航场景，并促进了智能社交机器人的发展。 |
| [^95] | [Robust Tickets Can Transfer Better: Drawing More Transferable Subnetworks in Transfer Learning.](http://arxiv.org/abs/2304.11834) | 该论文提出了一种新的迁移学习流程，通过绘制具有鲁棒性的子网络，改进了预训练模型在资源受限的边缘设备上的传输能力。 |
| [^96] | [A Characterization of Online Multiclass Learnability.](http://arxiv.org/abs/2303.17716) | 在线多类学习问题中，使用Multiclass Littlestone维度可以刻画标签数目为无界情况下的可学习性。 |
| [^97] | [Highly Accurate Quantum Chemical Property Prediction with Uni-Mol+.](http://arxiv.org/abs/2303.16982) | 本文提出了一种使用Uni-Mol+的新方法来高精度预测量子化学属性，它能够从2D分子图自动生成3D构象，并通过迭代优化得到优化后的构象，为预测QC属性提供更加准确的基础。 |
| [^98] | [Auxiliary Functions as Koopman Observables: Data-Driven Polynomial Optimization for Dynamical Systems.](http://arxiv.org/abs/2303.01483) | 提出了一种基于数据的动力系统分析方法，使用辅助函数作为Koopman可观测量，不需要明确的模型发现，可以适用于确定性和随机动力学，具有收敛性和性能优势。 |
| [^99] | [Continuous-Time Functional Diffusion Processes.](http://arxiv.org/abs/2303.00800) | 连续时间功能扩散过程引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。通过使用新的数学框架和扩展，FDPs可以在函数空间中构建新型生成模型，在处理连续数据时能够实现高质量的图像生成，所需参数数量比现有模型低几个数量级。 |
| [^100] | [Composite Optimization Algorithms for Sigmoid Networks.](http://arxiv.org/abs/2303.00589) | 本文使用复合优化算法解决Sigmoid网络问题，可以在非凸和非光滑问题的情况下收敛到全局最优解，并提供了一般指导来设置网络大小。 |
| [^101] | [On discrete symmetries of robotics systems: A group-theoretic and data-driven analysis.](http://arxiv.org/abs/2302.10433) | 本文研究了运动系统的离散形态对称性，并提出了一个理论和实践框架，用于识别系统的形态对称群，并分析对称性在数据增强和控制设计中的应用。 |
| [^102] | [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat.](http://arxiv.org/abs/2302.10289) | 本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。 |
| [^103] | [Black-Box Batch Active Learning for Regression.](http://arxiv.org/abs/2302.08981) | 本文提出了一种黑盒批量主动学习方法，该方法仅利用模型预测进行评估，适用于各种机器学习模型，并扩展了一系列白盒批量主动学习方法。 |
| [^104] | [Federated Variational Inference Methods for Structured Latent Variable Models.](http://arxiv.org/abs/2302.03314) | 本论文提出了一种基于结构化变分推断的联合学习方法，该方法适用于各种结构化概率模型。此外，还提供了一种通信高效的变体算法，证明了所提出算法的有效性和性能。 |
| [^105] | [Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning.](http://arxiv.org/abs/2301.10886) | 本文提出了一种名为AIRS的自动内在奖励塑造探索方法，可以提供高质量的内在激励以增强强化学习中的探索性能；并开发了高效可靠的内在奖励工具包。实验表明，AIRS性能卓越，能够胜过基准方案。 |
| [^106] | [Detection of Groups with Biased Representation in Ranking.](http://arxiv.org/abs/2301.00719) | 本文研究了在排名中检测具有偏倚表示的群体的问题，并提出了两种公平度量方法的高效搜索算法。同时，提出了一种利用Shapley值解释群体表示偏差的方法。 |
| [^107] | [Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers.](http://arxiv.org/abs/2212.11498) | 该论文提出了一种可扩展的多智能体强化学习方法，用于仓库物流中的机器人和人类同事合作。他们通过分层的MARL算法，让经理和工人代理根据全局目标进行协同训练，以最大化拣货速率。 |
| [^108] | [A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling.](http://arxiv.org/abs/2212.10936) | 本文提出了一种基于深度强化学习的模因算法，用于解决具有实际约束的灵活生产调度问题，并弥补元启发式研究中的缺陷。 |
| [^109] | [Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model.](http://arxiv.org/abs/2212.09811) | 本研究提出了一种节约内存的NLLB-200模型修剪方法，可在保持翻译质量的同时移除多达80％的专家，使得在单个32GB的GPU上运行模型成为可能。这对于大规模多语言机器翻译具有重要的意义。 |
| [^110] | [On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning.](http://arxiv.org/abs/2212.06573) | 本文提出了一个框架，利用多模式对比学习模型来识别恶意内容的目标，并系统地研究恶意模因的演化。通过研究反犹太主义模因，我们发现了CLIP生成的嵌入向量中存在的语义规律，从而揭示了恶意模因的创造方式。 |
| [^111] | [Equivariance with Learned Canonicalization Functions.](http://arxiv.org/abs/2211.06489) | 本文提出了一种使用学习的规范化函数来实现等变性的方法，避免了对神经网络架构的限制。实验结果表明，学习规范化函数可以在多个任务中与现有技术相比具有竞争力，并且速度更快。 |
| [^112] | [Breadth-First Pipeline Parallelism.](http://arxiv.org/abs/2211.05953) | 宽度优先的流水线并行计算方法结合了流水线和数据并行计算，通过在每个GPU上使用小批量大小和完全分片的数据并行计算，以提高训练吞吐量。在实验中，与Megatron-LM相比，在一个520亿参数的模型上，使用小批量大小每个GPU的训练吞吐量增加了高达43%。 |
| [^113] | [Layer Ensembles.](http://arxiv.org/abs/2210.04882) | 本文介绍了一种基于独立分类分布的不确定性估计方法，通过重复使用公共的层输出和对样本进行排序，可以大幅提高模型运行速度、降低内存使用，并且获得比常规深度集成更高的不确定性质量。 |
| [^114] | [Bicriteria Approximation Algorithms for Priority Matroid Median.](http://arxiv.org/abs/2210.01888) | 本文研究了优先级拟阵中值问题，提出了双目标近似算法，通过选择合适的设施子集实现同时最小化开放成本和满足约束条件。 |
| [^115] | [TabLeak: Tabular Data Leakage in Federated Learning.](http://arxiv.org/abs/2210.01785) | TabLeak是第一个针对表格数据的全面重构攻击的方法，解决了联邦学习中表格数据泄漏的挑战，包括解决混合离散连续优化问题和实现人类评估。 |
| [^116] | [Avoiding Post-Processing with Event-Based Detection in Biomedical Signals.](http://arxiv.org/abs/2209.11007) | 这项研究提出了一种基于事件检测的生物医学信号处理框架，避免了繁琐的后处理过程，直接将事件作为学习目标，拥有较强的实用性能。 |
| [^117] | [Identifying Patient-Specific Root Causes with the Heteroscedastic Noise Model.](http://arxiv.org/abs/2205.13085) | 本研究使用异方差噪声模型，开发了一种名为广义根因因果推断（GRCI）的算法，能够更准确地识别疾病的患者特定根本原因。 |
| [^118] | [Self-Supervised Time Series Representation Learning via Cross Reconstruction Transformer.](http://arxiv.org/abs/2205.09928) | 本文提出了一种称为交叉重构变压器(CRT)的方法，通过跨域丢弃-重构任务实现了时间序列的表示学习。CRT解决了构建数据对的先验知识、采样策略繁琐和采样偏差导致性能不稳定等问题，同时有效地建模了跨时谱关系以扩展表示的容量。 |
| [^119] | [k-strip: A novel segmentation algorithm in k-space for the application of skull stripping.](http://arxiv.org/abs/2205.09706) | k-strip是一种新型的基于深度学习的头骨剥离算法，该算法直接在信息丰富的k空间中工作，并且在与金标准的比较中取得了较高的相似性。 |
| [^120] | [GRAPHSHAP: Explaining Identity-Aware Graph Classifiers Through the Language of Motifs.](http://arxiv.org/abs/2202.08815) | GRAPHSHAP是一种基于Shapley值的方法，可以提供基于模样式的解释，用于解释基于身份感知的图分类器，而无需对模型或其训练数据有任何先验知识。 |
| [^121] | [Reward-Respecting Subtasks for Model-Based Reinforcement Learning.](http://arxiv.org/abs/2202.03466) | 论文提出了一种基于模型的强化学习的方法，通过添加奖励加成的子任务来发现选项，从而解决了以前方法中忽略原始奖励的问题。 |
| [^122] | [Deep Optimal Transport for Domain Adaptation on SPD Manifolds.](http://arxiv.org/abs/2201.05745) | 这项研究介绍了一种基于深度最优传输的方法，用于解决在SPD流形上的领域自适应问题。通过利用最优传输理论和SPD流形的对数欧几里得几何，我们克服了协方差矩阵操作的复杂性挑战。 |
| [^123] | [Creativity of AI: Hierarchical Planning Model Learning for Facilitating Deep Reinforcement Learning.](http://arxiv.org/abs/2112.09836) | 该论文提出了一种新颖的深度强化学习框架，通过在DRL中嵌入符号知识，解决了数据效率、解释性缺乏和可迁移性等关键问题。该框架通过循环训练过程，并利用学习的规划模型和符号选项来引导策略的改进。学到的符号选项减轻了对专家领域知识的要求，并提供了策略的内在可解释性，同时通过与符号规划模型的规划进一步提高了可迁移性和数据效率。 |
| [^124] | [When and How to Fool Explainable Models (and Humans) with Adversarial Examples.](http://arxiv.org/abs/2107.01943) | 本文研究了针对可解释机器学习模型的敌对攻击的可能性和限制，并提出了一个全面的框架来研究在人类评估下如何生成敌对示例。 |
| [^125] | [F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning.](http://arxiv.org/abs/2004.11145) | 本文提出了一个灵活完全分散化的演员-评论家多智能体强化学习框架，在大规模合作多智能体环境中，通过设计一个基于原始-对偶混合梯度下降的算法框架，可以分别学习每个智能体，并实现策略改进和价值评价。 |
| [^126] | [Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning.](http://arxiv.org/abs/2001.04413) | 提出了一种新的原则 "向后特征修正"，通过训练过程中自动修正较低层次特征的错误，使得深度学习能够进行深度（分层）学习。 |

# 详细

[^1]: ID算法在什么情况下失效？

    When does the ID algorithm fail?. (arXiv:2307.03750v1 [stat.ME])

    [http://arxiv.org/abs/2307.03750](http://arxiv.org/abs/2307.03750)

    ID算法解决了图形因果模型中干预分布的识别问题，但当输入分布无法识别时，所谓的"hedge准则"无效。

    

    ID算法解决了图形因果模型中形式为p(Y | do(a))的干预分布的识别问题，并且已经以多种方式进行了公式化。ID算法是正确的（在输入图所代表的因果模型中，输出观测数据分布的正确功能），并且是完整的（当输入的p(Y | do(a))在输入图所代表的因果模型中无法识别时，明确标记为失败）。参考文献[9]提供了一个结果，即所谓的"hedge准则"（Corollary 3），旨在以输入图中的一个称为"hedge"的结构来给出ID算法无法识别其输入的情况的图形特征。虽然ID算法确实是一个正确和完整的算法，并且只要输入分布无法识别，"hedge"结构就会出现，但是在[9]中提出的Corollary 3是不正确的。

    The ID algorithm solves the problem of identification of interventional distributions of the form p(Y | do(a)) in graphical causal models, and has been formulated in a number of ways [12, 9, 6]. The ID algorithm is sound (outputs the correct functional of the observed data distribution whenever p(Y | do(a)) is identified in the causal model represented by the input graph), and complete (explicitly flags as a failure any input p(Y | do(a)) whenever this distribution is not identified in the causal model represented by the input graph).  The reference [9] provides a result, the so called "hedge criterion" (Corollary 3), which aims to give a graphical characterization of situations when the ID algorithm fails to identify its input in terms of a structure in the input graph called the hedge. While the ID algorithm is, indeed, a sound and complete algorithm, and the hedge structure does arise whenever the input distribution is not identified, Corollary 3 presented in [9] is incorrect as sta
    
[^2]: 激励理论的贝叶斯推理在协作科学中的应用

    Incentive-Theoretic Bayesian Inference for Collaborative Science. (arXiv:2307.03748v1 [stat.ME])

    [http://arxiv.org/abs/2307.03748](http://arxiv.org/abs/2307.03748)

    本研究讨论了在协作科学中使用激励理论的贝叶斯推理方法，通过考虑研究者和决策者的不同激励机制，利用代理人的战略行为进行统计推断。

    

    当代科学研究是一项分布式的、协作的工作，由研究团队、监管机构、资助机构、商业合作伙伴和科学机构组成，彼此互动并面对不同的激励。为了保持科学严谨性，统计方法应该认识到这种情况。为此，我们研究了假设检验的情况，其中有一个代理人（例如研究人员或制药公司）对未知参数拥有私人先验知识，还有一个委托人（如政策制定者或监管机构）希望根据参数值做出决策。代理人根据他们的私人先验选择是否进行统计试验，然后试验的结果由委托人用来做出决策。我们展示了委托人如何进行统计推断，利用代理人的战略行为所透露的信息，也就是他们选择是否进行试验。具体来说，我们展示了如何计算p值，从而综合利用代理人的行为和试验的结果进行推理。

    Contemporary scientific research is a distributed, collaborative endeavor, carried out by teams of researchers, regulatory institutions, funding agencies, commercial partners, and scientific bodies, all interacting with each other and facing different incentives. To maintain scientific rigor, statistical methods should acknowledge this state of affairs. To this end, we study hypothesis testing when there is an agent (e.g., a researcher or a pharmaceutical company) with a private prior about an unknown parameter and a principal (e.g., a policymaker or regulator) who wishes to make decisions based on the parameter value. The agent chooses whether to run a statistical trial based on their private prior and then the result of the trial is used by the principal to reach a decision. We show how the principal can conduct statistical inference that leverages the information that is revealed by an agent's strategic behavior -- their choice to run a trial or not. In particular, we show how the p
    
[^3]: QIGen：用于大型语言模型的量化推理的高效内核生成

    QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models. (arXiv:2307.03738v1 [cs.LG])

    [http://arxiv.org/abs/2307.03738](http://arxiv.org/abs/2307.03738)

    QIGen是一种用于支持大型语言模型量化推理的自动代码生成方法，通过考虑目标架构和性能模型，实现了高性能和高准确性，并在LLaMA模型的基于CPU的推理任务上取得了比现有开源解决方案更好的效果。

    

    我们提出了一种新的自动代码生成方法，用于支持LLMs（如LLaMA或OPT）在现成的CPU上进行量化生成推理。我们的方法根据目标架构和性能模型进行设计，包括硬件特性和方法特定的准确性约束。在LLaMA模型的基于CPU的推理任务上的实验结果表明，我们的方法可以实现高性能和高准确性，与现有最佳开源解决方案相比更具优势。我们的初步实现代码可在https://github.com/IST-DASLab/QIGen找到。

    We present ongoing work on a new automatic code generation approach for supporting quantized generative inference on LLMs such as LLaMA or OPT on off-the-shelf CPUs. Our approach is informed by the target architecture and a performance model, including both hardware characteristics and method-specific accuracy constraints. Results on CPU-based inference for LLaMA models show that our approach can lead to high performance and high accuracy, comparing favorably to the best existing open-source solution. A preliminary implementation is available at https://github.com/IST-DASLab/QIGen.
    
[^4]: 使用激光和机器学习模型进行钢铁表面粗糙度参数计算

    Steel Surface Roughness Parameter Calculations Using Lasers and Machine Learning Models. (arXiv:2307.03723v1 [cs.LG])

    [http://arxiv.org/abs/2307.03723](http://arxiv.org/abs/2307.03723)

    本研究利用机器学习模型提高了钢铁表面粗糙度参数的计算准确性，通过对比不同方法，评估了在线测量转化的潜力。

    

    控制带钢表面纹理对于满足镀锌和轧制工艺中客户要求至关重要。传统方法依赖于生产后的测针测量，而在线技术则提供了对整个带钢进行非接触和实时测量的能力。然而，确保准确测量对于其在制造流程中的有效利用至关重要。此外，准确的在线测量使得在生产过程中能够实时调整制造加工参数，确保产品质量的一致性和轧机的闭环控制的可能性。本研究利用最先进的机器学习模型改进在线测量转化为更准确的Ra表面粗糙度指标。通过比较包括深度学习和非深度学习方法在内的一系列数据驱动方法与闭合形式转化，我们评估了它们提高表面粗糙度参数计算的潜力。

    Control of surface texture in strip steel is essential to meet customer requirements during galvanizing and temper rolling processes. Traditional methods rely on post-production stylus measurements, while on-line techniques offer non-contact and real-time measurements of the entire strip. However, ensuring accurate measurement is imperative for their effective utilization in the manufacturing pipeline. Moreover, accurate on-line measurements enable real-time adjustments of manufacturing processing parameters during production, ensuring consistent quality and the possibility of closed-loop control of the temper mill. In this study, we leverage state-of-the-art machine learning models to enhance the transformation of on-line measurements into significantly a more accurate Ra surface roughness metric. By comparing a selection of data-driven approaches, including both deep learning and non-deep learning methods, to the close-form transformation, we evaluate their potential for improving su
    
[^5]: Polybot：在接受变异性的同时训练多个机器人上的一个策略

    Polybot: Training One Policy Across Robots While Embracing Variability. (arXiv:2307.03719v1 [cs.RO])

    [http://arxiv.org/abs/2307.03719](http://arxiv.org/abs/2307.03719)

    该论文提出了一种在多个机器人平台上训练单个策略的方法，通过使用腕部摄像头和统一的代码库对观测和动作空间进行对齐，并通过对比学习来消除领域差异。实验证明该方法在不同机器人平台上具有良好的效果。

    

    由于获取机器人数据集的高成本，重用大型数据集对于将基于视觉的机器人操纵器扩展到日常情景至关重要。然而，机器人平台具有不同的控制方案、摄像机视角、运动学配置和末端执行器形态，从一个平台转移操纵技能面临着重要挑战。为了解决这个问题，我们提出一组关键设计决策，以在多个机器人平台上训练单个策略。我们的框架首先通过利用腕部摄像头和一个统一但模块化的代码库，对我们的策略在不同机体上进行观测和动作空间的对齐。为了消除剩余的领域差异，我们通过对比学习在不同机体之间对齐我们策略的内部表示。我们在一个跨6个任务和3个具有不同关节配置和尺寸的机器人的60小时数据集上评估了我们的方法：WidowX 250S，Franka Emika Panda和S。

    Reusing large datasets is crucial to scale vision-based robotic manipulators to everyday scenarios due to the high cost of collecting robotic datasets. However, robotic platforms possess varying control schemes, camera viewpoints, kinematic configurations, and end-effector morphologies, posing significant challenges when transferring manipulation skills from one platform to another. To tackle this problem, we propose a set of key design decisions to train a single policy for deployment on multiple robotic platforms. Our framework first aligns the observation and action spaces of our policy across embodiments via utilizing wrist cameras and a unified, but modular codebase. To bridge the remaining domain shift, we align our policy's internal representations across embodiments through contrastive learning. We evaluate our method on a dataset collected over 60 hours spanning 6 tasks and 3 robots with varying joint configurations and sizes: the WidowX 250S, the Franka Emika Panda, and the S
    
[^6]: SAR: 通过协同行动表示实现生理敏捷性和灵巧性的普适性

    SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation. (arXiv:2307.03716v1 [cs.RO])

    [http://arxiv.org/abs/2307.03716](http://arxiv.org/abs/2307.03716)

    本研究将生物进化中发展出的肌肉协同控制策略应用于人手和腿模型，发现通过协同行动表示（SAR）训练的策略在学习复杂任务时明显优于端到端强化学习。

    

    在高维系统中学习高效连续控制策略，包括肌肉骨骼代理，仍然是一个重大挑战。通过生物进化的过程中，生物体发展出了克服这种复杂性的鲁棒机制，学习了高度复杂的运动控制策略。是什么导致了这种鲁棒的行为灵活性呢？通过肌肉协同的模块化控制被认为是一种潜在的机制，使生物能够在简化和可推广的行动空间中学习肌肉控制。受到这种进化的运动控制策略的启发，我们使用生理准确的人手和腿模型作为测试平台，确定从简单任务中获得的协同行动表示（SAR）在学习更复杂任务中的作用程度。我们发现，在这两种情况下，利用SAR的策略明显优于端到端强化学习。通过SAR进行训练的策略优于端到端强化学习.

    Learning effective continuous control policies in high-dimensional systems, including musculoskeletal agents, remains a significant challenge. Over the course of biological evolution, organisms have developed robust mechanisms for overcoming this complexity to learn highly sophisticated strategies for motor control. What accounts for this robust behavioral flexibility? Modular control via muscle synergies, i.e. coordinated muscle co-contractions, is considered to be one putative mechanism that enables organisms to learn muscle control in a simplified and generalizable action space. Drawing inspiration from this evolved motor control strategy, we use physiologically accurate human hand and leg models as a testbed for determining the extent to which a Synergistic Action Representation (SAR) acquired from simpler tasks facilitates learning more complex tasks. We find in both cases that SAR-exploiting policies significantly outperform end-to-end reinforcement learning. Policies trained wit
    
[^7]: INT-FP-QSim: 大型语言模型和视觉转换器的混合精度和格式

    INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers. (arXiv:2307.03712v1 [cs.LG])

    [http://arxiv.org/abs/2307.03712](http://arxiv.org/abs/2307.03712)

    INT-FP-QSim是一个开源模拟器，用于评估大型语言模型和视觉转换器在不同精度和格式下的性能。通过使用不同的数值格式，我们研究了4位权重和4位或8位激活对模型性能的影响，并比较了不同方法的效果。

    

    大型语言模型的崛起导致了减少精度的运行的增加。降低精度的运行方式支持资源约束，并促进其民主化，使用户可以在个人设备上运行数十亿参数的语言模型。为了补充这一持续努力，我们提出了INT-FP-QSim：一个开源模拟器，可以灵活评估不同数值精度和格式下的语言模型和视觉转换器。INT-FP-QSim利用现有的开源库，如TensorRT、QPytorch和AIMET，实现了支持各种浮点和整数格式的组合模拟器。借助我们的模拟器，我们调查了不同数值格式对4位权重和4位或8位激活的语言模型和视觉转换器性能的影响。我们还比较了最近提出的方法，如自适应块浮点、SmoothQuant、GPTQ和RPTQ在模型性能上的差异。我们希望INT-FP-QSim能为研究人员和从业者在低精度环境下评估大型语言模型和视觉转换器的性能提供帮助。

    The recent rise of large language models (LLMs) has resulted in increased efforts towards running LLMs at reduced precision. Running LLMs at lower precision supports resource constraints and furthers their democratization, enabling users to run billion-parameter LLMs on their personal devices. To supplement this ongoing effort, we propose INT-FP-QSim: an open-source simulator that enables flexible evaluation of LLMs and vision transformers at various numerical precisions and formats. INT-FP-QSim leverages existing open-source repositories such as TensorRT, QPytorch and AIMET for a combined simulator that supports various floating point and integer formats. With the help of our simulator, we survey the impact of different numerical formats on the performance of LLMs and vision transformers at 4-bit weights and 4-bit or 8-bit activations. We also compare recently proposed methods like Adaptive Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We hope INT-FP-QSim
    
[^8]: 通过引导和限制表示的等变性单视角姿势预测

    Equivariant Single View Pose Prediction Via Induced and Restricted Representations. (arXiv:2307.03704v1 [cs.CV])

    [http://arxiv.org/abs/2307.03704](http://arxiv.org/abs/2307.03704)

    本文提出了一种算法，通过引导和限制表示的等变性约束，在二维图像中学习三维世界表示，并满足几何一致性属性。

    

    从二维图像中学习三维世界是计算机视觉中的一个基本问题。对于这样的任务，理想的神经网络架构应该利用物体可以在三维空间中旋转和平移的事实，预测关于新图像的信息。然而，在二维输入上施加SO(3)-等变性较为困难，因为三维旋转群对于二维平面没有自然的作用。具体而言，SO(3)的一个元素可能会使图像在平面外旋转。我们展示了一个从二维图像中学习三维世界表示的算法必须满足一定的几何一致性属性，我们将其表达为SO(2)-等变性约束。我们使用SO(2)在SO(3)上的引导和限制表示来构建和分类满足这些几何一致性约束的架构。我们证明了任何符合这些规则的架构都必须具备

    Learning about the three-dimensional world from two-dimensional images is a fundamental problem in computer vision. An ideal neural network architecture for such tasks would leverage the fact that objects can be rotated and translated in three dimensions to make predictions about novel images. However, imposing SO(3)-equivariance on two-dimensional inputs is difficult because the group of three-dimensional rotations does not have a natural action on the two-dimensional plane. Specifically, it is possible that an element of SO(3) will rotate an image out of plane. We show that an algorithm that learns a three-dimensional representation of the world from two dimensional images must satisfy certain geometric consistency properties which we formulate as SO(2)-equivariance constraints. We use the induced and restricted representations of SO(2) on SO(3) to construct and classify architectures which satisfy these geometric consistency constraints. We prove that any architecture which respects
    
[^9]: 可伸缩的通过分位数回归进行成员推断攻击

    Scalable Membership Inference Attacks via Quantile Regression. (arXiv:2307.03694v1 [cs.LG])

    [http://arxiv.org/abs/2307.03694](http://arxiv.org/abs/2307.03694)

    通过分位数回归进行成员推断攻击的方法，与现有的影子模型攻击相比，计算量更小但效果相当。

    

    成员推断攻击旨在通过对训练模型进行黑盒访问，确定特定示例是否在训练中使用。成员推断可以形式化为一个假设检验问题。现有最有效的攻击是通过训练许多与被攻击模型相同架构的“影子模型”（在随机数据子样本上训练）来估计某些测试统计量（通常是模型对真实标签的置信度）的分布。虽然有效，但这些攻击非常计算密集，特别是当被攻击的模型很大时。我们引入了一种新的攻击方式，通过对未在训练中使用的点上的被攻击模型产生的置信度分布进行分位数回归。我们证明了我们的方法与最先进的影子模型攻击相竞争，同时需要更少的计算量。

    Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \emph{shadow models} -i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large.  We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less comput
    
[^10]: 使用机器学习抑制动力系统中的未知干扰

    Suppressing unknown disturbances to dynamical systems using machine learning. (arXiv:2307.03690v1 [eess.SY])

    [http://arxiv.org/abs/2307.03690](http://arxiv.org/abs/2307.03690)

    本文提出了一种使用机器学习的无模型方法，可以仅通过系统在已知强迫函数影响下的观测，识别和抑制未知系统的未知干扰。这项方法对训练函数有非常温和的限制，能够稳健地识别和抑制大类别的未知干扰。

    

    识别和抑制动力系统中的未知干扰是一个在许多不同领域中应用的问题。在本文中，我们提出了一种无模型的方法，仅基于系统在已知强迫函数影响下的先前观测来识别和抑制未知系统的未知干扰。我们发现，在对训练函数有非常温和的限制下，我们的方法能够稳健地识别和抑制大类别的未知干扰。我们通过一个示例说明了我们的方案，其中识别和抑制了 Lorenz 系统的混沌干扰。

    Identifying and suppressing unknown disturbances to dynamical systems is a problem with applications in many different fields. In this Letter, we present a model-free method to identify and suppress an unknown disturbance to an unknown system based only on previous observations of the system under the influence of a known forcing function. We find that, under very mild restrictions on the training function, our method is able to robustly identify and suppress a large class of unknown disturbances. We illustrate our scheme with an example where a chaotic disturbance to the Lorenz system is identified and suppressed.
    
[^11]: 可微分湍流

    Differentiable Turbulence. (arXiv:2307.03683v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.03683](http://arxiv.org/abs/2307.03683)

    深度学习与物理启发式选择的深度学习架构相结合，实现了可微分湍流模型，在大涡模拟中有效地建模子网格尺度湍流，并能推广到多种流动配置。

    

    深度学习在改进大涡模拟(SGS)中子网格尺度湍流闭合模型的准确性方面越来越具有潜力。我们利用可微分湍流的概念，在使用物理启发式选择的深度学习架构与端到端可微分求解器相结合的情况下，学习了适用于二维湍流流动的高效且多功能的SGS模型。我们对所选择的架构中的归纳偏差进行了深入分析，发现包含小尺度的非局部特征对于有效的SGS建模最为关键，而大尺度特征可以提高事后解场的点精确度。滤波速度梯度张量可以通过将输入和输出分解为各向同性、偏离同性和反对称分量来直接映射到SGS应力。我们发现该模型可以推广到多种流动配置，包括较高和较低的速度 Reynolds 数。

    Deep learning is increasingly becoming a promising pathway to improving the accuracy of sub-grid scale (SGS) turbulence closure models for large eddy simulations (LES). We leverage the concept of differentiable turbulence, whereby an end-to-end differentiable solver is used in combination with physics-inspired choices of deep learning architectures to learn highly effective and versatile SGS models for two-dimensional turbulent flow. We perform an in-depth analysis of the inductive biases in the chosen architectures, finding that the inclusion of small-scale non-local features is most critical to effective SGS modeling, while large-scale features can improve pointwise accuracy of the a-posteriori solution field. The filtered velocity gradient tensor can be mapped directly to the SGS stress via decomposition of the inputs and outputs into isotropic, deviatoric, and anti-symmetric components. We see that the model can generalize to a variety of flow configurations, including higher and l
    
[^12]: 可信人工智能准则--AI评估目录

    Guideline for Trustworthy Artificial Intelligence -- AI Assessment Catalog. (arXiv:2307.03681v1 [cs.CY])

    [http://arxiv.org/abs/2307.03681](http://arxiv.org/abs/2307.03681)

    上述论文提出了一个可信人工智能准则--AI评估目录，旨在确保AI应用按照高质量标准开发，并有效防范新的AI风险，从而使AI发挥其潜力，减少不公平对待个体的风险。

    

    人工智能（AI）近年来取得了令人瞩目的进展，并代表着一项对经济和社会具有重要影响的关键技术。然而，很明显，只有按照高质量标准开发AI应用并有效防范新的AI风险，AI和基于其的商业模式才能充分发挥其潜力。例如，当处理个人数据（例如，支持信贷借款或员工招聘决策）时，AI存在不公平对待个体的风险。这些新风险的出现与AI应用的行为密切相关，尤其是基于机器学习（ML）的应用，其行为基本上是从大量数据中学习而来，而不是由固定编程规则预先确定的。因此，AI应用的可信度问题至关重要，是政治、商业和社会利益相关者众多重要出版物的主题。

    Artificial Intelligence (AI) has made impressive progress in recent years and represents a key technology that has a crucial impact on the economy and society. However, it is clear that AI and business models based on it can only reach their full potential if AI applications are developed according to high quality standards and are effectively protected against new AI risks. For instance, AI bears the risk of unfair treatment of individuals when processing personal data e.g., to support credit lending or staff recruitment decisions. The emergence of these new risks is closely linked to the fact that the behavior of AI applications, particularly those based on Machine Learning (ML), is essentially learned from large volumes of data and is not predetermined by fixed programmed rules.  Thus, the issue of the trustworthiness of AI applications is crucial and is the subject of numerous major publications by stakeholders in politics, business and society. In addition, there is mutual agreeme
    
[^13]: 嵌入语义边缘自动编码器中的不可降解小波变换对多语言安全改进和降噪的研究

    Undecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages. (arXiv:2307.03679v1 [cs.CL])

    [http://arxiv.org/abs/2307.03679](http://arxiv.org/abs/2307.03679)

    本研究提出了一种将不可降解小波变换与嵌入语义边缘自动编码器相结合的新策略，用于改善多语言安全措施和降噪。该系统通过提取特征并保留数据中的时间和地理链接，成功捕获重要信息，并提高了系统检测异常和区分合法内容与危险威胁的能力。

    

    通过将不可降解小波变换与嵌入语义边缘自动编码器（WESMA）相结合，本研究提出了一种改善安全措施和降噪多种语言的新策略。这些策略的整合旨在解决数据处理应用中的鲁棒性、隐私性和多语言性问题。不可降解小波变换被用作特征提取工具，以识别输入数据中突出的语言模式和结构特性。通过采用这种变换，提议的系统可以在保留数据中的时间和地理链接的同时，成功捕获重要信息。这通过增加系统检测异常、发现隐藏模式以及区分合法内容和危险威胁的能力来改善安全措施。嵌入语义边缘自动编码器还可以作为一个智能框架来降维和降噪数据。

    By combining the undecimated wavelet transform within a Word Embedded Semantic Marginal Autoencoder (WESMA), this research study provides a novel strategy for improving security measures and denoising multiple languages. The incorporation of these strategies is intended to address the issues of robustness, privacy, and multilingualism in data processing applications. The undecimated wavelet transform is used as a feature extraction tool to identify prominent language patterns and structural qualities in the input data. The proposed system may successfully capture significant information while preserving the temporal and geographical links within the data by employing this transform. This improves security measures by increasing the system's ability to detect abnormalities, discover hidden patterns, and distinguish between legitimate content and dangerous threats. The Word Embedded Semantic Marginal Autoencoder also functions as an intelligent framework for dimensionality and noise redu
    
[^14]: 评估大型语言模型在表示几何和空间关系的文本描述中的有效性

    Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations. (arXiv:2307.03678v1 [cs.CL])

    [http://arxiv.org/abs/2307.03678](http://arxiv.org/abs/2307.03678)

    本研究评估了大型语言模型在表示几何和空间关系文本描述中的有效性，发现虽然这些模型能够捕捉一些空间关系，但在估计数值和检索相关对象方面仍存在挑战。

    

    本研究旨在评估大型语言模型（LLMs）在表示几何和空间关系方面的能力。我们利用包括GPT-2和BERT在内的LLMs对几何的文本格式进行编码，然后将它们的嵌入输入分类器和回归器，以评估LLMs生成的嵌入在几何属性方面的有效性。实验表明，尽管LLMs生成的嵌入能够保留几何类型并捕捉一些空间关系（准确度高达73%），但在估计数值和检索空间相关对象方面仍存在挑战。本研究凸显了在捕捉底层地理空间数据的细微差别和复杂性以及整合领域知识以支持使用基础模型的各种GeoAI应用方面的改进需求。

    This research focuses on assessing the ability of large language models (LLMs) in representing geometries and their spatial relations. We utilize LLMs including GPT-2 and BERT to encode the well-known text (WKT) format of geometries and then feed their embeddings into classifiers and regressors to evaluate the effectiveness of the LLMs-generated embeddings for geometric attributes. The experiments demonstrate that while the LLMs-generated embeddings can preserve geometry types and capture some spatial relations (up to 73% accuracy), challenges remain in estimating numeric values and retrieving spatially related objects. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using foundation models.
    
[^15]: GeoPhy: 利用几何梯度实现可微分的系统发育推断

    GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies. (arXiv:2307.03675v1 [cs.LG])

    [http://arxiv.org/abs/2307.03675](http://arxiv.org/abs/2307.03675)

    GeoPhy是一种创新的、完全可微的系统发育推断方法，通过在连续几何空间中表示拓扑分布，实现了可扩展的变分推断，克服了不限制拓扑结构的挑战。

    

    系统发育推断是在分子进化模型基础上进行的，它对于理解生物数据中的进化关系至关重要。考虑到进化树变量的不确定性，包括树拓扑结构和分支上的进化距离，对于准确地从分子数据中推断物种关系以及需要进行变量边缘化的任务来说至关重要。变分贝叶斯方法是开发可扩展、实用模型的关键，然而，在不限制可能的树拓扑结构的组合数的情况下进行系统发育推断仍然具有挑战性。在本研究中，我们引入了一种新颖的、完全可微的系统发育推断公式，利用连续几何空间中的拓扑分布来表示。通过对设计空间和渐近矩的实际考虑，我们的方法GeoPhy可以实现变分推断而不限制拓扑结构的多样性。

    Phylogenetic inference, grounded in molecular evolution models, is essential for understanding the evolutionary relationships in biological data. Accounting for the uncertainty of phylogenetic tree variables, which include tree topologies and evolutionary distances on branches, is crucial for accurately inferring species relationships from molecular data and tasks requiring variable marginalization. Variational Bayesian methods are key to developing scalable, practical models; however, it remains challenging to conduct phylogenetic inference without restricting the combinatorially vast number of possible tree topologies. In this work, we introduce a novel, fully differentiable formulation of phylogenetic inference that leverages a unique representation of topological distributions in continuous geometric spaces. Through practical considerations on design spaces and control variates for gradient estimations, our approach, GeoPhy, enables variational inference without limiting the topolo
    
[^16]: 通过得分和流匹配实现无需模拟的Schr\"odinger桥

    Simulation-free Schr\"odinger bridges via score and flow matching. (arXiv:2307.03672v1 [cs.LG])

    [http://arxiv.org/abs/2307.03672](http://arxiv.org/abs/2307.03672)

    [SF]$^2$M是一种无需模拟的方法，用于推断随机动力学。它将连续时间随机生成建模解释为Schr\"odinger桥问题，并通过静态熵正则化最优传输来高效学习。在学习细胞动力学方面表现出更高的准确性和效率。

    

    我们提出了无需模拟的得分和流匹配（[SF]$^2$M），这是一种无需模拟的目标函数，用于推断给定来自任意分布的未配对源和目标样本的随机动力学。我们的方法推广了用于训练扩散模型的得分匹配损失和最近提出的用于训练连续归一化流的流匹配损失。[SF]$^2$M将连续时间随机生成建模解释为Schr\"odinger桥问题。它依赖于静态熵正则化最优传输或小批量近似，以有效地学习Schr\"odinger桥，而无需模拟学习过程。我们发现，与先前的基于模拟的方法相比，[SF]$^2$M更高效并提供了更准确的Schr\"odinger桥解决方案。最后，我们将[SF]$^2$M应用于从快照数据中学习细胞动力学的问题。值得注意的是，[SF]$^2$M是首个能够准确建模高维细胞动力学的方法。

    We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired source and target samples drawn from arbitrary distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schr\"odinger bridge (SB) problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can 
    
[^17]: 在线网络源优化与图内核MAB

    Online Network Source Optimization with Graph-Kernel MAB. (arXiv:2307.03641v1 [cs.LG])

    [http://arxiv.org/abs/2307.03641](http://arxiv.org/abs/2307.03641)

    提出了Grab-UCB算法用于在线学习大规模网络中最优的源放置位置，通过使用自适应的图词典模型来描述网络过程，并利用稀疏谱表示实现了数据高效的学习框架。

    

    我们提出了Grab-UCB，一种基于图内核的多臂赌博算法，用于在线学习在大规模网络中最优的源放置位置，以最大化先验未知网络过程所获得的奖励。由于不确定性，需要在线学习，然而这在维度灾难中受到了影响。为了实现样本效率，我们使用自适应的图词典模型来描述网络过程，这通常导致稀疏的谱表示。这使得数据高效的学习框架成为可能，其学习率与谱表示模型的维度相关，而不是网络的维度。然后，我们提出了Grab-UCB，一种在线顺序决策策略，它在优化行动策略的同时学习谱表示的参数。我们推导出了与网络参数相关的性能保证，这进一步影响顺序决策策略的学习曲线。

    We propose Grab-UCB, a graph-kernel multi-arms bandit algorithm to learn online the optimal source placement in large scale networks, such that the reward obtained from a priori unknown network processes is maximized. The uncertainty calls for online learning, which suffers however from the curse of dimensionality. To achieve sample efficiency, we describe the network processes with an adaptive graph dictionary model, which typically leads to sparse spectral representations. This enables a data-efficient learning framework, whose learning rate scales with the dimension of the spectral representation model instead of the one of the network. We then propose Grab-UCB, an online sequential decision strategy that learns the parameters of the spectral representation while optimizing the action strategy. We derive the performance guarantees that depend on network parameters, which further influence the learning curve of the sequential decision strategy We introduce a computationally simplifie
    
[^18]: 与神经ODE相关的连续线性参数变化系统的PAC界限

    PAC bounds of continuous Linear Parameter-Varying systems related to neural ODEs. (arXiv:2307.03630v1 [cs.LG])

    [http://arxiv.org/abs/2307.03630](http://arxiv.org/abs/2307.03630)

    本文研究了与神经ODE相关的LPV系统的PAC界限，该界限不依赖于积分区间。

    

    在连续时间中，我们考虑了学习线性参数变化（LPV）系统上的神经常微分方程（神经ODE）的问题。LPV系统包含双线性系统，已知对于非线性系统是全局逼近器。此外，大部分神经ODE可以嵌入到LPV系统中。作为我们的主要贡献，我们提供了与神经ODE相关的LPV系统稳定性下的可能近似正确（PAC）界限。所得到的界限的优点是它们不依赖于积分区间。

    We consider the problem of learning Neural Ordinary Differential Equations (neural ODEs) within the context of Linear Parameter-Varying (LPV) systems in continuous-time. LPV systems contain bilinear systems which are known to be universal approximators for non-linear systems. Moreover, a large class of neural ODEs can be embedded into LPV systems. As our main contribution we provide Probably Approximately Correct (PAC) bounds under stability for LPV systems related to neural ODEs. The resulting bounds have the advantage that they do not depend on the integration interval.
    
[^19]: GEANN: 多视角时间序列预测的可扩展图增强

    GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting. (arXiv:2307.03595v1 [cs.LG])

    [http://arxiv.org/abs/2307.03595](http://arxiv.org/abs/2307.03595)

    本研究介绍了一种使用图神经网络 (GNNs) 作为数据增强来解决多视角时间序列预测中的“冷启动”问题的方法。这些GNN-based特征能够捕捉复杂的跨序列关系，并且可以与预测任务进行端到端的优化。

    

    编码器-解码器深度神经网络在多视角时间序列预测中得到了越来越多的研究，尤其是在实际应用中。然而，为了准确预测，这些复杂模型通常依赖于大量的具有丰富历史数据的时间序列示例。一个越来越受关注的研究主题是预测缺乏足够历史数据的时间序列，通常被称为“冷启动”问题。在本文中，我们引入了一种新颖而简单的方法来解决这个问题，通过利用图神经网络 (GNNs) 作为数据增强来增强这些预测器使用的编码器。这些基于GNN的特征能够捕捉复杂的跨序列关系，并且它们的生成过程可以与预测任务进行端到端的优化。我们展示了我们的架构可以使用数据驱动或基于领域知识定义的图，能够扩展到包含数百万节点的多个非常大的图的信息。

    Encoder-decoder deep neural networks have been increasingly studied for multi-horizon time series forecasting, especially in real-world applications. However, to forecast accurately, these sophisticated models typically rely on a large number of time series examples with substantial history. A rapidly growing topic of interest is forecasting time series which lack sufficient historical data -- often referred to as the ``cold start'' problem. In this paper, we introduce a novel yet simple method to address this problem by leveraging graph neural networks (GNNs) as a data augmentation for enhancing the encoder used by such forecasters. These GNN-based features can capture complex inter-series relationships, and their generation process can be optimized end-to-end with the forecasting task. We show that our architecture can use either data-driven or domain knowledge-defined graphs, scaling to incorporate information from multiple very large graphs with millions of nodes. In our target app
    
[^20]: 线性二次调节器的加速优化景观

    Accelerated Optimization Landscape of Linear-Quadratic Regulator. (arXiv:2307.03590v1 [math.OC])

    [http://arxiv.org/abs/2307.03590](http://arxiv.org/abs/2307.03590)

    本文介绍了用于处理线性二次调节器（LQR）问题的加速优化框架，分析了SLQR和OLQR问题的收敛性。同时提出了LQR性能准则的Lipschitz Hessian特性，为现代优化技术的应用提供了重要指导。

    

    线性二次调节器（LQR）是最优控制领域的一个重要问题。本文介绍了处理LQR问题的一阶加速优化框架，并分别给出了SLQR和OLQR的收敛性分析。我们提出了LQR性能准则的Lipschitz Hessian特性，这对于应用现代优化技术来说是至关重要的。对于SLQR问题，我们引入了一个连续时间混合动态系统，并证明其解轨迹指数级收敛。

    Linear-quadratic regulator (LQR) is a landmark problem in the field of optimal control, which is the concern of this paper. Generally, LQR is classified into state-feedback LQR (SLQR) and output-feedback LQR (OLQR) based on whether the full state is obtained. It has been suggested in existing literature that both the SLQR and the OLQR could be viewed as \textit{constrained nonconvex matrix optimization} problems in which the only variable to be optimized is the feedback gain matrix. In this paper, we introduce a first-order accelerated optimization framework of handling the LQR problem, and give its convergence analysis for the cases of SLQR and OLQR, respectively.  Specifically, a Lipschiz Hessian property of LQR performance criterion is presented, which turns out to be a crucial property for the application of modern optimization techniques. For the SLQR problem, a continuous-time hybrid dynamic system is introduced, whose solution trajectory is shown to converge exponentially to the
    
[^21]: BOF-UCB: 一种用于非平稳环境下的上下界信心算法的贝叶斯优化频率算法

    BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits. (arXiv:2307.03587v1 [cs.LG])

    [http://arxiv.org/abs/2307.03587](http://arxiv.org/abs/2307.03587)

    BOF-UCB是一种用于非平稳环境下的背景线性赌博机的贝叶斯优化频率算法，其结合了贝叶斯和频率学派原则，提高了在动态环境中的性能。它利用贝叶斯更新推断后验分布，并使用频率学派方法计算上界信心界以平衡探索和开发。实验证明，BOF-UCB优于现有方法，是非平稳环境中顺序决策的有前途的解决方案。

    

    我们提出了一种新颖的贝叶斯优化频率上下界信心算法（BOF-UCB），用于非平稳环境下的随机背景线性赌博机。贝叶斯和频率学派原则的独特结合增强了算法在动态环境中的适应性和性能。BOF-UCB算法利用顺序贝叶斯更新推断未知回归参数的后验分布，并随后采用频率学派方法通过最大化后验分布上的期望收益来计算上界信心界（UCB）。我们提供了BOF-UCB性能的理论保证，并在合成数据集和强化学习环境中的经典控制任务中展示了其有效性。我们的结果表明，BOF-UCB优于现有的方法，在非平稳环境中进行顺序决策是一个有前途的解决方案。

    We propose a novel Bayesian-Optimistic Frequentist Upper Confidence Bound (BOF-UCB) algorithm for stochastic contextual linear bandits in non-stationary environments. This unique combination of Bayesian and frequentist principles enhances adaptability and performance in dynamic settings. The BOF-UCB algorithm utilizes sequential Bayesian updates to infer the posterior distribution of the unknown regression parameter, and subsequently employs a frequentist approach to compute the Upper Confidence Bound (UCB) by maximizing the expected reward over the posterior distribution. We provide theoretical guarantees of BOF-UCB's performance and demonstrate its effectiveness in balancing exploration and exploitation on synthetic datasets and classical control tasks in a reinforcement learning setting. Our results show that BOF-UCB outperforms existing methods, making it a promising solution for sequential decision-making in non-stationary environments.
    
[^22]: ContextLabeler数据集: 在野外使用的智能手机传感器数据的物理和虚拟传感器数据收集

    ContextLabeler Dataset: physical and virtual sensors data collected from smartphone usage in-the-wild. (arXiv:2307.03586v1 [cs.HC])

    [http://arxiv.org/abs/2307.03586](http://arxiv.org/abs/2307.03586)

    本研究介绍了一个数据收集活动和由智能手机传感器衍生而来的数据集，包含超过45K个样本，提供了关于用户日常活动的信息，并且采用了在野外实验的方式，消除了用户行为的偏见。

    

    本文描述了一项数据收集活动和由智能手机传感器衍生而来的数据集，该数据集表征了3名志愿者在两周内的日常生活活动。该数据集以一组CSV文件的形式发布，其中包含超过45K个数据样本，每个样本由1332个与各种物理和虚拟传感器相关的特征组成，包括动作传感器、运行应用程序、附近设备和天气条件等。此外，每个数据样本都带有一个与感知实验期间用户活动和情境相关的地面真值标签（例如，工作、在餐厅、做运动活动）。为了避免在数据收集过程中引入任何偏见，我们在野外进行了感知实验，即使用志愿者的设备，并且没有对用户行为定义任何约束。因此，收集到的数据集代表着一个有用的资源来源。

    This paper describes a data collection campaign and the resulting dataset derived from smartphone sensors characterizing the daily life activities of 3 volunteers in a period of two weeks. The dataset is released as a collection of CSV files containing more than 45K data samples, where each sample is composed by 1332 features related to a heterogeneous set of physical and virtual sensors, including motion sensors, running applications, devices in proximity, and weather conditions. Moreover, each data sample is associated with a ground truth label that describes the user activity and the situation in which she was involved during the sensing experiment (e.g., working, at restaurant, and doing sport activity). To avoid introducing any bias during the data collection, we performed the sensing experiment in-the-wild, that is, by using the volunteers' devices, and without defining any constraint related to the user's behavior. For this reason, the collected dataset represents a useful sourc
    
[^23]: 可编程合成表格数据生成

    Programmable Synthetic Tabular Data Generation. (arXiv:2307.03577v1 [cs.LG])

    [http://arxiv.org/abs/2307.03577](http://arxiv.org/abs/2307.03577)

    这项工作介绍了ProgSyn，第一个可编程的合成表格数据生成算法，它允许对生成的数据进行全面的自定义，并且通过预训练和微调生成模型来确保高质量的数据和遵守自定义规范。

    

    由于隐私、数据质量和数据共享的限制，大量的表格数据仍然被低效利用。尽管训练一个能够生成类似原始分布的合成数据的生成模型可以解决其中一些问题，但大多数应用程序还需要额外的生成数据约束。现有的合成数据方法存在局限性，因为它们通常只处理特定的约束条件，例如差分隐私（DP）或增加公平性，并且缺乏一个可访问的接口来声明一般规范。在这项工作中，我们介绍了ProgSyn，这是第一个可编程的合成表格数据生成算法，它允许对生成的数据进行全面的自定义。为了确保高质量的数据并遵守自定义规范，ProgSyn在原始数据集上进行预训练生成模型，并在提供的规范上自动推导出可微分损失进行微调。这些规范可以使用统计和。

    Large amounts of tabular data remain underutilized due to privacy, data quality, and data sharing limitations. While training a generative model producing synthetic data resembling the original distribution addresses some of these issues, most applications require additional constraints from the generated data. Existing synthetic data approaches are limited as they typically only handle specific constraints, e.g., differential privacy (DP) or increased fairness, and lack an accessible interface for declaring general specifications. In this work, we introduce ProgSyn, the first programmable synthetic tabular data generation algorithm that allows for comprehensive customization over the generated data. To ensure high data quality while adhering to custom specifications, ProgSyn pre-trains a generative model on the original dataset and fine-tunes it on a differentiable loss automatically derived from the provided specifications. These can be programmatically declared using statistical and
    
[^24]: 平滑边缘：利用Hadamard超参数化在稀疏正则化的平滑优化中的一般框架

    Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization. (arXiv:2307.03571v1 [cs.LG])

    [http://arxiv.org/abs/2307.03571](http://arxiv.org/abs/2307.03571)

    本文介绍了一种通用框架，可以在稀疏正则化中进行平滑优化，与主流的一阶优化方法兼容，并且能够得到匹配的全局最小值和等价的局部最小值。

    

    本文介绍了一种用于（结构化）稀疏正则化问题中的$\ell_q$和$\ell_{p,q}$正则化的平滑方法。这些非平滑且可能非凸的问题的优化通常依赖于专门的过程。相比之下，我们的一般框架与主流的一阶优化方法（如随机梯度下降和加速变体）兼容，无需任何修改。这是通过平滑优化转移实现的，其中选定模型参数的超参数化使用Hadamard乘积和惩罚的改变。在超参数问题中，通过用替代参数进行平滑和凸性的$\ell_2$正则化，能够在原始参数化中引入非平滑和非凸性的$\ell_q$或$\ell_{p,q}$正则化。我们证明了我们的方法不仅能够得到匹配的全局最小值，还能得到等价的局部最小值。这在非凸稀疏正则化中尤其有用，因为在这种情况下找到全局最小值非常困难。

    This paper introduces a smooth method for (structured) sparsity in $\ell_q$ and $\ell_{p,q}$ regularized optimization problems. Optimization of these non-smooth and possibly non-convex problems typically relies on specialized procedures. In contrast, our general framework is compatible with prevalent first-order optimization methods like Stochastic Gradient Descent and accelerated variants without any required modifications. This is accomplished through a smooth optimization transfer, comprising an overparametrization of selected model parameters using Hadamard products and a change of penalties. In the overparametrized problem, smooth and convex $\ell_2$ regularization of the surrogate parameters induces non-smooth and non-convex $\ell_q$ or $\ell_{p,q}$ regularization in the original parametrization. We show that our approach yields not only matching global minima but also equivalent local minima. This is particularly useful in non-convex sparse regularization, where finding global m
    
[^25]: MALIBO: 元学习应用于无似然贝叶斯优化

    MALIBO: Meta-learning for Likelihood-free Bayesian Optimization. (arXiv:2307.03565v1 [cs.LG])

    [http://arxiv.org/abs/2307.03565](http://arxiv.org/abs/2307.03565)

    MALIBO是一种元学习贝叶斯优化方法，通过直接学习跨任务的查询效用，并引入辅助模型以实现对新任务的稳健适应，克服了现有方法的可伸缩性和不确定性的限制。

    

    贝叶斯优化是一种优化昂贵黑盒函数的流行方法。传统的贝叶斯优化会从头开始优化每个新的目标任务，而元学习则是利用相关任务的知识来更快地优化新任务的一种方式。然而，现有的元学习贝叶斯优化方法依赖于标准模型，这些模型存在可伸缩性问题，并且对不同任务之间观察数据的尺度和噪声类型非常敏感。此外，它们常常忽视与任务相似性相关的不确定性，这导致在仅有有限观察数据或新任务与相关任务差异显著时，任务适应性不可靠。为了解决这些限制，我们提出了一种新颖的元学习贝叶斯优化方法，旨在绕开标准模型，直接学习跨任务的查询效用。我们的方法明确建模任务的不确定性，并引入了一个辅助模型，使其能够对新任务进行稳健适应。大量实验证明了我们方法的有效性。

    Bayesian optimization (BO) is a popular method to optimize costly black-box functions. While traditional BO optimizes each new target task from scratch, meta-learning has emerged as a way to leverage knowledge from related tasks to optimize new tasks faster. However, existing meta-learning BO methods rely on surrogate models that suffer from scalability issues and are sensitive to observations with different scales and noise types across tasks. Moreover, they often overlook the uncertainty associated with task similarity. This leads to unreliable task adaptation when only limited observations are obtained or when the new tasks differ significantly from the related tasks. To address these limitations, we propose a novel meta-learning BO approach that bypasses the surrogate model and directly learns the utility of queries across tasks. Our method explicitly models task uncertainty and includes an auxiliary model to enable robust adaptation to new tasks. Extensive experiments show that ou
    
[^26]: DWReCO在CheckThat! 2023中通过基于风格的数据采样增强客观性检测

    DWReCO at CheckThat! 2023: Enhancing Subjectivity Detection through Style-based Data Sampling. (arXiv:2307.03550v1 [cs.CL])

    [http://arxiv.org/abs/2307.03550](http://arxiv.org/abs/2307.03550)

    本文介绍了我们通过基于风格的数据采样来增强主观性检测任务的方法。我们使用GPT-3模型根据新闻记者视角的主观性检查清单生成额外的训练材料，并使用扩展的训练集微调语言特定的变形器模型。我们的实验结果表明，不同主观风格在所有语言中都是有效的，并且基于风格的过采样在土耳其语和英语中的效果优于改写方法。然而，在非英语语言中，GPT-3模型有时会生成乏味的结果。

    

    本文描述了我们在CheckThat!实验室主观性检测任务中的提交。为了解决任务中的类别不平衡问题，我们使用基于新闻记者视角的主观性检查清单，使用不同风格的提示来生成额外的训练材料，使用GPT-3模型。我们使用扩展的训练集来微调语言特定的变形器模型。我们在英语、德语和土耳其语上的实验表明，不同的主观风格在所有语言中都是有效的。此外，我们观察到，在土耳其语和英语中，基于风格的过采样比改写更好。最后，我们发现在非英语语言中，GPT-3模型有时会生成乏味的结果。

    This paper describes our submission for the subjectivity detection task at the CheckThat! Lab. To tackle class imbalances in the task, we have generated additional training materials with GPT-3 models using prompts of different styles from a subjectivity checklist based on journalistic perspective. We used the extended training set to fine-tune language-specific transformer models. Our experiments in English, German and Turkish demonstrate that different subjective styles are effective across all languages. In addition, we observe that the style-based oversampling is better than paraphrasing in Turkish and English. Lastly, the GPT-3 models sometimes produce lacklustre results when generating style-based texts in non-English languages.
    
[^27]: 使用图神经网络的罗马数字分析：从音符特征到按音预测

    Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features. (arXiv:2307.03544v1 [cs.SD])

    [http://arxiv.org/abs/2307.03544](http://arxiv.org/abs/2307.03544)

    本文提出了一种基于图神经网络的新方法，用于自动罗马数字分析。该方法可以直接处理乐谱中的每个音符，利用音符特征和音符之间的相互依赖关系，并通过新型边缩减算法产生按音的表示。在参考数据集上，ChordGNN模型表现优于现有的最先进模型，具有更高的罗马数字分析准确率。

    

    罗马数字分析是在调性音乐作品中识别和确定和弦以及其功能背景的重要任务。本文提出了一种基于符号音乐的自动罗马数字分析新方法。现有技术依赖于对乐谱的中间丢失压缩表示，我们提出了一种基于图神经网络(GNNs)的新方法，可以直接描述和处理乐谱中的每一个音符。所提出的架构可以利用音符特征和音符之间的相互依赖关系，并通过我们的新型边缩减算法产生按音的表示。我们的结果表明ChordGNN优于现有的最先进模型，在参考数据集上实现了更高的罗马数字分析准确率。此外，我们还探索了使用NADE和后处理和弦预测等技术的模型变体。本文的完整源代码可在https://github.com/mano上获取。

    Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music. This paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an intermediate lossy representation of the score, we propose a new method based on Graph Neural Networks (GNNs) that enable the direct description and processing of each individual note in the score. The proposed architecture can leverage notewise features and interdependencies between notes but yield onset-wise representation by virtue of our novel edge contraction algorithm. Our results demonstrate that ChordGNN outperforms existing state-of-the-art models, achieving higher accuracy in Roman Numeral analysis on the reference datasets. In addition, we investigate variants of our model using proposed techniques such as NADE, and post-processing of the chord predictions. The full source code for this work is available at https://github.com/mano
    
[^28]: 基于破产问题的垂直联邦学习中的激励分配

    Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem. (arXiv:2307.03515v1 [cs.LG])

    [http://arxiv.org/abs/2307.03515](http://arxiv.org/abs/2307.03515)

    本文提出了一种基于破产问题的方法来解决垂直联邦学习中激励分配的挑战，以确保公平性和稳定性。

    

    垂直联邦学习（VFL）是一种有前景的方法，用于合作训练在不同参与方之间垂直划分的私有数据的机器学习模型。在VFL设置中，理想情况下，主动方（拥有带标签样本特征的参与方）通过与某些被动方（拥有相同样本但没有标签的额外特征的参与方）合作，在保护隐私的情况下改进其机器学习模型。然而，激励被动方参与VFL可能具有挑战性。本文重点研究了基于被动方在VFL过程中的贡献来为他们分配激励的问题。我们将这个问题定义为核心游戏论概念的一种变体——破产问题，并使用塔木德划分规则来解决它。我们在合成和真实数据集上评估了我们提出的方法，并展示它确保了激励的公平性和稳定性。

    Vertical federated learning (VFL) is a promising approach for collaboratively training machine learning models using private data partitioned vertically across different parties. Ideally in a VFL setting, the active party (party possessing features of samples with labels) benefits by improving its machine learning model through collaboration with some passive parties (parties possessing additional features of the same samples without labels) in a privacy preserving manner. However, motivating passive parties to participate in VFL can be challenging. In this paper, we focus on the problem of allocating incentives to the passive parties by the active party based on their contributions to the VFL process. We formulate this problem as a variant of the Nucleolus game theory concept, known as the Bankruptcy Problem, and solve it using the Talmud's division rule. We evaluate our proposed method on synthetic and real-world datasets and show that it ensures fairness and stability in incentive a
    
[^29]: DEFT:利用模型层之间的梯度范数差异来实现可扩展的梯度稀疏化

    DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification. (arXiv:2307.03500v1 [cs.LG])

    [http://arxiv.org/abs/2307.03500](http://arxiv.org/abs/2307.03500)

    DEFT是一种利用模型层之间的梯度范数差异来实现可扩展的梯度稀疏化的方法，可以减少分布式深度学习中的通信流量，并在计算成本和梯度累积方面具有优势。

    

    梯度稀疏化是减少分布式深度学习中过多通信流量的广泛应用解决方案。然而，大多数现有的梯度稀疏化方法由于梯度选择的计算成本相当大和梯度累积增加的通信流量，其可扩展性相对较差。为了解决这些挑战，我们提出了一种新颖的梯度稀疏化方案DEFT，将梯度选择任务分解为子任务并分配给工作节点。 DEFT与现有的稀疏化方法不同，每个工作节点仅从所有梯度中选择梯度。因此，随着工作节点数量的增加，计算成本可以降低。此外，DEFT允许工作节点在非交叉的分区中选择梯度，因此即使工作节点数量增加，通信流量也可以根据用户要求进行维持。

    Gradient sparsification is a widely adopted solution for reducing the excessive communication traffic in distributed deep learning. However, most existing gradient sparsifiers have relatively poor scalability because of considerable computational cost of gradient selection and/or increased communication traffic owing to gradient build-up. To address these challenges, we propose a novel gradient sparsification scheme, DEFT, that partitions the gradient selection task into sub tasks and distributes them to workers. DEFT differs from existing sparsifiers, wherein every worker selects gradients among all gradients. Consequently, the computational cost can be reduced as the number of workers increases. Moreover, gradient build-up can be eliminated because DEFT allows workers to select gradients in partitions that are non-intersecting (between workers). Therefore, even if the number of workers increases, the communication traffic can be maintained as per user requirement.  To avoid the loss 
    
[^30]: HoughLaneNet: 使用深度Hough变换和动态卷积进行车道检测

    HoughLaneNet: Lane Detection with Deep Hough Transform and Dynamic Convolution. (arXiv:2307.03494v1 [cs.CV])

    [http://arxiv.org/abs/2307.03494](http://arxiv.org/abs/2307.03494)

    本文提出了一种使用深度Hough变换和动态卷积进行车道检测的方法，该方法利用车道的几何结构和分层特征聚合，能够有效地检测和分割复杂场景中的车道。

    

    由于车道的复杂性，车道检测任务在自动驾驶领域引起了广泛关注。车道的窄、断裂以及被交通拥堵所遮挡等特点使得车道检测困难重重。然而，观察发现车道具有几何结构，类似于一条直线，因此当利用这一特征时，车道检测结果得到了显著改善。为了解决这一挑战，我们提出了一种分层的深度Hough变换（DHT）方法，将图像中所有的车道特征组合到Hough参数空间中。此外，我们还改进了点选取方法，并引入了动态卷积模块，以有效区分原始图像中的车道。我们的网络架构包括一个主干网络，即ResNet或金字塔视觉变换器（Pyramid Vision Transformer），一种特征金字塔网络作为中间层提取多尺度特征，并使用分层的基于DHT的特征聚合头准确地进行车道分割。

    The task of lane detection has garnered considerable attention in the field of autonomous driving due to its complexity. Lanes can present difficulties for detection, as they can be narrow, fragmented, and often obscured by heavy traffic. However, it has been observed that the lanes have a geometrical structure that resembles a straight line, leading to improved lane detection results when utilizing this characteristic. To address this challenge, we propose a hierarchical Deep Hough Transform (DHT) approach that combines all lane features in an image into the Hough parameter space. Additionally, we refine the point selection method and incorporate a Dynamic Convolution Module to effectively differentiate between lanes in the original image. Our network architecture comprises a backbone network, either a ResNet or Pyramid Vision Transformer, a Feature Pyramid Network as the neck to extract multi-scale features, and a hierarchical DHT-based feature aggregation head to accurately segment 
    
[^31]: ITA:一种基于量化的Transformer的能效高的Attention和Softmax加速器

    ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers. (arXiv:2307.03493v1 [cs.AR])

    [http://arxiv.org/abs/2307.03493](http://arxiv.org/abs/2307.03493)

    ITA是一种基于量化的Transformer的能效高的Attention和Softmax加速器。通过利用8位量化和仅基于整数值的创新Softmax实现，ITA实现了高能效的推理，在16.9 TOPS/W的能效上超过了最先进的Transformer加速器，并在5.93 TOPS/mm$^2$的面积效率上超越了它们。

    

    Transformer网络已经成为自然语言处理任务的最先进方法，并在计算机视觉和音频处理等其他领域受到欢迎。然而，Transformer模型的高算术强度、大内存需求和复杂数据流依赖导致了其有效硬件加速面临新的挑战。在这项工作中，我们提出了ITA，一种针对嵌入式系统上高效推理的Transformer和相关模型的新型加速器架构，通过利用8位量化和仅基于整数值的创新Softmax实现。通过在流模式下实时计算，我们的Softmax实现最大程度地减少了数据移动和能量消耗。ITA在能效方面与最先进的Transformer加速器保持竞争力，达到了16.9 TOPS/W，同时在面积效率方面以5.93 TOPS/mm$^2$的成绩超越了它们，在22纳米完全耗尽的硅上。

    Transformer networks have emerged as the state-of-the-art approach for natural language processing tasks and are gaining popularity in other domains such as computer vision and audio processing. However, the efficient hardware acceleration of transformer models poses new challenges due to their high arithmetic intensities, large memory requirements, and complex dataflow dependencies. In this work, we propose ITA, a novel accelerator architecture for transformers and related models that targets efficient inference on embedded systems by exploiting 8-bit quantization and an innovative softmax implementation that operates exclusively on integer values. By computing on-the-fly in streaming mode, our softmax implementation minimizes data movement and energy consumption. ITA achieves competitive energy efficiency with respect to state-of-the-art transformer accelerators with 16.9 TOPS/W, while outperforming them in area efficiency with 5.93 TOPS/mm$^2$ in 22 nm fully-depleted silicon-on-insu
    
[^32]: 分布回归的神经网络学习理论

    Learning Theory of Distribution Regression with Neural Networks. (arXiv:2307.03487v1 [stat.ML])

    [http://arxiv.org/abs/2307.03487](http://arxiv.org/abs/2307.03487)

    本文提出了一个全连接神经网络框架用于分布回归问题，解决了传统神经网络无法直接处理概率分布输入的困难，并建立了逼近理论和学习理论。

    

    本文旨在通过全连接神经网络（FNN）建立分布回归的逼近理论和学习理论。与传统回归方法不同的是，分布回归的输入变量是概率测度。然后我们常常需要进行二阶段抽样过程来近似分布的实际信息。另一方面，传统的神经网络结构要求输入变量为向量。当输入样本是概率分布时，传统的深度神经网络方法无法直接使用，分布回归问题变得困难。因此，一个对于分布输入进行明确定义的神经网络结构是非常需求的。关于分布回归的神经网络实现没有数学模型和理论分析。为了克服技术难题并解决这个问题，我们建立了一个新颖的全连接神经网络框架。

    In this paper, we aim at establishing an approximation theory and a learning theory of distribution regression via a fully connected neural network (FNN). In contrast to the classical regression methods, the input variables of distribution regression are probability measures. Then we often need to perform a second-stage sampling process to approximate the actual information of the distribution. On the other hand, the classical neural network structure requires the input variable to be a vector. When the input samples are probability distributions, the traditional deep neural network method cannot be directly used and the difficulty arises for distribution regression. A well-defined neural network structure for distribution inputs is intensively desirable. There is no mathematical model and theoretical analysis on neural network realization of distribution regression. To overcome technical difficulties and address this issue, we establish a novel fully connected neural network framework
    
[^33]: 通过对比学习在强化学习中发现层次化成就

    Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])

    [http://arxiv.org/abs/2307.03486](http://arxiv.org/abs/2307.03486)

    通过对比学习方法，我们在强化学习中提出了新的成就蒸馏方法，可以加强代理对下一个解锁成就的预测能力，并优于先前的模型驱动和层次化方法。

    

    在生成环境中发现具有层次结构的成就是一个重大挑战。这需要智能体具备广泛的能力，包括泛化和长期推理。许多先前的方法基于模型驱动或层次化方法，认为显式的长期规划模块对于学习层次化成就是有益的。然而，这些方法需要大量的环境交互或大型模型，限制了它们的实用性。在这项工作中，我们发现近期实施实践中的近端策略优化（PPO）算法优于先前的方法。此外，我们发现PPO智能体可以在一定程度上预测下一个要解锁的成就，尽管预测的置信度较低。基于这一观察，我们提出了一种新颖的对比学习方法，称为成就蒸馏，可以加强PPO智能体对下一个解锁成就的预测能力。

    Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the 
    
[^34]: 无配对多视图图聚类与跨视图结构匹配

    Unpaired Multi-View Graph Clustering with Cross-View Structure Matching. (arXiv:2307.03476v1 [cs.LG])

    [http://arxiv.org/abs/2307.03476](http://arxiv.org/abs/2307.03476)

    无配对多视图图聚类方法结合跨视图结构匹配，解决了多视图数据未配对问题，提高了聚类性能。

    

    多视图聚类方法(MVC)通过有效地融合多个视图的信息以提高性能而受到越来越多的关注。大多数现有的MVC方法假设多视图数据是完全配对的，这意味着视图之间所有对应样本的映射是预先定义或提前给定的。然而，实际应用中的数据对应常常不完整，这是由于数据损坏或传感器差异引起的多视图文献中的数据未配对问题(DUP)。尽管已经尝试解决DUP问题，但存在以下问题：1）大多数方法关注特征表示而忽视了多视图数据的结构信息，而这对于聚类任务至关重要；2）现有的部分未配对问题的方法依赖于预先给定的跨视图对齐信息，导致它们无法处理完全未配对的问题；3）它们不可避免地存在参数降低了效率。

    Multi-view clustering (MVC), which effectively fuses information from multiple views for better performance, has received increasing attention. Most existing MVC methods assume that multi-view data are fully paired, which means that the mappings of all corresponding samples between views are pre-defined or given in advance. However, the data correspondence is often incomplete in real-world applications due to data corruption or sensor differences, referred as the data-unpaired problem (DUP) in multi-view literature. Although several attempts have been made to address the DUP issue, they suffer from the following drawbacks: 1) Most methods focus on the feature representation while ignoring the structural information of multi-view data, which is essential for clustering tasks; 2) Existing methods for partially unpaired problems rely on pre-given cross-view alignment information, resulting in their inability to handle fully unpaired problems; 3) Their inevitable parameters degrade the eff
    
[^35]: 在持续观察下的聚类问题中的差分隐私

    Differential Privacy for Clustering Under Continual Observation. (arXiv:2307.03430v1 [cs.DS])

    [http://arxiv.org/abs/2307.03430](http://arxiv.org/abs/2307.03430)

    本文提出了一种在持续观察下的差分隐私聚类机制，用于在被删除和插入数据点的数据集中进行聚类，这是第一个具有仅以更新次数的对数依赖性的增加误差的近似算法，并且乘法误差几乎与非隐私情况相同。

    

    我们考虑在$\mathbb{R}^d$中进行隐私聚类的问题，该问题在数据集中插入和删除点。具体来说，我们提供了一个在持续观察下的$\varepsilon$-差分隐私聚类机制，用于 $k$-means 目标。这是该问题的第一个近似算法，其增加的误差仅以更新次数 $T$ 的对数依赖性。乘法误差与非隐私情况几乎相同。为此，我们展示了如何在持续观察中进行维度缩减，并将其与用于 $k$-means 的差分隐私贪心逼近算法相结合。我们还部分地将我们的结果推广到 $k$-median 问题上。

    We consider the problem of clustering privately a dataset in $\mathbb{R}^d$ that undergoes both insertion and deletion of points. Specifically, we give an $\varepsilon$-differentially private clustering mechanism for the $k$-means objective under continual observation. This is the first approximation algorithm for that problem with an additive error that depends only logarithmically in the number $T$ of updates. The multiplicative error is almost the same as non privately. To do so we show how to perform dimension reduction under continual observation and combine it with a differentially private greedy approximation algorithm for $k$-means. We also partially extend our results to the $k$-median problem.
    
[^36]: 合并-分流混合Transformer网络用于头颈癌的生存预测

    Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer. (arXiv:2307.03427v1 [eess.IV])

    [http://arxiv.org/abs/2307.03427](http://arxiv.org/abs/2307.03427)

    这篇论文提出了一种合并-分流混合Transformer网络用于头颈癌的生存预测。该网络利用多模态影像和提取特定区域信息，通过合并编码器和分流解码器实现生存预测，取得了很好的效果。

    

    生存预测对于癌症患者至关重要，因为它为治疗计划提供了早期预后信息。最近，基于深度学习和医学影像的深度生存模型在生存预测方面表现出了很大的潜力。然而，现有的深度生存模型在利用多模态影像（如PET-CT）和提取特定区域信息（如原发肿瘤（PT）和转移淋巴结（MLN）区域的预后信息）方面尚未得到很好的发展。鉴于此，我们提出了一个用于从多模态影像进行生存预测的合并-分流学习框架。该框架具有一个合并编码器用于融合多模态信息，以及一个分流解码器用于提取特定区域的信息。在合并编码器中，我们提出了一种混合并行交叉注意力（HPCA）块，通过并行卷积层和交叉注意力Transformer有效地融合多模态特征。在分流解码器中，我们提出了一种区域

    Survival prediction is crucial for cancer patients as it provides early prognostic information for treatment planning. Recently, deep survival models based on deep learning and medical images have shown promising performance for survival prediction. However, existing deep survival models are not well developed in utilizing multi-modality images (e.g., PET-CT) and in extracting region-specific information (e.g., the prognostic information in Primary Tumor (PT) and Metastatic Lymph Node (MLN) regions). In view of this, we propose a merging-diverging learning framework for survival prediction from multi-modality images. This framework has a merging encoder to fuse multi-modality information and a diverging decoder to extract region-specific information. In the merging encoder, we propose a Hybrid Parallel Cross-Attention (HPCA) block to effectively fuse multi-modality features via parallel convolutional layers and cross-attention transformers. In the diverging decoder, we propose a Region
    
[^37]: 使用条件去噪扩散概率模型进行高光谱和多光谱图像融合

    Hyperspectral and Multispectral Image Fusion Using the Conditional Denoising Diffusion Probabilistic Model. (arXiv:2307.03423v1 [eess.IV])

    [http://arxiv.org/abs/2307.03423](http://arxiv.org/abs/2307.03423)

    本文提出了一种基于条件去噪扩散概率模型的高光谱和多光谱图像融合方法，通过前向扩散过程和逆向降噪过程实现对高空间分辨率HSI的去噪和复原。

    

    高光谱图像（HSI）具有大量反映物质特性的光谱信息，但由于成像技术的限制，其空间分辨率较低。与此相补充的是多光谱图像（MSI），如RGB图像，具有高空间分辨率但光谱波段不足。高光谱和多光谱图像融合是一种以成本效益的方式获取既具有高空间分辨率又具有高光谱分辨率的理想图像的技术。许多现有的HSI和MSI融合算法依赖于已知的成像退化模型，但这些模型在实践中通常不可用。本文提出了一种基于条件去噪扩散概率模型的深度融合方法，称为DDPM-Fus。具体而言，DDPM-Fus包含了逐步向高空间分辨率HSI（HrHSI）添加高斯噪声的前向扩散过程和从噪声版本中学习预测所需HrHSI的逆向降噪过程。

    Hyperspectral images (HSI) have a large amount of spectral information reflecting the characteristics of matter, while their spatial resolution is low due to the limitations of imaging technology. Complementary to this are multispectral images (MSI), e.g., RGB images, with high spatial resolution but insufficient spectral bands. Hyperspectral and multispectral image fusion is a technique for acquiring ideal images that have both high spatial and high spectral resolution cost-effectively. Many existing HSI and MSI fusion algorithms rely on known imaging degradation models, which are often not available in practice. In this paper, we propose a deep fusion method based on the conditional denoising diffusion probabilistic model, called DDPM-Fus. Specifically, the DDPM-Fus contains the forward diffusion process which gradually adds Gaussian noise to the high spatial resolution HSI (HrHSI) and another reverse denoising process which learns to predict the desired HrHSI from its noisy version 
    
[^38]: 从异质性中学习：用于超图的动态学习框架

    Learning from Heterogeneity: A Dynamic Learning Framework for Hypergraphs. (arXiv:2307.03411v1 [cs.LG])

    [http://arxiv.org/abs/2307.03411](http://arxiv.org/abs/2307.03411)

    本文提出了一个名为LFH的超图学习框架，能够动态构建超边并利用异质属性进行嵌入更新。实验结果表明，该框架在多个数据集上取得了良好的效果。

    

    图神经网络（GNN）因其在建模复杂图结构数据方面的能力和灵活性而在近年来日益受到关注。在所有图学习方法中，超图学习是一种在训练图的嵌入空间时探索隐含的高阶关联的技术。在本文中，我们提出了一个名为LFH的超图学习框架，能够利用图的异质属性进行动态超边构建和关注性嵌入更新。具体来说，在我们的框架中，首先通过利用显式的图结构信息生成高质量的特征向量。然后通过隐式超边的动态分组来构建超图，并进行类型特定的超图学习过程。为了评估我们提出的框架的有效性，我们在几个流行的数据集上进行了全面的实验。

    Graph neural network (GNN) has gained increasing popularity in recent years owing to its capability and flexibility in modeling complex graph structure data. Among all graph learning methods, hypergraph learning is a technique for exploring the implicit higher-order correlations when training the embedding space of the graph. In this paper, we propose a hypergraph learning framework named LFH that is capable of dynamic hyperedge construction and attentive embedding update utilizing the heterogeneity attributes of the graph. Specifically, in our framework, the high-quality features are first generated by the pairwise fusion strategy that utilizes explicit graph structure information when generating initial node embedding. Afterwards, a hypergraph is constructed through the dynamic grouping of implicit hyperedges, followed by the type-specific hypergraph learning process. To evaluate the effectiveness of our proposed framework, we conduct comprehensive experiments on several popular data
    
[^39]: 可扩展高维多变量线性回归用于特征分布式数据翻译标题

    Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data. (arXiv:2307.03410v1 [stat.ML])

    [http://arxiv.org/abs/2307.03410](http://arxiv.org/abs/2307.03410)

    这篇论文提出了一个适用于特征分布式数据的可扩展高维多变量线性回归算法，具有通信复杂度不依赖于特征维度和快速收敛性的优势，可应用于大规模数据集和具有多变量响应变量的场景。

    

    特征分布式数据是指根据特征划分并存储在多个计算节点上的数据，在具有大量特征的应用中越来越常见。本文提出了一个适用于这种数据的两阶段放松贪婪算法 (TSRGA)，用于应用多变量线性回归。TSRGA 的主要优势在于其通信复杂度不依赖于特征维度，使其能够高度扩展到非常大的数据集。此外，对于多变量响应变量，TSRGA 可用于产生低秩系数估计。通过模拟实验证明了TSRGA 的快速收敛性。最后，我们将提出的TSRGA 应用于一种金融应用中，利用来自 10-K 报告的非结构化数据，证明了其在具有许多密集大维矩阵的应用中的实用性。

    Feature-distributed data, referred to data partitioned by features and stored across multiple computing nodes, are increasingly common in applications with a large number of features. This paper proposes a two-stage relaxed greedy algorithm (TSRGA) for applying multivariate linear regression to such data. The main advantage of TSRGA is that its communication complexity does not depend on the feature dimension, making it highly scalable to very large data sets. In addition, for multivariate response variables, TSRGA can be used to yield low-rank coefficient estimates. The fast convergence of TSRGA is validated by simulation experiments. Finally, we apply the proposed TSRGA in a financial application that leverages unstructured data from the 10-K reports, demonstrating its usefulness in applications with many dense large-dimensional matrices.
    
[^40]: 使用目标条件预测编码作为离线强化学习的隐式规划器

    Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning. (arXiv:2307.03406v1 [cs.LG])

    [http://arxiv.org/abs/2307.03406](http://arxiv.org/abs/2307.03406)

    本论文研究了将决策制定视为离线收集的轨迹的监督学习问题，并探究了序列建模在轨迹压缩和策略学习中的作用。通过引入目标条件预测编码（GCPC），本文提出了一种能够产生强大轨迹表示并实现高性能策略的方法。

    

    最近的研究已经证明了将决策制定视为离线收集的轨迹的监督学习问题的有效性。然而，在轨迹数据上进行序列建模的好处尚不清楚。在这项工作中，我们研究了序列建模是否具备将轨迹压缩为有用表示并对策略学习有所贡献的能力。为了实现这一目标，我们采用了一个两阶段的框架，首先使用序列建模技术总结轨迹，然后利用这些表示学习策略以及一个期望的目标。这个设计使得许多现有的监督式离线强化学习方法可以被看作是我们框架的特例。在这个框架内，我们引入了目标条件预测编码（GCPC），这是一种带来强大轨迹表示并导致高性能策略的方法。我们在AntMaze，FrankaKitchen和Locomotion环境上进行了广泛的实证评估，并观察到...

    Recent work has demonstrated the effectiveness of formulating decision making as a supervised learning problem on offline-collected trajectories. However, the benefits of performing sequence modeling on trajectory data is not yet clear. In this work we investigate if sequence modeling has the capability to condense trajectories into useful representations that can contribute to policy learning. To achieve this, we adopt a two-stage framework that first summarizes trajectories with sequence modeling techniques, and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, and obser
    
[^41]: 探索大规模语言模型（LLMs）在图学习中的潜力

    Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])

    [http://arxiv.org/abs/2307.03393](http://arxiv.org/abs/2307.03393)

    本文探索了大规模语言模型（LLMs）在图学习中的潜力，并尝试了两种不同的流程：将LLMs作为增强器通过海量知识来增强节点的文本属性，并使用图神经网络（GNNs）生成预测，以及直接使用LLMs作为独立的预测器。

    

    图学习因其广泛的现实世界应用而引起了极大的关注。以文本节点属性为主的图学习最流行的流程主要依赖于图神经网络（GNN），并利用浅层文本嵌入作为初始节点表示，但存在通用知识和深刻语义理解方面的限制。近年来，大规模语言模型（LLMs）被证明具有广泛的常识和强大的语义理解能力，已经颠覆了现有的处理文本数据的工作流程。在本文中，我们旨在探索LLMs在图机器学习中的潜力，特别是节点分类任务，并研究两种可能的流程：LLMs作为增强器和LLMs作为预测器。前者利用LLMs通过其海量知识增强节点的文本属性，然后通过GNNs生成预测。后者试图直接使用LLMs作为独立的预测器。

    Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
    
[^42]: AI-UPV在EXIST 2023中使用大型语言模型在“学习与分歧”的框架下对性别歧视进行表征

    AI-UPV at EXIST 2023 -- Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime. (arXiv:2307.03385v1 [cs.CL])

    [http://arxiv.org/abs/2307.03385](http://arxiv.org/abs/2307.03385)

    以学习与分歧的机制为框架，使用大型语言模型进行性别歧视识别和表征的研究，以推动更具包容性和尊重性的在线环境。

    

    随着社交媒体平台的不断影响力增加，开发能够检测性别歧视和其他不尊重和仇恨行为的自动化系统，以促进更具包容性和尊重性的在线环境变得至关重要。然而，考虑到不同的仇恨类别和作者的意图，尤其是在学习与分歧的机制下，这些任务相当具有挑战性。本文描述了AI-UPV团队在CLEF 2023的EXIST（社交网络中的性别歧视识别）实验室中的参与情况。所提出的方法旨在通过直接从具有分歧的数据中进行训练，而不使用任何聚合标签，来处理性别歧视识别和表征的任务。同时，报告了考虑软性和硬性评估的性能。所提出的系统使用大型语言模型（如mBERT和XLM-RoBERTa）和集成策略来进行性别歧视识别和表征。

    With the increasing influence of social media platforms, it has become crucial to develop automated systems capable of detecting instances of sexism and other disrespectful and hateful behaviors to promote a more inclusive and respectful online environment. Nevertheless, these tasks are considerably challenging considering different hate categories and the author's intentions, especially under the learning with disagreements regime. This paper describes AI-UPV team's participation in the EXIST (sEXism Identification in Social neTworks) Lab at CLEF 2023. The proposed approach aims at addressing the task of sexism identification and characterization under the learning with disagreements paradigm by training directly from the data with disagreements, without using any aggregated label. Yet, performances considering both soft and hard evaluations are reported. The proposed system uses large language models (i.e., mBERT and XLM-RoBERTa) and ensemble strategies for sexism identification and 
    
[^43]: 向小型Transformer模型教授算术

    Teaching Arithmetic to Small Transformers. (arXiv:2307.03381v1 [cs.LG])

    [http://arxiv.org/abs/2307.03381](http://arxiv.org/abs/2307.03381)

    本研究研究了如何通过训练小型Transformer模型在没有先验训练的情况下高效学习基本算术运算，并提出了一种通过格式变化和使用链式思维样式数据来提高准确性的方法。

    

    大型语言模型如GPT-4，当在大量文本数据上进行训练时，即使这些任务并未直接编码在无监督的下一个标记预测目标中，也展现出了在通用任务（如基本算术）上的新兴能力。本研究调查了如何让从随机初始化训练的小型transformers模型，通过下一个标记预测目标高效学习加法、乘法和诸如平方根等基本算术运算。我们首先证明传统训练数据对于算术学习来说并不是最有效的，通过简单的格式变化可以显著提高准确性。这导致了训练数据规模的尖锐相变，其中一些情况可以通过与低秩矩阵补全的联系来解释。在此基础上，我们在包括中间步骤结果的链式思维样式数据上进行训练。即使完全无先验训练，模型仍然可以学习算术运算。

    Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, 
    
[^44]: 关于形式特征归因及其近似方法

    On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v1 [cs.AI])

    [http://arxiv.org/abs/2307.03380](http://arxiv.org/abs/2307.03380)

    这篇论文研究了解释性人工智能（XAI）中的形式特征归因方法及其近似方法。现有的特征选择和归因方法存在一些问题，而形式化的XAI方法虽然是一个有希望的解决方案，但仍存在一些限制。

    

    近年来，人工智能（AI）算法和机器学习（ML）模型得到了广泛应用。尽管取得了巨大成功，但ML模型脆弱性，公平性以及解释性的缺乏等重要问题需要积极发展可解释的人工智能（XAI）和形式化的ML模型验证。XAI的两个主要研究方向包括特征选择方法（例如，Anchors）和特征归因技术（例如，LIME和SHAP）。尽管有希望，但大多数现有的特征选择和归因方法都容易出现一系列关键问题，包括解释不正确和超出分布采样。近期一种形式化的XAI方法（FXAI）虽然作为以上方法的替代品并避免了这些问题，但仍存在一些限制。例如，除了可扩展性限制外，这种形式化方法无法解决特征归因问题。

    Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution prob
    
[^45]: 缓解任务感知对性别歧视、仇恨言论和有害语言检测的负面迁移问题

    Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and Toxic Language Detection. (arXiv:2307.03377v1 [cs.CL])

    [http://arxiv.org/abs/2307.03377](http://arxiv.org/abs/2307.03377)

    本文提出了一种基于任务感知的方法，用于解决性别歧视、仇恨言论和有害语言检测中的负面迁移问题，并能够减少负面迁移并提高性能。

    

    本文提出了一种新颖的方法来缓解负面迁移问题。在机器学习领域，通常的策略是采用单任务学习方法，训练一个监督模型来解决特定的任务。训练一个强大的模型需要大量的数据和大量的计算资源，这使得在数据不可用或收集成本高的情况下，这种解决方案不可行。因此，另一种基于任务之间信息共享的解决方案已经被开发出来：多任务学习（MTL）。尽管在MTL方面已经有了一些最新的进展，负面迁移问题仍然需要解决。负面迁移是一种现象，当噪声信息在任务之间共享时，会导致性能下降。本文提出了一种基于任务感知概念的新方法来缓解负面迁移问题。所提出的方法能够减少负面迁移，并提高性能。

    This paper proposes a novelty approach to mitigate the negative transfer problem. In the field of machine learning, the common strategy is to apply the Single-Task Learning approach in order to train a supervised model to solve a specific task. Training a robust model requires a lot of data and a significant amount of computational resources, making this solution unfeasible in cases where data are unavailable or expensive to gather. Therefore another solution, based on the sharing of information between tasks, has been developed: Multi-Task Learning (MTL). Despite the recent developments regarding MTL, the problem of negative transfer has still to be solved. Negative transfer is a phenomenon that occurs when noisy information is shared between tasks, resulting in a drop in performance. This paper proposes a new approach to mitigate the negative transfer problem based on the task awareness concept. The proposed approach results in diminishing the negative transfer together with an impro
    
[^46]: STG-MTL: 使用数据映射的可伸缩任务分组多任务学习

    STG-MTL: Scalable Task Grouping for Multi-Task Learning Using Data Map. (arXiv:2307.03374v1 [cs.LG])

    [http://arxiv.org/abs/2307.03374](http://arxiv.org/abs/2307.03374)

    本文提出了一种数据驱动方法，使用数据映射来解决多任务学习中的任务分组问题。这种方法具有可伸缩性和模块化，并且在大量任务下仍然有效。

    

    多任务学习是一种强大的技术，因其相对于传统的单任务学习而言具有性能改进而受到欢迎。然而，多任务学习往往具有挑战性，因为可能的任务分组数量呈指数增长，这使得选择最佳任务分组变得困难，并且一些分组可能会由于任务之间的负面干扰而导致性能下降。此外，现有解决方案严重受到可伸缩性问题的困扰，限制了实际应用。在本文中，我们提出了一种新的数据驱动方法，解决了这些挑战，并提供了一种基于手工特征，特别是数据映射的分类任务分组的可伸缩和模块化解决方案。我们的实验证明了该方法的有效性，即使在前所未有的任务数量下（高达100个）。

    Multi-Task Learning (MTL) is a powerful technique that has gained popularity due to its performance improvement over traditional Single-Task Learning (STL). However, MTL is often challenging because there is an exponential number of possible task groupings, which can make it difficult to choose the best one, and some groupings might produce performance degradation due to negative interference between tasks. Furthermore, existing solutions are severely suffering from scalability issues, limiting any practical application. In our paper, we propose a new data-driven method that addresses these challenges and provides a scalable and modular solution for classification task grouping based on hand-crafted features, specifically Data Maps, which capture the training behavior for each classification task during the MTL training. We experiment with the method demonstrating its effectiveness, even on an unprecedented number of tasks (up to 100).
    
[^47]: 蒸馏修剪：使用合成数据赢得彩票的方法

    Distilled Pruning: Using Synthetic Data to Win the Lottery. (arXiv:2307.03364v1 [cs.LG])

    [http://arxiv.org/abs/2307.03364](http://arxiv.org/abs/2307.03364)

    该论文介绍了一种使用蒸馏数据来修剪深度学习模型的新方法，能够比传统方法更快地找到稀疏的可训练子网络，具有资源高效的神经网络修剪潜力。

    

    该论文介绍了一种通过使用蒸馏数据来修剪深度学习模型的新方法。与传统策略主要关注体系结构或算法优化不同，我们的方法重新考虑了数据在这些场景中的作用。蒸馏数据集捕捉了更大数据集中的重要模式，并且我们展示了如何利用这种能力来实现计算效率高的修剪过程。我们的方法可以在CIFAR-10上比迭代幅值修剪更快地找到稀疏的可训练子网络（也称为彩票票）。实验结果突显了使用蒸馏数据进行资源高效的神经网络修剪、模型压缩和神经网络架构搜索的潜力。

    This work introduces a novel approach to pruning deep learning models by using distilled data. Unlike conventional strategies which primarily focus on architectural or algorithmic optimization, our method reconsiders the role of data in these scenarios. Distilled datasets capture essential patterns from larger datasets, and we demonstrate how to leverage this capability to enable a computationally efficient pruning process. Our approach can find sparse, trainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative Magnitude Pruning at comparable sparsity on CIFAR-10. The experimental results highlight the potential of using distilled data for resource-efficient neural network pruning, model compression, and neural architecture search.
    
[^48]: 通过主动遗忘实现联邦遗忘

    Federated Unlearning via Active Forgetting. (arXiv:2307.03363v1 [cs.LG])

    [http://arxiv.org/abs/2307.03363](http://arxiv.org/abs/2307.03363)

    本文提出了一种基于增量学习的新型联邦遗忘框架，解决了现有联邦遗忘方法在时间效率、数据影响估计不精确和计算负荷大等方面的问题。

    

    对机器学习模型隐私的关注日益增加，引发了对机器遗忘的探索，即一种消除训练数据对机器学习模型影响的过程。这种关注也出现在联邦学习的领域，促使研究人员解决联邦遗忘问题。然而，联邦遗忘仍然具有挑战性。现有的遗忘方法可以被广泛分为两种方法，即精确遗忘和近似遗忘。首先，在分布式情况下实施精确遗忘，通常依赖于分区-聚合框架，理论上不会提高时间效率。其次，现有的联邦（近似）遗忘方法在数据影响估计不精确、计算负荷大或两者都存在方面存在问题。为此，我们提出了一种基于增量学习的新型联邦遗忘框架，该框架不依赖于具体的模型和联邦设置。

    The increasing concerns regarding the privacy of machine learning models have catalyzed the exploration of machine unlearning, i.e., a process that removes the influence of training data on machine learning models. This concern also arises in the realm of federated learning, prompting researchers to address the federated unlearning problem. However, federated unlearning remains challenging. Existing unlearning methods can be broadly categorized into two approaches, i.e., exact unlearning and approximate unlearning. Firstly, implementing exact unlearning, which typically relies on the partition-aggregation framework, in a distributed manner does not improve time efficiency theoretically. Secondly, existing federated (approximate) unlearning methods suffer from imprecise data influence estimation, significant computational burden, or both. To this end, we propose a novel federated unlearning framework based on incremental learning, which is independent of specific models and federated se
    
[^49]: 在交叉问答背景下评估语言模型中的偏见态度关联

    Evaluating Biased Attitude Associations of Language Models in an Intersectional Context. (arXiv:2307.03360v1 [cs.CY])

    [http://arxiv.org/abs/2307.03360](http://arxiv.org/abs/2307.03360)

    这篇论文以已建立的文献为基础，量化了英语语言模型中社会群体的情绪关联，并发现语言模型对性别认同、社会阶级和性取向的信号表现出最大的偏见态度。

    

    语言模型是在大规模语料库上训练的，这些语料库中嵌入了心理学中已经记录的隐含偏见。社会群体的情绪关联（愉快/不愉快）决定了社会认知中对群体和概念的偏见态度。在此基础上，我们通过提供一个交叉问答背景的句子模板，量化了英语语言模型中社会群体的情绪关联。我们研究了与年龄、教育、性别、身高、智力、文化素养、种族、宗教、性别、性取向、社会阶级和体重有关的偏见。我们采用概念投影方法通过语言模型的上下文化词向量捕捉情绪关联的子空间。将基于投影的方法调整为量化偏见的嵌入关联测试，我们发现语言模型对性别认同、社会阶级和性取向的信号表现出最大的偏见态度。我们发现最大和表现最好的模型是...

    Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English language models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. Adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. We find that the largest and better-performing model
    
[^50]: CSCLog: 一种考虑组件子序列相关性的日志异常检测方法

    CSCLog: A Component Subsequence Correlation-Aware Log Anomaly Detection Method. (arXiv:2307.03359v1 [cs.LG])

    [http://arxiv.org/abs/2307.03359](http://arxiv.org/abs/2307.03359)

    CSCLog是一种考虑组件子序列相关性的日志异常检测方法，通过捕获子序列中的顺序依赖关系和建模子序列的隐式相关性来检测日志中的异常。

    

    基于系统日志的异常检测在智能运营中起着重要作用，由于日志模式的极端复杂性，这是一个具有挑战性的任务。现有的方法通过捕获日志序列中的顺序依赖关系来检测异常，但忽略了子序列的相互作用。为了解决这个问题，我们提出了一种考虑组件子序列相关性的日志异常检测方法：CSCLog。该方法不仅捕获子序列中的顺序依赖关系，还对子序列的隐式相关性进行建模。具体而言，基于组件从日志序列中提取子序列，并通过长短期记忆网络（LSTM）来捕获子序列中的顺序依赖关系。引入隐式相关性编码器来自适应地建模子序列的隐式相关性。此外，采用图卷积网络（GCN）来实现子序列的信息交互。最后，利用注意机制来融合信息。

    Anomaly detection based on system logs plays an important role in intelligent operations, which is a challenging task due to the extremely complex log patterns. Existing methods detect anomalies by capturing the sequential dependencies in log sequences, which ignore the interactions of subsequences. To this end, we propose CSCLog, a Component Subsequence Correlation-Aware Log anomaly detection method, which not only captures the sequential dependencies in subsequences, but also models the implicit correlations of subsequences. Specifically, subsequences are extracted from log sequences based on components and the sequential dependencies in subsequences are captured by Long Short-Term Memory Networks (LSTMs). An implicit correlation encoder is introduced to model the implicit correlations of subsequences adaptively. In addition, Graph Convolution Networks (GCNs) are employed to accomplish the information interactions of subsequences. Finally, attention mechanisms are exploited to fuse t
    
[^51]: 随机组合梯度下降算法的稳定性和泛化

    Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms. (arXiv:2307.03357v1 [cs.LG])

    [http://arxiv.org/abs/2307.03357](http://arxiv.org/abs/2307.03357)

    本文通过统计学习理论的算法稳定性，分析了随机组合梯度下降算法的稳定性和泛化性，引入了组合一致稳定性概念并与SCO问题的泛化性建立了定量关系。

    

    许多机器学习任务可以被形式化为随机组合优化（SCO）问题，例如强化学习、AUC最大化和元学习，其中目标函数涉及与期望相关的嵌套组合。虽然已经有大量研究致力于研究SCO算法的收敛行为，但对于它们的泛化性能如何，即从训练示例构建的学习算法在未来的测试示例上的行为如何，却很少有研究。在本文中，我们通过统计学习理论框架下的算法稳定性，提供了随机组合梯度下降算法的稳定性和泛化性分析。首先，我们引入了一种稳定性概念，称为组合一致稳定性，并建立了它与SCO问题的泛化性之间的定量关系。然后，我们为两种流行的随机组合优化问题建立了组合一致稳定性结果。

    Many machine learning tasks can be formulated as a stochastic compositional optimization (SCO) problem such as reinforcement learning, AUC maximization, and meta-learning, where the objective function involves a nested composition associated with an expectation. While a significant amount of studies has been devoted to studying the convergence behavior of SCO algorithms, there is little work on understanding their generalization, i.e., how these learning algorithms built from training examples would behave on future test examples. In this paper, we provide the stability and generalization analysis of stochastic compositional gradient descent algorithms through the lens of algorithmic stability in the framework of statistical learning theory. Firstly, we introduce a stability concept called compositional uniform stability and establish its quantitative relation with generalization for SCO problems. Then, we establish the compositional uniform stability results for two popular stochastic
    
[^52]: 跨领域时间序列数据的模型压缩中蒸馏出通用和联合知识

    Distilling Universal and Joint Knowledge for Cross-Domain Model Compression on Time Series Data. (arXiv:2307.03347v1 [cs.LG])

    [http://arxiv.org/abs/2307.03347](http://arxiv.org/abs/2307.03347)

    这篇论文介绍了一种用于跨领域模型压缩的新的框架，通过蒸馏教师模型中的通用特征级知识和双领域共享的联合logit级知识，实现了在资源有限环境中部署深度学习模型的难题。

    

    在许多实际的时间序列任务中，流行的深度学习模型的计算复杂性常常阻碍了在资源有限的环境（例如智能手机上）的部署。而且，由于模型训练（源）和部署（目标）阶段之间不可避免的领域漂移，在跨领域场景下压缩这些深度模型变得更加具有挑战性。尽管已经存在一些现有的工作探索了用于模型压缩的跨领域知识蒸馏，但它们要么偏向于源数据，要么在源数据和目标数据之间交织在一起。为此，我们设计了一个新的端到端框架，称为通用和联合知识蒸馏（UNI-KD）用于跨领域模型压缩。具体而言，我们提出通过对抗学习方案将教师模型的通用特征级知识和双领域共享的联合logit级知识传输到学生模型中。具体来说，一个特征级别的蒸馏网络被用来在源领域和目标领域之间转移通用特征知识，一个对抗学习模块被用来将联合logit级知识传递给学生模型。

    For many real-world time series tasks, the computational complexity of prevalent deep leaning models often hinders the deployment on resource-limited environments (e.g., smartphones). Moreover, due to the inevitable domain shift between model training (source) and deploying (target) stages, compressing those deep models under cross-domain scenarios becomes more challenging. Although some of existing works have already explored cross-domain knowledge distillation for model compression, they are either biased to source data or heavily tangled between source and target data. To this end, we design a novel end-to-end framework called Universal and joint knowledge distillation (UNI-KD) for cross-domain model compression. In particular, we propose to transfer both the universal feature-level knowledge across source and target domains and the joint logit-level knowledge shared by both domains from the teacher to the student model via an adversarial learning scheme. More specifically, a featur
    
[^53]: 使用自监督学习对多模态时间序列数据进行个性化再发性压力事件预测

    Personalized Prediction of Recurrent Stress Events Using Self-Supervised Learning on Multimodal Time-Series Data. (arXiv:2307.03337v1 [cs.LG])

    [http://arxiv.org/abs/2307.03337](http://arxiv.org/abs/2307.03337)

    本研究提出了一种使用可穿戴生物信号数据的多模态个性化压力预测系统，通过自监督学习在每个受试者的数据上进行预训练，然后进行压力预测任务微调。实验证明我们的方法可以实现使用少量标注来个性化预测压力情况。

    

    慢性压力会极大地影响身体和心理健康。可穿戴技术的出现允许跟踪生理信号，潜在地导致创新的压力预测和干预方法。然而，标签稀缺和数据异质性等挑战使得在实践中很难进行压力预测。为了应对这些问题，我们开发了一种使用可穿戴生物信号数据的多模态个性化压力预测系统。我们采用自监督学习（SSL）在每个受试者的数据上进行预训练，使模型在微调压力预测任务之前学习参与者生物信号的基线动态。我们在可穿戴压力和情感检测（WESAD）数据集上对模型进行了测试，结果显示我们的SSL模型在使用少于5%的标注情况下优于非SSL模型。这些结果表明我们的方法可以通过最少的注释实现对每个用户的个性化压力预测。

    Chronic stress can significantly affect physical and mental health. The advent of wearable technology allows for the tracking of physiological signals, potentially leading to innovative stress prediction and intervention methods. However, challenges such as label scarcity and data heterogeneity render stress prediction difficult in practice. To counter these issues, we have developed a multimodal personalized stress prediction system using wearable biosignal data. We employ self-supervised learning (SSL) to pre-train the models on each subject's data, allowing the models to learn the baseline dynamics of the participant's biosignals prior to fine-tuning the stress prediction task. We test our model on the Wearable Stress and Affect Detection (WESAD) dataset, demonstrating that our SSL models outperform non-SSL models while utilizing less than 5% of the annotations. These results suggest that our approach can personalize stress prediction to each user with minimal annotations. This para
    
[^54]: 具有编码数据结构的变分量子回归算法

    Variational quantum regression algorithm with encoded data structure. (arXiv:2307.03334v1 [quant-ph])

    [http://arxiv.org/abs/2307.03334](http://arxiv.org/abs/2307.03334)

    本文介绍了一个具有编码数据结构的变分量子回归算法，在量子机器学习中具有模型解释性，并能有效地处理互连度较高的量子比特。算法通过压缩编码和数字-模拟门操作，大大提高了在噪声中尺度量子计算机上的运行时间复杂度。

    

    变分量子算法(VQAs)被广泛应用于解决实际问题，如组合优化、量子化学模拟、量子机器学习和噪声量子计算机上的量子错误纠正。对于变分量子机器学习，尚未开发出将模型解释性内嵌到算法中的变分算法。本文构建了一个量子回归算法，并确定了变分参数与学习回归系数之间的直接关系，同时采用了将数据直接编码为反映经典数据表结构的量子幅度的电路。该算法特别适用于互连度较高的量子比特。通过压缩编码和数字-模拟门操作，运行时间复杂度在数据输入量编码的情况下对数级更有优势，显著提升了噪声中尺度量子计算机的性能。

    Variational quantum algorithms (VQAs) prevail to solve practical problems such as combinatorial optimization, quantum chemistry simulation, quantum machine learning, and quantum error correction on noisy quantum computers. For variational quantum machine learning, a variational algorithm with model interpretability built into the algorithm is yet to be exploited. In this paper, we construct a quantum regression algorithm and identify the direct relation of variational parameters to learned regression coefficients, while employing a circuit that directly encodes the data in quantum amplitudes reflecting the structure of the classical data table. The algorithm is particularly suitable for well-connected qubits. With compressed encoding and digital-analog gate operation, the run time complexity is logarithmically more advantageous than that for digital 2-local gate native hardware with the number of data entries encoded, a decent improvement in noisy intermediate-scale quantum computers a
    
[^55]: ACDNet：基于注意力引导的协同决策网络用于有效的药物推荐

    ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation. (arXiv:2307.03332v1 [cs.LG])

    [http://arxiv.org/abs/2307.03332](http://arxiv.org/abs/2307.03332)

    本文提出了一种基于注意力引导的协同决策网络（ACDNet）用于药物推荐，通过利用注意力机制和Transformer对患者的健康状况和药物记录进行建模，同时采用协同决策框架，从患者药物记录和具体药物之间的相似性出发，有效地个性化推荐药物。

    

    使用电子健康记录（EHR）进行药物推荐是具有挑战性的，因为涉及复杂的医疗数据。当前的方法从患者EHR中提取纵向信息以个性化推荐。然而，现有模型常常缺乏足够的患者表示，并忽视了考虑患者药物记录和具体药物之间相似性的重要性。因此，本文提出了一种基于注意力引导的协同决策网络（ACDNet）用于药物推荐。具体而言，ACDNet利用注意力机制和Transformer在全局和局部层面对患者的健康状况和药物记录进行建模。ACDNet还采用协同决策框架，利用药物记录与药物表示之间的相似性来促进推荐过程。在两个大型医疗数据集MIMIC-III和MIMIC-IV上的实验证明了ACDNet的有效性。

    Medication recommendation using Electronic Health Records (EHR) is challenging due to complex medical data. Current approaches extract longitudinal information from patient EHR to personalize recommendations. However, existing models often lack sufficient patient representation and overlook the importance of considering the similarity between a patient's medication records and specific medicines. Therefore, an Attention-guided Collaborative Decision Network (ACDNet) for medication recommendation is proposed in this paper. Specifically, ACDNet utilizes attention mechanism and Transformer to effectively capture patient health conditions and medication records by modeling their historical visits at both global and local levels. ACDNet also employs a collaborative decision framework, utilizing the similarity between medication records and medicine representation to facilitate the recommendation process. The experimental results on two extensive medical datasets, MIMIC-III and MIMIC-IV, cle
    
[^56]: 用于自监督预训练和下游数字天线阵列带宽回归的编码器-解码器网络

    Encoder-Decoder Networks for Self-Supervised Pretraining and Downstream Signal Bandwidth Regression on Digital Antenna Arrays. (arXiv:2307.03327v1 [cs.LG])

    [http://arxiv.org/abs/2307.03327](http://arxiv.org/abs/2307.03327)

    本研究首次将自监督学习应用于数字天线阵列数据，通过编码器-解码器网络自监督预训练和少量标记数据迁移训练，实现了优于随机初始化等效网络的数字阵列数据带宽回归任务。

    

    本研究首次将自监督学习应用于数字天线阵列数据，并采用编码器-解码器网络对数字阵列数据进行预训练，执行一种称为信道修复的自监督噪声重构任务，即网络推断被零掩盖的阵列数据的内容。自监督步骤不需要人工标记数据。然后，将预训练的编码器架构和权重转移到具有特定任务解码器的新网络，并在少量标记数据上对新网络进行训练。我们证明，对未标记数据进行预训练使得新网络在数字阵列数据的带宽回归任务中比从随机初始化训练的等效网络表现更好。

    This work presents the first applications of self-supervised learning applied to data from digital antenna arrays. Encoder-decoder networks are pretrained on digital array data to perform a self-supervised noisy-reconstruction task called channel in-painting, in which the network infers the contents of array data that has been masked with zeros. The self-supervised step requires no human-labeled data. The encoder architecture and weights from pretraining are then transferred to a new network with a task-specific decoder, and the new network is trained on a small volume of labeled data. We show that pretraining on the unlabeled data allows the new network to perform the task of bandwidth regression on the digital array data better than an equivalent network that is trained on the same labeled data from random initialization.
    
[^57]: 用于检测网络攻击和区分电力系统干扰类型的机器学习算法

    Machine Learning to detect cyber-attacks and discriminating the types of power system disturbances. (arXiv:2307.03323v1 [cs.LG])

    [http://arxiv.org/abs/2307.03323](http://arxiv.org/abs/2307.03323)

    本研究提出了一种基于机器学习的攻击检测模型，能够使用数据和日志来学习电力系统行为，在智能电网中有效地识别潜在的安全边界，并且使用随机森林模型能够达到90.56%的准确率，有助于操作员决策。

    

    本研究提出了一种基于机器学习的攻击检测模型，针对智能电网的电力系统。通过利用从相量测量设备（PMUs）收集的数据和日志，该模型旨在学习系统行为并有效地识别潜在的安全边界。所提出的方法包括数据集预处理、特征选择、模型创建和评估等关键阶段。为了验证我们的方法，我们使用了一个数据集，包括来自不同PMUs的15个独立数据集，继电器嗅探器警报和日志。构建和评估了三个机器学习模型：随机森林、逻辑回归和K-最近邻。研究结果表明，随机森林模型在检测电力系统干扰方面取得了最高的性能，在决策过程中有助于操作员进行决策。

    This research proposes a machine learning-based attack detection model for power systems, specifically targeting smart grids. By utilizing data and logs collected from Phasor Measuring Devices (PMUs), the model aims to learn system behaviors and effectively identify potential security boundaries. The proposed approach involves crucial stages including dataset pre-processing, feature selection, model creation, and evaluation. To validate our approach, we used a dataset used, consist of 15 separate datasets obtained from different PMUs, relay snort alarms and logs. Three machine learning models: Random Forest, Logistic Regression, and K-Nearest Neighbour were built and evaluated using various performance metrics. The findings indicate that the Random Forest model achieves the highest performance with an accuracy of 90.56% in detecting power system disturbances and has the potential in assisting operators in decision-making processes.
    
[^58]: 通过解缠的潜在表示辅助临床决策以获取可用治疗

    Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation. (arXiv:2307.03315v1 [cs.LG])

    [http://arxiv.org/abs/2307.03315](http://arxiv.org/abs/2307.03315)

    通过解决体外膜肺氧合(ECMO)治疗选择偏差和稀缺治疗案例的挑战，我们提出了一种名为Treatment Variational AutoEncoder (TVAE)的方法，用于个体化治疗分析，以支持临床决策。

    

    体外膜肺氧合(ECMO)是对于COVID-19患者的重要的生命支持方式，这些患者对传统治疗方法无效。然而，对于这种稀缺且技术复杂的治疗选择，适当的治疗决策一直备受争议，对于谁会从中受益仍然存在争议。为了支持临床决策，预测治疗需求和潜在的治疗与非治疗反应是至关重要的。针对这一临床挑战，我们提出了Treatment Variational AutoEncoder (TVAE)，一种用于个体化治疗分析的新方法。TVAE专门设计来解决像ECMO这样具有强烈治疗选择偏差和治疗案例稀缺的建模挑战。TVAE将治疗决策视为一个多尺度问题。我们将患者的潜在治疗分配以及事实和反事实结果作为他们固有特征的一部分进行建模，这些特征可以被表征，

    Extracorporeal membrane oxygenation (ECMO) is an essential life-supporting modality for COVID-19 patients who are refractory to conventional therapies. However, the proper treatment decision has been the subject of significant debate and it remains controversial about who benefits from this scarcely available and technically complex treatment option. To support clinical decisions, it is a critical need to predict the treatment need and the potential treatment and no-treatment responses. Targeting this clinical challenge, we propose Treatment Variational AutoEncoder (TVAE), a novel approach for individualized treatment analysis. TVAE is specifically designed to address the modeling challenges like ECMO with strong treatment selection bias and scarce treatment cases. TVAE conceptualizes the treatment decision as a multi-scale problem. We model a patient's potential treatment assignment and the factual and counterfactual outcomes as part of their intrinsic characteristics that can be repr
    
[^59]: 关于球面谐波表示的不变性、等变性、相关性和卷积的标量和矢量数据论文

    On Invariance, Equivariance, Correlation and Convolution of Spherical Harmonic Representations for Scalar and Vectorial Data. (arXiv:2307.03311v1 [cs.LG])

    [http://arxiv.org/abs/2307.03311](http://arxiv.org/abs/2307.03311)

    这篇论文介绍了球面谐波表示的理论基础和实际应用，包括旋转不变和等变特征，以及球面上信号的卷积和精确相关性。此外，还将这些方法推广到了矢量谐波表示。

    

    最近，球面谐波（SH）域中的数据数学表示方法在机器学习界重新引起了越来越多的关注。本技术报告对SH表示的理论基础和实际实现进行了深入介绍，总结了关于旋转不变和等变特征以及球面上信号的卷积和精确相关性的工作。此外，这些方法从标量SH表示扩展到矢量谐波（VH），为球面上的3D矢量场提供相同的功能

    The mathematical representations of data in the Spherical Harmonic (SH) domain has recently regained increasing interest in the machine learning community. This technical report gives an in-depth introduction to the theoretical foundation and practical implementation of SH representations, summarizing works on rotation invariant and equivariant features, as well as convolutions and exact correlations of signals on spheres. In extension, these methods are then generalized from scalar SH representations to Vectorial Harmonics (VH), providing the same capabilities for 3d vector fields on spheres
    
[^60]: 当公平分类遇到嘈杂的保护属性

    When Fair Classification Meets Noisy Protected Attributes. (arXiv:2307.03306v1 [cs.LG])

    [http://arxiv.org/abs/2307.03306](http://arxiv.org/abs/2307.03306)

    这项研究是对公平分类算法进行的一次首次的头对头比较，研究了依赖属性、容忍噪声和盲目属性的算法在预测性和公平性方面的表现，结果显示盲目属性和容忍噪声的公平分类器具有潜力。

    

    算法公平性的实施面临着许多实际挑战，其中之一就是数据集中受保护属性的可用性或可靠性。在现实世界的环境中，实际和法律上的障碍可能会阻止收集和使用人口统计数据，使得确保算法公平性变得困难。尽管最初的公平算法没有考虑这些限制，但最近的提议旨在通过将受保护属性的嘈杂性纳入考虑或根本不使用受保护属性来实现分类的算法公平性。据我们所知，这是首次对比基于属性、容忍噪声和盲目属性的公平分类算法在预测性和公平性这两个方面进行头对头研究。我们通过对四个真实数据集和合成扰动的案例研究评估了这些算法。我们的研究表明，盲目属性和容忍噪声的公平分类器可能会在预测性和公平性的双重轴上有潜力。

    The operationalization of algorithmic fairness comes with several practical challenges, not the least of which is the availability or reliability of protected attributes in datasets. In real-world contexts, practical and legal impediments may prevent the collection and use of demographic data, making it difficult to ensure algorithmic fairness. While initial fairness algorithms did not consider these limitations, recent proposals aim to achieve algorithmic fairness in classification by incorporating noisiness in protected attributes or not using protected attributes at all.  To the best of our knowledge, this is the first head-to-head study of fair classification algorithms to compare attribute-reliant, noise-tolerant and attribute-blind algorithms along the dual axes of predictivity and fairness. We evaluated these algorithms via case studies on four real-world datasets and synthetic perturbations. Our study reveals that attribute-blind and noise-tolerant fair classifiers can potentia
    
[^61]: 使用前softmax分数的归属方法的一个漏洞

    A Vulnerability of Attribution Methods Using Pre-Softmax Scores. (arXiv:2307.03305v1 [cs.LG])

    [http://arxiv.org/abs/2307.03305](http://arxiv.org/abs/2307.03305)

    这篇论文讨论了使用前softmax分数的归属方法的一个漏洞，该方法用于解释卷积神经网络分类器输出。与对抗性攻击不同，作者关注的是对归属方法进行小修改可能导致的影响，而不会改变模型的输出。

    

    我们讨论了一类用于解释卷积神经网络分类器输出的归属方法的一个漏洞。已知这种类型的网络容易受到对抗性攻击的影响，即输入的微小扰动可能会改变模型的输出。与此不同的是，我们关注的是对归属方法进行小修改可能导致的影响，而不会改变模型的输出。

    We discuss a vulnerability involving a category of attribution methods used to provide explanations for the outputs of convolutional neural networks working as classifiers. It is known that this type of networks are vulnerable to adversarial attacks, in which imperceptible perturbations of the input may alter the outputs of the model. In contrast, here we focus on effects that small modifications in the model may cause on the attribution method without altering the model outputs.
    
[^62]: 数据高效和高性能医学图像处理的等变球卷积神经网络

    Equivariant Spherical CNN for Data Efficient and High-Performance Medical Image Processing. (arXiv:2307.03298v1 [eess.IV])

    [http://arxiv.org/abs/2307.03298](http://arxiv.org/abs/2307.03298)

    等变球卷积神经网络能够提高医学图像处理的效率和性能，通过降低对特定训练集的依赖性，并且在去噪和重建方面表现出卓越的质量和计算效率。

    

    本研究突出了等变网络作为高效和高性能途径在断层扫描应用中的重要性。我们的研究建立在卷积神经网络（CNN）的局限性之上，CNN已经在各种医学影像系统的后处理中显示出了潜力。然而，传统CNN的效率严重依赖于一个不变和适当的训练集。为了解决这个问题，在本研究中，我们引入了一个等变网络，旨在减少CNN对特定训练集的依赖性。我们评估了等变CNN在球信号上在断层扫描医学成像问题中的有效性。我们的结果表明，球形CNN（SCNN）在去噪和重建基准问题上具有优越的质量和计算效率。此外，我们提出了一种新颖的方法，利用SCNN作为传统图像重建工具的补充，增强结果同时减少对训练集的依赖性。在所有案例中，我们观察到...

    This work highlights the significance of equivariant networks as efficient and high-performance approaches for tomography applications. Our study builds upon the limitations of Convolutional Neural Networks (CNNs), which have shown promise in post-processing various medical imaging systems. However, the efficiency of conventional CNNs heavily relies on an undiminished and proper training set. To tackle this issue, in this study, we introduce an equivariant network, aiming to reduce CNN's dependency on specific training sets. We evaluate the efficacy of equivariant CNNs on spherical signals for tomographic medical imaging problems. Our results demonstrate superior quality and computational efficiency of spherical CNNs (SCNNs) in denoising and reconstructing benchmark problems. Furthermore, we propose a novel approach to employ SCNNs as a complement to conventional image reconstruction tools, enhancing the outcomes while reducing reliance on the training set. Across all cases, we observe
    
[^63]: OmniBoost: 在多个DNN工作负载下增加异构嵌入式设备的吞吐量

    OmniBoost: Boosting Throughput of Heterogeneous Embedded Devices under Multi-DNN Workload. (arXiv:2307.03290v1 [cs.LG])

    [http://arxiv.org/abs/2307.03290](http://arxiv.org/abs/2307.03290)

    本文提出了OmniBoost，一种轻量级、可扩展的异构嵌入式设备多DNN管理器。通过利用随机空间探索和高精度性能估计器，相比其他先进方法，OmniBoost平均提高了4.6倍的吞吐量。

    

    现代深度神经网络(DNNs)具有显著的效率和准确性属性。这引入了由多个DNN应用程序组成的应用负载，提出了关于负载分配的新的挑战。新的嵌入式系统具备了多样化的加速器，但当前的运行时控制器无法充分利用这种异构性能。为了实现多DNN工作负载的高吞吐量，这样的控制器必须探索数十万种可能的解决方案来利用底层的异构性能。在本文中，我们提出了轻量级且可扩展的多DNN管理器OmniBoost，利用随机空间探索和高精度性能估计器，与其他先进方法相比，平均吞吐量提高了4.6倍。评估是在HiKey970开发板上进行的。

    Modern Deep Neural Networks (DNNs) exhibit profound efficiency and accuracy properties. This has introduced application workloads that comprise of multiple DNN applications, raising new challenges regarding workload distribution. Equipped with a diverse set of accelerators, newer embedded system present architectural heterogeneity, which current run-time controllers are unable to fully utilize. To enable high throughput in multi-DNN workloads, such a controller is ought to explore hundreds of thousands of possible solutions to exploit the underlying heterogeneity. In this paper, we propose OmniBoost, a lightweight and extensible multi-DNN manager for heterogeneous embedded devices. We leverage stochastic space exploration and we combine it with a highly accurate performance estimator to observe a x4.6 average throughput boost compared to other state-of-the-art methods. The evaluation was performed on the HiKey970 development board.
    
[^64]: 用于亚线性超体积遗憾度量的最优标量化方法

    Optimal Scalarizations for Sublinear Hypervolume Regret. (arXiv:2307.03288v1 [cs.LG])

    [http://arxiv.org/abs/2307.03288](http://arxiv.org/abs/2307.03288)

    研究了用于亚线性超体积遗憾度量的最优标量化方法，证明了具有均匀随机权重的超体积标量化方法在最小化超体积遗憾方面是最优的，并在多目标随机线性赌博机问题上进行了案例研究。

    

    标量化是一种通用的技术，可以应用于任何多目标设置中，将多个目标减少为一个，例如最近在RLHF中用于训练校准人类偏好的奖励模型。然而，一些人对这种经典方法持否定态度，因为已知线性标量化会忽略帕累托前沿的凹区域。为此，我们旨在找到简单的非线性标量化方法，以通过被支配的超体积来探索帕累托前沿上的多样化目标集。我们证明，具有均匀随机权重的超体积标量化令人惊讶地是为了证明最小化超体积遗憾而最优的，实现了 $O(T^{-1/k})$ 的最优亚线性遗憾界，同时匹配的下界表明在渐近情况下没有任何算法能做得更好。作为一个理论案例研究，我们考虑了多目标随机线性赌博机问题，并展示了通过利用超线性遗憾界的超体积标量化方法，

    Scalarization is a general technique that can be deployed in any multiobjective setting to reduce multiple objectives into one, such as recently in RLHF for training reward models that align human preferences. Yet some have dismissed this classical approach because linear scalarizations are known to miss concave regions of the Pareto frontier. To that end, we aim to find simple non-linear scalarizations that can explore a diverse set of $k$ objectives on the Pareto frontier, as measured by the dominated hypervolume. We show that hypervolume scalarizations with uniformly random weights are surprisingly optimal for provably minimizing the hypervolume regret, achieving an optimal sublinear regret bound of $O(T^{-1/k})$, with matching lower bounds that preclude any algorithm from doing better asymptotically. As a theoretical case study, we consider the multiobjective stochastic linear bandits problem and demonstrate that by exploiting the sublinear regret bounds of the hypervolume scalariz
    
[^65]: 语音和动力学同步的全面多尺度方法在虚拟说话头生成中的应用

    A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation. (arXiv:2307.03270v1 [cs.GR])

    [http://arxiv.org/abs/2307.03270](http://arxiv.org/abs/2307.03270)

    该论文提出了一种多尺度方法，通过使用多尺度音视同步损失和多尺度自回归生成对抗网络，实现了语音和头部动力学的同步生成。实验证明，在当前状态下取得了显著的改进。

    

    使用深度生成模型使用语音输入信号对静态面部图像进行动画化是一个活跃的研究课题，并且近期取得了重要的进展。然而，目前很大一部分工作都集中在嘴唇同步和渲染质量上，很少关注自然头部运动的生成，更不用说头部运动与语音的视听相关性了。本文提出了一种多尺度音视同步损失和多尺度自回归生成对抗网络，以更好地处理语音与头部和嘴唇动力学之间的短期和长期相关性。特别地，我们在多模态输入金字塔上训练了一堆同步模型，并将这些模型用作多尺度生成网络中的指导，以产生不同时间尺度上的音频对齐运动展开。我们的生成器在面部标志域中操作，这是一种标准的低维头部表示方法。实验证明，在头部运动动力学方面取得了显著的改进。

    Animating still face images with deep generative models using a speech input signal is an active research topic and has seen important recent progress. However, much of the effort has been put into lip syncing and rendering quality while the generation of natural head motion, let alone the audio-visual correlation between head motion and speech, has often been neglected. In this work, we propose a multi-scale audio-visual synchrony loss and a multi-scale autoregressive GAN to better handle short and long-term correlation between speech and the dynamics of the head and lips. In particular, we train a stack of syncer models on multimodal input pyramids and use these models as guidance in a multi-scale generator network to produce audio-aligned motion unfolding over diverse time scales. Our generator operates in the facial landmark domain, which is a standard low-dimensional head representation. The experiments show significant improvements over the state of the art in head motion dynamic
    
[^66]: 前列腺成像中分割基础模型的实证分析

    Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging. (arXiv:2307.03266v1 [eess.IV])

    [http://arxiv.org/abs/2307.03266](http://arxiv.org/abs/2307.03266)

    本文通过对前列腺成像中的新型基础模型UniverSeg进行了实证评估研究，并将其与传统方法进行了比较。结果表明基础模型可能是医学成像领域未来的方向。

    

    大多数医学图像分割的最先进技术依赖于深度学习模型。然而，这些模型通常在狭义任务上以监督的方式进行训练，需要昂贵的标记数据集。最近，在自然语言生成等多个机器学习领域取得的进展已经证明了构建基础模型的可行性和实用性，这些模型可以在几乎没有标记数据的情况下为各种不同的下游任务定制。这可能代表了医学成像的范式转变，我们预计基础模型可能塑造该领域的未来。本文考虑了一个最近开发的应用于医学图像分割的基础模型UniverSeg。我们在前列腺成像的背景下进行了经验评估研究，并将其与传统的训练任务特定分割模型的方法进行了比较。我们的结果和讨论突出了几个重要因素，这些因素在基础模型开发中可能非常重要。

    Most state-of-the-art techniques for medical image segmentation rely on deep-learning models. These models, however, are often trained on narrowly-defined tasks in a supervised fashion, which requires expensive labeled datasets. Recent advances in several machine learning domains, such as natural language generation have demonstrated the feasibility and utility of building foundation models that can be customized for various downstream tasks with little to no labeled data. This likely represents a paradigm shift for medical imaging, where we expect that foundation models may shape the future of the field. In this paper, we consider a recently developed foundation model for medical image segmentation, UniverSeg. We conduct an empirical evaluation study in the context of prostate imaging and compare it against the conventional approach of training a task-specific segmentation model. Our results and discussion highlight several important factors that will likely be important in the develo
    
[^67]: 视觉语言转换器：一项调查

    Vision Language Transformers: A Survey. (arXiv:2307.03254v1 [cs.CV])

    [http://arxiv.org/abs/2307.03254](http://arxiv.org/abs/2307.03254)

    视觉语言转换器是将预训练的transformer架构应用于视觉语言建模的研究领域，通过迁移学习，在同时进行视觉和语言任务中取得了显著改进。

    

    视觉语言任务，如回答关于图像的问题或生成描述图像的标题，是计算机难以完成的任务。最近的研究将预训练的transformer架构应用于视觉语言建模。相比以前的视觉语言模型，transformer模型在性能和多功能性方面有很大提高。它们通过在大型通用数据集上进行预训练，并在架构和参数值上进行微小改变后，将学习转移到新任务中。这种迁移学习已成为自然语言处理和计算机视觉中的标准建模实践。视觉语言转换器承诺在需要同时进行视觉和语言的任务中产生类似的进展。本文对目前可用的视觉语言转换器模型的研究进行了广泛综合，并对其优势进行了分析。

    Vision language tasks, such as answering questions about or generating captions that describe an image, are difficult tasks for computers to perform. A relatively recent body of research has adapted the pretrained transformer architecture introduced in \citet{vaswani2017attention} to vision language modeling. Transformer models have greatly improved performance and versatility over previous vision language models. They do so by pretraining models on a large generic datasets and transferring their learning to new tasks with minor changes in architecture and parameter values. This type of transfer learning has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks which require both vision and language. In this paper, we provide a broad synthesis of the currently available research on vision language transformer models and offer some analysis of their strength
    
[^68]: 神经网络场理论：非高斯性，作用量和局域性

    Neural Network Field Theories: Non-Gaussianity, Actions, and Locality. (arXiv:2307.03223v1 [hep-th])

    [http://arxiv.org/abs/2307.03223](http://arxiv.org/abs/2307.03223)

    本文研究了神经网络场理论，包括非高斯性、作用量和局域性。通过对网络参数的统计独立性进行微小破坏，可以得到相互作用理论，这种展开方法相比于常用的$1/N$展开在普适逼近定理方面表现更好，同时通过关联函数可以系统地重建作用量。

    

    场论中的路径积分测度和神经网络的集合描述的是函数分布。当中心极限定理适用于无限宽度（无限$N$）极限时，网络的集合对应于自由场理论。虽然在$1/N$的展开中对应于场论中的相互作用，但其他展开，如网络参数统计独立性的微小破坏，也可以导致相互作用理论。这些其他的展开可以比$1/N$展开更具优势，例如在普适逼近定理方面表现更好。通过给定场论的关联函数，可以使用一种新的费曼图规则，顶点为关联函数，系统地按照展开参数逐阶重建作用量。这种方法受到了Edgeworth展开的启发，可以为神经网络场理论导出作用量。

    Both the path integral measure in field theory and ensembles of neural networks describe distributions over functions. When the central limit theorem can be applied in the infinite-width (infinite-$N$) limit, the ensemble of networks corresponds to a free field theory. Although an expansion in $1/N$ corresponds to interactions in the field theory, others, such as in a small breaking of the statistical independence of network parameters, can also lead to interacting theories. These other expansions can be advantageous over the $1/N$-expansion, for example by improved behavior with respect to the universal approximation theorem. Given the connected correlators of a field theory, one can systematically reconstruct the action order-by-order in the expansion parameter, using a new Feynman diagram prescription whose vertices are the connected correlators. This method is motivated by the Edgeworth expansion and allows one to derive actions for neural network field theories. Conversely, the co
    
[^69]: 使用对抗模型量化不确定性

    Quantification of Uncertainty with Adversarial Models. (arXiv:2307.03217v1 [cs.LG])

    [http://arxiv.org/abs/2307.03217](http://arxiv.org/abs/2307.03217)

    该论文提出了使用对抗模型（QUAM）来更好地估计认知不确定性。QUAM识别整个积分下乘积较大的区域，而不仅仅是后验。与先前的方法相比，QUAM对认知不确定性的近似误差更小。

    

    在实际应用中，量化不确定性对于可操作的预测非常重要。预测不确定性的关键在于估计认知不确定性，它被定义为一个散度函数和后验的乘积的积分。当前的方法如Deep Ensembles或MC dropout在估计认知不确定性方面表现不佳，因为它们主要考虑后验在采样模型时。我们提出了使用对抗模型（QUAM）来更好地估计认知不确定性。QUAM识别整个积分下乘积较大的区域，而不仅仅是后验。因此，与先前的方法相比，QUAM对认知不确定性的近似误差更小。乘积较大的模型对应于对抗模型（不是对抗性示例！）。对抗模型既有较高的后验，也有其预测与其他模型之间的较高差异。

    Quantifying uncertainty is important for actionable predictions in real-world applications. A crucial part of predictive uncertainty quantification is the estimation of epistemic uncertainty, which is defined as an integral of the product between a divergence function and the posterior. Current methods such as Deep Ensembles or MC dropout underperform at estimating the epistemic uncertainty, since they primarily consider the posterior when sampling models. We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to better estimate the epistemic uncertainty. QUAM identifies regions where the whole product under the integral is large, not just the posterior. Consequently, QUAM has lower approximation error of the epistemic uncertainty compared to previous methods. Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of
    
[^70]: 基于区域关注的多视角表示学习用于城市区域嵌入

    Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])

    [http://arxiv.org/abs/2307.03212](http://arxiv.org/abs/2307.03212)

    提出了一种区域关注的多视角表示学习（ROMER）的算法，用于捕捉多视角之间的依赖关系，学习城市区域的表达能力，并在多源城市数据中优于现有方法。

    

    城市区域嵌入是一个重要且具有高度挑战性的问题，由于城市数据的复杂性和不断变化的性质。为了解决这些挑战，我们提出了一种区域关注的多视角表示学习（ROMER），以捕捉多视角之间的依赖关系，并学习城市区域的表达能力，而不受刚性邻域条件的限制。我们的模型专注于从多源城市数据中学习城市区域表示。首先，我们从移动流模式、POI语义和签到动态中捕捉多视角的相关性。然后，我们采用全局图注意网络来学习图中任意两个顶点的相似性。为了全面考虑和共享多个视角的特征，我们进一步提出了一个两阶段的融合模块，利用外部注意力学习权重来融合多视角嵌入。在真实世界数据集上进行的两个下游任务的大量实验证明，我们的模型优于现有的方法。

    Urban region embedding is an important and yet highly challenging issue due to the complexity and constantly changing nature of urban data. To address the challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighbourhood region conditions. Our model focus on learn urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics and check-in dynamics. Then, we adopt global graph attention networks to learn similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods
    
[^71]: PseudoCell: 将硬负样本挖掘作为伪标签用于基于深度学习的中心母细胞检测

    PseudoCell: Hard Negative Mining as Pseudo Labeling for Deep Learning-Based Centroblast Cell Detection. (arXiv:2307.03211v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.03211](http://arxiv.org/abs/2307.03211)

    PseudoCell是一种基于深度学习的目标检测框架，用于自动化中心母细胞检测。它通过将病理学家的中心母细胞标签与使用细胞形态特征对低采样假阳性预测得到的伪阴性标签相结合，可以准确地缩小需要检查组织的区域。

    

    基于深度学习的补丁分类模型已被应用于H&E染色组织样本的整个切片图像中，以帮助病理学家对滤泡性淋巴瘤患者进行评级。然而，这些方法仍然需要病理学家手动识别中心母细胞并提供精细化标签以实现最佳性能。为了解决这个问题，我们提出了PseudoCell，一种用于自动化中心母细胞检测的目标检测框架（源代码可在https://github.com/IoBT-VISTEC/PseudoCell.git找到）。该框架通过将病理学家的中心母细胞标签与使用细胞形态特征对低采样假阳性预测得到的伪阴性标签相结合。通过使用PseudoCell，可以减轻病理学家的工作负担，因为它准确地缩小了需要检查组织的区域。根据置信阈值，PseudoCell可以在WSI上消除58.18-99.35%的非中心母细胞组织区域。

    Patch classification models based on deep learning have been utilized in whole-slide images (WSI) of H&E-stained tissue samples to assist pathologists in grading follicular lymphoma patients. However, these approaches still require pathologists to manually identify centroblast cells and provide refined labels for optimal performance. To address this, we propose PseudoCell, an object detection framework to automate centroblast detection in WSI (source code is available at https://github.com/IoBT-VISTEC/PseudoCell.git). This framework incorporates centroblast labels from pathologists and combines them with pseudo-negative labels obtained from undersampled false-positive predictions using the cell's morphological features. By employing PseudoCell, pathologists' workload can be reduced as it accurately narrows down the areas requiring their attention during examining tissue. Depending on the confidence threshold, PseudoCell can eliminate 58.18-99.35% of non-centroblasts tissue areas on WSI
    
[^72]: 稀疏图线性动态系统

    Sparse Graphical Linear Dynamical Systems. (arXiv:2307.03210v1 [cs.LG])

    [http://arxiv.org/abs/2307.03210](http://arxiv.org/abs/2307.03210)

    该论文提出了一种稀疏图线性动态系统方法，用于处理时间序列数据集。该方法可以利用图形视角来学习模型参数，解决了现有方法的限制，并提供了概率性和可解释性的学习结果。

    

    时间序列数据集在科学和工程的众多领域中扮演重要角色，如生物医学、地球观测和网络分析。有关状态空间模型（SSMs）的广泛研究是一个强大的数学工具，它允许对时间序列进行概率性和可解释性的学习。在SSMs中估计模型参数被认为是最复杂的任务之一，先验知识的引入既可以简化解释也可以复杂化推断任务。最近的研究工作试图在一些模型参数中加入图形视角，但存在明显的限制，这项工作解决了这些限制。更一般地说，现有的图模型工具旨在包含静态信息或动态信息，其中静态信息侧重于独立随机变量之间的统计依赖（例如，图形Lasso方法），动态信息侧重于时间序列样本之间的因果关系。

    Time-series datasets are central in numerous fields of science and engineering, such as biomedicine, Earth observation, and network analysis. Extensive research exists on state-space models (SSMs), which are powerful mathematical tools that allow for probabilistic and interpretable learning on time series. Estimating the model parameters in SSMs is arguably one of the most complicated tasks, and the inclusion of prior knowledge is known to both ease the interpretation but also to complicate the inferential tasks. Very recent works have attempted to incorporate a graphical perspective on some of those model parameters, but they present notable limitations that this work addresses. More generally, existing graphical modeling tools are designed to incorporate either static information, focusing on statistical dependencies among independent random variables (e.g., graphical Lasso approach), or dynamic information, emphasizing causal relationships among time series samples (e.g., graphical 
    
[^73]: DENCLUE的最优带宽选择

    Optimal Bandwidth Selection for DENCLUE. (arXiv:2307.03206v1 [cs.LG])

    [http://arxiv.org/abs/2307.03206](http://arxiv.org/abs/2307.03206)

    本文提出了一种计算DENCLUE算法最优参数的新方法，并在实验部分讨论了其性能。

    

    在现代工业中，聚类算法是算法工程师的日常工作。尽管在2010年之前，聚类算法经历了快速增长，但在深度学习成为机器学习应用的实际工业标准之后，与该研究主题相关的创新停滞不前。2007年，提出了一种名为DENCLUE的基于密度的聚类算法，用于解决非线性数据结构的聚类问题。然而，直到2011年，该算法的参数选择问题仍然被大部分忽视。本文提出了一种计算DENCLUE算法最优参数的新方法，并在实验部分讨论了其性能。

    In modern day industry, clustering algorithms are daily routines of algorithm engineers. Although clustering algorithms experienced rapid growth before 2010. Innovation related to the research topic has stagnated after deep learning became the de facto industrial standard for machine learning applications. In 2007, a density-based clustering algorithm named DENCLUE was invented to solve clustering problem for nonlinear data structures. However, its parameter selection problem was largely neglected until 2011. In this paper, we propose a new approach to compute the optimal parameters for the DENCLUE algorithm, and discuss its performance in the experiment section.
    
[^74]: 缩放定律不具备可扩展性

    Scaling Laws Do Not Scale. (arXiv:2307.03201v1 [cs.LG])

    [http://arxiv.org/abs/2307.03201](http://arxiv.org/abs/2307.03201)

    本文讨论了缩放定律与人工智能模型性能之间的关系，并指出数据集规模的增加会引发不同社群的价值观和偏见风险。

    

    最近的研究提出了一种称为“缩放定律”的幂律关系，它描述了人工智能（AI）模型的性能与模型设计的各个方面（如数据集大小）之间的关系。换句话说，随着数据集（或模型参数等）的增加，基于该数据集训练的模型的性能将相应增加。然而，在总体上具有吸引力的同时，这种缩放定律关系忽视了用于衡量性能的指标可能是不稳定和有争议的，或者可能不符合不同人群对模型输出质量的感知。本文提出，随着用于训练大型AI模型的数据集规模增长，数据集中包含的不同社群（包括人口统计学群体）的数量可能会增加，每个社群可能具有不同的价值观。因此，数据集中所代表的社群可能存在价值观或偏见的风险。

    Recent work has proposed a power law relationship, referred to as ``scaling laws,'' between the performance of artificial intelligence (AI) models and aspects of those models' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase. However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or p
    
[^75]: 分析SplitFed Learning中的漏洞：评估其对数据污染攻击的鲁棒性

    Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks. (arXiv:2307.03197v1 [cs.LG])

    [http://arxiv.org/abs/2307.03197](http://arxiv.org/abs/2307.03197)

    本研究对SplitFed Learning中的数据污染攻击进行了研究和分析，并提出了三种新的攻击策略。实验结果表明这些攻击策略可以降低基于DCML的分类器的性能。

    

    分布式协作机器学习（DCML）是解决集中式机器学习中的隐私问题的一种潜在替代方案。Split learning（SL）和联邦学习（FL）是DCML中两种有效的学习方法。最近人们对FL和SL的混合形式SplitFed Learning（SFL）产生了较大兴趣。本研究是对SFL中数据污染攻击进行研究、分析和影响评估的最早尝试。我们提出了三种新的攻击策略，分别是非目标攻击、有目标攻击和基于距离的攻击，用于SFL。所有攻击策略旨在降低基于DCML的分类器的性能。我们对心电图信号分类和手写数字识别这两个不同案例进行了攻击实验，在恶意客户端的百分比和模型拆分层的选择方面进行了变化。

    Distributed Collaborative Machine Learning (DCML) is a potential alternative to address the privacy concerns associated with centralized machine learning. The Split learning (SL) and Federated Learning (FL) are the two effective learning approaches in DCML. Recently there have been an increased interest on the hybrid of FL and SL known as the SplitFed Learning (SFL). This research is the earliest attempt to study, analyze and present the impact of data poisoning attacks in SFL. We propose three kinds of novel attack strategies namely untargeted, targeted and distance-based attacks for SFL. All the attacks strategies aim to degrade the performance of the DCML-based classifier. We test the proposed attack strategies for two different case studies on Electrocardiogram signal classification and automatic handwritten digit recognition. A series of attack experiments were conducted by varying the percentage of malicious clients and the choice of the model split layer between the clients and 
    
[^76]: 提高人机协同系统的效率: 在人类专家中添加人工智能

    Improving the Efficiency of Human-in-the-Loop Systems: Adding Artificial to Human Experts. (arXiv:2307.03003v1 [cs.LG])

    [http://arxiv.org/abs/2307.03003](http://arxiv.org/abs/2307.03003)

    本研究提出了一个混合系统，在人类专家中添加人工智能，从之前由人类专家审查过的未知类别的数据实例中学习分类，以提高人机协同系统的效率。

    

    信息系统越来越多地利用人工智能（AI）和机器学习（ML）从大量数据中生成价值。然而，ML模型并不完美，可能会产生错误的分类。因此，人机协同（HITL）扩展了ML模型，为难以分类的实例添加了人工审核。本研究认为，持续依赖人类专家处理困难的模型分类会导致人力投入的大幅增加，增加了有限资源的压力。为解决这个问题，我们提出了一个混合系统，创建了人工专家，从之前由人类专家审查过的未知类别的数据实例中学习分类。我们的混合系统评估哪个人工专家适合分类来自未知类别的实例，并自动分配。随着时间的推移，这减少了人力投入，提高了系统的效率。我们的实验表明，我们的方法优于传统的HITL系统。

    Information systems increasingly leverage artificial intelligence (AI) and machine learning (ML) to generate value from vast amounts of data. However, ML models are imperfect and can generate incorrect classifications. Hence, human-in-the-loop (HITL) extensions to ML models add a human review for instances that are difficult to classify. This study argues that continuously relying on human experts to handle difficult model classifications leads to a strong increase in human effort, which strains limited resources. To address this issue, we propose a hybrid system that creates artificial experts that learn to classify data instances from unknown classes previously reviewed by human experts. Our hybrid system assesses which artificial expert is suitable for classifying an instance from an unknown class and automatically assigns it. Over time, this reduces human effort and increases the efficiency of the system. Our experiments demonstrate that our approach outperforms traditional HITL sy
    
[^77]: 弹性决策变压器

    Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02484](http://arxiv.org/abs/2307.02484)

    弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。

    

    本文介绍了弹性决策变压器（EDT），它是现有决策变压器（DT）及其变体的重大进展。尽管DT声称能够生成最佳轨迹，但实证证据表明它在轨迹拼接方面存在困难，轨迹拼接是指从一组次优轨迹中生成最优或接近最优轨迹的过程。提出的EDT通过在测试时间进行动作推断时调整DT中维护的历史长度来实现轨迹拼接，从而使自己与众不同。此外，当前轨迹是最优的时候，EDT通过保持较长的历史，当当前轨迹是次优的时候，EDT通过保持较短的历史来优化轨迹，使其能够与更优的轨迹进行“拼接”。广泛的实验表明，EDT能够填补基于DT和基于Q-Learning方法之间的性能差距。特别是，EDT在多任务情况下胜过基于Q-Learning的方法。

    This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
    
[^78]: 跨深度学习架构实现特征归因的协调: 提升解释性和一致性

    Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency. (arXiv:2307.02150v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02150](http://arxiv.org/abs/2307.02150)

    该论文研究了特征归因方法在不同深度学习架构之间的通用性，并探讨了在多个模型上协调特征归因的可行性。研究结果显示特征归因的协调有助于提高深度学习模型的解释性和一致性。

    

    确保机器学习模型的可信度和可解释性对于其在现实世界应用中至关重要。特征归因方法已经引起了许多关注，它们通过将重要性归因给个别输入特征来提供模型预测的局部解释。该研究探讨了特征归因在各种深度学习架构（如卷积神经网络（CNN）和视觉转换器）之间的泛化能力。我们旨在评估将特征归因方法作为未来检测器的可行性，并研究如何在采用不同架构但以相同数据分布训练的多个模型之间协调这些特征。通过探索这种协调，我们旨在开发出更一致和乐观的特征归因理解，提高深度学习模型之间局部解释的一致性。我们的研究结果凸显了实现特征归因协调性的潜力。

    Ensuring the trustworthiness and interpretability of machine learning models is critical to their deployment in real-world applications. Feature attribution methods have gained significant attention, which provide local explanations of model predictions by attributing importance to individual input features. This study examines the generalization of feature attributions across various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers. We aim to assess the feasibility of utilizing a feature attribution method as a future detector and examine how these features can be harmonized across multiple models employing distinct architectures but trained on the same data distribution. By exploring this harmonization, we aim to develop a more coherent and optimistic understanding of feature attributions, enhancing the consistency of local explanations across diverse deep-learning models. Our findings highlight the potential for harmonized feature att
    
[^79]: 基于粒子距离的GAN稳定性分析框架与Wasserstein渐变流

    Stability Analysis Framework for Particle-based Distance GANs with Wasserstein Gradient Flow. (arXiv:2307.01879v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.01879](http://arxiv.org/abs/2307.01879)

    本文提出了一个基于粒子距离的GAN稳定性分析框架，并使用Wasserstein渐变流对GAN的训练过程进行稳定性分析。研究发现，由于GAN的目标函数形式的原因，判别器的训练过程通常是不稳定的。为了解决这个问题，文中提出了一个稳定的训练方法。

    

    本文研究了生成网络的训练过程，该网络将粒子距离作为目标函数，例如MMD GAN，Cramer GAN和EIEG GAN。然而，这些GAN往往存在训练不稳定的问题。本文从概率密度动态的角度分析了这些GAN的训练过程的稳定性。我们将GAN中的判别器D看作是一个特征转换映射，它将高维数据映射到一个特征空间，而生成器G则将随机变量映射到类似于真实数据的样本。基于这个视角，我们可以通过概率密度函数的Wasserstein渐变流来进行GAN的稳定性分析。我们发现，由于GAN中$\min_G \max_D E(G, D)$的公式，判别器的训练过程通常是不稳定的。为了解决这个问题，我们提出了一个稳定的训练方法。

    In this paper, we investigate the training process of generative networks that use a type of probability density distance named particle-based distance as the objective function, e.g. MMD GAN, Cram\'er GAN, EIEG GAN. However, these GANs often suffer from the problem of unstable training. In this paper, we analyze the stability of the training process of these GANs from the perspective of probability density dynamics. In our framework, we regard the discriminator $D$ in these GANs as a feature transformation mapping that maps high dimensional data into a feature space, while the generator $G$ maps random variables to samples that resemble real data in terms of feature space. This perspective enables us to perform stability analysis for the training of GANs using the Wasserstein gradient flow of the probability density function. We find that the training process of the discriminator is usually unstable due to the formulation of $\min_G \max_D E(G, D)$ in GANs. To address this issue, we a
    
[^80]: 针对战略非局部分布偏移的耦合梯度流动

    Coupled Gradient Flows for Strategic Non-Local Distribution Shift. (arXiv:2307.01166v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.01166](http://arxiv.org/abs/2307.01166)

    该论文提出了一种框架，用于分析现实世界系统中的分布偏移动态，并且捕捉了学习算法与其应用的分布之间的反馈循环。通过耦合偏微分方程模型，考虑了战略性反应、非局部内生人口互动和其他外生分布偏移来源，以实现对时间上细微变化的捕捉。研究证明了在合作设置和竞争设置中，当算法通过梯度下降进行重新训练时，重新训练过程渐近收敛到一个稳定状态。

    

    我们提出了一种新颖的框架，用于分析现实世界系统中分布偏移的动态过程，该框架捕捉了学习算法与其应用的分布之间的反馈循环。以往的研究主要以对抗或过度简化的分布偏移结构来建模反馈引起的分布偏移。相比之下，我们提出了一种耦合的偏微分方程模型，通过考虑由于对算法决策的战略性反应、非局部内生人口互动和其他外生分布偏移来源而产生的复杂动态，捕捉分布随时间的细微变化。我们考虑机器学习中的两种常见设置：信息不对称的合作设置以及学习者面对战略用户的竞争设置。对于这两种设置，当算法通过梯度下降进行重新训练时，我们证明了重新训练过程收敛到一个稳定状态的渐近性。

    We propose a novel framework for analyzing the dynamics of distribution shift in real-world systems that captures the feedback loop between learning algorithms and the distributions on which they are deployed. Prior work largely models feedback-induced distribution shift as adversarial or via an overly simplistic distribution-shift structure. In contrast, we propose a coupled partial differential equation model that captures fine-grained changes in the distribution over time by accounting for complex dynamics that arise due to strategic responses to algorithmic decision-making, non-local endogenous population interactions, and other exogenous sources of distribution shift. We consider two common settings in machine learning: cooperative settings with information asymmetries, and competitive settings where a learner faces strategic users. For both of these settings, when the algorithm retrains via gradient descent, we prove asymptotic convergence of the retraining procedure to a steady-
    
[^81]: RObotic MAnipulation Network（ROMAN）--混合层次学习解决复杂的连续任务

    RObotic MAnipulation Network (ROMAN) -- Hybrid Hierarchical Learning for Solving Complex Sequential Tasks. (arXiv:2307.00125v1 [cs.RO])

    [http://arxiv.org/abs/2307.00125](http://arxiv.org/abs/2307.00125)

    RObotic MAnipulation Network（ROMAN）通过混合层次学习框架解决复杂的连续操作任务，实现了任务多样性和鲁棒的失败恢复。

    

    在实体人工智能中，解决长序列任务面临着重大挑战。使机器人系统能够执行多样化的连续任务，并具备广泛的操作技能是一个活跃的研究领域。在本文中，我们提出了一种混合层次学习框架，即ROBOTIC Manipulation Network（ROMAN），以解决机器人操作中的多个复杂任务的长时间任务。ROMAN通过集成行为克隆、模仿学习和强化学习来实现任务的多样性以及鲁棒的失败恢复。它包括一个中央操作网络，协调一组不同的神经网络，每个网络专注于不同的可重组子任务，生成它们在复杂的长时间操作任务中的正确连续动作。实验结果表明，通过协调和激活这些专门的操作专家，ROMAN生成了正确的顺序激活。

    Solving long sequential tasks poses a significant challenge in embodied artificial intelligence. Enabling a robotic system to perform diverse sequential tasks with a broad range of manipulation skills is an active area of research. In this work, we present a Hybrid Hierarchical Learning framework, the Robotic Manipulation Network (ROMAN), to address the challenge of solving multiple complex tasks over long time horizons in robotic manipulation. ROMAN achieves task versatility and robust failure recovery by integrating behavioural cloning, imitation learning, and reinforcement learning. It consists of a central manipulation network that coordinates an ensemble of various neural networks, each specialising in distinct re-combinable sub-tasks to generate their correct in-sequence actions for solving complex long-horizon manipulation tasks. Experimental results show that by orchestrating and activating these specialised manipulation experts, ROMAN generates correct sequential activations f
    
[^82]: 遭受苦难的烤面包机

    Suffering Toasters. (arXiv:2306.17258v1 [cs.AI])

    [http://arxiv.org/abs/2306.17258](http://arxiv.org/abs/2306.17258)

    本文旨在为人工智能、自我意识和代理问题提供更清晰的定义，我们提出了一种新的启发式方法来测试人工自我意识，并讨论了这种方法引发的一些问题。

    

    在人工智能（AI）领域，智能的广泛接受的定义仍然难以找到。由于我们对AI范式、架构和工具的快速发展，人们普遍认为自然产生的AI意识比以往更有可能。在本文中，我们声称所有当前的智能测试都不足以指出存在或缺乏象人类直觉感知的智能。我们借鉴科学哲学、心理学和其他领域的思想，提供了对人工智能、自我意识和代理问题的更清晰定义。我们进一步提出了一种测试人工自我意识的新启发式方法，并概述了可能的实现。最后，我们讨论了这种新启发式方法引发的一些问题，无论是哲学问题还是实现问题。

    A widely accepted definition of intelligence in the context of Artificial Intelligence (AI) still eludes us. Due to our exceedingly rapid development of AI paradigms, architectures, and tools, the prospect of naturally arising AI consciousness seems more likely than ever. In this paper, we claim that all current intelligence tests are insufficient to point to the existence or lack of intelligence \textbf{as humans intuitively perceive it}. We draw from ideas in the philosophy of science, psychology, and other areas of research to provide a clearer definition of the problems of artificial intelligence, self-awareness, and agency. We furthermore propose a new heuristic approach to test for artificial self-awareness and outline a possible implementation. Finally, we discuss some of the questions that arise from this new heuristic, be they philosophical or implementation-oriented.
    
[^83]: 理解迭代元训练的过拟合问题

    Understanding the Overfitting of the Episodic Meta-training. (arXiv:2306.16873v1 [cs.LG])

    [http://arxiv.org/abs/2306.16873](http://arxiv.org/abs/2306.16873)

    本研究通过引入知识蒸馏技术，解决了迭代元训练阶段的过拟合问题，该方法通过惩罚过度区分，保留教师模型的新类别泛化知识，优于标准元训练过程。同时，我们提出了最近邻对称Kullback-Leibler（NNSKL）散度来进一步推进知识蒸馏的极限。

    

    虽然两阶段少样本分类方法取得了成功，但在迭代元训练阶段，模型遭受严重的过拟合问题。我们假设这是由于过度区分造成的，即模型学习过于依赖适合基类区分的表面特征，而抑制了对新类别的泛化能力。为了惩罚过度区分，我们引入了知识蒸馏技术，在训练过程中保留来自教师模型的新类别泛化知识。具体而言，我们选择验证准确率最好的教师模型，并限制了学生模型线性分类器输出分布与教师模型之间的对称Kullback-Leibler（SKL）散度。这一简单的方法优于标准元训练过程。此外，我们进一步提出了用于元训练的最近邻对称Kullback-Leibler（NNSKL）散度，以推动知识蒸馏的极限。

    Despite the success of two-stage few-shot classification methods, in the episodic meta-training stage, the model suffers severe overfitting. We hypothesize that it is caused by over-discrimination, i.e., the model learns to over-rely on the superficial features that fit for base class discrimination while suppressing the novel class generalization. To penalize over-discrimination, we introduce knowledge distillation techniques to keep novel generalization knowledge from the teacher model during training. Specifically, we select the teacher model as the one with the best validation accuracy during meta-training and restrict the symmetric Kullback-Leibler (SKL) divergence between the output distribution of the linear classifier of the teacher model and that of the student model. This simple approach outperforms the standard meta-training process. We further propose the Nearest Neighbor Symmetric Kullback-Leibler (NNSKL) divergence for meta-training to push the limits of knowledge distill
    
[^84]: 通过对抗攻击评估深度图像去噪模型的相似性和鲁棒性

    Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack. (arXiv:2306.16050v1 [cs.CV])

    [http://arxiv.org/abs/2306.16050](http://arxiv.org/abs/2306.16050)

    通过对抗攻击评估了深度图像去噪模型的相似性和鲁棒性，发现现有模型容易被攻击。还研究了去噪模型的迁移性在图像去噪任务中的特性。

    

    深度神经网络在图像去噪领域有着广泛的应用，并且比传统的图像去噪方法更优越。然而，深度神经网络不可避免地显示出弱鲁棒性，在面对对抗攻击时易受损。本文发现了现有深度图像去噪方法之间的相似之处，它们都容易被对抗攻击欺骗。首先，我们提出了一种去噪-PGD方法，它是一种全对抗的去噪模型。当前主流的非盲去噪模型（DnCNN，FFDNet，ECNDNet，BRDNet），盲去噪模型（DnCNN-B，Noise2Noise，RDDCNN-B，FAN），即插即用（DPIR，CurvPnP）和展开去噪模型（DeamNet）应用于灰度和彩色图像都可以被同一组方法攻击。其次，由于去噪-PGD的迁移性在图像去噪任务中很突出，我们设计了实验来探索迁移性下的潜在特性。

    Deep neural networks (DNNs) have a wide range of applications in the field of image denoising, and they are superior to traditional image denoising. However, DNNs inevitably show vulnerability, which is the weak robustness in the face of adversarial attacks. In this paper, we find some similitudes between existing deep image denoising methods, as they are consistently fooled by adversarial attacks. First, denoising-PGD is proposed which is a denoising model full adversarial method. The current mainstream non-blind denoising models (DnCNN, FFDNet, ECNDNet, BRDNet), blind denoising models (DnCNN-B, Noise2Noise, RDDCNN-B, FAN), and plug-and-play (DPIR, CurvPnP) and unfolding denoising models (DeamNet) applied to grayscale and color images can be attacked by the same set of methods. Second, since the transferability of denoising-PGD is prominent in the image denoising task, we design experiments to explore the characteristic of the latent under the transferability. We correlate transferabi
    
[^85]: 通过重新加权优化轨迹增强对抗训练

    Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14275](http://arxiv.org/abs/2306.14275)

    本文提出了一种名为“加权优化轨迹（WOT）”的新方法，通过优化历史轨迹，解决了对抗训练中的鲁棒泛化问题。

    

    尽管对抗训练已成为提高深度神经网络鲁棒性的事实上的方法，但众所周知，简单的对抗训练遭受了令人畏缩的鲁棒过拟合问题，导致鲁棒泛化效果不佳。近年来已经提出了一些方法来解决这些缺点，如额外的规范化、对抗权重扰动和更多数据训练。然而，鲁棒泛化的改进仍然远不理想。在本文中，我们从全新的角度解决这一挑战--优化历史轨迹的精细化。我们提出了一种名为“加权优化轨迹（WOT）”的新方法，利用对抗训练的优化轨迹在时间上的特点。我们进行了大量实验证明了WOT在各种最新对抗攻击下的有效性。结果显示，WOT与现有方法完美融合。

    Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with t
    
[^86]: FedSelect: 个性化联邦学习中参数自定义选择的细调方法

    FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v1 [cs.LG])

    [http://arxiv.org/abs/2306.13264](http://arxiv.org/abs/2306.13264)

    本文提出了一种名为FedSelect的新联邦学习框架，通过寻找最佳客户端子网络从而直接个性化客户端子网络结构和参数，同时保留了全局知识，提高了客户端性能。

    

    联邦学习旨在通过在本地数据上微调客户端参数或针对本地任务个性化架构来提高客户端性能。然而，现有的方法要么在牺牲重要的全局知识的情况下进行个性化，要么在预先确定网络层以进行微调的情况下导致客户端模型中全局知识储存的不足。本文提出了一种新的联邦学习框架FedSelect，通过同时搜索并获得个性化最佳参数和用于全局聚合的其余参数，从而直接个性化客户子网络结构和参数。

    Recent advancements in federated learning (FL) seek to increase client-level performance by fine-tuning client parameters on local data or personalizing architectures for the local task. Existing methods for such personalization either prune a global model or fine-tune a global model on a local client distribution. However, these existing methods either personalize at the expense of retaining important global knowledge, or predetermine network layers for fine-tuning, resulting in suboptimal storage of global knowledge within client models. Enlightened by the lottery ticket hypothesis, we first introduce a hypothesis for finding optimal client subnetworks to locally fine-tune while leaving the rest of the parameters frozen. We then propose a novel FL framework, FedSelect, using this procedure that directly personalizes both client subnetwork structure and parameters, via the simultaneous discovery of optimal parameters for personalization and the rest of parameters for global aggregatio
    
[^87]: 基于机器学习的实时反馈控制InAs/GaAs量子点生长

    Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots. (arXiv:2306.12898v1 [cond-mat.mes-hall])

    [http://arxiv.org/abs/2306.12898](http://arxiv.org/abs/2306.12898)

    该论文提出了一种基于机器学习的实时反馈控制InAs/GaAs量子点生长方法。

    

    自组装的InAs / GaAs量子点（QDs）具有用于开发各种光电子器件的极高价值。建立特定密度的QDs的过程参数是一个多维优化挑战，通常通过耗时和迭代的试错来解决。在此，作者使用基于3D ResNet的机器学习（ML）模型，专门训练RHEED视频，并提供有关表面形貌的实时反馈。

    Self-assembled InAs/GaAs quantum dots (QDs) have properties highly valuable for developing various optoelectronic devices such as QD lasers and single photon sources. The applications strongly rely on the density and quality of these dots, which has motivated studies of the growth process control to realize high-quality epi-wafers and devices. Establishing the process parameters in molecular beam epitaxy (MBE) for a specific density of QDs is a multidimensional optimization challenge, usually addressed through time-consuming and iterative trial-and-error. Meanwhile, reflective high-energy electron diffraction (RHEED) has been widely used to capture a wealth of growth information in situ. However, it still faces the challenges of extracting information from noisy and overlapping images. Here, based on 3D ResNet, we developed a machine learning (ML) model specially designed for training RHEED videos instead of static images and providing real-time feedback on surface morphologies for pro
    
[^88]: 学习小波对椭圆算子的齐次化

    Learning Homogenization for Elliptic Operators. (arXiv:2306.12006v1 [math.NA])

    [http://arxiv.org/abs/2306.12006](http://arxiv.org/abs/2306.12006)

    本文提出了一种新的数据驱动方法，可以学习用于椭圆算子的齐次化映射，以建立考虑接口的齐次化本构定律，并且证明了该方法的有效性。

    

    多尺度偏微分方程在各种应用中都有出现。齐次化理论是消除小尺度依赖的强有力方法，可以得到简化的方程以及计算上的便利。在连续介质力学领域，齐次化对于导出包含微观物理学的本构定律以制定感兴趣的宏观量的平衡方程很关键。然而，获得齐次化本构定律通常是具有挑战性的，因为它们通常没有解析形式，并且可以展现在微观尺度上不存在的现象。针对这个问题，提出了数据驱动的学习本构定律方法。本文提出了一种新的基于数据驱动的学习椭圆算子齐次化映射的方法，可以说明与这种接口有关的齐次化建立。我们的方法通过构造适当的特征空间，以考虑底层几何学和微观结构，自适应地学习将这些接口合并到齐次化本构定律中。我们的数字结果表明，所提出的方法可以准确地捕捉有效的宏观行为，优于现有方法。

    Multiscale partial differential equations (PDEs) arise in various applications, and several schemes have been developed to solve them efficiently. Homogenization theory is a powerful methodology that eliminates the small-scale dependence, resulting in simplified equations that are computationally tractable. In the field of continuum mechanics, homogenization is crucial for deriving constitutive laws that incorporate microscale physics in order to formulate balance laws for the macroscopic quantities of interest. However, obtaining homogenized constitutive laws is often challenging as they do not in general have an analytic form and can exhibit phenomena not present on the microscale. In response, data-driven learning of the constitutive law has been proposed as appropriate for this task. However, a major challenge in data-driven learning approaches for this problem has remained unexplored: the impact of discontinuities and corner interfaces in the underlying material. These discontinui
    
[^89]: 非凸优化中的自适应策略

    Adaptive Strategies in Non-convex Optimization. (arXiv:2306.10278v1 [cs.LG])

    [http://arxiv.org/abs/2306.10278](http://arxiv.org/abs/2306.10278)

    本文介绍了应用于随机优化和深度神经网络训练中的自适应算法。算法可以自动适应不同的噪声水平和梯度比例范围，从而达到（近似）最优速度。

    

    如果算法不需要先验知识就可以表现得与知道这个问题特定参数的算法相竞争，那么就说算法对某个参数是自适应的。本论文介绍了我们在以下场景中开发自适应算法的工作：1. 在随机优化环境中，我们只收到随机梯度，并且评估这些梯度的噪声水平极大地影响了收敛速度。通常需要调整噪声水平才能实现最优速率，而我们开发了自适应算法，可以在不知道噪声范围的情况下自动保证（近似）最优速率。2. 在训练深度神经网络时，每个坐标轴上的梯度大小比例可以散布在非常广的范围内，除非采用像BatchNorm这样的归一化技术。在这种情况下，不考虑梯度比例问题的算法可能表现非常差。

    An algorithm is said to be adaptive to a certain parameter (of the problem) if it does not need a priori knowledge of such a parameter but performs competitively to those that know it. This dissertation presents our work on adaptive algorithms in following scenarios: 1. In the stochastic optimization setting, we only receive stochastic gradients and the level of noise in evaluating them greatly affects the convergence rate. Tuning is typically required when without prior knowledge of the noise scale in order to achieve the optimal rate. Considering this, we designed and analyzed noise-adaptive algorithms that can automatically ensure (near)-optimal rates under different noise scales without knowing it. 2. In training deep neural networks, the scales of gradient magnitudes in each coordinate can scatter across a very wide range unless normalization techniques, like BatchNorm, are employed. In such situations, algorithms not addressing this problem of gradient scales can behave very poor
    
[^90]: 离线优先经验重放

    Offline Prioritized Experience Replay. (arXiv:2306.05412v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.05412](http://arxiv.org/abs/2306.05412)

    本文提出了离线优先经验重放（OPER）方法来解决离线强化学习中的分布偏移问题。通过设计一类优先级函数来对高回报的转换进行优先处理，从而改善行为策略，并在此改进的策略约束下优化离线强化学习算法的解决方案。对于离线强化学习，OPER方法是一种有效的解决方案。

    

    离线强化学习面临着分布偏移问题。为了解决这个问题，现有的工作主要集中在设计学习策略和行为策略之间的复杂约束。然而，这些约束通过均匀采样等方式被应用到表现良好和表现差的行动上，这可能会对学习策略产生负面影响。为了缓解这个问题，我们提出了离线优先经验重放（OPER），其中包括一类优先级函数，用于将高回报的转换置于更频繁的访问中。通过理论分析，我们证明了这类优先级函数能够引起行为策略的改善，当策略约束到这个改进的策略上时，离线强化学习算法很可能得到更好的解决方案。我们开发了两种实用策略来获得基于拟合值网络的优先权重（OPER-A）或者u

    Offline reinforcement learning (RL) is challenged by the distributional shift problem. To address this problem, existing works mainly focus on designing sophisticated policy constraints between the learned policy and the behavior policy. However, these constraints are applied equally to well-performing and inferior actions through uniform sampling, which might negatively affect the learned policy. To alleviate this issue, we propose Offline Prioritized Experience Replay (OPER), featuring a class of priority functions designed to prioritize highly-rewarding transitions, making them more frequently visited during training. Through theoretical analysis, we show that this class of priority functions induce an improved behavior policy, and when constrained to this improved policy, a policy-constrained offline RL algorithm is likely to yield a better solution. We develop two practical strategies to obtain priority weights by estimating advantages based on a fitted value network (OPER-A) or u
    
[^91]: 不要相信你的眼睛：关于特征可视化的（不）可靠性。

    Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])

    [http://arxiv.org/abs/2306.04719](http://arxiv.org/abs/2306.04719)

    本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。

    

    神经网络是如何从像素中提取模式的？特征可视化通过优化来可视化高激活的模式，试图回答这个重要问题。如今，可视化方法构成了我们对神经网络内部工作的了解的基础，作为一种机械式的可解释性。在这里，我们问：特征可视化有多可靠？我们通过开发网络电路来诈骗特征可视化，使其显示完全与自然输入的正常网络行为毫无联系的任意模式。然后，我们提供证据表明在标准，未操纵网络中发生了类似的现象：特征可视化与标准输入处理非常不同，对神经网络如何处理自然图像的解释能力产生怀疑。我们通过理论证明支撑这一经验发现，由于优化过程中固有的限制，可以通过特征可视化可靠理解的功能集极其有限。

    How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
    
[^92]: 语言模型是有限实用说话者

    Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17760](http://arxiv.org/abs/2305.17760)

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。

    

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。特别地，我们展示了经过人类反馈的强化学习微调的大型语言模型（Ouyang等人，2022）具有概念上类似于 快与慢思考模型（Kahneman，2011）的思维模型，而这种思维模型被心理学家们归因于人类。我们讨论了从人类反馈中的强化学习作为快与慢思考模型的局限性，并提出了扩展这个框架的途径。本研究实质上凸显了采用认知概率建模方法来获得对语言模型的理解、评估和推进方面的深刻见解的价值。

    How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
    
[^93]: 无线网络中的分散式学习: 广播与随机接入的影响

    Decentralized Learning over Wireless Networks: The Effect of Broadcast with Random Access. (arXiv:2305.07368v1 [cs.NI])

    [http://arxiv.org/abs/2305.07368](http://arxiv.org/abs/2305.07368)

    本文研究了分散式学习在无线网络中的通信问题，发现优化接入概率以最大化成功链路数的期望值是提升系统收敛速度的有效策略。

    

    本文关注分散式学习的通信方面，涉及多个代理使用分散式随机梯度下降算法(D-SGD)在分布式数据上共同训练机器学习模型的过程。特别地，我们考虑了无线信道的广播性质以及通信拓扑中的链路动态，探究广播传输和概率性随机接入策略对D-SGD收敛性能的影响。实验结果表明，优化接入概率以最大化成功链路数的期望值是一种高效的提升系统收敛速度的策略。

    In this work, we focus on the communication aspect of decentralized learning, which involves multiple agents training a shared machine learning model using decentralized stochastic gradient descent (D-SGD) over distributed data. In particular, we investigate the impact of broadcast transmission and probabilistic random access policy on the convergence performance of D-SGD, considering the broadcast nature of wireless channels and the link dynamics in the communication topology. Our results demonstrate that optimizing the access probability to maximize the expected number of successful links is a highly effective strategy for accelerating the system convergence.
    
[^94]: SocNavGym：一个针对社交导航的强化学习仿真环境

    SocNavGym: A Reinforcement Learning Gym for Social Navigation. (arXiv:2304.14102v1 [cs.RO])

    [http://arxiv.org/abs/2304.14102](http://arxiv.org/abs/2304.14102)

    本文提出了SocNavGym，对于社交导航领域的研究提供了一个轻便、快速、易用的仿真环境，可生成各种各样的社交导航场景，并促进了智能社交机器人的发展。

    

    在人口密集的环境下，自主机器人在导航时需要遵守社交规范。机器学习，尤其是深度强化学习，最近在社交导航领域中取得了显著进展。这可以部分归因于生成的策略不受代码复杂性或处理的变量数量等人类限制。不幸的是，DRL算法缺乏安全保障，需要大量数据需求，导致在现实环境中的应用不太切实际。为了缩小这一差距，仿真环境被广泛使用。本文提出了SocNavGym，一个专门针对社交导航的先进仿真环境，可以生成各种各样的社交导航场景，并促进智能社交机器人的发展。SocNavGym轻便、快速、易于使用，并可轻松配置以生成不同类型的社交导航场景。此外，它还可以配置为使用不同的传感器，并支持无障碍环境的仿真。

    It is essential for autonomous robots to be socially compliant while navigating in human-populated environments. Machine Learning and, especially, Deep Reinforcement Learning have recently gained considerable traction in the field of Social Navigation. This can be partially attributed to the resulting policies not being bound by human limitations in terms of code complexity or the number of variables that are handled. Unfortunately, the lack of safety guarantees and the large data requirements by DRL algorithms make learning in the real world unfeasible. To bridge this gap, simulation environments are frequently used. We propose SocNavGym, an advanced simulation environment for social navigation that can generate a wide variety of social navigation scenarios and facilitates the development of intelligent social agents. SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly configured to generate different types of social navigation scenarios. It can also be configured to
    
[^95]: 鲁棒门票能够更好地传输: 在迁移学习中绘制更具传输性的子网络

    Robust Tickets Can Transfer Better: Drawing More Transferable Subnetworks in Transfer Learning. (arXiv:2304.11834v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11834](http://arxiv.org/abs/2304.11834)

    该论文提出了一种新的迁移学习流程，通过绘制具有鲁棒性的子网络，改进了预训练模型在资源受限的边缘设备上的传输能力。

    

    迁移学习利用在丰富数据的源任务上预训练的深度神经网络(DNN)的特征表示，以赋予下游任务的有效微调。然而，预训练模型往往过于庞大，无法提供可推广的表示，这限制了它们在资源受限的边缘设备上的部署。为了弥补这一差距，我们提出了一种新的迁移学习流程，利用我们的发现：鲁棒门票能够更好地传输，即通过适当引入对抗鲁棒性的方式绘制的子网络可以在传统的彩票门票子网络上取得更好的传输能力。大量实验和消融研究验证了我们提出的迁移学习流程在各种下游任务和稀疏模式中都能实现增强的准确度-稀疏度权衡，进一步丰富了彩票门票假设。

    Transfer learning leverages feature representations of deep neural networks (DNNs) pretrained on source tasks with rich data to empower effective finetuning on downstream tasks. However, the pretrained models are often prohibitively large for delivering generalizable representations, which limits their deployment on edge devices with constrained resources. To close this gap, we propose a new transfer learning pipeline, which leverages our finding that robust tickets can transfer better, i.e., subnetworks drawn with properly induced adversarial robustness can win better transferability over vanilla lottery ticket subnetworks. Extensive experiments and ablation studies validate that our proposed transfer learning pipeline can achieve enhanced accuracy-sparsity trade-offs across both diverse downstream tasks and sparsity patterns, further enriching the lottery ticket hypothesis.
    
[^96]: 在线多类可学习性的刻画。

    A Characterization of Online Multiclass Learnability. (arXiv:2303.17716v1 [cs.LG])

    [http://arxiv.org/abs/2303.17716](http://arxiv.org/abs/2303.17716)

    在线多类学习问题中，使用Multiclass Littlestone维度可以刻画标签数目为无界情况下的可学习性。

    

    我们考虑当标签数目是无界的时候在线多类学习问题。我们展示了Multiclass Littlestone维度，这个概念首次出现在\cite{DanielyERMprinciple}中，继续刻画了该场景下的在线可学习性。我们的结果补充了最近的工作，\cite{Brukhimetal2022}给出了当标签空间是无界的情况下批处理多类可学习性的刻画。

    We consider the problem of online multiclass learning when the number of labels is unbounded. We show that the Multiclass Littlestone dimension, first introduced in \cite{DanielyERMprinciple}, continues to characterize online learnability in this setting. Our result complements the recent work by \cite{Brukhimetal2022} who give a characterization of batch multiclass learnability when the label space is unbounded.
    
[^97]: 使用Uni-Mol+实现高精度的量子化学属性预测

    Highly Accurate Quantum Chemical Property Prediction with Uni-Mol+. (arXiv:2303.16982v1 [physics.chem-ph])

    [http://arxiv.org/abs/2303.16982](http://arxiv.org/abs/2303.16982)

    本文提出了一种使用Uni-Mol+的新方法来高精度预测量子化学属性，它能够从2D分子图自动生成3D构象，并通过迭代优化得到优化后的构象，为预测QC属性提供更加准确的基础。

    

    深度学习的最新发展在加速预测量子化学（QC）属性方面取得了显著进展，它消除了密度泛函理论等昂贵电子结构计算的需求。但是，以1D SMILES序列或2D分子图为基础的之前的方法未能实现高精度，因为QC属性主要取决于经电子结构方法优化的3D平衡构象。本文提出了一种名为Uni-Mol+的新方法来应对这一挑战。首先，给定一个2D分子图，Uni-Mol+从RDKit等廉价方法生成初始的3D构象。然后，初始构象被迭代地优化到其平衡构象，并进一步用于预测QC属性。所有这些步骤都是使用Transformer模型自动学习的。我们观察到，优化后的构象质量对QC属性预测至关重要。

    Recent developments in deep learning have made remarkable progress in speeding up the prediction of quantum chemical (QC) properties by removing the need for expensive electronic structure calculations like density functional theory. However, previous methods that relied on 1D SMILES sequences or 2D molecular graphs failed to achieve high accuracy as QC properties are primarily dependent on the 3D equilibrium conformations optimized by electronic structure methods. In this paper, we propose a novel approach called Uni-Mol+ to tackle this challenge. Firstly, given a 2D molecular graph, Uni-Mol+ generates an initial 3D conformation from inexpensive methods such as RDKit. Then, the initial conformation is iteratively optimized to its equilibrium conformation, and the optimized conformation is further used to predict the QC properties. All these steps are automatically learned using Transformer models. We observed the quality of the optimized conformation is crucial for QC property predict
    
[^98]: 辅助函数作为Koopman可观测量：基于数据的动力系统多项式优化方法

    Auxiliary Functions as Koopman Observables: Data-Driven Polynomial Optimization for Dynamical Systems. (arXiv:2303.01483v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2303.01483](http://arxiv.org/abs/2303.01483)

    提出了一种基于数据的动力系统分析方法，使用辅助函数作为Koopman可观测量，不需要明确的模型发现，可以适用于确定性和随机动力学，具有收敛性和性能优势。

    

    我们提出了一种灵活的基于数据的动力系统分析方法，不需要明确的模型发现。该方法源于从数据中逼近Koopman算子的技术，并且以一个可以通过数值求解的半定规划问题来实现。此外，该方法不关心数据是通过确定性还是随机过程生成的，因此用户无需进行任何调整即可应用于不同的情况。严格的收敛性结果证明了该方法的适用性，并将文献中类似的结果进行了扩展和统一。通过对确定性和随机动力学的吸引子上发现Lyapunov函数、执行遍历优化以及界定极值的示例，证明了该方法的性能。

    We present a flexible data-driven method for dynamical system analysis that does not require explicit model discovery. The method is rooted in well-established techniques for approximating the Koopman operator from data and is implemented as a semidefinite program that can be solved numerically. Furthermore, the method is agnostic of whether data is generated through a deterministic or stochastic process, so its implementation requires no prior adjustments by the user to accommodate these different scenarios. Rigorous convergence results justify the applicability of the method, while also extending and uniting similar results from across the literature. Examples on discovering Lyapunov functions, performing ergodic optimization, and bounding extrema over attractors for both deterministic and stochastic dynamics exemplify these convergence results and demonstrate the performance of the method.
    
[^99]: 连续时间功能扩散过程

    Continuous-Time Functional Diffusion Processes. (arXiv:2303.00800v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00800](http://arxiv.org/abs/2303.00800)

    连续时间功能扩散过程引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。通过使用新的数学框架和扩展，FDPs可以在函数空间中构建新型生成模型，在处理连续数据时能够实现高质量的图像生成，所需参数数量比现有模型低几个数量级。

    

    我们引入了功能扩散过程（FDPs），将基于得分的扩散模型推广到无限维函数空间。 FDPs需要一种新的数学框架来描述前向和反向动力学，并进行多个扩展以得出实际的训练目标。这些扩展包括Girsanov定理的无限维版本，以便能够计算ELBO，以及采样定理的无限维版本，以确保可数个点上的函数评估等价于无限维函数。我们使用FDPs在函数空间中构建了一种新型生成模型，不需要专门的网络架构，并且可以处理任何类型的连续数据。我们在真实数据上的结果显示，使用简单的多层感知机（MLP）结构，FDPs实现了高质量的图像生成，所需的参数数量比现有的扩散模型低几个数量级。

    We introduce Functional Diffusion Processes (FDPs), which generalize score-based diffusion models to infinite-dimensional function spaces. FDPs require a new mathematical framework to describe the forward and backward dynamics, and several extensions to derive practical training objectives. These include infinite-dimensional versions of Girsanov theorem, in order to be able to compute an ELBO, and of the sampling theorem, in order to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. We use FDPs to build a new breed of generative models in function spaces, which do not require specialized network architectures, and that can work with any kind of continuous data. Our results on real data show that FDPs achieve high-quality image generation, using a simple MLP architecture with orders of magnitude fewer parameters than existing diffusion models.
    
[^100]: 基于复合优化算法的Sigmoid网络求解方法

    Composite Optimization Algorithms for Sigmoid Networks. (arXiv:2303.00589v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.00589](http://arxiv.org/abs/2303.00589)

    本文使用复合优化算法解决Sigmoid网络问题，可以在非凸和非光滑问题的情况下收敛到全局最优解，并提供了一般指导来设置网络大小。

    

    本文使用复合优化算法求解Sigmoid网络问题，将Sigmoid网络转化为一个凸复合优化问题，并提出了基于线性化近端算法和交替方向乘子法的复合优化算法。在假设具有弱锐最小值和正则条件的情况下，该算法可以在非凸和非光滑问题的情况下收敛到目标函数的全局最优解。此外，收敛结果可以直接与训练数据量相关联，为设置Sigmoid网络的大小提供一般指导。在Franke函数拟合和手写数字识别方面的数值实验表明，所提出的算法表现良好且稳健。

    In this paper, we use composite optimization algorithms to solve sigmoid networks. We equivalently transfer the sigmoid networks to a convex composite optimization and propose the composite optimization algorithms based on the linearized proximal algorithms and the alternating direction method of multipliers. Under the assumptions of the weak sharp minima and the regularity condition, the algorithm is guaranteed to converge to a globally optimal solution of the objective function even in the case of non-convex and non-smooth problems. Furthermore, the convergence results can be directly related to the amount of training data and provide a general guide for setting the size of sigmoid networks. Numerical experiments on Franke's function fitting and handwritten digit recognition show that the proposed algorithms perform satisfactorily and robustly.
    
[^101]: 机器人系统的离散对称性: 基于群论和数据驱动分析的研究

    On discrete symmetries of robotics systems: A group-theoretic and data-driven analysis. (arXiv:2302.10433v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.10433](http://arxiv.org/abs/2302.10433)

    本文研究了运动系统的离散形态对称性，并提出了一个理论和实践框架，用于识别系统的形态对称群，并分析对称性在数据增强和控制设计中的应用。

    

    本文对运动系统的离散形态对称性进行了全面的研究，这在生物和人工运动系统中经常观察到，例如多腿、游泳和飞行的动物/机器人/虚拟角色。这些对称性源自系统形态中一个或多个平面/轴的对称性存在，导致身体部件的谐波复制和分布。我们阐述了形态对称性如何延伸到系统动力学、最优控制策略以及与系统动力学演化相关的所有本体感和外感测量中的对称性。在数据驱动方法的背景下，对称性代表一种归纳偏置，证明了数据增强或对称函数逼近器的使用。为了解决这个问题，我们提出了一个理论和实践框架，用于识别系统的形态对称群G并描述其在本体感和外感测量、最优控制策略以及系统动力学方面的对称性。该框架涉及到数据驱动和群论工具，例如主成分分析，置换测试和表示理论。提出的方法学通过确定一个仿生脊椎机器人的对称群并分析其对数据增强和控制设计的影响来进行说明。

    We present a comprehensive study on discrete morphological symmetries of dynamical systems, which are commonly observed in biological and artificial locomoting systems, such as legged, swimming, and flying animals/robots/virtual characters. These symmetries arise from the presence of one or more planes/axis of symmetry in the system's morphology, resulting in harmonious duplication and distribution of body parts. Significantly, we characterize how morphological symmetries extend to symmetries in the system's dynamics, optimal control policies, and in all proprioceptive and exteroceptive measurements related to the system's dynamics evolution. In the context of data-driven methods, symmetry represents an inductive bias that justifies the use of data augmentation or symmetric function approximators. To tackle this, we present a theoretical and practical framework for identifying the system's morphological symmetry group $\G$ and characterizing the symmetries in proprioceptive and exteroc
    
[^102]: 将黑匣子分解为可解释模型的混合物：路线规划，解释，重复。

    Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10289](http://arxiv.org/abs/2302.10289)

    本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。

    

    机器学习模型设计要么从解释性模型开始，要么从黑盒开始并事后解释。黑盒模型灵活但难以解释，而解释性模型本质上是可解释的。然而，解释性模型需要广泛的机器学习知识，并且往往比它们的黑盒变体不够灵活和表现不佳。本文旨在模糊黑盒的事后解释和构建可解释模型之间的界限。我们从黑盒开始，迭代地Carve出一种混合解释模型（MoIE）和一个残余网络。每个可解释模型专门处理一个样本子集，并使用一阶逻辑(FOL)对其进行解释，从黑盒中提供基本推理概念。我们通过灵活的残差路由其余的样本。我们在残转网络上重复该方法，直到所有可解释模型解释所需比例的数据。我们进行了大量实验，结果表明我们的路线规划，解释和重复方法在各种数据集上优于目前几种黑匣子模型解释方法，并产生高度可解释的模型。

    ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
    
[^103]: 黑盒批量主动学习回归

    Black-Box Batch Active Learning for Regression. (arXiv:2302.08981v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08981](http://arxiv.org/abs/2302.08981)

    本文提出了一种黑盒批量主动学习方法，该方法仅利用模型预测进行评估，适用于各种机器学习模型，并扩展了一系列白盒批量主动学习方法。

    

    批量主动学习是一种常用的方法，可以通过重复获取数据点的标签来高效地训练机器学习模型，尤其是对于大规模的初始未标记数据集。然而，许多最近的批量主动学习方法都是白盒方法，并且通常仅限于可微分的参数模型：它们使用基于模型嵌入或一阶和二阶导数的获取函数对未标记的数据点进行评分。在本文中，我们提出了一种黑盒批量主动学习回归任务的方法，作为白盒方法的扩展。关键的是，我们的方法仅依赖于模型的预测。这种方法适用于各种机器学习模型，包括常规的和贝叶斯深度学习模型以及非可微分模型，如随机森林。它基于贝叶斯原则，并利用最近的基于核的方法。这使得我们能够扩展一系列现有的最先进的白盒批量主动学习方法（BADGE）。

    Batch active learning is a popular approach for efficiently training machine learning models on large, initially unlabelled datasets by repeatedly acquiring labels for batches of data points. However, many recent batch active learning methods are white-box approaches and are often limited to differentiable parametric models: they score unlabeled points using acquisition functions based on model embeddings or first- and second-order derivatives. In this paper, we propose black-box batch active learning for regression tasks as an extension of white-box approaches. Crucially, our method only relies on model predictions. This approach is compatible with a wide range of machine learning models, including regular and Bayesian deep learning models and non-differentiable models such as random forests. It is rooted in Bayesian principles and utilizes recent kernel-based approaches. This allows us to extend a wide range of existing state-of-the-art white-box batch active learning methods (BADGE,
    
[^104]: 分布式潜变量模型的联合变分推断方法

    Federated Variational Inference Methods for Structured Latent Variable Models. (arXiv:2302.03314v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.03314](http://arxiv.org/abs/2302.03314)

    本论文提出了一种基于结构化变分推断的联合学习方法，该方法适用于各种结构化概率模型。此外，还提供了一种通信高效的变体算法，证明了所提出算法的有效性和性能。

    

    联邦学习方法使得在数据分布的情况下进行模型训练成为可能，而无需数据离开其原始位置，并且在各个领域引起了越来越多的关注。然而，现有方法存在局限性，无法应用于许多结构化概率模型。我们提出了一种基于结构化变分推断的通用而优雅的解决方案，并针对联邦学习环境进行了适应。此外，我们提供了一种类似于经典FedAvg算法的通信高效型变体。通过与分层贝叶斯神经网络和主题模型进行比较，我们证明了所提出算法的有效性和性能。

    Federated learning methods enable model training across distributed data sources without data leaving their original locations and have gained increasing interest in various fields. However, existing approaches are limited, excluding many structured probabilistic models. We present a general and elegant solution based on structured variational inference, widely used in Bayesian machine learning, adapted for the federated setting. Additionally, we provide a communication-efficient variant analogous to the canonical FedAvg algorithm. The proposed algorithms' effectiveness is demonstrated, and their performance is compared with hierarchical Bayesian neural networks and topic models.
    
[^105]: 深度强化学习中的自动内在奖励塑造探索方法研究

    Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10886](http://arxiv.org/abs/2301.10886)

    本文提出了一种名为AIRS的自动内在奖励塑造探索方法，可以提供高质量的内在激励以增强强化学习中的探索性能；并开发了高效可靠的内在奖励工具包。实验表明，AIRS性能卓越，能够胜过基准方案。

    

    本文提出了一种名为AIRS的自动内在奖励塑造方法，通过智能和适应性的塑造函数，提供高质量的内在激励以增强强化学习中的探索性能。AIRS可以根据实时估计的任务回报从预定义的函数集中选择塑造函数，提供可靠的探索激励并解决偏置目标问题。此外，我们开发了一个内在奖励工具包，提供多种内在奖励方法的高效可靠实现方式。我们将AIRS应用在MiniGrid、Procgen和DeepMind控制套件的多项任务中进行测试。大量仿真结果表明，AIRS可以胜过基准方案，并具有简单的架构和卓越的性能。

    We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
    
[^106]: 在排名中检测具有偏倚表示的群组

    Detection of Groups with Biased Representation in Ranking. (arXiv:2301.00719v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00719](http://arxiv.org/abs/2301.00719)

    本文研究了在排名中检测具有偏倚表示的群体的问题，并提出了两种公平度量方法的高效搜索算法。同时，提出了一种利用Shapley值解释群体表示偏差的方法。

    

    许多关键领域的实际决策工具都基于排名结果。随着对算法公平性的日益关注，最近的研究提出了用于排名公平性的度量方法。其中许多定义考虑在任何合理的k值中，不同的“受保护群体”在前k个排名项中的表示。如果已知保护的群体，确认算法的公平性是一个简单的任务。然而，群体的定义可能事先不知道。在本文中，我们研究了在前k个排名项目中检测具有偏倚表示的群体的问题，消除了预定义保护群体的需要。这样的群体数量可能是指数级的，使得问题变得困难。我们提出了高效的搜索算法来解决两种不同的公平度量方法：全局表示边界和比例表示。然后，我们提出了一种利用Shapley值概念解释群体表示偏差的方法。

    Real-life tools for decision-making in many critical domains are based on ranking results. With the increasing awareness of algorithmic fairness, recent works have presented measures for fairness in ranking. Many of those definitions consider the representation of different ``protected groups'', in the top-$k$ ranked items, for any reasonable $k$. Given the protected groups, confirming algorithmic fairness is a simple task. However, the groups' definitions may be unknown in advance. In this paper, we study the problem of detecting groups with biased representation in the top-$k$ ranked items, eliminating the need to pre-define protected groups. The number of such groups possible can be exponential, making the problem hard. We propose efficient search algorithms for two different fairness measures: global representation bounds, and proportional representation. Then we propose a method to explain the bias in the representations of groups utilizing the notion of Shapley values. We conclud
    
[^107]: 可扩展的多智能体强化学习在仓库物流中与机器人和人类同事合作

    Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers. (arXiv:2212.11498v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11498](http://arxiv.org/abs/2212.11498)

    该论文提出了一种可扩展的多智能体强化学习方法，用于仓库物流中的机器人和人类同事合作。他们通过分层的MARL算法，让经理和工人代理根据全局目标进行协同训练，以最大化拣货速率。

    

    我们设想一个仓库里有数十个移动机器人和人类分拣员一起工作，收集和交付仓库内的物品。我们要解决的基本问题是称为拣货问题，即这些工作代理人如何在仓库中协调他们的移动和行为以最大化性能（例如订单吞吐量）。传统的行业方法使用启发式方法需要大量的工程努力来为固有可变的仓库配置进行优化。相比之下，多智能体强化学习（MARL）可以灵活地应用于不同的仓库配置（例如大小，布局，工人数量/类型，物品补充频率），因为代理人通过经验学习如何最优地相互合作。我们开发了分层MARL算法，其中一个管理者为工人代理分配目标，并且管理者和工人的策略被共同训练以最大化全局目标（例如拣货速率）。

    We envision a warehouse in which dozens of mobile robots and human pickers work together to collect and deliver items within the warehouse. The fundamental problem we tackle, called the order-picking problem, is how these worker agents must coordinate their movement and actions in the warehouse to maximise performance (e.g. order throughput). Established industry methods using heuristic approaches require large engineering efforts to optimise for innately variable warehouse configurations. In contrast, multi-agent reinforcement learning (MARL) can be flexibly applied to diverse warehouse configurations (e.g. size, layout, number/types of workers, item replenishment frequency), as the agents learn through experience how to optimally cooperate with one another. We develop hierarchical MARL algorithms in which a manager assigns goals to worker agents, and the policies of the manager and workers are co-trained toward maximising a global objective (e.g. pick rate). Our hierarchical algorith
    
[^108]: 一种基于强化学习的模因算法用于社会技术生产调度

    A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling. (arXiv:2212.10936v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10936](http://arxiv.org/abs/2212.10936)

    本文提出了一种基于深度强化学习的模因算法，用于解决具有实际约束的灵活生产调度问题，并弥补元启发式研究中的缺陷。

    

    本文提出了一个应用深度强化学习的模因算法，用于解决实际双资源约束柔性作业车间调度问题（DRC-FJSSP）。近年来，对于DRL技术已经进行了广泛的研究，但是没有考虑到现实、灵活和以人为中心的车间。本文发现，在以订单为导向的间歇性制造中存在一个研究空白，它经常在具有高服务水平的中小型公司中表示。从这一领域的实际工业项目中，我们认识到需要描述灵活的机器、人工工作者和能力、设置和处理操作、物料到达时间、具有并行任务的复杂作业路径以进行物料清单（BOM）制造、顺序相关设置时间和（部分）自动化任务。另一方面，在DRC-FJSSP的背景下，已经进行了大量的元启发式研究。然而，缺乏适当的方法来解决生产过程的复杂性和不确定性，这是现实工业世界中面临的主要挑战。因此，提出了一种新的算法，以弥补相关领域的缺陷。

    The following article presents a memetic algorithm with applying deep reinforcement learning (DRL) for solving practically oriented dual resource constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years, there has been extensive research on DRL techniques, but without considering realistic, flexible and human-centered shopfloors. A research gap can be identified in the context of make-to-order oriented discontinuous manufacturing as it is often represented in medium-size companies with high service levels. From practical industry projects in this domain, we recognize requirements to depict flexible machines, human workers and capabilities, setup and processing operations, material arrival times, complex job paths with parallel tasks for bill of material (BOM) manufacturing, sequence-depended setup times and (partially) automated tasks. On the other hand, intensive research has been done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of sui
    
[^109]: 高效节约内存的NLLB-200：针对大规模多语言机器翻译模型的语言特定专家删减

    Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09811](http://arxiv.org/abs/2212.09811)

    本研究提出了一种节约内存的NLLB-200模型修剪方法，可在保持翻译质量的同时移除多达80％的专家，使得在单个32GB的GPU上运行模型成为可能。这对于大规模多语言机器翻译具有重要的意义。

    

    与传统的双语翻译系统相比，大规模多语言机器翻译具有吸引力，因为一个单一模型可以翻译成多种语言，并从知识转移中获益，尤其是对于低资源语言。然而，大规模多语言模型受到多语言性的限制，除非进行大规模扩展，否则会增加训练和推理成本。稀疏的专家混合模型是一种在不需要大量计算的情况下大幅增加模型容量的方法。最近发布的NLLB-200是这样一个模型的例子。它涵盖了202种语言，但仅推理就需要至少四个32GB的GPU。在这项工作中，我们提出了一种修剪方法，允许删除多达80％的专家，但翻译质量几乎没有损失，这使得在单个32GB的GPU上运行该模型成为可能。进一步分析表明，我们的修剪度量指标可以识别出语言特定的专家

    Compared to conventional bilingual translation systems, massively multilingual machine translation is appealing because a single model can translate into multiple languages and benefit from knowledge transfer for low resource languages. On the other hand, massively multilingual models suffer from the curse of multilinguality, unless scaling their size massively, which increases their training and inference costs. Sparse Mixture-of-Experts models are a way to drastically increase model capacity without the need for a proportional amount of computing. The recently released NLLB-200 is an example of such a model. It covers 202 languages but requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that allows the removal of up to 80\% of experts with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics allow to identify language-specific experts and p
    
[^110]: 通过多模式对比学习研究恶意模因的演化

    On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning. (arXiv:2212.06573v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2212.06573](http://arxiv.org/abs/2212.06573)

    本文提出了一个框架，利用多模式对比学习模型来识别恶意内容的目标，并系统地研究恶意模因的演化。通过研究反犹太主义模因，我们发现了CLIP生成的嵌入向量中存在的语义规律，从而揭示了恶意模因的创造方式。

    

    在社交媒体平台和现实世界中，恶意模因的传播对社会产生了不良影响。检测恶意模因是一项具有挑战性的任务，其中一个原因是模因的演化性质；新的恶意模因可以通过将恶意内涵与其他文化观念或符号融合而出现。本文提出了一个框架，利用多模式对比学习模型（特别是OpenAI的CLIP）来识别恶意内容的目标，并系统地研究恶意模因的演化。我们发现CLIP生成的嵌入向量中存在语义规律，描述了同一模态（图像）或跨模态（图像和文本）之间的语义关系。利用这一属性，我们研究了如何通过结合多个图像的可视化元素或将文本信息与恶意图像融合来创建恶意模因。通过重点研究反犹太主义模因，我们展示了我们的框架分析恶意模因演化的能力。

    The dissemination of hateful memes online has adverse effects on social media platforms and the real world. Detecting hateful memes is challenging, one of the reasons being the evolutionary nature of memes; new hateful memes can emerge by fusing hateful connotations with other cultural ideas or symbols. In this paper, we propose a framework that leverages multimodal contrastive learning models, in particular OpenAI's CLIP, to identify targets of hateful content and systematically investigate the evolution of hateful memes. We find that semantic regularities exist in CLIP-generated embeddings that describe semantic relationships within the same modality (images) or across modalities (images and text). Leveraging this property, we study how hateful memes are created by combining visual elements from multiple images or fusing textual information with a hateful image. We demonstrate the capabilities of our framework for analyzing the evolution of hateful memes by focusing on antisemitic me
    
[^111]: 使用学习的规范化函数实现等变性

    Equivariance with Learned Canonicalization Functions. (arXiv:2211.06489v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.06489](http://arxiv.org/abs/2211.06489)

    本文提出了一种使用学习的规范化函数来实现等变性的方法，避免了对神经网络架构的限制。实验结果表明，学习规范化函数可以在多个任务中与现有技术相比具有竞争力，并且速度更快。

    

    基于对称性的神经网络通常通过限制架构来实现对一组变换的不变性或等变性。在本文中，我们提出了一种替代方法，通过学习产生数据的规范表示来避免这种架构约束。这些规范化函数可以直接插入非等变主干架构中。我们提供了一些感兴趣的群组的明确实现方法。我们表明，这种方法具有普适性，同时提供可解释的洞察力。我们的主要假设得到了我们的实证结果的支持，即学习一个小的神经网络来执行规范化优于使用预定义的启发式。我们的实验结果表明，学习规范化函数在许多任务中，包括图像分类、$N$体动力学预测、点云分类和部分分割等学习等变函数的现有技术相比具有竞争力，同时速度更快。

    Symmetry-based neural networks often constrain the architecture in order to achieve invariance or equivariance to a group of transformations. In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data. These canonicalization functions can readily be plugged into non-equivariant backbone architectures. We offer explicit ways to implement them for some groups of interest. We show that this approach enjoys universality while providing interpretable insights. Our main hypothesis, supported by our empirical results, is that learning a small neural network to perform canonicalization is better than using predefined heuristics. Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including image classification, $N$-body dynamics prediction, point cloud classification and part segmentation, while being fas
    
[^112]: 宽度优先的流水线并行计算方法

    Breadth-First Pipeline Parallelism. (arXiv:2211.05953v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2211.05953](http://arxiv.org/abs/2211.05953)

    宽度优先的流水线并行计算方法结合了流水线和数据并行计算，通过在每个GPU上使用小批量大小和完全分片的数据并行计算，以提高训练吞吐量。在实验中，与Megatron-LM相比，在一个520亿参数的模型上，使用小批量大小每个GPU的训练吞吐量增加了高达43%。

    

    我们引入了一种新的训练调度方法——宽度优先的流水线并行计算，该方法优化了流水线和数据并行计算的结合。宽度优先的流水线并行计算通过在每个GPU上使用较小的批量大小并结合完全分片的数据并行计算，实现了高GPU利用率、降低训练时间、成本和内存使用。实验证明，相对于Megatron-LM，对于一个520亿参数的模型，使用较小的批量大小每个GPU的训练吞吐量增加了高达43%，从而在大型GPU集群上将训练时间和成本同样降低了。

    We introduce Breadth-First Pipeline Parallelism, a novel training schedule which optimizes the combination of pipeline and data parallelism. Breadth-First Pipeline Parallelism lowers training time, cost and memory usage by combining a high GPU utilization with a small batch size per GPU, and by making use of fully sharded data parallelism. Experimentally, we observed an increase of up to 43% in training throughput for a 52 billion-parameter model using a small batch size per GPU compared to Megatron-LM, which would reduce the training time and cost by the same amount on a large GPU cluster.
    
[^113]: Layer Ensembles. (arXiv:2210.04882v3 [cs.LG] UPDATED)

    Layer Ensembles. (arXiv:2210.04882v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04882](http://arxiv.org/abs/2210.04882)

    本文介绍了一种基于独立分类分布的不确定性估计方法，通过重复使用公共的层输出和对样本进行排序，可以大幅提高模型运行速度、降低内存使用，并且获得比常规深度集成更高的不确定性质量。

    

    深度集成是一种贝叶斯神经网络，可以通过收集每个网络的预测结果并计算预测结果之间的差异来估计多个神经网络预测的不确定性。在本文中，我们介绍了一种不确定性估计方法，该方法考虑了网络每一层的一组独立的分类分布，与常规的深度集成相比具有更多可能的样本，这些样本在层之间重叠。我们进一步引入了一种优化的推断过程，该过程可以重复使用公共的层输出，从而实现高达19倍的加速和二次减少内存使用。我们还展示了通过对样本进行排序可以进一步改进该方法，从而得到需要更少内存和时间来运行并且具有比深度集成更高不确定性质量的模型。

    Deep Ensembles, as a type of Bayesian Neural Networks, can be used to estimate uncertainty on the prediction of multiple neural networks by collecting votes from each network and computing the difference in those predictions. In this paper, we introduce a method for uncertainty estimation that considers a set of independent categorical distributions for each layer of the network, giving many more possible samples with overlapped layers than in the regular Deep Ensembles. We further introduce an optimized inference procedure that reuses common layer outputs, achieving up to 19x speed up and reducing memory usage quadratically. We also show that the method can be further improved by ranking samples, resulting in models that require less memory and time to run while achieving higher uncertainty quality than Deep Ensembles.
    
[^114]: 优先级拟阵中值问题的双目标近似算法

    Bicriteria Approximation Algorithms for Priority Matroid Median. (arXiv:2210.01888v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2210.01888](http://arxiv.org/abs/2210.01888)

    本文研究了优先级拟阵中值问题，提出了双目标近似算法，通过选择合适的设施子集实现同时最小化开放成本和满足约束条件。

    

    公平性考虑在最近几年中推动了新的聚类问题和算法。本文考虑了优先级拟阵中值问题，它是对最近研究的优先级k-中值问题的一般化。输入包括一组设施$\mathcal{F}$和一组客户$\mathcal{C}$，它们位于度量空间$(\mathcal{F} \cup \mathcal{C},d)$中，以及一个拟阵$\mathcal{M}=(\mathcal{F},\mathcal{I})$，表示设施的集合。此外，每个客户$j$都有一个指定的半径$r_j \ge 0$，每个设施$i \in \mathcal{F}$都有一个开放成本$f_i$。目标是选择一组设施的子集$S \subseteq \mathcal{F}$，使得$\sum_{i \in \mathcal{F}} f_i + \sum_{j \in \mathcal{C}} d(j,S)$最小，同时满足两个约束条件：(i) $S$是$\mathcal{M}$中的独立集（即$S \in \mathcal{I}$），(ii) 对于每个客户$j$，它到一个开放设施的距离最多为$r_j$（即$d(j,S) \le r_j$）。

    Fairness considerations have motivated new clustering problems and algorithms in recent years. In this paper we consider the Priority Matroid Median problem which generalizes the Priority $k$-Median problem that has recently been studied. The input consists of a set of facilities $\mathcal{F}$ and a set of clients $\mathcal{C}$ that lie in a metric space $(\mathcal{F} \cup \mathcal{C},d)$, and a matroid $\mathcal{M}=(\mathcal{F},\mathcal{I})$ over the facilities. In addition each client $j$ has a specified radius $r_j \ge 0$ and each facility $i \in \mathcal{F}$ has an opening cost $f_i$. The goal is to choose a subset $S \subseteq \mathcal{F}$ of facilities to minimize the $\sum_{i \in \mathcal{F}} f_i + \sum_{j \in \mathcal{C}} d(j,S)$ subject to two constraints: (i) $S$ is an independent set in $\mathcal{M}$ (that is $S \in \mathcal{I}$) and (ii) for each client $j$, its distance to an open facility is at most $r_j$ (that is, $d(j,S) \le r_j$). For this problem we describe the first
    
[^115]: TabLeak：联邦学习中的表格数据泄漏

    TabLeak: Tabular Data Leakage in Federated Learning. (arXiv:2210.01785v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01785](http://arxiv.org/abs/2210.01785)

    TabLeak是第一个针对表格数据的全面重构攻击的方法，解决了联邦学习中表格数据泄漏的挑战，包括解决混合离散连续优化问题和实现人类评估。

    

    尽管联邦学习（FL）承诺保护隐私，但最近在图像和文本领域的研究显示，训练更新会泄露私人客户数据。然而，在高风险的FL应用（例如医疗和金融）中使用的大多数表格数据中，数据泄露的风险尚未被探索。成功的表格数据攻击必须解决两个领域独特的关键挑战：（i）解决高方差混合离散连续优化问题的方法，以及（ii）使人类能够评估重构，因为与图像和文本数据不同，无法直接人工检查。在这项工作中，我们解决了这些挑战，并提出了TabLeak，第一个针对表格数据的全面重构攻击。TabLeak基于两个关键贡献：(i)一种利用softmax放松和池化集成解决优化问题的方法， (ii)一种基于熵的不确定性量化方案，以实现人类评估。

    While federated learning (FL) promises to preserve privacy, recent works in the image and text domains have shown that training updates leak private client data. However, most high-stakes applications of FL (e.g., in healthcare and finance) use tabular data, where the risk of data leakage has not yet been explored. A successful attack for tabular data must address two key challenges unique to the domain: (i) obtaining a solution to a high-variance mixed discrete-continuous optimization problem, and (ii) enabling human assessment of the reconstruction as unlike for image and text data, direct human inspection is not possible. In this work we address these challenges and propose TabLeak, the first comprehensive reconstruction attack on tabular data. TabLeak is based on two key contributions: (i) a method which leverages a softmax relaxation and pooled ensembling to solve the optimization problem, and (ii) an entropy-based uncertainty quantification scheme to enable human assessment. We e
    
[^116]: 避免后处理: 基于事件检测的生物医学信号处理

    Avoiding Post-Processing with Event-Based Detection in Biomedical Signals. (arXiv:2209.11007v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2209.11007](http://arxiv.org/abs/2209.11007)

    这项研究提出了一种基于事件检测的生物医学信号处理框架，避免了繁琐的后处理过程，直接将事件作为学习目标，拥有较强的实用性能。

    

    目标: 在生物医学信号处理中，寻找感兴趣的事件是一项常见任务。癫痫发作和信号伪影检测是两个关键的例子。基于时段的分类是典型的机器学习框架，用于检测这些信号事件，因为可以直接应用经典机器学习技术。通常，需要后处理才能实现良好的性能并强制时间依赖性。设计合适的后处理方案来将这些分类输出转化为事件是这一框架中费时费力的部分。方法: 我们提出了一种基于事件的建模框架，直接使用事件作为学习目标，摒弃了临时的后处理方案将模型输出转化为事件。我们通过模拟数据和真实世界数据展示了这种框架的实际效果，并将其与基于时段的建模方法进行比较。结果: 我们表明，基于事件的建模（无需后处理）具有实用性能。

    Objective: Finding events of interest is a common task in biomedical signal processing. The detection of epileptic seizures and signal artefacts are two key examples. Epoch-based classification is the typical machine learning framework to detect such signal events because of the straightforward application of classical machine learning techniques. Usually, post-processing is required to achieve good performance and enforce temporal dependencies. Designing the right post-processing scheme to convert these classification outputs into events is a tedious, and labor-intensive element of this framework. Methods: We propose an event-based modeling framework that directly works with events as learning targets, stepping away from ad-hoc post-processing schemes to turn model outputs into events. We illustrate the practical power of this framework on simulated data and real-world data, comparing it to epoch-based modeling approaches. Results: We show that event-based modeling (without post-proce
    
[^117]: 使用异方差噪声模型识别特定患者的根本原因

    Identifying Patient-Specific Root Causes with the Heteroscedastic Noise Model. (arXiv:2205.13085v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.13085](http://arxiv.org/abs/2205.13085)

    本研究使用异方差噪声模型，开发了一种名为广义根因因果推断（GRCI）的算法，能够更准确地识别疾病的患者特定根本原因。

    

    复杂疾病由许多因素引起，即使在相同的诊断类别中，这些因素可能在患者之间也会有所不同。然而，一些潜在的根本原因仍然可能在每个患者中引发疾病的发展。因此，我们专注于识别疾病的患者特定根本原因，即在结构方程模型中外生误差项的样本特定预测能力。我们从线性设置推广到异方差噪声模型，其中$ Y = m（X）+ \varepsilon\sigma（X）$，其中$m（X）$和$\sigma（X）$分别代表条件均值和均值绝对偏差的非线性函数。该模型保持可识别性，但引入了一些非平凡的挑战，需要一种名为广义根因因果推断（GRCI）的定制算法来正确提取误差项。与现有的替代方法相比，GRCI更准确地恢复了特定患者的根本原因。

    Complex diseases are caused by a multitude of factors that may differ between patients even within the same diagnostic category. A few underlying root causes may nevertheless initiate the development of disease within each patient. We therefore focus on identifying patient-specific root causes of disease, which we equate to the sample-specific predictivity of the exogenous error terms in a structural equation model. We generalize from the linear setting to the heteroscedastic noise model where $Y = m(X) + \varepsilon\sigma(X)$ with non-linear functions $m(X)$ and $\sigma(X)$ representing the conditional mean and mean absolute deviation, respectively. This model preserves identifiability but introduces non-trivial challenges that require a customized algorithm called Generalized Root Causal Inference (GRCI) to extract the error terms correctly. GRCI recovers patient-specific root causes more accurately than existing alternatives.
    
[^118]: 通过交叉重构变压器进行自监督时间序列表示学习

    Self-Supervised Time Series Representation Learning via Cross Reconstruction Transformer. (arXiv:2205.09928v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09928](http://arxiv.org/abs/2205.09928)

    本文提出了一种称为交叉重构变压器(CRT)的方法，通过跨域丢弃-重构任务实现了时间序列的表示学习。CRT解决了构建数据对的先验知识、采样策略繁琐和采样偏差导致性能不稳定等问题，同时有效地建模了跨时谱关系以扩展表示的容量。

    

    无监督/自监督的时间序列表示学习对于真实场景中稀缺标记样本至关重要。现有方法主要利用对比学习框架，自动学习理解相似和不相似的数据对。然而，它们受限于构建对的先验知识、繁琐的采样策略以及在遇到采样偏差时性能不稳定。此外，很少有工作专注于有效地建模跨时谱关系以扩展表示的容量。本文旨在从新的角度学习时间序列的表示，并提出交叉重构变压器(CRT)以统一地解决上述问题。CRT通过跨域丢弃-重构任务实现时间序列表示学习。具体而言，我们将时间序列转换为频域，并随机丢弃时间和频率域中的某些部分，然后通过重构任务来学习表示。

    Unsupervised/self-supervised representation learning in time series is critical since labeled samples are usually scarce in real-world scenarios. Existing approaches mainly leverage the contrastive learning framework, which automatically learns to understand the similar and dissimilar data pairs. Nevertheless, they are restricted to the prior knowledge of constructing pairs, cumbersome sampling policy, and unstable performances when encountering sampling bias. Also, few works have focused on effectively modeling across temporal-spectral relations to extend the capacity of representations. In this paper, we aim at learning representations for time series from a new perspective and propose Cross Reconstruction Transformer (CRT) to solve the aforementioned problems in a unified way. CRT achieves time series representation learning through a cross-domain dropping-reconstruction task. Specifically, we transform time series into the frequency domain and randomly drop certain parts in both ti
    
[^119]: k-strip: 一种在k空间中应用于头骨剥离的新型分割算法

    k-strip: A novel segmentation algorithm in k-space for the application of skull stripping. (arXiv:2205.09706v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2205.09706](http://arxiv.org/abs/2205.09706)

    k-strip是一种新型的基于深度学习的头骨剥离算法，该算法直接在信息丰富的k空间中工作，并且在与金标准的比较中取得了较高的相似性。

    

    目标：提出一种新型的基于深度学习的头骨剥离算法，该算法可以直接在信息丰富的k空间中工作。方法：使用来自不同机构的两个数据集共计36,900个MRI切片，我们训练了一个基于深度学习的模型，直接处理复杂的原始k空间数据。在图像域中使用HD-BET（脑提取工具）进行的头骨剥离被用作金标准。结果：两个数据集与金标准非常相似（DICE分数为92\%-98\%，Hausdorff距离低于5.5毫米）。眼部以上切片的DICE分数达到了99\%，而眼部周围和下方的准确性下降，输出部分模糊。k-strip的输出通常在与头骨之间的分界处平滑边缘。使用适当的阈值创建二进制掩模。结论：通过这个概念验证研究，我们能够展示在k空间中工作的可行性。

    Objectives: Present a novel deep learning-based skull stripping algorithm for magnetic resonance imaging (MRI) that works directly in the information rich k-space.  Materials and Methods: Using two datasets from different institutions with a total of 36,900 MRI slices, we trained a deep learning-based model to work directly with the complex raw k-space data. Skull stripping performed by HD-BET (Brain Extraction Tool) in the image domain were used as the ground truth.  Results: Both datasets were very similar to the ground truth (DICE scores of 92\%-98\% and Hausdorff distances of under 5.5 mm). Results on slices above the eye-region reach DICE scores of up to 99\%, while the accuracy drops in regions around the eyes and below, with partially blurred output. The output of k-strip often smoothed edges at the demarcation to the skull. Binary masks are created with an appropriate threshold.  Conclusion: With this proof-of-concept study, we were able to show the feasibility of working in th
    
[^120]: GRAPHSHAP：通过模样式的语言解释基于身份感知的图分类器

    GRAPHSHAP: Explaining Identity-Aware Graph Classifiers Through the Language of Motifs. (arXiv:2202.08815v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.08815](http://arxiv.org/abs/2202.08815)

    GRAPHSHAP是一种基于Shapley值的方法，可以提供基于模样式的解释，用于解释基于身份感知的图分类器，而无需对模型或其训练数据有任何先验知识。

    

    大多数解释黑盒分类器的方法（例如在表格数据、图像或时间序列上）依赖于衡量删除/扰动特征对模型输出的影响。这要求解释语言与分类器的特征空间匹配。然而，在处理图数据时，基本特征对应于描述图结构的边，特征空间与解释语言之间的匹配可能不合适。为了为图分类任务开发可行的解释，将特征空间（边）与所需的高级解释语言（如模样式）解耦是一个重大挑战。在本文中，我们引入了一种基于Shapley值的方法GRAPHSHAP，能够为身份感知的图分类器提供基于模样式的解释，不需要对模型或其训练数据有任何先验知识：唯一的要求是可以任意查询黑盒分类器。

    Most methods for explaining black-box classifiers (e.g. on tabular data, images, or time series) rely on measuring the impact that removing/perturbing features has on the model output. This forces the explanation language to match the classifier's feature space. However, when dealing with graph data, in which the basic features correspond to the edges describing the graph structure, this matching between features space and explanation language might not be appropriate. Decoupling the feature space (edges) from a desired high-level explanation language (such as motifs) is thus a major challenge towards developing actionable explanations for graph classification tasks. In this paper we introduce GRAPHSHAP, a Shapley-based approach able to provide motif-based explanations for identity-aware graph classifiers, assuming no knowledge whatsoever about the model or its training data: the only requirement is that the classifier can be queried as a black-box at will. For the sake of computationa
    
[^121]: 基于模型的强化学习中遵循奖励的子任务

    Reward-Respecting Subtasks for Model-Based Reinforcement Learning. (arXiv:2202.03466v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03466](http://arxiv.org/abs/2202.03466)

    论文提出了一种基于模型的强化学习的方法，通过添加奖励加成的子任务来发现选项，从而解决了以前方法中忽略原始奖励的问题。

    

    为了实现人工智能的远大目标，强化学习必须包括对抽象状态和时间的世界模型的规划。深度学习在状态抽象方面取得了进展，但时间抽象却很少被使用，尽管基于选项框架已经广泛发展了理论。其中一个原因是可能的选项空间很大，以前提出的选项发现方法没有考虑到选项模型在规划中的使用方式。通常通过提出子任务（例如达到瓶颈状态或最大化除奖励外的感知信号的累积和）来发现选项。解决每个子任务以生成一个选项，然后学习选项的模型并使其可用于规划过程。在大多数以前的研究中，子任务忽略了原始问题上的奖励，而我们提出的子任务使用原始奖励加上基于某个特征的奖励加成。

    To achieve the ambitious goals of artificial intelligence, reinforcement learning must include planning with a model of the world that is abstract in state and time. Deep learning has made progress with state abstraction, but temporal abstraction has rarely been used, despite extensively developed theory based on the options framework. One reason for this is that the space of possible options is immense, and the methods previously proposed for option discovery do not take into account how the option models will be used in planning. Options are typically discovered by posing subsidiary tasks, such as reaching a bottleneck state or maximizing the cumulative sum of a sensory signal other than reward. Each subtask is solved to produce an option, and then a model of the option is learned and made available to the planning process. In most previous work, the subtasks ignore the reward on the original problem, whereas we propose subtasks that use the original reward plus a bonus based on a fe
    
[^122]: 基于SPD流形的深度最优传输领域自适应

    Deep Optimal Transport for Domain Adaptation on SPD Manifolds. (arXiv:2201.05745v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.05745](http://arxiv.org/abs/2201.05745)

    这项研究介绍了一种基于深度最优传输的方法，用于解决在SPD流形上的领域自适应问题。通过利用最优传输理论和SPD流形的对数欧几里得几何，我们克服了协方差矩阵操作的复杂性挑战。

    

    近年来，机器学习界对于在对称正定（SPD）流形上解决领域自适应（DA）问题表现出了很大兴趣。这种兴趣源于医疗设备产生的复杂神经物理数据（如脑电图、脑磁图和扩散张量成像）在不同领域之间存在数据分布的偏移。这些数据表示以信号协方差矩阵的形式表示，并具有对称性和正定性的属性。然而，由于协方差矩阵的复杂操作特性，直接将先前的经验和解决方案应用于DA问题存在挑战。为了解决这个问题，我们的研究引入了一类基于深度学习的迁移学习方法，称为深度最优传输。这一类方法利用最优传输理论，并利用SPD流形的对数欧几里得几何。此外，我们还展示了...

    In recent years, there has been significant interest in solving the domain adaptation (DA) problem on symmetric positive definite (SPD) manifolds within the machine learning community. This interest stems from the fact that complex neurophysiological data generated by medical equipment, such as electroencephalograms, magnetoencephalograms, and diffusion tensor imaging, often exhibit a shift in data distribution across different domains. These data representations, represented by signal covariance matrices, possess properties of symmetry and positive definiteness. However, directly applying previous experiences and solutions to the DA problem poses challenges due to the manipulation complexities of covariance matrices.To address this, our research introduces a category of deep learning-based transfer learning approaches called deep optimal transport. This category utilizes optimal transport theory and leverages the Log-Euclidean geometry for SPD manifolds. Additionally, we present a com
    
[^123]: AI的创造力：用于促进深度强化学习的分层规划模型学习

    Creativity of AI: Hierarchical Planning Model Learning for Facilitating Deep Reinforcement Learning. (arXiv:2112.09836v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2112.09836](http://arxiv.org/abs/2112.09836)

    该论文提出了一种新颖的深度强化学习框架，通过在DRL中嵌入符号知识，解决了数据效率、解释性缺乏和可迁移性等关键问题。该框架通过循环训练过程，并利用学习的规划模型和符号选项来引导策略的改进。学到的符号选项减轻了对专家领域知识的要求，并提供了策略的内在可解释性，同时通过与符号规划模型的规划进一步提高了可迁移性和数据效率。

    

    尽管在实际应用中取得了巨大成功，但深度强化学习(DRL)仍然面临三个关键问题，即数据效率、解释性的缺乏和可迁移性。最近的研究显示，在DRL中嵌入符号知识有望解决这些挑战。受此启发，我们引入了一种新颖的具有符号选项的深度强化学习框架。我们的框架具有一个循环训练过程，通过与交互轨迹学习的规划模型（包括动作模型和分层任务网络模型）和符号选项来引导策略的改进。学到的符号选项减轻了对专家领域知识的要求，并提供了策略的内在可解释性。此外，通过与符号规划模型的规划，可进一步提高可迁移性和数据效率。

    Despite of achieving great success in real-world applications, Deep Reinforcement Learning (DRL) is still suffering from three critical issues, i.e., data efficiency, lack of the interpretability and transferability. Recent research shows that embedding symbolic knowledge into DRL is promising in addressing those challenges. Inspired by this, we introduce a novel deep reinforcement learning framework with symbolic options. Our framework features a loop training procedure, which enables guiding the improvement of policy by planning with planning models (including action models and hierarchical task network models) and symbolic options learned from interactive trajectories automatically. The learned symbolic options alleviate the dense requirement of expert domain knowledge and provide inherent interpretability of policies. Moreover, the transferability and data efficiency can be further improved by planning with the symbolic planning models. To validate the effectiveness of our framewor
    
[^124]: 如何在可解释模型 (以及人类) 中愚弄敌对示例

    When and How to Fool Explainable Models (and Humans) with Adversarial Examples. (arXiv:2107.01943v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.01943](http://arxiv.org/abs/2107.01943)

    本文研究了针对可解释机器学习模型的敌对攻击的可能性和限制，并提出了一个全面的框架来研究在人类评估下如何生成敌对示例。

    

    由于多种限制，如神经网络的解释性不足和对敌对示例或离群输入的鲁棒性不足，可靠部署诸如神经网络之类的机器学习模型仍然具有挑战性。在这篇探索性综述中，我们探讨了针对可解释机器学习模型的敌对攻击的可能性和限制。首先，我们将敌对示例的概念扩展到适用于可解释机器学习场景中，其中输入、输出分类和模型决策的解释由人类评估。接下来，我们提出了一个全面的框架来研究在人类评估下是否（以及如何）为可解释模型生成敌对示例，并引入和说明了新的攻击范式。特别是，我们的框架考虑了一系列相关但常常被忽略的因素，例如问题类型、用户专业知识或目标。

    Reliable deployment of machine learning models such as neural networks continues to be challenging due to several limitations. Some of the main shortcomings are the lack of interpretability and the lack of robustness against adversarial examples or out-of-distribution inputs. In this exploratory review, we explore the possibilities and limits of adversarial attacks for explainable machine learning models. First, we extend the notion of adversarial examples to fit in explainable machine learning scenarios, in which the inputs, the output classifications and the explanations of the model's decisions are assessed by humans. Next, we propose a comprehensive framework to study whether (and how) adversarial examples can be generated for explainable models under human assessment, introducing and illustrating novel attack paradigms. In particular, our framework considers a wide range of relevant yet often ignored factors such as the type of problem, the user expertise or the objective of the e
    
[^125]: F2A2: 灵活完全去中心化的合作多智能体强化学习的近似演员-评论家算法

    F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning. (arXiv:2004.11145v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2004.11145](http://arxiv.org/abs/2004.11145)

    本文提出了一个灵活完全分散化的演员-评论家多智能体强化学习框架，在大规模合作多智能体环境中，通过设计一个基于原始-对偶混合梯度下降的算法框架，可以分别学习每个智能体，并实现策略改进和价值评价。

    

    传统的中央集权的多智能体强化学习算法在复杂应用中有时不实用，因为智能体之间缺乏互动，存在维度灾难和计算复杂性。因此，出现了一些分散化的多智能体强化学习算法。然而，现有的分散化方法只能处理完全合作的设置，在训练过程中需要传输大量信息。传统的块坐标梯度下降方法可以简化计算，但会引起严重的偏差。本文提出了一个灵活的完全分散化的演员-评论家多智能体强化学习框架，可以组合大多数演员-评论家方法，并处理大规模一般合作多智能体环境。设计了一种基于原始-对偶混合梯度下降的算法框架，分别学习每个智能体来实现分散化。从每个智能体的角度来看，实现了策略改进和价值评价。

    Traditional centralized multi-agent reinforcement learning (MARL) algorithms are sometimes unpractical in complicated applications, due to non-interactivity between agents, curse of dimensionality and computation complexity. Hence, several decentralized MARL algorithms are motivated. However, existing decentralized methods only handle the fully cooperative setting where massive information needs to be transmitted in training. The block coordinate gradient descent scheme they used for successive independent actor and critic steps can simplify the calculation, but it causes serious bias. In this paper, we propose a flexible fully decentralized actor-critic MARL framework, which can combine most of actor-critic methods, and handle large-scale general cooperative multi-agent setting. A primal-dual hybrid gradient descent type algorithm framework is designed to learn individual agents separately for decentralization. From the perspective of each agent, policy improvement and value evaluatio
    
[^126]: 《向后特征修正：深度学习如何进行深度（分层）学习》

    Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning. (arXiv:2001.04413v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2001.04413](http://arxiv.org/abs/2001.04413)

    提出了一种新的原则 "向后特征修正"，通过训练过程中自动修正较低层次特征的错误，使得深度学习能够进行深度（分层）学习。

    

    深度学习也被称为分层学习，学习者通过将复杂的目标函数分解为一系列更简单的函数来降低样本和时间复杂度。本文通过对训练目标进行随机梯度下降，从理论上分析了多层神经网络如何有效和自动地进行这种分层学习。在概念上，我们提出了一种理论特征，即某些类型的深度（即超常层）神经网络在某些分层任务上仍然可以以高效率的样本和时间进行训练，而现有的算法（包括逐层训练、核方法等）均无法高效。我们建立了一种名为“向后特征修正”的新原则，在训练过程中，较低层次特征的错误可以自动修正。我们认为这是深度学习如何进行深度（分层）学习的关键所在。

    Deep learning is also known as hierarchical learning, where the learner _learns_ to represent a complicated target function by decomposing it into a sequence of simpler functions to reduce sample and time complexity. This paper formally analyzes how multi-layer neural networks can perform such hierarchical learning _efficiently_ and _automatically_ by SGD on the training objective.  On the conceptual side, we present a theoretical characterizations of how certain types of deep (i.e. super-constant layer) neural networks can still be sample and time efficiently trained on some hierarchical tasks, when no existing algorithm (including layerwise training, kernel method, etc) is known to be efficient. We establish a new principle called "backward feature correction", where the errors in the lower-level features can be automatically corrected when training together with the higher-level layers. We believe this is a key behind how deep learning is performing deep (hierarchical) learning, as 
    

