# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization.](http://arxiv.org/abs/2305.11095) | 本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。 |
| [^2] | [Universal Domain Adaptation from Foundation Models.](http://arxiv.org/abs/2305.11092) | 本论文对基于基础模型的通用域适应进行了研究，发现当前的UniDA方法无法超越基准表现，提出了一个简单的目标方法。 |
| [^3] | [Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces.](http://arxiv.org/abs/2305.11089) | 本文提出了一种新的理论公式来实现任意离散状态Markov过程的生成扩散模型，并介绍了一种名为“Blackout Diffusion”的应用，它可以从空图像中生成样本。 |
| [^4] | [Preference or Intent? Double Disentangled Collaborative Filtering.](http://arxiv.org/abs/2305.11084) | 本文提出了一种双重解缠协同过滤（DDCF）方法，该方法能够对意图和偏好因素进行分离，并通过解缠表示建立独立的稀疏偏好表示，从而提供更准确和可解释性的个性化推荐。 |
| [^5] | [A Comparative Study of Face Detection Algorithms for Masked Face Detection.](http://arxiv.org/abs/2305.11077) | 本文对口罩人脸检测算法进行了比较研究，评估了一组代表性的人脸检测器在口罩人脸检测方面的表现，并讨论了其性能的可能贡献因素。 |
| [^6] | [Enriching language models with graph-based context information to better understand textual data.](http://arxiv.org/abs/2305.11070) | 基于图形上下文信息的语言模型增强可更好的理解文本，实验表明该方法提高了BERT模型在Pubmed上的分类任务表现。 |
| [^7] | [Generating coherent comic with rich story using ChatGPT and Stable Diffusion.](http://arxiv.org/abs/2305.11067) | 本文介绍了一种利用ChatGPT和Stable Diffusion生成连贯漫画故事的方法，通过引入新的评估AI故事的方式，并使用LoRA、ControlNet等方法进行fine-tuning，取得了在角色忠实度和艺术风格上的最先进表现。 |
| [^8] | [PETAL: Physics Emulation Through Averaged Linearizations for Solving Inverse Problems.](http://arxiv.org/abs/2305.11056) | PETAL算法利用已知的物理知识，将正演模型的线性化嵌入到模型本身中。它通过训练一个替代正演模型来解决具有已知或部分已知基于物理模型的反问题。 |
| [^9] | [Small noise analysis for Tikhonov and RKHS regularizations.](http://arxiv.org/abs/2305.11055) | 该研究建立了一个小噪声分析框架，揭示了传统L2正则化范数的潜在不稳定性，并提出了一种自适应分数阶RKHS正则化器类来解决不稳定性，这些正则化器始终产生最佳的收敛速率。 |
| [^10] | [NODE-ImgNet: a PDE-informed effective and robust model for image denoising.](http://arxiv.org/abs/2305.11049) | 本文提出了一种新型的神经网络架构 NODE-ImgNet，该模型结合了神经常微分方程和卷积神经网络块，它是一种使用 PDE 的有效和稳健的图像去噪模型。该模型能够自然地避免学习过程中引入伪像，实现了更高准确性和参数效率，并在不同场景下展示了一致的有效性和优势。 |
| [^11] | [Difference of Submodular Minimization via DC Programming.](http://arxiv.org/abs/2305.11046) | 本文介绍了一种新的算法，利用DC规划算法来解决子模最小化问题，并证明收敛性质比现有算法更全面，同时在语音特征选择和文档摘要等应用中取得更好的性能。 |
| [^12] | [A unified framework for information-theoretic generalization bounds.](http://arxiv.org/abs/2305.11042) | 该论文提出了一种基于概率去相关引理和概率测度空间中一些其他技术的通用方法，可以得到新的学习算法的信息论泛化上限，并且能够恢复许多现有的泛化界，如基于互信息、条件互信息、随机chaining和PAC-Bayes不等式的界。 |
| [^13] | [High-dimensional Asymptotics of Denoising Autoencoders.](http://arxiv.org/abs/2305.11041) | 本文研究了去噪自编码器在高维极限下的性能表现，得出了去噪均方测试误差的闭式表达式，并揭示了有跳跃连接的自编码器相较于传统自编码器的优越性。 |
| [^14] | [Simulation of a Variational Quantum Perceptron using Grover's Algorithm.](http://arxiv.org/abs/2305.11040) | 本研究结合了量子变分电路和Grover算法，成功构建了新型的量子感知器QVP-G。对比经典感知器，我们证明了QVP-G在分类任务中比QVP更加准确并且更加高效。 |
| [^15] | [Deep PackGen: A Deep Reinforcement Learning Framework for Adversarial Network Packet Generation.](http://arxiv.org/abs/2305.11039) | 研究了网络入侵检测系统中的对抗攻击方法，提出了用深度强化学习生成对抗数据包的框架，旨在提高生成对抗性网络数据包的效果。 |
| [^16] | [Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature.](http://arxiv.org/abs/2305.11033) | 本文综述了最近文献中的可视化问答问题，提供了对该领域的深入分析和比较，包括结果、最新技术、常见错误以及未来研究的潜在改进点。 |
| [^17] | [Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL.](http://arxiv.org/abs/2305.11032) | 本文提出了一种乐观自然策略梯度的在线强化学习策略优化框架，采用乐观策略评估子程序以鼓励探索，适用于线性MDP，样本复杂度具有最优维度依赖关系。 |
| [^18] | [Massively Parallel Reweighted Wake-Sleep.](http://arxiv.org/abs/2305.11022) | 大规模并行重新加权唤醒-睡眠算法通过抽取$K^n$个可能的样本组合，避免了原方法中大量潜在变量数目导致有效性下降问题。 |
| [^19] | [Deep Metric Tensor Regularized Policy Gradient.](http://arxiv.org/abs/2305.11017) | 本文研究通过引入度量张量场$g_ab$将欧几里得策略参数空间推广到一般的黎曼流形上，并提出了一种新的深度度量张量正则化策略梯度算法。 |
| [^20] | [Mode Connectivity in Auction Design.](http://arxiv.org/abs/2305.11005) | 该论文研究了拍卖设计领域的一个基本问题，即最优拍卖设计。在研究中，作者证明了神经网络在一定条件下可以通过简单的分段线性路径连接不同的局部最优解，并取得了成功。 |
| [^21] | [Sharing Lifelong Reinforcement Learning Knowledge via Modulating Masks.](http://arxiv.org/abs/2305.10997) | 通过调制掩码机制来实现多个代理之间的特定知识共享和强化学习，最终实现高效的分布式终身学习。 |
| [^22] | [Understanding how Differentially Private Generative Models Spend their Privacy Budget.](http://arxiv.org/abs/2305.10994) | 本文分析了采用差分隐私训练的生成模型如何分配隐私预算，以及影响分配的因素。使用不同的模型适合于不同的任务和设置。 |
| [^23] | [SPENSER: Towards a NeuroEvolutionary Approach for Convolutional Spiking Neural Networks.](http://arxiv.org/abs/2305.10987) | SPENSER是一种基于神经进化算法探索的自动设计和参数化的SNNs 生成框架，可应用于图像分类。 |
| [^24] | [Learning Activation Functions for Sparse Neural Networks.](http://arxiv.org/abs/2305.10964) | 本论文针对稀疏神经网络的准确性下降问题，发现激活函数和超参数是导致问题的主要原因，提出学习为稀疏网络调整激活函数并分开超参数优化方案的解决方法。 |
| [^25] | [Actor-Critic Methods using Physics-Informed Neural Networks: Control of a 1D PDE Model for Fluid-Cooled Battery Packs.](http://arxiv.org/abs/2305.10952) | 使用演员-评论算法和物理解释神经网络，本文提出了一种用于控制冷却液冷却的电池包温度的方法，其实现了最优控制。 |
| [^26] | [In Defense of Pure 16-bit Floating-Point Neural Networks.](http://arxiv.org/abs/2305.10947) | 本文探讨了纯16位浮点神经网络的被忽视的效率，提供了理论分析来探讨16位和32位模型的差异，并可以定量解释16位模型与其32位对应物之间的条件。 |
| [^27] | [Lyapunov-Driven Deep Reinforcement Learning for Edge Inference Empowered by Reconfigurable Intelligent Surfaces.](http://arxiv.org/abs/2305.10931) | 本文提出了一种基于李雅普诺夫深度强化学习的动态学习算法，在可重构智能表面赋能的6G网络中，能够同时优化数据压缩方案、无线资源分配、计算资源和RIS反射参数，实现了能效推理分类。 |
| [^28] | [Multilingual Event Extraction from Historical Newspaper Adverts.](http://arxiv.org/abs/2305.10928) | 本文针对历史文本中未被充分开发的事件提取任务，介绍了一个新的多语言数据集，通过迁移学习和领域自适应技术，即使数据注释很少也可以获得出人意料的好结果。同时，所提出的模型提供了新的洞察力，展示了该时期奴役话语的语言使用和模式。研究表明，语言迁移可以有效用于历史文本中的多语言事件提取。 |
| [^29] | [Structural Pruning for Diffusion Models.](http://arxiv.org/abs/2305.10924) | 本文提出了一种名为Diff-Pruning的高效压缩方法，通过一个Taylor展开过程来识别重要权重，从而从预先存在的模型中学习轻量级扩散模型，性能稳定，并在训练效率上显著提高。 |
| [^30] | [Query Performance Prediction: From Ad-hoc to Conversational Search.](http://arxiv.org/abs/2305.10923) | 本文研究了针对从Ad-hoc到交互式搜索中查询性能预测(QPP)的有效方法，并探索了QPP方法在交互式搜索中是否具有推广应用的能力。 |
| [^31] | [Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement.](http://arxiv.org/abs/2305.10913) | 本文提出了一种利用语义先验细化的弱监督视觉-文本对齐方法，仅使用图像-句子对进行学习，其目标是实现实体表示中的区域-短语对应关系，通过联合两个主要模块的输出进行预测。 |
| [^32] | [RobustFair: Adversarial Evaluation through Fairness Confusion Directed Gradient Search.](http://arxiv.org/abs/2305.10906) | 本论文提出了一种基于公平混淆定向梯度搜索的谐波评估方法RobustFair，可以识别与虚假公平相结合的鲁棒性缺陷，提高DNN的鲁棒性和个体公平性。 |
| [^33] | [Estimation Beyond Data Reweighting: Kernel Method of Moments.](http://arxiv.org/abs/2305.10898) | 本论文提出了一种新的核矩法估计器，称为KMM，其用于超越数据重新加权的矩方法模型，解除了关于使用 $\varphi$-散度相关的限制。 |
| [^34] | [Minimum-Risk Recalibration of Classifiers.](http://arxiv.org/abs/2305.10886) | 本文介绍了最小风险重新校准的概念，在均方误差分解框架内提供了一种原则性方法，用于评估和重新校准概率分类器，并通过平衡校准和锐度确定了最优的桶数，从而产生了大约$O(n^{-2/3})$的风险上界。 |
| [^35] | [StawGAN: Structural-Aware Generative Adversarial Networks for Infrared Image Translation.](http://arxiv.org/abs/2305.10882) | 本文提出了一种名为StawGAN的结构感知生成对抗网络，可以将热红外图像翻译成白天彩色图像，并在目标域中提供更好的形状和高清晰度对象的翻译，达到了比其他最先进的图像翻译模型更准确的结果。 |
| [^36] | [Free Lunch for Privacy Preserving Distributed Graph Learning.](http://arxiv.org/abs/2305.10869) | 该论文提出了一种能够保护隐私的分布式图学习框架，通过学习特征和距离，而不需要实际的特征，来执行图形学习和其他下游任务。这是一种通用的框架。 |
| [^37] | [Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2305.10865) | 该论文提出了一种多智能体强化学习中的新方法SAMA，通过提前训练的语言模型和任务分解来解决ASG方法存在的样本效率问题和生成非实际任务奖励的子目标的问题。 |
| [^38] | [Quiver: Supporting GPUs for Low-Latency, High-Throughput GNN Serving with Workload Awareness.](http://arxiv.org/abs/2305.10863) | Quiver 是一种分布式基于 GPU 的 GNN 服务系统，通过利用工作负载指标来预测 GNN 请求的不规则计算，并管理 GPU 用于图采样和特征聚合的优化方法，实现了低延迟和高吞吐量，比现有系统性能提高多达 15x。 |
| [^39] | [Q-SHED: Distributed Optimization at the Edge via Hessian Eigenvectors Quantization.](http://arxiv.org/abs/2305.10852) | Q-SHED是一种基于海森矩阵特征向量量化的边缘分布式优化算法，具有通信高效、稳健收敛速率等特点。 |
| [^40] | [GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework.](http://arxiv.org/abs/2305.10841) | GETMusic提出了一种统一的音乐生成模型，包括新颖的音乐表示GETScore和扩散模型GETDiff。GETScore使用标记表示音符，将它们有序地组织起来，而GETDiff使用遮盖对目标轨道进行破坏，能够生成任意轨道。 |
| [^41] | [Uncertainty Quantification in Deep Neural Networks through Statistical Inference on Latent Space.](http://arxiv.org/abs/2305.10840) | 本研究提出了一种利用深度神经网络潜在空间表征进行不确定性量化的方法，该方法可以检测数据点的精确度并帮助自动侦测异常值。 |
| [^42] | [ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation.](http://arxiv.org/abs/2305.10838) | 该论文介绍了一种跨模态表征学习方法，用于电子自动化设计中高级综合工具的优化，并促进领域特定加速器（DSAs）的设计自动化化。 |
| [^43] | [Ahead-of-Time P-Tuning.](http://arxiv.org/abs/2305.10835) | 本文提出了 Ahead-of-Time （AoT）P-Tuning，一种新颖的微调方法，它通过在每个Transformer层之前添加输入相关的偏置，实现了应用于预训练语言模型的参数节约。该方法在GLUE和SuperGLUE基准数据集上优于BitFit，并可用于多任务推理，而推理开销却很小。 |
| [^44] | [FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder With Multiple STFTs.](http://arxiv.org/abs/2305.10823) | 本文提出了FastFit，一种使用多个STFT替换U-Net编码器的声音生成算法，将模型参数数量和生成时间减少了一半，并在保持高保真度的同时实现了近两倍的生成速度。 |
| [^45] | [Democratized Diffusion Language Model.](http://arxiv.org/abs/2305.10818) | 本文提出了一个基于CDCD框架的民主扩散语言模型（DDLM），并通过GLUE基准测试了其知识转移能力，为研究人员提供了DDLM训练和评估流程以及已训练的DDLM模型。 |
| [^46] | [Enhancing Speech Articulation Analysis using a Geometric Transformation of the X-ray Microbeam Dataset.](http://arxiv.org/abs/2305.10775) | 该论文提出了一种新的几何变换方法，将X-Ray Microbeam数据集中的解剖标记物的X-Y坐标沿中矢状面映射到多个相对测量中，进而改进了测量的准确性。 |
| [^47] | [Seq-HGNN: Learning Sequential Node Representation on Heterogeneous Graph.](http://arxiv.org/abs/2305.10771) | Seq-HGNN提出了一种新颖的异构图神经网络，具有序列节点表示，通过学习一种序列节点表示机制，避免了由单个向量节点表示引起的信息丢失。 |
| [^48] | [Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling.](http://arxiv.org/abs/2305.10769) | 本文提出了一种名为“追赶蒸馏”的方法，通过调整传统采样算法，让速度估计模型的当前时刻输出与其先前时刻输出和地面真实标签对齐，从而实现只需一次训练便能加速采样的效果。 |
| [^49] | [Automatic Design Method of Building Pipeline Layout Based on Deep Reinforcement Learning.](http://arxiv.org/abs/2305.10760) | 本文提出了一种基于深度强化学习的三维管道布局自动设计方法，可以在更短的时间内完成任务，并确保高质量布局结果。 |
| [^50] | [Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework.](http://arxiv.org/abs/2305.10758) | 本文提出了一种有效的GNN-to-MLP蒸馏框架，将GNNs中的低/高频知识注入MLP。通过将GNNs学习到的知识分解为低/高频成分，在空间域中推导它们的对应关系。此外，还解决了现有GNN-to-MLP蒸馏中的信息淹没问题。 |
| [^51] | [Physics Inspired Approaches Towards Understanding Gaussian Processes.](http://arxiv.org/abs/2305.10748) | 本文利用物理学方法分析了高斯过程模型的损失景观，提出了考虑更广泛的ν使得性能更佳的优化方法，同时提供了一种用于评估GP集成效果的方法和基于损失领域的物理属性的投票方法。 |
| [^52] | [Online Resource Allocation in Episodic Markov Decision Processes.](http://arxiv.org/abs/2305.10744) | 本文将长期资源分配问题形式化为剧集式MDP模型，提出在面临转换和奖励特性不确定的情况下进行在线资源分配问题的解决方案。该方案基于占有度量提供在线镜像下降算法，其期望遗憾受到界限约束 $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ . |
| [^53] | [A benchmark for computational analysis of animal behavior, using animal-borne tags.](http://arxiv.org/abs/2305.10740) | 该论文介绍了一个名为BEBE的动物行为计算分析基准，其中包括了1654小时的动物生态生理学数据，这是迄今为止最大、最具分类多样性的公开可用的数据集合。在这个基准上，作者使用了十种机器学习方法并确定了未来工作中需要解决的关键问题。 |
| [^54] | [Deep Temporal Graph Clustering.](http://arxiv.org/abs/2305.10738) | 提出通用框架TGC 用于 deep temporal graph clustering, 解决了时间图只能作为静态图处理的难题，实现了对动态信息的聚类。实验证明了 TGC 的优越性。 |
| [^55] | [FedMR: Federated Learning via Model Recombination.](http://arxiv.org/abs/2305.10730) | FedMR是一种基于模型重组的联邦学习范式，可以通过混洗和重组每个层的本地模型来缓解子优或有偏局部模型的问题，从而在推理性能方面表现出色。 |
| [^56] | [Boost Vision Transformer with GPU-Friendly Sparsity and Quantization.](http://arxiv.org/abs/2305.10727) | 本论文介绍了一种针对 Vision Transformer 模型的压缩方案，通过 2:4 结构化稀疏剪枝和基于稀疏蒸馏感知的量化训练实现了GPU加速. |
| [^57] | [Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping.](http://arxiv.org/abs/2305.10721) | 本文证明了线性映射在长期时间序列预测中的重要性，提出了RevIN和CI的方法来提高预测性能，同时发现线性映射可以有效地捕捉时间序列的周期特征。 |
| [^58] | [Discounted Thompson Sampling for Non-Stationary Bandit Problems.](http://arxiv.org/abs/2305.10718) | 该论文提出了一种针对非稳态多臂赌博机问题的折扣汤普森抽样算法（DS-TS），可以解决突然性变化和平滑性变化的问题，并且在两种情况下具有近乎最优的遗憾上限。 |
| [^59] | [A Survey on Time-Series Pre-Trained Models.](http://arxiv.org/abs/2305.10716) | 本综述全面回顾了时间序列预训练模型，其中监督、无监督和自监督是主要类别。通过使用这些模型，可以克服构建大规模标记数据集的困难，提高时间序列挖掘的性能和效率。 |
| [^60] | [Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency.](http://arxiv.org/abs/2305.10713) | 本论文提出了一种新的度量--Prompt平坦度，可以优化语言提示选择，提高模型分类的准确性和样本效率，实验证明结合现有度量可以提高性能和样本效率。 |
| [^61] | [A Framework Based on Symbolic Regression Coupled with eXtended Physics-Informed Neural Networks for Gray-Box Learning of Equations of Motion from Data.](http://arxiv.org/abs/2305.10706) | 该论文提出了一种基于符号回归和X-PINNs的方法，直接从数据中发现非线性方程的未知部分，并且在实验中表现出良好的精度和稳定性。 |
| [^62] | [ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval.](http://arxiv.org/abs/2305.10703) | 本文提出了一种基于检索增强的框架，通过渐进式密集检索从通用领域的无标签语料库中创建训练数据，实现了零样本文本分类，相较于最强的基线模型提高了4.3%的性能，与使用大型NLG模型的基线相比节省了约70％的时间。 |
| [^63] | [Dirichlet Diffusion Score Model for Biological Sequence Generation.](http://arxiv.org/abs/2305.10699) | 本文介绍了一种针对离散数据，使用概率单纯形空间中的扩散过程进行建模的生成SDE模型。称之为Dirchlet扩散分数模型。模型可以生成满足严格限制的样本，且适用于生成生物序列。 |
| [^64] | [The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond.](http://arxiv.org/abs/2305.10697) | 本文提出了异构群体强化学习中联邦Q学习的样本复杂度保证，讨论了同步和异步版本的线性加速，同时探究了等权重平均本地Q估计的缺陷。 |
| [^65] | [Unbiased Gradient Boosting Decision Tree with Unbiased Feature Importance.](http://arxiv.org/abs/2305.10696) | 本文提出了UnbiasedGBM方法以解决梯度提升决策树（GBDT）中特征选择过程中的偏见问题，并提出了无偏增益进行对特征重要性的测量。 |
| [^66] | [Gated Deeper Models are Effective Factor Learners.](http://arxiv.org/abs/2305.10693) | 本文提出一种门控深度学习模型，将人工设计的因子与深度神经网络相结合，能够在2048维度空间中生成更有意义的因子，并取得了在对2,000只中国市场股票实验中良好的效果。 |
| [^67] | [Sampling, Diffusions, and Stochastic Localization.](http://arxiv.org/abs/2305.10690) | 这篇论文介绍了扩散和随机定位的关系，证明了标准去噪扩散是一种随机定位，并提出了一种在对数步骤内从 Ising 模型的 Gibbs 测度中进行采样的算法。 |
| [^68] | [Black-Box Targeted Reward Poisoning Attack Against Online Deep Reinforcement Learning.](http://arxiv.org/abs/2305.10681) | 本文提出了一种针对在线深度强化学习的黑盒目标攻击方式，即通过奖励毒化攻击来干扰训练过程，攻击具有通用性，且攻击预算和计算资源需求较少，实验结果表明攻击高效，适用于多种DRL环境和学习器。 |
| [^69] | [Less Can Be More: Unsupervised Graph Pruning for Large-scale Dynamic Graphs.](http://arxiv.org/abs/2305.10673) | 本研究提出了一个无监督图剪枝框架，名为STEP，用于解决大规模动态图的训练和部署问题，该框架不需要标签数据，并且具有最先进的剪枝效率和有效性。 |
| [^70] | [MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection.](http://arxiv.org/abs/2305.10668) | 本文提出了一种名为MetaGAD的框架，用于学习从无标记节点到有标记节点之间的元转移知识，以进行少样本图异常检测。 |
| [^71] | [Posterior Inference on Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance.](http://arxiv.org/abs/2305.10664) | 本文提出了一种新的方法进行关于具有无界方差权重的贝叶斯神经网络的后验推断，并表明后验分布集中在具有非标准超参数依赖性的稀疏促进和均值收缩先验周围。 |
| [^72] | [Use of Speech Impairment Severity for Dysarthric Speech Recognition.](http://arxiv.org/abs/2305.10659) | 本文提出了一种使用发音障碍严重程度和说话人身份的口吃症语音识别技术，实现了显著的字错率降低。 |
| [^73] | [DeepEdit: Deep Editable Learning for Interactive Segmentation of 3D Medical Images.](http://arxiv.org/abs/2305.10655) | DeepEdit是一种基于深度学习的医学图像注释方法，它结合了自动和交互式分割，允许非常快速的数据集分割，且通过用户交互（如点击）可以进一步优化分割结果。 |
| [^74] | [STREAMLINE: Streaming Active Learning for Realistic Multi-Distributional Settings.](http://arxiv.org/abs/2305.10643) | STREAMLINE是一种新的流式主动学习框架，通过切片识别、切片感知预算和数据选择的三步流程，缓解了工作标记数据中场景驱动的切片不平衡问题。 |
| [^75] | [Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis.](http://arxiv.org/abs/2305.10638) | 本文提出了CORAL，一种用于在线无监督根本原因分析的新框架，可以自动触发该过程并增量更新模型，包括三个主要部分：触发点检测，增量因果图学习和基于网络传播的根本原因定位。 |
| [^76] | [Augmented Message Passing Stein Variational Gradient Descent.](http://arxiv.org/abs/2305.10636) | 本文提出了一种增强的消息传递斯坦变分梯度下降法(AUMP-SVGD)来应对 Stein Variational Gradient Descent (SVGD)方法的方差崩溃问题，我们的算法能够提高SVGD在高维问题中的有效性。 |
| [^77] | [Modified Gauss-Newton Algorithms under Noise.](http://arxiv.org/abs/2305.10634) | 本文探讨了在大规模统计设置中高斯牛顿方法及其随机版本的性能和非光滑版本的修改高斯牛顿或近端线性算法的对比表现，并勾勒出在统计噪声下修改高斯牛顿法的二次收敛区域。 |
| [^78] | [Smoothing the Landscape Boosts the Signal for SGD: Optimal Sample Complexity for Learning Single Index Models.](http://arxiv.org/abs/2305.10633) | 本文提出了使用平滑化的损失来优化在线SGD的信号，可以使用$n \gtrsim d^{k^\star/2}$个样本学习单指数模型$w^\star$，并与张量PCA和小批量SGD的正则化效应有关。 |
| [^79] | [A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism.](http://arxiv.org/abs/2305.10631) | 提出了一种基于多尺度特征金字塔网络和双重注意力机制的子腹部MRI图像分割算法，使用空洞卷积和多尺度特征金字塔编码以避免语义差距，设计双重注意力机制以保持空间信息并减少错位。 |
| [^80] | [Measuring and Mitigating Local Instability in Deep Neural Networks.](http://arxiv.org/abs/2305.10625) | 深度神经网络中，训练过程中的随机性可能导致模型的输出不稳定，作者提出了基于原则的指标来量化不稳定性并发现不稳定的预测并不是随机出现的，而是以数据相关的方式聚集在一起。作者研究了数据无关正则化方法来减轻这种不稳定性，并表明一些方法可以显着提高不稳定性，甚至在某些情况下优于更广泛使用的正则化方法。 |
| [^81] | [The star-shaped space of solutions of the spherical negative perceptron.](http://arxiv.org/abs/2305.10623) | 本文针对球形负感知器模型的解空间展开研究，发现在过度参数化的区域内解决方案流形显示出简单的连通性质。存在一个大的测地凸成分，对各种优化动力学具有吸引力，其中又有一个与大多数其他解决方案测地连接的非典型鲁棒解决方案的子集，从而产生了星形的几何形状。 |
| [^82] | [Evaluation Metrics for CNNs Compression.](http://arxiv.org/abs/2305.10616) | 本文提供了神经网络压缩的评估度量综述，从而为标准化神经网络压缩贡献力量。 |
| [^83] | [ACRoBat: Optimizing Auto-batching of Dynamic Deep Learning at Compile Time.](http://arxiv.org/abs/2305.10611) | ACRoBat是一种动态深度学习的自动批处理框架，在编译时进行混合静态+动态编译器优化和端到端张量代码生成，可将性能提高多达8.5倍。 |
| [^84] | [Tree of Thoughts: Deliberate Problem Solving with Large Language Models.](http://arxiv.org/abs/2305.10601) | 本研究提出了一种新的推理框架——思维之树（ToT），可以增强语言模型的问题解决能力，帮助语言模型进行深思熟虑的决策，以及自我评估和全局选择。 |
| [^85] | [Tensor Products and Hyperdimensional Computing.](http://arxiv.org/abs/2305.10572) | 本文探索了张量积在超维计算中的数学关系，将其确定为中心表示，并发现它是最通用、最具表现力和最压缩的表示，同时具有无误差解绑和检测的能力。 |
| [^86] | [Self-Supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET.](http://arxiv.org/abs/2305.10569) | 本研究引入自监督学习，提供了一种快速而准确的药代动力学建模方法，可以生成与预期生理学一致的像素级参数图。 |
| [^87] | [Counterfactually Comparing Abstaining Classifiers.](http://arxiv.org/abs/2305.10564) | 本文提出一种新的方法和视角来评估和比较放弃分类器，将放弃预测视为缺失数据。我们定义了放弃分类器的反事实得分，指的是分类器没有放弃预测时的预测性能期望。 |
| [^88] | [Short-Term Electricity Load Forecasting Using the Temporal Fusion Transformer: Effect of Grid Hierarchies and Data Sources.](http://arxiv.org/abs/2305.10559) | 本文研究了使用Temporal Fusion Transformer对不同时间范围和网络级别的短期电力负荷进行预测的潜力，结果显示TFT在变电站级别和使用额外数据源时具有显著的提高。 |
| [^89] | [Deep Multiple Instance Learning with Distance-Aware Self-Attention.](http://arxiv.org/abs/2305.10552) | 本文提出具有距离感知自注意力的深度多示例学习模型，该模型能根据补丁之间的空间关系动态调整权重，从而在多个基准数据集上提高了分类性能。 |
| [^90] | [Sparsity-depth Tradeoff in Infinitely Wide Deep Neural Networks.](http://arxiv.org/abs/2305.10550) | 本文研究了神经网络中稀疏深度和泛化性能的关系，发现在宽度足够大时，较稀疏的网络在浅层时表现更好。 |
| [^91] | [Discovering Individual Rewards in Collective Behavior through Inverse Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2305.10548) | 本论文介绍了一种离线逆向多智能体强化学习算法，通过利用演示，自动发现奖励函数并学习代理的有效策略，用于在复杂动态系统中寻找集体行为中的个体目标。 |
| [^92] | [Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks.](http://arxiv.org/abs/2305.10544) | GSPNs是一种新的概率框架，用于图表示学习，可以可计算地回答概率查询，并通过权重共享和树状计算图的优势获得了纯概率模型的效率和深度图网络的效果。 |
| [^93] | [Generalization Bounds for Neural Belief Propagation Decoders.](http://arxiv.org/abs/2305.10540) | 本文研究了神经置信传播译码器的泛化能力，提出了一组新的理论结果，界定了解码器的泛化间隙，结果与解码器的复杂程度有关。 |
| [^94] | [Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems.](http://arxiv.org/abs/2305.10528) | 本文提出了一种在自学习对话系统中利用历史回归事件报告来验证、保护和改进政策的方法，以解决在大规模商业环境中的经验连贯性和政策改进之间的平衡问题。 |
| [^95] | [Statistical Knowledge Assessment for Generative Language Models.](http://arxiv.org/abs/2305.10519) | 本文介绍了一个统计知识评估框架，用于评估生成式语言模型（GLMs）的知识输出。通过对14种GLMs进行综合比较，发现知识遵循缩放定律，而对指令遵循数据进行微调可能会损害模型的生成能力。 |
| [^96] | [Exact Recovery for System Identification with More Corrupt Data than Clean Data.](http://arxiv.org/abs/2305.10506) | 本文研究了在敌对环境下线性离散时间系统的系统识别问题。在周期性注入攻击时，系统动态可精确恢复样本复杂度为O(n)；当攻击以概率p进行时，精确恢复样本复杂度为O(log(n)p/(1-p)^2) 。即使有超过一半的数据受损，估计器仍可学习。 |
| [^97] | [Model-Free Robust Average-Reward Reinforcement Learning.](http://arxiv.org/abs/2305.10504) | 本文研究了无模型鲁棒平均奖励马尔可夫决策过程，提出两种算法并证明了它们收敛到最优解。我们给出了几个广泛使用的不确定性集合作为示例。 |
| [^98] | [EENED: End-to-End Neural Epilepsy Detection based on Convolutional Transformer.](http://arxiv.org/abs/2305.10502) | 本文提出了一种名为EENED的端到端神经元癫痫检测模型，结合了CNN和Transformer，能够同时捕捉EEG信号的全局依赖性和局部特征，并有望提高癫痫检测的准确性和可靠性。 |
| [^99] | [Edge Directionality Improves Learning on Heterophilic Graphs.](http://arxiv.org/abs/2305.10498) | 本文提出了一种新的有向图神经网络框架Dir-GNN，并在有向引用图上进行评估，结果表明它在预测缺失的引用链接方面优于现有的无向GNN和一些有向图模型。 |
| [^100] | [Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models.](http://arxiv.org/abs/2305.10474) | 本论文介绍了一种新的视频噪声先验，用于微调图像扩散模型，以实现更高质量的视频合成。经过广泛的实验验证，该模型已经取得了UCF-101和MSR-VTT基准测试的最佳结果。 |
| [^101] | [Predicting Side Effect of Drug Molecules using Recurrent Neural Networks.](http://arxiv.org/abs/2305.10473) | 该论文介绍了一种利用循环神经网络预测药物分子副作用的启发式方法，有效降低了模型复杂度和参数数量。 |
| [^102] | [Nine tips for ecologists using machine learning.](http://arxiv.org/abs/2305.10472) | 本论文介绍了九个技巧来帮助生态学家实施机器学习模型，这些技巧针对分类问题，旨在识别开发机器学习模型中的常见错误、陷阱或挑战，并提供了解决方法。 |
| [^103] | [Bike2Vec: Vector Embedding Representations of Road Cycling Riders and Races.](http://arxiv.org/abs/2305.10471) | Bike2Vec是一种通过历史比赛结果学习车手和比赛表示的方法，这些表示可以用于车手和比赛的特征提取和下游预测任务。 |
| [^104] | [Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence.](http://arxiv.org/abs/2305.10468) | 该论文提出了一个更为强大的人工神经网络模型，该模型中同一隐藏层中的隐藏神经元相互连接，可以学习复杂模式并加速收敛速度。 |
| [^105] | [Analysing Biomedical Knowledge Graphs using Prime Adjacency Matrices.](http://arxiv.org/abs/2305.10467) | 本文提出了一种使用质数邻接矩阵分析生物医学知识图谱的新方法，可以高效地计算网络的多个属性和提高结果，且不需要复杂的培训或流程。 |
| [^106] | [Reconstruction Error-based Anomaly Detection with Few Outlying Examples.](http://arxiv.org/abs/2305.10464) | 该论文探讨了在训练给定少量异常情况下，通过基于重构误差的神经网络结构，排除已知的异常点，用于检测异常情况的方法。 |
| [^107] | [Topology Optimization using Neural Networks with Conditioning Field Initialization for Improved Efficiency.](http://arxiv.org/abs/2305.10460) | 本文提出了一种条件场初始化的神经网络拓扑优化方法，通过使用先前的初始场，进一步提高了神经网络拓扑优化的效率。 |
| [^108] | [AnalogNAS: A Neural Network Design Framework for Accurate Inference with Analog In-Memory Computing.](http://arxiv.org/abs/2305.10459) | AnalogNAS是一种用于自动化设计深度神经网络的框架，可以针对模拟内存计算推理加速器进行优化，提高在边缘环境下的区域和功耗效率。 |
| [^109] | [Time Series Clustering With Random Convolutional Kernels.](http://arxiv.org/abs/2305.10457) | 该论文介绍了一种新方法用于时间序列聚类，该方法利用随机卷积结构将数据转换为增强的特征表示，再进行聚类，以识别异常值，该方法在时间序列聚类基准上实现了更好的结果。 |
| [^110] | [Comparison of classifiers in challenge scheme.](http://arxiv.org/abs/2305.10452) | 本文介绍了在挑战方案的限制下评估不同竞争者(算法)表现的问题，包括使用唯一的数据集、规定最小提交次数和一组性能指标等措施。关键问题是，这些差异能否对最终结果产生重大影响。 |
| [^111] | [How does agency impact human-AI collaborative design space exploration? A case study on ship design with deep generative models.](http://arxiv.org/abs/2305.10451) | 通过构建基于生成对抗网络的生成设计空间（GDS），本文探究了随机探索、半自动探索和自动探索三种不同模式的GDS探索方法在船舶设计中的适用性和效果。 |
| [^112] | [Understanding of Normal and Abnormal Hearts by Phase Space Analysis and Convolutional Neural Networks.](http://arxiv.org/abs/2305.10450) | 通过相空间分析和卷积神经网络结合诊断心脏疾病，在MIT-BIH数据库上取得了93.3%的平均准确率。 |
| [^113] | [Cooperation Is All You Need.](http://arxiv.org/abs/2305.10449) | 引入了一种基于“本地处理器民主”的算法Cooperator，该算法在强化学习中表现比Transformer算法更好。 |
| [^114] | [CBAGAN-RRT: Convolutional Block Attention Generative Adversarial Network for Sampling-Based Path Planning.](http://arxiv.org/abs/2305.10442) | 本文介绍了一种基于图像处理学习算法（CBAGAN-RRT）的路径规划方法，使用卷积块注意力生成对抗网络和一种新的损失函数，找到更优的最佳路径并提高算法的收敛速度，与先前的最先进算法相比，在图像质量生成指标和路径规划指标方面都表现更优。 |
| [^115] | [Learning the Visualness of Text Using Large Vision-Language Models.](http://arxiv.org/abs/2305.10434) | 该论文利用大型视觉语言模型如CLIP来检测文本的视觉性，并提出fine-tuning策略，将非视觉文本映射为NULL图像，匹配视觉文本与对应图像，以解锁在文本中嵌入相关图像的能力。 |
| [^116] | [Model-Contrastive Federated Domain Adaptation.](http://arxiv.org/abs/2305.10432) | 本文提出了一种名为FDAC的联邦领域自适应方法，通过对比学习和采用Vision Transformer（ViT）框架提取可适应特征，成功实现了在联邦设置中学习可转移的表示。实验表明，FDAC优于最先进的联邦领域自适应方法。 |
| [^117] | [Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility.](http://arxiv.org/abs/2305.10235) | 本研究是一项关于大型语言模型方面的实证研究，对主流语言模型进行了大量查询和分析，结果发现这些模型存在着鲁棒性、一致性和可信性方面的潜在风险。 |
| [^118] | [A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization.](http://arxiv.org/abs/2305.10089) | 本文证明了Wasserstein反向强化学习模型适用于多目标优化问题，可让学习者的奖励值和最优解模仿专家，具有一定的实用价值。 |
| [^119] | ["I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation.](http://arxiv.org/abs/2305.09941) | 本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。 |
| [^120] | [Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation.](http://arxiv.org/abs/2305.09860) | 本文研究了用于机器翻译最小贝叶斯风险解码的不同采样策略，并发现了epsilon采样方式能够使得解码结果显著地优于其他所有已测试的采样方式和束搜索解码。 |
| [^121] | [Learning from Aggregated Data: Curated Bags versus Random Bags.](http://arxiv.org/abs/2305.09557) | 本文研究了两种自然的聚合方法：基于共同特征将数据点分组的精选包和将数据点随机分组的随机包，对于精选包设置和广泛的损失函数范围内，我们展示了可以通过梯度下降学习而不会导致数据聚合导致性能下降的情况。 |
| [^122] | [Solar Active Region Magnetogram Image Dataset for Studies of Space Weather.](http://arxiv.org/abs/2305.09492) | 本数据集提供了一系列太阳活动区磁图，并提供相应的太阳耀斑标签。它可用于研究磁结构、其演化以及太阳耀斑的关系，并对于自动太阳耀斑预测方法的研究具有重要价值。 |
| [^123] | [Smart Policy Control for Securing Federated Learning Management System.](http://arxiv.org/abs/2305.09134) | 本文提出了一种基于智能合约的策略控制，可以保障联合学习管理系统的安全性，确保每个参与者都遵守数据保护政策，同时可以审计训练过程。 |
| [^124] | [What Matters in Reinforcement Learning for Tractography.](http://arxiv.org/abs/2305.09041) | 本论文深入探讨了强化学习在Tractography中不同的组成部分，提出了关于RL算法选择、代理人输入、奖励函数和播种策略的一系列建议。 |
| [^125] | [Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations.](http://arxiv.org/abs/2305.08099) | 本文提出了一种自监督神经因子分析模型，使用HuBERT中的聚类方法来发现隐藏的声学单元，并使用这些单元对齐SSL模型的特征，从而产生解耦后的语音表示，从而为专门任务提供了一种基于Utterance水平的无监督学习目标。实验结果表明，SSNFA模型在说话人识别、语言识别和情感识别等各种任务中均显著优于现有的SSL模型，并且没有任何特定任务的微调或监督。 |
| [^126] | [DRew: Dynamically Rewired Message Passing with Delay.](http://arxiv.org/abs/2305.08018) | 本文提出了一种能够应用于任何MPNN结构的框架，执行基于层的动态重连来确保逐渐密集化的图形。同时引入了一种延迟机制，允许跨层节点之间的跳跃连接。 |
| [^127] | [A Federated Learning-based Industrial Health Prognostics for Heterogeneous Edge Devices using Matched Feature Extraction.](http://arxiv.org/abs/2305.07854) | 提出了一种基于联邦学习的健康预测模型，该模型具有特征相似性匹配算法来区分学习来自异构边缘设备的数据，以便开发出更准确的预测模型。 |
| [^128] | [Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation.](http://arxiv.org/abs/2305.07804) | 本论文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，以改善小语言模型的性能，特别是在医学问答任务中。这种方法在微调后使模型性能提高，并提出了在特定领域问答任务中使用LLM所面临的挑战和潜在的研究方向。 |
| [^129] | [Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding.](http://arxiv.org/abs/2305.07424) | 本文提出了IS-CSE方法，通过实例平滑对比学习来学习无监督句子嵌入，以平滑嵌入在特征空间中的边界，从而提高模型的泛化性能。在标准的STS任务中取得了良好的得分。 |
| [^130] | [Hawkes Process based on Controlled Differential Equations.](http://arxiv.org/abs/2305.07031) | 本文提出了一种基于控制微分方程的Hawkes过程模型，可精确计算对数似然，并能够正确处理不规则时间序列，适用于社会扩散和地震预测。 |
| [^131] | [A proof of convergence of inverse reinforcement learning for multi-objective optimization.](http://arxiv.org/abs/2305.06137) | 本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。 |
| [^132] | [Mediapipe and CNNs for Real-Time ASL Gesture Recognition.](http://arxiv.org/abs/2305.05296) | Mediapipe和CNN用于实时美国手语手势识别。测试结果表明准确率可达99.95％，有潜力用于听力障碍人士的通信设备，并可以应用于其他相似手语。这项研究对计算机视觉和机器学习领域做出了重要贡献。 |
| [^133] | [Comparing Foundation Models using Data Kernels.](http://arxiv.org/abs/2305.05126) | 本文采用基于数据内核的方法比较基础模型，不受度量指标的约束，通过嵌入空间几何实现点对点和多模型比较，并成功诱导了一组与下游指标强相关的模型距离函数流形。 |
| [^134] | [Augmented Large Language Models with Parametric Knowledge Guiding.](http://arxiv.org/abs/2305.04757) | 这篇论文提出了一种带参数知识引导的增强型大语言模型框架，通过为LLMs装备信息引导模块来访问相关知识，同时保持LLMs的参数不变。这个框架可以提高黑盒LLMs在各种NLP任务上的性能。 |
| [^135] | [Posterior Sampling for Deep Reinforcement Learning.](http://arxiv.org/abs/2305.00477) | 本文提出了用于深度强化学习的后验采样算法PSDRL，结合了高效的不确定性量化和特殊设计的持续规划算法，使其在提高样本效率的同时显著优于之前的尝试。 |
| [^136] | [Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy.](http://arxiv.org/abs/2304.14762) | 本文提出了一种通过在样本中引入扰动，改进基于核化斯坦距的拟合优度检验方法的方法，以解决在同质但混合比例不同的情况下低功率的问题，并展示实验证据证明了该方法的功效。 |
| [^137] | [Segment Anything Model for Medical Images?.](http://arxiv.org/abs/2304.14660) | “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。 |
| [^138] | [Combining Adversaries with Anti-adversaries in Training.](http://arxiv.org/abs/2304.12550) | 该论文研究了在对抗训练中，通过结合对手和反对手(带有反对手扰动的样本)可以更有效地提高深度神经网络的公平性、鲁棒性和泛化性，在一些特定的学习场景中表现出更好的性能。 |
| [^139] | [The Disharmony Between BN and ReLU Causes Gradient Explosion, but is Offset by the Correlation Between Activations.](http://arxiv.org/abs/2304.11692) | 本研究阐述了BN和ReLU之间的不和谐是导致梯度爆炸的主要原因，同时发现输入之间的相关性可以缓解这个问题。提出一种基于二阶优化算法的自适应学习率算法，在大批量训练中表现优异，并可替代WarmUp，在小批量训练中也表现不错。 |
| [^140] | [Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading.](http://arxiv.org/abs/2304.10784) | Eyettention是第一个同时处理语言序列和时间序列的阅读模型，可以更准确地模拟阅读者的扫视路径，对机器学习的自然语言处理模型具有借鉴意义。 |
| [^141] | [DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards.](http://arxiv.org/abs/2304.10770) | 提出了一种探索强化学习算法DEIR，借助区分性模型实现理论上导出的内在奖励，能够高效且鲁棒地进行探索，适用于面对外部奖励稀疏的情况。 |
| [^142] | [Sociocultural knowledge is needed for selection of shots in hate speech detection tasks.](http://arxiv.org/abs/2304.01890) | HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。 |
| [^143] | [Neural Network Entropy (NNetEn): EEG Signals and Chaotic Time Series Separation by Entropy Features, Python Package for NNetEn Calculation.](http://arxiv.org/abs/2303.17995) | 该研究提出了一种新的熵估计方法NNetEn，用于有效地分离时间序列系统的混沌动态，并在分离混沌时间序列方面证明了其高效率。 |
| [^144] | [Sparse joint shift in multinomial classification.](http://arxiv.org/abs/2303.16971) | 该论文提出了一种稀疏联合偏移模型，用于解决整体数据集偏移问题，提供了传递SJS、修正类后验概率、SJS的可辨认性、SJS与协变量转移关系等新结果。 |
| [^145] | [List Online Classification.](http://arxiv.org/abs/2303.15383) | 本文研究了多标签列表的在线预测问题，提出了 $b$-ary Littlestone 维度可学习模型，并且在懵懂的情况下探索不同的情况。可以使用改编自 Littlestone 的 SOA 和 Rosenblatt 的感知器等算法进行预测，同时还建立了列表可学习的组合结果。 |
| [^146] | [Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific Machine Learning Problems.](http://arxiv.org/abs/2303.12928) | 本文从理论上将特定优化问题与多时间Hamilton-Jacobi PDEs联系起来，表明当解决这些问题时，同时解决了对应的多时间HJ PDEs和最优控制问题。利用这种联系，提出了一种新的算法，实现了深度神经网络泛化性能的改进。 |
| [^147] | [Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament.](http://arxiv.org/abs/2303.07925) | 本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。 |
| [^148] | [Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies.](http://arxiv.org/abs/2303.07551) | 本文提出通过在权重空间中合并训练于不同 MuJoCo 运动问题上的 Decision Transformer 的子集，形成多任务模型。通过共享一些辅助任务的训练以及共同使用预训练初始化，能够获得更好的结果。这个方向的研究有助于使代理的过程民主化和分发。 |
| [^149] | [Prevalence and major risk factors of non-communicable diseases: A Hospital-based Cross-Sectional Study in Dhaka, Bangladesh.](http://arxiv.org/abs/2303.04808) | 该研究调查了孟加拉国达卡市成年患者中多种非传染性疾病的患病率及其风险因素，其中心血管疾病最为普遍。男性参与者较女性更容易患有心血管疾病，但糖尿病不具有性别倾向。CVD和DM都会随着年龄的增长而增加。患有肥胖症的住院病人占五分之一。 |
| [^150] | [Certified Robust Neural Networks: Generalization and Corruption Resistance.](http://arxiv.org/abs/2303.02251) | 该论文提出了一种新颖的分布鲁棒损失函数，该函数通过认证级别的鲁棒性对两种常见的污染类型进行抵抗，并确保泛化保证，从而解决了鲁棒性和泛化之间的矛盾，具有极高的实用性。 |
| [^151] | [The Point to Which Soft Actor-Critic Converges.](http://arxiv.org/abs/2303.01240) | 本文证明了在极限情况下，Soft Actor-Critic算法和Soft Q-learning算法在最大熵框架下收敛于同一解，这一结论对优化算法具有较大的意义。 |
| [^152] | [Stochastic Approximation Approaches to Group Distributionally Robust Optimization.](http://arxiv.org/abs/2302.09267) | 本文提出了一种随机逼近法，用于组分布式鲁棒优化，该算法利用在线学习技术，将每轮所需的样本数从$m$个降至$1$个，同时保持相同的样本复杂度。 |
| [^153] | [Efficient Fraud Detection Using Deep Boosting Decision Trees.](http://arxiv.org/abs/2302.05918) | 本文提出了一种利用深度增强决策树的方法来进行欺诈检测，既可以利用决策树的可解释性，又可以提高神经网络的表示学习能力。同时，采用新的过采样策略来缓解数据不平衡的问题。 |
| [^154] | [Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition.](http://arxiv.org/abs/2302.05881) | 本文提出了一种新的方法框架GCDTC，利用数值先验和广义CP分解实现了更高的低秩张量补全精度；同时介绍了一个算法SPTC，作为该框架的一个实现。在实验中，该方法表现出比现有技术更好的性能。 |
| [^155] | [Reinforcement Learning with History-Dependent Dynamic Contexts.](http://arxiv.org/abs/2302.02061) | 介绍了一种称为DCMDPs的新型强化学习框架，用于处理依赖历史环境的情况。其中的逻辑DCMDPs通过利用聚合函数确定上下文转换，打破了对历史长度的指数依赖，并引入了一种实用的基于模型的算法。在推荐任务中展示了该方法的有效性。 |
| [^156] | [Learning Functional Transduction.](http://arxiv.org/abs/2302.00328) | 提出了一种利用元学习方法和向量值再生核Banach空间理论形成高效的上下文神经逼近器的混合方法，将其应用于函数空间，可以在给定少量输入和输出示例的情况下帮助快速捕捉到无穷多的功能关系。 |
| [^157] | [Oracle Complexity of Single-Loop Switching Subgradient Methods for Non-Smooth Weakly Convex Functional Constrained Optimization.](http://arxiv.org/abs/2301.13314) | 本文分析了一种交替次梯度法用于非凸优化问题的Oracle复杂度，其中约束函数为凸或弱凸，在只使用单环路的情况下取得了相同的复杂度，并可应用于非光滑问题。 |
| [^158] | [MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning.](http://arxiv.org/abs/2301.13287) | 提出了一个模型无关子集选择框架MILO，将子集选择与模型训练分离，通过易到难的课程实现了卓越的模型收敛和性能。 |
| [^159] | [Unifying Molecular and Textual Representations via Multi-task Language Modelling.](http://arxiv.org/abs/2301.12586) | 本文提出了第一个多任务语言模型，可以同时解决化学和自然语言领域的各种任务，无需昂贵的单一域或任务特定模型的预训练。 |
| [^160] | [ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning.](http://arxiv.org/abs/2301.12077) | 提出一种噪声局部标签学习的框架ALIM，通过权衡候选集和模型输出来减少检测误差的负面影响，可以与现有PLL方法集成，并在基准数据集上达到最先进性能。 |
| [^161] | [Domain-Agnostic Molecular Generation with Self-feedback.](http://arxiv.org/abs/2301.11259) | MolGen是一个专注于分子生成的预训练语言模型，使用了领域无关的分子前缀调整和自我反馈的范式，实现了化学有效性、多样性、新颖性和复杂性的突破，在分子生成领域表现出了出色的性能。 |
| [^162] | [Federated Recommendation with Additive Personalization.](http://arxiv.org/abs/2301.09109) | FedRAP是一种新的推荐系统生成方法，它使用联邦学习训练共享项嵌入和本地个性化视图，以捕捉用户对推荐项目感知的个体差异并且降低通信成本和延迟。 |
| [^163] | [Optimization of body configuration and joint-driven attitude stabilization for transformable spacecrafts under solar radiation pressure.](http://arxiv.org/abs/2301.08435) | 本文针对太阳辐射下变形航天器的姿态控制技术进行了研究，提出了关节角度优化方法和通过关节驱动的动量阻尼控制，使变形航天器比以前的方法在轨道和姿态控制方面获得更好的性能。 |
| [^164] | [Expected Gradients of Maxout Networks and Consequences to Parameter Initialization.](http://arxiv.org/abs/2301.06956) | 本文研究了Maxout网络关于输入和参数的梯度，提出了避免梯度消失和爆炸的参数初始化策略，并在实验中证明了其有效性。 |
| [^165] | [Generalized Neural Closure Models with Interpretability.](http://arxiv.org/abs/2301.06198) | 本论文提出了一种新的方法，使用统一的神经偏微分方程来提高动力学模型的预测能力和计算成本，并解决了解释性和适用性方面的局限性。 |
| [^166] | [A Rigorous Uncertainty-Aware Quantification Framework Is Essential for Reproducible and Replicable Machine Learning Workflows.](http://arxiv.org/abs/2301.05763) | 这篇论文讨论了一个基于贝叶斯范式的不确定性量化框架，可以提供一个广泛而严格的方法来评估机器学习和人工智能在科学工作流程中的可重复性和可信度。 |
| [^167] | [Exploring Tradeoffs in Spiking Neural Networks.](http://arxiv.org/abs/2212.09500) | 本研究探讨了在使用Time-To-First-Spike（TTFS）约束时，性能、能耗、速度和稳定性之间的权衡，并提出了一种允许多个脉冲的松弛版本的方法来改善这些权衡。实验证明放宽脉冲约束可以提高SNN的性能、能耗和鲁棒性，同时仍保持合理的预测速度。 |
| [^168] | [P2T2: a Physically-primed deep-neural-network approach for robust $T_{2}$ distribution estimation from quantitative $T_{2}$-weighted MRI.](http://arxiv.org/abs/2212.04928) | 本论文提出了一种物理引导的深度神经网络方法P2T2，可以鲁棒地从MRI数据中估计$T_2$分布，适用于临床实践和具有异构采集协议的大规模多机构试验。 |
| [^169] | [Graph Convolutional Neural Networks with Diverse Negative Samples via Decomposed Determinant Point Processes.](http://arxiv.org/abs/2212.02055) | 本论文提出了基于DPP的方法来获取多样的负样本，在图卷积神经网络中实现图表示学习，通过提供不同的信息来更新节点的表示，从而提高了准确性。 |
| [^170] | [Distilling Reasoning Capabilities into Smaller Language Models.](http://arxiv.org/abs/2212.00193) | 本文提出了一种知识蒸馏方法，可以把大型语言模型的逐步推理能力蒸馏到更小的模型中，提出了一种替代推理方案，使用苏格拉底式CoT来训练两个小型蒸馏模型的组合，可以用来分解和解决复杂的问题，且在多个推理数据集上表现出高精度的复杂推理能力，经常优于那些没有经过CoT推理方法训练的大模型。 |
| [^171] | [Simple and Scalable Algorithms for Cluster-Aware Precision Medicine.](http://arxiv.org/abs/2211.16553) | 本文提出了基于聚类和嵌入的简单高效方法，用于克服精准医疗中的高维问题和聚类问题，经验证该方法较当前方法具有更高的有效性和可行性。 |
| [^172] | [Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs.](http://arxiv.org/abs/2211.16468) | 在因果图中，提出了解决前门调整的线性时间算法，通过观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。 |
| [^173] | [What learning algorithm is in-context learning? Investigations with linear models.](http://arxiv.org/abs/2211.15661) | 研究者提出了一种假设，即基于转换器的上下文学习器可以隐含地编码学习算法，并根据上下文中出现的新示例更新这些隐式模型。通过构造和比较性质证明了这个假设，并提出了一种度量学习器预测器和从显式学习算法中获得的预测器之间相似程度的方法。 |
| [^174] | [CIM: Constrained Intrinsic Motivation for Sparse-Reward Continuous Control.](http://arxiv.org/abs/2211.15205) | 本文提出了一种基于约束的内在动机方法（CIM），它利用任务先验信息构建内在目标，并适应性地平衡内在和显式目标。在稀疏奖励连续控制任务上，CIM显著优于现有内在动机方法。 |
| [^175] | [AdaTask: A Task-aware Adaptive Learning Rate Approach to Multi-task Learning.](http://arxiv.org/abs/2211.15055) | 提出了一种名为AdaTask的任务感知自适应学习率方法，通过自适应地调整不同任务的学习率，以平衡不同任务的重要性，从而在各种基准测试上始终优于现有的MTL方法。 |
| [^176] | [On the Universal Approximation Property of Deep Fully Convolutional Neural Networks.](http://arxiv.org/abs/2211.14047) | 本文研究了深度全卷积神经网络对平移不变或等变函数的逼近性能，证明了残差和非残差变体在特定条件下可以实现普适逼近，同时这些条件是必要的。 |
| [^177] | [Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement.](http://arxiv.org/abs/2210.17287) | Diffiner是一种基于DNN的生成细化器，可用于改善经过SE方法预处理后的感知语音质量。它可以应用于各种SE方法，且具有高度的模块化潜力。 |
| [^178] | [Masked Autoencoders Are Articulatory Learners.](http://arxiv.org/abs/2210.15195) | 本文提出了基于深度学习的Masked Autoencoders方法，可以精确重构被追踪错误的口腔学记录，有效地应用于XRMB数据集研究中。 |
| [^179] | [Comparison of neural closure models for discretised PDEs.](http://arxiv.org/abs/2210.14675) | 本文系统地比较了三个离散PDEs的神经封闭模型的训练过程，并发现先离散化再优化的轨迹拟合是首选，比导数拟合更准确、比先优化再离散化更稳定，但计算成本较高。 |
| [^180] | [EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search.](http://arxiv.org/abs/2210.06015) | 提出了一个能耗感知的神经架构搜索表格基准 EC-NAS，该基准通过添加能耗和碳足迹信息，支持设计能效高的深度学习模型，并降低总能耗。 |
| [^181] | [Unrolled Compressed Blind-Deconvolution.](http://arxiv.org/abs/2209.14165) | 在S-MBD问题中,我们提出了一种非展开式压缩方法，从少量的时间测量中无损恢复稀疏滤波器，并提出了一个数据驱动的非展开式学习框架来解决该问题。 |
| [^182] | [A Measure of the Complexity of Neural Representations based on Partial Information Decomposition.](http://arxiv.org/abs/2209.10438) | 本文提出了一种基于部分信息分解的“表示复杂度”度量，用于量化跨多个神经元扩散的信息访问难度，并证明了其实用性。 |
| [^183] | [Accelerated Primal-Dual Methods for Convex-Strongly-Concave Saddle Point Problems.](http://arxiv.org/abs/2209.04604) | 本文提出了一种加速的线性化原始-对偶（ALPD）方法，通过结合加速梯度下降和线性化PD方法，解决了处理凸-强凹鞍点问题中LPD方法对于原始函数Lipschitz常数依赖性次优的问题。对于SPP具有半线性耦合函数，ALPD方法实现了最优的梯度复杂度，而针对具有一般非线性耦合函数的SPP，我们提出了不精确的ALPD方法。 |
| [^184] | [Three New Validators and a Large-Scale Benchmark Ranking for Unsupervised Domain Adaptation.](http://arxiv.org/abs/2208.07360) | 本文对无监督领域自适应中估计准确性的三种新验证器进行了研究，并在100万个检查点的大数据集上与其他五种验证器进行了比较和排名。实验结果表明，我们提出的两个验证器优于现有的验证器，并且最佳的估计准确性方法因任务类型而异。 |
| [^185] | [DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization.](http://arxiv.org/abs/2207.05631) | 这篇论文提出了一个名为DGPO的算法，可以在解决任务时发现多种策略，从而提高策略鲁棒性和与用户交互的乐趣。 |
| [^186] | [Task-Agnostic Continual Reinforcement Learning: Gaining Insights and Overcoming Challenges.](http://arxiv.org/abs/2205.14495) | 本文研究了无任务偏置连续强化学习（CL）和多任务（MTL）代理性能差异的因素，并提出了基于重放的循环强化学习（3RL）方法，它优于基线方法，并证明在数据、计算或高维度受限设置下尤其有益。 |
| [^187] | [The Selectively Adaptive Lasso.](http://arxiv.org/abs/2205.10697) | 本文提出了一种新算法——Selectively Adaptive Lasso（SAL），它基于HAL的理论构建，保留了无维度、非参数收敛速率的优点，同时也具有可扩展到大规模高维数据集的能力。这种算法将许多回归系数自动设置为零。 |
| [^188] | [Transformer-based out-of-distribution detection for clinically safe segmentation.](http://arxiv.org/abs/2205.10650) | 本文提出了一种基于Transformer的完整3D超范围检测模型，针对头部CT中出血分割任务的远超范围和接近超范围情况，在超范围的处理中能够鲁棒地识别并排除超范围情况，经过评估，该方法具有更好的超范围性能，能够在超范围情况下提供可靠的分割结果。 |
| [^189] | [A Study on Transformer Configuration and Training Objective.](http://arxiv.org/abs/2205.10505) | 本文提出了Bamboo配置策略，基于更深更窄的Transformer结构进行Masked自编码器训练，在图像和语言任务上取得了新的最先进结果。 |
| [^190] | [gLaSDI: Parametric Physics-informed Greedy Latent Space Dynamics Identification.](http://arxiv.org/abs/2204.12005) | 本文提出了一种参数化的动力学识别方法gLaSDI，通过自编码器与局部DI模型的结合和自适应贪心采样算法来实现高维非线性动态系统的数据驱动降阶建模，具有精确、高效、鲁棒的特点。 |
| [^191] | [Adversarial Scratches: Deployable Attacks to CNN Classifiers.](http://arxiv.org/abs/2204.09397) | 本文提出了一种新型的对抗性攻击方法——对抗性痕迹攻击。通过在图像中制造痕迹的方式进行攻击，该方法比其他最先进的攻击方法更加可部署，并在公开API和交通标志的图像测试中表现出更高的破解率，攻击次数更少。 |
| [^192] | [Reinforcement Learning Policy Recommendation for Interbank Network Stability.](http://arxiv.org/abs/2204.07134) | 本文基于强化学习策略推荐，分析了其对人工银行间市场的影响。根据个体信息和公共推荐，金融机构制定借贷协议。策略推荐能够合理引导金融行为者的信贷关系形成，核心-周边网络结构迅速稳定。 |
| [^193] | [PyDTS: A Python Package for Discrete-Time Survival (Regularized) Regression with Competing Risks.](http://arxiv.org/abs/2204.05731) | PyDTS是一个用于离散时间生存数据半参数竞争风险模型的Python包，支持包括LASSO和弹性网等正则化回归方法。 |
| [^194] | [Optimality and complexity of classification by random projection.](http://arxiv.org/abs/2108.06339) | 本文研究了一组低复杂度分类器，该分类器可以近似于任意连续函数和布尔函数，且在给定类条件密度的情况下，其误差与最优误差相同。 |
| [^195] | [Epistemic Neural Networks.](http://arxiv.org/abs/2107.08924) | 该论文提出了一种能够通过适量级别的递增计算来估计神经网络不确定性的Epistemic神经网络框架，使得传统神经网络能够在计算成本大幅下降的情况下超越大型集成模型，为模型联合预测的方法提供了一种新的接口。 |
| [^196] | [Multi-layer Perceptron Trainability Explained via Variability.](http://arxiv.org/abs/2105.08911) | 本研究旨在解释多层感知器的可训练性，引入了一个新概念 - 变异性，与 MLP 的激活数量呈正相关，与"坍塌到常数"现象呈负相关，是 MLP 可训练性的一个准确预测指标。 |
| [^197] | [Few-shot Partial Multi-view Learning.](http://arxiv.org/abs/2105.02046) | 本文提出了一个新任务——少样本部分多视角学习，旨在克服低数据环境下视角缺失问题的影响。 |
| [^198] | [Universal Approximation Properties for an ODENet and a ResNet: Mathematical Analysis and Numerical Experiments.](http://arxiv.org/abs/2101.10229) | 本论文证明了对于一类ODENet和一类ResNet，“宽度为n+m的ODENet可以逼近${\rm \mathbb{R}^n}$上紧致子集上的任何连续函数”，同时推导了损失函数对某个调整变量的梯度并用于构建ODENet的学习算法，并在MNIST上进行实验。 |
| [^199] | [Optimal No-regret Learning in Repeated First-price Auctions.](http://arxiv.org/abs/2003.09795) | 本文提出了重复首价拍卖的最优无悔学习算法，通过利用特定的反馈结构和支付函数，实现了接近最优的遗憾界限。 |
| [^200] | [Open-set learning with augmented category by exploiting unlabeled data (Open-LACU).](http://arxiv.org/abs/2002.01368) | Open-LACU是一种新的开放式学习策略，它可以将分类器推广到观察到的和未观察到的新颖类别之间，并通过定义不同的背景和未知类别来提高训练成本效益性，确保在存在未观察到的新颖类别时进行安全分类。 |

# 详细

[^1]: 激发Web规模语音模型的潜在能力以实现零-shot任务泛化

    Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])

    [http://arxiv.org/abs/2305.11095](http://arxiv.org/abs/2305.11095)

    本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。

    

    本文研究了最近提出的Web规模语音模型Whisper的新兴功能，在使用提示工程技术调整模型后，适应了未见过的AVSR，CS-ASR和ST三个任务。我们设计了特定于任务的提示，要么利用另一个大规模模型，要么简单地操作默认提示中的特殊标记。实验证明，与默认提示相比，我们提出的提示使这三个零-shot任务的性能提高了10%到45％，甚至在一些数据集上超过了SotA监督模型。此外，我们的实验揭示了Whisper的许多有趣属性，包括其提示的鲁棒性，对口音的偏好以及潜在空间中的多语言理解。代码可在https://github.com/jasonppy/PromptingWhisper上找到。

    We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
    
[^2]: 基于基础模型的通用域适应

    Universal Domain Adaptation from Foundation Models. (arXiv:2305.11092v1 [cs.LG])

    [http://arxiv.org/abs/2305.11092](http://arxiv.org/abs/2305.11092)

    本论文对基于基础模型的通用域适应进行了研究，发现当前的UniDA方法无法超越基准表现，提出了一个简单的目标方法。

    

    基础模型（例如CLIP或DINOv2）已经展现了在广泛视觉任务中卓越的学习和转移能力，通过在大型数据语料库上训练并适应特定的下游任务。然而，有趣的是，基础模型尚未完全探索通用域适应（UniDA），即使用源域标记数据和目标域未标记数据学习模型，使学习模型能够成功适应目标数据。在本文中，我们利用基础模型对现有状态下的UniDA方法进行了全面的实证研究。我们首先证明，尽管基础模型极大地提高了仅在源数据上训练模型的基准方法的性能，但现有的UniDA方法通常不能超越基准。这表明，使用基础模型的UniDA需要新的研究努力。为此，我们提出了一种非常简单的目标方法。

    Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transferring capabilities on a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first demonstrate that, while foundation models greatly improve the performance of the baseline methods that train the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. To this end, we propose a very simple method of target 
    
[^3]: 生成扩散模型在离散状态空间中的应用：Blackout Diffusion

    Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces. (arXiv:2305.11089v1 [cs.LG])

    [http://arxiv.org/abs/2305.11089](http://arxiv.org/abs/2305.11089)

    本文提出了一种新的理论公式来实现任意离散状态Markov过程的生成扩散模型，并介绍了一种名为“Blackout Diffusion”的应用，它可以从空图像中生成样本。

    

    典型的生成扩散模型依赖于高斯扩散过程来训练反向转换，然后可以用于从高斯噪声中生成样本。然而，真实世界中的数据常常处于离散状态空间中，包括许多科学应用。在这里，我们使用精确（而不是变分）分析，针对任意离散状态Markov过程开发了前向扩散过程的理论公式。我们将这个理论与现有的连续状态高斯扩散以及其他离散扩散方法联系起来，并确定了连续时间情况下的反向随机过程和得分函数，以及离散时间情况下的反向映射。作为此框架的例子，我们介绍了“Blackout Diffusion”，它学会了从空图像中生成样本而不是从噪声中生成。在CIFAR-10、Binarized MNIST和CelebA数据集上的数值实验证实了它的可行性。

    Typical generative diffusion models rely on a Gaussian diffusion process for training the backward transformations, which can then be used to generate samples from Gaussian noise. However, real world data often takes place in discrete-state spaces, including many scientific applications. Here, we develop a theoretical formulation for arbitrary discrete-state Markov processes in the forward diffusion process using exact (as opposed to variational) analysis. We relate the theory to the existing continuous-state Gaussian diffusion as well as other approaches to discrete diffusion, and identify the corresponding reverse-time stochastic process and score function in the continuous-time setting, and the reverse-time mapping in the discrete-time setting. As an example of this framework, we introduce ``Blackout Diffusion'', which learns to produce samples from an empty image instead of from noise. Numerical experiments on the CIFAR-10, Binarized MNIST, and CelebA datasets confirm the feasibili
    
[^4]: 偏好还是意图？双重解缠协同过滤

    Preference or Intent? Double Disentangled Collaborative Filtering. (arXiv:2305.11084v1 [cs.IR])

    [http://arxiv.org/abs/2305.11084](http://arxiv.org/abs/2305.11084)

    本文提出了一种双重解缠协同过滤（DDCF）方法，该方法能够对意图和偏好因素进行分离，并通过解缠表示建立独立的稀疏偏好表示，从而提供更准确和可解释性的个性化推荐。

    

    人们选择物品时通常有不同的意图，而在相同意图下他们的偏好也可能不同。传统的协同过滤方法通常将意图和偏好因素纠缠在建模过程中，这显著限制了推荐性能的稳健性和可解释性。为了解决这一问题，本文提出了一种名为双重解缠协同过滤（DDCF）的个性化推荐方法。一级解缠是为了将意图和偏好的影响因素分开，而第二级解缠是为了构建独立的稀疏偏好表示。实验结果表明，DDCF方法在推荐精度和可解释性方面均优于现有方法。

    People usually have different intents for choosing items, while their preferences under the same intent may also different. In traditional collaborative filtering approaches, both intent and preference factors are usually entangled in the modeling process, which significantly limits the robustness and interpretability of recommendation performances. For example, the low-rating items are always treated as negative feedback while they actually could provide positive information about user intent. To this end, in this paper, we propose a two-fold representation learning approach, namely Double Disentangled Collaborative Filtering (DDCF), for personalized recommendations. The first-level disentanglement is for separating the influence factors of intent and preference, while the second-level disentanglement is performed to build independent sparse preference representations under individual intent with limited computational complexity. Specifically, we employ two variational autoencoder net
    
[^5]: 口罩人脸检测算法的比较研究

    A Comparative Study of Face Detection Algorithms for Masked Face Detection. (arXiv:2305.11077v1 [cs.CV])

    [http://arxiv.org/abs/2305.11077](http://arxiv.org/abs/2305.11077)

    本文对口罩人脸检测算法进行了比较研究，评估了一组代表性的人脸检测器在口罩人脸检测方面的表现，并讨论了其性能的可能贡献因素。

    

    当前的人脸检测算法需要应对许多挑战，例如姿态，光照和比例的变化。最近吸引了越来越多的关注的一个子类别是遮挡脸部检测，或更具体地说，探测戴口罩的脸部。在新冠疫情爆发后的三年里，现有人脸检测算法在口罩人脸上的表现如何仍然缺乏实证证据。本文首先简要回顾了最先进的人脸检测器和专为口罩人脸问题而设计的检测器，以及现有的口罩人脸数据集。我们评估并比较了一组良好代表性的人脸检测器在口罩人脸检测方面的性能，并总结了关于其性能可能的贡献因素的讨论。

    Contemporary face detection algorithms have to deal with many challenges such as variations in pose, illumination, and scale. A subclass of the face detection problem that has recently gained increasing attention is occluded face detection, or more specifically, the detection of masked faces. Three years on since the advent of the COVID-19 pandemic, there is still a complete lack of evidence regarding how well existing face detection algorithms perform on masked faces. This article first offers a brief review of state-of-the-art face detectors and detectors made for the masked face problem, along with a review of the existing masked face datasets. We evaluate and compare the performances of a well-representative set of face detectors at masked face detection and conclude with a discussion on the possible contributing factors to their performance.
    
[^6]: 基于图形上下文信息的语言模型增强以更好地理解文本数据

    Enriching language models with graph-based context information to better understand textual data. (arXiv:2305.11070v1 [cs.CL])

    [http://arxiv.org/abs/2305.11070](http://arxiv.org/abs/2305.11070)

    基于图形上下文信息的语言模型增强可更好的理解文本，实验表明该方法提高了BERT模型在Pubmed上的分类任务表现。

    

    每天遇到的文本具有相互联系的情况相当多。例如，Wikipedia文章通过超链接引用其他文章，科学论文通过引用或（共同）作者与其他论文相关联，而推文则通过关注彼此或转发内容来关联。因此，类似于图形的结构可以表示现有的联系，并被视为捕捉文本的“上下文”。因此，提取和整合这种上下文信息到语言模型中是否有助于更好地自动理解文本？在本研究中，我们实验性地证明，将基于图形的上下文化纳入BERT模型会增强其在分类任务示例上的表现。具体而言，在Pubmed数据集上，我们观察到误差从8.51％降至7.96％，同时仅增加了1.6％的参数量。

    A considerable number of texts encountered daily are somehow connected with each other. For example, Wikipedia articles refer to other articles via hyperlinks, scientific papers relate to others via citations or (co)authors, while tweets relate via users that follow each other or reshare content. Hence, a graph-like structure can represent existing connections and be seen as capturing the "context" of the texts. The question thus arises if extracting and integrating such context information into a language model might help facilitate a better automated understanding of the text. In this study, we experimentally demonstrate that incorporating graph-based contextualization into BERT model enhances its performance on an example of a classification task. Specifically, on Pubmed dataset, we observed a reduction in error from 8.51% to 7.96%, while increasing the number of parameters just by 1.6%.  Our source code: https://github.com/tryptofanik/gc-bert
    
[^7]: 利用ChatGPT和Stable Diffusion生成内容丰富、故事连贯的漫画

    Generating coherent comic with rich story using ChatGPT and Stable Diffusion. (arXiv:2305.11067v1 [cs.CV])

    [http://arxiv.org/abs/2305.11067](http://arxiv.org/abs/2305.11067)

    本文介绍了一种利用ChatGPT和Stable Diffusion生成连贯漫画故事的方法，通过引入新的评估AI故事的方式，并使用LoRA、ControlNet等方法进行fine-tuning，取得了在角色忠实度和艺术风格上的最先进表现。

    

    过去的研究表明，使用神经网络可以在保持音乐家音乐风格的基础上，扩展未完成的音乐作品。最近大型语言模型和扩散模型的进展使得我们能够生成有趣的漫画故事，并保持艺术家的艺术风格。在本文中，我们使用ChatGPT生成情节和对话，然后使用stable diffusion生成漫画。我们介绍了一种评估AI生成故事的新方法，并通过使用LoRA、ControlNet等方法对stable diffusion进行fine-tuning，达到了在角色忠实度和艺术风格上的SOTA表现。

    Past work demonstrated that using neural networks, we can extend unfinished music pieces while maintaining the music style of the musician. With recent advancements in large language models and diffusion models, we are now capable of generating comics with an interesting storyline while maintaining the art style of the artist. In this paper, we used ChatGPT to generate storylines and dialogue and then generated the comic using stable diffusion. We introduced a novel way to evaluate AI-generated stories, and we achieved SOTA performance on character fidelity and art style by fine-tuning stable diffusion using LoRA, ControlNet, etc.
    
[^8]: 透过平均线性化求解反问题：PETAL算法

    PETAL: Physics Emulation Through Averaged Linearizations for Solving Inverse Problems. (arXiv:2305.11056v1 [eess.SP])

    [http://arxiv.org/abs/2305.11056](http://arxiv.org/abs/2305.11056)

    PETAL算法利用已知的物理知识，将正演模型的线性化嵌入到模型本身中。它通过训练一个替代正演模型来解决具有已知或部分已知基于物理模型的反问题。

    

    反问题描述的是通过观测数据还原信号的任务。通常来说，观测数据通过应用于未知信号的某些非线性正演模型而产生。反转非线性正演模型通常计算成本高昂，因为它通常涉及在一系列估计值上计算和反转线性模型。本文提出了一种简单的学习加权平均模型，将正演模型的线性化嵌入到模型本身中，显式地融合已知的物理知识，提出了PETAL(Physics Emulation Through Averaged Linearizations)算法，它通过训练一个替代正演模型(emulator)来解决具有已知或部分已知基于物理模型的反问题。

    Inverse problems describe the task of recovering an underlying signal of interest given observables. Typically, the observables are related via some non-linear forward model applied to the underlying unknown signal. Inverting the non-linear forward model can be computationally expensive, as it often involves computing and inverting a linearization at a series of estimates. Rather than inverting the physics-based model, we instead train a surrogate forward model (emulator) and leverage modern auto-grad libraries to solve for the input within a classical optimization framework. Current methods to train emulators are done in a black box supervised machine learning fashion and fail to take advantage of any existing knowledge of the forward model. In this article, we propose a simple learned weighted average model that embeds linearizations of the forward model around various reference points into the model itself, explicitly incorporating known physics. Grounding the learned model with phy
    
[^9]: Tikhonov和RKHS正则化的小噪声分析

    Small noise analysis for Tikhonov and RKHS regularizations. (arXiv:2305.11055v1 [stat.ML])

    [http://arxiv.org/abs/2305.11055](http://arxiv.org/abs/2305.11055)

    该研究建立了一个小噪声分析框架，揭示了传统L2正则化范数的潜在不稳定性，并提出了一种自适应分数阶RKHS正则化器类来解决不稳定性，这些正则化器始终产生最佳的收敛速率。

    

    正则化在机器学习和反问题中起着至关重要的作用。然而，各种正则化范数的基本比较分析仍然未解决。我们建立了一个小噪声分析框架，以评估Tikhonov和RKHS正则化范数在高斯噪声的不适定线性反问题中的效果。该框架研究了正则化估计器在小噪声极限下的收敛速率，并揭示了传统L2正则化的潜在不稳定性。我们通过提出一种创新的自适应分数阶RKHS正则化器类来解决这种不稳定性，通过调整分数光滑度参数，该类覆盖了L2 Tikhonov和RKHS正则化器。一个令人惊奇的观点是，通过这些分数阶RKHS进行过度平滑始终产生最佳的收敛速率，但最佳的超参数可能衰减得太快而无法在实践中进行选择。

    Regularization plays a pivotal role in ill-posed machine learning and inverse problems. However, the fundamental comparative analysis of various regularization norms remains open. We establish a small noise analysis framework to assess the effects of norms in Tikhonov and RKHS regularizations, in the context of ill-posed linear inverse problems with Gaussian noise. This framework studies the convergence rates of regularized estimators in the small noise limit and reveals the potential instability of the conventional L2-regularizer. We solve such instability by proposing an innovative class of adaptive fractional RKHS regularizers, which covers the L2 Tikhonov and RKHS regularizations by adjusting the fractional smoothness parameter. A surprising insight is that over-smoothing via these fractional RKHSs consistently yields optimal convergence rates, but the optimal hyper-parameter may decay too fast to be selected in practice.
    
[^10]: NODE-ImgNet: 一种使用 PDE 的有效和稳健的图像去噪模型

    NODE-ImgNet: a PDE-informed effective and robust model for image denoising. (arXiv:2305.11049v1 [eess.IV])

    [http://arxiv.org/abs/2305.11049](http://arxiv.org/abs/2305.11049)

    本文提出了一种新型的神经网络架构 NODE-ImgNet，该模型结合了神经常微分方程和卷积神经网络块，它是一种使用 PDE 的有效和稳健的图像去噪模型。该模型能够自然地避免学习过程中引入伪像，实现了更高准确性和参数效率，并在不同场景下展示了一致的有效性和优势。

    

    受传统偏微分方程方法的启发，我们提出了一种新颖的神经网络体系框架 NODE-ImgNet，将神经常微分方程（NODEs）与卷积神经网络（CNN）块相结合。 NODE-ImgNet 是本质上的 PDE 模型，动态系统是隐式学习的，而不是显式指定 PDE，自然地避免了学习过程中引入伪像的典型问题。通过调用这种 NODE 结构，可以将其视为残差网络（ResNet）的连续变体，继承其在图像去噪中的优点，我们的模型实现了增强的准确性和参数效率。特别是，在不同场景下，包括受高斯噪声干扰的灰度和彩色图像去噪，以及真实噪声图像学习方面，我们的模型展示了一致的有效性，并展示了从小图像数据集中学习的优势。

    Inspired by the traditional partial differential equation (PDE) approach for image denoising, we propose a novel neural network architecture, referred as NODE-ImgNet, that combines neural ordinary differential equations (NODEs) with convolutional neural network (CNN) blocks. NODE-ImgNet is intrinsically a PDE model, where the dynamic system is learned implicitly without the explicit specification of the PDE. This naturally circumvents the typical issues associated with introducing artifacts during the learning process. By invoking such a NODE structure, which can also be viewed as a continuous variant of a residual network (ResNet) and inherits its advantage in image denoising, our model achieves enhanced accuracy and parameter efficiency. In particular, our model exhibits consistent effectiveness in different scenarios, including denoising gray and color images perturbed by Gaussian noise, as well as real-noisy images, and demonstrates superiority in learning from small image datasets
    
[^11]: DC规划算法在子模最小化问题上的应用

    Difference of Submodular Minimization via DC Programming. (arXiv:2305.11046v1 [cs.LG])

    [http://arxiv.org/abs/2305.11046](http://arxiv.org/abs/2305.11046)

    本文介绍了一种新的算法，利用DC规划算法来解决子模最小化问题，并证明收敛性质比现有算法更全面，同时在语音特征选择和文档摘要等应用中取得更好的性能。

    

    在各种机器学习问题中，最小化两个子模（DS）函数的差异是一个自然产生的问题。虽然已经有人知道DS问题可以等价地转化为两个凸（DC）函数的差异最小化问题，但现有算法并没有充分利用这种联系。对于DC问题，一个经典的算法叫做DC算法（DCA）。我们介绍了DCA及其完整形式（CDCA）的变体，并将其应用于对应于DS最小化的DC程序中。我们扩展了DCA的现有收敛性质，并将它们与DS问题的收敛性质联系起来。我们的DCA结果与现有的DS算法满足相同的理论保证，同时提供了更完整的收敛性质描述。对于CDCA的情况，我们获得了更强的局部最小保证。我们的数字实验结果表明，我们提出的算法在两个应用——语音语料库选择特征优化和文档摘要中均优于现有的基线算法。

    Minimizing the difference of two submodular (DS) functions is a problem that naturally occurs in various machine learning problems. Although it is well known that a DS problem can be equivalently formulated as the minimization of the difference of two convex (DC) functions, existing algorithms do not fully exploit this connection. A classical algorithm for DC problems is called the DC algorithm (DCA). We introduce variants of DCA and its complete form (CDCA) that we apply to the DC program corresponding to DS minimization. We extend existing convergence properties of DCA, and connect them to convergence properties on the DS problem. Our results on DCA match the theoretical guarantees satisfied by existing DS algorithms, while providing a more complete characterization of convergence properties. In the case of CDCA, we obtain a stronger local minimality guarantee. Our numerical results show that our proposed algorithms outperform existing baselines on two applications: speech corpus sel
    
[^12]: 一种信息论通用泛化界统一框架

    A unified framework for information-theoretic generalization bounds. (arXiv:2305.11042v1 [cs.LG])

    [http://arxiv.org/abs/2305.11042](http://arxiv.org/abs/2305.11042)

    该论文提出了一种基于概率去相关引理和概率测度空间中一些其他技术的通用方法，可以得到新的学习算法的信息论泛化上限，并且能够恢复许多现有的泛化界，如基于互信息、条件互信息、随机chaining和PAC-Bayes不等式的界。

    

    本文提出了一种通用的方法来导出学习算法的信息论泛化界。主要的技术工具是基于改变测度和松弛Young不等式在$L_{\psi_p}$Orlicz空间中的概率去相关性引理。采用去相关性引理与其他技术，如对称化、耦合和概率测度空间中的chaining，我们得到了新的泛化误差上限，包括期望和高概率，同时，我们也恢复了许多现有的泛化界，包括基于互信息、条件互信息、随机chaining和PAC-Bayes不等式的界。此外，Fernique-Talagrand上界也作为一种特殊情况呈现出来。

    This paper presents a general methodology for deriving information-theoretic generalization bounds for learning algorithms. The main technical tool is a probabilistic decorrelation lemma based on a change of measure and a relaxation of Young's inequality in $L_{\psi_p}$ Orlicz spaces. Using the decorrelation lemma in combination with other techniques, such as symmetrization, couplings, and chaining in the space of probability measures, we obtain new upper bounds on the generalization error, both in expectation and in high probability, and recover as special cases many of the existing generalization bounds, including the ones based on mutual information, conditional mutual information, stochastic chaining, and PAC-Bayes inequalities. In addition, the Fernique-Talagrand upper bound on the expected supremum of a subgaussian process emerges as a special case.
    
[^13]: 去噪自编码器的高维渐近分析

    High-dimensional Asymptotics of Denoising Autoencoders. (arXiv:2305.11041v1 [cs.LG])

    [http://arxiv.org/abs/2305.11041](http://arxiv.org/abs/2305.11041)

    本文研究了去噪自编码器在高维极限下的性能表现，得出了去噪均方测试误差的闭式表达式，并揭示了有跳跃连接的自编码器相较于传统自编码器的优越性。

    

    本文研究应用带有绑定权重和跳跃连接的二层非线性自编码器来去噪高斯混合数据的问题。我们在高维极限下考虑，其中训练样本数和输入维数共同趋向于无穷大，而隐藏单元数保持有限。我们提供了去噪均方测试误差的闭式表达式。基于这个结果，我们定量地表征了所考虑的架构在自编码器（没有关联到主成分分析）的跳跃连接相关性之上的优势。此外，我们进一步证明了我们的结果准确地捕捉了一系列真实数据集上的学习曲线。

    We address the problem of denoising data from a Gaussian mixture using a two-layer non-linear autoencoder with tied weights and a skip connection. We consider the high-dimensional limit where the number of training samples and the input dimension jointly tend to infinity while the number of hidden units remains bounded. We provide closed-form expressions for the denoising mean-squared test error. Building on this result, we quantitatively characterize the advantage of the considered architecture over the autoencoder without the skip connection that relates closely to principal component analysis. We further show that our results accurately capture the learning curves on a range of real data sets.
    
[^14]: 使用Grover算法模拟变分量子感知器

    Simulation of a Variational Quantum Perceptron using Grover's Algorithm. (arXiv:2305.11040v1 [quant-ph])

    [http://arxiv.org/abs/2305.11040](http://arxiv.org/abs/2305.11040)

    本研究结合了量子变分电路和Grover算法，成功构建了新型的量子感知器QVP-G。对比经典感知器，我们证明了QVP-G在分类任务中比QVP更加准确并且更加高效。

    

    量子感知器、变分电路和Grover算法被认为是量子机器学习的有前途的组件。该论文介绍了一种结合了量子变分电路和Grover算法的新型量子感知器。通过计算它们的损失函数以及分析它们在分类任务上的准确性，并将这两个量子模型与经典感知器进行比较，我们研究了QVP和QVP-G的性能。结果表明，我们的两个量子模型比CP更有效率，我们的新型建议模型QVP-G胜过QVP，证明了Grover算法可以应用在分类任务上，而且还可以提高模型的准确性，除了非结构化搜索问题。

    The quantum perceptron, the variational circuit, and the Grover algorithm have been proposed as promising components for quantum machine learning. This paper presents a new quantum perceptron that combines the quantum variational circuit and the Grover algorithm. However, this does not guarantee that this quantum variational perceptron with Grover's algorithm (QVPG) will have any advantage over its quantum variational (QVP) and classical counterparts. Here, we examine the performance of QVP and QVP-G by computing their loss function and analyzing their accuracy on the classification task, then comparing these two quantum models to the classical perceptron (CP). The results show that our two quantum models are more efficient than CP, and our novel suggested model QVP-G outperforms the QVP, demonstrating that the Grover can be applied to the classification task and even makes the model more accurate, besides the unstructured search problems.
    
[^15]: Deep PackGen：一种用于生成对抗性网络数据包的深度强化学习框架

    Deep PackGen: A Deep Reinforcement Learning Framework for Adversarial Network Packet Generation. (arXiv:2305.11039v1 [cs.CR])

    [http://arxiv.org/abs/2305.11039](http://arxiv.org/abs/2305.11039)

    研究了网络入侵检测系统中的对抗攻击方法，提出了用深度强化学习生成对抗数据包的框架，旨在提高生成对抗性网络数据包的效果。

    

    最近人工智能和机器学习算法的发展以及更快的计算基础设施的可用性，通过开发机器学习辅助的网络入侵检测系统（NIDS），增强了网络安全操作中心（防御者）的安全姿态。与此同时，攻击者利用AI / ML模型的支持也增加了逃避安全检测的能力。因此，防御者需要积极准备针对利用NIDS检测机制的逃避攻击。最近的研究发现，对基于流和基于数据包的特征进行扰动可以欺骗机器学习模型，但这些方法存在局限性。对流基础特征进行扰动难以逆向工程，而使用对数据包特征进行扰动的样本不可重现。我们的方法框架“Deep PackGen”利用深度强化学习生成对抗性数据包，并旨在超越现有对抗生成性技术的局限性，提高生成对抗网络数据包的效果。

    Recent advancements in artificial intelligence (AI) and machine learning (ML) algorithms, coupled with the availability of faster computing infrastructure, have enhanced the security posture of cybersecurity operations centers (defenders) through the development of ML-aided network intrusion detection systems (NIDS). Concurrently, the abilities of adversaries to evade security have also increased with the support of AI/ML models. Therefore, defenders need to proactively prepare for evasion attacks that exploit the detection mechanisms of NIDS. Recent studies have found that the perturbation of flow-based and packet-based features can deceive ML models, but these approaches have limitations. Perturbations made to the flow-based features are difficult to reverse-engineer, while samples generated with perturbations to the packet-based features are not playable.  Our methodological framework, Deep PackGen, employs deep reinforcement learning to generate adversarial packets and aims to over
    
[^16]: 可视化问答：最近文献中技术和常见趋势综述

    Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature. (arXiv:2305.11033v1 [cs.CV])

    [http://arxiv.org/abs/2305.11033](http://arxiv.org/abs/2305.11033)

    本文综述了最近文献中的可视化问答问题，提供了对该领域的深入分析和比较，包括结果、最新技术、常见错误以及未来研究的潜在改进点。

    

    可视化问答（VQA）是自然语言处理和图像预测中的一个新兴问题，需要算法回答有关特定图像的问题。在本文中，作者分析了25项最新研究和6个数据集，并提供了下载链接。作者深入调研了该领域的多项研究，并提供了分析比较，包括结果、最新技术、常见错误以及未来研究的潜在改进点。

    Visual Question Answering (VQA) is an emerging area of interest for researches, being a recent problem in natural language processing and image prediction. In this area, an algorithm needs to answer questions about certain images. As of the writing of this survey, 25 recent studies were analyzed. Besides, 6 datasets were analyzed and provided their link to download. In this work, several recent pieces of research in this area were investigated and a deeper analysis and comparison among them were provided, including results, the state-of-the-art, common errors, and possible points of improvement for future researchers.
    
[^17]: 乐观自然策略梯度：一种简单高效的在线强化学习策略优化框架

    Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL. (arXiv:2305.11032v1 [cs.LG])

    [http://arxiv.org/abs/2305.11032](http://arxiv.org/abs/2305.11032)

    本文提出了一种乐观自然策略梯度的在线强化学习策略优化框架，采用乐观策略评估子程序以鼓励探索，适用于线性MDP，样本复杂度具有最优维度依赖关系。

    

    尽管策略优化算法对于近期强化学习的实证成功发挥了重要作用，但策略优化的现有理论理解仍然相当有限 - 它们要么局限于表格MDP，要么在在线强化学习中存在高度亚最优的样本复杂度问题。本文提出了一种简单高效的在线强化学习策略优化框架 - 乐观自然策略梯度。乐观自然策略梯度可以看作是将经典自然策略梯度算法[Kakade，2001]与乐观策略评估子程序简单组合以鼓励探索。对于$d$-维线性MDP，乐观自然策略梯度具有计算效率，并且在$\tilde{O}(d^2/\varepsilon^3)$ 次采样内学习 $\varepsilon$ -最优策略，这是第一个具有最优维度依赖关系$\tilde {\Theta}(d^2)$样本复杂度的计算高效算法。它也超越了目前领先的一些状态of-the-art算法。

    While policy optimization algorithms have played an important role in recent empirical success of Reinforcement Learning (RL), the existing theoretical understanding of policy optimization remains rather limited -- they are either restricted to tabular MDPs or suffer from highly suboptimal sample complexity, especial in online RL where exploration is necessary. This paper proposes a simple efficient policy optimization framework -- Optimistic NPG for online RL. Optimistic NPG can be viewed as simply combining of the classic natural policy gradient (NPG) algorithm [Kakade, 2001] with optimistic policy evaluation subroutines to encourage exploration. For $d$-dimensional linear MDPs, Optimistic NPG is computationally efficient, and learns an $\varepsilon$-optimal policy within $\tilde{O}(d^2/\varepsilon^3)$ samples, which is the first computationally efficient algorithm whose sample complexity has the optimal dimension dependence $\tilde{\Theta}(d^2)$. It also improves over state-of-the-a
    
[^18]: 大规模并行重新加权唤醒-睡眠

    Massively Parallel Reweighted Wake-Sleep. (arXiv:2305.11022v1 [cs.LG])

    [http://arxiv.org/abs/2305.11022](http://arxiv.org/abs/2305.11022)

    大规模并行重新加权唤醒-睡眠算法通过抽取$K^n$个可能的样本组合，避免了原方法中大量潜在变量数目导致有效性下降问题。

    

    重新加权唤醒-睡眠算法（RWS）是一种适用于非常通用的模型执行贝叶斯推断的机器学习方法。它从潜在近似后验概率中抽取$K$个样本，然后使用重要性加权来提供更好的真实后验概率估计。RWS然后更新其近似后验概率，向真实后验概率的重要性加权估计移动。然而，近期的研究表明，对于有效的重要性加权，所需样本数与潜在变量的数量呈指数关系。在所有但最小的模型中实现如此大数量的重要性样本是不可行的。 在这里，我们开发了大规模并行的RWS，通过抽取所有$n$个潜在变量的$K$个样本，并单独考虑所有$K^n$个可能的样本组合，避免了这个问题。虽然考虑$K^n$个组合似乎是不可行的，但所需的计算可以通过利用计算结构简化在多项式时间内完成。

    Reweighted wake-sleep (RWS) is a machine learning method for performing Bayesian inference in a very general class of models. RWS draws $K$ samples from an underlying approximate posterior, then uses importance weighting to provide a better estimate of the true posterior. RWS then updates its approximate posterior towards the importance-weighted estimate of the true posterior. However, recent work [Chattergee and Diaconis, 2018] indicates that the number of samples required for effective importance weighting is exponential in the number of latent variables. Attaining such a large number of importance samples is intractable in all but the smallest models. Here, we develop massively parallel RWS, which circumvents this issue by drawing $K$ samples of all $n$ latent variables, and individually reasoning about all $K^n$ possible combinations of samples. While reasoning about $K^n$ combinations might seem intractable, the required computations can be performed in polynomial time by exploiti
    
[^19]: 深度度量张量正则化策略梯度

    Deep Metric Tensor Regularized Policy Gradient. (arXiv:2305.11017v1 [cs.LG])

    [http://arxiv.org/abs/2305.11017](http://arxiv.org/abs/2305.11017)

    本文研究通过引入度量张量场$g_ab$将欧几里得策略参数空间推广到一般的黎曼流形上，并提出了一种新的深度度量张量正则化策略梯度算法。

    

    策略梯度算法是深度强化学习技术中重要的一类。过去的许多研究都着眼于使用一阶策略梯度信息来训练策略网络。与这些工作不同，本文的研究基于这样的信念：合理利用和控制与策略梯度相关的海森信息可以显著提高策略梯度算法的性能。我们关注的一个关键海森信息是海森跟踪值，它给出了欧几里得策略参数空间中策略梯度向量场的发散。我们的目标是通过在参数空间引入度量张量场$g_ab$来将欧几里得策略参数空间推广到一般的黎曼流形上。这通过新开发的数学工具、深度学习算法和度量张量深度神经网络(DNN)来实现。拥有这些技术发展，我们提出了一种新的深度度量张量正则化策略梯度算法。

    Policy gradient algorithms are an important family of deep reinforcement learning techniques. Many past research endeavors focused on using the first-order policy gradient information to train policy networks. Different from these works, we conduct research in this paper driven by the believe that properly utilizing and controlling Hessian information associated with the policy gradient can noticeably improve the performance of policy gradient algorithms. One key Hessian information that attracted our attention is the Hessian trace, which gives the divergence of the policy gradient vector field in the Euclidean policy parametric space. We set the goal to generalize this Euclidean policy parametric space into a general Riemmanian manifold by introducing a metric tensor field $g_ab$ in the parametric space. This is achieved through newly developed mathematical tools, deep learning algorithms, and metric tensor deep neural networks (DNNs). Armed with these technical developments, we propo
    
[^20]: 拍卖设计中的模式连通性

    Mode Connectivity in Auction Design. (arXiv:2305.11005v1 [cs.GT])

    [http://arxiv.org/abs/2305.11005](http://arxiv.org/abs/2305.11005)

    该论文研究了拍卖设计领域的一个基本问题，即最优拍卖设计。在研究中，作者证明了神经网络在一定条件下可以通过简单的分段线性路径连接不同的局部最优解，并取得了成功。

    

    最优拍卖设计是算法博弈论中的一个基本问题，即使在非常简单的情况下，这个问题也很难。最近不同的经济学可微分理论表明，神经网络可以有效地学习已知的最优拍卖机制，发现有趣的新机制。为了理论上证明它们的实证成功，我们聚焦于第一个这样的网络，RochetNet，并研究所谓的仿射极大化拍卖的广义版本。我们证明它们满足模式连通性，即局部最优解通过一个简单的分段线性路径连接，路径上的每个解都几乎和两个局部最优解之一一样好。模式连通性最近被证明是神经网络用于预测问题的一个有趣的经验和理论的属性。我们的结果是对可微分经济学领域中神经网络用于解决非线性设计问题的第一个这样的分析。

    Optimal auction design is a fundamental problem in algorithmic game theory. This problem is notoriously difficult already in very simple settings. Recent work in differentiable economics showed that neural networks can efficiently learn known optimal auction mechanisms and discover interesting new ones. In an attempt to theoretically justify their empirical success, we focus on one of the first such networks, RochetNet, and a generalized version for affine maximizer auctions. We prove that they satisfy mode connectivity, i.e., locally optimal solutions are connected by a simple, piecewise linear path such that every solution on the path is almost as good as one of the two local optima. Mode connectivity has been recently investigated as an intriguing empirical and theoretically justifiable property of neural networks used for prediction problems. Our results give the first such analysis in the context of differentiable economics, where neural networks are used directly for solving non-
    
[^21]: 通过调制掩码分享终身强化学习知识

    Sharing Lifelong Reinforcement Learning Knowledge via Modulating Masks. (arXiv:2305.10997v1 [cs.LG])

    [http://arxiv.org/abs/2305.10997](http://arxiv.org/abs/2305.10997)

    通过调制掩码机制来实现多个代理之间的特定知识共享和强化学习，最终实现高效的分布式终身学习。

    

    终身学习代理旨在在其生命周期内逐渐学习多个任务。这需要在学习新任务时利用以前的知识并避免遗忘。最近，一种特定类型的参数隔离方法——调制掩码在监督学习和强化学习中表现出了潜力。虽然终身学习算法主要在单个代理的情况下进行研究，但仍存在一个问题，即多个代理如何彼此分享终身学习知识。我们发现，调制掩码所使用的参数隔离机制特别适合在分布式和去中心化的终身学习系统中的代理之间交换知识。关键思想是将特定任务知识隔离到特定的掩码中，让代理可以按需转移特定的知识，从而实现强大而有效的分布式终身学习。我们假设动态年龄全分布式异步情况下进行实验。

    Lifelong learning agents aim to learn multiple tasks sequentially over a lifetime. This involves the ability to exploit previous knowledge when learning new tasks and to avoid forgetting. Modulating masks, a specific type of parameter isolation approach, have recently shown promise in both supervised and reinforcement learning. While lifelong learning algorithms have been investigated mainly within a single-agent approach, a question remains on how multiple agents can share lifelong learning knowledge with each other. We show that the parameter isolation mechanism used by modulating masks is particularly suitable for exchanging knowledge among agents in a distributed and decentralized system of lifelong learners. The key idea is that the isolation of specific task knowledge to specific masks allows agents to transfer only specific knowledge on-demand, resulting in robust and effective distributed lifelong learning. We assume fully distributed and asynchronous scenarios with dynamic age
    
[^22]: 理解差分隐私生成模型如何使用隐私预算

    Understanding how Differentially Private Generative Models Spend their Privacy Budget. (arXiv:2305.10994v1 [cs.LG])

    [http://arxiv.org/abs/2305.10994](http://arxiv.org/abs/2305.10994)

    本文分析了采用差分隐私训练的生成模型如何分配隐私预算，以及影响分配的因素。使用不同的模型适合于不同的任务和设置。

    

    采用差分隐私训练的生成模型被广泛应用于产生合成数据，同时减少隐私风险。但是在不同的应用场景中找到最适合的模型，需要权衡它们之间的隐私-效用关系。本文针对表格数据，分析了DP生成模型如何分配隐私预算，并探讨了影响隐私预算分配的主要因素。我们对图形和深度生成模型进行了广泛的评估，揭示了不同模型适用于不同设置和任务的独特特征。

    Generative models trained with Differential Privacy (DP) are increasingly used to produce synthetic data while reducing privacy risks. Navigating their specific privacy-utility tradeoffs makes it challenging to determine which models would work best for specific settings/tasks. In this paper, we fill this gap in the context of tabular data by analyzing how DP generative models distribute privacy budgets across rows and columns, arguably the main source of utility degradation. We examine the main factors contributing to how privacy budgets are spent, including underlying modeling techniques, DP mechanisms, and data dimensionality.  Our extensive evaluation of both graphical and deep generative models sheds light on the distinctive features that render them suitable for different settings and tasks. We show that graphical models distribute the privacy budget horizontally and thus cannot handle relatively wide datasets while the performance on the task they were optimized for monotonicall
    
[^23]: SPENSER：面向卷积脉冲神经网络的神经进化算法探索

    SPENSER: Towards a NeuroEvolutionary Approach for Convolutional Spiking Neural Networks. (arXiv:2305.10987v1 [cs.NE])

    [http://arxiv.org/abs/2305.10987](http://arxiv.org/abs/2305.10987)

    SPENSER是一种基于神经进化算法探索的自动设计和参数化的SNNs 生成框架，可应用于图像分类。

    

    脉冲神经网络（SNNs）由于其能源效率和生物可行性而受到了近来的关注。然而，SNNs 的性能仍然落后于传统的人工神经网络（ANNs），因为目前还没有关于SNNs 的最佳学习算法的共识。目前表现最好的SNNs 是基于ANNs 转化或使用基于脉冲的反向传播学习。最近的研究重点是开发和测试不同的学习策略，并进行手工架构和参数调整。神经进化（NE）已被证明是一种自动设计ANNs 和调整参数的有效方式，但其在SNNs 中的应用仍处于早期阶段。DENSER 是一种基于遗传算法（GA）和结构语法进化（SGE）原则的自动设计ANNs 和参数化的NE框架。本文提出了SPENSER，一种基于DENSER 的SNNs 生成NE框架，用于图像分类。

    Spiking Neural Networks (SNNs) have attracted recent interest due to their energy efficiency and biological plausibility. However, the performance of SNNs still lags behind traditional Artificial Neural Networks (ANNs), as there is no consensus on the best learning algorithm for SNNs. Best-performing SNNs are based on ANN to SNN conversion or learning with spike-based backpropagation through surrogate gradients. The focus of recent research has been on developing and testing different learning strategies, with hand-tailored architectures and parameter tuning. Neuroevolution (NE), has proven successful as a way to automatically design ANNs and tune parameters, but its applications to SNNs are still at an early stage. DENSER is a NE framework for the automatic design and parametrization of ANNs, based on the principles of Genetic Algorithms (GA) and Structured Grammatical Evolution (SGE). In this paper, we propose SPENSER, a NE framework for SNN generation based on DENSER, for image clas
    
[^24]: 学习为稀疏神经网络激活函数设置

    Learning Activation Functions for Sparse Neural Networks. (arXiv:2305.10964v1 [cs.LG])

    [http://arxiv.org/abs/2305.10964](http://arxiv.org/abs/2305.10964)

    本论文针对稀疏神经网络的准确性下降问题，发现激活函数和超参数是导致问题的主要原因，提出学习为稀疏网络调整激活函数并分开超参数优化方案的解决方法。

    

    稀疏神经网络（SNN）在推断时可以节省大量能量和内存，同时可以表现出类似于密集神经网络的性能。 然而，在高修剪比率下SNN的准确度降低可能在关键部署条件下成为问题。 在最近的研究中，通过复杂的修剪技术来缓解这个问题，但我们关注被忽略的因素：超参数和激活函数。 我们的分析表明，准确度下降可以额外归因于（i）普遍使用ReLU作为激活函数的默认选择，以及（ii）使用与密集网络相同的超参数来微调SNN。 因此，我们专注于学习为稀疏网络调整激活函数，并将其与稀疏网络的分开超参数优化方案相结合。 通过对在MNIST上训练的流行DNN模型（LeNet-5，VGG-16，ResNet-18和EfficientNet-B0）进行实验

    Sparse Neural Networks (SNNs) can potentially demonstrate similar performance to their dense counterparts while saving significant energy and memory at inference. However, the accuracy drop incurred by SNNs, especially at high pruning ratios, can be an issue in critical deployment conditions. While recent works mitigate this issue through sophisticated pruning techniques, we shift our focus to an overlooked factor: hyperparameters and activation functions. Our analyses have shown that the accuracy drop can additionally be attributed to (i) Using ReLU as the default choice for activation functions unanimously, and (ii) Fine-tuning SNNs with the same hyperparameters as dense counterparts. Thus, we focus on learning a novel way to tune activation functions for sparse networks and combining these with a separate hyperparameter optimization (HPO) regime for sparse networks. By conducting experiments on popular DNN models (LeNet-5, VGG-16, ResNet-18, and EfficientNet-B0) trained on MNIST, CI
    
[^25]: 使用物理解释神经网络的演员-评论方法：控制冷却液冷却的电池包的1D PDE模型。

    Actor-Critic Methods using Physics-Informed Neural Networks: Control of a 1D PDE Model for Fluid-Cooled Battery Packs. (arXiv:2305.10952v1 [cs.LG])

    [http://arxiv.org/abs/2305.10952](http://arxiv.org/abs/2305.10952)

    使用演员-评论算法和物理解释神经网络，本文提出了一种用于控制冷却液冷却的电池包温度的方法，其实现了最优控制。

    

    本文提出了一个演员评论算法，用于控制使用冷却液的电池组的温度。模型是由一个带有控制对流项的耦合的1D偏微分方程(PDE)建模的。哈密顿-雅各比-贝尔曼（HJB）方程是一个PDE，用于评估值函数的最优性并确定一个最优控制器。我们提出了一种算法，将价值网络视为物理解释神经网络(PINN)以解决连续时间的HJB方程，而不是离散时间的Bellman最优方程，并导出了一个为环境提供优化控制的最优控制器。我们的实验表明，一个混合策略方法，使用HJB方程更新价值网络并像PPO一样更新策略网络，可以在控制这个PDE系统方面实现最佳结果。

    This paper proposes an actor-critic algorithm for controlling the temperature of a battery pack using a cooling fluid. This is modeled by a coupled 1D partial differential equation (PDE) with a controlled advection term that determines the speed of the cooling fluid. The Hamilton-Jacobi-Bellman (HJB) equation is a PDE that evaluates the optimality of the value function and determines an optimal controller. We propose an algorithm that treats the value network as a Physics-Informed Neural Network (PINN) to solve for the continuous-time HJB equation rather than a discrete-time Bellman optimality equation, and we derive an optimal controller for the environment that we exploit to achieve optimal control. Our experiments show that a hybrid-policy method that updates the value network using the HJB equation and updates the policy network identically to PPO achieves the best results in the control of this PDE system.
    
[^26]: 关于纯16位浮点神经网络的辩护

    In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])

    [http://arxiv.org/abs/2305.10947](http://arxiv.org/abs/2305.10947)

    本文探讨了纯16位浮点神经网络的被忽视的效率，提供了理论分析来探讨16位和32位模型的差异，并可以定量解释16位模型与其32位对应物之间的条件。

    

    减少编码神经网络权重和激活所需的位数是非常可取的，因为它可以加快神经网络的训练和推理时间，同时减少内存消耗。因此，这一领域的研究引起了广泛关注，以开发利用更低精度计算的神经网络，比如混合精度训练。有趣的是，目前不存在纯16位浮点设置的方法。本文揭示了纯16位浮点神经网络被忽视的效率。我们通过提供全面的理论分析来探讨造成16位和32位模型的差异的因素。我们规范化了浮点误差和容忍度的概念，从而可以定量解释16位模型与其32位对应物之间密切逼近结果的条件。这种理论探索提供了新的视角。

    Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
    
[^27]: 基于可重构智能表面赋能的边缘推理的李雅普诺夫深度强化学习

    Lyapunov-Driven Deep Reinforcement Learning for Edge Inference Empowered by Reconfigurable Intelligent Surfaces. (arXiv:2305.10931v1 [cs.IT])

    [http://arxiv.org/abs/2305.10931](http://arxiv.org/abs/2305.10931)

    本文提出了一种基于李雅普诺夫深度强化学习的动态学习算法，在可重构智能表面赋能的6G网络中，能够同时优化数据压缩方案、无线资源分配、计算资源和RIS反射参数，实现了能效推理分类。

    

    本文针对具备可重构智能表面(RIS)的6G网络中无线边缘能耗低、时延低、准确度高的推理，提出了一种新颖的算法。我们考虑一个场景，其中一组设备不断产生收集数据，并通过动态排队系统处理。基于李雅普诺夫随机优化和深度强化学习(DRL)的婚姻，我们设计了一种动态学习算法，联合优化数据压缩方案、无线资源分配（即功率、传输预编码）、计算资源（即CPU周期）和RIS反射参数（即相移），以实现满足端到端(E2E)时延和推理准确性的能效推理分类。所提出的策略启用了动态控制系统和无线传播环境，实现了低复杂度优化。

    In this paper, we propose a novel algorithm for energy-efficient, low-latency, accurate inference at the wireless edge, in the context of 6G networks endowed with reconfigurable intelligent surfaces (RISs). We consider a scenario where new data are continuously generated/collected by a set of devices and are handled through a dynamic queueing system. Building on the marriage between Lyapunov stochastic optimization and deep reinforcement learning (DRL), we devise a dynamic learning algorithm that jointly optimizes the data compression scheme, the allocation of radio resources (i.e., power, transmission precoding), the computation resources (i.e., CPU cycles), and the RIS reflectivity parameters (i.e., phase shifts), with the aim of performing energy-efficient edge classification with end-to-end (E2E) delay and inference accuracy constraints. The proposed strategy enables dynamic control of the system and of the wireless propagation environment, performing a low-complexity optimization 
    
[^28]: 历史报纸广告中的多语言事件提取

    Multilingual Event Extraction from Historical Newspaper Adverts. (arXiv:2305.10928v1 [cs.CL])

    [http://arxiv.org/abs/2305.10928](http://arxiv.org/abs/2305.10928)

    本文针对历史文本中未被充分开发的事件提取任务，介绍了一个新的多语言数据集，通过迁移学习和领域自适应技术，即使数据注释很少也可以获得出人意料的好结果。同时，所提出的模型提供了新的洞察力，展示了该时期奴役话语的语言使用和模式。研究表明，语言迁移可以有效用于历史文本中的多语言事件提取。

    

    自然语言处理可以帮助历史学家分析比手工可行得多的文本材料。然而，开发这种方法有着实际的挑战。首先，获取大型注释历史文本数据集非常困难，因为只有专业领域的专家才能可靠地注释。其次，大部分现有的自然语言处理模型是在现代语言文本上训练的，因此在应用于历史语料库时效果显著降低。这对较少研究的任务以及非英语语言来说尤其棘手。本文针对这些挑战，聚焦于历史文本领域中未被充分开发的事件提取任务。我们介绍了一个新的多语言数据集，其中包括英语、法语和荷兰语，由早期殖民时期的报纸广告构成，报道了从奴役中自由的被奴役人。我们发现：即使数据注释很少，通过从现代数据集进行迁移学习和领域自适应技术，也可以获得出人意料的好结果；所提出的模型提供了新的洞察力，展示了该时期奴役话语的语言使用和模式；同时，语言迁移可以有效用于历史文本中的多语言事件提取。

    NLP methods can aid historians in analyzing textual materials in greater volumes than manually feasible. Developing such methods poses substantial challenges though. First, acquiring large, annotated historical datasets is difficult, as only domain experts can reliably label them. Second, most available off-the-shelf NLP models are trained on modern language texts, rendering them significantly less effective when applied to historical corpora. This is particularly problematic for less well studied tasks, and for languages other than English. This paper addresses these challenges while focusing on the under-explored task of event extraction from a novel domain of historical texts. We introduce a new multilingual dataset in English, French, and Dutch composed of newspaper ads from the early modern colonial period reporting on enslaved people who liberated themselves from enslavement. We find that: 1) even with scarce annotated data, it is possible to achieve surprisingly good results by 
    
[^29]: 扩散模型的结构剪枝

    Structural Pruning for Diffusion Models. (arXiv:2305.10924v1 [cs.LG])

    [http://arxiv.org/abs/2305.10924](http://arxiv.org/abs/2305.10924)

    本文提出了一种名为Diff-Pruning的高效压缩方法，通过一个Taylor展开过程来识别重要权重，从而从预先存在的模型中学习轻量级扩散模型，性能稳定，并在训练效率上显著提高。

    

    生成建模最近取得了显著的进展，主要是因为扩散概率模型（DPM）的转型意义。然而，这些模型的令人印象深刻的能力通常涉及到显著的计算开销，在训练和推理期间都是如此。为了应对这一挑战，我们提出了Diff-Pruning，一种专为从预先存在的模型中学习轻量级扩散模型而设计的高效压缩方法，无需进行大量的重新训练。Diff-Pruning的本质是通过剪枝时间步长的Taylor展开，在过滤掉无贡献扩散步骤和整合有信息的梯度来识别重要权重的过程。我们在四个不同数据集上进行的实证评估突出了我们所提出方法的两个主要优点：1）效率：它可以以原始训练投入的仅10％到20％的代价实现约50％的FLOPs减少; 2）一致性: 剪枝后的扩散模型产生的效果与原始模型相当，不会影响生成建模的质量。

    Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across four diverse datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) Consistency: the pruned diffusio
    
[^30]: 查询性能预测：从Ad-hoc到交互式搜索

    Query Performance Prediction: From Ad-hoc to Conversational Search. (arXiv:2305.10923v1 [cs.IR])

    [http://arxiv.org/abs/2305.10923](http://arxiv.org/abs/2305.10923)

    本文研究了针对从Ad-hoc到交互式搜索中查询性能预测(QPP)的有效方法，并探索了QPP方法在交互式搜索中是否具有推广应用的能力。

    

    查询性能预测(QPP)是信息检索中的一个核心任务。QPP的任务是在没有相关判断的情况下预测查询的检索质量。研究表明，QPP在Ad-hoc搜索中非常有效和有用。近年来，对话式搜索(CS)取得了相当大的进展 。有效的QPP能够帮助CS系统在下一轮决定适当的行动。尽管具有潜力，但CS的QPP研究还很少。本文通过重现和研究现有的QPP方法在CS上的有效性来填补这一研究空白。虽然在两种情况下的通道检索任务相同，但CS中的用户查询取决于对话历史，引入了新的QPP挑战。我们尤其是探讨从Ad-hoc搜索中QPP方法的研究结果在三个CS设置中的推广程度:(i) 评估基于查询重写的检索方法的不同查询的检索质量

    Query performance prediction (QPP) is a core task in information retrieval. The QPP task is to predict the retrieval quality of a search system for a query without relevance judgments. Research has shown the effectiveness and usefulness of QPP for ad-hoc search. Recent years have witnessed considerable progress in conversational search (CS). Effective QPP could help a CS system to decide an appropriate action to be taken at the next turn. Despite its potential, QPP for CS has been little studied. We address this research gap by reproducing and studying the effectiveness of existing QPP methods in the context of CS. While the task of passage retrieval remains the same in the two settings, a user query in CS depends on the conversational history, introducing novel QPP challenges. In particular, we seek to explore to what extent findings from QPP methods for ad-hoc search generalize to three CS settings: (i) estimating the retrieval quality of different query rewriting-based retrieval met
    
[^31]: 利用语义先验细化的弱监督视觉-文本对齐

    Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v1 [cs.CV])

    [http://arxiv.org/abs/2305.10913](http://arxiv.org/abs/2305.10913)

    本文提出了一种利用语义先验细化的弱监督视觉-文本对齐方法，仅使用图像-句子对进行学习，其目标是实现实体表示中的区域-短语对应关系，通过联合两个主要模块的输出进行预测。

    

    弱监督视觉-文本对齐的目标是仅利用图像-句子对学习实体表示中的区域-短语对应关系。与监督方法相比，其难度更大，因为无法获得边界框和文本短语的对应关系。因此，我们提出了语义先验细化模型（SPRM），其预测结果是通过组合两个主要模块的输出得到的。第一个未经训练的模块旨在返回文本短语和边界框之间的粗略对齐。第二个训练过的模块由两个子组件组成，用于细化粗略的对齐以提高最终短语-边界框对齐的准确性。该模型的训练目标是最大化图像和句子之间的多模态相似度，同时使同一句子和一个新的不相关的图像的多模态相似度最小化，以在训练过程中最大限度地提高训练效果。我们的方法在两个流行的数据集上展现了最先进的结果。

    Using only image-sentence pairs, weakly-supervised visual-textual grounding aims to learn region-phrase correspondences of the respective entity mentions. Compared to the supervised approach, learning is more difficult since bounding boxes and textual phrases correspondences are unavailable. In light of this, we propose the Semantic Prior Refinement Model (SPRM), whose predictions are obtained by combining the output of two main modules. The first untrained module aims to return a rough alignment between textual phrases and bounding boxes. The second trained module is composed of two sub-components that refine the rough alignment to improve the accuracy of the final phrase-bounding box alignments. The model is trained to maximize the multimodal similarity between an image and a sentence, while minimizing the multimodal similarity of the same sentence and a new unrelated image, carefully selected to help the most during training. Our approach shows state-of-the-art results on two popula
    
[^32]: RobustFair: 通过公平混淆定向梯度搜索的敌对评估

    RobustFair: Adversarial Evaluation through Fairness Confusion Directed Gradient Search. (arXiv:2305.10906v1 [cs.LG])

    [http://arxiv.org/abs/2305.10906](http://arxiv.org/abs/2305.10906)

    本论文提出了一种基于公平混淆定向梯度搜索的谐波评估方法RobustFair，可以识别与虚假公平相结合的鲁棒性缺陷，提高DNN的鲁棒性和个体公平性。

    

    DNN的可信度经常受到轻微敌对扰动的挑战，这不仅会破坏预测准确性（鲁棒性）而且可能为类似的输入导致有偏预测（个体公平性）。最近提出了准确公正度来强制实施准确性和个体公平之间的谐和平衡。它引入了公平混淆矩阵的概念来将预测分类为真正公平、真正有偏、假正公平和假有偏。本文提出了一种谐波评估方法RobustFair，使用通过公平混淆定向梯度搜索制作的敌对扰动，对DNN的准确公正性进行评估。通过使用Taylor展开来近似敌对实例的基本真实性，RobustFair可以特别识别与虚假公平纠缠在一起的鲁棒性缺陷，这通常在鲁棒性评估中难以捉摸，在个体公平评估中缺失。RobustFair可以提高鲁棒性和个体公平性。

    The trustworthiness of DNNs is often challenged by their vulnerability to minor adversarial perturbations, which may not only undermine prediction accuracy (robustness) but also cause biased predictions for similar inputs (individual fairness). Accurate fairness has been recently proposed to enforce a harmonic balance between accuracy and individual fairness. It induces the notion of fairness confusion matrix to categorize predictions as true fair, true biased, false fair, and false biased. This paper proposes a harmonic evaluation approach, RobustFair, for the accurate fairness of DNNs, using adversarial perturbations crafted through fairness confusion directed gradient search. By using Taylor expansions to approximate the ground truths of adversarial instances, RobustFair can particularly identify the robustness defects entangled for spurious fairness, which are often elusive in robustness evaluation, and missing in individual fairness evaluation. RobustFair can boost robustness and 
    
[^33]: 超越数据重新加权：核矩法估计

    Estimation Beyond Data Reweighting: Kernel Method of Moments. (arXiv:2305.10898v1 [cs.LG])

    [http://arxiv.org/abs/2305.10898](http://arxiv.org/abs/2305.10898)

    本论文提出了一种新的核矩法估计器，称为KMM，其用于超越数据重新加权的矩方法模型，解除了关于使用 $\varphi$-散度相关的限制。

    

    在机器学习与统计学等多个领域中都会出现矩约束和条件对应，其中，广义矩法（GMM）作为一个估计模型已经引起了人们的关注。然而，往往由于使用 $\varphi$-散度的相关限制将候选分布限制为数据样本的重新加权。而本论文提出了一种新的矩估计方法——基于最大均值偏差的经验似然估计器，即核矩法(KMM)，其实现超越了对数据的重新加权。

    Moment restrictions and their conditional counterparts emerge in many areas of machine learning and statistics ranging from causal inference to reinforcement learning. Estimators for these tasks, generally called methods of moments, include the prominent generalized method of moments (GMM) which has recently gained attention in causal inference. GMM is a special case of the broader family of empirical likelihood estimators which are based on approximating a population distribution by means of minimizing a $\varphi$-divergence to an empirical distribution. However, the use of $\varphi$-divergences effectively limits the candidate distributions to reweightings of the data samples. We lift this long-standing limitation and provide a method of moments that goes beyond data reweighting. This is achieved by defining an empirical likelihood estimator based on maximum mean discrepancy which we term the kernel method of moments (KMM). We provide a variant of our estimator for conditional moment
    
[^34]: 分类器最小风险重新校准

    Minimum-Risk Recalibration of Classifiers. (arXiv:2305.10886v1 [cs.LG])

    [http://arxiv.org/abs/2305.10886](http://arxiv.org/abs/2305.10886)

    本文介绍了最小风险重新校准的概念，在均方误差分解框架内提供了一种原则性方法，用于评估和重新校准概率分类器，并通过平衡校准和锐度确定了最优的桶数，从而产生了大约$O(n^{-2/3})$的风险上界。

    

    重新校准概率分类器对于提高预测模型的可靠性和准确性至关重要。尽管已经开发了许多重新校准算法，但仍缺乏一个综合的理论来整合校准和锐度（这对于保持预测力至关重要）。在本文中，我们在均方误差（MSE）分解框架内介绍了最小风险重新校准的概念，提供了一种评估和重新校准概率分类器的原则性方法。利用这个框架，我们分析了均匀质量分桶（UMB）重新校准方法，并建立了一个有限样本风险上界，其顺序为$\tilde{O}(B/n+1/B^2)$，其中$B$是桶的数量，$n$是样本大小。通过平衡校准和锐度，我们进一步确定了UMB的最优桶数与$n^{1/3}$成比例，从而产生了大约$O(n^{-2/3})$的风险界。此外，我们还应对了标签稀少问题。

    Recalibrating probabilistic classifiers is vital for enhancing the reliability and accuracy of predictive models. Despite the development of numerous recalibration algorithms, there is still a lack of a comprehensive theory that integrates calibration and sharpness (which is essential for maintaining predictive power). In this paper, we introduce the concept of minimum-risk recalibration within the framework of mean-squared-error (MSE) decomposition, offering a principled approach for evaluating and recalibrating probabilistic classifiers. Using this framework, we analyze the uniform-mass binning (UMB) recalibration method and establish a finite-sample risk upper bound of order $\tilde{O}(B/n + 1/B^2)$ where $B$ is the number of bins and $n$ is the sample size. By balancing calibration and sharpness, we further determine that the optimal number of bins for UMB scales with $n^{1/3}$, resulting in a risk bound of approximately $O(n^{-2/3})$. Additionally, we tackle the challenge of label
    
[^35]: StawGAN: 面向红外图像翻译的结构感知生成对抗网络

    StawGAN: Structural-Aware Generative Adversarial Networks for Infrared Image Translation. (arXiv:2305.10882v1 [cs.CV])

    [http://arxiv.org/abs/2305.10882](http://arxiv.org/abs/2305.10882)

    本文提出了一种名为StawGAN的结构感知生成对抗网络，可以将热红外图像翻译成白天彩色图像，并在目标域中提供更好的形状和高清晰度对象的翻译，达到了比其他最先进的图像翻译模型更准确的结果。

    

    本文解决了将夜间热红外图像（NTIT2DC）翻译成白天彩色图像的问题。我们引入了一种新型的模型，专注于提高目标生成的质量，而不仅仅是对其上色。所提出的结构感知（StawGAN）在目标域中实现了更好的形状和高清晰度对象的翻译。我们在DroneVeichle 数据集的航拍图像上对模型进行了测试，结果显示与其他最先进的图像翻译模型相比，所提出的方法产生了更准确的翻译。源代码可在https://github.com/LuigiSigillo/StawGAN找到。

    This paper addresses the problem of translating night-time thermal infrared images, which are the most adopted image modalities to analyze night-time scenes, to daytime color images (NTIT2DC), which provide better perceptions of objects. We introduce a novel model that focuses on enhancing the quality of the target generation without merely colorizing it. The proposed structural aware (StawGAN) enables the translation of better-shaped and high-definition objects in the target domain. We test our model on aerial images of the DroneVeichle dataset containing RGB-IR paired images. The proposed approach produces a more accurate translation with respect to other state-of-the-art image translation models. The source code is available at https://github.com/LuigiSigillo/StawGAN
    
[^36]: 面向隐私保护的分布式图学习免费午餐

    Free Lunch for Privacy Preserving Distributed Graph Learning. (arXiv:2305.10869v1 [cs.LG])

    [http://arxiv.org/abs/2305.10869](http://arxiv.org/abs/2305.10869)

    该论文提出了一种能够保护隐私的分布式图学习框架，通过学习特征和距离，而不需要实际的特征，来执行图形学习和其他下游任务。这是一种通用的框架。

    

    在社交网络、机器人、通信、医学等各种应用中，图形学习正在变得越来越普遍。这些属于不同实体的数据集通常包含关键的私人信息。然而，数据共享的隐私问题使得应用图形学习变得困难。现有的隐私保护方法通过提取用户侧的特征来预处理数据，并仅使用这些特征进行下一步的学习。然而，这些方法容易受到对私人属性进行推断的攻击。我们提出了一个新颖的隐私保护框架，用于分布式图形学习和基于图形的机器学习。为了在服务器端执行图形学习和其他下游任务，该框架旨在学习特征和距离，而不需要实际的特征，同时保留原始数据的结构特性。所提出的框架非常通用且高度适用。

    Learning on graphs is becoming prevalent in a wide range of applications including social networks, robotics, communication, medicine, etc. These datasets belonging to entities often contain critical private information. The utilization of data for graph learning applications is hampered by the growing privacy concerns from users on data sharing. Existing privacy-preserving methods pre-process the data to extract user-side features, and only these features are used for subsequent learning. Unfortunately, these methods are vulnerable to adversarial attacks to infer private attributes. We present a novel privacy-respecting framework for distributed graph learning and graph-based machine learning. In order to perform graph learning and other downstream tasks on the server side, this framework aims to learn features as well as distances without requiring actual features while preserving the original structural properties of the raw data. The proposed framework is quite generic and highly a
    
[^37]: 多智能体强化学习中的语义对齐任务分解

    Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v1 [cs.LG])

    [http://arxiv.org/abs/2305.10865](http://arxiv.org/abs/2305.10865)

    该论文提出了一种多智能体强化学习中的新方法SAMA，通过提前训练的语言模型和任务分解来解决ASG方法存在的样本效率问题和生成非实际任务奖励的子目标的问题。

    

    合作型MARL中的奖励稀疏问题着重于适当的信用分配。自动子目标生成（ASG）是最近出现的一种可行的MARL方法，其灵感来自于在内在驱动的增强学习中利用子目标。然而，从稀疏奖励中进行复杂任务规划的端到端学习无疑需要大量的培训样本。为了解决这个问题，我们提出了一种新的"解耦"决策方法，即在MARL中的语义对齐任务分解（SAMA），受到解耦表示学习的启发。

    The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the "over-representation" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel "disentangled" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thou
    
[^38]: Quiver: 基于工作负载感知的低延迟、高吞吐量的 GNN 服务支持 GPU

    Quiver: Supporting GPUs for Low-Latency, High-Throughput GNN Serving with Workload Awareness. (arXiv:2305.10863v1 [cs.DC])

    [http://arxiv.org/abs/2305.10863](http://arxiv.org/abs/2305.10863)

    Quiver 是一种分布式基于 GPU 的 GNN 服务系统，通过利用工作负载指标来预测 GNN 请求的不规则计算，并管理 GPU 用于图采样和特征聚合的优化方法，实现了低延迟和高吞吐量，比现有系统性能提高多达 15x。

    

    面向图神经网络 (GNN) 的推理服务系统必须在低延迟和高吞吐量之间取得平衡，但由于采样的图节点和聚合的 GNN 特征存在偏差，系统面临不规则计算的挑战。这使得有效利用 GPU 变得具有挑战性：仅使用 GPU 对少量图节点进行采样的性能低于基于 CPU 的采样；而对许多特征进行聚合会产生 GPU 和 CPU 之间的高数据移动成本。因此，目前的 GNN 服务系统使用 CPU 进行图采样和特征聚合，限制了吞吐量。我们描述了 Quiver，一种分布式基于 GPU 的 GNN 服务系统，具有低延迟和高吞吐量。Quiver 的关键思路是利用工作负载指标来预测 GNN 请求的不规则计算，并管理 GPU 用于图采样和特征聚合：(1) 对于图采样，Quiver 计算概率采样的图大小，这是一种预测图节点采样并行度的指标；(2) 对于特征聚合，Quiver 采用列求和的方式消除数据移动成本。我们的评估表明，Quiver 在实现 94％ 的 GPU 利用率的同时，比现有的 GNN 服务系统性能提高了多达 15 倍。

    Systems for serving inference requests on graph neural networks (GNN) must combine low latency with high throughout, but they face irregular computation due to skew in the number of sampled graph nodes and aggregated GNN features. This makes it challenging to exploit GPUs effectively: using GPUs to sample only a few graph nodes yields lower performance than CPU-based sampling; and aggregating many features exhibits high data movement costs between GPUs and CPUs. Therefore, current GNN serving systems use CPUs for graph sampling and feature aggregation, limiting throughput.  We describe Quiver, a distributed GPU-based GNN serving system with low-latency and high-throughput. Quiver's key idea is to exploit workload metrics for predicting the irregular computation of GNN requests, and governing the use of GPUs for graph sampling and feature aggregation: (1) for graph sampling, Quiver calculates the probabilistic sampled graph size, a metric that predicts the degree of parallelism in graph
    
[^39]: Q-SHED: 基于海森矩阵特征向量量化的边缘分布式优化

    Q-SHED: Distributed Optimization at the Edge via Hessian Eigenvectors Quantization. (arXiv:2305.10852v1 [eess.SY])

    [http://arxiv.org/abs/2305.10852](http://arxiv.org/abs/2305.10852)

    Q-SHED是一种基于海森矩阵特征向量量化的边缘分布式优化算法，具有通信高效、稳健收敛速率等特点。

    

    边缘网络需要通信高效、稳健的分布式优化算法。 新牛顿类型（NT）方法作为具有足够计算能力的边缘设备中具有鲁棒收敛速率的 DO 问题的促成者，最近被提倡。 本文中，我们提出了 Q-SHED，一种原始的用于 DO 的 NT 算法，其特征是基于增量海森矩阵特征向量量化的新型比特分配方案。 这种提议的技术与最近的 SHED 算法集成在一起，它继承了 SHED 算法的吸引人特征，例如所需的流量计算数量很少，同时在位分辨率上具有带宽可变性。我们在与竞争对手的经验评估中证实了算法的良好表现。

    Edge networks call for communication efficient (low overhead) and robust distributed optimization (DO) algorithms. These are, in fact, desirable qualities for DO frameworks, such as federated edge learning techniques, in the presence of data and system heterogeneity, and in scenarios where internode communication is the main bottleneck. Although computationally demanding, Newton-type (NT) methods have been recently advocated as enablers of robust convergence rates in challenging DO problems where edge devices have sufficient computational power. Along these lines, in this work we propose Q-SHED, an original NT algorithm for DO featuring a novel bit-allocation scheme based on incremental Hessian eigenvectors quantization. The proposed technique is integrated with the recent SHED algorithm, from which it inherits appealing features like the small number of required Hessian computations, while being bandwidth-versatile at a bit-resolution level. Our empirical evaluation against competing 
    
[^40]: GETMusic：使用统一的表示和扩散框架生成任意音乐曲目

    GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework. (arXiv:2305.10841v1 [cs.SD])

    [http://arxiv.org/abs/2305.10841](http://arxiv.org/abs/2305.10841)

    GETMusic提出了一种统一的音乐生成模型，包括新颖的音乐表示GETScore和扩散模型GETDiff。GETScore使用标记表示音符，将它们有序地组织起来，而GETDiff使用遮盖对目标轨道进行破坏，能够生成任意轨道。

    

    符号音乐生成旨在创建音符，为用户创作音乐提供帮助，例如从零开始生成目标乐器轨道或基于用户提供的源轨道进行创作。考虑到源轨道和目标轨道之间的多样化和灵活性组合，需要一个能够生成任意轨道的统一模型至关重要。以往的作品由于音乐表示和模型架构的固有限制而未能解决这个需求。为了解决这个问题，我们提出了一个名为GETMusic（`GET'代表GEnerate music Tracks）的统一表示和扩散框架，其中包括一种新颖的音乐表示GETScore和一个名为GETDiff的扩散型模型。GETScore将音符表示为标记，并在二维结构中井然有序地组织它们，轨道垂直堆叠并水平地随时间推进。在训练过程中，轨道随机被选为目标或源。在前向过程中，使用遮盖对目标轨道进行破坏。

    Symbolic music generation aims to create musical notes, which can help users compose music, such as generating target instrumental tracks from scratch, or based on user-provided source tracks. Considering the diverse and flexible combination between source and target tracks, a unified model capable of generating any arbitrary tracks is of crucial necessity. Previous works fail to address this need due to inherent constraints in music representations and model architectures. To address this need, we propose a unified representation and diffusion framework named GETMusic (`GET' stands for GEnerate music Tracks), which includes a novel music representation named GETScore, and a diffusion model named GETDiff. GETScore represents notes as tokens and organizes them in a 2D structure, with tracks stacked vertically and progressing horizontally over time. During training, tracks are randomly selected as either the target or source. In the forward process, target tracks are corrupted by masking
    
[^41]: 基于潜在空间的统计推断方法在深度神经网络中的不确定性量化

    Uncertainty Quantification in Deep Neural Networks through Statistical Inference on Latent Space. (arXiv:2305.10840v1 [cs.LG])

    [http://arxiv.org/abs/2305.10840](http://arxiv.org/abs/2305.10840)

    本研究提出了一种利用深度神经网络潜在空间表征进行不确定性量化的方法，该方法可以检测数据点的精确度并帮助自动侦测异常值。

    

    本文提出了一种基于潜在空间表征的算法，通过对深度神经网络所处理数据点的精确度进行评估，来解决当前广泛使用的评估方法“过于自信”的问题。具体地，我们使用了网络能够正确分类的部分训练集所生成的潜在空间表示，并以此建立了能够捕捉预测结果可能性的统计模型。在合成数据集上的测试表明，常用方法一般存在“过于自信”的问题，甚至对训练数据生成分布之外的数据点依然如此。与之相比，我们的方法可以发现精度欠佳的数据点，因此对于异常值的自动侦测是有帮助的。

    Uncertainty-quantification methods are applied to estimate the confidence of deep-neural-networks classifiers over their predictions. However, most widely used methods are known to be overconfident. We address this problem by developing an algorithm that exploits the latent-space representation of data points fed into the network, to assess the accuracy of their prediction. Using the latent-space representation generated by the fraction of training set that the network classifies correctly, we build a statistical model that is able to capture the likelihood of a given prediction. We show on a synthetic dataset that commonly used methods are mostly overconfident. Overconfidence occurs also for predictions made on data points that are outside the distribution that generated the training data. In contrast, our method can detect such out-of-distribution data points as inaccurately predicted, thus aiding in the automatic detection of outliers.
    
[^42]: ProgSG：用于电子设计自动化程序的跨模态表征学习

    ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation. (arXiv:2305.10838v1 [cs.LG])

    [http://arxiv.org/abs/2305.10838](http://arxiv.org/abs/2305.10838)

    该论文介绍了一种跨模态表征学习方法，用于电子自动化设计中高级综合工具的优化，并促进领域特定加速器（DSAs）的设计自动化化。

    

    近年来，领域特定加速器（DSAs）（例如Google的TPUs）在加速各种应用程序（例如深度学习、搜索、自动驾驶等）方面越来越受欢迎。为了促进DSA设计，使用高级综合（HLS），它允许开发人员将C和C ++软件代码中的高级描述编译为低级硬件描述语言（例如VHDL或Verilog）上的设计，并最终合成为ASIC或FPGA上的DSA。然而，现有的HLS工具仍需要微架构决策，以pragma（例如并行化和流水线指令）的形式表示。为了使更多人设计DSA，希望能够通过深度学习自动化做出这些决策以预测HLS设计的质量。这需要我们更深入地了解程序，即原始代码的组合。

    Recent years have witnessed the growing popularity of domain-specific accelerators (DSAs), such as Google's TPUs, for accelerating various applications such as deep learning, search, autonomous driving, etc. To facilitate DSA designs, high-level synthesis (HLS) is used, which allows a developer to compile a high-level description in the form of software code in C and C++ into a design in low-level hardware description languages (such as VHDL or Verilog) and eventually synthesized into a DSA on an ASIC (application-specific integrated circuit) or FPGA (field-programmable gate arrays). However, existing HLS tools still require microarchitecture decisions, expressed in terms of pragmas (such as directives for parallelization and pipelining). To enable more people to design DSAs, it is desirable to automate such decisions with the help of deep learning for predicting the quality of HLS designs. This requires us a deeper understanding of the program, which is a combination of original code 
    
[^43]: Ahead-of-Time P-Tuning：一种应用于预训练语言模型的参数节约的微调方法

    Ahead-of-Time P-Tuning. (arXiv:2305.10835v1 [cs.LG])

    [http://arxiv.org/abs/2305.10835](http://arxiv.org/abs/2305.10835)

    本文提出了 Ahead-of-Time （AoT）P-Tuning，一种新颖的微调方法，它通过在每个Transformer层之前添加输入相关的偏置，实现了应用于预训练语言模型的参数节约。该方法在GLUE和SuperGLUE基准数据集上优于BitFit，并可用于多任务推理，而推理开销却很小。

    

    本文提出了 Ahead-of-Time （AoT）P-Tuning，一种新颖的微调方法，可以在每个Transformer层之前添加输入相关的偏置，以应用于预训练的语言模型（LMs）。我们使用RoBERTa和DeBERTa模型在GLUE和SuperGLUE基准数据集上评估AoT P-Tuning，结果表明它优于BitFit，并且与其他基准方法相比，效率更高。此外，我们评估了AoT P-Tuning的推理开销，并证明它与已建立的基准方法相比，引入的开销可以忽略不计。我们的方法可以使用单个骨干LM进行多任务推理，从而成为实际应用的解决方案。

    In this paper, we propose Ahead-of-Time (AoT) P-Tuning, a novel parameter-efficient fine-tuning method for pre-trained Language Models (LMs) that adds input-dependent bias before each Transformer layer. We evaluate AoT P-Tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa models, showing that it outperforms BitFit and is comparable or better than other baseline methods for efficient fine-tuning. Additionally, we assess the inference overhead of AoT P-Tuning and demonstrate that it introduces negligible overhead compared to established baseline methods. Our method enables multi-task inference with a single backbone LM, making it a practical solution for real-world applications.
    
[^44]: FastFit: 用多个STFT替换U-Net编码器实现实时迭代神经声码器

    FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder With Multiple STFTs. (arXiv:2305.10823v1 [eess.AS])

    [http://arxiv.org/abs/2305.10823](http://arxiv.org/abs/2305.10823)

    本文提出了FastFit，一种使用多个STFT替换U-Net编码器的声音生成算法，将模型参数数量和生成时间减少了一半，并在保持高保真度的同时实现了近两倍的生成速度。

    

    本文提出了一种新颖的神经声码器结构FastFit，通过使用多个短时傅里叶变换（STFT）替换U-Net编码器，在不损失音频质量的情况下实现更快的生成速度。我们将每个编码器块都替换为一个STFT，其参数等于每个解码器块的时间分辨率，从而形成跳跃连接。FastFit几乎将模型的参数数量和生成时间减少了一半，同时保持高保真度。通过客观和主观评估，我们证明了该模型在保持高音频质量的同时，实现了基准迭代声码器近两倍的生成速度。我们进一步展示了FastFit在文本转语音评估场景中，包括多说话人和零样本文本转语音中，产生类似于其他基线的音频质量。

    This paper presents FastFit, a novel neural vocoder architecture that replaces the U-Net encoder with multiple short-time Fourier transforms (STFTs) to achieve faster generation rates without sacrificing sample quality. We replaced each encoder block with an STFT, with parameters equal to the temporal resolution of each decoder block, leading to the skip connection. FastFit reduces the number of parameters and the generation time of the model by almost half while maintaining high fidelity. Through objective and subjective evaluations, we demonstrated that the proposed model achieves nearly twice the generation speed of baseline iteration-based vocoders while maintaining high sound quality. We further showed that FastFit produces sound qualities similar to those of other baselines in text-to-speech evaluation scenarios, including multi-speaker and zero-shot text-to-speech.
    
[^45]: 民主扩散语言模型

    Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])

    [http://arxiv.org/abs/2305.10818](http://arxiv.org/abs/2305.10818)

    本文提出了一个基于CDCD框架的民主扩散语言模型（DDLM），并通过GLUE基准测试了其知识转移能力，为研究人员提供了DDLM训练和评估流程以及已训练的DDLM模型。

    

    尽管扩散模型在自然语言处理中有潜在好处，但目前公开的实现、训练模型或可重现的训练程序并不存在。为解决这些挑战，我们提出了基于CDCD框架的民主扩散语言模型（DDLM）。我们提出了一种用C4数据集简化的DDLM训练流程，并对训练模型的行为进行了深入分析。此外，我们引入了一种用于速度更快的采样的新型早期退出策略，该策略针对使用得分插值训练的模型。由于此前没有研究旨在使用预训练扩散LM解决下游任务（例如分类任务），我们在GLUE基准上进行了实验，以研究DDLM的知识转移能力。通过本文，我们提出了可供其他研究人员使用的DDLM训练和评估流程以及预先训练的DDLM模型，这些模型可在未来的D相关的研究中使用。

    Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
    
[^46]: 采用X射线微束数据几何变换增强语音发音分析

    Enhancing Speech Articulation Analysis using a Geometric Transformation of the X-ray Microbeam Dataset. (arXiv:2305.10775v1 [eess.AS])

    [http://arxiv.org/abs/2305.10775](http://arxiv.org/abs/2305.10775)

    该论文提出了一种新的几何变换方法，将X-Ray Microbeam数据集中的解剖标记物的X-Y坐标沿中矢状面映射到多个相对测量中，进而改进了测量的准确性。

    

    准确分析语音发音对于语音分析至关重要。然而，声门的X-Y坐标严重依赖于发言者的解剖结构和颗粒位置的可变性，现有的X射线微束数据集（XRMB）中的解剖标志物映射方法无法捕捉到发音道的整个解剖学。在本文中，我们提出了一种新的几何变换，改进了这些测量的准确性。我们的变换将解剖标记物的X-Y坐标沿中矢状面映射到6个相对测量中：唇缝张度（LA）、唇部突出（LP）、舌体收缩位置（TTCL）、度数（TBCD）、舌尖收缩位置（TTCL）和度数（TTCD）。我们的创新贡献是将腭板追踪延伸到推测的咽喉前线，从而改善了舌体收缩的测量。

    Accurate analysis of speech articulation is crucial for speech analysis. However, X-Y coordinates of articulators strongly depend on the anatomy of the speakers and the variability of pellet placements, and existing methods for mapping anatomical landmarks in the X-ray Microbeam Dataset (XRMB) fail to capture the entire anatomy of the vocal tract. In this paper, we propose a new geometric transformation that improves the accuracy of these measurements. Our transformation maps anatomical landmarks' X-Y coordinates along the midsagittal plane onto six relative measures: Lip Aperture (LA), Lip Protusion (LP), Tongue Body Constriction Location (TTCL), Degree (TBCD), Tongue Tip Constriction Location (TTCL) and Degree (TTCD). Our novel contribution is the extension of the palate trace towards the inferred anterior pharyngeal line, which improves measurements of tongue body constriction.
    
[^47]: Seq-HGNN: 学习异构图上的序列节点表示

    Seq-HGNN: Learning Sequential Node Representation on Heterogeneous Graph. (arXiv:2305.10771v1 [cs.LG])

    [http://arxiv.org/abs/2305.10771](http://arxiv.org/abs/2305.10771)

    Seq-HGNN提出了一种新颖的异构图神经网络，具有序列节点表示，通过学习一种序列节点表示机制，避免了由单个向量节点表示引起的信息丢失。

    

    在信息检索应用中，异构图神经网络（HGNN）得到了迅速发展。许多现有的HGNN设计了多种量身定制的图卷积来捕获异构图中的结构和语义信息。然而，现有的HGNN通常将每个节点表示为多层图卷积计算中的单个向量，这使得高层图卷积层无法区分来自不同关系和不同顺序的信息，导致信息传递中的信息丢失。为了解决这个问题，我们提出了一种新颖的异构图神经网络，即Seq-HGNN，它具有序列节点表示。为了避免由单个向量节点表示引起的信息丢失，我们首先设计了一种序列节点表示学习机制，在节点信息传递期间将每个节点表示为一系列元路径表示。

    Recent years have witnessed the rapid development of heterogeneous graph neural networks (HGNNs) in information retrieval (IR) applications. Many existing HGNNs design a variety of tailor-made graph convolutions to capture structural and semantic information in heterogeneous graphs. However, existing HGNNs usually represent each node as a single vector in the multi-layer graph convolution calculation, which makes the high-level graph convolution layer fail to distinguish information from different relations and different orders, resulting in the information loss in the message passing. %insufficient mining of information. To this end, we propose a novel heterogeneous graph neural network with sequential node representation, namely Seq-HGNN. To avoid the information loss caused by the single vector node representation, we first design a sequential node representation learning mechanism to represent each node as a sequence of meta-path representations during the node message passing. The
    
[^48]: 追赶蒸馏：加速采样只需一次训练

    Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling. (arXiv:2305.10769v1 [cs.LG])

    [http://arxiv.org/abs/2305.10769](http://arxiv.org/abs/2305.10769)

    本文提出了一种名为“追赶蒸馏”的方法，通过调整传统采样算法，让速度估计模型的当前时刻输出与其先前时刻输出和地面真实标签对齐，从而实现只需一次训练便能加速采样的效果。

    

    扩散概率模型在各种机器学习领域取得了令人瞩目的进展。然而，为了实现高质量的合成样本，通常需要执行大量的采样步骤，这阻碍了实时样本合成的可能性。传统的通过知识蒸馏加速采样的算法依赖于预训练的模型权重和离散时间步骤场景，需要额外的培训课程才能实现他们的目标。为了解决这些问题，我们提出了追赶蒸馏（CUD），它鼓励速度估计模型的当前时刻输出“追赶”其先前时刻输出。具体而言，CUD调整了原始的常微分方程（ODE）训练目标，以使当前时刻输出与地面真实标签和先前时刻输出对齐，利用基于龙格-库塔的多步对齐蒸馏进行精确的ODE估计，同时防止异步更新。

    Diffusion Probability Models (DPMs) have made impressive advancements in various machine learning domains. However, achieving high-quality synthetic samples typically involves performing a large number of sampling steps, which impedes the possibility of real-time sample synthesis. Traditional accelerated sampling algorithms via knowledge distillation rely on pre-trained model weights and discrete time step scenarios, necessitating additional training sessions to achieve their goals. To address these issues, we propose the Catch-Up Distillation (CUD), which encourages the current moment output of the velocity estimation model ``catch up'' with its previous moment output. Specifically, CUD adjusts the original Ordinary Differential Equation (ODE) training objective to align the current moment output with both the ground truth label and the previous moment output, utilizing Runge-Kutta-based multi-step alignment distillation for precise ODE estimation while preventing asynchronous updates
    
[^49]: 基于深度强化学习的管道布局自动设计方法

    Automatic Design Method of Building Pipeline Layout Based on Deep Reinforcement Learning. (arXiv:2305.10760v1 [cs.LG])

    [http://arxiv.org/abs/2305.10760](http://arxiv.org/abs/2305.10760)

    本文提出了一种基于深度强化学习的三维管道布局自动设计方法，可以在更短的时间内完成任务，并确保高质量布局结果。

    

    管道布局设计是建筑行业中的一个关键任务。目前，管道布局是由工程师手动设计的，这是费时费力的。自动化和简化这个过程可以减轻工程师的负担并节省时间。本文提出了一种基于深度强化学习（DRL）生成三维管道布局的方法。首先，我们将空间的几何特征抽象出来建立一个训练环境，并根据三个约束（管道长度、弯头和安装距离）定义奖励函数。接下来，我们通过代理与环境之间的交互收集数据并训练DRL模型。最后，我们使用训练有素的DRL模型来自动设计单个管道。我们的结果表明，DRL模型可以在更短的时间内完成空间管道布局任务，同时确保高质量的布局结果。

    The layout design of pipelines is a critical task in the construction industry. Currently, pipeline layout is designed manually by engineers, which is time-consuming and laborious. Automating and streamlining this process can reduce the burden on engineers and save time. In this paper, we propose a method for generating three-dimensional layout of pipelines based on deep reinforcement learning (DRL). Firstly, we abstract the geometric features of space to establish a training environment and define reward functions based on three constraints: pipeline length, elbow, and installation distance. Next, we collect data through interactions between the agent and the environment and train the DRL model. Finally, we use the well-trained DRL model to automatically design a single pipeline. Our results demonstrate that DRL models can complete the pipeline layout task in space in a much shorter time than traditional algorithms while ensuring high-quality layout outcomes.
    
[^50]: 从图神经网络中提取低/高频知识注入MLP：一种有效的GNN-to-MLP蒸馏框架

    Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework. (arXiv:2305.10758v1 [cs.LG])

    [http://arxiv.org/abs/2305.10758](http://arxiv.org/abs/2305.10758)

    本文提出了一种有效的GNN-to-MLP蒸馏框架，将GNNs中的低/高频知识注入MLP。通过将GNNs学习到的知识分解为低/高频成分，在空间域中推导它们的对应关系。此外，还解决了现有GNN-to-MLP蒸馏中的信息淹没问题。

    

    最近几年，图神经网络（GNNs）在处理与图相关的任务方面取得了巨大成功。但是，由于可实现的推断效率和可扩展性，MLPs仍然是实际工业应用的主力军。为了缩小它们之间的差距，可以直接从精心设计的教师GNN中提取知识到学生MLP中，这被称为GNN-to-MLP蒸馏。但是，蒸馏的过程通常会导致信息损失，“哪些GNN的知识模式更可能会被保留并蒸馏到MLP中？”成为一个重要问题。在本文中，我们首先在频谱域中将GNNs学习到的知识分解为低/高频成分，然后推导它们在空间域中的对应关系。此外，我们还确定了现有GNN-to-MLP蒸馏存在潜在信息淹没问题，即预训练的GNNs的高频知识可能被低频知识所覆盖。

    Recent years have witnessed the great success of Graph Neural Networks (GNNs) in handling graph-related tasks. However, MLPs remain the primary workhorse for practical industrial applications due to their desirable inference efficiency and scalability. To reduce their gaps, one can directly distill knowledge from a well-designed teacher GNN to a student MLP, which is termed as GNN-to-MLP distillation. However, the process of distillation usually entails a loss of information, and ``which knowledge patterns of GNNs are more likely to be left and distilled into MLPs?" becomes an important question. In this paper, we first factorize the knowledge learned by GNNs into low- and high-frequency components in the spectral domain and then derive their correspondence in the spatial domain. Furthermore, we identified a potential information drowning problem for existing GNN-to-MLP distillation, i.e., the high-frequency knowledge of the pre-trained GNNs may be overwhelmed by the low-frequency know
    
[^51]: 受物理启发的方法理解高斯过程

    Physics Inspired Approaches Towards Understanding Gaussian Processes. (arXiv:2305.10748v1 [cs.LG])

    [http://arxiv.org/abs/2305.10748](http://arxiv.org/abs/2305.10748)

    本文利用物理学方法分析了高斯过程模型的损失景观，提出了考虑更广泛的ν使得性能更佳的优化方法，同时提供了一种用于评估GP集成效果的方法和基于损失领域的物理属性的投票方法。

    

    通过内核可以将先验有关潜在函数的信念纳入高斯过程(GP)中以形成归纳偏置，但除了内核选择外，GP模型的决策过程仍然很难理解。本文利用物理学方法对GP模型的损失景观进行了分析，演示了Matern内核的ν连续性，并概述了梯度场关键点的灾变理论方面。通过将ν直接包含在Matern内核的超参数优化中，我们发现，尽管在文献中ν的典型值增加了计算速度，但其在性能方面远非最佳。我们还提供了一种事先评估GP集合效果的方法，并讨论了基于损失景观物理属性的各种投票方法。这些方法的实用性在多种合成和真实数据集上得到了证明。我们的发现提供了对GP模型决策过程的深入理解，并为超参数优化和模型选择提供了新的洞察。

    Prior beliefs about the latent function to shape inductive biases can be incorporated into a Gaussian Process (GP) via the kernel. However, beyond kernel choices, the decision-making process of GP models remains poorly understood. In this work, we contribute an analysis of the loss landscape for GP models using methods from physics. We demonstrate $\nu$-continuity for Matern kernels and outline aspects of catastrophe theory at critical points in the loss landscape. By directly including $\nu$ in the hyperparameter optimisation for Matern kernels, we find that typical values of $\nu$ are far from optimal in terms of performance, yet prevail in the literature due to the increased computational speed. We also provide an a priori method for evaluating the effect of GP ensembles and discuss various voting approaches based on physical properties of the loss landscape. The utility of these approaches is demonstrated for various synthetic and real datasets. Our findings provide an enhanced und
    
[^52]: 面向剧集式马尔可夫决策过程的在线资源分配问题

    Online Resource Allocation in Episodic Markov Decision Processes. (arXiv:2305.10744v1 [cs.DS])

    [http://arxiv.org/abs/2305.10744](http://arxiv.org/abs/2305.10744)

    本文将长期资源分配问题形式化为剧集式MDP模型，提出在面临转换和奖励特性不确定的情况下进行在线资源分配问题的解决方案。该方案基于占有度量提供在线镜像下降算法，其期望遗憾受到界限约束 $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ .

    

    本文研究了一个长期的资源分配问题，它需要在多个时间段内进行多阶段的决策过程。我们将这个问题形式化为一个剧集式有限时间段的马尔可夫决策过程中的在线资源分配问题，其中转换和奖励以及每一次的资源消耗函数都是非定态的。我们提供了一种等效的在线线性规划重构方法，基于占有度量，为此我们开发了一种在线镜像下降算法。我们的资源分配在线镜像下降算法处理了在估算真实可行集时的不确定性和误差，这是相对独立的。我们证明，对于随机奖励和资源消耗函数，在线镜像下降算法的期望遗憾受到界限约束，其界限受到 $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ 的约束，其中 $\rho\in(0,1)$ 是预算参数，$H$ 是地平线长度，$S$ 和 $A$ 是. . .

    This paper studies a long-term resource allocation problem over multiple periods where each period requires a multi-stage decision-making process. We formulate the problem as an online resource allocation problem in an episodic finite-horizon Markov decision process with unknown non-stationary transitions and stochastic non-stationary reward and resource consumption functions for each episode. We provide an equivalent online linear programming reformulation based on occupancy measures, for which we develop an online mirror descent algorithm. Our online dual mirror descent algorithm for resource allocation deals with uncertainties and errors in estimating the true feasible set, which is of independent interest. We prove that under stochastic reward and resource consumption functions, the expected regret of the online mirror descent algorithm is bounded by $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ where $\rho\in(0,1)$ is the budget parameter, $H$ is the length of the horizon, $S$ and $A$ are the
    
[^53]: 一种动物行为计算分析的基准，使用动物携带的标签。

    A benchmark for computational analysis of animal behavior, using animal-borne tags. (arXiv:2305.10740v1 [cs.LG])

    [http://arxiv.org/abs/2305.10740](http://arxiv.org/abs/2305.10740)

    该论文介绍了一个名为BEBE的动物行为计算分析基准，其中包括了1654小时的动物生态生理学数据，这是迄今为止最大、最具分类多样性的公开可用的数据集合。在这个基准上，作者使用了十种机器学习方法并确定了未来工作中需要解决的关键问题。

    

    动物携带的传感器（“生物记录器”）可以记录一系列动力学和环境数据，揭示动物生态生理学并改善保护工作。机器学习技术对于解释生物记录器记录的大量数据非常有用，但在这个领域中没有标准来比较不同的机器学习技术。为了解决这个问题，我们提出了Bio-logger Ethogram Benchmark（BEBE），这是一个带有行为注释，标准化建模任务和评估指标的数据集合。BEBE是迄今为止最大、最具分类多样性和公开可用的这种基准，包括来自九个分类单元中149个个体收集的1654小时数据。我们在BEBE上评估了十种不同的机器学习方法的性能，并确定了未来工作中需要解决的关键问题。数据集、模型和评估代码已公开发布在https://github.com/earthspecies/BEBE，以便社区使用。

    Animal-borne sensors ('bio-loggers') can record a suite of kinematic and environmental data, which can elucidate animal ecophysiology and improve conservation efforts. Machine learning techniques are useful for interpreting the large amounts of data recorded by bio-loggers, but there exists no standard for comparing the different machine learning techniques in this domain. To address this, we present the Bio-logger Ethogram Benchmark (BEBE), a collection of datasets with behavioral annotations, standardized modeling tasks, and evaluation metrics. BEBE is to date the largest, most taxonomically diverse, publicly available benchmark of this type, and includes 1654 hours of data collected from 149 individuals across nine taxa. We evaluate the performance of ten different machine learning methods on BEBE, and identify key challenges to be addressed in future work. Datasets, models, and evaluation code are made publicly available at https://github.com/earthspecies/BEBE, to enable community 
    
[^54]: 深度时间图聚类

    Deep Temporal Graph Clustering. (arXiv:2305.10738v1 [cs.LG])

    [http://arxiv.org/abs/2305.10738](http://arxiv.org/abs/2305.10738)

    提出通用框架TGC 用于 deep temporal graph clustering, 解决了时间图只能作为静态图处理的难题，实现了对动态信息的聚类。实验证明了 TGC 的优越性。

    

    最近深度图聚类已经引起了很多关注，因为它可以增强模型在无监督场景下的表示学习能力。然而，适用于时间图的深度聚类方法 - 可以捕获关键的动态交互信息，并没有得到充分的探索。这意味着在许多面向聚类的现实场景中，时间图只能作为静态图来处理。这不仅导致了动态信息的丢失，也引发了巨大的计算消耗。为了解决这个问题，我们提出了一个名为TGC的通用框架，用于时间图深度聚类，它调整了深度聚类技术（聚类分配分布和邻接矩阵重构），以适应时间图基于交互序列的批处理模式。此外，我们还从几个方面讨论了时间图聚类与现有静态图聚类的差异。实验证明了TGC的卓越性能。

    Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which adjusts deep clustering techniques (clustering assignment distribution and adjacency matrix reconstruction) to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and existing static graph clustering from several levels. To verify the superiority of the pro
    
[^55]: FedMR：基于模型重组的联邦学习

    FedMR: Federated Learning via Model Recombination. (arXiv:2305.10730v1 [cs.LG])

    [http://arxiv.org/abs/2305.10730](http://arxiv.org/abs/2305.10730)

    FedMR是一种基于模型重组的联邦学习范式，可以通过混洗和重组每个层的本地模型来缓解子优或有偏局部模型的问题，从而在推理性能方面表现出色。

    

    联邦学习（FL）使得客户端在不泄露原始数据的情况下进行全局模型训练，但现有的基于联邦平均（FedAvg）的方法在推理性能方面存在问题，特别是在客户端数据分布不均匀时。本文提出了一种名为FedMR（联邦模型重组）的新型联邦学习范式，通过对收集到的本地模型的每个层进行混洗和重组，在客户端进行局部训练时可以获得具有多样的初始模型的新模型，从而缓解了子优或有偏局部模型的问题。

    Although Federated Learning (FL) enables global model training across clients without compromising their raw data, existing Federated Averaging (FedAvg)-based methods suffer from the problem of low inference performance, especially for unevenly distributed data among clients. This is mainly because i) FedAvg initializes client models with the same global models, which makes the local training hard to escape from the local search for optimal solutions; and ii) by averaging model parameters in a coarse manner, FedAvg eclipses the individual characteristics of local models. To address such issues that strongly limit the inference capability of FL, we propose a novel and effective FL paradigm named FedMR (Federated Model Recombination). Unlike conventional FedAvg-based methods, the cloud server of FedMR shuffles each layer of collected local models and recombines them to achieve new models for local training on clients. Due to the diversified initialization models for clients coupled with 
    
[^56]: 利用GPU友好的稀疏化和量化优化Vision Transformer

    Boost Vision Transformer with GPU-Friendly Sparsity and Quantization. (arXiv:2305.10727v1 [cs.CV])

    [http://arxiv.org/abs/2305.10727](http://arxiv.org/abs/2305.10727)

    本论文介绍了一种针对 Vision Transformer 模型的压缩方案，通过 2:4 结构化稀疏剪枝和基于稀疏蒸馏感知的量化训练实现了GPU加速.

    

    Transformer 模型已经在自然语言处理领域得到广泛应用，在计算机视觉领域也备受关注。由于其堆叠的自注意力和交叉注意力块，将 Vision Transformer 部署到 GPU 上加速运行是具有挑战性的，目前研究较少。本文设计了一种压缩方案，最大限度利用了基于 2:4 细粒度结构稀疏性和量化的 GPU 友好性。通过 2:4 结构化稀疏剪枝将一个原始的大模型剪枝成稀疏模型，利用 FP16 数据类型对稀疏模型进行优化，然后通过基于稀疏蒸馏感知的量化训练将浮点稀疏模型进一步量化为固定点模型，利用整数张量计算对其进行优化，实现额外的 2:4 稀疏计算加速。在稀疏化和量化过程中使用混合策略知识蒸馏。所提出的压缩方案灵活，支持监督式微调。

    The transformer extends its success from the language to the vision domain. Because of the stacked self-attention and cross-attention blocks, the acceleration deployment of vision transformer on GPU hardware is challenging and also rarely studied. This paper thoroughly designs a compression scheme to maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and quantization. Specially, an original large model with dense weight parameters is first pruned into a sparse one by 2:4 structured pruning, which considers the GPU's acceleration of 2:4 structured sparse pattern with FP16 data type, then the floating-point sparse model is further quantized into a fixed-point one by sparse-distillation-aware quantization aware training, which considers GPU can provide an extra speedup of 2:4 sparse calculation with integer tensors. A mixed-strategy knowledge distillation is used during the pruning and quantization process. The proposed compression scheme is flexible to support superv
    
[^57]: 重新审视长期时间序列预测：线性映射的探究

    Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping. (arXiv:2305.10721v1 [cs.LG])

    [http://arxiv.org/abs/2305.10721](http://arxiv.org/abs/2305.10721)

    本文证明了线性映射在长期时间序列预测中的重要性，提出了RevIN和CI的方法来提高预测性能，同时发现线性映射可以有效地捕捉时间序列的周期特征。

    

    近年来，长期时间序列预测受到了越来越多的关注。虽然有各种专门设计来捕捉时间依赖性的方法，但是先前的研究表明，与其他复杂的架构相比，单个线性层可以实现竞争性的预测性能。本文彻底研究了最近方法的内在有效性，并得出了三个主要结论：1）线性映射对于先前的长期时间序列预测至关重要；2）RevIN（可逆规范化）和CI（通道独立）在提高总体预测性能方面发挥重要作用；3）当增加输入视野时，线性映射能够有效捕捉时间序列的周期特征，并具有对不同通道不同周期的鲁棒性。我们提供了理论和实验解释来支持我们的发现，并讨论了局限性和未来工作。我们框架的代码可在\url{https://git}中获得。

    Long-term time series forecasting has gained significant attention in recent years. While there are various specialized designs for capturing temporal dependency, previous studies have demonstrated that a single linear layer can achieve competitive forecasting performance compared to other complex architectures. In this paper, we thoroughly investigate the intrinsic effectiveness of recent approaches and make three key observations: 1) linear mapping is critical to prior long-term time series forecasting efforts; 2) RevIN (reversible normalization) and CI (Channel Independent) play a vital role in improving overall forecasting performance; and 3) linear mapping can effectively capture periodic features in time series and has robustness for different periods across channels when increasing the input horizon. We provide theoretical and experimental explanations to support our findings and also discuss the limitations and future works. Our framework's code is available at \url{https://git
    
[^58]: 针对非稳态赌博机问题的折扣汤普森抽样算法

    Discounted Thompson Sampling for Non-Stationary Bandit Problems. (arXiv:2305.10718v1 [cs.LG])

    [http://arxiv.org/abs/2305.10718](http://arxiv.org/abs/2305.10718)

    该论文提出了一种针对非稳态多臂赌博机问题的折扣汤普森抽样算法（DS-TS），可以解决突然性变化和平滑性变化的问题，并且在两种情况下具有近乎最优的遗憾上限。

    

    近年来，非稳态多臂赌博机问题受到了显著关注。NS-MAB通常在两种情况下进行建模：突然性变化和平滑性变化。在本文中，我们提出了带有高斯先验的折扣汤普森采样算法（DS-TS）以解决这两个非稳态设置。我们的算法通过将折扣因子纳入汤普森采样来被动适应变化。DS-TS方法经过实验验证，但缺乏对遗憾上限的分析。在温和的假设下，我们证明了带有高斯先验的DS-TS可以在突然性变化的情况下实现近乎最优的遗憾上限（$\tilde{O} (\sqrt {TB_T})$），在平滑性变化的情况下实现 $\tilde{O}(T^{\beta})$ 的近乎最优遗憾上限，其中 $T$ 是时间步数，$B_T$ 是断点数，$\beta$ 与收益分布的平滑性有关，$\tilde{O}$ 是对数遗憾上限。

    Non-stationary multi-armed bandit (NS-MAB) problems have recently received significant attention. NS-MAB are typically modelled in two scenarios: abruptly changing, where reward distributions remain constant for a certain period and change at unknown time steps, and smoothly changing, where reward distributions evolve smoothly based on unknown dynamics. In this paper, we propose Discounted Thompson Sampling (DS-TS) with Gaussian priors to address both non-stationary settings. Our algorithm passively adapts to changes by incorporating a discounted factor into Thompson Sampling. DS-TS method has been experimentally validated, but analysis of the regret upper bound is currently lacking. Under mild assumptions, we show that DS-TS with Gaussian priors can achieve nearly optimal regret bound on the order of $\tilde{O}(\sqrt{TB_T})$ for abruptly changing and $\tilde{O}(T^{\beta})$ for smoothly changing, where $T$ is the number of time steps, $B_T$ is the number of breakpoints, $\beta$ is asso
    
[^59]: 时间序列预训练模型综述

    A Survey on Time-Series Pre-Trained Models. (arXiv:2305.10716v1 [cs.LG])

    [http://arxiv.org/abs/2305.10716](http://arxiv.org/abs/2305.10716)

    本综述全面回顾了时间序列预训练模型，其中监督、无监督和自监督是主要类别。通过使用这些模型，可以克服构建大规模标记数据集的困难，提高时间序列挖掘的性能和效率。

    

    时间序列挖掘是一个重要的研究领域，因为它在实际应用中显示出巨大的潜力。依赖于大量标记数据的深度学习模型已经成功地用于时间序列挖掘。然而，由于数据注释成本的原因，构建大规模、良好标记的数据集是困难的。最近，预训练模型在时间序列领域逐渐引起关注，因为它们在计算机视觉和自然语言处理方面表现出色。在本综述中，我们全面回顾了时间序列预训练模型（TS-PTMs），旨在指导了解、应用和研究TS-PTMs。具体而言，我们先简要介绍了TSM中使用的典型深度学习模型。然后，我们根据预训练技术概述了TS-PTMs。我们探讨的主要类别包括监督、无监督和自监督TS-PTMs。此外，进行了广泛的实验来分析它们的优缺点。

    Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, Pre-Trained Models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments are conducted to analyze the advantages and disadvantage
    
[^60]: 平坦度感知的Prompt选择能提高精度和样本效率

    Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency. (arXiv:2305.10713v1 [cs.CL])

    [http://arxiv.org/abs/2305.10713](http://arxiv.org/abs/2305.10713)

    本论文提出了一种新的度量--Prompt平坦度，可以优化语言提示选择，提高模型分类的准确性和样本效率，实验证明结合现有度量可以提高性能和样本效率。

    

    随着大型语言模型的能力不断增长，提示已成为访问它们的主要方式。这激发了自动选择有效语言提示策略的发展。本文介绍Prompt平坦度，一种量化语言提示预期效用的新度量。该度量受统计学习中的平坦度正则化启发，量化模型对其参数扰动的稳健性。我们提供该度量的理论基础及其与其他Prompt选择度量的关系，从而全面了解现有方法。从经验上讲，我们表明，将Prompt平坦度与现有度量结合使用可以提高性能和样本效率。在6个分类基准测试中，我们的度量优于以前的Prompt选择度量，平均精度提高5％，Pearson相关性提高10％。

    With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce prompt flatness, a new metric to quantify the expected utility of a language prompt. This metric is inspired by flatness regularization in statistical learning that quantifies the robustness of the model towards its parameter perturbations. We provide theoretical foundations for this metric and its relationship with other prompt selection metrics, providing a comprehensive understanding of existing methods. Empirically, we show that combining prompt flatness with existing metrics improves both performance and sample efficiency. Our metric outperforms the previous prompt selection metrics with an average increase of 5% in accuracy and 10% in Pearson correlation across 6 classification benchmarks.
    
[^61]: 基于符号回归和eXtended物理启发神经网络的灰盒学习动力学方程的框架

    A Framework Based on Symbolic Regression Coupled with eXtended Physics-Informed Neural Networks for Gray-Box Learning of Equations of Motion from Data. (arXiv:2305.10706v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2305.10706](http://arxiv.org/abs/2305.10706)

    该论文提出了一种基于符号回归和X-PINNs的方法，直接从数据中发现非线性方程的未知部分，并且在实验中表现出良好的精度和稳定性。

    

    我们提出了一个框架和算法，用于直接从数据中发现非线性方程的未知部分。该框架基于eXtended物理启发神经网络（X-PINNs）、时空领域分解，但我们通过在领域界面上施加通量连续性来增强原始X-PINN方法。我们使用著名的Allen-Cahn方程来演示这种方法，采用Frobenius矩阵范数来评估X-PINN预测的精度，结果表现出极佳的性能。此外，采用符号回归从数据中确定方程的未知部分的封闭形式，并且结果证实了基于X-PINNs方法的准确性。为了测试该框架在类似实际数据的情况下的稳定性，将随机噪声添加到数据集中以模拟存在热噪声或仪器误差的情况。结果表明，该框架对大量噪声具有良好的稳定性。

    We propose a framework and an algorithm to uncover the unknown parts of nonlinear equations directly from data. The framework is based on eXtended Physics-Informed Neural Networks (X-PINNs), domain decomposition in space-time, but we augment the original X-PINN method by imposing flux continuity across the domain interfaces. The well-known Allen-Cahn equation is used to demonstrate the approach. The Frobenius matrix norm is used to evaluate the accuracy of the X-PINN predictions and the results show excellent performance. In addition, symbolic regression is employed to determine the closed form of the unknown part of the equation from the data, and the results confirm the accuracy of the X-PINNs based approach. To test the framework in a situation resembling real-world data, random noise is added to the datasets to mimic scenarios such as the presence of thermal noise or instrument errors. The results show that the framework is stable against significant amount of noise. As the final p
    
[^62]: ReGen: 通过渐进式密集检索生成训练数据的零样本文本分类方法

    ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval. (arXiv:2305.10703v1 [cs.CL])

    [http://arxiv.org/abs/2305.10703](http://arxiv.org/abs/2305.10703)

    本文提出了一种基于检索增强的框架，通过渐进式密集检索从通用领域的无标签语料库中创建训练数据，实现了零样本文本分类，相较于最强的基线模型提高了4.3%的性能，与使用大型NLG模型的基线相比节省了约70％的时间。

    

    随着大型语言模型（LLM）的发展，零样本学习在各种NLP任务中受到了许多关注。与以往使用数十亿级自然语言生成模型生成训练数据的方法不同，我们提出了一种检索增强的框架，从通用领域的无标签语料库中创建训练数据。为实现这一目标，我们首先进行对比预训练，使用类别描述性话语学习了一个无监督的密集检索器以提取最相关的文档。我们进一步提出了两种简单的策略，即展示增强的话语生成和自一致性引导过滤，以提高数据集的主题覆盖率，同时删除噪声样本。对九个数据集的实验表明，REGEN相较于最强的基线模型提高了4.3%的性能，并且与使用大型NLG模型的基线相比节省了约70％的时间。此外，REGEN可以自然地与最近提出的大型语言模型相结合。

    With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further propose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self-consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines and saves around 70% of the time compared to baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language mo
    
[^63]: 生物序列生成的Dirichlet扩散分数模型

    Dirichlet Diffusion Score Model for Biological Sequence Generation. (arXiv:2305.10699v1 [cs.LG])

    [http://arxiv.org/abs/2305.10699](http://arxiv.org/abs/2305.10699)

    本文介绍了一种针对离散数据，使用概率单纯形空间中的扩散过程进行建模的生成SDE模型。称之为Dirchlet扩散分数模型。模型可以生成满足严格限制的样本，且适用于生成生物序列。

    

    设计生物序列是一个重要的挑战，需要满足复杂的限制，因此使用深度生成模型来解决是很自然的问题。扩散生成模型在许多应用中取得了相当大的成功。基于分数的生成随机微分方程（SDE）模型是一种连续时间扩散模型框架，具有许多优点，但最初提出的SDE不是自然地用于建模离散数据。为了开发适用于离散数据（例如生物序列）的生成SDE模型，我们在概率单纯形空间中引入了一种扩散过程，其中随机分布的平稳分布是Dirichlet分布。这使得在连续空间中进行扩散对于建模离散数据是自然的。我们称这种方法为Dirchlet扩散分数模型。我们通过数独生成任务证明了这种技术可以生成满足严格限制的样本。这种生成模型也可以用于生成生物序列。

    Designing biological sequences is an important challenge that requires satisfying complex constraints and thus is a natural problem to address with deep generative modeling. Diffusion generative models have achieved considerable success in many applications. Score-based generative stochastic differential equations (SDE) model is a continuous-time diffusion model framework that enjoys many benefits, but the originally proposed SDEs are not naturally designed for modeling discrete data. To develop generative SDE models for discrete data such as biological sequences, here we introduce a diffusion process defined in the probability simplex space with stationary distribution being the Dirichlet distribution. This makes diffusion in continuous space natural for modeling discrete data. We refer to this approach as Dirchlet diffusion score model. We demonstrate that this technique can generate samples that satisfy hard constraints using a Sudoku generation task. This generative model can also 
    
[^64]: 异构群体强化学习中的福音：线性加速和更多可能

    The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond. (arXiv:2305.10697v1 [cs.LG])

    [http://arxiv.org/abs/2305.10697](http://arxiv.org/abs/2305.10697)

    本文提出了异构群体强化学习中联邦Q学习的样本复杂度保证，讨论了同步和异步版本的线性加速，同时探究了等权重平均本地Q估计的缺陷。

    

    当强化学习（RL）的数据由多个代理以分布式方式收集时，联邦RL算法允许协作学习，无需共享本地数据。本文考虑联邦Q学习，其目的是通过定期聚合仅在本地数据上训练的本地Q估计来学习最优Q函数。针对无限时间蒸馏标记决策过程，我们为同步和异步版本的联邦Q学习提供了样本复杂度保证。在两种情况下，我们的界限展示了与代理数量成线性加速以及其他显著问题参数的更尖锐的依赖关系。此外，现有的联邦Q学习方法采用等权重平均本地Q估计，这在异步设置中可能会高度次优，因为由于不同的本地行为策略，本地轨迹可能高度异构。现有的样本最优化策略在异步设置中存在巨大缺陷。

    When the data used for reinforcement learning (RL) are collected by multiple agents in a distributed manner, federated versions of RL algorithms allow collaborative learning without the need of sharing local data. In this paper, we consider federated Q-learning, which aims to learn an optimal Q-function by periodically aggregating local Q-estimates trained on local data alone. Focusing on infinite-horizon tabular Markov decision processes, we provide sample complexity guarantees for both the synchronous and asynchronous variants of federated Q-learning. In both cases, our bounds exhibit a linear speedup with respect to the number of agents and sharper dependencies on other salient problem parameters. Moreover, existing approaches to federated Q-learning adopt an equally-weighted averaging of local Q-estimates, which can be highly sub-optimal in the asynchronous setting since the local trajectories can be highly heterogeneous due to different local behavior policies. Existing sample com
    
[^65]: 无偏置特征重要性的无偏梯度提升决策树

    Unbiased Gradient Boosting Decision Tree with Unbiased Feature Importance. (arXiv:2305.10696v1 [cs.LG])

    [http://arxiv.org/abs/2305.10696](http://arxiv.org/abs/2305.10696)

    本文提出了UnbiasedGBM方法以解决梯度提升决策树（GBDT）中特征选择过程中的偏见问题，并提出了无偏增益进行对特征重要性的测量。

    

    梯度提升决策树（GBDT）在各种应用中取得了显著的成功。决策树的构建过程是GBDT中最关键的组成部分之一。然而，长期以来，决策树算法一直因其对具有大量潜在分裂的特征的偏见而受到批评。这种偏见在GBDT中引入了严重的可解释性和过拟合问题。因此，我们对GBDT的偏差进行了细致的分析，并表明偏差源于1）每个分裂增益估计中的系统性偏差和2）由于使用相同数据评估分裂改进并确定最佳分裂而导致的分裂发现算法中的偏差。基于这种分析，我们提出了无偏增益，这是一种新的使用袋外样本进行测量的增益重要性的无偏方法。此外，我们将无偏属性纳入到分裂发现算法中，并开发了UnbiasedGBM来解决GBDT中的偏见问题。

    Gradient Boosting Decision Tree (GBDT) has achieved remarkable success in a wide variety of applications. The split finding algorithm, which determines the tree construction process, is one of the most crucial components of GBDT. However, the split finding algorithm has long been criticized for its bias towards features with a large number of potential splits. This bias introduces severe interpretability and overfitting issues in GBDT. To this end, we provide a fine-grained analysis of bias in GBDT and demonstrate that the bias originates from 1) the systematic bias in the gain estimation of each split and 2) the bias in the split finding algorithm resulting from the use of the same data to evaluate the split improvement and determine the best split. Based on the analysis, we propose unbiased gain, a new unbiased measurement of gain importance using out-of-bag samples. Moreover, we incorporate the unbiased property into the split finding algorithm and develop UnbiasedGBM to solve the o
    
[^66]: 门控深度模型是有效的因子学习器

    Gated Deeper Models are Effective Factor Learners. (arXiv:2305.10693v1 [q-fin.PR])

    [http://arxiv.org/abs/2305.10693](http://arxiv.org/abs/2305.10693)

    本文提出一种门控深度学习模型，将人工设计的因子与深度神经网络相结合，能够在2048维度空间中生成更有意义的因子，并取得了在对2,000只中国市场股票实验中良好的效果。

    

    准确预测资产（例如特斯拉股票）的超额回报对于所有投资者都有益。然而，受到人类行为影响的市场动态的不可预测性使这成为一项具有挑战性的任务。在先前的研究中，研究者手工制定了一些因子作为信号来指导他们的投资过程。相反，本文从不同的角度看待这个问题，将深度学习模型与这些人工设计的因子相结合，预测超额回报的趋势。为此，我们提出了一个五层的深度神经网络，能够在2048维度空间中生成更有意义的因子。现代网络设计技术被用来增强鲁棒性训练并减少过拟合。此外，我们提出了一个门控网络，动态过滤噪声学习的特征，从而提高性能。我们对中国市场的近三年记录中的2,000只股票进行了模型评估。实验结果表明：

    Precisely forecasting the excess returns of an asset (e.g., Tesla stock) is beneficial to all investors. However, the unpredictability of market dynamics, influenced by human behaviors, makes this a challenging task. In prior research, researcher have manually crafted among of factors as signals to guide their investing process. In contrast, this paper view this problem in a different perspective that we align deep learning model to combine those human designed factors to predict the trend of excess returns. To this end, we present a 5-layer deep neural network that generates more meaningful factors in a 2048-dimensional space. Modern network design techniques are utilized to enhance robustness training and reduce overfitting. Additionally, we propose a gated network that dynamically filters out noise-learned features, resulting in improved performance. We evaluate our model over 2,000 stocks from the China market with their recent three years records. The experimental results show tha
    
[^67]: 采样，扩散和随机定位

    Sampling, Diffusions, and Stochastic Localization. (arXiv:2305.10690v1 [cs.LG])

    [http://arxiv.org/abs/2305.10690](http://arxiv.org/abs/2305.10690)

    这篇论文介绍了扩散和随机定位的关系，证明了标准去噪扩散是一种随机定位，并提出了一种在对数步骤内从 Ising 模型的 Gibbs 测度中进行采样的算法。

    

    扩散是从高维分布中抽样的成功技术，可以明确给出或从样本集中学习。它们实现了一个扩散过程，其端点是目标分布的样本，漂移通常表示为神经网络。随机定位是在高维中证明马尔科夫链和其他函数不等式混合的成功技术。[EAMS2022]中引入了随机定位的算法版本，以获得从某些统计力学模型中抽样的算法。本文有三个目标：（i）将[EAMS2022]的构造推广到其他随机定位过程；（ii）澄清扩散和随机定位之间的联系。特别是，我们展示了标准去噪扩散是随机定位，但其他通过所提出的视角自然提出的示例；（iii）描述从这种联系中得出的一些见解；特别是，我们提出了一种新的算法，可以在对数步骤内从 Ising 模型的 Gibbs 测度中进行采样。

    Diffusions are a successful technique to sample from high-dimensional distributions can be either explicitly given or learnt from a collection of samples. They implement a diffusion process whose endpoint is a sample from the target distribution and whose drift is typically represented as a neural network. Stochastic localization is a successful technique to prove mixing of Markov Chains and other functional inequalities in high dimension. An algorithmic version of stochastic localization was introduced in [EAMS2022], to obtain an algorithm that samples from certain statistical mechanics models.  This notes have three objectives: (i) Generalize the construction [EAMS2022] to other stochastic localization processes; (ii) Clarify the connection between diffusions and stochastic localization. In particular we show that standard denoising diffusions are stochastic localizations but other examples that are naturally suggested by the proposed viewpoint; (iii) Describe some insights that foll
    
[^68]: 在线深度强化学习的黑盒目标奖励毒化攻击

    Black-Box Targeted Reward Poisoning Attack Against Online Deep Reinforcement Learning. (arXiv:2305.10681v1 [cs.LG])

    [http://arxiv.org/abs/2305.10681](http://arxiv.org/abs/2305.10681)

    本文提出了一种针对在线深度强化学习的黑盒目标攻击方式，即通过奖励毒化攻击来干扰训练过程，攻击具有通用性，且攻击预算和计算资源需求较少，实验结果表明攻击高效，适用于多种DRL环境和学习器。

    

    我们提出了第一个在线深度强化学习的黑盒目标攻击，通过奖励毒化攻击来干扰训练过程。我们的攻击适用于未知算法学习的通用环境，并且需要有限的攻击预算和计算资源。我们利用通用框架并找到保证在学习算法的一般假设下攻击高效的条件。我们证明在这个框架下我们的攻击是最优的。实验结果表明，我们的攻击可以在有限的预算下高效地将学习代理引导到各种目标策略下，适用于多种流行的DRL环境和最先进的学习器。

    We propose the first black-box targeted attack against online deep reinforcement learning through reward poisoning during training time. Our attack is applicable to general environments with unknown dynamics learned by unknown algorithms and requires limited attack budgets and computational resources. We leverage a general framework and find conditions to ensure efficient attack under a general assumption of the learning algorithms. We show that our attack is optimal in our framework under the conditions. We experimentally verify that with limited budgets, our attack efficiently leads the learning agent to various target policies under a diverse set of popular DRL environments and state-of-the-art learners.
    
[^69]: 少即是多：面向大规模动态图的无监督图剪枝

    Less Can Be More: Unsupervised Graph Pruning for Large-scale Dynamic Graphs. (arXiv:2305.10673v1 [cs.LG])

    [http://arxiv.org/abs/2305.10673](http://arxiv.org/abs/2305.10673)

    本研究提出了一个无监督图剪枝框架，名为STEP，用于解决大规模动态图的训练和部署问题，该框架不需要标签数据，并且具有最先进的剪枝效率和有效性。

    

    大规模图的普及在训练和部署图神经网络（GNN）方面带来了极大的时间和存储挑战。最近的一些研究探索了将大原始图剪枝成一个小而高度信息化的图的解决方案，使得对修剪后的图和大图的训练和推断具有可比性的性能。虽然经验有效，但当前的研究重点是静态或非时间图，这些图对动态场景的直接应用受到限制。此外，它们需要标签作为基本事实来学习信息结构，这限制了它们对标签难以获得的新问题域的适用性。为了解决这个难题，我们提出并研究了针对动态图的无监督图剪枝问题。我们通过我们提出的STEP方法来解决这个问题，这是一个自我监督的时间剪枝框架，它学习从输入的动态图中去除潜在冗余的边缘。从技术和工业的角度来看，我们的方法克服了现有动态图剪枝技术的限制，不需要任何标记数据，并以剪枝效率和有效性方面实现了最先进的性能。

    The prevalence of large-scale graphs poses great challenges in time and storage for training and deploying graph neural networks (GNNs). Several recent works have explored solutions for pruning the large original graph into a small and highly-informative one, such that training and inference on the pruned and large graphs have comparable performance. Although empirically effective, current researches focus on static or non-temporal graphs, which are not directly applicable to dynamic scenarios. In addition, they require labels as ground truth to learn the informative structure, limiting their applicability to new problem domains where labels are hard to obtain. To solve the dilemma, we propose and study the problem of unsupervised graph pruning on dynamic graphs. We approach the problem by our proposed STEP, a self-supervised temporal pruning framework that learns to remove potentially redundant edges from input dynamic graphs. From a technical and industrial viewpoint, our method over
    
[^70]: MetaGAD：学习元转移进行少样本图异常检测

    MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection. (arXiv:2305.10668v1 [cs.LG])

    [http://arxiv.org/abs/2305.10668](http://arxiv.org/abs/2305.10668)

    本文提出了一种名为MetaGAD的框架，用于学习从无标记节点到有标记节点之间的元转移知识，以进行少样本图异常检测。

    

    图异常检测长期以来一直是各个领域信息安全问题中的重要问题，如金融欺诈、社会垃圾邮件、网络入侵等。目前大多数现有方法都是以无监督方式执行的，因为标记的异常在大规模情况下往往太昂贵。然而，由于缺乏有关异常的先前知识，可能会将被识别的异常视为数据噪声或不感兴趣的数据实例。在现实场景中，通常可获取有限的标记异常，这些标记异常具有推进图异常检测的巨大潜力。然而，探索少量标记异常和大量无标记节点来检测异常的工作相当有限。因此，本文研究了少样本图异常检测的新问题。我们提出了一种新的框架MetaGAD，学习元转移知识来进行图异常检测。实

    Graph anomaly detection has long been an important problem in various domains pertaining to information security such as financial fraud, social spam, network intrusion, etc. The majority of existing methods are performed in an unsupervised manner, as labeled anomalies in a large scale are often too expensive to acquire. However, the identified anomalies may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies. In realistic scenarios, it is often feasible to obtain limited labeled anomalies, which have great potential to advance graph anomaly detection. However, the work exploring limited labeled anomalies and a large amount of unlabeled nodes in graphs to detect anomalies is rather limited. Therefore, in this paper, we study a novel problem of few-shot graph anomaly detection. We propose a new framework MetaGAD to learn to meta-transfer the knowledge between unlabeled and labeled nodes for graph anomaly detection. Experimental 
    
[^71]: 权重具有无界方差的无限宽贝叶斯神经网络后验推断

    Posterior Inference on Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance. (arXiv:2305.10664v1 [stat.ML])

    [http://arxiv.org/abs/2305.10664](http://arxiv.org/abs/2305.10664)

    本文提出了一种新的方法进行关于具有无界方差权重的贝叶斯神经网络的后验推断，并表明后验分布集中在具有非标准超参数依赖性的稀疏促进和均值收缩先验周围。

    

    由Neal（1996）的经典而有影响力的作品已知，具有一层隐藏层的贝叶斯神经网络的无限宽度标度极限是一个高斯过程，当网络权重具有有界先验方差时。Neal的结果已扩展到具有多个隐藏层和卷积神经网络的网络，也具有高斯过程标度极限。高斯过程的易处理属性允许直接的后验推断和不确定性量化，相比有限宽度的网络，极大地简化了极限过程的研究。然而，具有无界方差的神经网络权重面临着独特的挑战。在这种情况下，经典的中心极限定理失效，据适当条件下的稳定$\alpha$过程的标度极限的文献较多的是前向模拟，而在这些权重下的后验推断问题仍然是一个未解决的问题。在本文中，我们提出了关于具有无界方差权重的贝叶斯神经网络后验推断的新理论洞察力。具体而言，我们建立了一种新的后验收缩速率结果，并表明后验分布集中在具有非标准超参数依赖性的稀疏促进和均值收缩先验周围。

    From the classical and influential works of Neal (1996), it is known that the infinite width scaling limit of a Bayesian neural network with one hidden layer is a Gaussian process, \emph{when the network weights have bounded prior variance}. Neal's result has been extended to networks with multiple hidden layers and to convolutional neural networks, also with Gaussian process scaling limits. The tractable properties of Gaussian processes then allow straightforward posterior inference and uncertainty quantification, considerably simplifying the study of the limit process compared to a network of finite width. Neural network weights with unbounded variance, however, pose unique challenges. In this case, the classical central limit theorem breaks down and it is well known that the scaling limit is an $\alpha$-stable process under suitable conditions. However, current literature is primarily limited to forward simulations under these processes and the problem of posterior inference under s
    
[^72]: 使用语音障碍程度进行口吃症语音识别

    Use of Speech Impairment Severity for Dysarthric Speech Recognition. (arXiv:2305.10659v1 [eess.AS])

    [http://arxiv.org/abs/2305.10659](http://arxiv.org/abs/2305.10659)

    本文提出了一种使用发音障碍严重程度和说话人身份的口吃症语音识别技术，实现了显著的字错率降低。

    

    口吃症语音识别中的关键挑战在于发音障碍严重程度因素与说话人身份等因素所导致的说话人层面的多样性。之前的研究主要集中于使用说话人身份来解决这个问题，而本文提出了一系列新的技术，即：a）多任务训练，包括发音障碍严重预测误差；b）以说话人-障碍程度为重点的辅助特征调整；c）仅针对说话人身份和障碍程度进行的结构化LHUC变换。在UASpeech上进行的实验表明，将额外的语音障碍程度纳入最先进的混合DNN、E2E Conformer和预训练的Wav2vec 2.0 ASR系统中，可以实现显著的字错率（WER）降低，最高可达4.78%（相对于14.03%的WER降低）。使用最佳系统，在UASpeech上可以获得已发布的最低WER为17.82%（对于非常低的可理解性，为51.25%）。

    A key challenge in dysarthric speech recognition is the speaker-level diversity attributed to both speaker-identity associated factors such as gender, and speech impairment severity. Most prior researches on addressing this issue focused on using speaker-identity only. To this end, this paper proposes a novel set of techniques to use both severity and speaker-identity in dysarthric speech recognition: a) multitask training incorporating severity prediction error; b) speaker-severity aware auxiliary feature adaptation; and c) structured LHUC transforms separately conditioned on speaker-identity and severity. Experiments conducted on UASpeech suggest incorporating additional speech impairment severity into state-of-the-art hybrid DNN, E2E Conformer and pre-trained Wav2vec 2.0 ASR systems produced statistically significant WER reductions up to 4.78% (14.03% relative). Using the best system the lowest published WER of 17.82% (51.25% on very low intelligibility) was obtained on UASpeech.
    
[^73]: DeepEdit：基于深度可编辑学习的医学图像交互式分割

    DeepEdit: Deep Editable Learning for Interactive Segmentation of 3D Medical Images. (arXiv:2305.10655v1 [eess.IV])

    [http://arxiv.org/abs/2305.10655](http://arxiv.org/abs/2305.10655)

    DeepEdit是一种基于深度学习的医学图像注释方法，它结合了自动和交互式分割，允许非常快速的数据集分割，且通过用户交互（如点击）可以进一步优化分割结果。

    

    医学图像自动分割是诊断和介入任务的关键步骤。然而，实现这一任务需要大量的标注图像，这对专家标注者来说可能是烦琐且耗时的工作。本文介绍了DeepEdit，一种基于深度学习的体积医学图像注释方法，它允许自动和半自动分割，并通过点击-based修正进行互动分割。DeepEdit将两种方法的优势相结合：非交互式（即使用nnU-Net、UNET或UNETR进行自动分割）和交互式分割方法（即DeepGrow），合并为一个单一的深度学习模型。它允许轻松集成基于不确定性的排名策略（即aleatoric和epistemic不确定性计算）和主动学习。我们提出并实现了一种方法，通过使用标准培训和用户交互仿真来训练DeepEdit。一旦训练完成，医生可以使用算法将体积医学图像进行半自动分割，并通过用户交互（如点击）来优化分割结果。实验结果表明，DeepEdit优于最先进的方法，并显着减少了专家标注者的注释时间和工作量。

    Automatic segmentation of medical images is a key step for diagnostic and interventional tasks. However, achieving this requires large amounts of annotated volumes, which can be tedious and time-consuming task for expert annotators. In this paper, we introduce DeepEdit, a deep learning-based method for volumetric medical image annotation, that allows automatic and semi-automatic segmentation, and click-based refinement. DeepEdit combines the power of two methods: a non-interactive (i.e. automatic segmentation using nnU-Net, UNET or UNETR) and an interactive segmentation method (i.e. DeepGrow), into a single deep learning model. It allows easy integration of uncertainty-based ranking strategies (i.e. aleatoric and epistemic uncertainty computation) and active learning. We propose and implement a method for training DeepEdit by using standard training combined with user interaction simulation. Once trained, DeepEdit allows clinicians to quickly segment their datasets by using the algorit
    
[^74]: STREAMLINE: 面向真实多分布场景的流式主动学习

    STREAMLINE: Streaming Active Learning for Realistic Multi-Distributional Settings. (arXiv:2305.10643v1 [cs.LG])

    [http://arxiv.org/abs/2305.10643](http://arxiv.org/abs/2305.10643)

    STREAMLINE是一种新的流式主动学习框架，通过切片识别、切片感知预算和数据选择的三步流程，缓解了工作标记数据中场景驱动的切片不平衡问题。

    

    深度神经网络在自动驾驶车辆、卫星成像等多个实际用例中表现出色，可以有效利用大量已标记训练数据。然而，学习无偏差模型取决于建立代表给定任务各种现实场景的数据集。在许多环境中，数据来自高流量流，每种情况出现在随机交错的情况下，频率各异。我们研究了实际的流式场景，其中数据实例以及从分布式数据流中取样。利用子模信息量度，我们提出了一种新的流式主动学习框架STREAMLINE，通过切片识别、切片感知预算和数据选择的三步流程，缓解了工作标记数据中场景驱动的切片不平衡问题。我们对STREAMLINE进行了广泛的现实世界流式场景评估。

    Deep neural networks have consistently shown great performance in several real-world use cases like autonomous vehicles, satellite imaging, etc., effectively leveraging large corpora of labeled training data. However, learning unbiased models depends on building a dataset that is representative of a diverse range of realistic scenarios for a given task. This is challenging in many settings where data comes from high-volume streams, with each scenario occurring in random interleaved episodes at varying frequencies. We study realistic streaming settings where data instances arrive in and are sampled from an episodic multi-distributional data stream. Using submodular information measures, we propose STREAMLINE, a novel streaming active learning framework that mitigates scenario-driven slice imbalance in the working labeled data via a three-step procedure of slice identification, slice-aware budgeting, and data selection. We extensively evaluate STREAMLINE on real-world streaming scenarios
    
[^75]: 增量因果图学习进行在线无监督根本原因分析

    Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis. (arXiv:2305.10638v1 [cs.LG])

    [http://arxiv.org/abs/2305.10638](http://arxiv.org/abs/2305.10638)

    本文提出了CORAL，一种用于在线无监督根本原因分析的新框架，可以自动触发该过程并增量更新模型，包括三个主要部分：触发点检测，增量因果图学习和基于网络传播的根本原因定位。

    

    根本原因分析（RCA）的任务是分析系统监控数据，以识别系统故障/失效的根本原因。有效的RCA可以大大加速系统故障恢复，并减轻系统损失或财务损失。然而，以前的研究大多集中在开发离线RCA算法上，这通常需要手动启动RCA过程，需要大量时间和数据来训练稳健的模型，然后需要从头开始重新训练新的系统故障。在本文中，我们提出了CORAL，一种新颖的在线RCA框架，可以自动触发RCA过程并增量更新RCA模型。CORAL包括触发点检测、增量解缠因果图学习和基于网络传播的根本原因定位。触发点检测组件旨在自动检测系统状态转换并进行准实时检测。为此，我们开发了一种基于m的在线触发点检测方法。

    The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.  In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on m
    
[^76]: 增强的消息传递斯坦变分梯度下降法

    Augmented Message Passing Stein Variational Gradient Descent. (arXiv:2305.10636v1 [cs.LG])

    [http://arxiv.org/abs/2305.10636](http://arxiv.org/abs/2305.10636)

    本文提出了一种增强的消息传递斯坦变分梯度下降法(AUMP-SVGD)来应对 Stein Variational Gradient Descent (SVGD)方法的方差崩溃问题，我们的算法能够提高SVGD在高维问题中的有效性。

    

    Stein Variational Gradient Descent (SVGD)是一种用于贝叶斯推理的基于粒子的流行方法。然而，它的收敛性遭受方差崩溃的影响，这会降低估计的准确性和多样性。本文研究了收敛过程中有限粒子的等向性属性，表明有限粒子的SVGD无法在整个样本空间中传播。相反，所有粒子倾向于在一定范围内聚集在粒子中心周围，并且我们提供了此聚类的分析界限。为进一步改善SVGD在高维问题中的有效性，我们提出了增强消息传递斯坦变分梯度下降法(AUMP-SVGD)方法，这是一种两阶段优化过程，不需要目标分布的稀疏性，不像MP-SVGD方法。我们的算法在各种基准问题中实现了令人满意的准确性，并克服了方差崩溃问题。

    Stein Variational Gradient Descent (SVGD) is a popular particle-based method for Bayesian inference. However, its convergence suffers from the variance collapse, which reduces the accuracy and diversity of the estimation. In this paper, we study the isotropy property of finite particles during the convergence process and show that SVGD of finite particles cannot spread across the entire sample space. Instead, all particles tend to cluster around the particle center within a certain range and we provide an analytical bound for this cluster. To further improve the effectiveness of SVGD for high-dimensional problems, we propose the Augmented Message Passing SVGD (AUMP-SVGD) method, which is a two-stage optimization procedure that does not require sparsity of the target distribution, unlike the MP-SVGD method. Our algorithm achieves satisfactory accuracy and overcomes the variance collapse problem in various benchmark problems.
    
[^77]: 在噪声下修改的高斯牛顿算法

    Modified Gauss-Newton Algorithms under Noise. (arXiv:2305.10634v1 [math.OC])

    [http://arxiv.org/abs/2305.10634](http://arxiv.org/abs/2305.10634)

    本文探讨了在大规模统计设置中高斯牛顿方法及其随机版本的性能和非光滑版本的修改高斯牛顿或近端线性算法的对比表现，并勾勒出在统计噪声下修改高斯牛顿法的二次收敛区域。

    

    高斯牛顿方法及其随机版本已广泛用于机器学习和信号处理中。它们的非光滑版本，修改的高斯牛顿或近端线性算法，在大规模统计设置中相对于梯度下降可以导致截然不同的结果。我们探索了这两类算法在理论上对一个理想化的统计示例和实验中学习问题（包括结构化预测）的截然不同的表现。在理论上，我们勾勒出在统计噪声下活跃的修改高斯牛顿法的二次收敛区域。在实验中，我们强调了随机（次）梯度下降优化非光滑复合目标的多功能性。

    Gauss-Newton methods and their stochastic version have been widely used in machine learning and signal processing. Their nonsmooth counterparts, modified Gauss-Newton or prox-linear algorithms, can lead to contrasting outcomes when compared to gradient descent in large-scale statistical settings. We explore the contrasting performance of these two classes of algorithms in theory on a stylized statistical example, and experimentally on learning problems including structured prediction. In theory, we delineate the regime where the quadratic convergence of the modified Gauss-Newton method is active under statistical noise. In the experiments, we underline the versatility of stochastic (sub)-gradient descent to minimize nonsmooth composite objectives.
    
[^78]: 平滑化风景可提升SGD信号：学习单指数模型的最优样本复杂度研究

    Smoothing the Landscape Boosts the Signal for SGD: Optimal Sample Complexity for Learning Single Index Models. (arXiv:2305.10633v1 [cs.LG])

    [http://arxiv.org/abs/2305.10633](http://arxiv.org/abs/2305.10633)

    本文提出了使用平滑化的损失来优化在线SGD的信号，可以使用$n \gtrsim d^{k^\star/2}$个样本学习单指数模型$w^\star$，并与张量PCA和小批量SGD的正则化效应有关。

    

    本文研究了在$d$维度上使用各向同性高斯分布来学习单指数模型$\sigma(w^\star \cdot x)$的任务。先前的研究表明，学习$w^\star$的样本复杂度是由链接函数$\sigma$的信息指数$k^\star$所决定的，它被定义为$\sigma$的第一个非零Hermite系数的指数。本文通过展示基于平滑损失的在线SGD使用$n \gtrsim d^{k^\star/2}$个样本可以学习$w^\star$，弥补了上下界之间的差距。作者还将其与张量PCA的统计分析和小批量SGD在经验损失上的隐式正则化效应联系起来。

    We focus on the task of learning a single index model $\sigma(w^\star \cdot x)$ with respect to the isotropic Gaussian distribution in $d$ dimensions. Prior work has shown that the sample complexity of learning $w^\star$ is governed by the information exponent $k^\star$ of the link function $\sigma$, which is defined as the index of the first nonzero Hermite coefficient of $\sigma$. Ben Arous et al. (2021) showed that $n \gtrsim d^{k^\star-1}$ samples suffice for learning $w^\star$ and that this is tight for online SGD. However, the CSQ lower bound for gradient based methods only shows that $n \gtrsim d^{k^\star/2}$ samples are necessary. In this work, we close the gap between the upper and lower bounds by showing that online SGD on a smoothed loss learns $w^\star$ with $n \gtrsim d^{k^\star/2}$ samples. We also draw connections to statistical analyses of tensor PCA and to the implicit regularization effects of minibatch SGD on empirical losses.
    
[^79]: 基于多尺度特征金字塔网络和双重注意力机制的腹部MRI图像分割算法

    A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism. (arXiv:2305.10631v1 [eess.IV])

    [http://arxiv.org/abs/2305.10631](http://arxiv.org/abs/2305.10631)

    提出了一种基于多尺度特征金字塔网络和双重注意力机制的子腹部MRI图像分割算法，使用空洞卷积和多尺度特征金字塔编码以避免语义差距，设计双重注意力机制以保持空间信息并减少错位。

    

    本研究旨在解决U-Net在分割直肠癌治疗期间的子腹部MRI图像时，由于多次卷积和池化操作导致编码和解码之间存在语义差距和错位问题。提出了一种基于多尺度特征金字塔网络和双重注意力机制的MRI图像分割方法。我们的创新在于设计了两个模块：1）在编码中使用了空洞卷积和多尺度特征金字塔网络以避免语义差距。2）设计了双重注意力机制，以保持U-Net的空间信息并减少错位。对子腹部MRI图像数据集的实验表明，该方法比其他方法表现更好。总之，多尺度特征金字塔网络可以减少语义差距，双重注意力机制可以使编码和解码之间的特征对齐。

    This study aimed to solve the semantic gap and misalignment issue between encoding and decoding because of multiple convolutional and pooling operations in U-Net when segmenting subabdominal MRI images during rectal cancer treatment. A MRI Image Segmentation is proposed based on a multi-scale feature pyramid network and dual attention mechanism. Our innovation is the design of two modules: 1) a dilated convolution and multi-scale feature pyramid network are used in the encoding to avoid the semantic gap. 2) a dual attention mechanism is designed to maintain spatial information of U-Net and reduce misalignment. Experiments on a subabdominal MRI image dataset show the proposed method achieves better performance than others methods. In conclusion, a multi-scale feature pyramid network can reduce the semantic gap, and the dual attention mechanism can make an alignment of features between encoding and decoding.
    
[^80]: 深度神经网络中的局部不稳定性测量和减少方法

    Measuring and Mitigating Local Instability in Deep Neural Networks. (arXiv:2305.10625v1 [cs.LG])

    [http://arxiv.org/abs/2305.10625](http://arxiv.org/abs/2305.10625)

    深度神经网络中，训练过程中的随机性可能导致模型的输出不稳定，作者提出了基于原则的指标来量化不稳定性并发现不稳定的预测并不是随机出现的，而是以数据相关的方式聚集在一起。作者研究了数据无关正则化方法来减轻这种不稳定性，并表明一些方法可以显着提高不稳定性，甚至在某些情况下优于更广泛使用的正则化方法。

    

    深度神经网络已经成为数百万用户依赖的实际场景应用的重要组成部分。然而，这些系统的构建者往往很难确保可靠的性能，因为像随机初始化这样的无关细节可能意外地改变训练系统的输出，可能带来灾难性的后果。我们通过研究模型稳定性问题，研究模型在训练过程中的随机性对模型预测结果的影响，即使在同一数据上重新训练，预测仍然会发生变化。对于自然语言理解（NLU）任务，我们发现其中相当一部分查询的预测存在不稳定性。我们提出了一些基于原则的指标，如跨训练运行或单次训练内的每个样本的“标签熵”，来量化这种现象。有趣的是，我们发现不稳定的预测并不是随机出现的，而是以数据相关的方式聚集在一起。我们研究了数据无关正则化方法来减轻这种不稳定性，并表明一些方法可以显着提高不稳定性，甚至在某些情况下优于更广泛使用的正则化方法。

    Deep Neural Networks (DNNs) are becoming integral components of real world services relied upon by millions of users. Unfortunately, architects of these systems can find it difficult to ensure reliable performance as irrelevant details like random initialization can unexpectedly change the outputs of a trained system with potentially disastrous consequences. We formulate the model stability problem by studying how the predictions of a model change, even when it is retrained on the same data, as a consequence of stochasticity in the training process. For Natural Language Understanding (NLU) tasks, we find instability in predictions for a significant fraction of queries. We formulate principled metrics, like per-sample ``label entropy'' across training runs or within a single training run, to quantify this phenomenon. Intriguingly, we find that unstable predictions do not appear at random, but rather appear to be clustered in data-specific ways. We study data-agnostic regularization meth
    
[^81]: 球形负感知器的解空间的星形特征

    The star-shaped space of solutions of the spherical negative perceptron. (arXiv:2305.10623v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2305.10623](http://arxiv.org/abs/2305.10623)

    本文针对球形负感知器模型的解空间展开研究，发现在过度参数化的区域内解决方案流形显示出简单的连通性质。存在一个大的测地凸成分，对各种优化动力学具有吸引力，其中又有一个与大多数其他解决方案测地连接的非典型鲁棒解决方案的子集，从而产生了星形的几何形状。

    

    对神经网络景观的经验研究表明，低能配置通常出现在复杂的连通结构中，在那里可以构建远距离解之间的零能路径。本文研究球形负感知器，一个作为连续约束满足问题的典型非凸神经网络模型。我们引入了一种计算从平衡点采样的顶点配置的单纯形中能量障碍的通用分析方法。在过度参数化的区域内，解决方案流形显示出简单的连通性质。存在一个大的测地凸成分，对各种优化动力学具有吸引力。在这个区域内，我们确定了一个与大多数其他解决方案测地连接的非典型鲁棒解决方案的子集，从而产生了星形的几何形状。我们分析性地表征了解空间的连接结构组织。

    Empirical studies on the landscape of neural networks have shown that low-energy configurations are often found in complex connected structures, where zero-energy paths between pairs of distant solutions can be constructed. Here we consider the spherical negative perceptron, a prototypical non-convex neural network model framed as a continuous constraint satisfaction problem. We introduce a general analytical method for computing energy barriers in the simplex with vertex configurations sampled from the equilibrium. We find that in the over-parameterized regime the solution manifold displays simple connectivity properties. There exists a large geodesically convex component that is attractive for a wide range of optimization dynamics. Inside this region we identify a subset of atypically robust solutions that are geodesically connected with most other solutions, giving rise to a star-shaped geometry. We analytically characterize the organization of the connected space of solutions and s
    
[^82]: CNN 压缩的评估度量

    Evaluation Metrics for CNNs Compression. (arXiv:2305.10616v1 [cs.LG])

    [http://arxiv.org/abs/2305.10616](http://arxiv.org/abs/2305.10616)

    本文提供了神经网络压缩的评估度量综述，从而为标准化神经网络压缩贡献力量。

    

    研究人员致力于开发不同的神经网络压缩技术，但社区似乎缺乏标准化的评估和比较不同压缩技术的方法，这是识别不同应用程序的最合适的压缩技术的关键。本文通过提出评估度量的综述来为神经网络压缩的标准化贡献。这些度量已被实现到NetZIP，一个标准化的神经网络压缩基准之中。我们通过三个案例研究展示一些被审查的度量，分别聚焦于目标分类、目标检测和边缘设备。

    There is a lot of research effort devoted by researcher into developing different techniques for neural networks compression, yet the community seems to lack standardised ways of evaluating and comparing between different compression techniques, which is key to identifying the most suitable compression technique for different applications. In this paper we contribute towards standardisation of neural network compression by providing a review of evaluation metrics. These metrics have been implemented into NetZIP, a standardised neural network compression bench. We showcase some of the metrics reviewed using three case studies focusing on object classification, object detection, and edge devices.
    
[^83]: ACRoBat：在编译时优化动态深度学习的自动批处理

    ACRoBat: Optimizing Auto-batching of Dynamic Deep Learning at Compile Time. (arXiv:2305.10611v1 [cs.LG])

    [http://arxiv.org/abs/2305.10611](http://arxiv.org/abs/2305.10611)

    ACRoBat是一种动态深度学习的自动批处理框架，在编译时进行混合静态+动态编译器优化和端到端张量代码生成，可将性能提高多达8.5倍。

    

    动态控制流是一种重要的技术，常用于设计表达力强和高效的深度学习计算，例如文本解析、机器翻译、提前退出深度模型等。然而，由此产生的控制流分叉使得批处理难以手动执行，这是一种重要的性能优化。本文提出了 ACRoBat 框架，通过执行混合静态+动态编译器优化和端到端张量代码生成，实现了动态深度学习计算的高效自动批处理。在 NVIDIA GeForce RTX 3070 GPU 上，ACRoBat 的性能比现有的自动批处理框架 DyNet 提高了多达 8.5 倍。

    Dynamic control flow is an important technique often used to design expressive and efficient deep learning computations for applications such as text parsing, machine translation, exiting early out of deep models and so on. However, the resulting control flow divergence makes batching, an important performance optimization, difficult to perform manually. In this paper, we present ACRoBat, a framework that enables efficient automatic batching for dynamic deep learning computations by performing hybrid static+dynamic compiler optimizations and end-to-end tensor code generation. ACRoBat performs up to 8.5X better than DyNet, a state-of-the-art framework for automatic batching, on an Nvidia GeForce RTX 3070 GPU.
    
[^84]: Tree of Thoughts: 利用大语言模型进行深思熟虑的问题解决

    Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v1 [cs.CL])

    [http://arxiv.org/abs/2305.10601](http://arxiv.org/abs/2305.10601)

    本研究提出了一种新的推理框架——思维之树（ToT），可以增强语言模型的问题解决能力，帮助语言模型进行深思熟虑的决策，以及自我评估和全局选择。

    

    语言模型越来越广泛地用于解决各种任务的通用问题，但在推理过程中仍然受限于基于标记、从左到右的决策过程。这意味着在需要探索、战略前瞻或初始决策发挥关键作用的任务中，他们可能会遇到困难。为了克服这些挑战，我们引入了一种新的语言模型推理框架——思维之树（ToT），它将通常用于提示语言模型的思维链方法泛化，并使用一致的文本单位（思维）进行探究，这些思维作为解决问题的中间步骤。思维之树允许语言模型通过考虑多个不同的推理路径和自我评估来进行深思熟虑的决策，并决定下一步的行动，同时在必要时向前或向后跟踪以进行全局选择。我们的实验表明，ToT显著增强了语言模型的解决问题能力。

    Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abil
    
[^85]: 张量积与超维计算

    Tensor Products and Hyperdimensional Computing. (arXiv:2305.10572v1 [stat.ML])

    [http://arxiv.org/abs/2305.10572](http://arxiv.org/abs/2305.10572)

    本文探索了张量积在超维计算中的数学关系，将其确定为中心表示，并发现它是最通用、最具表现力和最压缩的表示，同时具有无误差解绑和检测的能力。

    

    在之前对图嵌入的分析基础上，我们将一些结果推广和拓展到向量符号结构 (VSA) 和超维计算 (HDC) 的一般设置中。重要的是，我们探索超叠加、正交和张量积之间的数学关系。我们将张量积表示确定为中心表示，并具有一套独特的属性。这包括它是最通用和最具表现力的表示，也是最压缩的表示，具有无误差解绑和检测的能力。

    Following up on a previous analysis of graph embeddings, we generalize and expand some results to the general setting of vector symbolic architectures (VSA) and hyperdimensional computing (HDC). Importantly, we explore the mathematical relationship between superposition, orthogonality, and tensor product. We establish the tensor product representation as the central representation, with a suite of unique properties. These include it being the most general and expressive representation, as well as being the most compressed representation that has errorrless unbinding and detection.
    
[^86]: 动态PET中的自监督学习用于生理药代动力学建模

    Self-Supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET. (arXiv:2305.10569v1 [eess.IV])

    [http://arxiv.org/abs/2305.10569](http://arxiv.org/abs/2305.10569)

    本研究引入自监督学习，提供了一种快速而准确的药代动力学建模方法，可以生成与预期生理学一致的像素级参数图。

    

    动态正电子发射断层成像（dPET）可以提供药物示踪剂的时间分辨图像，从而定量测量生理过程。基于体素的生理药代动力学（PBPK）模型可以对时间-活性曲线（TAC）进行体素级别的建模，为临床工作流程提供相关诊断信息。传统的TAC拟合策略速度慢，并忽略相邻体素之间的空间关系。我们训练了一个时空UNet来估计给定F-18脱氧葡萄糖（FDG）dPET的TAC的动力学参数。本研究引入了自监督损失公式来强制测量TAC与使用学习到的动力学参数生成的TAC之间的相似性。我们的方法提供了器官级别上可比较的结果，与传统方法相比速度显著更快，同时生成的像素级参数图与预期的生理学一致。据我们所知，这是第一个自监督网络应用于PBPk模型的工作。

    Dynamic positron emission tomography imaging (dPET) provides temporally resolved images of a tracer enabling a quantitative measure of physiological processes. Voxel-wise physiologically-based pharmacokinetic (PBPK) modeling of the time activity curves (TAC) can provide relevant diagnostic information for clinical workflow. Conventional fitting strategies for TACs are slow and ignore the spatial relation between neighboring voxels. We train a spatio-temporal UNet to estimate the kinetic parameters given TAC from F-18-fluorodeoxyglucose (FDG) dPET. This work introduces a self-supervised loss formulation to enforce the similarity between the measured TAC and those generated with the learned kinetic parameters. Our method provides quantitatively comparable results at organ-level to the significantly slower conventional approaches, while generating pixel-wise parametric images which are consistent with expected physiology. To the best of our knowledge, this is the first self-supervised net
    
[^87]: 对放弃分类器进行反事实比较

    Counterfactually Comparing Abstaining Classifiers. (arXiv:2305.10564v1 [stat.ML])

    [http://arxiv.org/abs/2305.10564](http://arxiv.org/abs/2305.10564)

    本文提出一种新的方法和视角来评估和比较放弃分类器，将放弃预测视为缺失数据。我们定义了放弃分类器的反事实得分，指的是分类器没有放弃预测时的预测性能期望。

    

    放弃分类器可以选择在不确定时放弃对输入的预测。这些分类器在高风险决策问题中越来越受欢迎，因为它们可以保留不确定的预测，以提高其可靠性和安全性。然而，在评估黑盒放弃分类器时，我们缺乏一个原则性的方法来考虑分类器在它的放弃预测上的预测结果。当放射科医生不确定其诊断或当驾驶员在自动驾驶汽车中不注意时，这些缺失的预测结果至关重要。本文引入了一种新的方法和视角来评估和比较放弃分类器，将放弃预测视为缺失数据。我们的评估方法围绕着定义一个放弃分类器的反事实得分，即分类器没有放弃的情况下的预测性能的期望。我们指定了条件... (此处省略)

    Abstaining classifiers have the option to abstain from making predictions on inputs that they are unsure about. These classifiers are becoming increasingly popular in high-stake decision-making problems, as they can withhold uncertain predictions to improve their reliability and safety. When evaluating black-box abstaining classifier(s), however, we lack a principled approach that accounts for what the classifier would have predicted on its abstentions. These missing predictions are crucial when, e.g., a radiologist is unsure of their diagnosis or when a driver is inattentive in a self-driving car. In this paper, we introduce a novel approach and perspective to the problem of evaluating and comparing abstaining classifiers by treating abstentions as missing data. Our evaluation approach is centered around defining the counterfactual score of an abstaining classifier, defined as the expected performance of the classifier had it not been allowed to abstain. We specify the conditions unde
    
[^88]: 利用Temporal Fusion Transformer的短期电力负荷预测：网格层次和数据来源的影响

    Short-Term Electricity Load Forecasting Using the Temporal Fusion Transformer: Effect of Grid Hierarchies and Data Sources. (arXiv:2305.10559v1 [cs.LG])

    [http://arxiv.org/abs/2305.10559](http://arxiv.org/abs/2305.10559)

    本文研究了使用Temporal Fusion Transformer对不同时间范围和网络级别的短期电力负荷进行预测的潜力，结果显示TFT在变电站级别和使用额外数据源时具有显著的提高。

    

    能源转型的最新发展为配电网提出了特殊挑战。因此，精确的负荷预测变得越来越重要，以实现有效的网格管理。新颖的建模方法，例如Transformer架构，尤其是Temporal Fusion Transformer（TFT），已成为时间序列预测的有希望的方法。迄今为止，只有少数研究将TFT应用于电力负荷预测问题，主要考虑单个数据集和少量协变量。因此，我们研究了TFT架构在不同时间范围（提前一天和提前一周）和网络级别（网格和变电站级别）的小时短期负荷预测的潜力。我们发现，TFT架构在整个网格的提前一天预测中并不比最先进的LSTM模型提供更高的预测性能。然而，当采用分层方法以及包括额外的数据来源（例如天气变量）时，TFT的结果显示出显着的提高。我们的研究结果突出了在具体预测问题上精选数据源和建模技术的重要性。

    Recent developments related to the energy transition pose particular challenges for distribution grids. Hence, precise load forecasts become more and more important for effective grid management. Novel modeling approaches such as the Transformer architecture, in particular the Temporal Fusion Transformer (TFT), have emerged as promising methods for time series forecasting. To date, just a handful of studies apply TFTs to electricity load forecasting problems, mostly considering only single datasets and a few covariates. Therefore, we examine the potential of the TFT architecture for hourly short-term load forecasting across different time horizons (day-ahead and week-ahead) and network levels (grid and substation level). We find that the TFT architecture does not offer higher predictive performance than a state-of-the-art LSTM model for day-ahead forecasting on the entire grid. However, the results display significant improvements for the TFT when applied at the substation level with a
    
[^89]: 具有距离感知自注意力的深度多示例学习

    Deep Multiple Instance Learning with Distance-Aware Self-Attention. (arXiv:2305.10552v1 [cs.CV])

    [http://arxiv.org/abs/2305.10552](http://arxiv.org/abs/2305.10552)

    本文提出具有距离感知自注意力的深度多示例学习模型，该模型能根据补丁之间的空间关系动态调整权重，从而在多个基准数据集上提高了分类性能。

    

    传统的监督学习任务要求对训练集中的每个实例进行标记，但在许多实际应用中，标记仅对实例的集合（包）可用。这种问题设置被称为多重示例学习（MIL），在医疗领域尤其相关，高分辨率图像被分成较小的补丁，但标签适用于整个图像。最近的MIL模型能够通过采用自我关注来捕捉补丁之间的对应关系，使它们能够根据包中所有其他补丁对每个补丁进行不同的加权。然而，这些方法仍然没有考虑较大图像中补丁之间的相对空间关系，这在计算病理学中尤为重要。为此，我们引入了一种新的MIL模型，具有距离感知自注意力（DAS-MIL），它在建模补丁之间的交互作用时明确考虑相对空间信息。与现有相关模型不同，DAS-MIL使用距离感知注意机制根据补丁之间的距离动态调整补丁权重，从而提高了在多个基准数据集上的分类性能。

    Traditional supervised learning tasks require a label for every instance in the training set, but in many real-world applications, labels are only available for collections (bags) of instances. This problem setting, known as multiple instance learning (MIL), is particularly relevant in the medical domain, where high-resolution images are split into smaller patches, but labels apply to the image as a whole. Recent MIL models are able to capture correspondences between patches by employing self-attention, allowing them to weigh each patch differently based on all other patches in the bag. However, these approaches still do not consider the relative spatial relationships between patches within the larger image, which is especially important in computational pathology. To this end, we introduce a novel MIL model with distance-aware self-attention (DAS-MIL), which explicitly takes into account relative spatial information when modelling the interactions between patches. Unlike existing rela
    
[^90]: 无限宽的深度神经网络中稀疏深度的权衡

    Sparsity-depth Tradeoff in Infinitely Wide Deep Neural Networks. (arXiv:2305.10550v1 [cs.LG])

    [http://arxiv.org/abs/2305.10550](http://arxiv.org/abs/2305.10550)

    本文研究了神经网络中稀疏深度和泛化性能的关系，发现在宽度足够大时，较稀疏的网络在浅层时表现更好。

    

    本文研究了稀疏神经活动如何影响具有深度贝叶斯神经网络的泛化性能，特别是在宽度大的情况下。为此，我们得出了一个神经网络高斯过程(NNGP)核，其具有修正线性单元(ReLU)激活和预定数量的活跃神经元。使用NNGP核，我们观察到，在各种数据集上，较稀疏的网络在浅层时优于非稀疏的网络。通过扩展现有的核岭回归的一般化误差理论，我们验证了这一观察结果。

    We investigate how sparse neural activity affects the generalization performance of a deep Bayesian neural network at the large width limit. To this end, we derive a neural network Gaussian Process (NNGP) kernel with rectified linear unit (ReLU) activation and a predetermined fraction of active neurons. Using the NNGP kernel, we observe that the sparser networks outperform the non-sparse networks at shallow depths on a variety of datasets. We validate this observation by extending the existing theory on the generalization error of kernel-ridge regression.
    
[^91]: 通过反向多智能体强化学习发现集体行为中的个体奖励

    Discovering Individual Rewards in Collective Behavior through Inverse Multi-Agent Reinforcement Learning. (arXiv:2305.10548v1 [cs.LG])

    [http://arxiv.org/abs/2305.10548](http://arxiv.org/abs/2305.10548)

    本论文介绍了一种离线逆向多智能体强化学习算法，通过利用演示，自动发现奖励函数并学习代理的有效策略，用于在复杂动态系统中寻找集体行为中的个体目标。

    

    发现复杂动态系统（例如鱼群和细菌群落）中的集体行为中的个体目标是一个长期的挑战。逆强化学习是解决这一挑战的有效方法，但其在涉及连续状态-动作空间和多个交互代理的动态系统中的适用性受到限制。本研究通过引入一种离线逆向多智能体强化学习算法（IMARL）来解决这一挑战。我们的方法结合了ReF-ER技术和导向成本学习。通过利用演示，我们的算法自动发现奖励函数并学习代理的有效策略。通过广泛的实验，我们证明了所提出的策略捕捉到了提供数据中观察到的行为，并在包括OpenAI gym中的单代理模型和涉及多代理的模型中的问题域中取得了有希望的结果。

    The discovery of individual objectives in collective behavior of complex dynamical systems such as fish schools and bacteria colonies is a long-standing challenge. Inverse reinforcement learning is a potent approach for addressing this challenge but its applicability to dynamical systems, involving continuous state-action spaces and multiple interacting agents, has been limited. In this study, we tackle this challenge by introducing an off-policy inverse multi-agent reinforcement learning algorithm (IMARL). Our approach combines the ReF-ER techniques with guided cost learning. By leveraging demonstrations, our algorithm automatically uncovers the reward function and learns an effective policy for the agents. Through extensive experimentation, we demonstrate that the proposed policy captures the behavior observed in the provided data, and achieves promising results across problem domains including single agent models in the OpenAI gym and multi-agent models of schooling behavior. The pr
    
[^92]: 可计算的基于图诱导的和积网络进行概率图表示学习

    Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks. (arXiv:2305.10544v1 [cs.LG])

    [http://arxiv.org/abs/2305.10544](http://arxiv.org/abs/2305.10544)

    GSPNs是一种新的概率框架，用于图表示学习，可以可计算地回答概率查询，并通过权重共享和树状计算图的优势获得了纯概率模型的效率和深度图网络的效果。

    

    我们介绍了基于图诱导的和积网络 (GSPN)，它是一种新的概率框架，用于图表示学习，可以可计算地回答概率查询。受消息传递神经网络中由顶点引起的计算树的启发，我们建立了一组和积网络（SPN）的层次结构，其中父SPN的参数是其子级的后验混合概率的可学习变换。由于权重共享和GSPN的树状计算图，我们获得了纯概率模型的效率和深度图网络的效果。我们在缺乏监督的情况下，处理缺失数据和图分类问题，证明了该模型相对于流行的神经模型的竞争力。我们通过超参数和模型回答概率查询的能力进行定性分析。

    We introduce Graph-Induced Sum-Product Networks (GSPNs), a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. Inspired by the computational trees induced by vertices in the context of message-passing neural networks, we build hierarchies of sum-product networks (SPNs) where the parameters of a parent SPN are learnable transformations of the a-posterior mixing probabilities of its children's sum units. Due to weight sharing and the tree-shaped computation graphs of GSPNs, we obtain the efficiency and efficacy of deep graph networks with the additional advantages of a purely probabilistic model. We show the model's competitiveness on scarce supervision scenarios, handling missing data, and graph classification in comparison to popular neural models. We complement the experiments with qualitative analyses on hyper-parameters and the model's ability to answer probabilistic queries.
    
[^93]: 神经置信传播译码器的泛化边界

    Generalization Bounds for Neural Belief Propagation Decoders. (arXiv:2305.10540v1 [cs.IT])

    [http://arxiv.org/abs/2305.10540](http://arxiv.org/abs/2305.10540)

    本文研究了神经置信传播译码器的泛化能力，提出了一组新的理论结果，界定了解码器的泛化间隙，结果与解码器的复杂程度有关。

    

    越来越多的采用基于机器学习的方法来设计下一代通信系统的解码器。一个广泛使用的框架是神经置信传播（NBP），它将置信传播（BP）迭代展开为深度神经网络，参数以数据驱动方式进行训练。已经证明，NBP解码器相较于传统的解码算法有所改进。本文研究了NBP解码器的泛化能力。具体而言，解码器的泛化间隙是经验和期望误码率之间的差异。我们提出了新的理论结果，界定了这种差距，并表明它与解码器的复杂程度（即代码参数（块长度、消息长度、变量/检查节点度数）、解码迭代次数和训练数据集大小）有关。我们还展示了常规和不规则奇偶校验矩阵的结果。据我们所知，这是第一组关于神经置信传播译码器的理论结果。

    Machine learning based approaches are being increasingly used for designing decoders for next generation communication systems. One widely used framework is neural belief propagation (NBP), which unfolds the belief propagation (BP) iterations into a deep neural network and the parameters are trained in a data-driven manner. NBP decoders have been shown to improve upon classical decoding algorithms. In this paper, we investigate the generalization capabilities of NBP decoders. Specifically, the generalization gap of a decoder is the difference between empirical and expected bit-error-rate(s). We present new theoretical results which bound this gap and show the dependence on the decoder complexity, in terms of code parameters (blocklength, message length, variable/check node degrees), decoding iterations, and the training dataset size. Results are presented for both regular and irregular parity-check matrices. To the best of our knowledge, this is the first set of theoretical results on 
    
[^94]: 自学习对话系统中缺陷行为的可扩展和安全修复

    Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems. (arXiv:2305.10528v1 [cs.AI])

    [http://arxiv.org/abs/2305.10528](http://arxiv.org/abs/2305.10528)

    本文提出了一种在自学习对话系统中利用历史回归事件报告来验证、保护和改进政策的方法，以解决在大规模商业环境中的经验连贯性和政策改进之间的平衡问题。

    

    强化学习已成为最新自然语言对话人工智能的驱动力，改善了目标为导向的代理人与人之间更自然的互动，提高了用户的满意度，但在大型商业环境中，平衡政策改进和经验连贯性经常具有挑战性。本文提出了一种方法，即使用历史回归事件报告中的高精度样本对政策进行验证、安全保护和改进，以便在在线部署前进行修正。作者对真实的对话系统和实际的回归事件数据进行了大量实验，并将所提出的方法应用于他们的生产系统中。

    Off-Policy reinforcement learning has been a driving force for the state-of-the-art conversational AIs leading to more natural humanagent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose a method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable
    
[^95]: 生成式语言模型的统计知识评估

    Statistical Knowledge Assessment for Generative Language Models. (arXiv:2305.10519v1 [cs.CL])

    [http://arxiv.org/abs/2305.10519](http://arxiv.org/abs/2305.10519)

    本文介绍了一个统计知识评估框架，用于评估生成式语言模型（GLMs）的知识输出。通过对14种GLMs进行综合比较，发现知识遵循缩放定律，而对指令遵循数据进行微调可能会损害模型的生成能力。

    

    生成式语言模型（GLMs）展示了存储事实知识和高效回答查询的能力。但是，给定不同的提示，GLM是否始终生成事实正确的答案？本文介绍了一个由潜变量和KaRR度量指导的统计知识评估框架，该度量通过计算模型在各种文本形式上的连续概率量化其知识。我们使用我们的框架对14种GLM的知识进行了全面比较，包括LLaMA、Alpaca、OPT和其他模型。我们的统计知识评估涵盖了600种关系类型，并显示出与人类评估的强相关性（0.43 Kendall's $\tau$）。我们的发现揭示了具有相同支架结构的GLM的知识遵循缩放定律，并且在指令遵循数据上进行的微调可能会损害模型持续生成事实正确的文本的能力。

    Generative Language Models (GLMs) have demonstrated capabilities to store factual knowledge and answer queries efficiently. Given varying prompts, does a GLM consistently generate factually correct answers? In this paper, we introduce a statistical knowledge assessment framework guided by latent variables and the KaRR metric, which quantifies a model's knowledge by computing its continuous probability across diverse text forms. We conduct a comprehensive comparison of knowledge across 14 GLMs using our framework, including LLaMA, Alpaca, OPT, and others. Our statistical knowledge assessment encompasses 600 relation types and exhibits a strong correlation (0.43 Kendall's $\tau$) with human evaluation. Our findings reveal that the knowledge in GLMs with the same backbone architecture adheres to the scaling law, and that tuning on instruction-following data may compromise the model's ability to generate factually correct text consistently.
    
[^96]: 更多脏数据下的系统识别精确恢复

    Exact Recovery for System Identification with More Corrupt Data than Clean Data. (arXiv:2305.10506v1 [cs.LG])

    [http://arxiv.org/abs/2305.10506](http://arxiv.org/abs/2305.10506)

    本文研究了在敌对环境下线性离散时间系统的系统识别问题。在周期性注入攻击时，系统动态可精确恢复样本复杂度为O(n)；当攻击以概率p进行时，精确恢复样本复杂度为O(log(n)p/(1-p)^2) 。即使有超过一半的数据受损，估计器仍可学习。

    

    本文研究了在敌对环境下线性离散时间系统的系统识别问题，并分析了两种Lasso估计器的渐近和非渐近特性，涉及到对于攻击时刻的确定性和随机性两种不同场景。由于收集的样本相关，现有的Lasso结果不适用。我们发现，当系统稳定且攻击以周期性注入时，系统动态的精确恢复的样本复杂度为O(n)，其中n是状态的维度。当攻击在每个时间实例中以概率p进行时，精确恢复所需的样本复杂度将按O(log (n)p / (1-p)^2)进行缩放。该结果在渐近状态下意味着几乎肯定收敛于真实系统动态。作为副产品，即使超过一半的数据受损，我们的估计仍然能够学习。

    In this paper, we study the system identification problem for linear discrete-time systems under adversaries and analyze two lasso-type estimators. We study both asymptotic and non-asymptotic properties of these estimators in two separate scenarios, corresponding to deterministic and stochastic models for the attack times. Since the samples collected from the system are correlated, the existing results on lasso are not applicable. We show that when the system is stable and the attacks are injected periodically, the sample complexity for the exact recovery of the system dynamics is O(n), where n is the dimension of the states. When the adversarial attacks occur at each time instance with probability p, the required sample complexity for the exact recovery scales as O(\log(n)p/(1-p)^2). This result implies the almost sure convergence to the true system dynamics under the asymptotic regime. As a by-product, even when more than half of the data is compromised, our estimators still learn th
    
[^97]: 无模型鲁棒平均奖励强化学习

    Model-Free Robust Average-Reward Reinforcement Learning. (arXiv:2305.10504v1 [cs.LG])

    [http://arxiv.org/abs/2305.10504](http://arxiv.org/abs/2305.10504)

    本文研究了无模型鲁棒平均奖励马尔可夫决策过程，提出两种算法并证明了它们收敛到最优解。我们给出了几个广泛使用的不确定性集合作为示例。

    

    鲁棒马尔可夫决策过程通过在一个马尔可夫决策过程不确定性集合中优化最坏情况的性能来解决模型不确定性的挑战。本文着眼于无模型情况下的鲁棒平均奖励马尔可夫决策过程。我们首先理论上描述了鲁棒平均奖励Bellman方程的解结构，这对我们后面的收敛分析至关重要。接着，我们设计了两个无模型算法，鲁棒相对价值迭代(TD)和鲁棒RVI Q-learning，并理论上证明了它们收敛到最优解。我们提供了几个广泛使用的不确定性集合的示例，包括污染模型、总变差、卡方散度、KL散度和Wasserstein距离。

    Robust Markov decision processes (MDPs) address the challenge of model uncertainty by optimizing the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on the robust average-reward MDPs under the model-free setting. We first theoretically characterize the structure of solutions to the robust average-reward Bellman equation, which is essential for our later convergence analysis. We then design two model-free algorithms, robust relative value iteration (RVI) TD and robust RVI Q-learning, and theoretically prove their convergence to the optimal solution. We provide several widely used uncertainty sets as examples, including those defined by the contamination model, total variation, Chi-squared divergence, Kullback-Leibler (KL) divergence and Wasserstein distance.
    
[^98]: EENED：基于卷积变压器的端到端神经元癫痫检测

    EENED: End-to-End Neural Epilepsy Detection based on Convolutional Transformer. (arXiv:2305.10502v1 [eess.SP])

    [http://arxiv.org/abs/2305.10502](http://arxiv.org/abs/2305.10502)

    本文提出了一种名为EENED的端到端神经元癫痫检测模型，结合了CNN和Transformer，能够同时捕捉EEG信号的全局依赖性和局部特征，并有望提高癫痫检测的准确性和可靠性。

    

    最近，在EEG信号处理中，基于变压器（Transformer）和卷积神经网络（CNN）的模型表现出了很好的结果。变压器模型可以通过自我注意机制捕捉EEG信号的全局依赖性，而CNN模型可以捕捉如锯齿波之类的局部特征。在本文中，我们提出了一种名为EENED的端到端神经元癫痫检测模型，它结合了CNN和Transformer。具体地，通过在Transformer编码器中引入卷积模块，EENED可以学习患者EEG信号特征的时间依赖关系，并注意到与癫痫密切相关的局部EEG异常突变，如尖锐波的出现和缓慢波的散布。我们提出的框架结合了Transformer和CNN捕捉EEG信号不同尺度的特征的能力，有望提高癫痫检测的准确性和可靠性。我们的源代码将很快在GitHub上发布。

    Recently Transformer and Convolution neural network (CNN) based models have shown promising results in EEG signal processing. Transformer models can capture the global dependencies in EEG signals through a self-attention mechanism, while CNN models can capture local features such as sawtooth waves. In this work, we propose an end-to-end neural epilepsy detection model, EENED, that combines CNN and Transformer. Specifically, by introducing the convolution module into the Transformer encoder, EENED can learn the time-dependent relationship of the patient's EEG signal features and notice local EEG abnormal mutations closely related to epilepsy, such as the appearance of spikes and the sprinkling of sharp and slow waves. Our proposed framework combines the ability of Transformer and CNN to capture different scale features of EEG signals and holds promise for improving the accuracy and reliability of epilepsy detection. Our source code will be released soon on GitHub.
    
[^99]: 边方向性提高了异质图上的学习能力

    Edge Directionality Improves Learning on Heterophilic Graphs. (arXiv:2305.10498v1 [cs.LG])

    [http://arxiv.org/abs/2305.10498](http://arxiv.org/abs/2305.10498)

    本文提出了一种新的有向图神经网络框架Dir-GNN，并在有向引用图上进行评估，结果表明它在预测缺失的引用链接方面优于现有的无向GNN和一些有向图模型。

    

    图神经网络（GNN）已成为建模关系数据的事实标准工具。然而，尽管许多真实世界的图是有向的，但今天大多数GNN模型都通过使图成为无向图来完全忽略这些信息。这样做的原因是历史性的：1）许多早期的谱GNN变体明确要求图是无向的，2）关于同类图的第一批基准测试并未发现使用方向性有明显的增益。在本文中，我们展示了在异类设置中，将图形视为有向图可以增加图的内在同质性，这表明了从正确使用方向性信息中可能得到的好处。为此，我们引入了Directed Graph Neural Network（Dir-GNN），这是一个新的面向有向图的深度学习通用框架。Dir-GNN可以用于扩展任何消息传递神经网络（MPNN），以通过对每个节点执行单独的进出消息聚合来考虑边方向性信息。我们在有向引用图上评估了Dir-GNN，并证明它在预测缺失的引用链接方面优于现有的无向GNN和一些有向图模型。我们的结果表明，方向性信息可以提高在异质图上的学习能力，Dir-GNN可以有效地利用这些信息。

    Graph Neural Networks (GNNs) have become the de-facto standard tool for modeling relational data. However, while many real-world graphs are directed, the majority of today's GNN models discard this information altogether by simply making the graph undirected. The reasons for this are historical: 1) many early variants of spectral GNNs explicitly required undirected graphs, and 2) the first benchmarks on homophilic graphs did not find significant gain from using direction. In this paper, we show that in heterophilic settings, treating the graph as directed increases the effective homophily of the graph, suggesting a potential gain from the correct use of directionality information. To this end, we introduce Directed Graph Neural Network (Dir-GNN), a novel general framework for deep learning on directed graphs. Dir-GNN can be used to extend any Message Passing Neural Network (MPNN) to account for edge directionality information by performing separate aggregations of the incoming and outg
    
[^100]: 保留你自己的相关性：用于视频扩散模型的噪声先验

    Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models. (arXiv:2305.10474v1 [cs.CV])

    [http://arxiv.org/abs/2305.10474](http://arxiv.org/abs/2305.10474)

    本论文介绍了一种新的视频噪声先验，用于微调图像扩散模型，以实现更高质量的视频合成。经过广泛的实验验证，该模型已经取得了UCF-101和MSR-VTT基准测试的最佳结果。

    

    尽管扩散模型在生成高质量图像方面取得了巨大进展，但合成连续的动画帧，既具有光真实感，又具有时间相关性仍处于起步阶段。在可以使用成亿级图像数据集的同时，收集相似规模的视频数据仍然具有挑战性。此外，与其图像对应的模型相比，训练视频扩散模型的计算代价更高。在本文中，我们探讨了使用视频数据微调预训练的图像扩散模型作为视频合成任务的实用解决方案。我们发现，在视频扩散中天真地将图像噪声先验扩展为视频噪声先验会导致次优的性能。我们设计了一种精心设计的视频噪声先验，其在视频扩散中具有显著的更好性能。广泛的实验验证表明，我们的模型 Preserve Your Own Correlation (PYoCo) 在 UCF-101 和 MSR-VTT 基准测试中获得了零样本文本对视频的最佳结果。

    Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and temporally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still challenging. Also, training a video diffusion model is computationally much more expensive than its image counterpart. In this work, we explore finetuning a pretrained image diffusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffusion leads to sub-optimal performance. Our carefully designed video noise prior leads to substantially better performance. Extensive experimental validation shows that our model, Preserve Your Own Correlation (PYoCo), attains SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It also
    
[^101]: 利用循环神经网络预测药物分子的副作用

    Predicting Side Effect of Drug Molecules using Recurrent Neural Networks. (arXiv:2305.10473v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.10473](http://arxiv.org/abs/2305.10473)

    该论文介绍了一种利用循环神经网络预测药物分子副作用的启发式方法，有效降低了模型复杂度和参数数量。

    

    鉴定和验证分子属性，如副作用，是分子合成过程中最重要和耗时的步骤之一。我们提出了一种启发式方法，利用简单的神经网络，特别是循环神经网络，进行副作用预测，避免了依赖于复杂模型的问题并大大降低了参数数量。

    Identification and verification of molecular properties such as side effects is one of the most important and time-consuming steps in the process of molecule synthesis. For example, failure to identify side effects before submission to regulatory groups can cost millions of dollars and months of additional research to the companies. Failure to identify side effects during the regulatory review can also cost lives. The complexity and expense of this task have made it a candidate for a machine learning-based solution. Prior approaches rely on complex model designs and excessive parameter counts for side effect predictions. We believe reliance on complex models only shifts the difficulty away from chemists rather than alleviating the issue. Implementing large models is also expensive without prior access to high-performance computers. We propose a heuristic approach that allows for the utilization of simple neural networks, specifically the recurrent neural network, with a 98+% reduction 
    
[^102]: 生态学家使用机器学习的九个技巧

    Nine tips for ecologists using machine learning. (arXiv:2305.10472v1 [q-bio.PE])

    [http://arxiv.org/abs/2305.10472](http://arxiv.org/abs/2305.10472)

    本论文介绍了九个技巧来帮助生态学家实施机器学习模型，这些技巧针对分类问题，旨在识别开发机器学习模型中的常见错误、陷阱或挑战，并提供了解决方法。

    

    由于其高精度和灵活性，机器学习模型成为了生态学家合适且高效的工具。但是，实施机器学习模型并不是一项简单的任务，对于没有在这个领域有经验的生态学家来说可能会有些难以接受。在这里，我们提供了一系列技巧来帮助生态学家实施机器学习模型。我们专注于分类问题，因为许多生态学研究旨在将数据归入预定义的类别，例如生态状态或生物实体。这九个技巧中，每一个都提出了在开发机器学习模型方面的常见错误、陷阱或挑战，并提供了有助于生态学研究中使用机器学习的建议。

    Due to their high predictive performance and flexibility, machine learning models are an appropriate and efficient tool for ecologists. However, implementing a machine learning model is not yet a trivial task and may seem intimidating to ecologists with no previous experience in this area. Here we provide a series of tips to help ecologists in implementing machine learning models. We focus on classification problems as many ecological studies aim to assign data into predefined classes such as ecological states or biological entities. Each of the nine tips identifies a common error, trap or challenge in developing machine learning models and provides recommendations to facilitate their use in ecological studies.
    
[^103]: Bike2Vec: 路上自行车车手和比赛的向量嵌入表示

    Bike2Vec: Vector Embedding Representations of Road Cycling Riders and Races. (arXiv:2305.10471v1 [cs.LG])

    [http://arxiv.org/abs/2305.10471](http://arxiv.org/abs/2305.10471)

    Bike2Vec是一种通过历史比赛结果学习车手和比赛表示的方法，这些表示可以用于车手和比赛的特征提取和下游预测任务。

    

    向量嵌入已成功应用于多个领域，以获得非数字数据的有效表示，然后可用于各种下游任务。在职业公路自行车领域，我们提出了一种新的向量嵌入应用，通过展示一种基于历史结果学习车手和比赛表示的方法。我们使用无监督学习技术验证结果嵌入捕获了车手和比赛的有趣特征。这些嵌入可用于诸如早期人才识别和比赛结果预测等下游预测任务。

    Vector embeddings have been successfully applied in several domains to obtain effective representations of non-numeric data which can then be used in various downstream tasks. We present a novel application of vector embeddings in professional road cycling by demonstrating a method to learn representations for riders and races based on historical results. We use unsupervised learning techniques to validate that the resultant embeddings capture interesting features of riders and races. These embeddings could be used for downstream prediction tasks such as early talent identification and race outcome prediction.
    
[^104]: 连接隐藏神经元（CHNNet）：一种快速收敛的人工神经网络

    Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence. (arXiv:2305.10468v1 [cs.NE])

    [http://arxiv.org/abs/2305.10468](http://arxiv.org/abs/2305.10468)

    该论文提出了一个更为强大的人工神经网络模型，该模型中同一隐藏层中的隐藏神经元相互连接，可以学习复杂模式并加速收敛速度。

    

    人工神经网络的核心目的是模仿生物神经网络的功能。然而，与生物神经网络不同，传统的人工神经网络通常是按层次结构化的，这可能会妨碍神经元之间的信息流动，因为同一层中的神经元之间没有连接。因此，我们提出了一种更为强大的人工神经网络模型，其中同一隐藏层中的隐藏神经元是互相连接的，使得神经元能够学习复杂的模式并加速收敛速度。通过在浅层和深层网络中将我们提出的模型作为完全连接的层进行实验研究，我们证明这个模型可以显著提高收敛速率。

    The core purpose of developing artificial neural networks was to mimic the functionalities of biological neural networks. However, unlike biological neural networks, traditional artificial neural networks are often structured hierarchically, which can impede the flow of information between neurons as the neurons in the same layer have no connections between them. Hence, we propose a more robust model of artificial neural networks where the hidden neurons, residing in the same hidden layer, are interconnected, enabling the neurons to learn complex patterns and speeding up the convergence rate. With the experimental study of our proposed model as fully connected layers in shallow and deep networks, we demonstrate that the model results in a significant increase in convergence rate.
    
[^105]: 使用质数邻接矩阵分析生物医学知识图谱

    Analysing Biomedical Knowledge Graphs using Prime Adjacency Matrices. (arXiv:2305.10467v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.10467](http://arxiv.org/abs/2305.10467)

    本文提出了一种使用质数邻接矩阵分析生物医学知识图谱的新方法，可以高效地计算网络的多个属性和提高结果，且不需要复杂的培训或流程。

    

    生物医学任务中的大多数现象本质上是复杂的，并且在许多情况下，它们被表达为生物医学知识图谱（KG）上的信号。我们引入了一种新的表示框架——Prime Adjacency Matrix（PAM）用于生物医学KG，它允许非常高效的网络分析。PAM利用质数来表示整个KG，并快速计算网络的多个属性。我们通过对不同的生物医学知识图谱进行研究并提供两个案例研究：一个是针对COVID-19的药物重复利用，另一个是重要的元路径提取的应用性来说明这个框架的适用性。我们展示了使用非常简单的方法，没有任何的培训，可以在更短的时间内比原始的流程获得更好的结果。

    Most phenomena related to biomedical tasks are inherently complex, and in many cases, are expressed as signals on biomedical Knowledge Graphs (KGs). In this work, we introduce the use of a new representation framework, the Prime Adjacency Matrix (PAM) for biomedical KGs, which allows for very efficient network analysis. PAM utilizes prime numbers to enable representing the whole KG with a single adjacency matrix and the fast computation of multiple properties of the network. We illustrate the applicability of the framework in the biomedical domain by working on different biomedical knowledge graphs and by providing two case studies: one on drug-repurposing for COVID-19 and one on important metapath extraction. We show that we achieve better results than the original proposed workflows, using very simple methods that require no training, in considerably less time.
    
[^106]: 基于重构误差的少量异常样本检测

    Reconstruction Error-based Anomaly Detection with Few Outlying Examples. (arXiv:2305.10464v1 [cs.LG])

    [http://arxiv.org/abs/2305.10464](http://arxiv.org/abs/2305.10464)

    该论文探讨了在训练给定少量异常情况下，通过基于重构误差的神经网络结构，排除已知的异常点，用于检测异常情况的方法。

    

    基于重构误差的神经网络结构是一种经典的深度学习检测异常的方法，其表现出色。该方法通过训练自编码器来重构代表正常数据的样本集，然后指出那些重构误差足够大的数据为异常情况。然而，这些结构常常能够很好地重构数据中的异常情况。特别当训练集中存在异常情况时，这种现象更为明显。当这些异常情况有标签时，这种情况称为半监督，训练自编码器的最佳方法是忽略异常情况并在正常数据上最小化重构误差。本文旨在探讨让基于重构误差的结构能够让模型将已知的异常点排除在正常数据集之外的方法。具体而言，我们的策略利用了少量异常样本。

    Reconstruction error-based neural architectures constitute a classical deep learning approach to anomaly detection which has shown great performances. It consists in training an Autoencoder to reconstruct a set of examples deemed to represent the normality and then to point out as anomalies those data that show a sufficiently large reconstruction error. Unfortunately, these architectures often become able to well reconstruct also the anomalies in the data. This phenomenon is more evident when there are anomalies in the training set. In particular when these anomalies are labeled, a setting called semi-supervised, the best way to train Autoencoders is to ignore anomalies and minimize the reconstruction error on normal data. The goal of this work is to investigate approaches to allow reconstruction error-based architectures to instruct the model to put known anomalies outside of the domain description of the normal data. Specifically, our strategy exploits a limited number of anomalous e
    
[^107]: 使用条件场初始化的神经网络拓扑优化以提高效率

    Topology Optimization using Neural Networks with Conditioning Field Initialization for Improved Efficiency. (arXiv:2305.10460v1 [cs.LG])

    [http://arxiv.org/abs/2305.10460](http://arxiv.org/abs/2305.10460)

    本文提出了一种条件场初始化的神经网络拓扑优化方法，通过使用先前的初始场，进一步提高了神经网络拓扑优化的效率。

    

    我们提出了使用条件场初始化神经网络拓扑优化的方法。本文旨在（1）改进现有的神经网络拓扑优化方法，（2）通过在未优化域上使用先前的初始场，进一步提高神经网络拓扑优化的效率。我们的方法是建立一个拓扑神经网络，针对单个拓扑优化问题进行个案训练，以表示几何形状。它以域坐标作为输入，表征每个坐标处的密度，其中拓扑由连续的密度场表示。位移通过有限元求解器求解。我们利用初始设计域上计算出的应变能场作为附加的条件场输入，在整个优化过程中用于神经网络。与单独的神经网络相比，应变能场的加入提高了收敛速度。

    We propose conditioning field initialization for neural network based topology optimization. In this work, we focus on (1) improving upon existing neural network based topology optimization, (2) demonstrating that by using a prior initial field on the unoptimized domain, the efficiency of neural network based topology optimization can be further improved. Our approach consists of a topology neural network that is trained on a case by case basis to represent the geometry for a single topology optimization problem. It takes in domain coordinates as input to represent the density at each coordinate where the topology is represented by a continuous density field. The displacement is solved through a finite element solver. We employ the strain energy field calculated on the initial design domain as an additional conditioning field input to the neural network throughout the optimization. The addition of the strain energy field input improves the convergence speed compared to standalone neura
    
[^108]: AnalogNAS: 一种用于采用模拟内存计算实现准确推理的神经网络设计框架

    AnalogNAS: A Neural Network Design Framework for Accurate Inference with Analog In-Memory Computing. (arXiv:2305.10459v1 [cs.AR])

    [http://arxiv.org/abs/2305.10459](http://arxiv.org/abs/2305.10459)

    AnalogNAS是一种用于自动化设计深度神经网络的框架，可以针对模拟内存计算推理加速器进行优化，提高在边缘环境下的区域和功耗效率。

    

    深度学习技术的发展离不开高效的深度神经网络设计和新的硬件加速器。目前的深度神经网络设计主要针对常规平台的通用部署。而在边缘推理中，需要低延迟、紧凑且功耗效率高的模型，并且在成本方面必须划算。基于 typica von Neumann 结构的数字处理器在边缘人工智能中运用较为困难，因为需要大量的数据进出内存进行处理。相反，采用模拟/混合信号内存计算硬件加速器可以轻易超越 von Neumann 结构中的内存墙，加速推理工作负载。该硬件加速器可以提供更高的区域和功耗效率，这在边缘资源受限的环境中是至关重要的。本文提出了一种名为 AnalogNAS 的框架，用于自动化设计深度神经网络，并针对模拟内存计算推理加速器的部署进行优化。我们进行了大量的硬件仿真以评估框架的性能。

    The advancement of Deep Learning (DL) is driven by efficient Deep Neural Network (DNN) design and new hardware accelerators. Current DNN design is primarily tailored for general-purpose use and deployment on commercially viable platforms. Inference at the edge requires low latency, compact and power-efficient models, and must be cost-effective. Digital processors based on typical von Neumann architectures are not conducive to edge AI given the large amounts of required data movement in and out of memory. Conversely, analog/mixed signal in-memory computing hardware accelerators can easily transcend the memory wall of von Neuman architectures when accelerating inference workloads. They offer increased area and power efficiency, which are paramount in edge resource-constrained environments. In this paper, we propose AnalogNAS, a framework for automated DNN design targeting deployment on analog In-Memory Computing (IMC) inference accelerators. We conduct extensive hardware simulations to d
    
[^109]: 随机卷积核的时间序列聚类

    Time Series Clustering With Random Convolutional Kernels. (arXiv:2305.10457v1 [cs.LG])

    [http://arxiv.org/abs/2305.10457](http://arxiv.org/abs/2305.10457)

    该论文介绍了一种新方法用于时间序列聚类，该方法利用随机卷积结构将数据转换为增强的特征表示，再进行聚类，以识别异常值，该方法在时间序列聚类基准上实现了更好的结果。

    

    时间序列可描述广泛的自然和社会现象，如气候、地震、股票价格或网站访问趋势。时间序列聚类有助于找到异常值，这些异常值可能代表温度异常、火山爆发、市场干扰或欺诈性网站流量。基于自动特征提取技术的成功，特别是采用随机核技术，我们开发了一种新的时间序列聚类方法，包括两个步骤。首先，一个随机卷积结构将数据转换为增强的特征表示。然后，聚类算法对转换后的数据进行分类。该方法在时间序列聚类基准上改善了最先进的结果。

    Time series can describe a wide range of natural and social phenomena. A few samples are climate and seismic measures trends, stock prices, or website visits. Time-series clustering helps to find outliers that, related to these instances, could represent temperature anomalies, imminent volcanic eruptions, market disturbances, or fraudulent web traffic. Founded on the success of automatic feature extraction techniques, specifically employing random kernels, we develop a new method for time series clustering consisting of two steps. First, a random convolutional structure transforms the data into an enhanced feature representation. Afterwards, a clustering algorithm classifies the transformed data. The method improves state-of-the-art results on time series clustering benchmarks.
    
[^110]: 对比分类器在挑战方案中的表现

    Comparison of classifiers in challenge scheme. (arXiv:2305.10452v1 [cs.LG])

    [http://arxiv.org/abs/2305.10452](http://arxiv.org/abs/2305.10452)

    本文介绍了在挑战方案的限制下评估不同竞争者(算法)表现的问题，包括使用唯一的数据集、规定最小提交次数和一组性能指标等措施。关键问题是，这些差异能否对最终结果产生重大影响。

    

    近几十年来，挑战方案作为一种众包机制在科学研究中越来越受欢迎。特别是对于开发机器学习算法来说，挑战方案至关重要。本文讨论了在挑战方案的限制下评估不同竞争者（算法）表现的问题，包括使用唯一的数据集（大小固定）、规定最小提交次数和一组性能指标等措施。分类器按性能指标排序，但常常发现竞争对手间的性能差异微乎其微，甚至只有千分之一左右。因此关键问题是，这些差异能否对最终结果产生重大影响。

    In recent decades, challenges have become very popular in scientific research as these are crowdsourcing schemes. In particular, challenges are essential for developing machine learning algorithms. For the challenges settings, it is vital to establish the scientific question, the dataset (with adequate quality, quantity, diversity, and complexity), performance metrics, as well as a way to authenticate the participants' results (Gold Standard). This paper addresses the problem of evaluating the performance of different competitors (algorithms) under the restrictions imposed by the challenge scheme, such as the comparison of multiple competitors with a unique dataset (with fixed size), a minimal number of submissions and, a set of metrics chosen to assess performance. The algorithms are sorted according to the performance metric. Still, it is common to observe performance differences among competitors as small as hundredths or even thousandths, so the question is whether the differences 
    
[^111]: 机构如何影响人工智能与人类协同设计空间探索？以深度生成模型进行船舶设计案例研究。

    How does agency impact human-AI collaborative design space exploration? A case study on ship design with deep generative models. (arXiv:2305.10451v1 [cs.LG])

    [http://arxiv.org/abs/2305.10451](http://arxiv.org/abs/2305.10451)

    通过构建基于生成对抗网络的生成设计空间（GDS），本文探究了随机探索、半自动探索和自动探索三种不同模式的GDS探索方法在船舶设计中的适用性和效果。

    

    传统参数化方法通过根据基础设计生成变化来限制对多样化设计的探索。相反，生成模型通过利用现有设计创建紧凑且多样化的生成设计空间（GDS）来提供解决方案。然而，当前的探索方法在复杂的GDS中的有效性，特别是在船体设计中，仍不清楚。为此，我们首先使用生成对抗网络构建了一个GDS，该网络使用各种类型船舶的52,591种设计进行了训练。接下来，我们构建了三种探索模式：随机探索（REM）、半自动探索（SAEM）和自动探索（AEM），其中用户的参与程度各不相同，以探索GDS并获取新颖且优化的设计。在REM中，用户基于直觉手动探索GDS。在SAEM中，用户和优化器均驱动探索。优化器专注于探索多样化的优化设计，而用户将探索重点放在其自己的设计偏好上。AEM使用优化器探索GDS以生成优化的设计。

    Typical parametric approaches restrict the exploration of diverse designs by generating variations based on a baseline design. In contrast, generative models provide a solution by leveraging existing designs to create compact yet diverse generative design spaces (GDSs). However, the effectiveness of current exploration methods in complex GDSs, especially in ship hull design, remains unclear. To that end, we first construct a GDS using a generative adversarial network, trained on 52,591 designs of various ship types. Next, we constructed three modes of exploration, random (REM), semi-automated (SAEM) and automated (AEM), with varying levels of user involvement to explore GDS for novel and optimised designs. In REM, users manually explore the GDS based on intuition. In SAEM, both the users and optimiser drive the exploration. The optimiser focuses on exploring a diverse set of optimised designs, while the user directs the exploration towards their design preference. AEM uses an optimiser
    
[^112]: 通过相空间分析和卷积神经网络理解正常和异常心脏

    Understanding of Normal and Abnormal Hearts by Phase Space Analysis and Convolutional Neural Networks. (arXiv:2305.10450v1 [eess.IV])

    [http://arxiv.org/abs/2305.10450](http://arxiv.org/abs/2305.10450)

    通过相空间分析和卷积神经网络结合诊断心脏疾病，在MIT-BIH数据库上取得了93.3%的平均准确率。

    

    心脏疾病是现代工业化社会中致死因素之一，导致公共卫生系统的高昂开支。因高昂成本，开发分析方法以改善心脏诊断至关重要。本文通过非线性微分方程将心脏电活动建模，并研究起源于确定性动态的心脏频谱的变化。将时间序列心电图(ECG)图像提取相空间轨迹，并基于 MIT-BIH 数据库中记录的 44 个 MLII 图像应用相空间分析和卷积神经网络(CNN)方法进行处理。为了提高准确性，将相空间记录的最高 Q-R 距离之间画一条直线。对相空间图像训练二进制 CNN 分类模型以分类正常和异常心脏，该方法达到平均准确率 93.3%，证明了相空间分析和 CNN 均在心脏疾病诊断中的有效性。

    Cardiac diseases are one of the leading mortality factors in modern, industrialized societies, which cause high expenses in public health systems. Due to high costs, developing analytical methods to improve cardiac diagnostics is essential. The heart's electric activity was first modeled using a set of nonlinear differential equations. Following this, variations of cardiac spectra originating from deterministic dynamics are investigated. Analyzing a normal human heart's power spectra offers His-Purkinje network, which possesses a fractal-like structure. Phase space trajectories are extracted from the time series electrocardiogram (ECG) graph with third-order derivate Taylor Series. Here in this study, phase space analysis and Convolutional Neural Networks (CNNs) method are applied to 44 records via the MIT-BIH database recorded with MLII. In order to increase accuracy, a straight line is drawn between the highest Q-R distance in the phase space images of the records. Binary CNN classif
    
[^113]: 合作是你所需要的。 （arXiv:2305.10449v1 [cs.LG]）

    Cooperation Is All You Need. (arXiv:2305.10449v1 [cs.LG])

    [http://arxiv.org/abs/2305.10449](http://arxiv.org/abs/2305.10449)

    引入了一种基于“本地处理器民主”的算法Cooperator，该算法在强化学习中表现比Transformer算法更好。

    

    在超越“树突民主”之上，我们引入了一个名为Cooperator的“本地处理器民主”。在这里，我们将它们与基于Transformers的机器学习算法（例如ChatGPT）在置换不变神经网络强化学习（RL）中的功能进行比较。 Transformers基于长期以来的“积分-发射”“点”神经元的概念，而Cooperator则受到最近神经生物学突破的启示，这些突破表明，精神生活的细胞基础取决于新皮层中具有两个功能上不同点的上皮神经元。我们表明，当用于RL时，基于Cooperator的算法学习速度比基于Transformer的算法快得多，即使它们具有相同数量的参数。

    Going beyond 'dendritic democracy', we introduce a 'democracy of local processors', termed Cooperator. Here we compare their capabilities when used in permutation-invariant neural networks for reinforcement learning (RL), with machine learning algorithms based on Transformers, such as ChatGPT. Transformers are based on the long-standing conception of integrate-and-fire 'point' neurons, whereas Cooperator is inspired by recent neurobiological breakthroughs suggesting that the cellular foundations of mental life depend on context-sensitive pyramidal neurons in the neocortex which have two functionally distinct points. We show that when used for RL, an algorithm based on Cooperator learns far quicker than that based on Transformer, even while having the same number of parameters.
    
[^114]: CBAGAN-RRT: 卷积块注意力生成对抗网络用于基于采样的路径规划

    CBAGAN-RRT: Convolutional Block Attention Generative Adversarial Network for Sampling-Based Path Planning. (arXiv:2305.10442v1 [cs.RO])

    [http://arxiv.org/abs/2305.10442](http://arxiv.org/abs/2305.10442)

    本文介绍了一种基于图像处理学习算法（CBAGAN-RRT）的路径规划方法，使用卷积块注意力生成对抗网络和一种新的损失函数，找到更优的最佳路径并提高算法的收敛速度，与先前的最先进算法相比，在图像质量生成指标和路径规划指标方面都表现更优。

    

    基于采样的路径规划算法在自主机器人中发挥着重要作用。但是，基于RRT算法的一个常见问题是生成的初始路径不是最优的，而且收敛速度过慢，无法应用于实际场景。本文提出了一种使用卷积块注意力生成对抗网络和一种新的损失函数的图像处理学习算法（CBAGAN-RRT），以设计启发式算法，找到更优的最佳路径，并提高算法的收敛速度。我们的GAN模型生成的路径概率分布用于引导RRT算法的采样过程。我们在由 \cite {zhang2021generative} 生成的数据集上进行了网络的训练和测试，并证明了我们的算法在图像质量生成指标（如IOU分数，Dice分数）和路径规划指标（如路径长度和成功率）方面均优于先前的最先进算法。

    Sampling-based path planning algorithms play an important role in autonomous robotics. However, a common problem among the RRT-based algorithms is that the initial path generated is not optimal and the convergence is too slow to be used in real-world applications. In this paper, we propose a novel image-based learning algorithm (CBAGAN-RRT) using a Convolutional Block Attention Generative Adversarial Network with a combination of spatial and channel attention and a novel loss function to design the heuristics, find a better optimal path, and improve the convergence of the algorithm both concerning time and speed. The probability distribution of the paths generated from our GAN model is used to guide the sampling process for the RRT algorithm. We train and test our network on the dataset generated by \cite{zhang2021generative} and demonstrate that our algorithm outperforms the previous state-of-the-art algorithms using both the image quality generation metrics like IOU Score, Dice Score
    
[^115]: 利用大型视觉语言模型学习文本的视觉性

    Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v1 [cs.CL])

    [http://arxiv.org/abs/2305.10434](http://arxiv.org/abs/2305.10434)

    该论文利用大型视觉语言模型如CLIP来检测文本的视觉性，并提出fine-tuning策略，将非视觉文本映射为NULL图像，匹配视觉文本与对应图像，以解锁在文本中嵌入相关图像的能力。

    

    视觉文本会在人们的脑海中呈现图像，而非视觉文本则无法达到此效果。自动检测文本的视觉性将有助于在文本中嵌入相关图像。我们创建了一个数据集，包括3620个英语句子及其多个人类注释者提供的视觉性得分，并使用包含文本和视觉资产的文档来创建远程监督语料库，以评估文本的视觉性。

    Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will unlock the ability to augment text with relevant images, as neural text-to-image generation and retrieval models operate on the implicit assumption that the input text is visual in nature. We curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. Additionally, we use documents that contain text and visual assets to create a distantly supervised corpus of document text and associated images. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP that assume a one-to-one correspondence between text and image to the task of scoring text visualness from text input alone. Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images 
    
[^116]: 模型对比联邦领域自适应

    Model-Contrastive Federated Domain Adaptation. (arXiv:2305.10432v1 [cs.LG])

    [http://arxiv.org/abs/2305.10432](http://arxiv.org/abs/2305.10432)

    本文提出了一种名为FDAC的联邦领域自适应方法，通过对比学习和采用Vision Transformer（ViT）框架提取可适应特征，成功实现了在联邦设置中学习可转移的表示。实验表明，FDAC优于最先进的联邦领域自适应方法。

    

    联邦领域自适应（FDA）旨在将知识从源客户端（领域）合作地转移到相关但不同的目标客户端，而不需要传递任何客户端的本地数据。此外，源客户端具有不同的数据分布，这导致知识传递极具挑战性。尽管FDA取得了最近的进展，但我们经验性地发现，现有方法无法利用异构领域的模型，因此无法实现出色的性能。在本文中，我们提出了一种名为FDAC的基于模型的方法，旨在通过对比学习和Vision Transformer（ViT）解决基于联邦的领域自适应。特别地，对比学习可以利用未标记的数据训练出优秀的模型，而ViT架构在提取适应特征方面比卷积神经网络（CNN）表现更好。据我们所知，FDAC是第一次尝试在联邦设置中通过比较不同模型来学习可转移的表示。在各种基准测试上进行的广泛实验表明，FDAC优于最先进的FDA方法。

    Federated domain adaptation (FDA) aims to collaboratively transfer knowledge from source clients (domains) to the related but different target client, without communicating the local data of any client. Moreover, the source clients have different data distributions, leading to extremely challenging in knowledge transfer. Despite the recent progress in FDA, we empirically find that existing methods can not leverage models of heterogeneous domains and thus they fail to achieve excellent performance. In this paper, we propose a model-based method named FDAC, aiming to address {\bf F}ederated {\bf D}omain {\bf A}daptation based on {\bf C}ontrastive learning and Vision Transformer (ViT). In particular, contrastive learning can leverage the unlabeled data to train excellent models and the ViT architecture performs better than convolutional neural networks (CNNs) in extracting adaptable features. To the best of our knowledge, FDAC is the first attempt to learn transferable representations by 
    
[^117]: 评估LLM的隐藏风险：关于鲁棒性、一致性和可信性的实证研究

    Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])

    [http://arxiv.org/abs/2305.10235](http://arxiv.org/abs/2305.10235)

    本研究是一项关于大型语言模型方面的实证研究，对主流语言模型进行了大量查询和分析，结果发现这些模型存在着鲁棒性、一致性和可信性方面的潜在风险。

    

    大型语言模型（LLMs）的普及对于许多领域产生了重大影响，特别是在其开放式环境（如API、开源模型和插件）中。然而，随着LLMs的广泛部署，缺乏全面讨论和分析潜在风险的研究。因此，我们进行了一项初步但开创性的研究，涵盖了LLMs系统的鲁棒性、一致性和可信性。我们提出了一个自动化工作流程来处理大量查询/响应。总体而言，我们对包括ChatGPT、LLaMA和OPT在内的主流LLMs进行了100多万个查询。我们的工作流核心包括数据原语，随后是自动解释器，评估这些LLMs在不同的对抗性度量系统下的表现。结果，我们得出了几个、也许是不幸的结论，这些结论相当不同

    The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
    
[^118]: 一种多目标优化的Wasserstein反向强化学习模型的证明

    A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization. (arXiv:2305.10089v1 [cs.LG])

    [http://arxiv.org/abs/2305.10089](http://arxiv.org/abs/2305.10089)

    本文证明了Wasserstein反向强化学习模型适用于多目标优化问题，可让学习者的奖励值和最优解模仿专家，具有一定的实用价值。

    

    本文证明了Wasserstein反向强化学习模型可以在有限次迭代中让学习者的奖励值模仿专家的奖励值，并证明了在词典序的多目标优化中，Wasserstein反向强化学习模型可以让学习者的最优解模仿专家的最优解。

    We prove Wasserstein inverse reinforcement learning enables the learner's reward values to imitate the expert's reward values in a finite iteration for multi-objective optimizations. Moreover, we prove Wasserstein inverse reinforcement learning enables the learner's optimal solutions to imitate the expert's optimal solutions for multi-objective optimizations with lexicographic order.
    
[^119]: “我全然成为我自己”：以TGNB人群为中心，评估开放式语言生成中的偏见

    "I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])

    [http://arxiv.org/abs/2305.09941](http://arxiv.org/abs/2305.09941)

    本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。

    

    跨性别和非二元（TGNB）人群在日常生活中经历了不成比例的歧视和排斥。随着语言生成技术的日益普及和应用，进一步边缘化这一人群的可能性也在增加。虽然大量的NLP公平文献着重于阐明和解决性别偏见，但评估TGNB身份所带来的性别伤害需要理解这些身份如何独特地与社会性别规范互动以及与性别二元中心的视角相区分。这样的测量框架本质上需要以TGNB声音为中心，帮助指导包容性别的自然语言处理应该为谁服务。为实现这一目标，我们以TGNB社区和现有的跨学科文献为基础，评估了TGNB个体经历边缘化所形成的社会现实是如何影响和存在于开放式语言生成（OLG）中。首先理解TGNB个体的经历，我们提出了一个评估OLG系统的框架，旨在以TGNB人群为中心，度量与该人群相关的偏见。我们的框架包括特别为TGNB人群设计的调查工具，以及交叉分析结果的交叉方法。我们相信，这项工作将有助于实现更公平、更包容的自然语言处理社区，并潜在地解决NLP研究中广泛的交叉身份问题。

    Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
    
[^120]: Epsilon Sampling Rocks: 研究用于机器翻译最小贝叶斯风险解码的采样策略

    Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])

    [http://arxiv.org/abs/2305.09860](http://arxiv.org/abs/2305.09860)

    本文研究了用于机器翻译最小贝叶斯风险解码的不同采样策略，并发现了epsilon采样方式能够使得解码结果显著地优于其他所有已测试的采样方式和束搜索解码。

    

    机器翻译中的最小贝叶斯风险（MBR）解码已经显示出是一种强大的替代束搜索解码的方法，尤其是与基于神经网络的效用函数相结合时。然而，MBR解码的性能严重依赖于从模型中采样的方法和数量。本文探讨了用于MBR解码的不同采样方法对性能的影响。我们评估了一些流行的采样方法，例如祖先采样，核采样和top-k采样。基于我们对它们局限性的认识，我们尝试了最近提出的epsilon采样方法，该方法通过修剪所有小于epsilon的标记，以确保样本中的每个标记获得公平的概率质量。通过广泛的人类评估，我们证明了基于epsilon采样的MBR解码显著优于不仅是束搜索解码，而且还优于所有其他已测试的采样方法的MBR解码。

    Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k sampling. Based on our insights into their limitations, we experiment with the recently proposed epsilon-sampling approach, which prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass. Through extensive human evaluations, we demonstrate that MBR decoding based on epsilon-sampling significantly outperforms not only beam search decoding, but also MBR decoding with all other tested samp
    
[^121]: 大数据学习：精选包与随机包的对比研究

    Learning from Aggregated Data: Curated Bags versus Random Bags. (arXiv:2305.09557v1 [cs.LG])

    [http://arxiv.org/abs/2305.09557](http://arxiv.org/abs/2305.09557)

    本文研究了两种自然的聚合方法：基于共同特征将数据点分组的精选包和将数据点随机分组的随机包，对于精选包设置和广泛的损失函数范围内，我们展示了可以通过梯度下降学习而不会导致数据聚合导致性能下降的情况。

    

    保护用户隐私是许多机器学习系统部署的一个主要关注点，这些系统收集来自各种群体的数据。为了应对这种问题，一种方法是以聚合的形式收集和发布数据标签，从而可以将单个用户的信息与其他用户的信息组合起来。本文探讨了使用聚合数据标签而非单个标签来训练机器学习模型的可能性，具体来说，我们考虑了两种自然的聚合方法：基于共同特征将数据点分组的精选包和将数据点随机分组的随机包。对于精选包设置和广泛的损失函数范围内，我们展示了可以通过梯度下降学习而不会导致数据聚合导致性能下降的情况。我们的方法基于以下观察：损失函数的梯度之和可以表示为每个包的梯度的加权和，其中权重是包的大小。

    Protecting user privacy is a major concern for many machine learning systems that are deployed at scale and collect from a diverse set of population. One way to address this concern is by collecting and releasing data labels in an aggregated manner so that the information about a single user is potentially combined with others. In this paper, we explore the possibility of training machine learning models with aggregated data labels, rather than individual labels. Specifically, we consider two natural aggregation procedures suggested by practitioners: curated bags where the data points are grouped based on common features and random bags where the data points are grouped randomly in bag of similar sizes. For the curated bag setting and for a broad range of loss functions, we show that we can perform gradient-based learning without any degradation in performance that may result from aggregating data. Our method is based on the observation that the sum of the gradients of the loss functio
    
[^122]: 空间天气研究中的太阳活动区磁场图像数据集

    Solar Active Region Magnetogram Image Dataset for Studies of Space Weather. (arXiv:2305.09492v1 [astro-ph.SR])

    [http://arxiv.org/abs/2305.09492](http://arxiv.org/abs/2305.09492)

    本数据集提供了一系列太阳活动区磁图，并提供相应的太阳耀斑标签。它可用于研究磁结构、其演化以及太阳耀斑的关系，并对于自动太阳耀斑预测方法的研究具有重要价值。

    

    本数据集提供了美国国家航空航天局（NASA）太阳动力学观测卫星（SDO）的磁图（衡量磁场强度的图像）的全面收集。该数据集包含来自三个来源的数据，提供SDO地震学和磁学仪（HMI）太阳活跃区（大磁通区域，通常是爆发事件的源头）的磁图以及相应耀斑活动的标签。该数据集对于研究磁结构、其随时间的演化以及与太阳耀斑的关系的图像分析或太阳物理学研究将非常有用。该数据集将对那些研究自动太阳耀斑预测方法的研究人员产生兴趣，包括监督和无监督的机器学习（经典和深度）、二元和多类分类以及回归。该数据集是一个最小处理且用户可配置的一致大小太阳图像数据集。

    In this dataset we provide a comprehensive collection of magnetograms (images quantifying the strength of the magnetic field) from the National Aeronautics and Space Administration's (NASA's) Solar Dynamics Observatory (SDO). The dataset incorporates data from three sources and provides SDO Helioseismic and Magnetic Imager (HMI) magnetograms of solar active regions (regions of large magnetic flux, generally the source of eruptive events) as well as labels of corresponding flaring activity. This dataset will be useful for image analysis or solar physics research related to magnetic structure, its evolution over time, and its relation to solar flares. The dataset will be of interest to those researchers investigating automated solar flare prediction methods, including supervised and unsupervised machine learning (classical and deep), binary and multi-class classification, and regression. This dataset is a minimally processed, user configurable dataset of consistently sized images of sola
    
[^123]: 保障联合学习管理系统安全的智能策略控制

    Smart Policy Control for Securing Federated Learning Management System. (arXiv:2305.09134v1 [cs.CR])

    [http://arxiv.org/abs/2305.09134](http://arxiv.org/abs/2305.09134)

    本文提出了一种基于智能合约的策略控制，可以保障联合学习管理系统的安全性，确保每个参与者都遵守数据保护政策，同时可以审计训练过程。

    

    物联网设备在智慧城市、智能医疗系统和其他实际应用中的广泛应用导致了大量数据的生成，常常使用不同的机器学习模型进行分析。联合学习（FL）被认为是一种保护隐私的机器学习技术，在不共享原始数据的情况下多个参与者合作训练机器学习模型。然而，由于每个FL参与者实施了各种数据保护政策，当前的FL架构不允许对训练过程进行审计。另外，当前架构中没有全局模型可验证性。本文提出了基于智能合约的策略控制来保障FL管理系统的安全。首先，我们在FL参与者侧开发和部署基于智能合约的本地训练策略控制。这个策略控制被用来验证训练过程，确保每个FL参与者都遵守数据保护政策，从而实现FL管理系统的安全性。

    The widespread adoption of Internet of Things (IoT) devices in smart cities, intelligent healthcare systems, and various real-world applications have resulted in the generation of vast amounts of data, often analyzed using different Machine Learning (ML) models. Federated learning (FL) has been acknowledged as a privacy-preserving machine learning technology, where multiple parties cooperatively train ML models without exchanging raw data. However, the current FL architecture does not allow for an audit of the training process due to the various data-protection policies implemented by each FL participant. Furthermore, there is no global model verifiability available in the current architecture. This paper proposes a smart contract-based policy control for securing the Federated Learning (FL) management system. First, we develop and deploy a smart contract-based local training policy control on the FL participants' side. This policy control is used to verify the training process, ensuri
    
[^124]: 强化学习在Tractography中的作用

    What Matters in Reinforcement Learning for Tractography. (arXiv:2305.09041v1 [cs.LG])

    [http://arxiv.org/abs/2305.09041](http://arxiv.org/abs/2305.09041)

    本论文深入探讨了强化学习在Tractography中不同的组成部分，提出了关于RL算法选择、代理人输入、奖励函数和播种策略的一系列建议。

    

    最近，提出了利用深度强化学习（RL）来学习Tractography过程并训练代理人在没有手动筛选的参考流线的情况下重建白质结构。虽然报告的表现颇具竞争力，但所提出的框架复杂，并且对于其多个部分的作用和影响还知之甚少。在这项工作中，我们深入探讨了所提出框架的不同组成部分，例如RL算法的选择，播种策略，输入信号和奖励函数，并阐明了它们的影响。本次研究共训练了约7400个模型，共计近41000小时的GPU时间。我们的目标是指导热衷于探索深度RL在Tractography中可能性的研究人员，展示这种方法的优势和不足。因此，我们最终提出了关于RL算法的选择、代理人输入、奖励函数和播种策略的一系列建议。

    Recently, deep reinforcement learning (RL) has been proposed to learn the tractography procedure and train agents to reconstruct the structure of the white matter without manually curated reference streamlines. While the performances reported were competitive, the proposed framework is complex, and little is still known about the role and impact of its multiple parts. In this work, we thoroughly explore the different components of the proposed framework, such as the choice of the RL algorithm, seeding strategy, the input signal and reward function, and shed light on their impact. Approximately 7,400 models were trained for this work, totalling nearly 41,000 hours of GPU time. Our goal is to guide researchers eager to explore the possibilities of deep RL for tractography by exposing what works and what does not work with the category of approach. As such, we ultimately propose a series of recommendations concerning the choice of RL algorithm, the input to the agents, the reward function
    
[^125]: 自监督神经因子分析解耦语音表示

    Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v1 [cs.SD])

    [http://arxiv.org/abs/2305.08099](http://arxiv.org/abs/2305.08099)

    本文提出了一种自监督神经因子分析模型，使用HuBERT中的聚类方法来发现隐藏的声学单元，并使用这些单元对齐SSL模型的特征，从而产生解耦后的语音表示，从而为专门任务提供了一种基于Utterance水平的无监督学习目标。实验结果表明，SSNFA模型在说话人识别、语言识别和情感识别等各种任务中均显著优于现有的SSL模型，并且没有任何特定任务的微调或监督。

    

    自监督学习技术在自动语音识别方面已经展示了出色的性能，在低标注资源情况下证明非常有用，本文针对该技术在说话人、情感和语言识别等任务中的性能问题进行了探究。本文提出了一种因子分析模型，使用HuBERT中的聚类方法来发现隐藏的声学单元，并使用这些单元对齐SSL模型的特征，从而产生解耦后的语音表示，从而为专门任务提供了一种基于Utterance水平的无监督学习目标。实验结果表明，SSNFA模型在说话人识别、语言识别和情感识别等各种任务中均显著优于现有的SSL模型，并且没有任何特定任务的微调或监督。

    Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have demonstrated state-of-the-art performance on automatic speech recognition (ASR) and proved to be extremely useful in low label-resource settings. However, the success of SSL models has yet to transfer to utterance-level tasks such as speaker, emotion, and language recognition, which still require supervised fine-tuning of the SSL models to obtain good performance. We argue that the problem is caused by the lack of disentangled representations and an utterance-level learning objective for these tasks. Inspired by how HuBERT uses clustering to discover hidden acoustic units, we formulate a factor analysis (FA) model that uses the discovered hidden acoustic units to align the SSL features. The underlying utterance-level representations are disentangled from the content of speech using probabilistic inference on the aligned features. Furthermore, the variational lower bound derived from the FA model provides an ut
    
[^126]: DRew：带延迟的动态重连消息传递

    DRew: Dynamically Rewired Message Passing with Delay. (arXiv:2305.08018v1 [cs.LG])

    [http://arxiv.org/abs/2305.08018](http://arxiv.org/abs/2305.08018)

    本文提出了一种能够应用于任何MPNN结构的框架，执行基于层的动态重连来确保逐渐密集化的图形。同时引入了一种延迟机制，允许跨层节点之间的跳跃连接。

    

    已经证明，消息传递神经网络（MPNN）存在过度压缩现象，导致长程相互作用任务表现不佳。这主要归因于只在节点的相邻居之间进行局部消息传递。试图使图形“更连通”并且更适合长程任务的重连方法通常会失去基于图形距离提供的归纳偏差，因为它们会使远程节点在每一层中立即通信。在本文中，我们提出了一个框架，可应用于任何MPNN架构，以执行基于层的重连，以确保逐渐加密图形。我们还提出了一种延迟机制，它允许根据层和它们的相互距离在节点之间进行跳跃连接。我们在几个长程任务上验证了我们的方法，并表明其优于图形变换器和多跳MPNN。

    Message passing neural networks (MPNNs) have been shown to suffer from the phenomenon of over-squashing that causes poor performance for tasks relying on long-range interactions. This can be largely attributed to message passing only occurring locally, over a node's immediate neighbours. Rewiring approaches attempting to make graphs `more connected', and supposedly better suited to long-range tasks, often lose the inductive bias provided by distance on the graph since they make distant nodes communicate instantly at every layer. In this paper we propose a framework, applicable to any MPNN architecture, that performs a layer-dependent rewiring to ensure gradual densification of the graph. We also propose a delay mechanism that permits skip connections between nodes depending on the layer and their mutual distance. We validate our approach on several long-range tasks and show that it outperforms graph Transformers and multi-hop MPNNs.
    
[^127]: 基于匹配特征提取的异构边缘设备工业健康预测的联邦学习

    A Federated Learning-based Industrial Health Prognostics for Heterogeneous Edge Devices using Matched Feature Extraction. (arXiv:2305.07854v1 [cs.LG])

    [http://arxiv.org/abs/2305.07854](http://arxiv.org/abs/2305.07854)

    提出了一种基于联邦学习的健康预测模型，该模型具有特征相似性匹配算法来区分学习来自异构边缘设备的数据，以便开发出更准确的预测模型。

    

    数据驱动的工业健康预测需要丰富的训练数据才能开发准确可靠的预测模型。然而，严格的数据隐私法律和丰富的边缘工业数据需要分散式数据利用。因此，联邦学习（FL）是一个分散式和隐私保护的学习技术，非常适用于工业健康预测领域。然而，由于异构数据的数据异质性，以及由于不同的退化机制和不平等的数据集大小所导致的数据异构性，在联邦学习的基础上开发精度高的训练模型是一个关键的统计挑战。因此，FL在健康预测任务中的应用尚未充分研究。本文提出了一种具有特征相似性匹配的参数聚合算法的 FL 健康预测模型。

    Data-driven industrial health prognostics require rich training data to develop accurate and reliable predictive models. However, stringent data privacy laws and the abundance of edge industrial data necessitate decentralized data utilization. Thus, the industrial health prognostics field is well suited to significantly benefit from federated learning (FL), a decentralized and privacy-preserving learning technique. However, FL-based health prognostics tasks have hardly been investigated due to the complexities of meaningfully aggregating model parameters trained from heterogeneous data to form a high performing federated model. Specifically, data heterogeneity among edge devices, stemming from dissimilar degradation mechanisms and unequal dataset sizes, poses a critical statistical challenge for developing accurate federated models. We propose a pioneering FL-based health prognostic model with a feature similarity-matched parameter aggregation algorithm to discriminatingly learn from h
    
[^128]: Dr. LLaMA：通过生成式数据增强改善特定领域QA中的小语言模型

    Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])

    [http://arxiv.org/abs/2305.07804](http://arxiv.org/abs/2305.07804)

    本论文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，以改善小语言模型的性能，特别是在医学问答任务中。这种方法在微调后使模型性能提高，并提出了在特定领域问答任务中使用LLM所面临的挑战和潜在的研究方向。

    

    大型语言模型在自然语言处理方面取得了重大进展，但随着其规模的增长，也面临着计算开销和效率的挑战，特别是在特定领域的任务中。另一方面，小型语言模型由于容量和训练数据的限制，在这些任务中往往表现不佳。本文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，聚焦医学问答任务和PubMedQA数据集，以改善小语言模型的性能。我们的发现表明，LLM有效地细化和扩展现有的问题-答案对，在微调后，使得小型模型在特定领域QA数据集上性能提高。本研究强调了在特定领域问答任务中使用LLM面临的挑战，并提出了潜在的研究方向，最终旨在为专业应用创建更高效和能力更强的模型。

    Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
    
[^129]: 无监督句子嵌入的实例平滑对比学习

    Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v1 [cs.CL])

    [http://arxiv.org/abs/2305.07424](http://arxiv.org/abs/2305.07424)

    本文提出了IS-CSE方法，通过实例平滑对比学习来学习无监督句子嵌入，以平滑嵌入在特征空间中的边界，从而提高模型的泛化性能。在标准的STS任务中取得了良好的得分。

    

    基于对比学习的方法，如unsup-SimCSE，在学习无监督句子嵌入方面取得了最先进（SOTA）的性能。然而，在以前的研究中，用于对比学习的每个嵌入仅来自于一个句子实例，我们称这些嵌入为实例级嵌入。换句话说，在这种情况下，每个嵌入被视为是一类独特的类，这可能会损害泛化性能。在本研究中，我们提出了IS-CSE（实例平滑对比句子嵌入）来平滑特征空间中嵌入的边界。具体而言，我们根据语义相似性从动态内存缓冲区中检索嵌入以获得正嵌入组。然后我们通过自注意力操作对组中的嵌入进行聚合，以生成平滑实例嵌入以进行进一步分析。我们在标准的语义文本相似性（STS）任务中评估了我们的方法，并实现了平均78.30％，79.47％，77.73％和79.42％的得分。

    Contrastive learning-based methods, such as unsup-SimCSE, have achieved state-of-the-art (SOTA) performances in learning unsupervised sentence embeddings. However, in previous studies, each embedding used for contrastive learning only derived from one sentence instance, and we call these embeddings instance-level embeddings. In other words, each embedding is regarded as a unique class of its own, whichmay hurt the generalization performance. In this study, we propose IS-CSE (instance smoothing contrastive sentence embedding) to smooth the boundaries of embeddings in the feature space. Specifically, we retrieve embeddings from a dynamic memory buffer according to the semantic similarity to get a positive embedding group. Then embeddings in the group are aggregated by a self-attention operation to produce a smoothed instance embedding for further analysis. We evaluate our method on standard semantic text similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%, and 79.42% 
    
[^130]: 基于控制微分方程的Hawkes过程

    Hawkes Process based on Controlled Differential Equations. (arXiv:2305.07031v1 [cs.LG])

    [http://arxiv.org/abs/2305.07031](http://arxiv.org/abs/2305.07031)

    本文提出了一种基于控制微分方程的Hawkes过程模型，可精确计算对数似然，并能够正确处理不规则时间序列，适用于社会扩散和地震预测。

    

    Hawkes过程是一种常用的模型框架，用于对多个领域的序贯事件发生动态进行建模，例如社会扩散。在现实场景中，事件之间的间隔时间是不规则的。然而，现有基于神经网络的Hawkes过程模型不仅难以捕捉这种复杂的不规则动态，而且还会使用启发式方法计算事件的对数似然，因为它们大多基于设计用于规则离散输入的神经网络。为此，我们提出了基于控制微分方程(CDE)的Hawkes过程概念，通过采用类似于连续RNN的神经CDE技术。由于HP-CDE不断地读取数据，因此可以适当地处理不规则时间序列数据集，保留它们的不均匀时间空间，并且对数似然可以准确计算。此外，由于Hawkes过程和神经CDE都是在连续的时间域中首先开发的，它们具有相似的背景。因此，HP-CDE具有透明的结构，可以轻松适应实际场景，例如社会扩散，其中事件之间的间隔时间是不规则的。我们使用合成和真实的社交扩散和地震数据集演示了我们提出的模型的优势，并超过了现有的最先进的Hawkes过程模型。

    Hawkes processes are a popular framework to model the occurrence of sequential events, i.e., occurrence dynamics, in several fields such as social diffusion. In real-world scenarios, the inter-arrival time among events is irregular. However, existing neural network-based Hawkes process models not only i) fail to capture such complicated irregular dynamics, but also ii) resort to heuristics to calculate the log-likelihood of events since they are mostly based on neural networks designed for regular discrete inputs. To this end, we present the concept of Hawkes process based on controlled differential equations (HP-CDE), by adopting the neural controlled differential equation (neural CDE) technology which is an analogue to continuous RNNs. Since HP-CDE continuously reads data, i) irregular time-series datasets can be properly treated preserving their uneven temporal spaces, and ii) the log-likelihood can be exactly computed. Moreover, as both Hawkes processes and neural CDEs are first de
    
[^131]: 多目标优化的逆强化学习的收敛性证明研究

    A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])

    [http://arxiv.org/abs/2305.06137](http://arxiv.org/abs/2305.06137)

    本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。

    

    本文通过将等效于多目标优化的WIRL问题的逆问题与投影次梯度法相结合，证明了Wasserstein逆强化学习（WIRL）在多目标优化中的收敛性。此外，我们还证明了逆强化学习（最大熵逆强化学习，导引成本学习）在多目标优化中的收敛性。

    We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
    
[^132]: Mediapipe和CNN用于实时美国手语手势识别

    Mediapipe and CNNs for Real-Time ASL Gesture Recognition. (arXiv:2305.05296v1 [cs.CV])

    [http://arxiv.org/abs/2305.05296](http://arxiv.org/abs/2305.05296)

    Mediapipe和CNN用于实时美国手语手势识别。测试结果表明准确率可达99.95％，有潜力用于听力障碍人士的通信设备，并可以应用于其他相似手语。这项研究对计算机视觉和机器学习领域做出了重要贡献。

    

    本研究论文描述了一种使用现代计算机视觉和机器学习方法进行识别美国手语（ASL）运动的实时系统。所提出的方法利用Mediapipe库进行特征提取，使用卷积神经网络（CNN）进行ASL手势分类。测试结果表明，所提出的系统可以以99.95％的准确率检测所有ASL字母，表明它在为听力障碍人士设计的通信设备中有潜力。所提出的方法也可以应用于其他具有相似手部运动的手语，从而可能提高听力丧失人士的生活质量。总的来说，该研究证明了使用Mediapipe和CNN进行实时手语识别的有效性，对计算机视觉和机器学习领域做出了重要贡献。

    This research paper describes a realtime system for identifying American Sign Language (ASL) movements that employs modern computer vision and machine learning approaches. The suggested method makes use of the Mediapipe library for feature extraction and a Convolutional Neural Network (CNN) for ASL gesture classification. The testing results show that the suggested system can detect all ASL alphabets with an accuracy of 99.95%, indicating its potential for use in communication devices for people with hearing impairments. The proposed approach can also be applied to additional sign languages with similar hand motions, potentially increasing the quality of life for people with hearing loss. Overall, the study demonstrates the effectiveness of using Mediapipe and CNN for real-time sign language recognition, making a significant contribution to the field of computer vision and machine learning.
    
[^133]: 使用数据内核比较基础模型

    Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v1 [cs.LG])

    [http://arxiv.org/abs/2305.05126](http://arxiv.org/abs/2305.05126)

    本文采用基于数据内核的方法比较基础模型，不受度量指标的约束，通过嵌入空间几何实现点对点和多模型比较，并成功诱导了一组与下游指标强相关的模型距离函数流形。

    

    最近自主学习和神经网络扩展的进展使得可以创建大型基础模型，这些模型可以轻松地适应各种下游任务。目前比较基础模型的范式涉及在各种策划数据集上使用聚合指标进行基准测试。不幸的是，这种模型比较方法严重依赖于度量指标的选择，这使得它在理想度量不明显或不可用的情况下不适用。在这项工作中，我们提出了一种没有度量指标的基础模型比较方法，通过它们的嵌入空间几何来实现。我们的方法基于随机图理论，并促进点对点和多模型比较。此外，我们展示了如何使用我们的框架诱导一组配备有与一些下游指标强相关的距离函数的模型流形。

    Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models -- known as foundation models -- which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves benchmarking them with aggregate metrics on various curated datasets. Unfortunately, this method of model comparison is heavily dependent on the choice of metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a metric-free methodology for comparing foundation models via their embedding space geometry. Our methodology is grounded in random graph theory, and facilitates both pointwise and multi-model comparison. Further, we demonstrate how our framework can be used to induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics.
    
[^134]: 带参数知识引导的增强型大语言模型

    Augmented Large Language Models with Parametric Knowledge Guiding. (arXiv:2305.04757v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.04757](http://arxiv.org/abs/2305.04757)

    这篇论文提出了一种带参数知识引导的增强型大语言模型框架，通过为LLMs装备信息引导模块来访问相关知识，同时保持LLMs的参数不变。这个框架可以提高黑盒LLMs在各种NLP任务上的性能。

    

    大型语言模型 (LLM) 以其出色的语言理解和生成能力，极大地推进了自然语言处理（NLP）。但是，由于对相关数据的有限接触，它们在需要专业知识的领域特定任务上的表现可能不够优化。此外，大多数最先进的 LLM 缺乏透明度，只能通过 API 访问, 这阻止了进一步用领域定制数据进行微调。此外，向 LLM 所有者提供私有数据会导致数据隐私问题。为解决这些挑战，我们提出了新型的带参数知识引导 (PKG) 框架，该框架为 LLM 配备了知识引导模块，以访问相关知识，而无需改变 LLM 的参数。我们的 PKG 基于开源的“白盒”语言模型，允许离线存储 LLM 需要的任何知识。我们证明了我们的 PKG 框架可以提高“黑盒”LLM在各种NLP任务上的性能。

    Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source "white-box" language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of "black-box" LLMs on a range o
    
[^135]: 深度强化学习的后验采样

    Posterior Sampling for Deep Reinforcement Learning. (arXiv:2305.00477v1 [cs.LG])

    [http://arxiv.org/abs/2305.00477](http://arxiv.org/abs/2305.00477)

    本文提出了用于深度强化学习的后验采样算法PSDRL，结合了高效的不确定性量化和特殊设计的持续规划算法，使其在提高样本效率的同时显著优于之前的尝试。

    

    尽管深度强化学习算法取得了显著的成功，但样本效率仍然较低：它们需要大量的试错来找到好的策略。基于模型的算法通过构建可以用于规划的环境模型来提高样本效率。后验采样强化学习是这样一种基于模型的算法，在表格设置中由于其性能而引起了广泛的兴趣。本文介绍了用于深度强化学习的后验采样（PSDRL），这是第一个真正可扩展的后验采样强化学习的近似方法，保留了其基于模型的本质特征。PSDRL将潜在状态空间模型上的高效不确定性量化与基于值函数逼近的特殊设计的持续规划算法相结合。对Atari基准测试的广泛实验表明，PSDRL在提高样本效率的同时，显著优于以前的最先进尝试。

    Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. PSDRL combines efficient uncertainty quantification over latent state space models with a specially tailored continual planning algorithm based on value-function approximation. Extensive experiments on the Atari benchmark show that PSDRL significantly outperforms previous state-of-the-art attempts at scaling up posterior sampling while being 
    
[^136]: 利用扰动来改善基于核化斯坦距的拟合优度检验

    Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy. (arXiv:2304.14762v1 [stat.ML])

    [http://arxiv.org/abs/2304.14762](http://arxiv.org/abs/2304.14762)

    本文提出了一种通过在样本中引入扰动，改进基于核化斯坦距的拟合优度检验方法的方法，以解决在同质但混合比例不同的情况下低功率的问题，并展示实验证据证明了该方法的功效。

    

    核化斯坦距（KSD）是一种广泛用于拟合优度检验的基于得分的差异度量。即使目标分布具有未知的标准化因子，例如在贝叶斯分析中，也可以应用它。我们理论上和实验证明，当目标分布和替代分布具有相同且相距较远的模式但在混合比例上有所不同时，KSD检验可能会出现低功率问题。我们提出通过马尔科夫转移核对观测样本进行扰动，使其相对于目标分布不变。这使我们可以在扰动样本上使用KSD检验。我们提供的数值证据表明，使用适当选择的核时，所提出的方法可以比KSD检验具有更高的功率。

    Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely used in goodness-of-fit tests. It can be applied even when the target distribution has an unknown normalising factor, such as in Bayesian analysis. We show theoretically and empirically that the KSD test can suffer from low power when the target and the alternative distribution have the same well-separated modes but differ in mixing proportions. We propose to perturb the observed sample via Markov transition kernels, with respect to which the target distribution is invariant. This allows us to then employ the KSD test on the perturbed sample. We provide numerical evidence that with suitably chosen kernels the proposed approach can lead to a substantially higher power than the KSD test.
    
[^137]: 医学图像的“Segment Anything Model”模型？

    Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])

    [http://arxiv.org/abs/2304.14660](http://arxiv.org/abs/2304.14660)

    “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。

    

    “Segment Anything Model”（SAM）是第一个适用于常规图像分割的基础模型。它设计了一种新颖的可推广分割任务，通过自动和手动两种模式实现了使用预训练模型进行零样本图像分割。SAM在各种自然图像分割任务中取得了显着的成果。然而，由于复杂的模态、细微的解剖结构、不确定的复杂对象边界和广泛的对象尺度，医学图像分割（MIS）更具挑战性。SAM在各种自然图像分割任务中取得了显着的成果。同时，零样本和高效的MIS可以很好地减少注释时间并促进医学图像分析的发展。因此，SAM似乎是一个潜在的工具，并且其在大型医学数据集上的表现应该进一步验证。我们收集和整理了52个开源数据集，并建立了一个具有16个模态和68个对象的大型医学分割数据集。

    The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
    
[^138]: 训练中结合对手和反对手。

    Combining Adversaries with Anti-adversaries in Training. (arXiv:2304.12550v1 [cs.LG])

    [http://arxiv.org/abs/2304.12550](http://arxiv.org/abs/2304.12550)

    该论文研究了在对抗训练中，通过结合对手和反对手(带有反对手扰动的样本)可以更有效地提高深度神经网络的公平性、鲁棒性和泛化性，在一些特定的学习场景中表现出更好的性能。

    

    对抗训练是提高深度神经网络健壮性的有效学习技术。本研究在更一般的扰动范围下理论上研究了对抗训练对深度学习模型的公平性、鲁棒性和泛化性的影响。我们的理论探索表明，将对手和反对手 (带有反对手扰动的样本) 结合在训练中，在一些典型的学习场景 (如噪声标签学习和不平衡学习) 中能够更有效地实现更好的类别间公平性和鲁棒性和泛化性之间的平衡，相比于标准对抗训练。

    Adversarial training is an effective learning technique to improve the robustness of deep neural networks. In this study, the influence of adversarial training on deep learning models in terms of fairness, robustness, and generalization is theoretically investigated under more general perturbation scope that different samples can have different perturbation directions (the adversarial and anti-adversarial directions) and varied perturbation bounds. Our theoretical explorations suggest that the combination of adversaries and anti-adversaries (samples with anti-adversarial perturbations) in training can be more effective in achieving better fairness between classes and a better tradeoff between robustness and generalization in some typical learning scenarios (e.g., noisy label learning and imbalance learning) compared with standard adversarial training. On the basis of our theoretical findings, a more general learning objective that combines adversaries and anti-adversaries with varied b
    
[^139]: BN与ReLU之间的不和谐引起梯度爆炸，但被激活之间的相关性所抵消。

    The Disharmony Between BN and ReLU Causes Gradient Explosion, but is Offset by the Correlation Between Activations. (arXiv:2304.11692v1 [cs.LG])

    [http://arxiv.org/abs/2304.11692](http://arxiv.org/abs/2304.11692)

    本研究阐述了BN和ReLU之间的不和谐是导致梯度爆炸的主要原因，同时发现输入之间的相关性可以缓解这个问题。提出一种基于二阶优化算法的自适应学习率算法，在大批量训练中表现优异，并可替代WarmUp，在小批量训练中也表现不错。

    

    基于批标准化和ReLU等激活函数的深度神经网络可能会在训练初期由于时间梯度爆炸而出现不稳定。我们解释了ReLU如何比预期更多地减少方差，以及批标准化如何在恢复期间放大梯度，导致前向传播保持稳定而梯度爆炸。此外，我们还讨论了深度神经网络在训练过程中的动力学变化以及输入之间的相关性如何缓解这个问题。最后，我们提出了一种灵感来自二阶优化算法的更好的自适应学习率算法，在大批量训练中优于现有的学习率缩放方法，并可替换小批量训练中的WarmUp。

    Deep neural networks based on batch normalization and ReLU-like activation functions can experience instability during the early stages of training due to the high gradient induced by temporal gradient explosion. We explain how ReLU reduces variance more than expected, and how batch normalization amplifies the gradient during recovery, which causes gradient explosion while forward propagation remains stable. Additionally, we discuss how the dynamics of a deep neural network change during training and how the correlation between inputs can alleviate this problem. Lastly, we propose a better adaptive learning rate algorithm inspired by second-order optimization algorithms, which outperforms existing learning rate scaling methods in large batch training and can also replace WarmUp in small batch training.
    
[^140]: Eyettention：基于注意力机制的双序列模型以预测人类阅读时的扫视路径

    Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading. (arXiv:2304.10784v1 [cs.CL])

    [http://arxiv.org/abs/2304.10784](http://arxiv.org/abs/2304.10784)

    Eyettention是第一个同时处理语言序列和时间序列的阅读模型，可以更准确地模拟阅读者的扫视路径，对机器学习的自然语言处理模型具有借鉴意义。

    

    阅读时的眼动揭示了阅读者的认知过程和所阅读文本的特征。因此，阅读中扫视路径的分析已引起各个领域的关注，涵盖了从认知科学到语言学和计算机科学。然而，模拟阅读时人类的扫视路径的主要挑战在于它们是由双序列组成的：单词按照语言的语法规则排序，而注视则按照时间顺序排序。人类并不严格按左到右的顺序阅读，而是跳过或重复注视单词，并倒退到以前的单词，语言序列和时间序列的对齐并不容易。本文开发了Eyettention，这是第一个同时处理语言序列和时间序列的双序列模型。

    Eye movements during reading offer insights into both the reader's cognitive processes and the characteristics of the text that is being read. Hence, the analysis of scanpaths in reading have attracted increasing attention across fields, ranging from cognitive science over linguistics to computer science. In particular, eye-tracking-while-reading data has been argued to bear the potential to make machine-learning-based language models exhibit a more human-like linguistic behavior. However, one of the main challenges in modeling human scanpaths in reading is their dual-sequence nature: the words are ordered following the grammatical rules of the language, whereas the fixations are chronologically ordered. As humans do not strictly read from left-to-right, but rather skip or refixate words and regress to previous words, the alignment of the linguistic and the temporal sequence is non-trivial. In this paper, we develop Eyettention, the first dual-sequence model that simultaneously process
    
[^141]: DEIR: 基于区分性模型的情节内在奖励，高效且鲁棒的探索方法

    DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards. (arXiv:2304.10770v1 [cs.LG])

    [http://arxiv.org/abs/2304.10770](http://arxiv.org/abs/2304.10770)

    提出了一种探索强化学习算法DEIR，借助区分性模型实现理论上导出的内在奖励，能够高效且鲁棒地进行探索，适用于面对外部奖励稀疏的情况。

    

    探索是强化学习中的一个基本方面，其有效性关键地影响着强化学习算法的性能，尤其是面对稀疏的外部奖励时更为重要。最近的研究表明，从观测中估计新颖性的内在奖励可以有效鼓励探索。然而，由于环境的随机性以及代理的行为可能会影响观察结果，因此一个观测的新颖性与探索之间存在差距。为了准确估计探索行为，我们提出了DEIR，一种新颖的方法，其中我们从条件互信息项中理论上导出内在奖励，该奖励主要与代理的探索行为所贡献的新颖性成比例，并借助区分性的前向模型实现奖励。我们在MiniGrid中进行了广泛的实验，包括标准和硬核探索游戏，在结果上DEIR比基线学习更快并且具有更高的成功率和鲁棒性，适应环境动态变化。

    Exploration is a fundamental aspect of reinforcement learning (RL), and its effectiveness crucially decides the performance of RL algorithms, especially when facing sparse extrinsic rewards. Recent studies showed the effectiveness of encouraging exploration with intrinsic rewards estimated from novelty in observations. However, there is a gap between the novelty of an observation and an exploration in general, because the stochasticity in the environment as well as the behavior of an agent may affect the observation. To estimate exploratory behaviors accurately, we propose DEIR, a novel method where we theoretically derive an intrinsic reward from a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and materialize the reward with a discriminative forward model. We conduct extensive experiments in both standard and hardened exploration games in MiniGrid to show that DEIR quickly learns a better policy than baselines. Our eval
    
[^142]: 社会文化知识在仇恨言论检测任务中对选项的选择是必要的

    Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])

    [http://arxiv.org/abs/2304.01890](http://arxiv.org/abs/2304.01890)

    HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。

    

    我们引入了HATELEXICON，这是一个包含巴西，德国，印度和肯尼亚的蔑称和仇恨言论目标的词汇表，以帮助模型的训练和可解释性。我们展示了我们的词汇表如何用于解释模型预测，表明发展用于分类极端言论的模型，在进行预测时严重依赖目标词。此外，我们提出了一种通过HATELEXICON来辅助低资源环境下训练选项的方法，选项选择在小样本学习中尤为重要。在我们的工作中，我们使用HASOC数据对德语和印地语进行了几个示范学习，并将Multilingual HateCheck（MHC）作为基准。我们展示了根据我们的词汇表选择样本，相对于随机采样的模型，能够更好地在MHC上表现。因此，当仅有少量的训练样本时，使用我们的词汇表来选择包含更多社会文化信息的样本能够更好地提高在仇恨言论检测任务中的性能。

    We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
    
[^143]: 神经网络熵(NNetEn)：基于熵特征的脑电信号和混沌时间序列分离，用于NNetEn计算的Python包

    Neural Network Entropy (NNetEn): EEG Signals and Chaotic Time Series Separation by Entropy Features, Python Package for NNetEn Calculation. (arXiv:2303.17995v1 [cs.LG])

    [http://arxiv.org/abs/2303.17995](http://arxiv.org/abs/2303.17995)

    该研究提出了一种新的熵估计方法NNetEn，用于有效地分离时间序列系统的混沌动态，并在分离混沌时间序列方面证明了其高效率。

    

    熵测量是时间序列分类问题中有效的特征。传统的熵测量方法，例如香农熵，使用概率分布函数。然而，为了有效地分离时间序列，需要新的熵估计方法来表征系统的混沌动态。我们的神经网络熵(NNetEn)概念是基于特殊数据集(MNIST-10和SARS-CoV-2-RBV1)的分类，这些数据集与记录在LogNNet神经网络储层中的时间序列熵相关。NNetEn以原始方式估计时间序列的混沌动态。基于NNetEn算法，我们提出了两个新的分类度量：R2效率和皮尔逊效率。NNetEn的效率在使用离散分析(ANOVA)分离正弦映射的两个混沌时间序列方面得到验证。对于两个接近的动态时间序列 (r=1.1918和r=1.2243)，F比值达到了124的值，反映了高效率。

    Entropy measures are effective features for time series classification problems. Traditional entropy measures, such as Shannon entropy, use probability distribution function. However, for the effective separation of time series, new entropy estimation methods are required to characterize the chaotic dynamic of the system. Our concept of Neural Network Entropy (NNetEn) is based on the classification of special datasets (MNIST-10 and SARS-CoV-2-RBV1) in relation to the entropy of the time series recorded in the reservoir of the LogNNet neural network. NNetEn estimates the chaotic dynamics of time series in an original way. Based on the NNetEn algorithm, we propose two new classification metrics: R2 Efficiency and Pearson Efficiency. The efficiency of NNetEn is verified on separation of two chaotic time series of sine mapping using dispersion analysis (ANOVA). For two close dynamic time series (r = 1.1918 and r = 1.2243), the F-ratio has reached the value of 124 and reflects high efficien
    
[^144]: 多项式分类中的稀疏联合偏移

    Sparse joint shift in multinomial classification. (arXiv:2303.16971v1 [stat.ML])

    [http://arxiv.org/abs/2303.16971](http://arxiv.org/abs/2303.16971)

    该论文提出了一种稀疏联合偏移模型，用于解决整体数据集偏移问题，提供了传递SJS、修正类后验概率、SJS的可辨认性、SJS与协变量转移关系等新结果。

    

    稀疏联合偏移（SJS）是一种针对数据集整体偏移的可处理模型，可能会导致特征和标签的边际分布以及后验概率和类条件特征分布的变化。在没有标签观测的情况下，为目标数据集拟合SJS可能会产生标签的有效预测和类先验概率的估计。我们在特征集之间传递SJS方面提供了新的结果，提出了一个基于目标分布的类后验概率的条件修正公式，确定性SJS的可辨认性以及SJS和协变量转移之间的关系。此外，我们指出了用于估计SJS特征的算法中的不一致性，因为它们可能会妨碍寻找最优解。

    Sparse joint shift (SJS) was recently proposed as a tractable model for general dataset shift which may cause changes to the marginal distributions of features and labels as well as the posterior probabilities and the class-conditional feature distributions. Fitting SJS for a target dataset without label observations may produce valid predictions of labels and estimates of class prior probabilities. We present new results on the transmission of SJS from sets of features to larger sets of features, a conditional correction formula for the class posterior probabilities under the target distribution, identifiability of SJS, and the relationship between SJS and covariate shift. In addition, we point out inconsistencies in the algorithms which were proposed for estimating the characteristics of SJS, as they could hamper the search for optimal solutions.
    
[^145]: 基于列表的在线分类

    List Online Classification. (arXiv:2303.15383v1 [cs.LG])

    [http://arxiv.org/abs/2303.15383](http://arxiv.org/abs/2303.15383)

    本文研究了多标签列表的在线预测问题，提出了 $b$-ary Littlestone 维度可学习模型，并且在懵懂的情况下探索不同的情况。可以使用改编自 Littlestone 的 SOA 和 Rosenblatt 的感知器等算法进行预测，同时还建立了列表可学习的组合结果。

    

    我们研究多分类在线预测，其中学习者可以使用多个标签的列表进行预测（与传统设置中仅使用一种标签不同）。我们使用 $b$-ary Littlestone 维度表征了该模型中的可学习性。该维度是经典 Littlestone 维度的变体，其中二进制错误树被替换为 $(k+1)$-ary 错误树，其中 k 是列表中标签的数量。在懵懂的场景中，我们根据比较类中是否包含单标签或多标签函数以及它与算法使用的列表大小之间的权衡来探索不同的情况。我们发现在某些情况下可以实现负悔，同时提供了什么情况下实现负悔的完整特性化。作为我们工作的一部分，我们改编了经典算法，如 Littlestone 的 SOA 和 Rosenblatt 的感知器，以使用标签列表进行预测。我们还为可以进行列表学习的组合结果建立了基础。

    We study multiclass online prediction where the learner can predict using a list of multiple labels (as opposed to just one label in the traditional setting). We characterize learnability in this model using the $b$-ary Littlestone dimension. This dimension is a variation of the classical Littlestone dimension with the difference that binary mistake trees are replaced with $(k+1)$-ary mistake trees, where $k$ is the number of labels in the list. In the agnostic setting, we explore different scenarios depending on whether the comparator class consists of single-labeled or multi-labeled functions and its tradeoff with the size of the lists the algorithm uses. We find that it is possible to achieve negative regret in some cases and provide a complete characterization of when this is possible. As part of our work, we adapt classical algorithms such as Littlestone's SOA and Rosenblatt's Perceptron to predict using lists of labels. We also establish combinatorial results for list-learnable c
    
[^146]: 利用多时间 Hamilton-Jacobi PDE 解决一些科学机器学习问题

    Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific Machine Learning Problems. (arXiv:2303.12928v1 [cs.LG])

    [http://arxiv.org/abs/2303.12928](http://arxiv.org/abs/2303.12928)

    本文从理论上将特定优化问题与多时间Hamilton-Jacobi PDEs联系起来，表明当解决这些问题时，同时解决了对应的多时间HJ PDEs和最优控制问题。利用这种联系，提出了一种新的算法，实现了深度神经网络泛化性能的改进。

    

    Hamilton-Jacobi 偏微分方程(HJ PDEs)与广泛领域，如最优控制、微分游戏和成像科学有着深刻的联系。通过将时间变量视为更高维的量，HJ PDEs 可以扩展到多时间的情况。本文在特定机器学习优化问题与多时间Hopf公式之间建立了一种新的理论联系，该公式对应于某些多时间 HJ PDEs 的解的表示。通过这种联系，我们通过展示当我们解决这些学习问题时，我们也解决了一个多时间 HJ PDE 和相应的最优控制问题，从而增加了某些机器学习应用程序的训练过程的可解释性。作为这种联系的第一个探索，我们发展了正则化线性回归问题与线性二次调节器 (LQR) 之间的关系。然后，我们利用我们的理论框架设计了一种新的深度神经网络训练算法，实现了改进的泛化性能。

    Hamilton-Jacobi partial differential equations (HJ PDEs) have deep connections with a wide range of fields, including optimal control, differential games, and imaging sciences. By considering the time variable to be a higher dimensional quantity, HJ PDEs can be extended to the multi-time case. In this paper, we establish a novel theoretical connection between specific optimization problems arising in machine learning and the multi-time Hopf formula, which corresponds to a representation of the solution to certain multi-time HJ PDEs. Through this connection, we increase the interpretability of the training process of certain machine learning applications by showing that when we solve these learning problems, we also solve a multi-time HJ PDE and, by extension, its corresponding optimal control problem. As a first exploration of this connection, we develop the relation between the regularized linear regression problem and the Linear Quadratic Regulator (LQR). We then leverage our theoret
    
[^147]: 通过 Numerai 数据科学竞赛案例，理解时间表格和多变量时间序列的模型复杂度

    Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])

    [http://arxiv.org/abs/2303.07925](http://arxiv.org/abs/2303.07925)

    本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。

    

    本文探究了在多变量时间序列建模中使用不同特征工程和降维方法的应用。利用从 Numerai 数据竞赛创建的特征目标交叉相关时间序列数据集，我们证明在过度参数化的情况下，不同特征工程方法的性能与预测会收敛到可由再生核希尔伯特空间刻画的相同平衡态。我们提出了一种新的集成方法，该方法结合了不同的随机非线性变换，随后采用岭回归模型进行高维时间序列建模。与一些常用的用于序列建模的深度学习模型（如 LSTM 和 transformer）相比，我们的方法更加鲁棒（在不同的随机种子下具有较低的模型方差，且对架构的选择不太敏感），并且更有效率。我们方法的另一个优势在于模型的简单性，因为没有必要使用复杂的深度学习框架。

    In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
    
[^148]: 合并决策Transformer：多任务策略形成的权重平均化

    Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies. (arXiv:2303.07551v1 [cs.LG])

    [http://arxiv.org/abs/2303.07551](http://arxiv.org/abs/2303.07551)

    本文提出通过在权重空间中合并训练于不同 MuJoCo 运动问题上的 Decision Transformer 的子集，形成多任务模型。通过共享一些辅助任务的训练以及共同使用预训练初始化，能够获得更好的结果。这个方向的研究有助于使代理的过程民主化和分发。

    

    最近的研究展示了基于Transformer的通用语言、视觉和连续决策制定问题的策略的前景。为了创建这样的模型，我们通常需要集中的训练目标、数据和计算。如果我们能够更灵活地创建通用策略，通过合并多个任务特定的、单独训练的策略，则这样做就比较有意义。在本文中，我们通过在权重空间中合并或平均不同MuJoCo运动问题上训练的Decision Transformer的子集来迈出这个方向的初步步骤，形成没有集中训练的多任务模型。我们还建议在合并策略时可以获得更好的结果，如果所有策略都从共同的预训练初始化开始，并在问题特定的微调期间共同训练共享的辅助任务。一般来说，我们相信这个方向的研究可以帮助民主化和分发具有一般能力的代理的过程。

    Recent work has shown the promise of creating generalist, transformer-based, policies for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies, by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in weight space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also propose that when merging policies, we can obtain better results if all policies start from common, pre-trained initializations, while also co-training on shared auxiliary tasks during problem-specific finetuning. In general, we believe research in this direction can help democratize and distribute the process of which forms generally capable agents.
    
[^149]: 孟加拉达卡市基于医院的横断面研究：非传染性疾病的患病率及主要风险因素。

    Prevalence and major risk factors of non-communicable diseases: A Hospital-based Cross-Sectional Study in Dhaka, Bangladesh. (arXiv:2303.04808v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2303.04808](http://arxiv.org/abs/2303.04808)

    该研究调查了孟加拉国达卡市成年患者中多种非传染性疾病的患病率及其风险因素，其中心血管疾病最为普遍。男性参与者较女性更容易患有心血管疾病，但糖尿病不具有性别倾向。CVD和DM都会随着年龄的增长而增加。患有肥胖症的住院病人占五分之一。

    

    该研究旨在确定孟加拉国达卡市寻求营养指导的成年患者中多种非传染性疾病（NCD）的患病率，分析其风险因素。结果显示，性别、年龄组、肥胖与NCD（糖尿病、CKD、IBS、心血管疾病、慢性肾脏疾病、甲状腺疾病）之间有关联。NCD中最常见的是心血管问题（CVD），在所有参与者中占83.56%。CVD在男性参与者中更为普遍。相应地，男性参与者的血压分布比女性更高。另一方面，糖尿病并没有性别倾向。无论CVD还是DM，都具有年龄上的进展。慢性呼吸系统疾病在中年参与者中比年轻或老年人更为常见。基于数据，五分之一的住院患者患有肥胖症。我们对合并症进行了分析，发现31.5%的人口仅患有一种NCD，30.1%的人患有两种或两种以上的NCD。

    Objective: The study aimed to determine the prevalence of several non-communicable diseases (NCD) and analyze risk factors among adult patients seeking nutritional guidance in Dhaka, Bangladesh. Result: Our study observed the relationships between gender, age groups, obesity, and NCDs (DM, CKD, IBS, CVD, CRD, thyroid). The most frequently reported NCD was cardiovascular issues (CVD), which was present in 83.56% of all participants. CVD was more common in male participants. Consequently, male participants had a higher blood pressure distribution than females. Diabetes mellitus (DM), on the other hand, did not have a gender-based inclination. Both CVD and DM had an age-based progression. Our study showed that chronic respiratory illness was more frequent in middle-aged participants than in younger or elderly individuals. Based on the data, every one in five hospitalized patients was obese. We analyzed the co-morbidities and found that 31.5% of the population has only one NCD, 30.1% has t
    
[^150]: 论文标题：认证鲁棒神经网络：泛化和抗污染性

    Certified Robust Neural Networks: Generalization and Corruption Resistance. (arXiv:2303.02251v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.02251](http://arxiv.org/abs/2303.02251)

    该论文提出了一种新颖的分布鲁棒损失函数，该函数通过认证级别的鲁棒性对两种常见的污染类型进行抵抗，并确保泛化保证，从而解决了鲁棒性和泛化之间的矛盾，具有极高的实用性。

    

    最近的研究表明，鲁棒性（对“污染”的抵抗能力）可能与泛化存在矛盾。例如，对抗性训练旨在减少现代神经网络对小数据扰动的敏感性。令人惊讶的是，在对抗训练中，过拟合是一个主要问题，尽管在标准训练中几乎不存在。我们在这里提供了关于这种奇特的“鲁棒过拟合”现象的理论证据。随后，我们提出了一种新颖的分布鲁棒损失函数，将鲁棒性和泛化相结合。我们理论上和实证地证明了该损失具有认证级别的鲁棒性，可以抵抗两种常见的污染类型——数据逃避和攻击——同时确保泛化保证。通过精心设计的数字实验，我们展示了所得到的完整鲁棒（HR）训练程序具有SOTA的性能。最后，我们指出HR训练可以被解释为对抗性训练的直接扩展，并可以自然地应用于GAN和RL。

    Recent work have demonstrated that robustness (to "corruption") can be at odds with generalization. Adversarial training, for instance, aims to reduce the problematic susceptibility of modern neural networks to small data perturbations. Surprisingly, overfitting is a major concern in adversarial training despite being mostly absent in standard training. We provide here theoretical evidence for this peculiar "robust overfitting" phenomenon. Subsequently, we advance a novel distributionally robust loss function bridging robustness and generalization. We demonstrate both theoretically as well as empirically the loss to enjoy a certified level of robustness against two common types of corruption--data evasion and poisoning attacks--while ensuring guaranteed generalization. We show through careful numerical experiments that our resulting holistic robust (HR) training procedure yields SOTA performance. Finally, we indicate that HR training can be interpreted as a direct extension of adversar
    
[^151]: Soft Actor-Critic算法的收敛点

    The Point to Which Soft Actor-Critic Converges. (arXiv:2303.01240v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01240](http://arxiv.org/abs/2303.01240)

    本文证明了在极限情况下，Soft Actor-Critic算法和Soft Q-learning算法在最大熵框架下收敛于同一解，这一结论对优化算法具有较大的意义。

    

    Soft Actor-Critic是Soft Q-learning的成功后继者，尽管它们都处于最大熵框架下，但它们之间的关系仍不清楚。本文证明了它们在极限情况下收敛于相同的解，这一结果非常有吸引力，因为它将优化从困难的方式转化为了简单的方式。同样的证明也适用于其他正则项，例如KL散度。

    Soft actor-critic is a successful successor over soft Q-learning. While lived under maximum entropy framework, their relationship is still unclear. In this paper, we prove that in the limit they converge to the same solution. This is appealing since it translates the optimization from an arduous to an easier way. The same justification can also be applied to other regularizers such as KL divergence.
    
[^152]: 随机逼近法用于组分布式鲁棒优化

    Stochastic Approximation Approaches to Group Distributionally Robust Optimization. (arXiv:2302.09267v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09267](http://arxiv.org/abs/2302.09267)

    本文提出了一种随机逼近法，用于组分布式鲁棒优化，该算法利用在线学习技术，将每轮所需的样本数从$m$个降至$1$个，同时保持相同的样本复杂度。

    

    本文研究组分布式鲁棒优化（GDRO），目的是学习一个能在$m$个不同分布上表现良好的模型。首先，我们将GDRO建模为随机凸凹鞍点问题，并证明使用$m$个样本的随机镜像下降法(SMD)，能够实现$O(m(\log m)/\epsilon ^2)$个样本的复杂度，以找到一个$\epsilon$-最优解，这与$\Omega(m/\epsilon ^2)$的下界想匹配，除了一个对数因子。接下来，我们利用在线学习技术，将每轮所需的样本数从$m$个降至$1$个，同时保持相同的样本复杂度。具体而言，我们将GDRO构造为一个双人博弈，其中一个玩家简单地执行SMD，另一个执行一种用于非明显多臂老虎机的在线算法。接下来，我们考虑一个更实际的情况，即可以从每个分布中绘制的样本数量不同，并提出一种新的公式。

    This paper investigates group distributionally robust optimization (GDRO), with the purpose to learn a model that performs well over $m$ different distributions. First, we formulate GDRO as a stochastic convex-concave saddle-point problem, and demonstrate that stochastic mirror descent (SMD), using $m$ samples in each iteration, achieves an $O(m (\log m)/\epsilon^2)$ sample complexity for finding an $\epsilon$-optimal solution, which matches the $\Omega(m/\epsilon^2)$ lower bound up to a logarithmic factor. Then, we make use of techniques from online learning to reduce the number of samples required in each round from $m$ to $1$, keeping the same sample complexity. Specifically, we cast GDRO as a two-players game where one player simply performs SMD and the other executes an online algorithm for non-oblivious multi-armed bandits. Next, we consider a more practical scenario where the number of samples that can be drawn from each distribution is different, and propose a novel formulation
    
[^153]: 利用深度增强决策树进行高效欺诈检测

    Efficient Fraud Detection Using Deep Boosting Decision Trees. (arXiv:2302.05918v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.05918](http://arxiv.org/abs/2302.05918)

    本文提出了一种利用深度增强决策树的方法来进行欺诈检测，既可以利用决策树的可解释性，又可以提高神经网络的表示学习能力。同时，采用新的过采样策略来缓解数据不平衡的问题。

    

    欺诈检测是识别、监控和预防复杂数据中的潜在欺诈活动。机器学习提供了一种新的数据驱动方法来处理欺诈行为。本文提出了一种基于梯度增强和神经网络的深度增强决策树（DBDT）方法，它首先构建了软决策树，然后将其嵌入神经网络框架用于最终分类，既可以利用决策树的可解释性，又可以提高神经网络的表示学习能力。此外，我们提出一种新的过采样策略来缓解数据不平衡带来的影响。实验结果表明，我们的方法在准确性、可解释性和效率方面优于现有的最先进方法。

    Fraud detection is to identify, monitor, and prevent potentially fraudulent activities from complex data. The recent development and success in AI, especially machine learning, provides a new data-driven way to deal with fraud. From a methodological point of view, machine learning based fraud detection can be divided into two categories, i.e., conventional methods (decision tree, boosting...) and deep learning, both of which have significant limitations in terms of the lack of representation learning ability for the former and interpretability for the latter. Furthermore, due to the rarity of detected fraud cases, the associated data is usually imbalanced, which seriously degrades the performance of classification algorithms. In this paper, we propose deep boosting decision trees (DBDT), a novel approach for fraud detection based on gradient boosting and neural networks. In order to combine the advantages of both conventional methods and deep learning, we first construct soft decision 
    
[^154]: 探索基于数值先验的广义CP分解低秩张量补全算法

    Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05881](http://arxiv.org/abs/2302.05881)

    本文提出了一种新的方法框架GCDTC，利用数值先验和广义CP分解实现了更高的低秩张量补全精度；同时介绍了一个算法SPTC，作为该框架的一个实现。在实验中，该方法表现出比现有技术更好的性能。

    

    张量补全在计算机视觉、数据分析和信号处理等领域中具有重要意义。最近，低秩张量补全这一类别的方法得到了广泛研究，对补全张量施加低秩结构。虽然这些方法取得了巨大成功，但尚未考虑到张量元素的数值先验信息。忽略数值先验将导致丢失关于数据的重要信息，因此阻止算法达到最优精度。本研究试图构建一个新的方法框架，名为GCDTC（广义CP分解张量补全），以利用数值先验并实现更高的张量补全精度。在这个新引入的框架中，将广义的CP分解应用于低秩张量补全。本文还提出了一种名为SPTC（平滑泊松张量补全）的算法，用于非负整数张量补全，作为GCDTC框架的一个实现。通过对合成和真实世界数据集的大量实验，证明所提出的方法相比于现有技术具有更优的张量补全性能。

    Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
    
[^155]: 历史依赖动态环境下的强化学习

    Reinforcement Learning with History-Dependent Dynamic Contexts. (arXiv:2302.02061v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02061](http://arxiv.org/abs/2302.02061)

    介绍了一种称为DCMDPs的新型强化学习框架，用于处理依赖历史环境的情况。其中的逻辑DCMDPs通过利用聚合函数确定上下文转换，打破了对历史长度的指数依赖，并引入了一种实用的基于模型的算法。在推荐任务中展示了该方法的有效性。

    

    我们引入了动态上下文马尔可夫决策过程（DCMDPs），这是一种新的强化学习框架，用于处理依赖历史环境的情况。它推广了上下文MDP框架，以处理非马尔可夫环境，其中上下文随时间变化。我们考虑了这个模型的特殊情况，着重于逻辑DCMDPs，它通过利用聚合函数确定上下文转换来打破对历史长度的指数依赖。这种特殊结构使我们能够推导出一种类似于上限置信界算法的算法，并建立了遗憾界。受我们的理论结果的启发，我们引入了一种实用的基于模型的算法，用于逻辑DCMDPs，这个算法在一个潜在空间中进行规划，并使用历史依赖特征上的乐观主义。我们在一个推荐任务上展示了我们方法的有效性（使用MovieLens数据集），其中用户行为动态地随着推荐的变化而演变。

    We introduce Dynamic Contextual Markov Decision Processes (DCMDPs), a novel reinforcement learning framework for history-dependent environments that generalizes the contextual MDP framework to handle non-Markov environments, where contexts change over time. We consider special cases of the model, with a focus on logistic DCMDPs, which break the exponential dependence on history length by leveraging aggregation functions to determine context transitions. This special structure allows us to derive an upper-confidence-bound style algorithm for which we establish regret bounds. Motivated by our theoretical results, we introduce a practical model-based algorithm for logistic DCMDPs that plans in a latent space and uses optimism over history-dependent features. We demonstrate the efficacy of our approach on a recommendation task (using MovieLens data) where user behavior dynamics evolve in response to recommendations.
    
[^156]: 学习函数转导

    Learning Functional Transduction. (arXiv:2302.00328v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00328](http://arxiv.org/abs/2302.00328)

    提出了一种利用元学习方法和向量值再生核Banach空间理论形成高效的上下文神经逼近器的混合方法，将其应用于函数空间，可以在给定少量输入和输出示例的情况下帮助快速捕捉到无穷多的功能关系。

    

    机器学习中的研究分为两种方法来进行回归任务：转导方法直接从可用的数据中构建估计，但通常问题不具体。归纳方法可以更具体，但通常需要计算密集的解决方案搜索。在这项工作中，我们提出了一种混合方法，并展示了通过梯度下降元学习转导回归原则，通过利用向量值再生核Banach空间（RKBS）理论形成高效的上下文神经逼近器，我们将此方法应用于有限和无限维空间（函数值算子）上的函数空间，并展示了一旦训练完成，转导器几乎可以即时地捕捉到无穷多的功能关系，给定少量输入和输出示例，并返回新的图像估计。我们演示了我们元学的转导方法对于模拟复杂物理系统的模型的好处。

    Research in machine learning has polarized into two general approaches for regression tasks: Transductive methods construct estimates directly from available data but are usually problem unspecific. Inductive methods can be much more specific but generally require compute-intensive solution searches. In this work, we propose a hybrid approach and show that transductive regression principles can be meta-learned through gradient descent to form efficient in-context neural approximators by leveraging the theory of vector-valued Reproducing Kernel Banach Spaces (RKBS). We apply this approach to function spaces defined over finite and infinite-dimensional spaces (function-valued operators) and show that once trained, the Transducer can almost instantaneously capture an infinity of functional relationships given a few pairs of input and output examples and return new image estimates. We demonstrate the benefit of our meta-learned transductive approach to model complex physical systems influe
    
[^157]: 单环路交替次梯度法求解非光滑弱凸函数约束优化问题的Oracle复杂度分析

    Oracle Complexity of Single-Loop Switching Subgradient Methods for Non-Smooth Weakly Convex Functional Constrained Optimization. (arXiv:2301.13314v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.13314](http://arxiv.org/abs/2301.13314)

    本文分析了一种交替次梯度法用于非凸优化问题的Oracle复杂度，其中约束函数为凸或弱凸，在只使用单环路的情况下取得了相同的复杂度，并可应用于非光滑问题。

    

    本文研究了一类弱凸且约束为凸或弱凸的非凸约束优化问题。本文考虑了经典的交替次梯度法求解此类问题，它是一种直观易实现的一阶方法，但目前只有在凸优化问题中已知其Oracle复杂性分析。本文提出了第一项针对非凸问题的交替次梯度法Oracle复杂度分析，针对的问题是求得几乎最优解。本文分别对约束为凸和弱凸的情形进行讨论。与现有的双环路方法相比，交替次梯度法可应用于非光滑问题，仅使用单环路即可实现相同的复杂度，从而节省了内部迭代次数的调整。

    We consider a non-convex constrained optimization problem, where the objective function is weakly convex and the constraint function is either convex or weakly convex. To solve this problem, we consider the classical switching subgradient method, which is an intuitive and easily implementable first-order method whose oracle complexity was only known for convex problems. This paper provides the first analysis on the oracle complexity of the switching subgradient method for finding a nearly stationary point of non-convex problems. Our results are derived separately for convex and weakly convex constraints. Compared to existing approaches, especially the double-loop methods, the switching gradient method can be applied to non-smooth problems and achieves the same complexity using only a single loop, which saves the effort on tuning the number of inner iterations.
    
[^158]: MILO: 模型无关子集选择框架，用于高效模型训练和调优。

    MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning. (arXiv:2301.13287v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13287](http://arxiv.org/abs/2301.13287)

    提出了一个模型无关子集选择框架MILO，将子集选择与模型训练分离，通过易到难的课程实现了卓越的模型收敛和性能。

    

    训练深度网络和调优大型数据集的超参数是计算密集型的。减少训练成本的主要研究方向之一是通过选择很好的训练数据子集来实现。与简单的自适应随机子集选择基准相比，现有的智能子集选择方法由于耗时的子集选择步骤而不具竞争力，该步骤涉及计算依赖于模型的梯度和特征嵌入，并应用子模块目标的贪心最大化。我们的关键洞察是消除对下游模型参数的依赖，将子集选择作为预处理步骤，并使其能够在不增加成本的情况下训练多个模型。在这个工作中，我们提出了 MILO，一个模型无关的子集选择框架，它将子集选择与模型训练分离，同时通过使用一个易到难的课程实现了卓越的模型收敛和性能。通过实验结果验证了我们的方法。

    Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results in
    
[^159]: 通过多任务语言建模统一分子和文本表示

    Unifying Molecular and Textual Representations via Multi-task Language Modelling. (arXiv:2301.12586v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12586](http://arxiv.org/abs/2301.12586)

    本文提出了第一个多任务语言模型，可以同时解决化学和自然语言领域的各种任务，无需昂贵的单一域或任务特定模型的预训练。

    

    神经语言模型的最新进展已成功应用于化学领域，通过为分子设计和合成规划提供生成式解决方案。这些新方法有潜力推动数据驱动的科学发现的新时代。然而，每个任务仍然需要专门的模型，导致需要特定问题的微调，并忽视任务之间的关系。该领域的主要障碍是自然语言和化学表示之间缺乏统一表示，从而使人机交互变得复杂和有限。本文提出了第一个多域、多任务语言模型，可以同时解决化学和自然语言领域的各种任务。我们的模型可以同时处理化学和自然语言，无需昂贵的单一域或任务特定模型的预训练。有趣的是，在领域之间共享权重会在所有任务上产生更好的性能。我们的实验表明，我们的模型可以成功解决各种任务，包括分子生成、反合成预测、化学命名实体识别和文本摘要。

    The recent advances in neural language models have also been successfully applied to the field of chemistry, offering generative solutions for classical problems in molecular design and synthesis planning. These new methods have the potential to fuel a new era of data-driven automation in scientific discovery. However, specialized models are still typically required for each task, leading to the need for problem-specific fine-tuning and neglecting task interrelations. The main obstacle in this field is the lack of a unified representation between natural language and chemical representations, complicating and limiting human-machine interaction. Here, we propose the first multi-domain, multi-task language model that can solve a wide range of tasks in both the chemical and natural language domains. Our model can handle chemical and natural language concurrently, without requiring expensive pre-training on single domains or task-specific models. Interestingly, sharing weights across domai
    
[^160]: ALIM: 为噪声局部标签学习调整标签重要性机制

    ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning. (arXiv:2301.12077v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.12077](http://arxiv.org/abs/2301.12077)

    提出一种噪声局部标签学习的框架ALIM，通过权衡候选集和模型输出来减少检测误差的负面影响，可以与现有PLL方法集成，并在基准数据集上达到最先进性能。

    

    噪声局部标签学习（noisy PLL）是弱监督学习的一个重要分支。与要求地面真实标签藏在候选标签集中的PLL不同，噪声PLL放松了这个限制，并允许地面真实标签可能不在候选标签集中。为了解决这个具有挑战性的问题，大部分现有的工作试图检测噪声样本并为每个噪声样本估计地面真实标签。然而，检测误差是不可避免的。这些错误可以在训练期间积累，并持续影响模型优化。为此，我们提出了一种新的具有理论保证的噪声PLL框架，称为“调整标签重要性机制（ALIM）”。它旨在通过权衡初始候选集和模型输出来减少检测错误的负面影响。ALIM是一种可与现有PLL方法集成的插件策略。在基准数据集上的实验结果表明，我们的方法可以在噪声PLL任务中达到最先进的性能。

    Noisy partial label learning (noisy PLL) is an important branch of weakly supervised learning. Unlike PLL where the ground-truth label must conceal in the candidate label set, noisy PLL relaxes this constraint and allows the ground-truth label may not be in the candidate label set. To address this challenging problem, most of the existing works attempt to detect noisy samples and estimate the ground-truth label for each noisy sample. However, detection errors are unavoidable. These errors can accumulate during training and continuously affect model optimization. To this end, we propose a novel framework for noisy PLL with theoretical guarantees, called ``Adjusting Label Importance Mechanism (ALIM)''. It aims to reduce the negative impact of detection errors by trading off the initial candidate set and model outputs. ALIM is a plug-in strategy that can be integrated with existing PLL approaches. Experimental results on benchmark datasets demonstrate that our method can achieve state-of-
    
[^161]: 领域无关的分子生成与自我反馈

    Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11259](http://arxiv.org/abs/2301.11259)

    MolGen是一个专注于分子生成的预训练语言模型，使用了领域无关的分子前缀调整和自我反馈的范式，实现了化学有效性、多样性、新颖性和复杂性的突破，在分子生成领域表现出了出色的性能。

    

    分子的生成已经受到极大的关注，其革新了科学家设计分子结构的方式，并为化学和药物设计提供了宝贵的支持。然而，尽管在分子生成中使用语言模型具有潜力，但它们面临着许多挑战，比如生成语法或化学存在缺陷的分子，狭窄的领域专注以及由于缺乏注释数据或外部分子数据库而限制了生成多样性和可行性。因此，我们引入了MolGen，它是一个专门用于分子生成的预训练分子语言模型。MolGen通过重构一亿多个分子SELFIES获得了固有的结构和语法概念，并通过领域无关的分子前缀调整促进了不同领域之间的知识传递。此外，我们提出了一种自我反馈范式，启发预训练模型与最终下游目标对齐，有助于更稳健和高效的分子生成。我们在基准数据集上的实验表明，MolGen在化学有效性，多样性，新颖性和复杂性方面优于现有技术。

    The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
    
[^162]: 带增量个性化的联邦推荐

    Federated Recommendation with Additive Personalization. (arXiv:2301.09109v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09109](http://arxiv.org/abs/2301.09109)

    FedRAP是一种新的推荐系统生成方法，它使用联邦学习训练共享项嵌入和本地个性化视图，以捕捉用户对推荐项目感知的个体差异并且降低通信成本和延迟。

    

    基于联邦学习（FL）构建推荐系统是推动下一代互联网服务和隐私保护的新兴挑战。现有的方法通过FL训练共享项嵌入，同时在客户端保持用户嵌入私密性。然而，相同嵌入对所有客户端的项目不能捕捉到用户对同一项目感知的个体差异，因此导致个性化差。此外，FL中的密集项目嵌入导致通信成本和延迟昂贵。为解决这些挑战，我们提出了带增量个性化的联邦推荐（FedRAP），它通过FL学习项目的全局视图并在每个用户本地学习个性化视图。FedRAP通过正则化增加正则化权重来有效地学习局部和全局视图。我们提出了一种有效的课程表来逐渐学习本地和全局视图，并通过正则化促进两种视图之间的不同之处，使全局视图更稀疏以节省FL的通信成本。

    Building recommendation systems via federated learning (FL) is a new emerging challenge for advancing next-generation Internet service and privacy protection. Existing approaches train shared item embedding by FL while keeping the user embedding private on client side. However, item embedding identical for all clients cannot capture users' individual differences on perceiving the same item and thus leads to poor personalization. Moreover, dense item embedding in FL results in expensive communication cost and latency. To address these challenges, we propose Federated Recommendation with Additive Personalization (FedRAP), which learns a global view of items via FL and a personalized view locally on each user. FedRAP enforces sparsity of the global view to save FL's communication cost and encourages difference between the two views through regularization. We propose an effective curriculum to learn the local and global views progressively with increasing regularization weights. To produce
    
[^163]: 受太阳辐射力影响的变形航天器体态优化及其关节驱动姿态稳定

    Optimization of body configuration and joint-driven attitude stabilization for transformable spacecrafts under solar radiation pressure. (arXiv:2301.08435v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08435](http://arxiv.org/abs/2301.08435)

    本文针对太阳辐射下变形航天器的姿态控制技术进行了研究，提出了关节角度优化方法和通过关节驱动的动量阻尼控制，使变形航天器比以前的方法在轨道和姿态控制方面获得更好的性能。

    

    太阳帆是最有前途的太空探索系统之一，因其可以利用太阳辐射压力（SRP）理论上具有无限的比冲。近年来，一些研究人员提出了“可变形航天器”，可以通过驱动关节主动重新配置其机身。如果像太阳帆一样利用变形航天器，其控制自由度高的冗余度将极大增强轨道和姿态控制能力。但是，其大量的输入姿态使控制困难。因此，以前的研究人员施加了强制性的限制以限制其潜在的控制能力。本文针对太阳辐射下变形航天器的姿态控制技术进行了研究。作者提出了两种方法：一种是关节角度优化方法以获得任意的SRP力和力矩；另一种方法是通过关节驱动的动量阻尼控制。我们提出的方法可使变形航天器比以前的方法在轨道和姿态控制方面获得更好的性能。仿真结果说明了在三种不同的变形航天器中，所提出的方法的有效性。

    A solar sail is one of the most promising space exploration system because of its theoretically infinite specific impulse using solar radiation pressure (SRP). Recently, some researchers proposed "transformable spacecrafts" that can actively reconfigure their body configurations with actuatable joints. The transformable spacecrafts are expected to greatly enhance orbit and attitude control capability due to its high redundancy in control degree of freedom if they are used like solar sails. However, its large number of input poses difficulties in control, and therefore, previous researchers imposed strong constraints to limit its potential control capabilities. This paper addresses novel attitude control techniques for the transformable spacecrafts under SRP. The authors have constructed two proposed methods; one of those is a joint angle optimization to acquire arbitrary SRP force and torque, and the other is a momentum damping control driven by joint angle actuation. Our proposed meth
    
[^164]: Maxout网络的期望梯度及其对参数初始化的影响

    Expected Gradients of Maxout Networks and Consequences to Parameter Initialization. (arXiv:2301.06956v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.06956](http://arxiv.org/abs/2301.06956)

    本文研究了Maxout网络关于输入和参数的梯度，提出了避免梯度消失和爆炸的参数初始化策略，并在实验中证明了其有效性。

    

    本文研究了Maxout网络相对于输入和参数的梯度，并根据网络结构和参数分布得出梯度的矩上界。我们观察到，输入输出Jacobian的分布取决于输入，这使得稳定的参数初始化变得复杂。基于梯度矩，我们提出了避免在宽网络中梯度消失和爆炸的参数初始化策略。在深度全连接和卷积神经网络上的实验表明，这种策略改善了Maxout网络的SGD和Adam训练。此外，我们还得到了关于期望线性区域数量、期望曲线长度失真和NTK的精细界限结果。

    We study the gradients of a maxout network with respect to inputs and parameters and obtain bounds for the moments depending on the architecture and the parameter distribution. We observe that the distribution of the input-output Jacobian depends on the input, which complicates a stable parameter initialization. Based on the moments of the gradients, we formulate parameter initialization strategies that avoid vanishing and exploding gradients in wide networks. Experiments with deep fully-connected and convolutional networks show that this strategy improves SGD and Adam training of deep maxout networks. In addition, we obtain refined bounds on the expected number of linear regions, results on the expected curve length distortion, and results on the NTK.
    
[^165]: 具有可解释性的广义神经封闭模型

    Generalized Neural Closure Models with Interpretability. (arXiv:2301.06198v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.06198](http://arxiv.org/abs/2301.06198)

    本论文提出了一种新的方法，使用统一的神经偏微分方程来提高动力学模型的预测能力和计算成本，并解决了解释性和适用性方面的局限性。

    

    提高动力学模型的预测能力和计算成本通常是通过机器学习改进计算物理学的核心问题。然而，大多数学习结果在解释性和适用于不同的计算网格分辨率、初始和边界条件、领域几何和物理或问题特定参数方面存在局限性。在本研究中，我们通过开发统一的神经偏微分方程方法同时解决了所有这些挑战。我们直接在现有/低保真动力学模型的偏微分方程形式中加入了Markovian和非Markovian神经网络封闭参数化，从而实现了模型的拓展。Markovian项的设计旨在实现其分析能力。

    Improving the predictive capability and computational cost of dynamical models is often at the heart of augmenting computational physics with machine learning (ML). However, most learning results are limited in interpretability and generalization over different computational grid resolutions, initial and boundary conditions, domain geometries, and physical or problem-specific parameters. In the present study, we simultaneously address all these challenges by developing the novel and versatile methodology of unified neural partial delay differential equations. We augment existing/low-fidelity dynamical models directly in their partial differential equation (PDE) forms with both Markovian and non-Markovian neural network (NN) closure parameterizations. The melding of the existing models with NNs in the continuous spatiotemporal space followed by numerical discretization automatically allows for the desired generalizability. The Markovian term is designed to enable extraction of its analy
    
[^166]: 一种严格的不确定性感知量化框架对机器学习工作流之可重复再现性至关重要

    A Rigorous Uncertainty-Aware Quantification Framework Is Essential for Reproducible and Replicable Machine Learning Workflows. (arXiv:2301.05763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05763](http://arxiv.org/abs/2301.05763)

    这篇论文讨论了一个基于贝叶斯范式的不确定性量化框架，可以提供一个广泛而严格的方法来评估机器学习和人工智能在科学工作流程中的可重复性和可信度。

    

    机器学习或人工智能模型的预测和科学工作流程中包含的结果复现能力受多种因素影响。一种能够定量评估感兴趣量（QoI）复制性的不确定度感知度量标准，将有助于科学工作流程中应用机器学习和人工智能获得的结果具有更好的可信度。本文讨论了贝叶斯范式中的不确定性量化如何提供一种广泛和严格的框架，定量地评估复杂科学工作流程的可重复性。这样的框架将填补机器学习或人工智能在科学工作流程中的关键缺陷，因为它将使研究人员能够确定机器学习或人工智能模型预测变异对工作流程中预测的影响。我们期望这个框架将有助于设计更具重复性和可信度的工作流程。

    The ability to replicate predictions by machine learning (ML) or artificial intelligence (AI) models and results in scientific workflows that incorporate such ML/AI predictions is driven by numerous factors. An uncertainty-aware metric that can quantitatively assess the reproducibility of quantities of interest (QoI) would contribute to the trustworthiness of results obtained from scientific workflows involving ML/AI models. In this article, we discuss how uncertainty quantification (UQ) in a Bayesian paradigm can provide a general and rigorous framework for quantifying reproducibility for complex scientific workflows. Such as framework has the potential to fill a critical gap that currently exists in ML/AI for scientific workflows, as it will enable researchers to determine the impact of ML/AI model prediction variability on the predictive outcomes of ML/AI-powered workflows. We expect that the envisioned framework will contribute to the design of more reproducible and trustworthy wor
    
[^167]: 探索脉冲神经网络中的权衡

    Exploring Tradeoffs in Spiking Neural Networks. (arXiv:2212.09500v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2212.09500](http://arxiv.org/abs/2212.09500)

    本研究探讨了在使用Time-To-First-Spike（TTFS）约束时，性能、能耗、速度和稳定性之间的权衡，并提出了一种允许多个脉冲的松弛版本的方法来改善这些权衡。实验证明放宽脉冲约束可以提高SNN的性能、能耗和鲁棒性，同时仍保持合理的预测速度。

    

    脉冲神经网络（SNN）已成为低功耗计算的有前途的替代传统深度神经网络的方法。然而，SNN的效能不仅与性能有关，还与能耗、预测速度和对噪声的鲁棒性有关。最近的方法Fast＆Deep以及其他方法通过限制神经元最多只能发射一次脉冲而实现快速和节能计算。称为"Time-To-First-Spike（TTFS）"，但这种约束在许多方面限制了SNN的性能。在这项工作中，我们探讨了在使用此约束时性能、能耗、速度和稳定性之间的关系。更具体地，我们突出了权衡存在的情况，在付出稀疏度和预测延迟的代价下获得性能和稳健性。为了改善这些权衡，我们提出了Fast＆Deep的松弛版本，允许每个神经元发出多个脉冲。我们的实验表明，放松脉冲约束可以提高SNN的性能、能耗和鲁棒性，同时仍保持合理的预测速度。

    Spiking Neural Networks (SNNs) have emerged as a promising alternative to traditional Deep Neural Networks for low-power computing. However, the effectiveness of SNNs is not solely determined by their performance but also by their energy consumption, prediction speed, and robustness to noise. The recent method Fast \& Deep, along with others, achieves fast and energy-efficient computation by constraining neurons to fire at most once. Known as Time-To-First-Spike (TTFS), this constraint however restricts the capabilities of SNNs in many aspects. In this work, we explore the relationships between performance, energy consumption, speed and stability when using this constraint. More precisely, we highlight the existence of tradeoffs where performance and robustness are gained at the cost of sparsity and prediction latency. To improve these tradeoffs, we propose a relaxed version of Fast \& Deep that allows for multiple spikes per neuron. Our experiments show that relaxing the spike constra
    
[^168]: P2T2:一种深度神经网络物理引导的方法，用于从定量$T_2$加权MRI中鲁棒地估计$T_2$分布

    P2T2: a Physically-primed deep-neural-network approach for robust $T_{2}$ distribution estimation from quantitative $T_{2}$-weighted MRI. (arXiv:2212.04928v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2212.04928](http://arxiv.org/abs/2212.04928)

    本论文提出了一种物理引导的深度神经网络方法P2T2，可以鲁棒地从MRI数据中估计$T_2$分布，适用于临床实践和具有异构采集协议的大规模多机构试验。

    

    从多回波$T_2$加权MRI（$T_2W$）数据估计$T_2$弛豫时间分布可以为评估不同病理情况下的炎症、脱髓鞘、水肿和软骨组成提供有价值的生物标志物，包括神经退行性疾病，骨关节炎和肿瘤。已经提出了基于深度神经网络（DNN）的方法来解决从MRI数据估计$T_2$分布的复杂逆问题，但它们对低信噪比（SNR）的临床数据不够鲁棒，并且非常敏感于在采集过程中使用的回波时间（TE）的变化，因此它们在临床实践和具有异构采集协议的大规模多机构试验中的应用受到阻碍。我们提出了一种称为$P_2T_2$的物理引导的DNN方法，它将信号衰减正向模型与MRI信号合并到DNN结构中，以提高准确性和稳健性。

    Estimating $T_2$ relaxation time distributions from multi-echo $T_2$-weighted MRI ($T_2W$) data can provide valuable biomarkers for assessing inflammation, demyelination, edema, and cartilage composition in various pathologies, including neurodegenerative disorders, osteoarthritis, and tumors. Deep neural network (DNN) based methods have been proposed to address the complex inverse problem of estimating $T_2$ distributions from MRI data, but they are not yet robust enough for clinical data with low Signal-to-Noise ratio (SNR) and are highly sensitive to distribution shifts such as variations in echo-times (TE) used during acquisition. Consequently, their application is hindered in clinical practice and large-scale multi-institutional trials with heterogeneous acquisition protocols. We propose a physically-primed DNN approach, called $P_2T_2$, that incorporates the signal decay forward model in addition to the MRI signal into the DNN architecture to improve the accuracy and robustness o
    
[^169]: 通过分解行列式点过程使用多样的负样本来进行图卷积神经网络

    Graph Convolutional Neural Networks with Diverse Negative Samples via Decomposed Determinant Point Processes. (arXiv:2212.02055v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02055](http://arxiv.org/abs/2212.02055)

    本论文提出了基于DPP的方法来获取多样的负样本，在图卷积神经网络中实现图表示学习，通过提供不同的信息来更新节点的表示，从而提高了准确性。

    

    图卷积网络（GCNs）通过从节点和它们的拓扑中提取高级特征来实现图表示学习。本文采用了一种基于行列式点过程（DPP）的质量-多样性分解方法来获取多样的负样本，除边缘节点之外的所有节点都被考虑在内，这些节点通过提供不同的信息有利于表示更新。

    Graph convolutional networks (GCNs) have achieved great success in graph representation learning by extracting high-level features from nodes and their topology. Since GCNs generally follow a message-passing mechanism, each node aggregates information from its first-order neighbour to update its representation. As a result, the representations of nodes with edges between them should be positively correlated and thus can be considered positive samples. However, there are more non-neighbour nodes in the whole graph, which provide diverse and useful information for the representation update. Two non-adjacent nodes usually have different representations, which can be seen as negative samples. Besides the node representations, the structural information of the graph is also crucial for learning. In this paper, we used quality-diversity decomposition in determinant point processes (DPP) to obtain diverse negative samples. When defining a distribution on diverse subsets of all non-neighbourin
    
[^170]: 把推理能力压缩到更小的语言模型中

    Distilling Reasoning Capabilities into Smaller Language Models. (arXiv:2212.00193v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00193](http://arxiv.org/abs/2212.00193)

    本文提出了一种知识蒸馏方法，可以把大型语言模型的逐步推理能力蒸馏到更小的模型中，提出了一种替代推理方案，使用苏格拉底式CoT来训练两个小型蒸馏模型的组合，可以用来分解和解决复杂的问题，且在多个推理数据集上表现出高精度的复杂推理能力，经常优于那些没有经过CoT推理方法训练的大模型。

    

    逐步推理的方法（如CoT）在具有推理能力的大型语言模型中被证明非常有效。然而，CoT方法的成功基本上是与模型大小密切相关的，并且通常需要十亿级参数规模的模型才能使CoT工作。在本文中，我们提出了一种知识蒸馏方法，利用较大模型的逐步CoT推理能力，并将这些能力蒸馏到更小的模型中。在这项工作中，我们提出了一种替代推理方案：苏格拉底式CoT，它学习将原始问题分解为一系列子问题，并用它来指导中间推理步骤。我们使用苏格拉底式CoT来训练两个小型蒸馏模型的组合：问题分解器和子问题求解器。在实践中，给定一个新问题，这两个蒸馏模型以同步的方式工作，以分解和解决复杂的问题。在多个推理数据集（GSM8K，StrategyQA和SVAMP）上，我们展示了我们的蒸馏模型学会了高精度地执行复杂的推理任务，通常优于没有专门使用CoT推理方法进行训练的大型模型。

    Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models.  In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP
    
[^171]: 简单高效的基于聚类的精准医疗算法

    Simple and Scalable Algorithms for Cluster-Aware Precision Medicine. (arXiv:2211.16553v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16553](http://arxiv.org/abs/2211.16553)

    本文提出了基于聚类和嵌入的简单高效方法，用于克服精准医疗中的高维问题和聚类问题，经验证该方法较当前方法具有更高的有效性和可行性。

    

    利用人工智能实现数据驱动的个性化诊断、预后和治疗，为精准医疗带来了前所未有的变革。然而，生物医学数据的高维度和聚类结构使得在高维度、限制性观测的精准医疗领域中会遇到挑战。为了同时克服这两个问题，我们提出了一种简单高效的联合聚类和嵌入方法，将标准嵌入方法与凸聚类惩罚以模块化的方式结合。这种新颖的基于聚类的嵌入方法克服了当前联合嵌入和聚类方法的复杂性和局限性，我们通过层次聚类主成分分析（PCA）、局部线性嵌入（LLE）和规范相关分析（CCA）的简单实现进行了证明。通过数值实验和真实世界的案例，我们证明了我们方法的有效性。

    AI-enabled precision medicine promises a transformational improvement in healthcare outcomes by enabling data-driven personalized diagnosis, prognosis, and treatment. However, the well-known "curse of dimensionality" and the clustered structure of biomedical data together interact to present a joint challenge in the high dimensional, limited observation precision medicine regime. To overcome both issues simultaneously we propose a simple and scalable approach to joint clustering and embedding that combines standard embedding methods with a convex clustering penalty in a modular way. This novel, cluster-aware embedding approach overcomes the complexity and limitations of current joint embedding and clustering methods, which we show with straightforward implementations of hierarchically clustered principal component analysis (PCA), locally linear embedding (LLE), and canonical correlation analysis (CCA). Through both numerical experiments and real-world examples, we demonstrate that our 
    
[^172]: 因果图中前门调整的线性时间算法

    Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs. (arXiv:2211.16468v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.16468](http://arxiv.org/abs/2211.16468)

    在因果图中，提出了解决前门调整的线性时间算法，通过观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。

    

    从观测数据中估计因果效应是实证科学中的基本任务。当系统中涉及未观察到的混淆因素时，这变得尤为具有挑战性。本文侧重于前门调整——一种经典技术，它使用观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。虽然前门估计的统计特性已经很好地理解了，但它的算法方面长期以来一直未得到探究。最近，Jeong，Tian和Barenboim [NeurIPS 2022]提出了一种第一个多项式时间算法，用于在给定的有向无环图（DAG）中找到满足前门准则的集合，其运行时间为$O（n^3（n+m））$，其中$n$表示变量的数量，$m$表示因果图的边的数量。在我们的工作中，我们提供了第一个具有线性时间复杂度的算法，即$O（n+m）$，用于这项任务，从而达到了渐近最优的时间复杂性。

    Causal effect estimation from observational data is a fundamental task in empirical sciences. It becomes particularly challenging when unobserved confounders are involved in a system. This paper focuses on front-door adjustment -- a classic technique which, using observed mediators allows to identify causal effects even in the presence of unobserved confounding. While the statistical properties of the front-door estimation are quite well understood, its algorithmic aspects remained unexplored for a long time. Recently, Jeong, Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time algorithm for finding sets satisfying the front-door criterion in a given directed acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and $m$ the number of edges of the causal graph. In our work, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task, which thus reaches the asymptotically optimal time complexity. This result impli
    
[^173]: 什么是上下文学习算法？使用线性模型进行调查

    What learning algorithm is in-context learning? Investigations with linear models. (arXiv:2211.15661v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15661](http://arxiv.org/abs/2211.15661)

    研究者提出了一种假设，即基于转换器的上下文学习器可以隐含地编码学习算法，并根据上下文中出现的新示例更新这些隐式模型。通过构造和比较性质证明了这个假设，并提出了一种度量学习器预测器和从显式学习算法中获得的预测器之间相似程度的方法。

    

    神经序列模型，特别是转换器，展现了一种非凡的上下文学习能力。它们可以在输入中呈现的标记示例序列$(x,f(x))$构建新的预测器，而无需进一步的参数更新。我们调查假设：基于转换器的上下文学习器通过在其激活中编码较小的模型并根据上下文中出现的新示例更新这些隐式模型，实现了标准的学习算法。以线性回归作为原型问题，我们提供了三条这个假设的证据来源。首先，我们通过构造证明了转换器可以在梯度下降和闭形式的岭回归的基础上实现线性模型的学习算法。其次，我们展示了通过上下文学习训练出来的学习器与梯度下降、岭回归以及精确最小二乘回归所计算的预测器非常相似，在转换器的深度和数据集的噪声水平变化时，能够在不同的预测器之间进行转换。第三，我们提出了一种度量学习器预测器和从显式学习算法中获得的预测器之间相似程度的方法。这种度量表明，上下文学习可以隐含地编码学习算法，并在问题和上下文中根据需要进行切换。

    Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noi
    
[^174]: CIM：基于约束的内在动机方法应对稀疏奖励连续控制问题

    CIM: Constrained Intrinsic Motivation for Sparse-Reward Continuous Control. (arXiv:2211.15205v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15205](http://arxiv.org/abs/2211.15205)

    本文提出了一种基于约束的内在动机方法（CIM），它利用任务先验信息构建内在目标，并适应性地平衡内在和显式目标。在稀疏奖励连续控制任务上，CIM显著优于现有内在动机方法。

    

    内在动机是一种有前途的探索技术，可用于解决具有稀疏或没有显式奖励的强化学习任务。内在动机的设计存在两个技术难点：1）如何设计适当的内在目标来促进有效探索；2）如何与显式奖励进行有效整合，以帮助找到更好的解决方案。在目前的文献中，内在目标都是以任务无关的方式设计，并通过简单的加法与显式奖励进行整合（或在无奖励的预训练中单独使用）。在本文中，我们表明这些设计在典型的稀疏奖励连续控制任务中将会失败。为了应对这个问题，我们提出了基于约束的内在动机（CIM）方法，利用现有可获得的任务先验信息来构建约束的内在目标，并利用Lagrangian方法通过简单、高效和原则性的框架适应性地平衡内在和显式目标。我们在各种具有挑战性的稀疏奖励连续控制任务上评估了CIM，并展示了其明显优于现有内在动机方法的性能。

    Intrinsic motivation is a promising exploration technique for solving reinforcement learning tasks with sparse or absent extrinsic rewards. There exist two technical challenges in implementing intrinsic motivation: 1) how to design a proper intrinsic objective to facilitate efficient exploration; and 2) how to combine the intrinsic objective with the extrinsic objective to help find better solutions. In the current literature, the intrinsic objectives are all designed in a task-agnostic manner and combined with the extrinsic objective via simple addition (or used by itself for reward-free pre-training). In this work, we show that these designs would fail in typical sparse-reward continuous control tasks. To address the problem, we propose Constrained Intrinsic Motivation (CIM) to leverage readily attainable task priors to construct a constrained intrinsic objective, and at the same time, exploit the Lagrangian method to adaptively balance the intrinsic and extrinsic objectives via a si
    
[^175]: AdaTask: 一种面向多任务学习的任务感知自适应学习率方法

    AdaTask: A Task-aware Adaptive Learning Rate Approach to Multi-task Learning. (arXiv:2211.15055v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15055](http://arxiv.org/abs/2211.15055)

    提出了一种名为AdaTask的任务感知自适应学习率方法，通过自适应地调整不同任务的学习率，以平衡不同任务的重要性，从而在各种基准测试上始终优于现有的MTL方法。

    

    多任务学习（MTL）模型已在计算机视觉、自然语言处理和推荐系统等领域展现出令人瞩目的结果。尽管已经提出了许多方法，但这些方法如何在每个参数上平衡不同任务仍然不清楚。在本文中，我们提出通过每个任务对该参数进行的总更新来衡量参数的任务优势度。具体而言，我们通过指数衰减的平均更新（AU）来计算每个任务在该参数上的总更新数。基于这一新颖的度量标准，我们观察到现有MTL方法中的许多参数，尤其是在较高的共享层中的参数，仍然受到一个或几个任务的支配。AU的支配主要是由于一个或几个任务的梯度累积导致的。受此启发，我们提出了一种名为AdaTask的任务感知自适应学习率方法，以分离不同任务之间的累积梯度，从而平衡不同任务的重要性。AdaTask根据AU值自适应地调整不同任务的学习率，以平衡不同任务的重要性。我们在各种基准测试上评估了AdaTask，并证明它始终优于现有的MTL方法。

    Multi-task learning (MTL) models have demonstrated impressive results in computer vision, natural language processing, and recommender systems. Even though many approaches have been proposed, how well these approaches balance different tasks on each parameter still remains unclear. In this paper, we propose to measure the task dominance degree of a parameter by the total updates of each task on this parameter. Specifically, we compute the total updates by the exponentially decaying Average of the squared Updates (AU) on a parameter from the corresponding task.Based on this novel metric, we observe that many parameters in existing MTL methods, especially those in the higher shared layers, are still dominated by one or several tasks. The dominance of AU is mainly due to the dominance of accumulative gradients from one or several tasks. Motivated by this, we propose a Task-wise Adaptive learning rate approach, AdaTask in short, to separate the \emph{accumulative gradients} and hence the l
    
[^176]: 关于深度全卷积神经网络的普适逼近性质探究

    On the Universal Approximation Property of Deep Fully Convolutional Neural Networks. (arXiv:2211.14047v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14047](http://arxiv.org/abs/2211.14047)

    本文研究了深度全卷积神经网络对平移不变或等变函数的逼近性能，证明了残差和非残差变体在特定条件下可以实现普适逼近，同时这些条件是必要的。

    

    本文从动力系统角度研究了深度全卷积网络对平移不变或等变函数的逼近性能。我们证明了在通道宽度为常数时，深度残差全卷积网络及其连续层对应的网络可以实现对这些对称函数的普适逼近。此外，我们还证明了至少每层具有2个通道和至少2的卷积核尺寸的非残差变体也能达到相同的逼近性能。另外，我们还表明这些要求是必要的，因为通道数量较少或卷积核尺寸较小的网络不能实现普适逼近。

    We study the approximation of shift-invariant or equivariant functions by deep fully convolutional networks from the dynamical systems perspective. We prove that deep residual fully convolutional networks and their continuous-layer counterpart can achieve universal approximation of these symmetric functions at constant channel width. Moreover, we show that the same can be achieved by non-residual variants with at least 2 channels in each layer and convolutional kernel size of at least 2. In addition, we show that these requirements are necessary, in the sense that networks with fewer channels or smaller kernels fail to be universal approximators.
    
[^177]: Diffiner: 一种用于语音增强的多功能扩散生成细化器

    Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement. (arXiv:2210.17287v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.17287](http://arxiv.org/abs/2210.17287)

    Diffiner是一种基于DNN的生成细化器，可用于改善经过SE方法预处理后的感知语音质量。它可以应用于各种SE方法，且具有高度的模块化潜力。

    

    尽管基于深度神经网络（DNN）的语音增强（SE）方法优于以前的非DNN方法，但它们经常降低所生成输出的感知质量。为了解决这个问题，我们引入了一种基于DNN的生成细化器，Diffiner，旨在改善通过SE方法预处理过的感知语音质量。我们利用仅包含清晰语音的数据集训练了一个扩散生成模型。然后，我们的细化器有效地将通过去噪扩散恢复新生成的清晰部分与之前的SE方法造成的退化和失真部分混合在一起，从而产生细化的语音。一旦我们的细化器训练为一组清晰的语音，它就可以应用于各种SE方法，而无需为每个SE模块专门进行额外的训练。因此，我们的细化器可以是相对于SE方法的一个多功能的后处理模块，并具有高度的模块化潜力。实验结果表明，我们的方法改善了感知语音质量。

    Although deep neural network (DNN)-based speech enhancement (SE) methods outperform the previous non-DNN-based ones, they often degrade the perceptual quality of generated outputs. To tackle this problem, we introduce a DNN-based generative refiner, Diffiner, aiming to improve perceptual speech quality pre-processed by an SE method. We train a diffusion-based generative model by utilizing a dataset consisting of clean speech only. Then, our refiner effectively mixes clean parts newly generated via denoising diffusion restoration into the degraded and distorted parts caused by a preceding SE method, resulting in refined speech. Once our refiner is trained on a set of clean speech, it can be applied to various SE methods without additional training specialized for each SE module. Therefore, our refiner can be a versatile post-processing module w.r.t. SE methods and has high potential in terms of modularity. Experimental results show that our method improved perceptual speech quality rega
    
[^178]: 掩码自编码器是口腔学习的利器。

    Masked Autoencoders Are Articulatory Learners. (arXiv:2210.15195v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.15195](http://arxiv.org/abs/2210.15195)

    本文提出了基于深度学习的Masked Autoencoders方法，可以精确重构被追踪错误的口腔学记录，有效地应用于XRMB数据集研究中。

    

    口腔学记录下不同口腔部位的位置和运动，被广泛用于研究语音产生以及开发基于口腔学的语音合成器和语音反演系统。威斯康星大学X射线微束（XRMB）数据集是提供与音频记录同步的各种数据集之一。 XRMB口腔学记录使用放置在多个口腔部位的颗粒，可以由微束跟踪。然而，很大一部分口腔学记录被跟踪错误，一直无法使用。在这项工作中，我们提出了一种基于 Masked Autoencoders 的深度学习方法，可以准确重构 XRMB数据集的47个演讲者中41个的被追踪错误的口腔学记录。当八个口腔部位中的三个被误追踪时，我们的模型能够重构口腔学轨迹，与真实情况非常接近。

    Articulatory recordings track the positions and motion of different articulators along the vocal tract and are widely used to study speech production and to develop speech technologies such as articulatory based speech synthesizers and speech inversion systems. The University of Wisconsin X-Ray microbeam (XRMB) dataset is one of various datasets that provide articulatory recordings synced with audio recordings. The XRMB articulatory recordings employ pellets placed on a number of articulators which can be tracked by the microbeam. However, a significant portion of the articulatory recordings are mistracked, and have been so far unsuable. In this work, we present a deep learning based approach using Masked Autoencoders to accurately reconstruct the mistracked articulatory recordings for 41 out of 47 speakers of the XRMB dataset. Our model is able to reconstruct articulatory trajectories that closely match ground truth, even when three out of eight articulators are mistracked, and retrie
    
[^179]: 离散PDEs的神经封闭模型比较

    Comparison of neural closure models for discretised PDEs. (arXiv:2210.14675v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14675](http://arxiv.org/abs/2210.14675)

    本文系统地比较了三个离散PDEs的神经封闭模型的训练过程，并发现先离散化再优化的轨迹拟合是首选，比导数拟合更准确、比先优化再离散化更稳定，但计算成本较高。

    

    最近，神经封闭模型已经被提出作为一种用神经网络有效逼近多尺度系统中小尺度的方法。损失函数的选择和相关的训练过程对生成的神经封闭模型的精度和稳定性影响巨大。本文系统地比较了三种不同的方法："导数拟合"、先离散化再优化的轨迹拟合和先优化再离散化的轨迹拟合。导数拟合是概念上最简单和计算上最高效的方法，在一个测试问题（Kuramoto-Sivashinsky）上表现良好，但在另一个测试问题（Burgers）上表现不佳。轨迹拟合计算上更加昂贵，但更加鲁棒，因此是首选的方法。在两种轨迹拟合方法中，先离散化再优化的方法比先优化再离散化的方法生成的模型更准确。虽然先优化再离散化的方法在计算时间上更加高效，但稳定性较差，可能需要精心调整超参数。

    Neural closure models have recently been proposed as a method for efficiently approximating small scales in multiscale systems with neural networks. The choice of loss function and associated training procedure has a large effect on the accuracy and stability of the resulting neural closure model. In this work, we systematically compare three distinct procedures: "derivative fitting", "trajectory fitting" with discretise-then-optimise, and "trajectory fitting" with optimise-then-discretise. Derivative fitting is conceptually the simplest and computationally the most efficient approach and is found to perform reasonably well on one of the test problems (Kuramoto-Sivashinsky) but poorly on the other (Burgers). Trajectory fitting is computationally more expensive but is more robust and is therefore the preferred approach. Of the two trajectory fitting procedures, the discretise-then-optimise approach produces more accurate models than the optimise-then-discretise approach. While the optim
    
[^180]: EC-NAS: 面向神经架构搜索的能耗感知表格基准

    EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search. (arXiv:2210.06015v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06015](http://arxiv.org/abs/2210.06015)

    提出了一个能耗感知的神经架构搜索表格基准 EC-NAS，该基准通过添加能耗和碳足迹信息，支持设计能效高的深度学习模型，并降低总能耗。

    

    近年来，选择、训练和部署深度学习模型所需的能量消耗不断增加。本文旨在支持设计能效高、训练资源消耗较低、适用于实际边缘/移动计算环境并具有环境可持续性的深度学习模型。我们提出将能效作为神经架构搜索 (NAS) 的一项额外性能指标，并通过添加不同架构的能耗和碳足迹信息，提供更新的表格基准 EC-NAS 以在较低计算成本下评估 NAS 策略。EC-NAS 还包括用于预测能耗的代理模型，并有助于降低总能耗。

    Energy consumption from selecting, training and deploying deep learning models has continued to increase over the past few years. Our goal in this work is to support the design of energy-efficient deep learning models that are easier to train with lower compute resources, practical to deploy in real-world edge/mobile computing settings and environmentally sustainable. Tabular benchmarks for neural architecture search (NAS) allow the evaluation of NAS strategies at lower computational cost by providing pre-computed performance statistics. In this work, we suggest including energy efficiency as an additional performance criterion to NAS and present an updated tabular benchmark by including information on energy consumption and carbon footprint for different architectures. The benchmark called EC-NAS is made available open-source to support energy consumption-aware NAS research. EC-NAS also includes a surrogate model for predicting energy consumption, and helps us reduce the overall energ
    
[^181]: 非展开式压缩盲反卷积

    Unrolled Compressed Blind-Deconvolution. (arXiv:2209.14165v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2209.14165](http://arxiv.org/abs/2209.14165)

    在S-MBD问题中,我们提出了一种非展开式压缩方法，从少量的时间测量中无损恢复稀疏滤波器，并提出了一个数据驱动的非展开式学习框架来解决该问题。

    

    稀疏多通道盲反卷积问题(S-MBD)经常出现在诸如雷达/声纳/超声成像等许多工程应用中。为了降低计算和实现成本，我们提出了一种压缩方法，使得从时间上的少量测量中就能进行盲恢复。所提出的压缩方法通过先将信号经过滤波器再进行子采样测量，从而实现了显著的实现成本降低。我们从压缩测量中推导出了一个稀疏滤波器的可辨识性和恢复的理论保证。我们的结果允许设计一系列压缩滤波器。然后，我们提出了一个数据驱动的非展开式学习框架，学习压缩滤波器并解决S-MBD问题。编码器是一个循环推理网络，将压缩测量映射到稀疏滤波器的估计值。我们证明了我们的非展开式学习方法更加鲁棒，对选择有噪声的单位分解算法更加鲁棒。

    The problem of sparse multichannel blind deconvolution (S-MBD) arises frequently in many engineering applications such as radar/sonar/ultrasound imaging. To reduce its computational and implementation cost, we propose a compression method that enables blind recovery from much fewer measurements with respect to the full received signal in time. The proposed compression measures the signal through a filter followed by a subsampling, allowing for a significant reduction in implementation cost. We derive theoretical guarantees for the identifiability and recovery of a sparse filter from compressed measurements. Our results allow for the design of a wide class of compression filters. We, then, propose a data-driven unrolled learning framework to learn the compression filter and solve the S-MBD problem. The encoder is a recurrent inference network that maps compressed measurements into an estimate of sparse filters. We demonstrate that our unrolled learning method is more robust to choices o
    
[^182]: 基于部分信息分解的神经表示复杂度度量

    A Measure of the Complexity of Neural Representations based on Partial Information Decomposition. (arXiv:2209.10438v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2209.10438](http://arxiv.org/abs/2209.10438)

    本文提出了一种基于部分信息分解的“表示复杂度”度量，用于量化跨多个神经元扩散的信息访问难度，并证明了其实用性。

    

    在神经网络中，与任务相关的信息通常是由神经元群联合表示的。然而，关于这种分类标签的互信息如何在单个神经元之间分配的细节尚不清楚：虽然部分互信息只能从特定的单个神经元中获得，但其他部分则由多个神经元冗余或协同承载。本文展示了如何使用信息论的部分信息分解来分离这些不同的贡献，并提出了“表示复杂度”度量，用于量化跨多个神经元扩散的信息访问难度。我们证明了如何直接计算较小层的复杂度，并针对较大层提出了子抽样和粗粒化过程，并证明了对应的上限。在MNIST和CIFAR10任务上，我们在量化的深度神经网络中观察到表示复杂度，证明了我们方法的实用性。

    In neural networks, task-relevant information is represented jointly by groups of neurons. However, the specific way in which this mutual information about the classification label is distributed among the individual neurons is not well understood: While parts of it may only be obtainable from specific single neurons, other parts are carried redundantly or synergistically by multiple neurons. We show how Partial Information Decomposition (PID), a recent extension of information theory, can disentangle these different contributions. From this, we introduce the measure of "Representational Complexity", which quantifies the difficulty of accessing information spread across multiple neurons. We show how this complexity is directly computable for smaller layers. For larger layers, we propose subsampling and coarse-graining procedures and prove corresponding bounds on the latter. Empirically, for quantized deep neural networks solving the MNIST and CIFAR10 tasks, we observe that representati
    
[^183]: 加速原始-对偶方法处理凸-强凹鞍点问题

    Accelerated Primal-Dual Methods for Convex-Strongly-Concave Saddle Point Problems. (arXiv:2209.04604v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2209.04604](http://arxiv.org/abs/2209.04604)

    本文提出了一种加速的线性化原始-对偶（ALPD）方法，通过结合加速梯度下降和线性化PD方法，解决了处理凸-强凹鞍点问题中LPD方法对于原始函数Lipschitz常数依赖性次优的问题。对于SPP具有半线性耦合函数，ALPD方法实现了最优的梯度复杂度，而针对具有一般非线性耦合函数的SPP，我们提出了不精确的ALPD方法。

    

    本文探讨了一种原始-对偶（PD）方法处理鞍点问题（SPP），该方法采用原始函数的线性逼近代替标准的邻域步骤，从而得到线性化的PD（LPD）方法。对于凸-强凹鞍点问题，我们观察到LPD方法对于原始函数的Lipschitz常数有一个次优的依赖性。为了解决这个问题，我们将加速梯度下降的特性与LPD方法相结合，得到单循环加速线性化原始-对偶（ALPD）方法。ALPD方法在SPP具有半线性耦合函数时实现了最优梯度复杂度。我们还提出了一种针对具有一般非线性耦合函数的SPP的不精确ALPD方法，该方法保持了原始部分的最优梯度评估，并显著改善了耦合项的梯度评估。我们用数值实验验证了我们的结果。

    We investigate a primal-dual (PD) method for the saddle point problem (SPP) that uses a linear approximation of the primal function instead of the standard proximal step, resulting in a linearized PD (LPD) method. For convex-strongly concave SPP, we observe that the LPD method has a suboptimal dependence on the Lipschitz constant of the primal function. To fix this issue, we combine features of Accelerated Gradient Descent with the LPD method resulting in a single-loop Accelerated Linearized Primal-Dual (ALPD) method. ALPD method achieves the optimal gradient complexity when the SPP has a semi-linear coupling function. We also present an inexact ALPD method for SPPs with a general nonlinear coupling function that maintains the optimal gradient evaluations of the primal parts and significantly improves the gradient evaluations of the coupling term compared to the ALPD method. We verify our findings with numerical experiments.
    
[^184]: 三个新的验证器及用于无监督领域自适应的大规模基准排名

    Three New Validators and a Large-Scale Benchmark Ranking for Unsupervised Domain Adaptation. (arXiv:2208.07360v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.07360](http://arxiv.org/abs/2208.07360)

    本文对无监督领域自适应中估计准确性的三种新验证器进行了研究，并在100万个检查点的大数据集上与其他五种验证器进行了比较和排名。实验结果表明，我们提出的两个验证器优于现有的验证器，并且最佳的估计准确性方法因任务类型而异。

    

    超参数的改变可以对模型准确度产生巨大影响。因此，超参数调整在优化机器学习模型方面发挥着重要作用。超参数调整过程的一个重要组成部分是通过使用“验证器”评估模型检查点。在有标签的监督设置中，这些验证器通过计算在验证集上的准确率来评估检查点。相反，在无监督设置中，验证集没有这样的标签。没有任何标签，因此无法计算准确性，因此验证器必须估计准确性。但是估计准确性的最佳方法是什么？在本文中，我们考虑这个问题在无监督领域适应（UDA）的上下文中。具体而言，我们提出了三种新的验证器，并将其与其他五个现有的验证器在包含100万个检查点的大型数据集上进行比较和排名。广泛的实验结果表明，我们提供的两个验证器优于现有的验证器，并且估计准确性的最佳方法因UDA任务类型而异。

    Changes to hyperparameters can have a dramatic effect on model accuracy. Thus, the tuning of hyperparameters plays an important role in optimizing machine-learning models. An integral part of the hyperparameter-tuning process is the evaluation of model checkpoints, which is done through the use of "validators". In a supervised setting, these validators evaluate checkpoints by computing accuracy on a validation set that has labels. In contrast, in an unsupervised setting, the validation set has no such labels. Without any labels, it is impossible to compute accuracy, so validators must estimate accuracy instead. But what is the best approach to estimating accuracy? In this paper, we consider this question in the context of unsupervised domain adaptation (UDA). Specifically, we propose three new validators, and we compare and rank them against five other existing validators, on a large dataset of 1,000,000 checkpoints. Extensive experimental results show that two of our proposed validato
    
[^185]: DGPO: 使用多样化策略优化发现多种解决方案

    DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization. (arXiv:2207.05631v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.05631](http://arxiv.org/abs/2207.05631)

    这篇论文提出了一个名为DGPO的算法，可以在解决任务时发现多种策略，从而提高策略鲁棒性和与用户交互的乐趣。

    

    大多数强化学习算法都试图寻找解决给定任务的单个最佳策略。然而，学习多种解决方案通常是有价值的，例如，使智能体与用户的交互更加有趣，或者提高策略对意外干扰的鲁棒性。我们提出了一种名为多样化策略优化（DGPO）的在线算法，用于发现解决给定任务的多种策略。与现有工作不同的是，它通过在单次运行中训练共享策略网络实现此目的。具体而言，我们设计了一种基于信息理论多样性目标的内在奖励。我们的最终目标交替约束策略多样性和外在奖励。我们将约束优化问题转化为概率推断任务，并使用策略迭代来最大化得到的下界。实验结果表明，我们的方法能够在各种环境中有效地发现多样化的策略，包括 Atari 游戏和 Mujoco 模拟器，并且能够提供一系列性能和多样性之间的权衡。

    Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variet
    
[^186]: 无任务偏置连续强化学习：获取洞见并克服挑战

    Task-Agnostic Continual Reinforcement Learning: Gaining Insights and Overcoming Challenges. (arXiv:2205.14495v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14495](http://arxiv.org/abs/2205.14495)

    本文研究了无任务偏置连续强化学习（CL）和多任务（MTL）代理性能差异的因素，并提出了基于重放的循环强化学习（3RL）方法，它优于基线方法，并证明在数据、计算或高维度受限设置下尤其有益。

    

    连续学习（CL）使模型和代理能够在学习一系列任务的同时解决标准深度学习方法的局限性，如灾难性遗忘。本文调查了影响无任务偏置CL和多任务（MTL）代理性能差异的因素。我们提出两个假设：（1）无任务偏置方法可能在数据、计算或高维度受限设置中提供优势，（2）更快的适应可能特别有益于连续学习设置，帮助缓解灾难性遗忘的影响。为了调查这些假设，我们引入了基于重放的循环强化学习（3RL）方法用于无任务偏置CL代理。我们在合成任务和Meta-World基准测试上评估了3RL，其中包括50个独特的操作任务。我们的结果表明，3RL优于基线方法，甚至可以超过其多任务等价物。

    Continual learning (CL) enables the development of models and agents that learn from a sequence of tasks while addressing the limitations of standard deep learning approaches, such as catastrophic forgetting. In this work, we investigate the factors that contribute to the performance differences between task-agnostic CL and multi-task (MTL) agents. We pose two hypotheses: (1) task-agnostic methods might provide advantages in settings with limited data, computation, or high dimensionality, and (2) faster adaptation may be particularly beneficial in continual learning settings, helping to mitigate the effects of catastrophic forgetting. To investigate these hypotheses, we introduce a replay-based recurrent reinforcement learning (3RL) methodology for task-agnostic CL agents. We assess 3RL on a synthetic task and the Meta-World benchmark, which includes 50 unique manipulation tasks. Our results demonstrate that 3RL outperforms baseline methods and can even surpass its multi-task equivalen
    
[^187]: Selectively Adaptive Lasso选适应Lasso

    The Selectively Adaptive Lasso. (arXiv:2205.10697v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.10697](http://arxiv.org/abs/2205.10697)

    本文提出了一种新算法——Selectively Adaptive Lasso（SAL），它基于HAL的理论构建，保留了无维度、非参数收敛速率的优点，同时也具有可扩展到大规模高维数据集的能力。这种算法将许多回归系数自动设置为零。

    

    机器学习回归方法能够进行无需过多的参数假设的函数估计。虽然它们可以在预测误差方面表现出色，但大多数缺乏类半参数有效估计（例如，TMLE，AIPW）所需的理论收敛速度。高度自适应Lasso（HAL）是唯一经证明能够快速收敛到意义上的大类函数的回归方法，与预测变量的维度无关。不幸的是，HAL无法扩展计算。在本文中，我们在HAL理论的基础上构建选择自适应Lasso（SAL），一种新的算法，保留HAL的无维度、非参数收敛率，但也能扩展到大规模的高维数据集。为了实现这一目标，我们证明了一些与嵌套Donsker类中的经验损失最小化有关的一般理论结果。我们的算法是一种梯度下降形式，具有简单的分组规则，自动将许多回归系数设为零。

    Machine learning regression methods allow estimation of functions without unrealistic parametric assumptions. Although they can perform exceptionally in prediction error, most lack theoretical convergence rates necessary for semi-parametric efficient estimation (e.g. TMLE, AIPW) of parameters like average treatment effects. The Highly Adaptive Lasso (HAL) is the only regression method proven to converge quickly enough for a meaningfully large class of functions, independent of the dimensionality of the predictors. Unfortunately, HAL is not computationally scalable. In this paper we build upon the theory of HAL to construct the Selectively Adaptive Lasso (SAL), a new algorithm which retains HAL's dimension-free, nonparametric convergence rate but which also scales computationally to large high-dimensional datasets. To accomplish this, we prove some general theoretical results pertaining to empirical loss minimization in nested Donsker classes. Our resulting algorithm is a form of gradie
    
[^188]: 基于Transformer的临床安全分割的超范围检测

    Transformer-based out-of-distribution detection for clinically safe segmentation. (arXiv:2205.10650v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.10650](http://arxiv.org/abs/2205.10650)

    本文提出了一种基于Transformer的完整3D超范围检测模型，针对头部CT中出血分割任务的远超范围和接近超范围情况，在超范围的处理中能够鲁棒地识别并排除超范围情况，经过评估，该方法具有更好的超范围性能，能够在超范围情况下提供可靠的分割结果。

    

    在临床设置中，对部署的图像处理系统具备鲁棒性是至关重要的，特别是不能做出自信错误的预测。最流行的安全处理方法是训练能够提供其不确定性度量的网络，但这些网络往往在训练数据分布范围之外的输入情况下失败。最近，生成建模方法被提议作为一种替代方法；这些方法可以显式地量化数据样本的可能性，在执行进一步处理之前过滤掉任何超范围数据样本。在本文中，我们专注于图像分割，并评估了几种网络不确定性方法，针对头部CT中出血分割任务的远超范围( far-OOD )和接近超范围( near-OOD )情况。我们发现所有这些方法在超范围处理时都会提供自信错误的预测，因此不适合用于安全分割。我们提出一种基于Transformer的完整3D超范围检测模型，可以鲁棒地识别并排除超范围情况。相较于现有的最先进模型，我们提出的方法实现了更好的超范围性能，并且能够在超范围情况下提供可靠的分割结果。

    In a clinical setting it is essential that deployed image processing systems are robust to the full range of inputs they might encounter and, in particular, do not make confidently wrong predictions. The most popular approach to safe processing is to train networks that can provide a measure of their uncertainty, but these tend to fail for inputs that are far outside the training data distribution. Recently, generative modelling approaches have been proposed as an alternative; these can quantify the likelihood of a data sample explicitly, filtering out any out-of-distribution (OOD) samples before further processing is performed. In this work, we focus on image segmentation and evaluate several approaches to network uncertainty in the far-OOD and near-OOD cases for the task of segmenting haemorrhages in head CTs. We find all of these approaches are unsuitable for safe segmentation as they provide confidently wrong predictions when operating OOD. We propose performing full 3D OOD detecti
    
[^189]: Transformer配置与训练目标的研究

    A Study on Transformer Configuration and Training Objective. (arXiv:2205.10505v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10505](http://arxiv.org/abs/2205.10505)

    本文提出了Bamboo配置策略，基于更深更窄的Transformer结构进行Masked自编码器训练，在图像和语言任务上取得了新的最先进结果。

    

    基于Transformer的模型在许多任务，特别是视觉和语言任务上都取得了令人印象深刻的结果。在许多模型训练情况下，通常采用传统的配置。本文重新审视了这些传统配置，通过理论分析和实验评估，提出了Bamboo的配置策略，该策略使用更深更窄的Transformer结构进行Masked自编码器训练，并取得了新的最先进结果。

    Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are typically adopted. For example, we often set the base model with hidden dimensions (i.e. model width) to be 768 and the number of transformer layers (i.e. model depth) to be 12. In this paper, we revisit these conventional configurations. Through theoretical analysis and experimental evaluation, we show that the masked autoencoder is effective in alleviating the over-smoothing issue in deep transformer training. Based on this finding, we propose Bamboo, an idea of using deeper and narrower transformer configurations, for masked autoencoder training. On ImageNet, with such a simple change in configuration, re-designed model achieves 87.1% top-1 accuracy and outperforms SoTA models like MAE and BEiT. On language tasks, re-designed model outperforms BERT with default setting by 1.1 points on average, on GLUE da
    
[^190]: gLaSDI: 参数化物理知识指导的贪心潜空间动力学识别

    gLaSDI: Parametric Physics-informed Greedy Latent Space Dynamics Identification. (arXiv:2204.12005v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2204.12005](http://arxiv.org/abs/2204.12005)

    本文提出了一种参数化的动力学识别方法gLaSDI，通过自编码器与局部DI模型的结合和自适应贪心采样算法来实现高维非线性动态系统的数据驱动降阶建模，具有精确、高效、鲁棒的特点。

    

    本文提出了一种参数自适应的物理知识指导的贪心潜空间动力学识别方法（gLaSDI），用于高维非线性动态系统的精确、高效和鲁棒的数据驱动降阶建模。在提出的gLaSDI框架中，自编码器发现高维数据的内在非线性潜在表示，而动力学识别（DI）模型捕获局部潜空间动态。采用交互式训练算法对自编码器和局部DI模型进行训练，以识别简单的潜空间动态并提高数据驱动降阶建模的准确性和效率。为了最大化和加速参数空间的探索，以得到最佳的模型性能，本文提出了一种自适应贪心采样算法，将物理知识指导的残差误差指标和随机子集评估相结合，实现了实时搜索最优训练样本。此外，为了利用本地潜空间

    A parametric adaptive physics-informed greedy Latent Space Dynamics Identification (gLaSDI) method is proposed for accurate, efficient, and robust data-driven reduced-order modeling of high-dimensional nonlinear dynamical systems. In the proposed gLaSDI framework, an autoencoder discovers intrinsic nonlinear latent representations of high-dimensional data, while dynamics identification (DI) models capture local latent-space dynamics. An interactive training algorithm is adopted for the autoencoder and local DI models, which enables identification of simple latent-space dynamics and enhances accuracy and efficiency of data-driven reduced-order modeling. To maximize and accelerate the exploration of the parameter space for the optimal model performance, an adaptive greedy sampling algorithm integrated with a physics-informed residual-based error indicator and random-subset evaluation is introduced to search for the optimal training samples on the fly. Further, to exploit local latent-spa
    
[^191]: 对CNN分类器的对抗性痕迹攻击：可部署的攻击

    Adversarial Scratches: Deployable Attacks to CNN Classifiers. (arXiv:2204.09397v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.09397](http://arxiv.org/abs/2204.09397)

    本文提出了一种新型的对抗性攻击方法——对抗性痕迹攻击。通过在图像中制造痕迹的方式进行攻击，该方法比其他最先进的攻击方法更加可部署，并在公开API和交通标志的图像测试中表现出更高的破解率，攻击次数更少。

    

    越来越多的研究表明，深度神经网络容易受到对抗性示例的攻击，这些示例是应用于模型输入的小扰动，导致模型做出错误的预测。本文提出了一种新型的L0黑盒攻击——对抗性痕迹攻击：通过在图像中制造痕迹的方式进行攻击，它比其他最先进的攻击方法更具有可部署性。对抗性痕迹利用Bezier曲线来减少搜索空间的维度，并可能将攻击约束到特定位置。我们在几种情况下测试了对抗性痕迹攻击，包括公开API和交通标志的图像。结果表明，我们的攻击通常比其他部署最先进的方法实现更高的破解率，同时需要明显更少的攻击次数。

    A growing body of work has shown that deep neural networks are susceptible to adversarial examples. These take the form of small perturbations applied to the model's input which lead to incorrect predictions. Unfortunately, most literature focuses on visually imperceivable perturbations to be applied to digital images that often are, by design, impossible to be deployed to physical targets. We present Adversarial Scratches: a novel L0 black-box attack, which takes the form of scratches in images, and which possesses much greater deployability than other state-of-the-art attacks. Adversarial Scratches leverage B\'ezier Curves to reduce the dimension of the search space and possibly constrain the attack to a specific location. We test Adversarial Scratches in several scenarios, including a publicly available API and images of traffic signs. Results show that, often, our attack achieves higher fooling rate than other deployable state-of-the-art methods, while requiring significantly fewer
    
[^192]: 强化学习策略推荐对银行间网络稳定性的影响分析

    Reinforcement Learning Policy Recommendation for Interbank Network Stability. (arXiv:2204.07134v2 [econ.GN] UPDATED)

    [http://arxiv.org/abs/2204.07134](http://arxiv.org/abs/2204.07134)

    本文基于强化学习策略推荐，分析了其对人工银行间市场的影响。根据个体信息和公共推荐，金融机构制定借贷协议。策略推荐能够合理引导金融行为者的信贷关系形成，核心-周边网络结构迅速稳定。

    

    本文分析了策略推荐对人工银行间市场表现的影响。金融机构根据公共推荐和个体信息制定借贷协议。前者通过强化学习的最优策略来模拟，旨在最大化系统的适应性并收集有关经济环境的信息。策略推荐直接或间接地影响金融行为者的信贷关系形成过程。通过低利率或高流动性供给的最优选择来合理引导，再根据银行代理的资产负债表信息来确认他们在市场内最适宜向客户提供的流动性和利率组合。通过将公共和私有信号相结合，金融机构能够通过逐渐建立有选择性附加的信贷连接而创建或修建他们的信贷网络，从而生成动态网络。研究结果表明，在这种情况下，核心-周边网络结构具有迅速稳定的形式。

    In this paper, we analyze the effect of a policy recommendation on the performance of an artificial interbank market. Financial institutions stipulate lending agreements following a public recommendation and their individual information. The former is modeled by a reinforcement learning optimal policy that maximizes the system's fitness and gathers information on the economic environment. The policy recommendation directs economic actors to create credit relationships through the optimal choice between a low interest rate or a high liquidity supply. The latter, based on the agents' balance sheet, allows determining the liquidity supply and interest rate that the banks optimally offer their clients within the market. Thanks to the combination between the public and the private signal, financial institutions create or cut their credit connections over time via a preferential attachment evolving procedure able to generate a dynamic network. Our results show that the emergence of a core-pe
    
[^193]: PyDTS：用于离散时间竞争风险（正则化）回归的 Python 包

    PyDTS: A Python Package for Discrete-Time Survival (Regularized) Regression with Competing Risks. (arXiv:2204.05731v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.05731](http://arxiv.org/abs/2204.05731)

    PyDTS是一个用于离散时间生存数据半参数竞争风险模型的Python包，支持包括LASSO和弹性网等正则化回归方法。

    

    时间至事件分析（生存分析）用于响应时间是指预定事件发生的时间。由于时间本身是离散的或由于将失败时间分组为间隔或舍入测量，因此时间至事件数据有时是离散的。此外，个体的失败可能是几种不同的失败类型之一，称为竞争风险（事件）。大多数生存回归分析的方法和软件包假定时间是在连续尺度上测量的。众所周知，将标准的连续时间模型应用于离散时间数据可能导致离散时间模型的估计器存在偏差。介绍了 Python 包 PyDTS，用于模拟，估计和评估离散时间生存数据的半参数竞争风险模型。该包实现了快速过程，使有效地包括正则化回归方法，如 LASSO 和弹性网络等。一个模拟

    Time-to-event analysis (survival analysis) is used when the response of interest is the time until a pre-specified event occurs. Time-to-event data are sometimes discrete either because time itself is discrete or due to grouping of failure times into intervals or rounding off measurements. In addition, the failure of an individual could be one of several distinct failure types, known as competing risks (events). Most methods and software packages for survival regression analysis assume that time is measured on a continuous scale. It is well-known that naively applying standard continuous-time models with discrete-time data may result in biased estimators of the discrete-time models. The Python package PyDTS, for simulating, estimating and evaluating semi-parametric competing-risks models for discrete-time survival data, is introduced. The package implements a fast procedure that enables including regularized regression methods, such as LASSO and elastic net, among others. A simulation 
    
[^194]: 随机投影分类的最优性和复杂度。

    Optimality and complexity of classification by random projection. (arXiv:2108.06339v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.06339](http://arxiv.org/abs/2108.06339)

    本文研究了一组低复杂度分类器，该分类器可以近似于任意连续函数和布尔函数，且在给定类条件密度的情况下，其误差与最优误差相同。

    

    分类器的泛化误差与选择分类器的函数集的复杂度有关。我们研究了一组低复杂度分类器，包括通过随机一维特征做阈值处理。该特征通过将数据嵌入到由高次单项式参数化的更高维空间中后在随机直线上进行投影而得到。具体而言，扩展的数据被投影n次，并从这n个中选出表现在训练数据上最好的分类器。我们证明了这种类型的分类器是极其灵活的，因为它有可能近似于任何在紧致集上的连续函数，以及将支撑集拆分为可测子集的任何布尔函数。特别地，如果给定类条件密度的完全知识，则这些低复杂度分类器的误差将在k和n趋近于无穷大时收敛到最优（贝叶斯）误差。

    The generalization error of a classifier is related to the complexity of the set of functions among which the classifier is chosen. We study a family of low-complexity classifiers consisting of thresholding a random one-dimensional feature. The feature is obtained by projecting the data on a random line after embedding it into a higher-dimensional space parametrized by monomials of order up to k. More specifically, the extended data is projected n-times and the best classifier among those n, based on its performance on training data, is chosen. We show that this type of classifier is extremely flexible, as it is likely to approximate, to an arbitrary precision, any continuous function on a compact set as well as any boolean function on a compact set that splits the support into measurable subsets. In particular, given full knowledge of the class conditional densities, the error of these low-complexity classifiers would converge to the optimal (Bayes) error as k and n go to infinity. On
    
[^195]: 认知神经网络

    Epistemic Neural Networks. (arXiv:2107.08924v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.08924](http://arxiv.org/abs/2107.08924)

    该论文提出了一种能够通过适量级别的递增计算来估计神经网络不确定性的Epistemic神经网络框架，使得传统神经网络能够在计算成本大幅下降的情况下超越大型集成模型，为模型联合预测的方法提供了一种新的接口。

    

    智能依赖于智能体对其不知道的事物的了解。智能体预测多个输入标签的质量可以评估其对这种能力的掌握程度。集成式方法在原则上可以产生有效的预测，但训练大规模的集成模型的计算成本很高，从而可能会变得禁止。我们引入了Epinet：一种可以加强任何传统神经网络（包括大型预训练模型）的架构，并且可以通过适量级别的递增计算训练来估计不确定性。用Epinet，传统神经网络可以在计算成本大幅下降的情况下胜过由数百个或更多粒子组成的大型集成，同时不需要符合贝叶斯神经网络的传统框架。为了适应超越BNN的方法的发展，例如Epinet，我们介绍了作为产生联合预测模型的接口的知识神经网络（ENN）。

    Intelligence relies on an agent's knowledge of what it does not know. This capability can be assessed based on the quality of joint predictions of labels across multiple inputs. In principle, ensemble-based approaches produce effective joint predictions, but the computational costs of training large ensembles can become prohibitive. We introduce the epinet: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to estimate uncertainty. With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation. The epinet does not fit the traditional framework of Bayesian neural networks. To accommodate development of approaches beyond BNNs, such as the epinet, we introduce the epistemic neural network (ENN) as an interface for models that produce joint predictions.
    
[^196]: 通过变异性解释多层感知器的可训练性

    Multi-layer Perceptron Trainability Explained via Variability. (arXiv:2105.08911v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.08911](http://arxiv.org/abs/2105.08911)

    本研究旨在解释多层感知器的可训练性，引入了一个新概念 - 变异性，与 MLP 的激活数量呈正相关，与"坍塌到常数"现象呈负相关，是 MLP 可训练性的一个准确预测指标。

    

    尽管深度神经网络在各种应用中取得了巨大的成功，但深度学习的许多基本方面仍未完全理解，包括DNN的可训练性。本研究旨在辨别在相似条件下，什么使一个DNN模型比另一个更容易训练。特别地，我们的研究集中在带有相同参数数量的多层感知器 (MLP) 模型上。我们引入了一个新概念 - 变异性，以帮助解释深度学习的好处以及在训练非常深的 MLP 时所遇到的困难。简单地说，神经网络的变异性代表了数据空间中与良好缩放的随机权重相关的地形模式的丰富性。我们从实证上证明，变异性与激活数量呈正相关，与"坍塌到常数"现象呈负相关，后者与众所周知的梯度消失现象相关但并不完全相同。在一个小规模的数据集上的实验证明，变异性是 MLP 可训练性的一个准确预测指标。

    Despite the tremendous successes of deep neural networks (DNNs) in various applications, many fundamental aspects of deep learning remain incompletely understood, including DNN trainability. In a trainability study, one aims to discern what makes one DNN model easier to train than another under comparable conditions. In particular, our study focuses on multi-layer perceptron (MLP) models equipped with the same number of parameters. We introduce a new notion called variability to help explain the benefits of deep learning and the difficulties in training very deep MLPs. Simply put, variability of a neural network represents the richness of landscape patterns in the data space with respect to well-scaled random weights. We empirically show that variability is positively correlated to the number of activations and negatively correlated to a phenomenon called "Collapse to Constant", which is related but not identical to the well-known vanishing gradient phenomenon. Experiments on a small s
    
[^197]: 少样本部分多视角学习

    Few-shot Partial Multi-view Learning. (arXiv:2105.02046v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2105.02046](http://arxiv.org/abs/2105.02046)

    本文提出了一个新任务——少样本部分多视角学习，旨在克服低数据环境下视角缺失问题的影响。

    

    在现实世界中，数据往往具有多个视角，充分利用每个视角的信息对于使数据更具代表性很重要。然而，由于数据收集和预处理中的各种限制和失误，真实数据遭受视角缺失和数据稀缺是不可避免的。这两个问题的共存使得实现模式分类任务更具挑战性。目前，据我们所知，很少有适当的方法可以同时处理这两个问题。为了引起学术界对这一挑战的更多关注，本文提出了一个新的任务，称为少样本部分多视角学习，专注于克服低数据环境下视角缺失问题的负面影响。此任务的挑战有两个方面：（i）难以在缺失视角的干扰下克服数据稀缺的影响；（ii）有限的数据量加剧了信息的稀缺。

    It is often the case that data are with multiple views in real-world applications. Fully exploring the information of each view is significant for making data more representative. However, due to various limitations and failures in data collection and pre-processing, it is inevitable for real data to suffer from view missing and data scarcity. The coexistence of these two issues makes it more challenging to achieve the pattern classification task. Currently, to our best knowledge, few appropriate methods can well-handle these two issues simultaneously. Aiming to draw more attention from the community to this challenge, we propose a new task in this paper, called few-shot partial multi-view learning, which focuses on overcoming the negative impact of the view-missing issue in the low-data regime. The challenges of this task are twofold: (i) it is difficult to overcome the impact of data scarcity under the interference of missing views; (ii) the limited number of data exacerbates informa
    
[^198]: 一个ODENet和ResNet的通用逼近性质：数学分析与数值实验

    Universal Approximation Properties for an ODENet and a ResNet: Mathematical Analysis and Numerical Experiments. (arXiv:2101.10229v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.10229](http://arxiv.org/abs/2101.10229)

    本论文证明了对于一类ODENet和一类ResNet，“宽度为n+m的ODENet可以逼近${\rm \mathbb{R}^n}$上紧致子集上的任何连续函数”，同时推导了损失函数对某个调整变量的梯度并用于构建ODENet的学习算法，并在MNIST上进行实验。

    

    我们证明了一类ODENet和一类ResNet的通用逼近性质(UAP)，它们是具有跳跃连接的深度学习系统的简化数学模型。 UAP可以陈述如下:设$n$和$m$分别为输入数据和输出数据的维数，并假设$m\leq n$。然后我们证明了带有非多项式连续激活函数的宽度为$n+m$的ODENet可以逼近$\mathbb {R} ^ n$上紧致子集上的任何连续函数。我们还证明了当深度趋于无限时，ResNet具有相同的性质。此外，我们明确推导了损失函数对某个调整变量的梯度。 我们将其用于构建ODENet的学习算法。为了展示此算法的实用性，我们将其应用于MNIST上的回归问题、二分类和多项分类。

    We prove a universal approximation property (UAP) for a class of ODENet and a class of ResNet, which are simplified mathematical models for deep learning systems with skip connections. The UAP can be stated as follows. Let $n$ and $m$ be the dimension of input and output data, and assume $m\leq n$. Then we show that ODENet of width $n+m$ with any non-polynomial continuous activation function can approximate any continuous function on a compact subset on $\mathbb{R}^n$. We also show that ResNet has the same property as the depth tends to infinity. Furthermore, we derive the gradient of a loss function explicitly with respect to a certain tuning variable. We use this to construct a learning algorithm for ODENet. To demonstrate the usefulness of this algorithm, we apply it to a regression problem, a binary classification, and a multinomial classification in MNIST.
    
[^199]: 重复首价拍卖中的最优无悔学习

    Optimal No-regret Learning in Repeated First-price Auctions. (arXiv:2003.09795v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2003.09795](http://arxiv.org/abs/2003.09795)

    本文提出了重复首价拍卖的最优无悔学习算法，通过利用特定的反馈结构和支付函数，实现了接近最优的遗憾界限。

    

    本研究探讨了在重复首价拍卖中的在线学习问题，拍卖者只在每次拍卖结束后看到最高的出价，为了最大化收益，她必须进行适应性出价。然而，拍卖者只能面对被审查的反馈，如果她赢得出价，就无法观察到其他竞标者的最高出价，而其他竞标者的最高出价是从未知的分布中\textit{iid}抽取的。本文开发了第一个学习算法，通过利用首价拍卖的两个结构性质，即特定的反馈结构和支付函数，实现了接近最优的$\widetilde{O}(\sqrt{T})$遗憾界限。首价拍卖中的反馈机制结合了跨行动(出价)的图形反馈、跨上下文(私人价值)的交叉学习以及对上下文的部分排序，我们将其推广为部分排序情境赌博机。通过展示损失函数与优化结构之间的奇怪分离，我们建立了此框架的优点和缺点。我们的算法使用后验风险最小化的有效变体，称为“带有基于方差的正则化的连续调用正则化经验风险最小化器”。我们的分析利用了对问题本质结构和Kwon-Singer定理的仔细探索。

    We study online learning in repeated first-price auctions where a bidder, only observing the winning bid at the end of each auction, learns to adaptively bid in order to maximize her cumulative payoff. To achieve this goal, the bidder faces a censored feedback: if she wins the bid, then she is not able to observe the highest bid of the other bidders, which we assume is \textit{iid} drawn from an unknown distribution. In this paper, we develop the first learning algorithm that achieves a near-optimal $\widetilde{O}(\sqrt{T})$ regret bound, by exploiting two structural properties of first-price auctions, i.e. the specific feedback structure and payoff function.  The feedback in first-price auctions combines the graph feedback across actions (bids), the cross learning across contexts (private values), and a partial order over the contexts; we generalize it as the partially ordered contextual bandits. We establish both strengths and weaknesses of this framework, by showing a curious separa
    
[^200]: 利用未标记数据扩展类别的开放集学习（Open-LACU）

    Open-set learning with augmented category by exploiting unlabeled data (Open-LACU). (arXiv:2002.01368v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2002.01368](http://arxiv.org/abs/2002.01368)

    Open-LACU是一种新的开放式学习策略，它可以将分类器推广到观察到的和未观察到的新颖类别之间，并通过定义不同的背景和未知类别来提高训练成本效益性，确保在存在未观察到的新颖类别时进行安全分类。

    

    对于半监督学习（SSL）和开放式识别（OSR），已经进行了许多尝试以合成单个训练策略。然而，每次尝试都违反了开放集定义，因为这些方法在未标记的训练集中包含新颖的类别。本研究提出了一种新的学习策略，其中分类器能够在观察到的和未观察到的新颖类别之间进行推广，从而定义了观察到新颖类别的背景类别和未观察到新颖类别的未知类别。通过分类这两种新颖类别的方式，Open-LACU能够提高训练的成本效益性，并确保在存在未观察到的新颖类别时进行安全分类。

    Several efforts have been made to synthesize semi-supervised learning (SSL) and open set recognition (OSR) within a single training policy. However, each attempt violated the definition of an open set by incorporating novel categories within the unlabeled training set. Although such \textit{observed} novel categories are undoubtedly prevalent in application-grade datasets, they should not be conflated with the OSR-defined \textit{unobserved} novel categories, which only emerge during testing. This study proposes a new learning policy wherein classifiers generalize between observed and unobserved novel categories. Specifically, our open-set learning with augmented category by exploiting unlabeled data (Open-LACU) policy defines a background category for observed novel categories and an unknown category for unobserved novel categories. By separating these novel category types, Open-LACU promotes cost-efficient training by eliminating the need to label every category and ensures safe clas
    

